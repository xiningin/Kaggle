{"cell_type":{"7c3a20c3":"code","7cee408c":"code","3038eb3e":"code","537ef0d0":"code","bd330f1a":"code","03776a06":"code","14ca66f3":"code","a5b72a16":"code","a2d319da":"code","92c071f6":"code","9b103044":"code","f1b3b051":"code","13b281be":"code","8ed2ff54":"code","aa1d4679":"code","4b2ac788":"code","be6f33e3":"code","775a0ffc":"code","97eabb70":"code","dc12be03":"code","66be2b76":"code","e999ae34":"code","c51cae50":"code","edb9b9bb":"code","9cf2ed6d":"code","f13a75d1":"code","826521c2":"code","70d8186c":"code","93d7c402":"code","8317e782":"code","aaacc8d3":"code","3ebf55bf":"code","21fa3712":"code","5d9357ef":"code","72e19d63":"code","30eac241":"code","b4fcab2e":"code","13e6cd57":"code","970943e3":"code","038b64b9":"code","81f4b5ca":"code","725a67c7":"code","b58cd975":"code","559891cd":"code","b65fc975":"code","9b0dbc7e":"code","ba9355e0":"code","1f9ac172":"code","feae30fc":"code","24439068":"code","ce42c07f":"code","477daeee":"code","dd6e4ad7":"code","fffaab33":"code","df742eef":"code","bdc53031":"code","990cf0e0":"code","296bee36":"code","50782751":"code","c0683783":"code","335f09e8":"code","7c366522":"code","cb806a6d":"code","8a3c48ae":"code","78c813eb":"code","0f6f4c4f":"code","0af05396":"code","685b287e":"code","ca5aaa9b":"code","96c35f51":"code","cd486847":"code","441aabd6":"code","ecdb9a36":"code","9287686c":"markdown","94d84340":"markdown","e5f94a65":"markdown","66d9159f":"markdown","567b5228":"markdown","205c053f":"markdown","143b5af8":"markdown","87abf474":"markdown","f82310ae":"markdown","760afd03":"markdown","bbf9399a":"markdown","55b189c6":"markdown","5c656280":"markdown","f3466303":"markdown","43972736":"markdown","553726f2":"markdown","6930df7b":"markdown","345c890e":"markdown","b502a256":"markdown","3f9fbc21":"markdown","41401f93":"markdown","2e286037":"markdown","98927bc7":"markdown","9e970d79":"markdown","8a77e812":"markdown","d61a9d0f":"markdown","9ac28ea4":"markdown","d8821db2":"markdown","495a42aa":"markdown","257baece":"markdown","8fb6511b":"markdown","1b1db781":"markdown","73045701":"markdown","635fb162":"markdown","6c74de45":"markdown","36f64874":"markdown","edb63669":"markdown","cb4da3ce":"markdown","6d003d7a":"markdown","20882379":"markdown"},"source":{"7c3a20c3":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# sklearn\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\n\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\nfrom sklearn.preprocessing import StandardScaler, RobustScaler, QuantileTransformer\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import LinearRegression, Ridge, HuberRegressor, Lasso, ElasticNet, BayesianRidge\nfrom sklearn.kernel_ridge import KernelRidge\n\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import AdaBoostRegressor, GradientBoostingRegressor\nfrom sklearn.neural_network import MLPRegressor\n\n# outros modelos\nimport xgboost as xgb\nfrom xgboost import XGBRegressor\nimport lightgbm as lgb\nfrom lightgbm import LGBMRegressor\n\nfrom mlxtend.regressor import StackingRegressor\n\n# alertas\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)","7cee408c":"df_train = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ndf_test = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')","3038eb3e":"df_train.info()","537ef0d0":"df_train.head()","bd330f1a":"# Removendo ID\ndf_train.drop('Id',axis=1,inplace=True )\nid_test = df_test['Id']\ndf_test.drop('Id',axis=1,inplace=True )\n\ndf_train.head()","03776a06":"df_train.info()","14ca66f3":"df_train.describe().transpose()","a5b72a16":"df_train.describe(include = ['O']).transpose()","a2d319da":"plt.figure(figsize=(10,7))\nsns.histplot(df_train['SalePrice']);","92c071f6":"total = df_train.isnull().sum().sort_values(ascending=False)\npercent = (df_train.isnull().sum()\/df_train.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n\nmissing_data.head(25)","9b103044":"df_train.drop(['PoolQC', 'FireplaceQu', 'Fence', \n               'Alley', 'MiscFeature'], axis=1, inplace=True)\ndf_test.drop(['PoolQC', 'FireplaceQu', 'Fence',\n               'Alley', 'MiscFeature'], axis=1, inplace=True)","f1b3b051":"df_train.corr()['SalePrice'].sort_values(ascending=False)","13b281be":"plt.figure(figsize=(12, 9))\nsns.heatmap(df_train.corr());","8ed2ff54":"topCols = df_train.corr().nlargest(10, 'SalePrice')['SalePrice'].index\ntopCols","aa1d4679":"plt.figure(figsize=(12, 9))\nsns.heatmap(df_train[topCols].corr(), annot=True, fmt='.2f');","4b2ac788":"sns.pairplot(df_train[topCols], height=2);","be6f33e3":"numerical_columns = df_train.select_dtypes(exclude=['object']).columns.tolist()\nprint(numerical_columns)","775a0ffc":"df_train[\"SalePrice_Log\"] = np.log1p(df_train[\"SalePrice\"])","97eabb70":"plt.figure(figsize=(10, 7))\nsns.scatterplot(data=df_train, x='GrLivArea', y='SalePrice');","dc12be03":"outliers_GrLivArea = df_train.loc[(df_train['GrLivArea']>4000.0) & (df_train['SalePrice']<300000.0)]\noutliers_GrLivArea[['GrLivArea' , 'SalePrice']] # necess\u00e1rio remover","66be2b76":"# Para \u00e1reas internas\ndf_train['all_Liv_SF'] = df_train['TotalBsmtSF'] + df_train['1stFlrSF'] + df_train['2ndFlrSF'] \ndf_test['all_Liv_SF'] = df_test['TotalBsmtSF'] + df_test['1stFlrSF'] + df_test['2ndFlrSF'] \n\nprint(df_train['all_Liv_SF'].corr(df_train['SalePrice']))\nprint(df_train['all_Liv_SF'].corr(df_train['SalePrice_Log']))","e999ae34":"# Para \u00e1reas externas\ndf_train['all_SF'] = ( df_train['all_Liv_SF'] + df_train['GarageArea'] + df_train['MasVnrArea'] \n                       + df_train['WoodDeckSF'] + df_train['OpenPorchSF'] + df_train['ScreenPorch'] )\ndf_test['all_SF'] = ( df_test['all_Liv_SF'] + df_test['GarageArea'] + df_test['MasVnrArea']\n                      + df_test['WoodDeckSF'] + df_test['OpenPorchSF'] + df_train['ScreenPorch'] )\n\nprint(df_train['all_SF'].corr(df_train['SalePrice']))\nprint(df_train['all_SF'].corr(df_train['SalePrice_Log']))","c51cae50":"df_train['all_SF'].corr(df_train['all_Liv_SF']) # pode dar problema essa bicorrela\u00e7\u00e3o, mas ser\u00e1 ignorada","edb9b9bb":"# Outliers para allSF\noutliers_allSF = df_train.loc[(df_train['all_SF']>8000.0) & (df_train['SalePrice']<200000.0)]\noutliers_allSF[['all_SF' , 'SalePrice']]","9cf2ed6d":"df_train = df_train.drop(outliers_allSF.index)","f13a75d1":"df_train.corr().abs()[['SalePrice','SalePrice_Log']].sort_values(by='SalePrice', ascending=False)[2:16]","826521c2":"plt.figure(figsize=(15,10))\nsns.boxplot(data=df_train, x='OverallQual', y='SalePrice');","70d8186c":"outliers_OverallQual_4 = df_train.loc[(df_train['OverallQual']==4) & (df_train['SalePrice']>200000.0)]\noutliers_OverallQual_8 = df_train.loc[(df_train['OverallQual']==8) & (df_train['SalePrice']>500000.0)]\noutliers_OverallQual_9 = df_train.loc[(df_train['OverallQual']==9) & (df_train['SalePrice']>500000.0)]\noutliers_OverallQual_10 = df_train.loc[(df_train['OverallQual']==10) & (df_train['SalePrice']>700000.0)]\n\noutliers_OverallQual = pd.concat([outliers_OverallQual_4, outliers_OverallQual_8, \n                                  outliers_OverallQual_9, outliers_OverallQual_10])","93d7c402":"outliers_OverallQual[['OverallQual' , 'SalePrice']]","8317e782":"df_train = df_train.drop(outliers_OverallQual.index)","aaacc8d3":"df_train.corr().abs()[['SalePrice','SalePrice_Log']].sort_values(by='SalePrice', ascending=False)[2:16]","3ebf55bf":"# Criando vari\u00e1veis para transformar\ny , y_log = df_train[\"SalePrice\"] , df_train[\"SalePrice_Log\"]\ndf_train.drop([\"SalePrice\", \"SalePrice_Log\"] , axis=1, inplace=True)","21fa3712":"X_1 = df_train\ny_1 = y_log","5d9357ef":"X_2 = df_train\ny_2 = y","72e19d63":"numerical_features = df_train.select_dtypes(exclude=['object']).columns.tolist()\ncategorical_features = df_train.select_dtypes(include=['object']).columns.tolist()","30eac241":"X_1","b4fcab2e":"# Num\u00e9rico\nnumerical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='median')),\n    ('scaler', StandardScaler())])","13e6cd57":"# Categ\u00f3rico\ncategorical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))])","970943e3":"# Juntando\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numerical_transformer,   numerical_features),\n        ('cat', categorical_transformer, categorical_features)])","038b64b9":"# Pipeline com modelos de aprendizagem lineares\n\n# LinearRegression\npipe_Linear = Pipeline(\n    steps   = [('preprocessor', preprocessor),\n               ('Linear', LinearRegression()) ])    \n# Ridge\npipe_Ridge = Pipeline(\n    steps  = [('preprocessor', preprocessor),\n              ('Ridge', Ridge(random_state=5)) ])  \n# Lasso\npipe_Lasso = Pipeline(\n    steps  = [ ('preprocessor', preprocessor),\n               ('Lasso', Lasso(random_state=5)) ])\n# ElasticNet\npipe_ElaNet = Pipeline(\n    steps   = [ ('preprocessor', preprocessor),\n                ('ElaNet', ElasticNet(random_state=5)) ])\n\n# BayesianRidge\npipe_BayesRidge = Pipeline(\n    steps   = [ ('preprocessor', preprocessor),\n                ('BayesRidge', BayesianRidge(n_iter=500, compute_score=True)) ])\n","81f4b5ca":"# Pipeline com modelos de aprendizagem ensemble\n\n# GradientBoostingRegressor\npipe_GBR  = Pipeline(\n    steps = [ ('preprocessor', preprocessor),\n              ('GBR', GradientBoostingRegressor(random_state=5 )) ])\n\n# XGBRegressor\npipe_XGB  = Pipeline(\n    steps = [ ('preprocessor', preprocessor),\n              ('XGB', XGBRegressor(objective='reg:squarederror', metric='rmse', \n                      random_state=5, nthread = -1)) ])\n# LGBM\npipe_LGBM = Pipeline(\n    steps= [('preprocessor', preprocessor),\n            ('LGBM', LGBMRegressor(objective='regression', metric='rmse',\n                                  random_state=5)) ])\n# AdaBoostRegressor\npipe_ADA = Pipeline(\n    steps= [('preprocessor', preprocessor),\n            ('ADA', AdaBoostRegressor(DecisionTreeRegressor(), \n                random_state=5, loss='exponential')) ])","725a67c7":"list_pipelines = [pipe_Linear, pipe_Ridge, pipe_Lasso, pipe_ElaNet]","b58cd975":"print(\"model\", \"\\t\", \"mean rmse\", \"\\t\", \"std\", \"\\t\", \"\\t\", \"min rmse\")\nprint(\"-+\"*30)\nfor pipe in list_pipelines :\n    \n    scores = cross_val_score(pipe, X_1, y_1, scoring='neg_mean_squared_error', cv=5)\n    scores = np.sqrt(-scores)\n    print(pipe.steps[1][0], \"\\t\", \n          '{:08.6f}'.format(np.mean(scores)), \"\\t\",  \n          '{:08.6f}'.format(np.std(scores)),  \"\\t\", \n          '{:08.6f}'.format(np.min(scores)))","559891cd":"list_pipelines = [pipe_GBR, pipe_XGB, pipe_LGBM, pipe_ADA]","b65fc975":"print(\"model\", \"\\t\", \"mean rmse\", \"\\t\", \"std\", \"\\t\", \"\\t\", \"min rmse\")\nprint(\"-+\"*30)\n\nfor pipe in list_pipelines :\n    with warnings.catch_warnings():\n        warnings.filterwarnings(\"ignore\",category=FutureWarning)\n        scores = cross_val_score(pipe, X_1, y_1, scoring='neg_mean_squared_error', cv=5)\n        scores = np.sqrt(-scores)\n        print(pipe.steps[1][0], \"\\t\", \n          '{:08.6f}'.format(np.mean(scores)), \"\\t\",  \n          '{:08.6f}'.format(np.std(scores)),  \"\\t\", \n          '{:08.6f}'.format(np.min(scores)))","9b0dbc7e":"# Para teste com outras formas de padroniza\u00e7\u00e3o de dados\nlist_scalers = [StandardScaler(), \n                RobustScaler(), \n                QuantileTransformer(output_distribution='normal')]\n\n# Usaremos apenas o mais comum\nlist_scalers = [StandardScaler()]","ba9355e0":"parameters_Linear = { 'preprocessor__num__scaler': list_scalers,\n                     'Linear__fit_intercept':  [True,False],\n                     'Linear__normalize':  [True,False] }\n\ngscv_Linear = GridSearchCV(pipe_Linear, parameters_Linear, n_jobs=-1, \n                          scoring='neg_mean_squared_error', verbose=0, cv=5)\ngscv_Linear.fit(X_1, y_1)","1f9ac172":"print(np.sqrt(-gscv_Linear.best_score_))  \ngscv_Linear.best_params_","feae30fc":"parameters_Ridge = { 'preprocessor__num__scaler': list_scalers,\n                     'Ridge__alpha': [7,8,9],\n                     'Ridge__fit_intercept':  [True,False],\n                     'Ridge__normalize':  [True,False] }\n\ngscv_Ridge = GridSearchCV(pipe_Ridge, parameters_Ridge, n_jobs=-1, \n                          scoring='neg_mean_squared_error', verbose=0, cv=5)\ngscv_Ridge.fit(X_1, y_1)","24439068":"print(np.sqrt(-gscv_Ridge.best_score_))  \ngscv_Ridge.best_params_","ce42c07f":"parameters_Lasso = { 'preprocessor__num__scaler': list_scalers,\n                     'Lasso__alpha': [0.0005, 0.001],\n                     'Lasso__fit_intercept':  [True],\n                     'Lasso__normalize':  [True,False] }\n\ngscv_Lasso = GridSearchCV(pipe_Lasso, parameters_Lasso, n_jobs=-1, \n                          scoring='neg_mean_squared_error', verbose=1, cv=5)\ngscv_Lasso.fit(X_1, y_1)","477daeee":"print(np.sqrt(-gscv_Lasso.best_score_))  \ngscv_Lasso.best_params_","dd6e4ad7":"parameters_ElaNet = { 'ElaNet__alpha': [0.0005, 0.001],\n                      'ElaNet__l1_ratio':  [0.85, 0.9],\n                      'ElaNet__normalize':  [True,False] }\n\ngscv_ElaNet = GridSearchCV(pipe_ElaNet, parameters_ElaNet, n_jobs=-1, \n                          scoring='neg_mean_squared_error', verbose=1, cv=5)\ngscv_ElaNet.fit(X_1, y_1)","fffaab33":"print(np.sqrt(-gscv_ElaNet.best_score_))  \ngscv_ElaNet.best_params_","df742eef":"list_pipelines_gscv = [gscv_Linear,gscv_Ridge,gscv_Lasso,gscv_ElaNet]","bdc53031":"print(\"model\", \"\\t\", \"mean rmse\", \"\\t\", \"std\", \"\\t\", \"\\t\", \"min rmse\")\nprint(\"-+\"*30)\nfor gscv in list_pipelines_gscv :\n    \n    scores = cross_val_score(gscv.best_estimator_, X_1, y_1, \n                             scoring='neg_mean_squared_error', cv=5)\n    scores = np.sqrt(-scores)\n    print(gscv.estimator.steps[1][0], \"\\t\", \n          '{:08.6f}'.format(np.mean(scores)), \"\\t\",  \n          '{:08.6f}'.format(np.std(scores)),  \"\\t\", \n          '{:08.6f}'.format(np.min(scores)))","990cf0e0":"'''parameters_GBR = { 'GBR__n_estimators':  [400], \n                   'GBR__max_depth':  [3,4],\n                   'GBR__min_samples_leaf':  [5,6],                 \n                   'GBR__max_features':  [\"auto\",0.5,0.7],                  \n                 }\n                   \ngscv_GBR = GridSearchCV(pipe_GBR, parameters_GBR, n_jobs=-1, \n                        scoring='neg_mean_squared_error', verbose=1, cv=5)\ngscv_GBR.fit(X_1, y_1)'''","296bee36":"'''print(np.sqrt(-gscv_GBR.best_score_))  \ngscv_GBR.best_params_'''","50782751":"'''parameters_XGB = { 'XGB__learning_rate': [0.021,0.022],\n                   'XGB__max_depth':  [2,3],\n                   'XGB__n_estimators':  [2000], \n                   'XGB__reg_lambda':  [1.5, 1.6], \n                   'XGB__reg_alpha':  [1,1.5],                   \n# colsample_bytree , subsample               \n                  }\n                   \ngscv_XGB = GridSearchCV(pipe_XGB, parameters_XGB, n_jobs=-1, \n                        scoring='neg_mean_squared_error', verbose=1, cv=5)\ngscv_XGB.fit(X_1, y_1)'''","c0683783":"'''print(np.sqrt(-gscv_XGB.best_score_))  \ngscv_XGB.best_params_'''","335f09e8":"'''parameters_LGBM = { 'LGBM__learning_rate': [0.01,0.02],\n                    'LGBM__n_estimators':  [1000], \n                    'LGBM__num_leaves':  [8,10],\n                    'LGBM__bagging_fraction':  [0.7,0.8],\n                    'LGBM__bagging_freq':  [1,2],                  \n                   }\n\ngscv_LGBM = GridSearchCV(pipe_LGBM, parameters_LGBM, n_jobs=-1, \n                       scoring='neg_mean_squared_error', verbose=1, cv=5)\ngscv_LGBM.fit(X_1, y_1)'''","7c366522":"'''print(np.sqrt(-gscv_LGBM.best_score_))  \ngscv_LGBM.best_params_'''","cb806a6d":"'''parameters_ADA = { 'ADA__learning_rate': [3.5],\n                   'ADA__n_estimators':  [500], \n                   'ADA__base_estimator__max_depth':  [8,9,10],                  \n                 }\n\npipe_ADA = Pipeline(\n    steps= [('preprocessor', preprocessor),\n            ('ADA', AdaBoostRegressor(\n                DecisionTreeRegressor(min_samples_leaf=5,\n                                      min_samples_split=5), \n                random_state=5,loss='exponential')) ])\n\ngscv_ADA = GridSearchCV(pipe_ADA, parameters_ADA, n_jobs=-1, \n                       scoring='neg_mean_squared_error', verbose=1, cv=5)\ngscv_ADA.fit(X_1, y_1)'''","8a3c48ae":"'''print(np.sqrt(-gscv_ADA.best_score_))  \ngscv_ADA.best_params_'''","78c813eb":"'''list_pipelines_gscv = [gscv_GBR, gscv_XGB,gscv_LGBM,gscv_ADA]'''","0f6f4c4f":"'''print(\"model\", \"\\t\", \"mean rmse\", \"\\t\", \"std\", \"\\t\", \"\\t\", \"min rmse\")\nprint(\"-+\"*30)\nfor gscv in list_pipelines_gscv :\n    with warnings.catch_warnings():\n        warnings.filterwarnings(\"ignore\",category=FutureWarning)    \n        scores = cross_val_score(gscv.best_estimator_, X_1, y_1, \n                             scoring='neg_mean_squared_error', cv=5)\n        scores = np.sqrt(-scores)\n        print(gscv.estimator.steps[1][0], \"\\t\", \n          '{:08.6f}'.format(np.mean(scores)), \"\\t\",  \n          '{:08.6f}'.format(np.std(scores)),  \"\\t\", \n          '{:08.6f}'.format(np.min(scores)))'''","0af05396":"'''lnr = LinearRegression(n_jobs = -1)\n\nrdg = Ridge(alpha=3.0, copy_X=True, fit_intercept=True, random_state=1)\n\nrft = RandomForestRegressor(n_estimators = 12, max_depth = 3, n_jobs = -1, random_state=1)\n\ngbr = GradientBoostingRegressor(n_estimators = 40, max_depth = 2, random_state=1)\n\nmlp = MLPRegressor(hidden_layer_sizes = (90, 90), alpha = 2.75, random_state=1)'''","685b287e":"'''stack1 = StackingRegressor(regressors = [rdg, rft, gbr], \n                           meta_regressor = lnr)'''","ca5aaa9b":"'''pipe_STACK_1 = Pipeline(steps=[ ('preprocessor', preprocessor),\n                                ('stack1', stack1) ])\n\npipe_STACK_1.fit(X_1, y_1) '''","96c35f51":"'''\nscores = cross_val_score(pipe_STACK_1, X_1, y_1, scoring='neg_mean_squared_error', cv=5)\nscores = np.sqrt(-scores)\nprint(pipe_STACK_1.steps[1][0], \"\\t\", \n        '{:08.6f}'.format(np.mean(scores)), \"\\t\",  \n        '{:08.6f}'.format(np.std(scores)),  \"\\t\", \n        '{:08.6f}'.format(np.min(scores)))'''","cd486847":"model = gscv_Lasso.best_estimator_","441aabd6":"prediction = model.predict(df_test)","ecdb9a36":"subm = pd.DataFrame()\nsubm['Id'] = id_test\nsubm['SalePrice'] = np.expm1(prediction) # Exponencial - 1, por causa do log\nsubm.to_csv('Lasso.csv', index=False)","9287686c":"## Predi\u00e7\u00f5es com banco de dados de teste","94d84340":"#### Atributo _OverallQual_","e5f94a65":"**alpha** :\nRegularization strength, must be a positive float  \n**fit_intercept** : bool, default True  \n**normalize** : boolean, optional, default False  \n**copy_X** : boolean, optional, default True  \n**max_iter** : int  \n**tol** : float  \nPrecision of the solution  \n**solver** : {\u2018auto\u2019, \u2018svd\u2019, \u2018cholesky\u2019, \u2018lsqr\u2019, \u2018sparse_cg\u2019, \u2018sag\u2019, \u2018saga\u2019}  \nSolver to use in the computational routines","66d9159f":"## An\u00e1lises e pr\u00e9-processamentos","567b5228":"* Com exe\u00e7\u00e3o do AdaBoost, os hiper-par\u00e2metros padr\u00f5es funcionaram bem.","205c053f":"#### ElasticNet","143b5af8":"## Pr\u00e9-processamento e _pipelines_","87abf474":"#### Lasso","f82310ae":"#### AdaBoostRegressor","760afd03":"* Modelos lineares e _Rigde_ est\u00e3o bons com hiper-par\u00e2metros padr\u00f5es;\n\n* Demais precisam de otimiza\u00e7\u00e3o.","bbf9399a":"### Corela\u00e7\u00f5es","55b189c6":"#### GradientBoostingRegressor","5c656280":"### Explora\u00e7\u00e3o","f3466303":"### Atributos num\u00e9ricos","43972736":"### Valores nulos (_missing data_)","553726f2":"stack1","6930df7b":"#### Linear Regression","345c890e":"### Modelos lineares","b502a256":"## _Stacking_ ","3f9fbc21":"#### Ridge","41401f93":"#### LGBM","2e286037":"#### XGB","98927bc7":"#### Atributo _GrLivArea_","9e970d79":"#### Valida\u00e7\u00e3o cruzada dos modelos _ensemble_ aperfei\u00e7oados","8a77e812":"## Otimiza\u00e7\u00e3o de hiper-par\u00e2metros","d61a9d0f":"**fit_intercept** : boolean, optional, default True  \n**normalize** : boolean, optional, default False  \n**copy_X** : boolean, optional, default True  \n**n_jobs** : int or None, optional (default=None)  \nThe number of jobs to use for the computation. This will only provide speedup for n_targets > 1 and sufficient large problems. None means 1 unless in a joblib.parallel_backend context. -1 means using all processors.","9ac28ea4":"### Modelos _ensemble_","d8821db2":"**loss** : {\u2018ls\u2019, \u2018lad\u2019, \u2018huber\u2019, \u2018quantile\u2019}, optional (default=\u2019ls\u2019)  \n**learning_rate** : float, optional (default=0.1)  \n**n_estimators** : int (default=100)  \n**subsample** : float, optional (default=1.0)  \n**criterion** : string, optional (default=\u201dfriedman_mse\u201d)  \n**min_samples_split** : int, float, optional (default=2)  \nIf int: minimum number. If float: fraction  \n**min_samples_leaf** : int, float, optional (default=1)   \nIf int minimum number. If float fraction   \n**min_weight_fraction_leaf** : float, optional (default=0.)  \n**max_depth** : integer, optional (default=3)\nmaximum depth of the individual regression estimators.  \n**min_impurity_decrease** : float, optional (default=0.)\n\n**max_features** : int, float, string or None, optional (default=None)  \nThe number of features to consider when looking for the best split:  \nIf float: fraction  \nIf \u201cauto\u201d, then max_features=n_features.  \nIf \u201csqrt\u201d, then max_features=sqrt(n_features).  \nIf \u201clog2\u201d, then max_features=log2(n_features).  \nIf None, then max_features=n_features.  \nChoosing max_features < n_features leads to a reduction of variance and an increase in bias.  \n","495a42aa":"### Importando bibliotecas","257baece":"**num_iterations**, default=100, alias=num_iteration,num_tree,num_trees,num_round,num_rounds  \n**learning_rate**, default=0.1, alias=shrinkage_rate  \n**num_leaves**, default=31\n\n\n**max_depth**, default=-1, < 0 means no limit  \n**min_data_in_leaf**, default=20, type=int, alias=min_data_per_leaf , min_data  \n**min_sum_hessian_in_leaf**, default=1e-3, alias=min_sum_hessian_per_leaf, min_sum_hessian, min_hessian  \n**feature_fraction**, default=1.0, 0.0 < feature_fraction < 1.0, alias=sub_feature  \n**bagging_fraction**, default=1.0, 0.0 < bagging_fraction < 1.0, alias=sub_row  \n**bagging_freq**, default=0,   \nFrequency for bagging, 0 means disable bagging. k means will perform bagging at every k iteration   \n**early_stopping_round** , default=0, type=int, alias=early_stopping_rounds,early_stopping  \nWill stop training if one metric of one validation data doesn\u2019t improve in last early_stopping_round rounds  \n**lambda_l1** , default=0  \n**lambda_l2** , default=0","8fb6511b":"### Lendo e tratando o banco de dados","1b1db781":"#### Atributos combinados","73045701":"## Valida\u00e7\u00e3o cruzada (_cross-validation_)","635fb162":"### Modelos lineares","6c74de45":"## Leitura e explora\u00e7\u00e3o do banco de dados","36f64874":"General Parameters  \n**booster**: gbtree, gblinear or dart, default= gbtree   \n\nParameters for Tree Booster  \n**eta**, alias: learning_rate, 0<eta<1 , default=0.3  \n**gamma**, alias: min_split_loss,  default=0,  \nMinimum loss reduction required to make a further partition on a leaf node of the tree. The larger gamma is, the more conservative the algorithm will be.  \n**max_depth**, default=6  \n**min_child_weight**, default=1\nThe larger min_child_weight is, the more conservative the algorithm will be.  \n**max_delta_step** [default=0]  \n**subsample** [default=1],  range: (0,1]  \nSetting it to 0.5 means that XGBoost would randomly sample half of the training data prior to growing trees. Subsampling will occur once in every boosting iteration.  \n**colsample_bytree, colsample_bylevel, colsample_bynode** [default=1]  \nThis is a family of parameters for subsampling of columns.  \nAll colsample_by* parameters have a range of (0, 1], the default value of 1, and specify the fraction of columns to be subsampled.  \ncolsample_by* parameters work cumulatively. For instance, the combination {'colsample_bytree':0.5, 'colsample_bylevel':0.5, 'colsample_bynode':0.5} with 64 features will leave 8 features to choose from at each split.  \n**lambda** [default=1, alias: reg_lambda]  \nL2 regularization term on weights. Increasing this value will make model more conservative.  \n**alpha** [default=0, alias: reg_alpha]  \nL1 regularization term on weights. Increasing this value will make model more conservative.  \n**tree_method** string [default= auto]  \nChoices: auto, exact, approx, hist, gpu_hist  ","edb63669":"# Competi\u00e7\u00e3o _Kaggle_ - _House Prices_\n\nAn\u00e1lise de resolu\u00e7\u00f5es do desafio de regress\u00e3o de pre\u00e7o de casas (com *dataset* do [_Kaggle_](https:\/\/www.kaggle.com\/c\/house-prices-advanced-regression-techniques\/overview));\n\n---\n\n[Open in Colab](https:\/\/colab.research.google.com\/drive\/1nKsflZwXqhKX0FZZmXehTwROWCG-qRoL?usp=sharing) \n\n[Open in Kaggle](https:\/\/www.kaggle.com\/leonichel\/competi-o-kaggle-house-prices)\n\n---\n\n[Leonichel Guimar\u00e3es (PIBITI\/CNPq-FA-UEM)](https:\/\/github.com\/leonichel)\n\nProfessora Linnyer Ruiz (orientadora)\n\n---\n\nRefer\u00eancias bibliogr\u00e1ficas:\n\n* [Pedro Marcelino. Kaggle. **Comprehensive data exploration with Python**](https:\/\/www.kaggle.com\/pmarcelino\/comprehensive-data-exploration-with-python);\n\n* [Serigne. Kaggle. **Stacked Regressions to predict House Prices**](https:\/\/www.kaggle.com\/serigne\/stacked-regressions-top-4-on-leaderboard\/);\n\n* [Deja vu. Kaggle. **House Prices: Plotly, Pipelines and Ensemble**](https:\/\/www.kaggle.com\/dejavu23\/house-prices-plotly-pipelines-and-ensembles\/).\n\n---\n\nManna Team  |  UEM       |     CNPq\n:----------:|:----------:|:----------:|\n<img src=\"https:\/\/manna.team\/_next\/static\/images\/logo2-e283461cfa92b2105bfd67e8e530529e.png\" alt=\"Manna Team\" width=\"200\"\/> | <img src=\"https:\/\/marcoadp.github.io\/WebSiteDIN\/img\/logo-uem2.svg\" alt=\"UEM\" width=\"200\"\/> | <img src=\"https:\/\/www.gov.br\/cnpq\/pt-br\/canais_atendimento\/identidade-visual\/logo_cnpq.svg\" alt=\"CNPq\" width=\"200\"\/>","cb4da3ce":"### Modelos _ensemble_","6d003d7a":"#### Logar\u00edtimo","20882379":"#### Valida\u00e7\u00e3o cruzada dos modelos lineares aperfei\u00e7oados"}}