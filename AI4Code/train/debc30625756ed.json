{"cell_type":{"aef37cba":"code","f29baf92":"code","5ccfa05b":"code","027a4c16":"code","015e3f06":"code","c40c60d1":"code","ec2dd8be":"code","4b64a932":"code","132a6acb":"code","fe3172b9":"code","aa5509d9":"code","9a652f3c":"code","d1bca093":"code","bbcfb0c6":"code","189ef05e":"code","e1a54ac3":"code","9a62ab5e":"code","e2c96439":"code","97d46ec4":"markdown","8737b790":"markdown","c85a37f6":"markdown","3a32cc04":"markdown","4ef61b8d":"markdown","b6a9411d":"markdown","75ca8fa4":"markdown"},"source":{"aef37cba":"import numpy as np \nimport pandas as pd \nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom xgboost import XGBClassifier\nfrom matplotlib import colors \nfrom matplotlib.ticker import PercentFormatter \nimport numpy as np \nimport matplotlib.pyplot as plt \n\nimport os\nprint(os.listdir(\"..\/input\"))\n\nabnormal = pd.read_csv(\"..\/input\/ptbdb_abnormal.csv\", header = None) \nnormal = pd.read_csv(\"..\/input\/ptbdb_normal.csv\", header = None)\n\nabnormal = abnormal.drop([187], axis=1)\nnormal = normal.drop([187], axis=1)\n\nabnormal.head","f29baf92":"flatten_ab_y = (abnormal.values)\nflatten_ab_y  = flatten_ab_y[:,5:70].flatten()\n\nprint(flatten_ab_y.shape)\n\nab_x=np.arange(0,65)\nab_x = np.tile(ab_x, abnormal.shape[0])\n\nplt.hist2d(ab_x, flatten_ab_y, bins = (65,100), cmap = plt.cm.jet) \n\nplt.show()","5ccfa05b":"plt.plot((abnormal.values)[0][5:70])\nplt.show()\n\nplt.plot((abnormal.values)[50][5:70])\nplt.show()\n\nplt.plot((abnormal.values)[117][5:70])\nplt.show()\n\nplt.plot((abnormal.values)[1111][5:70])\nplt.show()\n\nplt.plot((abnormal.values)[100][5:70])\nplt.show()","027a4c16":"flatten_norm_y = normal.values\nflatten_norm_y  = flatten_norm_y[:,5:70].flatten()\n\nnorm_x=np.arange(0,65)\nnorm_x = np.tile(norm_x, normal.shape[0])\n\nplt.hist2d(norm_x,flatten_norm_y, bins=(65,100), cmap=plt.cm.jet)\nplt.show()","015e3f06":"plt.plot((normal.values)[0][5:70])\nplt.show()\n\nplt.plot((normal.values)[50][5:70])\nplt.show()\n\nplt.plot((normal.values)[117][5:70])\nplt.show()\n\nplt.plot((normal.values)[1111][5:70])\nplt.show()\n\nplt.plot((normal.values)[100][5:70])\nplt.show()","c40c60d1":"y_abnormal = np.ones((abnormal.shape[0]))\ny_abnormal = pd.DataFrame(y_abnormal)\n\ny_normal = np.zeros((normal.shape[0]))\ny_normal = pd.DataFrame(y_normal)\n\nX = pd.concat([abnormal, normal], sort=True)\ny = pd.concat([y_abnormal, y_normal] ,sort=True)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","ec2dd8be":"print(abnormal.dtypes, normal.dtypes)","4b64a932":"abnormal.shape","132a6acb":"normal.shape","fe3172b9":"np.any(X_train.isna().sum())","aa5509d9":"np.any(X_test.isna().sum())","9a652f3c":"seed=123\n\nclassifiers = [\n    LogisticRegression(class_weight='balanced', random_state=seed),\n    KNeighborsClassifier(3, n_jobs=-1),\n    SVC(gamma='auto', class_weight='balanced', random_state=seed),\n    RandomForestClassifier(random_state=seed, n_estimators = 1000),\n    MLPClassifier(alpha=1, max_iter=1000),\n    XGBClassifier(learning_rate =0.1,n_estimators=1000, max_depth=5, min_child_weight=1, gamma=0, subsample=0.8,\n                  colsample_bytree=0.8, objective= 'binary:logistic',nthread=4, scale_pos_weight=1,seed=seed)\n]\n\nnames = [\"Logistic\", \"Nearest Neighbors\", \"RBF SVM\", \"Random Forest\", \"Neural Net\", \"XGB\"]","d1bca093":"from sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score\nfrom sklearn.utils.validation import column_or_1d\n\nfor name, clf in zip(names, classifiers):\n        \n    y_train = column_or_1d(y_train, warn=True)\n    clf.fit(X_train, y_train)\n    print(f\"{name}: {round(accuracy_score(y_test, clf.predict(X_test)),3)}\")","bbcfb0c6":"clf = XGBClassifier(learning_rate =0.1,n_estimators=1000, max_depth=5, min_child_weight=1, gamma=0, subsample=0.8,\n                  colsample_bytree=0.8, objective= 'binary:logistic',nthread=4, scale_pos_weight=1,seed=seed)\n\nclf.fit(X_train, y_train)","189ef05e":"y_pred = clf.predict(X_test)","e1a54ac3":"y_pred = np.reshape(y_pred, (y_pred.shape[0],1))\n\ny_pred.shape","9a62ab5e":"y_pred = clf.predict(X_test)\n\nprecision = precision_score(y_test, y_pred)\nrecall = recall_score(y_test, y_pred)\naccuracy = accuracy_score(y_test, y_pred)\nf1 = f1_score(y_test, y_pred)\n\nprint(\"Precision:\", precision)\nprint(\"Recall:\", recall)\nprint(\"Accuracy:\", accuracy)\nprint(\"f1:\", f1)","e2c96439":"# feature importance\nfrom matplotlib import pyplot as plt\n\nfeature_imp = np.argsort(clf.feature_importances_)\nprint(np.flip(feature_imp))\n\n# plot\nplt.figure(figsize=(20,8))\n\nplt.bar(range(len(clf.feature_importances_)), clf.feature_importances_)\nplt.show()","97d46ec4":"**Display a feature importance graph. The top 18 features by importance are as follows:\n\n*4  32 137  50  43 112  29 124 108 113 165  33  84 151  46 149  69  31**","8737b790":"**From the above histogram color map for PTB data marked as abnormal, you can infer that most of the ECG features is widely distributed in the range of 0 - 0.4 and there is no fixed pattern to this data**","c85a37f6":"**From the above, you can see that PTB data marked as normal has a fixed bell shape in the data and it peaks between the features 20-30 **","3a32cc04":"**From the above histogram color map for PTB data marked as normal, you can infer that the graph of ECG features follow a standard bell shape and they peak in between the features 20-30**","4ef61b8d":"**From the above, you can see that PTB data marked as abnormal donot have any fixed pattern in the data**","b6a9411d":"**Check any of the features have a null**","75ca8fa4":"***Evaluate a bunch of ML Models against the test data, we find that XGBoost performs the best***"}}