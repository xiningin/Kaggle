{"cell_type":{"21f4f953":"code","5c4dca97":"code","ab7b4b17":"code","71fddd71":"code","d4e25f2a":"code","eae16030":"code","14197501":"code","b7326069":"code","16f92a6a":"code","266ad7bd":"code","7b03cf6d":"code","cab5894c":"code","ee288317":"code","7e1a8914":"code","1c735924":"code","d5ea80f8":"code","efc2dc17":"code","7a75a7c9":"code","4dbcef2d":"code","64287ebb":"code","fa0a994f":"code","a4acfb6f":"code","ab9f04e3":"code","ae9085f7":"code","17478eb6":"code","12b1dc33":"code","2d0e9252":"code","dc8db35b":"code","0c617f73":"code","44d64a32":"code","ace9fa5d":"code","d995772e":"code","60534229":"code","e8e94670":"code","b3526d8c":"code","0c31b041":"code","7879649b":"code","3409cf41":"code","d5424cc2":"code","dca69780":"code","cbc560b5":"code","8dbedb9c":"code","2b1fd0af":"code","711850cf":"code","4ca844aa":"code","95fc9afb":"code","da8da9bc":"code","c22ebb0d":"code","49724580":"code","e92a96cc":"code","645089f9":"code","d4f38506":"code","4ede4d88":"code","1ef6feac":"code","0dff78f6":"code","64bffdd8":"code","9c33ff6e":"code","c912f8dd":"code","43f032c0":"code","63124ad8":"code","af367af5":"code","a2caf797":"code","5a6154da":"code","0da2ae2d":"code","bcf8aac9":"code","544f5fc5":"code","4ebdff59":"code","f2d1be42":"code","c8ca8088":"markdown","248e766f":"markdown","863efb0d":"markdown","661eb963":"markdown","55e3efef":"markdown","74d7c196":"markdown","b907cb02":"markdown","71a3f149":"markdown","fcff0a76":"markdown","636b3f67":"markdown","cc08396a":"markdown","d33e2257":"markdown","c0c17b10":"markdown","6fb41503":"markdown","9de92f8b":"markdown","79c318d4":"markdown","899cadf2":"markdown","7670716c":"markdown","46230b42":"markdown","1dbe5c16":"markdown","8968645b":"markdown","88ae0cfb":"markdown","d0ec4328":"markdown","9ab70c01":"markdown","f2d55a14":"markdown","f66655cf":"markdown","4c8234ec":"markdown","5f1960ad":"markdown","0adb6d9d":"markdown","a6d1f306":"markdown","2291d180":"markdown","4229748f":"markdown","c24bc45b":"markdown","5c72e1a5":"markdown","82ffe7de":"markdown","83fc24e9":"markdown","9cb6bb44":"markdown","2460256b":"markdown","29ff1b6b":"markdown","2ebfe617":"markdown","10b81c63":"markdown","8660a247":"markdown"},"source":{"21f4f953":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt \nimport seaborn as sns\nimport sklearn","5c4dca97":"from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\nfrom sklearn.metrics import r2_score, mean_squared_error\nfrom sklearn.linear_model import LinearRegression, Ridge, Lasso\nfrom sklearn.preprocessing import PolynomialFeatures, StandardScaler, MinMaxScaler, Normalizer\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import AdaBoostRegressor","ab7b4b17":"## Forcing pandas to display any number of elements\ndef set_pandas_options() -> None:\n    pd.options.display.max_columns = 1000\n    pd.options.display.max_rows = 1000\n    pd.options.display.max_colwidth = 199\n    pd.options.display.width = None\n    pd.options.display.precision = 8\n    pd.options.display.float_format = '{:,.3f}'.format\nset_pandas_options()","71fddd71":"# importing data\ndf = pd.read_csv('\/kaggle\/input\/housesalesprediction\/kc_house_data.csv')","d4e25f2a":"df.head()","eae16030":"df.info()","14197501":"# distribtion of price variable\nplt.figure(figsize=(15,5))\nsns.distplot(df['price'], bins=20, kde=False)","b7326069":"# by looking the values count of these variable we see that these are categorial varable. \ncat_features = ['bedrooms', 'bathrooms', 'floors', 'waterfront', 'view', 'condition', 'grade']","16f92a6a":"for feature in cat_features:\n    plt.figure(figsize=(15,8))\n    sns.boxplot(y='price', x=feature, data=df)","266ad7bd":"num_feature = ['sqft_living', 'sqft_lot', 'sqft_above', 'sqft_basement', 'sqft_living15', 'sqft_lot15',\n              'yr_built', 'yr_renovated', 'lat', 'long', 'price']","7b03cf6d":"sns.pairplot(df[num_feature])","cab5894c":"# making target variable first column\ndef target_to_start(df, target):\n    feature = list(df)\n    feature.insert(0, feature.pop(feature.index(target)))\n    df = df.loc[:, feature]\n    return df\n    \ndf = target_to_start(df, 'price')","ee288317":"# removing id column because it not relevant.\ndf.drop('id', axis=1, inplace=True)","7e1a8914":"# coverting columns into integer from float.\ndf.price = df.price.astype(int)\ndf.bathrooms = df.bathrooms.astype(int)\ndf.floors = df.floors.astype(int)","1c735924":"#I remove bedrooms above 11 because price is not as high as no.of bedroom e.g with 33 bedrooms price is less than 9 bed rooms\ndf = df[df['bedrooms']<11]","d5ea80f8":"df = df[(df['bathrooms'] !=4) & (df['price'] != 7062500)]","efc2dc17":"df = df[(df['sqft_living']<13000) & (df['price']!=2280000)]","7a75a7c9":"# i think yr_built is not so useful feature so i convert this into age_of_house.\ndf['age_of_house'] = df['date'].apply(lambda x: int(x[:4])) - df['yr_built']\ndf.drop('yr_built', axis=1, inplace=True)","4dbcef2d":"# droping data columns because we do not need anymore.\ndf.drop('date', axis=1, inplace=True)","64287ebb":"# convert yr_renovated into categorical variable.\ndf['renovated'] = df['yr_renovated'].apply(lambda x: 0 if x == 0 else 1)\ndf.drop('yr_renovated', axis=1, inplace=True)","fa0a994f":"# Performing log transformation of numrical variable to get normal distribation.\ndf['price_log'] = np.log(df['price'])\ndf['sqft_living_log'] = np.log(df['sqft_living'])\ndf['sqft_lot_log'] = np.log(df['sqft_lot'])\ndf['sqft_above_log'] = np.log(df['sqft_above'])\ndf['sqft_living15_log'] = np.log(df['sqft_living15'])\ndf['sqft_lot15_log'] = np.log(df['sqft_lot15'])","a4acfb6f":"plt.figure(figsize=(15,5))\nsns.distplot(df['price_log'], bins=20, kde=False)","ab9f04e3":"df.to_pickle('clean_dataset')","ae9085f7":"df = pd.read_pickle('clean_dataset')","17478eb6":"# All features\nfeature1 = ['bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot', 'floors','waterfront', 'view', 'condition',\n            'grade', 'sqft_above', 'sqft_basement', 'zipcode', 'lat', 'long', 'sqft_living15','sqft_lot15',\n            'age_of_house', 'renovated']\n\n# features that correlation is greater than \"0.2\"\nfeature2 = ['bedrooms', 'bathrooms', 'sqft_living', 'floors', 'view',\n            'grade', 'sqft_above', 'sqft_basement', 'lat', 'sqft_living15']\n\n# numerical features with log_transform\nfeature3 = ['bedrooms', 'bathrooms', 'sqft_living_log', 'sqft_lot_log', 'floors','waterfront', 'view', 'condition',\n            'grade', 'sqft_above', 'sqft_basement', 'zipcode', 'lat', 'long', 'sqft_living15_log','sqft_lot15_log',\n            'age_of_house', 'renovated']\n\n# numerical features with log_transform where correlation is greater that \"0.2\"\nfeature4 = ['bedrooms', 'bathrooms', 'sqft_living_log', 'floors', 'view',\n            'grade', 'sqft_above', 'sqft_basement', 'lat', 'sqft_living15_log']","12b1dc33":"def correlation_of_each_feature(dataset, features):\n    # get correlations of each features in dataset\n    features.append('price_log')\n    corrmat = dataset[features].corr()\n    top_corr_features = corrmat.index\n    plt.figure(figsize=(20,20))\n    #plot heat map\n    sns.heatmap(dataset[top_corr_features].corr(),annot=True,cmap=\"RdYlGn\")\n\ncorrelation_of_each_feature(df, feature1.copy())","2d0e9252":"# Evaluation Matrix \nevaluation_df = pd.DataFrame(columns=['Name of Model','Feature Set', 'Target', 'R^2 of Training', 'R^2 of Testing', 'Mean Squaued Error Training',\n                                      'Mean Squaued Error Testing'])","dc8db35b":"# function to split data into training and testing set\ndef feature_target(features, target):\n    X = df[features]\n    y = df[target]\n    feature_train, feature_test, label_train, label_test = train_test_split(X, y, test_size=0.33, random_state=42)\n    return feature_train, feature_test, label_train, label_test","0c617f73":"def model_xyz(model, feature_train, feature_test, label_train, label_test, model_name='Linear Regression', feature_set=1,\n              target='price'):\n    model.fit(feature_train, label_train)  \n    y_pred_train = model.predict(feature_train)\n    y_pred_test = model.predict(feature_test)\n    r2_train = r2_score(label_train, y_pred_train)\n    r2_test = r2_score(label_test, y_pred_test)\n    rmse_train = mean_squared_error(label_train, y_pred_train)\n    rmse_test = mean_squared_error(label_test, y_pred_test)\n    \n    r = evaluation_df.shape[0]\n    evaluation_df.loc[r] = [model_name, feature_set, target, r2_train, r2_test, rmse_train, rmse_test]","44d64a32":"feature_train, feature_test, label_train, label_test = feature_target(feature1, 'price')\nlr = LinearRegression()\nmodel_xyz(lr, feature_train, feature_test, label_train, label_test,model_name= 'Simple linear Regression', feature_set= 1,\n          target='price')","ace9fa5d":"feature_train, feature_test, label_train, label_test = feature_target(feature2, 'price')\nlr = LinearRegression()\nmodel_xyz(lr, feature_train, feature_test, label_train, label_test,model_name= 'Simple linear Regression', feature_set= 2,\n          target='price')","d995772e":"feature_train, feature_test, label_train, label_test = feature_target(feature3, 'price_log')\nlr = LinearRegression()\nmodel_xyz(lr, feature_train, feature_test, label_train, label_test,model_name= 'Simple linear Regression', feature_set= 3,\n          target='price_log')","60534229":"feature_train, feature_test, label_train, label_test = feature_target(feature4, 'price_log')\nlr = LinearRegression()\nmodel_xyz(lr, feature_train, feature_test, label_train, label_test,model_name= 'Simple linear Regression', feature_set= 4,\n          target='price_log')","e8e94670":"evaluation_df","b3526d8c":"feature_train, feature_test, label_train, label_test = feature_target(feature1, 'price')\npolyfeat = PolynomialFeatures(degree = 2)\nfeature_train = polyfeat.fit_transform(feature_train)\nfeature_test = polyfeat.fit_transform(feature_test)\nlr = LinearRegression()\nmodel_xyz(lr, feature_train, feature_test, label_train, label_test,model_name= 'Linear Regression with degree 2',\n          feature_set= 1, target='price')","0c31b041":"feature_train, feature_test, label_train, label_test = feature_target(feature2, 'price')\npolyfeat = PolynomialFeatures(degree = 2)\nfeature_train = polyfeat.fit_transform(feature_train)\nfeature_test = polyfeat.fit_transform(feature_test)\nlr = LinearRegression()\nmodel_xyz(lr, feature_train, feature_test, label_train, label_test,model_name= 'Linear Regression with degree 2', \n          feature_set= 2, target='price')","7879649b":"feature_train, feature_test, label_train, label_test = feature_target(feature3, 'price_log')\npolyfeat = PolynomialFeatures(degree = 2)\nfeature_train = polyfeat.fit_transform(feature_train)\nfeature_test = polyfeat.fit_transform(feature_test)\nlr = LinearRegression()\nmodel_xyz(lr, feature_train, feature_test, label_train, label_test,model_name= 'Linear Regression with degree 2', \n          feature_set= 3, target='price_log')","3409cf41":"feature_train, feature_test, label_train, label_test = feature_target(feature4, 'price_log')\npolyfeat = PolynomialFeatures(degree = 2)\nfeature_train = polyfeat.fit_transform(feature_train)\nfeature_test = polyfeat.fit_transform(feature_test)\nlr = LinearRegression()\nmodel_xyz(lr, feature_train, feature_test, label_train, label_test,model_name= 'Linear Regression with degree 2', \n          feature_set= 4, target='price_log')","d5424cc2":"evaluation_df","dca69780":"feature_train, feature_test, label_train, label_test = feature_target(feature3, 'price_log')\npolyfeat = PolynomialFeatures(degree = 3)\nfeature_train = polyfeat.fit_transform(feature_train)\nfeature_test = polyfeat.fit_transform(feature_test)\nlr = LinearRegression()\nmodel_xyz(lr, feature_train, feature_test, label_train, label_test,model_name= 'Linear Regression with degree 3',\n          feature_set= 3, target='price_log')","cbc560b5":"feature_train, feature_test, label_train, label_test = feature_target(feature4, 'price_log')\npolyfeat = PolynomialFeatures(degree = 3)\nfeature_train = polyfeat.fit_transform(feature_train)\nfeature_test = polyfeat.fit_transform(feature_test)\nlr = LinearRegression()\nmodel_xyz(lr, feature_train, feature_test, label_train, label_test,model_name= 'Linear Regression with degree 3',\n          feature_set= 4, target='price_log')","8dbedb9c":"evaluation_df.iloc[6:,]","2b1fd0af":"feature_train, feature_test, label_train, label_test = feature_target(feature3, 'price_log')\npolyfeat = PolynomialFeatures(degree = 3)\nfeature_train = polyfeat.fit_transform(feature_train)\nfeature_test = polyfeat.fit_transform(feature_test)\nlr = Lasso(alpha=10)\nmodel_xyz(lr, feature_train, feature_test, label_train, label_test,model_name= 'Lasso Regression with degree 3',\n          feature_set= 3, target='price_log')","711850cf":"evaluation_df.iloc[8:,]","4ca844aa":"feature_train, feature_test, label_train, label_test = feature_target(feature3, 'price_log')\npolyfeat = PolynomialFeatures(degree = 3)\nfeature_train = polyfeat.fit_transform(feature_train)\nfeature_test = polyfeat.fit_transform(feature_test)\nlr = Lasso()\nsearch_grid={'alpha':[0.001,0.01,0.05,1,10,20]}\nsearch=GridSearchCV(estimator=lr, param_grid=search_grid, \n                    scoring='neg_mean_squared_error', n_jobs=1, cv=5)","95fc9afb":"search.fit(feature_train, label_train)","da8da9bc":"search.best_params_","c22ebb0d":"feature_train, feature_test, label_train, label_test = feature_target(feature3, 'price_log')\npolyfeat = PolynomialFeatures(degree = 3)\nfeature_train = polyfeat.fit_transform(feature_train)\nfeature_test = polyfeat.fit_transform(feature_test)\nlr = Lasso(alpha=20)\nmodel_xyz(lr, feature_train, feature_test, label_train, label_test,model_name= 'Lasso Regression with degree 3 with alpha=20',\n          feature_set= 3, target='price_log')","49724580":"evaluation_df","e92a96cc":"feature_train, feature_test, label_train, label_test = feature_target(feature3, 'price_log')\nlr =  DecisionTreeRegressor()\nsearch_grid={'max_depth':[6,7,8,9,10,11,12,13,14,15]}\nsearch=GridSearchCV(estimator=lr, param_grid=search_grid, \n                    scoring='neg_mean_squared_error', n_jobs=1, cv=3)","645089f9":"search.fit(feature_train, label_train)","d4f38506":"search.best_params_","4ede4d88":"feature_train, feature_test, label_train, label_test = feature_target(feature3, 'price_log')\nlr =  DecisionTreeRegressor(max_depth=9)\nmodel_xyz(lr, feature_train, feature_test, label_train, label_test,model_name= 'Decision Tree Regressor with alpha 9',\n          feature_set= 3, target='price_log')","1ef6feac":"evaluation_df","0dff78f6":"feature_train, feature_test, label_train, label_test = feature_target(feature3, 'price_log')\nada = AdaBoostRegressor(DecisionTreeRegressor(max_depth=9))\n\nsearch_grid={'n_estimators':[200,300,400,500],'learning_rate':[0.05, 0.1, 0.3, 1]}\n\nsearch=GridSearchCV(estimator=ada, param_grid=search_grid, \n                    scoring='neg_mean_squared_error', n_jobs=1, cv=3)","64bffdd8":"search.fit(feature_train, label_train)","9c33ff6e":"search.best_params_","c912f8dd":"feature_train, feature_test, label_train, label_test = feature_target(feature3, 'price_log')\nlr = AdaBoostRegressor(DecisionTreeRegressor(max_depth=9), learning_rate=1, n_estimators=500)\nmodel_xyz(lr, feature_train, feature_test, label_train, label_test,model_name= 'Decision Tree Regressor with alpha 9',\n          feature_set= 3, target='price_log')","43f032c0":"evaluation_df","63124ad8":"np.exp(0.032)","af367af5":"X = df[feature3]\ny = df['price_log']\nlr = AdaBoostRegressor(DecisionTreeRegressor(max_depth=9), learning_rate=1, n_estimators=500)\nscores = cross_val_score(lr, X, y, scoring='neg_mean_squared_error', cv=5)","a2caf797":"scores.mean()","5a6154da":"feature_train, feature_test, label_train, label_test = feature_target(feature3, 'price_log')\nlr = AdaBoostRegressor(DecisionTreeRegressor(max_depth=9), learning_rate=1, n_estimators=500)\nmodel = lr.fit(feature_train, label_train)\n","0da2ae2d":"pred = model.predict(feature_test)","bcf8aac9":"validation_df  = pd.DataFrame()","544f5fc5":"validation_df['actual'] = np.exp(label_test)","4ebdff59":"validation_df['predetion'] = np.exp(pred)","f2d1be42":"validation_df","c8ca8088":"## Validatation","248e766f":"Lets explore distribution of Target variable","863efb0d":"Now I try polynomial features.","661eb963":"The distribtion is Unimodel right skewed and centered at around 500000.0 and seem there is outlier in right side of distribtion. So need some kind of transformation to make it normal.","55e3efef":"When we look at the evaluation table, **2nd degree polynomial (all features, with price_log as target variable)** is doing good job forpredicting outcome. But **Adaboost with DecisionTree** is doing best.\nFuther improvements can also be made by using feature engineering.\nIf you like my notebook please do not forget to **Upvote**.","74d7c196":"So it may be overfitting so we need to apply **Regularization**.","b907cb02":"There is very good correlation of **sqft_living**, **sqft_above**, **sqft_basement**,**sqft_living15**, **lat** these feature and **price**. There are some data point that stand way out from rest of data. I need to further look at these point to declare as outlier.\nI can represent as **yr_renovated** as category variable to reduce complexity.","71a3f149":"I seen that model is not overfitting so no need of Regulariztion. Upto now transformed feature are perform well then other.","fcff0a76":"That Mean squared error of polynomial degree 3 **(0.056)** is greater then Mean squared error of polynomial degree 2 **(0.45)**. that not good way to go further. ","636b3f67":"From now onward I only go with transformed features because its give higher r^2 then other features. ","cc08396a":"The data is pretty clean there is no null value in data-set","d33e2257":"#### **Woo thats great!!!!!!** ","c0c17b10":"# Pridicting House Price ","6fb41503":"## EDA","9de92f8b":"With polynomial degree *3* Training R^2 is **0.88** higher then testing R^2 **0.79**.Infact it is less then ploynomial degree *2* testing r^2 **0.81**.","79c318d4":"## Importing tool and libraries","899cadf2":"### Decision Tree with AdaBoost","7670716c":"## Feature Engineering","46230b42":"**Boom** MSE get down to **0.032** from **0.047**.","1dbe5c16":"I found that **bathrooms**, **waterfront**, and **grade** are very good correlation with price. I also found that two of bedrooms *11* and *33* are very unusual that we might removed these point.I can see that bathrooms is floating number I can make these integer to reduce complexity.There are some outlier that furture away I also need to further look at these data point to confirm these are acutual outlier.","8968645b":"## Preprocessing and Removing Outlier","88ae0cfb":"Cool MSE is **0.033**  so my final model is Adaboost with decisionTreeRegressor.","d0ec4328":"### Numerical Feature","9ab70c01":"Its **MSE(0.051)** is little higher then MSE of linear regressinon with polynomial degree **2(0.47)**","f2d55a14":"let see how data is look.","f66655cf":"Similarly all other feature get normalize.","4c8234ec":"## MAchine Learning","5f1960ad":"Now with gridsearchcv I try to find best value for alpha.","0adb6d9d":"Linear regression with polynomial degree 2 with feature set **3** is given great result training **r^2 = 0.83** and testing **r^2 = 0.82**. That good but i try to improve it. Lets go with further degree of polynomial degree. ","a6d1f306":"### Categorial Variable","2291d180":"Now we validate our model by cross-validation.","4229748f":"## Overview","c24bc45b":"Importing libraries and data","5c72e1a5":"### Price","82ffe7de":"Now i apply Adaboost let see what happen!...","83fc24e9":"## First Look of data","9cb6bb44":"### Outlier","2460256b":" Welcome to my Kernel! In this kernel, I use various regression methods and try to predict the house prices by using them. As you can guess, there are various methods to suceed this and each method has pros and cons. I think **regression is one of the most important methods when used with Adaboost** because it gives us more insight about the data. When we ask why, it is easier to interpret the relation between the response and explanatory variables.","29ff1b6b":"Now we explore relation between target variable(price) and categorial variable.","2ebfe617":"By exploring further i find some potential outlier.","10b81c63":"## Feature selection","8660a247":"## Conclusion"}}