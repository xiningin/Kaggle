{"cell_type":{"cfe74950":"code","75c87d0b":"code","e93b4ce6":"code","81fabd9b":"code","56f803fa":"code","82dcc398":"code","13f6cf31":"code","2080834e":"code","489b6ea3":"code","f200c528":"code","305f8c3f":"markdown"},"source":{"cfe74950":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfrom torch  import nn\nimport torch\nfrom torchvision.models import *\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","75c87d0b":"a=torch.randn(2,64,56,56)\nb=torch.randn(2,128,56,56)\n ","e93b4ce6":"import torch\nfrom torch import nn\nfrom torch.nn import functional as F\n#from d2l import torch as d2l\n#THis is residual block means a block into whose output we add the input that was given to this block\nclass Residual(nn.Module):  #@save\n    \"\"\"The Residual block of ResNet.\"\"\"\n    def __init__(self, input_channels, num_channels, use_1x1conv=False,\n                 strides=1):\n        super().__init__()\n        self.conv1 = nn.Conv2d(input_channels, num_channels, kernel_size=3,\n                               padding=1, stride=strides)\n        self.conv2 = nn.Conv2d(num_channels, num_channels, kernel_size=3,\n                               padding=1)\n        if use_1x1conv:\n            self.conv3 = nn.Conv2d(input_channels, num_channels,\n                                   kernel_size=1, stride=strides)\n        else:\n            self.conv3 = None\n        self.bn1 = nn.BatchNorm2d(num_channels)\n        self.bn2 = nn.BatchNorm2d(num_channels)\n\n    def forward(self, X):\n        print('input',X.size())\n        Y = F.relu(self.bn1(self.conv1(X)))\n        print('input mid',X.size())\n        Y = self.bn2(self.conv2(Y))\n        print('Y',Y.size())\n        if self.conv3:# incase block is not identity block then this 1x1 is used to downsample Input feature map and match to the output channel of final layer of block\n            X = self.conv3(X)\n            print('Downsample and Increase channel',X.size())\n        Y += X\n        return F.relu(Y)","81fabd9b":"def resnet_block(input_channels, num_channels, num_residuals,\n                 first_block=False):\n    blk = []\n    for i in range(num_residuals):\n        if i == 0 and not first_block:\n            blk.append(\n                Residual(input_channels, num_channels, use_1x1conv=True,\n                         strides=2))\n        else:\n            blk.append(Residual(num_channels, num_channels))\n    return blk\nb1 = nn.Sequential(nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3),\n                   nn.BatchNorm2d(64), nn.ReLU(),\n                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\nb2 = nn.Sequential(*resnet_block(64, 64, 2, first_block=True))\nb3 = nn.Sequential(*resnet_block(64, 128, 2))\nb4 = nn.Sequential(*resnet_block(128, 256, 2))\nb5 = nn.Sequential(*resnet_block(256, 512, 2))","56f803fa":"b1#input block recipant of original input","82dcc398":"b2 # stacked residual blocks these are identity blocks as input size =output size","13f6cf31":"#size of conv kernel is  o= (H+2p-(d*k)-1)\/s +1\n","2080834e":"net = nn.Sequential(b1, b2, b3, b4, b5, nn.AdaptiveAvgPool2d((1, 1)),\n                    nn.Flatten(), nn.Linear(512, 10))","489b6ea3":"b3 #this conv block as input !=output so we need here downsampling of feature map and more no of Ouput channel","f200c528":"i=torch.randn(3,1,128,128)\nprint('first layer')\nx1=b1(i)\nprint('identity residual block')\nx2=b2(x1)\nprint('conv residual block needing additional 1x1 block')\n\nx3=b3(x2)\n#x1.size()","305f8c3f":"**Fundamental of Residual**\nResidual can be return as diff between  Activation of block and given input to it  R(x)=F(x)-x \nso F(x) becomes=R(x)+x \nObjective of Neural network is Y=F(x) ,that is output is some function of input, now that F(x) is expressed as  R(x)+x ,network will now try to minimize the R(x)  so that we meet our objective,so it learns Residual that is why we term it as Residual block. A block is called Residual if its input is add its own output .  \nPrincipal of residual came in place to resolve the problem of Vanishing gradients in a very Deep NN. It has also been observed that it is easier to learn residual of output and input, rather than only the input. And if you truly understand backpropagation and how severe the problem of vanishing gradient becomes with increasing the number of layers, then you can clearly see that because of these skip connections, we can propagate larger gradients to initial layers, "}}