{"cell_type":{"b3c48bb5":"code","9360621b":"code","9d717277":"code","077eef67":"code","dfce3a86":"code","dd0acb51":"code","936a9c6e":"code","57f0ee28":"code","477b8790":"code","93e84ad0":"code","828ca981":"code","e5384fee":"code","ce9b4e05":"code","1b479716":"code","ffddfa30":"code","464b7732":"code","5d10735d":"code","aa9d62cf":"code","8bba0313":"code","3bfefd6a":"code","0c37b09e":"code","3fe62ca0":"code","adef37fe":"code","7240563d":"code","d614ce10":"code","7b884f23":"code","bb2a3ae2":"code","da55da86":"code","348641d5":"code","b852c7da":"code","a3dd9afd":"code","458b35c9":"code","dd4f5214":"code","eb60658e":"code","e726cf20":"code","05861957":"code","e4f769cb":"code","704c325b":"code","9dc54dbd":"code","a76fb56e":"code","5ba3885c":"code","3f7abcd2":"code","9c7c44fe":"code","bf4d81b6":"code","deef2065":"code","c1030262":"markdown","a8390100":"markdown","62923e74":"markdown","b0b0edaf":"markdown","15f52ae1":"markdown","65943305":"markdown","28f45ff3":"markdown","a578c5a4":"markdown","724604b5":"markdown","2c5e01b9":"markdown","ea5a7a06":"markdown","dfb56702":"markdown","998b4c1d":"markdown","06a83682":"markdown","0098ae81":"markdown","fb75878f":"markdown","abab8797":"markdown","f2a50d20":"markdown"},"source":{"b3c48bb5":"import numpy as np\nimport os\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\nimport optuna\nfrom sklearn import linear_model\nfrom sklearn import metrics\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import cross_val_score\nfrom optuna.samplers import RandomSampler, GridSampler, TPESampler\nimport sklearn\nimport xgboost as xgb\nfrom scipy.misc import derivative\nfrom sklearn.metrics import mean_squared_error\nimport pickle\nimport category_encoders as ce \nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.compose import ColumnTransformer\nimport random\nimport lightgbm as lgb\nimport random","9360621b":"def seed_everything(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)","9d717277":"seed_everything(0)","077eef67":"df_train =  pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/train.csv\")\ndf_test = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/test.csv\")\ny_train = np.log(df_train['SalePrice'])\nX_train = df_train.drop(['SalePrice','Id'],axis=1)\nX_test = df_test.drop('Id',axis=1)\nprint('Train data: ',X_train.shape)\nprint('Test data: ',X_test.shape)","dfce3a86":"cat_nominal_features = pickle.load(open('..\/input\/features-housing\/cat_nominal_features.p', \"rb\" ))\ncat_ordinal_features = pickle.load(open('..\/input\/features-housing\/cat_ordinal_features.p', \"rb\" ))\nnum_features = pickle.load(open('..\/input\/features-housing\/num_features.p', \"rb\" ))\ncat_features = cat_nominal_features + cat_ordinal_features\nprint('Number of numeric features: ',len(num_features))\nprint('Number of ordinal features: ',len(cat_ordinal_features))\nprint('Number of nominal featuures: ',len(cat_nominal_features))","dd0acb51":"X_train[cat_ordinal_features] = X_train[cat_ordinal_features].fillna(np.nan)\nX_train[cat_nominal_features] = X_train[cat_nominal_features].fillna(np.nan)\nX_test[cat_ordinal_features] = X_test[cat_ordinal_features].fillna(np.nan)\nX_test[cat_nominal_features] = X_test[cat_nominal_features].fillna(np.nan)","936a9c6e":"ord_mapping=[{'col': 'Street', 'mapping': {'Grvl': 1, 'Pave': 2}},\n        {'col': 'Alley', 'mapping': {np.nan:0,'Grvl': 1, 'Pave': 2}}, \n        {'col': 'Utilities', 'mapping': {'NoSeWa': 1, 'AllPub':2}},\n        {'col': 'ExterQual', 'mapping': {'Po':1,'Fa':2,'TA':3,\n                                        'Gd':4,'Ex':5}},\n        {'col': 'ExterCond', 'mapping': {'Po':1,'Fa':2,'TA':3,\n                                        'Gd':4,'Ex':5}},\n        {'col': 'BsmtCond', 'mapping': {np.nan:0,'Po':1,'Fa':2,'TA':3,\n                                        'Gd':4,'Ex':5}},\n        {'col': 'BsmtQual', 'mapping': {np.nan:0,'Po':1,'Fa':2,'TA':3,\n                                        'Gd':4,'Ex':5}},\n        {'col': 'BsmtExposure', 'mapping': {np.nan:0,'No':1,'Mn':2,'Av':3,\n                                        'Gd':4}},\n        {'col': 'BsmtFinType1', 'mapping': {np.nan:0,'Unf':1,'LwQ':2,'Rec':3,\n                                        'BLQ':4,'ALQ':5,'GLQ':6}},\n        {'col': 'BsmtFinType2', 'mapping': {np.nan:0,'Unf':1,'LwQ':2,'Rec':3,\n                                        'BLQ':4,'ALQ':5,'GLQ':6}},     \n        {'col': 'HeatingQC', 'mapping': {'Po':1,'Fa':2,'TA':3,\n                                        'Gd':4,'Ex':5}},\n        {'col': 'CentralAir', 'mapping': {'Y':1,'N':0}},\n        {'col': 'KitchenQual', 'mapping': {'Po':1,'Fa':2,'TA':3,\n                                        'Gd':4,'Ex':5}}, \n        {'col': 'Functional', 'mapping': {'Typ':8,'Min1':7,'Min2':6,\n                                        'Mod':5,'Maj1':4,'Maj2':3,\n                                         'Sev':2,\"Sal\":1}},                                    \n        {'col': 'FireplaceQu', 'mapping': {np.nan:0,'Po':1,'Fa':2,'TA':3,\n                                        'Gd':4,'Ex':5}},\n        {'col': 'GarageFinish', 'mapping': {np.nan:0,'Unf':1,'RFn':2,'Fin':3}},\n        {'col': 'GarageQual', 'mapping': {np.nan:0,'Po':1,'Fa':2,'TA':3,\n                                        'Gd':4,'Ex':5}},\n        {'col': 'GarageCond', 'mapping': {np.nan:0,'Po':1,'Fa':2,'TA':3,\n                                        'Gd':4,'Ex':5}},\n        {'col': 'PavedDrive', 'mapping': {'N':1,'P':2,'Y':3}},\n        {'col': 'PoolQC', 'mapping': {np.nan:0,'Fa':1,'TA':2,\n                                        'Gd':3,'Ex':4}},\n        {'col': 'Fence', 'mapping': {np.nan:0,'MnWw':1,'GdWo':2,'MnPrv':3,\n                                        'GdPrv':4}}]\nmapping_cols = ['Street','Alley','Utilities','ExterQual','ExterCond','BsmtCond','BsmtQual','BsmtExposure',\n               'BsmtFinType1','BsmtFinType2','HeatingQC','CentralAir','KitchenQual',\n               'Functional','FireplaceQu','GarageFinish','GarageQual','GarageCond','PavedDrive',\n               'PoolQC','Fence']\n\ndef construct_ord_nom(ordinal_features,nominal_features):\n    mapp = []\n    ord_features = []\n    for c in ordinal_features:\n        if c not in mapping_cols:\n            continue\n        idx = mapping_cols.index(c)\n        mapp.append(ord_mapping[idx])\n        ord_features.append(c)\n    ce_ord = ce.OrdinalEncoder(cols=ord_features,mapping=mapp,\n                               handle_unknown='return_nan',handle_missing='return_nan')\n    ce_nom = ce.OneHotEncoder(cols=nominal_features,handle_unknown='return_nan',handle_missing='return_nan')\n    return ce_ord,ce_nom \n\ndef get_CT(ord_features,nom_features,num_features_new,ce_ord,ce_nom):\n    numeric_transformer = Pipeline(steps=[\n            ('imputer', SimpleImputer(strategy='constant',fill_value=-1)),\n            ])\n    ce_ord, ce_nom = construct_ord_nom(ord_features,nom_features)\n    ct1 = ColumnTransformer(\n            transformers=[\n                ('nominal',ce_nom,nom_features),\n                ('ordinal',ce_ord,ord_features),\n                ('num',numeric_transformer,num_features_new)\n                ],remainder = 'passthrough')\n    return ct1","57f0ee28":"ce_ord, ce_nom = construct_ord_nom(cat_ordinal_features,cat_nominal_features)\nCT = get_CT(cat_ordinal_features,cat_nominal_features,num_features,ce_ord,ce_nom)","477b8790":"X_train_new = CT.fit_transform(X_train)\nX_test_new = CT.transform(X_test)\nprint(X_train_new.shape)\nprint(X_test_new.shape)\ndtrain = lgb.Dataset(X_train_new,y_train)","93e84ad0":"class cparams():\n    def __init__(self): \n        self.seed = 0\n        self.num_iterations = 100 # Default = 100\n        self.learning_rate = 0.1 # Default = 0.1\n        self.num_leaves = 31 #Default = 31\n        self.min_child_samples = 20 #Default = 20\n        self.min_child_weight = 0.001 #Default = 0.001\n        self.bagging_fraction = 1.0\n        self.feature_fraction = 1.0\n        self.bagging_freq = 0\n        self.alpha = 0.25\n        self.gamma = 2.0\n        self.l1 = 0.0\n        self.l2 = 0.0\n        \n    def calibrate(self,num_round):\n        \n        param = {'boosting_type': 'gbdt', \n                'objective': 'regression',\n                'metric': 'rmse', \n                'learning_rate': self.learning_rate, \n                'num_leaves': self.num_leaves,     \n                'min_data_in_leaf': self.min_child_samples,   \n                'min_sum_hessian_in_leaf':self.min_child_weight, \n                'bagging_fraction': self.bagging_fraction, \n                'bagging_freq': self.bagging_freq,\n                'feature_fraction': self.feature_fraction, \n                'lambda_l1': self.l1,\n                'lambda_l2': self.l2,\n                'seed': self.seed\n        }\n        \n        # cv's seed used to generate folds passed to numpy.random.seed\n        bst = lgb.cv(param, dtrain,num_boost_round=num_round, stratified=False, \\\n                     shuffle=True,early_stopping_rounds=100,verbose_eval=10,seed=0)\n        return bst\n    \n    def get_param(self):\n        \n        param = {'boosting_type': 'gbdt', \n                 'objective': 'regression',\n                'metric': 'rmse', \n                'learning_rate': self.learning_rate, \n                'num_leaves': self.num_leaves,     \n                'min_data_in_leaf': self.min_child_samples,   \n                'min_sum_hessian_in_leaf':self.min_child_weight, \n                'bagging_fraction': self.bagging_fraction, \n                'bagging_freq': self.bagging_freq,\n                'feature_fraction': self.feature_fraction, \n                'lambda_l1': self.l1,\n                'lambda_l2': self.l2,\n                'seed': self.seed\n        }        \n        \n        return param","828ca981":"current_model = cparams()","e5384fee":"# Use default parameters\n# num_iterations(num_boost_round) = 100\n# max_depth = -1\n# num_leaves = 31\n# min_data_in_leaf(min_child_samples) = 20\n# min_sum_hessian_in_leaf(min_child_weight) = 0.001\n# feature_fraction(colsample_bytree) = 1.0\n# bagging_fraction(subsample) = 1.0\n# bagging_freq(subsample_freq) = 0\n# learning_rate = 0.1\n\nparams = {\n    'objective': 'regression',\n    'metric': 'rmse', \n    \"verbosity\": 1,\n    \"boosting_type\": \"gbdt\",\n    'seed':0\n}\n\neval_history = lgb.cv(\n    params, dtrain, verbose_eval=20,\n    stratified=False, num_boost_round=1000, early_stopping_rounds=100,\n    nfold=5,seed=0)","ce9b4e05":"print('Best score: ', eval_history['rmse-mean'][-1])\nprint('Number of estimators: ', len(eval_history['rmse-mean']))\ncurrent_model.num_iterations = len(eval_history['rmse-mean'])","1b479716":"study_name2 = 'lgb_leaves'\nstudy_leaves = optuna.create_study(study_name=study_name2,direction='maximize',sampler=TPESampler(0))","ffddfa30":"def opt_leaves(trial):\n    \n    params = {\n        'objective': 'regression',\n        'metric': 'rmse', \n        \"verbosity\": 1,\n        \"boosting_type\": \"gbdt\",\n        'seed':0,\n        'num_leaves':int(trial.suggest_loguniform(\"num_leaves\", 3,32))\n    }\n    \n    score = lgb.cv(\n        params, dtrain, verbose_eval=0, \n        stratified=False, num_boost_round=current_model.num_iterations,\n        nfold=5,seed=0)\n    return -score['rmse-mean'][-1]","464b7732":"study_leaves.optimize(opt_leaves, n_trials=50)","5d10735d":"print('Total number of trials: ',len(study_leaves.trials))\ntrial_leaves = study_leaves.best_trial\nprint('Best score : {}'.format(-trial_leaves.value))\nfor key, value in trial_leaves.params.items():\n    print(\"    {}: {}\".format(key, value))","aa9d62cf":"current_model.num_leaves = int(trial_leaves.params['num_leaves'])","8bba0313":"study_name3 = 'lgb_child_weight_sample'\nstudy_sample_weight = optuna.create_study(study_name=study_name3,direction='maximize',sampler=TPESampler(0))","3bfefd6a":"def opt_sample_weight(trial):\n    \n    params = {\n        'objective': 'regression',\n        'metric': 'rmse', \n        \"verbosity\": 1,\n        \"boosting_type\": \"gbdt\",\n        'seed':0,\n        'num_leaves':current_model.num_leaves,\n        'min_data_in_leaf':int(trial.suggest_discrete_uniform('data_in_leaf',4,32,q=2)),\n        'min_sum_hessian_in_leaf':trial.suggest_discrete_uniform('min_hessian',0.001,0.003,q=0.0005)\n    }\n    \n    score = lgb.cv(\n        params, dtrain, verbose_eval=0, \n        stratified=False, num_boost_round=current_model.num_iterations,\n        nfold=5,seed=0)\n    return -score['rmse-mean'][-1]","0c37b09e":"study_sample_weight.optimize(opt_sample_weight, n_trials=50)","3fe62ca0":"print('Total number of trials: ',len(study_sample_weight.trials))\ntrial_sample_weight = study_sample_weight.best_trial\nprint('Best score : {}'.format(-trial_sample_weight.value))\nfor key, value in trial_sample_weight.params.items():\n    print(\"    {}: {}\".format(key, value))","adef37fe":"current_model.min_child_samples = int(trial_sample_weight.params['data_in_leaf'])\ncurrent_model.min_child_weight = trial_sample_weight.params['min_hessian']","7240563d":"study_name4 = 'lgb_bagging'\nstudy_bagging = optuna.create_study(study_name=study_name4,direction='maximize',sampler=TPESampler(0))","d614ce10":"def opt_bagging(trial):\n    \n    params = {\n        'objective': 'regression',\n        'metric': 'rmse',  \n        \"verbosity\": 1,\n        \"boosting_type\": \"gbdt\",\n        'seed':0,\n        'num_leaves':current_model.num_leaves,\n        'min_data_in_leaf':current_model.min_child_samples,\n        'min_sum_hessian_in_leaf':current_model.min_child_weight,\n        'bagging_fraction': trial.suggest_discrete_uniform('bfrac',0.4,1.0,q=0.05),\n        'bagging_freq': int(trial.suggest_discrete_uniform('bfreq',1,7,q=1.0)),\n        'feature_fraction':trial.suggest_discrete_uniform('feature',0.4,1.0,q=0.05)\n    }\n    \n    score = lgb.cv(\n        params, dtrain, verbose_eval=0, \n        stratified=False, num_boost_round=current_model.num_iterations,\n        nfold=5,seed=0)\n    return -score['rmse-mean'][-1]","7b884f23":"study_bagging.optimize(opt_bagging, n_trials=50)","bb2a3ae2":"print('Total number of trials: ',len(study_bagging.trials))\ntrial_bagging = study_bagging.best_trial\nprint('Best score : {}'.format(trial_bagging.value))\nfor key, value in trial_bagging.params.items():\n    print(\"    {}: {}\".format(key, value))","da55da86":"current_model.bagging_fraction = trial_bagging.params['bfrac']\ncurrent_model.bagging_freq = int(trial_bagging.params['bfreq'])\ncurrent_model.feature_fraction = trial_bagging.params['feature']","348641d5":"study_name5 = 'l1_l2'\nstudy_reg = optuna.create_study(study_name=study_name5,direction='maximize',sampler=TPESampler(0))","b852c7da":"def opt_reg(trial):\n    \n    params = {\n        'objective': 'regression',\n        'metric': 'rmse', \n        \"verbosity\": 1,\n        \"boosting_type\": \"gbdt\",\n        'seed':0,\n        'num_leaves':current_model.num_leaves,\n        'min_data_in_leaf':current_model.min_child_samples,\n        'min_sum_hessian_in_leaf':current_model.min_child_weight,\n        'bagging_fraction': current_model.bagging_fraction,\n        'bagging_freq': current_model.bagging_freq,\n        'feature_fraction':current_model.feature_fraction,\n        'lambda_l1': trial.suggest_loguniform(\"lambda_l1\", 1e-7, 10),\n        'lambda_l2': trial.suggest_loguniform(\"lambda_l2\", 1e-7, 10)\n    }\n    \n    score = lgb.cv(\n        params, dtrain, verbose_eval=0, \n        stratified=False, num_boost_round=current_model.num_iterations,\n        nfold=5,seed=0)\n    return -score['rmse-mean'][-1]","a3dd9afd":"study_reg.optimize(opt_reg, n_trials=50)","458b35c9":"print('Total number of trials: ',len(study_reg.trials))\ntrial_reg = study_reg.best_trial\nprint('Best score : {}'.format(trial_reg.value))\nfor key, value in trial_reg.params.items():\n    print(\"    {}: {}\".format(key, value))","dd4f5214":"current_model.l1 = trial_reg.params['lambda_l1']\ncurrent_model.l2 = trial_reg.params['lambda_l2']","eb60658e":"current_model.learning_rate = 0.05\nlr1 = current_model.calibrate(10000) \nprint('Best score: ', lr1['rmse-mean'][-1])\nprint('Number of estimators: ', len(lr1['rmse-mean']))","e726cf20":"current_model.learning_rate = 0.01\nlr2 = current_model.calibrate(10000) \nprint('Best score: ', lr2['rmse-mean'][-1])\nprint('Number of estimators: ', len(lr2['rmse-mean']))","05861957":"current_model.learning_rate = 0.005\nlr3 = current_model.calibrate(10000) \nprint('Best score: ', lr3['rmse-mean'][-1])\nprint('Number of estimators: ', len(lr3['rmse-mean']))","e4f769cb":"## Get Current Parameters\ncurrent_model.learning_rate = 0.005 #Based on what we found above\ncurrent_param = current_model.get_param()\nfor key, value in current_param.items():\n    print(\"    {}: {}\".format(key, value))","704c325b":"from sklearn.model_selection import KFold\ndef cv_training(train_data,y_train_data):\n    kFold = KFold(n_splits=5, random_state=0, shuffle=True)\n    models = []\n    eval_history = []\n    oof_pred = []\n    oof_target = []\n    scores = []\n    for fold, (trn_idx, val_idx) in enumerate(kFold.split(train_data)):\n        #print(trn_idx)\n        #print(val_idx)\n        X_train = train_data[trn_idx]\n        X_val = train_data[val_idx]\n        y_train = y_train_data[trn_idx]\n        y_val = y_train_data[val_idx]\n        dtrain =  lgb.Dataset(X_train,y_train)\n        dval =  lgb.Dataset(X_val,y_val)\n        evals_result = {}\n        model = lgb.train(current_param, dtrain,num_boost_round=5000,\n                          evals_result=evals_result,valid_sets=dval,verbose_eval=20,early_stopping_rounds=100)\n        models.append(model)\n        y_pred = model.predict(X_val)\n        score_temp = np.sqrt(mean_squared_error(y_pred,y_val))\n        scores.append(score_temp)\n        oof_pred.append(y_pred)\n        oof_target.append(y_val)\n        eval_history.append(evals_result)\n    oof_pred = np.concatenate((oof_pred[0],oof_pred[1],oof_pred[2],\n                               oof_pred[3],oof_pred[4]))\n    oof_target = np.concatenate((oof_target[0],oof_target[1],oof_target[2],\n                                 oof_target[3],oof_target[4]))\n    oof_df = pd.DataFrame({'predictions':oof_pred,'target':oof_target})\n    return models, eval_history, scores, oof_df","9dc54dbd":"models, eval_history,scores,oof_df = cv_training(X_train_new,y_train)","a76fb56e":"oof_df.to_csv('oof_df.csv',index=False)\noof_df.head()","5ba3885c":"score = 0.0\nfor i in range(len(models)):\n    print(models[i].best_iteration)\n    print(eval_history[i]['valid_0']['rmse'][-1])\n    score = score + eval_history[i]['valid_0']['rmse'][-1]\nprint('Average score: ',score\/5)","3f7abcd2":"# Number of boost_round will be based on what we found above\nmodel_final = lgb.train(current_param, dtrain,num_boost_round=3920)\nypred = model_final.predict(X_test_new)","9c7c44fe":"# Make a histo\nplt.hist(np.exp(ypred))","bf4d81b6":"sub = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv')\nsubmission  = pd.DataFrame({\n    'Id': sub['Id'],\n    'SalePrice': np.exp(ypred)\n})\nsubmission.head()","deef2065":"submission.to_csv('submission.csv',index=False)","c1030262":"# Tune leaves\nThe next step is tune num_leaves parameter which is important\nsince it controls the complexity of the model\n","a8390100":"## Make Predictions","62923e74":"Transform the dataset and get it ready for tuning","b0b0edaf":"For this baselien model, we are going to\nuse the default parameters. This would give us a sense \nhow well our model is performing.","15f52ae1":"## Feature fraction \/ Bagging fraction\/ Bagging frequency\n\nFeature fraction:\nLightGBM will randomly select part of features on each iteration (tree) if feature_fraction smaller than 1.0. For example, if you set it to 0.8, LightGBM will select 80% of features before training each tree\n\ncan be used to speed up training\n\ncan be used to deal with over-fitting\n\nBagging fraction: like feature_fraction, but this will randomly select part of data without resampling\n\nBagging frequency: frequency for bagging","65943305":"Seperate features into nominal, ordinal and categorical features","28f45ff3":"## Lower down learning rate\nOnce we found all the parameters, we fix them and start lowering \ndown learning rate. Typically, as learning rate decreases, number of trees would\ngo up, we would want to find optimal number of trees with optimal learning rate","a578c5a4":"Construct one-hot and ordinal encoder using category_encoder","724604b5":"## Baseline Model Checking","2c5e01b9":"Create a class to track all parameters and tuning process","ea5a7a06":"# L1 and L2 regularization\nlambda_l1: l1 regularization\n\nlambda_l2: l2 regularization\n\nHigher values would make the model more conservative","dfb56702":"# Finalize Model\nWe are going to get current parameters, do 5-fold cross validation \nto get our out-of-sample file which can be further used for stacking.\nWe are also going to train a model on the entire training set and make our predictions","998b4c1d":"# Introduction\nThis notebook will walk you through step by step how to tune lightGBM using Optuna. Missing values\nwill be directly handled by LightGBM\n\nIt will generate an out-of-sample file which can be used for further stacking.\n\nParameters will be tuned in sequence. Once a value for a parameter is found, it will be fixed for \nsubsequent tuning. \"cparams\" class will be used to store all parameters and track the tuning process.\n\nThe sequence of tuning is that:\n\nnum_leaves -> min_data_in_leaf\/min_sum_hessian_in_leaf -> bagging_fraction\/bagging_freq\/feature_fraction\/ \n-> l1 and l2 regularization -> lower down learning rate\n\nThe meaning of parameters will be explained later on. \n\nThe main purpose is to demonstrate the tuning process. You can further tune the model by increasing the number\nof trails.","06a83682":"Missing values will be automatically handled by LightGBM, sometimes\nthis may give you better results","0098ae81":"## Load Data","fb75878f":"Let's check average out of sample score","abab8797":"Best score we found is 0.121 with number of estimators to be 3920\nYou can experiment more learning rates if you want","f2a50d20":"# Tune min child sample weight and min data in leaf\nmin_data_in_leaf and min_sum_hessian_in_leaf again these two parameters control the \ncomplexity of the model. If you have too few data in a leaf or min_sum_hessian_in_leaf is too small, \nthis may lead to overfitting."}}