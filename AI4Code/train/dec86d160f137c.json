{"cell_type":{"ec690b9f":"code","867e6224":"code","15540213":"code","b060b772":"code","dd9113c6":"code","198d7cb5":"code","e8953ae1":"code","847af02e":"code","e7d9f59a":"code","bc7aca37":"code","ac34fcee":"code","229dd828":"code","883d7e44":"code","a6084d6a":"markdown","8e7d113e":"markdown","0ba5bf16":"markdown","a5fcaf4a":"markdown"},"source":{"ec690b9f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nimport seaborn as sns\n%matplotlib inline\n\nnp.random.seed(2)\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nimport itertools\n\nfrom keras.utils.np_utils import to_categorical # convert to one-hot-encoding\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D\nfrom keras.optimizers import RMSprop\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import ReduceLROnPlateau\nfrom keras.layers.normalization import BatchNormalization\n\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session\nsns.set(style='white', context='notebook', palette='deep')","867e6224":"#load data\ndig_train_data = pd.read_csv(\"\/kaggle\/input\/digit-recognizer\/train.csv\")\ndig_test_data = pd.read_csv(\"\/kaggle\/input\/digit-recognizer\/test.csv\")\n\n#separate target and features\nY_train= dig_train_data.label\nX_train= dig_train_data.drop(\"label\", axis=1)\n\ndel dig_train_data      ","15540213":"#normalizing the data\nX_train = X_train\/255.0\ndig_test_data= dig_test_data\/255.0\n\n#reshaping the data\nX_train = X_train.values.reshape(-1,28,28,1)\ndig_test_data = dig_test_data.values.reshape(-1,28,28,1)\n\n#one-hot\nY_train = to_categorical(Y_train,num_classes = 10)","b060b772":"# splitting training and validation data\nrandom_seed= 2\nX_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size= 0.1, random_state = random_seed)","dd9113c6":"#visualize the data, Change the value of n to visualize different values\nn= 16\ng= plt.imshow(X_train[n][:,:,0])","198d7cb5":"#model\n#784->[Conv2D->Conv2D->Conv2D(with kernel_size=5, stride=2)->dropout]x2 ->Flatten-> Dense(128)->Dropout ->10(out)\nmodel= Sequential()\nmodel.add(Conv2D(filters=32, kernel_size =3, activation='relu',input_shape = (28,28,1)))\nmodel.add(BatchNormalization())\nmodel.add(Conv2D(filters=32, kernel_size=3,activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Conv2D(filters = 32,kernel_size =5,strides=2, padding = 'Same',activation='relu' ))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.4))\n \nmodel.add(Conv2D(filters=64, kernel_size =3, activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Conv2D(filters=64, kernel_size=3,activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Conv2D(filters = 64,kernel_size =5,strides=2, padding = 'Same',activation='relu' ))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.4))\n \nmodel.add(Flatten())\nmodel.add(Dense(256, activation=\"relu\"))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(10,activation=\"softmax\"))","e8953ae1":"#compiling the model\nmodel.compile(optimizer = 'adam', loss=\"categorical_crossentropy\", metrics= [\"accuracy\"])","847af02e":"#annealing\n#the learning rate gets halved if accuracy is not improved after 3 epochs\nlearning_rate_reduction = ReduceLROnPlateau(moitor='val_acc',\n                                            patience=3 ,\n                                            verbose=1,\n                                           factor =0.5,\n                                           min_lr=0.00001)","e7d9f59a":"epochs= 40\nbatch_size = 128","bc7aca37":"#Data agumentation. This changes the orientation of the images by either resizing or rotating thus creating more data\n#to train the model on.\ndatagen = ImageDataGenerator(\n        featurewise_center=False,  \n        samplewise_center=False,  \n        featurewise_std_normalization=False,  \n        samplewise_std_normalization=False,  \n        zca_whitening=False,  \n        rotation_range=10,  \n        zoom_range = 0.1,\n        width_shift_range=0.1, \n        height_shift_range=0.1,  \n        horizontal_flip=False, \n        vertical_flip=False) \n\n\ndatagen.fit(X_train)","ac34fcee":"#fitting after agumentation\nhistory = model.fit_generator(datagen.flow(X_train,Y_train, batch_size=batch_size),\n                              epochs = epochs, validation_data = (X_val,Y_val),\n                              verbose = 1, steps_per_epoch=X_train.shape[0] \/\/ batch_size\n                              , callbacks=[learning_rate_reduction])","229dd828":"#predicting results\nresults = model.predict(dig_test_data)\nresults = np.argmax(results, axis=1)\nresults = pd.Series(results, name=\"label\")","883d7e44":"submission = pd.concat([pd.Series(range(1,28001),name = \"ImageId\"),results],axis = 1)\nsubmission.to_csv(\"cnn_mnist_datagen.csv\",index=False)","a6084d6a":"Also read:\n* https:\/\/www.kaggle.com\/cdeotte\/how-to-choose-cnn-architecture-mnist\/notebook\n* https:\/\/www.kaggle.com\/yassineghouzam\/introduction-to-cnn-keras-0-997-top-6\n","8e7d113e":"# Prediction and submission","0ba5bf16":"**Convolutional neural networks** \n* In a convolutional neural network, every node is not connected to every next node.\n* When we use a regular fully connected model, most images would result in a large model(because of the number of pixels) and thus cause over-fitting. Thats why a convolutional neural network is better than a fully connected network.\n* A convolutional network has 4 layers: Convolutional layer, pooling, ReLU,and fully connected.\n* Using the above layers in the right combination gives us a good model.\n* Dense layer is what we used of any other neural network where we had a large number of nodes and the input was a linear matrix. Flatten layers allow you to change the shape of the data from a vector of 2d matrixes into the correct format for a dense layer to interpret. So we first add the flatten layer and then the dense layer.","a5fcaf4a":"Here I will present a model that is accurate by 99.69 percent on the validation set. I have a lot to learn, and this is more of a record of porogress than an ideal\/ most accurate model. I appreciate comments and corrections, and I hope the beginners out there might benefit from this notebook just like I did from other people's contributions!\n*directly fork and run to see performance. You can then tweak the parameters while annealing, epochs, the model etc*"}}