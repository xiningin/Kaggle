{"cell_type":{"5a413d4c":"code","13bdbb2c":"code","fb151ca9":"code","a6b99cfd":"code","42f90da5":"code","53393a37":"code","46895a7c":"code","f50a7d82":"code","608f15bf":"code","2837041a":"code","46a2a571":"code","8ec47480":"code","b0d1bd04":"code","10738b41":"code","97cd2b4d":"code","62e846bc":"code","7583c1bf":"code","3534f828":"code","33e07c20":"code","023ea294":"code","222b74db":"code","70b52401":"code","d477d38f":"code","2de64a86":"code","dc4f73fe":"code","0b33b2de":"code","b5b0cba7":"code","6b6c80d9":"code","06a2c4f1":"code","007109cf":"code","25d45dc4":"code","3e5882cc":"code","18487949":"code","1b6db484":"code","53daba77":"code","b5ac5ad3":"code","f59bc679":"code","430a9519":"code","9a256c94":"code","570f3270":"code","70bd62ea":"code","1de460c6":"code","330f13c5":"code","0377fa7d":"code","77d3d441":"code","134d43cf":"code","2655c65a":"code","b2290ee5":"code","8ee34f7f":"code","c99267e1":"code","74f49fe5":"code","40686a88":"code","53be470e":"code","7fc6ea3a":"code","8c1ee461":"code","77d0b93a":"code","aecc2f55":"markdown","eff5745e":"markdown","017beb74":"markdown","ac7b0c6e":"markdown","79dd034b":"markdown","d9349aa7":"markdown","8c1a0c48":"markdown","00323ee7":"markdown","902a4bcd":"markdown","d5a6da46":"markdown","4d2e1898":"markdown","a73cd740":"markdown","1f81dea5":"markdown","36eed8a4":"markdown","b8a9d6e1":"markdown","dd22b99c":"markdown","125969fd":"markdown","cee30f53":"markdown","a7b146f5":"markdown","5d7b5200":"markdown","c8cadb7a":"markdown","aed9ba3e":"markdown","c5bc1ab3":"markdown","1bf77a3b":"markdown","7829e8db":"markdown","0fd72f8f":"markdown","29972477":"markdown","65b737ee":"markdown","02bb5f2f":"markdown","0ee44842":"markdown","159d97cf":"markdown","4d941475":"markdown","ade69d66":"markdown","cc2d4d55":"markdown","94afb3db":"markdown","0b38ca28":"markdown","1954285c":"markdown","91789e1c":"markdown","0f69f080":"markdown","6a81aa25":"markdown"},"source":{"5a413d4c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","13bdbb2c":"import sklearn\nimport scipy.sparse \nimport lightgbm as lgb\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport pickle\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.ensemble import RandomForestRegressor\nfrom tqdm import tqdm\nimport warnings\nwarnings.filterwarnings('ignore')\n%matplotlib inline \n\npd.set_option('display.max_rows', 600)\npd.set_option('display.max_columns', 50)\nsns.set(rc={'figure.figsize':(20, 10)})","fb151ca9":"for p in [np, pd, sklearn, scipy, lgb, sns]:\n    print (p.__name__, p.__version__)","a6b99cfd":"sales = pd.read_csv('\/kaggle\/input\/competitive-data-science-predict-future-sales\/sales_train.csv')\nshops = pd.read_csv('\/kaggle\/input\/competitive-data-science-predict-future-sales\/shops.csv')\nitems = pd.read_csv('\/kaggle\/input\/competitive-data-science-predict-future-sales\/items.csv')\nitem_cats = pd.read_csv('\/kaggle\/input\/competitive-data-science-predict-future-sales\/item_categories.csv')\ntest = pd.read_csv('\/kaggle\/input\/competitive-data-science-predict-future-sales\/test.csv')","42f90da5":"shops.head()","53393a37":"sales_month = pd.DataFrame(sales.groupby(['date_block_num'])['item_cnt_day'].sum()).reset_index()\n\nsales_month.columns = ['date_block_num', 'sum_items_sold']\n\nsns.barplot(x ='date_block_num', y='sum_items_sold', \n            data=sales_month.reset_index());\n\nplt.title('Sales per month')","46895a7c":"comb_shop_item = pd.DataFrame(sales[['date_block_num', 'shop_id', \n                                     'item_id']].drop_duplicates().groupby('date_block_num').size()).reset_index()\n\ncomb_shop_item.columns = ['date_block_num', 'item-shop-size']\nsns.barplot(x ='date_block_num', y='item-shop-size', data=comb_shop_item);\nplt.title('Distinct shop-id with sales per month')","f50a7d82":"sns.set_context(\"talk\", font_scale=1)\nsales_month_shop_id = pd.DataFrame(sales.groupby(['shop_id'])['item_cnt_day'].sum()).reset_index()\n\nsales_month_shop_id.columns = ['shop_id', 'sum_sales']\nsns.barplot(x ='shop_id', y='sum_sales', data=sales_month_shop_id, palette='Paired')\nplt.title('Sales per shop');","608f15bf":"sns.set_context(\"talk\", font_scale=1.4)\nsales_item_id = pd.DataFrame(sales.groupby(['item_id'])['item_cnt_day'].sum())\nplt.xlabel('item id')\nplt.ylabel('sales')\nplt.plot(sales_item_id);","2837041a":"anomaly_item = sales_item_id.item_cnt_day.argmax()\nanomaly_item","46a2a571":"tuples_df = pd.Series(list(sales[['item_id', 'shop_id']].itertuples(index=False, name=None)))\ntuples_test = pd.Series(list(test[['item_id', 'shop_id']].itertuples(index=False, name=None)))\nprint(str(round(tuples_df.isin(tuples_test).sum()\/len(tuples_df),2)*100)+'%')","8ec47480":"shops.head()","b0d1bd04":"shops.loc[shops.shop_name == '\u0421\u0435\u0440\u0433\u0438\u0435\u0432 \u041f\u043e\u0441\u0430\u0434 \u0422\u0426 \"7\u042f\"', 'shop_name'] = '\u0421\u0435\u0440\u0433\u0438\u0435\u0432\u041f\u043e\u0441\u0430\u0434 \u0422\u0426 \"7\u042f\"'\nshops['city'] = shops['shop_name'].str.split(' ').map(lambda x: x[0])\nshops.loc[shops.city == '!\u042f\u043a\u0443\u0442\u0441\u043a', 'city'] = '\u042f\u043a\u0443\u0442\u0441\u043a'\nshops['city_code'] = LabelEncoder().fit_transform(shops['city'])\nshops = shops[['shop_id','city_code']].copy()\n\nitem_cats['split'] = item_cats['item_category_name'].str.split('-')\nitem_cats['type'] = item_cats['split'].map(lambda x: x[0].strip())\nitem_cats['type_code'] = LabelEncoder().fit_transform(item_cats['type'])\n# if subtype is nan then type\nitem_cats['subtype'] = item_cats['split'].map(lambda x: x[1].strip() if len(x) > 1 else x[0].strip())\nitem_cats['subtype_code'] = LabelEncoder().fit_transform(item_cats['subtype'])\nitem_cats = item_cats[['item_category_id','type_code', 'subtype_code', 'item_category_name']].copy()\n\n#items.drop(['item_name'], axis=1, inplace=True)\n","10738b41":"# making a word cloud for item categories name\n\nfrom wordcloud import WordCloud\nfrom wordcloud import STOPWORDS\n\nplt.rcParams['figure.figsize'] = (15, 12)\nstopwords = set(STOPWORDS)\nwordcloud = WordCloud(background_color = 'lightblue',\n                      max_words = 200, \n                      stopwords = stopwords,\n                     width = 1200,\n                     height = 800,\n                     random_state = 42).generate(str(item_cats['item_category_name']))\n\n\nplt.title('Wordcloud for Item Category Names', fontsize = 30)\nplt.axis('off')\nplt.imshow(wordcloud, interpolation = 'bilinear')","97cd2b4d":"# making a word cloud for item name\n\nfrom wordcloud import WordCloud\nfrom wordcloud import STOPWORDS\n\nplt.rcParams['figure.figsize'] = (15, 12)\nstopwords = set(STOPWORDS)\nwordcloud = WordCloud(background_color = 'pink',\n                      max_words = 200, \n                      stopwords = stopwords,\n                     width = 1200,\n                     height = 800,\n                     random_state = 42).generate(str(items['item_name']))\n\n\nplt.title('Wordcloud for Item Names', fontsize = 30)\nplt.axis('off')\nplt.imshow(wordcloud, interpolation = 'bilinear')","62e846bc":"from itertools import product\nimport scipy.sparse \nfrom tqdm import tqdm_notebook\nfrom itertools import product\nfrom sklearn.metrics import mean_squared_error\nimport gc\n\n\ndef downcast_dtypes(df):\n    \n    # Select columns to downcast\n    float_cols = [c for c in df if df[c].dtype == \"float64\"]\n    int_cols =   [c for c in df if df[c].dtype == \"int64\"]\n    \n    # Downcast\n    df[float_cols] = df[float_cols].astype(np.float32)\n    df[int_cols]   = df[int_cols].astype(np.int32)\n    \n    return df\n\ndef get_feature_matrix(sales, test, items, list_lags, date_block_threshold):\n    \n    \"\"\" This function create the model tablon\"\"\"\n  \n    # Create \"grid\" with columns\n    index_cols = ['shop_id', 'item_id', 'date_block_num']\n\n    # For every month we create a grid from all shops\/items combinations from that month\n    grid = [] \n    new_items = pd.DataFrame()\n    cur_items_aux=np.array([])\n    for block_num in sales['date_block_num'].unique():\n        cur_shops = sales.loc[sales['date_block_num'] == block_num, 'shop_id'].unique()\n        cur_items = sales.loc[sales['date_block_num'] == block_num, 'item_id'].append(pd.Series(cur_items_aux)).unique()\n        cur_items_aux = cur_items[pd.Series(cur_items).isin(test.item_id)]\n        grid.append(np.array(list(product(*[cur_shops, cur_items, [block_num]])),dtype='int32'))\n\n    # Turn the grid into a dataframe\n    grid = pd.DataFrame(np.vstack(grid), columns = index_cols,dtype=np.int32)\n\n    # Add submission shop_id-item_id in order to test predictions\n    test['date_block_num'] = 34\n    grid = grid.append(test[['shop_id', 'item_id', 'date_block_num']])\n\n    # Groupby data to get shop-item-month aggregates\n    gb = sales.groupby(index_cols,as_index=False).agg({'item_cnt_day':{'sum'}}).rename({\"item_cnt_day\":\"target\"}, axis=1)\n    # Fix column names\n    gb.columns = index_cols + ['target']\n    # Join it to the grid\n    all_data = pd.merge(grid, gb, how='left', on=index_cols).fillna(0)\n\n    # Same as above but with shop-month aggregates\n    gb = sales.groupby(['shop_id', 'date_block_num'],as_index=False).agg({'item_cnt_day':{'sum'}}).rename({\"item_cnt_day\":\"target_shop\"}, axis=1)\n    gb.columns = ['shop_id', 'date_block_num'] + ['target_shop']\n    \n    all_data = pd.merge(all_data, gb, how='left', on=['shop_id', 'date_block_num']).fillna(0)\n\n    # Same as above but with item-month aggregates\n    gb = sales.groupby(['item_id', 'date_block_num'],as_index=False).agg({'item_cnt_day':{'sum'}}).rename({\"item_cnt_day\":\"target_item\"}, axis=1)\n    gb.columns = ['item_id', 'date_block_num'] + ['target_item']\n    all_data = pd.merge(all_data, gb, how='left', on=['item_id', 'date_block_num']).fillna(0)\n\n    # Downcast dtypes from 64 to 32 bit to save memory\n    all_data = downcast_dtypes(all_data)\n    del grid, gb \n    gc.collect()\n    # List of columns that we will use to create lags\n    cols_to_rename = list(all_data.columns.difference(index_cols)) \n\n    shift_range = list_lags\n\n    for month_shift in tqdm_notebook(shift_range):\n        train_shift = all_data[index_cols + cols_to_rename].copy()\n    \n        train_shift['date_block_num'] = train_shift['date_block_num'] + month_shift\n    \n        foo = lambda x: '{}_lag_{}'.format(x, month_shift) if x in cols_to_rename else x\n        train_shift = train_shift.rename(columns=foo)\n\n        all_data = pd.merge(all_data, train_shift, on=index_cols, how='left').fillna(0)\n\n    del train_shift\n\n    # Don't use old data from year 2013\n    all_data = all_data[all_data['date_block_num'] >= date_block_threshold] \n\n    # List of all lagged features\n    fit_cols = [col for col in all_data.columns if col[-1] in [str(item) for item in shift_range]] \n    # We will drop these at fitting stage\n    to_drop_cols = list(set(list(all_data.columns)) - (set(fit_cols)|set(index_cols))) + ['date_block_num'] \n\n    # Category for each item\n    item_category_mapping = items[['item_id','item_category_id']].drop_duplicates()\n\n    all_data = pd.merge(all_data, item_category_mapping, how='left', on='item_id')\n    all_data = downcast_dtypes(all_data)\n    gc.collect();\n    \n    return [all_data, to_drop_cols]","7583c1bf":"list_lags = [1, 2, 3]\ndate_block_threshold = 12\nsales_for_modelling = sales[sales.item_id.isin(test.item_id)]\n\n[all_data, to_drop_cols]  = get_feature_matrix(sales_for_modelling, test, items, list_lags, date_block_threshold)","3534f828":"all_data.head()","33e07c20":"all_data = pd.merge(all_data, shops, how='left', on='shop_id')\nall_data = pd.merge(all_data, item_cats[['item_category_id', 'type_code', 'subtype_code']], how='left', on='item_category_id')","023ea294":"all_data.head()","222b74db":"mean_enc_item_cat = pd.DataFrame(all_data.groupby(['shop_id', \n                                                    'item_category_id']).target.agg(['mean', 'var']).reset_index())\n\nmean_enc_item_cat.columns = ['shop_id', 'item_category_id', 'mean_enc_cat_id', 'var_enc_cat_id']\nall_data = pd.merge(all_data, mean_enc_item_cat, how='left', on=['shop_id', 'item_category_id'])\n\nall_data = downcast_dtypes(all_data)","70b52401":"from sklearn.model_selection import KFold\n\nmean_encoded_col = ['item_id']\nTarget = 'target'\nSEED = 55\n\nglobal_mean =  all_data[Target].mean()\ny_tr = all_data[Target].values\n\nfor col in tqdm(mean_encoded_col):\n\n    col_tr = all_data[[col] + [Target]]\n    corrcoefs = pd.DataFrame(columns = ['Cor'])\n\n    # 3.1.1 Mean encodings - KFold scheme\n    \n    kf = KFold(n_splits = 5, shuffle = False, random_state = SEED)\n\n    col_tr[col + '_cnt_month_mean_Kfold'] = global_mean\n    \n    for tr_ind, val_ind in kf.split(col_tr):\n        X_tr, X_val = col_tr.iloc[tr_ind], col_tr.iloc[val_ind]\n        means = X_val[col].map(X_tr.groupby(col)[Target].mean())\n        X_val[col + '_cnt_month_mean_Kfold'] = means\n        col_tr.iloc[val_ind] = X_val\n        # X_val.head()\n    col_tr.fillna(global_mean, inplace = True)\n    corrcoefs.loc[col + '_cnt_month_mean_Kfold'] = np.corrcoef(y_tr, col_tr[col + '_cnt_month_mean_Kfold'])[0][1]\n\n    # 3.1.2 Mean encodings - Leave-one-out scheme\n    item_id_target_sum = col_tr.groupby(col)[Target].sum()\n    item_id_target_count = col_tr.groupby(col)[Target].count()\n    col_tr[col + '_cnt_month_sum'] = col_tr[col].map(item_id_target_sum)\n    col_tr[col + '_cnt_month_count'] = col_tr[col].map(item_id_target_count)\n    col_tr[col + '_target_mean_LOO'] = (col_tr[col + '_cnt_month_sum'] - col_tr[Target]) \/ (col_tr[col + '_cnt_month_count'] - 1)\n    col_tr.fillna(global_mean, inplace = True)\n    corrcoefs.loc[col + '_target_mean_LOO'] = np.corrcoef(y_tr, col_tr[col + '_target_mean_LOO'])[0][1]\n\n\n    # 3.1.3 Mean encodings - Smoothing\n    item_id_target_mean = col_tr.groupby(col)[Target].mean()\n    item_id_target_count = col_tr.groupby(col)[Target].count()\n    col_tr[col + '_cnt_month_mean'] = col_tr[col].map(item_id_target_mean)\n    col_tr[col + '_cnt_month_count'] = col_tr[col].map(item_id_target_count)\n    alpha = 100\n    col_tr[col + '_cnt_month_mean_Smooth'] = (col_tr[col + '_cnt_month_mean'] *  col_tr[col + '_cnt_month_count'] + global_mean * alpha) \/ (alpha + col_tr[col + '_cnt_month_count'])\n    col_tr[col + '_cnt_month_mean_Smooth'].fillna(global_mean, inplace=True)\n    corrcoefs.loc[col + '_cnt_month_mean_Smooth'] = np.corrcoef(y_tr, col_tr[col + '_cnt_month_mean_Smooth'])[0][1]\n\n\n    # 3.1.4 Mean encodings - Expanding mean scheme\n    cumsum = col_tr.groupby(col)[Target].cumsum() - col_tr[Target]\n    sumcnt = col_tr.groupby(col).cumcount()\n    col_tr[col + '_cnt_month_mean_Expanding'] = cumsum \/ sumcnt\n    col_tr[col + '_cnt_month_mean_Expanding'].fillna(global_mean, inplace=True)\n    corrcoefs.loc[col + '_cnt_month_mean_Expanding'] = np.corrcoef(y_tr, col_tr[col + '_cnt_month_mean_Expanding'])[0][1]\n\n    all_data_2 = pd.concat([all_data, col_tr[corrcoefs['Cor'].idxmax()]], axis = 1)\n    print(corrcoefs.sort_values('Cor'))\n","d477d38f":"all_data_2.head()","2de64a86":"from sklearn.model_selection import train_test_split\n\nX = all_data_2.drop(to_drop_cols, axis=1)\ny = all_data_2['target']\n\nX_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.3, random_state=42)","dc4f73fe":"sub_data = all_data_2[all_data_2.date_block_num==34].fillna(0)\nall_data = all_data_2[all_data_2.date_block_num<34].fillna(0)\n\nsub_data.head()","0b33b2de":"all_data.groupby('date_block_num')['item_id'].count()","b5b0cba7":"dates = all_data['date_block_num']\nboolean_test = (dates.isin([22,31,32,33]))\nboolean_train = ~boolean_test\ndates_train = dates[boolean_train]\ndates_val  = dates[boolean_test]\n\nX_train = all_data.loc[boolean_train].drop(to_drop_cols, axis=1)\nX_val =  all_data.loc[boolean_test].drop(to_drop_cols, axis=1)\n\ny_train = all_data.loc[boolean_train, 'target'].values\ny_val =  all_data.loc[boolean_test, 'target'].values","6b6c80d9":"print('X_train shape is ' + str(X_train.shape))\nprint('X_val shape is ' + str(X_val.shape))","06a2c4f1":"print(f'Cross-validation is the {round(X_val.shape[0]\/X_train.shape[0],2)*100} %' )","007109cf":"tuples_validation_submission = pd.Series(list(X_val[['item_id', 'shop_id']][dates_val==33].itertuples(index=False, name=None)))\nprint(f'The {round(tuples_test.isin(tuples_validation_submission).sum()\/len(tuples_test),2)*100} % of the item_id-shop_id are in the cv set ') ","25d45dc4":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import GridSearchCV, PredefinedSplit\nimport lightgbm as lgb\nimport xgboost as xgb","3e5882cc":"def rmse(*args):\n    \n    \"\"\" Funcion that calculates the root mean squared error\"\"\"\n    return np.sqrt(mean_squared_error(*args))\n\ndef clip20(x):\n    return np.clip(x, 0, 20)\n\ndef clip40(x):\n    return np.clip(x, 0, 20)","18487949":"learning_rates = [0.01, 0.02, 0.03, 0.04, 0.05, 0.06]\nbest_rmse = 9999999999999\n\nfor lr in learning_rates:\n    lgb_params = {\n               'feature_fraction': 0.75,\n               'metric': 'rmse',\n               'nthread':1, \n               'min_data_in_leaf': 2**7, \n               'bagging_fraction': 0.75, \n               'learning_rate': lr, \n               'objective': 'mse', \n               'bagging_seed': 2**7, \n               'num_leaves': 2**7,\n               'bagging_freq':1,\n               'verbose':0,\n               'force_col_wise':True\n              }\n\n    lgb_model = lgb.train(lgb_params, lgb.Dataset(X_train, label=clip40(y_train)), int(100 * (lr \/ 0.03)))\n    pred_lgb_val = lgb_model.predict(X_val)\n    score = rmse(clip20(y_val), clip20(pred_lgb_val))\n\n    if score < best_rmse:\n        best_rmse = score\n        best_lr = lr\n        best_lgb = lgb_model","1b6db484":"best_lr","53daba77":"X = X_train.append(X_val)\ny = np.append(y_train, y_val)","b5ac5ad3":"best_lgb_params = {\n               'feature_fraction': 0.75,\n               'metric': 'rmse',\n               'nthread':1, \n               'min_data_in_leaf': 2**7, \n               'bagging_fraction': 0.75, \n               'learning_rate': best_lr, \n               'objective': 'mse', \n               'bagging_seed': 2**7, \n               'num_leaves': 2**7,\n               'bagging_freq':1,\n               'verbose':0 \n              }\nbest_lgb = lgb.train(best_lgb_params, lgb.Dataset(X, label=clip40(y)), int(100 * (lr \/ 0.03)))","f59bc679":"model = xgb.XGBRegressor(\n    max_depth=8,\n    n_estimators=500,\n    min_child_weight=300, \n    colsample_bytree=0.8, \n    subsample=0.8, \n    eta=0.3,    \n    seed=42)\n\nmodel.fit(\n    X_train, \n    y_train, \n    eval_metric=\"rmse\", \n    eval_set=[(X_train, y_train), (X_val, y_val)], \n    verbose=True, \n    early_stopping_rounds = 10)","430a9519":"X = X_train.append(X_val)\nY = np.concatenate([y_train, y_val])\ntrain_ind=np.zeros(X.shape[0])\nfor i in range(0, len(X_train)):\n    train_ind[i] = -1\nps = PredefinedSplit(test_fold=(train_ind))","9a256c94":"param_grid={'max_features':[4, 6, 8], \n            'max_depth' : [4, 6, 8]}\ngs = GridSearchCV(cv = 3, \n                  estimator = RandomForestRegressor(n_estimators=200, n_jobs=-1), \n                  param_grid=param_grid, scoring='neg_mean_squared_error')","570f3270":"rf_model = RandomForestRegressor(n_estimators=50, max_depth=5)","70bd62ea":"rf_model.fit(X_train, y_train)","1de460c6":"pred_lgb_val = best_lgb.predict(X_val)\nprint('Train RMSE for lgb is  %f' % rmse(clip20(y_train), clip20(best_lgb.predict(X_train))))\nprint('Val RMSE for lgb is %f' % rmse(clip20(y_val), clip20(pred_lgb_val)))","330f13c5":"feat_importances = pd.Series(best_lgb.feature_importance(), index=X_val.columns)\nfeat_importances = feat_importances.nlargest(20)\nfeat_importances.plot(kind='barh')\nplt.title('Feature importance LGB')\nplt.show()","0377fa7d":"pred_xgb_val = model.predict(X_val)\nprint('Train RMSE for xgb is  %f' % rmse(clip20(y_train), clip20(model.predict(X_train))))\nprint('Val RMSE for xgb is %f' % rmse(clip20(y_val), clip20(pred_xgb_val)))","77d3d441":"feat_importances = pd.Series(model.feature_importances_, index=X_val.columns)\nfeat_importances = feat_importances.nlargest(20)\nfeat_importances.plot(kind='barh')\nplt.title('Feature importance XGB')\nplt.show()","134d43cf":"pred_rf_val = clip20(rf_model.predict(X_val.fillna(0)))\nprint('Train RMSE for rf is %f' % rmse(clip20(y_train), clip20(rf_model.predict(X_train))))\nprint('Val RMSE for rf is %f' % rmse(clip20(y_val), pred_rf_val))","2655c65a":"feat_importances = pd.Series(rf_model.feature_importances_, index=X_val.columns)\nfeat_importances = feat_importances.nlargest(20)\nfeat_importances.plot(kind='barh')\nplt.title('Feature importance RF')\nplt.show()","b2290ee5":"X_val_level2 = np.c_[pred_rf_val, pred_lgb_val, pred_xgb_val] ","8ee34f7f":"lr = LinearRegression()\nlr.fit(X_val_level2, clip40(y_val))\npred_lr_val =  clip20(lr.predict(X_val_level2))\n\nprint('Test rmse for stacking variables is %f' % rmse(clip20(y_val), clip20(pred_lr_val)))","c99267e1":"pred_test_lgb = best_lgb.predict(sub_data.drop(to_drop_cols, axis=1).fillna(0))","74f49fe5":"pred_test_xgb = model.predict(sub_data.drop(to_drop_cols, axis=1).fillna(0))","40686a88":"pred_test_rf = rf_model.predict(sub_data.drop(to_drop_cols, axis=1).fillna(0))","53be470e":"X_test_level2 = np.c_[pred_test_rf, pred_test_lgb, pred_test_xgb]","7fc6ea3a":"test_pred = clip20(lr.predict(X_test_level2))","8c1ee461":"test_pred.mean()","77d0b93a":"predictions = pd.DataFrame()\npredictions['shop_id'] = test.shop_id\npredictions['item_id'] = test.item_id\npredictions['item_cnt_month'] = test_pred\nsubmision = test[['ID', 'shop_id', 'item_id']].merge(predictions, on=['shop_id', 'item_id'], how='left').fillna(0)\nsubmision[['ID', 'item_cnt_month']].to_csv('submissionSales.csv',index=False)","aecc2f55":"## Submission","eff5745e":"# Final Project Coursera","017beb74":"#### Sales per shop","ac7b0c6e":"### XGBoost","79dd034b":"### LightGBM","d9349aa7":"### Stacking with linear regression","8c1a0c48":"# Models","00323ee7":"### Version","902a4bcd":"KFold","d5a6da46":"Predict for test data","4d2e1898":"#### Sales for item","a73cd740":"### Random Forest","1f81dea5":"## lightgbm","36eed8a4":"## Libraries","b8a9d6e1":"# Train and test","dd22b99c":"### Add text features","125969fd":"#### Sales","cee30f53":"Some item-shop of the train set appears in the test set.","a7b146f5":"## Random Forest","5d7b5200":"Predict","c8cadb7a":"There is one item with a large number of sales.","aed9ba3e":"## Mean encoding","c5bc1ab3":"## Data Leakage","1bf77a3b":"Option 1: random","7829e8db":"## Load data from Kaggle","0fd72f8f":"Option 2: date_block_num=34 is for competition submission","29972477":"Simple","65b737ee":"Join","02bb5f2f":"### Features from text","0ee44842":"## Features","159d97cf":"The first step is to load all the required libraries and load raw data files into memory.","4d941475":"Grid Search","ade69d66":"Create features of sales and lags 1, 2 y 3.","cc2d4d55":"# Stacking","94afb3db":"## XGBoost","0b38ca28":"The last months will validation set.","1954285c":"# Predict and evaluate","91789e1c":"## EDA","0f69f080":"#### shop-id","6a81aa25":"Best model with all the data."}}