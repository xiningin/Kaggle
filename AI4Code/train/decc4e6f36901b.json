{"cell_type":{"2314a2fe":"code","a5f8dcb5":"code","45383967":"code","67d3d116":"code","bbc582e9":"code","a131d7bc":"code","1fa9f8a4":"code","7905afbe":"code","6aaf9ea9":"code","26f08f97":"code","81279a62":"code","aa9f83a0":"code","45265a07":"code","6d731d1e":"code","0e10f129":"code","47f9b3f7":"code","18e0168f":"code","b0a84f6c":"code","09f93f6f":"code","9ec59729":"code","b8e028a9":"code","b0c3cf43":"code","438b0ec2":"code","4d3dd7f9":"code","b292eaf0":"code","455db0ef":"code","b2a42d93":"code","db5b372b":"code","57644d02":"code","29553f1a":"code","2a3934c2":"code","cf7036c8":"code","6f44156e":"code","124e395f":"code","0a574b82":"code","c50e8b4a":"code","e1055cb8":"code","e0ab9d55":"code","b5da0a89":"code","2fcca55d":"code","36f6db0c":"code","15d62afd":"code","e0bf5860":"code","be0bad66":"code","131d1ba7":"code","444b0ec2":"code","6bdbfafe":"code","07ee93a3":"markdown","e31322f7":"markdown","67b43f47":"markdown","ed7b80d6":"markdown","cf698499":"markdown","af208aa2":"markdown","7ac00bef":"markdown","be9dd9fc":"markdown","4bdf78f8":"markdown","9d3a8529":"markdown","119e0945":"markdown","edfb57c9":"markdown","43342213":"markdown","a1e491e2":"markdown","0e07f8c6":"markdown"},"source":{"2314a2fe":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nfrom wordcloud import WordCloud\nfrom sklearn.pipeline import Pipeline\n\n#to data preprocessing\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder\n\n#NLP tools\nimport re\nimport nltk\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n#train split and fit models\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom nltk.tokenize import TweetTokenizer\n\n#model selection\nfrom sklearn.metrics import confusion_matrix, accuracy_score, plot_confusion_matrix, classification_report\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","a5f8dcb5":"dataset = pd.read_csv('..\/input\/hate-speech-and-offensive-language-dataset\/labeled_data.csv')\ndataset.head()","45383967":"dataset.info()","67d3d116":"dataset.describe().T","bbc582e9":"dt_transformed = dataset[['class', 'tweet']]\ny = (dt_transformed.iloc[:, :-1].values).ravel()\ndt_transformed","a131d7bc":"# Dividindo o df em treino e teste\ndf_train, df_test = train_test_split(dt_transformed, test_size = 0.10, random_state = 42, stratify=dt_transformed['class'])\ndf_train.shape, df_test.shape","1fa9f8a4":"# Dividindo o df em treino e valida\u00e7\u00e3o\ndf_train, df_vad = train_test_split(df_train, test_size = 0.10, random_state = 42, stratify=df_train['class'])\ndf_train.shape, df_vad.shape","7905afbe":"df_train['class'].value_counts().plot(kind='bar')","6aaf9ea9":"def preprocessing(data):\n    stemmer = nltk.stem.RSLPStemmer()\n    all_stopwords = stopwords.words('english')\n    all_stopwords.remove('not')\n    corpus = []\n    for tweet in data:\n      review = re.sub(r\"@[A-Za-z0-9_]+\", \" \", tweet)\n      review = re.sub('RT', ' ', review)\n      review = re.sub(r\"https?:\/\/[A-Za-z0-9.\/]+\", \" \", review)\n      review = re.sub(r\"https?\", \" \", review)\n      review = re.sub('[^a-zA-Z]', ' ', review)\n      review = review.lower()\n      review = review.split()\n      ps = PorterStemmer()\n      review = [ps.stem(word) for word in review if not word in set(all_stopwords) if len(word) > 2]\n      review = ' '.join(review)\n      corpus.append(review)\n\n    return np.array(corpus)","26f08f97":"c_train = preprocessing(df_train['tweet'].values)\nc_vad = preprocessing(df_vad['tweet'].values)","81279a62":"tweet_tokenizer = TweetTokenizer() \nvectorizer = CountVectorizer(analyzer=\"word\", tokenizer=tweet_tokenizer.tokenize, max_features = 1010)\ndef tokenize(corpus, flag=0):\n    \n    #flag = 1 --> treino\n    if (flag):\n        return vectorizer.fit_transform(corpus).toarray()\n    else:\n        return vectorizer.transform(corpus).toarray()","aa9f83a0":"X_train = tokenize(c_train, 1)\nX_vad = tokenize(c_vad, 0)\ny_train = df_train['class'].values\ny_vad = df_vad['class'].values\nX_train.shape, X_vad.shape","45265a07":"# Logistic Regression\nmodel = LogisticRegression(multi_class='ovr', solver='liblinear', random_state = 0)\nmodel.fit(X_train, y_train.ravel())\ny_pred = model.predict(X_vad)","6d731d1e":"def set_confusion_matrix(clf, X, y, title):\n    plot_confusion_matrix(clf, X, y)\n    plt.title(title)\n    plt.show()","0e10f129":"set_confusion_matrix(model, X_vad, y_vad, type(model).__name__)","47f9b3f7":"target_names = ['class 0', 'class 1', 'class 2']\nprint(classification_report(y_vad, y_pred, target_names=target_names))","18e0168f":"conjunto = c_train\nhate_tweets = [sentence for sentence, label in zip(conjunto, y) if label == 0]\noff_tweets = [sentence for sentence, label in zip(conjunto, y) if label == 1]\nnone_tweets = [sentence for sentence, label in zip(conjunto, y) if label == 2]","b0a84f6c":"hate_words = ' '.join(hate_tweets)\noff_words = ' '.join(off_tweets)\nnone_words = ' '.join(none_tweets)","09f93f6f":"def get_wordcloud(text):\n    # Create and generate a word cloud image:\n    wordcloud = WordCloud().generate(text)\n\n    # Display the generated image:\n    plt.imshow(wordcloud, interpolation='bilinear')\n    plt.axis(\"off\")\n    plt.show()","9ec59729":"get_wordcloud(hate_words)\nget_wordcloud(off_words)\nget_wordcloud(none_words)","b8e028a9":"def wordListToFreqDict(wordlist):\n    wordfreq = [(wordlist.count(p))\/len(wordlist) for p in wordlist]\n    return dict(list(zip(wordlist,wordfreq)))","b0c3cf43":"def sortFreqDict(freqdict):\n    aux = [(freqdict[key], key) for key in freqdict]\n    aux.sort()\n    aux.reverse()\n    return aux","438b0ec2":"hate_dict = sortFreqDict(wordListToFreqDict(hate_words.split()))\noff_dict = sortFreqDict(wordListToFreqDict(off_words.split()))\nnone_dict = sortFreqDict(wordListToFreqDict(none_words.split()))","4d3dd7f9":"len(hate_dict), len(off_dict), len(none_dict)","b292eaf0":"def get_common(wordlist, n):\n    return ([w[1] for w in wordlist])[:n]\n\ncommon_words = list()\ncommon_words.append(get_common(hate_dict, 2000))\ncommon_words.append(get_common(off_dict, 1000))\ncommon_words.append(get_common(none_dict, 1000))\ncommon_words = np.unique(np.hstack(common_words))","455db0ef":"common_words_dict = ({i:j for i, j in zip(common_words, range(len(common_words)))})","b2a42d93":"X_train = tokenize(c_train, 1)\nX_vad = tokenize(c_vad, 0)\nX_train.shape, X_vad.shape","db5b372b":"# Logistic Regression\nmodel = LogisticRegression(multi_class='ovr', solver='liblinear', random_state = 0)\nmodel.fit(X_train, y_train.ravel())\ny_pred = model.predict(X_vad)","57644d02":"set_confusion_matrix(model, X_vad, y_vad, type(model).__name__)","29553f1a":"target_names = ['class 0', 'class 1', 'class 2']\nprint(classification_report(y_vad, y_pred, target_names=target_names))","2a3934c2":"n_off, n_none, n_hate = df_train['class'].value_counts()\nn_hate, n_off, n_none ","cf7036c8":"df_hate = df_train[df_train['class'] == 0]\ndf_off = df_train[df_train['class'] == 1]\ndf_none = df_train[df_train['class'] == 2]","6f44156e":"df_off_under = df_off.sample(n_hate, random_state=0)\ndf_none_under = df_none.sample(n_hate, random_state=0)\n\ndf_under = pd.concat([df_hate, df_off_under, df_none_under], axis=0)\nprint(df_under['class'].value_counts())","124e395f":"c_train = preprocessing(df_under['tweet'].values)\nc_vad = preprocessing(df_vad['tweet'].values)","0a574b82":"X_train = tokenize(c_train, 1)\nX_vad = tokenize(c_vad, 0)\ny_train = df_under['class'].values\ny_vad = df_vad['class'].values\nX_train.shape, X_vad.shape","c50e8b4a":"# Logistic Regression\nmodel_under = LogisticRegression(multi_class='ovr', solver='liblinear', random_state = 0)\nmodel_under.fit(X_train, y_train.ravel())\ny_pred = model_under.predict(X_vad)","e1055cb8":"set_confusion_matrix(model_under, X_vad, y_vad, type(model_under).__name__)","e0ab9d55":"target_names = ['class 0', 'class 1', 'class 2']\nprint(classification_report(y_vad, y_pred, target_names=target_names))","b5da0a89":"n_off, n_none, n_hate = df_train['class'].value_counts()\nn_hate, n_off, n_none ","2fcca55d":"df_hate_over = df_hate.sample(n_off, replace=True, random_state=0)\ndf_none_over = df_none.sample(n_off, replace=True, random_state=0)\ndf_over = pd.concat([df_off, df_hate_over, df_none_over], axis=0)\n\nprint('Random over-sampling:')\nprint(df_over['class'].value_counts())","36f6db0c":"c_train = preprocessing(df_over['tweet'].values)\nc_vad = preprocessing(df_vad['tweet'].values)","15d62afd":"X_train = tokenize(c_train, 1)\nX_vad = tokenize(c_vad, 0)\ny_train = df_over['class'].values\ny_vad = df_vad['class'].values\nX_train.shape, X_vad.shape","e0bf5860":"# Logistic Regression\nmodel_over = LogisticRegression(multi_class='ovr', solver='liblinear', random_state = 0)\nmodel_over.fit(X_train, y_train.ravel())\ny_pred = model_over.predict(X_vad)","be0bad66":"set_confusion_matrix(model_over, X_vad, y_vad, type(model_over).__name__)\ntarget_names = ['class 0', 'class 1', 'class 2']\nprint(classification_report(y_vad, y_pred, target_names=target_names))","131d1ba7":"c_test = preprocessing(df_test['tweet'].values)\n\nc_test.shape","444b0ec2":"X_test = tokenize(c_test, 0)\ny_test = df_test['class']\n\nX_test.shape, y_test.shape","6bdbfafe":"y_pred = model_over.predict(X_test)\nset_confusion_matrix(model_over, X_test, y_test, type(model_over).__name__)\ntarget_names = ['class 0', 'class 1', 'class 2']\nprint(classification_report(y_test, y_pred, target_names=target_names))","07ee93a3":"Pegando as palavras que mais aparecem em cada classe","e31322f7":"## Cleaning the texts","67b43f47":"# Importing the dataset","ed7b80d6":"0 - Discurso de \u00f3dio\n\n1 - Linguagem ofensiva\n\n2 - nenhum dos dois","cf698499":"## Analisando melhor as palavras de cada classe","af208aa2":"Treinando agora os modelos com estes dados:","7ac00bef":"### Previs\u00e3o dos dados de teste","be9dd9fc":"> ### UNDERSAMPLING\n\nVemos que os algoritmos ainda continuam confundindo bastante hate speech (0) com offensive language(1). Vamos tentar melhorar o problema de balanceamento desses dados.","4bdf78f8":"## Extraindo as features utilizando tokeniza\u00e7\u00e3o","9d3a8529":"### OVERSAMPLING","119e0945":"# Making the Confusion Matrix","edfb57c9":"## Treinando Logistic Regression","43342213":"Com a t\u00e9cnica de undersampling","a1e491e2":"Aumentar as amostras para que ficassem balanceadas resultou em resultados mais balanceados, aumentando a precis\u00e3o de 0.45 na classe 0 para 0.86. Conseguimos um f1-score geral inferior se comparado ao treinamento sem aumenta\u00e7\u00e3o de dados (obtivemos 0.88, comparados aos 0.90 obtidos anteriormente), mas o balanceamento dos resultados nos levou a dar prefer\u00eancia a essa solu\u00e7\u00e3o. Escolhemos ela ent\u00e3o para servir de modelo para rodar o conjunto de teste. generaliza bem o problema. O ideal seria aumentar as amostras com certa vari\u00e2ncia entre elas.","0e07f8c6":"# Importing the libraries"}}