{"cell_type":{"5985a0b6":"code","b71b9f07":"code","6d554ee0":"code","1348cbf3":"code","ad1193eb":"code","0e3c5b58":"code","2cc2a709":"code","56805272":"code","1d04e1d3":"code","ef31f6c5":"code","387b4df6":"code","95efcd34":"code","4f467739":"code","757c10c6":"code","7321a8ea":"code","cc8e30d1":"code","176debbe":"code","ab682b85":"code","fe0284d4":"code","76959898":"code","180a009a":"code","c6266ac1":"code","e134fb1c":"code","73f8e717":"code","feede131":"code","a63eee10":"code","f111a997":"code","8557a581":"code","ddc86865":"code","deb3e4b3":"code","61412201":"code","c324bbdc":"code","09c36609":"code","b00b9dda":"code","fac3c350":"code","d2c6cabb":"code","11d9e6b5":"code","de65ff94":"code","82081c51":"code","26dd9056":"code","0ee42b16":"code","5761323d":"code","6d57f151":"code","9d2e0093":"code","ab3e6796":"code","9742e729":"code","7ee95841":"code","bf528009":"code","c2ee263f":"code","81eea185":"code","95db20b2":"code","912b6215":"code","9623ebc7":"code","13436622":"code","e3621723":"code","a032a92a":"code","20291e9b":"code","fe5cdafa":"code","cd83cdce":"code","9e31c049":"code","c79aee39":"code","70ca9f2f":"code","d87a4b55":"code","7d20236d":"code","432c59d0":"code","8b6fe6d9":"code","fe5d672b":"code","bc69b36b":"code","d61ebaab":"code","cfe39d4c":"code","78e9e267":"code","80176024":"code","d6f59eda":"code","1b1c8d01":"code","7286869c":"code","4011baee":"code","659da8f0":"code","3adb10c6":"code","54108915":"code","fe6ccf33":"code","7bda9350":"code","0dee87b1":"code","880148f8":"code","9c9a70a8":"code","618fd04d":"code","7138105f":"code","a4e014dc":"code","c5f66db7":"code","1db1577e":"code","849533eb":"code","0cc7a408":"code","c5de2551":"code","0ec9df6f":"code","624b41d0":"code","7cf648a8":"code","ba2d9681":"code","23c6d09d":"code","4c081d22":"code","37345a12":"code","9d7dcb5d":"code","e3a0a675":"code","e48762ec":"markdown","693e144b":"markdown","fa9f710d":"markdown","6db798e8":"markdown","755a64fa":"markdown","61499ecf":"markdown","89f049f4":"markdown","d6f7b07a":"markdown","ad0eee76":"markdown","abb9fcbc":"markdown","0c7bff9c":"markdown","a7eb875b":"markdown","19201480":"markdown","4b2f96ae":"markdown","50164363":"markdown","69be88ea":"markdown","51cbb60e":"markdown","9865dc51":"markdown","1659e9ed":"markdown","40ca2abc":"markdown","4e0009b5":"markdown","d2b95b17":"markdown","e0b0517d":"markdown","a6ef3666":"markdown"},"source":{"5985a0b6":"# import cv2\n# import matplotlib.pyplot as plt\nimport glob\nimport os\nfrom tqdm import tqdm\n# %matplotlib inline","b71b9f07":"print(os.listdir(\"..\/input\"))","6d554ee0":"import cv2\nimport matplotlib.pyplot as plt\nimport numpy as np\n%matplotlib inline","1348cbf3":"full = cv2.imread('..\/input\/CV Object Detecton Dataset\/gorilla.jpg')#LOADING THE IMAGE of SOURCE IMAGE\nfull = cv2.cvtColor(full,cv2.COLOR_BGR2RGB)#CHANGING THE COLOR SPACE FROM BGR TO RGB\n\npart = cv2.imread('..\/input\/CV Object Detecton Dataset\/gorilla_shorted.jpg')#LOADING THE IMAGE OF TEMPLATE IMAGE\npart = cv2.cvtColor(part,cv2.COLOR_BGR2RGB)#CHANGING THE COLOR SPACE FROM BGR TO RGB","ad1193eb":"full.shape[::-1]","0e3c5b58":"plt.imshow(full)#SOURCE IMAGE","2cc2a709":"plt.imshow(part)#TEMPLATE IMAGE","56805272":"'''\n\nDIFFERENT METHODS AVAILABLE IN OPENCV 1ST IS COEFFICIENT BASED 3RD IS CORRELATION BASED \n5TH IS DIFFERENCE BASED AND 2ND,4TH AND 6TH ARE NORMALISED VERSION OF MENTIONED METHODS.\n\nAS, 3RD METHOD IS DIFFERENCE BASED SO IT WILL GIVE THE REGION WHERE IT \nWILL FIND THE DIFFERENCE BETWEEN THE SOURCE AND TEMPLATE IS LESS.\n\nSO,IT IS DIFFERENT THAN OTHE METHODS  \n\n'''\n\nmethods = ['cv2.TM_CCOEFF','cv2.TM_CCOEFF_NORMED','cv2.TM_CCORR','cv2.TM_CCORR_NORMED','cv2.TM_SQDIFF','cv2.TM_SQDIFF_NORMED']","1d04e1d3":"for i in methods:\n    func = eval(i)# eval() makes any string to a function if it is present in th the module\n    full_img = full.copy()\n    \n    res = cv2.matchTemplate(full_img,part,func)# THIS IS THE FUNCTION THAT WE USE TO DO THAT\n    '''\n    cv2.matchTemplate(img_gray,template,cv2.TM_CCOEFF_NORMED) the first parameter is the mainimage, \n    second parameter is the template to be matched and third parameter is the method used for matching.\n    \n    '''\n    \n    min_val,max_val,min_loc,max_loc = cv2.minMaxLoc(res)#OUTPUT OF 'cv2.matchTemplate()' CONTAINS THE MIN,MAX VALUE AND THEIR LOCATION\n    \n    if i in ['cv2.TM_SQDIFF','cv2.TM_SQDIFF_NORMED']:\n        '''\n        AS IT IS A DIFFERENCE BASED METHOD\n        '''\n        top_left = min_loc\n        height,width,channel = part.shape\n        bot_right = (top_left[0]+width,top_left[1]+height)\n    elif i in ['cv2.TM_CCORR']:\n        '''\n        IT WAS NOT SHOWING THE DESIRED OUTPUT THAT'S WHY I CUSTOMISED THAT IT IDENTIFY THE TEMPLATE IN SOURCE\n        '''\n        top_left = tuple(map(lambda i, j: i - j, min_loc, (-500,300)))\n        height,width,channel = part.shape\n        bot_right = (top_left[0]+width+200,top_left[1]+height+100)\n    else:\n        top_left = max_loc\n        height,width,channel = part.shape\n        bot_right = (top_left[0]+width,top_left[1]+height)\n        \n#     height,width,channel = part.shape\n#     bot_right = (top_left[0]+width,top_left[1]+height)\n    cv2.rectangle(full_img,top_left,bot_right,(255,0,0),25)#DRAWING RECTANGLE ON THE INDENTIFIED PORTION\n    font = cv2.FONT_HERSHEY_SIMPLEX#PUTTING TEXT TO SHOW WHICH METHOD IS USED\n    cv2.putText(full_img,text=i,org = (1400,700),fontFace=font,fontScale = 7,color = (255,0,0),thickness=25,lineType = cv2.LINE_AA)\n    \n    '''\n    SHOWING THE OUTPUT\ud83d\udc4d\n    \n    '''\n    plt.figure(figsize=(12,10))\n    plt.subplot(121)\n    plt.imshow(res)\n    plt.title('heatmap')\n    \n    plt.subplot(122)\n    plt.imshow(full_img)\n    plt.title('detection')\n    \n#     plt.suptitle(i)\n    plt.show()","ef31f6c5":"full = cv2.imread('..\/input\/CV Object Detecton Dataset\/gorilla.jpg')\nfull = cv2.cvtColor(full,cv2.COLOR_BGR2RGB)\n\npart = cv2.imread('..\/input\/CV Object Detecton Dataset\/gorilla_shorted.jpg')\npart = cv2.cvtColor(part,cv2.COLOR_BGR2RGB)","387b4df6":"full_img = full.copy()\n\nres = cv2.matchTemplate(full_img,part,cv2.TM_CCORR)\nmin_val,max_val,min_loc,max_loc = cv2.minMaxLoc(res)\ntop_left = tuple(map(lambda i, j: i - j, min_loc, (-500,300)))\nheight,width,channel = part.shape\nbot_right = (top_left[0]+width+200,top_left[1]+height+100)\ncv2.rectangle(full_img,top_left,bot_right,(255,0,0),25)\nfont = cv2.FONT_HERSHEY_SIMPLEX\ncv2.putText(full_img,text='TM_CCORR',org = (3000,500),fontFace=font,fontScale = 10,color = (255,0,0),thickness=25,lineType = cv2.LINE_AA)\n\nplt.figure(figsize=(12,10))\nplt.subplot(121)\nplt.imshow(res)\nplt.title('heatmap')\n\n# plt.figure(figsize=(8,8))   \nplt.subplot(122)\nplt.imshow(full_img)\nplt.title('detection')\n    \n# plt.suptitle('TM_CCOEFF')\nplt.show()","95efcd34":"img_rgb = cv2.imread('..\/input\/CV Object Detecton Dataset\/source1.jfif')\nimg_gray = cv2.cvtColor(img_rgb, cv2.COLOR_BGR2GRAY)\ntemplate = cv2.imread('..\/input\/CV Object Detecton Dataset\/template1.jfif',0)\nw, h = template.shape[::-1]\n\nres = cv2.matchTemplate(img_gray,template,cv2.TM_CCOEFF_NORMED)\nthreshold = 0.42# THIS IS THE THRESHOLD FOR WHICH WE GOT EVERY COIN WHICH MATCHES THE TEMPLATE\nloc = np.where( res >= threshold)\nfor pt in zip(*loc[::-1]):\n    cv2.rectangle(img_rgb, pt, (pt[0] + w, pt[1] + h), (0,0,255), 10)\n\nplt.imshow(img_rgb)","4f467739":"template1 = cv2.imread('..\/input\/CV Object Detecton Dataset\/template1.jfif')\nsourcee = cv2.imread('..\/input\/CV Object Detecton Dataset\/source1.jfif')\nsourcee.shape","757c10c6":"template1 = cv2.resize(template1,(1600,1200))\ntemplate1.shape","7321a8ea":"\nmultiple_img = np.concatenate((img_rgb, template1), axis=1)\nsourcee = cv2.resize(sourcee,(3200,1200))\nmultiple_img1 = np.concatenate((multiple_img,sourcee),axis=1)\n# concat = np.concatenate((img_rgb,template),axis=1)\nplt.figure(figsize=(12,14))\nplt.imshow(multiple_img1)\nplt.show()","cc8e30d1":"import cv2\nimport matplotlib.pyplot as plt\nimport numpy as np\n%matplotlib inline","176debbe":"#STEP1: LOADING IMAGES\nches = cv2.imread('..\/input\/CV Object Detecton Dataset\/chess1.png')\nches = cv2.cvtColor(ches,cv2.COLOR_BGR2RGB)\n\nwing = cv2.imread('..\/input\/CV Object Detecton Dataset\/wing_2.jfif')\nwing = cv2.cvtColor(wing,cv2.COLOR_BGR2RGB)\n\n","ab682b85":"plt.imshow(ches)","fe0284d4":"gray = cv2.cvtColor(ches,cv2.COLOR_RGB2GRAY)#STEP2: CONVERTING TO GRAY SCALE","76959898":"plt.imshow(gray,cmap='gray')","180a009a":"gray.shape","c6266ac1":"gray_img = np.float32(gray)#STEP3: IT IS A IMPORTANT STEP WHERE WE ARE CONVERTING THE GRAY SCALED IMAGE IN FLOATING TYPE\ndst = cv2.cornerHarris(gray_img,2,3,0.04)\n'''\nSTEP4: USING CORNERHARRIS ALGORITHUM \n1ST ARGUMENT: Input Image,\n2ND ARGUMENT: Neighborhood size,\n3RD ARGUMENT: Aperture parameter for the Sobel() operator\n4TH ARGUMENT: VALUE OF 'k'\n\nYOU CAN PLAY AROUND THE 2ND TO 3RD ARGUMENTS TO GET YOUR DESIRED OUTPUT\n'''","e134fb1c":"dst = cv2.dilate(dst,None)#YOU HAVE TO DILATE THE OUTPUT OF cv2.cornerHarris() TO VISUALIZE","73f8e717":"# THRESHOLD IS APPLIED WHICH IS 1% OF THE MAX VALUE OF dst, THOSE POINTS\n#WHICH ARE GREATER THAN THAT IS ASSIGNED THE VALUE RED, SO WE CAN VIEW THE POINTS\nches[dst>0.01*dst.max()] = [255,0,0]","feede131":"plt.imshow(ches)","a63eee10":"plt.imshow(wing)","f111a997":"wing_gray = cv2.cvtColor(wing,cv2.COLOR_RGB2GRAY)","8557a581":"wing_gray.shape","ddc86865":"plt.imshow(wing_gray,cmap = 'gray')","deb3e4b3":"wing_gray_img = np.float32(wing_gray)","61412201":"dst1 = cv2.cornerHarris(src=wing_gray_img,blockSize = 2,ksize=3,k=0.04)","c324bbdc":"dst1 = cv2.dilate(dst1,None)","09c36609":"wing[dst1>0.01*dst1.max()] = [255,0,0]","b00b9dda":"plt.imshow(wing)","fac3c350":"#LOADING THE IMAGES\nches = cv2.imread('..\/input\/CV Object Detecton Dataset\/chess1.png')\nches = cv2.cvtColor(ches,cv2.COLOR_BGR2RGB)\n\nwing = cv2.imread('..\/input\/CV Object Detecton Dataset\/wing_2.jfif')\nwing = cv2.cvtColor(wing,cv2.COLOR_BGR2RGB)\n\ngray = cv2.cvtColor(ches,cv2.COLOR_RGB2GRAY)\nwing_gray = cv2.cvtColor(wing,cv2.COLOR_RGB2GRAY)","d2c6cabb":"corners = cv2.goodFeaturesToTrack(gray,64,0.01,10)# APPLING THE ALGORITHM WICH IS NAMED BY THE NAME OF THE PAPER ITSELF","11d9e6b5":"corners = np.int0(corners)","de65ff94":"#EXTRACTING THE POINTS \nfor i in corners:\n    x,y = i.ravel()\n    cv2.circle(ches,(x,y),3,(255,0,0),-1)# DRAWING CIRCLE USING THOSE POINTS","82081c51":"plt.imshow(ches)","26dd9056":"corners1 = cv2.goodFeaturesToTrack(wing_gray,90,0.01,10)","0ee42b16":"corners1 = np.int0(corners1)","5761323d":"for m in corners1:\n    x1,y1 = m.ravel()\n    cv2.circle(wing,(x1,y1),3,(0,255,0),-1)","6d57f151":"plt.imshow(wing)","9d2e0093":"#LOADING AND SHOWING IMAGE\nedge_img = cv2.imread('..\/input\/CV Object Detecton Dataset\/chess1.png')\nplt.imshow(edge_img)","ab3e6796":"edge = cv2.Canny(edge_img,0,255)#APPLYING THE ALGORITHM","9742e729":"plt.imshow(edge)#SHOWING THE IMAGE","7ee95841":"med_val = np.median(edge_img)","bf528009":"upper = int(min(255,1.3*med_val))#ABOVE MENTIONED FORMULA FOR CLACULATING UPPER THRESHOLD\nlower = int(min(0,0.7*med_val))#ABOVE MENTIONED FORMULA FOR CLACULATING LOWER THRESHOLD","c2ee263f":"blurred_img = cv2.blur(edge_img,ksize = (5,5))#BLURING THE IMAGE","81eea185":"edge = cv2.Canny(blurred_img,lower,upper+60)# APPLYING THE ALGO","95db20b2":"plt.imshow(edge)#WE HAVE NOT GOT THE DESIRED OUTPUT BECAUSE THERE IS NO NOISE IN THE IMAGE.","912b6215":"edge = cv2.Canny(edge_img,lower,upper+450)","9623ebc7":"plt.imshow(edge)#BETTER OUTPUT","13436622":"edge_img1 = cv2.imread('..\/input\/CV Object Detecton Dataset\/SharedScreenshot1.jpg')\nplt.imshow(edge_img1)","e3621723":"med_val1 = np.median(edge_img1)","a032a92a":"upper1 = int(min(255,1.3*med_val1))\nlower1 = int(min(0,0.7*med_val1))","20291e9b":"blurred_img1 = cv2.blur(edge_img1,ksize = (5,5))","fe5cdafa":"edge1 = cv2.Canny(blurred_img1,lower1,upper1+60)","cd83cdce":"plt.imshow(edge1)# CAN BE TUNED","9e31c049":"edge_not_blur = cv2.Canny(edge_img1,upper1+130,lower1+60)","c79aee39":"plt.imshow(edge_not_blur)#ALMOST DESIRED OUTPUT","70ca9f2f":"lower1","d87a4b55":"# grid1 = cv2.imread('left14.jpg')\n# gray1 = cv2.cvtColor(grid1,cv2.COLOR_BGR2GRAY)\n\n# objp = np.zeros((6*7,3), np.float32)\n# objp[:,:2] = np.mgrid[0:7,0:6].T.reshape(-1,2)\n\n# criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 30, 0.001)\n\n# objpoints1 = []\n# imgpoints1 = []\n# ret1,cor1 = cv2.findChessboardCorners(grid1,(7,7))\n# if ret1 == True:\n#         objpoints1.append(objp)\n\n#         corners2 = cv2.cornerSubPix(gray1,cor1,(11,11),(-1,-1),criteria)\n#         imgpoints1.append(corners2)\n\n#         # Draw and display the corners\n#         grid_copy1 = grid1.copy()\n#         grid_copy1 = cv2.drawChessboardCorners(grid_copy1, (7,6), corners2,ret1)\n# # grid_copy = grid.copy()\n# # cv2.drawChessboardCorners(grid_copy,(7,7),cor,found)\n#         plt.imshow(grid_copy1)\n","7d20236d":"import cv2\nimport matplotlib.pyplot as plt\nimport numpy as np\n%matplotlib inline","432c59d0":"grid = cv2.imread('..\/input\/CV Object Detecton Dataset\/flat_board.png')#LOADING IMAGE","8b6fe6d9":"type(grid)","fe5d672b":"plt.imshow(grid)","bc69b36b":"found,cor = cv2.findChessboardCorners(grid,(7,7))#APPLYING THE ALGO","d61ebaab":"found","cfe39d4c":"grid_copy = grid.copy()\ncv2.drawChessboardCorners(grid_copy,(7,7),cor,found)#DRAWING THE CORNERS ON THE COPY OF THE IMAGE","78e9e267":"plt.imshow(grid_copy)#SHOWING THE IMAGE","80176024":"dot = cv2.imread('..\/input\/CV Object Detecton Dataset\/dot_grid.png')#loading the image","d6f59eda":"type(dot)","1b1c8d01":"plt.imshow(dot)#showing the image","7286869c":"found1,cor1 = cv2.findCirclesGrid(dot,(10,10),cv2.CALIB_CB_ASYMMETRIC_GRID)#APPLYING THE ALGO AS MENTIONED ABOVE","4011baee":"found1","659da8f0":"cv2.drawChessboardCorners(dot,(10,10),cor1,found1)# DRAWING CORNERS","3adb10c6":"plt.imshow(dot)#SHOWING THE OUTPUT","54108915":"import cv2\nimport matplotlib.pyplot as plt\nimport numpy as np\n%matplotlib inline","fe6ccf33":"image = cv2.imread(\"..\/input\/CV Object Detecton Dataset\/leaf.jfif\")\n","7bda9350":"type(image)","0dee87b1":"# convert to RGB\nimage = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n# convert to grayscale\ngray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)","880148f8":"# create a binary thresholded image\n_, binary = cv2.threshold(gray, 225, 255, cv2.THRESH_BINARY_INV)\n# show it\nplt.imshow(binary, cmap=\"gray\")\nplt.show()","9c9a70a8":"# find the contours from the thresholded image\ncontours, hierarchy = cv2.findContours(binary, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n# draw all contours\nimage = cv2.drawContours(image, contours, -1, (0, 255, 0), 2)","618fd04d":"# show the image with the drawn contours\nplt.imshow(image)\nplt.show()","7138105f":"#LOADING IMAGE\nfurier = cv2.imread('..\/input\/CV Object Detecton Dataset\/a_0.png',cv2.IMREAD_GRAYSCALE)\nf = np.fft.fft2(furier)#MAIN STEP1 PASSING THE IMAGE TO fast Fourier Transform function (FFT)\nfshift = np.fft.fftshift(f)# MAIN STEP2 PASSING THE OUTPUT OF FFT IN np.fft.fftshift()\nmagnitude_spectrum = 20*np.log(np.abs(fshift))#MAIN STEP3, THESE THREE STEPS WILL BE SAME FOR EVERYTIME\nmagnitude_spectrum = np.asarray(magnitude_spectrum, dtype=np.uint8)\nimg_and_magnitude = np.concatenate((furier, magnitude_spectrum), axis=1)","a4e014dc":"#SHOWING THE OUTPUT\nplt.figure(figsize=(12,8))\nplt.imshow(img_and_magnitude,cmap='gray')\nplt.show()","c5f66db7":"furier = cv2.imread('..\/input\/CV Object Detecton Dataset\/a_1.png',cv2.IMREAD_GRAYSCALE)\nf = np.fft.fft2(furier)\nfshift = np.fft.fftshift(f)\nmagnitude_spectrum = 20*np.log(np.abs(fshift))\nmagnitude_spectrum = np.asarray(magnitude_spectrum, dtype=np.uint8)\nimg_and_magnitude = np.concatenate((furier, magnitude_spectrum), axis=1)\n\nplt.figure(figsize=(12,8))\nplt.imshow(img_and_magnitude,cmap='gray')\nplt.show()","1db1577e":"furier = cv2.imread('..\/input\/CV Object Detecton Dataset\/c_0.png',cv2.IMREAD_GRAYSCALE)\nf = np.fft.fft2(furier)\nfshift = np.fft.fftshift(f)\nmagnitude_spectrum = 20*np.log(np.abs(fshift))\nmagnitude_spectrum = np.asarray(magnitude_spectrum, dtype=np.uint8)\nimg_and_magnitude = np.concatenate((furier, magnitude_spectrum), axis=1)\n\nplt.figure(figsize=(12,8))\nplt.imshow(img_and_magnitude,cmap='gray')\nplt.show()","849533eb":"furier = cv2.imread('..\/input\/CV Object Detecton Dataset\/c_1.png',cv2.IMREAD_GRAYSCALE)\nf = np.fft.fft2(furier)#MAIN STEP1 PASSING THE IMAGE TO fast Fourier Transform function (FFT)\nfshift = np.fft.fftshift(f)# MAIN STEP2 PASSING THE OUTPUT OF FFT IN np.fft.fftshift()\nmagnitude_spectrum = 20*np.log(np.abs(fshift))#MAIN STEP3, THESE THREE STEPS WILL BE SAME FOR EVERYTIME\nmagnitude_spectrum = np.asarray(magnitude_spectrum, dtype=np.uint8)\nimg_and_magnitude = np.concatenate((furier, magnitude_spectrum), axis=1)\n\n#SHOWING OUTPUT\nplt.figure(figsize=(12,8))\nplt.imshow(img_and_magnitude,cmap='gray')\nplt.show()","0cc7a408":"from IPython.display import YouTubeVideo\nYouTubeVideo('nVbaNcRldmw')\n# https:\/\/youtu.be\/nVbaNcRldmw","c5de2551":"#LOADING THE IMAGE\n\nnadia = cv2.imread('..\/input\/CV Object Detecton Dataset\/Nadia.jpg')\nface_color1 = cv2.cvtColor(nadia,cv2.COLOR_BGR2RGB)\ngray = cv2.cvtColor(nadia,cv2.COLOR_BGR2GRAY)\n# blurrr = cv2.GaussianBlur(gray,(35,35),0)\nplt.imshow(face_color1)\n","0ec9df6f":"# Read in the cascade classifiers for face and eyes \n\ndog_face = cv2.CascadeClassifier('..\/input\/CV Object Detecton Dataset\/haarcascade_frontalface_default.xml')\neye = cv2.CascadeClassifier('..\/input\/CV Object Detecton Dataset\/haarcascade_eye.xml')","624b41d0":"# create a function to detect face AND DRAWING RECTANGLE AROUND THE FACE\n\ndef detect_face(img):\n    face_img = img.copy()\n    face_gray = cv2.cvtColor(face_img,cv2.COLOR_BGR2GRAY)\n    face_rects = dog_face.detectMultiScale(face_img)\n    \n    for(x,y,w,h) in face_rects:\n        cv2.rectangle(face_img,(x,y),(x+w,y+h),(255,255,255),10)\n        roi_gray = face_gray[y:y+h,x:x+w]\n        roi_color = face_img[y:y+h,x:x+w]\n        eye_rects = eye.detectMultiScale(roi_gray)\n        for(x1,y1,w1,h1) in eye_rects:\n            cv2.rectangle(roi_color,(x1,y1),(x1+w1,y1+h1),(255,255,255),10)\n        \n        return face_img","7cf648a8":"re = detect_face(face_color1)","ba2d9681":"#SHOWING THE OUTPUT\nplt.figure(figsize=(12,8))\nplt.imshow(re)\nplt.show()","23c6d09d":"#LOADING THE IMAGE\nface = cv2.imread('..\/input\/CV Object Detecton Dataset\/solvay.jpg')\nfece_color = cv2.cvtColor(face,cv2.COLOR_BGR2RGB)\ngray1 = cv2.cvtColor(face,cv2.COLOR_BGR2GRAY)\n# blurrr = cv2.GaussianBlur(gray,(35,35),0)\n# plt.imshow(gray,cmap='gray')\n","4c081d22":"# Read in the cascade classifiers for face and eyes \n\nface_cascade = cv2.CascadeClassifier('..\/input\/CV Object Detecton Dataset\/haarcascade_frontalface_default.xml')\neye_cascade = cv2.CascadeClassifier('..\/input\/CV Object Detecton Dataset\/haarcascade_eye.xml')","37345a12":"face1 = face_cascade.detectMultiScale(gray1,1.3,5)#FACE COORDINATES ARE CALCULATED USING \"face_cascade.detectMultiScale()\"\n#DRAWING THE RECTANGLE FOR FACE AND EYES\nfor (x,y,w,h) in face1:\n    cv2.rectangle(fece_color,(x,y),(x+w,y+h),(255,0,0),10)\n    roi_gray1 = gray1[y:y+h,x:x+w]\n    roi_color1 = fece_color[y:y+h,x:x+w]\n    eyes1 = eye_cascade.detectMultiScale(roi_gray1)\n    for (ex,ey,ew,eh) in eyes1:\n        cv2.rectangle(roi_color1,(ex,ey),(ex+ew,ey+eh),(0,255,0),10)","9d7dcb5d":"#SHOWING THE OUTPUT\nplt.figure(figsize=(12,8))\nplt.imshow(fece_color)\nplt.show()","e3a0a675":"print('Faces Found:{}'.format(len(face1)))# NUMBER OF FACES FOUND","e48762ec":"### For Wing Image","693e144b":"### Corner detection part-II & Camera Calibration:\nThere are some built-in functions in OpenCV that helps to find corners even when the image is distorted due to different reasons. The most happening distortions are radial distortion and tangential distortion if you remember these were mentioned in our 12th physics books in optics chapters. The whole thing is part of camera calibration. Camera calibration means mapping or transformation of the 3D points(points are related to an object, whose pic we are taking) in the 2D coordinate. And as these translations are not perfect enough that's why we get those errors or distortions.\n\n<center><img src=\"https:\/\/i.imgur.com\/9AyVpqO.png\" width=\"400px\"><\/center>\n\n1. **Radial Distortion:** \n    Radial Distortion is the most common type that affects the images, In which when a camera captured pictures of straight lines appeared slightly curved or bent.\n\n<center><img src=\"https:\/\/i.imgur.com\/TIXWO3Q.png\" width=\"400px\"><\/center>\n\n2. **Tangential distortion:** \n    Tangential distortion occurs mainly because the lens is not parallelly aligned to the imaging plane, which makes the image to be extended a little while longer or tilted, it makes the objects appear farther away or even closer than they actually are.\n\n<center><img src=\"https:\/\/i.imgur.com\/Dg34LU4.png\" width=\"400px\"><\/center>\n\nSo, to remove this sort of distortions we need to find distortion parameters and intrinsic and extrinsic parameters of a camera. And removing distortion must be our first step before going ahead with any kind of processing or analysis of the image. To find all these parameters and remove distortion, what we have to do is to provide some sample images of a well-defined pattern (eg, chessboard). We find some specific points in it (square corners in chessboard). We know its coordinates in real-world space and we know its coordinates in the image. With these data, some mathematical problem is solved in the background to get the distortion coefficients. That is the summary of the whole story. For better results, we need at least 10 test patterns.\n\n<center><img src=\"https:\/\/i.imgur.com\/E2yi0jU.png\" width=\"400px\"><\/center>\n* \n\nSo, to find the pattern in a chessboard we need to use the builtin function of OpenCV called  `cv2.findChessboardCorners()`. we pass the arguments like-the image where we want to find the patterns, what is the size of the pattern like 8x8 or7x7 etc. and corners Output array of detected corners(you can pass it None or leave that as it is if you don't know). Instead of the chessboard, we can use some circular grid but then use the function `cv2.findCirclesGrid()` to find the pattern. It is said that fewer images are enough when using a circular grid. use `Shift+Tab` to know about the arguments.\n\nI tried `cv2.cornerSubPix()` for more accuracy as per the documentation but I could not obtain the desired output. If you can comment down below.\n\n#### Important Links & Image Source:\n1. https:\/\/medium.com\/analytics-vidhya\/camera-calibration-with-opencv-f324679c6eb7\n2. https:\/\/opencv-python-tutroals.readthedocs.io\/en\/latest\/py_tutorials\/py_calib3d\/py_calibration\/py_calibration.html","fa9f710d":"## Harris Corner Detection","6db798e8":"### With Blur","755a64fa":"### Contour Detection: \n Contour detection is a useful tool for shape analysis, object detection, and recognition. There are so many functions for doing contour detection, but I have done  Contour Detection in a getting started perspective.\n \n <center><img src=\"https:\/\/i.imgur.com\/PHaWsPa.jpg\" width=\"500px\"><\/center>\n \n#### What is Contour?\nContours are defined as simply a curve joining all the continuous points(along the boundary), having the same color or intensity. In simple it means a closed curve.I used `cv2.findContours()` for finding contours and `cv2.drawContours()` for drawing the contours.\n#### Guide:\n1. `cv2.findContours()` Finds contours in a binary image so, before passing any image directly one should apply thresholding or canny edge detection on it.\n2. Try to use a copy of the original image as `cv2.findContours()` modifies the input image.\n3. As the 2nd argument, we can use  `cv2.RETR_LIST`,`cv2.RETR_TREE` or `cv2.RETR_EXTERNAL` any one of them. I tried the first two but both gave me approximately the same result but, I did not try the last one, I guess it will be the same too.\n4. Try to use `cv2.CHAIN_APPROX_NONE` it will give you the whole contour but `cv2.CHAIN_APPROX_SIMPLE` give only a few points of the respective contours by that way it saves memory too.\n5. If there is noise in the background feel free to remove them but, do not blur it so much it might affect the detection.\n\n#### Important Links:\n1. https:\/\/www.thepythoncode.com\/article\/contour-detection-opencv-python","61499ecf":"## Corner Detection:\n\nThere is basically two well-known corner detection algorithm that are present in OpenCV one is Harris Corner Detection and another one is Shi-Tomasi Corner Detection. To understand these algorithms we need to first understand, what is a **corner**?\n**A corner is a point whose local neighborhood stands in two dominant and different edge directions**.In simple words, Corner is the junction of two or more edges, where the edge is a sudden change in image brightness.\n### Basic Intuition behind Harris Corner Detection and Shi-Tomasi Corner Detection:\nBasically, both algorithm is same, moreover, shi-Thomas corner detection is the improved version of harris corner detection. Both algorithm based on the principle, which says that **if a window is placed on a corner of an image then the movement of the window in any direction will provide us large change in intensity**. So, in both algorithm there will be a kernel which will be sliding across the image and there will be a formula that will be calculating the change in intensity. \n\n<center><img src=\"https:\/\/i.imgur.com\/jI63N0J.png\" width=\"300px\"><\/center>\n\n> Image Source: https:\/\/www.southampton.ac.uk\/~msn\/book\/new_demo\/corners\/.\n\n#### Formula:\nFor a window(W) located at (X, Y) with pixel intensity I(X, Y), formula for both Corner Detection is \u2013\n> f(X, Y) = \u03a3 (I(Xk, Yk) - I(Xk + \u0394X, Yk + \u0394Y))2  where (Xk, Yk) \u03f5 W\n\nIf, both are using the same formula the where is the difference between them? As per the dates Harris Corner Detection got published on 1988 and J.Shi and C.Tomasi Corner Detection published their paper by the name 'Good Features to Track' in 1994. J.Shi and C.Tomasi made a small modification to the Harris Corner Detection which ended up with better results.\nThe modification was made on the scoring function-\n* Harris corner detectior uses:\n    * > R =  \u03bb1,\u03bb2 - k(\u03bb1 - \u03bb2)\n* Shi-Tomashi corner detector uses :\n    * > R = min(\u03bb1, \u03bb2)\n\nHere, where \u03bb1, \u03bb2 are eigenvalues of resultant matrix and the number detected can be altered by varying the value of k.\n\n#### Important Links:\n1.https:\/\/opencv-python-tutroals.readthedocs.io\/en\/latest\/py_tutorials\/py_feature2d\/py_features_harris\/py_features_harris.html\n2. https:\/\/opencv-python-tutroals.readthedocs.io\/en\/latest\/py_tutorials\/py_feature2d\/py_shi_tomasi\/py_shi_tomasi.html\n","89f049f4":"### Without Blurring","d6f7b07a":"**We need to dilate the output of 'cv2.cornerHarris()' function for visualizing the output!!!**","ad0eee76":"## Fourier Transform:\n#### What is Fourier Transform?:\nIn general Fourier Transform says that\n> Any periodic function can be represented as a weighted sum of sins and cosines.\n\nThe Fourier Transform is an important image processing tool which is used to decompose an image into its sine and cosine components. The output of the transformation represents the image in the Fourier or frequency domain, while the input image is the spatial domain equivalent. In the Fourier domain image, each point represents a particular frequency contained in the spatial domain image. It is very helpful for object detection, you can understand by the below code. You will find that every 'A','C' is following a special pattern when Fourier Transform is applied to it through both fonts of both 'A' and both 'C' is different.\n\n<center><img src=\"https:\/\/i.imgur.com\/qPA6X7x.jpg\" width=\"300px\"><\/center>\n\nThe Fourier Transform is used in a wide range of applications, such as image analysis, image filtering, image reconstruction, and image compression.\n\n\n* Spatial Domain:\n> This concept is used most often when discussing the frequency with which image values change, that is, over how many pixels does a cycle of periodically repeating intensity variations occur. One would refer to the number of pixels over which a pattern repeats (its periodicity) in the spatial domain.\n\n* Frequency Domain:\n>The frequency domain is a space in which each image value at image position F represents the amount that the intensity values in the image I vary over a specific distance related to F. In the frequency domain, changes in image position correspond to changes in the spatial frequency, (or the rate at which image intensity values) are changing in the spatial domain image I. For example, suppose that there is the value 20 at the point that represents the frequency 0.1 (or 1 period every 10 pixels). This means that in the corresponding spatial domain image I the intensity values vary from dark to light and back to dark over a distance of 10 pixels and that the contrast between the lightest and darkest is 40 gray levels (2 times 20).\n\n[Source](http:\/\/homepages.inf.ed.ac.uk\/rbf\/HIPR2\/fourier.htm)\n#### Important Links:\n1. https:\/\/opencv-python-tutroals.readthedocs.io\/en\/latest\/py_tutorials\/py_imgproc\/py_transforms\/py_fourier_transform\/py_fourier_transform.html\n2. https:\/\/dsp.stackexchange.com\/questions\/1637\/what-does-frequency-domain-denote-in-case-of-images","abb9fcbc":"### Loading Images:\n","0c7bff9c":"\nThere are different Haar Features for detecting different face features, like edge features can be used for detecting eyebrows, lips, etc. then line features can be used for detecting nose, eyes etc.see the image below,\n<center><img src=\"https:\/\/i.imgur.com\/l3EGRWh.jpg\" width=\"300px\"><\/center>> But among all these features we calculated, most of them are irrelevant. For example, consider the image below. The top row shows two good features. The first feature selected seems to focus on the property that the region of the eyes is often darker than the region of the nose and cheeks. The second feature selected relies on the property that the eyes are darker than the bridge of the nose. But the same windows applying on cheeks or any other place is irrelevant. So how do we select the best features out of 160000+ features? It is achieved by Adaboost.\n<center><img src=\"https:\/\/i.imgur.com\/QhkUDxY.png\" width=\"300px\"><\/center>\n#### Different Steps of the Algorithm:\n1. It needs an image with a suspected object\n2. It will make the image grayscale because we need to use Haar Features.\n3. It will start with any one of the  Haar Features for detecting a specific region and after it finishes then, another feature will start detecting a specific region and it goes up to 6K features. That kernel will slide around the whole image and calculate \u0394. if the value of it is close to 1 then the algorithm will conform to presence of that specific features.\n\n<center><img src=\"https:\/\/i.imgur.com\/Vb6QAdl.png\" width=\"300px\"><\/center>\n\n\n4. For reducing the complexity of the operations from O(N^2) it uses the Integral image concept which takes the complexity factor to O(1).\n\n<left><img src=\"https:\/\/i.imgur.com\/gTDIStk.png\" width=\"300px\"><\/center>\n<right><img src=\"https:\/\/i.imgur.com\/3mgdWtg.png\" width=\"300px\"><\/center>\n\n5. The final classifier is a weighted sum of these weak classifiers. It is called weak because it alone can\u2019t classify the image, but together with others forms a strong classifier. The paper says even 200 features provide detection with 95% accuracy. Their final setup had around 6000 features. (Imagine a reduction from 160000+ features to 6000 features. That is a big gain).\n\nI am gonna write an article addressing how to create a Haar cascade classifier Using the famous GUI tool `Cascade Trainer GUI`. It is very easy to use but most of the cases people get error like- `OpenCV Error : Bad argument(Can not get new positive sample.The most possible reason is insufficient count of samples in given vec - file.) in CvCascadeImageReader::PosReader::get,file\\path_to_opencv\\apps\\traincascade\\imagestorage.cpp, line X` . So, I am gonna address these sorts of errors and how to tune the parameters available there.\n\n#### Important Links:\n1. https:\/\/opencv-python-tutroals.readthedocs.io\/en\/latest\/py_tutorials\/py_objdetect\/py_face_detection\/py_face_detection.html\n2. https:\/\/medium.com\/@krsatyam1996\/haar-cascade-face-identification-aa4b8bc79478\n","a7eb875b":"### Using Dot_Grid image:","19201480":"### Using pice of paper image","4b2f96ae":"## Canny Edge detection:\nThis algorithm is one of the most popular edge detection algorithums available in the computer vision community.Except that there are some gradient based algorithm Sobel-Feldman, Laplacian which helps us to detect edges.\n#### There are some specific steps involved in the algorithm:\n1. Apply gaussian filter to smooth the image in order toremove the noise.\n2. Finding Intensity Gradient of the Image.specificlly we try to find both the value of the gradient and the direction of the gradient(or the direction of change). To do that it uses sobel edge detector.After that, the main edge detection starts.Allways remember Gradient direction is always perpendicular to edges.\n3. Apply non-maximum suppression to get rid of spurious response to edge detection.The function of that step is same as it sounds. It checks wheather it is a local maximum in its neighborhood in the direction of gradient. After nonmaximum suppression, you'll get something called 'thin edges'.\n4. Track edge by hysteresis: Finalize the detection of edges by suppressing all the other edges that are weak and not connected to strong edges.See the image below,\n\n<center><img src=\"https:\/\/i.imgur.com\/IormJxR.png\" width=\"200px\"><\/center>\n\nThe edege 'A' is above the maxVal so it is a 'sure edge', though the edge 'B' is below maxVal it is connected to 'sure edge' so it is considered as a valid edge.But, as edge 'C' neither above maxVal nor connected to any 'sure edge' that's why it is considered as a 'weak edge'. So,by that we finally get the strong edges in the image.\n\n#### Guideline:\n1. OpenCV puts all the above in single function, cv2.Canny(). We will see how to use it. First argument is our input image. Second and third arguments are our minVal and maxVal respectively. Third argument is aperture_size. It is the size of Sobel kernel used for find image gradients. By default it is 3. Last argument is L2gradient which specifies the equation for finding gradient magnitude. If it is True, it uses the equation mentioned above which is more accurate, otherwise it uses this function: **Edge_Gradient(G) = |G_x| + |G_y|**. By default, it is False.\n2. For high resolution images where you only want general edges, it is usually a good idea to apply a custom blur.\n3. The canny algo. also requires a user to decide on low and high threshold values.There are some equestions mentioned in the code which helps to find that.\n4. In most of the cases we need to experiment with different values to get the desired output.\n\n#### Important Links:\n\n1. https:\/\/opencv-python-tutroals.readthedocs.io\/en\/latest\/py_tutorials\/py_imgproc\/py_canny\/py_canny.html\n","50164363":"### With Blurring","69be88ea":"## This Notebook is totally based on different Object Detection Techniques that we can impliment using opencv:\ud83d\udcbb\n\n### Table of Topic:\ud83d\ude0e\n\n1. Template Matching\n2. Corner Detection \n    * Harris Corner Detection\n    * Shi-Tomasi Corner Detection\n3. Grid Detection\n4. Edge detection\n5. Contour Detection\n6. Furier Transform\n7. Face Detection with OpenCV Haar Cascade\n","51cbb60e":"### Haar cascade face detection:\nHaar cascade is a kind of classifier which helps us to classify objects. This method of using a haar cascade classifier is proposed by Paul Viola and Michael Jones in their paper Rapid Object Detection using a Boosted Cascade of Simple Features. By using these we only can detect objects, we can't identify any particular object(like in case of face recognition, to whom that particular face belongs to).\n#### How it works?\nSo, what defines a human face? The answer would be its features and the features would be like- nose, eyes, lips, cheeks, eyebrows, etc. so, to detect face we need to detect these. And this is gonna happen with the help of Haar Features. Haar Features are a sequence of rescaled square-shaped functions. It was proposed by Alfred Haar in 1909 they are basically black and white kernels. They look like these,\n\n","9865dc51":"### Without Blurring","1659e9ed":"### Template Matching with Multiple Objects:\nAbove we searched for the gorilla image which appears in the source image once. But, when try to find the object that has multiple occurance in the source image then  **cv2.minMaxLoc()** will not give all the location of the object. In that case we have to use Thresholding.","40ca2abc":"### Shi-Tomasi Detection","4e0009b5":"#### Appling Corner Harris Algorithm on Wing.png image:","d2b95b17":"\n### SAME THINGS HAS BEEN DONE FOR USING A SINGLE METHOD FOR TEMPLATE MATCHING!!!\ud83d\ude0e\ud83d\udcbb\n","e0b0517d":"## 1. Template Matching:\nTemplate matching is one of the simplest forms of object detection. It is broadly used in the field of vehicle tracking, robotics, medical imaging, and manufacturing, etc. Using this process we find the image of the target object in a source image. \n\n<center><img src=\"https:\/\/i.imgur.com\/DhCFx61.png\" width=\"300px\"><\/center>\n\n\nIt is a very computationally cost since the matching process involves moving the template image to all possible positions in a source image. when the sliding process of the template image on the source image there is comparison measurement that is done by calculating the similarity or the distance between both the images.\n### Template Matching Measures:\nIn general, there are four methods that indicate the degree of similarity\n* Measures of Match (similarity)\n    1. Measures Based On Optimal Path Searching Techniques\n    2. Euclidean Distance\n    3. The Edit Distance\n    4. Measures Based On Correlations \n* Measures of Mismatch (dissimilarity)\n    1. Root mean square distance (RMS)\n    2. Sum of absolute differences (SAD)\nMost of the names are self-explanatory, if you don't feel so then it will be better if you look into [this link](https:\/\/www.slideshare.net\/mustafasalam167\/template-matching).\n\n### Problems with template matching \n1. If the template is the smaller or rotated or translated version of the target image that is present in the source image then it will identify the matches. In this cases, we will be using multi-scale template matching or feature-based template matching. [Pyimagesearch has a brilliant article on that](https:\/\/www.pyimagesearch.com\/2015\/01\/26\/multi-scale-template-matching-using-python-opencv\/).\n2. This technique requires a separate template for each scale and orientation.\n3. Template matching can be computationally expensive, especially for large templates.\n4. It is Sensitive to noise and occlusions .\n\n\nIn this technique what happens is that the smaller image slides itself and scans the bigger image for finding the object that we require. Here, the smaller image is the target image that contains the object that we require and the bigger image is the image where we find our target image. \n","a6ef3666":"### on face"}}