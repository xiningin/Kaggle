{"cell_type":{"420a5bc3":"code","7c7fdd7b":"code","9e7dd0d1":"code","e0d53e42":"code","cdc7aa74":"code","ec2adb07":"code","a56f9971":"code","2ec32fa5":"code","38fc8dd9":"code","c34a43b5":"code","0cacc387":"code","dcfc99d8":"code","7608c450":"code","a3014857":"code","f16eddb6":"code","fffe79e1":"code","fa13a0b3":"code","c5855126":"code","555bb256":"code","e2df1b06":"code","ea4d2bfd":"code","d1278ed9":"code","354cc11e":"code","b7b5a893":"code","4bb61d2c":"code","ed6ec801":"code","9b363bef":"code","f23aaaef":"code","889d65e8":"code","b0e9a54d":"code","7a93499c":"code","25cb5220":"code","8d924476":"code","d0a4e35c":"code","03e3e978":"code","920d5284":"code","4d5f71e9":"code","731174bd":"code","a37b50b9":"code","f381d3d7":"code","022b671b":"code","ece3df46":"code","a7fa7f87":"code","a296b9dc":"code","8e12e90f":"code","f4fa1d85":"code","a9f34c07":"code","7edfc316":"code","f891aa83":"code","39a4deeb":"code","53498492":"code","20cae814":"code","e571222e":"code","7974f8f2":"code","adce18b7":"code","7b298545":"code","d4256d78":"code","957af102":"code","dcc2cffe":"code","efea4080":"code","f4a26197":"code","531bbd73":"code","6b560835":"code","cc90980f":"code","8b0cea9b":"code","d0f16fd2":"code","cc02305a":"code","247afe9f":"code","7433bcd8":"code","17cafe87":"code","038b3e2c":"code","444495df":"code","b2b38707":"code","2958004d":"markdown","b45d2297":"markdown","0290b489":"markdown","f590b4ab":"markdown","6f450527":"markdown","4ce81999":"markdown","b939fde1":"markdown","64c578d6":"markdown","8992cf47":"markdown","67ed3c91":"markdown","8024e93e":"markdown","84dd0581":"markdown","5b8027e1":"markdown","b79ab3d2":"markdown","023c71f3":"markdown","9212b1c5":"markdown","f02211c3":"markdown","ed5f65fc":"markdown","247847b4":"markdown","51c567d3":"markdown","3d97d7aa":"markdown","d24bc3d5":"markdown","660e5085":"markdown","1f08203a":"markdown","ff622ff9":"markdown","bbacacf2":"markdown","21ad8337":"markdown","aa51dfa0":"markdown","8763d7dc":"markdown","8b9824c7":"markdown","38c6d8aa":"markdown","07dff45c":"markdown","adc141dd":"markdown","a2c657a1":"markdown","a884507d":"markdown","e076feaf":"markdown","d516557f":"markdown","5f78fe80":"markdown","a26a541c":"markdown","544ff3c4":"markdown","79452cdd":"markdown","d9943430":"markdown","fd6bc22f":"markdown","42756123":"markdown","8a359192":"markdown","5bfd1956":"markdown","e91d0e0a":"markdown","e287052f":"markdown"},"source":{"420a5bc3":"import warnings\nwarnings.filterwarnings(\"ignore\")","7c7fdd7b":"!pip install auto-sklearn==0.12.0\n!pip install scikit-learn==0.23.2","9e7dd0d1":"# algebra linear\nimport numpy as np \n# processamento de dados\nimport pandas as pd","e0d53e42":"# imprime os arquvios\nimport os\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","cdc7aa74":"df = pd.read_csv('\/kaggle\/input\/heart-disease-uci\/heart.csv')\ndf.sample(3)","ec2adb07":"# normalizador\nfrom sklearn import preprocessing","a56f9971":"# recupera os valores (X), e as classes (Y)\nX = df.drop('target', axis=1)\nY = df['target']","2ec32fa5":"min_max_scaler = preprocessing.MinMaxScaler()\nX = min_max_scaler.fit_transform(X)","38fc8dd9":"# visualiza\u00e7\u00e3o de dados\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# redutor de dimensionalidade\nfrom sklearn.decomposition import PCA","c34a43b5":"# redu\u00e7\u00e3o da dimensionalidade\npca = PCA(n_components=2).fit(X)\nX_reduced = pca.fit_transform(X)\n\n# registrando os valores num novo DataFrame\ndf_2d = pd.DataFrame(X_reduced)\ndf_2d.columns = ['0', '1']","0cacc387":"sns.scatterplot(data=df_2d, x='0', y='1')\nplt.show()","dcfc99d8":"sns.scatterplot(data=df_2d, x='0', y='1', hue=Y, style=Y)\nplt.show()","7608c450":"# m\u00e9tricas\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score","a3014857":"# vari\u00e1vel de resultado final\n# ser\u00e1 armazenado o resultado de todos experimentos\nexperiment = {}","f16eddb6":"# treinamento, test split\nfrom sklearn.model_selection import train_test_split","fffe79e1":"X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=26)","fa13a0b3":"print('treinamento:', len(y_train))\nprint('teste      :', len(y_test))","c5855126":"# classificador\nfrom sklearn.neighbors import KNeighborsClassifier","555bb256":"model1 = KNeighborsClassifier(n_neighbors=3)\nmodel1.fit(X_train, y_train)","e2df1b06":"y_pred = model1.predict(X_test)","ea4d2bfd":"acc = accuracy_score(y_pred, y_test)\npre = precision_score(y_pred, y_test)\nrec = recall_score(y_pred, y_test)\nf1  = f1_score(y_pred, y_test)","d1278ed9":"experiment['KNN'] = {'acc':acc, 'pre':pre, 'rec':rec, 'f1':f1}\n\nprint('acur\u00e1cia :', acc)\nprint('precis\u00e3o :', pre)\nprint('revoca\u00e7\u00e3o:', rec)\nprint('f1-score :', f1)","354cc11e":"# classificador\nfrom sklearn.naive_bayes import GaussianNB","b7b5a893":"model2 = GaussianNB()\nmodel2.fit(X_train, y_train)","4bb61d2c":"y_pred = model2.predict(X_test)","ed6ec801":"acc = accuracy_score(y_pred, y_test)\npre = precision_score(y_pred, y_test)\nrec = recall_score(y_pred, y_test)\nf1  = f1_score(y_pred, y_test)","9b363bef":"experiment['Naive Bayes'] = {'acc':acc, 'pre':pre, 'rec':rec, 'f1':f1}\n\nprint('acur\u00e1cia :', acc)\nprint('precis\u00e3o :', pre)\nprint('revoca\u00e7\u00e3o:', rec)\nprint('f1-score :', f1)","f23aaaef":"# classificador\nfrom sklearn.svm import SVC","889d65e8":"model3 = SVC()\nmodel3.fit(X_train, y_train)","b0e9a54d":"y_pred = model3.predict(X_test)","7a93499c":"acc = accuracy_score(y_pred, y_test)\npre = precision_score(y_pred, y_test)\nrec = recall_score(y_pred, y_test)\nf1  = f1_score(y_pred, y_test)","25cb5220":"experiment['SVM'] = {'acc':acc, 'pre':pre, 'rec':rec, 'f1':f1}\n\nprint('acur\u00e1cia :', acc)\nprint('precis\u00e3o :', pre)\nprint('revoca\u00e7\u00e3o:', rec)\nprint('f1-score :', f1)","8d924476":"# classificador\nfrom sklearn.tree import DecisionTreeClassifier","d0a4e35c":"model4 = DecisionTreeClassifier(random_state=26)\nmodel4.fit(X_train, y_train)","03e3e978":"y_pred = model4.predict(X_test)","920d5284":"acc = accuracy_score(y_pred, y_test)\npre = precision_score(y_pred, y_test)\nrec = recall_score(y_pred, y_test)\nf1  = f1_score(y_pred, y_test)","4d5f71e9":"experiment['Decision Tree'] = {'acc':acc, 'pre':pre, 'rec':rec, 'f1':f1}\n\nprint('acur\u00e1cia :', acc)\nprint('precis\u00e3o :', pre)\nprint('revoca\u00e7\u00e3o:', rec)\nprint('f1-score :', f1)","731174bd":"# visualizador da \u00e1rvore\nfrom sklearn.tree import plot_tree","a37b50b9":"plt.figure(figsize=(10,6))\n_ = plot_tree(model4) \nplt.show()","f381d3d7":"plt.figure(figsize=(10,6))\n_ = plot_tree(model4, max_depth=1) \nplt.show()","022b671b":"# classificador\nfrom sklearn.ensemble import RandomForestClassifier","ece3df46":"model5 = RandomForestClassifier(n_estimators=100, random_state=26)\nmodel5.fit(X_train, y_train)","a7fa7f87":"y_pred = model5.predict(X_test)","a296b9dc":"acc = accuracy_score(y_pred, y_test)\npre = precision_score(y_pred, y_test)\nrec = recall_score(y_pred, y_test)\nf1  = f1_score(y_pred, y_test)","8e12e90f":"experiment['Random Forest'] = {'acc':acc, 'pre':pre, 'rec':rec, 'f1':f1}\n\nprint('acur\u00e1cia :', acc)\nprint('precis\u00e3o :', pre)\nprint('revoca\u00e7\u00e3o:', rec)\nprint('f1-score :', f1)","f4fa1d85":"# ensemble\nfrom sklearn.ensemble import BaggingClassifier","a9f34c07":"model_base = SVC()\nmodel6 = BaggingClassifier(base_estimator=model_base, n_estimators=10, random_state=26)\nmodel6.fit(X_train, y_train)","7edfc316":"y_pred = model6.predict(X_test)","f891aa83":"acc = accuracy_score(y_pred, y_test)\npre = precision_score(y_pred, y_test)\nrec = recall_score(y_pred, y_test)\nf1  = f1_score(y_pred, y_test)","39a4deeb":"experiment['Bagging'] = {'acc':acc, 'pre':pre, 'rec':rec, 'f1':f1}\n\nprint('acur\u00e1cia :', acc)\nprint('precis\u00e3o :', pre)\nprint('revoca\u00e7\u00e3o:', rec)\nprint('f1-score :', f1)","53498492":"# ensemble\nfrom sklearn.ensemble import VotingClassifier","20cae814":"clf1 = SVC()\nclf2 = RandomForestClassifier(n_estimators=100, random_state=26)\nestimators=[('SVM', clf1), ('RandomForest', clf2)]\n\nmodel7 = VotingClassifier(estimators=estimators, voting='hard')\nmodel7.fit(X_train, y_train)","e571222e":"y_pred = model7.predict(X_test)","7974f8f2":"acc = accuracy_score(y_pred, y_test)\npre = precision_score(y_pred, y_test)\nrec = recall_score(y_pred, y_test)\nf1  = f1_score(y_pred, y_test)","adce18b7":"experiment['Ensemble'] = {'acc':acc, 'pre':pre, 'rec':rec, 'f1':f1}\n\nprint('acur\u00e1cia :', acc)\nprint('precis\u00e3o :', pre)\nprint('revoca\u00e7\u00e3o:', rec)\nprint('f1-score :', f1)","7b298545":"# automl\nimport autosklearn.classification as autoclassifier","d4256d78":"automl = autoclassifier.AutoSklearnClassifier(\n    time_left_for_this_task=120,\n    per_run_time_limit=30,\n    tmp_folder='\/automl\/tmp\/',\n    output_folder='\/automl\/output\/',\n)\nautoml.fit(X_train, y_train, dataset_name='heart-disease-uci')","957af102":"y_pred = automl.predict(X_test)","dcc2cffe":"acc = accuracy_score(y_pred, y_test)\npre = precision_score(y_pred, y_test)\nrec = recall_score(y_pred, y_test)\nf1  = f1_score(y_pred, y_test)","efea4080":"experiment['AutoML'] = {'acc':acc, 'pre':pre, 'rec':rec, 'f1':f1}\n\nprint('acur\u00e1cia :', acc)\nprint('precis\u00e3o :', pre)\nprint('revoca\u00e7\u00e3o:', rec)\nprint('f1-score :', f1)","f4a26197":"# output escondido, ficou muito grande\nautoml.show_models()","531bbd73":"# palheta de cores\nimport seaborn as sns","6b560835":"cm = sns.color_palette('Blues_r', as_cmap=True)\npd.DataFrame(experiment).T.style.background_gradient(subset=['acc', 'f1'], cmap=cm).highlight_max(axis=0)","cc90980f":"# random search\nfrom sklearn.model_selection import RandomizedSearchCV\n# uniform distribution\nfrom scipy.stats import uniform","8b0cea9b":"model = SVC()\nmodel.get_params()","d0f16fd2":"distributions = {\n    'random_state':[26],\n    'C':uniform(loc=1, scale=9),\n    'kernel': ['rbf', 'sigmoid'],\n}","cc02305a":"clf = RandomizedSearchCV(model, distributions, n_iter=50, random_state=26, refit='acc', cv=10)\nsearch = clf.fit(X_train, y_train)","247afe9f":"search.best_params_","7433bcd8":"search.best_score_","17cafe87":"y_pred = search.predict(X_test)","038b3e2c":"acc = accuracy_score(y_pred, y_test)\npre = precision_score(y_pred, y_test)\nrec = recall_score(y_pred, y_test)\nf1  = f1_score(y_pred, y_test)","444495df":"experiment['Random Search'] = {'acc':acc, 'pre':pre, 'rec':rec, 'f1':f1}\n\nprint('acur\u00e1cia :', acc)\nprint('precis\u00e3o :', pre)\nprint('revoca\u00e7\u00e3o:', rec)\nprint('f1-score :', f1)","b2b38707":"pd.DataFrame(experiment)[['SVM', 'Random Search']].T.style.highlight_max(axis=0)","2958004d":"**Discuss\u00e3o Ensemble**\n\nEnsemble obteve bons resultados, por\u00e9m n\u00e3o o melhor.   \nCom acur\u00e1cia de 78% e F1-score de 80%.   \n\n> **Nota**. Possui alto custo computacional, pois tem que treinar v\u00e1rios modelos.\n\n-----","b45d2297":"<a id=\"svm\"><\/a>\n\n## Support Vector Machines (SVM)","0290b489":"<a id=\"naive\"><\/a>\n\n## Naive Bayes","f590b4ab":"**Discuss\u00e3o Bagging**\n\nBagging obteve o melhor resultado, utilizando 10 modelos SVM.   \nCom acur\u00e1cia de 82% e F1-score de 84%.   \n\n> **Nota**. Possui alto custo computacional, pois tem que treinar v\u00e1rios modelos.\n\n-----","6f450527":"## Pr\u00e9-Processamento dos Dados\n\nVamos normalizar os dados entre valor 0 e 1, utilizando o transformador `MinMaxScaler`. \nEste transformador normaliza os valores por coluna, utilizando o valor m\u00e1ximo e m\u00ednimo para normalizar o valor real.\n\n> Este procedimento \u00e9 necess\u00e1rio, pois alguns algoritmos de classifica\u00e7\u00e3o se beneficiam de valores normalizados, tal como o K-NN.","4ce81999":"## Carregamento dos Dados","b939fde1":"### Hiper-par\u00e2metros\n\nQuais s\u00e3o os hiper-par\u00e2metros do SVM?","64c578d6":"**Discuss\u00e3o Naive Bayes**\n\nNeste conjunto de dados, o Naive Bayes teve um desempenho inferior ao k-NN.   \nContudo, apresentou uma acur\u00e1cia de 74% e F1-Score de 77%.\n\n-----","8992cf47":"Visualizando a \u00e1rvore inteira.","67ed3c91":"<a id=\"class\"><\/a>\n\n-----\n\n# Classifica\u00e7\u00e3o\n\n- Conjunto de dados.\n- Experimentos.\n\n[Voltar para o Topo](#top)","8024e93e":"<a id=\"data\"><\/a>\n\n-----\n\n# Dados\n\n- Carregamento dos dados.\n- Pr\u00e9-processamento dos dados.\n\n[Voltar para o Topo](#top)","84dd0581":"### Avalia\u00e7\u00e3o","5b8027e1":"### Avalia\u00e7\u00e3o","b79ab3d2":"Qual foi a acur\u00e1cia deste par\u00e2metro no dataset de valida\u00e7\u00e3o?","023c71f3":"### Avalia\u00e7\u00e3o","9212b1c5":"### Visualiza\u00e7\u00e3o\n\nTamb\u00e9m \u00e9 poss\u00edvel extrar as `DecisionTree`do `RandomForest` para visualiza\u00e7\u00e3o. Neste caso, \u00e9 necess\u00e1rio acessar cada uma das \u00e1rvores utilizando o comando `RandomForest.estimators_[indice]` e visualizar como demonstrado na se\u00e7\u00e3o da \u00c1rvore de Decis\u00e3o.","f02211c3":"### Avalia\u00e7\u00e3o\n\nAvaliando o melhor modelo no dataset de teste.","ed5f65fc":"Visualizando os dados transformados, colorindo pela classe.","247847b4":"<a id=\"knn\"><\/a>\n\n## K-NN\n\n_(k-Nearest Neighbors)_","51c567d3":"### Avalia\u00e7\u00e3o","3d97d7aa":"### Avalia\u00e7\u00e3o","d24bc3d5":"### Comparando o modelo padr\u00e3o com o Tuning\n\nSer\u00e1 que o Hyperparameter Tuning melhorou os resultados do SVM?","660e5085":"### Visualiza\u00e7\u00e3o\n\nPodemos ver o modelo ou o ensemble de modelos utilizado no AutoML.\n\n> Para isto, utilize o comando `automl.show_models()`.","1f08203a":"### Avalia\u00e7\u00e3o","ff622ff9":"# Classifica\u00e7\u00e3o - Presen\u00e7a de Doen\u00e7a Card\u00edaca\n\nEste notebook realiza um estudo, um conjunto de experimentos, sobre algoritmos de classifica\u00e7\u00e3o no dataset [Heart Disease UCI](https:\/\/www.kaggle.com\/ronitf\/heart-disease-uci). Um conjunto de dados que re\u00fane mais de 300 pacientes com 14 atributos, tais como idade, sexo, n\u00edvel do colesterol, n\u00edvel de a\u00e7ucar no sangue, entre outros. Nosso objetivo \u00e9 distinguir a presen\u00e7a (valor 1) ou aus\u00eancia (valor 0) de uma doen\u00e7a card\u00edaca.\n\n> Conte\u00fado voltado para iniciantes na \u00e1rea de Aprendizado de M\u00e1quina e Ci\u00eancia de Dados!\n\n\n<a id=\"top\"><\/a>\n\n## Conte\u00fado\n\n> **Nota.** Alguns c\u00f3digos foram ocultados a fim de facilitar a leitura.\n\nO notebook est\u00e1 organizado como segue:\n\n- [Dados](#data) - Carregamento dos dados, pr\u00e9-processamento.\n- [Visualiza\u00e7\u00e3o](#visual) - An\u00e1lise explorat\u00f3ria dos dados.\n- [Classifica\u00e7\u00e3o](#class) - Aplica\u00e7\u00e3o de algoritmos de Aprendizado de M\u00e1quina.\n    - [KNN](#knn) - Classifica\u00e7\u00e3o com k-NN.\n    - [Naive Bayes](#naive) - Classifica\u00e7\u00e3o com Naive Bayes.\n    - [Support Vector Machines](#svm) - Classifica\u00e7\u00e3o com Support Vector Machines.\n    - [\u00c1rvore de Decis\u00e3o](#decision) - Classifica\u00e7\u00e3o com Decision Tree.\n    - [Random Forest](#forest) - Classifica\u00e7\u00e3o com Random Forest.\n    - [Bagging](#bagging) - Classifica\u00e7\u00e3o com estrat\u00e9gia de Bagging.\n    - [Ensemble](#ensemble) - Classifica\u00e7\u00e3o com estrat\u00e9gia de Ensemble.\n    - [AutoML](#automl) - Classifica\u00e7\u00e3o usando Automated Machine Learning.\n- [Hyperparameter Tuning](#tuning) - Tuning de par\u00e2metros.","bbacacf2":"**Discuss\u00e3o \u00c1rvore de Decis\u00e3o**\n\nA \u00c1rvore de Decis\u00e3o obteve resultados razo\u00e1veis.   \nCom acur\u00e1cia de 68% e F1-score de 70%, abaixo do k-NN.   \n\nS\u00e3o excelente algoritmos de Aprendizado de M\u00e1quina para compreens\u00e3o\/estudo do neg\u00f3cio.\n\n-----","21ad8337":"Quais foram os melhores par\u00e2metros?","aa51dfa0":"<a id=\"automl\"><\/a>\n\n## AutoML\n\nAutomated Machine Learning.","8763d7dc":"### Avalia\u00e7\u00e3o","8b9824c7":"**Discuss\u00e3o Support Vector Machines (SVM)**\n\nO SVM foi o melhor modelo at\u00e9 o momento.   \nUltrapassando o k-NN, com acur\u00e1cia de 79% e F1-score de 82%.\n\n-----","38c6d8aa":"### Normaliza\u00e7\u00e3o dos Dados\n\nNesta se\u00e7\u00e3o vamos utilizar a normaliza\u00e7\u00e3o [MinMaxScaler](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.preprocessing.MinMaxScaler.html?highlight=minmaxscaler#sklearn.preprocessing.MinMaxScaler). Esta fun\u00e7\u00e3o de preprocessamento normaliza os dados conforme segue:\n\n$$x_{new} = x_{\\sigma} * (x_{max} - x_{min}) + x_{min}{}$$\n\nEm que:\n\n$$x_{\\sigma} = \\frac{(x - x_{min})}{(x_{max} - x_{min})}$$\n\n\nA grosso modo, normaliza os dados entre 0 e 1 e mantem sua distribui\u00e7\u00e3o original.","07dff45c":"# Conclus\u00e3o\n\nPor fim, o melhor algoritmo foi o SVM com a mesma acur\u00e1cia do KNN, mas com precis\u00e3o e f1-score maiores.   \nA estrat\u00e9gia de ensemble Bagging superou todos os classificadores, por\u00e9m teve maior custo computacional.","adc141dd":"Visualiza\u00e7\u00e3o dos Dados transformados em duas dimens\u00f5es.\n\n> **Note**. Os dados transformados n\u00e3o representam a realidade, ou seja, eles n\u00e3o s\u00e3o linearmente divididos como demonstrado visualmente. Contudo, indica que \u00e9 poss\u00edvel realizar uma divis\u00e3o nos dados.","a2c657a1":"<a id=\"forest\"><\/a>\n\n## Random Forest","a884507d":"Escolhendo uma distribui\u00e7\u00e3o dos par\u00e2metros, _i.e.,_ um espa\u00e7o de busca.","e076feaf":"<a id=\"bagging\"><\/a>\n\n## Bagging\n\nClassifica\u00e7\u00e3o com estrat\u00e9gia de Bagging, com algoritmo base SVM.","d516557f":"Segmenta os dados e as classes.","5f78fe80":"### Busca \/ Tuning\n\nExecutando a tunagem de par\u00e2metros pelo espa\u00e7o pr\u00e9-fixado, utilizando 10-fold cross validation e em no m\u00e1ximo 50 experimentos, bem como otimizando a acur\u00e1cia.","a26a541c":"<a id=\"tuning\"><\/a>\n\n-----\n\n# Hyperparameter Tuning\n\n[Random Search](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.RandomizedSearchCV.html) no algoritmo SVM.\n\n[Voltar para o Topo](#top)","544ff3c4":"### Visualiza\u00e7\u00e3o\n\nN\u00f3s conseguimos visualizar a \u00e1rvore de decis\u00e3o, como as ramifica\u00e7\u00f5es ocorreram.   \n\u00c9 muito \u00fatil para uma apresenta\u00e7\u00e3o de neg\u00f3cio, em que voc\u00ea consegue explicar a intelig\u00eancia induzida.","79452cdd":"**Discuss\u00e3o k-NN**\n\nO k-NN conseguiu classificar bem o conjunto de dados, alcan\u00e7ando resultados satisfat\u00f3rios.   \nAcur\u00e1cia de 79% e F1-score de 82%.\n\n-----","d9943430":"<a id=\"decision\"><\/a>\n\n## \u00c1rvore de Decis\u00e3o","fd6bc22f":"<a id=\"ensemble\"><\/a>\n\n## Ensemble\n\nClassifica\u00e7\u00e3o com estrat\u00e9gia de Ensemble, utilizando os algoritmos SVM e Random Forest.","42756123":"## Conjunto de Dados\n\nSepara os conjuntos de treinamento e teste.","8a359192":"<a id=\"visual\"><\/a>\n\n-----\n\n# Visualiza\u00e7\u00e3o\n\nNesta se\u00e7\u00e3o ser\u00e1 realizado a transforma\u00e7\u00e3o dos 13 atributos (removendo a classe) para 2 dimens\u00f5es utilizando um algortimos de redu\u00e7\u00e3o de dimensionadade, chamado de [PCA - Principal Component Analysis](https:\/\/en.wikipedia.org\/wiki\/Principal_component_analysis). \n\n> Faremos este procedimento a fim de visualizar os dados em 2 dimens\u00f5es.\n\n[Voltar para o Topo](#top)","5bfd1956":"**Discuss\u00e3o Random Forest**\n\nRandom Forest obteve um dos melhores resultados.   \nCom acur\u00e1cia de 75% e F1-score de 79%, abaixo do k-NN.   \n\nRandom Forests s\u00e3o um dos algoritmos mais utilizados em competi\u00e7\u00f5es de Aprendizado de M\u00e1quina.\n\n> **Nota**. Possui alto custo computacional, pois tem que treinar v\u00e1rios modelos.\n\n-----","e91d0e0a":"### Avalia\u00e7\u00e3o","e287052f":"Visualizando a primeira profundidade, apenas para observarmos os valores presentes na figura.\n\n> Na figura vemos: (1) O atributo selecionado e a quest\u00e3o (condi\u00e7\u00e3o de separa\u00e7\u00e3o) (nota, este nome pode ser personalizado); (2) a m\u00e9trica de impureza; (3) n\u00famero de exemplos; (4) n\u00famero de exemplo para cada classe; e (5) a cor significa a classe majorit\u00e1ria do respectivo n\u00f3."}}