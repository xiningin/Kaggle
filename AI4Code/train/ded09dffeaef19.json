{"cell_type":{"3859350e":"code","c047a818":"code","057b5005":"code","cbee6689":"code","c5c0bc8e":"code","3374c121":"code","1a71a7f0":"code","929d7332":"code","2397196b":"code","2e89822e":"code","e88c3f49":"code","e74e5daa":"code","c2e9d516":"code","634c55e1":"code","ab2943f9":"code","848ab3ad":"code","816ef4ca":"code","0ec41cbc":"code","19e60930":"code","c6e11b6d":"code","9df68129":"code","fb96f2c8":"code","ced1efb9":"code","af66b240":"code","d408740b":"code","7128e410":"code","a0397659":"code","c20ad69c":"code","12eaba71":"code","e58b8791":"code","310b1f7a":"code","b0f1c905":"code","6fc6b144":"code","d6ea06a6":"code","af7c8062":"code","6905ae58":"code","269b6d47":"code","6785674d":"code","cabb702e":"code","55439593":"code","c2ad0633":"code","24fbf918":"code","f8572e21":"code","13b7a1f1":"code","12570257":"code","b7c154a5":"code","4561ec2e":"code","1547ec22":"code","d34e07c2":"code","f8a5dff1":"code","a08522a9":"code","be9810be":"code","7aa56fd6":"code","3cb94797":"markdown","b258afc6":"markdown","35d38263":"markdown","6fc76f2c":"markdown","dabb9e20":"markdown"},"source":{"3859350e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","c047a818":"import seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.ensemble import RandomForestRegressor,GradientBoostingRegressor\nfrom sklearn.linear_model import Lasso\nfrom sklearn.metrics import r2_score\nfrom sklearn.model_selection import train_test_split,RandomizedSearchCV,GridSearchCV\n\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)","057b5005":"train = pd.read_csv(\"..\/input\/mercedes-benz-greener-manufacturing\/train.csv\")\ntest = pd.read_csv(\"..\/input\/mercedes-benz-greener-manufacturing\/test.csv\")\n\nprint (\"Training Dataset Shape {}\".format(train.shape))\nprint (\"Testing Dataset Shape {}\".format(test.shape))","cbee6689":"Submission = test[['ID']]\ntest.drop('ID',axis=1,inplace=True)\ntrain.drop('ID',axis=1,inplace=True)","c5c0bc8e":"train.info()","3374c121":"test.info()","1a71a7f0":"train.dtypes.value_counts()\n# Most of the Variables are of int type followed by object.","929d7332":"train.isna().sum()[train.isna().sum() > 0]\n# There is no column with missing values.","2397196b":"test.isna().sum()[test.isna().sum() > 0]\n# There is no column with missing values.","2e89822e":"train['y'].describe()","e88c3f49":"plt.figure(figsize=(10,8))\nsns.distplot(train['y'])","e74e5daa":"sns.boxplot(train['y'])\n# There is one outlier which is way above the other values. Makes sense to delete it","c2e9d516":"train.drop(train[train['y'] > 250].index[0],axis=0,inplace=True)","634c55e1":"train.head()","ab2943f9":"lst_train = []\nfor col in train.columns:\n    if train[col].nunique() == 1:\n        lst_train.append(col)\n        print (\"Column {} has Single Unique Value\".format(col))","848ab3ad":"lst_test = []\nfor col in test.columns:\n    if test[col].nunique() == 1:\n        lst_test.append(col)\n        print (\"Column {} has Single Unique Value\".format(col))","816ef4ca":"for col in train.columns:\n    if train[col].dtype == \"object\":\n        print (\"Unique Values in {} column\".format(col),train[col].nunique())","0ec41cbc":"for col in test.columns:\n    if test[col].dtype == \"object\":\n        print (\"Unique Values in {} column\".format(col),test[col].nunique())","19e60930":"plt.figure(figsize=(12,8))\nsns.boxplot(y=train['y'],x=train['X0'])\nplt.xlabel(\"X0 Values\")\nplt.ylabel(\"Values\")\nplt.title(\"Box Plot of Dependent Variable with X0 Categorical Variable\")","c6e11b6d":"plt.figure(figsize=(12,8))\nsns.boxplot(y=train['y'],x=train['X1'])\nplt.xlabel(\"X1 Values\")\nplt.ylabel(\"Values\")\nplt.title(\"Box Plot of Dependent Variable with X1 Categorical Variable\")","9df68129":"plt.figure(figsize=(12,8))\nsns.boxplot(y=train['y'],x=train['X2'])\nplt.xlabel(\"X2 Values\")\nplt.ylabel(\"Values\")\nplt.title(\"Box Plot of Dependent Variable with X2 Categorical Variable\")","fb96f2c8":"plt.figure(figsize=(12,8))\nsns.boxplot(y=train['y'],x=train['X3'])\nplt.xlabel(\"X3 Values\")\nplt.ylabel(\"Values\")\nplt.title(\"Box Plot of Dependent Variable with X3 Categorical Variable\")","ced1efb9":"plt.figure(figsize=(12,8))\nsns.boxplot(y=train['y'],x=train['X4'])\nplt.xlabel(\"X4 Values\")\nplt.ylabel(\"Values\")\nplt.title(\"Box Plot of Dependent Variable with X4 Categorical Variable\")","af66b240":"test['X4'].value_counts()","d408740b":"train.groupby('X4')['y'].agg({\"count\",\"min\",'max','mean','median'})","7128e410":"plt.figure(figsize=(12,8))\nsns.boxplot(y=train['y'],x=train['X5'])\nplt.xlabel(\"X5 Values\")\nplt.ylabel(\"Values\")\nplt.title(\"Box Plot of Dependent Variable with X5 Categorical Variable\")","a0397659":"plt.figure(figsize=(12,8))\nsns.boxplot(y=train['y'],x=train['X6'])\nplt.xlabel(\"X6 Values\")\nplt.ylabel(\"Values\")\nplt.title(\"Box Plot of Dependent Variable with X6 Categorical Variable\")","c20ad69c":"plt.figure(figsize=(12,8))\nsns.boxplot(y=train['y'],x=train['X8'])\nplt.xlabel(\"X8 Values\")\nplt.ylabel(\"Values\")\nplt.title(\"Box Plot of Dependent Variable with X8 Categorical Variable\")","12eaba71":"df = train[[col for col in train.columns if ((train[col].dtype == 'int') & (col not in ['ID','y']))]].columns\n\nlst_train = []\nfor col in train.columns:\n    if ((train[col].dtype == 'int') & (col not in ['ID','y'])):\n        lst_train.append(sum(train[col])\/len(train))\n\ndf_new = pd.DataFrame(lst_train,df,columns=['Ratio_Train']).sort_index()\n\nlst_test = []\nfor col in test.columns:\n    if ((test[col].dtype == 'int') & (col !='ID')):\n        lst_test.append(sum(test[col])\/len(test))\n\ndf_new['Ratio_Test'] = lst_test\n\ndf_new.sort_index()","e58b8791":"# Let's check the correlation between the variables and eliminate the one's that have high correlation\n# Threshold for removing correlated variables\nthreshold = 0.9\n\n# Absolute value correlation matrix\ncorr_matrix = train.corr().abs()\ncorr_matrix.head()\n\n# Some of the columns\/rows here are represented as NaN's because these variables have a single unique value (either 1 or 0)","310b1f7a":"# Upper triangle of correlations\nupper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\nupper.head()","b0f1c905":"# Select columns with correlations above threshold\nto_drop = [column for column in upper.columns if any(upper[column] > threshold)]\n\nprint('There are %d columns to remove.' % (len(to_drop)))\nprint (\"Following columns can be dropped {}\".format(to_drop))","6fc6b144":"train = train.drop(columns = to_drop)\ntest = test.drop(columns = to_drop)\n\nprint('Training shape: ', train.shape)\nprint('Testing shape: ', test.shape)","d6ea06a6":"# As we saw in the Boxplots above, this variable is dominated by a single value. Hence deleting this variable.\ntrain.drop('X4',axis=1,inplace=True)\ntest.drop('X4',axis=1,inplace=True)","af7c8062":"train.head()","6905ae58":"test.head()","269b6d47":"# Since there are lot of levels in the categorical variables, it makes sense to convert them to category data type instead of creating dummy variables\n# which will only add to the complexity by creating large number of features.\nfor col in train.columns:\n    if train[col].dtype == \"object\":\n        train[col] = train[col].astype('category')\n        train[col] = train[col].cat.codes","6785674d":"for col in test.columns:\n    if test[col].dtype == \"object\":\n        test[col] = test[col].astype('category')\n        test[col] = test[col].cat.codes","cabb702e":"X = train[[col for col in train.columns if col!='y']]\ny = train['y']","55439593":"rf = RandomForestRegressor(n_jobs=-1,random_state=42)\nrf.fit(X,y)","c2ad0633":"feature_importances = rf.feature_importances_\nfeature_importances = pd.DataFrame({'feature': list(X.columns), 'importance': feature_importances}).sort_values('importance', ascending = False)\nfeature_importances","24fbf918":"# Dropping the variables which have 0 feature importance. Random Forests are a good way to measure the feature importance of the variables. \nzero_features = list(feature_importances[feature_importances['importance'] == 0.0]['feature'])\nprint('There are %d features with 0.0 importance' % len(zero_features))\nfeature_importances","f8572e21":"train.drop(zero_features,axis=1,inplace=True)\ntest.drop(zero_features,axis=1,inplace=True)","13b7a1f1":"X = train[[col for col in train.columns if col!='y']]\ny = train['y']","12570257":"rf_2 = RandomForestRegressor(n_jobs=-1,random_state=42)\nrf_2.fit(X,y)","b7c154a5":"#Submission['y'] = rf_2.predict(test)\n#Submission.to_csv(\"Submission_1.csv\",index=None)","4561ec2e":"rf_3 = RandomForestRegressor(n_jobs=-1,random_state=42)\nparams = {\"n_estimators\":list(range(50,501,100)),\n          \"max_features\":[0.5,'sqrt','log2'],\n          \"min_samples_leaf\":[1,3,5,10,25],\n          \"min_samples_split\":[2,5,7,10,15],\n          \"max_depth\":[3,5,7,9]}\nr_search = RandomizedSearchCV(estimator=rf_3,param_distributions=params,cv=5,scoring='r2')\nr_search.fit(X,y)","1547ec22":"r_search.best_estimator_,r_search.best_params_,r_search.best_score_","d34e07c2":"feature_importances = r_search.best_estimator_.feature_importances_\nfeature_importances = pd.DataFrame({'feature': list(X.columns), 'importance': feature_importances}).sort_values('importance', ascending = False)\nfeature_importances","f8a5dff1":"zero_features = list(feature_importances[feature_importances['importance'] == 0.0]['feature'])\nprint('There are %d features with 0.0 importance' % len(zero_features))\nfeature_importances","a08522a9":"train.drop(zero_features,axis=1,inplace=True)\ntest.drop(zero_features,axis=1,inplace=True)","be9810be":"X = train[[col for col in train.columns if col!='y']]\ny = train['y']\n\nrf_4 = RandomForestRegressor(n_jobs=-1,random_state=42)\nparams = {\"n_estimators\":list(range(50,501,100)),\n          \"max_features\":[0.5,'sqrt','log2'],\n          \"min_samples_leaf\":[1,3,5,10,25],\n          \"min_samples_split\":[2,5,7,10,15],\n          \"max_depth\":[3,5,7,9]}\nr_search2 = RandomizedSearchCV(estimator=rf_4,param_distributions=params,cv=5,scoring='r2')\nr_search2.fit(X,y)\n\nr_search2.best_estimator_,r_search2.best_params_,r_search2.best_score_","7aa56fd6":"Submission['y'] = r_search2.predict(test)\nSubmission.to_csv(\"Submission_2.csv\",index=None)","3cb94797":"There is a lot of variance in this variable X0 as can be seen from the box plot.","b258afc6":"We can see that this variable is dominated by a single value and hence we see a wierd Box plot. It makes sense to delete this variable since it does not impart much information about the dependent variable.","35d38263":"From the Dataframe above, we can see that Although there are many columns which have ratio of 1's to total as 0 in the training dataset, the same columns do not have 0 ratio in the test dataset.","6fc76f2c":"The distribution is almost the same except for a few minor bumps.","dabb9e20":"Lot of Variance here as well."}}