{"cell_type":{"865ae760":"code","52d40275":"code","f1acd535":"code","804dda88":"code","b31a366f":"code","3c52c46d":"code","0f4dd3e5":"code","8831b94f":"code","837b18f8":"code","40c32693":"code","9e26a4d7":"code","b7fc7abe":"code","381db2fe":"code","addb140d":"code","b7160467":"code","28785136":"code","9af9acda":"code","ab81d66c":"code","03d42b9c":"markdown","effab6c7":"markdown","93d42a18":"markdown","fe03439d":"markdown","bb9f5fc9":"markdown","13be46ee":"markdown","c048fa86":"markdown","4fb0c6c3":"markdown","24eb509d":"markdown","5a344eca":"markdown","44c9a340":"markdown","a5400c21":"markdown","facb14fe":"markdown","ba7414c9":"markdown","81207d88":"markdown","b3a67dda":"markdown","8dac43ee":"markdown","458b340f":"markdown","5d3dcd03":"markdown"},"source":{"865ae760":"import numpy as np\nimport cv2\nimport os \nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score, cohen_kappa_score\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.utils import to_categorical","52d40275":"path = \"..\/input\/ocular-disease-recognition-odir5k\"\ndatafr = pd.read_csv(os.path.join(path, \"full_df.csv\"))\n# Image size\nROW = 224\nCOL = 224\n\n# Images file names\nfile_names = []\n\n# Loaded data\ntraining_images = []\nflags = []\n\n# Features\ngrayscaled_images = []\ninverted_images = []\nthresholded_images = []\ngray_histogram_of_images = []\nRGB_histogram_of_images = []\nconny_edged_images = []\nlaplacian_edged_images = []\nx_edged_images = []\ny_edged_images = []\n\nthreshold_mean = []\nthreshold_median = []\nthreshold_std_dev = []\n\nconny_mean = []\nconny_median = []\nconny_std_dev = []","f1acd535":"cutter = 0\ndivision = 1 # Increase to decrease number of images to load .. faster outputing for testing\nfile_names.clear()\nflags.clear()\nfor label, flag, file_name in zip(datafr[\"Left-Diagnostic Keywords\"], datafr[\"C\"], datafr[\"Left-Fundus\"]):\n    if((\"cataract\" in label) and (flag == 1)):\n        file_names.append(file_name)\n        flags.append(1)\n    elif((\"normal fundus\" in label) and (flag == 0)):\n        if(cutter%division == 0):\n            file_names.append(file_name)\n            flags.append(0)\n        cutter = cutter + 1\n\ncutter = 0\nfor label, flag, file_name in zip(datafr[\"Right-Diagnostic Keywords\"], datafr[\"C\"], datafr[\"Right-Fundus\"]):\n    if((\"cataract\" in label) and (flag == 1)):\n        file_names.append(file_name)\n        flags.append(1)\n    elif((\"normal fundus\" in label) and (flag == 0)):\n        if(cutter%division == 0):\n            file_names.append(file_name)\n            flags.append(0)\n        cutter = cutter + 1\n\nprint(\"Data Length =\",len(file_names), \"files\", len(flags), \"flags\")","804dda88":"plt.bar([0,1], [len([i for i in flags if i == 1]), len([i for i in flags if i == 0])], color = ['r', 'g'])\nplt.xticks([0, 1], ['Cataract', 'Normal'])\nplt.show()","b31a366f":"training_images.clear()\nfor idx, image_name in enumerate(file_names):\n    image = cv2.imread(os.path.join(path,\"preprocessed_images\",image_name))\n    try:\n        image = cv2.resize(image, (ROW, COL))\n        image = cv2.normalize(image, None, alpha = 0, beta = 255, norm_type = cv2.NORM_MINMAX, dtype =cv2.CV_8U)\n        training_images.append(image)\n    except:\n        del flags[idx]","3c52c46d":"def showSamples(images, gray = False):\n    figure, axes = plt.subplots(2, 2)\n    axes[0, 0].title.set_text(\"Cataract\")\n    axes[0, 1].title.set_text(\"Normal\")\n    \n    axes[0, 0].axis('off')\n    axes[0, 1].axis('off')\n    axes[1, 0].axis('off')\n    axes[1, 1].axis('off')\n    \n    axes[0, 0].imshow(images[0],cmap='gray') if gray else axes[0, 0].imshow(cv2.cvtColor(images[0], cv2.COLOR_BGR2RGB))\n    axes[0, 1].imshow(images[2],cmap='gray') if gray else axes[0, 1].imshow(cv2.cvtColor(images[2], cv2.COLOR_BGR2RGB))\n    axes[1, 0].imshow(images[17],cmap='gray') if gray else axes[1, 0].imshow(cv2.cvtColor(images[17], cv2.COLOR_BGR2RGB))\n    axes[1, 1].imshow(images[12],cmap='gray') if gray else axes[1, 1].imshow(cv2.cvtColor(images[12], cv2.COLOR_BGR2RGB))\n    plt.show()\n\nprint(np.array(training_images).shape)\nprint(training_images[1961].dtype)\nshowSamples(training_images)","0f4dd3e5":"grayscaled_images.clear()\nfor idx, image in enumerate(training_images):\n    gray_image = cv2.cvtColor(image,cv2.COLOR_BGR2GRAY)\n    gray_image = cv2.normalize(src=gray_image, dst=None, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_8U)\n    grayscaled_images.append(gray_image)\n\nshowSamples(grayscaled_images, True)","8831b94f":"thresholded_images.clear()\nfor idx, image in enumerate(grayscaled_images):\n    ret, image = cv2.threshold(image,127,255,cv2.THRESH_TOZERO_INV)\n    thresholded_images.append(image)\n\nshowSamples(thresholded_images, True)","837b18f8":"gray_histogram_of_images.clear()\nfor idx, image in enumerate(grayscaled_images):\n    image_histogram = cv2.equalizeHist(image)\n    gray_histogram_of_images.append(image_histogram)\n\nshowSamples(gray_histogram_of_images, True)","40c32693":"RGB_histogram_of_images.clear()\nfor idx, image in enumerate(training_images):\n    R, G, B = cv2.split(image)\n    image_histogram_R = cv2.equalizeHist(R)\n    image_histogram_G = cv2.equalizeHist(G)\n    image_histogram_B = cv2.equalizeHist(B)\n    image_histogram = cv2.merge((image_histogram_R, image_histogram_G, image_histogram_B))\n    RGB_histogram_of_images.append(image_histogram)\n\nshowSamples(RGB_histogram_of_images)","9e26a4d7":"conny_edged_images.clear()\nfor idx, image in enumerate(training_images):\n    image = cv2.Canny(image,30,200)\n    conny_edged_images.append(image)\n\nshowSamples(conny_edged_images, True)","b7fc7abe":"x_edged_images.clear()\nfor idx, image in enumerate(grayscaled_images):\n    sobelx = cv2.Sobel(image,cv2.CV_64F,1,0,ksize=5)\n    x_edged_images.append(sobelx)\n\nshowSamples(x_edged_images, True)","381db2fe":"y_edged_images.clear()\nfor idx, image in enumerate(grayscaled_images):\n    sobely = cv2.Sobel(image,cv2.CV_64F,0,1,ksize=5)  \n    y_edged_images.append(sobely)\n\nshowSamples(y_edged_images, True)","addb140d":"laplacian_edged_images.clear()\nfor idx, image in enumerate(grayscaled_images):\n    lap_image = cv2.Laplacian(image, cv2.CV_64F)\n    laplacian_edged_images.append(lap_image)\n\nshowSamples(laplacian_edged_images, True)","b7160467":"threshold_mean.clear()\nthreshold_median.clear()\nthreshold_std_dev.clear()\n\nfor idx, image in enumerate(thresholded_images):\n    mean = np.mean(image)\n    median = np.median(image)\n    std_dev = np.std(image)\n    \n    threshold_mean.append(mean)\n    threshold_median.append(median)\n    threshold_std_dev.append(std_dev)\n\nprint(threshold_mean[0], threshold_mean[2])\nprint(threshold_median[0], threshold_median[2])\nprint(threshold_std_dev[0], threshold_std_dev[2])\nprint()\nprint(threshold_mean[17], threshold_mean[12])\nprint(threshold_median[17], threshold_median[12])\nprint(threshold_std_dev[17], threshold_std_dev[12])","28785136":"conny_mean.clear()\nconny_median.clear()\nconny_std_dev.clear()\n\nfor idx, image in enumerate(conny_edged_images):\n    mean = np.mean(image)\n    median = np.median(image)\n    std_dev = np.std(image)\n    \n    conny_mean.append(mean)\n    conny_median.append(median)\n    conny_std_dev.append(std_dev)\n\nprint(conny_mean[0], conny_mean[2])\nprint(conny_median[0], conny_median[2])\nprint(conny_std_dev[0], conny_std_dev[2])\nprint()\nprint(conny_mean[17], conny_mean[12])\nprint(conny_median[17], conny_median[12])\nprint(conny_std_dev[17], conny_std_dev[12])","9af9acda":"training_features = np.vstack((threshold_mean,threshold_median,threshold_std_dev,conny_mean,conny_median,conny_std_dev)).T\n\nimage_train, image_test, flag_train, flag_test = train_test_split(training_features, np.asarray(flags), test_size=0.3, random_state=1)\n\nstdSc = StandardScaler()\nimage_train = stdSc.fit_transform(image_train)\nimage_test = stdSc.transform(image_test)\n\nimage_train = np.asarray(image_train).astype('float32')\nflag_train = np.asarray(flag_train).astype('float32')\n\nimage_test = np.asarray(image_test).astype('float32')\nflag_test = np.asarray(flag_test).astype('float32')","ab81d66c":"classifier = Sequential()\n\nlayer_info = Dense(activation='relu', input_dim=6, units=6)\nclassifier.add(layer_info)\n\nlayer_info = Dense(activation='relu', units=4)\nclassifier.add(layer_info)\n\nlayer_info = Dense(activation='sigmoid',units=1)\nclassifier.add(layer_info)\n\nclassifier.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n\nclassifier.fit(image_train, flag_train, batch_size=50, epochs=50)\n\nflag_prediction = classifier.predict(image_test).round()\n\ntn, fp, fn, tp = confusion_matrix(flag_test, flag_prediction).ravel()\n\nprint(\"True Negative =\",tn)\nprint(\"False Positive =\",fp)\nprint(\"False Negative =\",fn)\nprint(\"True Positive =\",tp)\n\n\nprint(confusion_matrix(flag_test, flag_prediction))\nprint(accuracy_score(flag_test, flag_prediction)*100)","03d42b9c":"## F5 RGB Histogram","effab6c7":"# Cataract Ratio","93d42a18":"# Enviroment variables","fe03439d":"## F9 Mean, Median, Mode, Standard Deviation of ***Conny***","bb9f5fc9":"## F3 Conny Edge detection","13be46ee":"## F4 Grayscale Histogram","c048fa86":"## F2 Threshold","4fb0c6c3":"# Images Sample","24eb509d":"# Feature Extraction","5a344eca":"# Filtering Cataract & Healthy eyes from others","44c9a340":"# Building, Training & Testing the NN Model","a5400c21":"## F1 Grayscale","facb14fe":"# Loading Images","ba7414c9":"## F7 Y Edges","81207d88":"## F6 X Edges","b3a67dda":"## F8 Laplacian Edges","8dac43ee":"# Formating the data for Machine Learning","458b340f":"# Imports","5d3dcd03":"## F9 Mean, Median, Mode, Standard Deviation of ***Threshold***"}}