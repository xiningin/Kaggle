{"cell_type":{"ff45a509":"code","0bbc675d":"code","8f507918":"code","97f6d191":"code","484e351e":"code","90268a2f":"code","2609cf0c":"code","6890db44":"code","4e03d966":"code","e2e4d26b":"code","9cf298af":"code","3208982c":"code","78be5be3":"markdown","1164268f":"markdown","55bcec84":"markdown","c7648bf9":"markdown","6c8f1809":"markdown","8c794007":"markdown","404e66f4":"markdown","250eb2fc":"markdown"},"source":{"ff45a509":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","0bbc675d":"import numpy as np\nfrom sklearn import preprocessing\n\nraw_csv_data = np.loadtxt('..\/input\/indian_liver_patient-NN.csv', delimiter = ',')\n\nunscaled_inputs_all = raw_csv_data[:,:-1]\ntargets_all = raw_csv_data[:,-1]","8f507918":"def balance_dataset(y_n):\n    if y_n == True:\n        num_one_targets = 0\n        num_two_targets = 0\n        for i in range(targets_all.shape[0]):\n            if targets_all[i] == 1:\n                num_one_targets += 1\n            if targets_all[i] == 0:\n                num_two_targets += 1\n\n        if num_one_targets > num_two_targets:\n            equal_prior = num_two_targets\n        else:\n            equal_prior = num_one_targets\n\n        print('Equal_prior: ', equal_prior)\n\n        indices_to_remove = []\n        num_one_counter = 0\n        num_two_counter = 0\n        for j in range(targets_all.shape[0]):\n            if targets_all[j] == 1:\n                num_one_counter += 1\n                if num_one_counter > equal_prior:\n                      indices_to_remove.append(j)\n            if targets_all[j] == 0:\n                if num_two_counter > equal_prior:\n                      indices_to_remove.append(j)\n\n        unscaled_inputs_equal_priors = np.delete(unscaled_inputs_all,indices_to_remove, axis = 0)\n        targets_equal_priors = np.delete(targets_all, indices_to_remove, axis = 0)\n    else:\n        unscaled_inputs_equal_priors = unscaled_inputs_all\n        targets_equal_priors = targets_all\n    return [unscaled_inputs_equal_priors,targets_equal_priors]","97f6d191":"[unscaled_inputs_pre_proc,targets_pre_proc] = balance_dataset(True) # True: for a balanced dataset, False: for the dataset as such","484e351e":"print(unscaled_inputs_pre_proc.shape)\nprint(targets_pre_proc.shape)","90268a2f":"#unscaled_inputs_pre_proc","2609cf0c":"# We standardize the variables to reduce the weight of higher numbers on the model. \nscaled_inputs = preprocessing.scale(unscaled_inputs_pre_proc)","6890db44":"#scaled_inputs","4e03d966":"shuffled_indices = np.arange(scaled_inputs.shape[0])\nnp.random.shuffle(shuffled_indices)\n\nshuffled_inputs = scaled_inputs[shuffled_indices]\nshuffled_targets = targets_pre_proc[shuffled_indices]\nshuffled_inputs.shape","e2e4d26b":"#shuffled_targets","9cf298af":"# We split the data into train and validation to prevent overfitting. 'test' dataset is for calculating the accuracy of the model.\n# We will see the test accuracy in the next notebook file.\n\nsamples_count = shuffled_inputs.shape[0]\n\n#You can create datasets with different proportions (80:10:10, 70:20:10)\ntrain_samples_count = int(0.8*samples_count)\nvalidation_samples_count = int(0.1*samples_count)\ntest_samples_count = samples_count - train_samples_count - validation_samples_count\n\ntrain_inputs = shuffled_inputs[:train_samples_count]\ntrain_targets = shuffled_targets[:train_samples_count]\n\nvalidation_inputs = shuffled_inputs[train_samples_count:train_samples_count+validation_samples_count]\nvalidation_targets = shuffled_targets[train_samples_count:train_samples_count+validation_samples_count]\n\ntest_inputs = shuffled_inputs[train_samples_count+validation_samples_count:]\ntest_targets = shuffled_targets[train_samples_count+validation_samples_count:]\n\n# To see if how the values in target dataset distributed. From a balanced dataset, we expect 1s and 0s in an approximate 50:50 proportion\nprint(np.sum(train_targets), train_samples_count, np.sum(train_targets) \/ train_samples_count)\nprint(np.sum(validation_targets), validation_samples_count, np.sum(validation_targets) \/ validation_samples_count)\nprint(np.sum(test_targets), test_samples_count, np.sum(test_targets) \/ test_samples_count)","3208982c":"np.savez('Liver_disease_data_train', inputs=train_inputs, targets=train_targets)\nnp.savez('Liver_disease_data_validation', inputs=validation_inputs, targets=validation_targets)\nnp.savez('Liver_disease_data_test', inputs=test_inputs, targets=test_targets)","78be5be3":"### Extract the data from the csv","1164268f":"### Standardize the inputs","55bcec84":"### Balance the dataset","c7648bf9":"### Save the three datasets in *.npz","6c8f1809":"# Indian Lever Patients Analysis-Deep Neural Networks-Preprocessing","8c794007":"### Shuffle the data","404e66f4":"We are going to perform a Supervised Learnind on the dataset. The last column contains the targets\n\nWe do the following tasks here:\n1. Preporcess the data\n    a. Removed records with null values\n    b. Changed the value '2' under columnt 'Dataset' to 0\n    c. Changed Male to 0 and Female to 1 under 'Gender' coulumn.\n    d. Remoed the headers from the file\n\n2. Balance the dataset\n3. Create 3 datasets: training, validation, and test\n    a. We can create datasets with different proportions (80:10:10, 70:20:10)\n4. Save those 3 datasets in a tensor friendly format (*.npz)\n    a. We use tensorflow to create the ML algorithm\n","250eb2fc":"### Split the dataset into train, validation, and test"}}