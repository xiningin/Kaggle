{"cell_type":{"eb8083ae":"code","6c5f28d1":"code","f4515e01":"code","be52b83b":"code","d428685e":"code","deb37190":"code","fde745de":"code","a7c84979":"code","e08184df":"code","4125e60b":"code","fc08700d":"code","806aa7da":"code","6adb1b52":"code","b1bf0100":"code","adc65603":"code","d45df874":"code","1284a2e5":"code","515cfb9a":"code","19a923bd":"code","aec7fb43":"code","2787e572":"code","0fa238de":"code","23756370":"code","64e078e0":"code","7aab4683":"code","f5399a28":"code","e3369a96":"markdown","bc1c2819":"markdown","dd976114":"markdown","b3f30fa1":"markdown","27691dae":"markdown","d4c609f6":"markdown","dc6f3e3d":"markdown","9d1b2175":"markdown","33aa1f30":"markdown","d890de92":"markdown","3c955c50":"markdown","cc8f1877":"markdown","a80d213e":"markdown","6df2477f":"markdown","dd9a15d6":"markdown","f9a8c212":"markdown","e3dd5ac7":"markdown","968651e6":"markdown","fd246615":"markdown","3f1be039":"markdown","f637f28c":"markdown","a85429c5":"markdown","4cc0a5c5":"markdown","ed549d0d":"markdown","c35c33ce":"markdown","82ac9e6a":"markdown","c2649241":"markdown","f446853d":"markdown","0de019cf":"markdown","5e2a11f7":"markdown","da484303":"markdown","1cdd16a8":"markdown","b78fe5ce":"markdown"},"source":{"eb8083ae":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\nfrom torch import optim\n\nimport sklearn\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","6c5f28d1":"import matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport matplotlib.pylab as pylab\nimport seaborn as sns\n\n%matplotlib inline\nmpl.style.use('ggplot')\nsns.set_style('white')\npylab.rcParams['figure.figsize'] =12,8","f4515e01":"path =  \"..\/input\/\"\n\ndf_train = pd.read_csv(f'{path}train.csv', index_col = 'PassengerId')\ndf_test = pd.read_csv(f'{path}test.csv', index_col = 'PassengerId')\n\ntarget = df_train['Survived']\ntarget.columns = ['Survived']\ndf_train = df_train.drop(labels = 'Survived', axis = 1)","be52b83b":"df_train['Training_set']= True\ndf_test['Training_set'] = False\n\ndf_full = pd.concat([df_train,df_test])","d428685e":"df_full.info()","deb37190":"df_full.isnull().sum()[df_full.isnull().sum()>0]","fde745de":"df_full = df_full.drop(labels = ['Ticket','Name', 'Cabin'], axis = 1)\ndf_full","a7c84979":"df_full.Age = df_full.Age.fillna(df_full.Age.mean())\ndf_full.Fare = df_full.Fare.fillna(df_full.Fare.mean())\ndf_full.Embarked = df_full.fillna(df_full.Embarked.mode()[0])\n\ndf_full = df_full.interpolate()\ndf_full = pd.get_dummies(df_full)\ndf_full","e08184df":"df_train = df_full[df_full['Training_set']==True]\ndf_test = df_full[df_full['Training_set']==False]","4125e60b":"df_train.drop(labels = 'Training_set', inplace = True, axis = 1)\ndf_test.drop(labels ='Training_set', inplace = True, axis = 1)","fc08700d":"torch.manual_seed(2) #setting a seed so that the results are reproducible\nmsk = np.random.randn(len(df_train)) < 0.8\n\ntraining_features = torch.tensor(df_train[msk].values.astype('float32'))\ndev_features = torch.tensor(df_train[~msk].values.astype('float32'))\n\ntraining_labels = torch.tensor(target[msk].values)\ndev_labels = torch.tensor(target[~msk].values)\n\ntest_features = torch.tensor(df_test.values.astype('float32'))","806aa7da":"model = nn.Sequential(nn.Linear(10,5),nn.ReLU(),nn.Dropout(p=0.2),nn.Linear(5,5),nn.ReLU(), nn.Dropout(p=0.1),nn.Linear(5,2),nn.LogSoftmax(dim =1))\n\ncriterion = nn.NLLLoss()\noptimizer = optim.Adam(model.parameters(), lr = 0.003)","6adb1b52":"def evaluate(model,test_features,test_labels=None,print_acc_and_cost = False, testing = False):\n    with torch.no_grad():\n        test_loss =0\n        log_preds = model(test_features)\n        preds=torch.exp(log_preds)\n        preds, survived = torch.max(preds, 1)\n        if(testing):\n            loss = criterion(log_preds,test_labels)\n            equals = survived==test_labels.view(*survived.shape)\n            accuracy = torch.mean(equals.type(torch.FloatTensor))\n            test_loss += loss.item()\n            if(print_acc_and_cost):\n                print(f'Testing accuracy : {accuracy*100:.3f}%')\n                print(f'Testing Loss : {test_loss:.3f}')\n            return test_loss\n        else:\n            return survived\n        #print(survived)","b1bf0100":"epochs = 1000\nsteps = 0\ntraining_losses,testing_losses = [],[]\nfor e in range(epochs):\n    running_loss = 0\n    optimizer.zero_grad()\n    \n    log_outputs = model(training_features)\n    loss = criterion(log_outputs,training_labels)\n    \n    loss.backward()\n    outputs = torch.exp(log_outputs)\n    \n    optimizer.step()\n    steps += 1\n    running_loss += loss.item()\n    training_losses.append(running_loss)\n    if(steps%100==0):\n        with torch.no_grad():\n            model.eval()\n            print(f'Epoch : {e+1}\/{epochs}...')\n            testing_losses.append(evaluate(model,dev_features, test_labels = dev_labels, print_acc_and_cost=True, testing = True))\n        print(f'Training loss {running_loss:.3f}\\n')\n    else:\n        with torch.no_grad():\n            model.eval()\n            testing_losses.append(evaluate(model,dev_features, test_labels = dev_labels, print_acc_and_cost=False, testing = True))\n    model.train()\n\nplt.plot(training_losses,label='Training loss')\nplt.plot(testing_losses, label='Testing loss')\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Loss\")\nplt.title(\"3 Layer Neural Network\\n\")\nplt.legend(frameon = False)","adc65603":"survived = evaluate(model,test_features)\nmy_submission = pd.DataFrame({'PassengerId':df_test.index,'Survived':survived})\nmy_submission.to_csv('.\/submission.csv', index = False)\n!ls","d45df874":"np.random.seed(0)\nfrom sklearn import model_selection\nX_train, X_val, y_train, y_val = model_selection.train_test_split(df_train,target, test_size = 0.2, train_size = 0.8, random_state = 0 )\n#print(X_train.shape,y_train.shape,X_val.shape, y_val.shape)","1284a2e5":"from sklearn import tree\nfrom sklearn import ensemble\nfrom sklearn import linear_model\nfrom sklearn import gaussian_process\nfrom sklearn import naive_bayes\nfrom sklearn import neighbors\nfrom sklearn import svm\nfrom sklearn import discriminant_analysis\nfrom xgboost.sklearn import XGBClassifier","515cfb9a":"MLA = [\n    ensemble.AdaBoostClassifier(),\n    ensemble.BaggingClassifier(),\n    ensemble.GradientBoostingClassifier(),\n    ensemble.ExtraTreesClassifier(),\n    ensemble.RandomForestClassifier(n_estimators = 100, random_state = 0),\n    \n    gaussian_process.GaussianProcessClassifier(),\n    \n    naive_bayes.BernoulliNB(),\n    naive_bayes.GaussianNB(),\n    \n    neighbors.KNeighborsClassifier(),\n    \n    svm.SVC(probability=True),\n    svm.NuSVC(probability = True),\n    svm.LinearSVC(),\n    \n    tree.DecisionTreeClassifier(),\n    tree.ExtraTreeClassifier(),\n    \n    discriminant_analysis.LinearDiscriminantAnalysis(),\n    discriminant_analysis.QuadraticDiscriminantAnalysis(),\n    \n    XGBClassifier(), \n    \n    linear_model.LogisticRegressionCV(),\n    linear_model.PassiveAggressiveClassifier(),\n    linear_model.RidgeClassifierCV(),\n    linear_model.SGDClassifier(),\n    linear_model.Perceptron()\n]","19a923bd":"MLA_columns = ['MLA_names', 'MLA_parameters', 'MLA_Train_Accuracy_Mean'\n               ,'MLA_Test_Accuracy_Mean', 'MLA_Test_Accuracy_3*STD', \n               'MLA_Time']\n\nMLA_compare = pd.DataFrame(columns = MLA_columns)","aec7fb43":"import warnings\nwarnings.filterwarnings('ignore')","2787e572":"MLA_Predict = y_val\nrow_index = 0\nfor alg in MLA:\n    MLA_name = alg.__class__.__name__\n    MLA_compare.loc[row_index, 'MLA_names'] = MLA_name\n    MLA_compare.loc[row_index, 'MLA_parameters'] = str(alg.get_params())\n    cv_results = model_selection.cross_validate(alg, X_train, y_train, cv=3, return_train_score = True)\n    MLA_compare.loc[row_index, 'MLA_Time'] = cv_results['fit_time'].mean()\n    MLA_compare.loc[row_index, 'MLA_Train_Accuracy_Mean'] = cv_results[\"train_score\"].mean()\n    MLA_compare.loc[row_index, 'MLA_Test_Accuracy_Mean'] = cv_results['test_score'].mean()\n    MLA_compare.loc[row_index, 'MLA_Test_Accuracy_3*STD'] = cv_results['test_score'].std()*3\n    \n    alg.fit(X_train, y_train)\n    MLA_Predict[MLA_name] = alg.predict(X_val)\n    row_index += 1\n    print(\".\", end=\"\")\nMLA_compare.sort_values(by = 'MLA_Test_Accuracy_Mean', ascending = False, inplace = True)\nMLA_compare","0fa238de":"sns.barplot(x = 'MLA_Test_Accuracy_Mean', y = 'MLA_names', data = MLA_compare, color = 'm')\n\nplt.title('Machine Learning Algorithms Accuracy Score \\n')\nplt.ylabel('Algortithm')\nplt.xlabel('Accuracy Test Score (%)')","23756370":"np.random.seed(0)\nfrom sklearn import model_selection\nX_train, X_val, y_train, y_val = model_selection.train_test_split(df_train,target, test_size = 0.2, train_size = 0.8, random_state = 0 )\n#print(X_train.shape,y_train.shape,X_val.shape, y_val.shape)","64e078e0":"from sklearn import ensemble\nalg = ensemble.GradientBoostingClassifier(n_estimators= 100, random_state = 0)","7aab4683":"train_score = np.mean(model_selection.cross_val_score(alg, X_train, y_train, cv=5))\n\nalg.fit(X_train, y_train)\nsurvived = alg.predict(X_val)\n\nprint(train_score)\n#print(test_score)\n#print(survived.shape,y_val.shape,X_val.shape)\nequals = survived == y_val.values.reshape(*survived.shape)\nprint(f\"Accuracy = {np.mean(equals.astype(int))*100:.3f}%\")","f5399a28":"survived_test = alg.predict(df_test)\nsubmission = pd.DataFrame({'PassengerId':df_test.index,'Survived':survived_test})\n#submission.to_csv('.\/submission.csv',index=False)\n!ls","e3369a96":"## Reading the input","bc1c2819":"## Data handling","dd976114":"## Model Comparison","b3f30fa1":"Resplit the data just to make sure there aren't any discrepancies. Plus if you don't perform the comparison, you can directly run from here.","27691dae":"Because we haven't set a lot of parameters in our algorithms list, we'll have a lot of warnings popup.<br>\n(hiding warnings just for the aesthetics)","d4c609f6":"We drop the columns that are irrelevant \/  have no effect on our model.","dc6f3e3d":"Let's look at a summary of the data info.","9d1b2175":"Importing various model packages.","33aa1f30":"Let's fill in the null values and remove categorical data using `.fillna()` and `.get_dummies()` respectively.","d890de92":"We iterate through the algorithms list and the store the details about them. Look at the table once it's ready.","3c955c50":"## Predictions","cc8f1877":"## Plotting libraries","a80d213e":"To figure out the number of null values in each data set.","6df2477f":"Since GradientBoostingClassifier turns out to be the most effective we use it to make predictions. <br>Fine tuning the hyperparameters can be done now.","dd9a15d6":"## Model Definition","f9a8c212":"## Method 2: Using Sklearn","e3dd5ac7":"We compare various algorithms. `GradientBoostingClassifier` turns out to have least test accuracy.","968651e6":"List out all the algorithms to be compared. ","fd246615":"## Predictions","3f1be039":"## Method 1: Neural Network with Pytorch","f637f28c":"Import the ensemble module from sklearn library. Set up the model along with parameters and hyperparameters.","a85429c5":"## Training the model","4cc0a5c5":"Convert the dataframes into torch tensors to be used for our model. We also perform dev\/train\/test splits for cross-validation of our model. ","ed549d0d":"Helper functions `evaluate`. Returns Test loss during validation\/testing else the output using `one hot encoding` ","c35c33ce":"We create a dataframe to compare our algorithms. The columns are as follows.","82ac9e6a":"We use the simpler `cross_val_score` now since we just need accuracy now. <br>\nAfter we fit out data on the training set we check to validate how well it generalizes.","c2649241":"Plotting the testing accuracy of various models.","f446853d":"We define our model as a `3 layer network`. <br>\nLINEAR -> RELU -> DROPOUT -> LINEAR -> RELU -> DROPOUT ->LINEAR -> LOGSOFTMAX <br>\nWith `ReLU` activations and a dropout probaility of 0.2, and the output using `LogSoftmax` activation.  <br>\nWe use negative log likelihood loss as our criterion. We use `Adam` optimizer with learning rate `0.003`.","0de019cf":"## Gradient Boosting Classifier","5e2a11f7":"Since we have a dataframe with workable values, we redistribute the data set into training and test sets. And drop the differentiating temporary column `Training_set`.","da484303":"## Training","1cdd16a8":"For the purpose of data manipulation, we concatenate the test and training sets, so that all changes are made equivalently on each of them. To differentiate between test and training, we mark them with a new temporary `Training_set` column `True` for the training set and `False` for the test set.","b78fe5ce":"Train test(validation) splitting."}}