{"cell_type":{"1c8a2ae8":"code","62da20b8":"code","79c60f7f":"code","75c581ec":"code","a8dcab8c":"code","e8d54e38":"code","30dd35cf":"code","277dc434":"code","caaaf73a":"code","91a7c195":"code","e1970251":"code","542bd734":"code","e5479012":"code","df11b1ea":"markdown","e06ecf9f":"markdown","960fec9f":"markdown","08e0773b":"markdown","9b4cebc3":"markdown","63d61c16":"markdown","3c8ee40f":"markdown","22a2cd29":"markdown","dc46637d":"markdown","3cbd8d92":"markdown","978a285b":"markdown","7b450867":"markdown","1439000b":"markdown"},"source":{"1c8a2ae8":"import numpy as np\nimport pandas as pd","62da20b8":"%time df_train = pd.read_csv('..\/input\/train.csv', dtype = {'acoustic_data': np.int16, 'time_to_failure': np.float32} ) # float32 is enough :)","79c60f7f":"ttf = df_train['time_to_failure'].values\nindex_start = np.nonzero(np.diff(ttf) > 0)[0] + 1\nindex_start = np.insert(index_start, 0, 0) # insert 1st period start manually\nprint(index_start)","75c581ec":"chunk_length = np.diff(np.append(index_start, df_train.shape[0]))\nprint(chunk_length)","a8dcab8c":"t_start = ttf[index_start]\nt_end = ttf[index_start + chunk_length - 1]\nprint('t_start =', t_start)\nprint('t_end   =', t_end)","e8d54e38":"dt_step = (t_start - t_end) \/ chunk_length\ndt_step","30dd35cf":"linsp_diff_max = []\nfor i in range(len(index_start)):\n    ttf_orig  = ttf[index_start[i] : index_start[i] + chunk_length[i]]\n    ttf_linsp = np.linspace(t_start[i], t_end[i], chunk_length[i])\n    linsp_diff_max.append(np.abs(ttf_orig - ttf_linsp).max())\nlinsp_diff_max = np.array(linsp_diff_max)\nprint(linsp_diff_max)\nprint('Max error =', linsp_diff_max.max())","277dc434":"df_train_info = pd.DataFrame({\n    'index_start':index_start,\n    'chunk_length':chunk_length,\n    't_start':t_start,\n    't_end':t_end,\n    'dt_step':dt_step,\n    'linsp_diff_max':linsp_diff_max\n})\ndf_train_info.to_csv('train_info.csv', index=False)\ndf_train_info","caaaf73a":"%time np.savez_compressed('train_acoustic_data.npz', acoustic_data=df_train['acoustic_data'].values)\n%ls -lh","91a7c195":"%%time\ndf_train_info = pd.read_csv('train_info.csv')\nac_data = np.load('train_acoustic_data.npz')['acoustic_data']\n\ndef get_quake_period(i):\n    index_start, chunk_length = df_train_info['index_start'][i], df_train_info['chunk_length'][i]\n    t_start, t_end = df_train_info['t_start'][i], df_train_info['t_end'][i]\n    ac_data_period = ac_data[ index_start : index_start + chunk_length ]\n    ttf_data_period = np.linspace(t_start, t_end, chunk_length, dtype=np.float32)\n    return ac_data_period, ttf_data_period","e1970251":"get_quake_period(3)","542bd734":"df_ssub = pd.read_csv('..\/input\/sample_submission.csv')\ndf_ssub.tail()","e5479012":"ac_data = []\n\nfor fname in df_ssub['seg_id'].values:\n    ac_data.append(pd.read_csv('..\/input\/test\/' + fname + '.csv').acoustic_data.values.astype(np.int16))\n    \nac_data = np.array(ac_data)\n\nnp.savez_compressed('test_acoustic_data.npz', acoustic_data=ac_data)\n\nprint('test data shape :', ac_data.shape)","df11b1ea":"What are the time_to_failure values of the 1st and last data point in each period?","e06ecf9f":"Well, almost: let's bring the huge amount of litte test files into a manageable format as well:","960fec9f":"How many data points are in each period?","08e0773b":"That's all. Happy Predicting!","9b4cebc3":"Let's save the acoustic data in compressed form. This is only done once. Yep, we're below 400MB:","63d61c16":"Let's collect all of this into a dataframe which will be saved (we will need it again to generate time_to_failure!)","3c8ee40f":"This kernel does not predict anything. Instead, it tries to address a problem that many newcomers to this competition may have: **The training data is *huge*, simply reading the full csv at once may be too much**. If you want to play around with the data on a local machine, you need something better. Fortunately, the data can be encoded much more efficiently:\n* time_to_failure is *for all practical purposes* (see explanation below) linear and does not need to be saved. It can be generated by numpy.linspace as needed\n* acoustic_data can be saved in a compressed npz file\n\nThe resulting file will be ~400MB in total, can be easily saved in the cloud, contains the full acoustic_data and can be loaded in ~ 10 sec on most machines.\n\n**Run this notebook as a Kaggle kernel and download the output files *train_acoustic_data.npz* and *train_info.csv* .**\n\nEDIT: 2019-04-17: added the same procedure for the test data as well.","22a2cd29":"Yes, that took forever.\n\nThere are 17 \"earthquake periods\" (the last one ends long before its quake). Let's find the beginning of each period:","dc46637d":"Now we're done. From now on, you only need *train_acoustic_data.npz* , *train_info.csv* and a function to extract a quake period and generate the time_to_failure data:","3cbd8d92":"Pack the 2624 test segments into a 2624 x 150000 numpy array and save it:","978a285b":"Now it gets interesting. If we approximate the time_to_failure column for each period with a numpy.linspace from t_start to t_end, what is the maximal difference (i.e. error) between this approximation and the original data? Result: 1.15 ms. **This is 3 orders of magnitude below the prediction accuracy that we can reasonably hope to achieve (O(1 sec)). We don't need the original time_to_failure column anymore.**","7b450867":"Assuming that the acoustic_data is contiguous (that's probably not strictly true, see the discussions, but it doesn't really matter anyway), what is the length of the time steps between data points in each period? The values should be approximately equal, and they are:","1439000b":"Let's extract period #3:"}}