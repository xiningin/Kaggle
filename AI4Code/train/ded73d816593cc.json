{"cell_type":{"c71f1b5c":"code","ce69b938":"code","8b9e9b05":"code","de79b966":"code","52fdafcf":"code","5f1ea861":"code","059fbec7":"code","ee80b972":"code","e133a187":"code","593432bc":"code","6c89131c":"code","6aeabae4":"code","db1e8e7c":"code","011f84f7":"code","df921518":"code","3d477ef6":"code","c2220691":"code","4d84d465":"code","3ab4a7c7":"code","1a8e8aff":"code","01aa0c0b":"code","4f60460d":"code","3df21f45":"code","50356b37":"code","835bbee8":"code","f3a02850":"code","7694d087":"code","a2bf31fb":"code","4cafd0fb":"code","77d6846d":"code","71e7a7f2":"code","06fa8809":"code","6757b28d":"code","f91fdd14":"code","3e80cedd":"code","6553532e":"code","6c488b62":"code","27e4e322":"code","da9dfc92":"code","585bf849":"code","2921ec9c":"code","5dfe8455":"code","570af057":"code","fa8c6efa":"code","a883b3ee":"code","e16d6fbc":"code","e772f91e":"code","fd33f796":"code","a1a6e6ff":"code","667096c0":"code","958bfabb":"code","e19c6154":"code","45a95c10":"code","a2e66f99":"code","3decc84a":"code","731db9fb":"code","1216352d":"code","07ac9d5e":"code","e601cbaa":"code","25d0f92c":"code","bb03f5de":"code","1faa1c62":"code","d3c65fc2":"code","4e8a58c3":"code","269a9531":"code","0cd85e2a":"code","fbdd4e47":"code","d8b4c4dc":"code","83c1aa72":"code","bf760ac5":"code","72f0d0ed":"code","712411ee":"code","4b5bcd2c":"code","cb19c07b":"code","da0e25e5":"code","a5ad57b9":"code","81eae7de":"code","6a31e564":"code","c09efc9b":"code","119ffe5b":"code","fbbc7270":"code","eb583e53":"code","9539dab2":"code","ff501df5":"code","56170961":"code","947f4fe8":"code","371f785d":"code","f3a40953":"code","dc6437c7":"code","d549676d":"code","a65945c9":"code","584cbdd8":"code","3c59416d":"code","7b44e10c":"markdown","d047b98e":"markdown","fb49633a":"markdown","7921cd42":"markdown","9958e642":"markdown","9a3d46b7":"markdown","d33b9a8f":"markdown","fe588302":"markdown","a4ce274e":"markdown","20046232":"markdown","7c5afb5c":"markdown","0deb842d":"markdown","9bca85ec":"markdown","94e50ce6":"markdown","b901ef62":"markdown","65d32f3b":"markdown","9ea6d072":"markdown","4383c975":"markdown","7957a101":"markdown","47c0fbb9":"markdown","d5452a27":"markdown","e4e35c57":"markdown","abc536a3":"markdown","7566ee00":"markdown","df410314":"markdown","712c445f":"markdown","0a3223a7":"markdown","bad0ef91":"markdown","38b20902":"markdown","2401b022":"markdown","582cf551":"markdown","d3ebd091":"markdown","df3befbf":"markdown","c4b68da8":"markdown"},"source":{"c71f1b5c":"# for reading data\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport scipy.stats as stat\nimport matplotlib.pyplot as plt \n%matplotlib inline\nfrom scipy import stats\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import MinMaxScaler\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom keras.utils import np_utils\n\n# For data splitting\nfrom sklearn.model_selection import train_test_split\n\n# import other functions we'll need for regression modeling\nfrom sklearn.linear_model import LogisticRegression # LR\nfrom sklearn.tree import DecisionTreeRegressor # DTR\nfrom sklearn.ensemble import RandomForestRegressor # RFR\nfrom sklearn.ensemble import GradientBoostingRegressor #GBR\n\n# regression error metrics\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import r2_score\n# classification error metrics\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\n\n# for modeling\nfrom keras import models\nfrom keras import layers\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout\nfrom keras.callbacks import EarlyStopping\nfrom keras import regularizers","ce69b938":"df = pd.read_csv(\"..\/input\/the-movies-dataset\/movies_metadata.csv\")","8b9e9b05":"#Summary of original dataset\ndf.info()","de79b966":"#Shape of original dataset\ndf.shape","52fdafcf":"#Let's look into first few rows of the dataset to check the content of each column\ndf.head()","5f1ea861":"# As I will be concentrating on nuerical columns let's delete the text columns, which I donot need for solving the business problem\ndf.drop(['id','belongs_to_collection', 'homepage', 'imdb_id', 'original_language', 'original_title', 'overview', 'poster_path', 'production_companies', 'production_countries', 'runtime', 'release_date', 'spoken_languages', 'status', 'tagline', 'title', 'video'], axis=1, inplace=True)\ndf.info()","059fbec7":"df[df['revenue'] == 0].shape","ee80b972":"df['revenue'] = df['revenue'].replace(0, np.nan)","e133a187":"df['budget'] = pd.to_numeric(df['budget'], errors='coerce')\ndf['budget'] = df['budget'].replace(0, np.nan)\ndf[df['budget'].isnull()].shape","593432bc":"df['return'] = df['revenue'] \/ df['budget']\ndf[df['return'].isnull()].shape","6c89131c":"df['adult'].value_counts()","6aeabae4":"df = df.drop('adult', axis=1)","db1e8e7c":"def clean_numeric(x):\n    try:\n        return float(x)\n    except:\n        return np.nan","011f84f7":"df['popularity'] = df['popularity'].apply(clean_numeric).astype('float')\ndf[\"vote_count\"] = df[\"vote_count\"].apply(clean_numeric).astype('float')\ndf['vote_average'] = df['vote_average'].apply(clean_numeric).astype('float')","df921518":"#Summary statistics of each feature\ndf['popularity'].describe()","3d477ef6":"sns.distplot(df['popularity'].fillna(df['popularity'].median()))\nplt.show()","c2220691":"df['popularity'].plot(logy=True, kind='hist')","4d84d465":"df['vote_count'].describe()","3ab4a7c7":"df['vote_average'] = df['vote_average'].replace(0, np.nan)\ndf['vote_average'].describe()","1a8e8aff":"sns.distplot(df['vote_average'].fillna(df['vote_average'].median()))","01aa0c0b":"df['budget'].describe()","4f60460d":"sns.distplot(df[df['budget'].notnull()]['budget'])","3df21f45":"df['budget'].plot(logy=True, kind='hist')","50356b37":"df['revenue'].describe()","835bbee8":"sns.distplot(df[df['revenue'].notnull()]['revenue'])","f3a02850":"#Let's check the number of missing values in the dataset\ndf.isnull().sum()","7694d087":"#Dropping rows with missing values\ndf.dropna(inplace=True)\n#Check the number of missing values to ensure we have none\ndf.isnull().sum()","a2bf31fb":"df[\"popularity\"] = np.round(pd.to_numeric(df.popularity, errors='coerce'),2)\ndf.info()\ndf.shape","4cafd0fb":"#converting 'genre' column into panda series and extract the type of the genre only from the column\ns = pd.Series(df['genres'], dtype= str)\ns1=s.str.split(pat=\"'\",expand=True)\ndf['genre_ed']=s1[5]","77d6846d":"#count of each genre in the dataset\ndf['genre_ed'].value_counts()","71e7a7f2":"#Remove rows for genres with count less than 100\ndf=df[~df['genre_ed'].isin(['Mystery', 'Family', 'Documentary', 'War', 'Music', 'Western', 'History', 'Foreign', 'TV Movie'])]","06fa8809":"df.drop(df[df['budget'] < 1000000].index, inplace=True)\n#df.drop(df[df['vote_count'] < 100].index, inplace=True)\ndf.drop(df[df['revenue'] < 2000000].index, inplace=True)\n#df.drop(df[df['vote_average'] == 0].index, inplace=True)\ndf.shape","6757b28d":"#Drop original column from dataset\ndf.drop(['genres'], axis=1, inplace=True)\n#get dummy columns for genre\ndf= pd.get_dummies(df, columns=[\"genre_ed\"])\ndf.info()\ndf.head()","f91fdd14":"#Summary statistics of dataset \ndf.describe()","3e80cedd":"#Let's check for outliers\ndf.boxplot(column=['budget', 'revenue'])","6553532e":"df.boxplot(column=['popularity', 'vote_average', 'vote_count'])","6c488b62":"df_o= df[(np.nan_to_num(np.abs(stats.zscore(df,nan_policy='omit')),0) < 3).all(axis=1)]","27e4e322":"df.shape\nprint ('Shape of original input dataset:', df.shape)\n#After removing outliers\ndf_o.shape\nprint ('Shape of input dataset after removing outliers:', df_o.shape)","da9dfc92":"Q1 = df_o.quantile(0.25)\nQ3 = df_o.quantile(0.75)\nIQR = Q3 - Q1\nprint(IQR)","585bf849":"df_out = df_o[~((df_o < (Q1 - 1.5 * IQR)) |(df_o > (Q3 + 1.5 * IQR))).any(axis=1)]","2921ec9c":"print ('Shape of input dataset after managing misspread:', df_out.shape)","5dfe8455":"df_out.boxplot(column=['budget', 'revenue'])","570af057":"df_out_high = df_out.apply(lambda x : True\n            if x['revenue'] > 13500000 or x['budget'] > 30000000 else False, axis = 1) \nnum_rows = len(df_out_high[df_out_high == True].index) \n  \nprint('Number of Rows in dataframe with revenue more than 13.5 million dollars or budget more than 30 million: ', \n      num_rows ) ","fa8c6efa":"df_out.drop(df_out[df_out['revenue'] > 13500000].index, inplace=True)\ndf_out.drop(df_out[df_out['budget'] > 30000000].index, inplace=True)","a883b3ee":"df_out.boxplot(column=['budget', 'revenue'])","e16d6fbc":"df_out['revenue'].describe()","e772f91e":"# recode the revenue into high and low\ndf_out['Revenue'] = 0\ndf_out.loc[df_out['revenue'] <7017731,'Revenue'] = 0\ndf_out.loc[df_out['revenue'] >=7017731,'Revenue'] = 1\ndf_out.head()","fd33f796":"# check distribution of target variable\ndf_out['Revenue'].value_counts()","a1a6e6ff":"df_out.drop(['revenue'],axis=1, inplace=True)","667096c0":"df_out","958bfabb":"# Assign X and Y\nX = df_out.drop(['Revenue'], axis=1)\ny = df_out['Revenue']\n\nprint(X.shape)\nprint(y.shape)","e19c6154":"import imblearn.under_sampling as u\n# example of random oversampling to balance the class distribution\nfrom collections import Counter\nfrom sklearn.datasets import make_classification\nfrom imblearn.over_sampling import RandomOverSampler\n\nX,y = make_classification(n_features = 16, n_samples=312) \n# Make classification default = 20, so I need to set the number\nros = u.RandomUnderSampler(sampling_strategy='majority')\nX_resampled, Y_resampled = ros.fit_resample(X, y)","45a95c10":"X_resampled = pd.DataFrame(X_resampled)\nprint(X_resampled.shape)\n\nY_resampled = pd.DataFrame(Y_resampled)\nprint(Y_resampled.shape)","a2e66f99":"X_resampled.columns = ['budget', 'popularity', 'vote_average', 'vote_count','return',\n       'genre_ed_Action', 'genre_ed_Adventure', 'genre_ed_Animation',\n       'genre_ed_Comedy', 'genre_ed_Crime', 'genre_ed_Drama',\n       'genre_ed_Fantasy', 'genre_ed_Horror', 'genre_ed_Romance',\n       'genre_ed_Science Fiction', 'genre_ed_Thriller']","3decc84a":"X_resampled","731db9fb":"# I will be performing 80\/20 split\nX_train, X_test, y_train, y_test = train_test_split(X_resampled, Y_resampled,\n                                                    test_size = 0.20,\n                                                    shuffle = True,\n                                                    random_state = 42)","1216352d":"X_train","07ac9d5e":"# check your work - does the shape match what you think it should be?\nprint(X_train.shape, X_test.shape, y_train.shape, y_test.shape)","e601cbaa":"#Fit the model\n# make a variable to store the general model\nLR = LogisticRegression()\n# fit the model - one line of code\nLR = LR.fit(X_train, y_train)","25d0f92c":"# store the predictions\ntrain_preds_LR = LR.predict(X_train) \ntest_preds_LR = LR.predict(X_test) ","bb03f5de":"#Evaluate the model\n# train confusion matrix\nconfusion_matrix(y_train, train_preds_LR)","1faa1c62":"# test confusion matrix\nconfusion_matrix(y_test, test_preds_LR)","d3c65fc2":"# extract TP, TN, FP, FN\ntn, fp, fn, tp = confusion_matrix(y_test, test_preds_LR).ravel()\n(tn, fp, fn, tp)","4e8a58c3":"print(classification_report(y_test, test_preds_LR))\nplt.savefig('baseline_accuracy.png') ","269a9531":"cm = confusion_matrix(y_test, test_preds_LR)\nplt.clf()\nplt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\nclassNames = ['False','True']\nplt.title('Logistic Regression accuracy')\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\ntick_marks = np.arange(len(classNames))\nplt.xticks(tick_marks, classNames)\nplt.yticks(tick_marks, classNames)\ns = [['TN','FP'], ['FN', 'TP']]\nfor i in range(2):\n    for j in range(2):\n        plt.text(j,i, str(s[i][j])+\" = \"+str(cm[i][j]))\n\n# plt.savefig(\"Logistic_regression_accuracy.png\")\n# images_dir = '\/content\/drive\/Shared drives\/Deep Learning Group Projects\/Project #1 \/Work'\n# plt.savefig(f\"{images_dir}\/Logistic_regression_accuracy.png\")\n# plt.show()","0cd85e2a":"# Setting up the model\nmodel1 = Sequential()\n# this is hidden layer 1\nmodel1.add(Dense(50,activation='relu', input_shape=(X.shape[1],))) # input shape is = (features,)\n# this is hidden layer 2\nmodel1.add(Dense(25, activation='relu'))\n# this is hidden layer 3\nmodel1.add(Dense(15, activation='relu'))\n# this is hidden layer 4\nmodel1.add(Dense(10, activation='relu'))\n# this is the output node\nmodel1.add(Dense(1, activation='sigmoid')) # the activation function here is 'linear' by default\nmodel1.summary()","fbdd4e47":"#  this compiles the model, specifies model evaluation metrics\nmodel1.compile(optimizer='Adam', loss='binary_crossentropy',\n              metrics=['accuracy'])","d8b4c4dc":"from keras.callbacks import EarlyStopping\nes = EarlyStopping(monitor='val_loss', mode='min', \n                   patience=10, \n                   verbose=1,\n                   restore_best_weights=True)\n# fit model\nhistory = model1.fit(X_train, y_train, \n                    validation_data=(X_test, y_test),\n                    epochs=4000, \n                    batch_size = 44,\n                    verbose=1, \n                    callbacks=[es]) #notice we won't have to manually watch it","83c1aa72":"history_dict = history.history\nhistory_dict.keys() \n\n# out of all of these, let's plot the val_mean_absolute_error","bf760ac5":"acc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = range(1, len(acc) + 1)\n\n# \"bo\" is for \"blue dot\"\nplt.plot(epochs, loss, 'bo', label='Training loss')\n# b is for \"solid blue line\"\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\n\n# plt.savefig(\"Test_valid_loss_DDN_withoutdropout.png\")\n# images_dir = '\/content\/drive\/Shared drives\/Deep Learning Group Projects\/Project #1 \/Work'\n# plt.savefig(f\"{images_dir}\/Test_valid_loss_DDN_withoutdropout.png\")\n# plt.show()","72f0d0ed":"plt.clf()   # clear figure\nacc_values = history_dict['accuracy']\nval_acc_values = history_dict['val_accuracy']\n\nplt.plot(epochs, acc, 'bo', label='Training acc')\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\n\n# plt.savefig(\"Accuracy_DDN_withoutdropout.png\")\n# images_dir = '\/content\/drive\/Shared drives\/Deep Learning Group Projects\/Project #1 \/Work'\n# plt.savefig(f\"{images_dir}\/Accuracy_DDN_withoutdropout.png\")\n# plt.show()","712411ee":"# see how the model did!\n# if you don't round to a whole number (0 or 1), the confusion matrix won't work!\npreds = np.round(model1.predict(X_test),0)\n\n# confusion matrix\nconfusion_matrix(y_test, preds) # order matters! (actual, predicted)\n\n# TP is bottom right\n# TN is top left\n# FP is top right\n# FN is bottom left\n\n# look at documentation for conf matrix on sklearn if you have questions!","4b5bcd2c":"print(classification_report(y_test, preds))","cb19c07b":"cm = confusion_matrix(y_test, preds)\nplt.clf()\nplt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\nclassNames = ['False','True']\nplt.title('DNN without callback')\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\ntick_marks = np.arange(len(classNames))\nplt.xticks(tick_marks, classNames)\nplt.yticks(tick_marks, classNames)\ns = [['TN','FP'], ['FN', 'TP']]\nfor i in range(2):\n    for j in range(2):\n        plt.text(j,i, str(s[i][j])+\" = \"+str(cm[i][j]))\n\n# plt.savefig(\"DNN_withoutdropout_confusion_matrix.png\")\n# images_dir = '\/content\/drive\/Shared drives\/Deep Learning Group Projects\/Project #1 \/Work'\n# plt.savefig(f\"{images_dir}\/DNN_withoutdropout_confusion_matrix.png\")\n# plt.show()","da0e25e5":"# Setting up the model\nmodel2 = Sequential()\n# this is hidden layer 1\nmodel2.add(Dense(50,activation='relu', input_shape=(X.shape[1],))) # input shape is = (features,)\nmodel2.add(Dropout(0.4)) # Dropout Layer 1\n# this is hidden layer 2\nmodel2.add(Dense(25, activation='relu'))\nmodel2.add(Dropout(0.3)) #Dropout Layer 2\n# this is hidden layer 3\nmodel2.add(Dense(10, activation='relu'))\nmodel2.add(Dropout(0.2)) #Dropout Layer 3\n# this is the output node\nmodel2.add(Dense(1, activation='sigmoid')) # the activation function here is 'linear' by default\nmodel2.summary()","a5ad57b9":"#  this compiles the model, specifies model evaluation metrics\nmodel2.compile(optimizer='Adam', loss='binary_crossentropy',\n              metrics=['accuracy'])","81eae7de":"from keras.callbacks import EarlyStopping\nes2 = EarlyStopping(monitor='val_loss', mode='min', \n                   patience=10, \n                   verbose=1,\n                   restore_best_weights=True)\n# fit model\nhistory2 = model2.fit(X_train, y_train, \n                    validation_data=(X_test, y_test),\n                    epochs=4000, \n                    batch_size = 44,\n                    verbose=1, \n                    callbacks=[es2]) #notice we won't have to manually watch it","6a31e564":"history_dict2 = history2.history\nhistory_dict2.keys() \n\n# out of all of these, let's plot the val_mean_absolute_error","c09efc9b":"acc2 = history2.history['accuracy']\nval_acc2 = history2.history['val_accuracy']\nloss2 = history2.history['loss']\nval_loss2 = history2.history['val_loss']\n\nepochs2 = range(1, len(acc2) + 1)\n\n# \"bo\" is for \"blue dot\"\nplt.plot(epochs2, loss2, 'bo', label='Training loss')\n# b is for \"solid blue line\"\nplt.plot(epochs2, val_loss2, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\n\n# plt.savefig(\"trial1_Test_valid_loss_DDN_withdropout.png\")\n# images_dir = '\/content\/drive\/Shared drives\/Deep Learning Group Projects\/Project #1 \/Work'\n# plt.savefig(f\"{images_dir}\/trial1_Test_valid_loss_DDN_withdropout.png\")\n","119ffe5b":"plt.clf()   # clear figure\nacc_values2 = history_dict2['accuracy']\nval_acc_values2 = history_dict2['val_accuracy']\n\nplt.plot(epochs2, acc2, 'bo', label='Training acc')\nplt.plot(epochs2, val_acc2, 'b', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\n\n# plt.savefig(\"trial1_accuracy_DDN_withdropout.png\")\n# images_dir = '\/content\/drive\/Shared drives\/Deep Learning Group Projects\/Project #1 \/Work'\n# plt.savefig(f\"{images_dir}\/trial1_accuracy_DDN_withdropout.png\")","fbbc7270":"# see how the model did!\n# if you don't round to a whole number (0 or 1), the confusion matrix won't work!\npreds2 = np.round(model2.predict(X_test),0)\n\n# confusion matrix\nconfusion_matrix(y_test, preds2) # order matters! (actual, predicted)\n\n# TP is bottom right\n# TN is top left\n# FP is top right\n# FN is bottom left\n\n# look at documentation for conf matrix on sklearn if you have questions!","eb583e53":"print(classification_report(y_test, preds2))","9539dab2":"cm = confusion_matrix(y_test, preds2)\nplt.clf()\nplt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\nclassNames = ['False','True']\nplt.title('DNN without callback')\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\ntick_marks = np.arange(len(classNames))\nplt.xticks(tick_marks, classNames)\nplt.yticks(tick_marks, classNames)\ns = [['TN','FP'], ['FN', 'TP']]\nfor i in range(2):\n    for j in range(2):\n        plt.text(j,i, str(s[i][j])+\" = \"+str(cm[i][j]))\n\n# plt.savefig(\"Trail1_DNN_withdropout_confusion_matrix.png\")\n# images_dir = '\/content\/drive\/Shared drives\/Deep Learning Group Projects\/Project #1 \/Work'\n# plt.savefig(f\"{images_dir}\/Trail1_DNN_withdropout_confusion_matrix.png\")","ff501df5":"# Setting up the model\nmodel3 = Sequential()\n# this is hidden layer 1\nmodel3.add(Dense(20,activation='relu', input_shape=(X.shape[1],))) # input shape is = (features,)\n# this is hidden layer 2\nmodel3.add(Dense(50, activation='relu'))\n# this is hidden layer 3\nmodel3.add(Dense(50, activation='relu'))\n# this is hidden layer 4\nmodel3.add(Dense(20, activation='relu'))\n# this is the output node\nmodel3.add(Dense(1, activation='sigmoid')) # the activation function here is 'linear' by default\nmodel3.summary()","56170961":"#  this compiles the model, specifies model evaluation metrics\nmodel3.compile(optimizer='Adam', loss='binary_crossentropy',\n              metrics=['accuracy'])","947f4fe8":"from keras.callbacks import EarlyStopping\nes3 = EarlyStopping(monitor='val_loss', mode='min', \n                   patience=10, \n                   verbose=1,\n                   restore_best_weights=True)\n# fit model\nhistory3 = model3.fit(X_train, y_train, \n                    validation_data=(X_test, y_test),\n                    epochs=4000, \n                    batch_size = 44,\n                    verbose=1, \n                    callbacks=[es3]) #notice we won't have to manually watch it","371f785d":"history_dict3 = history3.history\nhistory_dict3.keys() \n\n# out of all of these, let's plot the val_mean_absolute_error","f3a40953":"acc3 = history3.history['accuracy']\nval_acc3 = history3.history['val_accuracy']\nloss3 = history3.history['loss']\nval_loss3 = history3.history['val_loss']\n\nepochs3 = range(1, len(acc3) + 1)\n\n# \"bo\" is for \"blue dot\"\nplt.plot(epochs3, loss3, 'bo', label='Training loss')\n# b is for \"solid blue line\"\nplt.plot(epochs3, val_loss3, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\n\n# plt.savefig(\"trial2_train_valid__loss.png\")\n# images_dir = '\/content\/drive\/Shared drives\/Deep Learning Group Projects\/Project #1 \/Work'\n# plt.savefig(f\"{images_dir}\/trial2_train_valid__loss.png\")\n","dc6437c7":"plt.clf()   # clear figure\nacc_values3 = history_dict3['accuracy']\nval_acc_values3 = history_dict3['val_accuracy']\n\nplt.plot(epochs3, acc3, 'bo', label='Training acc')\nplt.plot(epochs3, val_acc3, 'b', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\n\n# plt.savefig(\"trial2_accuracy.png\")\n# images_dir = '\/content\/drive\/Shared drives\/Deep Learning Group Projects\/Project #1 \/Work'\n# plt.savefig(f\"{images_dir}\/trial2_accuracy.png\")","d549676d":"# see how the model did!\n# if you don't round to a whole number (0 or 1), the confusion matrix won't work!\npreds3 = np.round(model3.predict(X_test),0)\n\n# confusion matrix\nconfusion_matrix(y_test, preds3) # order matters! (actual, predicted)\n\n# TP is bottom right\n# TN is top left\n# FP is top right\n# FN is bottom left\n\n# look at documentation for conf matrix on sklearn if you have questions!","a65945c9":"print(classification_report(y_test, preds3))","584cbdd8":"cm = confusion_matrix(y_test, preds3)\nplt.clf()\nplt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\nclassNames = ['False','True']\nplt.title('DNN without callback')\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\ntick_marks = np.arange(len(classNames))\nplt.xticks(tick_marks, classNames)\nplt.yticks(tick_marks, classNames)\ns = [['TN','FP'], ['FN', 'TP']]\nfor i in range(2):\n    for j in range(2):\n        plt.text(j,i, str(s[i][j])+\" = \"+str(cm[i][j]))\n\n# plt.savefig(\"Trail2_confusion_matrix.png\")\n# images_dir = '\/content\/drive\/Shared drives\/Deep Learning Group Projects\/Project #1 \/Work'\n# plt.savefig(f\"{images_dir}\/Trail2_confusion_matrix.png\")\nplt.show()","3c59416d":"#Feature Importance for NN\n\nfrom sklearn.inspection import permutation_importance\n\nresults = permutation_importance(model2, X_train, y_train, scoring='neg_root_mean_squared_error')\n\nplt.figure(figsize=(10,10))\n\n#get importance\nimportance_nn = results.importances_mean\nsorted_idx = np.argsort(importance_nn)\npos = np.arange(sorted_idx.shape[0]) + .5\nplt.barh(pos, importance_nn[sorted_idx],align='center')\n\nplt.yticks(pos, X_train.columns[sorted_idx],fontsize=25)\nplt.xlabel('Permutation Feature Importance Scores', fontsize=25)\n#plt.xticks(fontsize=100)\nplt.title('Permutation Feature Importance for Neural Network', fontsize=30)\n\nplt.tight_layout()\n\nplt.show()","7b44e10c":"As mentioned earlier our original dataset has lot of missing values but our analysis can still be done with the small set of rows as well. I will be further exploring these in our outlier detection section.","d047b98e":"I will create a return feature that will help us capture a more accurate picture of the financial success of a movie. Presently, our data will not be able to judge if a 200 million dollar budget movie that earned 100 million dollar did better than a 50,000 dollars budget movie taking in 200,000 dollars. This feature will be able to capture that information.\n\nA return value > 1 would indicate profit whereas a return value < 1 would indicate a loss.","fb49633a":"# 4-1) Baseline Model: Logistic Regression","7921cd42":"# 4-4) Deep Learning Model #3: Dense Neural Network","9958e642":"I will perform the following feature engineering tasks:\n\n1. Round up popularity values to 2 decimal values and I will be converting those into integer datatypes.\n2. I will be creating dummy variables for column: genres","9a3d46b7":"The dataset has most rows for Drama and Comedy. I will concentrate only gneres which has more than 100 rows","d33b9a8f":"The Popularity score seems to be an extremely skewed quantity with a mean of only 2.9 but maximum values reaching as high as 547, which is almost 1800% greater than the mean. However, as can be seen from the distribution plot, almost all movies have a popularity score less than 10 (the 75th percentile is at 3.678902).","fe588302":"The budget feature has some unclean values that makes Pandas assign it as a generic object. I proceed to convert this into a numeric variable and replace all the non-numeric values with NaN. Finally, as with budget, I will convert all the values of 0 with NaN to indicate the absence of information regarding budget.","a4ce274e":"The distribution of revenue undergoes exponential decay just like budget.","20046232":"# Conclusion\nIn this analysis of the data, I have identified multiple key factors to that influence movie revenue.\n* Despite having an abundance of budget, Movie production houses struggle to make profits.It is I hope that the provided analysis gives production houses the tools and inspiration to rethink some of their current practices and implement actions that will help them focus on attracting right audience for their upcoming movies.\n* During Exploratory Data Analysis I found that Vote_count would be the most significant feature identified by the classifier. Hence, production houses should invest more in marketing and advertisement of their movies well ahead of time.\n* I found which variables can significant impact on the accuracy from the permutation featuring for DNN. \u2018Vote_Average\u2019 is the most important feature for increasing movie revenue. Thus, movie industries should consider the movie rate consistently.\n* Most of significant variables are consisted from \u2018movie genre\u2019. The most popular genre of movie is \u2018Drama\u2019. And then \u2018Horror\u2019, \u2018Action\u2019, \u2018Animation\u2019, \u2018Fantasy\u2019, \u2018comedy\u2019. Thus, Movie industries should care of which movie genre can be popular and attract more people\n* I assume that \u2018budget\u2019 could be the most important factor to impact on the movie revenue. Surprisingly, \u2018budget\u2019 is less impact factor to predict the success of movies.","7c5afb5c":"# 3) Split Data into X and Y","0deb842d":"Exploring Popularity, Vote Average and Vote Count","9bca85ec":"Looking at the range I can say the dataset has movies which are high budget, low revenue.","94e50ce6":"I see that the majority of the movies have a recorded revenue of 0. This indicates that I do not have information about the total revenue for these movies. Although this forms the majority of the movies available to us, I will still use revenue as an extremely important feature going forward from the remaining 7000 movies.","b901ef62":"Fit at least two reasonable different DL models and one baseline model with clean code (lots of comments).","65d32f3b":"As with popularity scores, the distribution of vote counts is extremely skewed with the median vote count standing at a paltry 10 votes. The most votes a single movie has got stands at 14,07","9ea6d072":"Below are columns of the datasaet:\n\n* adult - Belongs to adult movies or not\n* belongs_to_collection - Belong to movie collections or not\n* budget - The budget of a movie. Some movies don't have this, it appears as 0\n* genres - Main genre of the movie\n* homepage - The website where can see the movie\n* id - Identifier column\n* imdb_id - Movies id on IMDB\n* original_language - Original language of film\n* original_title - Original title of film\n* overview - Movie content overview\n* popularity - shows weather it is popular\n* poster_path - jpg. path of the movie poster\n* production_companies - The production company\n* production_countries - Country of origin\n* release_date - Release date (YYYY-MM-DD)\n* revenue - The revenue of a movie. Some movies don't have this, it appears as 0\n* runtime - Duration of the movie\n* spoken_languages - Languages spoken throughout the film\n* status - Release or others\n* tagline - Movie tagline to advertise\n* title - English title\n* video - True of False\n* vote_average - TMDB vote average\n* vote_count - TMDB vote count","4383c975":"# Movie Data Set\n\nThis dataset is derived from Kaggle \"The Movies Dataset\".I'll be working with \"movies_metadata\", the metadata for over 45,000 movies listed in the Full MovieLens Dataset. The dataset consists of movies released on or before July 2017. Data points include budget, revenue, release dates, languages, production companies, countries, TMDB vote counts and vote averages, etc.\n\nThrough data information, I can see that the original dataset has 45466 entries, and 24 total columns. Most of columns are object value which cannot be directly feed into neural networks. So I am going to process them and change into numeric variables.","7957a101":"The mean gross of a movie is 68.7 million dollars whereas the median gross is much lower at 16.8 million dollars, suggesting the skewed nature of revenue. The lowest revenue generated by a movie is just 1 dollar whereas the highest grossing movie of all time is astonishing 2.78 billion dollars.","47c0fbb9":"There are only 9 adult movies in this dataset. The adult feature therefore is not of much use to us and can be safely dropped.","d5452a27":"The distribution of movie budgets shows an exponential decay. More than 75% of the movies have a budget smaller than 25 million dollars.","e4e35c57":"# Data Gathering and Description\n\nIn this part, I'll set up the environment, import modules, read the data, and explore our data. Through the data processing, I'll change data types to fit models, handle missing values, and detect any outliers. After that, I'll wrangle the data, split data into X and y, and do some scalling preparing for modeling.","abc536a3":"# 4) Modeling","7566ee00":"# 4-2) Deep Learning Model #1: Dense Neural Network","df410314":"I have close to 5000 movies for which I have data on revenue and budget ratio. This is close to 10% of the entire dataset. Although this may seem small, this is enough to perform very useful analysis and discover interesting insights about the world of movies.","712c445f":"# 2) Data Wrangling","0a3223a7":"# 4-3) Deep Learning Model #2: Dense Neural Network","bad0ef91":"# 1) Head, Shape, Column names, data types, missing values, Outliers","38b20902":"It appears that Users in this dataset are extremely strict in their ratings. The mean rating is only a 5.6 on a scale of 10. Half the movies have a rating of less than or equal to 6","2401b022":"# Overview\n\n* 1) Data Preparation\n\n* 2) Exploratory Data Analysis\n\n* 3) Data Wrangling\n\n* 4) Modeling\n\n* 4-1) Baseline Model: Logistic Regression\n\n* 4-2) Deep Learning Model #1: Dense Neural Network (without dropout)\n\n* 4-3) Deep Learning Model #2: Dense Neural Network (with dropout)\n\n* 4-4) Deep Learning Model #3: Dense Neural Network (with dropout but with different layers)\n\n* Permutation Feature Importance for Neural Network\n\n* Results\n\n* Conclusion","582cf551":"The mean budget of a film is 21.6 million dollars whereas the median budget is far smaller at 8 million dollars. This strongly suggests the mean being influenced by outliers.","d3ebd091":"# Results\n\n* I implemented a based line model(logistic regression) and 3 Dense Neural Network models with dropout function and without dropout. Moreover, I adjust dense layers of dense neural network models. The best algorithm suited to solve the business problem is DNN3(Accuracy of 0.97)\n \n* The logistic regression had a accuracy of 0.92\n\n* The best model of DNN3 had a accuracy of 0.97\n\n* I found that with dropout function further improved the performance of the model. As I see in the confusion matrix for with dropout model of DNN2, I get a high accuracy rate of 0.94 compared to without the dropout model of DNN1 After using dropout and we decided to adjust the dense layers for finding the best accuracy. \n\n* I changed the dense layers \u201c50, 25, 15, 10\u201d combination to \u201c20, 50, 50, 20\u201d and I found that the adjusted version of dense layers and previous dense neural network results are different. DDN3 dense layers gave high accuracy. \n\n* Discussion Predicting movies' revenue can significantly impact the movie industries and is the prime focus area for movie companies to remain profitable. Hence, researchers worldwide had undertaken significant research to understand which factors can affect the increase in movie revenue. Factors such as budget, popularity, vote count, and movie genre can increase movie revenue.","df3befbf":"There are a total of 45,466 movies with 24 features. Most of the features have very few NaN values (apart from homepage and tagline). I will attempt at cleaning this dataset to a form suitable for analysis in the next section.","c4b68da8":"Let's explore budget and revenue a bit more!"}}