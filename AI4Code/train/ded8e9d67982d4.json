{"cell_type":{"ca359812":"code","2f361c8e":"code","01c12c49":"code","1d840ccf":"code","0b3a7eea":"code","7a5d5911":"code","7533cc3a":"code","e500f338":"code","791c6ae6":"code","00a475b9":"code","6f806fc3":"code","4f53da07":"code","fb3659a7":"code","587283de":"code","4b3163b3":"code","7ab5ca1b":"code","557ff2a5":"code","b80029a6":"code","a05b399d":"code","fdb6b809":"code","f9b1401e":"code","2f4561bd":"code","fcba20c7":"code","7487ee76":"code","fc773337":"code","11271f67":"code","780a218a":"code","2f704625":"code","61f76939":"code","810e450c":"code","b3684334":"code","378ce463":"code","157c82ac":"code","f0ce03f7":"code","9d5361c7":"code","d7e354dc":"code","18392179":"code","18c7377b":"markdown","d82b587f":"markdown","2b7a4a2b":"markdown","1360b930":"markdown","745f194a":"markdown","47503bc3":"markdown","d9c0bd46":"markdown","802ea403":"markdown","de4b67b6":"markdown","848438c8":"markdown","df33a193":"markdown","0468b5ac":"markdown","b755fd0a":"markdown","daa3b3ee":"markdown","fd04cf81":"markdown","7446bd61":"markdown","bafe35c6":"markdown","cd0d12a1":"markdown","25a5916b":"markdown","7e487a06":"markdown","d3353ed2":"markdown","a99e090d":"markdown","4498c547":"markdown","5fd6078e":"markdown","56482b67":"markdown","dda27a50":"markdown","4f1da025":"markdown","772dd903":"markdown","13b8409b":"markdown","5c44a1fb":"markdown","287de735":"markdown"},"source":{"ca359812":"\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom datetime import datetime\nfrom scipy import stats\nfrom scipy.stats import norm, skew #for some statistics\nfrom scipy.special import boxcox1p\nfrom scipy.stats import boxcox_normmax\n\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.linear_model import ElasticNetCV, LassoCV, RidgeCV\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.pipeline import make_pipeline\nfrom xgboost import XGBRegressor\n\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Any results you write to the current directory are saved as output.","2f361c8e":"\ntrain = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')\nprint(\"imported data..\")\nprint(\"Train set size:\", train.shape)\nprint(\"Test set size:\", test.shape)","01c12c49":"train.head()","1d840ccf":"test.head()","0b3a7eea":"train_ID = train['Id']\ntest_ID = test['Id']\n\ntrain.drop(['Id'], axis=1, inplace=True)\ntest.drop(['Id'], axis=1, inplace=True)","7a5d5911":"#Before the normalisation\nsns.set_style(\"white\")\nsns.set_color_codes(palette='deep')\nf, ax = plt.subplots(figsize=(8, 7))\n#Check the new distribution \nsns.distplot(train['SalePrice'], color=\"b\");\nax.xaxis.grid(False)\nax.set(ylabel=\"Frequency\")\nax.set(xlabel=\"SalePrice\")\nax.set(title=\"SalePrice distribution\")\nsns.despine(trim=True, left=True)\nplt.show()\n\n# Deleting the more visibly obvious outliers\ntrain = train[train.GrLivArea < 4500]\ntrain.reset_index(drop=True, inplace=True)","7533cc3a":"\n#We use the numpy fuction log1p \ntrain[\"SalePrice\"] = np.log1p(train[\"SalePrice\"])\nsns.set_style(\"white\")\nsns.set_color_codes(palette='deep')\nf, ax = plt.subplots(figsize=(8, 7))\n#Check the new distribution \nsns.distplot(train['SalePrice'] , fit=norm, color=\"b\");\n\n# Get the fitted parameters used by the function\n(mu, sigma) = norm.fit(train['SalePrice'])\nprint( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n\n#Now plot the distribution\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nax.xaxis.grid(False)\nax.set(ylabel=\"Frequency\")\nax.set(xlabel=\"SalePrice\")\nax.set(title=\"SalePrice distribution\")\nsns.despine(trim=True, left=True)\n\nplt.show()","e500f338":"y = train.SalePrice.reset_index(drop=True)\ntrain_features = train.drop(['SalePrice'], axis=1)\ntest_features = test\n\n# concatinate the train and the test set as features for tranformation to avoid mismatch\nfeatures = pd.concat([train_features, test_features]).reset_index(drop=True)\nprint('Features size:', features.shape)","791c6ae6":"\n# function for determining the threshold of missing values\ndef percent_missing(df):\n    data = pd.DataFrame(df)\n    df_cols = list(pd.DataFrame(data))\n    dict_x = {}\n    for i in range(0, len(df_cols)):\n        dict_x.update({df_cols[i]: round(data[df_cols[i]].isnull().mean()*100,2)})\n    \n    return dict_x\n\nmissing = percent_missing(features)\ndf_miss = sorted(missing.items(), key=lambda x: x[1], reverse=True)\nprint('Percent of missing data')\ndf_miss[0:10]","00a475b9":"# visualising missing values\nsns.set_style(\"white\")\nf, ax = plt.subplots(figsize=(8, 7))\nsns.set_color_codes(palette='deep')\nmissing = round(train.isnull().mean()*100,2)\nmissing = missing[missing > 0]\nmissing.sort_values(inplace=True)\nmissing.plot.bar(color=\"b\")\n# Tweak the visual presentation\nax.xaxis.grid(False)\nax.set(ylabel=\"Percent of missing values\")\nax.set(xlabel=\"Features\")\nax.set(title=\"Percent missing data by feature\")\nsns.despine(trim=True, left=True)","6f806fc3":"\n# Some of the non-numeric predictors are stored as numbers; we convert them into strings \nfeatures['MSSubClass'] = features['MSSubClass'].apply(str)\nfeatures['YrSold'] = features['YrSold'].astype(str)\nfeatures['MoSold'] = features['MoSold'].astype(str)\n\n# data description says NA means typical\nfeatures['Functional'] = features['Functional'].fillna('Typ')\n# has one NA value. Since this feature has mostly 'SBrkr', we can set that for the missing value.\nfeatures['Electrical'] = features['Electrical'].fillna(\"SBrkr\")\n#  Only one NA value, We set 'TA' (which is the most frequent) for the missing value in KitchenQual\nfeatures['KitchenQual'] = features['KitchenQual'].fillna(\"TA\")\n\n# Both Exterior 1 & 2 have only one missing value. We will just substitute in the most common string\nfeatures['Exterior1st'] = features['Exterior1st'].fillna(features['Exterior1st'].mode()[0])\nfeatures['Exterior2nd'] = features['Exterior2nd'].fillna(features['Exterior2nd'].mode()[0])\n# Fill in again with most frequent\nfeatures['SaleType'] = features['SaleType'].fillna(features['SaleType'].mode()[0])\n\n# data description says NA means \"No Pool\", majority of houses have no Pool at all in general.\nfeatures[\"PoolQC\"] = features[\"PoolQC\"].fillna(\"None\")\n\n# Replacing missing data with 0 (Since No garage = no cars in such garage.)\nfor col in ('GarageYrBlt', 'GarageArea', 'GarageCars'):\n    features[col] = features[col].fillna(0)\n# Replacing missing data with None\nfor col in ['GarageType', 'GarageFinish', 'GarageQual', 'GarageCond']:\n    features[col] = features[col].fillna('None')\n# For all these categorical basement-related features, NaN means that there is no basement\nfor col in ('BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2'):\n    features[col] = features[col].fillna('None')\n\n# 'RL' is by far the most common value. So we can fill in missing values with 'RL'\nfeatures['MSZoning'] = features.groupby('MSSubClass')['MSZoning'].transform(lambda x: x.fillna(x.mode()[0]))\n\n# group by neighborhood and fill in missing value by the median LotFrontage of all the neighborhood\nfeatures['LotFrontage'] = features.groupby('Neighborhood')['LotFrontage'].transform(lambda x: x.fillna(x.median()))\nprint('Features size:', features.shape)","4f53da07":"# Filling the rest of the categorical features\nobjects = []\nfor i in features.columns:\n    if features[i].dtype == object:\n        objects.append(i)\nfeatures.update(features[objects].fillna('None'))","fb3659a7":"# Filling in the rest of the NA's\nnumeric_dtypes = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\nnumerics = []\nfor i in features.columns:\n    if features[i].dtype in numeric_dtypes:\n        numerics.append(i)\nfeatures.update(features[numerics].fillna(0))\n","587283de":"# Check the state of missing values\nmissing = percent_missing(features)\ndf_miss = sorted(missing.items(), key=lambda x: x[1], reverse=True)\nprint('Percent of missing data')\ndf_miss[0:10]","4b3163b3":"# First we need to find all numeric features in the data\nnumeric_dtypes = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\nnumerics2 = []\nfor i in features.columns:\n    if features[i].dtype in numeric_dtypes:\n        numerics2.append(i)","7ab5ca1b":"# Box plots for all our numeric features\nsns.set_style(\"white\")\nf, ax = plt.subplots(figsize=(8, 7))\nax.set_xscale(\"log\")\nax = sns.boxplot(data=features[numerics2] , orient=\"h\", palette=\"Set1\")\n# Tweak the visual presentation\nax.xaxis.grid(False)\nax.set(ylabel=\"Feature names\")\nax.set(xlabel=\"Numeric values\")\nax.set(title=\"Numeric Distribution of Features\")\nsns.despine(trim=True, left=True)","557ff2a5":"# Find the skewed  numerical features\nskew_features = features[numerics2].apply(lambda x: skew(x)).sort_values(ascending=False)\n\nhigh_skew = skew_features[skew_features > 0.5]\nskew_index = high_skew.index\nprint(\"There are {} numerical features with Skew > 0.5 :\".format(high_skew.shape[0]))\nskewness = pd.DataFrame({'Skew' :high_skew})\nskew_features.head(10)\n","b80029a6":"# Normalise skewed features\nfor i in skew_index:\n    features[i] = boxcox1p(features[i], boxcox_normmax(features[i] + 1))\n    ","a05b399d":"sns.set_style(\"white\")\nf, ax = plt.subplots(figsize=(8, 7))\nax.set_xscale(\"log\")\nax = sns.boxplot(data=features[skew_index] , orient=\"h\", palette=\"Set1\")\n# Tweak the visual presentation\nax.xaxis.grid(False)\nax.set(ylabel=\"Feature names\")\nax.set(xlabel=\"Numeric values\")\nax.set(title=\"Numeric Distribution of Features\")\nsns.despine(trim=True, left=True)\n\n","fdb6b809":"# Calculating totals before droping less significant columns\n\n#  Adding total sqfootage feature \nfeatures['TotalSF']=features['TotalBsmtSF'] + features['1stFlrSF'] + features['2ndFlrSF']\n#  Adding total bathrooms feature\nfeatures['Total_Bathrooms'] = (features['FullBath'] + (0.5 * features['HalfBath']) +\n                               features['BsmtFullBath'] + (0.5 * features['BsmtHalfBath']))\n#  Adding total porch sqfootage feature\nfeatures['Total_porch_sf'] = (features['OpenPorchSF'] + features['3SsnPorch'] +\n                              features['EnclosedPorch'] + features['ScreenPorch'] +\n                              features['WoodDeckSF'])\n","f9b1401e":"# Not normaly distributed can not be normalised and has no central tendecy\nfeatures = features.drop(['MasVnrArea', 'OpenPorchSF', 'WoodDeckSF', 'BsmtFinSF1','2ndFlrSF'], axis=1)\n","2f4561bd":"# Check the distribution after dopping the skewed features\nnumeric_dtypes = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\nnumerics3 = []\nfor i in features.columns:\n    if features[i].dtype in numeric_dtypes:\n        numerics3.append(i)\n        \nskew_features = features[numerics3].apply(lambda x: skew(x)).sort_values(ascending=False)\nhigh_skew = skew_features[skew_features > 0.5]\nskew_index = high_skew.index\n\nsns.set_style(\"white\")\nf, ax = plt.subplots(figsize=(8, 7))\nax.set_xscale(\"log\")\nax = sns.boxplot(data=features[skew_index] , orient=\"h\", palette=\"Set1\")\n# Tweak the visual presentation\nax.xaxis.grid(False)\nax.set(ylabel=\"Feature names\")\nax.set(xlabel=\"Numeric values\")\nax.set(title=\"Numeric Distribution of Features\")\nsns.despine(trim=True, left=True)\n","fcba20c7":"# Adding new simplified features (1 = present, 0 = not present)\nfeatures['haspool'] = features['PoolArea'].apply(lambda x: 1 if x > 0 else 0)\nfeatures['hasgarage'] = features['GarageArea'].apply(lambda x: 1 if x > 0 else 0)\nfeatures['hasbsmt'] = features['TotalBsmtSF'].apply(lambda x: 1 if x > 0 else 0)\nfeatures['hasfireplace'] = features['Fireplaces'].apply(lambda x: 1 if x > 0 else 0)\n\nprint('Features size:', features.shape)","7487ee76":"# Encoding the finalized features\nfinal_features = pd.get_dummies(features).reset_index(drop=True)\nprint('Features size:', features.shape)\nfinal_features.head()","fc773337":"# Spliting the data back to train(X,y) and test(X_sub)\nX = final_features.iloc[:len(y), :]\nX_test = final_features.iloc[len(X):, :]\nprint('Features size for train(X,y) and test(X_test):')\nprint('X', X.shape, 'y', y.shape, 'X_test', X_test.shape)\n","11271f67":"# Finding numeric features\nsns.set_style(\"dark\")\nnumeric_dtypes = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\nnumerics4 = []\nfor i in X.columns:\n    if X[i].dtype in numeric_dtypes:\n        if i in ['TotalSF', 'Total_Bathrooms','Total_porch_sf','haspool','hasgarage','hasbsmt','hasfireplace']:\n            pass\n        else:\n            numerics4.append(i)     \n# visualising some more outliers in the data values\nfig, axs = plt.subplots(ncols=2, nrows=0, figsize=(12, 80))\nplt.subplots_adjust(right=2)\nplt.subplots_adjust(top=2)\n\ncmap = sns.cubehelix_palette(dark=0.3, light=0.8, as_cmap=True)\n\nfor i, feature in enumerate(list(X[numerics4]), 1):    \n    plt.subplot(len(list(numerics4)), 4, i)\n    sns.scatterplot(x=feature, y='SalePrice', hue='SalePrice', size='SalePrice', palette=cmap, data=train)\n        \n    plt.xlabel('{}'.format(feature), size=15,labelpad=12.5)\n    plt.ylabel('SalePrice', size=15, labelpad=12.5)\n    \n    for j in range(2):\n        plt.tick_params(axis='x', labelsize=12)\n        plt.tick_params(axis='y', labelsize=12)\n    \n    plt.legend(loc='best', prop={'size': 10})\n        \nplt.show()","780a218a":"# Removes outliers \noutliers = [30, 88, 462, 631, 1322]\nX = X.drop(X.index[outliers])\ny = y.drop(y.index[outliers])","2f704625":"\n# Removes colums where the threshold of zero's is (> 99.95), means has only zero values \noverfit = []\nfor i in X.columns:\n    counts = X[i].value_counts()\n    zeros = counts.iloc[0]\n    if zeros \/ len(X) * 100 > 99.95:\n        overfit.append(i)\n\noverfit = list(overfit)\noverfit.append('MSZoning_C (all)')\n\nX = X.drop(overfit, axis=1).copy()\nX_test = X_test.drop(overfit, axis=1).copy()\n\nprint('X', X.shape, 'y', y.shape, 'X_test', X_test.shape)","61f76939":"kfolds = KFold(n_splits=10, shuffle=True, random_state=42)\n\n# model scoring and validation function\ndef cv_rmse(model, X=X):\n    rmse = np.sqrt(-cross_val_score(model, X, y,scoring=\"neg_mean_squared_error\",cv=kfolds))\n    return (rmse)\n\n# rmsle scoring function\ndef rmsle(y, y_pred):\n    return np.sqrt(mean_squared_error(y, y_pred))","810e450c":"\n# setup models hyperparameters using a pipline\n# The purpose of the pipeline is to assemble several steps that can be cross-validated together, while setting different parameters.\n# This is a range of values that the model considers each time in runs a CV\ne_alphas = [0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007]\ne_l1ratio = [0.8, 0.85, 0.9, 0.95, 0.99, 1]\nalphas_alt = [14.5, 14.6, 14.7, 14.8, 14.9, 15, 15.1, 15.2, 15.3, 15.4, 15.5]\nalphas2 = [5e-05, 0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007, 0.0008]\n\n\n# Kernel Ridge Regression : made robust to outliers\nridge = make_pipeline(RobustScaler(), RidgeCV(alphas=alphas_alt, cv=kfolds))\n\n# LASSO Regression : made robust to outliers\nlasso = make_pipeline(RobustScaler(), LassoCV(max_iter=1e7, \n                    alphas=alphas2,random_state=42, cv=kfolds))\n\n# Elastic Net Regression : made robust to outliers\nelasticnet = make_pipeline(RobustScaler(), ElasticNetCV(max_iter=1e7, \n                         alphas=e_alphas, cv=kfolds, l1_ratio=e_l1ratio))\n\n# XGBoost\nxgboost = XGBRegressor(learning_rate=0.01,n_estimators=3460,\n                                     max_depth=3, min_child_weight=0,\n                                     gamma=0, subsample=0.7,\n                                     colsample_bytree=0.7,\n                                     objective='reg:linear', nthread=-1,\n                                     scale_pos_weight=1, seed=27,\n                                     reg_alpha=0.00006)\n\n# Random ForrestRegression\nrforest = RandomForestRegressor(n_estimators=500)\n\n\n# store models, scores and prediction values \nmodels = {'Ridge': ridge,\n          'Lasso': lasso, \n          'ElasticNet': elasticnet,\n          'XGBoost':xgboost,\n          'RandomForest': rforest}\npredictions = {}\nscores = {}\n\nfor name, model in models.items():\n    \n    model.fit(X, y)\n    predictions[name] = np.expm1(model.predict(X))\n    \n    score = cv_rmse(model, X=X)\n    scores[name] = (score.mean(), score.std())\n\n    ","b3684334":"# get the performance of each model on training data(validation set)\nprint('---- Score with CV_RMSLE-----')\nscore = cv_rmse(ridge)\nprint(\"Ridge score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\n\nscore = cv_rmse(lasso)\nprint(\"Lasso score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\n\nscore = cv_rmse(elasticnet)\nprint(\"ElasticNet score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\n\nscore = cv_rmse(xgboost)\nprint(\"XGBoost score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\n\nscore = cv_rmse(rforest)\nprint(\"RandomForest score {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\n\n\n\n\n","378ce463":"#Fit the training data X, y\nprint('----START Fit----',datetime.now())\nprint('Elasticnet')\nelastic_model = elasticnet.fit(X, y)\nprint('Lasso')\nlasso_model = lasso.fit(X, y)\nprint('Ridge')\nridge_model = ridge.fit(X, y)\nprint('XGBoost')\nxgb_model = xgboost.fit(X,y)\nprint('RandomForest')\nrf_model = rforest.fit(X,y)\n","157c82ac":"# model blending function using fitted models to make predictions\ndef blend_models(X):\n    return ((elastic_model.predict(X)) + (lasso_model.predict(X)) + \\\n            (ridge_model.predict(X)) + xgb_model.predict(X) + rf_model.predict(X))\/5\nblended_score = rmsle(y, blend_models(X))\nprint('RMSLE score on train data:')\nprint(rmsle(y, blend_models(X)))","f0ce03f7":"# visualise model performance\nsns.set_style(\"white\")\nfig, axs = plt.subplots(ncols=0, nrows=4, figsize=(8, 7))\nplt.subplots_adjust(top=3.5, right=2)\n\nfor i, model in enumerate(models, 1):\n    plt.subplot(5, 1, i)\n    plt.scatter(predictions[model], np.expm1(y))\n    plt.plot([0, 800000], [0, 800000], '--r')\n\n    plt.xlabel('{} Predictions (y_pred)'.format(model), size=15)\n    plt.ylabel('Real Values (y_train)', size=13)\n    plt.tick_params(axis='x', labelsize=12)\n    plt.tick_params(axis='y', labelsize=12)\n\n    plt.title('{} Predictions vs Real Values'.format(model), size=15)\n    plt.text(0, 700000, 'Mean RMSE: {:.6f} \/ Std: {:.6f}'.format(scores[model][0], scores[model][1]), fontsize=15)\n    ax.xaxis.grid(False)\n    sns.despine(trim=True, left=True)\nplt.show()","9d5361c7":"scores['Blender'] = (blended_score, 0)\nsns.set_style(\"white\")\nfig = plt.figure(figsize=(24, 12))\n\nax = sns.pointplot(x=list(scores.keys()), y=[score for score, _ in scores.values()], markers=['o'], linestyles=['-'])\nfor i, score in enumerate(scores.values()):\n    ax.text(i, score[0] + 0.002, '{:.6f}'.format(score[0]), horizontalalignment='left', size='large', color='black', weight='semibold')\n\nplt.ylabel('Score (RMSE)', size=20, labelpad=12.5)\nplt.xlabel('Model', size=20, labelpad=12.5)\nplt.tick_params(axis='x', labelsize=13.5)\nplt.tick_params(axis='y', labelsize=12.5)\n\nplt.title('Scores of Models', size=20)\n\nplt.show()","d7e354dc":"# get the target variable\/ y_test with X_test\nprint('Predict submission')\ny_test_r = pd.read_csv(\"..\/input\/sample_submission.csv\")\nsubmission = pd.read_csv(\"..\/input\/sample_submission.csv\")\ny_test = np.log1p(y_test_r.iloc[:,1].values)\nsubmission.iloc[:,1] = np.expm1(blend_models(X_test))\nblended_score = rmsle(y_test, blend_models(X_test))\n","18392179":"submission.to_csv(\"new_submission.csv\", index=False)\nprint('Save submission', datetime.now(),)","18c7377b":"# Ensemble prediction on blended models","d82b587f":"This is to make sure that SalesPrice values are distributed normaly using function log1p which  applies log(1+x) to all elements of the column which fixes the skeweness of the distribution.","2b7a4a2b":"We use the kfolds Cross Validation function where K=10 which is the number of holdout sets. The function has no shuffle attribute, we add then one line of code shuffle=True, in order to shuffle the dataset prior to cross-validation.\n","1360b930":"Lets look at the distribution of our target variable to see if it fits a normal distribution, due to the parametric nature of our analysis procedure.","745f194a":"Outlier removal is usually safe, for outliers that are very visible in certain features. We decided to delete these outliers as they are  likely to introduce bias in our regression functions.\n","47503bc3":"> The graph shows that our data now looks more normal","d9c0bd46":"We use the scipy function boxcox1p which computes the Box-Cox transformation. The goal is to find a simple transformation that leads to normality. ","802ea403":"Now we use 10-fold stacking , we first split the training data into 10 folds. Then we will do 10 iterations. In each iteration, we train every base model on 9 folds and predict on the remaining fold (holdout fold).","de4b67b6":"# Import Data","848438c8":"# Model Predictions","df33a193":"Since area related features are very important to determine house prices, we add a few more features which is the total area of floors, bathrooms and porch area of each house before we continue droping these numeric colum","0468b5ac":"# Start Model Building","b755fd0a":"# Submission","daa3b3ee":"# More feature engineering","fd04cf81":"**Data Processing**","7446bd61":"> The are no more skewed numerical variables","bafe35c6":"# Feature Engineering","cd0d12a1":"After imputing features with missing values, is there any remaining missing values?","25a5916b":"> The graph shows that our data is skewed to the right.","7e487a06":"Visualize the destributions of the numeric features. This will allow us to visually see the ditributions of all our numeric data.","d3353ed2":">  [MasVnrArea, OpenPorchSF, WoodDeckSF, BsmtFinSF1] could not be normalised therefore call to be droped.","a99e090d":"Separating the training(X,y) and the testing(X_sub) set","4498c547":"All of the models individually achieved scores between 0.10 and 0.13, but when the predictions of those models are blended, they get about 0.090. That's because those models are actually overfitting to certain degree. They are very good at predicting a subset of houses, and they fail at predicting the rest of the dataset. When their predictions are blended, those models complement each other.","5fd6078e":"# Evaluate Model Performance","56482b67":"These visualisation helps determine which values need to be imputed, We impute them by proceeding sequentially through features with missing values.","dda27a50":">  No more missing values","4f1da025":" > The visualisation show that is some data that is not normaly distributed(Skewed)","772dd903":"Looking at the distribution of the numeric features. Ideally we would like all our data to be normaly distributed. Id these is not normally distributed that could violate the assumptions of parametric statistics. If the data deviate strongly from the assumptions of a parametric procedure, using the parametric procedure could lead to incorrect conclusions.  ","13b8409b":"# Finalising features","5c44a1fb":"We now start the process of preparing our features, we first find, the percentage of missing data in each column and we determine whether the threshold of missing values is acceptable or not.","287de735":"We check the data after normalisation, is there any remaining skewed values?"}}