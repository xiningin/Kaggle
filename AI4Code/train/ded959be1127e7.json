{"cell_type":{"17c6e6e8":"code","444925c4":"code","0ab599e3":"code","caeb6efb":"code","e34dc8e8":"code","d966e816":"code","561d9fd0":"code","3ea44c17":"code","2f51bcef":"code","1e62b622":"code","9b1c57e9":"code","72a3ec26":"code","67c2b6ef":"code","14a10711":"code","fcd283d8":"code","c7933bdb":"code","92dcb562":"code","4b8b6149":"code","b5cc764c":"code","48970824":"code","f8c8b07b":"code","631c623d":"code","d1560343":"code","359f40e2":"code","a18e7457":"code","2355555b":"code","e03c849d":"code","18b50800":"code","dd17f6c9":"code","065a1b6b":"markdown","caadafe6":"markdown","9fc8943a":"markdown","45b6021e":"markdown","82ea7300":"markdown","b15d13b6":"markdown","cf6f0ae2":"markdown","2e2c6501":"markdown","f77b7029":"markdown","f08f9099":"markdown","e3e9744f":"markdown","47f7b932":"markdown"},"source":{"17c6e6e8":"import numpy as np\nimport pandas as pd\nfrom pandas import Series, DataFrame\n\nimport tokenizers\nfrom tqdm.notebook import tqdm\n\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import StratifiedKFold\n\nimport warnings\nwarnings.filterwarnings('ignore')","444925c4":"N_FOLDS = 5\nLEFT_PAD_LEN = 1 # some internal hyperparameter for my model","0ab599e3":"def read_train():\n    train=pd.read_csv('..\/input\/tweet-sentiment-extraction\/train.csv')\n    train['text']=train['text'].astype(str)\n    train['selected_text']=train['selected_text'].astype(str)\n    return train\n\ndef read_test():\n    test=pd.read_csv('..\/input\/tweet-sentiment-extraction\/test.csv')\n    test['text']=test['text'].astype(str)\n    return test\n\ntrain_df = read_train()\ntest_df = read_test()\n\ntrain_df = read_train()\ntest_df = read_test()\n\n# there was one NaN value inside tweets in train_df\nassert train_df[\"text\"].isna().sum() <= 1\ntrain_df[\"text\"] = train_df[\"text\"].fillna(\"\")\n\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=777)\nsplits = list(skf.split(np.arange(len(train_df)), train_df.sentiment.values))\nval_inds_arr = [val_inds for tr_inds, val_inds in splits]\nval_inds_arr","caeb6efb":"train_df.head()","e34dc8e8":"test_df.head()","d966e816":"def get_union_df(name=\"train_prediction\", inds_arr=None, agg_f=None):\n    \"\"\" function for gathering results from each fold (for test with aggregation (agg_f) and for oof without one) \"\"\"\n    df = DataFrame()\n    for n_fold in range(N_FOLDS):\n        fold_df = (\n            pd\n            .read_csv(\"..\/input\/predictionexample\/{}_{}.csv\".format(name, n_fold + 1))\n            .drop(\"Unnamed: 0\", axis=1)\n        )\n\n        if inds_arr is not None:\n            fold_df.index = inds_arr[n_fold]\n\n        df = pd.concat([df, fold_df])\n\n    if agg_f:\n        df = df.astype(np.float32)\n        df = df.groupby(df.index).agg(agg_f)\n        \n    return df.sort_index()","561d9fd0":"oof_start_prediction = get_union_df(name=\"validation_start_prediction\", inds_arr=val_inds_arr)\noof_end_prediction = get_union_df(name=\"validation_end_prediction\", inds_arr=val_inds_arr)\n\noof_start_prediction.shape, oof_end_prediction.shape","3ea44c17":"test_start_prediction = get_union_df(name=\"test_start_prediction\", agg_f=\"mean\")\ntest_end_prediction = get_union_df(name=\"test_end_prediction\", agg_f=\"mean\")\n\ntest_start_prediction.shape, test_end_prediction.shape","2f51bcef":"oof_start_prediction.head()","1e62b622":"test_start_prediction.head()","9b1c57e9":"PATH = '..\/input\/tf-roberta\/'\ntokenizer = tokenizers.ByteLevelBPETokenizer(\n    vocab_file=PATH+'vocab-roberta-base.json', \n    merges_file=PATH+'merges-roberta-base.txt', \n    lowercase=True,\n    add_prefix_space=True\n)","72a3ec26":"def jaccard(str1, str2): \n    a = set(str(str1).lower().split()) \n    b = set(str(str2).lower().split())\n    c = a.intersection(b)\n    return float(len(c)) \/ (len(a) + len(b) - len(c))\n\ndef get_pred(start_proba, end_proba, df, tokenizer):\n    pred = []\n    n_samples = len(start_proba)\n    for i in range(n_samples):\n        text = df['text'][df.index[i]]\n        a, b = np.argmax(start_proba[i]), np.argmax(end_proba[i])\n        if a > b: \n            pred_ = text # IMPROVE CV\/LB with better choice here\n        else:\n            cleaned_text = \" \" + \" \".join(text.split())\n            encoded_text = tokenizer.encode(cleaned_text)\n            pred_ids = encoded_text.ids[a - LEFT_PAD_LEN: b - LEFT_PAD_LEN + 1]\n            pred_ = tokenizer.decode(pred_ids)\n        pred += [pred_]\n\n    return pred","67c2b6ef":"train_df.head()","14a10711":"train_df[\"pred_selected_text\"] = get_pred(oof_start_prediction.values, oof_end_prediction.values, train_df, tokenizer)\ntrain_df[\"jaccard\"] = train_df.apply(lambda row: jaccard(row[\"selected_text\"], row[\"pred_selected_text\"]), axis=1)\n\ntrain_df.head()","fcd283d8":"oof_score = train_df[\"jaccard\"].mean()\nprint(f'oof score before optimization: {oof_score:.5f}')","c7933bdb":"oof_start_pred = oof_start_prediction.idxmax(1).astype(int)\noof_end_pred   = oof_end_prediction  .idxmax(1).astype(int)\n\noof_percent = (oof_start_pred > oof_end_pred).mean()\noof_count   = (oof_start_pred > oof_end_pred).sum()\nprint(f'[oof] start > end percent: {(100 * oof_percent):.4f}% ({oof_count} times)')","92dcb562":"test_start_pred = test_start_prediction.idxmax(1).astype(int)\ntest_end_pred   = test_end_prediction  .idxmax(1).astype(int)\n\n\ntest_percent = (test_start_pred > test_end_pred).mean()\ntest_count   = (test_start_pred > test_end_pred).sum()\nprint(f'[test] start > end percent: {(100 * test_percent):.4f}% ({test_count} times)')","4b8b6149":"bad_train_df = train_df[oof_start_pred > oof_end_pred]\nbad_test_df  = test_df[test_start_pred > test_end_pred]\n\n# as described above, we predict text as selected_case in this case\nassert np.all(bad_train_df[\"text\"] == bad_train_df[\"pred_selected_text\"])","b5cc764c":"old_bad_oof_score = bad_train_df[\"jaccard\"].mean()\nprint(f'[start > end] oof score before optimization: {old_bad_oof_score:.5f}')","48970824":"def get_hypo_df(start_proba, end_proba, beam_size=10):\n    start2top_proba = Series(start_proba).sort_values(ascending=False)[:beam_size]\n    end2top_proba   = Series(end_proba  ).sort_values(ascending=False)[:beam_size]\n\n    hypos = []\n    for start, start_proba in start2top_proba.items():\n        for end, end_proba in end2top_proba.items():\n            proba = 0.5 * (start_proba + end_proba)\n            hypos += [(start, end, proba)]\n\n    return DataFrame(hypos, columns=[\"start\", \"end\", \"proba\"])\n\ndef get_prediction(df, start_prediction, end_prediction, ind, tokenizer, n_concat=1):\n    start_proba = start_prediction.loc[ind].values\n    end_proba   = end_prediction  .loc[ind].values\n    hypo_df = get_hypo_df(start_proba, end_proba)\n\n    pred_inds = hypo_df[hypo_df[\"start\"] <= hypo_df[\"end\"]].sort_values(\"proba\", ascending=False).index\n    \n    pred_selected_texts = []\n    for pred_ind in pred_inds[:n_concat]:\n        a, b = hypo_df[\"start\"][pred_ind], hypo_df[\"end\"][pred_ind]\n\n        text = df[\"text\"][ind]\n        cleaned_text = \" \" + \" \".join(text.split())\n        encoded_text = tokenizer.encode(cleaned_text)\n        pred_ids = encoded_text.ids[a - LEFT_PAD_LEN: b - LEFT_PAD_LEN + 1]\n        pred_selected_text = tokenizer.decode(pred_ids)\n        pred_selected_texts += [pred_selected_text]\n\n    return \" \".join(pred_selected_texts)","f8c8b07b":"N_CONCATS = np.arange(1, 20)\n\nnew_jaccards = []\nfor n_concat in tqdm(N_CONCATS):\n    bad_train_df[\"new_pred_selected_text\"] = bad_train_df.index.map(lambda ind: get_prediction(\n        bad_train_df,\n        oof_start_prediction,\n        oof_end_prediction,\n        ind,\n        tokenizer,\n        n_concat=n_concat\n    ))\n    bad_train_df[\"new_jaccard\"] = bad_train_df.apply(lambda row: jaccard(row[\"selected_text\"], row[\"new_pred_selected_text\"]), axis=1)\n\n    new_jaccard = bad_train_df[\"new_jaccard\"].mean()\n    new_jaccards += [new_jaccard]","631c623d":"plt.figure(figsize=(16, 8))\n\nnew_jaccards = np.array(new_jaccards)\nold_jaccards = new_jaccards * 0 + old_bad_oof_score\nplt.plot(N_CONCATS, new_jaccards, label=\"new jaccard\")\nplt.plot(N_CONCATS, old_jaccards, label=\"old jaccard\")\n\nplt.legend()\nplt.xlabel(\"# concated predictions\")\n_ = plt.ylabel(\"jaccard\")","d1560343":"res = Series(index=N_CONCATS, data=new_jaccards)\n\nbest_n_concat = res.idxmax()\nnew_bad_oof_score = res.max()\n\nprint(f'[start > end] oof score before optimization: {old_bad_oof_score:.5f}')\nprint(f'[start > end] oof score after optimization : {new_bad_oof_score:.5f} (n_concat={best_n_concat})')","359f40e2":"train_df[\"new_pred_selected_text\"] = train_df[\"pred_selected_text\"]\nbad_inds = bad_train_df.index\ntrain_df[\"new_pred_selected_text\"].loc[bad_inds] = bad_train_df.index.map(lambda ind: get_prediction(\n    bad_train_df,\n    oof_start_prediction,\n    oof_end_prediction,\n    ind,\n    tokenizer,\n    n_concat=best_n_concat\n))\n\ntrain_df[\"new_jaccard\"] = train_df.apply(lambda row: jaccard(row[\"selected_text\"], row[\"new_pred_selected_text\"]), axis=1)","a18e7457":"old_score = train_df[\"jaccard\"].mean()\nnew_score = train_df[\"new_jaccard\"].mean()\nprint(f'[start > end] oof score before optimization: {old_score:.5f}')\nprint(f'[start > end] oof score after  optimization: {new_score:.5f}')","2355555b":"test_df.head()","e03c849d":"test_df[\"selected_text\"] = get_pred(test_start_prediction.values, test_end_prediction.values, test_df, tokenizer)\ntest_df[\"selected_text\"].loc[bad_test_df.index] = bad_test_df.index.map(lambda ind: get_prediction(\n    bad_test_df,\n    oof_start_prediction,\n    oof_end_prediction,\n    ind,\n    tokenizer,\n    n_concat=best_n_concat\n))","18b50800":"test_df[[\"textID\", \"selected_text\"]].loc[bad_test_df.index]","dd17f6c9":"test_df[[\"textID\", \"selected_text\"]].to_csv('submission.csv', index=False)","065a1b6b":"That's good - we take a boost:","caadafe6":"# Conclusion\n\nThis was method about improving prediction, when start prediction righter than end prediction. We get small boost on oof prediction, but it can make a difference cause of dense leaderboard. You also can try to achieve best improvement with this notebook in next directions:\n- Use this notebook for your predictions\n- Optimize $f$ function, that we use for getting pair (start, end) score.\n- Use concat method not only for start > end cases.\n- Use prediction dataset (https:\/\/www.kaggle.com\/koza4ukdmitrij\/predictionexample) for your own aims.\n\nHope it helps.","9fc8943a":"# Data Preparation\n\nI just take one of my prediction for testing strategy, you can fill this part of notebook with the same data. We need the next ones:\n- oof start\/end\/selected_text prediction (+ oof tweet text)\n- test start\/end\/selected_text prediction (+ test tweet text)\n- tokenizer, that you used in training time\n- [optional]: I also use splitter for recover correct indexes for oof prediction","45b6021e":"# Algorithm\n\n## Stats & Preparation\n\nAt first take a look, what percent of data was covered with particular start > end case:","82ea7300":"# OOF Prediction\n\nLoad tokenizer and get a oof prediction with (oof_start_prediction, oof_end_prediction):","b15d13b6":"Then implement new approach:","cf6f0ae2":"# Inference\n\nImplement method for test set:","2e2c6501":"It means, that we can improve only about 0.2% of data (for both oof and test prediction). That's tiny part of data, but it can make a difference and improve your third sign in public or private test data (for instance, at this moment score 0.714 has [181, 281] leaderboard range, 0.715 - [115, 180]). Save this part of data seprately:","f77b7029":"# Abstract\n\nLots of public notebooks (for instance, https:\/\/www.kaggle.com\/khoongweihao\/tse2020-roberta-cnn-random-seed-distribution and https:\/\/www.kaggle.com\/al0kharba\/tensorflow-roberta-0-712) used a hypothesis, that if in your prediction start position righter end position, than your prediction for selected text equals text. Indeed, that's not the worse strategy, cause in this kind of prediction we get positive jaccard value for each true selected_text label. But obviously, we can improve this strategy. This notebook suggests one of those.","f08f9099":"## Theory\n\nOK, let's try to implement the next idea for start > end case:\n- Just pick a pair (start, end) as prediction among $start \\leq end$ pairs with highest score $f(p_{start}, p_{end}) = \\frac{p_{start} + p_{end}}{2}$. That's idea from paper about BERT for SQuAD task: https:\/\/arxiv.org\/pdf\/1810.04805.pdf (4.2 SQuAD v1.1 section) (point of improvement: check other functions $f$).\n- We have a deal with cases, that, honestly, our model process really bad. So, it's possible, that correct pair (start, end) will have not first rank with $f$ function, but 2-3 rank, cause of model confidence absence. So, lets just concatenate some $n\\_concat$ top rank pairs (start, end) (yes, we can do that in our prediction!)","e3e9744f":"## Implementation\n\nTime to coding. At first, count jaccard scores for old approach:","47f7b932":"Find a boost on whole train set:"}}