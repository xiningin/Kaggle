{"cell_type":{"a4ec5262":"code","36ae0b8e":"code","520b4054":"code","af268e78":"code","0526f7bd":"code","6c3d1f90":"code","6f844917":"code","48201aef":"code","c290734a":"code","ece9d7f7":"code","3de196b2":"code","806a0a05":"code","3b9d24bf":"code","414f1ec9":"code","72bcfae6":"code","c8029265":"code","cdfa4409":"code","49524765":"code","45d22dba":"code","4400f1db":"code","05c1f159":"code","81165971":"code","e204d7d5":"code","9e2fbf60":"code","ef8eb581":"code","983047d5":"code","151da5ce":"code","a9725fad":"code","8a923a11":"code","3620d1e3":"markdown","6e3e255e":"markdown","b9de901a":"markdown","d6afea40":"markdown","b5716f4c":"markdown","45b69308":"markdown","808131d5":"markdown","56f0c1a2":"markdown","c812bd29":"markdown","e547203f":"markdown","5a9ba194":"markdown","da339cf0":"markdown","60ad17a7":"markdown","7e9970ee":"markdown","85e9d3a5":"markdown","6208dfd3":"markdown","7ea925e3":"markdown","1e16a25f":"markdown","9b36a55a":"markdown","d8f29a3d":"markdown","c0cd1b1d":"markdown","de13806a":"markdown","795dcd98":"markdown"},"source":{"a4ec5262":"import re\nimport time\nimport nltk\nimport random\nimport warnings\nimport os\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom wordcloud import WordCloud, STOPWORDS\nfrom nltk.corpus import stopwords\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.metrics import mean_squared_error as mse\nfrom sklearn.model_selection import KFold\n\nimport tensorflow as tf\nimport tensorflow.keras.layers as L\nimport tensorflow.keras.backend as K\nfrom tensorflow.keras import optimizers, losses, metrics, Model\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, LearningRateScheduler\nfrom transformers import TFAutoModelForSequenceClassification, TFAutoModel, AutoTokenizer\n\ndef seed_everything(seed=0):\n    random.seed(seed)\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    os.environ['TF_DETERMINISTIC_OPS'] = '1'\n    \nseed = 0\nseed_everything(seed)\nwarnings.filterwarnings('ignore')","36ae0b8e":"train = pd.read_csv(\"..\/input\/commonlitreadabilityprize\/train.csv\")\ntest = pd.read_csv(\"..\/input\/commonlitreadabilityprize\/test.csv\")","520b4054":"train.head()","af268e78":"test.head()","0526f7bd":"print(train.shape)\nprint(test.shape)","6c3d1f90":"train.isnull().sum()","6f844917":"print('Min Target Value = ', train['target'].min())\nprint('\\nText : ', train[train['target'] == train['target'].min()]['excerpt'][1705])\n\nprint('\\n\\nMax Target Value = ', train['target'].max())\nprint('\\nText : ', train[train['target'] == train['target'].max()]['excerpt'][2829])","48201aef":"sns.distplot(train['target'])\nplt.title('Target Distribution', size=15)\nplt.xlabel('Value')\nplt.ylabel('Frequency')","c290734a":"print('Min Standard Error : ', train['standard_error'].min())\nprint('Target Value : ', train[train['standard_error'] == train['standard_error'].min()]['target'][106])\n\nprint('\\nText : ',train[train['standard_error'] == train['standard_error'].min()]['excerpt'][106])\n\nprint('\\n\\nMax Standard Error : ', train['standard_error'].max())\nprint('Target Value : ', train[train['standard_error'] == train['standard_error'].max()]['target'][2235])\n\nprint('\\nText : ',train[train['standard_error'] == train['standard_error'].max()]['excerpt'][2235])","ece9d7f7":"sns.distplot(x=train['standard_error'], color='red')\nplt.title('Standard Error Distribution', size=15)\nplt.xlabel('Value')\nplt.ylabel('Frequency')\nplt.show()","3de196b2":"plt.figure(figsize=(10, 5))\nsns.scatterplot(x=train['target'], y=train['standard_error'], color='black', size=train['standard_error'])\nplt.title('Target vs Standard Error', size=15)\nplt.show()","806a0a05":"def clean_data(data):\n    cleaned_excerpt = []\n    for text in data['excerpt']:\n        text = re.sub('[^a-zA-Z]', ' ', text)\n        text = text.lower()\n        text = nltk.word_tokenize(text)\n        \n        text = [word for word in text if word not in stopwords.words('english')]\n        \n        lemma = nltk.WordNetLemmatizer()\n        text = [lemma.lemmatize(word) for word in text]\n        text = ' '.join(text)\n        \n        cleaned_excerpt.append(text)\n    return cleaned_excerpt","3b9d24bf":"train['cleaned_excerpt'] = clean_data(train)","414f1ec9":"train.head()","72bcfae6":"def top_n_ngrams(corpus, n_gram=(1, 1), n=None):\n    vec = CountVectorizer(ngram_range = n_gram).fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0)\n   \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq = sorted(words_freq, key=lambda x: x[1], reverse=True)\n    \n    return words_freq[:n]\n\nunigrams = top_n_ngrams(train['cleaned_excerpt'], n_gram = (1, 1), n=20)\nbigrams = top_n_ngrams(train['cleaned_excerpt'], n_gram = (2, 2), n=20)\ntrigrams = top_n_ngrams(train['cleaned_excerpt'], n_gram = (3, 3), n=20)","c8029265":"def create_dataframe(data):\n    word = []\n    freq = []\n    for d in data:\n        word.append(d[0])\n        freq.append(d[1])\n    return pd.DataFrame({'word': word, 'freq': freq})\n\nuni_df = create_dataframe(unigrams)\nbi_df = create_dataframe(bigrams)\ntri_df = create_dataframe(trigrams)","cdfa4409":"fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(8, 20))\nsns.barplot(x='freq', y='word', color='#00e6b8', data=uni_df, ax= ax1)\nsns.barplot(x='freq', y='word', color='#ff5050', data=bi_df, ax= ax2)\nsns.barplot(x='freq', y='word', color='#e600e6', data=tri_df, ax= ax3)\n\nax1.set_title('Top 20 Uni-grams', size=12)\nax2.set_title('Top 20 Bi-grams', size=12)\nax3.set_title('Top 20 Tri-grams', size=12)\nplt.show()","49524765":"plt.figure(figsize=(10, 10))\nwc = WordCloud(stopwords=STOPWORDS,background_color=\"white\", contour_width=2, contour_color='blue',\n               width=1500, height=750,max_words=150, max_font_size=256,random_state=42)\n\nwc.generate(' '.join(train['cleaned_excerpt']))\nplt.imshow(wc)\nplt.axis('off')\nplt.show()","45d22dba":"# Detect hardware, return appropriate distribution strategy\n\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print(f\"Running on TPU {tpu.master()}\")\nexcept ValueError:\n    tpu = None\n    \nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    strategy = tf.distribute.get_strategy()\nAUTO = tf.data.experimental.AUTOTUNE\nREPLICAS = strategy.num_replicas_in_sync\nprint(f'REPLICAS : {REPLICAS}')","4400f1db":"BATCH_SIZE = 8 * REPLICAS\nLEARNING_RATE = 1e-5 * REPLICAS\nEPOCHS = 35\nES_PATIENCE = 7\nPATIENCE = 2\nN_FOLDS = 5\nSEQ_LEN = 256\nBASE_MODEL = '\/kaggle\/input\/huggingface-roberta\/roberta-base\/'","05c1f159":"def custom_standardization(text):\n    text = text.lower()\n    text = text.strip()\n    return text\n\ndef sample_target(features, target):\n    mean, stddev = target\n    sampled_target = tf.random.normal([], mean=tf.cast(mean, dtype=tf.float32), stddev=tf.cast(stddev, dtype=tf.float32), dtype=tf.float32)\n    return (features, sampled_target)\n\ndef get_dataset(df, tokenizer, labeled=True, ordered=False, repeated=False, is_sampled=False, batch_size=32, seq_len=128):\n    \"\"\"\n        Return a Tensorflow dataset ready for training or inference\n    \"\"\"\n    text = [custom_standardization(text) for text in df['excerpt']]\n    \n    tokenized_inputs = tokenizer(text, max_length=seq_len, truncation=True, padding='max_length', return_tensors='tf')\n    \n    if labeled:\n        dataset = tf.data.Dataset.from_tensor_slices(({'input_ids': tokenized_inputs['input_ids'],\n                                                      'attention_mask' : tokenized_inputs['attention_mask']},\n                                                     (df['target'], df['standard_error'])))\n        if is_sampled:\n            dataset = dataset.map(sample_target, num_parallel_calls = tf.data.AUTOTUNE)\n    else:\n        dataset = tf.data.Dataset.from_tensor_slices({'input_ids': tokenized_inputs['input_ids'],\n                                                         'attention_mask': tokenized_inputs['attention_mask']})\n    if repeated:\n        dataset = dataset.repeat()\n    if not ordered:\n        dataset = dataset.shuffle(1024)\n    dataset = dataset.batch(batch_size)\n    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n        \n    return dataset","81165971":"def model_fn(encoder, seq_len=256):\n    input_ids = L.Input(shape=(seq_len,), dtype=tf.int32, name='input_ids')\n    input_attention_mask = L.Input(shape=(seq_len,), dtype=tf.int32, name='attention_mask')\n    \n    outputs = encoder({'input_ids': input_ids,\n                      'attention_mask': input_attention_mask})\n    \n    model = Model(inputs=[input_ids, input_attention_mask], outputs=outputs)\n    \n    optimizer = optimizers.Adam(lr=LEARNING_RATE)\n    model.compile(optimizer=optimizer,\n                 loss = losses.MeanSquaredError(),\n                 metrics=[metrics.RootMeanSquaredError()])\n    return model\nwith strategy.scope():\n    encoder = TFAutoModelForSequenceClassification.from_pretrained(BASE_MODEL, num_labels=1)\n    model = model_fn(encoder, SEQ_LEN)\n    \nmodel.summary()","e204d7d5":"tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\nskf = KFold(n_splits=N_FOLDS, shuffle=True, random_state=seed)\noof_pred = []\noof_labels = []\nhistory_list = []\ntest_pred = []\n\nfor fold, (idxT, idxV) in enumerate(skf.split(train)):\n    if tpu:\n        tf.tpu.experimental.initialize_tpu_system(tpu)\n    print(f'\\nFOLD: {fold+1}')\n    print(f'Train: {len(idxT)} Valid: {len(idxV)}')\n    \n    K.clear_session()\n    with strategy.scope():\n        encoder = TFAutoModelForSequenceClassification.from_pretrained(BASE_MODEL, num_labels=1)\n        model = model_fn(encoder, SEQ_LEN)\n        \n    model_path = f'model_{fold}.h5'\n    es = EarlyStopping(monitor='val_root_mean_squared_error',\n                       mode='min', patience=ES_PATIENCE,\n                       restore_best_weights=True, verbose=1)\n    checkpoint = ModelCheckpoint(model_path,\n                                 monitor='val_root_mean_squared_error',\n                                 mode='min', save_best_only=True,\n                                 save_weights_only=True)\n    \n    history = model.fit(x=get_dataset(train.loc[idxT],\n                                     tokenizer, repeated=True, is_sampled=True,\n                                     batch_size=BATCH_SIZE, seq_len=SEQ_LEN), \n                       validation_data=get_dataset(train.loc[idxV], tokenizer,\n                                                  ordered=True, batch_size=BATCH_SIZE,\n                                                  seq_len=SEQ_LEN),\n                       steps_per_epoch=50,\n                       callbacks=[es, checkpoint],\n                       epochs=EPOCHS,\n                       verbose=2).history\n    history_list.append(history)\n    model.load_weights(model_path)\n    \n    print(f\"#### Fold {fold+1} OOF RMSE = {np.min(history['val_root_mean_squared_error']):.4f}\")\n    \n    valid_ds = get_dataset(train.loc[idxV], tokenizer, ordered=True,\n                          batch_size=BATCH_SIZE, seq_len=SEQ_LEN)\n    oof_labels.append([target[0].numpy() for sample, target in iter(valid_ds.unbatch())])\n    x_oof = valid_ds.map(lambda sample, target: sample)\n    \n    oof_pred.append(model.predict(x_oof)['logits'])\n    \n    test_ds = get_dataset(test, tokenizer, labeled=False, ordered=True, batch_size=BATCH_SIZE, seq_len=SEQ_LEN)\n    x_test = test_ds.map(lambda sample: sample)\n    test_pred.append(model.predict(x_test)['logits'])","9e2fbf60":"def plot_metrics(history):\n    metric_list = list(history.keys())\n    size = len(metric_list) \/\/ 2\n    fig, axes = plt.subplots(size, 1, sharex='col', figsize=(20, size*5))\n    axes = axes.flatten()\n        \n    for index in range(len(metric_list)\/\/2):\n        metric_name = metric_list[index]\n        val_metric_name = metric_list[index+size]\n        axes[index].plot(history[metric_name], label='Train %s ' % metric_name)\n        axes[index].plot(history[val_metric_name], label='Validation %s' % metric_name)\n        axes[index].legend(loc='best', fontsize=16)\n        axes[index].set_title(metric_name)\n            \n    plt.xlabel('Epochs', fontsize=16)\n    sns.despine()\n    plt.show()\n    \nfor fold, history in enumerate(history_list):\n    print(f'Fold : {fold+1}')\n    plot_metrics(history)","ef8eb581":"y_true = np.concatenate(oof_labels)\ny_preds = np.concatenate(oof_pred)\n\nfor fold, history in enumerate(history_list):\n    print(f\"Fold {fold+1} RMSE : {np.min(history['val_root_mean_squared_error']):.4f}\")\n    \nprint(f\"OOF RMSE: {mse(y_true, y_preds, squared=False):.4f}\")","983047d5":"preds_df = pd.DataFrame({'Label': y_true, 'Prediction': y_preds[:, 0]})\n\nfig, ax = plt.subplots(1, 1, figsize=(20, 6))\nsns.distplot(preds_df['Label'], ax=ax, label='Label')\nsns.distplot(preds_df['Prediction'], ax=ax, label='Prediction')\nax.legend()\nplt.show()","151da5ce":"sns.jointplot(data=preds_df, x='Label', y='Prediction', kind='reg', height=10)\nplt.show()","a9725fad":"submission = test[['id']]\nsubmission['target'] = np.mean(test_pred, axis=0)\nsubmission","8a923a11":"submission.to_csv('submission.csv', index=False)","3620d1e3":"## Auxiliary functions","6e3e255e":"## Import required modules","b9de901a":"# CommonLit Readability Basic EDA and RoBerta-base\n","d6afea40":"## Training","b5716f4c":"We can see only one outlier present.","45b69308":"## Target : \nOur target variable starts at -3.67, the highest possible difficulty and stops at 1.71, which is the lowest difficulty to read.","808131d5":"Reference - https:\/\/www.kaggle.com\/dimitreoliveira\/commonlit-readability-eda-roberta-tf-baseline","56f0c1a2":"We can standard error has outliers","c812bd29":"## load dataset","e547203f":"## Let's plot top unigrams, bigrams and trigrams","5a9ba194":"## OOF Metrics","da339cf0":"## WordCloud","60ad17a7":"## Data preprocessing","7e9970ee":"## Hardware configuration for TPU","85e9d3a5":"## Model evaluation\n> We are evaluating the model on the OOF predictions, it stands for Out Of Fold, since we are training using K-Fold our model will see all the data, and the correct way to evaluate each fold is by looking at the predictions that are not from that fold.","6208dfd3":"## Model Parameters","7ea925e3":"## Let's see standard error","1e16a25f":"## Make Submission","9b36a55a":"## Model","d8f29a3d":"We don't have any missing values in the columns of our interest, i.e., excerpt, target and standard_error!","c0cd1b1d":"## Error analysis, label x prediction distribution\nHere we can compare the distribution from the labels and the predicted values, in a perfect scenario they should align.","de13806a":"## Model loss and metrics graph","795dcd98":"## Target vs Standard error"}}