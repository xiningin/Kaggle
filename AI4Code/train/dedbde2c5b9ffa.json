{"cell_type":{"aa34f2a8":"code","38ac648d":"code","53ef9fd4":"code","f77c9bea":"code","3e81998b":"code","7a0f571b":"code","ae27c987":"code","e2d6aa2c":"code","f4881166":"code","a3a1180d":"code","0342d747":"code","84096fe0":"code","63dc5980":"code","84d1b22d":"code","a6c6fa6c":"code","8a13d45c":"code","6b02de5b":"code","dc9af720":"code","ec62b148":"code","68c4b07d":"code","bf7008bb":"code","bfed7a4b":"code","9bfe64b9":"code","04a7ef58":"markdown","9d8e479c":"markdown","9a7dc3db":"markdown","f652c52a":"markdown","6d814234":"markdown","8d05d69c":"markdown"},"source":{"aa34f2a8":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","38ac648d":"from sklearn.feature_selection import VarianceThreshold","53ef9fd4":"df=pd.read_csv('\/kaggle\/input\/santander-customer-satisfaction\/train.csv')\ndf.shape","f77c9bea":"df.head()","3e81998b":"X = df.drop(labels=['TARGET'],axis=1)\ny = df['TARGET']","7a0f571b":"from sklearn.model_selection import train_test_split\n# separate dataset into train and test\nX_train, X_test, y_train, y_test = train_test_split(\n    df.drop(labels=['TARGET'], axis=1),\n    df['TARGET'],\n    test_size=0.3,\n    random_state=0)\n\nX_train.shape, X_test.shape","ae27c987":"var_thres=VarianceThreshold(threshold=0)\nvar_thres.fit(X_train)","e2d6aa2c":"var_thres.get_support()","f4881166":"### Finding non constant features\nsum(var_thres.get_support())","a3a1180d":"# Lets Find non-constant features \nlen(X_train.columns[var_thres.get_support()])","0342d747":"constant_columns = [column for column in X_train.columns\n                    if column not in X_train.columns[var_thres.get_support()]]\n\nprint(len(constant_columns))","84096fe0":"for column in constant_columns:\n    print(column)","63dc5980":"X_train.drop(constant_columns,axis=1)","84d1b22d":"#check for missing values\ndf.isnull().sum()","a6c6fa6c":"#import essential libraries\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n#Using Pearson Correlation\ncorrmat = X_train.corr()\nfig, ax = plt.subplots()\nfig.set_size_inches(12,12)\nsns.heatmap(corrmat,cmap=\"CMRmap_r\")","8a13d45c":"# with the following function we can select highly correlated features\n# it will remove the first feature that is correlated with anything other feature\n\ndef correlation(dataset, threshold):\n    col_corr = set()  # Set of all the names of correlated columns\n    corr_matrix = dataset.corr()\n    for i in range(len(corr_matrix.columns)):\n        for j in range(i):\n            if abs(corr_matrix.iloc[i, j]) > threshold: # we are interested in absolute coeff value\n                colname = corr_matrix.columns[i]  # getting the name of column\n                col_corr.add(colname)\n    return col_corr","6b02de5b":"corr_features = correlation(X_train, 0.9)\nlen(set(corr_features))","dc9af720":"corr_features","ec62b148":"X_train.drop(corr_features,axis=1)","68c4b07d":"from sklearn.ensemble import ExtraTreesClassifier\nimport matplotlib.pyplot as plt\nmodel = ExtraTreesClassifier()\nmodel.fit(X,y)","bf7008bb":"print(model.feature_importances_) #use inbuilt class feature_importances of tree based classifiers","bfed7a4b":"#plot graph of feature importances for better visualization\nfeat_importances = pd.Series(model.feature_importances_, index=X.columns)\nfeat_importances.nlargest(10).plot(kind='barh')\nplt.show()","9bfe64b9":"import seaborn as sns\n#get correlations of each features in dataset\ncorrmat = df.corr()\ntop_corr_features = corrmat.index\nplt.figure(figsize=(20,20))\n#plot heat map\ng=sns.heatmap(df[top_corr_features].corr(),annot=True,cmap=\"RdYlGn\") ","04a7ef58":"**As always, I hope you find this kernel useful and your UPVOTES would be highly appreciated.**","9d8e479c":"**Feature Selection- With Correlation**\n\nRemove the features which are highly correlated\n\n","9a7dc3db":"**Univariate Selection\nStatistical tests can be used to select those features that have the strongest relationship with the output variable.**\n\nThe scikit-learn library provides the SelectKBest class that can be used with a suite of different statistical tests to select a specific number of features.\n\nWe can use the chi-squared (chi\u00b2) statistical test for non-negative features to select 10 of the best features. ","f652c52a":"**Lets apply the variance threshold**\n\nFeature selector that removes all low-variance features.\nThis feature selection algorithm looks only at the features (X), not the desired outputs (y).","6d814234":"**Correlation Matrix with Heatmap\nCorrelation states how the features are related to each other or the target variable.**\n\nCorrelation can be positive (increase in one value of feature increases the value of the target variable) or negative (increase in one value of feature decreases the value of the target variable)\n\nHeatmap makes it easy to identify which features are most related to the target variable, we will plot heatmap of correlated features using the seaborn library.","8d05d69c":"**Feature Importance**\n\nYou can get the feature importance of each feature of your dataset by using the feature importance property of the model.\n\nFeature importance gives you a score for each feature of your data, the higher the score more important or relevant is the feature towards your output variable.\n\nFeature importance is an inbuilt class that comes with Tree Based Classifiers, we will be using Extra Tree Classifier for extracting the top 10 features for the dataset."}}