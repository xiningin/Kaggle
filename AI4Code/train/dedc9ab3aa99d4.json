{"cell_type":{"cdd328ec":"code","87a92699":"code","30b77260":"code","d514f586":"code","eb3f8fee":"code","9b0aa440":"code","a2b46d82":"code","c746a467":"code","304c8f5e":"code","edb6558a":"code","1f1362d8":"code","090c8e97":"code","dcbb37d7":"code","97267821":"code","de8aa2c9":"code","24bbc9b5":"code","a30843d6":"code","308e2c39":"code","bf904309":"code","86b8ae4c":"code","9b727958":"code","e4ec1d69":"code","5fd446e0":"code","19ff7ba9":"code","9fefcad5":"code","53d1b3b0":"markdown","73a13bdf":"markdown","b0252651":"markdown","86f30ed4":"markdown","397d2a93":"markdown","0c91f650":"markdown","7e93950d":"markdown","01714783":"markdown","70e0cfa4":"markdown","e3236d71":"markdown","c37eceaa":"markdown","d2f0ed20":"markdown","df13bff5":"markdown","27d5b3af":"markdown","f307551e":"markdown","ac786abf":"markdown","537fe8a8":"markdown","3a22fd88":"markdown"},"source":{"cdd328ec":"!pip install tensorflow==2.0.0\n!pip install chart_studio\n\nimport numpy as np \nimport pandas as pd\nimport os\n\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nplt.style.use('fivethirtyeight') \n\n%matplotlib inline\nfrom pylab import rcParams\nfrom plotly import tools\nimport chart_studio.plotly as py\nfrom plotly.offline import init_notebook_mode, iplot\ninit_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.figure_factory as ff\nimport statsmodels.api as sm\n\nimport math\nimport random","87a92699":"!ls ..\/input\/ashrae-energy-prediction\/","30b77260":"%%time\ntrain_df = pd.read_csv('..\/input\/ashrae-energy-prediction\/train.csv')\nweather_train_df = pd.read_csv('..\/input\/ashrae-energy-prediction\/weather_train.csv')\ntest_df = pd.read_csv('..\/input\/ashrae-energy-prediction\/test.csv')\nweather_test_df = pd.read_csv('..\/input\/ashrae-energy-prediction\/weather_test.csv')\nbuilding_meta_df = pd.read_csv('..\/input\/ashrae-energy-prediction\/building_metadata.csv')","d514f586":"train_df.head()","eb3f8fee":"weather_train_df.head()","9b0aa440":"test_df.head()","a2b46d82":"weather_test_df.head()","c746a467":"building_meta_df.head()","304c8f5e":"train_df = train_df.merge(building_meta_df, on=\"building_id\")","edb6558a":"train_df = train_df.merge(weather_train_df, on=[\"site_id\", \"timestamp\"])","1f1362d8":"sample_df = train_df.sample(20, random_state=0)\nsample_df","090c8e97":"train_df['primary_use'] = pd.Categorical(train_df['primary_use'])\ntrain_df['primary_use'] = train_df['primary_use'].cat.codes\ntrain_df.head(10)","dcbb37d7":"train_df.isnull().sum()","97267821":"100 * train_df.isnull().sum() \/ len(train_df)","de8aa2c9":"del train_df['floor_count']\ndel train_df['year_built']","24bbc9b5":"def extract_data(building_id):\n    return train_df[train_df['building_id'] == building_id]\n\nextracted_df = {}\nfor i in range(10):\n    df = extract_data(i)\n    sea_lev_pressure_mean = df['sea_level_pressure'].mean()\n    dew_temperature_mean = df['dew_temperature'].mean()\n    air_temperature_mean = df['air_temperature'].mean()\n    precip_depth_1_hr_mean = df['precip_depth_1_hr'].mean()\n    wind_speed_mean = df['wind_speed'].median()\n\n    df['meter'] = pd.to_numeric(df['meter'], errors='coerce').fillna(0).astype(np.float32)\n    df['meter_reading'] = pd.to_numeric(df['meter_reading'], errors='coerce').fillna(0).astype(np.float32)\n    df['site_id'] = pd.to_numeric(df['site_id'], errors='coerce').fillna(0).astype(np.float32)\n    df['primary_use'] = pd.to_numeric(df['primary_use'], errors='coerce').fillna(0).astype(np.float32)\n    df['square_feet'] = pd.to_numeric(df['square_feet'], errors='coerce').fillna(0).astype(np.float32)\n    \n    df['air_temperature'] = df['air_temperature'].fillna(-1)\n    df['cloud_coverage'] = df['cloud_coverage'].fillna(-1)\n    df['sea_level_pressure'] = df['sea_level_pressure'].fillna(sea_lev_pressure_mean)\n    df['dew_temperature'] = df['dew_temperature'].fillna(dew_temperature_mean)\n    df['air_temperature'] = df['air_temperature'].fillna(air_temperature_mean)\n    df['precip_depth_1_hr'] = df['precip_depth_1_hr'].fillna(precip_depth_1_hr_mean)\n    df['wind_direction'] = df['wind_direction'].apply(lambda x: random.random() * 360.0 if (math.isnan(x)) else x)\n    df['wind_speed'] = df['wind_speed'].fillna(wind_speed_mean).astype(np.float32)\n    extracted_df[i] = df\n    \nextracted_df[4]","a30843d6":"### Building simple LSTM network","308e2c39":"def input_function(x_tr, y_tr, x_d, y_d, x_t, y_t, bs_tr):\n    t_d = tf.data.Dataset.from_tensor_slices((x_tr, y_tr))\n    t_d = t_d.cache().batch(bs_tr).repeat()\n    d_d = tf.data.Dataset.from_tensor_slices((x_d, y_d))\n    d_d = d_d.batch(bs_tr).repeat()\n    v_d = tf.data.Dataset.from_tensor_slices((x_t, y_t))\n    v_d = v_d.batch(1).repeat()\n    return t_d, d_d, v_d","bf904309":"def split_data(ds, tar, st_ind, en_ind, hs,\n               ts, step):\n    data = []\n    lab = []\n\n    st_ind = st_ind + hs\n    if en_ind is None:\n        en_ind = len(ds) - ts\n\n    for i in range(st_ind, en_ind):\n        indices = range(i - hs, i, step)\n        data.append(ds[indices])\n        lab.append(tar[i + ts])\n\n    return np.array(data), np.array(lab)","86b8ae4c":"def prepare_data(data, hn, tv_split,\n                 ve_split, tn, step=1, bs=64):\n    ds = {}\n\n    for j in range(10):\n        x_tr, y_tr = split_data(data[j], data[j][:, 3], 0,\n                                          tv_split, hn,\n                                          tn, step)\n        x_d, y_d = split_data(data[j], data[j][:, 3],\n                                      tv_split, ve_split, hn,\n                                      tn, step)\n        x_t, y_t = split_data(data[j], data[j][:, 3],\n                                        ve_split, None, hn,\n                                        tn, step)\n        t_d, d_d, v_d = input_function(x_tr, y_tr, x_d, y_d, x_t, y_t, bs)\n        ds[j] = {}\n        ds[j]['train'] = t_d\n        ds[j]['dev'] = d_d\n        ds[j]['test'] = v_d\n\n    inp_d = x_tr.shape[-2:]\n\n    return inp_d, ds","9b727958":"def scale_data(data_dict, fc, fc_n):\n    td = {}\n    norm_param = {}\n\n    for i in range(10):\n        norm_param[i] = {}\n        features = data_dict[i][fc]\n        features.index = data_dict[i]['timestamp']\n        features.head()\n        for j in fc_n:\n            data_mean = features[j].mean(axis=0)\n            data_std = features[j].std(axis=0)\n            features[j] = (features[j] - data_mean) \/ data_std\n        td[i] = features.values\n    \n    return td, norm_param\n        \n","e4ec1d69":"def fit_model(ds, model, eval_int, epochs=10, trg='meter'):\n    tm_histories = {}\n    for i in range(10):\n        print(\"Fitting to %s\" % i)\n        tm_histories[i] = model.fit(ds[i][\"train\"],\n                                   validation_data=ds[i][\"dev\"],\n                                   steps_per_epoch=eval_int,\n                                   epochs=epochs,\n                                   validation_steps=1).history\n    return model, tm_histories","5fd446e0":"fc = ['meter', 'meter_reading', 'site_id',\n      'primary_use', 'air_temperature', 'cloud_coverage',\n      'dew_temperature', 'precip_depth_1_hr', 'sea_level_pressure',\n      'wind_direction', 'wind_speed']\n\nfc_n = ['meter_reading','air_temperature', 'cloud_coverage',\n       'dew_temperature', 'precip_depth_1_hr', 'sea_level_pressure',\n       'wind_direction', 'wind_speed']\n\ntv_split = int(len(extracted_df[4])* 0.7)\nve_split = int(len(extracted_df[4])* 0.85)\npast = 30\neval_int = 1\n\ntd_set, norm_params = scale_data(extracted_df, fc, fc_n)\n# print(td_set[1][:10])\ninp_d, ds = prepare_data(td_set, past, tv_split, ve_split, 1, step=1, bs=32)\n\n","19ff7ba9":"model = tf.keras.models.Sequential()\nmodel.add(tf.keras.layers.LSTM(units=30,\n                               return_sequences=True,\n                               input_shape=inp_d))\nmodel.add(tf.keras.layers.LSTM(units=32, return_sequences=True))\nmodel.add(tf.keras.layers.Dropout(0.8))\nmodel.add(tf.keras.layers.LSTM(units=16, return_sequences=True))\nmodel.add(tf.keras.layers.Dropout(0.8))\nmodel.add(tf.keras.layers.Dense(1))\nmodel.compile(optimizer=tf.keras.optimizers.Adam(clipvalue=1.0), loss='mse')","9fefcad5":"# model, hist_dict = fit_model(ds, model, eval_int, 500)","53d1b3b0":"Standardize features by removing mean and scaling according to variances.","73a13bdf":"# ASHRAE - Great Energy Predictor III\u00b6\n\nThis notebook contains basic exploration of provided data and creation of recurent neural network model with tensorflow. <br>This it was kept in a simple manner in order to provide base line solution for LSTM.\n<br><br>\n\n_Context_\n\nWith advancements in technology and increasing number of people world wide also the amount of energy is rising. To prevent negative impact of this growth we could find possibilities to optimize the energy consumed by buildings. In order to do that there is required some insights into \"energetic efficiency\" of buildings.","b0252651":"Fit to every data set with given 'building_id'.","86f30ed4":"As we can see there is quite lot of NaN's in some of the columns. In order to use the this data in neural network we have to change them into numerical values or drop them.","397d2a93":"Here we prepare data using previously defined functions","0c91f650":"Now let's change the 'primary_use' column to categorical.","7e93950d":"We've following files for exploration.","01714783":"Let's take data for first 10 building_id's and preprocess them for LSTM neural network.","70e0cfa4":"Splits data according to provided parameters.","e3236d71":"Prepares data for three sets of data (train, dev, val) for every 'building_id'.","c37eceaa":"### Preprocessing","d2f0ed20":"Let's take a look at sample rows from combined dataframe.","df13bff5":"Let's check how many 'NaN' values has each column.","27d5b3af":"Bellow we create LSTM model with tensorflow keras. Sample loss function is doesn't complie with the one provided in the task yet.","f307551e":"### Further analysis\nComming soon ...","ac786abf":"Dictionary 'extracted_df' contains data for buildings_ids in range \\[0, 10) which have to be reshaped in order to feed them into LSTM neural network.","537fe8a8":"### Shaping data\nIn order to run LSTM neural network data has to be in three dimentional shape where axis corresponds to following data:\n* x-axis time steps\n* y-axis data examples\n* z-axis features for single point in time\n\n![image.png](attachment:image.png)\n\nThere are fine written resources on the net describing how LSTM networks works and what's their architecture.\n* i.e. [here](http:\/\/colah.github.io\/posts\/2015-08-Understanding-LSTMs\/)","3a22fd88":"Let's start analysis from the reading datasets."}}