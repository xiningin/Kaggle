{"cell_type":{"f3c5bb75":"code","4cde4c0f":"code","e6c652d0":"code","f0965a00":"code","1ce1d4ff":"code","034a6680":"code","0c7a7bb9":"code","f0d90da6":"code","067872ce":"code","39a35930":"code","b40c514d":"code","a9a67350":"code","b0a8a57d":"code","f0bfcbb9":"code","2bd930ce":"code","01190b75":"code","532fba99":"code","bd29dcd0":"code","26f2572b":"code","9931b56e":"code","ad1fc333":"code","aecad2ed":"code","902dba75":"code","81f0d039":"code","7d50969e":"code","54f57c54":"code","8a3e61f9":"code","c84eebaa":"code","b42ee8fe":"code","427049f8":"code","a1690b37":"code","77b461df":"code","9660d81b":"code","9defab22":"code","2a50e177":"code","7d103a46":"code","a2baa190":"code","0488eb25":"code","2a2763db":"code","42b48c75":"code","471b1b1e":"code","434328ef":"code","ba1211f0":"code","1c1493ae":"code","8cce50c5":"code","684aad7e":"code","acb41118":"code","147405cb":"markdown","e2af3148":"markdown","283bc11b":"markdown","884dae95":"markdown","2373ae9d":"markdown","9edaa07a":"markdown","7214cd5f":"markdown","0e5f1894":"markdown","99597bf3":"markdown","2c6df0e6":"markdown","f776aeb2":"markdown","1bb01bb7":"markdown","e037eb1b":"markdown","31d5e658":"markdown","69490b7b":"markdown","935ca0df":"markdown","9334dc62":"markdown","9d8f046d":"markdown","29519e02":"markdown","dcc397a8":"markdown","85591445":"markdown","2fa25691":"markdown","f5ea56e1":"markdown","bc13d0da":"markdown","39e01b2d":"markdown","21f0cc27":"markdown","f0cd8ee8":"markdown","1f42d18d":"markdown","f0b727a8":"markdown","d6d49e7a":"markdown","a5ec4daf":"markdown","bc3853a3":"markdown","4f169d82":"markdown","d82ea559":"markdown"},"source":{"f3c5bb75":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","4cde4c0f":"df = pd.read_csv('\/kaggle\/input\/ccdata\/CC GENERAL.csv')\ndf.head()","e6c652d0":"df.info()","f0965a00":"X = df.drop('CUST_ID', axis=1)","1ce1d4ff":"X.isna().sum().sort_values(ascending=False).head()","034a6680":"from sklearn.impute import KNNImputer\n\nknn_imp = KNNImputer(n_neighbors=3)\nX = pd.DataFrame(knn_imp.fit_transform(X), columns=X.columns)","0c7a7bb9":"X.isna().sum().sum()","f0d90da6":"# Ah!!! The eye feast libraries...!\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns","067872ce":"X_sk_vals = X.skew()\nsns.barplot(x=X_sk_vals.values, y=X_sk_vals.index)","39a35930":"X_unsk = X.apply(lambda x: np.log(1+x)) # X_unsk --> Unskewed X\nX_unsk_vals = X_unsk.skew()             # X_unsk_vals --> Skew coefficients of X_unsk\n\nax = sns.barplot(x=X_sk_vals.values, y=X_sk_vals.index, alpha=0.2)\nax2 = ax.twinx()\nsns.barplot(x=X_unsk_vals.values, y=X_unsk_vals.index, ax=ax2)\nplt.show()","b40c514d":"#  KDE [Kernel Desnity Estimate] provides \"Probability Density Curve\"\n\nfig = plt.figure(figsize=(20, 10))\nfor idx, col in enumerate(X.columns, 1):\n    ax = fig.add_subplot(5, 4, idx)\n    sns.kdeplot(X[col], bw_adjust=1, ax=ax)\n    sns.kdeplot(X_unsk[col], bw_adjust=1, ax=ax, fill=True, color='black')\n\nplt.tight_layout()\n\n# Light plot is original data\n# Dark plot is transformed data","a9a67350":"fig = plt.figure(figsize=(20, 10))\nfor idx, col in enumerate(X.columns, 1):\n    ax = fig.add_subplot(5, 4, idx)\n    sns.boxplot(x=X[col], ax=ax)\n\nplt.tight_layout()","b0a8a57d":"corr = X.corr()\nmask = np.triu(np.ones_like(corr, dtype=bool))\nplt.figure(figsize=(17, 10))\nsns.heatmap(corr, mask=mask, annot=True)\ntext = \"\"\"- The lighter they are, the more they are dependent of each other.\n- The darker they are, the more they are oppositely dependent of each other.\n- The average they are, the more they say 'Have me met before?' to each other.\"\"\"\nplt.text(5, 3, text, size=13)\nplt.show()","f0bfcbb9":"check_corr_for = ['PURCHASES_INSTALLMENTS_FREQUENCY', 'PURCHASES_FREQUENCY', 'CASH_ADVANCE_TRX', 'CASH_ADVANCE_FREQUENCY', 'ONEOFF_PURCHASES_FREQUENCY']\nX[check_corr_for].corr().plot.kde(figsize=(10, 6))\nplt.tight_layout()","2bd930ce":"from sklearn.preprocessing import StandardScaler\n\nXs = StandardScaler().fit_transform(X) # Xs -> X standardized","01190b75":"from sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score","532fba99":"clusters_you_need = 5\nthe_number_that_follows_you = 7\nfavourite_number = the_number_that_follows_you\n\nassert(favourite_number not in [-np.inf, np.inf]), \"You're the one I'm looking for!. because your number is {}\".format(favourite_number)\nassert(isinstance(favourite_number, int)) , \"If I knew about you, I would've mentioned it as integer. because your number is {}\".format(favourite_number)\nassert(favourite_number >= 0), \"You've no place in this dimension. because your number is {}\".format(favourite_number)\n\nrandom_number = favourite_number\nrandom_state = random_number\nkh_mins_dummy = KMeans(n_clusters=clusters_you_need, random_state=random_state)\n# random_state is optional, which means, the above stupid procedure is all optional.\ncluster_results = kh_mins_dummy.fit_predict(Xs)","bd29dcd0":"cluster_range = range(2, 15)\n\nkm_WCSS_scores = []\nkm_sil_scores = []  # sil is nick name for silhouette\n\nfor kay in cluster_range:\n    kh_mins = KMeans(n_clusters=kay)\n    y_tmp = kh_mins.fit_predict(Xs)  # y_tmp are the cluster results based on Xs (Stan-dar-dised)\n    km_WCSS_scores.append(kh_mins.inertia_)  # that's how you get the score\n    km_sil_scores.append(silhouette_score(Xs, y_tmp))","26f2572b":"plt.figure(figsize=(20, 5))\n\nplt.subplot(1, 2, 1)\nplt.title('WCSS')\nplt.plot(cluster_range, km_WCSS_scores, marker='*')\n\n\nplt.subplot(1, 2, 2)\nplt.title('silhouette scores')\nplt.plot(cluster_range, km_sil_scores, marker='*')","9931b56e":"kh_mins = KMeans(n_clusters=8).fit(Xs)\nkh_mins_sil_score = silhouette_score(Xs, kh_mins.labels_)     \n# Well, if I'm right, you're thinking about something. If so, yes, they're the same.\nkh_mins_sil_score","ad1fc333":"from sklearn.cluster import DBSCAN\nfrom sklearn.neighbors import NearestNeighbors\nfrom sklearn.cluster import AgglomerativeClustering","aecad2ed":"# we need the distance of nearest neighbor to every point, so that we can get an idea about\n# how close neighbors are located to each other.\n# we're considering 2 neighbors because every point is a 1st neighbor to itself.\nknn = NearestNeighbors(n_neighbors=2).fit(Xs)\ndistances, indicies = knn.kneighbors(Xs)","902dba75":"distances[:7]\n# Observe that 1st column is all zeroes, because it's the distance to itself.","81f0d039":"neighbor_distances = distances[:, 1]\nsorted_neighbor_distances = sorted(neighbor_distances)\nplt.plot(sorted_neighbor_distances)","7d50969e":"# Well, most of the neighbors are within distance \"2\"\nplt.plot(sorted_neighbor_distances)\nplt.plot(range(len(sorted_neighbor_distances)), [2]*len(sorted_neighbor_distances))","54f57c54":"db_sil_scores = []\n\nfor ep in np.arange(0.2, 3, 0.2):\n    for min_samps in range(2, 10, 2):\n        y_tmp = DBSCAN(eps=ep, min_samples=min_samps).fit_predict(Xs)\n        cluster_count = len(np.unique(y_tmp))\n        db_sil_scores.append([ep, min_samps, cluster_count, silhouette_score(Xs, y_tmp)])\n\ndb_df = pd.DataFrame(db_sil_scores, columns=['eps', 'min_samples', 'cluster_count', 'sil_score']).sort_values('sil_score', ascending=False)","8a3e61f9":"db_df.head()","c84eebaa":"from scipy.cluster.hierarchy import dendrogram, linkage\nplt.figure(figsize=(20, 5))\ndendro = dendrogram(linkage(Xs[:200], 'average'))\n# Limited to 200 rows respecting the Kaggle servers.","b42ee8fe":"# Another cool visualisation\nsns.clustermap(Xs[:100], linewidths=0.002)","427049f8":"aggl_sil_scores = []\n\ncluster_range = range(2, 20, 2)\nfor i in cluster_range:\n    for link_type in ['average', 'single', 'ward']:\n        y_tmp = AgglomerativeClustering(n_clusters=i, linkage=link_type).fit_predict(Xs)\n        aggl_sil_scores.append([i, link_type, silhouette_score(Xs, y_tmp)])\n\naggl_df = pd.DataFrame(aggl_sil_scores, columns=['cluster_count', 'linkage', 'sil_score']).sort_values('sil_score', ascending=False)","a1690b37":"aggl_df.head()","77b461df":"#  Let's pick the top one with 4 clusters. 2 clusters would be pretty bland.\nagglo = AgglomerativeClustering(n_clusters=4, linkage='average').fit(Xs)\nagglo_sil_score = silhouette_score(Xs, agglo.labels_)\nagglo_sil_score","9660d81b":"X['cluster'] = agglo.labels_\nX['cluster'].value_counts()","9defab22":"agglo = AgglomerativeClustering(n_clusters=6, linkage='single').fit(Xs)\nagglo_sil_score = silhouette_score(Xs, agglo.labels_)\nagglo_sil_score","2a50e177":"X['cluster'] = agglo.labels_\nX['cluster'].value_counts()","7d103a46":"X['cluster'] = kh_mins.labels_\nX['cluster'].value_counts()","a2baa190":"from sklearn.decomposition import PCA\nfrom sklearn.preprocessing import normalize","0488eb25":"Xn = normalize(Xs)  \n# Normalizing Xs because..., wait for few cells.","2a2763db":"pca_test = PCA().fit(Xn)\nplt.plot(pca_test.explained_variance_ratio_.cumsum())\n# the following plot tells us the variance we manage to get aganist the number of dimensions.","42b48c75":"# Since, we just need PCA to visualize data, we can simply reduce dimensions to 2 or 3. \n# Remember, you can also reduce the Xs to lower dimensions and then apply clustering models.\n# Just make sure, you're not losing much variance.\n\npca_2d = PCA(n_components=2)\npca_3d = PCA(n_components=3)\n\nXn_2d = pca_2d.fit_transform(Xn)\nXn_3d = pca_3d.fit_transform(Xn)\n\nnormalize_Xs_because = PCA(n_components=2).fit_transform(Xs)","471b1b1e":"Xn_2d.shape, Xn_3d.shape, normalize_Xs_because.shape","434328ef":"plt.figure(figsize=(20, 9))\nx_, y_ = Xn_2d.T\n\nplt.subplot(1, 2, 1)\nsns.scatterplot(x=x_, y=y_)\n\nplt.subplot(1, 2, 2)\nsns.scatterplot(x=x_, y=y_, hue=X['cluster'])","ba1211f0":"import plotly.express as px\n\nx_, y_, z_ = Xn_3d.T\nfig = px.scatter_3d(x=x_, y=y_, z=z_, color=X['cluster'])\nfig.show()","1c1493ae":"plt.figure(figsize=(11, 5))\n\nplt.subplot(1, 2, 1)\nplt.text(6, 20, 'Without Normalization', size=15)\nx_, y_ = normalize_Xs_because.T\nsns.scatterplot(x=x_, y=y_)\n\nplt.subplot(1, 2, 2)\nx_, y_ = Xn_2d.T\nsns.scatterplot(x=x_, y=y_)","8cce50c5":"from sklearn.manifold import TSNE\n\ntsne = TSNE(n_components=2)\nXn_tsne_2d = tsne.fit_transform(Xn)","684aad7e":"plt.figure(figsize=(20, 9))\nx_, y_ = Xn_tsne_2d.T\n\nplt.subplot(1, 2, 1)\nsns.scatterplot(x=x_, y=y_)\n\nplt.subplot(1, 2, 2)\nsns.scatterplot(x=x_, y=y_, hue=X['cluster'])","acb41118":"for col in X:\n    grid = sns.FacetGrid(X, col='cluster')\n    grid.map(plt.hist, col)","147405cb":"# TRYING TO UNDERSTAND THIS DATA\n- **CUSTID** : [Categorical] ID of the customer which freaks out the clustering, hence we're gonna drop this.\n\n- **BALANCE** : How rich is this guy?\n\n- **BALANCEFREQUENCY** : How often this guy tops up their account $$\\$?\n\n- **PURCHASES** : How much money this person spent so far?\n\n- **ONEOFFPURCHASES** : The costliest bill so far.\n\n- **INSTALLMENTSPURCHASES** : The bills this person hesitated to make in one go.\n\n- **CASHADVANCE** : Cash in advance given by this person\n\n- **PURCHASESFREQUENCY** : How Mr.Beast is this guy?\n\n- **ONEOFFPURCHASESFREQUENCY** : How un-hesitating Mr.Beast is this guy?\n\n- **PURCHASESINSTALLMENTSFREQUENCY** : How hesitating Mr.Beast is this guy?\n\n- **CASHADVANCEFREQUENCY** : How frequently the cash in advance being paid\n\n- **CASHADVANCETRX** : Number of Transactions made with \"Cash in Advanced\"\n\n- **PURCHASESTRX** : How busy is this guy purchasing things?\n\n- **CREDITLIMIT** : The limit of this guy's paradise\n\n- **PAYMENTS** : Amount of Payment done by user\n\n- **MINIMUM_PAYMENTS** : Minimum amount of payments made by user\n\n- **PRCFULLPAYMENT** : Percent of full payment paid by user\n\n- **TENURE** : Tenure of credit card service for user","e2af3148":"Now, back to analyzing sil-hou-ette scores.","283bc11b":"Skip the following cell, if you're _______ [not sure]","884dae95":"## HANDLING OUTLIERS\nNow, let's try to look at some crazy people from the given data. Technically, *Outliers*","2373ae9d":"PCA [Principal Component Analyziz] is used to reduce dimesnions of a dataset \/ dataframe \/ matrix \/ vectors \/ you \/ me \/ that bird you saw today morning, without losing much information. The information lies in the variance. The more the variance you aquire, the less the data you lose. It based on SVD [Singular Value Decomposition] which is based on a principle of factorizing any matrix into three different matrices with unit vectors and eigen values.","9edaa07a":"# HIERARCHIAL CLUSTERING \nNow, with hier-arch-ial clustering, in which every point is it's own individual cluster, then they start merging into bigger cluster until you stop them.\n\nTo stop them, you need a score, and that score has some styles. for example, \"linkage\", it has few styles \"single\", \"average\", \"complete\", \"centroid\", etc.....\n\n- `a(1234) b(5678)` - SINGLE LINKAGE - distance between `4` and `5`\n- `a(1234) b(5678)` - COMPLETE LINKAGE - distance between `1` and `8`\n- `a(1234) b(5678)` - AVERAGE LINKAGE - distance of `all_possible_pairs_between_clusters_distances.mean()`\n- `a(1234) b(5678)` - CENTROID LINKAGE - distance between `a.centroid` and `b.centroid`\n\nthere's also this cool dendRogram visualisation of clsutering for hier-arch-ial.","7214cd5f":"# KMEANS CLUSTERING \nNow that we have our data neat and ready, we shall start builing our model with \"Kay-Meens\" algorithm. (where it clusters the data into 'Kay' clusters, and each cluster center is it's 'Meen'). [I just wanted to make sure that you don't pronounce it as 'Kh-Mins']","0e5f1894":"Alright, [AIright](Nothing,_I_just_wanted_to_show_you_that_I_used_\"I\"_insted_of_\"l\"), Alright!\n\nNow, let's work on clustering these guys based on their behaviour.","99597bf3":"Phew, This dataset seems to be the dataset of outliers. There're ton of outliers. We can't just remove them. Nothing much we can do now except moving on with the weight of outliers...","2c6df0e6":"Now, to see why we need to normalize before PCA","f776aeb2":"# DBSCAN CLUSTERING \nNow, with DEE-BEE-SKAN [Density Based Spatial Clustering of Applications with Noise]\n\nThis thing works like a virus or a parasite. It accumulates the neighboring points into it's clusters [with a constraint]. We have to have two parameters. \n\n- maximum distance - if a distance to a point is less than this distance, it accumulates. Like 6 feet distance for COVID-19.\n- minimum samples - inorder to form a cluster, it needs to have these many points in range.\n\nWell, one way to find maximum distance (epsilon) (eps) is with NearestNeighbors.","1bb01bb7":"Let's try some more algorithms, shall we?","e037eb1b":"Well, I guess we can't continue with DBSCAN, because if you observe the peak score results in only 2 clusters, where 1 is a cluster and another is a outlier. This might be helpful if your goal is to identify outliers.","31d5e658":"## STANDARDIZATION\nI guess we need to standardize the data, before diving any deeper.\n\n> Stan-dar-dai-zing? What the heck is that? \n\nWell, Glad you asked. Let's say I've a friend, and she has these crazy super powers (let's call her 'meta-human'). So, one day while we're casually hanging out, and suddenly a group of 10 aliens appeared out of no where and started attacking. We did some freaking out and started fighting them. However, my friend as she has super powers, took down 7 aliens with her mighty powers. and I as a normal person, took down 3 aliens. Now the question is, who worked hard?\nThis is where **stan-dar-dai-zing** comes into play.\n\nLet's say, on \"average\", humans has enough capacity to take down **5 aliens ( \u00b1 1 alien)**.\n\nAnd, on \"average\", meta-humans has capability of taking down **10 aliens (\u00b1 2 aliens)**.\n\nSo, my Z-score (standardised value) is `(my_value - average_value) \/ std. deviation` => `(3 - 5) \/ 1` => `-2`\n\nmy friend's Z-score is => `(7 - 10) \/ 2` => `-3\/2` => `-1.5`\n\nNow that, we've Z-scores. we can compare these scores to get the result. (Feeling bad to say that my friend worked harder than me.)","69490b7b":"# VISUALISATION\nNow, to the exciting part.\n\n## NOI\u22a5\u03fd\u2229\u15e1\u018e\u1d1a \u028e\u22a5I\u02e5\u2200NOISN\u018eWI\u15e1 - PCA\nand the visualisation of clusters ","935ca0df":"- KMeans - 0.22\n- DBSCAN - Ineligible\n- Agglom - 0.83 [\u2713]","9334dc62":"Yeah, these guys never know that we're gonna use their neighbors' to profile them. Sad for them. Let's enquire 3 of their neighbors and ask about them. *Knock Knock, FBI!* ","9d8f046d":"> \"Ah, Sounds well!, But what's the point in all this? Few things are dependent of other, and other few are independent of some others.\"\n\nAlthough you missed, there's a point to all this.\nYou see, as few varibles depend on each other, they can help us (and our model) to make assumptions about the data that we need to form clusters. If you are crazy enough to imagine multi-dimensional space and each of these guys (customers) as a points in higher dimensional space, then these correlations can tell us how they're oriented with each other [orientation in terms of axes].","29519e02":"# PREPROCESSING","dcc397a8":"Now, back to business.","85591445":"Why do I feel that 8 is the right number. Well, let's continue with 8.","2fa25691":"That is how I'd do if I get drunk. Here, I fixed my cluster count myself without any insights. which is sh_t.\n\nNow that we discovered what to avoid, let's get working to find how many clusters we need.\n\nIn general, there's thing thing called `silhouette_score`, which scores the clusters. The more the score, the good the clusters.\n\nAnd there's also another metric for Kay-Meens, which is `Within Cluster Summation of Squares [WCSS]`. It tells about the squared distances between clusters. People often call it as \"Elbow method\", the process of identifying the elbow in the WCSS plot, which brings us good results.","f5ea56e1":"## HANDLING SKEW\nLet's check if these guys tend towards something like *anything..!*","bc13d0da":"Well, Sure they are. Let's teach them the manners. Using [log transformation](https:\/\/en.wikipedia.org\/wiki\/Data_transformation_(statistics)) to cure the skewness.","39e01b2d":"Alright. That's it. \n\nLet's look at the how features are classified with clusters.","21f0cc27":"## HANDLING NA\nNow, let's check if someone hesitated to provide the data","f0cd8ee8":"Wh-What's that?\n\nThat's not good, most of the points lie in the single cluster. Well, let's pick another model.","1f42d18d":"and interpretation of clusters for another version. Bye for now!","f0b727a8":"Anyways, it's all just for visualisation\n\n## t-SNE  \nNow, let's try t-SNE [t-Distributed Stochastic Neighbor Embedding], which is a manifold learning algorithm. Simply like Doctor Strange - It has ability to bend spaces. ","d6d49e7a":"Now, studying how well these guys' actions depend on each other","a5ec4daf":"Well, Now that they're in good shape, let's see how this affected the guys' profiles.","bc3853a3":"Just making sure that.., \\**you already know..\\**","4f169d82":"Reducing the skewness is all about achieveing symmetry.","d82ea559":"Well, let's go with the KMeans instead."}}