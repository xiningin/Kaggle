{"cell_type":{"73d50147":"code","df2c20ab":"code","5e912a36":"code","1fbc6b3b":"code","4ba5c630":"code","ebd2e106":"code","60d27f80":"code","60b6181a":"code","6d6295d5":"code","576d0a9f":"code","53e3c0df":"code","d1269458":"code","4ca58052":"code","8eed8041":"code","6a7a7558":"code","6bf65a07":"markdown","3d96c807":"markdown","b754c94c":"markdown","fffde731":"markdown","82ef8fed":"markdown","ee146180":"markdown","7a134e73":"markdown","1c87dbbc":"markdown","4e632a33":"markdown","a60aa19a":"markdown"},"source":{"73d50147":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n# for dirname, _, filenames in os.walk('\/kaggle\/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","df2c20ab":"os.environ[\"WANDB_DISABLED\"] = \"true\"\n!pip install jiwer","5e912a36":"import glob\n\npath = '..\/input\/notebook-1-prepare-training-dataset\/train\/*'\n\ntrain = []\nfor file in glob.glob(path):\n    # make label from filename\n    filename = os.path.basename(file)\n    label = filename.split('_')[0]\n    train.append([file, label])\n\ndf = pd.DataFrame(train, columns=['file_name', 'text'])\ndf","1fbc6b3b":"from sklearn.model_selection import train_test_split\n\ntrain_df, test_df = train_test_split(df, test_size=0.15)\n# we reset the indices to start from zero\ntrain_df.reset_index(drop=True, inplace=True)\ntest_df.reset_index(drop=True, inplace=True)","4ba5c630":"import torch\nfrom torch.utils.data import Dataset\nfrom PIL import Image\n\nclass IAMDataset(Dataset):\n    def __init__(self, root_dir, df, processor, max_target_length=128):\n        self.root_dir = root_dir\n        self.df = df\n        self.processor = processor\n        self.max_target_length = max_target_length\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        # get file name + text \n        file_name = self.df['file_name'][idx]\n        text = self.df['text'][idx]\n        # prepare image (i.e. resize + normalize)\n        image = Image.open(self.root_dir + file_name).convert(\"RGB\")\n        pixel_values = self.processor(image, return_tensors=\"pt\").pixel_values\n        # add labels (input_ids) by encoding the text\n        labels = self.processor.tokenizer(text, \n                                          padding=\"max_length\", \n                                          max_length=self.max_target_length).input_ids\n        # important: make sure that PAD tokens are ignored by the loss function\n        labels = [label if label != self.processor.tokenizer.pad_token_id else -100 for label in labels]\n\n        encoding = {\"pixel_values\": pixel_values.squeeze(), \"labels\": torch.tensor(labels)}\n        return encoding","ebd2e106":"from transformers import TrOCRProcessor\n\nprocessor = TrOCRProcessor.from_pretrained(\"microsoft\/trocr-base-handwritten\")\ntrain_dataset = IAMDataset(root_dir='',\n                           df=train_df,\n                           processor=processor)\neval_dataset = IAMDataset(root_dir='',\n                           df=test_df,\n                           processor=processor)","60d27f80":"print(\"Number of training examples:\", len(train_dataset))\nprint(\"Number of validation examples:\", len(eval_dataset))","60b6181a":"encoding = train_dataset[0]\nfor k,v in encoding.items():\n  print(k, v.shape)","6d6295d5":"image = Image.open(train_dataset.root_dir + train_df['file_name'][0]).convert(\"RGB\")\nprint('Label: '+train_df['text'][0])\nimage","576d0a9f":"labels = encoding['labels']\nprint(labels)\n\nlabels[labels == -100] = processor.tokenizer.pad_token_id\nlabel_str = processor.decode(labels, skip_special_tokens=True)\nprint('Decoded Label:', label_str)","53e3c0df":"from datasets import load_metric\n\ncer_metric = load_metric(\"cer\")\n\ndef compute_metrics(pred):\n    labels_ids = pred.label_ids\n    pred_ids = pred.predictions\n\n    pred_str = processor.batch_decode(pred_ids, skip_special_tokens=True)\n    labels_ids[labels_ids == -100] = processor.tokenizer.pad_token_id\n    label_str = processor.batch_decode(labels_ids, skip_special_tokens=True)\n\n    cer = cer_metric.compute(predictions=pred_str, references=label_str)\n\n    return {\"cer\": cer}","d1269458":"from transformers import VisionEncoderDecoderModel\n\nmodel = VisionEncoderDecoderModel.from_pretrained(\"microsoft\/trocr-base-stage1\")","4ca58052":"# set special tokens used for creating the decoder_input_ids from the labels\nmodel.config.decoder_start_token_id = processor.tokenizer.cls_token_id\nmodel.config.pad_token_id = processor.tokenizer.pad_token_id\n# make sure vocab size is set correctly\nmodel.config.vocab_size = model.config.decoder.vocab_size\n\n# set beam search parameters\nmodel.config.eos_token_id = processor.tokenizer.sep_token_id\nmodel.config.max_length = 10\nmodel.config.early_stopping = True\nmodel.config.no_repeat_ngram_size = 3\nmodel.config.length_penalty = 2.0\nmodel.config.num_beams = 4","8eed8041":"from transformers import default_data_collator\nfrom transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n\ntraining_args = Seq2SeqTrainingArguments(\n    num_train_epochs=3,\n    predict_with_generate=True,\n    evaluation_strategy=\"steps\",\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,\n    fp16=True, \n    output_dir=\".\",\n    logging_steps=2,\n    save_steps=1000,\n    eval_steps=200,\n    save_total_limit=1,\n)\n\n# instantiate trainer\ntrainer = Seq2SeqTrainer(\n    model=model,\n    tokenizer=processor.feature_extractor,\n    args=training_args,\n    compute_metrics=compute_metrics,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n    data_collator=default_data_collator,\n)\n\ntrainer.train()","6a7a7558":"os.makedirs(\"model\/\")\nmodel.save_pretrained(\"model\/\")","6bf65a07":"## Ref: Fine-tune TrOCR on the IAM Handwriting Database\nhttps:\/\/github.com\/NielsRogge\/Transformers-Tutorials\/blob\/master\/TrOCR\/Fine_tune_TrOCR_on_IAM_Handwriting_Database_using_Seq2SeqTrainer.ipynb","3d96c807":"## Setup WANDB, install jiwer","b754c94c":"## Save Model","fffde731":"## Split Train \/ Test","82ef8fed":"## Prepare training dataframe","ee146180":"## Metric: CER","7a134e73":"## Load pretrained model (microsoft\/trocr-base-stage1)","1c87dbbc":"## Prepare dataset processor","4e632a33":"## Run Seq2Seq Trainer","a60aa19a":"## View first training data item"}}