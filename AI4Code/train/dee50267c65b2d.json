{"cell_type":{"1fb1db10":"code","ccad06d8":"code","dfda3a7d":"code","ceb79e11":"code","6fe02be0":"code","bb931f52":"code","16504599":"code","c7d32c7b":"code","c73ef55e":"code","c6f90abd":"code","c8998fa6":"code","ca599602":"code","e93f3916":"code","f6de85c3":"code","03a0bc82":"code","52657abb":"code","304e5989":"code","87723e9d":"code","e394e67f":"code","11140d65":"code","fee1c3dd":"code","72100ee4":"code","45dc7a0c":"code","aaf4c263":"code","b1220878":"code","570ee47d":"code","2f811e43":"code","82b4e219":"code","4d024e7e":"code","e6939429":"code","f56b50ce":"code","c50ddf01":"code","8b2a0f8b":"code","a514be27":"markdown","7d38d13c":"markdown","63c7e9d1":"markdown","a4a6508b":"markdown","de4afe38":"markdown","ee1c33ba":"markdown","9088512d":"markdown","c0e3c59e":"markdown","eb966740":"markdown","0cad626b":"markdown","6e3fdd79":"markdown","f24fe1d0":"markdown","b08d627a":"markdown","9763bb51":"markdown","d0c2154a":"markdown","0329b545":"markdown","5efb8c32":"markdown","8588ef07":"markdown","8061a409":"markdown","934d12de":"markdown","9c7b16c5":"markdown","1a40f421":"markdown"},"source":{"1fb1db10":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nfrom pandas.api import types\nimport sklearn\nfrom sklearn.model_selection import train_test_split, RandomizedSearchCV\nfrom scipy.stats import skew, uniform, truncnorm, randint\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom mlxtend.regressor import StackingCVRegressor\nfrom lightgbm import LGBMRegressor\nfrom xgboost import XGBRegressor\n\nraw_train_df = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/train.csv\")\nraw_test_df = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/test.csv\")\nprint(raw_test_df.shape, raw_train_df.shape)","ccad06d8":"train_df = raw_train_df.drop('Id', axis=1)\ntest_df = raw_test_df.drop('Id', axis=1)","dfda3a7d":"missing_percentage = train_df.isna().sum() \/ train_df.isna().count()\nmissing_percentage = missing_percentage[missing_percentage > 0]\nmissing_percentage.sort_values(ascending=False)","ceb79e11":"# some to drop\nattributes_to_drop = missing_percentage[missing_percentage > 0.5].index\ntrain_df = train_df.drop(attributes_to_drop, axis=1, inplace=False)\n\nattributes_to_drop","6fe02be0":"# some to fill\nattributes_to_fill = missing_percentage[missing_percentage <= 0.5].index\nmissing_percentage[missing_percentage <= 0.5]","bb931f52":"# train_df['Alley'] = train_df['Alley'].fillna('None')\n# train_df['PoolQC'] = train_df['PoolQC'].fillna('None')\n# train_df['Fence'] = train_df['Fence'].fillna('None')\n# train_df['MiscFeature'] = train_df['MiscFeature'].fillna('None')\ntrain_df['LotFrontage'] = train_df['LotFrontage'].fillna(0)\ntrain_df['FireplaceQu'] = train_df['FireplaceQu'].fillna('None')\n\n# Garage related\ntrain_df['GarageType'] = train_df['GarageType'].fillna('None')\ntrain_df['GarageYrBlt'] = train_df['GarageYrBlt'].fillna(0)\ntrain_df['GarageFinish'] = train_df['GarageFinish'].fillna('None')\ntrain_df['GarageQual'] = train_df['GarageQual'].fillna('None')\ntrain_df['GarageCond'] = train_df['GarageCond'].fillna('None')\n\n# Basement related\ntrain_df['BsmtQual'] = train_df['BsmtQual'].fillna('None')\ntrain_df['BsmtCond'] = train_df['BsmtCond'].fillna('None')\ntrain_df['BsmtExposure'] = train_df['BsmtQual'].fillna('None')\ntrain_df['BsmtFinType1'] = train_df['BsmtFinType1'].fillna('None')\ntrain_df['BsmtFinType2'] = train_df['BsmtFinType2'].fillna('None')\n\n# Masonry veneer related\ntrain_df.dropna(subset=['MasVnrType', 'MasVnrArea', 'Electrical'], inplace=True)\n","16504599":"# To check whether all the missing values have been processed\nstill_missing = train_df.isna().sum()\nassert(len(still_missing[still_missing > 0]) == 0)","c7d32c7b":"sns.scatterplot(train_df['GrLivArea'], train_df['SalePrice'])","c73ef55e":"train_df = train_df.drop(train_df[(train_df['GrLivArea'] > 4000) & (train_df['SalePrice'] < 300000)].index)\nsns.scatterplot(train_df['GrLivArea'], train_df['SalePrice'])","c6f90abd":"train_df['TotalSF'] = train_df['TotalBsmtSF'] + train_df['1stFlrSF'] + train_df['2ndFlrSF']\nsns.scatterplot(train_df['TotalSF'], train_df['SalePrice'])","c8998fa6":"train_df = train_df.drop(train_df[(train_df['TotalSF'] > 6000) & (train_df['SalePrice'] < 500000)].index)\nsns.scatterplot(train_df['TotalSF'], train_df['SalePrice'])","ca599602":"train_df = train_df.drop('TotalSF', axis=1)","e93f3916":"sns.scatterplot(train_df['OverallQual'], train_df['SalePrice'])","f6de85c3":"train_df = train_df.drop(train_df[(train_df['OverallQual'] == 4) & (train_df['SalePrice'] > 220000)].index)\ntrain_df = train_df.drop(train_df[(train_df['OverallQual'] == 8) & (train_df['SalePrice'] > 500000)].index)\nsns.scatterplot(train_df['OverallQual'], train_df['SalePrice'])","03a0bc82":"# If 'PoolArea' > 0, then the house has a pool, 'hasPool' = 1\ntrain_df['hasPool'] = train_df['PoolArea'].apply(lambda x: int(bool(x)))\ntrain_df['hasGarage'] = train_df['GarageArea'].apply(lambda x: int(bool(x)))\ntrain_df['hasBsmt'] = train_df['TotalBsmtSF'].apply(lambda x: int(bool(x)))\ntrain_df['hasFireplace'] = train_df['Fireplaces'].apply(lambda x: int(bool(x)))\ntrain_df['has2ndfloor'] = train_df['2ndFlrSF'].apply(lambda x: int(bool(x)))","52657abb":"columns_to_drop = []\nfor col in train_df:\n    count = train_df[col].value_counts()\n    max_percentage = count.iloc[0] \/ len(train_df)\n    if max_percentage > 0.99:\n        columns_to_drop.append(col)\n\nprint(columns_to_drop)\ntrain_df = train_df.drop(columns_to_drop, axis=1)","304e5989":"test_df = test_df.drop(['Alley', 'PoolQC', 'Fence', 'MiscFeature'], axis=1)\n\ntest_df['LotFrontage'] = test_df['LotFrontage'].fillna(0)\ntest_df['FireplaceQu'] = test_df['FireplaceQu'].fillna('None')\n\ntest_df['GarageType'] = test_df['GarageType'].fillna('None')\ntest_df['GarageYrBlt'] = test_df['GarageYrBlt'].fillna(0)\ntest_df['GarageFinish'] = test_df['GarageFinish'].fillna('None')\ntest_df['GarageQual'] = test_df['GarageQual'].fillna('None')\ntest_df['GarageCond'] = test_df['GarageCond'].fillna('None')\n\ntest_df['BsmtQual'] = test_df['BsmtQual'].fillna('None')\ntest_df['BsmtCond'] = test_df['BsmtCond'].fillna('None')\ntest_df['BsmtExposure'] = test_df['BsmtQual'].fillna('None')\ntest_df['BsmtFinType1'] = test_df['BsmtFinType1'].fillna('None')\ntest_df['BsmtFinType2'] = test_df['BsmtFinType2'].fillna('None')\ntest_df['MasVnrType'] = test_df['MasVnrType'].fillna('None')\ntest_df['MasVnrArea'] = test_df['MasVnrArea'].fillna('None')\n\ntest_df['hasPool'] = test_df['PoolArea'].apply(lambda x: int(bool(x)))\ntest_df['hasGarage'] = test_df['GarageArea'].apply(lambda x: int(bool(x)))\ntest_df['hasBsmt'] = test_df['TotalBsmtSF'].apply(lambda x: int(bool(x)))\ntest_df['hasFireplace'] = test_df['Fireplaces'].apply(lambda x: int(bool(x)))\ntest_df['has2ndfloor'] = test_df['2ndFlrSF'].apply(lambda x: int(bool(x)))\n\ntest_df = test_df.drop(columns_to_drop, axis=1)\n\n# Fix NA values in test set, fill in with 0\/mode\ntest_df['MSZoning'] = test_df.groupby('MSSubClass')['MSZoning'].transform(lambda x: x.fillna(x.mode()[0]))\ntest_df['Functional'] = test_df['Functional'].fillna('Typ') \ntest_df['BsmtHalfBath'] = test_df['BsmtHalfBath'].fillna(0)\ntest_df['BsmtFullBath'] = test_df['BsmtFullBath'].fillna(0)\ntest_df['Electrical'] = test_df['Electrical'].fillna(\"SBrkr\") \ntest_df['KitchenQual'] = test_df['KitchenQual'].fillna(\"TA\") \ntest_df['TotalBsmtSF'] = test_df['TotalBsmtSF'].fillna(0)\ntest_df['BsmtUnfSF'] = test_df['BsmtUnfSF'].fillna(0)\ntest_df['BsmtFinSF1'] = test_df['BsmtFinSF1'].fillna(0)\ntest_df['BsmtFinSF2'] = test_df['BsmtFinSF2'].fillna(0)\ntest_df['Exterior1st'] = test_df['Exterior1st'].fillna(test_df['Exterior1st'].mode()[0])\ntest_df['Exterior2nd'] = test_df['Exterior2nd'].fillna(test_df['Exterior1st'].mode()[0])\ntest_df['SaleType'] = test_df['SaleType'].fillna(test_df['SaleType'].mode()[0])\ntest_df['GarageCars'] = test_df['GarageCars'].fillna(0)\ntest_df['GarageArea'] = test_df['GarageArea'].fillna(0)\n\nstill_missing = test_df.isna().sum() \/ test_df.isna().count()\nassert(len(still_missing[still_missing > 0]) == 0)","87723e9d":"X_df = train_df.drop('SalePrice', axis=1, inplace=False)\nY_df = train_df['SalePrice']\nall_X_df = pd.concat([X_df, test_df])","e394e67f":"sns.distplot(Y_df)\nprint(\"Skewness of 'SalePrice': {}\".format(skew(Y_df)))","11140d65":"Y_df = Y_df.apply(lambda x: np.log(x + 1))\nsns.distplot(Y_df)\nprint(\"Skewness of log('SalePrice' + 1): {}\".format(skew(Y_df)))\n\nfor i in all_X_df.columns:\n    if types.is_numeric_dtype(all_X_df[i]):\n        skewness = skew(all_X_df[i])\n        if skewness > 0.5:\n            all_X_df[i] = all_X_df[i].apply(lambda x: np.log(x + 1))\nall_X_df.head()","fee1c3dd":"# one-hot\nprint(all_X_df.shape)\nall_X_df = pd.get_dummies(all_X_df)\nall_X_df.head()\nprint(all_X_df.head())","72100ee4":"train_size = len(Y_df)\nX_df = all_X_df.iloc[:train_size, :]\ntest_df = all_X_df.iloc[train_size:, :]","45dc7a0c":"X_train, X_test, Y_train, Y_test = train_test_split(X_df, Y_df, test_size=0.15, random_state=88)\nX_train.head() ","aaf4c263":"X = (X_train, X_test)\nY = (Y_train, Y_test)","b1220878":"def rmsle(y, y_pred):\n    return np.sqrt(mean_squared_error(y, y_pred))\n\ndef eval(model, X, Y):\n    X_train, X_test = X\n    Y_train, Y_test = Y\n    model.fit(X_train, Y_train)\n    print(type(model))\n    print(\"On train set:\", rmsle(Y_train, model.predict(X_train)))\n    print(\"On test set:\", rmsle(Y_test, model.predict(X_test)))\n\ndef random_search(model, params, X, Y):\n    X_train, X_test = X\n    Y_train, Y_test = Y\n    new_model = RandomizedSearchCV(model, params, n_iter=10, scoring='neg_mean_squared_error', cv=5, random_state=1)\n    new_model.fit(X_train, Y_train)\n    print(\"Random search...\")\n    print(new_model.best_params_)\n    print(\"On train set:\", rmsle(Y_train, new_model.predict(X_train)))\n    print(\"On test set:\", rmsle(Y_test, new_model.predict(X_test)))\n    return new_model.best_estimator_\n","570ee47d":"rfr = RandomForestRegressor(n_estimators=1000, criterion=\"mse\", max_depth=50, min_samples_split=2, min_samples_leaf=1,\n                            max_features=0.8, max_samples=0.8)\neval(rfr, X, Y)","2f811e43":"gbr = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05, max_depth=4, max_features='sqrt', \n                                min_samples_leaf=15, min_samples_split=10, loss='huber', random_state=88)    \neval(gbr, X, Y)                         ","82b4e219":"lgbm = LGBMRegressor(objective='regression', num_leaves=4, learning_rate=0.01, n_estimators=6000,\n                     max_bin=200, bagging_fraction=0.8, bagging_freq=5, bagging_seed=10,\n                     feature_fraction=0.1, feature_fraction_seed=10)\neval(lgbm, X, Y)","4d024e7e":"xgboost = XGBRegressor(learning_rate=0.05, n_estimators=4000, max_depth=3, min_child_weight=1,\n                       gamma=0.0006, subsample=0.5, colsample_bytree=0.5, objective='reg:squarederror', \n                       nthread=-1, scale_pos_weight=1, seed=88, reg_alpha=0.00006)\neval(xgboost, X, Y)","e6939429":"stacking = StackingCVRegressor(regressors=(rfr, gbr, lgbm, xgboost), meta_regressor=xgboost, use_features_in_secondary=True)\neval(stacking, X, Y)","f56b50ce":"def averaging(X):\n    return 0.1 * rfr.predict(X) + 0.2 * gbr.predict(X) + 0.2 * lgbm.predict(X) + 0.2 * xgboost.predict(X) + 0.3 * stacking.predict(X)\n\nprint(\"On train set:\", rmsle(Y_train, averaging(X_train)))\nprint(\"On test set:\", rmsle(Y_test, averaging(X_test)))","c50ddf01":"rfr.fit(X_df, Y_df)\ngbr.fit(X_df, Y_df)\nlgbm.fit(X_df, Y_df)\nxgboost.fit(X_df, Y_df)\nstacking.fit(X_df, Y_df)","8b2a0f8b":"predictions = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv\")\npredictions.iloc[:,1] = np.expm1(averaging(test_df))\npredictions.to_csv(\"submission.csv\", index=False)","a514be27":"#### Encoding for Categorical Features\n\nUse one-hot dummy encoding.","7d38d13c":"#### Skewness Analysis for Numeric Features\nWhen we look at the distribution of 'SalePrice', it is clearly skew and not normal. So it needs to be transformed before the regression.\n\nApply log to make it normal, and do the same for all the numeric features.","63c7e9d1":"Then concatenate train set and test set together to do some encoding.","a4a6508b":"# House Prices - Advanced Regression Techniques\nZhang Bolun \/ bolunz@u.nus.edu","de4afe38":"Use the stacking of four types of model: Random Forest, Gradient Boosting, LGBM and XGBootst.\n\nThe parameters might not be optimal, since I didn't spend much time tuning. But I left a random search function above for future improvement.","ee1c33ba":"#### Total area square feet","9088512d":"### Missing Values","c0e3c59e":"Train the models with full data.","eb966740":"### Outliers\nIt is common sense that there should be a positive correlation between some attributes and sale price of a house, such as the total area and overall quality. In this way, we examine these relationships to find outliers.\n\n#### GrLivArea - Above grade (ground) living area square feet","0cad626b":"#### Overall quality","6e3fdd79":"The score of my final submission is 0.12407, ranking 586 on the leaderboard by the time I submitted.","f24fe1d0":"## Train the Model","b08d627a":"## Data Pre-processing","9763bb51":"#### Avoid overfitting\nSimilar to what we've done to drop those columns filled with a majority of NA values. For a column, if the percentage of a certain value reaches a level, we consider to drop that whole column.","d0c2154a":"We can find that most of the NA values are not really 'missing values', instead they indicate that the property does not have a certain element. like the NA in those who start with 'Garage' or 'Bsmt' suggest the house does not have a garage or basement. This way we can simply fill the NA cells with infomation that indicates the absence of certain elements.\n\nHowever, for some attributes like 'PoolQC', 'MiscFeature' and 'Alley', most of the houses are not equipped with these elements, which means these features may not be critical, so we consider to simply drop these columns. Actually for an attribute, if the percentage of NA values reaches a certain level(say 50%), then we can drop the whole column.\n\nFor 'MasVnrType', it already has a category of 'None' in it, so the NA values in this column does not necessarily mean 'doesn't have masonry veneer'. Instead, it might be actually unknown. So we consider to drop those entries with NA values. Same procedure for 'Electrical'.","0329b545":"## Conclusions and Reflections","5efb8c32":"Split 15% of the training data into testing set at the convenience of evaluation. For the final submission, all the training data should be used to train the model.","8588ef07":"Finally, take the weighted average of all the predictions by the models above as the final prediction.\n\nAgain, the weights below is a naive assignment, which can be carefully tuned instead of remaining a uniform weight. Since Random Forest Regressor seems more unstable, I set it to 0.1 and the final stacking model to 0.3.","8061a409":"Finally, do the same thing to test dataset. For test set, there are some more columns that contain NA values, which need to be filled.","934d12de":"This is a classic regression problem, and the key point is to fully understand the dataset, since the results have shown that there is not much difference between the performance of these several models. \n\nTODOS:\n- **Processing missing data**: In this case, test set seems to have more missing data. Simply filling in the mode or a default value might actually affect the result.\n- **More feature engineering**: There must be more hidden features to generalize; also we can do some in-depth correlation analysis to choose to drop some unimportant features\n- **Finding outliers**: I only look at several variables like total area and overall quality, instinctively; there must be more hidden outliers if we did an exhausted examination on more variables.\n- **Further tuning** on model parameters: Use random search.\n- **Deep learning methods**: The data in this case is not that complicated, neural network based methods might cause severe overfitting, but still worth a try.","9c7b16c5":"### Features\n#### Hidden Categorical Features\n\nInspired by the dealing with NA value process, we find that there are many elements to consider in terms of a property, like pool, garage, basement. However, in the csv data file, there seems not to be any direct categorical features to indicate the presence of these elements, instead it has some numeric features like 'PoolArea' and 'GarageArea', etc. It is necessary to add these features to directly distinguish houses with a pool from those without one.","1a40f421":"## Prediction"}}