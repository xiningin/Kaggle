{"cell_type":{"05a978de":"code","478f4af1":"code","5e433c06":"code","4aaf452f":"code","1c896875":"code","c969408f":"code","b9d09115":"code","027501c1":"code","4c0ac9ba":"markdown","8b13563d":"markdown","2c28cd3d":"markdown","720a230b":"markdown","1890a9e0":"markdown","4b1edc95":"markdown","51b1729d":"markdown","57b3a36d":"markdown","88355de7":"markdown"},"source":{"05a978de":"#!\/usr\/bin\/python3\n# coding=utf-8\n#===========================================================================\n# load up the libraries\n#===========================================================================\nimport pandas  as pd\nimport matplotlib.pyplot as plt\n\n#===========================================================================\n# read in the data \n# (2 month rolling leaderboard data, downloaded on the 3.V.2020)\n#===========================================================================\ntitanic_lb = pd.read_csv('..\/input\/titanic-publicleaderboarddata-4v2020\/titanic-publicleaderboard.csv')\n\n#===========================================================================\n# make a frequency table\n#===========================================================================\nfrom collections import Counter\ntitanic_ft = Counter(titanic_lb[\"Score\"])","478f4af1":"plt.figure(figsize=(16,6))\nplt.xlabel  (\"Score\")\nplt.ylabel  (\"Frequency\")\nplt.xlim((0.0,1.0))\nplt.bar(titanic_ft.keys(), titanic_ft.values(), width=0.004)\nplt.show()","5e433c06":"plt.figure(figsize=(16,6))\nplt.xlabel  (\"Score\")\nplt.ylabel  (\"Frequency\")\nplt.xlim((0.6,0.85))\nplt.bar(titanic_ft.keys(), titanic_ft.values(), width=0.004)\nplt.show()","4aaf452f":"# find the maximum value (i.e. most frequent score) \n# and its corresponding key\nmaximum = max(titanic_ft, key=titanic_ft.get)\n# calculate the percentage of submissions that have this score\npercentage_max_score = ((100\/titanic_lb.shape[0])*titanic_ft[maximum])\nprint(\"Percentage of people with the most frequent score is:\",\n      str(round(percentage_max_score, 2)),\"%\")","1c896875":"# print the number of 'perfect' solutions\nprint(\"Number of 'perfect' (1.00000) submissions is: %i\" % titanic_ft[1.0])","c969408f":"# sum the number of submissions with a score > 0.8\nsum = 0\nfor key in titanic_ft:\n    if key > 0.8:\n        sum = sum + titanic_ft[key]\nprint(\"Number of submissions whose score is greater than 0.8 is:\",sum)","b9d09115":"# take away the 1.00000 bin\nnumber_gt_8_correct = sum - titanic_ft[1.0]\nprint(\"less those with a perfect 1.00000 is:\", number_gt_8_correct)","027501c1":"percentage_gt_eight = ((100\/titanic_lb.shape[0])*number_gt_8_correct)\nprint(\"Submissions with a score greater than 0.8 are in the top\", \n      str(round(percentage_gt_eight, 2)),\"%\")","4c0ac9ba":"# So, how great is 0.8?\nNow, to answer *How good is a score greater than 0.8?*","8b13563d":"### The 0.62679 peak: <font color='red'>(now 0.62200, see below for details)<\/font>\nThis peak corresponds to people submitting the [*'all dead'* submission file](https:\/\/www.kaggle.com\/carlmcbrideellis\/titanic-all-zeros-csv-file), in other words, in the submission file all `PassengerId` are assigned a zero, corresponding to no survivors. If we multiply `0.62200` by the number of entries in the file, `418`, shows us that the simple *all dead* file actually correctly guesses 260 data points. This in some way is a stark reminder of the tragedy that was the Titanic.\n### The 0.77511 peak:\nMost classifiers will give this result if very little feature engineering or hyperparameter tuning has been performed. The score corresponds to correctly classifying 324 passengers. For example, the script for the random forest model in the aforementioned *Titanic Tutorial* notebook, in conjunction with the parameters which were provided, returns this score of 0.77511.\n### The 1.00000 peak:\nThis is a most disappointing peak, as we have mentioned earlier, it is the result of cheating.\n\nWe can make a count of the number of these unfair submissions, currently there are over 100 of them:","2c28cd3d":"Firstly, we shall make a frequency plot of the *whole* leaderboard to get an overall feeling for the data:","720a230b":"We can see that the majority of the scores lie between `0.6` and `0.85`, so we shall zoom in on that area:","1890a9e0":"We can clearly see a distribution of scores, and in particular, three interesting peaks.\n### The 0.76555 peak:\nThis is by far the highest peak and corresponds to correctly classifying 320 results. This peak is due to people submitting the default `gender_submission.csv` file provided by the competition. The submission of this file alone represents more than 20% of the results that are seen on the leaderboard. This is not entirely surprising, given that the excellent [Titanic Tutorial](https:\/\/www.kaggle.com\/alexisbcook\/titanic-tutorial) by Alexis Cook suggests doing this as an exercise.","4b1edc95":"(\\*) Note: The data used in this notebook is taken from a snapshot of the 2 month rolling results, which are subject to change over time.\n<br><br><font color='red'>**Note**: As of the 1st of July 2020 there is now only one unified leaderboard, calculated using *all* the data. This slightly changes some of the 'old' scores. See<\/font> [\"Getting Started Competitions - 100% Public Leaderboard\"](https:\/\/www.kaggle.com\/c\/titanic\/discussion\/163366) \n<font color='red'>for details.<\/font>. ","51b1729d":"#### Answer: a score > 0.8 is well within the top 6%, and more than good enough for a gold medal\n### Now that is great!\n#### PS: If you are wondering, a score of above 0.785 would sufficient land yourself a silver medal (\\*)\n\n### Honourable mention: A score of 0.00000\nOccasionally people obtain a score of `0.00000` no matter what they do. This is *almost certainly* caused by submitting a `submission.csv` for which the `Survived` column is made up of floating point numbers (i.e. `0.0 and 1.0`) rather than integers (`0` and `1`). This can be easily remedied by simply passing them through `.astype(int)` before writing.\n\n### **See also my notebook:** [\"Titanic: In all the confusion...\"](https:\/\/www.kaggle.com\/carlmcbrideellis\/titanic-in-all-the-confusion) which covers the meaning of:\n* the **accuracy score**, \n* the **confusion matrix**, \n* the **$F_1$ score**, \n* the **ROC curve** and \n* the **AUC** (area under the curve)\n\nusing the Titanic data as an example.","57b3a36d":"we shall now remove the 'perfect' scores from our tally, as they are basically rubbish:","88355de7":"### Dear all,\nThe objective of this notebook is to shed some light on the question of *How good is my score in the Titanic competition?*\nWe all know the feeling of clicking on the '*Jump to your position on the leaderboard*', and we get a score of 0.7xxx (well, as least I do) then the next thing we do is scroll up and see all these 1.00000 results and we feel that \nwe are doing something wrong. \n\nThe first thing to say is that these *1.00000* submissions are cheating! Basically it works because the *hidden* data against which your submission is tested is actually [*public*](https:\/\/en.wikipedia.org\/wiki\/Passengers_of_the_RMS_Titanic#Passenger_list) data, which can be found on Wikipedia, etc. (it is the *Titanic* after all). In other words, if you hand-craft the submission file you will score a 1, with no machine learning required; hardly any need for python or R code, no need for trying new methods and learning new techniques, etc. etc.\n\nWhat is the point in doing that? **I really don't know...**\n\nOk, so, how good is my score? First we shall load in a snapshot of the leaderboard. This data can be downloaded directly from [the Titanic leaderboard page ](https:\/\/www.kaggle.com\/c\/3136\/publicleaderboarddata.zip) where it says Raw Data.\n"}}