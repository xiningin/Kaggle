{"cell_type":{"13630dca":"code","9cb4055e":"code","524d4b8e":"code","55e65f28":"code","7cdcf92d":"code","f0b3344a":"code","a283cbd2":"code","35499288":"code","126c4473":"code","5fa34981":"code","a95d91fa":"code","341329c1":"code","2db13881":"code","d5f8dfb0":"code","afc35133":"code","7f0768e8":"code","6e9673fb":"code","9a243969":"code","f873f668":"markdown","c4b5288b":"markdown","184dd9b4":"markdown","14242361":"markdown","b413ee0d":"markdown","017f7bf0":"markdown","af16f653":"markdown"},"source":{"13630dca":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\nimport numpy as np\nimport pandas as pd\nimport os\nimport matplotlib.image as mpimg\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Dataset\nimport torchvision.transforms as transforms\nfrom sklearn.decomposition import PCA\nimport os\nimport cv2\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport matplotlib.pyplot as plt\nimport seaborn as sns \nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom time import time\n\nfrom scipy import ndimage\n\nfrom sklearn import manifold, datasets\nimport glob\n\nfrom scipy.cluster.hierarchy import dendrogram\nfrom sklearn.cluster import AgglomerativeClustering\n\n# Any results you write to the current directory are saved as output.","9cb4055e":"train_dir = \"..\/input\/train\/train\/\"\ntest_dir = \"..\/input\/test\/test\/\"","524d4b8e":"train_path=train_dir\ntest_path=test_dir\ntrain_set = pd.read_csv('..\/input\/train.csv').sort_values('id')\ntrain_set.sort_values('id')\ntrain_labels = train_set['has_cactus']\ntrain_labels.head()","55e65f28":"sns.countplot(train_labels)","7cdcf92d":"files = sorted(glob.glob(train_path + '*.jpg'))\n\ntrain = [cv2.imread(image) for image in files]\n\ntrain = np.array(train, dtype='int32')\n\ntrain_images_set = np.reshape(train,[train.shape[0],train.shape[1]*train.shape[2]*train.shape[3]])","f0b3344a":"def plot_clustering(X_red, labels, title=None):\n    \n    # calculating the minimum and maximum values, so that we can use it to normalize X_red within min\/max range for plotting\n    x_min, x_max = np.min(X_red, axis=0), np.max(X_red, axis=0)\n    X_red = (X_red - x_min) \/ (x_max - x_min)\n    # setting the figure size or plot size\n    plt.figure(figsize=(6, 4))\n    for i in range(X_red.shape[0]):\n        # Plotting the text i.e. numbers\n        plt.text(X_red[i, 0], X_red[i, 1], str(labels[i]),\n                 color=plt.cm.seismic(labels[i]),\n                 fontdict={'weight': 'bold', 'size': 9})\n        \n    plt.xticks([])\n    plt.yticks([])\n    if title is not None:\n        plt.title(title, size=17)\n    plt.axis('off')\n    plt.tight_layout()","a283cbd2":"pca = PCA(n_components=2)\nprincipalComponents = pca.fit_transform(train_images_set)\nprincipalDf = pd.DataFrame(data = principalComponents, columns = ['principalcomponent1',\n                                                                  'principalcomponent2'])","35499288":"print(\"Computing embedding\")\n# Converting the data into 2D embedding\nX_red = manifold.SpectralEmbedding(n_components=2).fit_transform(principalDf)\nprint(\"Done.\")","126c4473":"from sklearn.cluster import AgglomerativeClustering\n\n# Calling the agglorimative clustering function from sklearn library.\nclustering = AgglomerativeClustering(linkage='ward', n_clusters=10)\n# startitng the timier\nt0 = time()\n# Fitting the data in agglorimative function on order to train it\nclustering.fit(X_red)\n# printing the time taken\nprint(\"%s : %.2fs\" % (\"linkage\", time() - t0))\n# Plotting the cluster distribution\nplot_clustering(X_red, train_labels, \"Agglomerative Clustering- distribution of clusters\" )\n\nplt.show()","5fa34981":"from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis as QDA\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_curve, auc\n\nX_train, X_test, y_train, y_test = train_test_split(principalDf, train_set['has_cactus'], test_size=0.33, random_state=42)\n  \nclf = QDA(store_covariance = True, tol = 0.000000001)\nclf.fit(X_train, y_train)\n\ny_pred = clf.predict(X_test)\nfrom sklearn.metrics import accuracy_score\naccuracy_score(y_test, y_pred)","a95d91fa":"LDA_CLF = LDA(solver = 'lsqr', tol=0.000000001)\nLDA_CLF.fit(X_train,y_train)\ny_lda_pred = LDA_CLF.predict(X_test)\naccuracy_score(y_test, y_lda_pred)","341329c1":"labels = pd.read_csv(\"..\/input\/train.csv\")\n\n\nclass ImageData(Dataset):\n    def __init__(self, df, data_dir, transform):\n        super().__init__()\n        self.df = df\n        self.data_dir = data_dir\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, index):       \n        img_name = self.df.id[index]\n        label = self.df.has_cactus[index]\n        \n        img_path = os.path.join(self.data_dir, img_name)\n        image = mpimg.imread(img_path)\n        image = self.transform(image)\n        return image, label\nlabels.head()","2db13881":"epochs = 25\nbatch_size = 20\ndevice = torch.device('cpu')\n\ndata_transf = transforms.Compose([transforms.ToPILImage(), transforms.ToTensor()])\ntrain_data = ImageData(df = labels, data_dir = train_dir, transform = data_transf)\ntrain_loader = DataLoader(dataset = train_data, batch_size = batch_size)\n\n#train_num = train_loader.numpy()\n\nnum_classes = 2","d5f8dfb0":"class ConvNet(nn.Module):\n    def __init__(self, num_classes=10):\n        super(ConvNet, self).__init__()\n        self.layer1 = nn.Sequential(\n            nn.Conv2d(3, 10, kernel_size=5, stride=1, padding=2),\n            nn.BatchNorm2d(10),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n            nn.Dropout2d(p=0.2))\n        self.layer2 = nn.Sequential(\n            nn.Conv2d(10, 32, kernel_size=5, stride=1, padding=2),\n            nn.BatchNorm2d(32),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2),nn.Dropout2d(p=0.5))\n        self.layer3 = nn.Sequential(\n            nn.Conv2d(32, 64, kernel_size=5, stride=1, padding=2),\n            nn.BatchNorm2d(64),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2))\n        self.fc = nn.Linear(1024, num_classes)\n        \n    def forward(self, x):\n        out = self.layer1(x)\n        out = self.layer2(out)\n        out = self.layer3(out)\n        out = out.view(out.shape[0],-1)\n        out = self.fc(out)\n        return out","afc35133":"net = ConvNet().to(device)\noptimizer = optim.Adam(net.parameters(), lr=0.001)\nloss_func = nn.CrossEntropyLoss()","7f0768e8":"train_loader","6e9673fb":"for epoch in range(epochs):\n    for i, (images, labels) in enumerate(train_loader):\n        images = images.to(device)\n        labels = labels.to(device)\n        # Forward\n        outputs = net(images)\n        loss = loss_func(outputs, labels)\n        # Backward and optimize\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        \n        if (i+1) % 500 == 0:\n            print ('Epoch [{}\/{}], Loss: {:.4f}'.format(epoch+1, epochs, loss.item()))","9a243969":"submit = pd.read_csv('..\/input\/sample_submission.csv')\ntest_data = ImageData(df = submit, data_dir = test_dir, transform = data_transf)\ntest_loader = DataLoader(dataset = test_data, shuffle=False)\n\npredict = []\nfor batch, (data, target) in enumerate(test_loader):\n    data, target = data.to(device), target.to(device)\n    output = net(data)\n    \n    num, pred = torch.max(output.data, 1)\n    predict.append(int(pred))\n\nsubmit['has_cactus'] = predict\nsubmit.to_csv('submission.csv', index=False)","f873f668":"## Agglomerative Clustering : \n\nIn this technique, initially each data point is considered as an individual cluster. At each iteration, the similar clusters merge with other clusters until one cluster or K clusters are formed.\nThe basic algorithm of Agglomerative is straight forward.\n\n1. Compute the proximity matrix\n2. Let each data point be a cluster\n3. Repeat: Merge the two closest clusters and update the proximity matrix\n4. Until only a single cluster remains\n5. Key operation is the computation of the proximity of two clusters","c4b5288b":"![link](https:\/\/i.imgur.com\/kllrJtc.png)","184dd9b4":"This clustering technique is divided into two types:\n* Agglomerative\n* Divisive\n\nWe are going to look into agglomertive clustering in this kernel.","14242361":"#### Hierarchical clustering is the hierarchical decomposition of the data based on group similarities","b413ee0d":"### Clustering\nClustering, in one sentence, is the extraction of natural groupings of similar data objects.\n\nThere are a couple of general ideas that occur quite frequently with respect to clustering:\n* The clusters should be naturally occurring in data.\n* The clustering should discover hidden patterns in the data.\n* Data points within the cluster should be similar.\n* Data points in two different clusters should not be similar.\n* Common algorithms used for clustering include K-Means, DBSCAN, and Gaussian Mixture Models.\n\n","017f7bf0":"# Clustering\n\n![Link](https:\/\/i.imgur.com\/qg73zpI.png)\n","af16f653":"## Linear Discriminant Analysis\n\nLinear Discriminant Analysis is a generative model for classification. It is a generalization of Fisher\u2019s linear discriminant. LDA works on continuous variables. If the classification task includes categorical variables, the equivalent technique is called the discriminant correspondance analysis.\n\nThe goal of Linear Discriminant Analysis is to project the features in higher dimension space onto a lower dimensional space to both reduce the dimension of the problem and achieve classification.\n\nIt would take a lot of time to do LDA on this data because it has 3072 columns. So I have applied PCA on it.\nThis is just a demonstartion of LDA in reality we do not apply PCA and then do LDA because in PCA we are interested in data points with the largest variations. In LDA, we are interested in maximizing the separability between the 2 known data groups to make better decisions.\n\n"}}