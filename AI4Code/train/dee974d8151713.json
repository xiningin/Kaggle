{"cell_type":{"6cf47c1d":"code","20274938":"code","e1648da8":"code","d0efb8d8":"code","e9d4e0c5":"code","510d3799":"code","03ac79f4":"code","b823d446":"code","e41fdb2f":"code","727adc4c":"code","22e0a137":"code","c5f71b2f":"code","50bc1b0f":"code","7303cd8c":"code","e8071100":"code","efbd136e":"code","9d13a929":"code","72da9531":"code","bc840a8b":"code","3a4cd3fc":"code","09b66256":"code","4ce9daac":"code","9f62fcc6":"code","b613dd49":"code","8933508e":"code","71d6a749":"code","2ccf8cbb":"code","09b49566":"code","1e8f6d59":"code","fdc6932f":"code","f6e6921f":"code","5242bc50":"code","7e1f4905":"code","28f6d32a":"code","38216a87":"code","3cfa97fa":"code","563d9580":"code","c36075aa":"code","03019d03":"code","d63b7fff":"code","5f9c71b2":"code","442d8460":"code","3e98502e":"code","83c7fc6a":"code","54018c38":"code","d0708c98":"code","36e8b98b":"code","8cbc049d":"code","6b76d91d":"code","0df7f497":"code","90f25799":"code","dce0b70e":"code","63336391":"code","2d666af6":"code","993a010c":"code","1b8bcea0":"code","92736cdb":"code","39bac639":"code","19456b1b":"code","a582d0dd":"code","0fce259f":"code","6a295b8e":"code","9abdeb8e":"markdown","b4bcdb3f":"markdown","64e46823":"markdown","03239593":"markdown","9b229ba4":"markdown","366212dc":"markdown","fbb4e2de":"markdown","9cce51b2":"markdown","b79d1258":"markdown","314b9434":"markdown","6a705bda":"markdown","a09ec9b9":"markdown","8ea59040":"markdown","42d6cf5c":"markdown","6b64d3f1":"markdown","ff783ad9":"markdown","1adf8d2e":"markdown","7a0bdf69":"markdown","5f08ee9e":"markdown","96961194":"markdown","b7b7ce66":"markdown","52284172":"markdown","eda6ad1a":"markdown","71d656b1":"markdown","20706926":"markdown","fc10e2ec":"markdown","e5fe114e":"markdown","f6cf4b22":"markdown","f7b3ee96":"markdown","82e5f62f":"markdown","b0ea9013":"markdown","9270e781":"markdown","7c99f3dc":"markdown","2912f7ac":"markdown","9fe84753":"markdown","c087253e":"markdown","773194d0":"markdown","2d14bafb":"markdown"},"source":{"6cf47c1d":"!pip install padelpy","20274938":"!pip install xgboost","e1648da8":"!pip install scikit-learn==1","d0efb8d8":"# Import libraries\nimport zipfile\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport glob\nfrom padelpy import padeldescriptor\nfrom scipy.stats import mannwhitneyu\nfrom sklearn.feature_selection import VarianceThreshold\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nimport xgboost as xgb\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score, f1_score","e9d4e0c5":"# Download data\n! mkdir Data \n! wget https:\/\/github.com\/dataprofessor\/beta-lactamase\/raw\/main\/beta_lactamase_CHEMBL29.zip --directory .\/Data","510d3799":"# Concatenate all cv files from zip file into a single unified dataframe\nzf = zipfile.ZipFile(\"Data\/beta_lactamase_CHEMBL29.zip\", \"r\")\ndf = pd.concat( (pd.read_csv( zf.open(f) ) for f in zf.namelist() ) )","03ac79f4":"# Take a look at the dataframe\ndf","b823d446":"# General measurements of the dataset \ndf.info()","e41fdb2f":"# Bar plot of Missing vs Non-Missing Data of pCHEMBL values\n\n# Data\nmissing = df.pchembl_value.isnull().sum()\nnonmissing = df.pchembl_value.notnull().sum()\n\nx = ['Missing', 'Non-Missing']\ny = [missing, nonmissing]\n\n# Setup plot\nfig, ax = plt.subplots()\n\n# Make bar plot\np = ax.bar(x, y, color = ['#F8766D', '#00BFC4'], ec = 'black')\n\nax.set_title('pChEMBL Missing Data', fontsize=14, fontweight='bold', pad=15)\n\nax.set_ylim(0,70000)\nplt.xticks(fontsize=14)\nplt.yticks(fontsize=14)\n\n# Label above bar\nfor index, data in enumerate(y):\n    plt.text(x=index-0.1 , y =data+1000 , s=f\"{data}\" , fontdict=dict(fontsize=14))\n\nfig.set_size_inches(5,4.5)\nplt.show()","727adc4c":"# Dataset with non-missing pChEMBL values\ndf2 = df[df.pchembl_value.notnull()]","22e0a137":"# General measurements of the dataset \ndf2.info()","c5f71b2f":"# Verify duplicated molecules by ChEMBL ID\nprint('Number of unique ChEMBL ID:', str(len(df2.molecule_chembl_id.unique()) )  )\nprint('Total number of ChEMBL ID: ', str(len(df2)))","50bc1b0f":"def combine_molecule_duplicates(dataset):\n    '''\n    Function to replace duplicated molecules by one single row with mean of standard_value and \n    pchembl_value of duplicated rows. The replacement is applied only if standard deviation of \n    pchembl_value is lower than 2.\n\n        Parameters:\n            dataset (DataFrame): A pandas DataFrame with duplicated molecules\n        \n        Returns: \n            dataset (DataFrame): A pandas DataFrame without duplicated molecules\n    '''\n    # Calculate standard deviation of all molecules\n    std_by_uniqueID = dataset.groupby(\"molecule_chembl_id\").std()\n\n    # Filter standard deviation of duplicated molecules lower than 2\n    std_by_uniqueID = std_by_uniqueID[std_by_uniqueID.pchembl_value < 2]\n\n    # Calculate mean of all molecules\n    mean_by_uniqueID = dataset.groupby(\"molecule_chembl_id\").mean()\n    \n    # Filter mean of duplicated molecules that have standard deviation lower than 2\n    mean_by_uniqueID = mean_by_uniqueID.filter(items = std_by_uniqueID.index, axis=0)\n\n    # Create a dictionary of rows with mean values of standard_value and pchembl_value\n    new_rows = {}\n\n    for i in mean_by_uniqueID.index:\n        rows = dataset.loc[dataset.molecule_chembl_id == i].copy()\n        row = rows.iloc[0].copy()\n        row.standard_value = mean_by_uniqueID.loc[i].standard_value\n        row.pchembl_value = mean_by_uniqueID.loc[i].pchembl_value\n        new_rows[i] = row\n    \n    # Convert dictionary to dataframa\n    df_new_rows = pd.DataFrame(new_rows).T\n    \n    # Delete duplicated molecules from the original dataset\n    dataset = dataset.drop_duplicates(subset=[\"molecule_chembl_id\"], keep=False)\n\n    # Add new rows to the original dataset\n    dataset = pd.concat([dataset, df_new_rows], axis=0).reset_index(drop=True)\n\n    return dataset","7303cd8c":"# Replace duplicated molecules by mean of their standard_value and pchembl_value\ndf2_non_duplic = combine_molecule_duplicates(df2)","e8071100":"# Cast pchembl and standard values from object to float \ncast_types = {'standard_value': np.float, 'pchembl_value': np.float}\n\ndf2_non_duplic = df2_non_duplic.astype(cast_types)","efbd136e":"# General measurements of the dataset \ndf2_non_duplic.info()","9d13a929":"# Verify that duplicates were deleted \nprint('Number of unique ChEMBL ID:', str(len(df2_non_duplic.molecule_chembl_id.unique()) )  )\nprint('Total number of ChEMBL ID: ', str(len(df2_non_duplic)) )","72da9531":"# Distribution of pchembl_values\ndf2_non_duplic.pchembl_value.hist(bins=40, figsize=(8,4), color='#00BFC4', ec='black')\n\nplt.title('Histogram of pChEMBL values', fontsize=14, fontweight='black', pad=15)\nplt.xticks(fontsize=12)\nplt.yticks(fontsize=12)\nplt.show()","bc840a8b":"# Convert pchembl values to binary ones\n# 0 means that molecules are inactive and 1 that they are active \ndf2_non_duplic[\"pchembl_bin\"] = (df2_non_duplic[\"pchembl_value\"] >= 5).astype(int)","3a4cd3fc":"# Bar plot of active and inactive molecules considering pCHEMBL values\n\n# Data\nactive = len(df2_non_duplic[df2_non_duplic.pchembl_bin == 1])\ninactive = len(df2_non_duplic[df2_non_duplic.pchembl_bin == 0])\n\nx = ['Active', 'Inactive']\ny = [active, inactive]\n\n# Setup plot\nfig, ax = plt.subplots()\n\n# Make bar plot\np = ax.bar(x, y, color = ['#F8766D', '#00BFC4'], ec = 'black')\n\nax.set_title('pChEMBL activity', fontsize=14, fontweight='bold', pad=15)\n#ax.set_xticklabels(x, fontweight='bold')\n\nax.set_ylim(0,70000)\nplt.xticks(fontsize=14)\nplt.yticks(fontsize=14)\n\n# Label above bar\nfor index, data in enumerate(y):\n    plt.text(x=index-0.1 , y =data+1000 , s=f\"{data}\" , fontdict=dict(fontsize=14))\n\nfig.set_size_inches(5,4.5)\nplt.show()","09b66256":"# Statistical analysis | Mann-Whitney U Test\ndef mannwhitney(descriptor, dataset, verbose=False):\n  # https:\/\/machinelearningmastery.com\/nonparametric-statistical-significance-tests-in-python\/\n\n# seed the random number generator\n  np.random.seed(1)\n\n# actives and inactives\n  selection = [descriptor, 'pchembl_bin']\n  df = dataset[selection]\n  active = df[df.pchembl_bin == 1]\n  active = active[descriptor]\n\n  inactive = df[df.pchembl_bin == 0]\n  inactive = inactive[descriptor]\n\n# compare samples\n  stat, p = mannwhitneyu(active, inactive)\n  #print('Statistics=%.3f, p=%.3f' % (stat, p))\n\n# interpret\n  alpha = 0.05\n  if p > alpha:\n    interpretation = 'Same distribution (fail to reject H0)'\n  else:\n    interpretation = 'Different distribution (reject H0)'\n  \n  results = pd.DataFrame({'Descriptor':descriptor,\n                          'Statistics':stat,\n                          'p':p,\n                          'alpha':alpha,\n                          'Interpretation':interpretation}, index=[0])\n\n  return results","4ce9daac":"mannwhitney('pchembl_value', df2_non_duplic)","9f62fcc6":"# Get rid of molecules without canonical simles \ndf3 = df2_non_duplic[df2_non_duplic.canonical_smiles.notnull()]","b613dd49":"# General measurements of the dataset \ndf3.info()","8933508e":"# Distribution of target proteins  \ndf3.target_pref_name.value_counts()[0:50].plot.bar(figsize=(24,4), color='#00BFC4', ec='black')\n\nplt.title('Top 50 Targets', fontsize=14, fontweight='black', pad=15)\nplt.show()","71d6a749":"# Filter only molecules tested on Beta-lactamase AmpC\ndf4 = df3[df3.target_pref_name == \"Beta-lactamase AmpC\"].reset_index(drop=True)\ndf4 = df4.sort_values(\"molecule_chembl_id\").reset_index(drop=True)","2ccf8cbb":"# Delete CHEMBL412073 moecule because it is not possible to calculate fingerprints of it \ndf4 = df4.drop(labels = [61151]).reset_index(drop=True)","09b49566":"df4.info()","1e8f6d59":"# Bar plot of active and inactive molecules considering pCHEMBL values of Beta-lactamase AmpC target\n\n# Data\nactive = len(df4[df4.pchembl_bin == 1])\ninactive = len(df4[df4.pchembl_bin == 0])\n\nx = ['Active', 'Inactive']\ny = [active, inactive]\n\n# Setup plot\nfig, ax = plt.subplots()\n\n# Make bar plot\np = ax.bar(x, y, color = ['#F8766D', '#00BFC4'], ec = 'black')\n\nax.set_title('pChEMBL activity', fontsize=14, fontweight='bold', pad=15)\n#ax.set_xticklabels(x, fontweight='bold')\n\nax.set_ylim(0,70000)\nplt.xticks(fontsize=14)\nplt.yticks(fontsize=14)\n\n# Label above bar\nfor index, data in enumerate(y):\n    plt.text(x=index-0.1 , y =data+1000 , s=f\"{data}\" , fontdict=dict(fontsize=14))\n\nfig.set_size_inches(5,4.5)\nplt.show()","fdc6932f":"# Top 10 activity types \ndf4.standard_type.value_counts()[:10].plot.bar(figsize=(8,4), color='#00BFC4', ec='black')\n\nplt.title('Top 10 activity types ', fontsize=14, fontweight='black', pad=15)\nplt.show()","f6e6921f":"# Histogram of BioAssay Ontology\nbao_labels = df4.bao_label.value_counts()\nbao_labels.plot.bar(figsize=(8,4), color='#00BFC4', ec='black')\n\nplt.title('Histogram of BioAssay Ontology', fontsize=14, fontweight='black', pad=15)\nplt.xticks(fontsize=12)\nplt.yticks(fontsize=12)\nplt.show()","5242bc50":"# Download and unzip fingerprint XML files\n! mkdir Data\/MolFingerprints\n! wget https:\/\/github.com\/dataprofessor\/padel\/raw\/main\/fingerprints_xml.zip --directory .\/Data\/MolFingerprints\n! unzip Data\/MolFingerprints\/fingerprints_xml.zip -d Data\/MolFingerprints","7e1f4905":"# List and sort fingerprint XML files\nxml_files = glob.glob(\"Data\/MolFingerprints\/*.xml\")\nxml_files.sort()","28f6d32a":"# Delete the most computational demanding molecular fingerprints\nxml_files.remove(\"Data\/MolFingerprints\/KlekotaRothFingerprintCount.xml\")\nxml_files.remove(\"Data\/MolFingerprints\/KlekotaRothFingerprinter.xml\")\nxml_files","38216a87":"# Create a list with shorten names of xml files\nFP_list = ['AtomPairs2DCount',\n 'AtomPairs2D',\n 'EState',\n 'CDKextended',\n 'CDK',\n 'CDKgraphonly',\n 'MACCS',\n 'PubChem',\n 'SubstructureCount',\n 'Substructure']","3cfa97fa":"# Create a dictionary with shorten names as keys and xml file names as values\nfp = dict(zip(FP_list, xml_files))\nfp","563d9580":"# Prepare data subset as input to PaDELpy and export it as .smi file\ndf_padelpy = pd.concat( [df4['canonical_smiles'], df4['molecule_chembl_id']], axis=1 )\ndf_padelpy.to_csv('Data\/molecule.smi', sep='\\t', index=False, header=False)\ndf_padelpy","c36075aa":"for i in fp:\n    path = \"Data\/MolFingerprints\/Results\/Original\"\n    fingerprint = i\n    fingerprint_output_file_path = ''.join([path,fingerprint]) \n    fingerprint_output_file = ''.join([fingerprint_output_file_path,'.csv']) \n    fingerprint_descriptortypes = fp[fingerprint]\n\n    padeldescriptor(mol_dir='molecule.smi', \n                    d_file=fingerprint_output_file, \n                    descriptortypes= fingerprint_descriptortypes,\n                    detectaromaticity=True,\n                    standardizenitro=True,\n                    standardizetautomers=True,\n                    threads=2,\n                    removesalt=True,\n                    log=True,\n                    fingerprints=True)","03019d03":"# Create a list with shorten names of xml files\nFP_list = ['AtomPairs2DCount',\n 'AtomPairs2D',\n 'EState',\n 'CDKextended',\n 'CDK',\n 'CDKgraphonly',\n 'MACCS',\n 'PubChem',\n 'SubstructureCount',\n 'Substructure']","d63b7fff":"# Download and unzip calculated molecular descriptor files\n! mkdir Data\/MolFingerprints\/Results\n! wget https:\/\/github.com\/sayalaruano\/MidtermProject-MLZoomCamp\/raw\/main\/Data\/MolFingerprints\/Results_10molecular_descript_betalact_Padel.zip --directory Data\/MolFingerprints\/Results\n! unzip Data\/MolFingerprints\/Results\/Results_10molecular_descript_betalact_Padel.zip -d Data\/MolFingerprints\/Results","5f9c71b2":"# Load original molecular descriptor files and storage them in a dictionary\ndescriptors = {}\n\nfor i in FP_list:\n    path = \"Data\/MolFingerprints\/Results\/\"\n    fingerprint_file_path = ''.join([path,i])\n    \n    descriptor_name_csv = ''.join([fingerprint_file_path,'.csv'])\n\n    print(descriptor_name_csv)\n\n    descriptors[str(i)] = pd.read_csv(descriptor_name_csv)\n\n    descriptors[str(i)] = descriptors[str(i)].sort_values(\"Name\").reset_index(drop=True)","442d8460":"# Function to remove low variance features\ndef remove_low_variance(input_data, threshold=0.1):\n    selection = VarianceThreshold(threshold)\n    selection.fit(input_data)\n    return input_data[input_data.columns[selection.get_support(indices=True)]]","3e98502e":"# Create a dictionary of dataframe with low variance descriptors\nlow_var_descriptors = {}\n\nfor i,j in descriptors.items():\n  temp = j.drop('Name', axis=1)\n  low_var_descriptors[i] = remove_low_variance(temp, threshold=(.8*(1-.8)))","83c7fc6a":"# Dataframe to save performance metrics of the best models\nresults = pd.DataFrame(columns=['Dataset', 'Model', 'ROC_AUC_cv', 'ROC_AUC_test',\n                                'Accuracy_cv', 'Accuracy_test', 'Precision_cv', \n                                'Precison_test', 'Recall_cv', 'Recall_test', \n                                'F1score_cv', 'F1score_test'])","54018c38":"# Series for the target variable \ny = df4.pchembl_bin","d0708c98":"# A different model for feature matrices of each fingerprint \nfor i, j in low_var_descriptors.items():\n\n    # Data splitting \n    X_train, X_test, y_train, y_test = train_test_split(j, y, test_size=0.2, \n                                                        random_state=42)\n\n    # Set the grid of parameters of GridSearchCV\n    tuned_parameters_log_reg = [\n        {\"penalty\": [\"l1\"], \"solver\": [\"saga\"], \"C\": [0.01, 0.1, 1, 10, 100], \n        \"max_iter\": [1000], \"n_jobs\":[-1], \"random_state\": [10]}\n    ]\n\n    print(f'# Tuning hyper-parameters Log reg {i}')\n    print()\n\n    # Scoring metrics to be calculated \n    scoring = {\"AUC\": \"roc_auc\", \"Accuracy\": \"accuracy\", \n               \"Precision\": \"precision\", \"Recall\": \"recall\", \n               \"F1score\": \"f1\"}\n    \n    # Create GridSearchCV object with its parameters \n    logreg = GridSearchCV(LogisticRegression(), tuned_parameters_log_reg, \n                          scoring=scoring, cv=3, refit=\"AUC\", \n                          return_train_score=True)\n    # Train models \n    logreg.fit(X_train, y_train)\n\n    print(\"Best parameters set found on development set:\")\n    print()\n    print(logreg.best_params_)\n    print(logreg.best_score_)\n\n    # Obtain performance metrics of the best model  \n    roc_auc_cv = max(logreg.cv_results_[\"mean_test_AUC\"])\n\n    acc_cv = max(logreg.cv_results_[\"mean_test_Accuracy\"])\n\n    prec_cv = max(logreg.cv_results_[\"mean_test_Precision\"])\n\n    recall_cv = max(logreg.cv_results_[\"mean_test_Recall\"])\n\n    f1score_cv = max(logreg.cv_results_[\"mean_test_F1score\"])\n\n    # Add information to the results dataframe\n    results = results.append({'Dataset': i, 'Model': 'LogReg', 'ROC_AUC_cv': roc_auc_cv,\n                               'Accuracy_cv': acc_cv, 'Precision_cv':prec_cv, \n                               'Recall_cv': recall_cv, 'F1score_cv': f1score_cv}, \n                                ignore_index=True)","36e8b98b":"# A different model for feature matrices of each fingerprint \nfor i, j in low_var_descriptors.items():\n\n    # Data splitting \n    X_train, X_test, y_train, y_test = train_test_split(j, y, test_size=0.2, random_state=42)\n\n    # Set the grid of parameters of GridSearchCV\n    tuned_parameters_rf = [\n        {\"max_depth\": [10, 20, 30, 50], \"n_estimators\": [10, 30, 50, 100, 200], \n          \"n_jobs\":[-1], \"random_state\": [10]}\n    ]\n\n    print(f'# Tuning hyper-parameters Random Forest {i}')\n    print()\n\n    # Scoring metrics to be calculated\n    scoring = {\"AUC\": \"roc_auc\", \"Accuracy\": \"accuracy\", \n               \"Precision\": \"precision\", \"Recall\": \"recall\", \n               \"F1score\": \"f1\"}\n\n    # Create GridSearchCV object with its parameters \n    rf = GridSearchCV(RandomForestClassifier(), tuned_parameters_rf, scoring=scoring, \n                      cv=3, refit=\"AUC\", return_train_score=True)\n    \n    # Train models \n    rf.fit(X_train, y_train)\n\n    print(\"Best parameters set found on development set:\")\n    print()\n    print(rf.best_params_)\n    print(rf.best_score_)\n\n    # Obtain performance metrics of the best model  \n    roc_auc_cv = max(rf.cv_results_[\"mean_test_AUC\"])\n\n    acc_cv = max(rf.cv_results_[\"mean_test_Accuracy\"])\n\n    prec_cv = max(rf.cv_results_[\"mean_test_Precision\"])\n\n    recall_cv = max(rf.cv_results_[\"mean_test_Recall\"])\n\n    f1score_cv = max(rf.cv_results_[\"mean_test_F1score\"])\n\n    # Add information to the results dataframe\n    results = results.append({'Dataset': i, 'Model': 'RandomForest', 'ROC_AUC_cv': roc_auc_cv,\n                               'Accuracy_cv': acc_cv, 'Precision_cv':prec_cv, \n                               'Recall_cv': recall_cv, 'F1score_cv': f1score_cv}, \n                                ignore_index=True)","8cbc049d":"# A different model for feature matrices of each fingerprint \nfor i, j in low_var_descriptors.items():\n\n    # Data splitting \n    X_train, X_test, y_train, y_test = train_test_split(j, y, test_size=0.2, random_state=42)\n\n    # Set the grid of parameters of GridSearchCV\n    tuned_parameters_xvg = [\n        {'eta': [0.1, 0.3, 0.5], \n        'max_depth': [5, 10, 20],\n        'min_child_weight': [1, 3, 5],\n        'objective': ['binary:logistic'],\n         'seed': [10],\n         'verbosity': [1]}\n    ]\n\n    print(f'# Tuning hyper-parameters XGBoost {i}')\n    print()\n\n    # Scoring metrics to be calculated \n    scoring = {\"AUC\": \"roc_auc\", \"Accuracy\": \"accuracy\", \n               \"Precision\": \"precision\", \"Recall\": \"recall\", \n               \"F1score\": \"f1\"}\n    \n    # Create GridSearchCV object with its parameters \n    xvg = GridSearchCV(xgb.XGBClassifier(), tuned_parameters_xvg, scoring=scoring, \n                      cv=3, refit=\"AUC\", return_train_score=True, n_jobs=-1)\n    \n    # Train models \n    xvg.fit(X_train, y_train)\n\n    print(\"Best parameters set found on development set:\")\n    print()\n    print(xvg.best_params_)\n    print(xvg.best_score_)\n    \n    # Obtain performance metrics of the best model \n    roc_auc_cv = max(xvg.cv_results_[\"mean_test_AUC\"])\n\n    acc_cv = max(xvg.cv_results_[\"mean_test_Accuracy\"])\n\n    prec_cv = max(xvg.cv_results_[\"mean_test_Precision\"])\n\n    recall_cv = max(xvg.cv_results_[\"mean_test_Recall\"])\n\n    f1score_cv = max(xvg.cv_results_[\"mean_test_F1score\"])\n\n    # Add information to the results dataframe\n    results = results.append({'Dataset': i, 'Model': 'XGBoost', 'ROC_AUC_cv': roc_auc_cv,\n                               'Accuracy_cv': acc_cv, 'Precision_cv':prec_cv, \n                               'Recall_cv': recall_cv, 'F1score_cv': f1score_cv }, \n                                ignore_index=True)\n\n# Export results \nresults.to_csv(\"results_ligreg_rf_svc_xgboost.csv\")","6b76d91d":"# Download results\n! mkdir Output\n! wget https:\/\/github.com\/sayalaruano\/MidtermProject-MLZoomCamp\/raw\/main\/Output\/results_ligreg_rf_svc_xgboost.csv --directory Output","0df7f497":"# Load results \nresults = pd.read_csv(\"Output\/results_ligreg_rf_svc_xgboost.csv\")\nresults.round(3).sort_values(\"ROC_AUC_cv\", ascending=False)","90f25799":"# Dictionary to save the 20 most correlated features with target variable for each \n# fingerprint\ncorr_feat_target = {}\nfor i,j in low_var_descriptors.items():\n\n    corr_feat = pd.DataFrame(j.corrwith(df4.pchembl_bin), \n                             columns = [\"Corr\"]).sort_values(by=['Corr'],\n                              ascending=False)\n\n    temp = j[corr_feat.head(20).index]\n    \n    corr_feat_target[i] = temp","dce0b70e":"# Dataframe to save performance metrics of the best models\nresults_corr_feat = pd.DataFrame(columns=['Dataset', 'Model', 'ROC_AUC_cv', \n                                'Accuracy_cv', 'Precision_cv', 'Recall_cv', \n                                'Recall_test', 'F1score_cv'])","63336391":"# A different model for feature matrices of each fingerprint \nfor i, j in corr_feat_target.items():\n\n    # Data splitting \n    X_train, X_test, y_train, y_test = train_test_split(j, y, test_size=0.2, \n                                                        random_state=42)\n\n    # Set the grid of parameters of GridSearchCV\n    tuned_parameters_log_reg = [\n        {\"penalty\": [\"l1\"], \"solver\": [\"saga\"], \"C\": [0.01, 0.1, 1, 10, 100], \n        \"max_iter\": [1000], \"n_jobs\":[-1], \"random_state\": [10]}\n    ]\n\n    print(f'# Tuning hyper-parameters Log reg {i}')\n    print()\n\n    # Scoring metrics to be calculated \n    scoring = {\"AUC\": \"roc_auc\", \"Accuracy\": \"accuracy\", \n               \"Precision\": \"precision\", \"Recall\": \"recall\", \n               \"F1score\": \"f1\"}\n    \n    # Create GridSearchCV object with its parameters \n    logreg = GridSearchCV(LogisticRegression(), tuned_parameters_log_reg, \n                          scoring=scoring, cv=3, refit=\"AUC\", \n                          return_train_score=True)\n    # Train models \n    logreg.fit(X_train, y_train)\n\n    print(\"Best parameters set found on development set:\")\n    print()\n    print(logreg.best_params_)\n    print(logreg.best_score_)\n\n    # Obtain performance metrics of the best model  \n    roc_auc_cv = max(logreg.cv_results_[\"mean_test_AUC\"])\n\n    acc_cv = max(logreg.cv_results_[\"mean_test_Accuracy\"])\n\n    prec_cv = max(logreg.cv_results_[\"mean_test_Precision\"])\n\n    recall_cv = max(logreg.cv_results_[\"mean_test_Recall\"])\n\n    f1score_cv = max(logreg.cv_results_[\"mean_test_F1score\"])\n\n    # Add information to the results dataframe\n    results_corr_feat = results_corr_feat.append({'Dataset': i, 'Model': 'LogReg', 'ROC_AUC_cv': roc_auc_cv,\n                               'Accuracy_cv': acc_cv, 'Precision_cv':prec_cv, \n                               'Recall_cv': recall_cv, 'F1score_cv': f1score_cv}, \n                                ignore_index=True)","2d666af6":"# A different model for feature matrices of each fingerprint \nfor i, j in corr_feat_target.items():\n\n    # Data splitting \n    X_train, X_test, y_train, y_test = train_test_split(j, y, test_size=0.2, random_state=42)\n\n    # Set the grid of parameters of GridSearchCV\n    tuned_parameters_rf = [\n        {\"max_depth\": [10, 20, 30, 50], \"n_estimators\": [10, 30, 50, 100, 200], \n          \"n_jobs\":[-1], \"random_state\": [10]}\n    ]\n\n    print(f'# Tuning hyper-parameters Random Forest {i}')\n    print()\n\n    # Scoring metrics to be calculated\n    scoring = {\"AUC\": \"roc_auc\", \"Accuracy\": \"accuracy\", \n               \"Precision\": \"precision\", \"Recall\": \"recall\", \n               \"F1score\": \"f1\"}\n\n    # Create GridSearchCV object with its parameters \n    rf = GridSearchCV(RandomForestClassifier(), tuned_parameters_rf, scoring=scoring, \n                      cv=3, refit=\"AUC\", return_train_score=True)\n    \n    # Train models \n    rf.fit(X_train, y_train)\n\n    print(\"Best parameters set found on development set:\")\n    print()\n    print(rf.best_params_)\n    print(rf.best_score_)\n\n    # Obtain performance metrics of the best model  \n    roc_auc_cv = max(rf.cv_results_[\"mean_test_AUC\"])\n\n    acc_cv = max(rf.cv_results_[\"mean_test_Accuracy\"])\n\n    prec_cv = max(rf.cv_results_[\"mean_test_Precision\"])\n\n    recall_cv = max(rf.cv_results_[\"mean_test_Recall\"])\n\n    f1score_cv = max(rf.cv_results_[\"mean_test_F1score\"])\n\n    # Add information to the results dataframe\n    results_corr_feat = results_corr_feat.append({'Dataset': i, 'Model': 'RandomForest', \n                                                  'ROC_AUC_cv': roc_auc_cv, \n                                                  'Accuracy_cv': acc_cv, \n                                                  'Precision_cv':prec_cv, \n                                                  'Recall_cv': recall_cv, \n                                                  'F1score_cv': f1score_cv}, \n                                                 ignore_index=True)","993a010c":"# A different model for feature matrices of each fingerprint \nfor i, j in corr_feat_target.items():\n\n    # Data splitting \n    X_train, X_test, y_train, y_test = train_test_split(j, y, test_size=0.2, random_state=42)\n\n    # Set the grid of parameters of GridSearchCV\n    tuned_parameters_xvg = [\n        {'eta': [0.1, 0.3, 0.5], \n        'max_depth': [5, 10, 20],\n        'min_child_weight': [1, 3, 5],\n        'objective': ['binary:logistic'],\n         'seed': [10],\n         'verbosity': [1]}\n    ]\n\n    print(f'# Tuning hyper-parameters XGBoost {i}')\n    print()\n\n    # Scoring metrics to be calculated \n    scoring = {\"AUC\": \"roc_auc\", \"Accuracy\": \"accuracy\", \n               \"Precision\": \"precision\", \"Recall\": \"recall\", \n               \"F1score\": \"f1\"}\n    \n    # Create GridSearchCV object with its parameters \n    xvg = GridSearchCV(xgb.XGBClassifier(), tuned_parameters_xvg, scoring=scoring, \n                      cv=3, refit=\"AUC\", return_train_score=True, n_jobs=-1)\n    \n    # Train models \n    xvg.fit(X_train, y_train)\n\n    print(\"Best parameters set found on development set:\")\n    print()\n    print(xvg.best_params_)\n    print(xvg.best_score_)\n    \n    # Obtain performance metrics of the best model \n    roc_auc_cv = max(xvg.cv_results_[\"mean_test_AUC\"])\n\n    acc_cv = max(xvg.cv_results_[\"mean_test_Accuracy\"])\n\n    prec_cv = max(xvg.cv_results_[\"mean_test_Precision\"])\n\n    recall_cv = max(xvg.cv_results_[\"mean_test_Recall\"])\n\n    f1score_cv = max(xvg.cv_results_[\"mean_test_F1score\"])\n\n    # Add information to the results dataframe\n    results_corr_feat = results_corr_feat.append({'Dataset': i, 'Model': 'XGBoost', \n                                                  'ROC_AUC_cv': roc_auc_cv,\n                                                  'Accuracy_cv': acc_cv, \n                                                  'Precision_cv':prec_cv, \n                                                  'Recall_cv': recall_cv, \n                                                  'F1score_cv': f1score_cv }, \n                                                 ignore_index=True)\n\n# Export results \nresults_corr_feat.to_csv(\"results_corr_feat_ligreg_rf_svc_xgboost.csv\")","1b8bcea0":"# Download results\n! wget https:\/\/github.com\/sayalaruano\/MidtermProject-MLZoomCamp\/raw\/main\/Output\/results_corr_feat_ligreg_rf_svc_xgboost.csv --directory Output","92736cdb":"# Load results \nresults_corr_feat = pd.read_csv(\"Output\/results_corr_feat_ligreg_rf_svc_xgboost.csv\")\nresults_corr_feat.round(3).sort_values(\"ROC_AUC_cv\", ascending=False)","39bac639":"# Data splitting \nX_train, X_test, y_train, y_test = train_test_split(low_var_descriptors[\"CDK\"], y, test_size=0.2, random_state=42)\n\n# Impementation of the best model \nrf = RandomForestClassifier(max_depth=100, n_estimators=200, n_jobs=-1, \n                            random_state=10).fit(X_train, y_train)","19456b1b":"# Predicting values with RF model on train and test dataset \ny_train_pred = rf.predict(X_train)\ny_test_pred = rf.predict(X_test)","a582d0dd":"# Dataset to store perfromance metrics\nresults_finalmodel = pd.DataFrame(columns=['ROC_AUC_train', 'ROC_AUC_test',\n                                'Accuracy_train', 'Accuracy_test', 'Precision_train', \n                                'Precison_test', 'Recall_train', 'Recall_test', \n                                'F1score_train', 'F1score_test'])","0fce259f":"roc_auc_train = roc_auc_score(y_train, y_train_pred)\n    \nroc_auc_test = roc_auc_score(y_test, y_test_pred)\n\nacc_train = accuracy_score(y_train, y_train_pred)\n\nacc_test = accuracy_score(y_test, y_test_pred)\n\nprec_train = precision_score(y_train, y_train_pred)\n\nprec_test = precision_score(y_test, y_test_pred)\n\nrecall_train = recall_score(y_train, y_train_pred)\n\nrecall_test = recall_score(y_test, y_test_pred)\n\nf1score_train = f1_score(y_train, y_train_pred)\n\nf1score_test = f1_score(y_test, y_test_pred)\n\nresults_finalmodel = results_finalmodel.append({'ROC_AUC_train': roc_auc_train,\n                          'ROC_AUC_test': roc_auc_test, 'Accuracy_train': acc_train, \n                          'Accuracy_test': acc_test, 'Precision_train':prec_train, \n                          'Precison_test': prec_test, 'Recall_train': recall_train, \n                          'Recall_test': recall_test, 'F1score_train': f1score_train, \n                          'F1score_test': f1score_test}, ignore_index=True)\n","6a295b8e":"results_finalmodel.round(3).T","9abdeb8e":"### **Target variable: pCHEMBL value**","b4bcdb3f":"## **Imports and data obtention**","64e46823":"### **Evaluation of the best model on test dataset**\n\nAccording to the summary tables presented above, the best model was Random Forest with `max_depth` of 10 and `n_estimators` of 200 as parameters, and low variance `CDK` fingerprint as feature matrix. \n\nThus, we trained this model again and evaluated it on the test dataset. \n","03239593":"#### **Random Forest**","9b229ba4":"### **Calculation of molecular descriptors from SMILES with PaDEL**\nYo can run this section by yourself, but due to the size of the dataset it can take a while, so I recommend you to use output files provided [here](hhttps:\/\/github.com\/sayalaruano\/MidtermProject-MLZoomCamp\/raw\/main\/Data\/MolFingerprints\/Results_10molecular_descript_betalact_Padel.zip), and move to the next section to load this data. ","366212dc":"#### **Logistic Regression**","fbb4e2de":"### **Load molecular descriptors and remove low variance features**","9cce51b2":"#### **XGBoost**","b79d1258":"#### **Preparation of files to calculate molecular descriptors**\nTo make this step yo need to install Padelpy library with pip, as follows: \n\n```python\npip install padelpy\n\n```","314b9434":"## **Feature matrix preparation**","6a705bda":"#### **Target protein**","a09ec9b9":"It is important to notice that I sorted rows of the datframe by molecule IDs, which will allow us to associate each row with the corrsponding molecular dexriptors calculated with PaDEL. We need to do this because PaDEL outputs are disorderd with respect to the input, so we will sort the originla dataset and the molecular descriptors by molecule IDs. ","8ea59040":"We observed that there are not null values in any feature of the dataset","42d6cf5c":"# **Drug discovery project about ligands that bind to Beta-Lactamases**\n\n[Sebasti\u00e1n Ayala Ruano](https:\/\/sayalaruano.github.io\/)\n\nThis notebook is part of my midterm project for the [Machine Learning Zoomcamp](https:\/\/datatalks.club\/courses\/2021-winter-ml-zoomcamp.html). The dataset and challenge was proposed by [Data Professor](https:\/\/github.com\/dataprofessor) as an Open Bioinformatics Research Project. \n\nThis project aims to evaluate activity of molecules that have been experimentally tested to bind or not bind to [Beta-Lactamases](https:\/\/www.rcsb.org\/structure\/4eyl). Some of these proteins allow multi-drug resistant bacteria or superbugs to inactivate a wide range of penicillin-like antibiotics, which is known as antimicrobial resistance (AMR). \n\nThe feature matrix to train machine learning models is obtained by calculating molecular descriptors from the `canonical_smiles` of molecules. These molecular descriptors are also known as molecular fingerprints, and they are property profiles of molecules, represented as vectors with each vector element representing the existence or the frequency of a structural feature. The extraction of molecular fingerprints from SMILES was performed with [PaDEL](http:\/\/www.yapcwsoft.com\/dd\/padeldescriptor\/) software, following instructions from [this video](https:\/\/youtu.be\/rEmDyZHz5U8).\n\nPaDEL has 12 available fingerprints, but we calculated 10 of them because KlekotaRothFingerprintCount and KlekotaRothFingerprinter required a long computing time to be obtained.\n\nFor this project, I tested three machine learning models, including Logistic Regression, Random Forest, and XGBoost, for a binary classification task. \n\nMore details about my midterm project are available in its [GitHub repository](https:\/\/github.com\/sayalaruano\/MidtermProject-MLZoomCamp). \n\nThis [video](https:\/\/youtu.be\/_GtEgiWWyK4) has more information about the Open Bioinformatics Research Project and how to collaborate in it.\n\n**Note:** `mkdir`, `wget`, and `unzip` are Unix based OS commands, so if you are in Windows, maybe these terminal tools won't work. ","6b64d3f1":"### **Other features**","ff783ad9":"## **Exploratory Data Analysis**","1adf8d2e":"<a href=\"https:\/\/colab.research.google.com\/github\/sayalaruano\/MidtermProject-MLZoomCamp\/blob\/main\/EDA_beta_lactamase_drug_discovery_project_version3.ipynb\" target=\"_parent\"><img src=\"https:\/\/colab.research.google.com\/assets\/colab-badge.svg\" alt=\"Open In Colab\"\/><\/a>","7a0bdf69":"The last table shows that using only correlated features with the target variable does not improve performance of our models.","5f08ee9e":"#### **Models' evaluation**","96961194":"We observed that the dataset is not unbalanced regarding pchemb_bin values\n\nAlso, I will use a modified version of the function to calculate Mann-Whitney U Test provided by data professor in [this notebook](https:\/\/github.com\/dataprofessor\/code\/blob\/master\/python\/CDD_ML_Part_2_Exploratory_Data_Analysis.ipynb). This test will be used for calculating if there are statistical significance differences between active and inactive categories regarding pchembl value.","b7b7ce66":"#### **Calculate descriptors**\nWe calculated only 10 descriptors. KlekotaRoth and KlekotaRothCount fingerprints were excluded because they required a long computing time to be obtained. The output data is available [here](hhttps:\/\/github.com\/sayalaruano\/MidtermProject-MLZoomCamp\/raw\/main\/Data\/MolFingerprints\/Results_10molecular_descript_betalact_Padel.zip).","52284172":"Considering all performacne metrics it is evident that this model is overfitting because it is performing well on the training dataset, but it is not doing well on test dataset. ","eda6ad1a":"I will use a function to replace duplicated molecules by one single row with mean of standard_value and pchembl_value of duplicated rows. The replacement is applied only if standard deviation of pchembl_value is lower than 2.","71d656b1":"#### **Logisic regression**","20706926":"#### **XGboost**","fc10e2ec":"## **Installation of libraries**","e5fe114e":"The histogram shows that distribution of this variable has a normal shape, which is because the formula of this coefficient includes log10, so there is no need to apply a transformation in this feature.","f6cf4b22":"We observed that distributions of active and inactive compounds in terms of pchembl value are statistically different.","f7b3ee96":"### **Models with low variance descriptors that were the most correlated with target variable**\nI tested if using only the 20 most correlated features with target variable as feature matrix improve our models. ","82e5f62f":"#### **Random Forest**","b0ea9013":"#### **Canonical smiles**","9270e781":"## **Machine learning models and feature enginnering**\nI testet three classifiers: \n* Logistic Regression \n* Random Forest \n* Xboost \n\nI performed feature tuning with skleanr class [GridSearchCV](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.GridSearchCV.html#sklearn.model_selection.GridSearchCV). This class allows to specify a grid with features for tuning, and also the number of folds if we want to use kfold cross validation for this process. So, I obtained the best set of features for each model in this way.","7c99f3dc":"## **Conclusions and further work**\n\nIn this notebook, I performed exploratory data analysis, calculation of molecular descriptors to represent molecules, and use these feature matrices as input of three ML classifiers, including Logistic Regression, Random Forest, and XGBoost.\n\nThe best model was Random Forest with `max_depth` of 10 and `n_estimators` of 200 as parameters, and low variance `CDK` fingerprint as feature matrix. However, it is obvious that this model is overfitting, so we should try to solve this problem by regularizing models, discarding irrelevant features, or adding more data to the training dataset. \n\nSome ideas for further work:\n\n* Apply techniques to avoid overfitting.\n* Add more filters to reduce the number of molecules of the dataset, and leave the most informative ones.\n* Delete molecules with intermediate values of bioactivity. \n* Create models with molecules that bind to other proteins than the Beta-lactamase AmpC, which I studied here.\n* Calculate KlekotaRothFingerprintCount and KlekotaRothFingerprinter fingerprints and train ML models with them.\n* Calculate 3D fingerprints, and other types of molecular descriptors to characterize the molecules. \n* Use pIC50 as the target variable instead of pchembl_value. \n* Try other machine learning classifiers, including support vector machines, neural networks, among others.\n* Implement regression models to predict the target value of bioactivity, instead of the classification models.\n* Try multi-task classification model instead of binary one.","2912f7ac":"We observed that the balance proportion was maintained after filtering molecules that bind to Beta-lactamase AmpC","9fe84753":"#### **Activity type**","c087253e":"#### **Models' evaluation**","773194d0":"### **Models with low variance descriptors**","2d14bafb":"#### **Bioassay ontology**"}}