{"cell_type":{"dccb21bc":"code","5db52c2e":"code","7318ed8c":"code","2d4dd777":"code","9f982cfb":"code","974d8efd":"code","57fcc11b":"code","7e161f71":"code","1d3bd3a6":"code","d0e187b5":"code","a8495569":"code","bb654229":"code","ce111411":"code","2481ec41":"code","1d86e35f":"code","d1a26080":"code","20da1085":"code","2e3731a6":"markdown","17443023":"markdown","b46ef315":"markdown","4189c2ed":"markdown","b9780506":"markdown","26b87fe1":"markdown","011b905d":"markdown","129194a4":"markdown","880d08dc":"markdown","c3bf08e9":"markdown","003babb1":"markdown","18cf8339":"markdown","b1e33125":"markdown","3544b4fb":"markdown","b1165a6a":"markdown","3fae7ca6":"markdown","746fca99":"markdown","e7a4d64b":"markdown","80329321":"markdown","18212c23":"markdown","671595c0":"markdown"},"source":{"dccb21bc":"import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import linear_model\nfrom sklearn.metrics import confusion_matrix,accuracy_score\nfrom sklearn.naive_bayes import GaussianNB,MultinomialNB,BernoulliNB\nfrom sklearn.decomposition import PCA\nfrom sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix\n\n#import the data into a dataframe\nincome_data=pd.read_csv('..\/input\/income-classification\/income_evaluation.csv')","5db52c2e":"# The target variable has 2 values (>50K and <50K), let us see how they are distributed.\nsns.set(style=\"darkgrid\")\nsns.countplot(x=\" income\", data=income_data)\nprint ('Target Value counts are')\nprint('\\t')\nprint(income_data[' income'].value_counts())","7318ed8c":"sns.pairplot(income_data)","2d4dd777":"plt.matshow(income_data.corr())","9f982cfb":"# making a copy of master data to do the activity\nraw_data=income_data.copy()\n\n# let us understand what are the datatypes of each column\nprint(raw_data.info())","974d8efd":"cat_lst=[' workclass',' education',' marital-status', ' occupation',' relationship',' race',' sex',' native-country',' income']\n\nfor i in cat_lst:\n    print ('unique values of column',i)\n    print('\\t')\n    print (raw_data[i].unique())\n    print('***********************************************')\n    print('\\t')","57fcc11b":"raw_data[[' workclass',' occupation',' native-country']]=raw_data[[' workclass',' occupation',' native-country']].replace(' ?',np.NaN)\nraw_data = raw_data.dropna(how='any',axis=0)","7e161f71":"# separating dependent and Independent Variables (Input and Output)\ny=raw_data[' income']\nx=raw_data.drop(' income',1)\n\n#applying one hot encoding to the entire independent variables\nraw_ohe = OneHotEncoder(categories='auto')\nx= raw_ohe.fit_transform(x).toarray()\n\n#applying label encoding to target variable.\nle=LabelEncoder()\ny=le.fit_transform(y)","1d3bd3a6":"#split x and y into Train and Test set\nx_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.25,stratify=y)","d0e187b5":"sgdc_clf = linear_model.SGDClassifier(max_iter=10, tol=1e-3)\nsgdc_clf.fit(x_train, y_train)\n\nsgdc_clf_predict=sgdc_clf.predict(x_test)\n\n#validating the model\nprint ('SGDC Accuracy',confusion_matrix(y_test,sgdc_clf_predict))\nprint ('SGDC Confusion Matrix',accuracy_score(y_test,sgdc_clf_predict))","a8495569":"print('SGDC F1 Score',f1_score(y_test,sgdc_clf_predict, average=\"macro\"))\nprint('SGDC Precision Score',precision_score(y_test,sgdc_clf_predict, average=\"macro\"))\nprint('SGDC Recall',recall_score(y_test,sgdc_clf_predict, average=\"macro\")) ","bb654229":"NB_clf = MultinomialNB()\nNB_clf.fit(x_train, y_train)\n\nNB_clf_predict=NB_clf.predict(x_test)\n\n#validating the model\nprint ('NB Accuracy',confusion_matrix(y_test,NB_clf_predict))\nprint ('NB Confusion Matrix',accuracy_score(y_test,NB_clf_predict))\nprint('NB F1 Score',f1_score(y_test,NB_clf_predict, average=\"macro\"))\nprint('NB Precision Score',precision_score(y_test,NB_clf_predict, average=\"macro\"))\nprint('NB Recall',recall_score(y_test,NB_clf_predict, average=\"macro\")) ","ce111411":"impute_data=income_data.copy()\n\nimpute_data[[' workclass',' occupation',' native-country']]=impute_data[[' workclass',' occupation',' native-country']].replace(' ?',np.NaN)\nimpute_data[' workclass'] = impute_data[' workclass'].fillna(impute_data[' workclass'].mode()[0])\nimpute_data[' occupation'] = impute_data[' occupation'].fillna(impute_data[' occupation'].mode()[0])\nimpute_data[' native-country'] = impute_data[' native-country'].fillna(impute_data[' native-country'].mode()[0])\n\nprint(impute_data.info())","2481ec41":"# seems like there are no more missing data.\n# seperating dependent and Independent Variables (Input and OutPut)\nimpute_data_y=impute_data[' income']\nimpute_data_x=impute_data.drop(' income',1)\n\n#applying one hot encoding to the entire independent variables\nimpute_data_ohe = OneHotEncoder(categories='auto')\nimpute_data_x= impute_data_ohe.fit_transform(impute_data_x).toarray()\n\n#applying label encoding to target variable.\nimpute_data_le=LabelEncoder()\nimpute_data_y=impute_data_le.fit_transform(impute_data_y)","1d86e35f":"#split x and y into Train and Test set\nimpute_data_x_train,impute_data_x_test,impute_data_y_train,impute_data_y_test=train_test_split(impute_data_x,impute_data_y,test_size=0.25,stratify=impute_data_y)","d1a26080":"impute_data_sgdc_clf = linear_model.SGDClassifier(max_iter=10, tol=1e-3)\nimpute_data_sgdc_clf.fit(impute_data_x_train, impute_data_y_train)\n\nimpute_data_sgdc_clf_predict=impute_data_sgdc_clf.predict(impute_data_x_test)\n\n#validating the model\nprint ('Imputed data SGDC Accuracy',confusion_matrix(impute_data_y_test,impute_data_sgdc_clf_predict))\nprint ('Imputed data SGDC Confusion Matrix',accuracy_score(impute_data_y_test,impute_data_sgdc_clf_predict))\nprint('Imputed data SGDC F1 Score',f1_score(impute_data_y_test,impute_data_sgdc_clf_predict, average=\"macro\"))\nprint('Imputed data SGDC Precision Score',precision_score(impute_data_y_test,impute_data_sgdc_clf_predict, average=\"macro\"))\nprint('Imputed data SGDC Recall',recall_score(impute_data_y_test,impute_data_sgdc_clf_predict, average=\"macro\")) ","20da1085":"impute_data_NB_clf = MultinomialNB()\nimpute_data_NB_clf.fit(impute_data_x_train, impute_data_y_train)\n\nimpute_data_NB_clf_predict=impute_data_NB_clf.predict(impute_data_x_test)\n\n#validating the model\nprint ('imputed data MNB Accuracy',confusion_matrix(impute_data_y_test,impute_data_NB_clf_predict))\nprint ('imputed data MNB Confusion Matrix',accuracy_score(impute_data_y_test,impute_data_NB_clf_predict))\nprint('Imputed data MNB F1 Score',f1_score(impute_data_y_test,impute_data_NB_clf_predict, average=\"macro\"))\nprint('Imputed data MNB Precision Score',precision_score(impute_data_y_test,impute_data_NB_clf_predict, average=\"macro\"))\nprint('Imputed data MNB Recall',recall_score(impute_data_y_test,impute_data_NB_clf_predict, average=\"macro\")) ","2e3731a6":"# 2.1 Applying ML algorithms on Method 1 data","17443023":"* It is binary classification, targets being >50K and <50K\n* Data we gathered was distributed 1\/3 and 2\/3 respectively\n* Though Data seemed to be skewed, it is alright since it is always going to be 80:20 rule and I personally felt it is enough to build a decent ML algorithm. It is binary classification so we should be good with what we have\n* There were total 6 numerical independent variables, tried to remove if any of them have correlation among them. However, they seemed to be vary and I have considered all of them for model building.\n* I tried, Na\u00efve  Bayes and SGDC model. Both seem to perform equally with ~85% accuracy consistently.\n* However, when I considered Type II error and other validation parameters (Precision, Recall and F1 score),  SGDC performing better\n* I tried 2 ways to model. Method 1 being by removing all the missing values and method 2 being imputing all the missing values\n* I preferred Method 1 since there were no difference in the performance of the models. Method 2 can be overfitting since we are using MODE to impute the missing values which can be biased and introducing weightage to those values. Also, there were only few hundred rows that had missing values among 31K rows. Therefore I found it okay to remove the missing values\n* So, SGDC in Method 1 is preferred Model for me. \n\n","b46ef315":"# 1. Import data, analyze variables, find the correlation among them","4189c2ed":"# 2. Method 1 : find and remove missing values","b9780506":"There are 6 numerical variables, let us understand what are the relation between them.","26b87fe1":"Method 2 is to identify all the missing values and their variables. Replace the missing values with MODE of the specific variable. We have chosen MODE because all these variables are happens to be categorical.","011b905d":"SGDC MODEL","129194a4":"# 3. Method 2: Find missing values and impute them with Mode","880d08dc":"After analyzing the pair plot and correlation, there seems to be no correlation among them. They are variant of each other. So let us consider all 6 of them.","c3bf08e9":"Navie Bayes MODEL","003babb1":"seems like columns ' workclass',' occupation',' native-country' have some missing values ie, (' ?'). let\u2019s remove them","18cf8339":"# Content","b1e33125":"# 3.1 Applying ML algorithms on Method 2 data","3544b4fb":"Finding all the unique values of categorical values","b1165a6a":"Method 1 is to remove missing values. In this case the missing values are identified by \u201c?\u201d. Once values are removed, apply ML algorithm on the data and validate it","3fae7ca6":" Lets us see unique values of all the categorical columns before converting them into numbers","746fca99":"Navie Bayes MODEL","e7a4d64b":"There are in total 14 independent variables and 1 dependent variable, we can reduce number of independent variables if we can find any co-relation among them. Let us analyze that","80329321":"        \n1. Import data, analyze variables, find the correlation among them\n2. Method 1: find and remove missing values\n\n    2.1 Applying ML algorithms on Method 1 data\n3. Method 2: Find missing values and impute them with Mode\n\n    3.1 Applying ML algorithms on Method 2 data\n4. Choosing Better Method, Model & Summary\n","18212c23":"SGDC MODEL","671595c0":"# 4. Choosing Better Method, Model & Summary"}}