{"cell_type":{"bf0341ab":"code","90e110d3":"code","8a4b00d9":"code","ff014a8d":"code","17375a69":"code","f27228d1":"code","ba237a86":"code","c54fbc75":"code","79cb0685":"code","002b8306":"code","28944ef9":"code","e3acdb6c":"code","83b05605":"code","5f51d66d":"code","2fe904aa":"code","4721ba03":"code","7fad9b34":"code","92db1d80":"code","18f101e7":"code","0ed721fe":"code","d2bd056a":"code","873347eb":"code","176576d4":"code","b35114d8":"code","0acacb76":"code","03dd7202":"code","c124970d":"code","a5a871fa":"code","3fcfbeab":"code","a8211f0a":"code","3f29225e":"code","b228a098":"code","b583d798":"code","478270d0":"code","86ea420a":"code","56add52b":"code","b559140f":"code","f6d88cf8":"code","14dcb862":"code","72c64aa3":"code","67534fcc":"code","b9943333":"code","0edcbd5e":"code","575d472a":"code","12bd7fbf":"code","e7b6d1f9":"code","2041f7a6":"code","abb75ee6":"code","22e4ad4d":"code","b07fff60":"code","ccebaaf6":"code","35b58e61":"code","b7f14d56":"code","b4b2963f":"markdown","8cc92aaf":"markdown","9c00afc2":"markdown","e5a979b0":"markdown","1492074f":"markdown","a7f153e7":"markdown","1b9582eb":"markdown","29332b72":"markdown","aa9884d5":"markdown","746d875e":"markdown","295c8424":"markdown","53529ae8":"markdown","652c0b2b":"markdown","bccd4b4a":"markdown","c64e87e4":"markdown","82ce063c":"markdown","193b52bc":"markdown","c37f1cc9":"markdown","d846ebaf":"markdown","caefac15":"markdown","c44a2ebd":"markdown","c4c13397":"markdown","d265108f":"markdown","242df12c":"markdown","88c672b7":"markdown","86609ca5":"markdown","0df2a623":"markdown","2ac37949":"markdown"},"source":{"bf0341ab":"import numpy as np \nimport pandas as pd \n\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","90e110d3":"# set styling for visualization\nsns.set_style('darkgrid', {'axes.facecolor': \"#E5EAEF\"})\nsns.set_context('notebook', font_scale=1, rc={'lines.linewidth': 2})\nplt.rcParams[\"figure.figsize\"] = (13,8)\n\n# default setting for jupyter notebook\npd.set_option(\"max_columns\", 100)\npd.set_option(\"max_rows\", 1000)\npd.set_option(\"max_colwidth\", None)","8a4b00d9":"# load data - http:\/\/www2.informatik.uni-freiburg.de\/~cziegler\/BX\/ \ndef load_data(path):\n    return (pd.read_csv(path, sep=\";\", encoding='CP1252', escapechar='\\\\'))\n\npath = \"\/kaggle\/input\/bookcrossing-dataset\/Book reviews\/Book reviews\/\"\n\nbook_ratings = load_data(path + \"BX-Book-Ratings.csv\")\nbooks = load_data(path + \"BX_Books.csv\") # warning, has one row with \";\" inside text field\nusers = load_data(path + \"BX-Users.csv\")","ff014a8d":"book_ratings.info()","17375a69":"book_ratings.head(2)","f27228d1":"books.info()","ba237a86":"books.head(2)","c54fbc75":"users.info()","79cb0685":"users.head()","002b8306":"# load data - goodreads\ndef load_gd_data(path):\n    return (pd.read_csv(path, sep=\",\", error_bad_lines=False))\n\npath = \"\/kaggle\/input\/goodbooks-10k\/\"\n\nbooks_gd = load_gd_data(path + \"books.csv\")\nratings_gd = load_gd_data(path + \"ratings.csv\")\nto_read_gd = load_gd_data(path + \"to_read.csv\")\nbook_tags_gd = load_gd_data(path + \"book_tags.csv\")\ntags_gd = load_gd_data(path + \"tags.csv\")","28944ef9":"# there is the same number of books and editions\nprint(books_gd.book_id.nunique())\nprint(books_gd.work_id.nunique())","e3acdb6c":"books_gd.head(2)","83b05605":"tags_gd.tail(2)","5f51d66d":"print(\n    \"The length of the dataset is\", \n    book_ratings.shape[0],\n    \", after we drop the duplicates, the length is\",\n    len(book_ratings[['User-ID', 'ISBN']].drop_duplicates()), \".\"\n)\n\nif book_ratings.shape[0] == len(book_ratings[['User-ID', 'ISBN']].drop_duplicates()):\n  print(\"This means there are no duplicate ratings in the data.\")\nelse:\n  print(\"This means there are duplicate ratings in the data.\")","2fe904aa":"sns.countplot(x=\"Book-Rating\", data=book_ratings);","4721ba03":"sns.countplot(x=\"Book-Rating\", data=book_ratings.query(\"`Book-Rating` > 0\"));","7fad9b34":"print(\n    \"Average rating is\",\n    round(book_ratings.query(\"`Book-Rating` > 0\")['Book-Rating'].mean(),2)\n)","92db1d80":"books.shape","18f101e7":"books.nunique()","0ed721fe":"sns.histplot(x=\"Year-Of-Publication\", data=books, bins=1000)\nplt.xlim(1900,2020);","d2bd056a":"# exclude books with publication year >= 2021\nbooks = books.query(\"`Year-Of-Publication` <= 2021\")","873347eb":"users.shape","176576d4":"users.nunique()","b35114d8":"sns.histplot(users[\"Age\"]);","0acacb76":"# dataset with outliers\nage_outliers = users.query(\"Age > 100 or Age < 6\")\n\n# user ids with invalid age\nuser_outliers = age_outliers[\"User-ID\"].to_list()\n\nage_outliers.shape","03dd7202":"# is there a pattern, do they come from specific location?\nage_outliers[\"Location\"].value_counts().to_frame().head()","c124970d":"# extract country from location and check most common\ncountries = users['Location'].apply(lambda row: str(row).split(',')[-1])\ncountries.value_counts().to_frame().head(10)","a5a871fa":"print(\n    \"There are\", books_gd.shape[0], \"books,\", \n    tags_gd.shape[0], \"tags,\", \n    ratings_gd.shape[0], \"ratings, and\", \n    ratings_gd[\"user_id\"].nunique(), \"users in goodreads dataset.\"\n)","3fcfbeab":"print(\n    \"The length of the dataset with ratings is\", \n    ratings_gd.shape[0],\n    \", after we drop the duplicates, the length is\",\n    len(ratings_gd[['book_id', 'user_id']].drop_duplicates()), \".\"\n)\n\nif ratings_gd.shape[0] == len(ratings_gd[['book_id', 'user_id']].drop_duplicates()):\n    print(\"This means there are no duplicate ratings in the data.\")\nelse:\n    print(\"This means there are duplicate ratings in the data.\")","a8211f0a":"print(\n    \"There are\", \n    len(books_gd.query(\"original_publication_year > 2021\")),\n    \"books with publication year over 2021.\"\n)","3f29225e":"sns.countplot(x=\"rating\", data=ratings_gd);","b228a098":"print(\n    \"Average rating is\",\n    round(ratings_gd['rating'].mean(),2)\n)","b583d798":"# join books and ratings, keep only explicit ratings\ndf_prep_step_1 = pd.merge(books, book_ratings.query(\"`Book-Rating` > 0\"), on='ISBN', how='inner')\n\n# join users data\ndf_prep_step_2 = pd.merge(df_prep_step_1, users, on='User-ID', how='inner')","478270d0":"# dataset with isbn to try to teach the recommender system to exclude very similar books\ndf_prep = df_prep_step_2.drop(['Publisher', 'Image-URL-S', 'Image-URL-M', 'Image-URL-L'], axis=1)\ndf_isbn = df_prep.drop_duplicates()","86ea420a":"print(\n    \"When joining the data, we lost\", \n    book_ratings.shape[0] - df_prep_step_1.shape[0],\n    \"ratings because some users rated books that were not in the books table, and\",\n    df_prep_step_1.shape[0] - df_prep_step_2.shape[0],\n    \"ratings because some users were not present in the users table.\",\n)","56add52b":"# exclude age outliers\ndf_isbn = df_isbn[~df_isbn[\"User-ID\"].isin(user_outliers)]\n\n# add country and drop location\ndf_isbn['Country'] = df_isbn['Location'].apply(lambda row: str(row).split(',')[-1])\ndf_isbn = df_isbn.drop('Location', axis=1)\n\n# fill in missings\ndf_isbn[['Book-Author', 'Country']] = df_isbn[['Book-Author', 'Country']].fillna('Unknown')\n\n# fill in missing age using mean - NOT IDEAL, FOR NOW ONLY FOR EXPLORATORY PURPOSES\nusers_mean_age = users[~users[\"User-ID\"].isin(user_outliers)]['Age'].mean()\ndf_isbn['Age'] = df_isbn['Age'].fillna(users_mean_age)","b559140f":"print(\n    \"In addition,\", df_prep.drop_duplicates().shape[0] - df_isbn.shape[0], \"ratings were removed due to exclusion of age outliers.\"\n)","f6d88cf8":"# missing data\ndf_isbn.isnull().sum()","14dcb862":"# example of users rating the same book multiple times\ndf_isbn.groupby(['Book-Title', 'Book-Author', 'User-ID']).size().reset_index(name='Count').sort_values(by='Count', ascending=False).query(\"Count > 1\").head(10)","72c64aa3":"# example of a user giving different ratings to the same book \ndf_isbn.query(\"`User-ID` == 11676 and `Book-Rating` > 0 and `Book-Title` == 'Pet Sematary'\")","67534fcc":"# example of a user giving the same rating multiple times to the same book\ndf_isbn.query(\"`User-ID` == 189835 and `Book-Rating` > 0 and `Book-Author` == 'Stephen King' and (`Book-Title` == 'The Shining' or `Book-Title` == 'The Gunslinger (The Dark Tower, Book 1)')\")","b9943333":"print(ratings_gd.shape)\nprint(len(ratings_gd[['user_id', 'book_id']].drop_duplicates()))","0edcbd5e":"# drop duplicate ratings\nratings_gd = ratings_gd.sort_values('rating').drop_duplicates(subset=['user_id', 'book_id'], keep='last')","575d472a":"# average ratings\navg_rating = books_gd.average_rating.mean()\n\n# choose reasonable percentile - find the number of ratings that 90% of books have\np_90 = books_gd.ratings_count.quantile(0.9)\n\n# now we filter out books that have less than p_90 ratings = qualified books for weighting\nselected_books = books_gd.copy().loc[books_gd[\"ratings_count\"] >= p_90]","12bd7fbf":"# function to compute weighted rating of each book\ndef weighted_rating(x, m=p_90, C=avg_rating):\n    v = x['ratings_count']\n    R = x['average_rating']\n    # calculation based on the IMDB formula\n    return (v\/(v+m) * R) + (m\/(m+v) * C)","e7b6d1f9":"# add column with new score\nselected_books['score'] = selected_books.apply(weighted_rating, axis=1)\n\n# sort books based on score calculated above\nselected_books = selected_books.sort_values('score', ascending=False)\n\n# top 20 books\nselected_books[['title', 'authors', 'ratings_count', 'average_rating', 'score']].head(20)","2041f7a6":"# get average rating and number of votes per isbn\ndf_groupby = df_isbn.groupby('ISBN').agg(['mean', 'count'])['Book-Rating'].reset_index()\n\n# average ratings\navg_rating_isbn = df_groupby['mean'].mean()\n\n# choose reasonable percentile - find the number of ratings that 99% of books have (= there are less ratings)\np_99_isbn = df_groupby['count'].quantile(0.99)\n\n# now we filter out books that have less than p_99 ratings = qualified books for weighting\nselected_books_isbn = df_groupby.copy().loc[df_groupby[\"count\"] >= p_99_isbn]\n\n# function to compute weighted rating of each book\ndef weighted_rating_isbn(x, m=p_99_isbn, C=avg_rating_isbn):\n    v = x['count']\n    R = x['mean']\n    # calculation based on the IMDB formula\n    return (v\/(v+m) * R) + (m\/(m+v) * C)\n\n# add column with new score\nselected_books_isbn['score'] = selected_books_isbn.apply(weighted_rating_isbn, axis=1)\n\n# sort books based on score calculated above\nselected_books_isbn = selected_books_isbn.sort_values('score', ascending=False)\n\n# top 20 books\npd.merge(selected_books_isbn, books, on='ISBN')[['Book-Title', 'Book-Author','count', 'mean', 'score']].drop_duplicates(['Book-Title', 'Book-Author']).head(20)","abb75ee6":"# selected features\nfeatures = books_gd[['book_id','original_title','authors','original_publication_year','language_code']]","22e4ad4d":"from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.metrics.pairwise import linear_kernel, cosine_similarity\n\n# create a list of columns to keep\ncolumns = ['original_title', 'authors','original_publication_year','language_code']\n\n# function to combine the features\ndef combine_features(data):\n    return ''.join(str(data['original_title']) + ' ' + str(data['authors']) + ' ' + str(data['original_publication_year']) + ' ' + str(data['language_code']))\n\n# create a column to store combined feature\nfeatures['combined_features'] = features.apply(combine_features, axis=1)\n\n# convert the text from the new column to a matrix of word counts\ncm = CountVectorizer().fit_transform(features['combined_features'])\ntf = TfidfVectorizer().fit_transform(features['combined_features'])\n\n# get the cosine similarity matrix from the matrixes\ncs_cm = cosine_similarity(cm, cm)\ncs_tf = linear_kernel(tf, tf)","b07fff60":"features.head()","ccebaaf6":"# get unique identificator of a book based on title, drop duplicates\nindices = pd.Series(features.index, index=features['original_title']).drop_duplicates()\n\ndef get_content_based_recommendations(title, cosine_sim):\n    # get the index of the book that matches the title\n    idx = indices[title]\n\n    # get the similarity scores of all books with that book\n    sim_scores = list(enumerate(cosine_sim[idx]))\n\n    # sort the books based on the similarity scores\n    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n\n    # get the scores of the 10 most similar books\n    sim_scores = sim_scores[1:11]\n\n    # get the book indices\n    book_indices = [i[0] for i in sim_scores]\n\n    # return the top 10 most similar books\n    return features['original_title'].iloc[book_indices]","35b58e61":"# not downweighting\nget_content_based_recommendations(\"The Hunger Games\", cs_cm)","b7f14d56":"# downweighting popular words\nget_content_based_recommendations(\"The Hunger Games\", cs_tf)","b4b2963f":"## Next steps\n\nThere are many ways to go from here:\n> **We can use this approach with more detailed attributes of the books (if we find the data) - tags, descriptions, genres, protagonists and many more.**    \n\nHowever, we still would get books that are too similar (i.e. Hunger Games + Mockingjay + Catching Fire). If the user already read Hunger Games, he\/she is more likely to already know about the whole saga. We could set the score to show for example the 10 most similar books and exclude the first 3, but this is artificial threshold which is not ideal. We could probably include sorting by ratings, or change the recommender logic to be based on genre, or to find similar authors instead of books. I expect similar problems also when using clustering, so it would be good to verify the hypothesis.\n\nAlso, now this approach is limited to selected book only so the links between users' favourite books would have to be implemented - and this could lead only to the spiral of recommending similar books, introducing no novelty or diversity of topics and genres.\n\nBUT it is more ethical as it does not need any information **about** the user (such as location or age), only his\/her interests. To drive ethical aspect more, we can ask the user before recommending if he\/she is okay with using the ratings to help others find their favourite books (**transparency**).\n\nImplementation:\n* when user gives five-star (Gooreads) \/ 9 or 10-star (Book-Crossing) rating to a book, we would apply this logic to find books that are similar to it\n* in case of more books with five-star ratings from the user, we can merge the recommended books and sort them based on their similarity score OR their overall rating\n\n> **We can use this approach as a first layer in our recommender system to prefilter books and remove the books with high similarity score.**\n\nIt could help us remove books that are too similar (same saga, same author) already from the beginning, to bring more novelty and diversity in our recommender.","8cc92aaf":"# Recommendation system approach\n\nIdeally, we want to recommend books:   \n* that the users **have not read yet and will likely to read**\n* that are similar to what users already read **but that are also NOT TOO similar**\n* that are diverse, so the similarity may not need to be high in all cases - we want to bring the user novel ideas, novel topics, novel genres\n* that are generally higher quality, based on ratings or popularity, **but we also want to recommend books that are new releases**\n* also, if we do not know anything about the user, we want to have at least some list of books ranked by popularity\/ratings based on some attributes","9c00afc2":"**A lot of these variables can be found in [Goodreads dataset](https:\/\/github.com\/zygmuntz\/goodbooks-10k)**\n* `books.csv`: additional detailed data about the books \n* `ratings.csv`: ratings (user + book + rating), sorted by time, rating 1-5 (Book Crossing has ratings 1-10)\n* `to_read.csv`: books saved to read by the user (user + book), sorted by time\n* `book_tags.csv`: tags assigned to the books (book + tag + count)\n* `tags.csv`: dimensional table (tag id + name), manual input by users\n\n**Solves the problem with editions, because it has** `work_id` **which refers to the book in the abstract sense** (i.e. `book_id` in ratings \/ to_read datasets maps to `work_id`, not `goodreads_book_id`, datasets have aggregated data for different editions). However, there is also a lot of preprocessing that needs to be done with this dataset (i.e. tags). More information also [here](http:\/\/fastml.com\/goodbooks-10k-a-new-dataset-for-book-recommendations\/).\n","e5a979b0":"# Preparation of the dataset\n\n## Book-Crossing","1492074f":"## Goodreads","a7f153e7":"## Book Ratings","1b9582eb":"# UI solution (draft)","29332b72":"> \ud83d\udca1 Idea\n> \n> **We could create \"reading personas\"** - instead of lists, there can be interactive UI where the user either fills in short survey to find his\/her preferences, or where the user can explore groups of people, where they are from (SES, continent), what their interests are, or opinions, and what they like to read. This can be a fun way to explore new readings. Inspiration [here](https:\/\/www.their.tube).","aa9884d5":"In `Book-Rating` dataset, there are no duplicite ratings based on ISBN and user combination.\n\nThe ratings are either implicit (= 0), or explicit (1-10). The explicit ratings are negatively skewed (more values are plotted on the right side).","746d875e":"## Books","295c8424":"## Potential problems with the data and questions that remain unresolved\n1. **Best method to impute missing age**: there are several approaches - mean, median. \n2. **Users rating different versions of the book**\n\nWhat does it say if a user rated the same book 2 and 8? Was it a mistake, or the preferences change in time, or there was something in relation to the edition that the user did not like? Should we take the last rating, or average rating, or exclude it completely from the dataset?\n\nAlso, if a user rated the same book 10 and 10, does it mean he liked it more than other books with only one 10-star rating? Do we exclude one rating, or keep them both in the dataset?","53529ae8":"### Next steps\n\n1. Identify how to structure the default lists\n  * based on genres: which genres? there are not only genres in tags dataset, so we would need to predefine which we want to show\n  * tags are assigned by users, which means manual input, typos, differences between british english \/ american english (favo(u)rites), so we would need to cleanup duplicate categories\n  * it could be user friendly to see:\n    * **TOP N currently read books**,\n    * **TOP N favourite books**, \n    * **TOP N sci-fi (or other genre) highly rated books of all time**, \n    * **TOP N favourite authors**,\n    * **TOP N trending books in *user's location or age categories*** (sociodemographic data)\n    * ... but trying to keep it simple with easy navigation\n\n2. Improve lists to recommend only one book and not multiple editions (`parent_id`)\n3. Exclude boxsets and similar\n4. Combine with a survey to increase personalization aspect which is missing","652c0b2b":"# Book recommender\n\nProblem: Book recommendations - \u201eI like Lord of the Rings, what else should I read?\u201c\n<br><br>\nGoal:\n- take some data, try some approaches, produce some code, get some results\n- then come and show us your solution and have a chat around it - show how you think\nabout a specific problem, how you are able to explain what approach you used and why, think about the limitations of the approach and how things could be improved if there was more time, what you think of the results and if they make sense, etc.\n<br>\n\nPhilosophy:\n- the actual result and code are not that important \u2013 the journey there and potential\nfuture paths are more important\n- if you are able to follow-through with some ideas then great, if you just start\nsomething and have a clear idea on how to proceed that is also useful\n- the expectation is that you will spend an evening or two with the task (but there are\nno bounds to proactivity if you enjoy playing with the problem)\n<br>\n\nData:\n- available open dataset: http:\/\/www2.informatik.uni-freiburg.de\/~cziegler\/BX\/\n- alternatively feel free to grab any other relevant data set\n<br>\n\nTools:\n- use whatever you are comfortable in (R, Python, Matlab, Java, SQL,...) or feel free to\nuse it as an opportunity to try out a new language\n- it is not a contest in finding the best black-box library and blindly using it \u2013 own\nsolutions are preferred even if they are simple\n- how to present: up to you \u2013 slides, walking through code, drawing, ...\n\nAdditional data:\n* [Goodreads dataset](https:\/\/github.com\/zygmuntz\/goodbooks-10k)","bccd4b4a":"In `Users` dataset, there are no duplicate users. Most of the users come from English speaking countries, majority from the U.S.\n\n**However, there are approx. 1,200 users with age over 100 and under 6 (>1%). We should discard them as invalid, but maybe we will be discarding a pattern from the dataset as there will always be users that do not fill in their age at all or will fill it incorrectly - what do they have in common? Or we could bring bias to our recommender if we let them in the dataset. We will treat them as outliers and remove them from data now, but this is something to be explored.**","c64e87e4":"`Book-Crossing` datasets will be joined to have book title, author, year of publication, user, his\/her rating, country, and age. \n\nThe ratings are connected to the ISBNs, so if were to build the recommender system using this data, we need to think about whether to switch the ISBNs with book titles to avoid recommending the same book only with different edition (also we need to make sure we are not grouping the same book titles from different authors). It could be possible to make recommender prefilter the data to exclude too similar books.\n\nAlso, the recommender system will be based on explicit ratings, even though implicit ratings can be helpful as well.\n\n> \ud83d\udca1 Idea\n>\n> Image information can be useful if we were to analyze the implicit ratings - what on the cover drives the users' attention to check the book? What are other characteristics that the books with implicit ratings have in common?","82ce063c":"## Goodreads","193b52bc":"![Renata s v\u00e1mi sd\u00edl\u00ed skicu..png](attachment:b47cf592-5dcf-4d0e-b4a3-a62ac9695b49.png)","c37f1cc9":"In comparison to Goodreads, there are 6 times less ratings in Book-Crossing dataset and no tags, however there are significantly more books and users. It has only explicit ratings, on a scale 1-5. Also, the tags would need a lot of cleanup, because they were created by users, so there are a lot of duplicities (`favourites`, `favorites`, `f a v o r i t e s` etc.) which in this state is not useful. \n\nWe might take advantage of the additional data of the `books.csv` dataset and join them on `Book-Crossing` dataset using `ISBN`, but since there is only a fragment of books, it would not be beneficial to build a recommender system using such little data. \n\nHowever, since we will need to figure out how to tackle the cold start problem when we do not know anything about the user and want to recommend most popular books, this dataset may come in handy since it is based on 10k most popular books. \n\n| | Goodreads | Book-Crossing |\n| ------| -------- | ------- |\n| Ratings | 6 mil | 1.1 mil |\n| Tags | 34k | - |\n| Users | 53k | 278k |\n| Books | 10k | 242k |","d846ebaf":"# Conceptual problems\n\n## Response bias\n* from psychometric point of view, comparing ratings from different users may become problematic due to bias - some users tend to use more extreme categories, some tend to use more neutral ratings\n* the wider the scale (1-5 vs 1-10), the harder it may be to map similar users\n> solution: we may look into **adjusted similarity** (link [here](https:\/\/www.cs.carleton.edu\/cs_comps\/0607\/recommend\/recommender\/itembased.html) or [here](https:\/\/stackoverflow.com\/questions\/40716459\/choice-between-an-adjusted-cosine-similarity-vs-regular-cosine-similarity))\n\n## Preference development\n* recommender systems may alter our decision making or habit formation, they may also keep us in our social bubble\n* BUT some people are less prone to change, close-minded (need for cognitive closure, [link](https:\/\/pubmed.ncbi.nlm.nih.gov\/7815301\/)), someone doesn't need as much novelty\n> solution 1: transparency, introducing the user out-of-the-box thinking only if they want, i.e. other personas and their interests   \n> solution 2: introduce reading goals, nudge to motivate people to read more, introduce \"reading path\"\n\n## Anchoring effect (heuristics)\n* test-retest ratings not reliable, may be skewed if the rating was altered\n* post-hoc rating adjustment (reverse-engineering) difficult \n* why risky?\n    * users: it can manipulate their preferences or economic behavior\n    * retailers: third party agents can manipulate recommendation systems\n    * designers: distorted ratings contaminates the inputs and reduces effectiveness of the recommender\n   \n> solution: bias aware interface design, proactively preventing from occurring, experiment with various rating representation forms (UI solution)\n\n## High-quality books x new releases\n* the approaches mostly rely on ratings, therefore books with very little ratings are at disadvantage\n* also, sparsity is usually tackled by keeping only popular books, again disadvantaging new releases\n> solution: nudge users to help rate newly released books (UI solution)\n\n## Real-time x preprocessed recommender (scalability problem)\n* real-time too costly, preprocessed with potentially outdated recommendations\n> solution 1: precalculate the model offline, then load the data to some database as key-value so it can run near real-time, retrain periodically (divide calculation into multiple steps, precalculate only first layers)   \n> solution 2: remove books with too little data   \n\n## Interpretability\n* content-based more interpretable than collaborative filtering, but collaborative filtering introduces more novelty\n* explicit ratings more interpretable than implicit ratings, but implicit easier to collect\n* always good to have some reverse check to validate the recommendations, but if it becomes too interpretable, we risk external influences\n> solution: hybrid approach (preliminary filtering and then ranking or average predicted rating combined by both approaches)\n\n## Overfitting\n* if the recommender becomes too personalized, there are two problems:\n    * it brings no novelty for the user\n    * it can learn to introduce extreme topics if the user already inclines towards such topics (rabbit hole problem, problematic due to ethics)\n> solution 1: we can try to inject randomness into the model   \n> solution 2: removing books that are too similar, or limit books from the same author","caefac15":"## Users","c44a2ebd":"# Content-based filtering\n\nBased on user ratings of books he\/she read, we can look into the metadata of the favourite books (i.e. title, genre, author, description, keywords) and look for similar books using these attributes. Idea is that if a user likes some book, he\/she will like a book that is similar to it. \n\n> **Pros**: fast, easily interpretable (= transparent to users), no need for other users' ratings (it will work with low number of users), more reliable in the beginning of recommending algorithm \n\n> **Cons**: dependent on books metadata, with more features, we risk spiraling in recommending the same genres and topics, no diversity and novelty so not really personalized\n\n## How does it work?\n\n1. Select the features based on which we measure the similarity between books\n    * ideally, is there some research on what are the best predictors? -> **NEEDS TO BE EXPLORED**\n2. Combine all the words in one column\n3. Convert them to the matrix format, so the books are as rows and words are as columns (words are converted into vectors with semantic meaning)\n    * popularity question: decide whether it makes sense to downweight words that occur a lot or not (`TfidfVectorizer` vs. `CountVectorizer`)\n    * each word is assigned `term frequency` (TF, number of times it appears in the column) and `inverse document frequency` (IDF, how significant the word is in the whole column)\n4. Calculate the similarity between the words\/vectors\n    * there are different ways to calculate similarity: possible to experiment with Pearson, Euclidean, Jaccard, cosine\n    * `cosine similarity` is used in this notebook: similarity is calculated as the cosine of the angle between 2 vectors of the books A and B, the closer the vectors, the smaller the angle and larger the cosine, preferred when data is sparse","c4c13397":"> \ud83d\udca1 Idea\n>\n> **What variables can I potentially miss?**\n> - `description` of the book: recommending similar topics, could be solved by categorization to tags \/ keywords\n> - author details (these are mainly hypotheses for further exploration):\n>   - `gender`: men might not be interested in women topics and novels, but could be solved by categorization to tags \/ keywords\n>   - `age`: user might be more interested in books from authors with similar age due to same cultural\/historical background, problematic with deceased authors \n>   - `location`: user may be interested in authors coming from similar location, i.e. Czech user interested in Czech author, or Polish user in Polish author \n> - `languages`, in which the book is available: we do not want to recommend books that are in the language not known to the user, because he\/she would not be able to read it anyway (prefilter the language at the beginning)\n> - `keywords`: similarity of the books, content-based approach\n> - `genre`: recommending similar books based on genre, they have higher probability that the user will read them\n> - `link between the genres`: can help us with expanding the recommendation to other genres, that user might not have read yet but might be interested in (i.e. reader or historical novels can be interested in war literature even though he\/she did not read anything like that yet)\n> - `protagonist`: we might relate more to the communication style of main characters with the same gender\n> - `writing style`: diary form, poetry, ich form, changing point of views, not categorized in genre but different people might be comfortable with different storytelling\n> - `user behavior`: this may contain data about how the users interacts with the books, what books he\/she saves for later to read, or to buy, wish lists, favourite authors, clicks from the frontend\n> - `parent book id`: ISBN may not be appropriate to be used for recommendations, because unlike films and songs, **they can have many editions**","d265108f":"As already indicated, there are books with multiple editions in the dataset - there are ~270k ISBNs and ~242k book titles. \n\nBooks with publication year > 2021 were excluded from the data.","242df12c":"# Lists of TOP books\n\nGeneral way to start would be to prepare a list of books ranked by popularity\/ratings, which will be useful in case we have no data from the user. This will be a starting point - for a new user, we recommend the most popular items (i.e. in the user's area). \n\n> Pros: solves cold start problem\n>\n> Cons: not personalized\n\nWhat we need is a dataset including average rating of the book and the number of users that gave the review. The goodreads dataset may serve better in this usecase because it has already preaggregated average ratings from thousands of users, including tags which can be used to create a list of TOP N currently read books, TOP N favourite books, TOP N books based on genre etc. \n\nHowever, trending books based on location may also be useful to users, this can be done using Book-Crossing dataset only.\n\nThere are two main problems:\n\n* weighting, we need to find overall `N`, because average rating of 9 from 10 users would score better than average rating 8.5 from 10000 users, which won't be correct, typically related to new releases\n  * for example IMDB has weighted calculation, which can be replicated\n* `ISBN` refers to a specific edition of the book, we need some `parent_id`\n  * should be accessible in goodreads datasets as `work_id`, note that there is the same number of `book_id` (specific edition) and `work_id`s, so we do not need to worry about this now, but this needs to be implemented for the future","88c672b7":"# Data Cleaning and Exploration","86609ca5":"## Best rated books\n\n### Goodreads","0df2a623":"## Goodreads dataset\n\nGoodreads dataset is used to test this scenario because it has less data (for exploration, it is more convenient), and has more metadata about the books.\n\n> \ud83d\udca1 Idea\n>\n> Is it necessary to have all the metadata? What are the most significant predictors that determines whether a user will choose to read the book? Is there literature on it, maybe Book-Crossing dataset has the only variables we actually need and we do not need Goodreads at all? Maybe most of the variance is explained by very little predictors?","2ac37949":"### Book-Crossing"}}