{"cell_type":{"be72c427":"code","1d48d101":"code","bdb85e09":"code","1cbd3730":"code","f43905ed":"code","4bc87705":"code","cd4ff5c2":"code","1a5fc366":"code","419eb4cc":"code","4fc39180":"code","d94f7f7b":"code","b602dfd8":"code","8f369055":"code","9426d81c":"code","5029dbac":"code","7c183960":"markdown","cc02f5a7":"markdown","e840d2f9":"markdown","acb6a6d0":"markdown","4a3bdf76":"markdown","81b40c1b":"markdown","c71510da":"markdown","29896937":"markdown","3dbbc5bf":"markdown","ca442def":"markdown","0beed629":"markdown","ef05c2a7":"markdown","a30ea2f6":"markdown"},"source":{"be72c427":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","1d48d101":"print(os.listdir(\"..\/input\/cnews\/cnews\/Data\"))","bdb85e09":"data_path=\"..\/input\/cnews\/cnews\/Data\"\ncolumn_name=['label', 'news']\ndf_train=pd.read_csv(os.path.join(data_path, 'cnews.train.txt'), sep='\\t', header=None, names = column_name)\ndf_val=pd.read_csv(os.path.join(data_path, 'cnews.val.txt'), sep='\\t', header=None, names = column_name)\ndf_test=pd.read_csv(os.path.join(data_path, 'cnews.test.txt'), sep='\\t', header=None, names = column_name)","1cbd3730":"from sklearn import preprocessing\nonehot_encoder=preprocessing.OneHotEncoder()\nonehot_encoder=onehot_encoder.fit(df_test[['label']])\ny_train = onehot_encoder.transform(df_train[['label']])\ny_val = onehot_encoder.transform(df_val[['label']])\ny_test = onehot_encoder.transform(df_test[['label']])","f43905ed":"df_train['lengh'] = df_train.news.apply(len)\ndf_train.describe()","4bc87705":"embed_size = 400\nmax_features = 10000\nmaxlen = 1500\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\n\nall_news=df_train.news.append(df_val.news).append(df_test.news)\ntokenizer = Tokenizer(max_features, char_level=True)\ntokenizer.fit_on_texts(list(all_news.values))\n\ndef text_to_sequences(textlist):\n    sequences = tokenizer.texts_to_sequences(textlist)\n    return pad_sequences(sequences, maxlen = maxlen) \n\nX_train = text_to_sequences(list(df_train.news.values))\nX_val = text_to_sequences(list(df_val.news.values))\nX_test = text_to_sequences(list(df_test.news.values))","cd4ff5c2":"from keras.models import Sequential \nfrom keras.optimizers import RMSprop \nfrom keras.layers import Dense, Embedding, Conv1D, MaxPooling1D, GlobalMaxPooling1D, BatchNormalization\n\nmodel = Sequential() \nmodel.add(Embedding(max_features, embed_size, input_length = maxlen)) \nmodel.add(Conv1D(256, 8, activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling1D(3)) \nmodel.add(Conv1D(128, 4, activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(GlobalMaxPooling1D())\nmodel.add(Dense(10, activation='softmax'))\nmodel.compile(optimizer=RMSprop(lr=0.01), loss='categorical_crossentropy', metrics=['accuracy'])","1a5fc366":"from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\ncallback_list = [\n    EarlyStopping(monitor='acc',patience=10 ),\n    ModelCheckpoint(filepath='model.h5', monitor='val_loss', save_best_only=True),\n    ReduceLROnPlateau(monitor='val_loss', factor=0.1,patience=3)\n]","419eb4cc":"history = model.fit(X_train, y_train, epochs=500, \n                    batch_size=1024, callbacks = callback_list, \n                    validation_data = (X_val, y_val))","4fc39180":"history_dict = history.history\n\nloss_train = history_dict['loss']\nloss_val = history_dict['val_loss']\nacc_train = history_dict['acc']\nacc_val = history_dict['val_acc']\nepochs = range(1, len(loss_val) + 1)","d94f7f7b":"import matplotlib.pyplot as plt\n\nplt.plot(epochs, loss_train, 'bo', label='Training loss')\nplt.plot(epochs, loss_val, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()","b602dfd8":"plt.plot(epochs, acc_train, 'bo', label='Training acc')\nplt.plot(epochs, acc_val, 'b', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.show()","8f369055":"model.metrics_names","9426d81c":"model.evaluate(X_test, y_test)","5029dbac":"from keras.layers import Bidirectional, CuDNNGRU, Dense, GlobalMaxPooling1D\n\nmodel = Sequential()\nmodel.add(Embedding(max_features, embed_size, input_length = maxlen))\nmodel.add(Bidirectional(CuDNNGRU(256, return_sequences=True)))\nmodel.add(GlobalMaxPooling1D())\nmodel.add(Dense(64, activation='relu'))\nmodel.add(Dense(10, activation='softmax'))\nmodel.compile(optimizer=RMSprop(lr=0.001), loss='categorical_crossentropy', metrics = ['acc'])","7c183960":"\u6b63\u786e\u738795%+\uff0c \u6bd4github\u4e0a\u90a3\u4e2a96%+\u7684\u7ed3\u679c\u7a0d\u5dee\u4e00\u70b9\u3002","cc02f5a7":"#\u672a\u6765\u7684\u4f18\u5316","e840d2f9":"\u5c06\u6807\u8bb0\u8f6c\u6362\u4e3aont hot\u683c\u5f0f","acb6a6d0":"1. \u5904\u7406\u6587\u6863**\u57fa\u4e8eword level\u8bad\u7ec3**\uff0c\u800c\u4e0d\u662fchar level\u3002\u5e76**\u4f7f\u7528\u9884\u8bad\u7ec3\u7684embedding**\u3002\u56e0\u4e3a5\u4e07\u6761training data\u53ef\u80fd\u4e0d\u591f\u591a\u3002\n2. \u57fa\u4e8e\u6587\u6863\u7684\u5206\u7c7b\u95ee\u9898\uff0c\u56e0\u4e3a\u6bcf\u4e2a\u7c7b\u522b\u7684\u9ad8\u9891\u8bcd\u4e0d\u540c\uff0c**\u7528\u4f20\u7edf\u7684\u673a\u5668\u5b66\u4e60\u65b9\u6cd5**\uff0c\u5982SVM\uff0cGBDT\uff0c\u6216\u8005\u5c31\u7528\u6700\u7b80\u5355\u903b\u8f91\u56de\u5f52\uff0c \u6548\u679c\u672a\u5fc5\u4f1a\u6bd4\u795e\u7ecf\u7f51\u7edc\u5dee\uff0c\u800c\u4e14\u8bad\u7ec3\u901f\u5ea6\u66f4\u5feb\u3002 \u8fd9\u6837\u7684\u6a21\u578b\u4e5f\u53ef\u4ee5\u4f5c\u4e3abaseline, \u548c\u795e\u7ecf\u7f51\u7edc\u6bd4\u8f83\u4e00\u4e0b\u3002 \u4e0d\u8fc7\u8fd9\u6837\uff0c\u5c31\u9700\u8981\u81ea\u5df1\u63d0\u53d6\u7279\u5f81\uff0c\u6bd4\u5982\u7528tf-idf\u63d0\u53d6\u4e00\u4e0b\u5173\u952e\u8bcd\uff0c\u6216\u8005\u5176\u4ed6\u7279\u5f81\u3002\u53ea\u662f\u8fd9\u6837\u4f1a\u6bd4\u8f83\u82b1\u65f6\u95f4\uff0c\u5148\u4e0d\u505a\u4e86\u3002","4a3bdf76":"model.evaluate(X_test, y_test)","81b40c1b":"history = model.fit(X_train, y_train, \n                    epochs=20, batch_size=512, \n                    callbacks = callback_list,\n                    validation_data = (X_val, y_val))","c71510da":"#\u6784\u9020\u6a21\u578b\n\u9009\u7528CNN\uff0c\u56e0\u4e3a\u8bed\u8a00\u5e76\u4e0d\u662f\u5e8f\u5217\u5f3a\u76f8\u5173\u7684\uff0c\u5c04\u95e8\u51fa\u73b0\u5728\u6bb5\u9996\u8fd8\u662f\u6bb5\u5c3e\uff0c\u65b0\u95fb\u5927\u6982\u7387\u90fd\u662f\u4f53\u80b2\u3002\n\u7528CNN\u5c0f\u7a97\u53e3\u4e00\u6bb5\u4e00\u6bb5\u626b\u63cf\u6587\u5b57\uff0c\u53ef\u80fd\u66f4\u9002\u5408\u5f53\u524d\u95ee\u9898","29896937":"\u5b9e\u9645\u8bd5\u8fc7\uff0c\u901f\u5ea6\u6bd4CNN\u6162\u5f97\u591a\uff0c\u6548\u679c\u4e5f\u4e0d\u662f\u5f88\u597d","3dbbc5bf":"##\u5c06\u65b0\u95fb\u5185\u5bb9\u8f6c\u4e3a\u6570\u5b57\u5e8f\u5217","ca442def":"##\u9009\u62e9\u622a\u53d6\u957f\u5ea6\n\u53ef\u4ee5\u770b\u5230\uff0c\u65b0\u95fb\u6700\u77ed\u4e3a8\uff0c \u6700\u957f\u76842\u4e07\u591a\uff0c\u5e73\u5747\u957f\u5ea69\u767e\u591a\u5b57\uff0c\u4f46\u901a\u5e38\u6765\u8bf4\uff0c\u6211\u4eec\u53ea\u8bfb\u65b0\u95fb\u5f00\u5934\uff0c\u5c31\u80fd\u77e5\u9053\u6587\u7ae0\u7c7b\u578b\u3002\u6240\u4ee5\uff0c\u8fd9\u91cc\u622a\u53d6\u65b0\u95fb\u524d500\u767e\u5b57\u7528\u4e8e\u5224\u65ad\u3002\uff08\u539f\u6765\u662f300\uff0c\u4f46\u6539\u5927\u70b9\u6709\u63d0\u5347\uff09","0beed629":"\u8fd9\u662f\u6211\u5728quora\u63d0\u95ee\u7c7b\u578b\u5206\u7c7b\u770b\u5230\u7684\u4e00\u4e2a\u4f8b\u5b50\uff0c\u5177\u4f53\u539f\u7406\u8fd8\u6ca1\u5f00\u59cb\u7814\u7a76\u3002\u4f46\u770b\u4ecb\u7ecd\u89c9\u5f97\u5f88\u6709\u9053\u7406\n\u8bf4\u6211\u4eec\u7b80\u5355\u6253\u4f9d\u9760CNN\uff0c\u6216\u8005RNN,\u53ea\u662f\u57fa\u4e8e\u4e00\u4e2a\u65b9\u5411\uff0c\u770b\u5f53\u524d\u4e0e\u524d\u6587\u7684\u5173\u7cfb\uff0c\u4f46\u81ea\u7136\u8bed\u8a00\uff0c\u5f53\u524d\u5185\u5bb9\uff0c\u540c\u6837\u4e5f\u4e0e\u540e\u6587\u6709\u5173\u3002\n\u8fd9\u4e2a\u6a21\u578b\u662f\u53cc\u5411\u7684\uff0c\u80fd\u540c\u65f6\u5b66\u5230\u524d\u6587\u4e0e\u540e\u6587\u7684\u5173\u7cfb\u3002","ef05c2a7":"\u65b9\u6848\u4e00\uff1a  \u6211\u5bfc\u5165\u4e86\u641c\u72d0\u65b0\u95fb\u7684Embedding vector\u3002\u57fa\u4e8e\u66f4\u5927\u6570\u636e\u91cf\u7684Embedding\u6a21\u578b\uff0c\u5e94\u8be5\u4f1a\u6709\u66f4\u597d\u7684\u8bad\u7ec3\u6548\u679c\u3002\u4f46\u6b64\u5904\uff0c\u6211\u9700\u8981\u5bf9\u539f\u59cb\u6587\u4ef6\u5206\u8bcd\uff0c \u8fd8\u8981\u5bfc\u5165\u641c\u72d0\u65b0\u95fb\u7684\u5411\u91cf\uff0c\u53ef\u80fd\u4f1a\u591a\u4e00\u4e9b\u5de5\u4f5c\u91cf\u3002\n\u65b9\u6848\u4e8c\uff1a \u5c31\u75285\u4e07\u6761training data\uff0c\u8bad\u7ec3\u4e00\u4e2achar level\u7684Embedding layer, \u53ef\u80fd\u6548\u679c\u4e0d\u4f1a\u592a\u597d\uff0c\u4f46\u53ef\u4ee5\u4f5c\u4e3a\u4e00\u4e2abaseline\u53c2\u8003\u3002\n\u6b64\u5904\u6211\u5148\u9009\u7528**\u65b9\u6848\u4e8c**\u8fdb\u884c\u5c1d\u8bd5\u3002","a30ea2f6":"#\u6570\u636e\u9884\u5904\u7406"}}