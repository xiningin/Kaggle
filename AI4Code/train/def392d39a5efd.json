{"cell_type":{"cdc56382":"code","d7acd862":"code","96ce5e56":"code","3a5cdd63":"code","0be4ff93":"code","c5abd090":"code","2d707288":"code","fa0ad100":"code","3872b125":"code","9f6ed9d8":"code","ff138817":"code","0cd75ba1":"code","256a5ac3":"code","2effab7d":"code","4c90ba92":"code","ef6856ee":"code","f630ef51":"code","254aafd7":"markdown","26b2c4b3":"markdown","d7ed2763":"markdown","64a9ba48":"markdown","08693c04":"markdown","ad0ef19d":"markdown","5dc2b886":"markdown","bacfeeff":"markdown","ecc690ee":"markdown","bb47443c":"markdown"},"source":{"cdc56382":"import warnings\nwarnings.filterwarnings(\"ignore\")\nimport os\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'","d7acd862":"import pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import MinMaxScaler,PolynomialFeatures\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression, Ridge, Lasso\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn import metrics\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras.models import *\nfrom tensorflow.keras.optimizers import *\nfrom tensorflow.keras.callbacks import LearningRateScheduler\nimport xgboost as xg\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.metrics import mean_squared_error","96ce5e56":"train_df = pd.read_csv(\"..\/input\/petfinder-pawpularity-score\/train.csv\")\ntest_df = pd.read_csv(\"..\/input\/petfinder-pawpularity-score\/test.csv\")","3a5cdd63":"train_df.head()","0be4ff93":"test_df.head()","c5abd090":"print(train_df.shape)\nprint(test_df.shape)","2d707288":"X = np.array(train_df[train_df.columns[1:-1]])\ny = np.array(train_df['Pawpularity'])","fa0ad100":"xtr, xte, ytr, yte = train_test_split(X, y, test_size=0.15,random_state=244)","3872b125":"model1 = RandomForestRegressor(max_depth = 15)\n\nmodel1.fit(xtr,ytr)\n\npredictions = model1.predict(xte)\n\nprint(\"Error: \" , np.sqrt(mean_squared_error(yte, predictions)))","9f6ed9d8":"model2 = xg.XGBRegressor(n_estimators = 512,max_depth = 20,objective ='reg:squarederror')\nmodel2.fit(xtr, ytr)\n\npredictions = model2.predict(xte)\n\nprint(\"Error: \" , np.sqrt(mean_squared_error(yte, predictions)))","ff138817":"poly2 = PolynomialFeatures(degree=2)\npoly3 = PolynomialFeatures(degree=3)\npoly4 = PolynomialFeatures(degree=4)\npoly5 = PolynomialFeatures(degree=5)\npoly6 = PolynomialFeatures(degree=6)\npoly7 = PolynomialFeatures(degree=7)\n\nxtr2 = poly2.fit_transform(xtr)\nxtr3 = poly3.fit_transform(xtr)\nxtr4 = poly4.fit_transform(xtr)\nxtr5 = poly5.fit_transform(xtr)\nxtr6 = poly6.fit_transform(xtr)\nxtr7 = poly7.fit_transform(xtr)\n\n\nmodel3 = LinearRegression()\n\nmodel3.fit(xtr2,ytr)\npredictions = model3.predict(poly2.fit_transform(xte))\nprint(\"Polynomial Regression error(degree=2): \", np.sqrt(mean_squared_error(yte,predictions)))\nmodel3.fit(xtr3,ytr)\npredictions = model3.predict(poly3.fit_transform(xte))\nprint(\"Polynomial Regression error(degree=3): \", np.sqrt(mean_squared_error(yte,predictions)))\nmodel3.fit(xtr4,ytr)\npredictions = model3.predict(poly4.fit_transform(xte))\nprint(\"Polynomial Regression error(degree=4): \", np.sqrt(mean_squared_error(yte,predictions)))","0cd75ba1":"X2 = poly2.fit_transform(X)\n\nmodel3.fit(X2,y)\npredictions = model3.predict(poly2.fit_transform(X))\nprint(\"Polynomial Regression error(degree=2): \", np.sqrt(mean_squared_error(y,predictions)))","256a5ac3":"model4 = Sequential()\nmodel4.add(InputLayer(input_shape=(xtr.shape[1],)))\n\nfor _ in range(3):\n    model4.add(Dense(128,activation=\"relu\",kernel_initializer=\"normal\"))\n\nmodel4.add(Dense(1,activation=\"linear\",kernel_initializer=\"normal\"))\n\noptim = Adam(\n    learning_rate=0.0005,\n    beta_1=0.9,\n    beta_2=0.999,\n    epsilon=1e-07,\n    amsgrad=False,\n    name=\"Adam\",\n)\n\nmodel4.compile(loss=\"mse\",optimizer='adam',metrics=\"mse\")\nmodel4.summary()","2effab7d":"model4.fit(xtr,ytr,batch_size=32,epochs=100,validation_data=(xte,yte))","4c90ba92":"print(\"Error on test data: \" , np.sqrt(model4.evaluate(xte, yte,verbose = 0)[1]))\nprint(\"Error on whole data: \" ,np.sqrt(model4.evaluate(X, y,verbose = 0)[1]))","ef6856ee":"submission_x = np.array(test_df[test_df.columns[1:]])\nId = np.array(test_df['Id'])","f630ef51":"predictions = model1.predict(submission_x)\nsubmission_df = pd.DataFrame()\n\nsubmission_df['Id'] = Id\nsubmission_df['Pawpularity'] = predictions\nsubmission_df.to_csv('submission.csv',index=False)","254aafd7":"## NN ? why not","26b2c4b3":"# Saving results","d7ed2763":"# Reading data","64a9ba48":"## XGBoost","08693c04":"# Building models and checking performance","ad0ef19d":"## Linear Regression (polynomial space)","5dc2b886":"## Random Forest Regressor","bacfeeff":"# Preparing data","ecc690ee":"As we can see polynom of degree 2 showed pretty fine results. Let's test it on whole data","bb47443c":"Here we can choose any model and by submitting check which one is the best. Let's take as an example Random Forests results"}}