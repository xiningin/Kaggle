{"cell_type":{"fbe57f8c":"code","4d383d74":"code","4b95ec3a":"code","bd8e7eae":"code","dc375405":"code","5567753b":"code","996951e1":"code","7413b3d3":"code","feee96d5":"code","113c6c8a":"code","53164d50":"code","6fdaa005":"code","26363f58":"code","61bdc32f":"code","3384eed5":"code","f2bb8b2e":"code","dadf4bac":"code","c301ca50":"code","e2f50345":"code","4e1eda9f":"code","c661324b":"code","b4c06203":"code","4cd293c7":"code","ce8ba1c8":"markdown","43187255":"markdown","948d5203":"markdown","236c19e1":"markdown","02ed812c":"markdown","b2d1573c":"markdown","5f45e576":"markdown","ab8cad31":"markdown","0e916f8e":"markdown","bf3244d4":"markdown","11839484":"markdown","1d9fbc4b":"markdown","5ce8b207":"markdown"},"source":{"fbe57f8c":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","4d383d74":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style('whitegrid')\n\nimport warnings\nwarnings.simplefilter(action='ignore')\n\n# use this code to show all columns\npd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', None)","4b95ec3a":"train = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv')\n\ntrain_test = pd.concat((train, test), sort=False).reset_index(drop=True)\ntrain_test.drop(['Id'], axis=1, inplace=True)\n\ntrain_test.head()","bd8e7eae":"print('Number of categorial types: ', len(train.select_dtypes(include='object').columns))\nprint('Categorial types: ', train.select_dtypes(include='object').columns)","dc375405":"# see different categorical encoders\n\nfrom category_encoders.ordinal import OrdinalEncoder\nfrom category_encoders.woe import WOEEncoder\nfrom category_encoders.target_encoder import TargetEncoder\nfrom category_encoders.sum_coding import SumEncoder\nfrom category_encoders.m_estimate import MEstimateEncoder\nfrom category_encoders.leave_one_out import LeaveOneOutEncoder\nfrom category_encoders.helmert import HelmertEncoder\nfrom category_encoders.cat_boost import CatBoostEncoder\nfrom category_encoders.james_stein import JamesSteinEncoder\nfrom category_encoders.one_hot import OneHotEncoder","5567753b":"%%time\nencoder = OrdinalEncoder()\nordinal_encoder_example  = encoder.fit_transform(train_test['LotConfig'])","996951e1":"# see  data\nordinal_encoder_example['Original_data'] = train_test['LotConfig']\nordinal_encoder_example = ordinal_encoder_example.rename(columns={'LotConfig': 'Ordinal_data'})\nordinal_encoder_example.head()","7413b3d3":"%%time\nTE_encoder = TargetEncoder()\ntrain_te = TE_encoder.fit_transform(train['LotShape'], train['SalePrice'])\ntest_te = TE_encoder.transform(test['LotShape'])","feee96d5":"# see for train data\ntarget_encoder_example = train_te.rename(columns={'LotShape': 'Target_encoder_data'})\ntarget_encoder_example['Original_data'] = train['LotShape']\ntarget_encoder_example.head()","113c6c8a":"%%time\nSE_encoder = SumEncoder('GarageType')\ntrain_se = SE_encoder.fit_transform(train['GarageType'], train['SalePrice'])\ntest_se = SE_encoder.transform(test['GarageType'])","53164d50":"# see for train data\nsum_encoder_example = train_se.rename(columns={'GarageType': 'Sum_encoder_data'})\nsum_encoder_example['Original_data'] = train['GarageType']\nsum_encoder_example.head()","6fdaa005":"%%time\nMEE_encoder = MEstimateEncoder()\ntrain_mee = MEE_encoder.fit_transform(train['KitchenQual'], train['SalePrice'])\ntest_mee = MEE_encoder.transform(train_test['KitchenQual'])","26363f58":"# see for train data\nme_encoder_example = train_mee.rename(columns={'KitchenQual': 'ME_encoder_data'})\nme_encoder_example['Original_data'] = train['KitchenQual']\nme_encoder_example.head()","61bdc32f":"%%time\nLOOE_encoder = LeaveOneOutEncoder()\ntrain_looe = LOOE_encoder.fit_transform(train['GarageFinish'], train['SalePrice'])\ntest_looe = LOOE_encoder.transform(test['GarageFinish'])","3384eed5":"# see for train data\nloo_encoder_example = train_looe.rename(columns={'GarageFinish': 'LOO_encoder_data'})\nloo_encoder_example['Original_data'] = train['GarageFinish']\nloo_encoder_example.head()","f2bb8b2e":"%%time\nHE_encoder = HelmertEncoder('Foundation')\ntrain_he = HE_encoder.fit_transform(train['Foundation'], train['SalePrice'])\ntest_he = HE_encoder.transform(test['Foundation'])","dadf4bac":"# see for train data\nhe_encoder_example = train_he.rename(columns={'Foundation': 'HE_encoder_data'})\nhe_encoder_example['Original_data'] = train['Foundation']\nhe_encoder_example.head()","c301ca50":"%%time\nCB_encoder = CatBoostEncoder()\ntrain_cb = CB_encoder.fit_transform(train['Neighborhood'], train['SalePrice'])\ntest_cb = CB_encoder.transform(test['Neighborhood'])","e2f50345":"# see for train data\ncb_encoder_example = train_cb.rename(columns={'Neighborhood': 'CB_encoder_data'})\ncb_encoder_example['Original_data'] = train['Neighborhood']\ncb_encoder_example.head()","4e1eda9f":"%%time\nJS_encoder = JamesSteinEncoder()\ntrain_js = JS_encoder.fit_transform(train['SaleCondition'], train['SalePrice'])\ntest_js = JS_encoder.transform(test['SaleCondition'])","c661324b":"# see for train data\njs_encoder_example = train_js.rename(columns={'SaleCondition': 'JS_encoder_data'})\njs_encoder_example['Original_data'] = train['SaleCondition']\njs_encoder_example.head()","b4c06203":"%%time\n# with  category_encoders, but the most common approach with pandas dummy.\nOHE_encoder = OneHotEncoder('RoofStyle')\ntrain_ohe = OHE_encoder.fit_transform(train['RoofStyle'])\ntest_ohe = OHE_encoder.transform(test['RoofStyle'])","4cd293c7":"# see for train data\noh_encoder_example = train_ohe.rename(columns={'RoofStyle': 'OH_encoder_data'})\noh_encoder_example['Original_data'] = train['RoofStyle']\noh_encoder_example.head()","ce8ba1c8":"# Explore the data:","43187255":"# 6 - Leave One Out Encoder\n\nLeave-one-out Encoding (LOO or LOOE) is another example of target-based encoders. The name of the method clearly speaks for itself: we calculate the mean target of category k for observation j if observation j is removed from the dataset:\n\n$x_i^k = (sum_{j\\not=i} y_i * (x_j ==k)) - y_i \/ (sum_{j\\not=i}x_j == k )$\n\nWhile encoding the test dataset, a category is replaced with the mean target of the category k in the train dataset:\n\n$x^k = sum ( y_i * (x_j ==k)) \/ sum (x_j == k) $\n\nOne of the disadvantages of LOO, just like with all other target-based encoders, is target leakage (the similar problem we have with Target encoder). But when it comes to LOO, this problem gets really dramatic, as far as we may perfectly classify the training dataset by making a single split: the optimal threshold for category k could be calculated with the following formula:\n\n$t^k = sum ( y_i * (x_j ==k)) - 0.5 \/ sum (x_j == k) $\n\nAnother problem with LOO is a shift between values in the train and the test samples. It has a high influence if we work with tree-based models.\n\nIf we go deeper, the encoding algorithm is slightly different between the training and test data sets. For the training data set, the record under consideration is left out, hence the name Leave One Out. The encoding is as follows for a certain value of a certain categorical variable. For the validation data or prediction data set, the definition is slightly different. We don\u2019t need to leave the current record out and we don\u2019t need the randomness factor. ","948d5203":"# 9 - James-Stein Encoder\n\nJames-Stein Encoder is a target-based encoder.\n\nThe idea behind James-Stein Encoder is simple. Estimation of the mean target for category k could be calculated according to the following formula:\n\n$ x^k = (1-B) * n^+\/ n + B*y^+\/y$\n\nEncoding is aimed to improve the estimation of the category\u2019s mean target (first member of the amount) by shrinking them towards a more central average (second member of the amount). The only hyperparameter in the formula is $B$ \u2014 the power of shrinking. It could be understood as the power of regularization, i.e. the bigger values of $B$ will result in the bigger weight of global mean (underfit), while the lower values of $B$ are, the bigger weight of condition mean (overfit).\nOne way to select $B$ is to tune it like a hyperparameter via cross-validation, but Charles Stein came up with another solution to the problem:\n\n$B = Var[y^k] \/ (Va[y^k]+Var[y])$","236c19e1":"**But, on this dataset perfom WOE Encoder is difficult, becase it needs a binary target value.**\n\nDocumentation: fit(X, y) - Fit encoder according to X and **binary** y.","02ed812c":"# 3 - Target Encoder\n\nTarget Encoding is the most popular type of encoding in Kaggle competitions. It takes information about the target to encode categories, which makes it extremely powerful. The encoded category values are calculated according to the following formulas:\n\n$ s = 1 \/ (1 + exp(-(n-mdl)\/a)) $\n\n$ x^k = prior * (1 - s) + s * n^+ \/ n $\n\n- $mdl$ - min data (samples) in leaf,\n- $a$ - smoothing parameter, which representing the power of regularization.\n\nRecommended values for them are in the range (1, 100). New values of category and values with just a single appearance in the training dataset are replaced with the prior ones.\n\nHowever, it has a huge disadvantage \u2014 target leakage: it uses information about the target. Because of the target leakage, the model overfits the training data which results in unreliable validation and lower test scores.\n\nBut you can reduce the effect of target leakage by increase regularization, add random noise to the representation of the category in the training dataset (some sort of augmentation), or use Double Validation.","b2d1573c":"# 2 - WOE Encoder\n\nWeight Of Evidence is a usually used target-based encoder in credit scoring.\n\nIt is a measure of the \u201cstrength\u201d of a grouping for separating good and bad risk (default). It is calculated from the basic odds ratio:\n\n$a = Distribution of Good Credit Outcomes$\n\n$b = Distribution of Bad Credit Outcomes$\n\n$WoE = ln(a \/ b)$\n\nIn the formulas above: \n- $Distribution of Goods$ - % of Good Customers in a particular group;\n- $Distribution of Bads$ - % of Bad Customers in a particular group;\n- $ln$ - Natural Log;\n\nKeep in mind,that positive WOE means $Distribution of Goods > Distribution of Bads$. If we have negative WOE, that means $Distribution of Goods < Distribution of Bads$.\n\n*Hint :* Log of a number > 1 means positive value. If less than 1, it means negative value.\n\nBut if we use formulas as is, it might lead to target leakage and overfit. To avoid that, regularization parameter $a$ is induced and WoE is calculated in the following way:\n\n$ nominator = (n^+ + a ) \/ (y^+ + 2*a) $\n\n$ denominator = (n - n^+ + a) \/ (y - y^+ + 2*a) $\n\n$ x^k = ln(nominator\/denominator) $\n","5f45e576":"# 10 - OneHotEncoder\n\nThe One Hot Encoding is another simple way to work with categorical columns. It takes a categorical column that has been Label Encoded and then splits the column into multiple columns. The numbers are replaced by 1s and 0s depending on which column has what value.\nOHE expands the size of your dataset, which makes it memory-inefficient encoder. There are several strategies to overcome the memory problem with OHE, one of which is working with sparse not dense data representation.","ab8cad31":"**We will use different types encoders only for one categorical column, because it is well demonstrate work and we have not any problems with RAM.**\n","0e916f8e":"# 8 - CatBoost Encoder\n\nCatboost is a recently created target-based categorical encoder. It is intended to overcome target leakage problems inherent in LOO. In order to do that, the authors of Catboost introduced the idea of \u201ctime\u201d: the order of observations in the dataset. Clearly, the values of the target statistic for each example rely only on the observed history. To calculate the statistic for observation j in train dataset, we may use only observations, which are collected before observation j, i.e. i\u2264j:\n\n$x_i^k = (sum_{j=0}^{j\u2264i} y_i * (x_j ==k)) - y_i + prior) \/ (sum_{j=0}^{j\u2264i}x_j == k )$\n\n\nTo prevent overfitting, the process of target encoding for train dataset is repeated several times on shuffled versions of the dataset and results are averaged. Encoded values of the test data are calculated the same way as in LOO Encoder:\n\n$x^k = sum ( y_i * (x_j ==k)) +prior \/ sum (x_j == k) $\n\n\n","bf3244d4":"# 5 - M-Estimate Encoder\n\nM-probability estimate of likelihood.\n\nM-Estimate Encoder is a simplified version of Target Encoder. It has only one hyperparameter \u2014 m, which represents the power of regularization. The higher value of m results into stronger shrinking. Recommended values for m is in the range of 1 to 100.\n\n","11839484":"# 7 - Helmert Encoder\n\nHelmert coding is a also commonly used type of categorical encoding for regression. \n\nIt compares each level of a categorical variable to the mean of the subsequent levels.\n\nThis type of encoding can be useful in certain situations where levels of the categorical variable are ordered, say, from lowest to highest, or from smallest to largest.","1d9fbc4b":"# 1 - Ordinal Encoder\n\nOne of the most popular encoders is Ordinal Encoder (OE). In OE, each unique category value is assigned an integer value. For example, 'Inside' in the LotConfig column means 1, 'FR2' means 2 and etc. It also called integer encoding and it is easy to reverse.\n\nIt used, if an ordinal encoding may be enough for variables. The integer values have a natural ordered relationship between each other and machine learning algorithms may be able to understand and harness this relationship.\n\nThis approach has a serious disadvantage: for categorical variables, it imposes an ordinal relationship where no such relationship may exist. This can cause problems and a one-hot encoding may be used instead.\n","5ce8b207":"# 4 - Sum Encoder\n\nIt also called Deviation Encoding or Effect Encoding.\n\nSum Encoder compares the mean of the target variable for a given level of a categorical column to the overall mean of the target. Sum Encoder is commonly used in Linear Regression (LR) types of models. The model with Sum Encoder the intercept represents the grand mean (across all conditions) and the coefficients can be interpreted directly as the main effects."}}