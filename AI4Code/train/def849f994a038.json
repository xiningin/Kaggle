{"cell_type":{"a314c724":"code","2e9335a6":"code","9bd47ba3":"code","11746851":"code","73f0f1d1":"code","0aba6f05":"code","ca216d5b":"code","f5949327":"code","09882522":"code","b7acbbd1":"code","6f258960":"code","b98ded46":"code","9fe03c3e":"code","5f79f5f5":"code","3dcaacaa":"code","220dc98b":"code","1cae86de":"code","9dd46b37":"code","5cc7cf0c":"code","0281c843":"code","a28e89c0":"code","a10c6189":"code","9aeda26a":"code","70e9ba5a":"code","a152ed2a":"code","40246ba0":"code","219319cb":"code","98de7bb1":"code","b8531204":"code","366a3a93":"code","cb7d5483":"markdown","604b60b6":"markdown","8a48f3ff":"markdown","271216c1":"markdown","12860fa5":"markdown","353d9cf6":"markdown","476ec455":"markdown","709f1ae6":"markdown","638bb01e":"markdown","5434ce41":"markdown","ae0bf116":"markdown","21388dd4":"markdown","7451aae0":"markdown","974bccbf":"markdown","158315cb":"markdown","dea0d574":"markdown","87d67285":"markdown"},"source":{"a314c724":"import pandas as pd\nimport numpy as np\nimport os\nimport random\nimport tensorflow as tf\nimport cv2\nfrom tqdm import tqdm\nimport datetime\nfrom tensorflow import keras\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, UpSampling2D, Concatenate \nfrom tensorflow.keras.layers import Input, Add, Conv2DTranspose\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.applications import VGG16\nfrom tensorflow.keras.optimizers import SGD, Adam\nfrom tensorflow.keras.losses import SparseCategoricalCrossentropy, MeanSquaredError, BinaryCrossentropy\nfrom tensorflow.keras.utils import plot_model\nfrom tensorflow.keras import callbacks\n\nfrom  matplotlib import pyplot as plt\nimport matplotlib.image as mpimg\nfrom IPython.display import clear_output\n%matplotlib inline\n\nfrom IPython.display import HTML\nfrom base64 import b64encode","2e9335a6":"# Load directories\ntrain_data_dir = \"..\/input\/kittiroadsegmentation\/training\/image_2\/\"\ntrain_gt_dir = \"..\/input\/kittiroadsegmentation\/training\/gt_image_2\/\"\n\ntest_data_dir = \"..\/input\/kittiroadsegmentation\/testing\/\"","9bd47ba3":"# Number of training examples\nTRAINSET_SIZE = int(len(os.listdir(train_data_dir)) * 0.8)\nprint(f\"Number of Training Examples: {TRAINSET_SIZE}\")\n\nVALIDSET_SIZE = int(len(os.listdir(train_data_dir)) * 0.1)\nprint(f\"Number of Validation Examples: {VALIDSET_SIZE}\")\n\nTESTSET_SIZE = int(len(os.listdir(train_data_dir)) - TRAINSET_SIZE - VALIDSET_SIZE)\nprint(f\"Number of Testing Examples: {TESTSET_SIZE}\")","11746851":"# Initialize Constants\nIMG_SIZE = 128\nN_CHANNELS = 3\nN_CLASSES = 1\nSEED = 123","73f0f1d1":"# Function to load image and return a dictionary\ndef parse_image(img_path: str) -> dict:\n    image = tf.io.read_file(img_path)\n    image = tf.image.decode_jpeg(image, channels=3)\n    image = tf.image.convert_image_dtype(image, tf.uint8)\n\n    # Three types of img paths: um, umm, uu\n    # gt image paths: um_road, umm_road, uu_road\n    mask_path = tf.strings.regex_replace(img_path, \"image_2\", \"gt_image_2\")\n    mask_path = tf.strings.regex_replace(mask_path, \"um_\", \"um_road_\")\n    mask_path = tf.strings.regex_replace(mask_path, \"umm_\", \"umm_road_\")\n    mask_path = tf.strings.regex_replace(mask_path, \"uu_\", \"uu_road_\")\n    \n    mask = tf.io.read_file(mask_path)\n    mask = tf.image.decode_png(mask, channels=3)\n    \n    non_road_label = np.array([255, 0, 0])\n    road_label = np.array([255, 0, 255])\n    other_road_label = np.array([0, 0, 0])\n    \n    # Convert to mask to binary mask\n    mask = tf.experimental.numpy.all(mask == road_label, axis = 2)\n    mask = tf.cast(mask, tf.uint8)\n    mask = tf.expand_dims(mask, axis=-1)\n\n    return {'image': image, 'segmentation_mask': mask}","0aba6f05":"# Generate dataset variables\nall_dataset = tf.data.Dataset.list_files(train_data_dir + \"*.png\", seed=SEED)\nall_dataset = all_dataset.map(parse_image)\n\ntrain_dataset = all_dataset.take(TRAINSET_SIZE + VALIDSET_SIZE)\nval_dataset = train_dataset.skip(TRAINSET_SIZE)\ntrain_dataset = train_dataset.take(TRAINSET_SIZE)\ntest_dataset = all_dataset.skip(TRAINSET_SIZE + VALIDSET_SIZE)","ca216d5b":"# Tensorflow function to rescale images to [0, 1]\n@tf.function\ndef normalize(input_image: tf.Tensor, input_mask: tf.Tensor) -> tuple:\n    input_image = tf.cast(input_image, tf.float32) \/ 255.0\n    return input_image, input_mask\n\n# Tensorflow function to apply preprocessing transformations\n@tf.function\ndef load_image_train(datapoint: dict) -> tuple:\n    input_image = tf.image.resize(datapoint['image'], (IMG_SIZE, IMG_SIZE))\n    input_mask = tf.image.resize(datapoint['segmentation_mask'], (IMG_SIZE, IMG_SIZE))\n\n    if tf.random.uniform(()) > 0.5:\n        input_image = tf.image.flip_left_right(input_image)\n        input_mask = tf.image.flip_left_right(input_mask)\n\n    input_image, input_mask = normalize(input_image, input_mask)\n\n    return input_image, input_mask\n\n# Tensorflow function to preprocess validation images\n@tf.function\ndef load_image_test(datapoint: dict) -> tuple:\n    input_image = tf.image.resize(datapoint['image'], (IMG_SIZE, IMG_SIZE))\n    input_mask = tf.image.resize(datapoint['segmentation_mask'], (IMG_SIZE, IMG_SIZE))\n\n    input_image, input_mask = normalize(input_image, input_mask)\n\n    return input_image, input_mask","f5949327":"BATCH_SIZE = 32\nBUFFER_SIZE = 1000\n\ndataset = {\"train\": train_dataset, \"val\": val_dataset, \"test\": test_dataset}\n\n# -- Train Dataset --#\ndataset['train'] = dataset['train'].map(load_image_train, num_parallel_calls=tf.data.AUTOTUNE)\ndataset['train'] = dataset['train'].shuffle(buffer_size=BUFFER_SIZE, seed=SEED)\ndataset['train'] = dataset['train'].repeat()\ndataset['train'] = dataset['train'].batch(BATCH_SIZE)\ndataset['train'] = dataset['train'].prefetch(buffer_size=tf.data.AUTOTUNE)\n\n#-- Validation Dataset --#\ndataset['val'] = dataset['val'].map(load_image_test)\ndataset['val'] = dataset['val'].repeat()\ndataset['val'] = dataset['val'].batch(BATCH_SIZE)\ndataset['val'] = dataset['val'].prefetch(buffer_size=tf.data.AUTOTUNE)\n\n#-- Testing Dataset --#\ndataset['test'] = dataset['test'].map(load_image_test)\ndataset['test'] = dataset['test'].batch(BATCH_SIZE)\ndataset['test'] = dataset['test'].prefetch(buffer_size=tf.data.AUTOTUNE)\n\nprint(dataset['train'])\nprint(dataset['val'])\nprint(dataset['test'])","09882522":"# Function to view the images from the directory\ndef display_sample(display_list):\n    plt.figure(figsize=(18, 18))\n\n    title = ['Input Image', 'True Mask', 'Predicted Mask']\n\n    for i in range(len(display_list)):\n        plt.subplot(1, len(display_list), i+1)\n        plt.title(title[i])\n        plt.imshow(tf.keras.preprocessing.image.array_to_img(display_list[i]))\n        plt.axis('off')\n        \n    plt.show()\n    \nfor image, mask in dataset[\"train\"].take(1):\n    sample_image, sample_mask = image, mask\n\ndisplay_sample([sample_image[0], sample_mask[0]])","b7acbbd1":"# Get VGG-16 network as backbone\nvgg16_model = VGG16()\nvgg16_model.summary()","6f258960":"# Define input shape\ninput_shape = (IMG_SIZE, IMG_SIZE, N_CHANNELS)","b98ded46":"# Generate a new model using the VGG network\n# Input\ninputs = Input(input_shape)\n\n# VGG network\nvgg16_model = VGG16(include_top = False, weights = 'imagenet', input_tensor = inputs)\n\n# Encoder Layers\nc1 = vgg16_model.get_layer(\"block3_pool\").output         \nc2 = vgg16_model.get_layer(\"block4_pool\").output         \nc3 = vgg16_model.get_layer(\"block5_pool\").output         \n\n# Decoder\nu1 = Conv2DTranspose(512, (2, 2), strides = (2, 2))(c3)\nd1 = Add()([u1, c2])\nd1 = Conv2D(256, 1, activation = 'sigmoid')(d1)\n\nu2 = Conv2DTranspose(256, (2, 2), strides = (2, 2))(d1)\nd2 = Add()([u2, c1])\nd2 = Conv2D(256, 1, activation = 'sigmoid')(d2)\n\n# Output\nu3 = Conv2DTranspose(N_CLASSES, (8, 8), strides = (8, 8))(d2)\noutputs = Conv2D(N_CLASSES, 1, activation = 'sigmoid')(u3)\n\nmodel = Model(inputs, outputs, name = \"VGG_FCN8\")","9fe03c3e":"m_iou = tf.keras.metrics.MeanIoU(2)\nmodel.compile(optimizer=Adam(),\n              loss=BinaryCrossentropy(),\n              metrics=[m_iou])","5f79f5f5":"# Function to create a mask out of network prediction\ndef create_mask(pred_mask: tf.Tensor) -> tf.Tensor:\n    # Round to closest\n    pred_mask = tf.math.round(pred_mask)\n    \n    # [IMG_SIZE, IMG_SIZE] -> [IMG_SIZE, IMG_SIZE, 1]\n    pred_mask = tf.expand_dims(pred_mask, axis=-1)\n    return pred_mask\n\n# Function to show predictions\ndef show_predictions(dataset=None, num=1):\n    if dataset:\n        # Predict and show image from input dataset\n        for image, mask in dataset.take(num):\n            pred_mask = model.predict(image)\n            display_sample([image[0], true_mask, create_mask(pred_mask)])\n    else:\n        # Predict and show the sample image\n        inference = model.predict(sample_image)\n        display_sample([sample_image[0], sample_mask[0],\n                        inference[0]])\n        \nfor image, mask in dataset['train'].take(1):\n    sample_image, sample_mask = image, mask\n\nshow_predictions()","3dcaacaa":"# Callbacks and Logs\nclass DisplayCallback(callbacks.Callback):\n    def on_epoch_end(self, epoch, logs=None):\n        clear_output(wait=True)\n        show_predictions()\n        print ('\\nSample Prediction after epoch {}\\n'.format(epoch+1))\n\nlogdir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n\ncallbacks = [\n    DisplayCallback(),\n    callbacks.TensorBoard(logdir, histogram_freq = -1),\n    callbacks.EarlyStopping(patience = 10, verbose = 1),\n    callbacks.ModelCheckpoint('best_model.h5', verbose = 1, save_best_only = True)\n]\n        \n# Set Variables\nEPOCHS = 200\nSTEPS_PER_EPOCH = TRAINSET_SIZE \/\/ BATCH_SIZE\nVALIDATION_STEPS = VALIDSET_SIZE \/\/ BATCH_SIZE","220dc98b":"model_history = model.fit(dataset['train'], epochs=EPOCHS,\n                          steps_per_epoch=STEPS_PER_EPOCH,\n                          validation_data = dataset[\"val\"],\n                          validation_steps=VALIDATION_STEPS,\n                          callbacks = callbacks)","1cae86de":"# Function to calculate mask over image\ndef weighted_img(img, initial_img, \u03b1=1., \u03b2=0.5, \u03b3=0.):\n    return cv2.addWeighted(initial_img, \u03b1, img, \u03b2, \u03b3)\n\n# Function to process an individual image and it's mask\ndef process_image_mask(image, mask):\n    # Round to closest\n    mask = tf.math.round(mask)\n    \n    # Convert to mask image\n    zero_image = np.zeros_like(mask)\n    mask = np.dstack((mask, zero_image, zero_image))\n    mask = np.asarray(mask, np.float32)\n    \n    # Convert to image image\n    image = np.asarray(image, np.float32)\n    \n    # Get the final image\n    final_image = weighted_img(mask, image)\n\n    return final_image","9dd46b37":"# Function to save predictions\ndef save_predictions(dataset):\n    # Predict and save image the from input dataset\n    index = 0\n    for batch_image, batch_mask in dataset:\n        for image, mask in zip(batch_image, batch_mask):\n            print(f\"Processing image : {index}\")\n            pred_mask = model.predict(tf.expand_dims(image, axis = 0))\n            save_sample([image, process_image_mask(image, pred_mask[0])], index)\n            index += 1\n\n# Function to save the images as a plot\ndef save_sample(display_list, index):\n    plt.figure(figsize=(18, 18))\n\n    title = ['Input Image', 'Predicted Mask']\n\n    for i in range(len(display_list)):\n        plt.subplot(1, len(display_list), i+1)\n        plt.title(title[i])\n        plt.imshow(tf.keras.preprocessing.image.array_to_img(display_list[i]))\n        plt.axis('off')\n        \n    plt.savefig(f\"outputs\/{index}.png\")\n    plt.show()","5cc7cf0c":"os.mkdir(\"outputs\")\nsave_predictions(dataset['test'])","0281c843":"# Function to view video\ndef play(filename):\n    html = ''\n    video = open(filename,'rb').read()\n    src = 'data:video\/mp4;base64,' + b64encode(video).decode()\n    html += '<video width=1000 controls autoplay loop><source src=\"%s\" type=\"video\/mp4\"><\/video>' % src \n    return HTML(html)","a28e89c0":"# Function to process an individual image\ndef process_image(image):\n    # Preprocess image\n    image = cv2.resize(image, (IMG_SIZE, IMG_SIZE))\n    # Get the binary mask\n    pred_mask = model.predict(np.expand_dims(image, axis = 0))\n    mask = np.round_(pred_mask[0])\n    \n    # Convert to mask image\n    zero_image = np.zeros_like(mask)\n    mask = np.dstack((mask, zero_image, zero_image)) * 255\n    mask = np.asarray(mask, np.uint8)\n    \n    # Get the final image\n    final_image = weighted_img(mask, image)\n    final_image = cv2.resize(final_image, (1280, 720))\n\n    return final_image","a10c6189":"# Make a new directory\nos.mkdir(\"videos\")","9aeda26a":"# Creating a VideoCapture object to read the video\nproject_video = \"project_video.mp4\"\noriginal_video = cv2.VideoCapture(test_data_dir + project_video)\nframe_width = int(original_video.get(3))\nframe_height = int(original_video.get(4))\n \n# Define the codec and create VideoWriter object.The output is stored in 'outpy.avi' file.\nfourcc = cv2.VideoWriter_fourcc('m','p','4','v')\nfps = 60\noutput = cv2.VideoWriter(\"videos\/\" + project_video, fourcc, fps, (frame_width,frame_height))\n\n# Process Video\nwhile(original_video.isOpened()):\n    ret, frame = original_video.read()\n\n    if ret == True:\n        # Write the frame into the file 'output.avi'\n        output.write(process_image(frame))\n\n    else:\n        break\n\n# When everything done, release the video capture and video write objects\noriginal_video.release()\noutput.release()","70e9ba5a":"play(\"videos\/\" + project_video)","a152ed2a":"# Creating a VideoCapture object to read the video\nproject_video = \"challenge.mp4\"\noriginal_video = cv2.VideoCapture(test_data_dir + project_video)\nframe_width = int(original_video.get(3))\nframe_height = int(original_video.get(4))\n \n# Define the codec and create VideoWriter object.The output is stored in 'outpy.avi' file.\nfourcc = cv2.VideoWriter_fourcc('m','p','4','v')\nfps = 60\noutput = cv2.VideoWriter(\"videos\/\" + project_video, fourcc, fps, (frame_width,frame_height))\n\n# Process Video\nwhile(original_video.isOpened()):\n    ret, frame = original_video.read()\n\n    if ret == True:\n        # Write the frame into the file 'output.avi'\n        output.write(process_image(frame))\n\n    else:\n        break\n\n# When everything done, release the video capture and video write objects\noriginal_video.release()\noutput.release()","40246ba0":"play(\"videos\/\" + project_video)","219319cb":"# Creating a VideoCapture object to read the video\nproject_video = \"challenge_video.mp4\"\noriginal_video = cv2.VideoCapture(test_data_dir + project_video)\nframe_width = int(original_video.get(3))\nframe_height = int(original_video.get(4))\n \n# Define the codec and create VideoWriter object.The output is stored in 'outpy.avi' file.\nfourcc = cv2.VideoWriter_fourcc('m','p','4','v')\nfps = 60\noutput = cv2.VideoWriter(\"videos\/\" + project_video, fourcc, fps, (frame_width,frame_height))\n\n# Process Video\nwhile(original_video.isOpened()):\n    ret, frame = original_video.read()\n\n    if ret == True:\n        # Write the frame into the file 'output.avi'\n        output.write(process_image(frame))\n\n    else:\n        break\n\n# When everything done, release the video capture and video write objects\noriginal_video.release()\noutput.release()","98de7bb1":"play(\"videos\/\" + project_video)","b8531204":"# Creating a VideoCapture object to read the video\nproject_video = \"harder_challenge_video.mp4\"\noriginal_video = cv2.VideoCapture(test_data_dir + project_video)\nframe_width = int(original_video.get(3))\nframe_height = int(original_video.get(4))\n \n# Define the codec and create VideoWriter object.The output is stored in 'outpy.avi' file.\nfourcc = cv2.VideoWriter_fourcc('m','p','4','v')\nfps = 60\noutput = cv2.VideoWriter(\"videos\/\" + project_video, fourcc, fps, (frame_width,frame_height))\n\n# Process Video\nwhile(original_video.isOpened()):\n    ret, frame = original_video.read()\n\n    if ret == True:\n        # Write the frame into the file 'output.avi'\n        output.write(process_image(frame))\n\n    else:\n        break\n\n# When everything done, release the video capture and video write objects\noriginal_video.release()\noutput.release()","366a3a93":"play(\"videos\/\" + project_video)","cb7d5483":"### Harder Challenge Video","604b60b6":"## References\n\n- [Kitti Dataset Processing](http:\/\/ronny.rest\/blog\/post_2017_09_06_kitti_road_data\/)\n- [Image Segmentation on Keras](https:\/\/yann-leguilly.gitlab.io\/post\/2019-12-14-tensorflow-tfdata-segmentation\/)","8a48f3ff":"## Training","271216c1":"## Define Network","12860fa5":"### Loss Function","353d9cf6":"## Load Dataset","476ec455":"### Apply Transformations","709f1ae6":"### Source Dataset","638bb01e":"### Project Video","5434ce41":"### Challenge Video 2","ae0bf116":"### Check Model","21388dd4":"## Import Libraries","7451aae0":"## Testing (Videos)","974bccbf":"### Challenge Video","158315cb":"# Fully Convolutional Network","dea0d574":"## Testing (Test Dataset)","87d67285":"### Train Model"}}