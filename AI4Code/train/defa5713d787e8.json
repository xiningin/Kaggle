{"cell_type":{"7bcf60d8":"code","1d07ae72":"code","7a09d9c0":"code","ff347383":"code","b7431481":"code","5d8d2b57":"code","f853d13e":"code","fff69f94":"code","3fa4faaa":"code","62944afc":"code","df868741":"code","5512b0a9":"code","424bdead":"code","9c7a02f7":"code","0aa34b4f":"code","98ddcf24":"code","d8c6be2f":"code","2ea1b80e":"code","b7381eda":"code","6195a850":"code","97d1fd57":"markdown","f315cb9a":"markdown","391f9895":"markdown","925fcd9c":"markdown","fea9777e":"markdown","048d7f75":"markdown","6818e901":"markdown","dc16a4b8":"markdown","b6144d87":"markdown","9d3ec4e0":"markdown","8d99ec02":"markdown","5aebfca7":"markdown","02399807":"markdown","9fa82d10":"markdown","7c1b13b3":"markdown","60eaf408":"markdown","9b8d0a3c":"markdown","0349f295":"markdown","f1ada4ae":"markdown"},"source":{"7bcf60d8":"import os\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import roc_auc_score\nimport matplotlib.pyplot as plt\nfrom itertools import combinations\nimport seaborn as sns\nimport lightgbm as lgb\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","1d07ae72":"%%time\nPATH_TO_DATA = '..\/input'\n\ndf_train_features = pd.read_csv(os.path.join(PATH_TO_DATA, \n                                             'train_features.csv'), \n                                    index_col='match_id_hash')\ndf_train_targets = pd.read_csv(os.path.join(PATH_TO_DATA, \n                                            'train_targets.csv'), \n                                   index_col='match_id_hash')\ndf_test_features = pd.read_csv(os.path.join(PATH_TO_DATA, 'test_features.csv'), \n                                   index_col='match_id_hash')\n\ny = df_train_targets['radiant_win'].values","7a09d9c0":"try:\n    import ujson as json\nexcept ModuleNotFoundError:\n    import json\n    print ('Please install ujson to read JSON oblects faster')\n    \ntry:\n    from tqdm import tqdm_notebook\nexcept ModuleNotFoundError:\n    tqdm_notebook = lambda x: x\n    print ('Please install tqdm to track progress with Python loops')","ff347383":"def read_matches(matches_file):\n    \n    MATCHES_COUNT = {\n        'test_matches.jsonl': 10000,\n        'train_matches.jsonl': 39675,\n    }\n    _, filename = os.path.split(matches_file)\n    total_matches = MATCHES_COUNT.get(filename)\n    \n    with open(matches_file) as fin:\n        for line in tqdm_notebook(fin, total=total_matches):\n            yield json.loads(line)","b7431481":"json_list = [] #store data that are read\nnumber_of_rows = 50 #how many lines to read \n\n#reading data from .jsonl file\nwith open(os.path.join(PATH_TO_DATA, 'train_matches.jsonl')) as fin:\n    for i in range(number_of_rows):\n        line = fin.readline()\n        json_list.append(json.loads(line))\n        \n#how many matches to read. For example I took 1\nfor i in range(1, 2):\n  for j in range(1, 2):#there is 5 players in each team. But I want to look on only one player.\n    print(json.dumps(json_list[i]['players'][j], indent=4, sort_keys=True))","5d8d2b57":"for i in range(1, 5): #now we will look at 4 matches\n  for j in range(1, 5):#and now will take 5 players\n    print(json.dumps(list(map(lambda x: x['id'][5:], json_list[i]['players'][j]['hero_inventory'])), indent=4, sort_keys=True))","f853d13e":"import collections\n\n\ndef extract_features_csv(match):\n    \n    row = [\n        ('match_id_hash', match['match_id_hash']),\n    ]\n\n    for slot, player in enumerate(match['players']):\n        if slot < 5:\n            player_name = 'r%d' % (slot + 1)\n        else:\n            player_name = 'd%d' % (slot - 4)\n\n        row.append( (f'{player_name}_items', list(map(lambda x: x['id'][5:], player['hero_inventory'])) ) )\n        #here u can extract other data\n\n    return collections.OrderedDict(row)\n\n    \ndef extract_targets_csv(match, targets):\n    return collections.OrderedDict([('match_id_hash', match['match_id_hash'])] + [\n        (field, targets[field])\n        for field in ['game_time', 'radiant_win', 'duration', 'time_remaining', 'next_roshan_team']\n    ])","fff69f94":"def create_features_from_jsonl(matches_file):\n  \n    df_new_features = []\n\n    # Process raw data and add new features\n    for match in read_matches(matches_file):\n        match_id_hash = match['match_id_hash']\n        features = extract_features_csv(match)\n\n        df_new_features.append(features)\n\n    df_new_features = pd.DataFrame.from_records(df_new_features).set_index('match_id_hash')\n    return df_new_features","3fa4faaa":"%%time\ntrain_df = create_features_from_jsonl(os.path.join(PATH_TO_DATA, 'train_matches.jsonl')).fillna(0)\ntest_df = create_features_from_jsonl(os.path.join(PATH_TO_DATA, 'test_matches.jsonl')).fillna(0)","62944afc":"#Let's look at extracted item's data\ntrain_df['r1_items'].head()","df868741":"import pickle as pkl\n\n#Better to save extracted data in files, because extracting takes time...\ntrain_df.to_pickle('df_train.pkl')\ntest_df.to_pickle('df_test.pkl')","5512b0a9":"def add_items_dummies(train_df, test_df):\n    \n    full_df = pd.concat([train_df, test_df], sort=False)\n    train_size = train_df.shape[0]\n\n    for team in 'r', 'd':\n        players = [f'{team}{i}' for i in range(1, 6)]\n        item_columns = [f'{player}_items' for player in players]\n\n        d = pd.get_dummies(full_df[item_columns[0]].apply(pd.Series).stack()).sum(level=0, axis=0)\n        dindexes = d.index.values\n\n        for c in item_columns[1:]:\n            d = d.add(pd.get_dummies(full_df[c].apply(pd.Series).stack()).sum(level=0, axis=0), fill_value=0)\n            d = d.ix[dindexes]\n\n        full_df = pd.concat([full_df, d.add_prefix(f'{team}_item_')], axis=1, sort=False)\n        full_df.drop(columns=item_columns, inplace=True)\n\n    train_df = full_df.iloc[:train_size, :]\n    test_df = full_df.iloc[train_size:, :]\n\n    return train_df, test_df","424bdead":"def drop_consumble_items(train_df, test_df):\n    \n    full_df = pd.concat([train_df, test_df], sort=False)\n    train_size = train_df.shape[0]\n\n    for team in 'r', 'd':\n        consumble_columns = ['tango', 'tpscroll', \n                             'bottle', 'flask',\n                            'enchanted_mango', 'clarity',\n                            'faerie_fire', 'ward_observer',\n                            'ward_sentry']\n        \n        starts_with = f'{team}_item_'\n        consumble_columns = [starts_with + column for column in consumble_columns]\n        full_df.drop(columns=consumble_columns, inplace=True)\n\n    train_df = full_df.iloc[:train_size, :]\n    test_df = full_df.iloc[train_size:, :]\n\n    return train_df, test_df","9c7a02f7":"%%time\nnew_train = pd.read_pickle('df_train.pkl')\nnew_test = pd.read_pickle('df_test.pkl')\n\nnew_train, new_test = add_items_dummies(new_train, new_test)\nnew_train, new_test = drop_consumble_items(new_train, new_test)\n\ntarget = pd.DataFrame(y)","0aa34b4f":"new_train.shape, target.shape, new_test.shape","98ddcf24":"# Features variable to look at features importance in the end\nfeatures = new_train.columns","d8c6be2f":"param = {\n        'bagging_freq': 5,  #handling overfitting\n        'bagging_fraction': 0.5,  #handling overfitting - adding some noise\n        'boost_from_average':'false',\n        'boost': 'gbdt',\n        'feature_fraction': 0.05, #handling overfitting\n        'learning_rate': 0.01,  #the changes between one auc and a better one gets really small thus a small learning rate performs better\n        'max_depth': -1,  \n        'metric':'auc',\n        'min_data_in_leaf': 50,\n        'min_sum_hessian_in_leaf': 10.0,\n        'num_leaves': 10,\n        'num_threads': 5,\n        'tree_learner': 'serial',\n        'objective': 'binary', \n        'verbosity': 1\n    }","2ea1b80e":"%%time\n#divide training data into train and validaton folds\nfolds = StratifiedKFold(n_splits=5, shuffle=False, random_state=17)\n\n#placeholder for out-of-fold, i.e. validation scores\noof = np.zeros(len(new_train))\n\n#for predictions\npredictions = np.zeros(len(new_test))\n\n#and for feature importance\nfeature_importance_df = pd.DataFrame()\n\n#RUN THE LOOP OVER FOLDS\nfor fold_, (trn_idx, val_idx) in enumerate(folds.split(new_train.values, target.values)):\n    \n    X_train, y_train = new_train.iloc[trn_idx], target.iloc[trn_idx]\n    X_valid, y_valid = new_train.iloc[val_idx], target.iloc[val_idx]\n    \n    print(\"Computing Fold {}\".format(fold_))\n    trn_data = lgb.Dataset(X_train, label = y_train)\n    val_data = lgb.Dataset(X_valid, label = y_valid)\n\n    \n    num_round = 5000 \n    verbose=1000 \n    stop=500 \n    \n    #TRAIN THE MODEL\n    clf = lgb.train(param, trn_data, num_round, valid_sets = [trn_data, val_data], verbose_eval=verbose, early_stopping_rounds = stop)\n    \n    #CALCULATE PREDICTION FOR VALIDATION SET\n    oof[val_idx] = clf.predict(new_train.iloc[val_idx], num_iteration=clf.best_iteration)\n    \n    #FEATURE IMPORTANCE\n    fold_importance_df = pd.DataFrame()\n    fold_importance_df[\"Feature\"] = features\n    fold_importance_df[\"importance\"] = clf.feature_importance()\n    fold_importance_df[\"fold\"] = fold_ + 1\n    feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n    \n    #CALCULATE PREDICTIONS FOR TEST DATA, using best_iteration on the fold\n    predictions += clf.predict(new_test, num_iteration=clf.best_iteration) \/ folds.n_splits\n\n#print overall cross-validatino score\nprint(\"CV score: {:<8.5f}\".format(roc_auc_score(target, oof)))","b7381eda":"cols = (feature_importance_df[[\"Feature\", \"importance\"]]\n        .groupby(\"Feature\")\n        .mean()\n        .sort_values(by=\"importance\", ascending=False)[:150].index)\nbest_features = feature_importance_df.loc[feature_importance_df.Feature.isin(cols)]\n\nplt.figure(figsize=(14,28))\nsns.barplot(x=\"importance\", y=\"Feature\", data=best_features.sort_values(by=\"importance\",ascending=False))\nplt.title('Features importance (averaged\/folds)')\nplt.tight_layout()\nplt.savefig('FI.png')","6195a850":"df_submission = pd.DataFrame({'radiant_win_prob': predictions}, \n                                 index=df_test_features.index)\nimport datetime\nsubmission_filename = 'submission_{}.csv'.format(\n    datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S'))\ndf_submission.to_csv(submission_filename)\nprint('Submission saved to {}'.format(submission_filename))","97d1fd57":"# Data Representation","f315cb9a":"So this way you can understand how data is stored and how to extract it.","391f9895":"### In this kernel I will describe extracting different features and will create features based on hero_items. I used others kernels methods (\ud83e\udd20).\n### So I want to thank authors!","925fcd9c":"# Predictions","fea9777e":"Now let's extract item's for every player in every match.","048d7f75":"# Extracting Data","6818e901":"In DOTA there are consumble items, which just restore a small amount of hp\/mana or teleports you. These items do not affect the outcome of the game, so let's remove it!","dc16a4b8":"# Feature Importance","b6144d87":"### Well, we got 0.76345 score. I think it's fine for only one feature[](http:\/\/) \ud83e\udd14","9d3ec4e0":"Ok, Here is the main function that will turn your arrays into features. Each value reflects how many times particular item was bought in each team.","8d99ec02":"![](https:\/\/pm1.narvii.com\/6711\/b35fe75b3f52b585f9e3efc483aa2c3cb7ea9c5c_hq.jpg)","5aebfca7":"So from this output we can understand how data is stored and than extract data from .jsonl file. For example, let's look at items names in 'hero_inventory' field","02399807":"Let's make a prediction. I'm using LightGBM","9fa82d10":"# Feature Engineering","7c1b13b3":"### You can improve this feature! There is still work to do!","60eaf408":"![](https:\/\/pp.userapi.com\/c853624\/v853624637\/1ae24\/I7UTTli-mCk.jpg)","9b8d0a3c":"# Submission File","0349f295":"### Please upvote!","f1ada4ae":"Here is a small example of how to look at data representation in .jsonl file"}}