{"cell_type":{"5c46d3ac":"code","bcac884f":"code","69c043d5":"code","29c410f3":"code","53067c39":"code","a0341515":"code","314863e7":"code","22709662":"code","01da7de5":"code","6e976330":"code","ba7b2a34":"code","1c7e9f55":"code","f5c61b4f":"code","a386cb06":"code","cc31591a":"code","f0f5093e":"markdown","d32782ae":"markdown","ec2f046f":"markdown","bd91f4bc":"markdown","9eddee91":"markdown","3e6dbec3":"markdown"},"source":{"5c46d3ac":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \nfrom sklearn.model_selection import train_test_split\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport keras\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","bcac884f":"test_df = pd.read_csv('\/kaggle\/input\/digit-recognizer\/test.csv')\ntrain_df = pd.read_csv('\/kaggle\/input\/digit-recognizer\/train.csv')","69c043d5":"test_df.head()","29c410f3":"train_df.head()","53067c39":"Y_train_df = train_df['label']\n#Y_train_df = pd.get_dummies(Y_train_df).values\n#Y_train_df = Y_train_df.values.reshape(-1, 1)\n#Y_train_df = standard_scaler.fit_transform(Y_train_df)\n#Y_train_df = keras.utils.np_utils.to_categorical(Y_train_df)\n#Y_train_df = Y_train_df.reshape(-1, 10, 1)\nY_train_df.shape","a0341515":"X_train_df = train_df.drop(columns=\"label\")\n#X_train_df = standard_scaler.fit_transform(X_train_df)\n#X_train_df = keras.utils.np_utils.to_categorical(X_train_df)\nX_train_df.shape","314863e7":"seed_num = 7\nnp.random.seed(seed_num)\n# split into 67% for train and 33% for test\nX_train, X_valid, y_train, y_valid = train_test_split(X_train_df, Y_train_df, test_size=0.15, random_state=seed_num)","22709662":"print(X_train)\nprint(X_valid)\nprint(y_train)\nprint(y_valid)\n","01da7de5":"def create_model(train_data):\n    # create model\n    model = keras.models.Sequential()\n    model.add(keras.layers.Dense(800, input_dim=train_data.shape[1], activation='relu'))\n    #model.add(keras.layers.Dense(800, activation='relu'))\n    model.add(keras.layers.Dense(500, activation='relu'))\n    #model.add(keras.layers.Dense(15, activation='relu'))\n    model.add(keras.layers.Dense(10))\n    model.add(keras.layers.Activation(\"softmax\"))\n    # Compile model\n    model.compile(optimizer ='sgd', loss = 'sparse_categorical_crossentropy') \n#               metrics =[keras.metrics.mae])\n    return model","6e976330":"model = create_model(X_train_df)","ba7b2a34":"history = model.fit(X_train, y_train, validation_data=(X_valid,y_valid), epochs=15, batch_size=1000)","1c7e9f55":"answer = model.predict(test_df)","f5c61b4f":"answer = answer.astype(int)","a386cb06":"y = np.argmax(answer, axis=-1)\ny","cc31591a":"frames_aswer = pd.DataFrame({\"ImageId\": pd.DataFrame(y).index.values+1,\n                             \"Label\": y\n                            })\nframes_aswer","f0f5093e":"Predict on **test** data","d32782ae":"Here we go) Let's create our semple recognition model.\nIn this kernel we use full dense custom model.\nYou may trying to get more efficient parameters) Doing it by yourself!","ec2f046f":"Let's read **data**:","bd91f4bc":"Now we can look in our data. Every pixel is encoded as int value in columns.","9eddee91":"There is an answer with ~ 0.81% rights!","3e6dbec3":"Starting train our model. Parameter **epochs** was picked up in trainings process."}}