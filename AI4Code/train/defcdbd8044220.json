{"cell_type":{"90b9d388":"code","2ddf4412":"code","a4ea73ca":"code","f69f3207":"code","65f7a86a":"code","62473419":"code","102c6f85":"code","c71ad043":"code","6a6f698b":"code","6c75c35f":"code","2f11f3f3":"code","624e306e":"code","4f612aba":"code","cfd92849":"code","dcbf2570":"code","0e60997e":"code","ca78681e":"code","e1a6c820":"code","1dd37174":"code","c7a1dbc0":"code","493de316":"code","d1425c7a":"code","a95611a4":"code","c518d0fb":"code","ed385481":"code","6d6606d8":"code","eb30b4cf":"code","e61f063e":"code","e633f781":"code","49f4ddc9":"code","8f3787ca":"code","029a85d9":"code","8c1e270f":"markdown","a529b13d":"markdown","1747c6d3":"markdown","ec32c82f":"markdown","b6d94cee":"markdown"},"source":{"90b9d388":"# !pip install opencv-python\n!pip install imutils\n!pip install dlib","2ddf4412":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","a4ea73ca":"import cv2,matplotlib.pyplot as plt,dlib,imutils\nfrom imutils import face_utils\n\ndetector = dlib.get_frontal_face_detector()\npredictor = dlib.shape_predictor(\"..\/input\/dlib-68\/shape_predictor_68_face_landmarks.dat\")","f69f3207":"image=plt.imread(\"..\/input\/recognizing-faces-in-the-wild\/train\/F0002\/MID1\/P00009_face3.jpg\")\n# image = imutils.resize(image, width=500)\ngray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\nrects = detector(gray, 1)\n\nfor rect in rects:\n    pred=predictor(gray,rect)\n    fig, ax1 = plt.subplots()\n\n    ax1.imshow(image)\n    ax1.scatter(face_utils.shape_to_np(pred)[:,0],face_utils.shape_to_np(pred)[:,1])\n    \n# del predictor","65f7a86a":"import random, itertools,glob\nclass Person:\n    def __init__(self,name,Family):\n        self.name=name\n        self.family=Family\n        self.related=set()\n        self.unrelated=set()","62473419":"relationlist=open(\"..\/input\/recognizing-faces-in-the-wild\/train_relationships.csv\").read().split(\"\\n\")[1:-1]\nFamilies={k.split(\"\/\")[0]:{} for k in relationlist}\nfor each in relationlist:\n    p1=each.split(\",\")[0].split(\"\/\")[1]\n    p2=each.split(\"\/\")[2]\n    Fam=each.split(\"\/\")[0]\n    Families[Fam].update({p1:Person(p1,Fam),p2:Person(p2,Fam)})\nFamilies\nfor Fam in Families:\n    Family=Families[Fam]\n    for Pers in Family:\n        Families[Fam][Pers].unrelated.update([k for k in set(Family.values()) if k.name!=Pers])\nfor relation in relationlist:\n    A,B=[Families[A.split(\"\/\")[0]][A.split(\"\/\")[1]] for A in relation.split(\",\")]\n    a,b=[r.split(\"\/\") for r in relation.split(\",\")]\n    Families[a[0]][a[1]].unrelated=A.unrelated - set([B])\n    Families[a[0]][a[1]].related=A.related   | set([B])\n    Families[b[0]][b[1]].unrelated=B.unrelated - set([A])\n    Families[b[0]][b[1]].related=B.related   | set([A])\n    \nfor F in Families:\n    for P in Families[F]:\n        if len(Families[F][P].unrelated)==0:\n            # For those that are fully related to those in the family, randomly choose 3 other people to be unrelated to\n            Families[F][P].unrelated= Families[F][P].unrelated | set([k for k in\n                                                                           [random.choice(list(random.choice(list(Families.values())).values())) for s in range(len(Families[F][P].related)+3)]\n                                                                          if k not in Families[F][P].related])\n        if len(Families[F][P].related)==0:\n            #ensure there are not any marooned individuals that are not related to anyone\n            print(\"Related\",F,P)\n        \n        Families[F][P].unrelated= Families[F][P].unrelated | set([random.choice(list(random.choice(list(Families.values())).values())) for s in range(3)])\n# Families","102c6f85":"train_data_parts=[]\n[[train_data_parts.append(Families[F][P]) for P in Families[F]] for F in Families]\ndel Families\ntrain_data_parts[:10]","c71ad043":"def metaglob(lis):\n    ret=[]\n    [ret.extend(glob.glob(\"..\/input\/recognizing-faces-in-the-wild\/train\/\"+A.family+\"\/\"+A.name+\"\/*.jpg\")) for A in lis]\n    return ret\n\npairs=[]\n[pairs.extend(itertools.product(*[metaglob([A]),\n                                  metaglob(A.related),\n                                  metaglob(A.unrelated)])) for A in train_data_parts]\ndel train_data_parts\nprint(\"Done\")","6a6f698b":"len(pairs)","6c75c35f":"from keras.utils.generic_utils import Progbar\n\ndef SixtyEight(image,k):\n    k.add(1)\n    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n    rects = detector(gray, 1)\n    for rect in rects:\n        pred=predictor(gray,rects[0])\n        re=face_utils.shape_to_np(pred)\n        re=(re-re.min(0))\/(re.max(0)-re.min(0))\n        return re\n    return None\n","2f11f3f3":"class callable_dict:\n    def __init__(self,fun):\n        self.dict=dict()\n        self.function=fun\n    def __getitem__(self, key):\n        if key in self.dict.keys():\n            return self.dict[key]\n        else:\n            self.dict[key]=self.function(plt.imread(key),Progbar(target=1, verbose=0))\n            return self.dict[key]","624e306e":"finalpair=pairs[::300]\n\ntrain=[[] for i in range(3)]\ns=Progbar(target=len(finalpair))\n\nmyDict=callable_dict(SixtyEight)\n\nfor p in finalpair:\n    s.add(1)\n    one,two,three=[myDict[x] for x in p]\n    if False not in [type(k)==np.ndarray for k in [one,two,three]]:\n        train[0].append(one)\n        train[1].append(two)\n        train[2].append(three)\n\ntrain=[np.array(k) for k in train]","4f612aba":"train[1].shape","cfd92849":"import tensorflow as tf\ndef triplet_loss(y_true, y_pred, alpha = 400,N=5):\n    \"\"\"\n    Implementation of the triplet loss function\n    Arguments:\n    y_true -- true labels, required when you define a loss in Keras, you don't need it in this function.\n    y_pred -- python list containing three objects:\n            anchor -- the encodings for the anchor data\n            positive -- the encodings for the positive data (similar to anchor)\n            negative -- the encodings for the negative data (different from anchor)\n    Returns:\n    loss -- real number, value of the loss\n    \"\"\"\n#     print((y_pred[0]))\n    N=y_pred.shape[1]\/\/3\n    anchor = y_pred[:,0:N]\n    positive = y_pred[:,N:N*2]\n    negative = y_pred[:,N*2:N*3]\n\n    # distance between the anchor and the positive\n    pos_dist = K.sqrt(K.sum(K.square(anchor-positive),axis=1)+.01)\n\n    # distance between the anchor and the negative\n    neg_dist = K.sqrt(K.sum(K.square(anchor-negative),axis=1)+.01)\n\n    # compute loss\n    basic_loss = (pos_dist-neg_dist+alpha)\n    loss = K.maximum(basic_loss,0.0)\n \n    return loss\n\ndef Neg_Dist(y_true, y_pred):\n    \"\"\"\n    Implementation of the triplet loss function\n    Arguments:\n    y_true -- true labels, required when you define a loss in Keras, you don't need it in this function.\n    y_pred -- python list containing three objects:\n            anchor -- the encodings for the anchor data\n            positive -- the encodings for the positive data (similar to anchor)\n            negative -- the encodings for the negative data (different from anchor)\n    Returns:\n    loss -- real number, value of the loss\n    \"\"\"\n#     print((y_pred[0]))\n    N=y_pred.shape[1]\/\/3\n    anchor = y_pred[:,0:N]\n    negative = y_pred[:,N*2:N*3]\n\n\n\n    # distance between the anchor and the negative\n    neg_dist = K.sum(K.square(anchor-negative),axis=1)\n\n \n    return neg_dist\n","dcbf2570":"from keras.models import Sequential, Model, Input\nfrom keras.layers import Dense, Dropout, Activation, Flatten, concatenate, Conv2D\nfrom keras.optimizers import Adagrad, Adam\nfrom keras.metrics import K","0e60997e":"pear=set(myDict.dict.keys())\n\ndef setup(pairs,pear):\n    ret=[]\n    prog=Progbar(2001)\n    while len(ret)<=2000 and pairs!=[]:\n        X=pairs.pop()\n        if len(set(X)-pear)==3:\n            prog.add(1)\n            ret.append(X)\n    return ret\n            \npear=setup(pairs,pear)","ca78681e":"testset=[[] for i in range(3)]\ns=Progbar(target=len(pear))\n\n\nfor X in pear:\n    s.add(1)\n    one,two,three=[myDict[x] for x in X]\n    if False not in [type(k)==np.ndarray for k in [one,two,three]]:\n        testset[0].append(one)\n        testset[1].append(two)\n        testset[2].append(three)\n\nholdout=[np.array(k) for k in testset]\n\ndel testset","e1a6c820":"def create_mod(inpu,outpu):\n    model= Sequential()\n    model.add(Dense(256, input_shape=(68,2)))\n    model.add(Flatten())\n    model.add(Dense(256))\n    model.add(Activation('relu'))\n    model.add(Dropout(rate=0.01))\n    model.add(Dense(256))\n    model.add(Activation('relu'))\n#     model.add(Dropout(rate=0.1))\n\n    model.add(Dense(outpu))\n    model.add(Activation(\"relu\"))\n    return model\n\nwith tf.device('\/device:GPU:1'):\n    anchor_in,pos_in,neg_in = Input(shape=(68,2)),Input(shape=(68,2)),Input(shape=(68,2))\n\n    mod=create_mod(224,100)\n\n    anchor_out=mod(anchor_in)\n    pos_out=mod(pos_in)\n    neg_out=mod(neg_in)\n\n    merged= concatenate([anchor_out,pos_out,neg_out], axis=-1)\n\n    model=Model(inputs=[anchor_in,pos_in,neg_in],outputs=merged)\n\n    model.compile(loss=triplet_loss,optimizer=Adam())\n\nmodel.fit(train,np.zeros(train[0].shape[0]),batch_size=10,epochs=2,validation_data=(holdout,np.zeros(holdout[0].shape[0])))","1dd37174":"pre=model.predict([train[0],train[1],train[2]])","c7a1dbc0":"    N=pre.shape[1]\/\/3\n    anchor = pre[:,0:N]\n    positive = pre[:,N:N*2]\n    negative = pre[:,N*2:N*3]","493de316":"np.sqrt(np.square(anchor-positive).sum(1)).mean(),np.sqrt(np.square(anchor-negative).sum(1)).mean()","d1425c7a":"p=pre.reshape((train[1].shape[0],3,N))\nHO=model.predict(holdout).reshape((holdout[1].shape[0],3,N))","a95611a4":"np.linalg.norm(p[400,0]-p[0,0],2)\n","c518d0fb":"p[0,0].shape","ed385481":"test=list(pd.read_csv(\"..\/input\/recognizing-faces-in-the-wild\/sample_submission.csv\")[\"img_pair\"])\ntests=set()\nfor k in test:\n    tests=tests | set(k.split(\"-\")) \nsorted(tests)\nk=Progbar(len(tests))\ncomp={fil:SixtyEight(plt.imread(\"..\/input\/recognizing-faces-in-the-wild\/test\/\"+fil),k) for fil in tests}\n# comp","6d6606d8":"test1=[]\ntest2=[]\nkeys=[]\nfor face in test:\n    one=comp[face.split(\"-\")[0]]\n    two=comp[face.split(\"-\")[1]]\n    if type(one) == np.ndarray and type(two) == np.ndarray:\n        test1.append(one)\n        test2.append(two)\n        keys.append(face)\ntest1=np.array(test1)\ntest2=np.array(test2)","eb30b4cf":"te=model.predict([test1,test1,test2])","e61f063e":"test1.shape\no=te.reshape((test1.shape[0],3,N))\nnp.array([[np.linalg.norm(k[0]-k[1],2),np.linalg.norm(k[0]-k[2],2)] for k in o]).mean(0)","e633f781":"kk=np.array([[np.linalg.norm(k[0]-k[1],2),np.linalg.norm(k[0]-k[2],2)] for k in p])\nplt.hist(kk,bins=20)\nplt.legend([\"Related\",\"Unrelated\"])\nplt.title(\"Training Data Distributions\")","49f4ddc9":"kk=np.array([[np.linalg.norm(k[0]-k[1],2),np.linalg.norm(k[0]-k[2],2)] for k in HO])\nplt.hist(kk,bins=20)\nplt.legend([\"Related\",\"Unrelated\"])\nplt.title(\"Holdout Data Distributions\")","8f3787ca":"kk=np.array([[np.linalg.norm(k[0]-k[1],2),np.linalg.norm(k[0]-k[2],2)] for k in o])[:,1]\nplt.hist(kk,bins=20,color=\"k\")","029a85d9":"ppd=pd.DataFrame({\"img_pair\":keys,\"is_related\":1-(kk-kk.min())\/(kk.max()-kk.min())})\nppd=ppd.append(pd.DataFrame({\"img_pair\":list(set(test)-set(keys)),\"is_related\":[1-(kk.mean()-kk.min())\/(kk.max()-kk.min()) for i in range(len(list(set(test)-set(keys))))]}))\nppd.to_csv(\"submission.csv\",index=False,header=True)","8c1e270f":"# An implimentation of Facial landmark Detection using the trained data from iBUG's 68 landmark model\n[Code Curtesy of Adrian Rosebrock's pyimage blog](https:\/\/www.pyimagesearch.com\/2017\/04\/03\/facial-landmarks-dlib-opencv-python\/)\nThis model identifies 68 landmarks around the face; 17 defining the shape of the jaw, 10 for the brows (5 each), 4 along the bridge of the nose, 5 along the tip and base of the nose, 12 for eye shape (6 each), 12 for the outer lip outline, 8 for the inside lip outline\n![](https:\/\/www.pyimagesearch.com\/wp-content\/uploads\/2017\/04\/facial_landmarks_68markup-768x619.jpg)\nThe results of the model are 68 (x,y) points for each given face, 136 data points overall","a529b13d":"# Triplet loss biometric loss function\n[Curtousey of Towards DataScience](https:\/\/towardsdatascience.com\/lossless-triplet-loss-7e932f990b24)","1747c6d3":">![Demonstration](https:\/\/cdn-images-1.medium.com\/max\/800\/0*_WNBFcRVEOz6QM7R.)","ec32c82f":"*OOFF* 65-75 Million photo pairs, probably excessive, lets go with some subset for now","b6d94cee":"# Referential Association of relationships\n> ~~An assumption is made that for triplett loss, simply using the un-related family members is good enough to use as negative cases, if this shows to be a bad assumption, changes can be made~~\n\n> Revision: the model had troubles coping with the trasitive non-equivelance (Person A related to B, B related to C, but C not related to A) as the only things to train on;added 3 random people to each person as unrelated persons and the model now trains nicely"}}