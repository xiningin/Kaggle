{"cell_type":{"17a51ef3":"code","e115ad2e":"code","ee7e7bdc":"code","642bb42b":"code","9a82c29c":"code","f961df62":"code","78554bc1":"code","367f4287":"code","695c42b5":"code","ffb4f389":"code","ccf00406":"code","d9cbf841":"code","4e05746b":"code","aee9eb81":"code","16e2c95d":"code","9f19efce":"code","96fd35f9":"code","9c8a0967":"code","c126c8f6":"code","6a8d3979":"code","d4b3f019":"code","542520e3":"code","7372ffd2":"code","f881d090":"code","074305b3":"code","4e7203a7":"code","0b882b6b":"code","a8f1bd10":"code","30b855ca":"code","b907575f":"code","79c6e899":"code","2be130ac":"code","fbba3de8":"code","2b4f6f90":"code","2bc59b66":"code","8d8e5d34":"code","a47c5048":"code","b8f5a924":"code","bc6eab3b":"code","07387e98":"code","9930aeb3":"markdown","80956ef3":"markdown","f2a97e5c":"markdown","19d44c93":"markdown","14f9d4bb":"markdown","d1218a79":"markdown","1aab7b80":"markdown","e1a4c75e":"markdown","fe4245d4":"markdown","c278052c":"markdown","816d23aa":"markdown","cf631ad7":"markdown","edf7d18b":"markdown","f45b5fbc":"markdown","304f5f87":"markdown","3a23f91f":"markdown","556f1268":"markdown","bd385ad0":"markdown"},"source":{"17a51ef3":"import pandas as pd\nimport numpy as np \nimport seaborn as sns \nimport matplotlib.pyplot as plt\n\nimport category_encoders as ce\n\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.model_selection import train_test_split\n\nimport warnings\n\nwarnings.filterwarnings('ignore')\n\n%matplotlib inline","e115ad2e":"data = pd.read_csv('..\/input\/adult-dataset\/adult.csv')","ee7e7bdc":"data.head() ","642bb42b":"data.columns = ['age', 'workclass', 'fnlwgt', 'education', 'never_married', 'marital_status', 'occupation', 'relationship',\n             'race', 'sex', 'capital_gain', 'capital_loss', 'hours_per_week', 'native_country', 'income']\n\ndata.tail() ","9a82c29c":"data.income = pd.get_dummies(data.income)[' >50K']","f961df62":"data.tail()","78554bc1":"data.info() ","367f4287":"categorical_names = []\nfor feature in data.columns: \n    if data[feature].dtype == object: \n        categorical_names.append(feature)\ncategorical_names","695c42b5":"data[categorical_names].head() ","ffb4f389":"data[categorical_names].isnull().any()","ccf00406":"data[categorical_names].isna().any()","d9cbf841":"for feature in data[categorical_names].columns:\n    print('FEATURE NAME:', feature)\n    print(data[feature].value_counts())\n    ","4e05746b":"for feature in data.columns:\n    data[feature].replace(' ?', np.nan, inplace=True)","aee9eb81":"# check this, \n\ndata[data.occupation == ' ?']","16e2c95d":"data.native_country.value_counts()","9f19efce":"data[categorical_names].isnull().any() ","96fd35f9":"plt.figure(figsize=(10,6))\nsns.heatmap(data[categorical_names].isnull(), yticklabels=False, cbar=False, cmap='viridis')\nplt.show() ","9c8a0967":"numerical_features = [var for var in data.columns if data[var].dtype!='O']\n\ndata[numerical_features].head() ","c126c8f6":"data[numerical_features].isnull().any()","6a8d3979":"data[categorical_names].isnull().mean()","d4b3f019":"# The mode of a set of values is the value that appears most often. It can be multiple values.\ndata.workclass.mode()","542520e3":"data.workclass.value_counts()","7372ffd2":"na_colls = data.isnull().any().loc[data.isnull().any().values == True].index\nna_colls","f881d090":"for i in na_colls:\n    data[i].fillna(data[i].mode()[0], inplace=True)\n\ndata.isnull().any() ","074305b3":"plt.figure(figsize=(10,6))\nsns.heatmap(data[categorical_names].isnull(), yticklabels=False, cbar=False, cmap='viridis')\nplt.show() ","4e7203a7":"data[categorical_names].head() ","0b882b6b":"for i in categorical_names: \n    print(str.upper(i), data[i].value_counts().shape[0]) ","a8f1bd10":"categorical_names_withoutone = categorical_names\ncategorical_names_withoutone.remove('native_country')\nencoder = ce.OneHotEncoder(cols=categorical_names_withoutone)\n\ndata_encoded = encoder.fit_transform(data)\n\ndata_encoded.head()","30b855ca":"print('native_country has got', data.native_country.value_counts().shape[0], 'features.')","b907575f":"mean_encoded_nativeCont = data_encoded.groupby(['native_country'])['income'].mean().to_dict() \ndata_encoded.native_country = data_encoded.native_country.map(mean_encoded_nativeCont)","79c6e899":"data_encoded.native_country","2be130ac":"target = data_encoded.income \nfeatures = data_encoded.drop('income', axis=1) \n\nX_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.33)\n\ngnb = GaussianNB() \ngnb.fit(X_train, y_train)","fbba3de8":"prediction = gnb.predict(X_test)\n\nprediction","2b4f6f90":"correct = (y_test == prediction).sum() \nprint('classified correctly', correct) \nwrong = X_test.shape[0] - correct \nprint('classified incorretly', wrong)","2bc59b66":"print('The Accuracy is', correct \/ X_test.shape[0])","8d8e5d34":"prediction_train = gnb.predict(X_train)\ncorrect_train = (y_train == prediction_train).sum()\nprint('classified correctly in train set', correct_train) \nwrong_train = X_train.shape[0] - correct_train\nprint('classified incorrectly in train set', wrong_train)","a47c5048":"print('The accuracy for train set is', correct_train \/ X_train.shape[0])","b8f5a924":"# chart styling info \n\nyaxis_label = '>50K'\nxaxis_label = '<=50K'","bc6eab3b":"log_probabilities = gnb.predict_proba(X_test)\nprob0 = log_probabilities[:,0]\nprob1 = log_probabilities[:,1]\n\nsummary_df = pd.DataFrame({yaxis_label: prob0, xaxis_label: prob1, 'labels':y_test})\nsummary_df","07387e98":"sns.lmplot(x=xaxis_label, y=yaxis_label, data=summary_df, height=6.5, fit_reg=False, legend=False,\n          scatter_kws={'alpha': 0.5, 's': 25}, hue='labels', markers=['o', 'x'], palette='hls')\n\n\n\nplt.show()","9930aeb3":"### Impute missing categorical variables with most frequent value","80956ef3":"# 2. Notebook Imports","f2a97e5c":"## Explore Categorical Variables","19d44c93":"### What is Naive? Why is it naive? \n\nIt is naive because it ignores all of the dependencies. It assumes event are independent. Features does not affect each othet. \nLet me explaing it an example. For a spam classifier, our equations would be like abowe. \n\n$$P(Spam \\, | \\, Word) = \\frac{P(Word \\, | \\, Spam) \\, P(Spam)} {P(Word)}$$ \n\nSo, for a sample sentence, \"we are good.\". It would be like abowe. \n\n$$ \\frac{P(We \\, | \\, Spam) \\, P(Spam)} {P(We)} x  \\frac{P(are \\, | \\, Spam) \\, P(Spam)} {P(are)} x \\frac{P(Good \\, | \\, Spam) \\, P(Spam)} {P(Good)}$$ ","14f9d4bb":"### Check for overfitting and underfitting\n\nThe training-set accuracy score is 0.7957827 while the test-set accuracy to be  0.79311 So, there is no sign of overfitting.","d1218a79":"# 1. Introduction the Naive Bayes Classifier \n\nNaive Bayes Classifier uses the Bayes\u2019 theorem to predict probabilities for each class such as the probability that given record or data point belongs to a particular class. \n\nIt can be used for; \n\n* Text classification\n* Sentiment analysis\n* Spam filtering\n* Recommender systems","1aab7b80":"# Metrics and Evaluation\n\n## Accuracy","e1a4c75e":"Let's see it with a plot. ","fe4245d4":"You can see abowe there is some missing values in our seperated dataframes. But pandas' methods like isna() or isnull() couldn't detect them because of their value. It's coded as a ?. \n\nIn this case, we are going to replace them with nan values and we visualize them. ","c278052c":"## Explore Numerical Variables","816d23aa":"We are going to use the method called One Hot Encoding. One hot encoding is the most widespread approach, and it works very well unless your categorical variable takes on a large number of values. \nOne hot encoding creates new (binary) columns, indicating the presence of each possible value from the original data. \n\n* In our data set, there is just one column that may be a problem for this technique. NATIVE_COUNTRY has got 41 different categories. It's not convenient for this method.","cf631ad7":"# Creation Model","edf7d18b":"# Encoding Categorial Variables ","f45b5fbc":"I am going to use the Mean Encoding Method. \nMean encoding represents a probability of your target variable, conditional on each value of the feature.\nLet's see this. ","304f5f87":"# 3. Import Dataset","3a23f91f":"## Visualising the Results","556f1268":"# Naive Bayes Classifier \n\nIn this work, we're going to cover lots of things about Naive Bayes Classifier. I implement our algorithm with Scikit-Learn. ","bd385ad0":"In this dataset we have numeric and categorical features. For numeric features, it's ok. There is no problem with them. But we have to analyze our categorical variables and we do some encoding for them. \n\nTo do this, let's take them."}}