{"cell_type":{"342b9719":"code","b4c76929":"code","ea00596d":"code","a9651755":"code","1151b7ef":"code","060de0b3":"code","5e659e00":"code","24c47c90":"code","c759beec":"code","1bf05036":"code","e185a312":"code","b985a6b6":"code","9fc68fc5":"code","fa8e56f5":"code","477724df":"code","28e0bcac":"code","cea3db5d":"code","8536d4f1":"code","63abdbe4":"code","c0892115":"code","76b60e01":"code","a99d97d6":"code","552fde5b":"markdown","9d463132":"markdown","3634fa47":"markdown","a92368fb":"markdown"},"source":{"342b9719":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport lightgbm as lgb\nfrom sklearn.model_selection import KFold\nfrom sklearn import model_selection, preprocessing, metrics\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.metrics import roc_auc_score\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\nimport shap\nimport os\nprint(os.listdir(\"..\/input\"))\nfrom sklearn import preprocessing\nimport xgboost as xgb\nimport gc\n\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","b4c76929":"%%time\ntrain = pd.read_csv('..\/input\/simple-linear-regression-benchmark\/new_train.csv')\ntest = pd.read_csv('..\/input\/simple-linear-regression-benchmark\/new_test.csv')","ea00596d":"train.shape","a9651755":"columns = train.columns\ncolumns","1151b7ef":"train.head()","060de0b3":"train.dtypes","5e659e00":"test.shape","24c47c90":"train = train.sample(frac=0.1, random_state=3)\ntest = test.sample(frac=0.05, random_state=3)\n\n#train = train.sample(frac=0.2, random_state=3)\n#test = test.sample(frac=0.1, random_state=3)\ngc.collect()","c759beec":"train['target'] = 0\ntest['target'] = 1","1bf05036":"train_test = pd.concat([train, test], axis =0)\n\n\n\n#imp = SimpleImputer(missing_values=np.nan, strategy='mean')\n#imp.fit(train_test[columns])\n#train_test[columns] = imp.transform(train_test[columns])\ntarget = train_test['target'].values\n\n\ndel train, test\ngc.collect()","e185a312":"train, test = model_selection.train_test_split(train_test, test_size=0.33, random_state=42, shuffle=True)\ndel train_test\ngc.collect()","b985a6b6":"train['target']","9fc68fc5":"#train_y = train['target'].values.reshape(-1, 1)\n#test_y = test['target'].values.reshape(-1,1)\ntrain_y = train['target'].values\ntest_y = test['target'].values\ndel train['target'], test['target']\ngc.collect()","fa8e56f5":"train = lgb.Dataset(train, label=train_y)\ntest = lgb.Dataset(test, label=test_y)\ngc.collect()","477724df":"#train = train.values\n#test = test.values\ngc.collect()","28e0bcac":"param = {'num_leaves': 50,\n         'min_data_in_leaf': 30, \n         'objective':'binary',\n         'max_depth': 2,\n         'learning_rate': 0.2,\n         \"min_child_samples\": 20,\n         \"boosting\": \"gbdt\",\n         \"feature_fraction\": 0.9,\n         \"bagging_freq\": 1,\n         \"bagging_fraction\": 0.9 ,\n         \"bagging_seed\": 44,\n         \"metric\": 'auc',\n         \"verbosity\": -1}","cea3db5d":"num_round = 50\nclf = lgb.train(param, train, num_round, valid_sets = [train, test], verbose_eval=50, early_stopping_rounds = 50)","8536d4f1":"#clf = LogisticRegression()\n#clf.fit(train, train_y)","63abdbe4":"#preds[:,0].shape","c0892115":"#test_y.flatten().shape","76b60e01":"#roc_auc_score(test_y.flatten(), preds[:,1])","a99d97d6":"feature_imp = pd.DataFrame(sorted(zip(clf.feature_importance(),columns)), columns=['Value','Feature'])\n\nplt.figure(figsize=(20, 10))\nsns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp.sort_values(by=\"Value\", ascending=False).head(20))\nplt.title('LightGBM Features')\nplt.tight_layout()\nplt.show()\nplt.savefig('lgbm_importances-01.png')\n","552fde5b":"AUC of 0.56 reveals some dicrepancy between the train and test sets. Let's take a look at the feature importances:","9d463132":"In this notebook we'll try to use adversarial validation in order to see how similar\/different the train and test sets are. Since we have to merge several datasets in order to get \"trainable\" data, we'll rely on other kernesl in which this has already been don. This starter version of Adversarial Validation will rely on my [Simple Linear Regression Benchmark kernel](https:\/\/www.kaggle.com\/tunguz\/simple-linear-regression-benchmark\/) for train and test sets. ","3634fa47":"Looks like Dew Temperature is the most distinct feature between the train and test datasets.","a92368fb":"AUC of 0.52 is actually pretty decent. Seems that at least at this level of analysis there is not much difference between the train and test sets. "}}