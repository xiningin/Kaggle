{"cell_type":{"f3dd84d9":"code","8fa86658":"code","f216befd":"code","ed4bb604":"code","a6cb794d":"code","28a780a1":"code","a994140a":"code","cd373a3d":"code","061e3adf":"code","38af9b87":"code","d51d56cc":"code","fc407518":"code","5d54fc0f":"code","5942c84d":"code","d6714a90":"code","268c2124":"code","45e7e952":"code","f29ec94c":"code","0641a263":"code","4fa58d6d":"code","efc6ae7e":"code","ef97e026":"code","aa5bf794":"code","c4b1cdc9":"code","b0256438":"code","11cd1a95":"code","ffe6923c":"code","946dda6c":"code","cfaeb941":"code","069adf74":"code","ef027852":"code","21e83926":"code","ae93d506":"code","72ad20db":"code","3693373f":"code","68ba552d":"code","8ab4dca8":"code","6580ab42":"code","3d48839d":"code","022ea62f":"code","747cc4bd":"code","b14f0412":"code","bb4c2b12":"code","c87ff491":"code","d0bf3772":"code","062d60c4":"code","23c0e6d5":"code","ad76e4f3":"code","5e7316fd":"code","5f3a2b6d":"code","b244ee10":"code","4d751455":"code","de593e3b":"code","1f8fdbee":"code","1d9be7c7":"code","ac2891ea":"code","e4a20d71":"code","e437415c":"code","10887644":"code","fbb47cdc":"code","a64c010d":"code","f715c4ff":"code","49e6d30b":"code","553bb1a6":"code","6daa8ae2":"code","6a3c01a4":"code","d0969db8":"code","54d97333":"code","2972b4a5":"markdown","4ceb1751":"markdown","f9611c91":"markdown","72eb647e":"markdown","a0295367":"markdown","3b4b54c3":"markdown","a74d081f":"markdown","23d01564":"markdown","82a97ab9":"markdown","0672a5ae":"markdown","0a9d56a1":"markdown","767c96c9":"markdown","54a60ef3":"markdown","118d2df1":"markdown","ea901860":"markdown","43903996":"markdown","01156b37":"markdown","214658a3":"markdown","f669377d":"markdown","0bb1e9ce":"markdown","d20eda8f":"markdown","37d9f266":"markdown","647cdbc5":"markdown","0a756ede":"markdown","cf73f6bd":"markdown","95c023f5":"markdown","45862ce8":"markdown","175db182":"markdown","43cc96c9":"markdown","663f4468":"markdown","604dde6d":"markdown","7b602e5f":"markdown","50f4000d":"markdown","bdd6c4d9":"markdown","c5ae070b":"markdown","2515056f":"markdown","3fe40345":"markdown","68ee9688":"markdown","6c9d4b70":"markdown","a9c48815":"markdown","c1db8514":"markdown","4b28c4b7":"markdown","a541d587":"markdown","55c3132c":"markdown","d9a0b8bc":"markdown","75313436":"markdown","def6f680":"markdown","88f16798":"markdown","3b7fecb4":"markdown","c6615792":"markdown","87a1e4a4":"markdown","67211a05":"markdown","f7534c39":"markdown","497b287a":"markdown","a30da16e":"markdown","d1445c14":"markdown","4488d153":"markdown"},"source":{"f3dd84d9":"            # This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","8fa86658":"# No warnings about setting value on copy of slice\npd.options.mode.chained_assignment = None\npd.set_option('display.max_columns', None)  \npd.set_option('display.expand_frame_repr', False)\npd.set_option('max_colwidth', -1)\n\n# Matplotlib visualization\nimport matplotlib.pyplot as plt\n%matplotlib inline\nplt.style.use(\"fivethirtyeight\")\n\n# Set default font size and facecolor\nplt.rcParams[\"font.size\"] = 24\nplt.rcParams[\"figure.facecolor\"] = \"white\"\nplt.rcParams[\"axes.facecolor\"] = \"white\"\n\n# Internal ipython tool for setting figure size\nfrom IPython.core.pylabtools import figsize\nfigsize(15, 12)\n\n# Seaborn for visualization\nimport seaborn as sns","f216befd":"data = pd.read_csv(\"..\/input\/Video_Games_Sales_as_at_22_Dec_2016.csv\")\ndata.info()","ed4bb604":"data = data.rename(columns={\"Year_of_Release\": \"Year\", \n                            \"NA_Sales\": \"NA\",\n                            \"EU_Sales\": \"EU\",\n                            \"JP_Sales\": \"JP\",\n                            \"Other_Sales\": \"Other\",\n                            \"Global_Sales\": \"Global\"})\ndata = data[data[\"Year\"].notnull()]\ndata = data[data[\"Genre\"].notnull()]\ndata[\"Year\"] = data[\"Year\"].apply(int)\ndata[\"Age\"] = 2018 - data[\"Year\"]\ndata.describe(include=\"all\")","a6cb794d":"# Histogram plot of Year of release\nnum_years = data[\"Year\"].max() - data[\"Year\"].min() + 1\nplt.hist(data[\"Year\"], bins=num_years, color=\"lightskyblue\", edgecolor=\"black\")\nplt.title(\"Distribution of year of release\")\nplt.xlabel(\"Year\")\nplt.ylabel(\"Number of games\");","28a780a1":"# Replacing \"tbd\" values with np.nan and transforming column to float type\ndata[\"User_Score\"] = data[\"User_Score\"].replace(\"tbd\", np.nan).astype(float)\n\ng = sns.jointplot(x=\"User_Score\", y=\"Critic_Score\", data=data, cmap=\"Blues\", kind=\"hex\", \n                  size=10, marginal_kws={\"hist_kws\" : {\"edgecolor\": \"black\", \"color\": \"lightskyblue\", \"alpha\": 1}}, \n                  annot_kws={\"loc\": 4, \"fontsize\": 18});\ng.ax_marg_x.grid(False)\ng.ax_marg_y.grid(False);","a994140a":"# Function to calculate missing values by column\ndef missing_values_table(df):\n        # Total missing values\n        mis_val = df.isnull().sum()\n        \n        # Percentage of missing values\n        mis_val_percent = 100 * df.isnull().sum() \/ len(df)\n        \n        # Make a table with the results\n        mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\n        \n        # Rename the columns\n        mis_val_table_ren_columns = mis_val_table.rename(\n        columns = {0 : \"Missing Values\", 1 : \"% of Total Values\"})\n        \n        # Sort the table by percentage of missing descending\n        mis_val_table_ren_columns = mis_val_table_ren_columns[\n            mis_val_table_ren_columns.iloc[:,1] != 0].sort_values(\n        \"% of Total Values\", ascending=False).round(1)\n        \n        # Print some summary information\n        print (\"Your selected dataframe has \" + str(df.shape[1]) + \" columns.\\n\"      \n            \"There are \" + str(mis_val_table_ren_columns.shape[0]) +\n              \" columns that have missing values.\")\n        \n        # Return the dataframe with missing information\n        return mis_val_table_ren_columns","cd373a3d":"missing_values_table(data)","061e3adf":"def rm_outliers(df, list_of_keys):\n    df_out = df\n    for key in list_of_keys:\n        # Calculate first and third quartile\n        first_quartile = df_out[key].describe()[\"25%\"]\n        third_quartile = df_out[key].describe()[\"75%\"]\n\n        # Interquartile range\n        iqr = third_quartile - first_quartile\n\n        # Remove outliers\n        removed = df_out[(df_out[key] <= (first_quartile - 3 * iqr)) |\n                    (df_out[key] >= (third_quartile + 3 * iqr))] \n        df_out = df_out[(df_out[key] > (first_quartile - 3 * iqr)) &\n                    (df_out[key] < (third_quartile + 3 * iqr))]\n    return df_out, removed","38af9b87":"data, rmvd_global = rm_outliers(data, [\"Global\"])\ndata.describe()","d51d56cc":"data[\"Has_Score\"] = data[\"User_Score\"].notnull() & data[\"Critic_Score\"].notnull()\nrmvd_global[\"Has_Score\"] = rmvd_global[\"User_Score\"].notnull() & rmvd_global[\"Critic_Score\"].notnull()","fc407518":"from matplotlib.lines import Line2D\nplt.hist(data[data[\"Has_Score\"]==True][\"Year\"], color=\"limegreen\", alpha=0.5, \n         bins=range(1980, 2021), edgecolor=\"black\")\nplt.hist(data[data[\"Has_Score\"]==False][\"Year\"], color=\"indianred\", alpha=0.5, \n         bins=range(1980, 2021), edgecolor=\"black\")\nplt.title(\"Distribution of year of release\")\nplt.xlabel(\"Year of release\")\nplt.ylabel(\"Number of games\")\nplt.legend(handles=[Line2D([0], [0], color=\"limegreen\", lw=20, label=\"True\", alpha=0.5),\n                    Line2D([0], [0], color=\"indianred\", lw=20, label=\"False\", alpha=0.5)],\n           title=\"Has_Score\", loc=6);","5d54fc0f":"plt.hist(data[data[\"Has_Score\"]==True][\"Global\"], color=\"limegreen\", alpha=0.5, \n         edgecolor=\"black\")\nplt.hist(data[data[\"Has_Score\"]==False][\"Global\"], color=\"indianred\", alpha=0.5, \n         edgecolor=\"black\")\nplt.title(\"Distribution of global sales\")\nplt.xlabel(\"Global sales, $M\")\nplt.ylabel(\"Number of games\")\nplt.legend(handles=[Line2D([0], [0], color=\"limegreen\", lw=20, label=\"True\", alpha=0.5),\n                    Line2D([0], [0], color=\"indianred\", lw=20, label=\"False\", alpha=0.5)],\n           title=\"Has_Score\", loc=7);","5942c84d":"data[\"Country\"] = data[[\"NA\", \"EU\", \"JP\", \"Other\"]].idxmax(1, skipna=True)\npalette = {True: \"limegreen\", False: \"indianred\"}\nsns.factorplot(y=\"Country\", hue=\"Has_Score\", data=data, size=8, kind=\"count\", palette=palette)\nsns.factorplot(y=\"Country\", x=\"Global\", hue=\"Has_Score\", data=data, size=8, kind=\"bar\", palette=palette,\n               estimator=lambda x: np.median(x));","d6714a90":"scored = data.dropna(subset=[\"User_Score\", \"Critic_Score\", \"Rating\"])\nscored.describe()","268c2124":"scored, rmvd_user_count = rm_outliers(scored, [\"User_Count\"])\nscored.describe()","45e7e952":"scored[\"Platform\"].unique(), scored[\"Genre\"].unique(), scored[\"Rating\"].unique()","f29ec94c":"import category_encoders as ce\n# Select the numeric columns\nnumeric_subset = scored.select_dtypes(\"number\").drop(columns=[\"NA\", \"EU\", \"JP\", \"Other\", \"Year\"])\n\n# Select the categorical column\ncategorical_subset = scored[[\"Platform\", \"Genre\", \"Rating\"]]\n\n# One hot encode\nencoder = ce.one_hot.OneHotEncoder()\ncategorical_subset = encoder.fit_transform(categorical_subset)\n\n# Join the two dataframes using concat\n# Make sure to use axis = 1 to perform a column bind\nfeatures = pd.concat([numeric_subset, categorical_subset], axis = 1)\n\n# Find correlations with the score \ncorrelations = features.corr()[\"Global\"].dropna().sort_values()","0641a263":"correlations.head(5)","4fa58d6d":"correlations.tail(5)","efc6ae7e":"# Extract the columns to  plot\nplot_data = features[[\"Global\", \"Critic_Score\", \"User_Score\",\n                      \"Critic_Count\", \"User_Count\"]]\n\n# Function to calculate correlation coefficient between two columns\ndef corr_func(x, y, **kwargs):\n    r = np.corrcoef(x, y)[0][1]\n    ax = plt.gca()\n    ax.annotate(\"r = {:.2f}\".format(r),\n                xy=(.2, .8), xycoords=ax.transAxes,\n                size = 20)\n    \n# Create the pairgrid object\ngrid = sns.PairGrid(data = plot_data, size = 3)\n\n# Upper is a scatter plot\ngrid.map_upper(plt.scatter, color = \"lightskyblue\", alpha = 0.6, marker=\".\", s=10)\n\n# Diagonal is a histogram\ngrid.map_diag(plt.hist, color = \"lightskyblue\", edgecolor=\"black\")\n\n# Bottom is correlation and density plot\ngrid.map_lower(corr_func)\ngrid.map_lower(sns.kdeplot, cmap = plt.cm.Blues)\n\n# Title for entire plot\nplt.suptitle(\"Pairs Plot of Game Scores\", size = 36, y = 1.02);","ef97e026":"features.shape","aa5bf794":"from sklearn.model_selection import train_test_split\nbasic_target = pd.Series(features[\"Global\"])\nbasic_features = features.drop(columns=\"Global\")\nfeatures_train, features_test, target_train, target_test = train_test_split(basic_features, basic_target, \n                                                                            test_size=0.2,\n                                                                            random_state=42)\nprint(features_train.shape)\nprint(features_test.shape)\nprint(target_train.shape)\nprint(target_test.shape)","c4b1cdc9":"def mae(y_true, y_pred):\n    return np.average(abs(y_true - y_pred))","b0256438":"baseline_guess = np.median(target_train)\nbasic_baseline_mae = mae(target_test, baseline_guess)\nprint(\"Baseline guess for global sales is: {:.02f}\".format(baseline_guess))\nprint(\"Baseline Performance on the test set: MAE = {:.04f}\".format(basic_baseline_mae))","11cd1a95":"from sklearn.linear_model import LinearRegression, Ridge\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.neighbors import KNeighborsRegressor","ffe6923c":"def fit_and_evaluate(model):\n    \n    # Train the model\n    model.fit(features_train, target_train)\n    \n    # Make predictions and evalute\n    model_pred = model.predict(features_test)\n    model_mae = mae(target_test, model_pred)\n    \n    # Return the performance metric\n    return model_mae","946dda6c":"lr = LinearRegression()\nlr_mae = fit_and_evaluate(lr)\n\nprint(\"Linear Regression Performance on the test set: MAE = {:.04f}\".format(lr_mae))","cfaeb941":"svm = SVR(C = 1000, gamma=0.1)\nsvm_mae = fit_and_evaluate(svm)\n\nprint(\"Support Vector Machine Regression Performance on the test set: MAE = {:.04f}\".format(svm_mae))","069adf74":"random_forest = RandomForestRegressor(random_state=60)\nrandom_forest_mae = fit_and_evaluate(random_forest)\n\nprint(\"Random Forest Regression Performance on the test set: MAE = {:.04f}\".format(random_forest_mae))","ef027852":"gradient_boosting = GradientBoostingRegressor(random_state=60)\ngradient_boosting_mae = fit_and_evaluate(gradient_boosting)\n\nprint(\"Gradient Boosting Regression Performance on the test set: MAE = {:.04f}\".format(gradient_boosting_mae))","21e83926":"knn = KNeighborsRegressor(n_neighbors=10)\nknn_mae = fit_and_evaluate(knn)\n\nprint(\"K-Nearest Neighbors Regression Performance on the test set: MAE = {:.04f}\".format(knn_mae))","ae93d506":"ridge = Ridge(alpha=10)\nridge_mae = fit_and_evaluate(ridge)\n\nprint(\"Ridge Regression Performance on the test set: MAE = {:.04f}\".format(ridge_mae))","72ad20db":"model_comparison = pd.DataFrame({\"model\": [\"Linear Regression\", \"Support Vector Machine\",\n                                           \"Random Forest\", \"Gradient Boosting\",\n                                            \"K-Nearest Neighbors\", \"Baseline (median)\", \"Ridge\"],\n                                 \"mae\": [lr_mae, svm_mae, random_forest_mae, \n                                         gradient_boosting_mae, knn_mae, basic_baseline_mae, ridge_mae]})\nmodel_comparison.sort_values(\"mae\", ascending=False).plot(x=\"model\", y=\"mae\", kind=\"barh\",\n                                                           color=\"lightskyblue\", legend=False)\nplt.ylabel(\"\"); plt.yticks(size=14); plt.xlabel(\"Mean Absolute Error\"); plt.xticks(size=14)\nplt.title(\"Model Comparison on Test MAE\", size=20);","3693373f":"# Loss function to be optimized\nloss = [\"ls\", \"lad\", \"huber\"]\n\n# Maximum depth of each tree\nmax_depth = [2, 3, 5, 10, 15]\n\n# Minimum number of samples per leaf\nmin_samples_leaf = [1, 2, 4, 6, 8]\n\n# Minimum number of samples to split a node\nmin_samples_split = [2, 4, 6, 10]\n\n# Maximum number of features to consider for making splits\nmax_features = [\"auto\", \"sqrt\", \"log2\", None]\n\nhyperparameter_grid = {\"loss\": loss,\n                       \"max_depth\": max_depth,\n                       \"min_samples_leaf\": min_samples_leaf,\n                       \"min_samples_split\": min_samples_split,\n                       \"max_features\": max_features}","68ba552d":"from sklearn.model_selection import RandomizedSearchCV\nbasic_model = GradientBoostingRegressor(random_state = 42)\n\nrandom_cv = RandomizedSearchCV(estimator=basic_model,\n                               param_distributions=hyperparameter_grid,\n                               cv=4, n_iter=20, \n                               scoring=\"neg_mean_absolute_error\",\n                               n_jobs=-1, verbose=1, \n                               return_train_score=True,\n                               random_state=42)","8ab4dca8":"random_cv.fit(features_train, target_train)","6580ab42":"random_results = pd.DataFrame(random_cv.cv_results_).sort_values(\"mean_test_score\", ascending=False)\nrandom_results.head(10)[[\"mean_test_score\", \"param_loss\",\n                         \"param_max_depth\", \"param_min_samples_leaf\", \"param_min_samples_split\",\n                         \"param_max_features\"]]","3d48839d":"random_cv.best_estimator_","022ea62f":"from sklearn.model_selection import GridSearchCV\ntrees_grid = {\"n_estimators\": [50, 100, 150, 200, 250, 300]}\n\nbasic_model = random_cv.best_estimator_\ngrid_search = GridSearchCV(estimator=basic_model, param_grid=trees_grid, cv=4, \n                           scoring=\"neg_mean_absolute_error\", verbose=1,\n                           n_jobs=-1, return_train_score=True)","747cc4bd":"grid_search.fit(features_train, target_train);","b14f0412":"results = pd.DataFrame(grid_search.cv_results_)\n\nplt.plot(results[\"param_n_estimators\"], -1 * results[\"mean_test_score\"], label = \"Testing Error\")\nplt.plot(results[\"param_n_estimators\"], -1 * results[\"mean_train_score\"], label = \"Training Error\")\nplt.xlabel(\"Number of Trees\"); plt.ylabel(\"Mean Abosolute Error\"); plt.legend();\nplt.title(\"Performance vs Number of Trees\");","bb4c2b12":"basic_final_model = grid_search.best_estimator_\nbasic_final_model","c87ff491":"basic_final_pred = basic_final_model.predict(features_test)\nbasic_final_mae = mae(target_test, basic_final_pred)\nprint(\"Final model performance on the test set: MAE = {:.04f}.\".format(basic_final_mae))","d0bf3772":"sns.kdeplot(basic_final_pred, label = \"Predictions\")\nsns.kdeplot(target_test, label = \"Test\")\nsns.kdeplot(target_train, label = \"Train\")\n\nplt.xlabel(\"Global Sales\"); plt.ylabel(\"Density\");\nplt.title(\"Test, Train Values and Predictions\");","062d60c4":"basic_residuals = basic_final_pred - target_test\n\nsns.kdeplot(basic_residuals, color = \"lightskyblue\")\nplt.xlabel(\"Error\"); plt.ylabel(\"Count\")\nplt.title(\"Distribution of Residuals\");","23c0e6d5":"def donut_chart(column, palette=\"Set2\"):\n    values = column.value_counts().values\n    labels = column.value_counts().index\n    plt.pie(values, colors=sns.color_palette(palette), \n            labels=labels, autopct=\"%1.1f%%\", \n            startangle=90, pctdistance=0.85)\n    #draw circle\n    centre_circle = plt.Circle((0,0), 0.70, fc=\"white\")\n    fig = plt.gcf()\n    fig.gca().add_artist(centre_circle)","ad76e4f3":"donut_chart(data[\"Platform\"])\nplt.title(\"Platforms\")\nplt.axis(\"equal\");","5e7316fd":"data[\"Platform\"].unique()","5f3a2b6d":"platforms = {\"Playstation\" : [\"PS\", \"PS2\", \"PS3\", \"PS4\"],\n             \"Xbox\" : [\"XB\", \"X360\", \"XOne\"], \n             \"PC\" : [\"PC\"],\n             \"Nintendo\" : [\"Wii\", \"WiiU\"],\n             \"Portable\" : [\"GB\", \"GBA\", \"GC\", \"DS\", \"3DS\", \"PSP\", \"PSV\"]}","b244ee10":"def get_group_label(x, groups=None):\n    if groups is None:\n        return \"Other\"\n    else:\n        for key, val in groups.items():\n            if x in val:\n                return key\n        return \"Other\"","4d751455":"data[\"Grouped_Platform\"] = data[\"Platform\"].apply(lambda x: get_group_label(x, groups=platforms))\ndonut_chart(data[\"Grouped_Platform\"])\nplt.title(\"Groups of platforms\")\nplt.axis(\"equal\");","de593e3b":"donut_chart(data[\"Genre\"], palette=\"muted\")\nplt.title(\"Genres\")\nplt.axis(\"equal\");","1f8fdbee":"scored[\"Grouped_Platform\"] = scored[\"Platform\"].apply(lambda x: get_group_label(x, platforms))\ndonut_chart(scored[\"Grouped_Platform\"])\nplt.title(\"Groups of platforms for games with score\")\nplt.axis(\"equal\");","1d9be7c7":"scored[scored[\"Grouped_Platform\"]==\"Other\"]","ac2891ea":"scored[\"Weighted_Score\"] = (scored[\"User_Score\"] * 10 * scored[\"User_Count\"] + \n                            scored[\"Critic_Score\"] * scored[\"Critic_Count\"]) \/ (scored[\"User_Count\"] + scored[\"Critic_Count\"])\ndevs = pd.DataFrame({\"dev\": scored[\"Developer\"].value_counts().index,\n                     \"count\": scored[\"Developer\"].value_counts().values})\nm_score = pd.DataFrame({\"dev\": scored.groupby(\"Developer\")[\"Weighted_Score\"].mean().index,\n                        \"mean_score\": scored.groupby(\"Developer\")[\"Weighted_Score\"].mean().values})\ndevs = pd.merge(devs, m_score, on=\"dev\")\ndevs = devs.sort_values(by=\"count\", ascending=True)\ndevs[\"percent\"] = devs[\"count\"] \/ devs[\"count\"].sum()\ndevs[\"top%\"] = devs[\"percent\"].cumsum() * 100\nn_groups = 5\ndevs[\"top_group\"] = (devs[\"top%\"] * n_groups) \/\/ 100 + 1\ndevs[\"top_group\"].iloc[-1] = n_groups\ndevs","e4a20d71":"pal = sns.color_palette(\"RdYlGn\", n_groups)\ng = sns.JointGrid(x=\"top%\", y=\"mean_score\", data=devs, size=12)\nlegend_elements = []\nfor k in range(0, n_groups):\n    g.ax_joint.scatter(devs[devs[\"top_group\"]==k+1][\"top%\"], \n                       devs[devs[\"top_group\"]==k+1][\"mean_score\"],\n                       color=pal[k], alpha=.9, edgecolor=\"black\")\n    legend_elements.append(Line2D([0], [0], label=k+1, marker=\"o\", ls=\"\", \n                                  mfc=pal[k], mec=pal[k], alpha=.9, markersize=15))\n    g.ax_marg_x.bar(np.arange(k * 100 \/ n_groups, (k+1) * 100 \/ n_groups), \n                    devs[devs[\"top_group\"]==k+1].shape[0], \n                    width=1, align=\"edge\", color=pal[k], alpha=.9)\ng.ax_marg_y.hist(devs[\"mean_score\"], color=pal[-1], alpha=.9,\n                 orientation=\"horizontal\", bins=25, edgecolor=\"white\")\ng.set_axis_labels(\"Top %\", \"Mean Weighted Score\")\ng.ax_joint.tick_params(labelsize=15)\ng.ax_marg_x.grid(False)\ng.ax_marg_y.grid(False)\n#g.ax_joint.legend(handles=legend_elements, title=\"Top Group\", loc=4)\nplt.subplots_adjust(top=0.9)\ng.fig.suptitle(\"Developers\");","e437415c":"data[\"Critic_Score\"].fillna(0.0, inplace=True)\ndata[\"Critic_Count\"].fillna(0.0, inplace=True)\ndata[\"User_Score\"].fillna(0.0, inplace=True)\ndata[\"User_Count\"].fillna(0.0, inplace=True)\ndata = data.join(devs.set_index(\"dev\")[\"top_group\"], on=\"Developer\")\ndata = data.rename(columns={\"top_group\": \"Developer_Rank\"})\ndata[\"Developer_Rank\"].fillna(0.0, inplace=True)\ndata[\"Rating\"].fillna(\"None\", inplace=True)","10887644":"tmp, rmvd_tmp = rm_outliers(data[data[\"User_Count\"] != 0], [\"User_Count\"])\ndata.drop(rmvd_tmp.index, axis=0, inplace=True)","fbb47cdc":"data[\"Weighted_Score\"] = (data[\"User_Score\"] * 10 * data[\"User_Count\"] + \n                            data[\"Critic_Score\"] * data[\"Critic_Count\"]) \/ (data[\"User_Count\"] + data[\"Critic_Count\"])\ndata[\"Weighted_Score\"].fillna(0.0, inplace=True)","a64c010d":"data.info()","f715c4ff":"import category_encoders as ce\n# Select the numeric columns\nnumeric_subset = data.select_dtypes(\"number\").drop(columns=[\"NA\", \"EU\", \"JP\", \"Other\", \"Year\"])\n\n# Select the categorical columns\ncategorical_subset = data[[\"Grouped_Platform\", \"Genre\", \"Rating\"]]\n\n# One hot encode\n# categorical_subset = pd.get_dummies(categorical_subset)\n\nmapping = []\nfor cat in categorical_subset.columns:\n    tmp = scored.groupby(cat).median()[\"Weighted_Score\"]\n    mapping.append({\"col\": cat, \"mapping\": [x for x in np.argsort(tmp).items()]})\n    \nencoder = ce.ordinal.OrdinalEncoder()\ncategorical_subset = encoder.fit_transform(categorical_subset, mapping=mapping)\n\n# Join the two dataframes using concat\n# Make sure to use axis = 1 to perform a column bind\nfeatures = pd.concat([numeric_subset, categorical_subset], axis = 1)\n\n# Find correlations with the score \ncorrelations = features.corr()[\"Global\"].dropna().sort_values()","49e6d30b":"target = pd.Series(features[\"Global\"])\nfeatures = features.drop(columns=\"Global\")\nfeatures_train, features_test, target_train, target_test = train_test_split(features, target, \n                                                                            test_size=0.2,\n                                                                            random_state=42)\nbaseline_guess = np.median(target_train)\nbaseline_mae = mae(target_test, baseline_guess)\nprint(\"Baseline guess for global sales is: {:.02f}\".format(baseline_guess))\nprint(\"Baseline Performance on the test set: MAE = {:.04f}\".format(baseline_mae))","553bb1a6":"model = GradientBoostingRegressor(random_state = 42)\n\nrandom_cv = RandomizedSearchCV(estimator=model,\n                               param_distributions=hyperparameter_grid,\n                               cv=4, n_iter=20, \n                               scoring=\"neg_mean_absolute_error\",\n                               n_jobs=-1, verbose=1, \n                               return_train_score=True,\n                               random_state=42)\nrandom_cv.fit(features_train, target_train);","6daa8ae2":"trees_grid = {\"n_estimators\": [50, 100, 150, 200, 250, 300]}\n\nmodel = random_cv.best_estimator_\ngrid_search = GridSearchCV(estimator=model, param_grid=trees_grid, cv=4, \n                           scoring=\"neg_mean_absolute_error\", verbose=1,\n                           n_jobs=-1, return_train_score=True)\ngrid_search.fit(features_train, target_train);","6a3c01a4":"results = pd.DataFrame(grid_search.cv_results_)\n\nplt.plot(results[\"param_n_estimators\"], -1 * results[\"mean_test_score\"], label = \"Testing Error\")\nplt.plot(results[\"param_n_estimators\"], -1 * results[\"mean_train_score\"], label = \"Training Error\")\nplt.xlabel(\"Number of Trees\"); plt.ylabel(\"Mean Abosolute Error\"); plt.legend();\nplt.title(\"Performance vs Number of Trees\");","d0969db8":"final_model = grid_search.best_estimator_\nfinal_pred = final_model.predict(features_test)\nfinal_mae = mae(target_test, final_pred)\nprint(\"Final model performance on the test set: MAE = {:.04f}.\".format(final_mae))","54d97333":"import matplotlib.gridspec as gridspec\nfigsize(20, 16)\n\nfig = plt.figure()\ngs = gridspec.GridSpec(2, 2)\n\nplt.suptitle(\"Video Games - Predicting Global Sales\", size=30, weight=\"bold\");\n\nax = fig.add_subplot(gs[0, 0])\nplt.sca(ax)\nsns.kdeplot(final_pred, color=\"limegreen\", label=\"Advanced Model\")\nsns.kdeplot(basic_final_pred, color=\"indianred\", label=\"Basic Model\")\nsns.kdeplot(target_test, color=\"royalblue\", label=\"Test\")\nplt.xlabel(\"Global Sales, $M\", size=20); plt.ylabel(\"Density\", size=20);\nplt.title(\"Distribution of Target Values\", size=24);\n\nresiduals = final_pred - target_test\nax = fig.add_subplot(gs[0, 1])\nplt.sca(ax)\nsns.kdeplot(residuals, color = \"limegreen\", label=\"Advanced Model\")\nsns.kdeplot(basic_residuals, color=\"indianred\", label=\"Basic Model\")\nplt.xlabel(\"Residuals, $M\", size=20);plt.ylabel(\"Density\", size=20);\nplt.title(\"Distribution of Errors\", size=24);\n\nfeature_importance = final_model.feature_importances_\nfeature_names = features.columns.tolist()\nfeature_importance = 100.0 * (feature_importance \/ feature_importance.max())\nsorted_idx = np.argsort(feature_importance)\npos = np.arange(sorted_idx.shape[0]) + .5\nax = fig.add_subplot(gs[1, 0])\nplt.sca(ax)\nplt.barh(pos, feature_importance[sorted_idx], align='center', color=\"goldenrod\")\nplt.yticks(pos, [feature_names[x] for x in sorted_idx], size=16)\nplt.xlabel('Relative Importance', size=20)\nplt.title('Variable Importance', size=24);\n\nmodel_comparison = pd.DataFrame({\"model\": [\"Baseline\", \"Basic\", \"Advanced\"],\n                                 \"mae\": [basic_baseline_mae, basic_final_mae, final_mae],\n                                 \"diff\": [\"0.00%\", \"-{:.2f}%\".format((1 - basic_final_mae \/ basic_baseline_mae) * 100), \"-{:.2f}%\".format((1 - final_mae \/ basic_baseline_mae) * 100)],\n                                 \"color\": [\"royalblue\", \"indianred\", \"limegreen\"]})\nmodel_comparison.sort_values(\"mae\", ascending=False)\npos = np.arange(3) + .5\nax = fig.add_subplot(gs[1, 1])\nplt.sca(ax)\nplt.barh(pos, model_comparison[\"mae\"], align=\"center\", color=model_comparison[\"color\"])\nfor i in [1, 2]:\n    plt.text(plt.xlim()[1], pos[i], model_comparison[\"diff\"][i], \n             verticalalignment=\"center\", horizontalalignment=\"right\")\nplt.yticks(pos, model_comparison[\"model\"], size=16); plt.xlabel(\"Mean Absolute Error\", size=20);\nplt.title(\"Test MAE\", size=24);","2972b4a5":"Here I am doing 3 things:\n1. Renaming columns for ease of use\n2. Droping games without a year of release or genre\n3. Creating a new column for age of the game","4ceb1751":"I will also remove outliers in User_Count column.","f9611c91":"Using grid search to find optimal value of the n_estimators parameter.","72eb647e":"For the baseline guess I'll use median value of global sales in the train dataset.","a0295367":"Gradient boosting regressor seems to be the best model, I will focus on this one.  \n  \n  First I am going to use randomized search to find the best parameters, and then I will use grid search for optimizing n_estimators. ","3b4b54c3":"I want to see in what region games were more popular, based on sales.","a74d081f":"Next I want to create some new features: weighted score and my own developer rating. First, I find percent of all games created by each developer, then calculate cumulative percent starting with devs with the least number of games. Finally, I divide them into 5 groups (20% each). Higher rank means more games developed.","23d01564":"Lowest correlation is Platform_PC, which seems strange to me. Highest are critic and user scores and counts, which is understandable. ","82a97ab9":"Second graph is a histogram of residuals - differences between real values and predictions. ","0672a5ae":"I will compare several simple models with different types of regression, and then focus on the best one for hyperparameter tuning. ","0a9d56a1":"I see that games with score are more evenly spread between 2000 and 2015, while there is a peak at 2010 for games without score. Probably there were so many games published in 2010 that a lot of them remained unnoticed by the community.  \n  \n  Another difference I noticed is in a range 1995-2000. Very few games from that period have scores, while the total number of games is quite significant. I think videogames were only starting to rise in popularity, but there was no dedicated platform (like Steam now) or specialised magazine to review games. ","767c96c9":"Let's look at the highest and lowest correlations with the global sales column. ","54a60ef3":"\"Advanced\" model gives better results (lower error on test set) which is a good achievement, but the model is still overfitting (graph above). There is definitely room for improvement. ","118d2df1":"Creating Weighted_Score column (earlier I did it for \"scored\" dataframe).","ea901860":"And to finish with the project, a nice group of plots summarizing the results.","43903996":"Removing outliers in User_Count column.","01156b37":"Defining a function to evaluate my model. I will use mean absolute error. ","214658a3":"To finish with the basic model I am going to draw 2 graphs. First one is comparison of densities of train values, test values and predictions.","f669377d":"Now I want to check the same thing for genres.","0bb1e9ce":"There are 17 unique platfoms, 12 unique genres and 5 ratings in the remaining data. In the advanced model I will try grouping platforms to reduce amount, but for now I will just one-hot encode them. ","d20eda8f":"Printing out 10 best estimators found by randomized search.","37d9f266":"There are ~17k games, but some of the data is missing. For instance,  only around half of all games has a critic score. This might be a problem for the prediction model, as I was thinking that critic score will be one of the main features. But I will come back to it later. Another issue is that the User_Score column has type object instead of float (like Critic_Score). It means that there are non-numeric values and I'll have to get rid of them.","647cdbc5":"## Basic model","0a756ede":"Splitting data into train and test sets.","cf73f6bd":"While EU and NA have approximately the same number of scored and not scored games, situation is absolutely different for Japan. Majority of games, that were more popular in Japan than in other regions, does not have a user or critic score. Mean global sales is less for games without score, which confirmes what we saw from the histogram above.","95c023f5":"Now I will do the same things as I did in the basic model, except for using Ordinal encoding for categorical values instead of OneHot. I tried different kinds of encodings, and ordinal seems to work best.","45862ce8":"For my basic model I am going to drop games that don't have a user score, critic score or rating. ","175db182":"2 things to see in the table above:\n1. The top value in User_Score column is \"tbd\" which I will mark as NaN.\n2. There are serious outliers in sales columns (Global, EU, NA, JP, Other) and User_Count column.","43cc96c9":"MAE dropped, but by a very small margin. Looks like hyperparameter tuning didn't really improve the model. I hope advanced model will have a better performance.","663f4468":"Games without score tend to have less global sales.","604dde6d":"Reading and exploring data","7b602e5f":"Predictions density is moved a little to the right, comparing to densities of initial values. The tail is also different. This might help tuning the model in the future. ","50f4000d":"Let's lock the final model and see how it performs on test data.","bdd6c4d9":"Now I want to see how the number of trees effects the performance. ","c5ae070b":"Setting visualisation parameters","2515056f":"Only 5.5k games, ~1\/3 of all games in a dataset. That doesn't seem good to me, but for a basic model it will be ok.","3fe40345":"Almost all games that have scores are for \"big\" platfroms: PC, PS, Xbox or portable. But there are few from the \"Other\" group. I was interested what these games were so I searched for them.","68ee9688":"All these games are for \"DC\" platfom which is Sega Dreamcast, the last of Sega consoles. It was released in 1998 and was the first of sixth generation consoles,  PS2, Gamecube and Xbox. Dreamcast was actually a very good and innovative  product which recieved a lot of positive credit, but it couldn't compete with Sony or Microsoft consoles and Sega was forced to stop the production.\n\n   In 2006 Sega started a new wave of sales of Dreamcast consoles and games, which were restored from the leftovers of first production. Following this, IGN re-launched their IGN Dreamcast section to review these games and compare them with PS3, Xbox 360 and Wii games.","6c9d4b70":"The distribution seems ok, even though there is a significant number of different genres.","a9c48815":"Feaures will consist of numeric columns (except for sales in regions and year - using age instead) and one-hot encoded categorical columns (platform, genre, rating).","c1db8514":"Looks much better.","4b28c4b7":"This is another usefull function I copied from William. It shows scatterplots, histograms and kdeplots of selected columns on a seaborn PairGrid.","a541d587":"The graph shows that the model is overfitting. Training error keeps decreasing, while test error stays almost the same. It means that the model learns training examples very well, but cannot generalize on new, unknown data. This is not a very good model, but I will leave it as is, and try to battle overfitting in the advanced model using imputing, feature selection and feature engineering.","55c3132c":"# Predicting video games sales\n\n## Introduction\nThis is my first project on kaggle in which I am trying to build a model that can predict video games sales based on other features from this dataset. Below I'll try to explain the steps I took, difficulties I encountered and possible solutions that came to my mind.  \n  \n  All comments and suggetions are appreciated.","d9a0b8bc":"## Advanced model","75313436":"As I mentioned above, there are outliers in sales columns. They might be usefull for training as they indicate bestseller games, but for now I am going to remove them and maybe add them later.  \n  \n  Again, I took this very useful function from William Koehrsen. Here an outlier is defined as a value greater than (or lesser than) third quartile (first quartile) plus 3 interquartile ranges (minus 3 interqurtile ranges). ","def6f680":"Most of the games have a pretty good score, 7+ (or 70+ for critic score). Pearson's correlation coefficient is 0.58 between critic and user scores. ","88f16798":"Before creating and fitting a model I have to fill in missing values. I am filling scores and counts with zeros, because there were no real zero scores or counts in the dataset, so it will indicate absence of scores.","3b7fecb4":"Next I want to see how many values there are missing in each column. I am using a function I found in one of [William Koehrsen](https:\/\/towardsdatascience.com\/@williamkoehrsen) blogs on Medium.","c6615792":"More than 50% of user and critic scores are missing. That's a lot! Usually you should drop features with too many missing values, but here I think that scores are invaluable for sales prediction, so I'll have to find a way around the problem.","87a1e4a4":"There are 39 features (1 is target) in the dataset after feature engineering and selection.","67211a05":"There are too many different platforms and most of them represent a very small percent of games. I am going to group platforms to reduce the number of features.","f7534c39":"A nice graph to see the realtion between developer rank and mean weighted score of developer's games.","497b287a":"Now let's do some visualisation of the data.","a30da16e":"I am going to build 2 models: a basic one and a more complicated. In a basic model i will drop games without a score (critic or user) and train it on the remaining data. I will also do minimum feature engineering or feature selection. After I am finished with the basic model, I am going to come back to the full dataset and try to impute missing values and create new features.  \n  \n  Let's investigate differences between the groups (with and without score). ","d1445c14":"## Preparations and EDA\nImporting data science packages and checking what files there are in the dataset","4488d153":"This is a universal function for training a model and evaluating its performance on test data."}}