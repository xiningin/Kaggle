{"cell_type":{"361bdac9":"code","8cdd0669":"code","0eb9bb99":"code","14f9f842":"code","baf029c1":"code","18948b84":"code","924ebeb5":"code","f68f110f":"code","b50df83f":"code","0de58234":"code","7e4c997d":"code","7241ad10":"code","d738b6ab":"code","79e282b1":"code","d62df3ca":"code","0a417531":"code","e45b479b":"code","a68d5b1c":"code","426c3116":"code","fbf5243e":"code","99bb3882":"code","cbf4b532":"code","94ce2ce7":"code","010d63d3":"code","a441fe3d":"code","1b6452ca":"code","904fa873":"code","6e9c7984":"code","4a2274b3":"code","178fc8e6":"code","855b3564":"code","4771d60f":"code","c1b3ac99":"code","578ec3bb":"code","fed0c5fc":"code","6389df96":"code","ef6cab81":"code","b747215b":"code","a04558de":"code","4401ed1f":"code","4357b7c2":"code","c6ace8e7":"code","295e74c5":"code","8c7c29a3":"code","aca2bbf8":"code","ca3cc5af":"code","5351f82d":"code","482e31ad":"code","626494cb":"code","dec8f95d":"code","075a7101":"code","72c519b3":"code","eeead044":"code","a6cd829f":"code","0a53248e":"code","15a3c29c":"code","c50aac9d":"code","113d023d":"code","423935ce":"code","8bdf0dce":"code","39375e53":"code","4ed4ad35":"code","e468fb7a":"code","c81a4781":"code","ecbe1a39":"code","fe689a62":"code","963f61c5":"code","e437159e":"code","6795497e":"code","8e8fc260":"code","34b091c2":"code","cb8fa1cd":"code","965b7b0d":"code","9572664d":"code","7a8e5223":"code","db114327":"code","8c5310ec":"code","344ecebb":"code","b17862b6":"code","6a9b4108":"code","6cca2d14":"code","a1770cb6":"code","5b65b43e":"code","28f027f9":"code","5eef8434":"code","5f89a8bf":"code","46897b27":"code","ba9b6746":"code","2882d1f4":"code","6bf1a5e4":"code","4cd2de70":"code","43544528":"code","3bc9ebda":"code","4c4d7f7f":"code","42308d10":"code","b27190df":"code","54dfa4c6":"code","38c7c9f6":"code","829e2454":"code","78cc00c8":"code","3199cbf8":"code","163606eb":"code","c9021032":"code","cafe64de":"code","5a633824":"code","0239f3e2":"code","e22a5d91":"code","139d181b":"code","0c75b7a9":"code","69c79f53":"code","d1e22f10":"code","e0df38ca":"code","bcadcd52":"code","741e0760":"code","2019676d":"code","72ae5acf":"code","f957f783":"code","ec544efc":"code","76ece231":"code","67ad6a84":"code","39310bca":"code","bddd11c9":"code","46cc34b5":"code","bf16fec0":"code","3f0cec9c":"code","ce19e120":"markdown","5abe666e":"markdown","11967bee":"markdown","27f7a98d":"markdown","70cdfe29":"markdown","e699b499":"markdown","be29ba64":"markdown","241f87ad":"markdown","28bef8c5":"markdown","991a960d":"markdown","b86635d4":"markdown","dcf6b37c":"markdown","bae48203":"markdown","ed69960b":"markdown","d21d6b4e":"markdown","2625c401":"markdown","14a7967b":"markdown","061e7a59":"markdown","96642708":"markdown","065e9637":"markdown","b4f54163":"markdown","afe84cf2":"markdown","0d50236b":"markdown","50cbd527":"markdown","67bc443d":"markdown","189723a6":"markdown","c3fac586":"markdown","6775c462":"markdown","9b9cf75e":"markdown","ef1d6951":"markdown","9c410bab":"markdown","4a1c85c9":"markdown","795efd0c":"markdown","a3f31dde":"markdown","7efb614f":"markdown","e02af0f4":"markdown","ce2162d9":"markdown","269e8a0f":"markdown","c183900b":"markdown","1b42cb31":"markdown","e24b3bd7":"markdown","13783a85":"markdown","1f73644d":"markdown","37f94228":"markdown","a7622058":"markdown","5ad60081":"markdown","28003d3a":"markdown","297fb6a5":"markdown","a06bfce5":"markdown","ac92a1e6":"markdown","9a006150":"markdown","5f94fe55":"markdown","b8cd4acd":"markdown","d8d6e4e2":"markdown","44efe8c6":"markdown","f896fe91":"markdown","d1be34ee":"markdown","aa9f2e93":"markdown","303999a3":"markdown","9526c0ee":"markdown","df0e5efc":"markdown","3a981583":"markdown","8730a002":"markdown","50e7984d":"markdown","8519b761":"markdown","809df265":"markdown","b8804613":"markdown","443a35c1":"markdown"},"source":{"361bdac9":"! pip install skater==1.0.4","8cdd0669":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"data\/\" directory.\n\nimport time, warnings\nimport datetime as dt\n\n#visualizations\nimport matplotlib.pyplot as plt\nfrom pandas.plotting import scatter_matrix\n%matplotlib inline\nimport seaborn as sns\n\n# Standard ML Models for comparison\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestClassifier,RandomForestRegressor\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\n\nfrom sklearn.metrics import r2_score,accuracy_score,classification_report\n# Splitting data into training\/testing\nfrom sklearn.model_selection import train_test_split,GridSearchCV\nfrom sklearn.preprocessing import StandardScaler,LabelEncoder,MinMaxScaler\nfrom sklearn.pipeline import Pipeline\nwarnings.filterwarnings(\"ignore\")\nfrom sklearn.cluster import KMeans\n\n# Importing libraries for building the neural network\nfrom keras.models import Sequential\nfrom keras.layers import Dense,LSTM\nfrom keras.wrappers.scikit_learn import KerasClassifier,KerasRegressor\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import cross_val_score\n\nimport xgboost as xgb\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split\n\nimport xgboost as xgb\nfrom eli5.sklearn import PermutationImportance\nimport eli5\n\nfrom skater.core.explanations import Interpretation\nfrom skater.model import InMemoryModel","0eb9bb99":"##reading and checking dataset\ndf_customer=pd.read_csv('..\/input\/brazilian-ecommerce\/olist_customers_dataset.csv')\ndf_location=pd.read_csv('..\/input\/brazilian-ecommerce\/olist_geolocation_dataset.csv')\ndf_items=pd.read_csv('..\/input\/brazilian-ecommerce\/olist_order_items_dataset.csv')\ndf_payments=pd.read_csv('..\/input\/brazilian-ecommerce\/olist_order_payments_dataset.csv')\ndf_reviews=pd.read_csv('..\/input\/brazilian-ecommerce\/olist_order_reviews_dataset.csv')\ndf_products=pd.read_csv('..\/input\/brazilian-ecommerce\/olist_products_dataset.csv')\ndf_orders=pd.read_csv('..\/input\/brazilian-ecommerce\/olist_orders_dataset.csv')\ndf_sellers=pd.read_csv('..\/input\/brazilian-ecommerce\/olist_sellers_dataset.csv')\ndf_name_trans=pd.read_csv('..\/input\/brazilian-ecommerce\/product_category_name_translation.csv')\n","14f9f842":"## getting order id by customer purchases \n\ndf_customer_order=pd.merge(df_customer,df_orders[['order_id','customer_id','order_purchase_timestamp']],on='customer_id')","baf029c1":"## payments in same order id are combined to get total spending on an order\npaid=df_payments[['order_id','payment_value']].groupby('order_id').sum().reset_index()","18948b84":"## now the total payment by each order is merged to the cutomer who has bought it to find the total amount purchase\ndf_customer_order_rev=pd.merge(df_customer_order,paid,on='order_id')","924ebeb5":"## unwanted columns are dropped\ndf_customer_order_rev.drop(['customer_zip_code_prefix','customer_city','customer_state'],axis=1,inplace=True)","f68f110f":"df_customer_order_rev['order_purchase_timestamp']=pd.to_datetime(df_customer_order_rev['order_purchase_timestamp']).dt.date","b50df83f":"## find the last date on which customer made the purchase\nrecency=pd.DataFrame(df_customer_order_rev.groupby('customer_unique_id')['order_purchase_timestamp'].max())","0de58234":"## we take the maximum date of purchase made by customers as the date to calculate the recency of the purchase\n## 2018-10-17\nrecency['recent_days']=recency['order_purchase_timestamp'].max()-recency['order_purchase_timestamp']\nrecency['recent_days']=recency['recent_days'].dt.days","7e4c997d":"## the number of times a unique customer has made purchase\nfrequency=pd.DataFrame(df_customer_order_rev.groupby('customer_unique_id')['customer_id'].count())","7241ad10":"monetary=pd.DataFrame(df_customer_order_rev[['customer_unique_id','payment_value']].groupby('customer_unique_id')['payment_value'].sum())","d738b6ab":"# the receny of visit, total monetary spent and freqency of purchase by each customer is found out and merged\ndf_rfm=pd.merge(recency,frequency,on='customer_unique_id')\ndf_rfm=pd.merge(df_rfm,monetary,on='customer_unique_id')","79e282b1":"## Freqency - Number of purchase made\n## Recency- Days from last purchase\n## Monetary-- total amount purchase for by a customer\ndf_rfm.drop(['order_purchase_timestamp'],axis=1,inplace=True)\ndf_rfm.reset_index(inplace=True)\ndf_rfm.columns=['Cust_unique_Id','Recency','Frequency','Monetary']\n#use CustomerID as index\ndf_rfm.set_index('Cust_unique_Id',inplace=True)\ndf_rfm","d62df3ca":"## the descriptive stats for the RFM analysis\ndf_rfm.describe()","0a417531":"(df_rfm[df_rfm['Frequency']>1].shape[0]\/96095)*100","e45b479b":"# Plot RFM distributions\nplt.figure(figsize=(12,10))\n# Plot distribution of R\nplt.subplot(3, 1, 1); sns.distplot(df_rfm['Recency'],kde=False)\n# Plot distribution of F\nplt.subplot(3, 1, 2); sns.distplot(df_rfm['Frequency'],kde=False)\n# Plot distribution of M\nplt.subplot(3, 1, 3); sns.distplot(df_rfm['Monetary'],kde=False)\n# Show the plot\nplt.show()","a68d5b1c":"quantiles = df_rfm.quantile(q=[0.25,0.5,0.75])\nquantiles.to_dict()","426c3116":"# Arguments (x = value, p = recency, monetary_value, frequency, d = quartiles dict)\ndef RScore(x,p,d):\n    if x <= d[p][0.25]:\n        return 4\n    elif x <= d[p][0.50]:\n        return 3\n    elif x <= d[p][0.75]: \n        return 2\n    else:\n        return 1\n# Arguments (x = value, p = recency, monetary_value, frequency, k = quartiles dict)\ndef FMScore(x,p,d):\n    if x <= d[p][0.25]:\n        return 1\n    elif x <= d[p][0.50]:\n        return 2\n    elif x <= d[p][0.75]: \n        return 3\n    else:\n        return 4","fbf5243e":"#create rfm segmentation table\nrfm_segmentation = df_rfm\nrfm_segmentation['R_Quartile'] = rfm_segmentation['Recency'].apply(RScore, args=('Recency',quantiles,))\nrfm_segmentation['F_Quartile'] = rfm_segmentation['Frequency'].apply(FMScore, args=('Frequency',quantiles,))\nrfm_segmentation['M_Quartile'] = rfm_segmentation['Monetary'].apply(FMScore, args=('Monetary',quantiles,))","99bb3882":"rfm_segmentation['RFMScore'] = rfm_segmentation.R_Quartile.map(str) \\\n                            + rfm_segmentation.F_Quartile.map(str) \\\n                            + rfm_segmentation.M_Quartile.map(str)\nrfm_segmentation.head()","cbf4b532":"### how many customers are in each segment\n\nprint(\"Best Customers: \",len(rfm_segmentation[rfm_segmentation['RFMScore']=='444']))\nprint('Loyal Customers: ',len(rfm_segmentation[rfm_segmentation['F_Quartile']==4]))\nprint(\"Big Spenders: \",len(rfm_segmentation[rfm_segmentation['M_Quartile']==4]))\nprint('Almost Lost: ', len(rfm_segmentation[rfm_segmentation['RFMScore']=='244']))\nprint('Lost Customers: ',len(rfm_segmentation[rfm_segmentation['RFMScore']=='144']))\nprint('Lost Cheap Customers: ',len(rfm_segmentation[rfm_segmentation['RFMScore']=='111']))","94ce2ce7":"best_cust=rfm_segmentation[rfm_segmentation['RFMScore']=='444']\n#calculate and show correlations\ncorr_matrix = best_cust.corr()\nsns.heatmap(corr_matrix)","010d63d3":"##merging sellers with orderid sold\ndf_seller_seg=pd.merge(df_items[['order_id','seller_id','price']],df_orders[['order_id','order_purchase_timestamp','order_delivered_carrier_date']],on='order_id')","a441fe3d":"##merging review scores of the order with the order ids\ndf_seller_seg=pd.merge(df_seller_seg,df_reviews[['order_id','review_score']],on='order_id')","1b6452ca":"##converting dates to datetime format\n\ndf_seller_seg['order_purchase_timestamp']=pd.to_datetime(df_seller_seg['order_purchase_timestamp']).dt.date\ndf_seller_seg['order_delivered_carrier_date']=pd.to_datetime(df_seller_seg['order_delivered_carrier_date']).dt.date","904fa873":"df_seller_seg['days_to_del']=(df_seller_seg['order_delivered_carrier_date']-df_seller_seg['order_purchase_timestamp']).dt.days","6e9c7984":"## dropping unwanted columns\ndf_seller_seg.drop(['order_purchase_timestamp','order_delivered_carrier_date'],axis=1,inplace=True)","4a2274b3":"## filling missing values with mean values\ndf_seller_seg['days_to_del'].fillna(df_seller_seg['days_to_del'].mean(),inplace=True)","178fc8e6":"## removing negative dates, these are outliers\ndf_seller_seg_1=df_seller_seg[df_seller_seg['days_to_del']>=0]","855b3564":"## makeing total number of sales, amount of total sales, average review, avergage days to deliver to carrier\ndf_seller_seg_2=df_seller_seg_1.groupby('seller_id').agg({'order_id':'count','price':'sum','review_score':'mean','days_to_del':'mean'})\ndf_seller_seg_2.columns=['Tot_sales','Tot_amount','Avg_review','Avg_delivery']","4771d60f":"## filling missing values with mean values\ndf_seller_seg_2['Avg_delivery'].fillna(df_seller_seg_2['Avg_delivery'].mean(),inplace=True)","c1b3ac99":"##splitting into quartiles\nquantiles = df_seller_seg_2.quantile(q=[0.25,0.5,0.75])\nquantiles.to_dict()","578ec3bb":"## feature statistic distribution\ndf_seller_seg_2.describe()","fed0c5fc":"df_seller_seg_2['Tot_sales_Quartile'] = df_seller_seg_2['Tot_sales'].apply(FMScore, args=('Tot_sales',quantiles,))\ndf_seller_seg_2['Tot_amount_Quartile'] = df_seller_seg_2['Tot_amount'].apply(FMScore, args=('Tot_amount',quantiles,))\ndf_seller_seg_2['Avg_review_Quartile'] = df_seller_seg_2['Avg_review'].apply(FMScore, args=('Avg_review',quantiles,))\ndf_seller_seg_2['Avg_delivery_Quartile'] = df_seller_seg_2['Avg_delivery'].apply(RScore, args=('Avg_delivery',quantiles,))","6389df96":"df_seller_seg_2['SellerScore'] = df_seller_seg_2['Tot_sales_Quartile'].map(str) \\\n                            + df_seller_seg_2['Tot_amount_Quartile'].map(str) \\\n                            + df_seller_seg_2['Avg_review_Quartile'] .map(str) \\\n                            + df_seller_seg_2['Avg_delivery_Quartile'].map(str)\ndf_seller_seg_2.head()","ef6cab81":"### how many sellers are in each segment\n\nprint(\"Best Sellers: \",len(df_seller_seg_2[df_seller_seg_2['SellerScore']=='4444']))\nprint('highest Reviewed Sellers: ',len(df_seller_seg_2[df_seller_seg_2['Avg_review_Quartile']==4]))\nprint(\"Fastest delivering sellers: \",len(df_seller_seg_2[df_seller_seg_2['Avg_delivery_Quartile']==4]))\n","b747215b":"## there can be overlap in this classification . For example highest reviewed sellers can be best Fastest delivering sellers\nFastest_delivering =df_seller_seg_2[df_seller_seg_2['Avg_delivery_Quartile']==4].reset_index()\nhighest_Reviewed=df_seller_seg_2[df_seller_seg_2['Avg_review_Quartile']==4].reset_index()\nlen(set(list(Fastest_delivering['seller_id'])).intersection(highest_Reviewed['seller_id'].tolist()))","a04558de":"best_cust=df_seller_seg_2[df_seller_seg_2['SellerScore']=='4444']\n#calculate and show correlations\ncorr_matrix = best_cust.corr()\nsns.heatmap(corr_matrix)","4401ed1f":"df_customer_order_rev=pd.merge(df_customer_order,paid,on='order_id')","4357b7c2":"## the count, mean, standard deviation,minimum, maximum,qunatiles are displayed of the total spend by customers\npd.DataFrame(df_customer_order_rev.groupby('customer_unique_id')['payment_value'].sum().describe())","c6ace8e7":"pd.DataFrame(df_customer_order_rev.groupby('customer_unique_id')['customer_id'].count().describe())","295e74c5":"pd.DataFrame(df_customer_order_rev.groupby('customer_city')['customer_id'].count().describe())","8c7c29a3":"df_state=pd.DataFrame(df_customer_order_rev.groupby('customer_state')['customer_id'].count()).reset_index()","aca2bbf8":"plt.figure(figsize=(12,8))\nplt.bar(df_state['customer_state'],df_state['customer_id'])\nplt.show()","ca3cc5af":"df_state['customer_id'].describe()","5351f82d":"df_sellers_stat=pd.merge(df_sellers,df_items,on='seller_id')","482e31ad":"## dropping unwanted columns\ndf_sellers_stat.drop(['seller_zip_code_prefix','order_item_id','product_id'],axis=1,inplace=True)","626494cb":"## the count, mean, standard deviation,minimum, maximum,qunatiles are displayed of the total spend by customers\npd.DataFrame(df_sellers_stat.groupby('seller_id')['price'].sum().describe())","dec8f95d":"pd.DataFrame(df_sellers_stat.groupby('seller_id')['order_id'].count().describe())","075a7101":"pd.DataFrame(df_sellers_stat.groupby('seller_city')['order_id'].count().describe())","72c519b3":"df_state=pd.DataFrame(df_sellers_stat.groupby('seller_state')['order_id'].count()).reset_index()","eeead044":"plt.figure(figsize=(12,8))\nplt.bar(df_state['seller_state'],df_state['order_id'])\nplt.show()","a6cd829f":"df_state['order_id'].describe()","0a53248e":"## combine orders , reviews, payments dataset with customer dataset and dropping unwanted columns\ndf1=pd.merge(df_customer.drop(columns=['customer_zip_code_prefix']),df_orders[['customer_id','order_id','order_purchase_timestamp']],on='customer_id')\ndf2=pd.merge(df1,df_reviews[['order_id','review_score']],on='order_id')\npaid=df_payments[['order_id','payment_value']].groupby('order_id').sum().reset_index()\ndf3=pd.merge(df2,paid,on='order_id')","15a3c29c":"## making purchase date in datetime format\ndf3['order_purchase_timestamp']=pd.to_datetime(df3['order_purchase_timestamp']).dt.date","c50aac9d":"number_of_days_for_purchase=180\nmax_date_in_data= df3['order_purchase_timestamp'].max()\ndata_split_date=max_date_in_data-dt.timedelta(days=number_of_days_for_purchase)","113d023d":"df_full=df3[df3['order_purchase_timestamp']<=data_split_date]\ndf_last=df3[df3['order_purchase_timestamp']>data_split_date]","423935ce":"df_last_180=pd.DataFrame({'customer_unique_id':df3['customer_unique_id'].values.tolist()})\ndf_last_180=df_last_180.merge(df_last.groupby(['customer_unique_id'])['payment_value'].sum().reset_index(),how='outer',on='customer_unique_id')\ndf_last_180.fillna(0,inplace=True)","8bdf0dce":"df_last_180['purchased']=np.where(df_last_180['payment_value']>0, 1,0)\ndf_last_180.head()","39375e53":"## total amount per customer\ntot_Amount=df_full.groupby('customer_unique_id')['payment_value'].sum().reset_index().rename(columns={'payment_value':'total_amount'})\n## average review given\navg_review=df_full.groupby('customer_unique_id')['review_score'].mean().reset_index().rename(columns={'review_score':'avg_review'})\n## months between first purchase and today\nmin_max_date=df_full.groupby('customer_unique_id')['order_purchase_timestamp'].agg([min,max])\nmin_max_date['diff_first_today']=(dt.datetime.today().date()-min_max_date['min']).dt.days\n## months from first to last purchase\nmin_max_date['max']=pd.to_datetime(min_max_date['max'])\nmin_max_date['min']=pd.to_datetime(min_max_date['min'])\nmin_max_date['diff_first_last']=(min_max_date['max']-min_max_date['min']).dt.days\n## recency of Sales \nmax_date=df_full['order_purchase_timestamp'].max()\n\nmin_max_date['recency']=(np.datetime64(max_date)-min_max_date['max'])\/np.timedelta64(1, 'M')\n## Frequency of Sales\nfrequency=df_full.groupby('customer_unique_id')['order_id'].count().reset_index().rename(columns={'order_id':'frequency'})","4ed4ad35":"## joining all the engineered features\ndataset=pd.merge(tot_Amount,avg_review,on='customer_unique_id')\ndataset=pd.merge(dataset,min_max_date,on='customer_unique_id')\ndataset=pd.merge(dataset,frequency,on='customer_unique_id')\ndataset=pd.merge(dataset,df_full[['customer_unique_id','customer_city','customer_state']],on='customer_unique_id')\ndataset.drop(['min','max'],axis=1,inplace=True)\n","e468fb7a":"### label encoding city and state names\nencoder=LabelEncoder()\ndataset['customer_city']=encoder.fit_transform(dataset['customer_city'])\ndataset['customer_state']=encoder.fit_transform(dataset['customer_state'])","c81a4781":"##merging with the label dataset we have created \ndataset_full=dataset.merge(df_last_180[['customer_unique_id','purchased']],on='customer_unique_id')\ndataset_full.drop(columns='customer_unique_id',inplace=True)","ecbe1a39":"##splitting to train and test dataset\nX_train,X_test,y_train,y_test=train_test_split(dataset_full.iloc[:,:-1],dataset_full.iloc[:,-1], test_size=0.2, random_state=31)\n","fe689a62":"## calculating gini scores for the models\ndef Gini(y_true, y_pred):\n    # check and get number of samples\n    assert y_true.shape == y_pred.shape\n    n_samples = y_true.shape[0]\n    \n    # sort rows on prediction column \n    # (from largest to smallest)\n    arr = np.array([y_true, y_pred]).transpose()\n    true_order = arr[arr[:,0].argsort()][::-1,0]\n    pred_order = arr[arr[:,1].argsort()][::-1,0]\n    \n    # get Lorenz curves\n    L_true = np.cumsum(true_order) \/ np.sum(true_order)\n    L_pred = np.cumsum(pred_order) \/ np.sum(pred_order)\n    L_ones = np.linspace(1\/n_samples, 1, n_samples)\n    \n    # get Gini coefficients (area between curves)\n    G_true = np.sum(L_ones - L_true)\n    G_pred = np.sum(L_ones - L_pred)\n    \n    # normalize to true Gini coefficient\n    return G_pred\/G_true","963f61c5":"\n# Evaluate several ml models by training on training set and testing on testing set\ndef evaluate(X_train, X_test, y_train, y_test):\n    # Names of models\n    model_name_list = ['Linear Regression',\n                      'Random Forest', 'Extra Trees',\n                       'Gradient Boosted','KNeighbors']\n\n    \n    # Instantiate the models\n    model1 = LinearRegression()\n    model3 = RandomForestClassifier(n_estimators=50)\n    model4 = ExtraTreesClassifier(n_estimators=50)\n    model6 = GradientBoostingClassifier(n_estimators=20)\n    model7= KNeighborsClassifier(n_neighbors = 5)\n    \n    # Dataframe for results\n    results = pd.DataFrame(columns=['r2', 'accuracy','gini'], index = model_name_list)\n    \n    # Train and predict with each model\n    for i, model in enumerate([model1, model3, model4, model6,model7]):\n   \n        model.fit(X_train, y_train)\n        predictions = model.predict(X_test)\n        \n        # Metrics\n        r2 = r2_score(y_test,predictions)\n        preds=np.where(predictions>0.5,1,0)\n        accuracy = accuracy_score(y_test,preds)\n        gini=Gini(y_test,preds)\n        \n        # Insert results into the dataframe\n        model_name = model_name_list[i]\n        results.loc[model_name, :] = [r2, accuracy,gini]\n    \n    return results","e437159e":"results=evaluate(X_train, X_test, y_train, y_test)","6795497e":"results","8e8fc260":"classifier = Sequential()\n#First Hidden Layer\nclassifier.add(Dense(4, activation='relu', kernel_initializer='random_normal', input_dim=8))\n#Second  Hidden Layer\nclassifier.add(Dense(4, activation='relu', kernel_initializer='random_normal'))\n#Output Layer\nclassifier.add(Dense(1, activation='sigmoid', kernel_initializer='random_normal'))","34b091c2":"#Compiling the neural network\nclassifier.compile(optimizer ='adam',loss='binary_crossentropy', metrics =['accuracy'])","cb8fa1cd":"#Fitting the data to the training dataset\nclassifier.fit(X_train,y_train, batch_size=10, epochs=10)","965b7b0d":"eval_model=classifier.evaluate(X_train, y_train)\neval_model","9572664d":"y_pred=classifier.predict(X_test)\ny_pred =(y_pred>0.5)\nr2 = r2_score(y_test,y_pred)\naccuracy = accuracy_score(y_test,y_pred)\nprint('r2 score is %d ',r2)\nprint('accuracy score is %d ',accuracy)","7a8e5223":"## we use the dataset used for previous machine learning models, the one which excludes the last 6 months of data, we \ndataset.head()","db114327":"## from this we want to create cluster or scoring for monetary,frequency and recency values and add it to the dataset\nquantiles = dataset[['recency','frequency','total_amount']].quantile(q=[0.25,0.5,0.75])\nquantiles.to_dict()\ndataset['R_Quartile'] = dataset['recency'].apply(RScore, args=('recency',quantiles,))\ndataset['F_Quartile'] = dataset['frequency'].apply(FMScore, args=('frequency',quantiles,))\ndataset['M_Quartile'] = dataset['total_amount'].apply(FMScore, args=('total_amount',quantiles,))","8c5310ec":"## Now we take the 6 month dataset we previously created and aggreagate to find the total spending of\n# customer in last 6 months, we drop the binary column used for the last experiment\ndf_last_180.drop(['purchased'],axis=1,inplace=True)\n## we merge this with dataset prepared above\ndataset=pd.merge(dataset,df_last_180,on='customer_unique_id')","344ecebb":"#order cluster method\n## this function is used to arange  the cluster of LTV \ndef order_cluster(cluster_field_name, target_field_name,df,ascending):\n    new_cluster_field_name = 'new_' + cluster_field_name\n    df_new = df.groupby(cluster_field_name)[target_field_name].mean().reset_index()\n    df_new = df_new.sort_values(by=target_field_name,ascending=ascending).reset_index(drop=True)\n    df_new['index'] = df_new.index\n    df_final = pd.merge(df,df_new[[cluster_field_name,'index']], on=cluster_field_name)\n    df_final = df_final.drop([cluster_field_name],axis=1)\n    df_final = df_final.rename(columns={\"index\":cluster_field_name})\n    return df_final","b17862b6":"#remove outliers\ndataset = dataset[dataset['payment_value']<dataset['payment_value'].quantile(0.99)]\n\n\n#creating 3 clusters\nkmeans = KMeans(n_clusters=5,random_state=123)\nkmeans.fit(dataset[['payment_value']])\ndataset['LTVCluster'] = kmeans.predict(dataset[['payment_value']])\n\n#order cluster number based on LTV\ndataset = order_cluster('LTVCluster', 'payment_value',dataset,True)\n\n#creatinga new cluster dataframe\ndataset2 = dataset.copy()\n\n#see details of the clusters\ndataset2.groupby('LTVCluster')['payment_value'].describe()","6a9b4108":"\n#calculate and show correlations\ncorr_matrix = dataset2.corr()\ncorr_matrix['LTVCluster'].sort_values(ascending=False)","6cca2d14":"#create X and y, X will be feature set and y is the label - LTV\nX = dataset2.drop(['LTVCluster','payment_value','customer_unique_id'],axis=1)\ny = dataset2['LTVCluster']\n\n#split training and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.05, random_state=56)","a1770cb6":"#XGBoost Multiclassification Model\nltv_xgb_model = xgb.XGBClassifier(max_depth=5, learning_rate=0.1,objective= 'multi:softprob',n_jobs=-1).fit(X_train, y_train)\n\nprint('Accuracy of XGB classifier on training set: {:.2f}'\n       .format(ltv_xgb_model.score(X_train, y_train)))\nprint('Accuracy of XGB classifier on test set: {:.2f}'\n       .format(ltv_xgb_model.score(X_test[X_train.columns], y_test)))\n\ny_pred = ltv_xgb_model.predict(X_test)\nprint(classification_report(y_test, y_pred))","5b65b43e":"ltv_cust=dataset2.drop(['customer_unique_id'],axis=1)\n#calculate and show correlations\ncorr_matrix = ltv_cust.corr()\nsns.heatmap(corr_matrix)","28f027f9":"## we make the customere purchase date as year \ndf_customer_order_rev['purcahse_yr']=pd.to_datetime(df_customer_order_rev['order_purchase_timestamp']).dt.year","5eef8434":"df_per_yr=pd.DataFrame(df_customer_order_rev.groupby(['customer_unique_id','purcahse_yr'])['payment_value'].sum())","5f89a8bf":"## the dataframe shows the spending of customer in each year\ndf_per_yr.head()","46897b27":"df_yr=pd.DataFrame(df_customer_order_rev.groupby(['purcahse_yr'])['payment_value'].sum()).reset_index()","ba9b6746":"# Now we can visualise the amount purcahsed within each year\nplt.figure(figsize=(12,8))\nbars= plt.bar(df_yr['purcahse_yr'].astype(str),df_yr['payment_value'])\n# access the bar attributes to place the text in the appropriate location\nfor bar in bars:\n    yval = bar.get_height()\n    plt.text(bar.get_x(), yval + 50000, int(yval), fontsize=14)\nplt.show()","2882d1f4":"\n## we can see total unique  customers in each year\n\ndf_cust_yr=df_customer_order_rev.groupby('purcahse_yr')['customer_unique_id'].nunique().reset_index()","6bf1a5e4":"plt.figure(figsize=(12,8))\nbars= plt.bar(df_cust_yr['purcahse_yr'].astype(str),df_cust_yr['customer_unique_id'])\n# access the bar attributes to place the text in the appropriate location\nfor bar in bars:\n    yval = bar.get_height()\n    plt.text(bar.get_x(), yval + 5, int(yval), fontsize=14)\nplt.show()","4cd2de70":"## we create bins of customers in each year\ndf_2016= df_customer_order_rev[df_customer_order_rev['purcahse_yr']==2016]['customer_unique_id'].unique()\ndf_2017= df_customer_order_rev[df_customer_order_rev['purcahse_yr']==2017]['customer_unique_id'].unique()\ndf_2018= df_customer_order_rev[df_customer_order_rev['purcahse_yr']==2018]['customer_unique_id'].unique()","43544528":"## the number of customers moved from 2016 to 2017 and 2018\nlis16_17=[]\nlis16_18=[]\nfor cust in list(df_2016):\n    if cust in list(df_2017):\n        lis16_17.append(cust)\n    if cust in list(df_2018):\n        lis16_18.append(cust)","3bc9ebda":"## the number of customers moved from 2017 to 2018 \nlis17_18=[]\nfor cust in list(df_2017):\n    if cust in list(df_2018):\n        lis17_18.append(cust)","4c4d7f7f":"## we are going to see number of customers leaving after each year\ndf_comp=pd.DataFrame({'labels':['Customers in the year','Customers from 2016','Customers from 2017'],\n               '2016':[len(list(df_2016)),0,0],        \n                '2017':[len(list(df_2017)),len(lis16_17),0],   \n                 '2018':[len(list(df_2018)),len(lis16_18),len(lis17_18)]})","42308d10":"df_comp","b27190df":"plt.figure(figsize=(15,8))\nN = len(df_comp)\nind = np.arange(N)\nwidth = 0.4\np1 = plt.bar(ind, df_comp.iloc[0,1:], width, color='r')\np2 = plt.bar(ind, df_comp.iloc[1,1:], width, bottom=df_comp.iloc[0,1:], color='b')\np3 = plt.bar(ind, df_comp.iloc[2,1:], width, \n             bottom=np.array(df_comp.iloc[0,1:])+np.array(df_comp.iloc[1,1:]), color='g')\n\n\nplt.yticks(fontsize=12)\nplt.xticks(ind, ['2016','2017','2018'], fontsize=12, rotation=90)\n\nplt.legend((p1[0], p2[0], p3[0]), (df_comp.iloc[0,0], df_comp.iloc[1,0], df_comp.iloc[2,0]), fontsize=12, ncol=4, framealpha=0, fancybox=True)\nplt.show()\nplt.show()\n\n","54dfa4c6":"df_customer_order['purchase_month']=pd.to_datetime(df_customer_order['order_purchase_timestamp']).dt.to_period('M').dt.to_timestamp()","38c7c9f6":"## we find the number of customers purchased in each month\ndf_cust_num=df_customer_order.groupby(['purchase_month'])['customer_unique_id'].count().reset_index().rename(columns={'customer_unique_id':'num_customers'})\ndf_cust_num.head()","829e2454":"## plotting monthly sales data\nplt.figure(figsize=(15,8))\n\n\nplt.plot(df_cust_num['purchase_month'],df_cust_num['num_customers'])\nplt.show()","78cc00c8":"#create a new dataframe to model the difference\ndf_diff = df_cust_num.copy()\n#add previous sales to the next row\ndf_diff['prev_num_customers'] = df_diff['num_customers'].shift(1)\n#drop the null values and calculate the difference\ndf_diff = df_diff.dropna()\ndf_diff['diff'] = (df_diff['num_customers'] - df_diff['prev_num_customers'])\n## the diff shows the difference in customers between consecutive months\ndf_diff.head(10)","3199cbf8":"## plotting sales difference\n## plotting monthly sales data\nplt.figure(figsize=(15,8))\n\n\nplt.plot(df_diff['purchase_month'],df_diff['diff'])\nplt.show()","163606eb":"#create dataframe for transformation from time series to supervised\ndf_supervised = df_diff.drop(['prev_num_customers'],axis=1)#adding lags\nfor inc in range(1,13):\n    field_name = 'lag_' + str(inc)\n    df_supervised[field_name] = df_supervised['diff'].shift(inc)\n    #drop null values\ndf_supervised = df_supervised.dropna().reset_index(drop=True)","c9021032":"## we ceated 12 time lag column of the diff as the features for training our model\ndf_supervised.head()","cafe64de":"#import MinMaxScaler and create a new dataframe for LSTM model\ndf_model = df_supervised.drop(['purchase_month','num_customers'],axis=1)\n#split train and test set\ntrain_set, test_set = df_model[0:-6].values, df_model[-6:].values","5a633824":"#apply Min Max Scaler\nscaler = MinMaxScaler(feature_range=(-1, 1))\nscaler = scaler.fit(train_set)\n# reshape training set\ntrain_set = train_set.reshape(train_set.shape[0], train_set.shape[1])\ntrain_set_scaled = scaler.transform(train_set)# reshape test set\ntest_set = test_set.reshape(test_set.shape[0], test_set.shape[1])\ntest_set_scaled = scaler.transform(test_set)","0239f3e2":"## after scaling the data we set the training and testing samples\nX_train_lstm, y_train_lstm = train_set_scaled[:, 1:], train_set_scaled[:, 0:1]\nX_train_lstm = X_train_lstm.reshape(X_train_lstm.shape[0], 1, X_train_lstm.shape[1])\nX_test_lstm, y_test_lstm = test_set_scaled[:, 1:], test_set_scaled[:, 0:1]\nX_test_lstm = X_test_lstm.reshape(X_test_lstm.shape[0], 1, X_test_lstm.shape[1])","e22a5d91":"## buliding a LSTM model\nmodel_lstm = Sequential()\nmodel_lstm.add(LSTM(4, batch_input_shape=(1, X_train_lstm.shape[1], X_train_lstm.shape[2]), stateful=True))\nmodel_lstm.add(Dense(1))\nmodel_lstm.compile(loss='mean_squared_error', optimizer='adam')\nmodel_lstm.fit(X_train_lstm, y_train_lstm, epochs=1, batch_size=1, verbose=1, shuffle=False)","139d181b":"y_pred = model_lstm.predict(X_test_lstm,batch_size=1)","0c75b7a9":"#reshape y_pred\ny_pred = y_pred.reshape(y_pred.shape[0], 1, y_pred.shape[1])#rebuild test set for inverse transform\npred_test_set = []\nfor index in range(0,len(y_pred)):\n    print (np.concatenate([y_pred[index],X_test_lstm[index]],axis=1))\n    pred_test_set.append(np.concatenate([y_pred[index],X_test_lstm[index]],axis=1))#reshape pred_test_set\npred_test_set = np.array(pred_test_set)\npred_test_set = pred_test_set.reshape(pred_test_set.shape[0], pred_test_set.shape[2])#inverse transform\npred_test_set_inverted = scaler.inverse_transform(pred_test_set)","69c79f53":"#create dataframe that shows the predicted sales\nresult_list = []\nsales_dates = list(df_cust_num[-7:].purchase_month)\nact_sales = list(df_cust_num[-7:].num_customers)\nfor index in range(0,len(pred_test_set_inverted)):\n    result_dict = {}\n    result_dict['pred_value'] = int(pred_test_set_inverted[index][0] + act_sales[index])\n    result_dict['date'] = sales_dates[index+1]\n    result_list.append(result_dict)\ndf_result = pd.DataFrame(result_list)","d1e22f10":"## plotting monthly sales data\nplt.figure(figsize=(15,8))\n\nplt.plot(df_cust_num['purchase_month'],df_cust_num['num_customers'])\nplt.plot(df_result['date'],df_result['pred_value'])\nplt.legend(['Actual','Predicted'])\nplt.show()","e0df38ca":"r2_score(list(df_cust_num[-6:].num_customers),df_result['pred_value'].values.tolist())","bcadcd52":"## we implement the best model Random Forest Classifier we got as output for customer purcahse prediction\n##splitting to train and test dataset\nX_train,X_test,y_train,y_test=train_test_split(dataset_full.iloc[:,:-1],dataset_full.iloc[:,-1], test_size=0.2, random_state=31)\nxgc = xgb.XGBClassifier(n_estimators=500, max_depth=5, base_score=0.5,\n                        objective='binary:logistic', random_state=42)\nxgc.fit(X_train, y_train)","741e0760":"perm = PermutationImportance(xgc, random_state=1).fit(X_test, y_test)\neli5.show_weights(perm, feature_names = X_test.columns.tolist())","2019676d":"interpreter = Interpretation(training_data=X_test, training_labels=y_test, \n                             feature_names=list(X_test.columns))\nim_model = InMemoryModel(xgc.predict_proba, examples=X_train, \n                         target_names=['no purcahse', 'purchase'])\nplots = interpreter.feature_importance.plot_feature_importance(im_model, ascending=True, \n                                                               n_samples=23000)","72ae5acf":"###2. for customer LTV prediction\n#create X and y, X will be feature set and y is the label - LTV\nX = dataset2.drop(['LTVCluster','payment_value','customer_unique_id'],axis=1)\ny = dataset2['LTVCluster']\n\n#split training and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.05, random_state=56)\n\n#XGBoost Multiclassification Model\nltv_xgb_model = xgb.XGBClassifier(max_depth=5, learning_rate=0.1,objective= 'multi:softprob',n_jobs=-1).fit(X_train, y_train)\n","f957f783":"perm = PermutationImportance(ltv_xgb_model, random_state=1).fit(X_test, y_test)\neli5.show_weights(perm, feature_names = X_test.columns.tolist())","ec544efc":"interpreter = Interpretation(training_data=X_test, training_labels=y_test, \n                             feature_names=list(X_test.columns))\nim_model = InMemoryModel(ltv_xgb_model.predict_proba, examples=X_train, \n                         target_names=['low ltv', 'mid ltv','high ltv'])\nplots = interpreter.feature_importance.plot_feature_importance(im_model, ascending=True, \n                                                               n_samples=23000)","76ece231":"## for calculating customer life time value, we use the same data from LTV prediction with RFM values\n## we first find the average order value\ndataset2['avg_purchase_val']= dataset2['total_amount']\/dataset2['frequency']\n## now find  purcahse frequency\ndataset2['purchase_freq']=sum(dataset2['frequency'])\/dataset2.shape[0]\n## Repeat Rate\ndataset2['repeat_rate']=dataset2[dataset2['frequency']>1].shape[0]\/dataset2.shape[0]\n##churn rate\ndataset2['churn_rate']=1-dataset2['repeat_rate']\n","67ad6a84":"## now customer value\n##AOV= average order value\n## CLTV = [AOV * Purchase Frequency\/Churn Rate] * Profit margin.\n# Profit Margin\n##l ets assume there is a 25% profit margin from sales\ndataset2['profit_margin']=dataset2['total_amount']*0.25\n# Customer Value (CLTV)\ndataset2['CLTV']=((dataset2['avg_purchase_val']*dataset2['purchase_freq'])\/dataset2['churn_rate'])*dataset2['profit_margin']","39310bca":"#remove outliers\ndataset2 = dataset2[dataset2['avg_purchase_val']<dataset2['avg_purchase_val'].quantile(0.99)]\n\n\n#creating 3 clusters\nkmeans = KMeans(n_clusters=5)\nkmeans.fit(dataset2[['CLTV']])\ndataset2['CLTVCluster'] = kmeans.predict(dataset2[['CLTV']])\n\n#order cluster number based on CLTV\ndataset2 = order_cluster('CLTVCluster', 'CLTV',dataset2,True)\n\n#creatinga new cluster dataframe\ndataset3= dataset2.copy()\n\n#see details of the clusters\ndataset3.groupby('CLTVCluster')['CLTV'].describe()","bddd11c9":"## we create a model for predicting the cluster of cltv of customers\n## for this we drop ltvcluster and amount vlaues we used in cltv calculation\nX = dataset3.drop(['LTVCluster','total_amount','avg_purchase_val','customer_unique_id','profit_margin','CLTV','CLTVCluster'],axis=1)\ny = dataset3['CLTVCluster']\n\n#split training and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.05, random_state=56)\n\n#XGBoost Multiclassification Model\nltv_xgb_model = xgb.XGBClassifier(max_depth=5,objective= 'multi:softprob', learning_rate=0.1,n_jobs=-1).fit(X_train, y_train)\n","46cc34b5":"accuracy_score(y_test,ltv_xgb_model.predict(X_test))","bf16fec0":"perm = PermutationImportance(ltv_xgb_model, random_state=1).fit(X_test, y_test)\neli5.show_weights(perm, feature_names = X_test.columns.tolist())","3f0cec9c":"interpreter = Interpretation(training_data=X_test, training_labels=y_test, \n                             feature_names=list(X_test.columns))\nim_model = InMemoryModel(ltv_xgb_model.predict_proba, examples=X_train, \n                         target_names=['low cltv', 'mid cltv','high cltv'])\nplots = interpreter.feature_importance.plot_feature_importance(im_model, ascending=True, \n                                                               n_samples=23000)","ce19e120":"### correlation matrix for best customers","5abe666e":"## Descriptive Statistics of Sellers\n<a class=\"anchor\" id=\"fifth-bullet\"><\/a>","11967bee":"The correlation is coming as nan since the quartile values are all 4 and standard deviation among values is zero","27f7a98d":"There are nan values in correlation values of quartile ranges ,it is becuase the standard deviation is zero since values are all 4","70cdfe29":"### Deep learning Model\n<a class=\"anchor\" id=\"tenth-bullet\"><\/a>","e699b499":"we get diff of the test set from the predicion for the last 6 month. we will inverse transform it to find the predicted number of customers in that 6 months","be29ba64":"the r2 score is -0.47 and is better at estimating the number of customers in the coming months","241f87ad":"The customers are split into 3 clusters and we can see that about 95% of the people are in cluster zero that is in the low CLTV","28bef8c5":"the accuracy of the model is coming as 99%. if we check the number of lower LTV customers they are 78416, which accounts to over 99% of the customers in our data. so the cluster prediction model is of no much use in this scenrio, as majority are one time customers","991a960d":"we are going to do binary classification to predict whether a customer will purchase within the next 6 months\nSince this is a classification problem we use several models here , For comparing the models we use metrics like r2 score and accuracy score.<br>\nThe models used here are:<br>\n    1. Linear Regression\n    2. Random Forest Classifier\n    3. Extra Trees Classifier\n    4. Gradient Boost Classifier\n    5. K nearest nerighbour Classifier","b86635d4":"In this we shuffle a column of the test set and see how it affects the accuracy of the prediction model<br>\nwe will do this in one model we have created to see which features are important and how shuffling them changes accuracy<br>\n1. Xgboost classifier for purchase  binary prediction\n2. Xgooost classifier for customer LTV cluster prediction\n","dcf6b37c":"Around 272 same sellers were highest reviewed and fastes delivering customers","bae48203":"The same scoring method from RFM analysis can be applied here too\n1. the higher the average review good seller\n2. lower the average delivery time to carrier good seller\n3. Higher the sales amount - good seller\n4. Higher the number of sales - good seller","ed69960b":"we can see than only 5 customers from 2016 was followed into 2017 for a purchase, all other customers were new<br>\nIn the case of 2017 and 2018 , the customer base got bigger, but then also only 682 customers from 2017 made purchase in 2018<br>\n\nLike our previous analysis majority of the site's customer were one time and new customers","d21d6b4e":"The customers who bought items in that 6 months were given 1 and others were given 0. these values will act as binary classification for our prediction model","2625c401":"#### Correaltion map of best seller","14a7967b":"## Table of Contents\n1.[Loading all packages and dataset](#first-bullet)<br>\n2 [Customer Segmentation with RFM analysis](#second-bullet)<br>\n3.[Seller Prioritisation](#third-bullet)<br>\n4.[Descriptive statistics of Customer](#fourth-bullet)<br>\n5.[Descriptive statistics of Seller](#fifth-bullet)<br>\n6.[Customer purchase prediction](#sixth-bullet)<br>\n&nbsp;&nbsp;&nbsp;&nbsp;6.1.[Data processing](#seventh-bullet)<br>\n&nbsp;&nbsp;&nbsp;&nbsp;6.2.[Feature Engineering](#eigth-bullet)<br>\n&nbsp;&nbsp;&nbsp;&nbsp;6.3.[Comparison of ML models](#nineth-bullet)<br>\n&nbsp;&nbsp;&nbsp;&nbsp;6.4.[Deep learning model](#tenth-bullet)<br>\n7.[Customer Cluster value prediction](#eleventh-bullet)<br>\n8.[Concluding Questions](#twelth-bullet)<br>\n9.[Forecating number of customers in coming months](#thirteen-bullet)<br>\n10.[Calcuate permutation importance](#fourteen-bullet)<br>\n11.[CLTV Model Creation](#fifteen-bullet)","061e7a59":"##### 4. Sales by Customer State","96642708":"### Feature Engineering \n<a class=\"anchor\" id=\"eigth-bullet\"><\/a>","065e9637":"We can see a steady increase in the purchase  done by the customers from the site. the revenue purchased increased to 8699763 in 2018.","b4f54163":"we can see that around 97% of customers are one time customers  and only 3% where recurring customers","afe84cf2":"##### 2. Statistics of total number of  Sales by Seller","0d50236b":"### 1.How much we earn from a customer pr year","50cbd527":"## Loading packages and dataset\n<a class=\"anchor\" id=\"first-bullet\"><\/a>","67bc443d":"### Calcuate permutation importance\n<a class=\"anchor\" id=\"fourteen-bullet\"><\/a>","189723a6":"Sellers can be prioritised by these factors:\n    1. Number of sales of made in total\n    2. Average time taken for seller to give to carrier\n    3. Amount made in total sales\n    4. Average review recived for the order","c3fac586":"### Concluding Questions\n<a class=\"anchor\" id=\"twelth-bullet\"><\/a>","6775c462":"We split the CLTV calculated into 3 customer based on\n1. Low CLTV\n2. Mid CLTV\n3. HIgh CLTV","9b9cf75e":"In predicting he customer LTV clsuter total amount purchased by the customer is of high importance and weighs around 25% of its importance","ef1d6951":"There are 611 cities and the total sales in each city is calculated and descriptive statistics is done","9c410bab":"\n\nWe will create two segmentation classes since, high recency is bad, while high frequency and monetary value is good.\n","4a1c85c9":"As around 97% of the customers were one time customers , most customer purchases didn't exceed one year","795efd0c":"## Descriptive Statistics of Customers \n<a class=\"anchor\" id=\"fourth-bullet\"><\/a>","a3f31dde":"## Customer Segmentation with RFM analysis\n<a class=\"anchor\" id=\"second-bullet\"><\/a>","7efb614f":"As r2 score reaches 1, the model is much capable of explaining the variance in purchase probability prediction of the customers. So Random Forest Classifier is the best classifier for the prediction.<br>\n\nThe gini coefficient of the forest classifier and Extratrees classifier is closer to one, meaning there is inequality in the predicted values.","e02af0f4":"Behavioral segmentation by 3 important features:\n\n    Recency \u2014 number of days since the last purchase\n    Frequency \u2014 number of transactions made over a given period\n    Monetary \u2014 amount spent over a given period of time","ce2162d9":"## Seller Prioritisation\n<a class=\"anchor\" id=\"third-bullet\"><\/a>","269e8a0f":"##### 1. Statistics of total buy amount by Seller","c183900b":"you can see around 78416 customers we have haven't made a purchsae in the next 6 months. The mid value cluster has 579 people only, and the high value LTV customes are only 463 .","1b42cb31":"### Forecating number of customers in coming months\n<a class=\"anchor\" id=\"thirteen-bullet\"><\/a>","e24b3bd7":"In the LTV classifier the total amunt and difference between first and last purchase days have high importance. the permutation values are also given. <br>\nthe monetary,frquency,recency quartile ranges are of zero importance and doesnt help in predictions","13783a85":"### CLTV Model Creation\n<a class=\"anchor\" id=\"fifteen-bullet\"><\/a>","1f73644d":"##### 3. Sales by Customer City","37f94228":"#### RFM Quartiles\nThe simplest way to create customers segments from RFM Model is to use Quartiles. We assign a score from 1 to 4 to Recency, Frequency and Monetary. Four is the best\/highest value, and one is the lowest\/worst value. A final RFM score is calculated simply by combining individual RFM score numbers.","a7622058":"### Evaluating Machine learning models\n<a class=\"anchor\" id=\"nineth-bullet\"><\/a>","5ad60081":"##### 1. Statistics of total buy amount by Customer","28003d3a":"Recency is the the most important feature in finding purchase binary prediction and shuffling this columns produced the most change of 0.0201+\/-0.0034.<br>\nThe second improtant feature in the xgboost classifier model is the difference between first purchse and today in days","297fb6a5":"Active and inactive customers can be found by this analysis method ","a06bfce5":"##### 4. Sales by Seller's State","ac92a1e6":"We are going to create a time series model to predict the number of customer in the ","9a006150":"the test set consist of the last 6 months of data ","5f94fe55":"we can see a dip in customers in the 3rd quarter. there was a reduction of around 6000 customers","b8cd4acd":"### 2.How many customers leave after an year","d8d6e4e2":"for the regressor model we created to predict the cltv of customers we can see frequency has the highest feature importance.We can see that feature like quartile ranges and churn rate has zero  importance","44efe8c6":"There are 4119 cities and the total sales in each city is calculated and descriptive statistics is done","f896fe91":"##### 2. Statistics of total number of  Sales by Customer","d1be34ee":"#### correlation matrix for LTV customers","aa9f2e93":"Here also we can see that both recency and diff_first_today has same feature importance. These two feature have around 30% importance each on predicting customer purchase","303999a3":"we can see the number of customers increased rapidly over the years<br>\nNow we will see how many customers from the previous years moved into the next years","9526c0ee":"# Life Time Value Analysis\n### (Brazilian E-Commerce Public Dataset by Olist)","df0e5efc":"### Customer lifetime value (LTV) cluster prediction\n<a class=\"anchor\" id=\"eleventh-bullet\"><\/a>","3a981583":"R2 score is very low (near zero) and negative. so the model is not so good at explaining the variance of the purchase predictive probability","8730a002":"We are going to sperate out 180 days (last 6 months) from the maximum day of purchase by customers out of the dataset. 2018\/4 to 2018\/10. <br>\nWe are using that data to predict whether the customer made a purchase in that  period. We are going to use data until 2018\/4  to make that prediction","50e7984d":"##### 3. Sales by Seller's City","8519b761":"### Dataset preparation\n<a class=\"anchor\" id=\"seventh-bullet\"><\/a>","809df265":"Lifetime Value: Total Gross Revenue - Total Cost\n        \nSInce total cost by each customer is not given we take Revenue as the life time value of the customer","b8804613":"We made 3 cluster for life time values<br>\n    Low LTV -cluster 0<br>\n    Mid LTV -cluster 1<br>\n    High LTV-cluster 2","443a35c1":"## Customer purchase prediction\n<a class=\"anchor\" id=\"sixth-bullet\"><\/a>"}}