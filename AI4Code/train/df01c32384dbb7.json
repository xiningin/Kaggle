{"cell_type":{"bfcf0907":"code","856f171a":"code","271374c0":"code","5cfaeab0":"code","1a641fda":"code","d89607c3":"code","749c0f7d":"code","c7a6d0f8":"code","c07a01d1":"code","fbd9380f":"code","b124d20b":"code","6a8c33b5":"code","1c2aac57":"code","e31f8700":"code","4e68f881":"code","b94183ba":"code","e5e7930d":"code","75ddb5a7":"code","a2d8a275":"markdown","b31aa52b":"markdown","36930afe":"markdown","e4d34901":"markdown","b5372e14":"markdown","79c55270":"markdown"},"source":{"bfcf0907":"import pandas as pd\nimport numpy as np\n\nSEED = 1234\n\n\ndata = pd.read_csv(\"\/kaggle\/input\/chinese-mnist-digit-recognizer\/chineseMNIST.csv\")\ndata.head()","856f171a":"data.shape","271374c0":"data[['label', 'character']].drop_duplicates()","5cfaeab0":"map_label = {label: i for i, label in enumerate(data.label.unique())}\ndata.label = data.label.map(map_label)","1a641fda":"map_label","d89607c3":"data.head()","749c0f7d":"data.label.unique()","c7a6d0f8":"from sklearn.model_selection import train_test_split\n\ntrain_df, val_df = train_test_split(data.drop('character', axis=1), train_size=0.8, random_state=SEED)\ntrain_df.shape, val_df.shape","c07a01d1":"train_df.label.nunique()","fbd9380f":"train_df.head()","b124d20b":"y_train = train_df.label\nX_train = train_df.drop('label', axis=1)\n\ny_val = val_df.label\nX_val = val_df.drop('label', axis=1)","6a8c33b5":"import time\n\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.metrics import accuracy_score","1c2aac57":"dict_model = {\n    \"SVC\": SVC,\n    \"RF\": RandomForestClassifier,\n}","e31f8700":"\"\"\"dict_pred = {}\nfor model_str, model in dict_model.items():\n    start = time.time()\n    dict_pred[model_str] = {}\n    try:\n        model = model(probability=True)\n    except TypeError:\n        model = model()\n    model.fit(X_train, y_train)\n    y_pred_prob = model.predict_proba(X_val)\n    dict_pred[model_str]['y_pred_prob'] = y_pred_prob\n    accuracy = accuracy_score(np.argmax(y_pred_prob, axis=1), y_val)\n    dict_pred[model_str]['accuracy'] = accuracy\n    print(f\"{round(time.time()-start, 2)}s\")\n    print(f\"{model_str}: accuracy={accuracy}\")\n\n    \ny_pred_prob_ensemble = np.average([dict_pred[mdl][\"y_pred_prob\"] for mdl in dict_model.keys()], axis=0)\n\naccuracy_ensemble = accuracy_score(np.argmax(y_pred_prob_ensemble, axis=1), y_val)\nprint(accuracy_ensemble)\"\"\"","4e68f881":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import regularizers\n\nshape = 64\n\nX_val = np.array(X_val)\nX_train = np.array(X_train)\n\nX_train = X_train.reshape(-1, shape, shape)\nX_val = X_val.reshape(-1, shape, shape)\n\nX_train = X_train.astype(\"float32\") \/ 255.0\nX_val = X_val.astype(\"float32\") \/ 255.0\n\nX_val = np.expand_dims(X_val, -1)\nX_train = np.expand_dims(X_train, -1)\n\nnum_classes = len(set(y_train))\ny_train = keras.utils.to_categorical(y_train, num_classes)\ny_val = keras.utils.to_categorical(y_val, num_classes)","b94183ba":"input_shape = (shape, shape, 1)\n\nmodel = keras.Sequential([\n    # Input layer\n    keras.Input(shape=input_shape, name='input'),\n    \n    layers.Conv2D(filters=32, kernel_size=5, strides=1, \n                  activation='relu', kernel_regularizer=regularizers.l2(0.0005), name='conv1'),\n    \n    layers.Conv2D(filters=32, kernel_size=5, strides=1, use_bias=False, name='conv2'),\n    \n    layers.BatchNormalization(),\n    layers.Activation('relu'),\n    layers.MaxPool2D(pool_size=2, strides=2, name='pool1'),\n    layers.Dropout(0.25),\n    \n    layers.Conv2D(filters=64, kernel_size=3, activation='relu', \n                  kernel_regularizer=regularizers.l2(0.0005), name='conv3'),\n    \n    layers.Conv2D(filters=64, kernel_size=3, use_bias=False, name='conv4'),\n    \n    layers.BatchNormalization(),\n    layers.Activation('relu'),\n    layers.MaxPool2D(pool_size=2, strides=2, name='pool2'),\n    layers.Dropout(0.25),\n    \n    layers.Flatten(name='flatten'),\n    layers.Dense(256, use_bias=False, name='dense1'),\n    layers.BatchNormalization(),\n    layers.Activation('relu'),\n    layers.Dropout(0.25),\n    \n    layers.Dense(128, use_bias=False, name='dense2'),\n    layers.BatchNormalization(),\n    layers.Activation('relu'),\n\n    layers.Dense(num_classes, activation='softmax', name='output')\n])","e5e7930d":"model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n\nmodel.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=50, batch_size=128)","75ddb5a7":"## Would be interesting to try data augmentation","a2d8a275":"## Almost the same script used on the classical MNIST dataset https:\/\/www.kaggle.com\/guillaumes\/sklearn-ensembling-keras-cnn","b31aa52b":"### Keras model","36930afe":"### ML modeling","e4d34901":"### data exploration","b5372e14":"### data importation","79c55270":"### data preprocessing"}}