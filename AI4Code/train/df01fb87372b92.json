{"cell_type":{"512be654":"code","5b471228":"code","453fff50":"code","6123512f":"code","b1eb435d":"code","99a7a507":"code","a72db2b6":"code","ab51dd4f":"code","39608d9c":"code","7d2c8d0b":"code","b760b957":"code","3e9a963d":"code","a8de9679":"code","95e7597d":"code","cdb92064":"code","93273dba":"code","468eecc8":"code","a0affa05":"code","77739558":"code","47be85a3":"code","7d9fd0f4":"code","97134ce0":"code","7b3d3919":"code","ddb2672d":"code","23a9e062":"code","dd9f7f80":"code","de273a89":"code","8f1fc2ea":"code","231dcbb4":"code","279e16dd":"markdown","b0163700":"markdown","c6fb4487":"markdown","bc80e0dc":"markdown","635c07a0":"markdown","6f53ae9f":"markdown","8c1ecc79":"markdown","1f42ccac":"markdown","7e2df6f1":"markdown","e877b018":"markdown","fb87d749":"markdown","7689f553":"markdown","29951653":"markdown","34ccd738":"markdown","b91fda59":"markdown","a32b4dfe":"markdown","fe826b5f":"markdown","b265755c":"markdown","8a97c36c":"markdown","24a0cebf":"markdown","875d0a7e":"markdown","ac237ab7":"markdown","20384324":"markdown","647c6d34":"markdown","584e300e":"markdown","836d8fa7":"markdown","7b22ec7e":"markdown","9adade10":"markdown","d7a90a2e":"markdown","890d1145":"markdown","98ce8a7f":"markdown","ea38b776":"markdown","105b9e6d":"markdown","93fceb2e":"markdown","68d4e19b":"markdown","d67e4f7a":"markdown","1a9c4706":"markdown","11b5fd80":"markdown","3ce32f4c":"markdown","8cd19424":"markdown","42863961":"markdown","ddd0de0d":"markdown"},"source":{"512be654":"import pandas as pd\nimport numpy as np\nimport scipy.stats as st\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import plot_confusion_matrix, accuracy_score, f1_score, make_scorer, precision_score, recall_score, balanced_accuracy_score\nfrom sklearn.model_selection import ShuffleSplit, train_test_split, GridSearchCV, cross_val_score, RepeatedStratifiedKFold, learning_curve, cross_validate, StratifiedKFold, RandomizedSearchCV, KFold\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.decomposition import PCA\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder\nfrom sklearn.svm import SVC, NuSVC\nfrom sklearn.feature_selection import mutual_info_classif, SelectKBest, chi2\nfrom sklearn.ensemble import RandomForestClassifier, VotingClassifier, AdaBoostClassifier\nimport xgboost as xgb\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.models import clone_model\nfrom tqdm.notebook import tqdm\nfrom sklearn.experimental import enable_iterative_imputer  # noqa\nfrom sklearn.impute import IterativeImputer\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom IPython.display import Image\nfrom IPython.core.display import HTML ","5b471228":"\"\"\"Data cleaning function\"\"\"\ndef clean_df(dataframe):\n    \n    # Convert the cabins column to number of cabins\n    for j in range(dataframe.shape[0]):\n        c = 1\n        if type(dataframe['Cabin'][j]) != float:\n            for i in range(len(str(dataframe.loc[j, 'Cabin']))):\n                if dataframe['Cabin'][j][i] == ' ':\n                    c += 1\n            dataframe.loc[j, 'Cabin'] =  str(c)\n    dataframe['Cabin'] = dataframe['Cabin'].fillna('0')\n    dataframe['Cabin'] = dataframe['Cabin'].astype(int)\n    dataframe.rename(columns = {'Cabin':'Cabins'}, inplace = True)\n    \n    # Correct fare zero values\n    dataframe.Fare.replace(0, np.nan, inplace=True)\n    \n    # Extract title from name\n    dataframe['Honorifics'] = dataframe['Name'].apply(lambda st: st[st.find(\",\")+2:st.find(\".\")])\n    dataframe['Honorifics'] = dataframe.Honorifics.replace('Mlle', 'Miss')\n    dataframe['Honorifics'] = dataframe.Honorifics.replace('Ms', 'Miss')\n    dataframe['Honorifics'] = dataframe.Honorifics.replace('Mme', 'Mrs')\n    dataframe['Honorifics'] = dataframe.Honorifics.replace('the Countess', 'Countess')\n    \n    # Calculate family size and fare per person\n    dataframe['FamilySize'] = dataframe.SibSp + dataframe.Parch\n    dataframe['FarePerPerson'] = dataframe.Fare \/ (dataframe.FamilySize+1)\n    \n    # Drop the rows where the Embarked column is empty\n    idc = list(dataframe.loc[dataframe['Embarked'].isna()].index)\n    dataframe.drop(index=idc, axis=1, inplace=True)\n\n    # Drop the PassengerId, ... columns\n    dataframe.drop(['PassengerId', 'Name', 'Ticket'], axis=1, inplace=True)\n\n    return dataframe\n\n\"\"\"Mutual Information scores\"\"\"\ndef mutual_info(X, y):\n\n    Xt = X.copy()\n\n    # Label encoding for categoricals\n    for colname in Xt.select_dtypes(\"object\"):\n        Xt[colname], _ = Xt[colname].factorize()\n\n    # All discrete features should now have integer dtypes (double-check this before using MI!)\n    discrete_features = Xt.dtypes == int\n\n    # Impute missing values\n    Xt = SimpleImputer().fit_transform(Xt)\n\n    # Calculate mi scores\n    mi_scores = mutual_info_classif(Xt, y, discrete_features=discrete_features)\n    mi_scores = pd.Series(mi_scores, name=\"MI Scores\", index=X.columns)\n    mi_scores = mi_scores.sort_values(ascending=False)[:8]\n    \n    # Plot\n    plt.figure(dpi=100, figsize=(8, 5))\n    scores = mi_scores.sort_values(ascending=True)\n    width = np.arange(len(scores))\n    ticks = list(scores.index)\n    plt.barh(width, scores)\n    plt.yticks(width, ticks)\n    plt.title(\"Mutual Information Scores\")\n    plt.show()\n    \n    pass\n\n\"\"\"Learning curves plotter\"\"\"\ndef plot_learning_curve(estimator, title, X, y, axes=None, ylim=None, cv=None,\n                        n_jobs=None, train_sizes=np.linspace(.1, 1.0, 5)):\n    \"\"\"\n    Generate 3 plots: the test and training learning curve, the training\n    samples vs fit times curve, the fit times vs score curve.\n\n    Parameters\n    ----------\n    estimator : estimator instance\n        An estimator instance implementing `fit` and `predict` methods which\n        will be cloned for each validation.\n\n    title : str\n        Title for the chart.\n\n    X : array-like of shape (n_samples, n_features)\n        Training vector, where ``n_samples`` is the number of samples and\n        ``n_features`` is the number of features.\n\n    y : array-like of shape (n_samples) or (n_samples, n_features)\n        Target relative to ``X`` for classification or regression;\n        None for unsupervised learning.\n\n    axes : array-like of shape (3,), default=None\n        Axes to use for plotting the curves.\n\n    ylim : tuple of shape (2,), default=None\n        Defines minimum and maximum y-values plotted, e.g. (ymin, ymax).\n\n    cv : int, cross-validation generator or an iterable, default=None\n        Determines the cross-validation splitting strategy.\n        Possible inputs for cv are:\n\n          - None, to use the default 5-fold cross-validation,\n          - integer, to specify the number of folds.\n          - :term:`CV splitter`,\n          - An iterable yielding (train, test) splits as arrays of indices.\n\n        For integer\/None inputs, if ``y`` is binary or multiclass,\n        :class:`StratifiedKFold` used. If the estimator is not a classifier\n        or if ``y`` is neither binary nor multiclass, :class:`KFold` is used.\n\n        Refer :ref:`User Guide <cross_validation>` for the various\n        cross-validators that can be used here.\n\n    n_jobs : int or None, default=None\n        Number of jobs to run in parallel.\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n    train_sizes : array-like of shape (n_ticks,)\n        Relative or absolute numbers of training examples that will be used to\n        generate the learning curve. If the ``dtype`` is float, it is regarded\n        as a fraction of the maximum size of the training set (that is\n        determined by the selected validation method), i.e. it has to be within\n        (0, 1]. Otherwise it is interpreted as absolute sizes of the training\n        sets. Note that for classification the number of samples usually have\n        to be big enough to contain at least one sample from each class.\n        (default: np.linspace(0.1, 1.0, 5))\n    \"\"\"\n    if axes is None:\n        _, axes = plt.subplots(1, 3, figsize=(20, 5))\n\n    axes[0].set_title(title)\n    if ylim is not None:\n        axes[0].set_ylim(*ylim)\n    axes[0].set_xlabel(\"Training examples\")\n    axes[0].set_ylabel(\"Score\")\n\n    train_sizes, train_scores, test_scores, fit_times, _ = \\\n        learning_curve(estimator, X, y, cv=cv, n_jobs=n_jobs,\n                       train_sizes=train_sizes,\n                       return_times=True)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    fit_times_mean = np.mean(fit_times, axis=1)\n    fit_times_std = np.std(fit_times, axis=1)\n\n    # Plot learning curve\n    axes[0].grid()\n    axes[0].fill_between(train_sizes, train_scores_mean - train_scores_std,\n                         train_scores_mean + train_scores_std, alpha=0.1,\n                         color=\"r\")\n    axes[0].fill_between(train_sizes, test_scores_mean - test_scores_std,\n                         test_scores_mean + test_scores_std, alpha=0.1,\n                         color=\"g\")\n    axes[0].plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n                 label=\"Training score\")\n    axes[0].plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n                 label=\"Cross-validation score\")\n    axes[0].legend(loc=\"best\")\n\n    # Plot n_samples vs fit_times\n    axes[1].grid()\n    axes[1].plot(train_sizes, fit_times_mean, 'o-')\n    axes[1].fill_between(train_sizes, fit_times_mean - fit_times_std,\n                         fit_times_mean + fit_times_std, alpha=0.1)\n    axes[1].set_xlabel(\"Training examples\")\n    axes[1].set_ylabel(\"fit_times\")\n    axes[1].set_title(\"Scalability of the model\")\n\n    # Plot fit_time vs score\n    axes[2].grid()\n    axes[2].plot(fit_times_mean, test_scores_mean, 'o-')\n    axes[2].fill_between(fit_times_mean, test_scores_mean - test_scores_std,\n                         test_scores_mean + test_scores_std, alpha=0.1)\n    axes[2].set_xlabel(\"fit_times\")\n    axes[2].set_ylabel(\"Score\")\n    axes[2].set_title(\"Performance of the model\")\n\n    return plt\n\n\"\"\"Stratified K-fold cross-validation\"\"\"\ndef sk_cross_val_scores(X, y, classifier):\n\n    skf = StratifiedKFold(n_splits=5)\n    accuracy = []\n\n    for tr_id, val_id in tqdm(skf.split(X,y)):\n\n        X_train, X_valid = X.iloc[tr_id], X.iloc[val_id]\n        y_train, y_valid = y.iloc[tr_id], y.iloc[val_id]\n\n        classifier.fit(X_train, y_train, verbose=-1)\n        prediction = classifier.predict(X_valid).round().flatten().astype(int)\n        score = accuracy_score(prediction, y_valid)\n        accuracy.append(score)\n    \n    mean_score = np.mean(np.array(accuracy))\n    std_score = np.std(np.array(accuracy))\n\n    return mean_score, std_score\n\n\"\"\"Correlation coefficient with target\"\"\"\ndef corrcoef(dataframe, target_column, correlation_method, width, height):\n\n    dft = dataframe.copy()\n    for colname in dft.select_dtypes(\"object\"):   \n        dft[colname], _ = dft[colname].factorize()\n        dft[colname].fillna(dft[colname].mean(), inplace = True)\n\n    corr = dft.corr(method=correlation_method)\n    x = corr[[target_column]].sort_values(by=target_column, ascending=False)\n    x = x[1:]\n    \n    plt.figure(figsize=(width,height))\n    plt.rcParams.update({'font.size': 14})\n    \n    sns.heatmap(data=x, annot = True, vmin=-1, vmax=1)\n    plt.title('Correlation Coefficient')\n    pass","453fff50":"path = '\/kaggle\/input\/titanic\/'\ndf = pd.read_csv(path + 'train.csv')\ndf.head(n=5)","6123512f":"df = clean_df(dataframe=df)","b1eb435d":"import matplotlib.gridspec as gridspec\n\n# letter sizes\n# Tsize = 20\n# tsize = 18\n# lsize = 16\n\n# set theme and title size\nsns.set_theme(style=\"darkgrid\")\n# plt.rcParams.update({'font.size': Tsize})\n\n# figure\nfig = plt.figure()\n# fig.suptitle('Features - Survival relationship')\nfig.set_figwidth(12)\nfig.set_figheight(20)\nfig.gca().set_aspect('equal', adjustable='box')\n\n# grid\nAX = gridspec.GridSpec(5,2)\nAX.update(wspace = 0.3, hspace = 0.3)\nfig.tight_layout()\n\nax1 = plt.subplot(AX[0,0])\nax2 = plt.subplot(AX[0,1])\nax3 = plt.subplot(AX[1,0])\nax4 = plt.subplot(AX[1,1])\nax5 = plt.subplot(AX[2,0])\nax6 = plt.subplot(AX[2,1])\nax7 = plt.subplot(AX[3,0])\nax8 = plt.subplot(AX[3,1])\nax9 = plt.subplot(AX[4,0:2])\nplt.xticks(rotation=45)\nplt.xticks(fontsize=14)\n\n# titles\n# ax1.set_title('Mean Fare vs Survival', fontsize=tsize)\n# ax2.set_title('Ticket class vs Survival', fontsize=tsize)\n# ax3.set_title('Family Size & Survival', fontsize=tsize)\n# ax4.set_title('Sex vs Survival', fontsize=tsize)\n# ax5.set_title('Number of cabins & Survival', fontsize=tsize)\n# ax6.set_title('Age & Survival', fontsize=tsize)\n# ax7.set_title('Embarkation port & Survival', fontsize=tsize)\n# ax8.set_title('Mean Fare per Person vs Survival', fontsize=tsize)\n# ax9.set_title('Honorifics vs Survival', fontsize=tsize)\n\n# plotting subplots\nsns.barplot(ax=ax1, data=df, x='Fare', y='Survived', capsize=.1, palette='Greens', orient=u'horizontal')\nsns.barplot(ax=ax2, data=df, x='Pclass', y='Survived', capsize=.1, palette='rocket')\nsns.boxplot(ax=ax3, data=df, x='FamilySize', y='Survived', orient=u'horizontal', order=[1,0])\nsns.violinplot(ax=ax4, data=df ,y='Survived', x='Sex', palette='coolwarm', scale='area')\nsns.kdeplot(ax=ax5, data=df, y='Survived',hue='Cabins', palette='coolwarm', multiple='stack')\nsns.kdeplot(ax=ax6, data=df, x='Age', y='Survived', fill=True, shade=True, color='tab:blue')\nport,_ = df.Embarked.factorize() # S=0, C=1, Q=2 \nsns.barplot(ax=ax7, data=df, x='Embarked', y='Survived', capsize=.1, palette='gnuplot2')\nsns.boxenplot(ax=ax8, data=df, x='Pclass', y='Fare', palette='rocket')\nsns.barplot(ax=ax9, \n            data=df, \n            x='Honorifics', \n            y='Survived', \n            order=list(df.\n                       groupby(by='Honorifics').\n                       mean()['Survived'].\n                       sort_values(ascending=False).\n                       index), \n           capsize=.2,\n           palette='turbo')\n\n# labels\nax1.set_xlabel('Mean Fare')\nax1.set_yticklabels(['no','yes'])\nax1.set_ylabel('Survived')\nax2.set_xlabel('Ticket Class')\nax2.set_ylabel('Probability of Survival')\nax3.set_yticks([1,0])\nax3.set_yticklabels(['no','yes'])\nax3.set_xlabel('Family Size')\nax4.set_yticks([0,1])\nax4.set_yticklabels(['no','yes'])\nax5.set_yticks([0,1])\nax5.set_yticklabels(['no','yes'])\nax5.set_xlabel('Probability Density');\nax6.set_xlabel('Age')\nax6.set_ylabel('Survived')\nax6.set_yticks([0,1])\nax6.set_yticklabels(['no','yes'])\nax7.set_xticks([0,1,2])\nax7.set_xticklabels(['Southampton','Cherbourg', 'Queenstown'])\nax7.set_ylabel('Probability of Survival')\nax8.set_xlabel('Ticket Class')\nax8.set_ylabel('Fare in $')\nax9.set_ylabel('Probability of Survival')\nplt.xticks(rotation=45);","99a7a507":"ci = st.norm.interval(alpha=0.95, \n                      loc=np.mean(df.loc[df.Pclass==1]['Survived']), \n                      scale=st.sem(df.loc[df.Pclass==1]['Survived']))\n\nprint(np.round(ci,3))","a72db2b6":"p_value = st.binom_test(x=df.loc[df.Pclass==1]['Survived'].sum(),\n                        n=df.loc[df.Pclass==1]['Survived'].shape[0], \n                        p=0.50, \n                        alternative='greater')\n\nif p_value > 0.05:\n    print('We fail to reject the null hypothesis')\nelse:\n    print('We reject the null hypothesis')","ab51dd4f":"yes = df.groupby(by='Pclass')['Survived'].sum()\nno = df.shape[0] - df.groupby(by='Pclass')['Survived'].sum()\ncontigency_table = np.vstack((np.array(yes),np.array(no))).T\n\np_value = st.chi2_contingency(observed=contigency_table, correction=True)[1]\n\nif p_value > 0.05:\n    print('We fail to reject the null hypothesis')\nelse:\n    print('We reject the null hypothesis')","39608d9c":"temp = df.copy()\n\nidc = list(temp.loc[temp['Embarked']=='Q'].index)\ntemp.drop(index=idc, axis=1, inplace=True)\n\nplt.figure(figsize=(15,5))\n\nplt.subplot(1,3,1)\ns,_ = temp.Sex.factorize() # male=0, female=1\nsns.barplot(data=temp, x='Embarked', y=s, capsize=.1, palette='gnuplot2')\nplt.ylabel('Female percentage')\nplt.xticks([0,1], ['Southampton','Cherbourg'])\n\nplt.subplot(1,3,2)\nsns.barplot(data=temp, x='Embarked', y='Pclass', capsize=.1, palette='gnuplot2')\nplt.ylabel('Average Ticket Class')\nplt.xticks([0,1], ['Southampton','Cherbourg'])\n\nplt.subplot(1,3,3)\nsns.barplot(data=temp, x='Embarked', y='Survived', hue='Sex', capsize=.1, palette='gnuplot2')\nplt.ylabel('Survival Probability')\nplt.xticks([0,1], ['Southampton','Cherbourg']);","7d2c8d0b":"mutual_info(X=df.drop(['Survived'], axis=1), y=df.Survived);","b760b957":"corrcoef(dataframe=df, target_column='Survived', correlation_method='pearson', \n         width=5, height=5)","3e9a963d":"df = pd.read_csv(path + 'train.csv')\ndata = 1 - df.groupby('Survived')['PassengerId'].count() \/ df.PassengerId.count()\nplt.style.use('fivethirtyeight')\npie, ax = plt.subplots(figsize=[10,6])\nlabels = data.keys()\nplt.pie(x=data, autopct=\"%.1f%%\", explode=[0.05]*2, labels=['Survived', 'Did not survive'], pctdistance=0.5, shadow=True)\nplt.title(\"Survival on the Titanic\", fontsize=14);","a8de9679":"df = pd.read_csv(path + 'train.csv')\ndf.head()","95e7597d":"df.describe()","cdb92064":"# Missing values per column\ndf.isna().sum()","93273dba":"df = clean_df(dataframe=df)\ndf.head()","468eecc8":"# Separate target from predictors\ny = df['Survived']\nX = df.drop(['Survived'], axis=1)\n\n# Select categorical columns\ncategorical_cols = [cname for cname in X.columns if X[cname].dtype in ['object']]\n\n# Select numerical columns\nnumerical_cols = [cname for cname in X.columns if X[cname].dtype in ['int64', 'float64']]\n\n# Preprocessing for numerical data\nnumerical_transformer = Pipeline(steps=[('scaler', MinMaxScaler()), \n                                        ('imputer', SimpleImputer())])\n\n# Preprocessing for categorical data\ncategorical_transformer = Pipeline(steps=[('onehot', OneHotEncoder(handle_unknown='ignore'))])\n\n# Bundle preprocessing for numerical and categorical data\npreprocessor = ColumnTransformer(transformers=[('num', numerical_transformer, numerical_cols), \n                                               ('cat', categorical_transformer, categorical_cols)])","a0affa05":"# RandomizedSearchCV\nn_iter = 10\ncv_rs = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\nverbose = 0\nrefit = True\nn_jobs = -1\n\n# Cross validation\ncv_cr = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\nscoring = 'accuracy'","77739558":"names = ['svm', 'raf', 'mlp', 'xgb', 'knn', 'ada']\nmodels = [SVC(random_state=0), \n          RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1, random_state=0), \n          MLPClassifier(alpha=1, max_iter=1000, random_state=0), \n          xgb.XGBClassifier(use_label_encoder=False, objective='binary:logistic', eval_metric='logloss'), \n          KNeighborsClassifier(3),\n          AdaBoostClassifier(random_state=0)]\n\nscores = []\nmnames = []\nfor model,name in zip(models,names):\n    pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n                                   ('model', model)])\n\n    score = cross_val_score(pipeline, X, y, cv=cv_rs, scoring=scoring)\n    mean_score = np.mean(np.array(score)).round(2)\n\n    scores.append(mean_score)\n    mnames.append(name)\n\nscores = dict(zip(mnames,scores))\nscores = sorted(scores.items(), key=lambda x:x[1], reverse=True)\nprint(scores)","47be85a3":"# bundle preprocessing and model\nsvm_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n                               ('model', SVC(class_weight=None, random_state=0))])\n\n# parameter grid \nsvm_params = {'model__C'     :[1e-3, 1e-2, 1e-1, 1, 1e1, 1e2, 1e3],\n              'model__gamma' :['scale', 'auto', 2, 1, 1e-1, 1e-2, 1e-3, 1e-4]}\n\n# Search for optimal parameters and fit the model\nsvm_classifier = RandomizedSearchCV(svm_pipeline, \n                                    param_distributions=svm_params, \n                                    n_iter=n_iter, \n                                    scoring=scoring,\n                                    n_jobs=n_jobs, \n                                    cv=cv_rs, \n                                    verbose=verbose,\n                                    refit=refit,\n                                    random_state=0)\n\n# Fit the classifier\nsvm_classifier.fit(X,y)\n\n# Cross validation\nscore = cross_val_score(svm_classifier.best_estimator_, X, y, cv=cv_cr, scoring=scoring)\nsvm_mu = np.mean(score)\nsigma = np.std(score, ddof=1)\nprint('SVM k-fold accuracy: (%0.2f \u00b1 %0.2f)' %(svm_mu*100, sigma*100) + '%')","7d9fd0f4":"margin_of_error = 2.262*sigma\/np.sqrt(30)\nprint(margin_of_error)","97134ce0":"# # Simple model\n# svm_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n#                                ('model', SVC())])\n# perc=[]\n# for i in tqdm(range(50)):\n#     X_tr, X_val, y_tr, y_val = train_test_split(X, y, test_size=0.33)\n#     model = svm_pipeline.fit(X_tr, y_tr)\n#     preds = model.predict(X_val)\n#     perc.append(sum(preds == y_val) \/ len(y_val))\n\n# prc = np.array(perc)\n# m = np.mean(prc)\n# s = np.std(prc)\n# print('SVM accuracy: (%0.2f \u00b1 %0.2f)' %(m*100, s*100) + '%')","7b3d3919":"title = r\"Learning Curves (SVM)\"\nplot_learning_curve(svm_classifier.best_estimator_, title, X, y, cv=5, n_jobs=-1);","ddb2672d":"# Bundle preprocessing and model\nraf_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n                               ('model', RandomForestClassifier(class_weight=None, \n                                                                random_state=0))])\n\n# parameter grid\nraf_params = {'model__bootstrap'         : [True, False],\n              'model__max_depth'         : [None, 5, 25, 50, 100],\n              'model__max_features'      : ['auto', 'sqrt', 1],\n              'model__min_samples_leaf'  : [1, 2, 4],\n              'model__min_samples_split' : [2, 5, 10],\n              'model__n_estimators'      : [10, 100, 500, 1000, 2000]}\n\n# Search for optimal parameters and fit the model\nraf_classifier = RandomizedSearchCV(raf_pipeline, \n                                    param_distributions=raf_params, \n                                    n_iter=n_iter,\n                                    scoring=scoring,\n                                    n_jobs=n_jobs, \n                                    cv=cv_rs, \n                                    verbose=verbose, \n                                    refit=refit, \n                                    random_state=0)\n\n# Fit the classifier\nraf_classifier.fit(X,y)\n\n# K-fold cross validation\nscore = cross_val_score(raf_classifier.best_estimator_, X, y, cv=cv_cr, scoring=scoring)\nraf_mu = np.mean(score)\nsigma = np.std(score)\nprint('RF k-fold accuracy: (%0.2f \u00b1 %0.2f)' %(raf_mu*100, sigma*100) + '%')","23a9e062":"# Bundle preprocessing and model\nxgb_pipeline = Pipeline(steps=[('preprocessor', preprocessor), \n                               ('model', xgb.XGBClassifier(use_label_encoder=False, \n                                                           objective='binary:logistic', \n                                                           eval_metric='logloss'))])\n\n# parameter space\nxgb_params = {\"model__learning_rate\"    : [0.05, 0.10, 0.15, 0.20, 0.25, 0.30], \n              \"model__max_depth\"        : [3, 4, 5, 6, 8, 10, 12, 15], \n              \"model__min_child_weight\" : [1, 3, 5, 7], \n              \"model__gamma\"            : [0.0, 0.1, 0.2 , 0.3, 0.4], \n              \"model__colsample_bytree\" : [0.3, 0.4, 0.5 , 0.7]}\n\n# setup\nxgb_classifier = RandomizedSearchCV(xgb_pipeline, \n                                    param_distributions=xgb_params, \n                                    n_iter=n_iter, \n                                    scoring=scoring, \n                                    n_jobs=-n_jobs, \n                                    cv=cv_rs, \n                                    verbose=verbose, \n                                    random_state=0)\n\n# fit the model\nxgb_classifier.fit(X,y)\n\n# K-fold cross validation\nscore = cross_val_score(xgb_classifier.best_estimator_, X, y, cv=cv_cr, scoring=scoring)\nxgb_mu = np.mean(score)\nsigma = np.std(score)\nprint('XGB k-fold accuracy: (%0.2f \u00b1 %0.2f)' %(xgb_mu*100, sigma*100) + '%')","dd9f7f80":"# split into train and validation sets\nX_tr, X_val, y_tr, y_val = train_test_split(X, y, test_size=0.33, stratify=y, random_state=0)\n\n# preprocessing\nX_train = preprocessor.fit_transform(X_tr)\nX_valid = preprocessor.transform(X_val)\n\n# convert to numpy array\ny_train = np.array(y_tr)\ny_valid = np.array(y_val)\n\n# construct neural network\nneural_net = keras.Sequential([layers.Dense(4, activation='relu', input_shape=[X_train.shape[1]]), \n                               layers.Dense(4, activation='relu'), \n                               layers.Dense(1, activation='sigmoid')])\n\nneural_net.compile(optimizer='adam', loss='binary_crossentropy', metrics=['binary_accuracy'])\n\n# stop before overfit\nearly_stopping = keras.callbacks.EarlyStopping(patience=10, \n                                               min_delta=0.001, \n                                               restore_best_weights=True)\n\n# fit the model\nnet_classifier = neural_net.fit(X_train, y_train, \n                                validation_data=(X_valid, y_valid), \n                                batch_size=512, \n                                epochs=1000,  \n                                verbose=0,\n                                callbacks=[early_stopping])\n\n# make predictions and print the accuracy score\ny_preds = net_classifier.model.predict(X_valid).round().flatten().astype(int)\nnet_score = (accuracy_score(y_valid, y_preds))\nprint('NN accuracy: %0.2f' %(net_score*100) + '%')\n\n# learning curves\nplt.style.use('default')\nhistory_df = pd.DataFrame(net_classifier.history)\nhistory_df.loc[:, ['loss', 'val_loss']].plot(figsize=(5,3))\nhistory_df.loc[:, ['binary_accuracy', 'val_binary_accuracy']].plot(figsize=(5,3));","de273a89":"# # Sk-fold cross validation (remember to comment after running and rerun the above)\n# net_mean, net_std = sk_cross_val_scores(X=pd.DataFrame(np.concatenate((X_train,X_valid), axis=0)), \n#                                         y=pd.DataFrame(np.concatenate((y_train,y_valid), axis=0)), \n#                                         classifier=net_classifier.model)\n\n# print('NN sk-fold accuracy: (%0.2f \u00b1 %0.2f)' %(net_mean*100, net_std*100) + '%')","8f1fc2ea":"models = (('svm', svm_classifier.best_estimator_), \n          ('raf', raf_classifier.best_estimator_), \n          ('xgb', xgb_classifier.best_estimator_))\n\nens_classifier = VotingClassifier(estimators=models)\nens_classifier.fit(X,y)\n\nscore = cross_val_score(ens_classifier, X, y, cv=cv_cr)\nens_mu = np.mean(score)\nsigma = np.std(score)\nprint('ENS k-fold accuracy: (%0.2f \u00b1 %0.2f)' %(ens_mu*100, sigma*100) + '%')","231dcbb4":"# Read and clean the test data\ntest_data = pd.read_csv(path + \"test.csv\")\ntest_data = clean_df(dataframe=test_data)\n\n# Put the test set's columns in the same order as the training set's columns\ntest_data = test_data[list(X.columns)]\n\n# # Predict using svm_classifier, rf_classifier, xgb_classifier or ens_classifier\n# predictions = raf_classifier.best_estimator_.predict(test_data)\n\n# or predict using nn_model\ntest_data_nn = preprocessor.transform(test_data)\npredictions = net_classifier.model.predict(test_data_nn).round().flatten().astype(int)\n\n# Save results to csv\nreload = pd.read_csv(path + 'test.csv')\noutput = pd.DataFrame({'PassengerId': reload.PassengerId, 'Survived': predictions})\noutput.to_csv('my_submission.csv', index=False)\n\n# Double-check\npd.read_csv('my_submission.csv')","279e16dd":"Next we will optimize each one and see their accuracy results.","b0163700":"### Common parameters","c6fb4487":"### e) Ensemble","bc80e0dc":"## Data Cleaning","635c07a0":"# <center> Introduction <\/center>","6f53ae9f":"### a) SVM","8c1ecc79":"*Note:* \n\n*Specifically we observe that as `Survived` goes from 0 to 1, Pclass goes from 3 to 1, i.e. as the ticket class decreases the probability of survival increases, hence the negative correlation.*\n\n*Also, if we translate `0=male` and `1=female`, as `Sex` increases (goes from being male to female), then the probability of survival also increases, hence the positive correlation.*","1f42ccac":"## Imports","7e2df6f1":"Below is the data dictionary, taken from [Kaggle's data description tab](https:\/\/www.kaggle.com\/c\/titanic\/data):","e877b018":"# <center> Machine Learning <\/center>","fb87d749":"### b) Random Forest","7689f553":"<div class=\"alert alert-block alert-info\"> \n    \nTo address the second project goal <b>\"predict whether a given passenger from the test.csv dataset survived or not\"<\/b>, we have created several models well over the 67% benchmark. To see which works best we have to make a prediction and test the results against the test set, whose answers are hidden on Kaggle (someone could always spoil all the fun and google the answers). Neural networks (with some minor modifications) attained the highest accuracy score, ranking at the top 6% in the competition leaderboard.\n\n<\/div>","29951653":"The function below `clean_df` does the following:\n\n- Creates the `Cabins`, `FarePerPerson`, `FamilySize` and `Honorifics` columns.\n- Removes the `PassengerId`, `Name`, `Ticket`, `Cabin` columns.\n- Replaces the 0 valued `Fares` with NaN's (not a value) in order to impute them later.\n- Removes the rows where the `Embarked` column has missing values.","34ccd738":"I have calculated some extra variables as shown below:\n\n| Variable | Definition | Derived from columns|\n| --- | --- | --- |\n| FamilySize | Family size | SibSp + Parch |\n| FarePerPerson | Fare per Person |  Fare \/ (FamilySize+1) |\n| Cabins | # of cabins | Cabin |\n| Honorifics | honorifics such as \"Mr, Mrs, etc\"| Name |","b91fda59":"The data suggests that we should reject our initial belief with high confidence. It is more likely that a 1st class passenger had a larger than 50% probability of surviving the Titanic.","a32b4dfe":"# <center> EDA (Exploratory Data Analysis) <\/center>","fe826b5f":"<a id='section_id'><\/a> We can see below that the passengers from Cherbourg had a higher percentage of females compared to those from Southampton. Also Cherbourg had more 1st class passengers than Southampton. Therefore Cherbourg males and females were more successful in surviving than Southampton males and females.","b265755c":"We will define the preprocessing to occur to the data before they enter a machine learning algorithm. \n\n**Numerical values** will be scaled between 0 and 1 (some algorithms find this helpful) and their missing values will be replaced with their mean. \n\n**Categorical values** like Sex have to be converted to numbers somehow. We will use One Hot Encoding for this. For example if we have a combination like Male-Female, the entries which have Male will be replaced by the values 01, while the entries which have Female will be replaced by the values 10.","8a97c36c":"In the plots above, the black lines\/whiskers at each bar represent a confidence interval. Let's take an example: \n\nAt the second plot we can see that a 1st class passenger's probability of surviving the Titanic is about 62%. However this number was deduced from a sample of passengers, not all passengers. This means that the true probability is different from 62%. \n\nThe confidence interval for this example is about 56-70%; this means that with high certainty, the 56-70% interval captures the true probability of a 1st class passenger surviving the Titanic. We can also find this manually:","24a0cebf":"<div class=\"alert alert-block alert-info\">\n    Therefore, to answer the project goal question <b>\u201cwhat sorts of people were more likely to survive?\u201d<\/b>, we see that the most desirable characteristics to have as passenger willing to survive on the Titanic, are:\n    <ul>\n        <li>Being female<\/li> \n        <li>Having a higher status honorific<\/li>\n        <li>Having a 1st class ticket with a cabin (higher ticket fare)<\/li>\n        <li>Being with family<\/li>\n    <\/ul>\n<\/div>","875d0a7e":"## Preprocessing","ac237ab7":"Submit results from a model of choice to the competition","20384324":"## Functions","647c6d34":"Let's have a look at the first 5 rows of the `train.csv` dataset. We will call the `Survived` column as **target** and all the rest columns as **features**:","584e300e":"*Edited from [Kaggle](https:\/\/www.kaggle.com\/c\/titanic):*\n\nThe sinking of the Titanic is one of the most infamous shipwrecks in history.\n\nOn April 15, 1912, during her maiden voyage, the widely considered \u201cunsinkable\u201d RMS Titanic sank after colliding with an iceberg. Unfortunately, there weren\u2019t enough lifeboats for everyone onboard, resulting in the death of 1502 out of 2224 passengers and crew. While there was some element of luck involved in surviving, it seems some groups of people were more likely to survive than others. We have access to two similar datasets. One dataset is titled `train.csv` and the other is titled `test.csv`.\n\n`Train.csv` will contain the details of a subset of the passengers on board (891 to be exact) and importantly, will reveal whether they survived or not, also known as the \u201cground truth\u201d. The `test.csv` dataset contains similar information but does not disclose the \u201cground truth\u201d for each passenger. \n\n<div class=\"alert alert-block alert-info\">\n    <b>Project Goals<\/b>: Predict whether a given passenger from the test.csv dataset survived or not. Also answer the question \u201cwhat sorts of people were more likely to survive?\u201d, using passenger data (ie name, age, gender, socio-economic class, etc).\n<\/div>","836d8fa7":"<center><img src=https:\/\/upload.wikimedia.org\/wikipedia\/commons\/6\/6e\/St%C3%B6wer_Titanic.jpg align=\"center\"\/><\/center>\n<center><i>Untergang der Titanic, by Willy St\u00f6wer, 1912<\/i><\/center>","7b22ec7e":"Next let's view some descriptive statistics. We can see that there are some Fares with value 0. We might want to fix that.","9adade10":"In similar vein, the [correlation coefficient](https:\/\/en.wikipedia.org\/wiki\/Pearson_correlation_coefficient) measures how much each `feature-target` pair 'moves together' or not. It takes values between -1 and +1. Positive correlation means that as a feature increases\/decreases, the target also increases\/decreases. Negative correlation means that as a features increases\/decreases, the target decreases\/increases (i.e. they move in opposite directions). Close to 0 correlation means that there is no such relationship.\n\nThe correlation coefficient indicates `Sex` (male or female) and `Pclass` (Ticket class) as the most correlated features with `Survived` (survival).","d7a90a2e":"### c) XGB","890d1145":"#### Evaluation\n\nWe will first evaluate some common ML algorithms to later optimize those that do best. The ones that do best are in descending order:\n\n- `SVM` [(Support Vector Machine)](https:\/\/en.wikipedia.org\/wiki\/Support-vector_machine)\n- `MLP`[(Neural Network)](https:\/\/en.wikipedia.org\/wiki\/Neural_network) \n- `XGB` [(Gradient Boosting)](https:\/\/en.wikipedia.org\/wiki\/Gradient_boosting)\n- We will also give `RAF` [(Random Forest)](https:\/\/en.wikipedia.org\/wiki\/Random_forest) a try.\n- And experiment with an [`ensemble model`](https:\/\/en.wikipedia.org\/wiki\/Ensemble_learning)","98ce8a7f":"| Variable | Definition | Key |\n| --- | --- | --- |\n| Survived | Survival | 0 = No, 1 = Yes |\n| Pclass | Ticket class | 1 = 1st, 2 = 2nd, 3 = 3rd |\n| SibSp | # of siblings \/ spouses aboard the Titanic |  |\n| Parch | # of parents \/ children aboard the Titanic |  |\n| Ticket | Ticket number |  |\n| Fare | Passenger fare in $ |  |\n| Cabin | Cabin number |  |\n| Embarked | Port of Embarkation | C = Cherbourg, Q = Queenstown, S = Southampton |\n\n**pclass**: A proxy for socio-economic status (SES)\n1st = Upper\n2nd = Middle\n3rd = Lower\n\n**age**: Age is fractional if less than 1. If the age is estimated, it is in the form of xx.5\n\n**sibsp**: \n- Sibling = brother, sister, stepbrother, stepsister\n- Spouse = husband, wife (mistresses and fianc\u00e9s were ignored)\n\n**parch**:\n- Parent = mother, father\n- Child = daughter, son, stepdaughter, stepson\n\nSome children travelled only with a nanny, therefore parch=0 for them.","ea38b776":"First of all we have to address the question **\"what would constitute a good model?\"**. Let's try to answer that quantitatively below.\n\nAccording to the training data, as seen below, 61.6% of the passengers did not survive. Therefore a simplistic model which always says that a person died for everyone, would be correct 61.6% of the time.\n\nIn reality 1502 out of 2224 people died, which makes it a 67% probability of death.\n\nTherefore **an informative model would be a model that exceeds the 67% benchmark**.","105b9e6d":"Let's re-read the training data and view the first 5 rows","93fceb2e":"Before obtaining the data, we might have held the prior belief that survival was independent of ticket class. This will be our null hypothesis. Then we obtain the data, and by looking at the second plot we're inclined to reject that belief. This will be our alternative hypothesis.\n\nLet's test for independence.","68d4e19b":"We indeed reject our prior belief, so we can say with high confidence that ticket class did play a role in survival.","d67e4f7a":"Let's plot the [mutual information](https:\/\/en.wikipedia.org\/wiki\/Mutual_information) scores between each `feature` and the target variable `Survived`. Mutual information measures how dependent each pair is, i.e. if I tell you information about a feature (e.g. Honorifics), how helpful is that about inferring survival? \n\nIt seems below that if we wanted to infer whether some person survived the Titanic shipwreck or not and we had a single question to ask them, the most informative single question would be to simply ask their name!","1a9c4706":"# <center> Submission <\/center>","11b5fd80":"Before obtaining the data, we may have held the prior belief that whether a 1st class passenger survived or not was random, meaning that his\/her probability of survival was 50%. This will be our null hypothesis. Now we obtain the data, which suggest that the probability of survival was larger than 50% (the plot shows it is around 62%). This will be the alternative hypothesis.\n\nWe may eyeball the numbers and say that 62% is larger than 50%, so we reject our null hypothesis. However this is not statistically sound, so we will use hypothesis testing to check if this result is statistically significant enough to make us reject our prior belief. As an extreme example, consider asking only one person what his favorite color is; which he replies is green. It is an overkill to generalize and state that everyone's favorite color is green.","3ce32f4c":"Going from left to right, top to bottom, in each plot below we see that:\n\n- The average fare of people who survived was higher than the average fare of people who did not survive.\n- 1st class passengers had a much higher probability to survive than 3rd class passengers.\n- For a passenger who boarded the Titanic without any family, it is more likely that he\/she did not survive.\n- Women had a clear advantage over men in terms of survival.\n- For a passenger who did not book a cabin, it is more likely that he\/she did not survive.\n- Most of the passengers were of age between 20-35 and did not survive.\n- People who embarked at Southampton were ill-fated, compared to those who embarked at Queenstown or Cherbourg. Also people who boarded at Cherbourg were slightly more likely to survive than not survive. Perhaps a combination of 'survival-like' characteristics such as female sex were more in common in people who boarded at Cherbourg than Southampton. We will see that [later](#section_id) in more detail.\n- Ticket class and fare are related as expected: as ticket class decreases, the ticket fare increases.\n- Honorifics were a key player in determining survival. Compare 'Mr' with less than 20% and 'Sir', with 100% survival probability.","8cd19424":"In the code chuck below, we can see how many missing values there are per column. Specifically:\n\n- In the `Embarked` column there are only 2 missing values, therefore we will remove the rows which contain them.\n- There are 177 missing `Age` values. It is quite common to replace those missing values with the mean age, so this is what we'll do.\n- There are 687 missing `Cabin` values. Those are missing because the majority of passenger did not book a cabin. We will drop this column and replace it as above with a `Cabins` column, which will represent the number of cabins. So when we have a missing value, this means 0 cabins.","42863961":"## Models","ddd0de0d":"### d) Neural Networks"}}