{"cell_type":{"273b1658":"code","f985c14c":"code","ddf5eae2":"code","87146b84":"code","b74c853d":"code","22724522":"code","a5b785a4":"code","4dc88bc5":"code","2f803e24":"code","bb7daa82":"code","01c8fefa":"code","dbddb6a6":"code","4f67785f":"code","fb287a01":"code","995be630":"code","90a1d488":"code","131100ee":"markdown","5d877238":"markdown","1b4a3534":"markdown"},"source":{"273b1658":"%matplotlib inline\nfrom glob import glob\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom skimage.io import imread\nimport zipfile as zf\nimport numpy as np\nimport h5py\nfrom keras.utils.io_utils import HDF5Matrix\nbase_h5_dir = '..\/input\/deeplesion-overview\/'\nbase_img_dir = '..\/input\/nih-deeplesion-subset\/'","f985c14c":"h5_path = os.path.join(base_h5_dir, 'deeplesion.h5')\nwith h5py.File(h5_path, 'r') as h:\n    print(list(h.keys()))\n    for k in ['image', 'mask']:\n        print(k, h[k].shape, h[k].dtype)\n    base_shape = h['image'].shape","ddf5eae2":"ct_window_func = lambda x: np.clip((x+175.0)\/275, -1, 1)\nget_xyf = lambda s, e: (HDF5Matrix(h5_path, 'image', start=s, end=e, normalizer=ct_window_func), \n                       HDF5Matrix(h5_path, 'mask', start=s, end=e),\n                       HDF5Matrix(h5_path, 'file_name', start=s, end=e)\n                      )\ntrain_split = 0.7\ncut_val = int(base_shape[0]*train_split)\ntrain_x, train_y, train_paths = get_xyf(0, cut_val)\ntest_x, test_y, test_paths = get_xyf(cut_val, None)\nprint(train_x.shape, test_x.shape)","87146b84":"from skimage.util.montage import montage2d as montage\nt_x, t_y = train_x[:8], train_y[:8]\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize = (20, 10))\nax1.imshow(montage(t_x[:, :, :, 0]), cmap = 'gray')\nax2.imshow(montage(t_y[:, :, :, 0]))","b74c853d":"# we dont need full resolution images so we can just use a downsampled version\nclass DownsampledHDF5Matrix(HDF5Matrix):\n    def __init__(self, datapath, dataset, downscale_factor, start=0, end=None, normalizer=None):\n        ds_func = lambda x: x[:, ::downscale_factor, ::downscale_factor, :]\n        ds_norm = ds_func if normalizer is None else lambda x: ds_func(normalizer(x))\n        self.downscale_factor = downscale_factor\n        super(DownsampledHDF5Matrix, self).__init__(datapath, dataset, start=start, end=end, normalizer=ds_norm)\n        t_val = self[0:1]\n        self._base_shape = t_val.shape[1:]\n        self._base_dtype = t_val.dtype\n    \n    @property\n    def shape(self):\n        \"\"\"Gets a numpy-style shape tuple giving the dataset dimensions.\n        # Returns\n            A numpy-style shape tuple. (self.data.shape[1]\/\/self.downscale_factor, self.data.shape[2]\/\/self.downscale_factor, self.data.shape[3])\n        \"\"\"\n        return (self.end - self.start,) + self._base_shape\n    \n    @property\n    def dtype(self):\n        \"\"\"Gets the datatype of the dataset.\n        # Returns\n            A numpy dtype string.\n        \"\"\"\n        return self._base_dtype","22724522":"ct_window_func = lambda x: np.clip((x+500.0)\/600, -1, 1)\nget_xyf = lambda s, e: (HDF5Matrix(h5_path, 'image', start=s, end=e, normalizer=ct_window_func), \n                       DownsampledHDF5Matrix(h5_path, 'mask', downscale_factor=4, start=s, end=e),\n                       [x.decode() for x in HDF5Matrix(h5_path, 'file_name', start=s, end=e)[:]]\n                      )\ntrain_split = 0.7\ncut_val = int(base_shape[0]*train_split)\ntrain_x, train_y, train_paths = get_xyf(0, cut_val)\ntest_x, test_y, test_paths = get_xyf(cut_val, None)\nprint(test_x.shape, test_y.shape)\nt_x, t_y = train_x[:8], train_y[:8]\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize = (20, 10))\nax1.imshow(montage(t_x[:, :, :, 0]), cmap = 'gray')\nax2.imshow(montage(t_y[:, :, :, 0]))","a5b785a4":"patient_df = pd.read_csv(os.path.join(base_img_dir, 'DL_info.csv'))\npatient_df = patient_df[patient_df['File_name'].isin(train_paths+test_paths)]\npatient_df['training'] = patient_df['File_name'].isin(train_paths)\npatient_df.sample(3)","4dc88bc5":"from keras.layers import Input, Activation, Conv2D, MaxPool2D, UpSampling2D, Dropout, concatenate, BatchNormalization, Cropping2D, ZeroPadding2D, SpatialDropout2D\nfrom keras.layers import Conv2DTranspose, Dropout, GaussianNoise\nfrom keras.models import Model\nfrom keras import backend as K\n\ndef up_scale(in_layer):\n    filt_count = in_layer._keras_shape[-1]\n    return Conv2DTranspose(filt_count\/\/2+2, kernel_size = (2,2), strides = (2,2), padding = 'same')(in_layer)\ndef up_scale_fancy(in_layer):\n    return UpSampling2D(size=(2,2))(in_layer)\n\ninput_layer = Input(shape=train_x.shape[1:])\nsp_layer = GaussianNoise(0.1)(input_layer)\nbn_layer = BatchNormalization()(sp_layer)\nc1 = Conv2D(filters=8, kernel_size=(5,5), activation='relu', padding='same')(bn_layer)\nl = MaxPool2D(strides=(2,2))(c1)\nc2 = Conv2D(filters=16, kernel_size=(3,3), activation='relu', padding='same')(l)\nl = MaxPool2D(strides=(2,2))(c2)\nc3 = Conv2D(filters=32, kernel_size=(3,3), activation='relu', padding='same')(l)\n\nl = MaxPool2D(strides=(2,2))(c3)\nc4 = Conv2D(filters=32, kernel_size=(3,3), activation='relu', padding='same')(l)\n\nl = SpatialDropout2D(0.25)(c4)\ndil_layers = [l]\nfor i in [2, 4, 6, 8, 12, 18, 24]:\n    dil_layers += [Conv2D(16,\n                          kernel_size = (3, 3), \n                          dilation_rate = (i, i), \n                          padding = 'same',\n                         activation = 'relu')(l)]\nl = concatenate(dil_layers)\n\nl = SpatialDropout2D(0.2)(concatenate([up_scale(l), c3], axis=-1))\nl = Conv2D(filters=128, kernel_size=(2,2), activation='linear', padding='same')(l)\nl = BatchNormalization()(l)\nl = Activation('relu')(l)\nl = Conv2D(filters=96, kernel_size=(2,2), activation='relu', padding='same')(l)\nl = Conv2D(filters=32, kernel_size=(2,2), activation='relu', padding='same')(l)\nl = Conv2D(filters=16, kernel_size=(2,2), activation='linear', padding='same')(l)\nl = Cropping2D((4,4))(l)\nl = BatchNormalization()(l)\nl = Activation('relu')(l)\n\nl = Conv2D(filters=1, kernel_size=(1,1), activation='sigmoid')(l)\noutput_layer = ZeroPadding2D((4,4))(l)\n\nseg_model = Model(input_layer, output_layer)\nseg_model.summary()","2f803e24":"import keras.backend as K\nfrom keras.optimizers import Adam\nfrom keras.losses import binary_crossentropy\ndef dice_coef(y_true, y_pred, smooth=1):\n    intersection = K.sum(y_true * y_pred, axis=[1,2,3])\n    union = K.sum(y_true, axis=[1,2,3]) + K.sum(y_pred, axis=[1,2,3])\n    return K.mean( (2. * intersection + smooth) \/ (union + smooth), axis=0)\ndef dice_p_bce(in_gt, in_pred):\n    return 0.0*binary_crossentropy(in_gt, in_pred) - dice_coef(in_gt, in_pred)\ndef true_positive_rate(y_true, y_pred):\n    return K.sum(K.flatten(y_true)*K.flatten(K.round(y_pred)))\/K.sum(y_true)\nseg_model.compile(optimizer=Adam(1e-4, decay=1e-6), loss=dice_p_bce, metrics=[dice_coef, 'binary_accuracy', true_positive_rate])","bb7daa82":"from keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping, ReduceLROnPlateau\nweight_path=\"{}_weights.best.hdf5\".format('seg_model')\n\ncheckpoint = ModelCheckpoint(weight_path, monitor='val_dice_coef', verbose=1, \n                             save_best_only=True, mode='max', save_weights_only = True)\n\nreduceLROnPlat = ReduceLROnPlateau(monitor='val_dice_coef', factor=0.5, \n                                   patience=3, \n                                   verbose=1, mode='max', epsilon=0.0001, cooldown=2, min_lr=1e-6)\nearly = EarlyStopping(monitor=\"val_dice_coef\", \n                      mode=\"max\", \n                      patience=15) # probably needs to be more patient, but kaggle time is limited\ncallbacks_list = [checkpoint, early, reduceLROnPlat]","01c8fefa":"from keras.preprocessing.image import ImageDataGenerator\ndg_args = dict(featurewise_center = False, \n                  samplewise_center = False,\n                  rotation_range = 7.5, \n                  width_shift_range = 0.02, \n                  height_shift_range = 0.02, \n                  shear_range = 0.01,\n                  zoom_range = [0.9, 1.25],  \n                  brightness_range = [0.9, 1.1],\n                  horizontal_flip = False, \n                  vertical_flip = False,\n                  fill_mode = 'nearest',\n                   data_format = 'channels_last')\n\nimage_gen = ImageDataGenerator(**dg_args)\ndg_args.pop('brightness_range')\nlabel_gen = ImageDataGenerator(**dg_args)\ndef train_gen(batch_size = 16, seed = None):\n    np.random.seed(seed if seed is not None else np.random.choice(range(9999)))\n    while True:\n        seed = np.random.choice(range(9999))\n        # keep the seeds syncronized otherwise the augmentation to the images is different from the masks\n        batch_count = train_x.shape[0]\/\/batch_size\n        batch_id = np.random.permutation(range(0, train_x.shape[0]-batch_size, batch_size))\n        for c_idx in batch_id:\n            g_x = image_gen.flow(train_x[c_idx:(c_idx+batch_size)], batch_size = batch_size, seed = seed, shuffle=True)\n            g_y = label_gen.flow(train_y[c_idx:(c_idx+batch_size)], batch_size = batch_size, seed = seed, shuffle=True)\n            yield next(g_x)\/255.0, next(g_y)","dbddb6a6":"cur_gen = train_gen(8)\nt_x, t_y = next(cur_gen)\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize = (20, 10))\nax1.imshow(montage(t_x[:, :, :, 0]), cmap='gray')\nax2.imshow(montage(t_y[:, :, :, 0]), cmap='gray_r')","4f67785f":"batch_size = 16\nloss_history = [seg_model.fit_generator(train_gen(batch_size), \n                             steps_per_epoch=train_x.shape[0]\/\/batch_size, \n                             epochs=40, \n                             validation_data=(test_x, test_y),\n                             callbacks=callbacks_list,\n                            workers=2)]","fb287a01":"def show_loss(loss_history):\n    epich = np.cumsum(np.concatenate(\n        [np.linspace(0.5, 1, len(mh.epoch)) for mh in loss_history]))\n    fig, (ax1, ax2, ax3, ax4) = plt.subplots(1, 4, figsize=(22, 10))\n    _ = ax1.plot(epich,\n                 np.concatenate([mh.history['loss'] for mh in loss_history]),\n                 'b-',\n                 epich, np.concatenate(\n            [mh.history['val_loss'] for mh in loss_history]), 'r-')\n    ax1.legend(['Training', 'Validation'])\n    ax1.set_title('Loss')\n\n    _ = ax2.plot(epich, np.concatenate(\n        [mh.history['true_positive_rate'] for mh in loss_history]), 'b-',\n                     epich, np.concatenate(\n            [mh.history['val_true_positive_rate'] for mh in loss_history]),\n                     'r-')\n    ax2.legend(['Training', 'Validation'])\n    ax2.set_title('True Positive Rate\\n(Positive Accuracy)')\n    \n    _ = ax3.plot(epich, np.concatenate(\n        [mh.history['binary_accuracy'] for mh in loss_history]), 'b-',\n                     epich, np.concatenate(\n            [mh.history['val_binary_accuracy'] for mh in loss_history]),\n                     'r-')\n    ax3.legend(['Training', 'Validation'])\n    ax3.set_title('Binary Accuracy (%)')\n    \n    _ = ax4.plot(epich, np.concatenate(\n        [mh.history['dice_coef'] for mh in loss_history]), 'b-',\n                     epich, np.concatenate(\n            [mh.history['val_dice_coef'] for mh in loss_history]),\n                     'r-')\n    ax4.legend(['Training', 'Validation'])\n    ax4.set_title('DICE')\n\nshow_loss(loss_history)","995be630":"seg_model.load_weights(weight_path)\nseg_model.save('seg_model.h5')","90a1d488":"fig, m_axs = plt.subplots(4,3, figsize = (20, 20))\n[c_ax.axis('off') for c_ax in m_axs.flatten()]\nfor idx, (ax1, ax2, ax3) in enumerate(m_axs):\n    ix = test_x[idx:idx+1]\n    iy = test_y[idx:idx+idx+1]\n    p_image = seg_model.predict(ix)\n    ax1.imshow(ix[0,:,:,0], cmap = 'gray')\n    ax1.set_title('Input Image')\n    ax2.imshow(iy[0,:,:,0], vmin = 0, vmax = 1, cmap = 'bone_r' )\n    ax2.set_title('Ground Truth')\n    ax3.imshow(p_image[0,:,:,0], vmin = 0, vmax = 1, cmap = 'viridis' )\n    ax3.set_title('Prediction')","131100ee":"## Load prepackaged image data\nHere we load the packaged image data from the hdf5 file and split it into training and testing groups","5d877238":"# Load the Patient Data\nSo we can reference the images later","1b4a3534":"# Simple U-Net\nThe notebook here just shows how to go about building a simple U-Net model and training it on some of the lesions. Given the subset of the data and looking at only one slice of a number of different lesions we don't expect fantastic performance, but it could serve as a simple baseline for comparing more complicated, pre-trained, object detection models"}}