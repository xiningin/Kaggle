{"cell_type":{"25d0af10":"code","c418c0b1":"code","c1ffc8d3":"code","f22a86e2":"code","c0588b87":"code","89c0a953":"code","c80be40e":"code","7055f2ea":"code","edf1645d":"code","e909179e":"code","ecc6ccb9":"code","5ed41fdc":"code","9ec4e4d0":"code","3f2dd436":"code","8858e823":"code","01c0fcf5":"code","930dc43d":"code","dadce3c1":"code","745dc516":"code","729a0af9":"code","9aaa94b8":"code","bae8a04a":"code","f53235b7":"code","ff0d54d0":"code","3b82d4fc":"code","410aa735":"code","39be8107":"code","5b8184a3":"code","ed9ead9d":"markdown","afe4213e":"markdown","ab43545b":"markdown","8bac26a6":"markdown","824bf99a":"markdown","45a58e30":"markdown","081d8416":"markdown","579b835d":"markdown","60b4485e":"markdown","b16012ac":"markdown","6a5c868b":"markdown","8177d229":"markdown","b00a791c":"markdown","410f51cc":"markdown","74e19c90":"markdown","a8bd17ef":"markdown"},"source":{"25d0af10":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns# data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c418c0b1":"train=pd.read_csv(\"..\/input\/home-credit-default-risk\/application_train.csv\")\ntrain.head()","c1ffc8d3":"missing_values = train.isna().sum()\/len(train)*100\nmissing_values[missing_values>0].sort_values(ascending  = False)","f22a86e2":"cols = train.columns\na = train.isna().sum()\/len(train)*100\nvariable = []\nfor i in range(0,len(cols)):\n    if a[i]>=60:\n        variable.append(cols[i])\nprint(variable)        ","c0588b87":"for col in cols:\n    train[col].fillna(train[col].mode()[0],inplace = True)","89c0a953":"missing_values = train.isna().sum()\/len(train)*100\nmissing_values","c80be40e":"train.var().sort_values(ascending = False)","7055f2ea":"numeric = train.select_dtypes(include=[np.number])\nvar = numeric.var()\nvariance = []\nfor i in range(len(var)):\n    if var[i]>=30:\n        variance.append(var[i])","edf1645d":"variance","e909179e":"df=train.drop('TARGET', 1)\nf , ax = plt.subplots(figsize = (14,12))\nplt.title('Correlation of Features- HeatMap',y=1,size=16)\nsns.heatmap(df.corr(),square = True,  vmax=0.8,annot = False)","ecc6ccb9":"from sklearn.ensemble import RandomForestRegressor\ndf=df.drop(['SK_ID_CURR','DAYS_ID_PUBLISH'], axis=1)\nmodel = RandomForestRegressor(random_state=1, max_depth=10)\ndf=pd.get_dummies(df)\nmodel.fit(df,train.TARGET)","5ed41fdc":"features = df.columns\nimportances = model.feature_importances_\nindices = np.argsort(importances[0:20])  # top 20 features\nplt.title('Feature Importances')\nplt.barh(range(len(indices)), importances[indices], color='b', align='center')\nplt.yticks(range(len(indices)), [features[i] for i in indices])\nplt.xlabel('Relative Importance')\nplt.show()","9ec4e4d0":"# Define dictionary to store our rankings\nranks = {}\n# Create our function which stores the feature rankings to the ranks dictionary\ndef ranking(ranks, names, order=1):\n    minmax = MinMaxScaler()\n    ranks = minmax.fit_transform(order*np.array([ranks]).T).T[0]\n    ranks = map(lambda x: round(x,2), ranks)\n    return dict(zip(names, ranks))\n# Construct our Linear Regression model\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.feature_selection import RFE\nlr = LinearRegression(normalize=True)\nlr.fit(df, train.TARGET)\n#stop the search when only the last feature is left\nrfe = RFE(lr, n_features_to_select=1, verbose =3 )\nrfe.fit(df, train.TARGET)\n\nfrom sklearn.preprocessing import MinMaxScaler\nranks[\"RFE\"] = ranking(list(map(float, rfe.ranking_)), train.columns, order=-1)\n# Create empty dictionary to store the mean value calculated from all the scores\nr = {}\nfor name in train.columns:\n    r[name] = round(np.mean([ranks[method][name] \n                             for method in ranks.keys()]), 2)\n# Put the mean scores into a Pandas dataframe\nmeanplot = pd.DataFrame(list(r.items()), columns= ['Feature','Mean Ranking'])\n# Sort the dataframe\nmeanplot = meanplot.sort_values('Mean Ranking', ascending=False)\n# Let's plot the ranking of the features\nsns.factorplot(x=\"Mean Ranking\", y=\"Feature\", data = meanplot, kind=\"bar\",size=16, aspect=0.75, palette='coolwarm')","3f2dd436":"train = pd.read_csv(\"..\/input\/fashionmnist\/fashion-mnist_train.csv\")\ntrain.head()","8858e823":"train_data = np.array(train,dtype = 'float32')\nimg = []\nfor i in range(len(train)):\n    image = train_data[i].flatten()\n    img.append(image)\nimg = np.array(img,dtype = 'float32')    \nimage.shape\n","01c0fcf5":"train = pd.read_csv(\"..\/input\/fashionmnist\/fashion-mnist_train.csv\",sep=',')    # Give the complete path of your train.csv file\nfeat_cols = [ 'pixel'+str(i) for i in range(img.shape[1]) ]\ndf = pd.DataFrame(img,columns=feat_cols)\ndf['label'] = train['label']","930dc43d":"df.head()","dadce3c1":"from sklearn.decomposition import FactorAnalysis\nfa = FactorAnalysis(n_components = 3).fit_transform(df[feat_cols].values)","745dc516":"%matplotlib inline\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(16,10))\nplt.title('Factor Analysis Components')\nplt.scatter(fa[:,0], fa[:,1],c='r',s=10)\nplt.scatter(fa[:,1], fa[:,2],c='b',s=10)\nplt.scatter(fa[:,2],fa[:,0],c='g',s=10)\nplt.legend((\"First Factor\",\"Second Factor\",\"Third Factor\"))","729a0af9":"from sklearn.decomposition import PCA\npca = PCA(n_components=4)\npca_result = pca.fit_transform(df[feat_cols].values)","9aaa94b8":"plt.figure(figsize=(14,6))\nplt.plot(np.cumsum(pca.explained_variance_ratio_))\nplt.xlabel('Number of Components')\nplt.ylabel('Cumulative Explained Variance');","bae8a04a":"index = np.arange(len(pca.explained_variance_ratio_))\nplt.figure(figsize=(14,6))\nplt.title('Principal Component Analysis')\nplt.bar(index, pca.explained_variance_ratio_*100)\nplt.xlabel('Principal Component', fontsize=10)\nplt.ylabel('Explained Variance', fontsize=10)\nplt.xticks(index, pca.explained_variance_ratio_*100, fontsize=10, rotation=30)\nplt.show()","f53235b7":"plt.figure(figsize=(16,10))\nplt.plot(range(4), pca.explained_variance_ratio_)\nplt.plot(range(4), np.cumsum(pca.explained_variance_ratio_))\nplt.title(\"PCA - Cumulative Explained Variance vs. Component-Explained Variance \")\nplt.legend((\"Component - Explained Variance\",\"Cumulative Sum - Explained Variance\"))","ff0d54d0":"plt.scatter(pca_result[:, 0], pca_result[:, 1],pca_result[:, 2], pca_result[:, 3],\n            edgecolor='none', alpha=0.9,\n            cmap=plt.cm.get_cmap('Spectral', 8))\nplt.colorbar();","3b82d4fc":"from sklearn.decomposition import TruncatedSVD \nsvd = TruncatedSVD(n_components=3, random_state=42).fit_transform(df[feat_cols].values)\nsvd.shape","410aa735":"plt.figure(figsize=(16,10))\nplt.title('SVD Components')\nplt.scatter(svd[:,0], svd[:,1],c='r',s=10)\nplt.scatter(svd[:,1], svd[:,2],c='b',s=10)\nplt.scatter(svd[:,2],svd[:,0],c='g',s=10)\nplt.legend((\"Principal Component 1\",\"Principal Component 2\",\"Principal Component 3\"))","39be8107":"from sklearn.decomposition import FastICA \nICA = FastICA(n_components=3, random_state=12) \nX=ICA.fit_transform(df[feat_cols].values)","5b8184a3":"plt.figure(figsize=(16,8))\nplt.title('ICA Components')\nplt.scatter(X[:,0], X[:,1],c='r',s=10)\nplt.scatter(X[:,1], X[:,2],c='b',s=10)\nplt.scatter(X[:,2], X[:,0],c='g',s=10)\nplt.legend((\"ICA Component 1\",\"ICA Component 2\",\"ICA Component 3\"))","ed9ead9d":"> Independent Component Analysis\n* In this analysis the groups or factors are not uncorrelated but independent\n* Variables are independent if its not dependent on other variables but uncorrelated means they cant be linearly correlated\n* Its the most used Dimensionality Reduction methods","afe4213e":"Recursive Feature Selection\n* In this technique we will check the influence of a variable on the overall model by iteratively dropping each variable and model using the remaining variables.Further Comparing the performances and deduce if the variable is feasable to drop.Its recursive\/Iterative process.\n* We first take all the n variables present in our dataset and train the model using them\n* We then calculate the performance of the model\n* Now, we compute the performance of the model after eliminating each variable (n times), i.e., we drop one   variable every time and train the model on the remaining n-1 variables\n* We identify the variable whose removal has produced the smallest (or no) change in the performance of the model, and then drop that variable\n* Repeat this process until no variable can be dropped\n* This method can be used when building Linear Regression or Logistic Regression models.","ab43545b":"We should consider dropping variables having correlations greater than 0.5-0.6 ","8bac26a6":"# Dimensionality Reduction","824bf99a":" Factor Analysis\n* Factor Analysis is a technique of grouping highly correlated variables such that each group of variables have high correlations between them nad weak correaltions with the other groups.\n* These groups are called factors","45a58e30":"Percentage of missing values in the set","081d8416":"High Correlation between Variables\n1. High Correlation with the target variable is always sought\n2. High correlation between input variables is avoideed as both variables show similar characterstics. ","579b835d":"Variables of less than 60% of missing values in the training set","60b4485e":"You can still see some redundant features in the graph","b16012ac":"In the above graph, the blue line represents component-wise explained variance while the orange line represents the cumulative explained variance. We are able to explain around 60% variance in the dataset using just four components. Let us now try to visualize each of these decomposed components:","6a5c868b":">  Hello World!! I am just a beginner in the field and still exploring so bear with my mistakes(if any) and i am open to any type of suggestions.Plus if you like it please UPVOTE.It really pushes me to learn more and more.","8177d229":"> Singular Value Decomposition\n* In this type of Decomposition the original variables are decomposed into 3 different matrices.Its main aim is to remove redundant information.SVD uses Eigen Value and Eigen Vectors for calculation.","b00a791c":"> Principle Component Analysis\n* Principal Component Analysis is a technique which is used to group variables into components.The variables are linearly correlated.\n* It works in way that first component gives the highest variance as compared to subsequent components","410f51cc":"Random Forest can be used to explore Feature Importances between  variables","74e19c90":"Low Variance Filter Technique","a8bd17ef":"1. Let us visualize the transformed variables by plotting the first three principal components:"}}