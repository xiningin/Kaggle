{"cell_type":{"2826d024":"code","ec6e3c22":"code","85a0d1df":"code","fe83cac8":"code","fc1df6e0":"code","5ff1933e":"code","67e9b088":"code","d586523f":"code","b75d637e":"code","3aa8a0c7":"code","e0447ba9":"code","53df36ab":"code","ebd4ee85":"code","a4c4b25e":"code","ca6d9042":"code","5ef505e5":"code","ae15d011":"code","bd07be5e":"code","3a235ea1":"code","14bc9d85":"code","6f2a1a96":"code","fb336dc1":"code","ab2f4043":"code","2ef4f42e":"code","26df6793":"code","50af2ba3":"code","0694634d":"code","4ac94ecb":"code","7b7a602a":"markdown","bb4f721f":"markdown","27547371":"markdown","de9873bc":"markdown","54f72a4a":"markdown","6022d767":"markdown","eebaa4ec":"markdown","a847c9f5":"markdown","b1efccf4":"markdown","1b3e0815":"markdown","b28bf617":"markdown","aea00e78":"markdown","0707b73c":"markdown","1f5d68f7":"markdown","0fb5e8ba":"markdown","58c282c0":"markdown","10c3d2c7":"markdown","ff350c10":"markdown","03fae073":"markdown"},"source":{"2826d024":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","ec6e3c22":"import seaborn as sns # For Plotting Graphs\nimport matplotlib.pyplot as plt # for Plotting Graphs\nfrom sklearn.svm import SVC # Support Vector Machine Classifier\nfrom sklearn.metrics import precision_score, recall_score,confusion_matrix, classification_report, accuracy_score, f1_score  ## Skearns Metrics\nfrom sklearn.neighbors import KNeighborsClassifier ## KNN Classifier\nfrom sklearn.model_selection import train_test_split ## Splitting Data set\nfrom xgboost import XGBClassifier ## Boosting Algo\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import roc_curve, auc ## Comparing Various Classifiers\nimport warnings # Removin Warnings\nwarnings.filterwarnings(\"ignore\")","85a0d1df":"Credit_card_fraud_csv = pd.read_csv(\"\/kaggle\/input\/creditcardfraud\/creditcard.csv\",delimiter = \",\") # Reading Csv file as Pandas DataFrame.\nCredit_card_fraud_csv.head() # Showing Top 5 Datapoints","fe83cac8":"# V1,V2,V3 ... etc are PCA transform of Actual Features.\nCredit_card_fraud_csv.shape","fc1df6e0":"Credit_card_fraud_csv.info()","5ff1933e":"Credit_card_fraud_csv.describe()","67e9b088":"def Pie(ratio, text = \"CREDIT CARD FRAUDS IN EUROPE IN 2013 \"):    \n    fig, ax = plt.subplots()\n    plt.rcParams['font.sans-serif'] = 'Arial'\n    plt.rcParams['font.family'] = 'sans-serif'\n    plt.rcParams['text.color'] = '#909090'\n    plt.rcParams['axes.labelcolor']= '#909090'\n    plt.rcParams['xtick.color'] = '#909090'\n    plt.rcParams['ytick.color'] = '#909090'\n    plt.rcParams['font.size']=12\n    labels = ['Legit transaction', \n             'Fraud transaction']\n    percentages = [1- ratio,ratio]\n    explode=(0.1,0)\n    ax.pie(percentages, explode=explode, labels=labels,  \n           colors= ['#009ACD', '#ADD8E6'], autopct='%1.4f%%', \n           shadow=False, startangle=0,   \n           pctdistance=1.4,labeldistance=1.8);\n    ax.axis('equal')\n    ax.set_title( text)\n    ax.legend(frameon=False, bbox_to_anchor=(1.5,0.8));\nPie(ratio = 0.00172)","d586523f":"Amount = Credit_card_fraud_csv[\"Amount\"]#[Credit_card_fraud_csv[\"Class\"] == 1]\nx = range(0,Amount.shape[0])\nplt.style.use('seaborn-whitegrid')\nplt.rcParams[\"figure.figsize\"] = (20,8)\nfig = plt.figure()\nax = plt.axes()\nax.plot(x,Amount,color='#0099CC')\n#plt.xlabel(\"\")\nplt.ylabel(\"Amount\")\nax.set_title(\"AMOUNT\");","b75d637e":"Fraud_Amount = Credit_card_fraud_csv[Credit_card_fraud_csv[\"Class\"] == 1][\"Amount\"]\nx = range(0,Fraud_Amount.shape[0])\nplt.style.use('seaborn-whitegrid')\nplt.rcParams[\"figure.figsize\"] = (13,5)\nfig = plt.figure()\nax = plt.axes()\nax.plot(x,Fraud_Amount,color='#0099CC')\n#plt.xlabel(\"\")\nplt.ylabel(\"Amount\")\nax.set_title(\"AMOUNT\");","3aa8a0c7":"Fraud_Amount.describe()","e0447ba9":"from sklearn.preprocessing import  RobustScaler\n\n\nrob_scaler = RobustScaler()\n\nSc_amount = rob_scaler.fit_transform(Credit_card_fraud_csv['Amount'].values.reshape(-1,1))\nSc_time   = rob_scaler.fit_transform(Credit_card_fraud_csv['Time'].values.reshape(-1,1))\nCredit_card_fraud_csv.insert(0, 'scaled_amount', Sc_amount)\nCredit_card_fraud_csv.insert(1, 'scaled_time', Sc_time)\nCredit_card_fraud_csv.drop(['Time','Amount'], axis=1, inplace=True)\nCredit_card_fraud_csv.head()","53df36ab":"def concatenate(X,Y):\n    return np.concatenate((X,Y))","ebd4ee85":"legit_tranc = Credit_card_fraud_csv[Credit_card_fraud_csv[\"Class\"] == 0]\nfraud_tranc = Credit_card_fraud_csv[Credit_card_fraud_csv[\"Class\"] == 1]\nLeX, LeXte,LeT,LeTe = train_test_split(np.array(legit_tranc.iloc[:,0:-1]),np.array(legit_tranc.iloc[:,-1]),test_size  = 0.35)\nFrX, FrXte,FrT,FrTe = train_test_split(np.array(fraud_tranc.iloc[:,0:-1]),np.array(fraud_tranc.iloc[:,-1]),test_size  = 0.5)\nXtrain = concatenate(LeX,FrX)\nXtest = concatenate(LeXte,FrXte)\nYtrain = concatenate(LeT,FrT)\nYtest = concatenate(LeTe,FrTe)\nXtrain.shape,Xtest.shape,Ytrain.shape,Ytest.shape,FrX.shape","a4c4b25e":"neigh = KNeighborsClassifier(n_neighbors=1,algorithm='auto',n_jobs = 10)\nneigh.fit(Xtrain, Ytrain)","ca6d9042":"def acc(y_test,prediction):\n    # Printing Accuracy\n    cm = confusion_matrix(y_test, prediction)\n    recall = np.diag(cm) \/ np.sum(cm, axis = 1)\n    precision = np.diag(cm) \/ np.sum(cm, axis = 0)\n    \n    print ('Recall:', recall)\n    print ('Precision:', precision)\n    print ('\\n clasification report:\\n', classification_report(y_test,prediction))\n    print ('\\n confussion matrix:\\n',confusion_matrix(y_test, prediction))\n    print(\"\\n Accuracy Percentage  is : {}%\".format(accuracy_score(Ytest,prediction) * 100))\n    ax = sns.heatmap([precision,recall],linewidths= 0.5,cmap=\"YlGnBu\")","5ef505e5":"y_pred = neigh.predict(Xtest)\nacc(Ytest,y_pred);","ae15d011":"Xtrain_un = concatenate(LeX[:246],FrX)\nYtrain_un = concatenate(LeT[:246],FrT)\nXtrain_un.shape,Xtest.shape,Ytrain_un.shape,Ytest.shape","bd07be5e":"Pie(Ytrain_un[Ytrain_un  == 1].sum()\/Ytrain_un.shape[0],text = \"Credit Card Fraud UnderSampled\")","3a235ea1":"Pie(Ytest[Ytest == 1].sum()\/Ytest.shape[0],text = \"Credit Card Fraud Test Sample\")","14bc9d85":"def train(clf,X,Y,x,y):\n    clf.fit(X,Y)\n    y_pred = clf.predict(x)\n    acc(y_pred,y)\n    return y_pred","6f2a1a96":"clf = SVC(C = 5.3, cache_size=1000, class_weight=\"balanced\", coef0=0.0,\n    decision_function_shape='ovr', gamma='auto', kernel='rbf',\n    max_iter=1000,  random_state=None, \n    tol=0.001, verbose=True)\nyp_UNS_svc = train(clf,Xtrain_un,Ytrain_un,Xtest,Ytest)","fb336dc1":"neigh = KNeighborsClassifier(n_neighbors=7,algorithm='auto',n_jobs = 9)\nyp_un_knn = train(neigh,Xtrain_un,Ytrain_un,Xtest,Ytest)","ab2f4043":"xgb = XGBClassifier(max_depth= 9,\n                           learning_rate=0.001,\n                           n_estimators=5000,\n                           objective='binary:logistic',\n                           gamma=0,\n                           seed=1)\ntrain(xgb,Xtrain_un,Ytrain_un,Xtest,Ytest)","2ef4f42e":"rndmfor = RandomForestClassifier(n_estimators=1000, max_depth=13,random_state=0)\nyp_uns_rndmfrs = train(rndmfor,Xtrain_un,Ytrain_un,Xtest,Ytest)","26df6793":"from imblearn.over_sampling import SMOTE\nsm = SMOTE(random_state=2)\nPie(Ytrain[Ytrain  == 1].sum()\/Ytrain.shape[0],text = \"Credit Card Fraud before Sampling\")\nXtrain_smote, Ytrain_smote = sm.fit_sample(Xtrain, Ytrain)\nPie(Ytrain_smote[Ytrain_smote  == 1].sum()\/Ytrain_smote.shape[0],text = \"Credit Card Fraud after SMOTE\")","50af2ba3":"clf = SVC(C = 5.3, cache_size=1000, class_weight=\"balanced\", coef0=0.0,\n    decision_function_shape='ovr', gamma='auto', kernel='rbf',\n    max_iter=1000,  random_state=None, \n    tol=0.001, verbose=True)\nyp_SMT_svc = train(clf,Xtrain_smote, Ytrain_smote,Xtest,Ytest)","0694634d":"neigh = KNeighborsClassifier(n_neighbors=3,algorithm='auto',n_jobs = 9)\nyp_sm_knn = train(neigh,Xtrain_smote, Ytrain_smote,Xtest,Ytest)","4ac94ecb":"logreg = LogisticRegression(C = 0.1)\nyp_sm_lg = train(logreg,Xtrain_smote, Ytrain_smote,Xtest,Ytest)","7b7a602a":"*Note: No Missing Data*","bb4f721f":"## Random Forest Classifier\n","27547371":"## SVC","de9873bc":"## KNN","54f72a4a":"# DATASET\n\n## Context of Data\n\nThe datasets contains transactions made by credit cards in September 2013 by european cardholders. This dataset presents transactions that occurred in two days, where we have 492 frauds out of 284,807 transactions. The dataset is highly unbalanced, the positive class (frauds) account for 0.172% of all transactions.\n\nIt contains only numerical input variables which are the result of a PCA transformation. Unfortunately, due to confidentiality issues, we cannot provide the original features and more background information about the data. Features V1, V2, ... V28 are the principal components obtained with PCA, the only features which have not been transformed with PCA are 'Time' and 'Amount'. Feature 'Time' contains the seconds elapsed between each transaction and the first transaction in the dataset. The feature 'Amount' is the transaction Amount, this feature can be used for example-dependant cost-senstive learning. Feature 'Class' is the response variable and it takes value 1 in case of fraud and 0 otherwise.","6022d767":"## Loading The Dataset","eebaa4ec":"### Random under-sampling for the majority class","a847c9f5":"### Accuracy is Very High but it could not predict many frauds","b1efccf4":"## Basic Model ","1b3e0815":"## XGBOOST","b28bf617":"## SVC","aea00e78":"# SMOTE","0707b73c":"## KNN","1f5d68f7":"## UNDERSAMPLING\n\nThis method works with majority class. It reduces the number of observations from majority class to make the data set balanced. This method is best to use when the data set is huge and reducing the number of training samples helps to improve run time and storage troubles.","0fb5e8ba":"## Spliting Data\n\nWe have around 500 fraud detection and rest is legit. I am going to divide it in 1:1 ratio in Train and Test","58c282c0":"### UNDERSAMPLING Did not worked well!","10c3d2c7":"# Handling Imbalanced Data\n\nThe methods are widely known as \u2018Sampling Methods\u2019. Generally, these methods aim to modify an imbalanced data into balanced distribution using some mechanism. The modification occurs by altering the size of original data set and provide the same proportion of balance.\n\nThese methods have acquired higher importance after many researches have proved that balanced data results in improved overall classification performance compared to an imbalanced data set. Hence, it\u2019s important to learn them.\n\nBelow are the methods used to treat imbalanced datasets:\n\n* Undersampling\n* Oversampling\n* Synthetic Data Generation\n* Cost Sensitive Learning\n \n \n*Source: Analytics Vidhya*","ff350c10":"### Scaling\n\n*We can also see that `Time` and `Amount` are not scaled as rest of our Features*","03fae073":"## Data Visualization and Interpretation\n\nWe can see that the data is highly `Imbalanced` and this lead to high **accuracy** as mostly it predicts the majority class. In real World Secanrio we may notice that fraud to legit transaction ratio is very low. Most of the transactions are legit but we need to prevent each and every fraud transactions. In such cases we want that are *False Positive* should be ***LOW***.\n\n\nWith imbalanced data sets, an algorithm doesn\u2019t get the necessary information about the minority class to make an accurate prediction. Hence, it is desirable to use ML algorithms with balanced data sets. Then, how should we deal with imbalanced datasets?\n\nThe term imbalanced refer to the disparity encountered in the dependent (response) variable. Therefore, an imbalanced classification problem is one in which the dependent variable has imbalanced proportion of classes. In other words, a data set that exhibits an unequal distribution between its classes is considered to be imbalanced."}}