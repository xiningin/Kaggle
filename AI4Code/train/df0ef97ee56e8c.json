{"cell_type":{"f8d19a60":"code","a937338a":"code","4ff44e42":"code","26bb6f95":"code","9af5a180":"code","bc773499":"code","206f438b":"code","3ff482bc":"code","855da475":"code","ed1529ae":"code","2f6f0123":"code","322072cf":"code","0923925d":"code","95459e23":"code","b199f4be":"markdown","3cd4867a":"markdown","783dd315":"markdown","238f7f98":"markdown","226c472d":"markdown","c261dc6c":"markdown","650f2361":"markdown","ce416dcb":"markdown","77090969":"markdown","77f76ebd":"markdown","c075f557":"markdown","7aa014ec":"markdown","8e9dfa0b":"markdown","92dd41aa":"markdown","db7348e5":"markdown","fbd54eee":"markdown","2cdddcfb":"markdown","af885528":"markdown","60a3dfeb":"markdown","02174ff2":"markdown"},"source":{"f8d19a60":"import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\ndataset = pd.read_csv('..\/input\/Bank_registries.csv')\nprint(dataset.shape)\ndataset.head()","a937338a":"X = dataset.iloc[:, 3:13].values\ny = dataset.iloc[:, 13].values\npd.DataFrame(X[0:4])","4ff44e42":"#Label Encoder transforma a numeros los niveles de la variable categorica. \n#OneHotEncoder desdobla en k-columnas binarias los k-niveles de cada variable\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\n\n#Cargamos el modelo y transformamos los niveles categoricos a numeros consecutivos para (Geograpy y gender)\nlabelencoder_X_1 = LabelEncoder()\nX[:, 1] = labelencoder_X_1.fit_transform(X[:, 1])\nlabelencoder_X_2 = LabelEncoder()\nX[:, 2] = labelencoder_X_2.fit_transform(X[:, 2])\npd.DataFrame(X).describe()\nprint(dataset.columns)\nX[1:10]\n","26bb6f95":"#hacemos Dummy Encodign, generando k-1 nuevas columnas para los k niveles de las variables categoricas\nonehotencoder = OneHotEncoder(categorical_features = [1])\nX = onehotencoder.fit_transform(X).toarray()\nX = X[:, 1:]\npd.DataFrame(X).describe()","9af5a180":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)","bc773499":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)","206f438b":"import keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense","3ff482bc":"classifier = Sequential()","855da475":"classifier.add(Dense(output_dim = 6, init = 'uniform', activation = 'relu', input_dim = 11))\nclassifier.add(Dense(output_dim = 6, init = 'uniform', activation = 'relu'))\nclassifier.add(Dense(output_dim = 1, init = 'uniform', activation = 'sigmoid'))","ed1529ae":"classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])","2f6f0123":"classifier.fit(X_train, y_train, batch_size = 100, nb_epoch = 500)\n","322072cf":"y_pred = classifier.predict(X_test)\ny_pred = (y_pred > 0.5)","0923925d":"from sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test, y_pred)\ncm","95459e23":"good = (cm[0][0] + cm[1][1])\/np.sum(cm)\nprint (good)","b199f4be":"## Por fin! seguimos modelando datos...","3cd4867a":"Dividimos los datos en Train (80%) y test (20%)","783dd315":"### A compilar!! (esto no era python??)","238f7f98":"Primero cargamos los datos, importamos librerias y vemos que forma tienen","226c472d":"Realmente no es un accuracy \u00e9pico, pero tampoco nos hemos matado a trabajar.\nRecuerda, tienes m\u00e1s camino por delante que por detras para profundizar en esto!!\n\n(pd: cualquier duda, contacta! :D)","c261dc6c":"Normalizamos los datos con Standard Scaler: Media = 0 y desviaci\u00f3n standar = 1","650f2361":"Vemos que tenemos *malvadas* variables categoricas as\u00ed que aplicamos one hot encoding y Dummy encoding.\n\n\u00bfQu\u00e9 mierda era eso?  \n    One Hot Enconding consiste en binarizar variables categoricas  \n    Dummy Encondig consiste en desdoblar una variable categorica en tantas columnas como niveles tenga, menos una.\n   \n![](https:\/\/sayingimages.com\/wp-content\/uploads\/Wtf-Lol-meme.png)\n\nPara el caso de los paises, puesto que tenemos k niveles, creamos k-1 nuevas columnas, donde representaremos con un 1(True) o 0(False) la pertenencia de esa persona a ese pais. ejemplo:\n\n\n|  | Alemania | Espa\u00f1a |\n|---------|----------|--------|\n| Alem\u00e1n | 1 | 0 |\n| Espa\u00f1ol | 0 | 1 |\n| Franc\u00e9s | 0 | 0 |  \n\nDe esta manera, para 3 (k) niveles, representamos toda la informacion con 2(k-1) columnas","ce416dcb":"![](https:\/\/media.makeameme.org\/created\/we-good-zgv5sb.jpg)","77090969":"### A predecir!\n\nseparamos las predicciones a un valor u otro con 0.5 como punto de corte","77f76ebd":"## Party time!! (o limpieza de datos, segun se mire...)","c075f557":"# **Redes neuronales aplicadas a datos bancarios!!**","7aa014ec":"Despu\u00e9s separamos las variables dependientes de la variable independiente a predecir (Exited). Ignoramos las columnas RowNumber, CustomerID y Surname porque no aportan valor ","8e9dfa0b":"## Ahora si... Redes neuronales!!","92dd41aa":"### Let's train al night long!!\n\n![](https:\/\/survivalpioneer.com\/wp-content\/uploads\/2018\/12\/Thomas-The-Train-Meme-16-200x300.jpg)\n\nEntrenamos la red con... mas parametros!!!  \n\n* Batch_size--> n\u00famero de observaciones que la red necesita entrenar antes de actualizar los pesos. \n* Epoch--> n\u00famero de iteraciones que realizaremos. No hay una regla especifica para escoger estos dos valores por lo que hay que hacerlo a prueba (esperabas ciencia verdad??)","db7348e5":"### Are we good?\n\nCalculamos la matriz de confusion para ver qu\u00e9 tal nos ha ido:\n\n","fbd54eee":"Has visto como subia ese accuracy?? mmmm... acuuuracy","2cdddcfb":"### Empecemos!\n![](https:\/\/memecrunch.com\/meme\/59A89\/let-s-party\/image.gif?w=499&c=1)\n\nA nuestra funcion le a\u00f1adir\u00e9mos capas(.add) con la funcion Dense. Pero... y los parametros?\n\n* Output_dim-->n\u00ba de nodos en la capa  \n* init--> inicializacion del descenso de gradiente estoc\u00e1stico (se que lo sabes, pero por si necesitas recordar... [link](https:\/\/unipython.com\/descenso-gradientes-estocastico-sgd\/)) en este caso la distribuci\u00f3n inicial de pesos de cada nodo sigue una variable aleatoria uniforme.  \n* input_dim--> es el numero de variables de entrada, el resto de capas lo heredan.  \n* Activation--> cada neurona tiene una funcion de activaci\u00f3n que determina la *intensidad* (max. 1) con la que transmite su se\u00f1al a la siguiente capa. las dos primeras capas usan la funcion [ReLU](https:\/\/es.wikipedia.org\/wiki\/Rectificador_(redes_neuronales) y la ultima una [sigmoide](https:\/\/es.wikipedia.org\/wiki\/Funci%C3%B3n_sigmoide) para clasificar\n\n![](https:\/\/i.stack.imgur.com\/bzQb3.png)\n\n![Definicion matem\u00e1tica](https:\/\/i.stack.imgur.com\/VqOpE.jpg \"Math jiberish\")\n\n","af885528":"M\u00e1s argumentos!!  \n\n* Optimizer--> [Adam](https:\/\/data.sngular.com\/es\/art\/60\/adam-automated-discovery-and-analysis-machine) es el algoritmo de descenso de gradiente estoc\u00e1stico que seleccionar\u00e1 los pesos \u00f3ptimos de la red.  \n* Loss--> funcion de perdida a optimizar. En este caso es una clasificacion binaria por lo que... [binary_crossentropy](https:\/\/towardsdatascience.com\/understanding-binary-cross-entropy-log-loss-a-visual-explanation-a3ac6025181a) (si fueran m\u00e1s categorias usariamos [categorical_crossentropy](https:\/\/gombru.github.io\/2018\/05\/23\/cross_entropy_loss\/))  \n* metrics-->Estamos muy arriba como para no incluir una metrica que nos diga cu\u00e1nto lo est\u00e1 petando nuestra red","60a3dfeb":"Con Sequetial inicializaremos la red y con Dense a\u00f1adiremos las capas ocultas","02174ff2":"## **Carga de datos **"}}