{"cell_type":{"d4878577":"code","597e22b6":"code","d388708a":"code","9316552a":"code","dc57c20d":"code","d7e88d5e":"code","b1f80c62":"code","31f7aece":"code","c95451e8":"code","3380b101":"code","8e3dec31":"code","d26753f7":"code","4cb26e5f":"code","696fda92":"code","6a91e3f0":"code","8b631a56":"code","8e6375ea":"code","48356536":"code","b07ce0f5":"code","42640df2":"code","62ef14ca":"code","0e3c2254":"code","be3c8f2e":"code","cadb2238":"code","9d2a88fc":"code","48bf1587":"code","05ae3a6f":"code","4f42f4cb":"code","19a41ae4":"code","ee85c49f":"code","8565425e":"code","8ead95a0":"code","16ee80bb":"code","6e55d0b9":"code","d82a8c02":"code","dc4b3f53":"code","6a3dfc9b":"code","ec0ae8cc":"code","e82bf3a9":"code","12e7221b":"code","f5558d97":"code","ff50c567":"code","db2cc45f":"code","21c67273":"code","ade280ea":"code","f35dcda7":"code","85d29dad":"code","8862fcb6":"code","0d07931e":"code","0cd0c1ee":"code","09bf252f":"code","f11cd05a":"code","651ffe68":"code","485cc31b":"code","6500496b":"code","0894bd83":"code","847d330a":"markdown","7de00b0a":"markdown","5ca0ddc9":"markdown","a2cb3232":"markdown","e05e05a1":"markdown","be03fabb":"markdown","5904210f":"markdown","88ea6692":"markdown","4aa9d90f":"markdown","66fa0622":"markdown","c8e6a0a3":"markdown","7e4f8ffc":"markdown","a5371b99":"markdown","f71a9e3a":"markdown","0df22c24":"markdown","ed4cc6d0":"markdown","bfa41e34":"markdown","e5119f3e":"markdown"},"source":{"d4878577":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport string\n%matplotlib inline\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","597e22b6":"train = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/test.csv')","d388708a":"baseLoc = '\/kaggle\/input\/nlp-getting-started\/'\ntrain_data = pd.read_csv(baseLoc + 'train.csv')\ntrain_data.shape","9316552a":"train.head()","dc57c20d":"train.info()","d7e88d5e":"train.describe()","b1f80c62":"train.isnull().sum()","31f7aece":"train.shape","c95451e8":"train.tail()","3380b101":"test.head()","8e3dec31":"target_value_counts = train['target'].value_counts()\ntarget_value_counts","d26753f7":"non_disasterous = train[train['target'] == 0]\nnon_disasterous","4cb26e5f":"disasterous = train[train['target'] == 1]\ndisasterous","696fda92":"sns.barplot(x = target_value_counts.index, y = target_value_counts)","6a91e3f0":"keywords_value_counts = train['keyword'].value_counts()\nkeywords_value_counts[:30]","8b631a56":"figure = plt.figure(figsize=(7,6))\nsns.barplot(x = keywords_value_counts[:20], y = keywords_value_counts.index[:20])","8e6375ea":"location_value_counts = train['location'].value_counts()\nlocation_value_counts[:20]","48356536":"figure = plt.figure(figsize=(7,6))\nsns.barplot(x = location_value_counts[:20], y = location_value_counts.index[:20])","b07ce0f5":"import re","42640df2":"# Removing punctuation, html tags, symbols, numbers, etc\n\ndef remove_noise(text):\n    # Dealing with Punctuation\n    text = re.sub('\\[.*?\\]', '', text)\n    text = re.sub('https?:\/\/\\S+|www\\.\\S+', '', text)\n    text = re.sub('<.*?>+', '', text)\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n    text = re.sub('\\n', '', text)\n    text = re.sub('\\w*\\d\\w*', '', text)\n    return text","62ef14ca":"train['text'] = train['text'].apply(lambda x:remove_noise(x))\ntest['text'] = test['text'].apply(lambda x:remove_noise(x))","0e3c2254":"# Converting the upper case to lower case\ntrain['text'] = train['text'].apply(lambda x : x.lower())\ntest['text'] = test['text'].apply(lambda x : x.lower())","be3c8f2e":"train['text'][2000]","cadb2238":"test['text'][1000]","9d2a88fc":"train.head()","48bf1587":"import nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\n\nstop = stopwords.words('english')","05ae3a6f":"def remove_stopwords(text):\n    text = [item for item in text.split() if item not in stop]\n    return ' '.join(text)\n\ntrain['cleaned_text'] = train['text'].apply(remove_stopwords)\ntest['cleaned_text'] = test['text'].apply(remove_stopwords)","4f42f4cb":"train.head()","19a41ae4":"test.head()","ee85c49f":"from nltk.stem.porter import PorterStemmer\n\nstemmer = PorterStemmer()\n\ndef stemming(text):\n    text = [stemmer.stem(word) for word in text.split()]\n    return ' '.join(text)\n    \ntrain['stemed_text'] = train['cleaned_text'].apply(stemming)\ntest['stemed_text'] = test['cleaned_text'].apply(stemming)\n\ntrain.head()","8565425e":"from wordcloud import WordCloud\nimport matplotlib.pyplot as plt\nfig, (ax1) = plt.subplots(1, figsize=[7, 7])\nwordcloud = WordCloud( background_color='white',\n                        width=600,\n                        height=600).generate(\" \".join(train['stemed_text']))\nax1.imshow(wordcloud)\nax1.axis('off')\nax1.set_title('Frequent Words',fontsize=16);","8ead95a0":"from sklearn.feature_extraction.text import CountVectorizer\n\ncount_vectorizer = CountVectorizer(analyzer='word', binary=True)\ncount_vectorizer.fit(train['stemed_text'])\n\ntrain_vectors = count_vectorizer.fit_transform(train['stemed_text'])\ntest_vectors = count_vectorizer.transform(test['stemed_text'])\n\nprint(train_vectors[0].todense())","16ee80bb":"y = train['target']","6e55d0b9":"def plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    See full source and example: \n    http:\/\/scikit-learn.org\/stable\/auto_examples\/model_selection\/plot_confusion_matrix.html\n    \n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, cm[i, j],\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')","d82a8c02":"from sklearn.metrics import accuracy_score, log_loss, confusion_matrix, f1_score\nimport numpy as np \nimport itertools","dc4b3f53":"from sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split(train_vectors, y , test_size=0.30, random_state=42)","6a3dfc9b":"from sklearn.naive_bayes import MultinomialNB\n\nmulti_nb = MultinomialNB(alpha=1.6)\n\nmulti_nb.fit(x_train, y_train)","ec0ae8cc":"pred = multi_nb.predict(x_test)","e82bf3a9":"multi_f1_score = f1_score(y_test, pred)\nprint(\"The F1 score for MultinomialNB is : {}\".format(multi_f1_score))\n\nacc_score = accuracy_score(y_test, pred)\n(\"The accuracy for MultinomialNB is : \",acc_score)","12e7221b":"conf_matrix = confusion_matrix(y_test, pred)\nplot_confusion_matrix(conf_matrix, classes=['FAKE', 'REAL'])","f5558d97":"from sklearn.model_selection import cross_val_score\nscore = cross_val_score(multi_nb, train_vectors, y, cv=3, scoring='f1', )\nscore","ff50c567":"multinb_classifier = MultinomialNB(alpha=0.1)","db2cc45f":"previous_score = 0\n\n# We are taking values from 0 to 1 with an increament of 0.1 \n\nfor alpha in np.arange(0,2,0.1):\n    sub_classifier = MultinomialNB(alpha=alpha)\n    sub_classifier.fit(x_train, y_train)\n    y_pred = sub_classifier.predict(x_test)\n    score = accuracy_score(y_test, y_pred)\n    \n    if score> previous_score:\n        classifier = sub_classifier\n        print(\"Alpha is : {} & Accuracy is : {}\".format(alpha, score))","21c67273":"from sklearn.linear_model import LogisticRegression\nlog_regression = LogisticRegression()\n\nlog_regression.fit(x_train, y_train)\n\nlog_pred = log_regression.predict(x_test)\n\nlog_acc_score = accuracy_score(y_test, log_pred)\nprint(\"The accuracy score for logistic regression is : \",log_acc_score)\n\nlog_reg_f1_score = f1_score(y_test, pred)\nprint(\"The F1 score for Logistic Regression is : {}\".format(log_reg_f1_score))","ade280ea":"conf_matrix = confusion_matrix(y_test, log_pred)\nplot_confusion_matrix(conf_matrix, classes=['FAKE', 'REAL'])","f35dcda7":"from sklearn.tree import DecisionTreeClassifier\n\ndtree = DecisionTreeClassifier()\n\ndtree.fit(x_train, y_train)\ndtree_predict = dtree.predict(x_test)\n\ndtree_score = accuracy_score(y_test, dtree_predict)\nprint(\"The accuracy score for Decision Tree is : {}\".format(dtree_score))\n\nconf_matrix = confusion_matrix(y_test, dtree_predict)\nplot_confusion_matrix(conf_matrix, classes=['FAKE', 'REAL'])\n\ndecision_tree_f1_score = f1_score(y_test, pred)\nprint(\"The F1 score for Decision Tree is : {}\".format(decision_tree_f1_score))","85d29dad":"from sklearn.ensemble import RandomForestClassifier\n\nrandom_forest = RandomForestClassifier()\nrandom_forest.fit(x_train, y_train)\n\nrandom_forest_predict = random_forest.predict(x_test)\n\nrandom_forest_score = accuracy_score(y_test, random_forest_predict)\nprint(\"The accuracy score for Decision Tree is : {}\".format(random_forest_score))\n\nconf_matrix = confusion_matrix(y_test, random_forest_predict)\nplot_confusion_matrix(conf_matrix, classes=['FAKE', 'REAL'])\n\nrandom_forest_f1_score = f1_score(y_test, pred)\nprint(\"The F1 score for Random Forest Classifier is : {}\".format(random_forest_f1_score))","8862fcb6":"import xgboost as xgb\n\nxgboost = xgb.XGBClassifier()\n\nxgboost.fit(x_train, y_train)\nxgboost_pred = xgboost.predict(x_test)\n\nxgboost_score = accuracy_score(y_test, xgboost_pred)\nprint(\"The accuracy score for XGboost is : {}\".format(xgboost_score))\n\nconf_matrix = confusion_matrix(y_test, xgboost_pred)\nplot_confusion_matrix(conf_matrix, classes=['FAKE', 'REAL'])\n\nxgboost_f1_score = f1_score(y_test, pred)\nprint(\"The F1 score for XGBoost is : {}\".format(xgboost_f1_score))","0d07931e":"from sklearn.neighbors import KNeighborsClassifier\n\nknn_clf = KNeighborsClassifier(n_neighbors = 3)\nknn_clf.fit(x_train, y_train)\ny_pred_knn = knn_clf.predict(x_test)\nacc_knn = knn_clf.score(x_train, y_train)\n\nprint(\"The accuracy score for knn is : {}\".format(acc_knn))\n\nconf_matrix = confusion_matrix(y_test, y_pred_knn)\nplot_confusion_matrix(conf_matrix, classes=['FAKE', 'REAL'])\n\nknn_f1_score = f1_score(y_test, pred)\nprint(\"The F1 score for K-NN Classifier is : {}\".format(knn_f1_score))","0cd0c1ee":"from sklearn.svm import SVC, LinearSVC\n\nsvm_clf = SVC()\nsvm_clf.fit(x_train, y_train)\ny_pred_svc = svm_clf.predict(x_test)\nacc_svc = svm_clf.score(x_train, y_train)\n\nprint(\"The accuracy score for SVM is : {}\".format(acc_svc))\n\nconf_matrix = confusion_matrix(y_test, y_pred_svc)\nplot_confusion_matrix(conf_matrix, classes=['FAKE', 'REAL'])\n\nsvm_f1_score = f1_score(y_test, pred)\nprint(\"The F1 score for SVM is : {}\".format(svm_f1_score))","09bf252f":"linear_svc_clf = LinearSVC()\nlinear_svc_clf.fit(x_train, y_train)\ny_pred_linear_svc = linear_svc_clf.predict(x_test)\nacc_linear_svc =linear_svc_clf.score(x_train, y_train)\n\nprint(\"The accuracy score for SVM is : {}\".format(acc_linear_svc))\n\nconf_matrix = confusion_matrix(y_test, y_pred_linear_svc)\nplot_confusion_matrix(conf_matrix, classes=['FAKE', 'REAL'])\n\nlinear_svm_f1_score = f1_score(y_test, pred)\nprint(\"The F1 score for Linear SVM is : {}\".format(linear_svm_f1_score))","f11cd05a":"from sklearn.linear_model import SGDClassifier\n\nsgdc_clf = SGDClassifier(max_iter=5, tol=None)\nsgdc_clf.fit(x_train, y_train)\ny_pred_sgd = sgdc_clf.predict(x_test)\nacc_sgd = sgdc_clf.score(x_train, y_train)\n\nprint(\"The accuracy score for SVM is : {}\".format(acc_sgd))\n\nconf_matrix = confusion_matrix(y_test, y_pred_sgd)\nplot_confusion_matrix(conf_matrix, classes=['FAKE', 'REAL'])\n\nsgdc_f1_score = f1_score(y_test, pred)\nprint(\"The F1 score for SGD is : {}\".format(sgdc_f1_score))","651ffe68":"models = pd.DataFrame({\n    'Model': ['MultinomialNB', 'Decision Tree', 'Random Forest', 'XgBoost', 'K-NN Classifier', 'SVM', 'linear SVM', 'SGD'],\n    \n    'Score': [acc_score, dtree_score, random_forest_score, xgboost_score, acc_knn, acc_svc, acc_svc, acc_sgd],\n    \n    'F1 Score' : [multi_f1_score, decision_tree_f1_score, random_forest_f1_score, xgboost_f1_score, knn_f1_score, svm_f1_score,\n                 linear_svm_f1_score, sgdc_f1_score]\n    })\n\nmodels.sort_values(by='Score', ascending=False)","485cc31b":"sample_submission = pd.read_csv(baseLoc + \"sample_submission.csv\")\n# Predicting model with the test data that was vectorized (test_vectors)\nsample_submission['target'] = multi_nb.predict(test_vectors)","6500496b":"sample_submission.to_csv(\"submission3.csv\", index=False)\n","0894bd83":"sample_submission.head(20)","847d330a":"#### Submission file to Kaggle","7de00b0a":"#### Comparing the models based on accuracy\n\n* Let's compare the accuracy score of all the classifier models used above.","5ca0ddc9":"### Applying MultinomialMB","a2cb3232":"### Using CountVectorizer to convert tweets into vectors","e05e05a1":"* Removing stopwords","be03fabb":"### Applying Linear SVM Algorithm","5904210f":"* Removing the punctuation from the dataset\n* Replacing the punctuation with blank spaces","88ea6692":"* removing stopwords","4aa9d90f":"### Applying Random Forest Classifier","66fa0622":"# Data Preprocessing","c8e6a0a3":"### Algorithm K-NN Classifier","7e4f8ffc":"## Hyperparameter tuning for MultinomialNB","a5371b99":"## Applying decision tree algorithm","f71a9e3a":"There are 4342 non-disasterous tweets and there are 3271 disasterous tweets","0df22c24":"### Applying Support Vector Machine (SVM)","ed4cc6d0":"## Applying Xgboot Algorithm","bfa41e34":"### Applying Stochastic Gradient Descent (SGD)","e5119f3e":"## Applying Logistic Regression"}}