{"cell_type":{"c51dee9a":"code","8ff9950a":"code","eeccdc4a":"code","8b647877":"code","350a3f0a":"code","9ddb2080":"code","4e3c218b":"code","5c983993":"code","058f8288":"code","8a19b92e":"code","842df3cb":"code","77040a38":"code","47910745":"code","d5b5d8c2":"code","f5e98d86":"code","9fe74eb4":"code","d7d0400b":"markdown"},"source":{"c51dee9a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nimport pandas as pd\nfrom tqdm.notebook import tqdm\nimport torch\nimport transformers as ppb # pytorch transformers\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom nltk.tokenize import word_tokenize\n\nimport nltk\nnltk.download('punkt')\n\nimport numpy as np\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, TfidfTransformer\nfrom sklearn.multioutput import MultiOutputClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import f1_score\nfrom sklearn.calibration import CalibratedClassifierCV\nfrom sklearn.model_selection import cross_val_score, cross_val_predict\nfrom sklearn.svm import LinearSVC\nfrom sklearn import metrics\nfrom sklearn.decomposition import TruncatedSVD\nfrom nltk.stem import WordNetLemmatizer  \nfrom nltk.stem.snowball import SnowballStemmer \nimport re\nimport string\nfrom nltk.tokenize import word_tokenize\nimport re\nfrom nltk.corpus import stopwords\nnltk.download('stopwords')\nnltk.download('wordnet')\n\nfrom sklearn.linear_model import SGDRegressor, SGDClassifier\nimport lightgbm as lgbm\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","8ff9950a":"pd.set_option(\"max.column\", 200)\nTARGET_COLUMNS = ['action', 'adventure', 'animation', 'biography', 'comedy',\n                    'crime', 'drama', 'family', 'fantasy', 'history', 'horror', 'music',\n                    'musical', 'mystery', 'romance', 'sci-fi', 'sport', 'thriller', 'war', 'western']","eeccdc4a":"class Config:\n    TRAIN = \"\/kaggle\/input\/made-hw-2\/train.csv\"\n    TEST = \"\/kaggle\/input\/made-hw-2\/test.csv\"\n    \n    PREP_TRAIN_NAME = f\".\/prep_train.csv\"\n    FOLD_TRAIN_NAME = f\".\/fold_train.csv\"\n    PREP_TEST_NAME = f\".\/prep_test.csv\"\n    \n    DATA_PATH = \".\"\n    DATA_TRAIN = f\"{DATA_PATH}\/prep_train.csv\"\n    DATA_TEST = f\"{DATA_PATH}\/prep_test.csv\"","8b647877":"transform_gen = lambda x: str(x).strip().strip(\"u'\")\n\ndef convert_genres_to_list(data: pd.DataFrame):\n    data['genres'] = data.genres.apply(lambda x: list(map(transform_gen, x[1:-1].split(','))))\n    return data\n\ndef preprocess_train():\n    train = pd.read_csv(Config.TRAIN)\n    train = convert_genres_to_list(train)\n    for new_col in tqdm(TARGET_COLUMNS, desc=\"Create target columns [train]\", \n                                            total=len(TARGET_COLUMNS)):\n        train[new_col] = 0\n\n    for _id, genre_column_list in tqdm(zip(train.id.values, train.genres.values), \n                                    desc=\"Convert target column to int [train]\", \n                                    total=len(train)):\n        for genre in genre_column_list:\n            train.loc[_id, genre] = 1\n    train.to_csv(Config.PREP_TRAIN_NAME, index=False)\n\ndef preprocess_test():\n    test = pd.read_csv(Config.TEST)\n    for new_col in tqdm(TARGET_COLUMNS, desc=\"Create target columns [test]\", \n                                            total=len(TARGET_COLUMNS)):\n        test[new_col] = 0\n    test.to_csv(Config.PREP_TEST_NAME, index=False)\n\ndef preprocess():\n    preprocess_train()\n    preprocess_test()","350a3f0a":"preprocess()","9ddb2080":"train = pd.read_csv(Config.DATA_TRAIN).sample(frac=1, random_state=2020).reset_index(drop=True)\ntest = pd.read_csv(Config.DATA_TEST)","4e3c218b":"def metric_f1_for_desc(desc, y_true, y_pred):\n    print(desc)\n    print(\"Micro: \", f1_score(y_true, y_pred, average='micro'))\n    print(\"Macro: \", f1_score(y_true, y_pred, average='macro'))\n    print(\"None: \", [(column, score) for score, column in zip(f1_score(y_true, y_pred, average=None), TARGET_COLUMNS)])\n    print(\"Samples: \", f1_score(y_true, y_pred, average='samples'))\n    print(\"Weighted: \", f1_score(y_true, y_pred, average='weighted'))","5c983993":"class SortVectorizer():\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X, y=None):\n        X.sort_indices()\n        return X\n    \nclass CountVectorizerDF:\n    \n    def __init__(self, tokenizer=word_tokenize, token_pattern=None):\n        self.vectorizer = CountVectorizer(tokenizer=tokenizer, token_pattern=token_pattern)\n        \n    def fit(self, X, y=None):\n        self._check_and_throw_if_failed_df(X)\n        self.vectorizer.fit(X[\"dialogue\"].values)\n        return self\n    \n    def transform(self, X):\n        self._check_and_throw_if_failed_df(X)\n        return self.vectorizer.transform(X[\"dialogue\"].values)\n    \n    def _check_and_throw_if_failed_df(self, X):\n        if not (type(X) is pd.DataFrame):\n            raise RuntimeError('Not a DF!')\n        if not ('dialogue' in X.columns):\n            raise RuntimeError('Not found dialogue column!')\n            \n            \nclass BayesRatioCalc:\n    EPS = 1e-9\n    def __init__(self, columns=TARGET_COLUMNS):\n        self.columns = columns\n        self.ratio = {i: None  for i in range(len(self.columns))}\n        \n\n        \n    def fit(self, x_train, y_train=None):\n        for index, column in tqdm(enumerate(self.columns)):\n            count_words_class_1 = x_train[(y_train[column] == 1).ravel()].sum(0)\n            count_words_class_0 = x_train[(y_train[column] == 0).ravel()].sum(0)\n            condition_prob_word_given_1 = (count_words_class_1 + 1) \/ (count_words_class_1.sum() + 1)\n            condition_prob_word_given_0 = (count_words_class_0 + 1) \/ (count_words_class_0.sum() + 1)\n\n            self.ratio[column] = np.log(condition_prob_word_given_1 \/ (condition_prob_word_given_0 + self.EPS))\n        return self\n    \n    def transform(self, X):\n        return { column: X.multiply(self.ratio[column]) for column in self.columns }\n    \n\n    \nclass NBSvmClf:\n\n    def __init__(self, threshold=0.275, n_classes=20, columns=TARGET_COLUMNS):\n        self.n_classes = n_classes\n        self.clfs = [None] * self.n_classes\n        self.columns = columns\n        self.threshold = threshold\n        for i in range(self.n_classes):\n            self.clfs[i] = LogisticRegression(C=0.4, dual=True, max_iter=8_000, solver=\"liblinear\", random_state=2020)\n        \n    def fit(self, x_train_nb, y_train=None):\n        for index, column in enumerate(self.columns):\n            self.clfs[index].fit(x_train_nb[column], y_train[column])\n        return self\n    \n    def predict_proba(self, x_test_nb):\n        return self._predict(x_test_nb)\n    \n    def predict(self, x_test_nb):\n        prediction = self._predict(x_test_nb)\n        return (prediction >= self.threshold).astype(int)\n\n    def _predict(self, x_test_nb):\n        prediction = np.zeros((np.shape(x_test_nb[self.columns[0]])[0], self.n_classes))\n        for index, column in enumerate(self.columns):\n            prediction[:, index] = self.clfs[index].predict_proba(x_test_nb[column])[:, 1]  \n        return prediction\n    \nclass Preprocess:\n    \n    def __init__(self):\n        pass\n    \n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X):\n        self._check_and_throw_if_failed_df(X)\n        X.loc[:, \"dialogue\"] = X[\"dialogue\"].apply(lambda text: process(text)).values\n        return X\n    \n    def _check_and_throw_if_failed_df(self, X):\n        if not (type(X) is pd.DataFrame):\n            raise RuntimeError('Not a DF!')\n        if not ('dialogue' in X.columns):\n            raise RuntimeError('Not found dialogue column!')\n            ","058f8288":"pipeline = Pipeline([\n    (\"vec\", CountVectorizerDF()),\n    (\"sort_vec\", SortVectorizer()),\n    (\"bayes\", BayesRatioCalc()),\n    (\"nbsvm\", NBSvmClf()),\n])","8a19b92e":"x_train_id, x_valid_id = train_test_split(train.id.values, random_state=2020, shuffle=True)","842df3cb":"df_train = train[train[\"id\"].isin(x_train_id)]\ndf_target_train = df_train[TARGET_COLUMNS]\ndf_valid = train[train[\"id\"].isin(x_valid_id)]\ndf_target_valid = df_valid[TARGET_COLUMNS]","77040a38":"%%time\npipeline.fit(df_train, df_target_train)","47910745":"metric_f1_for_desc(desc='NB-SVM', y_true=df_target_valid.values, y_pred=pipeline.predict(df_valid))","d5b5d8c2":"%%time\npipeline.fit(train, train[TARGET_COLUMNS])","f5e98d86":"def create_output_test(test, pipeline, output_file):\n    test[\"genres\"] = \"\"\n    test_prediction = pipeline.predict(test)\n    result = []\n    add_result = result.append\n    for target_digits in tqdm(test_prediction):\n        add_result(\" \".join([genre for label, genre in filter(lambda x: True if x[0] == 1 else False, \n                                                   [(digit_label, column) for digit_label, column in zip(target_digits, TARGET_COLUMNS)])]))\n    test[\"genres\"] = result    \n    test[[\"id\", \"genres\"]].head(150)\n    test[[\"id\", \"genres\"]].to_csv(output_file, index=False)","9fe74eb4":"create_output_test(test, pipeline, output_file=\"nb_svm_full.csv\")","d7d0400b":"# Full"}}