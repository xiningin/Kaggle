{"cell_type":{"f639c4da":"code","b0a5f0f2":"code","a1978ab5":"code","de875310":"code","1489ef07":"code","9a81e737":"code","f3863843":"code","05542d52":"code","539fadb3":"code","d9ba2526":"code","c9b7805f":"code","98a02f0d":"code","c0c36d28":"code","20c3973e":"code","c845288a":"code","61661a73":"code","37d07c23":"code","b92454a3":"code","783526b8":"code","30466e1f":"code","67e626d4":"code","02ca6dbb":"code","15fd4275":"code","b2146e64":"code","835b2eee":"code","c79e5fad":"code","863b5063":"code","e49b2383":"markdown","d6fd215a":"markdown","750fb7b5":"markdown","f3a4fd1d":"markdown","43ef72f5":"markdown","b9cc510e":"markdown","e5a4f7d2":"markdown","1b74a28a":"markdown","c016750a":"markdown","3974c82d":"markdown","96b785e2":"markdown","883d3d5b":"markdown","19c8fdac":"markdown","644405e9":"markdown","fd570fb9":"markdown","7565e211":"markdown"},"source":{"f639c4da":"# !pip install git+https:\/\/github.com\/rwightman\/pytorch-image-models","b0a5f0f2":"import os\nimport gc\nimport sys\nimport cv2\nimport copy\nimport time\nimport random\nfrom PIL import Image\n# from pickle import dump, load\n\n# Data Manipulation\nimport numpy as np\nimport pandas as pd\n\n# Pytorch Imports\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim import lr_scheduler\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.cuda import amp\n\n# Util Imports\n# https:\/\/docs.python.org\/3\/library\/collections.htmlos.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n# https:\/\/stackoverflow.com\/questions\/5900578\/how-does-collections-defaultdict-work\nimport joblib \nfrom tqdm import tqdm\nfrom collections import defaultdict\n\n# Scikit-Learn Imports\nfrom sklearn.metrics import mean_squared_error as MSE\nfrom sklearn.model_selection import StratifiedKFold, KFold\n\n# https:\/\/rwightman.github.io\/pytorch-image-models\/\n# import timm\n\n# Albumentations for Augmentations\n# https:\/\/albumentations.ai\/docs\/\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\n\n# `CUDA_LAUNCH_BLOCKING` make cuda report the error where it actually occurs.\nos.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"","a1978ab5":"# # https:\/\/www.kaggle.com\/kozodoi\/timm-pytorch-image-models\nsys.path.append('..\/input\/timm-pytorch-image-models\/pytorch-image-models-master')\nimport timm\nprint(timm.__version__)","de875310":"ROOT_DIR = \"..\/input\/petfinder-pawpularity-score\"\nTRAIN_DIR = \"..\/input\/petfinder-pawpularity-score\/train\"\nTEST_DIR = \"..\/input\/petfinder-pawpularity-score\/test\"","1489ef07":"CONFIG = dict(\n    seed = 42, model_name = 'tf_efficientnet_b4_ns', train_batch_size = 16,\n    valid_batch_size = 32, img_size = 512, epochs = 5, learning_rate = 1e-4,\n    scheduler = 'CosineAnnealingLR', min_lr = 1e-6, T_max = 20, T_0 = 25,\n    warmup_epochs = 0, weight_decay = 1e-6, n_accumulate = 1, n_fold = 5, \n    num_classes = 1, device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"),\n    competition = 'PetFinder', _wandb_kernel = 'ele'\n)","9a81e737":"# Sets the seed for the entire notebook, so that we can reproduce our results\ndef set_seed(seed = 42):\n    np.random.seed(seed)\n    random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    # Set a fixed value for the hash seed\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    # If True, causes cuDNN to only use deterministic convolutional algorithms\n    torch.backends.cudnn.deterministic = True\n    # If True, causes cuDNN to benchmark multiple convolution algorithms and select the fastest\n    torch.backends.cudnn.benchmark = False\n    \nset_seed(CONFIG['seed'])","f3863843":"def get_train_file_path(id):\n    return f\"{TRAIN_DIR}\/{id}.jpg\"","05542d52":"df = pd.read_csv(f\"{ROOT_DIR}\/train.csv\")\ndf['file_path'] = df['Id'].apply(get_train_file_path)\nprint(df.shape)","539fadb3":"# Finding out the feature columns\nfeature_cols = [col for col in df.columns if col not in ['Id', 'Pawpularity', 'file_path']]\nprint(feature_cols)","d9ba2526":"# https:\/\/pandas.pydata.org\/docs\/reference\/api\/pandas.cut.html\n# https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.KFold.html\n\ndef create_folds(df, n_splits = 5, n_groups = None):\n    df['kfold'] = 1\n    \n    # Corresponds to the case when we have a classification setting\n    # Will be creating folds on the basis of the target variable simply\n    if n_groups is None:\n        fold = KFold(n_splits = n_splits, random_state = CONFIG['seed'])\n        target = df['Pawpularity']\n        \n    # Corresponds to the case when we have a regression setting\n    # We will bin the target variable first, which will give us a setting similar to that of\n    # classification, and then, we will create the folds on the basis of the binned target variable\n    else:\n        fold = StratifiedKFold(n_splits = n_splits, shuffle = True, random_state = CONFIG['seed'])\n        target = pd.cut(df['Pawpularity'], n_groups, labels = False)\n        \n    for fold_no, (train_indices, val_indices) in enumerate(fold.split(target, target)):\n        df.loc[val_indices, 'kfold'] = fold_no\n        \n    return df","c9b7805f":"df = create_folds(df, n_splits = CONFIG['n_fold'], n_groups = 14)\ndf.head()","98a02f0d":"# By default, the imread function reads the image in BGR format\n# cvtColor takes the image from one color space to another color space, in this case, from BGR to RGB\nclass PawpularityDataset(Dataset):\n    def __init__(self, root_dir, df, transforms = None, is_test = False):\n        self.root_dir = root_dir\n        self.df = df\n        self.file_names = df['file_path'].values\n        if not is_test:\n            self.targets = df['Pawpularity'].values\n        self.meta = df[feature_cols].values\n        self.transforms = transforms\n        self.is_test = is_test\n        \n    def __len__(self):\n        return self.df.shape[0]\n    \n    def __getitem__(self, index):\n        img_path = self.file_names[index]\n        img = cv2.imread(img_path)\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        meta = self.meta[index, : ]\n        if not self.is_test:\n            target = self.targets[index]\n        \n        if self.transforms:\n            img = self.transforms(image = img)[\"image\"]\n        \n        if not self.is_test:\n            return img, meta, target\n        else:\n            return img, meta","c0c36d28":"# https:\/\/albumentations.ai\/docs\/\ndata_transforms = {\n    \"train\": A.Compose([\n        A.Resize(CONFIG['img_size'], CONFIG['img_size']),\n        A.HorizontalFlip(p = 0.5),\n        A.Normalize(),\n        ToTensorV2()\n    ]),\n    \"val\": A.Compose([\n        A.Resize(CONFIG['img_size'], CONFIG['img_size']),\n        A.Normalize(),\n        ToTensorV2()\n    ]),\n}","20c3973e":"# # Only for training, needs to be commented in inference\n# train_model = timm.create_model(CONFIG['model_name'], pretrained = True)\n# torch.save(train_model.state_dict(), 'effnetb4ns.pth')","c845288a":"class PawpularityModel(nn.Module):\n    def __init__(self, model_name, pretrained = True):\n        super(PawpularityModel, self).__init__()\n        self.model = timm.create_model(model_name, pretrained = False)\n        self.model.load_state_dict(torch.load(\"..\/input\/petfindermy-pawpularity-contest\/effnetb4ns.pth\"),\n              strict = False)   \n        self.n_features = self.model.classifier.in_features\n        self.model.reset_classifier(0)\n        self.fc = nn.Linear(self.n_features + 12, CONFIG['num_classes'])\n        self.dropout = nn.Dropout(p = 0.3)\n        \n    def forward(self, images, meta):\n        # features.shape = (batch_size, num_embeddings)\n        features = self.model(images)\n        features = self.dropout(features)\n        \n        # features.shape = (batch_size, num_embeddings + meta)\n        features = torch.cat([features, meta], dim = 1)\n        \n        # outputs = (batch_size, num_classes)\n        output = self.fc(features)\n        return output\n    \nmodel = PawpularityModel(CONFIG['model_name'])\nmodel.to(CONFIG['device'])","61661a73":"def criterion(outputs, targets):\n    return torch.sqrt(nn.MSELoss()(outputs.view(-1), targets.view(-1)))","37d07c23":"def train_one_epoch(model, optimizer, scheduler, dataloader, device, epoch):\n    model.train()\n    \n    # GradScaler makes the gradient values have a larger magnitude, so that, they don\u2019t flush to zero.\n    # https:\/\/pytorch.org\/docs\/stable\/amp.html#gradient-scaling\n    scaler = amp.GradScaler()\n    \n    dataset_size = 0\n    running_loss = 0.0\n    \n    # Defining the Iterator for TQDM\n    bar = tqdm(enumerate(dataloader), total = len(dataloader))\n    for step, (images, meta, targets) in bar:\n        images = images.to(device, dtype = torch.float)\n        meta = meta.to(device, dtype = torch.float)\n        targets = targets.to(device, dtype = torch.float)\n        \n        # Defining the Batch Size\n        batch_size = images.size(0)\n        \n        # Enabling Autocast for Automatic Mixed Precision \n        # https:\/\/developer.nvidia.com\/automatic-mixed-precision\n        with amp.autocast(enabled = True):\n            outputs = model(images, meta)\n            loss = criterion(outputs, targets)\n            loss = loss \/ CONFIG['n_accumulate']\n        scaler.scale(loss).backward()\n        \n        # When we have to train large models, and use small batch sizes, it takes a lot of computation\n        # time. In order to reduce that, we can do backprop after every few steps, instead of doing it\n        # after every step. In other words, we accumulate gradients for a 'n_accumulate' steps, and then\n        # we perform back-prop. \n        if (step + 1) % CONFIG['n_accumulate'] == 0:\n            scaler.step(optimizer)\n            scaler.update()\n            \n            # Zero out the Paraneter Gradients\n            optimizer.zero_grad()\n            \n            if scheduler is not None:\n                scheduler.step()\n                \n        running_loss += (loss.item() * batch_size)\n        dataset_size += batch_size\n        epoch_loss = running_loss \/ dataset_size\n        \n        # set_postfix allows us to display \n        # https:\/\/github.com\/tqdm\/tqdm\n        bar.set_postfix(Epoch = epoch, Train_Loss = epoch_loss, LR = optimizer.param_groups[0]['lr'])\n        \n    # All objects regardless of how long they have been in memory are considered for collection.\n    # However, objects that are referenced in managed code are not collected. Use this method to\n    # force the system to try to reclaim the maximum amount of available memory.\n    gc.collect()\n    \n    return epoch_loss","b92454a3":"# Since, we don't want to train the model while iterating on the validation set, hence we have \n# diasbled the gradient calculations in this function.\n# https:\/\/pytorch.org\/docs\/stable\/generated\/torch.no_grad.html\n@torch.no_grad()\n\ndef val_one_epoch(model, dataloader, device, epoch):\n    model.eval()\n    \n    dataset_size = 0\n    running_loss = 0.0\n    \n    TARGETS = []\n    PREDS = []\n    \n    # Defining the Iterator for TQDM\n    bar = tqdm(enumerate(dataloader), total = len(dataloader))\n    for step, (images, meta, targets) in bar:\n        images = images.to(device, dtype = torch.float)\n        meta = meta.to(device, dtype = torch.float)\n        targets = targets.to(device, dtype = torch.float)\n        \n        # Defining the Batch Size\n        batch_size = images.size(0)\n        \n        outputs = model(images, meta)\n        loss = criterion(outputs, targets)\n        \n        running_loss += (loss.item() * batch_size)\n        dataset_size += batch_size\n        epoch_loss = running_loss \/ dataset_size\n        \n        PREDS.append(outputs.view(-1).cpu().detach().numpy())\n        TARGETS.append(targets.view(-1).cpu().detach().numpy())\n        \n        # set_postfix allows us to display \n        # https:\/\/github.com\/tqdm\/tqdm\n        bar.set_postfix(Epoch = epoch, Val_Loss = epoch_loss, LR = optimizer.param_groups[0]['lr'])\n    \n    TARGETS = np.concatenate(TARGETS)\n    PREDS = np.concatenate(PREDS)\n    val_rmse = MSE(TARGETS, PREDS, squared=False)\n    \n    # All objects regardless of how long they have been in memory are considered for collection.\n    # However, objects that are referenced in managed code are not collected. Use this method to\n    # force the system to try to reclaim the maximum amount of available memory.\n    gc.collect()\n    \n    return epoch_loss, val_rmse","783526b8":"@torch.no_grad()\n\ndef predict_test(model, dataloader, device):\n    model.eval()\n    dataset_size = 0\n    PREDS = []\n    \n    # Defining the Iterator for TQDM\n    bar = tqdm(enumerate(dataloader), total = len(dataloader))\n    for step, (images, meta) in bar:\n        images = images.to(device, dtype = torch.float)\n        meta = meta.to(device, dtype = torch.float)\n        outputs = model(images, meta)\n\n        PREDS.append(outputs.view(-1).cpu().detach().numpy())\n    PREDS = np.concatenate(PREDS)\n    \n    # All objects regardless of how long they have been in memory are considered for collection.\n    # However, objects that are referenced in managed code are not collected. Use this method to\n    # force the system to try to reclaim the maximum amount of available memory.\n    gc.collect()\n    \n    return PREDS","30466e1f":"# train_loader & val_loader are initialized before this function is called\ndef run_training(model, optimizer, scheduler, device, num_epochs):\n    if torch.cuda.is_available():\n        print('[INFO] Using GPU: {}\\n'.format(torch.cuda.get_device_name()))\n        \n    start = time.time()\n    best_model_wts = copy.deepcopy(model.state_dict())\n    best_epoch_rmse = np.inf\n    history = defaultdict(list)\n    \n    for epoch in range(1, num_epochs+1):\n        gc.collect()\n        train_epoch_loss = train_one_epoch(model, optimizer, scheduler, dataloader = train_loader,\n            device = CONFIG['device'], epoch = epoch)\n        val_epoch_loss, val_epoch_rmse = val_one_epoch(model, dataloader = val_loader, \n             device = CONFIG['device'], epoch = epoch)\n        \n        history['Train Loss'].append(train_epoch_loss)\n        history['Valid Loss'].append(val_epoch_loss)\n        history['Valid RMSE'].append(val_epoch_rmse)\n        print(\"Val RMSE:\", val_epoch_rmse)\n        \n        # Deep Copy the Model\n        if val_epoch_rmse <= best_epoch_rmse:\n            print(f\"Validation Loss Improved ({best_epoch_rmse} - {val_epoch_rmse})\")\n            best_epoch_rmse = val_epoch_rmse\n            best_model_wts = copy.deepcopy(model.state_dict())\n            PATH = \"RMSE{:.4f}_epoch{:.0f}.bin\".format(best_epoch_rmse, epoch)\n            torch.save(model.state_dict(), PATH)\n        \n    end = time.time()\n    time_elapsed = end - start\n    print(\"Training complete in {:.0f}h {:.0f}m {:.0f}s\".format(\n        time_elapsed \/\/ 3600, (time_elapsed % 3600) \/\/ 60, (time_elapsed % 3600) % 60))\n    print(\"Best RMSE: {:.4f}\".format(best_epoch_rmse))\n    \n    # Load the best model weights\n    model.load_state_dict(best_model_wts)\n    \n    return model, history","67e626d4":"def prepare_loaders(fold):\n    df_train = df[df.kfold != fold].reset_index(drop = True)\n    df_val = df[df.kfold != fold].reset_index(drop = True)\n    \n    train_dataset = PawpularityDataset(TRAIN_DIR, df_train, transforms = data_transforms['train'])\n    val_dataset = PawpularityDataset(TRAIN_DIR, df_val, transforms = data_transforms['val'])\n    \n    train_loader = DataLoader(train_dataset, batch_size = CONFIG['train_batch_size'],\n        num_workers = 4, shuffle = True, pin_memory = True, drop_last = True)\n    val_loader = DataLoader(val_dataset, batch_size = CONFIG['valid_batch_size'],\n        num_workers = 4, shuffle = True, pin_memory = True, drop_last = True)\n    \n    return train_loader, val_loader","02ca6dbb":"def fetch_scheduler(optimizer):\n    if CONFIG['scheduler'] == 'CosineAnnealingLR':\n        scheduler = lr_scheduler.CosineAnnealingLR(optimizer, T_max = CONFIG['T_max'], \n            eta_min=CONFIG['min_lr'])\n    elif CONFIG['scheduler'] == 'CosineAnnealingWarmRestarts':\n        scheduler = lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0 = CONFIG['T_0'], \n                                                             eta_min = CONFIG['min_lr'])\n    elif CONFIG['scheduler'] == None:\n        return None\n        \n    return scheduler","15fd4275":"# Create Dataloaders\ntrain_loader, val_loader = prepare_loaders(fold = 0)","b2146e64":"# Define Optimizer & Scheduler\noptimizer = optim.Adam(model.parameters(), lr = CONFIG['learning_rate'], \n    weight_decay = CONFIG['weight_decay'])\nscheduler = fetch_scheduler(optimizer)","835b2eee":"# Start Training\nmodel, history = run_training(model, optimizer, scheduler, CONFIG['device'], CONFIG['epochs'])","c79e5fad":"def get_test_file_path(id):\n    return f\"{TEST_DIR}\/{id}.jpg\"\n\ndf_test = pd.read_csv(f\"{ROOT_DIR}\/test.csv\")\ndf_test['file_path'] = df_test['Id'].apply(get_test_file_path)\nprint(df_test.shape)\n\ntest_dataset = PawpularityDataset(TEST_DIR, df_test, transforms = data_transforms['val'], \n      is_test = True)\ntest_loader = DataLoader(test_dataset,\n        num_workers = 4, shuffle = True, pin_memory = True, drop_last = True)\n\npreds = predict_test(model, test_loader, CONFIG['device'])","863b5063":"submission = pd.DataFrame()\nsubmission['Id'] = df_test['Id']\nsubmission['Pawpularity'] = preds\nsubmission.to_csv('submission.csv',index = False)","e49b2383":"# Training Function","d6fd215a":"# Training & Inferencing","750fb7b5":"# Set Seed for Reproducibility","f3a4fd1d":"# Creating the Dataset Class","43ef72f5":"# Creating Folds","b9cc510e":"# PetFinder.my\n- Hola amigos, this notebook covers my code for the **PetFinder.my - Pawpularity Contest**, which can be found [here](https:\/\/www.kaggle.com\/c\/petfinder-pawpularity-score).\n- Reference Notebooks:\n    - [[Pytorch + W&B] Pawpularity Training](https:\/\/www.kaggle.com\/debarshichanda\/pytorch-w-b-pawpularity-training?scriptVersionId=75559544)\n    - [Experiment Tracking with Weights & Biases](https:\/\/www.kaggle.com\/ayuraj\/experiment-tracking-with-weights-and-biases\/notebook)\n    - [Interactive EDA using W&B Tables](https:\/\/www.kaggle.com\/ayuraj\/interactive-eda-using-w-b-tables)\n    - [Continuous Target Stratification](https:\/\/www.kaggle.com\/tolgadincer\/continuous-target-stratification?scriptVersionId=52551118&cellId=6)\n\n<br>\n\n![](https:\/\/storage.googleapis.com\/kaggle-media\/competitions\/Petfinder\/PetFinder%20-%20Logo.png)","e5a4f7d2":"# Training Configuration","1b74a28a":"# Defining the Augmentations","c016750a":"# Predict Function","3974c82d":"# Defining the Model Architecture","96b785e2":"# Validation Function","883d3d5b":"# Installing and Importing Packages","19c8fdac":"# Defining the Loss Function","644405e9":"# Run Training","fd570fb9":"# Making the Submission","7565e211":"# Read the Data"}}