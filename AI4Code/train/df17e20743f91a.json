{"cell_type":{"239d246d":"code","21e367f3":"code","bb1b30dd":"code","34887190":"code","f1f52d23":"code","3c894fea":"code","dca48542":"code","f6878680":"code","13900058":"code","93febe8e":"code","08dacf5b":"code","6b36fd4b":"code","a84e646a":"code","44911942":"code","a57679a2":"code","1741118d":"code","c9e56280":"code","be7de64b":"code","c7e82a25":"code","6d23eed6":"code","c621e80b":"code","b464d649":"code","07722c4a":"code","1c1c8445":"code","be37ec39":"code","353f3f4d":"code","0473f81c":"markdown","fa9e5c2f":"markdown","6d06f5ca":"markdown","359e4d7a":"markdown","6f42c8f8":"markdown","04a3aad7":"markdown","87e9ab4f":"markdown","1df5dce0":"markdown","80aa962a":"markdown","168a2d5a":"markdown","3129915c":"markdown","217f543a":"markdown","26838cb4":"markdown","3ce5f5a5":"markdown"},"source":{"239d246d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","21e367f3":"df=pd.read_csv(\"..\/input\/customersegmentation\/Telecust1.csv\")\ndf.head()","bb1b30dd":"import seaborn as sns\nimport matplotlib.pyplot as plt","34887190":"plt.figure(figsize=(20,10))\nsns.heatmap(df.corr(), cmap=\"viridis\",annot=True,linewidths=0.1)\n#Here we get a heatmap of correlation of all features, except from the target column because it is a string","f1f52d23":"df[\"custcat\"].value_counts()\n#Here we can see how many of each class is in our data","3c894fea":"plt.figure(figsize=(20,10))\nsns.set_style(\"darkgrid\")\nsns.countplot(x=\"custcat\",data=df)\n#Here we visualize our target column","dca48542":"X=df.drop(\"custcat\",axis=1) #Here we assign all the other features as our X independent variables\nX","f6878680":"from sklearn.preprocessing import StandardScaler","13900058":"scaler=StandardScaler()","93febe8e":"X= scaler.fit_transform(X)\nX","08dacf5b":"X.shape","6b36fd4b":"y=df[\"custcat\"].map({\"A\":1, \"B\":2, \"C\":3, \"D\":4})\ny","a84e646a":"from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split( X, y, test_size=0.2, random_state=4)\nprint ('Train set:', x_train.shape,  y_train.shape)\nprint ('Test set:', x_test.shape,  y_test.shape)","44911942":"from sklearn.neighbors import KNeighborsClassifier","a57679a2":"from sklearn.neighbors import KNeighborsClassifier\nknn=KNeighborsClassifier(n_neighbors=1)","1741118d":"knn.fit(x_train, y_train)#Here the algorithm fit the training data","c9e56280":"predictions=knn.predict(x_test) #here we make our predictions\npredictions","be7de64b":"from sklearn import metrics\n\nprint(\"Test set Accuracy: \", metrics.accuracy_score(y_test, predictions))","c7e82a25":"from sklearn.metrics import classification_report, confusion_matrix\nprint(confusion_matrix(y_test,predictions))","6d23eed6":"print(classification_report(y_test,predictions))\n","c621e80b":"error_rate=list()\n#here we iterate meny different k values and plot their error rates \n#and discover which one is better than others and has the lowest error rate\nfor i in range(1,40):\n    knn=KNeighborsClassifier(n_neighbors=i)\n    knn.fit(x_train,y_train)\n    prediction_i=knn.predict(x_test)\n    error_rate.append(np.mean(prediction_i != y_test))\n","b464d649":"# Now we will plot the prediction error rates of different k values\nplt.figure(figsize=(15,10))\nplt.plot(range(1,40),error_rate, color=\"blue\", linestyle=\"--\",marker=\"o\",markerfacecolor=\"red\",markersize=10)\nplt.title(\"Error Rate vs K Value\")\nplt.xlabel=\"K Value\"\nplt.ylabel(\"Error Rate\")","07722c4a":"knn=KNeighborsClassifier(n_neighbors=38)","1c1c8445":"knn.fit(x_train, y_train)","be37ec39":"predictions2=knn.predict(x_test)","353f3f4d":"print(classification_report(y_test,predictions2))\n","0473f81c":"# 3. Normalizing and Splitting Data","fa9e5c2f":"#### 281 Plus Service, 266 Basic-service, 236 Total Service, and 217 E-Service customers\n","6d06f5ca":"# 4. Using the K Nearest Neighbors Algorithm:","359e4d7a":"<font color=\"blue\" >\nWe have to transform our target column into a numerical values before impleneting the algorithm:","6f42c8f8":"<font color=\"blue\" >\nThe target field, called custcat, has four possible values that correspond to the four customer groups, as follows: 1- Basic Service 2- E-Service 3- Plus Service 4- Total Service\n\nOur objective is to build a classifier, to predict the class of unknown cases. We will use a specific type of classification called K nearest neighbour.","04a3aad7":"# 1. General Information About K Nearest Neighbors:","87e9ab4f":"<font color=\"blue\">\n    *K Nearest Neighbors is a classification algorithm\n\n*K nearest neighbors is a simple algorithm that stores all available cases and classifies new cases based on a similarity measure (e.g., distance functions).\n\nFirstly we store all the dataSecondly we calculate the data from x to all points in our data set, x indicationg particular new data point Then we sort the points near data by increasing distance from xFinally we predict the majority label of K, which is number and represent closest points\n\n*Choosing a K will effect what class a new point is assigned to: if we choose k=3, then the algorithm looks at the three nearest neighbors to this new point if we set k=6, then the algorithm looks at the six nearest neighbors to this new point and decide according to the majority of these 6 neighbors. If we set larger k values,we get a cleaner cutoff at the expense of mislabelling some points\n\n","1df5dce0":"<font color=\"blue\" >\nNow we can split our data train and test dataset as follows:","80aa962a":"<font color=\"blue\">\n    As we can see our model does not perform good when we use k=1.\n    \n    Instead of using different k vlaues which will be time consuming, we can use a function in order to choose the best k.","168a2d5a":"<font color=\"blue\">\n    As we can see in the figure above, k=38 gives the least error rate,so we will use it for better predictions","3129915c":"# 2. Exploratory Data Analysis:","217f543a":"### Here's an visualization of the K-Nearest Neighbors algorithm.\n\n<img src=\"https:\/\/ibm.box.com\/shared\/static\/mgkn92xck0z05v7yjq8pqziukxvc2461.png\">\n","26838cb4":"<font color=\"blue\" >\nIn this case, we have data points of Class A and B. We want to predict what the star (test data point) is. If we consider a k value of 3 (3 nearest data points) we will obtain a prediction of Class B. Yet if we consider a k value of 6, we will obtain a prediction of Class A.Therefore, the value of k is very important for our model's success.","3ce5f5a5":"<font color=\"blue\" >\n    With a better k value, our model predicts far better than before, Nonetheless K Means Classifier is not good for this dataset. We have to use another algorith that performs better.."}}