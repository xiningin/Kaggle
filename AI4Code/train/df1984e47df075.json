{"cell_type":{"e943be56":"code","82fcd5a5":"code","8ff802e9":"code","2c186e91":"code","8ddae3e9":"code","756ad5c6":"code","614d0149":"code","cb1bbfc0":"code","8085d1d6":"code","b65b13a1":"code","39d5d749":"code","f0d87810":"code","c9fd35da":"code","48cc3bef":"code","1873d1dd":"code","1f1f894e":"code","96f44749":"code","7be4deff":"code","c0fbad0a":"code","a749e47b":"code","27ddcb99":"code","bb8b871e":"code","6d6ca5e5":"code","8751979a":"code","6bfae985":"code","a10aba90":"code","091f0b70":"code","f591acca":"code","4d23b3a0":"code","f56dee29":"code","fd928078":"code","bfd39a6a":"code","75908fac":"code","b2bd145b":"code","50a41501":"code","ff042562":"code","f6d557b9":"markdown","2fdddd88":"markdown","9b7184fc":"markdown","3a5066ba":"markdown","cc87260e":"markdown","260589bf":"markdown","be2af518":"markdown","2d820488":"markdown","a624f5b5":"markdown","00dae69d":"markdown","a18fafec":"markdown","cb2b582f":"markdown","6f27cda0":"markdown","e2fa3178":"markdown","93c4aa54":"markdown","6b1bcd52":"markdown","3d801452":"markdown"},"source":{"e943be56":"# standard libs\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# sklearn libs\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import make_scorer, confusion_matrix\nfrom sklearn.model_selection import learning_curve\n\nsns.set_style('darkgrid')\n\n# The following line is needed to show plots inline in notebooks\n%matplotlib inline ","82fcd5a5":"train_data = pd.read_csv('..\/input\/titanic\/train.csv')\ntest_data = pd.read_csv('..\/input\/titanic\/test.csv')","8ff802e9":"train_data.head()","2c186e91":"train_data.describe()","8ddae3e9":"test_data.head()","756ad5c6":"test_data.describe()","614d0149":"sns.countplot(x='Survived', hue='Sex', data=train_data)","cb1bbfc0":"sns.countplot(x='Survived', hue='Pclass', data=train_data)","8085d1d6":"plt.hist(x='Fare', data=train_data, bins=10)","b65b13a1":"fig, ax = plt.subplots(figsize=(12, 8))\nsns.heatmap(train_data.isnull(), cmap='plasma', cbar=False, yticklabels=False, ax=ax)\nplt.title('NaN values in the training dataset')","39d5d749":"fig, ax = plt.subplots(figsize=(12, 8))\nsns.heatmap(test_data.isnull(), cmap='plasma', cbar=False, yticklabels=False, ax=ax)\nplt.title('NaN values in the test dataset')","f0d87810":"train_data[train_data.Embarked.isnull()]\n","c9fd35da":"test_data[test_data.Fare.isnull()]","48cc3bef":"train_data.dropna(axis=0, subset=['Embarked'], inplace=True)\n\n# verify the data has been dropped\ntrain_data[train_data.Embarked.isnull()]","1873d1dd":"test_data.dropna(axis=0, subset=['Fare'], inplace=True)\n\n# verify the data has been dropped\ntest_data[test_data.Fare.isnull()]","1f1f894e":"train_data[train_data.Age.isnull()]","96f44749":"fig, ax = plt.subplots()\nsns.boxplot(x='Pclass', y='Age', data=train_data)","7be4deff":"train_data.groupby('Pclass')['Age'].mean()","c0fbad0a":"def fill_age(age, pclass):\n    if age == age: \n        return age\n    if pclass == 1: \n        return 38\n    elif pclass == 2: \n        return 30\n    else: \n        return 25\n    \ntrain_data['Age'] = train_data.apply(lambda x: fill_age(x['Age'], x['Pclass']), axis=1)\ntest_data['Age'] = test_data.apply(lambda x: fill_age(x['Age'], x['Pclass']), axis=1)","a749e47b":"train_data[train_data.Age.isnull()]","27ddcb99":"test_data[test_data.Age.isnull()]","bb8b871e":"kde = sns.FacetGrid(train_data, hue='Survived', aspect=4)\nkde.map(sns.kdeplot, 'Age', shade=True)\nkde.add_legend()","6d6ca5e5":"train_data.drop('Cabin', axis=1, inplace=True)\ntest_data.drop('Cabin', axis=1, inplace=True)","8751979a":"# verify no missing data left \nfig, ax = plt.subplots(figsize=(12, 8))\nsns.heatmap(test_data.isnull(), cmap='plasma', cbar=False, yticklabels=False, ax=ax)\nplt.title('NaN values in the test dataset')","6bfae985":"fig, ax = plt.subplots(figsize=(12, 8))\nsns.heatmap(test_data.isnull(), cmap='plasma', cbar=False, yticklabels=False, ax=ax)\nplt.title('NaN values in the test dataset')","a10aba90":"train_data['Family'] = train_data['SibSp'] + train_data['Parch']\ntest_data['Family'] = test_data['SibSp'] + test_data['Parch']\n\ntrain_data.head()","091f0b70":"train_data.drop(['PassengerId', 'Name', 'Ticket', 'SibSp', 'Parch'], axis=1, inplace=True)\ntest_data.drop(['Name', 'Ticket', 'SibSp', 'Parch'], axis=1, inplace=True)\n\ntrain_data.head()","f591acca":"dummy_sex = pd.get_dummies(train_data['Sex'])\ndummy_embarked = pd.get_dummies(train_data['Embarked'])\n\n# add the dummy variables to the the training dataframe \ntrain_data = pd.concat([train_data, dummy_sex, dummy_embarked], axis=1)\n\n# drop 1 of each of the dummies as it is implied by other dummy columns\n# remaining dummy variables are 'female', 'C', and 'Q'\n# also drop the original data columns prior to being converted to dummy variables\ntrain_data.drop(['Sex', 'Embarked', 'male', 'S'], inplace=True, axis=1)\ntrain_data.head()","4d23b3a0":"dummy_sex = pd.get_dummies(test_data['Sex'])\ndummy_embarked = pd.get_dummies(test_data['Embarked'])\n\n# add the dummy variables to the the training dataframe \ntest_data = pd.concat([test_data, dummy_sex, dummy_embarked], axis=1)\n\n# drop 1 of each of the dummies as it is implied by other dummy columns\n# remaining dummy variables are 'female', 'C', and 'Q'\n# also drop the original data columns prior to being converted to dummy variables\ntest_data.drop(['Sex', 'Embarked', 'male', 'S'], inplace=True, axis=1)\ntest_data.head()","f56dee29":"X = train_data.drop(['Survived'],axis=1)\ny = train_data['Survived']","fd928078":"model = LogisticRegression()\nscaler = StandardScaler()\nkfold = KFold(n_splits=10)\nkfold.get_n_splits(X)\n\nbest_model = model\nbest_params = {}\nbest_accuracy = 0\nbest_std = 0\n\nfor C in [0.001,0.01,0.05,0.1,0.5,1,5,10, 100]:\n    for solver in ['newton-cg','lbfgs','liblinear','sag']:\n        \n        model = LogisticRegression(C=C, solver=solver)\n        accuracy = np.zeros(10)\n        np_idx = 0\n        \n        for train_idx, test_idx in kfold.split(X):\n            X_train, X_test = X.values[train_idx], X.values[test_idx]\n            y_train, y_test = y.values[train_idx], y.values[test_idx]\n\n            X_train = scaler.fit_transform(X_train)\n            X_test = scaler.transform(X_test)\n\n            model.fit(X_train, y_train)\n\n            predictions = model.predict(X_test)\n\n            TN = confusion_matrix(y_test, predictions)[0][0]\n            FP = confusion_matrix(y_test, predictions)[0][1]\n            FN = confusion_matrix(y_test, predictions)[1][0]\n            TP = confusion_matrix(y_test, predictions)[1][1]\n            total = TN + FP + FN + TP\n            ACC = (TP + TN) \/ float(total)\n\n            accuracy[np_idx] = ACC*100\n            np_idx += 1\n        \n        if np.mean(accuracy) > best_accuracy:\n            best_model = model\n            best_params = {'C':C, 'solver':solver}\n            best_accuracy = np.mean(accuracy)\n            best_std = np.std(accuracy)\n\nprint (best_params)\nprint (\"Best Score: {}%({}%)\".format(round(best_accuracy,3),round(best_std,3)))      \n\nprint (\"\\nThe optimal log model uses C={}, and a {} solver, and has a cross validation score of {}% with a standard deviation of {}%\".format(best_params['C'],best_params['solver'],round(best_accuracy,3),round(best_std,3)))","bfd39a6a":"model = LogisticRegression(C=best_params['C'],solver=best_params['solver'])\nscaler = StandardScaler()\n\nX_train = train_data.drop(['Survived'],axis=1)\ny_train = train_data['Survived']\n\nX_test = test_data.drop('PassengerId', axis=1)\n\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n\nmodel.fit(X_train, y_train)\n\npredictions = model.predict(X_test)","75908fac":"new_preds=[]\n\nfor i in range(len(predictions)):\n    if predictions[i] == 0: \n        new_preds.append('deceased')\n    else: \n        new_preds.append('survived')","b2bd145b":"labels, counts = np.unique(new_preds, return_counts=True)\nplt.bar(labels, counts, align='center')\nplt.gca().set_xticks(labels)\nplt.title('Predicted Survival Rate of Passengers in the Titanic Test Set')\nplt.show()","50a41501":"submission_df = pd.DataFrame({'PassengerId': test_data.PassengerId.astype(int), 'Survived': predictions})","ff042562":"# save the submission file to the working output directory \nsubmission_df.to_csv('.\/titanic_submission.csv', index=False)","f6d557b9":"Now lets take a look at the likelihood of survival given age. ","2fdddd88":"# Building the Classifier\nThe training data will be split into training and validation data. The model will use the training and validation data for hyperparameter tuning using a 10-fold cross validated grid search. All features will be rescaled using StandardScaler() to avoid variables with larger magnitudes having a larger impact on the solution than the variables with smaller magnitude. ","9b7184fc":"### Embarked & Fare \nThere are 2 passengers in the training data with no port of embarkation and one passenger in the test data with no information on their fare. These passengers are shown below. Since these combined missing data values are only for 3 passengers they will be removed from the data entirely.  ","3a5066ba":"### Cabin \nSince the cabin feature is missing so much data, it will be dropped outright.","cc87260e":"From the above, it can be seen that age descends with class. Now the mean values will be used to fill NaNs in the data. ","260589bf":"# Data Exploration","be2af518":"The above training dataset provides 11 features to use to predict the target variable (survival, 1 = survived, 0 = deceased). The features are as follows: \n1. PassengerID: ID of passenger travelling on titanic\n2. Pclass: class passenger was travelling in \n3. Name: passenger name \n4. Sex: passenger sex\n5. Age: passenger age \n6. SibSp: number of siblings or spouses travelling with the passenger \n7. Parch: number of parents or children travelling with the passenger\n8. Ticket: the ticket number of passenger \n9. Fare: fare paid by passenger for ticket \n10. Cabin: the cabin number of the passenger \n11. Embarked: port of embarkation (Q=Queenstown, S=Southhampton, C=Cherbourg) \n    ","2d820488":"From the above visualizations it can be seen that males were more likely than females to die during the sinking of the titanic, people in the third class cabins were more likely to die than people in first or second class cabins, and the majority of tickets purchased were for low fare. It should also be noted that there were far more people holding a low fare ticket, this may have contributed to the higher death rate among this category of passengers. ","a624f5b5":"# Data Cleaning\n## Missing Data\nFirst, the NaN values in both the training and testing sets will be visualized. It is useful to check visually for NaN values as identification is quick, easy, and accurate. After confirming existence\/absence of NaNs they will be reomved from both datasets. ","00dae69d":"The categorical variables will be encoded using dummy variables, this avoids the implication that the variables are ordered. The encoded variables will be Sex and Embarked. Pclass could be encoded as well, however, considering there is an order to the passenger class (i.e. class 3 < class 2 < class 1), this variable will remain numerically encoded. ","a18fafec":"Now lets visualize some of the trends in the training dataset. ","cb2b582f":"## Aggregating Features \nSome features in the dataset can be aggregated to reduce the number of features but retain the same information. Here, the new \"Family\" feature will aggregate the presence of a partner or relative (Parch, SibSp respectively) ","6f27cda0":"Now do the same as above for the test dataset.","e2fa3178":"## Final Cleaning and Categorizing\nNow, the features that are not useful will be dropped and the remaining non-numeric features will be categorized to make the data suitable for our classifier. \n\nSince SibSp and Parch were aggregated into Family these features can be dropped. Additionally, we can intuit that Ticket, PassngerID, and Name will not be useful features in the classification task because these features are all unique to each passenger, they will also be dropped. ","93c4aa54":"# Introduction \nThis notebook attempts to use the common classification technique, logistic regression, to classify passenger survival on the Titanic. \n\nThis analysis will include an initial exploratory data analysis, data cleaning, and prediction of survival with visualizations to identify correlations and other insights. ","6b1bcd52":"Now that the optimal hyperparameters have been determined, the entire training dataset can be used to train the classifier, the trained classifier can be used to predict survival of passengers in the test dataset. ","3d801452":"### Age\nNext, because so many values are missing from the age column, we will impute their values with the mean age by class."}}