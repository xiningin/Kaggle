{"cell_type":{"64f5c05e":"code","05451d08":"code","a22e2f96":"code","30a4e62f":"code","566c6881":"code","cbfc3557":"code","96934835":"code","aa72d268":"code","e5bdea96":"code","d250c968":"code","369f851b":"code","eb6bffbb":"code","daf0da20":"code","84ddffed":"code","cdf16aa3":"code","b0e0bfb0":"code","d0b5411d":"code","774f1f50":"code","4aebbc93":"code","e6690cbd":"code","067aeab4":"code","2b631602":"code","374c33c4":"code","7b506eba":"code","d247b627":"code","7fc22c00":"code","1b2e7232":"code","67ebd415":"code","ef731b2b":"code","cb02cc8a":"code","585b5911":"code","d5ce02f0":"code","28e19c9d":"code","3b40a46c":"code","df472296":"code","fc9de500":"code","bf9a6caf":"code","1fe2b8e3":"code","630e1314":"markdown","bd49a56d":"markdown","4f33df07":"markdown","41bb91ce":"markdown","5de70a07":"markdown","592fd0f0":"markdown","007c411d":"markdown","e21f6b88":"markdown","d15933ed":"markdown"},"source":{"64f5c05e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","05451d08":"# Load the Dataset\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Load transactions from pandas.\ndf = pd.read_csv(\"\/kaggle\/input\/beer-and-diaper\/beer and diaper.csv\", header=None)\n\n# Print the header\nprint(df.head())","a22e2f96":"# Split transaction strings into lists.\ntransactions = df.iloc[:,1].apply(lambda t: t.split(','))\n\n# Convert DataFrame into list of strings.\ntransactions = list(transactions)\n\n# Print the second transaction\nprint(transactions[1])\n\n# Count the number of transactions that contain bread and milk\ntransactions.count(['bread', 'milk'])","30a4e62f":"#df['Transactions']= df.values.tolist()\n#df","566c6881":"#df['Transactions'] = df['Transactions'].apply(lambda x: [i for i in x if str(i) != \"nan\"])","cbfc3557":"#df","96934835":"#transactions = df['Transactions']","aa72d268":"# Convert DataFrame column into list of strings\n#transactions = list(transactions)","e5bdea96":"#transactions.count(['burgers', 'meatballs', 'eggs'])","d250c968":"from itertools import permutations\n\n# Extract unique items.\nflattened = [item for transaction in transactions for item in transaction]\nitems = list(set(flattened))\n\n# Compute and print rules.\nrules = list(permutations(items, 2))\nprint(rules)","369f851b":"# Print the number of rules\nprint(len(rules))","eb6bffbb":"from mlxtend.preprocessing import TransactionEncoder\n\n# Instantiate transaction encoder\nencoder = TransactionEncoder().fit(transactions)","daf0da20":"# One-hot encode itemsets by applying fit and transform\nonehot = encoder.transform(transactions)\n\n# Convert one-hot encoded data to DataFrame\nonehot = pd.DataFrame(onehot, columns = encoder.columns_)\nprint(onehot)","84ddffed":"# Computing Support for Single Items\nprint(onehot.mean())","cdf16aa3":"# Computing Support for Multiple Items\nimport numpy as np\n\n# Define itemset that contains almonds and zucchini\nonehot['beer+bread'] = np.logical_and(onehot['beer'], onehot['bread'])\n\nprint(onehot.mean())","b0e0bfb0":"onehot=onehot.drop('beer+bread', axis=1)\nonehot","d0b5411d":"# Print first five items\nprint(onehot.head())","774f1f50":"# Computing support.\nsupportBD = np.logical_and(onehot['beer'],onehot['diaper']).mean()\nsupportB = onehot['beer'].mean()\nsupportD = onehot['diaper'].mean()\n\n# Compute and print confidence and lift.\nconfidence = supportBD \/ supportB\nlift = supportBD \/ (supportB * supportD)\n\n# Print results.\nprint(supportD, confidence, lift)","4aebbc93":"# Compute support for \"asparagus\" and \"almonds\"\nsupportBD = np.logical_and(onehot['beer'], onehot['diaper']).mean()\n\n# Compute support for \"asparagus\"\nsupportB = onehot['beer'].mean()\n\n# Compute support for \"almonds\"\nsupportD = onehot['diaper'].mean()\n\n# Compute and print leverage\nleverage = supportBD - supportB * supportD\nprint(leverage)","e6690cbd":"# Compute support for \"asparagus\" and \"almonds\"\nsupportBD = np.logical_and(onehot['beer'], onehot['diaper']).mean()\n\n# Compute support for \"asparagus\"\nsupportB = onehot['beer'].mean()\n\n# Compute support for NOT \"almonds\"\nsupportnD = 1.0 - onehot['diaper'].mean()\n\n# Compute support for \"asparagus\" and NOT \"almonds\"\nsupportBnD = supportB - supportBD\n\n# Compute conviction\nconviction = supportB*supportnD \/ supportBnD\nprint(conviction)","067aeab4":"# Let's define the functions to calculate the metrics from the original data.\nfrom itertools import permutations\n\ndef supportA(itemA, df):\n    return float(df[itemA].mean())    \n\ndef supportB(itemB, df):\n    return float(df[itemB].mean())\n\ndef confidence(itemA,itemB,df):\n    return float(np.logical_and(df[itemA],df[itemB]).mean() \/(df[itemA].mean()))\n\ndef lift(itemA,itemB,df):\n    return float(np.logical_and(df[itemA],df[itemB]).mean() \/(df[itemA].mean() * df[itemB].mean()))\n\ndef leverage(itemA,itemB,df):\n    return np.logical_and(df[itemA],df[itemB]).mean() - (df[itemA].mean()*df[itemB].mean())\n\ndef conviction(itemA, itemB, df):\n    # Compute support for A and B\n    supportAB = np.logical_and(df[itemA], df[itemB]).mean()\n    # Compute support for A\n    supportA = df[itemA].mean()\n    # Compute support for not B\n    supportnB = 1.0 - df[itemB].mean()\n    # Compute support for A not B\n    supportAnB = supportA - supportAB\n    # Compute conviction\n    return float(supportA*supportnB \/ supportAnB)","2b631602":"item_pairs = list()\nfor itemA,itemB in permutations(onehot,2):\n    item_pairs.append(list((itemA,itemB, #names\n                            onehot[itemA].sum(),onehot[itemB].sum(), #individual count\n                            np.logical_and(onehot[itemA],onehot[itemB]).sum(), #pair count\n                            supportA(itemA, onehot),\n                            supportB(itemB, onehot),\n                            confidence(itemA,itemB,onehot), #confidence\n                            lift(itemA,itemB,onehot), #lift\n                            leverage(itemA,itemB,onehot), # leverage\n                            conviction(itemA, itemB, onehot)\n                            ))) # ","374c33c4":"item_pairs = pd.DataFrame(item_pairs,columns = ['itemA','itemB',\n                                                'countItemA','countItemB',\n                                                'countItemA&B',\n                                                'Antecedent Support',\n                                                'Consequent Support',\n                                                'Confidence',\n                                                'Lift',\n                                                'Leverage',\n                                                'Conviction'])\n\nitem_pairs.sample(5)","7b506eba":"# Select subset of rules with low consequent support.\nrules = item_pairs[item_pairs['Consequent Support'] < 0.05]\nprint(len(rules))","d247b627":"rules","7fc22c00":"# Select subset of rules with lift > 1.5.\nrules_2 = rules[rules['Lift'] > 1.5]\nprint(len(rules_2))","1b2e7232":"rules_2","67ebd415":"# Import Apriori algorithm\nfrom mlxtend.frequent_patterns import apriori","ef731b2b":"# Compute frequent itemsets\nfrequent_itemsets = apriori(onehot, min_support = 0.0005,max_len = 4, use_colnames = True)\n\n# Print number of itemsets\nprint(len(frequent_itemsets))","cb02cc8a":"# Print frequent itemsets\nprint(frequent_itemsets.head())","585b5911":"# Import Apriori algorithm\nfrom mlxtend.frequent_patterns import apriori, association_rules\n\n# Compute association rules\nArules = association_rules(frequent_itemsets,\n                           metric = \"support\",\n                           min_threshold = 0.005)","d5ce02f0":"Arules","28e19c9d":"# Print the rules.\nprint(Arules)","3b40a46c":"# Raise the threshold\n# Compute association rules\nArules_2 = association_rules(frequent_itemsets,\n                           metric = \"support\",\n                           min_threshold = 0.010)\n\n# Print the rules.\nprint(Arules_2)","df472296":"# Raise the threshold\n# Compute association rules\nArules_3 = association_rules(frequent_itemsets,\n                           metric = \"support\",\n                           min_threshold = 0.050)\n\n# Print the rules.\nprint(Arules_3)","fc9de500":"fig, ax = plt.subplots(nrows=3, ncols=1, sharex=True, figsize=(10,5))\nsns.boxplot(x='antecedent support', data=Arules, linewidth=0.9, color=\"royalblue\", ax=ax[0])\nsns.boxplot(x='support', data=Arules, linewidth=0.9, color=\"royalblue\", ax=ax[1])\nsns.boxplot(x='confidence', data=Arules, linewidth=0.9, color=\"royalblue\", ax=ax[2])\nplt.tight_layout()\nplt.show()","bf9a6caf":"filtered_rules = Arules[(Arules['antecedent support'] > 0.5) &\n                        (Arules['support'] > 0.3) &\n                        (Arules['confidence'] > 0.5) &\n                        (Arules['lift'] > 1.00)]","1fe2b8e3":"filtered_rules","630e1314":"## Apriori and Computing Association Rule ","bd49a56d":"### \"Beer and Diaper\" is the most highly associated items.","4f33df07":"## Condifence & Lift","41bb91ce":"## Performing Multi-Metric Filtering","5de70a07":"## Calculating Metrics","592fd0f0":"## Computing Leverage","007c411d":"## Computing Conviction","e21f6b88":"## Simplest Metric","d15933ed":"## Apriori Algorithm"}}