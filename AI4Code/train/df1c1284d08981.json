{"cell_type":{"21be9b0d":"code","392b90f7":"code","aa98759f":"code","eab90ca0":"code","a646d89f":"code","9d4cf4ee":"code","d118bd6f":"code","d6783c48":"code","e84a06d6":"code","7549df25":"code","bb695595":"code","98867c3b":"code","8455adca":"code","7e21297c":"code","c8e1e37b":"code","1af40408":"code","925e00be":"code","062182c4":"code","ae01685a":"code","ee65fd25":"markdown","4f54a6c8":"markdown","8c814118":"markdown","7e67c60b":"markdown","6f800234":"markdown","cd664a1c":"markdown"},"source":{"21be9b0d":"import tensorflow as tf\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\nfrom tensorflow.keras.preprocessing import image_dataset_from_directory\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.applications import MobileNetV2, MobileNetV3Small, mobilenet_v2, mobilenet_v3","392b90f7":"IMAGE_SIZE = (250, 250)","aa98759f":"train_dataset = image_dataset_from_directory('..\/input\/face-mask-detection\/dataset', \n                                             image_size=IMAGE_SIZE,\n                                             shuffle=True,\n                                             seed=12,\n                                             batch_size=64,\n                                             validation_split=0.2,\n                                             subset=\"training\")\n\nvalidation_dataset = image_dataset_from_directory('..\/input\/face-mask-detection\/dataset', \n                                                  image_size=IMAGE_SIZE,\n                                                  shuffle=True,\n                                                  seed=12,\n                                                  batch_size=64,\n                                                  validation_split=0.2,\n                                                  subset=\"validation\")","eab90ca0":"class_names = train_dataset.class_names\nplt.figure(figsize=(10, 10))\nfor images, labels in train_dataset.take(1):\n    for i in range(9):\n        ax = plt.subplot(3, 3, i + 1)\n        plt.imshow(images[i].numpy().astype(\"uint8\"))\n        plt.title(class_names[labels[i]])\n        plt.axis(\"off\")","a646d89f":"IMAGE_SHAPE = IMAGE_SIZE + (3,)","9d4cf4ee":"preprocess_input = mobilenet_v2.preprocess_input","d118bd6f":"base_model = MobileNetV2(input_shape=IMAGE_SHAPE,\n                         include_top=False,\n                         weights='imagenet')\nbase_model.trainable = True","d6783c48":"inputs = tf.keras.Input(shape=IMAGE_SHAPE)\nx = preprocess_input(inputs)\nx = base_model(x)\nx = tf.keras.layers.GlobalAveragePooling2D()(x)\noutputs =  tf.keras.layers.Dense(1)(x)\nmodel = tf.keras.Model(inputs, outputs)","e84a06d6":"model.summary()","7549df25":"base_learning_rate = 0.0001\nmodel.compile(optimizer=tf.keras.optimizers.Adam(lr=base_learning_rate),\n              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n              metrics=['accuracy'])","bb695595":"history = model.fit(train_dataset,\n                    epochs=10,\n                    validation_data=validation_dataset)","98867c3b":"acc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\n\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nplt.figure(figsize=(8, 8))\nplt.subplot(2, 1, 1)\nplt.plot(acc, label='Training Accuracy')\nplt.plot(val_acc, label='Validation Accuracy')\nplt.legend(loc='lower right')\nplt.ylabel('Accuracy')\nplt.ylim([min(plt.ylim()),1])\nplt.title('Training and Validation Accuracy')\n\nplt.subplot(2, 1, 2)\nplt.plot(loss, label='Training Loss')\nplt.plot(val_loss, label='Validation Loss')\nplt.legend(loc='upper right')\nplt.ylabel('Cross Entropy')\nplt.ylim([0,1.0])\nplt.title('Training and Validation Loss')\nplt.xlabel('epoch')\nplt.show()","8455adca":"preprocess_input = mobilenet_v3.preprocess_input","7e21297c":"base_model = MobileNetV3Small(input_shape=IMAGE_SHAPE,\n                              include_top=False,\n                              weights='imagenet')\nbase_model.trainable = True","c8e1e37b":"inputs = tf.keras.Input(shape=IMAGE_SHAPE)\nx = preprocess_input(inputs)\nx = base_model(x)\nx = tf.keras.layers.GlobalAveragePooling2D()(x)\noutputs =  tf.keras.layers.Dense(1)(x)\nmodel = tf.keras.Model(inputs, outputs)","1af40408":"model.summary()","925e00be":"base_learning_rate = 0.0001\nmodel.compile(optimizer=tf.keras.optimizers.Adam(lr=base_learning_rate),\n              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n              metrics=['accuracy'])","062182c4":"history = model.fit(train_dataset,\n                    epochs=10,\n                    validation_data=validation_dataset)","ae01685a":"acc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\n\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nplt.figure(figsize=(8, 8))\nplt.subplot(2, 1, 1)\nplt.plot(acc, label='Training Accuracy')\nplt.plot(val_acc, label='Validation Accuracy')\nplt.legend(loc='lower right')\nplt.ylabel('Accuracy')\nplt.ylim([min(plt.ylim()),1])\nplt.title('Training and Validation Accuracy')\n\nplt.subplot(2, 1, 2)\nplt.plot(loss, label='Training Loss')\nplt.plot(val_loss, label='Validation Loss')\nplt.legend(loc='upper right')\nplt.ylabel('Cross Entropy')\nplt.ylim([0,1.0])\nplt.title('Training and Validation Loss')\nplt.xlabel('epoch')\nplt.show()","ee65fd25":"## MobileNetV3Small","4f54a6c8":"# Plot data","8c814118":"# Setting","7e67c60b":"# Data Loading","6f800234":"## MobileNetV2","cd664a1c":"# Training"}}