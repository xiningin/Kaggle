{"cell_type":{"b6f64568":"code","dda8ee83":"code","4b5d70f7":"code","c46aa95b":"code","60e5a659":"code","2532a836":"code","ddf3b171":"code","fb257f22":"code","e73595d1":"code","761e2ff1":"code","bf64cbbd":"code","8783e7bd":"code","74977538":"code","635c819a":"code","b79e285d":"code","a5034b61":"code","a88942f0":"code","e7c65187":"code","01114b54":"code","543e60e1":"code","d5e4e8be":"code","a442878e":"code","ac9edde2":"code","e5fb6b5b":"code","16fa2203":"code","31a880c6":"code","b828a2e3":"code","6c995856":"code","05c47bdc":"code","f44f4f24":"code","214b4234":"markdown","bd0b6ad7":"markdown","0d4cb39c":"markdown","38ad8787":"markdown","9e54739a":"markdown","9469cc12":"markdown","be6f752d":"markdown","41d71814":"markdown","90174652":"markdown","45aff423":"markdown","fbb6bebd":"markdown","3bb9eb14":"markdown","c88d9181":"markdown","28d54ce5":"markdown","99f79e27":"markdown","5361b5e3":"markdown","1a3e056d":"markdown","9b7d06bb":"markdown","b032da95":"markdown","b568ac6f":"markdown","8509ff00":"markdown","c5372d1e":"markdown","ddbb1ff9":"markdown","f207c4c3":"markdown","a28a4004":"markdown","e8764e8c":"markdown","5de91270":"markdown","12b11cb0":"markdown","ba507e69":"markdown","4dc62a5e":"markdown","6d964cf1":"markdown","e0be8c5c":"markdown","b7c1c2fd":"markdown","01cd9675":"markdown","2f60eb38":"markdown","1c71052e":"markdown","c43c4bde":"markdown","3539ae7b":"markdown","9ad7a031":"markdown","b89bc94e":"markdown","98fa5627":"markdown","3eac0603":"markdown","bbc8d709":"markdown","594d6a1f":"markdown","c901db02":"markdown"},"source":{"b6f64568":"!pip install ..\/input\/python-datatable\/datatable-0.11.0-cp37-cp37m-manylinux2010_x86_64.whl > \/dev\/null","dda8ee83":"import gc\n\nimport numpy as np\nimport pandas as pd\nimport datatable as dt\n\n\nimport matplotlib\nimport matplotlib.pyplot as plt\nfrom matplotlib.ticker import FuncFormatter\n\nfrom sklearn.metrics import roc_auc_score\n\nplt.style.use('ggplot')","4b5d70f7":"train = dt.fread('..\/input\/riiid-test-answer-prediction\/train.csv').to_pandas()\nquestions = pd.read_csv('..\/input\/riiid-test-answer-prediction\/questions.csv')\nlectures = pd.read_csv('..\/input\/riiid-test-answer-prediction\/lectures.csv')","c46aa95b":"train.head()","60e5a659":"timedelta = pd.to_timedelta(train['timestamp'], unit='ms')\nprint(f\"\\\nAverge: {timedelta.mean().floor('s')}\\n\\\nMedian: {timedelta.median().floor('s')}\\n\\\nMin:    {timedelta.min().floor('s')}\\n\\\nMax:    {timedelta.max().floor('s')}\")\ndel timedelta\n_ = gc.collect()\n\nfig = plt.figure(figsize=(15, 4))\nax = fig.add_subplot(111)\n\nax.hist((train['timestamp'] \/ (1000 * 60 * 60 * 24)), bins=100, rwidth=0.8)\n\nformat_yticks = matplotlib.ticker.FuncFormatter(lambda x, p: format(int(x), ','))\n\nax.set_xticks(range(0, int(train['timestamp'].max() \/ (1000 * 60 * 60 * 24)), 20))\nplt.xticks(rotation=70)\nax.set_xlabel('Days')\nax.yaxis.set_major_formatter(format_yticks)","2532a836":"hours = range(0, 1000 * 60 * 60 * 24, 1000 * 60 * 60 * 12)\ndays = range(1000 * 60 * 60 * 24, 1000 * 60 * 60 * 24 * 7, 1000 * 60 * 60 * 24)\nweeks = range(1000 * 60 * 60 * 24 * 7, 1000 * 60 * 60 * 24 * 7 * 4, 1000 * 60 * 60 * 24 * 7)\nmonths = range(1000 * 60 * 60 * 24 * 30, 1000 * 60 * 60 * 24 * 30 * 12, 1000 * 60 * 60 * 24 * 30 * 2)\nrest = [1000 * 60 * 60 * 24 * 30 * 12, train['timestamp'].max()]\n\nbins = list(hours) + list(days) + list(weeks) + list(months) + rest\n\nh,e = np.histogram(train['timestamp'], bins=bins)\n\nplt.figure(figsize=(15, 4))\n\nplt.xticks(np.arange(-0.5, len(h) + 0.5),\n           [\"0\", \"12 hours\", \"1 day\", \"2 days\", \"3 days\", \"4 days\", \"5 days\", \"6 days\", \"1 week\", \"2 weeks\", \"3 weeks\", \"1 month\", \" 3 months\", \"5 months\", \"7 months\", \"9 months\", \"11 monts\", \"1 year\", f\"max\"],\n           fontsize=14, rotation=60)\n\nformat_yticks = FuncFormatter(lambda x, p: format(int(x), ','))\n\nplt.bar(range(len(bins)-1), h, width=0.96,\n        color = [\"tab:blue\"] * len(hours) + [\"tab:orange\"] * len(days) + [\"tab:green\"] * len(weeks) + [\"tab:red\"] * len(months) + [\"tab:purple\"] * len(rest))\nplt.ylabel('Number of Interactions')\nplt.gcf().axes[0].yaxis.set_major_formatter(format_yticks)","ddf3b171":"print(f\"Total number of users: {train['user_id'].nunique():,}\\n\\\nNumber of interations \/ Number of users: {len(train['user_id']) \/ train['user_id'].nunique():.2f}\")","fb257f22":"user_counts = train['user_id'].value_counts()\n\nprint(f\"\\\nAverge: {user_counts.mean():.02f}\\n\\\nMedian: {user_counts.median():>6}\\n\\\nMin:    {user_counts.min():>6}\\n\\\nMax:    {user_counts.max():,}\")\n\nfig = plt.figure(figsize=(15,4))\nax = fig.add_subplot(111)\n\nax.hist(user_counts, bins=100, rwidth=0.8)\n\nax.set_xlabel('Number of Interactions')\nax.set_ylabel('Users counts')\nax.yaxis.set_major_formatter(format_yticks)","e73595d1":"bins = np.logspace(0, 8, base=6, endpoint=user_counts.max(), num=50, dtype=np.int)\n\nh,e = np.histogram(user_counts, bins=bins)\n\nfig = plt.figure(figsize=(15, 4))\nax = fig.add_subplot(111)\n\nplt.sca(ax)\nplt.xticks(np.arange(-0.5, len(h) + 0.5), bins)\n\nax.bar(range(len(bins)-1), h, width=0.9)\n\nplt.xticks(rotation=70)\nax.set_xlabel('Number of Interactions')\nax.set_ylabel('Users counts')\nax.yaxis.set_major_formatter(format_yticks)","761e2ff1":"content_id_in_dfs = np.isin(train['content_id'].unique(), np.concatenate([questions.question_id, lectures.lecture_id]))\nprint(f\"Are all content ids in question and lecture dataframes?: {np.all(content_id_in_dfs)}\")","bf64cbbd":"ids_intersect = np.intersect1d(questions.question_id, lectures.lecture_id)\nprint(f\"Questions ids and lectures ids not intersecting?: {ids_intersect.size == 0}\")","8783e7bd":"questions_not_in_train = np.isin(questions.question_id, train['content_id'][train['content_type_id'] == 0].unique(), invert=True)\nlectures_not_in_train = np.isin(lectures.lecture_id, train['content_id'][train['content_type_id'] == 1].unique(), invert=True)\nprint(f\"\\\nQuestions not in train.csv: {np.count_nonzero(questions_not_in_train)}\\n\\\nLectures not in train.csv:  {np.count_nonzero(lectures_not_in_train)}\")","74977538":"print(f\"\\\nUnique values:            {train['content_type_id'].nunique()}\\n\\\nQuestions:       {np.count_nonzero(train['content_type_id'] == 0):,}\\n\\\nLectures:         {np.count_nonzero(train['content_type_id'] == 1):,}\")","635c819a":"is_same_type_container = train['content_type_id'].groupby(train['task_container_id']).apply(lambda x: np.all(x == x.iloc[0]))\nprint(f\"\\\nTotal number of containers:                            {train['task_container_id'].nunique():,}\\n\\\nAny lectures and questions in the same container?:     {np.all(is_same_type_container) == False}\\n\\\nHow many containers with mixed content type:           {np.count_nonzero(is_same_type_container == False)}\\n\\\nHow many containers with one content type:             {np.count_nonzero(is_same_type_container)}\")","b79e285d":"print(f\"Unique values: {train['user_answer'].nunique()}\")","a5034b61":"fig = plt.figure(figsize=(5, 5))\nax = fig.add_subplot(111)\n\nax.hist(train['user_answer'])\n\nax.set_xticks(train['user_answer'].unique())\n\nax.set_xlabel('Answer Number')\nax.set_ylabel('Counts')\n\nax.yaxis.set_major_formatter(format_yticks)","a88942f0":"fig = plt.figure(figsize=(5, 5))\nax = fig.add_subplot(111)\n\nax.hist(train['answered_correctly'])\n\nax.set_xticks(train['answered_correctly'].unique())\n\nax.set_xlabel('Answer Correctness')\nax.set_ylabel('Counts')\nax.yaxis.set_major_formatter(format_yticks)","e7c65187":"total_na = train['prior_question_elapsed_time'].isna().sum()\ntimedelta = pd.to_timedelta(train['prior_question_elapsed_time'], unit='ms')\n\nprint(f\"\\\nTotal null values:      {total_na:,}\\n\\\nTotal non-null values:  {train.shape[0] - total_na:,}\\n\\\nAverge:                 {timedelta.mean().total_seconds():>12} s\\n\\\nMedian:                 {timedelta.median().total_seconds():>12} s\\n\\\nMin:                    {timedelta.min().total_seconds():>12} s\\n\\\nMax:                    {timedelta.max().total_seconds():>12} s\")\n\nfig = plt.figure(figsize=(15, 4))\nax = fig.add_subplot(111)\n\nax.hist(train['prior_question_elapsed_time'][train['prior_question_elapsed_time'].isna() == False] \/ 1000, bins = 50, rwidth=0.9)\n\nax.set_xlabel('Seconds')\nax.set_ylabel('User Count')\nax.yaxis.set_major_formatter(format_yticks)","01114b54":"counts = train['prior_question_had_explanation'].value_counts()\nprint(f\"\\\nTrue:  {counts[0]:,}\\n\\\nFalse: {counts[1]:,}\")","543e60e1":"questions.head()","d5e4e8be":"tags = questions.tags.apply(lambda x: [int(t) if t.isdigit() else -1 for t in str(x).split(' ')])\ntags = np.concatenate(tags.tolist())","a442878e":"print(f\"\\\nTotal questions: {questions.question_id.nunique()}\")","ac9edde2":"lectures.head()","e5fb6b5b":"print(f'Total number of lectures: {lectures.lecture_id.nunique()}')","16fa2203":"questions_ratios = train.query('content_type_id == 0').groupby('content_id')['answered_correctly'].mean()\nplt.figure(figsize=(15, 4))\nquestions_ratios.hist(bins=100, rwidth=0.8)\nplt.xlabel('Correctness Ratio')\n_ = plt.ylabel('Number of Questions')","31a880c6":"preds = questions_ratios[train.query('content_type_id == 0').content_id].values\nscore = roc_auc_score(train.query('content_type_id == 0').answered_correctly.values, preds)\nprint(f'Score: {score}')","b828a2e3":"users_ratios = train.query('content_type_id == 0').groupby('user_id')['answered_correctly'].mean()\nplt.figure(figsize=(15, 4))\nusers_ratios.hist(bins=100, rwidth=0.8)\nplt.xlabel('Correctness Ratio')\n_ = plt.ylabel('Number of Users')","6c995856":"preds = users_ratios[train.query('content_type_id == 0').user_id].values\nscore = roc_auc_score(train.query('content_type_id == 0').answered_correctly.values, preds)\nprint(f'Score: {score}')","05c47bdc":"containers_ratios = train.query('content_type_id == 0').groupby('task_container_id')['answered_correctly'].mean()\nplt.figure(figsize=(15, 4))\ncontainers_ratios.hist(bins=100, rwidth=0.8)\nplt.xlabel('Correctness Ratio')\n_ = plt.ylabel('Number of Containers')","f44f4f24":"preds = containers_ratios[train.query('content_type_id == 0').task_container_id].values\nscore = roc_auc_score(train.query('content_type_id == 0').answered_correctly.values, preds)\nprint(f'Score: {score}')","214b4234":"## 2.8 prior_question_elapsed_time:","bd0b6ad7":"## 2.2 user_id:","0d4cb39c":"## 5.1 Question difficulty:\nFirst let's see  if are difference in difficulty of anwering each question, we're gonna do that by calculating the ratio of correct answers for each question and plot it:","38ad8787":"The \"train.csv\" dataframe has +100 million records, which makes it impossible to load it fully on a kaggle notebook with 16gb of ram (the size of the file is about 5gb on disk but pandas requires around x5 of ram). There are many approaches to solve this issue, the one I have chosen is @rohanrao's using a package called datatable (check out his notebook [here](https:\/\/www.kaggle.com\/rohanrao\/riiid-with-blazing-fast-rid)).","9e54739a":"## 2.4 content_type_id:","9469cc12":"The average time it took a user to answer each question in the previous question bundle, ignoring any lectures in between. Is null for a user's first question bundle or lecture.","be6f752d":"The questions or lectures are bundeled together and given to users as batches, this column tell us how different contents are grouped.\n\nLet's get and idea on how they are contained...","41d71814":"This is important, because to distinguish between questions and lectures we need to use content_type_id column and not only rely on content_id.\n\nWe need to answer another question now, are all lectures and questions present is our training dataset?","90174652":"## 5.2 User's Performance:\nIt could be that users differ on how well they respond to questions, we will do the same as the previous section, group by user_id and aggregate the mean over the answer correctness column:","45aff423":"If the user answered correctly, -1 for lectures. This is the only feature that needs to predicted.","fbb6bebd":"Here we have a normal distribution skewed to the right, but why we have this guaussian distribution rather than a uniform distribution? it could be that our questions follow a bernouli distribtion with different parameter each, that means each questions has it own probabily of being answered correctly.\n\nLet's try to use answer correctness mean as a prediction, and calculate the score:","3bb9eb14":"## 2.5 task_container_id:","c88d9181":"This column tell us the duration between the user's first interaction and the current interaction in milliseconds.\nThis variable will give us an idea how active users were through time, let's see how this variable is distributed.","28d54ce5":"We start by describing and analyzing the train.csv columns one after the other:","99f79e27":"## 5.3 Container difficulty:","5361b5e3":"# 5. Looking for patterns:\nOur goal in this competition as mentioned before is to predict the user's answer correctness, in this section we're going to try to find any patterns that describe our target using different predictors available in the dataset and statistics.","1a3e056d":"## 2.9 prior_question_had_explanation:","9b7d06bb":"## 2.1 timestamp:","b032da95":"This is to indicate if the content is a lecture or a question, (0: question, 1: lecture).","b568ac6f":"That's a good score given that we used only a single feature, this show how import this feature is.","8509ff00":"## 2.3 content_id:","c5372d1e":"But does these ids represent ids uniquely for each question or lecture?...","ddbb1ff9":"This column represent the id of a questions or a lecture (the same id is also present in quesntions.csv and lecture.csv datasets). We can verify that this column is just ids from the other dataframes by using the code bellow:","f207c4c3":"Only 3 lectures are not present is the training dataset.\n\nWe will analyze more the content_id in a later sections along the corresponding dataframes.","a28a4004":"Whether or not the user saw an explanation and the correct response(s) after answering the previous question bundle.","e8764e8c":"Each user has his own id, also in the hidden test set we will have more new user ids. Let's see how many users we have in this dataset:","5de91270":"# 2. Train.csv:","12b11cb0":"I ploted the variable in days, it would better to look more closely in a more meaningfull timespan...","ba507e69":"This dataframe gives us infromation about the questions. These questions will be the same for the hidden test set.","4dc62a5e":"# 1. Importing Data:","6d964cf1":"# 6. Submission process:\nSubmissions should be done through notebooks, using the time series API.\n\nThe submission process is an iterative process, your model will get data as batches. For each batch your model have to send predictions of the current to the API before receiving the next batch.\n\nAlong each batch you receive from the API, labels of the preceding are reveiled, so it would be useful for your model to learn and predict in the same time. \n\n[API Demonstration](https:\/\/www.kaggle.com\/sohier\/competition-api-detailed-introduction).","e0be8c5c":"The user's answer to the question, if any. -1 as null, for lectures.","b7c1c2fd":"As excpected it gave us a lower score than the other predictors..","01cd9675":"# 4. lectures.csv:","2f60eb38":"## 2.6 user_answer:","1c71052e":"It seems that we have a mixture of two distributions, the left one (the dominant one) are users with less interaction and the right one are users orders of magnitude more interactions.","c43c4bde":"# 7. Private Dataset and Proportions:\nLastly, one important aspect in every competition, is how different the private is from the public test.\n\nOne thing worth highlighting is that the private dataset will have new users and the will have about 2.5 million questions.","3539ae7b":"# 3. questions.csv:","9ad7a031":"We plot the number of interactions per user variable, just to check if we have clusters of very active user and others with less activity:","b89bc94e":"We can see that many interactions happen in the first 12 hours, and also during the first and second month.","98fa5627":"This distribution has less variance than the other distributions, this suggests that containers don't differ much on difficulity. Container id won't be a good predictor for answer correctness.","3eac0603":"This distribution looks very skewed to the left, let re-plot this with a log scale:","bbc8d709":"Let's try this as a prediction...","594d6a1f":"![Riiid! App](https:\/\/venturebeat.com\/wp-content\/uploads\/2020\/07\/1-1-e1595515659939.jpeg?w=1200&strip=all)\n\n\nThe goal of this notebook is to make an explanatory, exploratory and storytelling style walkthrough this data, I'll try to highlight all the relevant parts and update regularly to add important information. Reading this notebook from top to bottom should give you the big picture perspective on this data.\n\n### Overview of the data:\nThis data is simply records of users' interactions with an educational app, each user has his\/her own unique id, these interactions are watching lectures or answering quesstions. You can think of the users as the data generating distribution since the users are given questions as input and expect answers as output. Or you can think of the app as the data generating distribution, by giving users and historic information about them as input you will expect their answers' correctness as output.\n\n### Goal of the modeling:\nOur goal in this competition is to make a model that will predict the users' answers correctness given the question and historic data about the users.","c901db02":"## 2.7 answered_correctly:"}}