{"cell_type":{"86195b3a":"code","4646a1c4":"code","9b02d070":"code","ceec4ab1":"code","a0a5bf1b":"code","69acaa39":"code","8e0a96b1":"code","1e00ce55":"code","f888fbe8":"code","e74bb80f":"code","be16937a":"code","f9846eb7":"code","ba55fee9":"code","bce6a083":"code","ba014b6b":"code","6f678908":"code","b2d170d9":"code","2dc2233c":"code","01b4141b":"code","022c9732":"code","170c0afc":"code","c4ec54d0":"code","d0ccf557":"code","e22ad801":"code","6ef93ee7":"code","17ae5bae":"code","423f4474":"code","f937dd09":"code","c3f8c65f":"code","0bd555ae":"code","b25da9d1":"code","ccc06651":"code","babad633":"code","47545c90":"code","ea962208":"code","3c9a61a1":"code","81cadb71":"code","b88f496d":"code","251b1e80":"code","ae5087a0":"code","1404e9a2":"code","d8365daf":"code","9c2be951":"code","45711459":"code","b9ff8d42":"code","cd371c55":"code","c1838393":"code","7b5757ca":"code","2797b2e6":"code","41723c84":"code","077fbc88":"code","b8efe83d":"code","8794afcf":"code","24689d7b":"code","386d264f":"code","1bcf8e46":"markdown","4bf50c1e":"markdown","ecdf78bc":"markdown","fdd59ccf":"markdown","06ccfa73":"markdown","3056ad93":"markdown","01b76cb1":"markdown","e68c990f":"markdown","0a7a7ee9":"markdown","ef16404f":"markdown","0d57a60b":"markdown","c421d36a":"markdown","e96321cc":"markdown","1c8fdcfd":"markdown","af954a5c":"markdown","49d57c6e":"markdown","403ccde4":"markdown","bde758e4":"markdown","25153895":"markdown","fcb1ab95":"markdown","97931498":"markdown","f1df5782":"markdown","a03abe41":"markdown","69e097c2":"markdown","f5adbbe7":"markdown","2cf6c236":"markdown","7eb97acb":"markdown","276864bd":"markdown","a49fd7eb":"markdown","15be5b21":"markdown","4f6b1be1":"markdown","8dc77a9f":"markdown","a2afca40":"markdown","1114254c":"markdown","5062e94d":"markdown","ee245831":"markdown","7e73939c":"markdown","585f0767":"markdown","9e2bf69c":"markdown","c5112d78":"markdown","85b5ee5c":"markdown","55581aa5":"markdown","df131618":"markdown","4f00c381":"markdown","ecd1a2e9":"markdown","167c111f":"markdown","6dc2d1e6":"markdown","b0d96ae9":"markdown","d0ae9c95":"markdown","a6fbef2a":"markdown","56a1a8bb":"markdown","9f3195b1":"markdown","1fa5bd88":"markdown","5814f6a7":"markdown","bc2e7afe":"markdown","21e3fdcd":"markdown","4ac2fe15":"markdown","e0a555bc":"markdown","7489c14b":"markdown","a28e7c1e":"markdown","6304eaf0":"markdown","db0b0282":"markdown","db031239":"markdown","74e0cbb8":"markdown","a70835e3":"markdown","fe43fc3c":"markdown","7e17b104":"markdown","1748bc20":"markdown","098b7362":"markdown","200dbbea":"markdown","f4e99e26":"markdown","c8d7b1d6":"markdown"},"source":{"86195b3a":"import pandas as pd\nimport numpy as np\n\n# Import clean data \npath = 'https:\/\/s3-api.us-geo.objectstorage.softlayer.net\/cf-courses-data\/CognitiveClass\/DA0101EN\/module_5_auto.csv'\ndf = pd.read_csv(path)","4646a1c4":"df.to_csv('module_5_auto.csv')","9b02d070":"df=df._get_numeric_data()\ndf.head()","ceec4ab1":"%%capture\n! pip install ipywidgets","a0a5bf1b":"from IPython.display import display\nfrom IPython.html import widgets \nfrom IPython.display import display\nfrom ipywidgets import interact, interactive, fixed, interact_manual","69acaa39":"def DistributionPlot(RedFunction, BlueFunction, RedName, BlueName, Title):\n    width = 12\n    height = 10\n    plt.figure(figsize=(width, height))\n\n    ax1 = sns.distplot(RedFunction, hist=False, color=\"r\", label=RedName)\n    ax2 = sns.distplot(BlueFunction, hist=False, color=\"b\", label=BlueName, ax=ax1)\n\n    plt.title(Title)\n    plt.xlabel('Price (in dollars)')\n    plt.ylabel('Proportion of Cars')\n\n    plt.show()\n    plt.close()","8e0a96b1":"def PollyPlot(xtrain, xtest, y_train, y_test, lr,poly_transform):\n    width = 12\n    height = 10\n    plt.figure(figsize=(width, height))\n    \n    \n    #training data \n    #testing data \n    # lr:  linear regression object \n    #poly_transform:  polynomial transformation object \n \n    xmax=max([xtrain.values.max(), xtest.values.max()])\n\n    xmin=min([xtrain.values.min(), xtest.values.min()])\n\n    x=np.arange(xmin, xmax, 0.1)\n\n\n    plt.plot(xtrain, y_train, 'ro', label='Training Data')\n    plt.plot(xtest, y_test, 'go', label='Test Data')\n    plt.plot(x, lr.predict(poly_transform.fit_transform(x.reshape(-1, 1))), label='Predicted Function')\n    plt.ylim([-10000, 60000])\n    plt.ylabel('Price')\n    plt.legend()","1e00ce55":"y_data = df['price']","f888fbe8":"x_data=df.drop('price',axis=1)","e74bb80f":"from sklearn.model_selection import train_test_split\n\n\nx_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.15, random_state=1)\n\n\nprint(\"number of test samples :\", x_test.shape[0])\nprint(\"number of training samples:\",x_train.shape[0])\n","be16937a":"from sklearn.linear_model import LinearRegression","f9846eb7":"lre=LinearRegression()","ba55fee9":"lre.fit(x_train[['horsepower']], y_train)","bce6a083":"lre.score(x_test[['horsepower']], y_test)","ba014b6b":"lre.score(x_train[['horsepower']], y_train)","6f678908":"from sklearn.model_selection import cross_val_score","b2d170d9":"Rcross = cross_val_score(lre, x_data[['horsepower']], y_data, cv=4)","2dc2233c":"Rcross","01b4141b":"print(\"The mean of the folds are\", Rcross.mean(), \"and the standard deviation is\" , Rcross.std())","022c9732":"-1 * cross_val_score(lre,x_data[['horsepower']], y_data,cv=4,scoring='neg_mean_squared_error')","170c0afc":"from sklearn.model_selection import cross_val_predict","c4ec54d0":"yhat = cross_val_predict(lre,x_data[['horsepower']], y_data,cv=4)\nyhat[0:5]","d0ccf557":"lr = LinearRegression()\nlr.fit(x_train[['horsepower', 'curb-weight', 'engine-size', 'highway-mpg']], y_train)","e22ad801":"yhat_train = lr.predict(x_train[['horsepower', 'curb-weight', 'engine-size', 'highway-mpg']])\nyhat_train[0:5]","6ef93ee7":"yhat_test = lr.predict(x_test[['horsepower', 'curb-weight', 'engine-size', 'highway-mpg']])\nyhat_test[0:5]","17ae5bae":"import matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns","423f4474":"Title = 'Distribution  Plot of  Predicted Value Using Training Data vs Training Data Distribution'\nDistributionPlot(y_train, yhat_train, \"Actual Values (Train)\", \"Predicted Values (Train)\", Title)","f937dd09":"Title='Distribution  Plot of  Predicted Value Using Test Data vs Data Distribution of Test Data'\nDistributionPlot(y_test,yhat_test,\"Actual Values (Test)\",\"Predicted Values (Test)\",Title)","c3f8c65f":"from sklearn.preprocessing import PolynomialFeatures","0bd555ae":"x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.45, random_state=0)","b25da9d1":"pr = PolynomialFeatures(degree=5)\nx_train_pr = pr.fit_transform(x_train[['horsepower']])\nx_test_pr = pr.fit_transform(x_test[['horsepower']])\npr","ccc06651":"poly = LinearRegression()\npoly.fit(x_train_pr, y_train)","babad633":"yhat = poly.predict(x_test_pr)\nyhat[0:5]","47545c90":"print(\"Predicted values:\", yhat[0:4])\nprint(\"True values:\", y_test[0:4].values)","ea962208":"PollyPlot(x_train[['horsepower']], x_test[['horsepower']], y_train, y_test, poly,pr)","3c9a61a1":"poly.score(x_train_pr, y_train)","81cadb71":"poly.score(x_test_pr, y_test)","b88f496d":"Rsqu_test = []\n\norder = [1, 2, 3, 4]\nfor n in order:\n    pr = PolynomialFeatures(degree=n)\n    \n    x_train_pr = pr.fit_transform(x_train[['horsepower']])\n    \n    x_test_pr = pr.fit_transform(x_test[['horsepower']])    \n    \n    lr.fit(x_train_pr, y_train)\n    \n    Rsqu_test.append(lr.score(x_test_pr, y_test))\n\nplt.plot(order, Rsqu_test)\nplt.xlabel('order')\nplt.ylabel('R^2')\nplt.title('R^2 Using Test Data')\nplt.text(3, 0.75, 'Maximum R^2 ')    ","251b1e80":"def f(order, test_data):\n    x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=test_data, random_state=0)\n    pr = PolynomialFeatures(degree=order)\n    x_train_pr = pr.fit_transform(x_train[['horsepower']])\n    x_test_pr = pr.fit_transform(x_test[['horsepower']])\n    poly = LinearRegression()\n    poly.fit(x_train_pr,y_train)\n    PollyPlot(x_train[['horsepower']], x_test[['horsepower']], y_train,y_test, poly, pr)","ae5087a0":"interact(f, order=(0, 6, 1), test_data=(0.05, 0.95, 0.05))","1404e9a2":"pr=PolynomialFeatures(degree=2)\nx_train_pr=pr.fit_transform(x_train[['horsepower', 'curb-weight', 'engine-size', 'highway-mpg','normalized-losses','symboling']])\nx_test_pr=pr.fit_transform(x_test[['horsepower', 'curb-weight', 'engine-size', 'highway-mpg','normalized-losses','symboling']])","d8365daf":"from sklearn.linear_model import Ridge","9c2be951":"RigeModel=Ridge(alpha=0.1)","45711459":"RigeModel.fit(x_train_pr, y_train)","b9ff8d42":"yhat = RigeModel.predict(x_test_pr)","cd371c55":"print('predicted:', yhat[0:4])\nprint('test set :', y_test[0:4].values)","c1838393":"Rsqu_test = []\nRsqu_train = []\ndummy1 = []\nALFA = 10 * np.array(range(0,1000))\nfor alfa in ALFA:\n    RigeModel = Ridge(alpha=alfa) \n    RigeModel.fit(x_train_pr, y_train)\n    Rsqu_test.append(RigeModel.score(x_test_pr, y_test))\n    Rsqu_train.append(RigeModel.score(x_train_pr, y_train))","7b5757ca":"width = 12\nheight = 10\nplt.figure(figsize=(width, height))\n\nplt.plot(ALFA,Rsqu_test, label='validation data  ')\nplt.plot(ALFA,Rsqu_train, 'r', label='training Data ')\nplt.xlabel('alpha')\nplt.ylabel('R^2')\nplt.legend()","2797b2e6":"from sklearn.model_selection import GridSearchCV","41723c84":"parameters1= [{'alpha': [0.001,0.1,1, 10, 100, 1000, 10000, 100000, 100000]}]\nparameters1","077fbc88":"RR=Ridge()\nRR","b8efe83d":"Grid1 = GridSearchCV(RR, parameters1,cv=4)","8794afcf":"Grid1.fit(x_data[['horsepower', 'curb-weight', 'engine-size', 'highway-mpg']], y_data)","24689d7b":"BestRR=Grid1.best_estimator_\nBestRR","386d264f":"BestRR.score(x_test[['horsepower', 'curb-weight', 'engine-size', 'highway-mpg']], y_test)","1bcf8e46":"You can also use the function 'cross_val_predict' to predict the output. The function splits up the data into the specified number of folds, using one fold to get a prediction while the rest of the folds are used as test data. First import the function:","4bf50c1e":" R^2 of the test data:","ecdf78bc":" We can calculate the average and standard deviation of our estimate:","fdd59ccf":"<h1 id=\"ref2\">Part 2: Overfitting, Underfitting and Model Selection<\/h1>\n\n<p>It turns out that the test data sometimes referred to as the out of sample data is a much better measure of how well your model performs in the real world.  One reason for this is overfitting; let's go over some examples. It turns out these differences are more apparent in Multiple Linear Regression and Polynomial Regression so we will explore overfitting in that context.<\/p>","06ccfa73":"We input the object, the feature in this case ' horsepower', the target data (y_data). The parameter 'cv'  determines the number of folds; in this case 4. ","3056ad93":"The <b>test_size<\/b> parameter sets the proportion of data that is split into the testing set. In the above, the testing set is set to 15% of the total dataset. ","01b76cb1":"<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">\n    <a href=\"https:\/\/cocl.us\/corsera_da0101en_notebook_top\">\n         <img src=\"https:\/\/s3-api.us-geo.objectstorage.softlayer.net\/cf-courses-data\/CognitiveClass\/DA0101EN\/Images\/TopAd.png\" width=\"750\" align=\"center\">\n    <\/a>\n<\/div>\n","e68c990f":"The following interface allows you to experiment with different polynomial orders and different amounts of data. ","0a7a7ee9":"Let's perform some model evaluation using our training and testing data separately. First  we import the seaborn and matplotlibb library for plotting.","ef16404f":"Figur 4 A polynomial regression model, red dots represent training data, green dots represent test data, and the blue line represents the model prediction. ","0d57a60b":"Let's compare the first five predicted samples to our test set ","c421d36a":"Fit the model ","e96321cc":"Let's create a Ridge regression object, setting the regularization parameter to 0.1 ","1c8fdcfd":"We can see the output of our model using the method  \"predict.\" then assign the values to \"yhat\".","af954a5c":"<h2 id=\"ref3\">Part 3: Ridge regression<\/h2> ","49d57c6e":"Figure 6:The blue line represents the R^2 of the test data, and the red line represents the R^2 of the training data. The x-axis represents the different values of Alfa ","403ccde4":"Let's import <b>GridSearchCV<\/b> from  the module <b>model_selection<\/b>.","bde758e4":" In this section, we will review Ridge Regression we will see how the parameter Alfa changes the model. Just a note here our test data will be used as validation data.","25153895":"We create a dictionary of parameter values:","fcb1ab95":"Let's Calculate the R^2 on the test data:","97931498":"we fit the model using the feature horsepower ","f1df5782":"Let's examine the distribution of the predicted values of the training data.","a03abe41":"Create a ridge regions object:","69e097c2":"We will use the function \"PollyPlot\" that we defined at the beginning of the lab to display the training data, testing data, and the predicted function.","f5adbbe7":"The object finds the best parameter values on the validation data. We can obtain the estimator with the best parameters and assign it to the variable BestRR as follows:","2cf6c236":"<p>Comparing Figure 1 and Figure 2; it is evident the distribution of the test data in Figure 1 is much better at fitting the data. This difference in Figure 2 is apparent where the ranges are from 5000 to 15 000. This is where the distribution shape is exceptionally different. Let's see if polynomial regression also exhibits a drop in the prediction accuracy when analysing the test dataset.<\/p>","7eb97acb":"The term Alfa is a hyperparameter, sklearn has the class  <b>GridSearchCV<\/b> to make the process of finding the best hyperparameter simpler.","276864bd":"We select the value of Alfa that minimizes the test error, for example, we can use a for loop. ","a49fd7eb":"We input the object, the feature in this case <b>'horsepower'<\/b> , the target data <b>y_data<\/b>. The parameter 'cv' determines the number of folds; in this case 4. We can produce an output:","15be5b21":"Figur 2: Plot of predicted value using the test data compared to the test data. ","4f6b1be1":"<h1 align=center><font size=5>Data Analysis with Python<\/font><\/h1>","8dc77a9f":"Prediction using test data: ","a2afca40":"<h2>Functions for plotting<\/h2>","1114254c":"The default scoring is R^2; each element in the array has the average  R^2 value in the fold:","5062e94d":" Similarly, you can obtain a prediction: ","ee245831":"<h2 id=\"ref4\">Part 4: Grid Search<\/h2>","7e73939c":"<h1>Table of content<\/h1>\n<ul>\n    <li><a href=\"#ref1\">Model Evaluation <\/a><\/li>\n    <li><a href=\"#ref2\">Over-fitting, Under-fitting and Model Selection <\/a><\/li>\n    <li><a href=\"#ref3\">Ridge Regression <\/a><\/li>\n    <li><a href=\"#ref4\">Grid Search<\/a><\/li>\n<\/ul>","585f0767":"<h4>Overfitting<\/h4>\n<p>Overfitting occurs when the model fits the noise, not the underlying process. Therefore when testing your model using the test-set, your model does not perform as well as it is modelling noise, not the underlying process that generated the relationship. Let's create a degree 5 polynomial model.<\/p>","9e2bf69c":"We will perform a degree 5 polynomial transformation on the feature <b>'horse power'<\/b>. ","c5112d78":"Now we randomly split our data into training and testing data  using the function <b>train_test_split<\/b>. ","85b5ee5c":"<h1 id=\"ref1\">Part 1: Training and Testing<\/h1>\n\n<p>An important step in testing your model is to split your data into training and testing data. We will place the target data <b>price<\/b> in a separate dataframe <b>y<\/b>:<\/p>","55581aa5":" We create a Linear Regression object:","df131618":"Lets import <b>model_selection<\/b> from the module <b>cross_val_score<\/b>.","4f00c381":"Create a ridge grid search object ","ecd1a2e9":"We see the R^2 gradually increases until an order three polynomial is used. Then the  R^2 dramatically decreases at four.","167c111f":"The red line in figure 6 represents the  R^2 of the test data, as Alpha increases the R^2 decreases; therefore as Alfa increases the model performs worse on the test data.  The blue line represents the R^2 on the validation data, as the value for Alfa increases the R^2 decreases.   ","6dc2d1e6":"Let's use 55 percent of the data for testing and the rest for training:","b0d96ae9":"We see the R^2 for the training data is 0.5567 while the R^2 on the test data was -29.87.  The lower the R^2, the worse the model, a Negative R^2 is a sign of overfitting.","d0ae9c95":"We can plot out the value of R^2 for different Alphas ","a6fbef2a":"The following function will be used in the next section; please run the cell.","56a1a8bb":"<h1> Model Evaluation and Refinement<\/h1>\n\nWe have built models and made predictions of vehicle prices. Now we will determine how accurate these predictions are. ","9f3195b1":"Let's import <b>LinearRegression<\/b> from the module <b>linear_model<\/b>.","1fa5bd88":" First lets only use numeric data ","5814f6a7":" Let's import  <b>Ridge<\/b>  from the module <b>linear models<\/b>.","bc2e7afe":"we can see the R^2 is much smaller using the test data.","21e3fdcd":"Let's create Multiple linear regression objects and train the model using <b>'horsepower'<\/b>, <b>'curb-weight'<\/b>, <b>'engine-size'<\/b> and <b>'highway-mpg'<\/b> as features.","4ac2fe15":" Libraries for plotting ","e0a555bc":" Let's perform a degree two polynomial transformation on our data. ","7489c14b":"Prediction using training data:","a28e7c1e":" R^2 of the training data:","6304eaf0":"<h2>Cross-validation Score<\/h2>","db0b0282":" We now test our model on the test data ","db031239":"Let's see how the R^2 changes on the test data for different order polynomials and plot the results:","74e0cbb8":"Like regular regression, you can fit the model using the method <b>fit<\/b>.","a70835e3":"So far the model seems to be doing well in learning from the training dataset. But what happens when the model encounters new data from the testing dataset? When the model generates new values from the test data, we see the distribution of the predicted values is much different from the actual target values. ","fe43fc3c":"Now let's create a linear regression model \"poly\" and train it.","7e17b104":"Let's take the first five predicted values and compare it to the actual targets. ","1748bc20":"Figure 1: Plot of predicted values using the training data compared to the training data. ","098b7362":"We see that the estimated function appears to track the data but around 200 horsepower, the function begins to diverge from the data points. ","200dbbea":" Sometimes you do not have sufficient testing data; as a result, you may want to perform Cross-validation. Let's  go over several methods that you can use for  Cross-validation. ","f4e99e26":"drop price data in x data","c8d7b1d6":"We can use negative squared error as a score by setting the parameter  'scoring' metric to 'neg_mean_squared_error'. "}}