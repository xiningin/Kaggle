{"cell_type":{"2e7c9e7f":"code","200b3f25":"code","5bf5f597":"code","ca6c04f2":"code","ed949f7d":"code","00e51924":"code","55a961ef":"code","9fc14cd9":"code","a7a264f9":"code","e8856cac":"code","3f3435cf":"code","f0715241":"code","0b8c24c9":"code","2fed1602":"code","e0db87d0":"code","1a1b5e26":"code","22d4b8ac":"code","aa8af3ff":"code","f861c01c":"code","536f8acd":"code","f5657f8b":"code","63adb9a2":"code","6173c72c":"code","4ddf38a5":"code","85667830":"code","26cf0566":"code","007c0387":"code","778a096f":"code","b07b370c":"markdown","be5ad614":"markdown","6d62121c":"markdown","cae1cc5d":"markdown","74ea3ff4":"markdown","89f3e1a5":"markdown","edf421ad":"markdown","d187678b":"markdown","a4bfb0f6":"markdown","5869d27b":"markdown","4f8cd586":"markdown"},"source":{"2e7c9e7f":"import pandas as pd\nimport numpy as np\nimport cv2\nimport json\nimport os\nimport matplotlib.pyplot as plt\nimport random\nimport seaborn as sns\nfrom keras.models import Sequential\nfrom keras import optimizers\nfrom keras import backend as K\nfrom keras.layers import Dense, Dropout, Activation, Flatten\nfrom keras.layers import Conv2D, MaxPooling2D, BatchNormalization\nfrom sklearn.model_selection import train_test_split\nfrom keras.preprocessing.image import ImageDataGenerator\n","200b3f25":"directory = \"..\/input\/face-mask-detection-dataset\/Medical mask\/Medical mask\/Medical Mask\/annotations\"\nimage_directory = \"..\/input\/face-mask-detection-dataset\/Medical mask\/Medical mask\/Medical Mask\/images\"\ndf = pd.read_csv(\"..\/input\/face-mask-detection-dataset\/train.csv\")\ndf_test = pd.read_csv(\"..\/input\/face-mask-detection-dataset\/submission.csv\")","5bf5f597":"cvNet = cv2.dnn.readNetFromCaffe('..\/input\/caffe-face-detector-opencv-pretrained-model\/architecture.txt','..\/input\/caffe-face-detector-opencv-pretrained-model\/weights.caffemodel')","ca6c04f2":"def getJSON(filePathandName):\n    with open(filePathandName,'r') as f:\n        return json.load(f)","ed949f7d":"def adjust_gamma(image, gamma=1.0):\n    invGamma = 1.0 \/ gamma\n    table = np.array([((i \/ 255.0) ** invGamma) * 255 for i in np.arange(0, 256)])\n    return cv2.LUT(image.astype(np.uint8), table.astype(np.uint8))","00e51924":"jsonfiles= []\nfor i in os.listdir(directory):\n    jsonfiles.append(getJSON(os.path.join(directory,i)))\njsonfiles[0]","55a961ef":"df = pd.read_csv(\"..\/input\/face-mask-detection-dataset\/train.csv\")\ndf.head()","9fc14cd9":"data = []\nimg_size = 124\nmask = ['face_with_mask']\nnon_mask = [\"face_no_mask\"]\nlabels={'mask':0,'without mask':1}\nfor i in df[\"name\"].unique():\n    f = i+\".json\"\n    for j in getJSON(os.path.join(directory,f)).get(\"Annotations\"):\n        if j[\"classname\"] in mask:\n            x,y,w,h = j[\"BoundingBox\"]\n            img = cv2.imread(os.path.join(image_directory,i),1)\n            img = img[y:h,x:w]\n            img = cv2.resize(img,(img_size,img_size))\n            data.append([img,labels[\"mask\"]])\n        if j[\"classname\"] in non_mask:\n            x,y,w,h = j[\"BoundingBox\"]\n            img = cv2.imread(os.path.join(image_directory,i),1)\n            img = img[y:h,x:w]\n            img = cv2.resize(img,(img_size,img_size))    \n            data.append([img,labels[\"without mask\"]])\nrandom.shuffle(data)   ","a7a264f9":"len(data)","e8856cac":"p = []\nfor face in data:\n    if(face[1] == 0):\n        p.append(\"Mask\")\n    else:\n        p.append(\"No Mask\")\nsns.countplot(p)","3f3435cf":"X = []\nY = []\nfor features,label in data:\n    X.append(features)\n    Y.append(label)","f0715241":"X[0].shape","0b8c24c9":"X = np.array(X)\/255.0\nX = X.reshape(-1,124,124,3)\nY = np.array(Y)","2fed1602":"np.unique(Y)","e0db87d0":"Y.shape","1a1b5e26":"model = Sequential()\n\nmodel.add(Conv2D(32, (3, 3), padding = \"same\", activation='relu', input_shape=(124,124,3)))\nmodel.add(Conv2D(64, (3, 3), activation='relu'))\nmodel.add(Conv2D(128, (3, 3), activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2,2)))\nmodel.add(Dropout(0.25))\n \nmodel.add(Flatten())\nmodel.add(Dropout(0.5))\nmodel.add(Dense(50, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(1, activation='sigmoid'))","22d4b8ac":"model.summary()","aa8af3ff":"model.compile(loss='binary_crossentropy', optimizer='adam' ,metrics=['accuracy'])","f861c01c":"xtrain,xval,ytrain,yval=train_test_split(X, Y,train_size=0.8,random_state=0)","536f8acd":"datagen = ImageDataGenerator(\n        featurewise_center=False,  \n        samplewise_center=False,  \n        featurewise_std_normalization=False,  \n        samplewise_std_normalization=False,  \n        zca_whitening=False,    \n        rotation_range=15,    \n        width_shift_range=0.1,\n        height_shift_range=0.1,  \n        horizontal_flip=True,  \n        vertical_flip=False)\ndatagen.fit(xtrain)","f5657f8b":"history = model.fit_generator(datagen.flow(xtrain, ytrain, batch_size=32),\n                    steps_per_epoch=xtrain.shape[0]\/\/32,\n                    epochs=50,\n                    verbose=1,\n                    validation_data=(xval, yval))","63adb9a2":"np.mean(history.history['accuracy'])","6173c72c":"np.mean(history.history['val_accuracy'])","4ddf38a5":"plt.plot(history.history['accuracy'],'g')\nplt.plot(history.history['val_accuracy'],'b')\nplt.title('Training Accuracy vs Validation Accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Validation'], loc='upper left')\nplt.show()","85667830":"plt.plot(history.history['loss'],'g')\nplt.plot(history.history['val_loss'],'b')\nplt.title('Training Loss vs Validation Loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Validation'], loc='upper left')\nplt.show()","26cf0566":"print(len(df_test[\"name\"]),len(df_test[\"name\"].unique()))","007c0387":"test_images = ['1114.png','1504.jpg', '0072.jpg','0012.jpg','0353.jpg','1374.jpg']","778a096f":"gamma = 2.0\nfig = plt.figure(figsize = (14,14))\nrows = 3\ncols = 2\naxes = []\nassign = {'0':'Mask','1':\"No Mask\"}\nfor j,im in enumerate(test_images):\n    image =  cv2.imread(os.path.join(image_directory,im),1)\n    image =  adjust_gamma(image, gamma=gamma)\n    (h, w) = image.shape[:2]\n    blob = cv2.dnn.blobFromImage(cv2.resize(image, (300,300)), 1.0, (300, 300), (104.0, 177.0, 123.0))\n    cvNet.setInput(blob)\n    detections = cvNet.forward()\n    for i in range(0, detections.shape[2]):\n        try:\n            box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n            (startX, startY, endX, endY) = box.astype(\"int\")\n            frame = image[startY:endY, startX:endX]\n            confidence = detections[0, 0, i, 2]\n            if confidence > 0.2:\n                im = cv2.resize(frame,(img_size,img_size))\n                im = np.array(im)\/255.0\n                im = im.reshape(1,124,124,3)\n                result = model.predict(im)\n                if result>0.5:\n                    label_Y = 1\n                else:\n                    label_Y = 0\n                cv2.rectangle(image, (startX, startY), (endX, endY), (0, 0, 255), 2)\n                cv2.putText(image,assign[str(label_Y)] , (startX, startY-10), cv2.FONT_HERSHEY_SIMPLEX, 1.5, (36,255,12), 2)\n        \n        except:pass\n    axes.append(fig.add_subplot(rows, cols, j+1))\n    plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\nplt.show()","b07b370c":"# Model Architecture and Training Process","be5ad614":"* Gamma Correction for making the image appear in more light.(Gamma = 2)\n* blobFromImage creates 4-dimensional blob from image. Optionally resizes and crops image from center, subtract mean values, scales values by scalefactor, swap Blue and Red channels. [Refer OpenCV documentation]\n* The blob is passed through the SSD network and detections are made with some confidence score.\n* Define a threshold confidence score, above which the detection will be considered as a candidate of being a face. (In this case confidence threshold = 0.2)\n* All the detections that qualify the confidence score are then passed to the architecture for classification into mask or non-mask image.","6d62121c":"**Steps we are gonna follow to solve this huge problems are:**\n1. Importing libraries\n","cae1cc5d":"## Model Testing\nThe test dataset has 1698 images and to evaluate the model I have taken a handful of images from this dataset as there are no labels for faces in the dataset.","74ea3ff4":"# Conclusion\n\n* By analyzing the results it can be observed that the whole system works well for faces that have spatial dominance i.e. in image at (1,1), (1,2) and (2,1) but fails in case of (2,2) where the faces are small and occupy less space in the overall image. To get better results, different image preprocessing techniques can be used, or confidence threshold can be kept lower, or one can try different blob size.","89f3e1a5":"The visualization below tells us that the Number of Mask images > Number of Non-Mask images, so this is an imbalanced dataset. But since we are using a SSD pretrained model, which is trained to detect non-mask faces, this imbalance would not matter a lot.","edf421ad":"* By using the mask label and non_mask label, the bounding box data from json files is extracted.\n* The faces from any particular image are extracted and stored in the data list along with its label for the training process.","d187678b":"# Training and Validation Visualizations","a4bfb0f6":"**Functions**\n\n1. JSON Function fetches the json file that has the data of bounding box in the training dataset.\n\n2. Gamma correction, or often simply gamma, is a nonlinear operation used to encode and decode luminance or tristimulus values in video or still image systems. In simple terms it is used to instill some light in the image. If gamma < 1, image will shift towards darker end of the spectrum and when gamma > 1, there will be more light in the image.","5869d27b":"# Introduction\n\nFace mask detection has a range of applications from capturing the movement of the face to facial recognition which at first requires the face to be detected with very good precision. Face detection is more relevant today as it is not only used on images, but also in video applications like real-time surveillance and face detection in videos.\n\nHigh precision image classification is now possible with advances in convolutional networks. Pixel level information is often needed after face detection, which most face detection methods do not provide.\n\nObtaining pixel-level detail has been a difficult part of semantic segmentation. Semantic segmentation is the process of assigning a label to each pixel in the image","4f8cd586":"# Data Preprocessing\n\nLets look at the JSON data given for training:\n\n* The Annotations field contains the data of all the faces present in a particular image.\n* There are various classnames but the true classnames are face_with_mask and face_no_mask."}}