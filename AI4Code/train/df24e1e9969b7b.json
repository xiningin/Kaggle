{"cell_type":{"954e5726":"code","422cec9a":"code","822950a5":"code","6916ac1f":"code","f499256b":"code","6d4e2133":"code","8b2b2776":"code","10efed68":"code","f36d2a43":"code","b8f9a1ba":"code","570f65a3":"code","32a97402":"code","0101b9ea":"code","a82ff1d1":"code","5ea7c99e":"code","ba477889":"code","c12b7514":"code","c3739880":"code","31da517f":"code","62cc4221":"code","3c1213ba":"code","23e2ac87":"code","bebcf2c5":"code","26c3c65c":"code","bfa74d48":"code","99cd1441":"code","8f359594":"code","d00e137e":"code","1d2881b7":"code","77e3fb55":"code","f47d5ff4":"code","a5cab130":"code","f34e9d50":"code","0ca0b40c":"code","a4dba95e":"code","f2d833dc":"code","b64be4ed":"code","89f2e3c1":"code","b2f1fc3a":"code","aaf6b68a":"code","db6d6027":"code","8183b5a3":"markdown","0ed1ddb5":"markdown","3a3c65cf":"markdown","547b562c":"markdown","b7a1c185":"markdown","79df4250":"markdown","fac7ea9e":"markdown","5dc1301a":"markdown","b93885f6":"markdown","34286ce6":"markdown","0cfb0830":"markdown","609a27d1":"markdown","312fd62f":"markdown","df254ad7":"markdown","75e83def":"markdown","6fdc33ce":"markdown","7593d384":"markdown","d0763611":"markdown","317e4b76":"markdown","e0fcc52a":"markdown","912397ea":"markdown","e1456753":"markdown","b9e6ee7a":"markdown","71f83b0c":"markdown","aedfca28":"markdown","368a0778":"markdown","9a04171a":"markdown","9bb2cc9e":"markdown"},"source":{"954e5726":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nimport tensorflow.keras.backend as K\nfrom sklearn.model_selection import StratifiedKFold\nfrom transformers import *\nimport tokenizers\nimport math\nfrom copy import deepcopy as dc\nimport gc\nprint('TF version',tf.__version__)","422cec9a":"MAX_LEN = 96\nPATH = '..\/input\/tf-roberta\/'\ntokenizer = tokenizers.ByteLevelBPETokenizer(\n    vocab_file=PATH+'vocab-roberta-base.json', \n    merges_file=PATH+'merges-roberta-base.txt', \n    lowercase=True,\n    add_prefix_space=True\n)\nEPOCHS = 1 # originally 3\nBATCH_SIZE = 32 # originally 32\nPAD_ID = 1\nSEED = 88888\nLABEL_SMOOTHING = 0.1\ntf.random.set_seed(SEED)\nnp.random.seed(SEED)\nsentiment_id = {'positive': 1313, 'neutral': 7974, 'negative': 2430}","822950a5":"train = pd.read_csv('..\/input\/tweet-sentiment-extraction\/train.csv').fillna('')\n# if you directly want the result database, just uncomment the following line :\n# train = pd.read_csv('..\/input\/extended-train-for-tweet\/extended_train.csv')\ntrain.head()","6916ac1f":"print(train.shape)\nn = train.shape[0]","f499256b":"from nltk.corpus import wordnet, stopwords\nstop = stopwords.words('english')\nstop += [\"_________________________________\", \"u\"]\nimport string\npunct = list(string.punctuation)\npunct.remove(\"-\")\npunct.append(\" \")","6d4e2133":"def get_synonyms(word):\n    \"\"\"\n    Get synonyms of a word\n    \"\"\"\n    if word.lower() in stop:\n        return [word], [1]\n    \n    synonyms = set()\n    for syn in wordnet.synsets(word): \n        for l in syn.lemmas(): \n            synonym = l.name().replace(\"_\", \" \").replace(\"-\", \" \").lower()\n            synonym = \"\".join([char for char in synonym if char in ' qwertyuiopasdfghjklzxcvbnm'])\n            synonyms.add(synonym) \n    \n    if word not in synonyms:\n        synonyms.add(word)\n        \n    n = len(synonyms)\n    \n    if n == 1: # we didn't find any synonyms for that word, therefore we will try to check if it's not because of some punctuation interfering\n        word_ = \"\".join(list(filter(lambda x: x not in punct, word)))\n        if word_.lower() in stop:\n            return [word, word_], [0.5, 0.5]\n        for syn in wordnet.synsets(word_): \n            for l in syn.lemmas(): \n                synonym = l.name().replace(\"_\", \" \").replace(\"-\", \" \").lower()\n                synonym = \"\".join([char for char in synonym if char in ' qwertyuiopasdfghjklzxcvbnm'])\n                synonyms.add(synonym) \n        if word_ not in synonyms:\n            synonyms.add(word_)\n            \n    n = len(synonyms)\n    if n == 1:\n        probabilities = [1]\n    else:\n        probabilities = [0.5 if w==word else 0.5\/(n-1) for w in synonyms]\n    \n    return list(synonyms), probabilities","8b2b2776":"for word in ['sad', 'SAD', 'Sad...', 'saaaaad']:\n    print(f'For word {word}, synonyms and corresponding probabilities are :')\n    print(get_synonyms(word))\n    print('-'*20)","10efed68":"def swap_words(words):\n    words = words.split()\n    if len(words) < 2:\n        return \" \".join(words), False\n    random_idx = np.random.randint(0, len(words)-1)\n    words[random_idx], words[random_idx+1] = words[random_idx+1], words[random_idx] \n    return \" \".join(words), True","f36d2a43":"for _ in range(5):\n    print(swap_words('The sun is shining today, this makes me feel so good !')[0])","b8f9a1ba":"def new_row(row, n_samples=1): \n    text, selected_text, textID = row['text'], row['selected_text'], row['textID']\n    oth_text = text.replace(selected_text, \" _________________________________ \")\n    new_selected_text = [get_synonyms(word) for word in selected_text.split()]\n    new_oth_text = [get_synonyms(word) for word in oth_text.split()]\n    new_sentences = [row]\n    for i in range(n_samples):\n        oth_text_ = \" \".join([np.random.choice(l_syn, p=p, replace=True) for l_syn, p in new_oth_text])\n        selected_text_ = \" \".join([np.random.choice(l_syn, p=p, replace=True) for l_syn, p in new_selected_text])\n        text_ = oth_text_.replace(\"_________________________________\", selected_text_)\n        if not selected_text_ in text_:\n            print(f'Original : {text} with target {selected_text}, oth_text {oth_text}\\nTransformed : {text_} with target {selected_text_}, oth_text {oth_text_}')\n            continue\n        row2 = dc(row)\n        row2['text'] = text_\n        row2['selected_text'] = selected_text_\n        row2['textID'] = f'new_{textID}'\n        new_sentences.append(row2)\n    for r in dc(new_sentences):\n        r_ = dc(r)\n        if np.random.choice([True, False]):\n            selected_text, boo = swap_words(r_['selected_text'])\n            if boo:\n                r_['text'] = r_['text'].replace(r_['selected_text'], selected_text)\n                r_['selected_text'] = selected_text\n            else:\n                oth_text, _ = swap_words(r_['text'].replace(r_['selected_text'], \" _________________________________ \"))\n                r_['text'] = oth_text.replace(\"_________________________________\",r_['selected_text'])\n        else:\n            oth_text, _ = swap_words(r_['text'].replace(r_['selected_text'], \" _________________________________ \"))\n            r_['text'] = oth_text.replace(\"_________________________________\", r_['selected_text'])\n        r_['textID'] = f'new_{textID}'\n        new_sentences.append(r_)\n    new_rows = pd.concat(new_sentences, axis=1).transpose().drop_duplicates(subset=['text'], inplace=False, ignore_index=True)\n    new_rows = new_rows.loc[new_rows['text'].apply(len)<150]\n    counter = 0\n    for i, row in new_rows.iterrows():\n        if row['textID'][:4] == 'new_':\n            row['textID'] = row['textID']+f'_{counter}'\n            counter += 1\n    return new_rows","570f65a3":"new_row(train.loc[np.random.choice(train.shape[0])], n_samples=8)","32a97402":"temp = [new_row(row, n_samples=2) for _, row in train.iterrows()]\naugmented_data = pd.concat(temp, axis=0)#.sample(frac=1)\ntrain['number'] = [t.shape[0] for t in temp]\ntrain['number'] = train['number'].cumsum()\ndel temp\ngc.collect()\naugmented_data.drop_duplicates(subset=['text'], inplace=False, ignore_index=True)\naugmented_data.reset_index(drop=True, inplace=True)\n# augmented_data.head(20)\nmatch_index = dc(train['number'])\ntrain.drop(columns='number', inplace=True)\nmatch_index = [0] + match_index.values.tolist()\nmatch_borders = list(zip(match_index[:-1], match_index[1:]))\ndel match_index\ngc.collect()\ntrain['brackets'] = match_borders\n# train.head()","0101b9ea":"augmented_data.to_csv('extended_train.csv', index=False)\ntrain = augmented_data\ndel augmented_data","a82ff1d1":"train['text_len'] = train['text'].apply(len)\ntrain.hist(column='text_len')\ntrain.loc[train.text_len<150].hist(column='text_len')\ntrain.loc[train.text_len>=150, 'textID'].apply(lambda x: 'new' in x).describe()","5ea7c99e":"train = train.loc[train.text_len<150]\ntrain.drop(columns=[\"text_len\"], inplace=True)\ntrain.reset_index(drop=True, inplace=True)","ba477889":"train.to_csv('extended_train.csv', index=False)","c12b7514":"print(f'We now have {round(train.shape[0]\/n, 1)} as much data as initially !')","c3739880":"ct = augmented_data.shape[0]\ninput_ids = np.ones((ct,MAX_LEN),dtype='int32')\nattention_mask = np.zeros((ct,MAX_LEN),dtype='int32')\ntoken_type_ids = np.zeros((ct,MAX_LEN),dtype='int32')\nstart_tokens = np.zeros((ct,MAX_LEN),dtype='int32')\nend_tokens = np.zeros((ct,MAX_LEN),dtype='int32')\n\nfor k, row in augmented_data.iterrows():\n    \n    # FIND OVERLAP\n    text1 = \" \"+\" \".join(row['text'].split())\n    text2 = \" \".join(row['selected_text'].split())\n    idx = text1.find(text2)\n    chars = np.zeros((len(text1)))\n    chars[idx:idx+len(text2)]=1\n    if text1[idx-1]==' ': \n        chars[idx-1] = 1 \n    enc = tokenizer.encode(text1) \n        \n    # ID_OFFSETS\n    offsets = []\n    idx=0\n    for t in enc.ids:\n        w = tokenizer.decode([t])\n        offsets.append((idx,idx+len(w)))\n        idx += len(w)\n    \n    # START END TOKENS\n    toks = []\n    for i,(a,b) in enumerate(offsets):\n        sm = np.sum(chars[a:b])\n        if sm>0: \n            toks.append(i) \n        \n    s_tok = sentiment_id[row['sentiment']]\n    input_ids[k,:len(enc.ids)+3] = [0, s_tok] + enc.ids + [2]\n    attention_mask[k,:len(enc.ids)+3] = 1\n    if len(toks)>0:\n        start_tokens[k,toks[0]+2] = 1\n        end_tokens[k,toks[-1]+2] = 1","31da517f":"ct_train = train.shape[0]\ninput_ids_train = np.ones((ct_train,MAX_LEN),dtype='int32')\nattention_mask_train = np.zeros((ct_train,MAX_LEN),dtype='int32')\ntoken_type_ids_train = np.zeros((ct_train,MAX_LEN),dtype='int32')\nstart_tokens_train = np.zeros((ct_train,MAX_LEN),dtype='int32')\nend_tokens_train = np.zeros((ct_train,MAX_LEN),dtype='int32')\n\nfor k, row in train.iterrows():\n    \n    # FIND OVERLAP\n    text1 = \" \"+\" \".join(row['text'].split())\n    text2 = \" \".join(row['selected_text'].split())\n    idx = text1.find(text2)\n    chars = np.zeros((len(text1)))\n    chars[idx:idx+len(text2)]=1\n    if text1[idx-1]==' ': \n        chars[idx-1] = 1 \n    enc = tokenizer.encode(text1) \n        \n    # ID_OFFSETS\n    offsets = []\n    idx=0\n    for t in enc.ids:\n        w = tokenizer.decode([t])\n        offsets.append((idx,idx+len(w)))\n        idx += len(w)\n    \n    # START END TOKENS\n    toks = []\n    for i,(a,b) in enumerate(offsets):\n        sm = np.sum(chars[a:b])\n        if sm>0: \n            toks.append(i) \n        \n    s_tok = sentiment_id[row['sentiment']]\n    input_ids_train[k,:len(enc.ids)+3] = [0, s_tok] + enc.ids + [2]\n    attention_mask_train[k,:len(enc.ids)+3] = 1\n    if len(toks)>0:\n        start_tokens_train[k,toks[0]+2] = 1\n        end_tokens_train[k,toks[-1]+2] = 1","62cc4221":"test = pd.read_csv('..\/input\/tweet-sentiment-extraction\/test.csv').fillna('')","3c1213ba":"def test_new_row(row, n_samples=1): \n    text, textID = row['text'], row['textID']\n    new_text = [get_synonyms(word) for word in text.split()]\n    new_sentences = [row]\n    for i in range(n_samples):\n        text_ = \" \".join([np.random.choice(l_syn, p=p, replace=True) for l_syn, p in new_text])\n        row2 = dc(row)\n        row2['text'] = text_\n        row2['textID'] = f'new_{textID}'\n        new_sentences.append(row2)\n    for r in dc(new_sentences):\n        r_ = dc(r)\n        text, boo = swap_words(r_['text'])\n        if boo:\n            r_['text'] = text\n            r_['textID'] = f'new_{textID}'\n            new_sentences.append(r_)\n    new_rows = pd.concat(new_sentences, axis=1).transpose().drop_duplicates(subset=['text'], inplace=False, ignore_index=True)\n    new_rows = new_rows.loc[new_rows['text'].apply(len)<150]\n    counter = 0\n    for i, row in new_rows.iterrows():\n        if row['textID'][:4] == 'new_':\n            row['textID'] = row['textID']+f'_{counter}'\n            counter += 1\n    return new_rows","23e2ac87":"temp = [test_new_row(row, n_samples=2) for _, row in test.iterrows()]\ntest_augmented_data = pd.concat(temp, axis=0)#.sample(frac=1)\ntest['number'] = [t.shape[0] for t in temp]\ntest['number'] = test['number'].cumsum()\ndel temp\ngc.collect()\ntest_augmented_data.drop_duplicates(subset=['text'], inplace=False, ignore_index=True)\ntest_augmented_data.reset_index(drop=True, inplace=True)\ntest_match_index = dc(test['number'])\ntest.drop(columns='number', inplace=True)\ntest_match_index = [0] + test_match_index.values.tolist()\ntest_match_borders = list(zip(test_match_index[:-1], test_match_index[1:]))\ndel test_match_index\ngc.collect()\ntest['brackets'] = test_match_borders","bebcf2c5":"test_augmented_data.head(10)","26c3c65c":"test.head()","bfa74d48":"ct = test.shape[0]\ntest_input_ids_t = np.ones((ct,MAX_LEN),dtype='int32')\n\nfor k, row in test.iterrows():\n\n    # INPUT_IDS\n    text1 = \" \"+\" \".join(row['text'].split())\n    enc = tokenizer.encode(text1)                \n    s_tok = sentiment_id[row['sentiment']]\n    test_input_ids_t[k,:len(enc.ids)+3] = [0, s_tok] + enc.ids + [2]\n    \nct = test_augmented_data.shape[0]\ninput_ids_t = np.ones((ct,MAX_LEN),dtype='int32')\nattention_mask_t = np.zeros((ct,MAX_LEN),dtype='int32')\ntoken_type_ids_t = np.zeros((ct,MAX_LEN),dtype='int32')\n\nfor k, row in test_augmented_data.iterrows():\n\n    # INPUT_IDS\n    text1 = \" \"+\" \".join(row['text'].split())\n    enc = tokenizer.encode(text1)                \n    s_tok = sentiment_id[row['sentiment']]\n    input_ids_t[k,:len(enc.ids)+3] = [0, s_tok] + enc.ids + [2]\n    attention_mask_t[k,:len(enc.ids)+3] = 1","99cd1441":"import pickle\n\ndef save_weights(model, dst_fn):\n    weights = model.get_weights()\n    with open(dst_fn, 'wb') as f:\n        pickle.dump(weights, f)\n\n\ndef load_weights(model, weight_fn):\n    with open(weight_fn, 'rb') as f:\n        weights = pickle.load(f)\n    model.set_weights(weights)\n    return model\n\ndef loss_fn(y_true, y_pred):\n    # adjust the targets for sequence bucketing\n    ll = tf.shape(y_pred)[1]\n    y_true = y_true[:, :ll]\n    loss = tf.keras.losses.categorical_crossentropy(y_true, y_pred,\n        from_logits=False, label_smoothing=LABEL_SMOOTHING)\n    loss = tf.reduce_mean(loss)\n    return loss\n\n\ndef build_model():\n    ids = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n    att = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n    tok = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n    padding = tf.cast(tf.equal(ids, PAD_ID), tf.int32)\n\n    lens = MAX_LEN - tf.reduce_sum(padding, -1)\n    max_len = tf.reduce_max(lens)\n    ids_ = ids[:, :max_len]\n    att_ = att[:, :max_len]\n    tok_ = tok[:, :max_len]\n\n    config = RobertaConfig.from_pretrained(PATH+'config-roberta-base.json')\n    bert_model = TFRobertaModel.from_pretrained(PATH+'pretrained-roberta-base.h5',config=config)\n    x = bert_model(ids_,attention_mask=att_,token_type_ids=tok_)\n    \n    x1 = tf.keras.layers.Dropout(0.1)(x[0])\n    x1 = tf.keras.layers.Conv1D(768, 2,padding='same')(x1)\n    x1 = tf.keras.layers.LeakyReLU()(x1)\n    x1 = tf.keras.layers.Dense(1)(x1)\n    x1 = tf.keras.layers.Flatten()(x1)\n    x1 = tf.keras.layers.Activation('softmax')(x1)\n    \n    x2 = tf.keras.layers.Dropout(0.1)(x[0]) \n    x2 = tf.keras.layers.Conv1D(768, 2,padding='same')(x2)\n    x2 = tf.keras.layers.LeakyReLU()(x2)\n    x2 = tf.keras.layers.Dense(1)(x2)\n    x2 = tf.keras.layers.Flatten()(x2)\n    x2 = tf.keras.layers.Activation('softmax')(x2)\n\n    model = tf.keras.models.Model(inputs=[ids, att, tok], outputs=[x1,x2])\n    optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5) \n    model.compile(loss=loss_fn, optimizer=optimizer)\n    \n    # this is required as `model.predict` needs a fixed size!\n    x1_padded = tf.pad(x1, [[0, 0], [0, MAX_LEN - max_len]], constant_values=0.)\n    x2_padded = tf.pad(x2, [[0, 0], [0, MAX_LEN - max_len]], constant_values=0.)\n    \n    padded_model = tf.keras.models.Model(inputs=[ids, att, tok], outputs=[x1_padded,x2_padded])\n    return model, padded_model","8f359594":"def jaccard(str1, str2): \n    a = set(str1.lower().split()) \n    b = set(str2.lower().split())\n    if (len(a)==0) & (len(b)==0): \n        return 0.5\n    c = a.intersection(b)\n    return float(len(c)) \/ (len(a) + len(b) - len(c))","d00e137e":"jac = []; VER='v0'; DISPLAY=1 # USE display=1 FOR INTERACTIVE\noof_start = np.zeros((input_ids.shape[0],MAX_LEN))\noof_end = np.zeros((input_ids.shape[0],MAX_LEN))\npreds_start = np.zeros((input_ids_t.shape[0],MAX_LEN))\npreds_end = np.zeros((input_ids_t.shape[0],MAX_LEN))\n\nskf = StratifiedKFold(n_splits=2,shuffle=True,random_state=SEED) #originally 5 splits\nfor fold,(idx_T,idx_V) in enumerate(skf.split(input_ids_train,train.sentiment.values)):\n    idxT = np.array([i for (a,b) in train.loc[idx_T, 'brackets'] for i in range(a, b)])\n    idxV = np.array([i for (a,b) in train.loc[idx_V, 'brackets'] for i in range(a, b)])\n    print('#'*25)\n    print('### FOLD %i'%(fold+1))\n    print('#'*25)\n    \n    K.clear_session()\n    model, padded_model = build_model()\n        \n    #sv = tf.keras.callbacks.ModelCheckpoint(\n    #    '%s-roberta-%i.h5'%(VER,fold), monitor='val_loss', verbose=1, save_best_only=True,\n    #    save_weights_only=True, mode='auto', save_freq='epoch')\n    inpT = [input_ids[idxT,], attention_mask[idxT,], token_type_ids[idxT,]]\n    targetT = [start_tokens[idxT,], end_tokens[idxT,]]\n    inpV = [input_ids[idxV,],attention_mask[idxV,],token_type_ids[idxV,]]\n    targetV = [start_tokens[idxV,], end_tokens[idxV,]]\n    # sort the validation data\n    shuffleV = np.int32(sorted(range(len(inpV[0])), key=lambda k: (inpV[0][k] == PAD_ID).sum(), reverse=True))\n    inpV = [arr[shuffleV] for arr in inpV]\n    targetV = [arr[shuffleV] for arr in targetV]\n    weight_fn = '%s-roberta-%i.h5'%(VER,fold)\n    for epoch in range(1, EPOCHS + 1):\n        # sort and shuffle: We add random numbers to not have the same order in each epoch\n        shuffleT = np.int32(sorted(range(len(inpT[0])), key=lambda k: (inpT[0][k] == PAD_ID).sum() + np.random.randint(-3, 3), reverse=True))\n        # shuffle in batches, otherwise short batches will always come in the beginning of each epoch\n        num_batches = math.ceil(len(shuffleT) \/ BATCH_SIZE)\n        batch_inds = np.random.permutation(num_batches)\n        shuffleT_ = []\n        for batch_ind in batch_inds:\n            shuffleT_.append(shuffleT[batch_ind * BATCH_SIZE: (batch_ind + 1) * BATCH_SIZE])\n        shuffleT = np.concatenate(shuffleT_)\n        # reorder the input data\n        inpT = [arr[shuffleT] for arr in inpT]\n        targetT = [arr[shuffleT] for arr in targetT]\n        model.fit(inpT, targetT, \n            epochs=epoch, initial_epoch=epoch - 1, batch_size=BATCH_SIZE, verbose=DISPLAY, callbacks=[],\n            validation_data=(inpV, targetV), shuffle=False)  # don't shuffle in `fit`\n        save_weights(model, weight_fn)\n\n    print('Loading model...')\n    # model.load_weights('%s-roberta-%i.h5'%(VER,fold))\n    load_weights(model, weight_fn)\n\n    print('Predicting OOF...')\n    oof_start[idxV,],oof_end[idxV,] = padded_model.predict([input_ids[idxV,],attention_mask[idxV,],token_type_ids[idxV,]],verbose=DISPLAY)\n    \n    print('Predicting Test...')\n    preds = padded_model.predict([input_ids_t,attention_mask_t,token_type_ids_t],verbose=DISPLAY)\n    preds_start += preds[0]\/skf.n_splits\n    preds_end += preds[1]\/skf.n_splits\n    \n    # DISPLAY FOLD JACCARD\n    alls = []\n    for k in idxV:\n        a = np.argmax(oof_start[k,])\n        b = np.argmax(oof_end[k,])\n        if a>b: \n            st = augmented_data.loc[k,'text'] # IMPROVE CV\/LB with better choice here\n        else:\n            text1 = \" \"+\" \".join(augmented_data.loc[k,'text'].split())\n            enc = tokenizer.encode(text1)\n            st = tokenizer.decode(enc.ids[a-2:b-1])\n        alls.append(jaccard(st,augmented_data.loc[k,'selected_text']))\n    jac.append(np.mean(alls))\n    print('>>>> FOLD %i Jaccard ='%(fold+1),np.mean(alls))\n    print()","1d2881b7":"print('>>>> OVERALL 3Fold CV Jaccard =',np.mean(jac))","77e3fb55":"print(jac) # Jaccard CVs","f47d5ff4":"alls = []\nfor k in range(input_ids_t.shape[0]):\n    a = np.argmax(preds_start[k,])\n    b = np.argmax(preds_end[k,])\n    if a>b: \n        st = test_augmented_data.loc[k,'text']\n    else:\n        text1 = \" \"+\" \".join(test_augmented_data.loc[k,'text'].split())\n        enc = tokenizer.encode(text1)\n        st = tokenizer.decode(enc.ids[a-2:b-1])\n    alls.append(st)\ntest_augmented_data['selected_text'] = alls","a5cab130":"test_augmented_data.head()","f34e9d50":"test_augmented_data = pd.read_csv('..\/input\/tweet-different-thresholds\/test_augmented_data.csv')","0ca0b40c":"test.brackets.values[-1][1]","a4dba95e":"len(test_augmented_data)","f2d833dc":"test_ = dc(test)\ndef get_submission(threshold):\n    test1 = dc(test_)\n    for ind, (a, b) in enumerate(test1.brackets.values.tolist()[0:5]):\n        df = test_augmented_data.loc[np.arange(a,b),['textID', 'text', 'selected_text']]\n        try:\n            selected_text = \" \".join(df.loc[df.textID.str.contains('new_')==False,'selected_text'].values[0].split())\n            text = \" \".join(df.loc[df.textID.str.contains('new_')==False,'text'].values[0].split())\n        except:\n            print(test1.loc[ind,:])\n            print(df)\n            break\n        print(f'Selected_text : {selected_text}, from text {text}')\n        text = text.lower().split(selected_text.lower())\n        print(f'When splitting :{text}')\n        text_before, text_after = text[0].split(), text[-1].split()\n        rows = df.loc[df.textID.str.contains('new_')]\n        words_dic = []\n        i = 0\n        for word in text_before:\n            print(f'In text_before, processing word {i} : {word}')\n            words_dic.append(word)\n            l, p = get_synonyms(word)\n            coeff = 0.5 if (word.lower() in stop or \"\".join(list(filter(lambda x: x not in punct, word))) in stop) else 1\n            rows[f'scoring_word_{i}'] = rows['selected_text'].apply(lambda x: coeff*np.average([w in x for w in l], weights=p))\n            i+=1\n        for word in selected_text.split():\n            print(f'In selected_text, processing word {i} : {word}')\n            words_dic.append(word)\n            l, p = get_synonyms(word)\n            coeff = 0.5 if (word.lower() in stop or \"\".join(list(filter(lambda x: x not in punct, word))) in stop) else 1\n            rows[f'scoring_word_{i}'] = rows['selected_text'].apply(lambda x: 2*coeff*np.average([w in x for w in l], weights=p))\n            i+=1\n        for word in text_after:\n            print(f'In text_after, processing word {i} : {word}')\n            words_dic.append(word)\n            l, p = get_synonyms(word)\n            coeff = 0.5 if (word.lower() in stop or \"\".join(list(filter(lambda x: x not in punct, word))) in stop) else 1\n            rows[f'scoring_word_{i}'] = rows['selected_text'].apply(lambda x: coeff*np.average([w in x for w in l], weights=p))\n            i+=1\n\n            \n        s = \" \".join([words_dic[int(f.split('_')[-1])] for f in (rows[[f'scoring_word_{j}' for j in range(i)]].mean(axis=0)>=threshold).index.tolist()])\n        test1.loc[ind, 'selected_text'] = \" \".join([words_dic[int(f.split('_')[-1])] for f in (rows[[f'scoring_word_{j}' for j in range(i)]].mean(axis=0)>=threshold).index.tolist()])\n\n    return test1\n\nget_submission(threshold=0.5)","b64be4ed":"test2 = get_submission(threshold=0.3)\ntest3 = get_submission(threshold = 1)","89f2e3c1":"test1.to_csv('submission_05.csv', index=False)\ntest2.to_csv('submission_03.csv', index=False)\ntest3.to_csv('submission_1.csv', index=False)","b2f1fc3a":"test_augmented_data.to_csv('test_augmented_data.csv', index=False)","aaf6b68a":"test[['textID','selected_text']].to_csv('submission.csv', index=False)","db6d6027":"pd.set_option('max_colwidth', 60)\ntest.sample(25)","8183b5a3":"# Build roBERTa Model\nWe use a pretrained roBERTa base model and add a custom question answer head. First tokens are input into `bert_model` and we use BERT's first output, i.e. `x[0]` below. These are embeddings of all input tokens and have shape `(batch_size, MAX_LEN, 768)`. Next we apply `tf.keras.layers.Conv1D(filters=1, kernel_size=1)` and transform the embeddings into shape `(batch_size, MAX_LEN, 1)`. We then flatten this and apply `softmax`, so our final output from `x1` has shape `(batch_size, MAX_LEN)`. These are one hot encodings of the start tokens indicies (for `selected_text`). And `x2` are the end tokens indicies.\n\n![bert.jpg](attachment:bert.jpg)","0ed1ddb5":"## Coherence in predictions : multivoting","3a3c65cf":"For swapping words, we just randomly select an index of word before the last, and swap it with it successor in the sentence. We also return a boolean which is False if we couldn't swap any words (only one word, or empty sentence).","547b562c":"We don't forget to save it :","b7a1c185":"# Training Data\nWe will now convert the training data into arrays that roBERTa understands. Here are example inputs and targets: \n![ids.jpg](attachment:ids.jpg)\nThe tokenization logic below is inspired by Abhishek's PyTorch notebook [here][1].\n\n[1]: https:\/\/www.kaggle.com\/abhishek\/roberta-inference-5-folds","79df4250":"# Train roBERTa Model\nWe train with 5 Stratified KFolds (based on sentiment stratification). Each fold, the best model weights are saved and then reloaded before oof prediction and test prediction. Therefore you can run this code offline and upload your 5 fold models to a private Kaggle dataset. Then run this notebook and comment out the line `model.fit()`. Instead your notebook will load your model weights from offline training in the line `model.load_weights()`. Update this to have the correct path. Also make sure you change the KFold seed below to match your offline training. Then this notebook will proceed to use your offline models to predict oof and predict test.","fac7ea9e":"# Test Data\nWe must tokenize the test data exactly the same as we tokenized the training data.","5dc1301a":"For each word of a sentence that isn't a stopword, we will randomly choose between itself and all his synonyms, in order to replace it in the modified sentence. The first function is for getting synonyms of a word, and probabilities of selecting each one of them when we will randomly rebuild a modified sentence.","b93885f6":"I have used two data augmentation methods, the easiest to implement : **synonym replacement** and **words swapping**. \n\nThe first one consist of replacing words by their synonyms, the second one to take two random consecutive words and swap their place. \n\nA lot of other techniques of NLP data augmentation exist, 2 other rather simple ones are random insertion and random deletion of words. The four of them are quite well described in [this article by Mael Fabien](https:\/\/maelfabien.github.io\/machinelearning\/NLP_8\/#data-augmentation-techniques), from whom I took inspiration. \n\nMore advanced techniques, such as text generation, deep contextualized embeddings, or back translation (translating to another language, then translating back to the original language often gives a slightly different sentence) are not considered in my work.","34286ce6":"# Kaggle Submission","0cfb0830":"We cannot do our transformations blinded. We must be careful of two points :\n- if we make a modification to the `selected_text` part, it must remain identical both in the `text` and `selected_text` ;\n- do not mix up uncarefully the selected and unselected parts of `text`.\n\nTherefore, the token `\"_________________________________\"` will be used to replace the `selected_text` in the `text` during modifications, so that we don't lose track.","609a27d1":"## You can find the enlarged train database (x6) here : \n### https:\/\/www.kaggle.com\/louise2001\/extended-train-for-tweet","312fd62f":"Now, we are ready to wrap this in a function which we will apply to all rows of our train data.","df254ad7":"We need to build the augmented test that we will use to cross-validate the test predictions:","75e83def":"Below is the code to create the augmented database, where you can change some parameters as you like. I published the result database with the default parameters [here](https:\/\/www.kaggle.com\/louise2001\/extended-train-for-tweet), please upvote it if you find it useful !","6fdc33ce":"Here, you can notice that we have a problem : some of the generated data is very long compared to the original data. This is problematic since in tweets, texts have limited length. What's more, that will become a big problem for our model where we don't want to put a gigantic `MAX_LEN` which will cost us a lot in computation time. Let's have a closer look :","7593d384":"All the tweets more than 150 characters long are generated ! We will drop those, since they will be a later problem.","d0763611":"# Metric","317e4b76":"# Please upvote this notebook and the corresponding dataset if they helped you or gave you new ideas !","e0fcc52a":"The model in this notebook is entirely based on the Roberta CNN notebook by [Wei Hao Khoong](https:\/\/www.kaggle.com\/khoongweihao) : for more information, see his original work [here](https:\/\/www.kaggle.com\/khoongweihao\/tse2020-roberta-cnn-random-seed-distribution). I thank him very much for his ready-to-use, well performing model. My add-on here is the data augmentation part.","912397ea":"Now we just have to apply it to the whole dataframe ! We don't forget to shuffle it afterwards, otherwise the batches will all have nearly identical rows.","e1456753":"As more and more examples are generated, we will need to generate `textID` values for them. \n\nI chose the following combination : `new` + a counter that we increment.\n\n`n_samples` is the number of sentences to generate via synonym replacement. Once we have all of them (`n_samples + 1`, counting the original), we double each of them by applying a word swapping, either inside the `selected_text` when possible (at least 2 words), or in the rest of the `text`.","b9e6ee7a":"# Data augmentation","71f83b0c":"# The Model","aedfca28":"# Acknowledgements","368a0778":"# Parametrization","9a04171a":"Now we just have to train the model with nearly 6 times as much data as initially !","9bb2cc9e":"try different submissions with different thresholds"}}