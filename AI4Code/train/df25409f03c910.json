{"cell_type":{"2713e935":"code","6b33499b":"code","43c79adb":"code","6788e607":"code","e1f3fbb8":"code","7e43275e":"code","c4d6c68e":"code","5a0a33f2":"code","4a9596ec":"code","a6c0e416":"code","f115d07c":"code","7f2275c4":"code","5f28527d":"code","2529d2f7":"code","3e0f7b0e":"code","08ba9baf":"code","e39f268f":"code","a4f77462":"code","7e27d8c6":"code","bc9a02cc":"code","c5c8cef9":"code","054abc27":"code","2ec0eb3e":"code","d8112520":"code","71b6fb3c":"code","153efb62":"code","0e7eaf59":"code","79a33697":"code","d90e1cf9":"code","a58317d6":"code","739628e0":"code","9342b50b":"code","540476c1":"code","89dcfe8c":"code","001e1c12":"code","625f7ed6":"code","61d9318a":"code","3c14e487":"code","15aa5984":"code","a9e3b44a":"code","2f132471":"code","9f23e1dd":"code","939d733c":"code","9fc45ed7":"code","8c580fb7":"code","4da2f367":"code","5d9cc257":"code","9d3a7b33":"code","72dc0927":"code","d99bef5d":"code","f184741d":"code","32c42774":"code","3c4585a4":"code","673e533c":"code","35f7a251":"code","7f028bf5":"code","1c2b315c":"code","942c7908":"code","6279fad0":"code","0efb572d":"code","fc8ceb54":"code","69437e61":"code","ee5d9bbf":"code","1467f376":"code","4af904dd":"code","44836f25":"code","a4ac84b2":"code","0f6d6bec":"code","ac22e738":"code","b8044347":"code","9e6f1950":"code","4d13f7e7":"code","cf795b53":"code","ee42f23d":"code","58db3183":"code","80c1f574":"code","9bffac1e":"code","3a3cbb47":"code","579faa2f":"code","69db7fde":"code","67e46455":"code","e9694707":"code","2c204f4d":"code","498dc5ea":"code","f56543b7":"code","1c17a0dc":"code","50342d52":"code","cb7007c6":"code","491a12c4":"code","1414d78f":"code","e41776ad":"code","d7084c2c":"code","13b7af13":"code","0c95015a":"code","b37004ec":"code","72a29e60":"code","b10f9afb":"code","1372b048":"code","8d1218a2":"code","364dfa3d":"code","94701ea0":"code","90a9f0ee":"code","27a297a8":"code","2a28317c":"code","dc2d8535":"code","5dd95cb1":"code","ef1aff8f":"code","38bf3246":"code","12d60c31":"code","3d1875c4":"code","b747afab":"code","c54c9b3e":"code","d72b23d6":"code","cf5db4b8":"code","292a797f":"markdown","29fff8fe":"markdown","f61990d3":"markdown","d3fa0b77":"markdown","f4627256":"markdown","771dd479":"markdown","4d0117d5":"markdown","cd439aaa":"markdown","a21877e7":"markdown","70c45f6f":"markdown","17b7cd49":"markdown","adc98ff3":"markdown","53d45294":"markdown","076bc81e":"markdown","780a84ed":"markdown","b54888c3":"markdown","a0ba1b96":"markdown","1e866299":"markdown","023d8b95":"markdown","1e457b90":"markdown","bd66a368":"markdown","66832da0":"markdown","d0290762":"markdown","e355a12a":"markdown","3f150987":"markdown","c41f4426":"markdown","8f6456f7":"markdown","6c9c27e2":"markdown","084fc40a":"markdown","367972f7":"markdown","d3628690":"markdown","5b72ca80":"markdown","5c67474c":"markdown","f8fdc778":"markdown","2d4744dc":"markdown","4304d4d7":"markdown","91a0845a":"markdown","8bb02a50":"markdown","4e90c815":"markdown","21de2c8f":"markdown","99976893":"markdown","4ca91841":"markdown","a2a21f2a":"markdown","aca31b06":"markdown","6aa736cf":"markdown","d947fdb0":"markdown","617f5298":"markdown","c056e30d":"markdown","be65863d":"markdown","72f0910e":"markdown","e26b9a20":"markdown","6de0f446":"markdown","84ad5340":"markdown","e4bc39e3":"markdown","a142c443":"markdown","9a9d448b":"markdown","3d7f8f43":"markdown","6d511154":"markdown","6a7b9cfc":"markdown","132808a9":"markdown","93887598":"markdown"},"source":{"2713e935":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns # data visualization library\nimport missingno\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# Preprocessing\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder, label_binarize\n\n# Machine learning\nimport catboost\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import model_selection, tree, preprocessing, metrics, linear_model\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import LinearRegression, LogisticRegression, SGDClassifier,  Perceptron\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom catboost import CatBoostClassifier, Pool, cv\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Ignore warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","6b33499b":"# Import data\n\ntrain=pd.read_csv('..\/input\/titanic\/train.csv',na_values='?')\ntest=pd.read_csv('..\/input\/titanic\/test.csv',na_values='?')\ngender=pd.read_csv('..\/input\/titanic\/gender_submission.csv')","43c79adb":"# View training data\ntrain.head()","6788e607":"# View test data\ntest.head()","e1f3fbb8":"# View gender data\n# How we want our data to appear for submission\ngender.head()","7e43275e":"# Shape of datasets\nprint(f\"Training dataset shape: {train.shape}\")\nprint(f\"Test dataset shape: {test.shape}\")","c4d6c68e":"# Dataset information\ntrain.info()\nprint('-'*40)\ntest.info()","5a0a33f2":"# Description of numeric data\ntrain.describe()","4a9596ec":"# Counts of missing data in train dataset\ntrain.isnull().sum()","a6c0e416":"# Counts of missing data in test dataset\ntest.isnull().sum()","f115d07c":"# Visual of missing data for train dataset\nmissingno.matrix(train, figsize = (30,10))\nplt.title('Trainning Missing Data',fontsize=30)","7f2275c4":"# Visual of missing data for test dataset\nmissingno.matrix(test, figsize = (30,10))\nplt.title('Testing Missing Data',fontsize=30)","5f28527d":"# Number of survivors\nfig = plt.figure(figsize=(20,1))\nsns.countplot(y='Survived', data=train);\nprint('Value Counts:')\nprint(train['Survived'].value_counts())","2529d2f7":"# View the data\nfig = plt.figure(figsize=(20,1))\nsns.countplot(y='Pclass', data=train);\nprint('Value Counts:')\nprint(train['Pclass'].value_counts())","3e0f7b0e":"# Relationship with survivability\ntrain[['Pclass','Survived']].groupby(['Pclass'], as_index=False).mean().sort_values(by='Survived',ascending=False)","08ba9baf":"# Visualize the relationship\nsns.barplot(x='Pclass',y='Survived',data=train)\nplt.title('Probablity of Survival by Class')\nplt.ylabel('Probablity of Surviving')","e39f268f":"x = train['Name'].nunique()\ny = len(train)\nprint(f'Number of unique names in the data {x}.\\nNumber of rows in the data {y}.')","a4f77462":"# View the data\nfig = plt.figure(figsize=(20,1))\nsns.countplot(y='Sex', data=train);\nprint('Value Counts:')\nprint(train['Sex'].value_counts())","7e27d8c6":"# View the relationship between Sex and Surviving\ntrain[['Sex','Survived']].groupby('Sex', as_index=False).mean().sort_values(by='Survived',ascending=False)","bc9a02cc":"# Visualization of the relationship\nsns.barplot(x='Sex',y='Survived',data=train)\nplt.title('Probablity of Survival by Sex')\nplt.ylabel('Probablity of Surviving')","c5c8cef9":"x = train['Age'].isnull().sum()\ny = len(train)\nz = ((x\/y)*100)\n\nprint(f'There are {x} missing age entries, out of {y} rows, accounting for {z}% of the data')","054abc27":"# View the distribution of the 'Age' field. \n# We won't bother with a pivot table at this point, as there are too many unique data points\nsns.distplot(train['Age'], label = 'Skewness: %.2f'%(train['Age'].skew()));\nplt.legend(loc = 'best')\nplt.title('Distribution of Passenger by Age')","2ec0eb3e":"# Relationship between age and surviving\nsns.kdeplot(train['Age'][train['Survived'] == 0], label = 'Did not survive')\nsns.kdeplot(train['Age'][train['Survived'] == 1], label = 'Survived')\nplt.xlabel('Age')\nplt.title('Passenger Age Distribution by Survival')\nplt.legend()","d8112520":"# Check for missing data\nprint(f\"Number of missing values is: {train['SibSp'].isnull().sum()}\")\n# Value counts of SibSp\nprint('Value Counts:')\ntrain['SibSp'].value_counts()","71b6fb3c":"# We can group the data by survived and number of siblings to help understand this data better\ntrain[['SibSp','Survived']].groupby(['SibSp'], as_index=False).mean().sort_values(by='Survived', ascending=False)","153efb62":"# Visualization of the relationship\nsns.barplot(x='SibSp',y='Survived',data=train)\nplt.title('Probablity of Survival by SibSp')\nplt.ylabel('Probablity of Surviving')","0e7eaf59":"# Check for missing data\nprint(f\"Number of missing values is: {train['Parch'].isnull().sum()}\")\n# Value counts of SibSp\nprint('Value Counts:')\ntrain['Parch'].value_counts()","79a33697":"# Check for missing data\nprint(f\"There are {train['Ticket'].isnull().sum()} missing values for the Ticket field.\")\n# Check for unique values\nprint(f\"There are {train['Ticket'].nunique()} unique enteries.\")","d90e1cf9":"train['Ticket'].value_counts().head(10)","a58317d6":"# Again check for missing data\nprint(f\"There are {train['Fare'].isnull().sum()} missing entries.\")\n# Check for number of unique enteries\nprint(f\"There are {train['Fare'].nunique()} unique enteries.\")","739628e0":"# View the distribution of the 'Age' field. \n# We won't bother with a pivot table at this point, as there are too many unique data points\nsns.distplot(train['Fare'], label = 'Skewness: %.2f'%(train['Fare'].skew()));\nplt.legend(loc = 'best')\nplt.title('Distribution of Passenger by Fare')","9342b50b":"# Relationship between Fare and surviving\nsns.kdeplot(train['Fare'][train['Survived'] == 0], label = 'Did not survive')\nsns.kdeplot(train['Fare'][train['Survived'] == 1], label = 'Survived')\nplt.xlabel('Fare')\nplt.title('Passenger Fare Distribution by Survival')\nplt.legend()","540476c1":"# Check for missing data\nprint(f\"There are {train['Cabin'].isnull().sum()} missing values.\")\n# Unique enteries\nprint(f\"There are {train['Cabin'].nunique()} unique values.\")\n# Percentage\nprint(f\"The missing data accounts for {(train['Cabin'].isnull().sum()\/len(train)*100)}% of values\")","89dcfe8c":"# View data\ntrain.head()","001e1c12":"# Check for missing data\nprint(f\"There are {train['Embarked'].isnull().sum()} missing values.\")\n# Check the distribution\nprint('Value Counts:')\ntrain['Embarked'].value_counts()","625f7ed6":"# Visualize\nsns.countplot(y='Embarked',data=train)\nplt.title('Passenger Count by Embarked Location')","61d9318a":"sns.barplot(x='Embarked',y='Survived',data=train)\nplt.title('Probablity of Survival by Embarkment Location')\nplt.ylabel('Probablity of Surviving')","3c14e487":"# Survival by gender and passenger class\ngender_class_sur = sns.factorplot(x = 'Pclass', y = 'Survived', hue = 'Sex', data = train, kind = 'bar')\ngender_class_sur.despine(left = True)\nplt.ylabel('Survival Probability')\nplt.title('Survival Probability by Sex and Passenger Class')","15aa5984":"sns.factorplot('Pclass', col = 'Embarked', data = train, kind = 'count')","a9e3b44a":"sns.heatmap(train[['Survived', 'SibSp', 'Parch', 'Age', 'Fare']].corr(), annot = True, fmt = '.2f')","2f132471":"# Drop both columns from both dataframe\ntrain = train.drop(['Cabin','Ticket'], axis=1)\ntest = test.drop(['Cabin','Ticket'], axis=1)","9f23e1dd":"print('Train missing data:')\nprint(train.isnull().sum())\nprint('-'*20)\nprint('Test missing data:')\nprint(test.isnull().sum())","939d733c":"# Train - Embarkment\n# Only the train dataset has missing data for this column, as previously mentioned we will replace it with the most common value\nmode = train['Embarked'].mode()[0]\nprint(f\"Replacing missing values with '{mode}'.\")\ntrain['Embarked'].fillna(mode, inplace = True)","9fc45ed7":"# Test - Fare\nmode = test['Fare'].mode()[0]\nprint(f\"Replacing missing value with '{mode}'.\")\ntest['Fare'].fillna(mode, inplace=True)","8c580fb7":"combined = pd.concat([train,test], axis=0).reset_index(drop=True)\ncombined.head()","4da2f367":"# Missing values for 'Age'\ncombined.isnull().sum()","5d9cc257":"sns.distplot(combined['Age'], label = 'Skewness: %.2f'%(combined['Age'].skew()));\nplt.legend(loc = 'best')\nplt.title('Distribution of Passenger by Age')","9d3a7b33":"# Missing data and unique entries\nprint(f\"There are {combined['Age'].isnull().sum()} missing data points.\")\nprint(f\"And there are {combined['Age'].nunique()} unique entries.\")","72dc0927":"# Create a variable to house the index number of missing age\nage_nan_indices = list(combined[combined['Age'].isnull()].index)\nlen(age_nan_indices)","d99bef5d":"# Loop through list and impute missing ages\n\nfor index in age_nan_indices:\n    median_age = combined['Age'].median()\n    predict_age = combined['Age'][(combined['SibSp'] == combined.iloc[index]['SibSp']) \n                                 & (combined['Parch'] == combined.iloc[index]['Parch'])\n                                 & (combined['Pclass'] == combined.iloc[index][\"Pclass\"])].median()\n    if np.isnan(predict_age):\n        combined['Age'].iloc[index] = median_age\n    else:\n        combined['Age'].iloc[index] = predict_age","f184741d":"# Confirm that all nan values have been replaced\ncombined['Age'].isnull().sum()","32c42774":"# View the distribution of 'Age' again\nsns.distplot(combined['Age'], label = 'Skewness: %.2f'%(combined['Age'].skew()));\nplt.legend(loc = 'best')\nplt.title('Distribution of Passenger by Age(After imputing missing data)')","3c4585a4":"combined['Sex'] = combined['Sex'].map({'male': 0, 'female': 1})","673e533c":"sns.factorplot(y = 'Age', x = 'Sex', hue = 'Pclass', kind = 'box', data = combined)\nplt.title('Distribution of Pclass by Sex')\nsns.factorplot(y = 'Age', x = 'Parch', kind = 'box', data = combined)\nplt.title('Distribution of Parch by Sex')\nsns.factorplot(y = 'Age', x = 'SibSp', kind = 'box', data = combined)\nplt.title('Distribution of SibSp by Sex')","35f7a251":"# Plot of correlation between numeric features and surviving\nsns.heatmap(combined.drop(['Survived', 'Name', 'PassengerId', 'Fare'], axis = 1).corr(), annot = True, cmap='viridis')","7f028bf5":"# Check our combined dataset again for any missing values\n# We see that survived is missing 418 entries, but that is alright as finding those values is the purpose of buiding this model\ncombined.isnull().sum()","1c2b315c":"# Create a new column housing the name prefix\ncombined['NamePrefix'] = train['Name'].apply(lambda x: x.split(',')[1].split('.')[0].strip())","942c7908":"# View new column\ncombined.head()","6279fad0":"# View value counts for newly created column\ncombined['NamePrefix'].value_counts()","0efb572d":"# Variables to consolidate NamePrefix\nMiss = ('Ms','Melle','Lady','Mme','Mlle')\nRare = ('Dr','Rev','Don','Jonkheer','Major','Col','Sir','the Countess','Capt')","fc8ceb54":"# Check number of unique nameprefix, perform replacement, and check unique numer again\nprint(f\"Number of unique NamePrefix {combined['NamePrefix'].nunique()}.\")\ncombined['NamePrefix'].replace(Miss,'Miss',inplace=True)\ncombined['NamePrefix'].replace(Rare,'Rare',inplace=True)\nprint(f\"New number of unique NamePrefix {combined['NamePrefix'].nunique()}.\")","69437e61":"# View the number of each nameprefix\ncombined['NamePrefix'].value_counts()","ee5d9bbf":"# View the relationship between Nameprefix and Surviving\nsns.barplot(x='NamePrefix',y='Survived',data=combined)\nplt.title('Probablity of Survival by NamePrefix')\nplt.ylabel('Probablity of Surviving')","1467f376":"# Drop the 'Name' feature from our dataset\ncombined.drop(labels='Name',axis=1,inplace=True)","4af904dd":"# View coombined dataset\ncombined.head()","44836f25":"# Calculate the family size\ncombined['FamilySize'] = combined['SibSp'] + combined['Parch'] + 1\ncombined[['SibSp', 'Parch', 'FamilySize']].head()","a4ac84b2":"# Create a feature if they are alone\ncombined['IsAlone'] = 0\ncombined.loc[combined['FamilySize'] == 1, 'IsAlone'] = 1","0f6d6bec":"# Create age bands and compute mean of survival by age bands\ncombined['AgeBin'] = pd.cut(combined['Age'], 5)\ncombined[['AgeBin', 'Survived']].groupby('AgeBin', as_index=False).mean().sort_values(by = 'AgeBin')","ac22e738":"# Now that we have the 'Age' feature in bins, we can assign ordinal values to each\ncombined.loc[combined['Age'] <= 16.136, 'Age'] = 0\ncombined.loc[(combined['Age'] > 16.136) & (combined['Age'] <= 32.102), 'Age'] = 1\ncombined.loc[(combined['Age'] > 32.102) & (combined['Age'] <= 48.068), 'Age'] = 2\ncombined.loc[(combined['Age'] > 48.068) & (combined['Age'] <= 64.034), 'Age'] = 3\ncombined.loc[combined['Age'] > 64.034 , 'Age'] = 4","b8044347":"# Convert ordinal 'Age' into integer for easier calculation\ncombined['Age'] = combined['Age'].astype('int')\ncombined['Age'].dtype","9e6f1950":"# Create the 'AgeClass' feature\ncombined['AgeClass'] = combined['Age'] * combined['Pclass']\ncombined.head()","4d13f7e7":"# Let's also remove some unnecessary features\ncombined.drop(labels=['SibSp','Parch','FamilySize','AgeBin'],axis=1,inplace=True)","cf795b53":"# View the data set\ncombined.head()","ee42f23d":"sns.distplot(combined['Fare'], label ='Skewness:%.2f'%(combined['Fare'].skew()))\nplt.legend(loc='best')\nplt.title('Distribution of Fare')","58db3183":"# Perform log transformation\ncombined['Fare'] = combined['Fare'].map(lambda x: np.log(x) if x > 0 else 0)","80c1f574":"# View data again\nsns.distplot(combined['Fare'], label ='Skewness:%.2f'%(combined['Fare'].skew()))\nplt.legend(loc='best')\nplt.title('Distribution of Fare (Post Log Transformation)')","9bffac1e":"# View data set\ncombined.head()","3a3cbb47":"# Convert 'Sex' to numeric\n#combined['Sex'] = combined['Sex'].map({'male': 0, 'female': 1})","579faa2f":"# Title and Embakred column\ncombined = pd.get_dummies(combined, columns = ['NamePrefix'])\ncombined = pd.get_dummies(combined, columns = ['Embarked'], prefix = 'Em')\ncombined.head()","69db7fde":"# Bin 'Fare' into 4 groups\ncombined['FareBand'] = pd.cut(combined['Fare'], 4)\ncombined[['FareBand', 'Survived']].groupby(['FareBand'], as_index=False).mean().sort_values(by = 'FareBand')","67e46455":"# Convert the 'Fare' feature into ordinals based on our 'FareBand'\ncombined.loc[combined['Fare'] <=1.56,'Fare'] = 0\ncombined.loc[(combined['Fare'] > 1.56)&(combined['Fare'] <=3.119),'Fare'] = 1\ncombined.loc[(combined['Fare'] > 3.119)&(combined['Fare'] <=4.679),'Fare'] = 2\ncombined.loc[combined['Fare'] >6.239,'Fare'] = 3\ncombined['Fare'] = combined['Fare'].astype(int)","e9694707":"# Remove 'FareBand' feature\ncombined.drop(labels='FareBand',axis=1,inplace=True)","2c204f4d":"combined.head()","498dc5ea":"# Separate training and test set from the combined dataframe\ntrain = combined[:len(train)]\ntest = combined[len(train):]","f56543b7":"train.head(2)","1c17a0dc":"test.head(2)","50342d52":"# Train\n# Drop pass id in train\n# convert survived to int\n\n# Test\n# Drop survived","cb7007c6":"# Train dataset\n# Drop 'PassengerId' column\n# Convert 'Survived' to int\ntrain.drop(labels='PassengerId',axis=1,inplace=True)\ntrain['Survived'] = train['Survived'].astype(int)","491a12c4":"# Test Dataset\n# Drop survived feature\ntest.drop(labels='Survived',axis=1,inplace=True)","1414d78f":"train.head()","e41776ad":"test.head()","d7084c2c":"# Split data\nY_train = train['Survived']\nX_train = train.drop(labels='Survived',axis=1)\nX_test = test.drop(labels='PassengerId',axis=1).copy()\nprint(f\"Y_train shape {Y_train.shape}\")\nprint(f\"X_train shape {X_train.shape}\")\nprint(f\"X_test shape {X_test.shape}\")","13b7af13":"# Logistic Regression model\nlogreg = LogisticRegression()\nlogreg.fit(X_train, Y_train)\nY_pred = logreg.predict(X_test)\nacc_log = round(logreg.score(X_train, Y_train) * 100, 2)\nprint(f\"Score: {acc_log}\")","0c95015a":"# SVM\nsvc = SVC()\nsvc.fit(X_train, Y_train)\nY_pred = svc.predict(X_test)\nacc_svc = round(svc.score(X_train, Y_train) * 100, 2)\nprint(f\"Score: {acc_svc}\")","b37004ec":"# KNN\nknn = KNeighborsClassifier(n_neighbors = 5)\nknn.fit(X_train, Y_train)\nY_pred = knn.predict(X_test)\nacc_knn = round(knn.score(X_train, Y_train) * 100, 2)\nprint(f\"Score: {acc_knn}\")","72a29e60":"# Gaussian \ngaussian = GaussianNB()\ngaussian.fit(X_train, Y_train)\nY_pred = gaussian.predict(X_test)\nacc_gaussian = round(gaussian.score(X_train, Y_train) * 100, 2)\nprint(f\"Score: {acc_gaussian}\")","b10f9afb":"# Perceptron\nperceptron = Perceptron()\nperceptron.fit(X_train, Y_train)\nY_pred = perceptron.predict(X_test)\nacc_perceptron = round(perceptron.score(X_train, Y_train) * 100, 2)\nprint(f\"Score: {acc_perceptron}\")","1372b048":"# Linear SVC\nlinear_svc = LinearSVC()\nlinear_svc.fit(X_train, Y_train)\nY_pred = linear_svc.predict(X_test)\nacc_linear_svc = round(linear_svc.score(X_train, Y_train) * 100, 2)\nacc_linear_svc","8d1218a2":"# Stochastic Gradient Descent\nsgd = SGDClassifier()\nsgd.fit(X_train, Y_train)\nY_pred = sgd.predict(X_test)\nacc_sgd = round(sgd.score(X_train, Y_train) * 100, 2)\nacc_sgd","364dfa3d":"# Decision Tree\ndecision_tree = DecisionTreeClassifier()\ndecision_tree.fit(X_train, Y_train)\nY_pred = decision_tree.predict(X_test)\nacc_decision_tree = round(decision_tree.score(X_train, Y_train) * 100, 2)\nprint(f\"Score: {acc_decision_tree}\")","94701ea0":"# Random Forest\nrandom_forest = RandomForestClassifier(n_estimators = 100)\nrandom_forest.fit(X_train, Y_train)\nY_pred = random_forest.predict(X_test)\nacc_random_forest = round(random_forest.score(X_train, Y_train) * 100, 2)\nacc_random_forest","90a9f0ee":"catboost = CatBoostClassifier()\ncatboost.fit(X_train, Y_train)\nY_pred = catboost.predict(X_test)\nacc_catboost = round(catboost.score(X_train, Y_train) * 100, 2)","27a297a8":"acc_catboost","2a28317c":"models = pd.DataFrame({'Model': ['Support Vector Machines', 'KNN', 'Logistic Regression', \n                                 'Random Forest', 'Naive Bayes', 'Perceptron', 'Stochastic Gradient Decent', \n                                 'Linear SVC', 'Decision Tree', 'CatBoost'],\n                       'Score': [acc_svc, acc_knn, acc_log, acc_random_forest, acc_gaussian, acc_perceptron,\n                                 acc_sgd, acc_linear_svc, acc_decision_tree, acc_catboost]})\n\nmodels.sort_values(by = 'Score', ascending = False, ignore_index = True)","dc2d8535":"# Create a list which contains classifiers \n\nclassifiers = []\nclassifiers.append(LogisticRegression())\nclassifiers.append(SVC())\nclassifiers.append(KNeighborsClassifier(n_neighbors = 5))\nclassifiers.append(GaussianNB())\nclassifiers.append(Perceptron())\nclassifiers.append(LinearSVC())\nclassifiers.append(SGDClassifier())\nclassifiers.append(DecisionTreeClassifier())\nclassifiers.append(RandomForestClassifier())\nclassifiers.append(CatBoostClassifier())\n\nlen(classifiers)","5dd95cb1":"# Create a list which contains cross validation results for each classifier\n\ncv_results = []\nfor classifier in classifiers:\n    cv_results.append(cross_val_score(classifier, X_train, Y_train, scoring = 'accuracy', cv = 10))","ef1aff8f":"# Mean and standard deviation of cross validation results for each classifier  \n\ncv_mean = []\ncv_std = []\nfor cv_result in cv_results:\n    cv_mean.append(cv_result.mean())\n    cv_std.append(cv_result.std())","38bf3246":"cv_res = pd.DataFrame({'Cross Validation Mean': cv_mean, 'Cross Validation Std': cv_std, 'Algorithm': ['Logistic Regression', 'Support Vector Machines', 'KNN', 'Gausian Naive Bayes', 'Perceptron', 'Linear SVC', 'Stochastic Gradient Descent', 'Decision Tree', 'Random Forest', 'CatBoost']})\ncv_res.sort_values(by = 'Cross Validation Mean', ascending = False, ignore_index = True)","12d60c31":"# Survival prediction by CatBoost\nY_pred","3d1875c4":"# confirm proper length\nlen(Y_pred)","b747afab":"# View gender dataset\ngender.head()","c54c9b3e":"# Create submission dataframe\n\nsubmit = pd.DataFrame({'PassengerId': test['PassengerId'], 'Survived': Y_pred})\nsubmit.head()","d72b23d6":"# Check shape\nsubmit.shape","cf5db4b8":"# Create and save csv file \n\nsubmit.to_csv(\"CB_titanic.csv\", index = False)","292a797f":"### Support Vector Machines","29fff8fe":"#### Feature - Fare\nPrice paid by the passenger for their ticket.","f61990d3":"# Titanic Prediction Model\n\nFirst crack at a machine learning model. Aiming to predicit which passengers will survive the Titanic disaster.","d3fa0b77":"This field is too messy for our use case. There is most likely a way to simplify this column, but for a first attempt at this competition we will not use it. Perhaps in later attempts it will be more usefull.","f4627256":"#### Observation\n- In the Titanic movie, when the ship is sinking we often hear \"women and children first\" the above plot and table really prove this point","771dd479":"### Correlations Between Numerical Features and Survivability","4d0117d5":"### Training Accuracy\nHow well has the model learned from the training set","cd439aaa":"#### Feature - Age\nThe age of the passenger","a21877e7":"#### Observation\n- While the majority of passengers boarded at Southampton, it was those who embarked at Cherbourg how had the best chance of survival\n- With only two missing entries for this feature, we will replace them with the most common embarkment location, Southampton","70c45f6f":"### Impute Missing Data","17b7cd49":"#### Feature - Ticket\nTicket number of the boarding passenger.","adc98ff3":"We can see her that those with one sibling of spouse had the highest chance of survival.","53d45294":"### Feature Analysis\nIn the case study of the Titanic, we are looking to see how different attributes effect a passengers chance for surviving. To achieve this we will go through each column in the dataset to better understand this relationship.","076bc81e":"### Embarkment by Class Type\nBy looking deeper into where a passenger boarded and their class we can further determine thier chance for survival","780a84ed":"### Passenger Class and Sex\nFrom the common knowledge of the event, and from film, we know that saving women and children's lives' was prioritized. To better understand just which women and children were saved we can further examine the relationhip between gender, class, and survivors ","b54888c3":"## Model Evaluation","a0ba1b96":"#### Feature - Sex\nThe sex of the passenger (male or female)","1e866299":"## Feature Engineering\nThe purpose of feature engineering is to create new columns from exisiting data","023d8b95":"#### Observation\n- After visualizing the correlation the numeric columns have with survivng, it become clear that the 'Fare' feature has the stongest correlation. This comes as no suprise as we have seen above, those in higher classes had a much higher chance of survival","1e457b90":"Observing our distribution of the Fare feature we can see that it has a strong right skew. In order to adress this issue we are going to perform a log transformation. This will help to normalize the data.","bd66a368":"## Split Datasets\nNow that we have our data in the proper format we can split up or data fro our training and testing purposes.\n\nTo being, we will first split our data into indpeendent variables (predictor variables), represented by X, and dependent variables represented by Y.\n\nFor our example, Y_train is the survived column from our train dataset, and X-train is the other columns. Our models will learn to classify survival(Y_train) based on all X_train and make predictions on X_test.","66832da0":"### Random Forest","d0290762":"#### Observation\n- By combining these two features we can see the effect that being first or second class, as well as being female had on your survival rate ","e355a12a":"#### Feature - Cabin\nCabin number the passenger stayed in.","3f150987":"### Drop Features\nAs we have previously noted, the Cabin feature and Ticket have either too many missing data points or in their current state are too complicated for our analysis ","c41f4426":"### CatBoost","8f6456f7":"#### Feature - SibSp\nThe number of siblings or spouses the passenger has onboard.\n","6c9c27e2":"## Prep data for Kaggle submission","084fc40a":"#### Feature - IsAlone\n- Create a simplified feature indicating if a passenger was travelling with family or alone","367972f7":"### Decision Tree","d3628690":"#### Data Descriptions\nTo better understand what is meant by each column heading\n- Survival: 0 = No, 1 = Yes\n\n- Pclass(Ticket class): 1 = First, 2 = Second, 3 = Third\n\n- Sex: Gender of the passenger\n\n- Age: Age in years\n\n- Sibsp: number of siblings\/spouses aboard the Titanic\n\n- Parch: number of parents\/children aboard the Titanic\n\n- Ticket: Ticket number\n\n- Fare: Price paid for ticket\n\n- Cabin: Cabin number\n\n- Embarked: Port of Embarkment, C = Cherbourg, Q = Queenstown, S = Southampton","5b72ca80":"### K-Nearest Neighbours","5c67474c":"## Exploratory Data Analysis\nThe process of visualizing and analyzing data to extract insights.","f8fdc778":"#### Target Feature - Survived\nWhether the passenger survived or not. This is the variable we want our model to predict.\n\nKey: 0 = perished, 1 = survived","2d4744dc":"### Stochastic Gradient Descent","4304d4d7":"#### Feature - Embarked\nWhere the passenger boarded the Titanic\n\nKey: C = Cherbourg, Q = Queenstown, S = Southampton","91a0845a":"#### Observation\n- The 'Fare' column has a high degree of skewness. In order for this to be usable in our model we will have to perform a log transformation on the data\n- Additionally, in order to add this to our model we will want to bin the data","8bb02a50":"#### Feature - Pclass\nThe ticket class of the passenger.\n\n- Key: 1 = 1st, 2 = 2nd, 3 = 3rd","4e90c815":"## Overview\n\nExploratory Data Analysis\n- Descriptive statistics, missing data, and column info type\n- Feature Analysis\n  - Relationships between features and target variable, surviving the Titanic\n\nData Cleansing - Cleaning any fields, and fitting them to the training and testing dataframes\n\nFeature Engineering\n\nData Preprocessing\n\nModel Building","21de2c8f":"#### Feature - Name","99976893":"#### Observation \n- As we can see form the second plot, our data is more evenly distributed and we have reduced the skewness","4ca91841":"From the above we can see that every entry in the Name field is a unique name. Catagorizing this will be harder. However later on we can create a new field, with the the name prefix (Mr., Mrs., Miss., etc...)","a2a21f2a":"#### Observation\n- With 77% of the data missing, using this feature in our model will be problematic. While there may exist a method to impute the missing data here, for a first run we will omit this field","aca31b06":"## Modeling\nNow that our data is processed and split into training and testing sets we can begin constructing our models.\n\nThe Titanic problem is a classification example, we will be using classification models from scikit-learn. The following are the models we will use:\n- Logistic Regression\n- Support Vector Machines\n- K-Nearest Neighbours\n- Gaussian naive bayes\n- Perceptron\n- Linear SVC\n- Stochastic gradient descent\n- Decision tree\n- Random forest\n- CatBoost\n\nThis process requires 3 simple steps: \n- Instantiate the model, \n- Fit the model to the training set \n- Predict the data in test set.","6aa736cf":"### Data Types, Summary Statistics, and Missing data","d947fdb0":"### Data Transformation","617f5298":"#### Feature - Parch\nThe number of parents or children the passenger has on the Titanic.\n\nThis feature is very similar in nature to the SibSp, so we will handle it the same.","c056e30d":"#### Feature - AgeClass\n- As we have seen above those passengers who were wealthy and older had a better chance of survival. We want to make sure this comes across in our model, so we will create a feature multiplying age by class\n- Before we can create this feature, we must bin the values for 'Age' this will tranform the data into ordinal values","be65863d":"### Load in Datasets","72f0910e":"#### Observation\n- As we can see from the table and plot above, those in better clases (ie. first class) had a higher chance of survival","e26b9a20":"### Linear SVC","6de0f446":"#### Combine Datasets\nAt this point we can combine our train and test datasets. In doing so we will be able to impute the missing values for the 'Age' feature more accurately.","84ad5340":"#### Feature - Name","e4bc39e3":"### Gaussian naive bayes","a142c443":"### Logistic Regression","9a9d448b":"Based on this plot we can see that the cabin column has mostly missing values, making it very hard to work with.\n\nOur age column is also missing some data, but we have a few optins to impute some data here to fill this column out. Additionally, Embarked column has 2 missing values, which is an easy fix.","3d7f8f43":"## Summary\nBest Test Score: 0.8219 \n\nTest Name: CatBoost\n\nKaggle score 0.74162","6d511154":"### Perceptron","6a7b9cfc":"## Relationships Between Multiple Features and Surviving\n\nWhen looking at the data we have found out some key indicators of passengers who survived. We are also able to combine some of these features to help get a fuller picture of those who survived.","132808a9":"## Data Preprocessing\nNow that we have completed the initial data exploration, we can start to prepare the features for our model. To accomplish this we will perfom a few tasks:\n- Drop features\n- Replace missing values\n- Data transformation","93887598":"## Feature Encoding\nFor machine learning models to work all input and output variables must be numeric. Through the process of feature encoding, we will convert necessary columns to numeric."}}