{"cell_type":{"0355a3e3":"code","f1ac8596":"code","19f857e6":"code","1bb8c427":"code","018cad3d":"code","aae545f2":"code","b20bcc4e":"code","6e18fa7b":"code","ea828fa0":"code","61558162":"code","3685ae61":"code","72b1e12d":"code","fe956c46":"code","e4ef8482":"code","94376a81":"code","94a1eaa0":"code","b710b538":"code","276da1c6":"code","60c1d2ce":"code","b77e0e73":"code","e6ed65a1":"code","19c3ff00":"code","ff3775ad":"code","62f96502":"code","adadd30b":"code","19506e17":"code","bc187689":"code","1e5c8496":"code","bbaf1b5f":"code","d49648b5":"code","a4acddf1":"code","311ba610":"code","ce56dd7a":"code","872883f1":"code","e3525087":"code","55a0e5c6":"code","916bddb8":"code","ce8af098":"code","102d72b6":"code","2cf4fa67":"code","71f0efb6":"code","6340d0e0":"code","40f72128":"code","1f0b47e8":"code","f8c6079e":"code","ae373205":"code","abe35e38":"code","b93d6dda":"code","b6dd5bc0":"code","15fa6932":"code","0bb6b7ea":"code","44576f49":"code","30a41907":"code","f756270d":"code","a54afb15":"code","b73f26ef":"code","f9cce54f":"code","b835f155":"code","48f7274a":"code","db48d665":"code","b5d7cb5e":"code","5507126c":"code","5f3de4b2":"code","820d1807":"code","9d99ab69":"code","7cfb5e8f":"code","c8b93973":"code","d065ab61":"code","78938ac0":"code","f355f744":"code","cb8a0100":"code","5c70ecc6":"code","7b5a3b89":"code","d38f5c22":"code","2ec58139":"markdown","d6414155":"markdown","5f90d9a0":"markdown","22e6d10c":"markdown","20fb9a0c":"markdown","ef976a31":"markdown","1ec46da7":"markdown","6cecdc26":"markdown","3ddcc49f":"markdown","0ac2a6d4":"markdown","6b3bda61":"markdown","430fdacf":"markdown","af944923":"markdown","8fbddeee":"markdown","09408e27":"markdown","1f3f8a85":"markdown","9695e4d9":"markdown","8b1dc408":"markdown","804e0a79":"markdown","a9b7fb76":"markdown","18b80f39":"markdown"},"source":{"0355a3e3":"!nvidia-smi","f1ac8596":"# \u041e\u0431\u043d\u043e\u0432\u043b\u0435\u043d\u0438\u0435 tensorflow\n!pip install tensorflow --upgrade\n# \u0417\u0430\u0433\u0440\u0443\u0436\u0430\u0435\u043c \u043e\u0431\u0432\u044f\u0437\u043a\u0443 \u043f\u043e\u0434 keras \u0434\u043b\u044f \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043d\u0438\u044f \u043f\u0440\u043e\u0434\u0432\u0438\u043d\u0443\u0442\u044b\u0445 \u0431\u0438\u0431\u043b\u0438\u043e\u0442\u0435\u043a \u0430\u0443\u0433\u043c\u0435\u043d\u0442\u0430\u0446\u0438\u0438\n!pip install git+https:\/\/github.com\/mjkvaak\/ImageDataAugmentor","19f857e6":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n#import pickle\nimport zipfile\nimport csv\nimport sys\nimport os\nimport gc\n\nimport tensorflow as tf\nimport tensorflow.keras as keras\nfrom tensorflow.keras.preprocessing import image\n#from tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\nfrom tensorflow.keras.callbacks import Callback, EarlyStopping\nfrom tensorflow.keras.regularizers import l2\nfrom tensorflow.keras import optimizers\nimport tensorflow.keras.models as Model\nimport tensorflow.keras.layers as Layer\n\n#from sklearn.model_selection import train_test_split, StratifiedKFold\n\nimport PIL\nfrom PIL import ImageOps, ImageFilter\n#\u0443\u0432\u0435\u043b\u0438\u0447\u0438\u043c \u0434\u0435\u0444\u043e\u043b\u0442\u043d\u044b\u0439 \u0440\u0430\u0437\u043c\u0435\u0440 \u0433\u0440\u0430\u0444\u0438\u043a\u043e\u0432\nfrom pylab import rcParams\nrcParams['figure.figsize'] = 10, 5\n#\u0433\u0440\u0430\u0444\u0438\u043a\u0438 \u0432 svg \u0432\u044b\u0433\u043b\u044f\u0434\u044f\u0442 \u0431\u043e\u043b\u0435\u0435 \u0447\u0435\u0442\u043a\u0438\u043c\u0438\n%config InlineBackend.figure_format = 'svg' \n%matplotlib inline\n\nprint(os.listdir(\"..\/input\"))\nprint('Python       :', sys.version.split('\\n')[0])\nprint('Numpy        :', np.__version__)\nprint('Tensorflow   :', tf.__version__)\nprint('Keras        :', tf.keras.__version__)","1bb8c427":"# \u0441\u043e\u0445\u0440\u0430\u043d\u044f\u0435\u043c \u0432\u0435\u0440\u0441\u0438\u0438 \u043f\u0430\u043a\u0435\u0442\u043e\u0432\n!pip freeze > requirements.txt","018cad3d":"# \u0412 setup \u0432\u044b\u043d\u043e\u0441\u0438\u043c \u043e\u0441\u043d\u043e\u0432\u043d\u044b\u0435 \u043d\u0430\u0441\u0442\u0440\u043e\u0439\u043a\u0438: \u0442\u0430\u043a \u0443\u0434\u043e\u0431\u043d\u0435\u0435 \u0438\u0445 \u043f\u0435\u0440\u0435\u0431\u0438\u0440\u0430\u0442\u044c \u0432 \u0434\u0430\u043b\u044c\u043d\u0435\u0439\u0448\u0435\u043c.\n\nRANDOM_SEED          = 42\n\nEPOCHS               = 8  # \u044d\u043f\u043e\u0445 \u043d\u0430 \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u0435\nBATCH_SIZE           = 8 # \u0443\u043c\u0435\u043d\u044c\u0448\u0430\u0435\u043c batch \u0435\u0441\u043b\u0438 \u0441\u0435\u0442\u044c \u0431\u043e\u043b\u044c\u0448\u0430\u044f, \u0438\u043d\u0430\u0447\u0435 \u043d\u0435 \u043f\u043e\u043c\u0435\u0441\u0442\u0438\u0442\u0441\u044f \u0432 \u043f\u0430\u043c\u044f\u0442\u044c \u043d\u0430 GPU\nLR                   = 1e-3\nVAL_SPLIT            = 0.15 # \u0441\u043a\u043e\u043b\u044c\u043a\u043e \u0434\u0430\u043d\u043d\u044b\u0445 \u0432\u044b\u0434\u0435\u043b\u044f\u0435\u043c \u043d\u0430 \u0442\u0435\u0441\u0442 = 15%\n\nCLASS_NUM            = 10  # \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u043a\u043b\u0430\u0441\u0441\u043e\u0432 \u0432 \u043d\u0430\u0448\u0435\u0439 \u0437\u0430\u0434\u0430\u0447\u0435\nIMG_SIZE             = 224 # \u043a\u0430\u043a\u043e\u0433\u043e \u0440\u0430\u0437\u043c\u0435\u0440\u0430 \u043f\u043e\u0434\u0430\u0435\u043c \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u044f \u0432 \u0441\u0435\u0442\u044c\nIMG_CHANNELS         = 3   # \u0443 RGB 3 \u043a\u0430\u043d\u0430\u043b\u0430\ninput_shape          = (IMG_SIZE, IMG_SIZE, IMG_CHANNELS)\n\nDATA_PATH = '..\/input\/sfcarclassif\/'\nPATH = \"..\/working\/\" # \u0440\u0430\u0431\u043e\u0447\u0430\u044f \u0434\u0438\u0440\u0435\u043a\u0442\u043e\u0440\u0438\u044f","aae545f2":"train_df = pd.read_csv(DATA_PATH+\"train.csv\")\nsample_submission = pd.read_csv(DATA_PATH+\"sample-submission.csv\")\ntrain_df.head()","b20bcc4e":"train_df.info()","6e18fa7b":"train_df.Category.value_counts()","ea828fa0":"train_df['Category'].nunique()","61558162":"# \u0418\u0441\u0445\u043e\u0434\u043d\u044b\u0435 \u0434\u0430\u043d\u043d\u044b\u0435 \u043c\u043e\u0436\u043d\u043e \u0440\u0430\u0441\u043f\u0430\u043a\u043e\u0432\u0430\u0442\u044c \u0432 output, \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u044f \u043d\u0438\u0436\u0435 \u043f\u0440\u0438\u0432\u0435\u0434\u0435\u043d\u043d\u044b\u0439 \u043a\u043e\u0434\n\n#print('\u0420\u0430\u0441\u043f\u0430\u043a\u043e\u0432\u044b\u0432\u0430\u0435\u043c \u043a\u0430\u0440\u0442\u0438\u043d\u043a\u0438')\n\n#for data_zip in ['train.zip', 'test.zip']:\n#    with zipfile.ZipFile('..\/'+data_zip,\"r\") as z:\n#        z.extractall(PATH)\n        \n#print(os.listdir(PATH))\n\n# \u041e\u0434\u043d\u0430\u043a\u043e \u043f\u0440\u0438 \u0442\u0430\u043a\u043e\u0439 \u0440\u0430\u0441\u043f\u0430\u043a\u043e\u0432\u043a\u0435 \u0432\u043e\u0437\u043d\u0438\u043a\u0430\u0435\u0442 \u043f\u0440\u043e\u0431\u043b\u0435\u043c\u0430 \u0441\u043e\u0445\u0440\u0430\u043d\u0435\u043d\u0438\u044f \u0444\u0430\u0439\u043b\u0430 submission\n# \u041f\u043e\u044d\u0442\u043e\u043c\u0443 \u0440\u0430\u0441\u043f\u0430\u043a\u043e\u0432\u0430\u043d\u043d\u044b\u0435 \u0438\u0441\u0445\u043e\u0434\u043d\u044b\u0435 \u0434\u0430\u043d\u043d\u044b\u0435 \u0434\u043e\u0431\u0430\u0432\u0438\u043b\u0438 \u0438\u0437 \u0431\u0430\u0437\u044b \u043a\u0430\u0433\u0433\u043b\u0430","3685ae61":"print('\u041f\u0440\u0438\u043c\u0435\u0440 \u043a\u0430\u0440\u0442\u0438\u043d\u043e\u043a')\nplt.figure(figsize=(12,8))\n\nrandom_image = train_df.sample(n=9)\nrandom_image_paths = random_image['Id'].values\nrandom_image_cat = random_image['Category'].values\n\nfor index, path in enumerate(random_image_paths):\n    im = PIL.Image.open(DATA_PATH+f'train\/train\/{random_image_cat[index]}\/{path}')\n    plt.subplot(3,3, index+1)\n    plt.imshow(im)\n    plt.title('Class: '+str(random_image_cat[index]))\n    plt.axis('off')\nplt.show()","72b1e12d":"#\u041f\u043e\u0441\u043c\u043e\u0442\u0440\u0438\u043c \u043d\u0430 \u043f\u0440\u0438\u043c\u0435\u0440\u044b \u043a\u0430\u0440\u0442\u0438\u043d\u043e\u043a \u0438 \u0438\u0445 \u0440\u0430\u0437\u043c\u0435\u0440\u044b, \u0447\u0442\u043e\u0431\u044b \u043f\u043e\u043d\u0438\u043c\u0430\u0442\u044c, \u043a\u0430\u043a \u0438\u0445 \u043b\u0443\u0447\u0448\u0435 \u043e\u0431\u0440\u0430\u0431\u0430\u0442\u044b\u0432\u0430\u0442\u044c \u0438 \u0441\u0436\u0438\u043c\u0430\u0442\u044c.\nimage = PIL.Image.open(DATA_PATH+f'train\/train\/0\/100380.jpg')\nimgplot = plt.imshow(image)\nplt.show()\nimage.size","fe956c46":"#delete df no longer needed\ndel train_df\n#collect residual garbage\ngc.collect()","e4ef8482":"from ImageDataAugmentor.image_data_augmentor import *\nimport albumentations as A","94376a81":"AUGMENTATIONS = A.Compose([\n    A.GaussianBlur(p=0.05),\n    A.RandomBrightness(limit=0.2, p=0.5),\n    A.ShiftScaleRotate(shift_limit=0.0625, \n                       scale_limit=0.01, \n                       interpolation=1, \n                       border_mode=4, \n                       rotate_limit=20, \n                       p=.75),\n    A.OneOf([\n        A.CenterCrop(height=224, width=200),\n        A.CenterCrop(height=200, width=224)],\n        p=0.5),\n    A.OneOf([\n        A.RandomBrightnessContrast(brightness_limit=0.3, \n                                                contrast_limit=0.3),\n        A.RandomBrightnessContrast(brightness_limit=0.1, \n                                                contrast_limit=0.1)],\n        p=0.5),\n    A.HorizontalFlip(p=0.5),\n    A.HueSaturationValue(p=0.5),\n    A.RGBShift(p=0.5),\n    A.FancyPCA(alpha=0.1, \n               always_apply=False, \n               p=0.5),\n    A.Resize(IMG_SIZE, IMG_SIZE)\n])","94a1eaa0":"train_gen = ImageDataAugmentor(rescale=1.\/255,\n                        augment=AUGMENTATIONS, \n                        seed=RANDOM_SEED,\n                        validation_split=VAL_SPLIT\n                       )\n\ntrain_datagen = train_gen.flow_from_directory(DATA_PATH+'train\/train', \n                                            class_mode='categorical', \n                                            batch_size=BATCH_SIZE, \n                                            target_size=(IMG_SIZE, IMG_SIZE),\n                                            shuffle=True,\n                                            subset='training'\n                                           )\ntest_datagen = train_gen.flow_from_directory(DATA_PATH+'train\/train', \n                                             class_mode='categorical', \n                                             batch_size=BATCH_SIZE, \n                                             target_size=(IMG_SIZE, IMG_SIZE),\n                                             shuffle=True,\n                                             subset='validation'\n                                            )","b710b538":"train_datagen.show_data(rows=3, cols=5)","276da1c6":"# \u0417\u0430\u0433\u0440\u0443\u0437\u043a\u0430 \u043c\u043e\u0434\u0435\u043b\u0438 efficientnet\n!pip install -q efficientnet","60c1d2ce":"#from tensorflow.keras.applications.xception import Xception\n#from tensorflow.keras.applications.inception_v3 import InceptionV3\nimport efficientnet.keras as efn ","b77e0e73":"# Pre-trained model\n\n#base_model = Xception(weights='imagenet', \n#                      include_top=False, \n#                      input_shape = input_shape)\n\n#base_model = InceptionV3(weights='imagenet', \n#                         include_top=False, \n#                         input_shape = input_shape)\n\nbase_model = efn.EfficientNetB5(weights='imagenet', \n                                include_top=False, \n                                input_shape = input_shape)","e6ed65a1":"base_model.summary()","19c3ff00":"# freeze the pre-trained model weights, train only the top layers\nbase_model.trainable = False","ff3775ad":"model=Model.Sequential()\nmodel.add(base_model)\nmodel.add(Layer.GlobalAveragePooling2D())\nmodel.add(Layer.Dense(256, \n                      activation='relu', \n                      bias_regularizer=l2(1e-4),\n                      activity_regularizer=l2(1e-5)))\nmodel.add(Layer.BatchNormalization())\nmodel.add(Layer.Dropout(0.25))\nmodel.add(Layer.Dense(CLASS_NUM, activation='softmax'))","62f96502":"model.summary()","adadd30b":"# Check the trainable status of the individual layers\nfor layer in model.layers:\n    print(layer, layer.trainable)","19506e17":"model.compile(loss=\"categorical_crossentropy\", \n              optimizer=optimizers.Adam(lr=LR), \n              metrics=[\"accuracy\"])","bc187689":"# \u0414\u043e\u0431\u0430\u0432\u0438\u043c ModelCheckpoint. \n# \u042d\u0442\u0430 \u0444\u0443\u043d\u043a\u0446\u0438\u044f \u043f\u043e\u0437\u0432\u043e\u043b\u044f\u0435\u0442 \u0441\u043e\u0445\u0440\u0430\u043d\u044f\u0442\u044c \u043f\u0440\u043e\u0433\u0440\u0435\u0441\u0441 \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f \u043c\u043e\u0434\u0435\u043b\u0438, \n# \u0447\u0442\u043e\u0431\u044b \u0432 \u043d\u0443\u0436\u043d\u044b\u0439 \u043c\u043e\u043c\u0435\u043d\u0442 \u043c\u043e\u0436\u043d\u043e \u0431\u044b\u043b\u043e \u0435\u0433\u043e \u043f\u043e\u0434\u0433\u0440\u0443\u0437\u0438\u0442\u044c \u0438 \u0434\u043e\u043e\u0431\u0443\u0447\u0438\u0442\u044c \u043c\u043e\u0434\u0435\u043b\u044c.\ncheckpoint = ModelCheckpoint('best_model.hdf5', \n                             monitor = ['val_accuracy'], \n                             verbose = 1,\n                             mode = 'max')\nearlystop = EarlyStopping(monitor = 'val_accuracy',\n                          patience = 4,\n                          restore_best_weights = True)\nreduce_lr = ReduceLROnPlateau(monitor='val_loss',\n                              factor=0.25,\n                              patience=2,\n                              min_lr=0.0000001,\n                              verbose=1,\n                              mode='auto')\ncallbacks_list = [checkpoint, earlystop, reduce_lr]","1e5c8496":"history = model.fit(\n        train_datagen,\n        steps_per_epoch = train_datagen.samples\/\/train_datagen.batch_size,\n        validation_data = test_datagen, \n        validation_steps = test_datagen.samples\/\/test_datagen.batch_size,\n        epochs = EPOCHS,\n        callbacks = callbacks_list\n)","bbaf1b5f":"#model.save('..\/working\/model_last.hdf5') \nmodel.load_weights('best_model.hdf5')","d49648b5":"scores = model.evaluate(test_datagen, verbose=1)\nprint(\"Accuracy: %.2f%%\" % (scores[1]*100))","a4acddf1":"def plot_history(history):\n    plt.figure(figsize=(10,5))\n    #plt.style.use('dark_background')\n    acc = history.history['accuracy'] \n    val_acc = history.history['val_accuracy']\n    loss = history.history['loss']\n    val_loss = history.history['val_loss']\n\n    epochs = range(len(acc))\n\n    plt.plot(epochs, acc, 'b', label='Training acc')\n    plt.plot(epochs, val_acc, 'g', label='Validation acc')\n    plt.title('Training and validation accuracy')\n    plt.legend()\n\n    #plt.figure()\n    plt.figure(figsize=(10,5))\n    #plt.style.use('dark_background')\n    plt.plot(epochs, loss, 'b', label='Training loss')\n    plt.plot(epochs, val_loss, 'g', label='Validation loss')\n    plt.title('Training and validation loss')\n    plt.legend()\n\n    plt.show()\n\nplot_history(history)","311ba610":"base_model.trainable = True\n\n# Fine-tune from this layer onwards\nfine_tune_at = len(base_model.layers)\/\/2\n\n# Freeze all the layers before the `fine_tune_at` layer\nfor layer in base_model.layers[:fine_tune_at]:\n    layer.trainable =  False","ce56dd7a":"LR=0.0001\nmodel.compile(loss=\"categorical_crossentropy\", \n              optimizer=optimizers.Adam(lr=LR), \n              metrics=[\"accuracy\"])","872883f1":"model.summary()","e3525087":"history = model.fit(\n        train_datagen,\n        steps_per_epoch = train_datagen.samples\/\/train_datagen.batch_size,\n        validation_data = test_datagen, \n        validation_steps = test_datagen.samples\/\/test_datagen.batch_size,\n        epochs = EPOCHS,\n        callbacks = callbacks_list\n)","55a0e5c6":"#model.save('..\/working\/model_step2.hdf5')\nmodel.load_weights('best_model.hdf5') ","916bddb8":"scores = model.evaluate(test_datagen, verbose=1)\nprint(\"Accuracy: %.2f%%\" % (scores[1]*100))","ce8af098":"plot_history(history)","102d72b6":"base_model.trainable = True\n# Fine-tune from this layer onwards\nfine_tune_at = len(base_model.layers)\/\/4\n\n# Freeze all the layers before the `fine_tune_at` layer\nfor layer in base_model.layers[:fine_tune_at]:\n    layer.trainable =  False","2cf4fa67":"LR=0.00001\n\nmodel.compile(loss=\"categorical_crossentropy\", \n              optimizer=optimizers.Adam(lr=LR), \n              metrics=[\"accuracy\"])","71f0efb6":"model.summary()","6340d0e0":"history = model.fit(\n        train_datagen,\n        steps_per_epoch = train_datagen.samples\/\/train_datagen.batch_size,\n        validation_data = test_datagen, \n        validation_steps = test_datagen.samples\/\/test_datagen.batch_size,\n        epochs = EPOCHS,\n        callbacks = callbacks_list\n)","40f72128":"#model.save('..\/working\/model_step3.hdf5')\nmodel.load_weights('best_model.hdf5')","1f0b47e8":"scores = model.evaluate(test_datagen, verbose=1)\nprint(\"Accuracy: %.2f%%\" % (scores[1]*100))","f8c6079e":"plot_history(history)","ae373205":"base_model.trainable = True\nLR=0.000001\n#EPOCHS = 10\n\nmodel.compile(loss=\"categorical_crossentropy\", \n              optimizer=optimizers.Adam(lr=LR), \n              metrics=[\"accuracy\"])","abe35e38":"history = model.fit(\n        train_datagen,\n        steps_per_epoch = train_datagen.samples\/\/train_datagen.batch_size,\n        validation_data = test_datagen, \n        validation_steps = test_datagen.samples\/\/test_datagen.batch_size,\n        epochs = EPOCHS,\n        callbacks = callbacks_list\n)","b93d6dda":"#model.save('..\/working\/model_step4.hdf5') \nmodel.load_weights('best_model.hdf5') ","b6dd5bc0":"scores = model.evaluate(test_datagen, verbose=1)\nprint(\"Accuracy: %.2f%%\" % (scores[1]*100))","15fa6932":"plot_history(history)","0bb6b7ea":"test_gen = ImageDataAugmentor(rescale=1.\/255)\ntest_sub_generator = test_gen.flow_from_dataframe(dataframe=sample_submission,\n                                            directory=DATA_PATH+'test\/test_upload\/',\n                                            x_col=\"Id\",\n                                            y_col=None,\n                                            shuffle=False,\n                                            class_mode=None,\n                                            target_size=(IMG_SIZE, IMG_SIZE),\n                                            batch_size=BATCH_SIZE)","44576f49":"test_sub_generator.reset()\npredictions = model.predict(test_sub_generator, \n                                      steps=len(test_sub_generator), \n                                      verbose=1) \npredictions = np.argmax(predictions, axis=-1) #multiple categories\nlabel_map = (train_datagen.class_indices)\nlabel_map = dict((v,k) for k,v in label_map.items()) #flip k,v\npredictions = [label_map[k] for k in predictions]","30a41907":"filenames_with_dir=test_sub_generator.filenames\nsubmission = pd.DataFrame({'Id':filenames_with_dir, 'Category':predictions}, \n                          columns=['Id', 'Category'])\nsubmission['Id'] = submission['Id'].replace('test_upload\/','')\n\nsubmission.to_csv('submission.csv', index=False)","f756270d":"EPOCHS               = 6\nBATCH_SIZE           = 2 \nLR                   = 1e-5\n\nIMG_SIZE             = 512\nIMG_CHANNELS         = 3\ninput_shape          = (IMG_SIZE, IMG_SIZE, IMG_CHANNELS)","a54afb15":"AUGMENTATIONS = A.Compose([\n    #A.RandomBrightness(limit=0.2, p=0.5),\n    A.ShiftScaleRotate(shift_limit=0.0625, \n                       scale_limit=0.01, \n                       interpolation=1, \n                       border_mode=4, \n                       rotate_limit=20, \n                       p=.75),\n    A.HorizontalFlip(p=0.5),\n    #A.HueSaturationValue(p=0.5)\n])","b73f26ef":"train_gen = ImageDataAugmentor(rescale=1.\/255,\n                        augment=AUGMENTATIONS, \n                        seed=RANDOM_SEED,\n                        validation_split=VAL_SPLIT\n                       )\n\ntrain_datagen = train_gen.flow_from_directory(DATA_PATH+'train\/train', \n                                            class_mode='categorical', \n                                            batch_size=BATCH_SIZE, \n                                            target_size=(IMG_SIZE, IMG_SIZE),\n                                            shuffle=True,\n                                            subset='training'\n                                           )\ntest_datagen = train_gen.flow_from_directory(DATA_PATH+'train\/train', \n                                             class_mode='categorical', \n                                             batch_size=BATCH_SIZE, \n                                             target_size=(IMG_SIZE, IMG_SIZE),\n                                             shuffle=True,\n                                             subset='validation'\n                                            )\n\ntest_gen = ImageDataAugmentor(rescale=1.\/255)\ntest_sub_generator = test_gen.flow_from_dataframe(dataframe=sample_submission,\n                                            directory=DATA_PATH+'test\/test_upload\/',\n                                            x_col=\"Id\",\n                                            y_col=None,\n                                            shuffle=False,\n                                            class_mode=None,\n                                            target_size=(IMG_SIZE, IMG_SIZE),\n                                            batch_size=BATCH_SIZE)","f9cce54f":"# Re-create the network with the new size of the input data\n\n#base_model = Xception(weights='imagenet', \n#                            include_top=False, \n#                            input_shape = input_shape)\n\n#base_model = InceptionV3(weights='imagenet', \n#                            include_top=False, \n#                            input_shape = input_shape)\n\n\nbase_model = efn.EfficientNetB5(weights='imagenet', \n                            include_top=False, \n                            input_shape = input_shape)","b835f155":"base_model.trainable = True","48f7274a":"model.compile(loss=\"categorical_crossentropy\", \n              optimizer=optimizers.Adam(lr=LR), \n              metrics=[\"accuracy\"])\nmodel.load_weights('best_model.hdf5')","db48d665":"history = model.fit(\n        train_datagen,\n        steps_per_epoch = train_datagen.samples\/\/train_datagen.batch_size,\n        validation_data = test_datagen, \n        validation_steps = test_datagen.samples\/\/test_datagen.batch_size,\n        epochs = EPOCHS,\n        callbacks = callbacks_list\n)","b5d7cb5e":"#!find \/kaggle\/working -name \"*.hdf5\" -type f -delete","5507126c":"scores = model.evaluate(test_datagen, verbose=1)\nprint(\"Accuracy: %.2f%%\" % (scores[1]*100))","5f3de4b2":"plot_history(history)","820d1807":"test_sub_generator.reset()\npredictions = model.predict(test_sub_generator, steps=len(test_sub_generator), verbose=1) \npredictions = np.argmax(predictions, axis=-1) #multiple categories\nlabel_map = (train_datagen.class_indices)\nlabel_map = dict((v,k) for k,v in label_map.items()) #flip k,v\npredictions = [label_map[k] for k in predictions]","9d99ab69":"filenames_with_dir=test_sub_generator.filenames\nsubmission = pd.DataFrame({'Id':filenames_with_dir, 'Category':predictions}, columns=['Id', 'Category'])\nsubmission['Id'] = submission['Id'].replace('test_upload\/','')","7cfb5e8f":"submission.to_csv('submission_IMG_SIZE.csv', index=False)","c8b93973":"model.load_weights('best_model.hdf5')","d065ab61":"AUGMENTATIONS = A.Compose([\n    A.GaussianBlur(p=0.05),\n    A.RandomBrightness(limit=0.2, p=0.5),\n    A.ShiftScaleRotate(shift_limit=0.0625, \n                       scale_limit=0.01, \n                       interpolation=1, \n                       border_mode=4, \n                       rotate_limit=20, \n                       p=.75),\n    A.OneOf([\n        A.RandomBrightnessContrast(brightness_limit=0.3, \n                                                contrast_limit=0.3),\n        A.RandomBrightnessContrast(brightness_limit=0.1, \n                                                contrast_limit=0.1)],\n        p=0.5),\n    A.HorizontalFlip(p=0.5),\n    A.HueSaturationValue(p=0.5),\n    A.RGBShift(p=0.5),\n    A.FancyPCA(alpha=0.1, \n               always_apply=False, \n               p=0.5),\n    A.Resize(IMG_SIZE, IMG_SIZE)\n])","78938ac0":"test_gen = ImageDataAugmentor(rescale=1.\/255,\n                        augment=AUGMENTATIONS, \n                        seed=RANDOM_SEED,\n                        validation_split=VAL_SPLIT\n                       )\n\ntest_sub_generator = test_gen.flow_from_dataframe(dataframe=sample_submission,\n                                      directory=DATA_PATH+'test\/test_upload',\n                                      x_col=\"Id\",\n                                      y_col=None,\n                                      target_size=(IMG_SIZE, IMG_SIZE),\n                                      batch_size=BATCH_SIZE,\n                                      class_mode=None,\n                                      shuffle=False)","f355f744":"tta_steps = 10\npredictions = []\n\nfor i in range(tta_steps):\n    preds = model.predict(test_sub_generator, verbose=1) \n    predictions.append(preds)\n\npred = np.mean(predictions, axis=0)","cb8a0100":"predictions = np.argmax(pred, axis=-1) #multiple categories\nlabel_map = (train_datagen.class_indices)\nlabel_map = dict((v,k) for k,v in label_map.items()) #flip k,v\npredictions = [label_map[k] for k in predictions]\nfilenames_with_dir=test_sub_generator.filenames\nsubmission = pd.DataFrame({'Id':filenames_with_dir, 'Category':predictions}, \n                          columns=['Id', 'Category'])\n\nsubmission['Id'] = submission['Id'].replace('test_upload\/','')","5c70ecc6":"scores = model.evaluate(test_datagen, verbose=1)\nprint(\"Accuracy: %.2f%%\" % (scores[1]*100))","7b5a3b89":"submission.to_csv('submission_TTA.csv', index=False)","d38f5c22":"#\u0423\u0434\u0430\u043b\u0438\u0442\u044c \u043f\u0430\u043f\u043a\u0443 \u0441 \u0440\u0430\u0441\u043f\u0430\u043a\u043e\u0432\u0430\u043d\u043d\u044b\u043c\u0438 \u043a\u0430\u0440\u0442\u0438\u043d\u043a\u0430\u043c\u0438\n#import shutil\n#shutil.rmtree(PATH)","2ec58139":"# Prediction after IMG_SIZE rise","d6414155":"# Fit","5f90d9a0":"## Step 4\n### complete weights defrosting","22e6d10c":"\u0414\u0430\u043d\u043d\u044b\u0435 \u0440\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u044b \u0440\u0430\u0432\u043d\u043e\u043c\u0435\u0440\u043d\u043e","20fb9a0c":"## Step 3\n### defrost three quarters of the pre-trained model weights","ef976a31":"# Test Time Augmentation (TTA)","1ec46da7":"## Step 5. IMG_SIZE rise","6cecdc26":"# \u0413\u0435\u043d\u0435\u0440\u0430\u0446\u0438\u044f \u0434\u0430\u043d\u043d\u044b\u0445","3ddcc49f":"\u041a\u0430\u0440\u0442\u0438\u043d\u043a\u0438 \u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u0435\u043d\u044b \u0440\u0430\u0437\u043d\u043e\u0433\u043e \u0440\u0430\u0437\u043c\u0435\u0440\u0430","0ac2a6d4":"# \u0410\u0443\u0433\u043c\u0435\u043d\u0442\u0430\u0446\u0438\u044f \u0434\u0430\u043d\u043d\u044b\u0445","6b3bda61":"\u0412 \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u0435 \u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u0435\u043d\u043e 10 \u043c\u0430\u0440\u043e\u043a \u043c\u0430\u0448\u0438\u043d","430fdacf":"Download pre-trained model","af944923":"## Step 2\n### defrost a half of the pre-trained model weights","8fbddeee":"# Model efficiency summary          \n         \nFine-tuning **Xception**         \n0% weights defrosting accuracy 59.25%      \n50% weights defrosting accuracy 91.88%        \n75% weights defrosting accuracy 93.99%        \n100% weights defrosting accuracy 93.99%       \nimage size rising accuracy 96.82%         \nTTA accuracy **96.52%**       \n          \nFine-tuning **InceptionV3**       \n0% weights defrosting accuracy 61.31%      \n50% weights defrosting accuracy 90.55%        \n75% weights defrosting accuracy 93.86%        \n100% weights defrosting accuracy 93.69%       \nimage size rising accuracy 94.68%         \nTTA accuracy **%**          \n\nFine-tuning **EfficientNetB5**       \n0% weights defrosting accuracy 73,55%      \n50% weights defrosting accuracy 93.69%        \n75% weights defrosting accuracy 94.72%        \n100% weights defrosting accuracy 95.02%       \nimage size rising accuracy 97.25%         \nTTA accuracy **%**     \n          \n","09408e27":"# \u041c\u043e\u0434\u0435\u043b\u044c","1f3f8a85":"\u041d\u0435\u0442 \u043f\u0440\u043e\u043f\u0443\u0441\u043a\u043e\u0432.","9695e4d9":"# Setup","8b1dc408":"Augment test images.         \nMake several predictions of one picture in a different way.         \n=> The final prediction is obtained by taking the average of several predictions.","804e0a79":"# EDA","a9b7fb76":"# \u0418\u0442\u043e\u0433\u0438             \n                    \n\u0412 \u043f\u0440\u043e\u0435\u043a\u0442\u0435 \u043f\u0440\u0438\u043c\u0435\u043d\u0435\u043d\u044b:     \n- transfer learning \u0438 fine-tuning (\u043e\u0431\u0443\u0447\u0435\u043d\u0438\u0435 \u0433\u043e\u043b\u043e\u0432\u044b -> 50% \u0440\u0430\u0437\u043c\u043e\u0440\u043e\u0437\u043a\u0430 \u0432\u0435\u0441\u043e\u0432 \u043f\u0440\u0435\u0434\u043e\u0431\u0443\u0447\u0435\u043d\u043d\u043e\u0439 \u043c\u043e\u0434\u0435\u043b\u0438 -> 75% \u0440\u0430\u0437\u043c\u043e\u0440\u043e\u0437\u043a\u0430 -> 100% \u0440\u0430\u0437\u043c\u043e\u0440\u043e\u0437\u043a\u0430)\n- \u0434\u043e\u043f\u043e\u043b\u043d\u0438\u0442\u0435\u043b\u044c\u043d\u044b\u0435 \u0444\u0443\u043d\u043a\u0446\u0438\u0438 callback \u0432 Keras        \n- \u043d\u0430\u0441\u0442\u0440\u043e\u0439\u043a\u0430 LR    \n- \u043d\u0430\u0441\u0442\u0440\u043e\u0439\u043a\u0430 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u043e\u0432 \u0440\u0435\u0433\u0443\u043b\u044f\u0440\u0438\u0437\u0430\u0446\u0438\u0438 \u043f\u043e\u043b\u043d\u043e\u0441\u0432\u044f\u0437\u043d\u043e\u0433\u043e \u0441\u043b\u043e\u044f \u043d\u0435\u0439\u0440\u043e\u043d\u043d\u043e\u0439 \u0441\u0435\u0442\u0438\n- \u043f\u0440\u0438\u043c\u0435\u043d\u0435\u043d \u0441\u043f\u043e\u0441\u043e\u0431 \u0437\u0430\u043f\u043e\u043b\u043d\u0435\u043d\u0438\u044f \u043f\u0440\u043e\u043f\u0443\u0441\u043a\u043e\u0432 c \u043f\u043e\u043c\u043e\u0449\u044c\u044e ImageDataAugmentor \u0441 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043d\u0438\u0435\u043c \u0431\u0438\u0431\u043b\u0438\u043e\u0442\u0435\u043a\u0438 \u0430\u0443\u0433\u043c\u0435\u043d\u0442\u0430\u0446\u0438\u0438 \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u0439 albumentations\n- \u043f\u043e\u0434\u043e\u0431\u0440\u0430\u043d\u044b \u043f\u0435\u0440\u0435\u043c\u0435\u043d\u043d\u044b\u0435 (\u0440\u0430\u0437\u043c\u0435\u0440 \u043a\u0430\u0440\u0442\u0438\u043d\u043a\u0438, \u0431\u0430\u0442\u0447, \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u044d\u043f\u043e\u0445)\n- \u0434\u043e\u0431\u0430\u0432\u043b\u0435\u043d\u0430 Batch Normalization \u0432 \u0430\u0440\u0445\u0438\u0442\u0435\u043a\u0442\u0443\u0440\u0435 \u201c\u0433\u043e\u043b\u043e\u0432\u044b\u201d \u043c\u043e\u0434\u0435\u043b\u0438\n- SOTA \u0430\u0440\u0445\u0438\u0442\u0435\u043a\u0442\u0443\u0440\u0430 \u0441\u0435\u0442\u0435\u0439 - Xception, InceptionV3, EfficientNetB5\n- \u0434\u043e\u0431\u0430\u0432\u043b\u0435\u043d\u0430 TTA (Test Time Augmentation)\n                  \n\u0412\u0432\u0438\u0434\u0443 \u043e\u0433\u0440\u0430\u043d\u0438\u0447\u0435\u043d\u0438\u0439 \u043d\u0430 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043d\u0438\u0435 \u0440\u0435\u0441\u0443\u0440\u0441\u0430\u043c\u0438 GPU \u0438 \u0432\u0440\u0435\u043c\u044f\u0437\u0430\u0442\u0440\u0430\u0442\u043d\u043e\u0441\u0442\u0438 \u0441\u0430\u043c\u043e\u0433\u043e \u043f\u0440\u043e\u0441\u0447\u0435\u0442\u0430 \u043c\u043e\u0434\u0435\u043b\u0438, \u043d\u0435 \u0443\u0434\u0430\u043b\u043e\u0441\u044c \u043f\u0440\u043e\u0442\u0435\u0441\u0442\u0438\u0440\u043e\u0432\u0430\u0442\u044c \u0441\u043b\u0435\u0434\u0443\u044e\u0449\u0438\u0435 \u0442\u0435\u0445\u043d\u0438\u043a\u0438 \u043d\u0430\u0441\u0442\u0440\u043e\u0439\u043a\u0438 \u043c\u043e\u0434\u0435\u043b\u0438:\n- \u043d\u0430\u0441\u0442\u0440\u043e\u0439\u043a\u0430 optimizer\n- \u043d\u0430\u0441\u0442\u0440\u043e\u0439\u043a\u0430 loss\n- \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043d\u0438\u0435 \u0432\u043d\u0435\u0448\u043d\u0438\u0445 \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u043e\u0432 \u0434\u043b\u044f \u0434\u043e\u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f \u043c\u043e\u0434\u0435\u043b\u0438\n               ","18b80f39":"# Prediction after fine-tuning"}}