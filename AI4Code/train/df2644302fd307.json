{"cell_type":{"a8fd1cb4":"code","1fa85dad":"code","1c8ed0e8":"code","5c5414cb":"code","9172079f":"code","4c1bcf1a":"code","23f656bc":"code","47b510e1":"code","cf347de2":"code","ef5ab8f7":"code","9fe9430d":"code","fb8e014f":"code","8a9d30bc":"code","55e2b1ba":"code","34b12747":"code","fa3c3d7a":"code","3e388c79":"code","10706d38":"code","98358193":"code","0b983881":"code","fd6a24d7":"code","63ea0ab1":"code","8498a07b":"code","427c8e20":"code","327c082a":"code","95a0d70e":"code","440220a5":"code","13dac162":"code","d0d91404":"code","64a35dc0":"code","669b2aa4":"code","6d7af15c":"markdown","89674d87":"markdown","5ba37d0b":"markdown","7402a683":"markdown","4f88c1a2":"markdown","d632bc2b":"markdown"},"source":{"a8fd1cb4":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","1fa85dad":"%matplotlib inline\n\ntrain = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\n\ntrain['train_test'] = 1\ntest['train_test'] = 0\ntest['Survived'] = np.NaN\nall_data = pd.concat([train,test])\n\nall_data.columns","1c8ed0e8":"#checking the shape of the data\nall_data.shape","5c5414cb":"#to understand the overall information avialable in the data we call the dscribe() func but with \"include all\" parameter to iclude numeric as well as char values \nall_data.describe(include='all')","9172079f":"#checking the null\/empty values in data\nall_data.isnull().sum()","4c1bcf1a":"#number of male\/female survived\/Not-survived\ngender_mode = pd.crosstab(train['Survived'],train['Sex'])\ngender_mode","23f656bc":"#Number of null and unique values in attributes across the data along with their data-types. \ntemp = pd.DataFrame(index=all_data.columns)\ntemp['data_type'] = all_data.dtypes\ntemp['null_count'] = all_data.isnull().sum()\ntemp['unique_count'] = all_data.nunique()\ntemp","47b510e1":"#normal train data discription\ntrain.describe()","cf347de2":"#checking the null\/empty values in Training data\ntrain.isnull().sum()","ef5ab8f7":"train.describe().columns","9fe9430d":"#seperating numerical and categorical variables\nnumr = train[['Age','SibSp','Parch','Fare']]\ncatr = train[['Survived','Pclass','Sex','Ticket','Cabin','Embarked']]","fb8e014f":"import matplotlib.pyplot as plt\n\n#Numerical attributes distibuted through histogram\n\nfor i in numr.columns:\n    plt.hist(numr[i])\n    plt.title(i)\n    plt.show()","8a9d30bc":"import seaborn as sns\n\n#Categorical attributes distibuted through histogram\n\nfor i in catr.columns:\n    sns.barplot(catr[i].value_counts().index,catr[i].value_counts()).set_title(i)\n    plt.show()","55e2b1ba":"import seaborn as sns\n\nprint(numr.corr())\nsns.heatmap(numr.corr())\n","34b12747":"#taking a look at survival of people falling in different categories(categorial-variables) \nprint(pd.pivot_table(train, index = 'Survived', columns = 'Pclass', values = 'Ticket' ,aggfunc ='count'))\nprint()\nprint(pd.pivot_table(train, index = 'Survived', columns = 'Sex', values = 'Ticket' ,aggfunc ='count'))\nprint()\nprint(pd.pivot_table(train, index = 'Survived', columns = 'Embarked', values = 'Ticket' ,aggfunc ='count'))","fa3c3d7a":"#feature engineering on titles like Mr,Mrs, Dr etc. \ntrain.Name.head(50)\ntrain['name_title'] = train.Name.apply(lambda x: x.split(',')[1].split('.')[0].strip())","3e388c79":"#Now extracting it;\ntrain['name_title'].value_counts()","10706d38":"#creating all categorical variables that I did before, for both training and test datasets here;\nall_data['cabin_multiple'] = all_data.Cabin.apply(lambda x: 0 if pd.isna(x) else len(x.split(' ')))\nall_data['cabin_adv'] = all_data.Cabin.apply(lambda x: str(x)[0])\nall_data['numeric_ticket'] = all_data.Ticket.apply(lambda x: 1 if x.isnumeric() else 0)\nall_data['ticket_letters'] = all_data.Ticket.apply(lambda x: ''.join(x.split(' ')[:-1]).replace('.','').replace('\/','').lower() if len(x.split(' ')[:-1]) >0 else 0)\nall_data['name_title'] = all_data.Name.apply(lambda x: x.split(',')[1].split('.')[0].strip())\n","98358193":"#impute nulls for continuous data \n\nall_data.Age = all_data.Age.fillna(train.Age.median())\nall_data.Fare = all_data.Fare.fillna(train.Fare.median())\n\n#drop null 'embarked' rows. Only 2 instances of this in training and 0 in test \nall_data.dropna(subset=['Embarked'],inplace = True)","0b983881":"all_data['norm_sibsp'] = np.log(all_data.SibSp+1)\nall_data['norm_sibsp'].hist()\n\nall_data['norm_fare'] = np.log(all_data.Fare+1)\nall_data['norm_fare'].hist()","fd6a24d7":"all_data.Pclass = all_data.Pclass.astype(str)\n\n#created dummy variables from categories (also can use OneHotEncoder)\nall_dummies = pd.get_dummies(all_data[['Pclass','Sex','Age','SibSp','Parch','norm_fare','Embarked','cabin_adv','cabin_multiple','numeric_ticket','name_title','train_test']])\n\n#Split to train test again\nX_train = all_dummies[all_dummies.train_test == 1].drop(['train_test'], axis =1)\nX_test = all_dummies[all_dummies.train_test == 0].drop(['train_test'], axis =1)\n\n\ny_train = all_data[all_data.train_test==1].Survived\ny_train.shape","63ea0ab1":"from sklearn.preprocessing import StandardScaler\n\n#Scaling the data\n\nscale = StandardScaler()\nall_dummies_scaled = all_dummies.copy()\nall_dummies_scaled[['Age','SibSp','Parch','norm_fare']]= scale.fit_transform(all_dummies_scaled[['Age','SibSp','Parch','norm_fare']])\nall_dummies_scaled\n\nX_train_scaled = all_dummies_scaled[all_dummies_scaled.train_test == 1].drop(['train_test'], axis =1)\nX_test_scaled = all_dummies_scaled[all_dummies_scaled.train_test == 0].drop(['train_test'], axis =1)\n\ny_train = all_data[all_data.train_test==1].Survived","8498a07b":"from sklearn.model_selection import cross_val_score\nfrom sklearn.naive_bayes import GaussianNB\n\n#Naive Bayes\n\ngnb = GaussianNB()\ncv = cross_val_score(gnb,X_train_scaled,y_train,cv=5)\nprint(cv)\nprint(cv.mean())","427c8e20":"#Logistic Regression\n\nfrom sklearn.linear_model import LogisticRegression\n\nlr = LogisticRegression(max_iter = 2000)\ncv = cross_val_score(lr,X_train_scaled,y_train,cv=5)\nprint(cv)\nprint(cv.mean()) ","327c082a":"#K-Nearest-Neighbors(KNN)\n\nfrom sklearn.neighbors import KNeighborsClassifier\n\nknn = KNeighborsClassifier()\ncv = cross_val_score(knn,X_train_scaled,y_train,cv=5)\nprint(cv)\nprint(cv.mean())","95a0d70e":"#Decision Tree\n\nfrom sklearn import tree\n\ndt = tree.DecisionTreeClassifier(random_state = 1)\ncv = cross_val_score(dt,X_train_scaled,y_train,cv=5)\nprint(cv)\nprint(cv.mean())","440220a5":"#Random Forest\n\nfrom sklearn.ensemble import RandomForestClassifier\n\nrf = RandomForestClassifier(random_state = 1)\ncv = cross_val_score(rf,X_train_scaled,y_train,cv=5)\nprint(cv)\nprint(cv.mean())","13dac162":"#Support Vector Classifier \n\nfrom sklearn.svm import SVC\n\nsvc = SVC(probability = True)\ncv = cross_val_score(svc,X_train_scaled,y_train,cv=5)\nprint(cv)\nprint(cv.mean())","d0d91404":"#\"soft\" classifier averages the confidence of each of the models. If a the average confidence is > 50% that it is a 1 it will be counted as such\n\nfrom sklearn.ensemble import VotingClassifier\nvoting_clf = VotingClassifier(estimators = [('lr',lr),('knn',knn),('rf',rf),('gnb',gnb),('svc',svc)], voting = 'soft') ","64a35dc0":"cv = cross_val_score(voting_clf,X_train_scaled,y_train,cv=5)\nprint(cv)\nprint(cv.mean())","669b2aa4":"#Submiting predictions\n\nvoting_clf.fit(X_train_scaled,y_train)\ny_hat_base_vc = voting_clf.predict(X_test_scaled).astype(int)\nbasic_submission = {'PassengerId': test.PassengerId, 'Survived': y_hat_base_vc}\nprediction = pd.DataFrame(data=basic_submission)\nprediction.to_csv('prediction.csv', index =False)","6d7af15c":"## Removing Anomalies\n * Training data is having some missing values in attributes like Age, Cabin and embarked \n * We will be be either removing them or filling them with mean\/median values","89674d87":"# Data Exploration","5ba37d0b":"# Data Preprocessing\n\n#### I will be using it for preparing the data for different upcoming models;\n\n* Droping the null values(eg;Embarked)\n\n* Filling missing values with mean\/median values\n\n* Scaling the data betwenn 1-0","7402a683":"I have imported the data. For this analysis, I will work with the Training set. I will be validating based on data from the training dataset and For final submission,I will make predictions based on the test dataset as per the rules.","4f88c1a2":"# Building Models","d632bc2b":"# Titanic Project Prediction\nIn this notebook I have predicted weather the passengers on the Titanic will survive or not, which is a Classification problem, using the data(train, test) available from the project"}}