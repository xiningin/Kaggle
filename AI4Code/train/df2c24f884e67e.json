{"cell_type":{"e1572232":"code","c042b31f":"code","c21e14ee":"code","1f881f92":"code","b7bd44aa":"code","8b31e7af":"code","69efedb5":"code","05ee50a5":"code","9a5d1f52":"code","bf3cf6fb":"code","83f7e6c5":"code","2a75c4c1":"code","9d6c0080":"code","bd95b3d3":"code","0742cb2f":"code","344d1813":"code","365a8365":"code","cd2272c2":"code","1e668cf9":"code","5e776b07":"code","45907b00":"code","d39b166a":"code","db01af82":"code","772f1382":"code","ee4339de":"code","27571d84":"code","a9a21bae":"code","883b9d12":"code","bb60c8fa":"code","d8659a11":"code","10eb663c":"code","6ad59117":"code","e4b40730":"code","5706a756":"code","429f9cb6":"code","0c3d7506":"code","0633b4d0":"code","6bbbe9a5":"code","7d6d9cdd":"code","45b30c17":"code","2f73a2b1":"code","ebb277d9":"markdown","3c6dcb90":"markdown","4b2e80ba":"markdown","32496e81":"markdown","fad820ac":"markdown","3b10cfb3":"markdown","fa60fb2f":"markdown","01078722":"markdown","a7850963":"markdown","5152bb7a":"markdown","db775139":"markdown","a0116bf5":"markdown","c5a6b909":"markdown","400a1ee8":"markdown","ac9b666b":"markdown","90d24457":"markdown","3e801013":"markdown","11188b57":"markdown","fb55bf8f":"markdown","deddbcec":"markdown"},"source":{"e1572232":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","c042b31f":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport warnings\nwarnings.filterwarnings('ignore')\nplt.rcParams['figure.figsize'] = (10.0, 8.0)\nimport seaborn as sns","c21e14ee":"df = pd.read_csv('..\/input\/telco-customer-churn\/WA_Fn-UseC_-Telco-Customer-Churn.csv')","1f881f92":"df.head()","b7bd44aa":"df.shape","8b31e7af":"# Numeric Columns\ndf.describe(include = [np.number])","69efedb5":"# Object Columns\ndf.describe(include = [np.object])","05ee50a5":"# Checking for Null Values\ndf.isnull().sum()","9a5d1f52":"# As there is Unique ID for each customer, hence no role would this column play in deciding output, hence dropping ID colummn","bf3cf6fb":"df.drop('customerID', axis = 1, inplace = True)","83f7e6c5":"# Total charge seems numeric in nature, but it is objective type, hence convert this into Numeric\ndf['TotalCharges'] = pd.to_numeric(df['TotalCharges'], errors = 'coerce')","2a75c4c1":"# To find number of Null in TotalCharges Column\nprint(\"Total Null Values: {}\".format(df['TotalCharges'].isnull().sum()))\nprint(\"% of Missing values: {}\".format(df['TotalCharges'].isnull().sum() * 100 \/ df.shape[0]))","9d6c0080":"df.dropna(how = 'any', inplace = True)","bd95b3d3":"# Checking data again\n\ndf.shape","0742cb2f":"# Identify Objective variables and find its relation with Output\nobject = [feature for feature in df.columns if df[feature].dtypes == \"O\"]","344d1813":"# Identify distribution of Output\ndf['Churn'].value_counts()","365a8365":"for value in object:\n    print(value)\n    print(df[value].value_counts())\n    print(pd.crosstab(df[value], df['Churn']))","cd2272c2":"df['MultipleLines'] = np.where(df['MultipleLines'] != \"Yes\", \"No\", \"Yes\")\ndf['OnlineSecurity'] = np.where(df['OnlineSecurity'] != \"Yes\", \"No\", \"Yes\")\ndf['OnlineBackup'] = np.where(df['OnlineBackup'] != \"Yes\", \"No\", \"Yes\")\ndf['DeviceProtection'] = np.where(df['DeviceProtection'] != \"Yes\", \"No\", \"Yes\")\ndf['TechSupport'] = np.where(df['TechSupport'] != \"Yes\", \"No\", \"Yes\")\ndf['StreamingTV'] = np.where(df['StreamingTV'] != \"Yes\", \"No\", \"Yes\")\ndf['StreamingMovies'] = np.where(df['StreamingMovies'] != \"Yes\", \"No\", \"Yes\")","1e668cf9":"#verifying above syntax\ndf['MultipleLines'].value_counts()","5e776b07":"# Using Label Encoder, we will convert all above object into numeric\n\nfrom sklearn.preprocessing import LabelEncoder\n\nlb = LabelEncoder()\nfor value in object:\n    df[value] = lb.fit_transform(df[value])","45907b00":"# Separating Dependent and Independent variables\n\nx = df.iloc[:,:-1]\ny = df.iloc[:,-1]","d39b166a":"# Using standard scaler, lets convert each numeric variable into similar numeric range\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nx = sc.fit_transform(x)\n\nx = pd.DataFrame(x)","db01af82":"# Split data into Train test \n\nfrom sklearn.model_selection import train_test_split\nxtrain, xtest, ytrain, ytest = train_test_split(x, y, stratify = y, test_size = 0.2, random_state = 0)","772f1382":"# We will use StratifiedCV to verify output,\n# This will iterate each model 5 times, to generate output accuracy","ee4339de":"from sklearn.model_selection import StratifiedKFold\n\n# Lets define a function to carryout StratifiedCV\n\ndef skfold_cv(x, y, algo, param, n_jobs = -1):\n    skf = StratifiedKFold(n_splits = 5, shuffle = True, random_state = 0)\n    \n    for i, j in skf.split(x, y):\n        xtrain, xtest = x.iloc[i, :], x.iloc[j, :]\n        ytrain, ytest = y.iloc[i], y.iloc[j]\n        \n        model = algo(**param)\n        model.fit(xtrain, ytrain)\n        ypred = model.predict(xtest)\n        print(accuracy_score(ytest, ypred))\n        print(confusion_matrix(ytest, ypred))","27571d84":"# Lets call LogisticRegression and RandomForestClassifier to carryout classification\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report","a9a21bae":"# Lets train basic model without any hyper parameter tuning\n\nlog_param = {}\nlogit_model = skfold_cv(x, y, LogisticRegression, log_param)","883b9d12":"#Now lets call RandomizedGridCV to carryout hyperparameter tuning\n\nfrom sklearn.model_selection import RandomizedSearchCV","bb60c8fa":"param_grid = {'penalty' : ['l1', 'l2'],\n             'C' : [0.001, 0.01, 0.1, 1, 10]}\n\nlog_clf = LogisticRegression(n_jobs = -1, random_state = 0)\n\nmodel = RandomizedSearchCV(estimator = log_clf, \n                          param_distributions = param_grid,\n                          scoring = 'accuracy', \n                          verbose = 10,\n                          n_jobs = -1,\n                          cv = StratifiedKFold(5, shuffle = True))\n\nmodel.fit(x, y)\n\nprint(f'Best score: {model.best_score_}')\nprint('Best parameters set:')\n\nbest_parameters = model.best_estimator_.get_params()\nfor param_name in sorted(param_grid.keys()):\n    print(f'\\t{param_name}: {best_parameters[param_name]}')","d8659a11":"# Using above parameter lets train model and find accuracy\n\nlog_param = {'penalty':'l2', 'C' : 0.1}\nlogit_model = skfold_cv(x, y, LogisticRegression, log_param)","10eb663c":"rf_param = {}\nrf_model = skfold_cv(x, y, RandomForestClassifier, rf_param)","6ad59117":"# Lets do some hyperparameter tunning to find best parameter to train Randomforest\n\nparam_grid = {'n_estimators' : [100, 200, 300, 400], \n             'max_depth' : [2,5,7,15],\n             'criterion' : ['gini', 'entropy'], \n             'min_samples_split' : [2,5,10,20,100], \n             'min_samples_leaf' : [2,5,10], \n             'max_features' : ['log2', 'sqrt', 'None']}\n\nrf_clf = RandomForestClassifier(n_jobs = -1, random_state = 0)\n\nmodel = RandomizedSearchCV(estimator = rf_clf,\n                          param_distributions = param_grid,\n                          scoring = 'neg_log_loss',\n                          verbose = 10,\n                          n_jobs = 1,\n                          cv = StratifiedKFold(5, shuffle = True))\n\nmodel.fit(x, y)\n\nprint(f'Best score: {model.best_score_}')\nprint('Best parameters set:')\n\nbest_parameters = model.best_estimator_.get_params()\nfor param_name in sorted(param_grid.keys()):\n    print(f'\\t{param_name}: {best_parameters[param_name]}')","e4b40730":"param_grid = {'criterion': ['entropy'],\n             'max_depth' : [7,10,15,20],\n             'max_features' : ['sqrt'],\n             'min_samples_leaf' : [5, 8,10,12, 15],\n             'min_samples_split' : [2,3,4,5,6,10],\n             'n_estimators' : [100,200,300,400]}","5706a756":"from sklearn.model_selection import GridSearchCV\n\nrf = RandomForestClassifier()\ngsearch = GridSearchCV(estimator=rf, param_grid = param_grid, cv = 3, n_jobs = -1)","429f9cb6":"# This will take lot of time\n\ngsearch.fit(xtrain, ytrain)","0c3d7506":"gsearch.best_estimator_","0633b4d0":"clf_final = RandomForestClassifier(criterion='entropy',\n                                   max_depth=20,\n                                   max_features='sqrt',\n                                   min_samples_leaf=12,\n                                   min_samples_split=3,\n                                   n_estimators = 300)\nclf_final.fit(xtrain, ytrain)\nclf_pred = clf_final.predict(xtest)\nprint(accuracy_score(ytest, clf_pred))\nprint(confusion_matrix(ytest, clf_pred))","6bbbe9a5":"from imblearn.combine import SMOTETomek\nfrom collections import Counter\nos=SMOTETomek(0.8)\nxover,yover=os.fit_resample(x,y)\nprint(\"The number of classes before fit {}\".format(Counter(y)))\nprint(\"The number of classes after fit {}\".format(Counter(yover)))","7d6d9cdd":"# lets train basic Randomforest classifier to get notion of how effective this imbalance is\n\nrf_param = {}\nrf_model = skfold_cv(xover, yover, RandomForestClassifier, rf_param)","45b30c17":"param_grid = {'n_estimators' : [100, 200, 300, 400], \n             'max_depth' : [2,5,7,15],\n             'criterion' : ['gini', 'entropy'], \n             'min_samples_split' : [2,5,10,20,100], \n             'min_samples_leaf' : [2,5,10], \n             'max_features' : ['log2', 'sqrt', 'None']}\n\nrf_clf = RandomForestClassifier(n_jobs = -1, random_state = 0)\n\nmodel = RandomizedSearchCV(estimator = rf_clf,\n                          param_distributions = param_grid,\n                          scoring = 'neg_log_loss',\n                          verbose = 10,\n                          n_jobs = 1,\n                          cv = StratifiedKFold(5, shuffle = True))\n\nmodel.fit(xover, yover)\n\nprint(f'Best score: {model.best_score_}')\nprint('Best parameters set:')\n\nbest_parameters = model.best_estimator_.get_params()\nfor param_name in sorted(param_grid.keys()):\n    print(f'\\t{param_name}: {best_parameters[param_name]}')","2f73a2b1":"rf_params = {'n_estimators' : 400,\n            'max_depth': 20,\n            'max_features': 'log2',\n            'criterion': 'entropy',\n            'min_samples_leaf': 5,\n            'min_samples_split': 20}\n\nrf_model = skfold_cv(xover, yover, RandomForestClassifier, rf_params)","ebb277d9":"No Major change in accuracy, may be imbalanced data is culprit\n\n\nLets now use Randomforest classifier to classify data","3c6dcb90":"Let's verify these data using GridserchCV, Im not trying to be greedy, but just cant trust Randomseach :)\n\nNow we will use values of parameters in viscinity of above values specified by Randomsearchcv","4b2e80ba":"# Data Exploration","32496e81":"Now, its upto you guyz, i will share new Notebook with Xgboost, Catboost using Hyperopt\n\n\nThanks to all of you and you should be proud of you, if you are reading this line, please make sure you follow alongwith me on your system","fad820ac":"Columns like MultipleLines has three class, No , Yes and 'No Phone Service'. We need to convert \"No Phone Service\" into No class. We Will use where function from Numpy","3b10cfb3":"It is Imbalanced dataset, we will handle this during model Building stage","fa60fb2f":"# Data Prepration\n\nImporting Libraries","01078722":"So we increased minority class to 80% of majority class ","a7850963":"Ok, After so many iterations, we finally have best parameters to train Randomforest classifier\n\nBest parameters set:\n\n\tcriterion: entropy\n    \n\tmax_depth: 15\n    \n\tmax_features: sqrt\n    \n\tmin_samples_leaf: 10\n    \n\tmin_samples_split: 2\n    \n\tn_estimators: 300\n    ","5152bb7a":"Lets use SMOTETomek from imblearn library\n\nYou guys, if following this code, Please comment full form of SMOTETomek, be honest dont google it :)","db775139":"\nPlease finish your breakfast, lunch and dinner, before you have final answer :)\n\nhurray we have value, After a decade long wait","a0116bf5":"We should have trusted Randomizedserch, we have approximately same answer for each parameter :(","c5a6b909":"No Gridsearch now, I am not cruel :)","400a1ee8":"We have best parameters from above combination of  paramaters,\n\nBest parameters set:\n\nC: 0.1\n    \npenalty: l2","ac9b666b":"# Data Cleaning & Preprocessing","90d24457":"As % is too small (Less than 10%) hence dropping them makes sense","3e801013":"Wow, this is frustrating, after so many iterations, no measurable change in accuracy, \n\nIt has to be imbalance present in dataset\n\nLets tackle that","11188b57":"# DATA DESCRIPTION \nThe dataset was obtained from Kaggle, which contains around 7000 rows and 21 columns with 17 categorical columns, and 3 numerical columns, and 1 label column.\n\ncustomerID: A unique ID that identifies each customer.\n\ngender: The customer\u2019s gender: Female,Male\n\nSeniorCitizen: Indicates if the customer is 65 or older: Yes, No\n\nPartner: Indicates if the customer has partner: Yes, No\n\nDependents: Indicates if the customer lives with any dependents: Yes, No. Dependents could be children, parents, grandparents, etc.\n\ntenure: Indicates the total amount of months that the customer has been with the company.\n\nPhoneService: Indicates if the customer subscribes to home phone service with the company: Yes, No\n\nMultiple Lines: Indicates if the customer subscribes to multiple telephone lines with the company: Yes, No, No phone service\n\nInternet Service: Indicates if the customer subscribes to Internet service with the company: No, DSL, Fiber optic, No\n\nOnline Security: Indicates if the customer subscribes to an additional online security service provided by the company: Yes, No, No internet service\n\nOnlineBackup: Indicates if the customer subscribes to an additional online backup service provided by the company: Yes, No, No internet service\n\nDeviceProtection: Indicates if the customer subscribes to an additional device protection plan for their Internet equipment provided by the company: Yes, No, No internet service\n\nTechSupport: Indicates if the customer subscribes to an additional technical support plan from the company with reduced wait times: Yes, No, No internet service\n\nStreamingTV: Indicates if the customer uses their Internet service to stream television programing from a third party provider: Yes, No, No internet service\n\nStreaming Movies: Indicates if the customer uses their Internet service to stream movies from a third party provider: Yes, No, No internet service\n\nContract: Indicates the customer\u2019s current contract type: Month-to-Month, One Year, Two Year.\n\nPaperless Billing: Indicates if the customer has chosen paperless billing: Yes, No\n\nPayment Method: Indicates how the customer pays their bill: Bank transfer, Credit card, Electronic check, Mailed Check\n\nMonthly Charge: Indicates the customer\u2019s current total monthly charge for all their services from the company.\n\nTotal Charges: Indicates the customer\u2019s total charges, calculated to the end of the quarter specified above.\n\nChurn: Yes = the customer left the company this quarter. No = the customer remained with the company. Directly related to Churn Value.\n\nReference: https:\/\/www.kaggle.com\/blastchar\/telco-customer-churn","fb55bf8f":"That is clear now, we have found culprit, it is imbalanceness of data\n\nNow, lets do some hyperparameter tuning","deddbcec":"Above syntax will convert object into numeric, and places where other then numeric value present, \nit will replace that value with Nan"}}