{"cell_type":{"14f55dc6":"code","5d7f6a0b":"code","2c3f78ef":"code","4b8b1c9a":"code","bcf1b6ef":"code","bca79cca":"code","67f9a654":"code","ed9b83c0":"code","628224b1":"code","e4a9cc28":"code","93e26ccf":"code","554ce06a":"code","6d9d2e90":"code","656c8b86":"code","62ce23cb":"code","acf8e485":"code","791d9c99":"code","57ea6355":"code","d8a54df7":"code","64221bfe":"code","b0642c04":"code","51bdce3a":"code","f9c2af73":"code","b80b6715":"code","aaf6b89c":"code","ac7ab363":"code","cc9ebeca":"code","0debb7b6":"code","863fba73":"markdown","7cceecac":"markdown","e83570c6":"markdown","471300b4":"markdown","34e2c6f6":"markdown","5bb7e576":"markdown","c37a3c05":"markdown","f303a6dd":"markdown","3a382a07":"markdown","eb59de42":"markdown","6215ff59":"markdown","8d4ad71c":"markdown","a099f254":"markdown","71a36014":"markdown","df49d243":"markdown","590e9694":"markdown","4e4df6bc":"markdown","5c66ca59":"markdown","4b260fc8":"markdown","aacd670b":"markdown","c8db168e":"markdown","b4a68a82":"markdown","1647fd8d":"markdown","d33ef33a":"markdown","b710250f":"markdown","3328ccd0":"markdown"},"source":{"14f55dc6":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.svm import SVC\n\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","5d7f6a0b":"train_data = pd.read_csv(\"\/kaggle\/input\/cap-4611-2021-fall-assignment-02\/train.csv\")\ntest_data = pd.read_csv(\"\/kaggle\/input\/cap-4611-2021-fall-assignment-02\/eval.csv\")","2c3f78ef":"print(train_data.head())","4b8b1c9a":"print(test_data.head())","bcf1b6ef":"print(list(train_data))","bca79cca":"print(list(test_data))","67f9a654":"train_data.dtypes","ed9b83c0":"train_data.isnull().sum()","628224b1":"test_data.dtypes","e4a9cc28":"test_data.isnull().sum()","93e26ccf":"ax = plt.subplots(figsize=(5,5))\ntrain_data[\"esrb_rating\"].value_counts().plot.bar(color='y');\nplt.title(\"Distribution from Ratings\")\nplt.show()","554ce06a":"plt.figure(figsize=(5, 5))\nsns.barplot(x=train_data[\"esrb_rating\"], y=train_data[\"blood\"])\nplt.title(\"Blood vs ratings \")\nplt.show()","6d9d2e90":"plt.figure(figsize=(5, 5))\nsns.barplot(x=train_data[\"esrb_rating\"], y=train_data[\"drug_reference\"])\nplt.title(\"Drug References vs ratings \")\nplt.show()","656c8b86":"plt.figure(figsize=(5, 5))\nsns.barplot(x=train_data[\"esrb_rating\"], y=train_data[\"language\"])\nplt.title(\"Language vs ratings \")\nplt.show()","62ce23cb":"plt.figure(figsize=(5, 5))\nsns.barplot(x=train_data[\"esrb_rating\"], y=train_data[\"sexual_content\"])\nplt.title(\"Sexual Content vs ratings \")\nplt.show()","acf8e485":"plt.figure(figsize=(5, 5))\nsns.barplot(x=train_data[\"esrb_rating\"], y=train_data[\"console\"])\nplt.title(\"Console vs ratings \")\nplt.show()","791d9c99":"plt.figure(figsize=(5, 5))\nsns.barplot(x=train_data[\"esrb_rating\"], y=train_data[\"no_descriptors\"])\nplt.title(\"No Descriptors vs ratings \")\nplt.show()","57ea6355":"train_data = train_data.drop([\"console\"], axis=1)\ntest_data = test_data.drop([\"console\"], axis=1)","d8a54df7":"train_data = train_data.rename(columns={\"strong_janguage\": \"strong_language\"})\ntest_data = test_data.rename(columns={\"strong_janguage\": \"strong_language\"})","64221bfe":"X = train_data.drop(columns=[\"esrb_rating\",\"title\"], axis=1)\ny = train_data[\"esrb_rating\"]\ntest_X = test_data\ny = y.map({'M': 0, 'ET': 1, 'E': 2, 'T': 3})\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, shuffle=True)\n","b0642c04":"pipeline_lr = Pipeline(steps=[(\"model\", LogisticRegression())], verbose=False)\nlr = GridSearchCV(estimator = pipeline_lr, param_grid={ 'model__penalty': [\"l1\", \"l2\"],\n                                                       'model__solver': [\"liblinear\"],\n                                                       'model__C': [10, 25, 50, 75, 100, 250]},\n                                                       scoring='accuracy', cv=5, verbose=0)\nlr.fit(X_train, y_train)\n\ny_pred = lr.predict(X_test)\nlr_score = lr.score(X_test, y_test)\nprint(f\"\\nLogistic Regression Score: {lr_score}\\n\")\n\nlr_cv = cross_val_score(lr, X, y, cv=15)\nlr_df = pd.DataFrame(lr_cv)\nprint(\"Logistic Regression\")\nprint('Describe:')\nprint(f\"{lr_df.describe().to_string()}\\n\")","51bdce3a":"pipeline_SVM = Pipeline(steps=[(\"model\", SVC())], verbose=False)\nSVmodel = GridSearchCV(estimator = pipeline_SVM, param_grid={ 'model__kernel': [\"rbf\"],\n                                                       'model__gamma': [.3],\n                                                       'model__C': [.05, .25, .5, 1.0]},\n                                                       scoring='accuracy', cv=5, verbose=0)\n\nSVmodel.fit(X_train, y_train)\n\ny_pred = SVmodel.predict(X_test)\nsvm_score = SVmodel.score(X_test, y_test)\n        \nprint(f\"\\nSupport Vector Machine Score: {svm_score}\\n\")\n\nsvm_cv = cross_val_score(SVmodel, X, y, cv=15)\nsvm_df = pd.DataFrame(svm_cv)\nprint(\"Support Vector Machine\")\nprint('Describe:')\nprint(f\"{svm_df.describe().to_string()}\\n\")\n","f9c2af73":"pipeline_dt = Pipeline(steps=[(\"model\", DecisionTreeClassifier())], verbose=False)\ndecTree = GridSearchCV(estimator = pipeline_dt, param_grid={ 'model__criterion': [\"entropy\"],\n                                                       'model__splitter': [\"best\"]},\n                                                       scoring='accuracy', cv=5, verbose=0)\n\ndecTree.fit(X_train, y_train)\n\ny_pred = decTree.predict(X_test)\ndT_score = decTree.score(X_test, y_test)\n\nprint(f\"Decision Tree Score: {dT_score}\\n\")\n\nDT_cv = cross_val_score(decTree, X, y, cv=15)\nDT_df = pd.DataFrame(DT_cv)\nprint(\"Decision Tree\")\nprint('Describe:')\n\nprint(f\"{DT_df.describe().to_string()}\\n\")\n","b80b6715":"pipeline_rf = Pipeline(steps=[(\"model\", RandomForestClassifier())], verbose=False)\nrfc_Classifier = GridSearchCV(estimator = pipeline_rf, param_grid={ 'model__criterion': [\"entropy\"],\n                                                       'model__n_estimators': [10,25],\n                                                       'model__max_depth': [5],\n                                                       'model__max_features' : ['sqrt']}, \n                                                       scoring='accuracy', cv=5, verbose=0)\n\nrfc_Classifier.fit(X_test,y_test)\n\ny_pred = rfc_Classifier.predict(X_test)\nrfc_Score = rfc_Classifier.score(X_test, y_test)\n\nprint(f\"Random Forest Score: {rfc_Score}\\n\")\n\nrfc_cv = cross_val_score(rfc_Classifier, X, y, cv=15)\nrfc_df = pd.DataFrame(rfc_cv)\nprint(\"\\nRandom Forest\")\nprint('Describe:')\nprint(f\"{rfc_df.describe().to_string()}\\n\")\n","aaf6b89c":"pipeline_knn = Pipeline(steps=[(\"model\", KNeighborsClassifier())], verbose=False)\nknn = GridSearchCV(estimator = pipeline_knn, param_grid={ 'model__weights': ['uniform'],\n                                                       'model__n_neighbors':[1,3,5],\n                                                       'model__algorithm': ['kd_tree'],\n                                                       'model__metric': ['minkowski']},\n                                                       scoring='accuracy', cv=5, verbose=0)\n\nknn.fit(X_test,y_test)\n\n\ny_pred = knn.predict(X_test)\nknn_Score = knn.score(X_test, y_test)\n\nprint(f\"K Nearest Neighbors Score: {knn_Score}\\n\")\n\nknn_cv = cross_val_score(knn, X, y, cv=15)\nknn_df = pd.DataFrame(knn_cv)\nprint(\"K Nearest Neighbors\")\nprint('Describe:')\nprint(f\"{knn_df.describe().to_string()}\\n\")","ac7ab363":"import xgboost as xgb\n\nxg_cl = xgb.XGBClassifier(use_label_encoder=False,objective='binary:logistic', n_estimators=10, seed=123)\nxg_cl.fit(X_test,y_test)\n\n\ny_pred = xg_cl.predict(X_test)\nxgScore = xg_cl.score(X_test, y_test)\n\nprint(f\" XG Boost Score: {xgScore}\\n\")\n","cc9ebeca":"bestScore = 0\nsubmitModel = lr\nmodelType = 'LR'\nfor i in range(0, 100):\n    \n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, shuffle=True)\n    \n    pipeline_lr = Pipeline(steps=[(\"model\", LogisticRegression())], verbose=False)\n    lr = GridSearchCV(estimator = pipeline_lr, param_grid={ 'model__penalty': [\"l1\", \"l2\"],\n                                                           'model__solver': [\"liblinear\"],\n                                                           'model__C': [10, 25, 50, 75, 100, 250,500,1000]},\n                                                           scoring='accuracy', cv=5, verbose=0)\n    \n    lr.fit(X_train, y_train)\n\n    # Displaying Data\n    lr_score = lr.score(X_test, y_test)\n    \n    \n#     xg_cl = xgb.XGBClassifier(use_label_encoder=False,objective='binary:logistic', n_estimators=10, seed=123)\n#     xg_cl.fit(X_test,y_test)\n\n#     xgScore = xg_cl.score(X_test, y_test)\n    \n    \n    if lr_score > bestScore:\n        bestScore = lr_score\n        submitModel = lr\n        modelType = 'LR'\n        \n#     if xgScore > bestScore:\n#         bestScore = xgScore\n#         submitModel = xg_cl\n#         modelType = 'XG'\n        \nprint(f\"\\nBest Model: {modelType}\")\nprint(f\"Best Score: {bestScore}\")","0debb7b6":"predictor = submitModel.predict(test_X)\noutput = pd.DataFrame({'id': test_data.id, 'esrb_rating': predictor})\noutput['esrb_rating'] = output['esrb_rating'].map({0:'M',1:'ET', 2:'E', 3:'T'})\noutput.to_csv('submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")\nprint(output.to_string())","863fba73":"So, there is a big coorelation between if there is a descriptor and the rating, so I will keep this one, I almost tossed it","7cceecac":"Luckily, there is need for an approach to handle missing values.","e83570c6":"# Here is where the data is being loaded\n* I added the train.csv file to a variable called train_data.\n* I added the eval.csv file to a variable called test_data.","471300b4":"Since this data consists of boolean values for most of the categories, there will be no outliers.\n\nI will change the ESRB ratings to numerical values from 0-3, but this will not affect weather there will be outliers.\n\nI will be doing a little bit of a exploratory data analysis to see if there is any unimportant data.","34e2c6f6":"# Here is where select the best model that I have generated and I will use the model to predict the target vector from the test data","5bb7e576":"# Here is where I will be looking for any outliers within the training data","c37a3c05":"I initially made the XG boost part of which model I thought was best, but I constantly kept getting bad scores.\n\nSo I am just sticking to Linear Regression.","f303a6dd":"# Ryan Kendrick","3a382a07":"# Here is where I build and train a Random Forest model on the training data","eb59de42":"I checked for missing data and thankfully, this time there is none!","6215ff59":"First, I will run the model with the best score a few times before making a selection. \n\nI have the most experience with LR and was able to get the score decently high.\n","8d4ad71c":"There is no need for the console or decriptors when predicting the ESRB rating.","a099f254":"# Here is where I attempt XG boost","71a36014":"# Here is where I build and train a K Nearest Neighbors model on the training data","df49d243":"This shows that there is a big correlation between blood and gore and weather it is either (E, ET) or (T, M)","590e9694":"First, I want to see if there are any categories with unexpected data types.\n\nIt looks accurate.","4e4df6bc":"# Here is where I build and train a Decision Tree model on the training data","5c66ca59":"I tried getting rid of these darn warnings but they ended up staying, SORRY!","4b260fc8":"# Here is where I will be describing any data transformations or feature enginnering","aacd670b":"This is where I will be splitting my data to test it on","c8db168e":"# Here is where I will be checking for missing values and errors in the training data","b4a68a82":"# Here is where I build and train a Logistic Regression model on the training data","1647fd8d":"There seems to be no correlatin between the console and the rating as I assumed","d33ef33a":"Also, I want to fix this strong janguage nonsense!","b710250f":"I will be also changing the esrb_ratings to numerical values with mapping.\n\nOnce there is a model chosen, I will remap the values back to the object type.","3328ccd0":"# Here is where I build and train a Support Vector Machine model on the training data"}}