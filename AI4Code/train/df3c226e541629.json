{"cell_type":{"63cab1d0":"code","8ae46faf":"code","1ce7b39f":"code","ae11ba1b":"code","d5820d4f":"code","343b954f":"code","8aa4b0db":"code","1869343e":"code","536c965c":"code","1db1d9e3":"code","308085b2":"code","897896c1":"code","31151b06":"code","9083da42":"code","9bc58533":"code","43c7da10":"code","e4be32f2":"code","3ae6bad5":"code","36d18b69":"code","7eaf9c13":"code","3f21158a":"code","32502bad":"code","cbf535bd":"code","df8ee3e4":"code","694b3468":"code","2eea77fe":"code","73bf1069":"code","8d478ef9":"code","3c388a28":"code","e1369a5f":"code","d1fa1ea5":"code","7bd17858":"code","b78787ac":"code","69125108":"code","8f1841bf":"code","c1284f97":"code","96ab8c48":"code","d86b1013":"code","65634cd5":"code","190496b1":"code","f5066c2d":"code","788cc916":"code","3e276f07":"code","550050ab":"code","f5f0583c":"code","aea0d851":"code","b1536ea3":"code","f7befee0":"code","c9f283bf":"code","080da4f7":"code","7bf9e8c0":"code","92d35375":"code","2bd98ce6":"code","dd5a6802":"code","de62d569":"code","b808a94d":"code","88ddd072":"code","c64c5580":"code","e5da7306":"code","00684c0a":"code","b6edd372":"code","1d2f046c":"code","d3fd4085":"code","e8cc9b4e":"code","ae4386f1":"code","c11f5ba0":"code","fa76acf5":"code","22712c37":"code","666eeef9":"code","4623961e":"code","d36f03cd":"code","d85739bf":"code","af7b1472":"code","9e979d8e":"code","38a41d56":"code","93fbd587":"code","7bc25482":"code","6df9a4c9":"code","9c88a0f4":"code","509c0ccc":"code","1b61d95c":"code","9077ac8d":"code","2b612801":"code","4cebc246":"code","9a631fcf":"code","63308a59":"code","c366a7e6":"code","2825b356":"code","c8d19a3b":"markdown","7c9db06c":"markdown","0f83b899":"markdown","eb6dc5d9":"markdown","eb7a0303":"markdown","fdf225e3":"markdown","ec80c137":"markdown","26bf68b3":"markdown","06b96cd5":"markdown","a33590c1":"markdown","7023a2c4":"markdown","540b7bdf":"markdown","79d64b8e":"markdown","863397d5":"markdown","f9bd9879":"markdown","84469361":"markdown","6c935882":"markdown","017c9ca6":"markdown","be918f75":"markdown","230f0451":"markdown","e91c14fe":"markdown","0601f5a1":"markdown","f74253e1":"markdown","11765fcc":"markdown","deed2f82":"markdown","8054f80a":"markdown","e2309b7b":"markdown","979caf96":"markdown","afd65820":"markdown","1ae37335":"markdown","81188d15":"markdown","d612cbda":"markdown","cdd513d5":"markdown","8ff24c84":"markdown","136d404e":"markdown","ce6cc728":"markdown","41d858ac":"markdown","a17fc1e0":"markdown","ba92022e":"markdown","7d0087f2":"markdown"},"source":{"63cab1d0":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","8ae46faf":"import os\nprint(os.listdir(\"..\/input\"))\n\nimport pandas as pd\n\ntrain = pd.read_csv('..\/input\/black-friday\/train.csv')\n","1ce7b39f":"train","ae11ba1b":"import os\nprint(os.listdir(\"..\/input\"))\n\nimport pandas as pd\n\ntest = pd.read_csv('..\/input\/black-friday\/test.csv')","d5820d4f":"test","343b954f":"# Importing another libraries\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","8aa4b0db":"train.head()","1869343e":"train.info()\n\n# In this data total 12 columns including DV (Purchase) and can see product category 2 , 3 having nun values.\n# having 5 string data which we need to encode.","536c965c":"train.describe()","1db1d9e3":"train.columns","308085b2":"train.dtypes","897896c1":"idsUnique = len(set(train.User_ID))\nidsTotal = train.shape[0]\nidsDupli = idsTotal - idsUnique\nprint(\"There are \" + str(idsDupli) + \" duplicate IDs for \" + str(idsTotal) + \" total entries\")","31151b06":"# As know from information already that in product category 2, 3 having huge null values\ntrain.isnull().sum()","9083da42":"# Checking proportion of female buyer and male buyer\ntrain['Gender'].value_counts()","9bc58533":"# Checking proportion customer based on their marital status.\ntrain['Marital_Status'].value_counts()","43c7da10":"# Checking maximun range of purchase\ntrain['Purchase'].max()","e4be32f2":"# Checking minimum range of purchase\ntrain['Purchase'].min()","3ae6bad5":"plt.style.use('fivethirtyeight')\nplt.figure(figsize=(12,7))\nsns.distplot(train.Purchase, bins = 25)\nplt.xlabel(\"Amount spent in Purchase\")\nplt.ylabel(\"Number of Buyers\")\nplt.title(\"Purchase amount Distribution\")","36d18b69":"print (\"Skew is:\", train.Purchase.skew())\nprint(\"Kurtosis: %f\" % train.Purchase.kurt())","7eaf9c13":"numeric_features = train.select_dtypes(include=[np.number])\nnumeric_features.dtypes","3f21158a":"sns.countplot(train.Occupation)","32502bad":"sns.countplot(train.Marital_Status)","cbf535bd":"sns.countplot(train.Product_Category_1)\nplt.xticks()","df8ee3e4":"sns.countplot(train.Product_Category_2)\nplt.xticks(rotation=90)","694b3468":"sns.countplot(train.Product_Category_3)\nplt.xticks(rotation=90)","2eea77fe":"corr = numeric_features.corr()\n\nprint (corr['Purchase'].sort_values(ascending=False)[:10], '\\n')\nprint (corr['Purchase'].sort_values(ascending=False)[-10:])","73bf1069":"#correlation matrix\nf, ax = plt.subplots(figsize=(20, 9))\nsns.heatmap(corr,  annot=True,annot_kws={'size': 15})","8d478ef9":"s = corr.unstack()\ns","3c388a28":"sns.countplot(train.Gender)","e1369a5f":"sns.countplot(train.Age)\n\n# As expected, most purchases are made by people between 18 to 45 years old.","d1fa1ea5":"sns.countplot(train.City_Category)","7bd17858":"sns.countplot(train.Stay_In_Current_City_Years)","b78787ac":"Occupation_pivot = \\\ntrain.pivot_table(index='Occupation', values=\"Purchase\", aggfunc=np.mean)\n","69125108":"Occupation_pivot","8f1841bf":"Occupation_pivot.plot(kind='bar', color='blue',figsize=(12,7))\nplt.xlabel(\"Occupation\")\nplt.ylabel(\"Purchase\")\nplt.title(\"Occupation and Purchase Analysis\")\nplt.xticks(rotation=0)\nplt.show()","c1284f97":"Marital_Status_pivot = \\\ntrain.pivot_table(index='Marital_Status', values=\"Purchase\", aggfunc=np.mean)","96ab8c48":"Marital_Status_pivot","d86b1013":"Marital_Status_pivot.plot(kind='bar', color='blue',figsize=(12,7))\nplt.xlabel(\"Marital_Status\")\nplt.ylabel(\"Purchase\")\nplt.title(\"Marital_Status and Purchase Analysis\")\nplt.xticks(rotation=0)\nplt.show()","65634cd5":"Product_category_1_pivot = train.pivot_table(index='Product_Category_1', values=\"Purchase\", aggfunc=np.mean)","190496b1":"Product_category_1_pivot","f5066c2d":"Product_category_1_pivot.plot(kind='bar', color='blue',figsize=(12,7))\nplt.xlabel(\"Product_Category_1\")\nplt.ylabel(\"Purchase\")\nplt.title(\"Product_Category_1 and Purchase Analysis\")\nplt.xticks(rotation=0)\nplt.show()","788cc916":"Product_category_1_pivot = train.pivot_table(index='Product_Category_1', values=\"Purchase\", aggfunc=np.sum)\nProduct_category_1_pivot","3e276f07":"Product_category_1_pivot.plot(kind='bar', color='blue',figsize=(12,7))\nplt.xlabel(\"Product_Category_1\")\nplt.ylabel(\"Purchase\")\nplt.title(\"Product_Category_1 and Purchase Analysis \" \"SUM\")\nplt.xticks(rotation=0)\nplt.show()","550050ab":"Product_category_2_pivot = train.pivot_table(index='Product_Category_2', values=\"Purchase\", aggfunc=np.mean)","f5f0583c":"Product_category_2_pivot","aea0d851":"Product_category_2_pivot.plot(kind='bar', color='blue',figsize=(12,7))\nplt.xlabel(\"Product_Category_2\")\nplt.ylabel(\"Purchase\")\nplt.title(\"Product_Category_2 and Purchase Analysis\")\nplt.xticks(rotation=0)\nplt.show()","b1536ea3":"Product_category_2_pivot = train.pivot_table(index='Product_Category_2', values=\"Purchase\", aggfunc=np.sum)","f7befee0":"Product_category_2_pivot","c9f283bf":"Product_category_2_pivot.plot(kind='bar', color='blue',figsize=(12,7))\nplt.xlabel(\"Product_Category_2\")\nplt.ylabel(\"Purchase\")\nplt.title(\"Product_Category_2 and Purchase Analysis \"  \"SUM\")\nplt.xticks(rotation=0)\nplt.show()","080da4f7":"Product_category_3_pivot = train.pivot_table(index='Product_Category_3', values=\"Purchase\", aggfunc=np.mean)\nProduct_category_3_pivot","7bf9e8c0":"Product_category_3_pivot.plot(kind='bar', color='blue',figsize=(12,7))\nplt.xlabel(\"Product_Category_3\")\nplt.ylabel(\"Purchase\")\nplt.title(\"Product_Category_3 and Purchase Analysis\")\nplt.xticks(rotation=0)\nplt.show()","92d35375":"Product_category_3_pivot = train.pivot_table(index='Product_Category_3', values=\"Purchase\", aggfunc=np.sum)\nProduct_category_3_pivot","2bd98ce6":"Product_category_3_pivot.plot(kind='bar', color='blue',figsize=(12,7))\nplt.xlabel(\"Product_Category_3\")\nplt.ylabel(\"Purchase\")\nplt.title(\"Product_Category_3 and Purchase Analysis \" \"SUM\")\nplt.xticks(rotation=0)\nplt.show()","dd5a6802":"gender_pivot = train.pivot_table(index='Gender', values=\"Purchase\", aggfunc=np.mean)\ngender_pivot\n","de62d569":"gender_pivot.plot(kind='bar', color='blue',figsize=(12,7))\nplt.xlabel(\"Gender\")\nplt.ylabel(\"Purchase\")\nplt.title(\"Gender and Purchase Analysis \" \"AVERAGE\")\nplt.xticks(rotation=0)\nplt.show()","b808a94d":"age_pivot = train.pivot_table(index='Age', values=\"Purchase\", aggfunc=np.sum)\nage_pivot\n","88ddd072":"age_pivot.plot(kind='bar', color='blue',figsize=(12,7))\nplt.xlabel(\"Age\")\nplt.ylabel(\"Purchase\")\nplt.title(\"Age and Purchase Analysis \" \"SUM\")\nplt.xticks(rotation=0)\nplt.show()","c64c5580":"city_pivot = train.pivot_table(index='City_Category', values=\"Purchase\", aggfunc=np.mean)\ncity_pivot\n","e5da7306":"city_pivot.plot(kind='bar', color='blue',figsize=(12,7))\nplt.xlabel(\"City_Category\")\nplt.ylabel(\"Purchase\")\nplt.title(\"City_Category and Purchase Analysis\")\nplt.xticks(rotation=0)\nplt.show()","00684c0a":"Stay_In_Current_City_Years_pivot = train.pivot_table(index='Stay_In_Current_City_Years', values=\"Purchase\", aggfunc=np.mean)\nStay_In_Current_City_Years_pivot\n","b6edd372":"Stay_In_Current_City_Years_pivot.plot(kind='bar', color='blue',figsize=(12,7))\nplt.xlabel(\"Stay_in_Current_City_Years\")\nplt.ylabel(\"Purchase\")\nplt.title(\"Stay_in_Current_City_Years and Purchase Analysis\")\nplt.xticks(rotation=0)\nplt.show()","1d2f046c":"# Join Train and Test Dataset\ntrain['source']='train'\ntest['source']='test'\n\ndata = pd.concat([train,test], ignore_index = True, sort = False)\n\nprint(train.shape, test.shape, data.shape)","d3fd4085":"#Check the percentage of null values per variable\n\n# 31% approx in product cat 2 and 69% approx in product cat 3 data is missing\n\ndata.isnull().sum()\/data.shape[0]*100","e8cc9b4e":"data[\"Product_Category_2\"]= data[\"Product_Category_2\"].fillna(-2.0).astype(\"float\")\n\ndata.Product_Category_2.value_counts().sort_index()","ae4386f1":"data.isnull().sum()","c11f5ba0":"data.Product_Category_3.value_counts().sort_index()","fa76acf5":"data[\"Product_Category_3\"]= \\\ndata[\"Product_Category_3\"].fillna(-2.0).astype(\"float\")","22712c37":"data.Product_Category_3.value_counts().sort_index()","666eeef9":"data.isnull().sum()","4623961e":"# Removing Product_Category_1 group 19 and 20 from Train as this is not in Product_Category_2 and 3","d36f03cd":"#Get index of all columns with product_category_1 equal 19 or 20 from train\n\ncondition = data.index[(data.Product_Category_1.isin([19,20])) & (data.source == \"train\")]\ndata = data.drop(condition)","d85739bf":"\ndata.shape","af7b1472":"# Categorical Values","9e979d8e":"#Apply function len(unique()) to every data variable\n\ndata.apply(lambda x: len(x.unique()))","38a41d56":"# Frequency Analysis","93fbd587":"#Filter categorical variables and get dataframe will all strings columns names except Item_identfier and outlet_identifier\ncategory_cols = data.select_dtypes(include=['object']).columns.drop([\"source\"])\n#Print frequency of categories\nfor col in category_cols:\n    #Number of times each value appears in the column\n    frequency = data[col].value_counts()\n    print(\"\\nThis is the frequency distribution for \" + col + \":\")\n    print(frequency)","7bc25482":"#Turn gender binary\ngender_dict = {'F':0, 'M':1}\ndata[\"Gender\"] = data[\"Gender\"].apply(lambda line: gender_dict[line])\n\ndata[\"Gender\"].value_counts()","6df9a4c9":"# Giving Age Numerical values\nage_dict = {'0-17':0, '18-25':1, '26-35':2, '36-45':3, '46-50':4, '51-55':5, '55+':6}\ndata[\"Age\"] = data[\"Age\"].apply(lambda line: age_dict[line])\n\ndata[\"Age\"].value_counts()","9c88a0f4":"city_dict = {'A':0, 'B':1, 'C':2}\ndata[\"City_Category\"] = data[\"City_Category\"].apply(lambda line: city_dict[line])\n\ndata[\"City_Category\"].value_counts()","509c0ccc":"#Import library:\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\n\n#New variable for outlet\ndata['Stay_In_Current_City_Years'] = le.fit_transform(data['Stay_In_Current_City_Years'])\n    \n#Dummy Variables:\ndata = pd.get_dummies(data, columns=['Stay_In_Current_City_Years'])\n\ndata.dtypes","1b61d95c":"# feature representing the count of each user\ndef getCountVar(compute_df, count_df, var_name):\n    grouped_df = count_df.groupby(var_name)\n    count_dict = {}\n    for name, group in grouped_df:\n        count_dict[name] = group.shape[0]\n\n    count_list = []\n    for index, row in compute_df.iterrows():\n        name = row[var_name]\n        count_list.append(count_dict.get(name, 0))\n    return count_list","9077ac8d":"#data[\"User_ID_Count\"]  = getCountVar(data, data, \"User_ID\")\ndata[\"Age_Count\"]  =getCountVar(data, data, \"Age\")\ndata[\"Occupation_Count\"]  =getCountVar(data, data, \"Occupation\")\ndata[\"Product_Category_1_Count\"]  =getCountVar(data, data,\"Product_Category_1\")\ndata[\"Product_Category_2_Count\"]  =getCountVar(data, data, \"Product_Category_2\")\ndata[\"Product_Category_3_Count\"]  =getCountVar(data, data,\"Product_Category_3\")\ndata[\"Product_ID_Count\"]  =getCountVar(data, data, \"Product_ID\")","2b612801":"#Divide into test and train:\ntrain = data.loc[data['source']==\"train\"]\ntest = data.loc[data['source']==\"test\"]\n\n#Drop unnecessary columns:\ntest.drop(['source'],axis=1,inplace=True)\ntrain.drop(['source'],axis=1,inplace=True)\n\n#Export files as modified versions:\ntrain.to_csv(\"train_modified.csv\",index=False)\ntest.to_csv(\"test_modified.csv\",index=False)","4cebc246":"# 4. Model","9a631fcf":"train_df = pd.read_csv('train_modified.csv')\ntest_df = pd.read_csv('test_modified.csv')","63308a59":"#Define target and ID columns:\ntarget = 'Purchase'\nIDcol = ['User_ID','Product_ID']\nfrom sklearn import model_selection, metrics\n\n\ndef modelfit(alg, dtrain, dtest, predictors, target, IDcol, filename):\n    #Fit the algorithm on the data\n    alg.fit(dtrain[predictors], dtrain[target])\n        \n    #Predict training set:\n    dtrain_predictions = alg.predict(dtrain[predictors])\n\n    #Perform model_selection:\n    cv_score = model_selection.cross_val_score(alg, dtrain[predictors],(dtrain[target]) , cv=20, scoring='neg_mean_squared_error')\n    cv_score = np.sqrt(np.abs(cv_score))\n    \n    #Print model report:\n    print(\"\\nModel Report\")\n    print(\"RMSE : %.4g\" % np.sqrt(metrics.mean_squared_error((dtrain[target]).values, dtrain_predictions)))\n    print(\"CV Score : Mean - %.4g | Std - %.4g | Min - %.4g | Max - %.4g\" % (np.mean(cv_score),np.std(cv_score),np.min(cv_score),np.max(cv_score)))\n    \n    #Predict on testing data:\n    dtest[target] = alg.predict(dtest[predictors])\n    \n    #Export submission file:\n    IDcol.append(target)\n    submission = pd.DataFrame({ x: dtest[x] for x in IDcol})\n    submission.to_csv(filename, index=False)","c366a7e6":"\nfrom sklearn.linear_model import LinearRegression\nLR = LinearRegression(normalize=True)\n\npredictors = train_df.columns.drop(['Purchase','Product_ID','User_ID'])\nmodelfit(LR, train_df, test_df, predictors, target, IDcol, 'LR.csv')\n\ncoef1 = pd.Series(LR.coef_, predictors).sort_values()","2825b356":"from sklearn.tree import DecisionTreeRegressor\nDT = DecisionTreeRegressor(max_depth=15, min_samples_leaf=100)\nmodelfit(DT, train_df, test_df, predictors, target, IDcol, 'DT.csv')\n\ncoef3 = pd.Series(DT.feature_importances_, predictors).sort_values(ascending=False)","c8d19a3b":"# Categorical Predictors\n## Distribution of the variable Gender\nMost of the buyers are males, but who spends more on each purchase: man or woman?","7c9db06c":"# Correlation between Numerical Predictor( IV) and Target variable(DV)","0f83b899":"# 5. Conclusion\nThe ML algorithm that perform the best was Decision Tree Model with RMSE = 2680 which got me in the first 42%","eb6dc5d9":"# 5. Function to create count features","eb7a0303":"## Distribution of the variable Stay_In_Current_City_Years\nThe tendency looks like the longest someone is living in that city the less prone they are to buy new things. Hence, if someone is new in town and needs a great number of new things for their house that they\u2019ll take advantage of the low prices in Black Friday to purchase all the things needed.","fdf225e3":"# 2. Converting Age to numeric values","ec80c137":"# Distribution of the variable City_Category","26bf68b3":"# BIVARIATE ANALYSIS based on Categorical Variables\n## 6 -Gender and Purchase analysis\n##### On average the male gender spends more money on purchase contrary to female, and it is possible to also observe this trend by adding the total value of purchase. This last conclusion is more reasonable since the percentage of male buyers is higher than female buyers.","06b96cd5":"# Distribution of the variable Product_Category_2","a33590c1":"# Distribution of the variable Marital_Status\nAs expected there are more single people buying products on Black Friday than married people, but do they spend more?","7023a2c4":"# Making Assumptions -\nLets think what factor can affect the purchase of product based on different segment. It could be so many things. So I am segregating this into different groups -\n\n## 1 - City Level Hypotheses:\nCity Type and Size : Urban or Tier 1 cities should have higher sales because of the higher income levels of people there.\n\nPopulation Density: Cities with densely populated areas should have higher sales because of more demand.\n\nYounger Population : Cities with younger populations might have higher tendency to spend more on Black Friday\n## 2 -Customer Level Hypotheses:\nIncome: People with higher income should spend more on products.\n\nAge and Gender: Men with ages ranging from 25 to 40 should spend more on techlogical products.\n\nFamily Size: Families should be more contained on spendings, just buying the best offers and only needed products.\n\nPurchase History: Customer with a purchase history should be more willing to purchase more products on this day.\n## 3 - Store Level Hypotheses:\nLocation: Stores with a location in well moved streets should have better sales.\n\nSize: Bigger stores with higher stores and variety of products should have better sales\n.\nCompetition: Stores with no competitors near by must have the highest sales.\n\nMarketing: Do stores which spend more on marketing should have the best sales results\n## 4 - Product Level Hypotheses:\nCategory: Most clients should be looking to buy technological products;\n\nPrice: Customer will spend more on products with higher discounts\n\nAdvertising: More advertised products should sell more\n\nVisibility: More visible products should sell more\n\nBrand: Clients will invest more on already known brands\n## Moreover, other questions may be interesting to follow up:\n\nWhich type of client spends more?\n\nWhich product category and store had the highest sales?\n\nWhat products usually buy families and single people?\n\nAccording to age and sex what are the most bought products?","540b7bdf":"#### There does not seem to be any predictor that would have a high impact on Purchase , since the highest correlation is give by Occupation with 0.0208. On the other hand, Product_Category_1 has a negative correlation with our target with the value -0.3437 which is somehow odd","79d64b8e":"# 6.Exporting Data","863397d5":"### It seems like our target variable has an almost Gaussian distribution\/ Normal Distribution.\n# Finding Skewness and Curtosis","f9bd9879":"# 1 - Exploaratory Data Analysis\n ## Distribution of the target variable: Purchase","84469361":"# Imputing the value Zero","6c935882":"# Distribution of the variable Occupation\nAs seen in the beginning, Occupation has at least 20 different values. Since we do not known to each occupation each number corresponds, is difficult to make any analysis.","017c9ca6":"# Distribution of the variable Product_Category_3","be918f75":"# Bivariate Analysis\u00b6\nnow it is time to understand the relationship between our target variable and predictors as well as the relationship among predictors.\n## 1 -Occupation and Purchase analysis\nAlthough there are some occupations which have higher representations, it seems that the amount each user spends on average is more or less the same for all occupations. Of course, in the end, occupations with the highest representations will have the highest amounts of purchases.","230f0451":"# 3. Feature Engineering\u00b6\n## 1. Converting gender to binary","e91c14fe":"# Distribution of the variable Product_Category_1\nFrom the distribution for products from category one, it is clear that three products stand out, number 1, 5 and 8. Unfortunately, we do not know which product each number represents.","0601f5a1":"# 2 - Marital_Status and Purchase Analysis\nWe had more single customers than married. However, on average an individual customer tends to spend the same amount independently if his\/her is married or not","f74253e1":"# 4. Converting Stay_In_Current_City_Years to binary","11765fcc":"# 3. Converting city_category to binary","deed2f82":"# Distribution of the variable Age","8054f80a":"# Check for duplicates","e2309b7b":"# 5 - Product_Category_3 and Purchase Analysis","979caf96":"# 8 - City_Category and Purchase analysis\nWe saw previously that city type \u2018B\u2019 had the highest number of purchases registered. However, the city whose buyers spend the most is city type \u2018C\u2019.","afd65820":"# 9 - Stay_In_Current_City_Years and Purchase analysis\nAgain, we see the same pattern seen before which show that on average people tend to spend the same amount on purchases regardeless of their group. People who are new in city are responsible for the higher number of purchase, however looking at it individually they tend to spend the same amount independently of how many years the have lived in their current city.","1ae37335":"Now that we\u2019ve analysed our target variable, let\u2019s consider our predictors(IV). Let\u2019s start by seeing which of our features are numeric.","81188d15":"# 7 -Age and Purchase Analysis\nTotal amount spent in purchase is in accordance with the number of purchases made, distributed by age.","d612cbda":"# Understanding the Data","cdd513d5":"# Checking the Null Values","8ff24c84":"# Analysis step\nTrying to identify the most important variables and defining the best regression model for predicting target variable.\n\nHence, this analysis will be divided into five stages:\n\nExploratory data analysis (EDA);\n\nData Pre-processing;\n\nFeature engineering;\n\nFeature Transformation;\n\nModeling;\n\nHyperparameter tuning\n\nEnsembling.","136d404e":"# Black-Friday-Regression-Analysis\nPredicting Prices for the products to be sold on Black Friday in US using Regression Analysis, Feature Engineering, Feature Selection, Feature Extraction and Data analysis - Data Visualizations.\n\nDescription The dataset here is a sample of the transactions made in a retail store. The store wants to know better the customer purchase behaviour against different products. Specifically, here the problem is a regression problem where we are trying to predict the dependent variable (the amount of purchase) with the help of the information contained in the other variables.\n\nClassification problem can also be settled in this dataset since several variables are categorical, and some other approaches could be \"Predicting the age of the consumer\" or even \"Predict the category of goods bought\". This dataset is also particularly convenient for clustering and maybe find different clusters of consumers within it.","ce6cc728":"# 4 - Product_Category_2 and Purchase Analysis","41d858ac":"# 3 - Product_category_1and Purchase analysis\nIf you see the value spent on average for Product_Category_1 you see that although there were more products bought for categories 1,5,8 the average amount spent for those three is not the highest. It is interesting to see other categories appearing with high purchase values despite having low impact on sales number.","a17fc1e0":"# 2 - Data Pre-Processing\nUsually, datasets for every challenge such as those presented in Analytics Vidhya or Kaggle come seperated as a train.csv and a test.csv. It is generally a good idea to combine both sets into one, in order to perform data cleaning and feature engineering and later divide them again. With this step we do not have to go through the trouble of repeting twice the same code, for both datasets. Let\u2019 s combine them into a dataframe datawith a sourcecolumn specifying where each observation belongs","ba92022e":"##### There seems to be no multicollinearity with our predictors which is a good thing, although there is some correlation among the product categories","7d0087f2":"# Analysation of Data\nIf we analyse it individually we see that we do not have any information regarding the stores. Moreover, there is some information related to the customer such as age group, sex, occupation and marital status. On the other hand, we have data on the city\u2019s size and how many years the customer has lived in it whereas on the product\u2019s side there is only information regarding the categories and the amount spent. It is my belief that Gender , Age , City_Category , Product_Category_1 are the predictors that will influence more the amount spent by a customer on this day.\n\n# Explanation of feature\nAge : should be treated as numerical. It presents age groups.\n\nCity_Category: We can convert this to numerical as well, with dummy variables. Should take a look at the frequency of the values\n\nOccupation : It seems like it has at least 16 different values, should see frequency and try to decrease this value.\n\nGender: There are possibly two gender, we can make this binary.\n\nProduct_ID: Should see if the string \u201cP\u201d means something and if there are other values.\n\nStay_In_Current_City_Years: We should deal with the \u2018+\u2019 symbol.\n\nProduct_Category_2 and Product_Category_3 : Have NaN values."}}