{"cell_type":{"4cdb1a11":"code","911953e4":"code","867db43f":"code","06fbf6f5":"code","53211307":"code","ce3b1978":"code","7bf768df":"code","7336a0ab":"code","a533155c":"code","e0002be7":"code","33a647bd":"code","046bb321":"code","3f76cd03":"code","934788ec":"code","e89898d5":"code","29131824":"code","4078e303":"code","09177545":"code","5880d5d4":"code","051a20d6":"code","c7d74a37":"code","092f699a":"code","a18144ec":"code","3ec2bcac":"code","65de1be3":"code","a66420e9":"code","66764ac2":"code","26f15f3c":"code","1997c634":"code","f3eda1de":"code","a8d5f094":"code","b72df2d1":"markdown","cf53da9e":"markdown","0b8da0c7":"markdown","29116514":"markdown","8d88f6a5":"markdown","4655bdbb":"markdown","be6f5a8c":"markdown","1b9fc125":"markdown","d924f83a":"markdown","667e243a":"markdown","4bf1ab06":"markdown","b2131a10":"markdown","036370a9":"markdown","f0d60fe2":"markdown","55504836":"markdown","5cac966b":"markdown"},"source":{"4cdb1a11":"import os\nimport gc\nimport warnings\n\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Dense, Input\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom kaggle_datasets import KaggleDatasets\nimport transformers\nimport traitlets\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom tqdm.notebook import tqdm\nfrom tokenizers import BertWordPieceTokenizer\nfrom sklearn.metrics import roc_auc_score\n\nwarnings.simplefilter(\"ignore\")\n\nnp.random.seed(100)","911953e4":"def fast_encode(texts, tokenizer, chunk_size=256, maxlen=512):\n    tokenizer.enable_truncation(max_length=maxlen)\n    tokenizer.enable_padding(max_length=maxlen)\n    all_ids = []\n    \n    for i in tqdm(range(0, len(texts), chunk_size)):\n        text_chunk = texts[i:i+chunk_size].tolist()\n        encs = tokenizer.encode_batch(text_chunk)\n        all_ids.extend([enc.ids for enc in encs])\n    \n    return np.array(all_ids)","867db43f":"def build_model(transformer, loss='binary_crossentropy', max_len=512):\n    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n    emb = transformer(input_word_ids)[0]\n    _avg = tf.keras.layers.GlobalAveragePooling1D()(emb)\n    _max = tf.keras.layers.GlobalMaxPooling1D()(emb)\n    x = tf.keras.layers.Concatenate()([_avg, _max])\n    x = tf.keras.layers.Dropout(0.15)(x)\n    out = Dense(1, activation='sigmoid')(x)\n    \n    model = Model(inputs=input_word_ids, outputs=out)\n    model.compile(Adam(lr=3e-5), loss=loss, metrics=[tf.keras.metrics.AUC()])\n    \n    return model","06fbf6f5":"AUTO = tf.data.experimental.AUTOTUNE\n\n# Create strategy from tpu\ntpu = tf.distribute.cluster_resolver.TPUClusterResolver()\ntf.config.experimental_connect_to_cluster(tpu)\ntf.tpu.experimental.initialize_tpu_system(tpu)\nstrategy = tf.distribute.experimental.TPUStrategy(tpu)\n\n# Data access\n#GCS_DS_PATH = KaggleDatasets().get_gcs_path('kaggle\/input\/') ","53211307":"# First load the real tokenizer\ntokenizer = transformers.BertTokenizer.from_pretrained('bert-base-multilingual-uncased')","ce3b1978":"valid = pd.read_csv('\/kaggle\/input\/jigsaw-multilingual-toxic-comment-classification\/validation.csv')\ntest = pd.read_csv('\/kaggle\/input\/jigsaw-multilingual-toxic-comment-classification\/test.csv')\nsub = pd.read_csv('\/kaggle\/input\/jigsaw-multilingual-toxic-comment-classification\/sample_submission.csv')","7bf768df":"lang_densety = test.groupby('lang').count()['id']\/ len(test)\ntarget_densety = valid.groupby('toxic').count()['id']\/ len(valid)","7336a0ab":"lang_densety","a533155c":"target_densety","e0002be7":"N_SAMPLES = 500000\n\ntrain_dfs = []\nfor lang in ['es', 'it', 'pt', 'tr', 'ru', 'fr']:\n    _df = pd.read_csv(f\"\/kaggle\/input\/jigsaw-train-multilingual-coments-google-api\/jigsaw-toxic-comment-train-google-{lang}-cleaned.csv\")\n    \n    _df0 = _df.loc[_df.toxic == 0, :]\n    _df1 = _df.loc[_df.toxic == 1, :]\n    \n    n_samples = int(N_SAMPLES * lang_densety[lang])\n    \n    n_samples_0 = int(n_samples * target_densety[0])\n    n_samples_1 = int(n_samples * target_densety[1])\n    \n    _df0 = _df0.sample(n_samples_0)\n    _df1 = _df1.sample(n_samples_1)\n    \n    train_dfs.append(pd.concat([_df0, _df1], ignore_index=True).sample(n_samples_0 + n_samples_1))\n    \ntrain = pd.concat(train_dfs, ignore_index=True)\ntrain = train.sample(min(N_SAMPLES, len(train)))\ntrain.toxic = train.toxic.round().astype(int)","33a647bd":"train.groupby('toxic').count()['id']\/ len(train)","046bb321":"class TextTransformation:\n    def __call__(self, text: str, lang: str = None) -> tuple:\n        raise NotImplementedError('Abstarct')\n        \nclass LowerCaseTransformation(TextTransformation):\n    def __call__(self, text: str, lang: str = None) -> tuple:\n        return text.lower(), lang\n    \nclass PunctuationTransformation(TextTransformation):\n    def __call__(self, text: str, lang: str = None) -> tuple:\n        for p in '?!.,\"#$%\\'()*+-\/:;<=>@[\\\\]^_`{|}~' + '\u201c\u201d\u2019' +\"\/-'\" + \"&\" + \"\u00a1\u00bf\":\n            if '\u2019' in text:\n                text = text.replace('\u2019', f' \\' ')\n                \n            if '\u2019' in text:\n                text = text.replace('\u2019', f' \\' ')\n              \n            if '\u2014' in text:\n                text = text.replace('\u2014', f' - ')\n                \n            if '\u2013' in text:\n                text = text.replace('\u2013', f' - ')   \n              \n            if '\u201c' in text:\n                text = text.replace('\u201c', f' \" ')   \n                \n            if '\u00ab' in text:\n                text = text.replace('\u00ab', f' \" ')   \n                \n            if '\u00bb' in text:\n                text = text.replace('\u00bb', f' \" ')   \n            \n            if '\u201d' in text:\n                text = text.replace('\u201d', f' \" ') \n                \n            if '`' in text:\n                text = text.replace('`', f' \\' ')              \n\n            text = text.replace(p, f' {p} ')\n                \n        return text.strip(), lang\n    \nclass NumericTransformation(TextTransformation):\n    def __call__(self, text: str, lang: str = None) -> tuple:\n        for i in range(10):\n            text = text.replace(str(i), f' {str(i)} ')\n        return text, lang\n    \nclass ETransformation(TextTransformation):\n    def __call__(self, text: str, lang: str = None) -> tuple:\n        for i in ['\\u00E8', '\\u00E9', '\\u00EA', '\\u00EB', '\\u0450', '\\u0451']:\n            text = text.replace(i, 'e')\n        return text, lang\n    \nclass ATransformation(TextTransformation):\n    def __call__(self, text: str, lang: str = None) -> tuple:\n        for i in ['\u00e0', '\u00e1', '\\u00E2', '\\u00E3', '\\u00E4', '\\u00E5']:\n            text = text.replace(i, 'a')\n        return text, lang\n    \nclass OTransformation(TextTransformation):\n    def __call__(self, text: str, lang: str = None) -> tuple:\n        for i in ['\u00f3', '\u00f2', '\u00f6','\u00f5', '\u00f4']:\n            text = text.replace(i, 'o')\n        return text, lang\n    \n\nclass CTransformation(TextTransformation):\n    def __call__(self, text: str, lang: str = None) -> tuple:\n        for i in ['\u00e7']:\n            text = text.replace(i, 'c')\n        return text, lang\n    \nclass ITransformation(TextTransformation):\n    def __call__(self, text: str, lang: str = None) -> tuple:\n        for i in ['\u00ed', '\u0131', '\u00ec']:\n            text = text.replace(i, 'i')\n        return text, lang\n    \nclass STransformation(TextTransformation):\n    def __call__(self, text: str, lang: str = None) -> tuple:\n        for i in ['\u015f']:\n            text = text.replace(i, 's')\n        return text, lang\n    \n    \nclass NTransformation(TextTransformation):\n    def __call__(self, text: str, lang: str = None) -> tuple:\n        for i in ['\u00f1', 'n']:\n            text = text.replace(i, 'n')\n        return text, lang\n    \n    \nclass UTransformation(TextTransformation):\n    def __call__(self, text: str, lang: str = None) -> tuple:\n        for i in ['\u00f9', '\u00fc', '\u00fb', '\u00fa']:\n            text = text.replace(i, 'u')\n        return text, lang\n    \nclass GTransformation(TextTransformation):\n    def __call__(self, text: str, lang: str = None) -> tuple:\n        for i in ['\u011f']:\n            text = text.replace(i, 'g')\n        return text, lang\n\nclass RTransformation(TextTransformation):\n    def __call__(self, text: str, lang: str = None) -> tuple:\n        for i in ['r']:\n            text = text.replace(i, 'r')\n        return text, lang\n    \nclass WikiTransformation(TextTransformation):\n    def __call__(self, text: str, lang: str = None) -> tuple:\n        text = text.replace('wikiproject', ' wiki project ')\n        for i in [' vikipedi ', ' wiki ', ' \u0432\u0438\u043a\u0438\u043f\u0435\u0434\u0438\u0438 ', \" \u0432\u0438\u043a\u0438 \", ' \u0432\u0438\u043a\u0438\u043f\u0435\u0434\u0438\u044f ', ' viki ', ' wikipedien ', ' \u0432\u0438\u043a\u0438\u043f\u0435\u0434\u0438\u044e ']:\n            text = text.replace(i, ' wikipedia ')\n        return text, lang\n    \nclass PixelTransformation(TextTransformation):\n    def __call__(self, text: str, lang: str = None) -> tuple:\n        for i in [' px ']:\n            text = text.replace(i, ' pixel ')\n        return text, lang\n    \n    \nclass RuTransformation(TextTransformation):\n    def __call__(self, text: str, lang: str = None) -> tuple:\n        if lang is not None and lang == 'ru' and 'http' not in text and 'jpg' not in text and 'wikipedia' not in text:\n            text = text.replace('t', '\u0442')\n            text = text.replace('h', '\u043d')\n            text = text.replace('b', '\u0432')\n            text = text.replace('c', 'c')\n            text = text.replace('k', '\u043a')\n            text = text.replace('e', '\u0435')\n            text = text.replace('a', '\u0430')\n        return text, lang\n    \n    \nclass CombineTransformation(TextTransformation):\n    def __init__(self, transformations: list):\n        self._transformations = transformations\n        \n    def __call__(self, text: str, lang: str = None) -> tuple:\n        for transformation in self._transformations:\n            text, lang = transformation(text, lang)\n        return text, lang\n    \n    def append(self, transformation: TextTransformation):\n        self._transformations.append(transformation)\n        \n        \ntransformer = CombineTransformation(\n    [\n        LowerCaseTransformation(),\n        PunctuationTransformation(),\n        NumericTransformation(),\n        ETransformation(),\n        ATransformation(),\n        OTransformation(),\n        CTransformation(),\n        ITransformation(),\n        STransformation(),\n        UTransformation(),\n        GTransformation(),\n        NTransformation(),\n        WikiTransformation(),\n        PixelTransformation(),\n        RuTransformation()\n    ]\n)","3f76cd03":"train['comment_text'] = [v[0] for v in train.apply(lambda x: transformer(x.comment_text), axis=1).values]\nvalid['comment_text'] = [v[0] for v in valid.apply(lambda x: transformer(x.comment_text, x.lang), axis=1).values]\ntest['content'] = [v[0] for v in test.apply(lambda x: transformer(x.content, x.lang), axis=1).values]","934788ec":"sentences = train[\"comment_text\"].apply(lambda x: x.split()).values.tolist() + valid[\"comment_text\"].apply(lambda x: x.split()).values.tolist() + test['content'].apply(lambda x: x.split()).values.tolist()","e89898d5":"import operator \nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndef unknown_plot(data):\n    fig, axes = plt.subplots(ncols=1, figsize=(10, 20))\n    plt.tight_layout()\n    \n    sns.barplot(y=list(data.keys()), x=list(data.values()), ax=axes, color='green')\n\n    axes.spines['right'].set_visible(False)\n    axes.set_xlabel('')\n    axes.set_ylabel('')\n    axes.tick_params(axis='x', labelsize=13)\n    axes.tick_params(axis='y', labelsize=13)\n\n    axes.set_title(f'Most unknown tokens', fontsize=15)\n\n    plt.show()\n    \ndef check_coverage(vocab,embeddings_index):\n    a = {}\n    oov = {}\n    k = 0\n    i = 0\n    for word in tqdm(vocab):\n        try:\n            a[word] = embeddings_index[word]\n            k += vocab[word]\n        except:\n\n            oov[word] = vocab[word]\n            i += vocab[word]\n            pass\n\n    print('Found embeddings for {:.2%} of vocab'.format(len(a) \/ len(vocab)))\n    print('Found embeddings for  {:.2%} of all text'.format(k \/ (k + i)))\n    sorted_x = sorted(oov.items(), key=operator.itemgetter(1))[::-1]\n\n    return sorted_x\n\n\ndef build_vocab(sentences, verbose =  True):\n    \"\"\"\n    :param sentences: list of list of words\n    :return: dictionary of words and their count\n    \"\"\"\n    vocab = {}\n    for sentence in tqdm(sentences, disable = (not verbose)):\n        for word in sentence:\n            try:\n                vocab[word] += 1\n            except KeyError:\n                vocab[word] = 1\n    return vocab\n\n\nvocab = build_vocab(sentences)\nunknown_vocab = check_coverage(vocab, tokenizer.get_vocab())\nlen(unknown_vocab)","29131824":"unknown_plot({k:v for i, (k,v) in enumerate(unknown_vocab) if i < 100})","4078e303":"del sentences","09177545":"# Save the loaded tokenizer locally\nsave_path = '\/kaggle\/working\/distilbert_base_uncased\/'\nif not os.path.exists(save_path):\n    os.makedirs(save_path)\ntokenizer.save_pretrained(save_path)\n\n# Reload it with the huggingface tokenizers library\nfast_tokenizer = BertWordPieceTokenizer('distilbert_base_uncased\/vocab.txt', lowercase=True)\nfast_tokenizer","5880d5d4":"x_train = fast_encode(train.comment_text.astype(str), fast_tokenizer, maxlen=512)\nx_valid = fast_encode(valid.comment_text.astype(str), fast_tokenizer, maxlen=512)\nx_test = fast_encode(test.content.astype(str), fast_tokenizer, maxlen=512)\ny_train = train.toxic.values\ny_valid = valid.toxic.values\n\ndel train_dfs, train, valid, test\ngc.collect()","051a20d6":"train_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices((x_train, y_train))\n    .repeat()\n    .shuffle(2048)\n    .batch(64)\n    .prefetch(AUTO)\n)\n\nvalid_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices((x_valid, y_valid))\n    .batch(64)\n    .cache()\n    .prefetch(AUTO)\n)\n\ntest_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices(x_test)\n    .batch(64)\n)\n","c7d74a37":"from tensorflow.keras import backend as K\n\ndef focal_loss(gamma=2., alpha=.15):\n    def focal_loss_fixed(y_true, y_pred):\n        pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))\n        pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))\n        return -K.mean(alpha * K.pow(1. - pt_1, gamma) * K.log(pt_1)) - K.mean((1 - alpha) * K.pow(pt_0, gamma) * K.log(1. - pt_0))\n    return focal_loss_fixed","092f699a":"%%time\nwith strategy.scope():\n    transformer_layer = transformers.TFBertModel.from_pretrained('bert-base-multilingual-uncased')\n    model = build_model(transformer_layer, loss=focal_loss(gamma=1.5), max_len=512)\nmodel.summary()","a18144ec":"from tensorflow.keras.callbacks import Callback \n\nclass RocAucCallback(Callback):\n    def __init__(self, test_data, score_thr):\n        self.test_data = test_data\n        self.score_thr = score_thr\n        self.test_pred = []\n        \n    def on_epoch_end(self, epoch, logs=None):\n        if logs['val_auc'] > self.score_thr:\n            print('\\nRun TTA...')\n            self.test_pred.append(self.model.predict(self.test_data))","3ec2bcac":"def build_lrfn(lr_start=0.000001, lr_max=0.000004, \n               lr_min=0.0000001, lr_rampup_epochs=5, \n               lr_sustain_epochs=3, lr_exp_decay=.87):\n    lr_max = lr_max * strategy.num_replicas_in_sync\n\n    def lrfn(epoch):\n        if epoch < lr_rampup_epochs:\n            lr = (lr_max - lr_start) \/ lr_rampup_epochs * epoch + lr_start\n        elif epoch < lr_rampup_epochs + lr_sustain_epochs:\n            lr = lr_max\n        else:\n            lr = (lr_max - lr_min) * lr_exp_decay**(epoch - lr_rampup_epochs - lr_sustain_epochs) + lr_min\n        return lr\n    \n    return lrfn","65de1be3":"import matplotlib.pyplot as plt\n\nplt.figure(figsize=(10, 7))\n\n_lrfn = build_lrfn()\nplt.plot([i for i in range(35)], [_lrfn(i) for i in range(35)]);","a66420e9":"lrfn = build_lrfn()\nlr_schedule = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose=1)\ner = tf.keras.callbacks.EarlyStopping(monitor='val_auc', patience=20, restore_best_weights=True, mode='max')\n\ntrain_history = model.fit(\n    train_dataset,\n    steps_per_epoch=200,\n    validation_data=valid_dataset,\n    callbacks=[lr_schedule, er],\n    epochs=35\n)","66764ac2":"bert_weights = model.get_weights()","26f15f3c":"import matplotlib.pyplot as plt\n\nplt.figure(figsize=(10, 7))\n\nlrfn = build_lrfn(lr_start=0.000001, lr_max=0.000001, \n               lr_min=0.0000001, lr_rampup_epochs=2, \n               lr_sustain_epochs=1, lr_exp_decay=.65)\nplt.plot([i for i in range(15)], [lrfn(i) for i in range(15)]);","1997c634":"from sklearn.model_selection import train_test_split, StratifiedKFold","f3eda1de":"skf = StratifiedKFold(n_splits=5)\n\ntest_preds = np.zeros((len(x_test),))\n\nfor tr_idx, vl_idx in skf.split(x_valid, y_valid):\n    lr_schedule = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose=1)\n    er = tf.keras.callbacks.EarlyStopping(monitor='val_auc', patience=20, restore_best_weights=True, mode='max')\n    x_tr, x_val, y_tr, y_val = x_valid[tr_idx], x_valid[vl_idx], y_valid[tr_idx], y_valid[vl_idx]\n\n    train_dataset = (\n        tf.data.Dataset\n        .from_tensor_slices((x_tr, y_tr))\n        .repeat()\n        .shuffle(2048)\n        .batch(32)\n        .prefetch(AUTO)\n    )\n\n    valid_dataset = (\n        tf.data.Dataset\n        .from_tensor_slices((x_val, y_val))\n        .batch(32)\n        .cache()\n        .prefetch(AUTO)\n    )\n    \n    model.set_weights(bert_weights)\n    train_history = model.fit(\n        train_dataset,\n        steps_per_epoch=40,\n        validation_data=valid_dataset,\n        callbacks=[lr_schedule, er],\n        epochs=15)\n    test_preds += model.predict(test_dataset)[:, 0] \/ 5","a8d5f094":"sub['toxic'] = test_preds\nsub.to_csv('submission.csv', index=False)","b72df2d1":"# Focal Loss","cf53da9e":"## Build datasets objects","0b8da0c7":"# LrScheduler","29116514":"### Second Stage","8d88f6a5":"## About this notebook\n\n*[Jigsaw Multilingual Toxic Comment Classification](https:\/\/www.kaggle.com\/c\/jigsaw-multilingual-toxic-comment-classification)* is the 3rd annual competition organized by the Jigsaw team. It follows *[Toxic Comment Classification Challenge](https:\/\/www.kaggle.com\/c\/jigsaw-toxic-comment-classification-challenge)*, the original 2018 competition, and *[Jigsaw Unintended Bias in Toxicity Classification](https:\/\/www.kaggle.com\/c\/jigsaw-unintended-bias-in-toxicity-classification)*, which required the competitors to consider biased ML predictions in their new models. This year, the goal is to use english only training data to run toxicity predictions on many different languages, which can be done using multilingual models, and speed up using TPUs.\n\nMany awesome notebooks has already been made so far. Many of them used really cool technologies like [Pytorch XLA](https:\/\/www.kaggle.com\/theoviel\/bert-pytorch-huggingface-starter). This notebook instead aims at constructing a **fast, concise, reusable, and beginner-friendly model scaffold**. It will focus on the following points:\n* **Using Tensorflow and Keras**: Tensorflow is a powerful framework, and Keras makes the training process extremely easy to understand. This is especially good for beginners to learn how to use TPUs, and for experts to focus on the modelling aspect.\n* **Using Huggingface's `transformers` library**: [This library](https:\/\/huggingface.co\/transformers\/) is extremely popular, so using this let you easily integrate the end result into your ML pipelines, and can be easily reused for your other projects.\n* **Native TPU usage**: The TPU usage is abstracted using the native `strategy` that was created using Tensorflow's `tf.distribute.experimental.TPUStrategy`. This avoids getting too much into the lower-level aspect of TPU management.\n* **Use a subset of the data**: Instead of using the entire dataset, we will only use the 2018 subset of the data available, which makes this much faster, all while achieving a respectable accuracy.","4655bdbb":"## Create fast tokenizer","be6f5a8c":"### First Stage","1b9fc125":"## Helper Functions","d924f83a":"## Load text data into memory","667e243a":"## Fast encode","4bf1ab06":"## Train Model","b2131a10":"## Submission","036370a9":"## Load model into the TPU","f0d60fe2":"## TPU Configs","55504836":"## RocAuc Callback","5cac966b":"# Reference\n* [Jigsaw TPU: DistilBERT with Huggingface and Keras](https:\/\/www.kaggle.com\/xhlulu\/jigsaw-tpu-distilbert-with-huggingface-and-keras)\n* [inference of bert tpu model ml w\/ validation](https:\/\/www.kaggle.com\/abhishek\/inference-of-bert-tpu-model-ml-w-validation)\n* [Overview of Text Similarity Metrics in Python](https:\/\/towardsdatascience.com\/overview-of-text-similarity-metrics-3397c4601f50)\n* [test-en-df](https:\/\/www.kaggle.com\/bamps53\/test-en-df)\n* [val_en_df](https:\/\/www.kaggle.com\/bamps53\/val-en-df)\n* [Jigsaw multilingual toxic - test translated](https:\/\/www.kaggle.com\/kashnitsky\/jigsaw-multilingual-toxic-test-translated)"}}