{"cell_type":{"01281afc":"code","884f3d0e":"code","2f779090":"code","769074b9":"code","ce0a2b1e":"code","27dba461":"code","91aca810":"code","b5f02b60":"code","644959d9":"code","63b7ef0e":"code","00c39d9a":"code","69f97437":"code","185acf29":"code","bda625be":"code","39e4e1ce":"code","c2b042f8":"code","e79ca3e2":"code","09a47246":"code","1163feab":"code","25e3c4b0":"code","b35c1e4e":"code","c695a763":"code","3914922e":"code","d351d40e":"code","f46d523a":"code","fc2c9366":"code","6d6cebf1":"code","597b16c3":"code","1c139a92":"code","9f1d77cc":"code","d1ae36aa":"code","14f90d12":"code","2b24f501":"code","5a8b8e0e":"code","e5064c2c":"code","9fa2159d":"code","655eec93":"code","f8bc9909":"code","ed2ef3cb":"code","a00ec323":"code","4016d11b":"code","74c0b72b":"code","bca4b684":"code","3a6f9b00":"code","2355abf4":"code","63f9ad5a":"code","2a855b51":"code","1184068e":"code","a94693de":"code","7c35999b":"code","78c517b8":"code","419c9f91":"code","2d02d45b":"code","7b4bbd92":"code","bcf229e2":"code","3b34de6c":"code","7ecd4016":"code","d9b11c11":"code","429bf4c0":"code","dc99d75d":"code","7eb5ec75":"code","029cd77a":"code","6544a654":"code","5c5ea2a9":"code","89cfff8e":"code","f984ea4d":"code","971239f8":"code","e043246f":"code","3b56e3b4":"code","18234ce0":"code","47a86fe8":"code","ff8a0c03":"code","c761ddb5":"code","abbc392b":"code","c7a777fe":"code","8074f754":"code","c76e8a2c":"code","6f4980fe":"code","0f57997c":"code","72e2c9ea":"code","05ca8863":"code","c983d094":"code","1dd2f6ca":"code","74d30c42":"code","047758f8":"code","0526ed6f":"code","29a3d4cb":"code","6edccc6b":"code","50d7b40d":"code","8b55d9a4":"code","787c8116":"code","71454d80":"code","195a86db":"markdown","9d33218a":"markdown","cf202547":"markdown","b50ba549":"markdown","99f01de4":"markdown","dd4d5da2":"markdown","4bd31d54":"markdown","eb8a6c71":"markdown","12be5a20":"markdown","8655a390":"markdown","055740cd":"markdown","af9311e1":"markdown","64d94c8c":"markdown"},"source":{"01281afc":"%matplotlib inline\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport shutil\nimport os\nimport pandas as pd\nimport matplotlib\nmatplotlib.use(u'nbAgg')\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nimport pickle\nfrom sklearn.manifold import TSNE\nfrom sklearn import preprocessing\nfrom tqdm import tqdm\nimport pandas as pd\nfrom multiprocessing import Process# this is used for multithreading\nimport multiprocessing\nimport codecs# this is used for file operations \nimport random as r\nfrom xgboost import XGBClassifier\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.calibration import CalibratedClassifierCV\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import log_loss\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom nltk import word_tokenize \nfrom nltk.util import ngrams\nimport h5py\nimport copy","884f3d0e":"source_directory_path = \"train\"\ndestination_directory_path = \"byteFiles\"\nfrom tqdm import tqdm\n\n# we will check if the folder 'byteFiles' exists if it not there we will create a folder with the same name\nif not os.path.isdir(destination_directory_path):\n    os.makedirs(destination_directory_path)\n\n# if we have folder called 'train' (train folder contains both .asm files and .bytes files) we will rename it 'asmFiles'\n# for every file that we have in our 'asmFiles' directory we check if it is ending with .bytes, if yes we will move it to\n# 'byteFiles' folder\n\n\n# so by the end of this snippet we will separate all the .byte files and .asm files\n\nfor source_filename in tqdm(os.listdir(source_directory_path)):\n    \n    if source_filename.endswith(\".bytes\"):\n        # construct path from filename\n        \n        source_file_path = os.path.join(source_directory_path, source_filename)\n\n        shutil.copy(source_file_path, destination_directory_path)","2f779090":"Y=pd.read_csv(\"trainLabels.csv\")\ntotal = len(Y)*1.\nax=sns.countplot(x=\"Class\", data=Y)\nfor p in ax.patches:\n        ax.annotate('{:.1f}%'.format(100*p.get_height()\/total), (p.get_x()+0.1, p.get_height()+5))\n\n#put 11 ticks (therefore 10 steps), from 0 to the total number of rows in the dataframe\nax.yaxis.set_ticks(np.linspace(0, total, 11))\n\n#adjust the ticklabel to the desired format, without changing the position of the ticks. \nax.set_yticklabels(map('{:.1f}%'.format, 100*ax.yaxis.get_majorticklocs()\/total))\nplt.show()","769074b9":"files=os.listdir('byteFiles')[:7000]","ce0a2b1e":"\nfilenames=Y['Id'].tolist()\nclass_y=Y['Class'].tolist()\nclass_bytes=[]\nsizebytes=[]\nfnames=[]\nfor file in files:\n    # print(os.stat('byteFiles\/0A32eTdBKayjCWhZqDOQ.txt'))\n    # os.stat_result(st_mode=33206, st_ino=1125899906874507, st_dev=3561571700, st_nlink=1, st_uid=0, st_gid=0, \n    # st_size=3680109, st_atime=1519638522, st_mtime=1519638522, st_ctime=1519638522)\n    # read more about os.stat: here https:\/\/www.tutorialspoint.com\/python\/os_stat.htm\n    statinfo=os.stat('byteFiles\/'+file)\n    # split the file name at '.' and take the first part of it i.e the file name\n    file=file.split('.')[0]\n    if any(file == filename for filename in filenames):\n        i=filenames.index(file)\n        class_bytes.append(class_y[i])\n        # converting into Mb's\n        sizebytes.append(statinfo.st_size\/(1024.0*1024.0))\n        fnames.append(file)\ndata_size_byte=pd.DataFrame({'ID':fnames,'size':sizebytes,'Class':class_bytes})\nprint (data_size_byte.head())","27dba461":"normalized_df=(data_size_byte['size']-data_size_byte['size'].mean())\/data_size_byte['size'].std()","91aca810":"a= Normalizer().fit_transform(data_size_byte.iloc[:,1])","b5f02b60":"ax = sns.boxplot(x=\"Class\", y=\"size\", data=data_size_byte)\nplt.title(\"boxplot of .bytes file sizes\")\nplt.show()","644959d9":"#files = os.listdir('byteFiles')\na=[]\nID=[]\nfor file in tqdm(files):\n    with open('byteFiles\/'+file,\"r\") as byte_file:\n        j=[]\n        file=file.split('.')[0]\n        ID.append(file)\n        for lines in byte_file:\n            line=lines.rstrip().split(\" \")\n            line.pop(0)\n            j.extend(line)\n    c=' '.join(x for x in j)\n    a.append(c)\nbyte_file.close()","63b7ef0e":"import pickle","00c39d9a":"with open('mypickle.pickle', 'wb') as f:\n    pickle.dump(a, f)","69f97437":"from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer \nfrom nltk.corpus import stopwords \n\nfrom nltk.tokenize import word_tokenize ","185acf29":"vectorizer = CountVectorizer(ngram_range = (2,2)) \nX1 = vectorizer.fit_transform(a)  \nfeatures = (vectorizer.get_feature_names()) ","bda625be":"with open('myfeature.pickle', 'wb') as f:\n    pickle.dump(features, f)","39e4e1ce":"with open('myvalues.pickle','wb') as v:\n    pickle.dump(X1,v)","c2b042f8":"ID=[]\nfor i in files:\n    i=i.split('.')[0]\n    ID.append(i)","e79ca3e2":"pickle_in = open(\"myfeature.pickle\",\"rb\")\nfeatures = pickle.load(pickle_in)","09a47246":"pickle_in = open(\"myvalues.pickle\",\"rb\")\nValues = pickle.load(pickle_in)","1163feab":"df=pd.DataFrame(ID,columns=['ID'])","25e3c4b0":"Values=Values.todense()","b35c1e4e":"bigram_Data=pd.DataFrame(Values,columns=features)","c695a763":"bigram_Data.head()","3914922e":"from sklearn.preprocessing import Normalizer\nbigram_data= Normalizer().fit_transform(bigram_Data)","d351d40e":"bigram_data[0]","f46d523a":"data=pd.DataFrame(bigram_data,columns=features)","fc2c9366":"data.head()","6d6cebf1":"result = pd.concat([df, data], axis=1, sort=False)\n","597b16c3":"result.head()\n","1c139a92":"final_result = pd.merge(result, data_size_byte, on='ID',how='left')\nfinal_result.head()","9f1d77cc":"final_result['size']=normalized_df","d1ae36aa":"final_result.head()","14f90d12":"with open('final_result.pickle', 'wb') as f:\n    pickle.dump(final_result, f)","2b24f501":"data_y=final_result['Class']\ndata_y","5a8b8e0e":"pickle_in = open(\"final_result.pickle\",\"rb\")\nfinal_result = pickle.load(pickle_in)","e5064c2c":"data_y=final_result['Class']","9fa2159d":"final_result.head()","655eec93":"xtsne=TSNE(perplexity=50)\n\nresults=xtsne.fit_transform(final_result.drop(['ID','Class'],axis=1))\n","f8bc9909":"vis_x = results[:, 0]\nvis_y = results[:, 1]\nplt.figure(figsize=(12.8, 9.6))\nplt.scatter(vis_x, vis_y, c=data_y, cmap=plt.cm.get_cmap(\"jet\", 9))\nplt.colorbar(ticks=range(10))\nplt.clim(0.5, 9)\nplt.show()","ed2ef3cb":"X_train, X_test, y_train, y_test = train_test_split(final_result.drop(['ID','Class'], axis=1), data_y,stratify=data_y,test_size=0.20)\n# split the train data into train and cross validation by maintaining same distribution of output varaible 'y_train' [stratify=y_train]\nX_train, X_cv, y_train, y_cv = train_test_split(X_train, y_train,stratify=y_train,test_size=0.20)","a00ec323":"train_class_distribution = y_train.value_counts().sortlevel()\ntest_class_distribution = y_test.value_counts().sortlevel()\ncv_class_distribution = y_cv.value_counts().sortlevel()\n\n#my_colors = 'rgbkymc'\ntrain_class_distribution.plot(kind='bar')\nplt.xlabel('Class')\nplt.ylabel('Data points per Class')\nplt.title('Distribution of yi in train data')\nplt.grid()\nplt.show()\n\n# ref: argsort https:\/\/docs.scipy.org\/doc\/numpy\/reference\/generated\/numpy.argsort.html\n# -(train_class_distribution.values): the minus sign will give us in decreasing order\nsorted_yi = np.argsort(-train_class_distribution.values)\nfor i in sorted_yi:\n    print('Number of data points in class', i+1, ':',train_class_distribution.values[i], '(', np.round((train_class_distribution.values[i]\/y_train.shape[0]*100), 3), '%)')\n\n    \nprint('-'*80)\n#my_colors = 'rgbkymc'\ntest_class_distribution.plot(kind='bar')\nplt.xlabel('Class')\nplt.ylabel('Data points per Class')\nplt.title('Distribution of yi in test data')\nplt.grid()\nplt.show()\n\n# ref: argsort https:\/\/docs.scipy.org\/doc\/numpy\/reference\/generated\/numpy.argsort.html\n# -(train_class_distribution.values): the minus sign will give us in decreasing order\nsorted_yi = np.argsort(-test_class_distribution.values)\nfor i in sorted_yi:\n    print('Number of data points in class', i+1, ':',test_class_distribution.values[i], '(', np.round((test_class_distribution.values[i]\/y_test.shape[0]*100), 3), '%)')\n\nprint('-'*80)\n#my_colors = 'rgbkymc'\ncv_class_distribution.plot(kind='bar')\nplt.xlabel('Class')\nplt.ylabel('Data points per Class')\nplt.title('Distribution of yi in cross validation data')\nplt.grid()\nplt.show()\n\n# ref: argsort https:\/\/docs.scipy.org\/doc\/numpy\/reference\/generated\/numpy.argsort.html\n# -(train_class_distribution.values): the minus sign will give us in decreasing order\nsorted_yi = np.argsort(-train_class_distribution.values)\nfor i in sorted_yi:\n    print('Number of data points in class', i+1, ':',cv_class_distribution.values[i], '(', np.round((cv_class_distribution.values[i]\/y_cv.shape[0]*100), 3), '%)')\n","4016d11b":"def plot_confusion_matrix(test_y, predict_y):\n    C = confusion_matrix(test_y, predict_y)\n    print(\"Number of misclassified points \",(len(test_y)-np.trace(C))\/len(test_y)*100)\n    # C = 9,9 matrix, each cell (i,j) represents number of points of class i are predicted class j\n    \n    A =(((C.T)\/(C.sum(axis=1))).T)\n    #divid each element of the confusion matrix with the sum of elements in that column\n    \n    # C = [[1, 2],\n    #     [3, 4]]\n    # C.T = [[1, 3],\n    #        [2, 4]]\n    # C.sum(axis = 1)  axis=0 corresonds to columns and axis=1 corresponds to rows in two diamensional array\n    # C.sum(axix =1) = [[3, 7]]\n    # ((C.T)\/(C.sum(axis=1))) = [[1\/3, 3\/7]\n    #                           [2\/3, 4\/7]]\n\n    # ((C.T)\/(C.sum(axis=1))).T = [[1\/3, 2\/3]\n    #                           [3\/7, 4\/7]]\n    # sum of row elements = 1\n    \n    B =(C\/C.sum(axis=0))\n    #divid each element of the confusion matrix with the sum of elements in that row\n    # C = [[1, 2],\n    #     [3, 4]]\n    # C.sum(axis = 0)  axis=0 corresonds to columns and axis=1 corresponds to rows in two diamensional array\n    # C.sum(axix =0) = [[4, 6]]\n    # (C\/C.sum(axis=0)) = [[1\/4, 2\/6],\n    #                      [3\/4, 4\/6]] \n    \n    labels = [1,2,3,4,5,6,7,8,9]\n    cmap=sns.light_palette(\"green\")\n    # representing A in heatmap format\n    print(\"-\"*50, \"Confusion matrix\", \"-\"*50)\n    plt.figure(figsize=(10,5))\n    sns.heatmap(C, annot=True, cmap=cmap, fmt=\".3f\", xticklabels=labels, yticklabels=labels)\n    plt.xlabel('Predicted Class')\n    plt.ylabel('Original Class')\n    plt.show()\n\n    print(\"-\"*50, \"Precision matrix\", \"-\"*50)\n    plt.figure(figsize=(10,5))\n    sns.heatmap(B, annot=True, cmap=cmap, fmt=\".3f\", xticklabels=labels, yticklabels=labels)\n    plt.xlabel('Predicted Class')\n    plt.ylabel('Original Class')\n    plt.show()\n    print(\"Sum of columns in precision matrix\",B.sum(axis=0))\n    \n    # representing B in heatmap format\n    print(\"-\"*50, \"Recall matrix\"    , \"-\"*50)\n    plt.figure(figsize=(10,5))\n    sns.heatmap(A, annot=True, cmap=cmap, fmt=\".3f\", xticklabels=labels, yticklabels=labels)\n    plt.xlabel('Predicted Class')\n    plt.ylabel('Original Class')\n    plt.show()\n    print(\"Sum of rows in precision matrix\",A.sum(axis=1))","74c0b72b":"alpha = [10 ** x for x in range(-5, 4)]\ncv_log_error_array=[]\nfor i in tqdm(alpha):\n    logisticR=LogisticRegression(penalty='l2',C=i,class_weight='balanced', n_jobs=-2)\n    logisticR.fit(X_train,y_train)\n    sig_clf = CalibratedClassifierCV(logisticR, method=\"sigmoid\")\n    sig_clf.fit(X_train, y_train)\n    predict_y = sig_clf.predict_proba(X_cv)\n    cv_log_error_array.append(log_loss(y_cv, predict_y, labels=logisticR.classes_, eps=1e-15))\n    \nfor i in range(len(cv_log_error_array)):\n    print ('log_loss for c = ',alpha[i],'is',cv_log_error_array[i])\n\nbest_alpha = np.argmin(cv_log_error_array)\n    \nfig, ax = plt.subplots()\nax.plot(alpha, cv_log_error_array,c='g')\nfor i, txt in enumerate(np.round(cv_log_error_array,3)):\n    ax.annotate((alpha[i],np.round(txt,3)), (alpha[i],cv_log_error_array[i]))\nplt.grid()\nplt.title(\"Cross Validation Error for each alpha\")\nplt.xlabel(\"Alpha i's\")\nplt.ylabel(\"Error measure\")\nplt.show()\n\nlogisticR=LogisticRegression(penalty='l2', C=alpha[best_alpha], class_weight='balanced', n_jobs=-2)\nlogisticR.fit(X_train,y_train)\nsig_clf = CalibratedClassifierCV(logisticR, method=\"sigmoid\")\nsig_clf.fit(X_train, y_train)\npred_y=sig_clf.predict(X_test)\n\npredict_y = sig_clf.predict_proba(X_train)\nprint ('log loss for train data',log_loss(y_train, predict_y, labels=logisticR.classes_, eps=1e-15))\npredict_y = sig_clf.predict_proba(X_cv)\nprint ('log loss for cv data',log_loss(y_cv, predict_y, labels=logisticR.classes_, eps=1e-15))\npredict_y = sig_clf.predict_proba(X_test)\nprint ('log loss for test data',log_loss(y_test, predict_y, labels=logisticR.classes_, eps=1e-15))\nplot_confusion_matrix(y_test, sig_clf.predict(X_test))","bca4b684":"alpha=[10,50,100,500,1000,2000,3000]\ncv_log_error_array=[]\ntrain_log_error_array=[]\nfrom sklearn.ensemble import RandomForestClassifier\nfor i in tqdm(alpha):\n    r_cfl=RandomForestClassifier(n_estimators=i,random_state=42,n_jobs=-2)\n    r_cfl.fit(X_train,y_train)\n    sig_clf = CalibratedClassifierCV(r_cfl, method=\"sigmoid\")\n    sig_clf.fit(X_train, y_train)\n    predict_y = sig_clf.predict_proba(X_cv)\n    cv_log_error_array.append(log_loss(y_cv, predict_y, labels=r_cfl.classes_, eps=1e-15))\n\nfor i in range(len(cv_log_error_array)):\n    print ('log_loss for c = ',alpha[i],'is',cv_log_error_array[i])\n\n\nbest_alpha = np.argmin(cv_log_error_array)\n\nfig, ax = plt.subplots()\nax.plot(alpha, cv_log_error_array,c='g')\nfor i, txt in enumerate(np.round(cv_log_error_array,3)):\n    ax.annotate((alpha[i],np.round(txt,3)), (alpha[i],cv_log_error_array[i]))\nplt.grid()\nplt.title(\"Cross Validation Error for each alpha\")\nplt.xlabel(\"Alpha i's\")\nplt.ylabel(\"Error measure\")\nplt.show()\n\n\nr_cfl=RandomForestClassifier(n_estimators=alpha[best_alpha],random_state=42,n_jobs=-2)\nr_cfl.fit(X_train,y_train)\nsig_clf = CalibratedClassifierCV(r_cfl, method=\"sigmoid\")\nsig_clf.fit(X_train, y_train)\n\n\npredict_y = sig_clf.predict_proba(X_train)\nprint('For values of best alpha = ', alpha[best_alpha], \"The train log loss is:\",log_loss(y_train, predict_y))\npredict_y = sig_clf.predict_proba(X_cv)\nprint('For values of best alpha = ', alpha[best_alpha], \"The cross validation log loss is:\",log_loss(y_cv, predict_y))\npredict_y = sig_clf.predict_proba(X_test)\nprint('For values of best alpha = ', alpha[best_alpha], \"The test log loss is:\",log_loss(y_test, predict_y))\nplot_confusion_matrix(y_test, sig_clf.predict(X_test))","3a6f9b00":"alpha=[10,50,100,500]\ncv_log_error_array=[]\nfor i in tqdm(alpha):\n    x_cfl=XGBClassifier(n_estimators=i,n_jobs=7)\n    x_cfl.fit(X_train,y_train)\n    sig_clf = CalibratedClassifierCV(x_cfl, method=\"sigmoid\")\n    sig_clf.fit(X_train, y_train)\n    predict_y = sig_clf.predict_proba(X_cv)\n    cv_log_error_array.append(log_loss(y_cv, predict_y, labels=x_cfl.classes_, eps=1e-15))\n\nfor i in range(len(cv_log_error_array)):\n    print ('log_loss for c = ',alpha[i],'is',cv_log_error_array[i])\n\n\nbest_alpha = np.argmin(cv_log_error_array)\n\nfig, ax = plt.subplots()\nax.plot(alpha, cv_log_error_array,c='g')\nfor i, txt in enumerate(np.round(cv_log_error_array,3)):\n    ax.annotate((alpha[i],np.round(txt,3)), (alpha[i],cv_log_error_array[i]))\nplt.grid()\nplt.title(\"Cross Validation Error for each alpha\")\nplt.xlabel(\"Alpha i's\")\nplt.ylabel(\"Error measure\")\nplt.show()\n\nx_cfl=XGBClassifier(n_estimators=alpha[best_alpha],n_jobs=7)\nx_cfl.fit(X_train,y_train)\nsig_clf = CalibratedClassifierCV(x_cfl, method=\"sigmoid\")\nsig_clf.fit(X_train, y_train)\n    \npredict_y = sig_clf.predict_proba(X_train)\nprint ('For values of best alpha = ', alpha[best_alpha], \"The train log loss is:\",log_loss(y_train, predict_y))\npredict_y = sig_clf.predict_proba(X_cv)\nprint('For values of best alpha = ', alpha[best_alpha], \"The cross validation log loss is:\",log_loss(y_cv, predict_y))\npredict_y = sig_clf.predict_proba(X_test)\nprint('For values of best alpha = ', alpha[best_alpha], \"The test log loss is:\",log_loss(y_test, predict_y))\nplot_confusion_matrix(y_test, sig_clf.predict(X_test))","2355abf4":"!curl --header 'Host: doc-0o-34-docs.googleusercontent.com' --user-agent 'Mozilla\/5.0 (X11; Ubuntu; Linux x86_64; rv:73.0) Gecko\/20100101 Firefox\/73.0' --header 'Accept: text\/html,application\/xhtml+xml,application\/xml;q=0.9,image\/webp,*\/*;q=0.8' --header 'Accept-Language: en-US,en;q=0.5' --referer 'https:\/\/drive.google.com\/drive\/folders\/1ujh2OJ92AiNJOdW4etCLR59-etaCVzl3' --cookie 'AUTH_tluo3cnhhlj32isrmt1s6kdbpeqcidnu_nonce=u9nj0q7hvo2h0' --header 'Upgrade-Insecure-Requests: 1' 'https:\/\/doc-0o-34-docs.googleusercontent.com\/docs\/securesc\/77fsupn6vldsoib8m55dua9tfcjdfqsl\/4tl425ddbaigkf06m5b2rd34ohsb7243\/1583730600000\/00484516897554883881\/04566853329451047403\/115fBXIIGhZXSI6WPrVOKCkOC2iUhVr2b?e=download&authuser=0&nonce=u9nj0q7hvo2h0&user=04566853329451047403&hash=f487rbsv3vflnii4hq56bj9n7q4p3h8j' --output 'asmoutputfile.csv'","63f9ad5a":"source_directory_path = \"train\"\ndestination_directory_path =\"asmFiles\"\nfrom tqdm import tqdm\n\n# we will check if the folder 'byteFiles' exists if it not there we will create a folder with the same name\nif not os.path.isdir(destination_directory_path):\n    os.makedirs(destination_directory_path)\n\n# if we have folder called 'train' (train folder contains both .asm files and .bytes files) we will rename it 'asmFiles'\n# for every file that we have in our 'asmFiles' directory we check if it is ending with .bytes, if yes we will move it to\n# 'byteFiles' folder\n\n\n# so by the end of this snippet we will separate all the .byte files and .asm files\n\nfor source_filename in tqdm(os.listdir(source_directory_path)):\n    \n    if source_filename.endswith(\".asm\"):\n        # construct path from filename\n        \n        source_file_path = os.path.join(source_directory_path, source_filename)\n\n        shutil.move(source_file_path, destination_directory_path)","2a855b51":"files=os.listdir('asmFiles')\nfilenames=Y['Id'].tolist()\nclass_y=Y['Class'].tolist()\nclass_bytes=[]\nsizebytes=[]\nfnames=[]\nfor file in files:\n    # print(os.stat('byteFiles\/0A32eTdBKayjCWhZqDOQ.txt'))\n    # os.stat_result(st_mode=33206, st_ino=1125899906874507, st_dev=3561571700, st_nlink=1, st_uid=0, st_gid=0, \n    # st_size=3680109, st_atime=1519638522, st_mtime=1519638522, st_ctime=1519638522)\n    # read more about os.stat: here https:\/\/www.tutorialspoint.com\/python\/os_stat.htm\n    statinfo=os.stat('asmFiles\/'+file)\n    # split the file name at '.' and take the first part of it i.e the file name\n    file=file.split('.')[0]\n    if any(file == filename for filename in filenames):\n        i=filenames.index(file)\n        class_bytes.append(class_y[i])\n        # converting into Mb's\n        sizebytes.append(statinfo.st_size\/(1024.0*1024.0))\n        fnames.append(file)\nasm_size_byte=pd.DataFrame({'ID':fnames,'size':sizebytes,'Class':class_bytes})\nprint (asm_size_byte.head())","1184068e":"ax = sns.boxplot(x=\"Class\", y=\"size\", data=asm_size_byte)\nplt.title(\"boxplot of .bytes file sizes\")\nplt.show()","a94693de":"result_asm=pd.read_csv('asmoutputfile.csv')\n","7c35999b":"result_asm.head()","78c517b8":"print(result_asm.shape)\nprint(asm_size_byte.shape)\nresult_asm = pd.merge(result_asm, asm_size_byte,on='ID', how='left')\nresult_asm.head()","419c9f91":"def normalize(df):\n    from tqdm import tqdm\n    result1 = df.copy()\n    for feature_name in tqdm(df.columns):\n        if (str(feature_name) != str('ID') and str(feature_name)!=str('Class')):\n            max_value = df[feature_name].max()\n            min_value = df[feature_name].min()\n            result1[feature_name] = (df[feature_name] - min_value) \/ (max_value - min_value)\n    return result1","2d02d45b":"result_asm = normalize(result_asm)\nresult_asm.head()","7b4bbd92":"ax = sns.boxplot(x=\"Class\", y=\".text:\", data=result_asm)\nplt.title(\"boxplot of .asm text segment\")\nplt.show()","bcf229e2":"ax = sns.boxplot(x=\"Class\", y=\".Pav:\", data=result_asm)\nplt.title(\"boxplot of .asm pav segment\")\nplt.show()","3b34de6c":"ax = sns.boxplot(x=\"Class\", y=\".data:\", data=result_asm)\nplt.title(\"boxplot of .asm data segment\")\nplt.show()","7ecd4016":"ax = sns.boxplot(x=\"Class\", y=\".data:\", data=result_asm)\nplt.title(\"boxplot of .asm data segment\")\nplt.show()","d9b11c11":"ax = sns.boxplot(x=\"Class\", y=\".data:\", data=result_asm)\nplt.title(\"boxplot of .asm data segment\")\nplt.show()","429bf4c0":"ax = sns.boxplot(x=\"Class\", y=\"jmp\", data=result_asm)\nplt.title(\"boxplot of .asm jmp opcode\")\nplt.show()","dc99d75d":"ax = sns.boxplot(x=\"Class\", y=\"mov\", data=result_asm)\nplt.title(\"boxplot of .asm mov opcode\")\nplt.show()","7eb5ec75":"ax = sns.boxplot(x=\"Class\", y=\"retf\", data=result_asm)\nplt.title(\"boxplot of .asm retf opcode\")\nplt.show()","029cd77a":"ax = sns.boxplot(x=\"Class\", y=\"push\", data=result_asm)\nplt.title(\"boxplot of .asm push opcode\")\nplt.show()","6544a654":"data_y = result_asm['Class']","5c5ea2a9":"# check out the course content for more explantion on tsne algorithm\n\n#multivariate analysis on byte files\n#this is with perplexity 50\nxtsne=TSNE(perplexity=50)\nresults=xtsne.fit_transform(result_asm.drop(['ID','Class'], axis=1).fillna(0))\nvis_x = results[:, 0]\nvis_y = results[:, 1   ]\nplt.scatter(vis_x, vis_y, c=data_y, cmap=plt.cm.get_cmap(\"jet\", 9))\nplt.colorbar(ticks=range(10))\nplt.clim(0.5, 9)\nplt.show()","89cfff8e":"asm_y = result_asm['Class']\nasm_x = result_asm.drop(['ID','Class','.BSS:','rtn','.CODE'], axis=1)","f984ea4d":"X_train_asm, X_test_asm, y_train_asm, y_test_asm = train_test_split(asm_x,asm_y ,stratify=asm_y,test_size=0.20)\nX_train_asm, X_cv_asm, y_train_asm, y_cv_asm = train_test_split(X_train_asm, y_train_asm,stratify=y_train_asm,test_size=0.20)","971239f8":"# find more about KNeighborsClassifier() here http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.neighbors.KNeighborsClassifier.html\n# -------------------------\n# default parameter\n# KNeighborsClassifier(n_neighbors=5, weights=\u2019uniform\u2019, algorithm=\u2019auto\u2019, leaf_size=30, p=2, \n# metric=\u2019minkowski\u2019, metric_params=None, n_jobs=1, **kwargs)\n\n# methods of\n# fit(X, y) : Fit the model using X as training data and y as target values\n# predict(X):Predict the class labels for the provided data\n# predict_proba(X):Return probability estimates for the test data X.\n#-------------------------------------\n# video link: https:\/\/www.appliedaicourse.com\/course\/applied-ai-course-online\/lessons\/k-nearest-neighbors-geometric-intuition-with-a-toy-example-1\/\n#-------------------------------------\n\n\n# find more about CalibratedClassifierCV here at \n# http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.calibration.CalibratedClassifierCV.html\n# ----------------------------\n# default paramters\n# sklearn.calibration.CalibratedClassifierCV(base_estimator=None, method=\u2019sigmoid\u2019, cv=3)\n#\n# some of the methods of CalibratedClassifierCV()\n# fit(X, y[, sample_weight])\tFit the calibrated model\n# get_params([deep])\tGet parameters for this estimator.\n# predict(X)\tPredict the target of new samples.\n# predict_proba(X)\tPosterior probabilities of classification\n#-------------------------------------\n# video link:\n#-------------------------------------\n\nalpha = [x for x in range(1, 21,2)]\ncv_log_error_array=[]\nfor i in alpha:\n    k_cfl=KNeighborsClassifier(n_neighbors=i)\n    k_cfl.fit(X_train_asm,y_train_asm)\n    sig_clf = CalibratedClassifierCV(k_cfl, method=\"sigmoid\")\n    sig_clf.fit(X_train_asm, y_train_asm)\n    predict_y = sig_clf.predict_proba(X_cv_asm)\n    cv_log_error_array.append(log_loss(y_cv_asm, predict_y, labels=k_cfl.classes_, eps=1e-15))\n    \nfor i in range(len(cv_log_error_array)):\n    print ('log_loss for k = ',alpha[i],'is',cv_log_error_array[i])\n\nbest_alpha = np.argmin(cv_log_error_array)\n    \nfig, ax = plt.subplots()\nax.plot(alpha, cv_log_error_array,c='g')\nfor i, txt in enumerate(np.round(cv_log_error_array,3)):\n    ax.annotate((alpha[i],np.round(txt,3)), (alpha[i],cv_log_error_array[i]))\nplt.grid()\nplt.title(\"Cross Validation Error for each alpha\")\nplt.xlabel(\"Alpha i's\")\nplt.ylabel(\"Error measure\")\nplt.show()\n\nk_cfl=KNeighborsClassifier(n_neighbors=alpha[best_alpha])\nk_cfl.fit(X_train_asm,y_train_asm)\nsig_clf = CalibratedClassifierCV(k_cfl, method=\"sigmoid\")\nsig_clf.fit(X_train_asm, y_train_asm)\npred_y=sig_clf.predict(X_test_asm)\n\n\npredict_y = sig_clf.predict_proba(X_train_asm)\nprint ('log loss for train data',log_loss(y_train_asm, predict_y))\npredict_y = sig_clf.predict_proba(X_cv_asm)\nprint ('log loss for cv data',log_loss(y_cv_asm, predict_y))\npredict_y = sig_clf.predict_proba(X_test_asm)\nprint ('log loss for test data',log_loss(y_test_asm, predict_y))\nplot_confusion_matrix(y_test_asm,sig_clf.predict(X_test_asm))","e043246f":"# read more about SGDClassifier() at http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.linear_model.SGDClassifier.html\n# ------------------------------\n# default parameters\n# SGDClassifier(loss=\u2019hinge\u2019, penalty=\u2019l2\u2019, alpha=0.0001, l1_ratio=0.15, fit_intercept=True, max_iter=None, tol=None, \n# shuffle=True, verbose=0, epsilon=0.1, n_jobs=1, random_state=None, learning_rate=\u2019optimal\u2019, eta0=0.0, power_t=0.5, \n# class_weight=None, warm_start=False, average=False, n_iter=None)\n\n# some of methods\n# fit(X, y[, coef_init, intercept_init, \u2026])\tFit linear model with Stochastic Gradient Descent.\n# predict(X)\tPredict class labels for samples in X.\n\n#-------------------------------\n# video link: https:\/\/www.appliedaicourse.com\/course\/applied-ai-course-online\/lessons\/geometric-intuition-1\/\n#------------------------------\n\n\nalpha = [10 ** x for x in range(-5, 4)]\ncv_log_error_array=[]\nfor i in alpha:\n    logisticR=LogisticRegression(penalty='l2',C=i,class_weight='balanced')\n    logisticR.fit(X_train_asm,y_train_asm)\n    sig_clf = CalibratedClassifierCV(logisticR, method=\"sigmoid\")\n    sig_clf.fit(X_train_asm, y_train_asm)\n    predict_y = sig_clf.predict_proba(X_cv_asm)\n    cv_log_error_array.append(log_loss(y_cv_asm, predict_y, labels=logisticR.classes_, eps=1e-15))\n    \nfor i in range(len(cv_log_error_array)):\n    print ('log_loss for c = ',alpha[i],'is',cv_log_error_array[i])\n\nbest_alpha = np.argmin(cv_log_error_array)\n    \nfig, ax = plt.subplots()\nax.plot(alpha, cv_log_error_array,c='g')\nfor i, txt in enumerate(np.round(cv_log_error_array,3)):\n    ax.annotate((alpha[i],np.round(txt,3)), (alpha[i],cv_log_error_array[i]))\nplt.grid()\nplt.title(\"Cross Validation Error for each alpha\")\nplt.xlabel(\"Alpha i's\")\nplt.ylabel(\"Error measure\")\nplt.show()\n\nlogisticR=LogisticRegression(penalty='l2',C=alpha[best_alpha],class_weight='balanced')\nlogisticR.fit(X_train_asm,y_train_asm)\nsig_clf = CalibratedClassifierCV(logisticR, method=\"sigmoid\")\nsig_clf.fit(X_train_asm, y_train_asm)\n\npredict_y = sig_clf.predict_proba(X_train_asm)\nprint ('log loss for train data',(log_loss(y_train_asm, predict_y, labels=logisticR.classes_, eps=1e-15)))\npredict_y = sig_clf.predict_proba(X_cv_asm)\nprint ('log loss for cv data',(log_loss(y_cv_asm, predict_y, labels=logisticR.classes_, eps=1e-15)))\npredict_y = sig_clf.predict_proba(X_test_asm)\nprint ('log loss for test data',(log_loss(y_test_asm, predict_y, labels=logisticR.classes_, eps=1e-15)))\nplot_confusion_matrix(y_test_asm,sig_clf.predict(X_test_asm))","3b56e3b4":"# --------------------------------\n# default parameters \n# sklearn.ensemble.RandomForestClassifier(n_estimators=10, criterion=\u2019gini\u2019, max_depth=None, min_samples_split=2, \n# min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=\u2019auto\u2019, max_leaf_nodes=None, min_impurity_decrease=0.0, \n# min_impurity_split=None, bootstrap=True, oob_score=False, n_jobs=1, random_state=None, verbose=0, warm_start=False, \n# class_weight=None)\n\n# Some of methods of RandomForestClassifier()\n# fit(X, y, [sample_weight])\tFit the SVM model according to the given training data.\n# predict(X)\tPerform classification on samples in X.\n# predict_proba (X)\tPerform classification on samples in X.\n\n# some of attributes of  RandomForestClassifier()\n# feature_importances_ : array of shape = [n_features]\n# The feature importances (the higher, the more important the feature).\n\n# --------------------------------\n# video link: https:\/\/www.appliedaicourse.com\/course\/applied-ai-course-online\/lessons\/random-forest-and-their-construction-2\/\n# --------------------------------\n\nalpha=[10,50,100,500,1000,2000,3000]\ncv_log_error_array=[]\nfor i in alpha:\n    r_cfl=RandomForestClassifier(n_estimators=i,random_state=42,n_jobs=-1)\n    r_cfl.fit(X_train_asm,y_train_asm)\n    sig_clf = CalibratedClassifierCV(r_cfl, method=\"sigmoid\")\n    sig_clf.fit(X_train_asm, y_train_asm)\n    predict_y = sig_clf.predict_proba(X_cv_asm)\n    cv_log_error_array.append(log_loss(y_cv_asm, predict_y, labels=r_cfl.classes_, eps=1e-15))\n\nfor i in range(len(cv_log_error_array)):\n    print ('log_loss for c = ',alpha[i],'is',cv_log_error_array[i])\n\n\nbest_alpha = np.argmin(cv_log_error_array)\n\nfig, ax = plt.subplots()\nax.plot(alpha, cv_log_error_array,c='g')\nfor i, txt in enumerate(np.round(cv_log_error_array,3)):\n    ax.annotate((alpha[i],np.round(txt,3)), (alpha[i],cv_log_error_array[i]))\nplt.grid()\nplt.title(\"Cross Validation Error for each alpha\")\nplt.xlabel(\"Alpha i's\")\nplt.ylabel(\"Error measure\")\nplt.show()\n\nr_cfl=RandomForestClassifier(n_estimators=alpha[best_alpha],random_state=42,n_jobs=-1)\nr_cfl.fit(X_train_asm,y_train_asm)\nsig_clf = CalibratedClassifierCV(r_cfl, method=\"sigmoid\")\nsig_clf.fit(X_train_asm, y_train_asm)\npredict_y = sig_clf.predict_proba(X_train_asm)\nprint ('log loss for train data',(log_loss(y_train_asm, predict_y, labels=sig_clf.classes_, eps=1e-15)))\npredict_y = sig_clf.predict_proba(X_cv_asm)\nprint ('log loss for cv data',(log_loss(y_cv_asm, predict_y, labels=sig_clf.classes_, eps=1e-15)))\npredict_y = sig_clf.predict_proba(X_test_asm)\nprint ('log loss for test data',(log_loss(y_test_asm, predict_y, labels=sig_clf.classes_, eps=1e-15)))\nplot_confusion_matrix(y_test_asm,sig_clf.predict(X_test_asm))","18234ce0":"# Training a hyper-parameter tuned Xg-Boost regressor on our train data\n\n# find more about XGBClassifier function here http:\/\/xgboost.readthedocs.io\/en\/latest\/python\/python_api.html?#xgboost.XGBClassifier\n# -------------------------\n# default paramters\n# class xgboost.XGBClassifier(max_depth=3, learning_rate=0.1, n_estimators=100, silent=True, \n# objective='binary:logistic', booster='gbtree', n_jobs=1, nthread=None, gamma=0, min_child_weight=1, \n# max_delta_step=0, subsample=1, colsample_bytree=1, colsample_bylevel=1, reg_alpha=0, reg_lambda=1, \n# scale_pos_weight=1, base_score=0.5, random_state=0, seed=None, missing=None, **kwargs)\n\n# some of methods of RandomForestRegressor()\n# fit(X, y, sample_weight=None, eval_set=None, eval_metric=None, early_stopping_rounds=None, verbose=True, xgb_model=None)\n# get_params([deep])\tGet parameters for this estimator.\n# predict(data, output_margin=False, ntree_limit=0) : Predict with data. NOTE: This function is not thread safe.\n# get_score(importance_type='weight') -> get the feature importance\n# -----------------------\n# video link2: https:\/\/www.appliedaicourse.com\/course\/applied-ai-course-online\/lessons\/what-are-ensembles\/\n# -----------------------\n\nalpha=[10,50,100,500,1000,2000,3000]\ncv_log_error_array=[]\nfor i in alpha:\n    x_cfl=XGBClassifier(n_estimators=i,nthread=-1)\n    x_cfl.fit(X_train_asm,y_train_asm)\n    sig_clf = CalibratedClassifierCV(x_cfl, method=\"sigmoid\")\n    sig_clf.fit(X_train_asm, y_train_asm)\n    predict_y = sig_clf.predict_proba(X_cv_asm)\n    cv_log_error_array.append(log_loss(y_cv_asm, predict_y, labels=x_cfl.classes_, eps=1e-15))\n\nfor i in range(len(cv_log_error_array)):\n    print ('log_loss for c = ',alpha[i],'is',cv_log_error_array[i])\n\n\nbest_alpha = np.argmin(cv_log_error_array)\n\nfig, ax = plt.subplots()\nax.plot(alpha, cv_log_error_array,c='g')\nfor i, txt in enumerate(np.round(cv_log_error_array,3)):\n    ax.annotate((alpha[i],np.round(txt,3)), (alpha[i],cv_log_error_array[i]))\nplt.grid()\nplt.title(\"Cross Validation Error for each alpha\")\nplt.xlabel(\"Alpha i's\")\nplt.ylabel(\"Error measure\")\nplt.show()\n\nx_cfl=XGBClassifier(n_estimators=alpha[best_alpha],nthread=-1)\nx_cfl.fit(X_train_asm,y_train_asm)\nsig_clf = CalibratedClassifierCV(x_cfl, method=\"sigmoid\")\nsig_clf.fit(X_train_asm, y_train_asm)\n    \npredict_y = sig_clf.predict_proba(X_train_asm)\n\nprint ('For values of best alpha = ', alpha[best_alpha], \"The train log loss is:\",log_loss(y_train_asm, predict_y))\npredict_y = sig_clf.predict_proba(X_cv_asm)\nprint('For values of best alpha = ', alpha[best_alpha], \"The cross validation log loss is:\",log_loss(y_cv_asm, predict_y))\npredict_y = sig_clf.predict_proba(X_test_asm)\nprint('For values of best alpha = ', alpha[best_alpha], \"The test log loss is:\",log_loss(y_test_asm, predict_y))\nplot_confusion_matrix(y_test_asm,sig_clf.predict(X_test_asm))","47a86fe8":"x_cfl=XGBClassifier()\n\nprams={\n    'learning_rate':[0.01,0.03,0.05,0.1,0.15,0.2],\n     'n_estimators':[100,200,500,1000,2000],\n     'max_depth':[3,5,10],\n    'colsample_bytree':[0.1,0.3,0.5,1],\n    'subsample':[0.1,0.3,0.5,1]\n}\nrandom_cfl=RandomizedSearchCV(x_cfl,param_distributions=prams,verbose=10,n_jobs=-1)\nrandom_cfl.fit(X_train_asm,y_train_asm)","ff8a0c03":"print (random_cfl.best_params_)","c761ddb5":"# Training a hyper-parameter tuned Xg-Boost regressor on our train data\n\n# find more about XGBClassifier function here \n# http:\/\/xgboost.readthedocs.io\/en\/latest\/python\/python_api.html?#xgboost.XGBClassifier\n# -------------------------\n# default paramters\n# class xgboost.XGBClassifier(max_depth=3, learning_rate=0.1, n_estimators=100, silent=True, \n# objective='binary:logistic', booster='gbtree', n_jobs=1, nthread=None, gamma=0, min_child_weight=1, \n# max_delta_step=0, subsample=1, colsample_bytree=1, colsample_bylevel=1, reg_alpha=0, reg_lambda=1, \n# scale_pos_weight=1, base_score=0.5, random_state=0, seed=None, missing=None, **kwargs)\n\n# some of methods of RandomForestRegressor()\n# fit(X, y, sample_weight=None, eval_set=None, eval_metric=None, early_stopping_rounds=None, verbose=True, xgb_model=None)\n# get_params([deep])\tGet parameters for this estimator.\n# predict(data, output_margin=False, ntree_limit=0) : Predict with data. NOTE: This function is not thread safe.\n# get_score(importance_type='weight') -> get the feature importance\n# -----------------------\n# video link2: https:\/\/www.appliedaicourse.com\/course\/applied-ai-course-online\/lessons\/what-are-ensembles\/\n# -----------------------\n\nx_cfl=XGBClassifier(n_estimators=500,subsample=0.3,learning_rate=0.15,colsample_bytree=1,max_depth=3)\nx_cfl.fit(X_train_asm,y_train_asm)\nc_cfl=CalibratedClassifierCV(x_cfl,method='sigmoid')\nc_cfl.fit(X_train_asm,y_train_asm)\n\npredict_y = c_cfl.predict_proba(X_train_asm)\nprint ('train loss',log_loss(y_train_asm, predict_y))\npredict_y = c_cfl.predict_proba(X_cv_asm)\nprint ('cv loss',log_loss(y_cv_asm, predict_y))\npredict_y = c_cfl.predict_proba(X_test_asm)\nprint ('test loss',log_loss(y_test_asm, predict_y))","abbc392b":"final_result.head()","c7a777fe":"result_asm.head()","8074f754":"print(final_result.shape)","c76e8a2c":"result_x = pd.merge(final_result,result_asm.drop(['Class'], axis=1),on='ID', how='left')","6f4980fe":"result_x.head()","0f57997c":"pickle_in = open(\"result_x.pickle\",\"rb\")\nresult_x = pickle.load(pickle_in)","72e2c9ea":"\nresult_y = result_x['Class']\nresult_x = result_x.drop(['ID','rtn','.BSS:','.CODE','Class'], axis=1)\nresult_x.head()","05ca8863":"result_y.head()","c983d094":"X_train, X_test_merge, y_train, y_test_merge = train_test_split(result_x, result_y,stratify=result_y,test_size=0.20)\nX_train_merge, X_cv_merge, y_train_merge, y_cv_merge = train_test_split(X_train, y_train,stratify=y_train,test_size=0.20)","1dd2f6ca":"# --------------------------------\n# default parameters \n# sklearn.ensemble.RandomForestClassifier(n_estimators=10, criterion=\u2019gini\u2019, max_depth=None, min_samples_split=2, \n# min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=\u2019auto\u2019, max_leaf_nodes=None, min_impurity_decrease=0.0, \n# min_impurity_split=None, bootstrap=True, oob_score=False, n_jobs=1, random_state=None, verbose=0, warm_start=False, \n# class_weight=None)\n\n# Some of methods of RandomForestClassifier()\n# fit(X, y, [sample_weight])\tFit the SVM model according to the given training data.\n# predict(X)\tPerform classification on samples in X.\n# predict_proba (X)\tPerform classification on samples in X.\n\n# some of attributes of  RandomForestClassifier()\n# feature_importances_ : array of shape = [n_features]\n# The feature importances (the higher, the more important the feature).\n\n# --------------------------------\n# video link:https:\/\/www.appliedaicourse.com\/course\/applied-ai-course-online\/lessons\/random-forest-and-their-construction-2\/\n# --------------------------------\n\nalpha=[10,50,100,500]\ncv_log_error_array=[]\nfrom sklearn.ensemble import RandomForestClassifier\nfor i in alpha:\n    r_cfl=RandomForestClassifier(n_estimators=i,random_state=42,n_jobs=-2)\n    r_cfl.fit(X_train_merge,y_train_merge)\n    sig_clf = CalibratedClassifierCV(r_cfl, method=\"sigmoid\")\n    sig_clf.fit(X_train_merge, y_train_merge)\n    predict_y = sig_clf.predict_proba(X_cv_merge)\n    cv_log_error_array.append(log_loss(y_cv_merge, predict_y, labels=r_cfl.classes_, eps=1e-15))\n\nfor i in range(len(cv_log_error_array)):\n    print ('log_loss for c = ',alpha[i],'is',cv_log_error_array[i])\n\n\nbest_alpha = np.argmin(cv_log_error_array)\n\nfig, ax = plt.subplots()\nax.plot(alpha, cv_log_error_array,c='g')\nfor i, txt in enumerate(np.round(cv_log_error_array,3)):\n    ax.annotate((alpha[i],np.round(txt,3)), (alpha[i],cv_log_error_array[i]))\nplt.grid()\nplt.title(\"Cross Validation Error for each alpha\")\nplt.xlabel(\"Alpha i's\")\nplt.ylabel(\"Error measure\")\nplt.show()\n\n\nr_cfl=RandomForestClassifier(n_estimators=alpha[best_alpha],random_state=42,n_jobs=-2)\nr_cfl.fit(X_train_merge,y_train_merge)\nsig_clf = CalibratedClassifierCV(r_cfl, method=\"sigmoid\")\nsig_clf.fit(X_train_merge, y_train_merge)\n\npredict_y = sig_clf.predict_proba(X_train_merge)\nprint ('For values of best alpha = ', alpha[best_alpha], \"The train log loss is:\",log_loss(y_train_merge, predict_y))\npredict_y = sig_clf.predict_proba(X_cv_merge)\nprint('For values of best alpha = ', alpha[best_alpha], \"The cross validation log loss is:\",log_loss(y_cv_merge, predict_y))\npredict_y = sig_clf.predict_proba(X_test_merge)\nprint('For values of best alpha = ', alpha[best_alpha], \"The test log loss is:\",log_loss(y_test_merge, predict_y))","74d30c42":"x_cfl=XGBClassifier()\n\nprams={\n    'learning_rate':[0.01,0.03,0.05,0.1,0.15,0.2],\n     'n_estimators':[100,200,500,1000,2000],\n     'max_depth':[3,5,10],\n    'colsample_bytree':[0.1,0.3,0.5,1],\n    'subsample':[0.1,0.3,0.5,1]\n}\n# had to keep n_jobs=2, more than that will lead to in-sufficient memory (52GB)\nrandom_cfl=RandomizedSearchCV(x_cfl,param_distributions=prams,verbose=0,n_jobs=3,cv=2)\nrandom_cfl.fit(X_train_merge, y_train_merge)","047758f8":"print(random_cfl.best_params_)","0526ed6f":"x_cfl=XGBClassifier(n_estimators=1000,max_depth=5,learning_rate=0.1,colsample_bytree=0.3,subsample=1,nthread=3,n_jobs=-1)\nx_cfl.fit(X_train_merge,y_train_merge,verbose=10)\nsig_clf = CalibratedClassifierCV(x_cfl, method=\"sigmoid\")\nsig_clf.fit(X_train_merge, y_train_merge)\n    \npredict_y = sig_clf.predict_proba(X_train_merge)","29a3d4cb":"predict_y = sig_clf.predict_proba(X_train_merge)","6edccc6b":"print ( \"The train log loss is:\",log_loss(y_train_merge, predict_y))\npredict_y = sig_clf.predict_proba(X_cv_merge)\nprint( \"The cross validation log loss is:\",log_loss(y_cv_merge, predict_y))\npredict_y = sig_clf.predict_proba(X_test_merge)\nprint(\"The test log loss is:\",log_loss(y_test_merge, predict_y))","50d7b40d":"print(\"The test log loss is:\",log_loss(y_test_merge, predict_y))","8b55d9a4":"from prettytable import PrettyTable","787c8116":"x = PrettyTable()\nx.field_names = [\"Algorithm\", \"Feature Engg\", \"Hyperparameter\", \"Train log-loss\", \"Test log-loss\"]","71454d80":"x.add_row([\"Logistic Regression\", \"Bytes bi-grams\", \"c=1000\", 0.047, 0.18])\nx.add_row([\"Random Forest\", \"Bytes bi-grams\", \"n_estimators=1000\", 0.0198, 0.077])\nx.add_row([\"XGBoost\", \"Bytes bi-grams\", \"n_estimators=100\", 0.0145, 0.0489])\nx.add_row([\"KNN\", \"ASM uni-grams\", \"n_neighbors=3\", 0.0476, 0.0894])\nx.add_row([\"Logistic Regression\", \"ASM uni-grams\", \"C=1000\", 0.396, 0.4156])\nx.add_row([\"Random Forest\", \"ASM uni-grams\", \"n_estimators=3000\", 0.0116, 0.0571])\nx.add_row([\"XgBoost\", \"ASM uni-grams\", \"n_estimators=200, max_depth=5\", 0.0102, 0.048])\nx.add_row([\"Random Forest\", \"Bytes bi-grams, ASM uni-grams\", \"n_estimators=100\", 0.0187, 0.0675])\nx.add_row([\"XgBoost\", \"Bytes bi-grams, ASM uni-grams\", \"n_estimators=1000, max_depth=5\", 0.0179, 0.0275])\n\nprint(x)","195a86db":"<h3>Random Forest Classifier on final features<\/h3>","9d33218a":"<h3>knn<\/h3>","cf202547":"<h3>XgBoost Classifier<\/h3>","b50ba549":"<h2>XgBoost Classifier on final features with best hyper parameters using Random search<\/h2>","99f01de4":"<h2>we will take only 7000 rows <\/h2>","dd4d5da2":"<h2>Machine Learning Model<\/h2>","4bd31d54":"<h3>logistic<\/h3>","eb8a6c71":"<h2>Multivariate Analysis on .asm file features<\/h2>","12be5a20":"<h3>Random Forest Classifie<\/h3>","8655a390":"<h2>Modeling with .asm files<\/h2>","055740cd":"<h3>Machine Learning models on features of both .asm and .bytes files<\/h3>","af9311e1":"<h2> Machine Learning models on features of .asm files<\/h2>","64d94c8c":"<h3>1. Logistic Regression<\/h3>"}}