{"cell_type":{"bd2de4ef":"code","ae34e1e8":"code","428a6f54":"code","74aca355":"code","c0c929db":"code","7cb727ea":"code","d6b1fed4":"code","d03a58f2":"code","caa4b173":"code","34441a8e":"code","67f00fb2":"code","e27a7c9f":"code","711c229c":"code","16a0e901":"code","3757b70d":"code","b0eeae20":"code","a9ff317d":"code","f531e41b":"code","c04b72d6":"code","210ff96a":"code","bf9ff9d1":"code","100e2174":"code","dae3f8fa":"code","40cd3fe8":"code","ca643732":"code","a7617af2":"code","c4c112b3":"code","8f1adc3a":"code","7393ffd8":"code","cd63a1c4":"code","0dcdd4bc":"code","9b3c0602":"code","1d542ebe":"code","a570077e":"code","778fde82":"code","ed585c35":"code","b79b9f60":"code","29019799":"code","b647d82d":"code","4e4c4934":"code","c515ae10":"code","32351bac":"code","dd67208f":"code","0f5d8939":"code","7d56e004":"code","923cc027":"code","04e33264":"code","a29c34c2":"code","92c9b624":"code","1cc5657d":"code","ddaa4dc5":"code","99fe9497":"code","470f39c8":"code","fff4c6cb":"code","9967c2ba":"code","c6be75ae":"code","ba175711":"code","da6581c7":"code","7db3cece":"code","32a2912b":"code","9dfb3f29":"code","9b00b318":"code","73d5f7f9":"code","3a80a5ae":"code","12442f7b":"code","64f4a2c3":"code","ef11cc66":"code","880f776b":"code","30259544":"code","c6aba576":"markdown","35bcfbe9":"markdown","7902ce09":"markdown","a0a9eb15":"markdown","2c3d2508":"markdown","d281bcc7":"markdown","158702d9":"markdown","ef7dd876":"markdown","273ce2dc":"markdown","32f0b0f9":"markdown","b0f14712":"markdown","40a9f6cf":"markdown","8bbdc501":"markdown","cdca8720":"markdown","6b515890":"markdown","2c68f718":"markdown","e4f21cc7":"markdown","075978ab":"markdown"},"source":{"bd2de4ef":"import numpy as np\nimport pandas as pd\nfrom tqdm import tqdm_notebook\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.preprocessing import StandardScaler, PolynomialFeatures, MinMaxScaler\nfrom sklearn.feature_selection import VarianceThreshold\nfrom sklearn.decomposition import PCA\nfrom sklearn.cluster import FeatureAgglomeration\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\nfrom sklearn.covariance import GraphicalLasso\nfrom sklearn.mixture import GaussianMixture\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.semi_supervised import LabelSpreading\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.pipeline import Pipeline\nimport warnings\nimport multiprocessing\nfrom scipy.optimize import minimize  \nwarnings.filterwarnings('ignore')","ae34e1e8":"def load_data(data):\n    return pd.read_csv(data)\n    \nwith multiprocessing.Pool() as pool:\n    train, test, sub = pool.map(load_data, ['..\/input\/train.csv', '..\/input\/test.csv', '..\/input\/sample_submission.csv'])","428a6f54":"cols = [c for c in train.columns if c not in ['id', 'target', 'wheezy-copper-turtle-magic']]","74aca355":"oof_qda1 = np.zeros(len(train))\npreds_qda1 = np.zeros(len(test))","c0c929db":"params = np.arange(0.1, 0.5, 0.1) # [0.1 0.2, 0.3, 0.4, 0.5]\nparameters = [{'reg_param': params}]\nreg_params = np.zeros(512)","7cb727ea":"%%time\nfor i in tqdm_notebook(range(512)):\n\n    train2 = train[train['wheezy-copper-turtle-magic']==i]\n    test2 = test[test['wheezy-copper-turtle-magic']==i]\n    idx1 = train2.index\n    idx2 = test2.index\n    train2.reset_index(drop=True, inplace=True)\n    test2.reset_index(drop=True, inplace=True)\n    \n    steps = [\n        ('vt', VarianceThreshold(threshold=2)),\n        ('sscaler', StandardScaler()),\n        #('rscaler', RobustScaler()),\n        #('mmscaler', MinMaxScaler())\n    ]\n    \n    pipe = Pipeline(steps=steps)\n    data = pd.concat([pd.DataFrame(train2[cols]), pd.DataFrame(test2[cols])])\n    data2 = pipe.fit_transform(data[cols])\n    train3 = data2[:train2.shape[0]]\n    test3 = data2[train2.shape[0]:]\n    \n    skf = StratifiedKFold(n_splits=11, random_state=42)\n    for train_index, test_index in skf.split(train2, train2['target']):\n\n        qda = QuadraticDiscriminantAnalysis()\n        clf = GridSearchCV(qda, parameters, cv=4)\n        clf.fit(train3[train_index,:], train2.loc[train_index]['target'])\n        reg_params[i] = clf.best_params_['reg_param']\n        oof_qda1[idx1[test_index]] = clf.predict_proba(train3[test_index,:])[:,1]\n        preds_qda1[idx2] += clf.predict_proba(test3)[:,1] \/ skf.n_splits","d6b1fed4":"print('QDA with VarianceThreshold and StandardScaler CV Score =',round(roc_auc_score(train['target'], oof_qda1), 5))","d03a58f2":"sub['target'] = preds_qda1\nsub['target'].to_csv('submission_qda1.csv', index=False)","caa4b173":"sub['target'] = preds_qda1\nsub['target'].hist(bins=100, alpha=0.6)","34441a8e":"oof_qda2 = np.zeros(len(train))\npreds_qda2 = np.zeros(len(test))","67f00fb2":"dict = dict()","e27a7c9f":"%%time\nfor s in range(512):\n\n    train2 = train[train['wheezy-copper-turtle-magic']==s]\n    test2 = test[test['wheezy-copper-turtle-magic']==s]\n    idx1 = train2.index\n    idx2 = test2.index\n    train2.reset_index(drop=True, inplace=True)\n    \n    sel = VarianceThreshold(threshold=1.5).fit(train2[cols])\n    train3 = sel.transform(train2[cols])\n    test3 = sel.transform(test2[cols])\n        \n    dict[s] = train3.shape[1]","711c229c":"%%time\nfor i in tqdm_notebook(range(512)):\n\n    train2 = train[train['wheezy-copper-turtle-magic']==i]\n    test2 = test[test['wheezy-copper-turtle-magic']==i]\n    idx1 = train2.index\n    idx2 = test2.index\n    train2.reset_index(drop=True, inplace=True)\n    test2.reset_index(drop=True, inplace=True)  \n    \n    steps = [\n        #('pca', PCA(n_components=dict[i], random_state=42))\n        ('scaler', StandardScaler()), \n        ('fa', FeatureAgglomeration(n_clusters=dict[i]))\n    ]\n    \n    pipe = Pipeline(steps=steps)\n    data = pd.concat([pd.DataFrame(train2[cols]), pd.DataFrame(test2[cols])])\n    data2 = pipe.fit_transform(data[cols])\n    train3 = data2[:train2.shape[0]]\n    test3 = data2[train2.shape[0]:]\n\n    skf = StratifiedKFold(n_splits=11, random_state=42)\n    for train_index, test_index in skf.split(train2, train2['target']):\n\n        clf = QuadraticDiscriminantAnalysis(0.5)\n        clf.fit(train3[train_index,:], train2.loc[train_index]['target'])\n        oof_qda2[idx1[test_index]] = clf.predict_proba(train3[test_index,:])[:,1]\n        preds_qda2[idx2] += clf.predict_proba(test3)[:,1] \/ skf.n_splits","16a0e901":"print('QDA with PCA CV Score =',round(roc_auc_score(train['target'], oof_qda2), 5))","3757b70d":"sub['target'] = preds_qda2\nsub.to_csv('submission_qda2.csv', index=False)","b0eeae20":"sub['target'] = preds_qda2\nsub['target'].hist(bins=100, alpha=0.6)","a9ff317d":"oof_gmm = np.zeros(len(train))\npreds_gmm = np.zeros(len(test))","f531e41b":"def get_mean_cov(x, y):\n    \n    model = GraphicalLasso()\n    ones = (y==1).astype(bool)\n    x2 = x[ones]\n    model.fit(x2)\n    p1 = model.precision_\n    m1 = model.location_\n    \n    onesb = (y==0).astype(bool)\n    x2b = x[onesb]\n    model.fit(x2b)\n    p2 = model.precision_\n    m2 = model.location_\n    \n    ms = np.stack([m1,m2])\n    ps = np.stack([p1,p2])\n    \n    return ms, ps","c04b72d6":"%%time\nfor i in tqdm_notebook(range(512)):\n\n    train2 = train[train['wheezy-copper-turtle-magic']==i]\n    test2 = test[test['wheezy-copper-turtle-magic']==i]\n    idx1 = train2.index\n    idx2 = test2.index\n    train2.reset_index(drop=True, inplace=True)\n    test2.reset_index(drop=True, inplace=True)\n    \n    steps = [\n        ('vt', VarianceThreshold(threshold=1.5))\n    ]\n    \n    pipe = Pipeline(steps=steps)\n    data = pd.concat([pd.DataFrame(train2[cols]), pd.DataFrame(test2[cols])])\n    data2 = pipe.fit_transform(data[cols])\n    train3 = data2[:train2.shape[0]]\n    test3 = data2[train2.shape[0]:]\n    \n    skf = StratifiedKFold(n_splits=11, random_state=42, shuffle=True)\n    for train_index, test_index in skf.split(train3, train2['target']):\n        \n        ms, ps = get_mean_cov(train3[train_index,:], train2.loc[train_index]['target'].values)\n        \n        clf = GaussianMixture(n_components=2, init_params='random', covariance_type='full', tol=0.001, reg_covar=0.001, max_iter=100, n_init=1, means_init=ms, precisions_init=ps)\n        clf.fit(np.concatenate([train3,test3],axis = 0))\n        oof_gmm[idx1[test_index]] = clf.predict_proba(train3[test_index,:])[:,0]\n        preds_gmm[idx2] += clf.predict_proba(test3)[:,0] \/ skf.n_splits","210ff96a":"print('GaussianMixture with VarianceThreshold CV Score =',round(roc_auc_score(train['target'], oof_gmm), 5))","bf9ff9d1":"sub['target'] = preds_gmm\nsub.to_csv('submission_gmm.csv', index=False)","100e2174":"sub['target'] = preds_gmm\nsub['target'].hist(bins=100, alpha=0.6)","dae3f8fa":"oof_lr = np.zeros(len(train))\npreds_lr = np.zeros(len(test))","40cd3fe8":"%%time\nfor i in tqdm_notebook(range(512)):\n\n    train2 = train[train['wheezy-copper-turtle-magic']==i]\n    test2 = test[test['wheezy-copper-turtle-magic']==i]\n    idx1 = train2.index\n    idx2 = test2.index\n    train2.reset_index(drop=True, inplace=True)\n    test2.reset_index(drop=True, inplace=True)\n    \n    steps = [\n        ('vt', VarianceThreshold(threshold=1.5)),\n        ('poly', PolynomialFeatures(degree=2)),\n        ('sc', StandardScaler())\n    ]\n    \n    pipe = Pipeline(steps=steps)\n    data = pd.concat([pd.DataFrame(train2[cols]), pd.DataFrame(test2[cols])])\n    data2 = pipe.fit_transform(data[cols])\n    train3 = data2[:train2.shape[0]]\n    test3 = data2[train2.shape[0]:]\n    \n    skf = StratifiedKFold(n_splits=11, random_state=42, shuffle=True)\n    for train_index, test_index in skf.split(train3, train2['target']):\n                \n        clf = LogisticRegression(solver='saga', penalty='l2', C=0.01, tol=0.001, n_jobs=-1)\n        clf.fit(train3[train_index,:], train2.loc[train_index]['target'])\n        oof_lr[idx1[test_index]] = clf.predict_proba(train3[test_index,:])[:,1]\n        preds_lr[idx2] += clf.predict_proba(test3)[:,1] \/ skf.n_splits","ca643732":"print('Logistic Regression with VarianceThreshold CV Score =',round(roc_auc_score(train['target'], oof_lr), 5))","a7617af2":"sub['target'] = preds_lr\nsub.to_csv('submission_lr.csv', index=False)","c4c112b3":"sub['target'] = preds_lr\nsub['target'].hist(bins=100, alpha=0.6)","8f1adc3a":"oof_ls = np.zeros(len(train))\npreds_ls = np.zeros(len(test))","7393ffd8":"%%time\nfor i in tqdm_notebook(range(512)):\n\n    train2 = train[train['wheezy-copper-turtle-magic']==i]\n    test2 = test[test['wheezy-copper-turtle-magic']==i]\n    idx1 = train2.index\n    idx2 = test2.index\n    train2.reset_index(drop=True, inplace=True)\n    test2.reset_index(drop=True, inplace=True)\n    \n    steps = [\n        ('vt', VarianceThreshold(threshold=1.5))\n    ]\n    \n    pipe = Pipeline(steps=steps)\n    data = pd.concat([pd.DataFrame(train2[cols]), pd.DataFrame(test2[cols])])\n    data2 = pipe.fit_transform(data[cols])\n    train3 = data2[:train2.shape[0]]\n    test3 = data2[train2.shape[0]:]\n    \n    skf = StratifiedKFold(n_splits=11, random_state=42, shuffle=True)\n    for train_index, test_index in skf.split(train3, train2['target']):\n        \n        clf = LabelSpreading(gamma=0.01, kernel='rbf', max_iter=10)\n        clf.fit(train3[train_index,:], train2.loc[train_index]['target'])\n        oof_ls[idx1[test_index]] = clf.predict_proba(train3[test_index,:])[:,1]\n        preds_ls[idx2] += clf.predict_proba(test3)[:,1] \/ skf.n_splits","cd63a1c4":"print('LabelSpreading with VarianceThreshold CV Score =',round(roc_auc_score(train['target'], oof_ls), 5))","0dcdd4bc":"sub['target'] = preds_ls\nsub.to_csv('submission_ls.csv', index=False)","9b3c0602":"sub['target'] = preds_ls\nsub['target'].hist(bins=100, alpha=0.6)","1d542ebe":"oof_knn = np.zeros(len(train))\npreds_knn = np.zeros(len(test))","a570077e":"%%time\nfor i in tqdm_notebook(range(512)):\n\n    train2 = train[train['wheezy-copper-turtle-magic']==i]\n    test2 = test[test['wheezy-copper-turtle-magic']==i]\n    idx1 = train2.index\n    idx2 = test2.index\n    train2.reset_index(drop=True, inplace=True)\n    test.reset_index(drop=True, inplace=True)\n    \n    steps = [\n        ('vt', VarianceThreshold(threshold=2)),\n        ('scaler', StandardScaler())\n    ]\n    \n    pipe = Pipeline(steps=steps)\n    data = pd.concat([pd.DataFrame(train2[cols]), pd.DataFrame(test2[cols])])\n    data2 = pipe.fit_transform(data[cols])\n    train3 = data2[:train2.shape[0]]\n    test3 = data2[train2.shape[0]:]\n    \n    skf = StratifiedKFold(n_splits=11, random_state=42)\n    for train_index, test_index in skf.split(train2, train2['target']):\n\n        clf = KNeighborsClassifier(n_neighbors=17, p=2.9, n_jobs=-1)\n        clf.fit(train3[train_index,:], train2.loc[train_index]['target'])\n        oof_knn[idx1[test_index]] = clf.predict_proba(train3[test_index,:])[:,1]\n        preds_knn[idx2] += clf.predict_proba(test3)[:,1] \/ skf.n_splits","778fde82":"print('kNN with VarianceThreshold and StandardScaler CV Score =',round(roc_auc_score(train['target'], oof_knn), 5))","ed585c35":"sub['target'] = preds_knn\nsub.to_csv('submission_knn.csv', index=False)","b79b9f60":"sub['target'] = preds_knn\nsub['target'].hist(bins=100, alpha=0.6)","29019799":"oof_nn = np.zeros(len(train))\npreds_nn = np.zeros(len(test))","b647d82d":"%%time\nfor i in tqdm_notebook(range(512)):\n\n    train2 = train[train['wheezy-copper-turtle-magic']==i]\n    test2 = test[test['wheezy-copper-turtle-magic']==i]\n    idx1 = train2.index\n    idx2 = test2.index\n    train2.reset_index(drop=True, inplace=True)\n    test2.reset_index(drop=True, inplace=True)\n    \n    steps = [\n        ('vt', VarianceThreshold(threshold=1.5)),\n        ('poly', PolynomialFeatures(degree=2)),\n        ('sc', StandardScaler())\n        # log scale\n    ]\n    \n    pipe = Pipeline(steps=steps)\n    data = pd.concat([pd.DataFrame(train2[cols]), pd.DataFrame(test2[cols])])\n    data2 = pipe.fit_transform(data[cols])\n    train3 = data2[:train2.shape[0]]\n    test3 = data2[train2.shape[0]:]\n    \n    skf = StratifiedKFold(n_splits=11, random_state=42, shuffle=True)\n    for train_index, test_index in skf.split(train3, train2['target']):\n                \n        clf = MLPClassifier(random_state=3, activation='relu', solver='lbfgs', tol=1e-06, hidden_layer_sizes=(250, ))\n        clf.fit(train3[train_index,:], train2.loc[train_index]['target'])\n        oof_nn[idx1[test_index]] = clf.predict_proba(train3[test_index,:])[:,1]\n        preds_nn[idx2] += clf.predict_proba(test3)[:,1] \/ skf.n_splits","4e4c4934":"print('NN with PolynomialFeatures and StandardScaler CV Score =',round(roc_auc_score(train['target'], oof_nn), 5))","c515ae10":"sub['target'] = preds_nn\nsub.to_csv('submission_nn.csv', index=False)","32351bac":"sub['target'] = preds_nn\nsub['target'].hist(bins=100, alpha=0.6)","dd67208f":"oof_qda3 = np.zeros(len(train))\npreds_qda3 = preds_gmm.copy()","0f5d8939":"%%time\nfor i in range(10):\n    \n    test['target'] = preds_qda3\n    test.loc[test['target'] > 0.955, 'target'] = 1\n    test.loc[test['target'] < 0.045, 'target'] = 0\n    usefull_test = test[(test['target'] == 1) | (test['target'] == 0)]\n    new_train = pd.concat([train, usefull_test]).reset_index(drop=True)\n    \n    print(usefull_test.shape[0], \"Test Record added for iteration : \", i + 1)\n    \n    new_train.loc[oof_qda3 > 0.995, 'target'] = 1\n    new_train.loc[oof_qda3 < 0.005, 'target'] = 0\n    \n    oof_qda3 = np.zeros(len(train))\n    preds_qda = np.zeros(len(test))\n    \n    for i in tqdm_notebook(range(512)):\n\n        train2 = new_train[new_train['wheezy-copper-turtle-magic']==i]\n        test2 = test[test['wheezy-copper-turtle-magic']==i]\n        idx1 = train[train['wheezy-copper-turtle-magic']==i].index\n        idx2 = test2.index\n        train2.reset_index(drop=True, inplace=True)\n        test2.reset_index(drop=True, inplace=True)\n        \n        steps = [\n            ('vt', VarianceThreshold(threshold=1.5)),\n            ('scaler', StandardScaler())\n        ]\n        \n        pipe = Pipeline(steps=steps)\n        train3 = pipe.fit_transform(train2[cols])\n        test3 = pipe.fit_transform(test2[cols])\n        \n        skf = StratifiedKFold(n_splits=11, random_state=42)\n        for train_index, test_index in skf.split(train2, train2['target']):\n            oof_test_index = [t for t in test_index if t < len(idx1)]\n            \n            clf = QuadraticDiscriminantAnalysis(reg_params[i])\n            clf.fit(train3[train_index,:],train2.loc[train_index]['target'])\n            if len(oof_test_index) > 0:\n                oof_qda3[idx1[oof_test_index]] = clf.predict_proba(train3[oof_test_index,:])[:,1]\n            preds_qda3[idx2] += clf.predict_proba(test3)[:,1] \/ skf.n_splits\n    \n    print('QDA with Pseued Label by GaussianMixture CV Score =',round(roc_auc_score(train['target'], oof_qda3), 5))","7d56e004":"sub['target'] = preds_qda3\nsub.to_csv('submission_gda3.csv', index=False)","923cc027":"sub['target'] = preds_qda3\nsub['target'].hist(bins=100, alpha=0.6)","04e33264":"oof_qda4 = oof_qda1.copy()\npreds_qda4 = preds_qda1.copy()","a29c34c2":"%%time\nfor i in range(10):\n    \n    test['target'] = preds_qda4\n    test.loc[test['target'] > 0.955, 'target'] = 1\n    test.loc[test['target'] < 0.045, 'target'] = 0\n    usefull_test = test[(test['target'] == 1) | (test['target'] == 0)]\n    new_train = pd.concat([train, usefull_test]).reset_index(drop=True)\n    \n    print(usefull_test.shape[0], \"Test Record added for iteration : \", i + 1)\n    \n    new_train.loc[oof_qda4 > 0.995, 'target'] = 1\n    new_train.loc[oof_qda4 < 0.005, 'target'] = 0\n    \n    oof_qda4= np.zeros(len(train))\n    preds_qd4 = np.zeros(len(test))\n    \n    for i in tqdm_notebook(range(512)):\n\n        train2 = new_train[new_train['wheezy-copper-turtle-magic']==i]\n        test2 = test[test['wheezy-copper-turtle-magic']==i]\n        idx1 = train[train['wheezy-copper-turtle-magic']==i].index\n        idx2 = test2.index\n        train2.reset_index(drop=True, inplace=True)\n        test2.reset_index(drop=True, inplace=True)\n        \n        steps = [\n            ('vt', VarianceThreshold(threshold=1.5)),\n            ('scaler', StandardScaler())\n        ]\n        \n        pipe = Pipeline(steps=steps)\n        train3 = pipe.fit_transform(train2[cols])\n        test3 = pipe.fit_transform(test2[cols])\n        \n        skf = StratifiedKFold(n_splits=11, random_state=42)\n        for train_index, test_index in skf.split(train2, train2['target']):\n            oof_test_index = [t for t in test_index if t < len(idx1)]\n            \n            clf = QuadraticDiscriminantAnalysis(reg_params[i])\n            clf.fit(train3[train_index,:],train2.loc[train_index]['target'])\n            if len(oof_test_index) > 0:\n                oof_qda4[idx1[oof_test_index]] = clf.predict_proba(train3[oof_test_index,:])[:,1]\n            preds_qda4[idx2] += clf.predict_proba(test3)[:,1] \/ skf.n_splits\n    \n    print('GSearch QDA with Pseued Label by QDA CV Score =',round(roc_auc_score(train['target'], oof_qda3), 5))","92c9b624":"sub['target'] = preds_qda4\nsub.to_csv('submission_gda3.csv', index=False)","1cc5657d":"sub['target'] = preds_qda4\nsub['target'].hist(bins=100, alpha=0.6)","ddaa4dc5":"print('All Model CV Score Summary')\nprint('---------------------------------')\nprint('1. QDA with VarianceThreshold and StandardScaler', roc_auc_score(train['target'], oof_qda1))\nprint('2. QDA with PCA', roc_auc_score(train['target'], oof_qda2))\nprint('3. GaussianMixture with VarianceThreshold', roc_auc_score(train['target'], oof_gmm))\nprint('4. Logistic Regression with PolynomialFeatures and StandardScaler', roc_auc_score(train['target'], oof_lr))\nprint('5. LabelSpreading with VarianceThreshold', roc_auc_score(train['target'], oof_ls))\nprint('6. kNN with VarianceThreshold and StandardScaler', roc_auc_score(train['target'], oof_knn))\nprint('P1. QDA with Pseued Label by GaussianMixture', roc_auc_score(train['target'], oof_qda3))\n","99fe9497":"preds_list = [preds_qda1, preds_qda2, preds_gmm, preds_lr, preds_ls, preds_knn, preds_nn, preds_qda3]\noof_list = [oof_qda1, oof_qda2, oof_gmm, oof_lr, oof_ls, oof_knn, oof_nn, oof_qda3]\nprint('The number of model is {}'.format(len(preds_list)))","470f39c8":"oof_avg = sum(oof_list) \/ len(oof_list)\npreds_avg = sum(preds_list) \/ len(preds_list)","fff4c6cb":"print('{} model blend CV score ='.format(len(preds_list)),round(roc_auc_score(train['target'], oof_avg),6))","9967c2ba":"sub['target'] = preds_avg\nsub.to_csv('submission_blend_avg.csv', index=False)","c6be75ae":"sub['target'] = preds_avg\nsub['target'].hist(bins=100, alpha=0.6)","ba175711":"oof_blend = np.zeros(len(train))\npreds_knn = np.zeros(len(test))","da6581c7":"oof_blend = 0.5*(oof_qda3 + oof_qda4)\npreds_blend = 0.5*(preds_qda3 + preds_qda4)","7db3cece":"print('Stacking model blend CV score =',round(roc_auc_score(train['target'], oof_blend),6))","32a2912b":"sub['target'] = preds_blend\nsub.to_csv('submission_blend.csv', index=False)","9dfb3f29":"sub['target'] = preds_blend\nsub['target'].hist(bins=100, alpha=0.6)","9b00b318":"oof_qda1 = oof_qda1.reshape(-1, 1)\noof_qda2 = oof_qda2.reshape(-1, 1)\noof_gmm = oof_gmm.reshape(-1, 1)\noof_lr = oof_lr.reshape(-1, 1)\noof_ls = oof_ls.reshape(-1, 1)\noof_knn = oof_knn.reshape(-1, 1)\noof_nn = oof_nn.reshape(-1, 1)\noof_qda3 = oof_qda3.reshape(-1, 1)","73d5f7f9":"preds_qda1 = preds_qda1.reshape(-1, 1)\npreds_qda2 = preds_qda2.reshape(-1, 1)\npreds_gmm = preds_gmm.reshape(-1, 1)\npreds_lr = preds_lr.reshape(-1, 1)\npreds_ls = preds_ls.reshape(-1, 1)\npreds_knn = preds_knn.reshape(-1, 1)\npreds_nn = preds_nn.reshape(-1, 1)\npreds_qda3 = preds_qda3.reshape(-1, 1)","3a80a5ae":"train_stck = np.concatenate([oof_qda1, oof_qda2, oof_gmm, oof_lr, oof_ls, oof_knn, oof_nn, oof_qda3], axis=1)\ntest_stack = np.concatenate([preds_qda1, preds_qda2, preds_gmm, preds_lr, preds_ls, preds_knn, preds_nn, preds_qda3], axis=1)","12442f7b":"oof_stack = np.zeros(len(train)) \npred_stack = np.zeros(len(test))","64f4a2c3":"for train_index, test_index in skf.split(train_stck, train['target']):\n    clf = LogisticRegression(solver='saga', penalty='l2', C=0.01, tol=0.001)\n    clf.fit(train_stck[train_index], train['target'][train_index])\n    oof_stack[test_index] = clf.predict_proba(train_stck[test_index,:])[:,1]\n    pred_stack += clf.predict_proba(test_stack)[:,1] \/ skf.n_splits","ef11cc66":"print('{} model stack CV score ='.format(len(preds_list)),round(roc_auc_score(train['target'], oof_stack),6))","880f776b":"sub['target'] = pred_stack\nsub.to_csv('submission_stack.csv', index=False)","30259544":"sub['target'] = pred_stack\nsub['target'].hist(bins=100, alpha=0.6)","c6aba576":"<div id=\"p2\">\n<\/div>\n## P2. GSearch QDA with Pseued Label by QDA","35bcfbe9":"<div id=\"1\">\n<\/div>\n## 1. QDA with VarianceThreshold and StandardScaler and get best paramater by GSearch\n- [VarianceThreshold](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.feature_selection.VarianceThreshold.html): Removes all low-variance features (under threshold)\n- [StandardScaler](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.preprocessing.StandardScaler.html): Standardize features by removing the mean and scaling to unit variance\n\n","7902ce09":"<div id=\"3\">\n<\/div>\n## 3. GaussianMixture with VarianceThreshold","a0a9eb15":"# Single Models","2c3d2508":"<div id=\"5\">\n<\/div>\n## 5. LabelSpreading with VarianceThreshold","d281bcc7":"<div id=\"4\">\n<\/div>\n## 4. Logistic Regression with VarianceThreshold, PolynomialFeatures and StandardScaler","158702d9":"<div id=\"s1\">\n<\/div>    \n## Logistic Regression Stacking","ef7dd876":"<div id=\"p1\">\n<\/div>\n## P1. GSearch QDA with Pseued Label by GaussianMixture","273ce2dc":"<div id=\"7\">\n<\/div>\n## 7. NN with PolynomialFeatures and StandardScaler","32f0b0f9":"<div id=\"2\">\n<\/div>\n## 2. QDA with PCA","b0f14712":"## Load data","40a9f6cf":"<div id=\"b1\">\n<\/div>\n## AVG Blending","8bbdc501":"<div id=\"6\">\n<\/div>\n## 6. kNN with VarianceThreshold and StandardScaler","cdca8720":"# Summary\nThe purpose of this kernel is that summarizing the result of several model output.  It seems that QDA and GMM had higher CV score than other models. Ensembling models got slightly better score but not boosting... so we need more explor something like make_classification method..\n\n## Single Models\n- [1. QDA with VarianceThreshold and StandardScaler](#1)\n   - CV: 0.96476\n- [2. QDA with PCA](#2)\n   - CV: 0.96457\n- [3. GaussianMixture with VarianceThreshold](#3)\n   - CV: 0.96748\n- [4. Logistic Regression with VarianceThreshold, PolynomialFeatures and StandardScaler](#4)\n   - CV: 0.95049\n- [5. LabelSpreading with VarianceThreshold](#5)\n   - CV: 0.93674\n- [6. kNN with VarianceThreshold and StandardScaler](#6)\n   - CV: 0.91593\n- [7. NN with PolynomialFeatures and StandardScaler](#7)\n   - CV: 0.94289\n\n## Single Model with Pseued Label\n- [P1. GSearch QDA with Pseued Label by GaussianMixture](#p1)\n   - CV: 0.96861\n\n## Blending\n- [AVG Blending](#b1)\n   - CV: 0.968017\n\n## Stacking\n- [Logistic Regression](#s1)\n    - CV: 0.969105","6b515890":"## Preparation","2c68f718":"# Ensembling","e4f21cc7":"## Stacking","075978ab":"## Stacking Model Blending"}}