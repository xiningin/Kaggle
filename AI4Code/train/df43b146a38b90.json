{"cell_type":{"5dd00483":"code","d973386e":"code","77f5e091":"code","44bec876":"code","0b375e7f":"code","13e34d3b":"code","4302edff":"code","df934a99":"code","77dc7e6c":"code","d3b93b59":"code","5e508ad5":"code","703b62ca":"code","b72f6df1":"code","dc7469b3":"code","2d93649f":"code","41eeb0e0":"code","46c2a77e":"code","4cef4165":"code","5e4b3aa4":"code","58fc3164":"code","e2afcbd8":"code","dadf56cb":"code","0ea3d5f9":"code","7d7f2a0b":"code","5cdc42d1":"code","978a38e5":"code","686b4dc0":"code","532da740":"code","ae21add8":"code","7526f274":"code","ac482202":"code","b47f7937":"code","389dd06d":"code","7950280b":"code","84559e76":"code","bb5b432b":"markdown","44cefa71":"markdown","0469f8ec":"markdown","a37f551f":"markdown","f74b246a":"markdown","397a8f1b":"markdown","b7058582":"markdown","8417b89b":"markdown","989bf5b2":"markdown","058dd3a0":"markdown","3d7a051d":"markdown","5ff5349f":"markdown"},"source":{"5dd00483":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt # plot data\nimport sklearn #scikit-learn library, where the magic happens!\nimport seaborn as sns # beautiful graphs\nimport re","d973386e":"df_train = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv',na_filter=False)\ndf_test = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv',na_filter=False)","77f5e091":"# Compute the correlation matrix\ncorr = df_train.corr()\n# Generate a mask for the upper triangle\nmask = np.triu(np.ones_like(corr, dtype=bool))\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(11, 9))\n# Generate a custom diverging colormap\ncmap = sns.diverging_palette(230, 20, as_cmap=True)\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(corr, cmap=cmap, mask=mask, vmax=.3, center=0, annot=False, square=True, linewidths=.5, cbar_kws={\"shrink\": .5})","44bec876":"df_train[df_train.columns[1:]].corr()['SalePrice'][:].abs().sort_values(ascending=False)","0b375e7f":"df_train_20 = df_train[['Id','SalePrice','OverallQual','GrLivArea','GarageCars','GarageArea','TotalBsmtSF','1stFlrSF','FullBath','TotRmsAbvGrd','YearBuilt','YearRemodAdd']]\ndf_test_20 = df_test[['Id','OverallQual','GrLivArea','GarageCars','GarageArea','TotalBsmtSF','1stFlrSF','FullBath','TotRmsAbvGrd','YearBuilt','YearRemodAdd']]","13e34d3b":"df_train_20","4302edff":"print('df_train:')\nprint(df_train_20.isnull().sum())\nprint('\\ndf_test:')\nprint(df_test_20.isnull().sum())","df934a99":"X_train = df_train_20.drop(columns=['SalePrice','Id'])\ny_train = df_train_20['SalePrice']\n\nX_test = df_test_20.drop(columns=['Id'])","77dc7e6c":"X_test.fillna(0, inplace=True)","d3b93b59":"# from sklearn.ensemble import RandomForestClassifier\n\n# tree = RandomForestClassifier(random_state=0)\n# tree.fit(X_train, y_train)\n\n# y_test = pd.Series(tree.predict(X_test))\n\n# df_final = pd.concat([df_test['Id'], y_test], axis=1, sort=False)\n# df_final = df_final.rename(columns={0:\"SalePrice\"})\n# df_final.to_csv(r'020731.csv', index = False)","5e508ad5":"df_train['Neighborhood'].value_counts()","703b62ca":"dict_neighbor = {\n'NAmes'  :{'lat': 42.045830,'lon': -93.620767},\n'CollgCr':{'lat': 42.018773,'lon': -93.685543},\n'OldTown':{'lat': 42.030152,'lon': -93.614628},\n'Edwards':{'lat': 42.021756,'lon': -93.670324},\n'Somerst':{'lat': 42.050913,'lon': -93.644629},\n'Gilbert':{'lat': 42.060214,'lon': -93.643179},\n'NridgHt':{'lat': 42.060357,'lon': -93.655263},\n'Sawyer' :{'lat': 42.034446,'lon': -93.666330},\n'NWAmes' :{'lat': 42.049381,'lon': -93.634993},\n'SawyerW':{'lat': 42.033494,'lon': -93.684085},\n'BrkSide':{'lat': 42.032422,'lon': -93.626037},\n'Crawfor':{'lat': 42.015189,'lon': -93.644250},\n'Mitchel':{'lat': 41.990123,'lon': -93.600964},\n'NoRidge':{'lat': 42.051748,'lon': -93.653524},\n'Timber' :{'lat': 41.998656,'lon': -93.652534},\n'IDOTRR' :{'lat': 42.022012,'lon': -93.622183},\n'ClearCr':{'lat': 42.060021,'lon': -93.629193},\n'StoneBr':{'lat': 42.060227,'lon': -93.633546},\n'SWISU'  :{'lat': 42.022646,'lon': -93.644853}, \n'MeadowV':{'lat': 41.991846,'lon': -93.603460},\n'Blmngtn':{'lat': 42.059811,'lon': -93.638990},\n'BrDale' :{'lat': 42.052792,'lon': -93.628820},\n'Veenker':{'lat': 42.040898,'lon': -93.651502},\n'NPkVill':{'lat': 42.049912,'lon': -93.626546},\n'Blueste':{'lat': 42.010098,'lon': -93.647269}\n}","b72f6df1":"df_train['Lat'] = df_train['Neighborhood'].map(lambda neighbor: dict_neighbor[neighbor]['lat'])\ndf_train['Lon'] = df_train['Neighborhood'].map(lambda neighbor: dict_neighbor[neighbor]['lon'])\n\ndf_test['Lat'] = df_test['Neighborhood'].map(lambda neighbor: dict_neighbor[neighbor]['lat'])\ndf_test['Lon'] = df_test['Neighborhood'].map(lambda neighbor: dict_neighbor[neighbor]['lon'])","dc7469b3":"df_train.select_dtypes('object').columns","2d93649f":"from sklearn import preprocessing\n\nfor columns in df_train.select_dtypes('object').columns:\n    enc = preprocessing.LabelEncoder()\n    enc.fit(pd.concat([df_train[columns].astype(str), df_test[columns].astype(str)],join='outer',sort=False))\n    df_train[columns] = enc.transform(df_train[columns])\n    df_test[columns] = enc.transform(df_test[columns])","41eeb0e0":"# for col in df_train.columns:\n#     print(df_train[col].astype(str).str.contains().any())","46c2a77e":"# X_train.fillna(0, inplace=True)\n# X_test.fillna(0, inplace=True)","4cef4165":"for columns in df_test.select_dtypes('object').columns:\n    df_test[columns] = pd.to_numeric(df_test[columns],errors='coerce')\n    \ndf_test.fillna(0, inplace=True)","5e4b3aa4":"# Compute the correlation matrix\ncorr = df_train.corr()\n\ncorr['SalePrice'].abs().sort_values(ascending=False).head(40)","58fc3164":"# cols = ['SalePrice','OverallQual','GrLivArea','GarageCars','ExterQual','GarageArea','TotalBsmtSF','1stFlrSF','BsmtQual','KitchenQual','FullBath','TotRmsAbvGrd','YearBuilt','YearRemodAdd']\n# g = sns.PairGrid(df_train[cols], height = 2.5)\n# g.map(sns.scatterplot)","e2afcbd8":"var = 'TotalBsmtSF'\ndata = pd.concat([df_train['SalePrice'], df_train[var]], axis=1)\ndata.plot.scatter(x=var, y='SalePrice', ylim=(0,800000));","dadf56cb":"df_train = df_train.drop((df_train[df_train['SalePrice']>700000]).index)\ndf_train = df_train.drop((df_train[df_train['TotalBsmtSF']>5000]).index)","0ea3d5f9":"var = 'GrLivArea'\ndata = pd.concat([df_train['SalePrice'], df_train[var]], axis=1)\ndata.plot.scatter(x=var, y='SalePrice', ylim=(0,800000));","7d7f2a0b":"df_train = df_train.drop((df_train[df_train['GrLivArea']>4000]).index)","5cdc42d1":"var = 'GrLivArea'\ndata = pd.concat([df_train['SalePrice'], df_train[var]], axis=1)\ndata.plot.scatter(x=var, y='SalePrice');","978a38e5":"from scipy.stats import norm\nsns.distplot(df_train['SalePrice'], fit=norm);\nfig = plt.figure()","686b4dc0":"df_train['SalePrice'] = np.log(df_train['SalePrice'])","532da740":"sns.distplot(df_train['SalePrice'], fit=norm);\nfig = plt.figure()","ae21add8":"var = 'GrLivArea'\ndata = pd.concat([df_train['SalePrice'], df_train[var]], axis=1)\ndata.plot.scatter(x=var, y='SalePrice');","7526f274":"df_train.isnull().sum().sum()","ac482202":"df_test.isnull().sum().sum()","b47f7937":"df_train.dtypes[df_train.dtypes == 'float64']","389dd06d":"df_test[['BsmtFinSF1','BsmtFinSF2','BsmtUnfSF','TotalBsmtSF','BsmtFullBath','BsmtHalfBath','GarageCars','GarageArea']] = df_test[['BsmtFinSF1','BsmtFinSF2','BsmtUnfSF','TotalBsmtSF','BsmtFullBath','BsmtHalfBath','GarageCars','GarageArea']].astype(int)\n\ndf_test.dtypes[df_test.dtypes == 'float64']","7950280b":"X_train = df_train.drop(columns=['SalePrice','Id','GarageArea','1stFlrSF','BsmtFinSF1','BsmtFinSF2'])\ny_train = df_train['SalePrice']\n\nX_test = df_test.drop(columns=['Id','GarageArea','1stFlrSF','BsmtFinSF1','BsmtFinSF2'])","84559e76":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import RandomForestRegressor\n\n\n#tree = RandomForestClassifier(max_depth = 10, min_samples_split = 4, n_estimators = 500, random_state=0)\n#tree = RandomForestClassifier(max_depth = 20, n_estimators = 1000, random_state=0)\n#tree = RandomForestRegressor(max_depth = 20, n_estimators = 1000, random_state=0)\ntree = RandomForestRegressor(max_depth = 20, n_estimators = 1000, random_state=0)\n\n\ntree.fit(X_train, y_train)\n\ny_test = pd.Series(tree.predict(X_test))\n\ndf_final = pd.concat([df_test['Id'], y_test], axis=1, sort=False)\ndf_final = df_final.rename(columns={0:\"SalePrice\"})\ndf_final['SalePrice'] = np.exp(df_final['SalePrice'])\ndf_final.to_csv(r'random_forest.csv', index = False)","bb5b432b":"Nice, we got down to 0.14428","44cefa71":"Ok, we have awayyyy to much info for a initial run.\nI would take to much time to to underst all that data, and if this was a business case, my manager would kill me to take that much time.\n\nSo let's go by Pareto principle and analize the 20% cases that will give us the 80% results.\n\n20% of the 36 colummns would be 7,2 columns, since the number of columns with a 0,5 corr. is 10, let's go with that.","0469f8ec":"Ok, now we have less of a cone shape, and more of a line with a lot of noise.","a37f551f":"## OneHot","f74b246a":"Ok, this give us a RMSE of 0.20731.\n\nLets try one with all the info.\n\n## All the info","397a8f1b":"Since we have only 25 neighborhoods, let's search their lat+lon by hand and do a replace with a dictionary.","b7058582":"## homoscedasticity","8417b89b":"## Remove Outliers","989bf5b2":"## Random Forrest","058dd3a0":"Ok,this image is a lot to take at once, but it show us that:\n\n- GrLivArea is highly corr. to 1stFlrSF, TotalBsmtSF\n- Some corr. from GarageCars and GarageArea\n- We have some outliers in the scaterplot of price and TotalBsmtSF, price and GarageArea and price and GrLivArea\n- We have a cone like shape on the scaterplot of SalePrice and 1stFlrSF, GarageArea, GrLivArea. This normaly indicates homoscedasticity, this tell us that we should probably try to normalize this values.","3d7a051d":"## New analysis","5ff5349f":"We have 81 columns, wich is a Loooooootttttt....\nWe will not touch the ones that are int or float (leave that to the machine), but let's analyze the other ones.\n\nMost of then are Quali info (Is the street acess to the propety gravel or paved?) wich we can do a initial OneHot and see were it get us.\n\nBut one of then is the **Neighborhood**: Physical locations within Ames city limits, this one if we do a OneHot we lose the reference that we get from learning where the neighbors are in reference to one another.\n\n## Neighborhood"}}