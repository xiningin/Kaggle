{"cell_type":{"0da28c23":"code","81402044":"code","8b3db731":"code","172968f9":"code","e85f4018":"code","90dcca65":"code","d2e7f992":"code","2590b207":"code","855752e5":"code","5b87c9ce":"code","866fdb6f":"code","12f09e83":"code","8c3fc686":"code","61d6356e":"code","0a7dda0a":"code","78c1a1ee":"code","ee74b218":"code","27dcdfe1":"code","8d8d4a23":"code","56e49ebb":"code","734ba583":"code","2b8e3c03":"code","df4603bc":"code","18bbf4ad":"code","fe333c9c":"code","92a8a307":"code","57b714a7":"code","b1f3ef16":"code","d8df7a74":"code","19e23823":"code","e316f551":"code","b12e6e24":"code","7e21b793":"code","d865bc3b":"code","9d5155db":"code","2aba7b9d":"code","d12f27c3":"code","abcce448":"code","e38e977a":"code","3e32e93f":"code","3e6e6141":"code","907c18f4":"code","41b34366":"code","2518e89b":"code","81bc5762":"code","1a36f1b3":"code","20b150a8":"code","ecfe276b":"code","a89fe28f":"code","38b6f053":"code","a7285979":"code","62d4e491":"code","edc5269e":"code","a23a0911":"code","7fb572ed":"code","5042f39b":"code","b305ee21":"code","4964f5f3":"code","e4d2fbfc":"code","c4b23cfd":"code","b5dc8765":"code","cbaa1fa3":"code","d7677a11":"code","796a0847":"code","643eaef8":"code","0e36b468":"code","13801ce9":"code","cfa50d60":"code","dbc7cd55":"code","bda1f1a1":"code","c627c5b2":"code","8d42b30b":"code","215a1be9":"code","0ccb1588":"code","9ab1b1a7":"code","bf701060":"code","ef7f264d":"code","53446bb5":"code","d1b77743":"code","8016fda4":"code","c6aace06":"code","f65ebaf9":"code","41188124":"code","a915915e":"code","4e443a8c":"code","c495ba42":"code","56cbc8c4":"code","8dadc910":"code","b45f33bc":"code","ea1c1b39":"code","892e6879":"code","311e7eb0":"code","14a1d163":"code","b44b93f3":"code","d0bfb143":"code","c18b292a":"code","335e2d45":"code","af4efdd5":"code","07548e08":"code","17a31736":"code","c1c13078":"code","8ddc2984":"code","a341033d":"code","a8359fbb":"code","e4bb5108":"code","e6bd230e":"code","8a6e4232":"code","f218380e":"code","64c55ceb":"code","c1b6a4af":"code","188b723c":"code","9cab232f":"code","ef9996bd":"code","368dafc7":"markdown","66e0a7ea":"markdown","6934a015":"markdown","2cf61aaf":"markdown","d7238287":"markdown","90bbf943":"markdown","b4cedaa9":"markdown","857cf52f":"markdown","50ef1bc0":"markdown","50f97444":"markdown","3351a365":"markdown","9aeab536":"markdown","f8c432e3":"markdown","826479cc":"markdown","c27be70f":"markdown","c9fd3896":"markdown","dc569088":"markdown","1ade7225":"markdown","0efce39d":"markdown","0aed980d":"markdown","2e2ff934":"markdown","23edb6e0":"markdown","6c136d49":"markdown","1eab1012":"markdown","888d6adb":"markdown","503ee1c4":"markdown","09ccd541":"markdown","0d4ab89e":"markdown","39fc3a65":"markdown","f9ce150f":"markdown","95f70c05":"markdown","2096edba":"markdown","66ce986f":"markdown","f9e5c9a6":"markdown","ff0f558c":"markdown","2b07ee92":"markdown","ad07c5f5":"markdown","201eef9e":"markdown","fb1ec448":"markdown","65330132":"markdown","4e1caa88":"markdown","c6ec3f63":"markdown","cbec68a1":"markdown","51a92a9f":"markdown","c21dcf36":"markdown","f993dfa2":"markdown","c500e72e":"markdown","d1463d21":"markdown","7a79d796":"markdown","2ae6ce8a":"markdown","3df286cc":"markdown"},"source":{"0da28c23":"# import libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom matplotlib.pyplot import xticks\nfrom nltk.corpus import stopwords\nimport nltk\nimport re\nfrom nltk.stem import WordNetLemmatizer\nimport string\nfrom nltk.tokenize import word_tokenize\nfrom nltk.util import ngrams\nfrom collections import defaultdict\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\nfrom keras.utils.vis_utils import plot_model\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, Callback\nimport tensorflow as tf\nfrom sklearn.metrics import f1_score\nfrom wordcloud import WordCloud,STOPWORDS\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.pipeline import Pipeline\nfrom keras.preprocessing.sequence import pad_sequences\nfrom numpy import array\nfrom numpy import asarray\nfrom numpy import zeros\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Dense,Flatten,Embedding,Activation,Dropout\nfrom keras.layers import Conv1D,MaxPooling1D,GlobalMaxPooling1D,LSTM","81402044":"# load train and test datasets\ntrain= pd.read_csv('..\/input\/nlp-getting-started\/train.csv')\ntest=pd.read_csv('..\/input\/nlp-getting-started\/test.csv')","8b3db731":"# check the no. of rows and columns in the dataset\ntrain.shape, test.shape","172968f9":"train.head()","e85f4018":"train.isnull().sum().sort_values(ascending = False)","90dcca65":"# function to draw bar plot\ndef draw_bar_plot(category,length,xlabel,ylabel,title,sub):\n    plt.subplot(2,2,sub)\n    plt.bar(category, length,color = 'grbkymc')\n    plt.legend()\n    plt.xlabel(xlabel, fontsize=15)\n    plt.ylabel(ylabel, fontsize=15)\n    plt.title(title, fontsize=15)\n    #plt.show()","d2e7f992":"# function to draw histogram\ndef draw_hist(xlabel, ylabel,title,target,sub,color):\n    plt.subplot(1,2,sub)\n    plt.hist(train[train.target==target][\"length\"],color = color)\n    plt.title(title,fontsize=25)\n    plt.xlabel(xlabel,fontsize=15)\n    plt.ylabel(ylabel,fontsize=15)\n    plt.ylim(0,1200)\n    plt.grid()","2590b207":"# function to draw graphs for stopwords and punctuations\ndef draw_bar_n_plot(data,title):\n# lets visualize the top 10 stop words\n    x,y=zip(*data)\n\n    plt.figure(figsize = (25,10))\n    plt.subplot(1,2,1)\n    plt.bar(x,y,color='grbkymc')\n    plt.title(\"Top 10 \"+ title,fontsize=25)\n    plt.xlabel(title,fontsize=15)\n    plt.ylabel(\"Count\",fontsize=15)\n    plt.grid()\n\n    plt.subplot(1,2,2)\n    plt.plot(x,y,'g')\n    plt.title(\"Top 10 \"+ title,fontsize=25)\n    plt.xlabel(title,fontsize=15)\n    plt.ylabel(\"Count\",fontsize=15)\n    plt.grid()","855752e5":"# check class distribution\n\nprint(\"No. of Real Disaster Tweets (Target = 1):\",len(train[train[\"target\"]==1]))\nprint(\"No. of Fake Disaster Tweets (Target = 0):\",len(train[train[\"target\"]==0]))","5b87c9ce":"# lets visualize the class distribution\nplt.figure(figsize = (12,8))\ndraw_bar_plot([\"Real\",\"Fake\"],[len(train[train.target==1]), len(train[train.target==0])],\"Real Vs Fake\",\"Number of Tweets\",\"Class Distribution\",1)","866fdb6f":"# we will now check the length of \"real disaster\" vs lenght of \"fake disaster\" tweets\n# lets first add a new field to the dataset called \"length\"\ndef length(text):    \n    return len(text)\n\ntrain[\"length\"]= train.text.apply(length)","12f09e83":"# lets see the distribution of length of tweets real vs fake\n\nplt.figure(figsize = (20,8))\ndraw_hist(\"Real Disaster Tweets\",\"Length of Tweets\",\"Length of Real Disaster Tweets\",1, 1,\"darkgreen\")\ndraw_hist(\"Fake Disaster Tweets\",\"Length of Tweets\",\"Length of Fake Disaster Tweets\",0, 2,\"darkred\")\n","8c3fc686":"# lets check the average lenght of real vs fake tweets\nprint(train.groupby(\"target\").mean()[\"length\"].sort_values(ascending = False))\n\n# lets visualize the class distribution\nplt.figure(figsize = (12,8))\ndraw_bar_plot([\"Real\",\"Fake\"],[train[train.target==1].mean()[\"length\"], train[train.target==0].mean()[\"length\"]],\"Real Vs Fake\",\"Average Length\",\"Average Text Length - Real Vs Fake\",1)","61d6356e":"# lets drop the column\ntrain.drop(\"length\",1,inplace=True)","0a7dda0a":"#lets save stopwords in a variable\nstop = list(stopwords.words(\"english\"))","78c1a1ee":"# stopwords present in the whole dataset\nsw = []\nfor message in train.text:\n    for word in message.split():\n        if word in stop:\n            sw.append(word)\n\n\n# lets convert the list to a dictinoary which would contain the stop words and their frequency\nwordlist = nltk.FreqDist(sw)\n# lets save the 10 most frequent stopwords\ntop10 = wordlist.most_common(10)","ee74b218":"# Graphs for top 10 stopwords present in all the tweets\ndraw_bar_n_plot(top10,\"Stopwords\")","27dcdfe1":"# save list of punctuation\/special characters in a variable\npunctuation = list(string.punctuation)","8d8d4a23":"# punctuations present in all the tweets \npun = []\nfor message in train.text:\n    for word in message.split():\n        if word in punctuation:\n            pun.append(word)\n\n\n# lets convert the list to a dictinoary which would contain the punctuations and their frequency\nwordlist = nltk.FreqDist(pun)\n# lets save the 10 most frequent stopwords\ntop10 = wordlist.most_common(10)","56e49ebb":"# draw graphs for top10 Punctuations\ndraw_bar_n_plot(top10,\"Punctuations\")","734ba583":"# Let's check stop words and punctuations in \"Real Disaster Tweets\"\nstop_real = []\npun_real  = []\nfor message in train[train.target==1][\"text\"]:\n    for word in message.split():\n        if word in stop:\n            stop_real.append(word)\n        if word in punctuation:\n            pun_real.append(word)\n\n\n# lets convert the list to a dictinoary which would contain the stop word and its frequency\nstop_real_wordlist = nltk.FreqDist(stop_real)\npun_real_wordlist =  nltk.FreqDist(pun_real)\n\n# lets save the 10 most frequent stopwords\nstop_real_top10 = stop_real_wordlist.most_common(10)\npun_real_top10  = pun_real_wordlist.most_common(10)","2b8e3c03":"# Let's check \"Fake Disaster Tweets\" and create a list of stop words and punctuations\nstop_fake = []\npun_fake  = []\nfor message in train[train.target==0][\"text\"]:\n    for word in message.split():\n        if word in stop:\n            stop_fake.append(word)\n        if word in punctuation:\n            pun_fake.append(word)\n\n\n# lets convert the list to a dictinoary which would contain the stop word and its frequency\nstop_fake_wordlist = nltk.FreqDist(stop_fake)\npun_fake_wordlist =  nltk.FreqDist(pun_fake)\n\n# lets save the 10 most frequent stopwords\nstop_fake_top10 = stop_fake_wordlist.most_common(10)\npun_fake_top10  = pun_fake_wordlist.most_common(10)","df4603bc":"x_stop_real,y_stop_real=zip(*stop_real_top10)\nx_pun_real, y_pun_real =zip(*pun_real_top10)\n\nx_stop_fake,y_stop_fake=zip(*stop_fake_top10)\nx_pun_fake, y_pun_fake=zip(*pun_fake_top10)\n\n\nplt.figure(figsize = (30,30))\ndraw_bar_plot(x_stop_real,y_stop_real,\"Stopwords\",\"Count\",\"Top 10 Stopwords - Real Tweets\",1)\ndraw_bar_plot(x_stop_fake,y_stop_fake,\"Stopwords\",\"Count\",\"Top 10 Stopwords - Fake Tweets\",2)\ndraw_bar_plot(x_pun_real,y_pun_real,\"Punctuations\",\"Count\",\"Top 10 Punctuations - Real Tweets\",3)\ndraw_bar_plot(x_pun_fake,y_pun_fake,\"Punctuations\",\"Count\",\"Top 10 Punctuations - Fake Tweets\",4)","18bbf4ad":"# create an object to convert the words to its lemma form\nlemma = WordNetLemmatizer()","fe333c9c":"# lets make a combine list of stopwords and punctuations\nsw_pun = stop + punctuation","92a8a307":"# function to preprocess the messages\ndef preprocess(tweet):\n    tweet = re.sub(r\"https?:\\\/\\\/t.co\\\/[A-Za-z0-9]+\", \"\", tweet) # removing urls \n    tweet = re.sub('[^\\w]',' ',tweet) # remove embedded special characters in words (for example #earthquake)         \n    tweet = re.sub('[\\d]','',tweet) # this will remove numeric characters\n    tweet = tweet.lower()\n    words = tweet.split()  \n    sentence = \"\"\n    for word in words:     \n        if word not in (sw_pun):  # removing stopwords & punctuations                \n            word = lemma.lemmatize(word,pos = 'v')  # converting to lemma    \n            if len(word) > 3: # we will consider words with length  greater than 3 only\n                sentence = sentence + word + ' '             \n    return(sentence)","57b714a7":"# apply preprocessing functions on the train and test datasets\ntrain['text'] = train['text'].apply(lambda s : preprocess(s))\ntest ['text'] = test ['text'].apply(lambda s : preprocess(s))","b1f3ef16":"# function to remove emojis\ndef remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)","d8df7a74":"# applying the function on the train and the test datasets\ntrain['text'] = train['text'].apply(lambda s : remove_emoji(s))\ntest ['text'] = test ['text'].apply(lambda s : remove_emoji(s))\n","19e23823":"# function to create vocab\nfrom collections import Counter\ndef create_vocab(df):\n    vocab = Counter()\n    for i in range(df.shape[0]):\n        vocab.update(df.text[i].split())\n    return(vocab)\n\n","e316f551":"# concatenate training and testing datasets\nmaster=pd.concat((train,test)).reset_index(drop=True)\n\n# call vocabulary creation function on master dataset\nvocab = create_vocab(master)\n\n# lets check the no. of words in the vocabulary\nlen(vocab)","b12e6e24":"# lets check the most common 50 words in the vocabulary\nvocab.most_common(50)\n","7e21b793":"# create the final vocab by considering words with more than one occurence\nfinal_vocab = []\nmin_occur = 2\nfor k,v in vocab.items():\n    if v >= min_occur:\n        final_vocab.append(k)","d865bc3b":"# lets check the no. of the words in the final vocabulary\nlen(final_vocab)\n","9d5155db":"# function to filter the dataset, keep only words which are present in the vocab\ndef filter(tweet):\n    sentence = \"\"\n    for word in tweet.split():  \n        if word in final_vocab:\n            sentence = sentence + word + ' '\n    return(sentence)","2aba7b9d":"# apply filter function on the train and test datasets\ntrain['text'] = train['text'].apply(lambda s : filter(s))\ntest ['text'] = test ['text'].apply(lambda s : filter(s))","d12f27c3":"# lets take a look at the update training dataset\ntrain.text.head()","abcce448":"# lets create seperate datasets from real and fake tweets\nreal = train[train.target==1].reset_index()\nfake = train[train.target==0].reset_index()","e38e977a":"# function to create top 100 n-grams\ndef get_ngrams(data,n):\n    all_words = []\n    for i in range(len(data)):\n        temp = data[\"text\"][i].split()\n        for word in temp:\n            all_words.append(word)\n\n    tokenized = all_words\n    esBigrams = ngrams(tokenized, n)\n\n    esBigram_wordlist = nltk.FreqDist(esBigrams)\n    top100 = esBigram_wordlist.most_common(100)\n    top100 = dict(top100)\n    df_ngrams = pd.DataFrame(sorted(top100.items(), key=lambda x: x[1])[::-1])\n    return df_ngrams","3e32e93f":"# function to visualize the top 100 n-grams in real and fake disaster tweets\ndef draw_barplots(real,fake,title):\n    plt.figure(figsize = (40,80),dpi=100)\n\n    plt.subplot(1,2,1)\n    sns.barplot(y=real[0].values[:100], x=real[1].values[:100], color='green')\n    plt.title(\"Top 100\" + title + \"in Real Tweets\",fontsize=15)\n    \n    plt.subplot(1,2,2)\n    sns.barplot(y=fake[0].values[:100], x=fake[1].values[:100],color='red')\n    plt.title(\"Top 100\" + title + \"in Fake Tweets\",fontsize=15)","3e6e6141":"# lets create top 100 unigrams\nreal_unigrams = get_ngrams(real,1)\nfake_unigrams = get_ngrams(fake,1)","907c18f4":"# lets visualize top 100 unigrams\ndraw_barplots(real_unigrams,fake_unigrams,\" Unigrams \")","41b34366":"# lets create top 100 bigrams\nreal_bigrams = get_ngrams(real,2)\nfake_bigrams = get_ngrams(fake,2)","2518e89b":"# lets visualize top 100 bigrams\n\ndraw_barplots(real_bigrams,fake_bigrams,\" Bigrams \")","81bc5762":"# lets create top 100 trigrams\nreal_trigrams = get_ngrams(real,3)\nfake_trigrams = get_ngrams(fake,3)","1a36f1b3":"# lets visualize top 100 trigrams\ndraw_barplots(real_trigrams,fake_trigrams,\" Trigrams \")","20b150a8":"def word_cloud(df):\n    comment_words = '' \n    stopwords = set(STOPWORDS) \n\n    # iterate through the csv file \n    for val in df.text: \n\n        # typecaste each val to string \n        val = str(val) \n\n        # split the value \n        tokens = val.split() \n        \n        for i in range(len(tokens)): \n            tokens[i] = tokens[i].lower()\n        \n        comment_words += \" \".join(tokens)+\" \"\n        #return comment_words\n\n    wordcloud = WordCloud(width = 800, height = 800, \n            background_color ='white', \n            stopwords = stopwords, \n            min_font_size = 10).generate(comment_words) \n  \n    # plot the WordCloud image                        \n    plt.figure(figsize = (8, 8), facecolor = None) \n    plt.imshow(wordcloud) \n    plt.axis(\"off\") \n    plt.tight_layout(pad = 0) \n    plt.show() ","ecfe276b":"# world cloud for real disaster tweets\nword_cloud(real)","a89fe28f":"# world cloud for fake disaster tweets\nword_cloud(fake)","38b6f053":"# function to calculate f1 score for each epoch\nimport keras.backend as K\ndef get_f1(y_true, y_pred): #taken from old keras source code\n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n    precision = true_positives \/ (predicted_positives + K.epsilon())\n    recall = true_positives \/ (possible_positives + K.epsilon())\n    f1_val = 2*(precision*recall)\/(precision+recall+K.epsilon())\n    return f1_val\n","a7285979":"# Bag of Words model\nfrom keras.preprocessing.text import Tokenizer\n\n# fit a tokenizer\ndef create_tokenizer(lines):\n    tokenizer = Tokenizer()\n    tokenizer.fit_on_texts(lines)\n    return tokenizer","62d4e491":"# lets use only tweet text to build the model\nX = train.text\ny = train.target\n\ntest_id = test.id\ntest.drop([\"id\",\"location\",\"keyword\"],1,inplace = True)","edc5269e":"# Test train split \nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)","a23a0911":"# create and apply tokenizer on the training dataset\ntokenizer = create_tokenizer(X_train)\nX_train_set = tokenizer.texts_to_matrix(X_train, mode = 'freq')\n","7fb572ed":"# define the model\ndef define_model(n_words):\n    # define network\n    model = Sequential()\n    model.add(Dense(128, input_shape=(n_words,), activation='relu'))\n    model.add(Dense(1, activation='sigmoid'))\n    # compile network\n    model.compile(loss='binary_crossentropy', optimizer='adam', metrics = [get_f1])\n    # summarize defined model\n    model.summary()\n    plot_model(model, to_file='model.png', show_shapes=True)\n    return model","5042f39b":"# create the model\nn_words = X_train_set.shape[1]\nmodel = define_model(n_words)","b305ee21":"#fit network\nmodel.fit(X_train_set,y_train,epochs=10,verbose=2)","4964f5f3":"# prediction on the test dataset\nX_test_set = tokenizer.texts_to_matrix(X_test, mode = 'freq')\ny_pred = model.predict_classes(X_test_set)","e4d2fbfc":"# important metrices\nprint(classification_report(y_test, y_pred))","c4b23cfd":"# apply tokenizer on the test dataset\ntest_set = tokenizer.texts_to_matrix(test.text, mode = 'freq')","b5dc8765":"# make predictions on the test dataset\ny_test_pred = model.predict_classes(test_set)","cbaa1fa3":"# lets prepare for the prediction submission\nsub = pd.DataFrame()\nsub['Id'] = test_id\nsub['target'] = y_test_pred\nsub.to_csv('submission_1.csv',index=False)","d7677a11":"# Fitting a tokenizer on text will create a list of unique words with an integer assigned to it\nt = Tokenizer()\nt.fit_on_texts(X_train.tolist())","796a0847":"# lets save the size of the vocab\nvocab_size = len(t.word_index) + 1","643eaef8":"# load the whole embedding into memory\nembeddings_index = dict()\nf = open('..\/input\/glove6b100dtxt\/glove.6B.100d.txt', mode='rt', encoding='utf-8')\nfor line in f:\n    values = line.split()\n    word = values[0]\n    coefs = asarray(values[1:], dtype='float32')\n    embeddings_index[word] = coefs\nf.close()\nprint('Loaded %s word vectors.' % len(embeddings_index))","0e36b468":"# we will now perform the encoding\nencoded_docs = t.texts_to_sequences(X_train.tolist())\n\n# embedding layer require all the encoded sequences to be of the same length, lets take max lenght as 100\n# and apply padding on the sequences which are of lower size\nmax_length = 100\npadded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\nprint(padded_docs)","13801ce9":"# create a weight matrix for words in training docs\nmis_spelled = []\nembedding_matrix = zeros((vocab_size, 100))\nfor word, i in t.word_index.items():\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[i] = embedding_vector\n    else:\n        mis_spelled.append(word)","cfa50d60":"# lets check how many words are not spelled correctly \nlen(mis_spelled)","dbc7cd55":"# define model\nmodel = Sequential()\ne = Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=100, trainable=False)\nmodel.add(e)\nmodel.add(Flatten())\nmodel.add(Dense(1, activation='sigmoid'))\n# compile the model\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=[get_f1])\n# summarize the model\nmodel.summary()\n# fit the model\nmodel.fit(padded_docs, y_train, epochs=50, verbose=0)","bda1f1a1":"loss, accuracy = model.evaluate(padded_docs, y_train, verbose=0)","c627c5b2":"print(accuracy)","8d42b30b":"encoded_docs = t.texts_to_sequences(X_test.tolist())\npadded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')","215a1be9":"# prediction on the test dataset\ny_pred = model.predict_classes(padded_docs)","0ccb1588":"encoded_docs = t.texts_to_sequences(test.text.tolist())\npadded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')","9ab1b1a7":"y_test_pred = model.predict_classes(padded_docs)","bf701060":"# lets prepare for the prediction submission\nsub = pd.DataFrame()\nsub['Id'] = test_id\nsub['target'] = y_test_pred\nsub.to_csv('submission_2.csv',index=False)","ef7f264d":"# fit a tokenizer\n#we have already fit a tokenizer on our data, pasting the same code again\n#t = Tokenizer()\n#t.fit_on_texts(X_train.tolist())","53446bb5":"max_length = max([len(s) for s in train.text])\nprint('Maximum length: %d' % max_length)","d1b77743":"# we will now perform the encoding\nencoded_docs = t.texts_to_sequences(X_train.tolist())\n\n# embedding layer require all the encoded sequences to be of the same length, lets take max lenght as 100\n# and apply padding on the sequences which are of lower size\n\npadded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\nprint(padded_docs)","8016fda4":"# define vocabulary size\n#  we laready have vocab size\nvocab_size","c6aace06":"# define the model\ndef define_model(vocab_size, max_length):\n    model = Sequential()\n    model.add(Embedding(vocab_size, 100, input_length=max_length))\n    model.add(Conv1D(filters=32, kernel_size=8, activation='relu'))\n    model.add(MaxPooling1D(pool_size=2))\n    model.add(Flatten())\n    model.add(Dense(10, activation='relu'))\n    model.add(Dense(1, activation='sigmoid'))\n    # compile network\n    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n    # summarize defined model\n    model.summary()\n    plot_model(model, to_file='model.png', show_shapes=True)\n    return model","f65ebaf9":"# define model\nmodel = define_model(vocab_size, max_length)\n# fit network\nmodel.fit(padded_docs, y_train, epochs=10, verbose=2)","41188124":"loss, accuracy = model.evaluate(padded_docs, y_train, verbose=0)","a915915e":"print(accuracy)","4e443a8c":"encoded_docs = t.texts_to_sequences(X_test.tolist())\npadded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')","c495ba42":"# prediction on the test dataset\ny_pred = model.predict_classes(padded_docs)","56cbc8c4":"encoded_docs = t.texts_to_sequences(test.text.tolist())\npadded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')","8dadc910":"y_test_pred = model.predict_classes(padded_docs)","b45f33bc":"# lets prepare for the prediction submission\nsub = pd.DataFrame()\nsub['Id'] = test_id\nsub['target'] = y_test_pred\nsub.to_csv('submission_cnn.csv',index=False)","ea1c1b39":"# save the model\nmodel.save('model.h5')","892e6879":"from keras.layers import Input\nfrom keras.layers.merge import concatenate\nfrom keras.models import Model\n# define the model \ndef define_model(length, vocab_size): \n    # channel 1 \n    inputs1 = Input(shape=(length,)) \n    embedding1 = Embedding(vocab_size, 100)(inputs1) \n    conv1 = Conv1D(32, 4, activation='relu')(embedding1) \n    drop1 = Dropout(0.5)(conv1) \n    pool1 = MaxPooling1D()(drop1) \n    flat1 = Flatten()(pool1)\n    \n    # channel 2 \n    inputs2 = Input(shape=(length,)) \n    embedding2 = Embedding(vocab_size, 100)(inputs2) \n    conv2 = Conv1D(32, 6, activation='relu')(embedding2) \n    drop2 = Dropout(0.5)(conv2) \n    pool2 = MaxPooling1D()(drop2) \n    flat2 = Flatten()(pool2) \n    \n    # channel 3 \n    inputs3 = Input(shape=(length,)) \n    embedding3 = Embedding(vocab_size, 100)(inputs3) \n    conv3 = Conv1D(32, 8, activation='relu')(embedding3) \n    drop3 = Dropout(0.5)(conv3) \n    pool3 = MaxPooling1D()(drop3) \n    flat3 = Flatten()(pool3)\n    \n    # merge \n    merged = concatenate([flat1, flat2, flat3]) \n    # interpretation \n    dense1 = Dense(10, activation='relu')(merged) \n    outputs = Dense(1, activation='sigmoid')(dense1) \n    model = Model(inputs=[inputs1, inputs2, inputs3], outputs=outputs) \n    # compile \n    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy']) \n    # summarize \n    model.summary() \n    plot_model(model, show_shapes=True, to_file='multichannel.png') \n    return model\n","311e7eb0":"# we will now perform the encoding\nencoded_docs = t.texts_to_sequences(X_train.tolist())\n\n# embedding layer require all the encoded sequences to be of the same length, lets take max lenght as 100\n# and apply padding on the sequences which are of lower size\n\npadded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n","14a1d163":"# define model\nmodel = define_model(max_length,vocab_size)\n# fit network\nmodel.fit([padded_docs,padded_docs,padded_docs], array(y_train), epochs=7, batch_size=16)","b44b93f3":"loss, accuracy = model.evaluate([padded_docs,padded_docs,padded_docs], y_train, verbose=0)","d0bfb143":"print(accuracy)","c18b292a":"encoded_docs = t.texts_to_sequences(X_test.tolist())\npadded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')","335e2d45":"# prediction on the test dataset\n_, acc = model.evaluate([padded_docs,padded_docs,padded_docs], array(y_test), verbose=0) \nprint('Train Accuracy: %.2f' % (acc*100)) ","af4efdd5":"encoded_docs = t.texts_to_sequences(test.text.tolist())\npadded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')","07548e08":"y_test_pred = model.predict([padded_docs,padded_docs,padded_docs])","17a31736":"# lets prepare for the prediction submission\nsub = pd.DataFrame()\nsub['Id'] = test_id\nsub['target'] = y_test_pred\nsub.to_csv('submission_multi-cnn.csv',index=False)","c1c13078":"!wget --quiet https:\/\/raw.githubusercontent.com\/tensorflow\/models\/master\/official\/nlp\/bert\/tokenization.py","8ddc2984":"from tensorflow.keras.layers import Dense, Input\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nimport tensorflow_hub as hub\n\nimport tokenization","a341033d":"# load train and test datasets\ntrain= pd.read_csv('..\/input\/nlp-getting-started\/train.csv')\ntest=pd.read_csv('..\/input\/nlp-getting-started\/test.csv')","a8359fbb":"### Add tokens to the data make it BERT compatible\ndef bert_encode(texts, tokenizer, max_len=512):\n    all_tokens = []\n    all_masks = []\n    all_segments = []\n    \n    for text in texts:\n        text = tokenizer.tokenize(text)\n            \n        text = text[:max_len-2]\n        input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\n        pad_len = max_len - len(input_sequence)\n        \n        tokens = tokenizer.convert_tokens_to_ids(input_sequence)\n        tokens += [0] * pad_len\n        pad_masks = [1] * len(input_sequence) + [0] * pad_len\n        segment_ids = [0] * max_len\n        \n        all_tokens.append(tokens)\n        all_masks.append(pad_masks)\n        all_segments.append(segment_ids)\n    \n    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)","e4bb5108":"def build_model(bert_layer, max_len=512):\n    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n    input_mask = Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n    segment_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"segment_ids\")\n\n    _, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n    clf_output = sequence_output[:, 0, :]\n    out = Dense(1, activation='sigmoid')(clf_output)\n    \n    model = Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=out)\n    model.compile(Adam(lr=2e-6), loss='binary_crossentropy', metrics=['accuracy'])\n    \n    return model","e6bd230e":"%%time\nmodule_url = \"https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_L-24_H-1024_A-16\/1\"\nbert_layer = hub.KerasLayer(module_url, trainable=True)","8a6e4232":"vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\ndo_lower_case = bert_layer.resolved_object.do_lower_case.numpy()","f218380e":"tokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)\n","64c55ceb":"train_input = bert_encode(train.text.values, tokenizer, max_len=160)\ntest_input = bert_encode(test.text.values, tokenizer, max_len=160)\ntrain_labels = train.target.values","c1b6a4af":"model = build_model(bert_layer, max_len=160)\nmodel.summary()","188b723c":"train_history = model.fit(\n    train_input, train_labels,\n    validation_split=0.2,\n    epochs=3,\n    batch_size=16\n)","9cab232f":"test_pred = model.predict(test_input)","ef9996bd":"submission=pd.DataFrame()\nsubmission['Id']=test_id\nsubmission['target'] = test_pred.round().astype(int)\nsubmission.to_csv('submission_3.csv', index=False)\n","368dafc7":"# Few key points\n\n* BERT is a deep learning model that has given state-of-the-art results on a wide variety of natural language processing tasks. It stands for Bidirectional Encoder Representations for Transformers. It has been pre-trained on Wikipedia and BooksCorpus and requires (only) task-specific fine-tuning.\n  \n* BERT is basically a bunch of Transformer encoders stacked together (not the whole Transformer architecture but just the encoder). The concept of bidirectionality is the key differentiator between BERT and its predecessor, OpenAI GPT. BERT is bidirectional because its self-attention layer performs self-attention on both directions.\n \n* BERT is pre-trained on a large corpus of unlabelled text including the entire Wikipedia(that\u2019s 2,500 million words!) and Book Corpus (800 million words). This pretraining step is really important for BERT's success. This is because as we train a model on a large text corpus, our model starts to pick up the deeper and intimate understandings of how the language works.\n\n* BERT is a deeply bidirectional model. Bidirectional means that BERT learns information from both the left and the right side of a token\u2019s context during the training phase.This bidirectional understanding is crucial to take NLP models to the next level.\n\n* Finally the biggest advantage of BERT is it brought about the ImageNet movement with it and the most impressive aspect of BERT is that we can fine-tune it by adding just a couple of additional output layers to create state-of-the-art models for a variety of NLP tasks.\n","66e0a7ea":"**We have a balanced dataset, which is good**","6934a015":"## Preprocessing Text for BERT\n\nThe input representation used by BERT is able to represent a single text sentence as well as a pair of sentences (eg., Question, Answering) in a single sequence of tokens.\n\n* The first token of every input sequence is the special classification token \u2013 [CLS]. This token is used in classification tasks as an aggregate of the entire sequence representation. It is ignored in non-classification tasks.\n* For single text sentence tasks, this [CLS] token is followed by the WordPiece tokens and the separator token \u2013 [SEP].\n\n![image.png](attachment:image.png)\n\n\n* BERT developers have set a a specific set of rules to represent languages before feeding into the model.\n\n\nFor starters, every input embedding is a combination of 3 embeddings:\n\n![image.png](attachment:image.png)\n\n*     Position Embeddings: BERT learns and uses positional embeddings to express the position of words in a sentence. These are added to overcome the limitation of Transformer which, unlike an RNN, is not able to capture \u201csequence\u201d or \u201corder\u201d information\n\n*     Segment Embeddings: BERT can also take sentence pairs as inputs for tasks (Question-Answering). That\u2019s why it learns a unique embedding for the first and the second sentences to help the model distinguish between them. In the above example, all the tokens marked as EA belong to sentence A (and similarly for EB)\n \n*     Token Embeddings: These are the embeddings learned for the specific token from the WordPiece token vocabulary\nFor a given token, its input representation is constructed by summing the corresponding token, segment, and position embeddings.\n\n\n* Tokenization: BERT uses WordPiece tokenization. The vocabulary is initialized with all the individual characters in the language, and then the most frequent\/likely combinations of the existing words in the vocabulary are iteratively added.","2cf61aaf":"# Code Implementation in Tensorflow 2.0","d7238287":"We have cleaned the datasets and have also created vocab, now lets perform n-gram analysis","90bbf943":"**Average length is also almost same for both the cases, so, we can conclude that length of the tweets has nothing to do with its real\/fake categorization**","b4cedaa9":"A standard model for document classi\ufb01cation is to use an Embedding layer as input, followed by a one-dimensional convolutional neural network, pooling layer, and then a prediction output layer. The kernel size in the convolutional layer de\ufb01nes the number of words to consider as  the convolution is passed across the input text document, providing a grouping parameter. \n\nA multi-channel convolutional neural network for document classi\ufb01cation involves using multiple versions of the standard model with di\ufb00erent sized kernels. This allows the document to be processed at di\ufb00erent resolutions or di\ufb00erent n-grams (groups of words) at a time, whilst the model learns how to best integrate these interpretations. \n\nWe will de\ufb01ne a model with three input channels for processing 4-grams, 6-grams, and 8-grams of movie review text. Each channel is comprised of the following elements:\n*  Input layer that de\ufb01nes the length of input sequences.\n*  Embedding layer set to the size of the vocabulary and 100-dimensional real-valued representations.\n*  Conv1D layer with 32 \ufb01lters and a kernel size set to the number of words to read at once.\n*  MaxPooling1D layer to consolidate the output from the convolutional layer.\n*  Flatten layer to reduce the three-dimensional output to two dimensional for concatenation.\n\nThe output from the three channels are concatenated into a single vector and process by a Dense layer and an output layer. The function below de\ufb01nes and returns the model. As part of de\ufb01ning the model, a summary of the de\ufb01ned model is printed and a plot of the model graph is created and saved to \ufb01le.\n","857cf52f":"Let's write a small function to draw graphs for stopwords and punctuations present in the tweets","50ef1bc0":"# STOPWORDS & PUNCTUATIONS","50f97444":"**Above graphs show that length of real as well as fake tweets varies in the same range 0 to 150 approx**","3351a365":"vocab size reduced drastically from 16k to 6k","9aeab536":"### Bigrams","f8c432e3":"We will use a 100-dimensional vector space.\n\nFinally, the maximum document length was calculated above in the max length variable used during padding.\n\nWe use a Convolutional Neural Network (CNN) as they have proven to be successful at document classification problems. A conservative CNN configuration is used with 32 filters (parallel fields for processing words) and a kernel size of 8 with a rectified linear (relu) activation function. This is followed by a pooling layer that reduces the output of the convolutional layer by half.\n\nNext, the 2D output from the CNN part of the model is attened to one long 2D vector to represent the features extracted by the CNN. The back-end of the model is a standard Multilayer Perceptron layers to interpret the CNN features. The output layer uses a sigmoid activation\nfunction to output a value between 0 and 1 for the negative and positive sentiment in the review.\n\nNext, we fit the network on the training data. We use a binary cross entropy loss function because the problem we are learning is a binary classification problem. The eficient Adam implementation of stochastic gradient descent is used and we keep track of accuracy in addition\nto loss during training. The model is trained for 10 epochs, or 10 passes through the training data. The network configuration and training schedule were found with a little trial and error,but are by no means optimal for this problem.","826479cc":"Now lets apply this vocab on our train and test datasets, we will keep only those words in training and testing datasets which appear in the vocabulary","c27be70f":"I will create an Artificial Neural Network, this competition is evaluated on f1 scores,which is not shown by default after every epoch, so lets create a function to  achieve the same.","c9fd3896":"# Exploratory Data Analysis","dc569088":"Now that the mapping of words to integers has been prepared, we can use it to encode the tweets in the training dataset. We can do that by calling the texts to sequences() function on the Tokenizer. We also need to ensure that all documents have the same length. This is a\nrequirement of Keras for eficient computation. \n\nWe could truncate tweets to the smallest size or zero-pad (pad with the value 0) tweets to the maximum length, or some hybrid. In this case,we will pad all tweets to the length of the longest tweet in the training dataset. First, we can find the longest review using the max() function on the training dataset and take its length.\n\nWe can then call the Keras function pad sequences() to pad the sequences to the maximum length by adding 0 values on the end.","1ade7225":"# Model Building & Evaluation","0efce39d":"Above world cloud image gives a good picture of the most common words used in the fake disaster tweets, you can clearly notice words like \"love\" \"know\" \"good\" etc. have very less probablity of appearning in the disaster tweets.","0aed980d":"lets consider only those words which have appeared more than once in the corpus\n","2e2ff934":"Above world cloud image gives a good picture of the most common words used in the real disaster tweets. words like \"news\" \"suicide\" \"bomber\" \"storm\" \"bomb\" \"kill\" are used heavily in the real disaster tweets which makes complete sense.","23edb6e0":"# Multi Channel n-gram CNN Model","6c136d49":"# Welcome to my Kernel ! ","1eab1012":"# Vocabulary creation\n","888d6adb":"Now we will develop a multi-channel convolutional neural network for the Tweet analysis prediction problem:\n\n1. Encode Data\n2. De\ufb01ne Model.\n3. Fit the model on the given data.\n4. Make predictions on test data.","503ee1c4":"### Punctuations present in the whole dataset","09ccd541":"### Trigrams","0d4ab89e":"# CNN with word Embeddings","39fc3a65":"### Stopwords present in the whole dataset","f9ce150f":"# Kindly upvote if you like my work :)","95f70c05":"# Word Embeddings\n\nWord embedding methods learn a real-valued vector representation for a predefined fixed sized vocabulary from a corpus of text. The learning process is either joint with the neural network model on some task, such as document classification, or is an unsupervised process, using document statistics.\n\nThere are basically 3 methods that can be used to learn a word embedding from text data. Methods are as follows:\n\n**1. Embedding layer**  \nAn embedding layer, is a word embedding that is learned jointly with a neural network model on a specific natural language processing task,It requires that document text be cleaned and prepared such that each word is one hot encoded. The size of the vector space is specified as part of the model, such as 50, 100, or 300 dimensions. The vectors are initialized with small random numbers. The embedding layer is used on the front end of a neural network and is fit in a\nsupervised way using the Backpropagation algorithm.\n\n**2. Word2Vec**\nWord2Vec is a statistical method for eficiently learning a standalone word embedding from a text corpus. It was developed by Tomas Mikolov, et al. at Google in 2013 as a response to make the neural-network-based training of the embedding more eficient and since then has become the de facto standard for developing pre-trained word embedding.\nTwo different learning models were introduced that can be used as part of the Word2Vec\napproach to learn the word embedding; they are:\n\n    2.1 Continuous Bag-of-Words, or CBOW mode\n          The CBOW model learns the embedding by predicting the current word based on its context.\n          \n    2.2 Continuous Skip-Gram Model\n          The continuous skip-gram model learns by predicting the surrounding words given a current word.\n\n**3. GLOVE**\nThe Global Vectors for Word Representation, or GloVe, algorithm is an extension to the Word2Vec method for efficiently learning word vectors, developed by Pennington, et al. at Stanford. Classical vector space model representations of words were developed using matrix factorization techniques such as Latent Semantic Analysis (LSA) that do a good job of using global text statistics but are not as good as the learned methods like Word2Vec at capturing meaning and demonstrating it on tasks like calculating analogies.\n","2096edba":"# N-gram Analysis\n\nn-gram is basically set of occurring words within given window so when\n\nn=1 it is Unigram\n\nn=2 it is bigram\n\nn=3 it is trigram and so on\n\nNow suppose machine try to understand the meaning of sentence \"I have a lovely dog\" than it will split sentences into specific chunk.\n\nIt will consider word one by one which is unigram so each word will be a gram.\n\n\"I\", \"have\", \"a\" , \"lovely\" , \"dog\"\n\nI will consider two word at a time so it will be biagram so each two djacent words will be biagram\n\n\"I have\" , \"have a\" , \"a lovely\" , \"lovely dog\"\n\nSo like this machine will split sentences into small group of words to understand its meaning.","66ce986f":"# Introduction\n\nThis particular challenge is perfect for data scientists looking to get started with Natural Language Processing.\n\nCompetition Description\nTwitter has become an important communication channel in times of emergency.\nThe ubiquitousness of smartphones enables people to announce an emergency they\u2019re observing in real-time. Because of this, more agencies are interested in programatically monitoring Twitter (i.e. disaster relief organizations and news agencies).\n\nBut, it\u2019s not always clear whether a person\u2019s words are actually announcing a disaster. \n\nTake an example:\nThe author explicitly uses the word \u201cABLAZE\u201d but means it metaphorically. This is clear to a human right away, especially with the visual aid. But it\u2019s less clear to a machine.\n\nIn this competition, you\u2019re challenged to build a machine learning model that predicts which Tweets are about real disasters and which one\u2019s aren\u2019t. You\u2019ll have access to a dataset of 10,000 tweets that were hand classified.","f9e5c9a6":"1. \"stop words\" usually refers to the most common words in a language. for example 'a', 'the' etc. These words are essential parts of any language but do not add anything significant to the meaning of a word.\n\n2. punctuation marks are marks such as a full stop, comma, or question mark, used in writing to separate sentences and their elements and to clarify meaning.","ff0f558c":"## Architecture of BERT\n\nBERT is a multi-layer bidirectional Transformer encoder. There are two models introduced in the paper.\n\nBERT base \u2013 12 layers (transformer blocks), 12 attention heads, and 110 million parameters.\nBERT Large \u2013 24 layers, 16 attention heads and, 340 million parameters.\n\nHere's a representation of BERT Architecture\n\n![image.png](attachment:image.png)","2b07ee92":"Lets create our own vocabulary","ad07c5f5":"In order to get accurate results from the predictive model, we need to remove these stop words & punctuations.\n\nApart from removing these stopwords & puncuations, we would also convert all the messages in lowercase so that words like \"Go\" & \"go\" can be treated as same word and not different words.\n\nWe will also convert the words to its lemma form (for example, lemma of word \"running\" would be run), converting words to their lemmas would also help improving the predictive power of our model.\n\nWe would also remove embedded special characters from the tweets, for example, #earthquake should be replaced by earthquake\n\nWe also need to remove the \"URLs\" from the tweets\n\nAnd then finally we remove the digits from the tweets\n\nLets write a small function \"preprocess\" to achive all these tasks.","201eef9e":"# Lets plot word cloud for real and fake tweets","fb1ec448":"# Re-loading input files, BERTS seems to do better on raw data","65330132":"# BERT - The Concept\nMost of the theoritical contents in this notebook are taken from this amazing notebook: https:\/\/www.kaggle.com\/abhinand05\/bert-for-humans-tutorial-baseline-version-2","4e1caa88":"**About the Kernel**\n\n1. Intial versions (Ver 1 to 9) -  Basic EDA and Vanila ANN\n2. Versions 9 - Word Embeddings with GLOVE \n3. Version 10 to 15 - BERT (my best model so far)\n4. Version 16 to 19 - CNN model\n5. Version 20 onwards - n-gram CNN model","c6ec3f63":"# GLOVE with KERAS Word Embeddings","cbec68a1":"We can see a lots of null values for \"keyword\" and \"location\" columns","51a92a9f":"**Lets visualize the top 10 stopwords and punctuations present in real and fake tweets** ","c21dcf36":"### Stopwords & Punctuations present in real vs fake tweets","f993dfa2":"### Unigrams","c500e72e":"# Data Cleaning","d1463d21":"We are now ready to define our neural network model. The model will use an Embedding layer as the first hidden layer. The Embedding layer requires the specification of the vocabulary size, the size of the real-valued vector space, and the maximum length of input documents. \n\nThe vocabulary size is the total number of words in our vocabulary, plus one for unknown words. This could be the vocab set length or the size of the vocab within the tokenizer used to integer encode the documents, for example:","7a79d796":"Nothing intuitive from the above graphs, no strong relation with real\/fake disaster tweets","2ae6ce8a":"I have written few functions to perform EDA.","3df286cc":"We can then use the maximum length as a parameter to a function to integer encode and pad the sequences."}}