{"cell_type":{"56b9d2a3":"code","90ad73c0":"code","a69ea7a1":"code","dc4e0e1b":"code","0cddd12b":"code","98cdb96f":"code","6922c764":"code","b2f0b378":"code","2c283148":"code","7fc066d5":"code","6755089a":"markdown","6ad75f88":"markdown","45d08a43":"markdown","f7a6cf5d":"markdown","c25c048f":"markdown","b6ea7f73":"markdown"},"source":{"56b9d2a3":"import os\nimport collections\nfrom datetime import datetime, timedelta\n\nos.environ[\"XRT_TPU_CONFIG\"] = \"tpu_worker;0;10.0.0.2:8470\"\n\n_VersionConfig = collections.namedtuple('_VersionConfig', 'wheels,server')\nVERSION = \"torch_xla==nightly\"\nCONFIG = {\n    'torch_xla==nightly': _VersionConfig('nightly', 'XRT-dev{}'.format(\n        (datetime.today() - timedelta(1)).strftime('%Y%m%d')))}[VERSION]\n\nDIST_BUCKET = 'gs:\/\/tpu-pytorch\/wheels'\nTORCH_WHEEL = 'torch-{}-cp36-cp36m-linux_x86_64.whl'.format(CONFIG.wheels)\nTORCH_XLA_WHEEL = 'torch_xla-{}-cp36-cp36m-linux_x86_64.whl'.format(CONFIG.wheels)\nTORCHVISION_WHEEL = 'torchvision-{}-cp36-cp36m-linux_x86_64.whl'.format(CONFIG.wheels)\n\n!export LD_LIBRARY_PATH=\/usr\/local\/lib:$LD_LIBRARY_PATH\n!apt-get install libomp5 -y\n!apt-get install libopenblas-dev -y\n\n!pip uninstall -y torch torchvision\n!gsutil cp \"$DIST_BUCKET\/$TORCH_WHEEL\" .\n!gsutil cp \"$DIST_BUCKET\/$TORCH_XLA_WHEEL\" .\n!gsutil cp \"$DIST_BUCKET\/$TORCHVISION_WHEEL\" .\n!pip install \"$TORCH_WHEEL\"\n!pip install \"$TORCH_XLA_WHEEL\"\n!pip install \"$TORCHVISION_WHEEL\"","90ad73c0":"import os\nimport re\nimport cv2\nimport time\nimport tensorflow\nimport collections\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nfrom glob import glob\nfrom PIL import Image\nimport requests, threading\nimport matplotlib.pyplot as plt\nfrom datetime import datetime, timedelta\n\nimport torch\nimport torchvision\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torchvision import datasets\nfrom torchvision import transforms\nfrom torch.autograd import Variable\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.optim.lr_scheduler import OneCycleLR\n\nimport torch_xla\nimport torch_xla.utils.utils as xu\nimport torch_xla.core.xla_model as xm\nimport torch_xla.debug.metrics as met\nimport torch_xla.distributed.data_parallel as dp\nimport torch_xla.distributed.parallel_loader as pl\nimport torch_xla.distributed.xla_multiprocessing as xmp\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\ntorch.manual_seed(42)\ntorch.set_default_tensor_type('torch.FloatTensor')","a69ea7a1":"# do not uncomment see https:\/\/github.com\/pytorch\/xla\/issues\/1587\n\n# xm.get_xla_supported_devices()\n# xm.xrt_world_size() # 1","dc4e0e1b":"DATASET_DIR = '\/kaggle\/input\/104-flowers-garden-of-eden\/jpeg-512x512'\nTRAIN_DIR  = DATASET_DIR + '\/train'\nVAL_DIR  = DATASET_DIR + '\/val'\nTEST_DIR  = DATASET_DIR + '\/test'\nBATCH_SIZE = 128 # per core \nNUM_EPOCH = 25","0cddd12b":"normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])\ntrain_transform = transforms.Compose([transforms.RandomResizedCrop(224),\n                                      transforms.RandomHorizontalFlip(0.5),\n                                      transforms.ToTensor(),\n                                      normalize])\n\nvalid_transform = transforms.Compose([transforms.Resize((224,224)),\n                                      transforms.ToTensor(),\n                                      normalize])","98cdb96f":"train = datasets.ImageFolder(TRAIN_DIR, transform=train_transform)\nvalid = datasets.ImageFolder(VAL_DIR, transform=train_transform)\ntrain = torch.utils.data.ConcatDataset([train, valid])\n\n# print out some data stats\nprint('Num training images: ', len(train))\nprint('Num test images: ', len(valid))","6922c764":"class MyModel(nn.Module):\n\n    def __init__(self):\n        super(MyModel, self).__init__()\n        \n        self.base_model = torchvision.models.densenet201(pretrained=True)\n        self.base_model.classifier = nn.Identity()\n        self.fc = torch.nn.Sequential(\n                    torch.nn.Linear(1920, 1024, bias = True),\n                    torch.nn.BatchNorm1d(1024),\n                    torch.nn.ReLU(inplace=True),\n                    torch.nn.Dropout(0.3),\n                    torch.nn.Linear(1024, 512, bias = True),\n                    torch.nn.BatchNorm1d(512),\n                    torch.nn.ReLU(inplace=True),\n                    torch.nn.Dropout(0.3),\n                    torch.nn.Linear(512, 104))\n        \n    def forward(self, inputs):\n        x = self.base_model(inputs)\n        return self.fc(x)","b2f0b378":"model = MyModel()\nprint(model)\ndel model","2c283148":"def train_model():\n    train = datasets.ImageFolder(TRAIN_DIR, transform=train_transform)\n    valid = datasets.ImageFolder(VAL_DIR, transform=train_transform)\n    train = torch.utils.data.ConcatDataset([train, valid])\n    \n    torch.manual_seed(42)\n    \n    train_sampler = torch.utils.data.distributed.DistributedSampler(\n        train,\n        num_replicas=xm.xrt_world_size(),\n        rank=xm.get_ordinal(),\n        shuffle=True)\n    \n    train_loader = torch.utils.data.DataLoader(\n        train,\n        batch_size=BATCH_SIZE,\n        sampler=train_sampler,\n        num_workers=0,\n        drop_last=True) # print(len(train_loader))\n    \n    \n    xm.master_print(f\"Train for {len(train_loader)} steps per epoch\")\n    \n    # Scale learning rate to num cores\n    learning_rate = 0.0001 * xm.xrt_world_size()\n\n    # Get loss function, optimizer, and model\n    device = xm.xla_device()\n\n    model = MyModel()\n    \n    for param in model.base_model.parameters(): # freeze some layers\n        param.requires_grad = False\n    \n    model = model.to(device)\n    loss_fn =  nn.CrossEntropyLoss()\n    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=5e-4)\n    scheduler = OneCycleLR(optimizer, \n                           learning_rate, \n                           div_factor=10.0, \n                           final_div_factor=50.0, \n                           epochs=NUM_EPOCH,\n                           steps_per_epoch=len(train_loader))\n    \n    \n    \n    def train_loop_fn(loader):\n        tracker = xm.RateTracker()\n        model.train()\n        total_samples, correct = 0, 0\n        for x, (data, target) in enumerate(loader):\n            optimizer.zero_grad()\n            output = model(data)\n            loss = loss_fn(output, target)\n            loss.backward()\n            xm.optimizer_step(optimizer)\n            tracker.add(data.shape[0])\n            pred = output.max(1, keepdim=True)[1]\n            correct += pred.eq(target.view_as(pred)).sum().item()\n            total_samples += data.size()[0]\n            scheduler.step()\n            if x % 10 == 0:\n                print('[xla:{}]({})\\tLoss={:.3f}\\tRate={:.2f}\\tGlobalRate={:.2f}'.format(\n                    xm.get_ordinal(), x, loss.item(), tracker.rate(),\n                    tracker.global_rate()), flush=True)\n        accuracy = 100.0 * correct \/ total_samples\n        print('[xla:{}] Accuracy={:.2f}%'.format(xm.get_ordinal(), accuracy), flush=True)\n        return accuracy\n\n    # Train loops\n    accuracy = []\n    for epoch in range(1, NUM_EPOCH + 1):\n        start = time.time()\n        para_loader = pl.ParallelLoader(train_loader, [device])\n        accuracy.append(train_loop_fn(para_loader.per_device_loader(device)))\n        xm.master_print(\"Finished training epoch {} train-acc {:.2f} in {:.2f} sec\"\\\n                        .format(epoch, accuracy[-1], time.time() - start))\n        xm.save(model.state_dict(), \".\/model.pt\")\n\n#         if epoch == 15: #unfreeze\n#             for param in model.base_model.parameters():\n#                 param.requires_grad = True\n\n    return accuracy","7fc066d5":"# Start training processes\ndef _mp_fn(rank, flags):\n    global acc_list\n    torch.set_default_tensor_type('torch.FloatTensor')\n    a = train_model()\n\nFLAGS={}\n_mp_fn(0, FLAGS)\n#xmp.spawn(_mp_fn, args=(FLAGS,), nprocs=8, start_method='fork')","6755089a":"## Training","6ad75f88":"## setup for pytorch\/xla on TPU","45d08a43":"## Dataset","f7a6cf5d":"## Model","c25c048f":"### Using only one core of the TPU","b6ea7f73":"## Imports"}}