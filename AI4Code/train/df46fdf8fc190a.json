{"cell_type":{"1c422113":"code","3715b699":"code","105c2771":"code","cf4e3da4":"code","ac291d58":"code","f7c06c98":"code","1f2130d1":"code","e234c70b":"code","01067c39":"code","d0880c6f":"code","4c4309e5":"code","bec977b5":"code","7e157c2b":"code","b245cf49":"code","f9f4cb53":"code","890acfb2":"code","924271d0":"code","46a8c74b":"code","3ea4d7a1":"code","79ab53f9":"code","ca397d59":"code","c80da1b7":"code","15b57c43":"code","21efc23f":"code","8142669c":"code","58353a8e":"code","78d0dd8a":"code","d3ff5f0a":"code","ef5e8d74":"code","1169ea4e":"code","e6fbca8e":"code","bb9bf2e2":"code","f606e598":"code","8d5ad150":"code","8d2011aa":"code","29304407":"code","7d0d503a":"code","8246361e":"code","42540f59":"code","31082371":"code","a39e8749":"code","48f57dbd":"code","aec8be50":"code","9c50899b":"code","e2ec7d41":"code","4e4c40de":"code","00eea3cd":"code","48d0ff06":"code","ee530cd9":"code","edd08743":"code","9d2853d2":"code","7700526f":"code","45fd2401":"code","0431860b":"code","6de6787d":"code","f26080ab":"code","84ec340d":"code","8a3b3fa6":"code","8666ef13":"code","18011fa2":"code","8b957dbe":"code","a3d184bd":"code","e835d9af":"code","8812c88a":"code","d7a480e6":"code","deb7cc97":"code","7dfdb6bb":"code","f01fa632":"code","86c9f6b8":"code","4e12aa51":"code","62225987":"code","696a19de":"code","56fded88":"code","336bdbb0":"code","81532a40":"code","fab05f92":"code","03c6252c":"code","deeeef76":"code","e99d87a3":"code","70edc2e6":"code","9973358f":"code","3af8e4b9":"code","450d37f8":"code","5e02136a":"code","7e57d69c":"code","01002ab2":"code","8eabb3f4":"code","57be7108":"code","78456fe0":"code","68eae6d8":"code","49d00eb2":"code","ce29b849":"code","c7a7fc4c":"code","1f39f73e":"code","5212437a":"code","703e0e90":"code","d3f852bd":"code","38005948":"code","e0571ee4":"code","ceb06c6d":"markdown","de0e2208":"markdown","279ea385":"markdown","fbdb0772":"markdown","0ced030b":"markdown","87740b40":"markdown","85caaef5":"markdown","b1879c13":"markdown","fc80649f":"markdown","7df70b93":"markdown","aa57a3ad":"markdown","97631d26":"markdown","c1204b53":"markdown","88c9073f":"markdown","f9ded303":"markdown","de2c37e4":"markdown","f6dc3420":"markdown","edf91042":"markdown","93764129":"markdown","93a1dd02":"markdown","3ebb412a":"markdown","74ed56cf":"markdown","efb447ec":"markdown","f7dc6701":"markdown","e9a49341":"markdown","3ad5aefa":"markdown","aca20e75":"markdown","9fd8e99f":"markdown","486e0fdd":"markdown","0d81327b":"markdown","1c7ec055":"markdown","d9677bf8":"markdown","08f7478c":"markdown","48bc1d06":"markdown","1fc35b3b":"markdown","1a1eb6e1":"markdown","9c5a2818":"markdown","48321a28":"markdown","79a52470":"markdown","e74ffed7":"markdown","d85f4eef":"markdown","73b9966f":"markdown","b4c7bf7e":"markdown","f557741b":"markdown","d90f1129":"markdown","6685283d":"markdown","5e3c512d":"markdown","d36df5c8":"markdown","084dfa60":"markdown","e1c9cd51":"markdown","8031330d":"markdown","d6db8aa1":"markdown","ceb7acee":"markdown","3a61f4bb":"markdown","58049387":"markdown","158fc9bd":"markdown","2d472431":"markdown","4f6819a1":"markdown","7b3053eb":"markdown","0e3c48b7":"markdown","5e8b02da":"markdown","005fc277":"markdown","acaadbdb":"markdown","3c9e0b6c":"markdown","69fd3663":"markdown","646c61f2":"markdown","64bf1b92":"markdown","864a8b7a":"markdown","798d6ac6":"markdown","20df9caa":"markdown","eae9f02c":"markdown","0e4a8bbd":"markdown","c91b6de4":"markdown","e9259fec":"markdown","a2179ad0":"markdown","efb59ca0":"markdown","04cb23c2":"markdown"},"source":{"1c422113":"# run these instructions only one time\n!pip install git+https:\/\/github.com\/fastai\/fastai@2e1ccb58121dc648751e2109fc0fbf6925aa8887\n!apt update && apt install -y libsm6 libxext6","3715b699":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport os\npath = '..\/input\/'\nprint(os.listdir(path))","105c2771":"train_data= pd.read_csv(path+'train.csv')\nprint(train_data.columns)\ntrain_data.describe()","cf4e3da4":"train_data.info()","ac291d58":"print(\"number of row = \",train_data.shape[0])","f7c06c98":"train_data['Died'] = 1-train_data['Survived']\ntrain_data.groupby('Sex').agg('sum')[['Survived','Died']].plot(\n    kind='bar', stacked=True, figsize=(10,6));","1f2130d1":"fig, axis = plt.subplots(3,2,figsize=(20,15))\nsns.barplot(x=\"Survived\", y=\"Fare\", ax=axis[0,0], data=train_data)\nsns.boxplot(x=\"Survived\", y=\"Fare\", ax=axis[0,1], data=train_data)\nsns.boxplot(train_data['Fare'], ax=axis[1][0])\nsns.barplot(train_data['Fare'], ax=axis[(1,1)])\nsns.kdeplot(train_data.Fare, ax=axis[(2,0)])\nsns.distplot(train_data.Fare, ax=axis[(2,1)]);","e234c70b":"train_data.head()","01067c39":"fig, axis = plt.subplots(1,2,figsize=(15,8))\nsns.barplot(x=\"Embarked\", y=\"Survived\", hue=\"Sex\", ax=axis[(0)], data=train_data);\nsns.barplot(x=\"Pclass\", y=\"Survived\", hue=\"Sex\", ax=axis[(1)], data=train_data);\nplt.figure(figsize=(10,5))\nsns.barplot(x=\"Parch\", y=\"Survived\", hue=\"Sex\", data=train_data);","d0880c6f":"plt.figure(figsize=(10,6))\nsns.violinplot(x='Sex',y='Age',hue='Survived', data=train_data, split=True);","4c4309e5":"plt.figure(figsize=(15,10))\nplt.hist([train_data[train_data['Survived'] == 1]['Fare'], train_data[train_data['Died'] == 1]['Fare']],\n        stacked=True, color=['g','r'], bins=70, label = ['Survived','Died'])\nplt.xlabel('Fare')\nplt.ylabel('Number of passnegers')\nplt.legend()\nplt.grid()","bec977b5":"plt.figure(figsize=(25,10))\nax=plt.subplot()\nax.scatter(train_data['Age'], train_data['Fare'], s=100)\nplt.xlabel('Age')\nplt.ylabel('Fare');","7e157c2b":"plt.figure(figsize=(25,10))\nax=plt.subplot()\n\nax.scatter(train_data[train_data['Survived'] == 1]['Age'], train_data[train_data['Survived'] == 1]['Fare'],\n          c='green', s=train_data[train_data['Survived'] == 1]['Fare'])\nax.scatter(train_data[train_data['Died'] == 1]['Age'], train_data[train_data['Died'] == 1]['Fare'],\n          c='red', s=train_data[train_data['Died'] == 1]['Fare']);\nplt.xlabel('Age')\nplt.ylabel('Fare');","b245cf49":"plt.figure(figsize=(25,15))\nsns.boxplot(x=train_data.Pclass, y=train_data.Fare);","f9f4cb53":"sns.kdeplot(train_data[train_data.Pclass==1]['Fare']);","890acfb2":"ax=plt.subplot()\nax.set_ylabel('Average fare')\ntrain_data.groupby('Pclass').mean()['Fare'].plot(kind='bar', ax=ax, figsize=(10,6) );\n#the line above is the same as :\n#train_data.groupby('Pclass').agg({'Fare':'mean'}).plot(kind='bar', ax=ax)","924271d0":"X_train = train_data.drop(['Survived','Died'],axis=1)\ny_train = train_data['Survived']\nX_test = pd.read_csv(path+'\/test.csv')\nfull_data = pd.concat([X_train,X_test], ignore_index=True)\n# otherwise : full_data=X_train.append(X_test)\nprint(X_train.shape[1])\nprint(full_data.shape[1])","46a8c74b":"def process_family(df):\n    # introducing a new feature : the size of families (including the passenger)\n    df['FamilySize'] = df['Parch'] + df['SibSp'] + 1\n    \n    # introducing other features based on the family size\n    df['Singleton'] = df['FamilySize'].map(lambda s: 1 if s == 1 else 0)\n    df['SmallFamily'] = df['FamilySize'].map(lambda s: 1 if 2 <= s <= 4 else 0)\n    df['LargeFamily'] = df['FamilySize'].map(lambda s: 1 if 5 <= s else 0)    \n    df.drop(['FamilySize', 'Parch', 'SibSp'], axis=1, inplace=True)\n    return df","3ea4d7a1":"full_data = process_family(full_data)\nfull_data.head()","79ab53f9":"print(full_data.Embarked.describe())\nprint('we have',full_data.Embarked.isna().sum(),'missing values in Emabrked column, which they are :')\nfull_data.loc[full_data.Embarked.isna()]","ca397d59":"full_data[(full_data.Pclass==1) & (full_data.Sex=='female')]['Embarked'].describe()","c80da1b7":"def process_embarked(df):\n    # two missing embarked values - filling them with the most frequent one in the train  set(S)\n    df.Embarked.fillna('C', inplace=True)\n    # dummy encoding \n    df_dummies = pd.get_dummies(df['Embarked'], prefix='Embarked')\n    df = pd.concat([df, df_dummies], axis=1)\n    df.drop('Embarked', axis=1, inplace=True)\n#     status('embarked')\n    return df","15b57c43":"full_data = process_embarked(full_data)\nfull_data.head()","21efc23f":"print('we have',full_data.Cabin.isna().sum(),'missing value in Cabin column.')\nfull_data.Cabin.describe()","8142669c":"from sklearn.preprocessing import LabelEncoder","58353a8e":"def process_cabin(df):\n    # replacing missing cabins with U (for Uknown)\n    df.Cabin.fillna('U', inplace=True)\n    \n    # mapping each Cabin value with the cabin letter\n    df['Cabin'] = df['Cabin'].apply(lambda x: x[0])\n    \n    #Label Encoding ...\n    df['Cabin']= LabelEncoder().fit_transform(df['Cabin'])\n    \n    # dummy encoding ...\n    #cabin_dummies = pd.get_dummies(df['Cabin'], prefix='Cabin')    \n    #df = pd.concat([df, cabin_dummies], axis=1)\n\n    #df.drop('Cabin', axis=1, inplace=True)\n    return df","78d0dd8a":"full_data = process_cabin(full_data)\nfull_data.head(3)","d3ff5f0a":"titles = set()\nfor name in full_data['Name']:\n    titles.add(name.split(',')[1].split('.')[0].strip())\ntitles","ef5e8d74":"def process_title (df):\n    df['Title'] = df['Name'].apply(lambda x:x.split(',')[1].split('.')[0].strip())\n    Title_Dict = {}\n    Title_Dict.update(dict.fromkeys(['Capt', 'Col', 'Major', 'Dr', 'Rev'], 'Officer'))\n    Title_Dict.update(dict.fromkeys(['Don', 'Sir', 'the Countess', 'Dona', 'Lady'], 'Royalty'))\n    Title_Dict.update(dict.fromkeys(['Mme', 'Ms', 'Mrs'], 'Mrs'))\n    Title_Dict.update(dict.fromkeys(['Mlle', 'Miss'], 'Miss'))\n    Title_Dict.update(dict.fromkeys(['Mr'], 'Mr'))\n    Title_Dict.update(dict.fromkeys(['Master','Jonkheer'], 'Master'))\n    df['Title'] = df['Title'].map(Title_Dict)\n    df.drop(['Name'], axis=1, inplace=True)\n    return df","1169ea4e":"full_data = process_title(full_data)\nfull_data.head(3)","e6fbca8e":"# Title_Dictionary = {\n#      'Capt':'Officier',\n#      'Col':'Officier',\n#      'Don':'Royalty',\n#      'Dona':'Royalty',\n#      'Dr':'Officier',\n#      'Jonkheer':'Royalty',\n#      'Lady':'Royalty',\n#      'Major':'Officier',\n#      'Master':'Master',\n#      'Miss':'Miss',\n#      'Mlle':'Miss',\n#      'Mme':'Mrs',\n#      'Mr':'Mr',\n#      'Mrs':'Mrs',\n#      'Ms':'Mrs',\n#      'Rev':'Officier',\n#      'Sir':'Royalty',\n#      'the Countess':'Royalty'   \n# }\n# def passenger_title(df):\n#     df['Title'] = df['Name'].apply(lambda x:x.split(',')[1].split('.')[0].strip())\n#     df['Title'] = df['Title'].apply( lambda x : Title_Dictionary[x])\n#     df.drop(['Name'], axis=1, inplace=True)\n#     return df","bb9bf2e2":"# full_data = passenger_title(full_data)\n# full_data.head()","f606e598":"# grouped_median_train = full_data.groupby(['Sex','Pclass','Title']).agg({'Age':'median'}).reset_index()\n# grouped_median_train.head()","8d5ad150":"#Adding the value of age for missing values based on the grouped_median_train","8d2011aa":"# def fill_age(row):\n#     condition = (\n#         (grouped_median_train['Sex'] == row['Sex']) & \n#         (grouped_median_train['Title'] == row['Title']) & \n#         (grouped_median_train['Pclass'] == row['Pclass'])\n#     ) \n#     return grouped_median_train[condition]['Age'].values[0]\n\n# def process_age(df):\n#     # a function that fills the missing values of the Age variable\n#     df['Age'] = df.apply(lambda row: fill_age(row) if np.isnan(row['Age']) else row['Age'], axis=1)\n#     return df","29304407":"# full_data = process_age(full_data)\n# full_data.head()","7d0d503a":"from sklearn.ensemble import RandomForestRegressor\nage_df = full_data[['Age', 'Pclass','Sex','Title']]\nage_df = pd.get_dummies(age_df)\nage_df.head(3)","8246361e":"known_age_df = age_df[age_df.Age.notna()]\nunknown_age_df = age_df[age_df.Age.isna()]\nprint(known_age_df.head(3))\nunknown_age_df.head(3)","42540f59":"X = known_age_df.drop(['Age'], axis=1)\ny = known_age_df['Age']","31082371":"rfr = RandomForestRegressor(random_state=0, n_estimators=100, n_jobs=-1)\nrfr.fit(X, y)","a39e8749":"predicted_ages = rfr.predict(unknown_age_df.drop(['Age'], axis=1))","48f57dbd":"full_data.loc[full_data.Age.isna(), 'Age']= predicted_ages","aec8be50":"full_data.Age.isna().sum()","9c50899b":"full_data.isna().sum()","e2ec7d41":"full_data[full_data.Fare.isna()]","4e4c40de":"full_data[(full_data.Pclass == 3) & (full_data.Sex == 'male') & (full_data.Singleton==1) & (full_data.Cabin == 8)].Fare.describe()","00eea3cd":"full_data['Fare'].fillna(full_data[(full_data.Pclass == 3) & (full_data.Sex == 'male') & (full_data.Singleton==1) & (full_data.Cabin == 8)].Fare.mean(), inplace= True)","48d0ff06":"full_data.isna().sum()","ee530cd9":"full_data['Title'].describe()","edd08743":"def encode_title(df):    \n    #dummification Title column\n    titles_dummies = pd.get_dummies(df['Title'], prefix='Title')\n    df = pd.concat([df, titles_dummies], axis=1)\n    #removing the title column since we have its dummies\n    df.drop('Title', axis=1, inplace=True)\n    \n    #lebel Encoding Title column:\n#   df['Title'] = LabelEncoder().fit_transform(df['Title'])\n    return df","9d2853d2":"full_data = encode_title(full_data)\nfull_data.head()","7700526f":"def encode_sex(df):    \n    #dummification Sex column\n    Sex_dummies = pd.get_dummies(df['Sex'])\n    df = pd.concat([df, Sex_dummies], axis=1)\n    \n    #removing the Sex column since we have its dummies\n    df.drop('Sex', axis=1, inplace=True)\n    return df","45fd2401":"full_data = encode_sex(full_data)\nfull_data.head()","0431860b":"full_data.Ticket.describe()","6de6787d":"Ticket_count = dict(full_data['Ticket'].value_counts())\nfull_data['Ticket_Count_Group'] = full_data['Ticket'].apply(lambda x : Ticket_count[x])\nfull_data.drop(['Ticket'], axis=1, inplace=True)\nfull_data.head()","f26080ab":"full_data.Ticket_Count_Group.unique().size","84ec340d":"enc = LabelEncoder()\nfull_data['Ticket_Count_Group'] = enc.fit_transform(full_data['Ticket_Count_Group'])\nfull_data.head()","8a3b3fa6":"print('the size of the train set is',train_data.shape)","8666ef13":"train_data = pd.concat([full_data[:891], train_data['Survived']], axis=1)","18011fa2":"train_data.head(10)","8b957dbe":"plt.figure(figsize=(10,8))\nsns.heatmap(train_data.corr());","a3d184bd":"plt.figure(figsize=(20,18))\ncorr_matrix = train_data.corr()\nmask = np.zeros_like(corr_matrix, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\nsns.heatmap(corr_matrix, mask=mask,cmap=cmap, vmax=1 , vmin=0, center=0.5,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5});","e835d9af":"X_train = full_data[:891]\nX_test = full_data[891:]\nprint(X_train.shape)\nprint(X_test.shape)","8812c88a":"def split_vals(df,num_sample_to_train): return df[:num_sample_to_train], df[num_sample_to_train:]\nvalid_count =60\nn_trn = len(X_train)-valid_count\nX_train1, X_valid1 = split_vals(X_train, n_trn)\ny_train1, y_valid1 = split_vals(y_train, n_trn)","d7a480e6":"X_train1.shape,y_train1.shape,X_valid1.shape,y_valid1.shape","deb7cc97":"from sklearn import metrics\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV\n\nrfc = RandomForestClassifier(n_estimators=180,\n                             min_samples_leaf=3,\n                             max_features=0.5,\n                             n_jobs=-1)\nrfc.fit(X_train1,y_train1)\nrfc.score(X_train1,y_train1)","7dfdb6bb":"y_predict=rfc.predict(X_valid1)\nmetrics.accuracy_score(y_valid1,y_predict)","f01fa632":"print(metrics.classification_report(y_valid1,y_predict))","86c9f6b8":"print(metrics.confusion_matrix(y_valid1,y_predict))","4e12aa51":"from fastai.imports import *\nfrom fastai.structured import *\nfrom pandas_summary import DataFrameSummary\nfrom sklearn.ensemble import RandomForestRegressor, RandomForestClassifier,GradientBoostingClassifier\nfrom IPython.display import display\nfrom sklearn import metrics\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import GridSearchCV\nimport seaborn as sns\nimport pylab as plot","62225987":"fi = rf_feat_importance(rfc, X_train1);\nfi","696a19de":"def plot_fi(fi): return fi.plot('cols', 'imp', 'barh', figsize=(12,7), legend=False)\nplot_fi(fi);","56fded88":"to_keep = fi[fi.imp>0.01].cols; \nprint(len(to_keep))\nto_keep","336bdbb0":"X_train = X_train[to_keep]\nX_train.head()","81532a40":"rfc = RandomForestClassifier(n_estimators=150,min_samples_leaf=3,max_features=0.5,n_jobs=-1)\nrfc.fit(X_train,y_train)\nrfc.score(X_train,y_train)","fab05f92":"X_test = X_test[to_keep]\nX_test.Fare.fillna(14, inplace=True)\noutput=rfc.predict(X_test)","03c6252c":"data_test = pd.read_csv(path+'\/test.csv')\ndf_output = pd.DataFrame()\ndf_output['PassengerId'] = data_test['PassengerId']\ndf_output['Survived'] = output\ndf_output[['PassengerId','Survived']].to_csv('submission11.csv', index=False)","deeeef76":"df_output.head(10)","e99d87a3":"X = X_train\ny = train_data['Survived']","70edc2e6":"from sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.feature_selection import SelectKBest\n\npipe=Pipeline([('select',SelectKBest(k='all')), \n               ('classify', RandomForestClassifier(random_state = 10, max_features = 'sqrt'))])\n\nparam_test = {'classify__n_estimators':list(range(20,50,2)), \n              'classify__max_depth':list(range(3,60,3))}\ngsearch = GridSearchCV(estimator = pipe, param_grid = param_test, scoring='roc_auc', cv=10)\ngsearch.fit(X,y)\nprint(gsearch.best_params_, gsearch.best_score_)","9973358f":"from sklearn.pipeline import make_pipeline\nselect = SelectKBest(k = 'all')\nclf = RandomForestClassifier(random_state = 10, warm_start = True, \n                                  n_estimators = 26,\n                                  max_depth = 6, \n                                  max_features = 'sqrt')\npipeline = make_pipeline(select, clf)\npipeline.fit(X, y)","3af8e4b9":"from sklearn import metrics\nfrom sklearn.model_selection import cross_val_score\ncv_score = cross_val_score(pipeline, X, y, cv= 10)\nprint(\"CV Score : Mean - %.7g | Std - %.7g \" % (np.mean(cv_score), np.std(cv_score)))","450d37f8":"predictions = pipeline.predict(X_test)\nsubmission = pd.DataFrame({\"PassengerId\": pd.read_csv(path+'test.csv')['PassengerId'], \"Survived\": predictions.astype(np.int32)})\nsubmission.to_csv(\"submission11.csv\", index=False)\nsubmission.info()","5e02136a":"X_train  = full_data[:891]\ny_train = train_data['Survived']\nX_test = full_data[891:]","7e57d69c":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import make_scorer, accuracy_score\nfrom sklearn.model_selection import GridSearchCV\n\nclf = RandomForestClassifier()\nparameters = {'n_estimators': [4, 6, 10, 12], \n              'max_features': ['log2', 'sqrt','auto'], \n              'criterion': ['entropy', 'gini'],\n              'max_depth': [2, 3, 5, 7, 10], \n              'min_samples_split': [2, 3, 5],\n              'min_samples_leaf': [1,5,8]\n             }\n\nacc_scorer = make_scorer(accuracy_score)\n\ngrid_obj = GridSearchCV(clf, parameters, scoring=acc_scorer)\ngrid_obj = grid_obj.fit(X_train, y_train)\n\nclf = grid_obj.best_estimator_\n\nclf.fit(X_train, y_train)","01002ab2":"predictions = clf.predict(X_train)\nprint(accuracy_score(y_train, predictions))","8eabb3f4":"predictions = clf.predict(X_test)\nsubmission = pd.DataFrame({\"PassengerId\": pd.read_csv(path+'test.csv')['PassengerId'], \"Survived\": predictions.astype(np.int32)})\nsubmission.to_csv(\"submission12.csv\", index=False)\nsubmission.info()","57be7108":"from sklearn.preprocessing import scale","78456fe0":"X_train = scale(X_train)\nX_test = scale(X_test)","68eae6d8":"from sklearn.linear_model import LogisticRegression","49d00eb2":"lr = LogisticRegression()\nlr.fit(X_train, y_train)","ce29b849":"y_predict=lr.predict(X_train)\nmetrics.accuracy_score(train_data['Survived'],y_predict)","c7a7fc4c":"predictions = lr.predict(X_test)\nsubmission = pd.DataFrame({\"PassengerId\": pd.read_csv(path+'test.csv')['PassengerId'], \"Survived\": predictions.astype(np.int32)})\nsubmission.to_csv(\"submission13.csv\", index=False)\nsubmission.info()","1f39f73e":"from sklearn.neighbors import KNeighborsClassifier","5212437a":"X_train = scale(full_data[:891].drop(['PassengerId'], axis=1))\ny_train = train_data['Survived']","703e0e90":"knn = KNeighborsClassifier()\nknn.fit( X_train , y_train)","d3f852bd":"y_predicted = knn.predict(X_train)\nmetrics.accuracy_score(y_train, y_predicted)","38005948":"X_test.shape, X_train.shape","e0571ee4":"predictions = knn.predict(np.delete(X_test, -1, axis=1))\nsubmission = pd.DataFrame({\"PassengerId\": pd.read_csv(path+'test.csv')['PassengerId'], \"Survived\": predictions.astype(np.int32)})\nsubmission.to_csv(\"submission14.csv\", index=False)\nsubmission.info()","ceb06c6d":"<h1 style=\"color:red;font-size:huge;\">to check wether if we have missing values let's check the number of rows<\/h1>","de0e2208":"<h1 style=\"font-family:courier;color:#04abed;font-size:40px\">Hello Data Science, Hello Kaggle _<\/h1>\n<h3 style=\"font-family:courier;\">Today we're gonna discover Data Science World, so take a deep breath since we're gonna go deeper than the Titanic ... <\/h3>","279ea385":"<h1 style=\"font-family:courier;color:#04abed;font-size:30px;\">Case study: Titanic Disaster<\/h1>\n<img src=\"http:\/\/image.noelshack.com\/fichiers\/2019\/05\/1\/1548690862-0.jpg\">\n<h1>Image created by Vincent Lugat, thank you Vincent Lugat for the image! <\/h1>","fbdb0772":"# The relation between the age and the survival odds","0ced030b":"<h1 style=\"color:red\">Boxplot VS Barplot:<\/h1>\n<img src=\"https:\/\/pagepiccinini.files.wordpress.com\/2016\/02\/unnamed-chunk-2-1.png\">\n<h2>Boxplot:<\/h2>\n<img src=\"https:\/\/pagepiccinini.files.wordpress.com\/2016\/02\/unnamed-chunk-3-1.png\">\n<h2>Barplot:<\/h2>\n<img src=\"https:\/\/pagepiccinini.files.wordpress.com\/2016\/02\/unnamed-chunk-4-1.png\">\n<h3>Since both data sets have the same mean and standard deviation barplots completely lose the difference between the two data sets.<\/h3> ","87740b40":"## Encoding Title :","85caaef5":"### Building and training the model","b1879c13":"### In the first two cases, it is safe to remove the data with missing values depending upon their occurrences, while in the third case removing observations with missing values can produce a bias in the model. \n<h3 style=\"color:red\">So we have to be really careful before removing observations. Note that imputation does not necessarily give better results.<\/h3>\n<img src=\"https:\/\/cdn-images-1.medium.com\/max\/1200\/1*_RA3mCS30Pr0vUxbp25Yxw.png\">","fc80649f":"<h1 style=\"color:red;font-size:huge;\">WTF !!!!<\/h1>\n<img src=\"https:\/\/media.tenor.com\/images\/d3a4616e47bb45b3ac850427b4b53282\/tenor.gif\" height=\"800px\" width=\"500px\">\n<h1 style=\"color:red;font-size:huge;\">But I'm not seeing Sex and Name columns !!!!!!<\/h1>\n<h1 style=\"color:green;font-size:huge;text-decoration: underline\">Note:<\/h1>\n<h2 style=\"color:#04abed;\"> the method pd.describe() show a brif description for only numerical features, but if it is applied on a single column it will give a brief description for that column even it wasn't numerical <\/h2>","7df70b93":"<h1 style=\"color:#e27106;\">Imputation of Categorical Variables:<\/h1>\n<h3>1.Mode imputation is one method but it will definitely introduce bias.<\/h3>\n<h3>2.Otherwise we can simply create a new category such as \"unknown\" for missing values.<\/h3>\n<h3>3.Prediction models, precisely classification models: In this case, we divide our data set into two sets: One set with no missing values for the variable (training) and another one with missing values (test).<\/h3>","aa57a3ad":"<h2 style=\"font-size:20px;color:red;\">Let\u2019s Understand Why We Need Data Science ?<\/h2>","97631d26":"<h1 style=\"font-family:courier;font_size:30px;color:#04abed;\">print('thank you, c u next session')\n<br\/>\nds.close_session(sess_num=2)_<\/h1>","c1204b53":"<h1 style=\"color:green;font-size:huge;\">How to deal with missing value ?!<\/h1>\n<h2 style=\"color:#04abed;font-size:huge;\">First Rule : There is NO good way to deal with missing data<\/h2>\n<h3 style=\"color:green;font-size:huge;\">There are two ways to do the job; Deletion or Imputation<\/h3>","88c9073f":"### Processing Embarked :","f9ded303":"<img src=\"https:\/\/memegenerator.net\/img\/instances\/57285661.jpg\">\n<h1 style=\"color:red;font-size:huge;\">We have some missing values for the Age and Embarked columns, But for Cabin column we've 687 missing values !!!<\/h1>","de2c37e4":"<h1 style=\"color:green;font-size:huge;\">Otherwise, we can use df.info() to show informations about all columns<\/h1>","f6dc3420":"<h1 style=\"color:red\">Outliers:<\/h1>","edf91042":"# confusion matrix \n<img src=\"https:\/\/www.researchgate.net\/profile\/D_Soeffker\/publication\/328860691\/figure\/fig2\/AS:691499431907328@1541877720208\/Confusion-matrix-and-related-performance-measures.png\">","93764129":"<img src=\"https:\/\/cdn-images-1.medium.com\/max\/800\/0*tkonfeb6Z9ubAllz.png\">","93a1dd02":"<h1 style=\"color:red; font-size:huge\">If you're opening this kernel for the first time, make sure to enable connection on your kernel (in the setting section, select internet connected instead of internet blocked) and run the cell below (uncomment the 2nd and the 3rd lines)<\/h1>","3ebb412a":"## But first let's know why data goes missing ?","74ed56cf":"<h1 style=\"font-family:courier;color:#04abed;font-size:25px;\">How to do Data Science?<\/h1>\nA typical data science process looks like this, which can be modified for specific use case:","efb447ec":"<h1 style=\"color:red;font-size:20px;font-family:courier;\">First rule !!!! _COMBINE THE TRAIN AND THE TEST SET TO APPLY THE SAME CHANGES ON BOTH.<\/h1>","f7dc6701":"<h1 style=\"color:red;fontsize:15px;font-family:courier;\">Now training the model on the entire data with only the important features.<\/h1>","e9a49341":"### Explore the Data\nAfter the data has been processed and converted into a form that can then be used for the later stages, you need to explore it further so as to get the characteristics of the data and find out more about the obvious trends, correlation and the not so obvious hidden relationships and more.","3ad5aefa":"### the code in the two cells nelowbelow do the same as the one in the last two cells above :","aca20e75":"<img src=\"https:\/\/d1jnx9ba8s6j9r.cloudfront.net\/blog\/wp-content\/uploads\/2017\/01\/Flow-of-unstructured-data.png\" >\nTraditionally, the data that we had was mostly structured and small in size, which could be analyzed by using the simple BI tools. Unlike data in the traditional systems which was mostly structured, today most of the data is unstructured or semi-structured. Let\u2019s have a look at the data trends in the image given above which shows that by 2020, more than 80 % of the data will be unstructured.\nThis data is generated from different sources like financial logs, text files, multimedia forms, sensors (IOT), and instruments. Simple BI tools are not capable of processing this huge volume and variety of data. This is why we need more complex and advanced analytical tools and algorithms for processing, analyzing and drawing meaningful insights out of it.","9fd8e99f":"## Trying KNN :","486e0fdd":"<h1 style=\"color:red;font-size:huge\">Let's Explore the Data<\/h1>","0d81327b":"### Using the regressor :\n#### we will use Pclass, Sex and Title colmuns to predict the missing age :","1c7ec055":"<h1 style=\"color:#e27106;\">Time-Series Specific Methods:<\/h1>\n<h2 style=\"color:#6823bc; text-decoration:underline\">Last Observation Carried Forward(LOCF) & Next Observation Carried Backward(NOCB):<\/h2>\n<h3>Both these methods can introduce bias in analysis, underestimate the variability of the estimated result, and perform poorly when data has a visible trend.<\/h3>\n<h2 style=\"color:#6823bc; text-decoration:underline\">Linear Interpolation :<\/h2>\n<h3> This method works well for a time series with some trend but is not suitable for seasonal data<\/h3>\n<h2 style=\"color:#6823bc; text-decoration:underline\">Seasonal Adjustment + Linear Interpolation :<\/h2>\n<h3>This method works well for data with both trend and seasonality<\/h3>","d9677bf8":"<img src=\"https:\/\/cdn-images-1.medium.com\/max\/600\/1*vJ1eIjoiOHUgO-T-M9pI1g.png\" width=\"800px\" height=\"500px\">\n<img src=\"https:\/\/cdn-images-1.medium.com\/max\/600\/1*8X9pf_HPhm524SXoFSGumw.png\" width=\"800px\" height=\"500px\">\n<img src=\"https:\/\/cdn-images-1.medium.com\/max\/600\/1*SaB-NJJ3yyW10usfN8vQrg.png\" width=\"800px\" height=\"500px\">\n<img src=\"https:\/\/cdn-images-1.medium.com\/max\/600\/1*D3zeoHAU4yStWhJ3nCorQg.png\" width=\"800px\" height=\"500px\">","08f7478c":"## Encoding ticket :","48bc1d06":"### loading packages and libraries","1fc35b3b":"#### Splitting the train set into train and dev set","1a1eb6e1":"# Now let's encode the data :","9c5a2818":"<h1 style=\"font-family:courier;color:#04abed;font-size:25px;\">Why Data Science is important?<\/h1>\nEvery phenomenon has a reason behind its occurrence. So has data science. Therefore it would be interesting to know the emerging trends that give data science the utmost importance to keep pace with the changing scenario:\n\n<img src=\"https:\/\/cdn.intellipaat.com\/blog\/wp-content\/uploads\/2016\/11\/Why-do-we-need-Data-Science.jpg\">\nEvery business has data but its business value depends on how much they know about the data they have.\nData Science has gained importance in recent times because it can help businesses to increase business value of its available data which in turn can help them to take competitive advantage against their competitors.\nIt can help us to know our customers better, it can help us to optimize our processes, it can help us to take better decisions. Because of data science, data has become strategic asset.\nIn the following chart, you can have a look at the business use cases where data science is being used in the industry.\n\n<img src=\"https:\/\/d1jnx9ba8s6j9r.cloudfront.net\/blog\/wp-content\/uploads\/2017\/01\/Data-Science-use-cases.png\">","48321a28":"### Understand the business or the Problem\nLearn about the issue at ground, ask the right questions which is at the center of what a Data Scientist does and forms the foundation for the later stages of the Data Scientist\u2019s role. Define the problem and convert it into a concrete framework which can then be worked upon.","79a52470":"### Build & validate the models (Analyze the Data)\nThis is where the magic happens. The data scientist deploys the various arsenals in his repository like machine learning, statistics and probability, linear and logistic regression, time-series analysis and more in order to make sense of the data. At the end of this step the Data Scientist would be able to gain valuable business insights like predictions, business process optimization, finding new ways of doing the same old things among other things.","e74ffed7":"## Preprocessing Name","d85f4eef":"### Before the training let's scale our data :","73b9966f":"<img src=\"https:\/\/i.pinimg.com\/564x\/98\/56\/50\/985650d8c8264e6723f6e020a9988c08.jpg\">\n<img src=\"https:\/\/i.pinimg.com\/564x\/a7\/b4\/9f\/a7b49fce57a7507f5103850f3ba34ff2.jpg\">\n<img src=\"https:\/\/conversionxl.com\/wp-content\/uploads\/2017\/01\/outlier2a.jpg\">\n<img src=\"https:\/\/conversionxl.com\/wp-content\/uploads\/2017\/01\/010211_dp_table_big.png\">","b4c7bf7e":"### we still have one missing value in the Fare column, so let's fix it :","f557741b":"## Encoding Sex :","d90f1129":"### Processing Cabin","6685283d":"<h1 style=\"color:#e27106;\">KNN:<\/h1>\n<h3>1. Continuous Data: The commonly used distance metrics for continuous data are Euclidean, Manhattan and Cosine<\/h3>\n<h3>2. Categorical Data: Hamming distance is generally used in this case<\/h3>","5e3c512d":"### we can see that the passengers with cheaper ticket fares are more likely to die","d36df5c8":"## Trying another model","084dfa60":"#### Let's explore how many title we have :","e1c9cd51":"### Above, we can see men who are between 20 to 40 are survived more compared to older aged men. But for the women odds don't depend on their age.","8031330d":"### Keeping only the variables which are significant for the model(>0.01)","d6db8aa1":"## Trying Logistic regression  :","ceb7acee":"### Feature importance","3a61f4bb":"<img src=\"https:\/\/pbs.twimg.com\/media\/CLQxUOHUwAEnHGn.png\">","58049387":"### Small green dots between x=0 & x=10 : Children who were survived.\n\n### Small red dots between x=10 & x=45: Adults who died (from a lower classes).\n\n### Large green dots between x=20 & x=45 : Adults with larger ticket fares who are survived.","158fc9bd":"### 1.Missing at Random (MAR): \nMissing at random means that the propensity for a data point to be missing is not related to the missing data, but it is related to some of the observed data\n### 2.Missing Completely at Random (MCAR):\nThe fact that a certain value is missing has nothing to do with its hypothetical value and with the values of other variables.\n### 3.Missing not at Random (MNAR):\nTwo possible reasons are that the missing value depends on the hypothetical value (e.g. People with high salaries generally do not want to reveal their incomes in surveys) or missing value is dependent on some other variable\u2019s value (e.g. Let\u2019s assume that females generally don\u2019t want to reveal their ages! Here the missing value in age variable is impacted by gender variable)","2d472431":"1. # Guess what guys !!!! we're TOP 11%\n![image.png](attachment:image.png)","4f6819a1":"## Preprocessing Age","7b3053eb":"<h2 style=\"color:#6823bc; text-decoration:underline\">Dropping Variables :<\/h2>\n<h3 style=\"color:red\">In my opinion, it is always better to keep data than to discard it.<\/h3>\n<h3>Sometimes you can drop variables if the data is missing for more than 60% observations but only if that variable is insignificant.<\/h3>\n<h3 style=\"color:green\">Having said that, imputation is always a preferred choice over dropping variables !<\/h3>\n<h3>Code :<\/h3>\n<h3 style=\"color:#ef04a1;font-family:courier\">del mydata.column_name<\/h3>\n<h3>Otherwise :<\/h3>\n<h3 style=\"color:#ef04a1;font-family:courier\">mydata.drop('column_name', axis=1, inplace=True)<\/h3>\n<br>\n<br>","0e3c48b7":"<h1 style=\"color:#e27106;\">Deletion :<\/h1>\n<h2 style=\"color:#6823bc; text-decoration:underline\">Listwise :<\/h2>\n<img src=\"https:\/\/images.slideplayer.com\/13\/3615244\/slides\/slide_13.jpg\">\n<h3>But in most cases, it is often disadvantageous to use listwise deletion. This is because the assumptions of MCAR are typically rare to support.<\/h3> \n<h3 style=\"color:red\">As a result, listwise deletion methods produce biased parameters and estimates.<\/h3>\n<h3>Code :<\/h3>\n<h3 style=\"color:#ef04a1;font-family:courier\">mydata.dropna(inplace=True)<\/h3>\n<br>\n<br>\n<h2 style=\"color:#6823bc; text-decoration:underline\">Pairwise :<\/h2>\n<img src=\"https:\/\/d35fo82fjcw0y8.cloudfront.net\/2016\/03\/03210603\/pairwise-deletion.jpg\">\n<h3>pairwise deletion analyses all cases in which the variables of interest are present and thus maximizes all data available by an analysis basis.<\/h3>\n<h3 style=\"color:red\">A strength to this technique is that it increases power in your analysis but it has many disadvantages. It assumes that the missing data are MCAR. If you delete pairwise then you\u2019ll end up with different numbers of observations contributing to different parts of your model, which can make interpretation difficult.<\/h3>\n<img src=\"https:\/\/cdn-images-1.medium.com\/max\/1000\/1*3Pcgo66zuwLY2HkUhVr2mg.png\">\n<br>\n<br>\n<h1 style=\"color:#04abed;\">Listwise VS Pairwise :<\/h1>\n<img src=\"https:\/\/discourse-cdn-sjc1.com\/business6\/uploads\/analyticsvidhya\/original\/1X\/8681016024c01025b6244f19c7be720e4ae76040.png\" height=\"300px\" width=\"500\">","5e8b02da":"### Feature Engineering","005fc277":"### Chart above says that more male passengers are died compared to females","acaadbdb":"<img src=\"https:\/\/i.pinimg.com\/564x\/c5\/15\/36\/c515368a93fba3252a221d9dbf3dc308.jpg\">","3c9e0b6c":"### Exploratory Data Analysis","69fd3663":"### Prepare & process the data\nData can rarely be used in its original form. It needs to be processed and various methods exist to convert it into a usable format. This is an essential part of every Data Scientist\u2019s job routine and this consumes a major chunk of his time and resources.","646c61f2":"## Trying Another model","64bf1b92":"#### In the training session we processed the Age as below, but this time I will use an imputatuin method which is regressor :\n#### the commented cells contain the approach we've used in the training session:","864a8b7a":"<h1 style=\"font-family:courier;color:#04abed;font-size:30px;\">Machine Learning for Data Science<\/h1>\n<h1 style=\"font-family:courier;color:green;font-size:25px;\">Classification VS Regression<\/h1>\n<img src=\"https:\/\/cdn-images-1.medium.com\/max\/1600\/0*WE3Sz--1NUEWBmUR.\">\n<h1 style=\"font-family:courier;color:green;font-size:25px;\">Linear Model : Logistic regression, SVM :<\/h1>\n<img src=\"https:\/\/s3.amazonaws.com\/stackabuse\/media\/implementing-svm-kernel-svm-python-scikit-learn-1.jpg\">\n<h1 style=\"font-family:courier;color:green;font-size:25px;\">Tree-based method :Decision tree, Random Forest, GBDT :<\/h1>\n<img src=\"https:\/\/docs.microsoft.com\/es-es\/azure\/machine-learning\/studio\/media\/algorithm-choice\/image5.png\">\n<h1 style=\"font-family:courier;color:green;font-size:25px;\">KNN :<\/h1>\n<img src=\"https:\/\/cambridgecoding.files.wordpress.com\/2016\/01\/knn2.jpg\">\n<img src=\"https:\/\/www.saedsayad.com\/images\/KNN_similarity.png\">\n<h1 style=\"font-family:courier;color:green;font-size:25px;\">Neural Nets for advanced Data science<\/h1>\n<img src=\"https:\/\/miro.medium.com\/max\/818\/0*1DqTRU7WREONm9oa.png\">\n<h1 style=\"font-family:courier;color:green;font-size:25px;\">Tools<\/h1>\n<img src=\"https:\/\/www.digitalvidya.com\/wp-content\/uploads\/2018\/05\/Data-Science-using-Python-Student-tool-image.jpg\">","798d6ac6":"### Collect the data\nAs the name implies the Data Scientist has to collect enough data in order to make sense of the problem at hand and get a better grip of the issue with respect to the time, money and resources needed to make the process successful.","20df9caa":"### Processing Family column :","eae9f02c":"<h1 style=\"color:#e27106;\">Linear Regression:<\/h1>\n<h3>First of all we need to select the appropriate features to esetimate the variable with missing values.<\/h3> <h3 style=\"color:red\">For that we need correlation matrix<\/h3>\n<img src=\"https:\/\/seaborn.pydata.org\/_images\/many_pairwise_correlations.png\">\n<h3>So we can now predict the missing values using the variables with the highest correlations with the variable having missing values, so they generate the regression equation which is used to predict missing values for incomplete cases.<\/h3>\n<img src=\"https:\/\/miro.medium.com\/max\/600\/1*iuqVEjdtEMY8oIu3cGwC1g.png\">\n<h3>In an iterative process, values for the missing variable are inserted and then all cases are used to predict the dependent variable. These steps are repeated until there is little difference between the predicted values from one step to the next, that is they converge. <\/h3>\n<h3 style=\"color:red\">Theoretically, this method provides good estimates for missing values. However it causes deflation of standard errors since the replaced values were predicted from other variables, so the tend to fit together too well <\/h3>","0e4a8bbd":"<h1 style=\"color:#e27106;\">Mean, Median and Mode :<\/h1>\n<h3>Filling the missing data with an estimation of the mean, median or mode without taking advantages of the time series characteristics or relationship between features<\/h3>\n<h3 style=\"color:red\">One disadvantage is that mean imputation reduces variance in the dataset.<\/h3>\n<h3>Code :<\/h3>\n<h3 style=\"color:#ef04a1;font-family:courier\">from sklearn.preprocessing import Imputer<\/h3>\n<h3 style=\"color:#ef04a1;font-family:courier\">values = mydata.values<\/h3>\n<h3 style=\"color:#ef04a1;font-family:courier\">imputer = Imputer(missing_values=\u2019NaN\u2019, strategy=\u2019mean\u2019)<\/h3>\n<h3 style=\"color:#ef04a1;font-family:courier\">transformed_values = imputer.fit_transform(values)<\/h3>\n<h3 style=\"color:red\">strategy can be changed to \"median\" and \u201cmost_frequent\u201d<\/h3>","c91b6de4":"### Communicate the Results then Deploy & monitor the performance\nAt the end of the entire process there is a need to communicate the findings to the right stake-holders in order to get the groundwork done for the action to be taken and deployment of the decisions that are taken.","e9259fec":"### Getting more insights","a2179ad0":"<h1 style=\"font-family:courier;color:#04abed;font-size:25px;\">What is Data Science ?<\/h1>\nData science is an interdisciplinary field that uses scientific methods, processes, algorithms and systems to extract knowledge and insights from data in various forms, both structured and unstructured, similar to data mining. ~Wikipedia\n\nData Science is a field where we apply \u2018science\u2019 to available \u2018data\u2019 in order to get the \u2018patterns\u2019 or \u2018insights\u2019 which can help a business to optimize operations or improvise decisions.\n\n<img src=\"https:\/\/cdn-images-1.medium.com\/max\/800\/1*ewxqYVXny5jyDQlo-xMuLA.png\"\/>","efb59ca0":"## Hell YEAH : We're getting closer guys : we're TOP 6% with this submission (submission11.csv)\n![image.png](attachment:image.png)","04cb23c2":"### so we have only 9 unique value in the Ticket_Count_Group column. And guess what  !!! its type is int (already encoded ;) )"}}