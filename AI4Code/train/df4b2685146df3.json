{"cell_type":{"8b9d0412":"code","6c74d6b3":"code","7cca9912":"code","60ffbf14":"code","1571c566":"code","eddb5e7f":"code","ccbe1c31":"code","9b2f7960":"code","3ff27c41":"code","621b8b83":"code","8a032598":"code","5c435de0":"code","35ee9b52":"code","606026d3":"code","eda654fa":"code","471602d2":"code","43a2a23c":"code","5480fb83":"code","cf0a92ae":"code","2f0f96a0":"code","fc639abb":"code","cd8e3959":"code","b9152754":"code","cb99678f":"code","5d29cf16":"markdown","db245c63":"markdown","6ffa9c35":"markdown","8968477e":"markdown","cc69d37b":"markdown","e563bffa":"markdown"},"source":{"8b9d0412":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","6c74d6b3":"\n#we are checking for Boston housing data, and fitting the linear regression model into the machine learning\nfrom sklearn.datasets import load_boston\nboston = load_boston()\nprint(boston.DESCR) #getting the DESCR in order to get the information about the dataframe columns\n\n#I found out later that this dataframe is missing the MEDV, hence I am using the csv file","7cca9912":"df = pd.read_csv('..\/input\/boston-housing\/boston_housing.csv')\ndf.head()\n","60ffbf14":"df.info() #to see the datatypes","1571c566":"sns.displot(df['medv'],kde=True) #using Seaborn's distribution plot tool, hoping to see the normal distribution curve, which it does.","eddb5e7f":"df.corr() #finding correlations R value between all the factors","ccbe1c31":"sns.heatmap(df.corr(),cmap='viridis') #to present it in a nicer looking way.","9b2f7960":"sns.clustermap(df.corr(),cmap='viridis') #Here, I would like to see clustered view of the data, to determine which parameters to be put in the model","3ff27c41":"df.columns #to remind myself all the dataframe columns","621b8b83":"# Attribute Information (in order):\n#         - CRIM     per capita crime rate by town\n#         - ZN       proportion of residential land zoned for lots over 25,000 sq.ft.\n#         - INDUS    proportion of non-retail business acres per town\n#         - CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n#         - NOX      nitric oxides concentration (parts per 10 million)\n#         - RM       average number of rooms per dwelling\n#         - AGE      proportion of owner-occupied units built prior to 1940\n#         - DIS      weighted distances to five Boston employment centres\n#         - RAD      index of accessibility to radial highways\n#         - TAX      full-value property-tax rate per $10,000\n#         - PTRATIO  pupil-teacher ratio by town\n#         - B        1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n#         - LSTAT    % lower status of the population\n#         - MEDV     Median value of owner-occupied homes in $1000's","8a032598":"X = df[['lstat', 'nox',\n       'rad', 'tax', 'dis']] \n#here we split the data to X and y rate\n# X is the features\/factors we want to explore (independent variables)\n\n#I picked 'lstat','nox','rad', 'tax', and 'dis' , based on brief observation of the clustermap above. You may try other independent variables you are interested in.","5c435de0":"y = df['medv'] # y is the dependent variable. Since we want to find a price prediction model, we put medv data series as y","35ee9b52":"from sklearn.model_selection import train_test_split #initializing train_test_split function","606026d3":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=101) \n#This line is simply dividing the data into test sets and train sets.\n#test size is % of total data passed for test. 0.4 means 40% of data from df is used for testing.\n#random_state just stating random splits. you can put other numbers if you want. I used 101, following the online lecture I watched.\n#the _train data is datasets we use for training the machine learning model. the _test is the data we use for testing","eda654fa":"from sklearn.linear_model import LinearRegression","471602d2":"lm = LinearRegression()","43a2a23c":"lm.fit(X_train,y_train) #used to fit(train) the linear regression model into the training data","5480fb83":"lm.coef_ #return corresponding coefficients for each features in the datasets","cf0a92ae":"X_train.columns #finding out the corresponding columns pertaining to the lm.coef_","2f0f96a0":"pd.DataFrame(lm.coef_,X.columns,columns=['Coeff']) #lm.coef_ is used to find the corresponding coefficient value relative to the\n\n","fc639abb":"predictions = lm.predict(X_test) #we use X_test because we want to pass on features data that the model has not seen before, which is the X_test (not the X_train)","cd8e3959":"predictions #checking if the code follows through\n","b9152754":"plt.scatter(y_test,predictions) #comparing the y_test versus the price predictions we just made\n#the relatively straight line means the predictions model might be pretty accurate at prediciting actual value","cb99678f":"sns.displot((y_test-predictions),kde=True) #plotting the residual plot\n#residual is difference between actual value (y_test) and prediction value","5d29cf16":"Hi there. This is my first time sharing code in Kaggle. Feel free to give constructive feedback if you want. Thank you.\n\nMy aim is to find a few factors that heavily influenced the median value of owner-occupied homes.\nI am going to use simple Linear Regression model here.\nI am also going to try simple machine learning projection method using scikit-learn's train test split, which I had just learned a couple of minutes ago from Udemy's Data Science Bootcamp. Here I am trying to put what I just learnt into practice using a real data, rather than made up data provided in the course.\n\nCheck out https:\/\/www.udemy.com\/course\/python-for-data-science-and-machine-learning-bootcamp\/ if you would like to know more. This is not sponsored content by the way, just giving credits to the lecturer.","db245c63":"**Creating Prediction Model using Scikit learn's Train Test Split**\n\nThe aim is to create a model that is able to predict the Boston median value data using the test data from the dataframe.\n\nHere, I am splitting the dataframe into X and y, which is needed for the scikit's train test split method.","6ffa9c35":"Here we got a normal distribution curve, which is what we are expecting from a residual plot. \n\nThe prediction model seems to work as per intended, judging from the residual plot.","8968477e":"The Coefficient data speaks about how much medv relates to the other factors. For example, every unit increase in lstat will result in -0.958 decrease in medv (which is around $958 decrease in median value with every % increase of lower status population in the area)\n\nWe can see a relatively high negative coefficient value between medv(Median value of owner-occupied homes in $1000's) and nox (nitric oxides concentration (parts per 10 million))","cc69d37b":"**Making price predictions using the Model**","e563bffa":"I observed high -ve R value correlation between lstat(% lower status population) and median price(medv) (-0.737).\n\nI also observed significant +ve correlation between rad(index of accessibility to radial highways) and tax(full-value property-tax rate per $10,000) as well (0.91). \n\nLastly, theres a pretty significant +ve correlation between indus(proportion of non-retail business acres per town) and nox pollution (0.76)."}}