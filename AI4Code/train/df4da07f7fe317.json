{"cell_type":{"33762f9b":"code","fa116679":"code","57952821":"code","75c107e1":"code","a905a220":"code","9364baad":"code","001332d4":"code","5b16aa3d":"code","6acb5d26":"code","f89b2d1a":"code","0b2c1cc4":"code","d124f866":"code","ee2c2163":"code","b260df05":"code","d74a6c10":"code","07d7a4ef":"code","17580bfa":"code","fe31467e":"code","6a2e3473":"code","135f39b9":"code","eaf3ab48":"code","40c99f7e":"code","900a1a6e":"code","b1edc2c2":"code","0963feb9":"code","795b4ff2":"code","4c65f524":"code","88e3e318":"code","d64eb927":"code","13ada23d":"code","ed5eaa9d":"code","9080bcd1":"code","93b7b58a":"code","9f8387f0":"code","781e543c":"code","2ff1141a":"code","e58ecfc6":"code","19b75cb9":"code","46517b12":"code","8079441d":"code","c5357543":"code","53812163":"code","c3b33a3a":"code","a9ec5522":"code","0f892a7f":"code","e8150104":"code","2ff518f4":"code","85e9ecea":"code","4a63916f":"code","7821e341":"code","917db2d7":"code","cc81d291":"code","cc34ce04":"code","307569f6":"code","e9c65c82":"code","3bd07e96":"code","05d2cdf2":"code","40f31d65":"code","b3b4e529":"code","0bad5d26":"code","8ec9ac13":"code","563d9fbb":"code","d7a3a9ae":"code","db60fd70":"code","90ab42ef":"code","bc90807f":"code","d22029e2":"code","e7dbf6f9":"code","2575f4cb":"markdown","37ba3085":"markdown","7cafd057":"markdown","d3ef12fa":"markdown","b4d49caf":"markdown","264680cb":"markdown","5d445e6d":"markdown","18c43053":"markdown","41a1a9b2":"markdown","fdab68c2":"markdown","6d0abd41":"markdown","32b2c187":"markdown","08ae7310":"markdown","a7f099ec":"markdown","8250948c":"markdown","28ebb071":"markdown","adfe990b":"markdown"},"source":{"33762f9b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","fa116679":"import matplotlib.pyplot as plt\nimport seaborn as sns","57952821":"df = pd.read_csv('\/kaggle\/input\/creditcardfraud\/creditcard.csv')","75c107e1":"df.head() # to take a quick look dataframe","a905a220":"df.shape # checking shape of the dataframe","9364baad":"df.isnull().sum() # checking total number of null values in all columns","001332d4":"df.describe()","5b16aa3d":"df.nunique() # checking number of unique values in all columns","6acb5d26":"df.columns # these are the columns in our dataset","f89b2d1a":"df.shape # checking numbers of column for making Correlation Matrix","0b2c1cc4":"df.shape # checking numbers of column for making Correlation Matrix","d124f866":"cols = list(range(31))  # list of all features\nfig, ax = plt.subplots(figsize=(30,30)) # defining the size and shape of the plot\ncorelation = df.iloc[:,cols].corr() \nsns.heatmap(corelation, xticklabels = corelation.columns, yticklabels = corelation.columns, annot = True)\nplt.savefig('Correlation matrix range all.png') # saving the plot","ee2c2163":"# we can see that there is very less correlation between main features (V1 to V28) so these are ready to feed in our model","b260df05":"# Analysis and Visualization of our problem","d74a6c10":"df.Class.value_counts() # checking the count of values in Class column","07d7a4ef":"df.Class.value_counts().plot(kind='bar', figsize = (10,10),xlabel = 'Fraud Status',ylabel = 'No. of Transactions', title = 'Bar Graph of 0 and 1 in Class column')\n# as we can see that 1 (fraud) is very less than 0 (Not fraud)","17580bfa":"df.Class.value_counts().plot(kind='bar',  loglog = 'sym',figsize = (7,7),xlabel = 'Fraud Status',ylabel = 'No. of Transactions', title = 'Logrithmic Bar Graph of 0 and 1 in Class column')","fe31467e":"# Now we clearly see that this data is imbalanced","6a2e3473":"fig, ax = plt.subplots(figsize=(10,4))  \nplt.scatter(df.Amount, df.Time, c=df.Class,alpha = 0.5,edgecolor='black',linewidth= 1, label = 'Not Fraud')\nplt.scatter(df.Amount, df.Time, marker = 'x', s = 20*df.Class , c = df.Class, label = 'Fraud')\nplt.title('Time vs Amount Scatter Plot')\nplt.xlabel('Amount')\nplt.ylabel('Time')\nplt.legend()\nplt.show()","135f39b9":"df.Amount[df.Class == 1].max()  # maximum amount related to the fraud case is 2125.87 units","eaf3ab48":"fig, ax = plt.subplots(figsize=(10,4))  \nplt.scatter(df.Amount, df.Class ,alpha = 0.5,edgecolor='black',linewidth= 1, label = 'Transaction')\nplt.axvline(df.Amount[df.Class == 1].max(),linewidth = 2,c = 'red' ,label='Max Amount of Fraud transaction(2125.87)' )\nplt.title('Fraud Status vs Amount Scatter Plot')\nplt.xlabel('Amount')\nplt.ylabel('Fraud Status')\nplt.legend()\nplt.show()","40c99f7e":"fig, ax = plt.subplots(figsize=(10,4))  \nplt.scatter(df.V22, df.Class ,alpha = 0.5,edgecolor='black',linewidth= 1, label = 'Transaction')\n\nplt.title('V22 vs Class Plot')\nplt.xlabel('V22')\nplt.ylabel('Fraud Status')\nplt.legend()\nplt.show()\n# V10 has -0.22, V12 has -0.26, V14 has -0.3, V16 has -0.2, V17 has -0.33 and V18 has -0.11 correlation with Class","900a1a6e":"df.Time[df.Class == 1].describe() # Checking Time column wrt Fraud cases","b1edc2c2":"X = df.iloc[:, df.columns != 'Class'].values # Selecting All columns except Class column \ny = df.iloc[:, df.columns == 'Class'].values # Selecting Only Class column","0963feb9":"X.shape # shape of X","795b4ff2":"y.shape # shape of y","4c65f524":"# train test split\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.2, random_state = 42)","88e3e318":"X_train.shape # shape of X_train","d64eb927":"X_test.shape # shape of X_test","13ada23d":"y_train.shape # shape of y_train","ed5eaa9d":"y_test.shape # shape of y_test","9080bcd1":"# for tracking time\ndef timer(start_time=None):\n    if not start_time:\n        start_time = datetime.now()\n        return start_time\n    elif start_time:\n        thour, temp_sec = divmod((datetime.now() - start_time).total_seconds(), 3600)\n        tmin, tsec = divmod(temp_sec, 60)\n        print('\\n Time taken: %i hours %i minutes and %s seconds.' % (thour, tmin, round(tsec, 2)))\n","93b7b58a":"# Xgboost model\nfrom xgboost import XGBClassifier\nclassifier = XGBClassifier()\n\nfrom datetime import datetime\n\nstart_time = timer(None) # timing starts from this point for \"start_time\" variable\nclassifier.fit(X_train, y_train.ravel())\ntimer(start_time) # timing ends here for \"start_time\" variable","9f8387f0":"# an error is prevented by replacing y_train with y_train.ravel() [it converts that array shape to (n, )]","781e543c":"from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, classification_report\ny_pred = classifier.predict(X_test)\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)\naccuracy_score(y_test, y_pred)","2ff1141a":"print(classification_report(y_test,y_pred))","e58ecfc6":"y_pred = classifier.predict(X)\ncm = confusion_matrix(y, y_pred)\nprint(cm)\naccuracy_score(y, y_pred)","19b75cb9":"print(classification_report(y,y_pred))","46517b12":"from sklearn.model_selection import GridSearchCV","8079441d":"params={\n \"learning_rate\"    : [0.05,0.1, 0.3] ,\n \"max_depth\"        : [ 12, 15]\n    \n}","c5357543":"grid_search= GridSearchCV(estimator = classifier,param_grid=params,scoring='f1',n_jobs=-1,cv=5)","53812163":"start_time = timer(None) # timing starts from this point for \"start_time\" variable\ngrid_search.fit(X_train,y_train.ravel())\ntimer(start_time) # timing ends here for \"start_time\" variable","c3b33a3a":"grid_search.best_estimator_","a9ec5522":"accuracy = grid_search.best_score_\naccuracy","0f892a7f":"grid_tuned_classifier = XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n              importance_type='gain', interaction_constraints='',\n              learning_rate=0.3, max_delta_step=0, max_depth=12,\n              min_child_weight=1, missing=None, monotone_constraints='()',\n              n_estimators=100, n_jobs=0, num_parallel_tree=1, random_state=0,\n              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n              tree_method='exact', validate_parameters=1, verbosity=None)\n\n# missing value 'nan' is replaced with 'None'","e8150104":"grid_tuned_classifier.fit(X_train, y_train.ravel())\ny_pred = grid_tuned_classifier.predict(X_test)\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)\nprint('Accuracy on X_test set ',accuracy_score(y_test, y_pred))","2ff518f4":"print('Classification report on X_test set\\n',classification_report(y_test,y_pred))","85e9ecea":"y_pred = grid_tuned_classifier.predict(X)\ncm = confusion_matrix(y, y_pred)\nprint(cm)\nprint('Accuracy on whole X set ',accuracy_score(y, y_pred))","4a63916f":"print('Classification report on whole X set\\n',classification_report(y,y_pred))","7821e341":"from sklearn.model_selection import RandomizedSearchCV","917db2d7":"params={\n \"learning_rate\"    : [0.05, 0.1, 0.2, 0.3, 1 ] ,\n \"max_depth\"        : [10, 12, 15],\n \"min_child_weight\" : [ 1, 3 ],\n \"gamma\"            : [ 0.0, 0.15, 0.3]\n    \n}","cc81d291":"random_search= RandomizedSearchCV(estimator = classifier,param_distributions=params,n_iter=10,scoring='f1',n_jobs=-1,cv=5)","cc34ce04":"start_time = timer(None) # timing starts from this point for \"start_time\" variable\nrandom_search.fit(X_train,y_train.ravel())\ntimer(start_time) # timing ends here for \"start_time\" variable","307569f6":"random_search.best_estimator_","e9c65c82":"random_search.best_params_","3bd07e96":"random_tuned_classifier = XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n              colsample_bynode=1, colsample_bytree=1, gamma=0.3, gpu_id=-1,\n              importance_type='gain', interaction_constraints='',\n              learning_rate=0.1, max_delta_step=0, max_depth=15,\n              min_child_weight=1, missing=None, monotone_constraints='()',\n              n_estimators=100, n_jobs=0, num_parallel_tree=1, random_state=0,\n              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n              tree_method='exact', validate_parameters=1, verbosity=None)\n# 'missing = nan' is changed with 'missing = None'","05d2cdf2":"random_tuned_classifier.fit(X_train, y_train.ravel())\ny_pred = random_tuned_classifier.predict(X_test)\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)\nprint('Accuracy on X_test set ',accuracy_score(y_test, y_pred))","40f31d65":"print('Classification report of X_test set',classification_report(y_test,y_pred))","b3b4e529":"y_pred = random_tuned_classifier.predict(X)\ncm = confusion_matrix(y, y_pred)\nprint(cm)\nprint('Accuracy on whole X set',accuracy_score(y, y_pred))","0bad5d26":"print('Classification report on whole X set',classification_report(y,y_pred))","8ec9ac13":"conclusion = pd.read_csv('..\/input\/conclusion-of-credit-card-fraud-detection\/conclusion.csv')","563d9fbb":"conclusion","d7a3a9ae":"conclusion.columns","db60fd70":"# Precision \/ Recall \/ f1-score Variation on various models of xtest set\n\nfig, ax = plt.subplots(figsize=(7,7))\nplt.plot(conclusion.model,conclusion.xtest_precision, label = 'Precision', marker = 'o')\nplt.plot(conclusion.model,conclusion.xtest_f1score, label = 'f1-score', c = 'Black', marker = 'o')\nplt.plot(conclusion.model,conclusion.xtest_recall, label = 'Recall', c = 'Red', marker = 'o')\nplt.title('Precision \/ Recall \/ f1-score Variation on various models of xtest set')\nplt.grid()\nplt.xlabel('Hyperparameter tuning')\nplt.ylabel('out of 1')\nplt.legend()\nplt.show()","90ab42ef":"# Precision \/ Recall \/ f1-score Variation on various models of X set\n\nfig, ax = plt.subplots(figsize=(7,7))\nplt.plot(conclusion.model,conclusion.x_precision, label = 'Precision', marker = 'o')\nplt.plot(conclusion.model,conclusion.x_f1score\t, label = 'f1-score', c = 'Black', marker = 'o')\nplt.plot(conclusion.model,conclusion.x_recall, label = 'Recall', c = 'Red', marker = 'o')\nplt.title('Precision \/ Recall \/ f1-score Variation on various models of X set')\nplt.grid()\nplt.xlabel('Hyperparameter tuning')\nplt.ylabel('out of 1')\nplt.legend()\nplt.show()","bc90807f":"# Accuracy Variation on various models of x_test and x set\n\nfig, ax = plt.subplots(figsize=(7,7))\nplt.plot(conclusion.model,conclusion.X_accuracy, label = 'X Accuracy', c = 'Red', marker = 'o')\nplt.plot(conclusion.model,conclusion.X_test_accuracy, label = 'X_test Accuracy', marker = 'o')\nplt.title('Accuracy Variation on various models of x_test and x set')\nplt.grid()\nplt.xlabel('Hyperparameter tuning')\nplt.ylabel('out of 1')\nplt.legend()\nplt.show()","d22029e2":"# Time taken to train various models of on X_train set\n\nfig, ax = plt.subplots(figsize=(7,7))\nplt.plot(conclusion.model,conclusion.time_taken, label = 'time in seconds', marker = 'o')\nplt.title('Time taken to train various models of on X_train set')\nplt.xlabel('Hyperparameter tuning')\nplt.ylabel('Time Taken in Seconds')\nplt.legend()\nplt.grid()\nplt.show()","e7dbf6f9":"#FINISHED--------------------------- A notebook by Ajay Chouhan -------------------------------------------------","2575f4cb":"# Conclusion","37ba3085":"# Default Xgboost Classifier model","7cafd057":"# Visualizing Correlation Matrix of all features","d3ef12fa":"# Hyperparameter tuning with RandomizedSearchCV","b4d49caf":"# Train-Test Split","264680cb":"X set prediction","5d445e6d":"### X set Prediction on default hyperparameters","18c43053":"### for better representation we can plot these values on logrithmic scale","41a1a9b2":"Whole X set prediction","fdab68c2":"X_test set prediction","6d0abd41":"# Understanding the data and doing Feature Engineering using Pandas","32b2c187":"### X_test prediction on default hyperparameters","08ae7310":"# Splitting the Features and Target variables","a7f099ec":"# Importing Libraries","8250948c":"X_test set prediction","28ebb071":"# Hyperparameters tuning with GridSearchCV","adfe990b":"Conclusion-\n\n1. Randomized Search CV approach of Hyperparameter tuning gave better results but in more time (with 13 parameters distribution). Grid Search CV takes more time and computation power when grid is bigger, so I used only 5 numbers of parameters in parameter grid. Although Randomized Search CV took more time but it covers more parameters.\n\n2. With the same grid size of parameters, Grid Search CV takes very much time, so this is prohibited for bigger grids. \n\n3. f1-score of 0.89 is achieved by RandomizedSearchCV Hyperparameter tuning on X_test set.\n\n4. RandomizedSearchCV always gives different results. I ran this 4 times and 0.89 was the best f1-score I got on the x_test set.\n\n5. In this notebook I focused more on Understanding the data with visualization and RandomizedSearchCV & GridSearchCV approaches of Hyper parameters tuning. Although better f1-score can be achieved by providing more parameters to these two approaches but it will be cost more time and computational power."}}