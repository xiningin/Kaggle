{"cell_type":{"bc811233":"code","38036473":"code","513e4708":"code","daaa26d9":"code","35a69c9f":"code","306ed72d":"code","b5e13b78":"code","4a14645b":"code","aeca4aef":"code","a58835b6":"code","8aa1b6c5":"code","d88ebec9":"code","f8c3a39a":"code","0f5e7117":"code","232bbae3":"code","fb185431":"code","af58919d":"code","31215ba8":"code","b912bccf":"code","07814cfe":"code","fee3a606":"code","31c7dbcb":"code","e7c1dcf7":"code","60a9587b":"code","565a5845":"code","19139635":"code","ed630536":"code","aa01e7bd":"code","8bb2306a":"code","66bbd7f6":"code","939348ee":"code","18900045":"code","9405a87c":"code","ca644b5b":"code","03c7efdc":"code","02c2a8cf":"code","30251852":"code","727058ff":"code","3169d9c3":"code","5e596ad6":"code","79169311":"code","68d418c5":"code","82306680":"code","09d99efe":"markdown","7e6d9ab7":"markdown","0bbd764f":"markdown","b5283305":"markdown","282203e6":"markdown","93ae4b20":"markdown","f74edd3f":"markdown","de265679":"markdown","539f4f41":"markdown","bee39a29":"markdown","a72ad790":"markdown","f6e12747":"markdown","723ebdfc":"markdown","4b39c9c3":"markdown","7e105454":"markdown","425ec375":"markdown","b20accf9":"markdown","bc158df9":"markdown","5a911336":"markdown","5b36050a":"markdown","c327dad7":"markdown","b6fa2b1d":"markdown","98773b04":"markdown","1390f9fc":"markdown","315dd202":"markdown"},"source":{"bc811233":"import os\nos.getcwd()","38036473":"# os.chdir('\/Users\/steven\/Documents\/Kaggle\/Titanic')","513e4708":"os.getcwd()","daaa26d9":"import pandas as pd","35a69c9f":"%pylab inline  \n# the %pylab statement here is just something to make sure visualizations we will make later on \n# will be printed within this window..","306ed72d":"train_df = pd.read_csv('..\/input\/train.csv', header=0)\n#above is the basic command to 'import' your csv data. Df is the new name for your 'imported' data \n#(df is short for dataframe, you can name this anyway you want, but including 'df' in your name is convention)\n\ntest_df = pd.read_csv('..\/input\/test.csv', header=0)\n#you don't have to use the test set but I am doing this to eveluate the model without uploading. You can slip this.\n\n#Other options for this (splitting dataset to train part and test part) involve importing 'train_test_split' from sklearn. \n#I have not used this option, but perhaps it is 'easier'..\n\ntrain_df.head(2)\n#with df.head(2) we can 'test' by previewing the first(head) 2 rows(2) of the dataframe(df)\n#You can see the final x by using 'df.tail(x)  (replace x with number of rows)","b5e13b78":"train_df\n#show full dataset (df). (if very large this can be very inconvenient but with our trainset it's ok)\n#notice it adds a rows total and columns total underneath (troubleshooting: if you do not see these totals\n# you can seperatly create this by using 'df.shape')","4a14645b":"#let's get slightly more meta. (data about the data, like what type is eacht variable ('column')?)\ntrain_df.info()\n#especially the information on the right is usefull at this point (the clomuns with values 'int64 and 'object' etc)\n# These values describing each variable should be identical to that of the testset (which in this case being\n# the Titanic datasets from Kaggle) they are. To test this you could repeat this procedure but use the test set instead\n# of the train set.","aeca4aef":"# More, More more!\n#Let's fully dive in this meta data description of the variables:\ntrain_df.describe()\n\n# Notice that the decribe function only gives back non-'object' (7 out of the 12) variables..","a58835b6":"train_df.Survived.value_counts()\n#the variable name (Survived) is with a capital letter because it has a capital letter in data set.\n#'value_counts' is the 'smart' part, the function.","8aa1b6c5":"train_df.Sex.value_counts().plot(kind='bar')\n#you can replace the variable with any of the 12 (for some with more visual succes than for others..)","d88ebec9":"train_df[train_df.Sex=='female']\n# double == because were making a comparison not setting up for creating)","f8c3a39a":"#before continuing let's do a quick check for 'missing values' (rows where gender is unknown)\n# by using the 'isnull' function:\ntrain_df[train_df.Sex.isnull()]","0f5e7117":"# Let's visualize the number of survival amongst woman and later the number of survival amongst men to campare.\ntrain_df[train_df.Sex=='female'].Survived.value_counts().plot(kind='bar', title='Survival among female')","232bbae3":"train_df[train_df.Sex=='male'].Survived.value_counts().plot(kind='bar', title='Survival among male')","fb185431":"# The same can be done for age. Here it can also be interesting to combine age with sex;\ntrain_df[(train_df.Age<11) & (train_df.Sex=='female')].Survived.value_counts().plot(kind='bar')\n# '11' is just an arbitrarily chosen number as value of age.","af58919d":"train_df[(train_df.Age<11)].Survived.value_counts().plot(kind='bar')","31215ba8":"import seaborn as sns\n# I don't know why seaborn is abbreviated as sns but you can choose anything you like as long as it is not used\n# by anything else. Sns seems to be convention.","b912bccf":"sns.barplot(x=\"Pclass\", y=\"Survived\", hue=\"Sex\", data=train_df);","07814cfe":"#If we don't mind stereotypes ;p we could change the colors so that we don't have to look at the legend\n# to remind us of the colorcoding for Sex:\n#Just use the same command but add ' palette={\"male\": \"blue\", \"female\": \"pink\"} '","fee3a606":"sns.barplot(x=\"Pclass\", y=\"Survived\", hue=\"Sex\", data=train_df, palette={\"male\": \"blue\", \"female\": \"pink\"});","31c7dbcb":"# Let's firs remove the variables we don't want:\ndef drop_features(df):\n    return df.drop(['Ticket', 'Name', 'Embarked'], axis=1)","e7c1dcf7":"# make bins for ages and name them for ease:\ndef simplify_ages(df):\n    df.Age = df.Age.fillna(-0.5)\n    bins = (-1, 0, 5, 12, 18, 25, 35, 60, 120)\n    group_names = ['Unknown', 'Baby', 'Child', 'Teenager', 'Student', 'Young Adult', 'Adult', 'Senior']\n    categories = pd.cut(df.Age, bins, labels=group_names)\n    df.Age = categories\n    return df","60a9587b":"#keep only the first letter (similar effect as making bins\/clusters):\ndef simplify_cabins(df):\n    df.Cabin = df.Cabin.fillna('N')\n    df.Cabin = df.Cabin.apply(lambda x: x[0])\n    return df","565a5845":"# make bins for fare prices and name them:\ndef simplify_fares(df):\n    df.Fare = df.Fare.fillna(-0.5)\n    bins = (-1, 0, 8, 15, 31, 1000)\n    group_names = ['Unknown', '1_quartile', '2_quartile', '3_quartile', '4_quartile']\n    categories = pd.cut(df.Fare, bins, labels=group_names)\n    df.Fare = categories\n    return df","19139635":"# createa all in transform_features function to be called later:\ndef transform_features(df):\n    df = simplify_ages(df)\n    df = simplify_cabins(df)\n    df = simplify_fares(df)\n    df = drop_features(df)\n    return df","ed630536":"# create new dataframe with different name:\ntrain_df2 = transform_features(train_df)\ntest_df2 = transform_features(test_df)","aa01e7bd":"train_df2.head()","8bb2306a":"sns.barplot(x=\"Age\", y=\"Survived\", hue=\"Sex\", data=train_df2, palette={\"male\": \"blue\", \"female\": \"pink\"});","66bbd7f6":"sns.barplot(x=\"Cabin\", y=\"Survived\", hue=\"Sex\", data=train_df2, palette={\"male\": \"blue\", \"female\": \"pink\"});","939348ee":"sns.barplot(x='Pclass', y='Survived', hue='Sex', data=train_df2, palette={'male': 'blue', 'female': 'pink'});","18900045":"from sklearn import preprocessing\ndef encode_features(df_train, df_test):\n    features = ['Fare', 'Cabin', 'Age', 'Sex']\n    df_combined = pd.concat([df_train[features], df_test[features]])\n    \n    for feature in features:\n        le = preprocessing.LabelEncoder()\n        le = le.fit(df_combined[feature])\n        df_train[feature] = le.transform(df_train[feature])\n        df_test[feature] = le.transform(df_test[feature])\n    return df_train, df_test\n    \ntrain_df2, test_df2 = encode_features(train_df2, test_df2)\ntrain_df2.head()","9405a87c":"train_df2.info()","ca644b5b":"X_train = train_df2.drop([\"Survived\", \"PassengerId\"], axis=1)\nY_train = train_df2[\"Survived\"]\nX_test  = test_df2.drop(\"PassengerId\", axis=1).copy()\n\n\n\n\n# I initially did not drop PassengerID. Keeping 8 variables ('features') in x-train and x-test. However, later on\n# (during the modelling part) this resulted in an accuracy (for the random forests and classification trees) of 1.00\n# Most likely I think it keeping PassengerId in this manner caused some form of label leakage. \n# After dropping this in both sets accuracy results were more realistic..\n\nX_train.shape, Y_train.shape , X_test.shape","03c7efdc":"X_train.head()","02c2a8cf":"Y_train.head()","30251852":"# Logistic Regression\n\n# Import from the the scikit-learn library (sklearn is the abbreviation for scikit-learn)\nfrom sklearn.linear_model import LogisticRegression\n\nlogreg = LogisticRegression()\nlogreg.fit(X_train, Y_train)\nY_pred = logreg.predict(X_test)\nacc_log = round(logreg.score(X_train, Y_train) * 100, 2)\nacc_log","727058ff":"# Decision Tree\n\n# Import from the the scikit-learn library (sklearn is the abbreviation for scikit-learn)\nfrom sklearn.tree import DecisionTreeClassifier\n\ndecision_tree = DecisionTreeClassifier()\ndecision_tree.fit(X_train, Y_train)\nY_pred = decision_tree.predict(X_test)\nacc_decision_tree = round(decision_tree.score(X_train, Y_train) * 100, 2)\nacc_decision_tree","3169d9c3":"# Random Forest\n\n# Import from the the scikit-learn library (sklearn is the abbreviation for scikit-learn)\nfrom sklearn.ensemble import RandomForestClassifier\n\nrandom_forest = RandomForestClassifier(n_estimators=100)\nrandom_forest.fit(X_train, Y_train)\nY_pred = random_forest.predict(X_test)\nrandom_forest.score(X_train, Y_train)\nacc_random_forest = round(random_forest.score(X_train, Y_train) * 100, 2)\n\nacc_random_forest","5e596ad6":"#Creating a csv with the predicted scores (Y as 0 and 1's for survival)\nsubmission = pd.DataFrame({\n        \"PassengerId\": test_df[\"PassengerId\"],\n        \"Survived\": Y_pred\n    })\n\n# But let's print it first to see if we don't see anything weird:","79169311":"submission.describe()","68d418c5":"os.getcwd()","82306680":"#submission.to_csv('..\/pathhere..\/submission.csv', index=False)","09d99efe":"***\n<br>\n## 5 Submission\n<br>\n<br>\n#### All looks fine. Let's turn it into a CSV and save it in a logical local place and upload it to Kaggle to find out real score (i.e. the score on their test set)\n<br>\n<br>","7e6d9ab7":"***\n<br>\n### 1B: Getting started: Having a looksy at the data (\"EDA\")\n\nSo now you are ready to start. You want to look a bit at the data. Two (there are a lot more) basic ways are;\n- 1: open your data files (csv's) with excel, save a copy as .xls and in excel go to the data tab and use the text to data wizard to use the comma's to seperate fields.<br>\n_the advantage of this is that after having a quick look you can you pivot tables functionality in excel to look deeper_\n- 2: show data in your jupyter notebook\n\n\nLet's assume you have already done number 1 (opening in excel and looking in the data, ideally using a pivot table). If you are starting at kaggle I am going to assume you are familiar with excel basics..\nFor number 2 (data in jupyter notebooke\/kaggle kernel) first step in to import a library that helps you work with csv files called 'pandas' _(this is your 'csv-reader' and allows you to create dataframes_ :\n<br>\n<br>","0bbd764f":"### 1A: Getting started: Orienting Yourself\nFirst things first. Even before looking at your data you want to orient yourself _(at least you want to do this if you are new to jupyter notebook and kaggle kernels like me)._ See 'where are working from' and if necesary change your working directory to where you have saved your data files (csv's). <br>To do this you need:\n<li>import os   _#to be able to use the os. commands_\n<li>os.getcwd()   _#to find were you are working from now (like 'pwd' in unix)_\n<li>os.chdir('....path..')   _#to change from you current direcotory to you desired directory (e.g. where your data csv's are). Like 'cd' in unix._<br>\n\n_If you are working fully within a kaggle kernel you can skip this. But is might be good to do this anyway for potential troubleshooting purposes in the future._","b5283305":"In reality we would have to repeat these visualisation command for all variables to see which variables might be intereseting for our model. <br>\n\nFor now we let's say we have repeated this for all variables. <br>\nThis will result in you wanting to keep (at least):\nSex, Age, Pclass, Cabin & Fares (and passengerID)","282203e6":"_Now we copy and past the command from above, and delete the 'fe' from 'female' (don't forget title) :_","93ae4b20":"<br>\nSo we have come some way now.<br>\nIt is time to really start segmenting and we have already made a start with gender (Sex).<br>\nWe choose 'Sex'  to start with because A) it's easy and B) by looking at the data in excel(see beginning) we should have a reasonable suspicion that survival rate is not equal for men and woman. <br>Making this a sensible start for segmenting.\n<br>\n<br>\nLet's show the data for woman only:\n<br>\n<br>","f74edd3f":"_Newbe to Kaggle and datascience in general. As I am creating my first kernel for the Titanic Survival prediction model (in python) I wrote down everything that I thought was unclear -for me at least- at first. The goal I set for myself was to get everythng working, create a model that predicted at least somewhat better than chance alone, and upload it to Kaggle.<br> \n<br>\nI hope this is useful to newbe kagglers like myself._\n<br>\n***\n<br>\n### 1 Decisions to make before starting your first datascience project (kaggle titanic..)\n- _First: the decision to either work 'within' Kaggle (the kaggle kernel) or use use your own downloadable platform. _\n<br>\n- _Second: 'Python or R?'_\n<br>\n<br>\n\n#### First decision:\nWhat is meant by 'own platform'?\nYou can do all of this outside of Kaggle and then upload (\/copy paste code) to kaggle when done. Obviously this requires some setup but the advantages are that you know where you stand when doing datascience projects oustide of kaggle. The setup is made quite easy by means of an application called 'anaconda navigator' which also has other benefits so I stronlgy suggest using this if going for your own setup.\n<br>\n<br>\n#### Second decision:\nDiscussion has been going forever. Rivalling the tabs vs spaces debate :P #piedpiper Being horrible at the javascript syntax I choose python because of its syntaxical ease but both have their advantages and disadvantages. \nMost important take-away; eiter will be fine, if starting choose the language your (desired) job\/company uses.\n<br>\n<br>\n\n\n### 2 Getting started:\n- 2A: Orienting Yourself\n- 2B: Having a looksy at the data (or Exploratory Data Analysis (EDA))\n<br>\n<br>\n\n### 3 Data prepping & further visualization\n<br>\n### 4 Modelling\n<br>\n### 5 Submission\n<br>\n ##### _(6 reiterate)_\n <br>\n <br>\n***","de265679":"_Before continuing; in kaggle your data-files are located at  '..\/input' . You can check this by navigating to them by scrolling up and using the foldout button with the '+' logo called 'Input Files'_","539f4f41":"So we see we are currently set in kaggle\/working. <BR>\nFor kaggle competitions this is fine. <br>\n<br>\nIf you are working 'locally' with Jupyter notebook, you can change this location with 'os.chdir('\/...path...')\nSo I look up were the folder is that contains the CSV's (train.csv & test.csv downloaded from kaggle) I intend to use.\nYou do this outside Jupyter, by just looking on your computer and looking at the path. <br>\nFor me it is \/Users\/steven\/Documents\/Kaggle\/Titanic so this will be used in the following command.\n<br>\n#### This is case sensitive so pay attention to whether your folders on your PC start with or without uppercase!\n<br>\n_### If you are working in a kaggle kernel, skip the next command._","bee39a29":"<br>\n_About these datatype names:_\n- int64 => whole numbers (can still be categorical)\n- object => string (can be categorical)\n- float64 => numeric with decimals (continous)\n<br>\n<br>\n\n_There is a lot of ambiguity when expressing what type a variables is. \nI think this is in part because 'datatypes' (above) are not the same as 'measurement levels'. In statistics a measurement level hierarchy is used to help you decide which analysis methods are appropriate. Added confusion arises because in some fields 'categorical' is the overlapping clustering contaning (a.o.) nominal, ordinal and ratio as subcategories, while in other fields categorical is synonomous for the ordinal measurement level._\n<br>\n<br>\n\n- nominal (groups)\n- ordinal (groups with hierarchy)\n- interval (numbers with equal differences beteen them)\n- ratio (numbers with equal differences but also a absolute zero-point)","a72ad790":"### Note that the bars are swapped. Our suspicion is confirmed (womand are more likely to have survived)\n_(meaning that in female segment survival occurred far more often than death. In male segment it is the other way around.)_\n<br>\n<br>","f6e12747":"<br>\n### Visualize further using Seaborn\n<br>\nImport the python library seaborn (as sns)<br>\n<br>\n_You can also choose to do this in the beginning when importing pandas, so you get a list in the beginning with every library to be imported. To do it at the beginning is convention (and makes it easy for outsiders to see al used libraries at once) but for the purpose of this getting started tutorial I think it is better like this._","723ebdfc":"<br>\n_As you can see the combination of a low age and female (i.e. little girls) have a quite different (higher) survivalrate compared to the total trainset average survival rate (.38)_\n\nLet's see if childeren regardless of gender ('Sex') also have better chances of survival:\n<br>\n<br>","4b39c9c3":"<br>\n_Let's see what it looks like, see if everything has gone as planned:_\n<br>\n<br>","7e105454":"Looks fine :)\n<br>\n<br>\n### Now let's do some seaborn visualizations with our new dataset:\n_(3vars per plot)_","425ec375":"<br>\nAaaaaaand we have a 0.7512 score. Not too bad for a very first model.<br>\n_(Not too well either because guessing at random would not be .5 but .62. ('mean as model'))_<br>\n<br>\n<br>\nOne of the advantages of starting out with a very basic model is that all feature engineering and inmprovement can be measured in accuracy gains against time spent..\n<br>\n<br>\nSince we haven't done any futher optimization like feature engineering yet, now it would be the time to start improving this 'base' model. _(\/\/ the titles in the passenger name variable would be a good start.)_\n<br>\n<br>\n***\n<br>\n_Please feel free to comment below, I will read and if possible incorporate them and please give tips were you think needed. I will try to learn from users' suggestions and tips:)_\n<br>","b20accf9":"***\n<br>\n## 4 Modelling\n<br>\n<br>\nReady to try some models:\n<br>\n<br>\n- 1) logistic regression \n- 2) decision tree\n- 3) random forests\n<br>\n<br>","bc158df9":"<br>\nNow we check by using same command as before _(and we see it is correct because it prints out the directory we wanted)_:\n<br>\n<br>","5a911336":"_(So we know in our train data set, high over survival change = 342\/(342+549) = .38 )_","5b36050a":"<br>\n## 3:  Data prepping & further visualization\n<br>\nNow that we have gotten familiarized with our data, we have to get the data in such a shape that we can use it.\n<br>\n<br>\n\nThis means dropping some features (variables) we don't want to use (Ticket, Name and Embarked), creating bins for other variables (Age and Fares) or changing some values of a variable to only the first letter (Cabins).\n<br>\n<br>\n\n","c327dad7":"<br>\nWith seaborn we can more easily make barplots that show us more at once. For example we can take the variable Pclass and defining it as the x-axis, while makeing our dependent variable 'Survived' the y-axis, while differentiating between men and women (defining Sex as hue):\n<br>\n<br>","b6fa2b1d":"We can clearly see childeren(< 11 years) in general have better chances than the overal population (trainset) but not as good as childeren (< 11 years) that are also girls.","98773b04":"<br>\n_Make sure that X-train and X-test have the same amount of variables ('features'; in this example '7').._\n<br>\n<br>\n### Almost ready to try some models  ('sci-kitlearn')\n<br>\n<br>\nBeing: <br>\n1) regression \n2) decision tree and \n3) random forests.<br>\n<br>\n<br>\n#### _first a quick looksy at the X-train and Y-train_:","1390f9fc":"_This shows up empty which is good news, saves us time _","315dd202":"<br>\nLooking at our 'dependent variable' (i.e. 'survived') we see the average (mean) of .38 survival rate.<br>\nWe know from the introduction on the problem (the kaggle description) _\"...killing 1502 out of 2224...\"_ so this seems about right because _1 - 1502\/2224 = roughly 1\/3 (.33)._ <br>\nKnowing our .38 is based on the training part of the data set while the given description is based on the total set it is close enough to state this is an honest sample.\n\nLet's say you'd want the actual total of people who survived and the total of people who died (without calculating this back from the mean):\n<br>\n<br>"}}