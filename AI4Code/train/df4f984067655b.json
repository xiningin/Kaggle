{"cell_type":{"3b4ddf77":"code","6b86b775":"code","da13515f":"code","cfc48b5d":"code","43d1db98":"code","6d463db4":"code","5cd5e124":"code","62673712":"code","6cf3bb00":"code","f719474b":"code","8d05c53b":"code","1ad54bbe":"code","cce30c72":"code","0f3628f1":"code","116fa57e":"code","60d446a6":"code","c6de9dc2":"code","c4ecd45d":"code","9f08cff4":"code","dada32c8":"code","8831943c":"code","18811321":"code","2fdf668d":"code","c5204503":"code","9e79b7c0":"code","70519d76":"code","070437af":"code","4121d624":"code","51c6411f":"markdown"},"source":{"3b4ddf77":"import pandas as pd\nimport datetime as dt\nimport seaborn as sns\nfrom keras.layers import Dense,Dropout,SimpleRNN,LSTM\nfrom keras.models import Sequential\nimport keras\nimport tensorflow as tf\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport sklearn.preprocessing","6b86b775":"data=pd.read_csv(\"\/kaggle\/input\/hourly-energy-consumption\/AEP_hourly.csv\")","da13515f":"data","cfc48b5d":"data.describe()","43d1db98":"data.info()","6d463db4":"data.isnull().sum()","5cd5e124":"#We have 24 data points for each day\n#I will try to predict hourly power consumptionfor any day provided we have data for n(3,5,7,10 or any value which gives better results) days prior to it\n","62673712":"#rescaling our data\nscaler = sklearn.preprocessing.MinMaxScaler()\n\ndata[\"normalised_data\"]=scaler.fit_transform(data['AEP_MW'].values.reshape(-1,1))","6cf3bb00":"data","f719474b":"n=7 #Number of prior days you want to pass to the LSTM model\nm=1 #Number of days for which you want to predict\nTrain=data[\"normalised_data\"][:-(24*n+24*m)]\nTest=data[\"normalised_data\"].tail(24*n+24*m)\nTest = Test.reset_index(drop=True) # resetting indices for test because we used .tail(), which keeps old indices in place","8d05c53b":"Train","1ad54bbe":"Test.shape","cce30c72":"#Preprocessing our dataset before feeding into LSTM\n\nX_Train = []\nY_Train = []\n\nfor i in range(24*n, Train.shape[0]):\n    \n    X_Train.append(Train[i-24*n:i])\n    \n    \n    Y_Train.append(Train[i])\n\n# Convert into Numpy Array\nX_Train = np.array(X_Train)\nY_Train = np.array(Y_Train)\n\nprint(X_Train.shape)\nprint(Y_Train.shape)","0f3628f1":"X_Train = np.reshape(X_Train, newshape=(X_Train.shape[0], X_Train.shape[1], 1))\nX_Train.shape","116fa57e":"X_Test = []\nY_Test = []\n\n\nfor i in range(24*n, Test.shape[0]):\n    \n    \n    X_Test.append(Test[i-24*n:i])\n    \n    \n    Y_Test.append(Test[i])\n\n# Convert into Numpy Array\nX_Test = np.array(X_Test)\nY_Test = np.array(Y_Test)\n\nprint(X_Test.shape)\nprint(Y_Test.shape)\n","60d446a6":"X_Test = np.reshape(X_Test, newshape=(X_Test.shape[0], X_Test.shape[1], 1))\nX_Test.shape","c6de9dc2":"regressor = Sequential()\n\n# Adding the first LSTM layer and some Dropout regularisation\nregressor.add(LSTM(units = 512, return_sequences = True, input_shape = (X_Train.shape[1], 1),))\n#regressor.add(Dropout(0.2))\n\n# Adding a second LSTM layer and some Dropout regularisation\nregressor.add(LSTM(units = 256, return_sequences = True,))\n#regressor.add(Dropout(0.1))\n\n#Adding a third LSTM layer and some Dropout regularisation\nregressor.add(LSTM(units = 128, return_sequences = True,))\n#regressor.add(Dropout(0.05))\n\n# Adding a fourth LSTM layer and some Dropout regularisation\nregressor.add(LSTM(units = 64))\n#regressor.add(Dropout(0.2))\nregressor.add(Dense(256, activation='relu'))\n# Adding the output layer\nregressor.add(Dense(units = 1))\n\n# Compiling the RNN\nregressor.compile(optimizer = 'adam', loss = 'mean_squared_error')","c4ecd45d":"regressor.fit(X_Train, Y_Train, epochs = 10, batch_size = 256)","9f08cff4":"pred=regressor.predict(X_Test)","dada32c8":"Y_Test","8831943c":"scaler.fit(data[\"AEP_MW\"].values.reshape(-1,1))","18811321":"rescaled=scaler.inverse_transform(pred)","2fdf668d":"rescaled","c5204503":"rescaled_test=scaler.inverse_transform(Y_Test.reshape(-1,1))","9e79b7c0":"rescaled_test","70519d76":"#plotpred\n\n\nsns.set_style(\"darkgrid\")\n\nplt.plot(np.arange(0,24),rescaled)\nplt.show()","070437af":"#plotactual\nsns.set_style(\"darkgrid\")\n\nplt.plot(np.arange(0,24),rescaled_test)\nplt.show()\n","4121d624":"sns.set_style(\"darkgrid\")\n\n#original-blue\nplt.plot(np.arange(0,24),rescaled_test)\n#predicted-red\nplt.plot(np.arange(0,24),rescaled,color=\"red\")\n\n\nplt.show()","51c6411f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session"}}