{"cell_type":{"9e44eaf2":"code","016fdeb1":"code","fa957641":"code","d0c9b085":"code","27e806d7":"code","68f13a1d":"code","08110ab9":"code","eb6e6ffc":"code","8befbc16":"code","ac29907a":"code","5c898360":"code","98821b35":"code","00d5ea4a":"code","d08b44cb":"code","ee060135":"code","d0eba394":"code","ae870408":"code","79b8898d":"code","d532cbe7":"code","dffb0c12":"code","984451cd":"markdown","5d660f3e":"markdown","31c7bc9d":"markdown","7bdefdaf":"markdown","14521898":"markdown","e6fbe93a":"markdown","c3b560a6":"markdown","096bd9f9":"markdown","3e99fe81":"markdown","8567c1f5":"markdown","179f59cd":"markdown","c2fbdb4e":"markdown","69253953":"markdown","181b7964":"markdown"},"source":{"9e44eaf2":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style('whitegrid')\n%matplotlib inline\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"..\/input\"]).decode(\"utf8\"))","016fdeb1":"df = pd.read_csv('..\/input\/Seed_Data.csv')\ndf.head()","fa957641":"print('Numbers of rows {} and number of columns {} '.format(df.shape[0], df.shape[1]))\nprint('\\n')\ndf.info()","d0c9b085":"df.describe()","27e806d7":"import warnings\nwarnings.filterwarnings(\"ignore\")\n\nsns.set(style=\"darkgrid\")\nsns.lmplot('A','C',data=df, hue='target',\n           palette='Set1',size=7,aspect=1.2,fit_reg=False);","68f13a1d":"sns.lmplot('A','A_Coef',data=df, hue='target',\n           palette='Set1',size=7,aspect=1.2,fit_reg=False);","08110ab9":"g = sns.FacetGrid(data = df, hue='target', palette='Set2', size=7, aspect=3)\ng = g.map(plt.hist,'A',bins=22,alpha=0.6)\nplt.legend();","eb6e6ffc":"from sklearn.cluster import KMeans\nkmeans = KMeans(n_clusters=3)","8befbc16":"kmeans.fit(df.drop('target',axis=1))","ac29907a":"centers = kmeans.cluster_centers_\ncenters","5c898360":"df['klabels'] = kmeans.labels_\ndf.head()","98821b35":"f, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, sharey=True,figsize = (12,8) )\n\n# For fitted with kmeans \nax1.set_title('K Means (K = 3)')\nax1.scatter(x = df['A'], y = df['A_Coef'], \n            c = df['klabels'], cmap='rainbow')\nax1.scatter(x=centers[:, 0], y=centers[:, 5],\n            c='black',s=300, alpha=0.5);\n\n# For original data \nax2.set_title(\"Original\")\nax2.scatter(x = df['A'], y = df['A_Coef'], \n            c = df['target'], cmap='rainbow')","00d5ea4a":"from scipy.cluster.hierarchy import dendrogram, linkage\nfrom sklearn.cluster import AgglomerativeClustering\nfrom sklearn.preprocessing import StandardScaler","d08b44cb":"X = df.iloc[:, [0,1,2,3,4,5,6]].values\n\nscaler = StandardScaler()\nX = scaler.fit_transform(X)","ee060135":"hc = AgglomerativeClustering(n_clusters= 3, affinity= 'euclidean', linkage= 'ward')\nprevisoes = hc.fit_predict(X)","d0eba394":"fig = plt.figure(figsize=(12,9))\nfig = dendograma = dendrogram(linkage(previsoes, method= 'ward'), color_threshold=1, show_leaf_counts=True,\n                             truncate_mode='lastp')","ae870408":"df.klabels.value_counts()","79b8898d":"df.target.value_counts()","d532cbe7":"sum_square = {}\n\n# Let's test for K from 1 to 10\nfor k in range(1, 10):\n    kmeans = KMeans(n_clusters=k).fit(df.drop('target',axis=1))\n    \n    sum_square[k] = kmeans.inertia_ ","dffb0c12":"plt.plot(list(sum_square.keys()), list(sum_square.values()),\n         linestyle ='-', marker = 'H', color = 'g',\n         markersize = 8,markerfacecolor = 'b');","984451cd":"We have below our two plots, the left being the clusters we generate through our Kmeans model and on the right we have the correct labels that came from Dataset.","5d660f3e":"**Reading file `Seed_Data.csv` and show the head of the file.**","31c7bc9d":"K-Means is probably the most well know clustering algorithm. It\u2019s taught in a lot of introductory data science and machine learning classes. It\u2019s easy to understand and implement in code! Check out the graphic below for an illustration.","7bdefdaf":"#####  Let's use the info () function to get a broader view of Dataset","14521898":"## KMeans Clustering\n\nTime for machine learning using KMeans clustering unsupervised algorithm.<br>\n\nClustering is a Machine Learning technique that involves the grouping of data points. Given a set of data points, we can use a clustering algorithm to classify each data point into a specific group. In theory, data points that are in the same group should have similar properties and\/or features, while data points in different groups should have highly dissimilar properties and\/or features. Clustering is a method of unsupervised learning and is a common technique for statistical data analysis used in many fields.\n\n\n##### Set 3 clusters","e6fbe93a":"Here we will generate a histogram to visualize the data by class","c3b560a6":"K-Means has the advantage that it\u2019s pretty fast, as all we\u2019re really doing is computing the distances between points and group centers; very few computations!","096bd9f9":"**Fitting the model to all the data except for the `'target'`.**\n* We can do do this using drop()","3e99fe81":"### Lets display the basic statistics, mean, std, max etc....\n* Generates descriptive statistics that summarize the central tendency, dispersion and shape of a dataset\u2019s distribution, excluding NaN values.","8567c1f5":"**Let's see, how area 'A' is related to the A_Coef using scatter plot.** Hint: `hue = target`","179f59cd":"## Exploratory Data Analysis\n\nLet's do some EDA here, always good to know our data!\n\n**How the area 'A' is related to the compactness 'C'. \n#### Luckily, we have the target values in column Target","c2fbdb4e":"## Elbow point \n**Estimate the elbow point to see if our selection for K was right!**","69253953":"\n##### Let's add a new column called klabels that contemplated our predictions with the algorithm Kmeans","181b7964":"# KMean Clustering\n## Varieties of the wheat seed dataset\nThis is a real dataset which provides **measurements of the geometrical properties of kernels belonging to three different varieties of the wheat**. A soft X-ray technique and GRAINS package were used to construct all seven, real-valued attributes. Original dataset is available at UCI Machine Learning Repository [Seed dataset](https:\/\/archive.ics.uci.edu\/ml\/datasets\/seeds). You can download the file and use it. <br>\nHowever, I recommend using the file \"**Seed_Data.csv**\".\nThe file is processed for columns names, separators (longer than 1 characters and also of different form), while reading. The datafile contain following 7 features and 1 target class. \n\nFeatures are:\n* A: Area \n* P: Perimeter  \n* C: Compactness {C = 4*pi*A\/P^2} \n* LK: Length of Kernel \n* WK: Width of Kernel\n* A_Coef: Asymmetry Coefficient \n* LKG: Length of Kernel Groove<br>\n\nTarget Class is:\n* target: target class (0, 1, 2)"}}