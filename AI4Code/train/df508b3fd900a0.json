{"cell_type":{"e458b627":"code","96dcdb93":"code","a46a536b":"code","f73e2193":"code","d172f9fe":"code","d395e630":"code","c18756c3":"code","ef3b14a2":"code","f06ece70":"code","e0f54446":"code","ab4ff338":"code","5874f186":"code","8eceed50":"code","c64264ac":"code","1b3441d2":"code","307d9da4":"code","578c0492":"code","f89efaba":"code","feaff2c6":"code","afc81987":"markdown","89f4814d":"markdown","c0b1952a":"markdown","f11a65cb":"markdown","f8015925":"markdown","ace54165":"markdown","fb359dd7":"markdown","5f40904a":"markdown","05d997d3":"markdown","8dc9f09c":"markdown","656c6a8d":"markdown","d775c6b4":"markdown","31a41075":"markdown","267ee86e":"markdown","9e8518d6":"markdown","7a4bbccd":"markdown","eb059415":"markdown","ae453e80":"markdown","3825c295":"markdown","f9d151a0":"markdown","17971e10":"markdown","5ba10906":"markdown","5a4ae33f":"markdown","777ce65f":"markdown"},"source":{"e458b627":"import tensorflow as tf \nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\ntf.random.set_seed(1)\nEPOCHS = 40\nLR = 0.0001\nOPT = tf.keras.optimizers.SGD(LR , 0.99)\nplt.style.use('fivethirtyeight')\nplt.rcParams[\"figure.figsize\"] = (18,6)","96dcdb93":"(x_train , y_train) , (x_test , y_test ) = tf.keras.datasets.mnist.load_data()\nx_train = x_train \/255 \nx_test = x_test\/255","a46a536b":"model = tf.keras.Sequential([tf.keras.layers.Flatten(input_shape = x_train.shape[1:]),\n                            tf.keras.layers.Dense(10,activation = \"softmax\")])\nmodel.compile(optimizer=OPT,\n              loss = \"sparse_categorical_crossentropy\",\n              metrics = [\"accuracy\"])\nmodel.summary()","f73e2193":"(x_train_partial , y_train_partial) = (x_train[:30000] ,  y_train[:30000])","d172f9fe":"partial_data = model.fit(x_train_partial , y_train_partial, \n                    validation_data=(x_test , y_test),\n                    epochs= EPOCHS ,\n                    verbose = 0 )","d395e630":"model = tf.keras.Sequential([tf.keras.layers.Flatten(input_shape = x_train.shape[1:]),\n                            tf.keras.layers.Dense(10,activation = \"softmax\")])\nmodel.compile(optimizer=OPT,\n              loss = \"sparse_categorical_crossentropy\",\n              metrics = [\"accuracy\"])\nfull_data = model.fit(x_train , y_train, \n                    validation_data=(x_test , y_test),\n                    epochs=EPOCHS ,\n                    verbose = 0 )","c18756c3":"fig , ax = plt.subplots(nrows = 1 , ncols =  2)\nax[0].plot(partial_data.history['val_accuracy'], label='Partial-Data'  )\nax[0].plot(full_data.history['val_accuracy'], label='Full-Data' , )\nax[0].set_xlabel(\"No. of iterations\")\nax[0].set_ylabel(\"Validation Accuracy\")\nax[0].legend(loc = 'lower right')\nax[1].plot(partial_data.history['val_loss'], label='Partial-Data' , )\nax[1].plot(full_data.history['val_loss'], label='Full-Data' , )\nax[1].set_xlabel(\"No. of iterations\")\nax[1].set_ylabel(\"Validation Loss\")\nax[1].legend(loc = 'upper right');","ef3b14a2":"model = tf.keras.Sequential([tf.keras.layers.Flatten(input_shape = x_train.shape[1:]),\n                             tf.keras.layers.Dense(10 , activation = \"relu\"),\n                            tf.keras.layers.Dense(10,activation = \"softmax\")])\nmodel.compile(optimizer=OPT,\n              loss = \"sparse_categorical_crossentropy\",\n              metrics = [\"accuracy\"])\none_added_layers = model.fit(x_train , y_train, \n                    validation_data=(x_test , y_test),\n                    epochs=EPOCHS ,\n                    verbose = 0 )","f06ece70":"model = tf.keras.Sequential([tf.keras.layers.Flatten(input_shape = x_train.shape[1:]),\n                             tf.keras.layers.Dense(10 , activation = \"relu\"),\n                             tf.keras.layers.Dense(20 , activation = \"relu\"),\n                            tf.keras.layers.Dense(10,activation = \"softmax\")])\nmodel.compile(optimizer=OPT,\n              loss = \"sparse_categorical_crossentropy\",\n              metrics = [\"accuracy\"])\ntwo_added_layers = model.fit(x_train , y_train, \n                    validation_data=(x_test , y_test),\n                    epochs=EPOCHS ,\n                    verbose = 0 )","e0f54446":"model = tf.keras.Sequential([tf.keras.layers.Flatten(input_shape = x_train.shape[1:]),\n                             tf.keras.layers.Dense(20 , activation = \"relu\"),\n                             tf.keras.layers.Dense(40 , activation = \"relu\"),\n                             tf.keras.layers.Dense(20 , activation = \"relu\"),\n                            tf.keras.layers.Dense(10,activation = \"softmax\")])\nmodel.compile(optimizer=OPT,\n              loss = \"sparse_categorical_crossentropy\",\n              metrics = [\"accuracy\"])\nthree_added_layers =  model.fit(x_train , y_train, \n                    validation_data=(x_test , y_test),\n                    epochs=EPOCHS ,\n                    verbose = 0 )","ab4ff338":"model = tf.keras.Sequential([tf.keras.layers.Flatten(input_shape = x_train.shape[1:]),\n                             tf.keras.layers.Dense(10 , activation = \"relu\"),\n                             tf.keras.layers.Dense(20 , activation = \"relu\"),\n                             tf.keras.layers.Dense(40 , activation = \"relu\"),\n                             tf.keras.layers.Dense(20 , activation = \"relu\"),\n                            tf.keras.layers.Dense(10,activation = \"softmax\")])\nmodel.compile(optimizer=OPT,\n              loss = \"sparse_categorical_crossentropy\",\n              metrics = [\"accuracy\"])\nfive_added_layers = model.fit(x_train , y_train, \n                    validation_data=(x_test , y_test),\n                    epochs=EPOCHS ,\n                    verbose = 0 )","5874f186":"fig , ax = plt.subplots(nrows = 1 , ncols =  2)\nax[0].plot(full_data.history['val_accuracy'], label='Simple Linear Model')\nax[0].plot(one_added_layers.history['val_accuracy'], label='One Hideen Layer')\nax[0].plot(two_added_layers.history['val_accuracy'], label='Two Hidden Layers')\nax[0].plot(three_added_layers.history['val_accuracy'], label='Three Hidden Layers')\nax[0].plot(five_added_layers.history['val_accuracy'], label='Five Hidden Layers')\nax[0].set_xlabel(\"No. of iterations\")\nax[0].set_ylabel(\"Validation Accuracy\")\nax[0].legend(loc = 'lower right')\nax[1].plot(full_data.history['val_loss'], label='Simple Linear Model')\nax[1].plot(one_added_layers.history['val_loss'], label='One Hideen Layer')\nax[1].plot(two_added_layers.history['val_loss'], label='Two Hidden Layers')\nax[1].plot(three_added_layers.history['val_loss'], label='Three Hidden Layers')\nax[1].plot(five_added_layers.history['val_loss'], label='Five Hidden Layers')\nax[1].set_xlabel(\"No. of iterations\")\nax[1].set_ylabel(\"Validation Loss\")\nax[1].legend(loc = 'upper right');","8eceed50":"model = tf.keras.Sequential([tf.keras.layers.Flatten(input_shape = x_train.shape[1:]),\n                             tf.keras.layers.Dense(80,activation = \"relu\"),\n                             tf.keras.layers.Dense(40,activation = \"relu\"),\n                             tf.keras.layers.Dense(20,activation = \"relu\"),\n                            tf.keras.layers.Dense(10,activation = \"softmax\")])\nmodel.compile(optimizer=OPT,\n              loss = \"sparse_categorical_crossentropy\",\n              metrics = [\"accuracy\"])\nsmall_units = model.fit(x_train , y_train, \n                    validation_data=(x_test , y_test),\n                    epochs=EPOCHS ,\n                    verbose = 0 )","c64264ac":"model = tf.keras.Sequential([tf.keras.layers.Flatten(input_shape = x_train.shape[1:]),\n                             tf.keras.layers.Dense(512,activation = \"relu\"),\n                             tf.keras.layers.Dense(128,activation = \"relu\"),\n                             tf.keras.layers.Dense(64,activation = \"relu\"),\n                            tf.keras.layers.Dense(10,activation = \"softmax\")])\nmodel.compile(optimizer=OPT,\n              loss = \"sparse_categorical_crossentropy\",\n              metrics = [\"accuracy\"])\nlarge_units = model.fit(x_train , y_train, \n                    validation_data=(x_test , y_test),\n                    epochs=EPOCHS ,\n                    verbose = 0 )","1b3441d2":"fig , ax = plt.subplots(nrows = 1 , ncols =  2)\nax[0].plot(full_data.history['val_accuracy'], label='Simple Linear Model')\nax[0].plot(small_units.history['val_accuracy'], label='Smaller Units')\nax[0].plot(large_units.history['val_accuracy'], label='Larger Units')\nax[0].set_xlabel(\"No. of iterations\")\nax[0].set_ylabel(\"Validation Accuracy\")\nax[0].legend(loc = 'lower right')\nax[1].plot(full_data.history['val_loss'], label='Simple Linear Model')\nax[1].plot(small_units.history['val_loss'], label='Smaller Units')\nax[1].plot(large_units.history['val_loss'], label='Larger Units')\nax[1].set_xlabel(\"No. of iterations\")\nax[1].set_ylabel(\"Validation Loss\")\nax[1].legend(loc = 'upper right');","307d9da4":"model = tf.keras.Sequential([tf.keras.layers.Flatten(input_shape = x_train.shape[1:]),\n                             tf.keras.layers.Dense(512,activation = \"relu\"),\n                             tf.keras.layers.BatchNormalization(),\n                             tf.keras.layers.Dense(128,activation = \"relu\"),\n                             tf.keras.layers.BatchNormalization(),\n                             tf.keras.layers.Dense(64,activation = \"relu\"),\n                            tf.keras.layers.Dense(10,activation = \"softmax\")])\nmodel.compile(optimizer=OPT,\n              loss = \"sparse_categorical_crossentropy\",\n              metrics = [\"accuracy\"])\nbn = model.fit(x_train , y_train, \n                    validation_data=(x_test , y_test),\n                    epochs=EPOCHS ,\n                    verbose = 0 )","578c0492":"fig , ax = plt.subplots(nrows = 1 , ncols =  2)\nax[0].plot(large_units.history['val_accuracy'], label='Best_model')\nax[0].plot(bn.history['val_accuracy'], label='Batch Normalization')\nax[0].set_xlabel(\"No. of iterations\")\nax[0].set_ylabel(\"Validation Accuracy\")\nax[0].legend(loc = 'lower right')\nax[1].plot(large_units.history['val_loss'], label='Best_model')\nax[1].plot(bn.history['val_loss'], label='Batch Normalization')\nax[1].set_xlabel(\"No. of iterations\")\nax[1].set_ylabel(\"Validation Loss\")\nax[1].legend(loc = 'upper right');","f89efaba":"model = tf.keras.Sequential([tf.keras.layers.Flatten(input_shape = x_train.shape[1:]),\n                             tf.keras.layers.Dense(512,activation = \"relu\"),\n                             tf.keras.layers.Dropout(0.3),\n                             tf.keras.layers.Dense(128,activation = \"relu\"),\n                             tf.keras.layers.Dropout(0.2),\n                             tf.keras.layers.Dense(64,activation = \"relu\"),\n                            tf.keras.layers.Dense(10,activation = \"softmax\")])\nmodel.compile(optimizer=OPT,\n              loss = \"sparse_categorical_crossentropy\",\n              metrics = [\"accuracy\"])\ndropout = model.fit(x_train , y_train, \n                    validation_data=(x_test , y_test),\n                    epochs=EPOCHS ,\n                    verbose = 0 )","feaff2c6":"fig , ax = plt.subplots(nrows = 1 , ncols =  2)\nax[0].plot(large_units.history['val_accuracy'], label='Best Model')\nax[0].plot(dropout.history['val_accuracy'], label='Dropout')\nax[0].set_xlabel(\"No. of iterations\")\nax[0].set_ylabel(\"Validation Accuracy\")\nax[0].legend(loc = 'lower right')\nax[1].plot(large_units.history['val_loss'], label='Best Model')\nax[1].plot(dropout.history['val_loss'], label='Dropout')\nax[1].set_xlabel(\"No. of iterations\")\nax[1].set_ylabel(\"Validation Loss\")\nax[1].legend(loc = 'upper right');","afc81987":"### Adding two extra hidden layers","89f4814d":"### Training on 50% of the total training data","c0b1952a":"### Adding three extra hidden layers","f11a65cb":"We will add Batch Norm Layers to our last best model to check its effect on training accuracy.","f8015925":"### Adding five extra hidden layers","ace54165":"# <center> FIXING HIGH VARIANCE IN NEURAL NETWORKS<\/center>","fb359dd7":"### [1. Effect of increasing  data](#data) ###\n### [2. Effect of increasing hidden layers](#layers) ###\n### [3. Effect of nodes](#nodes) ###\n### [4. Effect of Batch Normalization](#batch) ###\n### [5. Effect of Dropouts](#dropout) ###\n\n##    [Conclusions](#conclusion) ##","5f40904a":"### Adding one extra hidden layer","05d997d3":"# CONCLUSION\n<a id=\"conclusion\"><\/a>","8dc9f09c":"### Training on total training data","656c6a8d":"<a id=\"nodes\"><\/a>\n# EFFECT OF UNITS(NODES) IN LAYERS","d775c6b4":"Adding large units to a 3 hidden-layers model","31a41075":"# EFFECT OF BATCH NORMALIZATION \n<a id=\"batch\"><\/a>","267ee86e":"### MODEL DESIGN ","9e8518d6":"**Created by Sanskar Hasija**\n\n**FIXING HIGH VARIANCE ( OVERFITTING ) IN NUERAL NETWORKS**\n\n**14 August 2021**\n","7a4bbccd":"After training the same data on multiple models with different hyperparameters, we can conclude that the following changes\/updates can help us in fixing high variance:\n\n* Increasing the amount of training data.\n* Increasing the number of hidden layers.\n* Increasing the number of hidden units.\n* Adding Batch Normalization.\n* Adding Dropouts.\n* Training for a higher number of epochs.\n* Trying more neural networks.\n\nFor a more detailed analysis of every factor, check out my blog on Fixing Overfitting [here](https:\/\/medium.com\/mlearning-ai\/hyperparameter-tuning-fixing-overfitting-in-neural-networks-b983b21d60bd).","eb059415":"# EFFECT OF DROPOUTS:\n<a id=\"dropout\"><\/a>","ae453e80":"## LOADING DATA","3825c295":"<a id=\"layers\"><\/a>\n# EFFECT OF INCREASING HIDDEN LAYERS","f9d151a0":"## IMPORTS","17971e10":"In the next cell, the same model is created once more as a way to reset the weights from the previous trained data.","5ba10906":"We will first start with a simple linear model with 1 input node and 10 ouptut nodes with softmax activation.","5a4ae33f":"Adding small units to a 3 hidden-layers model","777ce65f":"<a id=\"data\"><\/a>\n# EFFECT OF INCREASING DATA"}}