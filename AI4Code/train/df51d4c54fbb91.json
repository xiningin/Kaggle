{"cell_type":{"3bca8004":"code","2cb94e18":"code","f13451d2":"code","a2eefbec":"code","47f5882e":"code","ab8d9327":"code","29f6796d":"code","a257819e":"code","f4e081a9":"code","279aa2f2":"code","d70b1510":"code","f352e550":"code","c3820763":"code","297f81fd":"code","c5f8847b":"code","23a93652":"code","57a7f452":"code","f2adc5d9":"code","02156192":"code","d81ad0dd":"code","fdf9200a":"code","d038f918":"code","9c51f715":"code","311c7c6f":"code","685dcb4c":"code","6ef4b767":"code","e668d6b6":"code","8e908125":"code","b96c0c91":"code","48119805":"code","8f030694":"code","d6e04e61":"code","53929289":"code","e224cd28":"code","da03d187":"code","84be24cd":"code","583f7bf2":"code","72bd7902":"code","041f722e":"code","21e05b1a":"code","1593b887":"code","503cc3a9":"code","020dd701":"code","78fcabe7":"code","34ef06cb":"markdown","68a214a4":"markdown","daa61e13":"markdown","0526fc8c":"markdown","9795dfe3":"markdown","7a8e6926":"markdown","8b521455":"markdown","d6d14d73":"markdown","344a31ca":"markdown","f9c00be8":"markdown","71b34d50":"markdown","62fa98c3":"markdown"},"source":{"3bca8004":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n","2cb94e18":"from sklearn.model_selection import train_test_split\nimport keras\nfrom keras.optimizers import RMSprop\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.layers import LeakyReLU\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.callbacks import LearningRateScheduler\nfrom keras.callbacks import EarlyStopping\n\nimport matplotlib.pyplot as plt ","f13451d2":"train = pd.read_csv('\/kaggle\/input\/Kannada-MNIST\/train.csv')\ntest = pd.read_csv(\"\/kaggle\/input\/Kannada-MNIST\/test.csv\")\nDig_MNIST = pd.read_csv(\"\/kaggle\/input\/Kannada-MNIST\/Dig-MNIST.csv\")","a2eefbec":"sample_sub = pd.read_csv(\"\/kaggle\/input\/Kannada-MNIST\/sample_submission.csv\")","47f5882e":"print(\"Train set shape = \" +str(train.shape))\nprint(\"Test set shape = \" +str(test.shape))\nprint(\"Dif set shape = \" +str(Dig_MNIST.shape))","ab8d9327":"train.head()","29f6796d":"X=train.iloc[:,1:].values \nY=train.iloc[:,0].values \nY[:10]","a257819e":"X = X.reshape(X.shape[0], 28, 28,1) \nprint(X.shape)\n","f4e081a9":"Y = keras.utils.to_categorical(Y, 10) \nprint(Y.shape)","279aa2f2":"test.head()","d70b1510":"x_test=test.drop('id', axis=1).iloc[:,:].values\nx_test = x_test.reshape(x_test.shape[0], 28, 28,1)\nx_test.shape","f352e550":"Dig_MNIST.head(20)","c3820763":"X_dig=Dig_MNIST.iloc[:,1:].values \nY_dig=Dig_MNIST.iloc[:,0].values \nX_dig = X_dig.reshape(X_dig.shape[0], 28, 28,1)\nY_dig = keras.utils.to_categorical(Y_dig, 10) \nprint(X_dig.shape,Y_dig.shape)","297f81fd":"X_train, X_valid, Y_train, Y_valid = train_test_split(X, Y, test_size = 0.10, random_state=42) ","c5f8847b":"train_datagen = ImageDataGenerator(rescale = 1.\/255.,\n                                   rotation_range = 10,\n                                   width_shift_range = 0.25,\n                                   height_shift_range = 0.25,\n                                   shear_range = 0.1,\n                                   zoom_range = 0.25,\n                                   horizontal_flip = False,\n                                   vertical_flip = False)","23a93652":"valid_datagen = ImageDataGenerator(rescale=1.\/255.) ","57a7f452":"import keras\nfrom keras.models import Model\nfrom keras.layers import Input, Dense, Flatten, Conv2D, MaxPooling2D, Dropout","f2adc5d9":"a = Input(shape=(28,28,1,))\nf = Flatten()(a)\nb = Dense(128, activation=\"relu\")(f)\nb = Dense(10, activation=\"softmax\")(b)\nsimple_model = Model(inputs=a, outputs=b)","02156192":"simple_model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])","d81ad0dd":"batch_size = 16\nhistory = simple_model.fit_generator(train_datagen.flow(X_train, Y_train, batch_size=batch_size), validation_data=valid_datagen.flow(X_valid, Y_valid, batch_size=batch_size),  epochs=1)","fdf9200a":"evalu = simple_model.evaluate_generator(valid_datagen.flow(X_dig, Y_dig, batch_size=batch_size))\nprint(\"loss : \" + str(evalu[0]))\nprint(\"accuracy : \" + str(evalu[1]))","d038f918":"accuracy = history.history['accuracy']\nval_accuracy = history.history['val_accuracy']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\nepochs = range(len(accuracy))\nplt.plot(epochs, accuracy, 'b', label='Training accuracy')\nplt.plot(epochs, val_accuracy, 'r', label='Test accuracy')\nplt.title('Accuracy')\nplt.legend()\nplt.show()\nplt.figure()\nplt.plot(epochs, loss, 'b', label='Training loss')\nplt.plot(epochs, val_loss, 'r', label='Test loss')\nplt.title('Loss')\nplt.legend()\nplt.show()","9c51f715":"a = Input(shape=(28,28,1,))\nc = Conv2D(128, (3,3), activation=\"relu\")(a)\nc = Conv2D(128, (3,3), activation=\"relu\")(c)\nc = Conv2D(128, (3,3), activation=\"relu\")(c)\nf = Flatten()(c)\nb = Dense(128, activation=\"relu\")(f)\nb = Dense(10, activation=\"softmax\")(b)\nconv_model = Model(inputs=a, outputs=b)","311c7c6f":"conv_model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])","685dcb4c":"batch_size = 16\nhistory = conv_model.fit_generator(train_datagen.flow(X_train, Y_train, batch_size=batch_size), validation_data=valid_datagen.flow(X_valid, Y_valid, batch_size=batch_size),  epochs=1)","6ef4b767":"evalu = conv_model.evaluate_generator(valid_datagen.flow(X_dig, Y_dig, batch_size=batch_size))\nprint(\"loss : \" + str(evalu[0]))\nprint(\"accuracy : \" + str(evalu[1]))","e668d6b6":"accuracy = history.history['accuracy']\nval_accuracy = history.history['val_accuracy']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\nepochs = range(len(accuracy))\nplt.plot(epochs, accuracy, 'b', label='Training accuracy')\nplt.plot(epochs, val_accuracy, 'r', label='Test accuracy')\nplt.title('Accuracy')\nplt.legend()\nplt.show()\nplt.figure()\nplt.plot(epochs, loss, 'b', label='Training loss')\nplt.plot(epochs, val_loss, 'r', label='Test loss')\nplt.title('Loss')\nplt.legend()\nplt.show()","8e908125":"a = Input(shape=(28,28,1,))\nc = Conv2D(128, (3,3), activation=\"relu\", padding='same', bias=False)(a)\nc = MaxPooling2D()(c)\nc = BatchNormalization()(c)\nc = Conv2D(128, (3,3), activation=\"relu\", padding='same', bias=False)(c)\nc = MaxPooling2D()(c)\nc = BatchNormalization()(c)\nc = Conv2D(128, (3,3), activation=\"relu\", padding='same', bias=False)(c)\nc = MaxPooling2D()(c)\nc = BatchNormalization()(c)\nf = Flatten()(c)\nb = Dropout(0.5)(f)\nb = Dense(128, activation=\"relu\", bias=False)(b)\nb = BatchNormalization()(b)\nb = Dropout(0.5)(b)\nb = Dense(128, activation=\"relu\", bias=False)(b)\nb = BatchNormalization()(b)\nb = Dense(10, activation=\"softmax\")(b)\nhard_model = Model(inputs=a, outputs=b)","b96c0c91":"hard_model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])","48119805":"batch_size = 16\nhistory = hard_model.fit_generator(train_datagen.flow(X, Y, batch_size=batch_size), validation_data=valid_datagen.flow(X_dig, Y_dig, batch_size=batch_size),  epochs=1)","8f030694":"evalu = hard_model.evaluate_generator(valid_datagen.flow(X_dig, Y_dig))\nprint(\"loss : \" + str(evalu[0]))\nprint(\"accuracy : \" + str(evalu[1]))","d6e04e61":"accuracy = history.history['accuracy']\nval_accuracy = history.history['val_accuracy']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\nepochs = range(len(accuracy))\nplt.plot(epochs, accuracy, 'b', label='Training accuracy')\nplt.plot(epochs, val_accuracy, 'r', label='Test accuracy')\nplt.title('Accuracy')\nplt.legend()\nplt.show()\nplt.figure()\nplt.plot(epochs, loss, 'b', label='Training loss')\nplt.plot(epochs, val_loss, 'r', label='Test loss')\nplt.title('Loss')\nplt.legend()\nplt.show()","53929289":"X_merg = np.concatenate((X, X_dig))\nY_merg = np.concatenate((Y, Y_dig))","e224cd28":"X_merg_train, X_merg_valid, Y_merg_train, Y_merg_valid = train_test_split(X_merg, Y_merg, test_size = 0.20, random_state=42) ","da03d187":"a = Input(shape=(28,28,1,))\nc = Conv2D(128, (3,3), activation=\"relu\", padding='same', bias=False)(a)\nc = MaxPooling2D()(c)\nc = BatchNormalization()(c)\nc = Conv2D(128, (3,3), activation=\"relu\", padding='same', bias=False)(c)\nc = MaxPooling2D()(c)\nc = BatchNormalization()(c)\nc = Conv2D(128, (3,3), activation=\"relu\", padding='same', bias=False)(c)\nc = MaxPooling2D()(c)\nc = BatchNormalization()(c)\nf = Flatten()(c)\nb = Dropout(0.5)(f)\nb = Dense(128, activation=\"relu\", bias=False)(b)\nb = BatchNormalization()(b)\nb = Dropout(0.5)(b)\nb = Dense(128, activation=\"relu\", bias=False)(b)\nb = BatchNormalization()(b)\nb = Dense(10, activation=\"softmax\")(b)\nhard_model = Model(inputs=a, outputs=b)","84be24cd":"hard_model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])","583f7bf2":"batch_size = 16\nhistory = hard_model.fit_generator(train_datagen.flow(X_merg_train, Y_merg_train, batch_size=batch_size), validation_data=valid_datagen.flow(X_merg_valid, Y_merg_valid, batch_size=batch_size),  epochs=12)","72bd7902":"evalu = hard_model.evaluate_generator(valid_datagen.flow(X_dig, Y_dig))\nprint(\"loss : \" + str(evalu[0]))\nprint(\"accuracy : \" + str(evalu[1]))","041f722e":"accuracy = history.history['accuracy']\nval_accuracy = history.history['val_accuracy']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\nepochs = range(len(accuracy))\nplt.plot(epochs, accuracy, 'b', label='Training accuracy')\nplt.plot(epochs, val_accuracy, 'r', label='Test accuracy')\nplt.title('Accuracy')\nplt.legend()\nplt.show()\nplt.figure()\nplt.plot(epochs, loss, 'b', label='Training loss')\nplt.plot(epochs, val_loss, 'r', label='Test loss')\nplt.title('Loss')\nplt.legend()\nplt.show()","21e05b1a":"predictions = hard_model.predict(x_test\/255.)","1593b887":"submission = pd.read_csv('..\/input\/Kannada-MNIST\/sample_submission.csv')","503cc3a9":"submission['label'] = np.argmax(predictions, axis = 1)","020dd701":"submission.head()","78fcabe7":"submission.to_csv(\"submission.csv\",index=False)","34ef06cb":"# Submite","68a214a4":"## Convolution\n","daa61e13":"# trainning part","0526fc8c":"Now we must reshape the date to make it Keras friendly.","9795dfe3":"We slice the dataframes to define the features and the labels","7a8e6926":"We use Keras ImageDataGenerator to artificially increase our training set.","8b521455":"## Hard model","d6d14d73":"We split the data into training and validation set.","344a31ca":"## Hardmodel merge dig+train","f9c00be8":"## Simple","71b34d50":"Now we convert the labels to categorical.","62fa98c3":"Let's load the data."}}