{"cell_type":{"735e7525":"code","899ec43d":"code","ed2e74aa":"code","3ae81db0":"code","520a3003":"code","95dc8e86":"code","98fbf5bc":"code","472d9efb":"code","46636dab":"code","49c74c75":"code","6964166e":"code","d98a7b76":"code","91c11382":"code","0b274d7b":"code","3784b180":"code","e5224d63":"code","68b39009":"code","a1dbbea6":"code","2f5ebf63":"code","5dcfe39d":"code","a4e42063":"markdown","6a73883e":"markdown","ea973111":"markdown","199e9425":"markdown","6d2f3b76":"markdown","50772726":"markdown","0614fc0c":"markdown","7977414f":"markdown","ae79cffe":"markdown","65700218":"markdown","4a2bcd7b":"markdown","2d15af7f":"markdown","94741d15":"markdown","09ad4100":"markdown","61782ea5":"markdown","f6c68648":"markdown","c03d1c45":"markdown","bb973c91":"markdown","85663f1a":"markdown","47c849c2":"markdown","dd8ad6e9":"markdown","aba293fc":"markdown","d4b225e2":"markdown"},"source":{"735e7525":"import numpy as np\nimport pandas as pd\npd.set_option('display.max_colwidth', -1)\n\nimport plotly\nimport plotly.offline as pyo\nimport plotly.graph_objects as go\n\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\n\n\nimport re\nimport glob\nimport json\nimport scipy.sparse\nimport pickle\nfrom pprint import pprint\n\n\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n\n# Gensim\nimport gensim\nimport gensim.corpora as corpora\nfrom gensim.utils import simple_preprocess\nfrom gensim.models import CoherenceModel\nfrom gensim import matutils, models\n\n\n# spacy for lemmatization\nimport spacy\n\n# Plotting tools\nimport pyLDAvis\nimport pyLDAvis.gensim  # don't skip this\nimport matplotlib.pyplot as plt\nfrom IPython.display import IFrame, HTML\n%matplotlib inline\n\n# Enable logging for gensim - optional\nimport logging\n\nimport warnings\nwarnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n","899ec43d":"include_terms = ['sars-cov-2', 'covid-19', '2019-ncov', 'ncov-2019', 'betacoronavirus', 'pandemic', 'wuhan', 'virulence', \n                 'incidence', 'prevalence', 'mortality', 'self-resolve', 'immunity', 'community', 'spread' 'olfactory', \n                 'dysfunction', 'osmia', 'respiratory', 'pneumonia']","ed2e74aa":"# load the data\n\nmetadata_path = '..\/input\/CORD-19-research-challenge\/metadata.csv'\nmeta_df = pd.read_csv(metadata_path, dtype={\n    'pubmed_id': str,\n    'Microsoft Academic Paper ID': str, \n    'doi': str\n})\n\n# get only the abstracts from the dataframe\n\nabstracts = dict(title = [], abstract = [])\n\nfor ind, row in meta_df.iterrows():\n    if pd.notna(row['abstract']) and pd.notna(row['title']):\n        short = row['abstract'][0:250]\n        t = row['title'][0:100]\n        abstracts['title'].append(t)\n        abstracts['abstract'].append(short)","3ae81db0":"pd.DataFrame(abstracts).rename(columns={'title':'Title', 'abstract':'Abstract'}).head()","520a3003":"def lower(data):\n    data = data.lower()\n    return data\n\n\ndef remove_punctuation(data):\n    symbols = \"!\\\"#$%&()*+.\/:;<=>?@[\\]^_`{|}~\\n\"\n    for i in symbols:\n        data = data.replace(i, \" \")\n    #replace the comma seperately\n    data = data.replace(',', ' ')\n    return data\n\ndef remove_apostraphe(data):\n    data = data.replace(\"'\", \"\")\n    return data\n\ndef remove_numbers(data):\n    nums = \"0123456789\"\n    for i in nums:\n        data = data.replace(i, \"\")\n        return data\n\ndef remove_stop_words(data):\n    stop_words = stopwords.words('english')\n    words = word_tokenize(str(data))\n    new_text = \"\"\n    for w in words:\n        if w not in stop_words and len(w) > 1:\n            new_text = new_text + \" \" + w\n    return new_text","95dc8e86":"# clean and replace with cleaned words\ntf_dict = {}\nidf_count = {}\nfor i, ab in meta_df['abstract'].items():\n    if pd.notna(ab):\n        sent_dict = {}\n        working = ab\n        working = lower(working)\n        working = remove_stop_words(working)\n        working = remove_punctuation(working)\n        working = remove_numbers(working)\n        working = remove_apostraphe(working)\n        working = remove_stop_words(working)\n        tokens = working.split(' ')\n    \n        for word in tokens:\n            for inc in include_terms:\n                if inc in word:\n                    if word in sent_dict.keys():\n                        sent_dict[word] = sent_dict[word]+1\n                    else:\n                        sent_dict[word] = 1\n                        if word in idf_count.keys():\n                            idf_count[word] = idf_count[word]+1\n                        else:\n                            idf_count[word] = 1\n        tf_dict[i] = sent_dict","98fbf5bc":"# calculate the tf-idf matrix\nindex = list(tf_dict.keys())\ncolumns = list(idf_count.keys())\nprint(\"There were {} keywords generated from the Included Terms list\".format(len(columns)))\n\n#create an empty array for the tf-idf matrix\ndata = np.zeros((len(index), len(columns)))\n\n#populate the tf-idf matrix\nfor i, doc in enumerate(index):\n    for j, word in enumerate(columns):\n        if word in tf_dict[doc]:\n            if idf_count[word] !=1:\n                data[i][j] = tf_dict[doc][word] * np.log(len(index)\/idf_count[word])\n            else:\n                data[i][j] = np.nan\n        else:\n            data[i][j] = np.nan\n            \n#turn the tf_idf matrix into a dataframe\ndf_tfidf = pd.DataFrame(data, index = index, columns = columns)","472d9efb":"df_tfidf.iloc[:10, :8]","46636dab":"cs = df_tfidf['community-acquired']\ntop_cs = cs.sort_values(ascending=False)[0:10]\nfinal_cs = meta_df['abstract'].iloc[top_cs.index].tolist()\nfinal_cs_title = meta_df['title'].iloc[top_cs.index].tolist()\nfinal_cs_abstra = []\nfor abstra in final_cs:\n    short = abstra[0:250]\n    final_cs_abstra.append(short)\n    \npd.DataFrame({'Title': final_cs_title, 'Abstract': final_cs_abstra})","49c74c75":"# ingest body text for articles \ndf_covid = pd.read_csv('\/kaggle\/input\/subsetlda2\/subset1000.csv') # Articles combined. \ntext = df_covid.drop([\"paper_id\",\"doi\",\"title_abstract_body\",\"Unnamed: 0\", \"abstract\", \"title\"], axis=1) # drop all columns except body_text\nwords = []\nfor ii in range(0,len(text)):\n    words.append(str(text.iloc[ii]['body_text']).split(\" \"))\n    \n# Build the bigram and trigram models\nbigram = gensim.models.Phrases(words, min_count=5, threshold=100) # higher threshold fewer phrases.\ntrigram = gensim.models.Phrases(bigram[words], threshold=100)  \n\n# Faster way to get a sentence clubbed as a trigram\/bigram\nbigram_mod = gensim.models.phrases.Phraser(bigram)\ntrigram_mod = gensim.models.phrases.Phraser(trigram)","6964166e":"# Define functions for stopwords, bigrams, trigrams and lemmatization\ndef remove_stopwords(texts):\n    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\ndef make_bigrams(texts):\n    return [bigram_mod[doc] for doc in texts]\ndef make_trigrams(texts):\n    return [trigram_mod[bigram_mod[doc]] for doc in texts]\ndef lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n    \"\"\"https:\/\/spacy.io\/api\/annotation\"\"\"\n    texts_out = []\n    for sent in texts:\n        doc = nlp(\" \".join(sent)) \n        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n    return texts_out","d98a7b76":"# NLTK Stop words\nfrom nltk.corpus import stopwords\nstop_words = stopwords.words('english') + stopwords.words('spanish') + stopwords.words('french')\nstop_words.extend(['doi', 'preprint', 'copyright', 'peer', 'reviewed', 'org', 'https', 'et', 'al', 'author', 'figure', 'rights', 'reserved', 'permission', 'used', 'using', 'biorxiv', 'medrxiv', 'license', 'fig', 'fig.', 'al.', 'Elsevier', 'PMC', 'CZI'])\n# Remove Stop Words\ndata_words_nostops = remove_stopwords(words)\n# Form Bigrams\ndata_words_bigrams = make_bigrams(data_words_nostops)\n\n# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\nnlp = spacy.load('en', disable=['parser', 'ner'])\nnlp.max_length = 1900000 # increased for size of body texgt \n# Do lemmatization keeping only noun, adj, vb, adv\ndata_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])","91c11382":"# Create Dictionary\nid2word = corpora.Dictionary(data_lemmatized)\n\n# Create Corpus\ntexts = data_lemmatized\n\n# Term Document Frequency\ncorpus = [id2word.doc2bow(text) for text in texts]","0b274d7b":"# Build LDA model with 20 Topics (Reduced Number of Topics for Kaggle)\n\nlda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n                                           id2word=id2word,\n                                           num_topics=20, \n                                           update_every=1,\n                                           chunksize=8000,\n                                           passes=4,\n                                           iterations=400, \n                                           eval_every=None,\n                                           alpha='auto',\n                                           per_word_topics=True)","3784b180":"# Print the Keyword in the 20 topics\nfrom pprint import pprint\n\npprint(lda_model.print_topics())\ndoc_lda = lda_model[corpus]","e5224d63":"from IPython.display import Image\nImage('\/kaggle\/input\/ldastaticimages\/lda_model-img.jpg', width = 1050)","68b39009":"from IPython.display import Image\nImage(filename='\/kaggle\/input\/staticimages\/coherences_numTopics.jpg', width = 850) ","a1dbbea6":"queries = pd.read_csv('..\/input\/covid19-processed-data\/queries.csv')\n\nqueries.rename(columns={'queries':'Queries'})","2f5ebf63":"use_and_bm25 = pd.read_csv('..\/input\/covid19-processed-data\/use_and_bm25.csv')\ntop5 = use_and_bm25[use_and_bm25['task_id'] == 2].head(5)[['task_text_x','excerpt']]\n\ntop5.rename(columns={'task_text_x':'Query', 'excerpt':'Excerpt'})","5dcfe39d":"question_answer_summary = pd.read_csv('..\/input\/covid19-processed-data\/question_answer_dataframe.csv',\n                                      usecols=['RISK_FACTOR_QUESTION',\n                                               'ANSWER_SUMMARY',\n                                               'TOP_5_ARTICLES'])\n\nqa_summary = question_answer_summary.rename(columns={'RISK_FACTOR_QUESTION':'Query',\n                                        'ANSWER_SUMMARY':'Summary',\n                                        'TOP_5_ARTICLES':'Top 5 Articles'})\nqa_summary[['Query','Summary', \"Top 5 Articles\"]]","a4e42063":"#### Helper Functions to Clean the Abstracts\n\nMany of these helper functions are sourced from articles written in Towards Data Science and various NLP notebooks on Kaggle.","6a73883e":"## Text Summarization\n\nThe goal was to summarize the best articles and provide the user with a concise \u2018answer\u2019 to the task question. \n\nThe top results were consolidated utilizing the BERT Executive Summarizer library (references: https:\/\/pypi.org\/project\/bert-extractive-summarizer\/ and https:\/\/arxiv.org\/abs\/1906.04165). This tool utilizes the HuggingFace Pytorch Transformers library. It works by embedding the sentences, running a clustering algorithm (kmeans), and finding the sentences that are closest to the cluster's centroids.\n\nThree text summarization approaches were evaluated: single sentence, paragraph, and article abstract. Single sentences were the highest scored sentences that most closely matched the question embeddings. A cutoff score was used to limit results. Paragraphs were obtained by using the sentences as well as the two sentences preceding and following it. Abstracts were collected from the articles containing the highest scored sentences. Model parameters were fine-tuned by trial-and-error and inspecting the results. Summarization of sentences resulted in the most concise and consistent answers. \n\n### Example ('risks for neonates, newborns, and pregnant women')\n\n```python \nfrom summarizer import Summarizer\n\ninput_tag_sentences = [\n    \" Therefore, the obstetrical outcomes from pregnant women with SARS-CoV-2 infection appear better than that for pregnant women with SARS. \",    \n    \" Previous studies have shown no evidence of perinatal SARS infection in infants born to mothers who had SARS infection during pregnancy. \",    \n    \" There were no moralities among pregnant women or newborns. \",     \n    \" Thirty-three pregnant women with Covid-19 and 28 newborns were identified. \",\n    \" When pregnant women become infected with viral pneumonia, they are more likely to have complications and progress to severe cases [8] . \",\n    \" Whereas pregnant women infected with coronavirus may have an increased risk of adverse neonatal outcomes, gametes do not transmit COVID-19. \",\n    \" Conclusions: Anaesthesia-related complications occur more frequently in the COVID-19 parturients and their newborns have a high risk of distress. \",\n    \" Besides, the delivery of infected parturients to designated hospitals was to ensure the safety of the pregnant women and their newborns, while preventing and controlling newborns' infection with SARS-Cov-2. \",\n    \" Anaesthesia-related complications occurred more frequently in the COVID-19 parturients, while their newborn born also have a high risk of distress and high admission to NICU. \",\n    \" Pregnant women are susceptible population of SARS-CoV-2 which are more likely to have complications and even progresse to severe illness. \",\n    \" Pregnant women are susceptible population of COVID-19 which are more likely to have complications and even progresse to severe illness. \",\n    \" Another study shows that pregnant women with pneumonia have an increased risk of developing low birth weight infants, preterm births, restricted fetal growth, and 5-minute Apgar score <7 compared to healthy pregnant women [10] . \",\n    \" There is little information about effects of COVID-19 on Pregnant women and newborns as a sensitive population. \",\n    \" When a baby is born vaginally it is exposed to the mother\\'s gut microbiome, therefore if a baby does get infected with coronavirus a few days after birth we currently cannot tell if the baby was infected in the womb or during birth. \",\n    \" It is known that pregnant women are potentially at increased risk of complications from any respiratory disease due to the physiological changes that occur in pregnancy. \",\n    \" There is evidence that the use of corticosteroids during pregnancy increase the risk of preterm birth, low birthweight and preeclampsia 34 . \",\n    \" The COVID-19 infection during pregnancy also increase risks of several adverse outcomes, including higher rates of C-section delivery, low birth weight, and preterm birth. \",\n    \" treatments, and pregnancy outcomes for the women who have been infected with COVID-19 during their pregnancy. \",\n    \" An increasing proportion of the women are now infected with this virus during their pregnancy, which may put them in danger in terms of adverse maternal and newborn outcomes. \",\n    \" Women during pregnancy often face several pregnancy related complications and more susceptible to respiratory pathogens that may put them at higher risk of adverse . \"\n\nmodel = Summarizer()\nresult = model(''.join(input_tag_sentences))\noutput_text_summary = ''.join(result)\n\nprint(output_text_summary)\n    \n\"The obstetrical outcomes from pregnant women with SARS-CoV-2 infection appear better than that for pregnant women with SARS. There were no moralities among pregnant women or newborns. Anaesthesia-related complications occur more frequently in the COVID-19 parturients and their newborns have a high risk of distress. Pregnant women are susceptible population of COVID-19 which are more likely to have complications and even progresse to severe illness. treatments, and pregnancy outcomes for the women who have been infected with COVID-19 during their pregnancy.\"\n```","ea973111":"### Risk Factor Questions and Answers","199e9425":"### LDA\n\nAs one would expect with a medical dataset, there was strong overlap in term frequencies and topics represented in the papers. Using the coherency metric, the first pass of the LDA was executed with the number of topics hyper parameter being 35. This helped parse out individual topics of interest while generating clusters of less than useful topics of interest, such as a cluster of topics relating to papers in a language other than English and another cluster of papers which seem to focus on aircraft and industrial health topics.\n\n![topic_32.png](attachment:topic_32.png)\nIn this image, you can see that the topic cluster is non-English words, indicating the presence of several non-English papers in the dataset.\n\n![topic_34.png](attachment:topic_34.png)\nIn this image, the topic keywords center around aviation and industry, and lack clear topic markers such as \"virus\" or \"viral.\" This was extremely useful in not just understanding the data, but filtering out papers which would just add unnecessary noise to the final models. However, while the presence of common keywords in each topic told us they would likely be relevant to us, the overwhelming presence did mean that some sub-topics were obscured.\n\nBelow are two word clouds for topics 0 and 1, showing that they appear near identical without further refining.\n\n| ![topic_0.png](attachment:topic_0.png) | ![topic_1.png](attachment:topic_1.png)\n|---|---\n\n\nBoth share the stem \"vir\" in several of the keywords. In order to gain more insight into the relevant subtopics, any category term occuring in more than 15 of the 35 topics was removed from the keyword list. Topics 0 and 1 now show more granularity.\n\n| ![topic_0.png](attachment:topic_0.png) | ![topic_1.png](attachment:topic_1.png)\n|---|---\n\nNow it's clear that topic 0 deals with the structure of the virus while topic 1 deals with vaccine and responses, most likely in mice. In the cell below is the code used to generate the LDA_vis and the full visualization of how the topics interconnect and overlap. Wordclouds for each topic with both common removed and common kept can be found in the `img` folder.","6d2f3b76":"### Ranking Articles Based on Their Relevance\n\nTo improve these results, the BM25 ranking algorithm was incorporated. The articles in the subset were ranked based on their relevance to each of the eight queries; this resulted in a numeric score. Additionally, the articles were ranked on their relevance to a list of Covid-19 keywords; this resulted in another numeric score. \n\nBelow is psuedocode on how to use the BM25 ranking algorithm.\n\n```python\nfrom gensim.summarization import bm25\n\nranker = bm25.BM25(articles) # 'articles' is a list of articles tokenized by word \n                             # Example - [['article', 'one'],['article', 'two']]\n    \nscores = ranker.get_scores(query) # 'query' is a search query tokenized by word\n                                  # Example - ['search', 'query']\n    \n# 'scores' will be a n by 1 vector, where n is the number of articles passed into the BM25 constructor.\n```\n\n### Creating an Overall Score\n\nFor each of the nearest neighbor sentences, there were now two numeric scores associated with the article they came from. These two numeric scores were combined with the reciprocal of the distance metric to create an overall relevancy score.\n\n```python\noverall_score = np.log(1 + (1 \/ distance)) * \\\n                np.log(1 + bm25_query_rank) * \\\n                np.log(1 + bm25_covid19_rank)\n```\n\nThe team used this overall score to rank the identified article excerpts based on their relevance to each query.","50772726":"## TODO\nMoving forward with this analysis. We hope to integrate the results of TF-IDF, LDA, and the excerpt extraction more closely with one another. More specifically, TF-IDF and LDA could be used to drive the subsetting of articles into meaningful groups. This would hopefully create more reliable and consistent results in the excerpt extraction and text summarization steps.","0614fc0c":"Ingest combined article body text. Build models for generating and identifying Bigrams and Trigrams ","7977414f":"## Imports and Loads\n\nThis section contains the imports and data loading. Much of the team worked on seperate tasks and to keep the size of the notebook from growing uncontrollably, what is shown here are results with explanations of the process behind the results.","ae79cffe":"#### Model Metrics\nTo save time on running in this notebook, we have uploaded a previous version of the team's LDA model metrics, with the results shown below.","65700218":"#### Search for Abstracts With Keywords\n\nThe team used the keywords of interest to calculate which abstracts had the highest tf-idf scores for those keywords. Most of the keywords used generated non-answers, but community-acquired did give some results, which is shown below.","4a2bcd7b":"## The Right Information at the Right Time to the Right Person\n\nThe team knew that in the two weeks they had to work on this first push, they would not be able to give a perfect machine-driven answer. Instead they aimed for the step prior -- to have the machine show the evidence to support a conclusion or query. In short, the team developed a model to process all the articles and deliver back the most relevant information. This information took the form of an excerpt, which contained a sentence identified as most relevant to a question, and the sentences that surrounded it.\n\n### Subsetting the Articles\n\nTo do this, the team used a process of several steps. First, a subset of papers was chosen. The articles were filtered on whether they contained keywords related to Covid-19 and the task that we chose to focus on, risk factors. \n\nFirst, articles that did not contain any of these keywords were filtered out.\n\n```python\ncovid19_keywords =['sars-cov-2', 'covid-19', '2019-ncov', \n                   'novel-coronavirus',\n                   'coronavirus 2019','wuhan pneumonia',\n                   '2019ncov', 'covid19',\n                   'sarscov2', 'coronavirus-2019']\n```\n\nThen, articles that did not contain any of these keywords were filtered out.\n\n```python\nrisk_keywords =['smoking', 'immunosuppress', 'pulmonary', \n                'pre-existing', 'co-infection', 'neonate',\n               'pregnant', 'socio-economic',\n               'economic', 'reproductive number', 'incubation period',\n               'serial interval', 'transmission', 'hospitalized',\n               'chronic', 'co-morbidity', 'lung', 'respiratory',\n               'high-risk', 'pneumonia', 'gastrointestinal']\n```\n\nThe code used to filter the papers was based on the work done in this notebook https:\/\/www.kaggle.com\/ajrwhite\/covid-19-thematic-tagging-with-regular-expressions\/notebook.\n\n### Finding Articles Relevant to Different Queries\n\nAfter finding this initial subset, work was done to try and identify potential answers to the questions in the risk factor task. The task was split into eight queries, written by the Booz Allen biology SME, and based on the Kaggle task.","2d15af7f":"<h1>Table of Contents<span class=\"tocSkip\"><\/span><\/h1>\n<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Imports-and-Loads\" data-toc-modified-id=\"Imports-and-Loads-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;<\/span>Imports and Loads<\/a><\/span><\/li><li><span><a href=\"#Exploratory-Data-Analysis\" data-toc-modified-id=\"Exploratory-Data-Analysis-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;<\/span>Exploratory Data Analysis<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#TF-IDF-with-Keywords\" data-toc-modified-id=\"TF-IDF-with-Keywords-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;<\/span>TF-IDF with Keywords<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Helper-Functions-to-Clean-the-Abstracts\" data-toc-modified-id=\"Helper-Functions-to-Clean-the-Abstracts-2.1.1\"><span class=\"toc-item-num\">2.1.1&nbsp;&nbsp;<\/span>Helper Functions to Clean the Abstracts<\/a><\/span><\/li><li><span><a href=\"#Clean-and-Create-TFIDF-Matrix\" data-toc-modified-id=\"Clean-and-Create-TFIDF-Matrix-2.1.2\"><span class=\"toc-item-num\">2.1.2&nbsp;&nbsp;<\/span>Clean and Create TFIDF Matrix<\/a><\/span><\/li><li><span><a href=\"#Search-for-Abstracts-With-Keywords\" data-toc-modified-id=\"Search-for-Abstracts-With-Keywords-2.1.3\"><span class=\"toc-item-num\">2.1.3&nbsp;&nbsp;<\/span>Search for Abstracts With Keywords<\/a><\/span><\/li><li><span><a href=\"#Conclusion\" data-toc-modified-id=\"Conclusion-2.1.4\"><span class=\"toc-item-num\">2.1.4&nbsp;&nbsp;<\/span>Conclusion<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#LDA\" data-toc-modified-id=\"LDA-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;<\/span>LDA<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Helper-Functions-and-Classes\" data-toc-modified-id=\"Helper-Functions-and-Classes-2.2.1\"><span class=\"toc-item-num\">2.2.1&nbsp;&nbsp;<\/span>Helper Functions and Classes<\/a><\/span><\/li><li><span><a href=\"#Data-Cleaning-and-Preprocessing\" data-toc-modified-id=\"Data-Cleaning-and-Preprocessing-2.2.2\"><span class=\"toc-item-num\">2.2.2&nbsp;&nbsp;<\/span>Data Cleaning and Preprocessing<\/a><\/span><\/li><li><span><a href=\"#Build-the-Bigrams-and-Trigrams\" data-toc-modified-id=\"Build-the-Bigrams-and-Trigrams-2.2.3\"><span class=\"toc-item-num\">2.2.3&nbsp;&nbsp;<\/span>Build the Bigrams and Trigrams<\/a><\/span><\/li><li><span><a href=\"#Update-Stop-Words-and-Remove\" data-toc-modified-id=\"Update-Stop-Words-and-Remove-2.2.4\"><span class=\"toc-item-num\">2.2.4&nbsp;&nbsp;<\/span>Update Stop Words and Remove<\/a><\/span><\/li><li><span><a href=\"#LDA:-Build-Model\" data-toc-modified-id=\"LDA:-Build-Model-2.2.5\"><span class=\"toc-item-num\">2.2.5&nbsp;&nbsp;<\/span>LDA: Build Model<\/a><\/span><\/li><li><span><a href=\"#Visualize-Results-from-Previous-Run\" data-toc-modified-id=\"Visualize-Results-from-Previous-Run-2.2.6\"><span class=\"toc-item-num\">2.2.6&nbsp;&nbsp;<\/span>Visualize Results from Previous Run<\/a><\/span><\/li><li><span><a href=\"#Model-Metrics\" data-toc-modified-id=\"Model-Metrics-2.2.7\"><span class=\"toc-item-num\">2.2.7&nbsp;&nbsp;<\/span>Model Metrics<\/a><\/span><\/li><\/ul><\/li><\/ul><\/li><li><span><a href=\"#The-Right-Information-at-the-Right-Time-to-the-Right-Person\" data-toc-modified-id=\"The-Right-Information-at-the-Right-Time-to-the-Right-Person-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;<\/span>The Right Information at the Right Time to the Right Person<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Subsetting-the-Articles\" data-toc-modified-id=\"Subsetting-the-Articles-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;<\/span>Subsetting the Articles<\/a><\/span><\/li><li><span><a href=\"#Finding-Articles-Relevant-to-Different-Queries\" data-toc-modified-id=\"Finding-Articles-Relevant-to-Different-Queries-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;<\/span>Finding Articles Relevant to Different Queries<\/a><\/span><\/li><li><span><a href=\"#Ranking-Articles-Based-on-Their-Relevance\" data-toc-modified-id=\"Ranking-Articles-Based-on-Their-Relevance-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;<\/span>Ranking Articles Based on Their Relevance<\/a><\/span><\/li><li><span><a href=\"#Creating-an-Overall-Score\" data-toc-modified-id=\"Creating-an-Overall-Score-3.4\"><span class=\"toc-item-num\">3.4&nbsp;&nbsp;<\/span>Creating an Overall Score<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#Text-Summarization\" data-toc-modified-id=\"Text-Summarization-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;<\/span>Text Summarization<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Example-('risks-for-neonates,-newborns,-and-pregnant-women')\" data-toc-modified-id=\"Example-('risks-for-neonates,-newborns,-and-pregnant-women')-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;<\/span>Example ('risks for neonates, newborns, and pregnant women')<\/a><\/span><\/li><\/ul><\/li><\/ul><\/div>","94741d15":"#### Conclusion\n\nThe investigation into the abstracts showed that some sort of topic filtering or clustering would help narrow down the articles. From that point, the team moved to unstructured clustering using LDA to determine what topic clusters were available for analysis","09ad4100":"#### Clean and Create TFIDF Matrix","61782ea5":"Utilize the Natural Language Toolkit to ID and remove English, French, and Spanish Stopwords. Extended the stopwords to include Medical terms. Then initialize the English Spacy model for word context. ","f6c68648":"# COVID-19 Risk Factors\n\n**Created by Data Scientists at Booz Allen Hamilton**\n* Jo C.\n* Tirik F.\n* Joseph J.\n* Colin L.\n* David T.\n* Shawn W.\n\nWith significant subject matter expertise from Erin M.\n\nWhat puts someone at risk of developing severe symptoms from a COVID-19 infection? Unfortunately, with nCOV-19 being novel, the answer is unclear. However, using available data and cross referencing with published work on other coronavirus outbreaks, we can infer what risk factors may be present. The Booz Allen team used several approaches and techniques to try to aid in understanding not just the dataset, but also what relevant information was contained within these papers. The team knew that they lacked the significant medical expertise to make judgments from the data, so they followed an approach of \"delivering the right results at the right time to the right person.\" In that sense, the final model pulls excerpts from articles that it believes answers whether a known risk factor is likely to be significantly present in COVID-19 outbreaks. \n\nIn the next iteration, the team hopes to advance the techniques developed here to refine understanding of the needs and create a tool capable of aiding researchers in their quest to understand and act against COVID-19.","c03d1c45":"Multiple models were used to find papers relevant to these eight queries. \n\nFirst, the papers in the subset were split into sentences, and each sentence was embedded using the Universal Sentence Encoder (USE) from Tensorflow (https:\/\/tfhub.dev\/google\/universal-sentence-encoder\/4). Each sentece was embedded into a 512 length vector. All these vectors were assembled into an array with each row representing a sentence. The array was stored using a hdf5 file because of its size. This array of sentences was used to train a KNN model. Then, each query was embedded using the USE, and its 20 nearest neighbors were found.\n\nBelow is pseudocode for the process taken to do this.\n\n```python\nimport tensorflow as tf\nimport tensorflow_hub as hub\n\nfrom sklearn.neighbors import NearestNeighbors\n\nembed = hub.load('https:\/\/tfhub.dev\/google\/universal-sentence-encoder\/4')\n\nembedded_query = embed('risks for neonates, newborns, and pregnant women')\n\nknn = NearestNeighbors(n_neighbors=20, algorithm='ball_tree').fit(embedded_sentences)\n\ndistances, indices = knn.kneighbors(encoded_query.reshape(1, -1))\n               \nneighbors = unembedded_sentences.loc[indices[0]]                                 \n```\n\nFor each of the nearest neighbor sentences, the three sentences before and after it (if those sentences existed) were used to form an excerpt.","bb973c91":"## Exploratory Data Analysis\n\nTo determine the contents and segmentation of the available dataset, the team conducted a series of exploratory tasks on the dataset. Since the dataset was so large and varied in topics available, the team used input from Booz Allen subject matter experts (SMEs) to create keyword lists for term frequency - inverse document frequency (tf-idf) and to identify topics following the use of latent Dirichlet allocation (LDA) to group similar datasets together.\n\nWhile not all of the work done in this exploratory phase is shown here, we included certain excerpts of interest to show our progress.\n\n### TF-IDF with Keywords\n\nAs a preliminary approach to the data analysis, the team used tf-idf to analyze the abstracts of the articles to see what abstracts might be of interest. Since the abstracts had an overwhelmingly large corpus, the team used a series of basic SME identified keywords to search on. The key terms are shown below.","85663f1a":"#### Visualize Results from Previous Run\n\nTo save time on running in this notebook, we have uploaded a previous version of the team's LDA model, with the results shown below. An interactive html is located in the Dataset. ","47c849c2":"Print the top 10 keywords for the 20 topics. An interactive html with the full model run is located in the dataset.","dd8ad6e9":"Generate Term Document Frequecy for topic clustering. Create corpus of lemmatized (stemmed) words and ID every word for LDA model. ","aba293fc":"Topic modeling will be performed through the use of Latent Dirichlet Allocation (LDA). LDA is a generative statistical model that allows sets of observations to be explained by unobserved groups that explain why some parts of the data are similar. To save time on running the model, an embedded html of the output has been added at the end of the LDA section.","d4b225e2":"#### LDA: Build Model"}}