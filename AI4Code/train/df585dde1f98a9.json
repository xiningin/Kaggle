{"cell_type":{"b8c27730":"code","1a9e083b":"code","493c964e":"code","23409913":"code","43b80ae4":"code","3e763faf":"code","a55955a8":"markdown"},"source":{"b8c27730":"import pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn import metrics\nfrom sklearn import datasets\nfrom sklearn.preprocessing import MinMaxScaler, RobustScaler","1a9e083b":"# import the iris dataset\n# in this example, all 4 features are considered\n\niris = datasets.load_iris()\nX = iris.data[:, [0,1,2,3]] # how to extract columns by index\ntarget_names = np.asarray(iris.target_names)\nfeature_names = np.asarray(iris.feature_names)\ny = np.array(iris.target, dtype=int)","493c964e":"# feature rescaling\n# min-max scaling or robust scaling\n\nscaler = RobustScaler()\nscaler.fit(X)\nX = scaler.transform(X)","23409913":"df = pd.DataFrame(data=X, columns=feature_names)\ndf['Label'] = y # add the Label column to df","43b80ae4":"# Split into training and test data with rule\n# Random sampling is not suggested\n\ndef split_tri(n, Label1, Label2, Label3):\n    everyn = n\n\n    test_Label1 = Label1[::everyn] # accessing every nth element\n    train_Label1 = Label1[np.mod(np.arange(Label1.shape[0]), everyn) != 0]\n    \n    test_Label2 = Label2[::everyn] # accessing every nth element\n    train_Label2 = Label2[np.mod(np.arange(Label2.shape[0]), everyn) != 0]\n    \n    test_Label3 = Label3[::everyn] # accessing every nth element\n    train_Label3 = Label3[np.mod(np.arange(Label3.shape[0]), everyn) != 0]\n    \n    # Put back together\n    test = np.vstack((test_Label1, test_Label2, test_Label3))\n    train = np.vstack((train_Label1, train_Label2, train_Label3))\n    \n    X_train = train[:,0:4]\n    X_test = test[:,0:4]\n    y_train = train[:,4] # Label\n    y_test = test[:,4] # Label\n    \n    return X_train, X_test, y_train, y_test\n\nLabel1 = df[df['Label']==0]\nLabel2 = df[df['Label']==1]\nLabel3 = df[df['Label']==2]\n\nX_train, X_test, y_train, y_test = split_tri(6, Label1, Label2, Label3)\n\nprint(\"training size: \"+str(len(y_train)))\nprint(\"test size: \"+str(len(y_test)))","3e763faf":"def plot_BV(degree_min, degree_max, C):\n    T_error = []\n    t_error = []\n\n    degree_min = degree_min\n    degree_max = degree_max\n\n    for degree in range(degree_min,degree_max+1):\n        lr = make_pipeline(PolynomialFeatures(degree=degree, interaction_only=False, include_bias=False), \n                           LogisticRegression(C=C, max_iter=5000, solver='lbfgs', multi_class='multinomial', penalty='l2'))\n        lr.fit(X_train, y_train)\n        \n        training_pred = np.array(lr.predict(X_train))\n        train_error = metrics.f1_score(training_pred, y_train, average='weighted')\n        T_error.append(train_error)\n        \n        test_pred =  np.array(lr.predict(X_test))\n        test_error = metrics.f1_score(test_pred, y_test, average='weighted')\n        t_error.append(test_error)\n\n    plt.plot(range(degree_min,degree_max+1), T_error,'r', marker=\"s\", label='Training')\n    plt.plot(range(degree_min,degree_max+1), t_error,'g', marker=\"o\", label='Test')\n\n    plt.legend(loc='lower right')\n    plt.ylabel('weighted f1-score')\n    plt.xlabel('Degree of Polynomial')\n    plt.title('Bias-Variance Analysis (C: '+str(C)+')')\n    plt.xticks(np.arange(degree_min,degree_max+1,step=1))\n    plt.grid()\n    plt.savefig(fname='.\/Bias_Variance_LR.jpg', dpi=600, format='jpg')\n    plt.show()\n    \nplot_BV(degree_min=1, degree_max=10, C=0.1)","a55955a8":"The iris dataset is used to demonstrate bias-variance decomposition for logistic regression.\n\nPolynomial decision boundary is possible with higher degree of polynomial."}}