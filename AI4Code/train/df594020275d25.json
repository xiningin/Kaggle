{"cell_type":{"8d48b0bc":"code","70e9f652":"code","5634be85":"code","a8e27cd6":"code","56668cfe":"code","41c1389d":"code","bab88b2a":"code","58916ff8":"code","271423cc":"code","cebb4caf":"code","6791158c":"code","7805b21b":"code","b295b0b9":"code","2e52507c":"code","d0a00354":"code","1a9b095c":"code","03796bb8":"code","c917083e":"code","5ad2664e":"code","0e8d7e2e":"code","b700202f":"code","3e9c75cd":"code","3dcde5f5":"code","83ea74e9":"code","c4a98733":"code","36ed46e3":"code","5dadf28f":"code","53b1fd10":"code","7eab811d":"code","8ef6b16c":"code","9003a015":"code","fd8a5a9a":"code","476fc370":"code","470765e0":"code","bece088a":"code","329a6a38":"code","97096a31":"code","ac4b3caa":"code","9e1d7a1b":"code","dc1ff087":"code","f9e53696":"code","2941cfee":"code","82758fc1":"code","36b7f92c":"code","345bdde4":"code","35ebb56d":"code","b77dd7ea":"markdown","dc5b6941":"markdown","038838d0":"markdown","ef18eabe":"markdown"},"source":{"8d48b0bc":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","70e9f652":"import pandas as pd\n\n#Area schools include\nSchools = pd.read_csv(\"..\/input\/tacoma-schools\/Schools.csv\")\nSchools.head(3)","5634be85":"Hate_Biases = pd.read_csv(\"..\/input\/hatebias-tacoma\/City_of_Tacoma_Hate_Bias__2013-2019_.csv\")\nFires = pd.read_csv(\"..\/input\/tacoma-fires\/Tacoma_Fire_Department_-_Fire_Incident_Data_Table.csv\")","a8e27cd6":"#Tacoma-specific\nFires.head(2)","56668cfe":"#Tacoma-specific\nHate_Biases.head(3)","41c1389d":"a=pd.concat([Schools, Fires, Hate_Biases], axis=1)\na.head(3)","bab88b2a":"#drop excess columns\nb=a.drop(['OBJECTID', 'Case Number','DIST_NO','Occurred On Day', 'Occurred On','Offense Status','GRADE','Disposition','PRS_ID', 'District', 'Age', 'Gender', 'Entity Name', 'WEBSITE', 'PHONE', 'Block', 'Sector', 'Resident Status', 'Offense Type', 'Ethnicity', 'Address of Incident (XY)'], axis=1)\nb.head(2)   ","58916ff8":"#Still too large, but also need several of these columns \n\nc=b.drop( ['Mobile_VehicleYear', 'Fire_OutsideAreaAffected','Structure_FireFloorOfOrigin','IncidentNumber', 'EstimatedContentLoss', 'Structure_BuildingStatus', 'IncidentID', 'Mobile_VehicleMake', 'Mobile_VehicleModel', 'X_COORD', 'IncidentDate', 'Structure_NumberOfOccupants', 'Structure_NumberOfBusinesses', 'Y_COORD', 'Structure_TotalSQFootBurned', 'Structure_TotalSQFootSmokeDamage', 'Structure_ConstructionType','Structure_AlarmInvolved', 'Structure_FloorOfOriginDescription', 'Structure_BurnDamage', 'Structure_SmokeDamage', 'Structure_AlarmType', 'Structure_NumberOfBasementLevels', 'Structure_NumberOfStories', 'Structure_AlarmEffectiveness', 'Structure_TotalSqFootage'], axis=1)\nc.head(2)","271423cc":"#Pick specific desired columns for a dataframe\ndf = c[['Address','Location', 'ZIP','ZipCode','Latitude','Longitude','NAME','Hate Bias', 'CITY', 'EstimatedTotalFireLoss']].copy()\ndf.head(3)","cebb4caf":"df.describe()","6791158c":"#Explore the columns\ndf.apply (pd.Series.value_counts)","7805b21b":"df['NAME'].value_counts(dropna=False)","b295b0b9":"df['ZipCode'].value_counts(dropna=False)","2e52507c":"df['CITY'].value_counts(dropna=False)","d0a00354":"import numpy as np\ndf=df.fillna(0)\n#find and change NaNs to a zero","1a9b095c":"df1=df[df.CITY == 'TACOMA']\ndf1.tail(3) #reflects the bottom rows","03796bb8":"#These entries are in Tacoma but lack the LAT LONGS \nerror1= df1[df1.Latitude == 0]\nerror1.head(2)","c917083e":"df1.Location[280]","5ad2664e":"#Update data to include the Lat and Long availiavle in the Location column\ndf1.at[280,'Latitude']= 47.18027\ndf1.at[280,'Longitude']=-122.426748\ndf1.tail(3)","0e8d7e2e":"#Repeat for other LAT\/LON rows \ndf1.Location[265]","b700202f":"#Update data to include the Lat and Long available in the Location column\ndf1.at[104,'Latitude']= 47.174562\ndf1.at[104,'Longitude']=-122.43407\ndf1.at[105,'Latitude']=47.216222\ndf1.at[105,'Longitude']=-122.425035\n\ndf1.at[129,'Latitude']= 47.222461\ndf1.at[129,'Longitude']=-122.477576\n\ndf1.at[268,'Latitude']=47.121624\ndf1.at[268,'Longitude']=-122.493964\ndf1.at[265,'Latitude']=47.220936\ndf1.at[265,'Longitude']=-122.337246","3e9c75cd":"#Verify changes\nerror2= df1[df1.Latitude == 0]\nerror2.head(2)\n#Still some to go..","3dcde5f5":"##df1.Location[228]\ndf1.Location[245]","83ea74e9":"df1.at[173,'Latitude']= 47.25396\ndf1.at[173,'Longitude']=-122.419891\ndf1.at[174,'Latitude']= 47.204561\ndf1.at[174,'Longitude']=-122.433263\ndf1.at[179,'Latitude']=47.228756\ndf1.at[179,'Longitude']=-122.499617\n\ndf1.at[186,'Latitude']=47.211272\ndf1.at[186,'Longitude']=-122.449777\ndf1.at[201,'Latitude']=47.261692\ndf1.at[201,'Longitude']=-122.463316\ndf1.at[228,'Latitude']=47.252557\ndf1.at[228,'Longitude']=-122.495463\n\ndf1.at[240,'Latitude']=47.200213\ndf1.at[240,'Longitude']=-122.468269\ndf1.at[245,'Latitude']=47.230702\ndf1.at[245,'Longitude']=-122.31735","c4a98733":"df1.head(3)","36ed46e3":"df2=df1.drop(['Location'], axis=1)\ndf2.head(3)","5dadf28f":"#Clean dataframe\nerrs1= df2[df2.ZipCode == 0]\nerrs1.head()","53b1fd10":"#Update the ZipCode column\ndf2.at[64,'ZipCode']= 98405\ndf2.at[71,'ZipCode']= 98406\ndf2.at[79,'ZipCode']= 98409\ndf2.at[83,'ZipCode']= 98445\ndf2.at[90,'ZipCode']= 98443\ndf2.at[92,'ZipCode']= 98405\ndf2.at[93,'ZipCode']= 98422\ndf2.at[94,'ZipCode']= 98444\ndf2.at[95,'ZipCode']= 98404\ndf2.at[96,'ZipCode']= 98444","7eab811d":"df2.at[3,'ZipCode']= 98403\ndf2.at[4,'ZipCode']= 98409\ndf2.at[7,'ZipCode']= 98408\ndf2.at[9,'ZipCode']= 98405\ndf2.at[10,'ZipCode']= 98405\ndf2.at[11,'ZipCode']= 98409\ndf2.at[19,'ZipCode']= 98408\ndf2.at[20,'ZipCode']= 98404\ndf2.at[23,'ZipCode']= 98404\ndf2.at[24,'ZipCode']= 98444","8ef6b16c":"df2.at[26,'ZipCode']= 98422\ndf2.at[27,'ZipCode']= 98405\ndf2.at[33,'ZipCode']= 98446\ndf2.at[38,'ZipCode']= 98446\ndf2.at[41,'ZipCode']= 98467\ndf2.at[45,'ZipCode']= 98444\ndf2.at[46,'ZipCode']= 98402\ndf2.at[51,'ZipCode']= 98446\ndf2.at[55,'ZipCode']= 98404\ndf2.at[58,'ZipCode']= 98422","9003a015":"#Verified changes\nerrs2= df2[df2.ZipCode == 0]\nerrs2.head()","fd8a5a9a":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\nsns.countplot(df2['ZipCode'])","476fc370":"sns.scatterplot(x='ZipCode',y='EstimatedTotalFireLoss',data=df2, color='red')","470765e0":"df3=df2.drop(['ZIP'], axis=1)\ndf3.head(3)","bece088a":"df3['Hate Bias'].value_counts()","329a6a38":"from sklearn.preprocessing import LabelEncoder\n#It requires the category \u2018object\u2019 column to become of \u2018category\u2019 type before running it.\n\ndf3['Hate Bias']= df3['Hate Bias'].astype('category')# Assigning numerical value \ndf3['Hate Bias catg'] = df3['Hate Bias'].cat.codes\ndf3.head(3)","97096a31":"import folium\n#Black and white map\nTacoma = folium.Map(location=[47.2529, -122.4443],\n                   tiles = \"Stamen Toner\", zoom_start = 12)\n\nTacoma ","ac4b3caa":"Tac = folium.Map(location=[47.2529, -122.4443],\n                   zoom_start = 12)\n#Schools as markers\n##Mark the first school in df3= Annie Wright School\nfolium.Marker([47.248468, -122.444963], popup='Annie Wright').add_to(Tac)\nTac","9e1d7a1b":"#repeat for all schools using a loop\nfor index, row in df.iterrows(): #using df \n    if row['NAME']!=0: #to avoid an error      \n        folium.Marker([row['Latitude'], row['Longitude']], popup=row['NAME']).add_to(Tac)\nTac","dc1ff087":"#Heat map\nfrom folium import plugins\nfrom folium.plugins import HeatMap\n\n# Ensure the data is float (float64 may not work as is)\ndf3['Latitude'] = df3['Latitude'].astype(float)\ndf3['Longitude'] = df3['Longitude'].astype(float)\n","f9e53696":"#Filters\n##Hate Bias reports\nheatMap = df3[df3['Hate Bias catg']>0] \nheatMap.head(3)","2941cfee":"#a new map without school markers\nTac1 = folium.Map(location=[47.2529, -122.4443],\n                   zoom_start = 12)","82758fc1":"# List comprehension to make out list of lists\nheat_data = [[row['Latitude'],row['Longitude']] for index, row in heatMap.iterrows()]\n\n# Plot it on the map\nHeatMap(heat_data).add_to(Tac1)\n\nTac1","36b7f92c":"heatMap2 = df3[df3['EstimatedTotalFireLoss']>0] \nheatMap2.head()","345bdde4":"#a new map for fires\nTac2 = folium.Map(location=[47.2529, -122.4443],\n                   zoom_start = 12)","35ebb56d":"heatMap2 = df3[df3['EstimatedTotalFireLoss']>0] \nheat_data1 = [[row['Latitude'],row['Longitude']] for index, row in heatMap2.iterrows()]\n\n# Plot \nHeatMap(heat_data1).add_to(Tac2)\n\nTac2","b77dd7ea":"The heatmap above reflects hate-bias reports and frequency at schools in Tacoma.    \nThe map below reflects the losses due to fires on or around schools in Tacoma.","dc5b6941":"The data above is not even in size or type.       \nExploring and cleaning more the data could help.   ","038838d0":"### Visualization ###","ef18eabe":"Label encode"}}