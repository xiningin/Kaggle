{"cell_type":{"88a058b7":"code","0c6c1f75":"code","adcc600b":"code","44446ec6":"code","f84fbf12":"code","354d1baf":"code","0642d602":"code","9665657f":"code","2c202152":"code","b3267ad0":"code","c62a26b8":"code","50fb566c":"code","af780a67":"code","3bbf61cc":"code","96b6d4cb":"code","15d84a58":"code","0082698c":"code","709d2cf9":"code","831ef9d7":"code","141237f7":"code","18c66499":"code","ba294359":"code","b73c8492":"markdown","3ff81902":"markdown","5ee5e350":"markdown","af5724d2":"markdown","dfc083de":"markdown","e70d0fe3":"markdown","11a790d4":"markdown","8cc3cd84":"markdown","ca90a8aa":"markdown","8985e927":"markdown","f18710ed":"markdown","2b332692":"markdown","6cb0aca0":"markdown","4c7a448f":"markdown","b34fefa3":"markdown"},"source":{"88a058b7":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport random\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.applications.resnet50 import ResNet50\nfrom tensorflow.keras.applications.inception_resnet_v2 import InceptionResNetV2\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras.models import Model, load_model\nfrom tensorflow.keras.initializers import glorot_uniform\nfrom tensorflow.keras.utils import plot_model\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras.optimizers import SGD\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint, LearningRateScheduler\nfrom sklearn.metrics import confusion_matrix, classification_report, accuracy_score\nfrom random import sample\nfrom IPython.display import display\nimport os\nimport PIL\n","0c6c1f75":"train = {}\ntest = {}\n\npath = \"..\/input\/intel-image-classification\"\n\n# Make dictionary storing images for each category under train data.\npath_train = os.path.join(path, \"seg_train\/seg_train\")\nfor i in os.listdir(path_train):\n    train[i] = os.listdir(os.path.join(path_train, i))\n\n# Make dictionary storing images for each category under test data.\npath_test = os.path.join(path, \"seg_test\/seg_test\")\nfor i in os.listdir(path_test):\n    test[i] = os.listdir(os.path.join(path_test, i))","adcc600b":"# View the number of images in the entire training and testing datasets respectively.\nlen_train = np.concatenate(list(train.values())).shape[0]\nlen_test = np.concatenate(list(test.values())).shape[0]\n\nprint(\"Number of images in training data : {}\".format(len_train))\nprint(\"Number of images in testing data : {}\".format(len_test))","44446ec6":"# Randomly display 5 images under each of the 6 categories from the training data.\n# You will see different images each time.\nfig, axs = plt.subplots(6, 5, figsize = (15, 15))\nfor i, item in enumerate(os.listdir(path_train)):\n    images = sample(train[item], 5)\n    \n    for j, image in enumerate(images):\n        img = PIL.Image.open(os.path.join(path_train, item, image))\n        axs[i, j].imshow(img)\n        axs[i, j].set(xlabel = item, xticks = [], yticks = [])\n\nfig.tight_layout()","f84fbf12":"# View the number of images in each of the 6 categories in the training data.\nfor item in train.keys():\n    print(item, len(train[item]))","354d1baf":"# Make a pie-chart to visualize the percentage contribution of each category.\n# This is often useful when you want your dataset to be balanced.\nfig, ax = plt.subplots()\nax.pie(\n    [len(train[item]) for item in train],\n    labels = train.keys(),\n    autopct = \"%1.1f%%\"\n)\nfig.show()","0642d602":"# Create an Image Generator and specify the type of data augmentation you want to apply.\n# Here we go with zooming, flipping (horizontally and vertically), and rescaling.\ntrain_datagen = ImageDataGenerator(\n    zoom_range = 0.2,\n    horizontal_flip = True,\n    vertical_flip = True,\n    rescale=1.\/255\n)\n\n# For test data we only rescale the data.\n# Never augment test data!!!\ntest_datagen = ImageDataGenerator(rescale=1.\/255)","9665657f":"# Create a generator for the images. \n# This will make images (including augmented ones) start flowing from the directory to the model.\n# Note that augmented images are not stored along with the original images. The process happens in memory.\n\n# Train generator\ntrain_generator = train_datagen.flow_from_directory(\n    path_train,\n    target_size=(256, 256),\n    batch_size=32,\n    class_mode='categorical'\n)\n\n# Test generator\ntest_generator = test_datagen.flow_from_directory(\n    path_test,\n    target_size=(256, 256),\n    batch_size=32,\n    class_mode='categorical'\n)","2c202152":"# This function implements a resnet block.\n# You can use a different architecture if you like.\ndef res_block(X, filter, stage):\n  \n    # Convolutional_block\n    X_copy = X\n\n    f1 , f2, f3 = filter\n    \n    # Main Path\n    X = Conv2D(f1, (1,1),strides = (1,1), name ='res_'+str(stage)+'_conv_a', kernel_initializer= glorot_uniform(seed = 0))(X)\n    X = MaxPool2D((2,2))(X)\n    X = BatchNormalization(axis =3, name = 'bn_'+str(stage)+'_conv_a')(X)\n    X = Activation('relu')(X) \n\n    X = Conv2D(f2, kernel_size = (3,3), strides =(1,1), padding = 'same', name ='res_'+str(stage)+'_conv_b', kernel_initializer= glorot_uniform(seed = 0))(X)\n    X = BatchNormalization(axis =3, name = 'bn_'+str(stage)+'_conv_b')(X)\n    X = Activation('relu')(X) \n\n    X = Conv2D(f3, kernel_size = (1,1), strides =(1,1),name ='res_'+str(stage)+'_conv_c', kernel_initializer= glorot_uniform(seed = 0))(X)\n    X = BatchNormalization(axis =3, name = 'bn_'+str(stage)+'_conv_c')(X)\n\n\n    # Short path\n    X_copy = Conv2D(f3, kernel_size = (1,1), strides =(1,1),name ='res_'+str(stage)+'_conv_copy', kernel_initializer= glorot_uniform(seed = 0))(X_copy)\n    X_copy = MaxPool2D((2,2))(X_copy)\n    X_copy = BatchNormalization(axis =3, name = 'bn_'+str(stage)+'_conv_copy')(X_copy)\n\n    # ADD\n    X = Add()([X, X_copy])\n    X = Activation('relu')(X)\n\n    # Identity Block 1\n    X_copy = X\n    \n\n    # Main Path\n    X = Conv2D(f1, (1,1),strides = (1,1), name ='res_'+str(stage)+'_identity_1_a', kernel_initializer= glorot_uniform(seed = 0))(X)\n    X = BatchNormalization(axis =3, name = 'bn_'+str(stage)+'_identity_1_a')(X)\n    X = Activation('relu')(X) \n\n    X = Conv2D(f2, kernel_size = (3,3), strides =(1,1), padding = 'same', name ='res_'+str(stage)+'_identity_1_b', kernel_initializer= glorot_uniform(seed = 0))(X)\n    X = BatchNormalization(axis =3, name = 'bn_'+str(stage)+'_identity_1_b')(X)\n    X = Activation('relu')(X) \n\n    X = Conv2D(f3, kernel_size = (1,1), strides =(1,1),name ='res_'+str(stage)+'_identity_1_c', kernel_initializer= glorot_uniform(seed = 0))(X)\n    X = BatchNormalization(axis = 3, name = 'bn_'+str(stage)+'_identity_1_c')(X)\n\n    # ADD\n    X = Add()([X, X_copy])\n    X = Activation('relu')(X)\n\n    # Identity Block 2\n    X_copy = X\n\n\n    # Main Path\n    X = Conv2D(f1, (1,1),strides = (1,1), name ='res_'+str(stage)+'_identity_2_a', kernel_initializer= glorot_uniform(seed = 0))(X)\n    X = BatchNormalization(axis =3, name = 'bn_'+str(stage)+'_identity_2_a')(X)\n    X = Activation('relu')(X) \n\n    X = Conv2D(f2, kernel_size = (3,3), strides =(1,1), padding = 'same', name ='res_'+str(stage)+'_identity_2_b', kernel_initializer= glorot_uniform(seed = 0))(X)\n    X = BatchNormalization(axis =3, name = 'bn_'+str(stage)+'_identity_2_b')(X)\n    X = Activation('relu')(X) \n\n    X = Conv2D(f3, kernel_size = (1,1), strides =(1,1),name ='res_'+str(stage)+'_identity_2_c', kernel_initializer= glorot_uniform(seed = 0))(X)\n    X = BatchNormalization(axis =3, name = 'bn_'+str(stage)+'_identity_2_c')(X)\n\n    # ADD\n    X = Add()([X, X_copy])\n    X = Activation('relu')(X)\n\n    return X","b3267ad0":"# Based on the defined resnet block, construct the neural network.\n\ninput_shape = (256,256,3)\n\n# Input tensor shape\nX_input = Input(input_shape)\n\n# Zero-padding\nX = ZeroPadding2D((3,3))(X_input)\n\n# 1- stage\nX = Conv2D(64, (7,7), strides= (2,2), name = 'conv1', kernel_initializer= glorot_uniform(seed = 0))(X)\nX = BatchNormalization(axis =3, name = 'bn_conv1')(X)\nX = Activation('relu')(X)\nX = MaxPooling2D((3,3), strides= (2,2))(X)\n\n# 2- stage\nX = res_block(X, filter= [64,64,256], stage= 2)\n\n# 3- stage\nX = res_block(X, filter= [128,128,512], stage= 3)\n\n# 4- stage\nX = res_block(X, filter= [256,256,1024], stage= 4)\n\n# 5- stage\nX = res_block(X, filter= [512,512,2048], stage= 5)\n\n# Average Pooling\nX = AveragePooling2D((2,2), name = 'Averagea_Pooling')(X)\n\n# Final layer\nX = Flatten()(X)\nX = Dropout(0.4)(X)\nX = Dense(6, activation = 'softmax', name = 'Dense_final', kernel_initializer= glorot_uniform(seed=0))(X)\n\n# Build model.\nmodel = Model(\n    inputs= X_input, \n    outputs = X, \n    name = 'Resnet18'\n)\n\n# Check out model summary.\nmodel.summary()","c62a26b8":"# Compile the model\nmodel.compile(\n    optimizer = \"adam\", \n    loss = \"categorical_crossentropy\", \n    metrics = [\"accuracy\"]\n)","50fb566c":"# Use early stopping to exit training if there is no improvement even after certain epochs (patience)\nearlystopping = EarlyStopping(\n    monitor = 'loss', \n    mode = 'min', \n    verbose = 1, \n    patience = 15\n)\n\n# Save the best model with lower validation loss\ncheckpointer = ModelCheckpoint(\n    filepath = \"weights.hdf5\", \n    verbose = 1, \n    save_best_only = True\n)","af780a67":"# Finally, fit the neural network model to the data.\n# Here we use 1 epoch for demonstration.\nhistory = model.fit_generator(\n    train_generator, \n    steps_per_epoch = train_generator.n \/\/ 32, \n    epochs = 1, \n    callbacks = [\n        checkpointer, \n        earlystopping\n    ]\n)","3bbf61cc":"# Evaluate the performance of the model\nevaluate = model.evaluate_generator(\n    test_generator, \n    steps = test_generator.n \/\/ 32, \n    verbose = 1\n)","96b6d4cb":"# Assign label names to the corresponding indexes\nlabels = {\n    0: 'buildings', \n    1: 'forest', \n    2: 'glacier', \n    3: 'mountain', \n    4: 'sea', \n    5: 'street'\n}","15d84a58":"# Load images and their predictions \n\nprediction = []\noriginal = []\nimage = []\ncount = 0\nfor i in os.listdir(path_test):\n    for item in os.listdir(os.path.join(path_test, i)):\n        # code to open the image\n        img= PIL.Image.open(os.path.join(path_test, i, item))\n        # resizing the image to (256,256)\n        img = img.resize((256, 256))\n        # appending image to the image list\n        image.append(img)\n        # converting image to array\n        img = np.asarray(img, dtype = np.float32)\n        # normalizing the image\n        img = img \/ 255\n        # reshaping the image into a 4D array\n        img = img.reshape(-1, 256, 256, 3)\n        # making prediction of the model\n        predict = model.predict(img)\n        # getting the index corresponding to the highest value in the prediction\n        predict = np.argmax(predict)\n        # appending the predicted class to the list\n        prediction.append(labels[predict])\n        # appending original class to the list\n        original.append(i)","0082698c":"# Get the test accuracy \nscore = accuracy_score(original, prediction)\nprint(\"Test Accuracy : {}\".format(score))","709d2cf9":"# Visualize the results at random\nfig = plt.figure(figsize = (100,100))\nfor i in range(20):\n    j = random.randint(0, len(image))\n    fig.add_subplot(20, 1, i+1)\n    plt.xlabel(\"Prediction: \" + prediction[j] +\"   Original: \" + original[j])\n    plt.imshow(image[j])\n    \nfig.tight_layout()\nplt.show()","831ef9d7":"# Check out the Classification Report \nprint(classification_report(np.asarray(prediction), np.asarray(original)))\n\n# Based on these values, you can try t improve your model.\n# For the sake of simplicity, hyperparameter tuning and model improvement was not done.","141237f7":"# View the 6x6 confusion matrix\nplt.figure(figsize = (7, 5))\ncm = confusion_matrix(np.asarray(prediction), np.asarray(original))\nsns.heatmap(\n    cm, \n    annot = True, \n    fmt = \"d\"\n)\nplt.show()","18c66499":"def grad_cam(img):\n    # Convert the image to array of type float32\n    img = np.asarray(img, dtype = np.float32)\n\n    # Reshape the image from (256,256,3) to (1,256,256,3)\n    img = img.reshape(-1, 256, 256, 3)\n    img_scaled = img \/ 255\n\n    # Name of the average pooling layer and dense final (you can see these names in the model summary)\n    classification_layers = [\"Averagea_Pooling\", \"Dense_final\"]\n\n    # Last convolutional layer in the model\n    final_conv = model.get_layer(\"res_5_identity_2_c\")\n\n    # Create a model with original model inputs and the last conv_layer as the output\n    final_conv_model = keras.Model(model.inputs, final_conv.output)\n\n    # Then we create the input for classification layer, which is the output of last conv layer\n    # In our case, output produced by the conv layer is of the shape (1,3,3,2048) \n    # Since the classification input needs the features as input, we ignore the batch dimension\n\n    classification_input = keras.Input(shape = final_conv.output.shape[1:])\n\n    # We iterate through the classification layers, to get the final layer and then append \n    # the layer as the output layer to the classification model.\n    temp = classification_input\n    for layer in classification_layers:\n        temp = model.get_layer(layer)(temp)\n    \n    classification_model = keras.Model(classification_input, temp)\n\n\n    # We use gradient tape to monitor the 'final_conv_output' to retrive the gradients\n    # corresponding to the predicted class\n    with tf.GradientTape() as tape:\n        # Pass the image through the base model and get the feature map \n        final_conv_output = final_conv_model(img_scaled)\n\n        # Assign gradient tape to monitor the conv_output\n        tape.watch(final_conv_output)\n      \n        # Pass the feature map through the classification model and use argmax to get the \n        # index of the predicted class and then use the index to get the value produced by final\n        # layer for that class\n        prediction = classification_model(final_conv_output)\n\n        predicted_class = tf.argmax(prediction[0][0][0])\n\n        predicted_class_value = prediction[:,:,:,predicted_class]\n  \n    # Get the gradient corresponding to the predicted class based on feature map.\n    # which is of shape (1,3,3,2048)\n    gradient = tape.gradient(predicted_class_value, final_conv_output)\n\n    # Since we need the filter values (2048), we reduce the other dimensions, \n    # which would result in a shape of (2048,)\n    gradient_channels = tf.reduce_mean(gradient, axis=(0, 1, 2))\n\n    # We then convert the feature map produced by last conv layer(1,6,6,1536) to (6,6,1536)\n    final_conv_output = final_conv_output.numpy()[0]\n\n    gradient_channels = gradient_channels.numpy()\n\n    # We multiply the filters in the feature map produced by final conv layer by the \n    # filter values that are used to get the predicted class. By doing this we inrease the\n    # value of areas that helped in making the prediction and lower the vlaue of areas, that \n    # did not contribute towards the final prediction\n    for i in range(gradient_channels.shape[-1]):\n        final_conv_output[:, :, i] *= gradient_channels[i]\n\n    # We take the mean accross the channels to get the feature map\n    heatmap = np.mean(final_conv_output, axis=-1)\n\n    # Normalizing the heat map between 0 and 1, to visualize it\n    heatmap_normalized = np.maximum(heatmap, 0) \/ np.max(heatmap)\n\n    # Rescaling and converting the type to int\n    heatmap = np.uint8(255 * heatmap_normalized )\n\n    # Create the colormap\n    color_map = plt.cm.get_cmap('jet')\n\n    # get only the rb features from the heatmap\n    color_map = color_map(np.arange(256))[:, :3]\n    heatmap = color_map[heatmap]\n\n    # convert the array to image, resize the image and then convert to array\n    heatmap = keras.preprocessing.image.array_to_img(heatmap)\n    heatmap = heatmap.resize((256, 256))\n    heatmap = np.asarray(heatmap, dtype = np.float32)\n\n    # Add the heatmap on top of the original image\n    final_img = heatmap * 0.4 + img[0]\n    final_img = keras.preprocessing.image.array_to_img(final_img)\n\n    return final_img, heatmap_normalized\n","ba294359":"# Visualize the images in the dataset\nfig, axs = plt.subplots(6,3, figsize = (16,32))\ncount = 0\nfor _ in range(6):\n    i = random.randint(0, len(image))\n    gradcam, heatmap = grad_cam(image[i])\n    axs[count][0].title.set_text(\"Original -\" + original[i])\n    axs[count][0].imshow(image[i])\n    axs[count][1].title.set_text(\"Heatmap\") \n    axs[count][1].imshow(heatmap)\n    axs[count][2].title.set_text(\"Prediction -\" + prediction[i]) \n    axs[count][2].imshow(gradcam)  \n    count += 1\n\nfig.tight_layout()","b73c8492":"![grad.png](attachment:grad.png)","3ff81902":"# Explainable AI: Scene Classification and GradCam Visualization","5ee5e350":"## Develop Neural Network Architecture","af5724d2":"![image.png](attachment:image.png)","dfc083de":"## Create data generator","e70d0fe3":"## Specify callbacks","11a790d4":"## Augment data","8cc3cd84":"## Model evaluation","ca90a8aa":"## Compile model","8985e927":"##  Import necessary libraries","f18710ed":"## Grad Cam Visualization","2b332692":"## Explore data","6cb0aca0":"## Read Data\n#### Name: Intel Image Classification\n#### Location: https:\/\/www.kaggle.com\/puneet6060\/intel-image-classification\n#### Uploaded By: Puneet Bansal\n#### About: Natural Scenes from around the world\n#### In this notebook we will look at classifying scenes into one of 6 categories- buildings, forest, glacier, mountain, sea, and street. The objective is to use a technique called GradCam to visualize the portions of the image that enable the model to classify. ","4c7a448f":"## Model training","b34fefa3":"### Credits: Ryan Ahmed, instructor of course - \"Explainable AI: Scene Classification and GradCam Visualization\" on coursera project network. You guys should definitely check it out!"}}