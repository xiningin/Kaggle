{"cell_type":{"02bb9fd9":"code","9b089528":"code","a40116df":"code","900adfd6":"code","f26bf3dd":"code","28ecf2bd":"code","eb84459a":"code","ed26cbdb":"code","57bc43f3":"code","5dbb362a":"code","b75c5ed6":"code","46df3420":"code","a52f0517":"code","91058ead":"code","cc765757":"code","0d21c695":"code","b8dd29cb":"code","76b9d749":"code","8694bb1f":"markdown","550ac767":"markdown","97148d30":"markdown","5013592c":"markdown","ee8bc3c1":"markdown","65335819":"markdown","f5982ecc":"markdown","1b42558a":"markdown","a199d7fe":"markdown","b3d84316":"markdown","6a3a1e41":"markdown","cbc7e60a":"markdown","b6acf021":"markdown","08d4c97b":"markdown","73173f3c":"markdown","09763c23":"markdown","00ea5d8b":"markdown","68343ed5":"markdown","007b8b56":"markdown","441f38af":"markdown","9e0f79a3":"markdown","5dc5f60e":"markdown","ad705e62":"markdown","0df7bef1":"markdown","d7862177":"markdown","94ebaa55":"markdown","3723d78b":"markdown","89eed169":"markdown","f9e979b8":"markdown"},"source":{"02bb9fd9":"%%HTML\n<style type=\"text\/css\">\n     \n    \ndiv.h2 {\n    background-color: #159957;\n    background-image: linear-gradient(120deg, #155799, #159957);\n    text-align: left;\n    color: white;              \n    padding:9px;\n    padding-right: 100px; \n    font-size: 20px; \n    max-width: 1500px; \n    margin: auto; \n    margin-top: 40px; \n}\n                                     \n                                      \nbody {\n  font-size: 12px;\n}    \n     \n                                    \n                                      \ndiv.h3 {\n    color: #159957; \n    font-size: 18px; \n    margin-top: 20px; \n    margin-bottom:4px;\n}\n   \n                                      \ndiv.h4 {\n    color: #159957;\n    font-size: 15px; \n    margin-top: 20px; \n    margin-bottom: 8px;\n}\n   \n                                      \nspan.note {\n    font-size: 5; \n    color: gray; \n    font-style: italic;\n}\n  \n                                      \nhr {\n    display: block; \n    color: gray\n    height: 1px; \n    border: 0; \n    border-top: 1px solid;\n}\n  \n                                      \nhr.light {\n    display: block; \n    color: lightgray\n    height: 1px; \n    border: 0; \n    border-top: 1px solid;\n}   \n    \n                                      \ntable.dataframe th \n{\n    border: 1px darkgray solid;\n    color: black;\n      <table align=\"left\">\n    ...\n  <\/table>\n    background-color: white;\n}\n    \n                                      \ntable.dataframe td \n{\n    border: 1px darkgray solid;\n    color: black;\n    background-color: white;\n    font-size: 11px;\n    text-align: center;\n} \n   \n            \n                                      \ntable.rules th \n{\n    border: 1px darkgray solid;\n    color: black;\n    background-color: white;\n    font-size: 11px;\n    align: left;\n}\n       \n                                      \ntable.rules td \n{\n    border: 1px darkgray solid;\n    color: black;\n    background-color: white;\n    font-size: 13px;\n    text-align: center;\n} \n   \n                                      \n                                      \ntable.rules tr.best\n{\n    color: green;\n}    \n    \n                                      \n.output { \n    align-items: left; \n}\n        \n                                      \n.output_png {\n    display: table-cell;\n    text-align: left;\n    margin:auto;\n}                                          \n                                                                    \n                                      \n                                      \n<\/style>  ","9b089528":"#**********IMPORTANT INFORMATION**************#\n# 86b5fa553252dd29bf3a1ee24d514bdd26afc425\n# \/kaggle\/input\/CORD-19-research-challenge\/comm_use_subset\/comm_use_subset\/a137eb51461b4a4ed3980aa5b9cb2f2c1cf0292a.json\n\n\n#------------------------------------------------------------\nimport os\n#------------------------------------------------------------\nimport json\nimport re\nimport requests\n#------------------------------------------------------------\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n#------------------------------------------------------------\nimport math\nimport numpy as np\nimport pandas as pd\nimport scipy as sp\nfrom scipy import stats\n#------------------------------------------------------------\nfrom sklearn import metrics\nfrom sklearn.utils import shuffle\nfrom gensim.models import Word2Vec\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA\n#--------------------- NLTK ---------------------------------\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk import WordNetLemmatizer\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\n#------------------------------------------------------------\nfrom IPython.display import HTML\nfrom IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = \"all\"\n# #------------------------------------------------------------\n\n\n\n\n# ---------------------------------------------------\n# list all files under the input directory\n# import os\n# for dirname, _, filenames in os.walk('\/kaggle\/input'):\n#     for filename in filenames:\n#         # print(os.path.join(dirname, filename))\n# Any results you write to the current directory are saved as output.\n#\n#\n#textFile = spark.read.text('path\/file.txt')\n# json_to_df = spark.read.json()\n# ---------------------------------------------------\n# import os\n# for dirname, _, filenames in os.walk('\/kaggle\/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n# \/kaggle\/input\/CORD-19-research-challenge\/2020-03-13\/json_schema.txt\n# \/kaggle\/input\/CORD-19-research-challenge\/2020-03-13\/COVID.DATA.LIC.AGMT.pdf\n# \/kaggle\/input\/CORD-19-research-challenge\/2020-03-13\/all_sources_metadata_2020-03-13.csv\n# \/kaggle\/input\/CORD-19-research-challenge\/2020-03-13\/all_sources_metadata_2020-03-13.readme\n# \/kaggle\/input\/CORD-19-research-challenge\/2020-03-13\/comm_use_subset\/comm_use_subset\/25621281691205eb015383cbac839182b838514f.json\n#\n#\n#  Row(sha='469ed0f00c09e2637351c9735c306f27acf3aace', source_x='CZI', title='First two\\xa0months of the 2019 Coronavirus Disease (COVID-19) epidemic in China: real-time surveillance and evaluation with a second derivative model', doi='10.1186\/s41256-020-00137-4', pmcid=None, pubmed_id=None, license='cc-by', abstract='Similar to outbreaks of many other infectious diseases, success in controlling the novel 2019 coronavirus infection requires a timely and accurate monitoring of the epidemic, particularly during its early period with rather limited data while the need for information increases explosively.', publish_time='2020', authors='Chen, Xinguang; Yu, Bin', journal='Global Health Research and Policy', Microsoft Academic Paper ID='3006645647', WHO #Covidence='#5595', has_full_text='True')\n\n\n\n#------------------------------------------------------------\n# install pyspark, need internet \n#------------------------------------------------------------\n! pip install pyspark \n#------------------------------------------------------------\n# pyspark in \/opt\/conda\/lib\/python3.6\/site-packages (2.4.5)\n# py4j==0.10.7 in \/opt\/conda\/lib\/python3.6\/site-packages (from pyspark) (0.10.7)\n#------------------------------------------------------------\n\nnltk.download('punkt')\nnltk.download('averaged_perceptron_tagger')\n\n\n","a40116df":"# ----------------------------------\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.types import *\nfrom pyspark.sql.functions import * \n\n# StructField, StructType, StringType, MapType, IntegerType, etc etc \n\n# print(\"Initiate Apache Spark session...\")\nspark = SparkSession.builder \\\n                     .master(\"local[*]\") \\\n                     .appName(\"COVID-19-Technical-Analysis\") \\\n                     .getOrCreate() \n\n\n# create paths for ease \n# * eventually we can expect a different data versus 2020-03-13 *\n# * update:  was in fact updated: * \n# Update this source structure as time passes: \nmaster_file_path = '\/kaggle\/input\/CORD-19-research-challenge'\nmeta_file_loc = f'{master_file_path}\/metadata.csv'\n# core input:  \/kaggle\/input\/CORD-19-research-challenge\/metadata.csv\n\n\n# --- read in the metadata csv file, you will want to infer the header ---\nsdf = spark.read.csv(meta_file_loc, \n                          header=True, \n                          inferSchema=True)\n                          # pyspark.sql.dataframe.DataFrame\n\n    \n# rename columns for clarity:  \nsdf = sdf.withColumnRenamed(\"source_x\",\"src\").withColumnRenamed(\"publish_time\",\"published_date\").withColumnRenamed('Microsoft Academic Paper ID',\"microsoft_paper_id\").withColumnRenamed('WHO #Covidence',\"WHO_covidence\") \n\n\n# myschema = StructType([\n#     StructField(\"ID_DAY\", DateType()),\n#     StructField(\"SID_STORE\", IntegerType()),\n#     StructField(\"NB_TICKET\", IntegerType()),\n#     StructField(\"F_TOTAL_QTY\", IntegerType()),\n#     StructField(\"F_VAL_SALES_AMT_TCK\", DoubleType()),\n#     StructField(\"SITE_FORMAT\", StringType())])\n\n\nspark\n# ---","900adfd6":"# single_data_record_example = meta_sdf.first().asDict()\n# temp_df = pd.DataFrame(single_data_record_example.items())\n# pd.DataFrame(meta_sdf.first().asDict().items()).style.set_properties(**{'text-align': 'left'}).set_table_styles([dict(selector='th', props=[('text-align', 'center')])])\n# temp_df = pd.DataFrame(single_data_record_example.items())\n# temp_df.style.set_properties(**{'text-align': 'left'}).set_table_styles([dict(selector='th', props=[('text-align', 'center')])])\n# temp_df.T. \\\n# style.set_properties(**{'text-align': 'left'}).set_table_styles([dict(selector='th', props=[('text-align', 'center')])])\n# -----------------------------------------------------------\n# >keep>  d = sdf.first().asDict()\n\n\n\n# ['sha',\n#  'src',\n#  'title',\n#  'doi',\n#  'pmcid',\n#  'pubmed_id',\n#  'license',\n#  'abstract',\n#  'published_date',\n#  'authors',\n#  'journal',\n#  'microsoft_paper_id',\n#  'WHO_covidence',\n#  'has_full_text',\n#  'full_text_file']\n\n\n\n\n# Find this one in the meta: \/kaggle\/input\/CORD-19-research-challenge\/comm_use_subset\/comm_use_subset\/a137eb51461b4a4ed3980aa5b9cb2f2c1cf0292a.json\n# a137eb51461b4a4ed3980aa5b9cb2f2c1cf0292a\n# a137eb51461b4a4ed3980aa5b9cb2f2c1cf0292a\n\n\n\n#sdf.where(sdf.sha=='a137eb51461b4a4ed3980aa5b9cb2f2c1cf0292a').asDict\n# issues:   d = sdf.where(sdf.sha=='a137eb51461b4a4ed3980aa5b9cb2f2c1cf0292a').asDict\n\n\n# tempdf = pd.DataFrame(d.items(), columns=['Key', 'Value'])\n# tempdf.style.set_properties(**{'text-align': 'left'}).set_table_styles([dict(selector='th', props=[('text-align', 'center')])]).hide_index()\n\n#     type(sdf.where(sdf.sha=='a137eb51461b4a4ed3980aa5b9cb2f2c1cf0292a').collect()  # is a list \n#     type(sdf.where(sdf.sha=='a137eb51461b4a4ed3980aa5b9cb2f2c1cf0292a'))  pyspark.sql.dataframe.DataFrame\n#     type(sdf.collect()[43651])  pyspark.sql.types.Row\n\n\n\n# sdf.where(sdf.sha=='a137eb51461b4a4ed3980aa5b9cb2f2c1cf0292a').show()\n# +--------------------+---+--------------------+--------------------+--------------------+--------------+--------------------+-------------+-------------+---------------+\n# |                 sha|src|               title|                 doi|            abstract|published_date|             authors|      journal|has_full_text| full_text_file|\n# +--------------------+---+--------------------+--------------------+--------------------+--------------+--------------------+-------------+-------------+---------------+\n# |a137eb51461b4a4ed...|PMC|The effect of inh...|10.1186\/s12918-01...|BACKGROUND: The c...|   2016 Sep 23|McDermott, Jason ...|BMC Syst Biol|         True|comm_use_subset|\n# +--------------------+---+--------------------+--------------------+--------------------+--------------+--------------------+-------------+-------------+---------------+\n\n\n\n\n# READ READ READ \n# sdf.where(sdf.sha=='a137eb51461b4a4ed3980aa5b9cb2f2c1cf0292a').collect()[0].asDict()  # this is a [Row(sha='a137eb51461b4a4ed3980aa5b9cb2f2c1cf0292a',\n# {'sha': 'a137eb51461b4a4ed3980aa5b9cb2f2c1cf0292a',\n#  'src': 'PMC',\n#  'title': 'The effect of inhibition of PP1 and TNF\u03b1 signaling on pathogenesis of SARS coronavirus',\n#  'doi': '10.1186\/s12918-016-0336-6',\n#  'abstract': 'BACKGROUND: The complex interplay between viral replication and host immune response during infection remains poorly understood. While many viruses are known to employ anti-immune strategies to facilitate their replication, highly pathogenic virus infections can also cause an excessive immune response that exacerbates, rather than reduces pathogenicity. To investigate this dichotomy in severe acute respiratory syndrome coronavirus (SARS-CoV), we developed a transcriptional network model of SARS-CoV infection in mice and used the model to prioritize candidate regulatory targets for further investigation. RESULTS: We validated our predictions in 18 different knockout (KO) mouse strains, showing that network topology provides significant predictive power to identify genes that are important for viral infection. We identified a novel player in the immune response to virus infection, Kepi, an inhibitory subunit of the protein phosphatase 1 (PP1) complex, which protects against SARS-CoV pathogenesis. We also found that receptors for the proinflammatory cytokine tumor necrosis factor alpha (TNF\u03b1) promote pathogenesis, presumably through excessive inflammation. CONCLUSIONS: The current study provides validation of network modeling approaches for identifying important players in virus infection pathogenesis, and a step forward in understanding the host response to an important infectious disease. The results presented here suggest the role of Kepi in the host response to SARS-CoV, as well as inflammatory activity driving pathogenesis through TNF\u03b1 signaling in SARS-CoV infections. Though we have reported the utility of this approach in bacterial and cell culture studies previously, this is the first comprehensive study to confirm that network topology can be used to predict phenotypes in mice with experimental validation. ELECTRONIC SUPPLEMENTARY MATERIAL: The online version of this article (doi:10.1186\/s12918-016-0336-6) contains supplementary material, which is available to authorized users.',\n#  'published_date': '2016 Sep 23',\n#  'authors': 'McDermott, Jason E.; Mitchell, Hugh D.; Gralinski, Lisa E.; Eisfeld, Amie J.; Josset, Laurence; Bankhead, Armand; Neumann, Gabriele; Tilton, Susan C.; Sch\u00e4fer, Alexandra; Li, Chengjun; Fan, Shufang; McWeeney, Shannon; Baric, Ralph S.; Katze, Michael G.; Waters, Katrina M.',\n#  'journal': 'BMC Syst Biol',\n#  'has_full_text': 'True',\n#  'full_text_file': 'comm_use_subset'}\n\n\n# new - keep:\nd = sdf.where(sdf.sha=='a137eb51461b4a4ed3980aa5b9cb2f2c1cf0292a').collect()[0].asDict()\ntempdf = pd.DataFrame(d.items(), columns=['Key', 'Value'])\ntempdf.style.set_properties(**{'text-align': 'left'}).set_table_styles([dict(selector='th', props=[('text-align', 'center')])]).hide_index()\n\n\n\n#sdf.collect()[43651]\n# working original - keep: \n# d = sdf.collect()[43651].asDict()\n# tempdf = pd.DataFrame(d.items(), columns=['Key', 'Value'])\n# tempdf.style.set_properties(**{'text-align': 'left'}).set_table_styles([dict(selector='th', props=[('text-align', 'center')])]).hide_index()","f26bf3dd":"\n# ['sha',\n#  'src',\n#  'title',\n#  'doi',\n#  'pmcid',\n#  'pubmed_id',\n#  'license',\n#  'abstract',\n#  'published_date',\n#  'authors',\n#  'journal',\n#  'microsoft_paper_id',\n#  'WHO_covidence',\n#  'has_full_text',\n#  'full_text_file']\n\n\n\n# Streamline our spark data frame (sdf): \nsdf = sdf.drop('pmcid', 'pubmed_id', 'license', 'WHO_covidence', 'microsoft_paper_id')\n\n\n\n# delete more ! \n# sdf.columns\n# --- output --- \n# ['sha',\n#  'src',\n#  'title',\n#  'doi',\n#  'abstract',\n#  'published_date',\n#  'authors',\n#  'journal',\n#  'has_full_text',\n#  'full_text_file']","28ecf2bd":"\n#sdf.select('title').show()\n\ntdf = spark.read.csv(meta_file_loc, \n                          header=True, \n                          inferSchema=True)    \n# rename columns for clarity:  \ntdf = tdf.withColumnRenamed(\"source_x\",\"src\").withColumnRenamed(\"publish_time\",\"published_date\").withColumnRenamed('Microsoft Academic Paper ID',\"microsoft_paper_id\").withColumnRenamed('WHO #Covidence',\"WHO_covidence\") \ntdf = tdf.drop('pmcid', 'pubmed_id', 'license', 'WHO_covidence', 'microsoft_paper_id')\n\n# tdf.show()\n# +--------------------+--------+--------------------+--------------------+--------------------+--------------+--------------------+--------------------+-------------+--------------+\n# |                 sha|     src|               title|                 doi|            abstract|published_date|             authors|             journal|has_full_text|full_text_file|\n# +--------------------+--------+--------------------+--------------------+--------------------+--------------+--------------------+--------------------+-------------+--------------+\n# |                null|Elsevier|Intrauterine viru...|10.1016\/0002-8703...|Abstract The etio...|    1972-12-31|   Overall, James C.|American Heart Jo...|        False|custom_license|\n# |                null|Elsevier|Coronaviruses in ...|10.1016\/0002-8703...|                null|    1980-03-31|Georgescu, Leonid...|American Heart Jo...|        False|custom_license|\n# |                null|Elsevier|Cigarette smoking...|10.1016\/0002-8703...|                null|    1980-03-31|    Friedman, Gary D|American Heart Jo...|        False|custom_license|\n\n\n\n# keep and use \nimport pyspark.sql.functions as F\n# new col with length of string of title (number of characters)\ntdf.withColumn('NewC', F.length('title')).show()\n\n","eb84459a":"\n#\/kaggle\/input\/CORD-19-research-challenge\/comm_use_subset\/comm_use_subset\/a137eb51461b4a4ed3980aa5b9cb2f2c1cf0292a.json\n\nexample_json_file_path = '\/kaggle\/input\/CORD-19-research-challenge\/comm_use_subset\/comm_use_subset\/a137eb51461b4a4ed3980aa5b9cb2f2c1cf0292a.json'\nsample = spark.read.json(example_json_file_path, multiLine=True)\n# sample.show(100, False) \n# >keep>   sample.printSchema()\n# sample.show(100)\n# |            abstract|         back_matter|         bib_entries|           body_text|            metadata|            paper_id|         ref_entries|\n","ed26cbdb":"\nd = sample.collect()[0].asDict()\ntempdf = pd.DataFrame(d.items(), columns=['Key', 'Value'])\ntempdf.style.set_properties(**{'text-align': 'left'}).set_table_styles([dict(selector='th', props=[('text-align', 'center')])]).hide_index()\n\n","57bc43f3":"for element in sample.collect():\n    print(element.paper_id)\n    \n\n# sample.columns\n# ['abstract',\n#  'back_matter',\n#  'bib_entries',\n#  'body_text',\n#  'metadata',\n#  'paper_id',\n#  'ref_entries']\n    \n    ","5dbb362a":"for element in sample.collect():\n    print(element.metadata.authors[1].first, element.metadata.authors[1].middle[0], element.metadata.authors[1].last)","b75c5ed6":"for element in sample.collect():\n    print(element.metadata.title)","46df3420":"\nfor element in sample.collect():\n    print(element.abstract[0].text)\n    ","a52f0517":"\nfor element in sample.collect():\n    print(element.abstract[1].text)\n    ","91058ead":"\nfor element in sample.collect():\n    print(element.body_text[0].text)\n    ","cc765757":"#----------------------------------------------------\n# dont care and dont need: \n\n# for element in sample.collect():\n#     print(element.bib_entries)\n    \n    \n# #     for sub in element.bib_entries:\n# #         print(sub)\n#----------------------------------------------------\n\n\n\n# df.rdd.map(lambda row: row.asDict())\n\n\n\n    \n# sample.collect()[0].asDict()\n# for k in sample.collect()[0].asDict():\n#     print(k[1].text)\n    \n    \n    \n\n\n# sdf.where(sdf.sha=='a137eb51461b4a4ed3980aa5b9cb2f2c1cf0292a').collect()[0].asDict()  # this is a [Row(sha='a137eb51461b4a4ed3980aa5b9cb2f2c1cf0292a',\n \n\n# ===================\n# follow up here: *** \n# ===================\n# KEEPER !  KEEP  \n\n# df_list_of_dict = [row.asDict() for row in sample.collect()]\n# df_list_of_dict[0] \n# {'abstract': [Row(cite_spans=[], ref_spans=[], section='Abstract', text='Background: The complex interplay between viral replication and host immune response during infection remains poorly understood. While many viruses are known to employ anti-immune strategies to facilitate their replication, highly pathogenic virus infections can also cause an excessive immune response that exacerbates, rather than reduces pathogenicity. To investigate this dichotomy in severe acute respiratory syndrome coronavirus (SARS-CoV), we developed a transcriptional network model of SARS-CoV infection in mice and used the model to prioritize candidate regulatory targets for further investigation.'),\n#   Row(cite_spans=[], ref_spans=[], section='Abstract', text='Results: We validated our predictions in 18 different knockout (KO) mouse strains, showing that network topology provides significant predictive power to identify genes that are important for viral infection. We identified a novel player in the immune response to virus infection, Kepi, an inhibitory subunit of the protein phosphatase 1 (PP1) complex, which protects against SARS-CoV pathogenesis. We also found that receptors for the proinflammatory cytokine tumor necrosis factor alpha (TNF\u03b1) promote pathogenesis, presumably through excessive inflammation. Conclusions: The current study provides validation of network modeling approaches for identifying important players in virus infection pathogenesis, and a step forward in understanding the host response to an important infectious disease. The results presented here suggest the role of Kepi in the host response to SARS-CoV, as well as inflammatory activity driving pathogenesis through TNF\u03b1 signaling in SARS-CoV infections. Though we have reported the utility of this approach in bacterial and cell culture studies previously, this is the first comprehensive study to confirm that network topology can be used to predict phenotypes in mice with experimental validation.')],\n\n    ","0d21c695":"#  Generating word counts -  NEEDS WORK \n\n# lines = tdf.select('title').collect()\n# #print(lines)\n\n\n# words = lines.flatMap(lambda line: line.split(\" \"))\n# word_counts = words.map(lambda word: (word,1))\n# word_counts\n\n\n\n# tdf.select('title').show(truncate=False)\n# tdf.select('title').show(n=12, truncate=False, vertical=True) \n\n\n# words = lines.flatMap(lambda line: line.split(\" \"))\n# word_counts = words.map(lambda word: (word,1))\n\n","b8dd29cb":"\n# # RIFF \/ RAFF: \n\n\n# df.head(1).T. \\\n# style.set_properties(**{'text-align': 'left'}).set_table_styles([dict(selector='th', props=[('text-align', 'center')])])\n\n\n# meta_sdf.first().asDict()\n\n\n# # for k,v in tips.items():\n# #     print(f'{k}:\\t{v}')\n# {'sha': 'c630ebcdf30652f0422c3ec12a00b50241dc9bd9',\n#  'source_x': 'CZI',\n#  'title': 'Angiotensin-converting enzyme 2 (ACE2) as a SARS-CoV-2 receptor: molecular mechanisms and potential therapeutic target',\n#  'doi': '10.1007\/s00134-020-05985-9',\n#  'pmcid': None,\n#  'pubmed_id': '32125455',\n#  'license': 'cc-by-nc',\n#  'abstract': None,\n#  'publish_time': '2020',\n#  'authors': 'Zhang, Haibo; Penninger, Josef M.; Li, Yimin; Zhong, Nanshan; Slutsky, Arthur S.',\n#  'journal': 'Intensive Care Med',\n#  'Microsoft Academic Paper ID': '2002765492',\n#  'WHO #Covidence': '#3252',\n#  'has_full_text': 'True'}\n\n\n\n# # meta_sdf.first().asDict()\n\n# # for k,v in meta_sdf.first().items():\n# #     print(f'{k}:\\t{v}')\n\n\n# temp = meta_sdf.first().asDict()\n\n# for k,v in temp.items():\n#     print(f'{k}:\\t{v}')  \n    \n    \n    \n# for k,v in tips.items():\n#     print(f'{k}:\\t{v}')\n\n\n\n# meta_sdf.first().asDict()\n# {'sha': 'c630ebcdf30652f0422c3ec12a00b50241dc9bd9',\n#  'source_x': 'CZI',\n#  'title': 'Angiotensin-converting enzyme 2 (ACE2) as a SARS-CoV-2 receptor: molecular mechanisms and potential therapeutic target',\n#  'doi': '10.1007\/s00134-020-05985-9',\n#  'pmcid': None,\n#  'pubmed_id': '32125455',\n#  'license': 'cc-by-nc',\n#  'abstract': None,\n#  'publish_time': '2020',\n#  'authors': 'Zhang, Haibo; Penninger, Josef M.; Li, Yimin; Zhong, Nanshan; Slutsky, Arthur S.',\n#  'journal': 'Intensive Care Med',\n#  'Microsoft Academic Paper ID': '2002765492',\n#  'WHO #Covidence': '#3252',\n#  'has_full_text': 'True'}\n\n\n# for item in row: print(item)\n    \n# for item in row.asDict():\n#     print(item)\n\n# for item in row.asDict().keys():\n#     print(item)\n    \n    \n    \n# for item in row.asDict().keys():\n# print(item)\n    \n    \n    \n    \n    \n# for item in row: print(item)\n    \n# for item in row.asDict():\n#     print(item)\n\n# for item in row.asDict().keys():\n#     print(item)\n    \n    \n    \n    \n# meta_sdf.first().asDict()\n\n# for k,v in meta_sdf.first().items():\n#     print(f'{k}:\\t{v}')\n\n# temp = meta_sdf.first().asDict()\n\n# for k,v in temp.items():\n#     print(f'{k}:\\t{v}')  \n\n# sha:\tc630ebcdf30652f0422c3ec12a00b50241dc9bd9\n# source_x:\tCZI\n# title:\tAngiotensin-converting enzyme 2 (ACE2) as a SARS-CoV-2 receptor: molecular mechanisms and potential therapeutic target\n# doi:\t10.1007\/s00134-020-05985-9\n# pmcid:\tNone\n# pubmed_id:\t32125455\n# license:\tcc-by-nc\n# abstract:\tNone\n# publish_time:\t2020\n# authors:\tZhang, Haibo; Penninger, Josef M.; Li, Yimin; Zhong, Nanshan; Slutsky, Arthur S.\n# journal:\tIntensive Care Med\n# Microsoft Academic Paper ID:\t2002765492\n# WHO #Covidence:\t#3252\n# has_full_text:\tTrue\n    \n    \n# meta_sdf.first().asDict()\n\n# for k,v in meta_sdf.first().items():\n#     print(f'{k}:\\t{v}')\n\n# temp = meta_sdf.first().asDict()\n\n# for k,v in temp.items():\n#     print(f'{k}:\\t{v}') \n# sha:\tc630ebcdf30652f0422c3ec12a00b50241dc9bd9\n# source_x:\tCZI\n# title:\tAngiotensin-converting enzyme 2 (ACE2) as a SARS-CoV-2 receptor: molecular mechanisms and potential therapeutic target\n# doi:\t10.1007\/s00134-020-05985-9\n# pmcid:\tNone\n# pubmed_id:\t32125455\n# license:\tcc-by-nc\n# abstract:\tNone\n# publish_time:\t2020\n# authors:\tZhang, Haibo; Penninger, Josef M.; Li, Yimin; Zhong, Nanshan; Slutsky, Arthur S.\n# journal:\tIntensive Care Med\n# Microsoft Academic Paper ID:\t2002765492\n# WHO #Covidence:\t#3252\n# has_full_text:\tTrue\n    \n    \n# <div class=\"h4\"><i>Let's examine the actual metadata file:<\/i><\/div>\n# * abc\n\n#----------------------------------------------------------------------------------------------------------------------------------\n# SUPERCOUNTS:\n#    meta_sdf.select('doi').distinct().count()\n#    meta_sdf.select('journal').distinct().count()\n#    meta_sdf.select('publish_time').distinct().show()  < - - MAKE A DATA FORM ! \n#    meta_sdf.count()\n#    meta_sdf.filter(meta_sdf.has_full_text ==\"True\").count()\n#    meta_sdf.select('publish_time').distinct().show()\n#    meta_sdf.filter(meta_sdf.has_full_text ==\"False\").count()\n#    meta_sdf.count()\n#    meta_sdf.select('journal').distinct().show(300,False)\n#----------------------------------------------------------------------------------------------------------------------------------\n# for dirname, _, filenames in os.walk('\/kaggle\/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# \/kaggle\/input\/CORD-19-research-challenge\/2020-03-13\/json_schema.txt\n# \/kaggle\/input\/CORD-19-research-challenge\/2020-03-13\/COVID.DATA.LIC.AGMT.pdf\n# \/kaggle\/input\/CORD-19-research-challenge\/2020-03-13\/all_sources_metadata_2020-03-13.csv\n# \/kaggle\/input\/CORD-19-research-challenge\/2020-03-13\/all_sources_metadata_2020-03-13.readme\n# \/kaggle\/input\/CORD-19-research-challenge\/2020-03-13\/comm_use_subset\/comm_use_subset\/25621281691205eb015383cbac839182b838514f.json\n#----------------------------------------------------------------------------------------------------------------------------------\n#  link to live jupyter:\n#    https:\/\/kkb-production.jupyter-proxy.kaggle.net\/k\/30518687\/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2IiwidHlwIjoiSldUIn0..MslS0K3Jv0xA9ckYVozGYg.Bh-iEBt7HITa9f1OAw1uQ1dE-Xr_oHQ9_1vYT2tx6CLnuffbRwMHpA1jWHuijs0-zQWj4Ql1NGxbwt9m7kB6LITko_lrWyIMxxToyPH8bK4.RN73ZD5-RWss9nqcYb5e_A\/proxy\/tree\n#\n#\n#  \n#    MY LIVE JUPYTER NOTEBOOK:  ( I CAN SAVE \/ ACCESS FILES HERE )\n#    https:\/\/www.kaggle.com\/tombresee\/ng-eda-covid-19\/edit\/run\/30478508\n#  \n# \n#\n# from IPython.display import FileLink\n# FileLink(r'whatswrong\/whatswrong')\n#\n#\n# master_file_path = '\/kaggle\/input\/CORD-19-research-challenge\/2020-03-13'\n# meta_file_loc = f'{master_file_path}\/all_sources_metadata_2020-03-13.csv'\n# pdf = pd.read_csv(meta_file_loc)\n# len(pdf)\n#\n#\n#\n# import os\n# for dirname, _, filenames in os.walk('\/kaggle\/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n#\n#\n# from IPython.display import HTML\n# import pandas as pd\n# import numpy as np\n\n\n# # df = pd.DataFrame(np.arange(1000), columns=['data'])\n\n# # df.to_csv('submission.csv')\n\n# def create_download_link(title = \"Download CSV file\", filename = \"data.csv\"):  \n#     html = '<a href={filename}>{title}<\/a>'\n#     html = html.format(title=title,filename=filename)\n#     return HTML(html)\n\n# # create a link to download the dataframe which was saved with .to_csv method\n# create_download_link(filename='whatswrong.csv')\n\n        \n#######################################################################################################################################\n#  https:\/\/www.kaggle.com\/shivamb\/interactive-d3-js-visualisations-in-kaggle-kernels#Interactive-D3.js-Visualisations-in-Kaggle-Kernels\n#######################################################################################################################################\n\n# h = display(HTML(htmlt1))\n# j = IPython.display.Javascript(js_t1)\n# IPython.display.display_javascript(j)\n\n#  meta_sdf.describe(['has_full_text']).show()\n\n\n\n\n# (1) Metadata for papers from these sources are combined: CZI, PMC, BioRxiv\/MedRxiv. (total records 29500)\n# \t- CZI 1236 records\n# \t- PMC 27337\n# \t- bioRxiv 566\n# \t- medRxiv 361\n# (2) 17K of the records have PDFs and the hash of the PDFs are in 'sha'\n# (3) For PMC sourced papers, one paper's metadata can be associated with one or more PDFs\/shas under that paper - a PDF\/sha correponding to the main article, and possibly additional PDF\/shas corresponding to supporting materials for the article.\n# (4)\t13K of the PDFs were processed with fulltext ('has_full_text'=True)\n# (5) Various 'keys' are populated with the metadata:\n# \t- 'pmcid': populated for all PMC paper records (27337 non null)\n# \t- 'doi': populated for all BioRxiv\/MedRxiv paper records and most of the other records (26357 non null)\n# \t- 'WHO #Covidence': populated for all CZI records and none of the other records (1236 non null)\n# \t- 'pubmed_id': populated for some of the records\n# \t- 'Microsoft Academic Paper ID': populated for some of the records\n\n\n\n# 2020-03-20\n# ---CHANGES---\n# * normalized doi, authors, title, abstract strings\n# * merged redundant rows in metadata file on doi, pmcid, and pubmed_id\n# * metadata sha column can now include multiple files (some PMC files have multiple associated PDFs)\n# * added column in metadata file \"full_text_file\" to signal the tar.gz file in which the full text json resides\n# ---SUMMARY---\n# total metadata rows: 44220\n# custom_license: 16959 full text (new: 15533)\n# noncomm_use_subset: 2353 full text (new: 385)\n# comm_use_subset: 9118 full text (new: 128)\n# biorxiv_medrxiv: 885 full text (new: 110)\n    \n    \n    \n#     Collecting pyspark\n#   Downloading pyspark-2.4.5.tar.gz (217.8 MB)\n#      |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 217.8 MB 4.5 kB\/s  eta 0:00:01\n# Collecting py4j==0.10.7\n#   Downloading py4j-0.10.7-py2.py3-none-any.whl (197 kB)\n#      |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 197 kB 36.7 MB\/s eta 0:00:01\n# Building wheels for collected packages: pyspark\n#   Building wheel for pyspark (setup.py) ... done\n#   Created wheel for pyspark: filename=pyspark-2.4.5-py2.py3-none-any.whl size=218258791 sha256=e472a65288dc2910642aabb9c044a1cc9d345adbb23c2a068bb2ddcc44b1fd40\n#   Stored in directory: \/root\/.cache\/pip\/wheels\/84\/30\/e3\/c51c5cd0229631e662d29d7b578a3e5949a4c8db033ffb70aa\n# Successfully built pyspark\n# Installing collected packages: py4j, pyspark\n# Successfully installed py4j-0.10.7 pyspark-2.4.5\n\n\n\n#   --- UPDATED --- \n# import os\n# for dirname, _, filenames in os.walk('\/kaggle\/'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n        \n# \/kaggle\/lib\/kaggle\/gcp.py\n# \/kaggle\/input\/CORD-19-research-challenge\/metadata.csv\n# \/kaggle\/input\/CORD-19-research-challenge\/json_schema.txt\n# \/kaggle\/input\/CORD-19-research-challenge\/metadata.readme\n# \/kaggle\/input\/CORD-19-research-challenge\/COVID.DATA.LIC.AGMT.pdf\n# \/kaggle\/input\/CORD-19-research-challenge\/noncomm_use_subset\/noncomm_use_subset\/d99acb4e99be7852aa61a688c9fbd38d44b5a252.json\n\n\n\n\n# (1) Metadata for papers from these sources are combined: CZI, PMC, BioRxiv\/MedRxiv. (total records 29500)\n# \t- CZI 1236 records\n# \t- PMC 27337\n# \t- bioRxiv 566\n# \t- medRxiv 361\n# (2) 17K of the records have PDFs and the hash of the PDFs are in 'sha'\n# (3) For PMC sourced papers, one paper's metadata can be associated with one or more PDFs\/shas under that paper - a PDF\/sha correponding to the main article, and possibly additional PDF\/shas corresponding to supporting materials for the article.\n# (4)\t13K of the PDFs were processed with fulltext ('has_full_text'=True)\n# (5) Various 'keys' are populated with the metadata:\n# \t- 'pmcid': populated for all PMC paper records (27337 non null)\n# \t- 'doi': populated for all BioRxiv\/MedRxiv paper records and most of the other records (26357 non null)\n# \t- 'WHO #Covidence': populated for all CZI records and none of the other records (1236 non null)\n# \t- 'pubmed_id': populated for some of the records\n# \t- 'Microsoft Academic Paper ID': populated for some of the records\n\n\n\n\n# 2020-03-20\n# ---CHANGES---\n# * normalized doi, authors, title, abstract strings\n# * merged redundant rows in metadata file on doi, pmcid, and pubmed_id\n# * metadata sha column can now include multiple files (some PMC files have multiple associated PDFs)\n# * added column in metadata file \"full_text_file\" to signal the tar.gz file in which the full text json resides\n# ---SUMMARY---\n# total metadata rows: 44220\n# custom_license: 16959 full text (new: 15533)\n# noncomm_use_subset: 2353 full text (new: 385)\n# comm_use_subset: 9118 full text (new: 128)\n# biorxiv_medrxiv: 885 full text (new: 110)\n\n\n# meta_sdf.filter((meta_sdf.sha)>40).show()\n\n\n\n\n \n#\n#\n#      COMMANDS: \n#                   meta_sdf.select(\"sha\").show(10,False)\n#\n#                   meta_sdf.collect()[221].asDict()\n#\n#                   meta_sdf.select(\"doi\").distinct().show(10,False)\n#\n#                   meta_sdf.select(\"sha\").filter(lamdba string: len(string)>40).show()\n#\n#                   meta_sdf.count()\n#\n#  meta_sdf.select('sha').describe().show(100,False)\n# +-------+----------------------------------------+\n# |summary|sha                                     |\n# +-------+----------------------------------------+\n# |count  |28512                                   |\n# |mean   |null                                    |\n# |stddev |null \n\n\n\n\n# # meta_sdf.filter(size(col(meta_sdf.sha)) > 40)\n# meta_sdf.where(size(col(meta_sdf.sha)) >=40).show()\n\n\n\n\n# add this:\n# <br><br><br>\n# <div class=\"h4\"><i>External References:<\/i><\/div>\n\n# ```meta_sdf.filter(meta_sdf.sha == 'aecbc613ebdab36753235197ffb4f35734b5ca63')```\n \n    \n    \n    \n    \n# add this ! ! ! ! ! !   and reuse: \n# meta_sdf.select('source_x').distinct().describe().show(100,False)\n\n# sdf.select('license').distinct().show(100,False)\n\n# sdf.select('full_text_file').distinct().show(100,False)\n\n# sdf.select('authors').distinct().show(1000,False)\n\n\n\n# issues, follow up: \n# sdf.select('journal').distinct().orderBy(sdf.journal.desc()).show(100, False)\n\n# sdf.select('journal').distinct().orderBy('journal').show(100, False)\n\n# sdf.select('journal').distinct().sort('journal', ascending=True).show(1000, False)   < - - - \n\n# sdf.select('journal').distinct().sort('journal', ascending=True).count()\n\n\n\n\n\n\n\n#++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n# counter = 0\n# import os\n# for dirname, _, filenames in os.walk('\/kaggle\/input'):\n#     for filename in filenames:\n#         counter = counter + 1 \n#         #print(os.path.join(dirname, filename))\n# print(counter)\n#++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n\n\n\n\n\n\n\n# ! cat \/kaggle\/input\/CORD-19-research-challenge\/comm_use_subset\/comm_use_subset\/a137eb51461b4a4ed3980aa5b9cb2f2c1cf0292a.json\n\n\n\n\n\n\n\n\n\n#https:\/\/stackoverflow.com\/questions\/49432167\/how-to-convert-rows-into-dictionary-in-pyspark\n\n\n\n\n\n\n\n#++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n#sample.select(\"metadata\").show(100, False)\n\n# df_list_of_dict = [row.asDict() for row in sample.collect()]\n# df_list_of_dict\n\n\n# for i in df_list_of_dict: \n#     print(\"\\n\", i)\n#++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n    \n\n\n# HELPER-X\n# import pyspark.sql.functions as F\n# # new col with length of string of title (number of characters)\n# tdf.withColumn('NewC', F.length('title')).show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# HELPER-X:  display df pretty \n# display(tdf.select('title').toPandas().style.hide_index())\n# HUGE OUTPUT *** \n\n\n\n\n\n# # HELPER-X:  html output \n# tempA = pd.DataFrame([100,200,300])\n# # from IPython.display import HTML\n# # HTML(tempA.to_html(index=False))\n# # 0\n# # 100\n# # 200\n# # 300\n# #tempA.style.hide_index()\n\n\n\n\n\n\n# tdf.show(n=2, truncate=False, vertical=True)\n# -RECORD 0-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n#  sha            | null                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      \n#  src            | Elsevier                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  \n#  title          | Intrauterine virus infections and congenital heart disease                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                \n#  doi            | 10.1016\/0002-8703(72)90077-4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \n#  abstract       | Abstract The etiologic basis for the vast majority of cases of congenital heart disease remains largely undefined. Viruses have been considered to be likely candidates since the recognition of the association between intrauterine rubella and congenital heart disease. Although the pathogenesis of cardiovascular defects is poorly understood, information gained from the study of congenital rubella syndrome suggests that mechanisms such as focal endothelial cell damage, resulting in obliteration of vascular supply, decreased growth rate, and shortened survival time of certain cells, and disturbed DNA replication in cells whose chromosomes were damaged secondary to the effects of virus replication may be operative in the production of defects in the developing fetus. In addition to rubella there is suggestive, but not conclusive, evidence that Coxsackie B3 and B4 virus infections during pregnancy can result in the birth of infants with a variety of types of congenital heart lesions and that intrauterine mumps virus infection may be etiologically related to the postnatal development of endocardial fibroelastosis (EFE). Although there are a number of other viruses that are potential etiologic agents of congenital heart disease, the current status of information is inadequate to allow even suggestive associations to be made. The most profitable areas for future investigation appear to be: (1) the epidemiology of congenital heart disease, (2) prospective studies of the association of maternal viral infection with abnormal offspring, (3) the in-depth virologic investigation of the infant with a cardiac defect, and (4) the development of experimental animal models of congenital heart disease. Successful control of virus-induced congenital heart disease will depend on the results of these investigations and the development of vaccines against the identified causative viruses and\/or safe and effective antiviral chemotherapy for the woman in early gestation who is infected with a known teratogenic agent. \n#  published_date | 1972-12-31                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                \n#  authors        | Overall, James C.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         \n#  journal        | American Heart Journal                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    \n#  has_full_text  | False                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \n#  full_text_file | custom_license                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \n# -RECORD 1-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n#  sha            | null                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      \n#  src            | Elsevier                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  \n#  title          | Coronaviruses in Balkan nephritis                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         \n#  doi            | 10.1016\/0002-8703(80)90355-5                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \n#  abstract       | null                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      \n#  published_date | 1980-03-31                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                \n#  authors        | Georgescu, Leonida; Diosi, Peter; Bu\u0163iu, Ioan; Plavo\u015fin, Livia; Herzog, Georgeta                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          \n#  journal        | American Heart Journal                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    \n#  has_full_text  | False                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \n#  full_text_file | custom_license                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \n# only showing top 2 rows","76b9d749":"#  Doing things you may need later:\n\n\n\n#************************************************************************************************\n# df = df.withColumn('wordCount', f.size(f.split(f.col('Description'), ' ')))\n# df.show()\n#+-------------------+-----------------+---------+\n#|              Dates|      Description|wordCount|\n#+-------------------+-----------------+---------+\n#|2015-05-14 03:53:00|   WARRANT ARREST|        2|\n#|2015-05-14 03:53:00|TRAFFIC VIOLATION|        2|\n#|2015-05-14 03:33:00|TRAFFIC VIOLATION|        2|\n#+-------------------+-----------------+---------+\n\ndf = tdf.withColumn('wordCountfromTitle', F.size(F.split(F.col('title'), ' ')))\n\ndf.show()\n\n\n\n","8694bb1f":"<div class=\"h3\"><i>Metadata (metadata.csv):<\/i><\/div> \nLet's examine the `metadata.csv` master file (previously named `all_sources_metadata_2020-03-13.csv`,   \nbut was updated 3-20-2020) that is included in our datasets  \n&ensp;  -  Renamed a few columns for clarity   \n&ensp;  -  A random data record sample from that file is shown below: ","550ac767":"<div class=\"h3\"><i>About Me:<\/i><\/div>\n* **Name:** &ensp; &ensp; &nbsp; &nbsp; Tom Bresee\n* **Location:** &nbsp;&nbsp; &nbsp;  Frisco, TX, USA","97148d30":"Now we can extract precisely which text we want, based on a fundamental dictionary-like Spark dataframe\n\nUltimately we do not care much about the excessive data such as BIB entries and ref_entries in the json files, we care way more about the core elements such as Title, Abstract, Body of Text, Article Text, etc","5013592c":"<a id='bkground'><\/a>\n<div class=\"h2\"><i>NG-EDA<\/i><\/div>\n<div class=\"h3\">Exploratory Data Analysis and Examination:<\/div>\n<div class=\"h3\"><i>COVID-19 Open Research Dataset (CORD-19)<\/i><\/div>\n<br>","ee8bc3c1":"<br><hr><br>\n\n<div class=\"h3\"><i>Examining JSON Files<\/i><\/div>\n\n* Let's look at the raw json data files ","65335819":"<div class=\"h3\"><i>Approach:<\/i><\/div>\n* Emphasis on _extremely_ clear visualization\n* New and innovative approaches to examining our data\n* Get beyond surface analysis and probe for true insights into this data \n* Practical data science\n* Utilization of **Apache Spark** as core filtering mechanism\n  * Thus most of our data will be in *pyspark.sql.dataframe.DataFrame* type format\n  * Need to do deep dive via NLP\/NLTK\n  * Working directly with Apache Spark is not simple (it can get quite complex and has a fairly steep learning curve), it is quite a bit more powerful than pandas, etc, but it is worth the effort, as we will gain added flexibility down the line if our dataset also increases exponentially...\n ","f5982ecc":"<div class=\"h4\"><i> - Observations:<\/i><\/div>\n* 28,450 of the entries have `sha` populated\n* The `sha` entry **may** contain more than a single sha entry, i.e. it may have sha1 ; sha2 (separated by a `;`)\n\n* The original metadata column `publish_time` is a bit confusing, I renamed it to `published_date`, as this is a physical date of publication and not a time per se. People will understand published date verbage easier. \n* 96% of the `WHO #Covidence` column data is missing, will delete this column\n* 98% of the `Microsoft Academic Paper ID` column data is missing (or not applicable), will delete this column \n* 40,750 of the entries have a `doi` populated\n* 42,230 unique paper titles \n* If one needs to sort based on the publication date, will need to convert the `published_date` to datetime format, as it defaults to string...\n ","1b42558a":"Abstract:","a199d7fe":"One of the Authors full name: ","b3d84316":"Let's parse thru some of the exact specifics of what we are looking for ... ","6a3a1e41":"<div class=\"h3\"><i>Subject Matter Domain\/Expertise:<\/i><\/div>\n* This data contains references to medical journals and text\n ","cbc7e60a":"<div class=\"h3\"><i>Our Directories and Files:<\/i><\/div>\n<br>\n**main directories:**  \n&ensp; biorxiv_medrxiv\/  \n&ensp;  comm_use_subset\/  \n&ensp;  custom_license\/  \n&ensp;  noncomm_use_subset\/  \n\n\n**root files: **   \n&ensp; &ensp; COVID.DATA.LIC.AGMT.pdf  \n&ensp; &ensp; json_schema.txt  \n&ensp; &ensp; metadata.csv  \n&ensp; &ensp; metadata.readme  \n","b6acf021":"... more to follow, I'm just getting started ...","08d4c97b":"<div class=\"h4\"><i> - Metafile Header Breakout:<\/i><\/div>\n**sha**  \n&ensp;&ensp; Secure Hash Algorithm (SHA) hash key (not applicable for our analysis)  \n&ensp;&ensp; 17K of the records have PDFs and the **hash** of the PDFs are in this column  \n&ensp;&ensp; SHA-1 encryption calculates a numeric footprint of 40 hexadecimal characters\n\n\n**src**  \n&ensp;&ensp; Source:  Either `PMC` (PubMed Central), `CZI` (Chan Zuckerberg Initiative), `bioRxiv`,   \n&ensp;&ensp; `Elsevier`, `medRxiv`, or `WHO` \n\n**title**  \n&ensp;&ensp; Actual Document Title (title of the work\/publication)\n \n**doi**  \n&ensp;&ensp; Digital Object Identifier  (almost always starts with 10.)  \n&ensp;&ensp; String of numbers\/letters\/symbols to permanently identify article    \n&ensp;&ensp; In theory, if they are truly unique identifiers, will allow to determine total doc unique count\n \n**pmcid**  \n&ensp;&ensp; pmcid, populated for all PMC paper records (example: `PMC3093355`)\n\n**pubmed_id**  \n&ensp;&ensp;  pubmet id (example: `32125455`) \n \n**license**  \n&ensp;&ensp; License\n \n**abstract**  \n&ensp;&ensp; Paper abstract \n \n**publish_time**  \n&ensp;&ensp; Publish Time (example: `2019 Nov 11`)  \n&ensp;&ensp; Note:  Some of the articles are labelled with only the year (`2020`)\n\n**authors**  \n&ensp;&ensp; Name of Author(s)\n\n**journal**  \n&ensp;&ensp; Journal Name  \n \n**Microsoft Academic Paper ID**  \n&ensp;&ensp; Microsoft Academic Paper ID (populated for some of the records)\n \n**WHO #Covidence**  \n&ensp;&ensp; WHO #Covidence (example: `#5386`)\n\n**has_full_text**  \n&ensp;&ensp; Does .json have full text ? (`True` or `False`)\n\n**full_text_file (new)**   \n&ensp;&ensp; Signals the tar.gz file in which the full text json resides\n\n\n\n","73173f3c":"<div class=\"h4\"><i> - Extensions:<\/i><\/div>\n* Now that we have a core approach for building out the json files to Apache Spark Dataframes, we can also if needed\/required **export all of the raw data to a remote MongoDB database**, using [this](https:\/\/www.mongodb.com\/products\/spark-connector) core connector\n\n* And thus we can extend a [notebook](https:\/\/github.com\/tombresee\/Prairie-Fire\/blob\/master\/ENTER\/ApacheSparkMongoConnector.ipynb) I created for exporting Spark information directly to a Mongo database.  Yet another added benefit of using our Spark-based approach, as we can leverage the flexibility that Mongo has especially with unstructured data... \n\n\n\n","09763c23":"<div class=\"h4\"><i> - Actual JSON files:<\/i><\/div>\n\n* **Total**:  29,315 unique .json files, spread among four subsets\n  * **1 - custom_license**: 16959 full text (new: 15533)\n  * **2 - noncomm_use_subset**: 2353 full text (new: 385)\n  * **3 - comm_use_subset**: 9118 full text (new: 128)\n  * **4 -biorxiv_medrxiv**: 885 full text (new: 110)\n  * I've created a full list of the dataset .json files =>  &ensp; [here](https:\/\/raw.githubusercontent.com\/tombresee\/COVID-19\/master\/ENTER\/all.txt)\n    * READ:  Every .json file is named by its sha hash key ! \n\n ","00ea5d8b":"Paper ID:  \nRecognize this id ?  It is the sha entry from the metafile entry above :) ","68343ed5":"Title:  (matches the name in our first table above)","007b8b56":"<div class=\"h4\"><i> - Condensation:<\/i><\/div>\n\n* Thus, we have taken a single json sample like [this](https:\/\/raw.githubusercontent.com\/tombresee\/COVID-19\/master\/ENTER\/json_sample.txt), and are now able to parse through specifics of the json file to get to what we really want, i.e. [this](https:\/\/raw.githubusercontent.com\/tombresee\/COVID-19\/master\/ENTER\/condensed.txt)\n\n","441f38af":"<div class=\"h4\"><i> - Observations:<\/i><\/div>\n* I find it surprising how much of the json file is taken up by Bibliographic References  (bib entries) and figure references\n ","9e0f79a3":"<div class=\"h4\"><i> - Metadata File:<\/i><\/div>\n* Let's start by going back to the metadata.csv file and breaking out word counts","5dc5f60e":"<br><hr><br>\n\n<div class=\"h3\"><i>NLP:  Word Counts<\/i><\/div>\n\nLet's start examining words","ad705e62":"<div class=\"h4\"><i> - Next Steps:<\/i><\/div>\n\n* Now that we have an algorithm for breaking out the json files to their text particulars, we will parse through all of the titles, article text, and medical verbage \n* Generate word-counts, noun phrases, sentiment, etc, which Apache Spark is spectacular at...","0df7bef1":"<div class=\"h4\"><i> -  Deconstructing a <u>single<\/u> json file sample to get a feel for the data:<\/i><\/div>\n<br>\n\nI have found the correlated .json file associated with the metadata.csv entry we examined above (i.e.  \n`The effect of inhibition of PP1 and TNF\u03b1 signaling on pathogenesis of SARS coronavirus`).    \nWe will decomposed this .json file so we can extract text, etc.  \n\nIn other words:  We have previously examined the meta file, found a set entry, now we will actually examine the core json file entry associated with that metadata entry.  Thus we will be able to see how the metadata file had 'meta' information about the json 'paper' file...\n\n","d7862177":"\n<div class=\"h4\"><i> -  Spark Schema:<\/i><\/div>\n\n  * Read my .json Spark inferred schema =>  &ensp; [here](https:\/\/raw.githubusercontent.com\/tombresee\/COVID-19\/master\/ENTER\/spark_json_schema_sample.txt)\n\n","94ebaa55":"<br><br><br><br><br><br><br><br><br><br><br><br>\n\n<div class=\"alert alert-warning\">\n<br>\n<b>Warning:<\/b>  &ensp; Medical information enclosed in the dataset files may sometimes have typos, mistakes, incorrect entries, or entries that were meant to be inserted into other columns.  It is important when filtering to keep your eye on this. \n<br><br>\n\n<b>Note:<\/b> &ensp; Reading in .csv files <u>only works well when the source file was created with proper utilization of the `,` as the delimiter<\/u>.  IF the comma is used to separate authors in a particular way, it will in fact skew the row of data.  It appears the source .csv files in some location did in fact enter author data but instead of using the `;` to separate multiple authors under the author column, `,` was used instead !   \n \n \n<br>\n\n<\/div>","3723d78b":"Deconstructing Apache Spark dataframe elements:","89eed169":"<div class=\"h4\"><i> - Summary of metadata file (for updated dataset):<\/i><\/div>\n<br>\n&ensp; **Number of raw record entries:** &ensp; 44,230    \n&ensp; **Number of unique journals quoted:** &ensp; 3,945  \n&ensp; **Number of populated sha entries:** &ensp; 28,450","f9e979b8":"Body Text (single snippet of many):"}}