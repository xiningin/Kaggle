{"cell_type":{"1c2fdf80":"code","99d88716":"code","bed409e3":"code","1647f707":"code","3af78e8d":"code","74688db3":"code","4caba519":"code","8cfdaff3":"code","84094d28":"code","6d9d9e0a":"code","49438bc2":"code","4baf1d65":"code","f68a2e02":"code","728e7982":"code","1f1b0fae":"code","54543a17":"code","ede86df4":"code","224a61ad":"code","95cdc638":"code","483bc722":"code","3633905c":"code","f27307ec":"code","d88727a0":"code","56da2ba9":"code","128dbb8a":"code","33e95cb5":"code","b844c8a9":"code","619efa74":"code","af349954":"code","bc747b46":"code","8a6711d5":"code","245a0550":"code","64c2258f":"code","e0da694c":"code","3d081291":"code","dacac508":"code","3f298949":"code","43b85a68":"code","df57f4ac":"code","ba9507da":"code","c50ed796":"code","1a69b684":"code","aff5ca18":"code","2a4c5286":"code","7e073969":"code","db66b039":"code","f98b9627":"code","22e0b528":"code","dc6feb08":"code","9f1c1b49":"code","c6d2eed8":"code","794ff8c4":"code","2f23d0c7":"code","cd1b64da":"code","d65896a1":"code","160ee777":"code","1fc46c0f":"code","7155f63a":"code","64641eb7":"code","4dbf4187":"code","9a9c64d4":"code","79142def":"code","edb6a148":"code","70f9121f":"code","3b5ce5f8":"code","1ecb162d":"code","83e6db2b":"code","303dcb21":"code","c4f322cf":"code","68964d4e":"code","46bbd074":"code","fa00db26":"code","8b0d80c6":"markdown","889bbb0e":"markdown","17bfd601":"markdown","86de39b2":"markdown","a356d55a":"markdown","efc32012":"markdown","cd4ed3c9":"markdown","b850a378":"markdown","09dc91b3":"markdown","c19e4120":"markdown"},"source":{"1c2fdf80":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport zipfile\nimport cv2\nimport plotly.express as px\nimport tensorflow as tf\nfrom tensorflow.python.keras import Sequential\nfrom tensorflow.keras import layers, optimizers\nfrom tensorflow.keras.applications.resnet50 import ResNet50\nfrom tensorflow.keras.layers import Input, Add, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D, AveragePooling2D, MaxPooling2D, Dropout\nfrom tensorflow.keras.models import Model, load_model\nfrom tensorflow.keras.initializers import glorot_uniform\nfrom tensorflow.keras.utils import plot_model\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint, LearningRateScheduler\nfrom IPython.display import display\nfrom tensorflow.keras import backend as K\nfrom sklearn.preprocessing import StandardScaler, normalize\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA\nfrom sklearn.metrics import silhouette_score\nfrom sklearn.manifold import TSNE\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport plotly.express as px\nimport plotly.graph_objects as go\n\n%matplotlib inline","99d88716":"sales_df = pd.read_csv('..\/input\/sample-sales-data\/sales_data_sample.csv', encoding = 'unicode_escape')\n\n# Note: MSRP is the manufacturer's suggested retail price (MSRP) or sticker price represents the suggested retail price of products. \n# MSRP is used to standardize the price of products over multiple company store locations.","bed409e3":"sales_df","1647f707":"# Let's view the types of data\nsales_df.dtypes","3af78e8d":"# Convert order date to datetime format\nsales_df['ORDERDATE'] = pd.to_datetime(sales_df['ORDERDATE'])\n\n# Check the type of data\nsales_df.dtypes","74688db3":"# Check the number of non-null values in the dataframe\nsales_df.info()","4caba519":"# Check the number of Null values in the data\nsales_df.isnull().sum()","8cfdaff3":"# since there are lot of Null values in 'addressline2', 'state', 'postal code' and 'territory' we can drop them. \n# Country would represent the order grographical information.\n# Also we can drop city, address1, phone number, contact_name, contact last_name and contact first_name since they are not required for the analysis\n\ndf_drop  = ['ADDRESSLINE1', 'ADDRESSLINE2', 'POSTALCODE', 'CITY', 'TERRITORY', 'PHONE', 'STATE', 'CONTACTFIRSTNAME', 'CONTACTLASTNAME', 'CUSTOMERNAME', 'ORDERNUMBER']\nsales_df = sales_df.drop(df_drop, axis = 1)\nsales_df.head()","84094d28":"sales_df.isnull().sum()","6d9d9e0a":"# Obtain the number of unique values in each column\nsales_df.nunique()","49438bc2":"sales_df['COUNTRY'].value_counts().index","4baf1d65":"sales_df['COUNTRY'].value_counts()","f68a2e02":"# Function to visulize the count of items in a given column\n# Note that Plotly is a Python graphing library that makes interactive, publication-quality graphs. \n\n\ndef barplot_visualization(x):\n  fig = plt.Figure(figsize = (12, 6))\n  fig = px.bar(x = sales_df[x].value_counts().index, y = sales_df[x].value_counts(), color = sales_df[x].value_counts().index, height = 600)\n  fig.show()","728e7982":"# Let's call this function for any given column such as 'COUNTRY'\nbarplot_visualization('COUNTRY')","1f1b0fae":"# Pass status column to the function \nbarplot_visualization('STATUS')","54543a17":"# Lets drop the status Column and save that to our original CSV File\nsales_df.drop(columns= ['STATUS'], inplace = True)\nsales_df","ede86df4":"barplot_visualization('PRODUCTLINE')","224a61ad":"barplot_visualization('DEALSIZE')","95cdc638":"# Function to add dummy variables to replace categorical variables\n\ndef dummies(x):\n  dummy = pd.get_dummies(sales_df[x])\n  sales_df.drop(columns = x , inplace = True)\n  return pd.concat([sales_df, dummy], axis = 1)","483bc722":"# Let's obtain dummy variables for the column 'COUNTRY'\nsales_df = dummies('COUNTRY')\nsales_df","3633905c":"sales_df = dummies('PRODUCTLINE')","f27307ec":"sales_df","d88727a0":"sales_df = dummies('DEALSIZE')\nsales_df","56da2ba9":"y = pd.Categorical(sales_df['PRODUCTCODE'])\n\ny","128dbb8a":"y = pd.Categorical(sales_df['PRODUCTCODE']).codes\ny","33e95cb5":"# Since the number unique product code is 109, if we add one-hot variables, there \n# would be additional 109 columns, we can avoid that by using categorical encoding\n# This is not the optimal way of dealing with it but it's important to avoid curse of dimensionality\n\nsales_df['PRODUCTCODE'] = pd.Categorical(sales_df['PRODUCTCODE']).codes","b844c8a9":"sales_df","619efa74":"sales_df","af349954":"# Group data by order date\nsales_df_group = sales_df.groupby(by = \"ORDERDATE\").sum()\nsales_df_group","bc747b46":"fig = px.line(x = sales_df_group.index, y = sales_df_group.SALES, title = 'Sales')\nfig.show()","8a6711d5":"# We can drop 'ORDERDATE' and keep the rest of the date-related data such as 'MONTH'\n\nsales_df.drop(\"ORDERDATE\", axis = 1, inplace = True)\nsales_df.shape","245a0550":"plt.figure(figsize = (20, 20))\ncorr_matrix = sales_df.iloc[:, :10].corr()\nsns.heatmap(corr_matrix, annot = True, cbar = False)","64c2258f":"# It looks like the Quarter ID and the monthly IDs are highly correlated\n# Let's drop 'QTR_ID' (or 'MONTH_ID') \nsales_df.drop(\"QTR_ID\", axis = 1, inplace = True)\nsales_df.shape","e0da694c":"import plotly.figure_factory as ff\n\nplt.figure(figsize = (10, 10))\n\nfor i in range(8):\n  # All Columns except OrderNumber\n  if sales_df.columns[i] != 'ORDERLINENUMBER':\n    fig = ff.create_distplot([sales_df[sales_df.columns[i]].apply(lambda x: float(x))], ['distplot'])\n    fig.update_layout(title_text = sales_df.columns[i])\n    fig.show()","3d081291":"# Visualize the relationship between variables using pairplots\nplt.figure(figsize = (20, 20))\n\nfig = px.scatter_matrix(sales_df,\n    dimensions = sales_df.columns[:8], color = 'MONTH_ID')\n\nfig.update_layout(\n    title = 'Sales Data',\n    width = 1300,\n    height = 1300,\n)\nfig.show()","dacac508":"sales_df","3f298949":"# Scale the data\nscaler = StandardScaler()\nsales_df_scaled = scaler.fit_transform(sales_df)","43b85a68":"sales_df_scaled.shape","df57f4ac":"scores = []\n\nrange_values = range(1, 15)\n\nfor i in range_values:\n  kmeans = KMeans(n_clusters = i)\n  kmeans.fit(sales_df_scaled)\n  scores.append(kmeans.inertia_) # intertia is the Sum of squared distances of samples to their closest cluster center\n\nplt.plot(scores, 'bx-')\nplt.title('Finding right number of clusters')\nplt.xlabel('Clusters')\nplt.ylabel('scores') \nplt.show()","ba9507da":"# Cluster the data using k-means\nkmeans = KMeans(5)\nkmeans.fit(sales_df_scaled)\nlabels = kmeans.labels_","c50ed796":"labels","1a69b684":"kmeans.cluster_centers_.shape","aff5ca18":"# Let's take a look at the cluster centers \ncluster_centers = pd.DataFrame(data = kmeans.cluster_centers_, columns = [sales_df.columns])\ncluster_centers ","2a4c5286":"# In order to understand what these numbers mean, let's perform inverse transformation\n\ncluster_centers = scaler.inverse_transform(cluster_centers)\ncluster_centers = pd.DataFrame(data = cluster_centers, columns = [sales_df.columns])\ncluster_centers","7e073969":"labels.shape","db66b039":"labels.max()","f98b9627":"labels.min()","22e0b528":"y_kmeans = kmeans.fit_predict(sales_df_scaled)\ny_kmeans","dc6feb08":"y_kmeans.shape","9f1c1b49":"# Add a label (which cluster) corresponding to each data point\nsale_df_cluster = pd.concat([sales_df, pd.DataFrame({'cluster':labels})], axis = 1)\nsale_df_cluster","c6d2eed8":"sales_df['ORDERLINENUMBER'] = sales_df['ORDERLINENUMBER'].apply(lambda x: float(x))","794ff8c4":"# plot histogram for each feature based on cluster \n\nfor i in sales_df.columns[:8]:\n  plt.figure(figsize = (30, 6))\n  for j in range(5):\n    plt.subplot(1, 5, j+1)\n    cluster = sale_df_cluster[sale_df_cluster['cluster'] == j]\n    cluster[i].hist()\n    plt.title('{}    \\nCluster - {} '.format(i,j))\n  \n  plt.show()","2f23d0c7":"# Reduce the original data to 3 dimensions using PCA for visualizig the clusters\n\npca = PCA(n_components = 2)\nprincipal_comp = pca.fit_transform(sales_df_scaled)\nprincipal_comp","cd1b64da":"pca_df = pd.DataFrame(data = principal_comp, columns = ['pca1', 'pca2'])\npca_df.head()","d65896a1":"# Concatenate the clusters labels to the dataframe\n\npca_df = pd.concat([pca_df, pd.DataFrame({'cluster':labels})], axis = 1)\npca_df","160ee777":"# Visualize clusters using 3D-Scatterplot\nfig = px.scatter_3d(pca_df, x = 'pca1', y = 'pca2', z = 'pca3', \n                    color = 'cluster', symbol = 'cluster', size_max = 18, opacity = 0.7)\n\nfig.update_layout(margin = dict(l = 0, r = 0, b = 0, t = 0))","1fc46c0f":"# Lets visualize it In 2D Dimension\nax = sns.scatterplot(x = 'pca1', y = 'pca2', hue= 'cluster', data = pca_df, palette =['red', 'green', 'blue','pink','yellow'] )","7155f63a":"sales_df.shape","64641eb7":"# from keras.optimizers import SGD\n\n# Glorot Uniform initializer: https:\/\/keras.rstudio.com\/reference\/initializer_glorot_uniform.html\n\ninput_df = Input(shape = (37,))\nx = Dense(50, activation = 'relu')(input_df)\nx = Dense(500, activation = 'relu', kernel_initializer = 'glorot_uniform')(x)\nx = Dense(500, activation = 'relu', kernel_initializer = 'glorot_uniform')(x)\nx = Dense(2000, activation = 'relu', kernel_initializer = 'glorot_uniform')(x)\nencoded = Dense(8, activation = 'relu', kernel_initializer = 'glorot_uniform')(x)\nx = Dense(2000, activation = 'relu', kernel_initializer = 'glorot_uniform')(encoded)\nx = Dense(500, activation = 'relu', kernel_initializer = 'glorot_uniform')(x)\ndecoded = Dense(37, kernel_initializer = 'glorot_uniform')(x)\n\n# autoencoder\nautoencoder = Model(input_df, decoded)\n\n# encoder - used for dimensionality reduction\nencoder = Model(input_df, encoded)\n\nautoencoder.compile(optimizer = 'adam', loss='mean_squared_error')","4dbf4187":"autoencoder.fit(sales_df, sales_df, batch_size = 128, epochs = 500, verbose = 3)","9a9c64d4":"autoencoder.save_weights('autoencoder_1.h5')","79142def":"pred = encoder.predict(sales_df_scaled)","edb6a148":"scores = []\n\nrange_values = range(1, 15)\n\nfor i in range_values:\n    kmeans = KMeans(n_clusters = i)\n    kmeans.fit(pred)\n    scores.append(kmeans.inertia_)\n\nplt.plot(scores, 'bx-')\nplt.title('Finding right number of clusters')\nplt.xlabel('Clusters')\nplt.ylabel('scores') \nplt.show()","70f9121f":"kmeans = KMeans(3)\nkmeans.fit(pred)\nlabels = kmeans.labels_\ny_kmeans = kmeans.fit_predict(sales_df_scaled)","3b5ce5f8":"df_cluster_dr = pd.concat([sales_df, pd.DataFrame({'cluster':labels})], axis = 1)\ndf_cluster_dr.head()","1ecb162d":"cluster_centers = pd.DataFrame(data = kmeans.cluster_centers_, columns = [sales_df.columns])\ncluster_centers ","83e6db2b":"cluster_centers = scaler.inverse_transform(cluster_centers)\ncluster_centers = pd.DataFrame(data = cluster_centers, columns = [sales_df.columns])\n\ncluster_centers","303dcb21":"\n# plot histogram for each feature based on cluster \nfor i in sales_df.columns[:8]:\n   plt.figure(figsize = (30, 6))\n   for j in range(3):\n        plt.subplot(1, 3, j+1)\n        cluster = df_cluster_dr[df_cluster_dr['cluster'] == j]\n        cluster[i].hist()\n        plt.title('{}    \\nCluster - {} '.format(i,j))\n  \n        plt.show()","c4f322cf":"# Reduce the original data to 3 dimension using PCA for visualize the clusters\npca = PCA(n_components = 3)\nprin_comp = pca.fit_transform(sales_df_scaled)\npca_df = pd.DataFrame(data = prin_comp, columns = ['pca1', 'pca2','pca3'])\npca_df.head()","68964d4e":"pca_df = pd.concat([pca_df, pd.DataFrame({'cluster':labels})], axis = 1)\npca_df.head()","46bbd074":"# Visualize clusters using 3D-Scatterplot\nfig = px.scatter_3d(pca_df, x = 'pca1', y = 'pca2', z = 'pca3',\n              color='cluster', symbol = 'cluster', size_max = 10, opacity = 0.7)\nfig.update_layout(margin = dict(l = 0, r = 0, b = 0, t = 0))","fa00db26":"# Lets visualize it In 2D Dimension\nax = sns.scatterplot(x = 'pca1', y = 'pca2', hue= 'cluster', data = pca_df, palette =['red', 'green', 'blue'] )","8b0d80c6":"We try to discover When doe the sales generally peak (which month)?","889bbb0e":"- From this we can observe that, 5th cluster seems to be forming the elbow of the curve.\n- Note that curve will change everytime we run the cell","17bfd601":"# PERFORM EXPLORATORY DATA ANALYSIS AND DATA CLEANING","86de39b2":"# IMPORTING LIBRARIES AND DATASETS","a356d55a":"- A trend exists between 'SALES' and 'QUANTITYORDERED'  \n- A trend exists between 'MSRP' and 'PRICEEACH'  \n- A trend exists between 'PRICEEACH' and 'SALES'\n- It seems that sales growth exists as we move from 2013 to 2014 to 2015 ('SALES' vs. 'YEAR_ID')\n- zoom in into 'SALES' and 'QUANTITYORDERED', you will be able to see the monthly information color coded on the graph","efc32012":"# APPLY PRINCIPAL COMPONENT ANALYSIS AND VISUALIZE THE RESULTS","cd4ed3c9":"# APPLY AUTOENCODERS (DIMENSIONALITY REDUCTION USING AUTOENCODERS)","b850a378":"#### Let's plot distplots\n - Distplot shows the (1) histogram, (2) kde plot and (3) rug plot.\n  1. Histogram: it's a graphical display of data using bars with various heights. Each bar groups numbers into ranges and taller bars show that more data falls in that range.\n  2. Kde Plot: Kernel Density Estimate is used for visualizing the Probability Density of a continuous variable.\n  3. Rug plot: plot of data for a single quantitative variable, displayed as marks along an axis (one-dimensional scatter plot).","09dc91b3":"# APPLY K-MEANS METHOD","c19e4120":"# Plot the correlation matrix between variables"}}