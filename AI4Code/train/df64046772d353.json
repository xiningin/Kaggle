{"cell_type":{"c3045d4f":"code","884c8c67":"code","944ba209":"code","c4f228e5":"code","7c10bc9a":"code","eb5993e6":"code","b63d8fe9":"code","bc1ffbdc":"code","559e4f16":"code","dab4d09c":"code","9c9123d0":"code","29931997":"code","3ff11993":"code","30b4624a":"code","2b546c16":"code","6ba24420":"code","1068ba00":"code","1c078853":"code","880a4910":"code","3dfe256f":"code","19a4bd14":"code","dc9e6a78":"code","0516b156":"code","563e1603":"code","3d3ba94e":"code","b09dac28":"code","17b62e23":"code","3fec032f":"code","efc857fe":"code","bda385d0":"code","6a682df5":"code","e68aa424":"code","7099eee3":"code","b80c4584":"code","83b8c343":"code","b326c068":"code","e55c0631":"code","52dc3b10":"code","b6e64c95":"code","16cadcfb":"code","6628e0be":"code","6dbf9074":"code","0c75e9ae":"code","9924d570":"code","d297b72c":"code","aa5af7e9":"code","e433e751":"code","02bcba0e":"code","a77e366e":"code","c507a0cd":"code","f5131020":"code","bdd1765d":"code","05fdb81a":"code","4c66b1dc":"code","89499726":"code","4e554a35":"code","dfa9fd80":"code","4c97506a":"code","e47a1256":"code","24f06ebe":"code","1b62b307":"code","d2d50c49":"code","9b84003d":"code","72a3d56e":"code","36365918":"code","2346898a":"code","a48ac040":"markdown","6992b775":"markdown","e8e844e6":"markdown","4d021178":"markdown","73a380b0":"markdown","bec82011":"markdown","1ba6b5a9":"markdown","090c3e63":"markdown","31e451c2":"markdown","24f12c8d":"markdown","acdd77e8":"markdown","5620dba5":"markdown","79b0d339":"markdown","de219fd2":"markdown","d06761f0":"markdown","6b13c8c7":"markdown","fedb146f":"markdown","d689496f":"markdown","0aa6ee99":"markdown","af38f975":"markdown","b64f1265":"markdown","04e9cad7":"markdown","435afa1b":"markdown","dd5b5a0d":"markdown","3a4a9588":"markdown","f5aa4117":"markdown","5c7b842f":"markdown","aff61136":"markdown","fca2db22":"markdown","9d3bf8df":"markdown","c9ce225c":"markdown","6e7d0d84":"markdown","0852baab":"markdown","688e1efc":"markdown","c7c171be":"markdown","5e9dedff":"markdown","92a05c9b":"markdown","ae8025f0":"markdown","a714bf23":"markdown","2b2a4ac2":"markdown","517a4854":"markdown","41de5035":"markdown","f850970e":"markdown","566fd617":"markdown","aa2fa8d2":"markdown","d95b5be3":"markdown"},"source":{"c3045d4f":"!pip install git+git:\/\/github.com\/stared\/livelossplot.git\n!pip install -q efficientnet\n","884c8c67":"import pandas as pd \nimport numpy as np \nimport cv2\n\nimport pydicom \nimport random\n\nimport os\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n\nfrom keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\nfrom keras.applications.vgg16 import VGG16, preprocess_input\nfrom keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\nfrom keras.layers import concatenate, Dense, Dropout, Input, Flatten, Activation, GlobalAveragePooling2D\nfrom keras.optimizers import Adam, SGD, RMSprop\nfrom keras.models import Sequential, Model, load_model\nfrom keras.utils import plot_model\n\nimport tensorflow as tf\nfrom tensorflow.python.keras import backend as K\n\nimport skimage.io\nimport multiprocessing\nfrom joblib import Parallel, delayed\nfrom tqdm import tqdm\n\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import StratifiedKFold\n\nimport plotly.graph_objects as go\n\nfrom livelossplot import PlotLossesKeras\n\nimport efficientnet.tfkeras as efn \n\n","944ba209":"base_directory_path = '..\/input\/siim-isic-melanoma-classification'\n\nbase_train_df = pd.read_csv(base_directory_path + '\/train.csv')\ntest_df = pd.read_csv(base_directory_path + '\/test.csv')\n\nbase_train_df = base_train_df.rename(columns = {'anatom_site_general_challenge': 'body_part'})\ntest_df = test_df.rename(columns = {'anatom_site_general_challenge': 'body_part'})\n\nprint('------------------------------------')\nprint('Train data size: ', len(base_train_df))\nprint('Test data size: ', len(test_df))\nprint('------------------------------------\\n')\n\n\nprint('------------------------------------')\nprint('# of categories in each column \\n')\nprint(base_train_df.nunique())\nprint('------------------------------------\\n')\n\n\nmissing = base_train_df.isnull().sum().sort_values(ascending = False) # taking values list(as an array) of series\npercent = missing * 100 \/ len(base_train_df)\nmissing_percent_df  = pd.concat([missing, percent], axis = 1, keys = ['missing', 'percent'])\nprint('------------------------------------')\nprint('% of missing values in columns \\n')\nprint(missing_percent_df)\nprint('------------------------------------\\n')\n","c4f228e5":"base_train_df.head()\n","7c10bc9a":"test_df\n","eb5993e6":"print('------------------------------------')\nprint('% of categories in \\'body parts\\' \\n')\nprint(base_train_df['body_part'].value_counts(ascending = True, normalize = True))\nprint('------------------------------------\\n')\n\nprint('------------------------------------')\nprint('% of categories in \\'gender\\' \\n')\nprint(base_train_df['sex'].value_counts(ascending = True, normalize = True))\nprint('------------------------------------\\n')\n\nprint('------------------------------------')\nprint('% of categories in \\'diagnosis\\' \\n')\nprint(base_train_df['diagnosis'].value_counts(ascending = True, normalize = True))\nprint('------------------------------------\\n')\n\nprint('------------------------------------')\nprint('% of categories in \\'benign_malignant\\' \\n')\nprint(base_train_df['benign_malignant'].value_counts(ascending = True, normalize = True))\nprint('------------------------------------\\n')\n","b63d8fe9":"pd.crosstab(base_train_df['patient_id'], base_train_df['target']).head()\n","bc1ffbdc":"# taking only malignant cases\nmalignant_df = base_train_df[base_train_df['benign_malignant'] == 'malignant']\n# excluding NAN sex values \nmalignant_df = base_train_df[base_train_df['sex'].isna() == False]\n\npd.crosstab(malignant_df['benign_malignant'], malignant_df['sex'])\n","559e4f16":"# taking only NAN sex values\nmissing_sex_df = base_train_df[base_train_df['sex'].isna() == True]\n\nprint('patients whose sex values are not known')\nprint(missing_sex_df['patient_id'].value_counts())\nprint('\\n')\n\nprint('# of benign vs malignant tumors in these patients')\nprint(missing_sex_df['benign_malignant'].value_counts())\nprint('\\n')\n\nprint('# of missing age values in these patients')\nprint(missing_sex_df['age_approx'].isnull().sum())\n","dab4d09c":"base_train_df.loc[base_train_df.patient_id == 'IP_5205991', 'sex'] = 'female'\nbase_train_df.loc[base_train_df.patient_id == 'IP_9835712', 'sex'] = 'female'\nbase_train_df['sex'].isnull().sum()","9c9123d0":"# taking only NAN age values\nmissing_age_df = base_train_df[base_train_df['age_approx'].isna() == True]\n\n# patients whose age values are not known\nprint(missing_age_df['patient_id'].value_counts())\nbase_train_df[base_train_df.patient_id == 'IP_0550106']\n","29931997":"# age median of the females \nfemale_age_median = base_train_df[base_train_df['sex'] == 'female'].age_approx.median()\n# age median of the males \nmale_age_median = base_train_df[base_train_df['sex'] == 'male'].age_approx.median()\n\nfor patient in missing_age_df.patient_id.unique():\n    l = base_train_df.loc[base_train_df.patient_id == patient, 'sex'].unique()\n    if l[0] == 'female':\n        base_train_df.loc[base_train_df.patient_id == patient, 'age_approx'] = female_age_median\n    elif l[0] == 'male':\n        base_train_df.loc[base_train_df.patient_id == patient, 'age_approx'] = male_age_median\n\nbase_train_df.age_approx.isnull().sum()","3ff11993":"base_train_df = base_train_df[~ base_train_df['age_approx'].isin([0])]\n#print(base_train_df[~ base_train_df['age_approx'].isin([0]))\nbase_train_df.age_approx.min()","30b4624a":"parts_train = base_train_df.copy()\n\nparts_train['flag'] = np.where(base_train_df['body_part'].isna() == True, 'missing', 'non_missing')\n\n# Figure\nf, (ax1, ax2) = plt.subplots(1, 2, figsize = (16, 5))\n\nsns.countplot(parts_train['flag'], hue = parts_train['sex'], ax = ax1)\n\nsns.distplot(parts_train[parts_train['flag'] == 'missing']['age_approx'], \n             hist = False, rug = True, label = 'missing', ax = ax2, kde_kws = dict(linewidth=4))\n\nsns.distplot(parts_train[parts_train['flag'] == 'non_missing']['age_approx'], \n             hist = False, rug = True, label = 'non_missing', ax = ax2, kde_kws = dict(linewidth=4))\n\n\nax1.set_title('sex for missing and non_missing body parts', fontsize = 13)\nax2.set_title('age for missing and non_missing body parts', fontsize = 13)\n\n","2b546c16":"base_train_df['body_part'].fillna('torso', inplace = True)\nbase_train_df['body_part'].isnull().sum()\n","6ba24420":"base_train_df = base_train_df[~base_train_df['diagnosis'].isin(['atypical melanocytic proliferation', 'cafe-au-lait macule'])]\n\nprint(base_train_df['diagnosis'].value_counts())\n","1068ba00":"BENIGN_SAMPLE = 10000\n\nmalignant_df = base_train_df[base_train_df['target'] == 1]\nbenign_df = base_train_df[base_train_df['target'] == 0]\n\nbase_train_df = pd.concat([benign_df.sample(BENIGN_SAMPLE, replace = False, random_state = 1234), malignant_df])\nbase_train_df = base_train_df.reset_index(drop = True)\n\nbase_train_df.head()","1c078853":"base_train_df['image_name'] = '..\/input\/siim-isic-melanoma-classification\/jpeg\/train\/' + base_train_df['image_name'] + '.jpg'\n\nbase_train_df\n","880a4910":"sample_image_name = os.path.basename(base_train_df['image_name'][1111])\nsample_image_name\n","3dfe256f":"sample_image = cv2.imread('..\/input\/siim-isic-melanoma-classification\/jpeg\/train\/' + 'ISIC_0431547.jpg')\nsample_image = cv2.cvtColor(sample_image, cv2.COLOR_BGR2RGB)\n\nsample_dicom_path = '..\/input\/siim-isic-melanoma-classification\/train\/' + 'ISIC_0431547' + '.dcm'\n\ndicom_file = pydicom.read_file(sample_dicom_path)\ndicom_file_to_imgArray = dicom_file.pixel_array # automatically extracts image\n\nf, (ax1, ax2) = plt.subplots(1, 2, figsize = (13, 5))\n\n\nax1.imshow(dicom_file_to_imgArray)\nax2.imshow(sample_image)\n\nprint(dicom_file)\n\n","19a4bd14":"test_df = pd.read_csv(base_directory_path + '\/test.csv')\ntest_df = test_df.rename(columns = {'anatom_site_general_challenge': 'body_part'})\n\ntest_df.isnull().sum().sort_values(ascending = False)\n","dc9e6a78":"parts_test = test_df.copy()\n\nparts_test['flag'] = np.where(parts_test['body_part'].isna() == True, 'missing', 'non_missing')\n#print(parts_test)\n#print(parts_test.flag.value_counts())\n\nmedian = parts_test[parts_test['flag'] == 'missing']['age_approx'].median()\n\nparts_test[(parts_test['flag'] == 'non_missing') & (parts_test['age_approx'] == median)]['body_part'].value_counts()\n","0516b156":"test_df['body_part'].fillna('torso', inplace = True)\nprint(test_df['body_part'].isnull().sum())\nprint(test_df.isnull().sum())","563e1603":"test_df.head()","3d3ba94e":"external_directory_path = '..\/input\/melanoma-merged-external-data-512x512-jpeg\/512x512-dataset-melanoma\/512x512-dataset-melanoma\/'\n\nexternal_df = pd.read_csv('..\/input\/melanoma-merged-external-data-512x512-jpeg\/marking.csv')\nexternal_df = external_df.rename(columns = {'anatom_site_general_challenge': 'body_part'})\n\nexternal_df","b09dac28":"external_df.dropna(inplace = True)\n","17b62e23":"base_train_df_copy = pd.read_csv(base_directory_path + '\/train.csv')\npatient_id_list = base_train_df_copy['patient_id'].tolist()\n\n\nsame_patients_df = external_df.loc[external_df['patient_id'].isin(patient_id_list)]\nnot_same_patients = external_df.loc[~external_df['patient_id'].isin(patient_id_list)]\n\nmalignant_cases_df = not_same_patients[not_same_patients['target'] == 1]\n\n\n\nexternal_train_df = pd.concat([same_patients_df, malignant_cases_df])\n\nexternal_train_df = external_train_df.reset_index(drop = True)\n\nexternal_train_df = external_train_df.rename(columns = {'image_id': 'image_name'})\nexternal_train_df['image_name'] = external_directory_path + external_train_df['image_name'] + '.jpg'\n\n\nexternal_train_df","3fec032f":"# checking if there are missing values\nbase_train_df.isnull().sum()","efc857fe":"base_train_df = pd.get_dummies(base_train_df, columns = ['body_part'], prefix = [''])\nbase_train_df = pd.get_dummies(base_train_df, columns = ['sex'], prefix = [''])\nbase_train_df['age_approx_norm'] = base_train_df['age_approx'] \/ base_train_df['age_approx'].max()\n\ndel base_train_df['patient_id'], base_train_df['benign_malignant'], base_train_df['age_approx'], base_train_df['diagnosis']\nbase_train_df = base_train_df[['image_name', '_female', '_male', '_head\/neck', '_lower extremity', '_oral\/genital', '_palms\/soles', '_torso', '_upper extremity', 'age_approx_norm', 'target']]\n\nbase_train_df.head()","bda385d0":"external_train_df['age_approx_norm'] = external_train_df['age_approx'] \/ external_train_df['age_approx'].max()\nexternal_train_df = pd.get_dummies(external_train_df, columns = ['body_part'], prefix = [''])\nexternal_train_df = pd.get_dummies(external_train_df, columns = ['sex'], prefix = [''])\ndel external_train_df['patient_id'], external_train_df['age_approx'], external_train_df['source'] #test_df['age_category_freq'], test_df['age_category'], \n\n\nexternal_train_df = external_train_df[['image_name', '_female', '_male', '_head\/neck', '_lower extremity', '_oral\/genital', '_palms\/soles', '_torso', '_upper extremity', 'age_approx_norm', 'target']]\nexternal_train_df.head()\n","6a682df5":"test_df = pd.get_dummies(test_df, columns=['sex'], prefix = [''])\ntest_df = pd.get_dummies(test_df, columns=['body_part'], prefix = [''])\ntest_df['age_approx_norm'] = test_df['age_approx'] \/ test_df['age_approx'].max()\n\ndel test_df['patient_id'], test_df['age_approx'] \n\ntest_df.head()\n","e68aa424":"#test_df['image_name'] = '..\/input\/siim-isic-melanoma-classification\/jpeg\/test' + test_df['image_name'] + '.jpg'\ntest_df['image_name'] = '..\/input\/melanoma-merged-external-data-512x512-jpeg\/512x512-test\/512x512-test\/' + test_df['image_name'] + '.jpg'\ntest_df.head()\n","7099eee3":"IMAGE_SHAPE = (224, 224, 3)\nMETA_DIM = 9\n\ndef cnn_net(name):\n    \n    if name == \"EfficientNet\":\n        \n        model = efn.EfficientNetB0(weights = 'imagenet', # noisy-student\n                                   include_top = False,\n                                   input_shape = IMAGE_SHAPE)\n\n    #model = VGG16(weights = 'imagenet', \n    #              include_top = False, \n    #              input_shape = IMAGE_SHAPE) # Note if 'top=False' then we can add 'pooling='avg'' \n                                               # it will also automatically flatten the layer after convolution\n        \n        for layer in model.layers: \n            layer.trainable = False\n            \n    if name == \"VGG16\":\n        model = VGG16(include_top = False, \n                      weights = 'imagenet', \n                      input_shape = IMAGE_SHAPE) # pooling = 'avg'\n\n    x = Flatten()(model.output)\n    #x = model.output\n    #x = GlobalAveragePooling2D()(x)\n    \n    #x = Dense(512, activation = 'relu')(x)\n    #x = Dropout(0.2)(x, training = True)\n    \n    #x = Dense(256, activation = 'relu')(x)\n    #x = Dropout(0.2)(x, training = True)\n \n    #x = Dense(128, activation = 'relu')(x)\n    #x = Dropout(0.2)(x, training = True)\n    \n    x = Dense(8, activation = 'relu')(x)\n    #x = Dropout(0.1)(x, training = True)\n\n    \n    #x = Dense(8, activation = 'relu')(x)\n        \n    model = Model(model.input, x)\n            \n    \n    return model\n\nCNN_NET = cnn_net('VGG16')\nplot_model(CNN_NET, to_file = 'model_architecture.png', show_shapes = True, show_layer_names = False)\n","b80c4584":"def mlp_net():\n    \n    model = Sequential()\n    model.add(Dense(8, input_dim = META_DIM, activation = \"relu\"))\n    model.add(Dense(4, input_dim = META_DIM, activation = \"relu\"))\n\n    #model.add(Dense(8, activation = \"relu\"))\n        \n    return model\n\nMLP_NET = mlp_net()\nplot_model(MLP_NET, to_file = 'model_architecture.png', show_shapes = True, show_layer_names = False)\n","83b8c343":"def concatenated_net(cnn, mlp):\n    \n    combinedInput = concatenate([cnn.output, mlp.output])\n    \n    #x = Dense(128, activation=\"relu\")(combinedInput)\n    #x = Dropout(0.2)(x, training = True)\n    \n    #x = Dense(64, activation=\"relu\")(combinedInput)\n    #x = Dropout(0.2)(x, training = True)\n\n    x = Dense(1, activation=\"sigmoid\")(combinedInput) # because our metric is AUC, i.e. \n                                                      # softmax with two neurons will not work\n    \n    model = Model(inputs = [cnn.input, mlp.input], outputs = x)\n    return model\n\nconcatenated_model = concatenated_net(CNN_NET, MLP_NET)\nplot_model(concatenated_model, to_file = 'model_architecture.png', show_shapes = True, show_layer_names = False)\n\n#SVG(model_to_dot(model, dpi=48, rankdir=\"LR\").create(prog='dot', format='svg'))\n","b326c068":"def focal_loss(alpha = 0.25, gamma = 2.0):\n    def focal_crossentropy(y_true, y_pred):\n        bce = K.binary_crossentropy(y_true, y_pred)\n        \n        y_pred = K.clip(y_pred, K.epsilon(), 1.- K.epsilon())\n        p_t = (y_true * y_pred) + ((1-y_true) * (1-y_pred))\n        \n        alpha_factor = 1\n        modulating_factor = 1\n\n        alpha_factor = y_true * alpha + ((1-alpha) * (1-y_true))\n        modulating_factor = K.pow((1-p_t), gamma)\n\n        # compute the final loss and return\n        return K.mean(alpha_factor * modulating_factor * bce, axis = -1)\n    \n    return focal_crossentropy\n","e55c0631":"opt = Adam(lr = 1e-05)\nconcatenated_model.compile(loss = focal_loss(), metrics = [tf.keras.metrics.AUC(name = 'auc')], optimizer = opt)\n#concatenated_model.compile(loss = 'binary_crossentropy', metrics = [tf.keras.metrics.AUC(name = 'auc')], optimizer = opt)\n","52dc3b10":"def custom_prep(image):\n    \n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\n    hairs = 10\n    thickness = 1\n    color = (0, 0, 0) # Black hair \n\n        \n    height, width, _ = image.shape\n    max_hair_number = random.randint(0, hairs)\n    \n    \n    for _ in range(max_hair_number):\n        # The start_point of the line is on upper left part [(0, w\/2), (0, h\/2)] of an image\n        start_point = (random.randint(0, width \/\/ 2), random.randint(0, height \/\/ 2))\n        # The end_point of the line \n        end_point = (random.randint(0, width), random.randint(0, height))\n        cv2.line(image, start_point, end_point, color, thickness)\n             \n        center_coordinates = (width \/\/ 2, height \/\/ 2) \n        axesLength = (random.randint(0, width \/\/ 2), random.randint(0, height \/\/ 2)) \n        angle = random.randint(0, 360)\n        start_angle = 0\n        end_angle = 180\n        cv2.ellipse(image, center_coordinates, axesLength, angle, start_angle, end_angle, color, thickness)\n\n    return image   \n\n","b6e64c95":"sample_image = cv2.imread('..\/input\/siim-isic-melanoma-classification\/jpeg\/train\/ISIC_8233560.jpg')\nsample_image_hair = custom_prep(sample_image)\n\nsample_image = cv2.cvtColor(sample_image, cv2.COLOR_BGR2RGB)\n\n\nf, (ax1, ax2) = plt.subplots(1, 2, figsize = (13, 5))\n\nax1.imshow(sample_image)\nax2.imshow(sample_image_hair)","16cadcfb":"sample_image_hair = cv2.cvtColor(sample_image_hair, cv2.COLOR_BGR2RGB)\n\ncv2.imwrite('filename.jpeg', sample_image_hair) ","6628e0be":"train, validation = train_test_split(external_train_df, # base_train_df\n                                     test_size = 0.15, \n                                     stratify = external_train_df['target']) # base_train_df\n","6dbf9074":"train_datagen = ImageDataGenerator(\n    rescale = 1. \/ 255.,\n    rotation_range = 180,\n    width_shift_range = 0.15,\n    height_shift_range = 0.15,\n    zoom_range = 0.1,\n    horizontal_flip = True,\n    vertical_flip = True,\n    brightness_range = [0.3,1.3],\n    fill_mode = 'reflect', # nearest\n    preprocessing_function = custom_prep  \n)\n\nval_datagen = ImageDataGenerator(rescale = 1.\/255)\n\n","0c75e9ae":"BATCH_SIZE = 8\n\ntrain_generator = train_datagen.flow_from_dataframe(\n    train,\n    x_col = 'image_name',\n    y_col = train.columns[1:],\n    target_size = (IMAGE_SHAPE[0], IMAGE_SHAPE[1]),\n    batch_size = BATCH_SIZE,\n    shuffle = True,\n    class_mode = 'raw')\n\nvalidation_generator = val_datagen.flow_from_dataframe(\n    validation,\n    x_col = 'image_name',\n    y_col = validation.columns[1:],\n    target_size = (IMAGE_SHAPE[0], IMAGE_SHAPE[1]),\n    shuffle = False,\n    batch_size = BATCH_SIZE,\n    class_mode='raw')\n","9924d570":"def own_train_generator_func():\n    count = 0\n    while True:\n        if count == len(train.index):\n            train_generator.reset()\n            #break\n        count += 1\n        data = train_generator.next()\n        \n        imgs = data[0]\n        meta = data[1][:,:-1]\n        targets = data[1][:,-1:]\n        \n        yield [imgs, meta], targets\n\ndef own_validation_generator_func():\n    count = 0\n    while True:\n        if count == len(validation.index):\n            validation_generator.reset()\n            #break\n        count += 1\n        data = validation_generator.next()\n                \n        imgs = data[0]\n        meta = data[1][:,:-1]\n        targets = data[1][:,-1:]\n        \n        yield [imgs, meta], targets\n        ","d297b72c":"EPOCHS = 3\n\nUPDATES_PER_EPOCH = train.shape[0] \/\/ BATCH_SIZE # + 1\nVALIDATION_STEPS = validation.shape[0] \/\/ BATCH_SIZE\n\nprint(\"Number of training and validation steps: {} and {}\".format(UPDATES_PER_EPOCH, VALIDATION_STEPS))\n","aa5af7e9":"def lrfn(epoch):\n    return 1e-4 * (0.7 ** np.floor(epoch \/ 3))\n\nlr_sched = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose = True)\n\n\nrng = [i for i in range(EPOCHS)]\ny = [lrfn(x) for x in rng]\nplt.plot(rng, y)\nprint(\"Learning rate schedule: {:.3g} to {:.3g} to {:.3g}\".format(y[0], max(y), y[-1]))\n","e433e751":"checkpoint = ModelCheckpoint(\n   'best_model.h5', \n    monitor = 'val_auc',\n    mode = 'max', # because val_acc is monitored\n    verbose = 0, \n    save_best_only = True, \n    save_weights_only = True)\n\nearly_stopper = EarlyStopping(\n    monitor = 'val_auc', \n    mode = 'max', \n    patience = 5, \n    restore_best_weights = True, \n    verbose = 1)\n\nreduce_lr = ReduceLROnPlateau(\n    monitor = 'val_loss', \n    mode = 'min',\n    factor = 0.5, \n    patience = 3, \n    min_lr = 1e-6,\n    min_delta = 1e-4,\n    verbose = 1)\n\ncallbacks_list = [PlotLossesKeras(), checkpoint,  early_stopper, lr_sched]\n\n","02bcba0e":"concatenated_model.fit(\n    own_train_generator_func(),\n    steps_per_epoch = UPDATES_PER_EPOCH,\n    epochs = EPOCHS,\n    validation_data = own_validation_generator_func(),\n    validation_steps = VALIDATION_STEPS,\n    callbacks = [checkpoint, PlotLossesKeras()])\n","a77e366e":"submission = pd.read_csv('\/kaggle\/input\/siim-isic-melanoma-classification\/sample_submission.csv')\nsubmission.head()\n","c507a0cd":"test_df['image_name'][0]","f5131020":"target = []\n\nfor index in range(len(test_df)):\n    img = cv2.imread(test_df['image_name'][index])\n    img = cv2.resize(img, (IMAGE_SHAPE[0], IMAGE_SHAPE[1]))\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    img = img.astype(np.float32) \/ 255.\n    img = np.reshape(img, (1, IMAGE_SHAPE[0], IMAGE_SHAPE[1], 3))\n\n    meta = test_df.iloc[index, 1:]\n    meta = meta.to_numpy()\n    meta = np.reshape(meta, (1, META_DIM))\n    meta = meta.astype(np.float32)\n\n    combined_input = [img, meta]\n    prediction = concatenated_model.predict(combined_input)\n    target.append(prediction[0][0])\n\n\nsubmission['target'] = target\n\n","bdd1765d":"submission.to_csv('combined_submission.csv', index = False)\n\n","05fdb81a":"stratified_kf = StratifiedKFold(\n    n_splits = 5, \n    shuffle = True, \n    random_state = 1234)\n    \nfold_indices = list(stratified_kf.split(external_train_df, external_train_df['target']))\n","4c66b1dc":"train_datagen = ImageDataGenerator(\n    rescale = 1. \/ 255.,\n    rotation_range = 30,\n    width_shift_range = 0.1,\n    height_shift_range = 0.1,\n    zoom_range = 0.1,\n    horizontal_flip = True,\n    vertical_flip = True,\n    brightness_range = [0.1,1.0],\n    fill_mode = 'reflect', # nearest\n    #preprocessing_function = custom_prep  \n)\n\nval_datagen = ImageDataGenerator(rescale = 1. \/ 255.)\n","89499726":"def lrfn(epoch):\n    return 1e-4 * (0.7 ** np.floor(epoch \/ 3))\n\ndef get_callbacks(weights_name):\n    \n    patience_es = 5\n    patience_lr = 3\n    \n    checkpoint = ModelCheckpoint(\n        filepath = weights_name, \n        monitor = 'val_auc',\n        mode = 'max',\n        verbose = 0,\n        save_best_only = True,\n        save_weights_only = True)\n    \n    early_stopper = EarlyStopping(\n        monitor = 'val_auc', \n        mode = 'max', \n        patience = patience_es, \n        restore_best_weights = True, \n        verbose = 1)\n    \n    reduce_lr = ReduceLROnPlateau(\n        monitor = 'val_loss', \n        mode = 'min',\n        factor = 0.5, \n        patience = patience_lr, \n        min_lr = 1e-6,\n        min_delta = 1e-4,\n        verbose = 1)\n    \n    lr_sched = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose = True)\n\n\n    return [checkpoint, early_stopper, lr_sched] # PlotLossesKeras(), reduce_lr\n\n","4e554a35":"def own_train_generator_func():\n    count = 0\n    while True:\n        if count == len(train.index):\n            train_generator.reset()\n            #break\n        count += 1\n        data = train_generator.next()\n        \n        imgs = data[0]\n        meta = data[1][:,:-1]\n        targets = data[1][:,-1:]\n        \n        yield [imgs, meta], targets\n\n        \ndef own_validation_generator_func():\n    count = 0\n    while True:\n        if count == len(validation.index):\n            validation_generator.reset()\n            #break\n        count += 1\n        data = validation_generator.next()\n                \n        imgs = data[0]\n        meta = data[1][:,:-1]\n        targets = data[1][:,-1:]\n        \n        yield [imgs, meta], targets\n        ","dfa9fd80":"BATCH_SIZE = 8\nEPOCHS = 3\n\n\nfor j, (train_indices, validation_indices) in enumerate(fold_indices):\n    \n    print('\\nFold ',j)\n\n    train = external_train_df.iloc[train_indices]\n    validation = external_train_df.iloc[validation_indices]\n    \n    UPDATES_PER_EPOCH = train.shape[0] \/\/ BATCH_SIZE # + 1\n    VALIDATION_STEPS = validation.shape[0] \/\/ BATCH_SIZE\n\n\n    train_generator = train_datagen.flow_from_dataframe(\n        train,\n        x_col = 'image_name',\n        y_col = train.columns[1:],\n        target_size = (IMAGE_SHAPE[0], IMAGE_SHAPE[1]),\n        batch_size = BATCH_SIZE,\n        shuffle = True,\n        class_mode = 'raw')\n\n    validation_generator = val_datagen.flow_from_dataframe(\n        validation,\n        x_col = 'image_name',\n        y_col = validation.columns[1:],\n        target_size = (IMAGE_SHAPE[0], IMAGE_SHAPE[1]),\n        shuffle = False,\n        batch_size = BATCH_SIZE,\n        class_mode = 'raw')\n    \n    \n    weights_name = 'fold_' + str(j) + '_weights.h5'\n    callbacks_list = get_callbacks(weights_name)\n\n    \n    concatenated_model.fit(\n    own_train_generator_func(),\n    steps_per_epoch = UPDATES_PER_EPOCH,\n    epochs = EPOCHS,\n    validation_data = own_validation_generator_func(),\n    validation_steps = VALIDATION_STEPS,\n    callbacks = callbacks_list)\n\n","4c97506a":"concatenated_model.load_weights('..\/input\/fold-weights\/fold_0_weights.h5')\n\ntarget_1 = []\n\nfor index in range(len(test_df)):\n    img = cv2.imread(test_df['image_name'][index])\n    img = cv2.resize(img, (IMAGE_SHAPE[0], IMAGE_SHAPE[1]))\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    img = img.astype(np.float32) \/ 255.\n    img = np.reshape(img, (1, IMAGE_SHAPE[0], IMAGE_SHAPE[1], 3))\n\n    meta = test_df.iloc[index, 1:]\n    meta = meta.to_numpy()\n    meta = np.reshape(meta, (1, META_DIM))\n    meta = meta.astype(np.float32)\n\n    combined_input = [img, meta]\n    prediction = concatenated_model.predict(combined_input)\n    target_1.append(prediction[0][0])\n","e47a1256":"concatenated_model.load_weights('..\/input\/fold-weights\/fold_0_weights.h5')\n\ntarget_1 = []\n\nfor index in range(len(test_df)):\n    img = cv2.imread(test_df['image_name'][index])\n    img = cv2.resize(img, (IMAGE_SHAPE[0], IMAGE_SHAPE[1]))\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    img = img.astype(np.float32) \/ 255.\n    img = np.reshape(img, (1, IMAGE_SHAPE[0], IMAGE_SHAPE[1], 3))\n\n    meta = test_df.iloc[index, 1:]\n    meta = meta.to_numpy()\n    meta = np.reshape(meta, (1, META_DIM))\n    meta = meta.astype(np.float32)\n\n    combined_input = [img, meta]\n    prediction = concatenated_model.predict(combined_input)\n    target_1.append(prediction[0][0])\n    \n    \nsubmission1 = pd.read_csv('\/kaggle\/input\/siim-isic-melanoma-classification\/sample_submission.csv')\nsubmission1['target'] = target_1\nsubmission1.to_csv('meta_and_images_fold_1.csv', index = False)\nsubmission1.head()","24f06ebe":"concatenated_model.load_weights('..\/input\/fold-weights\/fold_1_weights.h5')\n\ntarget_2 = []\n\nfor index in range(len(test_df)):\n    img = cv2.imread(test_df['image_name'][index])\n    img = cv2.resize(img, (IMAGE_SHAPE[0], IMAGE_SHAPE[1]))\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    img = img.astype(np.float32) \/ 255.\n    img = np.reshape(img, (1, IMAGE_SHAPE[0], IMAGE_SHAPE[1], 3))\n\n    meta = test_df.iloc[index, 1:]\n    meta = meta.to_numpy()\n    meta = np.reshape(meta, (1, META_DIM))\n    meta = meta.astype(np.float32)\n\n    combined_input = [img, meta]\n    prediction = concatenated_model.predict(combined_input)\n    target_2.append(prediction[0][0])\n\nsubmission2 = pd.read_csv('\/kaggle\/input\/siim-isic-melanoma-classification\/sample_submission.csv')\nsubmission2['target'] = target_2\nsubmission2.to_csv('meta_and_images_fold_2.csv', index = False)\nsubmission2.head()\n\n","1b62b307":"concatenated_model.load_weights('..\/input\/fold-weights\/fold_2_weights.h5')\n\ntarget_3 = []\n\nfor index in range(len(test_df)):\n    img = cv2.imread(test_df['image_name'][index])\n    img = cv2.resize(img, (IMAGE_SHAPE[0], IMAGE_SHAPE[1]))\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    img = img.astype(np.float32) \/ 255.\n    img = np.reshape(img, (1, IMAGE_SHAPE[0], IMAGE_SHAPE[1], 3))\n\n    meta = test_df.iloc[index, 1:]\n    meta = meta.to_numpy()\n    meta = np.reshape(meta, (1, META_DIM))\n    meta = meta.astype(np.float32)\n\n    combined_input = [img, meta]\n    prediction = concatenated_model.predict(combined_input)\n    target_3.append(prediction[0][0])\n\nsubmission3 = pd.read_csv('\/kaggle\/input\/siim-isic-melanoma-classification\/sample_submission.csv')\nsubmission3['target'] = target_3\nsubmission3.to_csv('meta_and_images_fold_3.csv', index = False)\nsubmission3.head()\n\n","d2d50c49":"concatenated_model.load_weights('..\/input\/fold-weights\/fold_3_weights.h5')\n\ntarget_4 = []\n\nfor index in range(len(test_df)):\n    img = cv2.imread(test_df['image_name'][index])\n    img = cv2.resize(img, (IMAGE_SHAPE[0], IMAGE_SHAPE[1]))\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    img = img.astype(np.float32) \/ 255.\n    img = np.reshape(img, (1, IMAGE_SHAPE[0], IMAGE_SHAPE[1], 3))\n\n    meta = test_df.iloc[index, 1:]\n    meta = meta.to_numpy()\n    meta = np.reshape(meta, (1, META_DIM))\n    meta = meta.astype(np.float32)\n\n    combined_input = [img, meta]\n    prediction = concatenated_model.predict(combined_input)\n    target_4.append(prediction[0][0])\n\nsubmission4 = pd.read_csv('\/kaggle\/input\/siim-isic-melanoma-classification\/sample_submission.csv')\nsubmission4['target'] = target_4\nsubmission4.to_csv('meta_and_images_fold_4.csv', index = False)\nsubmission4.head()\n","9b84003d":"concatenated_model.load_weights('..\/input\/fold-weights\/fold_4_weights.h5')\n\ntarget_5 = []\n\nfor index in range(len(test_df)):\n    img = cv2.imread(test_df['image_name'][index])\n    img = cv2.resize(img, (IMAGE_SHAPE[0], IMAGE_SHAPE[1]))\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    img = img.astype(np.float32) \/ 255.\n    img = np.reshape(img, (1, IMAGE_SHAPE[0], IMAGE_SHAPE[1], 3))\n\n    meta = test_df.iloc[index, 1:]\n    meta = meta.to_numpy()\n    meta = np.reshape(meta, (1, META_DIM))\n    meta = meta.astype(np.float32)\n\n    combined_input = [img, meta]\n    prediction = concatenated_model.predict(combined_input)\n    target_5.append(prediction[0][0])\n\nsubmission5 = pd.read_csv('\/kaggle\/input\/siim-isic-melanoma-classification\/sample_submission.csv')\nsubmission5['target'] = target_5\nsubmission5.to_csv('meta_and_images_fold_5.csv', index = False)\nsubmission5.head()\n","72a3d56e":"submission_concat = pd.concat([submission1, submission2, submission3, submission4, submission5], axis = 1)\ndel submission_concat['image_name']\n\nsubmission_concat.head()","36365918":"mean = submission_concat.mean(axis = 1)\nmedian = submission_concat.median(axis = 1)\nminimum = submission_concat.min(axis = 1)\nmaximum = submission_concat.max(axis = 1)\n","2346898a":"submission = pd.read_csv('\/kaggle\/input\/siim-isic-melanoma-classification\/sample_submission.csv')\nsubmission['target'] = median\nsubmission.to_csv('median.csv', index = False)\n","a48ac040":"# 9. Predictions","6992b775":"# 2.2 Test data  \n","e8e844e6":"There are 3 patients whose ages are missing: IP_0550106, IP_9835712, IP_5205991, the last two were those whose sex values were also missing and we replaced these missing sexes. Patient 'IP_0550106' is also female. ","4d021178":"First of all here we split the data into train and validation","73a380b0":"The function is drawing random lines and elipses on image thereby imitating hairs. The idea is taken from [here](https:\/\/www.kaggle.com\/c\/siim-isic-melanoma-classification\/discussion\/159176) and changed a little bit, here simly elipses and lines are drawn\n\n","bec82011":"This loss function is introduced in papaer [Focal Loss for Dense Object Detection](https:\/\/arxiv.org\/pdf\/1708.02002.pdf) for dealing the  class imbalance in data. ","1ba6b5a9":"It is seen that the same patient can have both benign and malignant tumors, e.g. see patient with id 'IP_0038545'.","090c3e63":"Here paths will is set based on what data we use for training. Note that test data is the same everywhere, i.e. the same images, the same patients, the same amount of data. Our task eventually is to make predictions on this data. If we train network on external data (where images are also resized to 512x512), then for testing we can use resized test images which is the same as competition test images just resized.\n\n\n","31e451c2":"# 7. Preparing for training: train\/validation split, ImageDataGenerator and flow_from_dataframe for metadata and images","24f12c8d":"Here is the link of external data https:\/\/www.kaggle.com\/shonenkov\/merge-external-data. ","acdd77e8":"Out of all malignant cases males have more malignant cases, hence in the data the missing sex values can be assigned to 'female' if target is benign, otherwise to 'male' if the target is malignant. But note also that males (51.6%) are occuring sligtly more than females (48.4%) in the whole training data set.","5620dba5":"# 4.1 Competition train data\n","79b0d339":"# 8. Training","de219fd2":"# 4.3 Test data\n","d06761f0":"# 1. Introduction\n\n## 1.1 Tumor vs Cancer\n\nTumor $=$ uncontrolled cell division (resulting in extra mass in a body).\n\nCancer $=$ uncontrolled cell division (resulting in extra mass in a body).\n\n>>>>> However,  $cancer \\neq tumor$. \n\nThere are 2 types of tumors: \n1. **benign** (noncancerous) \n2.  **malignant** (cancerous).\n\nBenign tumor $ = $ uncontrolled cellular growth localized in certain part of the body, i.e. they do not spread\/travel throughout the body.\n\nMalignant tumor $ = $ uncontrolled cellular growth not localized in certain part of the body, i.e. they do spread\/travel throughout the body.\n\nSpreading\/traveling throughout the body cells invade nearby tissues, and\/or form secondary tumors in other places, known as metastases.\n\n>>>>> So,  $cancer = malignant ~ tumor$. \n\n<img src = 'https:\/\/kd-group.ro\/images\/282075.png' width = 300>\n\nSome notes\n* not all cancers are characterized by tumor growth, e.g. there is no tumor involved in blood cancer. \n* if there is already a tumor, biopsy is crucial to determine if the growth is malignant or benign (see another Kaggle competition [another computer vision problem](https:\/\/www.kaggle.com\/c\/prostate-cancer-grade-assessment)).\n\nA biopsy is a medical procedure when tumor cells\/tissues are extracted and their pattern is examined under a microscope to determine\/classify the stage of tumor progression. In Melanoma competition we do not have biopsy images, we have just images of skin leisures, but some labels of images are determined by [biopsy samples](https:\/\/www.kaggle.com\/c\/siim-isic-melanoma-classification\/discussion\/155296). \n\n\n\n\n\n\n\n## 1.2 Melanoma\n\nThus, skin cancer $=$ uncontrolled cellular growth in skin.\n\nThere are 3 types of skin cancers based on types of skin cells involved\n1. basal cell carcinoma\n2. squamous cell carcinoma \n3. melanoma\n\nMelanoma develops in cells called melanocytes, cells that give skin its color by producing melanin (skin pigment). Melanoma is \n\n* the least common skin cancer\n* the most deadly skin cancer\n\nThe abnormal\/benign growth of melanocytes is called Mole (nevus). These tumor cells may now produce melanins that contain different colors: **brown**, **red**, **dark blue** and **gray**. Moles also can appear **flat** or **raised** on the skin.\n\nMoles that may have changed into skin cancer are often irregularly shaped, contain many colors, and of course are of bigger sizes. \n\n<img src = 'https:\/\/sa1s3optim.patientpop.com\/assets\/images\/provider\/photos\/1786876.jpg' width = 500>\n\n\n## 1.3 Problem statement\nThe goal is to predict whether the lesion in each image is benign or malignant using patient metadata. Metadata is defined as data that provides information about other data, in our case metadata is given by patient's age, sex, lesion location in the body, and this information may help to correctly classify images (another data). \n\nSo 2-input (hence 2-branched) combined neural network will be built in order to analyze both images of skin lesions and patient's information (like age, sex etc.) \n\n\nOne branch of the network responsible for handling image data is a pre-trained image classification network, like VGGNet, EfficientNet etc. The second branch is simple fully-connected small network to handle patient's information. The branches are then combined at the end and connected to a single neuron to obtain melanoma classification. \n\nNowadays, creating complex architectures is relatively easy with Keras functional API, but it is a little laborious to accept a batch of images+metadata and apply a series of random transformations only on images (i.e. only on one input). This is successfully done with flow_from_dataframe method of Keras ImageDataGenerator. \n\n\nAnd lastly here a user defined function of image transformation is added in ImageDataGenerator, the function is drawing random lines and elipses on image thereby imitating hairs.\n\nThe models developed in this notebook can run on both GPU and CPU (of course GPU is preferred) since no code changes are required for training Keras models on GPU\/CPU. However, in order to be able to train model on TPU one needs to load data from GCS (Google Cloud Storage) with the tf.data.Dataset and compile a model with TensorFlow distribution strategy. The implementation of 2-branched network for running on TPU can be found [here.](https:\/\/www.kaggle.com\/niteshx2\/full-pipeline-dual-input-cnn-model-with-tpus?scriptVersionId=40509001)\n\nThe notebook is primarily meant to show how to use ImageDataGenerator.flow_from_dataframe for generating batches of metadata+images. Hope this notebook will be useful especially for beginners.\n\n\n## 1.4 Why ML Algorithm\n1. Early detection is the most effective way to fight with cancerous tumors, i.e. cancer caught early has very high survival rate compared with cancers caught late. Usually treatment includes surgery, chemotherapy, radiotherapy.\n2. There can be significant inter-observer variability between dermatologists (or pathologists in case of prostate cancer, etc.). Inter-observer variation happens when the same patient examined by two or more observers may be given different diagnoses.\n3. Current ML algorithms can [achieve doctor-level](https:\/\/www.kaggle.com\/c\/prostate-cancer-grade-assessment) or even [outperform doctor-level](https:\/\/cs.stanford.edu\/people\/esteva\/nature\/#!) performance\n\n\n\n","6b13c8c7":"We see that sex distributions for missing and non-missing body parts are about the same. You can also see that age distributions for missing and non-missing body parts are also abut the same and they have the same peak. So one can take \npeak value (of age) of missing body parts, find most occuring body part (in non-missing dataframe) for the given peak value (of age) and replace the missing parts with it. The most occuring is torso, so we will replace with torso. This analysis is important since one should be careful how replaces missing values. See [this notebook](https:\/\/www.kaggle.com\/andradaolteanu\/siim-melanoma-competition-eda-augmentations) as a reference for EDA analysis.\n","fedb146f":"# 5.1 Image branch","d689496f":"Here we also deleted 'diagnosis' because it is not present in test data.","0aa6ee99":"Here we deleted rows where diagnosis='atypical melanocytic proliferation', and diagnosis = 'cafe-au-lait macule', because there is only one patient with those diagnosis.","af38f975":"# 5. Model","b64f1265":"Here missing body parts are replaces with the same strategy as it was done in train set. Age median is computed for missing 'body parts', then we check the most occuring 'body part' in non-missing 'body parts' which has age equal to computed median (of missing cases).","04e9cad7":"# 11. Ensemble Predictions on Test Data","435afa1b":"# 10. Training with k-fold","dd5b5a0d":"# 7.1 Splitting the data ","3a4a9588":"# 2.1 Train data of competition data  \n","f5aa4117":"# 2. Exploratory data analysis (EDA) of competition data  \n","5c7b842f":"# 6. User defined image preprocessing function","aff61136":"Here we deleted rows with age 0. Note there was only one patient with age 0: patient_id = 'IP_1300691'.","fca2db22":"See [here](https:\/\/www.kaggle.com\/residentmario\/tuning-your-learning-rate) for learning rate schedule discussion ","9d3bf8df":"# 5.2 Metadata branch","c9ce225c":"# 4. Encoding variables(categorical\/numerical)\n","6e7d0d84":"# 8.1 Callbacks and training","0852baab":"Here from external data we took all patients that are also present in competition data + all malignant cases (of external data). One can use whole data for training.","688e1efc":"# 4.2 External train data\n","c7c171be":"In test data only 'body part' has missing values.","5e9dedff":"# 5.3 Combined network","92a05c9b":"Note usually one takes y_col = 'target', but here we take rows of dataframe with all columns except 'image_name'.","ae8025f0":"Here any EDA is not done and I just drop NANs since the data is a lot, hence we don't care much of losing some data.","a714bf23":"* There are two patients (namely 'IP_5205991' and 'IP_9835712') whose sex values are unknown. \n* All skin leisures of those patients are benign (hence below sex values will be assigned to 'female'). \n* Ages of those patients are also absent.\n","2b2a4ac2":"If you want to train not all data in competition train set, you can use\/run code above which takes all malignant case and randomly taken benign cases.","517a4854":"# 5.4 Defining loss function and compiling the model","41de5035":"# 3. External train data\n","f850970e":"Here the word \"base\" in \"base_train_df\" points that we are dealing with training data provided in the competition. We use this notation because later we may use another data set for training. ","566fd617":"Above one can see that the data is severly imbalanced 1.763% of malignant cases vs 98.237 of benign cases.  \n","aa2fa8d2":"# 7.2 ImageDataGenerator and flow_from_dataframe","d95b5be3":"The code above replaces missing age by taking median of the ages such that if patient is e.g. female then median of ages of females will be taken."}}