{"cell_type":{"22f5e518":"code","758cf3e0":"code","cafe3f8d":"code","3f3508c9":"code","28f94aa2":"code","1a2524fb":"code","812d9e4b":"code","7be38269":"code","3cb95808":"code","b405454c":"code","6ca18af0":"code","b30de546":"code","5a4c8777":"code","6ef0ffdc":"code","84f24bf5":"code","ad3d7116":"code","921b5c39":"code","aeb6a5f1":"code","fa2ce43d":"code","505e9fb7":"code","cf259550":"code","52c9f926":"code","be2dec86":"code","8275a844":"code","3102ace4":"code","2614d56d":"code","546903e2":"code","ee66e102":"code","36511049":"code","ea2dea95":"code","9bdd07fb":"code","5c086509":"code","91bf1029":"code","b521eb3a":"code","87149958":"code","d50b4a1c":"code","04b53e14":"code","cc7c63d2":"code","0604e820":"code","5f7da6af":"code","c719f64b":"code","32e3523b":"code","438ce0a1":"code","0e8012e3":"code","cb71fe47":"code","534eb7d6":"code","6c5bfe86":"code","16a48c68":"code","2cd40224":"code","69447756":"code","b900cfe4":"code","a070a6b0":"code","abdd84e0":"code","498829e1":"code","d5afc8ba":"code","be526e7d":"code","ba1b5bad":"code","b2065822":"markdown","4c9695f3":"markdown","63f06f3b":"markdown","49455507":"markdown","f2a0f020":"markdown","75f7cdd1":"markdown","8b01f5e9":"markdown","a6f9f70b":"markdown","8eb21242":"markdown","e8bd550d":"markdown","7bcfecda":"markdown","c2435063":"markdown","a6078613":"markdown","128651b6":"markdown","0a3ad3b4":"markdown","2b516636":"markdown","12d37987":"markdown","028c93a1":"markdown","e51626e1":"markdown","7c02c036":"markdown","ce78d028":"markdown","41c25557":"markdown","7deb8ebd":"markdown","bc8028ff":"markdown","3f278dcb":"markdown","b886e52b":"markdown","94e90f4e":"markdown","9af7c180":"markdown"},"source":{"22f5e518":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns","758cf3e0":"df_tweet =  pd.read_csv(\"..\/input\/nlp-getting-started\/train.csv\")\nprint(df_tweet.shape)\n\nprint(df_tweet.info())","cafe3f8d":"df_tweet.isnull().sum()","3f3508c9":"df_tweet['target'].value_counts()","28f94aa2":"def convert_lower_case(df,columns):\n    for column in columns:\n        df[column].loc[df[column].isnull()] = \"\"\n        df[column] = df[column].apply(lambda x: x.lower())\n    return df\n\n\ndf_tweet = convert_lower_case(df_tweet,[ 'keyword', 'location', 'text'])\n\ndf_tweet.head()","1a2524fb":"df_tweet['location'].value_counts()[:20]","812d9e4b":"df_tweet = df_tweet.drop(['location'],axis = 1)","7be38269":"df_tweet['keyword'].value_counts()","3cb95808":"df_tweet['keyword'].unique()","b405454c":"df_tweet['keyword'] = df_tweet['keyword'].apply(lambda x: x if x != \"\" else  \"null\")\ndf_tweet['keyword'].unique()","6ca18af0":"import re\ndf_tweet['keyword'] = df_tweet['keyword'].apply(lambda x: x.replace('%20',\" \"))\ndf_tweet['keyword'] = df_tweet['keyword'].apply(lambda x: x.split())","b30de546":"from nltk.tokenize import word_tokenize\nfrom nltk.stem import PorterStemmer,SnowballStemmer,WordNetLemmatizer\nfrom nltk.corpus import stopwords","5a4c8777":"ps = PorterStemmer()\ntemp = df_tweet['keyword'].apply(lambda x: [ps.stem(a) for a in x])\ntemp = temp.apply(lambda x: \" \".join(x))\nprint(temp.unique())","6ef0ffdc":"print(len(temp.unique()))","84f24bf5":"sb = SnowballStemmer(language='english')\ntemp = df_tweet['keyword'].apply(lambda x: [sb.stem(a) for a in x])\ntemp = temp.apply(lambda x: \" \".join(x))\nprint(temp.unique())\n\nprint(len(temp.unique()))","ad3d7116":"wordnet = WordNetLemmatizer()\n\n\n#sb = SnowballStemmer(language='english')\ntemp = df_tweet['keyword'].apply(lambda x: [wordnet.lemmatize(a) for a in x])\ntemp = temp.apply(lambda x: \" \".join(x))\nprint(temp.unique())\n\nprint(len(temp.unique()))","921b5c39":"sb = SnowballStemmer(language='english')\ntemp = df_tweet['keyword'].apply(lambda x: [sb.stem(a) for a in x])\ntemp = temp.apply(lambda x: \" \".join(x))\nprint(temp.unique())\n\nprint(len(temp.unique()))\n\n#### Reduce no of classes\ndf_tweet['keyword'] = temp.copy()","aeb6a5f1":"df_test = pd.read_csv(\"..\/input\/nlp-getting-started\/test.csv\")\ndf_test = convert_lower_case(df_test,[ 'keyword', 'location', 'text'])","fa2ce43d":"### Drop Location columns\ndf_test = df_test.drop(['location'],axis=1)\n\n##  Replacing %20 with space\ndf_test['keyword'] = df_test['keyword'].apply(lambda x: x.replace('%20',\" \"))\ndf_test['keyword'] = df_test['keyword'].apply(lambda x: x.split())\n\nsb = SnowballStemmer(language='english')\ntemp = df_test['keyword'].apply(lambda x: [sb.stem(a) for a in x])\ntemp = temp.apply(lambda x: \" \".join(x))\nprint(temp.unique())\n\nprint(len(temp.unique()))\n\n#### Reduce no of classes\ndf_test['keyword'] = temp.copy()\ndf_test['keyword'] = df_test['keyword'].apply(lambda x: 'null' if x == \"\" else x)","505e9fb7":"from sklearn.preprocessing import LabelEncoder\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\nlb = LabelEncoder()\n\n\n#### Preparing the data for model \ndef prepare_data(df,test,column_encode,column_vectroize,vc):\n    lb = LabelEncoder()\n    df1 = df.copy()\n    test1 = test.copy()\n    #### Label Encding\n    \n    ###### Drop the columns\n    \n    train_keyword = df1.pop(column_encode)\n    test_keyword = test1.pop(column_encode)\n    \n    \n    train = pd.Series(lb.fit_transform(df[column_encode]))\n    test = pd.Series(lb.transform(test[column_encode]))\n    \n    ### Preprocessing text columns\n    df1['text'] = df1[\"text\"].apply(lambda x: word_tokenize(x))\n    df1['text'] = df1[\"text\"].apply(lambda x: [SnowballStemmer(language='english').stem(a) for a in x])\n    df1['text'] = df1['text'].apply(lambda x: \" \".join(x))\n    ### vectorize\n    \n    X_train = vc.fit_transform(df1['text']).toarray()\n    X_train = pd.DataFrame(X_train)\n    X_test = vc.transform(test1['text']).toarray()\n    X_test = pd.DataFrame(X_test)\n    #print(X_test.head())\n    X_train = pd.concat([X_train,train],axis=1)  \n    X_test = pd.concat([X_test,test],axis=1)  \n    #X_train.pop('keyword')\n    #X_test.pop('keyword')\n    return X_train,X_test","cf259550":"vc = CountVectorizer()\ntrain_df,test_df = prepare_data(df_tweet,df_test,'keyword','text',vc)","52c9f926":"print(test_df.shape,train_df.shape)\n","be2dec86":"\nnb = MultinomialNB()\nnb = nb.fit(train_df,df_tweet['target'])","8275a844":"y_train_pred = nb.predict(train_df)\nfrom sklearn.metrics import confusion_matrix\n\nconfusion_matrix(df_tweet['target'],y_train_pred)","3102ace4":"y_test_pred = nb.predict(test_df)\ny_test_pred","2614d56d":"res = pd.DataFrame({'id': df_test['id'], 'target': y_test_pred})","546903e2":"res.to_csv('submission.csv',index=False)","ee66e102":"\nvc = TfidfVectorizer()\ntrain_df,test_df = prepare_data(df_tweet,df_test,'keyword','text',vc)","36511049":"nb = MultinomialNB()\ny_train = df_tweet['target']\nnb = nb.fit(train_df,y_train)\ny_train_pred = nb.predict(train_df)\nconfusion_matrix(y_train,y_train_pred)","ea2dea95":"from tensorflow.keras import Sequential\nfrom tensorflow.keras.layers import Dense,Dropout\nfrom tensorflow.keras.layers.experimental.preprocessing import Rescaling","9bdd07fb":"\n\nmodel = Sequential()\nmodel.add(Dense(1024,activation='relu',input_shape =(18884,)))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(256,activation='relu'))\nmodel.add(Dropout(0.1))\nmodel.add(Dense(2,activation='softmax'))\n","5c086509":"vc = CountVectorizer()\ntrain_df,test_df = prepare_data(df_tweet,df_test,'keyword','text',vc)\n\nmodel.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['accuracy'])\nmodel.summary()","91bf1029":"history = model.fit(train_df,df_tweet['target'],validation_split=0.2,batch_size=60,epochs=10)","b521eb3a":"logs = history.history\n#plt.subplot(1,2,1)\nplt.plot(range(1,len(logs['accuracy'])+1),logs['accuracy'])\nplt.plot(range(1,len(logs['val_accuracy'])+1),logs['val_accuracy'])","87149958":"model = Sequential()\nmodel.add(Dense(1024,activation='relu',input_shape =(18884,)))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(256,activation='relu'))\nmodel.add(Dropout(0.1))\nmodel.add(Dense(2,activation='softmax'))\n\n\nmodel.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['accuracy'])\nmodel.summary()\n\n\nhistory = model.fit(train_df,df_tweet['target'],validation_split=0.2,batch_size=60,epochs=2)","d50b4a1c":"predictions = model.predict(test_df)\n\npredictions = pd.DataFrame(predictions)\n\npredictions.iloc[:,0] = predictions.iloc[:,0].apply(lambda x: 1 if x < 0.5 else 0)\n#predictions.iloc[:,1] = predictions.iloc[:,1].apply(lambda x: 1 if x > 0.5 else 0)\n\npredictions.iloc[:,0]","04b53e14":"df_test['target'] = predictions.iloc[:,0]","cc7c63d2":"df_test[['id','target']].to_csv('submission.csv',index=False)","0604e820":"from sklearn.ensemble import RandomForestClassifier\n\nfrom sklearn.decomposition import PCA\n","5f7da6af":"pca = PCA(n_components=500)\ntrain_df = pca.fit_transform(train_df)\ntrain_df = pd.DataFrame(train_df)","c719f64b":"test_df = pca.transform(test_df)","32e3523b":"from sklearn.ensemble import RandomForestClassifier\n\nparam = {\n    'max_depth': [10,20,50],\n    'min_samples_split': [10,50,100,500],\n    'n_estimators': [50,100]\n    \n}\n\nfrom sklearn.model_selection import GridSearchCV\n\nrf = GridSearchCV(estimator=RandomForestClassifier(random_state=100),param_grid=param,cv=5,scoring='balanced_accuracy',verbose=10,n_jobs=-1)\nrf.fit(train_df,df_tweet['target'])","438ce0a1":"df_tweet = pd.read_csv(\"..\/input\/nlp-getting-started\/train.csv\")\ndf_test = pd.read_csv(\"..\/input\/nlp-getting-started\/test.csv\")\nfrom tensorflow.keras.preprocessing.text import Tokenizer\ndf_tweet['text'] = df_tweet['text'].apply(lambda x: x.lower())\ndf_tweet['Sequence'] = df_tweet['text'] #+ \" \" +df_tweet['keyword']\ntrain_texts = list(df_tweet['Sequence'].apply(lambda x: x.split()))","0e8012e3":"train_words = list(set([j for i in train_texts for j in i]))\ntc = Tokenizer(filters=\" \")\ntc.fit_on_texts(train_words)","cb71fe47":"from tensorflow.keras.layers import LSTM,GRU,Embedding\ntrain_data = [tc.texts_to_sequences(x) for x in train_texts]","534eb7d6":"train_data = [[j[0] for j in i] for i in train_data]\n#print(train_data)\ntrain_data = [x[:25] for x in train_data]\n#print(train_data)\ntrain_data = [np.pad(np.asarray(x),(50 - len(x),0)) for x in train_data]","6c5bfe86":"print(train_data[0].shape,train_data[2])","16a48c68":"train_data = np.asarray(train_data)\ntrain_data.shape","2cd40224":"from tensorflow.keras.layers import GRU,SimpleRNN\nvocab = len(tc.word_index.values()) +1\nlstm = Sequential()\nlstm.add(Embedding(vocab,200,input_length = 50))\nlstm.add(GRU(256,return_sequences=False))\nlstm.add(Dropout(0.2))\nlstm.add(Dense(2,activation='softmax'))","69447756":"lstm.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics='accuracy')\nlstm.summary()","b900cfe4":"lstm.fit(train_data,df_tweet['target'].values,validation_split=0.25,batch_size=60,epochs=2)","a070a6b0":"df_test['text'] = df_test['text'].apply(lambda x: x.lower())\ntest_data = list(df_test['text'].apply(lambda x: x.split()))\ntest_data = [tc.texts_to_sequences(i) for i in test_data]\ntest_data =[[j if len(j) != 0 else [0] for j in i] for i in test_data]\ntest_data =[[j[0] for j in i] for i in test_data]","abdd84e0":"\ntest_data = [x[:25] for x in test_data]\ntest_data = np.asarray([np.pad(np.array(x),(50 - len(x),0)) for x in test_data])","498829e1":"predictions = lstm.predict(test_data)","d5afc8ba":"v = pd.DataFrame(predictions)\nv = v.iloc[:,0].apply(lambda x: 0 if x>0.5 else 1)\nv","be526e7d":"res = {\n    'id': list(df_test['id']),\n    'target':list(v)\n}\nres = pd.DataFrame(res)\nres.to_csv('submission1.csv',index=False)","ba1b5bad":"res['target'].head(30)","b2065822":"## Data is consistent and can be trained","4c9695f3":"## Model building","63f06f3b":"### We can see same places are taken different names\n- USA and United States are same\n- Almost 2500 + locations are blank \n- There are tweets which are city Specific. (Mumbai is in India and there is anothere tweet for Coountry level)\n- The distribution is very diverse \n\n#### Hence deciding to drop the column for further analysis\n\n","49455507":"### TF-IDF + Naive Bayes","f2a0f020":"### Try Random Forest","75f7cdd1":"### Porter stemmer reduced to 167 Classes\n- Now lets check snowball stemmer","8b01f5e9":"### Now let us see Lemmatizer","a6f9f70b":"### Check for null values","8eb21242":"### Let us experiment with this","e8bd550d":"### Let us see the distribution of keyword column","7bcfecda":"## The distribution of classes in the data set is fine \n\n- Let us do some data cleaning\n- We will bring the whole text into single case","c2435063":"### Count vectorizers were better for naive bayes\n- TF-IDF + Feed Forward Neural Network I have made tfIFdf vectors so there is no sementacic associations","a6078613":"### Reduce test data dimension","128651b6":"### We can see different versions like singular\n-  We will replace `%20` with `space` and stem them","0a3ad3b4":"### Columns are consistent let us fit naive bayes","2b516636":"### Predict on test set","12d37987":"### This is generating more classes in the data set\n- We will go with Either Porter Stemmmer or Snowball Stemer","028c93a1":"### Let us do the preprocessing in Test data also","e51626e1":"## Loading the tweet data set","7c02c036":"### Check for class imbalance","ce78d028":"### The classes are consistent in train and test data\n\nNow we can go for model builing part","41c25557":"### Let us see the number of distinct loactions","7deb8ebd":"### Let us have 3 Eopchs","bc8028ff":"from ","3f278dcb":"### Let us try Ensembles","b886e52b":"### Training the model","94e90f4e":"### Importing text preprocessing libraries","9af7c180":"### Predictions"}}