{"cell_type":{"b404e82a":"code","eb529128":"code","d4f7a439":"code","84775caa":"code","b1d447b7":"code","fee9c021":"code","6f4913bc":"code","5098425d":"code","c1644dcd":"code","24011f34":"code","cbbc5163":"code","40fbe8d6":"code","d457223a":"code","c7b3e78b":"code","7700f381":"code","9a868a60":"code","8440242f":"code","59669338":"code","02dd3bc1":"code","846a5fc2":"code","74a7eb4c":"code","1d6b6c97":"code","6b93470c":"code","2adfe642":"code","ff5afedf":"code","0e7d2519":"code","7941bb79":"code","2fb45772":"code","be14c3ec":"code","910b5782":"code","2b788a6a":"code","9e8ec21e":"code","f52a635d":"code","5e3115bb":"code","661602d6":"code","0ac0e7f0":"code","7478a434":"code","b7e347e9":"code","02e10257":"code","c874ff28":"code","48660278":"code","db3c8d5e":"code","506eeb2d":"code","69227493":"code","f4ddae03":"code","e507f428":"code","eef9fa2a":"code","760864e1":"code","7f078b77":"code","7a04c47f":"code","9fd00e64":"code","1cdc1e2f":"code","b71f0a0e":"code","d20013fc":"code","c75a9d5e":"code","4f30f57c":"code","c2c908e8":"code","64371275":"code","003c0347":"code","7d056363":"code","a1d32afd":"code","eeb2ad4c":"code","9c750003":"code","11028946":"code","0e2e6ccf":"code","04a850de":"code","a4f3c8da":"code","038507a7":"code","14e14c96":"code","3fa78de3":"markdown","1c32911e":"markdown","6ed0260d":"markdown","e6ea552e":"markdown","36e28c29":"markdown","1992352e":"markdown","f1e1dcd3":"markdown","4477a95b":"markdown","cb40f965":"markdown","2ac82f24":"markdown","c2e95af5":"markdown","1bcf0256":"markdown","ff9a03db":"markdown","6d5792f0":"markdown","cdd60d69":"markdown","6c8c08e2":"markdown","671d8a7b":"markdown","8277845c":"markdown","81318311":"markdown","9eb6214d":"markdown","f7397fb2":"markdown","54369088":"markdown","98b3f123":"markdown","2f9b3b84":"markdown","cd1bad30":"markdown","7f2e7d4c":"markdown","fadca465":"markdown","8551b1fa":"markdown","19ee3695":"markdown","1836cd5f":"markdown","f7ac17b8":"markdown","b64dc4eb":"markdown","e0ae34b4":"markdown","50c6e1aa":"markdown","d1d637f1":"markdown","b82dea37":"markdown","d5f069ef":"markdown","5b5147d3":"markdown","0d94b73d":"markdown","a0c97f86":"markdown","3fe747d5":"markdown","658029a7":"markdown","5175d21c":"markdown","ececa0d4":"markdown","95cfeb3f":"markdown","cb7d596d":"markdown","e8109501":"markdown","20bf97f2":"markdown","bb536478":"markdown","85140b42":"markdown"},"source":{"b404e82a":"import numpy as np # Numpy for neumeric algebra{\nimport pandas as pd # pandas for data \nimport seaborn as sns # seaborn and matplotlib for data visualization \nfrom matplotlib import pyplot as plt","eb529128":"df=pd.read_excel(\"..\/input\/bank-loan-modelling\/Bank_Personal_Loan_Modelling.xlsx\",\"Data\")","d4f7a439":"df.head()","84775caa":"print(\"There are {} rows & {} columns in our dataset.\".format(df.shape[0],df.shape[1]))","b1d447b7":"df.info()","fee9c021":"print(\"+++++++++++Check for null values in our dataset+++++++++++\")\n\nprint(df.isna().apply(pd.value_counts))","6f4913bc":"df.drop(['ID','ZIP Code'],axis=1,inplace=True)","5098425d":"df.describe().T","c1644dcd":"df.boxplot(return_type='axes',figsize=(10,8),column=['Age','Experience','Income','Family','Education']);","24011f34":"df.skew(numeric_only=True)","cbbc5163":"columns=list(df)\ndf[columns].hist(stacked=True,density=True, bins=100,color='Orange', figsize=(16,30), layout=(10,3)); ","40fbe8d6":"sns.distplot(df['Age'],kde=True,hist=False,color='Red')\nsns.distplot(df['Income'],kde=True,hist=False,color='Green')\nsns.distplot(df['Experience'],kde=True,hist=False,color='blue')\nplt.show()","d457223a":"Negative_values_in_exp=df[df['Experience']< 0]\nsns.distplot(Negative_values_in_exp['Age'],kde=True,label=\"Distribution of Age which has negative values\");\navg_exp=df['Experience'].mean()\nprint(\"The Average exp is : {}\".format(avg_exp))\nN1=Negative_values_in_exp[Negative_values_in_exp['Age'].between(22,31)]\navg_exp2=N1['Experience'].mean()\npositive_values_for_age_group=df[df['Experience']>0]\nget_mean_value=positive_values_for_age_group[positive_values_for_age_group['Age'].between(22,31)]\nmean_value=get_mean_value['Experience'].mean()\n\nprint(\"The Avg exp for people in this age grouop:{}\".format(avg_exp2))\nperct=df.shape[0]\nperct=Negative_values_in_exp.shape[0]\/perct\nperct = perct * 100\nprint(\"There are {} records which has negative values for Experience, approx {}% among age group between 22 to 30\".format(Negative_values_in_exp.shape[0],perct))\nprint(\"The mean value that we can use is :{} \".format(mean_value))\n\n","c7b3e78b":"# I am using mask function to change the negative values to mean value derived from data with the same age group \ndf['Experience']=df['Experience'].mask(df['Experience']<0,mean_value)    \nprint(\"After updating the negative records we get the mean value of exp to be:{}\".format(df['Experience'].mean()))","7700f381":"def plot_corr(df, size=10):\n    corr = df.corr()\n    fig, ax = plt.subplots(figsize=(size, 25))\n    ax.matshow(corr)\n    plt.xticks(range(len(corr.columns)), corr.columns)\n    plt.yticks(range(len(corr.columns)), corr.columns)","9a868a60":"plot_corr(df)","8440242f":"df=df.drop(['Experience'],axis=1)","59669338":"def edu(row):\n    if row['Education']==1:\n        return \"Undergrad\"\n    elif row['Education']==2:\n        return \"Graduate\"\n    else:\n        return \"Advanced\/Professional\"\ndf['EDU']=df.apply(edu,axis=1)","02dd3bc1":"EDU_dis=df.groupby('EDU')[\"Age\"].count()\nEDU_dis.plot.pie(shadow=True, startangle=170,autopct='%.2f')","846a5fc2":"def SD_CD(row):\n    if (row['Securities Account']==1) & (row['CD Account']==1):\n        return\"Holds Securities & deposit\"\n    elif(row['Securities Account']==0) & (row['CD Account']==0):\n        return\"Does not hold any securities or deposit\"\n    elif(row['Securities Account']==1) & (row['CD Account']==0):\n        return \"Holds only Securities Account\"\n    elif(row['Securities Account']==0) & (row['CD Account']==1):\n        return\"Holds only deposit\"  ","74a7eb4c":"df['Account_Holder_Category']=df.apply(SD_CD,axis=1)","1d6b6c97":"df['Account_Holder_Category'].value_counts().plot.pie(shadow=True, startangle=125,autopct='%.2f')","6b93470c":"sns.boxplot(df['Education'],df['Income'],hue=df['Personal Loan']);","2adfe642":"plt.figure(figsize=(12,8))\nsns.distplot(df[df['Personal Loan']==0]['Income'],kde=True,color='r',hist=False,label=\"Income distribution for customers with no personal Loan\")\nsns.distplot(df[df['Personal Loan']==1]['Income'],kde=True,color='G',hist=False,label=\"Income distribution for customers with personal Loan\")\nplt.legend()\nplt.title(\"Income Distribution\")","ff5afedf":"plt.figure(figsize=(12,8))\nsns.distplot(df[df['Personal Loan']==0]['CCAvg'],kde=True,hist=False,color='r',label=\"Credit card average for customers with no personal Loan\")\nsns.distplot(df[df['Personal Loan']==1]['CCAvg'],kde=True,hist=False,color='G',label=\"Credit card average for customers with personal Loan\")\nplt.legend()\nplt.title(\"CCAvg Distribution\")","0e7d2519":"plt.figure(figsize=(12,8))\nsns.distplot(df[df['Personal Loan']==0]['Mortgage'],kde=True,hist=False,color='r',label=\"Mortgage of customers with no personal Loan\")\nsns.distplot(df[df['Personal Loan']==1]['Mortgage'],kde=True,hist=False,color='G',label=\"Mortgage of customers with personal Loan\")\nplt.legend()\nplt.title(\"Mortgage Distribution\")","7941bb79":"col_names=['Securities Account','Online','CreditCard']\n\nfor i in col_names:\n    plt.figure(figsize=(14,12))\nj=2\nk=0\nfor i in col_names:\n    plt.subplot(2,j,j*(k+1)\/\/j)\n    sns.countplot(x=i,hue='Personal Loan',palette=\"Blues\", data=df)\n    k=k+1\n    plt.grid(True)\nplt.show()\n","2fb45772":"plt.figure(figsize=(14,12))\nsns.countplot(df['Account_Holder_Category'],hue=df['Personal Loan'],palette='Blues')\nplt.show();","be14c3ec":"df.head()","910b5782":"Data=df.drop(['EDU','Account_Holder_Category'],axis=1)","2b788a6a":"Data.head()","9e8ec21e":"Data.describe().T","f52a635d":"import scipy.stats as stats\nsns.scatterplot(Data['Age'],Data['Personal Loan'],hue=Data['Family'],alpha=0.8);\n\n#Frame the Hypothesis\nH0=\"Age does not have any impact on availing personal Loan\"\nHa=\"Age does have phenomenal significance on availing personal Loan\"\n\nAge_PL_Yes=np.array(Data[Data['Personal Loan']==1].Age)\nAge_PL_No=np.array(Data[Data['Personal Loan']==0].Age)\n\nt,p_value=stats.ttest_ind(Age_PL_Yes,Age_PL_No,axis=0)\n\nif p_value < 0.05:\n    #We reject Null\n    print(Ha,\"As the P_value is less than 0.05 with a value of :{}\".format(p_value))\nelse:\n    #We fail to reject Null\n    print (H0,\"As the P_value is Greater than 0.05 with a value of :{}\".format(p_value))","5e3115bb":"sns.scatterplot(Data['Age'],Data['Personal Loan'],hue=Data['Income'],alpha=0.8);\nIncome_PL_Yes=np.array((Data[Data['Personal Loan']==1]).Income)\nIncome_PL_No=np.array((Data[Data['Personal Loan']==0]).Income)\n\nH0=\"Income of a person does not have an impact on availing Personal Loan\"\nHa=\"Income of a person has significant impact on availing Personal Loan\"\n\nt,p_value=stats.ttest_ind(Income_PL_Yes,Income_PL_No,axis=0)\n\nif p_value < 0.05:\n    #Reject Null\n    print(Ha,\"As the P_value is less than 0.05 with a value of :{}\".format(p_value))\n    print(\"As you can see from the plot, those who availed Personal Loan tend to have higher income\")\nelse:\n    #We fail to reject Null\n    print (H0,\"As the P_value is Greater than 0.05 with a value of :{}\".format(p_value))\n    ","661602d6":"sns.scatterplot(Data['Age'],Data['Personal Loan'],hue=Data['Family'],alpha=0.8);\nFamily_PL_Yes=np.array((Data[Data['Personal Loan']==1]).Family)\nFamily_PL_No=np.array((Data[Data['Personal Loan']==0]).Family)\n\nH0=\"Number of persons in the family does not have an impact on availing Personal Loan\"\nHa=\"Number of persons in the family has significant impact on availing Personal Loan\"\n\nt,p_value=stats.ttest_ind(Family_PL_Yes,Family_PL_No,axis=0)\n\nif p_value < 0.05:\n    #Reject Null\n    print(Ha,\"As the P_value is less than 0.05 with a value of :{}\".format(p_value))\n    #print(\"As you can see from the plot, those who availed Personal Loan tend to have higher income\")\nelse:\n    #We fail to reject Null\n    print (H0,\"As the P_value is Greater than 0.05 with a value of :{}\".format(p_value))","0ac0e7f0":"TAB=pd.crosstab(Data['Education'],Data['Personal Loan'])\nchi,p_value,dof,expected=stats.chi2_contingency(TAB)\n\nH0=\"Educational qualification does not have an influence on a person to avail Loan\"\nHa=\"Educational qualification does have an influence on a person to avail Loan\"\n\nif p_value < 0.05:\n    #Reject Null\n    print(Ha)\nelse:\n    #Fail to reject Null\n    print(H0)","7478a434":"print(Data['Personal Loan'].value_counts())\nNo_of_customers_availed_PL=Data[Data['Personal Loan']==1].shape[0]\nNo_of_customers_availed_PL\nTotal_Cust=Data.shape[0]\npercet=(No_of_customers_availed_PL * 100)\/Total_Cust \nprint(\"Overall percentage of customers who have availed personal Loan:{}\".format(percet),\"%\")","b7e347e9":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import f1_score\n\n#Split our Data into Dependent Variables & Independent Variables \n\nX=Data.drop(['Personal Loan'],axis=1)\ny=Data['Personal Loan']\n","02e10257":"#Split dataset into train and test, as suggested this method will \n\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.3,random_state=23)","c874ff28":"Model1_raw=LogisticRegression(solver='liblinear')\nModel1_raw.fit(X_train,y_train)","48660278":"Model1_raw_coef=pd.DataFrame(Model1_raw.coef_)\nModel1_raw_coef","db3c8d5e":"Model1_raw.score(X_train,y_train)","506eeb2d":"Model1_raw.score(X_test,y_test)","69227493":"Model1_raw_prediction=Model1_raw.predict(X_test)","f4ddae03":"chk_model1=pd.DataFrame({\"Actual\":y_test,\"Predicted\": Model1_raw_prediction})\nTop=chk_model1.nlargest(25,'Predicted')\nTop.plot(kind='bar',figsize=(15,10));\nplt.grid(which='major', linestyle='-', linewidth='0.5', color='green')\nplt.grid(which='minor', linestyle=':', linewidth='0.5', color='black')\nplt.show()","e507f428":"cm_model1=confusion_matrix(y_test,Model1_raw_prediction,labels=[0,1])\nprint(cm_model1)\nacc_score_log1=accuracy_score(y_test,Model1_raw_prediction)\nf1_score_log1=f1_score(y_test,Model1_raw_prediction)\nprint(\"Accuracy Score  for Logistic Regression RAW DATA:{}\".format(acc_score_log1*100))\nprint(\"F1 Score  for Logistic Regression RAW DATA:{}\".format(f1_score_log1*100))\nprint(classification_report(y_test,Model1_raw_prediction))","eef9fa2a":"from  sklearn.utils import resample\ndf_majority=Data[Data['Personal Loan']==0]\ndf_minority=Data[Data['Personal Loan']==1]\ndf_upsample_minority=resample(df_minority,replace=True,random_state=12,n_samples=4520)\ndf_upsample=pd.concat([df_majority,df_upsample_minority])\ncount=df_upsample['Personal Loan'].value_counts()\ncount.plot(kind='bar',figsize=(5,5));","760864e1":"X_upsampled=df_upsample.drop(['Personal Loan'],axis=1)\nY_upsampled=df_upsample['Personal Loan']","7f078b77":"X_upsampled_test,X_upsampled_train,Y_upsampled_test,Y_upsampled_train=train_test_split(X_upsampled,Y_upsampled,random_state=1,test_size=0.3)","7a04c47f":"Model1_raw.fit(X_upsampled_train,Y_upsampled_train)","9fd00e64":"Pred_Upsample_Log=Model1_raw.predict(X_upsampled_test)\ncm_model1_upsample=confusion_matrix(Y_upsampled_test,Pred_Upsample_Log,labels=[0,1])","1cdc1e2f":"acc_score_log1=accuracy_score(y_test,Model1_raw_prediction)\nf1_score_log1=f1_score(y_test,Model1_raw_prediction)\nprint(\"Accuracy Score  for Logistic Regression with RAW DATA:{}\".format(acc_score_log1*100))\nprint(\"F1 Score  for Logistic Regression with RAW DATA:{}\".format(f1_score_log1*100))\nprint(\"+++++++++++++++THE CONFUSION MATRIX LOGISTIC REGRESSION  WITH RAW DATA++++++++++++++++\")\nprint(\"Confusion Matrix: \\n\",cm_model1)\nprint(\"+++++++++++++++CLASSIFICATION REPORT LOGISTIC REGRESSION WITH RAW DATA++++++++++++++++\")\nprint(classification_report(y_test,Model1_raw_prediction))\nacc_score_upsampled=accuracy_score(Y_upsampled_test,Pred_Upsample_Log)\nf1_score_upsampled=f1_score(Y_upsampled_test,Pred_Upsample_Log)\nprint(\"Accuracy Score  for Logistic Regression Upsampled Date:{}\".format(acc_score_upsampled*100))\nprint(\"F1 Score  for Logistic Regression:{}\".format(f1_score_upsampled*100))\nprint(\"+++++++++++++++THE CONFUSION MATRIX LOGISTIC REGRESSION WITH Upsampled Data+++++++++++++++++\")\nprint(\"Confusion Matrix: \\n\",cm_model1_upsample)\nprint(\"+++++++++++++++CLASSIFICATION REPORT LOGISTIC REGRESSION WITH Upsampled Data++++++++++++++++\")\nprint(classification_report(Y_upsampled_test,Pred_Upsample_Log))","b71f0a0e":"from sklearn.naive_bayes import GaussianNB\nModel2_nb=GaussianNB()","d20013fc":"Model2_nb.fit(X_upsampled_train,Y_upsampled_train)\nModel2_nb.fit(X_train,y_train)","c75a9d5e":"Model2_nb.fit(X_upsampled_test,Y_upsampled_test)\nModel2_nb.fit(X_test,y_test)","4f30f57c":"print(\"####################Raw Data Score####################\")\nprint(Model2_nb.score(X_train,y_train))\nprint(Model2_nb.score(X_test,y_test))\nprint(\"####################Sample Data Score####################\")\nprint(Model2_nb.score(X_upsampled_train,Y_upsampled_train))\nprint(Model2_nb.score(X_upsampled_test,Y_upsampled_test))","c2c908e8":"Pred_nb_raw=Model2_nb.predict(X_test)\nPred_nb_Upsampled=Model2_nb.predict(X_upsampled_test)\nCM_NB_RAW=confusion_matrix(y_test,Pred_nb_raw)\nCM_NB_UPSAMPLE=confusion_matrix(Y_upsampled_test,Pred_nb_Upsampled)","64371275":"acc_score_NB_Raw=accuracy_score(y_test,Pred_nb_raw)\nf1_score_NB_raw=f1_score(y_test,Pred_nb_raw)\nprint(\"Accuracy Score  for Naive Bayes Model RAW DATA:{}\".format(acc_score_NB_Raw*100))\nprint(\"F1 Score  for Naive Bayes Model RAW DATA:{}\".format(f1_score_NB_raw*100))\nprint(\"+++++++++++++++THE CONFUSION MATRIX RAW DATA++++++++++++++++\")\nprint(\"Confusion Matrix: \\n\",CM_NB_RAW)\nprint(\"+++++++++++++++CLASSIFICATION REPORT RAW DATA++++++++++++++++\")\nprint(classification_report(y_test,Pred_nb_raw))\nacc_score_NB_upsampled=accuracy_score(Y_upsampled_test,Pred_nb_Upsampled)\nf1_score_NB_upsampled=f1_score(Y_upsampled_test,Pred_nb_Upsampled)\nprint(\"Accuracy Score  for Naive Bayes Model Upsampled Data:{}\".format(acc_score_NB_upsampled*100))\nprint(\"F1 Score  for Naive Bayes Model:{}\".format(f1_score_NB_upsampled*100))\nprint(\"+++++++++++++++THE CONFUSION MATRIX Upsampled Data++++++++++++++++\")\nprint(\"Confusion Matrix: \\n\",CM_NB_UPSAMPLE)\nprint(\"+++++++++++++++CLASSIFICATION REPORT Upsampled Data++++++++++++++++\")\nprint(classification_report(Y_upsampled_test,Pred_Upsample_Log))","003c0347":"print(\"#############Inference from Logistic Regression & Naive bayes Model###############\")\n#print(\"Precision score of Logistic Regression is 84% , whereas Precsion of Naive bayes is 44%\\nThis means,out of all customers who are likely to buy personal loan Logistic model predicted 84% to be right.\\nRecall score for both Logistic and Naive bayes model is same (57%)\\nThis means out of all customers who will buy loan both models predicted only 57% correctly, which means both miss to provide insight about the other 43% data which is crucial\")\n\nif (acc_score_log1 > acc_score_upsampled):\n    print(\"Accuracy score of Logistic Regression with Raw Data is higher than Upsampled Data\")\nelse:\n    print(\"Accuracy score of Logistic Model with Upsampled Data is greater\")\nif (f1_score_log1 > f1_score_upsampled):\n    print(\"F1 score of Logistic model with Raw Data is higher than Upsampled Data\")\nelse:\n    print(\"F1 score of Logistic Model with Upsampled Data is greater\")\n   \n\n        ","7d056363":"from sklearn.neighbors import KNeighborsClassifier","a1d32afd":"Model3_KNN=KNeighborsClassifier(n_neighbors=11)","eeb2ad4c":"Model3_KNN.fit(X_train,y_train)","9c750003":"Predict_knn=Model3_KNN.predict(X_test)","11028946":"CM_KNN=confusion_matrix(y_test,Predict_knn)","0e2e6ccf":"Model3_KNN.fit(X_upsampled_train,Y_upsampled_train)","04a850de":"Predict_KNN_Sampled=Model3_KNN.predict(X_upsampled_test)","a4f3c8da":"CM_KNN_SAMPLED=confusion_matrix(Y_upsampled_test,Predict_KNN_Sampled)","038507a7":"acc_score_KNN_Raw=accuracy_score(y_test,Predict_knn)\nf1_score_KNN_raw=f1_score(y_test,Predict_knn)\nprint(\"Accuracy Score  for KNN with RAW DATA:{}\".format(acc_score_KNN_Raw*100))\nprint(\"F1 Score for KNN with RAW DATA:{}\".format(f1_score_KNN_raw*100))\nprint(\"+++++++++++++++THE CONFUSION MATRIX for KNN RAW DATA++++++++++++++++\")\nprint(\"Confusion Matrix: \\n\",CM_KNN)\nprint(\"+++++++++++++++CLASSIFICATION REPORT for KNN RAW DATA++++++++++++++++\")\nprint(classification_report(y_test,Predict_knn))\nacc_score_KNN_upsampled=accuracy_score(Y_upsampled_test,Predict_KNN_Sampled)\nf1_score_KNN_upsampled=f1_score(Y_upsampled_test,Predict_KNN_Sampled)\nprint(\"Accuracy Score  for KNN with Upsampled Data:{}\".format(acc_score_KNN_upsampled*100))\nprint(\"F1 Score  for for KNN with Upsampled Data:{}\".format(f1_score_KNN_upsampled*100))\nprint(\"+++++++++++++++THE CONFUSION MATRIX KNN Upsampled Data++++++++++++++++\")\nprint(\"Confusion Matrix: \\n\",CM_KNN_SAMPLED)\nprint(\"+++++++++++++++CLASSIFICATION REPORT KNN Upsampled Data++++++++++++++++\")\nprint(classification_report(Y_upsampled_test,Predict_KNN_Sampled))","14e14c96":"#Import metrics\n\nfrom sklearn.metrics import roc_curve,auc,roc_auc_score\n\n#Perform Predictions for all models both with raw data and upsampled data\n\nPRED_PROB_LOG_RAW=Model1_raw.predict_proba(X_test)\nPRED_PROB_LOG_SAMPLED=Model1_raw.predict_proba(X_upsampled_test)\n\nPRED_PROB_NB_RAW=Model2_nb.predict_proba(X_test)\nPRED_PROB_NB_SAMPLED=Model2_nb.predict_proba(X_upsampled_test)\n\nPRED_PROB_KNN_RAW=Model3_KNN.predict_proba(X_test)\nPRED_PROB_KNN_SAMPLED=Model3_KNN.predict_proba(X_upsampled_test)\n\n#calculate fpr,tpr,threshold\nfpr1, tpr1, thresh1 = roc_curve(y_test, PRED_PROB_LOG_RAW[:,1], pos_label=1)\nfpr2,tpr2,thresh2= roc_curve(Y_upsampled_test,PRED_PROB_LOG_SAMPLED[:,1],pos_label=1)\nfpr3,tpr3,thresh3=roc_curve(y_test,PRED_PROB_NB_RAW[:,1],pos_label=1)\nfpr4,tpr4,thresh4=roc_curve(Y_upsampled_test,PRED_PROB_NB_SAMPLED[:,1],pos_label=1)\nfpr5,tpr5,thresh5=roc_curve(y_test,PRED_PROB_KNN_RAW[:,1],pos_label=1)\nfpr6,tpr6,thresh6=roc_curve(Y_upsampled_test,PRED_PROB_KNN_SAMPLED[:,1],pos_label=1)\n\n\nrandom_probs = [0 for i in range(len(y_test))]\np_fpr, p_tpr, _ = roc_curve(y_test, random_probs, pos_label=1)\n\n\nAUC_LOG_RAW=roc_auc_score(y_test,PRED_PROB_LOG_RAW[:,1])\nAUC_LOG_SAMPLED=roc_auc_score(Y_upsampled_test,PRED_PROB_LOG_SAMPLED[:,1])\nAUC_NB_RAW=roc_auc_score(y_test,PRED_PROB_NB_RAW[:,1])\nAUC_NB_UPSAMPLED=roc_auc_score(Y_upsampled_test,PRED_PROB_NB_SAMPLED[:,1])\nAUC_KNN_RAW=roc_auc_score(y_test,PRED_PROB_KNN_RAW[:,1])\nAUC_KNN_UPSAMPLED=roc_auc_score(Y_upsampled_test,PRED_PROB_KNN_SAMPLED[:,1])\n\nAUC_SCORES=pd.array([AUC_LOG_RAW,AUC_LOG_SAMPLED,AUC_NB_RAW,AUC_NB_UPSAMPLED,AUC_KNN_RAW,AUC_KNN_UPSAMPLED])\n\n\n\n#Plot Area Under Curve\n\nplt.plot(fpr1,tpr1,linestyle='--',color='orange', label='Logistic Regression RAW')\nplt.plot(fpr2,tpr2,linestyle='solid',color='blue', label='Logistic Regression Sampled')\nplt.plot(fpr3,tpr3,linestyle='--',color='Green', label='Naive Bayes RAW')\nplt.plot(fpr4,tpr4,linestyle='solid',color='Red', label='Naive Bayes Sampled')\nplt.plot(fpr5,tpr5,linestyle='--',color='violet',label='KNN RAW')\nplt.plot(fpr6,tpr6,linestyle='solid',color='black',label='KNN Upsampled')\n\nplt.plot(p_fpr, p_tpr, linestyle=':', color='red')\n# title\nplt.title('ROC curve')\n# x label\nplt.xlabel('False Positive Rate')\n# y label\nplt.ylabel('True Positive rate')\n\nplt.legend(loc='best')\nplt.savefig('ROC',dpi=300)\nplt.show();\n\nprint(\"ROC_AUC_Score for Logistic Regression with Raw Data:{}\".format(AUC_LOG_RAW))\nprint(\"ROC_AUC_Score for Logistic Regression with Upsample  Data:{}\".format(AUC_LOG_SAMPLED))\nprint(\"ROC_AUC_Score for Naive Bayes with Raw Data:{}\".format(AUC_NB_RAW))\nprint(\"ROC_AUC_Score for Naive Bayes with Upsample  Data:{}\".format(AUC_NB_UPSAMPLED))\nprint(\"ROC_AUC_Score for KNN with Raw Data:{}\".format(AUC_KNN_RAW))\nprint(\"ROC_AUC_Score for KNN with Upsample  Data:{}\".format(AUC_KNN_UPSAMPLED))  \nprint(\"=======================================================================\")\nprint(\"The Best AUC_SCORE that we have got is :{}\".format(AUC_SCORES.max()))\n","3fa78de3":"From the above graph we could infer that , customers who hold deposit account & customers who do not hold either a securities account or deposit account have aviled personal loan","1c32911e":"Distribution of parameters ","6ed0260d":"ID and Zipcode might be removed as they may not be useful for our analysis","e6ea552e":"As said above, the extent of Class Imbalance is higher in our data. How are we going to ensure our model does not make biased decisions?\n\nThere are couple of ways to achieve this and this technique is called resampling, Resampling is of two types \n\n1. Up-sample Minority Class\n2. Down-sample Majority Class\n\nResample from skilearn is our solution.\n\nFor this problem Upsampling seemed to work better. \n\nNote: I did try down sampling by using resample(df_minority,replace=False) but it did not have any impact on Precision and recall scores for any models , But Up sampling had phenomenol impact on these metrics(Precision, Recall, F1 score). \n\nFurtherore, Upsampling allows us to retain the information infact we add more information to the dataset, wheras downsampling does not gives us this privilage. Hence, in most cases it is better to go for upsampling.","36e28c29":"Lets Explore the account holder's distribution","1992352e":"Imbalance in dataset:\n\nAs you could see, our Target variable is not equally distributed, only 9.6% of customers have availed Personal Loan. So, if our model is going to learn from this dataset and do the prediction chances are there that it might be biased towards the Majority class (In this case , Personal loan not being availed by the customer) and ignore the minority class. Hence , we should try to balance our dataset to make our model learn and predict with being biased and treat both classes equally for better result.\n","f1e1dcd3":"Split Independent and dependent variables","4477a95b":"# ROC score and AUC to see how good our models have worked ","cb40f965":"Let's read the dataset ","2ac82f24":"# Conclusion\n\n\nROC curve is plotted with TPR against the FPR where TPR is on y-axis and FPR is on the x-axis\n\nROC is a probability curve and AUC represents degree or measure of separability. It tells how much model is capable of distinguishing between classes\n\nHigher the AUC, better the model is at distinguishing between a customer buying personal Loan and Not buying Personal Loan.\n\nAn excellent model has AUC near to the 1 which means it has good measure of separability, As you can see all our models both with raw data and upsampled data have AUC scores near to 1. \n\nHowever, The AUC score for Logistic Regression Model with Upsampled data has the highest number, so does the ROC curve too. \n\n\nThe Blue line shows that our Logistic model on sampled data alomst covers more region and 96% can predict our class covering both cutomers who will buy and not buy Personal Loan.\n\nBy All means , Accuracy, Precision, Recall , ROC & AUC our Logistic regression model on sampled data is the best model for this classification problem though other models are equally close, Our Logistic model has the slight edge with better AUC , ROC as it has better class separability.","c2e95af5":"# KNN Algorithm with Raw and Upsampled Data ","1bcf0256":"ROC & AUC for Classification Models\n\nFrom our Analysis thus far , We could see that the accuracy scores with Raw data was good, But accuracy should not be considered as a metric of evaluation when there is class imbalance.\n\nHence we used upsampling to see how the parameters vary with a balanced dataset. \n\nPrecision,recall and F1 score with upsampled data was way better in upsampled data for all our models Logistic,KNN and Naive Bayes, but for some models preciosn was good for others recall was good. \n\nSo how do we conclude which model is good ???\n\nTo answer this question we will use ROC and AUC to evaluate the performance our models with False postive ratio\/True positive ratio\n","ff9a03db":"As , you can see the upsampled data produced a accouracy score of 91%, with 93% recall and 89% Precision. Whereas, Raw data had 54% recall and 84% Precision.\n\n##Take Away Logistic Regresison :\n-----------------------------------------------\n\n1. Recall of 93% infers, Among customers who bought the loan our model predicted 93% to be right.\n2. Precison Score of 89% implies among predcited positive how much was actual positive, Among total customers we predicted how    much customers actually bought the loan \n3. We should also ensure that the type1 & type 2 errors are minimal in an ideal model.\n4. In our Logistic model with Upsampled data, 357 customers were (False postive) predicted to buy the loan but they did not\n5. 223 were predicted not to buy the loan but they bought \n\nHere type2 error doesn't make a negative impact , as 223 who we predicted will not buy actually bought, this is not going to have much negative impact on the problem. ","6d5792f0":"# Plot Actual vs predicted to see how have we done so far....","cdd60d69":"Using df.head we see the initial 5 rows of how our data looks","6c8c08e2":"# Let's Check some hypothesis...","671d8a7b":"Though we have a very good accuracy score of 94%, We should not consider this score as our data is highly imbalanced. \n\nFor Imbalanced datasets it is wise to look for precision and recall rather than accuracy.\n\nRecall\/Sensitivity:: Our Logistic Regression model on  Raw data has given a recall score of 57%, which means among the total number of customers who bought the loan, our model predicted only 57% who bought the loan. This is bad, With this score the bank does not gain much.\n\nPrecision (True Positive Rate) means out of customers who we predicted to buy the loan how many actually bought the loan which is 84% , it is a decent score but it could be better ....","8277845c":"INFERENCE from Histogram","81318311":"Now lets see the distribution of Target class with a Bar plot","9eb6214d":"From the above plot we could say that Income of customers who availed personal loan are alomst same irrescpective of their Education","f7397fb2":"As you could see from a sample of top 25 values , our model has mostly predicted corretly compared to actual","54369088":"We could see that alomst 87% of customers do not hold any securities or deposit, and 3 % hold both securities as well as deposit. It will be good if we encourage those 87% to open any of these account as it will improve the assests of the bank","98b3f123":"Now class imbalance has been rectified , as you can see from above both the classes for our Dependent variable \"Personal Loan\" are same, we have done minority upsampling to have equal distribution of both the classes.\n\nWhat's Next ???\n\nWe going to run Logistic Regression & Naive bayes on both the Raw data and Upsampled data to see how key metrics such as Accuracy,Precision and Recall is going to vary in our Models.","2f9b3b84":"# ++++++++++++++++ TARGET VARIABLE CALLOUT ++++++++++++++++","cd1bad30":"# Exploratory Data Analysis","7f2e7d4c":"Exploring the column names and attributes is a crucial part in EDA ","fadca465":"Let's drop EDU & Account_Holder_Category columns as they may not be of need for the further prediction as I created and used them for EDA pupose only.","8551b1fa":"# Naive Bayes with both Raw & Upsampled Data","19ee3695":"As , you can see the upsampled data produced a accouracy score of 87%, with 94% recall and 83% Precision. Whereas, Raw data had 26% recall and 60% Precision.\n\n##Take Away KNN :\n---------------------------\n\n1. Recall of 94% infers, Among customers who bought the loan our model predicted 93% to be right.\n2. Precison Score of 83% implies among predcited positive how much was actual positive, Among total customers we predicted how    much customers actually bought the loan\n3. We should also ensure that the type1 & type 2 errors are minimal in an ideal model.\n4. In our KNN model with Upsampled data, 595 customers were (False postive) predicted to buy the loan but they did not\n5. 180 were predicted not to buy the loan but they bought.","1836cd5f":"1. Age & Experience are to an extent equally distributed\n2. Income & Credit card spending are skewed to the left\n3. We have more Undergraduates than Graduate and Advanced & Professional\n4. 60% of customers have enabled online banking and went digital","f7ac17b8":"We could see that Age & Experience are very strongly correlated, Hence it is fine for us to  go with Age and drop Experience to avoid multi-colinearity issue.","b64dc4eb":"Inference :We could see that We have more Undergraduates 41.92%  than graduates(28.06%) & Advanced Professional(30.02%)","e0ae34b4":"# Call the Train to split the data,Training Data=70%, Test Data=30%","50c6e1aa":"Split upsampled data with the same 70:30 ratio for training and testing ","d1d637f1":"we could see that CCAvg,Mortage,Personal Loan, Securities account,CD account are highly skewed\nAge,Experience,incomeFamily,Education are approximately symentric\n","b82dea37":"Age,Experience,income,Family,Education are approximately symentric","d5f069ef":"We do not have any null values in our data","5b5147d3":"Inference: We could see that all columns are non null.\n           The dataset we deal with has data types of int & float(Nuemeric Data)","0d94b73d":"Customers Who have availed personal loan seem to have higher income than those who do not have personal loan","a0c97f86":"# Preparing the data to train a model","3fe747d5":"1. Five point summary suggest that Experience has negative value(This should be fixed).\n2. we can see the Min, Max, mean and std deviation for all key attributes of the dataset\n3. Income has too much noise and slightly skewed right, Age and exp are equally distributed. ","658029a7":"How Age of a person is going to be a factor in availing loan ???\nDoes Income of a person have an impact on availing loan ???\nDoes the family size makes them to avail loan ???","5175d21c":"# Accuracy,F1, Precision & recall with imbalanced  data for Logistic Regression","ececa0d4":"# Model evaluation","95cfeb3f":"Let's see the distribution of Education Qualification from the dataset\nFor EDA I am adding a column to categorize the qualification for better visualization\n1. Undergrad\n2. Graduate\n3. Advanced\/Professional","cb7d596d":"Our Model has done pretty well with the raw data on both training  & testing dataset. The Scores are alomst 95% on both training and testing dataset.","e8109501":"As , you can see the upsampled data produced a accouracy score of 74%, with 93% recall and 89% Precision. Whereas, Raw data had 57% recall and 44% Precision which was low. Though accuracy was high other metrics with raw data on Naive Bayes were not effective\n\n##Take Away Naive Bayes :\n------------------------------------\n\n1. Recall of 93% infers, Among customers who bought the loan our model predicted 93% to be right.\n2. Precison Score of 89% implies among predcited positive how much was actual positive, Among total customers we predicted how    much customers actually bought the loan \n3. We should also ensure that the type1 & type 2 errors are minimal in an ideal model.\n4. In our Naive bayes model with Upsampled data, 277 customers were (False postive) predicted to buy the loan but they did not\n5. 1345 were predicted not to buy the loan but they bought, This is really bad. This model is giving a wrong signal to the bank.","20bf97f2":"People with high mortgage value, i.e more than 400K have availed personal Loan","bb536478":"# Distplot","85140b42":"In this problem our variable of interest (Dependent Variable) is Personal Loan column. \nWith all other attributes we have for a customer, we should build our model to make predictions for the bank to drive their stratergy to maximise customers who could avail personal loan"}}