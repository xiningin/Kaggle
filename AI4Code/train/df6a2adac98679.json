{"cell_type":{"ee81e3f8":"code","58905388":"code","6283f338":"code","37194f4c":"code","7465c39a":"code","ba297793":"code","e8806084":"code","a7ec1df4":"code","bf9fe123":"code","2901d5e3":"code","beaf3588":"code","f3c80d11":"code","38e26b50":"code","186d4f65":"code","da7a87a5":"code","8d668a51":"code","4154902a":"code","6ccf2352":"code","8de3a22d":"code","48a78513":"markdown","6c717ef2":"markdown","c60387e5":"markdown","abdb8f4f":"markdown","25a3f4ae":"markdown","1c942aa6":"markdown","2bf873c3":"markdown","cb1f7d7f":"markdown"},"source":{"ee81e3f8":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\nimport os\nimport sys\n# Any results you write to the current directory are saved as output.\n\nfrom datetime import datetime\nfrom IPython.core.display import display, HTML\nfrom matplotlib import pyplot    \nimport math\nimport csv\nbln_create_df_all_csv_file = True\nbln_create_words_csv_file = False\nint_df_all_version = 6\nbln_ready_to_commit = True\nbln_create_estimate_files = False\nbln_upload_input_estimates = False\nbln_recode_variables = True\npd.set_option(\"display.max_rows\", 101)\npd.set_option(\"display.max_columns\", 25)\n\ndf_time_check = pd.DataFrame(columns=['Stage','Start','End', 'Seconds', 'Minutes'])\nint_time_check = 0\ndat_start = datetime.now()\ndat_program_start = dat_start\n\nif not bln_ready_to_commit:\n    int_read_csv_rows = 100000\nelse:\n    int_read_csv_rows= None\n    \n# generate crosstabs  {0 = nothing; 1 = screen}\nint_important_crosstab = 1\nint_past_crosstab = 0\nint_current_crosstab = 1\n\nstr_text = \"Please note that as I can't guarantee the accuracy of the information provided in this kernel, \" + \\\n           \"you use the information at your own risk. I'm only using some of the data provided in the Chicago Crime dataset. \" + \\\n           \"This kernel is subject to change at any time.\"\ndisplay(HTML('<p style=\"color:orange;\">' + str_text + '<\/p>'))\n","58905388":"def get_translations_analysis_description(df_input, str_language, str_group, int_code):\n    # created by darryldias 25may2018\n    df_temp = df_input[(df_input['language']==str_language) & (df_input['group']==str_group) & (df_input['code']==int_code)] \\\n                    ['description']\n    return df_temp.iloc[0]\n\n#translations_analysis = pd.read_csv('..\/input\/ulabox-translations-analysis\/translations_analysis.csv')\nstrg_count_column = 'count'   #get_translations_analysis_description(translations_analysis, str_language, 'special', 2)\n\ndef start_time_check():\n    # created by darryldias 21may2018 - updated 8june2018\n    global dat_start \n    dat_start = datetime.now()\n    \ndef end_time_check(dat_start, str_stage):\n    # created by darryldias 21may2018 - updated 8june2018\n    global int_time_check\n    global df_time_check\n    int_time_check += 1\n    dat_end = datetime.now()\n    diff_seconds = (dat_end-dat_start).total_seconds()\n    diff_minutes = diff_seconds \/ 60.0\n    df_time_check.loc[int_time_check] = [str_stage, dat_start, dat_end, diff_seconds, diff_minutes]\n\ndef create_topline(df_input, str_item_column, str_count_column):\n    # created by darryldias 21may2018; updated by darryldias 29may2018\n    str_percent_column = 'percent'   #get_translations_analysis_description(translations_analysis, str_language, 'special', 3)\n    df_temp = df_input.groupby(str_item_column).size().reset_index(name=str_count_column)\n    df_output = pd.DataFrame(columns=[str_item_column, str_count_column, str_percent_column])\n    int_rows = df_temp.shape[0]\n    int_columns = df_temp.shape[1]\n    int_total = df_temp[str_count_column].sum()\n    flt_total = float(int_total)\n    for i in range(int_rows):\n        str_item = df_temp.iloc[i][0]\n        int_count = df_temp.iloc[i][1]\n        flt_percent = round(int_count \/ flt_total * 100, 1)\n        df_output.loc[i] = [str_item, int_count, flt_percent]\n    \n    df_output.loc[int_rows] = ['total', int_total, 100.0]\n    return df_output        \n\ndef get_size_raw(df_input):\n    return sys.getsizeof(df_input)\n\ndef get_size_mb(df_input):\n    int_size_raw = get_size_raw(df_input)\n    flt_size = float(int_size_raw) \/ 1000000 \n    return int(flt_size)\n\ndef get_dataframe_info(df_input, bln_output_csv = False, str_filename = None):\n    # created by darryldias 24may2018 - updated 25jan2019\n    int_rows = df_input.shape[0]\n    int_cols = df_input.shape[1]\n    flt_rows = float(int_rows)\n    int_size_mb = get_size_mb(df_input)\n\n    df_output = pd.DataFrame(columns=[\"Column\", \"Type\", \"Not Null\", 'Null', '% Not Null', '% Null'])\n    df_output.loc[0] = ['Table Row Count', '', int_rows, '', '', '']\n    df_output.loc[1] = ['Table Column Count', '', int_cols, '', '', '']\n    df_output.loc[2] = ['Table Size (MB)', '', int_size_mb, '', '', '']\n    int_table_row = 2\n    for i in range(int_cols):\n        str_column_name = df_input.columns.values[i]\n        str_column_type = df_input.dtypes.values[i]\n        int_not_null = df_input[str_column_name].count()\n        int_null = sum( pd.isnull(df_input[str_column_name]) )\n        flt_percent_not_null = round(int_not_null \/ flt_rows * 100, 1)\n        flt_percent_null = round(100 - flt_percent_not_null, 1)\n        int_table_row += 1\n        df_output.loc[int_table_row] = [str_column_name, str_column_type, int_not_null, int_null, flt_percent_not_null, flt_percent_null]\n\n    if bln_output_csv:\n        df_output.to_csv(str_filename)\n        print ('Dataframe information output created in file: ' + str_filename)\n        return None\n    return df_output\n\ndef check_numeric_var(str_question, int_groups):\n    # created by darryldias 3jul2018  \n    #print(df_output.iloc[3][2])\n    flt_min = application_all[str_question].min()\n    flt_max = application_all[str_question].max()\n    flt_range = flt_max - flt_min \n    flt_interval = flt_range \/ int_groups \n    df_output = pd.DataFrame(columns=['interval', 'value', 'count', 'percent', 'code1', 'code2'])\n\n    int_total = application_all[ (application_all[str_question] <= flt_max) ][str_question].count()\n    for i in range(0, int_groups + 1):\n        flt_curr_interval = i * flt_interval\n        flt_value = flt_min + flt_curr_interval\n        int_count = application_all[ (application_all[str_question] <= flt_value) ][str_question].count()\n        flt_percent = int_count \/  int_total * 100.0\n        str_code_value = \"{0:.6f}\".format(flt_value)\n        str_code1 = \"if row['\" + str_question + \"'] <= \" + str_code_value + \":\"\n        str_code2 = \"return '(x to \" + str_code_value + \"]'\"\n        df_output.loc[i] = [flt_curr_interval, flt_value, int_count, flt_percent, str_code1, str_code2]\n\n    return df_output\n\ndef show_folder_items(str_folder):\n    # darryldias 8jan2019\n    df_return = pd.DataFrame(columns=['Folder', 'Item'])\n    lst_items = sorted( os.listdir(str_folder) )\n    int_row = 0\n    for str_item in lst_items:\n        int_row += 1\n        df_return.loc[int_row] = [str_folder, str_item]\n    \n    return df_return\n","6283f338":"def get_column_analysis(int_analysis, int_code):\n    # created by darryldias 24jul2018 \n    df_date_ym_col = pd.read_csv('..\/input\/dd19-files\/date_ym_col.csv', nrows=int_read_csv_rows)\n    if int_code == 1:\n        lst = df_date_ym_col['description']\n        return [lst[0], lst[1], lst[2], lst[3], lst[4], lst[5], lst[6], lst[7], lst[8], lst[9], lst[10], lst[11], lst[12] ]\n        #return ['overall', 'homicide yes', 'homicide no', 'may18', 'jun18', 'jul18', 'aug18', 'sep18', 'oct18', \\\n        #         'nov18', 'dec18', 'jan19', 'feb19', 'mar19', 'apr19']\n    elif int_code == 2:\n        lst = df_date_ym_col['question']\n        return [lst[0], lst[1], lst[2], lst[3], lst[4], lst[5], lst[6], lst[7], lst[8], lst[9], lst[10], lst[11], lst[12] ]\n        #return ['overall', 'is_homicide', 'is_homicide', 'date_ym', 'date_ym', 'date_ym', 'date_ym', 'date_ym', 'date_ym', \\\n        #         'date_ym', 'date_ym', 'date_ym', 'date_ym', 'date_ym', 'date_ym']\n    elif int_code == 3:\n        lst = df_date_ym_col['value']\n        return [lst[0], lst[1], lst[2], lst[3], lst[4], lst[5], lst[6], lst[7], lst[8], lst[9], lst[10], lst[11], lst[12] ]\n        #return [1, 1, 0, 201805, 201806, 201807, 201808, 201809, 201810, 201811, 201812, 201901, 201902, 201903, 201904]\n    else:\n        return None\n\ndef create_crosstab_type1(df_input, str_row_question, int_output_destination):\n    # created by darryldias 10jun2018 - updated 27sep2018 \n    # got some useful code from:\n    # https:\/\/chrisalbon.com\/python\/data_wrangling\/pandas_missing_data\/\n    # https:\/\/www.tutorialspoint.com\/python\/python_lists.htm\n    # https:\/\/stackoverflow.com\/questions\/455612\/limiting-floats-to-two-decimal-points\n\n    if int_output_destination == 0:\n        return None\n    \n    str_count_desc = 'count'  #get_translations_analysis_description(translations_analysis, str_language, 'special', 3)\n    str_colpercent_desc = 'col percent'\n    \n    list_str_column_desc = get_column_analysis(1, 1)\n    list_str_column_question = get_column_analysis(1, 2)\n    list_str_column_category = get_column_analysis(1, 3)\n    int_columns = len(list_str_column_desc)\n    list_int_column_base = []\n    list_flt_column_base_percent = []\n    \n    df_group = df_input.groupby(str_row_question).size().reset_index(name='count')\n    int_rows = df_group.shape[0]\n\n    for j in range(int_columns):\n        int_count = df_input[ df_input[str_row_question].notnull() & (df_input[list_str_column_question[j]]==list_str_column_category[j]) ] \\\n                                [list_str_column_question[j]].count()\n        list_int_column_base.append(int_count)\n        if int_count == 0:\n            list_flt_column_base_percent.append('')\n        else:\n            list_flt_column_base_percent.append('100.0')\n        \n    list_output = []\n    list_output.append('row_question')\n    list_output.append('row_category')\n    list_output.append('statistic')\n    for k in range(1, int_columns+1):\n        str_temp = 'c' + str(k)\n        list_output.append(str_temp)\n    df_output = pd.DataFrame(columns=list_output)\n\n    int_row = 1\n    list_output = []\n    list_output.append(str_row_question)\n    list_output.append('')\n    list_output.append('')\n    for k in range(int_columns):\n        list_output.append(list_str_column_desc[k])\n    df_output.loc[int_row] = list_output\n    \n    int_row = 2\n    list_output = []\n    list_output.append(str_row_question)\n    list_output.append('total')\n    list_output.append(str_count_desc)\n    for k in range(int_columns):\n        list_output.append(list_int_column_base[k])\n    df_output.loc[int_row] = list_output\n    \n    int_row = 3\n    list_output = []\n    list_output.append(str_row_question)\n    list_output.append('total')\n    list_output.append(str_colpercent_desc)\n    for k in range(int_columns):\n        list_output.append(list_flt_column_base_percent[k])\n    df_output.loc[int_row] = list_output\n\n    for i in range(int_rows):\n        int_row += 1\n        int_count_row = int_row\n        int_row += 1\n        int_colpercent_row = int_row\n\n        str_row_category = df_group.iloc[i][0]\n\n        list_int_column_count = []\n        list_flt_column_percent = []\n        for j in range(int_columns):\n            int_count = df_input[ (df_input[str_row_question]==str_row_category) & \\\n                                  (df_input[list_str_column_question[j]]==list_str_column_category[j]) ] \\\n                                [list_str_column_question[j]].count()\n            list_int_column_count.append(int_count)\n            flt_base = float(list_int_column_base[j])\n            if flt_base > 0:\n                flt_percent = round(100 * int_count \/ flt_base,1)\n                str_percent = \"{0:.1f}\".format(flt_percent)\n            else:\n                str_percent = ''\n            list_flt_column_percent.append(str_percent)\n        \n        list_output = []\n        list_output.append(str_row_question)\n        list_output.append(str_row_category)\n        list_output.append(str_count_desc)\n        for k in range(int_columns):\n            list_output.append(list_int_column_count[k])\n        df_output.loc[int_count_row] = list_output\n        \n        list_output = []\n        list_output.append(str_row_question)\n        list_output.append(str_row_category)\n        list_output.append(str_colpercent_desc)\n        for k in range(int_columns):\n            list_output.append(list_flt_column_percent[k])\n        df_output.loc[int_colpercent_row] = list_output\n        \n    return df_output        \n\ndef get_ct_statistic2(df_input, str_row_question, str_col_question, str_col_category, str_statistic):\n    # created by darryldias 17jul2018\n    if str_statistic == 'total':\n        int_temp = df_input[ (df_input[str_col_question] == str_col_category) ][str_row_question].isnull().count() \n    elif str_statistic == 'notnull':\n        int_temp = df_input[ (df_input[str_col_question] == str_col_category) ][str_row_question].count() \n    elif str_statistic == 'null':\n        int_temp = df_input[ (df_input[str_col_question] == str_col_category) ][str_row_question].isnull().sum() \n    elif str_statistic == 'mean':\n        int_temp = df_input[ (df_input[str_col_question] == str_col_category) ][str_row_question].mean() \n    elif str_statistic == 'median':\n        int_temp = df_input[ (df_input[str_col_question] == str_col_category) ][str_row_question].median() \n    elif str_statistic == 'minimum':\n        int_temp = df_input[ (df_input[str_col_question] == str_col_category) ][str_row_question].min() \n    elif str_statistic == 'maximum':\n        int_temp = df_input[ (df_input[str_col_question] == str_col_category) ][str_row_question].max() \n    else:\n        int_temp = None\n    return int_temp\n \ndef create_crosstab_type2(df_input, str_row_question, int_output_destination):\n    # created by darryldias 24jul2018\n    if int_output_destination == 0:\n        return None\n\n    list_str_column_desc = get_column_analysis(1, 1)\n    list_str_column_question = get_column_analysis(1, 2)\n    list_str_column_category = get_column_analysis(1, 3)\n    int_analysis_columns = len(list_str_column_question)\n\n    list_str_statistics = ['total', 'notnull', 'null', 'mean', 'median', 'minimum', 'maximum']\n    list_str_counts = ['total', 'notnull', 'null']\n    int_statistics = len(list_str_statistics)\n\n    df_output = pd.DataFrame(columns=['row_question', 'row_category', 'statistic', 'c1', 'c2', 'c3', 'c4', 'c5'])\n    int_row = 1\n\n    list_values = []\n    list_values.append(str_row_question)\n    list_values.append('')\n    list_values.append('')\n    for j in range(int_analysis_columns):\n        list_values.append(list_str_column_desc[j])\n    df_output.loc[int_row] = list_values\n\n    for i in range(int_statistics):\n        str_statistic = list_str_statistics[i] \n        list_values = []\n        list_values.append(str_row_question)\n        if str_statistic in list_str_counts:\n            list_values.append(str_statistic)\n            list_values.append('count')\n        else:\n            list_values.append('numeric')\n            list_values.append(str_statistic)\n    \n        for j in range(int_analysis_columns):\n            str_col_question = list_str_column_question[j]\n            str_col_category = list_str_column_category[j]\n            num_statistic = get_ct_statistic2(df_input, str_row_question, str_col_question, str_col_category, str_statistic)\n            list_values.append(num_statistic)\n        int_row += 1\n        df_output.loc[int_row] = list_values\n    return df_output\n","37194f4c":"def percent_summary_1 (row, str_input_column):\n    # created by darryldias 27may2018   \n    if row[str_input_column] == 0 :   \n        return 'no'\n    elif row[str_input_column] > 0 :\n        return 'yes'\n    return 'Unknown'\n\ndef get_note(id):\n    df_temp = df_notes[ df_notes['id']==id ]\n    return df_temp.iloc[0][1]\n\ndef get_html_table(int_table_id):\n    int_column_html = 2\n    df_html_tables = pd.read_csv('..\/input\/dd19-files\/html_tables.csv', nrows=int_read_csv_rows)\n    df_html_tables = df_html_tables[ df_html_tables['table_id'] == int_table_id ]\n    int_rows = df_html_tables.shape[0]\n    str_return = ''\n    for i in range(int_rows):\n        str_return = str_return + df_html_tables.iloc[i][int_column_html] + '\\n'\n    return str_return\n\n\ndf_notes = pd.read_csv('..\/input\/dd19-files\/notes.csv', nrows=int_read_csv_rows)\n\n#str_note1 = get_note(1)\n#display(HTML('<h4 style=\"color:purple;\">' + str_note1 + '<\/br>' + str_note2 + '<\/h4>'))\nstr_html = '<ul style=\"color:purple;\">'\nfor i in [1,2,3,4,5]:\n    str_note = get_note(i)\n    str_html += '<li>' + str_note +'<\/li>'\nstr_html += '<\/ul>'\ndisplay(HTML(str_html))\ndisplay(HTML('the following table shows the number of homicides each day for the last 28 days'))\nstr_html_table = get_html_table(2)\ndisplay(HTML(str_html_table))","7465c39a":"#str_note = get_note(3)\n#display(HTML('<h4 style=\"color:purple;\">' + str_note + '<\/h4>'))\n\nstr_html_table = get_html_table(1)\n#print(str_html_table)\ndisplay(HTML(str_html_table))\n\ndf_table = pd.read_csv('..\/input\/dd19-files\/tables.csv', nrows=int_read_csv_rows)\ndf_table = df_table[ df_table['table_id'] == 1 ]\n\nplot_temp = df_table.plot(x='column', y='value', figsize=(10, 5), legend=None)\npyplot.xlabel('month')\npyplot.ylabel('homicides per day')\npyplot.grid()\n","ba297793":"str_html_table = get_html_table(3)\ndisplay(HTML(str_html_table))\n\ndf_table = pd.read_csv('..\/input\/dd19-files\/tables.csv', nrows=int_read_csv_rows)\ndf_table = df_table[ df_table['table_id'] == 3 ]\n\nplot_temp = df_table.plot(x='column', y='value', figsize=(10, 5), legend=None)\npyplot.xlabel('week')\npyplot.ylabel('homicides per day')\npyplot.grid()","e8806084":"df_crime = pd.read_csv('..\/input\/dd19-files\/homicides.csv', nrows=int_read_csv_rows)\n#df_crime = df_crime[ (df_crime['is_homicide'] == 1) ]\ncreate_crosstab_type1(df_crime, 'day_of_week_s1d', int_current_crosstab)","a7ec1df4":"create_crosstab_type1(df_crime, 'hour_s1d', int_current_crosstab)","bf9fe123":"create_crosstab_type1(df_crime, 'district_s1d', int_current_crosstab)","2901d5e3":"create_crosstab_type1(df_crime, 'arrest', int_current_crosstab)","beaf3588":"create_crosstab_type1(df_crime, 'domestic', int_current_crosstab)","f3c80d11":"create_crosstab_type1(df_crime, 'location_description_s1d', int_current_crosstab)","38e26b50":"create_crosstab_type1(df_crime, 'month', int_current_crosstab)","186d4f65":"create_crosstab_type1(df_crime, 'day', int_current_crosstab)","da7a87a5":"df_crime = pd.read_csv('..\/input\/dd19-files\/crime.csv', nrows=int_read_csv_rows)\n#create_crosstab_type1(df_crime, 'overall', int_important_crosstab)\n#create_crosstab_type1(df_crime, 'year', int_current_crosstab)\n#create_crosstab_type1(df_crime, 'month', int_current_crosstab)\n#create_crosstab_type1(df_crime, 'day', int_current_crosstab)\n#create_crosstab_type1(df_crime, 'primary_type_s1d', int_current_crosstab)\n#create_crosstab_type1(df_crime, 'district_s1d', int_current_crosstab)","8d668a51":"# a bug in the system - looks like it is fixed 25jun2019\nshow_folder_items('..\/input\/dd19-files')","4154902a":"df_crime.sample(10)","6ccf2352":"get_dataframe_info(df_crime)","8de3a22d":"#create_crosstab_type1(df_crime, 'location_description', int_important_crosstab)\nend_time_check(dat_program_start, 'overall')\ndf_time_check","48a78513":"## homicides per day","6c717ef2":"## other notes\n* community area tables are included in the htm crosstab files\n* for the htm crosstab files in the dd19_files dataset, if you click on a file then click the download icon the htm file should open in a new tab (and then you can copy \/ paste into Excel \/ Sheets for example)\n* district names were obtained from [https:\/\/home.chicagopolice.org\/community\/districts\/](https:\/\/home.chicagopolice.org\/community\/districts\/)\n* community area names were obtained from [https:\/\/home.chicagopolice.org\/community\/community-map\/](https:\/\/home.chicagopolice.org\/community\/community-map\/)\n* I am running a bigquery extraction (and some other data processing) in a private kernel prior to running this public kernel.","c60387e5":"## homicides by month crosstabs","abdb8f4f":"### month by month","25a3f4ae":"### week by week (rolling 4 week values)","1c942aa6":"## datasets","2bf873c3":"## overall crime by month crosstabs\n* these crosstabs can now be found in the relevant htm file in the dd19_files dataset\n* there are only a few tables included for checking purposes","cb1f7d7f":"## homicides by district crosstabs\n* these crosstabs can be found in the relevant htm file in the dd19_files dataset\n* also included in the dataset are the homicides by month tables"}}