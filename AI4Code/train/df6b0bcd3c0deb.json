{"cell_type":{"86c8d0c3":"code","e9107323":"code","5ddc9970":"code","ba5c6966":"code","2940e5db":"code","da7bffc4":"code","b1585f94":"code","51e18adb":"markdown","56e3a2a7":"markdown","34c3bbbb":"markdown","cf24a90e":"markdown","dcea0bb1":"markdown","c54175ce":"markdown","9df20732":"markdown"},"source":{"86c8d0c3":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport gc\nimport matplotlib.pyplot as plt\nfrom tensorflow import keras\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras import *\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom tensorflow.keras.callbacks import LearningRateScheduler, ReduceLROnPlateau\nfrom sklearn.preprocessing import RobustScaler, normalize\nfrom sklearn.model_selection import train_test_split, KFold\nfrom sklearn.metrics import mean_absolute_error\nfrom pickle import load\nimport json\n!cp ..\/input\/ventilator-feature-engineering\/VFE.py .","e9107323":"train = np.load('..\/input\/ventilator-feature-engineering\/x_train.npy')\ntargets = np.load('..\/input\/ventilator-feature-engineering\/y_train.npy')","5ddc9970":"# model creation\ndef create_lstm_model():\n\n    x0 = tf.keras.layers.Input(shape=(train.shape[-2], train.shape[-1]))  \n\n    lstm_layers = 4 # number of LSTM layers\n    lstm_units = [940, 540, 462, 316]\n    lstm = Bidirectional(keras.layers.LSTM(lstm_units[0], return_sequences=True))(x0)\n    for i in range(lstm_layers-1):\n        lstm = Bidirectional(keras.layers.LSTM(lstm_units[i+1], return_sequences=True))(lstm)    \n    lstm = Dropout(0.002)(lstm)\n    lstm = Dense(lstm_units[-1], activation='swish')(lstm)\n    lstm = Dense(1)(lstm)\n\n    model = keras.Model(inputs=x0, outputs=lstm)\n    model.compile(optimizer=\"adam\", loss=\"mae\")\n    \n    return model","ba5c6966":"with open('..\/input\/train-ventilator-lstm-model-part-i\/train_params.json', 'r') as fp:\n    config = json.load(fp)\n\nBATCH_SIZE = config['BATCH_SIZE']\nNFOLDS = config['NFODLS']\nSEED = config['SEED']\nEPOCHS = config['EPOCHS']","2940e5db":"# Function to get hardware strategy\ndef get_hardware_strategy():\n    try:\n        # TPU detection. No parameters necessary if TPU_NAME environment variable is\n        # set: this is always the case on Kaggle.\n        tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n        print('Running on TPU ', tpu.master())\n    except ValueError:\n        tpu = None\n\n    if tpu:\n        tf.config.experimental_connect_to_cluster(tpu)\n        tf.tpu.experimental.initialize_tpu_system(tpu)\n        strategy = tf.distribute.experimental.TPUStrategy(tpu)\n        tf.config.optimizer.set_jit(True)\n    else:\n        # Default distribution strategy in Tensorflow. Works on CPU and single GPU.\n        strategy = tf.distribute.get_strategy()\n\n    return tpu, strategy\n\ntpu, strategy = get_hardware_strategy()","da7bffc4":"hist = []\nfolds = [4] # folds to train\n\nwith strategy.scope():\n    kf = KFold(n_splits=NFOLDS, shuffle=True, random_state=SEED)\n    \n    for fold, (train_idx, test_idx) in enumerate(kf.split(train, targets)):\n        if fold in folds:\n            print('-'*15, '>', f'Fold {fold+1}', '<', '-'*15)\n            folds.append(fold)\n            X_train, X_valid = train[train_idx], train[test_idx]\n            y_train, y_valid = targets[train_idx], targets[test_idx]\n            \n            model = create_lstm_model()\n            model.compile(optimizer=\"adam\", loss=\"mae\")\n            \n            checkpoint_filepath = '\/kaggle\/working\/lstm_fold{}.hdf5'.format(fold)\n\n            lr = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=10, verbose=1)\n            es = EarlyStopping(monitor=\"val_loss\", patience=60, verbose=1, mode=\"min\", restore_best_weights=True)\n            sv = keras.callbacks.ModelCheckpoint(\n                checkpoint_filepath, monitor='val_loss', verbose=1, save_best_only=True,\n                save_weights_only=False, mode='auto', save_freq='epoch',\n                options=None\n            )\n            hist.append(model.fit(X_train, y_train, \n                                  validation_data=(X_valid, y_valid), \n                                  epochs=EPOCHS, batch_size=BATCH_SIZE, \n                                  callbacks=[lr, es, sv]))\n        \n            del X_train, X_valid, y_train, y_valid, model\n            gc.collect()","b1585f94":"colors = ['tab:blue', 'tab:orange', 'tab:green', 'tab:red', 'tab:purple']\nplt.figure(figsize=(16,16))\nfor i in range(len(hist)):\n    plt.plot(hist[i].history['loss'], linestyle='-', color=colors[i], label='Train, fold #{}'.format(str(folds[i])))\nfor i in range(len(hist)):\n    plt.plot(hist[i].history['val_loss'], linestyle='--', color=colors[i], label='Validation, fold #{}'.format(str(folds[i])))\nplt.ylim(top=1)\nplt.title('Model Loss')\nplt.ylabel('MAE')\nplt.xlabel('Epoch')\nplt.legend()\nplt.grid(which='major', axis='both')\nplt.show();","51e18adb":"![logo](https:\/\/cdn.freelogovectors.net\/wp-content\/uploads\/2018\/07\/tensorflow-logo.png)","56e3a2a7":"# Dataset","34c3bbbb":"# Training - LSTM based model\nThis notebook is part of a series:  \n  * [Ventilator: Feature engineering](https:\/\/www.kaggle.com\/mistag\/ventilator-feature-engineering)\n  * [Keras model tuning with Optuna](https:\/\/www.kaggle.com\/mistag\/keras-model-tuning-with-optuna)\n  * [[train] Ventilator LSTM Model - part I](https:\/\/www.kaggle.com\/mistag\/train-ventilator-lstm-model-part-i)\n  * [[train] Ventilator LSTM Model - part II](https:\/\/www.kaggle.com\/mistag\/train-ventilator-lstm-model-part-ii)\n  * [[train] Ventilator LSTM Model - part III](https:\/\/www.kaggle.com\/mistag\/train-ventilator-lstm-model-part-iii)\n  * [[train] Ventilator LSTM Model - part IV](https:\/\/www.kaggle.com\/mistag\/train-ventilator-lstm-model-part-iv)\n  * [[pred] Ventilator LSTM Model](https:\/\/www.kaggle.com\/mistag\/pred-ventilator-lstm-model)\n  \n## References\nThe code is based on these references:  \n  * [Improvement base on Tensor Bidirect LSTM](https:\/\/www.kaggle.com\/kensit\/improvement-base-on-tensor-bidirect-lstm-0-173\/notebook) by [Ken Sit](https:\/\/www.kaggle.com\/kensit)\n  * [Ensemble Folds with MEDIAN - [0.153]](https:\/\/www.kaggle.com\/cdeotte\/ensemble-folds-with-median-0-153) by [Chris Deotte](https:\/\/www.kaggle.com\/cdeotte)\n","cf24a90e":"# Training\nFirst, fetch some training parameters from the first training notebook:","dcea0bb1":"Let's take a look at the learning curves.","c54175ce":"# Model","9df20732":"HW strategy:"}}