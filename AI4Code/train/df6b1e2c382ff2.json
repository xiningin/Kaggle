{"cell_type":{"9a3309f8":"code","d16e90a7":"code","fea927ee":"code","b8e6ec2a":"code","f0d29798":"code","6d64ff6a":"code","157fdd6b":"code","a61076bb":"code","145728df":"code","3ae52c60":"code","b4350f30":"code","1379516d":"code","79b1df3e":"code","63826116":"code","aaf88cec":"code","28564744":"code","556bccea":"code","1cc4dba0":"code","c158fb86":"markdown","803bd426":"markdown","e675d47f":"markdown","64a1ddba":"markdown","57411a73":"markdown","26442f07":"markdown","ed0eee88":"markdown","f49f95bb":"markdown","af84bb2f":"markdown","7461c904":"markdown"},"source":{"9a3309f8":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport missingno as msno\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import OneHotEncoder\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import svm\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\n\nfrom sklearn.metrics import mean_squared_error as rmse\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import classification_report\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d16e90a7":"train = pd.read_csv(\"\/kaggle\/input\/cap-4611-2021-fall-assignment-02\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/cap-4611-2021-fall-assignment-02\/eval.csv\")","fea927ee":"train.head()\nprint(train.info())\nprint(train.columns)\ntrain.describe()","b8e6ec2a":"# Preparing Data: Checking Missing Values\ncols = ['console', 'alcohol_reference', 'animated_blood', 'blood', 'blood_and_gore', 'cartoon_violence', 'crude_humor', \n        'drug_reference', 'fantasy_violence', 'intense_violence', 'language', 'lyrics', 'mature_humor', 'mild_blood', \n        'mild_cartoon_violence', 'mild_fantasy_violence', 'mild_language', 'mild_lyrics', 'mild_suggestive_themes', \n        'mild_violence', 'no_descriptors', 'nudity', 'partial_nudity', 'sexual_content', 'sexual_themes', 'simulated_gambling',\n        'strong_janguage', 'strong_sexual_content', 'suggestive_themes', 'use_of_alcohol', 'use_of_drugs_and_alcohol',\n        'violence']\n\nfor i in cols:\n    train[cols].value_counts()\nmsno.matrix(train)","f0d29798":"# EDA: Feature Engineering, Correlation Matrix\ncorr = train.corr()\nax = sns.heatmap(\n    corr,\n    vmin=-1,\n    vmax=1,\n    center=0,\n    cmap=sns.diverging_palette(20, 220, n=200),\n    square=True\n)\nax.set_xticklabels(\n    ax.get_xticklabels(),\n    rotation=45,\n    horizontalalignment='right'\n);","6d64ff6a":"train.drop(['id'], axis=1)\n\n#Dataframe melt\ntrain_melt = (train.melt(id_vars='esrb_rating', value_vars=cols)\n                  .groupby([pd.Grouper(key='esrb_rating'), 'variable', 'value'])\n                  .size()\n                  .unstack(level=[1,2], fill_value=0)\n             )\ntrain_melt","157fdd6b":"seed_value = 42\n\nnp.random.seed(seed_value)\n\ny = train['esrb_rating'].array.reshape(-1) \nX = train[cols]\nprint(X.shape)\nprint(y.shape)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20)","a61076bb":"#Instantiating the model - Logistic Regression\nmodelLGR = LogisticRegression()\n\nlgr_params = {'penalty' : ['l2'], 'C' : np.logspace(-3,3,7), 'solver' : ['liblinear']}\n\n#Hyperparameter tuning via GridSearchCV\nlgr_grid = GridSearchCV(modelLGR, param_grid=lgr_params, cv=10, scoring='accuracy')\n\nlgr_grid.fit(X_train, y_train)\n\n\npredictLGR = lgr_grid.predict(X_test)\naccuracyLGR = accuracy_score(y_test, predictLGR)\nprint(accuracyLGR)","145728df":"svm_params = {'C': [0.1, 1, 10, 100, 1000], 'gamma': [1, 0.1, 0.01, 0.001, 0.0001], 'kernel': ['linear']}\n\n# Instantiating the model - Random Forest\nmodelSVM = svm.SVC()\n\n# Hyperparameter tuning via GridSearchCV\nsvm_grid = GridSearchCV(modelSVM, svm_params, cv=10, scoring='accuracy')\n\nsvm_grid.fit(X_train, y_train)\n\npredictSVM = svm_grid.predict(X_test)\naccuracySVM = accuracy_score(y_test, predictSVM)\nprint(accuracySVM)","3ae52c60":"dtr_params = {'max_leaf_nodes': list(range(2, 100)), 'min_samples_split': [2, 3, 4]}\n\n# Instantiating the model - Decision Tree\nmodelDTR = DecisionTreeClassifier()\n\n#Hyperparameter tuning via GridSearchCV\ndtr_grid = GridSearchCV(modelDTR, dtr_params, cv=10, scoring='accuracy')\n\ndtr_grid.fit(X_train, y_train)\n\npredictDTR = dtr_grid.predict(X_test)\naccuracyDTR = accuracy_score(y_test, predictDTR)\nprint(accuracyDTR)","b4350f30":"rfc_params = dict(n_estimators=[100, 350, 500])\n\n# Instantiating the model - Random Forest\nmodelRFC = RandomForestClassifier()\n\n#Hyperparameter tuning via GridSearchCV\nrfc_grid = GridSearchCV(modelRFC, rfc_params, cv=10, scoring='accuracy')\n\nrfc_grid.fit(X_train, y_train)\n\npredictRFC = rfc_grid.predict(X_test)\naccuracyRFC = accuracy_score(y_test, predictRFC)\nprint(accuracyRFC)","1379516d":"# I tested the grid search with different n_neighbors values but the computer would take forever to run, and they all came back as n_neighbors=1 anyway\nknn_params = dict(n_neighbors=list(range(1, 3)))\n\n# Instantiating model - KNN\nmodelKNN = KNeighborsClassifier()\n\n#Hyperparameter tuning via GridSearchCV\nknn_grid = GridSearchCV(modelKNN, knn_params, cv=10, scoring='accuracy')\n\nbest_model = knn_grid.fit(X_train, y_train)\n\n\npredictKNN = best_model.predict(X_test)\naccuracyKNN = accuracy_score(y_test, predictKNN)\nprint(accuracyKNN)","79b1df3e":"# Extrapolating the CV results from the GridSearch of every model and putting them into a dataframe based on desired data\ndf_lgr = pd.concat([pd.DataFrame(lgr_grid.cv_results_[\"params\"]),pd.DataFrame(lgr_grid.cv_results_[\"mean_test_score\"], columns=[\"Accuracy\"])],axis=1)\ndf_svm = pd.concat([pd.DataFrame(svm_grid.cv_results_[\"params\"]),pd.DataFrame(svm_grid.cv_results_[\"mean_test_score\"], columns=[\"Accuracy\"])],axis=1)\ndf_dtr = pd.concat([pd.DataFrame(dtr_grid.cv_results_[\"params\"]),pd.DataFrame(dtr_grid.cv_results_[\"mean_test_score\"], columns=[\"Accuracy\"])],axis=1)\ndf_rfc = pd.concat([pd.DataFrame(rfc_grid.cv_results_[\"params\"]),pd.DataFrame(rfc_grid.cv_results_[\"mean_test_score\"], columns=[\"Accuracy\"])],axis=1)\ndf_knn = pd.concat([pd.DataFrame(knn_grid.cv_results_[\"params\"]),pd.DataFrame(knn_grid.cv_results_[\"mean_test_score\"], columns=[\"Accuracy\"])],axis=1)\n\n# Describing stats of Logistic Regression model\ndf_lgr.describe()","63826116":"# Describing stats of Support Vector model\ndf_svm.describe()","aaf88cec":"# Describing stats of Decision Tree model\ndf_dtr.describe()","28564744":"# Describing stats of Random Forest model\ndf_rfc.describe()","556bccea":"# Describing stats of K-Nearest Neighbor model\ndf_knn.describe()","1cc4dba0":"# Using eval.csv to test best estimator (based on scores, I chose Random Forest)\ntest_dropid = test.drop(['id'], axis=1)\nprediction = rfc_grid.predict(test_dropid)\n\n# Generating Submission\noutput = pd.DataFrame({'id' : test['id'], 'esrb_rating' : prediction})\noutput.to_csv('submission.csv', index=False)\n\nprint(\"Output Successful\")","c158fb86":"#### Decision Tree","803bd426":"## Visualizing the Models\n\n###### *Comment: I hope this is sufficient for displaying the distribution of validation scores and summary report*","e675d47f":"## Testing the Best Estimator","64a1ddba":"#### **Observations**\n* 'id' and 'title' can both be dropped as they are just str values and don't have any impact on the esrb rating\n* There are no outliers since the data (at least the ones of interest) are boolean values and the dataset is categorical. Thus, no need to check for outliers.\n* No missing values, can continue with EDA without needing to get dummy values.\n* Str values must be encoded like 'title' and 'esrb_rating' since they are not the same type as the other categories\n* EDIT: The data has already been encoded","57411a73":"#### K-Nearest Neighbors","26442f07":"#### Logistic Regression","ed0eee88":"# Taniya Shaffer\n## CAP4611 Homework 2 Predicting ESRB Ratings\n### Preparing the DATA: Check for missing values within training data, and describe\/implement; Check for outliers, describe\/implement; feature engineering \"why\" \n### Need to build Logistic Regression, Support Vector Machine, Decision Tree, Random Forest, KNearestNeighbors\n### **NOTE: must use hyperparameter searches where applicable**\n### Print the contents of the submission.csv file in the notebook.\n## Last step: Make your notebook public","f49f95bb":"## Building the Models","af84bb2f":"#### Random Forest","7461c904":"#### Support Vector Machine"}}