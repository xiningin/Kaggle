{"cell_type":{"0eeca89a":"code","240e21b6":"code","f210a48b":"code","0522e494":"code","f1947a5f":"code","86a79499":"code","a2ce4104":"code","f9b98fc7":"code","68dd06ff":"code","e9124501":"code","062172d9":"code","b6747ff7":"code","59edf864":"code","35c0f9e0":"code","e06c97fe":"code","43aef85e":"code","9b1dddcd":"code","c858a5cf":"code","d5c11f80":"code","48c95b2a":"code","d6f41fad":"code","68b1e4fe":"code","8255ae54":"markdown","d3b870fb":"markdown","dce766ef":"markdown","ccf6da95":"markdown","0e979234":"markdown","f6dee5f7":"markdown","7c949f63":"markdown","0ba5ae71":"markdown","737a3ae6":"markdown","0f599339":"markdown","21c51043":"markdown","f650c200":"markdown","5ed0d49f":"markdown","228959c4":"markdown"},"source":{"0eeca89a":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\npd.set_option('display.max_columns', None)\nfrom sklearn.ensemble import GradientBoostingClassifier,RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom yellowbrick.classifier import ROCAUC\nimport plotly.graph_objects as go","240e21b6":"df = pd.read_csv('\/kaggle\/input\/german-credit-data-with-risk\/german_credit_data.csv')\ndf = df.drop(['Unnamed: 0'],axis=1)\ndf.head()","f210a48b":"df.columns = list(map(lambda name:name.replace(' ','_'),df.columns))","0522e494":"df.dtypes","f1947a5f":"# convet column into category types\ndf[['Sex','Job','Housing','Saving_accounts','Checking_account','Purpose']] = \\\n    df[['Sex','Job','Housing','Saving_accounts','Checking_account','Purpose']].astype('category')","86a79499":"df['Sex']=df['Sex'].cat.codes\ndf['Job']=df['Job'].cat.codes\ndf['Housing']=df['Housing'].cat.codes\ndf['Saving_accounts']=df['Saving_accounts'].cat.codes\ndf['Checking_account']=df['Checking_account'].cat.codes\ndf['Purpose']=df['Purpose'].cat.codes\ndf['Risk']=df['Risk'].map({'good':0,'bad':1})","a2ce4104":"df.head()","f9b98fc7":"X,y = df.loc[:, df.columns != 'Risk'],df['Risk']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","68dd06ff":"features=X_train.columns.to_list()","e9124501":"# features we are going to use for our model training\nfeatures","062172d9":"print(X_train.shape)\nprint(X_test.shape)","b6747ff7":"gbm = GradientBoostingClassifier(n_estimators=500,random_state=2)\n\nvisualizer = ROCAUC(gbm, classes=[ \"good\", \"bad\"])\nvisualizer.fit(X_train, y_train)        \nvisualizer.score(X_test, y_test)        \nvisualizer.show()                       ","59edf864":"rf = RandomForestClassifier(n_estimators=500,random_state=2)\n\nvisualizer = ROCAUC(rf, classes=[ \"good\", \"bad\"])\nvisualizer.fit(X_train, y_train)        \nvisualizer.score(X_test, y_test)        \nvisualizer.show() ","35c0f9e0":"\nX_test['gbm_predicted_probability'] = gbm.predict_proba(X_test[features])[:,1] # Predicted Proba for bads(=1)\nX_test['rf_predicted_probability'] = rf.predict_proba(X_test[features])[:,1] # Predicted Proba for bads(=1)\nX_test['Risk'] = y_test #Ground Truth\nX_test.head()","e06c97fe":"def k_s_statistics_gain_lift(data,predicted_probability,ground_truth,response_name='Risk'):\n    \"\"\"\n    This function gives K-S statistics Tables \n    KS Statistics is the difference between the cumulative Success and Non-Success Rate.Which gives optimal threshold for the group separation\n    inuputs:\n    data:dataframe \n    predicted_probability:string,coulmn name which contains predicted probability from the model\n    ground_truth:string,column name which contains actual labels in integer form\n    response_name:string,name of your success label e.g.deault,fraud,churn etc\n    \"\"\"\n    #Sort the data in descending order of predicted probabilities.\n    data= data.sort_values(by=predicted_probability, ascending=False)\n    #print(data)\n    #Cut deciles based on the predicted probabilities\n    data['decile_group'] = pd.qcut(data[predicted_probability], q=10)\n    #Create success and failure response column\n    \n    KS_data = data.groupby('decile_group').agg( #Group by Deciles of Predicted Probabilties\n            [\n                'count', #The total number of customers(data points) in the decile\n                'sum', #The total number of bad customers(Risk=1)\n            ]\n            )[ground_truth].sort_index(ascending=False)\n    KS_data.columns = ['Total count','Number of '+response_name]\n    KS_data['Number of '+'Non-'+response_name]=KS_data['Total count']-KS_data['Number of '+response_name]\n    KS_data[response_name+'_Rate'+'%'] = (KS_data['Number of '+response_name] \/ KS_data['Total count']).apply(lambda x:round(100*x,2))\n    KS_data['Percent of '+response_name+'%'] = (KS_data['Number of '+response_name]\/KS_data['Number of '+response_name].sum()).apply(lambda x:round(100*x,2))\n    KS_data['Percent of '+'Non-'+response_name+'%'] = (KS_data['Number of '+'Non-'+response_name]\/KS_data['Number of '+'Non-'+response_name].sum()).apply(lambda x:round(100*x,2))\n    KS_data['ks_stats'] = np.round(((KS_data['Number of '+response_name] \/ KS_data['Number of '+response_name].sum()).cumsum() -(KS_data['Number of '+'Non-'+response_name] \/ KS_data['Number of '+'Non-'+response_name].sum()).cumsum()), 4) * 100\n    KS_data['max_ks'] = KS_data['ks_stats'].apply(lambda x: '*****' if x == KS_data['ks_stats'].max() else '')\n    #Calculate Gain = Cumulative Percent of Events\/Total success events\n    KS_data['Gain'] = KS_data['Percent of '+response_name+'%'].cumsum() \n    #Calculate Lift = Ratio of Bads to the number of data points in the decile\n    KS_data['Lift'] = (KS_data['Gain']\/np.array(range(10,100+10,10))).apply(lambda x:round(x,2))     \n    return KS_data\n","43aef85e":"gbm_ks_data=k_s_statistics_gain_lift(data=X_test,predicted_probability='gbm_predicted_probability',ground_truth='Risk')\ngbm_ks_data","9b1dddcd":"rf_ks_data=k_s_statistics_gain_lift(data=X_test,predicted_probability='rf_predicted_probability',ground_truth='Risk')\nrf_ks_data","c858a5cf":"def model_selection_by_gain_chart(model_gains_dict):\n    fig = go.Figure()\n    fig.add_trace(go.Scatter(x=list(range(0,100+10,10)), y=list(range(0,100+10,10)),\n                    mode='lines+markers',name='Random Model'))\n    for model_name,model_gains in model_gains_dict.items():\n        model_gains.insert(0,0)\n        fig.add_trace(go.Scatter(x=list(range(0,100+10,10)), y=model_gains,\n                    mode='lines+markers',name=model_name))\n    fig.update_xaxes(\n        title_text = \"% of Data Set\",)\n\n    fig.update_yaxes(title_text = \"% of Gain\",)\n    fig.update_layout(title='Gain Charts',)\n    fig.show()\n    ","d5c11f80":"model_selection_by_gain_chart(model_gains_dict={'GradientBoosting':gbm_ks_data.Gain.to_list(),\n                                                'RandomForest':rf_ks_data.Gain.to_list()})","48c95b2a":"def model_selection_by_lift_chart(model_lift_dict):\n    fig = go.Figure()\n    fig.add_trace(go.Scatter(x=list(range(10,100+10,10)), y=np.repeat(1,10),\n                    mode='lines+markers',name='Random Lift'))\n    for model_name,model_lifts in model_lift_dict.items():\n        fig.add_trace(go.Scatter(x=list(range(10,100+10,10)), y=model_lifts,\n                    mode='lines+markers',name=model_name))\n    fig.update_xaxes(\n        title_text = \"% of Data Set\",)\n\n    fig.update_yaxes(title_text = \"Lift\",)\n    fig.update_layout(title='Lift Charts',)\n    fig.show()","d6f41fad":"model_selection_by_lift_chart(model_lift_dict={'GradientBoosting':gbm_ks_data.Lift.to_list(),\n                                                'RandomForest':rf_ks_data.Lift.to_list()})","68b1e4fe":"rf_ks_data","8255ae54":"# Read the German Credit Data and extract the features for building multiple models","d3b870fb":"# K-S \nK-S or Kolmogorov-Smirnov chart measures performance of classification models. More accurately, K-S is a measure of the degree of separation between the positive and negative distributions. The K-S is 100 if the scores partition the population into two separate groups in which one group contains all the positives and the other all the negatives. On the other hand, If the model cannot differentiate between positives and negatives, then it is as if the model selects cases randomly from the population. The K-S would be 0. In most classification models the K-S will fall between 0 and 100, and that the higher the value the better the model is at separating the positive from negative cases.\nModel Which gives higher KS statistics values is good over antoher model which has less KS statistics value.","dce766ef":"### Please do comment if any consern and also do vote if you like my work :-) \n### Thank You","ccf6da95":"# Model Selection Basis: KS statistics, Lift and Gain Charts.","0e979234":"# Import the libraries","f6dee5f7":"### Now our data is ready to build models. Here Risk is the target variable  Let's split a data into train-test","7c949f63":"## KS statistics Inference:\nKS statistics value for Random forest is 39.67 which  higher than GBM model ks value 37.26\nHecne  Random Forest model is out performing better than GBM.We can do this for different model perfomance check","0ba5ae71":"# Model 1:Gradient Boosting ","737a3ae6":"#  Gain Lift chart Inference:\nAlso from Gain and lift chart it seems RF model is having more gain and lift than GBM.\nSo from KS ,Gain and Lift RF is best model in this our scenario .You can try other models and check it out.","0f599339":"# More Inference basis  RF model :\n","21c51043":"1. Lift for 1st decile is 2.71 which means Decile 1 of Random Forest can get 2.71 times of the risky customers compared to random selection.\n2. From the table it is clear that we are able to get 78% risky customers within first 5 deciles.","f650c200":"# Model 2. RandomForest ","5ed0d49f":"# Model selection from  Gain and Lift graph   ","228959c4":"# Gain and Lift Charts\n\nGain and Lift charts help us in visualising the performance of our model in comparison to the base model\/no model.However, in contrast to the confusion matrix that evaluates models on the whole population gain or lift chart evaluates model performance in a portion of the population i.e Gain and Lift charts can help us in understanding how our model is performing on different sections of the data.\n\n1.[source](https:\/\/www.datavedas.com\/model-evaluation-classification-models\/#:~:text=Higher%20K%2DS%20value%20means%20that,separate%20class%20label%20of%20observations.)\n2.[source](https:\/\/towardsdatascience.com\/how-to-determine-the-best-model-6b9c584d0db4)"}}