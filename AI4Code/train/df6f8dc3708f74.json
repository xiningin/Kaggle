{"cell_type":{"63708e3f":"code","ad2c7c02":"code","6dd93e65":"code","a069821a":"code","51094af0":"code","85343305":"code","5461825f":"code","c1991b94":"code","9a9b3654":"code","7eab8c4e":"code","1a9e303b":"code","350f535c":"code","68be7b32":"code","a5bf3faa":"code","f6917969":"code","96a22fdf":"code","d3b1a62c":"code","904bfa7a":"code","448669c7":"code","5140f8ca":"code","edb7437e":"code","3b8479e7":"code","e07a717b":"code","8e85bf2f":"code","e420eb8d":"code","1a1b7bdc":"code","ce40ad1e":"code","71c3a478":"code","c1c599d6":"code","90358169":"code","14c5b70c":"code","173862d3":"code","d16e9be1":"code","a2bb0008":"code","9296ea69":"code","43184bc7":"code","78576228":"code","5c500017":"code","6135b024":"code","7d66caa4":"code","d7ed5f79":"code","910eb3f5":"code","c691d3ec":"code","b83a2730":"code","36d45115":"code","fe7acb80":"code","38c6e307":"code","3541cf5d":"code","9dafa5d8":"code","21c84ada":"code","65b27b00":"code","7c1e98d1":"code","1a0626e8":"code","9ab5f21e":"code","2e4b4ad7":"markdown","ea7a3259":"markdown","0b29a9cf":"markdown","600ea5d8":"markdown","337826e6":"markdown","c4746faf":"markdown","a192e4e5":"markdown","099e39bc":"markdown","ca7aa0e0":"markdown","af132d60":"markdown","921e6131":"markdown","f0a81082":"markdown","61ac87f5":"markdown","28a8354b":"markdown","a7b27a55":"markdown","ad0d442c":"markdown","0b3c0643":"markdown","437989c6":"markdown","af0ea8c4":"markdown","dbb64b39":"markdown","8797654c":"markdown","ef04e59a":"markdown","9d9f9a7d":"markdown"},"source":{"63708e3f":"#GENERAL\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport random\nimport time\n#PATH PROCESS\nimport os\nimport os.path\nfrom pathlib import Path\nimport glob\n#IMAGE PROCESS\nfrom PIL import Image\nfrom keras.preprocessing import image\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nimport cv2\nfrom keras.applications.vgg16 import preprocess_input, decode_predictions\nimport imageio\nfrom IPython.display import Image\nimport matplotlib.image as mpimg\nfrom skimage.transform import resize\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom matplotlib import cm\nimport zipfile\nfrom io import BytesIO\nfrom nibabel import FileHolder\nfrom nibabel.analyze import AnalyzeImage\nimport PIL\nfrom IPython import display\n#SCALER & TRANSFORMATION\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler\nfrom keras.utils.np_utils import to_categorical\nfrom sklearn.model_selection import train_test_split\nfrom keras import regularizers\nfrom sklearn.preprocessing import LabelEncoder\n#ACCURACY CONTROL\nfrom sklearn.metrics import confusion_matrix, accuracy_score, classification_report, roc_auc_score, roc_curve\nfrom sklearn.model_selection import GridSearchCV, cross_val_score\nfrom sklearn.metrics import mean_squared_error, r2_score\n#OPTIMIZER\nfrom keras.optimizers import RMSprop,Adam,Optimizer,Optimizer, SGD\n#MODEL LAYERS\nfrom tensorflow.keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D, BatchNormalization,MaxPooling2D,BatchNormalization,\\\n                        Permute, TimeDistributed, Bidirectional,GRU, SimpleRNN,\\\nLSTM, GlobalAveragePooling2D, SeparableConv2D, ZeroPadding2D, Convolution2D, ZeroPadding2D,Reshape, Conv2DTranspose, LeakyReLU,\\\nConvLSTM2D,Conv3D\nfrom keras import models\nfrom keras import layers\nimport tensorflow as tf\nfrom keras.applications import VGG16,VGG19,inception_v3\nfrom keras import backend as K\nfrom keras.utils import plot_model\nfrom keras.datasets import mnist\nimport keras\n#SKLEARN CLASSIFIER\nfrom xgboost import XGBClassifier, XGBRegressor\nfrom lightgbm import LGBMClassifier, LGBMRegressor\nfrom catboost import CatBoostClassifier, CatBoostRegressor\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\nfrom sklearn.ensemble import GradientBoostingClassifier, GradientBoostingRegressor\nfrom sklearn.ensemble import BaggingRegressor\nfrom sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\nfrom sklearn.neural_network import MLPClassifier, MLPRegressor\nfrom sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.cross_decomposition import PLSRegression\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import RidgeCV\nfrom sklearn.linear_model import Lasso\nfrom sklearn.linear_model import LassoCV\nfrom sklearn.linear_model import ElasticNet\nfrom sklearn.linear_model import ElasticNetCV\n#IGNORING WARNINGS\nfrom warnings import filterwarnings\nfilterwarnings(\"ignore\",category=DeprecationWarning)\nfilterwarnings(\"ignore\", category=FutureWarning) \nfilterwarnings(\"ignore\", category=UserWarning)","ad2c7c02":"Carbon_Video_Set = \"..\/input\/climate-change-video-set-nasa\/climate\/0the5j_1.mp4\"\nIce_Video_Set = \"..\/input\/climate-change-video-set-nasa\/climate\/nsceeu_1.mp4\"","6dd93e65":"Video_IMG_List = []\n\nCapture_Video = cv2.VideoCapture(Ice_Video_Set)\n\nwhile Capture_Video.isOpened():\n    \n    ret,frame = Capture_Video.read()\n    \n    if ret != True:\n        break\n        \n    if Capture_Video.isOpened():\n        Transformation_IMG = cv2.cvtColor(frame,cv2.COLOR_BGR2RGB)\n        Resize_IMG = cv2.resize(Transformation_IMG,(180,180))\n        (Resize_IMG - 127.5) \/ 127.5\n        Video_IMG_List.append(Resize_IMG)\n        \n        \nCapture_Video.release()","a069821a":"print(np.shape(np.array(Video_IMG_List)))","51094af0":"Main_Array = np.array(Video_IMG_List)","85343305":"print(Main_Array.shape)","5461825f":"def simple_vision(image):\n    \n    figure = plt.figure(figsize=(10,10))\n    \n    plt.xlabel(image.shape)\n    plt.ylabel(image.size)\n    plt.imshow(image)","c1991b94":"def threshold_vision(image):\n    \n    figure = plt.figure(figsize=(10,10))\n    \n    _,Threshold_IMG = cv2.threshold(image,130,255,cv2.THRESH_BINARY)\n    \n    plt.xlabel(Threshold_IMG.shape)\n    plt.ylabel(Threshold_IMG.size)\n    plt.imshow(Threshold_IMG)    ","9a9b3654":"def Red_inRange_vision(image,lower_red,upper_red,lower_red_two,upper_red_two):\n    \n    figure = plt.figure(figsize=(10,10))\n    mask_kernel_shape = np.ones((5,5),dtype=\"uint8\")\n    \n    img_hsv = cv2.cvtColor(image, cv2.COLOR_RGB2HSV)\n    \n    Mask_Img_One = cv2.inRange(img_hsv,lower_red,upper_red)\n    Dilate_Img_One = cv2.dilate(Mask_Img_One,mask_kernel_shape)\n    \n    Mask_Img_Two = cv2.inRange(img_hsv,lower_red_two,upper_red_two)\n    Dilate_Img_Two = cv2.dilate(Mask_Img_Two,mask_kernel_shape)\n    \n    Main_Mask = Dilate_Img_One+Dilate_Img_Two\n    \n    output_img = image.copy()\n    output_img[np.where(Main_Mask==0)] = 0\n    output_hsv = img_hsv.copy()\n    output_hsv[np.where(Main_Mask==0)] = 0\n    \n    \n    plt.xlabel(output_img.shape)\n    plt.ylabel(output_img.size)\n    plt.imshow(output_img)","7eab8c4e":"def White_inRange_vision(image,lower_white,upper_white):\n    \n    figure = plt.figure(figsize=(10,10))\n    mask_kernel_shape = np.ones((5,5),dtype=\"uint8\")\n    \n    img_hsv = cv2.cvtColor(image, cv2.COLOR_RGB2HSV)\n    \n    Mask_Img = cv2.inRange(img_hsv,lower_white,upper_white)\n    Dilate_Img = cv2.dilate(Mask_Img,mask_kernel_shape)\n    \n    image_bitwise = cv2.bitwise_and(image,image, mask = Dilate_Img)\n    \n    \n    plt.xlabel(image_bitwise.shape)\n    plt.ylabel(image_bitwise.size)\n    plt.imshow(image_bitwise)","1a9e303b":"simple_vision(Main_Array[12])","350f535c":"simple_vision(Main_Array[1])","68be7b32":"simple_vision(Main_Array[300])","a5bf3faa":"threshold_vision(Main_Array[26])","f6917969":"threshold_vision(Main_Array[6])","96a22fdf":"threshold_vision(Main_Array[16])","d3b1a62c":"lower_white = np.array([0,0,0], dtype=np.uint8)\nupper_white = np.array([0,0,255], dtype=np.uint8)\n\nWhite_inRange_vision(Main_Array[6],lower_white,upper_white)","904bfa7a":"lower_white = np.array([0,0,0], dtype=np.uint8)\nupper_white = np.array([0,0,255], dtype=np.uint8)\n\nWhite_inRange_vision(Main_Array[56],lower_white,upper_white)","448669c7":"lower_white = np.array([0,0,0], dtype=np.uint8)\nupper_white = np.array([0,0,255], dtype=np.uint8)\n\nWhite_inRange_vision(Main_Array[60],lower_white,upper_white)","5140f8ca":"lower_white = np.array([0,0,0], dtype=np.uint8)\nupper_white = np.array([0,0,255], dtype=np.uint8)\n\nWhite_inRange_vision(Main_Array[1],lower_white,upper_white)","edb7437e":"lower_white = np.array([0,0,0], dtype=np.uint8)\nupper_white = np.array([0,0,255], dtype=np.uint8)\n\nWhite_inRange_vision(Main_Array[100],lower_white,upper_white)","3b8479e7":"lower_white = np.array([0,0,0], dtype=np.uint8)\nupper_white = np.array([0,0,255], dtype=np.uint8)\n\nWhite_inRange_vision(Main_Array[23],lower_white,upper_white)","e07a717b":"figure,axis = plt.subplots(5,5,figsize=(10,10))\n\nfor indexing,operations in enumerate(axis.flat):\n    \n    IMG_Picking = Main_Array[indexing]\n    operations.set_xlabel(IMG_Picking.shape)\n    operations.set_ylabel(IMG_Picking.size)\n    operations.imshow(IMG_Picking)\n    \nplt.tight_layout()\nplt.show()","8e85bf2f":"iterations = 60\nvector_noise_shape = 180\ncount_example = 16\nbatch_size = 12\ncount_buffer = 60000\n\nseed = tf.random.normal([count_example,vector_noise_shape])","e420eb8d":"print(Main_Array.shape)","1a1b7bdc":"Train_Data = tf.data.Dataset.from_tensor_slices(Main_Array).shuffle(count_buffer).batch(batch_size)","ce40ad1e":"print(Train_Data)","71c3a478":"def Generator_Model():\n    \n    \n    Model = Sequential()\n    #\n    Model.add(Dense(90*90*128,use_bias=False,input_shape=(180,)))\n    Model.add(BatchNormalization())\n    Model.add(LeakyReLU())\n    #\n    Model.add(Reshape((90,90,128)))\n    #\n    Model.add(Conv2DTranspose(128,(3,3),padding=\"same\",use_bias=False))\n    Model.add(BatchNormalization())\n    Model.add(LeakyReLU())\n    \n    Model.add(Conv2DTranspose(64, (3,3), strides=(2,2), padding='same', use_bias=False))\n    Model.add(BatchNormalization())\n    Model.add(LeakyReLU())\n    #\n    Model.add(Conv2DTranspose(3,(3,3),padding=\"same\",use_bias=False,activation=\"tanh\"))\n    \n    \n    return Model","c1c599d6":"Generator = Generator_Model()","90358169":"def Discriminator_Model():\n    \n    Model = Sequential()\n    \n    Model.add(Conv2D(64,(3,3),padding=\"same\",input_shape=[180,180,3]))\n    Model.add(Dropout(0.3))\n    Model.add(LeakyReLU())\n    \n    \n    Model.add(Conv2D(128,(3,3),padding=\"same\"))\n    Model.add(Dropout(0.3))\n    Model.add(LeakyReLU())\n    \n    Model.add(Flatten())\n    Model.add(Dense(1))\n    \n    return Model","14c5b70c":"Discriminator = Discriminator_Model()","173862d3":"Loss_Function = tf.keras.losses.BinaryCrossentropy(from_logits=True)","d16e9be1":"def Discriminator_Loss(real_output,fake_output):\n    \n    real_loss = Loss_Function(tf.ones_like(real_output),real_output)\n    fake_loss = Loss_Function(tf.zeros_like(fake_output),fake_output)\n    total_loss = real_loss + fake_loss\n    \n    return total_loss","a2bb0008":"def Generator_Loss(fake_output):\n    \n    return Loss_Function(tf.ones_like(fake_output),fake_output)","9296ea69":"Generator_Optimizer = RMSprop(lr=0.0001,clipvalue=1.0,decay=1e-8)\nDiscriminator_Optimizer = RMSprop(lr=0.0001,clipvalue=1.0,decay=1e-8)","43184bc7":"def generate_and_save_function(Model, epoch, test_input):\n    \n    predictions = Model(test_input, training=False)\n    fig = plt.figure(figsize=(10, 10))\n    \n    for i in range(predictions.shape[0]):\n        plt.subplot(5, 5, i+1)\n        plt.imshow(predictions[i, :, :, 0] * 127.5 + 127.5)\n        plt.axis('off')\n\n    plt.savefig('images_epoch_{:04d}.png'.format(epoch))\n    plt.show()","78576228":"def action_function(images):\n    \n    random_noise_vector = tf.random.normal([batch_size,vector_noise_shape])\n    \n    with tf.GradientTape() as Generator_Tape,tf.GradientTape() as Discriminator_Tape:\n        Generator_Fake_Images = Generator(random_noise_vector,training=False)\n        real_output = Discriminator(images,training=True)\n        fake_output = Discriminator(Generator_Fake_Images,training=True)\n        Generator_Loss_Output = Generator_Loss(fake_output)\n        Discriminator_Loss_Output = Discriminator_Loss(real_output,fake_output)\n        \n    Generator_Gradients = Generator_Tape.gradient(Generator_Loss_Output,Generator.trainable_variables)\n    Discriminator_Gradients = Discriminator_Tape.gradient(Discriminator_Loss_Output,Discriminator.trainable_variables)\n    \n    Generator_Optimizer.apply_gradients(zip(Generator_Gradients,Generator.trainable_variables))\n    Discriminator_Optimizer.apply_gradients(zip(Discriminator_Gradients,Discriminator.trainable_variables))","5c500017":"def train_function(images_data,iterations):\n    for epoch in range(iterations):\n        start = time.time()\n        for batching in images_data:\n            action_function(batching)\n            \n        display.clear_output(wait=True)\n        generate_and_save_function(Generator,epoch+1,seed)\n    \n    display.clear_output(wait=True)\n    generate_and_save_function(Generator,epoch,seed)","6135b024":"train_function(Train_Data,iterations)","7d66caa4":"Predict_Random_Noise = tf.random.normal(shape=[60,vector_noise_shape])","d7ed5f79":"Generator_Prediction = Generator(Predict_Random_Noise)","910eb3f5":"figure,axis = plt.subplots(3,3,figsize=(12,12))\n\nfor i,ax in enumerate(axis.flat):\n    Image_Picking = Generator_Prediction[i]\n    ax.imshow(Image_Picking,cmap=\"gray\")\n    ax.set_xlabel(Image_Picking.shape)\n    \nplt.tight_layout()\nplt.show()","c691d3ec":"figure = plt.figure(figsize=(10,10))\nplt.axis(\"off\")\nplt.imshow(Generator_Prediction[25])\nplt.show()","b83a2730":"figure = plt.figure(figsize=(10,10))\nplt.axis(\"off\")\nplt.imshow(Generator_Prediction[55])\nplt.show()","36d45115":"figure = plt.figure(figsize=(10,10))\nplt.axis(\"off\")\nplt.imshow(Generator_Prediction[59])\nplt.show()","fe7acb80":"figure = plt.figure(figsize=(10,10))\nplt.axis(\"off\")\nplt.imshow(Generator_Prediction[9])\nplt.show()","38c6e307":"figure = plt.figure(figsize=(10,10))\nplt.axis(\"off\")\nplt.imshow(Generator_Prediction[1])\nplt.show()","3541cf5d":"print(Generator_Prediction[1].dtype)\nprint(type(Generator_Prediction[1]))","9dafa5d8":"Layer_Output = [layer.output for layer in Generator.layers[:8]]\nactivation_model = models.Model(inputs=Generator.input,outputs=Layer_Output)","21c84ada":"activations = activation_model.predict(seed)","65b27b00":"first_layer_act = activations[0]\nprint(first_layer_act.shape)","7c1e98d1":"plt.matshow(first_layer_act[:10,:10],cmap=\"viridis\")","1a0626e8":"plt.matshow(first_layer_act[:1,:10],cmap=\"viridis\")","9ab5f21e":"Generator.save(\"Generator.h5\")\nDiscriminator.save(\"Discriminator.h5\")","2e4b4ad7":"#### OPTIMIZERS","ea7a3259":"#### GENERATOR","0b29a9cf":"#### VISION FUNCTION","600ea5d8":"# LEARNING IN LAYER","337826e6":"#### PROCESS","c4746faf":"#### VISION PROCESS","a192e4e5":"# PACKAGES AND LIBRARIES","099e39bc":"#### TRAINING FUNCTIONS","ca7aa0e0":"#### MAIN PATH","af132d60":"#### NOTICE\n\n* You can see two separate projects by switching between versions. ","921e6131":"# PROCESS BEFORE DC-GAN TRAINING","f0a81082":"#### LOSS FUNCTIONS","61ac87f5":"#### TRANSFORMATION","28a8354b":"# SAVING MODEL","a7b27a55":"#### TO ARRAY","ad0d442c":"#### GENERATING AND DISPLAYING FUNCTION","0b3c0643":"#### DISCRIMINATOR","437989c6":"# DATA EXPORT AND TRANSFORMATION","af0ea8c4":"# TRAINING","dbb64b39":"# PREDICTION","8797654c":"# DCGAN PROCESS","ef04e59a":"#### PARAMETERS","9d9f9a7d":"# VISION"}}