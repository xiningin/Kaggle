{"cell_type":{"ee32d632":"code","d5699ddf":"code","93b404d4":"code","64897c20":"code","5f695320":"code","52c26bb5":"code","87bb742f":"code","6c6477aa":"code","d8ece16e":"code","78c10ebf":"code","a507b864":"code","95448396":"code","7647337c":"code","a95725fd":"code","2891cf89":"code","d0679ef3":"markdown","9f3c90c2":"markdown","46fe7321":"markdown","504d35f0":"markdown","7f9f53a5":"markdown","e1e3363d":"markdown","a078750a":"markdown","2ab5ef39":"markdown","22a41014":"markdown","e1617f91":"markdown","7ae0469b":"markdown","4b6a4428":"markdown","3b069511":"markdown","1a4f1a57":"markdown","751dd6ef":"markdown","23e01ea9":"markdown","ab40fc04":"markdown","af74964d":"markdown","ddde0e5f":"markdown"},"source":{"ee32d632":"%%writefile constant_0.py\n\ndef constant_play_agent_0(observation, configuration):\n    return 0","d5699ddf":"%%writefile constant_1.py\n\ndef constant_play_agent_1(observation, configuration):\n    return 1","93b404d4":"%%writefile constant_2.py\n\ndef constant_play_agent_2(observation, configuration):\n    return 2","64897c20":"%%writefile nash_equilibrium.py\n\nimport random\n\ndef nash_equilibrium_agent(observation, configuration):\n    return random.randint(0, 2)","5f695320":"%%writefile mirror_opponent.py\n\ndef mirror_opponent_agent(observation, configuration):\n    if observation.step > 0:\n        return observation.lastOpponentAction\n    else:\n        return 0","52c26bb5":"%%writefile mirror_shift_opponent_1.py\n\ndef mirror_shift_opponent_agent_1(observation, configuration):\n    if observation.step > 0:\n        return (observation.lastOpponentAction + 1) % 3\n    else:\n        return 0 ","87bb742f":"%%writefile mirror_shift_opponent_2.py\n\ndef mirror_shift_opponent_agent_2(observation, configuration):\n    if observation.step > 0:\n        return (observation.lastOpponentAction + 2) % 3\n    else:\n        return 0","6c6477aa":"%%writefile mirror_shift_opponent_0.py\n\ndef mirror_shift_opponent_agent_0(observation, configuration):\n    if observation.step > 0:\n        return (observation.lastOpponentAction)\n    else:\n        return 0","d8ece16e":"%%writefile mirror_shift_1.py\n\ndef mirror_shift_agent_1(observation, configuration):\n    if observation.step > 0:\n        return (observation.lastAction + 1) % 3\n    else:\n        return 0 ","78c10ebf":"%%writefile mirror_shift_2.py\n\ndef mirror_shift_agent_2(observation, configuration):\n    if observation.step > 0:\n        return (observation.lastAction + 2) % 3\n    else:\n        return 0 ","a507b864":"%%writefile transition_matrix.py\n\nimport numpy as np\nimport pandas as pd\nimport random\n\nT = np.zeros((3, 3))\nP = np.zeros((3, 3))\n\n# action_opponent_1 : action of the opponent with 1 step before\n# action_opponent_2 : action of the opponent with 2 steps before\n\naction_opponent_1, action_opponent_2 = None, None\n\ndef transition_matrix_agent(observation, configuration):\n    global T, P, action_opponent_1, action_opponent_2\n    if observation.step > 1:\n        action_opponent_1 = observation.lastOpponentAction\n        T[action_opponent_2, action_opponent_1] += 1\n        P = np.divide(T, np.maximum(1, T.sum(axis=1)).reshape(-1, 1))\n        action_opponent_2 = action_opponent_1\n        if np.sum(P[action_opponent_1, :]) == 1:\n            return int((np.random.choice(\n                [0, 1, 2],\n                p=P[action_opponent_1, :]\n            ) + 1) % 3)\n        else:\n            return int(np.random.randint(3))\n\n    else:\n        if observation.step == 1:\n            action_opponent_2 = observation.lastOpponentAction\n        return int(np.random.randint(3))","95448396":"from kaggle_environments import make","7647337c":"env = make(\"rps\", configuration={\"episodeSteps\": 500})","a95725fd":"# constant_agent_0 vs nash_equilibrium_agent\nenv.run([\"constant_0.py\", \"nash_equilibrium.py\"])\n\nenv.render(mode=\"ipython\", width=600, height=600)","2891cf89":"# mirror_shift_opponent_2 vs mirror_opponent_agent\nenv.run([\"mirror_shift_opponent_2.py\", \"mirror_opponent.py\"])\n\nenv.render(mode=\"ipython\", width=600, height=600)","d0679ef3":"We initialize a Kaggle environment, setting the type of the environment ( `rps` means `rock-paper-scissors`) and the number the episodes to run the simulator (in this case we set it to 500).","9f3c90c2":"\n\nFor each of the strategies we explain the principle behind it as well as providing a code implementation for an agent following this strategy. The strategies are:  \n* Play constantly the same option;  \n* Nash Equilibrium;  \n* Mirror Oponent Behaviour;  \n* Mirror Oponent Behaviour with Shift;  \n* Shift with Constant;  \n* Transition Matrix.   \n    \nThen, using the simulator provided by Kaggle, we are presenting the results of the competition between such agents.\n\nIn order to see the code, press the button `Code`. To see the output, press the button `Output`.","46fe7321":"<h3 style='background:#DBAAFF; border:0; color:black'><center>Constant agent vs. Nash Equilibrium agent<\/center><\/h3> \n ","504d35f0":"\n<h3 style='background:#DBAAFF; border:0; color:black'><center>Shift own last play with 2<\/center><\/h3> ","7f9f53a5":"<h2 style='background:#12AAFF; border:0; color:black'><center>Play constantly<\/center><\/h2> \n\n\nIn this strategy, the agent will play constantly the same option. This strategy is extremely simple to implement. In the same time, agents playing naively against cohorts of such agents might get into truble and have the score decreased.\n\nThe three code examples we include here are implementing all 3 possible option (Rock, Paper and Scissors).\n\n\n<h3 style='background:#DBAAFF; border:0; color:black'><center>Play rock<\/center><\/h3> \n","e1e3363d":"<h2 style='background:#12AAFF; border:0; color:black'><center>Shift with constant<\/center><\/h2> \n\n\nIn this strategy, one is constantly shifting his play every step with a fixed value. This will work well when the opponent is mirroring your strategy. \n\nThe implementation is very similar to the one from mirror with shift, only that we pick-up here our last action and shift it, not the opponent.\n\n\n<h3 style='background:#DBAAFF; border:0; color:black'><center>Shift own last play with 1<\/center><\/h3> ","a078750a":"<h2 style='background:#12AAFF; border:0; color:black'><center>Mirror opponent behavior with shift<\/center><\/h2> \n\n\nIn this strategy, one play competitor's last action with a fixed shift. For example, if the shift is fixed to one, if the oponent played Rock, we play Rock + 1, i.e. Paper, if the competitor played Paper, we play scissors etc. In this strategy, we assume that if the competitor is playing constant, we win all the time. The shift can be either 1, 2 or 0 (then it repeats). For shift = 0, the strategy is similar with miror opponent.\n\nThe implementations adds a `1, 2, or 0` shift to the observed opponent last action.\n\nNote: we can also parameterize the implementation, to provide the shift as a parameter here.\n\n<h3 style='background:#DBAAFF; border:0; color:black'><center>Mirror oponent with shift 1<\/center><\/h3> \n","2ab5ef39":"<h2 style='background:#12AAFF; border:0; color:black'><center>Mirror opponent behavior<\/center><\/h2> \n\n\n\nIn this strategy, the agent will play the competitor last action. The strategy is based on the assumption that if the oponent is implementing a constant strategy, it will tie and if not, there are equal probabilities to loose or prevail.\n\nThe implementation is using the information from the observation to retrieve the last opponent action and build the response based on this.","22a41014":"<h3 style='background:#DBAAFF; border:0; color:black'><center>Play scissors<\/center><\/h3> ","e1617f91":"<h3 style='background:#DBAAFF; border:0; color:black'><center>Mirror Shift with Constant agent vs. Mirror Opponent agent<\/center><\/h3> \n","7ae0469b":"\n<h3 style='background:#DBAAFF; border:0; color:black'><center>Papers, Articles, Blogs<\/center><\/h3> \n\n\n[Why Winning in Rock-Paper-Scissors (and in Life) Isn\u2019t Everything](https:\/\/www.quantamagazine.org\/the-game-theory-math-behind-rock-paper-scissors-20180402\/)\n \n[The Long, Winding History of Rock Paper Scissors](https:\/\/www.popularmechanics.com\/culture\/gaming\/a31213381\/rock-paper-scissors-history\/)\n\n[How to win at rock-paper-scissors](https:\/\/www.bbc.com\/news\/science-environment-27228416)\n\n[How to Always Win \u2018Rock, Paper, Scissors\u2019](https:\/\/medium.com\/@hazelclementine\/how-to-always-win-rock-paper-scissors-452eecb95dc5)\n\n[How to Win at Rock, Paper, Scissors](https:\/\/www.wikihow.com\/Win-at-Rock,-Paper,-Scissors)  \n\n[Rock, Paper, Scissors, Lizard, Spock - from Big Bang Theory TV Series](https:\/\/bigbangtheory.fandom.com\/wiki\/Rock,_Paper,_Scissors,_Lizard,_Spock)\n\n[Multi-AI competing and winning against humans in iterated\nRock-Paper-Scissors game ](https:\/\/arxiv.org\/ftp\/arxiv\/papers\/2003\/2003.06769.pdf)  \n\n\n[Game Theory and the Nash Equilibrium](http:\/\/www.science4all.org\/article\/game-theory\/)\n\n\n[How to play Rock Paper Scissors in Japanese \u2013 \u3058\u3083\u3093\u3051\u3093 (Janken) \u2013 Handy Illustrated Guide](https:\/\/cotoacademy.com\/how-to-play-rock-scissors-paper-in-japanese-janken-scissors-papaper-stone-japanese\/)  \n\n[Creating a Rock Paper Scissors Game in Java with a Markov Chain for the AI](https:\/\/ssaurel.medium.com\/creating-a-rock-paper-scissors-game-in-java-with-a-markov-chain-for-the-ai-7672954fd7f6)\n\n[Rock, paper, scissors: Nature\u2019s playful strategy](https:\/\/medium.com\/the-digital-biologist\/rock-paper-scissors-natures-playful-strategy-9ed75794e037)\n\n\n<h3 style='background:#DBAAFF; border:0; color:black'><center>Kernels<\/center><\/h3> \n\n\n\n[Rock Paper Scissors - Agents Comparison](https:\/\/www.kaggle.com\/ihelon\/rock-paper-scissors-agents-comparison)   \n\n[Rock Paper Scissors - Nash Equilibrium Strategy](https:\/\/www.kaggle.com\/ihelon\/rock-paper-scissors-nash-equilibrium-strategy)   \n\n[RPS: Opponent Transition Matrix](https:\/\/www.kaggle.com\/group16\/rps-opponent-transition-matrix)\n\n[Multi-armed bandit vs deterministic agents](https:\/\/www.kaggle.com\/ilialar\/multi-armed-bandit-vs-deterministic-agents)\n\n","4b6a4428":"<h2 style='background:#12AAFF; border:0; color:black'><center>Simulated battles<\/center><\/h2> \n\n\n\n","3b069511":"<h1 style='background:#DBAAFF; border:0; color:black'><center>Rock Paper Scissors Strategies<\/center><\/h1>\n\n<br>\n\n<center><img src=\"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/2\/23\/Playing_janken_-_school_in_Japan.jpg\" width=500><\/img><\/center>\n\n<br> \n\n<h2 style='background:#12AAFF; border:0; color:black'><center>Introduction<\/center><\/h2> \n \n\n<p>We will analyze here several of the strategies employed by various competitors.<\/p>    \n","1a4f1a57":"<h3 style='background:#DBAAFF; border:0; color:black'><center>Simple mirror oponent (shift 0)<\/center><\/h3> ","751dd6ef":"<h3 style='background:#DBAAFF; border:0; color:black'><center>Mirror oponent with shift 2<\/center><\/h3> ","23e01ea9":"<h3 style='background:#DBAAFF; border:0; color:black'><center>Play paper<\/center><\/h3> ","ab40fc04":"<h2 style='background:#12AAFF; border:0; color:black'><center>Transition matrix<\/center><\/h2> \n\n\nIn this strategy, one create a simple Markov Chain with 3 states and a 3x3 transition matrix. It is initialized with uniform probabilities and then learn the transition matrix from the data to predict the next move of the opponent.\nThe implementation is from this Kernel: [RPS: Opponent Transition Matrix](https:\/\/www.kaggle.com\/group16\/rps-opponent-transition-matrix)\n","af74964d":"<h2 style='background:#12AAFF; border:0; color:black'><center>References<\/center><\/h2> \n","ddde0e5f":"<h2 style='background:#12AAFF; border:0; color:black'><center>Nash Equilibrium<\/center><\/h2> \n\n\nNash Equilibrium is a proposed solution of a non-cooperative game involving two or more players in which each player is assumed to know the equilibrium strategies of the other players, and no player has anything to gain by changing only their own strategy. In particular, for Rock-Paper-Scissors, Nash Equilibrium requires each player adopting this strategy to play randomly so that each of the 3 options has equal probability, 1\/3 to be playes every time.  \n\nLet's analyse the succes of one agent playing Nash Equilibrium against the first type of agent. Let's suppose the first agent will play Rock all the time, then in 1\/3 cases will tie with the one playing Nash Equilibrium, in 1\/3 cases will prevail (when the oponent plays Scissors) and in 1\/3 cases will lose (when the oponent plays Paper).\n\nIn the next figure - from [Science4All](http:\/\/www.science4all.org\/article\/game-theory\/) - I show the payoff matrix for two opponets playing Nash Equilibrium strategy.\n\n\n![](http:\/\/www.science4all.org\/wp-content\/uploads\/2012\/08\/rock-paper-scissors1.png)\n\nConsequently, two agents playing both a Nash Equilibrium based strategy will most probably tie in the long run.\n\n\nIn a different case, when a player implementing this strategy is fighting various bots that play constant option, it might loose, depending on the dstribution of matches."}}