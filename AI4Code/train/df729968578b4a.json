{"cell_type":{"f81479d7":"code","a8a224ca":"code","91711cca":"code","98020c4c":"code","25d0ed93":"code","381fbda3":"code","e2eeee83":"code","2a75e2cf":"code","ac3ec88c":"code","d5076535":"code","eaaab508":"code","bacd8bb5":"code","e578c92c":"code","1c23d2a5":"code","cb39928f":"code","b6372445":"code","bb4f63a7":"code","6bfffa04":"code","f26d348b":"code","f9291dd8":"code","c8cba5ce":"code","e8c9a9a8":"code","d93db0c3":"code","8bba931c":"code","ef3cc772":"code","65791475":"code","644df15d":"code","268c6b00":"code","480e25b0":"code","ac88c2c7":"code","bbee5245":"code","c8094f29":"code","76553ad1":"code","870f9ff0":"code","5607434a":"code","0199207d":"code","b2a5a4ca":"code","93a8ec98":"markdown","7b23c8e9":"markdown","9f42f583":"markdown","35551300":"markdown","7ec16ffd":"markdown","167df7ff":"markdown","63dfb124":"markdown","a057d25e":"markdown","20261b66":"markdown","a0ccc997":"markdown","6f8c4591":"markdown","185b6be1":"markdown","36aba53d":"markdown","5e669d06":"markdown","7ac49c81":"markdown","6fb95843":"markdown","d701f715":"markdown","987b270c":"markdown","ad828229":"markdown","4f35dfb0":"markdown","b0b7a596":"markdown","e2472e31":"markdown","bcc4069b":"markdown","6d463171":"markdown","41b4ac31":"markdown","212bcb3d":"markdown","27af3389":"markdown","41b84485":"markdown","229dee1e":"markdown"},"source":{"f81479d7":"%matplotlib inline\nimport warnings\nwarnings.filterwarnings(\"ignore\")  #Ignoring unnecessory warnings\n\nimport sqlite3\nimport pandas as pd   #for data manipulation and analysis\nimport numpy as np    #for large and multi-dimensional arrays\nimport nltk        #Natural language processing tool-kit\nimport string\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.feature_extraction.text import TfidfTransformer \nfrom sklearn.feature_extraction.text import TfidfVectorizer #For TF-IDF\n\nfrom sklearn.feature_extraction.text import CountVectorizer #For Bag of words\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn import metrics\nfrom sklearn.metrics import roc_curve, auc\n\nimport re\nimport string\nfrom nltk.corpus import stopwords   #Stopwords corpus\nfrom nltk.stem import PorterStemmer  #Stemmer\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.stem import SnowballStemmer\nfrom nltk.stem.wordnet import WordNetLemmatizer\n\nfrom gensim.models import Word2Vec  #For Word2Vec\nfrom gensim.models import KeyedVectors\nimport pickle\n\nfrom tqdm import tqdm #Progress Meters or Progress Bars\nimport os\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV, train_test_split\nfrom sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, accuracy_score, roc_curve, auc\nfrom sklearn.calibration import CalibratedClassifierCV\nfrom datetime import datetime as dt","a8a224ca":"# using the SQLite Table to read data.\nconnection_sqlobject = sqlite3.connect(\"..\/input\/amazon-fine-food-reviews\/database.sqlite\")","91711cca":"df = pd.read_sql_query(\"\"\" SELECT * FROM Reviews\"\"\", connection_sqlobject)","98020c4c":"df.shape","25d0ed93":"ax=df.Score.value_counts().plot(kind='bar')\nplt.xlabel(\"Ratings\")\nplt.ylabel(\"Count of People\")","381fbda3":" filtered_data = pd.read_sql_query(\"\"\" SELECT * FROM Reviews WHERE Score != 3 \"\"\", connection_sqlobject) \n\n\n#Give reviews with Score > 3 a 'Positive' tag, and reviews with a score < 3 a 'Negative' tag.\npolarity = filtered_data['Score'].apply(lambda x : 'Positive' if x > 3 else 'Negative')\nfiltered_data['SentimentPolarity'] = polarity\n\nlabels = filtered_data['SentimentPolarity'].apply(lambda x : 1 if x == 'Positive' else 0)\nfiltered_data['Class_Labels'] = labels\n\nprint(\"Number of data points in our data\", filtered_data.shape[0])\nprint(\"Number of features in our data\", filtered_data.shape[1])\n\nfiltered_data.head()","e2eeee83":"ax = filtered_data.SentimentPolarity.value_counts().plot(kind='bar')\nplt.xlabel(\"Ratings Polarity\")\n# plt.ylabel(\"Count of People\")","2a75e2cf":"filtered_data['SentimentPolarity'].value_counts()","ac3ec88c":"filtered_data.info()","d5076535":"# checking duplicates\nfiltered_data.duplicated(subset=[\"UserId\",\"ProfileName\",\"Time\",\"Summary\",\"Text\"]).value_counts()","eaaab508":"#Sorting data according to ProductId in ascending order\nsorted_data=filtered_data.sort_values('ProductId', axis=0, ascending=True, inplace=False, kind='quicksort', na_position='last')","bacd8bb5":"#Deduplication of entries\nfinal=sorted_data.drop_duplicates(subset={\"UserId\",\"ProfileName\",\"Time\",\"Text\"}, keep='first', inplace=False)\nfinal.shape","e578c92c":"#Checking to see how much % of data still remains\n(final['Id'].size*1.0)\/(filtered_data['Id'].size*1.0)*100","1c23d2a5":"display= pd.read_sql_query(\"\"\"\nSELECT *\nFROM Reviews\nWHERE HelpfulnessNumerator > HelpfulnessDenominator\nORDER BY ProductID\n\"\"\", con)\n\ndisplay.head()","cb39928f":"final=final[final.HelpfulnessNumerator<=final.HelpfulnessDenominator]\n#Before starting the next phase of preprocessing lets see the number of entries left\nprint(final.shape)","b6372445":"#How many positive and negative reviews are present in our dataset?\nfinal['Score'].value_counts()","bb4f63a7":"from bs4 import BeautifulSoup","6bfffa04":"def decontracted(phrase):\n    # specific\n    phrase = re.sub(r\"won't\", \"will not\", phrase)\n    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n\n    # general\n    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n    return phrase","f26d348b":"stopwords = nltk.corpus.stopwords.words('english')\nstopwords.append('br')\nprint(stopwords)","f9291dd8":"from nltk.stem import SnowballStemmer\nsnow = SnowballStemmer('english')","c8cba5ce":"from tqdm import tqdm        # tqdm is for printing the status bar\npreprocessed_reviews = []\n\nfor sentance in tqdm(final['Text'].values):\n    sentance = re.sub(r\"http\\S+\", \"\", sentance) #removing url\n    sentance = BeautifulSoup(sentance, 'lxml').get_text() # removing html tags\n    sentance = decontracted(sentance) # exapnsion of words OR replacing apostrophe\n    sentance = re.sub(\"\\S*\\d\\S*\", \"\", sentance).strip()  # removing alphanumeric words\n    sentance = re.sub('[^A-Za-z]+', ' ', sentance)  # removing special character\n    sentance = ' '.join(e.lower() for e in sentance.split() if e.lower() not in stopwords) #removing stopwords\n#     sentence = ' '.join(snow.stem(word) for word in sentance) # Stemming\n    preprocessed_reviews.append(sentance.strip())","e8c9a9a8":"final['Preprocessed_reviews']=preprocessed_reviews\nfinal.head().T","d93db0c3":"positive_reviews = []\nnegative_reviews = []\nfor i in range(len(final.index)):\n    if final['Score'].values[i] == 'positive':\n        positive_reviews.append(final['Preprocessed_reviews'].values[i])\n    if final['Score'].values[i] == 'negative':\n        negative_reviews.append(final['Preprocessed_reviews'].values[i])","8bba931c":"print(\"Number postive reviews\",len(positive_reviews))\nprint(\"Number negative reviews\",len(negative_reviews))","ef3cc772":"positive_reviews_text = \" \".join(word for word in positive_reviews)\nnegative_reviews_text = \" \".join(word for word in negative_reviews)\n\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n\npositive_reviews_cloud = WordCloud(stopwords=STOPWORDS, background_color=\"white\").generate(positive_reviews_text)\nnegative_reviews_cloud = WordCloud(stopwords=STOPWORDS, background_color=\"white\").generate(negative_reviews_text)","65791475":"def show_word_cloud(cloud, title):\n  plt.figure(figsize = (20, 20))\n  plt.imshow(cloud, interpolation='bilinear')\n  plt.title(title)\n  plt.axis(\"off\")\n  plt.show();","644df15d":"show_word_cloud(positive_reviews_cloud, \"Common Words in Positive Reviews\")","268c6b00":"show_word_cloud(negative_reviews_cloud, \"Common Words in Negative Reviews\")","480e25b0":"count_vect = CountVectorizer() #in scikit-learn\ncount_vect.fit(preprocessed_reviews)\nprint(\"some feature names \", count_vect.get_feature_names()[:10])\nprint('='*50)\n\nfinal_counts = count_vect.transform(preprocessed_reviews)\nprint(\"the type of count vectorizer \",type(final_counts))\nprint(\"the shape of out text BOW vectorizer \",final_counts.get_shape())\nprint(\"the number of unique words \", final_counts.get_shape()[1])","ac88c2c7":"# ngram_range of (1, 1) means only unigrams, (1, 2) means unigrams and bigrams, and (2, 2) means only bigrams\n# min_df = ignore terms that have a document frequency strictly lower than the given threshold.\ncount_vect = CountVectorizer(ngram_range=(1,2),min_df=10)\nfinal_bigram_counts = count_vect.fit_transform(preprocessed_reviews)\nprint(\"the type of count vectorizer \",type(final_bigram_counts))\nprint(\"the shape of out text BOW vectorizer \",final_bigram_counts.get_shape())\nprint(\"the number of unique words including both unigrams and bigrams \", final_bigram_counts.get_shape()[1])","bbee5245":"tf_idf_vect = TfidfVectorizer(ngram_range=(1,2), min_df=10)\ntf_idf_vect.fit(preprocessed_reviews)\nprint(\"some sample features(unique words in the corpus)\",tf_idf_vect.get_feature_names()[0:10])\nprint('='*50)\n\nfinal_tf_idf = tf_idf_vect.transform(preprocessed_reviews)\nprint(\"the type of count vectorizer \",type(final_tf_idf))\nprint(\"the shape of out text TFIDF vectorizer \",final_tf_idf.get_shape())\nprint(\"the number of unique words including both unigrams and bigrams \", final_tf_idf.get_shape()[1])","c8094f29":"list_of_sentance=[]\nfor sentance in preprocessed_reviews:\n    list_of_sentance.append(sentance.split()) #splitting words","76553ad1":"# min_count = 5 considers only words that occured atleast 5 times\nw2v_model=Word2Vec(list_of_sentance,min_count=5,vector_size=50, workers=4)\nprint(w2v_model.wv.most_similar('great'))\nprint('='*50)\nprint(w2v_model.wv.most_similar('worst'))","870f9ff0":"w2v_words = list(w2v_model.wv.key_to_index)\nprint(\"number of words that occured minimum 5 times \",len(w2v_words))\nprint(\"sample words \", w2v_words[0:50])","5607434a":"# compute average word2vec for each review.\nsent_vectors = []; # the avg-w2v for each sentence\/review is stored in this list\nfor sent in tqdm(list_of_sentance): # for each review\/sentence\n    sent_vec = np.zeros(50) # as word vectors are of zero length 50.\n    cnt_words =0; # num of words with a valid vector in the sentence\/review\n    for word in sent: # for each word in a review\/sentence\n        if word in w2v_words:\n            vec = w2v_model.wv[word]\n            sent_vec += vec\n            cnt_words += 1\n    if cnt_words != 0:\n        sent_vec \/= cnt_words\n    sent_vectors.append(sent_vec)\nprint(len(sent_vectors))\nprint(len(sent_vectors[0]))","0199207d":"model = TfidfVectorizer()\ntf_idf_matrix = model.fit_transform(preprocessed_reviews)\n# we are converting a dictionary with word as a key, and the idf as a value\ndictionary = dict(zip(model.get_feature_names(), list(model.idf_)))","b2a5a4ca":"# TF-IDF weighted Word2Vec\ntfidf_feat = model.get_feature_names() # tfidf words\/col-names\n# final_tf_idf is the sparse matrix with row= sentence, col=word and cell_val = tfidf\n\ntfidf_sent_vectors = []; # the tfidf-w2v for each sentence\/review is stored in this list\nrow=0;\nfor sent in tqdm(list_of_sentance): # for each review\/sentence \n    sent_vec = np.zeros(50) # as word vectors are of zero length\n    weight_sum =0; # num of words with a valid vector in the sentence\/review\n    for word in sent: # for each word in a review\/sentence\n        if word in w2v_words and word in tfidf_feat:\n            vec = w2v_model.wv[word]\n            # tf_idf = tf_idf_matrix[row, tfidf_feat.index(word)]\n            # to reduce the computation we are \n            # dictionary[word] = idf value of word in whole courpus\n            # sent.count(word) = tf valeus of word in this review\n            tf_idf = dictionary[word]*(sent.count(word)\/len(sent))\n            sent_vec += (vec * tf_idf)\n            weight_sum += tf_idf\n    if weight_sum != 0:\n        sent_vec \/= weight_sum\n    tfidf_sent_vectors.append(sent_vec)\n    row += 1","93a8ec98":"We have 5-star rating system. It looks like we have more reviews with ratings 5, this can lead to unbalanced classes. We will treat rating 4 and 5 as positive and 3-star treat as neutral and rest as negative reviews.","7b23c8e9":"As can be seen above the same user has multiple reviews at the same Time with same Summary and Text.\n\n\nHence it was necessary to remove duplicates in order to get unbiased results for the analysis of the data.\n\n\nWe first sort the data according to ProductId and then just keep the first similar product review and delete the others.","9f42f583":"# **Featurization**","35551300":"# **Objective:**\nGiven a review, determine whether the review is positive or negative?\n\nIf we look at the `Score` column we can make out that the review is positive or not by using simple if-else condition. But we don't need to implement any ML here.\n\nSo for this problem, we will put our focus on to the Review\/Summary text. Based on the review text we will build a prediction model and determine if a future review is positive or negative. In this problem we will apply **classification** techniques called **Logistic Regression**.\n\n**Assumption:**\n- Score 4 or 5 : Positive Review\n- Score 1or 2 : Negative Review\n- Score 3 : Neutral Review","7ec16ffd":"**TF-IDF**\n- More importance to rarer words in whole document\/review.\n- More importance if word is frequent in a document\/review.\n\n**TF** measures the frequency of a word in a document\/review.\n$$ \\text{TF(Term frequency)} = \\frac{\\text {The number of times a word occurs in the review}}{\\text {Total number of words in the review}}$$\n\n**IDF** measures the informativeness of word in total document\/reviews.\n$$\\text {IDF(Inverse Document Frequency)} = log{\\frac{\\text{Total number of reviews}}{\\text{Number of reviews containing word i}}}$$\n\n\n**TF-IDF = TF x IDF**\n\n**Limitation**: It doesn't take the semantic meaning.\n\nEx: tasty & delicious both are synonyms\/almost similar word but in TF-IDF representaion both will be diffrent","167df7ff":"# **Loading the data**\n\nThe dataset is available in two forms\n\n1.     .csv file\n2.    SQLite Database\n\nIn order to load the data, We have used the SQLITE dataset as it easier to query the data and visualise the data efficiently.\n\nHere as we only want to get the global sentiment of the recommendations (positive or negative), we will purposefully ignore all Scores equal to 3. If the score > 3, then the recommendation wil be set to **\"positive\"**. Otherwise, it will be set to **\"negative\"**.","63dfb124":"# BAG OF WORDS\n\nA bag-of-words is a representation of text that describes the occurrence of words within a document.\nIf there are **d** unique words in our dictionary then for every sentence or review the vector will be of length **d** and count of word from review is stored at its particular location in vector. \n\nEx. pasta is tasty and pasta is good\n\n| a | and | be | good | is | pasta | tasty | the | ... |\n| --- | --- | --- | --- | --- | --- | --- | --- | --- |\n| 0 | 1 | 0 | 1 | 2 | 2 | 1 | 0 | .... |\n\n\nUsing scikit-learn's `CountVectorizer` we can get the BoW, one of parameter is `max_features =5000` it tells about to consider only top 5000 most frequently repeated words to place in a dictionary. so our dictionary length or vector length will be only 5000\n\n### BINARY BAG OF WORDS \nWe don't count the frequency of word, we just place 1 if the word appears in the review or else 0. In `CountVectorizer` there is a parameter `binary = true` this makes our BoW to binary BoW.\n\nEg.:\n   1. It was the best of times\n   2. It was the age of wisdom\n\nSet of all unique words\\\n{\u201cit\u201d,\u201cwas\u201d,\u201cthe\u201d,\u201cbest\u201d,\u201cof\u201d,\u201ctimes\u201d,\u201cage\u201d,\"wisdom\"}\n\nThere are 9 unique words.\n\n1. \"It was the best of times\" => [1, 1, 1, 1, 1, 1, 0, 0]\n2. \"it was the age of wisdom\" => [1, 1, 1, 0, 1, 0, 1, 1]\n","a057d25e":"## Removing the HTML tags\n\n`from bs4 import BeautifulSoup`\n\n`soup = BeautifulSoup(sentence, 'lxml').get_text()`\n\n\nhttps:\/\/stackoverflow.com\/questions\/16206380\/python-beautifulsoup-how-to-remove-all-tags-from-an-element","20261b66":"# TF-IDF","a0ccc997":"* HelpfulnessNumerator = (Yes) Review is good or not\n* HelpfulnessDenominator = (Yes + No) Review is good or not\n\nIt should be **Helpfullness Numerator is less than or equal to Helpfullness Denominator**","6f8c4591":"## Removing spacial character\n\nhttps:\/\/stackoverflow.com\/a\/5843547\/4084039\n\n`re.sub('[^A-Za-z0-9]+', ' ', sent_1500)`","185b6be1":"**Observation:** In Two rows the value of HelpfulnessNumerator is greater than HelpfulnessDenominator which is not practically possible hence these two rows too are removed from calcualtions.","36aba53d":"# Preprocessing Review Text\nNow that we have finished deduplication our data requires some preprocessing before we go on further with analysis and making the prediction model.","5e669d06":"## Pre-processing on all the reviews\n\n### Combining all the above statements","7ac49c81":"# **Introduction**\n\n![AmzonBanner](https:\/\/entrackr.com\/wp-content\/uploads\/2019\/02\/Amazon-1-1200x600.jpg)\n\nThis dataset consists of reviews of fine foods from amazon. The data span a period of more than 10 years, including all ~500,000 reviews up to October 2012.\n\nDataset consists of reviews of fine foods from Amazon.\n\nNumber of reviews: 568,454 \\\nNumber of users: 256,059 \\\nNumber of products: 74,258 \\\nTimespan: Oct 1999 - Oct 2012 \\\nNumber of Attributes\/Columns in data: 10 \n\nAttribute Information:\n\n1. Id\n2. ProductId - unique identifier for the product\n3. UserId - unqiue identifier for the user\n4. ProfileName\n5. HelpfulnessNumerator - number of users who found the review helpful\n6. HelpfulnessDenominator - number of users who indicated whether they found the review helpful or not\n7. Score - rating between 1 and 5\n8. Time - timestamp for the review\n9. Summary - brief summary of the review\n10. Text - text of the review","6fb95843":"## Removing words which contains numbers \n\nhttps:\/\/stackoverflow.com\/a\/18082370\/4084039\n\n`re.sub(\"\\S*\\d\\S*\", \"\", sentence).strip()`\n","d701f715":"## Removing urls from text\n\n`re.sub(r'http\\S+', '', Sentence)`\n\nhttps:\/\/stackoverflow.com\/a\/40823105\/4084039","987b270c":"# Word2Vec\nIt takes the semantic meaning of the words and understands the relationships of the words. It also looks at the neighbors of the word in the sentence.\n","ad828229":"**Drawbacks of BoW\/ Binary BoW**\nFor example: If we consider two reviews\n1. This pasta is very tasty\n2. This pasta is not tasty \n\nAfter stopwords removal both sentences will be converted to `pasta tasty` so both giving exact same meaning.\nThe problem is that we are not considering the front and back words related to every word, here comes **bi-gram** and **n-gram** techniques.\n\n# Bi-Grams and n-Grams.\n- removing stop words like \"not\" should be avoided before building n-grams\n- In Bi-gram we use pair of words in BOW. \n- Tri- grams: 3 Consecutive words,so as n-gram.\\\nBut this massively increases our dictionary size.","4f35dfb0":"**Average Word2Vec**\n\nSimple Averaging of the Word2Vec of all words.","b0b7a596":"# **Text Preprocessing**\n# Data Cleaning\n**Checking duplicates**","e2472e31":"# Reading Data","bcc4069b":"## Removing Stopwords\n\nStopwords are the unnecessary words that even if they are removed the sentiment of the sentence dosent change.\n\nEx - This pasta is so tasty ==> pasta tasty\n\n`<br\/>` these are removed\nbut `<br \/><br \/>` ==> after removing HTML tag, we are getting \"br br\"\n\nSo, We have to include br in stopwords","6d463171":"**TF-IDF weighted WORD2VEC**\n\nIn TF-IDF Word2Vec the Word2Vec value of each word is multiplied by the tf-idf value of that word and summed up and then divided by the sum of the tf-idf values of the sentence.","41b4ac31":"### Loading some library which needed throughout the analysis:","212bcb3d":"## Replacing apostrophe\/short words\n\nhttps:\/\/stackoverflow.com\/questions\/19790188\/expanding-english-language-contractions-in-python\/47091490#47091490","27af3389":"## **Don't forget to upvote if you Liked, this Notebook would be updated further**","41b84485":"filtering only positive and negative reviews i.e. not taking into consideration those reviews with Score=3","229dee1e":"## Stemming & Lemmatization\n\n| Stemming | Lemmatization |\n| --- | --- |\n| Stemming is faster because it chops words without knowing the context of the word in given sentences. | Lemmatization is slower as compared to stemming but it knows the context of the word before proceeding. |\n| Stemming is preferred when the meaning of the word is not important for analysis. | Lemmatization would be recommended when the meaning of the word is important for analysis. |\n| troubled -> troubl | troubled -> trouble |\n\n\nQ) Should I perform both lemmatization and stemming?\n\nANS: https:\/\/stackoverflow.com\/questions\/49354665\/should-i-perform-both-lemmatization-and-stemming\n\nSnowball stemmer is more powerful and an updated version of Porter\u2019s Stemmer.\n\nhttps:\/\/towardsdatascience.com\/stemming-corpus-with-nltk-7a6a6d02d3e5"}}