{"cell_type":{"bdb20352":"code","a2412c18":"code","c77aa294":"code","f7775000":"code","7cbf67de":"code","aad14344":"code","ff910a40":"code","93745843":"code","dc2ce2a4":"code","994f84b0":"code","55c3b19b":"code","20bc8bcb":"code","d7f9aae3":"code","a1a3543a":"code","c521c882":"code","326f0d6d":"code","994ff035":"code","51561b7b":"code","506cf4fd":"markdown","ebf09f9e":"markdown","56bfa4c6":"markdown","f468ab95":"markdown","ed224b7c":"markdown","29ab0c0c":"markdown","bf9a359b":"markdown","04c18ac2":"markdown","2a834188":"markdown","a1a1d519":"markdown","8bcb7a36":"markdown","c0e9eee6":"markdown","6a8a5477":"markdown","9fcfb26a":"markdown","6a16dd04":"markdown","4474d815":"markdown"},"source":{"bdb20352":"# First, look at everything.\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"..\/input\/customer-support-on-twitter\"]).decode(\"utf8\"))","a2412c18":"import numpy as np\nimport pandas as pd\nimport re\nimport nltk\nimport spacy\nimport string\npd.options.mode.chained_assignment = None\n\nfull_df = pd.read_csv(\"..\/input\/customer-support-on-twitter\/twcs\/twcs.csv\", nrows=5000)\ndf = full_df[[\"text\"]]\ndf[\"text\"] = df[\"text\"].astype(str)\nfull_df.head()","c77aa294":"df[\"text_lower\"] = df[\"text\"].str.lower()\ndf.head()","f7775000":"# drop the new column created in last cell\ndf.drop([\"text_lower\"], axis=1, inplace=True)\n\nPUNCT_TO_REMOVE = string.punctuation\ndef remove_punctuation(text):\n    \"\"\"custom function to remove the punctuation\"\"\"\n    return text.translate(str.maketrans('', '', PUNCT_TO_REMOVE))\n\ndf[\"text_wo_punct\"] = df[\"text\"].apply(lambda text: remove_punctuation(text))\ndf.head()","7cbf67de":"from nltk.corpus import stopwords\n\", \".join(stopwords.words('english'))","aad14344":"STOPWORDS = set(stopwords.words('english'))\ndef remove_stopwords(text):\n    \"\"\"custom function to remove the stopwords\"\"\"\n    return \" \".join([word for word in str(text).split() if word not in STOPWORDS])\n\ndf[\"text_wo_stop\"] = df[\"text_wo_punct\"].apply(lambda text: remove_stopwords(text))\ndf.head()","ff910a40":"from collections import Counter\ncnt = Counter()\nfor text in df[\"text_wo_stop\"].values:\n    for word in text.split():\n        cnt[word] += 1\n        \ncnt.most_common(10)","93745843":"FREQWORDS = set([w for (w, wc) in cnt.most_common(10)])\ndef remove_freqwords(text):\n    \"\"\"custom function to remove the frequent words\"\"\"\n    return \" \".join([word for word in str(text).split() if word not in FREQWORDS])\n\ndf[\"text_wo_stopfreq\"] = df[\"text_wo_stop\"].apply(lambda text: remove_freqwords(text))\ndf.head()","dc2ce2a4":"n_rare_words = 10\nRAREWORDS = set([w for (w, wc) in cnt.most_common()[:-n_rare_words-1:-1]])\nprint(RAREWORDS)","994f84b0":"# Drop the two columns which are no more needed \ndf.drop([\"text_wo_punct\", \"text_wo_stop\"], axis=1, inplace=True)\n\nn_rare_words = 10\nRAREWORDS = set([w for (w, wc) in cnt.most_common()[:-n_rare_words-1:-1]])\ndef remove_rarewords(text):\n    \"\"\"custom function to remove the rare words\"\"\"\n    return \" \".join([word for word in str(text).split() if word not in RAREWORDS])\n\ndf[\"text_wo_stopfreqrare\"] = df[\"text_wo_stopfreq\"].apply(lambda text: remove_rarewords(text))\ndf.head()","55c3b19b":"from nltk.stem.porter import PorterStemmer\n\n# Drop the two columns \ndf.drop([\"text_wo_stopfreq\", \"text_wo_stopfreqrare\"], axis=1, inplace=True) \n\nstemmer = PorterStemmer()\ndef stem_words(text):\n    return \" \".join([stemmer.stem(word) for word in text.split()])\n\ndf[\"text_stemmed\"] = df[\"text\"].apply(lambda text: stem_words(text))\ndf.head()","20bc8bcb":"from nltk.stem.snowball import SnowballStemmer\nSnowballStemmer.languages","d7f9aae3":"from nltk.stem import WordNetLemmatizer\n\nlemmatizer = WordNetLemmatizer()\ndef lemmatize_words(text):\n    return \" \".join([lemmatizer.lemmatize(word) for word in text.split()])\n\ndf[\"text_lemmatized\"] = df[\"text\"].apply(lambda text: lemmatize_words(text))\ndf.head()","a1a3543a":"lemmatizer.lemmatize(\"running\")","c521c882":"lemmatizer.lemmatize(\"running\", \"v\") # v for verb","326f0d6d":"print(\"Word is : stripes\")\nprint(\"Lemma result for verb : \",lemmatizer.lemmatize(\"stripes\", 'v'))\nprint(\"Lemma result for noun : \",lemmatizer.lemmatize(\"stripes\", 'n'))","994ff035":"wordnet_map = {\"N\":wordnet.NOUN, \"V\":wordnet.VERB, \"J\":wordnet.ADJ, \"R\":wordnet.ADV}\nprint(wordnet_map.get(wordnet.NOUN))","51561b7b":"from nltk.corpus import wordnet\nfrom nltk.stem import WordNetLemmatizer\n\nlemmatizer = WordNetLemmatizer()\nwordnet_map = {\"N\":wordnet.NOUN, \"V\":wordnet.VERB, \"J\":wordnet.ADJ, \"R\":wordnet.ADV}\ndef lemmatize_words(text):\n    pos_tagged_text = nltk.pos_tag(text.split())\n    return \" \".join([lemmatizer.lemmatize(word, wordnet_map.get(pos[0], wordnet.NOUN)) for word, pos in pos_tagged_text])\n\ndf[\"text_lemmatized\"] = df[\"text\"].apply(lambda text: lemmatize_words(text))\ndf.head()","506cf4fd":"Now let us redo the lemmatization process for our dataset.","ebf09f9e":"## Removal of Frequent words\n\nIn the previos preprocessing step, we removed the stopwords based on language information. But say, if we have a domain specific corpus, we might also have some frequent words which are of not so much importance to us. \n\nSo this step is to remove the frequent words in the given corpus. If we use something like tfidf, this is automatically taken care of.  \n\nLet us get the most common words adn then remove them in the next step","56bfa4c6":"We can see that words like `private` and `propose` have their `e` at the end chopped off due to stemming. This is not intented. What can we do fort hat? We can use Lemmatization in such cases.\n\nAlso this porter stemmer is for English language. If we are working with other languages, we can use snowball stemmer. The supported languages for snowball stemmer are","f468ab95":"Now we are getting the root form `run`. So we also need to provide the POS tag of the word along with the word for lemmatizer in nltk. Depending on the POS, the lemmatizer may return different results.\n\nLet us take the example, `stripes` and check the lemma when it is both verb and noun.","ed224b7c":"## Lemmatization\n\nLemmatization is similar to stemming in reducing inflected words to their word stem but differs in the way that it makes sure the root word (also called as lemma) belongs to the language. \n\nAs a result, this one is generally slower than stemming process. So depending on the speed requirement, we can choose to use either stemming or lemmatization. \n\nLet us use the `WordNetLemmatizer` in nltk to lemmatize our sentences","29ab0c0c":"## Removal of Punctuations\n\nOne another common text preprocessing technique is to remove the punctuations from the text data. This is again a text standardization process that will help to treat 'hurray' and 'hurray!' in the same way.\n\nWe also need to carefully choose the list of punctuations to exclude depending on the use case. For example, the `string.punctuation` in python contains the following punctuation symbols \n\n`!\"#$%&\\'()*+,-.\/:;<=>?@[\\\\]^_`{|}~`\n\nWe can add or remove more punctuations as per our need.","bf9a359b":"We can see that the trailing `e` in the `propose` and `private` is retained when we use lemmatization unlike stemming. \n\nWait. There is one more thing in lemmatization. Let us try to lemmatize `running` now.","04c18ac2":"Similarly we can also get the list for other languages as well and use them. ","2a834188":"**More to come. Stay tuned!**","a1a1d519":"We can now see that in the third row, `sent` got converted to `send` since we provided the POS tag for lemmatization.","8bcb7a36":"Wow. It returned `running` as such without converting it to the root form `run`. This is because the lemmatization process depends on the POS tag to come up with the correct lemma. Now let us lemmatize again by providing the POS tag for the word.","c0e9eee6":"## Introduction\n\nIn any machine learning task, cleaning or preprocessing the data is as important as model building if not more. And when it comes to unstructured data like text, this process is even more important. \n\nObjective of this kernel is to understand the various text preprocessing steps with code examples. \n\nSome of the common text preprocessing \/ cleaning steps are:\n* Lower casing\n* Removal of Punctuations\n* Removal of Stopwords\n* Removal of Frequent words\n* Removal of Rare words\n* Spelling correction\n* Stemming\n* Lemmatization\n* Removal of emoticons\n* Removal of URLs \n* Removal of HTML tags\n* Chat word conversion","6a8a5477":"## Removal of Rare words\n\nThis is very similar to previous preprocessing step but we will remove the rare words from the corpus. ","9fcfb26a":"## Removal of stopwords\n\nStopwords are commonly occuring words in a language like 'the', 'a' and so on. They can be removed from the text most of the times, as they don't provide valuable information for downstream analysis. In cases like Part of Speech tagging, we should not remove them as provide very valuable information about the POS.\n\nThese stopword lists are already compiled for different languages and we can safely use them. For example, the stopword list for english language from the nltk package can be seen below.\n","6a16dd04":"We can combine all the list of words (stopwords, frequent words and rare words) and create a single list to remove them at once.\n\n## Stemming\n\nStemming is the process of reducing inflected (or sometimes derived) words to their word stem, base or root form (From [Wikipedia](https:\/\/en.wikipedia.org\/wiki\/Stemming))\n\nFor example, if there are two words in the corpus `walks` and `walking`, then stemming will stem the suffix to make them `walk`. But say in another example, we have two words `console` and `consoling`, the stemmer will remove the suffix and make them `consol` which is not a proper english word.\n\nThere are several type of stemming algorithms available and one of the famous one is porter stemmer which is widely used. We can use nltk package for the same.","4474d815":"## Lower Casing\n\nLower casing is a common text preprocessing technique. The idea is to convert the input text into same casing format so that 'text', 'Text' and 'TEXT' are treated the same way. \n\nThis is more helpful for text featurization techniques like frequency, tfidf as it helps to combine the same words together thereby reducing the duplication and get correct counts \/ tfidf values.\n\nThis may not be helpful when we do tasks like Part of Speech tagging (where proper casing gives some information about Nouns and so on) and Sentiment Analysis (where upper casing refers to anger and so on)\n\nBy default, lower casing is done my most of the modern day vecotirzers and tokenizers like [sklearn TfidfVectorizer](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.feature_extraction.text.TfidfVectorizer.html) and [Keras Tokenizer](https:\/\/keras.io\/preprocessing\/text\/). So we need to set them to false as needed depending on our use case. "}}