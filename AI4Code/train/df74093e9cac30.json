{"cell_type":{"1547e3fa":"code","0c3e3f36":"code","3d086129":"code","386a9449":"code","448bd077":"code","d400c9ee":"code","b07270d1":"code","07be4c95":"code","cb8f1c55":"code","4cc61802":"code","bf8f4d14":"code","d129c452":"code","48832479":"code","88c8c25d":"code","d9ab8752":"code","88e26c5d":"code","7b16ab33":"code","fa2259b2":"code","35fa6508":"code","b1b668b0":"code","f4505c70":"code","f9c2e3d6":"code","42b68a1a":"code","1235e6ab":"code","40734c35":"code","9a163716":"code","da1a3cb5":"code","cdc32fa8":"code","28522883":"code","baed0d6b":"code","a38e2746":"code","64b0be16":"code","b3fa05cc":"code","c191a263":"code","0d79a3a1":"code","22f92aed":"code","97331995":"code","11df2e55":"code","da7da41e":"code","a4841efb":"code","90889c64":"code","df6b511d":"code","3b69cefa":"code","bef9bc97":"code","85cd6bb3":"code","0a90169b":"code","2b22f7c1":"code","eee05325":"code","d2cdf0da":"code","486ceafd":"code","16699506":"code","a1061913":"code","74ccf603":"code","855bbb0b":"code","324174c4":"code","1abe73aa":"code","1dff2fc2":"code","ea641f4c":"code","74a49183":"code","7fafb4a9":"code","9e51bd0b":"code","2c694d91":"code","bc61e80f":"code","a5a1ecba":"code","1b6cffdf":"code","7a35341e":"code","410b6dd0":"code","1ed00379":"code","d31d04f7":"code","a1b9f9a4":"code","18348f09":"code","deea85d0":"code","a8eae6f4":"code","3ff2ec9f":"code","ed45d503":"code","fd28d13e":"code","2bccb8b8":"code","b74f75e4":"code","1d0083bb":"code","136aae02":"code","dc1de651":"code","5aca06ac":"code","3e183ecf":"code","680de244":"code","cdb61868":"code","9c5bad96":"code","1ab3e3f8":"code","7c9639e5":"code","f54aa1d1":"code","357eb5ed":"code","64703f0b":"code","b409af29":"code","87866528":"code","64dabb07":"code","c92e20b0":"code","6728be80":"code","bcf8e849":"code","0dbc1ea2":"code","c0954b2b":"code","c799fc0b":"code","a97aa725":"code","a61483ae":"code","7c9c5dd1":"code","a7d1f9eb":"code","e6aad01a":"code","a993d509":"code","9b706186":"code","40022a91":"code","fec4fd84":"code","853a749c":"code","cb780f5b":"code","edf98bc2":"code","c40439bc":"code","94a4b10e":"code","99c8214b":"code","15bf864c":"code","60bc21fe":"code","f0358361":"code","3019889e":"code","97629b3b":"code","1924ddbb":"code","d9d1475b":"code","a712b3a1":"code","dcf90973":"code","ff543cdc":"code","75f84687":"code","de9976fb":"code","9453465b":"markdown","7c1ba0ed":"markdown","5013798c":"markdown","fea255f7":"markdown","3b369542":"markdown","354f63ba":"markdown","c5cc35fc":"markdown","bf8e5167":"markdown","5a0df370":"markdown","1427133d":"markdown","6f0edc88":"markdown","b392b685":"markdown","55840836":"markdown","d171bd1e":"markdown","ba412d43":"markdown","9fa7e685":"markdown","6989578e":"markdown","3df2dff0":"markdown","7a7ddf6d":"markdown","e97a3801":"markdown","3afd3837":"markdown","05a60a45":"markdown","1af2f018":"markdown","bcb70b6e":"markdown","1adde7ca":"markdown","c0c929f3":"markdown","02b78c93":"markdown","0a4a3c1c":"markdown","c600c344":"markdown","08c7ab1d":"markdown","7d36e8fb":"markdown","63c12839":"markdown","00e8fbf9":"markdown","5509b184":"markdown","ed7cefbf":"markdown","91a0a813":"markdown","83256f77":"markdown","6ce98db9":"markdown","4e00c341":"markdown","977be345":"markdown","b547b513":"markdown","93ac4f8d":"markdown","efa8ca27":"markdown","07d17520":"markdown","f43f7331":"markdown","4c2c613d":"markdown","8227fa04":"markdown","573042c2":"markdown","7e5655ee":"markdown","ee4c8235":"markdown","02d729a7":"markdown","cca8f260":"markdown","9ce88913":"markdown","f406a7d5":"markdown","dfe37b4e":"markdown","783dd633":"markdown","50bb4394":"markdown","666558c0":"markdown","3eef8063":"markdown","a222fdc0":"markdown","c469151d":"markdown","aef97b48":"markdown","e90cbf3d":"markdown","cf14f36f":"markdown","baa08560":"markdown","7b347836":"markdown","a6081736":"markdown","08267afd":"markdown","b6d5ca13":"markdown","bd4533ff":"markdown","38ab2e00":"markdown","a20a4261":"markdown","9e3185f1":"markdown","6e537f17":"markdown","facc1418":"markdown","be35c1ae":"markdown","82bca1a9":"markdown","ad8980ab":"markdown","be016c6d":"markdown","9ff9f82c":"markdown"},"source":{"1547e3fa":"# pandas will be required to work with Data Frame\nimport pandas as pd    \n\n#numpy will be required for mathematical functions\nimport numpy as np     \n\n#os gives access to some functions of the operating system \nimport os\n\n#matplotlib and seaborn will be required for data visualization\nimport matplotlib.pyplot as plt \nimport seaborn as sns   \n\n# train_test_split will be requiered to perform train test split\nfrom sklearn.model_selection import train_test_split  \n\n##Labels data that need to be represented in a matrix map with 0 and 1\nfrom sklearn.preprocessing import OneHotEncoder \n\n#allows you to selectively apply data preparation transforms\nfrom sklearn.compose import ColumnTransformer \n\n# import PolynomialFeatures to creat polynomial features \nfrom sklearn.preprocessing import PolynomialFeatures\n\n# import pickel to pickle (\"safe\") diffrent transformers an models\nimport pickle\n\n# import different samplers to sample the imbalance Data\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom imblearn.over_sampling import RandomOverSampler\nfrom imblearn.over_sampling import SMOTE\n\n# imblearn Pipline will be required if you want to include samplers \n# in the pipeline. sklearn.Pipeline does not allow sampler\nfrom imblearn.pipeline import make_pipeline\n\n# Pipeline is requirede to chained together differen Data Transforms\nfrom sklearn.pipeline import Pipeline\n\n# PCA is a linear dimensionality reductiontechnique that can be \n# utilized for extracting information from a high-dimensional\nfrom sklearn.decomposition import PCA\n\n# GridSearchCV is used for Hyperparameter tuning. \n# Given a set of different hyperparameters, GridSearchCV loops through all possible values\nfrom sklearn.model_selection import GridSearchCV\n\n# import diffrent scaler,as a preprocessing step before many machine learning models, \nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\n\n# import diffrent metrics as a useful measure of the success of prediction\nfrom sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score,roc_auc_score\n\n# cross_val_score evaluate a score by cross-validation\nfrom sklearn.model_selection import cross_val_score\n\n#Validation curve determine training and test scores for varying parameter values\nfrom sklearn.model_selection import validation_curve\n\n# Import machine learning algorithms for classification\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC \n\n# Import Libaries to build a Neural Network & Make Predictions\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.callbacks import EarlyStopping \nfrom tensorflow.keras.layers import Dropout\n\n#import VotingClassifier to creat ensemble classifier\nfrom sklearn.ensemble import VotingClassifier\n\n# import a dummy model for sanity check \nfrom sklearn.dummy import DummyClassifier\n\n#import pdp to create a partial dependence plot\nfrom pdpbox import pdp\n","0c3e3f36":"#list all files under the input directory\n\nfor dirname, _, filenames in os.walk('\/kaggle'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","3d086129":"#load Data\ndf = pd.read_csv('\/kaggle\/input\/carvana\/training_car.csv')\n# Displaying the fist 5 lines of the dataset\ndf.head() ","386a9449":"#distrubition of target \ndistrib_target = pd.crosstab(index=df.loc[:,\"IsBadBuy\"],\n                             columns=\"count\", \n                             normalize=\"columns\")\ndistrib_target","448bd077":"# preparation for plot\ndistrib_target.index= [\"No\",\"Yes\"] # label 0 and 1 with no and yes\ndistrib_target.loc[:,\"count\"] = distrib_target.loc[:,\"count\"] *100 # convert to percent\n\n# use Style\nplt.style.use('ggplot')\n\n# Plot the distribution\ndistrib_target.plot(kind=\"bar\", \n                   title=\"12,30 % of vehicles are a BadBuy\",\n                   linewidth=2, \n                   edgecolor='#000000',\n                   figsize=(8,6),\n                   legend=[]\n                   );\n# set ticks and labels \nplt.xticks(fontsize=16,rotation=0);\nplt.yticks(fontsize=16);\nplt.ylabel('Percent', fontsize=16);\n","d400c9ee":"#Let's look at the dimension of the Data \nprint(\"Data contain \",df.shape[0],\" Vehicles and \", df.shape[1], \" columns.\")","b07270d1":"df.dtypes","07be4c95":"df.head()","cb8f1c55":"#print sum and values of categorical DAta\n\n#define categorical columns\ncolumns_cat = [ 'Auction',  'Make', 'Model',\n                'Trim', 'Color','Transmission', \n                'WheelTypeID','WheelType','Nationality', \n                'Size','TopThreeAmericanName','PRIMEUNIT', \n                'AUCGUART','BYRNO', 'VNZIP1', 'VNST',\n                'IsOnlineSale']\n\n#print\nfor i in columns_cat:\n    print(i)\n    print(\"----\")\n    print('unique: ', df.loc[:,i].unique())\n    print('unique sum: ', len(df.loc[:,i].unique()))\n    print(\"-------------------------------------------------------------------------------\")\n  #  print(\"\\n\")\n\n#print\nprint(\"SubModel\")\nprint(\"-------\")\nprint('unique sum: ', len(df.loc[:,\"SubModel\"].unique()))","4cc61802":"df.WheelType.value_counts(dropna=False)","bf8f4d14":"df.WheelTypeID.value_counts(dropna=False)","d129c452":"# look into the distribution of \"Make\" divided according to \"IsBadBuy\"\n\n#crosstab\ndistrib_make = pd.crosstab(index=df.loc[:, 'Make'], \n                columns=df.loc[:,'IsBadBuy']).sort_values(\n                by=[1], ascending=False)\n\n# add columns\ndistrib_make.loc[:,\"0_percentage\"] = distrib_make.loc[:,0]\/(distrib_make.loc[:,0] + distrib_make.loc[:,1])\ndistrib_make.loc[:,\"1_percentage\"] = distrib_make.loc[:,1]\/(distrib_make.loc[:,0] + distrib_make.loc[:,1])\ndistrib_make.loc[:,\"sum\"] =(distrib_make.loc[:,0] + distrib_make.loc[:,1])\n\n#sort\ndistrib_make.sort_values(\n                by=[\"1_percentage\"], ascending=False)","48832479":"# the 10 most purchased vehicle in the dataset and the share of BadBuy\n\n#creat Corsstab\ndistrib_make = pd.crosstab(index=df.loc[:, 'Make'], \n                columns=df.loc[:,'IsBadBuy'])\n\n#creat column count\ndistrib_make.loc[:,\"count\"] = pd.crosstab(index=df.loc[:, 'Make'], \n                columns=\"count\")\n\n#sort\ndistrib_make = distrib_make.sort_values(\n             by=[\"count\"], ascending=False).head(10)\n\n#rename columns\ndistrib_make.columns = [\"no\", \"yes\", 'count']\n\n#import matplotlib\nimport matplotlib.pyplot as plt\n#initiate fig, ax\nfig, ax = plt.subplots(figsize=(10,6))\n\n#plot\ndistrib_make.iloc[:,0:2].plot(kind='bar',\n                                ax=ax, #stacked=True,\n                                linewidth=1, \n                                edgecolor='#000000'\n                                     );\n#set Title\nax.set_title(label=\"Top 10 purchased Vehicle and the share of BadBuy\",\n             family=\"serif\",\n             weight=\"semibold\",\n             size=14);\n\n#xlabel\nax.set_xlabel(xlabel=\"Make\",\n             size=12,\n             weight=\"semibold\");\n\n#set legend\nax.legend(title=\"IsBadBuy\",fontsize=10, facecolor=\"white\")\n\n#set ticks\nplt.xticks(fontsize=12,rotation=45);\nplt.yticks(fontsize=12);\n\n\n","88c8c25d":"#Distribution by nationality and share of BadBuy\nnationality_isbadbuy = pd.crosstab(index=df.loc[:, 'Nationality'], \n                columns=df.loc[:,'IsBadBuy']).sort_values(\n                by=[1], ascending=False)\n\n#rename Columns\nnationality_isbadbuy.columns = [\"no\", \"yes\"]\n\n# initiate fig and ax\nfig, ax = plt.subplots(figsize=(10,6))\n\n#plot\nnationality_isbadbuy.iloc[:,0:2].plot(kind='bar',\n                                      ax=ax, \n                                      #stacked=True,\n                                      linewidth=1, \n                                      edgecolor='#000000'\n                                     );\n#set title\nax.set_title(label=\"Distribution by nationality and share of BadBuy\",\n             family=\"serif\",\n             weight=\"semibold\",\n             size=14);\n#xlabel\nax.set_xlabel(xlabel=\"Nationality\",\n             size=13,\n             weight=\"semibold\");\n#set legend\nax.legend(title=\"IsBadBuy\",fontsize=10, facecolor=\"white\")\n\n#set ticks\nplt.xticks(fontsize=13,rotation=0);\nplt.yticks(fontsize=12);\n\n","d9ab8752":"# normalized distiribution of Nationality\npd.crosstab(index=df.loc[:, 'Nationality'], \n                columns=df.loc[:,'IsBadBuy'], normalize=\"index\").sort_values(\n                by=[1], ascending=False)","88e26c5d":"#Normalized percentage of BadBuy by nationality\nnationality_isbadbuy = pd.crosstab(index=df.loc[:, 'Nationality'], \n                columns=df.loc[:,'IsBadBuy'], normalize=\"index\").sort_values(\n                by=[1], ascending=False)\n\n#rename Columns\nnationality_isbadbuy.columns = [\"no\", \"yes\"]\n\n# initiate fig and ax\nfig, ax = plt.subplots(figsize=(10,6))\n\n#plot\nnationality_isbadbuy.iloc[:,1].plot(kind='bar',\n                                      ax=ax, \n                                      stacked=True,\n                                      linewidth=1, \n                                      edgecolor='#000000'\n                                     );\n#set title\nax.set_title(label=\"Percentage of BadBuy within own Group\",\n             family=\"serif\",\n             weight=\"semibold\",\n             size=14);\n\n#xlabel\nax.set_xlabel(xlabel=\"Nationality\",\n             size=13,\n             weight=\"semibold\");\n\n#ylabel\nax.set_ylabel(ylabel=\"Number of sold vehicles\",\n             size=14,\n             weight=\"semibold\");\n\n#set ticks\nplt.xticks(fontsize=13,rotation=0);\nplt.yticks(fontsize=12);\n","7b16ab33":"# normalized distiribution of Auction\npd.crosstab(index=df.loc[:, 'Auction'], \n                columns=df.loc[:,'IsBadBuy'], normalize=\"index\").sort_values(\n                by=[1], ascending=False)\n# Manheim seem do be the best Place for Auction ","fa2259b2":"pd.crosstab(index=df.loc[:, 'VNST'], \n                columns=df.loc[:,'IsBadBuy']).sort_values(\n                by=[1], ascending=False).plot(kind=\"bar\", figsize=(16,6))\n\n#set legend\nplt.legend(title=\"IsBadBuy\",fontsize=8, facecolor=\"white\");","35fa6508":"# look into the distribution of \"VNST\" divided according to \"IsBadBuy\"\n\n#crosstab\ndistrib_make = pd.crosstab(index=df.loc[:, 'VNST'], \n                columns=df.loc[:,'IsBadBuy']).sort_values(\n                by=[1], ascending=False)\n\n# add columns\ndistrib_make.loc[:,\"0_percentage\"] = distrib_make.loc[:,0]\/(distrib_make.loc[:,0] + distrib_make.loc[:,1])\ndistrib_make.loc[:,\"1_percentage\"] = distrib_make.loc[:,1]\/(distrib_make.loc[:,0] + distrib_make.loc[:,1])\ndistrib_make.loc[:,\"sum\"] =(distrib_make.loc[:,0] + distrib_make.loc[:,1])\n\n#sort\ndistrib_make.sort_values(\n                by=[\"1_percentage\"], ascending=False).head(20)","b1b668b0":"#create Data with Crosstab\nvehicle_age = pd.crosstab(index=df.loc[:,\"VehicleAge\"], columns=df.loc[:,\"IsBadBuy\"])\n\n#rename Columns\nvehicle_age.columns = [\"no\", \"yes\"]\n\n#initiate fig, ax\nfig, ax = plt.subplots(figsize=(10,6))\n\n# plot\nvehicle_age.iloc[:,0:2].plot(kind='bar',\n                                      ax=ax, \n                                      linewidth=1, \n                                      edgecolor='#000000'\n                                     );\n#set Title \nax.set_title(label=\"Vehicle Age and ratio of BadBuy\",\n             family=\"serif\",\n             weight=\"semibold\",\n             size=14);\n\n#xlabel\nax.set_xlabel(xlabel=\"VehicleAge\",\n             size=13,\n             weight=\"semibold\");\n\n#ylabel\nax.set_ylabel(ylabel=\"Number of sold vehicles\",\n             size=14,\n             weight=\"semibold\");\n\n#set legend\nax.legend(title=\"IsBadBuy\",fontsize=10)\n\n#set legend\nax.legend(title=\"IsBadBuy\",fontsize=10, facecolor=\"white\")\n\n#set ticks\nplt.xticks(fontsize=13,rotation=0);\nplt.yticks(fontsize=12);","f4505c70":"# normalized distiribution of Nationality\npd.crosstab(index=df.loc[:, 'VehicleAge'], \n                columns=df.loc[:,'IsBadBuy'], normalize=\"index\").sort_values(\n                by=[1], ascending=False)","f9c2e3d6":"vehicleage_isbadBuy = pd.crosstab(index=df.loc[:, 'VehicleAge'], \n                    columns=df.loc[:,'IsBadBuy'], normalize=\"index\").sort_values(\n                    by=[1], ascending=False)\n\n#rename Columns\nvehicleage_isbadBuy.columns = [\"no\", \"yes\"]\n\n# initiate fig and ax\nfig, ax = plt.subplots(figsize=(10,6))\n\n#plot\nvehicleage_isbadBuy.iloc[:,1].plot(kind='bar',\n                                      ax=ax, \n                                      stacked=True,\n                                      linewidth=1, \n                                      edgecolor='#000000'\n                                     );\n#set title\nax.set_title(label=\"Percentage of BadBuy within the own Group\",\n             family=\"serif\",\n             weight=\"semibold\",\n             size=14);\n\n#xlabel\nax.set_xlabel(xlabel=\"VehicleAge\",\n             size=13,\n             weight=\"semibold\");\n\n#set ticks\nplt.xticks(fontsize=13,rotation=0);\nplt.yticks(fontsize=12);\n","42b68a1a":"# transform to datetime\ndf.loc[:, 'PurchDate'] = pd.to_datetime(df.loc[:, 'PurchDate'])\ndisplay(df.loc[:,[\"VehYear\",\"VehicleAge\",\"PurchDate\"]].head(5))","1235e6ab":"#check NaNs and duplicates\nprint('is NaN')\nprint(\"---------------------\")\nprint(df.isna().sum())\nprint(\"---------------------\")\nprint()\nprint('Sum of Duplicates', df.duplicated().sum())","40734c35":"# PRIMEUNIT has many NaNs, check distribution\ndisplay(pd.crosstab(index=df.loc[:, 'PRIMEUNIT'], \n                columns=df.loc[:,'IsBadBuy'], normalize=\"index\").sort_values(\n                by=[1], ascending=False))\n\npd.crosstab(index=df.loc[:, 'PRIMEUNIT'], \n                columns=df.loc[:,'IsBadBuy'], dropna=False).sort_values(\n                by=[1], ascending=False)","9a163716":"# AUCGUART has many NaNs, check distribution\npd.crosstab(index=df.loc[:, 'AUCGUART'], \n                columns=df.loc[:,'IsBadBuy'], normalize=\"index\",dropna=\"False\").sort_values(\n                by=[1], ascending=False)","da1a3cb5":"#check correlations\nplt.figure(figsize=(14, 10))\nmask_matrix=np.triu(df.corr())\nsns.heatmap(df.corr(), annot=True, cmap=plt.cm.Reds,mask=mask_matrix);","cdc32fa8":"#check for outiers \ndf.loc[:,\"VehOdo\"].hist();","28522883":"print(df.loc[:,\"VehBCost\"].median())\nprint(df.loc[:,\"VehBCost\"].mean())\ndf.loc[:,\"VehBCost\"].hist();","baed0d6b":"#Chekc Vehicle <1000\n#mask\nmask = df.loc[:,\"VehBCost\"] <100\n#sum\nprint(mask.sum())\n#histogramm\ndf.loc[mask,\"VehBCost\"].hist();","a38e2746":"#scatterplot to look out for outlier\n\n#initiate fig, ax\nfig, ax = plt.subplots(2,2, figsize=(16,8))\n\n#scatterplot of the MMRA... Columns\nsns.scatterplot(x='MMRAcquisitionAuctionAveragePrice', \n                y='MMRAcquisitionRetailAveragePrice', data=df,  hue='IsBadBuy', ax=ax[0,0]);\nsns.scatterplot(x='MMRAcquisitionAuctionCleanPrice',\n                y='MMRAcquisitonRetailCleanPrice', data=df,  hue='IsBadBuy', ax=ax[0,1]);\nsns.scatterplot(x='MMRCurrentAuctionCleanPrice', \n                y='MMRCurrentRetailCleanPrice', data=df,  hue='IsBadBuy', ax=ax[1,0]);\nsns.scatterplot(x='MMRCurrentAuctionAveragePrice', \n                y='MMRCurrentRetailAveragePrice', data=df,  hue='IsBadBuy', ax=ax[1,1]);","64b0be16":"sns.scatterplot(x='VehBCost', y='VehOdo', data=df,  hue='IsBadBuy');","b3fa05cc":"#read in the dataframe again to remove previous changes\ndf = pd.read_csv('\/kaggle\/input\/carvana\/training_car.csv')\n\n#create target and feature \ntarget = df.loc[:,'IsBadBuy']\nfeatures = df.drop('IsBadBuy', axis=1)\n\n#train_test split\nfeatures_train, features_test, target_train, target_test = train_test_split(features, \n                                                                            target, \n                                                                            random_state=42,\n                                                                            test_size=0.15)\n#validation splitt\nfeatures_train, features_val, target_train, target_val = train_test_split(features_train, \n                                                                            target_train, \n                                                                            random_state=42,\n                                                                            test_size=0.2)","c191a263":"# save features_test as 'features_test.csv'\nfeatures_test.to_csv('\/kaggle\/working\/features_test.csv', index=False)","0d79a3a1":"#copy features_train\ndf = features_train.copy()","22f92aed":"#drop columns\ndf.drop(labels=['VehYear','WheelTypeID','VNST','BYRNO','VNZIP1','Model','SubModel','Trim', 'RefId'],\n        axis=1, \n        inplace=True)","97331995":"#print dictionary of mean to copy\ndict(df.mean())","11df2e55":"# Imputation for Numerical Values\n\nnumerical_values={'VehicleAge': 4.173608994547733,\n                  'VehOdo': 71487.7689051441,\n                  'MMRAcquisitionAuctionAveragePrice': 6125.751367647307,\n                  'MMRAcquisitionAuctionCleanPrice': 7369.29918872686,\n                  'MMRAcquisitionRetailAveragePrice': 8496.957861219787,\n                  'MMRAcquisitonRetailCleanPrice': 9850.127043002558,\n                  'MMRCurrentAuctionAveragePrice': 6132.1143425684995,\n                  'MMRCurrentAuctionCleanPrice': 7390.427419766315,\n                  'MMRCurrentRetailAveragePrice': 8776.13805126112,\n                  'MMRCurrentRetailCleanPrice': 10145.221201761995,\n                  'VehBCost': 6724.925161421526,\n                  'WarrantyCost': 1276.7419485929086}\n\n\ndf = df.fillna(value=numerical_values)","da7da41e":"#replace NaNs in Category Data with Unknown\ncategorical_values = {'Auction': 'Unknown',\n                     'Make': 'Unknown',\n                     'Model': 'PT Unknown',\n                     'Trim': 'Unknown',\n                     'SubModel': '4D Unknown',\n                     'Color': 'Unknown',\n                     'Transmission': 'Unknown',\n                     'WheelType': 'Unknown',\n                     'Nationality': 'Unknown',\n                     'Size': 'Unknown',\n                     'TopThreeAmericanName': 'Unknown',\n                     'PRIMEUNIT': 'Unknown',\n                     'AUCGUART': 'Unknown',\n                     'IsOnlineSale': 'Unknown'}\ndf.fillna(value=categorical_values,inplace=True)","a4841efb":"#check NaNs \ndf.isna().sum()","90889c64":"#change datetime\ndf.loc[:, 'PurchDate'] = pd.to_datetime(df.loc[:, 'PurchDate'])","df6b511d":"# change object Type to category\ncolumns_cat = [ 'Auction',  'Make',\n                'Color','Transmission', \n                'WheelType','Nationality', 'Size', \n                'TopThreeAmericanName','PRIMEUNIT', \n                'AUCGUART','IsOnlineSale']\n\nfor i in columns_cat:\n    df[i] = df[i].astype(\"category\")","3b69cefa":"#check types\ndf.dtypes","bef9bc97":"# replace MANUAL with Manual\ndf.Transmission.replace(to_replace='Manual', value = \"MANUAL\", inplace = True)","85cd6bb3":"# clean_data function\n# L\u00f6sung:\ndef clean_data(df):\n    \"\"\"Returns cleaned DataFrame.\n    \n    Transform datatypes:\n        -transform 'pickup_datetime' to datetime format\n        -transform object to category \n        -fill numericals NaNs with median \n        -fill categorical NaNs with Unknown\n    Args: \n        df (pd.DataFrame) : uncleaned DataFrame\n        \n    Returns:\n        df  (pd.DataFrame) : cleaned DataFrame\n    \n    \"\"\"\n\n    #drop columns\n    df.drop(['VehYear','WheelTypeID','VNST','BYRNO','VNZIP1','Model','SubModel','Trim','RefId'],axis=1,inplace=True)\n    \n    #to datetime\n    df.loc[:, 'PurchDate'] = pd.to_datetime(df.loc[:, 'PurchDate'])\n   \n    #deal with NaNs\n    # Imputation for Numerical Values\n\n    numerical_values={'VehicleAge': 4.173608994547733,\n                  'VehOdo': 71487.7689051441,\n                  'MMRAcquisitionAuctionAveragePrice': 6125.751367647307,\n                  'MMRAcquisitionAuctionCleanPrice': 7369.29918872686,\n                  'MMRAcquisitionRetailAveragePrice': 8496.957861219787,\n                  'MMRAcquisitonRetailCleanPrice': 9850.127043002558,\n                  'MMRCurrentAuctionAveragePrice': 6132.1143425684995,\n                  'MMRCurrentAuctionCleanPrice': 7390.427419766315,\n                  'MMRCurrentRetailAveragePrice': 8776.13805126112,\n                  'MMRCurrentRetailCleanPrice': 10145.221201761995,\n                  'VehBCost': 6724.925161421526,\n                  'WarrantyCost': 1276.7419485929086}\n\n    df = df.fillna(value=numerical_values)\n    \n        \n    # Imputation for Categorical Values \n    categorical_values = {'Auction': 'Unknown',\n                         'Make': 'Unknown',\n                         'Color': 'Unknown',\n                         'Transmission': 'Unknown',\n                         'WheelType': 'Unknown',\n                         'Nationality': 'Unknown',\n                         'Size': 'Unknown',\n                         'TopThreeAmericanName': 'Unknown',\n                         'PRIMEUNIT': 'Unknown',\n                         'AUCGUART': 'Unknown',\n                         'IsOnlineSale': 'Unknown'}\n    df.fillna(value=categorical_values,inplace=True)\n    \n   \n    #to category\n    columns_cat = [ 'Auction',  'Make', 'Color', \n            'Transmission', 'WheelType','Nationality', 'Size', \n            'TopThreeAmericanName','PRIMEUNIT', \n            'AUCGUART','IsOnlineSale']\n\n    for i in columns_cat:\n        df[i] = df[i].astype(\"category\")\n    \n    # replace MANUAL with Manual\n    df.Transmission.replace(to_replace='Manual', value = \"MANUAL\", inplace = True)\n    \n    return df","0a90169b":"# This notebook is quite large. So that I do not have to scroll every time \n# in the notebook and to initiate the functions, I save them\n\npickle.dump(clean_data, open('\/kaggle\/working\/clean_data.p', 'wb'))","2b22f7c1":"# crate Mask\nmask = df.loc[:,\"VehBCost\"] == 1\n\n# print sum of values == 1 and the shape of DataSet\nprint(mask.sum())\n\n#print shape\nprint(df.shape)\n\n#remove value\ndf = df.loc[~mask,:]\n\n#print shape\nprint(df.shape)","eee05325":"#sampling data function\ndef sampling_data(features, target):\n    \"\"\"Sample Dataframe.\n    remove VehBCost == 1 \n    \n    Args:\n        features (pd.DataFrame): Dataframe containing outliers\n        target (pd.Series) : Series with target values matching to features\n    \n    Returns:\n        (pd.DataFrame) : Filtered features\n        (pd.Series) : Filtered target\n    \n    \"\"\"\n    #crate Mask\n    mask = features.loc[:,\"VehBCost\"] == 1\n    #delete value \n    features = features.loc[~mask,:]\n    #match target\n    target = target[features.index]\n    \n    return features, target","d2cdf0da":"#save function\npickle.dump(sampling_data, open('\/kaggle\/working\/sampling_data.p', 'wb'))","486ceafd":"#copy data\ndf_train = features_train.copy()\ndf_val = features_val.copy()","16699506":"# use the functions\n# clean_data\ndf_train = clean_data(df_train)\n\n# clean_data feature validation\ndf_val = clean_data(df_val)","a1061913":"#drop PurchDate for test of different Resampler\ndf_train.drop(['PurchDate'],axis=1,inplace=True)\ndf_val.drop(['PurchDate'],axis=1,inplace=True)","74ccf603":"#one hot encoding\n\n# define cat_cols\ncat_cols = [ 'Auction', 'Make',  'Color', \n            'Transmission', 'WheelType','Nationality', 'Size', \n            'TopThreeAmericanName','PRIMEUNIT', \n            'AUCGUART','IsOnlineSale']\n\n#set up pipeline\nohe= OneHotEncoder(sparse=False,handle_unknown='ignore')\ncol_transformer = ColumnTransformer([(\"OHE\", ohe, cat_cols)], remainder=\"passthrough\")\n\n#fit col_transformer\ncol_transformer.fit(df_train)\n\n#restore column names for final Data Frames\nohe_names = col_transformer.named_transformers_['OHE'].get_feature_names(cat_cols)\nremaining_names = col_transformer._df_columns[col_transformer._remainder[2]]\n\ndf_train = pd.DataFrame(col_transformer.transform(df_train), \n                      columns=list(ohe_names)+list(remaining_names), \n                      index=df_train.index)\ndf_val = pd.DataFrame(col_transformer.transform(df_val), \n                      columns=list(ohe_names)+list(remaining_names), \n                      index=df_val.index)","855bbb0b":"from imblearn.pipeline import Pipeline \n\n# initialize\nundersampler = RandomUnderSampler(random_state=42)\noversampler = RandomOverSampler(random_state=42)\nsmotesampler = SMOTE( random_state=42)\n\n# Model for evaluation\ntree_clf = DecisionTreeClassifier(random_state=42)\n\n#creat search_space for Gridsearch\nsearch_space = {'estimator__max_depth': range(2, 16, 2),\n                'estimator__class_weight': [None, 'balanced']}\n\n#Different Samplers,to iterrate through\nsamplers = [('oversampling', oversampler),\n            ('undersampling', undersampler),\n            ('class_weights', 'passthrough'),\n            ('SMOTE', smotesampler)\n           ]\n\n\n# storage container for results\nresults = []\n\n# go through every sampler\nfor name, sampler in samplers:\n    #sampling\n    imb_pipe = Pipeline([#('transformer',col_transformer),\n                         ('sampler', sampler),\n                         ('estimator', tree_clf)\n                        ])\n    \n    #gridsearch and CV\n    grid = GridSearchCV(estimator=imb_pipe, \n                        param_grid=search_space,\n                        n_jobs=-1,\n                        cv=5,\n                        scoring='f1')\n    \n    grid.fit(df_train, target_train)\n    \n    #evaluation\n    model = grid.best_estimator_.named_steps['estimator']\n    target_pred = model.predict(df_val)\n    recall = recall_score(target_val,target_pred )\n    precision = precision_score(target_val,target_pred)\n\n    #save\n    scores = {'name': name,\n              'precision': precision,\n              'recall': recall,\n              'F1':grid.best_score_ ,\n             }\n    results.append(scores)\n    \n#show results\npd.DataFrame(results)","324174c4":"def resample_data_smote(features, target):\n    \"\"\"Using SMOTE for the imbalanced data set.\n        Returns balanced Data Set.\n\n    \n    Args: \n        features (pd.DataFrame) : unbalanced DataFrame\n        target (pd.DataFrame) : unbalanced DataFrame\n        \n    Returns:\n        features (pd.DataFrame) : balanced DataFrame\n        target (pd.DataFrame) : balanced DataFrame\n    \n    \"\"\"\n    from imblearn.over_sampling import SMOTE\n    \n    #initiate Smote\n    smotesampler = SMOTE()\n    #fit and resample\n    features, target = smotesampler.fit_resample(features, target)\n    \n    return features, target","1abe73aa":"#save function\npickle.dump(resample_data_smote, open('\/kaggle\/working\/smote.p', 'wb'))","1dff2fc2":"###Load Data\n#copy \ndf_train = features_train.copy()\ndf_val = features_val.copy()\n\n\n# clean_data\ndf_train = clean_data(df_train)\ndf_val =   clean_data(df_val)\n\n#sample Data\ndf_train, y_train = sampling_data(df_train,target_train)","ea641f4c":"#define cat_cols\ncat_cols = df_train.select_dtypes(include = 'category').columns.tolist()","74a49183":"# define num_cols \nnum_cols = ['VehicleAge','VehOdo','MMRAcquisitionAuctionAveragePrice',\n            'MMRAcquisitionAuctionCleanPrice','MMRAcquisitionRetailAveragePrice',\n            'MMRAcquisitonRetailCleanPrice','MMRCurrentAuctionAveragePrice',\n            'MMRCurrentAuctionCleanPrice','MMRCurrentRetailAveragePrice',\n            'MMRCurrentRetailCleanPrice','VehBCost','WarrantyCost']","7fafb4a9":"### Build Baselinemodel\n\n# instantiate model\nmodel = LogisticRegression(max_iter=1000)\n\n# build pipeline\n# set up pipeline\nohe= OneHotEncoder(sparse=False,handle_unknown='ignore')\ncol_transformer = ColumnTransformer([(\"OHE\", ohe, cat_cols)], remainder=\"passthrough\")\n\n# create Pipeline\nmodel_baseline = Pipeline([(\"col_transformer\", col_transformer),\n                           (\"scaler\",StandardScaler()),\n                           (\"model\", model)])","9e51bd0b":"# fit pipeline on cleaned training set\nmodel_baseline.fit(df_train.loc[:,num_cols+cat_cols], y_train)\n\n# predict and evaluate on test set\n\nmodel_results = []\n\n#predict\ntarget_test_pred = model_baseline.predict(df_val.loc[:,num_cols+cat_cols])\n\n#evaluate\naccuracy = accuracy_score(target_val,target_test_pred)\nrecall = recall_score(target_val,target_test_pred)\nprecision = precision_score(target_val,target_test_pred)\nf1_scr = f1_score(target_val,target_test_pred)\n\n#save\nscores = {'name': \"Baseline_LR\",\n          'accuracy': accuracy,\n          'precision': precision,\n          'recall': recall,\n          'F1': f1_scr,\n             }\nmodel_results.append(scores)\n    \n#show results\npd.DataFrame(model_results).T\n\n\n","2c694d91":"#save results\npickle.dump(model_results, open('\/kaggle\/working\/model_results.p', 'wb'))","bc61e80f":"###Load Data\n#copy \ndf_train = features_train.copy()\ndf_test = features_test.copy()\n\n\n# clean_data\ndf_train = clean_data(df_train)\ndf_test =   clean_data(df_test)\n\n#sample Data\ndf_train, y_train = sampling_data(df_train,target_train)","a5a1ecba":"#Extract informations from \"PurchDate\"\n#year\ndf_train.loc[:,\"PurchYear\"] = df_train.loc[:,\"PurchDate\"].dt.year\n\n#weekday\ndf_train.loc[:,\"PurchWeekday\"] = df_train.loc[:,\"PurchDate\"].dt.weekday\n\n#month\ndf_train.loc[:,\"PurchMonth\"] = df_train.loc[:,\"PurchDate\"].dt.month\n\n#drop PurchDate\ndf_train.drop(['PurchDate'],axis=1,inplace=True)","1b6cffdf":"cat_cols = df_train.select_dtypes(include = 'category').columns.tolist()","7a35341e":"#columnTransformer\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder\n\n# define num_cols and cat_cols\n\ncat_cols =  ['Auction',\n             'Make',\n             'Color',\n             'Transmission',\n             'WheelType',\n             'Nationality',\n             'TopThreeAmericanName',\n             'Size',\n             'PRIMEUNIT',\n             'AUCGUART']\n\n#set up pipeline\nohe= OneHotEncoder(sparse=False,handle_unknown='ignore')\ncol_transformer = ColumnTransformer([(\"OHE\", ohe, cat_cols)], remainder=\"passthrough\")\n\n#fit col_transformer\ncol_transformer.fit(df_train)\n\n#restore column names for final Data Frames\nohe_names = col_transformer.named_transformers_['OHE'].get_feature_names(cat_cols)\nremaining_names = col_transformer._df_columns[col_transformer._remainder[2]]\n\ndf_train = pd.DataFrame(col_transformer.transform(df_train), \n                      columns=list(ohe_names)+list(remaining_names), \n                      index=df_train.index)","410b6dd0":"#save fitted ohe transformer\nimport pickle\npickle.dump(col_transformer, open('\/kaggle\/working\/ohe.p', 'wb'))","1ed00379":"def engineer_features(df,ohe_transformer):\n    \"\"\"Add new Features to Dataframe.\n    \n    Add Weekday, Month, Year, \n    Perform OneHotEncoding on Auction, Make, Color, Transmission, WheelType,\n    Nationality, Size, TopThreeAmericanName, PRIMEUNIT, AUCGUART\n       \n    Args:\n        df (pd.DataFrame): Dataframe\n        ohe_transformer: The fitted ohe_transformer\n    \n    Returns:\n        (pd.DataFrame) : Dataframe with new Features\n    \"\"\"  \n    #informations from \"PurchDate\"\n    #year\n    df.loc[:,\"PurchYear\"] = df.loc[:,\"PurchDate\"].dt.year\n\n    #weekday\n    df.loc[:,\"PurchWeekday\"] = df.loc[:,\"PurchDate\"].dt.weekday\n\n    #month\n    df.loc[:,\"PurchMonth\"] = df.loc[:,\"PurchDate\"].dt.month\n\n    #drop PurchDate\n    df.drop(['PurchDate'],axis=1,inplace=True)\n    \n   \n    #OneHotEncoding\n    \n    # define cat_cols\n    cat_cols =  ['Auction',\n                 'Make',\n                 'Color',\n                 'Transmission',\n                 'WheelType',\n                 'Nationality',\n                 'TopThreeAmericanName',\n                 'Size',\n                 'PRIMEUNIT',\n                 'AUCGUART']\n\n    #restore column names for final Data Frames\n    ohe_names = ohe_transformer.named_transformers_['OHE'].get_feature_names(cat_cols)\n    remaining_names = ohe_transformer._df_columns[ohe_transformer._remainder[2]]\n\n    df = pd.DataFrame(ohe_transformer.transform(df), \n                          columns=list(ohe_names)+list(remaining_names), \n                          index=df.index)\n    \n    return df","d31d04f7":"#save function\npickle.dump(engineer_features, open('\/kaggle\/working\/engineer_features.p', 'wb'))","a1b9f9a4":"###Load Data\n#copy \ndf_train = features_train.copy()\ndf_val = features_val.copy()\n\n# clean_data\ndf_train = clean_data(df_train)\ndf_val =   clean_data(df_val)\n\n#sample Data\ndf_train, y_train = sampling_data(df_train,target_train)\n\n#load ohe transformer\nimport pickle\nohe_transformer = pickle.load(open('ohe.p', 'rb'))\n\n#transform\ndf_train = engineer_features(df_train,ohe_transformer)\ndf_val = engineer_features(df_val,ohe_transformer)","18348f09":"#correlated cols with VehBCost\ncols_corr=['MMRAcquisitionAuctionAveragePrice',\n        'MMRAcquisitionAuctionCleanPrice','MMRAcquisitionRetailAveragePrice',\n        'MMRAcquisitonRetailCleanPrice','MMRCurrentAuctionAveragePrice',\n        'MMRCurrentAuctionCleanPrice','MMRCurrentRetailAveragePrice',\n        'MMRCurrentRetailCleanPrice','VehBCost']\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.decomposition import PCA\n\n#Pipline\nstd_pca = Pipeline([('std', StandardScaler()), \n                    ('pca', PCA(n_components=0.9, random_state=0))])\n#fit\nstd_pca.fit(df_train.loc[:,cols_corr])\n\n#transform and append\n#df_train\npca_features = pd.DataFrame(std_pca.transform(df_train.loc[:,cols_corr]), \n                            columns=[\"PCA1\",\"PCA2\"], \n                           index=df_train.index)\ndf_train.loc[:,\"pca_1\"] = pca_features[\"PCA1\"]\ndf_train.loc[:,\"pca_2\"] = pca_features[\"PCA2\"]\ndf_train = df_train.drop(cols_corr,axis =\"columns\")\n\n#val\npca_features = pd.DataFrame(std_pca.transform(df_val.loc[:,cols_corr]), \n                            columns=[\"PCA1\",\"PCA2\"], \n                           index=df_val.index)\ndf_val.loc[:,\"pca_1\"] = pca_features[\"PCA1\"]\ndf_val.loc[:,\"pca_2\"] = pca_features[\"PCA2\"]\ndf_val = df_val.drop(cols_corr,axis =\"columns\")","deea85d0":"#plot Screeplot\n\nplt.bar(x=[\"PCA_01\",\"PCA_02\"], \n        height=std_pca.named_steps[\"pca\"].explained_variance_ratio_ ,                                 \n        linewidth=1, \n        edgecolor='#000000' );\nplt.title('Screeplot');\nplt.ylabel(\"Eplained Variance\");","a8eae6f4":"#save fitted pca\npickle.dump(std_pca, open('\/kaggle\/working\/pca.p', 'wb'))","3ff2ec9f":"def pca(df,pca_transformer):\n    \"\"\"Perform PCA on correlated Features.\n    \n    cols_corr=['MMRAcquisitionAuctionAveragePrice','MMRAcquisitionAuctionCleanPrice',\n            'MMRAcquisitonRetailCleanPrice','MMRCurrentAuctionAveragePrice',\n            'MMRCurrentAuctionCleanPrice','MMRCurrentRetailAveragePrice',\n            'MMRCurrentRetailCleanPrice','MMRAcquisitionRetailAveragePrice','VehBCost']\n       \n    Args:\n        df (pd.DataFrame): Dataframe\n        pca_transformer: the fitted pca_transformer\n    \n    Returns:\n        df (pd.DataFrame) : Dataframe with transformed cols_corr to \"pca_MMRA\"\n      \n    \n    \"\"\"\n    #correlated cols\n    cols_corr=['MMRAcquisitionAuctionAveragePrice',\n            'MMRAcquisitionAuctionCleanPrice','MMRAcquisitionRetailAveragePrice',\n            'MMRAcquisitonRetailCleanPrice','MMRCurrentAuctionAveragePrice',\n            'MMRCurrentAuctionCleanPrice','MMRCurrentRetailAveragePrice',\n            'MMRCurrentRetailCleanPrice','VehBCost']\n\n    \n    #transform\n    pca_features = pd.DataFrame(pca_transformer.transform(df.loc[:,cols_corr]), \n                                columns=[\"PCA1\",\"PCA2\"], \n                                index=df.index)\n    # append\n    df.loc[:,\"pca_1\"] = pca_features[\"PCA1\"]\n    df.loc[:,\"pca_2\"] = pca_features[\"PCA2\"]\n    \n    #drop cols_corr\n    df = df.drop(cols_corr,axis =\"columns\")\n\n    return df","ed45d503":"#save fitted pca_function\npickle.dump(pca, open('\/kaggle\/working\/pca_function.p', 'wb'))","fd28d13e":"###Load Data\n####################################################\n## copy \ndf_train = features_train.copy()\ndf_val = features_val.copy()\n\n####################################################\n## clean_data\ndf_train = clean_data(df_train)\ndf_val =   clean_data(df_val)\n\n####################################################\n## sample Data\ndf_train, y_train = sampling_data(df_train,target_train)\n\n####################################################\n## engineer features\n#load ohe transformer\nimport pickle\nohe_transformer = pickle.load(open('ohe.p', 'rb'))\n\n#transform\ndf_train = engineer_features(df_train,ohe_transformer)\ndf_val = engineer_features(df_val,ohe_transformer)\n\n# #####################################################\n# ## Apply pca\n# #load transformer\n# pca_transformer = pickle.load(open('pca.p', 'rb'))\n\n# #transform\n# df_train =  pca(df_train,pca_transformer)\n# df_val   =  pca(df_val,pca_transformer)\n\n# in the process of modeling it turned out that the classification works better if i don't use PCA ","2bccb8b8":"#initate Model\nfrom sklearn.ensemble import RandomForestClassifier\nmodel_forest = RandomForestClassifier(class_weight=\"balanced\", random_state=0)\n\n#fit\nmodel_forest.fit(df_train,y_train)\n\n#save results\nfeature_results=[]\n\n#predict\ntarget_val_pred = model_forest.predict(df_val)\n\n#evaluate\naccuracy = accuracy_score(target_val,target_val_pred)\nrecall = recall_score(target_val,target_val_pred)\nprecision = precision_score(target_val,target_val_pred)\nf1_scr = f1_score(target_val,target_val_pred)\n\n#save\nscores = {'name': \"RandomForestClassifier\",\n          'accuracy': accuracy,\n          'precision': precision,\n          'recall': recall,\n          'F1': f1_scr,\n             }\nfeature_results.append(scores)\n\n#show results\npd.DataFrame(feature_results).T\n","b74f75e4":"# deactivate the scientific notation\npd.options.display.float_format= \"{:.8f}\".format\n\n# show feature_importance \npd.DataFrame(model_forest.feature_importances_, index=df_train.columns).sort_values(by=0, ascending=False)","1d0083bb":"#print thea least important features\nfeature_importance = pd.DataFrame(model_forest.feature_importances_, index=df_train.columns)\nfeature_importance.sort_values(by=0, ascending=True).head(50).index\n","136aae02":"#drop Features\ndrop = ['Nationality_Unknown', 'TopThreeAmericanName_Unknown', 'Size_Unknown',\n       'Make_HUMMER', 'Make_PLYMOUTH', 'Make_VOLVO', 'PRIMEUNIT_YES',\n       'Make_CADILLAC', 'Color_Unknown', 'Transmission_Unknown', 'Make_ACURA',\n       'Make_INFINITI', 'Make_LEXUS', 'Make_SUBARU', 'AUCGUART_RED',\n       'Make_MINI', 'Make_VOLKSWAGEN', 'Make_ISUZU', 'Make_SCION',\n       'Color_NOT AVAIL', 'Nationality_OTHER', 'Make_LINCOLN', 'Color_YELLOW',\n       'Make_OLDSMOBILE', 'Color_OTHER', 'Make_HONDA', 'Color_PURPLE',\n       'Color_ORANGE', 'Make_GMC', 'Color_BROWN', 'Make_MITSUBISHI',\n       'Make_TOYOTA', 'Size_SPORTS', 'Make_BUICK', 'Size_SPECIALTY',\n       'Make_SUZUKI', 'Make_MAZDA', 'Make_MERCURY', 'Size_SMALL TRUCK',\n       'Size_LARGE SUV', 'Size_CROSSOVER', 'WheelType_Special', 'Make_JEEP',\n       'Nationality_TOP LINE ASIAN', 'Make_NISSAN', 'Color_BEIGE',\n       'Make_HYUNDAI', 'Make_KIA', 'Size_SMALL SUV', 'Make_SATURN']\n\ndf_train_drop =df_train.drop(drop,axis=\"columns\")\ndf_val_drop=df_val.drop(drop,axis=\"columns\")\n","dc1de651":"# Evaluate the dropping of Features\n#fit\nmodel_forest.fit(df_train_drop,y_train)\n\n#predict\ntarget_val_pred = model_forest.predict(df_val_drop)      \n\n#evaluate\naccuracy = accuracy_score(target_val,target_val_pred)\nrecall = recall_score(target_val,target_val_pred)\nprecision = precision_score(target_val,target_val_pred)\nf1_scr = f1_score(target_val,target_val_pred)\n\n#save\nscores = {'name': \"rfc_drop50features\",\n          'accuracy': accuracy,\n          'precision': precision,\n          'recall': recall,\n          'F1': f1_scr,\n             }\nfeature_results.append(scores)\n\n#show results\npd.DataFrame(feature_results).T\n","5aca06ac":"def feature_selection(df):\n    \"\"\"Drops the last 5 features, selected by Features Importance by the RandomForestClassifier.\n   \n   drops Features:\n      ['Nationality_Unknown', 'TopThreeAmericanName_Unknown', 'Size_Unknown',\n       'Make_HUMMER', 'Make_PLYMOUTH', 'Make_VOLVO', 'PRIMEUNIT_YES',\n       'Make_CADILLAC', 'Color_Unknown', 'Transmission_Unknown', 'Make_ACURA',\n       'Make_INFINITI', 'Make_LEXUS', 'Make_SUBARU', 'AUCGUART_RED',\n       'Make_MINI', 'Make_VOLKSWAGEN', 'Make_ISUZU', 'Make_SCION',\n       'Color_NOT AVAIL', 'Nationality_OTHER', 'Make_LINCOLN', 'Color_YELLOW',\n       'Make_OLDSMOBILE', 'Color_OTHER', 'Make_HONDA', 'Color_PURPLE',\n       'Color_ORANGE', 'Make_GMC', 'Color_BROWN', 'Make_MITSUBISHI',\n       'Make_TOYOTA', 'Size_SPORTS', 'Make_BUICK', 'Size_SPECIALTY',\n       'Make_SUZUKI', 'Make_MAZDA', 'Make_MERCURY', 'Size_SMALL TRUCK',\n       'Size_LARGE SUV', 'Size_CROSSOVER', 'WheelType_Special', 'Make_JEEP',\n       'Nationality_TOP LINE ASIAN', 'Make_NISSAN', 'Color_BEIGE',\n       'Make_HYUNDAI', 'Make_KIA', 'Size_SMALL SUV', 'Make_SATURN']\n    \n    Args: \n        df (pd.DataFrame) : uncleaned DataFrame\n        \n    Returns:\n        df  (pd.DataFrame) : cleaned DataFrame\n    \n    \"\"\"\n    \n    drop = ['Nationality_Unknown', 'TopThreeAmericanName_Unknown', 'Size_Unknown',\n       'Make_HUMMER', 'Make_PLYMOUTH', 'Make_VOLVO', 'PRIMEUNIT_YES',\n       'Make_CADILLAC', 'Color_Unknown', 'Transmission_Unknown', 'Make_ACURA',\n       'Make_INFINITI', 'Make_LEXUS', 'Make_SUBARU', 'AUCGUART_RED',\n       'Make_MINI', 'Make_VOLKSWAGEN', 'Make_ISUZU', 'Make_SCION',\n       'Color_NOT AVAIL', 'Nationality_OTHER', 'Make_LINCOLN', 'Color_YELLOW',\n       'Make_OLDSMOBILE', 'Color_OTHER', 'Make_HONDA', 'Color_PURPLE',\n       'Color_ORANGE', 'Make_GMC', 'Color_BROWN', 'Make_MITSUBISHI',\n       'Make_TOYOTA', 'Size_SPORTS', 'Make_BUICK', 'Size_SPECIALTY',\n       'Make_SUZUKI', 'Make_MAZDA', 'Make_MERCURY', 'Size_SMALL TRUCK',\n       'Size_LARGE SUV', 'Size_CROSSOVER', 'WheelType_Special', 'Make_JEEP',\n       'Nationality_TOP LINE ASIAN', 'Make_NISSAN', 'Color_BEIGE',\n       'Make_HYUNDAI', 'Make_KIA', 'Size_SMALL SUV', 'Make_SATURN']\n\n\n    df =df.drop(drop,axis=\"columns\")\n    \n    return df","3e183ecf":"# save fitted pca\npickle.dump(feature_selection, open('\/kaggle\/working\/feature_selection.p', 'wb'))","680de244":"# #load pickle, I can always start from here during modeling \n# clean_data = pickle.load(open('\/kaggle\/working\/clean_data.p', 'rb'))\n# sampling_data = pickle.load(open('\/kaggle\/working\/sampling_data.p', 'rb'))\n# resample_data_smote = pickle.load(open('\/kaggle\/working\/smote.p', 'rb'))\n# engineer_features = pickle.load(open('\/kaggle\/working\/engineer_features.p', 'rb'))\n# ohe_transformer = pickle.load(open('\/kaggle\/working\/ohe.p', 'rb'))\n# feature_selection = pickle.load(open('\/kaggle\/working\/feature_selection.p', 'rb'))","cdb61868":"#### apply clean_data, sampling_data, engineer_features and feature_selection\n\n\n####################################################\n## clean_data\nfeatures_train = clean_data(features_train)\nfeatures_val =   clean_data(features_val)\n\n####################################################\n## sample Data\nfeatures_train, target_train = sampling_data(features_train,target_train)\n\n####################################################\n## engineer features\n\n#load ohe transformer\nohe_transformer = pickle.load(open('ohe.p', 'rb'))\n\n#transform\nfeatures_train = engineer_features(features_train,ohe_transformer)\nfeatures_val = engineer_features(features_val,ohe_transformer)\n\n###################################################\n## feature_selection\nfeatures_train = feature_selection(features_train)\nfeatures_val = feature_selection(features_val)","9c5bad96":"#load model_results \nmodel_results = pickle.load(open('\/kaggle\/working\/model_results.p', 'rb'))","1ab3e3f8":"from sklearn.exceptions import DataConversionWarning\nimport warnings\nwarnings.filterwarnings(action='ignore')","7c9639e5":"# build unoptimized model\n\n#initiate RandomForestClassifier\nmodel_forest = RandomForestClassifier(class_weight=\"balanced\", random_state=0)\n\n#fit\nmodel_forest.fit(features_train,target_train)\n\n\n#predict\ntarget_val_pred = model_forest.predict(features_val)\n\n#evaluate\naccuracy = accuracy_score(target_val,target_val_pred)\nrecall = recall_score(target_val,target_val_pred)\nprecision = precision_score(target_val,target_val_pred)\nf1_scr = f1_score(target_val,target_val_pred)\n\n#save\nscores = {'name': \"RandomForestClassifier\",\n          'accuracy': accuracy,\n          'precision': precision,\n          'recall': recall,\n          'F1': f1_scr,\n             }\nmodel_results.append(scores)\n\n#show results\npd.DataFrame(model_results).T\n","f54aa1d1":"#save results\npickle.dump(model_results, open('\/kaggle\/working\/model_results.p', 'wb'))","357eb5ed":"model_results","64703f0b":"# set-up for validation curve: which max_depth-settings to try\nsearch_space = range(2, 35,4)\n\n# initialize figures with two axes\nfig, ax = plt.subplots(ncols=2, figsize=[16, 4])\nfig.suptitle('Validation curves of decision tree')\n\n#################################################################################\n# precision\n\n# calculate precision values of different hyperparameter settings and cross validation folds\nprecision_train, precision_val = validation_curve(estimator=model_forest, \n                                                  X=features_train, \n                                                  y=target_train, \n                                                  param_name='max_depth',\n                                                  param_range=search_space,\n                                                  cv=5,\n                                                  scoring='precision')\n\n# calculate average precision values for each hyperparameter setting\nprecision_train_mean = np.mean(precision_train, axis=1)\nprecision_val_mean = np.mean(precision_val, axis=1)\n\n# plot validation curve\nax[0].plot(search_space,\n           precision_train_mean, label='precision train')\nax[0].plot(search_space,\n           precision_val_mean, label='precision validation')\nax[0].set(ylim=[0, 1.05],\n          xlabel='Decision levels (max_depth)',\n          ylabel='Precision score',\n          title='Precision score and Decision levels')\nax[0].legend()\n#################################################################################\n# recall\n\n# calculate recall values of different hyperparameter settings and cross validation folds\nrecall_train, recall_val = validation_curve(estimator=model_forest, \n                                            X=features_train, \n                                            y=target_train, \n                                            param_name='max_depth',\n                                            param_range=search_space,\n                                            cv=5,\n                                            scoring='recall')\n\n# calculate average recall values for each hyperparameter setting\nrecall_train_mean = np.mean(recall_train, axis=1)\nrecall_val_mean = np.mean(recall_val, axis=1)\n\n# plot validation curve\nax[1].plot(search_space,\n           recall_train_mean, label='recall train')\nax[1].plot(search_space,\n           recall_val_mean, label='recall validation')\nax[1].set(ylim=[0, 1.05],\n          xlabel='Decision levels (max_depth)',\n          ylabel='Recall score',\n          title='Recall score and Decision levels')\nax[1].legend();","b409af29":"# set-up for validation curve: which max_depth-settings\nsearch_space = range(2, 40,4)\n\n# initialize figures with two axes\nfig, ax = plt.subplots(ncols=1, figsize=[8, 4])\nfig.suptitle('Validation curves of decision tree')\n\n#################################################################################\n# precision\n\n# calculate precision values of different hyperparameter settings and cross validation folds\nprecision_train, precision_val = validation_curve(estimator=model_forest, \n                                                  X=features_train, \n                                                  y=target_train, \n                                                  param_name='max_depth',\n                                                  param_range=search_space,\n                                                  cv=5,\n                                                  scoring='f1')\n\n# calculate average precision values for each hyperparameter setting\nprecision_train_mean = np.mean(precision_train, axis=1)\nprecision_val_mean = np.mean(precision_val, axis=1)\n\n# plot validation curve\nax.plot(search_space,\n           precision_train_mean, label='f1 train')\nax.plot(search_space,\n           precision_val_mean, label='f1 validation')\nax.set(ylim=[0, 1.05],\n          xlabel='Decision levels (max_depth)',\n          ylabel='F1 score',\n          title='F1 score and Decision levels')\nax.legend()","87866528":"#Run GridSearch with RandomForest\n\n# initiate model\n# I testet a lot of different values, but because i had to rerun the notebook i only let the best values in the Grid\nmodel_forest = RandomForestClassifier(class_weight=\"balanced\", random_state=0)\n\n\n# Create the parameter grid\ngrid_search_rfc = [{'max_depth':[13],#,10,11,12,13,14,15,\"none\"],       \n                    'max_features':[ 8],#,10,],  #[2, 3, 4, 5, 6, 8, 20,\"auto],\n                    'min_samples_split':[2], #[2, 3, 4],\n                    'min_samples_leaf':[2],  #[2, 3, 4]\n                    }]\n\n# Create an instance of GridSearch \ngrid_rfc = GridSearchCV(estimator=model_forest,\n                     param_grid = grid_search_rfc,\n                     scoring='f1',\n                     cv=5,\n                  \n                     n_jobs=-1)\n\n# Train the RandomForestClassifier\ngrid_rfc.fit(features_train, target_train)\n\n# Print the training score of the best model\nprint(\"best score\")\nprint(grid_rfc.best_score_)\n\n# Print the model parameters of the best model\nprint(\"best model parameter\")\nprint(grid_rfc.best_params_)\n\n# Print the val score of the best model\n#predict\ntarget_val_pred = grid_rfc.predict(features_val)\n\n#evaluate\naccuracy = accuracy_score(target_val,target_val_pred)\nrecall = recall_score(target_val,target_val_pred)\nprecision = precision_score(target_val,target_val_pred)\nf1_scr = f1_score(target_val,target_val_pred)\n\n#save\nscores = {'name': \"Rfc_GridSearch\",\n          'accuracy': accuracy,\n          'precision': precision,\n          'recall': recall,\n          'F1': f1_scr,\n             }\nmodel_results.append(scores)\n    \n#show results\npd.DataFrame(model_results)","64dabb07":"#### print roc-curve\n\nmodel_forest = RandomForestClassifier(max_depth=13, max_features=8, min_samples_leaf=2, class_weight=\"balanced\", random_state=0)\n\n# Train the RandomForestClassifier\nmodel_forest.fit(features_train, target_train)\n\n#create DataFrame with prediction\npred_val = grid_rfc.predict(features_val)  # predict val data\n\n# create DataFrame with one column named prediction\ndf_pred_val = pd.DataFrame(pred_val, columns=['prediction'])\n\n# predict probabilities and add them as new column\ndf_pred_val.loc[:, 'probability'] = model_forest.predict_proba(features_val)[:, 1]  \n\n# calculate roc-curve\nfrom sklearn.metrics import roc_curve\n\nfalse_positive_rate, recall, threshold = roc_curve(target_val, df_pred_val.loc[:, 'probability']) \n\n# module import and style setting\nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')\n\n# figure and axes intialisation\nfig, ax = plt.subplots()\n\n# reference lines\nax.plot([0, 1], ls = \"--\")  # blue diagonal\nax.plot([0, 0], [1, 0], c=\".7\", ls='--')  # grey vertical\nax.plot([1, 1], c=\".7\", ls='--')  # grey horizontal\n\n# roc curve\nax.plot(false_positive_rate, recall)\n\n# labels\nax.set_title(\"Receiver Operating Characteristic\")\nax.set_xlabel(\"False Positive Rate\")\nax.set_ylabel(\"Recall\");","c92e20b0":"# RandomForestClassifier with PolynomialFeatures\n#import\nfrom sklearn.preprocessing import PolynomialFeatures\n\n#initate PolynomialFeatures\npoly_transformer = PolynomialFeatures(degree=2,              # Te degree of the resulting polynomial\n                                      interaction_only=False, # Controls whether self interactons are included \n                                      include_bias=False)    # Controls whether the 1 is also icluded as a feature\n\n#initate model\nmodel_forest = RandomForestClassifier(class_weight=\"balanced\", random_state=0)\n\n#create Pipeline\nmodel_poly_rfc = Pipeline([(\"poly\", poly_transformer),\n                        (\"model_forest\", model_forest)])\n\n\n\n\n# Create the parameter grid\ngrid_search_rfc = [{'max_depth':[14], #10,14,15\n                    'max_features':[8],  #[2, 3, 4, 5, 6, 8],\n                    'min_samples_split':[2],  #[2, 3, 4],\n                    'min_samples_leaf':[2],  #[2, 3, 4]\n                    }]\n\n# Create an instance of GridSearch \ngrid_rfc = GridSearchCV(estimator=model_forest,\n                     param_grid = grid_search_rfc,\n                     scoring='f1',\n                     cv=3,\n                  \n                     n_jobs=-1)\n\n# Train the RandomForestClassifier\ngrid_rfc.fit(features_train, target_train)\n\n# Print the training score of the best model\nprint(\"best score\")\nprint(grid_rfc.best_score_)\n\n# Print the model parameters of the best model\nprint(\"best model parameter\")\nprint(grid_rfc.best_params_)\n\n# Print the val score of the best model\n#predict\ntarget_val_pred = grid_rfc.predict(features_val)\n\n#evaluate\naccuracy = accuracy_score(target_val,target_val_pred)\nrecall = recall_score(target_val,target_val_pred)\nprecision = precision_score(target_val,target_val_pred)\nf1_scr = f1_score(target_val,target_val_pred)\n\n#save\nscores = {'name': \"Rfc_GridSearch_poly\",\n          'accuracy': accuracy,\n          'precision': precision,\n          'recall': recall,\n          'F1': f1_scr,\n             }\nmodel_results.append(scores)\n    \n#show results\npd.DataFrame(model_results).T","6728be80":"# instantiate model\nmodel = LogisticRegression(max_iter=1000)\n\n# create Pipeline\nmodel_baseline = Pipeline([(\"scaler\",StandardScaler()),\n                           (\"model\", model)])\n\n# fit pipeline on cleaned training set\nmodel_baseline.fit(features_train, target_train)\n\n# predict and evaluate on test set\n\nmodel_results\n\n#predict\ntarget_val_pred = model_baseline.predict(features_val)\n\n#evaluate\naccuracy = accuracy_score(target_val,target_val_pred)\nrecall = recall_score(target_val,target_val_pred)\nprecision = precision_score(target_val,target_val_pred)\nf1_scr = f1_score(target_val,target_val_pred)\n\n#save\nscores = {'name': \"LR\",\n          'accuracy': accuracy,\n          'precision': precision,\n          'recall': recall,\n          'F1': f1_scr,\n             }\nmodel_results.append(scores)\n    \n#show results\npd.DataFrame(model_results).T\n\n\n","bcf8e849":"## Apply pca\n#load transformer\npca_transformer = pickle.load(open('\/kaggle\/working\/pca.p', 'rb'))\n\n#transform\nfeatures_train_pca =  pca(features_train,pca_transformer)\nfeatures_val_pca   =  pca(features_val,pca_transformer)\n\n#in this Case PCA helps to get better results","0dbc1ea2":"# tune hyperparameters Model LogisticRegression with GridSearch\n\n#initiate Model\nmodel_log = LogisticRegression(solver=\"liblinear\",class_weight=\"balanced\",random_state=42)\n\n#make Pipeline\nmodel_log_std = Pipeline([(\"scaler\",StandardScaler()),\n                           (\"model_log\", model_log)])\n\n# creat C_values\nimport numpy as np \nC_values = np.geomspace(0.001,1000,10)\n\n# creat parameter Grid\ngrid_search_log = [{'model_log__penalty':[\"l2\",\"l1\"],\n                    'model_log__C':C_values\n                    }]\n\n# Create an instance of GridSearch Cross-validation estimator\ngrid_log = GridSearchCV(estimator=model_log_std,\n                     param_grid = grid_search_log,\n                     scoring='f1',\n                     cv=3,\n                  \n                     n_jobs=-1)\n\n# Train the RandomForestClassifier\ngrid_log.fit(features_train_pca, target_train)\n\n# Print the training score of the best model\nprint(\"best score\")\nprint(grid_log.best_score_)\n\n# Print the model parameters of the best model\nprint(\"best model parameter\")\nprint(grid_log.best_params_)\n\n# Print the test score of the best model\n#predict\ntarget_val_pred = grid_log.predict(features_val_pca)\n\n#evaluate\naccuracy = accuracy_score(target_val,target_val_pred)\nrecall = recall_score(target_val,target_val_pred)\nprecision = precision_score(target_val,target_val_pred)\nf1_scr = f1_score(target_val,target_val_pred)\n\n#save\nscores = {'name': \"LR_GridSearch\",\n          'accuracy': accuracy,\n          'precision': precision,\n          'recall': recall,\n          'F1': f1_scr,\n             }\nmodel_results.append(scores)\n    \n#show results\npd.DataFrame(model_results).T ","c0954b2b":"model_results","c799fc0b":"#KNeighborsClassifier with unbalanced Data Set, unoptimized model\n\n#initiate Model\nmodel_KNC = KNeighborsClassifier()\n\n#make Pipeline\nmodel_KNC_std = Pipeline([(\"scaler\",StandardScaler()),\n                           (\"model_KNC\", model_KNC)])\n\n#fit Model\nmodel_KNC_std.fit(features_train,target_train)\n\n#predict\ntarget_val_pred = model_KNC_std.predict(features_val)\n\n#evaluate\naccuracy = accuracy_score(target_val,target_val_pred)\nrecall = recall_score(target_val,target_val_pred)\nprecision = precision_score(target_val,target_val_pred)\nf1_scr = f1_score(target_val,target_val_pred)\n\n#save\nscores = {'name': \"KNC_unbalanced\",\n          'accuracy': accuracy,\n          'precision': precision,\n          'recall': recall,\n          'F1': f1_scr,\n             }\nmodel_results.append(scores)\n    \n#show results\npd.DataFrame(model_results).T ","a97aa725":"#resample Data for KNeighborsClassifier\nfeatures_train_smote, target_train_smote = resample_data_smote(features_train, target_train)\ntarget_train_smote.mean()   ","a61483ae":"#KNeighborsClassifier with the balanced smote Data\n\n\n#initiate Model\nmodel_KNC = KNeighborsClassifier()\n\n#make Pipeline\nmodel_KNC_std = Pipeline([(\"scaler\",StandardScaler()),\n                           (\"model_KNC\", model_KNC)])\n\n#fit Model\nmodel_KNC_std.fit(features_train_smote,target_train_smote)\n\n#predict\ntarget_val_pred = model_KNC_std.predict(features_val)\n\n#evaluate\naccuracy = accuracy_score(target_val,target_val_pred)\nrecall = recall_score(target_val,target_val_pred)\nprecision = precision_score(target_val,target_val_pred)\nf1_scr = f1_score(target_val,target_val_pred)\n\n#save\nscores = {'name': \"KNC_smote\",\n          'accuracy': accuracy,\n          'precision': precision,\n          'recall': recall,\n          'F1': f1_scr,\n             }\nmodel_results.append(scores)\n    \n#show results\npd.DataFrame(model_results).T\n\n# After the data set has been balanced with Smote, \n# the f1 has become wores. \n# The goal is to have a high f1. That is why I work without the Smote data set","7c9c5dd1":"#grid search with KNeighborsClassifier, with aim at f1 in gridsearch\n# Create the parameter grid\n\n#initiate Model\nmodel_KNC = KNeighborsClassifier()\n\n#make Pipeline\nmodel_KNC_std = Pipeline([(\"scaler\",StandardScaler()),\n                           (\"model_KNC\", model_KNC),])\n\n\n# create parameter Grid\ngrid_search_knc = [{'model_KNC__n_neighbors': [ 100], # [50,100,150,200],\n                    'model_KNC__weights':[\"distance\"], # [\"uniform\",\"distance\"],\n                    'model_KNC__metric':[\"euclidean\" ]# [ 'euclidean',\"manhattan\"]\n                    }]\n\n# Create an instance of GridSearch Cross-validation estimator\ngrid_knc = GridSearchCV(estimator=model_KNC_std,\n                     param_grid = grid_search_knc,\n                     scoring='f1',\n                     cv=3,\n                     verbose =1,\n                     n_jobs=-1)\n\n# Train the RandomForestClassifier\ngrid_knc.fit(features_train, target_train)\n\n# Print the training score of the best model\nprint(\"best score\")\nprint(grid_knc.best_score_)\n\n# Print the model parameters of the best model\nprint(\"best model parameter\")\nprint(grid_knc.best_params_)\n\n# Print the test score of the best model\n#predict\ntarget_val_pred = grid_knc.predict(features_val)\n\n#evaluate\naccuracy = accuracy_score(target_val,target_val_pred)\nrecall = recall_score(target_val,target_val_pred)\nprecision = precision_score(target_val,target_val_pred)\nf1_scr = f1_score(target_val,target_val_pred)\n\n#save\nscores = {'name': \"KNC_unbalanced_GridSearch\",\n          'accuracy': accuracy,\n          'precision': precision,\n          'recall': recall,\n          'F1': f1_scr,\n             }\nmodel_results.append(scores)\n    \n#show results\npd.DataFrame(model_results).T","a7d1f9eb":"#Support Vector Machine, unoptimized model\nfrom sklearn.svm import SVC \n\n#initiate Model\nmodel_SVC = SVC(class_weight='balanced', random_state=42)\n\n#make Pipeline\nmodel_std_svc = Pipeline([(\"scaler\",StandardScaler()),\n                           (\"model_SVC\", model_SVC),])\n\n#fit Model\nmodel_std_svc.fit(features_train,target_train)\n\n#predict\ntarget_val_pred = model_std_svc.predict(features_val)\n\n\n#evaluate\naccuracy = accuracy_score(target_val,target_val_pred)\nrecall = recall_score(target_val,target_val_pred)\nprecision = precision_score(target_val,target_val_pred)\nf1_scr = f1_score(target_val,target_val_pred)\n\n#save\nscores = {'name': \"SVC\",\n          'accuracy': accuracy,\n          'precision': precision,\n          'recall': recall,\n          'F1': f1_scr,\n             }\nmodel_results.append(scores)\n    \n#show results\npd.DataFrame(model_results).T","e6aad01a":"# Because the SVC is very computationally intensive I use the RandomUnderSampler before I run GridSearch\nfrom imblearn.under_sampling import RandomUnderSampler\nsampler = RandomUnderSampler()\nfeatures_train_under, target_train_under = sampler.fit_resample(features_train, target_train)","a993d509":"features_train_under.shape","9b706186":"#grid search with #Support Vector Machine # took to long # now with undersample Data\n\n# initiate Model\nmodel_SVC = SVC( class_weight='balanced', random_state=42)\n\n# make Pipeline\nmodel_std_svc = Pipeline([(\"scaler\",StandardScaler()),\n                           (\"model_SVC\", model_SVC),])\n# creat k Values\nimport numpy as np \nk = np.geomspace(1,1000,10, dtype=\"int\")\nk = np.unique(k)\n\n# Create the parameter grid\ngrid_search_svc = [{'model_SVC__C':[0.5,1,10,100],\n                   'model_SVC__kernel':[\"rbf\"],\n                   'model_SVC__gamma':[0.001,0.01,0.1, 1, 10]\n                    \n                    }]\n\n# Create an instance of GridSearch Cross-validation estimator\ngrid_svc = GridSearchCV(estimator=model_std_svc,\n                     param_grid = grid_search_svc,\n                     scoring='f1',\n                     cv=3,\n                     verbose =1,\n                     n_jobs=-1)\n\n# fit the Model\ngrid_svc.fit(features_train_under, target_train_under)\n\n# Print the training score of the best model\nprint(\"best score\")\nprint(grid_svc.best_score_)\n\n# Print the model parameters of the best model\nprint(\"best model parameter\")\nprint(grid_svc.best_params_)\n\n# Print the test score of the best model\n#predict\ntarget_val_pred = grid_svc.predict(features_val)\n\n#evaluate\naccuracy = accuracy_score(target_val,target_val_pred)\nrecall = recall_score(target_val,target_val_pred)\nprecision = precision_score(target_val,target_val_pred)\nf1_scr = f1_score(target_val,target_val_pred)\n\n#save\nscores = {'name': \"SVC_GridSearch\",\n          'accuracy': accuracy,\n          'precision': precision,\n          'recall': recall,\n          'F1': f1_scr,\n             }\nmodel_results.append(scores)\n    \n#show results\npd.DataFrame(model_results).T","40022a91":"#Support Vector Classifier with parameter from UnderSample GridSearch \n\n#Support Vector Machine, unoptimized model\nfrom sklearn.svm import SVC \n\n#initiate Model\nmodel_SVC = SVC(class_weight='balanced', random_state=42, C=10, gamma=0.001, kernel=\"rbf\")\n\n#make Pipeline\nmodel_std_svc = Pipeline([(\"scaler\",StandardScaler()),\n                           (\"model_SVC\", model_SVC),])\n\n#fit Model\nmodel_std_svc.fit(features_train,target_train)\n\n#predict\ntarget_val_pred = model_std_svc.predict(features_val)\n\n#evaluate\naccuracy = accuracy_score(target_val,target_val_pred)\nrecall = recall_score(target_val,target_val_pred)\nprecision = precision_score(target_val,target_val_pred)\nf1_scr = f1_score(target_val,target_val_pred)\n\n#save\nscores = {'name': \"SVC_GridSearch\",\n          'accuracy': accuracy,\n          'precision': precision,\n          'recall': recall,\n          'F1': f1_scr,\n             }\nmodel_results.append(scores)\n    \n#show results\npd.DataFrame(model_results).T\n\n#F1 Score, best Score","fec4fd84":"# transform training- and validation data for the ANN\n\n#initatie MinMaxScaler\nscaler = MinMaxScaler()\n\n# transform Features\nfeatures_train_scaled = scaler.fit_transform(features_train)\nfeatures_val_scaled = scaler.transform(features_val)","853a749c":"#import metric\nfrom tensorflow.keras.metrics import Accuracy","cb780f5b":"#artifical neural network, ANN\n# early stopping\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.callbacks import EarlyStopping \nearly_stop  = EarlyStopping(monitor='val_loss', min_delta=0.001, patience=2)\n\n# define model\nmodel_ann_early = Sequential()\n\n# define hidden layers\nhidden_first = Dense(units=32, activation='relu', input_dim=features_train_scaled.shape[1])\nhidden_second = Dense(units=32, activation='relu')\nhidden_third = Dense(units=32, activation='relu')\nhidden_fourth = Dense(units=32, activation='relu')\nhidden_fifth = Dense(units=32, activation='relu')\n\n# define output layer\noutput_layer  = Dense(units=1, activation='sigmoid')\n\n# add 5 hidden layers with 50 units\nmodel_ann_early.add(hidden_first)\nmodel_ann_early.add(hidden_second)\nmodel_ann_early.add(hidden_third)\nmodel_ann_early.add(hidden_fourth)\nmodel_ann_early.add(hidden_fifth)\n\n# add output layer\nmodel_ann_early.add(output_layer)\n\n# compile model\nmodel_ann_early.compile(optimizer='adam', loss='binary_crossentropy', metrics=[Accuracy()])\n\n\n\n\n# train model using early stopping\nmodel_ann_early.fit(features_train_scaled, target_train,\n              epochs=200, batch_size=124,\n              callbacks=[early_stop],\n              validation_data=(features_val_scaled, target_val))","edf98bc2":"#predict and transform probabilites in True and False\ntarget_val_pred = model_ann_early.predict(features_val_scaled)\ntarget_val_pred = target_val_pred.flatten()\ntarget_val_pred = target_val_pred.flatten() > 0.5\n\n#evaluate\naccuracy = accuracy_score(target_val,target_val_pred)\nrecall = recall_score(target_val,target_val_pred)\nprecision = precision_score(target_val,target_val_pred)\nf1_scr = f1_score(target_val,target_val_pred)\n\n#save\nscores = {'name': \"ANN_EarlyStopping\",\n          'accuracy': accuracy,\n          'precision': precision,\n          'recall': recall,\n          'F1': f1_scr,\n             }\nmodel_results.append(scores)\n    \n#show results\npd.DataFrame(model_results).T","c40439bc":"#ANN with dropout Layer\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\n\n# regularization\nfrom tensorflow.keras.layers import Dropout\nmodel_ann_drop = Sequential()\n\n# hidden layer \nhidden_layer_first = Dense(units = 32,            # The dimensionality of the output space\n                     activation = \"relu\",       # The activation function\n                     input_dim = features_train.shape[1]   # Number of dimensions of the features\n                     )\nhidden_layer_second = Dense(units = 32,            # The dimensionality of the output space\n                      activation = \"relu\",       # The activation function \n                      )\nhidden_layer_third = Dense(units = 32,            # The dimensionality of the output space\n                     activation = \"relu\",       # The activation function \n                     )\nhidden_layer_fourth = Dense(units = 32,            # The dimensionality of the output space\n                     activation = \"relu\",       # The activation function \n                     )\nhidden_layer_fifth = Dense(units = 32,            # The dimensionality of the output space\n                     activation = \"relu\",       # The activation function \n                     )\n\n#dropout layer\ndropout_layer_first = Dropout(rate=0.3)\ndropout_layer_second  = Dropout(rate=0.3)\ndropout_layer_third = Dropout(rate=0.3)\ndropout_layer_fourth = Dropout(rate=0.3)\n\n#output_layer\noutput_layer  = Dense(units = 1,            # The dimensionality of the output space\n                activation = \"sigmoid\",       # The activation function\n                     )\n\n# add the layer\nmodel_ann_drop.add(hidden_layer_first)\nmodel_ann_drop.add(dropout_layer_first)\nmodel_ann_drop.add(hidden_layer_second)\nmodel_ann_drop.add(dropout_layer_second)\nmodel_ann_drop.add(hidden_layer_third)\nmodel_ann_drop.add(dropout_layer_third)\nmodel_ann_drop.add(hidden_layer_fourth)\nmodel_ann_drop.add(dropout_layer_fourth)\nmodel_ann_drop.add(hidden_layer_fifth)\nmodel_ann_drop.add(output_layer)\n\n#compile\nmodel_ann_drop.compile(optimizer = \"adam\", loss = 'binary_crossentropy', metrics = [Accuracy()])\n\n#fit\nhist_ann_drop = model_ann_drop.fit(features_train_scaled, target_train, \n                                   epochs=30, batch_size=64, \n                                   validation_data=(features_val_scaled, target_val))","94a4b10e":"#predict\ntarget_val_pred = model_ann_drop.predict(features_val_scaled)\ntarget_val_pred = target_val_pred.flatten()\ntarget_val_pred = target_val_pred.flatten() > 0.5\n\n#evaluate\naccuracy = accuracy_score(target_val,target_val_pred)\nrecall = recall_score(target_val,target_val_pred)\nprecision = precision_score(target_val,target_val_pred)\nf1_scr = f1_score(target_val,target_val_pred)\n\n#save\nscores = {'name': \"ANN_DropoutLayer\",\n          'accuracy': accuracy,\n          'precision': precision,\n          'recall': recall,\n          'F1': f1_scr,\n             }\nmodel_results.append(scores)\n    \n#show results\npd.DataFrame(model_results).T","99c8214b":"#initiate Models\nmodel_forest = RandomForestClassifier(class_weight=\"balanced\", random_state=0)\nmodel_log = LogisticRegression(solver=\"liblinear\",class_weight=\"balanced\",random_state=42)\nmodel_KNC = KNeighborsClassifier( metric= 'manhattan', n_neighbors= 100, weights= 'uniform')\n \n# build Pipeline\nmodel_KNC_std = Pipeline([(\"scaler\",StandardScaler()),\n                           (\"model_KNC\", model_KNC),])\nmodel_log_std = Pipeline([(\"scaler\",StandardScaler()),\n                           (\"model_log\", model_log)])\n\n#fit Models \nmodel_forest.fit(features_train, target_train)\nmodel_log_std.fit(features_train, target_train)\nmodel_KNC_std.fit(features_train, target_train) ","15bf864c":"from sklearn.ensemble import VotingClassifier\n\n\n#create search space\nsearch_space_ens = {\"voting\": [\"soft\",\"hard\"],\n                    \"weights\":[None, [0.3863, 0.3625, 0.3095]]}\n\n#iniatie VotingClassifier\nvoting_knn_log_rf = VotingClassifier(estimators=[(\"rf\", model_forest),\n                                                 (\"log\",model_log_std,),\n                                                 (\"knn\", model_KNC_std),\n                                                 ])\n\n#initiate GridSearch\nmodel_ens = GridSearchCV(estimator=voting_knn_log_rf,\n                        param_grid=search_space_ens,\n                        scoring=\"precision\",\n                        cv=5,\n                        n_jobs=-1,\n                        verbose=2)\n\n# fit the Model\nmodel_ens.fit(features_train, target_train)\n\n# Print the training score of the best model\nprint(\"best score\")\nprint(model_ens.best_score_)\n\n# Print the model parameters of the best model\nprint(\"best model parameter\")\nprint(model_ens.best_params_)\n\n# Print the val score of the best model\n#predict\ntarget_val_pred = model_ens.predict(features_val)\n\n#evaluate\naccuracy = accuracy_score(target_val,target_val_pred)\nrecall = recall_score(target_val,target_val_pred)\nprecision = precision_score(target_val,target_val_pred)\nf1_scr = f1_score(target_val,target_val_pred)\n\n#save\nscores = {'name': \"Ensemble\",\n          'accuracy': accuracy,\n          'precision': precision,\n          'recall': recall,\n          'F1': f1_scr,\n             }\nmodel_results.append(scores)\n    \n#show results\npd.DataFrame(model_results).T","60bc21fe":"pd.DataFrame(model_results)","f0358361":"\n#initate PolynomialFeatures\npoly_transformer = PolynomialFeatures(degree=2,              # Te degree of the resulting polynomial\n                                      interaction_only=False, # Controls whether self interactons are included \n                                      include_bias=False)    # Controls whether the 1 is also icluded as a feature\n\n#initate model\nmodel_forest = RandomForestClassifier(max_depth=14, \n                                      max_features=8,\n                                      min_samples_split=2,\n                                      min_samples_leaf=2,\n                                      class_weight=\"balanced\", \n                                      random_state=0)\n\n#create Pipeline\nmodel_poly_rfc = Pipeline([(\"poly\", poly_transformer),\n                        (\"model_forest\", model_forest)])\n\n\nmodel_poly_rfc.fit(features_train, target_train)","3019889e":"#### apply clean_data, sampling_data, engineer_features and feature_selection\n\n\n####################################################\n## clean_data\nfeatures_test =   clean_data(features_test)\n\n####################################################\n## engineer features\n\n#load ohe transformer\nohe_transformer = pickle.load(open('\/kaggle\/working\/ohe.p', 'rb'))\n\n#transform\nfeatures_test = engineer_features(features_test,ohe_transformer)\n\n###################################################\n## feature_selection\nfeatures_test = feature_selection(features_test)","97629b3b":"features_test_pred = model_poly_rfc.predict(features_test)\n\nfinal_results=[]\n\n#evaluate\naccuracy = accuracy_score(target_test,features_test_pred)\nrecall = recall_score(target_test,features_test_pred)\nprecision = precision_score(target_test,features_test_pred)\nf1_scr = f1_score(target_test,features_test_pred)\n\n#save\nscores = {'name': \"Ensemble\",\n          'accuracy': accuracy,\n          'precision': precision,\n          'recall': recall,\n          'F1': f1_scr,\n             }\nfinal_results.append(scores)\n    \n#show results\npd.DataFrame(final_results)","1924ddbb":"#dummy model, sanity check \nfrom sklearn.dummy import DummyClassifier\ndummy_model = DummyClassifier(strategy=\"stratified\")\ndummy_model.fit(features_train,target_train)\n\ntarget_test_pred = dummy_model.predict(features_test)\n#evaluate:\nprint(\"accuracy: \",accuracy_score(target_test,target_test_pred))\nprint(\"recall: \",recall_score(target_test,target_test_pred))\nprint(\"precision: \",precision_score(target_test,target_test_pred))\nprint(\"F1 Score: \",f1_score(target_test,target_test_pred))","d9d1475b":"#initiate RandomForestClassifier\nmodel = RandomForestClassifier(class_weight=\"balanced\", random_state=0)\n\n#fit\nmodel.fit(features_train,target_train)","a712b3a1":"# Plot top 18 important features\n\n# set fig, ax\nimport matplotlib.pyplot as plt\nfig, ax = plt.subplots(figsize=(14,7))\n\n\n# use Style\nplt.style.use('ggplot')\n\n# plot\nfeature_importance.plot(kind='barh',\n                         ax=ax, \n                         width=0.8,\n                         legend=False,\n                         linewidth=1, \n                         edgecolor='#000000');\n\n\n#title\nax.set_title(label=\"An unknown type of tire implies a bad purchase \",\n             family=\"serif\",\n             weight=\"semibold\",\n             size=14);\n#set xlabel\nax.set_xlabel(xlabel=\"Relative importance\",\n             position=[0,0],\n             horizontalalignment=\"left\",\n             size=13,\n             weight=\"semibold\");\n\nax.set_yticklabels(ax.get_yticklabels(), size=12, color=\"black\");\n\n#ad the percentage\nfor i in range(len(feature_importance)):\n    ax.text(s=\"{:.0f} %\".format(feature_importance.iloc[i,0]*100),\n            x=feature_importance.iloc[i,0]+0.0008,\n            y=i,\n            size=12,\n            color=\"black\")\n\n#xticks off\nax.set_xticks([],[]);\n\n\nfig.tight_layout()","dcf90973":"#import\nfrom pdpbox import pdp\n\n#calculate the partial dependence of the prediction on the Feature VehOdo\npdp_VehOdo = pdp.pdp_isolate(model=model, \n                                    dataset=features_train, \n                                    model_features=features_train.columns, \n                                    feature=\"VehOdo\")","ff543cdc":"#visualie the pdartial dependence plot\npdp.pdp_plot(pdp_isolate_out=pdp_VehOdo,\n                       feature_name=\"VehOdo\",\n                       plot_pts_dist=True,\n                       center=False);","75f84687":"#calculate the partial dependence of the prediction on the Feature VehicleAge\npdp_VehicleAge = pdp.pdp_isolate(model=model, \n                                    dataset=features_train, \n                                    model_features=features_train.columns, \n                                    feature=\"VehicleAge\")","de9976fb":"#visualie the pdartial dependence plot\npdp.pdp_plot(pdp_isolate_out=pdp_VehicleAge,\n                       feature_name=\"VehicleAge\",\n                       plot_pts_dist=True,\n                       center=False);","9453465b":"---","7c1ba0ed":"<p id=\"5.4\"><\/p>\n\n<p style=\"font-family: Arials; font-size: 16px; font-style: normal; font-weight: bold; letter-spacing: 3px; color: #e24a33; line-height:1.0\">5.4 Resample<\/p>\n\n<p style=\"font-family: Arials, sans-serif; font-size: 14px; color: rgba(0,0,0,.7)\">As found in the EDA, the target categories in the dataset are very unbalanced. It may be necessary to resample the training data set. There are several resample methods to choose from. Here I try to find out which method is the best for this dataset.  <\/p>","5013798c":"I can not spot any outlier in the scatter plot. ","fea255f7":"If we look at the normalized data, we see that the older cars have a much higher probability to be a BadBuy ","3b369542":"<p id=\"8.1\"><\/p>\n\n<p style=\"font-family: Arials; font-size: 16px; font-style: normal; font-weight: bold; letter-spacing: 3px; color: #e24a33; line-height:1.0\">8.1 Visualize Feature Importance<\/p>","354f63ba":"<p id=\"2.\"><\/p>\n\n# <span style=\"font-family: Arials; font-size: 20px; font-style: normal; font-weight: bold; letter-spacing: 3px; color: #e24a33\">2. Gather Data<\/span>\n<hr style=\"height: 0.5px; border: 0; background-color: #e24a33\">","c5cc35fc":"<p id=\"3.6\"><\/p>\n\n<p style=\"font-family: Arials; font-size: 16px; font-style: normal; font-weight: bold; letter-spacing: 3px; color: #e24a33; line-height:1.0\">3.6 Correlations<\/p>","bf8e5167":"<p id=\"5.2\"><\/p>\n\n<p style=\"font-family: Arials; font-size: 16px; font-style: normal; font-weight: bold; letter-spacing: 3px; color: #e24a33; line-height:1.0\">5.2 Datatype transformation<\/p>","5a0df370":"<p id=\"5.1\"><\/p>\n\n<p style=\"font-family: Arials; font-size: 16px; font-style: normal; font-weight: bold; letter-spacing: 3px; color: #e24a33; line-height:1.0\">5.1 Data Imputation<\/p>","1427133d":"<p style=\"font-family: Arials; line-height: 1.5; font-size: 16px; font-weight: bold; letter-spacing: 2px; text-align: center; color: #e24a33\">If you liked this notebook, please put your votes.<\/p>\n<p style=\"text-align: center\">\ud83d\ude0a\ud83d\ude0a\ud83d\ude0a<\/p>","6f0edc88":"**`Make`** is an important factor, some brands have higher probabilities to be a BadBuy, like PLYMOUTH, LEXUS, INFINITI, MINI, LINCOLN, ACURA, SUBARU","b392b685":"<p id=\"6.5.3\"><\/p>\n\n<p style=\"font-family: Arials; font-size: 16px; font-style: normal; font-weight: bold; letter-spacing: 3px; color: #e24a33; line-height:1.0\">6.5.3 KNeighborsClassifier<\/p>","55840836":"Let's look into the distribution of **`Make`** divided according to **`IsBadBuy`**","d171bd1e":"#### To easily reproduce the Dimensionality reduction, I define a function that executes each step.","ba412d43":"---","9fa7e685":"<p id=\"4.\"><\/p>\n\n# <span style=\"font-family: Arials; font-size: 20px; font-style: normal; font-weight: bold; letter-spacing: 3px; color: #e24a33\">4. Train Test Validation Split<\/span>\n<hr style=\"height: 0.5px; border: 0; background-color: #e24a33\">","6989578e":"Most BadBuy cars are between 3 and 7 years old but those are also the ones that are sold the most  ","3df2dff0":"<p id=\"6.5.1\"><\/p>\n\n<p style=\"font-family: Arials; font-size: 16px; font-style: normal; font-weight: bold; letter-spacing: 3px; color: #e24a33; line-height:1.0\">6.5.1 RandomForestClassifier<\/p>","7a7ddf6d":"<p id=\"3.\"><\/p>\n\n# <span style=\"font-family: Arials; font-size: 20px; font-style: normal; font-weight: bold; letter-spacing: 3px; color: #e24a33\">3. EDA - Understand data<\/span>\n<hr style=\"height: 0.5px; border: 0; background-color: #e24a33\">\n\n<p style=\"font-family: Arials, sans-serif; font-size: 14px; color: rgba(0,0,0,.7)\">In this block,exploratory data analysis (EDA) will be carried out. Thereby I look at the data types, missing values, at statistical measures of the central trend, search for outliers, look at the characteristics in the single features and also the relationship between Features. <br>In this part I do not change the data. I try to gain insights and then implement them after the Data Split in 5. Data Preparation. <\/p>","e97a3801":"The final Results for the test-set","3afd3837":"Visualize **`VNST`** and look for notable signs","05a60a45":"<p id=\"1.\"><\/p>\n\n# <span style=\"font-family: Arials; font-size: 20px; font-style: normal; font-weight: bold; letter-spacing: 3px; color: #e24a33\">1. Importing Libaries<\/span>\n<hr style=\"height: 0.5px; border: 0; background-color: #e24a33\">\n","1af2f018":"<p id=\"6.5.2\"><\/p>\n\n<p style=\"font-family: Arials; font-size: 16px; font-style: normal; font-weight: bold; letter-spacing: 3px; color: #e24a33; line-height:1.0\">6.5.2 Logistic Regression<\/p>","bcb70b6e":"<p id=\"8.\"><\/p>\n\n# <span style=\"font-family: Arials; font-size: 20px; font-style: normal; font-weight: bold; letter-spacing: 3px; color: #e24a33\">8. Model Interpretation<\/span>\n<hr style=\"height: 0.5px; border: 0; background-color: #e24a33\">","1adde7ca":"---","c0c929f3":"**`VehYear`** and **`VehicleAge`** have the Same Information, **`VehYear`** can be dropped","02b78c93":"#### To easily reproduce the feature engineering, I define a function that executes each step.","0a4a3c1c":"<p id=\"3.7\"><\/p>\n\n<p style=\"font-family: Arials; font-size: 16px; font-style: normal; font-weight: bold; letter-spacing: 3px; color: #e24a33; line-height:1.0\">3.7 Outlier Detection<\/p>","c600c344":"Figuring out how many features to drop is an iterative process.  I dropped the last 50 because it improves the F1 score. Since I'm aiming for F1 score here, this is optimal for me. If more features are dropped, the performance gets worse. ","08c7ab1d":"<p id=\"6.4\"><\/p>\n\n<p style=\"font-family: Arials; font-size: 16px; font-style: normal; font-weight: bold; letter-spacing: 3px; color: #e24a33; line-height:1.0\">6.4 Feature Selection<\/p>","7d36e8fb":"<p id=\"6.3\"><\/p>\n\n<p style=\"font-family: Arials; font-size: 16px; font-style: normal; font-weight: bold; letter-spacing: 3px; color: #e24a33; line-height:1.0\">6.3 Dimensionality Reduction<\/p>","63c12839":"<p id=\"3.2\"><\/p>\n\n<p style=\"font-family: Arials; font-size: 16px; font-style: normal; font-weight: bold; letter-spacing: 3px; color: #e24a33; line-height:1.0\">3.2 Data types<\/p>","00e8fbf9":"There are many missing values that need to be replaced","5509b184":"\n**`MMRA - Values`**, **`VehBCost`** are highly correlated","ed7cefbf":"<p id=\"5.3\"><\/p>\n\n<p style=\"font-family: Arials; font-size: 16px; font-style: normal; font-weight: bold; letter-spacing: 3px; color: #e24a33; line-height:1.0\">5.3 Deal with Outliers<\/p>","91a0a813":"<p id=\"6.5\"><\/p>\n\n<p style=\"font-family: Arials; font-size: 16px; font-style: normal; font-weight: bold; letter-spacing: 3px; color: #e24a33; line-height:1.0\">6.5 Train Model<\/p>\n\nNow the data is prepared and it can be started to generate classification models","83256f77":"#### In features like **`Model`**, **`Trim`**, **`SubModel`**, **`VNST`**, **`VINZIP1`** have many values <br>**`Transmission`** values 'MANUAL' and 'Manual' need to be merged","6ce98db9":"<p id=\"6.1\"><\/p>\n\n<p style=\"font-family: Arials; font-size: 16px; font-style: normal; font-weight: bold; letter-spacing: 3px; color: #e24a33; line-height:1.0\">6.1 Build a Baselinemodel<\/p>","4e00c341":"#### To easily reproduce the cleanup steps with the test, validation, and target data, I define a function that executes each step.","977be345":"---","b547b513":"Let's look into the distribution of **`Nationality`** divided according to **`IsBadBuy`**","93ac4f8d":"Auction place ADESA seems to have the highest number of BadBuy","efa8ca27":"We can clearly see that the data set is very unbalanced. <br> <br> Machine learning algorithms assume by default that the data is balanced. Classifiers learn better from a balanced distribution. Most classifiers have the option to set the hyperparameter class_weight =\"balanced\". An exception to this is the KNeighbors classifier, in this case a resampler can be used.<br><br> Since the data set is very unbalanced, Accuracy is not a good metric for a model. If the algorithm would learn to predict always no BadBuy, Accuaracy would be already 87,5%. Therefore I will focus on the F1_Score in the training.\n\n\n","07d17520":"**`WheelType`** and **`WheelTypeID`** are both the same. <br>\nI will drop **`WheelTypeID`** because **`WheelType`** has the actual typ as information","f43f7331":"Strongly correlated featues were discovered in the EDA. Therefore I perform here dimension reduction with a PCA.  Whether this is useful for the prediction and improves the result I will find out in the steps of model building. ","4c2c613d":"<p id=\"6.6\"><\/p>\n\n<p style=\"font-family: Arials; font-size: 16px; font-style: normal; font-weight: bold; letter-spacing: 3px; color: #e24a33; line-height:1.0\">6.6 Model selection<\/p>","8227fa04":"Some States has more ","573042c2":"#### I define a function that performs this step. Attention this step can only be executed on the train set","7e5655ee":"<p id=\"6.5.4\"><\/p>\n\n<p style=\"font-family: Arials; font-size: 16px; font-style: normal; font-weight: bold; letter-spacing: 3px; color: #e24a33; line-height:1.0\">6.5.4 Support Vector Machine<\/p>","ee4c8235":"<p id=\"3.3\"><\/p>\n\n<p style=\"font-family: Arials; font-size: 16px; font-style: normal; font-weight: bold; letter-spacing: 3px; color: #e24a33; line-height:1.0\">3.3 Categorical data<\/p>","02d729a7":"American cars have a lower probability of being a BadBuy","cca8f260":"Let's look into the distribution of **`Auction`** divided according to **`IsBadBuy`**","9ce88913":"Let's take a look at **`VehicleAge`**","f406a7d5":"check if **`WheelType`** and **`WheelTypeID`** hold the same information","dfe37b4e":"<p id=\"0.\"><\/p>\n\n<p style=\"font-family: Arials; font-size: 20px; font-style: bold; font-weight: bold; letter-spacing: 3px; color: #e24a33\">0. Prologue<\/p>\n<hr style=\"height: 0.5px; border: 0; background-color: #e24a33\">\n\n<p style=\"font-family: Arials, sans-serif; font-size: 14px; color: rgba(0,0,0,.7)\">\nDont get Kicked!<br>\n    \nOne of the biggest challenges of an auto dealership purchasing a used car at an auto auction is the risk of that the vehicle might have serious issues that prevent it from being sold to customers. These cars can be very costly to dealers after transportation cost, throw-away repair work, and market losses in reselling the vehicle.\n    \nIn the dataset these cars are represented in the column IsBadBuy. The challenge of this competition is to predict if the car purchased at the Auction is a good \/ bad buy. The target variable (IsBadBuy) is binary and the data contains missing values.\n    \n<\/p>\n\n\n<p style=\"font-family: Arials, sans-serif; font-size: 14px; color: rgba(0,0,0,.7)\"><strong>Data Dictionary:<\/strong><\/p>\n\nColumn No. | Column name | Description\n:---|:---|:----  \n1  |  `'RefID'` | Unique (sequential) number assigned to vehicles\n2  |  `'IsBadBuy'` | Identifies if the kicked vehicle was an avoidable purchase \n3  |  `'PurchDate'` | The Date the vehicle was Purchased at Auction\n4  |  `'Auction'` | Auction provider at which the  vehicle was purchased\n5  |  `'VehYear'` | The manufacturer's year of the vehicle\n6  |  `'VehicleAge'` | The Years elapsed since the manufacturer's year\n7  |  `'Make'` | Vehicle Manufacturer \n8  |  `'Model'` |Vehicle Model\n9  |  `'Trim'` | Vehicle Trim Level\n10 |  `'SubModel'` | Vehicle Submodel\n11  |  `'Color'` | Vehicle Color\n12  |  `'Transmission'` | Vehicles transmission type (Automatic, Manual)\n13  |  `'WheelTypeID'` | The type id of the vehicle wheel\n14  |  `'WheelType'` | The vehicle wheel type description (Alloy, Covers)\n15  |  `'VehOdo'` | The vehicles odometer reading\n16  |  `'Nationality'` | The Manufacturer's country\n17  |  `'Size'` | The size category of the vehicle (Compact, SUV, etc.)\n18  |  `'TopThreeAmericanName'` | Identifies if the manufacturer is one of the top three American manufacturers\n19  |  `'MMRAcquisitionAuctionAveragePrice'` | Acquisition price for this vehicle in average condition at time of purchase\t\n20  |  `'MMRAcquisitionAuctionCleanPrice'` | Acquisition price for this vehicle in the above Average condition at time of purchase\n21  |  `'MMRAcquisitionRetailAveragePrice'` | Acquisition price for this vehicle in the retail market in average condition at time of purchase\n22  |  `'MMRAcquisitonRetailCleanPrice'` | Acquisition price for this vehicle in the retail market in above average condition at time of purchase\n23  |  `'MMRCurrentAuctionAveragePrice'` |Acquisition price for this vehicle in average condition as of current day\t\n24  |  `'MMRCurrentAuctionCleanPrice'` | Acquisition price for this vehicle in the above condition as of current day\n25  |  `'MMRCurrentRetailAveragePrice'` | Acquisition price for this vehicle in the retail market in average condition as of current day\n26  |  `'MMRCurrentRetailCleanPrice'` | Acquisition price for this vehicle in the retail market in above average condition as of current day\n27  |  `'PRIMEUNIT'` | Identifies if the vehicle would have a higher demand than a standard purchase\n28  |  `'AUCGUART'` | The level guarntee provided by auction for the vehicle (Green light - Guaranteed\/arbitratable, Yellow Light - caution\/issue, red light - sold as is)\n29  |  `'BYRNO'` | Unique number assigned to the buyer that purchased the vehicle\n30  |  `'VNZIP1'` | Zipcode where the car was purchased\n31  |  `'VNST'` | State where the the car was purchased\n32  |  `'VehBCost'` |Acquisition cost paid for the vehicle at time of purchase\n33  |  `'IsOnlineSale'` | Identifies if the vehicle was originally purchased online\n34  |  `'WarrantyCost'` |Warranty price (term=36month  and millage=36K)","783dd633":"<p id=\"3.4\"><\/p>\n\n<p style=\"font-family: Arials; font-size: 16px; font-style: normal; font-weight: bold; letter-spacing: 3px; color: #e24a33; line-height:1.0\">3.4 Numeric data<\/p>","50bb4394":"---","666558c0":"#### And here again the steps combined in one function ","3eef8063":"<p id=\"3.1\"><\/p>\n\n<p style=\"font-family: Arials; font-size: 16px; font-style: normal; font-weight: bold; letter-spacing: 3px; color: #e24a33; line-height:1.0\">3.1 Show distribution of the target variable<\/p>","a222fdc0":"<p id=\"6.5.6\"><\/p>\n\n<p style=\"font-family: Arials; font-size: 16px; font-style: normal; font-weight: bold; letter-spacing: 3px; color: #e24a33; line-height:1.0\">6.5.6 Ensemble Classification<\/p>","c469151d":"Lets compare **`VehYear`**, **`VehicleAge`** and **`PurchDate`**","aef97b48":"---","e90cbf3d":"---\n","cf14f36f":"The best F1 score is available with Smote. Depending on whether precision or recall is important, you can select the appropriate resampler. ","baa08560":"<p id=\"6.5.5\"><\/p>\n\n<p style=\"font-family: Arials; font-size: 16px; font-style: normal; font-weight: bold; letter-spacing: 3px; color: #e24a33; line-height:1.0\">6.5.5 Artifical neural network<\/p>","7b347836":"----","a6081736":"<p id=\"part0\"><\/p>\n\n<p style=\"font-family: Arials; line-height: 1.5; font-size: 24px; font-weight: bold; letter-spacing: 2px; text-align: center; color: #e24a33\">Don't get Kicked! <br>\nData Science Workflow <br> EDA, Split, Data-Preparation, Feature Engenieering, Modeling<\/p>\n<br>\n\n<p style=\"font-family: Arials; font-size: 20px; font-style: normal; font-weight: bold; letter-spacing: 3px; color: #808080; line-height:1\">TABLE OF CONTENT<\/p>\n\n<p style=\"font-family: Arials; font-size: 16px; font-style: normal; font-weight: bold; letter-spacing: 2px; color: #808080; line-height:1\"><a href=\"#0.\" style=\"color:#808080\">0. Prologue<\/a><\/p>\n\n<p style=\"font-family: Arials; font-size: 16px; font-style: normal; font-weight: bold; letter-spacing: 2px; color: #808080; line-height:1\"><a href=\"#1.\" style=\"color:#808080\">1. Importing Libaries<\/a><\/p>\n\n<p style=\"font-family: Arials; font-size: 16px; font-style: normal; font-weight: bold; letter-spacing: 2px; color: #808080; line-height:1\"><a href=\"#2.\" style=\"color:#808080\">2. Gather Data<\/a><\/p>\n\n<p style=\"font-family: Arials; font-size: 16px; font-style: normal; font-weight: bold; letter-spacing: 2px; color: #808080; line-height:1\"><a href=\"#3.\" style=\"color:#808080\">3. EDA - Understand data<\/a><\/p>\n\n<p style=\"text-indent: 1vw; font-family: Arials; font-size: 14px; font-style: normal; font-weight: bold; letter-spacing: 2px; color: #808080; line-height:1; padding-left: 20px;\">\n<a href=\"#3.1\" style=\"color:#808080\">3.1 Show distribution of the target variable<\/a><\/p>\n\n<p style=\"text-indent: 1vw; font-family: Arials; font-size: 14px; font-style: normal; font-weight: bold; letter-spacing: 2px; color: #808080; line-height:1; padding-left: 20px\">\n<a href=\"#3.2\" style=\"color:#808080\">3.2 Data types<\/a><\/p>\n\n<p style=\"text-indent: 1vw; font-family: Arials; font-size: 14px; font-style: normal; font-weight: bold; letter-spacing: 2px; color: #808080; line-height:1; padding-left: 20px\">\n<a href=\"#3.3\" style=\"color:#808080\">3.3 Categorical data<\/a><\/p>\n\n<p style=\"text-indent: 1vw; font-family: Arials; font-size: 14px; font-style: normal; font-weight: bold; letter-spacing: 2px; color: #808080; line-height:1; padding-left: 20px\">\n<a href=\"#3.4\" style=\"color:#808080\">3.4 Numeric data<\/a><\/p>\n\n<p style=\"text-indent: 1vw; font-family: Arials; font-size: 14px; font-style: normal; font-weight: bold; letter-spacing: 2px; color: #808080; line-height:1; padding-left: 20px\">\n<a href=\"#3.5\" style=\"color:#808080\">3.5 Missing values and duplicates<\/a><\/p>\n\n<p style=\"text-indent: 1vw; font-family: Arials; font-size: 14px; font-style: normal; font-weight: bold; letter-spacing: 2px; color: #808080; line-height:1; padding-left: 20px\">\n<a href=\"#3.6\" style=\"color:#808080\">3.6 Correlations<\/a><\/p>\n\n<p style=\"text-indent: 1vw; font-family: Arials; font-size: 14px; font-style: normal; font-weight: bold; letter-spacing: 2px; color: #808080; line-height:1; padding-left: 20px\">\n<a href=\"#3.7\" style=\"color:#808080\">3.7 Outlier Detection<\/a><\/p>\n\n\n<p style=\"font-family: Arials; font-size: 16px; font-style: normal; font-weight: bold; letter-spacing: 2px; color: #808080; line-height:1\"><a href=\"#4.\" style=\"color:#808080\">4. Train Test Validation Split<\/a><\/p>\n\n<p style=\"font-family: Arials; font-size: 16px; font-style: normal; font-weight: bold; letter-spacing: 2px; color: #808080; line-height:1\"><a href=\"#5.\" style=\"color:#808080\">5. Data Preparation<\/a><\/p>\n   \n<p style=\"text-indent: 1vw; font-family: Arials; font-size: 14px; font-style: normal; font-weight: bold; letter-spacing: 2px; color: #808080; line-height:1; padding-left: 20px\">\n<a href=\"#5.1\" style=\"color:#808080\">5.1 Data Imputation<\/a><\/p>\n\n<p style=\"text-indent: 1vw; font-family: Arials; font-size: 14px; font-style: normal; font-weight: bold; letter-spacing: 2px; color: #808080; line-height:1; padding-left: 20px\">\n<a href=\"#5.2.\" style=\"color:#808080\">5.2 Datatype transformation<\/a><\/p>\n\n<p style=\"text-indent: 1vw; font-family: Arials; font-size: 14px; font-style: normal; font-weight: bold; letter-spacing: 2px; color: #808080; line-height:1; padding-left: 20px\">\n<a href=\"#5.3\" style=\"color:#808080\">5.3 Deal with Outliers<\/a><\/p>  \n\n<p style=\"text-indent: 1vw; font-family: Arials; font-size: 14px; font-style: normal; font-weight: bold; letter-spacing: 2px; color: #808080; line-height:1; padding-left: 20px\">\n<a href=\"#5.4\" style=\"color:#808080\">5.4 Resample<\/a><\/p>  \n\n<p style=\"font-family: Arials; font-size: 16px; font-style: normal; font-weight: bold; letter-spacing: 2px; color: #808080; line-height:1\"><a href=\"#6.\" style=\"color:#808080\">6. Modeling<\/a><\/p>\n   \n<p style=\"text-indent: 1vw; font-family: Arials; font-size: 14px; font-style: normal; font-weight: bold; letter-spacing: 2px; color: #808080; line-height:1; padding-left: 20px\">\n<a href=\"#6.1\" style=\"color:#808080\">6.1 Build a Baselinemodel<\/a><\/p>\n\n<p style=\"text-indent: 1vw; font-family: Arials; font-size: 14px; font-style: normal; font-weight: bold; letter-spacing: 2px; color: #808080; line-height:1; padding-left: 20px\">\n<a href=\"#6.2\" style=\"color:#808080\">6.2 Feature Engineering<\/a><\/p>\n\n<p style=\"text-indent: 1vw; font-family: Arials; font-size: 14px; font-style: normal; font-weight: bold; letter-spacing: 2px; color: #808080; line-height:1; padding-left: 20px\">\n<a href=\"#6.3\" style=\"color:#808080\">6.3 Dimensionality Reduction<\/a><\/p> \n\n<p style=\"text-indent: 1vw; font-family: Arials; font-size: 14px; font-style: normal; font-weight: bold; letter-spacing: 2px; color: #808080; line-height:1; padding-left: 20px\">\n<a href=\"#6.4\" style=\"color:#808080\">6.4 Feature selection<\/a><\/p>\n\n<p style=\"text-indent: 1vw; font-family: Arials; font-size: 14px; font-style: normal; font-weight: bold; letter-spacing: 2px; color: #808080; line-height:1; padding-left: 20px\">\n<a href=\"#6.5\" style=\"color:#808080\">6.5 Train model<\/a><\/p>\n\n<p style=\"text-indent: 1vw; font-family: Arials; font-size: 14px; font-style: normal; font-weight: bold; letter-spacing: 2px; color: #808080; line-height:1; padding-left: 50px\">\n<a href=\"#6.5.1\" style=\"color:#808080\">6.5.1 RandomForestClassifier<\/a><\/p>\n\n<p style=\"text-indent: 1vw; font-family: Arials; font-size: 14px; font-style: normal; font-weight: bold; letter-spacing: 2px; color: #808080; line-height:1; padding-left: 50px\">\n<a href=\"#6.5.2\" style=\"color:#808080\">6.5.2 Logistic Regression<\/a><\/p>\n\n<p style=\"text-indent: 1vw; font-family: Arials; font-size: 14px; font-style: normal; font-weight: bold; letter-spacing: 2px; color: #808080; line-height:1; padding-left: 50px\">\n<a href=\"#6.5.3\" style=\"color:#808080\">6.5.3 KNeighborsClassifier<\/a><\/p>\n\n<p style=\"text-indent: 1vw; font-family: Arials; font-size: 14px; font-style: normal; font-weight: bold; letter-spacing: 2px; color: #808080; line-height:1; padding-left: 50px\">\n<a href=\"#6.5.4\" style=\"color:#808080\">6.5.4 Support Vector Machine<\/a><\/p>\n\n<p style=\"text-indent: 1vw; font-family: Arials; font-size: 14px; font-style: normal; font-weight: bold; letter-spacing: 2px; color: #808080; line-height:1; padding-left: 50px\">\n<a href=\"#6.5.5\" style=\"color:#808080\">6.5.5 Artifical neural network<\/a><\/p>\n\n<p style=\"text-indent: 1vw; font-family: Arials; font-size: 14px; font-style: normal; font-weight: bold; letter-spacing: 2px; color: #808080; line-height:1; padding-left: 50px\">\n<a href=\"#6.5.6\" style=\"color:#808080\">6.5.6 Ensemble Classification<\/a><\/p>\n\n<p style=\"text-indent: 1vw; font-family: Arials; font-size: 14px; font-style: normal; font-weight: bold; letter-spacing: 2px; color: #808080; line-height:1; padding-left: 20px\">\n<a href=\"#6.6\" style=\"color:#808080\">6.6 Model selection<\/a><\/p>\n\n<p style=\"font-family: Arials; font-size: 16px; font-style: normal; font-weight: bold; letter-spacing: 2px; color: #808080; line-height:1\"><a href=\"#7.\" style=\"color:#808080\">7. Final Datapipeline<\/a><\/p>\n\n<p style=\"text-indent: 1vw; font-family: Arials; font-size: 14px; font-style: normal; font-weight: bold; letter-spacing: 2px; color: #808080; line-height:1; padding-left: 20px\">\n<a href=\"#7.1\" style=\"color:#808080\">7.1 Sanity Check<\/a><\/p>\n\n<p style=\"font-family: Arials; font-size: 16px; font-style: normal; font-weight: bold; letter-spacing: 2px; color: #808080; line-height:1\"><a href=\"#8.\" style=\"color:#808080\">8. Model Interpretation<\/a><\/p>\n   \n\n<p style=\"text-indent: 1vw; font-family: Arials; font-size: 14px; font-style: normal; font-weight: bold; letter-spacing: 2px; color: #808080; line-height:1; padding-left: 20px\">\n<a href=\"#8.1\" style=\"color:#808080\">8.1 Visualize Feature Importance<\/a><\/p>\n","08267afd":"Most of the Data labeled as object can be transformed to category. \nThe following features have a wrong data type: \n- **`IsBadBuy`** is labeld as int64 but have to be changed to category                               \n- **`PurchDate`** is labeld as object but have to be changed to DateTime\n- **`WheelTypeID`** is labeld as float64 but have to be changed to category\n- **`BYRNO`** is int64 as category\n- **`VNZIP1`** is int64 as category                          \n- **`IsOnlineSale`** is int64 as category\n\n","b6d5ca13":"----","bd4533ff":"<p id=\"6.2\"><\/p>\n\n<p style=\"font-family: Arials; font-size: 16px; font-style: normal; font-weight: bold; letter-spacing: 3px; color: #e24a33; line-height:1.0\">6.2 Feature Engineering<\/p>","38ab2e00":"Feature selection is the process of selecting a subset of relevant features for use in model. Feature selection techniques can shorten training times, make models easier to interpret and improve data's compatibility with a learning model. There are several technics to select features. In this Block i use the RandomForestClassifier because the tree-based strategies used by random forests naturally ranks the features. ","a20a4261":"a car costs only one dollar","9e3185f1":"<p id=\"3.5\"><\/p>\n\n<p style=\"font-family: Arials; font-size: 16px; font-style: normal; font-weight: bold; letter-spacing: 3px; color: #e24a33; line-height:1.0\">3.5 Missing Values and Duplicates<\/p>","6e537f17":"<p id=\"7.\"><\/p>\n\n# <span style=\"font-family: Arials; font-size: 20px; font-style: normal; font-weight: bold; letter-spacing: 3px; color: #e24a33\">7. Sanity Check<\/span>\n<hr style=\"height: 0.5px; border: 0; background-color: #e24a33\">\n","facc1418":"---","be35c1ae":"<p id=\"5.\"><\/p>\n\n# <span style=\"font-family: Arials; font-size: 20px; font-style: normal; font-weight: bold; letter-spacing: 3px; color: #e24a33\">5.Data Preparation<\/span>\n<hr style=\"height: 0.5px; border: 0; background-color: #e24a33\">\n\n<p style=\"font-family: Arials, sans-serif; font-size: 14px; color: rgba(0,0,0,.7)\">The Data Preparation has the goal to find a way to clean the data sets for the model and to bring them into a readable format for the model. At the end of the Data Preparation all steps are combined to one function <\/p>\n\n","82bca1a9":"<p id=\"6.\"><\/p>\n\n# <span style=\"font-family: Arials; font-size: 20px; font-style: normal; font-weight: bold; letter-spacing: 3px; color: #e24a33\">6. Modeling<\/span>\n<hr style=\"height: 0.5px; border: 0; background-color: #e24a33\">\n\n<p style=\"font-family: Arials, sans-serif; font-size: 14px; color: rgba(0,0,0,.7)\">Now let's move on to the core task of a Data Scientist, building machine learning models. Before I start generating and selecting new features, I first build simple baseline model. It is a first simple model without any hyperparameter optimization. This way I can see if the following steps in model optimization bring an improvement.<\/p>","ad8980ab":"Interesting that Top Line Asian has more BadBuys than Other Asian","be016c6d":"<p style=\"font-family: Arials; font-size: 16px; font-style: normal; font-weight: bold; letter-spacing: 3px; color: #e24a33; line-height:1.0\">Note for the further steps<\/p>\n\n\n- **`Model`**, **`Submodel`** and **`Trim`** have too many expressions for machine learning.  I drop the features. \n\n- **`WheelTyp`** and **`WheelTypID`** are identical, where WheelTyp has more information about the type of tires. WheelTypeID can be dropped.\n\n- **`VehYear`** and **`VehicleAge`** have a highly negative correlation. It is important to know how old the car is. Therefore VehYear can be dropped\n\n- **`VNZIP`**, Zipcode where car has been purchased. Has to many values and may not have any significance and could be dropped. \n\n- **`VNST`**, state should also not be a criteria for the evaluation, can be dropped\n\n- **`BYRNO`**, Number assigned to the buyer that purchased the car, can be dropped \n\n- **`VehBCost`**, One car has the pirce of 1\n\n- **`MMRA-Features`**, **`VehBCost`** are highly correlated\n\n- **`RefId`** has no Information and can be dropped","9ff9f82c":"---"}}