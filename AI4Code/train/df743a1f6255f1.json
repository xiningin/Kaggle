{"cell_type":{"2da82cdc":"code","4718ba99":"code","612c9f26":"code","e2e60d25":"code","d638144f":"code","dfab638b":"code","848f7fa0":"code","80256d44":"code","9b8ab7ec":"code","a739cba4":"code","8cb166e9":"code","6ca7cb66":"code","9d6fadab":"code","c1b7f4ab":"markdown","287537bb":"markdown","344ba77a":"markdown","5d7b821c":"markdown","2d99c863":"markdown","ea623ba8":"markdown","ed53dbe3":"markdown","66cfe9af":"markdown","65ede374":"markdown","f5084120":"markdown","3b33d7ae":"markdown","37a0faae":"markdown","752e1ce1":"markdown","2e23c7a1":"markdown","3f633b26":"markdown","263bcd20":"markdown"},"source":{"2da82cdc":"import numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport keras\nimport tensorflow as tf\nfrom keras.layers import Dense\nfrom keras.models import Sequential\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import initializers\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error as MSE\n","4718ba99":"train = pd.read_csv(\"..\/input\/cap-4611-2021-fall-assignment-3\/train.csv\")\ntest = pd.read_csv(\"..\/input\/cap-4611-2021-fall-assignment-3\/eval.csv\")","612c9f26":"print(train.describe)\nprint(test.shape)","e2e60d25":"id_col_test = test['id']\ntrain = train.drop(\"id\",axis = 1)\ntest = test.drop(\"id\",axis = 1)","d638144f":"print(train.isnull().sum().sum())\ntrain = train.dropna()\n","dfab638b":"print(train[\"pubchem_id\"].describe())\n","848f7fa0":"correlation = train[\"pubchem_id\"].corr(train[\"Eat\"])\nprint(correlation)\ntrain = train.drop(\"pubchem_id\",axis=1)\ntest = test.drop(\"pubchem_id\",axis=1)","80256d44":"    \n#def normalizeDF(df):\n #   for column in df:\n    #    if column!=\"Eat\":\n     #       dfMax = df[column].max()\n      #      df[column]=df[column]\/dfMax\n            \n            \n#normalizeDF(train)\n#normalizeDF(test)\nprint(train[\"Eat\"].describe())\n\n","9b8ab7ec":"X = train.drop(\"Eat\",axis=1)\ny = train[\"Eat\"]\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)","a739cba4":"def display_learning_curves(history):\n    plt.plot(history.history['loss'])\n    plt.title('model loss')\n    plt.ylabel('loss')\n    plt.xlabel('epoch')\n    plt.legend(['train', 'test'], loc='upper left')\n    plt.show()\nmodel = Sequential()\nlayer = layers.Dense(\n    units=40,\n    kernel_initializer=initializers.RandomNormal(stddev=0.01),\n    bias_initializer=initializers.Zeros(),\n    activation=\"relu\"\n)\nlayer2 = layers.Dense(\n    units=40,\n    kernel_initializer=initializers.RandomNormal(stddev=0.01),\n    bias_initializer=initializers.Zeros(),\n    activation=\"relu\"\n)\nlayer3 = layers.Dense(\n    units=40,\n    kernel_initializer=initializers.RandomNormal(stddev=0.01),\n    bias_initializer=initializers.Zeros(),\n    activation=\"relu\"\n)\nmodel.add(layer)\nmodel.add(layer2)\nmodel.add(layer3)\nmodel.add(Dense(1))\nmodel.compile(optimizer='adam', loss='mean_squared_error')\n\nhistory1 = model.fit(X_train,y_train,epochs=700,batch_size=1700)\nprediction = model.predict(X_test)\ntest_score = np.sqrt(MSE(y_test, prediction))\nprint(\"The Score For Model 1 is\"+str(test_score))\ndisplay_learning_curves(history1)","8cb166e9":"\nmodel2 = Sequential()\nmodel2.add(layers.Dense(\n    units=1700,\n    kernel_initializer=initializers.RandomNormal(stddev=0.01),\n    bias_initializer=initializers.Zeros(),\n    activation=\"relu\"\n))\nmodel2.add(layers.Dense(\n    units=400,\n    kernel_initializer=initializers.RandomNormal(stddev=0.01),\n    bias_initializer=initializers.Zeros(),\n    activation=\"relu\"\n))\nmodel2.add(layers.Dense(\n    units=400,\n    kernel_initializer=initializers.RandomNormal(stddev=0.01),\n    bias_initializer=initializers.Zeros(),\n    activation=\"relu\"\n))\nmodel2.add(Dense(1))\nmodel2.compile(optimizer='adam', loss='mean_squared_error')\nhistory2 = model2.fit(X_train,y_train,epochs=200,batch_size=1700)\nprediction = model2.predict(X_test)\ntest_score = np.sqrt(MSE(y_test, prediction))\ndisplay_learning_curves(history2)\n\nprint(\"The Score For Model 2 is\"+str(test_score))","6ca7cb66":"model3 = Sequential()\nfor x in range (15):\n    model3.add(layers.Dense(\n    units=20,\n    kernel_initializer=initializers.RandomNormal(stddev=0.01),\n    bias_initializer=initializers.Zeros(),\n    activation=\"relu\"))\nmodel3.add(Dense(1))\nmodel3.compile(optimizer='adam', loss='mean_squared_error')\nhistory3 = model3.fit(X_train,y_train,epochs=500,batch_size=1700)\n\n\nprediction = model3.predict(X_test)\ntest_score = np.sqrt(MSE(y_test, prediction))\ndisplay_learning_curves(history3)\nprint(\"The Score For Model 3 is\"+str(test_score))","9d6fadab":"prediction = model3.predict(test)\nprediction_df=pd.DataFrame()\nprediction_df[\"id\"]=id_col_test\nprediction_df[\"Eat\"]=prediction\nprediction_df.to_csv(\"submission.csv\",index=False)\nprint(prediction_df.to_string())","c1b7f4ab":"# Imports\nHere we import the useful libraries we will need for this assignment","287537bb":"## Checking for missing values","344ba77a":"## Pubchem ID\nThis column is not to the same scale as the other ones, lets check its distribution","5d7b821c":"# Exploratory Data Analysis\/Cleaning","2d99c863":"## Turning In The Best model\nBased on the validation scores the 3rd model is the most effective one.","ea623ba8":"# Cleaning\nHere we get rid of ID and keep it for our later submission","ed53dbe3":"# Train the model\nHere we train the models","66cfe9af":"## Normalizing the Dataset\nHere we normalize the values in the dataset so it works better with gradient descent\nThis didn't work","65ede374":"## Model 1\nThis model has a small amount of units per layer and a high batch size","f5084120":"# Loading the data\nHere we load the data into our test and train sets","3b33d7ae":"[](http:\/\/)","37a0faae":"## Model 2\nThis model has one layer that is 1700 units on the first layer and 400 on the rest","752e1ce1":"# Feature Engineering","2e23c7a1":"## Train Test Split\nHere we split our train set for validation","3f633b26":"Lets check if there is any correlation, if there isn't we will drop from the datasets","263bcd20":"## Model 3\n15 layers of 20 units"}}