{"cell_type":{"5f35a84c":"code","901959e7":"code","82d2dee5":"code","0538165f":"code","46844caa":"code","819f9345":"code","ac835026":"code","8d204ed3":"code","7a946905":"code","1148fe5e":"code","b5f532fa":"code","50ef41c9":"code","8951d249":"code","6862f136":"code","be4a224e":"code","f09c268b":"code","5a2f3d78":"code","c269d797":"code","e466e4f5":"code","40d465de":"code","de07b6fd":"code","f6369d57":"code","72054696":"code","4a465ef4":"code","45a246ca":"code","46fbc3c4":"code","e42a52c5":"code","b273748c":"code","96fd7d77":"code","9efeb800":"code","8eda7196":"code","158814d9":"code","3e835dea":"code","0ee2b8c0":"code","847518ed":"code","3a894f62":"code","5ace2454":"code","bf13891f":"code","8ece96ee":"code","b40a7bce":"code","97f5f63d":"code","68789e6e":"code","deac1200":"code","1a23f795":"code","37e31965":"code","8fc4b6ff":"code","d58a972c":"code","47cec542":"code","68c4d455":"code","120ba2ba":"code","60ab710f":"code","cf10c5f5":"code","c2ec0f00":"code","792c642b":"code","e0fc15fc":"code","e4f1aa6e":"code","48858aff":"code","0945ddbc":"code","034e8754":"markdown","f93fcfea":"markdown","d8e0488a":"markdown","bdaf3c3c":"markdown","719a3ec7":"markdown","e9d401d9":"markdown","b71dfd49":"markdown","885abf27":"markdown","80c52388":"markdown","b106beb5":"markdown","693a47b1":"markdown","c1682835":"markdown","fd627508":"markdown","6e6a08d0":"markdown","0cdca6f4":"markdown","f51b432e":"markdown","97b587dc":"markdown","87b9ab4f":"markdown","68d9927e":"markdown","1428ddd8":"markdown","1b87e8a4":"markdown"},"source":{"5f35a84c":"'''importing the required libraries\n'''\nimport pandas as pd\n\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import r2_score\nfrom sklearn.preprocessing import MinMaxScaler\n\nfrom sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LinearRegression\n\nimport statsmodels.api as sm\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Supress Warnings\n\nimport warnings\nwarnings.filterwarnings('ignore')","901959e7":"'''\n    Please update the location of the CSV file.\n    reading the dataset from the required location\n'''\ndf = pd.read_csv(r'..\/input\/carprice-assignment\/CarPrice_Assignment.csv')\ndf.describe()","82d2dee5":"df.info()","0538165f":"'''\n    splitting the column name CarName to both carname and company_name\n'''\ndf[['company_name','CarName']] = df.CarName.apply(lambda x: pd.Series(str(x).split(\" \",1)))","46844caa":"df.info()","819f9345":"'''\n    checking for the data quality in the column CarName\n'''\ndf.CarName.unique()","ac835026":"'''\n    checking for the data quality in the company_name\n'''\n\ndf.company_name.unique()","8d204ed3":"'''\n    replacing the column vaues to correct the typing mistakes to resolve the data quality issues\n'''\n\ndf['company_name'] = df['company_name'].replace('maxda', 'mazda')\ndf['company_name'] = df['company_name'].replace('Nissan', 'nissan')\ndf['company_name'] = df['company_name'].replace('porcshce', 'porsche')\ndf['company_name'] = df['company_name'].replace('toyouta', 'toyota')\ndf['company_name'] = df['company_name'].replace('vokswagen', 'volkswagen')\ndf['company_name'] = df['company_name'].replace('vw', 'volkswagen')","7a946905":"'''verifying that the data quakity issues are no longer present in the data set'''\ndf.company_name.unique()","1148fe5e":"     #start visualising\nsns.pairplot(df)\nplt.show()\n\n# we should go with linear regresssion because for few variables we can see a \n#positive co-relation between the numerical variables","b5f532fa":"#in order to visualise a categorical variable we should use a box plot\nplt.figure(figsize=(30, 18))\n\nplt.subplot(3, 4, 1)\nsns.boxplot(x = 'enginetype', y = 'price', data = df)\n\nplt.subplot(3, 4, 2)\nsns.boxplot(x = 'fueltype', y = 'price', data = df)\n\nplt.subplot(3, 4, 3)\nsns.boxplot(x = 'aspiration', y = 'price', data = df)\n\nplt.subplot(3, 4, 4)\nsns.boxplot(x = 'doornumber', y = 'price', data = df)\n\nplt.subplot(3, 4, 5)\nsns.boxplot(x = 'carbody', y = 'price', data = df)\n\nplt.subplot(3, 4, 6)\nsns.boxplot(x = 'drivewheel', y = 'price', data = df)\n\nplt.subplot(3, 4, 7)\nsns.boxplot(x = 'carbody', y = 'price', data = df)\n\nplt.subplot(3, 4, 8)\nsns.boxplot(x = 'cylindernumber', y = 'price', data = df)\n\nplt.subplot(3, 4, 9)\nsns.boxplot(x = 'fuelsystem', y = 'price', data = df)\n\nplt.subplot(3, 4, 10)\nsns.boxplot(x = 'symboling', y = 'price', data = df)\n#boxplot boundaries represents - 25%, median, 75 %","50ef41c9":"'''plotting the heatmap to find the correlation amongst the columns'''\nplt.figure(figsize=(20,12))\nsns.heatmap(df.corr(),annot=True,cmap=\"YlGnBu\")\nplt.show()","8951d249":"# creating dummy variables for all the categorical columns\n\ndummy_var = ['carbody','symboling','fuelsystem','cylindernumber','drivewheel','carbody','doornumber','aspiration',\n              'fueltype','enginetype','company_name']\ndummy_var_df = pd.get_dummies(df[dummy_var],drop_first=True)\n\ndummy_var_df.head()","6862f136":"'''now concat the dummy data frame with a main dataframe'''\ndf = pd.concat([df,dummy_var_df],axis=1)\ndf.head()","be4a224e":"'''drop the columns for which the dummy variables are already created'''\ndf = df.drop(dummy_var,axis=1)\ndf.head()","f09c268b":"'''generating the train and test data set'''\ndf_train, df_test= train_test_split(df,train_size=0.7,random_state=100)\nprint(df_train.shape)\nprint(df_test.shape)","5a2f3d78":"df_train.info()","c269d797":"'''convert the ctaegorical column enginelocation to a continuous variable'''\ndf_train.enginelocation.unique()","e466e4f5":"df_train.enginelocation.value_counts()","40d465de":"df['enginelocation'] = df['enginelocation'].replace('front', '1')\ndf['enginelocation'] = df['enginelocation'].replace('rear', '0')","de07b6fd":"# min -max scaling\n\n# 1. Instantiate the objest of the imported class\n\nscaler = MinMaxScaler()\n\nnum_variables =['wheelbase','carlength','carwidth','carheight','curbweight','enginesize','boreratio','stroke','compressionratio','horsepower','peakrpm','citympg','highwaympg','price']\n\n#2. Fit on data\ndf_train[num_variables] = scaler.fit_transform(df_train[num_variables])\ndf_train.head()","f6369d57":"#df_train = df_train[num_variables]\ndf_train.enginelocation.unique()","72054696":"y_train = df_train.pop('price')\nX_train = df_train","4a465ef4":"y_train.head()","45a246ca":"X_train.pop('CarName')\nX_train.pop('enginelocation')","46fbc3c4":"# Running RFE with the output number of the variable equal to 10\nlm = LinearRegression()\nlm.fit(X_train, y_train)\n\nrfe = RFE(lm, 10)             # running RFE\nrfe = rfe.fit(X_train, y_train)\n\nX_train.info()","e42a52c5":"list(zip(X_train.columns,rfe.support_,rfe.ranking_))","b273748c":"col = X_train.columns[rfe.support_]\ncol","96fd7d77":"X_train.columns[~rfe.support_]","9efeb800":"# Creating X_test dataframe with RFE selected variables\nX_train_rfe = X_train[col]","8eda7196":"X_train_rfe = sm.add_constant(X_train_rfe)","158814d9":"lm = sm.OLS(y_train,X_train_rfe).fit()   # Running the linear model","3e835dea":"#Let's see the summary of our linear model\nprint(lm.summary())","0ee2b8c0":"X_train_new = X_train_rfe.drop([\"cylindernumber_three\"], axis = 1)","847518ed":"X_train_lm = sm.add_constant(X_train_new)","3a894f62":"lm = sm.OLS(y_train,X_train_lm).fit()   # Running the linear model","5ace2454":"print(lm.summary())","bf13891f":"X_train_new = X_train_new.drop(['const'], axis=1)","8ece96ee":"# Calculate the VIFs for the new model\nvif = pd.DataFrame()\nX = X_train_new\nvif['Features'] = X.columns\nvif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","b40a7bce":"X_train_new = X_train_new.drop(['boreratio'], axis=1)","97f5f63d":"X_train_lm = sm.add_constant(X_train_new)\nlm = sm.OLS(y_train,X_train_lm).fit()   # Running the linear model\nprint(lm.summary())","68789e6e":"'''Dropping the company name company_name_porsche feature as it has the highest p-value'''\nX_train_new = X_train_new.drop(['company_name_porsche'], axis=1)","deac1200":"X_train_lm = sm.add_constant(X_train_new)\nlm = sm.OLS(y_train,X_train_lm).fit()   # Running the linear model\nprint(lm.summary())","1a23f795":"'''removing the curbweight feature because of high p-value'''\nX_train_new = X_train_new.drop(['curbweight'], axis=1)","37e31965":"vif = pd.DataFrame()\nX = X_train_lm\nvif['Features'] = X.columns\nvif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","8fc4b6ff":"'''dropping the feature company_name_subaru as it has very high VIF and this shows multi collinearity'''\nX_train_new = X_train_new.drop(['company_name_subaru'], axis=1)","d58a972c":"X_train_lm = sm.add_constant(X_train_new)\nlm = sm.OLS(y_train,X_train_lm).fit()   # Running the linear model\nprint(lm.summary())","47cec542":"'''dropping the feature enginetype_ohcf as it has very high p-value'''\nX_train_new = X_train_new.drop(['enginetype_ohcf'], axis=1)","68c4d455":"X_train_lm = sm.add_constant(X_train_new)\nlm = sm.OLS(y_train,X_train_lm).fit()   # Running the linear model\nprint(lm.summary())","120ba2ba":"vif = pd.DataFrame()\nX = X_train_lm\nvif['Features'] = X.columns\nvif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","60ab710f":"y_train_price = lm.predict(X_train_lm)\n# Plot the histogram of the error terms\nfig = plt.figure()\nsns.distplot((y_train - y_train_price), bins = 20)\nfig.suptitle('Error Terms', fontsize = 20)                  # Plot heading \nplt.xlabel('Errors', fontsize = 18)                         # X-label","cf10c5f5":"df_test[num_variables] = scaler.transform(df_test[num_variables])","c2ec0f00":"df_test.describe()","792c642b":"y_test = df_test.pop('price')\nX_test = df_test","e0fc15fc":"# Now let's use our model to make predictions.\n\n# Creating X_test_new dataframe by dropping variables from X_test\nX_test_new = X_test[X_train_new.columns]\n\n# Adding a constant variable \nX_test_new = sm.add_constant(X_test_new)","e4f1aa6e":"# Making predictions\ny_pred = lm.predict(X_test_new)","48858aff":"# Plotting y_test and y_pred to understand the spread.\nfig = plt.figure()\nplt.scatter(y_test,y_pred)\nfig.suptitle('y_test vs y_pred', fontsize=20)              # Plot heading \nplt.xlabel('y_test', fontsize=18)                          # X-label\nplt.ylabel('y_pred', fontsize=16)                          # Y-label","0945ddbc":"# evaluation\nr2_score(y_true=y_test, y_pred = y_pred)","034e8754":"> even from the heatmap it is visible that the correlation of price is high with the below -:\n\n- curbweight and enginesize highest\n- horsepower,carwidth,highwaympg,carlength etc","f93fcfea":"> Step 4: Building the model now","d8e0488a":"1. here we have less amount of data, hence we can not remove outliers as they can significantly imapct the calculation of p-value, and slope of the line \n2. lesser variation can be observed with the columns like doornumber \n3. fuel type and aspiration have significance variation\/impact where as drive wheel, carbody, fuelsystem, cylindernumber, drivewheel have significant impact","bdaf3c3c":"> Since, we have high values, hence we will do the rescaling\n\n> Rescaling\nAfter train and split we will do rescaling\n\n> why we do rescaling\neg here the values of price are very high comparing to the number of stroke hence the coefficeint of price will be very smaller in comparison to stroke \n\n> eg if coefficients are _ price = 0.0005 stroke = 400.\n- This does not mean that stroke is insignificant, this is one of the advantages of rescaling \n- It gives the results in range of 0-1 3) minimization routine\/ optimization increases\n\n> How to rescale ?\n1. Min-Max scaling or normalization between 0 and 1\n2. Standardization (mean -0, sigma 1)\n\n> Min-Max scaling or normalization ?\n- it says (x-xmin)\/(xmax-xmin) - this is how the value is converted to a range from 0 to 1\n- Standardization -: (x - mu)\/sigma - this gives a 0 mean and 1 sigma value","719a3ec7":"@Author: Tushar","e9d401d9":"## Making Predictions\nSimilarly Scaling the test data as well.","b71dfd49":"> Step-3 Training the model","885abf27":"- y_test vs y_pred - seems to be in a linear fashion as it should be. Because if it all they were not similar the graph would not have come along y=x line\n\n- r2_score for a prediction data set was found to be 0.790 vs (R2 and adj R2) for a test data set was 0.864 and 0.860 respectively This shows that the model has been trained properly as there is no significant drop in the R2 value\n\n- final equation would look like -: y = ax1 + bx2 + cx3 + ....+ nxn\n\n- y = 2.78*enginesize + 2.53*carwidth + 1.12*enginetype_rotor + company_name_bmw*1.08 + 7.58","80c52388":"> Step 5: Model Evaluation","b106beb5":">fit()\n- it basically computes the xmax and xmin\n- we never do the fit() on the test set so that it does not learn anything\n\n> transform()\n- It computes the (x-xmin)\/(xmax-xmin)\n- you fit the scaler on the training data set and you transform the test data set\n\n> fit_transform()\n- does both","693a47b1":"1. 'maxda', 'mazda', 'Nissan', 'nissan','porsche','porcshce','toyota', 'toyouta','vokswagen', 'volkswagen',\n\n> From the above value pairs it is clear that there are some typos in this, hence we will correct all of this.\nCorrection will include -:\nchanging maxda to mazda changing Nissan to nissan --to bring similarity in the data changing porcshce to porsche changing toyouta\nto toyota changing vokswagen to volkswagen","c1682835":"> after splitting also we have non null values, we will check for carname and company name values now","fd627508":"> Residual Analysis of the train data\n- So, now to check if the error terms are also normally distributed (which is infact, one of the major assumptions of linear regression)\n-  let us plot the histogram of the error terms and see what it looks like.","6e6a08d0":"- cylindernumber_three is insignificant and hence it can be dropped, as it has a higher p-value","0cdca6f4":"1. In most of the columns in the above graph we can see a positive co-reation, eg columns like -: wheelbase,carlength,carwidth,enginetype etc have positive or negative correlation and that too with all the columns linearly\n\n2. Hence we will go for multiple linear regression as we can see such linear patterns in almost all the columns and few of which seems to be in a straight line eg-: \n  1. engine size and horse power \n  2. wheelbase and carlength \n  3. wheelbase and curbweight\n  4. curbweight and engine size \n  5. curbweight and carlength","f51b432e":"- Now, we will be dropping the boreratio as it has the highest p-value amongst all the features and VIF is also high","97b587dc":"> The df.info shows we have no nan\/empty columns, now we will split the car name into two columns company_name and carname","87b9ab4f":"> Problem Statement\nA Chinese automobile company Geely Auto aspires to enter the US market by setting up their manufacturing unit there and \nproducing cars locally to give competition to their US and European counterparts. \nThey have contracted an automobile consulting company to understand the factors on which the pricing of cars depends. Specifically, \nthey want to understand the factors affecting the pricing of cars in the American market, since those may be very different from the\nChinese market. The company wants to know:\nWhich variables are significant in predicting the price of a car\nHow well those variables describe the price of a car\nBased on various market surveys, the consulting firm has gathered a large dataset of different types of cars across the Americal \nmarket.  \n\n> Business Goal \nYou are required to model the price of cars with the available independent variables. It will be used by the management to understand\nhow exactly the prices vary with the independent variables. They can accordingly manipulate the design of the cars, the business \nstrategy etc. to meet certain price levels. Further, the model will be a good way for management to understand the pricing dynamics\nof a new market.","68d9927e":"- we will drop the constant to find out the variance now","1428ddd8":"> now all of the company names have been corrected, lets plot the graph and see if there is any relation between the columns or not","1b87e8a4":"> now, we have the data and it is divided into the minimum columns, which were expected till this point As a next step we will look for the data cleaning, and see if we have got the correct company\/column names or not"}}