{"cell_type":{"6dc0df69":"code","429cca3c":"code","08244891":"code","e1275e6c":"code","0ad284bd":"code","6e5fc31b":"code","34cca771":"code","80e1eae2":"code","c0de999c":"code","1f948268":"code","837be526":"code","f6dc44ac":"code","05ac54a6":"code","ff8ba0e9":"code","042f6cf3":"code","1e0b9a06":"code","930a0026":"code","bd5fabaa":"code","ee0c1278":"code","0c9ad6c8":"code","425fc3ba":"code","99bc2984":"code","611a4716":"code","ea5dfd3e":"code","20eea0df":"code","e01ba102":"code","5d3587ec":"code","f38e9358":"code","29e63755":"code","3602c6c6":"code","5290cb22":"code","bfd34d4a":"code","13b572c8":"code","d069faa7":"code","dc6ecf65":"code","05b5d0bc":"code","6d13eef8":"code","21eb7fa3":"code","41867121":"code","0410cf34":"code","b5b9e857":"code","e3173120":"markdown","bdd447c7":"markdown","512481bf":"markdown","7267f80a":"markdown","47ab3f60":"markdown","1c7b940c":"markdown","69f02afd":"markdown"},"source":{"6dc0df69":"%matplotlib inline\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns; sns.set()","429cca3c":"# Condense display values for EDA.\npd.options.display.float_format = '{:,.0f}'.format","08244891":"train = pd.read_csv('..\/input\/train.csv')","e1275e6c":"train.head()","0ad284bd":"train.describe(include='all')","6e5fc31b":"# How many features are there?\nlen(train.drop(['ID_code', 'target'], axis=1).columns)","34cca771":"# Plot first 100 features.\ntrain.iloc[:, 2:100].plot(kind='box', figsize=[16,8])","80e1eae2":"# Plot last 100 features.\ntrain.iloc[:, 100:].plot(kind='box', figsize=[16,8])","c0de999c":"# Plot densities.\n# Densities are easier to visualize if we remove outliers first.\ntrain_x = train.iloc[:, 2:]\ntrain_no_outliers = train_x[train_x.apply(lambda x :(x-x.mean()).abs()<(2*x.std()) ).all(1)]","1f948268":"# Plot densities 1-100.\ntrain_no_outliers.iloc[:, :100].plot.density(ind=1000, figsize=[16,8], legend=False)","837be526":"# There is one feature that has extremely high density near 5 or 10.","f6dc44ac":"# Plot densities 100-200.\ntrain_no_outliers.iloc[:, 100:].plot.density(ind=1000, figsize=[16,8], legend=False)","05ac54a6":"# What does the target look like?\ntrain.target.value_counts().plot(kind=\"bar\")\nplt.figure()\nsns.violinplot(x=train.target.values, y=train.index.values, palette=\"husl\")\nplt.figure()\nsns.stripplot(x=train.target.values, y=train.index.values,\n              jitter=True, color=\"black\", size=0.5, alpha=0.5)","ff8ba0e9":"# Scale data\n# The plots above are hard to read. Lets scale our data.\n# Scaling also is good when doing pca. See https:\/\/stats.stackexchange.com\/a\/69159\/7167\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\ntrain_scaled = train.copy()\ntrain_scaled.iloc[:, 2:] = scaler.fit_transform(train.iloc[:, 2:])","042f6cf3":"train_scaled.plot(kind='box', figsize=[16,8])","1e0b9a06":"train_scaled.iloc[:, 2:100].plot.density(ind=30, figsize=[16,8], legend=False)","930a0026":"train_scaled.iloc[:, 100:].plot.density(ind=30, figsize=[16,8], legend=False)","bd5fabaa":"# Most correlated features\ncorrelations = train.iloc[:, 2:].corr().abs().unstack().sort_values(kind=\"quicksort\").reset_index()\ncorrelations = correlations[correlations['level_0'] != correlations['level_1']]\ncorrelations.tail(10)","ee0c1278":"# Separate out the features.\nx = train_scaled.iloc[:, 2:].values\n# Separate out the target.\ny = train_scaled.iloc[:, 1].values","0c9ad6c8":"#sns.boxplot('var_0','target',data=train, hue='target')\n# plot boxplots by target value 0, 1\n# imbar on ","425fc3ba":"from sklearn.decomposition import PCA\npca = PCA(2)\nprojected = pca.fit_transform(x)","99bc2984":"print(projected)","611a4716":"plt.scatter(projected[:, 0], projected[:, 1],\n           c=y, edgecolor='none', alpha=0.5,\n           cmap=plt.cm.get_cmap('copper', 10))\nplt.xlabel('component 1')\nplt.ylabel('component 2')\nplt.title('Santander 2d PCA scaled')\nplt.colorbar();","ea5dfd3e":"pca = PCA().fit(x)\nplt.plot(np.cumsum(pca.explained_variance_ratio_))\nplt.xlabel('number of components')\nplt.ylabel('cumulative explained variance');\nplt.title('Santander scaled PCA cumulative explained variance')","20eea0df":"# The 100 most descriptive features explain 50% of the variance.","e01ba102":"# Let's try randomized pca to ignore outliers. Pick the 100 most descriptive features for rpca.","5d3587ec":"rpca = PCA(n_components=100, svd_solver='randomized')\nrpca.fit(x)","f38e9358":"plt.plot(np.cumsum(rpca.explained_variance_ratio_))\nplt.xlabel('number of components')\nplt.ylabel('cumulative explained variance');\nplt.title('Santander scaled Radomized PCA cumulative explained variance')","29e63755":"# Ignoring outliers doesn't reduce the number of features we need for explanation.\n# This dataset is likely the 200 principle components of some larger dataset.","3602c6c6":"# PCA without scaling.\n# If we don't scale we can't visualize the correlation between component 1 and the target.\n# This is still good to look at as we will visualize that we can get\n# 90% cumulative explained variance with 100 unscaled features.\n\nx_raw = train.iloc[:, 2:].values\ny_raw = train.iloc[:, 1].values\npca_raw = PCA(2)\nprojected_raw = pca_raw.fit_transform(x_raw)\nplt.scatter(projected_raw[:, 0], projected_raw[:, 1],\n           c=y, edgecolor='none', alpha=0.5,\n           cmap=plt.cm.get_cmap('copper', 10))\nplt.xlabel('component 1')\nplt.ylabel('component 2')\nplt.title('Santander 2d PCA unscaled')\nplt.colorbar();\n\nplt.figure()\npca_raw = PCA().fit(x_raw)\nplt.plot(np.cumsum(pca_raw.explained_variance_ratio_))\nplt.xlabel('number of components')\nplt.ylabel('cumulative explained variance');\nplt.title('Santander unscaled PCA cumulative explained variance')\n\nplt.figure()\nrpca_raw = PCA(n_components=100, svd_solver='randomized')\nrpca_raw.fit(x_raw)\nplt.plot(np.cumsum(rpca_raw.explained_variance_ratio_))\nplt.xlabel('number of components')\nplt.ylabel('cumulative explained variance');\nplt.title('Santander unscaled Radomized PCA cumulative explained variance')","5290cb22":"import lightgbm as lgb\nfrom sklearn.model_selection import train_test_split","bfd34d4a":"# Create stratified validation split.\n# Stratifying makes the splits have the same class distribution (purchase\/no-purchase).\ntrain_x, validation_x, train_y, validation_y = train_test_split(x, y, stratify=y)","13b572c8":"train_data = lgb.Dataset(train_x, label=train_y)","d069faa7":"validation_data = lgb.Dataset(validation_x, label=validation_y, reference=train_data)","dc6ecf65":"bst = lgb.train({\n    'boosting': 'gbdt', #'dart', # Dropouts meet Multiple Additive Regression Trees, default='gbdt'\n    'learning_rate': 0.01, # smaller increases accuracy, default=0.1\n    'max_bin': 511, # larger increases accuracy, default=255\n    'metric': 'auc',\n    'num_leaves': 63, # larger increases accuracy, default=31\n    'num_trees': 100,\n    'num_iteration': 500, # default=100\n    'objective': 'binary',\n    },\n    train_data,\n    num_boost_round=500, # may be redundant with params#num_iteration\n    valid_sets=[validation_data],\n    early_stopping_rounds=100,\n    verbose_eval=100, # logs every 100 trees\n)","05b5d0bc":"bst.save_model('model.txt', num_iteration=bst.best_iteration)","6d13eef8":"lgb.plot_importance(bst, figsize=(16,8))","21eb7fa3":"lgb.create_tree_digraph(bst)","41867121":"# Generate submission\ntest = pd.read_csv('..\/input\/test.csv')\ntest_x = test.iloc[:, 1:].values # Drop the ID_code\nypred = bst.predict(test_x)\ntest_code = test.iloc[:, 0]\nsubmission = pd.concat([test_code, pd.Series(ypred, name='target')], axis=1)\nsubmission.to_csv('submissions.csv', index=False)\nsubmission.head()","0410cf34":"nunique  = train.nunique()","b5b9e857":"!head submissions.csv","e3173120":"TODO\n* https:\/\/jakevdp.github.io\/PythonDataScienceHandbook\/05.13-kernel-density-estimation.html\n* https:\/\/jakevdp.github.io\/PythonDataScienceHandbook\/05.11-k-means.html","bdd447c7":"Score is 0.863 on leaderboard (lb).","512481bf":"We have class imbalanced class problem.","7267f80a":"## PCA\n\nBorrowed from DataScience handbook Chapter 5\n\nhttps:\/\/jakevdp.github.io\/PythonDataScienceHandbook\/05.09-principal-component-analysis.html","47ab3f60":"## Decision Tree","1c7b940c":"The most descriptive feature in the dataset (component 1) is positively correlated with the target!","69f02afd":"# _**\ud83c\udf39\ud83c\udfc6RoseGold \ud83c\udfc6\ud83c\udf39**_\n\nContents:\n1. Exploratory Data Analysis\n2. Principle Components Analysis\n3. Build LightGBM model"}}