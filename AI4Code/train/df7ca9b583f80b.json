{"cell_type":{"8286934a":"code","c8ad618a":"code","53a78578":"code","ce7c9e53":"code","ec25166f":"code","0570c8c8":"code","d4227a36":"code","17ce263a":"code","5801a10e":"code","c97b536e":"code","7911e71e":"code","f6d21124":"code","8b6755b1":"code","817d5524":"code","538dbf98":"code","eb016142":"code","f3a55b48":"code","cc571d8f":"code","fbb02c96":"markdown","7ce84015":"markdown","b14c1b69":"markdown","eae84b91":"markdown","44a9aa2e":"markdown","38916c6c":"markdown","bda14ef3":"markdown","ccfafd4c":"markdown","b13985a6":"markdown","18af084e":"markdown","0fa5b383":"markdown","9c50fd6e":"markdown","68c7a26d":"markdown","ca5b2641":"markdown","3355d00c":"markdown","83ce68dd":"markdown","69dc3afb":"markdown","db5736f4":"markdown","c0f35708":"markdown","7c0a3887":"markdown"},"source":{"8286934a":"#!pip install --upgrade Pillow","c8ad618a":"import cv2\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom skimage.filters import threshold_otsu\nimport numpy as np\nfrom glob import glob\nimport scipy.misc\nfrom matplotlib.patches import Circle,Ellipse\nfrom matplotlib.patches import Rectangle\nimport os\nfrom PIL import Image\nimport keras\nfrom matplotlib import pyplot as plt\nimport numpy as np\nimport gzip\n%matplotlib inline\nfrom keras.layers import Input,Conv2D,MaxPooling2D,UpSampling2D\nfrom keras.models import Model\nfrom keras.optimizers import RMSprop\nfrom keras.layers.normalization import BatchNormalization\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","53a78578":"import imageio\ndata = glob('..\/input\/fvc2002-fingerprints\/fingerprints\/DB*\/*')\nimages = []\nfrom matplotlib.pyplot import imread\n\ndef readImages(data):\n    for i in range(len(data)):\n        img = cv2.imread(data[i])\n        img = cv2.resize(img,(224,224))\n        images.append(img)\n        \n    return images\n\nimages = readImages(data)","ce7c9e53":"images_arr = np.asarray(images)\nimages_arr = images_arr.astype('float32')\nimages_arr.shape","ec25166f":"print(\"Dataset (images) shape: {shape}\".format(shape=images_arr.shape))","0570c8c8":"\n# Display random images in training data\nprint(\"Display random images in training data\")\nfor i in range(20,25):\n    plt.figure(figsize = (12,12))\n    plt.subplot(5, 5, i+1)\n    plt.imshow(cv2.cvtColor(images_arr[i], cv2.COLOR_BGR2RGB))\n    plt.axis('off')\n    plt.tight_layout()\n    plt.show()\n\"\"\"\nx = \"..\/input\/fvc2002-fingerprints\/fingerprints\/DB1_B\/\"\ndef plotImages(title,directory):\n    print(title)\n    plt.figure(figsize = (12,12))\n    for i in range(25):\n        plt.subplot(5, 5, i+1)\n        img = cv2.imread( directory+ \"\/\" + x[i])\n        plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB)); plt.axis('off')\n        plt.tight_layout()\n    plt.show()\n        \nplotImages(\"Images of malaria infected cells\",\"..\/input\/fvc2002-fingerprints\/fingerprints\/DB1_B\/\") \"\"\"\n","d4227a36":"images_arr = images_arr.reshape(-1, 224,224, 1)\nimages_arr.shape","17ce263a":"images_arr.dtype\n","5801a10e":"np.max(images_arr)\nimages_arr = images_arr \/ np.max(images_arr)","c97b536e":"np.max(images_arr), np.min(images_arr)\n","7911e71e":"from sklearn.model_selection import train_test_split\ntrain_X,valid_X,train_ground,valid_ground = train_test_split(images_arr,images_arr,test_size=0.2,random_state=13)","f6d21124":"batch_size = 128\nepochs = 300\nx, y = 224, 224\ninput_img = Input(shape = (x, y, 1))","8b6755b1":"def autoencoder(input_img):\n    #encoder\n    #input = 28 x 28 x 1 (wide and thin)\n    conv1 = Conv2D(32, (3, 3), activation='relu', padding='same')(input_img) #28 x 28 x 32\n    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1) #14 x 14 x 32\n    conv2 = Conv2D(64, (3, 3), activation='relu', padding='same')(pool1) #14 x 14 x 64\n    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2) #7 x 7 x 64\n    conv3 = Conv2D(128, (3, 3), activation='relu', padding='same')(pool2) #7 x 7 x 128 (small and thick)\n\n    #decoder\n    conv4 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv3) #7 x 7 x 128\n    up1 = UpSampling2D((2,2))(conv4) # 14 x 14 x 128\n    conv5 = Conv2D(64, (3, 3), activation='relu', padding='same')(up1) # 14 x 14 x 64\n    up2 = UpSampling2D((2,2))(conv5) # 28 x 28 x 64\n    decoded = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(up2) # 28 x 28 x 1\n    return decoded\n\nautoencoder = Model(input_img, autoencoder(input_img))","817d5524":"autoencoder.compile(loss='mean_squared_error', optimizer = RMSprop())","538dbf98":"autoencoder.summary()","eb016142":"#Training\nautoencoder_train = autoencoder.fit(train_X, train_ground, batch_size=batch_size,epochs=epochs,verbose=1,validation_data=(valid_X, valid_ground))","f3a55b48":"\nloss = autoencoder_train.history['loss']\nval_loss = autoencoder_train.history['val_loss']\nepochs = range(300)\nplt.figure()\nplt.figure(figsize=(15,10))\nplt.plot(epochs, loss, 'b', label='Training loss')\nplt.plot(epochs, val_loss, 'r', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\nplt.show()\n","cc571d8f":"#Prediction\npred = autoencoder.predict(valid_X)#Reconstruction of Test Images\nplt.figure(figsize=(20, 4))\nprint(\"Test Images\")\nfor i in range(5):\n    plt.subplot(1, 5, i+1)\n    plt.imshow(valid_ground[i, ..., 0], cmap='gray')\nplt.show()    \nplt.figure(figsize=(20, 4))\nprint(\"Reconstruction of Test Images\")\nfor i in range(5):\n    plt.subplot(1, 5, i+1)\n    plt.imshow(pred[i, ..., 0], cmap='gray')  \nplt.show()","fbb02c96":"In order for your model to generalize well, you split the data into two parts: training and a validation set. You will train your model on 80% of the data and validate it on 20% of the remaining training data.\n\n\nThis will also help you in reducing the chances of overfitting, as you will be validating your model on data it would not have seen in the training phase.","7ce84015":"Let\u2019s visualize the layers that you created in the above step by using the summary function; this will show a number of parameters (weights and biases) in each layer and also the total parameters in your model.","b14c1b69":"#### Now convert these images into a float32 array.","eae84b91":"We don\u2019t need training and testing labels that\u2019s why we will pass the training images twice. Our training images will both act as the input as well as the ground truth similar to the labels you have in the classification task.\n\n\nNow we are all set to define the network and feed the data into the network.\n\n\nThe Convolutional Autoencoder\n\nThe images are of size 224 x 224 x 1 or a 50,176-dimensional vector. We convert the image matrix to an array, rescale it between 0 and 1, reshape it so that it\u2019s of size 224 x 224 x 1, and feed this as an input to the network.\n\n\nAlso, we will use a batch size of 128 using a higher batch size of 256 or 512 is also preferable it all depends on the system you train your model. It contributes heavily in determining the learning parameters and affects the prediction accuracy. We will train your network for 300 epochs.","44a9aa2e":"It\u2019s finally time to train the model with Keras\u2019 fit() function! The model trains for 300 epochs.","38916c6c":"### Data Exploration","bda14ef3":"From the above figures, you can observe that your model did a fantastic job of reconstructing the test images that you predicted using the model. At least visually, the test and the reconstructed images look almost similar.","ccfafd4c":"From the above output, you can see that the data has a shape of 320 x 224 x 224 since there are 320 samples each of the 224 x 224-dimensional matrix.\n\n\nTake a look at the first 5 images in our dataset:","b13985a6":"Once you have the data loaded properly, you are all set to analyze it in order to get some intuition about the dataset.","18af084e":"Finally! You trained the model on the fingerprint dataset for 300 epochs, Now, let\u2019s plot the loss plot between training and validation data to visualize the model performance.\n\n","0fa5b383":"### Building a Convolutional Autoencoder\n\nAfter having an overview of the fingerprint, its features, it is time to utilize our newly developed skill to build a Neural network that is capable of recreating or reconstructing fingerprint images.\nSo, first of all, we\u2019ll explore the dataset including what kind of images it has, how to read the images, how to create an array of the images, exploring the fingerprint images, and finally preprocessing them to be able to feed them in the model.\n\n\nI have used a convolutional autoencoder for training the model. Next, we will visualize the training and validation loss plot and finally predict the test set.\n\n\nHere I\u2019m assuming you guys are comfortable with Convoluti?onal Neural Networks and AutoEncoders. Anyway, I\u2019ll try to explain them as a oneliner.\n\n\nA Convolutional neural network (CNN) is a neural network that has one or more convolutional layers and is used mainly for image processing, classification, and segmentation.\n\n\n### OK. What's an AutoEncoder?\n\n\nAutoencoders are a family of Neural Networks for which the input is the same as the output. They work by compressing the input into a latent-space representation and then reconstructing the output from this representation. Check this out for more.\n\n\n### Now, what're a Convolutional Autoencoders?\n\n\nThe convolution operator allows filtering an input signal in order to extract some part of its content. Autoencoders in their traditional formulation do not take into account the fact that a signal can be seen as a sum of other signals. Convolutional Autoencoders, instead, use the convolution operator to exploit this observation. They learn to encode the input in a set of simple signals and then try to reconstruct the input from them. For more check [this out](https:\/\/pgaleone.eu\/neural-networks\/2016\/11\/24\/convolutional-autoencoders\/).\n\n","9c50fd6e":"# Recreating Fingerprints using Convolutional Autoencoders","68c7a26d":"As you might already know well before, the autoencoder is divided into two parts: there are an encoder and a decoder.\n\n\nEncoder\n\nThe first layer will have 32 filters of size 3 x 3, followed by a downsampling (max-pooling) layer,\n\nThe second layer will have 64 filters of size 3 x 3, followed by another downsampling layer,\n\nThe final layer of the encoder will have 128 filters of size 3 x 3.\n\nDecoder\n\nThe first layer will have 128 filters of size 3 x 3 followed by an upsampling layer,\n\nThe second layer will have 64 filters of size 3 x 3 followed by another upsampling layer,\n\nThe final layer of the encoder will have one filter of size 3 x 3.\n\nThe max-pooling layer will downsample the input by two times each time you use it, while the upsampling layer will upsample the input by two times each time it is used.\n\n\nNote: The number of filters, the filter size, the number of layers, number of epochs you train your model, are all hyperparameters and should be decided based on your own intuition, you are free to try new experiments by tweaking with these hyperparameters and measure the performance of your model. And that is how you will slowly learn the art of deep learning!","ca5b2641":"Note that you also have to specify the loss type via the argument loss. In this case, that\u2019s the mean squared error, since the loss after every batch will be computed between the batch of predicted output and the ground truth using mean squared error pixel by pixel:","3355d00c":"### Fingerprints: As Unique as You\n![iS21U6E-e1557599507335-740x556.jpg](attachment:iS21U6E-e1557599507335-740x556.jpg)\n\nImagine you misplaced your smartphone and start panicking because there\u2019s a lot of personal information on that phone. You\u2019re worried because you don\u2019t want whoever picks it up to be able to access it. But then you remember that you secured it so that no one could use it, should this scenario ever arise. The only person your phone will unlock for is you, and it knows it\u2019s you because you use your fingerprint.\n\n\nFingerprints and also toe prints can be used to identify a single individual because they are unique to each person and they do not change over time. Amazingly, even identical twins have fingerprints that are different from each other, and none of your fingers have the same print as the others. Fingerprints consist of ridges, which are the raised lines, and furrows, which are the valleys between those lines. And it\u2019s the pattern of those ridges and furrows that are different for everyone.\n\n\nThe patterns of the ridges are what is imprinted on a surface when your finger touches it. If you get fingerprinted the ridges are printed on the paper and can be used to match fingerprints you might leave elsewhere.\n","83ce68dd":"### Prediction\nFinally, you can see that the validation loss and the training loss both are in sync. It shows that your model is not overfitting: the validation loss is decreasing and not increasing,\n\n\nTherefore, you can say that your model\u2019s generalization capability is good.\n\n\nFinally, it\u2019s time to reconstruct the test images using the `predict()` function of Keras and see how well your model is able to reconstruct the test data.\n","69dc3afb":"The dataset that I\u2019m using is the FVC2002 fingerprint dataset. It consists of 4 different sensor fingerprints namely Low-cost Optical Sensor, Low-cost Capacitive Sensor, Optical Sensor and Synthetic Generator, each sensor having varying image sizes. The dataset has 320 images, 80 images per sensor.","db5736f4":"#### Load all the required libraries.","c0f35708":"As we can see that the fingerprints are not very clear, it will be interesting to see if the convolutional autoencoder is able to learn the features and is able to reconstruct these images properly.\n\n\nThe images of the dataset are grayscale images with pixel values ranging from 0 to 255 having a dimension of 224 x 224, so before we feed the data into the model, it is very important to preprocess it. We\u2019ll first convert each 224 x 224 image of the dataset into a matrix of size 224 x 224 x 1. 1 for Grayscale image, which we can then feed into the Neural Network:","7c0a3887":"### Characteristics of Fingerprints\n\nThe first encounter with fingerprints makes them look complicated. They may leave you wondering how forensic and law enforcement people make use of them. Fingerprints may look complicated, but the fact is that they have general ridge patterns and each person\u2019s fingerprint is unique making it possible to systematically classify them.\n\n\nFingerprints have three basic ridge patterns: \u201cArch\u201d, \u201cLoop\u201d and \u201cWhorl\/Core\u201d.\n\n![3eee0b_4bb9ce2a485e4e02ac1673fc863e4b5f_mv2.webp](attachment:3eee0b_4bb9ce2a485e4e02ac1673fc863e4b5f_mv2.webp)\n\nIn this pattern type, ridges enter on one side and exit on the other side. 5% of the total world\u2019s population is believed to have arches in their fingerprints.\n\n\n![3eee0b_78f87f077c504c0b819877e81fce8509_mv2.webp](attachment:3eee0b_78f87f077c504c0b819877e81fce8509_mv2.webp)\n\n\nThis pattern type has ridges entering on one side and exiting on the same side. 60\u201365% of the world\u2019s population is believed to have loops in their fingerprints.\n\n\n![3eee0b_98f101a52b694991bea27581141dcde2_mv2.webp](attachment:3eee0b_98f101a52b694991bea27581141dcde2_mv2.webp)\n\nConsists of circles, more than one loop, or a mixture of pattern type. 30\u201335% of the world\u2019s population is believed to have whorls in their fingerprints.\n\n\nThe uniqueness of a fingerprint is exclusively determined by the local ridge characteristics and their relationships. The ridges and valleys in a fingerprint alternate, flowing in a local constant direction. The two most prominent local ridge characteristics are: 1) ridge ending and, 2) ridge bifurcation. A ridge ending is defined as the point where a ridge ends abruptly. A ridge bifurcation is defined as the point where a ridge forks or diverges into branch ridges. Collectively, these features are called minutiae.\n\n"}}