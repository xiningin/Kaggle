{"cell_type":{"0897604f":"code","2d303248":"code","f577219e":"code","dda7a39d":"code","fa62bcea":"code","3c579ffc":"code","ceb44765":"code","00c0ce80":"code","ab776d6b":"code","edc424a3":"code","467179c1":"code","fe2eb509":"code","3d71b673":"code","53b3e5a1":"code","fef8f8ea":"code","95c59b31":"code","97905fa7":"code","30573dd9":"code","6b209c57":"code","6f7e026c":"code","36bd23b8":"code","68a58e37":"code","4ceb4a3d":"code","09895b6a":"code","8d0ed5c5":"code","79fa2642":"code","4444493a":"code","df41ab08":"code","13439a01":"code","0f31db0c":"code","2e99e577":"code","65b8cb96":"code","dd51e0f6":"code","9f4b83af":"code","f5973478":"code","d2a8e6fa":"code","3158a686":"code","04aee812":"code","9bba2c00":"code","4ee20618":"code","ee7c3c8a":"code","b7c218db":"code","0c75040b":"code","ec9a9c6c":"code","9f139940":"code","bbd69cd9":"code","0479a088":"code","8d777f6b":"code","6a5b62b2":"markdown","0ab434ff":"markdown","2752492e":"markdown","d7cbc21f":"markdown","9f2a8f44":"markdown","e61e2b49":"markdown","235c344d":"markdown","ea7180c9":"markdown","6aca343f":"markdown","c3d4c72a":"markdown","f10fea42":"markdown"},"source":{"0897604f":"# For local time and timing the runtime of the whole notebook\n\nfrom datetime import datetime\nimport pytz\nimport datetime\nIST = pytz.timezone('Asia\/Kolkata')\nstart=datetime.datetime.now(IST).replace(microsecond=0)\ns_hrs=start.time()\nprint(\"Notebook started on:\",start.date(),\"at\",s_hrs,\"hrs\")","2d303248":"print(\"Command execution started at:\"+str(datetime.datetime.now(IST).replace(microsecond=0).time())+\" hrs\")\n\nimport sys\n!cp ..\/input\/rapids\/rapids.0.19.0 \/opt\/conda\/envs\/rapids.tar.gz\n!cd \/opt\/conda\/envs\/ && tar -xzvf rapids.tar.gz > \/dev\/null\nsys.path = [\"\/opt\/conda\/envs\/rapids\/lib\/python3.7\/site-packages\"] + sys.path\nsys.path = [\"\/opt\/conda\/envs\/rapids\/lib\/python3.7\"] + sys.path\nsys.path = [\"\/opt\/conda\/envs\/rapids\/lib\"] + sys.path \n!cp \/opt\/conda\/envs\/rapids\/lib\/libxgboost.so \/opt\/conda\/lib\/\n\nprint(\"\\nCommand execution ended at:\"+str(datetime.datetime.now(IST).replace(microsecond=0).time())+\" hrs\")","f577219e":"print(\"Command execution started at:\"+str(datetime.datetime.now(IST).replace(microsecond=0).time())+\" hrs\\n\")\n\n!nvidia-smi\n\nprint(\"\\nCommand execution ended at:\"+str(datetime.datetime.now(IST).replace(microsecond=0).time())+\" hrs\")","dda7a39d":"# Installing gdown library for importing data from Google Drive\nprint(\"Command execution started at:\"+str(datetime.datetime.now(IST).replace(microsecond=0).time())+\" hrs\\n\")\n\n!conda install -y gdown \nimport gdown \n\nprint(\"\\nCommand execution ended at:\"+str(datetime.datetime.now(IST).replace(microsecond=0).time())+\" hrs\")","fa62bcea":"# Importing files from Google Drive \nprint(\"Command execution started at:\"+str(datetime.datetime.now(IST).replace(microsecond=0).time())+\" hrs\\n\")\n\n!gdown https:\/\/drive.google.com\/uc?id=15r0vqWk7JyUazZxCDAQg5Kb1qVPuIB1w #1.csv\n!gdown https:\/\/drive.google.com\/uc?id=1HFhEwWFd6dWVos-PVlQT-MqAksm6eZk1 #2.csv\n!gdown https:\/\/drive.google.com\/uc?id=1ZO1VqaqtuULqQy8QnLKfyJtxN37eOKN3 #3.csv\n!gdown https:\/\/drive.google.com\/uc?id=1xjX-F3iRYxDJ4KIpFO2s9xSBmLj0bvyX #4.csv\n!gdown https:\/\/drive.google.com\/uc?id=1EdI84fMIeSnV-wL90N5fg7gQNHyM43AF #5.csv\n!gdown https:\/\/drive.google.com\/uc?id=1swoBNbc590Y2iEyvuygahe9Eq6zJdWZZ #6.csv\n    \nprint(\"\\nCommand execution ended at:\"+str(datetime.datetime.now(IST).replace(microsecond=0).time())+\" hrs\")","3c579ffc":"# Standard Imports\nprint(\"Command execution started at:\"+str(datetime.datetime.now(IST).replace(microsecond=0).time())+\" hrs\")\n\nimport pandas\nimport os\nimport numpy \nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nprint(\"\\nCommand execution ended at:\"+str(datetime.datetime.now(IST).replace(microsecond=0).time())+\" hrs\")","ceb44765":"print(\"Command execution started at:\"+str(datetime.datetime.now(IST).replace(microsecond=0).time())+\" hrs\")\n\nimport cudf as pd\n\nimport cuml\n\nimport cupy as np\n\nimport io, requests\n\nprint(\"\\nCommand execution ended at:\"+str(datetime.datetime.now(IST).replace(microsecond=0).time())+\" hrs\")","00c0ce80":"# Assigning files\nprint(\"Command execution started at:\"+str(datetime.datetime.now(IST).replace(microsecond=0).time())+\" hrs\")\n\naisles=pd.read_csv(\".\/1.csv\")\n\ndept=pd.read_csv(\".\/2.csv\")\n\nord_prior=pd.read_csv(\".\/3.csv\")\n\norders=pd.read_csv(\".\/5.csv\")\n\nprod=pd.read_csv(\".\/6.csv\")\n\nprint(\"\\nCommand execution ended at:\"+str(datetime.datetime.now(IST).replace(microsecond=0).time())+\" hrs\")","ab776d6b":"# Column names\n\nprint(\"Command execution started at:\"+str(datetime.datetime.now(IST).replace(microsecond=0).time())+\" hrs\\n\")\n\nprint(aisles.columns)\nprint(dept.columns)\nprint(orders.columns)\nprint(prod.columns)\nprint(ord_prior.columns)\n\nprint(\"\\nCommand execution ended at:\"+str(datetime.datetime.now(IST).replace(microsecond=0).time())+\" hrs\")","edc424a3":"# Merging Aisles and Department tables into Product tables\n\nprint(\"Command execution started at:\"+str(datetime.datetime.now(IST).replace(microsecond=0).time())+\" hrs\\n\")\n\nprod_dep=pd.merge( prod, dept, how='left', on='department_id')\nprod_fin=pd.merge(prod_dep, aisles, how='left', on='aisle_id')\nprint(prod_fin.isnull().sum())\ndel prod, dept, prod_dep, aisles\n\nprint(\"\\nCommand execution ended at:\"+str(datetime.datetime.now(IST).replace(microsecond=0).time())+\" hrs\")","467179c1":"# Confirming the dtypes before merging for compatibility\n\nprint(\"Command execution started at:\"+str(datetime.datetime.now(IST).replace(microsecond=0).time())+\" hrs\\n\")\n\nprint(\"Orders Prior Datatypes:\\n\",ord_prior.dtypes)\nprint(\"\\nProduct Datatypes:\\n\",prod_fin.dtypes)\n\nprint(\"\\nCommand execution ended at:\"+str(datetime.datetime.now(IST).replace(microsecond=0).time())+\" hrs\")","fe2eb509":"# Verifying the variables still in the memory\n\nprint(\"Command execution started at:\"+str(datetime.datetime.now(IST).replace(microsecond=0).time())+\" hrs\\n\")\n\n%whos DataFrame\n\nprint(\"\\nCommand execution ended at:\"+str(datetime.datetime.now(IST).replace(microsecond=0).time())+\" hrs\")","3d71b673":"# Merging the Orders Prior and Product tables\n\nprint(\"Command execution started at:\"+str(datetime.datetime.now(IST).replace(microsecond=0).time())+\" hrs\\n\")\n\nord_prod=pd.merge(ord_prior, prod_fin, how='left', on='product_id')\ndel ord_prior, prod_fin\n\nprint(\"\\nCommand execution ended at:\"+str(datetime.datetime.now(IST).replace(microsecond=0).time())+\" hrs\")","53b3e5a1":"print(\"Command execution started at:\"+str(datetime.datetime.now(IST).replace(microsecond=0).time())+\" hrs\\n\")\n\nprint(ord_prod.head(10))\nprint(\"\\n\\n\")\nprint(orders.head())\n\nprint(\"\\nCommand execution ended at:\"+str(datetime.datetime.now(IST).replace(microsecond=0).time())+\" hrs\")","fef8f8ea":"# Checking dtypes for compatibility and null values\n\nprint(\"Command execution started at:\"+str(datetime.datetime.now(IST).replace(microsecond=0).time())+\" hrs\\n\")\n\nprint(\"Orders Prior Datatypes:\\n\")\nprint(ord_prod.dtypes)\nprint(\"\\nOrders Datatypes:\\n\")\nprint(orders.dtypes)\n\nprint(\"\\nCommand execution ended at:\"+str(datetime.datetime.now(IST).replace(microsecond=0).time())+\" hrs\")","95c59b31":"# Merging Orders prior and Orders tables\n\nprint(\"Command execution started at:\"+str(datetime.datetime.now(IST).replace(microsecond=0).time())+\" hrs\")\n\ndata=pd.merge(ord_prod, orders, how='left', on='order_id')\ndel ord_prod, orders\n\nprint(\"\\nCommand execution ended at:\"+str(datetime.datetime.now(IST).replace(microsecond=0).time())+\" hrs\")","97905fa7":"# Removing Na Values from the final data table\n\nprint(\"Command execution started at:\"+str(datetime.datetime.now(IST).replace(microsecond=0).time())+\" hrs\\n\")\n\nprint(\"Number of NaN Values present in the dataset: \\n\")\nprint(data.isnull().sum())\nprint(\"\\n**************************************\")\nprint(\"           REMOVING NaN VALUES\")\nprint(\"**************************************\")\ndata.dropna(axis=0, inplace=True)\nprint(\"\\nNumber of NaN Values present in the dataset: \\n\")\nprint(data.isnull().sum())\n\nprint(\"\\nCommand execution ended at:\"+str(datetime.datetime.now(IST).replace(microsecond=0).time())+\" hrs\")","30573dd9":"# Verifying the variables still in the memory\n\nprint(\"Command execution started at:\"+str(datetime.datetime.now(IST).replace(microsecond=0).time())+\" hrs\\n\")\n\n%who_ls\n\nprint(\"\\nCommand execution ended at:\"+str(datetime.datetime.now(IST).replace(microsecond=0).time())+\" hrs\")","6b209c57":"# Verifying that all the columns required have been suceessfully merged\n\nprint(\"Command execution started at:\"+str(datetime.datetime.now(IST).replace(microsecond=0).time())+\" hrs\\n\")\n\nprint(data.columns)\nprint(\"\\n*******************\\n\")\nprint(data.dtypes)\n\nprint(\"\\nCommand execution ended at:\"+str(datetime.datetime.now(IST).replace(microsecond=0).time())+\" hrs\")","6f7e026c":"# Getting the final table output\n\nprint(\"Command execution started at:\"+str(datetime.datetime.now(IST).replace(microsecond=0).time())+\" hrs\\n\")\n\n# data.head(10000).to_csv(\"Sample.csv\")\nprint(data.head(10000))\n\nprint(\"Command execution ended at:\"+str(datetime.datetime.now(IST).replace(microsecond=0).time())+\" hrs\")","36bd23b8":"# Droping \"eval_set\" column from the dataset\n\nprint(\"Command execution started at:\"+str(datetime.datetime.now(IST).replace(microsecond=0).time())+\" hrs\\n\")\n\ndata=data.drop(\"eval_set\",axis=1)\nprint(data)\n\nprint(\"Command execution ended at:\"+str(datetime.datetime.now(IST).replace(microsecond=0).time())+\" hrs\")","68a58e37":"print(\"Command execution started at:\"+str(datetime.datetime.now(IST).replace(microsecond=0).time())+\" hrs\\n\")\n\ndata=data.to_pandas()\n\nprint(\"Command execution ended at:\"+str(datetime.datetime.now(IST).replace(microsecond=0).time())+\" hrs\")","4ceb4a3d":"feature=\"aisle_id\"\nfeature","09895b6a":"print(\"Command execution started at:\"+str(datetime.datetime.now(IST).replace(microsecond=0).time())+\" hrs\\n\")\n\n# data.pivot_table(df, values=['D', 'E'], index=['A', 'C'],aggfunc={'D': np.mean,'E': np.mean})   Syntax\n\ncord_id=(data.pivot_table(index=['aisle_id'], \n                          values=['order_id'], \n                          aggfunc={'order_id':'count'})).reset_index()\navord_hod=(data.pivot_table(index=['aisle_id'], \n                              values=['order_hour_of_day'], \n                              aggfunc={'order_hour_of_day':np.mean})).reset_index()\navord_dow=(data.pivot_table(index=['aisle_id'], \n                              values=['order_dow'], \n                              aggfunc={'order_dow':np.mean})).reset_index()\navord_dspo=(data.pivot_table(index=['aisle_id'], \n                               values=['days_since_prior_order'], \n                               aggfunc={'days_since_prior_order':np.mean})).reset_index()\navaddtc=(data.pivot_table(index=['aisle_id'], \n                            values=['add_to_cart_order'], \n                            aggfunc={'add_to_cart_order':np.mean})).reset_index()\nminaddtc=(data.pivot_table(index=['aisle_id'], \n                             values=['add_to_cart_order'], \n                             aggfunc={'add_to_cart_order':np.min})).reset_index()\nmaxaddtc=(data.pivot_table(index=['aisle_id'], \n                             values=['add_to_cart_order'], \n                             aggfunc={'add_to_cart_order':np.max})).reset_index()\nmaxord_dspo=(data.pivot_table(index=['aisle_id'], \n                                values=['days_since_prior_order'], \n                                aggfunc={'days_since_prior_order':np.max})).reset_index()\n\n\nprint(\"Command execution ended at:\"+str(datetime.datetime.now(IST).replace(microsecond=0).time())+\" hrs\")","8d0ed5c5":"print(\"Command execution started at:\"+str(datetime.datetime.now(IST).replace(microsecond=0).time())+\" hrs\\n\")\n\ndept_piv=pd.merge(cord_id, avord_hod, how='left', on='aisle_id')\ndept_piv=pd.merge(dept_piv, avord_dow, how='left', on='aisle_id')\ndept_piv=pd.merge(dept_piv, avord_dspo, how='left', on='aisle_id')\ndept_piv=pd.merge(dept_piv, avaddtc, how='left', on='aisle_id')\ndept_piv=pd.merge(dept_piv, maxaddtc, how='left', on='aisle_id')\n\ndel cord_id, avord_hod, avord_dow, avord_dspo, avaddtc, maxaddtc\n\n# dept_piv=dept_piv.to_pandas()\n\nprint(\"Command execution ended at:\"+str(datetime.datetime.now(IST).replace(microsecond=0).time())+\" hrs\")","79fa2642":"print(\"Command execution started at:\"+str(datetime.datetime.now(IST).replace(microsecond=0).time())+\" hrs\\n\")\n\n# df1.columns = ['Customer_unique_id', 'Product_type', 'Province'] \ndept_piv.columns=['aisle_id',\n                  'count_order_id',\n                  'avg_order_hod',\n                  'avg_ord_dow',\n                  'avg_ord_dspo',\n                  'avg_addtc',\n                  'max_addtc']\ndept_piv\n\nprint(\"Command execution ended at:\"+str(datetime.datetime.now(IST).replace(microsecond=0).time())+\" hrs\")","4444493a":"print(\"Command execution started at:\"+str(datetime.datetime.now(IST).replace(microsecond=0).time())+\" hrs\\n\")\n\ncorrMatrix = dept_piv.corr()\nfig_dims = (20, 10)\nfig, ax = plt.subplots(figsize=fig_dims)\nsns.heatmap(corrMatrix, annot=True)\nplt.show()\ndel corrMatrix\n\nprint(\"\\nCommand execution ended at:\"+str(datetime.datetime.now(IST).replace(microsecond=0).time())+\" hrs\")","df41ab08":"print(\"Command execution started at:\"+str(datetime.datetime.now(IST).replace(microsecond=0).time())+\" hrs\\n\")\n\nsns.pairplot(dept_piv)\n\nprint(\"Command execution ended at:\"+str(datetime.datetime.now(IST).replace(microsecond=0).time())+\" hrs\")","13439a01":"print(\"Command execution started at:\"+str(datetime.datetime.now(IST).replace(microsecond=0).time())+\" hrs\\n\")\n\nr=len(dept_piv.columns)\nprint(r)\n\nprint(\"\\nCommand execution ended at:\"+str(datetime.datetime.now(IST).replace(microsecond=0).time())+\" hrs\")","0f31db0c":"print(\"Command execution started at:\"+str(datetime.datetime.now(IST).replace(microsecond=0).time())+\" hrs\")\n\nfor col_1 in range (0,r):\n  for col_2 in range (col_1,r):\n    if col_1==col_2:\n     pass\n    else:\n        w=dept_piv.iloc[:,col_1]\n        z=dept_piv.iloc[:,col_2]\n        data_num=pandas.concat([w, z], axis=1)\n        print(data_num)\n        print(\"\\n\")\n      # Selecting the data for Scaling and Clustering\n        print(data_num.dtypes,\"\\n\")\n        q=data_num.columns\n        print(data_num.head(),\"\\n\")\n        print(type(data_num),\"\\n\")\n        data_num.iloc[:,0]=np.float64(data_num.iloc[:,0])\n        data_num.iloc[:,1]=np.float64(data_num.iloc[:,1])\n        print(data_num.dtypes,\"\\n\")\n        print(data_num.head(),\"\\n\")\n        data_scaled=cuml.preprocessing.scale(pd.from_pandas(data_num), axis=0)\n      # Creating clusters for Visual representation\n        K=range(1,10)\n        wss = []\n        for k in K:\n            kmeans = cuml.cluster.KMeans(n_clusters=k,init=\"scalable-k-means++\")\n            means=kmeans.fit(data_scaled)\n            t= means.inertia_\n            wss.append(t)\n            print(\"K-Means processed for k=\",k)\n            print(\"For K =\",k,\", WSS =\",t,\";\\n\")\n        kwss=pd.DataFrame()\n        kwss[\"K\"]=K\n        kwss[\"WSS\"] = wss\n        print(kwss)\n        del kwss\n        x=q[0]\n        y=q[1]\n        plt.plot(K, wss, 'bx')\n        plt.xlabel('k')\n        plt.ylabel('Average distortion')\n        plt.title(\"\\n\"+str(x)+\" vs \"+str(y)+\" Elbow Plot\")\n        plt.suptitle(\"For the feature \"+str(feature),ha='center', y=1)\n        plt.show()\n        plt.figure(clear=True)\n        del K, wss\n        k=[]\n        Sil_Score=[]\n        print(\"Starting the Silhouette Measure calculation at:\"+str(datetime.datetime.now(IST).replace(microsecond=0).time())+\" hrs\\n\")\n        for i in range(2,11):\n            labels=cuml.cluster.KMeans(n_clusters=i).fit(data_scaled).labels_\n            g=cuml.metrics.cluster.silhouette_score(data_scaled,labels,metric=\"euclidean\")\n            k.append(i)\n            Sil_Score.append(g)\n            print (\"Silhoutte score for k= \"+str(i)+\" is \"+str(g)+\" at \"+str(datetime.datetime.now(IST).replace(microsecond=0).time())+\" hrs\")\n        km=pd.DataFrame()\n        km['K']=k\n        km['Silhouette Score']=Sil_Score\n        del k, Sil_Score\n        print(\"\\n\")\n        print(km,\"\\n\")\n        print(km.dtypes,\"\\n\")\n        Kmeans=km.sort_values(by=\"Silhouette Score\",ascending=False)\n        print(\"****************************************************\")\n        print(\"           Sorting the Silhouette Score\")\n        print(\"****************************************************\")\n        print(Kmeans)\n        print(\"\\nHighest K Values is for K=\"+str(Kmeans.iloc[0,0])+\", Silhouette Value=\"+str(Kmeans.iloc[0,1]))\n        a=Kmeans.iloc[0, 0]\n        del km, Kmeans\n        kmeans=cuml.cluster.KMeans(n_clusters=a, init=\"scalable-k-means++\")\n        kmeans=kmeans.fit(data_num)\n        kmeans.cluster_centers_\n        data_num['Clusters'] = kmeans.labels_\n        data_num['Clusters']\n        sns.scatterplot(x=x, y=y,hue = 'Clusters',  data=data_num)\n        plt.title(str(x)+\" vs \"+str(y))\n        plt.suptitle(\"For the feature \"+str(feature),ha='center', y=1)\n        plt.savefig(\"Feature \"+str(feature)+\", variables \"+str(x)+' vs '+str(y)+'.jpg')\n        plt.figure(clear=True)\n      \nprint(\"\\nCommand execution ended at:\"+str(datetime.datetime.now(IST).replace(microsecond=0).time())+\" hrs\")","2e99e577":"feature=\"product_id\"\nfeature","65b8cb96":"print(\"Command execution started at:\"+str(datetime.datetime.now(IST).replace(microsecond=0).time())+\" hrs\\n\")\n\n# data.pivot_table(df, values=['D', 'E'], index=['A', 'C'],aggfunc={'D': np.mean,'E': np.mean})   Syntax\n\ncord_id=(data.pivot_table(index=['product_id'], \n                          values=['order_id'], \n                          aggfunc={'order_id':'count'})).reset_index()\navord_hod=(data.pivot_table(index=['product_id'], \n                              values=['order_hour_of_day'], \n                              aggfunc={'order_hour_of_day':np.mean})).reset_index()\navord_dow=(data.pivot_table(index=['product_id'], \n                              values=['order_dow'], \n                              aggfunc={'order_dow':np.mean})).reset_index()\navord_dspo=(data.pivot_table(index=['product_id'], \n                               values=['days_since_prior_order'], \n                               aggfunc={'days_since_prior_order':np.mean})).reset_index()\navaddtc=(data.pivot_table(index=['product_id'], \n                            values=['add_to_cart_order'], \n                            aggfunc={'add_to_cart_order':np.mean})).reset_index()\nminaddtc=(data.pivot_table(index=['product_id'], \n                             values=['add_to_cart_order'], \n                             aggfunc={'add_to_cart_order':np.min})).reset_index()\nmaxaddtc=(data.pivot_table(index=['product_id'], \n                             values=['add_to_cart_order'], \n                             aggfunc={'add_to_cart_order':np.max})).reset_index()\nmaxord_dspo=(data.pivot_table(index=['product_id'], \n                                values=['days_since_prior_order'], \n                                aggfunc={'days_since_prior_order':np.max})).reset_index()\n\nprint(\"Command execution ended at:\"+str(datetime.datetime.now(IST).replace(microsecond=0).time())+\" hrs\")","dd51e0f6":"print(\"Command execution started at:\"+str(datetime.datetime.now(IST).replace(microsecond=0).time())+\" hrs\\n\")\n\ndept_piv=pd.merge(cord_id, avord_hod, how='left', on='product_id')\ndept_piv=pd.merge(dept_piv, avord_dow, how='left', on='product_id')\ndept_piv=pd.merge(dept_piv, avord_dspo, how='left', on='product_id')\ndept_piv=pd.merge(dept_piv, avaddtc, how='left', on='product_id')\ndept_piv=pd.merge(dept_piv, maxaddtc, how='left', on='product_id')\n\ndel cord_id, avord_hod, avord_dow, avord_dspo, avaddtc, maxaddtc\n\n# dept_piv=dept_piv.to_pandas()\n\nprint(\"Command execution ended at:\"+str(datetime.datetime.now(IST).replace(microsecond=0).time())+\" hrs\")","9f4b83af":"print(\"Command execution started at:\"+str(datetime.datetime.now(IST).replace(microsecond=0).time())+\" hrs\\n\")\n\n# df1.columns = ['Customer_unique_id', 'Product_type', 'Province'] \ndept_piv.columns=['product_id',\n                  'count_order_id',\n                  'avg_order_hod',\n                  'avg_ord_dow',\n                  'avg_ord_dspo',\n                  'avg_addtc',\n                  'max_addtc']\ndept_piv\n\nprint(\"Command execution ended at:\"+str(datetime.datetime.now(IST).replace(microsecond=0).time())+\" hrs\")","f5973478":"print(\"Command execution started at:\"+str(datetime.datetime.now(IST).replace(microsecond=0).time())+\" hrs\\n\")\n\ncorrMatrix = dept_piv.corr()\nfig_dims = (20, 10)\nfig, ax = plt.subplots(figsize=fig_dims)\nsns.heatmap(corrMatrix, annot=True)\nplt.show()\ndel corrMatrix\n\nprint(\"\\nCommand execution ended at:\"+str(datetime.datetime.now(IST).replace(microsecond=0).time())+\" hrs\")","d2a8e6fa":"print(\"Command execution started at:\"+str(datetime.datetime.now(IST).replace(microsecond=0).time())+\" hrs\\n\")\n\nsns.pairplot(dept_piv)\n\nprint(\"Command execution ended at:\"+str(datetime.datetime.now(IST).replace(microsecond=0).time())+\" hrs\")","3158a686":"print(\"Command execution started at:\"+str(datetime.datetime.now(IST).replace(microsecond=0).time())+\" hrs\\n\")\n\nr=len(dept_piv.columns)\nprint(r)\n\nprint(\"\\nCommand execution ended at:\"+str(datetime.datetime.now(IST).replace(microsecond=0).time())+\" hrs\")","04aee812":"print(\"Command execution started at:\"+str(datetime.datetime.now(IST).replace(microsecond=0).time())+\" hrs\")\n\nfor col_1 in range (0,r):\n  for col_2 in range (col_1,r):\n    if col_1==col_2:\n     pass\n    else:\n        w=dept_piv.iloc[:,col_1]\n        z=dept_piv.iloc[:,col_2]\n        data_num=pandas.concat([w, z], axis=1)\n        print(data_num)\n        print(\"\\n\")\n      # Selecting the data for Scaling and Clustering\n        print(data_num.dtypes,\"\\n\")\n        q=data_num.columns\n        print(data_num.head(),\"\\n\")\n        print(type(data_num),\"\\n\")\n        data_num.iloc[:,0]=np.float64(data_num.iloc[:,0])\n        data_num.iloc[:,1]=np.float64(data_num.iloc[:,1])\n        print(data_num.dtypes,\"\\n\")\n        print(data_num.head(),\"\\n\")\n        data_scaled=cuml.preprocessing.scale(pd.from_pandas(data_num), axis=0)\n      # Creating clusters for Visual representation\n        K=range(1,10)\n        wss = []\n        for k in K:\n            kmeans = cuml.cluster.KMeans(n_clusters=k,init=\"scalable-k-means++\")\n            means=kmeans.fit(data_scaled)\n            t= means.inertia_\n            wss.append(t)\n            print(\"K-Means processed for k=\",k)\n            print(\"For K =\",k,\", WSS =\",t,\";\\n\")\n        kwss=pd.DataFrame()\n        kwss[\"K\"]=K\n        kwss[\"WSS\"] = wss\n        print(kwss)\n        del kwss\n        x=q[0]\n        y=q[1]\n        plt.plot(K, wss, 'bx')\n        plt.xlabel('k')\n        plt.ylabel('Average distortion')\n        plt.title(\"\\n\"+str(x)+\" vs \"+str(y)+\" Elbow Plot\")\n        plt.suptitle(\"For the feature \"+str(feature),ha='center', y=1)\n        plt.show()\n        plt.figure(clear=True)\n        del K, wss\n        k=[]\n        Sil_Score=[]\n        print(\"Starting the Silhouette Measure calculation at:\"+str(datetime.datetime.now(IST).replace(microsecond=0).time())+\" hrs\\n\")\n        for i in range(2,11):\n            labels=cuml.cluster.KMeans(n_clusters=i).fit(data_scaled).labels_\n            g=cuml.metrics.cluster.silhouette_score(data_scaled,labels,metric=\"euclidean\")\n            k.append(i)\n            Sil_Score.append(g)\n            print (\"Silhoutte score for k= \"+str(i)+\" is \"+str(g)+\" at \"+str(datetime.datetime.now(IST).replace(microsecond=0).time())+\" hrs\")\n        km=pd.DataFrame()\n        km['K']=k\n        km['Silhouette Score']=Sil_Score\n        del k, Sil_Score\n        print(\"\\n\")\n        print(km,\"\\n\")\n        print(km.dtypes,\"\\n\")\n        Kmeans=km.sort_values(by=\"Silhouette Score\",ascending=False)\n        print(\"****************************************************\")\n        print(\"           Sorting the Silhouette Score\")\n        print(\"****************************************************\")\n        print(Kmeans)\n        print(\"\\nHighest K Values is for K=\"+str(Kmeans.iloc[0,0])+\", Silhouette Value=\"+str(Kmeans.iloc[0,1]))\n        a=Kmeans.iloc[0, 0]\n        del km, Kmeans\n        kmeans=cuml.cluster.KMeans(n_clusters=a, init=\"scalable-k-means++\")\n        kmeans=kmeans.fit(data_num)\n        kmeans.cluster_centers_\n        data_num['Clusters'] = kmeans.labels_\n        data_num['Clusters']\n        sns.scatterplot(x=x, y=y,hue = 'Clusters',  data=data_num)\n        plt.title(str(x)+\" vs \"+str(y))\n        plt.suptitle(\"For the feature \"+str(feature),ha='center', y=1)\n        plt.savefig(\"Feature \"+str(feature)+\", variables \"+str(x)+' vs '+str(y)+'.jpg')\n        plt.figure(clear=True)\n      \nprint(\"\\nCommand execution ended at:\"+str(datetime.datetime.now(IST).replace(microsecond=0).time())+\" hrs\")","9bba2c00":"feature=\"user_id\"\nfeature","4ee20618":"print(\"Command execution started at:\"+str(datetime.datetime.now(IST).replace(microsecond=0).time())+\" hrs\\n\")\n\n# data.pivot_table(df, values=['D', 'E'], index=['A', 'C'],aggfunc={'D': np.mean,'E': np.mean})   Syntax\n\ncord_id=(data.pivot_table(index=['user_id'], \n                          values=['order_id'], \n                          aggfunc={'order_id':'count'})).reset_index()\navord_hod=(data.pivot_table(index=['user_id'], \n                              values=['order_hour_of_day'], \n                              aggfunc={'order_hour_of_day':np.mean})).reset_index()\navord_dow=(data.pivot_table(index=['user_id'], \n                              values=['order_dow'], \n                              aggfunc={'order_dow':np.mean})).reset_index()\navord_dspo=(data.pivot_table(index=['user_id'], \n                               values=['days_since_prior_order'], \n                               aggfunc={'days_since_prior_order':np.mean})).reset_index()\navaddtc=(data.pivot_table(index=['user_id'], \n                            values=['add_to_cart_order'], \n                            aggfunc={'add_to_cart_order':np.mean})).reset_index()\nminaddtc=(data.pivot_table(index=['user_id'], \n                             values=['add_to_cart_order'], \n                             aggfunc={'add_to_cart_order':np.min})).reset_index()\nmaxaddtc=(data.pivot_table(index=['user_id'], \n                             values=['add_to_cart_order'], \n                             aggfunc={'add_to_cart_order':np.max})).reset_index()\nmaxord_dspo=(data.pivot_table(index=['user_id'], \n                                values=['days_since_prior_order'], \n                                aggfunc={'days_since_prior_order':np.max})).reset_index()\n\n\nprint(\"Command execution ended at:\"+str(datetime.datetime.now(IST).replace(microsecond=0).time())+\" hrs\")","ee7c3c8a":"print(\"Command execution started at:\"+str(datetime.datetime.now(IST).replace(microsecond=0).time())+\" hrs\\n\")\n\ndept_piv=pd.merge(cord_id, avord_hod, how='left', on='user_id')\ndept_piv=pd.merge(dept_piv, avord_dow, how='left', on='user_id')\ndept_piv=pd.merge(dept_piv, avord_dspo, how='left', on='user_id')\ndept_piv=pd.merge(dept_piv, avaddtc, how='left', on='user_id')\ndept_piv=pd.merge(dept_piv, maxaddtc, how='left', on='user_id')\n\ndel cord_id, avord_hod, avord_dow, avord_dspo, avaddtc, maxaddtc\n\n# dept_piv=dept_piv.to_pandas()\n\nprint(\"Command execution ended at:\"+str(datetime.datetime.now(IST).replace(microsecond=0).time())+\" hrs\")","b7c218db":"print(\"Command execution started at:\"+str(datetime.datetime.now(IST).replace(microsecond=0).time())+\" hrs\\n\")\n\n# df1.columns = ['Customer_unique_id', 'Product_type', 'Province'] \ndept_piv.columns=['user_id',\n                  'count_order_id',\n                  'avg_order_hod',\n                  'avg_ord_dow',\n                  'avg_ord_dspo',\n                  'avg_addtc',\n                  'max_addtc']\ndept_piv\n\nprint(\"Command execution ended at:\"+str(datetime.datetime.now(IST).replace(microsecond=0).time())+\" hrs\")","0c75040b":"print(\"Command execution started at:\"+str(datetime.datetime.now(IST).replace(microsecond=0).time())+\" hrs\\n\")\n\ncorrMatrix = dept_piv.corr()\nfig_dims = (20, 10)\nfig, ax = plt.subplots(figsize=fig_dims)\nsns.heatmap(corrMatrix, annot=True)\nplt.show()\ndel corrMatrix\n\nprint(\"\\nCommand execution ended at:\"+str(datetime.datetime.now(IST).replace(microsecond=0).time())+\" hrs\")","ec9a9c6c":"print(\"Command execution started at:\"+str(datetime.datetime.now(IST).replace(microsecond=0).time())+\" hrs\\n\")\n\nsns.pairplot(dept_piv)\n\nprint(\"Command execution ended at:\"+str(datetime.datetime.now(IST).replace(microsecond=0).time())+\" hrs\")","9f139940":"print(\"Command execution started at:\"+str(datetime.datetime.now(IST).replace(microsecond=0).time())+\" hrs\\n\")\n\nr=len(dept_piv.columns)\nprint(r)\n\nprint(\"\\nCommand execution ended at:\"+str(datetime.datetime.now(IST).replace(microsecond=0).time())+\" hrs\")","bbd69cd9":"print(\"Command execution started at:\"+str(datetime.datetime.now(IST).replace(microsecond=0).time())+\" hrs\")\n\nfor col_1 in range (0,r):\n  for col_2 in range (col_1,r):\n    if col_1==col_2:\n     pass\n    else:\n        w=dept_piv.iloc[:,col_1]\n        z=dept_piv.iloc[:,col_2]\n        data_num=pandas.concat([w, z], axis=1)\n        print(data_num)\n        print(\"\\n\")\n      # Selecting the data for Scaling and Clustering\n        print(data_num.dtypes,\"\\n\")\n        q=data_num.columns\n        print(data_num.head(),\"\\n\")\n        print(type(data_num),\"\\n\")\n        data_num.iloc[:,0]=np.float64(data_num.iloc[:,0])\n        data_num.iloc[:,1]=np.float64(data_num.iloc[:,1])\n        print(data_num.dtypes,\"\\n\")\n        print(data_num.head(),\"\\n\")\n        data_scaled=cuml.preprocessing.scale(pd.from_pandas(data_num), axis=0)\n      # Creating clusters for Visual representation\n        K=range(1,10)\n        wss = []\n        for k in K:\n            kmeans = cuml.cluster.KMeans(n_clusters=k,init=\"scalable-k-means++\")\n            means=kmeans.fit(data_scaled)\n            t= means.inertia_\n            wss.append(t)\n            print(\"K-Means processed for k=\",k)\n            print(\"For K =\",k,\", WSS =\",t,\";\\n\")\n        kwss=pd.DataFrame()\n        kwss[\"K\"]=K\n        kwss[\"WSS\"] = wss\n        print(kwss)\n        del kwss\n        x=q[0]\n        y=q[1]\n        plt.plot(K, wss, 'bx')\n        plt.xlabel('k')\n        plt.ylabel('Average distortion')\n        plt.title(\"\\n\"+str(x)+\" vs \"+str(y)+\" Elbow Plot\")\n        plt.suptitle(\"For the feature \"+str(feature),ha='center', y=1)\n        plt.show()\n        plt.figure(clear=True)\n        del K, wss\n        k=[]\n        Sil_Score=[]\n        print(\"Starting the Silhouette Measure calculation at:\"+str(datetime.datetime.now(IST).replace(microsecond=0).time())+\" hrs\\n\")\n        for i in range(2,11):\n            labels=cuml.cluster.KMeans(n_clusters=i).fit(data_scaled).labels_\n            g=cuml.metrics.cluster.silhouette_score(data_scaled,labels,metric=\"euclidean\")\n            k.append(i)\n            Sil_Score.append(g)\n            print (\"Silhoutte score for k= \"+str(i)+\" is \"+str(g)+\" at \"+str(datetime.datetime.now(IST).replace(microsecond=0).time())+\" hrs\")\n        km=pd.DataFrame()\n        km['K']=k\n        km['Silhouette Score']=Sil_Score\n        del k, Sil_Score\n        print(\"\\n\")\n        print(km,\"\\n\")\n        print(km.dtypes,\"\\n\")\n        Kmeans=km.sort_values(by=\"Silhouette Score\",ascending=False)\n        print(\"****************************************************\")\n        print(\"           Sorting the Silhouette Score\")\n        print(\"****************************************************\")\n        print(Kmeans)\n        print(\"\\nHighest K Values is for K=\"+str(Kmeans.iloc[0,0])+\", Silhouette Value=\"+str(Kmeans.iloc[0,1]))\n        a=Kmeans.iloc[0, 0]\n        del km, Kmeans\n        kmeans=cuml.cluster.KMeans(n_clusters=a, init=\"scalable-k-means++\")\n        kmeans=kmeans.fit(data_num)\n        kmeans.cluster_centers_\n        data_num['Clusters'] = kmeans.labels_\n        data_num['Clusters']\n        sns.scatterplot(x=x, y=y,hue = 'Clusters',  data=data_num)\n        plt.title(str(x)+\" vs \"+str(y))\n        plt.suptitle(\"For the feature \"+str(feature),ha='center', y=1)\n        plt.savefig(\"Feature \"+str(feature)+\", variables \"+str(x)+' vs '+str(y)+'.jpg')\n        plt.figure(clear=True)\n      \nprint(\"\\nCommand execution ended at:\"+str(datetime.datetime.now(IST).replace(microsecond=0).time())+\" hrs\")","0479a088":"# End time of the notebook runtime\n\nprint(\"Command execution started at:\"+str(datetime.datetime.now(IST).replace(microsecond=0).time())+\" hrs\")\n\nend=datetime.datetime.now(IST).replace(microsecond=0)\ne_hrs=end.time()\nprint(\"Notebook ended on:\", end.date(), \"at\", e_hrs)","8d777f6b":"# Total runtime of the notebook\n\nprint(\"Code started on:\", start.date(), \"at\",s_hrs,\"hrs.\\nCode ended on:\", end.date(), \"at\",e_hrs,\"\\n\\nTotal duration of the code runtime:\",(end-start),\"hrs\")","6a5b62b2":"print(\"Command execution started at:\"+str(datetime.datetime.now(IST).replace(microsecond=0).time())+\" hrs\")\n\n# Enable GPU Acceleration\n\n## For Google Colab\n\n!nvidia-smi\n\n## Install RAPIDS\n!git clone https:\/\/github.com\/rapidsai\/rapidsai-csp-utils.git\n!bash rapidsai-csp-utils\/colab\/rapids-colab.sh stable\n\nimport sys, os, shutil\n\nsys.path.append('\/usr\/local\/lib\/python3.7\/site-packages\/')\nos.environ['NUMBAPRO_NVVM'] = '\/usr\/local\/cuda\/nvvm\/lib64\/libnvvm.so'\nos.environ['NUMBAPRO_LIBDEVICE'] = '\/usr\/local\/cuda\/nvvm\/libdevice\/'\nos.environ[\"CONDA_PREFIX\"] = \"\/usr\/local\"\nfor so in ['cudf', 'rmm', 'nccl', 'cuml', 'cugraph', 'xgboost', 'cuspatial']:\n  fn = 'lib'+so+'.so'\n  source_fn = '\/usr\/local\/lib\/'+fn\n  dest_fn = '\/usr\/lib\/'+fn\n  if os.path.exists(source_fn):\n    print(f'Copying {source_fn} to {dest_fn}')\n    shutil.copyfile(source_fn, dest_fn)\n\nif not os.path.exists('\/usr\/lib64'):\n    os.makedirs('\/usr\/lib64')\nfor so_file in os.listdir('\/usr\/local\/lib'):\n  if 'libstdc' in so_file:\n    shutil.copyfile('\/usr\/local\/lib\/'+so_file, '\/usr\/lib64\/'+so_file)\n    shutil.copyfile('\/usr\/local\/lib\/'+so_file, '\/usr\/lib\/x86_64-linux-gnu\/'+so_file)\n\nprint(\"\\nCommand execution ended at:\"+str(datetime.datetime.now(IST).replace(microsecond=0).time())+\" hrs\")","0ab434ff":"# Enable GPU Acceleration\n## Input Rapids Data File (For Kaggle):\n\n### Add Data\n### Search by URL\n### Paste the URL: https:\/\/www.kaggle.com\/cdeotte\/rapids\n### Add","2752492e":"# Standard Imports","d7cbc21f":"\n# For Jupyter Notebook\n\n%%time\n\ndir=\"G:\\\\IIM-IPBA\\\\Capstone Project\\\\Dataset\"\n\nos.chdir(dir)\n\naisles=pd.read_csv(\"1.csv\")\n\ndept=pd.read_csv(\"2.csv\")\n\norders=pd.read_csv(\"5.csv\")\n\nprod=pd.read_csv(\"6.csv\")\n\nord_prior=pd.read_csv(\"3.csv\")","9f2a8f44":"# Aisle Id","e61e2b49":"# Data Merging","235c344d":"# Product Id","ea7180c9":"# END","6aca343f":"# User Id","c3d4c72a":"# All commands finished","f10fea42":"# All the merging has been completed\n\n# Grouping the data for various features using Pivot table"}}