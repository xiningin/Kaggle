{"cell_type":{"1d696e10":"code","9ef986c8":"code","df44e5eb":"code","2502311d":"code","3813c403":"code","20e938c0":"code","79be321a":"code","0fead90e":"code","681a67e1":"code","7b8806da":"code","b79c809f":"code","98b71d7f":"code","f4e96f82":"code","1c9e21bd":"code","5fd410c7":"code","32cabfef":"code","05f7e36a":"code","958ae429":"code","83c51344":"code","711975c9":"code","7bdb4e9c":"code","dc162d6f":"code","edaef2b2":"code","4267bb6d":"code","ac5de2ef":"code","8d4fee43":"code","2e513245":"code","3b836aed":"code","11e47548":"code","9de80f3a":"code","50d76ea9":"code","186d5b9a":"code","a8ce25c5":"code","036993aa":"code","bdd59e35":"code","c2ca4b12":"code","b618ccf1":"code","78458532":"code","cfab33d8":"markdown","72df2414":"markdown","ee4237ac":"markdown","550772d2":"markdown","1bf926c9":"markdown","64e3080f":"markdown","18a4202e":"markdown","6e8fe74e":"markdown","1cd609a3":"markdown","7e505066":"markdown","e056ccbc":"markdown","772b562f":"markdown","d6f638ff":"markdown","16f47d67":"markdown","48628fb0":"markdown","28fe3110":"markdown","e721096e":"markdown","22d5f7be":"markdown","c2844c02":"markdown","fce36fe8":"markdown","5c6591fc":"markdown","0d9822ce":"markdown","ddfa608b":"markdown","cb909004":"markdown","d002d33e":"markdown","48366e45":"markdown","4e1b655d":"markdown"},"source":{"1d696e10":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","9ef986c8":"#Importing libraries for initial exploratory analysis\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport matplotlib as mlt\n%matplotlib inline\n\n#For Feature Engineering:\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import StandardScaler\n\n#For Predictive Modeling\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.linear_model import LogisticRegressionCV\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import *\nfrom sklearn.model_selection import GridSearchCV\n\n\n\n#For Model Evaluation\nfrom sklearn.metrics import precision_recall_fscore_support as score\nfrom sklearn.metrics import confusion_matrix, accuracy_score, roc_auc_score,f1_score,classification_report,recall_score\nfrom sklearn.preprocessing import label_binarize\n\n","df44e5eb":"## Loading the data\ndatafile = \"\/kaggle\/input\/heart-failure-prediction\/heart.csv\"\ndf = pd.read_csv(datafile, sep=',')\ndf.head()","2502311d":"sns.pairplot(df,hue=\"HeartDisease\");","3813c403":"df.describe()","20e938c0":"df['HeartDisease'].value_counts()","79be321a":"cor = df.corr()\nplt.figure(figsize=(7,7))\nsns.heatmap(cor, annot=True);\nplt.title('Correlation of the features with the Heart disease');","0fead90e":"def feature_processing(data, numeric_cols, categorical_cols, target_col, skew_limit = 0.75):\n    data_num = data[numeric_cols].apply(pd.to_numeric, errors = ('coerce'))\n    skew_vals = data_num.skew()\n    skew_cols = (skew_vals.sort_values(ascending = False).to_frame().rename(columns = {0:'Skew'}).query('abs(Skew) > {}'.format(skew_limit)))\n    for col in skew_cols.index.values:\n        data_num[col] = data_num[col].apply(np.log1p)\n    \n    data_num.fillna(data_num.mean(), inplace = True)\n    data_num = data_num.replace([np.inf, -np.inf],0)\n    \n    other_num_cols = [x for x in numeric_cols if x not in skew_cols]\n    \n    scaler = StandardScaler()\n    scaler.fit(data_num[other_num_cols])\n    data_num[other_num_cols] = scaler.transform(data_num[other_num_cols])\n        \n    \n    print(\"Features that had skew are:\", skew_cols)\n    \n        \n    data_cat = data[categorical_cols]\n    data_cat = data_cat.apply(LabelEncoder().fit_transform)\n    data_transformed = pd.concat([data_num, data_cat], axis = 1)\n    data_transformed[target_col] = data[target_col]\n    \n    return data_transformed","681a67e1":"target_col = ['HeartDisease']\nall_features = [x for x in df.columns if x not in target_col]\nnumeric_cols = ['Age','RestingBP','Cholesterol','FastingBS','MaxHR','Oldpeak']\ncategorical_cols = [x for x in all_features if x not in numeric_cols]\n\ndata_transformed = feature_processing(df, numeric_cols = numeric_cols, categorical_cols = categorical_cols,\n                                      target_col = target_col )","7b8806da":"data_transformed","b79c809f":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(data_transformed[numeric_cols + categorical_cols],\n                                                    data_transformed['HeartDisease'], test_size = 0.2, \n                                                    random_state = 2890)\n","98b71d7f":"print(\"X_train shape:\", X_train.shape, \"X_test shape:\", X_test.shape,\"y_train shape:\", y_train.shape,\"y_test shape:\", y_test.shape)","f4e96f82":"y_train.value_counts(normalize = True)\n","1c9e21bd":"y_test.value_counts(normalize = True)","5fd410c7":"lr = LogisticRegression(solver='liblinear').fit(X_train, y_train)\nlr_l1 = LogisticRegressionCV(Cs=10, cv=4, penalty='l1', solver='liblinear').fit(X_train, y_train)\nlr_l2 = LogisticRegressionCV(Cs=10, cv=4, penalty='l2', solver='liblinear').fit(X_train, y_train)\n\ny_pred_lr = list()\ny_prob_lr = list()\n\ncoeff_labels_lr = ['lr', 'l1', 'l2']\ncoeff_models_lr = [lr, lr_l1, lr_l2]\n\nfor lab,mod in zip(coeff_labels_lr, coeff_models_lr):\n    y_pred_lr.append(pd.Series(mod.predict(X_test), name=lab))\n    y_prob_lr.append(pd.Series(mod.predict_proba(X_test).max(axis=1), name=lab))\n    \ny_pred_lr = pd.concat(y_pred_lr, axis=1)\ny_prob_lr = pd.concat(y_prob_lr, axis=1)\n\ny_pred_lr.head()","32cabfef":"metrics = list()\ncm = dict()\n\nfor lab in coeff_labels_lr:\n\n    # Preciision, recall, f-score from the multi-class support function\n    precision, recall, fscore, _ = score(y_test, y_pred_lr[lab], average='weighted')\n    \n    # The usual way to calculate accuracy\n    accuracy = accuracy_score(y_test, y_pred_lr[lab])\n    \n    # ROC-AUC scores can be calculated by binarizing the data\n    auc = roc_auc_score(label_binarize(y_test, classes=[0,1]),\n              label_binarize(y_pred_lr[lab], classes=[0,1]), \n              average='weighted')\n    \n    # Last, the confusion matrix\n    cm[lab] = confusion_matrix(y_test, y_pred_lr[lab])\n    \n    metrics.append(pd.Series({'precision':precision, 'recall':recall, \n                              'fscore':fscore, 'accuracy':accuracy,\n                              'auc':auc}, \n                             name=lab))\n\nmetrics = pd.concat(metrics, axis=1)","05f7e36a":"metrics","958ae429":"max_k = 40\nf1_scores = list()\nerror_rates = list() # 1-accuracy\n\nfor k in range(1, max_k):\n    \n    knn = KNeighborsClassifier(n_neighbors=k, weights='distance')\n    knn = knn.fit(X_train, y_train)\n    \n    y_pred_knn = knn.predict(X_test)\n    f1 = f1_score(y_pred_knn, y_test)\n    f1_scores.append((k, round(f1_score(y_test, y_pred_knn), 4)))\n    error = 1-round(accuracy_score(y_test, y_pred_knn), 4)\n    error_rates.append((k, error))\n    \nf1_results = pd.DataFrame(f1_scores, columns=['K', 'F1 Score'])\nerror_results = pd.DataFrame(error_rates, columns=['K', 'Error Rate'])","83c51344":"# Plot F1 results\nsns.set_context('talk')\nsns.set_style('ticks')\n\nplt.figure(dpi=300)\nax = f1_results.set_index('K').plot( figsize=(12, 12), linewidth=6)\nax.set(xlabel='K', ylabel='F1 Score')\nax.set_xticks(range(1, max_k, 2));\nplt.title('KNN F1 Score')\nplt.savefig('knn_f1.png')","711975c9":"sns.set_context('talk')\nsns.set_style('ticks')\n\nplt.figure(dpi=300)\nax = error_results.set_index('K').plot( figsize=(12, 12), linewidth=6)\nax.set(xlabel='K', ylabel='Error Rate')\nax.set_xticks(range(1, max_k, 2))\nplt.title('KNN Elbow Curve')\nplt.savefig('knn_elbow.png')","7bdb4e9c":"knn = KNeighborsClassifier(n_neighbors=22, weights='distance')\nknn = knn.fit(X_train, y_train)\ny_pred_knn = knn.predict(X_test)\n   ","dc162d6f":"# Initialize the random forest estimator\n# Note that the number of trees is not setup here\nRF = RandomForestClassifier(oob_score=True, \n                            random_state=42, \n                            warm_start=True,\n                            n_jobs=-1)\n\noob_list = list()\n\n# Iterate through all of the possibilities for \n# number of trees\nfor n_trees in [15, 20, 30, 40, 50, 100, 150, 200, 300, 400]:\n    \n    # Use this to set the number of trees\n    RF.set_params(n_estimators=n_trees)\n\n    # Fit the model\n    RF.fit(X_train, y_train)\n\n    # Get the oob error\n    oob_error = 1 - RF.oob_score_\n    \n    # Store it\n    oob_list.append(pd.Series({'n_trees': n_trees, 'oob': oob_error}))\n\nrf_oob_df = pd.concat(oob_list, axis=1).T.set_index('n_trees')\n\nrf_oob_df","edaef2b2":"sns.set_context('talk')\nsns.set_style('white')\n\nax = rf_oob_df.plot(legend=False, marker='o', figsize=(14, 7), linewidth=5)\nax.set(ylabel='out-of-bag error - Random Forest');","4267bb6d":"# Random forest with 100 estimators\nrf = RF.set_params(n_estimators=200)\ny_pred_rf = rf.predict(X_test)","ac5de2ef":"### BEGIN SOLUTION\nerror_list = list()\n\n# Iterate through various possibilities for number of trees\ntree_list = [2,5,7,15, 25, 50, 60, 75, 85, 100]\nfor n_trees in tree_list:\n    \n    # Initialize the gradient boost classifier\n    GBC = GradientBoostingClassifier(n_estimators=n_trees, random_state=42)\n\n    # Fit the model\n    print(f'Fitting model with {n_trees} trees')\n    GBC.fit(X_train.values, y_train.values)\n    y_pred_gb = GBC.predict(X_test)\n\n    # Get the error\n    error = 1.0 - accuracy_score(y_test, y_pred_gb)\n    \n    # Store it\n    error_list.append(pd.Series({'n_trees': n_trees, 'error': error}))\n\nerror_df_gb = pd.concat(error_list, axis=1).T.set_index('n_trees')\n\nerror_df_gb","8d4fee43":"sns.set_context('talk')\nsns.set_style('white')\n\nax = error_df_gb.plot(marker='o', figsize=(12, 8), linewidth=5)\nax.set(xlabel='Number of Trees', ylabel='Error')\nax.set_xlim(0, max(error_df_gb.index)*1.1);\n","2e513245":"# The parameters to be fit\nparam_grid = {'n_estimators': tree_list,\n              'learning_rate': [0.1, 0.01, 0.001, 0.0001],\n              'subsample': [1.0, 0.5],\n              'max_features': [1, 2, 3, 4]}\n\n# The grid search object\nGV_GBC = GridSearchCV(GradientBoostingClassifier(random_state=42), \n                      param_grid=param_grid, \n                      scoring='accuracy',\n                      n_jobs=-1)\n\n# Do the grid search\nGV_GBC = GV_GBC.fit(X_train, y_train)","3b836aed":"# The best model\ngb_model = GV_GBC.best_estimator_\ny_pred_gb = GV_GBC.predict(X_test)\ngb_model","11e47548":"estimators = [\n    ('KNN', knn),\n    ('Random_forest', rf),\n    ('Gradient_boosting', GV_GBC )\n]\n\nmodel_stack = StackingClassifier(estimators = estimators, final_estimator = lr)\nmodel_stack.fit(X_train, y_train)\ny_label_stack = model_stack.predict(X_test)\ny_proba_stack = model_stack.predict_proba(X_test)\n","9de80f3a":"#Logistics Regression\nprint(classification_report(lr.predict(X_test), y_test))","50d76ea9":"#KNN\nprint(classification_report(knn.predict(X_test), y_test))","186d5b9a":"#Random Forest\nprint(classification_report(rf.predict(X_test), y_test))","a8ce25c5":"#GB\nprint(classification_report(GV_GBC.predict(X_test), y_test))","036993aa":"#Stack\nprint(classification_report(model_stack.predict(X_test), y_test))","bdd59e35":"models_accuracy_scores = {}\nmodels_accuracy_scores[\"Logistic Regression\"] = [f1_score(y_test,lr.predict(X_test),average=\"weighted\"),\n                                                 lr.score(X_test,y_test),\n                                                 recall_score(y_test,lr.predict(X_test), average = 'binary')]\nmodels_accuracy_scores[\"K-Nearest Neighbors\"] = [f1_score(y_test,knn.predict(X_test),average=\"weighted\"),\n                                                 knn.score(X_test,y_test),\n                                                 recall_score(y_test,knn.predict(X_test), average = 'binary')]\nmodels_accuracy_scores[\"Random Forest\"] = [f1_score(y_test,rf.predict(X_test),average=\"weighted\"),\n                                                 rf.score(X_test,y_test),\n                                                 recall_score(y_test,rf.predict(X_test), average = 'binary')]\nmodels_accuracy_scores[\"Gradient Boosting\"] = [f1_score(y_test,GV_GBC.predict(X_test),average=\"weighted\"),\n                                                 GV_GBC.score(X_test,y_test),\n                                                 recall_score(y_test,GV_GBC.predict(X_test), average = 'binary')]\nmodels_accuracy_scores[\"Stacking\"] = [f1_score(y_test,model_stack.predict(X_test),average=\"weighted\"),\n                                                 model_stack.score(X_test,y_test),\n                                                 recall_score(y_test,model_stack.predict(X_test), average = 'binary')]\n","c2ca4b12":"models_accuracy_scores","b618ccf1":"names = [\"F1-Score\",\"Accuracy\",\"Recall\"]\ndf_scores = pd.DataFrame(models_accuracy_scores.values(),columns = names,index=models_accuracy_scores.keys())\n    \nfor names in df_scores.columns:\n    fig = plt.figure(figsize = (10, 5))\n    ax = sns.barplot(y=df_scores[names],x=df_scores.index)\n    ax.set_xticklabels(ax.get_xticklabels(), rotation=40, ha=\"right\") \n    plt.xlabel(\"Methods\")\n    plt.ylabel(names)\n    plt.title(\"Best Perfomed Method\")\n    plt.show()","78458532":"df_scores","cfab33d8":"### Apply an ML Model to predict heart disease\nNow that we have done some exploratory analysis and have prepared data to apply ML models, we will look into some of the models and run evaluate those on precision, recall, f1, AUCs, accuracy and decide which one gives us the most stable model. The crucial most part would be to tune those models on hyperparameter to ensure we are maximizing the accuracy but at the same time ensuring those are not being overfitted.\n\nSteps to follow:\n1. Train test split the data\n2. Run the following models = Logistics Regression, KNN, Random Forest and Stacking (Ensemble Technique) with all of them as estimators\n3. Evaluate their performance on out of test samples\n4. Conclude the stable most model ","72df2414":"#### Gradient Boosting","ee4237ac":"### Model Evaluation","550772d2":"The minimum error to witness from here is at n_trees = 200","1bf926c9":"As we can see, the test set has exactly ratio of patient types with the disease whereas in the training set, there's more sample for patients with the Heart Disease but the imbalance is not too much","64e3080f":"#### KNN","18a4202e":"The data is only slightly unbalanced with 508 patients having a heart disease whereas 410 not having a heart disease. ","6e8fe74e":"#### ENSEMBLE: STACKING OF LR, KNN, RANDOM FOREST AND GRADIENT BOOSTING","1cd609a3":"1. In presence of more data, we can introduce a 20% additional validation set that can confirm the out of sample test accuracy and confirm if our model was an overfit.\n\n2. Try more feature engineering approaches\n\n","7e505066":"We can see that the normal logistics regression method yields the best AUCs, accuracy, precision, recall and f scores. so it's naturally the model that we would go with","e056ccbc":"It can be seen from the pair plot that heart disease seems to be distinctive amongst people with:\n1. More common amongst elderly age however\n2. Higher Heart Rate (MaxHR)\n3. More common in high sugar patients compared to low sugar\n\nThe variable - variable pairplot also shows how different features can determine probability of a patient with heart disease \n1. high age and high cholesterol, the chances of a heart disease are more common.\n2. People of lower age than 40 with high Heart Rate don't seem to have high probability of a heart disease\n\nLet's look in to the Feature information in a little more detail to explore mean values, standard deviation to understand the kind of data we are working with","772b562f":"#### Random Forest","d6f638ff":"### Exploratory Analysis","16f47d67":"### How can i improve this model?","48628fb0":"Let's compare performance of each to decide which one to go with","28fe3110":"Despite seeing from above, the best number of trees for GB is 50. Let's run Gridsearch for multiple parameter \nsearching that are best for this model","e721096e":"There are 12 features in total. Here's the detail explanation of each feature:\n1. Age: Age of the patient in years\n2. Sex: Gender of the patient. [M: Male, F: Female]\n3. ChestPain Type: Type of chest pain[ TA: Typical Angina, ATA: Atypical Angina, NAP: Non Anginal Pain, ASY: Asymptomatic]\n4. Resting BP: Resting blood pressure in units mm Hg\n5. Cholesterol: Serum Cholesterol in units mm\/dl\n6. FastingBS: Fasting Blood Sugar 1 if FastingBS > 120 mg\/dl, 0 Otherwise\n7. RestingECG: resting electrocardiogram results [Normal: Normal, ST: having ST-T wave abnormality (T wave inversions and\/or ST elevation or depression of > 0.05 mV), LVH: showing probable or definite left ventricular hypertrophy by Estes' criteria]\n8. MaxHR: maximum heart rate achieved [Numeric value between 60 and 202]\n9. ExerciseAngina: exercise-induced angina [Y: Yes, N: No]\n10. Oldpeak: oldpeak = ST [Numeric value measured in depression]\n11. ST_Slope: the slope of the peak exercise ST segment [Up: upsloping, Flat: flat, Down: downsloping]\n12. HeartDisease: output class [1: heart disease, 0: Normal]","22d5f7be":"#### Logistics Regression","c2844c02":"To begin with, let's explore pairplot across variables that might affect chances of Heart Disease","fce36fe8":"The following notebook encompasses a heart disease dataset with 918 observations. I've taken this dataset from kaggle https:\/\/www.kaggle.com\/fedesoriano\/heart-failure-prediction. It includes the features that are attributes of a patient such as Age, Sex, Chest Pain Type, Resting BP etc. And the Target variable tells whether the patient has a heart disease or not. \n\nThe purpose of this notebook is to come up with a model that can accurately predict given the attributes of a patient if they have a cardiovascular disease or not. A machine learning model of this kind can be useful in early detection and management","5c6591fc":"Let's go over the Accuracy Scores of each model and plot it","0d9822ce":"Let's explore the correlation of the features (numerical only)","ddfa608b":"From evaluating the model, the key findings are that:\n1. The highest accuracy is yielded by Random Forest and Stacking\n2. Random Forest however is better at predicting minority class as well through its score of Recall","cb909004":"From here, we can see that the optimal value of K is 22 after which the F1 scores drops and the error rate starts increasing","d002d33e":"Firstly, let's go over the classification report of each models","48366e45":"The correlation matrix is also a good place to check whether there are highly correlated features amongst independent variables as well. it would be useful to remove them if we decide to go with a non tree ML algorithm because they would cause multicollinearity. Keeping in mind a threshold of 95% correlation, i don't see any of the features being that correlated so we won't be removing any of the variable.\n\nMoreover, Heart Disease is highly correlated with most of the features, mostly MaxHR and Oldpeak (i.e. 40%). These two could be two important most features in our predictive model","4e1b655d":"### Feature Engineering\nNow that we have looked into the data, its time we engineer the features to maximize the prediction validations.\nFirstly, we determine which are numerical features and which are categorical.\nFollowing are the steps to follow after:\n\nNumerical Features:\n1. We check for skew. if there is a skew of over 0.75, then we apply log transformation. This helps us in normalizing the data and improves accuracy\n2. Standard scaling of other variables as it helps improving accuracy for non tree ML models\n3. If there is missing values, those can be imputed by a mean value\n4. Sometimes, log transformation can form infinite values, those can be converted to zeroes\n\nCategorical features:\n1. Label Encode as Predictive models other than lgb requires encoding \n\nI am going to define a function that will do all of it on its own. We will need to give an input of feature names as numerical and categorical"}}