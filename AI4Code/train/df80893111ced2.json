{"cell_type":{"38fde6ae":"code","c8c7dda8":"code","8411b11a":"code","e656054d":"code","015ca537":"code","de6eca54":"code","798d5b0b":"code","69e74be2":"code","7e72bb5a":"code","c8ff09d8":"code","1498c890":"code","e6d12faa":"code","a365bf65":"code","afca8434":"code","8623f540":"code","0d32fd93":"code","550b5d56":"code","470a7d32":"code","61284b99":"code","8f520c9e":"code","af91d513":"code","ed8743c0":"code","c1b1adaa":"code","6a2dd79d":"code","6e98d231":"code","005eeed7":"code","89e7f96d":"code","49fbc5c5":"code","863b6ca9":"code","fe4dbdb8":"code","af2a2f6e":"code","17a6627f":"code","750cb2ad":"code","3f93cb9b":"code","8000ab07":"code","02b1aac2":"code","0610f6a9":"code","47a46bf7":"code","776f486d":"code","14793ea1":"code","3edb0f8c":"code","ea972681":"code","dc33a0f0":"code","28584cb3":"code","cc229629":"code","3a4aea08":"code","bdee503b":"code","26f88b84":"code","5fb8bc73":"code","f7a044c5":"code","787a39bd":"code","59b825a4":"code","bbcae6d0":"code","b63f1c4d":"code","34bc5d5b":"code","9020f5c6":"code","8d7706bc":"markdown","23173683":"markdown","012d0d41":"markdown","46ea841e":"markdown"},"source":{"38fde6ae":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c8c7dda8":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns","8411b11a":"df= pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv' ,usecols=[\"SalePrice\", \"MSSubClass\", \"MSZoning\", \"LotFrontage\", \"LotArea\",\n                                         \"Street\", \"YearBuilt\", \"LotShape\", \"1stFlrSF\", \"2ndFlrSF\"]).dropna()\ndf.head()","e656054d":"df.shape","015ca537":"df.head()","de6eca54":"df.info()","798d5b0b":"for i in df.columns:\n    print(\"Column names {} and Unique Values are {}\".format(i,len(df[i].unique())))","69e74be2":"import datetime\ndatetime.datetime.now().year","7e72bb5a":"# We will create Derived Feature i.e Total Years ,we dont want YearBuilt \ndf['Total Years']=datetime.datetime.now().year-df['YearBuilt']","c8ff09d8":"df.drop('YearBuilt',axis=1,inplace=True)","1498c890":"df.columns","e6d12faa":"### Creating Categorical Variables\ncat_feat=[\"MSSubClass\",\"MSZoning\",\"Street\",\"LotShape\"]\nout_feat=\"SalePrice\"","a365bf65":"## Unique Values of MSSubClass now we will conert in categorical variable and label encoding\n\nfrom sklearn.preprocessing import LabelEncoder\nlbl_encoders={}\nlbl_encoders[\"MSSubClass\"]=LabelEncoder()\nlbl_encoders[\"MSSubClass\"].fit_transform(df[\"MSSubClass\"])","afca8434":"lbl_encoders","8623f540":"from sklearn.preprocessing import LabelEncoder\nlbl_encoders={}\nfor feature in cat_feat:\n    lbl_encoders[feature]=LabelEncoder()\n    df[feature]=lbl_encoders[feature].fit_transform(df[feature])","0d32fd93":"df","550b5d56":"#Stacking and converting into Tensors\ncat_feat=np.stack([df[\"MSSubClass\"],df[\"MSZoning\"],df[\"Street\"],df[\"LotShape\"]],1)\ncat_feat","470a7d32":"#Convert numpy to Tensors\n# Categorical Features cannot be converted to Float\nimport torch\ncat_feat= torch.tensor(cat_feat, dtype=torch.int64)\ncat_feat","61284b99":"#### create continuous Variable\ncont_feat=[]\nfor i in df.columns:\n    if i in [\"MSSubClass\",\"MSZoning\",\"Street\",\"LotShape\",\"SalePrice\"]:\n        pass\n    else:\n        cont_feat.append(i)\n        ","8f520c9e":"cont_feat","af91d513":"### Stacking continuous variable to a tensor\ncont_values=np.stack([df[i].values for i in cont_feat],axis=1)\ncont_values=torch.tensor(cont_values,dtype=torch.float)\ncont_values","ed8743c0":"cont_values.dtype","c1b1adaa":"### dependent Feature\ny=torch.tensor(df['SalePrice'].values,dtype=torch.float).reshape(-1,1)   ##converting to 2D feature\ny","6a2dd79d":"df.info()","6e98d231":"cat_feat.shape,cont_values.shape,y.shape","005eeed7":"len(df['MSSubClass'].unique())","89e7f96d":"cat_dims=[len(df[col].unique()) for col in [\"MSSubClass\", \"MSZoning\", \"Street\", \"LotShape\"]]\ncat_dims","49fbc5c5":"#Thumb Rule says--Output dimension ahould be set based on the input variable \n#The formula is (min(50,featur_dimension\/2))\nembedding_dims=[(x,min(50,(x+1)\/\/2)) for x in cat_dims]\nembedding_dims","863b6ca9":"import torch \nimport torch.nn as nn\nimport torch.nn.functional as F\nembed_representation=nn.ModuleList([nn.Embedding(inp,out) for inp,out in embedding_dims])\nembed_representation","fe4dbdb8":"cat_feat","af2a2f6e":"cat_featz=cat_feat[:4]\ncat_featz","17a6627f":"pd.set_option('display.max_rows',500)\nembedding_val=[]\nfor i,e in enumerate(embed_representation):               ## e is responsible for converting value to Vector\n    \n    embedding_val.append(e(cat_feat[:,i]))","750cb2ad":"embedding_val","3f93cb9b":"# Stacking should be Column Wise So we will be using Concatination Operation using Embedding Value\n\nz=torch.cat(embedding_val,1)            # So now all are stacked in one row\nz","8000ab07":"#We will apply Dropout layer which will help in avoiding Overfitting \n#After executing Some of the values become 0.So I am dropping 40% values\ndropout=nn.Dropout(.4)","02b1aac2":"final_embed=dropout(z)\nfinal_embed","0610f6a9":"### Create a Feed Forward Neural network\n\nimport torch \nimport torch.nn as nn\nimport torch.nn.functional as F\nclass FeedForwardNN(nn.Module):\n    \n    def __init__(self,embedding_dims,n_cont,out_sz,layers,p=0.5):\n        super().__init__()\n        self.embeds = nn.ModuleList([nn.Embedding(inp,out) for inp,out in embedding_dims])\n        self.emb_drop = nn.Dropout(p)\n        self.bn_cont = nn.BatchNorm1d(n_cont)\n        \n        layerlist=[]\n        n_emb= sum(out for inp,out in embedding_dims)                    ### calculate the total dimension of embedding layer\n        n_in= n_emb + n_cont\n        \n        for i in layers:\n            layerlist.append(nn.Linear(n_in,i))\n            layerlist.append(nn.ReLU(inplace=True))\n            layerlist.append(nn.BatchNorm1d(i))\n            layerlist.append(nn.Dropout(p))\n            n_in=i\n       \n        layerlist.append(nn.Linear(layers[-1],out_sz))\n        \n        self.layers=nn.Sequential(*layerlist)\n     \n    def forward(self,x_cat,x_cont):\n        embeddings=[]\n        for i,e in enumerate(self.embeds):\n            embeddings.append(e(x_cat[:,i]))\n        x= torch.cat(embeddings,1)                      ## concatinating the embeddings and applying Dropout\n        x= self.emb_drop(x)\n    \n        x_cont= self.bn_cont(x_cont)\n        x= torch.cat([x,x_cont], 1)\n        x= self.layers(x)\n        return x","47a46bf7":"torch.manual_seed(100)\nmodel=FeedForwardNN(embedding_dims, len(cont_feat),1,[100,50],p=0.1)","776f486d":"model","14793ea1":"loss_func= nn.MSELoss()       ## Convert into RMSE later\noptimizer= torch.optim.Adam(model.parameters(),lr=0.1)","3edb0f8c":"df.shape","ea972681":"cont_values","dc33a0f0":"cont_values.shape","28584cb3":"# Train test split\n\nbatch_size=1200\ntest_size= int(batch_size*0.15)\ntrain_categorical=  cat_feat[:batch_size-test_size]\ntest_categorical= cat_feat[batch_size-test_size:batch_size]\ntrain_cont= cont_values[:batch_size-test_size]\ntest_cont= cont_values[batch_size-test_size:batch_size]\ny_train= y[:batch_size-test_size]\ny_test= y[batch_size-test_size:batch_size]","cc229629":"len(train_categorical),len(test_categorical),len(train_cont),len(test_cont),len(y_train),len(y_test)","3a4aea08":"\nepochs=5000\nfinal_losses=[]\nfor i in range(epochs):\n    i=i+1\n    y_pred= model(train_categorical,train_cont)\n    loss= torch.sqrt(loss_func(y_pred,y_train))     ## RMSE\n    final_losses.append(loss)\n    if i%10==1:\n        print(\"Epoch number: {} and the Loss: {}\".format(i,loss.item()))\n    optimizer.zero_grad()\n    loss.backward()         ##back propogation\n    optimizer.step()\n    ","bdee503b":"import matplotlib.pyplot as plt\n%matplotlib inline\nplt.plot(range(epochs),final_losses)\nplt.ylabel('RMSE loss')\nplt.xlabel('Epochs');","26f88b84":"#### Validate the test data\ny_pred=\"\"\nwith torch.no_grad():\n    y_pred= model(test_categorical,test_cont)\n    loss=torch.sqrt(loss_func(y_pred,y_test))\n    \nprint(\"RMSE: {}\" .format(loss))","5fb8bc73":"data_verify= pd.DataFrame(y_test.tolist(),columns=[\"test\"])\ndata_predicted=pd.DataFrame(y_pred.tolist(),columns=[\"Prediction\"])","f7a044c5":"data_predicted","787a39bd":"data_predicted.to_csv('torchpredictions.csv')","59b825a4":"final_output=pd.concat([data_verify,data_predicted],axis=1)\nfinal_output[\"Difference\"]= final_output['test']-final_output['Prediction']\nfinal_output.head()","bbcae6d0":"## Svaing the model\n## Save the model\ntorch.save(model,'HousePrice.pt')","b63f1c4d":"torch.save(model.state_dict,'HouseWeights.pt')           ## state_dict helps in saving Weights","34bc5d5b":"## Loading the saved Model\nemb_size=[(15,8),(5,3),(2,1),(4,2)]\nmodel1= FeedForwardNN(emb_size,5,1,[100,50],p=0.4)\n","9020f5c6":"model1.eval","8d7706bc":"## Before Proceeding into the depth of notebook ,have a look at the given fastAI useful Links.These are useful because in this house price data we are dealing with Categorical Dataset and categorical embeddings\n### http:\/\/docs.fast.ai\/tabular.html\n### https:\/\/www.fast.ai\/2018\/04\/29\/categorical-embeddings\/\n","23173683":"### Define Loss and Optimizer","012d0d41":"### Embedding Size For Categorical columns","46ea841e":"# **Thankyou for visiting the kernel. I will be happy if you find this useful .Please leave an upvote which is a kind of motivation.**\n\n### I have tried to make the useful codes understandable by marking the comments still anything to learn or add .please feel free to do so in comments."}}