{"cell_type":{"a2d7c171":"code","ee47ddf2":"code","70ffa243":"code","a4d0fb0d":"code","19e59e27":"code","cbed0c6d":"code","2a6776db":"code","1a6b67d8":"code","33e81d65":"code","d68efb5d":"code","cb92de7a":"code","5f1382f4":"code","4ca3e051":"code","d09f6ed8":"code","b15d435d":"code","71c8e3fc":"code","fa7e01e2":"code","67bcde3d":"code","5366a161":"code","aecab166":"code","f92d8fe8":"markdown","6822afa5":"markdown","78ed4572":"markdown","409d9e2f":"markdown","228adfd0":"markdown","ac0086cd":"markdown","0539e871":"markdown"},"source":{"a2d7c171":"import pandas as pd\nimport itertools\nimport numpy as np\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import sent_tokenize\nimport random\nimport re","ee47ddf2":"# tennis.csv contains 8 online articles about tennis\ndf = pd.read_csv(\"..\/input\/reports\/tennis.csv\")\ndf","70ffa243":"# Show the 8 articles\nfor art in df['article_text']:\n    print(f'{art}\\n\\n')","a4d0fb0d":"sentences = []\nfor s in df['article_text']:\n    sentences.append(sent_tokenize(s))\n\n# Extract the longest sentence of each text to be able to check,\n# if the algorithm performs better then just picking the longest\n# sentence\nlongest_sentences = []\n    \nfor item in sentences:\n    sorteditems = sorted(item, key=len)\n    longest_sentences.append((sorteditems[-1]))\n\nlongest_sentences","19e59e27":"# Flatten nested list of sentences \n# to have a single list of all sentences\nsentences = list(itertools.chain(*sentences))\nsentences[:5]","cbed0c6d":"clean_sentences = pd.Series(sentences).str.replace(\"[^a-zA-Z]\", \" \")\nclean_sentences = [s.lower() for s in clean_sentences]\nstop_words = stopwords.words('english')\n\ndef remove_stopwords(words: list) -> str:\n    \"\"\"Remove stopwords from a list of words and return the remaining words\n       as a string joint by the space char. \n    \"\"\"\n    sentence = \" \".join([word for word in words if word not in stop_words])\n    return sentence\n\nclean_sentences = [remove_stopwords(r.split()) for r in clean_sentences]\nprint(sentences[:5])\nclean_sentences[:5]","2a6776db":"word_embeddings = {}\n\nf = open('..\/input\/glove6b\/glove.6B.300d.txt', encoding='utf-8')\nfor line in f:\n    values = line.split()\n    word = values[0]\n    coefs = np.asarray(values[1:], dtype='float32')\n    word_embeddings[word] = coefs\nf.close()","1a6b67d8":"word_embeddings['world']","33e81d65":"sentence_vectors = []\nwords_not_in_word_embeddings = set()\n\ndef get_vectors_from_word_embeddings(w):\n    try:\n        vec = word_embeddings[w]\n    except KeyError:\n        vec = np.zeros((300,))\n        words_not_in_word_embeddings.add(w)\n    return vec\n        \nfor sentence in clean_sentences:\n    if len(sentence) != 0:\n        word_list = sentence.split()\n        vec_list = []\n        for w in word_list:\n            vec = get_vectors_from_word_embeddings(w)\n            vec_list.append(vec)\n        v = sum(vec_list)\/(len(word_list)+0.001)\n    else:\n        v = np.zeros((300,))\n    sentence_vectors.append(v)","d68efb5d":"words_not_in_word_embeddings","cb92de7a":"print(len(sentences))\nprint(len(clean_sentences))\nlen(sentence_vectors)","5f1382f4":"sentence_vectors[0]","4ca3e051":"from sklearn.metrics.pairwise import cosine_similarity\n\n# Spoiler alert: 119\ndataset_length = len(sentences)\n\nsimilarity_matrix = np.zeros([dataset_length, dataset_length])\nfor i in range(dataset_length):\n    for j in range(dataset_length):\n        # ignore the diagonal\n        if i != j:\n            similarity_matrix[i][j] = cosine_similarity(sentence_vectors[i].reshape(1,300), sentence_vectors[j].reshape(1,300))[0,0]\n","d09f6ed8":"# 119x119 Matrix\nsimilarity_matrix[118][117]","b15d435d":"import networkx as nx\n\nnx_graph = nx.from_numpy_array(similarity_matrix)\nscores = nx.pagerank(nx_graph)","71c8e3fc":"scores","fa7e01e2":"ranked_sentences = sorted(((scores[i],s) for i,s in enumerate(sentences)), reverse=True)","67bcde3d":"print(len(ranked_sentences))\nranked_sentences[:5]","5366a161":"# ranked_sentences contains the highest ranked sentence for every text. It is a list of tuples (score, sentence)\n# How can this be mapped back to the texts? Why do we need to compute the pagerank for every sentence against every other sentence?\nfor sentence in ranked_sentences:\n    print(sentence[0], sentence[1])","aecab166":"from termcolor import colored\nfor i, article in enumerate(df['article_text']):\n    print(colored((\"ARTICLE:\".center(50)),'yellow'))\n    print('\\n')\n    print(colored((article),'blue'))\n    print('\\n')\n    print(colored((\"SUMMARY:\".center(50)),'green'))\n    print('\\n')\n    print(colored((f\"Summary in Text? {ranked_sentences[i][1] in article}\".center(50)),'green'))\n    print('\\n')\n    print(colored((f'{ranked_sentences[i][1]} - Score: {ranked_sentences[i][0]}'),'cyan'))\n    print('\\n')","f92d8fe8":"## Cleaning the sentences\n\n* replace every non alphabetic character with a space\n* lowercase all the words\n* remove all stopwords as per `nltk.corpus.stopwords`","6822afa5":"## Apply PageRank algorithm\n\nRunning the PageRank algorithm on the similarity matrix, which determines the most relevant sentence in an article. [NetworkX PageRank](https:\/\/networkx.org\/documentation\/networkx-1.2\/reference\/generated\/networkx.pagerank.html)\n\n### XXX from score to ranked sentences\n\nThe whole networkx part is a bit unclear to me in the moment.","78ed4572":"## Compute a vector for every sentence\n\nIn the last step the dictionary `word_embeds` has been created. It contains 300 vectors for every occuring word. In the following step 300 vectors are composed (by summing the word vectors) on the sentence level. For normalization the vector values of the sentence are divided by the number of words in the sentence.","409d9e2f":"## Tokenize the texts into sentences\n\nUsing: [nltk.tokenize](https:\/\/www.nltk.org\/api\/nltk.tokenize.html) - split the texts into lists of sentences and create a single list of all the sentences. ","228adfd0":"## Create a word embedings dictionary\n\nThe project makes use of a pre-trained dataset that contains vector representation for words. The dataset has been created using the unsupervised learning algorithm GloVe [GloVe: Global Vectors for Word Representation at Stanford](https:\/\/nlp.stanford.edu\/projects\/glove\/). It uses the Wikipedia 2014 + Gigaword 5 (6B tokens, 400K vocab, uncased, 300d vectors, 822 MB download) data set, which is published under the `Public Domain Dedication and License v1.0`.\n\nThe dictionary `word_embeddings` contains a word as key and 300 numeric vector values as value.","ac0086cd":"## Compute a similarity matrix\n\nUse `cosine_similarity` from `sklearn` ([Cosine similarity](https:\/\/scikit-learn.org\/stable\/modules\/metrics.html#cosine-similarity)) to compute how similar \n1 sentence to every other sentence in the dataset is.\n\n### XXX reshape\n\nI am not 100% sure, why we are applying `reshape`. The high level explanation I found elsewhere is, that\n`scikit-learn` requires the reshaped version of the array.\n\n```python\n  >>> sentence_vectors[0]\n  ... array([-4.61076051e-02,...], dtype=float32)\n  >>> sentence_vectors[0].reshape(1,300)\n  ... array([[-4.61076051e-02,...]], dtype=float32)\n```","0539e871":"**Note:** I've been playing around with this approach following this article (https:\/\/thecleverprogrammer.com\/2020\/08\/24\/summarize-text-with-machine-learning\/) and this kaggle notebook (https:\/\/www.kaggle.com\/gauravduttakiit\/summarize-text-with-machine-learning\/). Seems like the ranked sentence to text matching is off somewhere. See last cell in this notebook or even check on the original https:\/\/www.kaggle.com\/gauravduttakiit\/summarize-text-with-machine-learning\/, where the extracted sentence, with the highest score is not in the text and therefore is not a good summary of the text.\n\n## Extractive Approach\n\nThe Extractive approach takes sentences directly from the document according to a scoring function to form a cohesive summary. This method works by identifying the important sections of the text cropping and assembling parts of the content to produce a condensed version.\n\nThis project summarizes a text using the TextRank algorithm, which is an extractive and unsupervised machine learning algorithm. [NetworkX PageRank](https:\/\/networkx.org\/documentation\/networkx-1.2\/reference\/generated\/networkx.pagerank.html)\n\n## Steps\n\n* Tokenize texts into sentences to obtain\n* Cleaning the sentences (remove punctuation, case, stopwords)\n* Create a word embedings dictionary from GloVe dataset\n* Compute vectors for every sentence\n* Compute a similarity matrix\n* Apply PageRank algorithm to find the most important sentence per text"}}