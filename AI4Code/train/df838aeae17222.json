{"cell_type":{"530743b3":"code","b016c357":"code","fc1e1333":"code","5d1a1635":"code","495fad1a":"code","74923db4":"code","c3caa29f":"code","2d74e450":"code","9667d86a":"code","5dbff1ce":"code","9115af5f":"code","0c686c90":"code","cca4a469":"code","ac6242cb":"code","e8c2f268":"code","306b9a7a":"code","452468c7":"code","46f5f399":"code","c7337940":"code","d14e375c":"code","71e155cc":"code","6319ac76":"code","85d20f64":"code","fe1b4bdc":"code","1d238468":"code","4b48a4ac":"code","f8a94037":"code","0628b02b":"code","673cb5b0":"code","13dc6680":"code","5cbd104d":"code","60940ba7":"code","45af703a":"code","5b4ab9f3":"code","67b10691":"code","91b92770":"code","0adc51d3":"code","44046a12":"code","41c5bc63":"code","ecc315fa":"code","d4068104":"code","9e4df2fa":"code","db7db616":"code","b3b0df94":"code","2a921139":"code","9ee97d52":"code","7b60d269":"code","19b3da8c":"code","bc707be1":"code","cb9cf42c":"code","7fe37823":"code","35f1e31b":"code","6cfd7f30":"code","16dda401":"code","3d870b0e":"code","7a09f90f":"code","238bb8a6":"code","520da029":"code","58a3b61a":"code","64361417":"code","8fc2adba":"code","a9f104f1":"code","c51dc022":"code","763e4085":"code","194719ac":"code","f405729a":"code","ab54a11e":"code","c80c9e88":"code","cf8e2e7e":"code","88103811":"code","5df4f23d":"code","97231a9d":"code","d9c94c52":"code","a61b5075":"code","607e70af":"code","69b4c2a9":"code","c7616318":"code","f07436ac":"code","c2b51b52":"code","a41aa513":"code","6ab47b47":"code","e809b83c":"code","213f247c":"code","48a4e9a2":"code","3cc3251d":"code","7c8e9978":"code","43173acf":"code","9dd84d58":"code","af857af9":"code","2aa64b81":"code","86a1def1":"code","4053d763":"code","bcb41277":"code","24a9a705":"code","15c5263f":"code","512ba326":"code","71ef19fd":"code","4755f8df":"code","79fe634e":"code","97eaddba":"code","b38b5456":"code","bfe1b58b":"code","27709c9d":"code","55aa25ca":"code","5f826f62":"code","88667cc9":"code","4e15fcf9":"code","9c769c65":"code","a586e91e":"code","54ab3fe7":"code","dca884d6":"code","ff3d1847":"code","2645a27f":"code","a0ce936b":"code","d2226eb3":"code","042fd2b0":"code","f9b48bbd":"code","3c937bce":"code","e7597d6c":"code","ebc3739f":"code","c0f21275":"code","52ac13ad":"code","393dbb8f":"code","290935d3":"code","e9a25f1e":"code","d4586fe3":"code","0e5a409d":"code","8003ea07":"code","9d55161d":"code","0232724d":"code","63b965dc":"code","a23451ab":"code","5f35ab8d":"code","63e92e6e":"code","e73f7dd3":"code","f3d0b040":"code","84e5d129":"code","d73e1ac0":"markdown","f8b71065":"markdown","130e9392":"markdown","f775905b":"markdown","5c71028d":"markdown","641100a2":"markdown","d8c0504d":"markdown","f4107786":"markdown","240a95b2":"markdown","27af4afb":"markdown","f8e84a52":"markdown","54ed6db1":"markdown"},"source":{"530743b3":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\nimport warnings\nwarnings.filterwarnings('ignore')\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","b016c357":"import numpy as np\nimport pandas as pd\nimport cv2\nfrom PIL import Image\nimport scipy\n\nimport tensorflow as tf\nfrom tensorflow.keras.applications import *\nfrom tensorflow.keras.optimizers import *\nfrom tensorflow.keras.losses import *\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras.models import *\nfrom tensorflow.keras.callbacks import *\nfrom tensorflow.keras.preprocessing.image import *\nfrom tensorflow.keras.utils import *\nfrom sklearn.neural_network import MLPClassifier\n# import pydot\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import *\nfrom sklearn.model_selection import *\nimport tensorflow.keras.backend as K\n\nfrom tqdm import tqdm, tqdm_notebook\nfrom colorama import Fore\nimport json\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom glob import glob\nfrom skimage.io import *\n%config Completer.use_jedi = False\nimport time\nfrom sklearn.decomposition import PCA\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\nimport lightgbm as lgb\nfrom xgboost import XGBClassifier\nfrom sklearn.ensemble import AdaBoostClassifier,RandomForestClassifier\n\nfrom sklearn.metrics import confusion_matrix\n\nprint(\"All modules have been imported\")","fc1e1333":"info=pd.read_csv(\"..\/input\/prepossessed-arrays-of-binary-data\/1000_Binary Dataframe\")\ninfo=info.drop('Unnamed: 0',axis=1)\ninfo.head()","5d1a1635":"info.level.value_counts()","495fad1a":"sns.set_style('darkgrid')\nfig, ax = plt.subplots(figsize=(10,5))\nsns.barplot(x=info.level.unique(),y=info.level.value_counts(),palette='Blues_r',ax=ax)","74923db4":"sizes = info['level'].values\nsns.distplot(sizes, kde=False)","c3caa29f":"Binary_90 = np.load('..\/input\/prepossessed-arrays-of-binary-data\/1000_Binary_images_data_90.npz')\nX_90=Binary_90['a']\nBinary_128 = np.load('..\/input\/prepossessed-arrays-of-binary-data\/1000_Binary_images_data_128.npz')\nX_128=Binary_128['a']\nBinary_264 = np.load('..\/input\/prepossessed-arrays-of-binary-data\/1000_Binary_images_data_264.npz')\nX_264=Binary_264['a']\ny=info['level'].values\n\n\nprint(X_90.shape)\nprint(X_128.shape)\nprint(X_264.shape)\nprint(y.shape)","2d74e450":"print(\"Shape before reshaping X_90\" +str(X_90.shape))\nX_90=X_90.reshape(1000,90,90,3)\nprint(\"Shape after reshaping X_90\" +str(X_90.shape))\nprint(\"\\n\\n\")\n\nprint(\"Shape before reshaping X_128\" +str(X_128.shape))\nX_128=X_128.reshape(1000,128,128,3)\nprint(\"Shape after reshaping X_128\" +str(X_128.shape))\nprint(\"\\n\\n\")\n\nprint(\"Shape before reshaping X_264\" +str(X_264.shape))\nX_264=X_264.reshape(1000,264,264,3)\nprint(\"Shape after reshaping X_264\" +str(X_264.shape))","9667d86a":"plt.title(\"90*90*3 Image\")\nplt.imshow(X_90[1])\nplt.show()\n\nplt.title(\"128*128*3 Image\")\nplt.imshow(X_128[1])\nplt.show()\n\nplt.title(\"264*264*3 Image\")\nplt.imshow(X_264[1])\nplt.show()","5dbff1ce":"y.shape","9115af5f":"X=np.array(X_264)\nY=np.array(y)\n# Y=to_categorical(Y,5)\nx_train, x_test1, y_train, y_test1 = train_test_split(X, Y, test_size=0.4, random_state=42)\nx_val, x_test, y_val, y_test = train_test_split(x_test1, y_test1, test_size=0.5, random_state=42)\nprint(len(x_train),len(x_val),len(x_test))","0c686c90":"Y1=pd.DataFrame(Y)\nY1.value_counts()","cca4a469":"# Defining our DNN Model\ndnn_model=Sequential()\ndnn_model.add(Dense(8, input_dim=3, kernel_initializer = 'uniform', activation = 'relu'))\ndnn_model.add(BatchNormalization())\ndnn_model.add(Dropout(0.2))\ndnn_model.add(Dense(16, kernel_initializer = 'uniform', activation = 'relu' ))\ndnn_model.add(BatchNormalization())\ndnn_model.add(Dropout(0.2))\ndnn_model.add(Dense(32, kernel_initializer = 'uniform', activation = 'relu' ))\ndnn_model.add(BatchNormalization())\ndnn_model.add(Dropout(0.2))\ndnn_model.add(Dense(64, kernel_initializer = 'uniform', activation = 'relu' ))\ndnn_model.add(BatchNormalization())\ndnn_model.add(Dropout(0.2))\ndnn_model.add(Dense(128, kernel_initializer = 'uniform', activation = 'relu'))\ndnn_model.add(BatchNormalization())\ndnn_model.add(Dropout(0.2))\ndnn_model.add(Dense(256, kernel_initializer = 'uniform', activation = 'relu' ))\ndnn_model.add(BatchNormalization())\ndnn_model.add(Dropout(0.2))\ndnn_model.add(Dense(128, kernel_initializer = 'uniform', activation = 'relu' ))\ndnn_model.add(BatchNormalization())\ndnn_model.add(Dropout(0.2))\ndnn_model.add(Dense(64, kernel_initializer = 'uniform', activation = 'relu' ))\ndnn_model.add(BatchNormalization())\ndnn_model.add(Dropout(0.2))\ndnn_model.add(Dense(32, kernel_initializer = 'uniform', activation = 'relu' ))\ndnn_model.add(BatchNormalization())\ndnn_model.add(Dropout(0.2))\ndnn_model.add(Dense(16, kernel_initializer = 'uniform', activation = 'relu' ))\ndnn_model.add(BatchNormalization())\ndnn_model.add(Dropout(0.2))\ndnn_model.add(Dense(8, kernel_initializer = 'uniform', activation = 'relu' ))\ndnn_model.add(BatchNormalization())\ndnn_model.add(Dropout(0.2))\ndnn_model.add(Dense(3,activation='softmax'))\ndnn_model.summary()","ac6242cb":"from sklearn.pipeline import make_pipeline\nfrom sklearn.pipeline import Pipeline\nnames = [\n        \"K Nearest Neighbour Classifier\",\n        'SVM',\n        \"Random Forest Classifier\",\n        \"AdaBoost Classifier\", \n        \"XGB Classifier\",\n        \"MLP Classifier\"\n         ]\nclassifiers = [\n    KNeighborsClassifier(n_neighbors = 5, algorithm='ball_tree', leaf_size=30),\n    SVC(),\n    RandomForestClassifier(max_depth=9,criterion = 'entropy'),\n    AdaBoostClassifier(),\n    XGBClassifier(),\n    MLPClassifier()\n        ]\nzipped_clf = zip(names,classifiers)","e8c2f268":"def classifier_summary(pipeline, X_train, y_train, X_val, y_val,X_test,y_test):\n    sentiment_fit = pipeline.fit(X_train, y_train)\n    \n    y_pred_train= sentiment_fit.predict(X_train)\n    y_pred_val = sentiment_fit.predict(X_val)\n    y_pred_test = sentiment_fit.predict(X_test)\n    \n    y_pred_train = [1 if x>0.5 else 0 for x in y_pred_train]\n    y_pred_val = [1 if x>0.5 else 0 for x in y_pred_val]\n    y_pred_test = [1 if x>0.5 else 0 for x in y_pred_test]\n    \n    train_accuracy = np.round(accuracy_score(y_train, y_pred_train),4)*100\n    train_precision = np.round(precision_score(y_train, y_pred_train, average='weighted'),4)\n    train_recall = np.round(recall_score(y_train, y_pred_train, average='weighted'),4)\n    train_F1 = np.round(f1_score(y_train, y_pred_train, average='weighted'),4)\n    train_kappa =  np.round(cohen_kappa_score(y_train, y_pred_train),4)\n    \n    \n    val_accuracy = np.round(accuracy_score(y_val, y_pred_val),4)*100\n    val_precision = np.round(precision_score(y_val, y_pred_val, average='weighted'),4)\n    val_recall = np.round(recall_score(y_val, y_pred_val, average='weighted'),4)\n    val_F1 = np.round(f1_score(y_val, y_pred_val, average='weighted'),4)\n    val_kappa =  np.round(cohen_kappa_score(y_val, y_pred_val),4)\n   \n    \n    test_accuracy = np.round(accuracy_score(y_test, y_pred_test),4)*100\n    test_precision = np.round(precision_score(y_test, y_pred_test, average='weighted'),2)\n    test_recall = np.round(recall_score(y_test, y_pred_test, average='weighted'),2)\n    test_F1 = np.round(f1_score(y_test, y_pred_test, average='weighted'),2)\n    test_kappa =  np.round(cohen_kappa_score(y_test, y_pred_test),2) \n  \n    \n    \n    print()\n    print('------------------------ Train Set Metrics------------------------')\n    print()\n    print(\"Accuracy core : {}%\".format(train_accuracy)) \n    \n    print('------------------------ Validation Set Metrics------------------------')\n    print()\n    print(\"Accuracy score : {}%\".format(val_accuracy))\n                          \n    print('------------------------ Test Set Metrics------------------------')\n    print()\n    print(\"Accuracy score : {}%\".format(test_accuracy))\n    print(\"F1_score : {}\".format(test_F1))\n    print(\"Kappa Score : {} \".format(test_kappa))\n    print(\"Recall score: {}\".format(test_recall))\n    print(\"Precision score : {}\".format(test_precision))\n          \n    print(\"-\"*80)\n    print()","306b9a7a":"def classifier_comparator(X_train,y_train,X_val,y_val,X_test,y_test,classifier=zipped_clf): \n    result = []\n    for n,c in classifier:\n        checker_pipeline = Pipeline([('Classifier', c)])\n        print(\"------------------------------Fitting {} on input_data-------------------------------- \".format(n))\n        #print(c)\n        classifier_summary(checker_pipeline,X_train, y_train, X_val, y_val,X_test,y_test)","452468c7":"base_model= ResNet50(input_shape=(264,264,3), weights='imagenet', include_top=False)\nx = base_model.output\nx = Dropout(0.5)(x)\nx = Flatten()(x)\nx = BatchNormalization()(x)\nx = Dense(16,kernel_initializer='he_uniform')(x)\nx = BatchNormalization()(x)\nx = Activation('relu')(x)\nx = Dropout(0.5)(x)\nx = Dense(32,kernel_initializer='he_uniform')(x)\nx = BatchNormalization()(x)\nx = Activation('relu')(x)\nx = Dropout(0.5)(x)\nx = Dense(64,kernel_initializer='he_uniform')(x)\nx = BatchNormalization()(x)\nx = Activation('relu')(x)\nx = Dropout(0.5)(x)\nx = Dense(128,kernel_initializer='he_uniform')(x)\nx = BatchNormalization()(x)\nx = Activation('relu')(x)\nx = Dropout(0.5)(x)\nx = Dense(256,kernel_initializer='he_uniform')(x)\nx = BatchNormalization()(x)\nx = Activation('relu')(x)\nx = Dropout(0.5)(x)\npredictions = Dense(3, activation='softmax')(x)\n\nmodel_feat = Model(inputs=base_model.input,outputs=predictions)\n\ntrain_features = model_feat.predict(x_train)\nval_features=model_feat.predict(x_val)\ntest_features=model_feat.predict(x_test)","46f5f399":"from sklearn.pipeline import make_pipeline\nfrom sklearn import pipeline\nfrom sklearn.pipeline import Pipeline\nnames = [\n        \"K Nearest Neighbour Classifier\",\n        'SVM',\n        \"Random Forest Classifier\",\n        \"AdaBoost Classifier\", \n        \"XGB Classifier\",\n        \"MLP Classifier\"\n         ]\nclassifiers = [\n    KNeighborsClassifier(n_neighbors = 5, algorithm='ball_tree', leaf_size=30),\n    SVC(),\n    RandomForestClassifier(max_depth=9,criterion = 'entropy'),\n    AdaBoostClassifier(),\n    XGBClassifier(),\n    MLPClassifier()\n        ]\nzipped_clf = zip(names,classifiers)\ndef classifier_summary(pipeline, X_train, y_train, X_val, y_val,X_test,y_test):\n    sentiment_fit = pipeline.fit(X_train, y_train)\n    \n    y_pred_train= sentiment_fit.predict(X_train)\n    y_pred_val = sentiment_fit.predict(X_val)\n    y_pred_test = sentiment_fit.predict(X_test)\n    \n    train_accuracy = np.round(accuracy_score(y_train, y_pred_train),4)*100\n    train_precision = np.round(precision_score(y_train, y_pred_train, average='weighted'),4)\n    train_recall = np.round(recall_score(y_train, y_pred_train, average='weighted'),4)\n    train_F1 = np.round(f1_score(y_train, y_pred_train, average='weighted'),4)\n    train_kappa =  np.round(cohen_kappa_score(y_train, y_pred_train),4)\n    \n    \n    val_accuracy = np.round(accuracy_score(y_val, y_pred_val),4)*100\n    val_precision = np.round(precision_score(y_val, y_pred_val, average='weighted'),4)\n    val_recall = np.round(recall_score(y_val, y_pred_val, average='weighted'),4)\n    val_F1 = np.round(f1_score(y_val, y_pred_val, average='weighted'),4)\n    val_kappa =  np.round(cohen_kappa_score(y_val, y_pred_val),4)\n    \n    \n    test_accuracy = np.round(accuracy_score(y_test, y_pred_test),4)*100\n    test_precision = np.round(precision_score(y_test, y_pred_test, average='weighted'),2)\n    test_recall = np.round(recall_score(y_test, y_pred_test, average='weighted'),2)\n    test_F1 = np.round(f1_score(y_test, y_pred_test, average='weighted'),2)\n    test_kappa =  np.round(cohen_kappa_score(y_test, y_pred_test),2) \n  \n    \n    \n    print()\n    print('------------------------ Train Set Metrics------------------------')\n    print()\n    print(\"Accuracy core : {}%\".format(train_accuracy))\n    \n    print('------------------------ Validation Set Metrics------------------------')\n    print()\n    print(\"Accuracy score : {}%\".format(val_accuracy))\n   \n    print('------------------------ Test Set Metrics------------------------')\n    print(\"Accuracy score : {}%\".format(test_accuracy))\n    \n    print()\n    print(\"Accuracy score : {}%\".format(test_accuracy))\n    print(\"F1_score : {}\".format(test_F1))\n    print(\"Kappa Score : {} \".format(test_kappa))\n    print(\"Recall score: {}\".format(test_recall))\n    print(\"Precision score : {}\".format(test_precision))\n    \n    print(\"-\"*80)\n    print()\n    \ndef classifier_comparator(X_train,y_train,X_val,y_val,X_test,y_test,classifier=zipped_clf): \n    result = []\n    for n,c in classifier:\n        checker_pipeline = Pipeline([('Classifier', c)])\n        print(\"------------------------------Fitting {} on input_data-------------------------------- \".format(n))\n        #print(c)\n        classifier_summary(checker_pipeline,X_train, y_train, X_val, y_val,X_test,y_test)","c7337940":"classifier_comparator(train_features,y_train,val_features,y_val,test_features,y_test,classifier=zipped_clf)","d14e375c":"train_y=to_categorical(y_train,3)\nval_y=to_categorical(y_val,3)\ntest_y=to_categorical(y_test,3)\ndnn_model.compile(optimizer='adam',loss='categorical_crossentropy', metrics=['accuracy'])\nhistory = dnn_model.fit(train_features, train_y,validation_data=(val_features,val_y), epochs=10)\nloss_value , accuracy = dnn_model.evaluate(train_features, train_y)\nprint('Train_accuracy is:' + str(accuracy))\nloss_value , accuracy = dnn_model.evaluate(val_features, val_y)\nprint('Validation_accuracy is := ' + str(accuracy))\nloss_value , accuracy = dnn_model.evaluate(test_features, test_y)\nprint('test_accuracy is : = ' + str(accuracy))","71e155cc":"print(\"Performance Report:\")\ny_pred10=dnn_model.predict_classes(test_features)\ny_test10=[np.argmax(x) for x in test_y]\ny_pred_prb10=dnn_model.predict_proba(test_features)\ntarget=['0','1']\nfrom sklearn import metrics\nprint('Accuracy score is :', np.round(metrics.accuracy_score(y_test10, y_pred10),4))\nprint('Precision score is :', np.round(metrics.precision_score(y_test10, y_pred10, average='weighted'),4))\nprint('Recall score is :', np.round(metrics.recall_score(y_test10,y_pred10, average='weighted'),4))\nprint('F1 Score is :', np.round(metrics.f1_score(y_test10, y_pred10, average='weighted'),4))\nprint('Cohen Kappa Score:', np.round(metrics.cohen_kappa_score(y_test10, y_pred10),4))\nprint('\\t\\tClassification Report:\\n', metrics.classification_report(y_test10, y_pred10,target_names=target))","6319ac76":"knn = KNeighborsClassifier(n_neighbors = 5, algorithm='ball_tree', leaf_size=30)\nknn.fit(train_features, y_train)\nplot_confusion_matrix(knn, test_features, y_test)","85d20f64":"svc = SVC()\nsvc.fit(train_features, y_train)\nplot_confusion_matrix(svc, test_features, y_test)","fe1b4bdc":"rf = RandomForestClassifier()\nrf.fit(train_features, y_train)\nplot_confusion_matrix(rf, test_features, y_test)","1d238468":"ada = AdaBoostClassifier()\nada.fit(train_features, y_train)\nplot_confusion_matrix(ada, test_features, y_test)","4b48a4ac":"xgbc = XGBClassifier()\nxgbc.fit(train_features, y_train)\nplot_confusion_matrix(xgbc, test_features, y_test)","f8a94037":"mlpc = MLPClassifier()\nmlpc.fit(train_features, y_train)\nplot_confusion_matrix(mlpc, test_features, y_test)","0628b02b":"base_model= VGG16(input_shape=(264,264,3), weights='imagenet', include_top=False)\nx = base_model.output\nx = Dropout(0.5)(x)\nx = Flatten()(x)\nx = BatchNormalization()(x)\nx = Dense(16,kernel_initializer='he_uniform')(x)\nx = BatchNormalization()(x)\nx = Activation('relu')(x)\nx = Dropout(0.5)(x)\nx = Dense(32,kernel_initializer='he_uniform')(x)\nx = BatchNormalization()(x)\nx = Activation('relu')(x)\nx = Dropout(0.5)(x)\nx = Dense(64,kernel_initializer='he_uniform')(x)\nx = BatchNormalization()(x)\nx = Activation('relu')(x)\nx = Dropout(0.5)(x)\nx = Dense(128,kernel_initializer='he_uniform')(x)\nx = BatchNormalization()(x)\nx = Activation('relu')(x)\nx = Dropout(0.5)(x)\nx = Dense(256,kernel_initializer='he_uniform')(x)\nx = BatchNormalization()(x)\nx = Activation('relu')(x)\nx = Dropout(0.5)(x)\npredictions = Dense(3, activation='softmax')(x)\n\nmodel_feat = Model(inputs=base_model.input,outputs=predictions)\n\ntrain_features = model_feat.predict(x_train)\nval_features=model_feat.predict(x_val)\ntest_features=model_feat.predict(x_test)","673cb5b0":"from sklearn.pipeline import make_pipeline\nfrom sklearn.pipeline import Pipeline\nnames = [\n        \"K Nearest Neighbour Classifier\",\n        'SVM',\n        \"Random Forest Classifier\",\n        \"AdaBoost Classifier\", \n        \"XGB Classifier\",\n        \"MLP Classifier\"\n         ]\nclassifiers = [\n    KNeighborsClassifier(n_neighbors = 5, algorithm='ball_tree', leaf_size=30),\n    SVC(),\n    RandomForestClassifier(max_depth=9,criterion = 'entropy'),\n    AdaBoostClassifier(),\n    XGBClassifier(),\n    MLPClassifier()\n        ]\nzipped_clf = zip(names,classifiers)\ndef classifier_summary(pipeline, X_train, y_train, X_val, y_val,X_test,y_test):\n    sentiment_fit = pipeline.fit(X_train, y_train)\n    \n    y_pred_train= sentiment_fit.predict(X_train)\n    y_pred_val = sentiment_fit.predict(X_val)\n    y_pred_test = sentiment_fit.predict(X_test)\n    \n    train_accuracy = np.round(accuracy_score(y_train, y_pred_train),4)*100\n    train_precision = np.round(precision_score(y_train, y_pred_train, average='weighted'),4)\n    train_recall = np.round(recall_score(y_train, y_pred_train, average='weighted'),4)\n    train_F1 = np.round(f1_score(y_train, y_pred_train, average='weighted'),4)\n    train_kappa =  np.round(cohen_kappa_score(y_train, y_pred_train),4)\n    \n    \n    val_accuracy = np.round(accuracy_score(y_val, y_pred_val),4)*100\n    val_precision = np.round(precision_score(y_val, y_pred_val, average='weighted'),4)\n    val_recall = np.round(recall_score(y_val, y_pred_val, average='weighted'),4)\n    val_F1 = np.round(f1_score(y_val, y_pred_val, average='weighted'),4)\n    val_kappa =  np.round(cohen_kappa_score(y_val, y_pred_val),4)\n   \n    \n    test_accuracy = np.round(accuracy_score(y_test, y_pred_test),4)*100\n    test_precision = np.round(precision_score(y_test, y_pred_test, average='weighted'),2)\n    test_recall = np.round(recall_score(y_test, y_pred_test, average='weighted'),2)\n    test_F1 = np.round(f1_score(y_test, y_pred_test, average='weighted'),2)\n    test_kappa =  np.round(cohen_kappa_score(y_test, y_pred_test),2) \n  \n    \n    \n    print()\n    print('------------------------ Train Set Metrics------------------------')\n    print()\n    print(\"Accuracy core : {}%\".format(train_accuracy))\n    \n    print('------------------------ Validation Set Metrics------------------------')\n    print()\n    print(\"Accuracy score : {}%\".format(val_accuracy))\n    print('------------------------ Test Set Metrics------------------------')\n    print()\n    print(\"Accuracy score : {}%\".format(test_accuracy))\n    print(\"F1_score : {}\".format(test_F1))\n    print(\"Kappa Score : {} \".format(test_kappa))\n    print(\"Recall score: {}\".format(test_recall))\n    print(\"Precision score : {}\".format(test_precision))\n    \n    print(\"-\"*80)\n    print()\n    \ndef classifier_comparator(X_train,y_train,X_val,y_val,X_test,y_test,classifier=zipped_clf): \n    result = []\n    for n,c in classifier:\n        checker_pipeline = Pipeline([('Classifier', c)])\n        print(\"------------------------------Fitting {} on input_data-------------------------------- \".format(n))\n        #print(c)\n        classifier_summary(checker_pipeline,X_train, y_train, X_val, y_val,X_test,y_test)","13dc6680":"classifier_comparator(train_features,y_train,val_features,y_val,test_features,y_test,classifier=zipped_clf)","5cbd104d":"train_y=to_categorical(y_train,3)\nval_y=to_categorical(y_val,3)\ntest_y=to_categorical(y_test,3)\ndnn_model.compile(optimizer='sgd',loss='categorical_crossentropy', metrics=['accuracy'],)\nhistory = dnn_model.fit(train_features, train_y,validation_data=(val_features,val_y), epochs=10)\nloss_value , accuracy = dnn_model.evaluate(train_features, train_y)\nprint('Train_accuracy is:' + str(accuracy))\nloss_value , accuracy = dnn_model.evaluate(val_features, val_y)\nprint('Validation_accuracy is := ' + str(accuracy))\nloss_value , accuracy = dnn_model.evaluate(test_features, test_y)\nprint('test_accuracy is : = ' + str(accuracy))","60940ba7":"print(\"Performance Report:\")\ny_pred2=dnn_model.predict_classes(test_features)\ny_test2=[np.argmax(x) for x in test_y]\ny_pred_prb2=dnn_model.predict_proba(test_features)\ntarget=['0','1']\nfrom sklearn import metrics\nprint('Accuracy score is :', np.round(metrics.accuracy_score(y_test2, y_pred2),4))\nprint('Precision score is :', np.round(metrics.precision_score(y_test2, y_pred2, average='weighted'),4))\nprint('Recall score is :', np.round(metrics.recall_score(y_test2,y_pred2, average='weighted'),4))\nprint('F1 Score is :', np.round(metrics.f1_score(y_test2, y_pred2, average='weighted'),4))\nprint('Cohen Kappa Score:', np.round(metrics.cohen_kappa_score(y_test2, y_pred2),4))\nprint('\\t\\tClassification Report:\\n', metrics.classification_report(y_test2, y_pred2,target_names=target))","45af703a":"knn = KNeighborsClassifier(n_neighbors = 5, algorithm='ball_tree', leaf_size=30)\nknn.fit(train_features, y_train)\nplot_confusion_matrix(knn, test_features, y_test)","5b4ab9f3":"svc = SVC()\nsvc.fit(train_features, y_train)\nplot_confusion_matrix(svc, test_features, y_test)","67b10691":"rf = RandomForestClassifier()\nrf.fit(train_features, y_train)\nplot_confusion_matrix(rf, test_features, y_test)","91b92770":"ada = AdaBoostClassifier()\nada.fit(train_features, y_train)\nplot_confusion_matrix(ada, test_features, y_test)","0adc51d3":"xgbc = XGBClassifier()\nxgbc.fit(train_features, y_train)\nplot_confusion_matrix(xgbc, test_features, y_test)","44046a12":"mlpc = MLPClassifier()\nmlpc.fit(train_features, y_train)\nplot_confusion_matrix(mlpc, test_features, y_test)","41c5bc63":"base_model= VGG19(input_shape=(264,264,3), weights='imagenet', include_top=False)\nx = base_model.output\nx = Dropout(0.5)(x)\nx = Flatten()(x)\nx = BatchNormalization()(x)\nx = Dense(16,kernel_initializer='he_uniform')(x)\nx = BatchNormalization()(x)\nx = Activation('relu')(x)\nx = Dropout(0.5)(x)\nx = Dense(32,kernel_initializer='he_uniform')(x)\nx = BatchNormalization()(x)\nx = Activation('relu')(x)\nx = Dropout(0.5)(x)\nx = Dense(64,kernel_initializer='he_uniform')(x)\nx = BatchNormalization()(x)\nx = Activation('relu')(x)\nx = Dropout(0.5)(x)\nx = Dense(128,kernel_initializer='he_uniform')(x)\nx = BatchNormalization()(x)\nx = Activation('relu')(x)\nx = Dropout(0.5)(x)\nx = Dense(256,kernel_initializer='he_uniform')(x)\nx = BatchNormalization()(x)\nx = Activation('relu')(x)\nx = Dropout(0.5)(x)\npredictions = Dense(3, activation='softmax')(x)\n\nmodel_feat = Model(inputs=base_model.input,outputs=predictions)\n\ntrain_features = model_feat.predict(x_train)\nval_features=model_feat.predict(x_val)\ntest_features=model_feat.predict(x_test)","ecc315fa":"from sklearn.pipeline import make_pipeline\nfrom sklearn.pipeline import Pipeline\nnames = [\n        \"K Nearest Neighbour Classifier\",\n        'SVM',\n        \"Random Forest Classifier\",\n        \"AdaBoost Classifier\", \n        \"XGB Classifier\",\n        \"MLP Classifier\"\n         ]\nclassifiers = [\n    KNeighborsClassifier(n_neighbors = 5, algorithm='ball_tree', leaf_size=30),\n    SVC(),\n    RandomForestClassifier(max_depth=9,criterion = 'entropy'),\n    AdaBoostClassifier(),\n    XGBClassifier(),\n    MLPClassifier()\n        ]\nzipped_clf = zip(names,classifiers)\ndef classifier_summary(pipeline, X_train, y_train, X_val, y_val,X_test,y_test):\n    sentiment_fit = pipeline.fit(X_train, y_train)\n    \n    y_pred_train= sentiment_fit.predict(X_train)\n    y_pred_val = sentiment_fit.predict(X_val)\n    y_pred_test = sentiment_fit.predict(X_test)\n    \n    train_accuracy = np.round(accuracy_score(y_train, y_pred_train),4)*100\n    train_precision = np.round(precision_score(y_train, y_pred_train, average='weighted'),4)\n    train_recall = np.round(recall_score(y_train, y_pred_train, average='weighted'),4)\n    train_F1 = np.round(f1_score(y_train, y_pred_train, average='weighted'),4)\n    train_kappa =  np.round(cohen_kappa_score(y_train, y_pred_train),4)\n    \n    \n    val_accuracy = np.round(accuracy_score(y_val, y_pred_val),4)*100\n    val_precision = np.round(precision_score(y_val, y_pred_val, average='weighted'),4)\n    val_recall = np.round(recall_score(y_val, y_pred_val, average='weighted'),4)\n    val_F1 = np.round(f1_score(y_val, y_pred_val, average='weighted'),4)\n    val_kappa =  np.round(cohen_kappa_score(y_val, y_pred_val),4)\n   \n    \n    test_accuracy = np.round(accuracy_score(y_test, y_pred_test),4)*100\n    test_precision = np.round(precision_score(y_test, y_pred_test, average='weighted'),2)\n    test_recall = np.round(recall_score(y_test, y_pred_test, average='weighted'),2)\n    test_F1 = np.round(f1_score(y_test, y_pred_test, average='weighted'),2)\n    test_kappa =  np.round(cohen_kappa_score(y_test, y_pred_test),2) \n  \n    \n    \n    print()\n    print('------------------------ Train Set Metrics------------------------')\n    print()\n    print(\"Accuracy core : {}%\".format(train_accuracy))\n    \n    print('------------------------ Validation Set Metrics------------------------')\n    print()\n    print(\"Accuracy score : {}%\".format(val_accuracy))\n    print('------------------------ Test Set Metrics------------------------')\n    print()\n    print(\"Accuracy score : {}%\".format(test_accuracy))\n    print(\"F1_score : {}\".format(test_F1))\n    print(\"Kappa Score : {} \".format(test_kappa))\n    print(\"Recall score: {}\".format(test_recall))\n    print(\"Precision score : {}\".format(test_precision))\n    \n    print(\"-\"*80)\n    print()\n    \ndef classifier_comparator(X_train,y_train,X_val,y_val,X_test,y_test,classifier=zipped_clf): \n    result = []\n    for n,c in classifier:\n        checker_pipeline = Pipeline([('Classifier', c)])\n        print(\"------------------------------Fitting {} on input_data-------------------------------- \".format(n))\n        #print(c)\n        classifier_summary(checker_pipeline,X_train, y_train, X_val, y_val,X_test,y_test)","d4068104":"classifier_comparator(train_features,y_train,val_features,y_val,test_features,y_test,classifier=zipped_clf)","9e4df2fa":"train_y=to_categorical(y_train,3)\nval_y=to_categorical(y_val,3)\ntest_y=to_categorical(y_test,3)\ndnn_model.compile(optimizer='adam',loss='categorical_crossentropy', metrics=['accuracy'])\nhistory = dnn_model.fit(train_features, train_y,validation_data=(val_features,val_y), epochs=10)\nloss_value , accuracy = dnn_model.evaluate(train_features, train_y)\nprint('Train_accuracy is:' + str(accuracy))\nloss_value , accuracy = dnn_model.evaluate(val_features, val_y)\nprint('Validation_accuracy is := ' + str(accuracy))\nloss_value , accuracy = dnn_model.evaluate(test_features, test_y)\nprint('test_accuracy is : = ' + str(accuracy))","db7db616":"print(\"Performance Report:\")\ny_pred1=dnn_model.predict_classes(test_features)\ny_test1=[np.argmax(x) for x in test_y]\ny_pred_prb1=dnn_model.predict_proba(test_features)\ntarget=['0','1']\nfrom sklearn import metrics\nprint('Accuracy score is :', np.round(metrics.accuracy_score(y_test1, y_pred1),4))\nprint('Precision score is :', np.round(metrics.precision_score(y_test1, y_pred1, average='weighted'),4))\nprint('Recall score is :', np.round(metrics.recall_score(y_test1,y_pred1, average='weighted'),4))\nprint('F1 Score is :', np.round(metrics.f1_score(y_test1, y_pred1, average='weighted'),4))\nprint('Cohen Kappa Score:', np.round(metrics.cohen_kappa_score(y_test1, y_pred1),4))\nprint('\\t\\tClassification Report:\\n', metrics.classification_report(y_test1, y_pred1,target_names=target))","b3b0df94":"knn = KNeighborsClassifier(n_neighbors = 5, algorithm='ball_tree', leaf_size=30)\nknn.fit(train_features, y_train)\nplot_confusion_matrix(knn, test_features, y_test)","2a921139":"svc = SVC()\nsvc.fit(train_features, y_train)\nplot_confusion_matrix(svc, test_features, y_test)","9ee97d52":"rf = RandomForestClassifier()\nrf.fit(train_features, y_train)\nplot_confusion_matrix(rf, test_features, y_test)","7b60d269":"ada = AdaBoostClassifier()\nada.fit(train_features, y_train)\nplot_confusion_matrix(ada, test_features, y_test)","19b3da8c":"xgbc = XGBClassifier()\nxgbc.fit(train_features, y_train)\nplot_confusion_matrix(xgbc, test_features, y_test)","bc707be1":"mlpc = MLPClassifier()\nmlpc.fit(train_features, y_train)\nplot_confusion_matrix(mlpc, test_features, y_test)","cb9cf42c":"base_model= ResNet101(input_shape=(264,264,3), weights='imagenet', include_top=False)\nx = base_model.output\nx = Dropout(0.5)(x)\nx = Flatten()(x)\nx = BatchNormalization()(x)\nx = Dense(16,kernel_initializer='he_uniform')(x)\nx = BatchNormalization()(x)\nx = Activation('relu')(x)\nx = Dropout(0.5)(x)\nx = Dense(32,kernel_initializer='he_uniform')(x)\nx = BatchNormalization()(x)\nx = Activation('relu')(x)\nx = Dropout(0.5)(x)\nx = Dense(64,kernel_initializer='he_uniform')(x)\nx = BatchNormalization()(x)\nx = Activation('relu')(x)\nx = Dropout(0.5)(x)\nx = Dense(128,kernel_initializer='he_uniform')(x)\nx = BatchNormalization()(x)\nx = Activation('relu')(x)\nx = Dropout(0.5)(x)\nx = Dense(256,kernel_initializer='he_uniform')(x)\nx = BatchNormalization()(x)\nx = Activation('relu')(x)\nx = Dropout(0.5)(x)\npredictions = Dense(3, activation='softmax')(x)\n\nmodel_feat = Model(inputs=base_model.input,outputs=predictions)\n\ntrain_features = model_feat.predict(x_train)\nval_features=model_feat.predict(x_val)\ntest_features=model_feat.predict(x_test)","7fe37823":"from sklearn.pipeline import make_pipeline\nfrom sklearn.pipeline import Pipeline\nnames = [\n        \"K Nearest Neighbour Classifier\",\n        'SVM',\n        \"Random Forest Classifier\",\n        \"AdaBoost Classifier\", \n        \"XGB Classifier\",\n        \"MLP Classifier\"\n         ]\nclassifiers = [\n    KNeighborsClassifier(n_neighbors = 5, algorithm='ball_tree', leaf_size=30),\n    SVC(),\n    RandomForestClassifier(max_depth=9,criterion = 'entropy'),\n    AdaBoostClassifier(),\n    XGBClassifier(),\n    MLPClassifier()\n        ]\nzipped_clf = zip(names,classifiers)\ndef classifier_summary(pipeline, X_train, y_train, X_val, y_val,X_test,y_test):\n    sentiment_fit = pipeline.fit(X_train, y_train)\n    \n    y_pred_train= sentiment_fit.predict(X_train)\n    y_pred_val = sentiment_fit.predict(X_val)\n    y_pred_test = sentiment_fit.predict(X_test)\n    \n    train_accuracy = np.round(accuracy_score(y_train, y_pred_train),4)*100\n    train_precision = np.round(precision_score(y_train, y_pred_train, average='weighted'),4)\n    train_recall = np.round(recall_score(y_train, y_pred_train, average='weighted'),4)\n    train_F1 = np.round(f1_score(y_train, y_pred_train, average='weighted'),4)\n    train_kappa =  np.round(cohen_kappa_score(y_train, y_pred_train),4)\n    \n    \n    val_accuracy = np.round(accuracy_score(y_val, y_pred_val),4)*100\n    val_precision = np.round(precision_score(y_val, y_pred_val, average='weighted'),4)\n    val_recall = np.round(recall_score(y_val, y_pred_val, average='weighted'),4)\n    val_F1 = np.round(f1_score(y_val, y_pred_val, average='weighted'),4)\n    val_kappa =  np.round(cohen_kappa_score(y_val, y_pred_val),4)\n   \n    \n    test_accuracy = np.round(accuracy_score(y_test, y_pred_test),4)*100\n    test_precision = np.round(precision_score(y_test, y_pred_test, average='weighted'),2)\n    test_recall = np.round(recall_score(y_test, y_pred_test, average='weighted'),2)\n    test_F1 = np.round(f1_score(y_test, y_pred_test, average='weighted'),2)\n    test_kappa =  np.round(cohen_kappa_score(y_test, y_pred_test),2) \n  \n    \n    \n    print()\n    print('------------------------ Train Set Metrics------------------------')\n    print()\n    print(\"Accuracy core : {}%\".format(train_accuracy))\n    \n    print('------------------------ Validation Set Metrics------------------------')\n    print()\n    print(\"Accuracy score : {}%\".format(val_accuracy))\n    print('------------------------ Test Set Metrics------------------------')\n    print()\n    print(\"Accuracy score : {}%\".format(test_accuracy))\n    print(\"F1_score : {}\".format(test_F1))\n    print(\"Kappa Score : {} \".format(test_kappa))\n    print(\"Recall score: {}\".format(test_recall))\n    print(\"Precision score : {}\".format(test_precision))\n    \n    print(\"-\"*80)\n    print()\n    \ndef classifier_comparator(X_train,y_train,X_val,y_val,X_test,y_test,classifier=zipped_clf): \n    result = []\n    for n,c in classifier:\n        checker_pipeline = Pipeline([('Classifier', c)])\n        print(\"------------------------------Fitting {} on input_data-------------------------------- \".format(n))\n        #print(c)\n        classifier_summary(checker_pipeline,X_train, y_train, X_val, y_val,X_test,y_test)","35f1e31b":"classifier_comparator(train_features,y_train,val_features,y_val,test_features,y_test,classifier=zipped_clf)","6cfd7f30":"train_y=to_categorical(y_train,3)\nval_y=to_categorical(y_val,3)\ntest_y=to_categorical(y_test,3)\ndnn_model.compile(optimizer='adam',loss='categorical_crossentropy', metrics=['accuracy'])\nhistory = dnn_model.fit(train_features, train_y,validation_data=(val_features,val_y), epochs=10)\nloss_value , accuracy = dnn_model.evaluate(train_features, train_y)\nprint('Train_accuracy is:' + str(accuracy))\nloss_value , accuracy = dnn_model.evaluate(val_features, val_y)\nprint('Validation_accuracy is := ' + str(accuracy))\nloss_value , accuracy = dnn_model.evaluate(test_features, test_y)\nprint('test_accuracy is : = ' + str(accuracy))","16dda401":"print(\"Performance Report:\")\ny_pred3=dnn_model.predict_classes(test_features)\ny_test3=[np.argmax(x) for x in test_y]\ny_pred_prb3=dnn_model.predict_proba(test_features)\ntarget=['0','1']\nfrom sklearn import metrics\nprint('Accuracy score is :', np.round(metrics.accuracy_score(y_test3, y_pred3),4))\nprint('Precision score is :', np.round(metrics.precision_score(y_test3, y_pred3, average='weighted'),4))\nprint('Recall score is :', np.round(metrics.recall_score(y_test3,y_pred3, average='weighted'),4))\nprint('F1 Score is :', np.round(metrics.f1_score(y_test3, y_pred3, average='weighted'),4))\nprint('Cohen Kappa Score:', np.round(metrics.cohen_kappa_score(y_test3, y_pred3),4))\nprint('\\t\\tClassification Report:\\n', metrics.classification_report(y_test3, y_pred3,target_names=target))","3d870b0e":"knn = KNeighborsClassifier(n_neighbors = 5, algorithm='ball_tree', leaf_size=30)\nknn.fit(train_features, y_train)\nplot_confusion_matrix(knn, test_features, y_test)","7a09f90f":"svc = SVC()\nsvc.fit(train_features, y_train)\nplot_confusion_matrix(svc, test_features, y_test)","238bb8a6":"rf = RandomForestClassifier()\nrf.fit(train_features, y_train)\nplot_confusion_matrix(rf, test_features, y_test)","520da029":"ada = AdaBoostClassifier()\nada.fit(train_features, y_train)\nplot_confusion_matrix(ada, test_features, y_test)","58a3b61a":"xgbc = XGBClassifier()\nxgbc.fit(train_features, y_train)\nplot_confusion_matrix(xgbc, test_features, y_test)","64361417":"mlpc = MLPClassifier()\nmlpc.fit(train_features, y_train)\nplot_confusion_matrix(mlpc, test_features, y_test)","8fc2adba":"base_model= MobileNetV2(input_shape=(264,264,3), weights='imagenet', include_top=False)\nx = base_model.output\nx = Dropout(0.5)(x)\nx = Flatten()(x)\nx = BatchNormalization()(x)\nx = Dense(16,kernel_initializer='he_uniform')(x)\nx = BatchNormalization()(x)\nx = Activation('relu')(x)\nx = Dropout(0.5)(x)\nx = Dense(32,kernel_initializer='he_uniform')(x)\nx = BatchNormalization()(x)\nx = Activation('relu')(x)\nx = Dropout(0.5)(x)\nx = Dense(64,kernel_initializer='he_uniform')(x)\nx = BatchNormalization()(x)\nx = Activation('relu')(x)\nx = Dropout(0.5)(x)\nx = Dense(128,kernel_initializer='he_uniform')(x)\nx = BatchNormalization()(x)\nx = Activation('relu')(x)\nx = Dropout(0.5)(x)\nx = Dense(256,kernel_initializer='he_uniform')(x)\nx = BatchNormalization()(x)\nx = Activation('relu')(x)\nx = Dropout(0.5)(x)\npredictions = Dense(3, activation='softmax')(x)\n\nmodel_feat = Model(inputs=base_model.input,outputs=predictions)\n\ntrain_features = model_feat.predict(x_train)\nval_features=model_feat.predict(x_val)\ntest_features=model_feat.predict(x_test)","a9f104f1":"from sklearn.pipeline import make_pipeline\nfrom sklearn.pipeline import Pipeline\nnames = [\n        \"K Nearest Neighbour Classifier\",\n        'SVM',\n        \"Random Forest Classifier\",\n        \"AdaBoost Classifier\", \n        \"XGB Classifier\",\n        \"MLP Classifier\"\n         ]\nclassifiers = [\n    KNeighborsClassifier(n_neighbors = 5, algorithm='ball_tree', leaf_size=30),\n    SVC(),\n    RandomForestClassifier(max_depth=9,criterion = 'entropy'),\n    AdaBoostClassifier(),\n    XGBClassifier(),\n    MLPClassifier()\n        ]\nzipped_clf = zip(names,classifiers)\ndef classifier_summary(pipeline, X_train, y_train, X_val, y_val,X_test,y_test):\n    sentiment_fit = pipeline.fit(X_train, y_train)\n    \n    y_pred_train= sentiment_fit.predict(X_train)\n    y_pred_val = sentiment_fit.predict(X_val)\n    y_pred_test = sentiment_fit.predict(X_test)\n    \n    train_accuracy = np.round(accuracy_score(y_train, y_pred_train),4)*100\n    train_precision = np.round(precision_score(y_train, y_pred_train, average='weighted'),4)\n    train_recall = np.round(recall_score(y_train, y_pred_train, average='weighted'),4)\n    train_F1 = np.round(f1_score(y_train, y_pred_train, average='weighted'),4)\n    train_kappa =  np.round(cohen_kappa_score(y_train, y_pred_train),4)\n    \n    \n    val_accuracy = np.round(accuracy_score(y_val, y_pred_val),4)*100\n    val_precision = np.round(precision_score(y_val, y_pred_val, average='weighted'),4)\n    val_recall = np.round(recall_score(y_val, y_pred_val, average='weighted'),4)\n    val_F1 = np.round(f1_score(y_val, y_pred_val, average='weighted'),4)\n    val_kappa =  np.round(cohen_kappa_score(y_val, y_pred_val),4)\n   \n    \n    test_accuracy = np.round(accuracy_score(y_test, y_pred_test),4)*100\n    test_precision = np.round(precision_score(y_test, y_pred_test, average='weighted'),2)\n    test_recall = np.round(recall_score(y_test, y_pred_test, average='weighted'),2)\n    test_F1 = np.round(f1_score(y_test, y_pred_test, average='weighted'),2)\n    test_kappa =  np.round(cohen_kappa_score(y_test, y_pred_test),2) \n  \n    \n    \n    print()\n    print('------------------------ Train Set Metrics------------------------')\n    print()\n    print(\"Accuracy core : {}%\".format(train_accuracy))\n    \n    print('------------------------ Validation Set Metrics------------------------')\n    print()\n    print(\"Accuracy score : {}%\".format(val_accuracy))\n    print('------------------------ Test Set Metrics------------------------')\n    print()\n    print(\"Accuracy score : {}%\".format(test_accuracy))\n    print(\"F1_score : {}\".format(test_F1))\n    print(\"Kappa Score : {} \".format(test_kappa))\n    print(\"Recall score: {}\".format(test_recall))\n    print(\"Precision score : {}\".format(test_precision))\n    \n    print(\"-\"*80)\n    print()\n    \ndef classifier_comparator(X_train,y_train,X_val,y_val,X_test,y_test,classifier=zipped_clf): \n    result = []\n    for n,c in classifier:\n        checker_pipeline = Pipeline([('Classifier', c)])\n        print(\"------------------------------Fitting {} on input_data-------------------------------- \".format(n))\n        #print(c)\n        classifier_summary(checker_pipeline,X_train, y_train, X_val, y_val,X_test,y_test)","c51dc022":"classifier_comparator(train_features,y_train,val_features,y_val,test_features,y_test,classifier=zipped_clf)","763e4085":"train_y=to_categorical(y_train,3)\nval_y=to_categorical(y_val,3)\ntest_y=to_categorical(y_test,3)\ndnn_model.compile(optimizer='adam',loss='categorical_crossentropy', metrics=['accuracy'])\nhistory = dnn_model.fit(train_features, train_y,validation_data=(val_features,val_y), epochs=10)\nloss_value , accuracy = dnn_model.evaluate(train_features, train_y)\nprint('Train_accuracy is:' + str(accuracy))\nloss_value , accuracy = dnn_model.evaluate(val_features, val_y)\nprint('Validation_accuracy is := ' + str(accuracy))\nloss_value , accuracy = dnn_model.evaluate(test_features, test_y)\nprint('test_accuracy is : = ' + str(accuracy))","194719ac":"print(\"Performance Report:\")\ny_pred4=dnn_model.predict_classes(test_features)\ny_test4=[np.argmax(x) for x in test_y]\ny_pred_prb4=dnn_model.predict_proba(test_features)\ntarget=['0','1']\nfrom sklearn import metrics\nprint('Accuracy score is :', np.round(metrics.accuracy_score(y_test4, y_pred4),4))\nprint('Precision score is :', np.round(metrics.precision_score(y_test4, y_pred4, average='weighted'),4))\nprint('Recall score is :', np.round(metrics.recall_score(y_test4,y_pred4, average='weighted'),4))\nprint('F1 Score is :', np.round(metrics.f1_score(y_test4, y_pred4, average='weighted'),4))\nprint('Cohen Kappa Score:', np.round(metrics.cohen_kappa_score(y_test4, y_pred4),4))\nprint('\\t\\tClassification Report:\\n', metrics.classification_report(y_test4, y_pred4,target_names=target))","f405729a":"knn = KNeighborsClassifier(n_neighbors = 5, algorithm='ball_tree', leaf_size=30)\nknn.fit(train_features, y_train)\nplot_confusion_matrix(knn, test_features, y_test)","ab54a11e":"svc = SVC()\nsvc.fit(train_features, y_train)\nplot_confusion_matrix(svc, test_features, y_test)","c80c9e88":"rf = RandomForestClassifier()\nrf.fit(train_features, y_train)\nplot_confusion_matrix(rf, test_features, y_test)","cf8e2e7e":"ada = AdaBoostClassifier()\nada.fit(train_features, y_train)\nplot_confusion_matrix(ada, test_features, y_test)","88103811":"xgbc = XGBClassifier()\nxgbc.fit(train_features, y_train)\nplot_confusion_matrix(xgbc, test_features, y_test)","5df4f23d":"mlpc = MLPClassifier()\nmlpc.fit(train_features, y_train)\nplot_confusion_matrix(mlpc, test_features, y_test)","97231a9d":"base_model= MobileNet(input_shape=(264,264,3), weights='imagenet', include_top=False)\nx = base_model.output\nx = Dropout(0.5)(x)\nx = Flatten()(x)\nx = BatchNormalization()(x)\nx = Dense(16,kernel_initializer='he_uniform')(x)\nx = BatchNormalization()(x)\nx = Activation('relu')(x)\nx = Dropout(0.5)(x)\nx = Dense(32,kernel_initializer='he_uniform')(x)\nx = BatchNormalization()(x)\nx = Activation('relu')(x)\nx = Dropout(0.5)(x)\nx = Dense(64,kernel_initializer='he_uniform')(x)\nx = BatchNormalization()(x)\nx = Activation('relu')(x)\nx = Dropout(0.5)(x)\nx = Dense(128,kernel_initializer='he_uniform')(x)\nx = BatchNormalization()(x)\nx = Activation('relu')(x)\nx = Dropout(0.5)(x)\nx = Dense(256,kernel_initializer='he_uniform')(x)\nx = BatchNormalization()(x)\nx = Activation('relu')(x)\nx = Dropout(0.5)(x)\npredictions = Dense(3, activation='softmax')(x)\n\nmodel_feat = Model(inputs=base_model.input,outputs=predictions)\n\ntrain_features = model_feat.predict(x_train)\nval_features=model_feat.predict(x_val)\ntest_features=model_feat.predict(x_test)","d9c94c52":"from sklearn.pipeline import make_pipeline\nfrom sklearn.pipeline import Pipeline\nnames = [\n        \"K Nearest Neighbour Classifier\",\n        'SVM',\n        \"Random Forest Classifier\",\n        \"AdaBoost Classifier\", \n        \"XGB Classifier\",\n        \"MLP Classifier\"\n         ]\nclassifiers = [\n    KNeighborsClassifier(n_neighbors = 5, algorithm='ball_tree', leaf_size=30),\n    SVC(),\n    RandomForestClassifier(max_depth=9,criterion = 'entropy'),\n    AdaBoostClassifier(),\n    XGBClassifier(),\n    MLPClassifier()\n        ]\nzipped_clf = zip(names,classifiers)\ndef classifier_summary(pipeline, X_train, y_train, X_val, y_val,X_test,y_test):\n    sentiment_fit = pipeline.fit(X_train, y_train)\n    \n    y_pred_train= sentiment_fit.predict(X_train)\n    y_pred_val = sentiment_fit.predict(X_val)\n    y_pred_test = sentiment_fit.predict(X_test)\n    \n    train_accuracy = np.round(accuracy_score(y_train, y_pred_train),4)*100\n    train_precision = np.round(precision_score(y_train, y_pred_train, average='weighted'),4)\n    train_recall = np.round(recall_score(y_train, y_pred_train, average='weighted'),4)\n    train_F1 = np.round(f1_score(y_train, y_pred_train, average='weighted'),4)\n    train_kappa =  np.round(cohen_kappa_score(y_train, y_pred_train),4)\n    \n    \n    val_accuracy = np.round(accuracy_score(y_val, y_pred_val),4)*100\n    val_precision = np.round(precision_score(y_val, y_pred_val, average='weighted'),4)\n    val_recall = np.round(recall_score(y_val, y_pred_val, average='weighted'),4)\n    val_F1 = np.round(f1_score(y_val, y_pred_val, average='weighted'),4)\n    val_kappa =  np.round(cohen_kappa_score(y_val, y_pred_val),4)\n   \n    \n    test_accuracy = np.round(accuracy_score(y_test, y_pred_test),4)*100\n    test_precision = np.round(precision_score(y_test, y_pred_test, average='weighted'),2)\n    test_recall = np.round(recall_score(y_test, y_pred_test, average='weighted'),2)\n    test_F1 = np.round(f1_score(y_test, y_pred_test, average='weighted'),2)\n    test_kappa =  np.round(cohen_kappa_score(y_test, y_pred_test),2) \n  \n    \n    \n    print()\n    print('------------------------ Train Set Metrics------------------------')\n    print()\n    print(\"Accuracy core : {}%\".format(train_accuracy))\n    \n    print('------------------------ Validation Set Metrics------------------------')\n    print()\n    print(\"Accuracy score : {}%\".format(val_accuracy))\n    print('------------------------ Test Set Metrics------------------------')\n    print()\n    print(\"Accuracy score : {}%\".format(test_accuracy))\n    print(\"F1_score : {}\".format(test_F1))\n    print(\"Kappa Score : {} \".format(test_kappa))\n    print(\"Recall score: {}\".format(test_recall))\n    print(\"Precision score : {}\".format(test_precision))\n    \n    print(\"-\"*80)\n    print()\n    \ndef classifier_comparator(X_train,y_train,X_val,y_val,X_test,y_test,classifier=zipped_clf): \n    result = []\n    for n,c in classifier:\n        checker_pipeline = Pipeline([('Classifier', c)])\n        print(\"------------------------------Fitting {} on input_data-------------------------------- \".format(n))\n        #print(c)\n        classifier_summary(checker_pipeline,X_train, y_train, X_val, y_val,X_test,y_test)","a61b5075":"classifier_comparator(train_features,y_train,val_features,y_val,test_features,y_test,classifier=zipped_clf)","607e70af":"train_y=to_categorical(y_train,3)\nval_y=to_categorical(y_val,3)\ntest_y=to_categorical(y_test,3)\ndnn_model.compile(optimizer='adam',loss='categorical_crossentropy', metrics=['accuracy'])\nhistory = dnn_model.fit(train_features, train_y,validation_data=(val_features,val_y), epochs=10)\nloss_value , accuracy = dnn_model.evaluate(train_features, train_y)\nprint('Train_accuracy is:' + str(accuracy))\nloss_value , accuracy = dnn_model.evaluate(val_features, val_y)\nprint('Validation_accuracy is := ' + str(accuracy))\nloss_value , accuracy = dnn_model.evaluate(test_features, test_y)\nprint('test_accuracy is : = ' + str(accuracy))","69b4c2a9":"print(\"Performance Report:\")\ny_pred5=dnn_model.predict_classes(test_features)\ny_test5=[np.argmax(x) for x in test_y]\ny_pred_prb5=dnn_model.predict_proba(test_features)\ntarget=['0','1']\nfrom sklearn import metrics\nprint('Accuracy score is :', np.round(metrics.accuracy_score(y_test5, y_pred5),4))\nprint('Precision score is :', np.round(metrics.precision_score(y_test5, y_pred5, average='weighted'),4))\nprint('Recall score is :', np.round(metrics.recall_score(y_test5,y_pred5, average='weighted'),4))\nprint('F1 Score is :', np.round(metrics.f1_score(y_test5, y_pred5, average='weighted'),4))\nprint('Cohen Kappa Score:', np.round(metrics.cohen_kappa_score(y_test5, y_pred5),4))\nprint('\\t\\tClassification Report:\\n', metrics.classification_report(y_test5, y_pred5,target_names=target))","c7616318":"knn = KNeighborsClassifier(n_neighbors = 5, algorithm='ball_tree', leaf_size=30)\nknn.fit(train_features, y_train)\nplot_confusion_matrix(knn, test_features, y_test)","f07436ac":"svc = SVC()\nsvc.fit(train_features, y_train)\nplot_confusion_matrix(svc, test_features, y_test)","c2b51b52":"rf = RandomForestClassifier()\nrf.fit(train_features, y_train)\nplot_confusion_matrix(rf, test_features, y_test)","a41aa513":"ada = AdaBoostClassifier()\nada.fit(train_features, y_train)\nplot_confusion_matrix(ada, test_features, y_test)","6ab47b47":"xgbc = XGBClassifier()\nxgbc.fit(train_features, y_train)\nplot_confusion_matrix(xgbc, test_features, y_test)","e809b83c":"mlpc = MLPClassifier()\nmlpc.fit(train_features, y_train)\nplot_confusion_matrix(mlpc, test_features, y_test)","213f247c":"base_model= MobileNet(input_shape=(264,264,3), weights='imagenet', include_top=False)\nx = base_model.output\nx = Dropout(0.5)(x)\nx = Flatten()(x)\nx = BatchNormalization()(x)\nx = Dense(16,kernel_initializer='he_uniform')(x)\nx = BatchNormalization()(x)\nx = Activation('relu')(x)\nx = Dropout(0.5)(x)\nx = Dense(32,kernel_initializer='he_uniform')(x)\nx = BatchNormalization()(x)\nx = Activation('relu')(x)\nx = Dropout(0.5)(x)\nx = Dense(64,kernel_initializer='he_uniform')(x)\nx = BatchNormalization()(x)\nx = Activation('relu')(x)\nx = Dropout(0.5)(x)\nx = Dense(128,kernel_initializer='he_uniform')(x)\nx = BatchNormalization()(x)\nx = Activation('relu')(x)\nx = Dropout(0.5)(x)\nx = Dense(256,kernel_initializer='he_uniform')(x)\nx = BatchNormalization()(x)\nx = Activation('relu')(x)\nx = Dropout(0.5)(x)\npredictions = Dense(3, activation='softmax')(x)\n\nmodel_feat = Model(inputs=base_model.input,outputs=predictions)\n\ntrain_features = model_feat.predict(x_train)\nval_features=model_feat.predict(x_val)\ntest_features=model_feat.predict(x_test)","48a4e9a2":"from sklearn.pipeline import make_pipeline\nfrom sklearn.pipeline import Pipeline\nnames = [\n        \"K Nearest Neighbour Classifier\",\n        'SVM',\n        \"Random Forest Classifier\",\n        \"AdaBoost Classifier\", \n        \"XGB Classifier\",\n        \"MLP Classifier\"\n         ]\nclassifiers = [\n    KNeighborsClassifier(n_neighbors = 5, algorithm='ball_tree', leaf_size=30),\n    SVC(),\n    RandomForestClassifier(max_depth=9,criterion = 'entropy'),\n    AdaBoostClassifier(),\n    XGBClassifier(),\n    MLPClassifier()\n        ]\nzipped_clf = zip(names,classifiers)\ndef classifier_summary(pipeline, X_train, y_train, X_val, y_val,X_test,y_test):\n    sentiment_fit = pipeline.fit(X_train, y_train)\n    \n    y_pred_train= sentiment_fit.predict(X_train)\n    y_pred_val = sentiment_fit.predict(X_val)\n    y_pred_test = sentiment_fit.predict(X_test)\n    \n    train_accuracy = np.round(accuracy_score(y_train, y_pred_train),4)*100\n    train_precision = np.round(precision_score(y_train, y_pred_train, average='weighted'),4)\n    train_recall = np.round(recall_score(y_train, y_pred_train, average='weighted'),4)\n    train_F1 = np.round(f1_score(y_train, y_pred_train, average='weighted'),4)\n    train_kappa =  np.round(cohen_kappa_score(y_train, y_pred_train),4)\n    \n    \n    val_accuracy = np.round(accuracy_score(y_val, y_pred_val),4)*100\n    val_precision = np.round(precision_score(y_val, y_pred_val, average='weighted'),4)\n    val_recall = np.round(recall_score(y_val, y_pred_val, average='weighted'),4)\n    val_F1 = np.round(f1_score(y_val, y_pred_val, average='weighted'),4)\n    val_kappa =  np.round(cohen_kappa_score(y_val, y_pred_val),4)\n   \n    \n    test_accuracy = np.round(accuracy_score(y_test, y_pred_test),4)*100\n    test_precision = np.round(precision_score(y_test, y_pred_test, average='weighted'),2)\n    test_recall = np.round(recall_score(y_test, y_pred_test, average='weighted'),2)\n    test_F1 = np.round(f1_score(y_test, y_pred_test, average='weighted'),2)\n    test_kappa =  np.round(cohen_kappa_score(y_test, y_pred_test),2) \n  \n    \n    \n    print()\n    print('------------------------ Train Set Metrics------------------------')\n    print()\n    print(\"Accuracy core : {}%\".format(train_accuracy))\n    \n    print('------------------------ Validation Set Metrics------------------------')\n    print()\n    print(\"Accuracy score : {}%\".format(val_accuracy))\n    print('------------------------ Test Set Metrics------------------------')\n    print()\n    print(\"Accuracy score : {}%\".format(test_accuracy))\n    print(\"F1_score : {}\".format(test_F1))\n    print(\"Kappa Score : {} \".format(test_kappa))\n    print(\"Recall score: {}\".format(test_recall))\n    print(\"Precision score : {}\".format(test_precision))\n    \n    print(\"-\"*80)\n    print()\n    \ndef classifier_comparator(X_train,y_train,X_val,y_val,X_test,y_test,classifier=zipped_clf): \n    result = []\n    for n,c in classifier:\n        checker_pipeline = Pipeline([('Classifier', c)])\n        print(\"------------------------------Fitting {} on input_data-------------------------------- \".format(n))\n        #print(c)\n        classifier_summary(checker_pipeline,X_train, y_train, X_val, y_val,X_test,y_test)","3cc3251d":"classifier_comparator(train_features,y_train,val_features,y_val,test_features,y_test,classifier=zipped_clf)","7c8e9978":"train_y=to_categorical(y_train,3)\nval_y=to_categorical(y_val,3)\ntest_y=to_categorical(y_test,3)\ndnn_model.compile(optimizer='adam',loss='categorical_crossentropy', metrics=['accuracy'])\nhistory = dnn_model.fit(train_features, train_y,validation_data=(val_features,val_y), epochs=10)\nloss_value , accuracy = dnn_model.evaluate(train_features, train_y)\nprint('Train_accuracy is:' + str(accuracy))\nloss_value , accuracy = dnn_model.evaluate(val_features, val_y)\nprint('Validation_accuracy is := ' + str(accuracy))\nloss_value , accuracy = dnn_model.evaluate(test_features, test_y)\nprint('test_accuracy is : = ' + str(accuracy))","43173acf":"print(\"Performance Report:\")\ny_pred6=dnn_model.predict_classes(test_features)\ny_test6=[np.argmax(x) for x in test_y]\ny_pred_prb6=dnn_model.predict_proba(test_features)\ntarget=['0','1']\nfrom sklearn import metrics\nprint('Accuracy score is :', np.round(metrics.accuracy_score(y_test6, y_pred6),4))\nprint('Precision score is :', np.round(metrics.precision_score(y_test6, y_pred6, average='weighted'),4))\nprint('Recall score is :', np.round(metrics.recall_score(y_test6,y_pred6, average='weighted'),4))\nprint('F1 Score is :', np.round(metrics.f1_score(y_test6, y_pred6, average='weighted'),4))\nprint('Cohen Kappa Score:', np.round(metrics.cohen_kappa_score(y_test6, y_pred6),4))\nprint('\\t\\tClassification Report:\\n', metrics.classification_report(y_test6, y_pred6,target_names=target))","9dd84d58":"knn = KNeighborsClassifier(n_neighbors = 5, algorithm='ball_tree', leaf_size=30)\nknn.fit(train_features, y_train)\nplot_confusion_matrix(knn, test_features, y_test)","af857af9":"svc = SVC()\nsvc.fit(train_features, y_train)\nplot_confusion_matrix(svc, test_features, y_test)","2aa64b81":"rf = RandomForestClassifier()\nrf.fit(train_features, y_train)\nplot_confusion_matrix(rf, test_features, y_test)","86a1def1":"ada = AdaBoostClassifier()\nada.fit(train_features, y_train)\nplot_confusion_matrix(ada, test_features, y_test)","4053d763":"xgbc = XGBClassifier()\nxgbc.fit(train_features, y_train)\nplot_confusion_matrix(xgbc, test_features, y_test)","bcb41277":"mlpc = MLPClassifier()\nmlpc.fit(train_features, y_train)\nplot_confusion_matrix(mlpc, test_features, y_test)","24a9a705":"base_model= InceptionResNetV2(input_shape=(264,264,3), weights='imagenet', include_top=False)\nx = base_model.output\nx = Dropout(0.5)(x)\nx = Flatten()(x)\nx = BatchNormalization()(x)\nx = Dense(16,kernel_initializer='he_uniform')(x)\nx = BatchNormalization()(x)\nx = Activation('relu')(x)\nx = Dropout(0.5)(x)\nx = Dense(32,kernel_initializer='he_uniform')(x)\nx = BatchNormalization()(x)\nx = Activation('relu')(x)\nx = Dropout(0.5)(x)\nx = Dense(64,kernel_initializer='he_uniform')(x)\nx = BatchNormalization()(x)\nx = Activation('relu')(x)\nx = Dropout(0.5)(x)\nx = Dense(128,kernel_initializer='he_uniform')(x)\nx = BatchNormalization()(x)\nx = Activation('relu')(x)\nx = Dropout(0.5)(x)\nx = Dense(256,kernel_initializer='he_uniform')(x)\nx = BatchNormalization()(x)\nx = Activation('relu')(x)\nx = Dropout(0.5)(x)\npredictions = Dense(3, activation='softmax')(x)\n\nmodel_feat = Model(inputs=base_model.input,outputs=predictions)\n\ntrain_features = model_feat.predict(x_train)\nval_features=model_feat.predict(x_val)\ntest_features=model_feat.predict(x_test)","15c5263f":"from sklearn.pipeline import make_pipeline\nfrom sklearn.pipeline import Pipeline\nnames = [\n        \"K Nearest Neighbour Classifier\",\n        'SVM',\n        \"Random Forest Classifier\",\n        \"AdaBoost Classifier\", \n        \"XGB Classifier\",\n        \"MLP Classifier\"\n         ]\nclassifiers = [\n    KNeighborsClassifier(n_neighbors = 5, algorithm='ball_tree', leaf_size=30),\n    SVC(),\n    RandomForestClassifier(max_depth=9,criterion = 'entropy'),\n    AdaBoostClassifier(),\n    XGBClassifier(),\n    MLPClassifier()\n        ]\nzipped_clf = zip(names,classifiers)\ndef classifier_summary(pipeline, X_train, y_train, X_val, y_val,X_test,y_test):\n    sentiment_fit = pipeline.fit(X_train, y_train)\n    \n    y_pred_train= sentiment_fit.predict(X_train)\n    y_pred_val = sentiment_fit.predict(X_val)\n    y_pred_test = sentiment_fit.predict(X_test)\n    \n    train_accuracy = np.round(accuracy_score(y_train, y_pred_train),4)*100\n    train_precision = np.round(precision_score(y_train, y_pred_train, average='weighted'),4)\n    train_recall = np.round(recall_score(y_train, y_pred_train, average='weighted'),4)\n    train_F1 = np.round(f1_score(y_train, y_pred_train, average='weighted'),4)\n    train_kappa =  np.round(cohen_kappa_score(y_train, y_pred_train),4)\n    \n    \n    val_accuracy = np.round(accuracy_score(y_val, y_pred_val),4)*100\n    val_precision = np.round(precision_score(y_val, y_pred_val, average='weighted'),4)\n    val_recall = np.round(recall_score(y_val, y_pred_val, average='weighted'),4)\n    val_F1 = np.round(f1_score(y_val, y_pred_val, average='weighted'),4)\n    val_kappa =  np.round(cohen_kappa_score(y_val, y_pred_val),4)\n   \n    \n    test_accuracy = np.round(accuracy_score(y_test, y_pred_test),4)*100\n    test_precision = np.round(precision_score(y_test, y_pred_test, average='weighted'),2)\n    test_recall = np.round(recall_score(y_test, y_pred_test, average='weighted'),2)\n    test_F1 = np.round(f1_score(y_test, y_pred_test, average='weighted'),2)\n    test_kappa =  np.round(cohen_kappa_score(y_test, y_pred_test),2) \n  \n    \n    \n    print()\n    print('------------------------ Train Set Metrics------------------------')\n    print()\n    print(\"Accuracy core : {}%\".format(train_accuracy))\n    \n    print('------------------------ Validation Set Metrics------------------------')\n    print()\n    print(\"Accuracy score : {}%\".format(val_accuracy))\n    print('------------------------ Test Set Metrics------------------------')\n    print()\n    print(\"Accuracy score : {}%\".format(test_accuracy))\n    print(\"F1_score : {}\".format(test_F1))\n    print(\"Kappa Score : {} \".format(test_kappa))\n    print(\"Recall score: {}\".format(test_recall))\n    print(\"Precision score : {}\".format(test_precision))\n    \n    print(\"-\"*80)\n    print()\n    \ndef classifier_comparator(X_train,y_train,X_val,y_val,X_test,y_test,classifier=zipped_clf): \n    result = []\n    for n,c in classifier:\n        checker_pipeline = Pipeline([('Classifier', c)])\n        print(\"------------------------------Fitting {} on input_data-------------------------------- \".format(n))\n        #print(c)\n        classifier_summary(checker_pipeline,X_train, y_train, X_val, y_val,X_test,y_test)","512ba326":"classifier_comparator(train_features,y_train,val_features,y_val,test_features,y_test,classifier=zipped_clf)","71ef19fd":"train_y=to_categorical(y_train,3)\nval_y=to_categorical(y_val,3)\ntest_y=to_categorical(y_test,3)\ndnn_model.compile(optimizer='adam',loss='categorical_crossentropy', metrics=['accuracy'])\nhistory = dnn_model.fit(train_features, train_y,validation_data=(val_features,val_y), epochs=10)\nloss_value , accuracy = dnn_model.evaluate(train_features, train_y)\nprint('Train_accuracy is:' + str(accuracy))\nloss_value , accuracy = dnn_model.evaluate(val_features, val_y)\nprint('Validation_accuracy is := ' + str(accuracy))\nloss_value , accuracy = dnn_model.evaluate(test_features, test_y)\nprint('test_accuracy is : = ' + str(accuracy))","4755f8df":"print(\"Performance Report:\")\ny_pred7=dnn_model.predict_classes(test_features)\ny_test7=[np.argmax(x) for x in test_y]\ny_pred_prb7=dnn_model.predict_proba(test_features)\ntarget=['0','1']\nfrom sklearn import metrics\nprint('Accuracy score is :', np.round(metrics.accuracy_score(y_test7, y_pred7),4))\nprint('Precision score is :', np.round(metrics.precision_score(y_test7, y_pred7, average='weighted'),4))\nprint('Recall score is :', np.round(metrics.recall_score(y_test7,y_pred7, average='weighted'),4))\nprint('F1 Score is :', np.round(metrics.f1_score(y_test7, y_pred7, average='weighted'),4))\nprint('Cohen Kappa Score:', np.round(metrics.cohen_kappa_score(y_test7, y_pred7),4))\nprint('\\t\\tClassification Report:\\n', metrics.classification_report(y_test7, y_pred7,target_names=target))","79fe634e":"knn = KNeighborsClassifier(n_neighbors = 5, algorithm='ball_tree', leaf_size=30)\nknn.fit(train_features, y_train)\nplot_confusion_matrix(knn, test_features, y_test)","97eaddba":"svc = SVC()\nsvc.fit(train_features, y_train)\nplot_confusion_matrix(svc, test_features, y_test)","b38b5456":"rf = RandomForestClassifier()\nrf.fit(train_features, y_train)\nplot_confusion_matrix(rf, test_features, y_test)","bfe1b58b":"ada = AdaBoostClassifier()\nada.fit(train_features, y_train)\nplot_confusion_matrix(ada, test_features, y_test)","27709c9d":"xgbc = XGBClassifier()\nxgbc.fit(train_features, y_train)\nplot_confusion_matrix(xgbc, test_features, y_test)","55aa25ca":"mlpc = MLPClassifier()\nmlpc.fit(train_features, y_train)\nplot_confusion_matrix(mlpc, test_features, y_test)","5f826f62":"base_model= InceptionResNetV2(input_shape=(264,264,3), weights='imagenet', include_top=False)\nx = base_model.output\nx = Dropout(0.5)(x)\nx = Flatten()(x)\nx = BatchNormalization()(x)\nx = Dense(16,kernel_initializer='he_uniform')(x)\nx = BatchNormalization()(x)\nx = Activation('relu')(x)\nx = Dropout(0.5)(x)\nx = Dense(32,kernel_initializer='he_uniform')(x)\nx = BatchNormalization()(x)\nx = Activation('relu')(x)\nx = Dropout(0.5)(x)\nx = Dense(64,kernel_initializer='he_uniform')(x)\nx = BatchNormalization()(x)\nx = Activation('relu')(x)\nx = Dropout(0.5)(x)\nx = Dense(128,kernel_initializer='he_uniform')(x)\nx = BatchNormalization()(x)\nx = Activation('relu')(x)\nx = Dropout(0.5)(x)\nx = Dense(256,kernel_initializer='he_uniform')(x)\nx = BatchNormalization()(x)\nx = Activation('relu')(x)\nx = Dropout(0.5)(x)\npredictions = Dense(3, activation='softmax')(x)\n\nmodel_feat = Model(inputs=base_model.input,outputs=predictions)\n\ntrain_features = model_feat.predict(x_train)\nval_features=model_feat.predict(x_val)\ntest_features=model_feat.predict(x_test)","88667cc9":"from sklearn.pipeline import make_pipeline\nfrom sklearn.pipeline import Pipeline\nnames = [\n        \"K Nearest Neighbour Classifier\",\n        'SVM',\n        \"Random Forest Classifier\",\n        \"AdaBoost Classifier\", \n        \"XGB Classifier\",\n        \"MLP Classifier\"\n         ]\nclassifiers = [\n    KNeighborsClassifier(n_neighbors = 5, algorithm='ball_tree', leaf_size=30),\n    SVC(),\n    RandomForestClassifier(max_depth=9,criterion = 'entropy'),\n    AdaBoostClassifier(),\n    XGBClassifier(),\n    MLPClassifier()\n        ]\nzipped_clf = zip(names,classifiers)\ndef classifier_summary(pipeline, X_train, y_train, X_val, y_val,X_test,y_test):\n    sentiment_fit = pipeline.fit(X_train, y_train)\n    \n    y_pred_train= sentiment_fit.predict(X_train)\n    y_pred_val = sentiment_fit.predict(X_val)\n    y_pred_test = sentiment_fit.predict(X_test)\n    \n    train_accuracy = np.round(accuracy_score(y_train, y_pred_train),4)*100\n    train_precision = np.round(precision_score(y_train, y_pred_train, average='weighted'),4)\n    train_recall = np.round(recall_score(y_train, y_pred_train, average='weighted'),4)\n    train_F1 = np.round(f1_score(y_train, y_pred_train, average='weighted'),4)\n    train_kappa =  np.round(cohen_kappa_score(y_train, y_pred_train),4)\n    \n    \n    val_accuracy = np.round(accuracy_score(y_val, y_pred_val),4)*100\n    val_precision = np.round(precision_score(y_val, y_pred_val, average='weighted'),4)\n    val_recall = np.round(recall_score(y_val, y_pred_val, average='weighted'),4)\n    val_F1 = np.round(f1_score(y_val, y_pred_val, average='weighted'),4)\n    val_kappa =  np.round(cohen_kappa_score(y_val, y_pred_val),4)\n   \n    \n    test_accuracy = np.round(accuracy_score(y_test, y_pred_test),4)*100\n    test_precision = np.round(precision_score(y_test, y_pred_test, average='weighted'),2)\n    test_recall = np.round(recall_score(y_test, y_pred_test, average='weighted'),2)\n    test_F1 = np.round(f1_score(y_test, y_pred_test, average='weighted'),2)\n    test_kappa =  np.round(cohen_kappa_score(y_test, y_pred_test),2) \n  \n    \n    \n    print()\n    print('------------------------ Train Set Metrics------------------------')\n    print()\n    print(\"Accuracy core : {}%\".format(train_accuracy))\n    \n    print('------------------------ Validation Set Metrics------------------------')\n    print()\n    print(\"Accuracy score : {}%\".format(val_accuracy))\n    print('------------------------ Test Set Metrics------------------------')\n    print()\n    print(\"Accuracy score : {}%\".format(test_accuracy))\n    print(\"F1_score : {}\".format(test_F1))\n    print(\"Kappa Score : {} \".format(test_kappa))\n    print(\"Recall score: {}\".format(test_recall))\n    print(\"Precision score : {}\".format(test_precision))\n    \n    print(\"-\"*80)\n    print()\n    \ndef classifier_comparator(X_train,y_train,X_val,y_val,X_test,y_test,classifier=zipped_clf): \n    result = []\n    for n,c in classifier:\n        checker_pipeline = Pipeline([('Classifier', c)])\n        print(\"------------------------------Fitting {} on input_data-------------------------------- \".format(n))\n        #print(c)\n        classifier_summary(checker_pipeline,X_train, y_train, X_val, y_val,X_test,y_test)","4e15fcf9":"classifier_comparator(train_features,y_train,val_features,y_val,test_features,y_test,classifier=zipped_clf)","9c769c65":"train_y=to_categorical(y_train,3)\nval_y=to_categorical(y_val,3)\ntest_y=to_categorical(y_test,3)\ndnn_model.compile(optimizer='adam',loss='categorical_crossentropy', metrics=['accuracy'])\nhistory = dnn_model.fit(train_features, train_y,validation_data=(val_features,val_y), epochs=10)\nloss_value , accuracy = dnn_model.evaluate(train_features, train_y)\nprint('Train_accuracy is:' + str(accuracy))\nloss_value , accuracy = dnn_model.evaluate(val_features, val_y)\nprint('Validation_accuracy is := ' + str(accuracy))\nloss_value , accuracy = dnn_model.evaluate(test_features, test_y)\nprint('test_accuracy is : = ' + str(accuracy))","a586e91e":"print(\"Performance Report:\")\ny_pred8=dnn_model.predict_classes(test_features)\ny_test8=[np.argmax(x) for x in test_y]\ny_pred_prb8=dnn_model.predict_proba(test_features)\ntarget=['0','1']\nfrom sklearn import metrics\nprint('Accuracy score is :', np.round(metrics.accuracy_score(y_test8, y_pred8),4))\nprint('Precision score is :', np.round(metrics.precision_score(y_test8, y_pred8, average='weighted'),4))\nprint('Recall score is :', np.round(metrics.recall_score(y_test8,y_pred8, average='weighted'),4))\nprint('F1 Score is :', np.round(metrics.f1_score(y_test8, y_pred8, average='weighted'),4))\nprint('Cohen Kappa Score:', np.round(metrics.cohen_kappa_score(y_test8, y_pred8),4))\nprint('\\t\\tClassification Report:\\n', metrics.classification_report(y_test8, y_pred8,target_names=target))","54ab3fe7":"knn = KNeighborsClassifier(n_neighbors = 5, algorithm='ball_tree', leaf_size=30)\nknn.fit(train_features, y_train)\nplot_confusion_matrix(knn, test_features, y_test)","dca884d6":"svc = SVC()\nsvc.fit(train_features, y_train)\nplot_confusion_matrix(svc, test_features, y_test)","ff3d1847":"rf = RandomForestClassifier()\nrf.fit(train_features, y_train)\nplot_confusion_matrix(rf, test_features, y_test)","2645a27f":"ada = AdaBoostClassifier()\nada.fit(train_features, y_train)\nplot_confusion_matrix(ada, test_features, y_test)","a0ce936b":"xgbc = XGBClassifier()\nxgbc.fit(train_features, y_train)\nplot_confusion_matrix(xgbc, test_features, y_test)","d2226eb3":"mlpc = MLPClassifier()\nmlpc.fit(train_features, y_train)\nplot_confusion_matrix(mlpc, test_features, y_test)","042fd2b0":"base_model= InceptionResNetV2(input_shape=(264,264,3), weights='imagenet', include_top=False)\nx = base_model.output\nx = Dropout(0.5)(x)\nx = Flatten()(x)\nx = BatchNormalization()(x)\nx = Dense(16,kernel_initializer='he_uniform')(x)\nx = BatchNormalization()(x)\nx = Activation('relu')(x)\nx = Dropout(0.5)(x)\nx = Dense(32,kernel_initializer='he_uniform')(x)\nx = BatchNormalization()(x)\nx = Activation('relu')(x)\nx = Dropout(0.5)(x)\nx = Dense(64,kernel_initializer='he_uniform')(x)\nx = BatchNormalization()(x)\nx = Activation('relu')(x)\nx = Dropout(0.5)(x)\nx = Dense(128,kernel_initializer='he_uniform')(x)\nx = BatchNormalization()(x)\nx = Activation('relu')(x)\nx = Dropout(0.5)(x)\nx = Dense(256,kernel_initializer='he_uniform')(x)\nx = BatchNormalization()(x)\nx = Activation('relu')(x)\nx = Dropout(0.5)(x)\npredictions = Dense(3, activation='softmax')(x)\n\nmodel_feat = Model(inputs=base_model.input,outputs=predictions)\n\ntrain_features = model_feat.predict(x_train)\nval_features=model_feat.predict(x_val)\ntest_features=model_feat.predict(x_test)","f9b48bbd":"from sklearn.pipeline import make_pipeline\nfrom sklearn.pipeline import Pipeline\nnames = [\n        \"K Nearest Neighbour Classifier\",\n        'SVM',\n        \"Random Forest Classifier\",\n        \"AdaBoost Classifier\", \n        \"XGB Classifier\",\n        \"MLP Classifier\"\n         ]\nclassifiers = [\n    KNeighborsClassifier(n_neighbors = 5, algorithm='ball_tree', leaf_size=30),\n    SVC(),\n    RandomForestClassifier(max_depth=9,criterion = 'entropy'),\n    AdaBoostClassifier(),\n    XGBClassifier(),\n    MLPClassifier()\n        ]\nzipped_clf = zip(names,classifiers)\ndef classifier_summary(pipeline, X_train, y_train, X_val, y_val,X_test,y_test):\n    sentiment_fit = pipeline.fit(X_train, y_train)\n    \n    y_pred_train= sentiment_fit.predict(X_train)\n    y_pred_val = sentiment_fit.predict(X_val)\n    y_pred_test = sentiment_fit.predict(X_test)\n    \n    train_accuracy = np.round(accuracy_score(y_train, y_pred_train),4)*100\n    train_precision = np.round(precision_score(y_train, y_pred_train, average='weighted'),4)\n    train_recall = np.round(recall_score(y_train, y_pred_train, average='weighted'),4)\n    train_F1 = np.round(f1_score(y_train, y_pred_train, average='weighted'),4)\n    train_kappa =  np.round(cohen_kappa_score(y_train, y_pred_train),4)\n    \n    \n    val_accuracy = np.round(accuracy_score(y_val, y_pred_val),4)*100\n    val_precision = np.round(precision_score(y_val, y_pred_val, average='weighted'),4)\n    val_recall = np.round(recall_score(y_val, y_pred_val, average='weighted'),4)\n    val_F1 = np.round(f1_score(y_val, y_pred_val, average='weighted'),4)\n    val_kappa =  np.round(cohen_kappa_score(y_val, y_pred_val),4)\n   \n    \n    test_accuracy = np.round(accuracy_score(y_test, y_pred_test),4)*100\n    test_precision = np.round(precision_score(y_test, y_pred_test, average='weighted'),2)\n    test_recall = np.round(recall_score(y_test, y_pred_test, average='weighted'),2)\n    test_F1 = np.round(f1_score(y_test, y_pred_test, average='weighted'),2)\n    test_kappa =  np.round(cohen_kappa_score(y_test, y_pred_test),2) \n  \n    \n    \n    print()\n    print('------------------------ Train Set Metrics------------------------')\n    print()\n    print(\"Accuracy core : {}%\".format(train_accuracy))\n    \n    print('------------------------ Validation Set Metrics------------------------')\n    print()\n    print(\"Accuracy score : {}%\".format(val_accuracy))\n    print('------------------------ Test Set Metrics------------------------')\n    print()\n    print(\"Accuracy score : {}%\".format(test_accuracy))\n    print(\"F1_score : {}\".format(test_F1))\n    print(\"Kappa Score : {} \".format(test_kappa))\n    print(\"Recall score: {}\".format(test_recall))\n    print(\"Precision score : {}\".format(test_precision))\n    \n    print(\"-\"*80)\n    print()\n    \ndef classifier_comparator(X_train,y_train,X_val,y_val,X_test,y_test,classifier=zipped_clf): \n    result = []\n    for n,c in classifier:\n        checker_pipeline = Pipeline([('Classifier', c)])\n        print(\"------------------------------Fitting {} on input_data-------------------------------- \".format(n))\n        #print(c)\n        classifier_summary(checker_pipeline,X_train, y_train, X_val, y_val,X_test,y_test)","3c937bce":"classifier_comparator(train_features,y_train,val_features,y_val,test_features,y_test,classifier=zipped_clf)","e7597d6c":"train_y=to_categorical(y_train,3)\nval_y=to_categorical(y_val,3)\ntest_y=to_categorical(y_test,3)\ndnn_model.compile(optimizer='adam',loss='categorical_crossentropy', metrics=['accuracy'])\nhistory = dnn_model.fit(train_features, train_y,validation_data=(val_features,val_y), epochs=10)\nloss_value , accuracy = dnn_model.evaluate(train_features, train_y)\nprint('Train_accuracy is:' + str(accuracy))\nloss_value , accuracy = dnn_model.evaluate(val_features, val_y)\nprint('Validation_accuracy is := ' + str(accuracy))\nloss_value , accuracy = dnn_model.evaluate(test_features, test_y)\nprint('test_accuracy is : = ' + str(accuracy))","ebc3739f":"print(\"Performance Report:\")\ny_pred9=dnn_model.predict_classes(test_features)\ny_test9=[np.argmax(x) for x in test_y]\ny_pred_prb9=dnn_model.predict_proba(test_features)\ntarget=['0','1']\nfrom sklearn import metrics\nprint('Accuracy score is :', np.round(metrics.accuracy_score(y_test9, y_pred9),4))\nprint('Precision score is :', np.round(metrics.precision_score(y_test9, y_pred9, average='weighted'),4))\nprint('Recall score is :', np.round(metrics.recall_score(y_test9,y_pred9, average='weighted'),4))\nprint('F1 Score is :', np.round(metrics.f1_score(y_test9, y_pred9, average='weighted'),4))\nprint('Cohen Kappa Score:', np.round(metrics.cohen_kappa_score(y_test9, y_pred9),4))\nprint('\\t\\tClassification Report:\\n', metrics.classification_report(y_test9, y_pred9,target_names=target))","c0f21275":"knn = KNeighborsClassifier(n_neighbors = 5, algorithm='ball_tree', leaf_size=30)\nknn.fit(train_features, y_train)\nplot_confusion_matrix(knn, test_features, y_test)","52ac13ad":"svc = SVC()\nsvc.fit(train_features, y_train)\nplot_confusion_matrix(svc, test_features, y_test)","393dbb8f":"rf = RandomForestClassifier()\nrf.fit(train_features, y_train)\nplot_confusion_matrix(rf, test_features, y_test)","290935d3":"ada = AdaBoostClassifier()\nada.fit(train_features, y_train)\nplot_confusion_matrix(ada, test_features, y_test)","e9a25f1e":"xgbc = XGBClassifier()\nxgbc.fit(train_features, y_train)\nplot_confusion_matrix(xgbc, test_features, y_test)","d4586fe3":"mlpc = MLPClassifier()\nmlpc.fit(train_features, y_train)\nplot_confusion_matrix(mlpc, test_features, y_test)","0e5a409d":"base_model= InceptionResNetV2(input_shape=(264,264,3), weights='imagenet', include_top=False)\nx = base_model.output\nx = Dropout(0.5)(x)\nx = Flatten()(x)\nx = BatchNormalization()(x)\nx = Dense(16,kernel_initializer='he_uniform')(x)\nx = BatchNormalization()(x)\nx = Activation('relu')(x)\nx = Dropout(0.5)(x)\nx = Dense(32,kernel_initializer='he_uniform')(x)\nx = BatchNormalization()(x)\nx = Activation('relu')(x)\nx = Dropout(0.5)(x)\nx = Dense(64,kernel_initializer='he_uniform')(x)\nx = BatchNormalization()(x)\nx = Activation('relu')(x)\nx = Dropout(0.5)(x)\nx = Dense(128,kernel_initializer='he_uniform')(x)\nx = BatchNormalization()(x)\nx = Activation('relu')(x)\nx = Dropout(0.5)(x)\nx = Dense(256,kernel_initializer='he_uniform')(x)\nx = BatchNormalization()(x)\nx = Activation('relu')(x)\nx = Dropout(0.5)(x)\npredictions = Dense(3, activation='softmax')(x)\n\nmodel_feat = Model(inputs=base_model.input,outputs=predictions)\n\ntrain_features = model_feat.predict(x_train)\nval_features=model_feat.predict(x_val)\ntest_features=model_feat.predict(x_test)","8003ea07":"from sklearn.pipeline import make_pipeline\nfrom sklearn.pipeline import Pipeline\nnames = [\n        \"K Nearest Neighbour Classifier\",\n        'SVM',\n        \"Random Forest Classifier\",\n        \"AdaBoost Classifier\", \n        \"XGB Classifier\",\n        \"MLP Classifier\"\n         ]\nclassifiers = [\n    KNeighborsClassifier(n_neighbors = 5, algorithm='ball_tree', leaf_size=30),\n    SVC(),\n    RandomForestClassifier(max_depth=9,criterion = 'entropy'),\n    AdaBoostClassifier(),\n    XGBClassifier(),\n    MLPClassifier()\n        ]\nzipped_clf = zip(names,classifiers)\ndef classifier_summary(pipeline, X_train, y_train, X_val, y_val,X_test,y_test):\n    sentiment_fit = pipeline.fit(X_train, y_train)\n    \n    y_pred_train= sentiment_fit.predict(X_train)\n    y_pred_val = sentiment_fit.predict(X_val)\n    y_pred_test = sentiment_fit.predict(X_test)\n    \n    train_accuracy = np.round(accuracy_score(y_train, y_pred_train),4)*100\n    train_precision = np.round(precision_score(y_train, y_pred_train, average='weighted'),4)\n    train_recall = np.round(recall_score(y_train, y_pred_train, average='weighted'),4)\n    train_F1 = np.round(f1_score(y_train, y_pred_train, average='weighted'),4)\n    train_kappa =  np.round(cohen_kappa_score(y_train, y_pred_train),4)\n    \n    \n    val_accuracy = np.round(accuracy_score(y_val, y_pred_val),4)*100\n    val_precision = np.round(precision_score(y_val, y_pred_val, average='weighted'),4)\n    val_recall = np.round(recall_score(y_val, y_pred_val, average='weighted'),4)\n    val_F1 = np.round(f1_score(y_val, y_pred_val, average='weighted'),4)\n    val_kappa =  np.round(cohen_kappa_score(y_val, y_pred_val),4)\n   \n    \n    test_accuracy = np.round(accuracy_score(y_test, y_pred_test),4)*100\n    test_precision = np.round(precision_score(y_test, y_pred_test, average='weighted'),2)\n    test_recall = np.round(recall_score(y_test, y_pred_test, average='weighted'),2)\n    test_F1 = np.round(f1_score(y_test, y_pred_test, average='weighted'),2)\n    test_kappa =  np.round(cohen_kappa_score(y_test, y_pred_test),2) \n  \n    \n    \n    print()\n    print('------------------------ Train Set Metrics------------------------')\n    print()\n    print(\"Accuracy core : {}%\".format(train_accuracy))\n    \n    print('------------------------ Validation Set Metrics------------------------')\n    print()\n    print(\"Accuracy score : {}%\".format(val_accuracy))\n    print('------------------------ Test Set Metrics------------------------')\n    print()\n    print(\"Accuracy score : {}%\".format(test_accuracy))\n    print(\"F1_score : {}\".format(test_F1))\n    print(\"Kappa Score : {} \".format(test_kappa))\n    print(\"Recall score: {}\".format(test_recall))\n    print(\"Precision score : {}\".format(test_precision))\n    \n    print(\"-\"*80)\n    print()\n    \ndef classifier_comparator(X_train,y_train,X_val,y_val,X_test,y_test,classifier=zipped_clf): \n    result = []\n    for n,c in classifier:\n        checker_pipeline = Pipeline([('Classifier', c)])\n        print(\"------------------------------Fitting {} on input_data-------------------------------- \".format(n))\n        #print(c)\n        classifier_summary(checker_pipeline,X_train, y_train, X_val, y_val,X_test,y_test)","9d55161d":"classifier_comparator(train_features,y_train,val_features,y_val,test_features,y_test,classifier=zipped_clf)","0232724d":"train_y=to_categorical(y_train,3)\nval_y=to_categorical(y_val,3)\ntest_y=to_categorical(y_test,3)\ndnn_model.compile(optimizer='adam',loss='categorical_crossentropy', metrics=['accuracy'])\nhistory = dnn_model.fit(train_features, train_y,validation_data=(val_features,val_y), epochs=10)\nloss_value , accuracy = dnn_model.evaluate(train_features, train_y)\nprint('Train_accuracy is:' + str(accuracy))\nloss_value , accuracy = dnn_model.evaluate(val_features, val_y)\nprint('Validation_accuracy is := ' + str(accuracy))\nloss_value , accuracy = dnn_model.evaluate(test_features, test_y)\nprint('test_accuracy is : = ' + str(accuracy))","63b965dc":"print(\"Performance Report:\")\ny_pred9=dnn_model.predict_classes(test_features)\ny_test9=[np.argmax(x) for x in test_y]\ny_pred_prb9=dnn_model.predict_proba(test_features)\ntarget=['0','1']\nfrom sklearn import metrics\nprint('Accuracy score is :', np.round(metrics.accuracy_score(y_test9, y_pred9),4))\nprint('Precision score is :', np.round(metrics.precision_score(y_test9, y_pred9, average='weighted'),4))\nprint('Recall score is :', np.round(metrics.recall_score(y_test9,y_pred9, average='weighted'),4))\nprint('F1 Score is :', np.round(metrics.f1_score(y_test9, y_pred9, average='weighted'),4))\nprint('Cohen Kappa Score:', np.round(metrics.cohen_kappa_score(y_test9, y_pred9),4))\nprint('\\t\\tClassification Report:\\n', metrics.classification_report(y_test9, y_pred9,target_names=target))","a23451ab":"knn = KNeighborsClassifier(n_neighbors = 5, algorithm='ball_tree', leaf_size=30)\nknn.fit(train_features, y_train)\nplot_confusion_matrix(knn, test_features, y_test)","5f35ab8d":"svc = SVC()\nsvc.fit(train_features, y_train)\nplot_confusion_matrix(svc, test_features, y_test)","63e92e6e":"rf = RandomForestClassifier()\nrf.fit(train_features, y_train)\nplot_confusion_matrix(rf, test_features, y_test)","e73f7dd3":"ada = AdaBoostClassifier()\nada.fit(train_features, y_train)\nplot_confusion_matrix(ada, test_features, y_test)","f3d0b040":"xgbc = XGBClassifier()\nxgbc.fit(train_features, y_train)\nplot_confusion_matrix(xgbc, test_features, y_test)","84e5d129":"mlpc = MLPClassifier()\nmlpc.fit(train_features, y_train)\nplot_confusion_matrix(mlpc, test_features, y_test)","d73e1ac0":"# VGG-16","f8b71065":"# DNN Model","130e9392":"# VGG-19","f775905b":"# DenseNet121","5c71028d":"# InceptionResNetV2","641100a2":"# ResNet50","d8c0504d":"# DenseNet169","f4107786":"# MobileNetV2","240a95b2":"# MobileNet","27af4afb":"# InceptionV3","f8e84a52":"# ResNet101","54ed6db1":"# XceptionNet"}}