{"cell_type":{"b98dde4d":"code","044fa088":"code","652ce362":"code","d245bdc0":"code","a99b74e4":"code","8e789adf":"code","f71764de":"code","165e70d8":"code","53d33d02":"code","c10bc399":"markdown"},"source":{"b98dde4d":"!pip install warp-rnnt","044fa088":"!pip install pytorch-edit-distance","652ce362":"import time\nimport torch\nfrom warp_rnnt import rnnt_loss\nfrom torch.utils.data import DataLoader\nfrom torch.nn.utils import clip_grad_norm_\nfrom torch.nn.functional import log_softmax, relu, elu\nfrom torch.optim import Adam\nfrom torch.optim.lr_scheduler import StepLR\nfrom open_stt_utils import Labels, Transducer, AudioDataset, BucketingSampler, AverageMeter, collate_fn_rnnt\nfrom pytorch_edit_distance import remove_blank, wer, AverageWER, AverageCER","d245bdc0":"labels = Labels()\nprint(len(labels))","a99b74e4":"model = Transducer(128, len(labels), 512, 256, am_layers=3, lm_layers=3, dropout=0.4)\nmodel.load_state_dict(torch.load('\/kaggle\/input\/open-stt-rnnt\/asr.bin', map_location='cpu'))\nmodel.cuda()","8e789adf":"train = [\n    ['open-stt-public-youtube1120-hq', 'data.csv']\n]\n\ntest = [\n    ['open-stt-val', 'asr_calls_2_val.csv'],\n    ['open-stt-val', 'buriy_audiobooks_2_val.csv'],\n    ['open-stt-val', 'public_youtube700_val.csv']\n]\n\ntrain = AudioDataset(train, labels)\ntest = AudioDataset(test, labels)\n\ntrain.filter_by_conv(model.encoder.conv)\ntrain.filter_by_length(500)\n\ntest.filter_by_conv(model.encoder.conv)\ntest.filter_by_length(1000)\n\nsampler = BucketingSampler(train, 32)\n\ntrain = DataLoader(train, pin_memory=True, num_workers=4, collate_fn=collate_fn_rnnt, batch_sampler=sampler)\ntest = DataLoader(test, pin_memory=True, num_workers=4, collate_fn=collate_fn_rnnt, batch_size=32)","f71764de":"optimizer = Adam(model.parameters(), lr=1e-5, weight_decay=1e-5)\nscheduler = StepLR(optimizer, step_size=250, gamma=0.99)","165e70d8":"N = 10\nalpha = 0.01\nblank = torch.tensor([labels.blank()], dtype=torch.int).cuda()\nspace = torch.tensor([labels.space()], dtype=torch.int).cuda()","53d33d02":"for epoch in range(10):\n    \n    start = time.time()\n\n    sampler.shuffle(epoch)\n\n    grd_train = AverageMeter('gradient')\n    rwd_train = AverageMeter('reward')\n    err_train = AverageMeter('train')\n    err_valid = AverageMeter('valid')\n    cer_valid = AverageCER(blank, space)\n    wer_valid = AverageWER(blank, space)\n\n    num_batch = 0\n\n    for xs, ys, xn, yn in train:\n\n        optimizer.zero_grad()\n\n        xs = xs.cuda(non_blocking=True)\n        ys = ys.cuda(non_blocking=True)\n        xn = xn.cuda(non_blocking=True)\n        yn = yn.cuda(non_blocking=True)\n\n        model.train()\n\n        zs, xs, xn = model(xs, ys, xn, yn)\n        \n        model.eval()\n\n        with torch.no_grad():\n\n            ys = ys.t().contiguous()\n\n            xs_e = xs.repeat(N, 1, 1)\n            xn_e = xn.repeat(N)\n            ys_e = ys.repeat(N, 1)\n            yn_e = yn.repeat(N)\n\n            hs_e = model.greedy_decode(xs_e, sampled=True)\n\n            remove_blank(hs_e, xn_e, blank)\n\n            Err = wer(hs_e, ys_e, xn_e, yn_e, blank, space)\n\n            xn_e_safe = torch.max(xn_e, torch.ones_like(xn_e)).float()\n\n            SymAcc = 1 - 0.5 * Err * (1 + yn_e.float() \/ xn_e_safe)\n\n            rewards = relu(SymAcc).reshape(N, -1)\n\n            hs_e = hs_e.reshape(N, len(xs), -1)\n            xn_e = xn_e.reshape(N, len(xs))\n            \n        model.train()\n        \n        rewards = rewards.cuda()\n            \n        rwd_train.update(rewards.mean().item())\n\n        rewards -= rewards.mean(dim=0)\n        \n        # Stabilize training\n        elu(rewards, alpha=0.5, inplace=True)\n\n        total_loss = 0\n\n        if alpha > 0:\n\n            nll = rnnt_loss(zs, ys, xn, yn)\n\n            loss = alpha * nll.mean()\n            loss.backward(retain_graph=True)\n\n            total_loss = loss.item()\n\n        for n in range(N):\n\n            ys = hs_e[n]\n            yn = xn_e[n]\n\n            # Cut unnecessary padding\n            ys = ys[:, :yn.max()].contiguous()\n\n            zs = model.forward_decoder(xs, ys.t(), yn)\n\n            nll = rnnt_loss(zs, ys, xn, yn)\n\n            loss = nll * rewards[n]\n\n            loss = loss.mean() \/ N\n\n            loss.backward(retain_graph=True)\n\n            total_loss += loss.item()\n\n        grad_norm = clip_grad_norm_(model.parameters(), 10)\n\n        optimizer.step()\n        scheduler.step()\n\n        err_train.update(total_loss)\n        grd_train.update(grad_norm)\n\n        num_batch += 1\n        if num_batch == 500:\n            break\n    \n    model.eval()\n\n    with torch.no_grad():\n        for xs, ys, xn, yn in test:\n\n            xs = xs.cuda(non_blocking=True)\n            ys = ys.cuda(non_blocking=True)\n            xn = xn.cuda(non_blocking=True)\n            yn = yn.cuda(non_blocking=True)\n\n            zs, xs, xn = model(xs, ys, xn, yn)\n\n            ys = ys.t().contiguous()\n\n            loss = rnnt_loss(zs, ys, xn, yn, average_frames=False, reduction=\"mean\")\n            err_valid.update(loss.item())\n            \n            xs = model.greedy_decode(xs)\n\n            remove_blank(xs, xn, blank)\n\n            cer_valid.update(xs, ys, xn, yn)\n            wer_valid.update(xs, ys, xn, yn)\n    \n    minutes = (time.time() - start) \/\/ 60\n    \n    with open('asr.log', 'a') as log:\n        log.write('epoch %d lr %.6f %s %s %s %s %s %s time %d\\n' % (\n            epoch + 1, scheduler.get_lr()[0],\n            grd_train, rwd_train, err_train,\n            err_valid, cer_valid, wer_valid,\n            minutes\n        ))\n    \n    torch.save(model.state_dict(), 'asr.bin')","c10bc399":"### Reinforcement Learning to fine-tune the RNN-Transducer model\n\nIt is my favorite part from series of end-to-end acoustic models in PyTorch. In the following links you can find the previous parts:\n1. [open-stt-lm](https:\/\/www.kaggle.com\/sorokin\/open-stt-lm) - Character-based RNN language model\n2. [open-stt-ctc](https:\/\/www.kaggle.com\/sorokin\/open-stt-ctc) - CNN-RNN acoustic model with CTC loss\n3. [open-stt-rnnt](https:\/\/www.kaggle.com\/sorokin\/open-stt-rnnt) - Character-based RNN language model and CNN-RNN acoustic model with RNN-T loss\n\nIn this kernel I will apply a REINFORCE algorithm with simple sampling strategy. For each example from the training set, N hypothesis will be generated through a decoder. These hypothesis will be used to estimate the expected reward, which will be maximized during the training procedure.\n\nAs a result of this fine-tuning approach, it is possible to reduce WER of the original model by a few absolute percent. You can find more information at the github [project](https:\/\/github.com\/1ytic\/open_stt_e2e).\n\n**V1**: Initial version with Expected Loss implementation.\n\n**V2**: Used a reward engineering method, called Symmetric Accuracy (SymAcc), taken from [work](http:\/\/www.apsipa.org\/proceedings\/2018\/pdfs\/0001934.pdf). SymAcc discounts the accuracy when the length of the hypothesis is shorter than the reference, which motivates the system to try longer hypothesis.\n\n**V5**: Added pytorch-edit-distance package to speed up training and accurately calculate WER during evaluation. Increased learning rate and stabilized training with ELU(rewards, alpha=0.5)."}}