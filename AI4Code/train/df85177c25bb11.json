{"cell_type":{"acae85bb":"code","83929b1f":"code","cd746ca0":"code","54c4cac9":"code","d717fe10":"code","18e1718d":"code","58668090":"code","75c1bc28":"code","bdead851":"code","64d4b8c5":"code","58ba13e0":"code","2034a025":"code","414380ab":"code","403975d1":"code","fa1dedc0":"code","7fe90283":"code","930abad8":"code","818dd6a8":"code","81c14535":"code","08092379":"markdown","5102d638":"markdown","4d7f11de":"markdown","ccde086c":"markdown","8ed947b4":"markdown","73c2dbdc":"markdown","4c9a4875":"markdown","d8f6d10e":"markdown","484ef80f":"markdown","08d9cc3b":"markdown"},"source":{"acae85bb":"%matplotlib inline\nimport matplotlib.pyplot as plt\n\nimport numpy as np\nfrom sklearn.kernel_approximation import RBFSampler","83929b1f":"# let's use len_x * len_y samples to draw the visualization.\nlen_x = 200\nlen_y = 200\n\n# let's have the value space in our 2D example run from min_val to max_val\nmin_val = -2\nmax_val = +2\n\n# * the image will be drawn by using matplotlib's imshow, which expects a 3d array\/list of pixels\n#   in the following form: [rows, columns, rgb-values]\n# * so we generate a mesh grid that we can use to map the x, y values to colors in the\n#   same order, as imshow needs it\n# * since imshow draws from top\/left to bottom\/right, we need to make sure, that the y-axis\n#   runs from max_val to min_val and in contrast the x-axis from min_val to max_val\n# * the complex number syntax is just an oddity of numpy.mgrid - refer to the documentation\ngrid_y, grid_x = np.mgrid[max_val:min_val:complex(0, len_y), min_val:max_val:complex(0, len_x)]\ngrid_x = grid_x.ravel()\ngrid_y = grid_y.ravel()","cd746ca0":"# Gaussian Radial Basis Function\n# for example as defined here https:\/\/en.wikipedia.org\/wiki\/Radial_basis_function\ndef rbf(x, l, g):\n    x = np.array(x)\n    l = np.array(l)\n    return np.exp(-(g * np.linalg.norm(x - l)**2))","54c4cac9":"# apply the RBF to our value spectrum ranging from min_val to max_val (e.g. from -2 to +2)\n# and by doing so, generate the image that we want to show\nimage = []\n\nfor y, x in zip(grid_y, grid_x):\n    X = [x, y]       # 2D vector for RBF\n    C = [0.7, 0.7]   # center of the RBF\n    image.append(3 * [rbf(X, C, 1.0)]) # RBF with gamma = 1.0\n\n# hint: the above 3 * [] statement makes sure, that R, G and B color values are equal,\n# so that the result will be a grayscale from black to white","d717fe10":"# transform the flat image data into the [rows, columns, RGB] form that the imshow routine expects\nimage = np.array(image).reshape(len_y, len_x, 3)","18e1718d":"# draw the values of the feature transformation of the x, y coordinates to the RBF\nplt.figure(figsize=(8, 8))\nplt.imshow(image, extent=[min_val, max_val, min_val, max_val], interpolation=\"none\")\nplt.show()","58668090":"# we are using just one exemplar \/ center by setting n_components to 1\n# also note that we use the same gamma value as abve\nrbfs = RBFSampler(gamma=1.0, n_components=1)","75c1bc28":"# reading the documentation shows, that the fit function does not consider the data at all, but only\n# the dimensionality of the data, so we can pass some dummy numbers\nrbfs.fit([[0, 0]])","bdead851":"# transformation function that takes into consideration, that rbf.transform returns an array with one\n# element, so using \"min\" (or \"max\") extracts that; additionally and strangely enough, RBFSampler's transformation\n# can also yield negative numbers\ndef rbf_distance(x, y):\n    return np.min(rbfs.transform([[x, y]]))","64d4b8c5":"# same visualization technique as above ...\nimage2 = []\n\nfor y, x in zip(grid_y, grid_x):\n    image2.append(3 * [rbf_distance(x, y)])\n\nimage2 = np.array(image2).reshape(len_y, len_x, 3)\n\n# ... but this time we need to make sure that the output is normalized to be between 0 and 1\n# (something we would not have to do if RBFSampler actually behaved like RBFs)\nimage2 -= np.min(image2)  # make sure that all values are > 0\nimage2 \/= np.max(image2)  # normalize between 0 .. 1","58ba13e0":"# draw it\nplt.figure(figsize=(8, 8))\nplt.imshow(image2, extent=[min_val, max_val, min_val, max_val], interpolation=\"none\")\nplt.show()","2034a025":"EXEMPLARS = 20\nCs_GAMMA = 3.0\n\n# create EXEMPLARS amount of centers that fit into min_val .. max_val\nCs = []\nCs_width = max_val - min_val\nfor i in range(EXEMPLARS):\n    Cs.append([np.random.rand()*Cs_width - Cs_width\/2.0, np.random.rand()*Cs_width - Cs_width\/2.0, Cs_GAMMA])","414380ab":"# to visualize multiple RBFs, we are adding up all distances to all centers for the current pixel\ndef multi_rbf(x):\n    ret_val = 0.0\n    x = np.array(x)\n    for c in Cs:\n        l = [c[0], c[1]]\n        ret_val += rbf(x, l, c[2])\n    return ret_val","403975d1":"# plot using the technique described above in detail\nimage3 = []\n\nfor y, x in zip(grid_y, grid_x):\n    image3.append(3 * [multi_rbf([x, y])])   \n    \nimage3 = np.array(image3).reshape(len_y, len_x, 3)\nimage3 \/= np.max(image3) # as we are adding up all RBFs distances, we need to normalize to 0 .. 1\n\nplt.figure(figsize=(8, 8))\nplt.imshow(image3, extent=[min_val, max_val, min_val, max_val], interpolation=\"none\")\nplt.show()","fa1dedc0":"rbfs2 = RBFSampler(gamma=Cs_GAMMA, n_components=EXEMPLARS)\nrbfs2.fit([[0, 0]])","7fe90283":"# similar mechanism as above: to visualize multiple RBFs, we are adding up all distances to all centers\n# for the current pixel with the speciality, that we also have negative values here\ndef rbf_distance(x, y):\n    return np.sum(rbfs2.transform([[x, y]]))","930abad8":"image4 = []\n\nfor y, x in zip(grid_y, grid_x):\n    image4.append(3 * [rbf_distance(x, y)])","818dd6a8":"image4 = np.array(image4).reshape(len_y, len_x, 3)\n\n# special post processing needed:\n# we have potentially large negative values; by clipping them, the resulting image is \"a bit less crowded\"\nimage4 = np.clip(image4, np.min(image4)*0.4, 100)\n\n# the usual \"make all values positive and scale to 0 .. 1\"\nimage4 -= np.min(image4)\nimage4 \/= np.max(image4)","81c14535":"plt.figure(figsize=(8, 8))\nplt.imshow(image4, extent=[min_val, max_val, min_val, max_val], interpolation=\"none\")\nplt.show()","08092379":"Interpretation:\n\nIn the source code of RBFSampler we see, that there is a projection and a cosine function:\n\nhttps:\/\/github.com\/scikit-learn\/scikit-learn\/blob\/7b136e9\/sklearn\/kernel_approximation.py#L126\n\nThis can explain the above-mentioned pattern in the image.\n\nBut unfortunatelly this can not explain my initial questions and at this stage, I would have wondered, how `RBFSampler` can be of any use in the context of RBFs.","5102d638":"#### Step 4 - Using RBFSampler with 20 Exemplars","4d7f11de":"#### Step 2 - Using RBFSampler with 1 Exemplar\nTrying to reproduce the result from step (1) with RBFSampler.","ccde086c":"## RBFSampler actually is not using any Radial Basis Functions (RBFs)\n\ndone by [sy2002](http:\/\/www.sy2002.de) on 12th of April 2019, refined and documented on 17th of April 2019\n\n### Naive Assumption\n\nWhile programming a\n[Reinforcement Learning solution](https:\/\/github.com\/sy2002\/ai-playground\/blob\/master\/rl-q-rbf-cartpole.py)\nto the\n[Inverse Pendulum problem](http:\/\/gym.openai.com\/envs\/CartPole-v1\/),\nI did not care, how\n[RBFSampler](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.kernel_approximation.RBFSampler.html)\nworks under the hood in detail. Deducting from the name, I naively thought that it would do the following:\n\n1. When fitting it using `RBFSampler.fit(...)`, it would use the samples given in the fit function for stochastically finding optimal centers (aka \"Exemplars\") for an amount of Radial Basis Functions (RBFs) as specified by the parameter `n_components` in the constructor of the RBFSampler.\n2. When performing a feature transformation using `RBFSampler.transform(...)`, it would return the distances to all these Exemplars as new features.\n3. ... well, just what you would expect from RBFs :-)\n\nSpoiler alarm: It does not - but it still works, so it is kind of fair and valid to think about RBFSampler, as if it would be RBFs. Read on to find out why.\n\n### RTFM - The scikit-learn team never claimed that RBFSampler would work like an RBF sampler\n\nMy naive assumption was not well-grounded. The scikit-learn manual states, that the RBFSampler class \"approximates [the] feature map of an RBF kernel by Monte Carlo approximation of its Fourier transform\". And it even goes on mentioning that \"it implements a variant of Random Kitchen Sinks\" by pointing to the\n[underlying paper](https:\/\/papers.nips.cc\/paper\/3182-random-features-for-large-scale-kernel-machines.pdf).\nPretty cryptic for a beginner like me. Investing a serious amount of time, I might have been able to understand this paper, but I was too impatient to work through it, so I tried Google and found another\n[background article](http:\/\/www.argmin.net\/2017\/12\/05\/kitchen-sinks\/)\non the topic. Unfortunatelly, this one also did not enlighten me.\n\nLast but not least, also the\n[Issue Tracker of scikit-learn Team](https:\/\/github.com\/scikit-learn\/scikit-learn\/issues\/4298#issuecomment-376812492)\nmentions the phenomenon and reiterates on a topic that is also mentioned in the documentation: \"Note that 'fitting' the feature function does not actually depend on the data given to the fit function. Only the dimensionality of the data is used.\"\n\nSo I began asking myself some questions:\n\n* Why is RBFSampler called RBFSampler, when in reality it does \"something else\"?\n* Why isn't it caring at all about the given data while fitting?\n* Where does the algorithm take the centers from?\n* Why can it return negative numbers (while RBFs cannot)?\n* What is it actually doing?\n* Is \"something else\" maybe mathmatically equivalent to RBFs, at least when it comes to the outcome in the context of machine learning - but in contrast to RBFs it is faster?\n\n### Creating an experiment to understand the nature of the beast\n\nLet's assume the RBFSampler actually would use Radial Basis Functions (RBFs). If you would then use it in a two-dimensional (2D) space - a plane - then it would behave like this: Given a coordinate vector (x, y) in 2D space and one RBF centered at (cx, cy). Then the RBFSampler.transform([x, y]) function would output one floating point number, which would be something like the \"distance\" to the center of the RBF.\n[Wikipedia](https:\/\/en.wikipedia.org\/wiki\/Radial_basis_function#Examples) is nicely illustrating this, for example for Gaussian RBFs.\n\nThe following code experiment is performing four steps and for each step, it paints the result as a 2D image, where high values of the RBF (or RBFSampler) are painted white and low values are painted black. The four steps are:\n\n1. Manually implement a Gaussian RBF\n2. Use RBFSampler with `n_components=1` to try to reproduce the same result as with the manual implementation\n3. Repeat step (1) for 20 Gaussian RBFs with random centers\n4. Repeat step (2) also with 20 centers \/ Exemplars using `n_components=20`","8ed947b4":"Interpretation:\n    \nAha! This looks pretty much like step (3), I would even go as far as it looks identically, given that\nthe stochastic nature of the beast obviously leads to 20 different centers than in our manual step (3).","73c2dbdc":"Interpretation:\n\nAgain, this is exactly what you would have expected: Due to the wide Gamma factor of 3.0 for the Gaussian RBF, we are seeing 20 (admittedly) pretty blurred circles that flow into each other.","4c9a4875":"### Conclusion\n\n[Random Kitchen Sinks](https:\/\/papers.nips.cc\/paper\/3182-random-features-for-large-scale-kernel-machines.pdf)\nare for sure not equivalent to Radial Basis Functions. But as this experiment shows for the 2D space,\nthey are approximating them pretty well.\n\nSkimming over the papers it appears to me, that the computational performance is much better than calculating\nRBFs manually and this might have been the reason, why the scikit-learn team finally decided to settle for\nRandom Kitchen Sinks and still call them `RBFSampler`.","d8f6d10e":"#### Step 1 - Manual RBF\n\nManually programmed RBF, visualized in 2D space.","484ef80f":"#### Step 3 - Manually creating a \"network\" of 20 RBFs\nRandomly find 20 centers for 20 RBFs and paint the resulting image by adding up the distances to each RBF center at each point of the plane.\n","08d9cc3b":"Interpretation:\n\nThis is exactly what one would expect from a 2D feature transformation using an RBF: Depending on where the center point of the RBF is - in this example (0.7, 0.7) - and depending on the gamma value for the Gaussian distribution - in this example 1.0 - we get a circle shaped gradient that fades from black (0.0) to white (1.0)."}}