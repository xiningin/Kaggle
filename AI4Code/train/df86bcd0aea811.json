{"cell_type":{"ea5cc9b8":"code","8797e370":"code","343847d9":"code","12f819e7":"code","f3901d66":"code","84e16392":"code","fc5696f3":"code","5f3cf3cc":"code","0e8ed22f":"code","15af78a1":"code","d0a04823":"code","1644e366":"code","a91d33f0":"markdown","9a9f8338":"markdown","d5c8f200":"markdown","3c84ffb7":"markdown","6b72627e":"markdown","e7b39217":"markdown","f87ad4b9":"markdown","e9750ab7":"markdown","c6dfbf74":"markdown","366b47d5":"markdown","a9242574":"markdown","b9fd4e06":"markdown","b2c85e91":"markdown","58eee448":"markdown"},"source":{"ea5cc9b8":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n#for dirname, _, filenames in os.walk('\/kaggle\/input'):\n#    for filename in filenames:\n#        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","8797e370":"data = pd.DataFrame(np.array([np.linspace(-7,7, num = 15), [0,0,0,0,1,1,1,1,1,1,1,0,0,0,0]]).T, columns = ['feature_x', 'target'])\n\ndata","343847d9":"sns.scatterplot(x = 'feature_x', y = np.zeros(data.shape[0]), data = data, hue = 'target')","12f819e7":"def s1(x): return x - 3.5 # ==> x * w1 + b1, with w1 = 1 and b1 = -3.5\ndef s2(x): return x + 3.5 # ==> x * w1 + b1, with w1 = 1 and b1 = 3.5","f3901d66":"def simple_class(x):\n    return s1(x) < 0 and s2(x) > 0","84e16392":"sp=sns.scatterplot(x = 'feature_x', y = np.zeros(data.shape[0]), data = data, hue = 'target')\n\nsp.axvline(-3.5, c='green')\nsp.axvline(3.5, c='green')","fc5696f3":"data['feature_x^2'] = data['feature_x']**2","5f3cf3cc":"sns.scatterplot(x = 'feature_x', y = 'feature_x^2', data = data, hue = 'target')","0e8ed22f":"sns.scatterplot(x = 'feature_x', y = 'feature_x^2', data = data, hue = 'target').axhline(3.5**2, c='green')","15af78a1":"\ndata['act(feature_x-3.5)'] = np.tanh(data['feature_x']-3.5)\ndata['act(feature_x+3.5)'] = np.tanh(data['feature_x']+3.5)\n","d0a04823":"sp=sns.scatterplot(x = 'act(feature_x-3.5)', y = 'act(feature_x+3.5)', data = data, hue = 'target')\ni=np.linspace(-1,0,50)\nsp.plot(i,i+1, c='green')","1644e366":"data['act(feature_x-3.5)'] = (data['feature_x']-3.5)\ndata['act(feature_x+3.5)'] = (data['feature_x']+3.5)\nsns.scatterplot(x = 'act(feature_x-3.5)', y = 'act(feature_x+3.5)', data = data, hue = 'target')","a91d33f0":"# Simple classification","9a9f8338":"Without the nonlinear function the features would still be spread in a row like this:","d5c8f200":"Sofare we have seen two ways to separate the classes.\n\n1. Applying two linear seperaters followed by a clunky case distinction\n2. Use one linear function after carfully engineering a new feature\n\nNow let's have a look at how perceptrons combine the best of the two former examples. A perceptron is basicly a linear function followed by a nonlinear (activation) function:\n\n*activate(x * w + b)*\n\n(Not every non linear function is suitable for neural nets, do to the training process (backpropagation) of the weights. But that's another topic.)\nLet's take the common activation function *tanh* for this example, though other functions like x^2, sigmoid, relu or even sign(x) would work here, too. ","3c84ffb7":"Now the data after generating the new feature can easily linearly be separated: ","6b72627e":"# Putting it all together","e7b39217":"Another approch to classify the data could be to do some feature engineering first.\nLet's do a nonlineare transformation of the input feature by taking the square:","f87ad4b9":"# Bending the feature space","e9750ab7":"# Data\nLet's have a look at the data.","c6dfbf74":"The two perceptrons generated two new feature that \"bent the feature space\", so that the classes can now easily be separated by a linear function. In a real neural net choosing the number of percetrons and the kind of activation function is roughly said secondary (;D) because the training of the weights and biases determin which percetrons will be more or less active. \n","366b47d5":"# Intro\nThis notebook is part of a [discussion](https:\/\/www.kaggle.com\/discussion\/196486) about linearity and nonlinearity in neural nets. \nAs an example here is a binary classification problem with only one input feature. The notebooks shows intuitivly how nonlinearity in form of activation functions work in neural nets. \n\nTo follow along the reader should be familiar with the basic concept of a perceptron and multilayer neural networks.\n\n## Further References:\nGreat showcase of nonlinearity on XOR with keras: https:\/\/towardsdatascience.com\/visualizing-the-non-linearity-of-neural-networks-c55b2a14ad7a","a9242574":"And then classify like this:","b9fd4e06":"One way to separate the two classes is to define to functions (in the form of *x * w + b*) like this:","b2c85e91":"Thanks for reading. Please feel free to leave a comment if there are any issues or join the discussion on the [discussion](https:\/\/www.kaggle.com\/discussion\/196486) board.","58eee448":"There is only one feature *x* and a binary class target. The targets are spread as shown in the plot below."}}