{"cell_type":{"4783b3bc":"code","62e0ddfc":"code","1fd35273":"code","1247b4ce":"code","391bbe0c":"code","ac2ef2b5":"code","215e1530":"code","132fd26d":"code","dc1c2150":"code","204d3f5f":"code","67111a47":"code","0074d5e2":"code","447d48de":"code","22b99efa":"code","ad5c0f4e":"code","e2768451":"code","5497cf1c":"code","01554e7b":"markdown","afc21717":"markdown","3f8d4c53":"markdown","e7860ab4":"markdown","88a03706":"markdown","5ea8a028":"markdown","0c6daa80":"markdown","85e97c22":"markdown","a621fde8":"markdown","f8a75d75":"markdown"},"source":{"4783b3bc":"!pip install neural-structured-learning\n!pip install tensorflow-text","62e0ddfc":"import numpy as np\nimport pandas as pd\n\nfrom sklearn.model_selection import train_test_split\n\nimport neural_structured_learning as nsl\n\nimport tensorflow as tf\nimport tensorflow_text as text\nimport tensorflow_hub as tfh\n\ntf.keras.backend.clear_session()","1fd35273":"df = pd.read_csv('\/kaggle\/input\/disaster-tweets-cleaned\/df.csv')\ntest_df = pd.read_csv('\/kaggle\/input\/disaster-tweets-cleaned\/test_df.csv')\n\nX_tr, X_val, y_tr, y_val = train_test_split(\n    df['ctext'].values, df['target'].values,\n    test_size = 0.15,\n    shuffle = True, stratify = df['target']\n)\nX_test = test_df['ctext'].values\ny_test = test_df['target'].values\ny_tr = np.reshape(y_tr, (-1, 1)).astype(np.float32)\ny_val = np.reshape(y_val, (-1, 1)).astype(np.float32)\ny_test = np.reshape(y_test, (-1, 1)).astype(np.float32)\nprint(X_tr.shape, y_tr.shape)\nprint(X_val.shape, y_val.shape)\nprint(X_test.shape, y_test.shape)","1247b4ce":"preprocessor = tfh.KerasLayer(\"https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_preprocess\/3\")\nembedding_handler = 'https:\/\/tfhub.dev\/tensorflow\/small_bert\/bert_en_uncased_L-4_H-128_A-2\/2'\nembedding_layer = tfh.KerasLayer(embedding_handler, trainable = True, name = 'embedder')","391bbe0c":"def make_model(seq_length = 40 ):\n    text_input = tf.keras.layers.Input(shape=(), dtype=tf.string)\n    encoder_inputs = preprocessor(text_input)\n    x = embedding_layer(encoder_inputs,)\n    x = x['pooled_output']\n    output = tf.keras.layers.Dense(1, activation = 'sigmoid')(x)\n    model = tf.keras.Model(text_input, output)\n    model.compile(loss = 'binary_crossentropy', \n                  optimizer = tf.keras.optimizers.Adam(1e-4), \n                  metrics = ['acc'])\n    return model\n\nmodel = make_model()\nmodel.fit(x = X_tr, y = y_tr, epochs = 3, validation_data = (X_val, y_val))","ac2ef2b5":"text_input = tf.keras.layers.Input(shape=(), dtype=tf.string)\nencoder_inputs = preprocessor(text_input)\nembedd = model.get_layer('embedder')(encoder_inputs)\nembedder = tf.keras.Model(text_input, embedd)","215e1530":"embed_X_tr = embedder(X_tr)['pooled_output'].numpy()\nembed_X_val = embedder(X_val)['pooled_output'].numpy()\nembed_X_test = embedder(X_test)['pooled_output'].numpy()\n\nnp.save('X_tr', embed_X_tr)\nnp.save('X_val', embed_X_val)\nnp.save('X_test', embed_X_test)","132fd26d":"embed_X_tr = np.load('X_tr.npy')\nembed_X_val = np.load('X_val.npy')\nembed_X_test = np.load('X_test.npy')","dc1c2150":"IMAGE_INPUT_NAME = 'image'\nLABEL_INPUT_NAME = 'label'\nbatch_size = 256","204d3f5f":"def convert_to_tuples(features):\n    return features[IMAGE_INPUT_NAME], features[LABEL_INPUT_NAME]\n\ndef convert_to_dictionaries(image, label):\n    return {IMAGE_INPUT_NAME: image, LABEL_INPUT_NAME: label}\n\ntrain_dataset = tf.data.Dataset.from_tensor_slices((embed_X_tr, y_tr))\nval_dataset = tf.data.Dataset.from_tensor_slices((embed_X_val, y_val))\ntrain_dataset = train_dataset.batch(batch_size)\nval_dataset = val_dataset.batch(batch_size)","67111a47":"def make_feed_forward_model():\n    inputs = tf.keras.Input(shape=(128,), dtype='float32', name=IMAGE_INPUT_NAME)\n    dense_layer = tf.keras.layers.Dense(128, activation='relu')(inputs)\n    dense_layer = tf.keras.layers.Dense(32, activation='relu')(dense_layer)\n    outputs = tf.keras.layers.Dense(1, activation='sigmoid')(dense_layer)\n    return tf.keras.Model(inputs=inputs, outputs=outputs)\n\nclassifier = make_feed_forward_model()\nclassifier.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['acc'])\nclassifier.fit(train_dataset, validation_data = val_dataset, epochs = 15, verbose = 1)","0074d5e2":"results = classifier.evaluate(embed_X_test, y_test)\nprint(results)","447d48de":"adv_config = nsl.configs.make_adv_reg_config(\n    multiplier = 0.5,\n    adv_step_size = 0.5,\n    adv_grad_norm='infinity',\n)","22b99efa":"base_adv_model = make_feed_forward_model()\nadv_model = nsl.keras.AdversarialRegularization(\n    base_adv_model,\n    label_keys=[LABEL_INPUT_NAME],\n    adv_config=adv_config,\n)\ntrain_set_for_adv_model = tf.data.Dataset.from_tensor_slices((embed_X_tr, y_tr))\nval_set_for_adv_model = tf.data.Dataset.from_tensor_slices((embed_X_val, y_val))\ntrain_set_for_adv_model = train_set_for_adv_model.map(convert_to_dictionaries).batch(batch_size)\nval_set_for_adv_model = val_set_for_adv_model.map(convert_to_dictionaries).batch(batch_size)","ad5c0f4e":"adv_model.compile(optimizer='adam', \n                  loss='binary_crossentropy', \n                  metrics=['binary_accuracy'])\n\nadv_model.fit(train_set_for_adv_model,\n              validation_data = val_set_for_adv_model, \n              epochs = 15)","e2768451":"results = base_adv_model.evaluate(embed_X_test, y_test)\nprint(results)","5497cf1c":"embed_X_test.shape","01554e7b":"# BertTokenizer\/Bert","afc21717":"# Install libs","3f8d4c53":"# Read\/shuffle\/split","e7860ab4":"# Fine Tune Bert","88a03706":"# Data Pipeline","5ea8a028":"# Adversarial Model","0c6daa80":"# Get Feature extractor from Fine-Tuned Bert","85e97c22":"# Create embeddings for the data","a621fde8":"Import libs","f8a75d75":"# Base Model"}}