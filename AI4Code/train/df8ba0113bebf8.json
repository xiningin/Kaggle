{"cell_type":{"47f8738b":"code","38d43559":"code","20ca55ee":"code","87080804":"code","f1b9af1c":"code","90c506e0":"code","d3f32fe3":"code","c55760f6":"code","3a1bf7f0":"code","14181a13":"code","374cc6e5":"code","dec5e2e4":"code","a0291d79":"code","75a13bb3":"markdown","1ee2492f":"markdown","55a19db9":"markdown","ed14e539":"markdown","a856002e":"markdown","b75bf3a8":"markdown","9f7fb025":"markdown","b453f943":"markdown","e92933b3":"markdown","fd6d5444":"markdown","497943e9":"markdown","bd66a37a":"markdown","4845479b":"markdown","234ed958":"markdown","d6b671a0":"markdown","2bd39a15":"markdown","d4b46c19":"markdown","02f209e0":"markdown"},"source":{"47f8738b":"#importing libraries which will be required in the process\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n#supress warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")","38d43559":"#loading dataset using pandas and showing first five rows\ndf = pd.read_csv('\/kaggle\/input\/advertising-data\/Advertising.csv')\ndf.head()","20ca55ee":"# X -> Independent variable, which is TV\nX = df.iloc[:,1:2].values\n\n# y -> Dependent varible, which is Sales\ny = df.iloc[:,-1:].values","87080804":"#Now we will see a scatter plot to show relation between independent and dependent varaiable\nplt.scatter(X,y,alpha=0.5)\nplt.xlabel('TV',size=10)\nplt.ylabel('Sales',size=10)\nplt.title('Scatter Plot of TV vs Sales',size=15)\nplt.show()","f1b9af1c":"def FeatureScaling(X):\n    \"\"\"\n    is function takes an array as an input, which needs to be scaled down.\n    Apply Standardization technique to it and scale down the features with mean = 0 and standard deviation = 1\n    \n    Input <- 2 dimensional numpy array\n    Returns -> Numpy array after applying Feature Scaling\n    \"\"\"\n    mean = np.mean(X,axis=0)\n    std = np.std(X,axis=0)\n    for i in range(X.shape[1]):\n        X[:,i] = (X[:,i]-mean[i])\/std[i]\n\n    return X","90c506e0":"# scaling variable TV using defined function\nX = FeatureScaling(X)","d3f32fe3":"print(\"X before adding column of 1's:\",X[:2],sep=\"\\n\")\nX = np.append(arr=np.ones((X.shape[0],1)),values=X,axis=1)\nprint(\"\\nX after adding column of 1's:\",X[:2],sep=\"\\n\")","c55760f6":"#ComputeCost function determines the cost (sum of squared errors) \n\ndef ComputeCost(X,y,theta):\n    \"\"\"\n    This function takes three inputs and uses the Cost Function to determine the cost (basically error of prediction vs\n    actual values)\n    Cost Function: Sum of square of error in predicted values divided by number of data points in the set\n    J = 1\/(2*m) *  Summation(Square(Predicted values - Actual values))\n    \n    Input <- Take three numoy array X,y and theta\n    Return -> The cost calculated from the Cost Function\n    \"\"\"\n    m=X.shape[0] #number of data points in the set\n    J = (1\/(2*m)) * np.sum((X.dot(theta) - y)**2)\n    return J","3a1bf7f0":"#Gradient Descent Algorithm to minimize the Cost and find best parameters in order to get best line for our dataset\n\ndef GradientDescent_New(X,y,theta,alpha,no_of_iters):\n    \"\"\"\n    Gradient Descent Algorithm to minimize the Cost\n    \n    Input <- X, y and theta are numpy arrays\n            X -> Independent Variables\/ Features\n            y -> Dependent\/ Target Variable\n            theta -> Parameters \n            alpha -> Learning Rate i.e. size of each steps we take\n            no_of_iters -> Number of iterations we want to perform\n        Return -> theta (numpy array) which are the best parameters for our dataset to fit a linear line\n             and Cost Computed (numpy array) for each iteration\n    \"\"\"\n    m=X.shape[0]\n    J_Cost = []\n    theta_array = []\n    for i in range(no_of_iters):\n        error = np.dot(X.transpose(),(X.dot(theta)-y))\n        theta = theta - alpha * (1\/m) * error\n        J_Cost.append(ComputeCost(X,y,theta))\n        \n        #below code is to note theta value of every 30th iteration, which we will be using further in this notebook\n        if (i+1)%30 == 0:\n            theta_array.append(theta)\n    \n    return theta, np.array(J_Cost), np.array(theta_array)","14181a13":"#number of iterations\niters = 300\n\n#learning rate\nalpha = 0.01\n\n#initializing theta\ntheta = np.zeros((X.shape[1],1))\n\n#finally computing values using function\ntheta, J_Costs, theta_array = GradientDescent_New(X,y,theta,alpha,iters)","374cc6e5":"plt.figure(figsize=(8,5))\nplt.plot(J_Costs,color=\"g\")\nplt.title('Convergence of Gradient Descent Algorithm')\nplt.xlabel('No. of iterations')\nplt.ylabel('Cost')\nplt.show()","dec5e2e4":"# Removing column of 1's from X in order to visualize the data.\nX = X[:,1:]","a0291d79":"for i in range(10):\n    plt.figure(figsize=(40,10))\n    plt.subplot(2,5,i+1)\n    b0, b1 = round(float(theta_array[i,0]),2), round(float(theta_array[i,1]),2)\n    y_pred = b0 + b1 * X\n    mse = round(J_Costs[30*i+30-1],2)\n    plt.scatter(X,y,alpha=0.5)\n    plt.plot(X,y_pred,color=\"r\")\n    plt.xlabel('TV',size=10)\n    plt.ylabel('Sales',size=10)\n    plt.title('Sales = {} + {} * TV (after {} iterations, MSE: {})'.format(b0,b1,30*i+30,mse),size=14)\n    plt.show()","75a13bb3":"Now we will visualize, how `Gradient Descent Algorithm` minimized the error.","1ee2492f":"This sole purpose of this notebook is to show how `Gradient Descent Algorithm` works, also it serves as a code implementation for the article `Understanding Gradient Descent Algorithm through Golf`. If you have not checked the article, click [here](https:\/\/www.linkedin.com\/pulse\/understanding-gradient-descent-algorithm-through-golf-amit-mittal\/), I have explained the concept in layman's language using an example of Golf.","55a19db9":"## 3. Defining Gradient Descent Function","ed14e539":"Now, I have defined function `ComputeCost` which computes the Mean Squared Errors, which will be optimized by gradient descent algorithm.","a856002e":"We can clearly see from above grap that, gradient descent algorithm converged after somewhere 275 iterations.","b75bf3a8":"Here,I have defined a function `FeatureScaling` which maps the value of numeric data in a column between -1 to 1, this is also called as `Standard Scalar`.\n\nThe purpose of scaling the columns (variables) is to have values of every column in same range. It helps in converging the Gradient Descent Algorithm faster also. ","9f7fb025":"Since, I will be defining function to perform gradient descent, so we will be required to add a column of `1's` in X.","b453f943":"Now, we will use the above function to minimize the `Mean Squared Error`and find the optimal value of theta's to get the best line.","e92933b3":"Now we will visualize, how gradient descent algoroithm helps to find the best line which describes linear relationship between how money spent on `TV` advertisements and impact `Sales` (by minimizing errors).\nSince, it is not possible to show how theta changes after every iteration. Here, I have shown changes after every 30th iteration.\n\n**We can see that how `intercept` and `slope` changes after every 30th iteration.**","fd6d5444":"# Understanding the Gradient Descent Algorithm","497943e9":"### End Note:\nI thank, Andrew N.G. for his Machine Learning course on Coursera, which helped me to write those functions","bd66a37a":"## 2. Feature Scaling","4845479b":"## 4. Visualizing the Results","234ed958":"## Content\n1. Importing Libraries and Loading Dataset\n2. Feature Scaling\n3. Defining Gradient Descent Function\n4. Visualizing the Results\n5. Conclusion","d6b671a0":"## 1. Importing Libraries and Loading Dataset","2bd39a15":"## 5. Conclusion","d4b46c19":"For the learning purpose, we will consider only one independent variable `TV`. It will also help in visualizing the results easily. Reason to consider `TV` as independent variable is because it has very good positive correlation of `0.90` with dependent varaiable `Sales`.","02f209e0":"As you we can see above, how `Gradient Descent Algorithm` helped to minimize the `Mean Squared Error` and helped to find the Best line which can help in prediciting `Sales` for a company, if we are given how mich they spend on `TV` advertisements.\n\nThis notebook delivered, how the algorithm works in attaining the best fit line for the dataset. Gradient Descent is algorithm is very popular algorithm used in Machine Learning model to minimize the errors. You can read about it more from other sourses in order to learn better.\n\nI hope you liked the idea of explaining the concept, if any feedback, please type in comment sections.\nStill if you haven't read the article, click [here](https:\/\/www.linkedin.com\/pulse\/understanding-gradient-descent-algorithm-through-golf-amit-mittal\/).\n\nMany Thanks :)"}}