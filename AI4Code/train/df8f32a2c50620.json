{"cell_type":{"d798de04":"code","be06a602":"code","e0c03495":"code","a462a778":"code","a2695e7f":"code","bf8272c1":"code","74b4cd78":"code","822df53b":"code","6c3c0de3":"code","690da1d6":"code","120b20c7":"code","3beb4836":"code","cf52aa09":"code","7ff7df14":"code","f968d31b":"code","bc0a1b82":"code","996c417e":"code","995708a4":"code","2af49cff":"code","85d5821c":"code","72dd16f9":"code","439aa162":"code","17da9fc4":"code","368db95e":"code","73f75bde":"code","bd185a67":"code","8dce55c8":"code","cd533136":"markdown"},"source":{"d798de04":"import pandas as pd\nimport seaborn as sns\nimport numpy as np\n\nimport matplotlib\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n\ndef warn(*args, **kwargs):\n    pass\nimport warnings\nwarnings.warn = warn\n\n#LABEL ENCODER\nfrom sklearn.preprocessing import LabelEncoder\nencoder = LabelEncoder()\n\nfrom sklearn.neighbors import KNeighborsClassifier\n\nfrom sklearn.utils import shuffle\n\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import precision_score, recall_score, f1_score\n\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\n\nfrom math import sqrt\nimport math\nfrom sklearn.metrics import classification_report\n\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.svm import SVC\nfrom sklearn.preprocessing import StandardScaler\nfrom mlxtend.plotting import plot_decision_regions\n\nfrom sklearn import metrics\nfrom sklearn.model_selection import cross_validate\n\n\nfrom sklearn.tree import DecisionTreeClassifier \n\n\nfrom sklearn.tree import export_graphviz\nfrom sklearn.externals.six import StringIO  \nfrom IPython.display import Image  \nimport pydotplus\n\nfrom sklearn.metrics import accuracy_score","be06a602":"#Function for drawing category\n\n\ndef draw(name, restrict=0):\n    category_count = data[name].value_counts()\n    if restrict > 0:\n        category_count= category_count[:restrict]\n    plt.figure(figsize=(10,5))\n    sns.barplot(category_count.index, category_count.values, alpha=0.8)\n    plt.ylabel('Number of '+name, fontsize=12)\n    plt.xlabel(name, fontsize=5)\n    plt.show()\n\n#DOWNLOAD THE DATA\ndata = pd.read_csv('https:\/\/query.data.world\/s\/txomiyjwpcnn3debgfyk5sshdiasmh')\n\n### DRAW BEFORE THE DROP ####\n\ndraw(\"Age\")\ndraw(\"Demographic information\")\ndraw(\"Status\")\n\n### DROP FEATURES ###\ndata = data.drop([0])\ndata = data.drop(data.columns[[4,5,7,8,10,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,62,63]], axis=1)\ndata = data.dropna()\n\n###AFTER feature extraction our age scale between 55-75 age\ndraw(\"Age\")\ndraw(\"Demographic information\")\ndraw(\"Status\")\n\n## FEATURE SCALING ##\ndata['Age'].unique()\ndata['Age'] = data['Age'].apply(lambda x: int(x))\nprint(data['Demographic information'].unique())\ndata[\"Demographic information\"] = encoder.fit_transform(data[\"Demographic information\"])\nprint(data['Tremor'].unique())\ndata[\"Tremor\"] = encoder.fit_transform(data[\"Tremor\"])\nprint(data['Medication'].unique())\ndata[\"Medication\"] = encoder.fit_transform(data[\"Medication\"])\nprint(data['Medication.3'].unique())\ndata[\"Medication.3\"] = encoder.fit_transform(data[\"Medication.3\"])\nprint(data['Medication.5'].unique())\ndata[\"Medication.5\"] = encoder.fit_transform(data[\"Medication.3\"])\n\ndef toFloat(name):\n    data[name] = data[name].apply(lambda x: float(x))\ntoFloat(\"Speech examination: speaking task of monologue\")\ntoFloat(\"Speech examination: speaking task of monologue.1\")\ntoFloat(\"Speech examination: speaking task of monologue.2\")\ntoFloat(\"Speech examination: speaking task of monologue.3\")\ntoFloat(\"Speech examination: speaking task of monologue.4\")\ntoFloat(\"Speech examination: speaking task of monologue.5\")\ntoFloat(\"Speech examination: speaking task of monologue.6\")\ntoFloat(\"Speech examination: speaking task of monologue.7\")\ntoFloat(\"Speech examination: speaking task of monologue.8\")\ntoFloat(\"Speech examination: speaking task of monologue.9\")\n\n\n\n#print(data[name].unique())\ntoFloat('Speech examination: speaking task of reading passage')\ntoFloat('Speech examination: speaking task of reading passage.1')\ntoFloat('Speech examination: speaking task of reading passage.2')\ntoFloat('Speech examination: speaking task of reading passage.3')\ntoFloat('Speech examination: speaking task of reading passage.4')\ntoFloat('Speech examination: speaking task of reading passage.5')\ntoFloat('Speech examination: speaking task of reading passage.6')\ntoFloat('Speech examination: speaking task of reading passage.7')\ntoFloat('Speech examination: speaking task of reading passage.8')\ntoFloat('Speech examination: speaking task of reading passage.9')\ntoFloat('Speech examination: speaking task of reading passage.10')\ntoFloat('Speech examination: speaking task of reading passage.11')\n\n\nprint(data['Status'].unique())\ndata[\"Status\"] = encoder.fit_transform(data[\"Status\"])\n\n\nprint(data['History '].unique())\ndata[\"History \"] = encoder.fit_transform(data[\"History \"])\n\n\n","e0c03495":"print(data['Status'].value_counts())","a462a778":"corr = data.corr()\ncorr['Status']","a2695e7f":"## SPLIT X AND Y \ndata = shuffle(data)\nx,y = data.loc[:,data.columns != 'Status'], data.loc[:,'Status']","bf8272c1":"## Function for test train split\ndef splitTestTrain():\n    return train_test_split(x, y, test_size = 0.2, random_state = 42)\n\n## Function for single precision, recall, f1 etc\n\ndef scorer(y_test,prediction):\n    print('Confusion Matrix')\n    print(confusion_matrix(y_test, prediction))\n    print()\n    print(classification_report(y_test,prediction))\n\n## Cross validation x,y \n\ndef cross_val_multiple(func):\n    print('\\033[91mK FOLD CROSS VALIDATION\\033[0m')\n    accuracy = cross_val_score(func, x, y, cv=5, scoring=\"accuracy\")\n    precision = cross_val_score(func, x, y, cv=5, scoring=\"precision\")\n    recall = cross_val_score(func, x, y, cv=5, scoring=\"recall\")\n    f1 = cross_val_score(func, x, y, cv=5, scoring=\"f1\")\n    print('accuracy_score: ',np.mean(precision))\n    print('precision_score:',np.mean(precision))\n    print('recall_score:',np.mean(recall))\n    print('f1_score:',np.mean(f1))\n\n\n## Cros val for svm\ndef cross_val_for_svm(func):\n    x_train, x_test, y_train, y_test = splitTestTrain()\n    \n    sc = StandardScaler()\n    x_scaled = sc.fit_transform(x)\n    x_test_scaled = sc.transform(x_test)\n    x_scaled = sc.transform(x)\n    \n    pca = PCA(n_components=2)\n    pca.fit(x_train_scaled)\n    \n    x_pca_train = pca.transform(x_train_scaled)\n    x_pca_test = pca.transform(x_test_scaled)\n    x_pca =  pca.transform(x_scaled)\n    \n    print('\\033[91mK FOLD CROSS VALIDATION\\033[0m')\n    accuracy = cross_val_score(func, x_pca, y, cv=5, scoring=\"accuracy\")\n    precision = cross_val_score(func, x_pca, y, cv=5, scoring=\"precision\")\n    recall = cross_val_score(func, x_pca, y, cv=5, scoring=\"recall\")\n    f1 = cross_val_score(func, x_pca, y, cv=5, scoring=\"f1\")\n    print('accuracy_score: ',np.mean(accuracy))\n    print('precision_score:',np.mean(precision))\n    print('recall_score:',np.mean(recall))\n    print('f1_score:',np.mean(f1))\n    \n\n##DRAW Logistic Regression\n\ndef sigmoid(x):\n    a = []\n    for item in x:\n        a.append(1\/(1+math.exp(-item)))\n    return a\n\ndef bound(x):\n    a = []\n    for item in x:\n        a.append(0.5)\n    return a\n  \ndef drawLGR(y_proba, y_test):\n    scaler = MinMaxScaler(feature_range=(-5, 5))\n    a = scaler.fit_transform(np.array([x_test[\"Speech examination: speaking task of reading passage\"].tolist()]).T)\n    pp = np.arange(-5, 5, 0.01)\n    sig = sigmoid(pp)\n    bo = bound(pp)\n    plt.plot(pp,sig)\n    plt.plot(pp,bo)\n    i = 0\n    for z,o in y_proba > 0.5:\n        real = y_test.tolist()[i]\n        ft = a[i]\n        if real == 1:\n            c = \"r\"\n        if real == 0:\n            c = \"g\"\n        if z == True:\n            p = 0\n        if z == False:\n            p = 1\n        plt.scatter(ft, p, s=10, c=c)\n    i+=1\n    plt.show()\n","74b4cd78":"'''\nKNN TEST WITH K = 3 \n'''\n\nx_train, x_test, y_train, y_test = splitTestTrain()\nknn = KNeighborsClassifier(n_neighbors = 3)\nknn.fit(x_train,y_train)\nprediction = knn.predict(x_test)\nprint('With KNN (K=3) accuracy is: ',accuracy_score(y_test,prediction))\nscorer(y_test, prediction)\n##K FOLD CROS VALIDATION\ncross_val_multiple(knn)\n\n","822df53b":"##KNN GRID SEARCH\nx_train, x_test, y_train, y_test = splitTestTrain()\nk_range = list(range(1, 31))\nparam_grid = dict(n_neighbors=k_range)\n\nscoring = ['accuracy','f1','precision','recall']\ngrid = GridSearchCV(knn, param_grid, cv=5, scoring=scoring, refit='recall')\n\nbest_model = grid.fit(x_train, y_train)\nprint('\\033[91mBest estimator paramaters\\033[0m')\nprint(best_model.best_estimator_.get_params())\n\npd.DataFrame(grid.cv_results_).to_csv('export.csv')\ny_pred = best_model.best_estimator_.predict(x_test)\n\nscorer(y_test, prediction)\ncross_val_multiple(best_model.best_estimator_)","6c3c0de3":"#Naive bayes there is a no hyperparamters for navie bayes\nx_train, x_test, y_train, y_test = splitTestTrain()\nnb = GaussianNB()\nnb.fit(x_train,y_train)\nprediction = nb.predict(x_test)\n\nprint('Naive bayes Accuracy is: ',accuracy_score(y_test,prediction))\nscorer(y_test, prediction)\ncross_val_multiple(nb)\n","690da1d6":"## LOGISTIC REGRESSION\nx_train, x_test, y_train, y_test = splitTestTrain()\nlr = LogisticRegression()\nlr.fit(x_train,y_train)\nprediction = lr.predict(x_test)\nprint('Default LOGISTIC REGRESSION accuracy is: ',accuracy_score(y_test,prediction))\nscorer(y_test, prediction)\n\n##K FOLD CROS VALIDATION\ncross_val_multiple(lr)\n","120b20c7":"##Logistic regression grid search\nx_train, x_test, y_train, y_test = splitTestTrain() \ngrid={\"C\":np.logspace(-3,3,7), \"penalty\":[\"l1\",\"l2\"]}# l1 lasso l2 ridge\nlogreg = LogisticRegression()\nlogreg_cv = GridSearchCV(logreg,grid,cv=10)\nlogreg_cv.fit(x_train,y_train)\n\nprint(\"tuned hpyerparameters :(best parameters) \",logreg_cv.best_params_)\nprint(\"accuracy :\",logreg_cv.best_score_)","3beb4836":"## Train with best parameters\nx_train, x_test, y_train, y_test = splitTestTrain()\nlr = LogisticRegression(C=0.01,penalty=\"l1\")\nlr.fit(x_train,y_train)\nprediction = lr.predict(x_test)\nprint('Default LOGISTIC REGRESSION accuracy is: ',accuracy_score(y_test,prediction))\nscorer(y_test, prediction)\n\n##K FOLD CROS VALIDATION\ncross_val_multiple(lr)","cf52aa09":"## SVM WITHOUT FEATURE SCALING\nx_train, x_test, y_train, y_test = splitTestTrain()\n\nclf = SVC()\nclf.fit(x_train, y_train)\nprediction = clf.predict(x_test)\n\nprint('SVM accuracy is: ',accuracy_score(y_test,prediction))\nscorer(y_test, prediction)\ncross_val_multiple(clf)","7ff7df14":"## SVM WITH FEATURE SCALING WITH Linear Kernel\n\nx_train, x_test, y_train, y_test = splitTestTrain()\n\n##Scale the feautres\nsc = StandardScaler()\nx_train_scaled = sc.fit_transform(x_train)\nx_test_scaled = sc.transform(x_test)\n\npca = PCA(n_components=2)\npca.fit(x_train_scaled)\n\nx_pca_train = pca.transform(x_train_scaled)\nx_pca_test = pca.transform(x_test_scaled)\n\nclf = SVC(kernel='linear')\nclf.fit(x_pca_train, y_train)\n\nprediction = clf.predict(x_pca_test)\n\nprint('SVM accuracy is: ',accuracy_score(y_test,prediction))\nscorer(y_test, prediction)\n\n\ncross_val_for_svm(clf)\n","f968d31b":"# Plotting decision regions\nplot_decision_regions(x_pca_train, y_train.as_matrix(), clf=clf, legend=2)\n\n# Adding axes annotations\nplt.xlabel('Scaled X ')\nplt.ylabel('Scaled Y')\nplt.title('Linear svm Train')\nplt.show()","bc0a1b82":"# Plotting decision regions\nplot_decision_regions(x_pca_test, y_test.as_matrix(), clf=clf, legend=2)\n# Adding axes annotations\nplt.xlabel('Scaled X ')\nplt.ylabel('Scaled Y')\nplt.title('Linear svm Test')\nplt.show()","996c417e":"Cs = [0.001, 0.01, 0.1, 1, 10]\ngammas = [0.001, 0.01, 0.1, 1]\nparam_grid = {'C': Cs, 'gamma' : gammas}\ngrid_search = GridSearchCV(SVC(kernel='rbf'), param_grid, cv=5)\n\ngrid_search.fit(x_pca_train, y_train)\ngrid_search.best_params_\nprint(\"tuned hpyerparameters :(best parameters for svm kernel=rbf) \",grid_search.best_params_)","995708a4":"clf = SVC(kernel='rbf', C=10, gamma=1)\nclf.fit(x_pca_train, y_train)\n\nprediction = clf.predict(x_pca_test)\n\nprint('SVM accuracy is: ',accuracy_score(y_test,prediction))\nscorer(y_test, prediction)\n\ncross_val_for_svm(clf)\n","2af49cff":"# Plotting decision regions\nplot_decision_regions(x_pca_train, y_train.as_matrix(), clf=clf, legend=2)\n\n# Adding axes annotations\nplt.xlabel('Scaled X ')\nplt.ylabel('Scaled Y')\nplt.title('Rbf svm Train')\nplt.show()","85d5821c":"# Plotting decision regions\nplot_decision_regions(x_pca_test, y_test.as_matrix(), clf=clf, legend=2)\n# Adding axes annotations\nplt.xlabel('Scaled X ')\nplt.ylabel('Scaled Y')\nplt.title('Rbf svm Test')\nplt.show()","72dd16f9":"##DesicionTree\n\nx_train, x_test, y_train, y_test = splitTestTrain()\nclf = DecisionTreeClassifier()\n\n# Train Decision Tree Classifer\nclf = clf.fit(x_train,y_train)\n#Predict the response for test dataset\nprediction = clf.predict(x_test)\n# Model Accuracy, how often is the classifier correct?\n#print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\nprint('Default DecisionTree is: ',accuracy_score(y_test,prediction))\nscorer(y_test, prediction)\n\n##K FOLD CROS VALIDATION\ncross_val_multiple(clf)\n","439aa162":"dot_data = StringIO()\nexport_graphviz(clf, out_file=dot_data,  \n                filled=True, rounded=True,\n                special_characters=True)\ngraph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \nImage(graph.create_png())","17da9fc4":"##DesicionTree\n\nx_train, x_test, y_train, y_test = splitTestTrain()\nclf = DecisionTreeClassifier(criterion='entropy')\n\n# Train Decision Tree Classifer\nclf = clf.fit(x_train,y_train)\n#Predict the response for test dataset\nprediction = clf.predict(x_test)\n# Model Accuracy, how often is the classifier correct?\n#print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\nprint('Default DecisionTree is: ',accuracy_score(y_test,prediction))\nscorer(y_test, prediction)\n\n##K FOLD CROS VALIDATION\ncross_val_multiple(clf)\n","368db95e":"dot_data = StringIO()\nexport_graphviz(clf, out_file=dot_data,  \n                filled=True, rounded=True,\n                special_characters=True)\ngraph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \nImage(graph.create_png())","73f75bde":"## GRID SEARCH DECISION TREE\nx_train, x_test, y_train, y_test = splitTestTrain()\n\ndt = DecisionTreeClassifier()\nparam_grid = [{'max_depth':np.arange(3, 20),\n              'min_samples_leaf':[1, 5, 10, 20, 50, 100]}]\nscoring = ['accuracy','f1','precision','recall']\ngrid = GridSearchCV(dt, param_grid, cv=5, scoring=scoring, refit='recall')\n\nbest_model = grid.fit(x_train, y_train)\nprint(best_model.best_estimator_.get_params())\n","bd185a67":"##DesicionTree\n\nx_train, x_test, y_train, y_test = splitTestTrain()\nclf = DecisionTreeClassifier(criterion='gini',max_depth=3, min_samples_leaf=1, min_samples_split=2,presort=False,splitter='best' )\n\n# Train Decision Tree Classifer\nclf = clf.fit(x_train,y_train)\n#Predict the response for test dataset\nprediction = clf.predict(x_test)\n# Model Accuracy, how often is the classifier correct?\n#print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\nprint('Default DecisionTree is: ',accuracy_score(y_test,prediction))\nscorer(y_test, prediction)\n\n##K FOLD CROS VALIDATION\ncross_val_multiple(clf)\n","8dce55c8":"dot_data = StringIO()\nexport_graphviz(clf, out_file=dot_data,  \n                filled=True, rounded=True,\n                special_characters=True)\ngraph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \nImage(graph.create_png())","cd533136":"**IMPORT THE ALL LIBRARIES**"}}