{"cell_type":{"99a6b07b":"code","f27ddbc0":"code","417a2d45":"code","c4cecb4e":"code","702e31b0":"code","e0749cf0":"code","384e2273":"code","d38696f6":"code","22197402":"code","893eba8f":"code","0ebadf80":"code","0dc772b6":"code","f05de48f":"code","5aa3f1e2":"code","b0d1db53":"code","cb407300":"code","676e2f31":"code","efd0fe72":"code","4834003d":"code","4bf8de3b":"code","98ae5019":"code","def8475d":"code","80b727bd":"code","6df9efc5":"code","da7882b4":"code","18261ae5":"code","4b964e7f":"code","dcf2d064":"code","f3f7da27":"code","21d2ef62":"code","88f47335":"code","b77a735d":"code","f5ab9d5a":"code","28b25262":"code","501ac570":"code","4bf8cdfd":"code","13b729b4":"code","db275ccb":"code","a263f76f":"code","46e752b4":"code","b9650294":"code","bd74344b":"code","0669df3b":"code","f617191f":"code","ea338c55":"code","d3802658":"code","2a1433b8":"code","9eecc3a6":"code","0fb7670b":"code","014ddf1c":"code","da5a3ff8":"code","9d5a74f7":"code","95d6bc25":"code","631a59a0":"code","16d98489":"code","bf52f032":"markdown","e9511fdf":"markdown","bc2ce344":"markdown"},"source":{"99a6b07b":"import numpy as np \nimport pandas as pd\n\nimport seaborn as sns\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import StandardScaler\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport torch\nfrom torch import nn\n","f27ddbc0":"!gdown https:\/\/drive.google.com\/uc?id=13wZWQQyoERqGo8mwy8Mn3MID-Oc_d6F9","417a2d45":"df_train = pd.read_csv(\"fifa2021_training.csv\")","c4cecb4e":"pd.set_option('display.max_columns', None)\ndf_train.head(5)","702e31b0":"df_train.shape","e0749cf0":"df_train.dtypes","384e2273":"df_train.isnull().sum()","d38696f6":"!gdown https:\/\/drive.google.com\/uc?id=1Ikf2k53PqXHqmVxCjyCTDQRObPdJ2qXP","22197402":"df_test = pd.read_csv(\"fifa2021_test.csv\")","893eba8f":"pd.set_option('display.max_columns', None)\ndf_test.head(5)","0ebadf80":"df_train.shape","0dc772b6":"df_test.shape","f05de48f":"df_test.isnull().sum()    ","5aa3f1e2":"# Las columnas son las mismas\ndf_train.columns[0:-1] == df_test.columns","b0d1db53":"# ESTA CONCATENACI\u00d3N LA REALIZO AQU\u00cd PARA USAR LA FUNCI\u00d3N pd.get_dummies de pandas QUE NO GUARDA EN MEMORIA COMO FUE LA CODIFICACI\u00d3N,\n# Y SI NO LO HICIERA AHORA, Y QUISIERA APLICARLA LUEGO, PODR\u00cdA PASAR QUE A ALGUNA DE LOS FEATURES DE TEST NO TUVIERA ALGUNO DE LOS VALORES POSIBLES \n# Y POR LO TANTO LUEGO ME FALTAR\u00cdAN COLUMNAS.\n# OTRA ALTERNATIVA SER\u00cdA USAR ALGUNA DE SKLEARN QUE TIENE LOS M\u00c9TODOS FIT(), PERO ESTOY USANDO EL EJEMPLO DEL LIBRO QUE LO HACE AS\u00cd\ndf_data = pd.concat((df_train, df_test), ignore_index=True)\ndf_data.head()","cb407300":"df_data.index","676e2f31":"# Los concaten\u00f3 y como df_test no tiene la columna \"Position\" la llena con NaN\ndf_data.iloc[13920:13925].Position","efd0fe72":"# OJO QUE DE NINGUNA MUJER SE TIENE DATOS para \"Value\" ni \"Wage\"\nprint(df_data[df_data.Sex == \"Female\"].Value.size)\nprint(df_data[df_data.Sex == \"Female\"].Value.isnull().sum())\nprint(df_data[df_data.Sex == \"Female\"].Wage.size)\nprint(df_data[df_data.Sex == \"Female\"].Wage.isnull().sum())","4834003d":"df_data.shape","4bf8de3b":"df_data_drop = df_data.drop(columns=['ID', 'Name', 'BirthDate','Club',\n                                          'Club_KitNumber','Club_JoinedClub','Club_ContractLength'])","98ae5019":"df_data_drop.loc[(df_data_drop.Value.isnull()), 'Value']","def8475d":"df_train.Position.unique()","80b727bd":"MID_mask = df_data_drop['Position'] == 'MID'\nMID_value_media = df_data_drop[MID_mask].Value.mean() \nMID_wage_media = df_data_drop[MID_mask].Wage.mean() \nprint(MID_value_media)\nprint(MID_wage_media)","6df9efc5":"DEF_mask = df_data_drop['Position'] == 'DEF'\nDEF_value_media = df_data_drop[DEF_mask].Value.mean() \nDEF_wage_media = df_data_drop[DEF_mask].Wage.mean() \nprint(DEF_value_media)\nprint(DEF_wage_media)\n\n","da7882b4":"GK_mask = df_data_drop['Position'] == 'GK'\nGK_value_media = df_data_drop[GK_mask].Value.mean() \nGK_wage_media = df_data_drop[GK_mask].Wage.mean() \nprint(GK_value_media)\nprint(GK_wage_media)","18261ae5":"FWD_mask = df_data_drop['Position'] == 'FWD'\nFWD_value_media = df_data_drop[FWD_mask].Value.mean() \nFWD_wage_media = df_data_drop[FWD_mask].Wage.mean() \nprint(FWD_value_media)\nprint(FWD_wage_media)","4b964e7f":"# LOS DDATOS ANTERIORES LOS DEJO S\u00d3LO COMO INFORMATIVO\n# HAC\u00cdA ESTO DE COMPLETAR LAS MEDIAS DEPENDIENDO DE LA POSICI\u00d3N, PERO CONSULT\u00c9 Y ME INDICARON QUE ESTO HACE QUE LA RED \n# NO APRENDA.. Y PEOR PARA LAS MUJERES.\n# CUANDO NO SE TIENEN DATOS, ES MEJOR LLENARLOS CON LA MEDIA EN ESTE CASO, PARA QUE AUQNUE TENGAN POSICIONES DIFERENTES, \n# ESE RUIDO HACE QUE APRENDA MEJOR.","dcf2d064":"# S\u00f3lo quedan nulos los Position de Test y en los Values de los de Test que ya le faltaban de antes\ndf_data_drop.isnull().sum()","f3f7da27":"df_data_drop.loc[0:13920]","21d2ef62":"# VUELVO A LA VERSI\u00d3N ORIGINAL DONDE REEMPLAZO TODOS CON LA MEDIA. TAMBI\u00c9N LO ESTOY HACIENDO EN LOS DE TEST PORQUE ESTAN TODO JUNTO\nvalue_media = df_data_drop.loc[0:13920].Value.mean()    # USO LA MEDIA S\u00d3LO DE LOS DE ENTRENAMIENTO\nwage_media = df_data_drop.loc[0:13920].Wage.mean()      # USO LA MEDIA S\u00d3LO DE LOS DE ENTRENAMIENTO\n\nprint(value_media)\nprint(wage_media)\n\ndf_data_drop.loc[(df_data_drop.Value.isnull()), 'Value'] = value_media\ndf_data_drop.loc[(df_data_drop.Wage.isnull()), 'Wage'] = wage_media\n","88f47335":"# S\u00f3lo quedan nulos los Position de Test \ndf_data_drop.isnull().sum()","b77a735d":"# hacer los dummies antes de separar en TRAIN y TEST\ndf_data_dummies = pd.get_dummies(df_data_drop, drop_first = False)","f5ab9d5a":"#Evaluando duplicados en datos de entrenamiento\nduplicates_mask = df_data_dummies.iloc[:13921].duplicated(keep=\"first\")\nprint(\"registros duplicados en df_train: \", any(duplicates_mask))\nprint(\"cantidad de registros duplicados en df_train: \", duplicates_mask.sum())\n\n##### hay que eliminar los duplicados\ndf_train_temp = df_data_dummies[:13921].drop_duplicates(keep=\"first\")\n","28b25262":"df_train_temp.columns","501ac570":"# Generando los diferentes grupo de datos:\n\n# df_X_train\ndf_X_train = df_train_temp.iloc[:13921].drop(columns=['Position_DEF', 'Position_FWD', 'Position_GK','Position_MID'])\n\n# df_X_test\ndf_X_test = df_data_dummies.iloc[13921:].drop(columns=['Position_DEF', 'Position_FWD', 'Position_GK','Position_MID'])\n\n# df_Y_train\n# Me quedo con las columna \"Position\" evaluadas en los \u00edndices que me quedaron lugo de eliminar los duplicados\ndf_Y_train_temp = df_data_drop.iloc[df_X_train.index]['Position']\n\n# NO HAGO LAS DUMMIES DE LAS \"Y\" PORQUE LA FUNCI\u00d3N d2l.train_ch3() NO ESPERA ESO, SINO RNOS DE ETIQUETAS\n#df_Y_train =  pd.get_dummies(df_Y_train_temp, drop_first = False)\n\n# POR LO TANTO, SE REEMPLAZA LA ETIQUETA POR UN NRO.\nprint(\"df_Y_train_temp.unique(): \", df_Y_train_temp.unique())\n\nlabels_position = df_Y_train_temp.unique().tolist()\nmapping = dict( zip(labels_position,range(len(labels_position))))\ndf_Y_train = pd.DataFrame(df_Y_train_temp).replace({'Position': mapping})\n\nprint(\"df_Y_train.unique(): \", df_Y_train.Position.unique())\n","4bf8cdfd":"print(df_X_train.shape)\nprint(df_Y_train.shape)\nprint((df_X_train.index == df_Y_train.index).sum())\nprint(df_X_test.shape)","13b729b4":"# NORMALIZACI\u00d3N\nscaler = StandardScaler()\n\nprint(scaler.fit(df_X_train))\n\nX_train_norm = scaler.fit_transform(df_X_train)\n\ndf_X_train_norm = pd.DataFrame(X_train_norm, index = df_X_train.index, columns=df_X_train.columns)\n\n# A Test s\u00f3lo TRANSFORM()\nX_test_norm = scaler.transform(df_X_test)\n\ndf_X_test_norm = pd.DataFrame(X_test_norm, index = df_X_test.index, columns=df_X_test.columns)\n","db275ccb":"# df_X_train_norm, df_X_test_norm , df_Y_train\n# t_X_train      , t_X_test       , t_Y_train\n\nt_X_train = torch.tensor(df_X_train_norm.values).float()\nt_Y_train = torch.tensor(df_Y_train.values).squeeze()\n\nt_X_test = torch.tensor(df_X_test_norm.values).float()","a263f76f":"from torch.utils import data\n!pip install d2l # USARLO EN COLAB\nfrom d2l import torch as d2l","46e752b4":"def accuracy(y_hat, y): \n    \"\"\"Compute the number of correct predictions.\"\"\"\n    if len(y_hat.shape) > 1 and y_hat.shape[1] > 1:\n        y_hat = y_hat.argmax(axis=1)\n    cmp = y_hat.type(y.dtype) == y\n    return float(cmp.type(y.dtype).sum())","b9650294":"def evaluate_accuracy(net, features, labels):  \n    \"\"\"Compute the accuracy for a model on a dataset.\"\"\"\n    if isinstance(net, torch.nn.Module):\n        net.eval()  # Set the model to evaluation mode\n    metric = Accumulator(2)  # No. of correct predictions, no. of predictions\n    X = features\n    y = labels\n    metric.add(accuracy(net(X), y), y.numel())\n    return metric[0] \/ metric[1]","bd74344b":"class Accumulator:  \n    \"\"\"For accumulating sums over `n` variables.\"\"\"\n    def __init__(self, n):\n        self.data = [0.0] * n\n\n    def add(self, *args):\n        self.data = [a + float(b) for a, b in zip(self.data, args)]\n\n    def reset(self):\n        self.data = [0.0] * len(self.data)\n\n    def __getitem__(self, idx):\n        return self.data[idx]","0669df3b":"def train(net, train_features, train_labels, test_features, test_labels,\n          num_epochs, learning_rate, weight_decay, batch_size):\n    train_ls, test_ls = [], []\n    train_iter = d2l.load_array((train_features, train_labels), batch_size)\n# -----------------------------------------\n    # Usando: The Adam optimization algorithm is used here\n    optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate,\n                                weight_decay=weight_decay)\n# -----------------------------------------\n    # Usando SGD\n    # optimizer = torch.optim.SGD(net.parameters(), lr=learning_rate)\n# -----------------------------------------\n    for epoch in range(num_epochs):\n        for X, y in train_iter:\n            optimizer.zero_grad()\n            l = loss(net(X), y)\n            l.backward()\n            optimizer.step()\n        train_ls.append(evaluate_accuracy(net, train_features, train_labels))\n        if test_labels is not None:\n            test_ls.append(evaluate_accuracy(net, test_features, test_labels))\n    return train_ls, test_ls","f617191f":"def get_k_fold_data(k, i, X, y):\n    assert k > 1\n    fold_size = X.shape[0] \/\/ k\n    X_train, y_train = None, None\n    for j in range(k):\n        idx = slice(j * fold_size, (j + 1) * fold_size)\n        X_part, y_part = X[idx, :], y[idx]\n        if j == i:\n            X_valid, y_valid = X_part, y_part\n        elif X_train is None:\n            X_train, y_train = X_part, y_part\n        else:\n            X_train = torch.cat([X_train, X_part], 0)\n            y_train = torch.cat([y_train, y_part], 0)\n    return X_train, y_train, X_valid, y_valid","ea338c55":"from sklearn.metrics import balanced_accuracy_score","d3802658":"df_train.Position.hist()","2a1433b8":"print(MID_mask.sum())\nprint(DEF_mask.sum())\nprint(GK_mask.sum())\nprint(FWD_mask.sum())\ntotal = MID_mask.sum() + DEF_mask.sum()+ GK_mask.sum() + FWD_mask.sum()\nprint(MID_mask.sum()\/total)\nprint(DEF_mask.sum()\/total)\nprint(GK_mask.sum()\/total)\nprint(FWD_mask.sum()\/total)\nprint(total)\nt_pesos = [(MID_mask.sum()\/total), (DEF_mask.sum()\/total), (GK_mask.sum()\/total), (FWD_mask.sum()\/total)]\nt_pesos","9eecc3a6":"# FUNCI\u00d3N PARA ENTRENAR EL MODELO y VALIDAR CON K-FOLD\n\ndef k_fold(k, X_train, y_train, num_epochs, learning_rate, weight_decay,\n           batch_size):\n    train_l_sum, valid_l_sum = 0, 0\n    for i in range(k):\n        data = get_k_fold_data(k, i, X_train, y_train)\n        net = get_net()\n        net.apply(init_weights);  \n        train_ls, valid_ls = train(net, *data, num_epochs, learning_rate,\n                                   weight_decay, batch_size)\n        train_l_sum += train_ls[-1]\n        valid_l_sum += valid_ls[-1]\n        if i == 0:\n            d2l.plot(list(range(1, num_epochs + 1)), [train_ls, valid_ls],\n                     xlabel='epoch', ylabel='CrossEntropyLoss', xlim=[1, num_epochs],\n                     legend=['train', 'valid'], yscale='linear')\n        print(f'fold {i + 1}, train accuracy {float(train_ls[-1]):f}, '\n              f'valid accuracy {float(valid_ls[-1]):f}')\n        \n        ######emi########\n        train_preds_k = net(X_train).argmax(axis=1).detach().numpy()\n        print(f'train balanced_accuracy_score {float(balanced_accuracy_score(y_train.detach().numpy(), train_preds_k)):f}')\n        ######emi########\n    return train_l_sum \/ k, valid_l_sum \/ k","0fb7670b":"# FUNCI\u00d3N PARA ENTRENAR EL MODELO CON TODOS LOS DATOS DE TRAIN\n\ndef train_and_pred(train_features, test_features, train_labels, test_data, labels_position,\n                   num_epochs, lr, weight_decay, batch_size):\n    net = get_net()\n    net.apply(init_weights);\n    train_ls, _ = train(net, train_features, train_labels, None, None,\n                        num_epochs, lr, weight_decay, batch_size)\n    d2l.plot(np.arange(1, num_epochs + 1), [train_ls], xlabel='epoch',\n             ylabel='accuracy', xlim=[1, num_epochs], yscale='linear')\n    print(f'train accuracy {float(train_ls[-1]):f}')\n    \n    ######emi########\n    train_preds = net(train_features).argmax(axis=1).detach().numpy()\n    print(f'train balanced_accuracy_score {float(balanced_accuracy_score(train_labels, train_preds)):f}')\n    ######emi########\n    \n    # Apply the network to the test set\n    preds = net(test_features).argmax(axis=1).detach().numpy()\n    # Reformat it to export to Kaggle\n    test_data['Category'] = pd.Series(preds.reshape(1, -1)[0])    # ac\u00e1 le agrego las predicciones de Position a mi dataframe de test\n    mapping = dict( zip(range(len(labels_position)), labels_position))\n    test_data['Category']= pd.DataFrame(test_data['Category']).replace({'Category': mapping})\n    submission = pd.concat([test_data['ID'], test_data['Category']], axis=1)\n    #submission.to_csv('submission_f.csv', index=False)\n    return (submission)","014ddf1c":"# DEFINICI\u00d3N DEL MODELO\nloss = nn.CrossEntropyLoss()\n\nin_features = t_X_train.shape[1]\n\ndropout1 = 0\n\ndef get_net():\n    net = nn.Sequential(nn.Linear(in_features, 256), nn.ReLU(),\n                        # Add a dropout layer after the first fully connected layer\n                        #nn.Dropout(dropout1), nn.Linear(256, 256), nn.ReLU(),\n                        nn.Linear(256, 4))\n    return net\n\n\ndef init_weights(m):\n    if type(m) == nn.Linear:\n        nn.init.normal_(m.weight, std=0.01)\n\n#net.apply(init_weights);","da5a3ff8":"# SELECCI\u00d3N DEL MODELO con K-FOLD\nk, num_epochs, lr, weight_decay, batch_size = 3, 25, 0.5, 0, 1024\n# k, num_epochs, lr, batch_size = 4, 30, 0.1, 512\n\ntrain_l, valid_l = k_fold(k, t_X_train, t_Y_train, num_epochs, lr,\n                          weight_decay, batch_size)\nprint(f'{k}-fold validation: avg train accuracy: {float(train_l):f}, '\n      f'avg valid accuracy: {float(valid_l):f}')","9d5a74f7":"# ENTRENAMIENTO DEL MODELO CON TODOS LOS DATOS\ny_predicho = train_and_pred(t_X_train, t_X_test, t_Y_train, df_test, labels_position, \n                           num_epochs, lr, weight_decay, batch_size)\n","95d6bc25":"y_predicho","631a59a0":"#df_submission = pd.read_csv(\"submission_C.csv\")","16d98489":"#df_submission","bf52f032":"##### VER SI HAY UNA MEJOR MANERA DE COMPLETAR \"Value\" y \"Wage\" o SINO LAS ELIMINO EN EL PUNTO ANTERIOR.\n\nVer como Referencia: https:\/\/www.kaggle.com\/rk2802\/handling-missing-values-fifa-19","e9511fdf":"One Hot Encoding para las Featrues OBJECT\nVer la Correlaci\u00f3n a ver si alguna afecta\nEliminar columnas que no afectten a la predicci\u00f3n (Club_KitNumber \/ Club_JoinedClub \/  Club_ContractLength)\n","bc2ce344":"### Referencia D2L (4.10. Predicting House Prices on Kaggle)"}}