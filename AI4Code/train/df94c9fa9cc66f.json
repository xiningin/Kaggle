{"cell_type":{"3fae109e":"code","f16c7434":"code","77d4c54f":"code","0e835f87":"markdown","89199836":"markdown"},"source":{"3fae109e":"!pip install bs4","f16c7434":"from __future__ import division\nfrom bs4 import BeautifulSoup\nimport requests\nimport shutil\nimport os.path\nfrom tqdm.notebook import tqdm\nimport os\nimport json\n\nroot_path = '.'\nn_caption_pages = 15   #this number *15 is the total number of captions per template\nn_temp_pages = 200  #this number *15 is the total number of templates\nUerrors = 0\nos.makedirs(root_path, exist_ok=True)","77d4c54f":"for page in tqdm(range(1,n_temp_pages+1)):\n    if page == 1:\n        url = 'https:\/\/memegenerator.net\/memes\/popular\/alltime'\n    else:\n        url = 'https:\/\/memegenerator.net\/memes\/popular\/alltime\/page\/' + str(page)\n\n    r = requests.get(url)\n    soup = BeautifulSoup(r.text,'html.parser')\n    chars = soup.find_all(class_='char-img')\n    links = [char.find('a') for char in chars]\n    imgs = [char.find('img') for char in chars]\n    assert len(links) == len(imgs)\n    for j,img in enumerate(imgs):\n        img_url = img['src']\n        response = requests.get(img_url, stream=True)\n        temp_img_name = img_url.split('\/')[-1]\n        temp_img_path = os.path.join(root_path, temp_img_name)\n        with open(temp_img_path,'wb') as img_file:\n            shutil.copyfileobj(response.raw, img_file)\n        del response\n        \n        texts = []\n        for k in range(1,n_caption_pages+1):\n            if k == 1:\n                URL = 'https:\/\/memegenerator.net' + links[j]['href']\n            else:\n                URL = 'https:\/\/memegenerator.net' + links[j]['href'] + '\/images\/popular\/alltime\/page\/' + str(k)\n\n            R = requests.get(URL)\n            SOUP = BeautifulSoup(R.text,'html.parser')\n            CHARS = SOUP.find_all(class_='optimized-instance-container')\n            TEXTS = [(char.find(class_='optimized-instance-text0').get_text(),\n                      char.find(class_='optimized-instance-text1').get_text()) for char in CHARS]\n            TEXTS = list(set(TEXTS))\n            texts += TEXTS\n        json_name = temp_img_name.split('.')[0] + '.json'\n        json_path = os.path.join(root_path, json_name)\n        with open(json_path, 'w') as json_file:\n            json.dump(texts, json_file)\n        ","0e835f87":"# Scraper","89199836":"# Import, configure"}}