{"cell_type":{"dad833b9":"code","e436ffbc":"code","8f4c9fe9":"code","c7321471":"code","e0167eae":"code","64c3f68b":"code","8f71d499":"code","a46e8fc5":"code","aa4c5733":"code","f43a4256":"code","fcda0c15":"code","15ce1298":"code","7844bd7e":"code","c93eaddb":"code","6b0a96e2":"code","dfbbf282":"code","7ec4046e":"code","c6316766":"markdown","bc3ee5cd":"markdown","9bf5036a":"markdown","c574bc4e":"markdown","2100d157":"markdown","c631cc3e":"markdown","1b991f6c":"markdown","6fd17444":"markdown","20c7f4a3":"markdown","abd5d071":"markdown","5cfcdd77":"markdown","784706ab":"markdown","f50c862d":"markdown","7046c5d5":"markdown","f1fb8d7c":"markdown","aa4d3b2c":"markdown"},"source":{"dad833b9":"import numpy as np\nimport pandas as pd\nimport pickle\nimport matplotlib.pyplot as plt\nfrom scipy import signal\nfrom sklearn import linear_model\nfrom sklearn.metrics import roc_curve, roc_auc_score\nfrom tensorflow import keras\nimport glob\nimport matplotlib\nimport matplotlib.gridspec as gridspec\n\nmatplotlib.rcParams['pdf.fonttype'] = 42\nmatplotlib.rcParams['svg.fonttype'] = 'none'","e436ffbc":"#%% script params\n\nsave_figures = True\nsave_figures = False\nall_file_endings_to_use = ['.png', '.pdf', '.svg']\n\ndata_folder   = '\/kaggle\/input\/fiter-and-fire-paper\/results_data_mnist\/'\nfigure_folder = '\/kaggle\/working\/'\n\nbuild_dataframe_from_scratch = False\n","8f4c9fe9":"#%% helper functions\n\n\ndef create_single_PSP_profile(tau_rise, tau_decay, temporal_filter_length=50):\n\n    safety_factor = 1.5\n    if tau_rise >= (tau_decay \/ safety_factor):\n        tau_decay = safety_factor * tau_rise\n\n    exp_r = signal.exponential(M=temporal_filter_length, center=0, tau=tau_rise , sym=False)\n    exp_d = signal.exponential(M=temporal_filter_length, center=0, tau=tau_decay, sym=False)\n\n    post_syn_potential = exp_d - exp_r\n    post_syn_potential \/= post_syn_potential.max()\n\n    return post_syn_potential\n\n\ndef construct_normlized_synaptic_filter(tau_rise_vec, tau_decay_vec):\n\n    num_synapses = tau_rise_vec.shape[0]\n    temporal_filter_length = int(4 * tau_decay_vec.max()) + 1\n\n    syn_filter = np.zeros((num_synapses, temporal_filter_length))\n\n    for k, (tau_r, tau_d) in enumerate(zip(tau_rise_vec, tau_decay_vec)):\n        syn_filter[k,:] = create_single_PSP_profile(tau_r, tau_d, temporal_filter_length=temporal_filter_length)\n\n    return syn_filter\n\n\ndef simulate_filter_and_fire_cell_training(presynaptic_input_spikes, synaptic_weights, tau_rise_vec, tau_decay_vec,\n                                           refreactory_time_constant=20, v_reset=-75, v_threshold=-55, current_to_voltage_mult_factor=2):\n\n    temporal_filter_length = int(5 * refreactory_time_constant) + 1\n    refreactory_filter = signal.exponential(M=temporal_filter_length,center=0,tau=refreactory_time_constant,sym=False)[np.newaxis,:]\n\n    # padd input and get all synaptic filters\n    normlized_syn_filter = np.flipud(construct_normlized_synaptic_filter(tau_rise_vec, tau_decay_vec))\n    padded_input = np.hstack((np.zeros(normlized_syn_filter.shape), presynaptic_input_spikes))\n\n    # calc local currents\n    local_normlized_currents = np.zeros(presynaptic_input_spikes.shape, dtype=np.float16)\n    for k in range(normlized_syn_filter.shape[0]):\n        local_normlized_currents[k] = signal.convolve(padded_input[k], normlized_syn_filter[k], mode='valid')[1:]\n\n    # multiply by weights to get the somatic current\n    soma_current = signal.convolve(local_normlized_currents, synaptic_weights, mode='valid')\n\n    # simulate the cell\n    soma_voltage = v_reset + current_to_voltage_mult_factor * soma_current.ravel()\n    output_spike_times_in_ms = []\n    # after a spike inject current that is exactly required to bring the cell back to v_reset (this current slowly decays)\n    for t in range(len(soma_voltage)):\n        # after a spike inject current that is exactly required to bring the cell back to v_reset (this current slowly decays)\n        if (soma_voltage[t] > v_threshold) and ((t + 1) < len(soma_voltage)):\n            t_start = t + 1\n            t_end = min(len(soma_voltage), t_start + temporal_filter_length)\n            soma_voltage[t_start:t_end] -= (soma_voltage[t + 1] - v_reset) * refreactory_filter.ravel()[:(t_end - t_start)]\n            output_spike_times_in_ms.append(t)\n\n    return local_normlized_currents, soma_voltage, output_spike_times_in_ms\n\n\ndef simulate_filter_and_fire_cell_training_long(presynaptic_input_spikes, synaptic_weights, tau_rise_vec, tau_decay_vec,\n                                                refreactory_time_constant=20, v_reset=-75, v_threshold=-55, current_to_voltage_mult_factor=2):\n\n\n    total_duration_ms = presynaptic_input_spikes.shape[1]\n    max_duration_per_call_ms = 50000\n    overlap_time_ms = 500\n\n    if max_duration_per_call_ms >= total_duration_ms:\n        local_normlized_currents, soma_voltage, output_spike_times_in_ms = simulate_filter_and_fire_cell_training(presynaptic_input_spikes, synaptic_weights, tau_rise_vec, tau_decay_vec,\n                                                                                                                  refreactory_time_constant=refreactory_time_constant, v_reset=v_reset, v_threshold=v_threshold,\n                                                                                                                  current_to_voltage_mult_factor=current_to_voltage_mult_factor)\n        return local_normlized_currents, soma_voltage, output_spike_times_in_ms\n\n\n    local_normlized_currents = np.zeros(presynaptic_input_spikes.shape, dtype=np.float16)\n    soma_voltage = np.zeros((total_duration_ms,))\n    output_spike_times_in_ms = []\n\n    num_sub_calls = int(np.ceil(total_duration_ms \/ (max_duration_per_call_ms - overlap_time_ms)))\n    end_ind = overlap_time_ms\n    for k in range(num_sub_calls):\n        start_ind = end_ind - overlap_time_ms\n        end_ind = min(start_ind + max_duration_per_call_ms, total_duration_ms)\n\n        curr_loc_norm_c, curr_soma_v, curr_out_sp_t = simulate_filter_and_fire_cell_training(presynaptic_input_spikes[:,start_ind:end_ind], synaptic_weights, tau_rise_vec, tau_decay_vec,\n                                                                                             refreactory_time_constant=refreactory_time_constant, v_reset=v_reset, v_threshold=v_threshold,\n                                                                                             current_to_voltage_mult_factor=current_to_voltage_mult_factor)\n\n        # update fields\n        if k == 0:\n            local_normlized_currents[:,start_ind:end_ind] = curr_loc_norm_c\n            soma_voltage[start_ind:end_ind] = curr_soma_v\n            output_spike_times_in_ms += curr_out_sp_t\n        else:\n            local_normlized_currents[:,(start_ind+overlap_time_ms):end_ind] = curr_loc_norm_c[:,overlap_time_ms:end_ind]\n            soma_voltage[(start_ind+overlap_time_ms):end_ind] = curr_soma_v[overlap_time_ms:]\n            curr_out_sp_t = [x for x in curr_out_sp_t if x >= (overlap_time_ms-1)]\n            output_spike_times_in_ms += [(start_ind + x) for x in curr_out_sp_t]\n\n    return local_normlized_currents, soma_voltage, output_spike_times_in_ms\n\n\ndef simulate_filter_and_fire_cell_inference(presynaptic_input_spikes, synaptic_weights, tau_rise_vec, tau_decay_vec,\n                                            refreactory_time_constant=20, v_reset=-75, v_threshold=-55, current_to_voltage_mult_factor=2):\n\n    temporal_filter_length = int(5 * refreactory_time_constant) + 1\n    refreactory_filter = signal.exponential(M=temporal_filter_length,center=0,tau=refreactory_time_constant,sym=False)[np.newaxis,:]\n\n    # padd input and get all synaptic filters\n    normlized_syn_filter = construct_normlized_synaptic_filter(tau_rise_vec, tau_decay_vec)\n    padded_input = np.hstack((np.zeros(normlized_syn_filter.shape), presynaptic_input_spikes))\n\n    # calc somatic current\n    weighted_syn_filter  = synaptic_weights * normlized_syn_filter\n    soma_current = signal.convolve(padded_input, weighted_syn_filter, mode='valid')[:,1:]\n\n    # simulate the cell\n    soma_voltage = v_reset + current_to_voltage_mult_factor * soma_current.ravel()\n    output_spike_times_in_ms = []\n    # after a spike inject current that is exactly required to bring the cell back to v_reset (this current slowly decays)\n    for t in range(len(soma_voltage)):\n\n        # after a spike inject current that is exactly required to bring the cell back to v_reset (this current slowly decays)\n        if (soma_voltage[t] > v_threshold) and ((t + 1) < len(soma_voltage)):\n            t_start = t + 1\n            t_end = min(len(soma_voltage), t_start + temporal_filter_length)\n            soma_voltage[t_start:t_end] -= (soma_voltage[t + 1] - v_reset) * refreactory_filter.ravel()[:(t_end - t_start)]\n            output_spike_times_in_ms.append(t)\n\n    return soma_voltage, output_spike_times_in_ms\n\n\ndef simulate_filter_and_fire_cell_inference_long(presynaptic_input_spikes, synaptic_weights, tau_rise_vec, tau_decay_vec,\n                                                refreactory_time_constant=20, v_reset=-75, v_threshold=-55, current_to_voltage_mult_factor=2):\n\n    total_duration_ms = presynaptic_input_spikes.shape[1]\n    max_duration_per_call_ms = 50000\n    overlap_time_ms = 500\n\n    if max_duration_per_call_ms >= total_duration_ms:\n        soma_voltage, output_spike_times_in_ms = simulate_filter_and_fire_cell_inference(presynaptic_input_spikes, synaptic_weights, tau_rise_vec, tau_decay_vec,\n                                                                                         refreactory_time_constant=refreactory_time_constant, v_reset=v_reset, v_threshold=v_threshold,\n                                                                                         current_to_voltage_mult_factor=current_to_voltage_mult_factor)\n        return soma_voltage, output_spike_times_in_ms\n\n\n    soma_voltage = np.zeros((total_duration_ms,))\n    output_spike_times_in_ms = []\n\n    num_sub_calls = int(np.ceil(total_duration_ms \/ (max_duration_per_call_ms - overlap_time_ms)))\n    end_ind = overlap_time_ms\n    for k in range(num_sub_calls):\n        start_ind = end_ind - overlap_time_ms\n        end_ind = min(start_ind + max_duration_per_call_ms, total_duration_ms)\n\n        curr_soma_v, curr_out_sp_t = simulate_filter_and_fire_cell_inference(presynaptic_input_spikes[:,start_ind:end_ind], synaptic_weights, tau_rise_vec, tau_decay_vec,\n                                                                             refreactory_time_constant=refreactory_time_constant, v_reset=v_reset, v_threshold=v_threshold,\n                                                                             current_to_voltage_mult_factor=current_to_voltage_mult_factor)\n\n        # update fields\n        if k == 0:\n            soma_voltage[start_ind:end_ind] = curr_soma_v\n            output_spike_times_in_ms += curr_out_sp_t\n        else:\n            soma_voltage[(start_ind+overlap_time_ms):end_ind] = curr_soma_v[overlap_time_ms:]\n            curr_out_sp_t = [x for x in curr_out_sp_t if x >= (overlap_time_ms-1)]\n            output_spike_times_in_ms += [(start_ind + x) for x in curr_out_sp_t]\n\n    return soma_voltage, output_spike_times_in_ms\n\n\n# use local currents as \"features\" and fit a linear model to the data\ndef prepare_training_dataset(local_normlized_currents, desired_output_spikes, spike_safety_range_ms=10, negative_subsampling_fraction=0.1):\n\n    # remove all \"negative\" time points that are too close to spikes\n    desired_output_spikes_LPF = signal.convolve(desired_output_spikes, np.ones((spike_safety_range_ms,)), mode='same') > 0.1\n    desired_timepoints = ~desired_output_spikes_LPF\n\n    # massivly subsample the remaining timepoints\n    desired_timepoints[np.random.rand(desired_timepoints.shape[0]) > negative_subsampling_fraction] = 0\n    desired_timepoints[desired_output_spikes > 0.1] = 1\n\n    X = local_normlized_currents.T[desired_timepoints,:]\n    y = desired_output_spikes[desired_timepoints]\n\n    return X, y\n","c7321471":"#%% load MNIST dataset and show the data\n\n(x_train_original, y_train), (x_test_original, y_test) = keras.datasets.mnist.load_data()\n\nnum_rows = 5\nnum_cols = 7\n\nplt.close('all')\nplt.figure(figsize=(20,15))\nfor k in range(num_rows * num_cols):\n    rand_sample_ind = np.random.randint(x_train_original.shape[0])\n    plt.subplot(num_rows, num_cols, k + 1);\n    plt.imshow(x_train_original[k]); plt.title('digit \"%s\"' %(y_train[k]))\n","e0167eae":"#%% Crop the data and binarize it\n\nh_crop_range = [4,24]\nw_crop_range = [4,24]\n\npositive_threshold = 150\n\nx_train_original = x_train_original[:,h_crop_range[0]:h_crop_range[1],w_crop_range[0]:w_crop_range[1]] > positive_threshold\nx_test_original  = x_test_original[: ,h_crop_range[0]:h_crop_range[1],w_crop_range[0]:w_crop_range[1]] > positive_threshold\n\nnum_rows = 5\nnum_cols = 7\n\nplt.close('all')\nplt.figure(figsize=(20,15))\nfor k in range(num_rows * num_cols):\n    rand_sample_ind = np.random.randint(x_train_original.shape[0])\n    plt.subplot(num_rows, num_cols, k + 1);\n    plt.imshow(x_train_original[k]); plt.title('digit \"%s\"' %(y_train[k]))\n","64c3f68b":"#%% Transform Xs to spatio-temporal spike trains\n\nspatial_extent_factor = 5\ntemporal_extent_factor_numerator = 2\ntemporal_extent_factor_denumerator = 1\n\nnum_const_firing_channels = 20\ntemporal_silence_ms = 70\n\n# extend according to \"temporal_extent_factor\"\nkernel = np.ones((1, spatial_extent_factor, temporal_extent_factor_numerator), dtype=bool)\n\nx_train = x_train_original.copy()\nx_test  = x_test_original.copy()\n\n# reshape X according to what is needed\nx_train = np.kron(x_train, kernel)\nx_test = np.kron(x_test, kernel)\n\n# subsample according to \"temporal_extent_factor_denumerator\"\nx_train = x_train[:,:,::temporal_extent_factor_denumerator]\nx_test = x_test[:,:,::temporal_extent_factor_denumerator]\n\n# padd with ones on top (for \"bias\" learning)\ntop_pad_train = np.ones((1, num_const_firing_channels, x_train.shape[2]), dtype=bool)\ntop_pad_test  = np.ones((1, num_const_firing_channels, x_test.shape[2] ), dtype=bool)\n\n# add a few zero rows for clear seperation for visualization purpuses\ntop_pad_train[:,-5:,:] = 0\ntop_pad_test[:,-5:,:] = 0\n\nx_train = np.concatenate((np.tile(top_pad_train, [x_train.shape[0],1,1]), x_train), axis=1)\nx_test  = np.concatenate((np.tile(top_pad_test , [x_test.shape[0],1,1] ), x_test ), axis=1)\n\n# pad with \"temporal_silence_ms\" zeros in the begining of each pattern (for silence between patterns)\nleft_pad_train = np.zeros((1, x_train.shape[1], temporal_silence_ms), dtype=bool)\nleft_pad_test  = np.zeros((1, x_test.shape[1] , temporal_silence_ms), dtype=bool)\n\nx_train = np.concatenate((np.tile(left_pad_train, [x_train.shape[0],1,1]), x_train), axis=2)\nx_test  = np.concatenate((np.tile(left_pad_test , [x_test.shape[0],1,1] ), x_test ), axis=2)\n\n# add background activity\ndesired_background_activity_firing_rate_Hz = 10\nbackground_activity_fraction = desired_background_activity_firing_rate_Hz \/ 1000\n\nx_train[np.random.rand(x_train.shape[0], x_train.shape[1], x_train.shape[2]) < background_activity_fraction] = 1\nx_test[ np.random.rand(x_test.shape[0] , x_test.shape[1] , x_test.shape[2] ) < background_activity_fraction] = 1\n\n# subsample the input spikes\ndesired_average_input_firing_rate_Hz = 20\nactual_mean_firing_rate_Hz = 1000 * x_train.mean()\n\nfraction_of_spikes_to_eliminate = desired_average_input_firing_rate_Hz \/ actual_mean_firing_rate_Hz\n\nx_train = x_train * (np.random.rand(x_train.shape[0], x_train.shape[1], x_train.shape[2]) < fraction_of_spikes_to_eliminate)\nx_test  = x_test  * (np.random.rand(x_test.shape[0], x_test.shape[1], x_test.shape[2]) < fraction_of_spikes_to_eliminate)\n\nfinal_mean_firing_rate_Hz = 1000 * x_train.mean()\n\n# display the patterns\nnum_rows = 5\nnum_cols = 7\n\nplt.close('all')\nplt.figure(figsize=(20,15))\nfor k in range(num_rows * num_cols):\n    rand_sample_ind = np.random.randint(x_train.shape[0])\n    plt.subplot(num_rows, num_cols, k + 1);\n    plt.imshow(x_train[k], cmap='gray'); plt.title('digit \"%s\"' %(y_train[k]))\n","8f71d499":"#%% Create \"one-vs-all\" dataset\n\npositive_digit = 3\n#num_train_positive_patterns = 7000\nnum_train_positive_patterns = 1000 # limited size of dataset due to memory issues inside kaggle notebooks\n\nrelease_probability = 1.0\napply_release_prob_during_train = False\napply_releash_prob_during_test = False\n\ny_train_binary = y_train == positive_digit\ny_test_binary = y_test == positive_digit\n\nnum_train_positive_patterns = min(int(y_train_binary.sum()), num_train_positive_patterns)\n\nnum_train_negative_patterns = int(2.0 * num_train_positive_patterns)\n\npositive_inds = np.where(y_train_binary)[0]\nnegative_inds = np.where(~y_train_binary)[0]\n\nselected_train_positives = np.random.choice(positive_inds, size=num_train_positive_patterns)\nselected_train_negatives = np.random.choice(negative_inds, size=num_train_negative_patterns)\n\nall_selected = np.random.permutation(np.concatenate((selected_train_positives, selected_train_negatives)))\n\nX_train_spikes = x_train[all_selected]\nY_train_spikes = y_train_binary[all_selected]\n\nX_test_spikes = x_test.copy()\nY_test_spikes = y_test_binary.copy()\n\nzero_pred_baseline_accuracy = 100 * (1 - Y_test_spikes.mean())","a46e8fc5":"#%% Create a regularized logistic regression baseline\n\nlogistic_reg_model = linear_model.LogisticRegression(C=0.1, fit_intercept=False, penalty='l2',verbose=False)\n\n# fit model\nlogistic_reg_model.fit(X_train_spikes.reshape([X_train_spikes.shape[0],-1]), Y_train_spikes)\n\n# predict and calculate AUC on train data\nY_train_spikes_hat = logistic_reg_model.predict_proba(X_train_spikes.reshape([X_train_spikes.shape[0],-1]))[:,1]\nY_test_spikes_hat = logistic_reg_model.predict_proba(X_test_spikes.reshape([X_test_spikes.shape[0],-1]))[:,1]\n\ntrain_AUC = roc_auc_score(Y_train_spikes, Y_train_spikes_hat)\ntest_AUC = roc_auc_score(Y_test_spikes, Y_test_spikes_hat)\n\nprint('for (# pos = %d, # neg = %d): (train AUC, test AUC) = (%.5f, %.5f)' %(num_train_positive_patterns, num_train_negative_patterns, train_AUC, test_AUC))\n\nplt.close('all')\nplt.figure(figsize=(8,8))\nplt.imshow(logistic_reg_model.coef_.reshape([X_train_spikes.shape[1], X_train_spikes.shape[2]]));\nplt.title('Learned Weights \\n (spatio-temporal (\"image\") logistic regression)');\n","aa4c5733":"#%% Calculate and Display LogReg Accuracy\n\nLL_false_positive_list, LL_true_positive_list, LL_thresholds_list = roc_curve(Y_test_spikes, Y_test_spikes_hat)\n\nnum_pos_class = int((Y_test_spikes == True).sum())\nnum_neg_class = int((Y_test_spikes == False).sum())\n\ntp = LL_true_positive_list * num_pos_class\ntn = (1 - LL_false_positive_list) * num_neg_class\nLL_accuracy_list = (tp + tn) \/ (num_pos_class + num_neg_class)\n\nLL_false_positive_list = LL_false_positive_list[LL_false_positive_list < 0.05]\nLL_true_positive_list = LL_true_positive_list[:len(LL_false_positive_list)]\nLL_thresholds_list = LL_thresholds_list[:len(LL_false_positive_list)]\nLL_accuracy_list = LL_accuracy_list[:len(LL_false_positive_list)]\n\nLL_false_positive_list = 100 * LL_false_positive_list\nLL_true_positive_list = 100 * LL_true_positive_list\nLL_accuracy_list = 100 * LL_accuracy_list\n\nLL_accuracy_max = LL_accuracy_list.max()\n\nLL_accuracy_subsampled = LL_accuracy_list[30::30]\nLL_thresholds_subsampled = LL_thresholds_list[30::30]\n\nacc_bar_x_axis = range(LL_accuracy_subsampled.shape[0])\n\nplt.close('all')\nplt.figure(figsize=(15,8));\nplt.subplot(1,2,1); plt.bar(x=acc_bar_x_axis,height=LL_accuracy_subsampled);\nplt.xticks(acc_bar_x_axis, LL_thresholds_subsampled, rotation='vertical');\nplt.title('max accuracy = %.2f%s' %(LL_accuracy_max,'%'), fontsize=24)\nplt.ylim(87.8,100); plt.xlabel('threshold', fontsize=20); plt.ylabel('Accuracy (%)', fontsize=20);\nplt.plot([acc_bar_x_axis[0]-1, acc_bar_x_axis[-1]+1], [zero_pred_baseline_accuracy, zero_pred_baseline_accuracy], color='r')\nplt.subplot(1,2,2); plt.plot(LL_false_positive_list, LL_true_positive_list);\nplt.ylabel('True Positive (%)', fontsize=20); plt.xlabel('False Positive (%)', fontsize=20);\n","f43a4256":"#%% Fit a F&F model\n\n# main parameters\nconnections_per_axon = 5\nmodel_type = 'F&F'\n#model_type = 'I&F'\n\n# neuron model parameters\nnum_axons = X_train_spikes[0].shape[0]\nnum_synapses = connections_per_axon * num_axons\n\nv_reset     = -80\nv_threshold = -55\ncurrent_to_voltage_mult_factor = 3\nrefreactory_time_constant = 15\n\n# synapse non-learnable parameters\nif model_type == 'F&F':\n    tau_rise_range  = [1,18]\n    tau_decay_range = [8,27]\nelif model_type == 'I&F':\n    tau_rise_range  = [1,1]\n    tau_decay_range = [27,27]\n\ntau_rise_vec  = np.random.uniform(low=tau_rise_range[0] , high=tau_rise_range[1] , size=(num_synapses, 1))\ntau_decay_vec = np.random.uniform(low=tau_decay_range[0], high=tau_decay_range[1], size=(num_synapses, 1))\n\n# synapse learnable parameters\nsynaptic_weights_vec = np.random.normal(size=(num_synapses, 1))\n\n# prepare input spikes\naxons_input_spikes = np.concatenate([X_train_spikes[k] for k in range(X_train_spikes.shape[0])],axis=1)\n\n# prepare output spikes\npattern_duration_ms = X_train_spikes[0].shape[1]\noutput_spike_offset = 1\noutput_kernel = np.zeros((pattern_duration_ms,))\noutput_kernel[-output_spike_offset] = 1\n\ndesired_output_spikes = np.kron(Y_train_spikes, output_kernel)\n\nplt.close('all')\nplt.figure(figsize=(30,15));\nplt.imshow(axons_input_spikes[:,:1101], cmap='gray')\nplt.title('input axons raster', fontsize=22)\nplt.ylabel('axon index', fontsize=22);\nplt.xlabel('time [ms]', fontsize=22);\n\nplt.figure(figsize=(30,1));\nplt.plot(desired_output_spikes[:1101]); plt.xlim(0,1101)\nplt.ylabel('desired output spike', fontsize=22)\nplt.xlabel('time [ms]', fontsize=22);\n","fcda0c15":"#%% simulate cell and extract normlized currents\n\npresynaptic_input_spikes = np.kron(np.ones((connections_per_axon,1), dtype=bool), axons_input_spikes).astype(bool)\n\nlocal_normlized_currents, soma_voltage, output_spike_times_in_ms = simulate_filter_and_fire_cell_training_long(presynaptic_input_spikes,\n                                                                                                               synaptic_weights_vec, tau_rise_vec, tau_decay_vec,\n                                                                                                               refreactory_time_constant=refreactory_time_constant,\n                                                                                                               v_reset=v_reset, v_threshold=v_threshold,\n                                                                                                               current_to_voltage_mult_factor=current_to_voltage_mult_factor)","15ce1298":"#%% fit linear model to local currents\n\nfilter_and_fire_model = linear_model.LogisticRegression(C=10000, fit_intercept=False, penalty='l2', solver='liblinear')\n\nspike_safety_range_ms = 20\nnegative_subsampling_fraction = 0.1 # reduced due to memory issues in kaggle notebooks\n\nX, y = prepare_training_dataset(local_normlized_currents, desired_output_spikes,\n                                spike_safety_range_ms=spike_safety_range_ms,\n                                negative_subsampling_fraction=negative_subsampling_fraction)\n\n# fit model\nfilter_and_fire_model.fit(X, y)\n\nprint('number of data points = %d (%.2f%s positive class)' %(X.shape[0], 100 * y.mean(),'%'))\n\n# calculate train AUC\ny_hat = filter_and_fire_model.predict_proba(X)[:,1]\ntrain_AUC = roc_auc_score(y, y_hat)\n\n# display some training data predictions\nnum_timepoints_to_show = 10000\nfitted_output_spike_prob = filter_and_fire_model.predict_proba(local_normlized_currents[:,:num_timepoints_to_show].T)[:,1]\n\nplt.close('all')\nplt.figure(figsize=(30,10))\nplt.plot(1.05 * desired_output_spikes[:num_timepoints_to_show] - 0.025); plt.title('train AUC = %.5f' %(train_AUC), fontsize=22)\nplt.plot(fitted_output_spike_prob[:num_timepoints_to_show]); plt.xlabel('time [ms]'); plt.legend(['GT', 'prediction'], fontsize=22);\n","7844bd7e":"#%% display learned weights\n\nnormlized_syn_filter = construct_normlized_synaptic_filter(tau_rise_vec, tau_decay_vec)\n\n# collect learned synaptic weights\nFF_learned_synaptic_weights = np.fliplr(filter_and_fire_model.coef_).T\nweighted_syn_filter = FF_learned_synaptic_weights * normlized_syn_filter\n\naxon_spatio_temporal_pattern = np.zeros((num_axons, weighted_syn_filter.shape[1]))\nfor k in range(num_axons):\n    axon_spatio_temporal_pattern[k] = weighted_syn_filter[k::num_axons].sum(axis=0)\n\naxon_spatio_temporal_pattern_short = axon_spatio_temporal_pattern[:,:X_train_spikes.shape[2]]\n\nplt.close('all')\nplt.figure(figsize=(18,8))\nplt.subplot(1,2,1); plt.imshow(logistic_reg_model.coef_.reshape([X_train_spikes.shape[1], X_train_spikes.shape[2]])); plt.title('logistic regression', fontsize=20)\nplt.subplot(1,2,2); plt.imshow(np.flip(axon_spatio_temporal_pattern_short)); plt.title('filter and fire neuron', fontsize=20);\n","c93eaddb":"#%% Make a prediction on the entire test trace\n\nnum_test_patterns = X_test_spikes.shape[0]\n\n# prepare test outputs\noutput_spike_tolorance_window_duration = 20\noutput_spike_tolorance_window_offset   = 5\noutput_kernel_test = np.zeros((X_test_spikes[0].shape[1],))\noutput_kernel_test[-output_spike_tolorance_window_duration:] = 1\n\ndesired_output_spikes_test = np.kron(Y_test_spikes[:num_test_patterns], output_kernel_test)\ndesired_output_spikes_test = np.concatenate((np.zeros((output_spike_tolorance_window_offset,)), desired_output_spikes_test[:-output_spike_tolorance_window_offset]))\n\n# prepare test inputs\naxons_input_spikes_test = np.concatenate([X_test_spikes[k] for k in range(num_test_patterns)],axis=1)\npresynaptic_input_spikes_test = np.kron(np.ones((connections_per_axon,1), dtype=bool), axons_input_spikes_test).astype(bool)\n\n# add synaptic unrelability (\"release probability\" that is not 100%)\nif apply_releash_prob_during_test:\n    presynaptic_input_spikes_test = presynaptic_input_spikes_test * (np.random.rand(presynaptic_input_spikes_test.shape[0], presynaptic_input_spikes_test.shape[1]) < release_probability)\n\nFF_weight_mult_factors_list = [x for x in [1,2,3,4,5,6,9,12,20,50,120,250]]\nFF_weight_mult_factors_list = [x for x in [0.1,1,3.16,10,100]] #reduced due to memory issues in kaggle notebooks\nFF_accuracy_list = []\nFF_true_positive_list = []\nFF_false_positive_list = []\nfor weight_mult_factor in FF_weight_mult_factors_list:\n\n    # collect learned synaptic weights\n    synaptic_weights_post_learning = weight_mult_factor * FF_learned_synaptic_weights\n\n    soma_voltage_test, output_spike_times_in_ms_test = simulate_filter_and_fire_cell_inference_long(presynaptic_input_spikes_test,\n                                                                                                    synaptic_weights_post_learning, tau_rise_vec, tau_decay_vec,\n                                                                                                    refreactory_time_constant=refreactory_time_constant,\n                                                                                                    v_reset=v_reset, v_threshold=v_threshold,\n                                                                                                    current_to_voltage_mult_factor=current_to_voltage_mult_factor)\n\n\n    output_spikes_test = np.zeros(soma_voltage_test.shape)\n    try:\n        output_spikes_test[np.array(output_spike_times_in_ms_test)] = 1.0\n    except:\n        print('no output spikes created')\n\n\n    # calculate test accuracy\n    compact_desired_output_test = Y_test_spikes[:num_test_patterns]\n\n    compact_desired_output_test2 = np.zeros(compact_desired_output_test.shape, dtype=bool)\n    compact_predicted_output_test = np.zeros(compact_desired_output_test.shape, dtype=bool)\n\n    # go over all patterns and extract the prediction (depends if the spikes are in the desired window)\n    for pattern_ind in range(num_test_patterns):\n        start_ind = pattern_duration_ms * pattern_ind + output_spike_tolorance_window_offset\n        end_ind = start_ind + pattern_duration_ms\n\n        # extract prediction\n        predicted_spike_train_for_pattern = output_spikes_test[start_ind:end_ind]\n        desired_spike_train_for_pattern = desired_output_spikes_test[start_ind:end_ind]\n\n        compact_desired_output_test2[pattern_ind]  = desired_spike_train_for_pattern.sum() > 0.1\n\n        if Y_test_spikes[pattern_ind] == 1:\n            # check if there is a spike in the desired window only\n            compact_predicted_output_test[pattern_ind] = (desired_spike_train_for_pattern * predicted_spike_train_for_pattern).sum() > 0.1\n        else:\n            # check if there is any spike in the full pattern duration\n            compact_predicted_output_test[pattern_ind] = predicted_spike_train_for_pattern.sum() > 0.1\n\n    # small verificaiton\n    assert((compact_desired_output_test == compact_desired_output_test2).sum() == num_test_patterns)\n\n    # display accuracy\n    percent_accuracy = 100 * (compact_desired_output_test == compact_predicted_output_test).mean()\n    true_positive    = 100 * (np.logical_and(compact_desired_output_test == True , compact_predicted_output_test == True).sum() \/ (compact_desired_output_test == True).sum())\n    false_positive   = 100 * (np.logical_and(compact_desired_output_test == False, compact_predicted_output_test == True).sum() \/ (compact_desired_output_test == False).sum())\n\n    print('weights mult factor = %.1f: Accuracy = %.3f%s. (TP, FP) = (%.3f%s, %.3f%s)' %(weight_mult_factor, percent_accuracy,'%',true_positive,'%',false_positive,'%'))\n\n    FF_accuracy_list.append(percent_accuracy)\n    FF_true_positive_list.append(true_positive)\n    FF_false_positive_list.append(false_positive)\n","6b0a96e2":"#%% \"after learning\" Build the nice looking figure of before and after learning\n\n# get the max accuracy weight matrix\nmax_accuracy_weight_mult_factor = FF_weight_mult_factors_list[np.argsort(np.array(FF_accuracy_list))[-1]]\n\nsynaptic_weights_vec_after_learning = max_accuracy_weight_mult_factor * FF_learned_synaptic_weights\n\n# simulate the max accuracy output after learning\nsoma_voltage_test, output_spike_times_in_ms_test = simulate_filter_and_fire_cell_inference_long(presynaptic_input_spikes_test,\n                                                                                                synaptic_weights_vec_after_learning, tau_rise_vec, tau_decay_vec,\n                                                                                                refreactory_time_constant=refreactory_time_constant,\n                                                                                                v_reset=v_reset, v_threshold=v_threshold,\n                                                                                                current_to_voltage_mult_factor=current_to_voltage_mult_factor)\n\n\noutput_spikes_test_after_learning_full = np.zeros(soma_voltage_test.shape)\ntry:\n    output_spikes_test_after_learning_full[np.array(output_spike_times_in_ms_test)] = 1.0\nexcept:\n    print('no output spikes created')\n\n#%% \"before learning\" Simulate response to test set before learning\n\nsynaptic_weights_vec_before_learning = 0.01 + 0.3 * np.random.normal(size=(num_synapses, 1))\n\n# simulate response to test set before learning (randomly permuted learned weights vector)\nsoma_voltage_test, output_spike_times_in_ms_test = simulate_filter_and_fire_cell_inference_long(presynaptic_input_spikes_test,\n                                                                                                synaptic_weights_vec_before_learning, tau_rise_vec, tau_decay_vec,\n                                                                                                refreactory_time_constant=refreactory_time_constant,\n                                                                                                v_reset=v_reset, v_threshold=v_threshold,\n                                                                                                current_to_voltage_mult_factor=current_to_voltage_mult_factor)\n\n\noutput_spikes_test_before_learning_full = np.zeros(soma_voltage_test.shape)\ntry:\n    output_spikes_test_before_learning_full[np.array(output_spike_times_in_ms_test)] = 1.0\nexcept:\n    print('no output spikes created')\n","dfbbf282":"#%% organize everything into a figure\n\n# test digit images\nextention_kernel = np.ones((1, spatial_extent_factor, temporal_extent_factor_numerator), dtype=bool)\nx_test_original_extended = np.kron(x_test_original, extention_kernel)\nleft_pad_test  = np.zeros((1, x_test_original_extended.shape[1] , temporal_silence_ms), dtype=bool)\nx_test_original_extended  = np.concatenate((np.tile(left_pad_test , [x_test_original_extended.shape[0],1,1] ), x_test_original_extended ), axis=2)\nx_test_axons_input_spikes = np.concatenate([x_test_original_extended[k] for k in range(x_test_original_extended.shape[0])],axis=1)\n\ntest_set_full_duration_ms = x_test_axons_input_spikes.shape[1]\n\n# select a subset of time to display\nnum_digits_to_display = 9\nstart_time = x_test_original_extended.shape[2] * np.random.randint(int(test_set_full_duration_ms \/ x_test_original_extended.shape[2] - temporal_silence_ms))\nend_time = start_time + 1 + x_test_original_extended.shape[2] * num_digits_to_display + temporal_silence_ms\n\nnum_spikes_in_window = 0\nwhile num_spikes_in_window != 3:\n    start_time = x_test_original_extended.shape[2] * np.random.randint(int(test_set_full_duration_ms \/ x_test_original_extended.shape[2] - temporal_silence_ms))\n    end_time = start_time + 1 + x_test_original_extended.shape[2] * num_digits_to_display + temporal_silence_ms\n\n    output_spike_tolorance_window_duration = 20\n    output_spike_tolorance_window_offset   = 5\n\n    output_kernel_test = np.zeros((X_test_spikes[0].shape[1],))\n    output_kernel_test[-1] = 1\n    desired_output_spikes_test_full = np.kron(Y_test_spikes[:num_test_patterns], output_kernel_test)\n    desired_output_spikes_test = desired_output_spikes_test_full[start_time:end_time]\n\n    num_spikes_in_window = desired_output_spikes_test.sum()\n\n# make sure we have something decent to show (randomise start and end times untill we do)\noutput_spikes_test_after_learning = output_spikes_test_after_learning_full[start_time:end_time]\nwhile output_spikes_test_after_learning.sum() < 2:\n    start_time = x_test_original_extended.shape[2] * np.random.randint(int(test_set_full_duration_ms \/ x_test_original_extended.shape[2] - temporal_silence_ms))\n    end_time = start_time + 1 + x_test_original_extended.shape[2] * num_digits_to_display + temporal_silence_ms\n    output_spikes_test_after_learning = output_spikes_test_after_learning_full[start_time:end_time]\n\nmin_time_ms = 0\nmax_time_ms = end_time - start_time\n\ntime_sec = np.arange(min_time_ms, max_time_ms) \/ 1000\nmin_time_sec = min_time_ms \/ 1000\nmax_time_sec = max_time_ms \/ 1000\n\n\nbefore_color = '0.15'\nafter_color = 'blue'\ntarget_color = 'red'\n\n# input digits\nx_test_input_digits = x_test_axons_input_spikes[:,start_time:end_time]\n\n# axon input raster\nsyn_activation_time, syn_activation_index = np.nonzero(axons_input_spikes_test[-x_test_axons_input_spikes.shape[0]:,start_time:end_time].T)\nsyn_activation_time = syn_activation_time \/ 1000\nsyn_activation_index = x_test_axons_input_spikes.shape[0] - syn_activation_index\n\n# output before learning\noutput_spikes_test_before_learning = output_spikes_test_before_learning_full[start_time:end_time]\n\n# output after learning\noutput_spikes_test_after_learning = output_spikes_test_after_learning_full[start_time:end_time]\n\n# desired output\noutput_spike_tolorance_window_duration = 20\noutput_spike_tolorance_window_offset   = 5\noutput_kernel_test = np.zeros((X_test_spikes[0].shape[1],))\noutput_kernel_test[-1] = 1\ndesired_output_spikes_test_full = np.kron(Y_test_spikes[:num_test_patterns], output_kernel_test)\ndesired_output_spikes_test = desired_output_spikes_test_full[start_time:end_time]\n\n\n# build full figure\nplt.close('all')\nfig = plt.figure(figsize=(19,22))\ngs_figure = gridspec.GridSpec(nrows=4,ncols=3)\ngs_figure.update(left=0.03, right=0.97, bottom=0.57, top=0.985, wspace=0.1, hspace=0.11)\n\nax_digits            = plt.subplot(gs_figure[0,:])\nax_axons             = plt.subplot(gs_figure[1:3,:])\nax_learning_outcomes = plt.subplot(gs_figure[3,:])\n\nax_digits.imshow(x_test_input_digits, cmap='gray'); ax_digits.set_title('Input Digits', fontsize=18)\nax_digits.set_xticks([])\nax_digits.set_yticks([])\nax_digits.spines['top'].set_visible(False)\nax_digits.spines['bottom'].set_visible(False)\nax_digits.spines['left'].set_visible(False)\nax_digits.spines['right'].set_visible(False)\n\nax_axons.scatter(syn_activation_time, syn_activation_index, s=6, c='k');\nax_axons.set_ylabel('Input Axons Raster', fontsize=18)\nax_axons.set_xlim(min_time_sec, max_time_sec);\nax_axons.set_xticks([])\nax_axons.set_yticks([])\nax_axons.spines['top'].set_visible(False)\nax_axons.spines['bottom'].set_visible(False)\nax_axons.spines['left'].set_visible(False)\nax_axons.spines['right'].set_visible(False)\n\n\nax_learning_outcomes.plot(time_sec, 2.2 + output_spikes_test_before_learning, c=before_color, lw=2.5);\nax_learning_outcomes.plot(time_sec, 1.1 + output_spikes_test_after_learning, c=after_color, lw=2.5);\nax_learning_outcomes.plot(time_sec, 0.0 + desired_output_spikes_test, c=target_color, lw=2.5);\n\nax_learning_outcomes.set_xlim(min_time_sec, max_time_sec);\nax_learning_outcomes.set_xticks([])\nax_learning_outcomes.set_yticks([])\nax_learning_outcomes.spines['top'].set_visible(False)\nax_learning_outcomes.spines['bottom'].set_visible(False)\nax_learning_outcomes.spines['left'].set_visible(False)\nax_learning_outcomes.spines['right'].set_visible(False)\n\nax_learning_outcomes.text(0.025,2.5, 'Before Learning', color=before_color, fontsize=20)\nax_learning_outcomes.text(0.025,1.4, 'After Learning', color=after_color, fontsize=20)\nax_learning_outcomes.text(0.025,0.3, 'Desired Output', color=target_color, fontsize=20)\n\n# load data into dataframe\n\nlist_of_files = glob.glob(data_folder + 'MNIST_*.pickle')\nprint(len(list_of_files))\n\nif build_dataframe_from_scratch:\n\n    # Load one saved pickle\n    filename_to_load = list_of_files[np.random.randint(len(list_of_files))]\n    loaded_script_results_dict = pickle.load(open(filename_to_load, \"rb\" ))\n\n    # display basic fields in the saved pickle file\n    print('-----------------------------------------------------------------------------------------------------------')\n    print('loaded_script_results_dict.keys():')\n    print('----------')\n    print(list(loaded_script_results_dict.keys()))\n    print('-----------------------------------------------------------------------------------------------------------')\n    print('loaded_script_results_dict[\"script_main_params\"].keys():')\n    print('----------')\n    print(list(loaded_script_results_dict[\"script_main_params\"].keys()))\n    print('-----------------------------------------------------------------------------------------------------------')\n    print('positive_digit =', loaded_script_results_dict['script_main_params']['positive_digit'])\n    print('connections_per_axon =', loaded_script_results_dict['script_main_params']['connections_per_axon'])\n    print('digit_sample_image_shape_cropped =', loaded_script_results_dict['script_main_params']['digit_sample_image_shape_cropped'])\n    print('digit_sample_image_shape_expanded =', loaded_script_results_dict['script_main_params']['digit_sample_image_shape_expanded'])\n    print('num_train_positive_patterns =', loaded_script_results_dict['script_main_params']['num_train_positive_patterns'])\n    print('temporal_silence_ms =', loaded_script_results_dict['script_main_params']['temporal_silence_ms'])\n    print('-----------------------------------------------------------------------------------------------------------')\n    print('model_accuracy_LR =', loaded_script_results_dict['model_accuracy_LR'])\n    print('model_accuracy_FF =', loaded_script_results_dict['model_accuracy_FF'])\n    print('model_accuracy_IF =', loaded_script_results_dict['model_accuracy_IF'])\n    print('model_accuracy_baseline =', loaded_script_results_dict['model_accuracy_baseline'])\n    print('-----------------------------------------------------------------------------------------------------------')\n\n    try:\n        print('-----------------------------------------------------------------------------------------------------------')\n        print('positive_digit =', loaded_script_results_dict['script_main_params']['positive_digit'])\n        print('num_train_positive_patterns =', loaded_script_results_dict['script_main_params']['num_train_positive_patterns'])\n        print('release_probability =', loaded_script_results_dict['script_main_params']['release_probability'])\n        print('train_epochs =', loaded_script_results_dict['script_main_params']['train_epochs'])\n        print('test_epochs =', loaded_script_results_dict['script_main_params']['test_epochs'])\n        print('create_output_burst =', loaded_script_results_dict['script_main_params']['create_output_burst'])\n        print('-----------------------------------------------------------------------------------------------------------')\n    except:\n        print('no prob release fields')\n\n    # display the learned weights\n    # plt.close('all')\n    # plt.figure(figsize=(24,10))\n    # plt.subplot(1,3,1); plt.imshow(loaded_script_results_dict['learned_weights_LR']); plt.title('logistic regression', fontsize=24)\n    # plt.subplot(1,3,2); plt.imshow(loaded_script_results_dict['learned_weights_FF']); plt.title('filter and fire neuron', fontsize=24)\n    # plt.subplot(1,3,3); plt.imshow(loaded_script_results_dict['learned_weights_IF']); plt.title('integrate and fire neuron', fontsize=24)\n\n    # go over all files and insert into a large dataframe\n    columns_to_use = ['digit','M_connections','N_axons', 'T', 'N_positive_samples',\n                      'Accuracy LR', 'Accuracy FF', 'Accuracy IF', 'Accuracy baseline',\n                      'release probability', 'train_epochs', 'test_epochs']\n    results_df = pd.DataFrame(index=range(len(list_of_files)), columns=columns_to_use)\n\n    for k, filename_to_load in enumerate(list_of_files):\n        loaded_script_results_dict = pickle.load(open(filename_to_load, \"rb\" ))\n\n        results_df.loc[k, 'digit']              = loaded_script_results_dict['script_main_params']['positive_digit']\n        results_df.loc[k, 'M_connections']      = loaded_script_results_dict['script_main_params']['connections_per_axon']\n        results_df.loc[k, 'N_axons']            = loaded_script_results_dict['script_main_params']['digit_sample_image_shape_expanded'][0]\n        results_df.loc[k, 'T']                  = loaded_script_results_dict['script_main_params']['digit_sample_image_shape_expanded'][1]\n        results_df.loc[k, 'N_positive_samples'] = loaded_script_results_dict['script_main_params']['num_train_positive_patterns']\n        results_df.loc[k, 'Accuracy LR']        = loaded_script_results_dict['model_accuracy_LR']\n        results_df.loc[k, 'Accuracy FF']        = loaded_script_results_dict['model_accuracy_FF']\n        results_df.loc[k, 'Accuracy IF']        = loaded_script_results_dict['model_accuracy_IF']\n        results_df.loc[k, 'Accuracy baseline']  = loaded_script_results_dict['model_accuracy_baseline']\n\n        try:\n            results_df.loc[k, 'release probability'] = loaded_script_results_dict['script_main_params']['release_probability']\n            results_df.loc[k, 'train_epochs']        = loaded_script_results_dict['script_main_params']['train_epochs']\n            results_df.loc[k, 'test_epochs']         = loaded_script_results_dict['script_main_params']['test_epochs']\n        except:\n            results_df.loc[k, 'release probability'] = 1.0\n            results_df.loc[k, 'train_epochs']        = 1\n            results_df.loc[k, 'test_epochs']         = 1\n\n    print(results_df.shape)\n\n    # save the dataframe\n    filename = 'MNIST_classification_LR_FF_IF_%d_rows_%d_cols.csv' %(results_df.shape[0], results_df.shape[1])\n    results_df.to_csv('\/kaggle\/working\/' + filename, index=False)\n    results_df = pd.read_csv('\/kaggle\/working\/' + filename)\nelse:\n    # open previously saved file\n    filename = 'MNIST_classification_LR_FF_IF_5162_rows_12_cols.csv'\n    results_df = pd.read_csv('\/kaggle\/input\/fiter-and-fire-paper\/' + filename)\n\nprint(results_df.shape)\n\n# display accuracy per digit at specific condition for (I&F, F&F, LR)\nselected_T = 40\nselected_M = 5\nselected_N_axons = 100\nselected_N_samples = 4000\nselected_release_prob = 1.0\n\ncondition_rows = results_df.loc[:, 'T'] == selected_T\ncondition_rows = np.logical_and(condition_rows, results_df.loc[:, 'M_connections']       == selected_M)\ncondition_rows = np.logical_and(condition_rows, results_df.loc[:, 'N_axons']             == selected_N_axons)\ncondition_rows = np.logical_and(condition_rows, results_df.loc[:, 'N_positive_samples']  >= selected_N_samples)\ncondition_rows = np.logical_and(condition_rows, results_df.loc[:, 'release probability'] == selected_release_prob)\n\naverage_for_condition_per_digit_df = results_df.loc[condition_rows,:].groupby('digit').mean()\nstddev_for_condition_per_digit_df  = results_df.loc[condition_rows,:].groupby('digit').std()\ncounts_for_condition_per_digit_df  = results_df.loc[condition_rows,:].groupby('digit').count().iloc[:,0]\n\ndigits_list = average_for_condition_per_digit_df.index.tolist()\nLR_accuracy       = average_for_condition_per_digit_df['Accuracy LR'].tolist()\nFF_accuracy       = average_for_condition_per_digit_df['Accuracy FF'].tolist()\nIF_accuracy       = average_for_condition_per_digit_df['Accuracy IF'].tolist()\nbaseline_accuracy = average_for_condition_per_digit_df['Accuracy baseline'].tolist()\n\nLR_stderr = stddev_for_condition_per_digit_df['Accuracy LR'].tolist()\nFF_stderr = stddev_for_condition_per_digit_df['Accuracy FF'].tolist()\nIF_stderr = stddev_for_condition_per_digit_df['Accuracy IF'].tolist()\n\n# display plot\nbar_plot_x_axis = 1.0 * np.arange(len(digits_list))\nbar_widths = 0.7 \/ 3\nx_tick_names = ['\"%s\"' %(str(x)) for x in digits_list]\n\ngs_accuracy_per_digit = gridspec.GridSpec(nrows=2,ncols=1)\ngs_accuracy_per_digit.update(left=0.045, right=0.675, bottom=0.32, top=0.52, wspace=0.1, hspace=0.1)\nax_accuracy_per_digit = plt.subplot(gs_accuracy_per_digit[:,:])\n\nax_accuracy_per_digit.bar(bar_plot_x_axis + 0 * bar_widths, IF_accuracy, bar_widths, yerr=IF_stderr, color='0.05'  , label='I&F')\nax_accuracy_per_digit.bar(bar_plot_x_axis + 1 * bar_widths, FF_accuracy, bar_widths, yerr=FF_stderr, color='orange', alpha=0.95, label='F&F')\nax_accuracy_per_digit.bar(bar_plot_x_axis + 2 * bar_widths, LR_accuracy, bar_widths, yerr=LR_stderr, color='0.45'  , label='Spatio\\nTemporal LR')\n\nfor k in range(len(digits_list)):\n    if k == 0:\n        ax_accuracy_per_digit.plot([bar_plot_x_axis[k] - 0.7 * bar_widths, bar_plot_x_axis[k] + 2.7 * bar_widths], [baseline_accuracy[k], baseline_accuracy[k]], color='r', label='Baseline')\n    else:\n        ax_accuracy_per_digit.plot([bar_plot_x_axis[k] - 0.7 * bar_widths, bar_plot_x_axis[k] + 2.7 * bar_widths], [baseline_accuracy[k], baseline_accuracy[k]], color='r')\n\nax_accuracy_per_digit.set_title('Accuracy Comparison per digit (M = %d, T = %d (ms))' %(selected_M, selected_T), fontsize=22)\nax_accuracy_per_digit.set_yticks([90,92,94,96,98]);\nax_accuracy_per_digit.set_yticklabels([90,92,94,96,98], fontsize=16);\nax_accuracy_per_digit.set_ylim(87.9,98.9);\nax_accuracy_per_digit.set_xticks(bar_plot_x_axis + bar_widths);\nax_accuracy_per_digit.set_xticklabels(x_tick_names, rotation=0, fontsize=26);\nax_accuracy_per_digit.set_ylabel('Test accuracy (%)', fontsize=18)\nax_accuracy_per_digit.legend(fontsize=18, ncol=1);\nax_accuracy_per_digit.set_xlim(-0.5,10);\nax_accuracy_per_digit.spines['top'].set_visible(False)\nax_accuracy_per_digit.spines['right'].set_visible(False)\n\n# display accuracy across all digits as function of pattern presentation duration for (I&F, F&F, LR)\nselected_M = 5\nselected_N_axons = 100\nselected_N_samples = 4000\nselected_release_prob = 1.0\n\ncondition_rows = results_df.loc[:, 'M_connections'] == selected_M\ncondition_rows = np.logical_and(condition_rows, results_df.loc[:, 'N_axons']             == selected_N_axons)\ncondition_rows = np.logical_and(condition_rows, results_df.loc[:, 'N_positive_samples']  >= selected_N_samples)\ncondition_rows = np.logical_and(condition_rows, results_df.loc[:, 'release probability'] == selected_release_prob)\n\nall_T_values = sorted(results_df['T'].unique().tolist())\ndigits_list  = sorted(results_df['digit'].unique().tolist())\nnum_digits   = len(digits_list)\n\nLR_acc_mean = []\nFF_acc_mean = []\nIF_acc_mean = []\nbaseline_acc_mean = []\n\nLR_acc_std = []\nFF_acc_std = []\nIF_acc_std = []\nbaseline_acc_std = []\n\nfor selected_T in all_T_values:\n    curr_rows = np.logical_and(condition_rows, results_df.loc[:, 'T'] == selected_T)\n\n    average_acc_per_T_per_digit_df = results_df.loc[curr_rows,:].groupby('digit').mean()\n    stddev_acc_per_T_per_digit_df  = results_df.loc[curr_rows,:].groupby('digit').std()\n    # print(selected_T, ':\\n', results_df.loc[curr_rows,:].groupby('digit').count().iloc[:,0])\n\n    LR_acc_mean.append(average_acc_per_T_per_digit_df['Accuracy LR'].mean())\n    FF_acc_mean.append(average_acc_per_T_per_digit_df['Accuracy FF'].mean())\n    IF_acc_mean.append(average_acc_per_T_per_digit_df['Accuracy IF'].mean())\n    baseline_acc_mean.append(average_acc_per_T_per_digit_df['Accuracy baseline'].mean())\n\n    LR_acc_std.append(average_acc_per_T_per_digit_df['Accuracy LR'].std() \/ np.sqrt(num_digits))\n    FF_acc_std.append(average_acc_per_T_per_digit_df['Accuracy FF'].std() \/ np.sqrt(num_digits))\n    IF_acc_std.append(average_acc_per_T_per_digit_df['Accuracy IF'].std() \/ np.sqrt(num_digits))\n    baseline_acc_std.append(average_acc_per_T_per_digit_df['Accuracy baseline'].std() \/ np.sqrt(num_digits))\n\ngs_accuracy_vs_T = gridspec.GridSpec(nrows=2,ncols=1)\ngs_accuracy_vs_T.update(left=0.045, right=0.3375, bottom=0.04, top=0.28, wspace=0.1, hspace=0.1)\nax_accuracy_vs_T = plt.subplot(gs_accuracy_vs_T[:,:])\n\nax_accuracy_vs_T.errorbar(all_T_values, LR_acc_mean, yerr=LR_acc_std, linewidth=4, color='0.45')\nax_accuracy_vs_T.errorbar(all_T_values, FF_acc_mean, yerr=FF_acc_std, linewidth=4, color='orange')\nax_accuracy_vs_T.errorbar(all_T_values, IF_acc_mean, yerr=IF_acc_std, linewidth=4, color='0.05')\nax_accuracy_vs_T.errorbar(all_T_values, baseline_acc_mean, yerr=baseline_acc_std, linewidth=1, color='red')\nax_accuracy_vs_T.set_xlabel('Pattern presentation duration - T (ms)', fontsize=18)\nax_accuracy_vs_T.set_ylabel('Test Accuracy (%)', fontsize=18)\nax_accuracy_vs_T.set_ylim(89.5,96.5)\nax_accuracy_vs_T.set_xticks(all_T_values)\nax_accuracy_vs_T.set_xticklabels(all_T_values, fontsize=15)\nax_accuracy_vs_T.set_yticks([90,92,94,96]);\nax_accuracy_vs_T.set_yticklabels([90,92,94,96], fontsize=15);\nax_accuracy_vs_T.spines['top'].set_visible(False)\nax_accuracy_vs_T.spines['right'].set_visible(False)\n\n\n# display accuracy as function of M, for a specific digit and multiple release probabilities\nselected_digit = 2\nselected_N_axons = 100\nselected_N_samples = 2048\n\ncondition_rows = results_df.loc[:, 'digit'] == selected_digit\ncondition_rows = np.logical_and(condition_rows, results_df.loc[:, 'N_axons']             == selected_N_axons)\ncondition_rows = np.logical_and(condition_rows, results_df.loc[:, 'N_positive_samples']  == selected_N_samples)\n\nall_M_values = sorted(results_df.loc[condition_rows, 'M_connections'].unique().tolist())\nall_P_values = sorted(results_df.loc[condition_rows, 'release probability'].unique().tolist())\n\nall_M_values = [1,2,3,5,8]\n\nresults_dict_Acc_vs_M = {}\nfor selected_release_P in all_P_values:\n    results_dict_Acc_vs_M[selected_release_P] = {}\n    results_dict_Acc_vs_M[selected_release_P]['LR_acc_mean'] = []\n    results_dict_Acc_vs_M[selected_release_P]['FF_acc_mean'] = []\n    results_dict_Acc_vs_M[selected_release_P]['IF_acc_mean'] = []\n    results_dict_Acc_vs_M[selected_release_P]['baseline_acc_mean'] = []\n\n    results_dict_Acc_vs_M[selected_release_P]['LR_acc_std'] = []\n    results_dict_Acc_vs_M[selected_release_P]['FF_acc_std'] = []\n    results_dict_Acc_vs_M[selected_release_P]['IF_acc_std'] = []\n    results_dict_Acc_vs_M[selected_release_P]['baseline_acc_std'] = []\n\n\nfor selected_M in all_M_values:\n    curr_condition_rows = np.logical_and(condition_rows, results_df.loc[:, 'M_connections'] == selected_M)\n\n    for selected_release_P in all_P_values:\n        curr_rows = np.logical_and(curr_condition_rows, results_df.loc[:, 'release probability'] == selected_release_P)\n\n        num_rows = curr_rows.sum()\n        num_rows = 1\n        results_dict_Acc_vs_M[selected_release_P]['LR_acc_mean'].append(results_df.loc[curr_rows,'Accuracy LR'].mean())\n        results_dict_Acc_vs_M[selected_release_P]['FF_acc_mean'].append(results_df.loc[curr_rows,'Accuracy FF'].mean())\n        results_dict_Acc_vs_M[selected_release_P]['IF_acc_mean'].append(results_df.loc[curr_rows,'Accuracy IF'].mean())\n        results_dict_Acc_vs_M[selected_release_P]['baseline_acc_mean'].append(results_df.loc[curr_rows,'Accuracy baseline'].mean())\n\n        results_dict_Acc_vs_M[selected_release_P]['LR_acc_std'].append(results_df.loc[curr_rows,'Accuracy LR'].std() \/ np.sqrt(num_rows))\n        results_dict_Acc_vs_M[selected_release_P]['FF_acc_std'].append(results_df.loc[curr_rows,'Accuracy FF'].std() \/ np.sqrt(num_rows))\n        results_dict_Acc_vs_M[selected_release_P]['IF_acc_std'].append(results_df.loc[curr_rows,'Accuracy IF'].std() \/ np.sqrt(num_rows))\n        results_dict_Acc_vs_M[selected_release_P]['baseline_acc_std'].append(results_df.loc[curr_rows,'Accuracy baseline'].std() \/ np.sqrt(num_rows))\n\ngs_accuracy_vs_M = gridspec.GridSpec(nrows=2,ncols=1)\ngs_accuracy_vs_M.update(left=0.37, right=0.66, bottom=0.04, top=0.28, wspace=0.1, hspace=0.1)\nax_accuracy_vs_M = plt.subplot(gs_accuracy_vs_M[:,:])\n\nselected_release_P = 1.0\nax_accuracy_vs_M.errorbar(all_M_values, results_dict_Acc_vs_M[selected_release_P]['LR_acc_mean'], yerr=results_dict_Acc_vs_M[selected_release_P]['LR_acc_std'], lw=4, color='0.45')\nax_accuracy_vs_M.errorbar(all_M_values, results_dict_Acc_vs_M[selected_release_P]['FF_acc_mean'], yerr=results_dict_Acc_vs_M[selected_release_P]['FF_acc_std'], lw=4, color='orange')\nax_accuracy_vs_M.errorbar(all_M_values, results_dict_Acc_vs_M[selected_release_P]['IF_acc_mean'], yerr=results_dict_Acc_vs_M[selected_release_P]['IF_acc_std'], lw=4, color='0.05')\n\nselected_release_P = 0.5\nax_accuracy_vs_M.errorbar(all_M_values, results_dict_Acc_vs_M[selected_release_P]['LR_acc_mean'], yerr=results_dict_Acc_vs_M[selected_release_P]['LR_acc_std'], lw=4, ls=':', color='0.45')\nax_accuracy_vs_M.errorbar(all_M_values, results_dict_Acc_vs_M[selected_release_P]['FF_acc_mean'], yerr=results_dict_Acc_vs_M[selected_release_P]['FF_acc_std'], lw=4, ls=':', color='orange')\nax_accuracy_vs_M.errorbar(all_M_values, results_dict_Acc_vs_M[selected_release_P]['IF_acc_mean'], yerr=results_dict_Acc_vs_M[selected_release_P]['IF_acc_std'], lw=4, ls=':', color='0.05')\n\nlegend_list = ['release Prob = 1.0', 'release Prob = 1.0', 'release Prob = 1.0',\n               'P = 0.5', 'P = 0.5', 'P = 0.5']\nax_accuracy_vs_M.legend(legend_list, ncol=2, mode='expand', fontsize=18)\n\nax_accuracy_vs_M.set_xlabel('Number of Multiple Contacts - M', fontsize=18)\nax_accuracy_vs_M.set_xticks(all_M_values)\nax_accuracy_vs_M.set_xticklabels(all_M_values, fontsize=15)\nax_accuracy_vs_M.set_yticks([91,93,95,97]);\nax_accuracy_vs_M.set_yticklabels([91,93,95,97], fontsize=15);\nax_accuracy_vs_M.set_ylim(90.5,98.2)\nax_accuracy_vs_M.spines['top'].set_visible(False)\nax_accuracy_vs_M.spines['right'].set_visible(False)\nax_accuracy_vs_M.set_xlim(0.8,8.2)\n\n\n# display accuracy as function of number of positive training samples\nselected_digit = 7\nselected_T = 30\nselected_M = 5\nselected_N_axons = 100\nmax_N_samples = 1100\nnum_train_epochs = 15\n\ncondition_rows = results_df.loc[:, 'digit'] == selected_digit\ncondition_rows = np.logical_and(condition_rows, results_df.loc[:, 'T']                  == selected_T)\ncondition_rows = np.logical_and(condition_rows, results_df.loc[:, 'M_connections']      == selected_M)\ncondition_rows = np.logical_and(condition_rows, results_df.loc[:, 'N_axons']            == selected_N_axons)\ncondition_rows = np.logical_and(condition_rows, results_df.loc[:, 'N_positive_samples'] <= max_N_samples)\n\ngood_epoch_rows = np.logical_or(results_df.loc[:, 'release probability'] == 1.0, results_df.loc[:, 'train_epochs'] == num_train_epochs)\ncondition_rows  = np.logical_and(condition_rows, good_epoch_rows)\n\nall_N_samples_values = sorted(results_df.loc[condition_rows, 'N_positive_samples'].unique().tolist())\nall_P_values         = sorted(results_df.loc[condition_rows, 'release probability'].unique().tolist())\n\nall_N_samples_values = [16, 32, 64, 128, 256, 512, 1024]\n\nresults_dict_Acc_vs_N_samples = {}\nfor selected_release_P in all_P_values:\n    results_dict_Acc_vs_N_samples[selected_release_P] = {}\n    results_dict_Acc_vs_N_samples[selected_release_P]['LR_acc_mean'] = []\n    results_dict_Acc_vs_N_samples[selected_release_P]['FF_acc_mean'] = []\n    results_dict_Acc_vs_N_samples[selected_release_P]['IF_acc_mean'] = []\n    results_dict_Acc_vs_N_samples[selected_release_P]['baseline_acc_mean'] = []\n\n    results_dict_Acc_vs_N_samples[selected_release_P]['LR_acc_std'] = []\n    results_dict_Acc_vs_N_samples[selected_release_P]['FF_acc_std'] = []\n    results_dict_Acc_vs_N_samples[selected_release_P]['IF_acc_std'] = []\n    results_dict_Acc_vs_N_samples[selected_release_P]['baseline_acc_std'] = []\n\n\nfor selected_N_samples in all_N_samples_values:\n    curr_condition_rows = np.logical_and(condition_rows, results_df.loc[:, 'N_positive_samples'] == selected_N_samples)\n\n    for selected_release_P in all_P_values:\n        curr_rows = np.logical_and(curr_condition_rows, results_df.loc[:, 'release probability'] == selected_release_P)\n        num_rows = curr_rows.sum()\n\n        results_dict_Acc_vs_N_samples[selected_release_P]['LR_acc_mean'].append(results_df.loc[curr_rows,'Accuracy LR'].mean())\n        results_dict_Acc_vs_N_samples[selected_release_P]['FF_acc_mean'].append(results_df.loc[curr_rows,'Accuracy FF'].mean())\n        results_dict_Acc_vs_N_samples[selected_release_P]['IF_acc_mean'].append(results_df.loc[curr_rows,'Accuracy IF'].mean())\n        results_dict_Acc_vs_N_samples[selected_release_P]['baseline_acc_mean'].append(results_df.loc[curr_rows,'Accuracy baseline'].mean())\n\n        results_dict_Acc_vs_N_samples[selected_release_P]['LR_acc_std'].append(results_df.loc[curr_rows,'Accuracy LR'].std() \/ np.sqrt(num_rows))\n        results_dict_Acc_vs_N_samples[selected_release_P]['FF_acc_std'].append(results_df.loc[curr_rows,'Accuracy FF'].std() \/ np.sqrt(num_rows))\n        results_dict_Acc_vs_N_samples[selected_release_P]['IF_acc_std'].append(results_df.loc[curr_rows,'Accuracy IF'].std() \/ np.sqrt(num_rows))\n        results_dict_Acc_vs_N_samples[selected_release_P]['baseline_acc_std'].append(results_df.loc[curr_rows,'Accuracy baseline'].std() \/ np.sqrt(num_rows))\n\n\ngs_accuracy_vs_N_samples = gridspec.GridSpec(nrows=2,ncols=1)\ngs_accuracy_vs_N_samples.update(left=0.6925, right=0.97, bottom=0.04, top=0.28, wspace=0.1, hspace=0.1)\nax_accuracy_vs_N_samples = plt.subplot(gs_accuracy_vs_N_samples[:,:])\n\nselected_release_P = 1.0\nax_accuracy_vs_N_samples.errorbar(all_N_samples_values, results_dict_Acc_vs_N_samples[selected_release_P]['LR_acc_mean'], yerr=results_dict_Acc_vs_N_samples[selected_release_P]['LR_acc_std'], lw=4, color='0.45')\nax_accuracy_vs_N_samples.errorbar(all_N_samples_values, results_dict_Acc_vs_N_samples[selected_release_P]['FF_acc_mean'], yerr=results_dict_Acc_vs_N_samples[selected_release_P]['FF_acc_std'], lw=4, color='orange')\nax_accuracy_vs_N_samples.errorbar(all_N_samples_values, results_dict_Acc_vs_N_samples[selected_release_P]['IF_acc_mean'], yerr=results_dict_Acc_vs_N_samples[selected_release_P]['IF_acc_std'], lw=4, color='0.05')\n\nselected_release_P = 0.5\nax_accuracy_vs_N_samples.errorbar(all_N_samples_values, results_dict_Acc_vs_N_samples[selected_release_P]['LR_acc_mean'], yerr=results_dict_Acc_vs_N_samples[selected_release_P]['LR_acc_std'], lw=4, ls=':', color='0.45')\nax_accuracy_vs_N_samples.errorbar(all_N_samples_values, results_dict_Acc_vs_N_samples[selected_release_P]['FF_acc_mean'], yerr=results_dict_Acc_vs_N_samples[selected_release_P]['FF_acc_std'], lw=4, ls=':', color='orange')\nax_accuracy_vs_N_samples.errorbar(all_N_samples_values, results_dict_Acc_vs_N_samples[selected_release_P]['IF_acc_mean'], yerr=results_dict_Acc_vs_N_samples[selected_release_P]['IF_acc_std'], lw=4, ls=':', color='0.05')\n\nax_accuracy_vs_N_samples.set_xlabel('Number of Positive Training Patterns - N', fontsize=18)\nax_accuracy_vs_N_samples.set_xticks(all_N_samples_values)\nax_accuracy_vs_N_samples.set_xticklabels(all_N_samples_values, fontsize=15)\nax_accuracy_vs_N_samples.set_ylim(89.5,97.2)\nax_accuracy_vs_N_samples.set_yticks([90,92,94,96]);\nax_accuracy_vs_N_samples.set_yticklabels([90,92,94,96], fontsize=15);\nax_accuracy_vs_N_samples.spines['top'].set_visible(False)\nax_accuracy_vs_N_samples.spines['right'].set_visible(False)\nax_accuracy_vs_N_samples.set_xticks([16,128,256,512,1024]);\nax_accuracy_vs_N_samples.set_xticklabels([16,128,256,512,1024], fontsize=15);\n\n\n# open digit 3 and display the learned weights of the 3 models for it\ndigit = 3\nT = 50\nN_axons = 100\nM_connections = 5\ntemporal_silence_ms = 70\n\nmin_pos_samples = 5000\n\nall_LR_weights = []\nall_FF_weights = []\nall_IF_weights = []\n\nfor k, filename_to_load in enumerate(list_of_files):\n    loaded_script_results_dict = pickle.load(open(filename_to_load, \"rb\" ))\n\n    digit_OK   = loaded_script_results_dict['script_main_params']['positive_digit'] == digit\n    M_OK       = loaded_script_results_dict['script_main_params']['connections_per_axon'] == M_connections\n    N_axons_OK = loaded_script_results_dict['script_main_params']['digit_sample_image_shape_expanded'][0] == N_axons\n    T_OK       = loaded_script_results_dict['script_main_params']['digit_sample_image_shape_expanded'][1] == T\n    N_pos_OK   = loaded_script_results_dict['script_main_params']['num_train_positive_patterns'] >= min_pos_samples\n    ISS_OK     = loaded_script_results_dict['script_main_params']['temporal_silence_ms'] == temporal_silence_ms\n\n    if digit_OK and M_OK and N_axons_OK and T_OK and N_pos_OK and ISS_OK:\n        all_LR_weights.append(loaded_script_results_dict['learned_weights_LR'])\n        all_FF_weights.append(loaded_script_results_dict['learned_weights_FF'])\n        all_IF_weights.append(loaded_script_results_dict['learned_weights_IF'])\n\nall_LR_weights = np.array(all_LR_weights)\nall_FF_weights = np.array(all_FF_weights)\nall_IF_weights = np.array(all_IF_weights)\n\n# the first \"num_const_firing_channels\" axons are a \"bias\" term, so don't show them\nh_start = loaded_script_results_dict['script_main_params']['num_const_firing_channels']\n# the first \"temporal_silence_ms\" time points are silence, so they are boring\nw_start = 39\n\nrand_index = np.random.randint(all_LR_weights.shape[0])\n\nsingle_trial_weights_LR = all_LR_weights[rand_index][h_start:,w_start:]\nsingle_trial_weights_FF = all_FF_weights[rand_index][h_start:,w_start:]\nsingle_trial_weights_IF = all_IF_weights[rand_index][h_start:,w_start:]\n\nmean_weights_LR = all_LR_weights.mean(axis=0)[h_start:,w_start:]\nmean_weights_FF = all_FF_weights.mean(axis=0)[h_start:,w_start:]\nmean_weights_IF = all_IF_weights.mean(axis=0)[h_start:,w_start:]\n\n\ndef get_weight_symmetric_range(weights_matrix):\n    top_value = np.percentile(weights_matrix, 99)\n    bottom_value = np.percentile(weights_matrix, 1)\n    symmetric_range = np.array([-1,1]) * max(np.abs(top_value), np.abs(bottom_value))\n\n    return symmetric_range\n\n\nsymmetric_weight_range = get_weight_symmetric_range(single_trial_weights_LR)\nsymmetric_weight_range = get_weight_symmetric_range(mean_weights_LR)\ncolormap = 'viridis'\nvmin = symmetric_weight_range[0]\nvmax = symmetric_weight_range[1]\n\ngs_learned_weights = gridspec.GridSpec(nrows=2,ncols=3)\ngs_learned_weights.update(left=0.6925, right=0.97, bottom=0.32, top=0.529, wspace=0.06, hspace=0.06)\n\nax_learned_weights_00 = plt.subplot(gs_learned_weights[0,0])\nax_learned_weights_01 = plt.subplot(gs_learned_weights[0,1])\nax_learned_weights_02 = plt.subplot(gs_learned_weights[0,2])\nax_learned_weights_10 = plt.subplot(gs_learned_weights[1,0])\nax_learned_weights_11 = plt.subplot(gs_learned_weights[1,1])\nax_learned_weights_12 = plt.subplot(gs_learned_weights[1,2])\n\nax_learned_weights_00.set_title('Spatio-Temporal\\n Logistic Regression', fontsize=13);\nax_learned_weights_00.set_ylabel('single trial', fontsize=14)\nax_learned_weights_00.imshow(single_trial_weights_LR, vmin=vmin, vmax=vmax, cmap=colormap);\nax_learned_weights_01.imshow(single_trial_weights_FF, vmin=vmin, vmax=vmax, cmap=colormap);\nax_learned_weights_01.set_title('Filter & Fire\\n neuron', fontsize=13)\nax_learned_weights_02.imshow(single_trial_weights_IF, vmin=vmin, vmax=vmax, cmap=colormap);\nax_learned_weights_02.set_title('Integrate & Fire\\n neuron', fontsize=13)\n\nax_learned_weights_10.set_ylabel('mean of %d trials' %(all_LR_weights.shape[0]), fontsize=14)\nax_learned_weights_10.imshow(mean_weights_LR, vmin=vmin, vmax=vmax, cmap=colormap);\nax_learned_weights_11.imshow(mean_weights_FF, vmin=vmin, vmax=vmax, cmap=colormap);\nax_learned_weights_12.imshow(mean_weights_IF, vmin=vmin, vmax=vmax, cmap=colormap);\n\ndef set_xy_ticks_ticklabels_to_None(ax_input):\n    ax_input.set_xticks([])\n    ax_input.set_xticklabels([])\n    ax_input.set_yticks([])\n    ax_input.set_yticklabels([])\n\nset_xy_ticks_ticklabels_to_None(ax_learned_weights_00)\nset_xy_ticks_ticklabels_to_None(ax_learned_weights_01)\nset_xy_ticks_ticklabels_to_None(ax_learned_weights_02)\nset_xy_ticks_ticklabels_to_None(ax_learned_weights_10)\nset_xy_ticks_ticklabels_to_None(ax_learned_weights_11)\nset_xy_ticks_ticklabels_to_None(ax_learned_weights_12)\n","7ec4046e":"save_figures = True\n\n# save figure\nif save_figures:\n    figure_name = 'F&F_MNIST_Figure_3_%d' %(np.random.randint(200))\n    for file_ending in all_file_endings_to_use:\n        if file_ending == '.png':\n            fig.savefig(figure_folder + figure_name + file_ending, bbox_inches='tight')\n        else:\n            fig.savefig(figure_folder + figure_name + file_ending, bbox_inches='tight')\n","c6316766":"## Display the effective learned weights","bc3ee5cd":"## Calculate and Display Spatio-Temporal Logistic Regression Accuracy","9bf5036a":"## Run some simulations before and after learning","c574bc4e":"## Helper functions","2100d157":"## Script params","c631cc3e":"## Transform binary digit images to spatio-temporal spike trains","1b991f6c":"## Crop the data and binarize it","6fd17444":"## Load MNIST dataset and show the data","20c7f4a3":"## Save the figure with several different file formats","abd5d071":"## Build the final figure","5cfcdd77":"## Fit linear model to local currents (fit F&F model)","784706ab":"## Fit a F&F model","f50c862d":"## Make a prediction on the entire test trace","7046c5d5":"## Create a regularized logistic regression baseline","f1fb8d7c":"## Create \"one-vs-all\" dataset","aa4d3b2c":"## Simulate cell and extract normlized currents"}}