{"cell_type":{"7d15d59a":"code","3dd7e051":"code","df21d357":"code","ee17afd6":"code","d1187c20":"code","7bd91913":"code","87fce0d1":"code","6665c9c7":"code","9fe84a01":"code","628277f2":"code","8332bd8e":"code","e46669e4":"code","df7c1031":"code","0a380765":"code","b1651c3b":"code","9a65deac":"code","6571df15":"code","1f612638":"code","7e3a01d0":"code","f606fec6":"code","0b82eb42":"code","1abb2bac":"code","07c3f69b":"code","3f07ce2a":"markdown","702c5000":"markdown","7ebd1722":"markdown","d963ca7a":"markdown","124e49b3":"markdown","d2e904da":"markdown","55135848":"markdown","01be239e":"markdown","4857f6ef":"markdown","e71e28db":"markdown","1ef3e615":"markdown","8abe94aa":"markdown","8470e5c3":"markdown","78e84282":"markdown"},"source":{"7d15d59a":"import warnings\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom scipy.stats.stats import pearsonr\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, recall_score, precision_score\n\n%matplotlib inline\n# Suppress warnings\nwarnings.filterwarnings(\"ignore\")","3dd7e051":"def cross_validate(estimator, train, validation):\n    X_train = train[0]\n    Y_train = train[1]\n    X_val = validation[0]\n    Y_val = validation[1]\n    train_predictions = classifier.predict(X_train)\n    train_accuracy = accuracy_score(train_predictions, Y_train)\n    train_recall = recall_score(train_predictions, Y_train)\n    train_precision = precision_score(train_predictions, Y_train)\n\n    val_predictions = classifier.predict(X_val)\n    val_accuracy = accuracy_score(val_predictions, Y_val)\n    val_recall = recall_score(val_predictions, Y_val)\n    val_precision = precision_score(val_predictions, Y_val)\n\n    print('Model metrics')\n    print('Accuracy  Train: %.2f, Validation: %.2f' % (train_accuracy, val_accuracy))\n    print('Recall    Train: %.2f, Validation: %.2f' % (train_recall, val_recall))\n    print('Precision Train: %.2f, Validation: %.2f' % (train_precision, val_precision))","df21d357":"train_raw = pd.read_csv('..\/input\/train.csv')\ntest_raw = pd.read_csv('..\/input\/test.csv')\ntest_ids = test_raw['PassengerId'].values\n\n# Join data to analyse and process the set as one.\ntrain_raw['train'] = 1\ntest_raw['train'] = 0\ndata = train_raw.append(test_raw, sort=False)","ee17afd6":"data.head()","d1187c20":"data.describe()","7bd91913":"features = ['Age', 'Embarked', 'Fare', 'Parch', 'Pclass', 'Sex', 'SibSp']\ntarget = 'Survived'\n\ndata = data[features + [target] + ['train']]\n# Categorical values need to be transformed into numeric.\ndata['Sex'] = data['Sex'].replace([\"female\", \"male\"], [0, 1])\ndata['Embarked'] = data['Embarked'].replace(['S', 'C', 'Q'], [1, 2, 3])\ndata['Age'] = pd.qcut(data['Age'], 10, labels=False)","87fce0d1":"# Split data into train and test.\ntrain = data.query('train == 1')\ntest = data.query('train == 0')\n\n# Drop missing values from the train set.\ntrain.dropna(axis=0, inplace=True)\nlabels = train[target].values","6665c9c7":"train.head()","9fe84a01":"columns = train[features + [target]].columns.tolist()\nnColumns = len(columns)\nresult = pd.DataFrame(np.zeros((nColumns, nColumns)), columns=columns)\n\n# Apply Pearson correlation on each pair of features.\nfor col_a in range(nColumns):\n    for col_b in range(nColumns):\n        result.iloc[[col_a], [col_b]] = pearsonr(train.loc[:, columns[col_a]], train.loc[:,  columns[col_b]])[0]\n        \nfig, ax = plt.subplots(figsize=(10,10))\nax = sns.heatmap(result, yticklabels=columns, vmin=-1, vmax=1, annot=True, fmt='.2f', linewidths=.2)\nax.set_title('PCC - Pearson correlation coefficient')\nplt.show()","628277f2":"continuous_numeric_features = ['Age', 'Fare', 'Parch', 'SibSp']\nfor feature in continuous_numeric_features:\n    sns.distplot(train[feature])\n    plt.show()","8332bd8e":"train.drop(['train', target, 'Pclass'], axis=1, inplace=True)\ntest.drop(['train', target, 'Pclass'], axis=1, inplace=True)","e46669e4":"X_train, X_val, Y_train, Y_val = train_test_split(train, labels, test_size=0.2, random_state=1)","df7c1031":"X_train.head()","0a380765":"X_train1, X_train2, Y_train1, Y_train2 = train_test_split(X_train, Y_train, test_size=0.3, random_state=12)","b1651c3b":"classifier = GaussianNB()","9a65deac":"classifier.fit(X_train2, Y_train2)","6571df15":"print('Metrics with only 30% of train data')\ncross_validate(classifier, (X_train, Y_train), (X_val, Y_val))","1f612638":"classifier.partial_fit(X_train1, Y_train1)","7e3a01d0":"print('Metrics with the remaining 70% of train data')\ncross_validate(classifier, (X_train, Y_train), (X_val, Y_val))","f606fec6":"print('Probability of each class')\nprint('Survive = 0: %.2f' % classifier.class_prior_[0])\nprint('Survive = 1: %.2f' % classifier.class_prior_[1])","0b82eb42":"print('Mean of each feature per class')\nprint('               Age         Embarked   Fare         Parch       Sex         SibSp')\nprint('Survive = 0: %s' % classifier.theta_[0])\nprint('Survive = 1: %s' % classifier.theta_[1])","1abb2bac":"print('Variance of each feature per class')\nprint('Survive = 0: %s' % classifier.sigma_[0])\nprint('Survive = 1: %s' % classifier.sigma_[1])","07c3f69b":"# Unfortunately sklearn naive Bayes algorithm currently do not make inference with missing data (but should do), so we need to input missing data.\ntest.fillna(test.mean(), inplace=True)\ntest_predictions = classifier.predict(test)\nsubmission = pd.DataFrame({'PassengerId': test_ids})\nsubmission['Survived'] = test_predictions.astype('int')\nsubmission.to_csv('submission.csv', index=False)\nsubmission.head(40)","3f07ce2a":"### Load data","702c5000":"One advantage of Bayesian models is that it works well enough with small data, having more would give you more accurate probabilities but it's not data hungry as something like deep learning.\n\n### Pre-process\n* feature selection, data cleaning, feature engineering and data imputation","7ebd1722":"### Split train data into two parts","d963ca7a":"### Correlation study\n* As we saw Naive Bayes models expect the features to be independent, so let's apply the Pearson correlation coefficient on them to give us a hint about how independent they are from the others.","124e49b3":"#### Fit the first part\n* Fitting data here is really fast.","d2e904da":"<h1>Naive Bayes - Titanic survival<\/h1>\n<h2>Probabilistic Machine Learning<\/h2>\n\n","55135848":"About the correlation between the features, we can see that \"Fare\" and \"Pclass\" seem to be highly related, so i'll remove \"Pclass\". Also features like \"Sex\", \"Pclass\" and \"Fare\" should be good predictors.\n\n### Distribution study\n* Also the model expect the features to come from a Gaussian (or normal) distribution, so let's check that as well.","01be239e":"### Overview the data","4857f6ef":"### Split data in train and validation (80% ~ 20%)","e71e28db":"### Apply the model on the test data and create submission","1ef3e615":"#### Update the model with the second part\n* Nice thing about this kind of model, you can update it by just fitting the model again with more data.","8abe94aa":"Our processed train set","8470e5c3":"As you can see our results improved after we updated  the model with the remaining data.\n\nThe sklearn model also give us some interesting options from the model API about the target class.","78e84282":"Looking at our continuous numeric features we can see that \"Fare\", \"Parch\" and \"SibSp\", have a distribution close to normal, but with a left side skew, \"Age\" have a distribution a a bit different from the other but maybe it's close enough to Gaussian."}}