{"cell_type":{"b556fce4":"code","1ce63d6d":"code","ec2e0206":"code","98b52e81":"code","084af25f":"code","2ff369b7":"code","f486c03a":"code","83061c0c":"code","5f1d0ca4":"code","d4936e99":"code","74f76d3e":"code","218d6878":"code","b2833b1b":"code","d8f7f8e5":"code","d37afac0":"code","11c42734":"code","fffbf202":"code","1c0afa00":"code","6caf0f92":"code","3a1395ae":"code","507ebc78":"code","7d92ffeb":"code","43754f9a":"code","bb4c10c3":"code","52477abb":"code","3a8c5fc5":"code","7a7960aa":"code","3aa97ee8":"code","e9babf88":"code","1dd322f5":"code","163625ff":"code","bab195fa":"markdown","c081b554":"markdown","3955923a":"markdown","2788f909":"markdown","c18c4efc":"markdown","b9bd3527":"markdown","c2407c6d":"markdown","e6ece1e2":"markdown","16f2c6eb":"markdown","75854618":"markdown","b8f0f0d1":"markdown","0f31842e":"markdown","51f13aa2":"markdown","d248e581":"markdown","9ed18753":"markdown","600c2634":"markdown","cbd61524":"markdown","2e8678f3":"markdown","0a42f959":"markdown","e2b63486":"markdown","91746161":"markdown","2b9f4a16":"markdown","6c957bb9":"markdown","9033ea73":"markdown","b88d6b31":"markdown","343e1d82":"markdown","1db69cde":"markdown","7a081e5d":"markdown","ccddc532":"markdown","40d6be45":"markdown","4d1ecb24":"markdown","ab2acb97":"markdown","73225b3b":"markdown","ada16c90":"markdown","19bbb961":"markdown","db7b4c93":"markdown","7b09d581":"markdown","1e0f154f":"markdown","5b38e8ba":"markdown","7649d683":"markdown","260e7d8d":"markdown","f3a27862":"markdown","2a3d43d9":"markdown","794e98b2":"markdown","8073e60c":"markdown","47886283":"markdown"},"source":{"b556fce4":"\"\"\"\n!pip install comet_ml\nfrom comet_ml import Experiment\n\n# Setting the API key (saved as environment variable)\nexperiment = Experiment(api_key=\"THysD8zqvW8wCiFTidV67jLP2\",\n                        project_name=\"climate-change-belief-analysis\", \n                        workspace=\"jamakasilwane\")                      \n\"\"\"","1ce63d6d":"# Standard libraries\nimport re\nimport csv\nimport nltk\nimport spacy\nimport string\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt \n\n# Style\nimport matplotlib.style as style \nsns.set(font_scale=1.5)\nstyle.use('seaborn-pastel')\nstyle.use('seaborn-poster')\nfrom PIL import Image\nfrom wordcloud import WordCloud\n\n# Downloads\nnlp = spacy.load('en')\nnltk.download('punkt')\nnltk.download('wordnet')\nnltk.download('stopwords')\nnltk.download('averaged_perceptron_tagger')\n\n# Preprocessing\nimport en_core_web_sm\nfrom collections import Counter\nfrom nltk.probability import FreqDist\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import WordNetLemmatizer \nfrom nltk.corpus import stopwords, wordnet  \nfrom sklearn.feature_extraction.text import CountVectorizer   \nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer \nfrom sklearn.model_selection import train_test_split, RandomizedSearchCV\n\n# Building classification models\nfrom sklearn.svm import LinearSVC\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\n\n# Model evaluation\nfrom sklearn import metrics\nfrom sklearn.metrics import classification_report, confusion_matrix, f1_score, precision_score, recall_score","ec2e0206":"# import dataset \ntrain = pd.read_csv('..\/input\/climate-change-belief-analysis\/train.csv')\ntest = pd.read_csv('..\/input\/climate-change-belief-analysis\/test.csv')\nsample = pd.read_csv('..\/input\/climate-change-belief-analysis\/sample_submission.csv')","98b52e81":"# Taking general look at both datasets\n\nprint(train.shape)\nprint(test.shape)\n\ndisplay(train.head())\ndisplay(test.head())\n\npercent_duplicates = round((1-(train['message'].nunique()\/len(train['message'])))*100,2)\nprint('Duplicated tweets in train data:')\nprint(percent_duplicates,'%')","084af25f":"def update(df):\n    \n    \"\"\"\n    This function creates a copy of the original train data and \n    renames the classes, converting them from numbers to words\n    \n    Input: \n    df: original dataframe\n        datatype: dataframe\n    \n    Output:\n    df: modified dataframe\n        datatype: dataframe \n        \n    \"\"\"\n\n    df = train.copy()\n    sentiment = df['sentiment']\n    word_sentiment = []\n\n    for i in sentiment :\n        if i == 1 :\n            word_sentiment.append('Pro')\n        elif i == 0 :\n            word_sentiment.append('Neutral')\n        elif i == -1 :\n            word_sentiment.append('Anti')\n        else :\n            word_sentiment.append('News')\n\n    df['sentiment'] = word_sentiment\n    \n    return df\n\ndf = update(train)\ndf.head()","2ff369b7":"def hashtag_extract(tweet):\n    \n    \"\"\"\n    This function takes in a tweet and extracts the top 15 hashtag(s) using regular expressions\n    These hashtags are stored in a seperate dataframe \n    along with a count of how frequenty they occur\n    \n    Input:\n    tweet: original tweets\n           datatype: 'str'\n           \n    Output:\n    hashtag_df: dataframe containing the top hashtags in the tweets\n              datatype: dataframe         \n    \"\"\"\n    \n    hashtags = []\n    \n    for i in tweet:\n        ht = re.findall(r\"#(\\w+)\", i)\n        hashtags.append(ht)\n        \n    hashtags = sum(hashtags, [])\n    frequency = nltk.FreqDist(hashtags)\n    \n    hashtag_df = pd.DataFrame({'hashtag': list(frequency.keys()),\n                       'count': list(frequency.values())})\n    hashtag_df = hashtag_df.nlargest(15, columns=\"count\")\n\n    return hashtag_df\n\n# Extracting the hashtags from tweets in each class\npro = hashtag_extract(df['message'][df['sentiment'] == 'Pro'])\nanti = hashtag_extract(df['message'][df['sentiment'] == 'Anti'])\nneutral = hashtag_extract(df['message'][df['sentiment'] == 'Neutral'])\nnews = hashtag_extract(df['message'][df['sentiment'] == 'News'])\n\npro.head()","f486c03a":"def TweetCleaner(tweet):\n    \n    \"\"\"\n    This function uses regular expressions to remove url's, mentions, hashtags, \n    punctuation, numbers and any extra white space from tweets after converting \n    everything to lowercase letters.\n\n    Input:\n    tweet: original tweet\n           datatype: 'str'\n\n    Output:\n    tweet: modified tweet\n           datatype: 'str'\n    \"\"\"\n    # Convert everything to lowercase\n    tweet = tweet.lower() \n    \n    # Remove mentions   \n    tweet = re.sub('@[\\w]*','',tweet)  \n    \n    # Remove url's\n    tweet = re.sub(r'https?:\\\/\\\/.*\\\/\\w*', '', tweet)\n    \n    # Remove hashtags\n    tweet = re.sub(r'#\\w*', '', tweet)    \n    \n    # Remove numbers\n    tweet = re.sub(r'\\d+', '', tweet)  \n    \n    # Remove punctuation\n    tweet = re.sub(r\"[,.;':@#?!\\&\/$]+\\ *\", ' ', tweet)\n    \n    # Remove that funny diamond\n    tweet = re.sub(r\"U+FFFD \", ' ', tweet)\n    \n    # Remove extra whitespace\n    tweet = re.sub(r'\\s\\s+', ' ', tweet)\n    \n    # Remove space in front of tweet\n    tweet = tweet.lstrip(' ')                        \n    \n    return tweet\n\n# Clean the tweets in the message column\ndf['message'] = df['message'].apply(TweetCleaner)\ndf['message'] = df['message'].apply(TweetCleaner)\n\ndf.head()","83061c0c":"def lemma(df):\n    \n    \"\"\"\n    This function modifies the original train dataframe.\n    A new column for the length of each tweet is added.\n    The tweets are then tokenized and each word is assigned a part of speech tag \n    before being lemmatized\n    \n    Input:\n    df: original dataframe\n        datatype: dataframe \n        \n    Output:\n    df: modified dataframe\n        datatype: dataframe\n    \"\"\"\n    \n    df['length'] = df['message'].str.len()\n    df['tokenized'] = df['message'].apply(word_tokenize)\n    df['pos_tags'] = df['tokenized'].apply(nltk.tag.pos_tag)\n\n    def get_wordnet_pos(tag):\n\n        if tag.startswith('J'):\n            return wordnet.ADJ\n\n        elif tag.startswith('V'):\n            return wordnet.VERB\n\n        elif tag.startswith('N'):\n            return wordnet.NOUN\n\n        elif tag.startswith('R'):\n            return wordnet.ADV\n    \n        else:\n            return wordnet.NOUN\n        \n    wnl = WordNetLemmatizer()\n    df['pos_tags'] = df['pos_tags'].apply(lambda x: [(word, get_wordnet_pos(pos_tag)) for (word, pos_tag) in x])\n    df['lemmatized'] = df['pos_tags'].apply(lambda x: [wnl.lemmatize(word, tag) for word, tag in x])\n    df['lemmatized'] = [' '.join(map(str, l)) for l in df['lemmatized']]  \n    return df\n\ndf = lemma(df)\ndf.head()","5f1d0ca4":"def frequency(tweet):\n    \n    \"\"\"\n    This function determines the frequency of each word in a collection of tweets \n    and stores the 25 most frequent words in a dataframe, \n    sorted from most to least frequent\n    \n    Input: \n    tweet: original tweets\n           datatype: 'str'\n           \n    Output: \n    frequency: dataframe containing the top 25 words \n               datatype: dataframe          \n    \"\"\"\n    \n    # Count vectorizer excluding english stopwords\n    cv = CountVectorizer(stop_words='english')\n    words = cv.fit_transform(tweet)\n    \n    # Count the words in the tweets and determine the frequency of each word\n    sum_words = words.sum(axis=0)\n    words_freq = [(word, sum_words[0, i]) for word, i in cv.vocabulary_.items()]\n    words_freq = sorted(words_freq, key=lambda x: x[1], reverse=True)\n    \n    # Create a dataframe to store the top 25 words and their frequencies\n    frequency = pd.DataFrame(words_freq, columns=['word', 'freq'])\n    frequency = frequency.head(25)\n    \n    return frequency\n\n# Extract the top 25 words in each class\npro_frequency = frequency(df['lemmatized'][df['sentiment']=='Pro'])\nanti_frequency = frequency(df['lemmatized'][df['sentiment']=='Anti'])\nnews_frequency = frequency(df['lemmatized'][df['sentiment']=='News'])\nneutral_frequency = frequency(df['lemmatized'][df['sentiment']=='Neutral'])","d4936e99":"# Extract the words in the tweets for the pro and anti climate change classes \nanti_words = ' '.join([text for text in anti_frequency['word']])\npro_words = ' '.join([text for text in pro_frequency['word']])\nnews_words = ' '.join([text for text in news_frequency['word']])\nneutral_words = ' '.join([text for text in neutral_frequency['word']])\n\n# Create wordcloud for the anti climate change class\nanti_wordcloud = WordCloud(width=800, \n                           height=500, \n                           random_state=110, \n                           max_font_size=110, \n                           background_color='white',\n                           colormap=\"Reds\").generate(anti_words)\n\n# Create wordcolud for the pro climate change class\npro_wordcloud = WordCloud(width=800, \n                          height=500, \n                          random_state=73, \n                          max_font_size=110, \n                          background_color='white',\n                          colormap=\"Greens\").generate(pro_words)\n\n# Create wordcolud for the news climate change class\nnews_wordcloud = WordCloud(width=800, \n                          height=500, \n                          random_state=0, \n                          max_font_size=110, \n                          background_color='white',\n                          colormap=\"Blues\").generate(news_words)\n\n# Create wordcolud for the neutral climate change class\nneutral_wordcloud = WordCloud(width=800, \n                          height=500, \n                          random_state=10, \n                          max_font_size=110, \n                          background_color='white',\n                          colormap=\"Oranges\").generate(neutral_words)\n\npro_frequency.tail()","74f76d3e":"def entity_extractor(tweet):\n    \n    \"\"\"\n    This function extracts the top 10 people, organizations and geopolitical entities \n    in a collection of tweets. \n    The information is then saved in a new dataframe\n\n    Input:\n    tweet: lemmatized tweets\n           datatype: 'str'\n\n    Output:\n    df: dataframe containing the top 10 people, organizations and gpe's in a collection of tweets\n        datatype: dataframe ('str')\n    \"\"\"\n    \n    def get_people(tweet):  \n        words = nlp(tweet)\n        people = [w.text for w in words.ents if w.label_== 'PERSON']\n        return people\n    \n    def get_org(tweet):\n        words = nlp(tweet)\n        org = [w.text for w in words.ents if w.label_== 'ORG']\n        return org\n    \n    def get_gpe(tweet):\n        words = nlp(tweet)\n        gpe = [w.text for w in words.ents if w.label_== 'GPE']\n        return gpe\n    \n    # Extract the top 10 people\n    people = tweet.apply(lambda x: get_people(x)) \n    people = [x for sub in people for x in sub]\n    people_counter = Counter(people)\n    people_count = people_counter.most_common(10)\n    people, people_count = map(list, zip(*people_count))\n    \n    # Extract the top 10 organizations\n    org = tweet.apply(lambda x: get_org(x)) \n    org = [x for sub in org for x in sub]\n    org_counter = Counter(org)\n    org_count = org_counter.most_common(10)\n    org, org_count = map(list, zip(*org_count))\n    \n    # Extract the top 10 geopolitical entities\n    gpe = tweet.apply(lambda x: get_gpe(x)) \n    gpe = [x for sub in gpe for x in sub]\n    gpe_counter = Counter(gpe)\n    gpe_count = gpe_counter.most_common(10)\n    gpe, gpe_count = map(list, zip(*gpe_count))\n    \n    # Create a dataframe to store the information\n    df = pd.DataFrame({'people' : people})\n    df['geopolitics'] = gpe\n    df['organizations'] = org\n    \n    return df\n\n# Extract top entities for each class\nanti_info = entity_extractor(df['lemmatized'][df['sentiment']=='Anti'])\npro_info = entity_extractor(df['lemmatized'][df['sentiment']=='Pro'])\nnews_info = entity_extractor(df['lemmatized'][df['sentiment']=='News'])\nneutral_info = entity_extractor(df['lemmatized'][df['sentiment']=='Neutral'])","218d6878":"# Display target distribution\nstyle.use('seaborn-pastel')\n\nfig, axes = plt.subplots(ncols=2, \n                         nrows=1, \n                         figsize=(20, 10), \n                         dpi=100)\n\nsns.countplot(df['sentiment'], ax=axes[0])\n\nlabels=['Pro', 'News', 'Neutral', 'Anti'] \n\naxes[1].pie(df['sentiment'].value_counts(),\n            labels=labels,\n            autopct='%1.0f%%',\n            shadow=True,\n            startangle=90,\n            explode = (0.1, 0.1, 0.1, 0.1))\n\nfig.suptitle('Tweet distribution', fontsize=20)\nplt.show()","b2833b1b":"# Plot the distribution of the length tweets for each class using a box plot\nsns.boxplot(x=df['sentiment'], y=df['length'], data=df, palette=(\"Blues_d\"))\nplt.title('Tweet length for each class')\nplt.show()","d8f7f8e5":"# Plot pro and anti wordclouds next to one another for comparisson\nf, axarr = plt.subplots(2,2, figsize=(35,25))\naxarr[0,0].imshow(pro_wordcloud, interpolation=\"bilinear\")\naxarr[0,1].imshow(anti_wordcloud, interpolation=\"bilinear\")\naxarr[1,0].imshow(neutral_wordcloud, interpolation=\"bilinear\")\naxarr[1,1].imshow(news_wordcloud, interpolation=\"bilinear\")\n\n# Remove the ticks on the x and y axes\nfor ax in f.axes:\n    plt.sca(ax)\n    plt.axis('off')\n\naxarr[0,0].set_title('Pro climate change\\n', fontsize=35)\naxarr[0,1].set_title('Anti climate change\\n', fontsize=35)\naxarr[1,0].set_title('Neutral\\n', fontsize=35)\naxarr[1,1].set_title('News\\n', fontsize=35)\n#plt.tight_layout()\nplt.show()\n\nprint(\"Pro climate change buzzwords 20-25 shown here for clarity \\n- The wordcloud doesn't seem to pick up on 'http'\")\ndisplay(pro_frequency.tail())","d37afac0":"# Plot the frequent hastags for pro and anti climate change classes\nsns.barplot(data=pro,y=pro['hashtag'], x=pro['count'], palette=(\"Blues_d\"))\nplt.title('Frequent PRO climate change hashtags')\nplt.tight_layout()","11c42734":"sns.barplot(data=anti,y=anti['hashtag'], x=anti['count'], palette=(\"Blues_d\"))\nplt.title('Frequent ANTI climate change hashtags')\nplt.tight_layout()","fffbf202":"# Plot the frequent hastags for the news and neutral classes\nsns.barplot(y=news['hashtag'], x=news['count'], palette=(\"Blues_d\"))\nplt.title('Frequent climate change NEWS hashtags')\nplt.tight_layout()","1c0afa00":"sns.barplot(y=neutral['hashtag'], x=neutral['count'], palette=(\"Blues_d\"))\nplt.title('Frequent NEUTRAL climate change hashtags')\nplt.tight_layout()","6caf0f92":"print('Pro climate change information')\ndisplay(pro_info.head(9))\n","3a1395ae":"print('Anti climate change information')\ndisplay(anti_info)","507ebc78":"# Split the dataset into train & validation (25%) for model training\n\n# Seperate features and tagret variables\nX = train['message']\ny = train['sentiment']\n\n# Split the train data to create validation dataset\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.25, random_state=42)","7d92ffeb":"# Random Forest Classifier\nrf = Pipeline([('tfidf', TfidfVectorizer()),\n               ('clf', RandomForestClassifier(max_depth=5, \n                                              n_estimators=100))])\n\n# Na\u00efve Bayes:\nnb = Pipeline([('tfidf', TfidfVectorizer()),\n               ('clf', MultinomialNB())])\n\n# K-NN Classifier\nknn = Pipeline([('tfidf', TfidfVectorizer()),\n                ('clf', KNeighborsClassifier(n_neighbors=5, \n                                             metric='minkowski', \n                                             p=2))])\n\n# Logistic Regression\nlr = Pipeline([('tfidf',TfidfVectorizer()),\n               ('clf',LogisticRegression(C=1, \n                                         class_weight='balanced', \n                                         max_iter=1000))])\n# Linear SVC:\nlsvc = Pipeline([('tfidf', TfidfVectorizer()),\n                 ('clf', LinearSVC(class_weight='balanced'))])","43754f9a":"# Random forest \nrf.fit(X_train, y_train)\ny_pred_rf = rf.predict(X_valid)\n\n# Niave bayes\nnb.fit(X_train, y_train)\ny_pred_nb = nb.predict(X_valid)\n\n# K - nearest neighbors\nknn.fit(X_train, y_train)\ny_pred_knn = knn.predict(X_valid)\n\n# Linear regression\nlr.fit(X_train, y_train)\ny_pred_lr = lr.predict(X_valid)\n\n# Linear SVC\nlsvc.fit(X_train, y_train)\ny_pred_lsvc = lsvc.predict(X_valid)","bb4c10c3":"# Generate a classification Report for the random forest model\nprint(metrics.classification_report(y_valid, y_pred_rf))\n\n# Generate a normalized confusion matrix\ncm = confusion_matrix(y_valid, y_pred_rf)\ncm_norm = cm \/ cm.sum(axis=1).reshape(-1,1)\n\n# Display the confusion matrix as a heatmap\nsns.heatmap(cm_norm, \n            cmap=\"YlGnBu\", \n            xticklabels=rf.classes_, \n            yticklabels=rf.classes_, \n            vmin=0., \n            vmax=1., \n            annot=True, \n            annot_kws={'size':10})\n\n# Adding headings and lables\nplt.title('Random forest classification')\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\nplt.show()\n","52477abb":"# Generate a classification Report for the Naive Bayes model\nprint(metrics.classification_report(y_valid, y_pred_nb))\n\n# Generate a normalized confusion matrix\ncm = confusion_matrix(y_valid, y_pred_nb)\ncm_norm = cm \/ cm.sum(axis=1).reshape(-1,1)\n\n# Display the confusion matrix as a heatmap\nsns.heatmap(cm_norm, \n            cmap=\"YlGnBu\", \n            xticklabels=nb.classes_, \n            yticklabels=nb.classes_, \n            vmin=0., \n            vmax=1., \n            annot=True, \n            annot_kws={'size':10})\n\n# Adding headings and lables\nplt.title('Naive Bayes classification')\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\nplt.show()\n","3a8c5fc5":"# Generate a classification Report for the K-nearest neighbors model\nprint(metrics.classification_report(y_valid, y_pred_knn))\n\n# Generate a normalized confusion matrix\ncm = confusion_matrix(y_valid, y_pred_knn)\ncm_norm = cm \/ cm.sum(axis=1).reshape(-1,1)\n\n# Display the confusion matrix as a heatmap\nsns.heatmap(cm_norm, \n            cmap=\"YlGnBu\", \n            xticklabels=knn.classes_, \n            yticklabels=knn.classes_, \n            vmin=0., \n            vmax=1., \n            annot=True, \n            annot_kws={'size':10})\n\n# Adding headings and lables\nplt.title('K - nearest neighbors classification')\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\nplt.show()","7a7960aa":"# Generate a classification Report for the model\nprint(metrics.classification_report(y_valid, y_pred_lr))\n\ncm = confusion_matrix(y_valid, y_pred_lr)\ncm_norm = cm \/ cm.sum(axis=1).reshape(-1,1)\n\nsns.heatmap(cm_norm, \n            cmap=\"YlGnBu\", \n            xticklabels=lr.classes_, \n            yticklabels=lr.classes_, \n            vmin=0., \n            vmax=1., \n            annot=True, \n            annot_kws={'size':10})\n\n# Adding headings and lables\nplt.title('Logistic regression classification')\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\nplt.show()","3aa97ee8":"# Generate a classification Report for the linear SVC model\nprint(metrics.classification_report(y_valid, y_pred_lsvc))\n\n# Generate a normalized confusion matrix\ncm = confusion_matrix(y_valid, y_pred_lsvc)\ncm_norm = cm \/ cm.sum(axis=1).reshape(-1,1)\n\n# Display the confusion matrix as a heatmap\nsns.heatmap(cm_norm, \n            cmap=\"YlGnBu\", \n            xticklabels=lsvc.classes_, \n            yticklabels=lsvc.classes_, \n            vmin=0., \n            vmax=1., \n            annot=True, \n            annot_kws={'size':10})\n\n# Adding headings and lables\nplt.title('Linear SVC classification')\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\nplt.show()","e9babf88":"# This code is intentionally commented out - Code takes >10 minutes to run. \n\n\"\"\"\n# Set ranges for the parameters that we want to tune\nparams = {'clf__C': [0.1, 0.2, 0.3, 0.4, 0.5],\n          'tfidf__ngram_range' : [(1,1),(1,2)],\n          'clf__max_iter': [1500, 2000, 2500, 3000],\n          'tfidf__min_df': [2, 3, 4],\n          'tfidf__max_df': [0.8, 0.9]}\n\n# Perform randomized search & extract the optimal parameters\nRandomized = RandomizedSearchCV(text_clf_lsvc, param_distributions=params, cv=5, scoring='accuracy', n_iter=5, random_state=42)\nRandomized.fit(X_train,y_train)\nRandomized.best_estimator_\n\"\"\"\n\n# Retrain linear SVC using optimal hyperparameters:\nlsvc_op = Pipeline([('tfidf', TfidfVectorizer(max_df=0.8,\n                                                    min_df=2,\n                                                    ngram_range=(1,2))),\n                  ('clf', LinearSVC(C=0.3,\n                                    class_weight='balanced',\n                                    max_iter=3000))])\n\n# Fit and predict\nlsvc_op.fit(X_train, y_train)\ny_pred = lsvc_op.predict(X_valid)\n\nprint('F1 score improved by',\n      round(100*((metrics.accuracy_score(y_pred, y_valid) - metrics.accuracy_score(y_pred_lsvc, y_valid)) \/metrics.accuracy_score(y_pred_lsvc, y_valid)),0), \n      '%')\n","1dd322f5":"\"\"\"\n# Saving each metric to add to a dictionary for logging\nf1 = f1_score(y_valid, y_pred, average='weighted')\nprecision = precision_score(y_valid, y_pred, average='weighted')\nrecall = recall_score(y_valid, y_pred, average='weighted')\n\n# Create dictionaries for the data we want to log          \nmetrics = {\"f1\": f1,\n           \"recall\": recall,\n           \"precision\": precision}\n\nparams= {'classifier': 'linear SVC',\n         'max_df': 0.8,\n         'min_df': 2,\n         'ngram_range': '(1,2)',\n         'vectorizer': 'Tfidf',\n         'scaling': 'no',\n         'resampling': 'no',\n         'test_train random state': '0'}\n  \n# Log info on comet\nexperiment.log_metrics(metrics)\nexperiment.log_parameters(params)\n\n# End experiment\nexperiment.end()\n\n# Display results on comet page\nexperiment.display()\n\n\"\"\"","163625ff":"test = pd.read_csv('..\/input\/climate-change-belief-analysis\/test.csv')\ny_test = lsvc_op.predict(test['message'])\noutput = pd.DataFrame({'tweetid': test.tweetid,\n                       'sentiment': y_test})\noutput.to_csv('submission.csv', index=False)\noutput","bab195fa":"### Observations: \n- The hashtags in the news category are less emotive and aim to bring awareness to high profile topics related to climate change that are or were trending in the news. Examples of such hashtags include News and Science which would be used to indicate that the tweet contains information from a news outlet or a scientific study. \n\n- GreatBarrierReef, Environment and ClimateChange are more examples of hashtags that are more aimed at drawing attention to and sharing information around climate change\n\n- ParisAgreement, COP22 and Trump are popular hashtags. Trump made headlines when he pulled out of the climate agreement, so it makes sense that these hashtags would be trending in climate change news. \n\n- ClimateMarch - many protests have been held, some even global, to raise awareness about climate change. These protests usually make headlines and are featured on news sites. ","c081b554":"## Random forest classification\nRandom Forest is a tree-based machine learning algorithm that leverages the power of multiple decision trees for making decisions. As the name suggests, it is a \u201cforest\u201d of trees!\n\nThe following diagram is a visual representation of the random forest classification method:\n\n![1*58f1CZ8M4il0OZYg2oRN4w.png](https:\/\/miro.medium.com\/max\/1400\/1*58f1CZ8M4il0OZYg2oRN4w.png)","3955923a":"<a id=\"subsection-two\"><\/a>\n## Hashtag extraction\nHashtags are extracted from the original  tweets and stored in seperate dataframes for each class. This is done before tweet cleaning to ensure no information is lost.","2788f909":"<a id=\"section-eight\"><\/a>\n# Hyperparameter tuning \nOnce our top performing model has been selected, we attempt to improve it by performing some hyperparameter tuning.\n\nAfter the optimal parameters are determined the linear SVC model is retrained using these parameters, resulting in a 2% increase in the F1 score.","c18c4efc":"<a id=\"section-two\"><\/a>\n# First glance at the raw data\nThe test and train data contain more than 10000 tweets... That's a lot of words!\n\nThe tweets are divided into 4 classes:\n\n* [ 2 ] News : Tweets linked to factual news about climate change.\n\n* [ 1 ] Pro : Tweets that support the belief of man-made climate change.\n\n* [ 0 ] Neutral : Tweets that neither support nor refuse beliefs of climate change.\n\n*  [-1 ] Anti : Tweets that do not support the belief of man-made climate change.\n\nRetweets account for 10% of the train data","b9bd3527":"<a id=\"subsection-three\"><\/a>\n## Climate change buzzwords\nThe figures below display the 25 most common words found in the tweets for each classes. \n","c2407c6d":"<a id=\"subsection-one\"><\/a>\n## Target variable distribution \nTaking a closer look at the distribution of the tweets we notice that the data is severely imbalanced with the majority of tweets falling in the 'pro' category, supporting the belief of man-made climate change.","e6ece1e2":"# Table of contents:\n- [Import libraries and datasets](#section-one)\n- [First glance at the raw data](#section-two)\n- [Preprocessing](#section-three)\n  * [Create copy](#subsection-one)\n  * [Hashtag extraction](#subsection-two)\n  * [Tweet cleaning](#subsection-three)\n  * [Parts of speech tagging and lemmatization](#subsection-four)\n  * [Word frequency](#subsection-five)\n  * [Specific named entity extraction](#subsection-six)\n- [Exploratory data analysis](#section-four)\n  * [Target variable distribution](#subsection-one)\n  * [Tweet length distribution](#subsection-two)\n  * [Climate change buzzwords](#subsection-three)\n  * [Hashtags](#subsection-four)\n  * [People, places and organizations to watch](#subsection-five)\n- [Building classification models](#section-five)\n  * [Train-validation split](#subsection-one)\n  * [Pipelines](#subsection-three)\n  * [Train models](#subsection-four)\n- [Model evaluation](#section-six)\n  * [Random forest](#subsection-one)\n  * [Naive Bayes](#subsection-two)\n  * [K nearest neighbors](#subsection-three)\n  * [Logistic regression](#subsection-four)\n  * [Linear SVC](#subsection-five)\n- [Model Selection](#section-seven)\n- [Hyperparameter tuning](#section-eight)\n- [Submission](#section-nine)\n- [Conclusion](#section-ten)\n- [Acknowledgements](#section-eleven)\n\n<img src=\"https:\/\/i.gifer.com\/RD07.gif\">\n","16f2c6eb":"## K Nearest Neighbors classification\nThe KNN algorithm uses \u2018feature similarity\u2019 to predict the values of any new data points. This means that the new point is assigned a value based on how closely it resembles the points in the training set.\n\nThere are various methods for calculating how closely the new datapoint resembles the points in the training set, of which the most commonly known methods are \u2013 Euclidian, Manhattan (for continuous) and Hamming distance (for categorical).\n\n\nNew datapoint( Ci ) and multi-classes {1,2,3}. \n\nKNN Classifier would use one of the distance criteria to classify the new datapoint class.\n\n![A-typical-example-of-a-KNN-classification-for-a-two-class-problem-ie-the-pink-and.png](https:\/\/adrianromano.com\/wp-content\/uploads\/2019\/02\/A-typical-example-of-a-KNN-classification-for-a-two-class-problem-ie-the-pink-and.png)\n","75854618":"### Observations:\n\n- The top 3 buzzwords accross all classes are climate change and rt (retweet). The frequency of rt ( Retweet ) means that a lot of the same information and\/or opinions are being shared and viewed by large audiences. This is true for all 4 classes\n\n- 'Trump' is a frequently occuring word in all 4 classes. This is unsurprising given his controversial view on the topic.\n\n- Words like real, believe, think, fight, etc. occur frequently in pro climate change tweets. In contrast, anti climate change tweets contain words such as 'hoax', 'scam', 'tax', 'liberal' and 'fake'. There is a stark difference in tone and use of emotive language in these 2 sets of tweets. From this data we could reason that people who are anti climate change believe that global warming is a 'hoax' and feel negatively towards a tax\u2013based approach to slowing global climate change\n\n- words like 'science' and 'scientist' occur frequently as well which could imply that people are tweeting about scientific studies that support their views on climate change.\n\n- EPA, the United States Environmental Protection Agency is another climate change 'buzzword' that appears frequently across classes.\n\n- https occurs frequently in pro climate change tweets, implying that many links are being shared around the topic of climate change. These could be links to  petitions, websites and\/or articles related to climate change. Interesting to note: https only occurs in the top 25 words for the pro climate change class. Why aren't we seeing more links in the news class?\n","b8f0f0d1":"### Observations: \n\n- Donald Trump is the top mentioned person in pro climate change tweets. This could be because many climate change activists and people who feel strongly about climate change have a lot to say about Trump and his opinions regarding climate change. People tend to tweet more when it comes to things they are unhappy about, want to change or create awareness about. \n\n- Another top mentioned person is Edward Scott Pruitt, an American lawyer, lobbyist and Republican politician. Pruitt served as the Administrator of the Environmental Protection Agency (EPA) in 2017 which is the top mentioned organization in pro climate change tweets.\n\n- The GOP is the 4th most mentioned organization which is a nickname for the the Republican Party. This could be because Republicans have become less convinced over time that the effects of pollution from human activities are the cause of climate change. In a 2019 Gallup poll, 89% of Democrats compared to 34% of Republicans said they believe increases in the Earth\u2019s temperature are due more to the effects of pollution from human activities than because of natural changes in the environment. These tweets mentioning the republican party could in fact be criticism from Democrats.\n\n- The 5th most mentioned organization is Exxon, an oil and natural resource company. Among the oil spills that occurred in the last five decades, Exxon Valdez Oil Spill remains a prominent one. In the accident that took place almost 30 years ago, over 11 million gallons of crude oil was released into the waters of the Gulf of Alaska, hurting the ecosystem badly as it killed hundreds of thousands of species. Exxon has previously been under investigation for potential fraud by withholding information on the role of fossil fuels in driving up temperatures. Considering this information, it comes as no surprise that Exxon is trending in pro climate change tweets for all the wrong reasons.\n\n- The natural resource defense council and united nations are also frequently mentioned organizations in pro climate change tweets. The NRDC is a non-profit international environmental advocacy group using law, science and the support of more than 2 million members and online activists to protect the planet's wildlife.\n\n- America and China are responsible for 40% of the world's carbon emissions and are the most mentioned geopolitical entities in pro climate change tweets, most likely for this reason.","0f31842e":"<a id=\"section-seven\"><\/a>\n# Model Selection \n\n### Linear SVC has achieved the highest F1 score of 0.75 and is therefore our model of choice moving forward.\n","51f13aa2":"<a id=\"section-four\"><\/a>\n# Exploratory data analysis \n### *Torture the data and it will confess to everything...*\n\n![6a0120a5e84a91970c0223c8514578200c-500wi](https:\/\/acuate.typepad.com\/.a\/6a0120a5e84a91970c0223c8514578200c-500wi)","d248e581":"<a id=\"section-six\"><\/a>\n# Model evaluation\nThe performance of each model will be evaluated based on the precision, accuracy and F1 score achieved when the model is used to predict the classes for the validation data. We will be looking at the following to determine and visualize these metrics:\n- Classification report \n- Confusion matrix\n\nThe best model will be selected based on the weighted F1 score. ","9ed18753":"### Observations: \n\n- One of the most popular hashtags used in pro climate change tweets is 'BeforeTheFlood' which refers to a 2016 documentary where actor Leonardo DiCaprio meets with scientists, activists and world leaders to discuss the dangers of climate change and possible solutions.\n\n- We also see a lot of ImVotingBecause and IVotedBecause hashtags in pro climate change tweets. For Democrats, climate change is now one of the two most important issues in politics, according to a new poll. Among all voters, the warming planet is now one of the most important issues in American politics. The poll was conducted by Climate Nexus, a nonpartisan nonprofit group, in partnership with researchers at Yale and George Mason University, and included nearly 2,000 registered voters.\n\n- COP22, ParisAgreement and Trump are the 5th, 6th and 8th most used hashtags in pro climate change tweets. Last year Trump\u2019s administration formally began the process to exit the climate deal, in which nearly 200 nations pledged to reduce greenhouse gas emissions and assist poor nations struggling with the consequences of a warming Earth. Tweets that include these hashtags are most likely centered around peoples' opinions and criticism of Trump's decision and the implications thereof.\n\n- An interesting hashtag that made the top 15 is auspol which is short for Australian politics. Scientists have published the first assessment quantifying the role of climate change in the recent Australian bushfires. Global warming boosted the risk of the hot, dry weather that's likely to cause bushfires by at least 30%, they say. But the study suggests the figure is likely to be much greater. It says that if global temperatures rise by 2C, as seems likely, such conditions would occur at least four times more often. The analysis has been carried out by the World Weather Attribution consortium. Furthermore, Australia is ranked third in the world for climate change denial behind the United States and Sweden. Australian Prime Minister Scott Morrison, who has resisted calls for the country to reduce its carbon emissions, has been accused of deemphasizing the the link between the bushfires and climate change, saying during a 2019 November interview that there isn\u2019t \u201ccredible scientific evidence\u201d that curbing emissions would diminish the fires. Tweets including this hashtag could be trying to raise awareness about the link between a warming earth and bushfires in Australia. \n","600c2634":"<a id=\"subsection-three\"><\/a>\n## Tweet cleaning \nRemove 'noisy entities' such as URL's, punctuations, mentions, numbers and extra white space. The data is further normalized by converting all letters to lowercase.\n","cbd61524":"### Observations:\n-  KNN is able to successfully classify the tweets.\n-  This model also classifies most tweets as pro climate change but to a smaller degree compared to the previous 2 models.\n-  The precision, accuracy and F1 scores have improved significantly for the pro, anti and neutral classes.\n-  There is a drop in the F1 score for the pro climate change class as the predictions become more balanced.\n-  The overall F1 score is 0.69 which is very close to our target but not there yet... The search continues. ","2e8678f3":"### Observations:\n- Linear SVC is able to successfully classify the tweets.\n- This model classifies most tweets successfully with clear boundaries and less confusion  surrounding the pro climate change class compared to the first 3 models.\n- This model shows a higher degree of confusion surrounding the pro class compared to logistic regression.\n- This, however, leads to an increase in the precision, accuracy and f1 score for the pro class which makes up the majority of the tweets.\n- Linear SVC has achieved the highest F1 score of 0.75 ","0a42f959":"<a id=\"subsection-five\"><\/a>\n## People, places and organizations to watch\nWe performed NER on the train data in order to determine the most mentioned people, organizations and geopolitical entities in each class. This lead us to some interesting insights:","e2b63486":"\n### Observations: \n- Interestingly enough, Trump does not feature in the top 10 mentioned people in anti climate change tweets, instead the most mentioned person is Al Gore, American politician, environmentalist and former vice president. Al Gore is pro climate change and vocal about it, having published a number of books on the topic. It's possible that his publications are a popular topic of discussion and his ideas are being critistized by the anti climate change community. \n\n- Former Trump adviser, Steve Bannon is the 5th most mentioned person in anti climate change tweets. Stephen Bannon has called government support of alternative energy \"madness.\" His conservative website, Breitbart News, relentlessly pursues the idea that global warming is an invention of activists, university researchers and renewable energy industry profiteers determined to assert global governance for their own gain. Anti climate change tweets could be sharing and retweeting a lot of his views and stories from his website.\n\n- Golden eagle medium claim fictional - Thousands of golden eagles have been killed by wind turbines. A 2013 study published in The Wildlife Society Bulletin found that wind turbines killed an estimated 573,000 birds annually in the United States. And that figure was 7 years ago. Some of these anti climate change tweets could be raising concern around wind turbines and the danger they pose to these birds. \n","91746161":"## Logistic regression classification\nLogistic Regression uses the probability of a data point to belonging to a certain class to classify each datapoint to it's best estimated class\n\nLogistic regression has been rated as the best performing model for linearly separable data especially if it's predicting binary data(Yes & NO or 1 & 0), and performs better when there's no class imbalance. \n\nThe figure below is the sigmoid function logistic regression models use to make predictions:\n\n![1*a5QwiyaSyvRa6n3VKYVEnQ.png](https:\/\/cdn-media-1.freecodecamp.org\/images\/1*a5QwiyaSyvRa6n3VKYVEnQ.png)\n","2b9f4a16":"<a id=\"subsection-one\"><\/a>\n### Train - Validation split\nBefore we pass our data through our custom pipelines we have to split our train data into features and target variables. After this step we can split our train data into a train and validation set. This will allow us to evaluate our model performance and chose the best model to use for our submission","6c957bb9":"### Observations:\n- Although the Naive Bayes model is a slight improvement on the random forest model it still performs poorly\n- This model classifies most tweets as pro climate change with improved predictions for the news class.\n- The precision, accuracy and F1 scores have improved significantly for the news class but remain low for neutral and anti.\n- The overall F1 score is 0.63. Again this score could only be achieved  since the majority of tweets are in fact pro climate change. ","9033ea73":"## Linear SVC classification\nIn the SVM algorithm, we plot each data item as a point in n-dimensional space (where n is number of features you have) with the value of each feature being the value of a particular coordinate. The goal of the SVM algorithm is to create the best line or decision boundary that can seperate n-dimensional space into classes so that we can easily put the new data point in the correct category in the future. This best decision boundary is called a hyperplane.\n\nSVM chooses the extreme points\/vectors that help in creating the hyperplane. These extreme cases are called as support vectors. Consider the below diagram in which there are two different categories that are classified using a decision boundary or hyperplane:\n\n![support-vector-machine-algorithm.png](https:\/\/static.javatpoint.com\/tutorial\/machine-learning\/images\/support-vector-machine-algorithm.png)","b88d6b31":"<a id=\"section-eleven\"><\/a>\n# Acknowledgements \n### \"We stand on the shoulders of giants...\"\n\n- Kashyap, A. (2017). Top 5 tricks to make plots look better. [online] Medium. Available at: https:\/\/medium.com\/@andykashyap\/top-5-tricks-to-make-plots-look-better-9f6e687c1e08 [Accessed 17 Jun. 2020].\n\n- Stack Abuse. (n.d.). Python for NLP: Parts of Speech Tagging and Named Entity Recognition. [online] Available at: https:\/\/stackabuse.com\/python-for-nlp-parts-of-speech-tagging-and-named-entity-recognition\/ [Accessed 17 Jun. 2020].\n\n- Dataquest. (2019). Classify Text Using spaCy. [online] Available at: https:\/\/www.dataquest.io\/blog\/tutorial-text-classification-in-python-using-spacy\/ [Accessed 17 Jun. 2020].\n\n- Stack Overflow. (n.d.). python - Capture organization names from a dataframe. [online] Available at: https:\/\/stackoverflow.com\/questions\/55956671\/capture-organization-names-from-a-dataframe [Accessed 17 Jun. 2020].\n\n- Dr. Sebastian Raschka. (2014). Turn Your Twitter Timeline into a Word Cloud. [online] Available at: https:\/\/sebastianraschka.com\/Articles\/2014_twitter_wordcloud.html [Accessed 17 Jun. 2020].\n\n- matplotlib.org. (n.d.). Basic pie chart \u2014 Matplotlib 3.1.2 documentation. [online] Available at: https:\/\/matplotlib.org\/3.1.1\/gallery\/pie_and_polar_charts\/pie_features.html#sphx-glr-gallery-pie-and-polar-charts-pie-features-py [Accessed 17 Jun. 2020].\n\n- Beri, A. (2020). Stemming vs Lemmatization. [online] Medium. Available at: https:\/\/towardsdatascience.com\/stemming-vs-lemmatization-2daddabcb221 [Accessed 17 Jun. 2020]\n\n- kaggle.com. (n.d.). Create Table of Contents in a Notebook. [online] Available at: https:\/\/www.kaggle.com\/dcstang\/create-table-of-contents-in-a-notebook [Accessed 18 Jun. 2020].\n\n- Analytics Vidhya (2019). 6 Easy Steps to Learn Naive Bayes Algorithm (with code in Python). [online] Analytics Vidhya. Available at: https:\/\/www.analyticsvidhya.com\/blog\/2017\/09\/naive-bayes-explained\/.\n\n- www.javatpoint.com. (n.d.). Support Vector Machine (SVM) Algorithm - Javatpoint. [online] Available at: https:\/\/www.javatpoint.com\/machine-learning-support-vector-machine-algorithm [Accessed 18 Jun. 2020].\n\n- R, A. (2018). APPLYING RANDOM FOREST (CLASSIFICATION) \u2014 MACHINE LEARNING ALGORITHM FROM SCRATCH WITH REAL\u2026. [online] Medium. Available at: https:\/\/medium.com\/@ar.ingenious\/applying-random-forest-classification-machine-learning-algorithm-from-scratch-with-real-24ff198a1c57 [Accessed 18 Jun. 2020].\n\n- SUNIL RAY. (2017) [online] Available at: Understanding Support Vector Machine(SVM) algorithm from examples (along with code) https:\/\/www.analyticsvidhya.com\/blog\/2017\/09\/understaing-support-vector-machine-example-code\/ [Accessed 18 June 2020]\n\n- Bowman, K. (n.d.). Democrats And Republicans Divided On Climate Change. [online] Forbes. Available at: https:\/\/www.forbes.com\/sites\/bowmanmarsico\/2019\/04\/19\/democrats-and-republicans-divided-on-climate-change\/#67dc91163198 [Accessed 19 Jun. 2020].\n\n- The Independent. (2019). More than half of people say climate change will influence how they vote in general election. [online] Available at: https:\/\/www.independent.co.uk\/environment\/climate-change-crisis-latest-general-election-green-party-vote-boris-johnson-a9175756.html [Accessed 19 Jun. 2020].\n\n- Anon, (n.d.). Protect Eagles from Wind Turbine Fatalities \u2013 American Eagle Foundation. [online] Available at: https:\/\/www.eagles.org\/take-action\/wind-turbine-fatalities\/.\n\n<img src=\"https:\/\/www.afb.org\/serve\/media\/7406\">","343e1d82":"# Start Comet experiment\nWe will be using Comet as a form of version control throughout the development of our model","1db69cde":"### Observations:\n\n- Logistic regression is able to successfully classify the tweets.\n- This model classifies most tweets successfully with clear boundaries and less confusion  surrounding the pro climate change class.\n- The precision, accuracy and F1 scores have improved significantly for the pro, anti and neutral classes.\n- There is a drop in the F1 score for the pro climate change class as the predictions become more balanced.\n- The overall F1 score is 0.71 which is on target. Let's see if we can improve. ","7a081e5d":"<a id=\"subsection-one\"><\/a>\n## Create copy\nThe first step in the preprocessing is to create a copy of the train dataframe for the EDA. The original dataframe will therefore be preserved. We proceed to rename the classes, converting the labels from numbers to the words they represent. This will make creating visuals with appropriate labels easier.","ccddc532":"<a id=\"subsection-three\"><\/a>\n### Train the models\nThe models are trained by passing the train data through each custom pipeline. The trained models are then used to predict the classes for the validation data set.\n","40d6be45":"<a id=\"section-nine\"><\/a>\n# Submission","4d1ecb24":"### Observations:\n-  From the confusion matrix above we notice that the random forest classification model does a very poor job on our data set. The model classifies all the tweets as pro climate change tweets.\n-  This results in precision, recall and F1 scores of zero for the anti, neutral and news classes.\n-  Tree based classification models are especially vulnerable to overfitting when the train data is imbalanced which is the case with our data. The model could be greatly improved by using resampling techniques such as oversampling the anti class and\/or undersampling the pro class. This will allow the model to learn how to classify each class equally, improving its accuracy.\n-  The overall F1 score is 0.55. This is a relatively high score for a model that simply classifies all tweets into a single class. This score could only be achieved since the majority of the tweets are in fact pro climate change. \n","ab2acb97":"# Introduction\n\n### Climate change through the lens of twitter.\n\nGiven the recent explosion of Big Data, there is a growing demand for analyzing non traditional data sources. Social Media data is a huge source of this data in the form of chats, messages, news feeds and all of it is in an unstructured form. Text analytics is a process that helps analyze this unstructured data and look for patterns or infer popular sentiment which can help organizations in their decision making.\n\nTwitter data is a powerful source of information on a wide list of topics. This data can be analyzed to find trends related to specific topics, measure popular sentiment, obtain feedback on past decisions and also help make future decisions.\nClimate change has received extensive attention from public opinion in the last couple of years, after being considered for decades as an exclusive scientific debate. Governments and world-wide organizations such as the United Nations are working harder than ever on raising and maintaining public awareness toward this global issue.\n\nThe aim of this project is to gauge the public perception of climate change using twitter data. This will allow companies access to a broad base of consumer sentiment, spanning multiple demographics and geographic categories - Thus increasing their insights and informing future marketing strategies.\n\n### Problem statement \nIncrease Thrive Market\u2019s advertising efficiency by using machine learning to create effective marketing tools that can identify whether or not a person believes in climate change and could possibly be converted to a new customer based on their tweets.\n\n##### *Note to reader: This notebook was designed to be viewed in light mode*\n\n<img src=\"https:\/\/www.adweek.com\/wp-content\/uploads\/2017\/11\/twitter-280-for-all-PAGE-2017.png\">","73225b3b":"<a id=\"subsection-two\"><\/a>\n## Tweet length distribution \nFrom the plots below we can see that tweets that fall in the pro climate change class are generally longer and the shortest tweets belong to the anti climate change class.\nWe also notice that neutral climate change tweets tend to have the most variability in tweet length. ","ada16c90":"\n### Observations: \n\n- In the neutral category, a few of the top hashtags were not directly related to climate change eg. FirstDayOfSpring and AprilFoolsDay. \n- Hashtags like GlobalWarming, AmReading and QandA could suggest that these tweets are made by people who are still undecided on the topic but could be open to discussions and are interested in finding new\/more information. \n- In general neutral climate change tweets have hashtags that are not as polarized as the anti and pro hashtags.","19bbb961":"<a id=\"section-ten\"><\/a>\n# Conclusion \n\nMore than half of the tweets examined support the beilief of man-made climate change. Futhermore, climate change is now one of the two most important issues in politics for Democrats. The data also suggests that the majority of anti climate change tweets come from Republicans and Trump supporters \n\n\nLast year Trump\u2019s administration formally began the process to exit the climate deal, in which nearly 200 nations pledged to reduce greenhouse gas emissions and assist poor nations struggling with the consequences of a warming Earth. We noticed that the majority of tweets about climate change accross all classes involve the Paris agreement, COP22, Trump and Trump related hashtags\/mentions. It was intresting to note that the most links are being shared in the pro climate change class and not in the news related class. \n\nOur final kaggle subission made use of a tuned linear SVC model and achived an F1 score of 0.74.\n\nFor further information regarding the possible business applications of these insights and as well as access to our interactive classification model and data visualizations please visit our streamlit app:\n\nLink: http:\/\/54.194.109.116:5000\/\n\n![eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpbWFnZSI6Imh0dHBzOi8vYXNzZXRzLnJibC5tcy82NDQ1Njg5L29yaWdpbi5qcGciLCJleHBpcmVzX2F0IjoxNjEzNTMwMDI5fQ.Zdk3zYuONz4XMIR5u9TdvrJURQ4uv7r9hWyJe9JZV3w\/img.jpg?width=980](https:\/\/assets.rebelmouse.io\/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpbWFnZSI6Imh0dHBzOi8vYXNzZXRzLnJibC5tcy82NDQ1Njg5L29yaWdpbi5qcGciLCJleHBpcmVzX2F0IjoxNjEzNTMwMDI5fQ.Zdk3zYuONz4XMIR5u9TdvrJURQ4uv7r9hWyJe9JZV3w\/img.jpg?width=980)","db7b4c93":"<a id=\"section-one\"><\/a>\n# Import libraries and datasets \nFirst we need to load the libraries we are going to use throughout our notebook. \nAfter which we will load our train and test data.","7b09d581":"### Observations: \n\n- MAGA and Trump are the number 1 and 3rd most frequently used hashtags in anti climate change tweets. MAGA (Make America great again) is the slogan that Donald Trump used during his campaign for elections in 2016. On Twitter, his supporters took to the hashtag \u201c#MAGA.\u201d The MAGA hat and hashtag became, and remain, symbols of support for Trump. From this information we can infer that most anti climate change tweets come from Trump suporters.\n\n- DrainTheSwamp also made the list of the top anti climate change hashtags. Trump used this metaphor to describe his plan to fix problems in the federal government. During his presidential campaign, Trump claimed then that he would raise taxes on the wealthy, particularly the hedge fund managers. This is another hashtag implying support for trump which futher supports our assumtion that most trump suporters on twitter also fall in the anti climate change category.\n\n- TCOT which stands for Top Conservative On Twitter takes the number 6 spot. The term provides a way for conservatives in particular and Republicans in general to locate and follow the tweets of like-minded individuals. We're sensing a pattern here: Trump, top conservatives on twitter, make America great again, Drain the swamp... \n\n- FakeNews and ClimateScam are pretty popular hashtags too. People who are anti climate change could be tweeting and retweeting information and opinions they disagree with followed by the hashtags 'fakenews' and 'climatescam' in an attemot to discredit both the information and the source  ","1e0f154f":"<a id=\"section-three\"><\/a>\n# Preprocessing  \n Before we continue exploring our data we will have to do some preprocessing in order to gain maximum insights.\n \n### Plan of action:\n- Copy the dataframe and rename the class labels for better data visualization\n- Extract hashtags and store them in separate dataframes for each class\n- Remove 'noisy entities' such as URL's, punctuations, mentions, numbers and extra white space. \n- Tokenization\n- Perform part of speech tagging (POS) and lemmatization\n- Create dataframes to store the top 25 words and their respective frequencies in each class\n- Specific named entity extraction\n ","5b38e8ba":"<a id=\"subsection-six\"><\/a>\n## Specific named entity recognition and extraction\nExtracting the top 10 organisations, people and geopolitical entities in each class. This information is then stored in separate dataframes for each class.","7649d683":"<a id=\"subsection-five\"><\/a>\n## Word frequency\nCreating separate dataframes to store the 25 most frequent words and their respective frequencies for each class. Once this information has been extracted we will use these words to create wordclouds for each class.\n\nWord clouds are a popular approach in nlp tasks, here they help us visualize and gain a better understanding of what is being said in each class.","260e7d8d":"<a id=\"section-five\"><\/a>\n# Building classification models\n\nWe will be making use of a pipeline to build our classification models. This pipeline will vectorize the text data before fitting it to our chosen model.\n\nThe following 5 models will be considered:\n- Random forest \n- Naive Bayes\n- K nearest neighbors\n- Logistic regression\n- Linear SVC","f3a27862":"<a id=\"subsection-two\"><\/a>\n### Pipelines\n\nThe pipelines consist of 2 steps, vectorization and model fitting.\n\nMachines, unlike humans, cannot understand the raw text. Machines can only see numbers. Particularly, statistical techniques such as machine learning can only deal with numbers. Therefore, we need to convert our text into numbers. \n\nThe TFIDF vectorizer assigns word frequency scores that try to highlight words that are more interesting, e.g. frequent in a document but not across documents. The TfidfVectorizer will tokenize documents, learn the vocabulary and inverse document frequency weightings, and allow you to encode new documents. Another advantage of this method is that the resulting vectors are already scaled.\n","2a3d43d9":"# End Comet experiment ","794e98b2":"## Naive Bayes classification\nNaive Bayes is a classification algorithm that uses the principle of Bayes theorem to make classifications and assumes independent variables to be statistically independent from each other .\n\nBayes Theorem:\n\n$$ P(A \\mid B) = \\frac{P(B \\mid A) * P(A)}{P(B)}$$\n\n* P(A|B) is the posterior probability of class (A, target) given predictor (B, attributes).\n\n* P(A) is the prior probability of class.\n\n* P(B|A) is the likelihood which is the probability of the predictor given class.\n\n* P(B) is the prior probability of the predictor.\n\nNaive Bayes has 3 Classification Methods\n\n**Gaussian** : \nIt is used in classification and it assumes that features follow a normal distribution.\n\n**Bernoulli** : \nThe binomial model is useful if your feature vectors are binary (i.e. zeros and ones). One application would be text classification with a \u2018bag of words\u2019 model where the 1s & 0s are \u201cword occurs in the document\u201d and \u201cword does not occur in the document\u201d respectively.\n\n**Multinomial** : \nIt is used for discrete counts. For example, let\u2019s say, we have a text classification problem. Here we can consider Bernoulli trials which is one step further and instead of \u201cword occurring in the document\u201d, we have \u201ccount how often word occurs in the document\u201d, you can think of it as \u201cnumber of times outcome number x_i is observed over the n trials\u201d.\n\nWe will be using the multinomial method ","8073e60c":"<a id=\"subsection-four\"><\/a>\n## Hashtags\nHashtags have long been an important tool on Twitter for helping users organize and sort their tweets. They're a great way to indicate that your content is relevant to a certain topic and to get your tweets in front of an interested audience. \n\nConsidering this, we decided it might be insightful to see what the most frequent hashtags in each class are. This will help us gain a better understanding of what kind of information is being consumed and shared in each class.","47886283":"<a id=\"subsection-four\"><\/a>\n## Parts of speech tagging and lemmatization\nIn this step we start by determining the length of each tweet and storing this information in a new column. We then tokenize the tweets before performing POS tagging on each word followed by lemmatization. \n\nIn lemmatization, we reduce the word into dictionary root form. For instance \"cats\" is converted into \"cat\". Lemmatization is done in order to avoid creating features that are semantically similar but syntactically different. Lemmatization is preferred over stemming since stemming is a crude method for cataloging related words; it essentially chops off letters from the end until the stem is reached. This works fairly well in most cases, but unfortunately English has many exceptions where a more sophisticated process is required"}}