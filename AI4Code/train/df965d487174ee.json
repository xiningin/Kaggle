{"cell_type":{"dae8413f":"code","83977dde":"code","b9ac559a":"code","8fb55565":"code","cd6992b7":"code","22dc9129":"code","c12db057":"code","63f6de7d":"code","6bbc3c36":"code","6b9bd2d8":"code","5ca81fa6":"code","cdd63ec5":"code","95b7bfe1":"code","f7552a7c":"code","99797caf":"code","2578e0e9":"code","e891e74c":"code","432b000e":"code","b777fc8c":"code","504e3b50":"code","5d44d5e8":"code","bf653247":"code","c3aabc64":"code","64314eea":"code","fa1e91c9":"code","5ea6a5ff":"code","12438ec4":"code","e42d995a":"code","4032be88":"code","7d4a8c53":"code","70d35dad":"code","3dc726a4":"code","b73e97cc":"code","5f106bde":"code","62c1c352":"code","1cb1810b":"markdown","22846976":"markdown","e44cadf2":"markdown","113f3f23":"markdown","cf7cfd54":"markdown","0a4fa9a4":"markdown","cf84e8db":"markdown","95fabb64":"markdown","9c1af310":"markdown","8b17e8f6":"markdown","280a7945":"markdown","d02622bd":"markdown","a4a77291":"markdown","6658c568":"markdown","4fc2dae4":"markdown","f8a727aa":"markdown","82af2aae":"markdown","bdfd90a7":"markdown","3a3a97fd":"markdown","723babd7":"markdown","49d06a33":"markdown","72109661":"markdown"},"source":{"dae8413f":"import pandas as pd \nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.preprocessing import KBinsDiscretizer, OneHotEncoder, StandardScaler, LabelEncoder\nfrom sklearn.metrics import confusion_matrix, plot_confusion_matrix\nfrom sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier\nfrom sklearn.utils.class_weight import compute_class_weight, compute_sample_weight\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\nfrom sklearn.naive_bayes import BernoulliNB, BaseEstimator, BaseNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import plot_confusion_matrix, classification_report\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import accuracy_score\n\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom imblearn.over_sampling import RandomOverSampler\n\n\ndef plot_pretty_cf(predictor, xtest, ytest, cmap='Greys', normalize='true', \n                   title=None, label_dict={}):\n    fig, ax = plt.subplots(figsize=(8, 8))\n    plot_confusion_matrix(predictor, xtest, ytest, cmap=cmap, normalize=normalize, ax=ax)\n    ax.set_title(title, size='xx-large', pad=20, fontweight='bold')\n    if label_dict != {}:\n      ax.set_xticklabels([label_dict[int(x.get_text())] for x in ax.get_xticklabels()], rotation=35)\n      ax.set_yticklabels([label_dict[int(x.get_text())] for x in ax.get_yticklabels()])\n    else: \n      ax.set_xticklabels([str(x).replace('_', ' ').title()[12:-2] for x in ax.get_xticklabels()], rotation=35)\n      ax.set_yticklabels([str(x).replace('_', ' ').title()[12:-2] for x in ax.get_yticklabels()])\n    ax.set_xlabel('Predicted Label', size='x-large')\n    ax.set_ylabel('True Label', size='x-large')\n    plt.show()\n    \n    \ndef class_weight_applier(y_train, y_test):\n  y_classes = y_train.unique()\n  le = LabelEncoder()\n  y_integers = le.fit_transform(y_train)\n\n  # create a dict of labels : their integer representations\n  label_dict = dict(zip(le.classes_, np.unique(y_integers)))\n  flipped_dict = {value:key for key, value in label_dict.items()}\n\n  # # get the class weights\n  class_weights = compute_class_weight('balanced', np.unique(y_integers), y_integers)\n  sample_weights = compute_sample_weight('balanced', y_integers)\n  class_weights_dict = dict(zip(le.transform(list(le.classes_)), class_weights))\n\n  # convert the target to the numerical categories\n  y_train = y_train.apply(lambda x: label_dict[x])\n  y_test = y_test.apply(lambda x: label_dict[x])\n\n  return y_train, y_test, flipped_dict","83977dde":"df = pd.read_csv('..\/input\/history-of-philosophy\/phil_nlp.csv')\n\ndf.sample(5)","b9ac559a":"# split the data\nx_train, x_test, y_train, y_test = train_test_split(df['sentence'], df['school'])","8fb55565":"# vectorize\ntfidvectorizer = TfidfVectorizer(decode_error='ignore', stop_words=[])\ntf_idf_data_train = tfidvectorizer.fit_transform(x_train)\ntf_idf_data_test = tfidvectorizer.transform(x_test)","cd6992b7":"# build the classifier, train it, get predictions\nnb_classifier = MultinomialNB()\nnb_classifier.fit(tf_idf_data_train, y_train)\nnb_classifier_preds = nb_classifier.predict(tf_idf_data_test)","22dc9129":"plot_pretty_cf(nb_classifier, tf_idf_data_test, y_test, title='Baseline NB Model')","c12db057":"print(classification_report(y_test, nb_classifier_preds))","63f6de7d":"df['school'].value_counts(normalize=True)","6bbc3c36":"x_train, x_test, y_train, y_test = train_test_split(df['sentence'], df['school'])","6b9bd2d8":"tfidvectorizer = TfidfVectorizer(decode_error='ignore', stop_words=[])\ntf_idf_data_train = tfidvectorizer.fit_transform(x_train)\ntf_idf_data_test = tfidvectorizer.transform(x_test)","5ca81fa6":"y_train, y_test, flipped_dict = class_weight_applier(y_train, y_test)","cdd63ec5":"ros = RandomOverSampler(sampling_strategy='all')","95b7bfe1":"x_under, y_under = ros.fit_sample(tf_idf_data_train, y_train)","f7552a7c":"nb_undersampled = MultinomialNB()\nnb_undersampled.fit(x_under, y_under)\nnb_undersampled_preds = nb_undersampled.predict(tf_idf_data_test)","99797caf":"plot_pretty_cf(nb_undersampled, tf_idf_data_test, y_test, \n               title='NB w\/ Undersampling', label_dict=flipped_dict)","2578e0e9":"print(classification_report(y_test, nb_undersampled_preds))","e891e74c":"rus = RandomUnderSampler(sampling_strategy='all')","432b000e":"x_over, y_over = rus.fit_sample(tf_idf_data_train, y_train)","b777fc8c":"nb_oversampled = MultinomialNB()\nnb_oversampled.fit(x_under, y_under)\nnb_oversampled_preds = nb_oversampled.predict(tf_idf_data_test)","504e3b50":"plot_pretty_cf(nb_undersampled, tf_idf_data_test, y_test, \n               title='NB w\/ Oversampling', label_dict=flipped_dict)","5d44d5e8":"print(classification_report(y_test, nb_oversampled_preds))","bf653247":"x_train, x_test, y_train, y_test = train_test_split(df['lemmatized_str'], df['school'])\n\n# vectorize\ntfidvectorizer = TfidfVectorizer(decode_error='ignore', stop_words=[])\ntf_idf_data_train = tfidvectorizer.fit_transform(x_train)\ntf_idf_data_test = tfidvectorizer.transform(x_test)","c3aabc64":"y_train, y_test, flipped_dict = class_weight_applier(y_train, y_test)","64314eea":"rus = RandomUnderSampler(sampling_strategy='all')\n\nx_over_lemma, y_over_lemma = rus.fit_sample(tf_idf_data_train, y_train)","fa1e91c9":"nb_lemma = MultinomialNB()\nnb_lemma.fit(x_over_lemma, y_over_lemma)\nnb_lemma_preds = nb_lemma.predict(tf_idf_data_test)\n\nplot_pretty_cf(nb_lemma, tf_idf_data_test, y_test, \n               title='NB w\/ Lemmatization', label_dict=flipped_dict)","5ea6a5ff":"print(classification_report(y_test, nb_lemma_preds))","12438ec4":"# vectorize, this time adjusting the ngram range to include bigrams\ntfidvectorizer = TfidfVectorizer(decode_error='ignore', \n                                 stop_words=[], \n                                 ngram_range=(1,2))\ntf_idf_data_train = tfidvectorizer.fit_transform(x_train)\ntf_idf_data_test = tfidvectorizer.transform(x_test)","e42d995a":"y_train, y_test, flipped_dict = class_weight_applier(y_train, y_test)","4032be88":"rus = RandomUnderSampler(sampling_strategy='all')\n\nx_over_bgram, y_over_bgram = rus.fit_sample(tf_idf_data_train, y_train)","7d4a8c53":"nb_bigrams = MultinomialNB()\nnb_bigrams.fit(x_over_bgram, y_over_bgram)\nnb_bigrams_preds = nb_bigrams.predict(tf_idf_data_test)\n\nplot_pretty_cf(nb_bigrams, tf_idf_data_test, y_test, \n               title='NB w\/ Bigrams', label_dict=flipped_dict)","70d35dad":"print(classification_report(y_test, nb_bigrams_preds))","3dc726a4":"x_train, x_test, y_train, y_test = train_test_split(df['sentence'], df['school'])\n\n# vectorize\ntfidvectorizer = TfidfVectorizer(decode_error='ignore', \n                                 stop_words=[])\ntf_idf_data_train = tfidvectorizer.fit_transform(x_train)\ntf_idf_data_test = tfidvectorizer.transform(x_test)\n\ny_train, y_test, flipped_dict = class_weight_applier(y_train, y_test)","b73e97cc":"rus = RandomUnderSampler(sampling_strategy='all')\n\nx_over, y_over = rus.fit_sample(tf_idf_data_train, y_train)","5f106bde":"rf = RandomForestClassifier()\nrf.fit(x_over, y_over)\nrf_preds = rf.predict(tf_idf_data_test)\n\nplot_pretty_cf(rf, tf_idf_data_test, y_test, \n               title='Untuned RF', label_dict=flipped_dict)","62c1c352":"print(classification_report(y_test, rf_preds))","1cb1810b":"This notebook shows an example of Naive Bayes models and Random Forest models run on the history of philosophy dataset.","22846976":"### Imports and Loading Data","e44cadf2":"### NB with Lemmatization","113f3f23":"### Random Forest Classifier","cf7cfd54":"Unfortunately the random forest model got only 60%, worse than any Bayesian model. A result like this is poor enough that spending time refining it may just not be worth the effort, especially when there are more promising avenues still to explore. \n\nOverall, the Bayesian models were able to reach 77% accuracy when corrected for class imbalance. When one takes into account the number of classes (10) involved, that is a respectable result. ","0a4fa9a4":"## Naive Bayes & Random Forest Classification","cf84e8db":"Not bad, we got a sold increase in accuracy. Let's check if oversampling helps any more.","95fabb64":"Accuracy in the low 70s over 10 classes is not too bad, but we can at least aim higher than this. If we look at it, a lot of failures were along the lines of lines of class imbalance. ","9c1af310":"Then we vectorize. After a few attempts, we found that the best models were those where no stopwords were involved.","8b17e8f6":"### Baseline NB Bayes Model","280a7945":"Random Forests don't always do well on this kind of task, but it's perhaps worth trying. We'll just do an untuned model to see if it gets any kind of results worth exploring. ","d02622bd":"### NB with Bigrams","a4a77291":"Perhaps correcting for class imbalance could improve the model. ","6658c568":"Here we will use imblearn's over and undersampler to correct for class imbalance.","4fc2dae4":"First we need to split up the data into test and train.","f8a727aa":"#### Oversampling","82af2aae":"Not great, and worse than non-lemmatized versions. This makes sense since lemmatization essentially masks information that might have had some small part to play in the classification math.","bdfd90a7":"It seems like bigrams actually made the model worse. Let's try something totally different - random forests!","3a3a97fd":"Unsurprisingly, not much of a different result. It seems like Multinomial Naive Bayes can give us about 77% accuracy. \n\nIt's perhaps worth checking if lemmatization can help the model.","723babd7":"#### Undersampling","49d06a33":"### NB Corrected for Class Imbalance","72109661":"While singular words may not always be indicative of a school, certain phrases are often almost entirely exclusive to a school. So it stands to reason that incorporating bigrams into our data would help the model."}}