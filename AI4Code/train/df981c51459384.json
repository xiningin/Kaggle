{"cell_type":{"6e4035ed":"code","fe485172":"code","03835277":"code","cbbca2ed":"code","8928c212":"code","7d63647c":"code","becc5168":"code","c117727e":"code","b43c3be9":"code","5813e871":"code","842500d5":"code","1ce78bfb":"code","903b51c7":"code","5b6e2758":"code","9baa9284":"code","606b4b2e":"code","44200be4":"code","ca25a644":"code","bfdd2b58":"code","5e688fe5":"markdown","468c4cab":"markdown","90838254":"markdown","086a1de0":"markdown","84266623":"markdown","8f4c58cd":"markdown","b5e1f702":"markdown","fbc664fa":"markdown","c4c22437":"markdown","1d94f419":"markdown"},"source":{"6e4035ed":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nfrom sklearn.model_selection import train_test_split\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","fe485172":"def submit(model, test_features, test_ids, filename):\n    loss_pred = model.predict(test_features)\n    submission = pd.DataFrame({\"id\": test_ids, \"loss\": loss_pred.reshape(-1)})\n    submission.to_csv(filename, index = False)","03835277":"train_data = pd.read_csv(\"\/kaggle\/input\/tabular-playground-series-aug-2021\/train.csv\")\n\ntest_data = pd.read_csv(\"\/kaggle\/input\/tabular-playground-series-aug-2021\/test.csv\")","cbbca2ed":"train_data.head()","8928c212":"test_data.head()","7d63647c":"train_data.info()","becc5168":"train_data.describe().transpose()","c117727e":"train_data.shape","b43c3be9":"corr_score = train_data.corr()","5813e871":"corr_score[\"loss\"].sort_values(ascending = False)","842500d5":"train_data.pop(\"id\")\ntest_ids = test_data.pop(\"id\")","1ce78bfb":"train_mean = train_data.mean()\ntrain_std = train_data.std()","903b51c7":"train_target_mean = train_mean.pop(\"loss\")\ntrain_targets_std = train_std.pop(\"loss\")","5b6e2758":"validation_split = 0.2","9baa9284":"train_features, validation_features = train_test_split(train_data, test_size = validation_split)","606b4b2e":"train_targets, validation_targets = train_features.pop(\"loss\"), validation_features.pop(\"loss\")","44200be4":"should_scale = False\nif should_scale == True:\n    train_features = (train_features - train_mean) \/ train_std\n    validation_features = (validation_features - train_mean) \/ train_std\n    test_features = (test - train_mean) \/ train_std\n    print(test_features.head())\n    print(train_features.head())\n    print(validation_features.head())\nelse:\n    test_features = test_data","ca25a644":"import catboost\nimport time\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nbegin = time.time()\nparameters = {\n    \"depth\": [6, 7, 8],\n    \"learning_rate\": [0.08, 0.1],\n    \"iterations\": [300, 350], \n}\ndef train_catboost(hyperparameters, X_train, X_val, y_train, y_val):\n    keys = hyperparameters.keys()\n    best_index = {key:0 for key in keys}\n    best_cat = None\n    best_score = 10e8\n    for (index, key) in enumerate(keys):\n        print(\"Find best parameter for %s\" %(key))\n        items = hyperparameters[key]\n        best_parameter = None\n        temp_best = 10e8\n        for (key_index, item) in enumerate(items):\n            iterations = hyperparameters[\"iterations\"][best_index[\"iterations\"]] if key != \"iterations\" else item\n            learning_rate = hyperparameters[\"learning_rate\"][best_index[\"learning_rate\"]] if key != \"learning_rate\" else item\n            depth = hyperparameters[\"depth\"][best_index[\"depth\"]] if key != \"depth\" else item\n            print(\"Train with iterations: %d learning_rate: %.2f depth:%d\"%(iterations, learning_rate, depth))\n            cat = catboost.CatBoostRegressor(\n                iterations = iterations, \n                learning_rate = learning_rate,\n                depth = depth\n            )\n            cat.fit(X_train, y_train, verbose=False)\n            y_pred = cat.predict(X_val)\n            score = np.sqrt(mean_squared_error(y_val, y_pred))\n            print(\"RMSE: %.2f\"%(score))\n            if score < temp_best:\n                temp_best = score\n                best_index[key] = key_index\n                best_parameter = item\n            if score < best_score:\n                best_score = score\n                best_cat = cat\n        print(\"Best Parameter for %s: \"%(key), best_parameter)\n    best_parameters = {\n        \"iterations\": hyperparameters[\"iterations\"][best_index[\"iterations\"]],\n        \"learning_rate\": hyperparameters[\"learning_rate\"][best_index[\"learning_rate\"]],\n        \"depth\": hyperparameters[\"depth\"][best_index[\"depth\"]]\n    }\n    return best_cat, best_score, best_parameters\nbest_cat, best_score, best_parameters = train_catboost(parameters, train_features, validation_features, train_targets, validation_targets)\nprint(\"Best CatBoost Model: \", best_cat)\nprint(\"Best MAE: \", best_score)\nelapsed = time.time() - begin \nprint(\"Elapsed time: \", elapsed)\nsubmit(best_cat, test_features, test_ids, \"submission.csv\")","bfdd2b58":"from sklearn.model_selection import KFold\nfold = 1\nfor train_indices, val_indices in KFold(n_splits=5, shuffle=True).split(train_data):\n    print(\"Training with Fold %d\"%(fold))\n    X_train = train_data.iloc[train_indices]\n    X_val = train_data.iloc[val_indices]\n    y_train = X_train.pop(\"loss\")\n    y_val = X_val.pop(\"loss\")\n    if should_scale:\n        X_train = (X_train - train_mean) \/ train_std\n        X_val = (X_val - train_mean) \/ train_std\n    cat = catboost.CatBoostRegressor(\n        iterations = best_parameters[\"iterations\"], \n        learning_rate = best_parameters[\"learning_rate\"],\n        depth = best_parameters[\"depth\"]\n    )\n    cat.fit(X_train, y_train, verbose=False)\n    y_pred = cat.predict(X_val)\n    score = np.sqrt(mean_squared_error(y_val, y_pred))\n    print(\"RMSE: %.2f\"%(score))\n    submit(cat, test_features, test_ids, \"submission_fold%d.csv\"%(fold))\n    fold += 1","5e688fe5":"# Import Packages","468c4cab":"# Common Functions","90838254":"# **Thank You**","086a1de0":"# Data Preprocessing","84266623":"### Train Validation Split","8f4c58cd":"### Data Scaling","b5e1f702":"### Drop Id Column","fbc664fa":"### Model Develpoment","c4c22437":"# EDA","1d94f419":"#### Using Catboost"}}