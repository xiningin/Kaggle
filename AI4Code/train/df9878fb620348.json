{"cell_type":{"f6f36c73":"code","6aac0580":"code","3432eb2c":"code","7734ed2f":"code","fc3dfe96":"code","d49090dd":"code","0bc54358":"code","557f11e2":"code","4d922e72":"code","19f7c6ba":"code","d132d71d":"code","76eee20e":"code","405ebdbc":"code","79b2c765":"code","c198c5e6":"code","d2242c62":"code","299392cf":"code","80f5891f":"code","53765b5a":"code","9f71669d":"code","bb38db26":"code","e5f0c33a":"code","99a98c60":"code","e924a12e":"code","ed49f3d4":"markdown","a38193e8":"markdown","2de2c189":"markdown","13ea320a":"markdown","311358f6":"markdown","17c52b59":"markdown","b84c2a71":"markdown","05525a8b":"markdown","7e0a1165":"markdown"},"source":{"f6f36c73":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","6aac0580":"import numpy as np\nimport pandas as pd\nimport time\n\n# For plotting\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\n%matplotlib inline\n\n#PCA\nfrom sklearn.decomposition import PCA\n#TSNE\nfrom sklearn.manifold import TSNE\n#UMAP\nimport umap\n\nimport plotly.io as plt_io\nimport plotly.graph_objects as go","3432eb2c":"train = pd.read_csv('\/kaggle\/input\/sign-language-mnist\/sign_mnist_train\/sign_mnist_train.csv')\ntrain.head()","7734ed2f":"train.shape\n# Get indexes where name column doesn't have value john","fc3dfe96":"train = train[train['label'] < 10]","d49090dd":"train","0bc54358":"## Setting the label and the feature columns\ny = train.loc[:,'label'].values\nx = train.loc[:,'pixel1':].values","557f11e2":"np.unique(y)","4d922e72":"from sklearn.preprocessing import StandardScaler\n## Standardizing the data\nstandardized_data = StandardScaler().fit_transform(x)","19f7c6ba":"y","d132d71d":"## Importing and Apply PCA\nstart = time.time()\npca = PCA(n_components=3) # project from 784 to 2 dimensions\nprincipalComponents = pca.fit_transform(standardized_data)\nprincipal_df = pd.DataFrame(data = principalComponents, columns = ['principal component 1', 'principal component 2','principal component 3'])\nprincipal_df.shape\nprint('Duration: {} seconds'.format(time.time() - start))","76eee20e":"def plot_2d(component1, component2):\n    \n    fig = go.Figure(data=go.Scatter(\n        x = component1,\n        y = component2,\n        mode='markers',\n        marker=dict(\n            size=20,\n            color=y, #set color equal to a variable\n            colorscale='Rainbow', # one of plotly colorscales\n            showscale=True,\n            line_width=1\n        )\n    ))\n    fig.update_layout(margin=dict( l=100,r=100,b=100,t=100),width=2000,height=1200)                 \n    fig.layout.template = 'plotly_dark'\n    \n    fig.show()\n\n","405ebdbc":"def plot_3d(component1,component2,component3):\n\n    fig = go.Figure(data=[go.Scatter3d(\n        x=component1,\n        y=component2,\n        z=component3,\n        mode='markers',\n        marker=dict(\n            size=10,\n            color=y,                # set color to an array\/list of desired values\n            colorscale='Rainbow',   # choose a colorscale\n            opacity=1,\n            line_width=1\n        )\n    )])\n\n    # tight layout\n    fig.update_layout(margin=dict(l=50,r=50,b=50,t=50),width=1800,height=1000)\n    fig.layout.template = 'plotly_dark'\n    \n    fig.show()","79b2c765":"plot_2d(principalComponents[:, 0],principalComponents[:, 1])","c198c5e6":"plot_3d(principalComponents[:, 0],principalComponents[:, 1],principalComponents[:, 2])","d2242c62":"# t-SNE does consume a lot of memory so let's consider only a subset of the dataset. \n\nstart = time.time()\npca_50 = PCA(n_components=50)\npca_result_50 = pca_50.fit_transform(standardized_data)\ntsne = TSNE(random_state = 42, n_components=3,verbose=0, perplexity=40, n_iter=300).fit_transform(pca_result_50)\nprint('Duration: {} seconds'.format(time.time() - start))","299392cf":"plot_2d(tsne[:, 0],tsne[:, 1])","80f5891f":"plot_3d(tsne[:, 0],tsne[:, 1],tsne[:, 2])","53765b5a":"start = time.time()\nreducer = umap.UMAP(random_state=42,n_components=3)\nembedding = reducer.fit_transform(standardized_data)\nprint('Duration: {} seconds'.format(time.time() - start))","9f71669d":"plot_2d(reducer.embedding_[:, 0],reducer.embedding_[:, 1])","bb38db26":"plot_3d(reducer.embedding_[:, 0],reducer.embedding_[:, 1],reducer.embedding_[:, 2])","e5f0c33a":"from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n\nstart = time.time()\nX_LDA = LDA(n_components=3).fit_transform(standardized_data,y)\nprint('Duration: {} seconds'.format(time.time() - start))","99a98c60":"plot_2d(X_LDA[:, 0],X_LDA[:, 1])","e924a12e":"plot_3d(X_LDA[:, 0],X_LDA[:, 1],X_LDA[:, 2])","ed49f3d4":"# Summary\n\n**We have explored four dimensionality reduction techniques for data visualization : (PCA, t-SNE, UMAP, LDA)and tried to use them to visualize a high-dimensional dataset in 2d and 3d plots.**\n\n- **PCA** did not work quite well in categorizing the different signs (10). However, instead of arbitrarily choosing the number dimensions to 3, it is much better to choose the number of dimensions that add up to a sufficiently large proportion of variance, but since this is data visualization problem that was the most reasonable thing to do.\n\n- **TSNE** managed to do better work on separating the clusters, the visualization in 2d and 3d was better than PCA definitely. However, it took a very long time to compute its embeddings.t-SNE doesn\u2019t have major use outside visualisation.\n\n- **UMAP** turned out to be the most effective manifold learning in terms of displaying the different clusters with clear separations, However not good enough clusters for multi-class pattern classification.\n\n- **LDA** outperformed all the above techniques Excellent computation time (second fastest) as well as proving the well-separated clusters we were expecting.","a38193e8":"# t-SNE ( T-distributed stochastic neighbour embedding )\n(t-SNE) or T-distributed stochastic neighbour embedding created in 2008 by (Laurens van der Maaten and Geoffrey Hinton) for dimensionality reduction that is particularly well suited for the visualization of high-dimensional datasets.\n\n(t-SNE) takes a high dimensional data set and reduces it to a low dimensional graph that retains a lot of the original information. It does so by giving each data point a location in a two or three-dimensional map. This technique finds clusters in data thereby making sure that an embedding preserves the meaning in the data. t-SNE reduces dimensionality while trying to keep similar instances close and dissimilar instances apart.[2]\n\nFor a quick a Visualization of this technique, refer to the animation below (it is taken from an amazing tutorial by Cyrille Rossant, I highly recommend to check out his amazing tutorial.\nlink: https:\/\/www.oreilly.com\/content\/an-illustrated-introduction-to-the-t-sne-algorithm\/","2de2c189":"# IF you like this Notebook \u270c\ufe0f, Don't Forget to Upvote the Kernel Thank you!, See You on to the NExxt One \ud83d\ude09","13ea320a":"# UMAP ( Uniform Manifold Approximation and Projection )\n\nUniform Manifold Approximation and Projection created in 2018 by (Leland McInnes, John Healy, James Melville) is a general-purpose manifold learning and dimension reduction algorithm.\nUMAP is a nonlinear dimensionality reduction method, it is very effective for visualizing clusters or groups of data points and their relative proximities.\n\nThe significant difference with TSNE is scalability, it can be applied directly to sparse matrices thereby eliminating the need to applying any Dimensionality reduction such as PCA or Truncated SVD(Singular Value Decomposition) as a prior pre-processing step.[1]\nPut simply, it is similar to t-SNE but with probably higher processing speed, therefore, faster and probably better visualization. (let\u2019s find it out in the tutorial below)","311358f6":"# Comparison between the Dimension Reduction Techniques: PCA vs t-SNE vs UMAP vs LDA\n\nBy comparing the visualisations produced by the four models, we can see that PCA was not able to do such a good job in differentiating the signs. The main drawback of PCA is that it is highly influenced by outliers present in the data. Moreover, PCA is a linear projection, which means it can\u2019t capture non-linear dependencies, its goal is to find the directions (the so-called principal components) that maximize the variance in a dataset.\n\nt-SNE does a better job as compared to PCA when it comes to visualising the different patterns of the clusters. Similar labels are clustered together, even though there are big agglomerates of data points on top of each other, certainly not good enough to expect a clustering algorithm to perform well.\n\nUMAP outperformed t-SNE and PCA, if we look at the 2d and 3d plot, we can see mini-clusters that are being separated well. It is very effective for visualizing clusters or groups of data points and their relative proximities. However, for this use case certainly not good enough to expect a clustering algorithm to distinguish the patterns.\n\nFinally LDA, outperformed all the previous techiniques in all aspects. Excellent computation time (second fastest) as well as proving the well separated clusters we were expecting.\nUMAP is much faster than t-SNE, another problem faced by the latter is the need for another dimensionality reduction method prior, otherwise, it would take a longer time to compute.","17c52b59":"\n\n# Main Approaches for Dimensionality Reduction\nThe two main approaches to reducing dimensionality: Projection and Manifold Learning.\n* Projection: This technique deals with projecting every data point which is in high dimension, onto a subspace suitable lower-dimensional space in a way which approximately preserves the distances between the points.\n* Manifold Learning: Many dimensionality reductions algorithm work by modelling the manifold on which the training instance lie; this is called Manifold learning. It relies on the manifold hypothesis or assumption, which holds that most real-world high-dimensional datasets lie close to a much lower-dimensional manifold, this assumption in most of the cases is based on observation or experience rather than theory or pure logic.[4]\nNow let's briefly explain the three techniques before jumping into solving the use case.","b84c2a71":"# LDA ( Linear Discriminant Analysis )\n\nLinear Discriminant Analysis (LDA) is most commonly used as a dimensionality reduction technique in the pre-processing step for pattern-classification.\nThe goal is to project a dataset onto a lower-dimensional space with good class-separability in order to avoid overfitting and also reduce computational costs.\n\nThe general approach is very similar to PCA, rather than finding the component axes that maximize the variance of our data, we are additionally interested in the axes that maximize the separation between multiple classes(LDA).LDA is \u201csupervised\u201d and computes the directions (\u201clinear discriminants\u201d) that will represent the axes that maximize the separation between multiple classes.\n","05525a8b":"# PCA (Principal Component Analysis)\nOne of the most known dimensionality reduction \u201cunsupervised\u201d algorithm is PCA(Principal Component Analysis).\nThis works by identifying the hyperplane which lies closest to the data and then projects the data on that hyperplane while retaining most of the variation in the data set.\nPrincipal Components.\n\nThe axis that explains the maximum amount of variance in the training set is called the Principal Components.\nThe axis orthogonal to this axis is called the second principal component. As we go for higher dimensions, PCA would find a third component orthogonal to the other two components and so on, for visualization purposes we always stick to 2 or maximum 3 principal components.\nIt is very important to choose the right hyperplane so that when the data is projected onto it, it the maximum amount of information about how the original data is distributed.","7e0a1165":"# What is Dimensionality Reduction?\n\nMany Machine Learning problems involve thousands of features, having such a large number of features bring along many problems, the most important ones are:\n* Makes the training extremely slow\n* Makes it difficult to find a good solution\n\nThis is known as the curse of dimensionality and the Dimensionality Reduction is the process of reducing the number of features to the most relevant ones in simple terms.\nReducing the dimensionality does lose some information, however as most compressing processes it comes with some drawbacks, even though we get the training faster, we make the system perform slightly worse, but this is ok! \u201csometimes reducing the dimensionality can filter out some of the noise present and some of the unnecessary details\u201d.\n\nMost Dimensionality Reduction applications are used for:\n* Data Compression\n* Noise Reduction\n* Data Classification\n* Data Visualization\n\nOne of the most important aspects of Dimensionality reduction, it is Data Visualization. Having to drop the dimensionality down to two or three, make it possible to visualize the data on a 2d or 3d plot, meaning important insights can be gained by analysing these patterns in terms of clusters and much more."}}