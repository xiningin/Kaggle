{"cell_type":{"867ed1be":"code","9dbe9f8a":"code","44511033":"code","26983c58":"code","9f219fdc":"code","ee7d3f07":"code","e95b84cc":"code","2f1ca99b":"code","413e1414":"code","2bbbc085":"code","09d0a302":"code","d73b9337":"code","ff725bba":"code","2f2f93d8":"code","0a39fd7c":"code","147be303":"code","7615185c":"code","9c06c5e6":"code","2911cd2b":"code","e791595f":"code","5fb66b7f":"code","f198b48c":"code","40e76bb1":"code","84259696":"code","c2d77382":"code","40186976":"code","f1dd0344":"code","7afd36a8":"code","c0537f28":"code","d0cbc37e":"code","490d1922":"code","3179ae1a":"code","512448e4":"code","5be2fd47":"code","955914c6":"code","d2d699ee":"code","01e8d5bd":"code","4f5fc433":"code","3555df8f":"code","084188f7":"code","e7143cab":"code","9083b525":"code","71719c70":"code","7ec16b4c":"code","58c61bf5":"code","a5ac8ef7":"code","2e485306":"code","fd14d704":"code","92ab46d0":"code","dfa184bf":"code","2fe768af":"code","aa934854":"code","c802a441":"code","7f5c1e2c":"code","477e244f":"code","ab5bcb3d":"code","bf72e60c":"code","c9e9f5ac":"code","9aa84c2f":"code","97563996":"code","77bfcc03":"code","80de18cf":"code","72960502":"code","115ff6d1":"code","f4716f11":"code","5f7128ec":"code","3763a729":"code","c4f26c94":"code","92b234df":"code","16476dcd":"code","3ffc415f":"code","556fcbaa":"code","c28b090a":"code","d32a9e9d":"code","b4b0542c":"code","187cc202":"code","50877d33":"code","318e19d0":"code","3b33666f":"code","061109c7":"code","d2224aaf":"code","479c8342":"code","35f48973":"code","3436a135":"code","69996f2b":"code","296eee2f":"code","73009ba3":"code","a6339c6a":"code","c115825f":"code","0b87e79a":"code","1f595ebd":"code","c11eff7a":"code","9a428c5d":"code","781f8740":"code","0c6dbd9b":"code","01a43cfe":"code","74505038":"code","2b5bc359":"code","9d44e4e2":"code","3b247fd4":"code","0f63c2cc":"code","143f1cb7":"code","b69b6e2d":"code","26d94a57":"code","c9fe980b":"code","49d905ef":"markdown","1ee36cab":"markdown","369f2bd7":"markdown","a4389046":"markdown","e0c83e11":"markdown","7e0fa2b8":"markdown","7af3c43c":"markdown","42feb903":"markdown","1683ffdd":"markdown","9f1befec":"markdown","3c5cc064":"markdown","b23cf672":"markdown","770a04ee":"markdown","385bd0f6":"markdown","951069f5":"markdown","e07b10da":"markdown","1a2c59b5":"markdown","ee48df63":"markdown","fc71b74f":"markdown","246e6244":"markdown","9317f1d8":"markdown","9aa0a495":"markdown","17fe9d52":"markdown","34dce644":"markdown","4ada18ca":"markdown","f02d8f99":"markdown","30ac654c":"markdown","0cc14eba":"markdown","068ac05a":"markdown","3afeee25":"markdown","8fa1b638":"markdown","1202490c":"markdown","0d4c3063":"markdown","e27e6103":"markdown","03c4b92f":"markdown","0c567577":"markdown","9aed23b5":"markdown","6253ff06":"markdown","431991cc":"markdown","50dcd41f":"markdown","200c80df":"markdown","986b9e91":"markdown","e968efee":"markdown","63a3a039":"markdown","531306f3":"markdown","ae5b02ab":"markdown","a4151b6f":"markdown","c3ac6d4a":"markdown","56f80bd3":"markdown","ee2f1e86":"markdown","f0d1c12a":"markdown","9a83feff":"markdown","b5e6f2bf":"markdown","686d146a":"markdown","02336eed":"markdown","7df5777b":"markdown","2729a33f":"markdown","3b0d8951":"markdown","97f5802a":"markdown","5563f263":"markdown","329f7da1":"markdown","8bd0af8c":"markdown","0f8d441a":"markdown","2408bd27":"markdown","ba40997f":"markdown","748934c7":"markdown","1f1ca26b":"markdown","815fda12":"markdown","b9b91b70":"markdown","59f32059":"markdown","41e88931":"markdown","8548b766":"markdown","8c026e48":"markdown","d0eb2890":"markdown","bc276bf6":"markdown"},"source":{"867ed1be":"!pip install yfinance","9dbe9f8a":"# time series data for tesla \nimport yfinance as yf\n\ndf_tesla = yf.download(tickers = 'TSLA', start = '2019-01-01', end = '2019-12-01', progress = False)\ndf_tesla.head()","44511033":"# time series data for facebook\ndf_fb = yf.download(tickers = 'FB', start = '2019-01-01', end = '2019-12-01', progress = False)\ndf_fb.head()","26983c58":"# time series data for Amazon\ndf_amzn = yf.download(tickers = 'AMZN', start = '2019-01-01', end = '2019-12-01', progress = False)\ndf_amzn.head()","9f219fdc":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport warnings \nwarnings.filterwarnings('ignore')\n\nplt.style.use('fivethirtyeight')\n%matplotlib inline","ee7d3f07":"# reading dataset\ndf = pd.read_csv('..\/input\/customers-in-a-shop\/Customers_in_a_Shop.csv', header = None)\ndf.columns = ['Date', 'Customers']\ndf['Date'] = pd.to_datetime(df['Date'], format = '%Y-%m')\ndf = df.set_index('Date')\ndf.head()","e95b84cc":"# checking for missing values\n\nplt.rcParams['figure.figsize'] = (17, 6)\nplt.plot(df)\nplt.title('\\nNumber of Customers Visiting in a Ice Cream shop since 1950\\n', fontsize = 20)\nplt.show()","2f1ca99b":"# mean imputation\n\ndf['Customers_mean'] = df['Customers'].fillna(df['Customers'].mean())\nplt.plot(df['Customers_mean'])\nplt.title('Mean Imputation of missing values')\nplt.show()","413e1414":"# last observation carried forward\n\ndf['Customers_locf'] = df['Customers'].bfill()\nplt.plot(df['Customers_locf'])\nplt.title('\\nLast Observed Carried Forward Method for Missing Values\\n', fontsize = 20)\nplt.show()","2bbbc085":"# Linear Interpolation\n\ndf['Customers_linear'] = df['Customers'].interpolate(method = 'linear')\nplt.plot(df['Customers_linear'])\nplt.title('\\nLinear Interpolation of Missing Values\\n', fontsize = 20)\nplt.show()","09d0a302":"# let's find the dates where we have missing values\ndf.index[df['Customers'].isnull()]","d73b9337":"df.loc['1960-03'].fillna((df['1949-03':'1959-03':12].sum())\/df['1949-03':'1959-03':12].shape[0], inplace = True)\ndf.loc['1954-06'].fillna((df['1949-06':'1953-06':12].sum())\/df['1949-06':'1953-06':12].shape[0], inplace = True)\ndf.loc['1951-07'].fillna((df['1949-07':'1950-07':12].sum())\/df.loc['1949-07':'1950-07':12].shape[0], inplace=True)\ndf.loc['1951-06'].fillna((df['1949-06':'1950-06':12].sum())\/df['1949-06':'1950-06':12].shape[0], inplace=True)","ff725bba":"df.isnull().sum().sum()","2f2f93d8":"plt.plot(df['Customers'])\nplt.title('\\nSeasonal Interpolation of Missing Values\\n', fontsize = 20)\nplt.show()","0a39fd7c":"sns.boxplot(df['Customers_linear'])\nplt.show()","147be303":"df['Customers_linear'].sort_values(ascending = False).head(10)","7615185c":"df['Customers_linear'].loc[(df['Customers_linear'] >= 700)] = 622","9c06c5e6":"sns.boxplot(df['Customers_linear'])\nplt.show()","2911cd2b":"import statsmodels.api as sm\n\ndecomposition = sm.tsa.seasonal_decompose(df['Customers_linear'], model = 'additive')\ndecomposition.plot()\n\nplt.show()","e791595f":"decomposition = sm.tsa.seasonal_decompose(df['Customers_linear'], model = 'multiplicative')\ndecomposition.plot()\nplt.show()","5fb66b7f":"length_train = 115\ntrain = df.iloc[:length_train, :]\ntest = df.iloc[length_train:, :]","f198b48c":"train.shape","40e76bb1":"test.head()","84259696":"test.shape","c2d77382":"y_naive = test.copy()\ny_naive['forecasted_naive'] = train['Customers_linear'][length_train - 1]","40186976":"plt.rcParams['figure.figsize'] = (17, 8)\nplt.plot(train['Customers_linear'], label = 'Train')\nplt.plot(test['Customers_linear'], label = 'Test')\nplt.plot(y_naive['forecasted_naive'], label = 'naive_forecast')\n\nplt.legend(fontsize = 15)\nplt.title('Naive Forecasting Method')\nplt.show()","f1dd0344":"y_avg = test.copy()\n\ny_avg['forecasted_avg'] = train['Customers_linear'].mean()","7afd36a8":"plt.plot(train['Customers_linear'], label = 'Train')\nplt.plot(test['Customers_linear'], label = 'Test')\nplt.plot(y_avg['forecasted_avg'], label = 'Simple Average Forecast')\n\nplt.legend(fontsize = 15)\nplt.title('Simple Average Forecasting Method')\nplt.show()","c0537f28":"y_moving = df.copy()\n\nwindow = 9\ny_moving['moving_average_forecast'] = df['Customers_linear'].rolling(window).mean()\ny_moving['moving_average_forecast'][length_train:] = y_moving['moving_average_forecast'][length_train - 1]","d0cbc37e":"plt.plot(train['Customers_linear'], label = 'Train')\nplt.plot(test['Customers_linear'], label = 'Test')\nplt.plot(y_moving['moving_average_forecast'], label = 'Simple Average Forecast')\n\nplt.legend(fontsize = 15)\nplt.title('Simple Average Forecasting Method')\nplt.show()","490d1922":"from statsmodels.tsa.holtwinters import SimpleExpSmoothing\n\nmodel = SimpleExpSmoothing(train['Customers_linear'])\nmodel_fit = model.fit(smoothing_level = 0.2)\n\ny_exp = test.copy()\ny_exp['Exponential_forecast'] = model_fit.forecast(24)","3179ae1a":"plt.plot(train['Customers_linear'], label = 'Train')\nplt.plot(test['Customers_linear'], label = 'Test')\nplt.plot(y_exp['Exponential_forecast'], label = 'Simple Exponential Smoothing')\n\nplt.legend(fontsize = 15)\nplt.title('Simple Exponential Smoothing Forecasting Method')\nplt.show()","512448e4":"from statsmodels.tsa.holtwinters import ExponentialSmoothing\\\n\nmodel = ExponentialSmoothing(train['Customers_linear'], seasonal_periods = 12, trend = 'multiplicative')\nmodel_fit = model.fit(smoothing_level = 0.2, smoothing_slope = 0.04)\n\ny_holtexponential = test.copy()\ny_holtexponential['holtexponential_smoothing'] = model_fit.forecast(24)","5be2fd47":"plt.plot(train['Customers_linear'], label = 'Train')\nplt.plot(test['Customers_linear'], label = 'Test')\nplt.plot(y_holtexponential['holtexponential_smoothing'], label = 'Holt Exponential Smoothing')\n\nplt.legend(fontsize = 15)\nplt.title('Holt Exponential Smoothing Forecasting Method')\nplt.show()","955914c6":"model = ExponentialSmoothing(train['Customers_linear'], seasonal_periods = 12, trend = 'multiplicative', seasonal = 'additive')\nmodel_fit = model.fit(smoothing_level = 0.2, smoothing_slope = 0.04)\n\ny_holtwinter = test.copy()\ny_holtwinter['holtwinter_forecast'] = model_fit.forecast(36)","d2d699ee":"plt.plot(train['Customers_linear'], label = 'Train')\nplt.plot(test['Customers_linear'], label = 'Test')\nplt.plot(y_holtwinter['holtwinter_forecast'], label = 'Holt Winters Exponential Smoothing')\n\nplt.legend(fontsize = 15)\nplt.title('Holt Winters Exponential Smoothing Forecasting Method')\nplt.show()","01e8d5bd":"from statsmodels.tsa.stattools import adfuller\n\nresult = adfuller(df['Customers_linear'], autolag = 'AIC')\nprint(f\"ADF Statistic: {result[0]}\")\nprint(f\"p-value: {result[1]}\")\nprint(f\"n_lags: {result[2]}\")\nfor key, value in result[4].items():\n    print('Critical Values:')\n    print(f\"{key}, {value}\")","4f5fc433":"from statsmodels.tsa.stattools import kpss\n\nresult = kpss(df['Customers_linear'])\nprint(f\"KPSS Statistics: {result[0]}\")\nprint(f\"p-value: {result[1]}\")\nprint(f\"num_lags: {result[2]}\")\nfor key, value in result[3].items():\n    print('Critical Values:')\n    print(f\"{key}, {value}\")","3555df8f":"from scipy.stats import boxcox\n\nplt.rcParams['figure.figsize'] = (17, 6)\ndata_boxcox = pd.Series(boxcox(df['Customers_linear'], lmbda = 0), index = df.index)\nplt.plot(data_boxcox, label  = 'After Box Cox Transformation')\n\nplt.legend()\nplt.title('Number of Customers Visiting in an Ice Cream since 1950\\n', fontsize = 20)\nplt.show()","084188f7":"data_boxcox_difference = pd.Series(data_boxcox - data_boxcox.shift(), index = df.index)\ndata_boxcox_difference.dropna(inplace = True)\nplt.plot(data_boxcox_difference, label = 'After Box Cox Transformation and Differencing ')\n\nplt.legend()\nplt.title('Number of Customers Visisting in a Ice Cream Shop since 1950\\n', fontsize = 20)\nplt.show()","e7143cab":"# loading and plotting acf\n\nfrom statsmodels.graphics.tsaplots import plot_acf\n\nplot_acf(data_boxcox_difference, ax = plt.gca(), lags = 10)\nplt.show()","9083b525":"# loading and plotting pacf\n\nfrom statsmodels.graphics.tsaplots import plot_pacf\n\nplot_pacf(data_boxcox_difference, ax = plt.gca(), lags = 30)\nplt.show()","71719c70":"length_train = 115\n\ntrain_data_boxcox = data_boxcox[:length_train]\ntest_data_boxcox = data_boxcox[length_train:]\n\ntrain_data_boxcox_difference = data_boxcox_difference[:length_train-1]\ntest_data_boxcox_difference = data_boxcox_difference[length_train-1:]","7ec16b4c":"from statsmodels.tsa.arima_model import ARIMA\n\nmodel_ar = ARIMA(train_data_boxcox_difference, order=(1,0,0))\nmodel_fit = model_ar.fit()","58c61bf5":"y_ar_new = data_boxcox_difference.copy()\ny_ar_new['ar_forecast_boxcox_difference'] = model_fit.predict(data_boxcox_difference.index.min(),\n                                                              data_boxcox_difference.index.max())\n\ny_ar_new['ar_forecast_boxcox'] = y_ar_new['ar_forecast_boxcox_difference'].cumsum()\ny_ar_new['ar_forecast_boxcox'] = y_ar_new['ar_forecast_boxcox'].add(data_boxcox[0])\ny_ar_new['ar_forecast'] = np.exp(y_ar_new['ar_forecast_boxcox'])","a5ac8ef7":"plt.rcParams['figure.figsize'] = (17, 8)\n\nplt.plot(train['Customers_linear'], label = 'Train')\nplt.plot(test['Customers_linear'], label = 'Test')\nplt.plot(y_ar_new['ar_forecast'][test.index.min():], label = 'AR model')\n\nplt.legend(fontsize = 15)\nplt.title('Auto Regressive Model\\n', fontsize = 20)\nplt.show()","2e485306":"model_ma = ARIMA(train_data_boxcox_difference, order=(0,0,7))\nmodel_fit = model_ma.fit()","fd14d704":"y_ma_new = data_boxcox_difference.copy()\ny_ma_new['ma_forecast_boxcox_differencing'] = model_fit.predict(data_boxcox_difference.index.min(),\n                                                              data_boxcox_difference.index.max())\ny_ma_new['ma_forecast_boxcox'] = y_ma_new['ma_forecast_boxcox_differencing'].cumsum()\ny_ma_new['ma_forecast_boxcox'] = y_ma_new['ma_forecast_boxcox'].add(data_boxcox[0])\ny_ma_new['ma_forecast'] = np.exp(y_ma_new['ma_forecast_boxcox'])","92ab46d0":"plt.plot(train['Customers_linear'], label = 'Train')\nplt.plot(test['Customers_linear'], label = 'Test')\nplt.plot(y_ma_new['ma_forecast'][test.index.min():], label = 'MA model')\n\nplt.legend(fontsize = 15)\nplt.title('Moving Average regressive model\\n', fontsize = 20)\nplt.show()","dfa184bf":"from statsmodels.tsa.arima_model import ARIMA\n\nmodel_arma = ARIMA(train_data_boxcox_difference, order = (1, 0, 5))\nmodel_fit = model_arma.fit()","2fe768af":"y_arma_new = data_boxcox_difference.copy()\ny_arma_new['arma_forecast_boxcox_difference'] = model_fit.predict(data_boxcox_difference.index.min(),\n                                                                  data_boxcox_difference.index.max())\ny_arma_new['arma_forecast_boxcox'] = y_arma_new['arma_forecast_boxcox_difference'].cumsum()\ny_arma_new['arma_forecast_boxcox'] = y_arma_new['arma_forecast_boxcox'].add(data_boxcox[0])\ny_arma_new['arma_forecast'] = np.exp(y_arma_new['arma_forecast_boxcox'])","aa934854":"plt.plot(train['Customers_linear'], label = 'Train')\nplt.plot(test['Customers_linear'], label = 'Test')\nplt.plot(y_arma_new['arma_forecast'][test.index.min():], label = 'ARMA model')\n\nplt.legend(fontsize = 15)\nplt.title('Auto regressive Moving Average model\\n', fontsize = 20)\nplt.show()","c802a441":"from statsmodels.tsa.arima_model import ARIMA\n\nmodel_arima = ARIMA(train_data_boxcox, order = (9, 1, 5))\nmodel_arima_fit = model_arima.fit()","7f5c1e2c":"y_arima_new = data_boxcox_difference.copy()\ny_arima_new['arima_forecast_boxcox_difference'] = model_arima_fit.predict(data_boxcox_difference.index.min(),\n                                                                  data_boxcox_difference.index.max())\ny_arima_new['arima_forecast_boxcox'] = y_arima_new['arima_forecast_boxcox_difference'].cumsum()\ny_arima_new['arima_forecast_boxcox'] = y_arima_new['arima_forecast_boxcox'].add(data_boxcox[0])\ny_arima_new['arima_forecast'] = np.exp(y_arima_new['arima_forecast_boxcox'])","477e244f":"plt.plot(train['Customers_linear'], label = 'Train')\nplt.plot(test['Customers_linear'], label = 'Test')\nplt.plot(y_arima_new['arima_forecast'][test.index.min():], label = 'ARIMA model')\n\nplt.legend(fontsize = 15)\nplt.title('Auto Regressive Integrated Moving Average Model\\n', fontsize = 20)\nplt.show()","ab5bcb3d":"from statsmodels.tsa.statespace.sarimax import SARIMAX\n\nmodel_sarima = SARIMAX(train_data_boxcox_difference, order = (1, 1, 1), seasonal_order = (1, 1, 1, 6))\nmodel_sarima_fit = model_sarima.fit()","bf72e60c":"y_sarima_new = data_boxcox_difference.copy()\ny_sarima_new['sarima_forecast_boxcox_difference'] = model_sarima_fit.predict(data_boxcox_difference.index.min(),\n                                                                  data_boxcox_difference.index.max())\ny_sarima_new['sarima_forecast_boxcox'] = y_sarima_new['sarima_forecast_boxcox_difference'].cumsum()\ny_sarima_new['sarima_forecast_boxcox'] = y_sarima_new['sarima_forecast_boxcox'].add(data_boxcox[0])\ny_sarima_new['sarima_forecast'] = np.exp(y_sarima_new['sarima_forecast_boxcox'])","c9e9f5ac":"plt.plot(train['Customers_linear'], label = 'Train')\nplt.plot(test['Customers_linear'], label = 'Test')\nplt.plot(y_sarima_new['sarima_forecast'][test.index.min():], label = 'ARIMA model')\n\nplt.legend(fontsize = 15)\nplt.title('Seasonal Auto Regressive Integrated Moving Average Model\\n', fontsize = 20)\nplt.show()","9aa84c2f":"data = pd.read_csv(\"..\/input\/delhi-city-temperature-time-series-data\/datasets_312121_636393_DailyDelhiClimateTrain.csv\")","97563996":"data['date'] = pd.to_datetime(data['date'],format=\"%Y-%m\")\ndata = data.set_index('date')","77bfcc03":"data.head()","80de18cf":"data.shape","72960502":"plt.plot(data['meantemp'])\nplt.title(\"Temperature of Delhi\\n\", fontsize = 25)\nplt.show()","115ff6d1":"from statsmodels.tsa.stattools import kpss\n\nresult = kpss(data['meantemp'])\nprint(f'KPSS Statistic: {result[0]}')\nprint(f'p-value: {result[1]}')\nprint(f'num lags: {result[2]}')\nprint('Critial Values:')\nfor key, value in result[3].items():\n    print('Critial Values:')\n    print(f'   {key}, {value}') ","f4716f11":"from statsmodels.graphics.tsaplots import plot_acf\nplot_acf(data['meantemp'], ax=plt.gca(), lags=10)\nplt.show()","5f7128ec":"from statsmodels.graphics.tsaplots import plot_pacf\nplot_pacf(data['meantemp'], ax=plt.gca(), lags=30)\nplt.show()","3763a729":"length_train = 1046\ntrain_climate = data.iloc[:length_train,:]\ntest_climate = data.iloc[length_train:,: ]","c4f26c94":"from statsmodels.tsa.arima_model import ARIMA\n\nmodel = ARIMA(train_climate['meantemp'], order=(1,0,3), exog= train_climate['humidity'])\nmodel_fit = model.fit()","92b234df":"y_arimax = data.copy()\ny_arimax['arimax_forecast'] = model_fit.predict(test_climate['meantemp'].index.min(),\n                                                test_climate['meantemp'].index.max(),\n                                                exog= test_climate['humidity'])","16476dcd":"plt.figure(figsize=(17,8))\n\nplt.plot(train_climate['meantemp'], label = 'Train')\nplt.plot(test_climate['meantemp'], label = 'Test')\nplt.plot(y_arimax['arimax_forecast'][test_climate['meantemp'].index.min():], \n         label = 'ARIMAX model')\n\nplt.legend()\nplt.title('Auto regressive Integrated Moving Average with external variable model')\nplt.show()","3ffc415f":"from statsmodels.tsa.statespace.sarimax import SARIMAX\n\nmodel = SARIMAX(train_climate['meantemp'], order=(7,0,3), seasonal_order=(2,1,1,12), exog =train_climate['humidity'])\nmodel_fit = model.fit()","556fcbaa":"y_sarimax = data.copy()\ny_sarimax['sarimax_forecast'] = model_fit.predict(test_climate['meantemp'].index.min(),\n                                                  test_climate['meantemp'].index.max(),\n                                                  exog= test_climate['humidity'])","c28b090a":"plt.plot(train_climate['meantemp'], label = 'Train')\nplt.plot(test_climate['meantemp'], label = 'Test')\nplt.plot(y_sarimax['sarimax_forecast'][test_climate['meantemp'].index.min():], label = 'SARIMAX model')\n\nplt.legend()\nplt.title('Seasonal Auto regressive Integrated Moving Average with external variable model')\nplt.show()","d32a9e9d":"mfe_naive = np.round(np.mean(test['Customers_linear'] - y_naive['forecasted_naive']), 2)\nmfe_naive","b4b0542c":"mfe_simple_avg = np.round(np.mean(test['Customers_linear'] - y_avg['forecasted_avg']), 2)\nmfe_simple_avg","187cc202":"mfe_moving_avg = np.round(np.mean(test['Customers_linear'] - y_moving['moving_average_forecast']), 2)\nmfe_moving_avg","50877d33":"mfe_exponential_smoothing = np.round(np.mean(test['Customers_linear'] - y_exp['Exponential_forecast']), 2)\nmfe_exponential_smoothing","318e19d0":"mfe_holtexponential = np.round(np.mean(test['Customers_linear'] - y_holtexponential['holtexponential_smoothing']), 2)\nmfe_holtexponential","3b33666f":"mfe_holtwinter = np.round(np.mean(test['Customers_linear'] - y_holtwinter['holtwinter_forecast']), 2)\nmfe_holtwinter","061109c7":"mfe_ar = np.round(np.mean(test['Customers_linear'] - y_ar_new['ar_forecast']), 2)\nmfe_ar","d2224aaf":"mfe_ma = np.round(np.mean(test['Customers_linear'] - y_ma_new['ma_forecast']), 2)\nmfe_ma","479c8342":"mfe_arma = np.round(np.mean(test['Customers_linear'] - y_arma_new['arma_forecast']), 2)\nmfe_arma","35f48973":"mfe_arima = np.round(np.mean(test['Customers_linear'] - y_arima_new['arima_forecast']), 2)\nmfe_arima","3436a135":"mfe_sarima = np.round(np.mean(test['Customers_linear'] - y_sarima_new['sarima_forecast']), 2)\nmfe_sarima","69996f2b":"mae_naive = np.round(np.mean(np.abs(test['Customers_linear'] - y_naive['forecasted_naive'])))\nmae_naive","296eee2f":"mae_simple_avg = np.round(np.mean(np.abs(test['Customers_linear'] - y_avg['forecasted_avg'])))\nmae_simple_avg","73009ba3":"mae_moving_avg = np.round(np.mean(np.abs(test['Customers_linear'] - y_moving['moving_average_forecast'])))\nmae_moving_avg","a6339c6a":"mae_exponential_smoothing = np.round(np.mean(np.abs(test['Customers_linear'] - y_exp['Exponential_forecast'])))\nmae_exponential_smoothing","c115825f":"mae_holtexponential = np.round(np.mean(np.abs(test['Customers_linear'] - y_holtexponential['holtexponential_smoothing'])))\nmae_holtexponential","0b87e79a":"mae_holtwinter = np.round(np.mean(np.abs(test['Customers_linear'] - y_holtwinter['holtwinter_forecast'])))\nmae_holtwinter","1f595ebd":"mae_ar = np.round(np.mean(np.abs(test['Customers_linear'] - y_ar_new['ar_forecast'])))\nmae_ar","c11eff7a":"mae_ma = np.round(np.mean(np.abs(test['Customers_linear'] - y_ma_new['ma_forecast'])))\nmae_ma","9a428c5d":"mae_arma = np.round(np.mean(np.abs(test['Customers_linear'] - y_arma_new['arma_forecast'])))\nmae_arma","781f8740":"mae_arima = np.round(np.mean(np.abs(test['Customers_linear'] - y_arima_new['arima_forecast'])))\nmae_arima","0c6dbd9b":"mae_sarima = np.round(np.mean(np.abs(test['Customers_linear'] - y_sarima_new['sarima_forecast'])))\nmae_sarima","01a43cfe":"mape_naive = np.round(np.mean(np.abs(test['Customers_linear'] - y_naive['forecasted_naive'])\/test['Customers_linear'])*100, 2)\nmape_naive","74505038":"mape_simple_avg = np.round(np.mean(np.abs(test['Customers_linear'] - y_avg['forecasted_avg'])\/test['Customers_linear'])*100, 2)\nmape_simple_avg","2b5bc359":"mape_moving_avg = np.round(np.mean(np.abs(test['Customers_linear'] - y_moving['moving_average_forecast'])\/\n                                   test['Customers_linear'])*100, 2)\nmape_moving_avg","9d44e4e2":"mape_exponential_smoothing = np.round(np.mean(np.abs(test['Customers_linear'] - y_exp['Exponential_forecast'])\/\n                                   test['Customers_linear'])*100, 2)\nmape_exponential_smoothing","3b247fd4":"mape_holtwinter = np.round(np.mean(np.abs(test['Customers_linear'] - y_holtwinter['holtwinter_forecast'])\/\n                                   test['Customers_linear'])*100, 2)\nmape_holtwinter","0f63c2cc":"mape_ar = np.round(np.mean(np.abs(test['Customers_linear'] - y_ar_new['ar_forecast'])\/ test['Customers_linear'])*100, 2)\nmape_ar","143f1cb7":"mape_ma = np.round(np.mean(np.abs(test['Customers_linear'] - y_ma_new['ma_forecast'])\/ test['Customers_linear'])*100, 2)\nmape_ma","b69b6e2d":"mape_arma = np.round(np.mean(np.abs(test['Customers_linear'] - y_arma_new['arma_forecast'])\/ test['Customers_linear'])*100, 2)\nmape_arma","26d94a57":"mape_arima = np.round(np.mean(np.abs(test['Customers_linear'] - y_arima_new['arima_forecast'])\/test['Customers_linear'])*100, 2)\nmape_arima","c9fe980b":"mape_sarima = np.round(np.mean(np.abs(test['Customers_linear'] - y_sarima_new['sarima_forecast'])\/\n                               test['Customers_linear'])*100, 2)\nmape_sarima","49d905ef":"## How to choose the Right Model ?\n\n- The Thumb rule for using various time series models are :\n - When we have data points less than 10 values.\n - When we have data points more than 10 values.","1ee36cab":"## Types of Forecasting\n- Forecasting is the process of making predictions of the future based on the past and present data and most commonly by analysis of trends.\n- There are two types of forecasting\n - Quantitative Forecasting\n - Qualitative Forecasting\n \n**Quantitative Forecasting**\n- Based on past data which used numerical features.\n- Data driven and lesser bias.\n- Time series analysis and statistics\n\n**Qualitative Forecasting**\n- Done when we do not have past data to analyse.\n- It used Delphi method which involves subject matter experts.","369f2bd7":"### Which Decomposition to choose in which scenario ?\n\n- We use additive model, when the magnitude of seasonality does not change in relation to time whereas multiplicative is used when the magnitude of the seasonal pattern in the data depends on the magnitude of the data.","a4389046":"**Simple Moving Average seems to have captured the trend as well as seasonality.**","e0c83e11":"#### Last Observation Carried Forward (LOCF)\n\n- We impute the missing values with the previous value in the data.\n- To use this method we use the bfill function.","7e0fa2b8":"#### Recovering Original Time Series","7af3c43c":"- The p-value is approx 1 and its value is larger than 0.05, hence it fails to reject the null hypothesis. Hence, the data has a unit root and is non-stationary.\n- The mean value is not stationary.\n- The variance is fluctating over time.","42feb903":"#### Recovering Original Time Series","1683ffdd":"**There are two ways of decomposing a time series data**\n- i) Additional Seasonal decomposition\n- ii) Multiplicative Seasonal decomposition","9f1befec":"## Kwiatkowski-Phillips-Schmidt-Shin (KPSS) Test\n\n- The Kwiatkowski-Phillips-Schmidt-Shin (KPSS) test figures out if a time series is stationary around a mean or linear trend, or is non-stationary due to a unit root. A stationary time series is one where statistical properties like mean and variance are constant over time.\n\n- For KPSS test, \n - The Null Hypothesis : The series is stationary when p-value > 0.05.\n - The Alternate Hypothesis : The series is not stationary when p-value <= 0.05.","3c5cc064":"### Methods of Imputing Missing Values\n\n- There are four different ways of imputing missing values in aTime Series\n - Mean Imputation\n - Last Observation Carried Forward (LOCF)\n - Linear Interpolation\n - Seasonal Interpolation","b23cf672":"## Basic Forecasting Techniques","770a04ee":"### Splitting the train and test data","385bd0f6":"**What is the difference between the ARIMA and the ARIMAX model**\n- There is no extra parameter but extra variable in the case of ARIMAX and the SARIMAX model.","951069f5":"**We can conclude that in terms of mean forecast error Moving Average Model works better than the rest of the models.**","e07b10da":"### Plotting Train, Test and Forecast","1a2c59b5":"### 1. Mean Forecast Error (MFE)\n\n- Mean Forecast Error shows deviation of the forecast from the actual demand.\n- This is the mean of the sum of all the difference between the actual values and forecasted values.","ee48df63":"### 4. Simple Exponential Smoothing\n\n- In simple moving average method, we consider the last few observations to forecast the future values, and each of the past data contributes equally towards the forecast, But intuitively shouldn't the most recent observation influence more than any other obseravtion?\n- This is the main idea behind the working of a simple exponential smoothing.\n- Simple Exponential Smooothing technique captures the Level of the time series data.","fc71b74f":"**Key elements of SARIMAX Model**\n- i) Performs Seasonal Differencing to make data Stationary over the seasn.\n- ii) Models future observations as a linear regression of past observations and past seasonal errors.\n- iii) Model future observations as a linear regression of the external variable.","246e6244":"## Multiplicative Seasonal Decomposition\n\n- Multiplicative Seasonal Decomposition is when we multiply the individual components to get the time series data.\n\n$$y(t) = Level * Trend * Seasonality * Cyclicity$$","9317f1d8":"### Differencing \n- Differencing is a technique used to make the mean constant for a time series.\n- Differencing removes the Trend from a time series.","9aa0a495":"## Auto Regressive Model \n- Regressive model is forecasting the future observations as a linear regression of one or more past observations.\n- This model has a parameter called \"p\" which is the lag order p that means p is the maximum number of lags that we consider in order to forecast the future observations.\n- Autoregressive model equation would be :\n$$y(t) =  \u03b2_0 + \u03b2_1 y(t-2) + \u03b2_2 y(t-4) + \u03b2_3 y(t-6)$$","17fe9d52":"**Let's Calculate MFE of Simple Average Method, Holt Winter Exponential Smoothing and ARIMA models.**","34dce644":"## Components of Time Series\n\n- The Components of time series data are Level, Trend, Seasonality, Cyclicity and Noise.\n - Level :- Level is the baseline for entire time series. It is the average of the time series and the baseline to which we add different other components.\n - Trend :- The trend is the indication of whether the time series has moved higher or lower over the time period.\n - Seasonality :- Seasonality is the pattern in time series repeats after a fixed interval of time.\n - Cyclicity :- Cyclicity is the pattern in the time series which repeats itself after some interval of time but the interval of time is not fixed in the case of cyclicity unlikely the seasonality.\n - Noise :- Noise is the random variation in the time series and does not have any pattern. We can not use Noise to forecast the future.","4ada18ca":"#### Mean Imputation\n\n- We fill the missing values by the mean of the dataset.\n- Imputation of every missing value with the mean of the data can distort the seasonality of the data and also it will not take into consideration the nature of the data.","f02d8f99":"**Holt exponential smoothing works best among the three Models.**","30ac654c":"### From Where we can get Time Series Data ?\n\n- From Kaggle datasets \n- From Google Trends\n- By using yfinance Library","0cc14eba":"### Determining Model : Data Points < 10 \n\n- Let's suppose when we have time series data of **fewer than 10** observations that are **Noisy** then we should use **Simple Moving Average** Method because it heps cancel out the Noise.\n    - An example in which simple moving average method works well is the daily forecasting of stock prices, this is because the number of observations is fewer and the data is noisy.\n    - That's a Simple Moving Average Method is able to predict the forecast better since it takes a variation of very few data points.\n    \n- If the data points are **fewer than 10**, but the data is **neither noisy nor has any seasonal pattern** then the **naive method** works well because it will forecast the next values of the train data. In the case of a higher number of observations generally more than 10, the naive forecast tends to overpredict or underpredict the values.\n    - An example of usage of the naive method would be in forecasting the sales of a grocery store that opened recently. In this case, the sales are not much dependent on any seasonal component, and thus the naive method can be used here.\n    \n- In the case of **non noisy data** and **seasonality** with data points **fewer than 10**, the **seasonal naive method** works well.\n    - An example in which the seasonal naive method works well is forecasting the sales of a newly store that sells umbrellas on a monthly basis.","068ac05a":"### ADF Test for Stationarity\n- ADF test is a common statistical test used to test whether a given Time Series is Stationary or not. It is one of most commonly used statistical test when it comes to analyzing the stationarity of a series.\n\n- Null Hypothesis :- The Time Series is not stationary.\n- Alternate Hypothesis :- The Time series is stationary.\n\n- p-value > 0.05 :- Fails to reject null hypothesis, the data has a unit root and not stationary.\n- p-value < 0.05 :- Reject null hypothesis, the ata does not have a unit root and is stationary","3afeee25":"### Working of AR Models\n\n- We need to forecast a variable yt.\n- We need past observations such as yt - 1, yt - 2, yt - 3 etc.\n- The fundamental assumptions of Autoregressive Model \n - Stationarity\n - Auto Correlation","8fa1b638":"**Mean Absolute Error for Holt Exponential Smoothing is less than the other model's errors.**","1202490c":"### Plot train, test and forecast","0d4c3063":"### 2. Simple Average Method\n\n- In this method, we take the future predictions as the average of all historical data.","e27e6103":"### 6. Holt Winters Exponential Smoothing\n\n- Holt Winter Exponential Smoothing captures all Level, Trend, Seasonality.\n- The forecast equation is :\n$$y(t) = I(t) + b(t) + s(t + 1 - m)$$\n- where m is the number of times a season repeats in atime period.","03c4b92f":"#### Recovering Original Time Series","0c567577":"## Handling Outliers Values\n\n- Outliers are the extreme values that fall a long way outside of the other observations.\n\n#### Common Reasons for Outliers\n- Entry Errors\n- Measurement Errors\n- Natural Errors\n\n#### Why Should we worry about outliers ?\n- If we feed a Data set having such outliers then our predictive model will get highly confused and will start producing biased results.\n\n**To detect outliers we can use boxplots**\n- A boxplot is a standardized way of displaying the distribution of data.\n- Any data points which are less than Q1 - 1.5 * IQR or greater than Q3 + 1.5 * IQR are considered to be outliers.","9aed23b5":"**What is the role of this External Variable?**\n- External variables will help us to forecast the time series based on some external factors also.","6253ff06":"### Box Cox Transformation\n- Box Cox transformation transforms non normal dependent variables into normal distribution.\n- Transformation technique is used to make the variance constant for a time series.","431991cc":"### 5. Holt Exponential Smoothing\n\n- Holt's Exponential Smoothing captures the Level and Trend of the time series in the forecasting.\n- The forecast equation is a function of both level and trend.\n- y(t+1) = I(t) + b(t)\n- where I(t) is the level component and b(t) is the trend component.\n- The trend component is calculated as shown\n- b(t) = \u03b2(l(t) - l(t-1)) + (1-\u03b2)b(t-1)\n- Here beta is the smoothing parameter for trend.","50dcd41f":"**Whenever we have data we will go for quantitative forecasting whereas if in any case we do not have enough data, we choose to go for qualitative forecasting.**","200c80df":"#### Recovering Original Time Series","986b9e91":"### 3. Mean Absolute Percentage Error\n\n- Mean Absolute Percentage Error calculates the percentage of mean absolute error to get a clear idea of how much the forecasted values deviates from the actual values.","e968efee":"### Splitting data into train and test datasets","63a3a039":"#### Recovering Original Time Series","531306f3":"## Evaluation Metrics","ae5b02ab":"## ARIMAX","a4151b6f":"### 1. Naive Forecasting Method\n\n- The naive method is the simplest method of all forecasting method.\n- It looks at the last historical data and extrapolates it for all the future values without adjusting or attempting to establish casual factors.","c3ac6d4a":"### Plotting Train, Test and Forecasted data","56f80bd3":"## Auto Regressive Integrated Moving Average Model (ARIMA)\n\n- It transform the time series using Box Cox and then itself takes care of the differencing and remove the trend from the time series.\n- We have three parameters to be used :-\n- p is the highest lag in the model.\n- d id the degree of differencing to make the series stationary.\n- q is the number of past errors terms included.","ee2f1e86":"### Applications of Time Series\n- Economic Forecasting :- Process of attempting to predict the future condition ofthe economy using a combination of widely followed indicators.\n- Sales Forecasting :- Process of estimating future sales.\n- Inventory Planning :- Creating forecasts to determine how much inventory should be on hand to meet consumer demand.\n- Workforce Planning :- Process of forecasting workforce supply and demand.\n","f0d1c12a":"### Why is Evaluation of models is important ?\n\n- We can understand and analyze whether the model's performance is bad, average or good.\n- When we have two or more than two models, we can compare the working of these models and can determine which among all models is the best for a particular scenario.\n- These evaluation or performance metrics help us to justify the predictions or forecasting.\n- Evaluation metrics tells us how much errors our model can make so that we have a certain level of a confidence in our models.","9a83feff":"## Difference between Regression Analysis and Time Series Analysis\n\n#### Regression Analysis \n- To find pattern in data.\n- And using those patterns to predict dependent variable.\n\n#### Time Series Analysis \n- To identify trends in data\n- Using trends to forecast future events.","b5e6f2bf":"<p style = \"font-size : 35px; font-family : 'Comic Sans MS'; text-align : center; border-radius: 5px 5px;\"><strong>Time Series from Basic to Advance<\/strong><\/p>","686d146a":"### What is Time Series data?\n\n- Time Series is simply a series of data points having a time component in it.","02336eed":"## Auto Correlation Function\n\n- ACF is an (complete) auto-correlation function which gives us values of auto-correlation of any series with its lagged values.","7df5777b":"## Auto Regressive Moving Average Model (ARMA)\n\n- ARMA Model combines both AR and MA model.\n- It takes into account one or more past observations as well as the past errors.\n- The ARMA model contains two parameters p and q.\n- p is the highest lag in the time series.\n- q is the number of past errors included.","2729a33f":"## Converting Non-Stationary to Stationary\n\n- There are two tools for converting a non-stationary series into a stationary series:\n - Differencing\n - Transformation","3b0d8951":"## Auto Regressive Models\n\n- Auto Regressive model predict future behaviour based on past behaviour.\n- It is used when there is some correlation between values in a time series and the values that precede and suceed them.\n- In Autoregression we use a Regression model to formulate a time series problem. We make linear combination of past observations and use them to forecast future observations.","97f5802a":"### 3. Simple Moving Averages\n\n- Simple Average Method fails to capture any trend or seasonality from the time series data. To overcome this problem, we can take into account only a few of last historical data because the last few observations have more impact in the future rather than the first observation.\n\n- In this method, we take the futture predictions equal to the average of a moving window. A window can be a time period of 3 months, 6 months, 9 months or 1 year depending upon the scenarios.","5563f263":"## Determining Model : Data Points > 10\n\n- When we have time series data of **greater than 10** data points and we need to **capture only the level** of the time series then we will use **Simple Exponential Smoothing** technique as it works the best in this case.\n- When we have time series data of **greater than 10** data points and we need to **capture both the level and trend** in time series then we will either use **Holt Exponential Smoothing** technique or **ARIMA** technique.\n- When we have time series data of **greater than 10** data points and we need to **capture all the level trend and seasonality** in that case we will use **Holt Winter exponential smoothing** or **SARIMA**.\n- If there is **Exogenous Variable** present and we want to **capture all the components** of a time series that is level, trend and seasonality, In that case we will go for **ARIMAX** or **SARIMAX** model.","329f7da1":"### 2. Mean Absolute Error (MAE)\n\n- Mean Absolute Error Takes the absolute values of the differences between actual values and forecasted values.","8bd0af8c":"#### Seasonal Interpolation \n\n- We impute the missing values with the average of corresponding data points from the previous seasonal data and next seasonal data.","0f8d441a":"### Plotting Train, Test and Forecast","2408bd27":"### Outlier Treatment","ba40997f":"<p style = \"font-size : 30px; font-family : 'Comic Sans MS'; text-align : center; border-radius: 5px 5px;\"><strong>If you like the notebook, Please do Upvote<\/strong> <\/p>","748934c7":"## Additional Seasonal Decomposition\n- Additional Seasonal Decomposition is when we add the individual components to get the time series data\n\n$$y(t) = Level + Trend + Seasonality + Cyclicity$$","1f1ca26b":"##  Moving Average Model\n\n- In Moving Average Model, we consider the past forecasted errors to forecast the future values.\n- The Moving Average Model has a parameter called \"q\" which is the size of the Moving average window over which linear combinations of errors are calculated.\n- The Mathematical equation is :-\n$$y(t) = \u00b5 + \u03c6(k)*\u03b5(t-k)$$\n- \u00b5 is the mean of the series\n- \u03b5(t-k) is the past forecasted value\n- \u03c6(k) is the weight associated with error value","815fda12":"### Stationarity\n- Stationarity means that the statistical properties of a process generating a time series do not change over time.\n- The Statistical properties are :- \n - Mean\n - Variance\n - Covariance\n- These remain same irrespective of the time at which you observe.\n\n#### Why Stationarity is important ?\n- Stationarity processess are easier to analyze and model because their statistical properties do not change over time.\n- If te statistical properties are not constant over time then it is said to be a Non-Stationary Time series.\n\n**How to check Stationary ?**\n- There are two popular statistical tests using which we can test the stationary of a time series.\n - Augmented Dickey Fuller (ADF) test.\n - Kwiatkowski Phillips Schmidt Shin (KPSS) test.","b9b91b70":"## Seasonal Auto Regressive Integrated Moving Average Model (SARIMA)\n\n- SARIMA model bring all the features of ARIMA model along with the seasonality.\n\n- The key elements performed in SARIMA are :-\n - 1. The Time Series is differenced to make it stationary.\n - 2. The SARIMA equation is a linear combination of past observations and past errors.\n - 3. Seasonal Differencing is performed on the time series.\n - 4. SARIMA models future seasonality as a linear combination of past seasonality observations and past seasonality errors.","59f32059":"## Partial Auto Correlation Function (PACF)\n\n- Partial Auto Correlation Function (PACF) gives the partial correlation of a stationary time series with its own lagged values, regressed the values of the time series at all shorter lags. It contrasts with the autocorrelation function, which does not control for other lags.","41e88931":"## Handling Missing Values in Time Series Data","8548b766":"### Plotting Train, Test and Forecast","8c026e48":"## SARIMAX Model\n- The Sarimax model is the model which along with the seasonal and non-seasonal components, also models an external variable.","d0eb2890":"### Plotting Train, Test and Forecast","bc276bf6":"## Time Series Decomposition\n\n**Why is it necessary to decompose a time series data ?**\n- As we know that the Time Series data contains some kind of pattern in them, So decomposing a time series data into its different components help us to find that underlying pattern.\n\n- This also improves our understanding of a time series data."}}