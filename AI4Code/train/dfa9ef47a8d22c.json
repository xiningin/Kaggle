{"cell_type":{"48a4a63b":"code","6b8c3db9":"code","4c1ce2f1":"code","ace5b02a":"code","7742adad":"code","64fcfccb":"code","2e6cfe08":"code","f770379c":"code","d18b6fe5":"markdown","d635f597":"markdown","785447c0":"markdown","20b72e80":"markdown","82c4b160":"markdown","d5c08ceb":"markdown","e90a6308":"markdown","62c9d5a9":"markdown"},"source":{"48a4a63b":"# Get Apple stock price data from https:\/\/www.macrotrends.net\/stocks\/charts\/AAPL\/apple\/stock-price-history\nimport pandas as pd\nimport wandb\n\n# Read in dataset\napple = pd.read_csv(\"..\/input\/kernel-files\/apple.csv\")\napple = apple[-1000:]\nwandb.init(project=\"visualize-models\", name=\"a_metric\")","6b8c3db9":"%%wandb\n# Log the metric\nfor price in apple['close']:\n    wandb.log({\"Stock Price\": price})","4c1ce2f1":"# Get the dataset from UCI\n!wget https:\/\/archive.ics.uci.edu\/ml\/machine-learning-databases\/dermatology\/dermatology.data -qq\n\n# modified from https:\/\/github.com\/dmlc\/xgboost\/blob\/master\/demo\/multiclass_classification\/train.py\n# Import wandb\nimport wandb\nimport numpy as np\nimport xgboost as xgb\n\nwandb.init(project=\"visualize-models\", name=\"xgboost\")\n\n# label need to be 0 to num_class -1\ndata = np.loadtxt('.\/dermatology.data', delimiter=',',\n        converters={33: lambda x:int(x == '?'), 34: lambda x:int(x) - 1})\nsz = data.shape\n\ntrain = data[:int(sz[0] * 0.7), :]\ntest = data[int(sz[0] * 0.7):, :]\n\ntrain_X = train[:, :33]\ntrain_Y = train[:, 34]\n\ntest_X = test[:, :33]\ntest_Y = test[:, 34]\n\nxg_train = xgb.DMatrix(train_X, label=train_Y)\nxg_test = xgb.DMatrix(test_X, label=test_Y)\n# setup parameters for xgboost\nparam = {}\n# use softmax multi-class classification\nparam['objective'] = 'multi:softmax'\n# scale weight of positive examples\nparam['eta'] = 0.1\nparam['max_depth'] = 6\nparam['silent'] = 1\nparam['nthread'] = 4\nparam['num_class'] = 6\nwandb.config.update(param)\n\nwatchlist = [(xg_train, 'train'), (xg_test, 'test')]\nnum_round = 5","ace5b02a":"%%wandb\n# Add the wandb xgboost callback\nbst = xgb.train(param, xg_train, num_round, watchlist, callbacks=[wandb.xgboost.wandb_callback()])\n# get prediction\npred = bst.predict(xg_test)\nerror_rate = np.sum(pred != test_Y) \/ test_Y.shape[0]\nprint('Test error using softmax = {}'.format(error_rate))\n\nwandb.summary['Error Rate'] = error_rate","7742adad":"from sklearn.datasets import load_boston\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import Ridge\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport wandb\n# wandb.init(anonymous='allow', project=\"sklearn\")\nwandb.init(project=\"visualize-models\", name=\"sklearn\")\n\n# Load data\nboston = load_boston()\nX = pd.DataFrame(boston.data, columns=boston.feature_names)\ny = boston.target\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n\n# Train model, get predictions\nreg = Ridge()\nreg.fit(X, y)\ny_pred = reg.predict(X_test)\n\n# Visualize model performance\nwandb.sklearn.plot_regressor(reg, X_train, X_test, y_train, y_test, 'Ridge')","64fcfccb":"# WandB \u2013 Import the W&B library\nimport wandb\nfrom wandb.keras import WandbCallback\n\nfrom keras.datasets import fashion_mnist\nfrom keras.models import Sequential\nfrom keras.layers import Conv2D, MaxPooling2D, Dropout, Dense, Flatten\nfrom keras.utils import np_utils\nfrom keras.optimizers import SGD\nfrom keras.callbacks import TensorBoard","2e6cfe08":"# Default values for hyper-parameters\ndefaults=dict(\n    dropout = 0.2,\n    hidden_layer_size = 32,\n    layer_1_size = 32,\n    learn_rate = 0.01,\n    decay = 1e-6,\n    momentum = 0.9,\n    epochs = 5,\n    )\n\n# Initialize a new wandb run and pass in the config object\n# wandb.init(anonymous='allow', project=\"kaggle\", config=defaults)\nwandb.init(project=\"visualize-models\", config=defaults, name=\"neural_network\")\nconfig = wandb.config\n\n(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()\nlabels=[\"T-shirt\/top\",\"Trouser\",\"Pullover\",\"Dress\",\"Coat\",\n        \"Sandal\",\"Shirt\",\"Sneaker\",\"Bag\",\"Ankle boot\"]\n\nimg_width=28\nimg_height=28\n\nX_train = X_train.astype('float32')\nX_train \/= 255.\nX_test = X_test.astype('float32')\nX_test \/= 255.\n\n#reshape input data\nX_train = X_train.reshape(X_train.shape[0], img_width, img_height, 1)[:10000]\nX_test = X_test.reshape(X_test.shape[0], img_width, img_height, 1)[:10000]\n\n# one hot encode outputs\ny_train = np_utils.to_categorical(y_train)[:10000]\ny_test = np_utils.to_categorical(y_test)[:10000]\nnum_classes = y_test.shape[1]\n\n# build model\nmodel = Sequential()\nmodel.add(Conv2D(config.layer_1_size, (5, 5), activation='relu',\n                            input_shape=(img_width, img_height,1)))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(config.dropout))\nmodel.add(Flatten())\nmodel.add(Dense(num_classes, activation='softmax'))\n\nsgd = SGD(lr=config.learn_rate, decay=config.decay, momentum=config.momentum, nesterov=True)\nmodel.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])","f770379c":"%%wandb\n# Add WandbCallback() to the fit function\nmodel.fit(X_train, y_train,  validation_data=(X_test, y_test), epochs=config.epochs,\n    callbacks=[WandbCallback(data_type=\"image\", labels=labels)])","d18b6fe5":"# Why Weights & Biases Gives You An Edge On Kaggle Competitions\n\nExcited to announce that [Weights & Biases](https:\/\/www.wandb.com\/) now comes baked into your Kaggle kernels! Used by the likes of [OpenAI](http:\/\/openai.com\/) and [Github](https:\/\/github.blog\/2019-09-26-introducing-the-codesearchnet-challenge\/), W&B is part of the new standard of best practices for machine learning.\n\nI thought I'd write a quick post on why my fellow Kagglers might find W&B useful, and how you can integrate it into your projects with just a few lines of code. You can find the [full blog post here](https:\/\/www.wandb.com\/articles\/better-models-faster-with-weights-biases) \u2013 I highly encourage you check it out to run a more efficient ML process than your peers.\n\n**Tl;dr \u2013** W&B helps you visualize your model performance and predictions, find the best models fast and share insights learned from your models.\n\nHere are a few use cases in which W&B is specially useful for Kagglers:\n1. **Track and compare models:** I want to test out my hypotheses fast and iterate quickly to find the best model\n2. **Visualize model performance for debugging:** I want to see how my models are performing in real time and debug them\n3. **Efficient hyperparameter search:** I want to find the best model faster than everyone else\n4. **Resource efficient model training:** I want to be efficient with my model training and not spend more money than I need to\n5. **Show off my model:** I want to share my models and key insights with my teammates and the Kaggle community\n\nThe blog post goes into detail about how W&B helps address all of these needs. In this Kernel, I want to show you how to integrate W&B in your models in just a few lines of code.\n\nFirst, here's a quick overview of some of the visualizations W&B automatically creates for your models:\n\n### Visualize model performance and training metrics\n<img src=\"https:\/\/paper-attachments.dropbox.com\/s_92F7A2BE132D5E4492B0E3FF3430FFF0FB2390A4135C0D77582A2D21A2EF8567_1574108018540_Screenshot+2019-11-18+12.13.29.png\" style=\"width: 80%\"\/>\n\n### Visualize the affect of hyperparameters on model performance\n<img src=\"https:\/\/paper-attachments.dropbox.com\/s_C0EC7008D045FC80715C08E7386E0BBDA59DC92DEE34C734FEA67BF25E4BA5CC_1578297697453_image.png\" style=\"width: 80%\"\/>\n\n### Visualize model predictions\n<img src=\"https:\/\/paper-attachments.dropbox.com\/s_C0EC7008D045FC80715C08E7386E0BBDA59DC92DEE34C734FEA67BF25E4BA5CC_1578297173261_vzye9ei.png\" style=\"width: 80%\"\/>\n\n### Visualize gradients to help deal with vanishing and exploding gradients\n<img src=\"https:\/\/paper-attachments.dropbox.com\/s_C0EC7008D045FC80715C08E7386E0BBDA59DC92DEE34C734FEA67BF25E4BA5CC_1578298883597_image.png\" style=\"width: 80%\"\/>\n\n### Visualize system metrics, including GPU usage\n<img src=\"https:\/\/paper-attachments.dropbox.com\/s_C0EC7008D045FC80715C08E7386E0BBDA59DC92DEE34C734FEA67BF25E4BA5CC_1578297329745_image.png\" style=\"width: 80%\"\/>\n\n### Visualize images, videos, html, audio, 3D objects, plots, tables, point clouds\n<img src=\"https:\/\/paper-attachments.dropbox.com\/s_C0EC7008D045FC80715C08E7386E0BBDA59DC92DEE34C734FEA67BF25E4BA5CC_1578297638486_image.png\" style=\"width: 80%\"\/>\n\nAs you can see W&B structures your metrics & predictions, and presents them in a way that provides actionable insights about your training process.\n\nAlright, now that you know how W&B can be useful, let's see how you can integrate it into your project and get all these visualizations, with just a few lines of code.\n\nIf you have an existing kernel, you just need to switch your Docker image to 'Latest Available'. All new kernels should work out of the box.","d635f597":"# 4. Monitor neural network performance\n\nStart out by installing the experiment tracking library and setting up your free W&B account:\n\n*   **import wandb** \u2013 Import the wandb library\n*   **from wandb.keras import WandbCallback** \u2013 Import the wandb [keras callback](https:\/\/docs.wandb.com\/library\/frameworks\/keras)","785447c0":"# 3. Monitor scikit learn performance\n\nLogging sklearn plots with Weights & Biases is simple.\n\n### Step 1: First import wandb and initialize a new run.\n```\nimport wandb\nwandb.init(project=\"visualize-sklearn\")\n\n# load and preprocess dataset\n# train a model\n```\n\n### Step 2: Visualize individual plots.\n```\n# Visualize single plot\nwandb.sklearn.plot_confusion_matrix(y_true, y_probas, labels)\n```\n\n### Or visualize all plots at once:\n```python\n# Visualize all classifier plots\nwandb.sklearn.plot_classifier(clf, X_train, X_test, y_train, y_test, y_pred, y_probas, labels, model_name='SVC', feature_names=None)\n\n# All regression plots\nwandb.sklearn.plot_regressor(reg, X_train, X_test, y_train, y_test,  model_name='Ridge')\n\n# All clustering plots\nwandb.sklearn.plot_clusterer(kmeans, X_train, cluster_labels, labels=None, model_name='KMeans')\n```","20b72e80":"# 1. Log any metric with Weights and Biases\n*   **wandb.init()** \u2013 Initialize a new W&B run. Each run is single execution of the training script.\n- **wandb.log()** \u2013 Logs custom objects like images, videos, audio files, HTML, plots, point clouds etc.\n-   **%%wandb** \u2013 Add this at the top of a cell to show model metrics live below the cell","82c4b160":"## Find the best neural network model with hyperparameter sweeps\n\nSearching through high dimensional hyperparameter spaces to find the winning model can get unwieldy very fast. W&B's hyperparameter sweeps provide an organized and efficient way to conduct a battle royale of models and pick the best one.\n\nRunning a hyperparameter sweep with Weights & Biases is very easy. You can use [this guide](https:\/\/docs.wandb.com\/sweeps\/overview\/getting-started) to get started.\n\n![](https:\/\/paper-attachments.dropbox.com\/s_A8A9577ACEF2EF9A66A68CAA0D798FE3970C9A78CA8BF44A10FA307611490E90_1572034183402_Screenshot+2019-10-25+13.09.37.png)","d5c08ceb":"## More Resources\n\nCheck out some other cool things you can do with Weights & Biases:\n* [Visualize sklearn models](https:\/\/app.wandb.ai\/lavanyashukla\/visualize-sklearn\/reports\/Visualize-Sklearn-Model-Performance--Vmlldzo0ODIzNg)\n* [Visualize model predictions](https:\/\/app.wandb.ai\/lavanyashukla\/visualize-predictions\/reports\/Visualize-Model-Predictions--Vmlldzo1NjM4OA\/)\n* [Track model performance](https:\/\/app.wandb.ai\/lavanyashukla\/visualize-models\/reports\/Visualize-Model-Performance--Vmlldzo1NTk2MA)","e90a6308":"# 2. Monitor boosting model performance\n\nStart out by importing the experiment tracking library and setting up your free W&B account:\n\n*   **import wandb** \u2013 Import the wandb library\n*   **callbacks=[wandb.xgboost.wandb_callback()]** \u2013 Add the wandb [XGBoost callback](https:\/\/docs.wandb.com\/library\/frameworks\/xgboost), or\n*   **callbacks=[wandb.lightgbm.wandb_callback()]** \u2013 Add the wandb LightGBM callback","62c9d5a9":"*   **wandb.init()** \u2013 Initialize a new W&B run. Each run is single execution of the training script.\n*   **wandb.config** \u2013 Save all your hyperparameters in a config object. This lets you use W&B app to sort and compare your runs by hyperparameter values.\n*   **callbacks=[WandbCallback()]** \u2013 Fetch all layer dimensions, model parameters and log them automatically to your W&B dashboard."}}