{"cell_type":{"8fdee361":"code","de2bdd17":"code","a5f26a63":"code","3ce5356b":"code","c37cc9cd":"code","107bd932":"code","524c8d11":"code","caf0f8cf":"code","5a5c6d79":"code","430a531b":"code","408f0185":"code","60ef2a5c":"code","9db44106":"code","74b5fcaf":"code","43d6c4dc":"code","e65cbc5f":"code","89322dfb":"code","bb19e491":"code","f252a303":"code","27fa9f6c":"code","68e360dd":"code","cc9ebcb1":"code","23fbdaf5":"code","52356c35":"code","ecd9b12d":"code","ca92a2dd":"code","a31c047a":"code","d720ee54":"code","fd645dff":"code","55c70df0":"code","787df22c":"code","f03e8c08":"code","cd4a3c1d":"code","3c691cf6":"markdown","deee9694":"markdown","87c22ea2":"markdown","95b9a272":"markdown","4fab41a6":"markdown","865a3379":"markdown","cd0b59f3":"markdown","fb84a455":"markdown","bbae1fc4":"markdown","b70fe365":"markdown","6a6f7e7b":"markdown","6ba1f9f6":"markdown","9301fb85":"markdown","cc4cb80a":"markdown","4596ec74":"markdown","229942f8":"markdown","384e5ff7":"markdown","9b5fcc8f":"markdown","13ada24d":"markdown","6f4e83fc":"markdown","5b9c4c2e":"markdown","41b2ef75":"markdown","23c0ec24":"markdown","7e0efa57":"markdown"},"source":{"8fdee361":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nfrom plotly.offline import init_notebook_mode, iplot\ninit_notebook_mode(connected=True)\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","de2bdd17":"df = pd.read_csv('\/kaggle\/input\/medium-articles-dataset\/medium_data.csv')\ndf.head()","a5f26a63":"import seaborn as sns\nsns.distplot(df['claps'])","3ce5356b":"df.isna().sum()","c37cc9cd":"import plotly.express as px\nfig = px.bar(x=list(df['publication'].value_counts().index), y=list(df['publication'].value_counts()))\nfig.show()","107bd932":"import plotly.express as px\nfig = px.bar(x=list(df.isna().sum().index), y=list(df.isna().sum()), title='Nan values')\nfig.show()","524c8d11":"import plotly.express as px\nfig = px.box(df,x='publication', y='claps', title='Box plot claps')\nfig.show()","caf0f8cf":"import plotly.express as px\nfig = px.scatter(df,y='claps', x='reading_time', title='Read time vs Claps', color='publication')\nfig.show()","5a5c6d79":"fig = px.scatter(df,y='claps', x='responses', title='Claps vs Response', color='publication')\nfig.show()","430a531b":"fig = px.scatter(df,y='responses', x='reading_time', title='Response vs Read time', color='publication')\nfig.show()","408f0185":"from plotly.subplots import make_subplots\nimport plotly.graph_objects as go\n\ndef plot_scatter(df):\n    fig = make_subplots(rows=len(df['publication'].unique()), cols=3, specs=[[{\"type\": \"scatter\"}, {\"type\": \"scatter\"}, {\"type\": \"scatter\"}],\n                                                                            [{\"type\": \"scatter\"}, {\"type\": \"scatter\"}, {\"type\": \"scatter\"}],\n                                                                            [{\"type\": \"scatter\"}, {\"type\": \"scatter\"}, {\"type\": \"scatter\"}],\n                                                                            [{\"type\": \"scatter\"}, {\"type\": \"scatter\"}, {\"type\": \"scatter\"}],\n                                                                            [{\"type\": \"scatter\"}, {\"type\": \"scatter\"}, {\"type\": \"scatter\"}],\n                                                                            [{\"type\": \"scatter\"}, {\"type\": \"scatter\"}, {\"type\": \"scatter\"}],\n                                                                            [{\"type\": \"scatter\"}, {\"type\": \"scatter\"}, {\"type\": \"scatter\"}]])\n    i=1\n    for public in df['publication'].unique():\n        aux = df[df['publication'] == public]\n        fig.append_trace(go.Scatter(x=aux['claps'], y=aux['responses'], name=str(public)+\" claps vs \" + str(public)+ \" responses\",  mode='markers'), row=i, col=1)\n        fig.append_trace(go.Scatter(x=aux['claps'], y=aux['reading_time'], name=str(public)+\" claps vs \" + str(public)+ \" reading_time\",  mode='markers'), row=i, col=2)\n        fig.append_trace(go.Scatter(x=aux['reading_time'], y=aux['responses'], name=str(public)+\" reading_time\" + \" vs \" + str(public)+ \" responses\",  mode='markers'), row=i, col=3)\n        i+=1\n    fig.show()\nplot_scatter(df)","60ef2a5c":"from sklearn.feature_extraction.text import CountVectorizer\n\nimport nltk\nimport re\nfrom tqdm import tqdm \ntqdm.pandas()\ndef get_top_n_words(corpus, n=None, vocabulary=None):\n    vec = CountVectorizer(vocabulary=vocabulary).fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in     vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]\n\n\ndef preprocess(df):\n    stopwords = nltk.corpus.stopwords.words('english')\n    df['title_process'] = df['title'].astype(str)\n    df['title_process'] = df['title_process'].progress_apply(lambda x : x.lower())\n    df['title_process'] = df['title_process'].progress_apply(lambda x : nltk.word_tokenize(x))\n    df['title_process'] = df['title_process'].progress_apply(lambda x : [item for item in x if item not in stopwords])\n    df['title_process'] = df['title_process'].progress_apply(lambda x : \" \".join(x))\n    df['title_process'] = df['title_process'].str.replace('@[^\\s]+', \"\")\n    df['title_process'] = df['title_process'].str.replace('https?:\\\/\\\/.*[\\r\\n]*', '')\n    df['title_process'] = df['title_process'].str.normalize('NFKD').str.encode('ascii', errors='ignore').str.decode('utf-8')\n    df['title_process'] = df['title_process'].str.replace('\\d+', '')\n    df['title_process'] = df['title_process'].str.replace('[^\\w\\s]', '')\n    return df\n\ndf = preprocess(df)\nlist_ = get_top_n_words(df['title_process'], n=20)\nnew_list_words = [ seq[0] for seq in list_ ]\nnew_list_values = [ seq[1] for seq in list_ ]\nfig = px.bar(y=new_list_words, x=new_list_values, title='Real news Frequency words',  orientation='h')\nfig.show()","9db44106":"from sklearn.feature_extraction.text import TfidfVectorizer\n\nvectorizer = TfidfVectorizer()\nX = vectorizer.fit_transform(df['title_process'])","74b5fcaf":"from sklearn.feature_selection import SelectKBest, f_regression\nfrom sklearn.ensemble import RandomForestRegressor\n\nrandom = RandomForestRegressor()\nrandom.fit(X, df['claps'])\n","43d6c4dc":"import plotly.express as px\ndict_ = dict(zip(vectorizer.get_feature_names(), random.feature_importances_))\ndict_ = {k: v for k, v in sorted(dict_.items(), key=lambda item: item[1], reverse=True)}\ndict_\nfig = px.bar(x=list(dict_.values())[0:50], y=list(dict_.keys())[0:50], orientation='h')\nfig.show()","e65cbc5f":"from tqdm import tqdm\nimport plotly.graph_objects as go\n\ndef bar_plot(df):\n        fig = make_subplots(rows=4, cols=2, specs=[[{\"type\": \"bar\"}, {\"type\": \"bar\"}],\n                                                    [{\"type\": \"bar\"}, {\"type\": \"bar\"}],\n                                                    [{\"type\": \"bar\"}, {\"type\": \"bar\"}],\n                                                    [{\"type\": \"bar\"}, {\"type\": \"bar\"}]])\n        i =1\n        j =1\n        for class_ in tqdm(df['publication'].unique()):\n            aux = df[df['publication']==class_]\n            vectorizer = TfidfVectorizer()\n            X = vectorizer.fit_transform(aux['title_process'])\n            random = RandomForestRegressor()\n            random.fit(X, aux['claps'])\n            dict_ = dict(zip(vectorizer.get_feature_names(), random.feature_importances_))\n            dict_ = {k: v for k, v in sorted(dict_.items(), key=lambda item: item[1], reverse=True)}\n            fig.append_trace(go.Bar(x=list(dict_.values())[0:50], y=list(dict_.keys())[0:50], orientation='h', name=class_), row=i, col=j)\n            if j == 2:\n                i+=1\n                j=0\n            j+=1\n        fig.show()\n            \nbar_plot(df)","89322dfb":"def parallel(df):\n    for class_ in tqdm(df['publication'].unique()):\n        aux = df[df['publication'] == class_]\n        vectorizer = TfidfVectorizer()\n        X = vectorizer.fit_transform(aux['title_process'])\n        random = RandomForestRegressor()\n        random.fit(X, aux['claps'])\n        dict_ = dict(zip(vectorizer.get_feature_names(), random.feature_importances_))\n        dict_ = {k: v for k, v in sorted(dict_.items(), key=lambda item: item[1], reverse=True)}\n        frame = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names())\n\n        frame = frame[list(dict_.keys())[0:10]]\n        fig = px.parallel_categories(frame, title=class_+\" 10 important features correlated\")\n        fig.show()","bb19e491":"parallel(df)","f252a303":"def contains_word(word, text):\n    if str(word) in text:\n        return True\n    return False\n\ndef get_stds(df):\n    mask = []\n    stds = {}\n    df['claps'] = df['claps'].astype(float)\n    for class_ in tqdm(df['publication'].unique()):\n        aux = df[df['publication'] == class_]\n        vectorizer = TfidfVectorizer()\n        X = vectorizer.fit_transform(aux['title_process'])\n        random = RandomForestRegressor()\n        random.fit(X, aux['claps'])\n        dict_ = dict(zip(vectorizer.get_feature_names(), random.feature_importances_))\n        dict_ = {k: v for k, v in sorted(dict_.items(), key=lambda item: item[1], reverse=True)}\n        for word in list(dict_.keys())[0:10]:\n            for text in aux['title_process']:\n                bool_ = contains_word(word, text)\n                mask.append(bool_)\n            select = aux[mask]\n            if select.shape[0] == 1:\n                a= list(select['claps'])\n                stds[class_+' '+word]=(a[0], 1, a[0])\n            else:\n                stds[class_+' '+word]=(select['claps'].std(),select.shape[0], select['claps'].mean()) \n            mask = []\n    return stds\nstds = get_stds(df)","27fa9f6c":"stds","68e360dd":"list_1 = []\nlist_2 = []\nlist_3 = []\nfor a, b, c in list(stds.values()):\n    list_1.append(a)\n    list_2.append(b)\n    list_3.append(c)\nfig = px.scatter(x=list(stds.keys()), y=list_3, size=list_1)\nfig.show()","cc9ebcb1":"import lightgbm as lgb\nfrom sklearn.model_selection import GridSearchCV\n\nlightgbm_params ={'learning_rate':[0.0001, 0.001, 0.003, 0.01, 0.1],\n                  'n_estimators':[3,5,10,20, 50, 100],\n                 'max_depth':[4, 6, 10, 15, 20, 50]}\ngbm = lgb.LGBMRegressor(random_state = 42)\nclf_gbm = GridSearchCV(gbm, lightgbm_params, cv=4, n_jobs= 4, verbose = 1)\nvectorizer = TfidfVectorizer()\nX = vectorizer.fit_transform(df['title_process'])\nclf_gbm.fit(X, df['claps'])\nprint(clf_gbm.best_estimator_)\nprint(clf_gbm.best_score_)","23fbdaf5":"from sklearn.ensemble import AdaBoostRegressor\nadam_boosting_params = {'learning_rate':[0.0001, 0.001, 0.003, 0.01, 0.1,1],\n                        'n_estimators':[10,20, 50, 100]}\nada = AdaBoostRegressor(random_state=42)\nclf_ada = GridSearchCV(ada, adam_boosting_params, cv=4,n_jobs= 4, verbose = 1)\nclf_ada.fit(X, df['claps'])\nprint(clf_ada.best_estimator_)\nprint(clf_ada.best_score_)","52356c35":"from sklearn.ensemble import RandomForestRegressor\nlightgbm_params ={'n_estimators':[3,5,10,20, 50, 100],\n                 'max_depth':[4, 6, 10, 15, 20, 50]}\n\nrandom = RandomForestRegressor(random_state = 42)\nclf_random = GridSearchCV(random, lightgbm_params, cv=4, n_jobs= 4, verbose = 1)\nclf_random.fit(X, df['claps'])\nprint(clf_random.best_estimator_)\nprint(clf_random.best_score_)","ecd9b12d":"from sklearn.linear_model import Ridge\nridge_params = {'alpha': [0.0001,0.001, 0.01, 1, 0.1, 10, 100, 1000, 10000, 100000, 1000000]}\n\nrid = Ridge(random_state=42)\nclf_ada = GridSearchCV(rid, ridge_params, cv=4,  scoring='neg_mean_squared_error',n_jobs= 4, verbose = 1)\nclf_ada.fit(X, df['claps'])\nprint(clf_ada.best_estimator_)\nprint(clf_ada.best_score_)","ca92a2dd":"from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nX_train, X_test, y_train, y_test = train_test_split(\n    X,  df['claps'], random_state=42\n)","a31c047a":"scores ={}\nclf = lgb.LGBMRegressor(learning_rate=0.01, max_depth=50, n_estimators=20, random_state=42)\nmodel = clf.fit(X_train, y_train)\npred = model.predict(X_test)\nscore = np.sqrt(mean_squared_error(y_test, pred))\nscores['lgb'] = score","d720ee54":"import plotly.graph_objects as go\n\ndef plot_predict(pred, true):\n    indexs = []\n    for i in range(len(pred)):\n        indexs.append(i)\n        \n\n    fig = go.Figure()\n\n    fig.add_trace(go.Line(\n        x=indexs,\n        y=pred,\n        name=\"Predict\"\n    ))\n\n    fig.add_trace(go.Line(\n        x=indexs,\n        y=true,\n        name=\"Test\"\n    ))\n\n    fig.show()","fd645dff":"plot_predict(pred, y_test)","55c70df0":"clf = AdaBoostRegressor(learning_rate=0.01, n_estimators=10, random_state=42)\nmodel = clf.fit(X_train, y_train)\npred = model.predict(X_test)\nscore = np.sqrt(mean_squared_error(y_test, pred))\nscores['ada'] = score\nplot_predict(pred, y_test)","787df22c":"clf = Ridge(alpha=10, random_state=42)\nmodel = clf.fit(X_train, y_train)\npred = model.predict(X_test)\nscore = np.sqrt(mean_squared_error(y_test, pred))\nscores['ridge'] = score\nplot_predict(pred, y_test)","f03e8c08":"clf = RandomForestRegressor(max_depth=4, random_state=42)\nmodel = clf.fit(X_train, y_train)\npred = model.predict(X_test)\nscore = np.sqrt(mean_squared_error(y_test, pred))\nscores['rf'] = score\nplot_predict(pred, y_test)","cd4a3c1d":"result = pd.DataFrame([])\nresult['model'] = list(scores.keys())\nresult['score'] = list(scores.values())\nresult = result.sort_values(['score'])\nresult.head(10)","3c691cf6":"* Interesting how what has been observed before spreads in almost all types of publication of the medium, only the relationship between better marketing with claps and reading time that differs, maybe the type of content that is worked on is different and requires more details about what is being treated and pleases the target audience","deee9694":"* It is interesting to observe how all publications with these words have a high standard deviation value, so the titles have titles with many evaluations and others with few, but these high values \u200b\u200bof deviation indicate that the majority of publications in the title have these words are highly well rated\n","87c22ea2":"* ","95b9a272":"<a id=\"1\"><\/a> <br>\n<font size=\"+3\" color=\"black\"><b>1 - Introduction<\/b><\/font><br><a id=\"1\"><\/a>\n<br> \n\n*  this notebook will look for a pattern between claps and article medium title","4fab41a6":"* Subtitle and image has nan values, to this kernel none of them will be used, we just evaluate correlation between claps and title","865a3379":"* by classes in general we could not identify words that are strong and striking perhaps to justify the number of claps, to try to better understand the data we will do the same analysis distributed by each type of publication\n","cd0b59f3":"* let's observe with the main words correlate","fb84a455":"* we will try to assess the impact of these words within the evaluations if there is a behavioral relationship in relation to their presence and the claps","bbae1fc4":"<img src='https:\/\/miro.medium.com\/max\/5000\/0*1fuhZuSSFl-861-T' style='height:400px'>","b70fe365":"* There are 7 types of web sites that pertende medium group in dataset","6a6f7e7b":"* According to the number of claps the number of responses to the article grows, it is normal when people like the content, comment praising or taking doubts about what was written","6ba1f9f6":"* Now to identify which words have the most influence on the issue of claps, let's use the random forest to get the most relevant words\n","9301fb85":"<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n  <h1 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\"  role=\"tab\" aria-controls=\"home\">&nbsp;Summary:<\/h1>\n  <a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#1\" role=\"tab\" aria-controls=\"profile\">1. Introduction<span class=\"badge badge-primary badge-pill\">1<\/span><\/a>\n  <a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#2\" role=\"tab\" aria-controls=\"messages\">2. EDA<span class=\"badge badge-primary badge-pill\">2<\/span><\/a>\n   <a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#3\" role=\"tab\" aria-controls=\"messages\">3. Models<span class=\"badge badge-primary badge-pill\">3<\/span><\/a>\n<\/div>","cc4cb80a":"<a id=\"3\"><\/a> <br>\n<font size=\"+3\" color=\"black\"><b>3 - Models<\/b><\/font><br><a id=\"3\"><\/a>\n<br> ","4596ec74":"<a id=\"2\"><\/a> <br>\n<font size=\"+3\" color=\"black\"><b>2 - EDA<\/b><\/font><br><a id=\"2\"><\/a>\n<br> ","229942f8":"* claps distribution is concrate between 0 and 5000\n* So maybe is necessary observe outlier presence in this data","384e5ff7":"* interesting to observe how most of the time the most important words of each class correlate having a tfidf value of 0 or thus the absence of these words since tfidf or sklearn is not 0 when the idf log value of 0","9b5fcc8f":"* Some keywords can be observed when we think about certain articles such as design, machine, learning which are words that are very present in certain types of publications\n","13ada24d":"* Publications that have high read time has low number os claps. In general people look for small texts that can be highly informative, people may be watching the reading time and failing to see the full article due to the time that will be used and this prevents them from coming to give their evaluations\n ","6f4e83fc":"* we will now observe how is the relation of the words of the title and the claps received by the articles","5b9c4c2e":"* within each of the classes we will observe how these general observations propagate","41b2ef75":"* as previously noted articles that have a high reading time have few interactions from people","23c0ec24":"* by class keywords for publications are raised, if you want to have better visibility of the words from a zoom in each of the above graphs\n","7e0efa57":"* So like we observer in claps distribution, not is normal articles receive so much claps\n\n* So to better undestand the data I will look to correlation among time read and response with claps"}}