{"cell_type":{"e8302025":"code","b2728b3d":"code","b153bd5b":"code","e686015e":"code","d5a23c61":"code","198669ed":"code","518cdae0":"code","7fe8183a":"code","b1249ac7":"code","b8d832bb":"code","0e325ede":"code","914018a6":"code","453831f4":"code","3f963084":"markdown","0869d5c9":"markdown","61dc3a7f":"markdown"},"source":{"e8302025":"from numbers import Real\nimport pandas as pd\nimport numpy as np\n\nfrom sklearn import ensemble\nfrom sklearn import metrics\nfrom sklearn import model_selection\nfrom sklearn import decomposition\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn import pipeline\nfrom functools import partial\nfrom skopt import space\nfrom skopt import gp_minimize \nfrom xgboost import XGBClassifier\n\n\nimport optuna\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","b2728b3d":"df_train = pd.read_csv('..\/input\/tabular-playground-series-nov-2021\/train.csv')\nX_test = pd.read_csv('..\/input\/tabular-playground-series-nov-2021\/test.csv')\nsample_submission = pd.read_csv('..\/input\/tabular-playground-series-nov-2021\/sample_submission.csv')","b153bd5b":"df_train.head()","e686015e":"print(df_train.shape)\nprint(X_test.shape)","d5a23c61":"y = df_train['target']\ndf_train.pop('target')\ndf_train.pop('id')\nX_test.pop('id')\nX=df_train\n\ndel df_train","198669ed":"print(X.shape)\nprint(X_test.shape)","518cdae0":"st_scaler = StandardScaler()\nX = st_scaler.fit_transform(X)\nX_test = st_scaler.fit_transform(X_test)","7fe8183a":"\"\"\"def optimize(trial, x, y):\n\n   \n    n_estimators = trial.suggest_int('n_estimators', 100, 1500)\n    max_depth = trial.suggest_int('max_depth', 3, 25)\n    learning_rate = trial.suggest_uniform('learning_rate', 1e-3, 0.25)\n    subsample = trial.suggest_uniform('subsample', 0.6, 1.0)\n    colsample_bytree = trial.suggest_uniform('colsample_bytree', 0.6, 1)\n    \n\n    \n    model = XGBClassifier(n_estimators = n_estimators, max_depth = max_depth, \n                          learning_rate = learning_rate, \n                          subsample = subsample,\n                         colsample_bytree = colsample_bytree,\n                            random_state  = 42, \n                          use_label_encoder=False, \n                         tree_method = 'gpu_hist',\n                         gpu_id = 0,\n                         predictor = 'gpu_predictor')\n    \n\n    kf = model_selection.StratifiedKFold(n_splits=5)\n    accuracies = []\n    for idx in kf.split(X=x, y = y):\n        train_idx, test_idx = idx[0], idx[1]\n        xtrain = x[train_idx]\n        ytrain = y[train_idx]\n\n        xtest = x[test_idx]\n        ytest = y[test_idx]\n\n        model.fit(xtrain, ytrain, \n                  eval_set = [(xtest, ytest)],\n              early_stopping_rounds = 100,\n              eval_metric = 'auc',\n             verbose = False)\n        preds = model.predict_proba(xtest)[:,1]\n        fold_acc = metrics.roc_auc_score(ytest, preds)\n        accuracies.append(fold_acc)\n\n    return -1.0*np.mean(accuracies)\"\"\"","b1249ac7":"\"\"\"optimization_function = partial(optimize, x=X, y=y)\n    \nstudy = optuna.create_study(direction = \"minimize\")\nstudy.optimize(optimization_function, n_trials=15)\"\"\"","b8d832bb":"best_params = {'n_estimators': 1243, \n               'max_depth': 3, 'learning_rate': 0.11716080504654952, \n               'subsample': 0.8098472503300551, \n               'colsample_bytree': 0.7047898383605972}","0e325ede":"folds = model_selection.StratifiedKFold(n_splits = 5, random_state = 42, shuffle = True)\ny_pred = np.zeros(len(X_test))\nscores = []\nfor fold, (trn_idx, val_idx) in enumerate(folds.split(X, y)):\n    \n    X_train, X_val = X[trn_idx], X[val_idx]\n    y_train, y_val = y[trn_idx], y[val_idx]\n\n    model =  XGBClassifier(**best_params, random_state  = 42, \n                          use_label_encoder=False, \n                         tree_method = 'gpu_hist',\n                         gpu_id = 0,\n                         eval_metric = 'error',\n                         predictor = 'gpu_predictor')\n   \n    model.fit(X_train, y_train, eval_set = [(X_train, y_train), (X_val, y_val)], \n              verbose = False, early_stopping_rounds = 100)\n    final_preds = model.predict_proba(X_val)[:,1]\n    fold_score = metrics.roc_auc_score(y_val, final_preds)\n    scores.append(fold_score)\n    y_pred += model.predict_proba(X_test)[:,1] \/ folds.n_splits \n\nprint(scores)","914018a6":"sample_submission.head()","453831f4":"sample_submission['target'] = y_pred\nsample_submission.to_csv('Submission.csv',index = False)","3f963084":"### Dividing dependent and independent variables and adding new features\nIt can be seen that columns like \"Id\" are unique, hence wont contribute for our predictions. Therefore, these must be removed from both training and testing datasets.","0869d5c9":"### Optuna Search using XGBClassifier\n#### Important note\nThe following cell can be uncommented to run the hyperparameter tuning process which uses optuna method","61dc3a7f":"## Best Parameters\n{'n_estimators': 1243, 'max_depth': 3, 'learning_rate': 0.11716080504654952, 'subsample': 0.8098472503300551, 'colsample_bytree': 0.7047898383605972}"}}