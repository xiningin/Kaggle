{"cell_type":{"429cd13d":"code","c7379eaf":"code","d669437c":"code","402579a3":"code","288ff90b":"code","2f698c33":"code","32fcb550":"code","6347f68d":"code","43ed2226":"code","0a7a082b":"code","9bef7c0e":"code","fa60dafe":"code","109aee5c":"code","89cbd073":"code","3024227a":"code","cacf6269":"code","d106a36f":"code","bc00e69f":"code","6247d123":"code","e123fbab":"code","335a6357":"code","096afdba":"code","e9983aaa":"code","d5c3b1bc":"code","b04c3efc":"code","7190091d":"code","25a3792b":"code","dccc28b9":"code","5e6fedc5":"code","56d5199d":"code","fc0ca534":"code","00233625":"code","f3dd96aa":"code","b97c61e1":"code","a69451b8":"code","00938f46":"code","08e578f7":"code","e2661219":"code","868beac3":"code","75362cbb":"code","84a26655":"code","e33425f2":"code","d2617ec7":"code","c4bb8929":"code","0437a15b":"code","540f49f1":"code","35233182":"code","2f33feb4":"markdown","200c7f0a":"markdown","fc7b3f6b":"markdown","5687a82b":"markdown","23302969":"markdown","f604d99b":"markdown","68d820b0":"markdown","111a8e86":"markdown","ba0779f5":"markdown","9330aa74":"markdown"},"source":{"429cd13d":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nfrom fastai.vision import *","c7379eaf":"verbose = 1","d669437c":"import os\nif verbose >= 1:\n    for dirname, _, filenames in os.walk('\/kaggle\/input'):\n        for filename in filenames:\n            print(os.path.join(dirname, filename))","402579a3":"input_path = '..\/input\/planet-understanding-the-amazon-from-space\/'\npath = Config.data_path()\/'planet'\npath.mkdir(parents=True, exist_ok=True)\nif verbose >= 1:\n    print(path)","288ff90b":"# Files already exist here, so not need to untar\nif verbose >= 2:\n    ! ls -al {input_path}\/train-jpg","2f698c33":"if verbose >= 2:\n    ! ls -al \/tmp\/.fastai\/data\/planet\/","32fcb550":"# Copying Data to a writable path since DataBunch will be created from here and the model will be saved here (hence needs write permisions)\n# Will take some time\nimport time\nstart_time = time.time()\n!cp -r {input_path}train_v2.csv {path}\/.\n!cp -r {input_path}train-jpg {path}\/.\nend_time = time.time()\nprint(\"Time Taken: {}\".format(end_time - start_time))","6347f68d":"df_train = pd.read_csv(path\/'train_v2.csv')\ndf_train.head()","43ed2226":"tfms = get_transforms(flip_vert=True, max_lighting=0.1, max_zoom=1.05, max_warp=0.)","0a7a082b":"if verbose >= 2:\n    doc(ImageList.from_csv)","9bef7c0e":"np.random.seed(42)\nsrc = (ImageList.from_csv(path=path, csv_name='train_v2.csv', folder='train-jpg', suffix='.jpg')  # Where to find the data? -> in path\/'train-jpg' folder\n       .split_by_rand_pct(0.2) # How to split in train\/valid? -> randomly with the default 20% in valid\n       .label_from_df(label_delim=' ')) # How to label? -> use the second column of the csv file and split the tags by ' '","fa60dafe":"data = (src.transform(tfms, size=128) # Data augmentation? -> use tfms with a size of 128\n        .databunch().normalize(imagenet_stats)) # Finally -> use the defaults for conversion to databunch","109aee5c":"data.show_batch(rows=3, figsize=(12,9))","89cbd073":"arch = models.resnet50","3024227a":"# Kaggle comes with internet off. So have to copy over model to the location where fastai would have downloaded it.\n# https:\/\/forums.fast.ai\/t\/how-can-i-load-a-pretrained-model-on-kaggle-using-fastai\/13941\/23\n\n!mkdir -p \/tmp\/.cache\/torch\/checkpoints\n!cp ..\/input\/resnet50\/resnet50.pth \/tmp\/.cache\/torch\/checkpoints\/resnet50-19c8e357.pth","cacf6269":"# We are looking for multiple labels here, so we look for anything with prob > thresh (you decide what the threshold to use) \n# Can be achieved by creating a partial function --> Create something like the other function with some arguments fixed to certain values\nacc_02 = partial(accuracy_thresh, thresh=0.2)\nf_score = partial(fbeta, thresh=0.2) \nlearn = cnn_learner(data, arch, metrics=[acc_02, f_score])","d106a36f":"learn.lr_find()\nlearn.recorder.plot()","bc00e69f":"lr = 0.01","6247d123":"learn.fit_one_cycle(5, slice(lr))","e123fbab":"learn.recorder.plot_losses()","335a6357":"learn.save('stage-1-rn50')","096afdba":"learn.unfreeze()","e9983aaa":"learn.lr_find()\nlearn.recorder.plot()","d5c3b1bc":"learn.fit_one_cycle(5, slice(1e-5, lr\/5))  # lr is 0.01, so lr\/5 is 0.002","b04c3efc":"learn.save('stage-2-rn50')","7190091d":"data = (src.transform(tfms, size=256)\n        .databunch().normalize(imagenet_stats))\n\n# Start with the same learner, just replace the data with the new dataset of 256x256 inputs\nlearn.data = data\ndata.train_ds[0][0].shape","25a3792b":"learn.freeze()","dccc28b9":"learn.lr_find()\nlearn.recorder.plot()","5e6fedc5":"lr=1e-2\/2","56d5199d":"learn.fit_one_cycle(5, slice(lr))","fc0ca534":"learn.save('stage-1-256-rn50')","00233625":"learn.unfreeze()","f3dd96aa":"learn.lr_find()\nlearn.recorder.plot()","b97c61e1":"learn.fit_one_cycle(5, slice(1e-5, lr\/5))","a69451b8":"learn.recorder.plot_losses()","00938f46":"learn.save('stage-2-256-rn50')","08e578f7":"learn.export()","e2661219":"! ls ..\/input\/planet-understanding-the-amazon-from-space\/","868beac3":"# Will take some time\nimport time\nstart_time = time.time()\n!cp -r {input_path}test-jpg-v2 {path}\/.\nend_time = time.time()\nprint(\"Time Taken: {}\".format(end_time - start_time))","75362cbb":"test = ImageList.from_folder(path\/'test-jpg-v2') #.add(ImageList.from_folder(path\/'test-jpg-additional'))\nlen(test)","84a26655":"learn = load_learner(path, test=test)\npreds, _ = learn.get_preds(ds_type=DatasetType.Test)","e33425f2":"thresh = 0.2\nlabelled_preds = [' '.join([learn.data.classes[i] for i,p in enumerate(pred) if p > thresh]) for pred in preds]","d2617ec7":"labelled_preds[:5]","c4bb8929":"fnames = [f.name[:-4] for f in learn.data.test_ds.items]","0437a15b":"df = pd.DataFrame({'image_name':fnames,\n                   'tags':labelled_preds},\n                  columns=['image_name', 'tags'])","540f49f1":"df.to_csv('submission.csv', index=False)","35233182":"! ls ..\/working","2f33feb4":"### <font color = red>RULE OF THUMB:<\/font>\n* **End value is about 1 order of magnitude lower than the one in stage1 (or divide by 5)**\n* **Start value should come from the learning rate finder and should be well below the value at which the loss starts to become worse. <font color = red>Look for the strongest downward slope that is sticking around for a while<\/font>. Pick that as the lower end of the learning rate (used for the earlier layers). <font color = red>If there is no downward trend like in the plot above, then pick about 1 order of magnitude before it starts to get worse**<\/font>","200c7f0a":"### <font color = red>**Changing the metric will not change the way the model is trained. This is just for printing progress.**<\/font>","fc7b3f6b":"### <font color = red>Warp is set to 0. Warp is used to mimic taking images from different angles (maybe you are higher than the 'cat' or cat is at an elevation and you are takikng image from below). For Satellite images, this is not the case so we set warp to 0. <\/font>","5687a82b":"### <font color = red> Dataset (from PyTorch) --> DataLoader (from PyTorch) --> DataBunch (from fastai) <\/font>\n* **Dataset - has `__item__` and `__len__` to index the data**\n* **DataLoader - used to create mini-batches**\n* **DataBunch - Combination of DataLoaders for Train, Validation and Test sets**\n\n\nTo put this in a `DataBunch` while using the [data block API](https:\/\/docs.fast.ai\/data_block.html), we then need to using `ImageList` (and not `ImageDataBunch`). This will make sure the model created has the proper loss function to deal with the multiple classes.To put this in a `DataBunch` while using the [data block API](https:\/\/docs.fast.ai\/data_block.html), we then need to using `ImageList` (and not `ImageDataBunch`). This will make sure the model created has the proper loss function to deal with the multiple classes.\n\n**The data block API lets you customize the creation of a DataBunch by isolating the underlying parts of that process in separate blocks, mainly:**\n1. Where are the inputs and how to create them?\n2. How to split the data into a training and validation sets?\n3. How to label the inputs?\n4. What transforms to apply?\n5. How to add a test set?\n6. How to wrap in dataloaders and create the DataBunch?\n\n","23302969":"To create a `Learner` we use the same function as in lesson 1. Our base architecture is resnet50 again, but the metrics are a little bit differeent: we use `accuracy_thresh` instead of `accuracy`. In lesson 1, we determined the predicition for a given class by picking the final activation that was the biggest, but here, each activation can be 0. or 1. `accuracy_thresh` selects the ones that are above a certain threshold (0.5 by default) and compares them to the ground truth.\n\nAs for Fbeta, it's the metric that was used by Kaggle on this competition. See [here](https:\/\/en.wikipedia.org\/wiki\/F1_score) for more details.","f604d99b":"## Create Databunch","68d820b0":"## With Larger Image Size\n\n### <font color = red> Kaggle Images are 256x256, but we used 128x128 to get a decent model. Now, we can use this model and transfer learning to train a better model with the 256x256 images. <\/font>\n\n","111a8e86":"## Create Learner","ba0779f5":"## Predictions","9330aa74":"### <font color = red>Before you unfreeze, you will almost always get this shape. **Find the steepest slope (not the bottom) and use that as your learning rate.** <\/font>"}}