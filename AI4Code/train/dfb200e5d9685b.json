{"cell_type":{"452f6d60":"code","8ccf25f8":"code","15b33a2f":"code","3e21c5db":"code","0ee1b765":"code","0bd8b535":"code","8db929ee":"code","d6a84c44":"code","1c7de3df":"code","8bf8d623":"code","e0a4677e":"code","f963d939":"code","041e9ca0":"code","d4e6a607":"code","e0971dcd":"code","df9c512c":"code","82f8b4c6":"code","c0733c74":"code","8c4d4872":"code","2eed22bb":"code","34934c9f":"code","c97228cd":"code","29008ec0":"code","6808d3af":"code","b8d49d90":"markdown","84d41161":"markdown","74fe8893":"markdown","8be7af62":"markdown","f845e2a8":"markdown","8380f76d":"markdown","5a63d232":"markdown","7d741037":"markdown","edd3bbe0":"markdown"},"source":{"452f6d60":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport re\nimport os\nfrom xgboost import XGBClassifier\nfrom wordcloud import WordCloud\nfrom nltk import pos_tag\nimport nltk\nnltk.download('stopwords')\nnltk.download('punkt')\nnltk.download('averaged_perceptron_tagger')\nnltk.download('wordnet')\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import stopwords, wordnet\nfrom nltk.tokenize import sent_tokenize, word_tokenize\nimport string\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.metrics import classification_report,confusion_matrix,accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression,SGDClassifier, LinearRegression\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom keras.layers import Dense, Conv1D, MaxPool1D, Flatten, Dropout, Embedding, LSTM\nfrom keras.models import Sequential\nfrom keras.utils import to_categorical","8ccf25f8":"# na\u010d\u00edtanie d\u00e1t\ntrain_df = pd.read_csv('..\/input\/nlp-getting-started\/train.csv')\ntest_df = pd.read_csv('..\/input\/nlp-getting-started\/test.csv')\nsubmission = pd.read_csv('..\/input\/nlp-getting-started\/sample_submission.csv')","15b33a2f":"train_df.head()","3e21c5db":"test_df.head()","0ee1b765":"# filling nan values in the columns. \ntrain_df.keyword.fillna('', inplace=True)\ntrain_df.location.fillna('', inplace=True)\n\ntest_df.keyword.fillna('', inplace=True)\ntest_df.location.fillna('', inplace=True)","0bd8b535":"train_df['text'] = train_df['text'] + ' ' + train_df['keyword'] + ' ' + train_df['location']\ntest_df['text'] = test_df['text'] + ' ' + test_df['keyword'] + ' ' + test_df['location']\n\ndel train_df['keyword']\ndel train_df['location']\ndel train_df['id']\ndel test_df['keyword']\ndel test_df['location']\ndel test_df['id']","8db929ee":"train_df.head()","d6a84c44":"test_df.head()","1c7de3df":"sns.countplot(train_df.target)","8bf8d623":"# treba odstr\u00e1ni\u0165 slov\u00e1 ako \"a\", \"that\" alebo \"there\", tzv. stopwords, ktor\u00e9 n\u00e1m nenapom\u00e1haj\u00fa pri rozli\u0161ovan\u00ed\nstop = set(stopwords.words('english'))\npunctuations = list(string.punctuation)\nstop.update(punctuations)\nprint(stop)","e0a4677e":"# Funkcie na o\u010distenie textu od \u010d\u00edsel, linkov\ndef remove_numbers(text):\n    text = ''.join([i for i in text if not i.isdigit()])         \n    return text\n\ndef remove_URL(text):\n    url = re.compile(r'https?:\/\/\\S+|www\\.\\S+')\n    return url.sub(r'',text)\n\ndef remove_html(text):\n    html=re.compile(r'<.*?>')\n    return html.sub(r'',text)\n\ndef remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\" \n                           u\"\\U0001F300-\\U0001F5FF\"\n                           u\"\\U0001F680-\\U0001F6FF\" \n                           u\"\\U0001F1E0-\\U0001F1FF\"  \n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)","f963d939":"train_df.text = train_df.text.apply(remove_numbers)\ntrain_df.text = train_df.text.apply(remove_URL)\ntrain_df.text = train_df.text.apply(remove_html)\ntrain_df.text = train_df.text.apply(remove_emoji)\ntrain_df.head()","041e9ca0":"test_df.text = test_df.text.apply(remove_numbers)\ntest_df.text = test_df.text.apply(remove_URL)\ntest_df.text = test_df.text.apply(remove_html)\ntest_df.text = test_df.text.apply(remove_emoji)\ntest_df.head()","d4e6a607":"# ur\u010denie slovn\u00e9ho druhu\ndef get_simple_pos(tag):\n    if tag.startswith('J'):\n        return wordnet.ADJ\n    elif tag.startswith('V'):\n        return wordnet.VERB\n    elif tag.startswith('N'):\n        return wordnet.NOUN\n    elif tag.startswith('R'):\n        return wordnet.ADV\n    else:\n        return wordnet.NOUN","e0971dcd":"lemmatizer = WordNetLemmatizer()\ndef clean_text(text):\n    clean_text = []\n    for w in word_tokenize(text):\n        if w.lower() not in stop:\n            pos = pos_tag([w])\n            new_w = lemmatizer.lemmatize(w, pos=get_simple_pos(pos[0][1]))\n            clean_text.append(new_w)\n    return \" \".join(clean_text)","df9c512c":"train_df.text = train_df.text.apply(clean_text)\ntest_df.text = test_df.text.apply(clean_text)","82f8b4c6":"real = train_df.text[train_df.target[train_df.target==1].index]\nfake = train_df.text[train_df.target[train_df.target==0].index]","c0733c74":"# rozdel\u00edme si d\u00e1ta na treningov\u00fa a valida\u010dn\u00fa \u010das\u0165\nx_train_text, x_val_text, y_train, y_val = train_test_split(train_df.text, train_df.target, test_size=0.2, random_state=0)","8c4d4872":"#niektor\u00e9 slov\u00e1, ktor\u00e9 sa vyskytuj\u00fa ve\u013emi \u010dasto m\u00f4\u017eeme ignorova\u0165, ke\u010f\u017ee slov\u00e1 ktor\u00e9 sa pr\u00edli\u0161 \u010dasto vyskytuj\u00fa\n#n\u00e1m nepom\u00f4\u017eu pri predpovedan\u00ed - to ist\u00e9 plat\u00ed pri slov\u00e1ch ktor\u00e9 s\u00fa ve\u013emi zriedkav\u00e9\n#na ignorovanie tak\u00fdchto slov pou\u017e\u00edvame min_df a max_df\ntv=TfidfVectorizer(min_df=0,max_df=0.8,use_idf=True,ngram_range=(1,3))\n\n\ntv_train_reviews=tv.fit_transform(x_train_text)\ntv_val_reviews=tv.transform(x_val_text)\ntv_test_reviews=tv.transform(test_df.text)\n\nprint('tfidf_train:',tv_train_reviews.shape)\nprint('tfidf_validation:',tv_val_reviews.shape)\nprint('tfidf_test:',tv_test_reviews.shape)","2eed22bb":"model = Sequential()\n\nmodel.add(Dense(units = 512 , activation = 'relu' , input_dim = tv_train_reviews.shape[1]))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(units = 256 , activation = 'relu'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(units = 100 , activation = 'relu'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(units = 10 , activation = 'relu'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(units = 1 , activation = 'sigmoid'))\n\nmodel.compile(optimizer = 'nadam' , loss = 'binary_crossentropy' , metrics = ['accuracy'])\n\nmodel.summary()","34934c9f":"history = model.fit(tv_train_reviews, y_train, validation_data=(tv_val_reviews, y_val), batch_size=128, epochs=5)","c97228cd":"# Porovnanie treningov\u00fdch a valida\u010dn\u00fdch d\u00e1t\nplt.figure(figsize=(10,12))\nplt.subplot(221)\nplt.title('Loss')\nplt.plot(history.history['loss'], label='train')\nplt.plot(history.history['val_loss'], label='test')\n\nplt.subplot(222)\nplt.title('Accuracy')\nplt.plot(history.history['accuracy'], label='train')\nplt.plot(history.history['val_accuracy'], label='test')\nplt.legend()\nplt.show()","29008ec0":"model_val_predict = model.predict_classes(tv_val_reviews)\ncm = confusion_matrix(y_val, model_val_predict)\ncm = pd.DataFrame(cm , index = [i for i in range(2)] , columns = [i for i in range(2)])\nplt.figure(figsize = (8,6))\nsns.heatmap(cm,cmap= \"Blues\", linecolor = 'black' , linewidth = 1 , annot = True, fmt='')","6808d3af":"y_pred = model.predict_classes(tv_test_reviews)\nsubmission.target = y_pred\nsubmission.to_csv(\"submission.csv\" , index = False)","b8d49d90":"# Predikcia pre testovacie d\u00e1ta","84d41161":"# 3I0203 UI1","74fe8893":"# Kni\u017enice","8be7af62":"# Odstr\u00e1nenie \"stopwords\", \u010d\u00edsel, linkov, emotik\u00f3n","f845e2a8":"**We can see the target column is balanced.**","8380f76d":"# Model","5a63d232":"# Rozdelenie d\u00e1t","7d741037":"**Lematiz\u00e1cia - proces skupinkovania slov s podobn\u00fdm v\u00fdznamom.**\n\n","edd3bbe0":"**Vytvorenie modelu**"}}