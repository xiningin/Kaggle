{"cell_type":{"25816c5a":"code","5f15b87b":"code","61301cab":"code","5a78e226":"code","0f53106f":"code","3a561c7d":"code","4e977900":"code","6221c868":"code","e4cdd920":"code","a0bb616d":"code","8683c2f3":"code","20f8a68d":"code","282b0056":"code","97f9726b":"code","5c0520ce":"code","799f5994":"code","d5a04d79":"code","feb19297":"code","06614073":"code","36215df5":"code","03c19d69":"code","a07662e2":"code","c1c54298":"code","dcb9d0b4":"code","fc7947a2":"code","9a3f3070":"code","ce83dfe2":"code","30370d42":"code","32f596d4":"code","54814059":"code","a6205855":"code","658468f6":"code","2b352573":"code","e2dcbd07":"code","abc36d5e":"code","945083d1":"code","85224d99":"code","077aec2c":"code","b385dacd":"code","8de46fa0":"code","1bd79d17":"code","83dfdbbd":"code","a64fc30f":"code","44f130a4":"code","bee2892b":"code","ca79fe08":"code","dd2b6889":"code","ac530d39":"code","c19e22de":"code","c990c350":"code","172411a1":"code","4d18e599":"code","78e1d875":"code","76e15f4f":"code","f41ee5ba":"code","57e744ed":"code","6ad8196b":"code","5e652d48":"code","7b49a300":"code","8016b6f6":"code","92f4feeb":"code","27e2c273":"code","66b32fbe":"code","919a7e64":"markdown","79b2b2f5":"markdown","930da327":"markdown","c824bce8":"markdown","e47d9f5d":"markdown","567cd405":"markdown","cbc775eb":"markdown","07c578fa":"markdown","45d13ad7":"markdown","341b9e12":"markdown","d3763027":"markdown","91f5865c":"markdown","47921043":"markdown","75016334":"markdown","be808e90":"markdown","2ea23ea3":"markdown","1bf80ac1":"markdown","5d6af15f":"markdown","69023df9":"markdown","1cf27d96":"markdown","6f117ac6":"markdown","b075cb55":"markdown","d755f1d8":"markdown","188129c4":"markdown","0726ffdb":"markdown","42d66c1a":"markdown","f8f7598e":"markdown","0108dcd7":"markdown","1d15dbb8":"markdown","c24bfd06":"markdown","f7af28da":"markdown","6d4dc33d":"markdown","a5026698":"markdown","ebe3038a":"markdown","668486ef":"markdown","ccb75175":"markdown","c1746e1e":"markdown","661495f2":"markdown","c370687c":"markdown","c382803b":"markdown","be90e677":"markdown","63c3cd3a":"markdown"},"source":{"25816c5a":"import pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nimport seaborn as sns\nsns.set(style=\"whitegrid\")\n\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\n\nimport optuna\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nimport lightgbm as lgbm\nfrom xgboost import XGBClassifier","5f15b87b":"# check data files\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","61301cab":"path    = '\/kaggle\/input\/titanic\/'\n\nf_train = pd.read_csv(path + 'train.csv')\nf_test  = pd.read_csv(path + 'test.csv')","5a78e226":"f_train","0f53106f":"f_train.info()","3a561c7d":"f_train.describe()","4e977900":"def missing_value(df):\n    value = (df.isnull().mean())\n    return value","6221c868":"# check missing value from train data\nmissing_value(f_train)","e4cdd920":"# check missing value from test data\nmissing_value(f_test)","a0bb616d":"mask = np.zeros_like(f_train.corr(), dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n\nplt.figure(figsize=(15,12))\nplt.title(\"Correlations Among Features\",fontsize = 20)\nsns.heatmap(f_train.corr(),annot=True, cmap = 'RdBu', mask = mask)","8683c2f3":"plt.figure(figsize=(10,6))\nsns.countplot(data = f_train,\n              x = 'Survived',\n              palette='RdBu')","20f8a68d":"temp = f_train.groupby('Survived')['PassengerId'].count().reset_index()\ntemp.rename(columns={'PassengerId': 'count'}, inplace = True)\ntemp['Survived'].replace({ 0 : 'Not Survived', 1 : 'Survived'}, inplace = True)\n\nfig = px.pie(temp, values='count', names='Survived',color_discrete_sequence=px.colors.sequential.RdBu)\nfig.show()","282b0056":"plt.figure(figsize=(10,8))\nsns.countplot(data = f_train,\n              x = 'Sex',\n              hue = 'Survived',\n              palette='RdBu')","97f9726b":"temp = f_train[f_train.Sex == 'male'].groupby('Survived')['PassengerId'].count().reset_index()\ntemp.rename(columns={'PassengerId': 'count'}, inplace = True)\ntemp['Survived'].replace({ 0 : 'Not Survived', 1 : 'Survived'}, inplace = True)\n\ntemp1 = f_train[f_train.Sex == 'female'].groupby('Survived')['PassengerId'].count().reset_index()\ntemp1.rename(columns={'PassengerId': 'count'}, inplace = True)\ntemp1['Survived'].replace({ 0 : 'Not Survived', 1 : 'Survived'}, inplace = True)\n\nfig = px.pie(temp, values='count', names='Survived',title = 'Survival rate of male',color_discrete_sequence=px.colors.sequential.deep)\nfig.show()\n\nfig = px.pie(temp1, values='count', names='Survived',title = 'Survival rate of female',color_discrete_sequence=px.colors.sequential.deep)\nfig.show()","5c0520ce":"plt.figure(figsize=(10,8))\nsns.barplot(data = f_train,\n              x = 'Pclass',\n              y = 'Survived',\n              palette='RdBu')","799f5994":"# passengers distribution\n\ntemp = f_train.groupby('Pclass')['PassengerId'].count().reset_index()\ntemp.rename(columns={'PassengerId': 'count'}, inplace = True)\n\nfig = px.pie(temp, values='count', names='Pclass',title = 'Class',color_discrete_sequence=px.colors.sequential.deep)\nfig.show()","d5a04d79":"f_train.Name","feb19297":"# create new feature ( title name )\nf_train['Title'] = f_train.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)","06614073":"fig = px.histogram(f_train, x=\"Age\", color= 'Survived', marginal=\"rug\", barmode='overlay')\nfig.show()","36215df5":"fig = px.box(f_train, x=\"Title\", y=\"Age\", color='Survived',color_discrete_sequence=px.colors.sequential.RdBu)\nfig.show()","03c19d69":"# Title with missing Age\nf_train[f_train.Age.isnull()].Title.value_counts()","a07662e2":"f_train.groupby('Title')['Age'].median()","c1c54298":"# Fill Missing Age with median Age in Title Group\n\nage_missing = list(f_train[f_train.Age.isnull()].Title.unique())\n\nfor i in age_missing:\n    median_age = f_train.groupby('Title')['Age'].median()[i]\n    f_train.loc[f_train['Age'].isnull() & (f_train['Title'] == i), 'Age'] = median_age","dcb9d0b4":"# check missing value\nmissing_value(f_train)","fc7947a2":"# merge some title\n\nmapping = {'Mlle': 'Miss', 'Major': 'Rare', 'Col': 'Rare', 'Sir': 'Rare', 'Don': 'Rare', 'Mme': 'Mrs',\n          'Jonkheer': 'Rare', 'Lady': 'Rare', 'Capt': 'Rare', 'Countess': 'Rare', 'Ms': 'Miss', 'Dona': 'Rare',\n           'Dr': 'Rare', 'Rev': 'Rare'}\n\nf_train.replace({'Title': mapping}, inplace=True)\n\n# group age\n\nbins = [ 0, 12, 30, 60, np.inf]\nlabels = ['Children', 'Teenager', 'Adult', 'Senior']\nf_train['AgeGroup'] = pd.cut(f_train[\"Age\"], bins, labels = labels)\n","9a3f3070":"plt.figure(figsize=(20,8))\nsns.barplot(data = f_train,\n              x = 'Title',\n              y = 'Survived',\n              palette='RdBu')","ce83dfe2":"plt.figure(figsize=(20,8))\nsns.barplot(data = f_train,\n              x = 'AgeGroup',\n              y = 'Survived',\n             hue = 'Sex',\n              palette='RdBu')","30370d42":"plt.figure(figsize=(20,8))\nsns.barplot(data = f_train,\n              x = 'SibSp',\n              y = 'Survived',\n              palette='RdBu')","32f596d4":"plt.figure(figsize=(20,8))\nsns.barplot(data = f_train,\n              x = 'Parch',\n              y = 'Survived',\n              palette='RdBu')","54814059":"# create new features\n\nf_train['Family'] = f_train['SibSp'] + f_train['Parch'] + 1\nf_train['TravelAlone']=np.where(f_train['Family']>1, 0, 1)","a6205855":"plt.figure(figsize=(20,8))\nsns.barplot(data = f_train,\n              x = 'Family',\n              y = 'Survived',\n              palette='RdBu')","658468f6":"plt.figure(figsize=(20,8))\nsns.barplot(data = f_train,\n              x = 'TravelAlone',\n              y = 'Survived',\n              palette='RdBu')","2b352573":"fig = px.scatter(f_train, y = \"Age\", x= \"Fare\", color= \"Survived\")\nfig.show()","e2dcbd07":"plt.figure(figsize=(20,8))\nsns.barplot(data = f_train,\n              x = 'Embarked',\n              y = 'Survived',\n              hue = 'Pclass',\n              palette='RdBu')","abc36d5e":"\n# Group Fare\n\nf_train['Fare_Bin'] = pd.qcut(f_train['Fare'], 5)\n\n# Label Encoding\n\nlabel = LabelEncoder()\n\nf_train['AgeGroup'] = label.fit_transform(f_train['AgeGroup'])\nf_train['Fare_Bin'] = label.fit_transform(f_train['Fare_Bin'])\nf_train['Title'] = label.fit_transform(f_train['Title'])\nf_train['Sex'] = label.fit_transform(f_train['Sex'])\n\n# Drop column\n\ndrop_list = ['Name','SibSp','Parch','Ticket','Fare','Cabin','Embarked','Age','PassengerId']\n\nf_train.drop(drop_list, axis = 1, inplace =True)","945083d1":"mask = np.zeros_like(f_train.corr(), dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n\nplt.figure(figsize=(15,12))\nplt.title(\"Correlations Among Features\",fontsize = 20)\nsns.heatmap(f_train.corr(),annot=True, cmap = 'RdBu', mask = mask)","85224d99":"# do the same thing with test data\n\n# Fill Missing value age and fare\n\nf_test['Title'] = f_test.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\nage_missing = list(f_test[f_test.Age.isnull()].Title.unique())\n\nfor i in age_missing:\n    median_age = f_test.groupby('Title')['Age'].median()[i]\n    f_test.loc[f_test['Age'].isnull() & (f_test['Title'] == i), 'Age'] = median_age\n    \nf_test.Age.fillna(28, inplace=True)  # because in the test data there is only one person whose title with Ms, so I took it from the train data\n\n\nmissing_value = f_test[(f_test.Pclass == 3) & \n                       (f_test.Embarked == \"S\") & \n                       (f_test.Sex == \"male\")].Fare.mean()\n\nf_test.Fare.fillna(missing_value, inplace=True)\n\n\n# merge some title\n\nmapping = {'Mlle': 'Miss', 'Major': 'Rare', 'Col': 'Rare', 'Sir': 'Rare', 'Don': 'Rare', 'Mme': 'Mrs',\n          'Jonkheer': 'Rare', 'Lady': 'Rare', 'Capt': 'Rare', 'Countess': 'Rare', 'Ms': 'Miss', 'Dona': 'Rare',\n           'Dr': 'Rare', 'Rev': 'Rare'}\n\nf_test.replace({'Title': mapping}, inplace=True)\n\n\n# group Age\n\nbins = [ 0, 12, 30, 60, np.inf]\nlabels = ['Children', 'Teenager', 'Adult', 'Senior']\nf_test['AgeGroup'] = pd.cut(f_test[\"Age\"], bins, labels = labels)\n\n\n# create new feature\n\nf_test['Family'] = f_test['SibSp'] + f_test['Parch']\nf_test['TravelAlone']=np.where(f_test['Family']>0, 0, 1)\nf_test['Fare_Bin'] = pd.qcut(f_test['Fare'], 5)\n\n\n# Label Encoding\n\nlabel = LabelEncoder()\n\nf_test['AgeGroup'] = label.fit_transform(f_test['AgeGroup'])\nf_test['Fare_Bin'] = label.fit_transform(f_test['Fare_Bin'])\nf_test['Title'] = label.fit_transform(f_test['Title'])\nf_test['Sex'] = label.fit_transform(f_test['Sex'])\n\n# Drop column\n\ndrop_list = ['Name','SibSp','Parch','Ticket','Fare','Cabin','Embarked','Age']\n\nf_test.drop(drop_list, axis = 1, inplace =True)","077aec2c":"# prepare data for modeling\n\nX = f_train.drop('Survived', axis = 1)\nY = f_train.Survived\n\nX_test = f_test\nX_test = X_test.drop('PassengerId',axis = 1)\n\nx_train, x_val, y_train, y_val = train_test_split(X, Y, test_size = 0.22, random_state = 0)","b385dacd":"print(x_train.shape, x_val.shape, y_train.shape, y_val.shape)","8de46fa0":"def basic_model(x_train,y_train,x_val,y_val):\n\n    # Gradient Boosting Classifier\n    model = GradientBoostingClassifier()\n    model.fit(x_train, y_train)\n    y_pred  = model.predict(x_val)\n    acc_gbc = round(accuracy_score(y_pred, y_val) * 100, 2)\n    print(f'Gradient Boosting Classifier : Score {acc_gbc}')\n    \n    # Random Forest Classifier\n    model = RandomForestClassifier()\n    model.fit(x_train, y_train)\n    y_pred = model.predict(x_val)\n    acc_rfc = round(accuracy_score(y_pred, y_val) * 100, 2)\n    print(f'Random Forest Classifier : Score {acc_rfc}')\n\n    # Support Vector Machines\n    model = SVC()\n    model.fit(x_train, y_train)\n    y_pred = model.predict(x_val)\n    acc_svc = round(accuracy_score(y_pred, y_val) * 100, 2)\n    print(f'Support Vector Machines : Score {acc_svc}')\n\n    # LightGBM Classifier\n    model  = lgbm.LGBMClassifier()\n    model.fit(x_train, y_train)\n    y_pred = model.predict(x_val)\n    acc_lgbm = round(accuracy_score(y_pred, y_val) * 100, 2)\n    print(f'LightGBM Classifier : Score {acc_lgbm}')\n\n   # XGB Classifier\n    model  = XGBClassifier()\n    model.fit(x_train, y_train)\n    y_pred = model.predict(x_val)\n    acc_xgb = round(accuracy_score(y_pred, y_val) * 100, 2)\n    print(f'XGB Classifier : Score {acc_xgb}') \n\n\n    return acc_gbc, acc_rfc, acc_svc, acc_lgbm, acc_xgb","1bd79d17":"# basic model\nacc_gbc, acc_rfc, acc_svc, acc_lgbm, acc_xgb = basic_model(x_train, y_train, x_val, y_val)","83dfdbbd":"models_basic = pd.DataFrame({\n    'Model': ['Gradient Boosting Classifier','Random Forest Classifier',\n              'Support Vector Machines', 'LightGBM Classifier',\n              'XGB Classifier'],\n              \n\n    'Score Basic Model': [acc_gbc, acc_rfc, acc_svc, acc_lgbm, acc_xgb]\n              })\n\nmodels_basic.sort_values(by='Score Basic Model', ascending=False)","a64fc30f":"# helper function\n\nclass model_objectif(object):\n    def __init__(self, models, x, y):\n        self.models = models\n        self.x = x\n        self.y = y\n\n    def __call__(self, trial):\n        models, x, y = self.models, self.x, self.y\n\n        classifier_name = models\n\n        if classifier_name == \"RFC\": \n            \n            model = RandomForestClassifier( \n                n_estimators = trial.suggest_int('n_estimators', 10, 100),\n                criterion = trial.suggest_categorical('criterion', ['gini', 'entropy']),\n                max_depth = trial.suggest_int('max_depth', 1, 6),\n                min_samples_split = trial.suggest_int('min_samples_split', 2, 16),\n                max_features = trial.suggest_categorical('max_features', ['auto', 'sqrt', 'log2', None]))\n            \n            model.fit(x_train, y_train)\n\n        elif classifier_name == \"SVM\": \n            \n            model = SVC( \n                C = trial.suggest_loguniform('C', 0.1, 10),\n                kernel = trial.suggest_categorical('kernel', ['linear', 'rbf', 'sigmoid']),\n                gamma  = trial.suggest_categorical('gamma', [\"scale\", \"auto\"]))\n\n            model.fit(x_train, y_train)\n\n        elif classifier_name == \"GBC\":\n            \n            model = GradientBoostingClassifier( \n                n_estimators = trial.suggest_int('n_estimators', 10, 100),\n                min_samples_leaf =  trial.suggest_int('min_samples_leaf', 1, 10),\n                max_depth = trial.suggest_int('max_depth', 1, 6),\n                min_samples_split = trial.suggest_int('min_samples_split', 2, 16),\n                max_features = trial.suggest_categorical('max_features', ['auto', 'sqrt', 'log2', None]))\n\n            model.fit(x_train, y_train)\n\n            \n        elif classifier_name == \"LGBM\": \n            \n            model = lgbm.LGBMClassifier(\n                reg_alpha=trial.suggest_loguniform('reg_alpha', 1e-4, 100.0),\n                reg_lambda=trial.suggest_loguniform('reg_lambda', 1e-4, 100.0),\n                num_leaves=trial.suggest_int('num_leaves', 10, 40))\n\n            model.fit(x_train, y_train, eval_set = [(x_val, y_val)],\n                        early_stopping_rounds=20, verbose=-1)\n            \n        elif classifier_name == \"XGBC\":\n\n            model = XGBClassifier(\n                xgb_max_depth = trial.suggest_int('max_depth', 1, 6),\n                xgb_n_estimators = trial.suggest_int('n_estimators', 10, 100),\n                min_child_weight = trial.suggest_int('min_samples_split', 1, 20),\n                colsample_bytree = trial.suggest_uniform('colsample_bytree', 0.3, 1.0),\n                subsample = trial.suggest_uniform('colsample_bytree', 0.3, 1.0),)\n\n            model.fit(x_train, y_train, eval_set = [(x_val, y_val)],\n                        early_stopping_rounds=20, verbose=-1)\n              \n\n        y_pred   = model.predict(x_val)\n        acc_score = round(accuracy_score(y_pred, y_val) * 100, 2)\n        return acc_score\n\ndef parameters(study_model):\n\n    print(\"Number of finished trials: {}\".format(len(study_model.trials)))\n    print(\"Best trial:\")\n    trial = study_model.best_trial\n    best_params = study_model.best_params\n    print(\"  Score_value: {}\".format(trial.value))\n    print(\"  Params: \")\n    for key, value in trial.params.items():\n        print(\"    {}: {}\".format(key, value))\n\n    return best_params, trial.value\n\n\ndef visual_study(study_model):\n\n    fig = optuna.visualization.plot_optimization_history(study_model)\n    fig.show()\n    fig = optuna.visualization.plot_parallel_coordinate(study_model)\n    fig.show()\n    fig = optuna.visualization.plot_slice(study_model)\n    fig.show()\n","44f130a4":"models = 'RFC'\nobjective = model_objectif(models, X, Y)\nstudy_RFC = optuna.create_study(direction='maximize')\nstudy_RFC.optimize(objective, n_trials=100) # n_trail = 100","bee2892b":"best_params_RFC, best_score_RFC = parameters(study_RFC)","ca79fe08":"visual_study(study_RFC)","dd2b6889":"models = 'SVM'\nobjective = model_objectif(models, X, Y)\nstudy_SVM = optuna.create_study(direction='maximize')\nstudy_SVM.optimize(objective, n_trials=100) # n_trail = 100","ac530d39":"best_params_SVM, best_score_SVM = parameters(study_SVM)","c19e22de":"visual_study(study_SVM)","c990c350":"models = 'GBC'\nobjective = model_objectif(models, X, Y)\nstudy_GBC = optuna.create_study(direction='maximize')\nstudy_GBC.optimize(objective, n_trials=100) # n_trail = 100","172411a1":"best_params_GBC, best_score_GBC = parameters(study_GBC)","4d18e599":"visual_study(study_GBC)","78e1d875":"models = 'XGBC'\nobjective = model_objectif(models, X, Y)\nstudy_XGBC = optuna.create_study(direction='maximize')\nstudy_XGBC.optimize(objective, n_trials=100) # n_trail = 100","76e15f4f":"best_params_XGBC, best_score_XGBC = parameters(study_XGBC)","f41ee5ba":"visual_study(study_XGBC)","57e744ed":"models = 'LGBM'\nobjective = model_objectif(models, X, Y)\nstudy_LGBM = optuna.create_study(direction='maximize')\nstudy_LGBM.optimize(objective, n_trials=100) # n_trail = 100","6ad8196b":"best_params_LGBM, best_score_LGBM = parameters(study_LGBM)","5e652d48":"visual_study(study_LGBM)","7b49a300":"# create dataframe score tuning\nmodels_tuning = pd.DataFrame({\n    \n    'Model': ['Gradient Boosting Classifier',\n              'Random Forest Classifier',\n              'Support Vector Machines', \n              'LightGBM Classifier',\n              'XGB Classifier'],\n              \n\n    'Score Tuning Model': [best_score_GBC, \n                           best_score_RFC, \n                           best_score_SVM, \n                           best_score_LGBM, \n                           best_score_XGBC]\n                          })\n\n# merge with score before tuning\nmodel_all = pd.merge(models_basic, models_tuning, on = 'Model')\nmodel_all.sort_values(by='Score Tuning Model', ascending=False, inplace = True)\nmodel_all","8016b6f6":"def submit_pred(df, test_data):\n    \n    model_name = df.Model.values[0]\n    \n    if model_name == 'Random Forest Classifier':\n        model  = RandomForestClassifier(**best_params_RFC)\n        model.fit(x_train, y_train)\n        y_pred   = model.predict(test_data)\n        \n    if model_name == 'Gradient Boosting Classifier':\n        model  = GradientBoostingClassifier(**best_params_GBC)\n        model.fit(x_train, y_train)\n        y_pred   = model.predict(test_data)\n        \n    if model_name == 'Support Vector Machines':\n        model  = SVC(**best_params_SVM)\n        model.fit(x_train, y_train)\n        y_pred   = model.predict(test_data)\n        \n    if model_name == 'LightGBM Classifier':\n        model  = lgbm.LGBMClassifier(**best_params_LGBM)\n        model.fit(x_train, y_train, eval_set = [(x_val, y_val)],\n                  early_stopping_rounds=20, verbose=-1)\n        y_pred   = model.predict(test_data)\n    \n    if model_name == 'XGB Classifier':\n        model  = XGBClassifier(**best_params_XGBC)\n        model.fit(x_train, y_train, eval_set = [(x_val, y_val)],\n                  early_stopping_rounds=20, verbose=-1)\n        y_pred   = model.predict(test_data)\n        \n    print(f'model use {model_name}')\n    \n    return y_pred","92f4feeb":"# Sumbit with best Classifier and best Parameters\n\ny_pred = submit_pred(model_all, X_test)\n\nsubmission = pd.DataFrame({\n    \"PassengerId\": f_test['PassengerId'], \n    \"Survived\": y_pred\n})","27e2c273":"submission.head(10)","66b32fbe":"submission.to_csv('submission.csv', index=False)","919a7e64":"There are totally 891 Titanic passengers. Among them, only 342 survived, which makes the survival rate as low as 38.4%.","79b2b2f5":"****\n\n## Feature Engineer","930da327":"****\n\n### RESULT","c824bce8":"## File descriptions\n\n> * train.csv  -  the training set\n> * test.csv  - the test set\n> * gender_submission.csv  - Example Submission","e47d9f5d":"****\n### Random Forest Classifier\n\nparameters that I use on the model\n> * n_estimators\n> * criterion\n> * max_depth\n> * min_samples_split\n> * max_features","567cd405":"***\n## Fare","cbc775eb":"***\n## Age and Name","07c578fa":"***\n## Embarked","45d13ad7":"Passengers who travel in small groups have a better change of survival than other passengers.","341b9e12":"<h1><center>Thank you for reading my notebook, upvote if you like this notebook :)<\/center><\/h1>\n\n****","d3763027":"***\n\n## Optuna Hyperparameters","91f5865c":"## Import necessary libraries","47921043":"I think filling in the missing value in the age feature using the median value from the title feature is a better approach than using the median directly.","75016334":"<h1> Introduction <\/h1>\n\n****\n\n<p style=\"text-align:justify;\"> The sinking of the Titanic is one of the most infamous shipwrecks in history. On April 15, 1912, during her maiden voyage, the widely considered \u201cunsinkable\u201d RMS Titanic sank after colliding with an iceberg. Unfortunately, there weren\u2019t enough lifeboats for everyone onboard, resulting in the death of 1502 out of 2224 passengers and crew. While there was some element of luck involved in surviving, it seems some groups of people were more likely to survive than others. In this challenge, we ask you to build a predictive model that answers the question: \u201cwhat sorts of people were more likely to survive?\u201d using passenger data (ie name, age, gender, socio-economic class, etc).<\/p>\n\n***\n\n<p style=\"text-align:justify;\"> Hello, I saw a lot of other titanic notebooks that used gridsearch and RandomizedSearchCV as a tuning parameter in the model. In this notebook, I will do with a different approach, using optuna (Bayesian model-based methods) as a tuning parameter and make comparisons with several models. <\/p>\n\nin this notebook i will use five models classifier:\n\n* Gradient Boosting Classifier\n* Random Forest Classifier\n* Support Vector Machines\n* LightGBM Classifier\n* XGB Classifier","be808e90":"****\n## Familia (Sibsp and Parch)","2ea23ea3":"****\n\n### XGB Classifier\n\nparameters that I use on the model\n> * xgb_max_depth\n> * xgb_n_estimators\n> * min_child_weight\n> * colsample_bytree\n> * subsample","1bf80ac1":"The age distribution is rather skewed, most passengers are 15 - 35 years old. In this feature, there is a missing value of nearly 20%, we can fill in the Missing values with median ages, but using the median ages of all data sets is not a good choice.","5d6af15f":"![titanic-540df9a9c4097.jpg](attachment:titanic-540df9a9c4097.jpg)","69023df9":"****\n## Survival","1cf27d96":"****\n### LightGBM \n\nparameters that I use on the model\n\n> * reg_alpha\n> * reg_lambda\n> * num_leaves","6f117ac6":"Titanic passengers embarked from 3 places, Southampton (S), Cherbourg (C), Queenstown (Q). I think there is no correlation between this feature and survivors. I would drop this feature later.","b075cb55":"If we classify survival based on ages and sex, and we can see passengers and crew generally still follow the 'Women and Children First' protocol, and this makes female passengers and children have a high survival rate than other.","d755f1d8":"![download%20%281%29.png](attachment:download%20%281%29.png)","188129c4":"****\n## Gender","0726ffdb":"<p style=\"text-align:justify;\">Optuna is an automatic hyperparameter optimization software framework, particularly designed for machine learning. It features an imperative, define-by-run style user API. with optuna, we can choose whatever parameters that we want to optimization. so what makes optuna different than other hyperparameters optimization? In short, optuna is an implementation of the Bayesian model-based method. They use past evaluation results to choose the next values to evaluate. make it faster and more efficient to find better hyperparameters then another method.<\/p>\n\nIf you are interested about hyperparameters and how it's different one and another, here is a very good article\n\n* [https:\/\/towardsdatascience.com\/a-conceptual-explanation-of-bayesian-model-based-hyperparameter-optimization-for-machine-learning-b8172278050f](http:\/\/towardsdatascience.com\/a-conceptual-explanation-of-bayesian-model-based-hyperparameter-optimization-for-machine-learning-b8172278050f)\n* [https:\/\/towardsdatascience.com\/automated-machine-learning-hyperparameter-tuning-in-python-dfda59b72f8a](https:\/\/towardsdatascience.com\/automated-machine-learning-hyperparameter-tuning-in-python-dfda59b72f8a)","42d66c1a":"In training data, there are missing values in <code>Age<\/code> (19.8%), <code>Cabin<\/code> (77.1%), <code>Embarked<\/code> (0.2%) Features, and in test data, there is a value missing in <code>Age<\/code> (20. 5%), <code>Cabin<\/code> (78.2%), <code>Fare<\/code> (0.2%) Features. I will check each data later, but I think I would drop <code>cabin<\/code> feature because too many missing values","f8f7598e":"the model that I used is :\n\n1. Gradient Boosting Classifier\n2. Random Forest Classifier\n3. Support Vector Machines\n4. LightGBM Classifier\n5. XGB Classifier","0108dcd7":"Passenger class distribution is divided into three classes, most passengers are in 3 classes, and at least in 2 classes. Based on that class, the chance to survive in a higher class is better","1d15dbb8":"There is an increase in scores in the model, and I think we need to choose the right parameters to be optimized to be better than this.","c24bfd06":"from the table, we can see the score from  five basic models, and then we want to tune some parameters in the model.","f7af28da":"***\n# Modelling","6d4dc33d":"****\n\n### Gradient Boosting Classifier\n\nparameters that I use on the model\n> * n_estimators\n> * min_samples_leaf\n> * max_depth\n> * min_samples_split\n> * max_features","a5026698":"if you're not traveling alone, you have a better chance to survive","ebe3038a":"****\n## Passengers Class","668486ef":"from this plot, passengers who pay more have a better chance of survival","ccb75175":"****\n\n### Support Vector Machines\n\nparameters that I use on the model\n> * C\n> * kernel\n> * gamma","c1746e1e":"## Data fields\n\n> * PassengerId  - ID of Passenger\n> * Survival     - Survival passenger ( 1 = survived , 0 = not survived )\n> * Pclass       - Ticket Class of Passenger ( 1 = 1st, 2 = 2nd, 3 = 3rd )\n> * Age          - Age\n> * Sibsp\t     - Siblings \/ Spouses aboard the Titanic\t\n> * Parch        - parents \/ children aboard the Titanic\n> * Ticket       - Ticket Number\n> * Fare\t     - Passenger fare\n> * Cabin        - Cabin number\n> * Embarked     - Port of Embarkation ( C = Cherbourg, Q = Queenstown, S = Southampton )","661495f2":"From the names of the passengers, we can create a new feature, 'Title'","c370687c":"\n<center><h1> Titanic Prediction (Auto-models Optuna + EDA) <\/h1><\/center>\n\n***","c382803b":"***\n# Exploring Data","be90e677":"## References\n\n> * https:\/\/www.kaggle.com\/masumrumi\/a-statistical-analysis-ml-workflow-of-titanic\n> * https:\/\/www.kaggle.com\/gunesevitan\/titanic-advanced-feature-engineering-tutorial\n> * https:\/\/towardsdatascience.com\/a-conceptual-explanation-of-bayesian-model-based-hyperparameter-optimization-for-machine-learning-b8172278050f\n> * https:\/\/www.kaggle.com\/masumrumi\/a-statistical-analysis-ml-workflow-of-titanic","63c3cd3a":"based on gender comparison, the survival rate of male is lower than that of female, that means female has a better chance of survival than male"}}