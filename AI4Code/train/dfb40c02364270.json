{"cell_type":{"ddc822bc":"code","aa78bd1a":"code","7c82de94":"code","d1231893":"code","32ab7a38":"code","a691f503":"code","37720d6b":"code","c01326fe":"code","a064869f":"code","e06e274f":"code","adf8d58f":"code","781c9dd7":"code","8367271f":"code","c34c9a2b":"code","20370683":"markdown","3a648565":"markdown","c4b2203e":"markdown","ac6ac157":"markdown","bc91bd3f":"markdown","09d8f94c":"markdown","d3bc3e1a":"markdown","367c4f78":"markdown","ec9055c8":"markdown","8dd3ecb5":"markdown","72e25a23":"markdown","a4a1b491":"markdown","4f57e5a7":"markdown","bfc475b1":"markdown","fd0274e9":"markdown","63a1518d":"markdown"},"source":{"ddc822bc":"import os\nimport numpy as np\nimport pandas as pd \nimport random\nimport cv2\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport keras.backend as K\nfrom keras.models import Model, Sequential\nfrom keras.layers import Input, Dense, Flatten, Dropout, BatchNormalization\nfrom keras.layers import Conv2D, SeparableConv2D, MaxPool2D, LeakyReLU, Activation\nfrom keras.optimizers import Adam\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\nimport tensorflow as tf\n\nseed = 232\nnp.random.seed(seed)\ntf.random.set_seed(seed)","aa78bd1a":"input_path = '..\/input\/chest-xray-pneumonia\/\/chest_xray\/chest_xray\/'","7c82de94":"for _set in ['train', 'val', 'test']:\n    n_normal = len(os.listdir(input_path + _set + '\/NORMAL'))\n    n_infect = len(os.listdir(input_path + _set + '\/PNEUMONIA'))\n    print('Set: {}, normal images: {}, pneumonia images: {}'.format(_set, n_normal, n_infect))","d1231893":"\ndef process_data(img_dims, batch_size):\n    # Data generation objects\n    train_datagen = ImageDataGenerator(rescale=1.\/255, zoom_range=0.3, vertical_flip=True)\n    test_val_datagen = ImageDataGenerator(rescale=1.\/255)\n    \n    # This is fed to the network in the specified batch sizes and image dimensions\n    train_gen = train_datagen.flow_from_directory(\n    directory=input_path+'train', \n    target_size=(img_dims, img_dims), \n    batch_size=batch_size, \n    class_mode='binary', \n    shuffle=True)\n\n    test_gen = test_val_datagen.flow_from_directory(\n    directory=input_path+'test', \n    target_size=(img_dims, img_dims), \n    batch_size=batch_size, \n    class_mode='binary', \n    shuffle=True)\n    \n    # I will be making predictions off of the test set in one batch size\n    # This is useful to be able to get the confusion matrix\n    test_data = []\n    test_labels = []\n\n    for cond in ['\/NORMAL\/', '\/PNEUMONIA\/']:\n        for img in (os.listdir(input_path + 'test' + cond)):\n            img = plt.imread(input_path+'test'+cond+img)\n            img = cv2.resize(img, (img_dims, img_dims))\n            img = np.dstack([img, img, img])\n            img = img.astype('float32') \/ 255\n            if cond=='\/NORMAL\/':\n                label = 0\n            elif cond=='\/PNEUMONIA\/':\n                label = 1\n            test_data.append(img)\n            test_labels.append(label)\n        \n    test_data = np.array(test_data)\n    test_labels = np.array(test_labels)\n    \n    return train_gen, test_gen, test_data, test_labels","32ab7a38":"img_dims = 150\nepochs = 10\nbatch_size = 32\n\ntrain_gen, test_gen, test_data, test_labels = process_data(img_dims, batch_size)","a691f503":"test_data.shape","37720d6b":"\ninputs = Input(shape=(img_dims, img_dims, 3))\n\nfrom keras.applications.inception_v3 import InceptionV3\nbase_model = InceptionV3(weights='imagenet',include_top=False,input_shape=(img_dims, img_dims, 3))\nx = base_model.output\n\nx = Dropout(0.5)(x)\nfrom keras.layers import GlobalAveragePooling2D\nx = GlobalAveragePooling2D()(x)\nx = Dense(128,activation='relu')(x)\nx = BatchNormalization()(x)\noutput = Dense(1,activation = 'sigmoid')(x)\n\n# Creating model and compiling\nmodel = Model(inputs=base_model.input, outputs=output)\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n\n# Callbacks\ncheckpoint = ModelCheckpoint(filepath='best_weights.hdf5', save_best_only=True, save_weights_only=True)\nlr_reduce = ReduceLROnPlateau(monitor='val_loss', factor=0.3, patience=2, verbose=2, mode='max')\nearly_stop = EarlyStopping(monitor='val_loss', min_delta=0.1, patience=1, mode='min')","c01326fe":"model.summary()","a064869f":"hist = model.fit_generator(\n           train_gen, steps_per_epoch=train_gen.samples \/\/ batch_size, \n           epochs=epochs, validation_data=test_gen, \n           validation_steps=test_gen.samples \/\/ batch_size, callbacks=[checkpoint, lr_reduce])","e06e274f":"fig, ax = plt.subplots(1, 2, figsize=(10, 3))\nax = ax.ravel()\n\nfor i, met in enumerate(['accuracy', 'loss']):\n    ax[i].plot(hist.history[met])\n    ax[i].plot(hist.history['val_' + met])\n    ax[i].set_title('Model {}'.format(met))\n    ax[i].set_xlabel('epochs')\n    ax[i].set_ylabel(met)\n    ax[i].legend(['train', 'val'])","adf8d58f":"from sklearn.metrics import accuracy_score, confusion_matrix\n\npreds = model.predict(test_data)\n\nacc = accuracy_score(test_labels, np.round(preds))*100\ncm = confusion_matrix(test_labels, np.round(preds))\ntn, fp, fn, tp = cm.ravel()\n\nprint('CONFUSION MATRIX ------------------')\nprint(cm)\n\nprint('\\nTEST METRICS ----------------------')\nprecision = tp\/(tp+fp)*100\nrecall = tp\/(tp+fn)*100\nprint('Accuracy: {}%'.format(acc))\nprint('Precision: {}%'.format(precision))\nprint('Recall: {}%'.format(recall))\nprint('F1-score: {}'.format(2*precision*recall\/(precision+recall)))\n\nprint('\\nTRAIN METRIC ----------------------')\nprint('Train acc: {}'.format(np.round((hist.history['accuracy'][-1])*100, 2)))","781c9dd7":"import seaborn as sn\nimport pandas as pd","8367271f":"df_cm = pd.DataFrame(cm, index = ('normal', 'pneumonia'), columns = ('normal', 'pneumonia'))\nplt.figure(figsize = (5,4))\nplt.title('Confusion Matrix')\nplt.xlabel('Predicted')\nplt.ylabel('Actual')\nsn.set(font_scale=1.4)\nsn.heatmap(df_cm, annot=True, fmt='g')\nplt.xlabel('Predicted')\nplt.ylabel('Actual')\nplt.show()","c34c9a2b":"model.save('dnnV3.h5')","20370683":"Now, let's take a look at how many NORMAL and PNEUMONIA images are in each TRAIN, VAL, TEST folders.\n\nApparently, not many! That's the case with any medical imagery. You wouldn't get too many images for training you model unless you've a large database of that disease. It's difficult to acquire them too. We gotta work with what we have and produce best results.","3a648565":"DONE !!!\n\nLet's visualize this training on a graph.","c4b2203e":"**TIME TO FIT THE MODEL.**\n\n**FINALLY !!!!!**","ac6ac157":"Let's create some constants so we don't have to repeat these numbers but instead use only the assigned names.","bc91bd3f":"As you can see, **Testing accuracy is 88%+** and **RECALL SCORE IS 99% !!!!**\n\nThat **IS** something!\n\nLet's see our confusion matrix properly this time using Seaborn Visualization library.","09d8f94c":"Let's save the weights.","d3bc3e1a":"As is with every model, training accuracy is almost touching to 100% whereas Validation accuracy is lowered. But, if you notice, the Loss has significantly reduced on first epoch itself! That's a good sign!\n\nNow, let's see some predictions and other results on **Test Dataset.**","367c4f78":"The next few steps include:\n1. **Data Augmentation**\n1. **Feed training and testing images to the network**\n1. **Create labels for the images**\n\nThe practice of data augmentation is an effective way to increase the size of the training set. Augmenting the training examples allow the network to \u201csee\u201d more diversified data points during training. Please read more about Data Augmentation to get a better understanding on what it does to our training set.\n\nThen we defined a couple of data generators: one for training data, and the other for validation data. A data generator is capable of loading the required amount of data (a mini batch of images) directly from the source folder, convert them into training data (fed to the model) and training targets (a vector of attributes \u2014 the supervision signal).","ec9055c8":"Less than 5 PNEUMONIA cases are predicted as NORMAL cases !!!! THAT SAYS SOMETHING ABOUT THIS MODEL !!!\n\nRecall Score in Medical Diagnosis plays an important role as it might sometimes depend on someone's lives!","8dd3ecb5":"Here I present you a **CNN Model using Keras (TensorFlow as backend)** to train a model with **CHEST X-RAY PNEUMONIA IMAGES**. This model's **ACCURACY is 90%** which can be improved further by optimizing it's hyperparameters. However, it depends on one's computational power. Also, if you notice, it's **RECALL SCORE is 99%** which tells a lot about this model's efficiency in **PNEUMONIA CASES**.\n\nI will give you step by step explanation of how we arrived at this mind-blowing accuracy and recall scores! FOLLOW ME.","72e25a23":"Import **InceptioV3 weights** since we're training this model with pre-trained weights.\n\nSet the Dropout rate to 0.5 for the model. This Dropout literally drops out or switches off few neurons in order to generalise the model for unknown data.\n\nWe're using **Sigmoid function** which suits best for the **Output Layer**.\n\n**Adam Optimizer** for Optimization, **Binary Crossentropy** since we have 2 labels.","a4a1b491":"I hope you enjoyed this tutorial and find it useful!!\n\nSee you soon!","4f57e5a7":"Checking the assigned constants and shape.","bfc475b1":"Setting the path location to 'input_path' variable.\n\nNow, let's look at the images in the given folders.\n\nThere are 3 default folders on Kaggle on any given dataset. In this case, we have images which are labeled as **NORMAL AND PNEUMONIA** inside **TRAIN, TEST, AND VALIDATION** folders. Let's look at one image from all the 3 folders which are labeled as NORMAL AND PNEUMONIA. \n\n***Let's see if you can identify any differences between NORMAL and PNEUMONIA infected images. Can you? Majority of us cannot. If you do, then you've a good future as a radiologist!***\n\nLet's move on.","fd0274e9":"We've also set 'Learning Rate(lr)' (which means how big a step we take while reaching to Global\/Local Minima) and 'Early Stopping' (this makes sure when to stop overfitting as the model learns from the trained data while fitting the images on validation set).\n\nAlso, make sure to save your best weights, that is, whenever you've good\/best accuracy level. So that, model can start from the learned weights instead of learning from scratch everytime the iteration starts.\n\nLet's look at our model's summary.","63a1518d":"Let's call the necessary libraries and set the seed to 232. This is to reproduce the same results whenever we run the model."}}