{"cell_type":{"b62e05af":"code","25ca3573":"code","5cc22461":"code","c5041236":"code","afcbfdc7":"code","29321ed5":"code","3ae13772":"code","af902a21":"code","33f3ef47":"code","320203b2":"code","ea4f7640":"code","b6da1a99":"code","2d8bfe63":"code","046cbd8c":"code","cf01fcc8":"code","fd3aae76":"markdown","140c6f1a":"markdown","5ae462c3":"markdown","5a58aa7a":"markdown","c2a95333":"markdown","afe312f2":"markdown","6b87fc83":"markdown","db05c153":"markdown","a20c6df9":"markdown","3f97f0bb":"markdown","3d6ef8ea":"markdown","4d1f9a02":"markdown","a518db65":"markdown"},"source":{"b62e05af":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session\n\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\nfrom sklearn.preprocessing import FunctionTransformer, OneHotEncoder\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom xgboost import XGBClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\npd.set_option('display.max_rows', None)","25ca3573":"def save_file (predictions):\n    \"\"\"Save submission file.\"\"\"\n    # Save test predictions to file\n    output = pd.DataFrame({'PassengerId': sample_sub_file.PassengerId,\n                       'Survived': predictions})\n    output.to_csv('submission.csv', index=False)\n    print (\"Submission file is saved\")\n    \ndef transform_age(df):\n    ''' A function that transforms the Age column of the Titanic dataset.\n        'Age' feature is transformed into a categorical data of the passengers\n        such that masters and people whose age are smaller than 16 is defined\n        as child.'''\n    # Make a copy to avoid changing original data\n    X_temp = df.copy()\n    \n    # Create Age_new column\n    pd.DataFrame.insert(X_temp, len(X_temp.columns),\"Age_new\",\"\",False)    \n    \n    # Get the index values\n    index_values = X_temp.index.values.astype(int)\n    \n    for i in index_values:\n        age = X_temp.at[i, 'Age'].astype(float)\n        name = X_temp.loc[i,'Name']\n        if name.find('.'):\n            title = name.split('.')[0].split()[-1]\n\n        if np.isnan(age):\n            if title == \"Master\":\n                X_temp.loc[i,'Age_new'] = \"Child\"\n            else:\n                X_temp.loc[i,'Age_new'] = \"Adult\"\n        else:\n            if age < 16:\n                X_temp.loc[i,'Age_new'] = \"Child\"\n            else:\n                X_temp.loc[i,'Age_new'] = \"Adult\"\n        \n    drop = [\"Age\", \"Name\"]\n    X_temp.drop(drop, axis=1, inplace=True)\n    X_temp.rename(columns={'Age_new':'Age'}, inplace=True)\n    return X_temp\n\ndef transform_family(df):\n    '''A funtion that calculates the family size by summing Parch and SibSp columns into the 'Fcount' column. Afterward Parch \n    and SibSp columns are dropped.'''\n    # Make a copy to avoid changing original data\n    X_temp = df.copy()\n    \n    # Create Fcount column\n    pd.DataFrame.insert(X_temp, len(X_temp.columns),\"Fcount\",\"\",False)    \n    \n    # Get the index values\n    index_values = X_temp.index.values.astype(int)\n    \n    for i in index_values:\n        X_temp.loc[i, 'Fcount'] = X_temp.loc[i, 'Parch'] + X_temp.loc[i,'SibSp']\n        \n    X_temp[\"Fcount\"] = X_temp[\"Fcount\"].astype('int64')\n    X_temp.drop(['Parch', 'SibSp'], axis=1, inplace=True)\n\n    return X_temp\n\nprint(\"Functions loaded\")","5cc22461":"# Loading data\ntrain_data = pd.read_csv('\/kaggle\/input\/titanic\/train.csv', index_col='PassengerId')\ntest_data = pd.read_csv('\/kaggle\/input\/titanic\/test.csv', index_col='PassengerId')\nsample_sub_file = pd.read_csv('\/kaggle\/input\/titanic\/gender_submission.csv')\n\n# Make a copy to avoid changing original data\nX = train_data.copy()\ny = X.Survived\nX_test = test_data.copy()\n\n# Remove target from predictors\nX.drop(['Survived'], axis=1, inplace=True)\nprint(\"['Survived'] column dropped from training data!\")\n\n# Remove Ticket, Cabin, Embarked columns. We will not use them.\ncols_dropped = [\"Ticket\", \"Cabin\", \"Embarked\"]\nX.drop(cols_dropped, axis = 1, inplace = True)\nX_test.drop(cols_dropped, axis = 1, inplace = True)\nprint(\"{} dropped from both training and test data!\".format(cols_dropped))\n\nprint(\"\\nShape of training data: {}\".format(X.shape))\nprint(\"Shape of target: {}\".format(y.shape))\nprint(\"Shape of test data: {}\".format(X_test.shape))\nprint(\"Shape of submission data: {}\".format(sample_sub_file.shape))\n\n# Split the data for validation\nX_train, X_valid, y_train, y_valid = train_test_split(X,y, random_state=2)\n\nprint(\"\\nShape of X_train data: {}\".format(X_train.shape))\nprint(\"Shape of X_valid: {}\".format(X_valid.shape))\nprint(\"Shape of y_train: {}\".format(y_train.shape))\nprint(\"Shape of y_valid: {}\".format(y_valid.shape))\n\nprint(\"\\nFiles Loaded\")","c5041236":"X_train.head()","afcbfdc7":"# Define the custom transformers for the pipeline\nage_transformer = FunctionTransformer(transform_age)\nfamily_transformer = FunctionTransformer(transform_family)","29321ed5":"X_temp = age_transformer.fit_transform(X)\nX_temp = family_transformer.fit_transform(X_temp)","3ae13772":"X_temp[5:10]","af902a21":"# Define transformers\n\n# Define the custom transformers for the pipeline\nage_transformer = FunctionTransformer(transform_age)\nfamily_transformer = FunctionTransformer(transform_family)\n\n# Define transformer for categorical columns using a pipeline\ncat_cols = [\"Sex\", \"Age\", \"Pclass\"]\ncategorical_transformer = Pipeline(steps=[('onehot', OneHotEncoder(drop = 'first', sparse = False))\n])\n\n# Define column transformer for categorical data\ncolumn_transformer = ColumnTransformer(transformers=[('cat', categorical_transformer, cat_cols)], remainder='passthrough')","33f3ef47":"# Define Model\nmodel = XGBClassifier(seed=42)","320203b2":"# Define preprocessor\npreprocessor = Pipeline(steps=[('age', age_transformer),\n                              ('family', family_transformer),\n                              ('column', column_transformer)])\n\n# Make a copy to avoid changing original data \nX_valid_eval=X_valid.copy()\n\n# Preprocessing of validation data\nX_valid_eval = preprocessor.fit(X_train, y_train).transform (X_valid_eval)\n\n# Display the number of remaining columns after transformation \nprint(\"We have\", X_valid_eval.shape[1], \"features left\")","ea4f7640":"# Create and Evaluate the Pipeline\n# Bundle preprocessing and modeling code in a pipeline\nmy_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n                              ('model', model)\n                             ])","b6da1a99":"# Preprocessing of training data, fit model \nX_cv = X.copy()\nX_sub = X_test.copy()","2d8bfe63":"# Cross-validation\nscores = cross_val_score(my_pipeline, X_cv, y,\n                              cv=5,\n                              scoring='accuracy')\n\nprint(\"MAE score:\\n\", scores)\nprint(\"MAE mean: {}\".format(scores.mean()))\nprint(\"MAE std: {}\".format(scores.std()))","046cbd8c":"# Preprocessing of training data, fit model \nmy_pipeline.fit(X_cv, y)\n\n# Get predictions\npreds = my_pipeline.predict(X_sub)","cf01fcc8":"# Use predefined utility function\nsave_file(preds)","fd3aae76":"# Saving and submission   <a id='saving'><\/a>","140c6f1a":"# Function Transformers  <a id='functiontransformers'><\/a> ","5ae462c3":"# Helper functions   <a id='functions'><\/a>   ","5a58aa7a":"<div class=\"alert alert-block alert-info\">\nFirst, we will load the data. Next, we will drop the ticket column since we do not need it. Finally, we will split our data into training and test data sets.\n<\/div> ","c2a95333":"<div class=\"alert alert-block alert-info\">First, let's see how to define a function transformer. We can then fit our dataset with the function transformers to see the result.\n<\/div>","afe312f2":"## Preprocessing   <a id='preprocessing'><\/a>   ","6b87fc83":"# Cross-validation <a id='cross-validation'><\/a>","db05c153":"# References   <a id='references'><\/a>\n* [10-simple-hacks-to-speed-up-your-data-analysis - Parul Pandey](https:\/\/www.kaggle.com\/parulpandey\/10-simple-hacks-to-speed-up-your-data-analysis)\n* [Dataset Transformations - Scikit-learn](https:\/\/scikit-learn.org\/stable\/data_transforms.html)\n* [Intermediate Machine Learning Course - Pipelines](https:\/\/www.kaggle.com\/alexisbcook\/pipelines)\n* [Kaggle Learn](https:\/\/www.kaggle.com\/learn\/overview)","a20c6df9":"<div class=\"alert alert-block alert-info\">\n<b>Tip:<\/b> We will use some utility functions throughout the notebook. Collecting them in one place is a good idea. It makes the code more organized.\n<\/div>","3f97f0bb":"# Prediction   <a id='prediction'><\/a>","3d6ef8ea":"## Loading Data   <a id='loading'><\/a>   ","4d1f9a02":"<div class=\"alert alert-block alert-info\">Although we have already defined the function transformers above, we will start from scratch and redefine them in this part for pipelines. This is for the convenience of those who want to copy and paste the code.\n<\/div>","a518db65":"# Introduction  <a id='introduction'><\/a>\n\nThis is a fast and simple starter code for those who want to use function transformers within sklearn pipelines. We will use Titanic dataset for this purpose. \n\nKagglers who are interested with using early_stopping_rounds and cross-validation with pipelines may refer to my notebook [Housing Prices: Pipeline Starter Code](https:\/\/www.kaggle.com\/erkanhatipoglu\/housing-prices-pipeline-starter-code).\n\nKagglers who are interested with using grid search may refer to my notebook [Housing Prices: GridSearchCV Example](https:\/\/www.kaggle.com\/erkanhatipoglu\/housing-prices-gridsearchcv-example).\n\nKagglers who are interested in more advanced subjects of sklearn pipelines may refer to my notebook [Introduction to Sklearn Pipelines with Titanic](https:\/\/www.kaggle.com\/erkanhatipoglu\/introduction-to-sklearn-pipelines-with-titanic).\n\nThank you for reading.\n\n\n\n# Table of Contents\n* [Introduction](#introduction)\n* [Helper Functions](#functions)\n* [Loading Data](#loading)\n* [Function Transformers](#functiontransformers) \n* [Preprocessing](#preprocessing) \n* [Cross-validation](#cross-validation)    \n* [Prediction](#prediction) \n* [Saving and submission](#saving)  \n* [References](#references)"}}