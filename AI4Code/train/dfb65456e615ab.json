{"cell_type":{"dfd17068":"code","53c07e96":"code","0c11d5e1":"code","17b30fb5":"code","44559c9c":"code","2b638987":"code","273c77fc":"code","3dee3032":"code","1ec30f96":"code","3b7547d1":"code","bbe6c04e":"code","7c00bf0d":"code","6f2defef":"code","1543b49d":"code","c05bbcfd":"code","cc8e6ace":"code","e53d102b":"code","25914889":"code","8a5412d9":"code","609bd5a8":"code","af1d0944":"code","d3196e76":"code","adb79e31":"code","13c2fb15":"code","48bdca55":"code","76b4c70f":"code","2843dd27":"code","7495fe60":"code","5963bef2":"code","7fa8949d":"code","6b7d0e8d":"code","01a2412b":"code","d7ce1925":"code","991f64d2":"code","d4d4a776":"code","afe429af":"code","490f7181":"code","a47c4e97":"code","f23d229a":"code","2f0889cf":"code","b5c26445":"code","d5e203ac":"code","754e6210":"code","1dcdcb10":"code","e12016e6":"code","dbd65ea9":"code","423726e6":"code","17e5112e":"code","7e3e8ceb":"code","f1572ddb":"code","7575c6de":"code","188d7967":"code","4ad87790":"code","8f6f88ab":"code","61c44529":"markdown","e4079dba":"markdown","3b682726":"markdown","58485315":"markdown","f40cc3b1":"markdown","6aba2185":"markdown","1bd76cb4":"markdown","8ebd07d0":"markdown","be6f867e":"markdown","ad281ba8":"markdown","f289233d":"markdown"},"source":{"dfd17068":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\n# import numpy as np # linear algebra\n# import pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# import os\n# for dirname, _, filenames in os.walk('\/kaggle\/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","53c07e96":"import os\nimport numpy as np\nimport cv2                 \nfrom random import shuffle\nfrom tqdm import tqdm  \nimport tensorflow as tf \nfrom tensorflow.keras import Model\nfrom tensorflow.keras.utils import plot_model\nimport matplotlib.pyplot as plt\nimport matplotlib.cm as cm\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau , ModelCheckpoint\nfrom collections import Counter","0c11d5e1":"tf.test.is_gpu_available()","17b30fb5":"TrianImage=\"\/kaggle\/input\/chest-xray-covid19-pneumonia\/Data\/train\/\"\nTestImage=\"\/kaggle\/input\/chest-xray-covid19-pneumonia\/Data\/test\/\"\nNormalimages = os.listdir(TrianImage + \"\/NORMAL\")\nPneumonaimages = os.listdir(TrianImage + \"\/PNEUMONIA\")\nCOVID19images = os.listdir(TrianImage + \"\/COVID19\")","44559c9c":"print(len(Normalimages), len(Pneumonaimages), len(COVID19images))\nNUM_TRAINING_IMAGES = len(Normalimages) + len(Pneumonaimages) + len(COVID19images)\nprint(NUM_TRAINING_IMAGES)","2b638987":"plt.figure(figsize=(10,10))\nfor i in range(9):\n    plt.subplot(3, 3, i + 1)\n    plt.imshow(plt.imread(os.path.join(TrianImage + \"\/NORMAL\",Normalimages[i])),cmap='gray')\n    plt.title(\"NORMAL\")\nplt.show()","273c77fc":"plt.figure(figsize=(10,10))\nfor i in range(9):\n    plt.subplot(3, 3, i + 1)\n    plt.imshow(plt.imread(os.path.join(TrianImage + \"\/PNEUMONIA\",Pneumonaimages[i])),cmap='gray')\n    plt.title(\"PNEUMONIA\")\nplt.show()","3dee3032":"plt.figure(figsize=(10,10))\nfor i in range(9):\n    plt.subplot(3, 3, i + 1)\n    plt.imshow(plt.imread(os.path.join(TrianImage + \"\/COVID19\",COVID19images[i])),cmap='gray')\n    plt.title(\"COVID19\")\nplt.show()","1ec30f96":"# https:\/\/gist.github.com\/fchollet\/7eb39b44eb9e16e59632d25fb3119975\n# https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/preprocessing\/image\/ImageDataGenerator\n\nimage_size = 224 \nBATCH_SIZE = 16 \nSTEPS_PER_EPOCH = NUM_TRAINING_IMAGES \/\/ BATCH_SIZE\n\ndata_path = '\/kaggle\/input\/chest-xray-covid19-pneumonia\/Data'\n\ntrain_datagen = ImageDataGenerator(rescale = 1.\/255,\n                                   zoom_range = 0.2,\n                                   rotation_range=15,\n                                   horizontal_flip = True)\n\ntest_datagen = ImageDataGenerator(rescale = 1.\/255)\n\ntraining_set = train_datagen.flow_from_directory(data_path + '\/train',\n                                                 target_size = (image_size, image_size),\n                                                 batch_size = BATCH_SIZE,\n                                                 class_mode = 'categorical',\n                                                 shuffle=True)\n\ntesting_set = test_datagen.flow_from_directory(data_path + '\/test',\n                                            target_size = (image_size, image_size),\n                                            batch_size = BATCH_SIZE,\n                                            class_mode = 'categorical',\n                                            shuffle = True)","3b7547d1":"# https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/utils\/Sequence\nprint(\"train batch \", training_set.__getitem__(0)[0].shape)\nprint(\"test batch \", testing_set.__getitem__(0)[0].shape)\nprint(\"sample train label \\n\", training_set.__getitem__(0)[1][:5])","bbe6c04e":"training_set.class_indices","7c00bf0d":"testing_set.class_indices","6f2defef":"labels = ['COVID19', 'NORMAL', 'PNEUMONIA']\n\nsample_data = testing_set.__getitem__(1)[0] \nsample_label = testing_set.__getitem__(1)[1] \n\nplt.figure(figsize=(10,8))\nfor i in range(12):\n    plt.subplot(3, 4, i + 1)\n    plt.axis('off')\n    plt.imshow(sample_data[i])\n    plt.title(labels[np.argmax(sample_label[i])])","1543b49d":"!pip install efficientnet\nimport efficientnet.tfkeras as efn","c05bbcfd":"def display_training_curves(training, validation, title, subplot):\n    if subplot%10==1: # set up the subplots on the first call\n        plt.subplots(figsize=(10,10), facecolor='#F0F0F0')\n        plt.tight_layout()\n    ax = plt.subplot(subplot)\n    ax.set_facecolor('#F8F8F8')\n    ax.plot(training)\n    ax.plot(validation)\n    ax.set_title('model '+ title)\n    ax.set_ylabel(title)\n    ax.set_xlabel('epoch')\n    ax.legend(['train', 'valid.'])","cc8e6ace":"# https:\/\/keras.io\/examples\/vision\/grad_cam\/\nfrom tensorflow import keras\n\ndef make_gradcam_heatmap(img_array, model, last_conv_layer_name, classifier_layer_names):\n    # First, we create a model that maps the input image to the activations\n    # of the last conv layer\n    last_conv_layer = model.get_layer(last_conv_layer_name)\n    last_conv_layer_model = keras.Model(model.inputs, last_conv_layer.output)\n\n    # Second, we create a model that maps the activations of the last conv\n    # layer to the final class predictions\n    classifier_input = keras.Input(shape=last_conv_layer.output.shape[1:])\n    x = classifier_input\n    for layer_name in classifier_layer_names:\n        x = model.get_layer(layer_name)(x)\n    classifier_model = keras.Model(classifier_input, x)\n\n    # Then, we compute the gradient of the top predicted class for our input image\n    # with respect to the activations of the last conv layer\n    with tf.GradientTape() as tape:\n        # Compute activations of the last conv layer and make the tape watch it\n        last_conv_layer_output = last_conv_layer_model(img_array)\n        tape.watch(last_conv_layer_output)\n        # Compute class predictions\n        preds = classifier_model(last_conv_layer_output)\n        top_pred_index = tf.argmax(preds[0])\n        top_class_channel = preds[:, top_pred_index]\n\n    # This is the gradient of the top predicted class with regard to\n    # the output feature map of the last conv layer\n    grads = tape.gradient(top_class_channel, last_conv_layer_output)\n\n    # This is a vector where each entry is the mean intensity of the gradient\n    # over a specific feature map channel\n    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n\n    # We multiply each channel in the feature map array\n    # by \"how important this channel is\" with regard to the top predicted class\n    last_conv_layer_output = last_conv_layer_output.numpy()[0]\n    pooled_grads = pooled_grads.numpy()\n    for i in range(pooled_grads.shape[-1]):\n        last_conv_layer_output[:, :, i] *= pooled_grads[i]\n\n    # The channel-wise mean of the resulting feature map\n    # is our heatmap of class activation\n    heatmap = np.mean(last_conv_layer_output, axis=-1)\n\n    # For visualization purpose, we will also normalize the heatmap between 0 & 1\n    heatmap = np.maximum(heatmap, 0) \/ np.max(heatmap)\n    return heatmap, top_pred_index.numpy()","e53d102b":"# https:\/\/keras.io\/examples\/vision\/grad_cam\/\ndef superimposed_img(image, heatmap):\n    # We rescale heatmap to a range 0-255\n    heatmap = np.uint8(255 * heatmap)\n\n    # We use jet colormap to colorize heatmap\n    jet = cm.get_cmap(\"jet\")\n\n    # We use RGB values of the colormap\n    jet_colors = jet(np.arange(256))[:, :3]\n    jet_heatmap = jet_colors[heatmap]\n\n    # We create an image with RGB colorized heatmap\n    jet_heatmap = keras.preprocessing.image.array_to_img(jet_heatmap)\n    jet_heatmap = jet_heatmap.resize((image_size, image_size))\n    jet_heatmap = keras.preprocessing.image.img_to_array(jet_heatmap)\n\n    # Superimpose the heatmap on original image\n    superimposed_img = jet_heatmap * 0.4 + image\n    superimposed_img = keras.preprocessing.image.array_to_img(superimposed_img)\n    return superimposed_img","25914889":"# label smoothing https:\/\/www.linkedin.com\/pulse\/label-smoothing-solving-overfitting-overconfidence-code-sobh-phd\/\ndef categorical_smooth_loss(y_true, y_pred, label_smoothing=0.1):\n    loss = tf.keras.losses.categorical_crossentropy(y_true, y_pred, label_smoothing=label_smoothing)\n    return loss","8a5412d9":"# training call backs \nlr_reduce = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, epsilon=0.0001, patience=3, verbose=1)\nes_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, verbose=1)","609bd5a8":"# https:\/\/stackoverflow.com\/questions\/42586475\/is-it-possible-to-automatically-infer-the-class-weight-from-flow-from-directory\ncounter = Counter(training_set.classes)                          \nmax_val = float(max(counter.values()))       \nclass_weights = {class_id : max_val\/num_images for class_id, num_images in counter.items()}\nclass_weights","af1d0944":"# dfault input shapes \n\nprint(tf.keras.applications.DenseNet201(weights='imagenet').input_shape)\nprint(efn.EfficientNetB7(weights='imagenet').input_shape) \nprint(tf.keras.applications.VGG16(weights='imagenet').input_shape) ","d3196e76":"pretrained_densenet = tf.keras.applications.DenseNet201(input_shape=(image_size, image_size, 3), weights='imagenet', include_top=False)\n\nfor layer in pretrained_densenet.layers:\n  layer.trainable = False\n\nx1 = pretrained_densenet.output\nx1 = tf.keras.layers.AveragePooling2D(name=\"averagepooling2d_head\")(x1)\nx1 = tf.keras.layers.Flatten(name=\"flatten_head\")(x1)\nx1 = tf.keras.layers.Dense(64, activation=\"relu\", name=\"dense_head\")(x1)\nx1 = tf.keras.layers.Dropout(0.5, name=\"dropout_head\")(x1)\nmodel_out = tf.keras.layers.Dense(3, activation='softmax', name=\"predictions_head\")(x1)\n\nmodel_densenet = Model(inputs=pretrained_densenet.input, outputs=model_out)\nmodel_densenet.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),loss=categorical_smooth_loss,metrics=['accuracy'])\n\nmodel_densenet.summary()","adb79e31":"history_densenet = model_densenet.fit_generator(training_set, validation_data=testing_set, callbacks=[lr_reduce, es_callback], epochs=30)   ","13c2fb15":"display_training_curves(history_densenet.history['loss'], history_densenet.history['val_loss'], 'loss', 211)\ndisplay_training_curves(history_densenet.history['accuracy'], history_densenet.history['val_accuracy'], 'accuracy', 212)","48bdca55":"last_conv_layer_name = \"conv5_block32_concat\"\nclassifier_layer_names = [\n    \"bn\",\n    \"relu\",\n    \"averagepooling2d_head\",\n    \"flatten_head\",\n    \"dense_head\",\n    \"dropout_head\",\n    \"predictions_head\"\n]","76b4c70f":"# test image\nfile_path =  '\/test\/COVID19\/COVID19(164).jpg'\ntest_image = cv2.imread(data_path + file_path)\ntest_image = cv2.resize(test_image, (224,224),interpolation=cv2.INTER_NEAREST)\nplt.imshow(test_image)\ntest_image = np.expand_dims(test_image,axis=0)","2843dd27":"heatmap, top_index = make_gradcam_heatmap(test_image, model_densenet, last_conv_layer_name, classifier_layer_names)\nprint(\"predicted as\", labels[top_index])","7495fe60":"plt.matshow(heatmap)\nplt.show()","5963bef2":"s_img = superimposed_img(test_image[0], heatmap)\nplt.imshow(s_img)","7fa8949d":"# sample_data = testing_set.__getitem__(0)[0] \n# sample_label = testing_set.__getitem__(0)[1] \n\nplt.figure(figsize=(10,8))\nfor i in range(12):\n    plt.subplot(3, 4, i + 1)\n    plt.axis('off')\n    heatmap, top_index = make_gradcam_heatmap(np.expand_dims(sample_data[i], axis=0), model_densenet, last_conv_layer_name, classifier_layer_names)\n    img = np.uint8(255 * sample_data[i])\n    s_img = superimposed_img(img, heatmap)\n    plt.imshow(s_img)\n    plt.title(labels[np.argmax(sample_label[i])] + \" pred as: \" + labels[top_index], fontsize=8)\n    ","6b7d0e8d":"model_densenet.save(\"model_densenet_30_09596.h5\")","01a2412b":"# https:\/\/github.com\/keras-team\/keras\/issues\/9064\npretrained_efnet = efn.EfficientNetB7(input_shape=(image_size, image_size, 3), weights='noisy-student', include_top=False)\n\nfor layer in pretrained_efnet.layers:\n  layer.trainable = False\n\nx2 = pretrained_efnet.output\nx2 = tf.keras.layers.AveragePooling2D(name=\"averagepooling2d_head\")(x2)\nx2 = tf.keras.layers.Flatten(name=\"flatten_head\")(x2)\nx2 = tf.keras.layers.Dense(64, activation=\"relu\", name=\"dense_head\")(x2)\nx2 = tf.keras.layers.Dropout(0.5, name=\"dropout_head\")(x2)\nmodel_out = tf.keras.layers.Dense(3, activation='softmax', name=\"predictions_head\")(x2)\n\nmodel_efnet = Model(inputs=pretrained_efnet.input, outputs=model_out)\nmodel_efnet.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),loss=categorical_smooth_loss,metrics=['accuracy'])\nmodel_efnet.summary()","d7ce1925":"history_efnet = model_efnet.fit_generator(training_set, validation_data=testing_set, callbacks=[lr_reduce, es_callback], epochs=30)  ","991f64d2":"display_training_curves(history_efnet.history['loss'], history_efnet.history['val_loss'], 'loss', 211)\ndisplay_training_curves(history_efnet.history['accuracy'], history_efnet.history['val_accuracy'], 'accuracy', 212)","d4d4a776":"last_conv_layer_name = \"top_conv\"\nclassifier_layer_names = [\n    \"top_bn\",\n    \"top_activation\",\n    \"averagepooling2d_head\",\n    \"flatten_head\",\n    \"dense_head\",\n    \"dropout_head\",\n    \"predictions_head\"\n]","afe429af":"# sample_data = testing_set.__getitem__(0)[0] \n# sample_label = testing_set.__getitem__(0)[1] \n\nplt.figure(figsize=(10,8))\nfor i in range(12):\n    plt.subplot(3, 4, i + 1)\n    plt.axis('off')\n    heatmap, top_index = make_gradcam_heatmap(np.expand_dims(sample_data[i], axis=0), model_efnet, last_conv_layer_name, classifier_layer_names)\n    img = np.uint8(255 * sample_data[i])\n    s_img = superimposed_img(img, heatmap)\n    plt.imshow(s_img)\n    plt.title(labels[np.argmax(sample_label[i])] + \" pred as: \" + labels[top_index], fontsize=8)","490f7181":"model_efnet.save(\"model_efnet_30_09169.h5\")","a47c4e97":"pretrained_vgg = tf.keras.applications.VGG16(input_shape=(image_size, image_size, 3), weights='imagenet', include_top=False)\n\nfor layer in pretrained_vgg.layers:\n  layer.trainable = False\n\nx3 = pretrained_vgg.output\nx3 = tf.keras.layers.AveragePooling2D(name=\"averagepooling2d_head\")(x3)\nx3 = tf.keras.layers.Flatten(name=\"flatten_head\")(x3)\nx3 = tf.keras.layers.Dense(128, activation=\"relu\", name=\"dense_head\")(x3)\nx3 = tf.keras.layers.Dropout(0.5, name=\"dropout_head\")(x3)\nx3 = tf.keras.layers.Dense(64, activation=\"relu\", name=\"dense_head_2\")(x3)\nx3 = tf.keras.layers.Dropout(0.5, name=\"dropout_head_2\")(x3)\nmodel_out = tf.keras.layers.Dense(3, activation='softmax', name=\"predictions_head\")(x3)\n\nmodel_vgg = Model(inputs=pretrained_vgg.input, outputs=model_out)\nmodel_vgg.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),loss=categorical_smooth_loss,metrics=['accuracy'])\n# model_vgg.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),loss=\"categorical_crossentropy\",metrics=['accuracy'])\nmodel_vgg.summary()","f23d229a":"history_vgg = model_vgg.fit_generator(training_set, validation_data=testing_set, callbacks=[lr_reduce, es_callback], epochs=30)  ","2f0889cf":"display_training_curves(history_vgg.history['loss'], history_vgg.history['val_loss'], 'loss', 211)\ndisplay_training_curves(history_vgg.history['accuracy'], history_vgg.history['val_accuracy'], 'accuracy', 212)","b5c26445":"last_conv_layer_name = \"block5_pool\"\nclassifier_layer_names = [\n    \"averagepooling2d_head\",\n    \"flatten_head\",\n    \"dense_head\",\n    \"dropout_head\",\n    \"dense_head_2\",\n    \"dropout_head_2\",\n    \"predictions_head\"\n]","d5e203ac":"plt.figure(figsize=(10,8))\nfor i in range(12):\n    plt.subplot(3, 4, i + 1)\n    plt.axis('off')\n    heatmap, top_index = make_gradcam_heatmap(np.expand_dims(sample_data[i], axis=0), model_vgg, last_conv_layer_name, classifier_layer_names)\n    img = np.uint8(255 * sample_data[i])\n    s_img = superimposed_img(img, heatmap)\n    plt.imshow(s_img)\n    plt.title(labels[np.argmax(sample_label[i])] + \" pred as: \" + labels[top_index], fontsize=8)","754e6210":"model_vgg.save(\"model_vgg_30_09286.h5\")","1dcdcb10":"# model_densenet\n# model_efnet\n# model_vgg\n\nfor layer in model_densenet.layers:\n  layer.trainable = False\n\nfor layer in model_efnet.layers:\n  layer.trainable = False\n\nfor layer in model_vgg.layers:\n  layer.trainable = False\n\nvisible = tf.keras.layers.Input(shape=(image_size, image_size, 3), name=\"input\")\nx1 = model_densenet(visible)\nx2 = model_efnet(visible)\nx3 = model_vgg(visible)\nmerge = tf.keras.layers.concatenate([x1, x2, x3], name=\"concatallprobs\")\nmodel_out = tf.keras.layers.Dense(3, activation='softmax', name=\"predictions\")(merge)\n\nmodel_densenet_efnet_vgg = Model(inputs=visible, outputs=model_out)\nmodel_densenet_efnet_vgg.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),loss='categorical_crossentropy',metrics=['accuracy'])\nmodel_densenet_efnet_vgg.summary()","e12016e6":"plot_model(model_densenet_efnet_vgg, show_shapes=True, to_file='model_densenet_efnet_vgg.png')","dbd65ea9":"history = model_densenet_efnet_vgg.fit_generator(training_set, validation_data=testing_set, callbacks=[lr_reduce, es_callback], epochs=7)","423726e6":"display_training_curves(history.history['loss'], history.history['val_loss'], 'loss', 211)\ndisplay_training_curves(history.history['accuracy'], history.history['val_accuracy'], 'accuracy', 212)","17e5112e":"model_densenet_efnet_vgg.save(\"model_densenet_efnet_vgg_09581.h5\")","7e3e8ceb":"# model2= Model(inputs=model1.input, outputs=model1.get_layer(layer_name).output)\n\nmodel_densenet_nohead = Model(inputs=model_densenet.input, outputs=model_densenet.get_layer(\"dense_head\").output)\nmodel_efnet_nohead = Model(inputs=model_efnet.input, outputs=model_efnet.get_layer(\"dense_head\").output)\nmodel_vgg_nohead = Model(inputs=model_vgg.input, outputs=model_vgg.get_layer(\"dense_head_2\").output)\n\nfor layer in model_densenet_nohead.layers:\n  layer.trainable = False\n\nfor layer in model_efnet_nohead.layers:\n  layer.trainable = False\n\nfor layer in model_vgg_nohead.layers:\n  layer.trainable = False\n\nvisible = tf.keras.layers.Input(shape=(image_size, image_size, 3), name=\"input\")\nx1 = model_densenet_nohead(visible)\nx2 = model_efnet_nohead(visible)\nx3 = model_vgg_nohead(visible)\nmerge = tf.keras.layers.concatenate([x1, x2, x3], name=\"concatallprobs\")\nmodel_out = tf.keras.layers.Dense(3, activation='softmax', name=\"predictions\")(merge)\n\nmodel_densenet_efnet_vgg_nohead = Model(inputs=visible, outputs=model_out)\nmodel_densenet_efnet_vgg_nohead.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),loss='categorical_crossentropy',metrics=['accuracy']) #categorical_smooth_loss\nmodel_densenet_efnet_vgg_nohead.summary()","f1572ddb":"plot_model(model_densenet_efnet_vgg_nohead, show_shapes=True, to_file='model_densenet_efnet_vgg_nohead.png')","7575c6de":"# training call backs \nlr_reduce = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, epsilon=0.0001, patience=1, verbose=1)\nes_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, verbose=1)","188d7967":"history_nohead = model_densenet_efnet_vgg_nohead.fit_generator(training_set, validation_data=testing_set, callbacks=[lr_reduce, es_callback], epochs=20)","4ad87790":"model_densenet_efnet_vgg_nohead.save(\"model_densenet_efnet_vgg_nohead_09xxx.h5\")","8f6f88ab":"# pretrained_densenet = tf.keras.applications.DenseNet201(input_shape=(image_size, image_size, 3), weights='imagenet', include_top=False)\n# pretrained_efnet = efn.EfficientNetB7(input_shape=(image_size, image_size, 3), weights='noisy-student', include_top=False)\n\n\n# for layer in pretrained_densenet.layers:\n#   layer.trainable = False\n\n# for layer in pretrained_efnet.layers:\n#   layer.trainable = False\n\n# visible = tf.keras.layers.Input(shape=(image_size, image_size, 3))\n\n# # x1 = pretrained_densenet.output\n# x1 = pretrained_densenet(visible)\n# x1 = tf.keras.layers.AveragePooling2D()(x1)\n# x1 = tf.keras.layers.Flatten()(x1)\n# x1 = tf.keras.layers.Dense(32, activation=\"relu\")(x1)\n# x1 = tf.keras.layers.Dropout(0.5)(x1)\n# #x1 = tf.keras.layers.Dense(64, activation=\"relu\")(x1)\n\n# # x2 = pretrained_efnet.output\n# x2 = pretrained_efnet(visible)\n# x2 = tf.keras.layers.AveragePooling2D()(x2)\n# x2 = tf.keras.layers.Flatten()(x2)\n# x2 = tf.keras.layers.Dense(32, activation=\"relu\")(x2)\n# x2 = tf.keras.layers.Dropout(0.5)(x2)\n# #x2 = tf.keras.layers.Dense(64, activation=\"relu\")(x2)\n\n# merge = tf.keras.layers.concatenate([x1, x2])\n# x = tf.keras.layers.Dense(32, activation='relu')(merge)\n# x = tf.keras.layers.Dropout(0.5)(x)\n# model_out = tf.keras.layers.Dense(3, activation='softmax')(x)\n\n# #model_densenet = Model(inputs=pretrained_densenet.input, outputs=out_dense_model)\n# model_densenet_efnet = Model(inputs=visible, outputs=model_out)\n# model_densenet_efnet.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),loss='categorical_crossentropy',metrics=['accuracy'])\n# model_densenet_efnet.summary()","61c44529":"### Helpers","e4079dba":"### Another setup: Ensembel with more shared layers (optional)","3b682726":"## Pretrained VGG","58485315":"## Ensemble (simply at the last layer)\nThe network tries to find a combination of final probs (not the best way, but very simple where we have only 30 learnable parameters)","f40cc3b1":"## Ensemble: without the heads\nGives the network the chance to learn from the dense layers before the softmax","6aba2185":"## Data sets ","1bd76cb4":"## Models","8ebd07d0":"## Pretrained efnet","be6f867e":"## Explor data folders","ad281ba8":"## Pretrained densenet","f289233d":"## Prediction Sample"}}