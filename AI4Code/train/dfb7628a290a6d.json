{"cell_type":{"681416dc":"code","1f934947":"code","f626788c":"code","c45b1f5d":"code","7192fab3":"code","8af1e9f3":"code","22c3935c":"code","2e53fa4e":"code","1789cc1c":"code","96d6006d":"code","72e68ec5":"code","c0d9fb3d":"code","ece868df":"code","22acec56":"code","9653b56a":"code","fd38b0bc":"code","9c30826f":"code","0f9bbe57":"code","882e0c54":"code","9370f2f4":"code","6eeddfd4":"code","fb2bb557":"code","8a2b3fb6":"code","160757d6":"code","f7fb137d":"code","4b4d5ddd":"code","23e9362d":"code","eb2b04e7":"code","2a62df2a":"code","2c1d6f87":"code","26b20285":"code","a1325d72":"code","233f6fe5":"code","53954a07":"code","8dc12062":"code","56921826":"code","555f2a5f":"code","17bb5ce7":"code","1885d84e":"code","982112e1":"code","782c4e18":"code","8ddd95b0":"code","136933ff":"code","44ec67ad":"code","fe7ca8f4":"code","ebe6a218":"code","2ae5b7f0":"code","332efe33":"code","48da621d":"code","f37fe93b":"code","cb41ef43":"code","b0470dde":"code","7216b0e6":"code","a8e18e6e":"code","f087a59c":"code","a0bf8817":"code","ba2a3660":"code","9930cbea":"code","62fdc852":"code","b3713f9e":"code","5ccf8829":"code","5f98f568":"code","01e84ee2":"code","70b8bb90":"code","e167db27":"code","37f573a8":"code","e1122d05":"code","d52b908e":"code","341be6a7":"code","e10cb850":"code","7c2e917a":"code","8d486995":"code","25a7e68a":"code","ece1e36d":"markdown","855bd71d":"markdown","f16b4c78":"markdown","26c4ca95":"markdown","8e9c1e88":"markdown","b9335058":"markdown","3a52b711":"markdown","9d652bc8":"markdown","68335421":"markdown","2cbecf1f":"markdown","9dbf9e6d":"markdown","dec0885d":"markdown","6649f3e3":"markdown","45350748":"markdown","32b1031c":"markdown","f100f3b9":"markdown","47f665ed":"markdown","1b097563":"markdown","ec2dbb7e":"markdown"},"source":{"681416dc":"!pip install scispacy","1f934947":"#!pip install https:\/\/s3-us-west-2.amazonaws.com\/ai2-s2-scispacy\/releases\/v0.2.4\/en_ner_bc5cdr_md-0.2.4.tar.gz","f626788c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\n# for dirname, _, filenames in os.walk('\/kaggle\/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\n\n\n\nimport re \nimport string \nimport nltk \nimport spacy \nimport pandas as pd \nimport numpy as np \nimport math \nfrom tqdm import tqdm \n\nfrom spacy.matcher import Matcher \nfrom spacy.tokens import Span \nfrom spacy import displacy \nimport ast\nimport networkx as nx\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport os\nimport functools\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom statsmodels.tsa.seasonal import seasonal_decompose\nimport datetime\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport nltk\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\n\nimport scipy.stats.distributions as dists\n\n#from scispacy.umls_linking import UmlsEntityLinker\n#import en_ner_bc5cdr_md\n\nfrom ipywidgets import widgets, interact, interactive, fixed, interact_manual, interactive_output, GridspecLayout, Layout, Button, Box, VBox, Accordion, DatePicker\nfrom IPython.display import display, Image, HTML, Markdown, clear_output\n\ndisplay(HTML(\"<style>.container { width:100% !important; }<\/style>\"))\ndisplay(HTML(\"<style>.container { background-image: linear-gradient(-90deg, #FFFFFF,#DDDDDD,#FFFFFF,#DDDDDD, #FFFFFF) !important; }<\/style>\"))\n\nimport json\n!pip install rank_bm25 -q\nfrom pathlib import Path, PurePath\nfrom time import time\nimport nltk\nfrom nltk.corpus import stopwords\nimport re\nimport string\nimport torch\nfrom rank_bm25 import BM25Okapi\nfrom math import ceil\n! pip install -U pip\n\n\nimport spacy\n\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport nltk\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\n\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ncolor = sns.color_palette()\n\n%matplotlib inline\n\nfrom plotly import tools\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport networkx as nx\nenglish_stopwords = list(set(stopwords.words('english')))","c45b1f5d":"! python -m spacy download 'en_core_web_md'\nimport en_core_web_md\nnlp = en_core_web_md.load()","7192fab3":"! pip install swifter\nimport swifter","8af1e9f3":"def to_covid_json(json_files):\n    jsonl = []\n    for file_name in tqdm(json_files):\n        row = {\"doc_id\": None, \"title\": None, \"abstract\": None, \"body\": None}\n\n        with open(file_name) as json_data:\n            data = json.load(json_data)\n            print(data)\n            \n            break\n\n            row['doc_id'] = data['paper_id']\n            row['title'] = data['metadata']['title']\n            \n            abstract_list = [abst['text'] for abst in data['abstract']]\n            abstract = \"\\n\".join(abstract_list)\n            row['abstract'] = abstract\n\n            # And lastly the body of the text. \n            body_list = [bt['text'] for bt in data['body_text']]\n            body = \"\\n\".join(body_list)\n            row['body'] = body\n            \n        jsonl.append(row)\n    \n    return jsonl\n    \n\ndef get_data():\n    try:\n        with open('df_cache.pickle', 'rb') as f:\n            df = pickle.load(f)\n    except FileNotFoundError:\n        df = pd.DataFrame(to_covid_json(json_files))\n        with open('df_cache.pickle', 'wb') as f:\n            pickle.dump(df, f)\n    return df\n","22c3935c":"data_path = '..\/input\/CORD-19-research-challenge'\nmetadata_path = data_path + '\/metadata.csv'\nmetadata_df = pd.read_csv(metadata_path, dtype={'Microsoft Academic Paper ID': str, 'pubmed_id': str})","2e53fa4e":"metadata_df.head()","1789cc1c":"cnt_srs = metadata_df['source_x'].value_counts()\ntrace = go.Bar(\n    x=cnt_srs.index,\n    y=cnt_srs.values,\n    marker=dict(\n        color=cnt_srs.values,\n        colorscale = 'Picnic',\n        reversescale = True\n    ),\n)\nlayout = go.Layout(\n    title='Research Paper Source Count',\n    font=dict(size=18)\n)\n\ndata = [trace]\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig, filename=\"TargetCount\")","96d6006d":"labels = (np.array(cnt_srs.index))\nsizes = (np.array((cnt_srs \/ cnt_srs.sum())*100))\n\ntrace = go.Pie(labels=labels, values=sizes)\nlayout = go.Layout(\n    title='Research Paper Distribution',\n    font=dict(size=18),\n    width=600,\n    height=600,\n)\ndata = [trace]\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig, filename=\"usertype\")","72e68ec5":"metadata_df.head()","c0d9fb3d":"from wordcloud import WordCloud, STOPWORDS\n\n# Thanks : https:\/\/www.kaggle.com\/aashita\/word-clouds-of-various-shapes ##\ndef plot_wordcloud(text, mask=None, max_words=200, max_font_size=100, figure_size=(24.0,16.0), \n                   title = None, title_size=40, image_color=False):\n    \n    \n    stopwords = english_stopwords\n    more_stopwords = ['Abstract', 'Background', 'NaN','human','analyses','reveal','common','recent','army']\n    stopwords.extend(more_stopwords)\n\n    wordcloud = WordCloud(background_color='black',\n                    stopwords = stopwords,\n                    max_words = max_words,\n                    max_font_size = max_font_size, \n                    random_state = 42,\n                    width=800, \n                    height=400,\n                    mask = mask)\n    wordcloud.generate(str(text))\n    \n    plt.figure(figsize=figure_size)\n    if image_color:\n        image_colors = ImageColorGenerator(mask);\n        plt.imshow(wordcloud.recolor(color_func=image_colors), interpolation=\"bilinear\");\n        plt.title(title, fontdict={'size': title_size,  \n                                  'verticalalignment': 'bottom'})\n    else:\n        plt.imshow(wordcloud);\n        plt.title(title, fontdict={'size': title_size, 'color': 'black', \n                                  'verticalalignment': 'bottom'})\n    plt.axis('off');\n    plt.tight_layout()  \n    \nplot_wordcloud(metadata_df[\"abstract\"], title=\"Word Cloud Of Abstract\")","ece868df":"metadata_df['abstract_length'] = metadata_df['abstract'].apply(lambda x : len(str(x).split()))\nabstract_length = metadata_df[metadata_df['abstract_length']<3500]['abstract_length']","22acec56":"abstract_length_distribution = go.Box(y=abstract_length.values, name = 'Research Paper Abstract Length', boxmean=True)\ndata = [abstract_length_distribution]\nlayout = go.Layout(title = \"Abstract Length Distribution\")\nfig = go.Figure(data=data,layout=layout)\npy.iplot(fig)","9653b56a":"# from spacy import displacy\n\n# import spacy\n# bc5 = en_ner_bc5cdr_md.load()\n\n# example_text = metadata_df['abstract'][5]\n# example_text = 'Anti-malarial drug hydroxychloroquine shot to fame as it is shown to have shortened the time to clinical recovery of COVID-19 patients.'\n# doc = bc5(example_text)\n# colors = {\n#     'CHEMICAL': 'lightpink',\n#     'DISEASE': 'lightorange',\n# }\n# displacy.render(doc, style='ent', options={\n#     'colors': colors\n# })","fd38b0bc":"# edges = []\n# example_text = 'Hydroxychloroquine is used in clinical recovery of COVID-19 patients.'\n# doc = bc5(example_text)\n# for token in doc:\n#     # FYI https:\/\/spacy.io\/docs\/api\/token\n#     for child in token.children:\n#         edges.append(('{0}'.format(token.lower_),\n#                       '{0}'.format(child.lower_)))\n\n# graph = nx.DiGraph(edges)","9c30826f":"# options = {\n#     'node_color': 'yellow',\n#     'node_size': 100,\n#     'width': 1,\n#     'arrowstyle': '-|>',\n#     'arrowsize': 12,\n# }\n\n\n# nx.draw_networkx(graph, arrows=True, **options)","0f9bbe57":"\ndef strip_characters(text):\n    t = re.sub('\\(|\\)|:|,|;|\\.|\u2019|\u201d|\u201c|\\?|%|>|<', '', text)\n    t = re.sub('\/', ' ', t)\n    t = t.replace(\"'\",'')\n    return t\ndef clean(text):\n    t = text.lower()\n    t = strip_characters(t)\n    return t\ndef tokenize(text):\n    words = nltk.word_tokenize(text)\n    return list(set([word for word in words \n                     if len(word) > 1\n                     and not word in english_stopwords\n                     and not (word.isnumeric() and len(word) is not 4)\n                     and (not word.isnumeric() or word.isalpha())] )\n               )\ndef preprocess(text):\n    t = clean(text)\n    tokens = tokenize(t)\n    return tokens","882e0c54":"metadata_df['abstract'].fillna('', inplace = True)\nmetadata_df['title'].fillna('', inplace = True)","9370f2f4":"print(\"Documents where title is empty but abstract is present \",len(metadata_df[(metadata_df['title'] == '') &  (metadata_df['abstract']!= '')]))\nprint(\"Documents where title is present but abstract are absent\", len(metadata_df[(metadata_df['title'] != '') &  (metadata_df['abstract']== '')]))","6eeddfd4":"def fill_abstract_with_title(abstract , title):\n    if abstract == '':\n        return title\n    else:\n        return abstract\n\ndef fill_title_with_abstract(abstract, title):\n    if title == '':\n        return abstract.split('\\n')[0]\n    else:\n        return title","fb2bb557":"metadata_df['abstract'] = metadata_df.apply(lambda x: fill_abstract_with_title(x['abstract'], x['title']), axis = 1)\nmetadata_df['title'] = metadata_df.apply(lambda x: fill_title_with_abstract(x['abstract'], x['title']), axis = 1)","8a2b3fb6":"print(\"Documents where title is empty but abstract is present \",len(metadata_df[(metadata_df['title'] == '') &  (metadata_df['abstract']!= '')]))\nprint(\"Documents where title is present but abstract are absent\", len(metadata_df[(metadata_df['title'] != '') &  (metadata_df['abstract']== '')]))","160757d6":"w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\nwordnet_lemmatizer = WordNetLemmatizer()\nstop_words = set(stopwords.words('english')) \n\n\ndef lemmatize_text(text):\n\n    word_tokens = word_tokenize(text)    \n    filtered_sentence = [] \n\n    for w in word_tokens: \n        filtered_sentence.append(w)\n        \n    lemma_word = []\n\n    for w in filtered_sentence:\n        word1 = wordnet_lemmatizer.lemmatize(w, pos = \"n\")\n        word2 = wordnet_lemmatizer.lemmatize(word1, pos = \"v\")\n        word3 = wordnet_lemmatizer.lemmatize(word2, pos = (\"a\"))\n        lemma_word.append(word3)\n\n    return ' '.join(lemma_word)","f7fb137d":"metadata_df['abstract_cleaned'] = metadata_df['abstract'].apply(lambda x: clean(str(x)))\n# metadata_df['abstract_cleaned'] = metadata_df['abstract_cleaned'].apply(lambda x: lemmatize_text(str(x)))\n\nCOVID19_SYNONYMS = [\n                    'covid',\n                    'coronavirus disease 19',\n                    'sars cov 2', # Note that search function replaces '-' with ' '\n                    '2019 ncov',\n                    '2019ncov',\n                    r'2019 n cov\\b',\n                    r'2019n cov\\b',\n                    'ncov 2019',\n                    r'\\bn cov 2019',\n                    'coronavirus 2019',\n                    'wuhan pneumonia',\n                    'wuhan virus',\n                    'wuhan coronavirus',\n                    r'coronavirus 2\\b'\n]\n\ndef abstract_title_filter(df, search_string):\n    return (df.abstract.str.lower().str.replace('-', ' ')\n            .str.contains(search_string, na=False) |\n            df.title.str.lower().str.replace('-', ' ')\n            .str.contains(search_string, na=False))\n\ndef count_and_tag(df: pd.DataFrame,\n                  synonym_list: list,\n                  tag_suffix: str) -> (pd.DataFrame, pd.Series):\n    counts = {}\n    df[f'tag_{tag_suffix}'] = False\n    for s in synonym_list:\n        synonym_filter = abstract_title_filter(df, s)\n        counts[s] = sum(synonym_filter)\n        df.loc[synonym_filter, f'tag_{tag_suffix}'] = True\n    print(f'Added tag_{tag_suffix} to DataFrame')\n    return df, pd.Series(counts)\n\ndef add_tag_covid19(df):\n    # Customised approach to include more complicated logic\n    df, covid19_counts = count_and_tag(df, COVID19_SYNONYMS, 'disease_covid19')\n    novel_corona_filter = (abstract_title_filter(df, 'novel corona') &\n                           df.publish_time.str.startswith('2020', na=False))\n    df.loc[novel_corona_filter, 'tag_disease_covid19'] = True\n    covid19_counts = covid19_counts.append(pd.Series(index=['novel corona'],\n                                                     data=[novel_corona_filter.sum()]))\n    return df, covid19_counts","4b4d5ddd":"covid_19_df = add_tag_covid19(metadata_df)\ndf_dedupe = covid_19_df[0][covid_19_df[0]['tag_disease_covid19'] == True].reset_index(drop = True)","23e9362d":"import re\nfrom gensim.models import FastText","eb2b04e7":"to_remove_words = ['\\n','Abstract','Intrduction',' +']\ndef join_noun_chunks(text):\n    doc = nlp(text)\n    noun_chunks = [chunk.text for chunk in doc.noun_chunks]\n    tokens = [token.text for token in doc]\n    counter = 0\n    i = 0\n    #tokens = text.split()\n    final_text = ''\n    #print('Noun Chunks are ', noun_chunks)\n    while (i < (len(tokens))) and counter<len(noun_chunks):\n        if(tokens[i] in noun_chunks[counter]):\n            final_text = final_text + ' ' + '_'.join(noun_chunks[counter].split())\n            i += len(noun_chunks[counter].split())\n            counter += 1\n        else:     \n            final_text = final_text + ' ' + tokens[i] \n            i+=1\n            \n    return final_text.split()","2a62df2a":"df_dedupe['abstract_n_gram'] = df_dedupe['abstract'].swifter.apply(join_noun_chunks)","2c1d6f87":"def tokenize(text):\n    return [word.strip(string.punctuation).lower() for word in text.split() if word not in stop_words]\n\n\ndef create_sentences_df(data):\n    sentences = []\n    ids = []\n    for idx, row in data.iterrows():\n        \n        # Create list of sentences from the paper's text\n        sents = nltk.sent_tokenize(str(row['abstract_cleaned']))\n        # Create a list where all elements are the paper's index number\n        ids.extend([idx] * len(sents))\n        sentences.extend(sents)\n\n    df = pd.DataFrame({\"Id\": ids, \"Sentence\": sentences})\n    # Drop sentences that are too short or too long\n    df = df[(df.Sentence.map(len) >= 10) & (df.Sentence.map(len) <= 510)]\n\n    # Remove period at the end of the sentence and also any period not followed by a digit\n    df['Sentence'] = df['Sentence'].apply(lambda x: re.sub('\\.(?!\\d)', '', x))\n\n    return df","26b20285":"sentences_df = create_sentences_df(metadata_df)\nsentences_df = sentences_df['Sentence'].apply(tokenize)","a1325d72":"from gensim.summarization.summarizer import summarize\nfrom gensim.utils import simple_preprocess\nfrom gensim.models.word2vec import Word2Vec\nimport gensim","233f6fe5":"model_w2v = Word2Vec(\n    sentences_df,\n    size=200,\n    window=5,\n    min_count=10,\n    sg=1,\n    workers=10,\n    iter=10)\n# Normalize the word embeddings\nmodel_w2v.init_sims(replace=True)","53954a07":"similar_word_list = [elem[0] for elem in model_w2v.most_similar(lemmatize_text('virus'))]","8dc12062":"def tsne_plot(model, word_list):\n    \"Creates and TSNE model and plots it\"\n    labels = []\n    tokens = []\n\n    for word in word_list:\n        tokens.append(model[word])\n        labels.append(word)\n    \n    tsne_model = TSNE(perplexity=40, n_components=2, init='pca', n_iter=2500, random_state=23)\n    new_values = tsne_model.fit_transform(tokens)\n\n    x_cords = []\n    y_cords = []\n    \n    for value in new_values:\n        x_cords.append(value[0])\n        y_cords.append(value[1])\n        \n    fig = go.Figure()\n\n    fig.add_trace(go.Scatter(\n        x=x_cords,\n        y=y_cords,\n        mode=\"lines+markers+text\",\n        name=\"Lines, Markers and Text\",\n        text=word_list,\n        textposition=\"top center\"\n    ))\n        \n    fig.show()","56921826":"from sklearn.manifold import TSNE\ntsne_plot(model_w2v, similar_word_list)","555f2a5f":"from gensim.models import FastText\nmodel_fastext = FastText(min_count=1)\nmodel_fastext.build_vocab(sentences_df)\nmodel_fastext.train(sentences = sentences_df, total_examples=len(sentences_df), epochs=10)","17bb5ce7":"from gensim.test.utils import get_tmpfile\nfname = get_tmpfile(\"fasttext_w2v.model\")\nmodel_fastext.save(fname)\nmodel_fastext = FastText.load(fname)","1885d84e":"model_fasttext_covid = FastText(min_count=1, size=200, window=5)\nmodel_fasttext_covid.build_vocab(df_dedupe['abstract_n_gram'])","982112e1":"model_fasttext_covid.train(sentences = df_dedupe['abstract_n_gram'], total_examples=len(df_dedupe['abstract_n_gram']), epochs=10)","782c4e18":"covid_kaggle_questions = {\n\"data\":[\n          {\n              \"task\": \"What is known about transmission, incubation, and environmental stability?\",\n              \"questions\": [\n                  \"Is the virus transmitted by aerisol, droplets, food, close contact, fecal matter, or water?\",\n                  \"How long is the incubation period for the virus?\",\n                  \"Can the virus be transmitted asymptomatically or during the incubation period?\",\n                  \"How does weather, heat, and humidity affect the tramsmission of 2019-nCoV?\",\n                  \"How long can the 2019-nCoV virus remain viable on common surfaces?\"\n              ]\n          },\n          {\n              \"task\": \"What do we know about COVID-19 risk factors?\",\n              \"questions\": [\n                  \"What risk factors contribute to the severity of 2019-nCoV?\",\n                  \"How does hypertension affect patients?\",\n                  \"How does heart disease affect patients?\",\n                  \"How does copd affect patients?\",\n                  \"How does smoking affect patients?\",\n                  \"How does pregnancy affect patients?\",\n                  \"What is the fatality rate of 2019-nCoV?\",\n                  \"What public health policies prevent or control the spread of 2019-nCoV?\"\n              ]\n          },\n          {\n              \"task\": \"What do we know about virus genetics, origin, and evolution?\",\n              \"questions\": [\n                  \"Can animals transmit 2019-nCoV?\",\n                  \"What animal did 2019-nCoV come from?\",\n                  \"What real-time genomic tracking tools exist?\",\n                  \"What geographic variations are there in the genome of 2019-nCoV?\",\n                  \"What effors are being done in asia to prevent further outbreaks?\"\n              ]\n          },\n          {\n              \"task\": \"What do we know about vaccines and therapeutics?\",\n              \"questions\": [\n                  \"What drugs or therapies are being investigated?\",\n                  \"Are anti-inflammatory drugs recommended?\"\n              ]\n          },\n          {\n              \"task\": \"What do we know about non-pharmaceutical interventions?\",\n              \"questions\": [\n                  \"Which non-pharmaceutical interventions limit tramsission?\",\n                  \"What are most important barriers to compliance?\"\n              ]\n          },\n          {\n              \"task\": \"What has been published about medical care?\",\n              \"questions\": [\n                  \"How does extracorporeal membrane oxygenation affect 2019-nCoV patients?\",\n                  \"What telemedicine and cybercare methods are most effective?\",\n                  \"How is artificial intelligence being used in real time health delivery?\",\n                  \"What adjunctive or supportive methods can help patients?\"\n              ]\n          },\n          {\n              \"task\": \"What do we know about diagnostics and surveillance?\",\n              \"questions\": [\n                  \"What diagnostic tests (tools) exist or are being developed to detect 2019-nCoV?\"\n              ]\n          },\n          {\n              \"task\": \"Other interesting questions\",\n              \"questions\": [\n                  \"What is the immune system response to 2019-nCoV?\",\n                  \"Can personal protective equipment prevent the transmission of 2019-nCoV?\",\n                  \"Can 2019-nCoV infect patients a second time?\"\n              ]\n          }\n   ]\n}","8ddd95b0":"for tasks in covid_kaggle_questions[\"data\"]:\n    doc = nlp(tasks['task'])\n    for chunk in doc.noun_chunks:\n        if(chunk.text == 'What' or chunk.text == 'we'):\n            continue\n        print(lemmatize_text(chunk.text))\n        print('FastText Model')\n        print('\\t',model_fastext.most_similar(lemmatize_text(chunk.text)))\n        print('\\nFast Text Filtered Model')\n        print('\\t',model_fasttext_covid.most_similar(lemmatize_text(chunk.text)))\n#         print('Word2Vec Model')\n#         try:\n#             print('\\t',model_w2v.most_similar(lemmatize_text(chunk.text)))\n#         except:\n#             print('Phrase Not in Vocabulary')\n        print('\\n\\n')","136933ff":"def get_statistical_vector(w2v_model, input_words):\n    \n    words = input_words.split(' ')\n    embedding_matrix = []\n    \n    for word in words:\n        try:\n            embedding_matrix.append(w2v_model[word])\n        except:\n            embedding_matrix.append([0]*len(w2v_model['the']))\n    \n    embedding_max = np.max(embedding_matrix, axis = 0)\n    embedding_min = np.min(embedding_matrix, axis = 0)\n    embedding_avg = np.mean(embedding_matrix, axis = 0)\n    \n    return embedding_max, embedding_min, embedding_avg","44ec67ad":"embedding_max, embedding_min, embedding_avg = get_statistical_vector(model_w2v, 'risk factor')","fe7ca8f4":"model_w2v.similar_by_vector(embedding_max)","ebe6a218":"model_w2v.similar_by_vector(embedding_min)","2ae5b7f0":"model_w2v.similar_by_vector(embedding_avg)","332efe33":"from nltk.corpus import stopwords\nraw_search_str = metadata_df.abstract.fillna('') + metadata_df.title.fillna('')","48da621d":"# adapted from https:\/\/www.kaggle.com\/dgunning\/building-a-cord19-research-engine-with-bm25\nenglish_stopwords = list(set(stopwords.words('english')))","f37fe93b":"def remove_special_character(text):\n    \"\"\"\n    Remove all special character from text string\n    \"\"\"\n    return text.translate(str.maketrans('', '', string.punctuation))\n\n\ndef tokenize(text):\n    \"\"\"\n    Tokenize with NLTK\n\n    Rules:\n        - drop all words of 1 and 2 characters\n        - drop all stopwords\n        - drop all numbers\n    \"\"\"\n    words = nltk.word_tokenize(text)\n    return list(set([word for word in words \n                     if len(word) > 1\n                     and not word in english_stopwords\n                     and not word.isnumeric() \n                    ])\n               )\n\ndef preprocess(text):\n    \"\"\"\n    Clean and tokenize text input\n    \"\"\"\n    return tokenize(remove_special_character(text.lower()))","cb41ef43":"preprocess('medical care')","b0470dde":"def search_bm25_dataframe(query, num):\n    new_words = ''\n    doc = nlp(query)\n    key_expanded_final = []\n    for chunk in doc.noun_chunks:\n        if(chunk.text == 'What' or chunk.text == 'we'):\n            continue\n        else:\n            similar_words = model_fasttext_covid.most_similar(chunk.text)\n            keywords_expanded = [word[0].replace('_',' ') for word in similar_words[:5]]\n            keywords_expanded = [word for word in keywords_expanded if word not in query]\n            \n        key_expanded_final = key_expanded_final + keywords_expanded\n    \n    key_expanded_final = ' '.join(list(set(key_expanded_final)))    \n        \n    print('Expanded Keywords', key_expanded_final)\n    query = query + key_expanded_final\n    \n    query = preprocess(query)\n    doc_scores = bm25.get_scores(query)\n    selected_indexes = np.argsort(doc_scores)[-num:]\n    #print('Selected Indexes are ',selected_indexes)\n    result_df = metadata_df.iloc[selected_indexes]\n    #print('Result Dataframe ',result_df.head())\n    result_df['scores'] = doc_scores[selected_indexes]\n    #print(result_df['scores'])\n    result_df = result_df[result_df['scores']>0]\n    result_df = result_df.reset_index(drop = True)\n    \n    #print(result_df)\n    return result_df  ","7216b0e6":"metadata_df['tokenized_abstract_title'] = metadata_df.apply(lambda x: preprocess(str(x['abstract'])+str(x['title'])), axis = 1)\nbm25 = BM25Okapi(metadata_df['tokenized_abstract_title'].tolist())","a8e18e6e":"import torch\nfrom transformers import BertTokenizer\nfrom transformers import BertForQuestionAnswering\n\nNUM_CONTEXT_FOR_EACH_QUESTION = 10\n\ntorch_device = 'cuda' if torch.cuda.is_available() else 'cpu'\n\nprint(\"Code running on: {}\".format(torch_device) )","f087a59c":"BERT_SQUAD = 'bert-large-uncased-whole-word-masking-finetuned-squad'\n\nmodel = BertForQuestionAnswering.from_pretrained(BERT_SQUAD)\ntokenizer = BertTokenizer.from_pretrained(BERT_SQUAD)\n\nmodel = model.to(torch_device)\nmodel.eval()","a0bf8817":"context = search_bm25_dataframe('Is the virus transmitted by aerisol, droplets, food, close contact, fecal matter, or water?', 20)","ba2a3660":"#search_bm25_dataframe('What are the risk factors of coronavirus ?', 20)","9930cbea":"def get_rel_span(question , context):\n    \n    encoded_dict = tokenizer.encode_plus(\n                    question, context, # Sentence to encode.\n                    add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n                    max_length = 256,  # Pad & truncate all sentences.\n                    pad_to_max_length = True,\n                    return_attention_mask = True,   # Construct attn. masks.\n                    return_tensors = 'pt'     # Return pytorch tensors.\n               )\n    \n    input_ids = encoded_dict['input_ids'].to(torch_device)\n    token_type_ids = encoded_dict['token_type_ids'].to(torch_device)\n    \n    all_tokens = tokenizer.convert_ids_to_tokens(input_ids[0])\n    start_scores, end_scores = model(input_ids, token_type_ids=token_type_ids)\n    answer = tokenizer.convert_tokens_to_string(all_tokens[torch.argmax(start_scores) : torch.argmax(end_scores)+1])\n    \n    return answer\n    ","62fdc852":"## Getting Sample Answer before generalizing. \n\ndef on_search_button_click(_):\n    #print('Entered')\n    with out:\n        clear_output(True)\n        #print('Entered')\n        question = search_box.value\n        #print('Question is ', question)\n        context = search_bm25_dataframe(question, 20)\n\n        context = context[:min(len(context), 10)]\n        context['rel_span'] = context['abstract'].apply(lambda x:get_rel_span(question,x))\n        context= context[['url','title','doi','pmcid','abstract','publish_time','rel_span']].reset_index(drop = True)\n        show_df = context[['title','abstract','rel_span']].reset_index(drop = True)\n        display(show_df)\n        #display(context)\n        #display(context)\n        items = []\n        summaries = []\n        \n        for i,result in enumerate(context['rel_span']):\n            sent = context.loc[i, 'abstract']\n            word_tokens = word_tokenize(str(sent))\n            summary = ''\n            for word in word_tokens:\n                if word in stop_words:\n                    summary += word+' '\n                    continue\n                if word in str(result):\n                    summary += '<mark>'+word+'<\/mark>'\n                else:\n                    summary += word+' '\n                    \n            summaries.append(summary)\n            \n            \n\n        for i in range(len(context)):\n            sum_text = widgets.HTML(value='<h3><a href='+str(context['url'][i])+'>'+str(context['title'][i])+'<\/a><\/h3><b>'+str(context['publish_time'][i])+'<\/b>: '+'\\n'+str(summaries[i]),\n                                placeholder='',description='')\n            items.append(sum_text)\n\n        box3 = VBox(children=items)\n        display(box3)","b3713f9e":"from ipywidgets import widgets, interact, interactive, fixed, interact_manual, interactive_output, GridspecLayout, Layout, Button, Box, VBox, Accordion, DatePicker\nfrom IPython.html import widgets\nfrom IPython.display import display, Image, HTML, Markdown, clear_output","5ccf8829":"label_layout = Layout(width='900px',height='30px')\nbutton_layout = Layout(width='100px',height='30px') \n\nsearch_box = widgets.Text(\n    placeholder='What are the factors responsible for coronavirus attack ?',\n    description='Search:', layout = label_layout\n)\n\nsearch_button = widgets.Button(description=\"Search\", button_style='warning', layout = button_layout)\nsearch_button.on_click(on_search_button_click)","5f98f568":"out = widgets.Output()","01e84ee2":"for tasks in covid_kaggle_questions[\"data\"]:\n    print(tasks['task'])\n    for question in tasks['questions']:\n        print('\\t',question)\n    print('\\n\\n')","70b8bb90":"widgets.VBox([search_box, search_button, out])","e167db27":"# def get_answer_question(query, query_context):\n    \n# #     print(query)\n# #     print(query_context)\n#     encoded_dict = tokenizer.encode_plus(\n#                         query, query_context, # Sentence to encode.\n#                         add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n#                         max_length = 256,  # Pad & truncate all sentences.\n#                         pad_to_max_length = True,\n#                         return_attention_mask = True,   # Construct attn. masks.\n#                         return_tensors = 'pt'     # Return pytorch tensors.\n#                    )\n#     input_ids = encoded_dict['input_ids'].to(torch_device)\n#     token_type_ids = encoded_dict['token_type_ids'].to(torch_device)\n\n#     all_tokens = tokenizer.convert_ids_to_tokens(input_ids[0])\n#     start_scores, end_scores = model(input_ids, token_type_ids=token_type_ids)\n#     answer = tokenizer.convert_tokens_to_string(all_tokens[torch.argmax(start_scores) : torch.argmax(end_scores)+1])\n\n#     #print(answer)\n#     return answer\n\n\n# def get_all_context(query, num_results):\n    \n#     possible_contexts_df = search_bm25_dataframe(query, num_results)\n#     possible_contexts = possible_contexts_df['abstract'].str.replace(\"Abstract\", \"\").tolist()\n    \n#     return possible_contexts\n\n\n# def get_all_answers(question, all_context):\n#     # Return a list of all answers, given a question and a list of context\n    \n#     all_answers = []\n    \n#     for context in all_context:\n#         all_answers.append(get_answer_question(question, context))\n#     return all_answers\n        ","37f573a8":"# def create_output_results(question, all_contexts, all_answers, summary_answer, summary_context):\n#     # Return results on a dictionary\n    \n#     def find_start_end_index_substring(context, answer):   \n#         search_re = re.search(re.escape(answer.lower()), context.lower())\n#         if search_re:\n#             return search_re.start(), search_re.end()\n#         else:\n#             return 0, len(context)\n        \n#     output = {}\n#     output['question'] = question\n#     output['summary_answer'] = summary_answer\n#     output['summary_context'] = summary_context\n#     results = []\n#     for c, a in zip(all_contexts, all_answers):\n\n#         span = {}\n#         span['context'] = c\n#         span['answer'] = a\n#         span['start_index'], span['end_index'] = find_start_end_index_substring(c,a)\n\n#         results.append(span)\n    \n#     output['results'] = results\n        \n#     return output","e1122d05":"# def get_results(question, summarize=False, num_results=NUM_CONTEXT_FOR_EACH_QUESTION, verbose=True):\n#     # Return a dict object containg a list of all context and answers related to the (sub)question\n\n#     if verbose:\n#         print(\"Getting context ...\")\n#     all_contexts = get_all_context(question, num_results)\n    \n#     if verbose:\n#         print(\"Answering to all questions ...\")\n#     all_answers = get_all_answers(question, all_contexts)\n    \n#     summary_answer = ''\n#     summary_context = ''\n#     if verbose and summarize:\n#         print(\"Adding summary ...\")\n#     if summarize:\n#         summary_answer = get_summary(all_answers)\n#         summary_context = get_summary(all_contexts)\n    \n#     if verbose:\n#         print(\"output.\")\n    \n#     return create_output_results(question, all_contexts, all_answers, summary_answer, summary_context)","d52b908e":"# NUM_CONTEXT_FOR_EACH_QUESTION = 20","341be6a7":"# covid_kaggle_questions = {\n# \"data\":[\n#           {\n#               \"task\": \"What is known about transmission, incubation, and environmental stability?\",\n#               \"questions\": [\n#                   \"Is the virus transmitted by aerisol, droplets, food, close contact, fecal matter, or water?\",\n#                   \"How long is the incubation period for the virus?\",\n#                   \"Can the virus be transmitted asymptomatically or during the incubation period?\",\n#                   \"How does weather, heat, and humidity affect the tramsmission of 2019-nCoV?\",\n#                   \"How long can the 2019-nCoV virus remain viable on common surfaces?\"\n#               ]\n#           },\n#           {\n#               \"task\": \"What do we know about COVID-19 risk factors?\",\n#               \"questions\": [\n#                   \"What risk factors contribute to the severity of 2019-nCoV?\",\n#                   \"How does hypertension affect patients?\",\n#                   \"How does heart disease affect patients?\",\n#                   \"How does copd affect patients?\",\n#                   \"How does smoking affect patients?\",\n#                   \"How does pregnancy affect patients?\",\n#                   \"What is the fatality rate of 2019-nCoV?\",\n#                   \"What public health policies prevent or control the spread of 2019-nCoV?\"\n#               ]\n#           },\n#           {\n#               \"task\": \"What do we know about virus genetics, origin, and evolution?\",\n#               \"questions\": [\n#                   \"Can animals transmit 2019-nCoV?\",\n#                   \"What animal did 2019-nCoV come from?\",\n#                   \"What real-time genomic tracking tools exist?\",\n#                   \"What geographic variations are there in the genome of 2019-nCoV?\",\n#                   \"What effors are being done in asia to prevent further outbreaks?\"\n#               ]\n#           },\n#           {\n#               \"task\": \"What do we know about vaccines and therapeutics?\",\n#               \"questions\": [\n#                   \"What drugs or therapies are being investigated?\",\n#                   \"Are anti-inflammatory drugs recommended?\"\n#               ]\n#           },\n#           {\n#               \"task\": \"What do we know about non-pharmaceutical interventions?\",\n#               \"questions\": [\n#                   \"Which non-pharmaceutical interventions limit tramsission?\",\n#                   \"What are most important barriers to compliance?\"\n#               ]\n#           },\n#           {\n#               \"task\": \"What has been published about medical care?\",\n#               \"questions\": [\n#                   \"How does extracorporeal membrane oxygenation affect 2019-nCoV patients?\",\n#                   \"What telemedicine and cybercare methods are most effective?\",\n#                   \"How is artificial intelligence being used in real time health delivery?\",\n#                   \"What adjunctive or supportive methods can help patients?\"\n#               ]\n#           },\n#           {\n#               \"task\": \"What do we know about diagnostics and surveillance?\",\n#               \"questions\": [\n#                   \"What diagnostic tests (tools) exist or are being developed to detect 2019-nCoV?\"\n#               ]\n#           },\n#           {\n#               \"task\": \"Other interesting questions\",\n#               \"questions\": [\n#                   \"What is the immune system response to 2019-nCoV?\",\n#                   \"Can personal protective equipment prevent the transmission of 2019-nCoV?\",\n#                   \"Can 2019-nCoV infect patients a second time?\"\n#               ]\n#           }\n#    ]\n# }","e10cb850":"# all_tasks = []\n\n\n# for i, t in enumerate(covid_kaggle_questions['data']):\n#     print(\"Answering question to task {}. ...\".format(i+1))\n#     answers_to_question = []\n#     for q in t['questions']:\n#             answers_to_question.append(get_results(q, verbose=False))\n#     task = {}\n#     task['task'] = t['task']\n#     task['questions'] = answers_to_question\n    \n#     all_tasks.append(task)","7c2e917a":"# all_rows = []\n# for tasks in all_tasks:\n#     task_question = tasks['task']\n#     for sub_question_summary in tasks['questions']:\n#         sub_question = sub_question_summary['question']\n#         for result in sub_question_summary['results']:\n#             result_context = result['context']\n#             result_answer = result['answer']\n#             all_rows.append([task_question, sub_question, result_context, result_answer])\n        ","8d486995":"# final_results_df = pd.DataFrame(all_rows)","25a7e68a":"# final_results_df.to_excel('Question Answering System.xlsx', index = False)","ece1e36d":"![image.png](attachment:image.png)","855bd71d":"We need to give Keywords to Rank BM25 Model, That would help us to get the relevant corpus in which Answer might be present.\n\nWe can find the words similar to those key-words from trained embeddings , And these similar Words would help in expanfing the Query. \n\n\n\n\n\n![image.png](attachment:image.png)","f16b4c78":"### Applying The Logic :\n\n* **If Abstract is Empty :**\n    \n    1. Fill by title\n    \n* **If Title is Empty :**\n    \n    1. Fill by Abstract \n    ","26c4ca95":"![image.png](attachment:image.png)","8e9c1e88":"![image.png](attachment:image.png)","b9335058":"## Query Expansion Logic ","3a52b711":"# **Task Details**\n\nWhat is known about transmission, incubation, and environmental stability? What do we know about natural history, transmission, and diagnostics for the virus? What have we learned about infection prevention and control?\n\nSpecifically, we want to know what the literature reports about:\n\nRange of incubation periods for the disease in humans (and how this varies across age and health status) and how long individuals are contagious, even after recovery.\n\nPrevalence of asymptomatic shedding and transmission (e.g., particularly children).\n\nSeasonality of transmission.\n\nPhysical science of the coronavirus (e.g., charge distribution, adhesion to hydrophilic\/phobic surfaces, environmental survival to inform decontamination efforts for affected areas and provide information about viral shedding).\n\nPersistence and stability on a multitude of substrates and sources (e.g., nasal discharge, sputum, urine, fecal matter, blood).\n\nPersistence of virus on surfaces of different materials (e,g., copper, stainless steel, plastic).\n\nNatural history of the virus and shedding of it from an infected person\n\nImplementation of diagnostics and products to improve clinical processes\n\nDisease models, including animal models for infection, disease and transmission\n\nTools and studies to monitor phenotypic change and potential adaptation of the virus\n\nImmune response and immunity\n\nEffectiveness of movement control strategies to prevent secondary transmission in health care and community settings\n\nEffectiveness of personal protective equipment (PPE) and its usefulness to reduce risk of transmission in health care and community settings\nRole of the environment in transmission","9d652bc8":"## Handling Out Of Vocabulary Words","68335421":"# **Analysis**","2cbecf1f":"## FastText Embedding ","9dbf9e6d":"## Named Entity Recognition","dec0885d":"## **Search Engine Implementation**","6649f3e3":"## Dependency Parser Graph Analysis (Relation Extractor) ","45350748":"## Preprocessing Documents","32b1031c":"## Load MetaData","f100f3b9":"## Training Word2Vec Model ","47f665ed":"## Loading Fast-Text Vectors On CoronaVirus Data","1b097563":"# **Approach to Solve This Problem**\n\n1. ** Analysis (MetaData Articles) **\n\n\n    * Analysis Over The Source Of Research Paper\n        * Counts Of Source\n        * Distribution Of Source\n        \n    * Analysis Over The Abstract Data\n        * Word Cloud\n        * Distribution Of Letters in the Abstracts. \n        \n    *  Named Entity Recognition\n    \n    * Dependency Parser Analysis (Graph Representation)\n    \n\n\n2. ** Modeling **\n\n\n    * Analysis Over The Embeddings \n        * Training Embedding Over The Research Data. \n        * Plotting The Embedding Vectors\n        \n    *  Query Expansion\n    \n    *  Bert Question Answering Module \n   \n     ","ec2dbb7e":"## **Bert Question Answering Module**"}}