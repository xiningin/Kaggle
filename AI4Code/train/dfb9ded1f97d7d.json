{"cell_type":{"3e40f8f5":"code","07ae191b":"code","66023c5d":"code","874d2221":"code","82ca5311":"code","d7332690":"code","f5bc74c9":"code","ff1fdf83":"code","b09008ce":"code","9ff98f2e":"code","303a3d31":"code","4744b5b0":"code","7ab7cb18":"code","2c080e95":"code","5dd97fb8":"code","c9153589":"code","74f0b236":"code","b9a3887d":"code","7510f79f":"code","43875379":"code","197bdb62":"code","d0aadbdb":"code","14134a99":"code","239ff8aa":"code","7086db35":"code","deaeeaab":"code","54872266":"code","77b17775":"code","76894aa3":"code","4d82effb":"code","e1d826bf":"markdown","fbe2a4c8":"markdown","9a9d3fb0":"markdown","df44dcaa":"markdown","69ac4c20":"markdown","7acacb11":"markdown","f6f1d5bb":"markdown","5c29544f":"markdown","12d8aa36":"markdown","4b963c79":"markdown","f84be33e":"markdown","6bfc9bc8":"markdown","3d942d92":"markdown","880898ac":"markdown","f0f3479a":"markdown","27229763":"markdown","37255ea2":"markdown"},"source":{"3e40f8f5":"! pip install fastai2","07ae191b":"from fastai2.basics import *\nfrom fastai2.vision.all import *\nfrom fastai2.callback.all import *\nimport matplotlib.pyplot as plt","66023c5d":"# Load the data\nflower_path = '\/kaggle\/input\/104-flowers-garden-of-eden\/jpeg-512x512'\nTRAIN_DIR  = flower_path + '\/train'\nVAL_DIR  = flower_path + '\/val'\nTEST_DIR  = flower_path + '\/test\/'","874d2221":"item_tfms = Resize(256)\nbatch_tfms = [RandomResizedCrop(224), *aug_transforms(mult=1.0, do_flip=True, max_rotate=30.0, max_zoom=1.5,\n                            max_lighting=.8, max_warp=0.3, p_lighting=.9)]\nbs=128","82ca5311":"# High level fastai API using fastai\ndata = ImageDataLoaders.from_folder(flower_path, valid='val',batch_tfms=batch_tfms, \n                                   item_tfms=item_tfms, bs=bs)","d7332690":"# total no of classes\ndata.c","f5bc74c9":"data.vocab","ff1fdf83":"data.show_batch(max_n=9)","b09008ce":"path = Path(flower_path+'\/train\/')\npath.ls()","9ff98f2e":"flower_blocks = DataBlock(blocks=(ImageBlock, CategoryBlock),\n                   get_items=get_image_files,\n                   splitter=GrandparentSplitter(valid_name='val'),\n                   get_y=[parent_label],\n                   item_tfms=Resize(460),\n                   batch_tfms=aug_transforms(size=224, max_rotate=30, min_scale=0.75))","303a3d31":"dls = flower_blocks.dataloaders(TRAIN_DIR,  bs=8)","4744b5b0":"dls.show_batch(max_n=9)","7ab7cb18":"block = DataBlock(blocks=(ImageBlock, CategoryBlock),\n                 splitter=RandomSplitter(),\n                 get_items=get_image_files,\n                 get_y=parent_label,\n                 batch_tfms=batch_tfms)","2c080e95":"dls = block.dataloaders(path, bs=32)","5dd97fb8":"dls.show_batch()","c9153589":"from torchvision.models import resnet50\nfrom fastai2.metrics import accuracy_multi","74f0b236":"from sklearn.model_selection import StratifiedKFold","b9a3887d":"imgs = get_image_files(path)\nrandom.shuffle(imgs)\nlbls = [parent_label(im) for im in imgs]","7510f79f":"def get_dls(bs, size, val_idx):\n    dblock = DataBlock(blocks=(ImageBlock, CategoryBlock),\n                   get_items=get_image_files,\n                   get_y=parent_label,\n                   splitter=IndexSplitter(val_idx),\n                   item_tfms = Resize(256),\n                   batch_tfms = [RandomResizedCrop(size), \n                                 *aug_transforms(mult=1.0, do_flip=True, \n                                                 max_rotate=30.0, max_zoom=1.5,\n                                                  max_lighting=.8, max_warp=0.3, \n                                                 p_lighting=.9),\n                                 Normalize.from_stats(*imagenet_stats)])\n    return dblock.dataloaders(path, bs=bs)","43875379":"%%time\nval_pct = []\nskf = StratifiedKFold(n_splits=2, shuffle=True)\ni = 0\n\nfor _, val_idx in skf.split(np.array(imgs), lbls):\n    dls = get_dls(32, 128, val_idx)\n    learn = cnn_learner(dls, resnet50, metrics=accuracy)\n    learn.fine_tune(1, cbs=[EarlyStoppingCallback(monitor='accuracy')])\n#     learn.dls = get_dls(32, 224, val_idx)\n#     learn.fine_tune(3, 1e-3, cbs=[EarlyStoppingCallback(monitor='accuracy')])\n    preds, targs = learn.tta()\n    print(accuracy(preds, targs).item())\n    val_pct.append(accuracy(preds, targs).item())\n    i+=1","197bdb62":"interep = ClassificationInterpretation.from_learner(learn)\nlosses, idxs = interep.top_losses()\n\nlen(data.valid_ds)==len(losses)==len(idxs)","d0aadbdb":"interep.plot_top_losses(9, figsize=(15,10))","14134a99":"interep.plot_confusion_matrix(figsize=(25,25\n                                    ), dpi=60)","239ff8aa":"interep.most_confused(min_val=3)","7086db35":"sub= pd.read_csv('..\/input\/tpu-getting-started\/sample_submission.csv')","deaeeaab":"sub.head()","54872266":"n = sub.shape[0]\ntest_folder = TEST_DIR  ","77b17775":"img = plt.imread(TEST_DIR+'416e24d42.jpeg')\nprint(learn.predict(img)[0])","76894aa3":"for i in range(n):\n  idc = sub.iloc[i][0]\n  k = TEST_DIR + idc + '.jpeg'\n  k = plt.imread(k)\n  ans = learn.predict(k)[0]\n  sub.loc[[i],1:] = str(ans)\n\nprint(\"Done Prediction saved ---> \")","4d82effb":"sub.to_csv('submission.csv', index=False)\nsub.head()","e1d826bf":"# Submit predictions","fbe2a4c8":"We'll all wrap them up in a nice little package of a DataBlock. Think of the DataBlock as a list of instructions to do when we're building batches and our DataLoaders. It doesn't need any items explicitly to be done, and instead is a blueprint of how to operate. We define it like so:","9a9d3fb0":"# Look at our results <a id='5'>","df44dcaa":"# Classifying flowers with Fastaiv2\n\n![Screenshot_2020-03-21%20Flower%20with%20TPUs%20-%20Advanced%20augmentation.png](attachment:Screenshot_2020-03-21%20Flower%20with%20TPUs%20-%20Advanced%20augmentation.png)\n\n\n    \n**In this kernel, I will briefly explain how to participate in this competition with latest version of fastaiv2, which is scheduled to be released on early June.**\n\n\n<font size=3 color=\"red\">Please upvote this kernel if you like it. It motivates me to produce more quality content :)<\/font>","69ac4c20":"Calling ImageDataloader and specifying flower_path, trasformations and batch size","7acacb11":"# Installation <a id='1'>","f6f1d5bb":"\n\nWe'll also be utilizing the progressive resizing technique, where we train initially on a smaller set of images before moving upwards to a larger size. We're doing this as our data comes from a variety of sized images, so this will be a good way to get the most out of our data.\n\nNow you're probably confused about the difference between pre-sizing and progressive resizing. I'll try to explain the difference. Pre-sizing is where we initially make our image larger before applying a random crop of the image (such as 256x256 to 224x224). This can bring an opportunity for some smaller or finer details to show up more prominantly.\n\nProgressive resizing is a technique where we start training at a small image size and then increase this image size while training. Here we'll start at an image size of 128x128 and then train on an image size of 224x224 afterwards\n\nNow let's make a function to help us build our DataLoaders in such a way as to support progressive resizing\n","5c29544f":"Just importing the libraries, this is a bad practice of importing everthing, don't you know that?\n","12d8aa36":"# Contents\n\n* [<font size=4>Installation<\/font>](#1)\n* [<font size=4>Flower classification - High level API<\/font>](#2)\n* [<font size=4>Flower classification - low level API<\/font>](#3)\n* [<font size=4>Train Model<\/font>](#4)\n* [<font size=4>Look at our results<\/font>](#5)\n* [<font size=4>Ending Note<\/font>](#6)","4b963c79":"As noted earlier, there are two kinds of transforms: `item_tfms and batch_tfms`. Each do what it sounds like: an item transform is applied on an individual item basis, and a batch transform is applied over each batch of data. The role of the item transform is to prepare everything for a batch level (and to apply any specific item transformations you need), and the batch transform is to further apply any augmentations on the batch level efficently (normalization of your data also happens on a batch level). One of the biggest differences between the two though is where each is done. Item transforms are done on the CPU while batch transforms are performed on the GPU.","f84be33e":"# Train model <a id='4'>","6bfc9bc8":"We used 5 Folds for K-Fold Cross Validation, we'll recreate the process based on this notebook. We'll also use a few different image augmentations and Test Time Augmentation (TTA).","3d942d92":"# Acknowledgment\n\nThis notebook is a product of my experience participating in fastai2 megastudy group conducted by Zach Mueller. More details about the online study group and fantastic fastai2 resources can be found in the [Practical Deep Learning for Coders 2.0 repo](https:\/\/github.com\/muellerzr\/Practical-Deep-Learning-for-Coders-2.0)","880898ac":"# Ending note <a id=\"6\"><\/a>\n\n<font size=4 color=\"red\">This concludes my fastai2 kernel. Please upvote this kernel if you like it. It took me more 30 hours to write this kernel to get familarised with fastai2 <\/font>","f0f3479a":"# Flower classification - medium-level API <a id='3'>","27229763":"Now that we know this, let's build a basic transformation pipeline that looks something like so:\n\n- Resize our images to a fixed size (224x224 pixels)\n- After they are batched together, choose a quick basic augmentation function\n- Normalize all of our image data\n\nLet's build it!","37255ea2":"# Flower classification - High level API <a id='2'>"}}