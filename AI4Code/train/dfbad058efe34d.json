{"cell_type":{"dc07755c":"code","3b8b11b4":"code","ab96a4e2":"code","bf57f6b2":"code","646b6e89":"code","f1e50e87":"code","42836bdb":"code","595f7f44":"code","6a9aec1d":"code","0badb790":"code","4d6f8d3b":"code","0270eb25":"code","605c976d":"code","cfbccfb8":"code","cc6b9828":"code","378b0855":"code","461b7285":"code","08fc172e":"code","731c48e6":"code","35a0198f":"code","1888a59f":"code","8f8dda61":"code","953f460f":"code","a9180db9":"code","731ad07a":"code","eb7aee50":"code","b8c8268f":"code","39bac553":"code","d44b6ab4":"code","796781d9":"code","99f36416":"code","5add6e2e":"code","3ab8ff6a":"code","e511db58":"code","7317f12d":"code","df6e7def":"code","618610b1":"code","9017d0b5":"code","a8cfa665":"markdown","6ceebd95":"markdown","4348804a":"markdown","90568e4e":"markdown","8ba00b1a":"markdown","c28bd101":"markdown","42514adb":"markdown","e12d0001":"markdown","84a5f8a9":"markdown","a05812ec":"markdown","a7cdf931":"markdown","20495601":"markdown","4ce72fb4":"markdown","fb39728d":"markdown","b9b1922d":"markdown","9e51fd21":"markdown","5a65f3c2":"markdown","b6b9d6b6":"markdown","9dd38f7b":"markdown","7f8cccd3":"markdown","0f75b557":"markdown","f294d83a":"markdown","af90e34b":"markdown","0016f4d5":"markdown","e44507bb":"markdown","1ef798d0":"markdown","9b868c0c":"markdown","3ae2c266":"markdown","ae6278a6":"markdown"},"source":{"dc07755c":"import numpy as np \nimport pandas as pd \nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport os","3b8b11b4":"print(os.listdir(\"..\/input\"))","ab96a4e2":"# Loading data\ntrain = pd.read_csv(\"..\/input\/train.csv\")\ntest = pd.read_csv(\"..\/input\/test.csv\")","bf57f6b2":"display(train.head(5))\n\nprint(\"Train shape: \", train.shape)","646b6e89":"display(test.head(5))\n\nprint(\"Test shape: \", test.shape)","f1e50e87":"# Checking null values\ntrain.isnull().sum()","42836bdb":"plt.pie(train[\"Survived\"].value_counts(),explode=[0, 0.02],autopct='%1.1f%%', labels=train[\"Survived\"].value_counts().index)\nplt.title(\"Survival Rate\")\nplt.show()","595f7f44":"f,ax=plt.subplots(1,2,figsize=(18,5))\n\nsns.countplot('Sex',hue='Survived',data=train, ax=ax[0])\nax[0].set_title(\"Male\/Female survival plot\")\n\nsurvivors = train.query(\"Survived == 1\")\nsurvivors[\"Sex\"].value_counts().plot.pie(explode=[0, 0.02],autopct='%1.1f%%', \n        labels=survivors[\"Sex\"].value_counts().index, ax=ax[1])\nax[1].set_title(\"Male\/Female Survival Rate\")\nplt.show()","6a9aec1d":"# Removing the target feature from the remaining dataset\nnew_train = train.drop(\"Survived\", axis=1)\njoin_df = pd.concat([new_train, test])","0badb790":"# Let's have a general view of our feature Age\nnp.unique(join_df[\"Age\"])","4d6f8d3b":"# Replacing the nulls\nmale_mean = train.query(\"Sex == 'male' and Survived==1\")[\"Age\"].mean()\nfemale_mean = train.query(\"Sex == 'female' and Survived==1\")[\"Age\"].mean()\n\njoin_df.loc[(join_df.Age.isnull())&(join_df.Sex=='female'),'Age']=female_mean\njoin_df.loc[(join_df.Age.isnull())&(join_df.Sex=='male'),'Age']=male_mean\n\nprint(\"Total null: {}\".format(join_df.Age.isnull().sum()))\n\n# Rounding the ages\njoin_df[\"Age\"] = join_df[\"Age\"].map(lambda age: int(age))","0270eb25":"survivor_list = train.query(\"Survived == 1\")\n\nprint(\"Survivors mean age is {:.0f}\".format(survivor_list[\"Age\"].mean()))\nprint(\"Survivors mean age for males is {:.0f}\".format(survivor_list.query(\"Sex == 'male'\")[\"Age\"].mean()))\nprint(\"Survivors mean age for females is {:.0f}\".format(survivor_list.query(\"Sex == 'female'\")[\"Age\"].mean()))\nprint(\"Minimal Survivor Age is {:.0f}\".format(min(survivor_list[\"Age\"])))\nprint(\"Maximum Survivor Age is {:.0f}\".format(max(survivor_list[\"Age\"])))\n\nplt.figure(figsize=(25,6))\nsns.barplot(train['Age'],train['Survived'], ci=None)\nplt.xticks(rotation=90);\nplt.show()","605c976d":"# Let's see our classes\nnp.unique(join_df[\"Pclass\"])","cfbccfb8":"train[\"Pclass\"].value_counts()","cc6b9828":"class_count_dict = dict(train[\"Pclass\"].value_counts().sort_index())\n\nfor k,v in class_count_dict.items():\n    print(\"People from the {} class: {}\".format(k, v))","378b0855":"f,ax=plt.subplots(3,2,figsize=(15,15))\n\ntrain[\"Pclass\"].value_counts().plot.pie(explode=[0, 0.02, 0.02],autopct='%1.1f%%', \n        labels=survivors[\"Pclass\"].value_counts().index, ax=ax[0][0])\nax[0][0].set_title(\"Class Survival Proportion\")\n\n\nsns.countplot(train[\"Pclass\"], ax=ax[0][1])\nax[0][1].set_title(\"Count passengers count\")\n\nsns.countplot('Pclass',hue='Survived',data=train, ax=ax[1][0])\nax[1][0].set_title(\"General Survivors per Class\")\n\nsns.countplot('Pclass',hue='Sex',data=train, ax=ax[1][1])\nax[1][1].set_title(\"General Class per Sex\")\n\nsns.countplot('Pclass',hue='Sex',data=train.query(\"Survived == 1\"), ax=ax[2][0])\nax[2][0].set_title(\"Survivors Class per Sex\")\n\nsns.barplot(x='Pclass',y='Survived',data=train, ax=ax[2][1])\nax[2][1].set_title(\"Survivors Rate per Class\")","461b7285":"# Check unique embarked places\nprint(\"Unique places: \", train.Embarked.unique())\n\n# First of all, as we have only a few null values, lets fill with the place that had more embarks\nf,ax=plt.subplots(1,1,figsize=(6,5))\n\ntrain[\"Embarked\"].value_counts().plot.pie(explode=[0, 0.02, 0.02],autopct='%1.1f%%', \n                                              labels=train[\"Embarked\"].value_counts().index, ax=ax)","08fc172e":"print(\"Null values: \", train.Embarked.isnull().sum())\n# Treating missing\ntrain['Embarked'].fillna('S',inplace=True)\njoin_df['Embarked'].fillna('S',inplace=True)\nprint(\"Null values after cleaning: \", train.Embarked.isnull().sum())","731c48e6":"f,ax=plt.subplots(3,2,figsize=(15,15))\n\nsns.countplot(train[\"Embarked\"], ax=ax[0][0])\nax[0][0].set_title(\"Quantity of people that embarked in place\")\n\nsns.countplot('Embarked',hue='Survived',data=train, ax=ax[0][1])\nax[0][1].set_title(\"Survived quantity by place of embark\")\n\nsns.countplot('Embarked',hue='Pclass',data=train, ax=ax[1][0])\nax[1][0].set_title(\"Quantity of class embarked by place\")\n\nsns.countplot('Embarked',hue='Sex',data=train, ax=ax[1][1])\nax[1][1].set_title(\"Sex by place\")\n\nsns.countplot('Embarked',hue='Sex',data=train, ax=ax[1][1])\nax[1][1].set_title(\"Sex by place\")\n\nsns.barplot(x='Embarked',y='Survived',data=train, ax=ax[2][0])\nax[2][0].set_title(\"Embarked vs Survived rate\")\n\ntrain[\"Embarked\"].value_counts().plot.pie(explode=[0, 0.02, 0.02],autopct='%1.1f%%', \n                                              labels=train[\"Embarked\"].value_counts().index, ax=ax[2][1])\nax[2][1].set_title(\"Embarked place proportion\")","35a0198f":"# Creating the feature family_size\ntrain[\"family_size\"] = train[\"SibSp\"] + train[\"Parch\"]\ntrain.head(5)\n\n# Replicating to joined data frame\njoin_df[\"family_size\"] = join_df[\"SibSp\"] + join_df[\"Parch\"]","1888a59f":"f,ax=plt.subplots(3,2,figsize=(15,15))\n\nsns.countplot('SibSp',hue='Survived',data=train, ax=ax[0][0])\nax[0][0].set_title(\"Survived by siblings\/spouses quantity\")\nsns.barplot(x='SibSp',y='Survived',data=train, ax=ax[0][1])\nax[0][1].set_title(\"Survived by siblings\/spouses rate\")\n\nsns.countplot('Parch',hue='Survived',data=train, ax=ax[1][0])\nax[1][0].set_title(\"Survived by parents\/children quantity\")\nsns.barplot(x='Parch',y='Survived',data=train, ax=ax[1][1])\nax[1][1].set_title(\"Survived by parents\/children rate\")\n\nsns.countplot('family_size',hue='Survived',data=train, ax=ax[2][0])\nax[2][0].set_title(\"Survived by family size\")\nsns.barplot(x='family_size',y='Survived',data=train, ax=ax[2][1])\nax[2][1].set_title(\"Survived by family size rate\")\n\nplt.subplots_adjust(wspace=0.2,hspace=0.5)\n","8f8dda61":"# Getting the correlation between features\nsns.heatmap(train.corr(),annot=True,cmap='RdYlGn',linewidths=0.2) \n# Get current figure\nfig=plt.gcf()\nfig.set_size_inches(12,8)\nplt.show()\n","953f460f":"# Converting Sex into numerical values\ntrain[\"Sex\"].replace([\"male\", \"female\"], [0, 1], inplace=True)\n# Converting the embark place into numerics labels\ntrain[\"Embarked\"].replace([\"S\", \"C\", \"Q\"], [0, 1, 2], inplace=True)","a9180db9":"# Replicating to joined data frame\njoin_df[\"Sex\"].replace([\"male\", \"female\"], [0, 1], inplace=True)\n\n# Replicating to joined data frame\njoin_df[\"Embarked\"].replace([\"S\", \"C\", \"Q\"], [0, 1, 2], inplace=True)","731ad07a":"\n# Transforming age by range\n# Using as base to our range of ages\nplt.figure(figsize=(25,6))\nsns.barplot(train['Age'],train['Survived'], ci=None)\nplt.xticks(rotation=90);\nplt.show()\n\n# Creating new field\ntrain[\"Age_Range\"] = 0\ntrain.loc[train[\"Age\"]<=15, \"Age_Range\"] = 0\ntrain.loc[(train[\"Age\"]>15)&(train[\"Age\"]<=35), \"Age_Range\"] = 1\ntrain.loc[(train[\"Age\"]>35)&(train[\"Age\"]<=55), \"Age_Range\"] = 2\ntrain.loc[train[\"Age\"]>55, \"Age_Range\"] = 3","eb7aee50":"# Creating new field\njoin_df[\"Age_Range\"] = 0\njoin_df.loc[join_df[\"Age\"]<=15, \"Age_Range\"] = 0\njoin_df.loc[(join_df[\"Age\"]>15)&(join_df[\"Age\"]<=35), \"Age_Range\"] = 1\njoin_df.loc[(join_df[\"Age\"]>35)&(join_df[\"Age\"]<=55), \"Age_Range\"] = 2\njoin_df.loc[join_df[\"Age\"]>55, \"Age_Range\"] = 3","b8c8268f":"# Dropping features\ntrain.drop(columns=[\"PassengerId\", \"Name\", \"Ticket\", \"Cabin\", \"Fare\", \"Parch\", \"SibSp\", \"Age\"], inplace=True)\njoin_df.drop(columns=[\"PassengerId\", \"Name\", \"Ticket\", \"Cabin\", \"Fare\", \"Parch\", \"SibSp\", \"Age\"], inplace=True)","39bac553":"# Changing the columns names\ntrain.columns = [\"survived\", \"p_class\", \"sex\", \"embarked\", \"family_size\", \"age_range\"]\njoin_df.columns = [\"p_class\", \"sex\", \"embarked\", \"family_size\", \"age_range\"]","d44b6ab4":"# Getting the correlation between features\nsns.heatmap(train.corr(),annot=True,cmap='RdYlGn',linewidths=0.2) \n\n# Get current figure\nfig=plt.gcf()\nfig.set_size_inches(12,8)\nplt.show()","796781d9":"from sklearn.linear_model import LogisticRegression \nfrom sklearn.ensemble import RandomForestClassifier \nfrom sklearn.naive_bayes import GaussianNB \nfrom sklearn.tree import DecisionTreeClassifier \nfrom sklearn.model_selection import train_test_split \nfrom sklearn import metrics\nimport xgboost as xgb\n","99f36416":"# Splitting the data frames again\nprint(\"Test Shape: {}\".format(test.shape))\nprint(\"Train Shape: {}\".format(train.shape))\nprint(\"Merged Shape: {}\".format(join_df.shape))\n\ntest_shape = test.shape[0]\ntrain_shape = train.shape[0]","5add6e2e":"# Target feature\ny = train[\"survived\"]\n\n# Removing the target feature from the remaining dataset\nX = join_df[:train_shape]\ntest = join_df[train_shape:]\n\n# Splitting in test and train\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n\nprint(\"X_train shape: \", X_train.shape)\nprint(\"X_test shape: \", X_test.shape)\nprint(\"y_train shape: \", y_train.shape)\nprint(\"y_test shape: \", y_test.shape)","3ab8ff6a":"# liblinear because its a small dataset\nmodel = LogisticRegression(solver='liblinear')\nmodel.fit(X_train, y_train)\n\nlinear_regression_prediction = model.predict(X_test)\n\nprint('Logistic regression accuracy: ',metrics.accuracy_score(linear_regression_prediction, y_test))","e511db58":"model = RandomForestClassifier(n_estimators=100)\nmodel.fit(X_train, y_train)\n\nrandom_forest_prediction = model.predict(X_test)\n\nprint('Random forest accuracy: ', metrics.accuracy_score(random_forest_prediction, y_test))","7317f12d":"model = DecisionTreeClassifier()\nmodel.fit(X_train, y_train)\n\ndecision_tree_prediction = model.predict(X_test)\n\nprint('Decision tree accuracy: ', metrics.accuracy_score(decision_tree_prediction, y_test))","df6e7def":"model = GaussianNB()\nmodel.fit(X_train, y_train)\n\ngaussian_prediction = model.predict(X_test)\n\nprint('Naive Bayes accuracy: ', metrics.accuracy_score(gaussian_prediction, y_test))","618610b1":"model = xgb.XGBClassifier(n_estimators=100,\n                          n_jobs=4,\n                          learning_rate=0.03,\n                          subsample=0.8,\n                          colsample_bytree=0.8)\n\nmodel.fit(X_train, y_train)\nxgb_prediction = model.predict(X_test)\n\nprint('XGB prediction: ', metrics.accuracy_score(xgb_prediction, y_test))","9017d0b5":"test_passenger_id = pd.read_csv('..\/input\/gender_submission.csv')['PassengerId']\n# We will use the XGB prediction for the submition\nxgb_prediction = model.predict(test)\nsubmission = pd.concat([pd.DataFrame(test_passenger_id), pd.DataFrame({'Survived':xgb_prediction})], axis=1)\nsubmission.to_csv('predictions.csv', index=False)","a8cfa665":"### Class vs Survival Rate\n\nNice! Until now we now that:\n - More females survived.\n - We have some ranges of ages that had more chances of survival.\n \nNow we want to know the behavior of those people. With the feature \"class\" we can know if the ones in the passengers in the first class had more chances of survival (maybe because of the money and status) than the ones in the second and third classes. Or maybe we find a surprise and people from the lower classes survived most.","6ceebd95":"As 72.4% of the passengers embarked in Southampton, we will fill the nulls with 'S'","4348804a":"# Exploring the features\nNow we will understand the relationship of our features against our target feature.","90568e4e":"# Importing data ","8ba00b1a":"### Important notes:\n- We can notice that we have more survivors from 1st class, which turns class an important feature.\n- The pattern that females survived more maintain with classes.\n-  Surprisingly (or not!) the rule of the class survival only exist with the 1st class. Maybe it's because the difference in the quantity of people in second class and third class, or only the class really had preference.","c28bd101":"### Family vs Survived\nTo finish our features understandment, let's check if the family's size mattered to the survival rate.","42514adb":"**Data summary**:\n- sibsp: siblings \/ spouses aboard the Titanic\t\n- parch: parents \/ children aboard the Titanic\n- sibsp + parch = family size","e12d0001":"# Feature engineering\nNow that we done our EDA let's do some feature engineering into our features.","84a5f8a9":"### Logistic Regression","a05812ec":"### Random Forest","a7cdf931":"# Explorating our main feature\n\nAs survive is our \"target feature\" we will do a further exploration into this variable.","20495601":"# Check first informations about the data ","4ce72fb4":"### Naive Bayes","fb39728d":"We have a lot of NULL values (if we look at the beginning of the kernel, there are 177 null values to Age) and some number with .5.\nLet's treat our float numbers turning them into integer and fill our nulls.","b9b1922d":"### Embarked vs Survival Rate\nWhat if the place they embarked also influences the survival rate? Can we find something useful?\n\nData summary:\n- C = Cherbourg\n- Q = Queenstown\n- S = Southampton","9e51fd21":"# Import libraries and check directory","5a65f3c2":"### Sex vs Survival Rate\nThe first feature we will analyze is the \"Sex\" feature.\n\nWhich sex had more survivors?","b6b9d6b6":"## Models\nNow that we have our features the way we want, let's create a few models to see which one will perform better","9dd38f7b":"Looking at the graphics we can realize that most of the survivors were female. Perhaps the story that children and women first land is true, now let's see if this applies for age too.","7f8cccd3":"## Concat data frames\nBefore go ahead we will concat the train and test data frames, this way both of them will have the same features changes","0f75b557":"### Importants notes:\n- One interesting point in these graphics is that families with 1-3 people had more chance of survive.\n- So we will consider family size an important feature for our algorithm and ignore the SibSp and Parch features","f294d83a":"### Importants notes:\n- If we join the information that we gathered in the analysis of the classes with the information of the graphs above, we can infere that people from Queenstown had less chance of survival.\n- The relation of the class with the place turn the place of embark important","af90e34b":"### XGBoost","0016f4d5":"With the barplot we can have a clear vision of the ages that had more chances of survival:\n\n- Children up to 15 years old\n- Some adults from 28 years old to 35 years\n- Some older ages like 48 to 53 years old and 63 years old\n\n","e44507bb":"Looking at our pie chart we can notice that most passengers did not survived. But we want to know more about them, some questions comes up like 'Which type of passenger survived?', 'Which type of class did they buy?',  'Where they embarked?'.","1ef798d0":"# Submission","9b868c0c":"### Age vs Survival Rate","3ae2c266":"### Decision Tree","ae6278a6":"## Main observations\n- Females had more chances of survival than males.\n- People in the range of 5-14 years (kids) had more chance of survival.\n- Even that Southampton has more people embarked, more people from Cherbourg survived.\n- People from 1st class probably had priority and consequently more chances of survival.\n- Family of size 1-3 had more chances of survival."}}