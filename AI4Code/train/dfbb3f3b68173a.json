{"cell_type":{"6aeade92":"code","66dc8eb8":"code","98d4833f":"code","5b01f2ee":"code","fa34921d":"code","1015be30":"code","da29d103":"code","2781875d":"code","8d45af80":"code","a7513ae4":"code","0d43110f":"code","95b014dd":"code","f70546aa":"code","bef4cb7f":"code","46f09e05":"code","afd76872":"code","3f46eb24":"code","fe86c3e8":"code","d18c39eb":"code","e06e3e55":"code","757173c5":"markdown","b9918925":"markdown","cc06b344":"markdown","44cffa29":"markdown","3c59b58e":"markdown","eef1c593":"markdown","1b365445":"markdown","11f754b5":"markdown"},"source":{"6aeade92":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport time\nimport pickle\nimport re \nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nfrom wordcloud import WordCloud\nfrom sklearn import metrics\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\nfrom sklearn.linear_model import LogisticRegression\n\nimport sys\nif not sys.warnoptions:\n    import warnings\n    warnings.simplefilter(\"ignore\")\n%matplotlib inline","66dc8eb8":"DATASET_COLUMNS  = [\"sentiment\", \"ids\", \"date\", \"flag\", \"user\", \"text\"]\n\ndataset = pd.read_csv('\/kaggle\/input\/sentiment140\/training.1600000.processed.noemoticon.csv', encoding=\"ISO-8859-1\", names = DATASET_COLUMNS)","98d4833f":"dataset.head()\n","5b01f2ee":"dataset.info()","fa34921d":"dataset['sentiment'].unique()","1015be30":"dataset = dataset[['sentiment', 'text']]\ndataset['sentiment'] = dataset['sentiment'].replace(4,1)","da29d103":"dataset['sentiment'].unique()","2781875d":"\"\"\"\nIf stop words and word are not installed, comment lines can be downloaded by running.\n\"\"\"\n#stop_words = nltk.download('stopwords')\n#word_net = nltk.download('wordnet')\n\n\nemojis = {':)': 'smile', ':-)': 'smile', ';d': 'wink', ':-E': 'vampire', ':(': 'sad', \n          ':-(': 'sad', ':-<': 'sad', ':P': 'raspberry', ':O': 'surprised',\n          ':-@': 'shocked', ':@': 'shocked',':-$': 'confused', ':\\\\': 'annoyed', \n          ':#': 'mute', ':X': 'mute', ':^)': 'smile', ':-&': 'confused', '$_$': 'greedy',\n          '@@': 'eyeroll', ':-!': 'confused', ':-D': 'smile', ':-0': 'yell', 'O.o': 'confused',\n          '<(-_-)>': 'robot', 'd[-_-]b': 'dj', \":'-)\": 'sadsmile', ';)': 'wink', \n          ';-)': 'wink', 'O:-)': 'angel','O*-)': 'angel','(:-D': 'gossip', '=^.^=': 'cat'}\n\nps = PorterStemmer()\n\ntext, sentiment = list(dataset['text']), list(dataset['sentiment'])","8d45af80":"def preprocess(data_text):\n    processed_text = []\n    \n    word_lem = nltk.WordNetLemmatizer()\n    \n    url_pattern = r\"((http:\/\/)[^ ]*|(https:\/\/)[^ ]*|( www\\.)[^ ]*)\"\n    user_pattern = '@[^\\s]+'\n    alpha_pattern = \"[^a-zA-Z0-9]\"\n    sequence_pattern = r\"(.)\\1\\1+\"\n    seq_replace_pattern = r\"\\1\\1\"\n    \n    for tweet in data_text:\n        tweet = tweet.lower()\n        \n        tweet = re.sub(url_pattern, ' ', tweet)\n        \n        for emoji in emojis.keys():\n            tweet = tweet.replace(emoji, \"EMOJI\" + emojis[emoji])\n            \n        tweet = re.sub(user_pattern, \" \", tweet)\n        \n        tweet = re.sub(alpha_pattern, \" \", tweet)\n\n        tweet = re.sub(sequence_pattern, seq_replace_pattern, tweet)\n\n        tweet_words = ''\n\n        for word in tweet.split():\n            if word not in nltk.corpus.stopwords.words('english'):\n                if len(word) > 1:\n                    word = word_lem.lemmatize(word)\n                    tweet_words += (word + ' ')\n        processed_text.append(tweet_words)\n      \n    return processed_text","a7513ae4":"t = time.time()\nprocessed_text = preprocess(text)\nprint(f'Text Preprocessing complete.')\nprint(f'Time Taken: {round(time.time()-t)} seconds')","0d43110f":"processed_text[0:25]","95b014dd":"data_pos = processed_text[800000:]\nwc = WordCloud(max_words = 300000,background_color ='white', width = 1920 , height = 1080,\n              collocations=False).generate(\" \".join(data_pos))\nplt.figure(figsize = (40,40))\nplt.imshow(wc)","f70546aa":"data_pos = processed_text[:800000]\nwc = WordCloud(max_words = 300000,background_color ='white', width = 1920 , height = 1080,\n              collocations=False).generate(\" \".join(data_pos))\nplt.figure(figsize = (40,40))\nplt.imshow(wc)","bef4cb7f":"X_train, X_test, y_train, y_test = train_test_split(processed_text, sentiment, test_size = 0.05, random_state = 0)","46f09e05":"vectoriser = TfidfVectorizer(ngram_range=(1,2), max_features = 500000)\nvectoriser.fit(X_train)","afd76872":"X_train = vectoriser.transform(X_train)\nX_test = vectoriser.transform(X_test)","3f46eb24":"def model_evaluate(model):\n    y_pred = model.predict(X_test)\n    print(classification_report(y_test, y_pred))\n    cm = confusion_matrix(y_test, y_pred)\n    \n    categories = ['Negative', 'Positive']\n    group_names = ['True Negative', 'False Positive', 'False Negative', 'True Positive']\n    group_percentages = ['{0:.2%}'.format(value) for value in cm.flatten() \/ np.sum(cm)] \n    labels = [f'{v1}\\n{v2}' for v1, v2 in zip(group_names,group_percentages)]\n    labels = np.asarray(labels).reshape(2,2)\n    \n    sns.heatmap(cm, annot = labels, cmap = 'Blues',fmt = '',\n                xticklabels = categories, yticklabels = categories)\n\n    plt.xlabel(\"Predicted values\", fontdict = {'size':14}, labelpad = 10)\n    plt.ylabel(\"Actual values\"   , fontdict = {'size':14}, labelpad = 10)\n    plt.title (\"Confusion Matrix\", fontdict = {'size':18}, pad = 20)","fe86c3e8":"t = time.time()\nmodel = LogisticRegression()\nmodel.fit(X_train, y_train)\nmodel_evaluate(model)\nprint(f'Logistic Regression complete.')\nprint(f'Time Taken: {round(time.time()-t)} seconds')","d18c39eb":"file = open('vectoriser-ngram-(1,2).pickle','wb')\npickle.dump(vectoriser, file)\nfile.close()\n\nfile = open('sentiment_logistic.pickle','wb')\npickle.dump(model, file)\nfile.close()","e06e3e55":"if __name__==\"__main__\":\n    \n    def load_models():\n\n        file = open('vectoriser-ngram-(1,2).pickle', 'rb')\n        vectoriser = pickle.load(file)\n        file.close()\n\n        file = open('sentiment_logistic.pickle', 'rb')\n        log_model = pickle.load(file)\n        file.close()\n\n        return vectoriser, log_model\n    \n    def predict(vectoriser, model, text):\n\n        textdata = vectoriser.transform(preprocess(text))\n        sentiment = model.predict(textdata)\n\n        data = []\n        for text, pred in zip(text, sentiment):\n            data.append((text,pred))\n\n        df = pd.DataFrame(data, columns = ['text','sentiment'])\n        df = df.replace([0,1], [\"Negative\",\"Positive\"])\n        return df\n\n    vectoriser, log_model = load_models()\n    \n    text = [\"Data science is a very enjoyable job.\",\n            \"Twitter is unnecessary.\",\n            \"I dont feel good.\"]\n    \n    df = predict(vectoriser, log_model, text)\n    print(df.head())","757173c5":"### Let's do a few tries.\n","b9918925":"A small function for easier and easier use of the model. It can make your work quite easy when using more than one model.","cc06b344":"**TfidfVectorizer**\n\n* The goal of using tf-idf is to scale down the impact of tokens that occur very frequently in a given corpus and that are hence empirically less informative than features that occur in a small fraction of the training corpus.","44cffa29":"## Some sample tweets.","3c59b58e":"### Modelling","eef1c593":"The most important part in sentiment analysis is the pre-processing part.","1b365445":"### Save the model.","11f754b5":"This process may take a long time."}}