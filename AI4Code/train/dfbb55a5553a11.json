{"cell_type":{"149dc777":"code","4725866a":"code","a4d42938":"code","88b9dc53":"code","3dfc6373":"code","9ec96782":"code","a860a48d":"code","3bbf6803":"code","4b1a00ca":"code","6897b386":"code","7d0e4076":"code","14382ad5":"code","c63c1245":"code","ea38fd07":"code","882436e8":"code","fa4b73ae":"code","18694b06":"code","640b8f2b":"code","2c62c4bb":"code","a3b6edcd":"code","fb7ff8d5":"code","924e070c":"code","26735a28":"code","26130c1f":"code","681c210f":"code","4a8d00f9":"code","90eaa95b":"code","ded71e88":"code","c23adbd6":"code","9d64a47e":"code","bda435e0":"markdown","b649ff71":"markdown","f9f6f0ab":"markdown"},"source":{"149dc777":"!pip -q install timm\n!pip install -U albumentations -q","4725866a":"from __future__ import print_function\n\nimport glob\nfrom itertools import chain\nimport os\nimport random\nimport zipfile\nimport copy\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom PIL import Image\nfrom sklearn.model_selection import train_test_split\nfrom torch.optim.lr_scheduler import StepLR\nfrom torch.utils.data import DataLoader, Dataset\nfrom torchvision import datasets, transforms\nfrom tqdm.notebook import tqdm\n\nimport timm\n\nimport albumentations as A\n\n%config InlineBackend.figure_format = 'retina'","a4d42938":"print(f\"Torch: {torch.__version__}\")","88b9dc53":"# Training settings\nbatch_size = 8\nepochs = 2\nlr = 3e-5\ngamma = 0.7\nseed = 42","3dfc6373":"def seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n\nseed_everything(seed)","9ec96782":"import torchvision\nfrom torchvision.transforms import ToTensor\nfrom torchvision import transforms\n\ntrain_data = torchvision.datasets.ImageFolder('..\/input\/emsig-train-valid-split\/ESIG-2\/train', transform=ToTensor(),\n)\nvalid_data = torchvision.datasets.ImageFolder('..\/input\/emsig2021\/train_class', transform=transforms.Compose([\n    transforms.Resize((384, 384), interpolation=3),\n    transforms.ToTensor()\n]))\ntest_data = torchvision.datasets.ImageFolder('..\/input\/emsig-test-crop-2', transform=transforms.Compose([\n    transforms.Resize((384, 384), interpolation=3),\n    transforms.ToTensor()\n]))","a860a48d":"from torchvision.datasets.folder import *\nfrom matplotlib import pyplot as plt\nimport cv2\nimport numpy as np\nimport torch.utils.data as Data\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\n\nclass MyFolder(DatasetFolder):\n\n    def __init__(self,root: str,transform = None,target_transform = None,loader = default_loader,is_valid_file= None):\n        super(MyFolder, self).__init__(root, loader, IMG_EXTENSIONS if is_valid_file is None else None,\n                                          transform=transform,\n                                          target_transform=target_transform,\n                                          is_valid_file=is_valid_file)\n        self.imgs = self.samples\n\n    def __getitem__(self, index: int):\n        \"\"\"\n        Args:\n            index (int): Index\n\n        Returns:\n            tuple: (sample, target) where target is class_index of the target class.\n        \"\"\"\n        path, target = self.samples[index]\n        # sample = self.loader(path)\n        sample = cv2.imread(path)\n        sample = cv2.cvtColor(sample, cv2.COLOR_BGR2RGB)\n        if self.transform is not None:\n            sample = self.transform(image=sample)[\"image\"]\n        if self.target_transform is not None:\n            target = self.target_transform(target)\n            target = target.float()\n\n        return sample, target\n\n\ntrain_transform = A.Compose(\n    [\n        A.Resize(384, 384),\n        A.CoarseDropout(max_holes=8, max_height=32, max_width=32, min_holes=1, min_height=None, min_width=None, fill_value=0, mask_fill_value=None, always_apply=False, p=0.5),\n        A.GaussianBlur(blur_limit=(3, 7), sigma_limit=0, always_apply=False, p=0.5),\n        A.GaussNoise(var_limit=(10.0, 100.0), mean=0, per_channel=True, always_apply=False, p=0.5),\n        A.ISONoise(color_shift=(0.01, 0.1), intensity=(0.1, 0.5), always_apply=False, p=0.2),\n        A.Sharpen(alpha=(0.2, 0.5), lightness=(0.5, 1.0), always_apply=False, p=0.5),\n        A.VerticalFlip(p=0.2),\n        ToTensorV2(),\n    ]\n)\ndata_dir_train = \"..\/input\/emsig2021\/train_class\"\n# train_dataset = MyFolder(data_dir_train, transform=train_transform)\n# train_loader = Data.DataLoader(\n#         dataset=train_dataset,\n#         num_workers=1,\n#         batch_size=9,\n#         shuffle=True\n#     )","3bbf6803":"def visualize_augmentations(dataset, idx=0, samples=10, cols=5):\n    dataset = copy.deepcopy(dataset)\n    dataset.transform = A.Compose([t for t in dataset.transform if not isinstance(t, (A.Normalize, ToTensorV2))])\n    rows = samples \/\/ cols\n    figure, ax = plt.subplots(nrows=rows, ncols=cols, figsize=(12, 6))\n    for i in range(samples):\n        image, _ = dataset[idx]\n        ax.ravel()[i].imshow(image)\n        ax.ravel()[i].set_axis_off()\n    plt.tight_layout()\n    plt.show()","4b1a00ca":"# visualize_augmentations(train_dataset)","6897b386":"import torch.utils.data as data\nfrom torch.autograd import Variable\nimport numpy as np\n\ntrain_loader = data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\nvalid_loader = data.DataLoader(valid_data, batch_size=batch_size, shuffle=True)\ntest_loader  = data.DataLoader(test_data, batch_size=batch_size, shuffle=False) ","7d0e4076":"class ImageFolderWithPaths(torchvision.datasets.ImageFolder):\n    \"\"\"Custom dataset that includes image file paths. Extends\n    torchvision.datasets.ImageFolder\n    \"\"\"\n\n    # override the __getitem__ method. this is the method that dataloader calls\n    def __getitem__(self, index):\n        # this is what ImageFolder normally returns \n        original_tuple = super(ImageFolderWithPaths, self).__getitem__(index)\n        # the image file path\n        path = self.imgs[index][0]\n        # make a new tuple that includes original and the path\n        tuple_with_path = (original_tuple + (path,))\n        return tuple_with_path","14382ad5":"test_data_path = ImageFolderWithPaths('..\/input\/emsig-test', transform=transforms.Compose([\n    transforms.Resize((384, 384), interpolation=3),\n    transforms.ToTensor()\n]))","c63c1245":"test_loader_path  = data.DataLoader(test_data_path, batch_size=batch_size, shuffle=False) ","ea38fd07":"print(len(train_data), len(train_loader))\nprint(len(valid_data), len(valid_loader))\nprint(len(test_data), len(test_loader))","882436e8":"from pprint import pprint\nmodel_names = timm.list_models(pretrained=True)\npprint(model_names)","fa4b73ae":"device = 'cuda'\n\nmodel = timm.create_model('vit_base_r50_s16_384', pretrained=True, num_classes=6).to(device)","18694b06":"# loss function\ncriterion = nn.CrossEntropyLoss()\n# optimizer\noptimizer = optim.Adam(model.parameters(), lr=lr)\n# scheduler\nscheduler = StepLR(optimizer, step_size=1, gamma=gamma)\n\nn_epochs_stop = 1\n\nmin_val_loss = 100","640b8f2b":"epoch_l = []\nloss_l = []\nacc_l = []\nv_loss_l = []\nv_acc_l = []","2c62c4bb":"for epoch in range(epochs):\n    epoch_loss = 0\n    epoch_accuracy = 0\n\n    for data, label in tqdm(train_loader):\n        data = data.float()\n        data = data.to(device)\n        label = label.to(device)\n\n        output = model(data)\n        loss = criterion(output, label)\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        acc = (output.argmax(dim=1) == label).float().mean()\n        epoch_accuracy += acc \/ len(train_loader)\n        epoch_loss += loss \/ len(train_loader)\n\n    with torch.no_grad():\n        epoch_val_accuracy = 0\n        epoch_val_loss = 0\n        for data, label in valid_loader:\n            data = data.float()\n            data = data.to(device)\n            label = label.to(device)\n\n            val_output = model(data)\n            val_loss = criterion(val_output, label)\n\n            acc = (val_output.argmax(dim=1) == label).float().mean() \n            epoch_val_accuracy += acc \/ len(valid_loader)\n            epoch_val_loss += val_loss \/ len(valid_loader)\n        \n        epoch_l.append(epoch+1)\n        loss_l.append(epoch_loss)\n        acc_l.append(epoch_accuracy)\n        v_loss_l.append(epoch_val_loss)\n        v_acc_l.append(epoch_val_accuracy)\n        \n        print(f\"Epoch : {epoch+1} - loss : {epoch_loss:.4f} - acc: {epoch_accuracy:.4f} - val_loss : {epoch_val_loss:.4f} - val_acc: {epoch_val_accuracy:.4f}\\n\")\n        \n        if epoch_val_loss < min_val_loss:\n            #Saving the model\n            best_model = copy.deepcopy(model.state_dict())\n            epochs_no_improve = 0\n            min_val_loss = epoch_val_loss\n            early_stoped = False\n\n        else:\n            epochs_no_improve += 1\n            # Check early stopping condition\n            if epochs_no_improve == n_epochs_stop:\n                print('Early stopping!' )\n                model.load_state_dict(best_model)\n                early_stoped = True\n                break\n    if early_stoped:\n        break\n\n    ","a3b6edcd":"torch.save(model, '.\/cait_model_pretrained.pt')","fb7ff8d5":"y_pred_list = []\ny_train_list = []\nall_path=[]\n# y_true_list = []\nwith torch.no_grad():\n    for x_batch, y_batch, path in tqdm(test_loader_path): #test_loader_path\n        x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n        y_test_pred = model(x_batch)\n        _, y_pred_tag = torch.max(y_test_pred, dim = 1)\n        y_pred_list.append(y_pred_tag.cpu().numpy())\n        all_path.append(path)\n#         y_true_list.append(y_batch.cpu().numpy())","924e070c":"def flatten(new:list, target:list):\n    for li in target:\n        for value in list(li):\n            new.append(value)\n\ny_pred = []\n# y_true = []\nflatten(y_pred, y_pred_list)\n# flatten(y_true, y_true_list)","26735a28":"case = []\nfor batches in all_path:\n    for sample in batches:\n        case.append(sample[22:][:-4])","26130c1f":"sub = pd.DataFrame(\n    {'case': case,\n     'class': y_pred\n    })\nsub","681c210f":"sub.to_csv('sub_vitbase.csv')","4a8d00f9":"# from sklearn.metrics import accuracy_score, f1_score\n# print(\"Overall accuracy:\", accuracy_score(y_true, y_pred))\n# print(\"Overall F1:\", f1_score(y_true, y_pred, average='weighted'))","90eaa95b":"# from sklearn.metrics import precision_recall_fscore_support as score\n\n# precision, recall, fscore, support = score(y_true, y_pred)\n\n# print('precision: {}'.format(precision))\n# print('recall: {}'.format(recall))\n# print('fscore: {}'.format(fscore))\n# print('support: {}'.format(support))","ded71e88":"# from sklearn.metrics import confusion_matrix\n# import numpy as np\n# import pandas as pd\n# import seaborn as sns\n\n# def plot_cm(y_true, y_pred, figsize=(10,9)):\n#     cm = confusion_matrix(y_true, y_pred, labels=np.unique(y_true))\n#     cm_sum = np.sum(cm, axis=1, keepdims=True)\n#     cm_perc = cm \/ cm_sum.astype(float) * 100\n#     annot = np.empty_like(cm).astype(str)\n#     nrows, ncols = cm.shape\n#     for i in range(nrows):\n#         for j in range(ncols):\n#             c = cm[i, j]\n#             p = cm_perc[i, j]\n#             if i == j:\n#                 s = cm_sum[i]\n#                 annot[i, j] = '%.1f%%\\n%d\/%d' % (p, c, s)\n#             elif c == 0:\n#                 annot[i, j] = ''\n#             else:\n#                 annot[i, j] = '%.1f%%\\n%d' % (p, c)\n#     cm = pd.DataFrame(cm, index=np.unique(y_true), columns=np.unique(y_true))\n#     cm.index.name = 'Actual'\n#     cm.columns.name = 'Predicted'\n#     fig, ax = plt.subplots(figsize=figsize)\n#     sns.heatmap(cm, cmap= \"YlGnBu\", annot=annot, fmt='', ax=ax)\n\n# plot_cm(y_true, y_pred)\n\n# display()","c23adbd6":"loss_l_c = []\nacc_l_c = []\nv_loss_l_c = []\nv_acc_l_c = []\nfor x in loss_l:\n    x = x.cpu().detach().numpy()\n    loss_l_c.append(x)\nfor x in acc_l:\n    x = x.cpu().detach().numpy()\n    acc_l_c.append(x)\nfor x in v_loss_l:\n    x = x.cpu().detach().numpy()\n    v_loss_l_c.append(x)\nfor x in v_acc_l:\n    x = x.cpu().detach().numpy()\n    v_acc_l_c.append(x)","9d64a47e":"import plotly.graph_objects as go\n\n# Create traces\nfig = go.Figure()\nfig.add_trace(go.Scatter(x=epoch_l, y=loss_l_c,\n                    mode='lines+markers',\n                    name='Train loss'))\nfig.add_trace(go.Scatter(x=epoch_l, y=acc_l_c,\n                    mode='lines+markers',\n                    name='Train accuracy'))\nfig.add_trace(go.Scatter(x=epoch_l, y=v_loss_l_c,\n                    mode='lines+markers',\n                    name='Validation loss'))\nfig.add_trace(go.Scatter(x=epoch_l, y=v_acc_l_c,\n                    mode='lines+markers',\n                    name='Validation accuracy'))\nfig.update_layout(\n    title='ConViT(pre-trained)',\n    autosize=False,\n    width=1000,\n    height=600,\n)\n\nfig.show()","bda435e0":"### Visual Transformer","b649ff71":"## Effecient Attention","f9f6f0ab":"### Training"}}