{"cell_type":{"95a3957b":"code","b079436e":"code","8fda0e86":"code","ceecbe5a":"code","647d2a40":"code","bcf9ec8b":"code","b75e20cc":"code","5cf295eb":"code","64cb6aef":"code","4748c678":"code","4e83edb6":"code","e1c9d920":"code","3fdb9961":"code","af31b1a8":"code","f88eed0b":"code","4a2c57a1":"code","6e513a7e":"code","f0717360":"code","6bb372bd":"code","3570f593":"code","25d3c21c":"code","42c28359":"code","f66056b1":"code","bbf93851":"code","17aa9971":"code","f9365e12":"code","9b17b80b":"markdown","15441add":"markdown","17eb8f15":"markdown","97738b5b":"markdown","49a9c721":"markdown","cb1c849e":"markdown","f941c4f7":"markdown","d41f0ca6":"markdown","fda0565a":"markdown","65a5dd55":"markdown","17fe7b80":"markdown","9fc9ddb0":"markdown"},"source":{"95a3957b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.\n\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nfrom sklearn.metrics import roc_curve, auc\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import metrics\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import KFold","b079436e":"parkinson_df = pd.read_csv('..\/input\/parkinsons2.csv')","8fda0e86":"parkinson_df.head().transpose()","ceecbe5a":"parkinson_df.columns","647d2a40":"#Since column names are big it will be easy to do plots and calculations if the column names are small\nparkinson_df.columns = ['Fo','Fhi','Flo','Jitter(%)','Jitter(Abs)','RAP','PPQ','DDP','Shimmer','Shimmer(dB)','APQ3','APQ5','APQ','DDA','NHR','HNR','RPDE','DFA','spread1','spread2','D2','PPE','status']","bcf9ec8b":"parkinson_df.info()","b75e20cc":"parkinson_df.describe().transpose()","5cf295eb":"parkinson_df[parkinson_df.isnull().any(axis=1)]","64cb6aef":"parkinson_df.boxplot(figsize=(24,8))","4748c678":"parkinson_df.corr()","4e83edb6":"parkinson_df['status'].value_counts().sort_index()","e1c9d920":"X = parkinson_df.drop(['Fhi','NHR','status'],axis=1)\nY = parkinson_df['status']","3fdb9961":"#Splitting the data into train and test in 70\/30 ratio with random state as 2.\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=2)","af31b1a8":"LR = LogisticRegression()\nLR.fit(X_train, Y_train)","f88eed0b":"Y1_predict = LR.predict(X_test)\nY1_predict","4a2c57a1":"Y_acc = metrics.accuracy_score(Y_test,Y1_predict)\nprint(\"Accuracy of the model is {0:2f}\".format(Y_acc*100))\nY_cm=metrics.confusion_matrix(Y_test,Y1_predict)\nprint(Y_cm)","6e513a7e":"#Sensitivity\nTPR=Y_cm[1,1]\/(Y_cm[1,0]+Y_cm[1,1])\nprint(\"Sensitivity of the model is {0:2f}\".format(TPR))","f0717360":"#Specificity\nTNR=Y_cm[0,0]\/(Y_cm[0,0]+Y_cm[0,1])\nprint(\"Specificity of the model is {0:2f}\".format(TNR))","6bb372bd":"Y_CR=metrics.classification_report(Y_test,Y1_predict)\nprint(Y_CR)","3570f593":"fpr,tpr, _ = roc_curve(Y_test, Y1_predict)\nroc_auc = auc(fpr, tpr)\n\nprint(\"Area under the curve for the given model is {0:2f}\".format(roc_auc))\nplt.figure()\nplt.plot(fpr, tpr)\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.0])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic')\nplt.show()","25d3c21c":"X = parkinson_df.drop(['Fhi','NHR','status'],axis=1)\nY = parkinson_df['status']","42c28359":"# K-fold cross validation for the given model:\n#Since the dataset contains 197 rows, we are taking the number of splits as 3\nkf=KFold(n_splits=3,shuffle=True,random_state=2)\nacc=[]\nfor train,test in kf.split(X,Y):\n    M=LogisticRegression()\n    Xtrain,Xtest=X.iloc[train,:],X.iloc[test,:]\n    Ytrain,Ytest=Y[train],Y[test]\n    M.fit(Xtrain,Ytrain)\n    Y_predict=M.predict(Xtest)\n    acc.append(metrics.accuracy_score(Ytest,Y_predict))\n    print(metrics.confusion_matrix(Ytest,Y_predict))\n    print(metrics.classification_report(Ytest,Y_predict))\nprint(\"Cross-validated Score:{0:2f} \".format(np.mean(acc)))","f66056b1":"#Accuracy for each fold\nacc","bbf93851":"#Error\nerror=1-np.array(acc)\nerror","17aa9971":"# Variance Error of the model\nnp.var(error,ddof=1)","f9365e12":"fpr,tpr, _ = roc_curve(Ytest, Y_predict)\nroc_auc = auc(fpr, tpr)\n\nprint(\"Area under the curve for the given model is {0:2f}\".format(roc_auc))\nplt.figure()\nplt.plot(fpr, tpr)\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.0])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic')\nplt.show()","9b17b80b":"#### So, for the above K-fold cross validation model, Precision for each fold is 0.82,0.89,0.88 respectively, Recall for each fold is 0.83,0.88,0.88 respectively and the overall Accuracy is 86.15%","15441add":"#### From the above table,we observe that 'Fhi'(Maximum vocal fundamental frequency) and 'NHR'(Measure of ratio of noise to tonal components in the voice) are having less correlation(-0.166136 and 0.189429 respectively) with respect to status . So, we can drop these two features based on their correlations.","17eb8f15":"#### From the above information, we observe that all the given features are continuous except 'status'(since given in the description).","97738b5b":"We observe that there are 195 rows and 23 columns in the given dataset.","49a9c721":"#### So, for the above model, Accuracy is 81.35% , Sensitivity is 93.61% and Specificity is 33.33%","cb1c849e":"#### Area under the curve is 0.6347 implies it is a good model.","f941c4f7":"#### From the above box plots, we observe that there are less outliers. So, the model will not be affected by the outliers.","d41f0ca6":"#### Area under the curve is 0.8416 implies it is a very good model.","fda0565a":"#### Description of the columns:\nMDVP:Fo(Hz) - Average vocal fundamental frequency \n\nMDVP:Fhi(Hz) - Maximum vocal fundamental frequency \n\nMDVP:Flo(Hz) - Minimum vocal fundamental frequency \n\nMDVP:Jitter(%),MDVP:Jitter(Abs),MDVP:RAP,MDVP:PPQ,Jitter:DDP - Several measures of variation in fundamental frequency \n\nMDVP:Shimmer,MDVP:Shimmer(dB),Shimmer:APQ3,Shimmer:APQ5,MDVP:APQ,Shimmer:DDA - Several measures of variation in amplitude \n\nNHR,HNR - Two measures of ratio of noise to tonal components in the voice \n\nRPDE,D2 - Two nonlinear dynamical complexity measures \n\nDFA - Signal fractal scaling exponent \n\nspread1,spread2,PPE - Three nonlinear measures of fundamental frequency variation \n\nstatus - Health status of the subject (one) - Parkinson's, (zero) - healthy","65a5dd55":"#### From the above two cells,we observe that there are no missing values in the given dataset.","17fe7b80":"#### Most are having Parkinson disease. The ratio is almost 1:3 in favor of status 1. So, the model's ability to predict status 1 will be better than predicting status 0.","9fc9ddb0":"### By Comparing the above two models, we observe that by doing K-fold cross validation, accuracy has been improved from 81.35% to 86.15% and area under the curve has been improved from 0.6347 to 0.8416. So, we can conclude that K-fold cross validation model will be the better model for this dataset."}}