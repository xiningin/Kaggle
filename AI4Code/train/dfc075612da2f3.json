{"cell_type":{"d9b96275":"code","d92ede0b":"code","cfcca2f9":"code","1e65fe95":"code","c692bf58":"code","ef3d462f":"code","3db5124e":"code","63511e0e":"code","2c6bfab0":"code","6d92f9ed":"code","0ec82df5":"code","c6154296":"code","076abad5":"code","ad8e976a":"code","43aa0bc0":"code","7eced3e0":"code","3d24c1a9":"code","b37bbe65":"code","c033c5f5":"code","d7927136":"code","69213fee":"code","4a3ea2e5":"code","792a9930":"code","833ff752":"code","b9574992":"code","c8519dfa":"code","4a545c32":"code","62aee1fc":"code","f6945042":"code","a851766e":"code","9ae6bb03":"code","8bd8a87a":"code","a2bfef5f":"code","5de75899":"code","262cc01f":"markdown","f3b70741":"markdown","c5ff3fad":"markdown","3e8a8c3d":"markdown","7a1502d4":"markdown","7c664c61":"markdown","f35f230e":"markdown","ac951d64":"markdown","9648bfa4":"markdown","be7dacf0":"markdown","5bdcf1a6":"markdown","d7058da6":"markdown","55e11b5e":"markdown","15570515":"markdown","32efbe19":"markdown","f4d49ad1":"markdown","a5a35db6":"markdown","38bfdfea":"markdown","a532a1b9":"markdown","0976bd05":"markdown","457e5ce3":"markdown","9fd36c7e":"markdown","29143463":"markdown"},"source":{"d9b96275":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session\n","d92ede0b":"!pip install orange3","cfcca2f9":"import numpy as np\nimport pandas as pd\n\nimport seaborn as sns\n\nimport os\nos.environ['PROJ_LIB'] = r'C:\\Users\\TOP Artes\\.conda\\pkgs\\proj4-5.2.0-ha925a31_1\\Library\\share'\nfrom mpl_toolkits.basemap import Basemap\nimport matplotlib.pyplot as plt\n# from Orange.evaluation import graph_ranks\n\nsns.set_style('whitegrid')\n# %matplotlib inline\n\n\nclass Analise(object):\n    \n    \"\"\"\n    Classe respons\u00e1vel pelas impress\u00f5es dos gr\u00e1ficos de an\u00e1lises explorat\u00f3rias dos dados Estaduais e DF\"\"\"\n    \n    def __init__(self):\n        pass\n\n\n    def compara_dispersao(self, df):        \n\n        df_top = df.sort_values(['MEDIA_NOTAS'], ascending=False).head((500))\n        df_top['rank'] = 'TOP'\n        df_down = df[df['MEDIA_NOTAS']>0].sort_values(['MEDIA_NOTAS'], ascending=True).head((500))\n        df_down['rank'] = 'BOTTOM'\n        df_plot = pd.concat([df_top, df_down], axis=0)\n        \n        \n        ax = sns.pairplot(df_plot[['rank','MEDIA_NOTAS','HAB\/ESCOLA','MATRIC\/ESCOLA',\n                                   'DOCENTE\/ESCOLA','MATRICULA\/DOCENTE']], hue='rank')\n        ax.set(ylim=(0, None),xlim=(0, None))\n        ax.fig.suptitle('Gr\u00e1fico de disper\u00e7\u00e3o para compara\u00e7\u00e3o \\n 500 melhores X 500 piores(M\u00e9dia > 0)', y=1.05)             \n        ax.fig.text(s='Fonte: INSTITUTO NACIONAL DE ESTUDOS E PESQUISAS EDUCACIONAIS AN\u00cdSIO TEIXEIRA.\\n'\n'Sinopse Estat\u00edstica da Educa\u00e7\u00e3o B\u00e1sica de 2010 a 2019. Bras\u00edlia: Inep, 2020.\\n'\n'Dispon\u00edvel em: <http:\/\/portal.inep.gov.br\/sinopses-estatisticas-da-educacao-basica>. Acesso em: 05\/06\/2020\\n'\n\n'Fonte: IBGE. Diretoria de Pesquisas - DPE -  Coordena\u00e7\u00e3o de Popula\u00e7\u00e3o e Indicadores Sociais - COPIS.\\n'\n'Dispon\u00edvel em: <http:\/\/www.dados.gov.br\/dataset\/cd-censo-demografico>. Acesso em: 05\/06\/2020\\n'\n    \n'Fonte: IBGE. Diretoria de Pesquisas - DPE -  Coordena\u00e7\u00e3o de Popula\u00e7\u00e3o e Indicadores Sociais - COPIS.\\n'\n'Dispon\u00edvel em: <https:\/\/www.ibge.gov.br\/geociencias\/downloads-geociencias.html>. Acesso em: 05\/06\/2020\\n', x=0.01, y=-0.12, fontsize=9, ha='left', va='bottom')\n        plt.show()\n        \n        return\n    \n    \n\n    def compara_melhores(self, df):\n        \n        df_top = df.sort_values(['MEDIA_NOTAS'], ascending=False).head((500))\n        df_top['rank'] = 'TOP'\n        \n        municipios = df_top['COD. MUNIC'].head(12).values\n\n        width=0.35\n        s = 1\n        plt.figure(figsize=(18,50))\n        ax = plt.subplot()\n        ax.set_ylim(0,25)\n        \n        for i in range(len(municipios)):\n            \n            ax = plt.subplot(len(municipios),3,s, sharey=ax, sharex=ax)\n            \n            if s == 1:        \n            \n                # Define o local e texto da fonte dos dados\n                ax.text(x=1.7, y=1.7, s='Munic\u00edpios com as 12 melhores m\u00e9dias de 2014 a 2018',\n                fontsize=16, ha='center', va='bottom', transform=ax.transAxes)\n                    \n            \n            ax.bar(df[df[\"COD. MUNIC\"]==municipios[i]]['ano'].astype(int) - width\/2,\n                    df[df[\"COD. MUNIC\"]==municipios[i]]['DOCENTE\/ESCOLA'],\n                    label='DOCENTE\/ESCOLA', width=width)\n            \n            media = df[df[\"COD. MUNIC\"]==municipios[i]][\"MEDIA_NOTAS\"].values\n            \n            for j, p in enumerate(ax.patches):\n                ax.annotate(f\"M\u00e9dia nts\\n {format(media[j], '.2f')}\",\n                            (p.get_x() + p.get_width(), 25),\n                            ha = 'center', va = 'center', xytext = (0, 10),\n                            textcoords = 'offset points')\n        \n                ax.annotate(format(p.get_height(), '.0f'),\n                            (p.get_x() + p.get_width() \/ 2.,p.get_height()),\n                            ha = 'center', va = 'center', xytext = (0, 10),\n                            textcoords = 'offset points')\n            \n            \n            ax.bar(df[df[\"COD. MUNIC\"]==municipios[i]]['ano'].astype(int) + width\/2,\n                    df[df[\"COD. MUNIC\"]==municipios[i]]['MATRICULA\/DOCENTE'],\n                    label='MATRICULA\/DOCENTE', width=width)   \n        \n            for p in ax.patches:\n              ax.annotate(format(p.get_height(), '.0f'),\n                         (p.get_x() + p.get_width() \/ 2., p.get_height()),\n                         ha = 'center', va = 'center', xytext = (0, 10),\n                         textcoords = 'offset points')  \n            \n            if s == 3:    \n                ax.legend()\n                # Posiciona a legenda fora do gr\u00e1fico\n                ax.legend(bbox_to_anchor=(.6, 2), loc=2, borderaxespad=0.)\n                \n            if s == 10:                \n                self.get_ref(ax)\n                \n            municipio = df[df[\"COD. MUNIC\"]==municipios[i]][[\"NOME_MUNIC\u00cdPIO\",\"NM_UF_SIGLA\"]].values[0][0] \n            uf = df[df[\"COD. MUNIC\"]==municipios[i]][[\"NOME_MUNIC\u00cdPIO\",\"NM_UF_SIGLA\"]].values[0][1] \n            ax.set_title(f'{municipio} \/ {uf}\\nMediana de 2014 a 2018 - {round(np.median(media),2)}\\n\\n', y=1.05)\n            s+=1\n        plt.subplots_adjust(hspace=.7, wspace=.1)\n        plt.show()\n        \n        return\n\n        \n\n\n    def compara_piores(self, df):\n        \n        \n        df_down = df[df['MEDIA_NOTAS']>0].sort_values(['MEDIA_NOTAS'], ascending=True).head((500))\n        df_down['rank'] = 'BOTTOM'\n \n        municipios = df_down['COD. MUNIC'].head(12).values\n\n        width=0.35\n        s = 1\n        plt.figure(figsize=(18,50))\n        ax = plt.subplot()\n        ax.set_ylim(0,25)\n        for i in range(len(municipios)):\n            \n            ax = plt.subplot(len(municipios),3,s, sharey=ax, sharex=ax)  \n            \n            if s == 1:        \n            \n                # Define o local e texto da fonte dos dados\n                ax.text(x=1.7, y=1.7, s='Munic\u00edpios com as 12 piores m\u00e9dias de 2014 a 2018',\n                fontsize=16, ha='center', va='bottom', transform=ax.transAxes)  \n            \n            ax.bar(df[df[\"COD. MUNIC\"]==municipios[i]]['ano'].astype(int) - width\/2,\n                    df[df[\"COD. MUNIC\"]==municipios[i]]['DOCENTE\/ESCOLA'],\n                    label='DOCENTE\/ESCOLA', width=width)\n            \n            media = df[df[\"COD. MUNIC\"]==municipios[i]][\"MEDIA_NOTAS\"].values\n            \n            for j, p in enumerate(ax.patches):\n                ax.annotate(f\"M\u00e9dia nts\\n {format(media[j], '.2f')}\",\n                            (p.get_x() + p.get_width(), 25),\n                            ha = 'center', va = 'center', xytext = (0, 10),\n                            textcoords = 'offset points')\n        \n                ax.annotate(format(p.get_height(), '.0f'),\n                            (p.get_x() + p.get_width() \/ 2.,p.get_height()),\n                            ha = 'center', va = 'center', xytext = (0, 10),\n                            textcoords = 'offset points')\n        \n            \n            \n            ax.bar(df[df[\"COD. MUNIC\"]==municipios[i]]['ano'].astype(int) + width\/2,\n                    df[df[\"COD. MUNIC\"]==municipios[i]]['MATRICULA\/DOCENTE'],\n                    label='MATRICULA\/DOCENTE', width=width)\n        \n            for p in ax.patches:\n              ax.annotate(format(p.get_height(), '.0f'),\n                         (p.get_x() + p.get_width() \/ 2., p.get_height()),\n                         ha = 'center', va = 'center', xytext = (0, 10),\n                         textcoords = 'offset points')    \n            \n            if s == 3:    \n                ax.legend()\n                # Posiciona a legenda fora do gr\u00e1fico\n                ax.legend(bbox_to_anchor=(.6, 2), loc=2, borderaxespad=0.)                            \n                \n            if s == 10:                \n                self.get_ref(ax)\n                \n            municipio = df[df[\"COD. MUNIC\"]==municipios[i]][[\"NOME_MUNIC\u00cdPIO\",\"NM_UF_SIGLA\"]].values[0][0] \n            uf = df[df[\"COD. MUNIC\"]==municipios[i]][[\"NOME_MUNIC\u00cdPIO\",\"NM_UF_SIGLA\"]].values[0][1] \n            ax.set_title(f'{municipio} \/ {uf}\\nMediana de 2014 a 2018 - {round(np.median(media),2)}\\n\\n', y=1.05)\n            s+=1\n            \n        plt.subplots_adjust(hspace=.7, wspace=.1)\n        plt.show()\n    \n        return\n    \n    \n    \n    def compara_correlacao(self, df):\n        \n        plt.figure(figsize=(20,10))\n        ax = plt.axes()\n        ax.set_title('\\nCorrela\u00e7\u00e3o entre as principais vari\u00e1veis\\n\\n')\n        sns.heatmap(df[df.columns[4:]].corr(), annot=True, ax=ax)\n        self.get_ref(ax, -0.12, -.45)\n        plt.show()\n        \n        return\n    \n    \n    \n    def compara_docente(self, df):     \n        \n        df_rank = self.get_rank(df)\n    \n        medianas_top = df_rank.sort_values(['MEDIA_NOTAS'], ascending=False).head(30)\n        \n        medianas_down = df_rank.sort_values(['MEDIA_NOTAS'], ascending=True).head(30)\n        \n        municipios_top = [df[df['COD. MUNIC'] == cod]['NOME_MUNIC\u00cdPIO'].values[0] + \" \/ \"+ df[df['COD. MUNIC'] == cod]['NM_UF_SIGLA'].values[0]\n                          for cod in medianas_top['COD. MUNIC'].values]\n        \n        municipios_down = [df[df['COD. MUNIC'] == cod]['NOME_MUNIC\u00cdPIO'].values[0] + \" \/ \"+ df[df['COD. MUNIC'] == cod]['NM_UF_SIGLA'].values[0]\n                          for cod in medianas_down['COD. MUNIC'].values]\n        width = 0.35\n        \n        plt.figure(figsize=(20,8))\n        ax = plt.subplot(2,1,1)\n        \n        ax.set_ylim(0,20)\n        \n        ax.bar(municipios_top, medianas_top['DOCENTE\/ESCOLA'], align='edge',\n                            label='DOCENTE\/ESCOLA', width=width)\n        for p in ax.patches:\n          ax.annotate(format(p.get_height(), '.0f'),\n                     (p.get_x() + p.get_width() \/ 2., p.get_height()),\n                     ha = 'center', va = 'center', xytext = (0, 10),\n                     textcoords = 'offset points')  \n        ax.bar(municipios_top, medianas_top['MATRICULA\/DOCENTE'], align='edge',\n                            label='MATRICULA\/DOCENTE', width=-width)\n        for p in ax.patches:\n          ax.annotate(format(p.get_height(), '.0f'),\n                     (p.get_x() + p.get_width() \/ 2., p.get_height()),\n                     ha = 'center', va = 'center', xytext = (0, 10),\n                     textcoords = 'offset points')  \n        ax.set_title('30 Melhores medianas das m\u00e9dias de 2014 a 2018\\n\\\n        Campara\u00e7\u00e3o das medianas das vari\u00e1veis DOCENTE\/ESCOLA e MATR\u00cdCULA\/DOCENTE')\n        plt.xticks(rotation=60)\n        \n        ax = plt.subplot(2,1,2, sharey=ax)\n        ax.bar(municipios_down, medianas_down['DOCENTE\/ESCOLA'], align='edge',\n                            label='DOCENTE\/ESCOLA', width=width)\n        for p in ax.patches:\n            ax.annotate(format(p.get_height(), '.0f'),\n                       (p.get_x() + p.get_width() \/ 2., p.get_height()),\n                       ha = 'center', va = 'center', xytext = (0, 10),\n                       textcoords = 'offset points')  \n        ax.bar(municipios_down, medianas_down['MATRICULA\/DOCENTE'], align='edge',\n                            label='MATRICULA\/DOCENTE', width=-width)\n        for p in ax.patches:\n          ax.annotate(format(p.get_height(), '.0f'),\n                     (p.get_x() + p.get_width() \/ 2., p.get_height()),\n                     ha = 'center', va = 'center', xytext = (0, 10),\n                     textcoords = 'offset points')  \n        ax.set_title('30 Piores medianas das m\u00e9dias de 2014 a 2018\\n\\\n        Campara\u00e7\u00e3o das medianas das vari\u00e1veis DOCENTE\/ESCOLA e MATR\u00cdCULA\/DOCENTE')\n        plt.xticks(rotation=60)\n        \n        ax.legend()\n        # Posiciona a legenda fora do gr\u00e1fico\n        ax.legend(bbox_to_anchor=(.9, 3.8), loc=2, borderaxespad=0.)\n        self.get_ref(ax, -0.01, -1.85)\n        plt.subplots_adjust(hspace=1.5)\n        plt.show()\n        \n        return\n    \n    def compara_escola(self, df):     \n        \n        df_rank = self.get_rank(df)\n    \n        medianas_top = df_rank.sort_values(['MEDIA_NOTAS'], ascending=False).head(30)\n        \n        medianas_down = df_rank.sort_values(['MEDIA_NOTAS'], ascending=True).head(30)\n        \n        municipios_top = [df[df['COD. MUNIC'] == cod]['NOME_MUNIC\u00cdPIO'].values[0] + \" \/ \"+ df[df['COD. MUNIC'] == cod]['NM_UF_SIGLA'].values[0]\n                          for cod in medianas_top['COD. MUNIC'].values]\n        \n        municipios_down = [df[df['COD. MUNIC'] == cod]['NOME_MUNIC\u00cdPIO'].values[0] + \" \/ \"+ df[df['COD. MUNIC'] == cod]['NM_UF_SIGLA'].values[0]\n                          for cod in medianas_down['COD. MUNIC'].values]\n        width = 0.35\n        \n        plt.figure(figsize=(20,8))\n        ax = plt.subplot(2,1,1)\n        \n        ax.set_ylim(0,800)\n        \n        ax.bar(municipios_top, medianas_top['HAB\/ESCOLA'], align='edge',\n                            label='HAB\/ESCOLA', width=width, color='r')\n        for p in ax.patches:\n          ax.annotate(format(p.get_height(), '.0f'),\n                     (p.get_x() + p.get_width() \/ 2., p.get_height()),\n                     ha = 'center', va = 'center', xytext = (0, 10),\n                     textcoords = 'offset points')  \n        ax.bar(municipios_top, medianas_top['MATRIC\/ESCOLA'], align='edge',\n                            label='MATRIC\/ESCOLA', width=-width, color='g')\n        for p in ax.patches:\n          ax.annotate(format(p.get_height(), '.0f'),\n                     (p.get_x() + p.get_width() \/ 2., p.get_height()),\n                     ha = 'center', va = 'center', xytext = (0, 10),\n                     textcoords = 'offset points')  \n        ax.set_title('30 Melhores medianas das m\u00e9dias de 2014 a 2018\\n\\\n        Campara\u00e7\u00e3o das medianas das vari\u00e1veis HAB\/ESCOLA e MATR\u00cdCULA\/ESCOLA')\n        plt.xticks(rotation=60)\n        \n        ax = plt.subplot(2,1,2, sharey=ax)\n        ax.bar(municipios_down, medianas_down['HAB\/ESCOLA'], align='edge',\n                            label='HAB\/ESCOLA', width=width, color='r')\n        for p in ax.patches:\n            ax.annotate(format(p.get_height(), '.0f'),\n                       (p.get_x() + p.get_width() \/ 2., p.get_height()),\n                       ha = 'center', va = 'center', xytext = (0, 10),\n                       textcoords = 'offset points')  \n        ax.bar(municipios_down, medianas_down['MATRIC\/ESCOLA'], align='edge',\n                            label='MATRIC\/ESCOLA', width=-width, color='g')\n        for p in ax.patches:\n          ax.annotate(format(p.get_height(), '.0f'),\n                     (p.get_x() + p.get_width() \/ 2., p.get_height()),\n                     ha = 'center', va = 'center', xytext = (0, 10),\n                     textcoords = 'offset points')  \n        ax.set_title('30 Piores medianas das m\u00e9dias de 2014 a 2018\\n\\\n        Campara\u00e7\u00e3o das medianas das vari\u00e1veis HAB\/ESCOLA e MATR\u00cdCULA\/ESCOLA')\n        plt.xticks(rotation=60)\n        \n        ax.legend()\n        # Posiciona a legenda fora do gr\u00e1fico\n        ax.legend(bbox_to_anchor=(.9, 3.8), loc=2, borderaxespad=0.)\n        self.get_ref(ax, -0.01, -1.85)\n        plt.subplots_adjust(hspace=1.5)\n        plt.show()\n        \n        return\n    \n    \n    def compara_densidade(self, df):       \n        \n        df_rank = self.get_rank(df)\n\n        \n        locations = pd.read_csv('\/kaggle\/input\/estrutura-educacional-x-desempenho-enem\/localidadesBR_LON_LAT.csv', sep=';', decimal=',')\n        locations.columns = ['COD. MUNIC','longd','latd']\n        df_rank = pd.merge(df_rank,locations, on='COD. MUNIC')\n        \n        \n        # Extract the data we're interested in\n        lat = df_rank['latd'].values\n        lon = df_rank['longd'].values\n        matriculas = df_rank['MATRIC\/ESCOLA'].values\n        escolas = df_rank['HAB\/ESCOLA'].values\n        matriculas_doc = df_rank['MATRICULA\/DOCENTE'].values\n        docentes_esc = df_rank['DOCENTE\/ESCOLA'].values\n        medias = df_rank['MEDIA_NOTAS'].values\n        \n        \n        # 1. Draw the map background\n        fig = plt.figure(figsize=(40, 40))\n        \n        ax = fig.add_subplot(311)        \n        ax.set_title('Densidade de Docentes \/ Escolas e Matr\u00edculas \/ Docentes') \n        \n        m = Basemap(projection='lcc', resolution='l', \n                    lat_0=-12, lon_0=-55,\n                    width=5E6, height=5.2E6)\n        m.shadedrelief()\n        m.drawcoastlines(color='gray')\n        m.drawcountries(color='gray')\n        m.drawstates(color='gray')\n        \n        # 2. scatter city data, with color reflecting population\n        # and size reflecting area\n        m.scatter(lon, lat, latlon=True,\n                  c=docentes_esc, s=matriculas_doc,\n                  cmap='Oranges', alpha=0.7)\n        \n        # 3. create colorbar and legend\n        plt.colorbar(label=r'Docentes \/ Escolas')\n        plt.clim(-10, max(matriculas_doc))\n        \n        # make legend with dummy points\n        for a in [int(min(docentes_esc)), int(np.median(docentes_esc)), int(max(docentes_esc))]:\n            plt.scatter([], [], c='k', alpha=0.5, s=a,\n                        label=str(a) + ' Matr\u00edculas \/ Docentes')\n            \n        plt.legend(scatterpoints=1, frameon=True,\n                   labelspacing=1, loc='lower left');\n        \n        self.get_annotate(m, df, df_rank)\n        \n        ax = fig.add_subplot(312)        \n        ax.set_title('Densidade de Habitantes e Matr\u00edculas \/ Escolas')\n        \n        m = Basemap(projection='lcc', resolution='l', \n                    lat_0=-12, lon_0=-55,\n                    width=5E6, height=5.2E6)\n        m.shadedrelief()\n        m.drawcoastlines(color='gray')\n        m.drawcountries(color='gray')\n        m.drawstates(color='gray')\n        \n        # 2. scatter city data, with color reflecting Matr\u00edculas \/ Escolas\n        # and size reflecting Escolas \/ Habitantes\n        m.scatter(lon, lat, latlon=True,\n                  c=matriculas, s=escolas\/matriculas*1.7,\n                  cmap='Reds', alpha=0.7)\n        \n        # 3. create colorbar and legend\n        plt.colorbar(label=r'Matr\u00edculas \/ Escolas')\n        plt.clim(min(matriculas)-10, max(matriculas))\n        \n        # make legend with dummy points\n        for a in [int(min(escolas)), int(np.median(escolas)), int(max(escolas))]:\n            plt.scatter([], [], c='k', alpha=0.5, s=np.log10(a)*5,\n                        label=str(a) + ' Habitantes \/ Escolas')\n        plt.legend(scatterpoints=1, frameon=True,\n                   labelspacing=1, loc='lower left');\n        plt.subplots_adjust(hspace=.07)\n        \n        self.get_annotate(m, df, df_rank)\n        \n        ax = fig.add_subplot(313)        \n        ax.set_title('Densidade das medianas de 2014 a 2018') \n        \n        m = Basemap(projection='lcc', resolution='l', \n                    lat_0=-12, lon_0=-55,\n                    width=5E6, height=5.2E6)\n        m.shadedrelief()\n        m.drawcoastlines(color='gray')\n        m.drawcountries(color='gray')\n        m.drawstates(color='gray')\n        \n        # 2. scatter city data, with color reflecting population\n        # and size reflecting area\n        m.scatter(lon, lat, latlon=True,\n                  c=medias, s=np.log2(medias),\n                  cmap='inferno', alpha=0.7)\n        \n        # 3. create colorbar and legend\n        plt.colorbar(label=r'Meidana das m\u00e9dias (2014 a 2018)')\n        plt.clim(0, max(medias))\n        \n        # make legend with dummy points\n        for a in [int(min(medias)), int(np.median(medias)), int(max(medias))]:\n            plt.scatter([], [], c='k', alpha=0.5, s=np.log2(a),\n                        label=str(a) + ' Medianas das M\u00e9dias')\n            \n        plt.legend(scatterpoints=1, frameon=True,\n                   labelspacing=1, loc='lower left');\n        \n        self.get_annotate(m, df, df_rank)\n        self.get_ref(ax,0.02,-0.15)\n        \n        plt.show()\n        \n        return\n        \n    def get_rank(self, df):        \n\n        df_rank = df.groupby('COD. MUNIC')[['HAB\/ESCOLA','MATRIC\/ESCOLA',\n                                               'DOCENTE\/ESCOLA','MATRICULA\/DOCENTE',\n                                               'MEDIA_NOTAS']].median().reset_index()\n        \n        return df_rank\n    \n    \n    def get_outliers(self, df):\n        \n        plt.figure(figsize=(20,15))\n\n        plt.subplot(211)\n        sns.boxplot(x='ano', y='MEDIA_NOTAS', data=df)\n        plt.title('Detec\u00e7\u00e3o dos \"outliiers\" de cada ano')\n        \n        #plt.figure(figsize=(20,20))\n        plt.subplot(212)\n        ax = sns.boxplot(x='ano', y='MEDIA_NOTAS', data=df, hue='CLASSE')\n        \n        plt.title('Detec\u00e7\u00e3o dos \"outliiers\" de cada ano separados por classe')\n        \n        labels=['Insatisfat\u00f3rio', 'Regular', 'Satisfat\u00f3rio']\n        h,l = ax.get_legend_handles_labels()\n        ax.legend(handles=h, labels=labels)\n        \n        self.get_ref(ax, -0.03, -0.4)\n        \n        plt.show()\n        \n        return\n\n    \n    def get_annotate(self ,m, df, df_rank):\n        \n        plt.annotate(f'Melhores',\n                     xy=m(-65, 8),  xycoords='data',\n                        xytext=m(-73, 8), textcoords='data',\n                        color='k',size = 'small',\n                        arrowprops=dict(arrowstyle=\"->\", color='g'),\n                        bbox=dict(boxstyle=\"round\", fc=\"w\"))\n        \n        plt.annotate(f'Piores    ',\n                     xy=m(-65, 6),  xycoords='data',\n                        xytext=m(-73, 6), textcoords='data',\n                        color='k',size = 'small',\n                        arrowprops=dict(arrowstyle=\"->\", color='r'),\n                        bbox=dict(boxstyle=\"round\", fc=\"w\"))\n        \n        medianas_top = df_rank.sort_values(['MEDIA_NOTAS'], ascending=False).head(5)\n    \n        x, y =  m(medianas_top.iloc[0]['longd'], medianas_top.iloc[0]['latd'])\n        x2, y2 = m(medianas_top.iloc[0]['longd']+(5), medianas_top.iloc[0]['latd']-5)\n        \n        plt.annotate(f'{df[df[\"COD. MUNIC\"]==medianas_top.iloc[0][\"COD. MUNIC\"]][\"NOME_MUNIC\u00cdPIO\"].values[0]} - {df[df[\"COD. MUNIC\"]==medianas_top.iloc[0][\"COD. MUNIC\"]][\"NM_UF_SIGLA\"].values[0]}\\nMediana {round(medianas_top.iloc[0][\"MEDIA_NOTAS\"],2)}',\n                     xy=(x, y),  xycoords='data',\n                        xytext=(x2, y2), textcoords='data',\n                        color='k',size = 'small',\n                        arrowprops=dict(arrowstyle=\"->\", color='g'))\n        \n        x, y =  m(medianas_top.iloc[1]['longd'], medianas_top.iloc[1]['latd'])\n        x2, y2 = m(medianas_top.iloc[1]['longd']+(5), medianas_top.iloc[1]['latd'])\n        \n        plt.annotate(f'{df[df[\"COD. MUNIC\"]==medianas_top.iloc[1][\"COD. MUNIC\"]][\"NOME_MUNIC\u00cdPIO\"].values[0]} - {df[df[\"COD. MUNIC\"]==medianas_top.iloc[1][\"COD. MUNIC\"]][\"NM_UF_SIGLA\"].values[0]}\\nMediana {round(medianas_top.iloc[1][\"MEDIA_NOTAS\"],2)}',\n                     xy=(x, y),  xycoords='data',\n                        xytext=(x2, y2), textcoords='data',\n                        color='k',size = 'small',\n                        arrowprops=dict(arrowstyle=\"->\", color='g'))\n        \n        x, y =  m(medianas_top.iloc[2]['longd'], medianas_top.iloc[2]['latd'])\n        x2, y2 = m(medianas_top.iloc[2]['longd']+(8), medianas_top.iloc[2]['latd']+2)\n        \n        plt.annotate(f'{df[df[\"COD. MUNIC\"]==medianas_top.iloc[2][\"COD. MUNIC\"]][\"NOME_MUNIC\u00cdPIO\"].values[0]} - {df[df[\"COD. MUNIC\"]==medianas_top.iloc[2][\"COD. MUNIC\"]][\"NM_UF_SIGLA\"].values[0]}\\nMediana {round(medianas_top.iloc[2][\"MEDIA_NOTAS\"],2)}',\n                     xy=(x, y),  xycoords='data',\n                        xytext=(x2, y2), textcoords='data',\n                        color='k',size = 'small',\n                        arrowprops=dict(arrowstyle=\"->\", color='g'))\n        \n        x, y =  m(medianas_top.iloc[3]['longd'], medianas_top.iloc[3]['latd'])\n        x2, y2 = m(medianas_top.iloc[3]['longd']+(5), medianas_top.iloc[3]['latd']-3)\n        \n        plt.annotate(f'{df[df[\"COD. MUNIC\"]==medianas_top.iloc[3][\"COD. MUNIC\"]][\"NOME_MUNIC\u00cdPIO\"].values[3]} - {df[df[\"COD. MUNIC\"]==medianas_top.iloc[3][\"COD. MUNIC\"]][\"NM_UF_SIGLA\"].values[3]}\\nMediana {round(medianas_top.iloc[3][\"MEDIA_NOTAS\"],2)}',\n                     xy=(x, y),  xycoords='data',\n                        xytext=(x2, y2), textcoords='data',\n                        color='k',size = 'small',\n                        arrowprops=dict(arrowstyle=\"->\", color='g'))\n        \n        x, y =  m(medianas_top.iloc[4]['longd'], medianas_top.iloc[4]['latd'])\n        x2, y2 = m(medianas_top.iloc[4]['longd']+(5), medianas_top.iloc[4]['latd']-4)\n        \n        plt.annotate(f'{df[df[\"COD. MUNIC\"]==medianas_top.iloc[4][\"COD. MUNIC\"]][\"NOME_MUNIC\u00cdPIO\"].values[0]} - {df[df[\"COD. MUNIC\"]==medianas_top.iloc[4][\"COD. MUNIC\"]][\"NM_UF_SIGLA\"].values[0]}\\nMediana {round(medianas_top.iloc[4][\"MEDIA_NOTAS\"],2)}',\n                     xy=(x, y),  xycoords='data',\n                        xytext=(x2, y2), textcoords='data',\n                        color='k',size = 'small',\n                        arrowprops=dict(arrowstyle=\"->\", color='g'))\n        \n        \n        \n        medianas_down = df_rank.sort_values(['MEDIA_NOTAS'], ascending=True).head(5)\n    \n        x, y =  m(medianas_down.iloc[0]['longd'], medianas_down.iloc[0]['latd'])\n        x2, y2 = m(medianas_down.iloc[0]['longd']-7, medianas_down.iloc[0]['latd']+28)\n        \n        plt.annotate(f'{df[df[\"COD. MUNIC\"]==medianas_down.iloc[0][\"COD. MUNIC\"]][\"NOME_MUNIC\u00cdPIO\"].values[0]} - {df[df[\"COD. MUNIC\"]==medianas_down.iloc[0][\"COD. MUNIC\"]][\"NM_UF_SIGLA\"].values[0]}\\nMediana {round(medianas_down.iloc[0][\"MEDIA_NOTAS\"],2)}',\n                     xy=(x, y),  xycoords='data',\n                        xytext=(x2, y2), textcoords='data',\n                        color='k',size = 'small',\n                        arrowprops=dict(arrowstyle=\"->\", color='r'))\n    \n        x, y =  m(medianas_down.iloc[1]['longd'], medianas_down.iloc[1]['latd'])\n        x2, y2 = m(medianas_down.iloc[1]['longd']+2, medianas_down.iloc[1]['latd']+20)\n        \n        plt.annotate(f'{df[df[\"COD. MUNIC\"]==medianas_down.iloc[1][\"COD. MUNIC\"]][\"NOME_MUNIC\u00cdPIO\"].values[0]} - {df[df[\"COD. MUNIC\"]==medianas_down.iloc[1][\"COD. MUNIC\"]][\"NM_UF_SIGLA\"].values[0]}\\nMediana {round(medianas_down.iloc[1][\"MEDIA_NOTAS\"],2)}',\n                     xy=(x, y),  xycoords='data',\n                        xytext=(x2, y2), textcoords='data',\n                        color='k',size = 'small',\n                        arrowprops=dict(arrowstyle=\"->\", color='r')) \n        \n        x, y =  m(medianas_down.iloc[2]['longd'], medianas_down.iloc[2]['latd'])\n        x2, y2 = m(medianas_down.iloc[2]['longd']+2, medianas_down.iloc[2]['latd']+12)\n        \n        plt.annotate(f'{df[df[\"COD. MUNIC\"]==medianas_down.iloc[2][\"COD. MUNIC\"]][\"NOME_MUNIC\u00cdPIO\"].values[0]} - {df[df[\"COD. MUNIC\"]==medianas_down.iloc[2][\"COD. MUNIC\"]][\"NM_UF_SIGLA\"].values[0]}\\nMediana {round(medianas_down.iloc[2][\"MEDIA_NOTAS\"],2)}',\n                     xy=(x, y),  xycoords='data',\n                        xytext=(x2, y2), textcoords='data',\n                        color='k',size = 'small',\n                        arrowprops=dict(arrowstyle=\"->\", color='r'))\n        \n        x, y =  m(medianas_down.iloc[3]['longd'], medianas_down.iloc[3]['latd'])\n        x2, y2 = m(medianas_down.iloc[3]['longd']+11, medianas_down.iloc[3]['latd']+26)\n        \n        plt.annotate(f'{df[df[\"COD. MUNIC\"]==medianas_down.iloc[3][\"COD. MUNIC\"]][\"NOME_MUNIC\u00cdPIO\"].values[0]} - {df[df[\"COD. MUNIC\"]==medianas_down.iloc[3][\"COD. MUNIC\"]][\"NM_UF_SIGLA\"].values[0]}\\nMediana {round(medianas_down.iloc[3][\"MEDIA_NOTAS\"],2)}',\n                     xy=(x, y),  xycoords='data',\n                        xytext=(x2, y2), textcoords='data',\n                        color='k',size = 'small',\n                        arrowprops=dict(arrowstyle=\"->\", color='r'))\n        \n        x, y =  m(medianas_down.iloc[4]['longd'], medianas_down.iloc[4]['latd'])\n        x2, y2 = m(medianas_down.iloc[4]['longd']-1, medianas_down.iloc[4]['latd']+12)\n        \n        plt.annotate(f'{df[df[\"COD. MUNIC\"]==medianas_down.iloc[4][\"COD. MUNIC\"]][\"NOME_MUNIC\u00cdPIO\"].values[0]} - {df[df[\"COD. MUNIC\"]==medianas_down.iloc[4][\"COD. MUNIC\"]][\"NM_UF_SIGLA\"].values[0]}\\nMediana {round(medianas_down.iloc[4][\"MEDIA_NOTAS\"],2)}',\n                     xy=(x, y),  xycoords='data',\n                        xytext=(x2, y2), textcoords='data',\n                        color='k',size = 'small',\n                        arrowprops=dict(arrowstyle=\"->\", color='r'))\n        \n        return\n\n    def get_ref(self, ax, x=0, y=0):\n        \n        x = -0.03 if x == 0 else x\n        y = -0.85 if y == 0 else y\n        \n                # Define o local e texto da fonte dos dados\n        ax.text(x=x, y=y, s='Fonte: INSTITUTO NACIONAL DE ESTUDOS E PESQUISAS EDUCACIONAIS AN\u00cdSIO TEIXEIRA.\\n'\n'Sinopse Estat\u00edstica da Educa\u00e7\u00e3o B\u00e1sica de 2010 a 2019. Bras\u00edlia: Inep, 2020.\\n'\n'Dispon\u00edvel em: <http:\/\/portal.inep.gov.br\/sinopses-estatisticas-da-educacao-basica>. Acesso em: 05\/06\/2020\\n'\n\n'Fonte: IBGE. Diretoria de Pesquisas - DPE -  Coordena\u00e7\u00e3o de Popula\u00e7\u00e3o e Indicadores Sociais - COPIS.\\n'\n'Dispon\u00edvel em: <http:\/\/www.dados.gov.br\/dataset\/cd-censo-demografico>. Acesso em: 05\/06\/2020\\n'\n    \n'Fonte: IBGE. Diretoria de Pesquisas - DPE -  Coordena\u00e7\u00e3o de Popula\u00e7\u00e3o e Indicadores Sociais - COPIS.\\n'\n'Dispon\u00edvel em: <https:\/\/www.ibge.gov.br\/geociencias\/downloads-geociencias.html>. Acesso em: 05\/06\/2020\\n',\n    fontsize=9, ha='left', va='bottom', transform=ax.transAxes)\n        \n        return","1e65fe95":"# Importa a biblioteca para estrutura\u00e7\u00e3o dos dados e visualiza\u00e7\u00e3o dos dados   \nimport pandas as pd\n\nclass Municipio(object):\n    \n    def __init__(self):\n        self.df_raw = pd.DataFrame()   \n        self.df = pd.DataFrame()      \n        self.lst_anos = list([2014,2015,2016,2017,2018])\n     \n    def gerar_dados(self) -> pd.DataFrame:\n        \n        \"\"\"\n        Recebe a lista com os anos a serem concatenados no dataframe\n        Retorna o DataFrame de desempenho(MEDIANA) no ENEM de cada estado \"\"\"\n        \n        # Carrega dos dados das tabelas de 2014 a 2019        \n        for ano in self.lst_anos:\n            \n            df_tmp = pd.DataFrame()\n            df_tmp['ano'] = ano\n            df_territorio_ano = pd.read_csv(f'\/kaggle\/input\/estrutura-educacional-x-desempenho-enem\/{ano}_territorio_mun.csv', sep=';', decimal=',')            \n            df_territorio_ano = df_tmp.join(df_territorio_ano, how='outer')\n            df_territorio_ano.columns = ['ano', 'ID', 'CD_GCUF', 'NM_UF',\n                                         'NM_UF_SIGLA', 'COD. MUNIC',\n                                         'NOME_MUNIC\u00cdPIO', 'AREA_km\u00b2']\n            \n            df_territorio_ano['ano'].fillna(int(ano), inplace=True)\n            \n            \n            df_pop_ano = pd.read_csv(f'\/kaggle\/input\/estrutura-educacional-x-desempenho-enem\/{ano}_pop_mun.csv', sep=';', decimal=',')\n            \n            # Remove a coluna duplicada\n            # Pradroniza o nome da coluna COD. MUNIC para mesclar os DataFrames\n            df_pop_ano.drop(['UF','COD. UF','COD_MUNIC','NOME DO MUNIC\u00cdPIO'], axis=1, inplace=True)\n            \n            df_territorio_ano = pd.merge(df_territorio_ano[['ano','NM_UF_SIGLA','COD. MUNIC','NOME_MUNIC\u00cdPIO','AREA_km\u00b2']], df_pop_ano, on='COD. MUNIC')           \n            \n            df_educacao_ano = pd.read_csv(f'\/kaggle\/input\/estrutura-educacional-x-desempenho-enem\/{ano}_educacao_basica.csv', sep=';', decimal=',')            \n            df_tmp = pd.merge(df_territorio_ano, df_educacao_ano, on='COD. MUNIC')\n            \n            df_enem = pd.read_csv(f'\/kaggle\/input\/estrutura-educacional-x-desempenho-enem\/{ano}_MICRODADOS_ENEM_500K.csv', sep=';')\n            \n            if ano == 2014:\n                df_enem1                    = df_enem[['COD_MUNICIPIO_RESIDENCIA','NOTA_CN', 'NOTA_CH', 'NOTA_LC', 'NOTA_MT','NU_NOTA_REDACAO']].groupby('COD_MUNICIPIO_RESIDENCIA').median()\n                df_enem1['CAND_ESC_PUB']    = df_enem[df_enem['TP_ESCOLA']==1].groupby('COD_MUNICIPIO_RESIDENCIA')['TP_ESCOLA'].count()\n                df_enem1['CAND_ESC_PRI']    = df_enem[df_enem['TP_ESCOLA']==2].groupby('COD_MUNICIPIO_RESIDENCIA')['TP_ESCOLA'].count()               \n\n                df_enem1['ENS_REGULAR']     = df_enem[df_enem['IN_TP_ENSINO']==1].groupby('COD_MUNICIPIO_RESIDENCIA')['IN_TP_ENSINO'].count()\n                df_enem1['ENS_EJA']         = df_enem[df_enem['IN_TP_ENSINO']==2].groupby('COD_MUNICIPIO_RESIDENCIA')['IN_TP_ENSINO'].count()\n                df_enem1['ENS_ESPECIAL']    = df_enem[df_enem['IN_TP_ENSINO']==4].groupby('COD_MUNICIPIO_RESIDENCIA')['IN_TP_ENSINO'].count()\n                df_enem1 = df_enem1.fillna(0)\n                \n            else:\n                df_enem1                    = df_enem[['CO_MUNICIPIO_RESIDENCIA','NU_NOTA_CN','NU_NOTA_CH', 'NU_NOTA_LC', 'NU_NOTA_MT','NU_NOTA_REDACAO']].groupby('CO_MUNICIPIO_RESIDENCIA').median()\n                \n                df_enem1['CAND_ESC_PUB']    = df_enem[df_enem['TP_ESCOLA']==2].groupby('CO_MUNICIPIO_RESIDENCIA')['TP_ESCOLA'].count()\n                df_enem1['CAND_ESC_PRI']    = df_enem[df_enem['TP_ESCOLA']==3].groupby('CO_MUNICIPIO_RESIDENCIA')['TP_ESCOLA'].count()               \n\n                df_enem1['ENS_REGULAR']     = df_enem[df_enem['TP_ENSINO']==1].groupby('CO_MUNICIPIO_RESIDENCIA')['TP_ENSINO'].count()\n                df_enem1['ENS_EJA']         = df_enem[df_enem['TP_ENSINO']==2].groupby('CO_MUNICIPIO_RESIDENCIA')['TP_ENSINO'].count()\n                df_enem1['ENS_ESPECIAL']    = df_enem[df_enem['TP_ENSINO']==4].groupby('CO_MUNICIPIO_RESIDENCIA')['TP_ENSINO'].count()\n                df_enem1 = df_enem1.fillna(0)\n\n            df_enem = df_enem1.reset_index()\n            df_enem.columns = ['COD. MUNIC','NU_NOTA_CN','NU_NOTA_CH','NU_NOTA_LC',\n                               'NU_NOTA_MT','NU_NOTA_REDACAO','CAND_ESC_PUB','CAND_ESC_PRI',\n                               'ENS_REGULAR','ENS_EJA','ENS_ESPECIAL']\n            \n            df_tmp = pd.merge(df_tmp, df_enem, on='COD. MUNIC')\n            df_tmp = df_tmp.fillna(0)\n            self.df_raw = pd.concat([self.df_raw, df_tmp], ignore_index=True)\n        \n        return self.df_raw\n \n    \n    def tratar_dados(self, list) -> pd.DataFrame:\n        \n        \"\"\"\n        Recebe a lista com as colunas e o DataaFrame de dados a serem tratados e convertidos\n        Retorna o DataFrame com os dados tratados -> NaN = 0 \/ StringObj = Int \"\"\" \n        \n        lst_colunas = list\n        self.df = self.df_raw.copy()\n        # Considera que se n\u00e3o h\u00e1 n\u00famero informado n\u00e3o existe no estado\n        # Trata valores an\u00f4malos das tabelas considerando que \"NaN\" \u00e9 ZERO(Escolas, Matr\u00edculas, Docentes etc)        \n        for col in lst_colunas:           \n\n            # Remove indica\u00e7\u00e3o das anota\u00e7\u00f5es sobre o CENSO realizado no respectivo ano para padroniza\u00e7\u00e3o dos dados\n            # Padroniza os tipos de dados da coluna POPULA\u00c7\u00c3O ESTIMADA convertendo para n\u00fameros inteiros\n            self.df[col] = self.df[col].str.replace(r\"\\(.*?\\)\",\"\")\n            self.df[col] = self.df[col].str.replace(\" -   \",'0')\n            self.df[col] = self.df[col].str.replace(\".\",\"\")\n            self.df[col].fillna(0, inplace=True)\n            self.df[col] = self.df[col].astype(str).astype(int)\n        \n        return self.df\n        \n    def calcular_densidade(self) -> pd.DataFrame:\n        \n        \"\"\"\n        Recebe um lista de tuplas com os indices de intervalos das colunas e o DataFrame com a estrutura de dados com as colunas na ordem padronizada\n        Retorna DataFrame com a soma de escolas, matr\u00edculas e corpo docente de cada estado \/ ano \"\"\"\n\n        \n        # Calcula a densidade de popula\u00e7\u00e3o, escolas, matr\u00edculas e docentes por km\u00b2 por km\u00b2 de cada estado \n        # Estimativa de habitante\/km\u00b2\n        self.df['HAB\/km\u00b2']                   = (self.df.iloc[:,5].transpose().values\/ \\\n                                                           self.df.iloc[:,4].transpose().values)\n        \n        # Estimativa de Escolas\/km\u00b2\n        self.df['ESCOLA\/km\u00b2']                = (self.df.iloc[:,63].values\/ \\\n                                                           self.df.iloc[:,4].values)\n        \n        # Estimativa de habitantes(km\u00b2)\/escolas(km\u00b2)df_raw\n        self.df['HAB\/ESCOLA']                = (self.df.iloc[:,5].values\/ \\\n                                                           self.df.iloc[:,63].values)\n         \n        # Estimativa de matr\u00edculas(km\u00b2)\/escolas(km\u00b2)\n        self.df['MATRIC\/ESCOLA']            = (self.df.iloc[:,60].values\/ \\\n                                                            self.df.iloc[:,63].values)\n         \n        # Estimativa do corpo docente(km\u00b2)\/escolas(km\u00b2)\n        self.df['DOCENTE\/ESCOLA']           = (self.df.iloc[:,64].values\/ \\\n                                                           self.df.iloc[:,63].values)\n        \n        # Estimativa do corpo docente(km\u00b2)\/matr\u00edculas(km\u00b2)\n        self.df['MATRICULA\/DOCENTE']        = (self.df.iloc[:,60].values\/ \\\n                                                            self.df.iloc[:,64].values)\n        \n        return self.df\n    \n    \n    \n    def calcular_totais(self) -> pd.DataFrame:\n            \n        \"\"\"\n        Recebe um lista de tuplas com os indices de intervalos das colunas e o DataFrame com a estrutura de dados com as colunas na ordem padronizada\n        Retorna DataFrame com a soma de escolas, matr\u00edculas e corpo docente de cada estado \/ ano\"\"\"\n        \n        self.df['MEDIA_NOTAS']            = self.df.iloc[:,49:54].transpose().mean()\n        \n        # Soma os valores das respectivas colunas Matr\u00edculas, Escolas, Docentes\n        self.df['TOTAL_MATRICULA']       = sum(self.df.iloc[:,6:13].transpose().values)\n        self.df['TOTAL_MATRICULA_PUB']   = sum(self.df.iloc[:,[27,28,29,31,32,33]].transpose().values)\n        self.df['TOTAL_MATRICULA_PRI']   = sum(self.df.iloc[:,[30,34]].transpose().values)\n        self.df['TOTAL_ESCOLA']          = sum(self.df.iloc[:,13:20].transpose().values)\n        self.df['TOTAL_DOCENTE']         = sum(self.df.iloc[:,20:27].transpose().values)\n        self.df['TOTAL_DOCENTE_PUB']     = sum(self.df.iloc[:,[35,37]].transpose().values)\n        self.df['TOTAL_DOCENTE_PRI']     = sum(self.df.iloc[:,[36,38]].transpose().values)\n        self.df['TOTAL_ESCOLA_PUB']      = sum(self.df.iloc[:,[42,43,45,46]].transpose().values)\n        self.df['TOTAL_ESCOLA_PRI']      = sum(self.df.iloc[:,[44,48]].transpose().values)\n        \n        return self.df\n    \n            \n    \n    def classificar(self):\n        \n        self.df.loc[self.df['MEDIA_NOTAS'] >= 600, 'CLASSE'] = 2         # Classifica os resultados satisfat\u00f3rios como 2\n        self.df.loc[self.df['MEDIA_NOTAS'] < 600, 'CLASSE'] = 1          # Classifica os resultados regulares como 1\n        self.df.loc[self.df['MEDIA_NOTAS'] < 450, 'CLASSE'] = 0         # Classifica os resultados insatisfat\u00f3rios como 0\n        \n        return self.df","c692bf58":"import seaborn as sns\nimport matplotlib.pyplot as plt\nsns.set_style('whitegrid')\n\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import AdaBoostRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import VotingRegressor\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.svm import SVR\n\nfrom sklearn.metrics import mean_absolute_error\n\nfrom sklearn.model_selection import train_test_split, RandomizedSearchCV\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.feature_selection import SelectFromModel\n\n\nclass Regressor(object):\n    \n    def __init__(self, df):\n        \n        df_reg = df.query('MEDIA_NOTAS > 0').copy()\n        \n        self.X = df_reg.drop(['ano', 'NM_UF_SIGLA', 'COD. MUNIC', 'NOME_MUNIC\u00cdPIO',\n             'NU_NOTA_CN', 'NU_NOTA_CH', 'NU_NOTA_LC', 'NU_NOTA_MT',\n             'NU_NOTA_REDACAO','MEDIA_NOTAS'], inplace=False, axis=1).values\n        \n        self.y = df.query('MEDIA_NOTAS > 0')['MEDIA_NOTAS'].values\n    \n    \n\n    def scale_data(self, X, y):\n        \n        self.X_scaler = StandardScaler()\n        self.y_scaler = StandardScaler()   \n        \n        X_scaled = self.X_scaler.fit_transform(X)\n        y_scaled = self.y_scaler.fit_transform(y.reshape(-1,1))\n    \n        return X_scaled, y_scaled\n    \n    \n    \n    def select_features(self, model, X, y):\n    \n        sfm = SelectFromModel(model, threshold=0.004)\n        sfm.fit(X, y.ravel())\n        X = sfm.transform(X)     \n        \n        return X\n    \n    \n    \n    def get_params(self, X, y, model, grid):\n        \n        random = RandomizedSearchCV(scoring=\"neg_mean_absolute_error\", estimator = model, param_distributions = grid, n_iter = 100, cv = 3, verbose=2, random_state=1, n_jobs = -1)\n            # Fit the random search model\n        random.fit(X, y.ravel())\n        \n        return random.best_params_\n    \n    \n    \n    def tree(self, X, y, base=True, plot=False):\n        \"\"\"\n        Adaboost + Decision Tree Regressors\n        \"\"\"\n        def get_grid():\n            \n            grid = {'max_depth':[110, 120, 130, 140, 150, 160, 170, 180, 190, 200, 210, 220, 230, 240, 250, 260, 270, 280, 290, 300,  None],\n                    'max_leaf_nodes':[2, 5, 10, 15, 20, None],\n                    'min_impurity_decrease':[1e-1, 1e-2, 1e-3, 1e-4, 1e-5, 1e-6, 1e-7, 1e-8],\n                    'max_features':['auto','sqrt'],\n                    'min_samples_leaf': [1, 2, 4],\n                    'min_samples_split': [2, 5, 10, 15, 20],\n                    'criterion':['friedman_mse', 'mse']}\n            \n            return grid\n        \n            \n        kwargs = {'random_state':1, 'loss':'square', 'n_estimators':500, 'learning_rate':1e-6} if not base else {'random_state':1}\n        \n        \n        regressor = AdaBoostRegressor(DecisionTreeRegressor(random_state= 1),**kwargs) if base else AdaBoostRegressor(DecisionTreeRegressor(random_state= 1, **self.get_params(X, y, DecisionTreeRegressor(), get_grid())),**kwargs)\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1, test_size=0.35)\n        \n        regressor.fit(X_train, y_train.ravel())\n        \n        previsoes = self.y_scaler.inverse_transform(regressor.predict(X_test))\n        y_test = self.y_scaler.inverse_transform(y_test)\n        \n        mae = mean_absolute_error(y_test, previsoes)\n        \n        print(f'{regressor} - MAE = {mae}\\n\\n')\n        \n        if plot:\n            self.plot_results(y_test, previsoes, 'Decision Tree', mae)\n        \n        return regressor\n    \n\n\n    def forest(self, X, y, base=True, plot=False):\n        \"\"\"\n        Random Forest Regressor\n        \"\"\"        \n        def get_grid():\n            \n            grid = {'bootstrap': [True, False],\n               'max_depth': [110, 120, 130, 140, 150, 160, 170, 180, 190, 200, 210, 220, 230, 240, 250, 260, 270, 280, 290, 300,  None],\n               'max_features': ['auto', 'sqrt'],\n               'min_samples_leaf': [1, 2, 4],\n               'min_samples_split': [2, 5, 10, 15, 20],\n               'n_estimators': [130, 180, 230, 280, 320, 380, 420, 470, 520],\n               'ccp_alpha':[1e-1, 1e-2, 1e-3, 1e-4, 1e-5, 1e-6, 1e-7, 1e-8],\n               'min_impurity_decrease':[1e-1, 1e-2, 1e-3, 1e-4, 1e-5, 1e-6, 1e-7, 1e-8]}\n            \n            return grid\n        \n        regressor = RandomForestRegressor(random_state=1) if base else RandomForestRegressor(random_state=1, **self.get_params(X, y, RandomForestRegressor(), get_grid()))\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1, test_size=0.35)\n        \n        regressor.fit(X_train, y_train.ravel())\n        \n        previsoes = self.y_scaler.inverse_transform(regressor.predict(X_test))\n        y_test = self.y_scaler.inverse_transform(y_test)\n        \n        mae = mean_absolute_error(y_test, previsoes)\n        \n        print(f'{regressor} - MAE = {mae}\\n\\n')\n        \n        if plot:\n            self.plot_results(y_test, previsoes, 'Random Forest', mae)\n\n        return regressor\n    \n    \n    \n    def svr(self, X, y, base=True, plot=False):\n        \"\"\"\n        SVR Support Vector Regressor\n        \"\"\"\n        \n        def get_grid():\n            \n            grid = {'max_iter':[200,500,1000,2000,3000,4000,5000],  \n                    'kernel':['rbf'],\n                    'tol':[1e-1,1e-2,1e-3,1e-4,1e-5,1e-6,1e-7,1e-8,1e-9],\n                    'C':[1,3,5,7,11,13,17,19,23,27,31,37,41,45],\n                    'epsilon':[0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9],\n                    'degree':[1,2,3,4,5],\n                    'gamma':['scale','auto'],\n                    'coef0':[1],\n                    'shrinking':[False,True]}\n            \n            return grid\n        \n        regressor = SVR() if base else SVR(**self.get_params(X, y, SVR(), get_grid()))\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1, test_size=0.35)\n        \n        regressor.fit(X_train, y_train.ravel())\n        \n        previsoes = self.y_scaler.inverse_transform(regressor.predict(X_test))\n        y_test = self.y_scaler.inverse_transform(y_test)\n        \n        mae = mean_absolute_error(y_test, previsoes) \n        \n        print(f'{regressor} - MAE = {mae}\\n\\n')\n        \n        if plot:\n            self.plot_results(y_test, previsoes, 'Support Vector', mae)\n            \n        return regressor\n    \n    \n    \n    def mlp(self, X, y, base=True, plot=False):\n\n        \"\"\"\n        Neural Network - MLP Regressor\n        \"\"\"\n        \n        def get_grid():\n            \n            grid = {'max_iter':[100,200,300,400,500,600,700,800,900,1000],\n                    'early_stopping':[False,True],\n                    'momentum':[0.1,0.3,0.5,0.9],\n                    'nesterovs_momentum':[True, False],\n                    'validation_fraction':[1e-1,1e-2,1e-3,1e-4,1e-5,1e-6,1e-7,1e-8,1e-9],\n                    'learning_rate_init':[1e-1,1e-2,1e-3,1e-4,1e-5,1e-6,1e-7,1e-8,1e-9],\n                    'activation':['identity','logistic','tanh','relu'],\n                    'solver':['sgd','lbfgs','adam'],\n                    'beta_1':[0.1,0.3,0.5,0.9],\n                    'beta_2':[0.991,0.993,0.995,0.999],\n                    'tol':[1e-1,1e-2,1e-3,1e-4,1e-5,1e-6,1e-7,1e-8,1e-9],\n                    'alpha':[1e-1,1e-2,1e-3,1e-4,1e-5,1e-6,1e-7,1e-8,1e-9],\n                    'shuffle':[True],\n                    'learning_rate':['constant','invscaling','adaptive'],\n                    #'power_t':[1,3,5,7,9,11,13,15,17,19,23,27,29,33,37,41,45],\n                    'warm_start':[True,False]}\n            \n            return grid\n        \n        \n        regressor = MLPRegressor(random_state=1) if base else MLPRegressor(random_state=1, **self.get_params(X, y, MLPRegressor(), get_grid()), hidden_layer_sizes=(int((X.shape[1])\/2)+1,int((X.shape[1])\/2)+1))\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1, test_size=0.35)\n        \n        regressor.fit(X_train, y_train.ravel())\n        \n        previsoes = self.y_scaler.inverse_transform(regressor.predict(X_test))\n        y_test = self.y_scaler.inverse_transform(y_test)\n        \n        mae = mean_absolute_error(y_test,previsoes)\n                \n        print(f'{regressor} - MAE = {mae}\\n\\n')\n        \n        if plot:\n            self.plot_results(y_test, previsoes,'Neural Network', mae)\n        \n        return regressor\n    \n\n\n    def gradient(self, X, y, base=True, plot=False, estimator=None):\n        \"\"\"\n        Gradient Boosting \n        \"\"\"\n        \n        def get_grid():\n            \n            grid = {'criterion':['mse','friedman_mse'],\n                    'max_features':['auto','sqrt'],\n                    'loss':['ls','lad','huber','quantile'],\n                    'learning_rate':[1e-1,1e-2,1e-3,1e-4,1e-5,1e-6,1e-7,1e-8,1e-9],\n                    'subsample':[1e-1,1e-2,1e-3,1e-4,1e-5,1e-6,1e-7,1e-8,1e-9],\n                    'tol':[1e-1,1e-2,1e-3,1e-4,1e-5,1e-6,1e-7,1e-8,1e-9],\n                    'max_depth':[1,3,5,7,9,11,13,15,17,19,23,25,29,35],\n                    'min_samples_split':[1,3,5,7,9,11],\n                    'n_estimators':[100,200,300,400,500],\n                    #'validation_fraction':[1e-1,1e-2,1e-3,1e-4,1e-5,1e-6,1e-7,1e-8,1e-9],\n                    'warm_start':[True, False],\n                    'alpha':[1e-1,1e-2,1e-3,1e-4,1e-5,1e-6,1e-7,1e-8,1e-9],\n                    'ccp_alpha':[0.0,1e-1,1e-2,1e-3,1e-4,1e-5,1e-6,1e-7,1e-8,1e-9]}\n            \n            return grid\n        \n        regressor = GradientBoostingRegressor(random_state=1) if base else GradientBoostingRegressor(random_state=1, **self.get_params(X, y, GradientBoostingRegressor(), get_grid()))\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1, test_size=0.35)  \n        \n        regressor.fit(X_train, y_train.ravel())\n        \n        previsoes = self.y_scaler.inverse_transform(regressor.predict(X_test))\n        y_test = self.y_scaler.inverse_transform(y_test)\n        \n        mae = mean_absolute_error(y_test, previsoes)\n        \n        print(f'{regressor} - MAE = {mae}\\n\\n')\n        \n        if plot:\n            self.plot_results(y_test, previsoes, 'Gradient Boosting', mae)\n            \n        return regressor\n    \n    \n\n    def voting(self, X, y, models, select=False):\n        \"\"\"\n        Voting Regressor\n        \"\"\"\n        X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1, test_size=0.35)\n        \n        regressor = VotingRegressor(models)\n        \n        regressor.fit(X_train, y_train.ravel())\n        \n        previsoes = self.y_scaler.inverse_transform(regressor.predict(X_test))\n        y_test = self.y_scaler.inverse_transform(y_test)\n        \n        mae = mean_absolute_error(y_test, previsoes)\n        \n        self.plot_results(y_test, previsoes, 'Voting Regressor', mae, select)\n        \n        return regressor\n    \n    \n    \n    def plot_results(self, y, previsoes, model, mae, select):\n        \n        plt.figure(figsize=(20,20))\n        plt.subplot(2,2,1)\n        ax = sns.regplot(y, previsoes, marker='o', color='c', scatter_kws={'s':20,'edgecolor':'w',\"alpha\":0.7}, label='Targets Vs Previs\u00f5es')\n        \n        if select:\n            plt.scatter(y, y, s=20, label='Orginais')\n            \n        ax.set(xlim=(0,1000), ylim=(0,1000), xlabel='Targets', ylabel='Previs\u00f5es', title=f'Targets Vs. Previs\u00f5es\\n{model} (MAE = {round(mae, 4)})')\n        plt.legend()\n        \n        plt.subplot(2,2,2)\n        ax = sns.regplot(y, previsoes-y.ravel(), marker='o', color='r', scatter_kws={'s':20,'edgecolor':'w',\"alpha\":0.7}, label='Diferen\u00e7a (Previs\u00f5es - Targets)')\n        ax.set(xlim=(0,1000), ylim=(-500,500), xlabel='Targets', ylabel='Previs\u00f5es - Targets', title=f'Diferen\u00e7a Previs\u00f5es - Targets\\n{model} (M\u00e9dia dos erros = {round((previsoes-y.ravel()).mean(), 4)})')\n        \n        return","ef3d462f":"\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import VotingClassifier\n\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.feature_selection import SelectFromModel\n\nclass Classificador(object):\n    \n    def __init__(self, df):\n        \n        df_class = df.query('MEDIA_NOTAS > 0').copy()\n\n        self.X = df_class.drop(['ano', 'NM_UF_SIGLA', 'COD. MUNIC', 'NOME_MUNIC\u00cdPIO',\n                                'NU_NOTA_CN', 'NU_NOTA_CH', 'NU_NOTA_LC', 'NU_NOTA_MT',\n                                'NU_NOTA_REDACAO','MEDIA_NOTAS','CLASSE'],\n                               inplace=False, axis=1).values\n        \n        self.y = df_class['CLASSE'].values\n    \n    \n    def scale_data(self, X):\n        \n        self.X_scaler = StandardScaler()  \n        \n        X_scaled = self.X_scaler.fit_transform(X)\n    \n        return X_scaled\n    \n        \n    def select_features(self, model, X, y):\n    \n        sfm = SelectFromModel(model, threshold=0.04)\n        sfm.fit(X, y.ravel())\n        X = sfm.transform(X)\n        \n        return X\n    \n    \n    def tree(self, X, base=True):\n        \"\"\"\n        Adaboost + Decision Tree Classifier\n        \"\"\"\n\n        kwargs = {'tree':{'random_state': 1, 'max_depth':3, 'criterion':'entropy'}, 'boost':{ 'n_estimators':1, 'learning_rate':1e-1}} if not base else {'tree':{},'boost':{}}\n                \n        X_train, X_test, y_train, y_test = train_test_split(X, self.y, random_state=1, test_size=0.35)\n        \n        classificador = AdaBoostClassifier(DecisionTreeClassifier(**kwargs['tree']),**kwargs['boost'])\n        \n        classificador.fit(X_train, y_train.ravel())\n        \n        score = classificador.score(X_test, y_test)\n        \n        print(f'Adaboost + Decision Tree Classifier - Score = {score}')\n        \n        previsoes = classificador.predict(X_test)\n        \n        return classificador\n    \n    \n    def forest(self, X, base=True):\n        \"\"\"\n        Random Forest Classifier\n        \"\"\"\n        \n        kwargs = {'random_state':1,'n_jobs':-1,'n_estimators':10, 'criterion':'gini', 'max_depth':1, 'max_features':'auto','ccp_alpha':1e-1,'min_impurity_decrease':1e-1} if not base else {}\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, self.y, random_state=1, test_size=0.35)\n        \n        classificador = RandomForestClassifier(**kwargs)\n        \n        classificador.fit(X_train, y_train.ravel())\n        \n        score = classificador.score(X_test, y_test)\n        \n        print(f'Random Forest Classifier - Score = {score}')\n        \n        previsoes = classificador.predict(X_test)\n        \n        return classificador\n    \n    \n    def svc(self, X, base=True):\n        \"\"\"\n        SVC Support Vector Classifier\n        \"\"\"\n        X_train, X_test, y_train, y_test = train_test_split(X, self.y, random_state=1, test_size=0.25)\n        \n        kwargs = {'max_iter':1500,'kernel':'rbf','tol':1e-3,'C':5.7,'decision_function_shape':'ovr', 'degree':1,'gamma':'scale','coef0':1, 'probability':True} if not base else {}\n        \n        classificador = SVC(**kwargs)\n        \n        classificador.fit(X_train, y_train.ravel())\n        \n        previsoes = classificador.predict(X_test)\n        \n        score = classificador.score(X_test, y_test)\n        \n        print(f'SVC Support Vector Classifier - Score = {score}')\n        \n        previsoes = classificador.predict(X_test)\n            \n        return classificador\n    \n    \n    def mlp(self, X, base=True):\n\n        \"\"\"\n        Neural Network - MLP Classifier\n        \"\"\"\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, self.y, random_state=1, test_size=0.35)\n        \n        kwargs = {'max_iter':500,'early_stopping':True,'learning_rate_init':0.01,'activation':'tanh', 'solver':'adam','beta_1':0.5,'beta_2':0.1,'tol':1e-1, 'shuffle':True, 'learning_rate':'adaptive', 'hidden_layer_sizes':(int((X.shape[1])\/2)+1,int((X.shape[1])\/2)+1)} if not base else {}   \n        \n        classificador = MLPClassifier(**kwargs)\n        \n        classificador.fit(X_train, y_train.ravel())\n        \n        previsoes = classificador.predict(X_test)\n        \n        score = classificador.score(X_test, y_test)\n        \n        print(f'Neural Network - MLP Classifier - Score = {score}')\n        \n        previsoes = classificador.predict(X_test)\n            \n        return classificador\n    \n    \n    def gradient(self, X, base=True):\n        \"\"\"\n        Gradient Boosting \n        \"\"\"\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, self.y, random_state=1, test_size=0.35)\n        \n        kwargs = {'random_state':1,'criterion':'friedman_mse','max_features':'sqrt','loss':'deviance','learning_rate':0.01,'tol':1e-1, 'max_depth':3, 'n_estimators':3} if not base else {}   \n        \n        classificador = GradientBoostingClassifier(**kwargs)\n        \n        classificador.fit(X_train, y_train.ravel())\n        \n        previsoes = classificador.predict(X_test)\n        \n        score = classificador.score(X_test, y_test)\n        \n        print(f'Gradient Boosting - Score = {score}')\n        \n        previsoes = classificador.predict(X_test)\n            \n        return classificador\n    \n    def voting(self, X, y, models):\n        \"\"\"\n        Voting Classifier\n        \"\"\"\n        X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1, test_size=0.35)\n        \n        ecls = VotingClassifier(estimators=models, voting='soft', flatten_transform=True)\n        \n        ecls.fit(X_train, y_train.ravel())\n        \n        previsoes = ecls.predict(X_test)\n        \n        score = ecls.score(X_test, y_test)\n        \n        print(score)\n        \n        previsoes = ecls.predict(X_test)\n        \n        return ecls","3db5124e":"import numpy as np\n\nfrom sklearn.model_selection import StratifiedKFold\nfrom scipy.stats import wilcoxon, friedmanchisquare, rankdata\nfrom Orange.evaluation import compute_CD, graph_ranks\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import mean_absolute_error\n\n\n\nclass Validator:    \n    \n    \n    def __init__(self):\n        \n        self.kfold              = StratifiedKFold\n        self.wilcoxon           = wilcoxon\n        self.friedmanchisquare  = friedmanchisquare\n        self.rankdata           = rankdata\n        self.resultados_mean    = list([])\n        \n        \n        \n    def get_results(self, X, y, models):\n        \n        dct_results = {}\n        \n        dct_results['tree']     = self.validate_models(X,y,models[0])\n        dct_results['forest']   = self.validate_models(X,y,models[1])\n        dct_results['svr']      = self.validate_models(X,y,models[2])\n        dct_results['mlp']      = self.validate_models(X,y,models[3])\n        dct_results['gr']       = self.validate_models(X,y,models[4])\n        \n        results = np.c_[dct_results['tree'], dct_results['forest'], dct_results['svr'], dct_results['mlp'], dct_results['gr']]\n        \n        return results, dct_results\n    \n    \n    \n    def validate_models(self, X, y, model, resultados=[]) -> np.array:\n        self.resultados_mean    = []\n        \n        for i in range(0,30):\n            \n            kfold = self.kfold(n_splits=10, shuffle=True, random_state=i)\n            \n            for indice_treinamento, indice_teste in kfold.split(X, np.zeros(shape=(X.shape[0], 1))):\n                \n                model.fit(X[indice_treinamento], y[indice_treinamento].ravel())\n                # print(model.score(X[indice_teste], y[indice_teste].ravel()))\n                mae = mean_absolute_error(y[indice_teste], model.predict(X[indice_teste]))\n        \n            resultados.append(mae*(-1))\n            result = np.asarray(resultados)\n            self.resultados_mean.append(result.mean())\n        \n        return self.resultados_mean\n    \n    \n        \n    def wilcoxon_method(self, df_results):\n        \n        models_par = {}\n        \n        results = df_results.mean().sort_values(ascending=False).head(2)\n        \n        models_par[results.index[0]] = results[results.index[0]]\n        models_par[results.index[1]] = results[results.index[1]]\n        \n        result_wilcox = wilcoxon(df_results[results.index[0]],df_results[results.index[1]],zero_method='pratt')\n        \n        return result_wilcox, models_par\n    \n    \n    \n    \n    def compare_results(self, results, lst_models):\n        \n        # wil_result, models_par = self.wilcoxon_method(results, lst_models)\n        \n        fried_result = self.friedmanchisquare(*results)\n        \n        ranks = np.array([self.rankdata(-p) for p in results])\n        \n        # Calculating the average ranks.\n        average_ranks = np.mean(ranks, axis=0)\n\n        names = [lst_models[i]+' - '+str(round(average_ranks[i], 3)) for i in range(len(average_ranks))]\n        \n        # This method computes the critical difference for Nemenyi test with alpha=0.1.\n        # For some reason, this method only accepts alpha='0.05' or alpha='0.1'.\n        cd = compute_CD(average_ranks, n=len(results),alpha='0.05', test='nemenyi')\n        \n        # This method computes the critical difference for Bonferroni-Dunn test with alpha=0.05.\n        # For some reason, this method only accepts alpha='0.05' or alpha='0.1'.\n        cd1 = compute_CD(average_ranks, n=len(results), alpha='0.05', test='bonferroni-dunn')\n        \n        return fried_result, ranks, names, (cd, cd1), average_ranks\n    \n    \n    \n    def plot_comparisons(self, fried_result, names, cd, cd1, average_ranks):\n    \n        # This method generates the plot.\n        graph_ranks(average_ranks, names=names,\n                        cd=cd, width=10, textspace=1.5)\n        \n        plt.title(f'Friedman-Nemenyi={round(fried_result.pvalue, 4)}\\nCD={round(cd, 3)}')\n        plt.show()\n        \n        # This method generates the plot.\n        graph_ranks(average_ranks, names=names,\n                        cd=cd1, cdmethod=0, width=10, textspace=1.5)\n        plt.title(f'Bonferroni-Dunn\\nCD={round(cd1, 3)}')\n        plt.show()\n    \n        return\n    \n    \n            \n    def visualizar_resultados_validacao(self, wilcox, friedman, models_par, cds, average_ranks, algorithms):\n        \n        print('\\n'.join('{} average rank: {}'.format(a, r) for a, r in zip(algorithms, average_ranks)))\n        \n        par = 'n\u00e3o s\u00e3o equivalentes' if wilcox.pvalue >= 0.05 else 's\u00e3o equivalentes'\n        # Imprime a conclus\u00e3o da compara\u00e7\u00e3o\n        print(f\"\\nDe acordo com o resultado do 'Wilcoxon signed-rank' com o p-value = {round(wilcox.pvalue, 4)}.\\n\\\nOs modelos treinados:{list(models_par.items())[0]} e {list(models_par.items())[1]} {par}.\\n\\\nConsiderando o n\u00edvel de signific\u00e2ncia de (\u03b1) = 0.05.\\n\\n\\\n'The Wilcoxon signed-rank test was not designed to compare multiple random variables.\\n\\\nSo, when comparing multiple classifiers, an 'intuitive' approach would be to apply the Wilcoxon test to all possible pairs.\\n\\\nHowever, when multiple tests are conducted, some of them will reject the null hypothesis only by chance (Dem\u0161ar, 2006).\\n\\\nFor the comparison of multiple classifiers, Dem\u0161ar (2006) recommends the Friedman test.'\\n\\n\")\n        \n        rank = 'n\u00e3o s\u00e3o equivalentes' if friedman.pvalue <= 0.05 else 's\u00e3o equivalentes'\n        print(f\"O teste de Friedman calculou o p-value = {round(friedman.pvalue, 4)}.\\n\\\nConsiderando o n\u00edvel de signific\u00e2ncia de (\u03b1) = 0.05, todos os modelos {rank}.\\n\\\nTendo em vista o Critical Distance (CD), somente os modelos com a diferen\u00e7a entre as m\u00e9dias maior que, {cds[0]} Friedman-Nemenyi \/ {cds[1]} bonferroni-dunn podem ser considerados pior(es) e melhor(es).\\n\\n\\\n'Considering that the null-hypothesis was rejected, we usually have two scenarios for a post-hoc test (Dem\u0161ar, 2006):\\n\\\nAll classifiers are compared to each other. In this case we apply the Nemenyi post-hoc test.\\n\\\nAll classifiers are compared to a control classifier. In this scenario we apply the Bonferroni-Dunn post-hoc test.'\")\n        \n        print('Os testes foram realizados para validar classificadores no caso de uso citado por Dem\u0161ar em 2006. Sendo os mesmos crit\u00e9rios de avalia\u00e7\u00e3o, maior melhor, a m\u00e9trica foi adaptada para obedecer esses crit\u00e9rios e obter os resultados corretos. O \"Mean Absolute Error\" foi convertido para \"Negative Mean Absolute Error\".')\n        \n        return","63511e0e":"\"\"\"\nInstancia as classes com as fun\u00e7\u00f5es inerentes ao projeto,\no c\u00f3digo das classes foram inseridas ao final para facilitar visualiza\u00e7\u00e3o\"\"\"\nmunicipio = Municipio()\nanalise = Analise()\n\ndf = municipio.gerar_dados()                    # Carrega os dados selecionados das tabelas\ndf.info()                                       # Imprime as informa\u00e7\u00f5es das features","2c6bfab0":"\"\"\"\nOs valores NaN foram substituidos por 0 considerando que existem.\nDe acordo com o dicio\u00e1rio do dataset, as quantidades n\u00e3o informadas ou que de fato n\u00e3o existem\nforam registradas como h\u00edfen(NaN) \"\"\"\ndf = municipio.tratar_dados(list(df.columns[5:-10]))    # Trata os valores faltantes e seus tipos\ndf = municipio.calcular_totais()                        # Calcula os totais de Escolas, Docentes e Matr\u00edculas\ndf = municipio.calcular_densidade()                     # Calcula as densidades de Habitantes, Escolas, Docentes e Matr\u00edculas\ndf = municipio.classificar()                            # Classifica os resultados satisfat\u00f3rios como 2\n                                                        # Classifica os resultados regulares como 1\n                                                        # Classifica os resultados insatisfat\u00f3rios como 0\ndf.shape                                                # Imprime o formato final do DataFrame","6d92f9ed":"df.head(10)","0ec82df5":"analise.compara_dispersao(df)","c6154296":"\"\"\"Plota a compara\u00e7\u00e3o das vari\u00e1veis\nDocentes \/ Escolas maior e Matr\u00edculas \/ Docentes\ne as m\u00e9dias de 2014 a 2018 dos 12 munic\u00edpios\nque obtiveram os melhores resultados\"\"\"\nanalise.compara_melhores(df)","076abad5":"\"\"\"Plota a compara\u00e7\u00e3o das vari\u00e1veis\nDocentes \/ Escolas e Matr\u00edculas \/ Docentes\ne as m\u00e9dias de 2014 a 2018 dos 12 munic\u00edpios\nque obtiveram os piores resultados\"\"\"\nanalise.compara_piores(df)","ad8e976a":"\"\"\"\nPlota a compara\u00e7\u00e3o das vari\u00e1veis Docentes \/ Escolas e Matr\u00edculas \/ Docentes\ne as m\u00e9dias de 2014 a 2018 dos 30 munic\u00edpios com os piores e melhores resultados\"\"\"\nanalise.compara_docente(df)","43aa0bc0":"\"\"\"\nPlota a compara\u00e7\u00e3o das vari\u00e1veis Habitantes \/ Escolas e Matr\u00edculas \/ Escolas\ne as m\u00e9dias de 2014 a 2018 dos 30 munic\u00edpios com os piores e melhores resultados\"\"\"\nanalise.compara_escola(df)","7eced3e0":"df.drop(df.columns[6:49], inplace=True, axis=1)        # Remove as features irrelevantes\nanalise.compara_correlacao(df)  ","3d24c1a9":"analise.compara_densidade(df)","b37bbe65":"analise.get_outliers(df)","c033c5f5":"regressor = Regressor(df)\n\"\"\"\nO pr\u00e9-processamento \u00e9 feito pelo StandardScaler.\n\"\"\"\n\nX, y = regressor.scale_data(regressor.X, regressor.y)   # Escalona os dados","d7927136":"\"\"\"\nPara sele\u00e7\u00e3o dos dados ser\u00e1 utilizado o Sklearn SelectFromModel com base no Atributo 'feature_importances_'.\n\"\"\"\nregressor_dt    = regressor.tree(X, y, plot=False)\nregressor_rf    = regressor.forest(X, y, plot=False)\nregressor_sv    = regressor.svr(X, y, plot=False)\nregressor_mlp   = regressor.mlp(X, y, plot=False)\nregressor_gr    = regressor.gradient(X, y, plot=False)","69213fee":"import pandas as pd\n\nvalidator = Validator()\n\ndf_results = pd.read_csv('\/kaggle\/input\/estrutura-educacional-x-desempenho-enem\/results.csv')\ndf_results.mean(axis=0)","4a3ea2e5":"result_fried, ranks, names, cds, average_ranks = validator.compare_results(df_results.values, df_results.columns)\n    \nvalidator.plot_comparisons(result_fried, names, cds[0], cds[1], average_ranks)\n    \nresult_wilcox, models_par = validator.wilcoxon_method(df_results)\n\nvalidator.visualizar_resultados_validacao(result_wilcox, result_fried, models_par, cds, average_ranks, list(df_results.columns))","792a9930":"regressor_vr    = regressor.voting(X, y, [('rf',regressor_rf),('svr',regressor_sv)]) # Combina os 2 melhores modelos","833ff752":"X = regressor.select_features(regressor_rf, X, y) # Seleciona as features com import\u00e2ncia > 0,004","b9574992":"lst_features = df.drop(['ano', 'NM_UF_SIGLA', 'COD. MUNIC', 'NOME_MUNIC\u00cdPIO','NU_NOTA_CN', 'NU_NOTA_CH', 'NU_NOTA_LC', 'NU_NOTA_MT','NU_NOTA_REDACAO','MEDIA_NOTAS'], inplace=False, axis=1).columns\nprint(*[lst_features[i] for i, val in enumerate(regressor_rf.feature_importances_) if val > 0.004], sep=' - ')","c8519dfa":"regressor_dt_tn    = regressor.tree(X, y, base=False)\nregressor_rf_tn    = regressor.forest(X, y, base=False)\nregressor_sv_tn    = regressor.svr(X, y, base=False)\nregressor_mlp_tn   = regressor.mlp(X, y, base=False)\nregressor_gr_tn    = regressor.gradient(X, y, base=False)","4a545c32":"df_results_tn = pd.read_csv('\/kaggle\/input\/estrutura-educacional-x-desempenho-enem\/results_tn.csv')\ndf_results_tn.mean(axis=0).sort_values(ascending=False)","62aee1fc":"result_fried, ranks, names, cds, average_ranks = validator.compare_results(df_results_tn.values, list(df_results_tn.columns))\n    \nvalidator.plot_comparisons(result_fried, names, cds[0], cds[1], average_ranks)\n\nresult_wilcox, models_par = validator.wilcoxon_method(df_results_tn)\n\nvalidator.visualizar_resultados_validacao(result_wilcox, result_fried, models_par, cds, average_ranks, list(df_results_tn.columns))","f6945042":"regressor_vr_tn    = regressor.voting(X, y,[('rf',regressor_rf_tn),('mlp', regressor_mlp_tn)])","a851766e":"regressor_vr_tn_rank    = regressor.voting(X, y,[('gr',regressor_gr_tn),('mlp', regressor_mlp_tn)])","9ae6bb03":"df_results_final = pd.read_csv('\/kaggle\/input\/estrutura-educacional-x-desempenho-enem\/results_final.csv')\ndf_results_final.mean(axis=0)","8bd8a87a":"result_fried, ranks, names, cds, average_ranks = validator.compare_results(df_results_final.values, list(df_results_final.columns))\n    \nvalidator.plot_comparisons(result_fried, names, cds[0], cds[1], average_ranks)\n\nresult_wilcox, models_par = validator.wilcoxon_method(df_results_final)\n\nvalidator.visualizar_resultados_validacao(result_wilcox, result_fried, models_par, cds, average_ranks, list(df_results_final.columns))\n","a2bfef5f":"regressor_vr_tn = regressor.voting(X, y,[('rf',regressor_rf_tn), ('mlp',regressor_mlp_tn)], select=True)","5de75899":"classificador = Classificador(df)\n\nX = classificador.scale_data(classificador.X)\n\nclass_dt   = classificador.tree(X, base=False)\nclass_rf   = classificador.forest(X, base=False)\nclass_gb   = classificador.gradient(X, base=False)\nclass_mlp  = classificador.mlp(X, base=False)\nclass_sv   = classificador.svc(X, base=False)","262cc01f":"Carrega a classe 'Regressor'","f3b70741":"Ap\u00f3s testar v\u00e1rias combina\u00e7\u00f5es, o \u00faltimo teste ser\u00e1 feito com o baseline e os dois melhores resultados. RandomForest + MLPRegressor, com os melhores resultados e o GradientBoosting + MLPRgressor, recomendados pelo rank.","c5ff3fad":"O mapa de correla\u00e7\u00f5es indica que de fato n\u00e3o correla\u00e7\u00e3o relevante entre as vari\u00e1veis pertinentes \u00e0s hip\u00f3teses levantadas para este estudo, com exce\u00e7\u00e3o das pr\u00f3prias notas usadas para calcular as m\u00e9dias, a vari\u00e1vel menos irrelevante \u00e9 a propor\u00e7\u00e3o de docentes por escolas com valor de 0,21. Algumas vari\u00e1veis possuem correla\u00e7\u00f5es que seguem a coer\u00eancia de sua natureza, como por exemplo, matr\u00edcula\/escola, hab\/escola e matr\u00edcula\/docente. Assim como as vari\u00e1veis citadas, a popula\u00e7\u00e3o estimada tem forte rela\u00e7\u00e3o com total de escolas, total de docentes e total de matr\u00edculas.","3e8a8c3d":"#### O estudo tem como objetivo principal verificar a correla\u00e7\u00e3o entre a estrutura educacional do ensino fundamental dos estados e DF e o desempenho(Mediana) no exame nacional do ensino m\u00e9dio(ENEM).\n\nO estudo ser\u00e1 dividido em 2 partes, an\u00e1lise dos dados munipais e unidades federativas.\n\nA base de dados cont\u00e9m as medianas de cada mat\u00e9ria de cada unidade federativa e seus munic\u00edpios de 2010 a 2018, o desempenho ser\u00e1 comparado pela m\u00e9dia simples das medianas de cada mat\u00e9ria de cada estado e DF em cada ano. Tamb\u00e9m ser\u00e3o utilizados os dados do censo demogr\u00e1fico de 2019.\n\n<ol>\n    <li>A Exten\u00e7\u00e3o territorial influencia no desempenho?<\/li>\n    <li>A densidade populacional(hab\/km\u00b2) influencia no desempenho?<\/li>\n    <li>A propor\u00e7\u00e3o de habitantes\/Escola influencia no desempenho?<\/li>\n    <li>A propor\u00e7\u00e3o de matr\u00edculas\/Escola influencia no desempenho?<\/li>\n    <li>A propor\u00e7\u00e3o de docentes\/Escola influencia no desempenho?<\/li>\n    <li>A propor\u00e7\u00e3o de docentes\/matr\u00edcula influencia no desempenho?<\/li>\n<\/ol>\n\nObs.: N\u00e3o ser\u00e3o consideradas as caracter\u00edsticas individuais dos estudantes. ","7a1502d4":"Os resultados acima foram obtidos executando o c\u00f3digo abaixo:\n\n`\nimport numpy as np`\n\n`\ndct_results_final = {'baseline': validator.validate_models(X,y,regressor_vr),\n                 'Select_Valid': validator.validate_models(X,y,regressor_vr_tn_gr_mpl),\n                 'Best_result': validator.validate_models(X,y,regressor_vr_tn_rf_mpl)}`\n                 \n`\nresults_final = np.c_[results_final['baseline'],results_final['Select_Valid'],results_final['Best_result']]`\n\n`\ndf_results_final = pd.DataFrame(columns=list(dct_results_final.keys()), data=results_final)\ndf_results_final.to_csv('CSVs\/results_final.csv', index=False)`\n\n=======================================================================================================================\nbaseline = Combina\u00e7\u00e3o do Random Forest com  a m\u00e9dia de MAE = -0.5050664249685748 e Support Vector com  a m\u00e9dia de MAE = -0.5060723702424922. (Hiperpar\u00e2metros padr\u00f5es)\n\nSelect_Valid = Combina\u00e7\u00e3o do Gradient Boosting com  a m\u00e9dia de MAE = -0.512804895655462 e Multi-layer Perceptron com  a m\u00e9dia de MAE = -0.5179152628027912. (Hiperpar\u00e2metros selecionados pelo *RandomizedSearchCV*)\n\nBest_result = Combina\u00e7\u00e3o do Random Forest com a m\u00e9dia de MAE = -0.520176838214957 e Multi-layer Perceptron com a m\u00e9dia de MAE = -0.5179152628027912. (Hiperpar\u00e2metros selecionados pelo *RandomizedSearchCV*)\n\n**Obs.: Os valores das m\u00e9dias de erro absoluto apresentados nos teste s\u00e3o dos valores escalonados e operadores invertidos (Negative Mean Absolute Error).**","7c664c61":"O c\u00f3digo abaixo cria o DataFrame com os resultados obtidos pelo *'StratifiedKFold'* fazendo 30 testes no modelo *'Cross Validate'* com 10 *'Splits'*\n\n`\nmodels = [regressor_dt,regressor_rf,regressor_sv,regressor_mlp, regressor_gr]\nresults, dct_results = validator.get_results(X, y, models)\ndf_results = pd.DataFrame(columns=list(dct_results.keys()), data=results)`","f35f230e":"As vari\u00e1veis comparadas n\u00e3o demonstram nenhum padr\u00e3o em rela\u00e7\u00e3o \u00e0s melhores m\u00e9dias obtidas nos 5 anos analisados, mas se pode observar que nem sempre os munic\u00edpios obtiveram bons resultados.","ac951d64":"Carrega a classe 'Municipio'","9648bfa4":"#### Estrutura educacional X Desempenho ENEM: [Kaggle](https:\/\/www.kaggle.com\/andraguiar\/estrutura-educacional-x-desempenho-enem)\n\nA base escolhida \u00e9 a de Censo demogr\u00e1fico e sin\u00f3pses do ENEM, diponibilizadas nos sites dos \u00f3rg\u00e3os competentes ([IBGE](http:\/\/www.dados.gov.br\/dataset\/cd-censo-demografico) e [INEP](http:\/\/portal.inep.gov.br\/sinopses-estatisticas-da-educacao-basica)) para conulta p\u00fablica. Os dados foram coletados entre 2010 e 2019 contendo a regi\u00e3o geogr\u00e1fica, UF e dados estat\u00edsticos do Brasil e do exame nacional do ensino m\u00e9dio. Ser\u00e3o consideradas somente as informa\u00e7\u00f5es estruturais educacionais e desempenho no ENEM para identificar poss\u00edveis correla\u00e7\u00f5es entre quantidade de escolas, docentes e matr\u00edculas, e desempenho no exame(ENEM). Tendo em vista que ainda n\u00e3o foram disponibilizados os dados de 2019, ser\u00e3o feitas algumas predi\u00e7\u00f5es e definidos idicadores de acordo com a mediana utilizando regressores e classificadores supervisionados, juntamente com t\u00e9cnicas de minera\u00e7\u00e3o e pr\u00e9-processamento de dados.","be7dacf0":"Acima est\u00e3o as m\u00e9dias negativas dos erros absolutos dos valores escalonados.","5bdcf1a6":"Carrega a classe 'Analise'","d7058da6":"O gr\u00e1fico acima compara as 500 melhores m\u00e9dias com as 500 piores m\u00e9dias. E n\u00e3o h\u00e1 linearidade aparente entre as m\u00e9dias e as outras vari\u00e1veis, mas d\u00e1 para notar que a maioria dos munic\u00edpios que obtiveram as melhores notas t\u00eam a densidade de Docentes \/ Escolas maior e de Matr\u00edculas \/ Docentes menor que os munic\u00edpios com os piores resultados. Mas n\u00e3o se pode dizer que h\u00e1 correla\u00e7\u00e3o relevante entre os resultados e essas vari\u00e1veis.","55e11b5e":"Os mapas plotados acima ilustram as densidades das vari\u00e1veis observadas e analisadas anteriormente identificando os 5 melhores e 5 piores resultados(Mediana das m\u00e9dias acumuladas de 2014 a 2018).\n\nO primeiro indica que a densidade de docentes por escolas no norte e nordeste s\u00e3o consider\u00e1velmente menores e matr\u00edculas por docentes maior que no restante no pa\u00eds.\n\nO segundo indica que a densidade de matr\u00edculas e habitantes por escolas s\u00e3o maiores no sul e sudeste que no restante no pa\u00eds.\n\nO terceiro indica que os melhores resultados est\u00e3o no sul e sudeste.\n\nAnalisando os tr\u00eas mapas, pode-se dizer que os melhores resultados est\u00e3o onde h\u00e1 a propor\u00e7\u00e3o de habitantes e docentes por escolas maior, matr\u00edculas por docentes e escolas menor, mas n\u00e3o h\u00e1 comprova\u00e7\u00e3o matem\u00e1tica de correla\u00e7\u00e3o entre as vari\u00e1veis analisadas. H\u00e1 alguns indicadores que n\u00e3o est\u00e3o relacionados com a estrutura educacional, como pode ser visto no terceiro mapa, alguns dos piores resultados est\u00e3o em regi\u00f5es isoladas no mapa, hipoteticamente o dif\u00edcil acesso \u00e0s escolas esteja relacionado aos maus resultados juntamente com outras vari\u00e1veis estruturais gerais da regi\u00e3o que n\u00e3o est\u00e3o sendo levadas em considera\u00e7\u00e3o neste estudo.","15570515":"Os modelos para o baseline foram selecionados a partir da m\u00e9dia de 30 testes utilizando *Cross Validation 'StratifiedKFold'*, levando em considera\u00e7\u00e3o os crit\u00e9rios do *Friedman Test*.\n\nO regressor com o melhor resultado foi o Random Forest e est\u00e1 definido como refer\u00eancia para sele\u00e7\u00e3o das features features com relev\u00e2ncia > 0.004.","32efbe19":"Carrega a classe 'Validator'","f4d49ad1":"Os dados faltantes foram considerados como 0. Ex.: Se n\u00e3o havia quantidade de escolas especiais registradas \u00e9 porque n\u00e3o foram  informadas ou n\u00e3o existiam de fato.","a5a35db6":"Como observado no gr\u00e1fico anterior, pelo gr\u00e1fico acima fica n\u00edtido que os munic\u00edpios com os melhores resultados t\u00eam a propor\u00e7\u00e3o de docentes por escolas maior que matr\u00edculas por docentes. Apesar disso, ainda n\u00e3o se pode considerar rela\u00e7\u00e3o direta com os bons ou maus resultados dos munic\u00edpios.","38bfdfea":"O gr\u00e1fico acima compara a propor\u00e7\u00e3o de habitantes por escolas e matr\u00edculas por escolas e mostra que os melhores resultados foram obtidos pelos munic\u00edpios com as maiores quantidades de habitantes por escolas, mas n\u00e3o se pode considerar a rela\u00e7\u00e3o direta com os bons ou maus resultados.","a532a1b9":"Carrega a classe 'Classificador'","0976bd05":"Apesar do teste \u00fanico ter apresentado o RandomForest e o MLPRegressor com os melhores resultados, os 30 testes *'Wilcoxon signed-rank', 'Friedman-Nemenyi' e 'Bonferroni-dunn'* apresentaram resultados diferentes.","457e5ce3":"O c\u00f3digo abaixo cria o DataFrame com os resultados obtidos pelo *'StratifiedKFold'* fazendo 30 testes no modelo *'Cross Validate'* com 10 *'Splits'*\n\n`\nmodels = [regressor_dt_tn,regressor_rf_tn,regressor_sv_tn,regressor_mlp_tn, regressor_gr_tn]\nresults, dct_results = validator.get_results(X, y, models)\ndf_results_tn = pd.DataFrame(columns=list(dct_results.keys()), data=results)`","9fd36c7e":"Define o baseline com todas a features escalonadas e hiperpar\u00e2metros padr\u00f5es dos modelos selecionados para combina\u00e7\u00e3o, que apesar do teste \u00fanico indicar o RandomForestRegressor e MLPRegressor, ap\u00f3s realizar os 30 testes os resultados mais equilibrados foram do RandomForestRegressor e Support Vector Regressor(SVR).","29143463":"Comparando os 12 munic\u00edpios com as melhores m\u00e9dias com os 12 munic\u00edpios j\u00e1 se percebe que a propor\u00e7\u00e3o de matr\u00edculas por docente \u00e9 maior na maioria dos munic\u00edpios com os piores resultados, enquanto a propor\u00e7\u00e3o de docentes por escolas \u00e9 maior nos munic\u00edpios com os melhores resultados, mas n\u00e3o se pode considerar como rela\u00e7\u00e3o direta com os resultados ruins."}}