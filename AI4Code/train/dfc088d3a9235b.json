{"cell_type":{"2dd97525":"code","8e661cff":"code","eaf43c93":"code","b4d7d168":"code","2ebf4327":"code","4f75e252":"code","500cfc6c":"code","e47443b3":"code","28cfcc09":"code","6bfccba4":"code","983949f7":"code","da212c84":"code","8c6f9d7c":"code","bfa481e6":"code","72c905b9":"code","37762c5e":"code","895ac36e":"code","65a14f14":"code","efb84e93":"code","3ef40ce3":"code","716f8796":"code","f0d42e43":"code","f04120ef":"code","fa023551":"code","08542fc8":"code","21e1864f":"code","02dbdcd0":"code","9404db7f":"code","3295c250":"code","265a8f3a":"code","9c5aac59":"code","3a626ac1":"code","db9af5ab":"code","8c79d28a":"code","2f752d5d":"markdown","cc125fa0":"markdown","a5399ee8":"markdown","f7d4381a":"markdown","59109838":"markdown"},"source":{"2dd97525":"\n\nimport numpy as np \nimport pandas as pd\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n","8e661cff":"import nltk\nfrom nltk.tokenize import word_tokenize\nimport string\nimport pandas as pd\nfrom bs4 import BeautifulSoup","eaf43c93":"nltk.download('all')","b4d7d168":"dataset = pd.read_csv('..\/input\/imdb-dataset-of-50k-movie-reviews\/IMDB Dataset.csv')","2ebf4327":"# First five rows of the dataset\n\ndataset.head()","4f75e252":"print(\"Shape of the dataset = \", dataset.shape)\nprint(\"# of rows = \", dataset.shape[0])\nprint(\"# of columns = \", dataset.shape[1])","500cfc6c":"# Let's check the no. of positive and negative reviews in the dataset\n\ndataset.sentiment.value_counts()","e47443b3":"# Now we are going to label the sentiment column \n# For positive - 1\n# For negetive - 0\n\ndataset['sentiment'] = pd.get_dummies(dataset['sentiment'],drop_first = True)","28cfcc09":"dataset.head()","6bfccba4":"from nltk import WordNetLemmatizer, pos_tag, word_tokenize\nfrom nltk.corpus import wordnet\n\ndef get_wordnet_pos(tag):\n    if tag.startswith('J'):\n        return wordnet.ADJ\n    elif tag.startswith('V'):\n        return wordnet.VERB\n    elif tag.startswith('N'):\n        return wordnet.NOUN\n    elif tag.startswith('R'):\n        return wordnet.ADV\n    else:\n        return wordnet.NOUN","983949f7":"wn = nltk.WordNetLemmatizer()\nstopwords = nltk.corpus.stopwords.words('english')\n\ndef text_preprocess(text):\n    soup = BeautifulSoup(text, 'html.parser').text\n    no_punctuation = \"\".join([c for c in soup if c not in string.punctuation]).lower()\n    tokens = pos_tag(word_tokenize(no_punctuation))\n    clean_text = [word for word in tokens if word not in stopwords]\n    lemma = [wn.lemmatize(word[0],get_wordnet_pos(word[1])) for word in clean_text]\n    lemma = ' '.join(lemma)\n    return lemma","da212c84":"dataset['clean_review'] = dataset['review'].apply(lambda x: text_preprocess(x))","8c6f9d7c":"dataset.head()","bfa481e6":"from collections import Counter\n\ndef counter_word(text):\n  count = Counter()\n  for row in text.values:\n    for word in row.split():\n      count[word] += 1\n  return count\n\ntext = dataset.clean_review\ncounter = counter_word(text)","72c905b9":"print(f\"There are {len(counter)} unique words in the clean_review column\")","37762c5e":"num_words = len(counter)\nmax_length = 100","895ac36e":"lst = []\nfor i in range(len(dataset)):\n    lst.append(len(dataset.clean_review[i]))\n        ","65a14f14":"counter","efb84e93":"len(dataset.clean_review[1])","3ef40ce3":"from keras.preprocessing.text import Tokenizer\n\ntokenizer = Tokenizer(num_words=num_words)\ntokenizer.fit_on_texts(dataset.clean_review)","716f8796":"word_index = tokenizer.word_index","f0d42e43":"dataset_sequences = tokenizer.texts_to_sequences(dataset.clean_review)\n","f04120ef":"from keras.preprocessing.sequence import pad_sequences\n\ndataset_padded = pad_sequences(dataset_sequences, maxlen=max_length,padding='post',truncating='post')","fa023551":"dataset_padded.shape","08542fc8":"y = dataset['sentiment']","21e1864f":"from sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test = train_test_split(dataset_padded,y,test_size=0.25,random_state=0)","02dbdcd0":"X_train.shape,X_test.shape,y_train.shape,y_test.shape","9404db7f":"from keras.models import Sequential\nfrom keras.layers import Embedding,Dropout,Dense,LSTM\nfrom keras.initializers import Constant\nfrom keras.optimizers import Adam\n\nmodel = Sequential()\n\nmodel.add(Embedding(num_words,50, input_length = max_length))\nmodel.add(LSTM(64,dropout=0.5))\nmodel.add(Dense(256,activation='relu'))\nmodel.add(Dense(1,activation='sigmoid'))\n\noptimizer = Adam(learning_rate=3e-4)\n\nmodel.compile(loss='binary_crossentropy',optimizer=optimizer,metrics=['accuracy'])\nmodel.summary()","3295c250":"model.fit(X_train,y_train,batch_size=128,epochs=10,validation_data=(X_test,y_test))","265a8f3a":"score = model.evaluate(X_test,y_test)\nscore","9c5aac59":"dataset[dataset['sentiment']==0]","3a626ac1":"text=dataset.review[49996]","db9af5ab":"testing_text = text\nclean_testing_text = text_preprocess(testing_text)\nsentence = np.array([clean_testing_text])\nsentence_sequence = tokenizer.texts_to_sequences(sentence)\nsentence_padded =  pad_sequences(sentence_sequence,maxlen=max_length)\n\npred = model.predict(sentence_padded)","8c79d28a":"if pred >= 0.5:\n    print('Positive')\nelse:\n    print('Negative')","2f752d5d":"## Import Data","cc125fa0":"From the above result we can say that the dataset is balanced","a5399ee8":"## Let's Explore the Dataset\n","f7d4381a":"## Required library","59109838":"## Text Cleaning\n\n"}}