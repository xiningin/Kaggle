{"cell_type":{"ffd69061":"code","2f5a1003":"code","6e2d1fcc":"code","c2f657f7":"code","45b5e1f7":"code","eeeb2ae3":"code","2ddfb060":"code","b962adc4":"code","0bbc5bd3":"code","38aa0d07":"code","7180a995":"code","57f19b81":"code","c105aa6b":"code","085cb6cb":"code","c3c6aaef":"code","9ee0b33b":"code","e5f6ab83":"code","6091a7f0":"code","2618dbe3":"code","42b7f0e8":"code","497a8cdb":"code","2e22b620":"code","217b6288":"code","06206780":"code","7dd6fd4f":"markdown","fb1a839f":"markdown","f994f34a":"markdown","99cf93e0":"markdown","2c0682f3":"markdown","bc838fe3":"markdown","bcb21c98":"markdown","160b6925":"markdown","e83944cf":"markdown","ef2e5694":"markdown","12c8bf13":"markdown","7b312bd8":"markdown","8c427db4":"markdown","ca90c17b":"markdown","f0c0ae4b":"markdown","617581e4":"markdown","d5e999d1":"markdown","36ecfbbf":"markdown","6e962c65":"markdown","9bfaada3":"markdown","c0c1ff3d":"markdown","1f245913":"markdown","e83bd894":"markdown","77b2e3e7":"markdown","f2e669de":"markdown","03b6d1e7":"markdown","aa17c981":"markdown","cf37437d":"markdown"},"source":{"ffd69061":"# import das bibliotecas\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.decomposition import PCA","2f5a1003":"# leitura e exibi\u00e7\u00e3o dos dados de treinamento \ntrain = pd.read_csv(\"..\/input\/usp-pj01\/train_Iris.csv\",index_col=0)\ntrain.head()","6e2d1fcc":"# leitura e exibi\u00e7\u00e3o dos dados de teste \ntest = pd.read_csv(\"..\/input\/usp-pj01\/test_Iris.csv\")\ntest.head()","c2f657f7":"train.describe()","45b5e1f7":"test.describe()","eeeb2ae3":"train.shape\ntrain.info()","2ddfb060":"plt.figure(figsize=(8, 4))\n# Comprimento da p\u00e9tala\nsns.boxplot(x=\"Species\", y=\"PetalLengthCm\", data=train)\nplt.xlabel('Esp\u00e9cie', fontsize=18)\nplt.ylabel('Comprimento da p\u00e9tala', fontsize=16)\nplt.xticks(fontsize=14)\nplt.yticks(fontsize=14)\nplt.show(True)","b962adc4":"plt.figure(figsize=(8, 4))\n# Largura da p\u00e9tala\nsns.boxplot(x=\"Species\", y=\"PetalWidthCm\", data=train)\nplt.xlabel('Esp\u00e9cie', fontsize=18)\nplt.ylabel('Largura da p\u00e9tala', fontsize=16)\nplt.xticks(fontsize=14)\nplt.yticks(fontsize=14)\nplt.show(True)","0bbc5bd3":"plt.figure(figsize=(8, 4))\n# Comprimento da s\u00e9pala\t\t\nsns.boxplot(x=\"Species\", y=\"SepalLengthCm\", data=train)\nplt.xlabel('Esp\u00e9cie', fontsize=18)\nplt.ylabel('Comprimento da s\u00e9pala', fontsize=16)\nplt.xticks(fontsize=14)\nplt.yticks(fontsize=14)\nplt.show(True)","38aa0d07":"plt.figure(figsize=(8, 4))\n# Largura da s\u00e9pala\t\t\nsns.boxplot(x=\"Species\", y=\"SepalWidthCm\", data=train)\nplt.xlabel('Esp\u00e9cie', fontsize=18)\nplt.ylabel('Largura da s\u00e9pala', fontsize=16)\nplt.xticks(fontsize=14)\nplt.yticks(fontsize=14)\nplt.show(True)","7180a995":"sns.pairplot(train,hue='Species')\nplt.show()","57f19b81":"pca = PCA(n_components=2)\npca_result = pca.fit_transform(train.iloc[:, [0,1,2,3]])\n\ntarget_names = train.iloc[:, 4]\n\nplt.figure(figsize=(9, 5))\ncolors = ['navy', 'turquoise', 'darkorange']\naux = 0\nlw = 2\n\nfor target_name in np.unique(target_names):\n    nodes = np.where(target_names == target_name)\n    plt.scatter(pca_result[nodes, 0], pca_result[nodes, 1], color = colors[aux], alpha=.8, lw=lw,                          label=target_name)\n    aux += 1\nplt.legend(loc='best', shadow=False, scatterpoints=1)\nplt.title(\"PCA of IRIS dataset\")\nplt.xlabel(\"Principal Component 1\")\nplt.ylabel(\"Principal Component 2\")\nplt.show()\n\nprint(\"Variability explained by the first two components\",\n      round(100 * sum(pca.explained_variance_ratio_),2), \"%\" )","c105aa6b":"train['ratio1'] = (train['PetalLengthCm'])\/(train['SepalLengthCm'])\ntrain['ratio2'] = (train['PetalWidthCm'])\/(train['SepalWidthCm'])\n\nsns.displot(train, x=\"ratio1\", hue=\"Species\", kind=\"kde\", fill=True)\nsns.displot(train, x=\"ratio2\", hue=\"Species\", kind=\"kde\", fill=True)\nplt.show()","085cb6cb":"train.head()","c3c6aaef":"factor = pd.factorize(train['Species'])\ny = factor[0]\n#X = np.c_[train.iloc[:,1:5].values,train['ratio1'],train['ratio2']] incluindo as vari\u00e1veis ratio1 e ratio2\nX = train.iloc[:,0:4].values\nXtest = test.iloc[:,1:5].values\nnrow,ncol = X.shape","9ee0b33b":"# from sklearn.preprocessing import StandardScaler\n# scaler = StandardScaler()\n\n# scaler.fit(X)\n# X_scaled=scaler.transform(X)\n# Xtest_scaled=scaler.transform(Xtest)\n\n# X = X_scaled\n# Xtest = Xtest_scaled","e5f6ab83":"classes = y\ncl = np.unique(classes)\nncl = np.zeros(len(cl))\nfor i in np.arange(0, len(cl)):\n    a = classes == cl[i]\n    ncl[i] = len(classes[a])\n    \nnumbers = np.arange(0, len(cl))\nplt.bar(numbers, ncl,  alpha=.75)\nplt.xticks(numbers, cl)\nplt.title('N\u00famero de elementos em cada classe')\nplt.show(True)","6091a7f0":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.model_selection import cross_val_score\n\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=10, random_state=1)\nRFmacc = []\nsdacc = []\nmpre = []\nsdpre = []\nmrecall = []\nsdrecall = []\nvn = []\nfor n in range(1, 10,1):\n    model = RandomForestClassifier(n_estimators=n)\n    scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n    scores_precision = cross_val_score(model, X, y, scoring='precision_macro', cv=cv, n_jobs=-1)\n    scores_recall = cross_val_score(model, X, y, scoring='recall_macro', cv=cv, n_jobs=-1)\n    RFmacc.append(np.mean(scores))\n    sdacc.append(np.std(scores))\n    mpre.append(np.mean(scores_precision))\n    sdpre.append(np.std(scores_precision))\n    mrecall.append(np.mean(scores_recall))\n    sdrecall.append(np.std(scores_recall))\n    vn.append(n)\n\nbest_n = np.argmax(RFmacc)\nprint('Melhor n:', vn[best_n])\nprint('ACC:', np.round(RFmacc[best_n],2), '+\/-', np.round(sdacc[best_n],2))\nprint('Precision:', np.round(mpre[best_n],2), '+\/-', np.round(sdpre[best_n],2))\nprint('Recall:', np.round(mrecall[best_n],2), '+\/-', np.round(sdrecall[best_n],2))","2618dbe3":"plt.figure(figsize=(10,5))\nplt.errorbar(vn, RFmacc, yerr= sdacc, label='ACC')\nplt.xlabel('n', fontsize = 15)\nplt.ylabel('Score', fontsize = 15)\nplt.legend()\nplt.show()","42b7f0e8":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.model_selection import cross_val_score\n#from multiscorer import MultiScorer\n\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=10, random_state=1)\nKNNmacc = []\nsdacc = []\nmpre = []\nsdpre = []\nmrecall = []\nsdrecall = []\nvk = []\nfor k in range(1, 30):\n    model = KNeighborsClassifier(n_neighbors=k, metric = 'euclidean')\n    scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n    scores_precision = cross_val_score(model, X, y, scoring='precision_macro', cv=cv, n_jobs=-1)\n    scores_recall = cross_val_score(model, X, y, scoring='recall_macro', cv=cv, n_jobs=-1)\n    KNNmacc.append(np.mean(scores))\n    sdacc.append(np.std(scores))  \n    mpre.append(np.mean(scores_precision))\n    sdpre.append(np.std(scores_precision))\n    mrecall.append(np.mean(scores_recall))\n    sdrecall.append(np.std(scores_recall))\n    vk.append(k)\n    \nbest_k = np.argmax(KNNmacc)+1\nprint('Melhor k:', best_k)\nprint('ACC:', np.round(KNNmacc[best_k-1],2), '+\/-', np.round(sdacc[best_k-1],2))\nprint('Precision:', np.round(mpre[best_k-1],2), '+\/-', np.round(sdpre[best_k-1],2))\nprint('Recall:', np.round(mrecall[best_k-1],2), '+\/-', np.round(sdrecall[best_k-1],2))","497a8cdb":"plt.figure(figsize=(10,5))\nplt.errorbar(vk, KNNmacc, yerr= sdacc, label='ACC')\nplt.xlabel('k', fontsize = 15)\nplt.ylabel('Score', fontsize = 15)\nplt.legend()\nplt.show()","2e22b620":"from sklearn.svm import SVC\n\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=10, random_state=1)\nSVMmacc = []\nsdacc = []\nmpre = []\nsdpre = []\nmrecall = []\nsdrecall = []\nvc = []\n\nfor c in np.arange(0.5, 5, 0.5):\n    model = SVC(kernel='linear',C = c, gamma = 'auto')\n    scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n    scores_precision = cross_val_score(model, X, y, scoring='precision_macro', cv=cv, n_jobs=-1)\n    scores_recall = cross_val_score(model, X, y, scoring='recall_macro', cv=cv, n_jobs=-1)\n\n    SVMmacc.append(np.mean(scores))\n    sdacc.append(np.std(scores))\n    mpre.append(np.mean(scores_precision))\n    sdpre.append(np.std(scores_precision))\n    mrecall.append(np.mean(scores_recall))\n    sdrecall.append(np.std(scores_recall))\n    vc.append(c)\n\nbest_c = np.argmax(SVMmacc)\nprint('Melhor C:', vc[best_c])\nprint('Accuracy:', np.round(SVMmacc[best_c],2), '+\/-', np.round(sdacc[best_c],2))\nprint('Precision:', np.round(mpre[best_c],2), '+\/-', np.round(sdpre[best_c],2))\nprint('Recall:', np.round(mrecall[best_c],2), '+\/-', np.round(sdrecall[best_c],2))","217b6288":"plt.figure(figsize=(10,5))\nplt.errorbar(vc, SVMmacc, yerr= sdacc, label='ACC')\nplt.xlabel('c', fontsize = 15)\nplt.ylabel('Score', fontsize = 15)\nplt.legend()\nplt.show()","06206780":"from xgboost import XGBClassifier\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import  precision_score, recall_score, accuracy_score\n\n# grid de parametros\nparam_grid = {\n        'max_depth': [3, 4, 5, 6, 7, 8, 9, 10, 11, 12],\n        'min_child_weight': np.arange(0.0001, 0.5, 0.001),\n        'gamma': np.arange(0.0,40.0,0.005),\n        'learning_rate': np.arange(0.0005,0.3,0.0005),\n        'subsample': np.arange(0.01,1.0,0.01),\n        'colsample_bylevel': np.round(np.arange(0.1,1.0,0.01)),\n        'colsample_bytree':np.arange(0.1,1.0,0.01),\n}\n\n# modelo de classifica\u00e7\u00e3o\nmodel = XGBClassifier()\n\n# Randomized Search CV\nkfold = KFold(n_splits=10, shuffle=True, random_state=10)\ngrid_search = RandomizedSearchCV(model, param_grid, scoring=\"accuracy\", n_iter = 20, cv=kfold)\ngrid_result = grid_search.fit(X,y)\n\n# valida\u00e7\u00e3o cruzada com os tr\u00eas m\u00e9tricas\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=10, random_state=10)\nscores = cross_val_score(grid_search.best_estimator_, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\nscores_precision = cross_val_score(grid_search.best_estimator_, X, y, scoring='precision_macro', cv=cv, n_jobs=-1)\nscores_recall = cross_val_score(grid_search.best_estimator_, X, y, scoring='recall_macro', cv=cv, n_jobs=-1)\n\n# m\u00e9dia e desvio padr\u00e3o das m\u00e9tricas\nprint(\"Accuracy: mean =\", np.round(np.mean(scores), 2), \"std =\", np.round(np.std(scores), 2))\nprint(\"Precision: mean =\", np.round(np.mean(scores_precision), 2), \"std =\", np.round(np.std(scores_precision), 2))\nprint(\"Recall: mean =\",  np.round(np.mean(scores_recall), 2), \"std =\", np.round(np.std(scores_recall),2))\n\nprint(\"\\n\")\n\n# melhores m\u00e9tricas\nprint(\"Best Accuracy: \", np.round(np.max(scores), 2))\nprint(\"Best Precision: \", np.round(np.max(scores_precision), 2))\nprint(\"Best Recall: \",  np.round(np.max(scores_recall), 2))\n\nprint(\"\\n\")\n\n# propor\u00e7\u00e3o que as melhores m\u00e9tricas ocorrem\nprint(\"Best Accuracy Proportion: \", np.round(np.mean(scores == np.max(scores)), 2))\nprint(\"Best Precision Proportion: \", np.round(np.mean(scores_precision == np.max(scores_precision)), 2))\nprint(\"Best Recall Proportion: \", np.round(np.mean(scores_recall == np.max(scores_recall)), 2))\n\nprint(\"\\n\")\n\n# histograma da acur\u00e1cia\nsns.histplot(scores, stat=\"probability\")\nplt.xlabel(\"Accuracy\")\nplt.ylabel(\"Proportion\")\nplt.show()\n\nprint(\"\\n\")\n\n# histograma da precis\u00e3o\nsns.histplot(scores_precision, stat=\"probability\")\nplt.xlabel(\"Precision\")\nplt.ylabel(\"Proportion\")\nplt.show()\n\nprint(\"\\n\")\n\n# histograma do recall\nsns.histplot(scores_recall, stat=\"probability\")\nplt.xlabel(\"Recall\")\nplt.ylabel(\"Proportion\")\nplt.show()\n\nprint(\"\\n\")\n\n# ajuste do modelo com dados de treinamento\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\ngrid_result = grid_search.fit(X_train,y_train)\n\n# predi\u00e7\u00e3o\ny_pred = grid_result.predict(Xtest)\n\nteste_dataframe = pd.DataFrame(data = {\"Id\": test['Id'], \"Category\": y_pred})\nteste_dataframe.Category.replace([0,1,2],['Iris-versicolor','Iris-virginica','Iris-setosa'], inplace=True)\nteste_dataframe.to_csv('submission2.csv', index=False)\nteste_dataframe","7dd6fd4f":"Para facilitar o processamento, vamos converter os dados para o formato numpy para podemos realizar a classifica\u00e7\u00e3o.","fb1a839f":"##3) Modelo de classifica\u00e7\u00e3o","f994f34a":"Assim como antes as larguras das p\u00e9talas da virginica s\u00e3o as maiores, enquanto que as larguras das p\u00e9talas da setosa s\u00e3o as menores. A variabilidade das larguras das p\u00e9talas da versicolor e virginica s\u00e3o semelhantes, enquanto que a variabilidade da setosa \u00e9 consideravelmente menor.\n\nMostramos tamb\u00e9m o boxplot do comprimento da s\u00e9pala para cada umas das esp\u00e9cies.","99cf93e0":"###3.1) Random Forest","2c0682f3":"De modo geral todos os modelos foram, relativamente, satisfat\u00f3rios. Isso se deve, sobretudo, \u00e0 boa qualidade do conjunto de dados. \n\nRessaltamos que o Gradient boosting obteve m\u00e9trica igual a 1 (todas as tr\u00eas) em mais de 40% dos casos de valida\u00e7\u00e3o. Feito not\u00f3rio, haja vista a baixa probabilidade associada \u00e0 ocorr\u00eancia de que tal evento. ","bc838fe3":"Temos ent\u00e3o um dataset de treinamento com 120 observa\u00e7\u00f5es e com 5 features, incluindo a nossa vari\u00e1vel resposta.","bcb21c98":"Agora, as tr\u00eas inst\u00e2ncias (versicolor, virginica e setosa) apresentam uma semelhante variabilidade dos dados de comprimento de s\u00e9pala. Os dados referentes \u00e0 virginica s\u00e3o, na mediana, pouco superiores das demais enquanto que da setosa, na mediana, \u00e9 um pouco inferior.\n\nE, por fim, apresentamos o boxplot da largura da s\u00e9pala para cada uma das esp\u00e9cies.","160b6925":"## 1) An\u00e1lise descritiva dos dados de treinamento e de teste","e83944cf":"Apenas com a largura ou comprimento da p\u00e9tala da Iris \u00e9 poss\u00edvel separar a esp\u00e9cie Setosa das demais, porque n\u00e3o h\u00e1 intersec\u00e7\u00e3o das distribu\u00ed\u00e7\u00f5es emp\u00edricas desta esp\u00e9cie com as das demais. Al\u00e9m disso, mediante os gr\u00e1ficos de dispers\u00e3o 2 a 2 \u00e9 possivel notar que as tr\u00eas esp\u00e9cies de flores ocupam regi\u00f5es diferentes dos gr\u00e1ficos, um indicativo que os modelos de classifica\u00e7\u00e3o ser\u00e3o satisfat\u00f3rios para esse dataset.\n\nAbaixo apresentamos a proje\u00e7\u00e3o da vari\u00e1vel resposta sobre um espa\u00e7o bidimensional. Para isso, utilisamos PCA (Principal component analysis).","ef2e5694":"Como \u00e9 poss\u00edvel estabelecer regi\u00f5es de classifica\u00e7\u00e3o do tipo vari\u00e1vel > valor pertence \u00e0 uma esp\u00e9cie e o contr\u00e1rio \u00e0s outras esp\u00e9cies, o modelo de classifica\u00e7\u00e3o baseado em ensemble de \u00e1rvores \u00e9 apropriado para este problema. Portanto, vamos utilizar florestas aleat\u00f3rias para a classifica\u00e7\u00e3o das esp\u00e9cies Iris.","12c8bf13":"###3.2) K vizinhos","7b312bd8":"## 2) Tentativa de criar novas vari\u00e1veis com a raz\u00e3o entre a vari\u00e1veis da p\u00e9tala e da s\u00e9pala.","8c427db4":"An\u00e1lise do balanceamento das classes","ca90c17b":"Mediante a Figura acima nota-se que os dados apresentam uma boa \"separabilidade\", isto \u00e9, os dados instanciados com setosa, versicolor e virginica est\u00e3o ocupando regi\u00f5es diferentes do gr\u00e1fico acima e essas regi\u00f5es apresentam baixa intersec\u00e7\u00e3o. Al\u00e9m disso, as duas principais componentes explicam cerca de 97,68% da variabilidade da matriz de vari\u00e2ncia e covari\u00e2ncia.\n","f0c0ae4b":"A partir deste ponto os gr\u00e1ficos ser\u00e3o referentes apenas ao dataset de treinamento, haja vista que apenas ele ser\u00e1 utilizado para treinar e consequentemente construir o modelo de classifica\u00e7\u00e3o.\n\nA seguir, apresentamos o boxplot do comprimento da p\u00e9tala para cada umas das esp\u00e9cies.","617581e4":"Abaixo apresentamos um resumo descritivo dos dados de treinamento.","d5e999d1":"Numericamente, os dados do conjunto de teste possuem estat\u00edsticas muito pr\u00f3ximas das dos dados do conjunto de treinamento. Isso \u00e9 uma boa qualidade, pois \u00e9 um indicativo de que o modelo treinado ser\u00e1 bem avaliado com o conjunto de teste.","36ecfbbf":"###3.3) Support Vector Machine","6e962c65":"Padroniza\u00e7\u00e3o dos dados (n\u00e3o melhorou a classifica\u00e7\u00e3o). N\u00e3o vale a pena fazer a padroniza\u00e7\u00e3o dos dados.","9bfaada3":"Abaixo apresentamos um resumo descritivo dos dados de teste.","c0c1ff3d":"###3.4) Gradient boosting","1f245913":"Para a largura da s\u00e9pala os dados relacionados \u00e0 setosa s\u00e3o maiores e tamb\u00e9m apresentam uma consider\u00e1vel variabilidade. Os dados referentes \u00e0 verisicolor s\u00e3o um pouco inferiores das demais e no geral as tr\u00eas apresentam uma variabilidade semelhante.","e83bd894":"Utilizando o modelo de Gradient boosting para a submiss\u00e3o.\n","77b2e3e7":"# Classifica\u00e7\u00e3o dos dados Iris\n\nD\u00e9bora Mayumi Rissato - 5288223\n\nDouglas Decicino de Andrade - 10883512\n\nPaulino Ribeiro Villas Boas - 2950178\n\nRenan Silva Chun - 10691817\n\nRenan de Oliveira da Cruz - 10801090","f2e669de":"Mediante aos gr\u00e1ficos nota-se que n\u00e3o se obteve algo muito diferente do que j\u00e1 tinhamos. Ou seja, essas novas vari\u00e1veis contribuem com a mesma informa\u00e7\u00e3o j\u00e1 contida no *dataset* (vide a semelhan\u00e7a com os gr\u00e1ficos *kernel distribution estimation* apresentados na an\u00e1lise descritiva). Em raz\u00e3o disso n\u00e3o utilizaremos no modelo de classifica\u00e7\u00e3o essas novas vari\u00e1veis.","03b6d1e7":"Nota-se, visualmente, que as tr\u00eas classes est\u00e3o bem balanceadas.  ","aa17c981":"Mediante os boxplots acima notamos que os comprimentos das p\u00e9talas relacionadas \u00e0 virginica s\u00e3o os maiores, enquando que os comprimentos das p\u00e9talas da setosa s\u00e3o os menores. Ademais, percebemos que versicolor e virgina apresentam variabilidade semelhante.\n\nAgora, apresentamos o boxplot da largura da p\u00e9tala para cada umas das esp\u00e9cies.","cf37437d":"Inicialmente, inserimos os dados e apresentamos as primeiras \tcinco observa\u00e7\u00f5es.\n\nA base de dados utilizada foi a da iris sendo composto por\n\n**id:** identifica\u00e7\u00e3o da flor\n\n**SepalLengthCm:** Comprimento da S\u00e9pala da flor em centimetros\n\n**SepalWidthCm:** Largura da S\u00e9pala da flor em centimetros\n\n**PetalLengthCm:** Comprimento da P\u00e9tala da flor em centimetros\n\n**PetalWidthCm:** Largura da P\u00e9tala da flor em centimetros\n\n**Species:** Esp\u00e9cie da flor"}}