{"cell_type":{"69e7a3a3":"code","8295235b":"code","239f67d0":"code","61eab6a1":"code","dc4a8c0d":"code","9076d7f8":"code","6520fb74":"code","1bc446c7":"code","83bc179d":"code","6680280c":"code","4fdacfd4":"code","f3018b45":"code","7e841503":"code","da5512ac":"code","5a7a6f4a":"code","d230ca25":"code","e148dad6":"code","a9e3184a":"code","c47b11f2":"code","efe48e8b":"code","f8e85e8b":"code","7f5972c0":"code","2a2acbcd":"code","9bcb41c3":"code","e1299c49":"code","acbcc51f":"code","c604aaf2":"code","5f8bfeda":"code","5105d782":"code","441d27ba":"code","d3f1e9f6":"code","a0b1c567":"code","1b7b26d5":"code","865940aa":"code","2366887e":"code","a88b443c":"code","7974751c":"code","9d24bceb":"code","89ed568c":"code","53fe2921":"code","dc54a762":"code","b300a348":"code","1e9b20c8":"code","393a3e0f":"code","21fd8a8f":"code","0d767d17":"code","13576472":"code","5b88c889":"code","94c0f361":"code","34137e4c":"code","5d3d74b4":"code","b1d9a123":"code","03d8d804":"code","bad6c911":"code","415e0204":"code","d9904885":"code","2ca4979a":"code","8e885f4d":"code","15e65d1f":"code","fad6d6c0":"code","057eaf68":"code","b66f0d2f":"code","a1a9fa0a":"code","ca4166e0":"code","7945bcc2":"code","b718114b":"code","687eb92a":"code","d3658708":"markdown","be2c9100":"markdown","fa7ea165":"markdown","870ce756":"markdown","5fd9ecb4":"markdown","4ec82252":"markdown","52197f0c":"markdown","7472eae8":"markdown","f524a558":"markdown","56dd03c3":"markdown","efc67423":"markdown","699ef685":"markdown","d6a485c8":"markdown","9e6afdd4":"markdown","92eb36a0":"markdown","1b4e967d":"markdown","ad6a4ac0":"markdown","fef5d698":"markdown","5b9410aa":"markdown","540d483f":"markdown","beccefed":"markdown","d6dfd808":"markdown","0c00c59d":"markdown","9e838cec":"markdown","a8ea78ae":"markdown","98177ac8":"markdown","d618c872":"markdown","f74428a4":"markdown","43be6c79":"markdown","d8c63b10":"markdown","847f396e":"markdown","6977e120":"markdown","6e0b77a6":"markdown","fb38af39":"markdown","3141db73":"markdown","45a19a2f":"markdown","3c39ea29":"markdown","d35879bc":"markdown","a46c51ab":"markdown","c65db8b6":"markdown","73662a9e":"markdown","2e9713ea":"markdown","41cf6890":"markdown","9fb4424d":"markdown","9c83a878":"markdown","16912629":"markdown","b6bb91ec":"markdown","e42b864f":"markdown","04b9f76c":"markdown","7c4600a9":"markdown"},"source":{"69e7a3a3":"import os\nimport tarfile\nimport urllib\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom sklearn.model_selection import StratifiedShuffleSplit\nfrom pandas.plotting import scatter_matrix\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.compose import make_column_transformer\nfrom sklearn.preprocessing import StandardScaler,MinMaxScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error,mean_absolute_error,median_absolute_error,r2_score\nfrom sklearn.linear_model import Ridge\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.feature_selection import SelectKBest,f_regression\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import GradientBoostingRegressor\n\nimport tensorflow as tf\nfrom tensorflow.estimator import LinearRegressor","8295235b":"DOWNLOAD_ROOT = \"https:\/\/raw.githubusercontent.com\/ageron\/handson-ml2\/master\/\"\nHOUSING_PATH = os.path.join(\"datasets\", \"housing\")\nHOUSING_URL = DOWNLOAD_ROOT + \"datasets\/housing\/housing.tgz\"\n\ndef fetch_housing_data(housing_url=HOUSING_URL, housing_path=HOUSING_PATH):\n    if not os.path.isdir(housing_path):\n        os.makedirs(housing_path)\n    tgz_path = os.path.join(housing_path, \"housing.tgz\")\n    urllib.request.urlretrieve(housing_url, tgz_path)\n    housing_tgz = tarfile.open(tgz_path)\n    housing_tgz.extractall(path=housing_path)\n    housing_tgz.close()\n\nfetch_housing_data()\n\ndef load_housing_data(housing_path=HOUSING_PATH):\n    csv_path = os.path.join(housing_path, \"housing.csv\")\n    return pd.read_csv(csv_path)","239f67d0":"housing = load_housing_data()\nhousing.head(10)","61eab6a1":"housing.info()","dc4a8c0d":"housing['ocean_proximity'].value_counts()","9076d7f8":"housing.describe()","6520fb74":"housing.hist(bins = 50, figsize= (20,15))","1bc446c7":"housing = housing[housing['median_house_value']< 500001]\nhousing.reset_index(drop=True,inplace = True)\n","83bc179d":"housing.info()","6680280c":"#based on median_icome data, we try to classify it into five categories\nhousing['income_category'] = pd.cut(housing['median_income'],bins = [0,1.5,3,4.5,6,np.inf],labels = [1,2,3,4,5])\n#splitting\nss_split = StratifiedShuffleSplit(n_splits = 1 , test_size = 0.20, random_state = 42)\nfor train_index, test_index in ss_split.split(housing, housing['income_category']):\n    train_set = housing.loc[train_index]\n    test_set = housing.loc[test_index]\n#dropping income_category attribute\nfor set in (train_set,test_set):\n    set.drop('income_category',axis=1,inplace = True )","4fdacfd4":"#making a copy of training data\ntrain_set.plot(kind = 'scatter', x = 'longitude', y = 'latitude', alpha = 0.1)\ntrain_set.plot(kind = 'scatter', x = 'longitude', y = 'latitude', alpha = 0.5, s= train_set['population']\/100, \n             label = 'population' , figsize = (20,10) , c = 'median_house_value', cmap = plt.get_cmap('jet'), \n             colorbar= True)","f3018b45":"corr_matrix = train_set.corr()\ncorr_matrix['median_house_value'].sort_values(ascending = False)","7e841503":"attributes = ['median_house_value','median_income','total_rooms','housing_median_age','latitude']\nscatter_matrix(train_set[attributes],figsize = (12,8))","da5512ac":"#try out various attribute combinations\ntrain_set['rooms_per_household'] = train_set['total_rooms']\/train_set['households']\ntrain_set['bedrooms_per_room'] = train_set['total_bedrooms']\/train_set['total_rooms']\ntrain_set['population_per_household'] = train_set['population']\/train_set['households']\ncorr_matrix = train_set.corr()\ncorr_matrix['median_house_value'].sort_values(ascending=False)","5a7a6f4a":"#try out various attribute combinations\ntest_set['rooms_per_household'] = test_set['total_rooms']\/test_set['households']\ntest_set['bedrooms_per_room'] = test_set['total_bedrooms']\/test_set['total_rooms']\ntest_set['population_per_household'] = test_set['population']\/test_set['households']\n","d230ca25":"## Splitting X & y from Data sets to extract the dependent variable y away from processing data\n\n#y_train = train_set['median_house_value'].values\n#X_train = train_set.copy()\n#X_train.drop('median_house_value', axis = 1, inplace = True)\n#y_test = test_set['median_house_value'].values\n#X_test = test_set.copy()\n#X_test.drop('median_house_value', axis = 1, inplace = True)","e148dad6":"#impute = SimpleImputer(missing_values=np.nan, strategy = 'median')\n#train_num = X_train.drop('ocean_proximity', axis =1)\n#impute.fit_transform(train_num)\n#train_num.head(10)","a9e3184a":"#impute.statistics_","c47b11f2":"#std_scaler = StandardScaler()\n#train_num_array = std_scaler.fit_transform(train_num)\n#train_num = pd.DataFrame(train_num_array , columns = train_num.columns , index = train_num.index)","efe48e8b":"#train_num['ocean_proximity'] = X_train['ocean_proximity']\n#X_train = train_num.copy()","f8e85e8b":"#category_trans = make_column_transformer((OneHotEncoder(),['ocean_proximity']),remainder = 'passthrough')\n#X_train = category_trans.fit_transform(X_train)\n##X_train = pd.DataFrame(housing_array , columns = housing.columns , index = housing.index)\n#X_train.shape","7f5972c0":"# Splitting X & y from Data sets to extract the dependent variable y away from processing data\n\ny_train = train_set['median_house_value'].values\nX_train = train_set.copy()\nX_train.drop('median_house_value', axis = 1, inplace = True)\ny_test = test_set['median_house_value'].values\nX_test = test_set.copy()\nX_test.drop('median_house_value', axis = 1, inplace = True)\n\ntrain_num = X_train.drop('ocean_proximity', axis =1)\ntest_num = X_test.drop('ocean_proximity', axis =1)\nX_train.shape","2a2acbcd":"num_attributes = list(train_num)\ncat_attributes = ['ocean_proximity']\n\nnum_pipeline = Pipeline([('imputer', SimpleImputer(strategy='median')),('std_scaler', StandardScaler())])\npipeline = ColumnTransformer([('num_Pipeline', num_pipeline,num_attributes),\n                                 ('category', OneHotEncoder(),cat_attributes)],remainder='passthrough')\nX_train= pipeline.fit_transform(X_train)\n\nX_train.shape                                                     ","9bcb41c3":"X_test= pipeline.transform(X_test)\nX_test.shape","e1299c49":"reg_model = LinearRegression()\nreg_model.fit(X_train,y_train)\ny_pred = reg_model.predict(X_train)\n","acbcc51f":"print('the training score = ',reg_model.score(X_train,y_train))\nmse = mean_squared_error(y_train,y_pred)\nrmse = np.sqrt(mse)\nprint('the root mean squared error = ', rmse)","c604aaf2":"ridge_model = Ridge()\nridge_model.fit(X_train,y_train)\ny_pred = ridge_model.predict(X_train)","5f8bfeda":"print('the training score = ',ridge_model.score(X_train,y_train))\nmse = mean_squared_error(y_train,y_pred)\nrmse = np.sqrt(mse)\nprint('the root mean squared error = ', rmse)","5105d782":"tree_model = DecisionTreeRegressor(random_state=42)\ntree_model.fit(X_train,y_train)\ny_pred = tree_model.predict(X_train)","441d27ba":"print('the training score = ',tree_model.score(X_train,y_train))\nmse = mean_squared_error(y_train,y_pred)\nrmse = np.sqrt(mse)\nprint('the root mean squared error = ', rmse)","d3f1e9f6":"tree_score = cross_val_score(tree_model,X_train,y_train,scoring= 'neg_mean_squared_error', cv=10)\ntree_rmse_score = np.sqrt(-tree_score)","a0b1c567":"def dispaly_scores(scores):\n    print('scores : ', scores)\n    print('mean = ', scores.mean())\n    print('standard deviation = ',scores.std())\n\ndispaly_scores(tree_rmse_score)","1b7b26d5":"forest_model = RandomForestRegressor()\nforest_model.fit(X_train,y_train)\ny_pred = forest_model.predict(X_train)","865940aa":"print('the training score = ',forest_model.score(X_train,y_train))\nmse = mean_squared_error(y_train,y_pred)\nrmse = np.sqrt(mse)\nprint('the root mean squared error = ', rmse)","2366887e":"forest_score = cross_val_score(forest_model,X_train,y_train,scoring= 'neg_mean_squared_error', cv=10)\nforest_rmse_score = np.sqrt(-forest_score)\ndispaly_scores(forest_rmse_score)","a88b443c":"svr_model = SVR()\nsvr_model.fit(X_train,y_train)\ny_pred = svr_model.predict(X_train)","7974751c":"print('the training score = ',svr_model.score(X_train,y_train))\nmse = mean_squared_error(y_train,y_pred)\nrmse = np.sqrt(mse)\nprint('the root mean squared error = ', rmse)","9d24bceb":"gbr_model = GradientBoostingRegressor()\ngbr_model.fit(X_train,y_train)\ny_pred = gbr_model.predict(X_train)","89ed568c":"print('the training score = ',gbr_model.score(X_train,y_train))\nmse = mean_squared_error(y_train,y_pred)\nrmse = np.sqrt(mse)\nprint('the root mean squared error = ', rmse)","53fe2921":"gbr_score = cross_val_score(gbr_model,X_train,y_train,scoring= 'neg_mean_squared_error', cv=10)\ngbr_rmse_score = np.sqrt(-gbr_score)\ndispaly_scores(gbr_rmse_score)","dc54a762":"tune_pipeline = Pipeline([\n     ('selector',SelectKBest(f_regression)),\n     ('model',RandomForestRegressor(random_state = 42))])\n\ngrid_search = GridSearchCV( estimator = tune_pipeline, param_grid = {'selector__k':[14,16] , \n  'model__n_estimators':np.arange(360,370,10),'model__max_depth':[15]}, n_jobs=-1, scoring=[\"neg_mean_squared_error\",'neg_mean_absolute_error'],refit = 'neg_mean_absolute_error', cv=5, verbose=3)\n","b300a348":"grid_search.fit(X_train,y_train)\nprint('the best parameters : ',grid_search.best_params_)\nprint('the best score = ', np.sqrt(-grid_search.best_score_))","1e9b20c8":"grid_search.best_estimator_.score(X_train,y_train)","393a3e0f":"tune_pipeline_svr = Pipeline([\n     ('selector',SelectKBest(f_regression)),\n     ('model',SVR())])\n\ngrid_search_svr = GridSearchCV( estimator = tune_pipeline_svr, param_grid = {'selector__k':[14,16] , \n  'model__kernel':['linear'],'model__C':[5000,10000],'model__epsilon':[0.3,3]}, n_jobs=-1, scoring=\"neg_mean_squared_error\", cv=5, verbose=3)","21fd8a8f":"grid_search_svr.fit(X_train,y_train)\nprint('the best parameters : ',grid_search_svr.best_params_)\nprint('the best score = ', np.sqrt(-grid_search_svr.best_score_))","0d767d17":"tune_pipeline_gbr = Pipeline([\n     ('selector',SelectKBest(f_regression)),\n     ('model',GradientBoostingRegressor(random_state=42))])\n\ngrid_search_gbr = GridSearchCV( estimator = tune_pipeline_gbr, param_grid = {'selector__k':[14,16] , \n  'model__loss':['ls'],'model__max_depth':[6,7],'model__learning_rate':[0.1,0.2],'model__n_estimators':[500]}, n_jobs=-1, scoring=[\"neg_mean_squared_error\",'neg_mean_absolute_error'],refit = 'neg_mean_absolute_error', cv=5, verbose=3)","13576472":"grid_search_gbr.fit(X_train,y_train)\nprint('the best parameters : ',grid_search_gbr.best_params_)\nprint('the best score = ', np.sqrt(-grid_search_gbr.best_score_))","5b88c889":"tune_pipeline_ridge = Pipeline([\n     ('selector',SelectKBest(f_regression)),\n     ('model',Ridge(random_state=42))])\n\ngrid_search_ridge = GridSearchCV( estimator = tune_pipeline_ridge, param_grid = {'selector__k':[15,16] , \n  'model__alpha':[0.5,1]}, n_jobs=-1, scoring=\"neg_mean_squared_error\", cv=5, verbose=3)","94c0f361":"grid_search_ridge.fit(X_train,y_train)\nprint('the best parameters : ',grid_search_ridge.best_params_)\nprint('the best score = ', np.sqrt(-grid_search_ridge.best_score_))","34137e4c":"grid_search_gbr.best_estimator_.score(X_train,y_train)","5d3d74b4":"final_model = grid_search_gbr.best_estimator_\ny_pred = final_model.predict(X_test)","b1d9a123":"final_model.score(X_test,y_test)","03d8d804":"rmse = np.sqrt(mean_squared_error(y_test,y_pred))\nmae = mean_absolute_error(y_test,y_pred)\nmedian_ae = median_absolute_error(y_test,y_pred)","bad6c911":"print(rmse)\nprint(mae)\nprint(median_ae)","415e0204":"y_train = train_set['median_house_value']\nX_train = train_set.copy()\nX_train.drop('median_house_value', axis = 1, inplace = True)\ny_test = test_set['median_house_value']\nX_test = test_set.copy()\nX_test.drop('median_house_value', axis = 1, inplace = True)\nX_train.info()","d9904885":"train_num = X_train.drop('ocean_proximity', axis =1)\ntest_num = X_test.drop('ocean_proximity', axis =1)\nnum_attributes = list(train_num)\ncat_attributes = ['ocean_proximity']\n\nnum_pipeline = Pipeline([('imputer', SimpleImputer(strategy='median')),('std_scaler', StandardScaler())])\nprocessing_pipeline = ColumnTransformer([('num_Pipeline', num_pipeline,num_attributes)],remainder='passthrough')\n\nX_train_ = processing_pipeline.fit_transform(X_train)\nX_test_ = processing_pipeline.transform(X_test)\nX_train = pd.DataFrame(X_train_,columns=num_attributes+['ocean_proximity'],index=X_train.index)\nX_test = pd.DataFrame(X_test_,columns=num_attributes+['ocean_proximity'],index=X_test.index)\nX_train[[i for i in X_train.columns if i not in ['ocean_proximity']]] = X_train[[i for i in X_train.columns if i not in ['ocean_proximity']]].apply(pd.to_numeric)\nX_test[[i for i in X_test.columns if i not in ['ocean_proximity']]] = X_test[[i for i in X_test.columns if i not in ['ocean_proximity']]].apply(pd.to_numeric)\nX_test.info()","2ca4979a":"feature_columns_numeric = [tf.feature_column.numeric_column(m) for m in train_num.columns]\nfeature_columns_categorical = [tf.feature_column.categorical_column_with_hash_bucket('ocean_proximity',\n                                                                                     hash_bucket_size=1000)]\nfeature_columns = feature_columns_numeric + feature_columns_categorical\nfeature_columns","8e885f4d":"dataset = tf.data.Dataset.from_tensor_slices((dict(X_train), y_train))\ndataset","15e65d1f":"def feed_input(features_dataframe, target_dataframe, num_of_epochs=10, shuffle=True, batch_size=32):\n  def input_feed_function():\n    dataset = tf.data.Dataset.from_tensor_slices((dict(features_dataframe), target_dataframe))\n    if shuffle:\n      dataset = dataset.shuffle(2000)\n    dataset = dataset.batch(batch_size).repeat(num_of_epochs)\n    return dataset\n  return input_feed_function\n\ntrain_feed_input = feed_input(X_train, y_train)\ntrain_feed_input_testing = feed_input(X_train, y_train, num_of_epochs=1, shuffle=False)\ntest_feed_input = feed_input(X_test, y_test, num_of_epochs=1, shuffle=False)\n","fad6d6c0":"regression_model = LinearRegressor(feature_columns)\nregression_model.train(train_feed_input,steps=1000)","057eaf68":"train_predictions = regression_model.predict(train_feed_input_testing)\ntest_predictions = regression_model.predict(test_feed_input)","b66f0d2f":"train_predictions_series = pd.Series([p['predictions'][0] for p in train_predictions])\ntest_predictions_series = pd.Series([p['predictions'][0] for p in test_predictions])","a1a9fa0a":"train_predictions_df = pd.DataFrame(train_predictions_series, columns=['predictions'])\ntest_predictions_df = pd.DataFrame(test_predictions_series, columns=['predictions'])","ca4166e0":"y_train.reset_index(drop=True, inplace=True)\ntrain_predictions_df.reset_index(drop=True, inplace=True)\n\ny_test.reset_index(drop=True, inplace=True)\ntest_predictions_df.reset_index(drop=True, inplace=True)","7945bcc2":"train_labels_with_predictions_df = pd.concat([y_train, train_predictions_df], axis=1)\ntest_labels_with_predictions_df = pd.concat([y_test, test_predictions_df], axis=1)","b718114b":"def calculate_errors_and_r2(y_true, y_pred):\n  mean_squared_err = (mean_squared_error(y_true, y_pred))\n  root_mean_squared_err = np.sqrt(mean_squared_err)\n  r2 = round(r2_score(y_true, y_pred)*100,0)\n  return mean_squared_err, root_mean_squared_err, r2","687eb92a":"train_mean_squared_error, train_root_mean_squared_error, train_r2_score_percentage = calculate_errors_and_r2(y_train, train_predictions_series)\ntest_mean_squared_error, test_root_mean_squared_error, test_r2_score_percentage = calculate_errors_and_r2(y_test, test_predictions_series)\n\nprint('Training Data Mean Squared Error = ', train_mean_squared_error)\nprint('Training Data Root Mean Squared Error = ', train_root_mean_squared_error)\nprint('Training Data R2 = ', train_r2_score_percentage)\n\nprint('Test Data Mean Squared Error = ', test_mean_squared_error)\nprint('Test Data Root Mean Squared Error = ', test_root_mean_squared_error)\nprint('Test Data R2 = ', test_r2_score_percentage)","d3658708":"### 1.LinearRegression Model","be2c9100":"# TRAINING A MACHINE LEARNING MODEL ","fa7ea165":"#### Ridge Regression Model","870ce756":"# FINE TUNE THE MODEL \n\ntechnique to mix feature selection and hyperparameter tuning in the same procedure, considering the feature set as a hyperparameter itself. ","5fd9ecb4":"1.as we see there is underfitting in the model \n\n2.the scores of both train and test are not good\n\n''''also the mean square error of test and predicted data is about 60,500$, \nit`s a big error compared to range of median_housing_value'''","4ec82252":"###### Model Evaluation","52197f0c":"###### Model Evaluation","7472eae8":"#### Random Forest Regressor","f524a558":"###### Cross validation Score","56dd03c3":"### 5.SVR Model","efc67423":"### It`s better here to use stratified sampling in splitting to represent all classes in our test set data.\n### for example if we should represent all categories in median_income data, we must classify this data into categories \n","699ef685":"### 6.Gradient Boosting Regressor Model","d6a485c8":"# Validation","9e6afdd4":"# VISUALIZING & EXPLORING DATA","92eb36a0":"# SPLITTING DATA ","1b4e967d":"### 4.Random Forest Regressor Model","ad6a4ac0":"# Tensorflow Linear Regressor","fef5d698":"-------------the Random Forest Regressor is better than  SVR model-------------------\n\nlet`s try \n#### Gradient Boosting Regressor","5b9410aa":"# Model Training","540d483f":"the Gradient Boosting Regressor is better than Random Forest Regressor Model\n\nit has rmse about 44,000$,  compared to  ,45,800$ from Random Forest","beccefed":"After loading data and exploring it, we will start from data splitting step ","d6dfd808":"# DATA PREPROCESSING PIPELINE \n\nTHIS IS AN ALTERNATIVE SOLUTION WHICH IS MORE FLEXIBLE, SIMPLE, AND CONSUMINNG LESS TIME FOR DATA PREPROCESSING OPERATIONS SUCH DATA CLEANING, FEATURE SCALING, AND HANDLING TEXT \n\nIT DEPENDS ON PIPELINE CLASS FROM PIPELINE MODULE RELATED TO SKLEARN","0c00c59d":"### ATTRIBUTE COMBINATIONS","9e838cec":"# DATA PREPROCESSING\n### DATA CLEANING","a8ea78ae":"# FEATURE SELECTION & CORRELATION","98177ac8":"# Prediction","d618c872":"1.as we see there is underfitting in the model \n\n2.the scores of both train and test are not good\n\n''''also the mean square error of test and predicted data is about 60,500$, \nit`s a big error compared to range of median_housing_value'''","f74428a4":"#### SVR ","43be6c79":"###### Model Evaluation","d8c63b10":"### 2.Ridge Regression Model","847f396e":"# LOAD DATA\n","6977e120":"## Train & Test Split","6e0b77a6":"###### Cross validation Score","fb38af39":"the model seems it has no error!!!!!!\nI think it has badly overfitting.\nSo we have to evaluate it using cross validation test.","3141db73":"# Finally from these models we can say the best model till now is GRADIENT BOOSTING REGRESSOR with:\n# cross validation score reaches 98.3% and test score equal to 83%","45a19a2f":"# IMPORT","3c39ea29":"## Data Preparation","d35879bc":"# EVALUATE THE MODEL ON THE TEST SET\n\n##### here we use the Gradient Boosting Regressor","a46c51ab":"### HANDLING TEXT & CATEGORICAL ATTRIBUTES","c65db8b6":"### Input Pipeline","73662a9e":"### 3.Decision Tree Model","2e9713ea":"### FEATURE SCALING\n","41cf6890":"###### Model Evaluation","9fb4424d":"###### Model Evaluation","9c83a878":"it`s more bad than Linear Regression as it has rmse about 64,500$\nthe model has badly overfit the data.\n\nlet`s try random forest model ","16912629":"# DATA STRUCTURE","b6bb91ec":"### Feature columns processing\n","e42b864f":"###### Cross validation Score","04b9f76c":"# Removing Outliers \n\nI think from histogram the median_house_value has outliers near to 500000$","7c4600a9":"###### Model Evaluation"}}