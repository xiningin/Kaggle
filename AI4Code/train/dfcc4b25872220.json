{"cell_type":{"63bb2dba":"code","b2f3bf12":"code","959fcae7":"code","9666cfda":"code","c71dbb9d":"code","2d919b80":"code","16bcdb5d":"code","8e595b68":"code","6ed9752c":"code","62a4507f":"code","5868355c":"code","7b0b9527":"code","81f75e51":"code","608e29b6":"code","d0728911":"code","24532a5e":"code","efbc415e":"code","309bcc88":"code","92f11390":"code","ccd8c6c4":"code","e32b8dc8":"code","dc629729":"code","8046f8fe":"code","686e04ad":"code","3c9882fa":"code","93630426":"code","52ef46b5":"code","bf98fdd3":"code","cf895879":"code","5c89f3bc":"code","10be5d0f":"code","cb368919":"code","2adc41e6":"code","a17639d7":"code","d9e3ffc4":"code","52dd93ed":"code","3ee95145":"code","482791f7":"code","fb8ec1c2":"code","4314c325":"code","cb101652":"code","8fc0d283":"code","c71aa775":"code","58b7785f":"code","6381c91b":"code","67dd6aa0":"code","bbdd26b5":"code","fb2c54f4":"code","5fc1f36b":"code","b16ae0f3":"code","c24d1ca0":"code","f82843d5":"code","353b2736":"code","5c238a4b":"code","00204cf4":"code","cc503b31":"code","65d61965":"code","dd5beaed":"code","40e9c198":"code","c5f30138":"code","1a3b1571":"code","95dbf79b":"code","1e8ceff3":"code","195d515a":"code","25bb4580":"code","dabcc136":"code","b61e2a33":"code","672a429a":"code","3716ead4":"code","2b6d5d0d":"code","70383062":"code","51b9748e":"code","5533c05d":"code","683c4091":"code","bf919461":"code","a33b1d49":"code","2143a551":"markdown","2799fc2d":"markdown","6df9f77f":"markdown","aff28de8":"markdown","7164173c":"markdown","76eacfe7":"markdown","cd34f3dc":"markdown","3e929f0f":"markdown","65780a48":"markdown","db352c56":"markdown","f4e17030":"markdown","fe984203":"markdown","c498966e":"markdown","5b3bb76d":"markdown","ca3d0126":"markdown","ca064fd1":"markdown","8af04225":"markdown","21c45051":"markdown","ef80094d":"markdown","d9b96e36":"markdown","719b4f11":"markdown","5a566c99":"markdown","073c288c":"markdown","b0d1475e":"markdown","89a4f179":"markdown","5d814638":"markdown","f5b0c502":"markdown","51036289":"markdown","d07e5a0e":"markdown","999fcc72":"markdown","ce86cc28":"markdown","3ff7c080":"markdown","bc29671e":"markdown","cf310fa1":"markdown"},"source":{"63bb2dba":"import numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nplt.style.use(\"seaborn-whitegrid\")\nimport warnings            \nwarnings.filterwarnings(\"ignore\") ","b2f3bf12":"y_2018 = pd.read_csv(\"\/kaggle\/input\/world-happiness\/2018.csv\");\ny_2019 = pd.read_csv(\"\/kaggle\/input\/world-happiness\/2019.csv\");\n\ndata = pd.concat([y_2018,y_2019],sort=False)\ndata","959fcae7":"data.describe().T","9666cfda":"data.info()","c71dbb9d":"data.rename(columns={\n    \"Overall rank\": \"rank\",\n    \"Country or region\": \"country\",\n    \"Score\": \"score\",\n    \"GDP per capita\": \"gdp\",\n    \"Social support\": \"social\",\n    \"Healthy life expectancy\": \"healthy\",\n    \"Freedom to make life choices\": \"freedom\",\n    \"Generosity\": \"generosity\",\n    \"Perceptions of corruption\": \"corruption\"\n},inplace=True)\ndel data[\"rank\"]","2d919b80":"data.columns[data.isnull().any()]","16bcdb5d":"data.isnull().sum()","8e595b68":"data[data[\"corruption\"].isnull()]","6ed9752c":"avg_data_corruption = data[data[\"score\"] > 6.774].mean().corruption\ndata.loc[data[\"corruption\"].isnull(),[\"corruption\"]] = avg_data_corruption\ndata[data[\"corruption\"].isnull()]","62a4507f":"df = data.copy()\ndf = df.select_dtypes(include=[\"float64\",\"int64\"])\ndf.head()","5868355c":"column_list = [\"score\",\"gdp\",\"social\",\"healthy\",\"freedom\",\"generosity\",\"corruption\"]\nfor col in column_list:\n    sns.boxplot(x = df[col])\n    plt.xlabel(col)\n    plt.show()","7b0b9527":"# for corruption\ndf_table = df[\"corruption\"]\n\nQ1 = df_table.quantile(0.25)\nQ3 = df_table.quantile(0.75)\nIQR = Q3 - Q1\n\nlower_bound = Q1 - 1.5*IQR\nupper_bound = Q3 + 1.5*IQR\nprint(\"lower bound is \" + str(lower_bound))\nprint(\"upper bound is \" + str(upper_bound))\nprint(\"Q1: \", Q1)\nprint(\"Q3: \", Q3)","81f75e51":"outliers_vector = (df_table < (lower_bound)) | (df_table > (upper_bound))\noutliers_vector","608e29b6":"outliers_vector = df_table[outliers_vector]\noutliers_vector.index.values","d0728911":"df_table = data.copy()\ndf_table[\"corruption\"].iloc[outliers_vector.index.values] = df_table[\"corruption\"].mean()\ndf_table[\"corruption\"].iloc[outliers_vector.index.values]","24532a5e":"data = df_table","efbc415e":"sns.jointplot(x=\"gdp\",y=\"score\",data=df_table,kind=\"reg\")\nplt.show()","309bcc88":"from sklearn.linear_model import LinearRegression\n\nX = data[[\"gdp\"]]\nX.head","92f11390":"y = data[[\"score\"]]\ny.head","ccd8c6c4":"reg = LinearRegression()\nmodel = reg.fit(X,y)\nprint(\"intercept: \", model.intercept_)\nprint(\"coef: \", model.coef_)\nprint(\"rscore. \", model.score(X,y))","e32b8dc8":"# prediction\nplt.figure(figsize=(12,6))\ng = sns.regplot(x=data[\"gdp\"],y=data[\"score\"],ci=None,scatter_kws = {'color':'r','s':9})\ng.set_title(\"Model Equation\")\ng.set_ylabel(\"score\")\ng.set_xlabel(\"gdb\")\nplt.show()","dc629729":"# model.intercep_ + model.coef_ * 1\nmodel.predict([[1]])","8046f8fe":"gdb_list = [[0.25],[0.50],[0.75],[1.00],[1.25],[1.50]]\nmodel.predict(gdb_list)\nfor g in gdb_list:\n    print(\"The happiness value of the country with a gdp value of \",g,\": \",model.predict([g]))","686e04ad":"def linear_reg(col,text,prdctn):\n    \n    sns.jointplot(x=col,y=\"score\",data=df_table,kind=\"reg\")\n    plt.show()\n    \n    X = data[[col]]\n    y = data[[\"score\"]]\n    reg = LinearRegression()\n    model = reg.fit(X,y)\n    \n    # prediction\n    plt.figure(figsize=(12,6))\n    g = sns.regplot(x=data[col],y=data[\"score\"],ci=None,scatter_kws = {'color':'r','s':9})\n    g.set_title(\"Model Equation\")\n    g.set_ylabel(\"score\")\n    g.set_xlabel(col)\n    plt.show()\n    \n    print(text,\": \", model.predict([[prdctn]]))","3c9882fa":"linear_reg(\"social\",\"The happiness value of the country whose sociability value is 2:\",2)","93630426":"column_list = [\"score\",\"gdp\",\"social\",\"healthy\",\"freedom\",\"generosity\",\"corruption\"]","52ef46b5":"linear_reg(\"healthy\",\"The happiness value of the country whose healthiest value is 1.20:\",1.20)","bf98fdd3":"linear_reg(\"freedom\",\"The happiness value of the country whose freedom value is 0.89:\",0.89)","cf895879":"import statsmodels.api as sms\n\nX = df.drop(\"score\",axis=1)\ny = df[\"score\"]\n\n# OLS(dependent,independent)\nlm = sms.OLS(y,X)\nmodel = lm.fit()\nmodel.summary()","5c89f3bc":"# create model with sckit learn\n\nlm = LinearRegression()\nmodel = lm.fit(X,y)\nprint(\"constant: \",model.intercept_)\nprint(\"coefficient: \",model.coef_)","10be5d0f":"# PREDICTION\n# Score = 0.929921*gdp + 1.06504217*social + 0.94321492*healthy + 1.40426054*freedom + 0.52070628*generosity + 0.88114008*corruption\n\nnew_data = [[1],[2],[1.25],[1.75],[1.50],[0.75]]\nnew_data = pd.DataFrame(new_data).T\nnew_data","cb368919":"model.predict(new_data)","2adc41e6":"# calculating the amount of error\n\nfrom sklearn.metrics import mean_squared_error\n\nMSE = mean_squared_error(y,model.predict(X))\nRMSE = np.sqrt(MSE)\n\nprint(\"MSE: \", MSE)\nprint(\"RMSE: \", RMSE)","a17639d7":"from sklearn.model_selection import train_test_split\n\nX = df.drop(\"score\",axis=1)\ny = df[\"score\"]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nX_train.head()","d9e3ffc4":"X_test.head()","52dd93ed":"y_train.head()","3ee95145":"y_test.head()","482791f7":"lm = LinearRegression()\nlm.fit(X_train, y_train)\nprint(\"Training error\",np.sqrt(mean_squared_error(y_train,model.predict(X_train))))\nprint(\"Test error\",np.sqrt(mean_squared_error(y_test,model.predict(X_test))))","fb8ec1c2":"from sklearn.model_selection import cross_val_score\n\ncross_val_score(model, X_train, y_train, cv=10, scoring=\"neg_mean_squared_error\")","4314c325":"cvs_avg_mse = np.mean(-cross_val_score(model, X_train, y_train, cv=20, scoring=\"neg_mean_squared_error\"))\ncvs_avg_rmse = np.sqrt(cvs_avg_mse)\n\nprint(\"Cross Val Score MSE = \",cvs_avg_mse)\nprint(\"Cross Val Score RMSE = \",cvs_avg_rmse)","cb101652":"# Required Libraries\nimport numpy as np\nimport pandas as pd\nfrom sklearn.linear_model import Ridge\nfrom sklearn.metrics import mean_squared_error,r2_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import model_selection\nfrom sklearn.linear_model import RidgeCV","8fc0d283":"X = df.drop(\"score\",axis=1)\ny = df[\"score\"]\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nridge_model = Ridge(alpha=0.1).fit(X_train, y_train)\nridge_model","c71aa775":"ridge_model.coef_","58b7785f":"ridge_model.intercept_","6381c91b":"lambdas = 10**np.linspace(10,-2,100)*0.5 # Creates random numbers\nridge_model =  Ridge()\ncoefs = []\n\nfor i in lambdas:\n    ridge_model.set_params(alpha=i)\n    ridge_model.fit(X_train,y_train)\n    coefs.append(ridge_model.coef_)\n    \nax = plt.gca()\nax.plot(lambdas, coefs)\nax.set_xscale(\"log\")","67dd6aa0":"ridge_model = Ridge().fit(X_train,y_train)\n\ny_pred = ridge_model.predict(X_train)\n\nprint(\"predict: \", y_pred[0:10])\nprint(\"real: \", y_train[0:10].values)","bbdd26b5":"RMSE = np.mean(mean_squared_error(y_train,y_pred)) # rmse = square root of the mean of error squares\nprint(\"train error: \", RMSE)","fb2c54f4":"Verified_RMSE = np.sqrt(np.mean(-cross_val_score(ridge_model, X_train, y_train, cv=20, scoring=\"neg_mean_squared_error\")))\nprint(\"Verified_RMSE: \", Verified_RMSE)","5fc1f36b":"# test error\ny_pred = ridge_model.predict(X_test)\nRMSE = np.mean(mean_squared_error(y_test,y_pred))\nprint(\"test error: \", RMSE)","b16ae0f3":"ridge_model = Ridge(10).fit(X_train,y_train)\ny_pred = ridge_model.predict(X_test)\nnp.sqrt(mean_squared_error(y_test,y_pred))","c24d1ca0":"ridge_model = Ridge(30).fit(X_train,y_train)\ny_pred = ridge_model.predict(X_test)\nnp.sqrt(mean_squared_error(y_test,y_pred))","f82843d5":"ridge_model = Ridge(90).fit(X_train,y_train)\ny_pred = ridge_model.predict(X_test)\nnp.sqrt(mean_squared_error(y_test,y_pred))","353b2736":"lambdas1 = 10**np.linspace(10,-2,100)\nlambdas2 = np.random.randint(0,1000,100)\n\nridgeCV = RidgeCV(alphas = lambdas1,scoring = \"neg_mean_squared_error\", cv=10, normalize=True)\nridgeCV.fit(X_train,y_train)","5c238a4b":"ridgeCV.alpha_","00204cf4":"# final model\nridge_tuned = Ridge(alpha = ridgeCV.alpha_).fit(X_train,y_train)\ny_pred = ridge_tuned.predict(X_test)\nnp.sqrt(mean_squared_error(y_test,y_pred))","cc503b31":"# for lambdas2\nridgeCV = RidgeCV(alphas = lambdas2,scoring = \"neg_mean_squared_error\", cv=10, normalize=True)\nridgeCV.fit(X_train,y_train)\nridge_tuned = Ridge(alpha = ridgeCV.alpha_).fit(X_train,y_train)\ny_pred = ridge_tuned.predict(X_test)\nnp.sqrt(mean_squared_error(y_test,y_pred))","65d61965":"# Required Libraries\nimport numpy as np\nimport pandas as pd\nfrom sklearn.linear_model import Ridge,Lasso\nfrom sklearn.metrics import mean_squared_error,r2_score\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn import model_selection\nfrom sklearn.linear_model import RidgeCV, LassoCV","dd5beaed":"X = df.drop(\"score\",axis=1)\ny = df[\"score\"]\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n\nlasso_model = Lasso().fit(X_train,y_train)","40e9c198":"print(\"intercept: \", lasso_model.intercept_)\nprint(\"coef: \", lasso_model.coef_)","c5f30138":"# coefficients for different lambda values\n\nalphas = np.random.randint(0,10000,10)\nlasso = Lasso()\ncoefs = []\n\nfor a in alphas:\n    lasso.set_params(alpha=a)\n    lasso.fit(X_train,y_train)\n    coefs.append(lasso.coef_)","1a3b1571":"ax = plt.gca()\nax.plot(alphas,coefs)\nax.set_xscale(\"log\")","95dbf79b":"lasso_model","1e8ceff3":"lasso_model.predict(X_train)[0:5]","195d515a":"lasso_model.predict(X_test)[0:5]","25bb4580":"y_pred = lasso_model.predict(X_test)\nnp.sqrt(mean_squared_error(y_test,y_pred))","dabcc136":"r2_score(y_test,y_pred)","b61e2a33":"lasso_cv_model = LassoCV(cv=10,max_iter=100000).fit(X_train,y_train)\nlasso_cv_model","672a429a":"lasso_cv_model.alpha_","3716ead4":"lasso_tuned = Lasso().set_params(alpha= lasso_cv_model.alpha_).fit(X_train,y_train)\ny_pred = lasso_tuned.predict(X_test)\nnp.sqrt(mean_squared_error(y_test,y_pred))","2b6d5d0d":"# Required Libraries\nimport numpy as np\nimport pandas as pd\nfrom sklearn.linear_model import Ridge,Lasso,ElasticNet\nfrom sklearn.metrics import mean_squared_error,r2_score\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn import model_selection\nfrom sklearn.linear_model import RidgeCV, LassoCV, ElasticNetCV","70383062":"X = df.drop(\"score\",axis=1)\ny = df[\"score\"]\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n\nenet_model = ElasticNet().fit(X_train,y_train)","51b9748e":"enet_model.coef_","5533c05d":"enet_model.intercept_","683c4091":"# prediction\nenet_model.predict(X_train)[0:10]","bf919461":"enet_model.predict(X_test)[0:10]","a33b1d49":"y_pred = enet_model.predict(X_test)\nnp.sqrt(mean_squared_error(y_test,y_pred))","2143a551":"There are empty elements in only one column. Let's look at how many.","2799fc2d":"## Ridge Regression - Model","6df9f77f":"# Veriable Description\n\n1. Overall rank: Ranking of countries by happiness level\n2. Country or region: Country or region names\n3. Score: Happiness scores\n4. GDP per capita: Value representing the country's income and expense levels\n5. Social support\n6. Healthy life expectancy\n7. Freedom to make life choices\n8. Generosity\n9. Perceptions of corruption","aff28de8":"# Multiple Linear Regression <a id=\"2\"><\/a>\nThe main purpose is to find the linear function that expresses the relationship between dependent and independent variables.","7164173c":"# Data Preparation <a id=\"0\"><\/a>\n## Inconsistent Observation\n* 95% of a machine learning model is said to be preprocessing and 5% is model selection. For this we need to teach the data to the model correctly. In order to prepare the available data for machine learning, we must apply certain pre-processing methods. One of these methods is the analysis of outliers. The outlier is any data point that is substantially different from the rest of the observations in a data set. In other words, it is the observation that goes far beyond the general trend.\n\n![](https:\/\/miro.medium.com\/max\/854\/1*RW-vfIbKZh-UGsLfTAWpyw.png)","76eacfe7":"There are two values \u200b\u200babove. One of them is unverified, the other is the values \u200b\u200bthat represent the square root of the sum of the verified error squares. As you can see, the unverified value is almost half of the verified value. This result shows us that it is more correct to use the second method, not the first method, while taking the square root of the mean of the error squares.","cd34f3dc":"# Simple Linear Regressions <a id=\"1\"><\/a>\nSimple linear regression is a statistical method that allows us to summarize and analyze the relationships between two continuous (quantitative) variables:\n\n## score - gdp\nFirstly let's observe the relationship between gdp and score with the help of graphics.\n* independent variable : x\n* dependent variable : y","3e929f0f":"We have observed that there are outliers in the \"social\" and \"corruption\" column. This may cause us to negatively affect us while training our data set.","65780a48":"In contrast to the different beta values, the changes in the coefficients of the variables in our data set appear in the graph above. As can be seen, as the coefficients increase, it approaches zero.","db352c56":"rscore meaning:\n* For example, the gdp argument used here describes 63% of the data.","f4e17030":"# Lasso Regression <a id=\"5\"><\/a>\nThe aim is to find the coefficients that minimize the sum of error squares by applying a penalty to these coefficients.\n* Lasso regression = L1\n* Ridge regression = L2\n\n* It has been proposed to eliminate the disadvantage of leaving the related-unrelated variables in the model of the Ridge regression.\n* Coefficients near zero in Lasso.\n* But when the L1 norm is big enough in lambda, some coefficients make it zero. Thus, it makes the selection of the variable.\n* It is very important to choose Lambda correctly, CV is used here too.\n* Ridge and Lasso methods are not superior to each other.\n\n## Lasso Regression - Model ","fe984203":"## Lasso Regression - Prediction","c498966e":"Let's create a class and make the job easier.","5b3bb76d":"## Ridge Model - Model Tuning","ca3d0126":"Deleting data is not suitable for this data set. That's why we will fill out the outliers with the average.","ca064fd1":"## score - social","8af04225":"# Load and Check Data <a id=\"-1\"><\/a>","21c45051":"R-squared: Percentages of independent variables that explain the change in dependent variables. <br>\nF-statistic: Expresses the significance of the model. <br>\ncoef: refers to coefficients. <br>\nstd err: standard errors. <br><br>\n\nHere we can make the following comments.\n- When the gdp value is increased by 1, the score increases by 0.8114.\n- When there is an increase of 1 unit from the social value, the score increases by 1.9740.\n...","ef80094d":"An alpha value will be assigned with each coefficient. Error coefficients will be examined according to these values.","d9b96e36":"- [Load and Check Data](#-1)\n- [Data Preparation](#0)\n- [Simple Linear Regressions](#1)\n- [Multiple Linear Regression](#2)\n- [Ridge Regression](#4)\n- [Lasso Regression](#5)\n- [ElasticNet Regression](#6)","719b4f11":"What we want to do here is; For example, to answer the question of what is the happiness level of a country with a gdp value of 1. In other words, to estimate the desired value with the existing data set.","5a566c99":"Outlier values behave differently from other data models and they increase the error with overfitting, so the outlier model must be detected and some operations must be performed on it.\n### 1.Using Box Graph\nWe can see contradictory observations with many visualization techniques. One of them is the box chart. If there is an outlier, this is drawn as the point, but the other population is grouped together and displayed in boxes.","073c288c":"# ElasticNet Regression <a id=\"6\"><\/a>\n* The aim is to find the coefficients that minimize the sum of error squares by applying a penalty.\n* ElasticNet combines L1 and L2 approaches.The aim is to find the coefficients that minimize the sum of error squares by applying a penalty.\n\n## ElasticNet Regression - Model & Prediction","b0d1475e":"## score - freedom","89a4f179":"# Ridge Regression <a id=\"4\"><\/a>\nThe aim is to find the coefficients that minimize the sum of error squares by applying a penalty to these coefficients.\n* It is resistant to over learning.\n* It is biased but its variance is low.\n* It is better than OLS when there are too many parameters.\n* Builds a model with all variables. It does not exclude the unrelated variables from the model, it approximates its coefficients to zero.\n\n![](https:\/\/i.ibb.co\/2SJtqyB\/Ek-A-klama-2020-04-21-202339.jpg)\n\n* The delta parameter that gives the smallest \"cross validation\" value is selected.\n* With this delta selected, the model is fit for observations again.","5d814638":"# Simple Linear & Multiple Linear Regression - Model Tuning <a id=\"3\"><\/a>","f5b0c502":"We can use alpha_ feature to attract the most appropriate value.","51036289":"## Lasso Regression - Model Tuning","d07e5a0e":"# Missing Value ","999fcc72":"Let's change the column names for convenience.","ce86cc28":"Every time we change the random_state value we defined at first, a different result is returned. We need to find out which of these returns the best result. For this we need to do the following.","3ff7c080":"We can find out which value will work better by trial and error. But with the method we will use below, we can find the most appropriate value more easily and quickly.","bc29671e":"## Ridge Regression - Prediction","cf310fa1":"## score - healthy"}}