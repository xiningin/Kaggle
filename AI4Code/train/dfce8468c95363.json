{"cell_type":{"5846ffd7":"code","2bb14321":"code","3c871823":"code","9f3d72d8":"code","d34fe211":"code","3a3018c3":"code","20966aa7":"code","2ccc6cfc":"code","2f05b9bd":"code","3fc767ef":"code","00047eed":"code","c935a729":"code","09bd7475":"markdown","61882539":"markdown","19dcbce6":"markdown","acc74c86":"markdown","a8a39e67":"markdown","808e61fe":"markdown","be1397d9":"markdown"},"source":{"5846ffd7":"a = int(input('Enter your Name or Enrollment no:- '))","2bb14321":"import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\nfrom sklearn import metrics","3c871823":"# Load input data\nX = np.loadtxt('..\/input\/dataclustering\/data_clustering.txt', delimiter =',')\n\n# Defining numbers of classes we need\nnum_clusters = 5\n\n# Plot input data\nplt.figure()\nplt.scatter(X[:,0], X[:,1], marker='o', facecolors='none', \n        edgecolors='black', s=80)\nx_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\ny_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\nplt.title(f'Input data ( {a} )')\nplt.xlim(x_min, x_max)\nplt.ylim(y_min, y_max)\nplt.xticks(())\nplt.yticks(())","9f3d72d8":"# Create KMeans object \nkmeans = KMeans(init='k-means++', n_clusters=num_clusters, n_init=10)\n\n# Train the KMeans clustering model\nkmeans.fit(X)\n\n# Step size of the mesh\nstep_size = 0.01\n\n# Define the grid of points to plot the boundaries\nx_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\ny_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\nx_vals, y_vals = np.meshgrid(np.arange(x_min, x_max, step_size), \n        np.arange(y_min, y_max, step_size))","d34fe211":"# Predict output labels for all the points on the grid \noutput = kmeans.predict(np.c_[x_vals.ravel(), y_vals.ravel()])\n\n# Plot different regions and color them \noutput = output.reshape(x_vals.shape)\nplt.figure()\nplt.clf()\nplt.imshow(output, interpolation='nearest',\n           extent=(x_vals.min(), x_vals.max(), \n               y_vals.min(), y_vals.max()),\n           cmap=plt.cm.Paired, \n           aspect='auto', \n           origin='lower')\n# Overlay input points\nplt.scatter(X[:,0], X[:,1], marker='o', facecolors='none', \n        edgecolors='black', s=80)\n\n# Plot the centers of clusters\ncluster_centers = kmeans.cluster_centers_\nplt.scatter(cluster_centers[:,0], cluster_centers[:,1], \n        marker='o', s=210, linewidths=4, color='black', \n        zorder=12, facecolors='black')\n\nx_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\ny_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\nplt.title(f'Boundaries of clusters ( {a} )')\nplt.xlim(x_min, x_max)\nplt.ylim(y_min, y_max)\nplt.xticks(())\nplt.yticks(())\nplt.show()","3a3018c3":"# Defining numbers of classes we need        <=== 2\nnum_clusters = 2\n\nkmeans = KMeans(init='k-means++', n_clusters=num_clusters, n_init=10)\n\n# Train the KMeans clustering model\nkmeans.fit(X)\n\n# Step size of the mesh\nstep_size = 0.01\n\n# Define the grid of points to plot the boundaries\nx_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\ny_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\nx_vals, y_vals = np.meshgrid(np.arange(x_min, x_max, step_size), \n        np.arange(y_min, y_max, step_size))\n\n# Predict output labels for all the points on the grid \noutput = kmeans.predict(np.c_[x_vals.ravel(), y_vals.ravel()])\n\n# Plot different regions and color them \noutput = output.reshape(x_vals.shape)\nplt.figure()\nplt.clf()\nplt.imshow(output, interpolation='nearest',\n           extent=(x_vals.min(), x_vals.max(), \n               y_vals.min(), y_vals.max()),\n           cmap=plt.cm.Paired, \n           aspect='auto', \n           origin='lower')\n# Overlay input points\nplt.scatter(X[:,0], X[:,1], marker='o', facecolors='none', \n        edgecolors='black', s=80)\n\n# Plot the centers of clusters\ncluster_centers = kmeans.cluster_centers_\nplt.scatter(cluster_centers[:,0], cluster_centers[:,1], \n        marker='o', s=210, linewidths=4, color='black', \n        zorder=12, facecolors='black')\n\nx_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\ny_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\nplt.title(f'Boundaries of 2 clusters ( {a} )')\nplt.xlim(x_min, x_max)\nplt.ylim(y_min, y_max)\nplt.xticks(())\nplt.yticks(())\nplt.show()","20966aa7":"# Defining numbers of classes we need        <=== 3\nnum_clusters = 3\n\nkmeans = KMeans(init='k-means++', n_clusters=num_clusters, n_init=10)\n\n# Train the KMeans clustering model\nkmeans.fit(X)\n\n# Step size of the mesh\nstep_size = 0.01\n\n# Define the grid of points to plot the boundaries\nx_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\ny_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\nx_vals, y_vals = np.meshgrid(np.arange(x_min, x_max, step_size), \n        np.arange(y_min, y_max, step_size))\n\n# Predict output labels for all the points on the grid \noutput = kmeans.predict(np.c_[x_vals.ravel(), y_vals.ravel()])\n\n# Plot different regions and color them \noutput = output.reshape(x_vals.shape)\nplt.figure()\nplt.clf()\nplt.imshow(output, interpolation='nearest',\n           extent=(x_vals.min(), x_vals.max(), \n               y_vals.min(), y_vals.max()),\n           cmap=plt.cm.Paired, \n           aspect='auto', \n           origin='lower')\n# Overlay input points\nplt.scatter(X[:,0], X[:,1], marker='o', facecolors='none', \n        edgecolors='black', s=80)\n\n# Plot the centers of clusters\ncluster_centers = kmeans.cluster_centers_\nplt.scatter(cluster_centers[:,0], cluster_centers[:,1], \n        marker='o', s=210, linewidths=4, color='black', \n        zorder=12, facecolors='black')\n\nx_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\ny_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\nplt.title(f'Boundaries of 3 clusters ( {a} )')\nplt.xlim(x_min, x_max)\nplt.ylim(y_min, y_max)\nplt.xticks(())\nplt.yticks(())\nplt.show()","2ccc6cfc":"# Defining numbers of classes we need        <=== 4\nnum_clusters = 4\n\nkmeans = KMeans(init='k-means++', n_clusters=num_clusters, n_init=10)\n\n# Train the KMeans clustering model\nkmeans.fit(X)\n\n# Step size of the mesh\nstep_size = 0.01\n\n# Define the grid of points to plot the boundaries\nx_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\ny_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\nx_vals, y_vals = np.meshgrid(np.arange(x_min, x_max, step_size), \n        np.arange(y_min, y_max, step_size))\n\n# Predict output labels for all the points on the grid \noutput = kmeans.predict(np.c_[x_vals.ravel(), y_vals.ravel()])\n\n# Plot different regions and color them \noutput = output.reshape(x_vals.shape)\nplt.figure()\nplt.clf()\nplt.imshow(output, interpolation='nearest',\n           extent=(x_vals.min(), x_vals.max(), \n               y_vals.min(), y_vals.max()),\n           cmap=plt.cm.Paired, \n           aspect='auto', \n           origin='lower')\n# Overlay input points\nplt.scatter(X[:,0], X[:,1], marker='o', facecolors='none', \n        edgecolors='black', s=80)\n\n# Plot the centers of clusters\ncluster_centers = kmeans.cluster_centers_\nplt.scatter(cluster_centers[:,0], cluster_centers[:,1], \n        marker='o', s=210, linewidths=4, color='black', \n        zorder=12, facecolors='black')\n\nx_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\ny_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\nplt.title(f'Boundaries of 4 clusters ( {a} )')\nplt.xlim(x_min, x_max)\nplt.ylim(y_min, y_max)\nplt.xticks(())\nplt.yticks(())\nplt.show()","2f05b9bd":"# Defining numbers of classes we need        <=== 5\nnum_clusters = 5\n\nkmeans = KMeans(init='k-means++', n_clusters=num_clusters, n_init=10)\n\n# Train the KMeans clustering model\nkmeans.fit(X)\n\n# Step size of the mesh\nstep_size = 0.01\n\n# Define the grid of points to plot the boundaries\nx_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\ny_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\nx_vals, y_vals = np.meshgrid(np.arange(x_min, x_max, step_size), \n        np.arange(y_min, y_max, step_size))\n\n# Predict output labels for all the points on the grid \noutput = kmeans.predict(np.c_[x_vals.ravel(), y_vals.ravel()])\n\n# Plot different regions and color them \noutput = output.reshape(x_vals.shape)\nplt.figure()\nplt.clf()\nplt.imshow(output, interpolation='nearest',\n           extent=(x_vals.min(), x_vals.max(), \n               y_vals.min(), y_vals.max()),\n           cmap=plt.cm.Paired, \n           aspect='auto', \n           origin='lower')\n# Overlay input points\nplt.scatter(X[:,0], X[:,1], marker='o', facecolors='none', \n        edgecolors='black', s=80)\n\n# Plot the centers of clusters\ncluster_centers = kmeans.cluster_centers_\nplt.scatter(cluster_centers[:,0], cluster_centers[:,1], \n        marker='o', s=210, linewidths=4, color='black', \n        zorder=12, facecolors='black')\n\nx_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\ny_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\nplt.title(f'Boundaries of 5 clusters ( {a} )')\nplt.xlim(x_min, x_max)\nplt.ylim(y_min, y_max)\nplt.xticks(())\nplt.yticks(())\nplt.show()","3fc767ef":"# Defining numbers of classes we need        \nnum_clusters = 5\n\nkmeans = KMeans(init='k-means++', n_clusters=num_clusters, n_init=5) # <<===== Changing the value of n_init\n\n# Train the KMeans clustering model\nkmeans.fit(X)\n\n# Step size of the mesh\nstep_size = 0.01\n\n# Define the grid of points to plot the boundaries\nx_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\ny_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\nx_vals, y_vals = np.meshgrid(np.arange(x_min, x_max, step_size), \n        np.arange(y_min, y_max, step_size))\n\n# Predict output labels for all the points on the grid \noutput = kmeans.predict(np.c_[x_vals.ravel(), y_vals.ravel()])\n\n# Plot different regions and color them \noutput = output.reshape(x_vals.shape)\nplt.figure()\nplt.clf()\nplt.imshow(output, interpolation='nearest',\n           extent=(x_vals.min(), x_vals.max(), \n               y_vals.min(), y_vals.max()),\n           cmap=plt.cm.Paired, \n           aspect='auto', \n           origin='lower')\n# Overlay input points\nplt.scatter(X[:,0], X[:,1], marker='o', facecolors='none', \n        edgecolors='black', s=80)\n\n# Plot the centers of clusters\ncluster_centers = kmeans.cluster_centers_\nplt.scatter(cluster_centers[:,0], cluster_centers[:,1], \n        marker='o', s=210, linewidths=4, color='black', \n        zorder=12, facecolors='black')\n\nx_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\ny_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\nplt.title(f'Cluseter with initialization value of 5 ( {a} )')\nplt.xlim(x_min, x_max)\nplt.ylim(y_min, y_max)\nplt.xticks(())\nplt.yticks(())\nplt.show()","00047eed":"# Defining numbers of classes we need        \nnum_clusters = 5\n\nkmeans = KMeans(init='k-means++', n_clusters=num_clusters, n_init=10) # <<===== Changing the value of n_init\n\n# Train the KMeans clustering model\nkmeans.fit(X)\n\n# Step size of the mesh\nstep_size = 0.01\n\n# Define the grid of points to plot the boundaries\nx_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\ny_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\nx_vals, y_vals = np.meshgrid(np.arange(x_min, x_max, step_size), \n        np.arange(y_min, y_max, step_size))\n\n# Predict output labels for all the points on the grid \noutput = kmeans.predict(np.c_[x_vals.ravel(), y_vals.ravel()])\n\n# Plot different regions and color them \noutput = output.reshape(x_vals.shape)\nplt.figure()\nplt.clf()\nplt.imshow(output, interpolation='nearest',\n           extent=(x_vals.min(), x_vals.max(), \n               y_vals.min(), y_vals.max()),\n           cmap=plt.cm.Paired, \n           aspect='auto', \n           origin='lower')\n# Overlay input points\nplt.scatter(X[:,0], X[:,1], marker='o', facecolors='none', \n        edgecolors='black', s=80)\n\n# Plot the centers of clusters\ncluster_centers = kmeans.cluster_centers_\nplt.scatter(cluster_centers[:,0], cluster_centers[:,1], \n        marker='o', s=210, linewidths=4, color='black', \n        zorder=12, facecolors='black')\n\nx_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\ny_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\nplt.title(f'Cluseter with initialization value of 10 ( {a} )')\nplt.xlim(x_min, x_max)\nplt.ylim(y_min, y_max)\nplt.xticks(())\nplt.yticks(())\nplt.show()","c935a729":"# Defining numbers of classes we need        \nnum_clusters = 5\n\nkmeans = KMeans(init='k-means++', n_clusters=num_clusters, n_init=20) # <<===== Changing the value of n_init\n\n# Train the KMeans clustering model\nkmeans.fit(X)\n\n# Step size of the mesh\nstep_size = 0.01\n\n# Define the grid of points to plot the boundaries\nx_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\ny_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\nx_vals, y_vals = np.meshgrid(np.arange(x_min, x_max, step_size), \n        np.arange(y_min, y_max, step_size))\n\n# Predict output labels for all the points on the grid \noutput = kmeans.predict(np.c_[x_vals.ravel(), y_vals.ravel()])\n\n# Plot different regions and color them \noutput = output.reshape(x_vals.shape)\nplt.figure()\nplt.clf()\nplt.imshow(output, interpolation='nearest',\n           extent=(x_vals.min(), x_vals.max(), \n               y_vals.min(), y_vals.max()),\n           cmap=plt.cm.Paired, \n           aspect='auto', \n           origin='lower')\n# Overlay input points\nplt.scatter(X[:,0], X[:,1], marker='o', facecolors='none', \n        edgecolors='black', s=80)\n\n# Plot the centers of clusters\ncluster_centers = kmeans.cluster_centers_\nplt.scatter(cluster_centers[:,0], cluster_centers[:,1], \n        marker='o', s=210, linewidths=4, color='black', \n        zorder=12, facecolors='black')\n\nx_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\ny_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\nplt.title(f'Cluseter with initialization value of 20 ( {a} )')\nplt.xlim(x_min, x_max)\nplt.ylim(y_min, y_max)\nplt.xticks(())\nplt.yticks(())\nplt.show()","09bd7475":"#### Q2. Use different initialization values (i.e. 5\/10\/20) and check the effect of it on resultant boundries.\n##### Solution: ","61882539":"# K-Means Clustering\n\n#### In general Idea :-\nClustering is a simple process in  which we are creating server which consist of nodes or small compute units and which will be control by a master node which alots a particular task to perform by each node. \n\n## What is clustering in Machine learning?\n\nOver Here we are dealing with data, which are nothing but numbers which are points of different classes. Now if we compare the above defination of clustering with clustering in machine learning, we get something like this:\n\n- Representing Node as data points and master node as Centroid or centre of class.\n\nNow if look into it than we can say that Clustering in Machine learning is nothing but seperation of the data according to position of Centroid and boundries which are separating all the classes from each other. \n\n\n## Information about K-Means Clustering:\n\nThe KMeans algorithm clusters data by trying to separate samples in n groups of equal variance, minimizing a criterion known as the inertia or within-cluster sum-of-squares (see below). This algorithm requires the number of clusters to be specified. It scales well to large number of samples and has been used across a large range of application areas in many different fields.\n\nThe k-means algorithm divides a set of $N$ samples $X$ into $K$ disjoint clusters $C$, each described by the mean $\\mu_j$ of the samples in the cluster. The means are commonly called the cluster \u201ccentroids\u201d; note that they are not, in general, points from $X$, although they live in the same space.\n\nThe K-Means algorithm aims to choose centroids that minimise the **inertia** , or **within cluster sum-of-squares criterion**:\n\n**$$\\sum_{i=0}^{n}\\min_{\\mu_j \\in C}(||x_i - \\mu_j||^2)$$**\n\nInertia can be recognized as a measure of how internally coherent clusters are. It suffers from various drawbacks:\n\n- Inertia makes the assumption that clusters are convex and isotropic, which is not always the case. It responds poorly to elongated clusters, or manifolds with irregular shapes.\n\n- Inertia is not a normalized metric: we just know that lower values are better and zero is optimal. But in very high-dimensional spaces, Euclidean distances tend to become inflated (this is an instance of the so-called \u201ccurse of dimensionality\u201d). Running a dimensionality reduction algorithm such as Principal component analysis (PCA) prior to k-means clustering can alleviate this problem and speed up the computations.\n\n\nK-means is often referred to as Lloyd\u2019s algorithm.\n\nIn basic terms, the algorithm has three steps:\n\n- The first step chooses the initial centroids, with the most basic method being to choose $k$ samples from the dataset $X$.\n\n- After initialization, K-means consists of looping between the two other steps. The first step assigns each sample to its nearest centroid.\n\n- The second step creates new centroids by taking the mean value of all of the samples assigned to each previous centroid.\n\n- The difference between the old and the new centroids are computed and the algorithm repeats these last two steps until this value is less than a threshold. \n\n- In other words, it repeats until the centroids do not move significantly.\n\nK-means is equivalent to the expectation-maximization algorithm with a small, all-equal, diagonal covariance matrix.\n\n<img src = \"https:\/\/scikit-learn.org\/stable\/_images\/sphx_glr_plot_kmeans_digits_0011.png\" width = \"700\">\n\nThe algorithm can also be understood through the concept of Voronoi diagrams. First the Voronoi diagram of the points is calculated using the current centroids. Each segment in the Voronoi diagram becomes a separate cluster. Secondly, the centroids are updated to the mean of each segment. The algorithm then repeats this until a stopping criterion is fulfilled. Usually, the algorithm stops when the relative decrease in the objective function between iterations is less than the given tolerance value. This is not the case in this implementation: iteration stops when centroids move less than the tolerance.","19dcbce6":"#### For Practice purpose:- \n\n#### Q1. . Implement the k-means clustering algorithm with different number of clusters. (i.e. 2\/3\/4\/5) and comment on criteria to determine number of clusters.\n\n##### Solution:","acc74c86":"Given enough time, K-means will always converge, however this may be to a local minimum. This is highly dependent on the initialization of the centroids. As a result, the computation is often done several times, with different initializations of the centroids. One method to help address this issue is the k-means++ initialization scheme, which has been implemented in scikit-learn (use the ``init='k-means++'`` parameter). This initializes the centroids to be (generally) distant from each other, leading to probably better results than random initialization, as shown in the reference.\n\nK-means++ can also be called independently to select seeds for other clustering algorithms.\n\nThe algorithm supports sample weights, which can be given by a parameter ``sample_weight``. This allows to assign more weight to some samples when computing cluster centers and values of inertia. For example, assigning a weight of 2 to a sample is equivalent to adding a duplicate of that sample to the dataset $X$.\n\nK-means can be used for vector quantization. This is achieved using the transform method of a trained model of `KMeans`.","a8a39e67":"# Thank you for reading this Notebook, if you like please UpVote this Notebook.....\ud83d\udc4d\n\n#### We can also use different kinds of data over here, like the sklearn breast cancer, wine, ad hoc, etc.....","808e61fe":"### Modules used for this:\n\n- **`numpy` :- Numerical Python used for mathematical operations and scientific calculations.**\n\n- **`matplotlib` :- A visualization tool used for plotting purpose.**\n\n- **`sklearn` :- Scikit-learn is a ML library, used to apply different ML algorithm, Which are already defined in it**","be1397d9":"#### Here the darkest spots are centroids."}}