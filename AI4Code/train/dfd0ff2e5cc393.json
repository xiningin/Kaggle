{"cell_type":{"47d13ab4":"code","c3d67cae":"code","64b69222":"code","1e4dd604":"code","7b34f375":"code","9198158a":"code","1d70b047":"code","f391be7f":"markdown","efdef4ac":"markdown","e07cae0f":"markdown","1537a13b":"markdown","8b51857f":"markdown","5f0b996a":"markdown","9a525daa":"markdown","d4e6c862":"markdown","cd73f359":"markdown"},"source":{"47d13ab4":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport os\nimport pandas as pd\nimport numpy as np\nimport sklearn\nfrom sklearn.model_selection import KFold\nfrom sklearn.tree import DecisionTreeClassifier\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nprint(os.listdir(\"..\/input\/prepare-data-for-decision-trees-algorithms\"))\n\n# Any results you write to the current directory are saved as output.","c3d67cae":"# Now load the training set\nprint('Loading the training set:')\ndtypes = {\n    'MachineIdentifier': 'category',\n    'AVProductsInstalled': 'float32',\n    'CountryIdentifier': 'float32',\n    'OrganizationIdentifier': 'float32',\n    'GeoNameIdentifier': 'float32',\n    'LocaleEnglishNameIdentifier': 'float32',\n    'OsBuild': 'int16',\n    'OsSuite': 'float32',\n    'OsPlatformSubRelease': 'float32',\n    'SkuEdition': 'float32',\n    'IeVerIdentifier': 'float32',\n    'SmartScreen': 'float32',\n    'Census_MDC2FormFactor': 'float32',\n    'Census_ProcessorCoreCount': 'float16',\n    'Census_ProcessorManufacturerIdentifier': 'float32',\n    'Census_PrimaryDiskTotalCapacity': 'float32',\n    'Census_PrimaryDiskTypeName': 'float32',\n    'Census_SystemVolumeTotalCapacity': 'float32',\n    'Census_TotalPhysicalRAM': 'float32',\n    'Census_ChassisTypeName': 'float32',\n    'Census_InternalPrimaryDiagonalDisplaySizeInInches': 'float16',\n    'Census_InternalPrimaryDisplayResolutionHorizontal': 'float16',\n    'Census_InternalPrimaryDisplayResolutionVertical': 'float16',\n    'Census_PowerPlatformRoleName': 'float32',\n    'Census_InternalBatteryType': 'float32',\n    'Census_InternalBatteryNumberOfCharges': 'float32',\n    'Census_OSBranch': 'float32',\n    'Census_OSBuildNumber': 'int16',\n    'Census_OSBuildRevision': 'int32',\n    'Census_OSEdition': 'float32',\n    'Census_OSSkuName': 'float32',\n    'Census_OSInstallTypeName': 'float32',\n    'Census_OSInstallLanguageIdentifier': 'float32',\n    'Census_OSUILocaleIdentifier': 'float32',\n    'Census_OSWUAutoUpdateOptionsName': 'float32',\n    'Census_GenuineStateName': 'float32',\n    'Census_ActivationChannel': 'float32',\n    'Census_IsFlightingInternal': 'float32',\n    'Census_ThresholdOptIn': 'float16',\n    'Census_IsSecureBootEnabled': 'int8',\n    'Census_IsWIMBootEnabled': 'float32',\n    'Census_IsTouchEnabled': 'int8',\n    'Wdft_IsGamer': 'float32',\n    'Wdft_RegionIdentifier': 'float32',\n    'HasDetections': 'int8',\n    'EngineVersion_0': 'float32',\n    'EngineVersion_1': 'float32',\n    'EngineVersion_2': 'float32',\n    'EngineVersion_3': 'float32',\n    'AppVersion_0': 'float32',\n    'AppVersion_1': 'float32',\n    'AppVersion_2': 'float32',\n    'AppVersion_3': 'float32',\n    'AvSigVersion_0': 'float32',\n    'AvSigVersion_1': 'float32',\n    'AvSigVersion_2': 'float32',\n    'AvSigVersion_3': 'float32',\n    'Census_OSVersion_0': 'float32',\n    'Census_OSVersion_1': 'float32',\n    'Census_OSVersion_2': 'float32',\n    'Census_OSVersion_3': 'float32'\n}\ntraining_set = pd.read_csv('..\/input\/prepare-data-for-decision-trees-algorithms\/training_decisionTrees.csv', dtype=dtypes)\nprint('Training set loaded')\nprint(training_set.shape)\n","64b69222":"# Handle Nans\ndef create_nan_dict(data):\n    ret = dict()\n    for col in data:\n        if col != 'HasDetections' and col != 'MachineIdentifier':\n            ret[col] = data[col].astype('float32').mean()\n    return ret\n\n\ndef remove_nans(data, nan_dict):\n    for col in data:\n        if col != 'HasDetections' and col != 'MachineIdentifier':\n            data[col] = data[col].fillna(nan_dict[col])\n\nprint('Handling NaN in training set')\nnan_dict = create_nan_dict(training_set)\nremove_nans(training_set, nan_dict)\nprint('Done')","1e4dd604":"columns_to_remove = []\nfor col_name in training_set.columns.values:\n    if col_name == 'HasDetections' or col_name == 'MachineIdentifier':\n        continue\n    unique_values = training_set[col_name].value_counts(dropna=False)\n    msg = 'column ' + col_name + ' have ' + str(len(unique_values)) + ' unique values. The bigger category has ' + str(100 * unique_values.values[0] \/ training_set.shape[0]) + ' percent of the data'\n    if len(unique_values)==1:\n        msg = msg + \" - removed\"\n        del training_set[col_name]  \n        columns_to_remove.append(col_name)\n    print(msg)\n\nprint('')\nprint('Untill now ' + str(len(columns_to_remove)) + ' colums removed')\nprint(training_set.shape)","7b34f375":"def fine_tune_decision_tree(training_set, k_fold):\n    results = dict()\n    avg_grade = dict()\n    std_grade = dict()\n    min_grade = dict()\n    max_grade = dict()\n    best_sample_leaf = 0\n    best_grade = 0.5\n    for min_samples_leaf in [200, 400, 500, 600, 800, 1000]:\n        features = [c for c in training_set.columns if c not in ['MachineIdentifier', 'HasDetections']]\n        dt = DecisionTreeClassifier(min_samples_leaf=min_samples_leaf)\n        results[min_samples_leaf] = []\n\n        # Train and tests the data on k_fold splits and store the results\n        for train_indices, test_indices in k_fold.split(training_set):\n            print('Start fitting')\n            dt.fit(training_set[features].iloc[train_indices], training_set['HasDetections'].iloc[train_indices])\n            print('End fitting - Start predicting')\n            prob = dt.predict_proba(training_set[features].iloc[test_indices])\n            fpr, tpr, thresholds = sklearn.metrics.roc_curve(training_set['HasDetections'].iloc[test_indices], prob[:, 1])\n            results[min_samples_leaf].append(sklearn.metrics.auc(fpr, tpr))\n            print('End predicting')\n\n        grade = np.mean(results[min_samples_leaf])\n        if grade > best_grade:\n            best_grade = grade\n            best_sample_leaf = min_samples_leaf\n        avg_grade[min_samples_leaf] = grade\n        std_grade[min_samples_leaf] = np.std(results[min_samples_leaf])\n        min_grade[min_samples_leaf] = np.min(results[min_samples_leaf])\n        max_grade[min_samples_leaf] = np.max(results[min_samples_leaf])\n\n    # Now plot the result.\n    n_leafs = avg_grade.keys()\n    avgs = [avg_grade[l] for l in n_leafs]\n    stds = [std_grade[l] for l in n_leafs]\n    plt.figure()\n    plt.errorbar(n_leafs, avgs, stds)\n    plt.title('decision tree classifier k fold results')\n    plt.xlabel('number of minimum sample in a leaf')\n    plt.ylabel('ROC curve area')\n    plt.show()\n\n    return DecisionTreeClassifier(min_samples_leaf=best_sample_leaf)\n\n# Define decision tree predictor and fine tune its variables\nk_fold = KFold(n_splits=5, shuffle=True)\nclassifier = fine_tune_decision_tree(training_set, k_fold)","9198158a":"features = [c for c in training_set.columns if c not in ['MachineIdentifier', 'HasDetections']]\nX = training_set[features]\ny = training_set['HasDetections']\ndel training_set\nclassifier.fit(X, y)","1d70b047":"del X\ndel y\ntest = pd.read_csv('..\/input\/prepare-data-for-decision-trees-algorithms\/test_decisionTrees.csv', dtype=dtypes)\nremove_nans(test, nan_dict)\n\nX = test[features]\ny_pred = classifier.predict_proba(X)[:, 1]\nto_submit = pd.DataFrame(test['MachineIdentifier'])\nto_submit['HasDetections'] = y_pred\nto_submit.to_csv('decisionTreeClassifierRes.csv', index=False)","f391be7f":"# decision tree method lerning","efdef4ac":"First:\nscikit learn decision tree can have NaN in the data. So whenever I have NaN, I convert it to the average of the column.\nI will try other methos later.","e07cae0f":"# Perpare the data","1537a13b":"# Calculting prediction on the test set and saving the results","8b51857f":"This part is a fine tuning. For now, the only parameter that need to be fine turned is min_samples_leaf\nI try some min_samples_leaf values.\nFor each value, I take 4\/5 of the trainnig set as train and 1\/5 as test (KFold). I train and check the ROC results. I take the min_samples_leaf value that gives me the best result.","5f0b996a":"Now we remove colums that have only one value. Those colums are not relevant for Decision Tree and can be removed for run time","9a525daa":"# Learning","d4e6c862":"# Load the dataset\nLoad the dataset from a pre-prepered dataset - See https:\/\/www.kaggle.com\/itamargr\/prepare-data-for-decision-trees-algorithms)","cd73f359":"Now train with the hole set"}}