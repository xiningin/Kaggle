{"cell_type":{"ec626f98":"code","066f18ea":"code","30714971":"code","c0fe83d2":"code","2b283d4b":"code","0534ffcd":"code","4dd13481":"code","e83534c6":"code","e67c2542":"code","02354d1d":"code","d8bb1fd6":"code","95c20355":"markdown","7de986aa":"markdown","11d38e5b":"markdown","473a4368":"markdown","911c3bd0":"markdown","a4dc3acd":"markdown","adb2db1e":"markdown","98a43aed":"markdown","eaaa548a":"markdown","e6a2fa08":"markdown","4c74acc6":"markdown"},"source":{"ec626f98":"import re\nimport numpy as np\nimport pandas as pd\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.models import Model\n","066f18ea":"lines = open('..\/input\/chatbot-data\/cornell movie-dialogs corpus\/movie_lines.txt', encoding='utf-8',\n             errors='ignore').read().split('\\n')\n\nconvers = open('..\/input\/chatbot-data\/cornell movie-dialogs corpus\/movie_conversations.txt', encoding='utf-8',\n             errors='ignore').read().split('\\n')\n","30714971":"exchn = []\nfor conver in convers:\n    exchn.append(conver.split(' +++$+++ ')[-1][1:-1].replace(\"'\", \" \").replace(\",\",\"\").split())\n\ndiag = {}\nfor line in lines:\n    diag[line.split(' +++$+++ ')[0]] = line.split(' +++$+++ ')[-1]\n\n## delete\ndel(lines, convers, conver, line)","c0fe83d2":"questions = []\nanswers = []\n\nfor conver in exchn:\n    for i in range(len(conver) - 1):\n        questions.append(diag[conver[i]])\n        answers.append(diag[conver[i+1]])\ndel(diag, exchn, conver, i)\n\n# More preprocessing of QnA\n# Maximum Length of Questions= 13    \n\nsorted_ques = []\nsorted_ans = []\nfor i in range(len(questions)):\n    if len(questions[i]) < 13:\n        sorted_ques.append(questions[i])\n        sorted_ans.append(answers[i])\n","2b283d4b":"def clean_text(txt):\n    txt = txt.lower()\n    txt = re.sub(r\"i'm\", \"i am\", txt)\n    txt = re.sub(r\"he's\", \"he is\", txt)\n    txt = re.sub(r\"she's\", \"she is\", txt)\n    txt = re.sub(r\"that's\", \"that is\", txt)\n    txt = re.sub(r\"what's\", \"what is\", txt)\n    txt = re.sub(r\"where's\", \"where is\", txt)\n    txt = re.sub(r\"\\'ll\", \" will\", txt)\n    txt = re.sub(r\"\\'ve\", \" have\", txt)\n    txt = re.sub(r\"\\'re\", \" are\", txt)\n    txt = re.sub(r\"\\'d\", \" would\", txt)\n    txt = re.sub(r\"won't\", \"will not\", txt)\n    txt = re.sub(r\"can't\", \"can not\", txt)\n    txt = re.sub(r\"[^\\w\\s]\", \"\", txt)\n    return txt\n\nclean_ques = []\nclean_ans = []\n\nfor line in sorted_ques:\n    clean_ques.append(clean_text(line))\n    \nfor line in sorted_ans:\n    clean_ans.append(clean_text(line))\n\nfor i in range(len(clean_ans)):\n    clean_ans[i] = ' '.join(clean_ans[i].split()[:13])\n\ndel(answers, questions, line,sorted_ans, sorted_ques)\n\n\n# trimming\nclean_ans=clean_ans[:35000]\nclean_ques=clean_ques[:35000]","0534ffcd":"#  Count Occurences \nword2count = {}\n\nfor line in clean_ques:\n    for word in line.split():\n        if word not in word2count:\n            word2count[word] = 1\n        else:\n            word2count[word] += 1\n            \nfor line in clean_ans:\n    for word in line.split():\n        if word not in word2count:\n            word2count[word] = 1\n        else:\n            word2count[word] += 1\n            \ndel(word, line)\n\n\n# Remove less frequent Words by threshold frequency\nthresh = 5\nvocab = {}\nword_num = 0\nfor word, count in word2count.items():\n    if count >= thresh:\n        vocab[word] = word_num\n        word_num += 1\n        \n## delete\ndel(word2count, word, count, thresh,word_num)       \n   \n","4dd13481":"for i in range(len(clean_ans)):\n    clean_ans[i] = '<SOS> ' + clean_ans[i] + ' <EOS>'\n    \ntokens = ['<PAD>', '<EOS>', '<OUT>', '<SOS>']\nx = len(vocab)\n\nfor token in tokens:\n    vocab[token] = x\n    x += 1\n    \nvocab['cameron'] = vocab['<PAD>']\nvocab['<PAD>'] = 0\n\ndel(x,token, tokens) \n\n# Inverse Answers Dictionary \ninv_vocab = {w:v for v, w in vocab.items()}\ndel(i)","e83534c6":"encoder_inp = []\nfor line in clean_ques:\n    lst = []\n    for word in line.split():\n        if word not in vocab:\n            lst.append(vocab['<OUT>'])\n        else:\n            lst.append(vocab[word])\n        \n    encoder_inp.append(lst)\n    \ndecoder_inp = []\nfor line in clean_ans:\n    lst = []\n    for word in line.split():\n        if word not in vocab:\n            lst.append(vocab['<OUT>'])\n        else:\n            lst.append(vocab[word])        \n    decoder_inp.append(lst)\n\ndel(clean_ans, clean_ques, line, lst, word)\n\n# Padding the inputs for LSTM Model\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\nencoder_inp = pad_sequences(encoder_inp, 13, padding='post', truncating='post')\ndecoder_inp = pad_sequences(decoder_inp, 13, padding='post', truncating='post')\ndecoder_final_output = []\n\nfor i in decoder_inp:\n    decoder_final_output.append(i[1:]) \ndecoder_final_output = pad_sequences(decoder_final_output, 13, padding='post', truncating='post')\ndel(i)\n\n# Label Encoding\ndecoder_final_output = to_categorical(decoder_final_output, len(vocab))\nprint(decoder_final_output.shape)\n","e67c2542":"from tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Dense, Embedding, LSTM, Input\n\nenc_inp = Input(shape=(13, ))\ndec_inp = Input(shape=(13, ))\n\nVOCAB_SIZE = len(vocab)\nembed = Embedding(VOCAB_SIZE+1, output_dim=50, \n                  input_length=13,\n                  trainable=True                  \n                  )\n\nenc_embed = embed(enc_inp)\nenc_lstm = LSTM(400, return_sequences=True, return_state=True)\nenc_op, h, c = enc_lstm(enc_embed)\nenc_states = [h, c]\n\ndec_embed = embed(dec_inp)\ndec_lstm = LSTM(400, return_sequences=True, return_state=True)\ndec_op, _, _ = dec_lstm(dec_embed, initial_state=enc_states)\n\ndense = Dense(VOCAB_SIZE, activation='softmax')\n\ndense_op = dense(dec_op)\n\nmodel = Model([enc_inp, dec_inp], dense_op)\n\nmodel.compile(loss='categorical_crossentropy',metrics=['acc'],optimizer='adam')\n\nmodel.fit([encoder_inp, decoder_inp],decoder_final_output,epochs=500)\n","02354d1d":"from tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input\n\nenc_model = Model([enc_inp], enc_states)\n\n# decoder Model\ndecoder_state_input_h = Input(shape=(400,))\ndecoder_state_input_c = Input(shape=(400,))\n\ndecoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n\ndecoder_outputs, state_h, state_c = dec_lstm(dec_embed , \n                                    initial_state=decoder_states_inputs)\n\ndecoder_states = [state_h, state_c]\n\ndec_model = Model([dec_inp]+ decoder_states_inputs,[decoder_outputs]+ decoder_states)\n\ndec_model.compile(loss='categorical_crossentropy',metrics=['acc'],optimizer='adam')","d8bb1fd6":"#import numpy as np\n\n#from keras.preprocessing.sequence import pad_sequences\n#print(\"##########################################\")\n#print(\"#       start chatting ver. 1.0          #\")\n#print(\"##########################################\")\n\n\n#prepro1 = \"\"\n#while prepro1 != 'q':\n #   prepro1  = input(\"you : \")\n    \n  #  prepro1 = clean_text(prepro1)\n   # prepro = [prepro1]\n\n    #txt = []\n    #for x in prepro:\n        \n        #lst = []\n        #for y in x.split():\n           \n         #   try:\n          #      lst.append(vocab[y])\n            \n           # except:\n            #    lst.append(vocab['<OUT>'])\n        #txt.append(lst)\n\n    #txt = pad_sequences(txt, 13, padding='post')\n\n    #stat = enc_model.predict( txt )\n\n    #empty_target_seq = np.zeros( ( 1 , 1) )\n\n    #empty_target_seq[0, 0] = vocab['<SOS>']\n\n    #stop_condition = False\n    #decoded_translation = ''\n\n    #while not stop_condition :\n\n        #dec_outputs , h, c= dec_model.predict([ empty_target_seq] + stat )\n        #decoder_concat_input = dense(dec_outputs)\n\n        #sampled_word_index = np.argmax( decoder_concat_input[0, -1, :] )\n      \n        #sampled_word = inv_vocab[sampled_word_index] + ' '\n\n        #if sampled_word != '<EOS> ':\n         #   decoded_translation += sampled_word  \n\n        #if sampled_word == '<EOS> ' or len(decoded_translation.split()) > 13:\n        #    stop_condition = True \n\n        #empty_target_seq = np.zeros( ( 1 , 1 ) )  \n        #empty_target_seq[ 0 , 0 ] = sampled_word_index\n        #stat = [h, c]  \n\n   # print(\"chatbot attention : \", decoded_translation )\n   # print(\"==============================================\")  ","95c20355":"# Loading The Dataset","7de986aa":"# Adding SOS and EOS","11d38e5b":"Here we are using Inference model as frontend to take input questions from the user .\nFor the sake of better documentation we are commenting out the below cell to avoid any errors while saving the notebook","473a4368":"# Cleaning of Dataset","911c3bd0":"# Creating Decoding Model Using LSTM","a4dc3acd":"# Creating Vocabulary","adb2db1e":"# Inference Model for Frontend","98a43aed":"# Creating List of Questions and Answers","eaaa548a":"# Creating Encoding  Model Using LSTM","e6a2fa08":"# Creating Encoder and Decoder Inputs","4c74acc6":"# Data Preprocessing"}}