{"cell_type":{"00b8e5e5":"code","96469908":"code","fee6b257":"code","caf6f915":"code","9b71b657":"code","8282c55d":"code","dd272402":"code","33cd555b":"code","a590940d":"code","e4f60e5c":"code","df11be45":"code","ef219893":"code","29a0d2ac":"code","e93c1d6e":"code","622fd48d":"code","f93b2a95":"code","54efef22":"code","84cdf1c2":"code","05d0e50b":"code","e7e7d877":"code","1de52f5b":"code","26fa29aa":"code","52f33a87":"code","fbe1b5ed":"code","2245b01b":"code","26d098fb":"code","f06d8a99":"code","e1148936":"code","8653f026":"code","ffeb3f52":"code","7ef2a735":"code","4b56d474":"code","7d52d1df":"code","2fb89e3c":"code","50fe2eab":"code","74ba9061":"code","1b4d1e7a":"code","44a3b3cc":"code","e16d3920":"code","94eed5af":"code","46e62b09":"markdown","e4bcaf06":"markdown","fc15c126":"markdown","82975799":"markdown","f25f9a60":"markdown","2337894f":"markdown","37ef42e9":"markdown","32cb58bd":"markdown","123bf40e":"markdown","5f1fc5e6":"markdown","fa74d79f":"markdown","5d8b4441":"markdown","e81208ab":"markdown","cf3a2919":"markdown","87e9a150":"markdown","27d0d101":"markdown","7fe31eac":"markdown","2acc1112":"markdown"},"source":{"00b8e5e5":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport scipy.stats as ss\nfrom scipy.stats import chi2_contingency\nfrom sklearn.preprocessing import LabelEncoder\nle=LabelEncoder()\nfrom sklearn.ensemble import RandomForestClassifier\n!pip install pycaret\nfrom pycaret.classification import *","96469908":"data=pd.read_csv(\"..\/input\/league-of-legends-diamond-ranked-games-10-min\/high_diamond_ranked_10min.csv\")\ndata.head()","fee6b257":"data.shape","caf6f915":"#Checking for missing values\ndata.isnull().sum()","9b71b657":"#Checking datatypes of individual feature\ndata.dtypes","8282c55d":"#Dropping 'gameId' feature as it's not required in model building and prediction\ndata.drop([\"gameId\"],1,inplace=True)","dd272402":"blue_features=[]\nred_features=[]\nfor col in list(data):\n    if(col[0]=='r'):\n        red_features.append(col)\n    if(col[0]=='b'):\n        blue_features.append(col)","33cd555b":"blues=data[blue_features]\nred_features.append(\"blueWins\")\nreds=data[red_features]","a590940d":"#Dividing features into numerical and categorical features\ncategorical_reds=[]\ncategorical_blues=[]\nnumerical_reds=[]\nnumerical_blues=[]\nfor col in list(reds):\n    if(len(reds[col].unique())<=30):\n        categorical_reds.append(col)\n    else:\n        numerical_reds.append(col)\n\nfor col in list(blues):\n    if(len(blues[col].unique())<=30):\n        categorical_blues.append(col)\n    else:\n        numerical_blues.append(col)","e4f60e5c":"print(\"Number of Categorical Features for Blue Team\",len(categorical_blues))\nprint(\"Number of Categorical Features for Red Team\",len(categorical_reds))\nprint(\"Number of Numerical Features for Blue Team\",len(numerical_blues))\nprint(\"Number of Numerical Features for Red Team\",len(numerical_reds))","df11be45":"def Chi_square(col_1,col_2):\n    X=reds[col_1].astype('str')\n    Y=reds[col_2].astype('str')\n    observed_values=pd.crosstab(Y,X)\n    chi2, p, dof, expected = ss.chi2_contingency(observed_values)\n    if(p>0.05):\n        print(col_1,\" is not required\")\n    else:\n        print(col_1,\" is required\")\n        \nfor col in categorical_reds:\n    Chi_square(col,\"blueWins\")","ef219893":"def Chi_square(col_1,col_2):\n    X=blues[col_1].astype('str')\n    Y=blues[col_2].astype('str')\n    observed_values=pd.crosstab(Y,X)\n    chi2, p, dof, expected = ss.chi2_contingency(observed_values)\n    if(p>0.05):\n        print(col_1,\" is not required\")\n    else:\n        print(col_1,\" is required\")\n        \nfor col in categorical_blues:\n    Chi_square(col,\"blueWins\")","29a0d2ac":"X=reds[numerical_reds]\ny=le.fit_transform(reds[\"blueWins\"])\n\nimport statsmodels.api as sm\ncols_red = list(X.columns)\npmax = 1\nwhile (pmax>0.05):\n    p=[]\n    X_1 = X[cols_red]\n    X_1 = sm.add_constant(X_1)\n    model = sm.OLS(y,X_1).fit()\n    p = pd.Series(model.pvalues.values[1:],index = cols_red)      \n    pmax = max(p)\n    feature_with_p_max = p.idxmax()\n    if(pmax>0.05):\n        cols_red.remove(feature_with_p_max)\n    else:\n        breakselected_features_BE = cols_red\nprint(\"Best features using Backward Elimination: \",cols_red)","e93c1d6e":"X=blues[numerical_blues]\ny=le.fit_transform(blues[\"blueWins\"])\n\nimport statsmodels.api as sm\ncols_blue = list(X.columns)\npmax = 1\nwhile (pmax>0.05):\n    p=[]\n    X_1 = X[cols_blue]\n    X_1 = sm.add_constant(X_1)\n    model = sm.OLS(y,X_1).fit()\n    p = pd.Series(model.pvalues.values[1:],index = cols_blue)      \n    pmax = max(p)\n    feature_with_p_max = p.idxmax()\n    if(pmax>0.05):\n        cols_blue.remove(feature_with_p_max)\n    else:\n        breakselected_features_BE = cols_blue\nprint(\"Best features using Backward Elimination: \",cols_blue)","622fd48d":"Xr_rfc=reds.drop([\"blueWins\"],1)\nyr_rfc=reds[\"blueWins\"]","f93b2a95":"rfc_r=RandomForestClassifier(random_state=0)\nrfc_r.fit(Xr_rfc,yr_rfc)","54efef22":"plt.figure(figsize=(10,10))\nplt.barh(list(Xr_rfc),rfc_r.feature_importances_)\nplt.title(\"Feature Imporatance using Random Forest Classifier\")\nplt.ylabel(\"Features\")\nplt.xlabel('Feature Importance Value')","84cdf1c2":"Xb_rfc=blues.drop([\"blueWins\"],1)\nyb_rfc=blues[\"blueWins\"]","05d0e50b":"rfc_b=RandomForestClassifier(random_state=0)\nrfc_b.fit(Xb_rfc,yb_rfc)","e7e7d877":"plt.figure(figsize=(10,10))\nplt.barh(list(Xb_rfc),rfc_b.feature_importances_)","1de52f5b":"models=setup(data=blues,\n             categorical_features=categorical_blues.remove('blueWins'),\n             ignore_features=list(set(numerical_blues)-set(cols_blue)),\n             target='blueWins',\n             silent=True,\n             session_id=269)","26fa29aa":"model_results=compare_models()\nmodel_results","52f33a87":"logreg_model=create_model('lr')","fbe1b5ed":"tunned_logreg_model=tune_model('lr')","2245b01b":"plot_model(estimator=tunned_logreg_model,plot='parameter')","26d098fb":"plot_model(estimator=tunned_logreg_model,plot='feature')","f06d8a99":"plot_model(estimator=tunned_logreg_model,plot='pr')","e1148936":"plot_model(estimator=tunned_logreg_model,plot='confusion_matrix')","8653f026":"plot_model(estimator=tunned_logreg_model,plot='class_report')","ffeb3f52":"plot_model(tunned_logreg_model)","7ef2a735":"model_red=setup(data=reds,\n               categorical_features=categorical_reds.remove('blueWins'),\n               ignore_features=list(set(numerical_reds)-set(cols_red)),\n               target='blueWins',\n               silent=True,\n               session_id=299)","4b56d474":"compare_models()","7d52d1df":"logreg_model=create_model('lr')","2fb89e3c":"tunned_lr_model=tune_model('lr')","50fe2eab":"plot_model(estimator=tunned_lr_model,plot='parameter')","74ba9061":"plot_model(estimator=tunned_lr_model,plot='feature')","1b4d1e7a":"plot_model(estimator=tunned_lr_model,plot='confusion_matrix')","44a3b3cc":"plot_model(estimator=tunned_lr_model,plot='pr')","e16d3920":"plot_model(estimator=tunned_lr_model,plot='class_report')","94eed5af":"plot_model(tunned_lr_model)","46e62b09":"#### Chi-squre test for Feature Importance considering Features of Red Team","e4bcaf06":"### Feature Selection using Backward Elimination for Numerical Features","fc15c126":"##### Random Forest Feature importance for Red Team Features","82975799":"#### Building model using only Red Team Features","f25f9a60":"### Chi-Square test for Feature Importance of Categorical Features","2337894f":"#### Building model using only Blue Team Features","37ef42e9":"### Model building using Pycaret Library","32cb58bd":"Using Backward Elimination method for numerical features of Blue Team","123bf40e":"### Feature Importance using Random Forest Classifier","5f1fc5e6":"#### Chi-squre test for Feature Importance considering Features of Blue Team","fa74d79f":"Using Backward Elimination method for numerical features of Red team","5d8b4441":"##### Random Forest Feature Importance considering only Blue Team features","e81208ab":"#### Importing the data","cf3a2919":"#### Importing Required Packages","87e9a150":"As majority of the features in the data are categorical, also the target feature is categorical we can use Chi-Square test for to get the feature importance.","27d0d101":"**As per the information available in the data there are two teams Red and Blue. Since both the teams are playing against each there there is negative correlation among the features realted to Blue team and features realted to Red team.**\n\nSo here we have two options to work around with:\n\n1) Predicting the \"blueWins\" based on the features of only Blue Team \n\n2) Predicting the \"blueWins\" based on the features of only Red team","7fe31eac":"ALL CATEGORICAL FEATURES ARE IMPORTANT CONSIDERING BOTH THE TEAMS","2acc1112":"Random Forest is considered to be one of the most unbiased model. As it creates multiple Decision Trees taking into account Random Features for each Decision Tree.\n\nBecause of this randomness the Random Forest Classifier considerd to be giving most unbiased Feature Importance"}}