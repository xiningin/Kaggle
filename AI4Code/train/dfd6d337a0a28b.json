{"cell_type":{"4a37d079":"code","c3846304":"code","c7325ddb":"code","f6186f89":"code","7f3b2f5d":"code","d94cfc97":"code","f8208a26":"code","4402b987":"code","c5ff869c":"code","9f713b1d":"code","82fc6302":"code","538b33dc":"code","e9facef3":"code","f9f7b97f":"code","6820188c":"code","e538495d":"code","9679e6d2":"code","170359ad":"code","8bb6a230":"code","88659769":"code","b38374e7":"code","a6e1cd0a":"code","19ec2000":"code","9b592115":"code","11c5c741":"code","2ba221ef":"code","77e5b501":"code","5801e485":"code","6e23c4d5":"code","f0419a86":"code","f2c70d68":"code","95a89ee9":"code","36eafd72":"code","b7a64b0d":"code","c5443fc4":"code","31555c08":"code","0bad6ff7":"code","42dfc2de":"code","3a8eb0e2":"code","e5264b34":"code","7161fe2f":"code","56f6c51a":"code","1410b6fd":"code","9f27564e":"code","b4393024":"code","be758cd6":"code","3b847ccf":"code","29192e7a":"code","cc4b016f":"code","0dc226b7":"code","1644327b":"code","c27fb880":"code","02a3df73":"code","89f81fae":"code","46d69eaf":"code","02588122":"code","88c30ed1":"code","bf6f479e":"code","20ffcebb":"code","3f4a53c8":"code","6f86c510":"code","086e586b":"code","290dcf0d":"markdown","ff6b2084":"markdown","e75cb192":"markdown","d409cbe5":"markdown","0b6633a1":"markdown","0654daf0":"markdown","441fd09d":"markdown","24bae65d":"markdown","91ad1f72":"markdown","18475ab2":"markdown","9e033a80":"markdown","1e690569":"markdown","7f26787e":"markdown","b45bc22b":"markdown","5e663e60":"markdown","acdc0c59":"markdown","b5c9967f":"markdown","b3d1fd41":"markdown","75445fdc":"markdown","21278ed3":"markdown","4016a059":"markdown","ea409d00":"markdown"},"source":{"4a37d079":"# importing necessary libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns","c3846304":"# reading the data\ndata = pd.read_csv('..\/input\/pima-indians-diabetes-database\/diabetes.csv')\ndata.head()","c7325ddb":"# checking for missing values\ndata.isnull().sum()","f6186f89":"# checking the data types\ndata.dtypes","7f3b2f5d":"# descriptive statistics\ndata.describe()","d94cfc97":"# Checking number of missing values for these columns\ncolumns = ['Glucose','BloodPressure','SkinThickness','Insulin','BMI']\nfor i in columns:\n    print(\"missing values in\",i,\" :\",(data[i] == 0).sum())","f8208a26":"# Checking outliers and distribution of these columns for imputation\nfor i in columns:\n    fig,axes = plt.subplots(1,2,figsize=(10,5))\n    sns.boxplot(x=data[i],orient = 'v',ax = axes[0])\n    sns.distplot(data[i],ax = axes[1])\n    fig.tight_layout()\n    ","4402b987":"# imputing missing values i.e. '0'\ndata['Glucose'].replace(0,data['Glucose'].mean(),inplace = True)\ndata['BloodPressure'].replace(0,data['BloodPressure'].mean(),inplace = True)\ndata['BMI'].replace(0,data['BMI'].mean(),inplace = True)\ndata['SkinThickness'].replace(0,data['SkinThickness'].median(),inplace = True)\ndata['Insulin'].replace(0,data['Insulin'].median(),inplace = True)","c5ff869c":"data.describe()","9f713b1d":"fig, ax = plt.subplots(figsize=(15,10))\nsns.boxplot(data=data, width= 0.5,ax=ax,  fliersize=3)","82fc6302":"for i in data.columns:\n    plt.figure(figsize = (5,5))\n    sns.distplot(data[i])","538b33dc":"# removing outliers\ndata = data[data['SkinThickness']<80]\ndata = data[data['Insulin']<580]\ndata = data[data['BMI']<60]\ndata.shape","e9facef3":"# Looking at the distribution again \nfor i in data.columns:\n    plt.figure(figsize = (5,5))\n    sns.distplot(data[i])","f9f7b97f":"# separating dependent and independent features\nX = data.drop(\"Outcome\",axis=1)\ny = data['Outcome']","6820188c":"# heatmap for checking correlation\ncorr_matrix = data.corr()\nsns.heatmap(corr_matrix)","e538495d":"# scaling the features\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_scaled = sc.fit_transform(X)\nX_scaled = pd.DataFrame(X_scaled,columns = ['Pregnancies','Glucose','BloodPressure','SkinThickness','Insulin','BMI','DiabetesPedigreeFunction','Age'])","9679e6d2":"# splitting training and test data (80:20) ratio\nfrom sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test = train_test_split(X_scaled,y,test_size = 0.20,random_state = 30)","170359ad":"from sklearn.svm import SVC\nclf_svm1 = SVC()\nclf_svm1.fit(X_train,y_train)\ny_pred = clf_svm1.predict(X_test)","8bb6a230":"# accuracy of SVC\nfrom sklearn.metrics import confusion_matrix,accuracy_score,precision_score,recall_score,f1_score\naccuracy = accuracy_score(y_test,y_pred)\nprint(\"SVC Accuracy:\",accuracy)","88659769":"# plotting the confusion matrix \nfrom sklearn.metrics import plot_confusion_matrix\nplot_confusion_matrix(clf_svm1,X_test,y_test,values_format = 'd',display_labels = ['diabetic','not diabetic'])","b38374e7":"# selecting different parameters to use for improving the SVC Accuracy\nparam_grid = [\n    {'C' : [0.5,1,10,100],\n     'gamma' : ['scale','auto',1,0.1,0.01,0.001,0.0001],\n    'kernel' : ['linear','poly','rbf']}\n]","a6e1cd0a":"# Hyperparameter optimisation\nfrom sklearn.model_selection import GridSearchCV\n\noptimal_params = GridSearchCV(SVC(),param_grid,cv = 5,scoring = 'accuracy',verbose = 0)\noptimal_params.fit(X_train,y_train)","19ec2000":"print(optimal_params.best_params_)","9b592115":"clf_svm2 = SVC(C = 100, gamma = 0.0001,probability = True)\nclf_svm2.fit(X_train,y_train)\nsvm_y_pred = clf_svm2.predict(X_test)","11c5c741":"from sklearn.metrics import confusion_matrix,accuracy_score\naccuracy = accuracy_score(y_test,svm_y_pred)\nprint(\"SVC Accuracy score:\",accuracy)","2ba221ef":"from sklearn.metrics import recall_score,precision_score,f1_score\nrecall = recall_score(y_test,svm_y_pred)\nprecision = precision_score(y_test,svm_y_pred)\nf1 = f1_score(y_test,svm_y_pred)\nprint(\"SVC Recall:\",recall)\nprint(\"SVC Precision:\",precision)\nprint(\"SVC F1:\",f1)","77e5b501":"# printing the classification report\nfrom sklearn.metrics import classification_report\nprint(classification_report(y_test,svm_y_pred))","5801e485":"# plotting confusion matrix again\nplot_confusion_matrix(clf_svm2,X_test,y_test,values_format = 'd',display_labels = ['diabetic','not diabetic'])","6e23c4d5":"from sklearn.linear_model import LogisticRegression\nlr_clf1 = LogisticRegression()\nlr_clf1.fit(X_train,y_train)\nlr_y_pred = lr_clf1.predict(X_test)","f0419a86":"accuracy_lr = accuracy_score(y_test,lr_y_pred)\nprint(\"Logistic Regression Accuracy:\",accuracy_lr)","f2c70d68":"# Plotting confusion matrix\nplot_confusion_matrix(lr_clf1,X_test,y_test,values_format = 'd',display_labels = ['diabetic','not diabetic'])","95a89ee9":"# # Hyperparameter optimisation\nfrom sklearn.model_selection import GridSearchCV\ngrid_values = {'penalty': ['l2'], 'C': [0.001,0.01,0.1,1,10,100,1000]}\nlr_optimal_params = GridSearchCV(LogisticRegression(),grid_values,cv =5,verbose = 0)\nlr_optimal_params.fit(X_train,y_train)\n","36eafd72":"print(lr_optimal_params.best_params_)","b7a64b0d":"lr_clf2 = LogisticRegression(C = 1, penalty = 'l2')\nlr_clf2.fit(X_train,y_train)\nlr_y_pred = lr_clf2.predict(X_test)\naccuracy = accuracy_score(y_test,lr_y_pred)\nprint(\"Accuracy score:\",accuracy)","c5443fc4":"plot_confusion_matrix(lr_clf2,X_test,y_test,values_format = 'd',display_labels = ['diabetic','not diabetic'])","31555c08":"from sklearn.metrics import recall_score,precision_score,f1_score\nrecall = recall_score(y_test,lr_y_pred)\nprecision = precision_score(y_test,lr_y_pred)\nf1 = f1_score(y_test,lr_y_pred)\nprint(\"LR Recall:\",recall)\nprint(\"LR Precision:\",precision)\nprint(\"LR F1:\",f1)","0bad6ff7":"# printing the classification report\nfrom sklearn.metrics import classification_report\nprint(classification_report(y_test,lr_y_pred))","42dfc2de":"# applying random forest\nfrom sklearn.ensemble import RandomForestClassifier\nrf_clf = RandomForestClassifier(random_state = 32)\nmodel = rf_clf.fit(X_train,y_train)\nrf_y_pred = model.predict(X_test)\naccuracy = accuracy_score(y_test,rf_y_pred)\nprint(\"Accuracy score:\",accuracy)","3a8eb0e2":"params = [{\n    'n_estimators' : [10,20,50,100,200,300,400],\n    'criterion' : ['gini','entropy'],\n    'max_leaf_nodes' : range(8,32)\n}]\noptimal_params = GridSearchCV(RandomForestClassifier(random_state = 32),params,cv =5,verbose = 0)\noptimal_params.fit(X_train,y_train)","e5264b34":"print(optimal_params.best_params_)","7161fe2f":"rf_clf = RandomForestClassifier(n_estimators=10,max_leaf_nodes = 29,criterion = 'entropy',random_state = 32 )\nmodel = rf_clf.fit(X_train,y_train)\nrf_y_pred = model.predict(X_test)\naccuracy = accuracy_score(y_test,rf_y_pred)\nprint(\"Accuracy score:\",accuracy)","56f6c51a":"plot_confusion_matrix(rf_clf,X_test,y_test,values_format = 'd',display_labels = ['diabetic','not diabetic'])","1410b6fd":"from sklearn.metrics import recall_score,precision_score,f1_score\nrecall = recall_score(y_test,rf_y_pred)\nprecision = precision_score(y_test,rf_y_pred)\nf1 = f1_score(y_test,rf_y_pred)\nprint(\"RF Recall:\",recall)\nprint(\"RF Precision:\",precision)\nprint(\"RF F1:\",f1)","9f27564e":"# printing the classification report\nfrom sklearn.metrics import classification_report\nprint(classification_report(y_test,rf_y_pred))","b4393024":"from sklearn.ensemble import GradientBoostingClassifier\ngb_clf = GradientBoostingClassifier()\ngb_clf.fit(X_train,y_train)\ngb_y_pred = gb_clf.predict(X_test)\naccuracy = accuracy_score(y_test,gb_y_pred)\nprint(\"Accuracy score:\",accuracy)","be758cd6":"gb_params = [{\n    'learning_rate' : [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0],\n    'criterion' : ['friedman_mse', 'mse', 'mae'],\n    'min_samples_leaf' : range(1,6)\n}]\ngb_optimal_params = GridSearchCV(GradientBoostingClassifier(),gb_params,cv =5,verbose = 0)\ngb_optimal_params.fit(X_train,y_train)\n","3b847ccf":"print(gb_optimal_params.best_params_)","29192e7a":"gb_clf = GradientBoostingClassifier(criterion = 'mse',learning_rate = 0.1,min_samples_leaf = 4 )\ngb_clf.fit(X_train,y_train)\ngb_y_pred = gb_clf.predict(X_test)\naccuracy = accuracy_score(y_test,gb_y_pred)\nprint(\"Accuracy score:\",accuracy)","cc4b016f":"plot_confusion_matrix(gb_clf,X_test,y_test,values_format = 'd',display_labels = ['diabetic','not diabetic'])","0dc226b7":"from sklearn.metrics import recall_score,precision_score,f1_score\nrecall = recall_score(y_test,gb_y_pred)\nprecision = precision_score(y_test,gb_y_pred)\nf1 = f1_score(y_test,gb_y_pred)\nprint(\"GB Recall:\",recall)\nprint(\"GB Precision:\",precision)\nprint(\"GB F1:\",f1)","1644327b":"# printing the classification report\nfrom sklearn.metrics import classification_report\nprint(classification_report(y_test,gb_y_pred))","c27fb880":"# Applying Knn and finding the best value of 'k'\nfrom sklearn.neighbors import KNeighborsClassifier\n\ntest_scores = []\ntrain_scores = []\naccuracy_max = 0\nk_max = 0\nfor i in range(1,30):\n    knn = KNeighborsClassifier(i)\n    knn.fit(X_train,y_train)\n    y_pred = knn.predict(X_test)\n    \n    train_accuracy = knn.score(X_train,y_train)\n    train_scores.append(train_accuracy)\n    test_accuracy = accuracy_score(y_test,y_pred)\n    test_scores.append(test_accuracy)\n    if test_accuracy > accuracy_max:\n        accuracy_max = test_accuracy\n        k_max = i\nprint(\"maximum_test_accuracy:\",accuracy_max, \"is achieved at k:\",k_max)","02a3df73":"knn1 = KNeighborsClassifier(7)\nknn1.fit(X_train,y_train)\nknn_y_pred = knn1.predict(X_test)","89f81fae":"plot_confusion_matrix(knn1,X_test,y_test,values_format = 'd',display_labels = ['diabetic','not diabetic'])","46d69eaf":"# Hyperparameter optimisation\nfrom sklearn.model_selection import GridSearchCV\nparam_grid = {'n_neighbors':np.arange(1,50)}\nknn2 = KNeighborsClassifier()\nknn_cv= GridSearchCV(knn2,param_grid,cv =5,verbose = 0)\nknn_cv.fit(X_train,y_train)\n\nprint(\"Best Parameters: \" + str(knn_cv.best_params_))","02588122":"knn2 = KNeighborsClassifier(n_neighbors = 17)\nknn2.fit(X_train,y_train)\nknn_y_pred = knn2.predict(X_test)\nacc = accuracy_score(y_test,knn_y_pred)\nacc","88c30ed1":"from sklearn.metrics import recall_score,precision_score,f1_score\nrecall = recall_score(y_test,knn_y_pred)\nprecision = precision_score(y_test,knn_y_pred)\nf1 = f1_score(y_test,knn_y_pred)\nprint(\"KNN Recall:\",recall)\nprint(\"KNN Precision:\",precision)\nprint(\"KNN F1:\",f1)","bf6f479e":"# plotting training and test scores of KNN model\nplt.figure(figsize=(12,5))\np = sns.lineplot(range(1,30),train_scores,marker='*',label='Train Score')\np = sns.lineplot(range(1,30),test_scores,marker='o',label='Test Score')","20ffcebb":"svc_y_pred_proba = clf_svm2.predict_proba(X_test)[:,1]\nlr_y_pred_proba = lr_clf2.predict_proba(X_test)[:,1]\nrf_y_pred_proba = rf_clf.predict_proba(X_test)[:,1]\ngb_y_pred_proba = gb_clf.predict_proba(X_test)[:,1]\nknn_y_pred_proba = knn1.predict_proba(X_test)[:,1]","3f4a53c8":"from sklearn.metrics import roc_curve\nfpr_svc, tpr_svc, thresholds_svc = roc_curve(y_test, svc_y_pred_proba)\nfpr_lr, tpr_lr, thresholds_lr = roc_curve(y_test, lr_y_pred_proba)\nfpr_rf, tpr_rf, thresholds_rf = roc_curve(y_test, rf_y_pred_proba)\nfpr_gb, tpr_gb, thresholds_gb = roc_curve(y_test, gb_y_pred_proba)\nfpr_knn, tpr_knn, thresholds_knn = roc_curve(y_test, knn_y_pred_proba)","6f86c510":"# Plotting the ROC Curve\nplt.plot([0,1],[0,1],'k--')\nplt.plot(fpr_svc,tpr_svc, label='SVC')\nplt.plot(fpr_knn,tpr_knn, label='KNN')\nplt.plot(fpr_rf,tpr_rf, label='RF')\nplt.plot(fpr_gb,tpr_gb, label='GB')\nplt.plot(fpr_lr,tpr_lr, label='LR')\nplt.xlabel('FPR')\nplt.ylabel('TPR')\nplt.legend()\nplt.title('ROC curve')\nplt.show()","086e586b":"#Area under ROC curve\nfrom sklearn.metrics import roc_auc_score\nknn_auc = roc_auc_score(y_test,knn_y_pred_proba)\nlr_auc = roc_auc_score(y_test,lr_y_pred_proba)\nsvc_auc = roc_auc_score(y_test,svc_y_pred_proba)\nrf_auc = roc_auc_score(y_test,rf_y_pred_proba)\ngb_auc = roc_auc_score(y_test,gb_y_pred_proba)\nprint(\"Area under KNN ROC curve:\",knn_auc)\nprint(\"Area under LR ROC curve:\",lr_auc)\nprint(\"Area under SVC ROC curve:\",svc_auc)\nprint(\"Area under RF ROC curve:\",rf_auc)\nprint(\"Area under GB ROC curve:\",gb_auc)","290dcf0d":"# Random Forest Classifier","ff6b2084":"We can see that maximum value of k = 17 where the test score is maximum.","e75cb192":" We can observe that every model has almost same accuracy score.\n But Recall score is different for each model.\n Recall is the ratio of correctly predicted positive values out of all the actual positive values.\n Here, recall signifies the correctly identified diabetic patients out of all the diabetic patients. This value should be maximum.\n Random Forest has the maximum recall value.","d409cbe5":"Area under Logistic Regression,Support Vector Classifier and Random Forest is almost same and more than KNN and Gradient Boost.","0b6633a1":"# Logistic Regression","0654daf0":"# EDA","441fd09d":"We have successfully imputed missing value.","24bae65d":"Looking at the outliers in some specific columns and their distribution, outliers are removed","91ad1f72":"# Gradient Boosting","18475ab2":"# KNN Classifier","9e033a80":"# Support Vector Classifier","1e690569":"# Standard Scaling","7f26787e":"Accuracy remained same","b45bc22b":"Now let's check the outliers in our data and how to handle them.","5e663e60":"There's not much correlation in the data. Hence, no problem of multicollinearity","acdc0c59":"# End of the Notebook","b5c9967f":"Now, data distribution looks better than before.","b3d1fd41":"Accuracy is improved ","75445fdc":"As we can see, minimum value for \"Glucose\", \"BloodPressure\", \"SkinThickness\", \"Insulin\", \"BMI\" is 0, but this can not be possible. Probably, 0 is for missing values.","21278ed3":"If you like this Kernel, please upvote. I am new to kaggle and this is my first upload. \nAlso, suggestions for improvements and mistakes are welcome","4016a059":"We can plot ROC curve to identify the best model.","ea409d00":"Looking at the outliers and distribution, we can impute the missing values in \"Glucose\", \"BloodPressure\" and \"BMI\" with \"mean\" value and \"SkinThickness\" and \"Insulin\" with \"median\" value."}}