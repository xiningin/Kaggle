{"cell_type":{"651e43b2":"code","a6a57e84":"code","39969672":"code","7d98540b":"code","3a22637d":"code","63900877":"code","9dffd35b":"code","62836db8":"code","ff929cff":"code","af50ebb4":"code","821ec681":"code","b5aee03f":"code","57e89729":"code","a12f65c8":"code","8af786f0":"code","e775fc72":"code","c4102281":"code","411901f7":"code","61dd4388":"code","52a3627b":"code","3d47009f":"code","f19a7e5e":"code","a938f10f":"code","69c36218":"code","ff4f9423":"code","140d99b1":"code","070a7874":"code","fcec82cb":"code","9b1572bb":"code","094be0c9":"code","30b0d01f":"code","90287bc0":"code","0ce2dbc6":"code","6f35a8c3":"code","b5b376d0":"code","dc4c9814":"code","da7dd0dd":"code","ea2b7425":"code","607be315":"code","0525885e":"markdown","22562748":"markdown","4b4c4f2c":"markdown","a89fbe15":"markdown","34704c43":"markdown","98c4bdf5":"markdown","29099b2a":"markdown","983408ba":"markdown","a837343a":"markdown","7a2b1c1c":"markdown","78af28d5":"markdown","aa1dbf8a":"markdown","1d268d0b":"markdown","33d44496":"markdown","b0165e7a":"markdown","8a314945":"markdown","475c21cf":"markdown","d060a4b1":"markdown","2bf1b045":"markdown","fe6d4960":"markdown","9a707094":"markdown","3ee77c7d":"markdown","b906fc6b":"markdown","8da32f19":"markdown","4019ee8e":"markdown","c05609b1":"markdown","05b30537":"markdown","8943c845":"markdown","6ffc6d00":"markdown","30cd2684":"markdown","1f81be60":"markdown","3fdc55c0":"markdown","e7ca9ff3":"markdown","75f52d24":"markdown","504b696a":"markdown","483b8502":"markdown","6c430635":"markdown","c2721a13":"markdown","97a13639":"markdown","39795728":"markdown","18a0d302":"markdown","b45d64cb":"markdown","2f7573f5":"markdown","5aa66198":"markdown","2708211e":"markdown","54b35aab":"markdown","2afdd803":"markdown","8cb2a6db":"markdown","94ae9c76":"markdown","06830cc7":"markdown","32187c17":"markdown","337208cf":"markdown","4b55dc1b":"markdown","e161d150":"markdown","42085bd0":"markdown","51d60d80":"markdown","851aece9":"markdown","ea5c5604":"markdown","d62abc06":"markdown","6965076a":"markdown","b6e990ba":"markdown","f79ab662":"markdown"},"source":{"651e43b2":"import numpy as np # Linear algebra.\nimport pandas as pd # Data processing, CSV file I\/O (e.g. pd.read_csv).\nimport datatable as dt # Data processing, CSV file I\/O (e.g. dt.fread).\n\nimport seaborn as sns # Visualization.\nimport matplotlib.pyplot as plt # Visualization.\n\n# Machine Learning block.\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom xgboost import XGBClassifier\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import KFold, StratifiedKFold\nfrom sklearn.preprocessing import LabelEncoder\n\nimport random\nfrom tqdm import tqdm_notebook as tqdm\nfrom collections import defaultdict\nfrom copy import deepcopy\nimport warnings\nwarnings.filterwarnings('ignore')\n\nprint(f'\\n[INFO] Libraries set up has been completed.')","a6a57e84":"%%time\ndf_train = dt.fread('..\/input\/tabular-playground-series-dec-2021\/train.csv').to_pandas()\ndf_test = dt.fread('..\/input\/tabular-playground-series-dec-2021\/test.csv').to_pandas()\ndf_sub = pd.read_csv('..\/input\/tabular-playground-series-dec-2021\/sample_submission.csv')\n\n# Datatable reads target as bool by default.\nmask_bool = df_train.dtypes == bool\nbool_train = df_train.dtypes[mask_bool].index\nbool_test = df_test.dtypes[mask_bool].index\n\ndf_train[bool_train] = df_train[bool_train].astype('int8')\ndf_test[bool_train] = df_test[bool_train].astype('int8')\n\ndf_train.info(verbose=False)","39969672":"def reduce_mem_usage(df, verbose=True):\n    start_mem = df.memory_usage().sum()\/1024**2\n    numerics = ['int8', 'int16', 'int32', 'int64',\n                'float16', 'float32', 'float64']\n\n    for col in df.columns:\n        col_type = df[col].dtypes\n        limit = abs(df[col]).max()\n\n        for tp in numerics:\n            cond1 = str(col_type)[0] == tp[0]\n            if tp[0] == 'i': cond2 = limit <= np.iinfo(tp).max\n            else: cond2 = limit <= np.finfo(tp).max\n\n            if cond1 and cond2:\n                df[col] = df[col].astype(tp)\n                break\n\n    end_mem = df.memory_usage().sum()\/1024**2\n    reduction = (start_mem - end_mem)*100\/start_mem\n    if verbose:\n        print(f'[INFO] Mem. usage decreased to {end_mem:.2f}'\n              f' MB {reduction:.2f}% reduction.')\n    return df\n\ndf_train.drop(columns='Id', inplace=True)\ndf_test.drop(columns='Id', inplace=True)\n\ndf_train = reduce_mem_usage(df_train, verbose=True)\ndf_test = reduce_mem_usage(df_test, verbose=True)\ndf_train.head(5)","7d98540b":"df_target_count = df_train['Cover_Type'].value_counts()\ns1 = df_target_count[:3]\ns2 = pd.Series(sum(df_target_count[3:]), index=[\"4-5-6-7\"])\ns3 = s1.append(s2)\n\nf, axes = plt.subplots(ncols=2, figsize=(15, 4))\nplt.subplots_adjust(wspace=0)\n\nouter_sizes = s3\ninner_sizes = s3\/4\nouter_colors = ['#1B518A', '#3470A3', '#79ABC9', '#E54232']\ninner_colors = ['#3470A3', '#79ABC9', '#A7C8D9']\n\naxes[0].pie(\n    outer_sizes,colors=outer_colors, \n    labels=s3.index.tolist(), \n    startangle=90,frame=True, radius=1.3, \n    explode=(.05,.05,.05,.5),\n    wedgeprops={ 'linewidth' : 1, 'edgecolor' : 'white'}, \n    textprops={'fontsize': 12, 'weight': 'bold'}\n)\n\ntextprops = {\n    'size':13, \n    'weight': 'bold', \n    'color':'white'\n}\n\naxes[0].pie(\n    inner_sizes, colors=inner_colors,\n    radius=1, startangle=90,\n    autopct='%1.f%%',explode=(.1,.1,.1, -.5),\n    pctdistance=0.8, textprops=textprops\n)\n\ncenter_circle = plt.Circle((0,0), .68, color='black', \n                           fc='white', linewidth=0)\naxes[0].add_artist(center_circle)\n\nx = df_target_count\ny = df_target_count.index.astype(str)\nsns.barplot(\n    x=x, y=y, ax=axes[1],\n    palette='Blues_r', orient='horizontal'\n)\n\naxes[1].spines['top'].set_visible(False)\naxes[1].spines['right'].set_visible(False)\naxes[1].tick_params(\n    axis='x',         \n    which='both',      \n    bottom=False,      \n    labelbottom=False\n)\n\nfor i, v in enumerate(df_target_count):\n    axes[1].text(v, i+0.1, str(v), color='black', \n                 fontweight='bold', fontsize=12)\n \nplt.tight_layout()    \nplt.show()","3a22637d":"seed = 322\ndf_train_sample = df_train.sample(n=30000, random_state=seed)\ndf_test_sample = df_test.sample(n=30000, random_state=seed)\n\nnp.random.seed(seed) \nfeatures_choice = np.random.choice(\n    df_train_sample.keys()[1:-1], size=12, replace=False\n)\n\nmask = sorted(features_choice.tolist()) + ['Cover_Type']\ndf_sample_twelve = df_train_sample[mask]\ndf_sample_twelve.head(3)","63900877":"fig, ax = plt.subplots(nrows=12, figsize=(24, 24))\n\nfor i, feature in enumerate(sorted(features_choice)):\n     sns.scatterplot(\n         ax=ax[i], x=df_sample_twelve.index,\n         y=feature,data=df_sample_twelve,\n         hue='Cover_Type',palette='Blues_r',\n         legend=True,\n     )","9dffd35b":"missing_values_train = df_train.isna().any().sum()\nmissing_values_test = df_test.isna().any().sum()\n\nprint(f'\\n[INFO] {missing_values_train} missing value(s) has\/have been detected in the train dataset.')\nprint(f'[INFO] {missing_values_test} missing value(s) has\/have been detected in the test dataset.')","62836db8":"pd.set_option('display.float_format', lambda x: '%.2f' % x)\ndf_train.iloc[:, :-1].describe().T.sort_values(by='std', ascending=False)\\\n                     .style.background_gradient(cmap='Blues')\\\n                     .bar(subset=[\"mean\",], color='green')\\\n                     .bar(subset=[\"max\"], color='#BB0000')","ff929cff":"pd.set_option('display.float_format', lambda x: '%.2f' % x)\ndf_test.iloc[:, :-1].describe().T.sort_values(by='std', ascending=False)\\\n                     .style.background_gradient(cmap='Blues')\\\n                     .bar(subset=[\"mean\",], color='green')\\\n                     .bar(subset=[\"max\"], color='#BB0000')","af50ebb4":"df_train.drop(columns=['Soil_Type7', 'Soil_Type15'], inplace=True)\ndf_test.drop(columns=['Soil_Type7', 'Soil_Type15'], inplace=True)","821ec681":"%%time\nfigsize = (6*6, 6*6)\nfig = plt.figure(figsize=figsize)\ntitle = 'Probability Density Function Estimation'\nfor idx, col in enumerate(df_test.columns):\n    ax = plt.subplot(8, 7, idx + 1)\n    sns.kdeplot(\n        data=df_train_sample, hue='Cover_Type', fill=True,\n        x=col, palette='cividis'\n    )\n            \n    ax.set_ylabel(''); ax.spines['top'].set_visible(False), \n    ax.set_xlabel(''); ax.spines['right'].set_visible(False)\n    ax.set_title(f'{col}', loc='right', \n                 weight='bold', fontsize=10)\n\nfig.supxlabel(f'\\n\\n{title} Train\\n\\n', ha='center', \n              fontweight='bold', fontsize=30)\nplt.tight_layout()\nplt.show()\n\nfig = plt.figure(figsize=figsize)\nfor idx, col in enumerate(df_test.columns):\n    ax = plt.subplot(8, 7, idx + 1)\n    sns.kdeplot(\n    data=df_train_sample, fill=True,\n    x=col, color='#1086CE', label='Train'\n    )\n    sns.kdeplot(\n        data=df_test_sample, fill=False,\n        x=col, color='#E54232', label='Test'\n    )\n\n    ax.set_xticks([]); ax.set_xlabel(''); \n    ax.set_ylabel(''); ax.spines['right'].set_visible(False)\n    ax.set_yticks([]); ax.spines['top'].set_visible(False)\n    ax.set_title(f'{col}', loc='right', \n                 weight='bold', fontsize=10)\n    \nfig.supxlabel(f'\\n\\n{title} Train vs Test set', ha='center', \n              fontweight='bold', fontsize=30)\n       \nplt.tight_layout()\nplt.show()","b5aee03f":"fig = plt.figure(figsize=(15,10))\n_skew = df_train[df_test.columns].skew()\nsns.barplot(x=_skew, y=_skew.index, \n            lw=0, palette='Blues_r')\nplt.show()","57e89729":"cont = sum(df_train[df_test.columns].nunique() > 2)\ncat = sum(df_train[df_test.columns].nunique() == 2)\ndf_cat = pd.DataFrame({\n    'feature': ['continuous', 'categorical'],\n    'total_num': [cont, cat]\n})\n\nf, axes = plt.subplots(ncols=2, figsize=(15, 4))\nplt.subplots_adjust(wspace=0)\n\nouter_sizes = df_cat.total_num\ninner_sizes = outer_sizes\/4\nouter_colors = ['#1B518A', '#3470A3']\ninner_colors = ['#3470A3', '#79ABC9']\n\naxes[0].pie(\n    outer_sizes,colors=outer_colors, \n    labels=df_cat.feature.tolist(), \n    startangle=90,frame=True, radius=1.3, \n    explode=(.05,.05),\n    wedgeprops={ 'linewidth' : 1, 'edgecolor' : 'white'}, \n    textprops={'fontsize': 12, 'weight': 'bold'}\n)\n\ntextprops = {\n    'size':13, \n    'weight': 'bold', \n    'color':'white'\n}\n\naxes[0].pie(\n    inner_sizes, colors=inner_colors,\n    radius=1, startangle=90,\n    autopct='%1.f%%',explode=(.1,.1),\n    pctdistance=0.8, textprops=textprops\n)\n\ncenter_circle = plt.Circle((0,0), .68, color='black', \n                           fc='white', linewidth=0)\naxes[0].add_artist(center_circle)\n\nx = df_cat.total_num\ny = df_cat.feature.tolist()\nsns.barplot(\n    x=x, y=y, ax=axes[1],\n    palette='Blues_r', orient='horizontal'\n)\n\naxes[1].spines['top'].set_visible(False)\naxes[1].spines['right'].set_visible(False)\naxes[1].tick_params(\n    axis='x',         \n    which='both',      \n    bottom=False,      \n    labelbottom=False\n)\n\nfor i, v in enumerate(x):\n    axes[1].text(v, i+0.1, str(v), color='black', \n                 fontweight='bold', fontsize=12)\n \nplt.tight_layout()    \nplt.show()","a12f65c8":"corr = df_train_sample.corr()\n\nfig, axes = plt.subplots(figsize=(20, 10))\nmask = np.zeros_like(corr)\nmask[np.triu_indices_from(mask)] = True\nsns.heatmap(corr, mask=mask, linewidths=.5, cmap='ocean')\n\nplt.show()","8af786f0":"agg_features = ['sum','mean','std','max','min','kurt']\nfeatures = df_train.keys().tolist()[:-1]\n\nfor ft in agg_features:\n    \n    class_method = getattr(pd.DataFrame, ft)\n    df_train_sample[ft] = class_method(df_train_sample[features], axis=1)\n    df_test_sample[ft] = class_method(df_test_sample[features], axis=1)\n\ndf_train_sample.head(3)","e775fc72":"def feature_engineering(train, test):\n    \"\"\"\n    Applies clipping and reformats train and test sets.\n    Adds Manhhattan and Euclidean to Hydrology.\n    :param train: pd.DataFrame\n    :param test: pd.DataFrame\n    :return: pd.DataFrame, pd.DataFrame\n    \"\"\"\n\n    # Manhattan distance to Hydrology\n    train[\"mht_dist_hydrlgy\"] = np.abs(train.iloc[:, 3]) + np.abs(train.iloc[:, 4])\n    test[\"mht_dist_hydrlgy\"] = np.abs(test.iloc[:, 3]) + np.abs(test.iloc[:, 4])\n\n    # Euclidean distance to Hydrology\n    train[\"eucl_dist_hydrlgy\"] = np.sqrt(train.iloc[:, 3]**2 + train.iloc[:, 4]**2)\n    test[\"eucl_dist_hydrlgy\"] = np.sqrt(test.iloc[:, 3]**2 + test.iloc[:, 4]**2)\n\n    # Clips hillshades 0 to 255 index.\n    hillshades = [col for col in train.columns if col.startswith('Hill')]\n    train[hillshades] = train[hillshades].clip(0, 255)\n    test[hillshades] = test[hillshades].clip(0, 255)\n\n    # Clips 'Aspect' 0 to 360 degrees.\n    col = 'Aspect'\n    train.loc[train[col] < 0, col] += 360\n    train.loc[train[col] > 359, col] -= 360\n    test.loc[test[col] < 0, col] += 360\n    test.loc[test[col] > 359, col] -= 360\n\n    return train, test\n","c4102281":"def print_shapes(X_train, y_train, X_valid, y_valid):\n  \"\"\"\n  Prints shapes of train\/valid splits.\n  :param: X_train (numpy.ndarray)\n  :param: y_train (numpy.ndarray)\n  :param: X_valid(numpy.ndarray)\n  :param: y_valid (numpy.ndarray)\n  :return: None\n  \"\"\"\n\n  print(f'\\n[INFO] shape of X_train: {X_train.shape}.')\n  print(f'[INFO] shape of y_train: {y_train.shape}.')\n  print(f'[INFO] shape of X_valid: {X_valid.shape}.')\n  print(f'[INFO] shape of y_valid: {y_valid.shape}.\\n')\n\n\ndef accuracy(y_true, y_pred):\n    \"\"\"\n    Prints shapes of train\/valid splits.\n    :param: y_true (numpy.ndarray)\n    :param: y_pred (numpy.ndarray)\n    :return: float\n    \"\"\"\n    \n    num_correct = sum(y_pred == y_true)\n    acc_score = num_correct \/ y_true.shape[0]\n    \n    return acc_score\n\n\ndef train_model(x, y, clf, clf_name, xgb=False):\n    \"\"\"\n    Trains model by using selected classifier.\n    :param: x (scaled numpy.ndarray)\n    :param: y (scaled numpy.ndarray)\n    :param: clf (model classifier)\n    :param: clf_name (str)\n    :return: clf, float\n    \"\"\"\n    \n    best_clf_acc = 0\n\n    for fold, (idx_train, idx_valid) in enumerate(kf.split(X, Y)):\n\n            X_train, y_train = X[idx_train, :], Y[idx_train]\n            X_valid, y_valid = X[idx_valid, :], Y[idx_valid]\n\n            if fold == 0:\n                print_shapes(X_train, y_train, X_valid, y_valid)\n            \n            if xgb:\n                clf.fit(\n                    X_train, y_train,\n                    eval_set=[(X_valid, y_valid)],\n                    early_stopping_rounds=200,\n                    verbose=0\n                )\n            else:\n                clf.fit(X_train, y_train)\n\n            y_pred = clf.predict(X_valid)\n            clf_acc = accuracy(y_true=y_valid, y_pred=y_pred)\n            print(f'[INFO] Fold: {fold+1}. {clf_name} '\n                  f'Acc. score: {clf_acc:.6f}.')\n\n            if clf_acc > best_clf_acc:\n                best_clf = deepcopy(clf)\n                best_clf_acc = clf_acc\n    \n    return best_clf, best_clf_acc","411901f7":"seed = 1\nepochs = 10\npatience = 5\nbatch_size = 16384\nlr = 0.02\nn_folds = 5\nkf = KFold(\n    n_splits=n_folds, \n    shuffle=True, \n    random_state=seed\n)","61dd4388":"# We are going to remove \"Cover_Type\" 5, since there is only one example.\ndf_train = df_train[df_train.Cover_Type != 5].reset_index()\nprint(f'[INFO] df_train shape: {df_train.shape}.')","52a3627b":"scaler = StandardScaler()\nfeatures = df_test.keys()\nX = scaler.fit_transform(df_train[features])\nY = df_train['Cover_Type'].astype(dtype=int) # TabNet can save model only with int64.\nX_test = scaler.fit_transform(df_test)","3d47009f":"X = X[:10000, :]\nY = Y[:10000]\nX_test = X_test[:10000, :]","f19a7e5e":"for k in [2, 3, 5, 7, 20]:\n    knn_clf = KNeighborsClassifier(n_neighbors=k)\n    clf_name = 'KNN'\n    best_knn_clf, best_knn_acc = train_model(x=X, clf_name=clf_name, \n                                             y=Y, clf=knn_clf)\n    print(f'[INFO] K_num: {k}. Acc_valid: {best_knn_acc:.6f}.')","a938f10f":"dt_clf = DecisionTreeClassifier()\nclf_name = 'Decision Trees'\nbest_dt_clf, best_dt_acc = train_model(x=X, clf_name=clf_name, \n                                       y=Y, clf=dt_clf)","69c36218":"rf_clf = RandomForestClassifier()\nclf_name = 'Random Forest'\nbest_rf_clf, best_rf_acc = train_model(x=X, clf_name=clf_name, \n                                       y=Y, clf=rf_clf)","ff4f9423":"gnb_clf = GaussianNB()\nclf_name = 'Gaussian Naive Bayes'\nbest_gnb_clf, best_gnb_acc = train_model(x=X, clf_name=clf_name, \n                                         y=Y, clf=gnb_clf)","140d99b1":"xgb_clf = XGBClassifier(\n    max_depth=8, \n    n_estimators=10000, \n    learning_rate=0.01, \n    objective='multi:softmax',\n    eval_metric='mlogloss',\n    tree_method='gpu_hist',\n    random_state=seed,\n)\n\nclf_name = 'XGB'\nbest_xgb_clf, best_xgb_acc = train_model(x=X, clf_name=clf_name, \n                                         y=Y, clf=xgb_clf)","070a7874":"!pip install pytorch_tabnet -q\nfrom pytorch_tabnet.multitask import TabNetMultiTaskClassifier\nprint(f'\\n[INFO] TabNet set up has been completed.')","fcec82cb":"#######################################################################\n# This part is commented out for the sake of PyTorch model submission.#\n#######################################################################\n# X = scaler.fit_transform(df_train[features])\n# Y = df_train['Cover_Type'].astype(dtype=int) # TabNet can save model only with int64.\n# X_test = scaler.fit_transform(df_test)\n\nbest_tabnet_acc = 0\n\nclf = TabNetMultiTaskClassifier(n_d=64, n_a=64, n_steps=5,\n                                gamma=1.5, n_independent=2, n_shared=2,\n                                lambda_sparse=1e-4, momentum=0.3, clip_value=2.,\n                                optimizer_fn=torch.optim.Adam,\n                                optimizer_params=dict(lr=lr),\n                                scheduler_params = {\"gamma\": 0.95,\n                                \"step_size\": 20},\n                                scheduler_fn=torch.optim.lr_scheduler.StepLR, epsilon=1e-15)\n\n\nfor fold, (idx_train, idx_valid) in enumerate(kf.split(X, Y)):\n\n        X_train, y_train = X[idx_train, :], Y[idx_train].values.reshape(-1,1)\n        X_valid, y_valid = X[idx_valid, :], Y[idx_valid].values.reshape(-1,1)\n\n        if fold == 0:\n            print_shapes(X_train, y_train, X_valid, y_valid)\n            \n        print(f'[INFO] Fold: {fold+1}.')\n         \n        clf.fit(\n            X_train, y_train,\n            eval_set=[(X_valid, y_valid)],\n            eval_metric=['accuracy'],\n            max_epochs=epochs, patience=patience,\n            batch_size=batch_size, virtual_batch_size=256\n        )\n        \n        y_pred = clf.predict(X_valid)\n        tabnet_acc = accuracy(y_true=y_valid[:,0 ], \n                              y_pred=y_pred[0].astype(int))\n        \n        if tabnet_acc > best_tabnet_acc:\n            best_tabnet_clf = deepcopy(clf)\n            best_tabnet_acc = tabnet_acc\n        \ny_pred = best_tabnet_clf.predict(X_valid)\nbest_tabnet_acc = accuracy(y_true=y_valid[:,0 ], y_pred=y_pred[0].astype(int))\nprint(f\"[INFO] FINAL TEST SCORE FOR : {best_tabnet_acc}.\")       ","9b1572bb":"train_csv_path = '..\/input\/tabular-playground-series-dec-2021\/train.csv'\ntest_csv_path = '..\/input\/tabular-playground-series-dec-2021\/test.csv'\n\ntrain = dt.fread(train_csv_path).to_pandas()\ntest = dt.fread(test_csv_path).to_pandas()\n\n# Removes class 5, resets indices and\n# rearranges classes (e.g. [1,2,3,4,6,7] -> [0,1,2,3,4,5];\n# creates mapper to store these rearrangements.\ntrain = train[train.Cover_Type != 5].reset_index(drop=True)\nle = LabelEncoder()\ntarget = le.fit_transform(train.Cover_Type)\n\ncol_drop=['Id', 'Soil_Type7', 'Soil_Type15']    \nif col_drop:\n    train.drop(columns=col_drop, inplace=True)\n    test.drop(columns=col_drop, inplace=True)\n    print(f'[INFO] \"Id\" columns have been removed successfully.')\n\n# Datatable reads [0, 1] as bool by default.\n# Converts bool to int32.\nmask_bool = train.dtypes == bool\nbool_cols = train.dtypes[mask_bool].index\n\ntrain[bool_cols] = train[bool_cols].astype('int32')\ntest[bool_cols] = test[bool_cols].astype('int32')\n\n# Applies feature engineering\ntrain = train.iloc[:, :-1]\ntrain, test = feature_engineering(train, test)\ntrain['Cover_Type'] = target\n\ntrain.head()","094be0c9":"class TabularDataset(Dataset):\n    def __init__(self, x, y):\n        \"\"\"\n        Defines PyTorch dataset.\n        :param x: np.ndarray\n        :param y: np.ndarray\n        \"\"\"\n\n        self.len = x.shape[0]\n        self.x = torch.Tensor(x).float()\n        self.y = torch.LongTensor(y).long().flatten()\n\n    def __getitem__(self, idx):\n        return self.x[idx], self.y[idx]\n\n    def __len__(self):\n        return self.len","30b0d01f":"class Model(nn.Module):\n    def __init__(self, in_features, num_cls):\n        super().__init__()\n\n        self.fc1 = nn.Linear(in_features, 128)\n        self.bn1 = nn.BatchNorm1d(128)\n        self.fc2 = nn.Linear(128, 64)\n        self.bn2 = nn.BatchNorm1d(64)\n        self.fc3 = nn.Linear(64, 32)\n        self.bn3 = nn.BatchNorm1d(32)\n        self.fc4 = nn.Linear(32, 16)\n        self.fc_out = nn.Linear(16, num_cls)\n\n        self.activation = nn.SELU()\n        self.classifier = nn.Sigmoid()\n        self.dropout = nn.Dropout(p=0.2)\n\n    def forward(self, x):\n\n        x = self.activation(self.fc1(x))\n        x = self.bn1(x)\n        x = self.activation(self.fc2(x))\n        x = self.bn2(x)\n        x = self.activation(self.fc3(x))\n        x = self.bn3(x)\n        x = self.activation(self.fc4(x))\n        x = self.fc_out(x)\n\n        return x\n\n\ndef init_weights(layer):\n    if isinstance(layer, nn.Linear):\n        nn.init.xavier_normal_(layer.weight.data)","90287bc0":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"[INFO] Available device: {device}.\\n\\n\")\nmodel = Model(in_features=52, num_cls=6).to(torch.device(device))\n\ntry:\n    from torchsummary import summary\nexcept:\n    print(\"Installing Torchsummary..........\")\n    ! pip install torchsummary -q\n    from torchsummary import summary\n    \nsummary(model, (52,))","0ce2dbc6":"def accuracy(output, target, topk=(1,)):\n    \"\"\"\n    Computes the precision@k for the specified values of k\n    \"\"\"\n    with torch.no_grad():\n        maxk = max(topk)\n        batch_size = target.size(0)\n\n        _, pred = output.topk(maxk, 1, True, True)\n        pred = pred.t()\n        correct = pred.eq(target.view(1, -1).expand_as(pred))\n\n        res = []\n        for k in topk:\n            correct_k = correct[:k].view(-1).float().sum(0, keepdim=True)\n            res.append(correct_k.mul_(100.0 \/ batch_size))\n        return res\n\n\nclass MetricMonitor:\n    def __init__(self, float_precision=3):\n        self.float_precision = float_precision\n        self.reset()\n\n    def reset(self):\n        self.metrics = defaultdict(lambda: {\"val\": 0, \"count\": 0, \"avg\": 0})\n\n    def update(self, metric_name, val):\n        metric = self.metrics[metric_name]\n\n        metric[\"val\"] += val\n        metric[\"count\"] += 1\n        metric[\"avg\"] = metric[\"val\"] \/ metric[\"count\"]\n\n    def __str__(self):\n        return \" | \".join(\n            [\n                \"{metric_name}: {avg:.{float_precision}f}\".format(\n                    metric_name=metric_name, avg=metric[\"avg\"], float_precision=self.float_precision\n                )\n                for (metric_name, metric) in self.metrics.items()\n            ]\n        )\n\n\ndef set_seed(seed):\n    \"\"\"\n    Fixes seed for the reproducible results.\n    \"\"\"\n\n    random.seed(seed)\n    np.random.seed(seed)\n\n    torch.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False","6f35a8c3":"def train_loop(train_loader, model, criterion, optimizer, epoch, device):\n    metric_monitor = MetricMonitor()\n    model.train()\n    stream = tqdm(train_loader)\n    for i, (x, y) in enumerate(stream, start=1):\n        features = x.to(device)\n        target = y.to(device)\n        output = model(features)\n        loss = criterion(output, target)\n        acc_train = accuracy(output, target)\n        metric_monitor.update(\"Loss\", loss.item())\n        metric_monitor.update(\"Accuracy\", acc_train[0].item())\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        desc = \"Epoch: {epoch}. Train.      {metric_monitor}\"\n        stream.set_description(\n          desc.format(epoch=epoch, metric_monitor=metric_monitor)\n        )\n    \n    loss_avg = metric_monitor.metrics[\"Loss\"]['avg']\n    acc_avg = metric_monitor.metrics[\"Accuracy\"]['avg']\n\n    return loss_avg, acc_avg\n\n\ndef val_loop(val_loader, model, criterion, epoch, device):\n    metric_monitor = MetricMonitor()\n    model.eval()\n    stream = tqdm(val_loader)\n    with torch.no_grad():\n        for i, (x, y) in enumerate(stream, start=1):\n            features = x.to(device)\n            target = y.to(device)\n            output = model(features)\n            loss = criterion(output, target)\n            acc_val = accuracy(output, target)\n\n            metric_monitor.update(\"Loss\", loss.item())\n            metric_monitor.update(\"Accuracy\", acc_val[0].item())\n            desc = \"Epoch: {epoch}. Validation.      {metric_monitor}\"\n            stream.set_description(\n                desc.format(epoch=epoch, metric_monitor=metric_monitor)\n            )\n            \n    loss_avg = metric_monitor.metrics[\"Loss\"]['avg']\n    acc_avg = metric_monitor.metrics[\"Accuracy\"]['avg']\n\n    return loss_avg, acc_avg","b5b376d0":"param = {\n        'seed': 1,\n        'nfold': 10,\n        'lr': 9e-5,\n        'wd': 1e-5,\n        'plateau_factor': .5,\n        'plateau_patience': 4,\n        'batch': 1024,\n        'epochs': 40,\n        'early_stopping': 9\n    }","dc4c9814":"X = train.iloc[:, :-1].values\ny = train.iloc[:, -1:].values\n\n# StratifiedKfold data split.\nskf = StratifiedKFold(\n    n_splits=param['nfold'],\n    shuffle=True,\n    random_state=param['seed']\n)\n    \n\nfor fold, (idx_train, idx_val) in enumerate(skf.split(X, y)):\n    \n    # Model, weights and seed init.\n    model = Model(in_features=54, num_cls=6)\n    model.apply(init_weights)\n    model = model.to(torch.device(device))\n    set_seed(param['seed'])\n\n    # Loss and optimizer.\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(\n        model.parameters(),\n        lr=param['lr'],\n        weight_decay=param['wd']\n    )\n\n    scheduler = ReduceLROnPlateau(\n                optimizer=optimizer,\n                factor=param['plateau_factor'],\n                patience=param['plateau_patience'],\n                mode='max', verbose=True\n            )\n    \n   \n    scaler = StandardScaler()\n    X_train, y_train = scaler.fit_transform(X[idx_train, :]), y[idx_train]\n    X_val, y_val = scaler.transform(X[idx_val, :]), y[idx_val]\n    print(\n        f'\\n[INFO] Fold: {fold+1}, '\n        f'X_train shape: {X_train.shape}, '\n        f'X_val shape: {X_val.shape}.\\n'\n    )\n\n    trainset = TabularDataset(X_train, y_train)\n    valset = TabularDataset(X_val, y_val)\n    train_loader = DataLoader(trainset, batch_size=param['batch'], shuffle=True)\n    val_loader = DataLoader(valset, batch_size=param['batch'], shuffle=True)\n    wait_counter = 0\n    valid_acc_best = 0\n    \n    for epoch in range(1, param['epochs'] + 1):\n        train_loss, train_acc = train_loop(train_loader, model, criterion, optimizer, epoch, device)\n        valid_loss, valid_acc = val_loop(val_loader, model, criterion, epoch, device)\n        \n        if valid_acc > valid_acc_best:\n            valid_acc_best = valid_acc\n            wait_counter = 0\n            best_model = deepcopy(model)\n            print(f'\\n[INFO] The best model has been saved.\\n')\n        else:\n            wait_counter += 1\n            if wait_counter > param['early_stopping']:\n                print(f\"\\n[INFO] There's been no improvement \"\n                      f\"in val_acc. Early stopping has been invoked.\")\n                break","da7dd0dd":"X_test = scaler.transform(test)\ntestset = TabularDataset(X_test, np.ones((X_test.shape[0], 1)))\ntest_loader = DataLoader(testset, batch_size=1024)\ny_pred_list = []\n\nbest_model.eval()\nwith torch.no_grad():\n    for X_batch, _ in tqdm(test_loader):\n        X_batch = X_batch.to(device)\n        y_test_pred = model(X_batch.float())\n        _, y_pred_tags = torch.max(y_test_pred, dim=1)\n        y_pred_list.extend(y_pred_tags.cpu().numpy())\n\n\n# Rearranges classes back (e.g. [0,1,2,3,4,5] -> [1,2,3,4,6,7];\n# Creates mapped test_y preds list.\ntest_y = le.inverse_transform(y_pred_list)\n            \ndf_sub['Cover_Type'] = test_y\ndf_sub.to_csv('submission.csv', index=False)\ndf_sub            ","ea2b7425":"results = pd.DataFrame({\n    'model': [\n        'KNN', 'Decision Trees', \n        'Random Forest','Gaussian Naive Bayes', \n        'XGB', 'TabNet'\n    ],\n    'auc_score': [\n        best_knn_acc, best_dt_acc, best_rf_acc, \n        best_gnb_acc, best_xgb_acc, best_tabnet_acc\n    ]\n})\n\ncell_hover = {\n    'selector': 'td:hover',\n    'props': [('background-color', '#ffffb3')]\n}\nindex_names = {\n    'selector': '.index_name',\n    'props': 'font-style: italic; color: white; font-weight:normal;'\n}\nheaders = {\n    'selector': 'th:not(.index_name)',\n    'props': 'background-color: #1086CE; color: white;'\n}\n\ns = results.sort_values(by='auc_score', ascending=False)\ns = s.style.format({'auc_score': '{:.4f}'}).hide_index()\ns.set_table_styles([cell_hover, index_names, headers])\n\ns.set_table_styles([\n    {'selector': 'th.col_heading', 'props': 'text-align: left;'},\n    {'selector': 'td', 'props': 'text-align: left;'},\n], overwrite=False)\n\ns","607be315":"#######################################################################\n# This part is commented out for the sake of PyTorch model submission.#\n#######################################################################\n# X_test = scaler.fit_transform(df_test)\n# y_pred = clf.predict(X_test)\n# y_pred = y_pred[0].astype(int)  \n\n# df_sub['Cover_Type'] = y_pred\n# df_sub.to_csv('submission.csv', index=False)\n# df_sub","0525885e":"## <p style=\"background-color:#1086CE; font-family:newtimeroman; color:white; font-size:120%; text-align:center;border-radius: 10px;\">Table of Contents<\/p>\n* [1. Import of Libraries](#1)\n* [2. Data Visualization](#2)\n* [3. Data Preprocessing](#3)\n    * [3.1 Handle Missing Values](#3.1)\n    * [3.2 Smoothing data \/ Reformating \/ Dropping](#3.2)\n* [4. Exploratory Data Analysis](#4)\n* [5. Feature engineering](#5)\n* [6. Modeling](#6)\n    * [6.1 k-Nearest Neighbors](#6.1)\n    * [6.2 Decision Trees](#6.2)\n    * [6.3 Random Forest](#6.3)\n    * [6.4 Gaussian Naive Bayes](#6.4)\n    * [6.5 XGB](#6.5)\n    * [6.6 TabNet](#6.6)\n* [7. PyTorch NN Model](#7)   \n* [8. Conclusions](#8)\n* [9. References](#9)","22562748":"> It reads the data 3x time faster than pandas. Moreover, we've been able to reduce the memory usage from 1.7 GB to 351 MB by simply downcasting some columns to int8.\n>\n> We do not really need **\"id\"** column. It will help us to reduce memory usage even more.\n> Let's fix it and cast our dtypes to the smaller ones (references: [**link**](https:\/\/www.kaggle.com\/c\/tabular-playground-series-dec-2021\/discussion\/294356)):\n","4b4c4f2c":"<a id='2'><\/a>\n# <p style=\"background-color:#1086CE; font-family:newtimeroman; color:white; font-size:120%; text-align:center;border-radius: 10px;\">2. Data visualization<\/p>\n\nSpecial thanks and reference: [SIDDHESH PUJARI](https:\/\/www.kaggle.com\/siddheshpujari) and his [notebook](https:\/\/www.kaggle.com\/siddheshpujari\/forest-cover-eda#Dataset-Info).\n>The dataset includes four wilderness areas located in the Roosevelt National Forest in northern Colorado.\n>The Roosevelt National Forest is a National Forest, located in north central Colorado.\n>Each observation is a 30m x 30m patch.\n>\n>* **Elevation** - Elevation in meters.\n>* **Aspect** - Aspect in degrees azimuth.\n>To study how aspect works , please refer the following website [link](https:\/\/pro.arcgis.com\/en\/pro-app\/tool-reference\/3d-analyst\/how-aspect-works.htm) that explains how it works.\n>\n>* **Slope** - Slope in degrees. To study how the slope works, please refer the following website [link](https:\/\/pro.arcgis.com\/en\/pro-app\/tool-reference\/3d-analyst\/slope.htm).\n>* **Horizontal_Distance_To_Hydrology** - Horz Dist to nearest surface water features.\n>* **Vertical_Distance_To_Hydrology** - Vert Dist to nearest surface water features.\n>* **Horizontal_Distance_To_Roadways** - Horz Dist to nearest roadway.\n>* **Hillshade_9am (0 to 255 index)** - Hillshade index at 9am, summer solstice.\n>To study how aspect works , please refer the following website [link](https:\/\/pro.arcgis.com\/en\/pro-app\/tool-reference\/3d-analyst\/hillshade.htm)\n>\n>* **Hillshade_Noon (0 to 255 index)** - Hillshade index at noon, summer solstice.\n>* **Hillshade_3pm (0 to 255 index)** - Hillshade index at 3pm, summer solstice.\n>* **Horizontal_Distance_To_Fire_Points** - Horz Dist to nearest wildfire ignition points.\n>* **Wilderness_Area** (4 binary columns, 0 = absence or 1 = presence) - Wilderness area designation.\n>* **Soil_Type** (40 binary columns, 0 = absence or 1 = presence) - Soil Type designation.\n> Cover_Type (7 types, integers 1 to 7) - Forest Cover Type designation.\n>\n> **Seven Types of Forest Cover**:\n>* 1 - Spruce\/Fir.\n>* 2 - Lodgepole Pine.\n>* 3 - Ponderosa Pine.\n>* 4 - Cottonwood\/Willow.\n>* 5 - Aspen.\n>* 6 - Douglas-fir.\n>* 7 - Krummholz.\n>\n> **Let's read the data first** (I strongly recommend using 'datatable' to for faster data reading):","a89fbe15":"<a id='7.4'><\/a>\n## <p style=\"background-color:#1086CE; font-family:newtimeroman; color:white; font-size:120%; text-align:left;border-radius: 10px;\">___Device and model summary<\/p>","34704c43":"<a id='6.2'><\/a>\n## <p style=\"background-color:#1086CE; font-family:newtimeroman; color:white; font-size:120%; text-align:center;border-radius: 10px;\">6.2 Decision Trees<\/p>","98c4bdf5":"<a id='6.6'><\/a>\n## <p style=\"background-color:#1086CE; font-family:newtimeroman; color:white; font-size:120%; text-align:center;border-radius: 10px;\">6.6 TabNet<\/p>\n\n> This is a pyTorch implementation of Tabnet [[2]](#9.2) by [**Dreamquark-ai**](https:\/\/github.com\/dreamquark-ai\/tabnet). \n>\n> TabNet uses a sequential attention mechanism to choose a subset of semantically meaningful features to process at each decision step.\n> According to the paper this model outperforms XGBoost:\n\n\n                   \n><style type=\"text\/css\">\n.tg  {border-collapse:collapse;border-color:#93a1a1;border-spacing:0;}\n.tg td{background-color:#fdf6e3;border-color:#93a1a1;border-style:solid;border-width:1px;color:#002b36;\n  font-family:Arial, sans-serif;font-size:14px;overflow:hidden;padding:5px 15px;word-break:normal;}\n.tg th{background-color:#657b83;border-color:#93a1a1;border-style:solid;border-width:1px;color:#fdf6e3;\n  font-family:Arial, sans-serif;font-size:14px;font-weight:normal;overflow:hidden;padding:5px 15px;word-break:normal;}\n.tg .tg-6f7u{background-color:#ffffff;border-color:#002b36;font-style:italic;font-weight:bold;text-align:center;vertical-align:top}\n.tg .tg-604u{background-color:#1086CE;border-color:#002b36;text-align:center;vertical-align:top}\n.tg .tg-580s{background-color:#ffffff;border-color:#002b36;font-size:14px;text-align:center;vertical-align:top}\n.tg .tg-vgqm{background-color:#1086CE;border-color:#002b36;font-size:14px;text-align:center;vertical-align:top}\n.tg .tg-xd2w{background-color:#ffffff;border-color:#002b36;text-align:center;vertical-align:top}\n<\/style>\n<table align=\"center\"class=\"tg\">\n<thead>\n  <tr>\n    <th class=\"tg-vgqm\"><span style=\"font-weight:bold;font-style:italic;color:#000\">Model<\/span><\/th>\n    <th class=\"tg-604u\"><span style=\"font-weight:bold;font-style:italic;color:#000\">Test MSE<\/span><\/th>\n  <\/tr>\n<\/thead>\n<tbody>\n  <tr>\n    <td class=\"tg-580s\">MLP<\/td>\n    <td class=\"tg-xd2w\">512.62<\/td>\n  <\/tr>\n  <tr>\n    <td class=\"tg-xd2w\">XGBoost<\/td>\n    <td class=\"tg-xd2w\">490.83<\/td>\n  <\/tr>\n  <tr>\n    <td class=\"tg-xd2w\">LightGBM<\/td>\n    <td class=\"tg-xd2w\">504.76<\/td>\n  <\/tr>\n  <tr>\n    <td class=\"tg-xd2w\">CatBoost<\/td>\n    <td class=\"tg-xd2w\">489.74<\/td>\n  <\/tr>\n  <tr>\n    <td class=\"tg-xd2w\"><span style=\"font-weight:bold;font-style:italic\">TabNet<\/span><\/td>\n    <td class=\"tg-6f7u\"><span style=\"font-weight:bold;font-style:italic\">485.12<\/span><\/td>\n  <\/tr>\n    <caption>Performance for Rossmann Store Sales dataset<\/caption>\n<\/tbody>\n<\/table>\n\n> The library provides very convinient way to fit and predict. Let's build our model:","29099b2a":"### ***Model***","983408ba":"<a id='9'><\/a>\n# <p style=\"background-color:#1086CE; font-family:newtimeroman; color:white; font-size:120%; text-align:center;border-radius: 10px;\">9. References<\/p>","a837343a":"<a id='5'><\/a>\n# <p style=\"background-color:#1086CE; font-family:newtimeroman; color:white; font-size:120%; text-align:center;border-radius: 10px;\">5. Feature engineering<\/p>\n\n> [**Feature engineering**](https:\/\/www.omnisci.com\/technical-glossary\/feature-engineering#:~:text=Feature%20engineering%20refers%20to%20the,machine%20learning%20or%20statistical%20modeling.) refers to the process of using domain knowledge to select and transform the most relevant variables from raw data.\n>\n> One of the naive approach to engineer features, is to aggregate them.  \n>\n> The problem with aggregation is that we might encounter **multicollinearity** (e.g., the high correlation of the explanatory variables). \"It should be noted that the presence of multicollinearity does not mean that the model is\nmisspecified. You only start to talk about it when you think that it is\naffecting the regression results seriously.\" [[1]](#9.1)\n>\n> Now let's create our aggregated features:\n","7a2b1c1c":"### ***Helper functions***","78af28d5":"<a id='6.1'><\/a>\n## <p style=\"background-color:#1086CE; font-family:newtimeroman; color:white; font-size:120%; text-align:center;border-radius: 10px;\">6.1 K-Nearest Neighbors<\/p>","aa1dbf8a":"> **Well**, now it is even more clear that we got numerical and categorical features.\n","1d268d0b":"<a id='6.4'><\/a>\n## <p style=\"background-color:#1086CE; font-family:newtimeroman; color:white; font-size:120%; text-align:center;border-radius: 10px;\">6.4 Gaussian Naive Bayes<\/p>","33d44496":"### ***Model***","b0165e7a":"<a id='6.5'><\/a>\n## <p style=\"background-color:#1086CE; font-family:newtimeroman; color:white; font-size:120%; text-align:center;border-radius: 10px;\">6.5 XGB<\/p>","8a314945":"#### Data Science approach:\nCredit to: [@Chryzal](https:\/\/www.kaggle.com\/chryzal) - [Notebook](https:\/\/www.kaggle.com\/chryzal\/features-engineering-for-you\/notebook) and [@gulshanmishra](https:\/\/www.kaggle.com\/gulshanmishra) [Notebook](https:\/\/www.kaggle.com\/gulshanmishra\/tps-dec-21-tensorflow-nn-feature-engineering)","475c21cf":"<a id='7.5'><\/a>\n## <p style=\"background-color:#1086CE; font-family:newtimeroman; color:white; font-size:120%; text-align:left;border-radius: 10px;\">___Utils<\/p>","d060a4b1":"<a href=\"#Title\" role=\"button\" aria-pressed=\"true\" >Back to the beginning \ud83d\udd19<\/a>","2bf1b045":"![image.png](attachment:8514398b-85d7-462c-8558-d439713ee2c5.png)","fe6d4960":"> **We have plotted Probability Density Function estimation for each feature. What does it tell us?**\n>* The features are distributed differently;\n>* The data is not perfectly symmetrical. The most of the features either right or left skewed.\n>* Only **Hillshade_3pm** is bell-shaped-like (e.g., Gaussian distribution);\n>* Let's plot the skewness:","9a707094":"<a id=''><\/a>\n# <p style=\"background-color:#1086CE; font-family:newtimeroman; color:white; font-size:120%; text-align:center;border-radius: 10px;\">Work in progress...<\/p>\n","3ee77c7d":"### ***Model***","b906fc6b":"> We have run six different models. Hyperparameters tuning and features engineering have not been used during training.\nNow it is the time to rank our models based on their accuracy score. ","8da32f19":"<a id='4'><\/a>\n# <p style=\"background-color:#1086CE; font-family:newtimeroman; color:white; font-size:120%; text-align:center;border-radius: 10px;\">4. Exploratory Data Analysis<\/p>\n\n> Let's take a closer look at the distribution of the features:","4019ee8e":"<a href=\"#Title\" role=\"button\" aria-pressed=\"true\" >Back to the beginning \ud83d\udd19<\/a>","c05609b1":"<a href=\"#Title\" role=\"button\" aria-pressed=\"true\" >Back to the beginning \ud83d\udd19<\/a>","05b30537":"<a id='1'><\/a>\n# <p style=\"background-color:#1086CE; font-family:newtimeroman; color:white; font-size:120%; text-align:center;border-radius: 10px;\">1. Import of Libraries<\/p>","8943c845":"<a id='3.2'><\/a>\n## <p style=\"background-color:#1086CE; font-family:newtimeroman; color:white; font-size:120%; text-align:center;border-radius: 10px;\">3.2 Smoothing data \/ Reformating \/ Dropping<\/p>\n\n> Special thanks to **EDUARDO GUTIERREZ** and his notebook for .describe method representation and styling. [**Eduardo's notebook**](https:\/\/www.kaggle.com\/eduardogutierrez\/tps-nov-21-exploratory-data-analysis). ","6ffc6d00":"<a id='7.7'><\/a>\n## <p style=\"background-color:#1086CE; font-family:newtimeroman; color:white; font-size:120%; text-align:left;border-radius: 10px;\">___CFG<\/p>","30cd2684":"<a id='7.3'><\/a>\n## <p style=\"background-color:#1086CE; font-family:newtimeroman; color:white; font-size:120%; text-align:left;border-radius: 10px;\">___Model and weights initialization<\/p>","1f81be60":"> Well **`df_train`** is **extremely class-imbalanced**. The classes 4, 5, 6 and 7 combined form only 1% of all observations.\n>\n> Assumption: If we are to choose folds for training, we are better going with Kfolds.\n>\n> **Next**, let's get 30000 samples and plot it:","3fdc55c0":"<a id='7'><\/a>\n# <p style=\"background-color:#1086CE; font-family:newtimeroman; color:white; font-size:120%; text-align:center;border-radius: 10px;\">7. PyTorch NN Model<\/p>\n\n> **Versions notes**:\n> * Version1: Model hidden layers + Batch_Norm [128, BN, 64, BN, 32, BN, 16]. - LB acc 0.943.\n> * Version2: Added lr and scheduler - LB acc: 0.95215.\n> * Version3: A bit of magic - LB: 0.95686 TOP 13.\n> * Version4: Feature engineering did not work.\n> * Version5: Focal loss did not work either.\n> * Version6: Try drop out and add regularization did not work. -> best LB acc: 0.95697.\n> * Version7: Embeddings did not work. -> best LB acc: 0.95707.\n> * Version8: Aggregated engineered features did not work.\n>\n> **Things to try**:\n> * Add lr scheduler.\u2714\n> * Add early stoppping.\u2714\n> * Add evaluation plots and confusion matrices.\n> * Remove batch norms.\u2714\n> * Try different number of folds.\u2714\n> * Increase number of epochs.\u2714\n> * Try different sizes of hidden layers.\u2714\n> * Try drop out and add regularization. \u2714 \n> * Add voting classifier.\u2714\n> * Features engineering.\u2714\n> * Embeddings.\u2714\n    ","e7ca9ff3":"<a id='9.1'><\/a>\n<p >[1] C. Dougherty. Introduction to Econometrics 5th edition, pages 171-174, 2016.<\/p>\n<a id='9.2'><\/a>\n<p >[2] Arik, S. O., & Pfister, T. (2019). TabNet: Attentive Interpretable Tabular Learning. arXiv preprint arXiv:1908.07442.<\/p> ","75f52d24":"> Alright, the most of the features are skewed right. \n>\n> Let's see what is the total representation of 'continuous' and 'categorical' features::","504b696a":"### ***Model***","483b8502":"<a id='6'><\/a>\n# <p style=\"background-color:#1086CE; font-family:newtimeroman; color:white; font-size:120%; text-align:center;border-radius: 10px;\">6. Modeling<\/p>\n\n> Now it is modeling time. There are many models to choose from. We are going to try following ones:\n>\n>* k-Nearest Neighbors\n>* Decision Trees\n>* Random Forest\n>* Gaussian Naive Bayes\n>* Stochastic Gradient Decent\n>* XGB","6c430635":"<a id='3'><\/a>\n# <p style=\"background-color:#1086CE; font-family:newtimeroman; color:white; font-size:120%; text-align:center;border-radius: 10px;\">3. Data Preprocessing<\/p>\n>**[Data preprocessing](https:\/\/en.wikipedia.org\/wiki\/Data_pre-processing)** can refer to manipulation or dropping of data before it is used in order to ensure or enhance performance, and is an important step in the data mining process. The phrase \"garbage in, garbage out\" is particularly applicable to data mining and machine learning projects.\n>\n>**Let's have a sanity check if we have any missing values:**","c2721a13":"#### **Naive approach**:","97a13639":"<a id='6.3'><\/a>\n## <p style=\"background-color:#1086CE; font-family:newtimeroman; color:white; font-size:120%; text-align:center;border-radius: 10px;\">6.3 Random Forest<\/p>","39795728":"# <p style=\"background-color:#1086CE; font-family:newtimeroman; color:white; font-size:180%; text-align:center; border-radius: 15px;\">A Complete Guide: EDA + PyTorch Model<\/p>\n<a id=\"Title\"><\/a>\n\n## PyTorch NN model is in the end of notebook.\n## If you want to skip main part, please, click to proceed: [7. PyTorch NN Model](#7)   \n\n\n>This notebook is a walk through guide for dealing with common data science competition.\n>* The **objective** of this notebook is to apply step-by-step approach to solve tabular data competition.\n>* The **subject** of this notebook is a multi-classification task, based on \"[the synthetic dataset generated using a CTGAN on a real dataset](https:\/\/www.kaggle.com\/c\/tabular-playground-series-dec-2021\/overview). The original dataset comes from [Forest Cover Type Prediction competition](https:\/\/www.kaggle.com\/c\/forest-cover-type-prediction\/overview). You are asked to predict the forest cover type (the predominant kind of tree cover) from strictly cartographic variables (as opposed to remotely sensed data). The actual forest cover type for a given 30 x 30 meter cell was determined from US Forest Service (USFS) Region 2 Resource Information System data. The independent variables were then derived from data obtained from the US Geological Survey and USFS\". ","18a0d302":"<a id='7.9'><\/a>\n## <p style=\"background-color:#1086CE; font-family:newtimeroman; color:white; font-size:120%; text-align:left;border-radius: 10px;\">___Inference and submission<\/p>","b45d64cb":"<a id='7.8'><\/a>\n## <p style=\"background-color:#1086CE; font-family:newtimeroman; color:white; font-size:120%; text-align:left;border-radius: 10px;\">___Training main<\/p>","2f7573f5":"<a href=\"#Title\" role=\"button\" aria-pressed=\"true\" >Back to the beginning \ud83d\udd19<\/a>","5aa66198":"### ***Dataset and Scaler***","2708211e":"<a id=''><\/a>\n# <p style=\"background-color:#1086CE; font-family:newtimeroman; color:white; font-size:120%; text-align:center;border-radius: 10px;\">Any suggestions to improve this notebook will be greatly appreciated. P\/s If I have forgotten to reference someone's work, please, do not hesitate to leave your comment. Any questions, suggestions or complaints are most welcome. Upvotes keep me motivated... Thank you.<\/p>\n","54b35aab":"<a id='8'><\/a>\n# <p style=\"background-color:#1086CE; font-family:newtimeroman; color:white; font-size:120%; text-align:center;border-radius: 10px;\">8. Conclusions<\/p>","2afdd803":"<a id='7.6'><\/a>\n## <p style=\"background-color:#1086CE; font-family:newtimeroman; color:white; font-size:120%; text-align:left;border-radius: 10px;\">___Train and valid loops with tqdm bar<\/p>","8cb2a6db":"<a id='7.2'><\/a>\n## <p style=\"background-color:#1086CE; font-family:newtimeroman; color:white; font-size:120%; text-align:left;border-radius: 10px;\">___Dataset<\/p>","94ae9c76":"### Missing Values, how to handle\n\n* **Option 1: Fill NaN with Outlier or Zero.**\nIn this specific example filling the missing value with an outlier value such as np.inf or 0 seems to be very naive. However, using values like -999, is sometimes a good idea.\n\n* **Option 2: Fill NaN with Mean Value.**\nFilling NaNs with the mean value is also not sufficient and naive, and doesn't seems to be a good option.\n\n* **Option 3: Fill NaN with Last Value with .ffill().**\nFilling NaNs with the last value could be bit better.\n\n* **Option 4: Fill NaN with Linearly Interpolated Value with .interpolate().**\nFilling NaNs with the interpolated values is the best option in this small examlple but it requires knowledge of the neighouring values.","06830cc7":"> If we wish to label the strength of the features association, for absolute values of correlation, 0-0.19 is regarded as very weak (the most of our examples are: -0.20-0.20). **\"Elevation\"** has a positive correlation with **\"Wilderness_Area4\"**. **\"Wilderness_Area1\"** and **\"Wilderness_Area3\"** are also correlated.\n","32187c17":"<a href=\"#Title\" role=\"button\" aria-pressed=\"true\" >Back to the beginning \ud83d\udd19<\/a>","337208cf":"<a href=\"#Title\" role=\"button\" aria-pressed=\"true\" >Back to the beginning \ud83d\udd19<\/a>","4b55dc1b":"### ***Submission***","e161d150":"### ***CFG***","42085bd0":"> Table Style and Table rendering can be found here: [**link**](https:\/\/pandas.pydata.org\/pandas-docs\/stable\/user_guide\/style.html)","51d60d80":"> Some observations from the table above:\n> * **Soil_Type7** and **Soil_Type15** are zeroes both in **`df_train`** and **`df_test`**.\n>\n> Let's drop it, it is not going to affect our trainig phase:","851aece9":"> **Now let's plot our target values**:","ea5c5604":"<a href=\"#Title\" role=\"button\" aria-pressed=\"true\" >Back to the beginning \ud83d\udd19<\/a>","d62abc06":"### ***Model***","6965076a":"> Let's us take a look at features correlation matrix:","b6e990ba":"<a id='7.1'><\/a>\n## <p style=\"background-color:#1086CE; font-family:newtimeroman; color:white; font-size:120%; text-align:left;border-radius: 10px;\">___Load Data<\/p>","f79ab662":"### ***Model***"}}