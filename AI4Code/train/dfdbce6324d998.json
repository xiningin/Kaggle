{"cell_type":{"d16097ed":"code","285416d8":"code","323fd8a3":"code","b80a8183":"code","914dc320":"code","14c99c08":"code","ea2fe3f3":"code","688fa747":"code","7f868b8c":"code","57e51629":"code","eb65dfc3":"code","9af2a20c":"code","97951b85":"code","a6fdafbc":"code","e21593f4":"code","a7d638ed":"code","f7adfdfb":"code","dc2c9be6":"code","abd6e1f9":"code","75a16be5":"code","8a5d6611":"code","28867334":"code","b426ec10":"code","08a1aa68":"code","6063655d":"code","ca759147":"code","fcbcf9fa":"code","dbf6ec36":"code","2410fa92":"code","4c3856b2":"code","d8851f3f":"code","27d5c4e0":"code","8a1fff2a":"code","b10c2772":"code","45f5d082":"code","41073ca4":"code","2c452267":"code","3e090bf2":"code","476a48e0":"code","c47f24cc":"code","4adfd203":"code","a29a8e51":"code","d9090fe2":"code","4364f1dc":"code","081559d5":"code","50958e53":"code","a1243142":"code","311d1488":"code","45e54fe3":"code","2f152211":"code","7c0d6168":"code","560f2f53":"code","e6751ed6":"markdown","156cba97":"markdown","d0573d9a":"markdown","c002784a":"markdown","fa53df3e":"markdown","0937bdc1":"markdown","66fa7f5a":"markdown","b7faa793":"markdown","fad3bf62":"markdown","b3514b25":"markdown","179a0077":"markdown","814a4002":"markdown","db8c81fa":"markdown","f6005d8f":"markdown","898b79c8":"markdown","476a333d":"markdown","9696b1ca":"markdown","4920c071":"markdown","b07a017e":"markdown","5c2756e0":"markdown","f2548d36":"markdown","4c2f9e88":"markdown","4a353f6f":"markdown","8d4bac31":"markdown","4f85cbc2":"markdown","20f3dc05":"markdown","a004a306":"markdown","e5142e90":"markdown","05d21658":"markdown","38448c95":"markdown","bc64711c":"markdown","e499f717":"markdown","02db146c":"markdown","a50b2bd5":"markdown","e7c88f38":"markdown","f48354ad":"markdown","d96437fc":"markdown","420d9df9":"markdown","5e3a6d14":"markdown","1174dbbc":"markdown","914b0244":"markdown","58ebe2d7":"markdown"},"source":{"d16097ed":"import numpy as np\nimport pandas as pd\nimport nltk\nimport random\nimport os\nfrom os import path\nfrom PIL import Image\n\n# Visualization\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nfrom subprocess import check_output\nfrom wordcloud import WordCloud, STOPWORDS\n\n# Set Plot Theme\nsns.set_palette([\n    \"#30a2da\",\n    \"#fc4f30\",\n    \"#e5ae38\",\n    \"#6d904f\",\n    \"#8b8b8b\",\n])\n# Alternate # plt.style.use('fivethirtyeight')\n\n# Pre-Processing\nimport string\nfrom nltk.tokenize import RegexpTokenizer\nfrom nltk.corpus import stopwords\nimport re\nfrom nltk.stem import PorterStemmer\n\n# Modeling\nimport statsmodels.api as sm\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\nfrom nltk.sentiment.util import *\nfrom nltk.util import ngrams\nfrom collections import Counter\nfrom gensim.models import word2vec\n\n# Warnings\nimport warnings\nwarnings.filterwarnings('ignore')","285416d8":"# Read and Peak at Data\ndf = pd.read_csv(\"..\/input\/womens-ecommerce-clothing-reviews\/Womens Clothing E-Commerce Reviews.csv\",index_col =[0])\ndf.head()","323fd8a3":"#Number of unique values\ndf.nunique()","b80a8183":"df.describe().T.drop(\"count\",axis=1)","914dc320":"# Continous Distributions\nf, ax = plt.subplots(1,3,figsize=(16,8), sharey=False)\nsns.distplot(df.Age, ax=ax[0])\nax[0].set_title(\"Age Distribution\")\nax[0].set_ylabel(\"Density\")\nsns.distplot(df[\"Positive Feedback Count\"], ax=ax[1])\nax[1].set_title(\"Positive Feedback Count Distribution\")\nsns.distplot(np.log10((df[\"Positive Feedback Count\"][df[\"Positive Feedback Count\"].notnull()]+1)), ax=ax[2])\nax[2].set_title(\"Positive Feedback Count Distribution\\n[Log 10]\")\nax[2].set_xlabel(\"Log Positive Feedback Count\")\nplt.tight_layout()\nplt.show()","14c99c08":"df.info()","ea2fe3f3":"df.drop(labels =['Clothing ID','Title'],axis = 1,inplace = True)#Dropping unwanted columns","688fa747":"df[df['Review Text'].isnull()].shape\n","7f868b8c":"data = df[~df['Review Text'].isnull()]  #Dropping columns which don't have any review\ndata.shape","57e51629":"import plotly.express as px\n\npx.histogram(data, x = data['Rating'], color = data[\"Recommended IND\"])\n","eb65dfc3":"px.histogram(data, x = data['Class Name'])","9af2a20c":"px.scatter(data, x=\"Age\", y=\"Positive Feedback Count\", facet_row=\"Recommended IND\", facet_col=\"Rating\",trendline=\"ols\",category_orders={\"Rating\": [1,2,3,4,5],'Recommended IND':[0,1]})","97951b85":"px.box(data, x=\"Age\", y=\"Division Name\", orientation=\"h\",color = 'Recommended IND')\n","a6fdafbc":"err1 = data['Review Text'].str.extractall(\"(&amp)\")\nerr2 = data['Review Text'].str.extractall(\"(\\xa0)\")\nprint('with &amp',len(err1[~err1.isna()]))\nprint('with (\\xa0)',len(err2[~err2.isna()]))\n\ndata['Review Text'] = data['Review Text'].str.replace('(&amp)','')\ndata['Review Text'] = data['Review Text'].str.replace('(\\xa0)','')\n\nerr1 = data['Review Text'].str.extractall(\"(&amp)\")\nprint('with &amp',len(err1[~err1.isna()]))\nerr2 = data['Review Text'].str.extractall(\"(\\xa0)\")\nprint('with (\\xa0)',len(err2[~err2.isna()]))","e21593f4":"!pip install TextBlob\nfrom textblob import *\n\ndata['polarity'] = data['Review Text'].map(lambda text: TextBlob(text).sentiment.polarity)\ndata['polarity']","a7d638ed":"px.histogram(data, x = 'polarity',color=\"Rating\", opacity = 0.5)","f7adfdfb":"px.box(data, y=\"polarity\", x=\"Department Name\", orientation=\"v\",color = 'Recommended IND')","dc2c9be6":"data['review_len'] = data['Review Text'].astype(str).apply(len)\npx.histogram(data, x = 'review_len' ,color = \"Recommended IND\")","abd6e1f9":"data['token_count'] = data['Review Text'].apply(lambda x: len(str(x).split()))\npx.histogram(data, x = 'token_count',color = \"Recommended IND\")","75a16be5":"sam = data.loc[data.polarity == 1,['Review Text']].sample(3).values\nfor i in sam:\n    print(i[0])","8a5d6611":"sam = data.loc[data.polarity == 0.5,['Review Text']].sample(3).values\nfor i in sam:\n    print(i[0])","28867334":"sam = data.loc[data.polarity < 0,['Review Text']].sample(3).values\nfor i in sam:\n    print(i[0])","b426ec10":"negative = (len(data.loc[data.polarity <0,['Review Text']].values)\/len(data))*100\npositive = (len(data.loc[data.polarity >0.5,['Review Text']].values)\/len(data))*100\nneutral  = len(data.loc[data.polarity >0 ,['Review Text']].values) - len(data.loc[data.polarity >0.5 ,['Review Text']].values)\nneutral = neutral\/len(data)*100\n\nfrom matplotlib import pyplot as plt \nplt.figure(figsize =(10, 7)) \nplt.pie([positive,negative,neutral], labels = ['Positive','Negative','Neutral'] , colors = [\"green\" ,\"red\" ,\"mediumslateblue\"])","08a1aa68":"from sklearn.feature_extraction.text import CountVectorizer\ndef top_n_ngram(corpus,n = None,ngram = 1):\n    vec = CountVectorizer(stop_words = 'english',ngram_range=(ngram,ngram)).fit(corpus)\n    bag_of_words = vec.transform(corpus) #Have the count of  all the words for each review\n    sum_words = bag_of_words.sum(axis =0) #Calculates the count of all the word in the whole review\n    words_freq = [(word,sum_words[0,idx]) for word,idx in vec.vocabulary_.items()]\n    words_freq = sorted(words_freq,key = lambda x:x[1],reverse = True)\n    return words_freq[:n]","6063655d":"common_words = top_n_ngram(data['Review Text'], 10,1)\ndf = pd.DataFrame(common_words, columns = ['ReviewText' , 'count'])\nplt.figure(figsize =(10,5))\ndf.groupby('ReviewText').sum()['count'].sort_values(ascending=False).plot(\nkind='bar', title='Top 10 unigrams in review after removing stop words')","ca759147":"common_words = top_n_ngram(data['Review Text'], 20,2)\ndf = pd.DataFrame(common_words, columns = ['ReviewText' , 'count'])\nplt.figure(figsize =(10,5))\ndf.groupby('ReviewText').sum()['count'].sort_values(ascending=False).plot(\nkind='bar', title='Top 20 bigrams in review after removing stop words')","fcbcf9fa":"blob= TextBlob(str(data['Review Text']))\npos = pd.DataFrame(blob.tags,columns =['word','pos'])\npos1 = pos.pos.value_counts()[:20]\nplt.figure(figsize = (10,5))\npos1.plot(kind='bar',title ='Top 20 Part-of-speech taggings')","dbf6ec36":"y = data['Recommended IND']\nX = data.drop(columns = 'Recommended IND')","2410fa92":"import seaborn as sns\nsns.heatmap(X.corr(),annot =True, cmap = \"icefire\")","4c3856b2":"set1 =set()\ncor = X.corr()\nfor i in cor.columns:\n    for j in cor.columns:\n        if cor[i][j]>0.8 and i!=j:\n            set1.add(i)\nprint(set1)\nX = X.drop(labels = ['token_count'],axis = 1)\nprint(\"correlation: \", X.corr())","d8851f3f":"class1 =[]\nfor i in X.polarity:\n    if float(i)>=0.0:\n        class1.append(1)\n    elif float(i)<0.0:\n        class1.append(0)\nX['sentiment'] = class1\n\nX.groupby(X['sentiment']).describe().T","27d5c4e0":"print(\"Shape of X: \" , X.shape)\nprint(\"Shape of y: \" , y.shape)","8a1fff2a":"import nltk\nimport re\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\n\n\ncorpus =[]\nX.index = np.arange(len(X))","b10c2772":"len(X)","45f5d082":"from tqdm import tqdm\nfor i in tqdm(range(len(X))):\n    review = re.sub('[^a-zA-z]',' ',X['Review Text'][i])\n    review = review.lower()\n    review = review.split()\n    ps = PorterStemmer()\n    review =[ps.stem(i) for i in review if not i in set(stopwords.words('english'))]\n    review =' '.join(review)\n    corpus.append(review)\ncorpus[0:5]","41073ca4":"len(corpus)","2c452267":"from sklearn.feature_extraction.text import CountVectorizer as CV\ncv  = CV(max_features = 3000,ngram_range=(1,1))\nX_cv = cv.fit_transform(corpus).toarray()\ny = y.values\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X_cv, y, test_size = 0.20, random_state = 0)\nfrom sklearn.naive_bayes import BernoulliNB\nclassifier = BernoulliNB()\nclassifier.fit(X_train, y_train)\n\ny_pred = classifier.predict(X_test)\nfrom sklearn.metrics import accuracy_score\nfrom sklearn import metrics\nacc = accuracy_score(y_test, y_pred)\nprint(\"Accuracy of the classifier: \",acc)\nprint(\"Confusion matrix is :\\n\",metrics.confusion_matrix(y_test,y_pred))\nprint(\"Classification report: \\n\" ,metrics.classification_report(y_test,y_pred))","3e090bf2":"from sklearn.feature_extraction.text import TfidfVectorizer as TV\ntv  = TV(ngram_range =(1,1),max_features = 3000)\nX_tv = tv.fit_transform(corpus).toarray()\nX_train, X_test, y_train, y_test = train_test_split(X_tv, y, test_size = 0.20, random_state = 0)\nfrom sklearn.naive_bayes import MultinomialNB\nclassifier = MultinomialNB()\nclassifier.fit(X_train, y_train)\n\ny_pred = classifier.predict(X_test)\nacc = accuracy_score(y_test, y_pred)\nprint(\"Accuracy of the classifier: \",acc)\nprint(\"Confusion matrix is :\\n\",metrics.confusion_matrix(y_test,y_pred))\nprint(\"Classification report: \\n\" ,metrics.classification_report(y_test,y_pred))","476a48e0":"import tensorflow as tf\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\ntokenizer = Tokenizer(num_words = 3000)\ntokenizer.fit_on_texts(corpus)\nsequences = tokenizer.texts_to_sequences(corpus)\npadded = pad_sequences(sequences, padding='post')\nword_index = tokenizer.word_index\ncount = 0\nfor i,j in word_index.items():\n    if count == 11:\n        break\n    print(i,j)\n    count = count+1","c47f24cc":"embedding_dim = 64\nmodel = tf.keras.Sequential([\n    tf.keras.layers.Embedding(3000, embedding_dim),\n    tf.keras.layers.GlobalAveragePooling1D(),\n    tf.keras.layers.Dense(6, activation='relu'),\n    tf.keras.layers.Dense(1, activation='sigmoid')\n])\n\nmodel.summary()","4adfd203":"num_epochs = 10\n\nmodel.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\nmodel.fit(padded,y,epochs= num_epochs,validation_split= 0.39)","a29a8e51":"loss = model.history.history\nloss = pd.DataFrame(loss)","d9090fe2":"f, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\nt = f.suptitle('Basic ANN Performance', fontsize=12)\nf.subplots_adjust(top=0.85, wspace=0.3)\n\nepoch_list = range(1,11)\nax1.plot(epoch_list, loss['accuracy'], label='Train Accuracy')\nax1.plot(epoch_list, loss['val_accuracy'], label='Validation Accuracy')\nax1.set_xticks(np.arange(0, 11, 1))\nax1.set_ylabel('Accuracy Value')\nax1.set_xlabel('Epoch')\nax1.set_title('Accuracy')\nl1 = ax1.legend(loc=\"best\")\n\nax2.plot(epoch_list, loss['loss'], label='Train Loss')\nax2.plot(epoch_list, loss['val_loss'], label='Validation Loss')\nax2.set_xticks(np.arange(0, 11, 1))\nax2.set_ylabel('Loss Value')\nax2.set_xlabel('Epoch')\nax2.set_title('Loss')\nl2 = ax2.legend(loc=\"best\")","4364f1dc":"sample_string = \"I hate it\"\nsample = tokenizer.texts_to_sequences(sample_string)\npadded_sample = pad_sequences(sample, padding='post')\nprint(\"Padded sample\", padded_sample.T)\nprint(\"Probabilty of a person recommending :\",model.predict(padded_sample.T)[0][0]*100,\"%\")","081559d5":"sample_string = \"i love the fabric\"\nsample = tokenizer.texts_to_sequences(sample_string)\npadded_sample = pad_sequences(sample, padding='post')\nprint(\"Padded sample\", padded_sample.T)\nprint(\"Probabilty of a person recommending :\",model.predict(padded_sample.T)[0][0]*100,\"%\")","50958e53":"y = data['Recommended IND'].tolist()\nX = list(data[\"Review Text\"])\n\n# Separate out the sentences and labels into training and test sets\ntraining_size = int(len(X) * 0.8)\n\ntraining_sentences = X[0:training_size]\ntesting_sentences = X[training_size:]\ntraining_labels = y[0:training_size]\ntesting_labels = y[training_size:]\n\n# Make labels into numpy arrays for use with the network later\ntraining_labels_final = np.array(training_labels)\ntesting_labels_final = np.array(testing_labels)","a1243142":"vocab_size = 1000\nembedding_dim = 16\nmax_length = 100\ntrunc_type='post'\npadding_type='post'\noov_tok = \"<OOV>\"\n\n\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\ntokenizer = Tokenizer(num_words = vocab_size, oov_token=oov_tok)\ntokenizer.fit_on_texts(training_sentences)\nword_index = tokenizer.word_index\nsequences = tokenizer.texts_to_sequences(training_sentences)\npadded = pad_sequences(sequences,maxlen=max_length, padding=padding_type, \n                       truncating=trunc_type)\n\ntesting_sequences = tokenizer.texts_to_sequences(testing_sentences)\ntesting_padded = pad_sequences(testing_sequences,maxlen=max_length, \n                               padding=padding_type, truncating=trunc_type)","311d1488":"reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n\ndef decode_review(text):\n    return ' '.join([reverse_word_index.get(i, '?') for i in text])\n\nprint(decode_review(padded[1]))\nprint(training_sentences[1])","45e54fe3":"# Build a basic sentiment network\n# Note the embedding layer is first, \n# and the output is only 1 node as it is either 0 or 1 (negative or positive)\nmodel = tf.keras.Sequential([\n    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n    tf.keras.layers.Flatten(),\n    tf.keras.layers.Dense(6, activation='relu'),\n    tf.keras.layers.Dense(1, activation='sigmoid')\n])\nmodel.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\nmodel.summary()","2f152211":"num_epochs = 10\nmodel.fit(padded, training_labels_final, epochs=num_epochs, validation_data=(testing_padded, testing_labels_final))","7c0d6168":"loss = model.history.history\nloss = pd.DataFrame(loss)\nloss","560f2f53":"\n\nf, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\nt = f.suptitle('Basic ANN Performance', fontsize=12)\nf.subplots_adjust(top=0.85, wspace=0.3)\n\nepoch_list = range(1,11)\nax1.plot(epoch_list, loss['accuracy'], label='Train Accuracy')\nax1.plot(epoch_list, loss['val_accuracy'], label='Validation Accuracy')\nax1.set_xticks(np.arange(0, 11, 1))\nax1.set_ylabel('Accuracy Value')\nax1.set_xlabel('Epoch')\nax1.set_title('Accuracy')\nl1 = ax1.legend(loc=\"best\")\n\nax2.plot(epoch_list, loss['loss'], label='Train Loss')\nax2.plot(epoch_list, loss['val_loss'], label='Validation Loss')\nax2.set_xticks(np.arange(0, 11, 1))\nax2.set_ylabel('Loss Value')\nax2.set_xlabel('Epoch')\nax2.set_title('Loss')\nl2 = ax2.legend(loc=\"best\")","e6751ed6":"Most of the people are between 35 - 50 years.Trend suggest that the core market segment for this clothing brand is women between 34 and 50. With its single peak and slight right tail, the distribution of age is more or less normal.","156cba97":"# Creating Term frequency- Inverse Document Frequency Technique (tf-idf)","d0573d9a":"# Imputing the Data\n* Dropping Clothing ID and Title\n* Checking Review Text columns\n* Dropping columns which don't have any review","c002784a":"# Visualising Class name counts ","fa53df3e":"# NLTK\n\nNLTK stands for Natural Language Toolkit. This toolkit is one of the most powerful NLP libraries which contains packages to make machines understand human language and reply to it with an appropriate response. Tokenization, Stemming, Lemmatization, Punctuation, Character count.\n\nUsing nltk we can determine the mood of text, NLTK\u2019s N-grams, and gensim.models\u2019s word2vec. It also includes statsmodels.api which offers an array of linear models.","0937bdc1":"# Model II","66fa7f5a":"# RE -> Tokenizing -> Stemming -> Corpus Creation ","b7faa793":"* Creating X and y for training and test data","fad3bf62":"![](https:\/\/i.pinimg.com\/originals\/dd\/80\/9c\/dd809cd103e3f94c252b6073c474bcac.png)","b3514b25":"# Creating Count Vector","179a0077":"# Visualising Model history","814a4002":"# Importing the Data","db8c81fa":"Hence, the model predicts that for a comment \"I love the fabric\" there is 99.98 % chance of recommending it to someone","f6005d8f":"# Visualizing Top 20 Part-of-Speech\n","898b79c8":"# Bag of words\nTo create the bag of words model, we need to create a matrix where the columns correspond to the most frequent words in our dictionary where rows correspond to the document or sentences.","476a333d":"# Visualising Heatmaps","9696b1ca":"   # Visualizing Top 10 Unigrams","4920c071":"# Plotting Divisions and Age with Recommended as hue","b07a017e":"Checking the number of unique values for each column","5c2756e0":"# Visualising Polarity of Review Texts","f2548d36":"# Creating Corpus","4c2f9e88":"# Creating N grams \n\nN-grams of texts are extensively used in *text mining* and *natural language processing* tasks. They are basically a set of co-occuring words within a given window and when computing the n-grams you typically move one word forward (although you can move X words forward in more advanced scenarios). ","4a353f6f":"![](https:\/\/www.guru99.com\/images\/1\/122118_0534_NLPNaturalL3.png)","8d4bac31":"# Problem Description\n# Features present :\n* Clothing ID: Integer Categorical variable that refers to the specific piece being reviewed.\n* Age: Positive Integer variable of the reviewers age.\n* Title: String variable for the title of the review.\n* Review Text: String variable for the review body.\n* Rating: Positive Ordinal Integer variable for the product score granted by the customer from 1 Worst, to 5 Best.\n* Positive Feedback Count: Positive Integer documenting the number of other customers who found this review positive.\n* Division Name: Categorical name of the product high level division.\n* Department Name: Categorical name of the product department name.\n* Class Name: Categorical name of the product class name.\n\n# Target :\n\n* Recommended IND: Binary variable stating where the customer recommends the product where 1 is recommended, 0 is not recommended.","4f85cbc2":"# Defining our data","20f3dc05":"Hence, the model predicts that for a comment \"I hate it\" there is only 24.61% chance of recommending it to someone","a004a306":"# Creating a model to check Recommended","e5142e90":"# Importing Modules","05d21658":"# Visualizing Top 20 Bigrams","38448c95":"# Visualising Data\n* Checking Rating vs Recommended IND column","bc64711c":"# Statistical Description of Data","e499f717":"# Reviews with Positive Polarity ","02db146c":"# Checking the model on a random example\n* Check if the review will be recommended or not.","a50b2bd5":"# Handling Multi - Colinearity","e7c88f38":"# Creating a ANN structure","f48354ad":"# Checking age distribution and positive feedback","d96437fc":"# Reviews with Neutral Polarity ","420d9df9":"# Creating the model\n\n* Stemming is the process of producing morphological variants of a root\/base word. Stemming programs are commonly referred to as stemming algorithms or stemmers using PorterStemmer from NLTK\n","5e3a6d14":"# Visualising the polarity of Reviews using a pie chart","1174dbbc":"# MODEL I","914b0244":"We see that almost 3000 title data is missing. But since we will not use this feature we won't trim the data further","58ebe2d7":"# Reviews with negative Polarity"}}