{"cell_type":{"3e4ac497":"code","93e8f531":"code","a71a5a1d":"code","22759a56":"code","6ecdcf1c":"code","239c780e":"code","c196126d":"code","a0de831c":"code","e279c47e":"code","e1fe73b2":"code","7196be29":"code","f4c0df68":"code","a7ef32f8":"code","54079a4f":"code","73ae1844":"code","309a6b0b":"code","a8df8f63":"code","153a4683":"code","457d0984":"code","0d176a15":"code","c76d6479":"code","ee1540ff":"code","614d6227":"code","6d5ffcf0":"code","6d5adb4d":"code","2b9ea916":"code","47eedccb":"code","45741710":"code","c4724da2":"code","c165bc95":"code","25ddfc73":"code","9e33900b":"code","a4bf0197":"code","fc3aa64b":"code","05105971":"code","eeb36947":"code","0c05d10d":"code","15c3004a":"code","1d790de6":"code","15db3480":"code","0c157ef5":"code","eba05e6c":"code","d0930111":"code","5d7152a6":"code","bff83b0e":"code","f1fb8d4e":"code","ae24e818":"code","18a3b9e8":"code","c197fbd9":"markdown","381ba0bf":"markdown","d3391569":"markdown","3b676bf8":"markdown","27fe2d69":"markdown","629375f7":"markdown","d86e393e":"markdown","90bb2691":"markdown","c7a45bf2":"markdown","5a4ac7a5":"markdown","ea0e2192":"markdown","61b1d599":"markdown","5b945900":"markdown","dff59f3c":"markdown","330db843":"markdown","944e154e":"markdown","4cd7c40c":"markdown","abcab8f9":"markdown","2a6f4e2e":"markdown","ce10378d":"markdown","12b73cc4":"markdown","105274b2":"markdown","42692972":"markdown","30aab4bb":"markdown","779ed4e3":"markdown","f903032e":"markdown","f7ec2205":"markdown","cb704c12":"markdown","e6b461d9":"markdown","04f8a0ca":"markdown"},"source":{"3e4ac497":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd\nimport seaborn as sns# data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","93e8f531":"df = pd.read_csv('\/kaggle\/input\/hmeq-data\/hmeq.csv')\ndf.head()","a71a5a1d":"df.shape, df.info()","22759a56":"# Estat\u00edsticas Descritivas\ndf.describe(include='all')","6ecdcf1c":"#Utilizando o pandas profiling para auxiliar a EDA\nimport pandas_profiling as pp\npp.ProfileReport(df)","239c780e":"# avalia\u00e7\u00e3o das vari\u00e1veis num\u00e9ricas por meio de histogramas\nimport matplotlib.pyplot as plt\n%matplotlib inline\ndf.hist(figsize=(20,10))","c196126d":"MissingValues =df.isnull().sum().rename_axis('Colunas').reset_index(name='Missing Values')\nMissingValues","a0de831c":"# retirando os na\ndf2 = df.copy()\ndf2.dropna(axis=0,how='any',inplace= True)\ndf2.info(), df2.isna().any() \n","e279c47e":"import matplotlib.pyplot as plt\n%matplotlib inline\ndf2.hist(figsize=(25,14),bins=10)","e1fe73b2":"# Correla\u00e7\u00e3o das vari\u00e1veis num\u00e9ricas\nplt.figure(figsize= (15, 15))\n\nsns.heatmap(df2.corr(), square=True, annot=True, linewidth=0.5)","7196be29":"dfWithBin = df.copy()\nbins=[0,3,15] \ngroup=['Low','High'] \ndfWithBin['DELINQ_bin']=pd.cut(dfWithBin['DELINQ'],bins,labels=group)\nLOAN_bin=pd.crosstab(dfWithBin['DELINQ_bin'],dfWithBin['BAD'])\nLOAN_bin.div(LOAN_bin.sum(1).astype(float), axis=0).plot(kind=\"bar\", stacked=True,title='Cruzamento de Linhas de Cr\u00e9dito Inadimplentes e Maus pagadores')\nplt.xlabel('DELINQ')\nP= plt.ylabel('%')","f4c0df68":"#avaliacao dos default loans\n\ndf2[df2['BAD']==1].drop('BAD', axis=1).describe().style.format(\"{:.2f}\")","a7ef32f8":"# Avaliando as vari\u00e1veis categ\u00f3ricas em relacao ao pefil do pagador\n\nJOB=pd.crosstab(df['JOB'],df['BAD'])\nJOB.div(JOB.sum(1).astype(float), axis=0).plot(kind=\"bar\", stacked=True, title='Tipos de Empregos e Clientes', figsize=(4,4))","54079a4f":"REASON=pd.crosstab(df['REASON'],df['BAD'])\nREASON.div(REASON.sum(1).astype(float), axis=0).plot(kind=\"bar\", stacked=True, title='Tipos de Empregos e Raz\u00f5es', figsize=(4,4))","73ae1844":"# Gerando Dummies para modelos que utilizam apenas variaveis num\u00e9ricas\n\ndf2 = pd.get_dummies(df2, columns=['REASON', 'JOB'])","309a6b0b":"df2.head().T","a8df8f63":"#Normalizando os dados para facilitar poss\u00edvel visualizacoes\n\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\ndf3 = pd.DataFrame(sc.fit_transform(df2), columns=df2.columns)","153a4683":"# importando a biblioteca\nfrom sklearn.model_selection import train_test_split","457d0984":"#Etapa 1- Primeiro Separando em Treino e Teste, par\u00e2metro test_size = 0.25 (default)\ntreino, teste = train_test_split(df2, random_state=42)\n\n#Etapa 2 -  Separando o Treino em treino e validacao, para refinar o modelo\n#treino, validacao = train_test_split(treino, random_state=42)\n\ntreino.shape, teste.shape # validacao.shape, ","0d176a15":"teste.describe()","c76d6479":"# Verificando se as amostras possuem similaridade, avaliando se h\u00e1 discrep\u00e2ncia alta considerando a m\u00e9dia e desvio padr\u00e3o de cada uma. Pela an\u00e1lise verifica-se que a amostra gerada \n# possuem estat\u00edsticas pr\u00f3ximas, portanto atendem ao requisito.\ntreino.describe()","ee1540ff":"#Selecionando as colunas que usaremos para treinar o modelo\nnao_usadas = ['BAD']\n\n# Lista das colunas que ser\u00e3o usadas\nusadas = [c for c in treino.columns if c not in nao_usadas]","614d6227":"# Avaliando desempenho do modelo\n#importando m\u00e9trica\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import f1_score\nresults = pd.DataFrame(columns=['Modelo', 'Accuracy', 'F1score'])","6d5ffcf0":"# importanto o modelo\nfrom sklearn.ensemble import RandomForestClassifier\n\n#instanciando o modelo\nrf = RandomForestClassifier(n_estimators=200,random_state=42)","6d5adb4d":"# treinando o modelo\nrf.fit(treino[usadas], treino['BAD'])\n\n#Prevendo os dados de validacao\n\n# gerando predicoes do modelo com os dados de teste\npred_teste = rf.predict(teste[usadas])\n\n#Medindo a acuracia nos dados de teste\nresults.loc[0]= ['RandonForest sem ajuste', accuracy_score(teste['BAD'],pred_teste), f1_score(teste['BAD'],pred_teste)]\n\naccuracy_score(teste['BAD'],pred_teste), f1_score(teste['BAD'],pred_teste)\n","2b9ea916":"# Avaliando a importancia de cada coluna (cada vari\u00e1vel de entrada)\npd.Series(rf.feature_importances_, index=usadas).sort_values().plot.barh()","47eedccb":"# importando a bilbioteca para plotar o gr\u00e1fico de Matriz de Confus\u00e3o\nimport scikitplot as skplt\n\n# Matriz de Confus\u00e3o - Dados de Valida\u00e7\u00e3o\nskplt.metrics.plot_confusion_matrix(teste['BAD'], pred_teste)","45741710":"# Setando parametros\nrf2 = RandomForestClassifier(max_depth=None, random_state=42, n_jobs=-1, n_estimators=900,\n                            min_impurity_decrease=1e-3, min_samples_leaf=2,  class_weight='balanced')\n# treinando o modelo RF2\nrf2.fit(treino[usadas], treino['BAD'])","c4724da2":"#relizando a predicao do RF2 com base teste\npred_teste2 = rf2.predict(teste[usadas])\n\n#m\u00e9trica para RF2 validacao\nresults.loc[1]= ['RandonForest COM ajuste', accuracy_score(teste['BAD'],pred_teste2), f1_score(teste['BAD'],pred_teste2)]\n\naccuracy_score(teste['BAD'],pred_teste2), f1_score(teste['BAD'],pred_teste2)","c165bc95":"# Matriz de Confus\u00e3o - Dados de Valida\u00e7\u00e3o\nskplt.metrics.plot_confusion_matrix(teste['BAD'], pred_teste2)","25ddfc73":"# Importar o modelo\nfrom xgboost import XGBClassifier\n\n# Instanciar o modelo\nxgb = XGBClassifier(n_jobs=-1, random_state=42)\n\n# treinando o modelo\nxgb.fit(treino[usadas],treino['BAD']) \n\n# Fazendo predi\u00e7\u00f5es\n#pred_xgb_validacao = xgb.predict(validacao[usadas])\n\n# Metr\u00edcas XGB validacao\n#accuracy_score(validacao['BAD'],pred_xgb_validacao), balanced_accuracy_score(validacao['BAD'],pred_xgb_validacao), f1_score(validacao['BAD'],pred_xgb_validacao)","9e33900b":"# Fazendo predi\u00e7\u00f5es\npred_xgb_teste = xgb.predict(teste[usadas])\n\n# Metr\u00edcas XGB teste\nresults.loc[2]= ['XGBoost', accuracy_score(teste['BAD'],pred_xgb_teste), f1_score(teste['BAD'],pred_xgb_teste)]\n\naccuracy_score(teste['BAD'],pred_xgb_teste), f1_score(teste['BAD'],pred_xgb_teste)","a4bf0197":"# Matriz de Confus\u00e3o - Dados de Valida\u00e7\u00e3o\nskplt.metrics.plot_confusion_matrix(teste['BAD'], pred_xgb_teste)","fc3aa64b":"# Importa\u00e7\u00e3o bibliotecas\n# Importa\u00e7\u00e3o GridSearchCV.\nfrom sklearn.model_selection import GridSearchCV\n\n# Uso do constructor do XGBoost para criar um classifier.\nxgb2 = XGBClassifier(n_jobs=-1) # Sem nada dentro, pois vamos \"variar\" os par\u00e2metros.","05105971":"# Para o balaceamento do gridSearchCV foram realizadas tr\u00eas rodadas, a partir dos best score de cada \u00e9poca. \nparametros = {'n_estimators':[100,500, 900, 1100],\n              'learning_rate':[0.02,0.08,0.09,1.5]}","eeb36947":"# Importando o Make Scorer\nfrom sklearn.metrics import make_scorer\n\n# Importando os m\u00f3dulos de c\u00e1lculo de m\u00e9tricas\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import f1_score","0c05d10d":"# Criando um dicion\u00e1rio com as m\u00e9tricas que desejo calcular.\nmeus_scores = {'accuracy' :make_scorer(accuracy_score),\n               'recall'   :make_scorer(recall_score),\n               'precision':make_scorer(precision_score),\n               'f1'       :make_scorer(f1_score)}\n\n# Exemplo para o uso scoring igual ao meus_scores.\ngrid = GridSearchCV(estimator = xgb2,\n                      param_grid = parametros,\n                      cv = 10,\n                      scoring = meus_scores,   # \u00c9 o meus_scores\n                      refit = 'f1')            # Observe que foi configurado para f1\n\n# Imprime o melhor score(f1) e melhor par\u00e2metro \ngrid.fit(treino[usadas],treino['BAD'])","15c3004a":"grid.best_score_, grid.best_params_","1d790de6":"#  Caso queira dar uma olhada nos outros scores\npd.DataFrame(grid.cv_results_).sort_values('rank_test_f1')[:3].T","15db3480":"# Criando um objeto que os melhores parametros.\nxgb_gs = grid.best_estimator_\n\n# Visualizar o objeto para conferir os parametros.\nxgb_gs","0c157ef5":"#primeira epoca \n# Fazendo predi\u00e7\u00f5es teste\npred_xgb_gs_teste = xgb_gs.predict(teste[usadas])\n\n# Metr\u00edcas XGB teste\nresults.loc[3]= ['XGBoost com GridSearchCV',accuracy_score(teste['BAD'],pred_xgb_gs_teste), f1_score(teste['BAD'],pred_xgb_gs_teste)]\n\naccuracy_score(teste['BAD'],pred_xgb_gs_teste), f1_score(teste['BAD'],pred_xgb_gs_teste)","eba05e6c":"# Matriz de Confus\u00e3o - Dados de Valida\u00e7\u00e3o\nskplt.metrics.plot_confusion_matrix(teste['BAD'], pred_xgb_gs_teste)","d0930111":"#instanciando o modelo\nrf2=RandomForestClassifier(n_jobs=-1)\n\n#setando parametros para o gridSearchCV\nparam_dict = { 'n_estimators':[100,400,800,1000],\n               'criterion': ['gini','entropy']\n              }\n\ngrid2 = GridSearchCV(rf2, param_dict, cv=10)\n\n#treinando modelo\ngrid2.fit(treino[usadas], treino['BAD'])","5d7152a6":"#Resultados\ngrid2.best_params_ , grid2.best_score_","bff83b0e":"# Criando um objeto que os melhores parametros.\nrf2_gs2 = grid2.best_estimator_\n\n# Visualizar o objeto para conferir os parametros.\nrf2_gs2\n","f1fb8d4e":"# predicao teste\npred_rf2_gs2_teste = rf2_gs2.predict(teste[usadas])\n\n# metricas predicao teste\nresults.loc[4]= ['RandomForest com GridSearchCV', accuracy_score(teste['BAD'],pred_rf2_gs2_teste),f1_score(teste['BAD'],pred_rf2_gs2_teste)]\n\naccuracy_score(teste['BAD'],pred_rf2_gs2_teste), f1_score(teste['BAD'],pred_rf2_gs2_teste)","ae24e818":"# Matriz de Confus\u00e3o - Dados de Valida\u00e7\u00e3o\nskplt.metrics.plot_confusion_matrix(teste['BAD'], pred_rf2_gs2_teste)","18a3b9e8":"results","c197fbd9":"Com os ajustes de par\u00e2metros, houve uma melhora pequena no F1 Score.","381ba0bf":"## Gera\u00e7\u00e3o Amostras de Treino e Teste","d3391569":"# IESB\n## P\u00f3s Gradua\u00e7\u00e3o em Ci\u00eancia de Dados\n\n### **Disciplina** - Data Mining e Machine Learning II\n### **Projeto de Conclus\u00e3o da Disciplina **\n### **Aluno:** Nilson Romero Michiles J\u00fanior\n### **Turma:** Asa Norte\n\n \n#### Descri\u00e7\u00e3o do Problema e Objetivo\n\nOs Bancos possuem uma necessidade de manter seu percentual de inadimpl\u00eancia baixo, sendo mais oneroso ainda tentar recuperar cr\u00e9ditos de habita\u00e7\u00e3o(Home Equity), tendo em vista as prote\u00e7\u00f5es que a legisla\u00e7\u00e3o concede aos inadimplentes dessa modalidade. Visto isso, urge a necessidade dos Bancos atuarem preventivamente para identificar poss\u00edveis maus pagadores e evitar poss\u00edveis adversidades futuras. \n\nAssim, esse modelo de dados apresenta preditor para identificar esses maus pagadores, por meio de um conjunto de dados de Home Equity com   cerca de seis mil empr\u00e9stimos concedidos no passado. Esses dados cont\u00eam v\u00e1rias informa\u00e7\u00f5es sobre a situa\u00e7\u00e3o do cliente no momento do empr\u00e9stimo e tamb\u00e9m cont\u00eam uma coluna 'RUIM', que indica se o cliente deixou de pagar o empr\u00e9stimo posteriormente. Podemos usar esse conjunto de dados juntamente com a vari\u00e1vel\/etiqueta \"RUIM\" para treinar modelos de aprendizado de m\u00e1quina, o que nos ajudaria a prever a probabilidade de algu\u00e9m deixar o empr\u00e9stimo no futuro com base na situa\u00e7\u00e3o atual. \n\nAssim, ao final deste notebook ser\u00e1 proposto o melhor modelo a ser usado para prever a etiqueta supramencionada baseada no padr\u00e3o da situa\u00e7\u00e3o apresentada nos dados. Esse problema pode ser classificado como problema de classifica\u00e7\u00e3o bin\u00e1ria, pois o modelo prever\u00e1 se uma pessoa seria o padr\u00e3o.\n\n\n#### Metodologia\n\nA base de dados \"Home Equity\" possui dados pessoas e informa\u00e7\u00f5es de empr\u00e9stimo de 5.960 empr\u00e9stimos recentes. Para cada empr\u00e9stimo existem 12 vari\u00e1veis registradas. A vari\u00e1vel alvo (BAD) indica quando o cliente n\u00e3o pagou o empr\u00e9stimo (valor 1), e quando ele honrou o compromisso (valor 0).\n\nSer\u00e3o utilizados os modelos Random Forest Classifier, XGBosst e XGBoost com aux\u00edlio do GridSearchCV para otimiza\u00e7\u00e3o do modelo","3b676bf8":"\nPela an\u00e1lise da Matriz de Confus\u00e3o, considerando que \u00e9 uma base bastante desbalanceada, como se observa no gr\u00e1fico abaixo. Assim, a an\u00e1lise das m\u00e9tricas de especifidade e esfor\u00e7o pode real\u00e7ar os falsos positivos","27fe2d69":"### Featuring Engineering","629375f7":"### Utilizando o RandonForest Classifier com ajuste nos par\u00e2metros\n\nForam ajustados os par\u00e2metros de aumentando o n\u00famero de estimadores para 900, quando o default \u00e9 100, e informando que o n\u00famero de folhas aceitavel para as ramifica\u00e7\u00f5es das \u00e1rvores de decis\u00e3o como 2.\n","d86e393e":"## Modelo RandomForest Classifier","90bb2691":"## Tratamento dos Dados\n\nPara o tratamento, ser\u00e3o avaliadas a exist\u00eancia de missing values ou valores null.","c7a45bf2":"A avalia\u00e7\u00e3o dos histogramas mostra inicialmente que :\n- A vari\u00e1vel BAD (Target) possui poucos valores 1 para treinamento do modelo\n- A maior parte dos valores totais de financiamento (LOAN) possuem uma distribui\u00e7\u00e3o pr\u00f3xima da normalidade, e os valores a receber(MORTDUE), na m\u00e9dia, s\u00e3o maiores que os totais emprestados. Observa-se o terror dos juros banc\u00e1rios\n- Os valores das propriedades possuem distribui\u00e7\u00e3o pr\u00f3xima dos valores dos financiamentos\n- O DEROG, algo equivalente \u00e0 um aviso de negativa\u00e7\u00e3o do servi\u00e7o de prote\u00e7\u00e3o ao consumidor, \u00e9 baixo, contudo possui uma correla\u00e7\u00e3o pr\u00f3xima de moderada(p=0,25) com os maus pagadores.\n- O DELINQ, linhas de cr\u00e9dito com inadimpl\u00eancia, tamb\u00e9m possui correla\u00e7\u00e3o pr\u00f3xima \u00e0 moderada(p=0,27) com maus pagadores\n- O n\u00famero de linhas de cr\u00e9dito possui correla\u00e7\u00e3o, mas a intensidade \u00e9 menor(p=0,13), com maus pagadores\n- Por fim, a base possui um indicador (D\u00e9bitos\/Renda) que possui uma correla\u00e7\u00e3o pr\u00f3xima a moderada (p=0,23), sendo um bom indicador.","5a4ac7a5":"Na an\u00e1lise inicial, se observa que a vari\u00e1vel Target('BAD'), possui um n\u00famero pequeno de 'maus pagadores'(=1), o que indica que \u00e9 uma base desbalanceada, sendo necess\u00e1rios ajustes ou m\u00e9tricas espec\u00edficas para essa distribui\u00e7\u00e3o","ea0e2192":"### Dicion\u00e1rio de Dados\nO dicion\u00e1rio de dados das colunas dispon\u00edveis no Dataset est\u00e3o elencadas abaixo:\n\n**BAD:** 1 = client defaulted on loan 0 = loan repaid\n\n**LOAN**: Amount of the loan request\n\n**MORTDUE**: Amount due on existing mortgage\n\n**VALUE**: Value of current property\n\n**REASON**: DebtCon = debt consolidation ; \nHomeImp = home improvement\n\n**JOB**: Six occupational categories\n\n**YOJ**: Years at present job\n\n**DEROG**: Number of major derogatory reports\n\n**DELINQ**: Number of delinquent credit lines\n\n**CLAGE**: Age of oldest trade line in months\n\n**NINQ**: Number of recent credit lines\n\n**CLNO**: Number of credit lines\n\n**DEBTINC**: Debt-to-income ratio","61b1d599":"# Conclus\u00e3o\n\n**Pela An\u00e1lise dos Modelos, verifica-se que o que apresentou o melhor resultado foi XGBoost, com as m\u00e9tricas de acur\u00e1ria 0.958 e F1 Score 0.6846)**","5b945900":"Nota: Neste trabalho foi realizada a modelagem utilzando uma amostra para valida\u00e7\u00e3o inclusive, contudo, devida a baixa quantidade de registros, foi utilizado apenas treino e teste ","dff59f3c":"### Explora\u00e7\u00e3o do Dataset","330db843":"Para melhor ajuste ao modelo, foram dummizadas as tabelas com type Object","944e154e":"Pela an\u00e1lise, se observa que o grupo que trabalha com vendas e empreendedores possuem um n\u00famero maior de maus pagadores","4cd7c40c":"## An\u00e1lise Descritiva Explorat\u00f3ria (EDA)\n\n","abcab8f9":"Pelo quadro acima, se observa que a base de dados n\u00e3o foi tratada, havendo uma quantidade alta de missings. Para a an\u00e1lise foram exclu\u00eddos as linhas que possuiam algum valor com NA, restando 3364 linhas.","2a6f4e2e":"## Modelo XGBoost com GridSearchCV\n\nO GridSearchCV \u00e9 um m\u00f3dulo do Scikit Learn que \u00e9 amplamente usado para automatizar grande parte do processo de tuning. O objetivo prim\u00e1rio do GridSearchCV \u00e9 a cria\u00e7\u00e3o de combina\u00e7\u00f5es de par\u00e2metros para posteriormente avali\u00e1-las.","ce10378d":"## Modelo Random Forest Classifier e GridSearchCV \n\nPor fim, a t\u00edtulo de avalia\u00e7\u00e3o, foi testada a Random Forest com tuning pelo GridSearchCV para verifiar poss\u00edvel melhora no modelo.","12b73cc4":"O XGBoost apresentou uma melhora um pouco maior no modelo, contudo n\u00e3o foi significante.","105274b2":"Observa-se que Dataset possui 5960 linhase  13 colunas, sendo apenas as colunas 'JOB' e 'REASON' com valores n\u00e3o n\u00famericos","42692972":"A vari\u00e1vel raz\u00e3o do d\u00e9bito apresentou valores pr\u00f3ximos para as duas categorias, o que n\u00e3o d\u00e1 muita informa\u00e7\u00e3o ao modelo","30aab4bb":"As Florestas aleat\u00f3rias ou florestas de decis\u00e3o aleat\u00f3ria s\u00e3o modelos ensemble das DecisionTree, que utilizam um m\u00e9todo de aprendizado conjunto para classifica\u00e7\u00e3o,regress\u00e3o e outras tarefas que operam construindo uma infinidade de \u00e1rvores de decis\u00e3o no momento do treinamento e gerando a classe que \u00e9 o modo das classes ou a previs\u00e3o m\u00e9dia das \u00e1rvores individuais.","779ed4e3":"Se observa que, os devedores(BAD)  em m\u00e9dia fizeram empr\u00e9stimos de 19.260,00, contudo sua d\u00edvida em m\u00e9dia est\u00e1 em $ 73.864,00.","f903032e":"### Importando o Dataset ","f7ec2205":"## M\u00e9tricas de Avalia\u00e7\u00e3o\n\nPara a avalia\u00e7\u00e3o do modelo ser\u00e3o utilizadas duas m\u00e9tricas,sendo a Accuracy(Acur\u00e1cia) e o F1 Score, melhor detalhados abaixo:\n\n<img src=\"https:\/\/miro.medium.com\/max\/1000\/1*t1vf-ofJrJqtmam0KSn3EQ.png\" width=\"250px\"\/>\n\nA **Accuracy** mede a performance do modelo como um todo, contudo n\u00e3o \u00e9 uma m\u00e9trica interessante em situa\u00e7\u00f5es de bases muito desbalanceadas.\n\nA **Precision** \u00e9 importante quando os Falsos Positivos s\u00e3o considerados mais prejudiciais que os Falsos Negativos. Sendo uma m\u00e9trica interessante para o modelo em an\u00e1lise caso o apetite \u00e0 risco do Banco seja baixo, logo se para o Banco acertar na predi\u00e7\u00e3o dos maus pagadores seja mais importante que acabar deixando de emprestar para algum bom pagador que o modelo etiquetou errado.\n\nO **Recall**, ao contr\u00e1rio,  pode ser usada em situa\u00e7\u00f5es em que os Falsos Negativos s\u00e3o mais prejudiciais que os Falso Positivos. Nesse sentido, o foco seria ter mais produtos financiamentos aprovados, assim, o Banco sofre mais deixando de vender para os bons pagadores do que aceitando um mau pagador etiquetado como bom.\n\nO **F1 Score** \u00e9 uma m\u00e9dia harm\u00f4nica entre de Precision e Recall, portanto, quando tem-se um F1-Score baixo, \u00e9 um indicativo de que ou a precis\u00e3o ou o recall est\u00e1 baixo.\n\n\n\n\n","cb704c12":"Para a an\u00e1lise inicial, h\u00e1 a biblioteca Pandas Profiling que gera um report com an\u00e1lise de todas os campos e suas estat\u00edsticas. Por meio deste relat\u00f3rio \u00e9 poss\u00edvel ter uma no\u00e7\u00e3o das distribui\u00e7oes dos dados e correla\u00e7\u00f5es","e6b461d9":"A t\u00edtulo de demonstra\u00e7\u00e3o da correla\u00e7\u00e3o entre os valores, o gr\u00e1fico acima explicita que um n\u00famero alto de linhas de cr\u00e9dito inadimplentes tem rela\u00e7\u00e3o positiva com maus pagadores.","04f8a0ca":"## Modelo XGBoost\n\nO XGBoost \u00e9 uma implementa\u00e7\u00e3o de \u00e1rvores de decis\u00e3o aprimoradas por gradiente, projetadas para velocidade e desempenho.Sua sigla significa eXtreme Gradient Boosting, e sua vantagem \u00e9 devida a uma implementa\u00e7\u00e3o de m\u00e1quinas de aumento de gradiente."}}