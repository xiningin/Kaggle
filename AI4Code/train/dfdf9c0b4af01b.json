{"cell_type":{"62e1890f":"code","8644735b":"code","89af7b4c":"code","c283481f":"code","8ca9eaf0":"code","2af80b96":"code","c1077488":"code","cf7eb9ec":"code","54c02660":"code","fd449975":"code","3b93235d":"code","486acbf2":"code","d5fe80b2":"code","847cb35b":"code","95e1b4f2":"code","b126bb6a":"code","92a4fdb3":"code","7702ed65":"code","eca329a7":"code","a2e5f089":"markdown","7e0a4ad5":"markdown","75d95465":"markdown","5aaf38e3":"markdown","fcfc1d78":"markdown","53b3a3f5":"markdown","c5be7c93":"markdown","44bdf4de":"markdown","34e740e1":"markdown","be9b2e0b":"markdown","8c7a1eb7":"markdown","6a076f52":"markdown","32354a47":"markdown","bef271b4":"markdown","9ae2d4ff":"markdown","81146e0f":"markdown","f04eee98":"markdown","f96fe47f":"markdown","db0fda48":"markdown","b11e5521":"markdown"},"source":{"62e1890f":"import numpy as np\nimport pandas as pd\nimport lightgbm as lgb\nimport graphviz as graphviz\nimport shap\n\ntrain = pd.read_csv('..\/input\/titanic\/train.csv')\n\n# Features to be created (for some of the ideas see: https:\/\/www.kaggle.com\/gunesevitan\/titanic-advanced-feature-engineering-tutorial\/comments)\ntrain['FamilyMembers'] = train['SibSp'] + train['Parch']\ntrain['Female'] = train['Sex'].map({'male': 0, 'female': 1}).astype(np.int8)\ntrain['Pclass_cat'] = (train['Pclass']-1).astype(np.int8)\ntrain['Adult'] = (train['Age']>16).astype(np.int8)\ntrain['Adult'].values[train['Age'].isna()] = 1 # Looking at titles of people, I suspect those with missing age are mostly adult\n# Imputation of age\ntrain['Age'] = train.groupby(['Pclass', 'Female'])['Age'].transform(lambda x: x.fillna(x.median()))\n","8644735b":"new_data = pd.DataFrame({'Person': ['Rose', 'Jack'], 'Pclass': [1, 3], 'Age': [17, 20], 'FamilyMembers': [1,0], 'Female': [1, 0]})\nnew_data","89af7b4c":"# Lists of features to be used\ncontinuous_features = ['Pclass', 'Age', 'FamilyMembers']\ndiscrete_features = ['Female']\nmodel_columns = discrete_features + continuous_features\nids_of_categorical = [0]\n\n\nX = np.array( train[model_columns] )    \ny = np.array( train['Survived'] ).flatten()\ndtrain = lgb.Dataset(X, label=y)\n\nparams = {\n    \"objective\": \"binary\",\n    \"metric\": \"binary_error\",\n    \"verbosity\": -1,\n    \"boosting_type\": \"gbdt\",\n    \"seed\": 538,\n    \"learning_rate\": 1,\n    'num_leaves': 4, \n    'feature_fraction': 1.0, \n    'bagging_fraction': 1.0, \n    'bagging_freq': 15, \n    'min_child_samples': 5} \n\n\nlgbfit = lgb.train(params,\n                   dtrain,\n                   categorical_feature=ids_of_categorical,\n                   verbose_eval=True,                                      \n                   num_boost_round=3)","c283481f":"def get_a_graph(tree_index=0):\n    gv1 = lgb.create_tree_digraph(lgbfit, tree_index=tree_index, show_info='data_percentage') # orientation='vertical', \n    gv1s = gv1.source.replace('Column_0', 'Female').replace('Column_1', 'Pclass').replace('Column_2', 'Age').replace('Column_3', 'FamilyMembers')\n    #graphviz.digraph.render(gv1s, format='pdf', filepath='Digraph.gv')\n    graph = graphviz.Source(gv1s)\n    graph.render('dtree_render',view=True)\n    return graph","8ca9eaf0":"get_a_graph(tree_index=0)","2af80b96":"get_a_graph(tree_index=1)","c1077488":"get_a_graph(tree_index=2)","cf7eb9ec":"predictions = pd.DataFrame({'Person': ['Rose', 'Jack'], 'Log-odds': [3.362, -1.861]})\npredictions['Probability of survival'] = np.exp(predictions['Log-odds']) \/ ( 1 + np.exp(predictions['Log-odds']) )\npredictions","54c02660":"lgbfit.predict(np.array( new_data[model_columns] ))","fd449975":"alldata = pd.concat( [train[model_columns], new_data[model_columns] ]).reset_index(drop=True)\nexplainer = shap.TreeExplainer(lgbfit)\nshap_values = explainer.shap_values(alldata)","3b93235d":"print(explainer.expected_value)\nprint(np.exp(explainer.expected_value)\/(1+np.exp(explainer.expected_value)))","486acbf2":"shap.initjs()\nshap.force_plot(explainer.expected_value[1], shap_values[1][891], alldata.iloc[891])","d5fe80b2":"shap.force_plot(explainer.expected_value[1], shap_values[1][892], alldata.iloc[892])","847cb35b":"shap.force_plot(explainer.expected_value[1], shap_values[1], alldata[model_columns])","95e1b4f2":"shap.dependence_plot(\"Pclass\", shap_values[1], alldata[model_columns])","b126bb6a":"shap.dependence_plot(\"Female\", shap_values[1], alldata[model_columns])","92a4fdb3":"shap.dependence_plot(\"Age\", shap_values[1], alldata[model_columns])","7702ed65":"shap.dependence_plot(\"FamilyMembers\", shap_values[1], alldata[model_columns])","eca329a7":"shap.summary_plot(shap_values[1], alldata[model_columns])","a2e5f089":"We can also look at each feature\n* We can notice that our model suggests that in third class females might have been less likely to survive than models.\n* The influence of being very young is biggest for young boys (i.e. it raises their probability a lot above other males)\n* Having a lot of family members negatively affects the predicted probability of survival, but this is a feature that is heavily correlated with passenger class.","7e0a4ad5":"The SHAP values suggest that for Rose the probability of survbival is increased to higher than 38% primarily because she is female and a first class passenger, while other features matter less (but are all positives, too).","75d95465":"Thus, the predictions for these two passengers are:\n* Rose: the sum of scores is 1.908 + 0.952 + 0.502 = 3.362\n* Jack: -1.385 - 0.280 - 0.196 = \u22121.861\n\nSo, what do these log-odds translate to in terms of probabilities?","5aaf38e3":"# What does the fitted model looks like? What are its predictions for Jack and Rose?\n\nThis gets to the heart of this notebook. What is a LightGBM model, really? What does it look like?\n\nWell, let's plot the model we just created!","fcfc1d78":"# Explanations that are less about an individual prediction, but describe the model more globally\n\nThese were, so far, local explanations for the predictions given for a specific person. However, you can also examine SHAP values globally.","53b3a3f5":"When I say that a factor pushes the predictions, I do not imply that this is a causal effect. For example, one predictor we did not use was the fare a person paid. Obviously, you do not die as a direct cause of how much you paid. In fact, you'd probably be just like any other first class passenger, if you somehow were a first class passenger that negotiated a fantastic deal (not sure that was even an option). The reason the model attributes some of a decreased probability of survival to a lowered fare is that that variable is essentially capture the same thing as passenger class. Passenger class, was then of course something that affected the ship's crews' efforts to save passengers. Note also that the explanations depend on the training data used to create them. Once we go to a very different data distribution, odd things may happen including [adverserial examples](https:\/\/ui.adsabs.harvard.edu\/abs\/2019arXiv191102508S\/abstract) (where we fool SHAP into giving inappropriate explanations).\n\nHow did these SHAP values get calculated? We look at the average change in predictions due to a predictor across all possible orders in which we could consider the predictors. For LightGBM (or xgboost), there are efficient algorithms for doing this, as implemented in the [shap Python package](https:\/\/github.com\/slundberg\/shap), but the package also supports generic models and has some specific tools for deep learning (e.g. vision applications). The SHAP approach has a game theoretic basis and comes from the work on assigning credit to players in cooperative multi-player games by Lloyd S. Shapley in the 1950s, for which he receiver the Nobel prize in economic sciences in 2012.","c5be7c93":"In the final tree, Reose ends up in leaf 3 (she is female, but not aged 11.5 years or younger). This gets her a score of +0.502.\n\nJack is not female, but aged below 52.5 years, so he gets a score of -0.196.","44bdf4de":"Here are the new data, for which we wish to make a prediction:","34e740e1":"# How were these trees created in the first place?\n\nWe build one tree at a time. So, first the first one, then the second one etc., and each time we try to \"fix\" what the previous one still got wrong in terms of predicting the training data. We set the hyperparameters that you saw earlier to control exactly how this is done. Obviously, there's a risk that we end up with way too simple a model (as we did with our example above) or a completely overfit model that reacts to pure chance differences in the training data. To avoid that, we would typically look at what hyperparameter choices perform best in a validation, where we imitate the prediction task. I.e. we take some of our data as training data and reserve some other data as \"validation\" data to see how well we end up predicting it. There's lots of ways of doing this, but one of the most popular ones is cross-validation. I explain more on this in [another notebook](https:\/\/www.kaggle.com\/bjoernholzhauer\/lightgbm-tuning-with-optuna).\n\nNote that LightGBM (and similar models like xgboost) will never create a branch in a tree for predictor values outside of the data range. So it does not really extrapolate beyond the training data and predictions are constant beyond the range of features in the training data. It also tends to - unlike e.g. logistric regression - lead to predicted probabilities that are not well calibrated.","be9b2e0b":"For each tree, you follow the branches based on the characteristics of a person.\n\nSo, in the first tree Rose is female, in passenger class 1, so she ends up in the leaf node 0 (top node). This gets her a score of +1.908. In case you are wondering what this score means, it is an additive terms on the log-odds scale. Positive values are \"good\" (i.e. increase the predicted probability of survival) and negative values are \"bad\". We will add up these scores for each person across the three trees we created and then convert the resulting log-odds into a \"probability\". I put the quotation marks around the word probabilitiy, because it is sort of debatable whether that's what LightGBM provides. Sure, it's a number between 0 and 1, but it's not usually very well calibrated (i.e. if I tell you \"These 100 people have a probability of survival of 95%\", then you would not actually expect 95 of the 100 to survive - which is awkward). I will drop the quotation marks from here onwards, but just wanted to highlight this at least once.\n    \nIn contrast, for Jack, who is not female and not aged below 6.5 years, we end up in leaf 3, which gets him a score of -1.385 (negative scores are \"bad\").","8c7a1eb7":"# Explaining the predictions\n\nWith just 3 trees, going through the trees and seeing what was going on was easy to follow. It would be harder to explain the predictions for Jack and Rose for a larger set of deeper trees. And one would use more and deeper trees in practice, in order to capture non-linear effects and high-dimensional interactions, which are the key strengths of LightGBM and why it is so popular in machine learning competitions and in practice.\n\nOne way of explaining LightGBM predictions are SHapley Additive exPlanations - or SHAP values. They describe what pushes predictions away from a base value.\n\nLet's use the great [shap package](https:\/\/github.com\/slundberg\/shap) to create a tree explainer.","6a076f52":"As we can see, this translates into a predicted 97% probability of survival for Rose and 13% for Jack. \n\nYou may have noticed the function $f(x) = \\frac{exp(x)}{1 + exp(x)}$ or equivalently $f(x) = \\frac{1}{1 + exp(-x)}$ that we used to convert the log-odds to probabilities. This is the sigmoid function and it is the inverse of the logit function $\\log(y) - \\log(1-y)$. You may wonder why we worked on this log-odds scale in the first place. The reason is that this ensure that we can just add up numbers across trees and any number from $(-\\infty, \\infty)$ maps to a probability in $(0, 1)$. If we worked with probabilities or the logarithm of probabilities that just would not work.","32354a47":"# Some references and great links\n\n* [Video by S. Lundberg](https:\/\/youtu.be\/ngOBhhINWb8) on SHAP\n* Lundberg, S.M. and Lee, S.I., 2017. A unified approach to interpreting model predictions. In Advances in neural information processing systems (pp. 4765-4774). http:\/\/papers.nips.cc\/paper\/7062-a-unified-approach-to-interpreting-model-predictions\n* Lundberg, S.M., Erion, G., Chen, H., DeGrave, A., Prutkin, J.M., Nair, B., Katz, R., Himmelfarb, J., Bansal, N. and Lee, S.I., 2020. From local explanations to global understanding with explainable AI for trees. Nature machine intelligence, 2(1), pp.56-67. https:\/\/rdcu.be\/b0z70\n* Lundberg, S.M., Nair, B., Vavilala, M.S., Horibe, M., Eisses, M.J., Adams, T., Liston, D.E., Low, D.K.W., Newman, S.F., Kim, J. and Lee, S.I., 2018. Explainable machine-learning predictions for the prevention of hypoxaemia during surgery. Nature biomedical engineering, 2(10), pp.749-760. https:\/\/rdcu.be\/baVbR\n* SHAP Python package (page gives nice example plots): https:\/\/github.com\/slundberg\/shap\n* Slack, D., Hilgard, S., Jia, E., Singh, S. and Lakkaraju, H., 2020, February. [Fooling lime and shap: Adversarial attacks on post hoc explanation methods](https:\/\/arxiv.org\/abs\/1911.02508). In Proceedings of the AAAI\/ACM Conference on AI, Ethics, and Society (pp. 180-186).\n* [Chapter 5](https:\/\/christophm.github.io\/interpretable-ml-book\/shap.html) of Christoph Molnar's \"Interpretable Machine Learning - A Guide for Making Black Box Models Explainable\"\n* [LightGBM](https:\/\/github.com\/microsoft\/LightGBM)\n* Chen, T. and Guestrin, C., 2016, August. Xgboost: A scalable tree boosting system. In Proceedings of the 22nd ACM SIGKDD International Conference on knowledge discovery and data mining (pp. 785-794). doi:10.1145\/2939672.2939785 https:\/\/arxiv.org\/abs\/1603.02754\n* Other highly rated SHAP notebooks with a decent amount of explanations: https:\/\/www.kaggle.com\/dansbecker\/shap-values, https:\/\/www.kaggle.com\/dansbecker\/advanced-uses-of-shap-values, https:\/\/www.kaggle.com\/cast42\/lightgbm-model-explained-by-shap\n* Lloyd S. Shapley facts at nobelprize.org\n* And, just in case you thought a perfectly explainable and unbiased model is automatically ethical, refer to this modern classic: Keyes, O., Hutson, J. and Durbin, M., 2019, May. [A mulching proposal: Analysing and improving an algorithmic system for turning the elderly into high-nutrient slurry](https:\/\/arxiv.org\/abs\/1908.06166). In Extended Abstracts of the 2019 CHI Conference on Human Factors in Computing Systems (pp. 1-11).","bef271b4":"In the second tree, Rose ends up in leaf 0 (she is a passenger in `Pclass == 1`), so she gets a score of +0.952 from this tree.\n\nJack is not a passenger in first class, has fewer than 3.5 family members on board, and is not aged 13.5 years or below. Thus, he ends up in leaf node 3 (score of -0.280).","9ae2d4ff":"# Double-checking that our manual calculation matches the model predictions\n\nAs we can see below, up to some rounding issues in some of the later decimal places, our manual calculation for each tree exactly matches the predictions we get using the `predict` function of LightGBM.","81146e0f":"# Our data\n\nLuckily, we have on Kaggle the data from this data, which should be directly relevant to making predictions for this question - thinks might be much more complicated if the data were from a very different time period or culture. However, note that our data of course have some limitations, there is only limited information on each passenger. For example, we do not know, which passengers may have had a muderous fianc\u00e9 or romantic rival on board.\n\nIn fact, we will not even use all the available features to construct our model, just a subset of four features.","f04eee98":"# A whirlwind tour of LightGBM and SHAP explanations: Will Rose or Jack survive?\n\nThis notebook aims to illustrate what [LightGBM](https:\/\/lightgbm.readthedocs.io\/en\/latest\/) models look like and how we can explain their predictions using [SHAP](https:\/\/github.com\/slundberg\/shap) (SHapley Additive exPlanations) values. All the discussion is also valid for [xgboost](https:\/\/xgboost.readthedocs.io\/en\/latest\/). I have tried very hard to keep this very approachable and non-technical. Let me know, if you feel I oversimplified too much. At the end of the notebook, I have added some links and references I'd recommend as further reading (depending on what you want).\n\nTo illustrate everything, we will use the example of two hypothetical passengers of the Titanic call \"Jack\" and \"Rose\", for which we will generate predictions and try to explain these predictions using SHAP.\n![jack%20and%20rose.jpg](attachment:jack%20and%20rose.jpg)\n*Image generated using [deepart.io](https:\/\/deepart.io\/) using neural style transfer.*\n\nSo, which of these two people is more likely to survive the sinking of the Titanic?","f96fe47f":"# Let's fit a LightGBM model\n\nLet's be clear, this section does not try to build a good LightGBM model (in fact, for that have a look at [another notebook of mine](https:\/\/www.kaggle.com\/bjoernholzhauer\/lightgbm-tuning-with-optuna)). Below is the code to fit the model that we will use in the rest of the notebook. However, this is just a toy model and does not constitute good machine learning practices. \n\n**Disclaimer:** We are creating a very simple model that does not, in any way, constitute a good predictive model. **WARNING: Do not try this at home!** The hyper-parameters for our example were not chosen to produce a good model. Instead, they were picked so that we have a small number of easy to inspect shallow trees.","db0fda48":"Now, let's look at Jack. His probability is lowered from the 38% to 13% (-1.86 on the log-odds scale), primarily because he is male and a 3rd class passenger. It is also a slight negative that he is aged 20 (i.e. not a child). That's approximately counter-balanced by him not having any family members onboard.","b11e5521":"They force plots we create below suggest what pushes explanations away froma base value, here a 38% probability of survival (-0.476 on the log-odds scale) or 62% probability of death (0.476 on the log-odds scale)."}}