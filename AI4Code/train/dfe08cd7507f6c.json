{"cell_type":{"279a5147":"code","ebee8323":"code","585ffba3":"code","6dbb4c5c":"code","c559b2e0":"code","ee9c14cf":"code","1e595c44":"code","bb523334":"code","8eb8f9f3":"code","11c7bb4c":"code","987418b4":"code","2ea660a2":"code","5b245219":"code","7c68ca3c":"code","ccfdd532":"code","e7e49a33":"code","e435bf54":"code","d25b4a1d":"code","bb2ab46b":"code","45e97b95":"code","8bfca587":"code","06b383a9":"code","021a3b14":"code","c528f50d":"code","4abb26f1":"code","e31bfde1":"code","06cf760b":"markdown"},"source":{"279a5147":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ebee8323":"import torchvision.transforms as transforms\nfrom torch.utils.data.sampler import SubsetRandomSampler\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.optim import lr_scheduler\nfrom torch.utils.data import TensorDataset, DataLoader, Dataset\nimport torchvision\nfrom torchvision import models\nimport torch.optim as optim\nimport pandas as pd\nimport numpy as np\nimport cv2\nimport os\nfrom sklearn import preprocessing\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport time\nfrom tqdm import tqdm","585ffba3":"from pathlib import Path\nimport urllib\n\nimport os\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image\nimport matplotlib.pyplot as plt","6dbb4c5c":"input_root_dir = \"..\/input\/food-101\/food-101\/food-101\"\ninput_root_path = Path(input_root_dir)\nprint(os.listdir(input_root_dir))\nimage_dir_path = input_root_path\/'images'","c559b2e0":"!cat {input_root_dir}\/README.txt","ee9c14cf":"class_path = input_root_dir+'\/meta\/classes.txt'\ntrain_img_name_path = input_root_dir+'\/meta\/train.txt'\ntest_img_name_path = input_root_dir+'\/meta\/test.txt'\n","1e595c44":"def file2list(path):\n    file1 = open(path,'r')\n    lines = file1.readlines()\n    final_list = [line.strip() for line in lines]\n    return final_list","bb523334":"classes = file2list(class_path)\ntrain_data = file2list(train_img_name_path)\ntest_data = file2list(test_img_name_path)\nle = preprocessing.LabelEncoder()\ntargets = le.fit_transform(classes)","8eb8f9f3":"class FoodData(Dataset):\n    def __init__(self,img_path,img_dir,size,transform=None):\n        self.img_path = img_path\n        self.img_dir = img_dir\n        self.transform = transform\n        self.size = size\n#         self.mode = mode\n        \n    def __len__(self):\n        return len(self.img_path)\n    \n    def __getitem__(self,index):\n        label,img_name = self.img_path[index].split('\/')\n        path = self.img_dir+'\/images\/'+label+'\/'+img_name+'.jpg'\n        img = cv2.imread(path)\n        img = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n        img = cv2.resize(img,(self.size,self.size))\n        if self.transform:\n            img = self.transform(img)\n        return {\n                'gt': img,\n                'label': torch.tensor(le.transform([label])[0])\n            }\n        ","11c7bb4c":"class Cutout(object):\n    \"\"\"Randomly mask out one or more patches from an image.\n    Args:\n        n_holes (int): Number of patches to cut out of each image.\n        length (int): The length (in pixels) of each square patch.\n    \"\"\"\n    def __init__(self, n_holes, length):\n        self.n_holes = n_holes\n        self.length = length\n\n    def __call__(self, img):\n        \"\"\"\n        Args:\n            img (Tensor): Tensor image of size (C, H, W).\n        Returns:\n            Tensor: Image with n_holes of dimension length x length cut out of it.\n        \"\"\"\n        h = img.size(1)\n        w = img.size(2)\n\n        mask = np.ones((h, w), np.float32)\n\n        for n in range(self.n_holes):\n            y = np.random.randint(h)\n            x = np.random.randint(w)\n\n            y1 = np.clip(y - self.length \/\/ 2, 0, h)\n            y2 = np.clip(y + self.length \/\/ 2, 0, h)\n            x1 = np.clip(x - self.length \/\/ 2, 0, w)\n            x2 = np.clip(x + self.length \/\/ 2, 0, w)\n\n            mask[y1: y2, x1: x2] = 0.\n\n        mask = torch.from_numpy(mask)\n        mask = mask.expand_as(img)\n        img = img * mask\n\n        return img","987418b4":"from PIL import Image, ImageEnhance, ImageOps\nimport numpy as np\nimport random\n\n\nclass ImageNetPolicy(object):\n    \"\"\" Randomly choose one of the best 24 Sub-policies on ImageNet.\n        Example:\n        >>> policy = ImageNetPolicy()\n        >>> transformed = policy(image)\n        Example as a PyTorch Transform:\n        >>> transform=transforms.Compose([\n        >>>     transforms.Resize(256),\n        >>>     ImageNetPolicy(),\n        >>>     transforms.ToTensor()])\n    \"\"\"\n    def __init__(self, fillcolor=(128, 128, 128)):\n        self.policies = [\n            SubPolicy(0.4, \"posterize\", 8, 0.6, \"rotate\", 9, fillcolor),\n            SubPolicy(0.6, \"solarize\", 5, 0.6, \"autocontrast\", 5, fillcolor),\n            SubPolicy(0.8, \"equalize\", 8, 0.6, \"equalize\", 3, fillcolor),\n            SubPolicy(0.6, \"posterize\", 7, 0.6, \"posterize\", 6, fillcolor),\n            SubPolicy(0.4, \"equalize\", 7, 0.2, \"solarize\", 4, fillcolor),\n\n            SubPolicy(0.4, \"equalize\", 4, 0.8, \"rotate\", 8, fillcolor),\n            SubPolicy(0.6, \"solarize\", 3, 0.6, \"equalize\", 7, fillcolor),\n            SubPolicy(0.8, \"posterize\", 5, 1.0, \"equalize\", 2, fillcolor),\n            SubPolicy(0.2, \"rotate\", 3, 0.6, \"solarize\", 8, fillcolor),\n            SubPolicy(0.6, \"equalize\", 8, 0.4, \"posterize\", 6, fillcolor),\n\n            SubPolicy(0.8, \"rotate\", 8, 0.4, \"color\", 0, fillcolor),\n            SubPolicy(0.4, \"rotate\", 9, 0.6, \"equalize\", 2, fillcolor),\n            SubPolicy(0.0, \"equalize\", 7, 0.8, \"equalize\", 8, fillcolor),\n            SubPolicy(0.6, \"invert\", 4, 1.0, \"equalize\", 8, fillcolor),\n            SubPolicy(0.6, \"color\", 4, 1.0, \"contrast\", 8, fillcolor),\n\n            SubPolicy(0.8, \"rotate\", 8, 1.0, \"color\", 2, fillcolor),\n            SubPolicy(0.8, \"color\", 8, 0.8, \"solarize\", 7, fillcolor),\n            SubPolicy(0.4, \"sharpness\", 7, 0.6, \"invert\", 8, fillcolor),\n            SubPolicy(0.6, \"shearX\", 5, 1.0, \"equalize\", 9, fillcolor),\n            SubPolicy(0.4, \"color\", 0, 0.6, \"equalize\", 3, fillcolor),\n\n            SubPolicy(0.4, \"equalize\", 7, 0.2, \"solarize\", 4, fillcolor),\n            SubPolicy(0.6, \"solarize\", 5, 0.6, \"autocontrast\", 5, fillcolor),\n            SubPolicy(0.6, \"invert\", 4, 1.0, \"equalize\", 8, fillcolor),\n            SubPolicy(0.6, \"color\", 4, 1.0, \"contrast\", 8, fillcolor),\n            SubPolicy(0.8, \"equalize\", 8, 0.6, \"equalize\", 3, fillcolor)\n        ]\n\n\n    def __call__(self, img):\n        policy_idx = random.randint(0, len(self.policies) - 1)\n        return self.policies[policy_idx](img)\n\n    def __repr__(self):\n        return \"AutoAugment ImageNet Policy\"\n\n\nclass CIFAR10Policy(object):\n    \"\"\" Randomly choose one of the best 25 Sub-policies on CIFAR10.\n        Example:\n        >>> policy = CIFAR10Policy()\n        >>> transformed = policy(image)\n        Example as a PyTorch Transform:\n        >>> transform=transforms.Compose([\n        >>>     transforms.Resize(256),\n        >>>     CIFAR10Policy(),\n        >>>     transforms.ToTensor()])\n    \"\"\"\n    def __init__(self, fillcolor=(128, 128, 128)):\n        self.policies = [\n            SubPolicy(0.1, \"invert\", 7, 0.2, \"contrast\", 6, fillcolor),\n            SubPolicy(0.7, \"rotate\", 2, 0.3, \"translateX\", 9, fillcolor),\n            SubPolicy(0.8, \"sharpness\", 1, 0.9, \"sharpness\", 3, fillcolor),\n            SubPolicy(0.5, \"shearY\", 8, 0.7, \"translateY\", 9, fillcolor),\n            SubPolicy(0.5, \"autocontrast\", 8, 0.9, \"equalize\", 2, fillcolor),\n\n            SubPolicy(0.2, \"shearY\", 7, 0.3, \"posterize\", 7, fillcolor),\n            SubPolicy(0.4, \"color\", 3, 0.6, \"brightness\", 7, fillcolor),\n            SubPolicy(0.3, \"sharpness\", 9, 0.7, \"brightness\", 9, fillcolor),\n            SubPolicy(0.6, \"equalize\", 5, 0.5, \"equalize\", 1, fillcolor),\n            SubPolicy(0.6, \"contrast\", 7, 0.6, \"sharpness\", 5, fillcolor),\n\n            SubPolicy(0.7, \"color\", 7, 0.5, \"translateX\", 8, fillcolor),\n            SubPolicy(0.3, \"equalize\", 7, 0.4, \"autocontrast\", 8, fillcolor),\n            SubPolicy(0.4, \"translateY\", 3, 0.2, \"sharpness\", 6, fillcolor),\n            SubPolicy(0.9, \"brightness\", 6, 0.2, \"color\", 8, fillcolor),\n            SubPolicy(0.5, \"solarize\", 2, 0.0, \"invert\", 3, fillcolor),\n\n            SubPolicy(0.2, \"equalize\", 0, 0.6, \"autocontrast\", 0, fillcolor),\n            SubPolicy(0.2, \"equalize\", 8, 0.6, \"equalize\", 4, fillcolor),\n            SubPolicy(0.9, \"color\", 9, 0.6, \"equalize\", 6, fillcolor),\n            SubPolicy(0.8, \"autocontrast\", 4, 0.2, \"solarize\", 8, fillcolor),\n            SubPolicy(0.1, \"brightness\", 3, 0.7, \"color\", 0, fillcolor),\n\n            SubPolicy(0.4, \"solarize\", 5, 0.9, \"autocontrast\", 3, fillcolor),\n            SubPolicy(0.9, \"translateY\", 9, 0.7, \"translateY\", 9, fillcolor),\n            SubPolicy(0.9, \"autocontrast\", 2, 0.8, \"solarize\", 3, fillcolor),\n            SubPolicy(0.8, \"equalize\", 8, 0.1, \"invert\", 3, fillcolor),\n            SubPolicy(0.7, \"translateY\", 9, 0.9, \"autocontrast\", 1, fillcolor)\n        ]\n\n\n    def __call__(self, img):\n        policy_idx = random.randint(0, len(self.policies) - 1)\n        return self.policies[policy_idx](img)\n\n    def __repr__(self):\n        return \"AutoAugment CIFAR10 Policy\"\n\n\nclass SVHNPolicy(object):\n    \"\"\" Randomly choose one of the best 25 Sub-policies on SVHN.\n        Example:\n        >>> policy = SVHNPolicy()\n        >>> transformed = policy(image)\n        Example as a PyTorch Transform:\n        >>> transform=transforms.Compose([\n        >>>     transforms.Resize(256),\n        >>>     SVHNPolicy(),\n        >>>     transforms.ToTensor()])\n    \"\"\"\n    def __init__(self, fillcolor=(128, 128, 128)):\n        self.policies = [\n            SubPolicy(0.9, \"shearX\", 4, 0.2, \"invert\", 3, fillcolor),\n            SubPolicy(0.9, \"shearY\", 8, 0.7, \"invert\", 5, fillcolor),\n            SubPolicy(0.6, \"equalize\", 5, 0.6, \"solarize\", 6, fillcolor),\n            SubPolicy(0.9, \"invert\", 3, 0.6, \"equalize\", 3, fillcolor),\n            SubPolicy(0.6, \"equalize\", 1, 0.9, \"rotate\", 3, fillcolor),\n\n            SubPolicy(0.9, \"shearX\", 4, 0.8, \"autocontrast\", 3, fillcolor),\n            SubPolicy(0.9, \"shearY\", 8, 0.4, \"invert\", 5, fillcolor),\n            SubPolicy(0.9, \"shearY\", 5, 0.2, \"solarize\", 6, fillcolor),\n            SubPolicy(0.9, \"invert\", 6, 0.8, \"autocontrast\", 1, fillcolor),\n            SubPolicy(0.6, \"equalize\", 3, 0.9, \"rotate\", 3, fillcolor),\n\n            SubPolicy(0.9, \"shearX\", 4, 0.3, \"solarize\", 3, fillcolor),\n            SubPolicy(0.8, \"shearY\", 8, 0.7, \"invert\", 4, fillcolor),\n            SubPolicy(0.9, \"equalize\", 5, 0.6, \"translateY\", 6, fillcolor),\n            SubPolicy(0.9, \"invert\", 4, 0.6, \"equalize\", 7, fillcolor),\n            SubPolicy(0.3, \"contrast\", 3, 0.8, \"rotate\", 4, fillcolor),\n\n            SubPolicy(0.8, \"invert\", 5, 0.0, \"translateY\", 2, fillcolor),\n            SubPolicy(0.7, \"shearY\", 6, 0.4, \"solarize\", 8, fillcolor),\n            SubPolicy(0.6, \"invert\", 4, 0.8, \"rotate\", 4, fillcolor),\n            SubPolicy(0.3, \"shearY\", 7, 0.9, \"translateX\", 3, fillcolor),\n            SubPolicy(0.1, \"shearX\", 6, 0.6, \"invert\", 5, fillcolor),\n\n            SubPolicy(0.7, \"solarize\", 2, 0.6, \"translateY\", 7, fillcolor),\n            SubPolicy(0.8, \"shearY\", 4, 0.8, \"invert\", 8, fillcolor),\n            SubPolicy(0.7, \"shearX\", 9, 0.8, \"translateY\", 3, fillcolor),\n            SubPolicy(0.8, \"shearY\", 5, 0.7, \"autocontrast\", 3, fillcolor),\n            SubPolicy(0.7, \"shearX\", 2, 0.1, \"invert\", 5, fillcolor)\n        ]\n\n\n    def __call__(self, img):\n        policy_idx = random.randint(0, len(self.policies) - 1)\n        return self.policies[policy_idx](img)\n\n    def __repr__(self):\n        return \"AutoAugment SVHN Policy\"\n\n\nclass SubPolicy(object):\n    def __init__(self, p1, operation1, magnitude_idx1, p2, operation2, magnitude_idx2, fillcolor=(128, 128, 128)):\n        ranges = {\n            \"shearX\": np.linspace(0, 0.3, 10),\n            \"shearY\": np.linspace(0, 0.3, 10),\n            \"translateX\": np.linspace(0, 150 \/ 331, 10),\n            \"translateY\": np.linspace(0, 150 \/ 331, 10),\n            \"rotate\": np.linspace(0, 30, 10),\n            \"color\": np.linspace(0.0, 0.9, 10),\n            \"posterize\": np.round(np.linspace(8, 4, 10), 0).astype(np.int),\n            \"solarize\": np.linspace(256, 0, 10),\n            \"contrast\": np.linspace(0.0, 0.9, 10),\n            \"sharpness\": np.linspace(0.0, 0.9, 10),\n            \"brightness\": np.linspace(0.0, 0.9, 10),\n            \"autocontrast\": [0] * 10,\n            \"equalize\": [0] * 10,\n            \"invert\": [0] * 10\n        }\n\n        # from https:\/\/stackoverflow.com\/questions\/5252170\/specify-image-filling-color-when-rotating-in-python-with-pil-and-setting-expand\n        def rotate_with_fill(img, magnitude):\n            rot = img.convert(\"RGBA\").rotate(magnitude)\n            return Image.composite(rot, Image.new(\"RGBA\", rot.size, (128,) * 4), rot).convert(img.mode)\n\n        func = {\n            \"shearX\": lambda img, magnitude: img.transform(\n                img.size, Image.AFFINE, (1, magnitude * random.choice([-1, 1]), 0, 0, 1, 0),\n                Image.BICUBIC, fillcolor=fillcolor),\n            \"shearY\": lambda img, magnitude: img.transform(\n                img.size, Image.AFFINE, (1, 0, 0, magnitude * random.choice([-1, 1]), 1, 0),\n                Image.BICUBIC, fillcolor=fillcolor),\n            \"translateX\": lambda img, magnitude: img.transform(\n                img.size, Image.AFFINE, (1, 0, magnitude * img.size[0] * random.choice([-1, 1]), 0, 1, 0),\n                fillcolor=fillcolor),\n            \"translateY\": lambda img, magnitude: img.transform(\n                img.size, Image.AFFINE, (1, 0, 0, 0, 1, magnitude * img.size[1] * random.choice([-1, 1])),\n                fillcolor=fillcolor),\n            \"rotate\": lambda img, magnitude: rotate_with_fill(img, magnitude),\n            \"color\": lambda img, magnitude: ImageEnhance.Color(img).enhance(1 + magnitude * random.choice([-1, 1])),\n            \"posterize\": lambda img, magnitude: ImageOps.posterize(img, magnitude),\n            \"solarize\": lambda img, magnitude: ImageOps.solarize(img, magnitude),\n            \"contrast\": lambda img, magnitude: ImageEnhance.Contrast(img).enhance(\n                1 + magnitude * random.choice([-1, 1])),\n            \"sharpness\": lambda img, magnitude: ImageEnhance.Sharpness(img).enhance(\n                1 + magnitude * random.choice([-1, 1])),\n            \"brightness\": lambda img, magnitude: ImageEnhance.Brightness(img).enhance(\n                1 + magnitude * random.choice([-1, 1])),\n            \"autocontrast\": lambda img, magnitude: ImageOps.autocontrast(img),\n            \"equalize\": lambda img, magnitude: ImageOps.equalize(img),\n            \"invert\": lambda img, magnitude: ImageOps.invert(img)\n        }\n\n        self.p1 = p1\n        self.operation1 = func[operation1]\n        self.magnitude1 = ranges[operation1][magnitude_idx1]\n        self.p2 = p2\n        self.operation2 = func[operation2]\n        self.magnitude2 = ranges[operation2][magnitude_idx2]\n\n\n    def __call__(self, img):\n        if random.random() < self.p1: img = self.operation1(img, self.magnitude1)\n        if random.random() < self.p2: img = self.operation2(img, self.magnitude2)\n        return img","2ea660a2":"# from autoaugment import ImageNetPolicy\nfrom albumentations.pytorch import ToTensor\nimport albumentations as A\naugmentation_pipeline = A.Compose(\n    [\n        A.HorizontalFlip(p = 0.5), # apply horizontal flip to 50% of images\n        A.VerticalFlip(p =0.5),\n        A.OneOf(\n            [\n                # apply one of transforms to 50% of images\n                A.RandomContrast(), # apply random contrast\n                A.RandomGamma(), # apply random gamma\n                A.RandomBrightness(), # apply random brightness\n                A.RandomBrightnessContrast(),\n            ],\n            p = 0.5\n        ),\n        A.Cutout(num_holes=10, max_h_size=20, max_w_size=20, fill_value=0, p=0.5),\n        A.Blur(blur_limit=(15, 15), p=0.5),\n        A.Normalize(\n            mean=[0.485, 0.456, 0.406],\n            std=[0.229, 0.224, 0.225]),\n        \n        ToTensor() # convert the image to PyTorch tensor\n    ],\n    p = 1\n)\ntransforms_train = transforms.Compose([\n    transforms.ToPILImage(),\n    transforms.RandomRotation(90),\n#     transforms.CenterCrop(10),\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomVerticalFlip(),\n    transforms.ColorJitter(),\n#     ImageNetPolicy(),\n    \n    transforms.ToTensor(),\n    transforms.Normalize( mean = np.array([0.485, 0.456, 0.406]),\n    std = np.array([0.229, 0.224, 0.225]))\n])\ntrain_transforms = transforms.Compose([transforms.ToPILImage(),\n                                        transforms.RandomRotation(30),\n                                       transforms.RandomResizedCrop(224),\n                                       transforms.RandomHorizontalFlip(),ImageNetPolicy(),\n                                       transforms.ToTensor(),\n                                       transforms.Normalize([0.485, 0.456, 0.406],\n                                                            [0.229, 0.224, 0.225])])\ntrain_dataset = FoodData(train_data,input_root_dir,256,transforms_train)","5b245219":"batch = 64\nvalid_size = 0.2\nnum = train_data.__len__()\n# Dividing the indices for train and cross validation\nindices = list(range(num))\nnp.random.shuffle(indices)\nsplit = int(np.floor(valid_size*num))\ntrain_idx,valid_idx = indices[split:], indices[:split]\n\n#Create Samplers\ntrain_sampler = SubsetRandomSampler(train_idx)\nvalid_sampler = SubsetRandomSampler(valid_idx)\n\ntrain_loader = DataLoader(train_dataset, batch_size = batch, sampler = train_sampler)\nvalid_loader = DataLoader(train_dataset, batch_size = batch, sampler = valid_sampler)","7c68ca3c":"# transforms_test = transforms.Compose([\n#     transforms.ToPILImage(),\n#     transforms.ToTensor(),\n#     transforms.Normalize( mean = np.array([0.485, 0.456, 0.406]),\n#     std = np.array([0.229, 0.224, 0.225]))\n# ])\ntest_transforms = transforms.Compose([transforms.ToPILImage(),\n                                      transforms.CenterCrop(224),\n                                      transforms.ToTensor(),\n                                      transforms.Normalize([0.485, 0.456, 0.406],\n                                                           [0.229, 0.224, 0.225])])\n\ntest_data = FoodData(test_data,input_root_dir,256,transform = test_transforms)\n\ntest_loader = DataLoader(test_data, batch_size=batch, shuffle=False)","ccfdd532":"device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nprint(device)","e7e49a33":"dataloaders = {}\ndataset_sizes = {}\ndataloaders['train'] = train_loader\ndataloaders['val'] = valid_loader\ndataset_sizes['train'] = train_sampler.__len__()\ndataset_sizes['val'] = valid_sampler.__len__()","e435bf54":"def mixup_data(x, y, alpha=1.0, use_cuda=True):\n\n    '''Compute the mixup data. Return mixed inputs, pairs of targets, and lambda'''\n    if alpha > 0.:\n        lam = np.random.beta(alpha, alpha)\n    else:\n        lam = 1.\n    batch_size = x.size()[0]\n    if use_cuda:\n        index = torch.randperm(batch_size).cuda()\n    else:\n        index = torch.randperm(batch_size)\n\n    mixed_x = lam * x + (1 - lam) * x[index,:]\n    y_a, y_b = y, y[index]\n    return mixed_x, y_a, y_b, lam\n\ndef mixup_criterion(y_a, y_b, lam):\n    return lambda criterion, pred: lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)\n","d25b4a1d":"from torch.autograd import Variable\ndef train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n    since = time.time()\n\n    best_model_wts = copy.deepcopy(model.state_dict())\n    best_acc = 0.0\n\n    for epoch in range(num_epochs):\n        print('Epoch {}\/{}'.format(epoch, num_epochs - 1))\n        print('-' * 10)\n\n        # Each epoch has a training and validation phase\n        for phase in ['train', 'val']:\n            if phase == 'train':\n                model.train()  # Set model to training mode\n            else:\n                model.eval()   # Set model to evaluate mode\n\n            running_loss = 0.0\n            running_corrects = 0\n\n            # Iterate over data.\n            for data in tqdm(dataloaders[phase]):\n                inputs = data['gt'].squeeze(0).to(device)\n                labels = data['label'].to(device)\n#                 if phase == 'train':\n#                   inputs, labels_a, labels_b, lam = mixup_data(inputs, labels, 0.4, device)\n#                   inputs, targets_a, targets_b = Variable(inputs), Variable(labels_a), Variable(labels_b)\n                  \n                inputs = inputs.to(device)\n#                 if phase == 'train':\n#                   labels_a = labels_a.to(device)\n#                   labels_b = labels_b.to(device)\n#                 else:\n#                   labels = labels.to(device)\n                labels = labels.to(device)\n                # zero the parameter gradients\n                optimizer.zero_grad()\n\n                # forward\n                # track history if only in train\n                with torch.set_grad_enabled(phase == 'train'):\n                    outputs = model(inputs)\n#                     loss_func = mixup_criterion(labels_a,labels_b,lam)\n                    \n                    _, preds = torch.max(outputs, 1)\n\n                    loss = criterion(outputs, labels)\n#                     if phase=='train':\n#                       loss = loss_func(criterion,outputs)\n#                     else:\n#                         loss = criterion(outputs,labels) \n\n                    # backward + optimize only if in training phase\n                    if phase == 'train':\n                        loss.backward()\n                        optimizer.step()\n\n                # statistics\n                running_loss += loss.item() * inputs.size(0)\n                running_corrects += torch.sum(preds == labels.data)\n#                 if phase == 'train':\n#                   running_corrects += lam * preds.eq(labels_a.data).sum() + (1 - lam) * preds.eq(labels_b.data).sum()\n#                 else:\n#                     running_corrects += torch.sum(preds == labels.data)\n            if phase == 'train':\n                scheduler.step()\n\n            epoch_loss = running_loss \/ dataset_sizes[phase]\n            epoch_acc = running_corrects.double() \/ dataset_sizes[phase]\n\n            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n                phase, epoch_loss, epoch_acc))\n\n            # deep copy the model\n            if phase == 'val' and epoch_acc > best_acc:\n                best_acc = epoch_acc\n                best_model_wts = copy.deepcopy(model.state_dict())\n                torch.save(model.state_dict(), 'best_model_so_far.pth')\n\n        print()\n\n    time_elapsed = time.time() - since\n    print('Training complete in {:.0f}m {:.0f}s'.format(\n        time_elapsed \/\/ 60, time_elapsed % 60))\n    print('Best val Acc: {:4f}'.format(best_acc))\n\n    # load best model weights\n    model.load_state_dict(best_model_wts)\n    return model","bb2ab46b":"import torch\nimport torch.nn.parallel\nimport numpy as np\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom IPython import embed\n\nclass Downsample(nn.Module):\n    def __init__(self, pad_type='reflect', filt_size=3, stride=2, channels=None, pad_off=0):\n        super(Downsample, self).__init__()\n        self.filt_size = filt_size\n        self.pad_off = pad_off\n        self.pad_sizes = [int(1.*(filt_size-1)\/2), int(np.ceil(1.*(filt_size-1)\/2)), int(1.*(filt_size-1)\/2), int(np.ceil(1.*(filt_size-1)\/2))]\n        self.pad_sizes = [pad_size+pad_off for pad_size in self.pad_sizes]\n        self.stride = stride\n        self.off = int((self.stride-1)\/2.)\n        self.channels = channels\n\n        if(self.filt_size==1):\n            a = np.array([1.,])\n        elif(self.filt_size==2):\n            a = np.array([1., 1.])\n        elif(self.filt_size==3):\n            a = np.array([1., 2., 1.])\n        elif(self.filt_size==4):    \n            a = np.array([1., 3., 3., 1.])\n        elif(self.filt_size==5):    \n            a = np.array([1., 4., 6., 4., 1.])\n        elif(self.filt_size==6):    \n            a = np.array([1., 5., 10., 10., 5., 1.])\n        elif(self.filt_size==7):    \n            a = np.array([1., 6., 15., 20., 15., 6., 1.])\n\n        filt = torch.Tensor(a[:,None]*a[None,:])\n        filt = filt\/torch.sum(filt)\n        self.register_buffer('filt', filt[None,None,:,:].repeat((self.channels,1,1,1)))\n\n        self.pad = get_pad_layer(pad_type)(self.pad_sizes)\n\n    def forward(self, inp):\n        if(self.filt_size==1):\n            if(self.pad_off==0):\n                return inp[:,:,::self.stride,::self.stride]    \n            else:\n                return self.pad(inp)[:,:,::self.stride,::self.stride]\n        else:\n            return F.conv2d(self.pad(inp), self.filt, stride=self.stride, groups=inp.shape[1])\n\ndef get_pad_layer(pad_type):\n    if(pad_type in ['refl','reflect']):\n        PadLayer = nn.ReflectionPad2d\n    elif(pad_type in ['repl','replicate']):\n        PadLayer = nn.ReplicationPad2d\n    elif(pad_type=='zero'):\n        PadLayer = nn.ZeroPad2d\n    else:\n        print('Pad type [%s] not recognized'%pad_type)\n    return PadLayer\n\nclass Downsample1D(nn.Module):\n    def __init__(self, pad_type='reflect', filt_size=3, stride=2, channels=None, pad_off=0):\n        super(Downsample1D, self).__init__()\n        self.filt_size = filt_size\n        self.pad_off = pad_off\n        self.pad_sizes = [int(1. * (filt_size - 1) \/ 2), int(np.ceil(1. * (filt_size - 1) \/ 2))]\n        self.pad_sizes = [pad_size + pad_off for pad_size in self.pad_sizes]\n        self.stride = stride\n        self.off = int((self.stride - 1) \/ 2.)\n        self.channels = channels\n\n        # print('Filter size [%i]' % filt_size)\n        if(self.filt_size == 1):\n            a = np.array([1., ])\n        elif(self.filt_size == 2):\n            a = np.array([1., 1.])\n        elif(self.filt_size == 3):\n            a = np.array([1., 2., 1.])\n        elif(self.filt_size == 4):\n            a = np.array([1., 3., 3., 1.])\n        elif(self.filt_size == 5):\n            a = np.array([1., 4., 6., 4., 1.])\n        elif(self.filt_size == 6):\n            a = np.array([1., 5., 10., 10., 5., 1.])\n        elif(self.filt_size == 7):\n            a = np.array([1., 6., 15., 20., 15., 6., 1.])\n\n        filt = torch.Tensor(a)\n        filt = filt \/ torch.sum(filt)\n        self.register_buffer('filt', filt[None, None, :].repeat((self.channels, 1, 1)))\n\n        self.pad = get_pad_layer_1d(pad_type)(self.pad_sizes)\n\n    def forward(self, inp):\n        if(self.filt_size == 1):\n            if(self.pad_off == 0):\n                return inp[:, :, ::self.stride]\n            else:\n                return self.pad(inp)[:, :, ::self.stride]\n        else:\n            return F.conv1d(self.pad(inp), self.filt, stride=self.stride, groups=inp.shape[1])\n\ndef get_pad_layer_1d(pad_type):\n    if(pad_type in ['refl', 'reflect']):\n        PadLayer = nn.ReflectionPad1d\n    elif(pad_type in ['repl', 'replicate']):\n        PadLayer = nn.ReplicationPad1d\n    elif(pad_type == 'zero'):\n        PadLayer = nn.ZeroPad1d\n    else:\n        print('Pad type [%s] not recognized' % pad_type)\n    return PadLayer","45e97b95":"import torch.nn as nn\nimport torch.utils.model_zoo as model_zoo\n\ndef conv3x3(in_planes, out_planes, stride=1, groups=1):\n    \"\"\"3x3 convolution with padding\"\"\"\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n                 padding=1, groups=groups, bias=False)\n\ndef conv1x1(in_planes, out_planes, stride=1):\n    \"\"\"1x1 convolution\"\"\"\n    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1, norm_layer=None, filter_size=1):\n        super(BasicBlock, self).__init__()\n        if norm_layer is None:\n            norm_layer = nn.BatchNorm2d\n        if groups != 1:\n            raise ValueError('BasicBlock only supports groups=1')\n        # Both self.conv1 and self.downsample layers downsample the input when stride != 1\n        self.conv1 = conv3x3(inplanes, planes)\n        self.bn1 = norm_layer(planes)\n        self.relu = nn.ReLU(inplace=True)\n        if(stride==1):\n            self.conv2 = conv3x3(planes,planes)\n        else:\n            self.conv2 = nn.Sequential(Downsample(filt_size=filter_size, stride=stride, channels=planes),\n                conv3x3(planes, planes),)\n        self.bn2 = norm_layer(planes)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        identity = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        if self.downsample is not None:\n            identity = self.downsample(x)\n\n        out += identity\n        out = self.relu(out)\n\n        return out\n\n\nclass Bottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1, norm_layer=None, filter_size=1):\n        super(Bottleneck, self).__init__()\n        if norm_layer is None:\n            norm_layer = nn.BatchNorm2d\n        # Both self.conv2 and self.downsample layers downsample the input when stride != 1\n        self.conv1 = conv1x1(inplanes, planes)\n        self.bn1 = norm_layer(planes)\n        self.conv2 = conv3x3(planes, planes, groups) # stride moved\n        self.bn2 = norm_layer(planes)\n        if(stride==1):\n            self.conv3 = conv1x1(planes, planes * self.expansion)\n        else:\n            self.conv3 = nn.Sequential(Downsample(filt_size=filter_size, stride=stride, channels=planes),\n                conv1x1(planes, planes * self.expansion))\n        self.bn3 = norm_layer(planes * self.expansion)\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        identity = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            identity = self.downsample(x)\n\n        out += identity\n        out = self.relu(out)\n\n        return out\n\n\nclass ResNet(nn.Module):\n\n    def __init__(self, block, layers, num_classes=1000, zero_init_residual=False,\n                 groups=1, width_per_group=64, norm_layer=None, filter_size=1, pool_only=True):\n        super(ResNet, self).__init__()\n        if norm_layer is None:\n            norm_layer = nn.BatchNorm2d\n        planes = [int(width_per_group * groups * 2 ** i) for i in range(4)]\n        self.inplanes = planes[0]\n\n        if(pool_only):\n            self.conv1 = nn.Conv2d(3, planes[0], kernel_size=7, stride=2, padding=3, bias=False)\n        else:\n            self.conv1 = nn.Conv2d(3, planes[0], kernel_size=7, stride=1, padding=3, bias=False)\n        self.bn1 = norm_layer(planes[0])\n        self.relu = nn.ReLU(inplace=True)\n\n        if(pool_only):\n            self.maxpool = nn.Sequential(*[nn.MaxPool2d(kernel_size=2, stride=1), \n                Downsample(filt_size=filter_size, stride=2, channels=planes[0])])\n        else:\n            self.maxpool = nn.Sequential(*[Downsample(filt_size=filter_size, stride=2, channels=planes[0]), \n                nn.MaxPool2d(kernel_size=2, stride=1), \n                Downsample(filt_size=filter_size, stride=2, channels=planes[0])])\n\n        self.layer1 = self._make_layer(block, planes[0], layers[0], groups=groups, norm_layer=norm_layer)\n        self.layer2 = self._make_layer(block, planes[1], layers[1], stride=2, groups=groups, norm_layer=norm_layer, filter_size=filter_size)\n        self.layer3 = self._make_layer(block, planes[2], layers[2], stride=2, groups=groups, norm_layer=norm_layer, filter_size=filter_size)\n        self.layer4 = self._make_layer(block, planes[3], layers[3], stride=2, groups=groups, norm_layer=norm_layer, filter_size=filter_size)\n        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n        self.fc = nn.Linear(planes[3] * block.expansion, num_classes)\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                if(m.in_channels!=m.out_channels or m.out_channels!=m.groups or m.bias is not None):\n                    # don't want to reinitialize downsample layers, code assuming normal conv layers will not have these characteristics\n                    nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n                else:\n                    print('Not initializing')\n            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n\n        # Zero-initialize the last BN in each residual branch,\n        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n        # This improves the model by 0.2~0.3% according to https:\/\/arxiv.org\/abs\/1706.02677\n        if zero_init_residual:\n            for m in self.modules():\n                if isinstance(m, Bottleneck):\n                    nn.init.constant_(m.bn3.weight, 0)\n                elif isinstance(m, BasicBlock):\n                    nn.init.constant_(m.bn2.weight, 0)\n\n    def _make_layer(self, block, planes, blocks, stride=1, groups=1, norm_layer=None, filter_size=1):\n        if norm_layer is None:\n            norm_layer = nn.BatchNorm2d\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            # downsample = nn.Sequential(\n            #     conv1x1(self.inplanes, planes * block.expansion, stride, filter_size=filter_size),\n            #     norm_layer(planes * block.expansion),\n            # )\n\n            downsample = [Downsample(filt_size=filter_size, stride=stride, channels=self.inplanes),] if(stride !=1) else []\n            downsample += [conv1x1(self.inplanes, planes * block.expansion, 1),\n                norm_layer(planes * block.expansion)]\n            # print(downsample)\n            downsample = nn.Sequential(*downsample)\n\n        layers = []\n        layers.append(block(self.inplanes, planes, stride, downsample, groups, norm_layer, filter_size=filter_size))\n        self.inplanes = planes * block.expansion\n        for _ in range(1, blocks):\n            layers.append(block(self.inplanes, planes, groups=groups, norm_layer=norm_layer, filter_size=filter_size))\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n\n        x = self.avgpool(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n\n        return x\ndef resnet50(pretrained=False, filter_size=1, pool_only=True, **kwargs):\n    \"\"\"Constructs a ResNet-50 model.\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    \"\"\"\n    model = ResNet(Bottleneck, [3, 4, 6, 3], filter_size=filter_size, pool_only=pool_only, **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls['resnet50']))\n    return model","8bfca587":"!wget https:\/\/www.dropbox.com\/s\/j8bhu6tddbqy5th\/resnet50_lpf3-a4e868d2.pth.tar?dl=0 -O resnet50_lpf3.pth.tar","06b383a9":"import copy\n# model_ft = models.resnet50(pretrained=True)\n# model_ft = resnet50(filter_size=3)\n# model_ft.load_state_dict(torch.load('resnet50_lpf3.pth.tar')['state_dict'])\n# num_ftrs = model_ft.fc.in_features\n# Here the size of each output sample is set to 2.\n# Alternatively, it can be generalized to nn.Linear(num_ftrs, len(class_names)).\n# model_ft.fc = nn.Linear(num_ftrs, 101)\nmodel_ft.load_state_dict(torch.load('best_model_so_far.pth'))\nmodel_ft = model_ft.to(device)\n\ncriterion = nn.CrossEntropyLoss()\n\n# Observe that all parameters are being optimized\noptimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)\n# optimizer = optim.Adam(model_ft.parameters(), lr=0.01, betas=[0.9, 0.999])\n\n# Decay LR by a factor of 0.1 every 7 epochs\nexp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=3, gamma=0.1)\n","021a3b14":"model_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler,\n                       num_epochs=2)          ","c528f50d":"!pip install ttach           ","4abb26f1":"import ttach as tta\ntransforms = tta.Compose(\n    [\n    # tta.FiveCrops(128,128),   \n    tta.HorizontalFlip(),\n    tta.VerticalFlip(),\n    # tta.FiveCrops(128,128),\n    tta.Rotate90(angles=[0, 90]),\n#     tta.Scale(scales=[1, 2, 4]),\n        \n    ]\n)\nmodel_ft.load_state_dict(torch.load('best_model_so_far.pth'))\nmodel_ft.eval()\ntta_model = tta.ClassificationTTAWrapper(model_ft, transforms,merge_mode='mean')","e31bfde1":"correct = 0\ntotal = 0\npred_list = []\ncorrect_list = []\nwith torch.no_grad():\n    for images in tqdm(valid_loader):\n        data = images['gt'].squeeze(0).to(device)\n        target = images['label'].to(device)\n        outputs = model_ft(data)\n        _, predicted = torch.max(outputs.data, 1)\n        total += target.size(0)\n        pr = predicted.detach().cpu().numpy()\n        for i in pr:\n          pred_list.append(i)\n        tg = target.detach().cpu().numpy()\n        for i in tg:\n          correct_list.append(i)\n        correct += (predicted == target).sum().item()\n\nprint('Accuracy of the network on the 10000 test images: %f %%' % (\n    100 * correct \/ total))","06cf760b":"https:\/\/www.kaggle.com\/kaushal2896\/cifar-10-simple-cnn-with-cutmix-using-pytorch"}}