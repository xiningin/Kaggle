{"cell_type":{"6b737620":"code","aae9c65b":"code","a41c7410":"code","04116959":"code","771b2236":"code","6949de4d":"code","0e47c6a6":"code","188d487c":"code","15d6d2f0":"code","d552bdc0":"code","0d14f17c":"code","854840ff":"code","c219a2e8":"code","71a2398d":"code","78785137":"code","4718f156":"code","3221f20c":"code","46fcbec8":"code","1f0f01bc":"code","831f6a8b":"code","2397bcbc":"code","1909eebb":"code","4f55b8a5":"code","5c2250b7":"code","f911ef29":"code","f6675ba8":"code","fc2f1e76":"code","b90e4c5b":"code","21c96b8b":"code","dd4d9d37":"code","4fc60f43":"markdown","4cdc28b4":"markdown"},"source":{"6b737620":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","aae9c65b":"# Loading of dependencies\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport xgboost\nimport csv as csv\nfrom xgboost import plot_importance\nfrom matplotlib import pyplot\nfrom sklearn.model_selection import cross_val_score,KFold\nfrom sklearn.metrics import mean_absolute_error\nimport matplotlib.pyplot as plt\nfrom scipy.stats import skew\nfrom collections import OrderedDict\nfrom numpy import loadtxt\nfrom xgboost import XGBClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score","a41c7410":"import pandas as pd\nTitanic_test = pd.read_csv('\/kaggle\/input\/Test.csv')\nTitanic_train = pd.read_csv('\/kaggle\/input\/Train.csv')","04116959":"Titanic_train.head()","771b2236":"Titanic_test.head()","6949de4d":"Data_value = Titanic_train.drop(['PassengerId', 'Survived'], axis=1)\nData_target = Titanic_train['Survived']","0e47c6a6":"## Splitting of traindata into two\nX_train, X_test, y_train, y_test = train_test_split(Data_value, Data_target,random_state = 21,test_size = 0.30)\n           ","188d487c":"X_train.shape, X_test.shape, y_train.shape, y_test.shape","15d6d2f0":"X_test.head()","d552bdc0":"# Building of XGBoost Classifier\nmodel = XGBClassifier(n_estimators= 89,gamma=0.1,eta = 0.05,\n              learning_rate=0.01,colsample_bytree=0.8, max_delta_step=0, max_depth=4,\n              min_child_weight=1, missing=None, n_jobs=1,\n              nthread=None, objective='binary:logistic', random_state= 123,\n              reg_alpha=0.75, reg_lambda=0.45, scale_pos_weight=1, seed= 42,\n              silent=None, subsample= 0.8)\n\neval_set = [(X_train, y_train), (X_test, y_test)]\nmodel.fit(X_train, y_train.values.ravel(), early_stopping_rounds=10, eval_metric=[\"error\", \"logloss\"], eval_set=eval_set, verbose=True)","0d14f17c":"# Checking Train accuracy\naccuracy = model.score(X_train, y_train)\naccuracy\nprint(\"accuracy: %.1f%%\" % (accuracy * 100.0))","854840ff":"## Evaluation of Test\nX_test.head()","c219a2e8":"# make predictions for test data through X_data set above\ny_pred = model.predict(X_test)\n#predictions = [round(value) for value in y_pred]\nprint(y_pred)","71a2398d":"# Print classification report\nfrom sklearn.metrics import classification_report\nprint(classification_report(y_test, y_pred))","78785137":"# Prediction accuracy\nfrom sklearn.metrics import accuracy_score\naccuracy = accuracy_score(y_test, y_pred)\nprint(\"Accuracy: %.1f%%\" % (accuracy * 100))","4718f156":"# retrieve performance metrics\nresults = model.evals_result()\nepochs = len(results['validation_0']['error'])\nx_axis = range(0, epochs)\n\n# plot log loss\nfig, ax = pyplot.subplots()\nax.plot(x_axis, results['validation_0']['logloss'], label='Train')\nax.plot(x_axis, results['validation_1']['logloss'], label='Test')\nax.legend()\npyplot.ylabel('Log Loss')\npyplot.title('XGBoost Log Loss')\npyplot.show()\n\n# plot classification error\nfig, ax = pyplot.subplots()\nax.plot(x_axis, results['validation_0']['error'], label='Train')\nax.plot(x_axis, results['validation_1']['error'], label='Test')\nax.legend()\npyplot.ylabel('Classification Error')\npyplot.title('XGBoost Classification Error')\npyplot.show()","3221f20c":"# Confusion Matrix\nfrom sklearn.metrics import confusion_matrix\nconf = confusion_matrix(y_test,y_pred)\nconf","46fcbec8":"## Make a prediction on test datset \nsub_test = Titanic_test.drop(['PassengerId'], axis =1)\nsub_test_pred = model.predict(sub_test).astype(int)","1f0f01bc":"# Make a submission to Kaggle\nAllsub = pd.DataFrame({'PassengerId':Titanic_test['PassengerId'],\n                       'Survived' :sub_test_pred})\nAllsub.to_csv(\"Submission_Titanic_XGBoost.csv\", index = False)","831f6a8b":"from sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\nX = Titanic_train.iloc[:,2:] # Independent Columns\ny = Titanic_train.iloc[:,1] # target column i.e Survived\nTitanic_train.head(1)","2397bcbc":"# Apply SelectKBest class to extract top best features\nbestfeatures = SelectKBest(score_func=chi2, k= 'all')\nfit = bestfeatures.fit(X,y)","1909eebb":"dfscores = pd.DataFrame(fit.scores_)\ndfcolumns = pd.DataFrame(X.columns)","4f55b8a5":"# Concatenate two dataframes for better visualisation\nfeatureScores = pd.concat([dfcolumns,dfscores], axis =1)\n# Naming of dataframe columns\nfeatureScores.columns = ['Independent variables', 'Score']","5c2250b7":"featureScores","f911ef29":"# Print 11 best features\nprint(featureScores.nlargest(11,'Score'))","f6675ba8":"from sklearn.ensemble import ExtraTreesClassifier\nimport matplotlib.pyplot as plt\nmodel2 = ExtraTreesClassifier()\nmodel2.fit(X,y)","fc2f1e76":"print(model2.feature_importances_)","b90e4c5b":"## Plot Graph of feature importances for better visualization\nfeat_importances = pd.Series(model2.feature_importances_, index = X.columns) \nfeat_importances.nlargest(11).plot(kind = 'barh')\nplt.show()","21c96b8b":"## Checking features correlation \n## (using Correaltion Matrix with Heatmap)\nimport seaborn as sns\n## To extract the best 11 features from our train data\n# so that we can plot only those 11 best features using HeatMap\n# according to their column number in our train data set.\nX1 = Titanic_train.iloc[:,9:19]\ncorrmat = X1.corr()\ntop_corr_features = corrmat.index\nplt.figure(figsize=(10000,10000))","dd4d9d37":"# Plot Heat-Map\ng = sns.heatmap(X1[top_corr_features].corr(),\n                annot=True,cmap='RdYlGn')","4fc60f43":"# SELECTION OF BEST FEATURES FROM THE TRAIN DATASET","4cdc28b4":"# FEATURES IMPORTANCE"}}