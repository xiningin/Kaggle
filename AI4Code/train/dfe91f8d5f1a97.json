{"cell_type":{"abf595af":"code","34dd48f4":"code","615d47f5":"code","a032e383":"code","7d960177":"code","3614fae4":"code","f0dd89d5":"code","eb55eb39":"code","f85be207":"code","b0967a55":"code","54c71a13":"code","191ef490":"code","4870251f":"code","da37689c":"code","e9571a53":"code","6d22e222":"code","ab09e26b":"code","a5b95e2a":"code","eb21afd4":"code","0059b53f":"markdown","d01ad23a":"markdown","23bfae73":"markdown","1a15639c":"markdown","8c6eee81":"markdown","f0d0b997":"markdown","80194ae6":"markdown","970fc397":"markdown","83f1fe13":"markdown","f70a4c0e":"markdown","1de66346":"markdown","5ffb8f48":"markdown","c1cd19b1":"markdown","6347f4a6":"markdown","1de78acb":"markdown","479ad086":"markdown","9a7a13ab":"markdown"},"source":{"abf595af":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)","34dd48f4":"dataset = pd.read_csv('\/kaggle\/input\/spam-or-not-spam-dataset\/spam_or_not_spam.csv')","615d47f5":"dataset.info()","a032e383":"dataset.head()","7d960177":"dataset = dataset.dropna()","3614fae4":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(dataset.email, dataset.label, test_size=0.1)","f0dd89d5":"print(X_train.shape)\nprint(X_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)","eb55eb39":"from sklearn.base import BaseEstimator, TransformerMixin\nfrom nltk.stem.porter import *\n\nclass EmailStemming(BaseEstimator, TransformerMixin):\n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X):\n        x_temp = []\n        stemmer = PorterStemmer()\n        for email in X:\n            x_temp.append([stemmer.stem(word) for word in email.split()])\n        X = x_temp\n        \n        return X","f85be207":"from collections import Counter\nfrom sklearn.base import BaseEstimator, TransformerMixin\n\n\nclass FeatureVectors(BaseEstimator, TransformerMixin):\n    def __init__(self, corpus=None, corpus_len=100):\n        self.corpus = corpus\n        self.corpus_len = corpus_len\n        \n    def fit(self, X, y=None):\n        if self.corpus == None:\n            self.corpus = Counter([\n                word \n                for email in X\n                for word in email\n            ]).most_common(self.corpus_len)\n        return self\n    \n    def transform(self, X):\n        corpus_list = [key for key, _ in self.corpus]\n        x_temp = []\n        for email in X:\n            x_temp.append(np.array([email.count(word) for word in corpus_list]))\n        X = x_temp\n        return np.array(X)","b0967a55":"from sklearn.pipeline import Pipeline\n\ncorpus_len = 1000\npreprocess_pipeline = Pipeline([\n    ('email_stemming', EmailStemming()),\n    ('feature_vectors', FeatureVectors(corpus_len=corpus_len)),\n])\n\nX_train_proc = preprocess_pipeline.fit_transform(X_train, y_train)","54c71a13":"X_train_proc.shape, y_train.shape","191ef490":"from sklearn.svm import SVC\nfrom sklearn.model_selection import cross_val_score\n\nsvm_clf = SVC(gamma=\"auto\")\nsvm_scores = cross_val_score(svm_clf, X_train_proc, y_train, cv=10)\nsvm_scores.mean()","4870251f":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_score\n\nlog_clf = LogisticRegression(penalty='l2', max_iter=1000, fit_intercept=True)\nlog_scores = cross_val_score(log_clf, X_train_proc, y_train, cv=10)\nlog_scores.mean()","da37689c":"from sklearn.ensemble import RandomForestClassifier\n\nforest_clf = RandomForestClassifier(n_estimators=100)\nforest_scores = cross_val_score(forest_clf, X_train_proc, y_train, cv=10)\nforest_scores.mean()","e9571a53":"from sklearn.neighbors import KNeighborsClassifier\n\nknn_clf = KNeighborsClassifier()\nknn_scores = cross_val_score(knn_clf, X_train_proc, y_train, cv=10)\nknn_scores.mean()","6d22e222":"%matplotlib inline\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\n\nfrom sklearn.metrics import f1_score\nfrom sklearn.model_selection import train_test_split\n\ndef plot_learning_curves(model, X, y):\n    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=10)\n    train_errors, val_errors = [], []\n    for m in range(20, len(X_train), 10):\n        model.fit(X_train[:m], y_train[:m])\n        y_train_predict = model.predict(X_train[:m])\n        y_val_predict = model.predict(X_val)\n        train_errors.append(f1_score(y_train[:m], y_train_predict) * 100)\n        val_errors.append(f1_score(y_val, y_val_predict) * 100)\n\n    plt.plot(train_errors, \"r-+\", linewidth=2, label=\"train\")\n    plt.plot(val_errors, \"b-\", linewidth=3, label=\"val\")\n    plt.legend(loc=\"upper right\", fontsize=14)\n    plt.xlabel(\"\", fontsize=14)\n    plt.ylabel(\"F1\", fontsize=14)             ","ab09e26b":"log_clf = LogisticRegression(penalty='l2', C=1.0, max_iter=1000, fit_intercept=True)\nplot_learning_curves(log_clf, X_train_proc, y_train)\nplt.axis([0, 100, 0, 100])\nplt.show()","a5b95e2a":"X_test_proc = preprocess_pipeline.transform(X_test)\n\nlog_clf = LogisticRegression(penalty='l2', C=1.0, max_iter=1000, fit_intercept=True)\nlog_clf.fit(X_train_proc, y_train)\ny_pred = log_clf.predict(X_test_proc)","eb21afd4":"from sklearn.metrics import f1_score, accuracy_score\n\nprint(f'F1 score: {f1_score(y_pred, y_test)}')\nprint(f'Accuracy score: {accuracy_score(y_pred, y_test)}')","0059b53f":"# Get and Explore Data","d01ad23a":"Take a look at the data we have available to get a better understanding of what we are dealing with","23bfae73":"> We create a data processing pipeline, that will apply our custom transformers to the data passed to it","1a15639c":"Thank you very much for the read, consider upvoting if you found something useful on this Notebook","8c6eee81":"> Lets try some different Machine Learning algorithms to see what fits our training dataset best","f0d0b997":"# Feature Engineering","80194ae6":"> Drop some unwanted values that would be problematic for training","970fc397":"> This custom transformer creates a corpus from all the unique words available on all emails, and then creates a vector for each email that count the words of that email present on the corpus\n\n> For example:\n> If the corpus contained [best, the, is, big] the email \"He is the best of the best\" would be vector [2, 2, 1, 0]","83f1fe13":"F1 score and Accuracy","f70a4c0e":"# Make predictions","1de66346":"> We train the model on subsets of the dataset in order to get a sense of learning progress marked by the blue line. As we can see the f1 score starts from 40% and gets gradually better it converges near the end with the red line","5ffb8f48":"# Training Visualization","c1cd19b1":"# Train and Select Models","6347f4a6":"# Thank you","1de78acb":"> Logistic regression fits well to the training set so it will be used for making predictions on the test set after passing it through the processing pipeline","479ad086":"> We create a custom sklearn transformer that performes stemming (stemming is the process of reducing inflected (or sometimes derived) words to their word stem, base or root form) using an ntlk stemmer","9a7a13ab":"> Split data to train and test sets. We will use the train set for fitting our model to the data and the use the test set in order to make our predictions and see how well the model generalizes"}}