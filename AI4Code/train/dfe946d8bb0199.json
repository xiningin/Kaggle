{"cell_type":{"a9fba6cb":"code","09ec94f2":"code","6bbae2ff":"code","56fa6c1e":"code","8b496583":"code","d39d0332":"code","7d5a5917":"code","82b25923":"code","88c0ca0e":"code","a141862c":"code","4a92b4a4":"code","81c56122":"code","4558ae64":"code","2ee4e923":"code","372575f1":"code","3270da5b":"code","73b2644f":"code","dea14e47":"code","db0995fb":"code","288be97f":"code","18045180":"code","e3bcc207":"code","d2704937":"markdown","6bb9b3ce":"markdown","a16159c6":"markdown","8a04da9d":"markdown","41f221ca":"markdown","5e8a525f":"markdown","f92019f5":"markdown","d87bbec3":"markdown"},"source":{"a9fba6cb":"import numpy as np\nimport pandas as pd","09ec94f2":"## load train and test data\ntrain_data= pd.read_csv('..\/input\/30-days-of-ml\/train.csv')\ntest_data=pd.read_csv('..\/input\/30-days-of-ml\/test.csv')","6bbae2ff":"train_data.describe()","56fa6c1e":"train_data.info()","8b496583":"train_data.isnull().sum()","d39d0332":"test_data.isnull().sum()","7d5a5917":"from sklearn.model_selection import KFold\ntrain_data['fold']=-1\nkf=KFold(n_splits=5,shuffle=True,random_state=42)\nfor fold,(ti,vi) in enumerate(kf.split(train_data)):\n    train_data.loc[vi,'fold']=fold","82b25923":"train_data.fold.value_counts()","88c0ca0e":"pd.plotting.register_matplotlib_converters()\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns","a141862c":"figure, axis = plt.subplots(5, 3)\nfigure.set_figheight(25)\nfigure.set_figwidth(20)\n\naxis[0, 0].scatter(x=train_data['cont0'],y=train_data['target'],s=.5)\naxis[1, 0].scatter(x=train_data['cont1'],y=train_data['target'],s=.5)\naxis[2, 0].scatter(x=train_data['cont2'],y=train_data['target'],s=.5)\naxis[0, 2].scatter(x=train_data['cont3'],y=train_data['target'],s=.5)\naxis[0, 1].scatter(x=train_data['cont4'],y=train_data['target'],s=.5)\naxis[1, 1].scatter(x=train_data['cont5'],y=train_data['target'],s=.5)\naxis[2, 1].scatter(x=train_data['cont6'],y=train_data['target'],s=.5)\naxis[1, 2].scatter(x=train_data['cont7'],y=train_data['target'],s=.5)\naxis[2, 2].scatter(x=train_data['cont8'],y=train_data['target'],s=.5)\naxis[3, 0].scatter(x=train_data['cont9'],y=train_data['target'],s=.5)\naxis[3, 1].scatter(x=train_data['cont10'],y=train_data['target'],s=.5)\naxis[3, 2].scatter(x=train_data['cont11'],y=train_data['target'],s=.5)\naxis[4, 0].scatter(x=train_data['cont12'],y=train_data['target'],s=.5)\naxis[4, 1].scatter(x=train_data['cont13'],y=train_data['target'],s=.5)\n","4a92b4a4":"sns.histplot(data=train_data,x='target')","81c56122":"for i in range(10):\n    print(train_data['cat'+str(i)].value_counts())","4558ae64":"figure, axis = plt.subplots(3, 4)\nfigure.set_figheight(9)\nfigure.set_figwidth(12)\naxis[0, 0].scatter(x=train_data['cat0'],y=train_data['target'],s=5)\naxis[0, 1].scatter(x=train_data['cat1'],y=train_data['target'],s=5)\naxis[0, 2].scatter(x=train_data['cat2'],y=train_data['target'],s=5)\naxis[0, 3].scatter(x=train_data['cat3'],y=train_data['target'],s=5)\naxis[1, 0].scatter(x=train_data['cat4'],y=train_data['target'],s=5)\naxis[1, 1].scatter(x=train_data['cat5'],y=train_data['target'],s=5)\naxis[1, 2].scatter(x=train_data['cat6'],y=train_data['target'],s=5)\naxis[1, 3].scatter(x=train_data['cat7'],y=train_data['target'],s=5)\naxis[2, 0].scatter(x=train_data['cat8'],y=train_data['target'],s=5)\naxis[2, 1].scatter(x=train_data['cat9'],y=train_data['target'],s=5)\n","2ee4e923":"# traindf=traindf.drop(columns=['cont8','cont9','cont10'])\n# testdf=testdf.drop(columns=['cont8','cont9','cont10'])","372575f1":"traindf=train_data.copy()\ntestdf=test_data.copy()\n\nfor i in range(10):\n    map_=list(train_data['cat'+str(i)].unique())\n    map_.sort()\n    traindf['cat'+str(i)]=traindf['cat'+str(i)].apply(lambda x : map_.index(x))\n    testdf['cat'+str(i)]=testdf['cat'+str(i)].apply(lambda x : map_.index(x))","3270da5b":"sns.heatmap(traindf.corr(),annot=True,cmap='rocket',linewidths=0.2,annot_kws={'size':3})\nfig=plt.gcf()\nfig.set_size_inches(14,10)\nplt.xticks(fontsize=14)\nplt.yticks(fontsize=14)\nplt.show()","73b2644f":"# cat_cols=[column for column in traindf.columns if 'cat' in column]\n# cont_cols=[column for column in traindf.columns if 'cont' in column]\nfrom sklearn.metrics import mean_squared_error\nimport lightgbm as lgb\nfrom xgboost import XGBRegressor\nuseful_cols=[column for column in traindf.columns if column not in ['target','id','fold']]\nX=traindf[useful_cols]\ny=traindf['target']","dea14e47":"params1={'n_estimators': 10000,\n 'max_depth': 2,\n 'learning_rate': 0.074,\n 'gamma': 0.4,\n 'booster': 'gbtree',\n 'min_child_weight': 1,\n 'subsample': 0.7912492436244456,\n 'colsample_bytree': 0.1613480080803224,\n 'reg_alpha': 12.65778876193281,\n 'reg_lambda': 50.25603582806218\n        }\nparams2={'n_estimators': 10000,\n 'max_depth': 2,\n 'learning_rate': 0.35,\n 'booster': 'gbtree',\n 'subsample': 0.93,\n 'colsample_bytree': 0.85,\n 'reg_alpha': 35,\n 'reg_lambda': 35,\n 'n_jobs' :-1\n        }\nparams3={'n_estimators': 10000,\n 'max_depth': 3,\n 'learning_rate': 0.035,\n 'min_child_weight': 6,\n 'subsample': 0.92,\n 'colsample_bytree': 0.11,\n 'reg_alpha': 1.22,\n 'reg_lambda': 36\n        }\n# model1 = XGBRegressor(**params1)\n# model2 = XGBRegressor(**params2)\n# model1.fit(X_train,y_train, early_stopping_rounds=300, eval_set=[(X_val, y_val)], verbose=1000)\n# model2.fit(X_train,y_train, early_stopping_rounds=300, eval_set=[(X_val, y_val)], verbose=1000)","db0995fb":"## Loading the saved blends\n\nblend=pd.read_csv('..\/input\/30-days-blend\/blend (1).csv')\nblend_test=pd.read_csv('..\/input\/30-days-blend\/blend_test (1).csv')\n\n# predictions=[]\n# blend=traindf.copy();\n# blend['pred1']=0\n# blend['pred2']=0\n# blend['pred3']=0\n# blend_test=testdf.copy();\n# blend_test['pred1']=0\n# blend_test['pred2']=0\n# blend_test['pred3']=0\n# for i in range(5):\n#     train=traindf.loc[traindf.fold!=i]\n#     val=traindf.loc[traindf.fold==i]\n#     X_train=train[useful_cols]\n#     y_train=train['target']\n#     X_val=val[useful_cols]\n#     y_val=val['target']\n#     model1 = XGBRegressor(**params1,random_state=i)\n#     model2 = XGBRegressor(**params2,random_state=i)\n#     model3 = XGBRegressor(**params3,random_state=i)\n# #     print(model1.predict(train))\n#     model1.fit(X_train,y_train, early_stopping_rounds=300, eval_set=[(X_val, y_val)], verbose=0)\n#     model2.fit(X_train,y_train, early_stopping_rounds=300, eval_set=[(X_val, y_val)], verbose=0)\n#     model3.fit(X_train,y_train, early_stopping_rounds=300, eval_set=[(X_val, y_val)], verbose=0)\n#     blend.loc[blend.fold!=i,'pred1']+=model1.predict(X_train)\n#     blend.loc[blend.fold!=i,'pred2']+=model2.predict(X_train)\n#     blend.loc[blend.fold!=i,'pred3']+=model3.predict(X_train)\n#     blend_test['pred1']+=model1.predict(testdf[useful_cols])\n#     blend_test['pred2']+=model2.predict(testdf[useful_cols])\n#     blend_test['pred3']+=model3.predict(testdf[useful_cols])\n    \n#     preds=(model1.predict(X_val)+model2.predict(X_val)+model3.predict(X_val))\/3\n#     print(\"Fold \" + str(i),mean_squared_error(y_val,preds,squared=False))\n# #     predictions.append((model1.predict(testdf[useful_cols])+model2.predict(testdf[useful_cols])+model3.predict(testdf[useful_cols]))\/3)\n# blend.to_csv('.\/blend.csv')\n# blend_test.to_csv('.\/blend_test.csv')","288be97f":"blend_test","18045180":"### Here we just average out the three, but the regression gave me better results ;)\n\npreds_test=(blend_test['pred1']+blend_test['pred2']+blend_test['pred3'])\/15","e3bcc207":"submission = pd.DataFrame({'id':testdf['id'],'target':preds_test})\nsubmission.to_csv('submission_kfold.csv',index = False)  ","d2704937":"# Model Training\n\n### I have already done 3 tunings on XGB, Took couple of hours on CPU !","6bb9b3ce":"### Plotting continuous features","a16159c6":"### It's nice that we don't need to deal with null values !\n","8a04da9d":"# Hey Everyone !\n\n### In this notebook we'll go through the approach I used for the 30 days of ML Challenge !\n\n### We'll go through EDA, Feature Engineering and model building \n## Let's get started !","41f221ca":"### Plotting categorical features","5e8a525f":"# EDA\n\n### For this problem we don't have much to explore, but lets see what we dealing with\n","f92019f5":"# Feature Engineering","d87bbec3":"# Conclusion\n\n### Here I performed averaging, but performing regression or adding some weights intelligently gave me better results "}}