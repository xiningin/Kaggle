{"cell_type":{"b5fa3423":"code","edd383e4":"code","864dcc9a":"code","2d0fe43d":"code","9ce842b0":"code","6a6c9130":"code","af048758":"code","7516aa75":"code","0995e15c":"code","e6ae67d9":"code","8be633f5":"code","d40e2e08":"code","2b6f049d":"code","2b022729":"code","ff2bf941":"code","e924daaa":"code","0f1caa46":"code","425221e2":"code","8abc699d":"markdown","c9cfa8c8":"markdown","f15818c9":"markdown","72bee1d2":"markdown","bbe86ef5":"markdown","f8407280":"markdown","b3be190d":"markdown","9e46f897":"markdown","3cbe9840":"markdown"},"source":{"b5fa3423":"import matplotlib.pyplot as plt # plotting\nimport numpy as np # linear algebra\nimport os # accessing directory structure\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom scipy import sparse, stats","edd383e4":"for dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","864dcc9a":"df1 = pd.read_csv('\/kaggle\/input\/all_japanese_stocks.csv', delimiter=',')\ndf1.dataframeName = 'all_japanese_stocks.csv'\ncolumns = df1.columns.to_list()\ncolumns[0] = \"code\"\ndf1.columns = columns\ndf1[\"Date\"] = pd.to_datetime(df1[\"Date\"])\nnRow, nCol = df1.shape\nprint(f'There are {nRow} rows and {nCol} columns')","2d0fe43d":"df1.head(5)","9ce842b0":"df = df1.set_index([\"code\", \"Date\"])\ndf = df[(df[\"Volume\"] > 0) & (df[\"Close\"] < df[\"Close\"].rolling(10).mean()*2)]\ndf[\"TurnOver\"] = df[\"Close\"]*df[\"Volume\"]\nprint(df.shape)\ndf.head(5)","6a6c9130":"turnover = df[[\"TurnOver\"]].unstack().T\nturnover.index = turnover.index.get_level_values(1)\nturnover = turnover.sort_index()\nturnover = turnover.dropna(thresh=2000, axis=1).ffill().dropna()\nturnover.head(5)","af048758":"volume = df[[\"Volume\"]].unstack().T\nvolume.index = volume.index.get_level_values(1)\nvolume = volume.sort_index()\nvolume = volume.dropna(thresh=2000, axis=1).ffill().dropna()\nvolume.head(5)","7516aa75":"target_stocks = turnover.mean().nlargest(1000, keep='last')\ntarget_stocks.index","0995e15c":"close_price = df[[\"Close\"]].unstack().T\nclose_price.index = close_price.index.get_level_values(1)\nclose_price = close_price.sort_index()\nclose_price = close_price.dropna(thresh=2000, axis=1).ffill().dropna()\nfirst_date = close_price.index[0]\nclose_price = close_price[target_stocks.index]\nclose_price.head(5)","e6ae67d9":"open_price = df[[\"Open\"]].unstack().T\nopen_price.index = open_price.index.get_level_values(1)\nopen_price = open_price.sort_index()\nopen_price = open_price.dropna(thresh=2000, axis=1).ffill().dropna()\nfirst_date = open_price.index[0]\nopen_price = open_price[target_stocks.index]\nopen_price.head(5)","8be633f5":"# Hodrick Prescott filter\ndef hp_filter(x, lamb=5000):\n    w = len(x)\n    b = [[1]*w, [-2]*w, [1]*w]\n    D = sparse.spdiags(b, [0, 1, 2], w-2, w)\n    I = sparse.eye(w)\n    B = (I + lamb*(D.transpose()*D))\n    return sparse.linalg.dsolve.spsolve(B, x)\n\n\ndef mad(data, axis=None):\n    return np.mean(np.abs(data - np.mean(data, axis)), axis)\n\n\ndef AnomalyDetection(x, alpha=0.2, lamb=5000):\n    \"\"\"\n    x         : pd.Series\n    alpha     : The level of statistical significance with which to\n                accept or reject anomalies. (expon distribution)\n    lamb      : penalize parameter for hp filter\n    return r  : Data frame containing the index of anomaly\n    \"\"\"\n    # calculate residual\n    xhat = hp_filter(x, lamb=lamb)\n    resid = x - xhat\n\n    # drop NA values\n    ds = pd.Series(resid)\n    ds = ds.dropna()\n\n    # Remove the seasonal and trend component,\n    # and the median of the data to create the univariate remainder\n    md = np.median(x)\n    data = ds - md\n\n    # process data, using median filter\n    ares = (data - data.median()).abs()\n    data_sigma = data.mad() + 1e-12\n    ares = ares\/data_sigma\n\n    # compute significance\n    p = 1. - alpha\n    R = stats.expon.interval(p, loc=ares.mean(), scale=ares.std())\n    threshold = R[1]\n\n    # extract index, np.argwhere(ares > md).ravel()\n    r_id = ares.index[ares > threshold]\n\n    return r_id","d40e2e08":"np.random.seed(42)\n\n# sample signals\nN = 1024  # number of sample points\nt = np.linspace(0, 2*np.pi, N)\ny = np.sin(t) + 0.02*np.random.randn(N)\n\n# outliers are assumed to be step\/jump events at sampling points\nM = 3  # number of outliers\nfor ii, vv in zip(np.random.rand(M)*N, np.random.randn(M)):\n    y[int(ii):] += vv\n\n# detect anomaly\nr_idx = AnomalyDetection(y, alpha=0.1)\n\n# plot the result\nplt.figure()\nplt.plot(y, 'b-')\nplt.plot(r_idx, y[r_idx], 'ro')","2b6f049d":"from tqdm.auto import tqdm\n\ny = volume[target_stocks.index[0]]\n\nN = len(y)\nnum_data = 100\nstep = 1#N\/\/10\n\nr_idx = []\nindex = y.index\n\nfor n in tqdm(range(num_data, N-num_data)):\n    _r_idx = AnomalyDetection(y[max(n-100, 0):n], alpha=0.1)\n    if len(_r_idx) > 0:\n        if _r_idx[-1] == index[n-1]:\n            r_idx.append(_r_idx[-1])\n\nfig, ax = plt.subplots(figsize=(15, 5))\n\nax.plot(y, color='b',linestyle='-', label=\"volume --code : {} --step : {}days\".format(target_stocks.index[0], step))\nax.scatter(r_idx, y[r_idx], color='r', marker='o')\nax.grid(True)\nax.legend()","2b022729":"aftermath = []\n\nprice = open_price[target_stocks.index[0]]\nfor index in r_idx:\n    aftermath.append(price[index:].iloc[1:31].values)\n\naftermath = pd.DataFrame(aftermath).T\naftermath \/= aftermath.iloc[0]\nprint(aftermath.shape)\naftermath.head(5)","ff2bf941":"aftermath.plot(figsize=(15, 5), grid=True)","e924daaa":"aftermath.mean(axis=1).plot(figsize=(15, 5), grid=True)","0f1caa46":"up_aftermath = []\n\n_open_price = open_price[target_stocks.index[0]]\n_close_price = close_price[target_stocks.index[0]]\n\nfor index in r_idx:\n    if _close_price[index] > _open_price[index]:\n        up_aftermath.append(price[index:].iloc[1:31].values)\n\nup_aftermath = pd.DataFrame(up_aftermath).T\nup_aftermath \/= up_aftermath.iloc[0]\nup_aftermath.plot(figsize=(15, 2), grid=True, alpha=0.5, legend=False)\nplt.show()\nup_aftermath.mean(axis=1).plot(figsize=(15, 2), grid=True, label=\"average\", color=\"black\")","425221e2":"down_aftermath = []\n\n_open_price = open_price[target_stocks.index[0]]\n_close_price = close_price[target_stocks.index[0]]\n\nfor index in r_idx:\n    if _close_price[index] < _open_price[index]:\n        down_aftermath.append(price[index:].iloc[1:31].values)\n\ndown_aftermath = pd.DataFrame(down_aftermath).T\ndown_aftermath \/= down_aftermath.iloc[0]\ndown_aftermath.plot(figsize=(15, 2), grid=True, alpha=0.5, legend=False)\nplt.show()\ndown_aftermath.mean(axis=1).plot(figsize=(15, 2), grid=True, label=\"average\", color=\"black\")","8abc699d":"# Anomaly Detection\n\nIn this notebook, let's try to use anomaly detection by refering https:\/\/www.kaggle.com\/liubenyuan\/time-series-and-anomaly-detection\n\nAnomaly Detection (ad) Using hp filter and mad test","c9cfa8c8":"## Introduction\n\nIn this notebook, let's do anomaly detection on volume first.\n\nThen check future price movement by aftermath plot!","f15818c9":"So let's use this anomaly detection method to volume data.\n\nIn this notebook, we use some techniques such as Walkforward to calculate anomaly step by step.","72bee1d2":"## Exploratory Analysis","bbe86ef5":"Let's take a quick look at what the data looks like:","f8407280":"Sample usage of this method","b3be190d":"There is 1 csv file in the current version of the dataset:\n","9e46f897":"### Let's check 1st file: \/kaggle\/input\/all_japanese_stocks.csv","3cbe9840":"## Conclusion\n\nAnomaly detection might be helpful to extract alpha.\n\nAs we see in the last graph, close price tends to go up for 30 days after there is an anomaly in volume.\n\nGood Luck!"}}