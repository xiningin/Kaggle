{"cell_type":{"0bbaf6a0":"code","f20f0305":"code","02e135ed":"code","77a3b212":"code","ee2e6d49":"code","6abbcd6d":"code","56e93296":"code","16d9f182":"code","3d9b4fed":"code","295a691f":"code","841e8672":"code","148c261a":"code","e4ac57b4":"code","cdb697bd":"code","8e117f97":"code","9df3f432":"code","6baf28c1":"code","5f4452f2":"code","20155a84":"code","52659b6d":"markdown","9381b962":"markdown","f9a341a4":"markdown","08911898":"markdown","87d92fdc":"markdown","6ae08436":"markdown","8a1026c8":"markdown","8137b0ad":"markdown","a7b265c5":"markdown","599910c8":"markdown","d4fc786d":"markdown","1109e5e6":"markdown","f76b9ad1":"markdown","ad7b57e4":"markdown","ea666c59":"markdown","21642db9":"markdown","a4635f76":"markdown","6256981b":"markdown","559f8bca":"markdown","ac0c9a88":"markdown","a5e685ee":"markdown","4f992820":"markdown","8ef244a5":"markdown","4bba666f":"markdown","2fab44cb":"markdown","8ade5cd7":"markdown","b508be9a":"markdown","3c39317e":"markdown","e1e68691":"markdown"},"source":{"0bbaf6a0":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom IPython.display import Image","f20f0305":"features = [ 'fLength', 'fWidth', 'fSize', 'fConc', 'fConc1', 'fAsym', 'fM3Long', 'fM3Trans', 'fAlpha', 'fDist' ]\nraw_data = pd.read_csv('..\/input\/magic-gamma-telescope-dataset\/telescope_data.csv', names=features + ['class'], skiprows=1)","02e135ed":"raw_data","77a3b212":"from sklearn import preprocessing\nle = preprocessing.LabelEncoder()\nle.fit(raw_data['class'])\nraw_data['class'] = le.transform(raw_data['class'])\nX = raw_data[features].values\ny = raw_data['class'].values","ee2e6d49":"raw_data['class'].plot.hist()","6abbcd6d":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)","56e93296":"def evaluate_clf(X_train, X_test, y_train, y_test, clf, name=\"Classifier\"):\n  from sklearn.metrics import f1_score, accuracy_score\n  # fit the classifier\n  clf.fit(X_train, y_train)\n  pred = clf.predict(X_test)\n  # evaluate prediction using acc and f1 score\n  score_f1 = f1_score(y_test, pred)\n  score_acc = accuracy_score(y_test, pred)\n  print('{} acc-score: {}'.format(name, score_acc))\n  print('{} f1-score: {}'.format(name, score_f1))","16d9f182":"from sklearn.tree import DecisionTreeClassifier\nevaluate_clf(X_train, X_test, y_train, y_test, DecisionTreeClassifier(), \"Decision Tree\")","3d9b4fed":"Image('..\/input\/gaussiannotebookimg\/2.png')","295a691f":"from sklearn.naive_bayes import GaussianNB\nevaluate_clf(X_train, X_test, y_train, y_test, GaussianNB(), \"Gaussian NB\")","841e8672":"raw_data[features].corr()","148c261a":"raw_data[features].plot.scatter('fSize', 'fConc')","e4ac57b4":"Image('..\/input\/gaussiannotebookimg\/3.png')","cdb697bd":"from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nevaluate_clf(X_train, X_test, y_train, y_test, LinearDiscriminantAnalysis(), \"LDA\")","8e117f97":"Image('..\/input\/gaussiannotebookimg\/4.png')","9df3f432":"from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\nevaluate_clf(X_train, X_test, y_train, y_test, QuadraticDiscriminantAnalysis(), \"QDA\")","6baf28c1":"Image('..\/input\/gaussiannotebookimg\/5.png')","5f4452f2":"from sklearn.base import BaseEstimator\nfrom sklearn.mixture import GaussianMixture\n\nclass GaussianMixtureClassifier(BaseEstimator):\n  \n  def __init__(self, n_components=1):\n    self.n_components = n_components\n\n  def fit(self, X, y):\n    # find number of classes\n    self.n_classes = int(y.max() + 1)\n    # create a GM for each class\n    self.gm_densities = [GaussianMixture(self.n_components, covariance_type='full') for _ in range(self.n_classes)]\n    # fit the Mixture densities for each class\n    for c in range(self.n_classes):\n      # find the correspond items\n      temp = X[np.where(y == c)]\n      # estimate density parameters using EM\n      self.gm_densities[c].fit(temp)\n\n  def predict(self, X):\n    # calculate log likelihood for each class\n    log_likelihoods = np.hstack([ self.gm_densities[c].score_samples(X).reshape((-1, 1)) for c in range(self.n_classes) ])\n    # return the class whose density maximizes the log likelihoods\n    log_likelihoods = log_likelihoods.argmax(axis=1)\n    return log_likelihoods","20155a84":"evaluate_clf(X_train, X_test, y_train, y_test, GaussianMixtureClassifier(n_components=2), \"Gaussian Mixture\")","52659b6d":"### Why not to use different Covariance Matrices?\nWe can do so, However if we have many classes, it will increase the number of parameters significantly and can lead to overfittig. This method is called *Quadratic Discriminant Analysis*  or *QDA*.","9381b962":"### So, Now how to improve the model?\nBy far, for every class we used one Normal Distribution. If that's not enough let's use more of them. We can use a convex combination of them! this is called Gaussian Mixture Model.","f9a341a4":"### Create the train and test sets","08911898":"### Performing LDA","87d92fdc":"# Gaussian Models for Classification\n### From Naive Bayes to Gaussian Mixture Model\n\nContents:\n- Maximum Likelihood and Maximum a posterior classifiers\n- Gaussian Naive Bayes\n- Linear Discriminant Analysis\n- Quadratic Discriminant Analysis\n- Gaussian Mixture Model\n- How to improve the model?","6ae08436":"## Dataset\nIn this notebook I wanna use MAGIC Gamma Telescope Dataset. It's a Binary classification problem that has 10 real valued features. we want to classify every item as Gamma(signal) or Hadron.","8a1026c8":"Well, in comparison to Decision Tree result it's disappointing. But we can improve it. Before that, we must know what's the problem.","8137b0ad":"## How to improve the result further?\n- Encode your domain knowledge about the problem as prior distributions.\n- Try optimizing hyperparameters of the model, number of Gaussians for each class, Covariance Matrix type for each class.\n- Everything is not Gaussian. again use your domain knowledge and find proper distribution for each feature.\n- Do a little feature engineering","a7b265c5":"## Define The Enemy!\nDecision Tree is an Excellent classifier. First we'll evaluate it's performance then try to beat it by using Gaussian models.","599910c8":"As you can see dataset is not balanced, hence for evaluating the model in addition to accuracy we'll also check the f1-score","d4fc786d":"Much better than Naive Bayes but still worse than Decision Tree.","1109e5e6":"### How to fix the feature correlation problem?\nIn Naive Bayes model, Covariance Matrix of Normal ditributions of classes was diagonal. Instead, we can use a full Covariance Matrix, But for now let's use a unique Covariance Matrix for each class. Indeed we assume that the correlation of features for every class is same. Such a model is called *LDA* or *Linear Discriminant Analysis*.","f76b9ad1":"### Finally we beated the Decision Tree!!!\nNow you can see the mixture model outperforms the Decision Tree.","ad7b57e4":"### Why Gaussian Naive Bayes doesn't perform well?\nIn this case that's mainly because of the assumption that indicates features are independent.\nFrom the geometrical point of view it means the elipsoid of Normal ditribution can not rotate and only can be scaled along it's axes.\nTo understand better, let's check the correlation of features.","ea666c59":"We can see some features are highly correlated. for example see the scatter plot for **fSize**\nand **fConc**","21642db9":"### QDA Result","a4635f76":"I will be happy to know your comments :)","6256981b":"### Why LDA result was better?\nIt seems for this dataset, increaing flexibility of Normal distribution doesn't have a big impact on performance and instead caused overfitting. Unfortunately Decision Tree is still better. let's find another way.","559f8bca":"### Reading the Dataset","ac0c9a88":"## Gaussian Naive Bayes Classifier\nLet's start by a simple Gaussian model. In this model we assume that the features are independent. That means a diagonal covariance matrix for each Gaussian distribution. Because of the assumption of feature inedpendence it's called Naive. ","a5e685ee":"## References\nThese resources help me a lot to write this notebook.  \n- [1] Machine Learning: A Probabilistic Perspective, Kevin P. Murphy  \nSee Chapter 4 for Gaussian Models and Chapter 11 for Mixture Models and EM Algorithm  \n- [2] Coursera, Advanced Machine Learning Specialization, Bayesian Methods for Machine Learning Course","4f992820":"### Checking class imbalance","8ef244a5":"### Creating a Scikit-Learn classifier based on Gaussian Mixture \nNow we use scikit implementation of EM Algorithm for Gaussian Mixture models to create a custom estimator for classification.","4bba666f":"### Encode class labels","2fab44cb":"## Linear Discriminant Analysis (LDA)","8ade5cd7":"It is better to write a function for easier evaluation of the classifiers. ","b508be9a":"## Maximum Likelihood and Maximum a Posterior Classifiers\nIf you can find a proper probability distribution for every class, then you can calculate the likelihood of a new data item and find the class for which the likelihood is maximum. Sometimes you have a prior knowledge about you classes and you can encode it as prior distributions. In this notebook we use Multivariate Normal or Gaussian Distribution for modeling density of classes. We will start by a simple model later we'll improve it until out performing the Decision Tree classifier.\n","3c39317e":"Now, we create a Gaussian Mixture Classifier with a mixture of 2 Gaussian distributions per class.","e1e68691":"### Expectation Maximization Algorithm\nEstimating parameters of a single Gaussian distribution is trivial and includes only calculation of mean and covariance.However parameter estimation for a Gaussian Mixture model is not so trivial. Also optimizing parameters of these models for minimizing the negative log likelihood is not easy for gradient-based optimizers. that's because the Covariance Matrix should be Positive-definite which is not an easy to handle constraint for many optimizers [2]. Instead we can use the Expectation Maximization Algorithm. Usually used for estimating parameters of Graphical models who use latent variables (in our case coefficients of gaussian distributions). Fortunately EM algorithm for Gaussian Mixture model has been implemented in Scikit-learn, hence there is no need to implement it manually, However implementing EM is not hard at all. for details of EM see the reference [1]. "}}