{"cell_type":{"bf1d3c19":"code","4e0e8f8c":"code","8dd6c56c":"code","938bc64a":"code","dbf6f0d4":"code","36a077b6":"code","3c083578":"code","255f2bd1":"code","65dc51c6":"code","ef081932":"code","3bda3734":"code","16f7d54d":"code","4904fb45":"code","737bfc4d":"code","54ca5b11":"code","ff7903f5":"code","92050a47":"code","a140dbde":"code","ed9eae90":"code","5a59abd5":"code","8f56047c":"code","d14aabb4":"code","58496455":"code","cc379998":"code","fa0954ed":"code","7e55058e":"code","6b12749b":"code","f5729960":"markdown","7a31df9b":"markdown","0f2207b1":"markdown","94067349":"markdown","1eccbe09":"markdown","7b58efc8":"markdown","aac31112":"markdown","9962d6a6":"markdown","7e680657":"markdown","9378ecf5":"markdown","f24d5633":"markdown","e0499c47":"markdown","36a2bb1a":"markdown"},"source":{"bf1d3c19":"!pip install causal-impact","4e0e8f8c":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ntrain_df = pd.read_csv('..\/input\/ashrae-energy-prediction\/train.csv')\nweather_df = pd.read_csv('..\/input\/ashrae-energy-prediction\/weather_train.csv')\nmeta_df = pd.read_csv('..\/input\/ashrae-energy-prediction\/building_metadata.csv')","8dd6c56c":"train_df.head()","938bc64a":"weather_df.head()\n# There are some missing values. We should also eventually ensure that all of the values fall within a reasonable range. ","dbf6f0d4":"meta_df.head()\n# Missing values as well. ","36a077b6":"train_df['meter'].value_counts()","3c083578":"train_df['timestamp'][0] ","255f2bd1":"train_df['timestamp'] = pd.to_datetime(train_df['timestamp'])\n\ntrain_df['month'] = train_df['timestamp'].dt.month\ntrain_df['weekday'] = train_df['timestamp'].dt.dayofweek\ntrain_df['monthday'] = train_df['timestamp'].dt.day\ntrain_df['hour'] = train_df['timestamp'].dt.hour\ntrain_df['minute'] = train_df['timestamp'].dt.minute","65dc51c6":"train_df['minute'].unique() # Looks like the data doesn't go down to minute resolution. Lets drop it. ","ef081932":"train_df = train_df.drop(['minute'], axis = 1)","3bda3734":"plt.plot(train_df[train_df['building_id'] == 0]['meter_reading'], alpha = 0.8)\nplt.plot(train_df[train_df['building_id'] == 1]['meter_reading'], alpha = 0.8)\nplt.plot(train_df[train_df['building_id'] == 2]['meter_reading'], alpha = 0.8)\nplt.plot(train_df[train_df['building_id'] == 500]['meter_reading'], alpha = 0.8)","16f7d54d":"pd.plotting.lag_plot(train_df[train_df['building_id'] == 0]['meter_reading'])\nplt.plot([0,400],[0,400])\n# Look at the 3 clusters. ","4904fb45":"pd.plotting.lag_plot(train_df[train_df['building_id'] == 500]['meter_reading'])\nplt.plot([0,400],[0,400])","737bfc4d":"pd.plotting.autocorrelation_plot(train_df[train_df['building_id'] == 500]['meter_reading'])\nplt.show()\npd.plotting.autocorrelation_plot(train_df[train_df['building_id'] == 500]['meter_reading'][:300])\nplt.show()","54ca5b11":"train_df[train_df['meter'] == 2].head()","ff7903f5":"train_df[train_df['building_id'] == 745].head()","92050a47":"print(train_df[train_df['building_id'] == 745].meter.unique())\nprint(train_df[train_df['building_id'] == 1414].meter.unique())","a140dbde":"sns.distplot(weather_df['air_temperature'].dropna())\nplt.show()","ed9eae90":"all_df = pd.merge(train_df, meta_df, on = 'building_id', how = 'left')\nall_df.head()","5a59abd5":"weather_df['timestamp'] = pd.to_datetime(weather_df['timestamp']) # Convert weather to the correct format before merging\nall_df = pd.merge(all_df, weather_df, on = ['site_id', 'timestamp'], how = 'left')\nall_df['date'] = all_df['timestamp'].dt.date\nall_df.head()","8f56047c":"data = all_df.groupby(['date', 'monthday'])[['meter']].mean() \\\n        .join(all_df.groupby(['date', 'monthday'])['air_temperature'].mean()).sort_values(['date']).reset_index()\ndata.head()","d14aabb4":"data['air_temperature'].iloc[200:500].plot()\nplt.plot([264,264],[14,28])","58496455":"from causal_impact import CausalImpact\ndata['weekend'] = (pd.to_datetime(data.date).dt.dayofweek >= 5).astype(int) \/ 100 + 0.66\ndata_time = data[['meter','weekend']].iloc[200:350].rename({'meter':'y','weekend':'x1'}, axis = 1).reset_index(drop = True)\n\nci = CausalImpact(data_time, 264-200)\nci.run(max_iter=1000)\nci.plot()","cc379998":"data['meter'].iloc[200:500].plot()\nplt.plot([264,264],[0.66,0.68])","fa0954ed":"from statsmodels.regression.linear_model import OLS\ndata['constant'] = 1\nmodel = OLS(data['meter'], data[['air_temperature','weekend', 'constant']])\nresults = model.fit()\nresults.summary()","7e55058e":"plt.plot(results.resid)","6b12749b":"sns.distplot(results.resid)\nplt.show()","f5729960":"### The meters are not necessarily consecutive numbers.","7a31df9b":"### Causation\nCounterfactuals are one of the easiest ways to show causation. The goal of a counterfactual approach is to estimate how **Y** (energy use) would be different had **X** (climate) been something else. There are two common ways of establishing a counterfactual with data. \n\n- Matching: \nMatching based approaches try to replicate a controlled experiment. We can match buildings in cold weather to similar buildings in warm weather and compare differences in energy use between similar buildings. \nhttps:\/\/github.com\/benmiroglio\/pymatch\n- Model Based Counterfactuals:\nThe idea here is to \"learn\" a counter factual with a model. One way we can do this is to create a forecast assuming the temperature stays the same, and compare that forecast to the real results once the temperature changes. This isn't a great example since the weather is always changing and this approach works best with point-in-time interventions, but it illustrates how to use this approach.\nhttps:\/\/github.com\/tcassou\/causal_impact","0f2207b1":"### Some buildings have multiple power meters.","94067349":"### What does the distribution of weather look like?","1eccbe09":"\n### Then consider some relevant factors such as: How difficult it will be to answer the question, what external data can be brough in, is there any relevant research, how much will the topic impress judges?\n\n### 1. We would likely need some electricity\/building pricing information or data before and after the tax\/policy change to be able to solve it. This is an active topic of discussion in NYC, so there are probably many different opinions that can be discussed in good narrative. There might be difficulties since data is noisy, and there might not be a visible reaction from changes to electricity prices (risky). Maybe there are some other policies which can be better examined, but if this can be pulled off, it will be a top contender. \n### 2. This is very do-able, even with just linear regression. Not much external data required, just some estimates of the effects of climate change. Just create a model which predicts power usage from weather and adjust the weather to climate change predictions. Might be less impressive to the judges, but has serious potential to win if done right.\n### 3.  Solvable, but harder than 2. Would need pricing data on renewable power, and how to convert weather to power generation. Very impressive if you manage to solve it (similar idea won us the Championship).\n\n### Let's take a crack at #2 since it is the easiest. ","7b58efc8":"### Let's look at the autocorrelation of these plots, and look at a lagplot.","aac31112":"### Python Libraries to know\n- Pandas: For loading and manipulating data. \n- NumPy: Working with numbers and matrices. \n- Matplotlib (Seaborn too): For plotting and graphing data. Matplotlib will do the basics, and Seaborn can do some more advanced plots. \n- Sklearn: Has a bunch of different models and useful functions in an easy to use format.\n- Statsmodels: Many models, time series models and statistical tests. \n\n### Nice to know (but not necessary)\n- PyTorch or Tensorflow: Neural networks and gradient descent optimization. Last year there was text data for both competitions I went to, but I'm under the impression the winners didn't rely heavily on NLP.\n- LightGBM or XGBoost: Easy to use black-box gradient boosting trees models. Handles missing values automatically. You can use shapley values to \"interepret\" the model. \n- PyMC3, PyStan or other PPL: Very powerful but has a learning curve. For bayesian modeling.  \n- CVXPY: Convex optimization. Useful if you are trying to optimize over a convex function. You can always use a general optimizer (ie particle swarm, bayesian optimization, grid-search) instead. \n\n### Useful Resources:\nKaggle\n\nhttps:\/\/datasetsearch.research.google.com\/","9962d6a6":"### Now we have the data in a format we can use, so we now need to think of some problems we can solve with this data. It is a good idea to Google what relevant news stories there are around power consumption, buildings, and weather. Then also look to see if there is any relevant research and papers about the topic. \n\n### Some ideas I came up with are:\n### 1. New York recently came up with a tax on inefficient buildings. How will this tax affect power consumption?\n### 2. How will climate change affect power consumption?\n### 3. What is the most effective way to transition to renewable power? What combinaiton of Solar, Wind and Batteries would be the most cost effective and would be robust to prolonged bad weather. \n","7e680657":"##  Look at what kind of data is in these files","9378ecf5":"## Explore train_df","f24d5633":"## Look at individual buildings.","e0499c47":"## Timestamp is not in a date time format, so let's convert to the pandas date format, and then add some additional features!","36a2bb1a":"### We have 3 dataframes, but they should be merged into one so we can feed it to a model.\n### The site_id will be mapped to the building_id between the train and the meta, and the weather will be mapped to the site and time of the training data."}}