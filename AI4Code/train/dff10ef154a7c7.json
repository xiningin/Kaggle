{"cell_type":{"8063bd76":"code","418debd6":"code","8ff8dabc":"code","d71cca8a":"code","6dd46c18":"code","b4d38f60":"code","bb4afa3d":"code","56c3c127":"code","59ec56aa":"code","340329aa":"code","bb2d88f4":"code","87cb4270":"code","5e7ad915":"code","e76cb8d5":"code","868416d1":"code","4199c97a":"code","f4290c92":"code","38cbdea7":"code","06bdd0e4":"code","e3c569e6":"code","f7be7999":"code","294d63e4":"code","4fe14d79":"code","5224d5f0":"code","b7e0fc26":"code","49c0f6b1":"code","65c0c537":"code","e3bc0a44":"code","a1b1509b":"code","0b552c1b":"code","ad1e51cc":"code","22b8dfb2":"code","dad0a7ea":"code","f6de0666":"code","94e12ab0":"code","ea240960":"markdown","83f3a5ab":"markdown","98edcd67":"markdown","d228fd00":"markdown","09e44326":"markdown","aca2d203":"markdown","48e85dee":"markdown","2366f912":"markdown","91c38820":"markdown","20030512":"markdown","a95283a2":"markdown","0336f7e0":"markdown","d5a7d8b3":"markdown","5f328ba3":"markdown","18c88e29":"markdown","4efb89f3":"markdown","9d919c21":"markdown","dc495359":"markdown","89a403d3":"markdown","8c6b4a1c":"markdown","8812ebf3":"markdown","957435fb":"markdown","2f298c51":"markdown","41b0b1a5":"markdown","4ef04854":"markdown","93a1627b":"markdown","ade4ff98":"markdown","22892cbf":"markdown","0354d01c":"markdown","3ae57455":"markdown","0d7ec5b0":"markdown","ce1c69f9":"markdown","e1531399":"markdown"},"source":{"8063bd76":"import pandas as pd\nimport numpy as np\nimport math\nimport datetime\n\nimport lightgbm as lgb\nimport xgboost as xgb\nfrom sklearn.model_selection import KFold\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.linear_model import Ridge, Lasso\nfrom lightgbm.sklearn import LGBMRegressor\nfrom xgboost.sklearn import XGBRegressor\nfrom catboost import CatBoostRegressor\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin\nfrom mlxtend.regressor import StackingCVRegressor\n\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import cross_val_predict\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import make_scorer\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom IPython.display import Image\nimport warnings\n\n%matplotlib inline\nwarnings.filterwarnings('ignore')\nsns.set(style='white', context='notebook', palette='deep')","418debd6":"Image(url= \"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/1\/10\/Logo-ELO-NEG-Black.png\")","8ff8dabc":"# Historical and new transactions data\nhist_trans = pd.read_csv('..\/input\/historical_transactions.csv')\nnew_trans = pd.read_csv('..\/input\/new_merchant_transactions.csv')\n\n# Train and Test data\ntrain = pd.read_csv('..\/input\/train.csv', parse_dates=['first_active_month'])\ntest = pd.read_csv('..\/input\/test.csv', parse_dates=['first_active_month'])\n\ntrain_idx = train.shape[0]\ntest_idx = test.shape[0]\n\nprint(\"--------------------------\")\nprint(\"Train shape: \", train.shape)\nprint(\"Test shape: \", test.shape)\nprint(\"--------------------------\")\nprint(\"Historical transactions shape: \", hist_trans.shape)\nprint(\"New transactions shape: \", new_trans.shape)","d71cca8a":"print(\"----------------------------------------------------------------\")\nprint(\"Train\")\nprint(\"----------------------------------------------------------------\")\nprint(train.info())\nprint(\"\\n----------------------------------------------------------------\")\nprint(\"Test\")\nprint(\"----------------------------------------------------------------\")\nprint(train.info())\nprint(\"\\n----------------------------------------------------------------\")\nprint(\"Historical transactions\")\nprint(\"----------------------------------------------------------------\")\nprint(hist_trans.info())\nprint(\"\\n----------------------------------------------------------------\")\nprint(\"New transactions\")\nprint(\"----------------------------------------------------------------\")\nprint(new_trans.info())","6dd46c18":"train.head()","b4d38f60":"test.head()","bb4afa3d":"hist_trans.head()","56c3c127":"new_trans.head()","59ec56aa":"print(\"Target description:\\n\\n\", train['target'].describe())\nprint(\"\\n--------------------------------------------------------------------------------------------\")\nprint(\"\\nTarget values:\\n\\n\", train['target'].value_counts())","340329aa":"fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(20,6))\nax1, ax2 = axes.flatten()\n\n# Distribution\nsns.distplot(train['target'], ax=ax1, color='Green')\n\n# Sorted correlations with target\nsorted_corrs = train.corr()['target'].sort_values(ascending=False)\nsns.heatmap(train[sorted_corrs.index].corr(), ax=ax2)\n\nax1.set_title('Target Distribution')\nax2.set_title('Correlations')\nplt.show()\ndel sorted_corrs","bb2d88f4":"under_30 = train.loc[train['target'] < -30, 'target'].count()\nprint(\"Under -30:\", under_30, \"values.\")","87cb4270":"print(\"MISSING VALUES BEFORE CLEANING\\n\")\nprint(\"--------------------------------------------------\\nTrain:\\n--------------------------------------------------\\n\", train.isnull().sum())\nprint(\"\\n--------------------------------------------------\\nTest:\\n--------------------------------------------------\\n\", test.isnull().sum())\nprint(\"\\n--------------------------------------------------\\nHistorical transactions:\\n--------------------------------------------------\\n\", hist_trans.isnull().sum())\nprint(\"\\n--------------------------------------------------\\nNew transactions:\\n--------------------------------------------------\\n\", new_trans.isnull().sum())","5e7ad915":"test_missing = test[test.isnull()['first_active_month']]\nidx_test_missing = test_missing.index\ntest_missing","e76cb8d5":"same_category = test[(test['feature_1'] == 5) & (test['feature_2'] == 2) & (test['feature_3'] == 1)]\ntest.loc[idx_test_missing, 'first_active_month'] = same_category['first_active_month'].mode()[0]\n\ndel same_category\ntest.iloc[11578]","868416d1":"hist_trans.dropna(subset=['category_3', 'merchant_id'], inplace=True)","4199c97a":"hist_trans['category_2'].describe()","f4290c92":"hist_trans['category_2'].fillna((math.floor(hist_trans['category_2'].mean())), inplace=True)","38cbdea7":"new_trans.dropna(inplace=True)","06bdd0e4":"print(\"MISSING VALUES AFTER CLEANING\\n\")\nprint(\"--------------------------------------------------\\nTrain:\\n--------------------------------------------------\\n\", train.isnull().sum())\nprint(\"\\n--------------------------------------------------\\nTest:\\n--------------------------------------------------\\n\", test.isnull().sum())\n#print(\"\\n--------------------------------------------------\\nMerchant:\\n--------------------------------------------------\\n\", merchants.isnull().sum())\nprint(\"\\n--------------------------------------------------\\nHistorical transactions:\\n--------------------------------------------------\\n\", hist_trans.isnull().sum())\nprint(\"\\n--------------------------------------------------\\nNew transactions:\\n--------------------------------------------------\\n\", new_trans.isnull().sum())","e3c569e6":"# Merge train and test for data processing\ndata = pd.concat([train, test], ignore_index=True)\n\n# Check shapes match\nprint(\"Train ({}) + Test ({}) observations: {}\".format(train.shape[0], test.shape[0], train.shape[0] + test.shape[0]))\nprint(\"Merged shape:\", data.shape)\n\ndel train\ndel test","f7be7999":"# Year and month, separately\ndata['year'] = data['first_active_month'].dt.year\ndata['month'] = data['first_active_month'].dt.month\n\n# Elapsed time, until the latest date on the dataset\ndata['elapsed_time'] = (datetime.date(2018, 2, 1) - data['first_active_month'].dt.date).dt.days\n\n# Categorical features: 'feature_1', 'feature_2' and 'feature_3'\ncont = 1\nfor col in ['feature_1', 'feature_2', 'feature_3']:\n    dummy_col = pd.get_dummies(data[col], prefix='f{}'.format(cont))\n    data = pd.concat([data, dummy_col], axis=1)\n    data.drop(col, axis=1, inplace=True)\n    cont += 1\n    \ndata.head()","294d63e4":"new_trans['new'] = 1\nhist_trans['new'] = 0\n\n# Concatenate new_trans and hist_trans\ntrans_data = pd.concat([new_trans, hist_trans])\n\ndel new_trans\ndel hist_trans","4fe14d79":"# Change Yes\/No for 0\/1 in 'authorized_flag' and 'category_1'\nyes_no_dict = {'Y':1, 'N':0}\ntrans_data['authorized_flag'] = trans_data['authorized_flag'].map(yes_no_dict)\ntrans_data['category_1'] = trans_data['category_1'].map(yes_no_dict)\n\n# Create five different cols for 'category_2'\ndummy_col = pd.get_dummies(trans_data['category_2'], prefix='category_2')\ntrans_data = pd.concat([trans_data, dummy_col], axis=1)\ntrans_data.drop('category_2', axis=1, inplace=True)\n    \n# Create three different cols for categorical A\/B\/C in 'category_3'\ndummy_col = pd.get_dummies(trans_data['category_3'], prefix='cat3')\ntrans_data = pd.concat([trans_data, dummy_col], axis=1)\ntrans_data.drop('category_3', axis=1, inplace=True)\n\ntrans_data.head()","5224d5f0":"def aggregate_historical_transactions(trans_data):\n    \n    trans_data.loc[:, 'purchase_date'] = pd.DatetimeIndex(trans_data['purchase_date']).astype(np.int64)*1e-9\n    \n    agg_func = {\n        'authorized_flag': ['sum', 'mean'],\n        'category_1': ['mean'],\n        'category_2_1.0': ['mean'],\n        'category_2_2.0': ['mean'],\n        'category_2_3.0': ['mean'],\n        'category_2_4.0': ['mean'],\n        'category_2_5.0': ['mean'],\n        'cat3_A': ['mean'],\n        'cat3_B': ['mean'],\n        'cat3_C': ['mean'],\n        'merchant_id': ['nunique'],\n        'merchant_category_id': ['nunique'],\n        'state_id': ['nunique'],\n        'city_id': ['nunique'],\n        'subsector_id': ['nunique'],\n        'purchase_amount': ['count', 'sum', 'median', 'max', 'min', 'std'],\n        'installments': ['count', 'sum', 'median', 'max', 'min', 'std'],\n        'purchase_date': [np.ptp],\n        'month_lag': ['min', 'max'],\n        'new':[lambda x:x.value_counts().index[0]] # Mode\n        }\n    \n    agg_history = trans_data.groupby(['card_id']).agg(agg_func)\n    agg_history.columns = ['_'.join(col).strip() for col in agg_history.columns.values]\n    agg_history.reset_index(inplace=True)\n\n    df = (trans_data.groupby('card_id').size().reset_index(name='transactions_count'))\n    \n    agg_history = pd.merge(df, agg_history, on='card_id', how='left')\n    \n    return agg_history\n\ntrans_data = aggregate_historical_transactions(trans_data)\ntrans_data.head()","b7e0fc26":"# Merch data (train + test) with trans_data (historical + new transactions)\nprocessed_data = pd.merge(data, trans_data, on='card_id', how='left')\ndel data\ndel trans_data\nprint(processed_data.shape)\nprocessed_data.head()","49c0f6b1":"# Train and Test\ntrain = processed_data[:train_idx]\ntest = processed_data[train_idx:]\n\ndel processed_data\n\n# There are some nan values after feature eng in 'purchase_amount_std' and 'installments_std'\ncols = ['purchase_amount_std', 'installments_std']\n\nfor col in cols:\n    train[col].fillna((train[col].value_counts().index[0]), inplace=True)\n    test[col].fillna((test[col].value_counts().index[0]), inplace=True)\n\ntarget = train['target']\n\ncols_2_remove = ['target', 'card_id', 'first_active_month']\nfor col in cols_2_remove:  \n    del train[col]\n    del test[col] \n\n# Check on shapes\nprint(\"--------------------------\")\nprint(\"Train shape: \", train.shape)\nprint(\"Test shape: \", test.shape)\nprint(\"--------------------------\")","65c0c537":"lgb_params = {'num_leaves': 50,\n         'min_data_in_leaf': 30, \n         'objective':'regression',\n         'max_depth': -1,\n         'learning_rate': 0.005,\n         \"boosting\": \"gbdt\",\n         \"feature_fraction\": 0.9,\n         \"bagging_freq\": 1,\n         \"bagging_fraction\": 0.9,\n         \"bagging_seed\": 11,\n         \"metric\": 'rmse',\n         \"lambda_l1\": 0.1,\n         \"verbosity\": -1}\n\nFOLDs = KFold(n_splits=5, shuffle=True, random_state=1989)\n\noof_lgb = np.zeros(len(train))\npredictions_lgb = np.zeros(len(test))\n\nfeatures_lgb = list(train.columns)\nfeature_importance_df_lgb = pd.DataFrame()\n\nfor fold_, (trn_idx, val_idx) in enumerate(FOLDs.split(train)):\n    trn_data = lgb.Dataset(train.iloc[trn_idx], label=target.iloc[trn_idx])\n    val_data = lgb.Dataset(train.iloc[val_idx], label=target.iloc[val_idx])\n\n    print(\"LGB \" + str(fold_) + \"-\" * 50)\n    num_round = 2000\n    clf = lgb.train(lgb_params, trn_data, num_round, valid_sets = [trn_data, val_data], verbose_eval=200, early_stopping_rounds = 2000)\n    oof_lgb[val_idx] = clf.predict(train.iloc[val_idx], num_iteration=clf.best_iteration)\n\n    fold_importance_df_lgb = pd.DataFrame()\n    fold_importance_df_lgb[\"feature\"] = features_lgb\n    fold_importance_df_lgb[\"importance\"] = clf.feature_importance()\n    fold_importance_df_lgb[\"fold\"] = fold_ + 1\n    feature_importance_df_lgb = pd.concat([feature_importance_df_lgb, fold_importance_df_lgb], axis=0)\n    predictions_lgb += clf.predict(test, num_iteration=clf.best_iteration) \/ FOLDs.n_splits\n    \n\ndel fold_importance_df_lgb\ndel trn_data\ndel val_data\n\nprint(np.sqrt(mean_squared_error(oof_lgb, target)))","e3bc0a44":"train.rename(index=str, columns={\"new_<lambda>\": \"new_mode\"}, inplace=True)\ntest.rename(index=str, columns={\"new_<lambda>\": \"new_mode\"}, inplace=True)\n\nxgb_params = {'eta': 0.001, 'max_depth': 7, 'subsample': 0.8, 'colsample_bytree': 0.8, \n          'objective': 'reg:linear', 'eval_metric': 'rmse', 'silent': True}\n\nFOLDs = KFold(n_splits=5, shuffle=True, random_state=1989)\n\noof_xgb = np.zeros(len(train))\npredictions_xgb = np.zeros(len(test))\n\nfor fold_, (trn_idx, val_idx) in enumerate(FOLDs.split(train)):\n    trn_data = xgb.DMatrix(data=train.iloc[trn_idx], label=target.iloc[trn_idx])\n    val_data = xgb.DMatrix(data=train.iloc[val_idx], label=target.iloc[val_idx])\n    watchlist = [(trn_data, 'train'), (val_data, 'valid')]\n    print(\"xgb \" + str(fold_) + \"-\" * 50)\n    num_round = 2000\n    xgb_model = xgb.train(xgb_params, trn_data, num_round, watchlist, early_stopping_rounds=100, verbose_eval=200)\n    oof_xgb[val_idx] = xgb_model.predict(xgb.DMatrix(train.iloc[val_idx]), ntree_limit=xgb_model.best_ntree_limit+50)\n\n    predictions_xgb += xgb_model.predict(xgb.DMatrix(test), ntree_limit=xgb_model.best_ntree_limit+50) \/ FOLDs.n_splits\n\ndel trn_data\ndel val_data\ndel watchlist\n\nnp.sqrt(mean_squared_error(oof_xgb, target))","a1b1509b":"print(\"-----------------\\nScores on train\\n-----------------\")\nprint('lgb:', np.sqrt(mean_squared_error(oof_lgb, target)))\nprint('xgb:', np.sqrt(mean_squared_error(oof_xgb, target)))\n\ntotal_sum = 0.5*oof_lgb + 0.5*oof_xgb\n\nprint(\"CV score: {:<8.5f}\".format(mean_squared_error(total_sum, target)**0.5))","0b552c1b":"cols = (feature_importance_df_lgb[[\"feature\", \"importance\"]]\n        .groupby(\"feature\")\n        .mean()\n        .sort_values(by=\"importance\", ascending=False)[:1000].index)\n\nbest_features = feature_importance_df_lgb.loc[feature_importance_df_lgb.feature.isin(cols)]\n\nplt.figure(figsize=(14,14))\nsns.barplot(x=\"importance\",\n            y=\"feature\",\n            data=best_features.sort_values(by=\"importance\",\n                                           ascending=False))\nplt.title('LightGBM Features (avg over folds)')\nplt.tight_layout()\nplt.savefig('lgbm_importances.png')\ndel feature_importance_df_lgb","ad1e51cc":"# Model definition\ntrain_y = target\n\n# Same lgbm and xgb models as before\nlgbm_model = LGBMRegressor(\n                objective=\"regression\", metric=\"rmse\", \n                max_depth=7, min_child_samples=20, \n                reg_alpha= 1, reg_lambda=1,\n                num_leaves=64, learning_rate=0.001, \n                subsample=0.8, colsample_bytree=0.8, \n                verbosity=-1\n)\n\nxgb_model = XGBRegressor(\n                eta=0.001, max_depth=7, \n                subsample=0.8, colsample_bytree=0.8, \n                objective='reg:linear', eval_metric='rmse', \n                silent=True\n)\n\n\n# Test catboost, random forest, decision tree, knn, ridge and lasso models individual performance, for averaged and stacked model\ncatboost_model = CatBoostRegressor(iterations=150)\nrf_model = RandomForestRegressor(n_estimators=25, min_samples_leaf=25, min_samples_split=25)\ntree_model = DecisionTreeRegressor(min_samples_leaf=25, min_samples_split=25)\nknn_model = KNeighborsRegressor(n_neighbors=25, weights='distance')\nridge_model = Ridge(alpha=75.0)\nlasso_model = Lasso(alpha=0.75)\n\n# ------------------------------------------------------------------------------------------------\n# Average regressor\nclass AveragingRegressor(BaseEstimator, RegressorMixin, TransformerMixin):\n    def __init__(self, regressors):\n        self.regressors = regressors\n        self.predictions = None\n\n    def fit(self, X, y):\n        for regr in self.regressors:\n            regr.fit(X, y)\n        return self\n\n    def predict(self, X):\n        self.predictions = np.column_stack([regr.predict(X) for regr in self.regressors])\n        return np.mean(self.predictions, axis=1)\n    \n# Averaged & stacked models \naveraged_model = AveragingRegressor([catboost_model, xgb_model, rf_model, lgbm_model])\n\n\nstacked_model = StackingCVRegressor(\n    regressors=[catboost_model, xgb_model, rf_model, lgbm_model],\n    meta_regressor=Ridge()\n)\n\n# Test performance\ndef rmse_fun(predicted, actual):\n    return np.sqrt(np.mean(np.square(predicted - actual)))\n\nrmse = make_scorer(rmse_fun, greater_is_better=False)\n\nmodels = [\n     ('CatBoost', catboost_model),\n     ('XGBoost', xgb_model),\n     ('LightGBM', lgbm_model),\n     ('DecisionTree', tree_model),\n     ('RandomForest', rf_model),\n     ('Ridge', ridge_model),\n     ('Lasso', lasso_model),\n     ('KNN', knn_model),\n     ('Averaged', averaged_model),\n     ('Stacked', stacked_model),\n]\n\n\nscores = [\n    -1.0 * cross_val_score(model, train.values, train_y.values, scoring=rmse).mean()\n    for _,model in models\n]","22b8dfb2":"dataz = pd.DataFrame({ 'Model': [name for name, _ in models], 'Error (RMSE)': scores })\ndataz.plot(x='Model', kind='bar')\nplt.savefig('stacked_scores.png')","dad0a7ea":"dataz","f6de0666":"# Stacked model predictions (best score)\nstacked_model.fit(train.values, target.values)    \npredictions_stacked = stacked_model.predict(test.values)","94e12ab0":"# LightGBM\/Xgboost\nsub_df = pd.read_csv('..\/input\/sample_submission.csv')\nsub_df[\"target\"] = 0.5 * predictions_lgb + 0.5 * predictions_xgb\nsub_df.to_csv(\"submission_lgbxgboost.csv\", index=False)\n\n# Stacked\nsub_df = pd.read_csv('..\/input\/sample_submission.csv')\nsub_df[\"target\"] = predictions_stacked\nsub_df.to_csv(\"submission_stacked.csv\", index=False)","ea240960":"<a id='section5'><\/a>\n## Read in the data","83f3a5ab":"For 'category_2', since it has about 10% of missing values, let's replace them with the rounded average value (since values for this column include [1.0, 2.0, 3.0, 4.0, 5.0]), as seen below.","98edcd67":"<a id='section25'><\/a>\n## Feature importance","d228fd00":"<a id='section30'><\/a>\n# Submission","09e44326":"More preprocessing: 'category_1', 'category_2' and 'category_3'.","aca2d203":"<a id='section6'><\/a>\n### Description","48e85dee":"Similarly, in this case let's drop the missing rows for 'category_3' and 'merchant_id' (less than 1% of total).","2366f912":"<a id='section13'><\/a>\n### New transactions\nOnce more, since there are missing values in 'category_3', 'merchant_id', 'category_2' and they add up to no more than about 5%, let's  drop the corresponding rows.","91c38820":"<a id='section15'><\/a>\n## Train\/Test","20030512":"<a id='section14'><\/a>\n# Feature engineering","a95283a2":"<a id='section12'><\/a>\n### Historical transactions","0336f7e0":"Lastly, let's confirm no null values are present after cleaning.","d5a7d8b3":"<a id='section10'><\/a>\n### Train\/Test\nThere's no null values for train. However, there seem to be one observation with a missing 'first_active_month' in test.","5f328ba3":"---\n<a id='section20'><\/a>\n# Data preparation\n<a id='section56'><\/a>\n## Merge","18c88e29":"<a id='section2'><\/a>\n# Import libraries","4efb89f3":"# Table of contents\n*  [Introduction](#section1) \n*  [Libraries](#section2) \n*  [EDA](#section4)\n      - [Read the data in](#section5)\n          - [Description](#section6)\n          - [Target column](#section7)\n      - [Cleaning](#section8)\n          - [Missing values](#section9)\n              - [Train\/Test](#section10)\n              - [Historical transactions](#section12)\n              - [New transactions](#section13)\n      - [Feature engineering](#section14)\n          - [Train\/Test](#section15)\n          - [Historical + New transactions](#section18)\n          - [Aggregate function](#section40)   \n* [Data preparation](#section20)     \n   - [Merge](#section56)\n   - [Final Train\/Test](#section57)\n* [Modeling and testing](#section21)\n    - [LightGBM](#section22)\n    - [Xgboost](#section23)\n    - [Summary of results](#section24)\n    - [Feature importance](#section25)\n    - [Ensembled model: averaged and stacked](#section26)\n        - [Model definition](#section27)\n        - [Predictions](#section28)\n        - [Results](#section29)\n* [Submission](#section30)\n* [References](#section31)\n\nby @samaxtech\n\nIf you found this kernel helpful or would like to share your thoughts feel free to upvote and leave a comment! :)","9d919c21":"---\n<a id='section1'><\/a>\n# Introduction\nAs part of the Elo Merchant Category Recommendation Kaggle competition, this kernel aims to predict a merchant loyalty score for a certain credict card holder.","dc495359":"We can look for all the observations that match the same 'feature_1', 'feature_2' and 'feature_3' values as that, and replace it with the 'first_active_month' that corresponds to their mode.","89a403d3":"<a id='section23'><\/a>\n## Xgboost","8c6b4a1c":"<a id='section8'><\/a>\n# Cleaning\n<a id='section9'><\/a>\n## Missing","8812ebf3":"<a id='section24'><\/a>\n## Summary of results","957435fb":"<a id='section28'><\/a>\n## Results","2f298c51":"<a id='section22'><\/a>\n## LightGBM","41b0b1a5":"<a id='section29'><\/a>\n## Predictions","4ef04854":"---\n<a id='section40'><\/a>\n### Aggregate function\nAggregate function, grouped by 'card_id': min, max, mean, median, std, sum, nunique, range. \n\nAdded:\n- Count on 'installments' and 'purchase_amount'.\n- Mode() on 'new' column (previously created).\n- Mean on new trans_data's category_2 dummy columns.\n- Mean on trans_data's category_4.\n- Mean on 'cat3_A', 'cat3_B' and 'cat3_C' (old 'category_3').\n- Mean on merchants' new dummy columns.","93a1627b":"---\n<a id='section31'><\/a>\n# References\nSpecial thanks to the following references:\n- https:\/\/www.kaggle.com\/mjbahmani\/a-data-science-framework-for-elo (@mjbahmani)\n- https:\/\/www.kaggle.com\/youhanlee\/hello-elo-ensemble-will-help-you (@youhanlee)\n- https:\/\/www.kaggle.com\/peterhurford\/you-re-going-to-want-more-categories-lb-3-737 (@peterhurford)\n- https:\/\/www.kaggle.com\/eikedehling\/comparing-models-xgb-lgb-rf-and-stacking (@eikedehling)","ade4ff98":"<a id='section18'><\/a>\n### Historical + New transactions\nLet's create a column called 'new' on 'hist_trans' and 'new_trans' such that, before concatening them, they have the age reference:\n\n- 1: New\n- 0: Historical","22892cbf":"<a id='section7'><\/a>\n### Target column","0354d01c":"---\n<a id='section21'><\/a>\n# Modeling and testing","3ae57455":"<a id='section57'><\/a>\n## Final Train\/Test ","0d7ec5b0":"There seem to be 2207 values around -33 for the target column, which follows a normal distribution. Let's take that into account. Also, 'feature_3' correlates better than 'feature_2' and 'feature_1' with 'target'.\n\nLet's confirm the number of values under -30.","ce1c69f9":"---\n<a id='section26'><\/a>\n# Ensembled model: averaged and stacked\nThe follwing tests lgbm, xgb, catboost, random forest, decision tree, knn, ridge and lasso models individual performance, and compared for averaged and stacked models.\n<a id='section27'><\/a>\n## Model definition","e1531399":"---\n<a id='section4'><\/a>\n# EDA "}}