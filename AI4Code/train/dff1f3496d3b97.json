{"cell_type":{"381df760":"code","6031d779":"code","7d8c138a":"code","e7558d3e":"code","bb784faa":"code","f0d3bc7c":"code","25fd1276":"code","528b8758":"code","59b51b6c":"code","15f0fc71":"code","85a11e48":"code","e2b15115":"code","8ceab477":"code","b7b13c4a":"code","a0a0dc89":"code","e42a1227":"code","8d8a93de":"code","5186366c":"code","94286a8e":"code","d2041891":"code","2773be67":"code","c22c8ab4":"code","4049df12":"code","21cfc973":"code","4ac006c5":"code","0918c1b2":"code","b06d8cf4":"code","dcb5e671":"code","4e185b73":"code","7792bae5":"code","4b41d752":"code","86b0ea76":"code","ce1b20de":"code","2c77531e":"code","07ba5b54":"code","ee644b19":"code","8ea07e21":"code","13cd4ae8":"code","c7c4fc5a":"code","91c42c49":"code","a5ef43b2":"code","a784d4b6":"code","84921be7":"code","5cc9bb91":"code","abdc5355":"code","bfe21f6c":"code","5e05ecab":"code","8f764a03":"code","a725acb6":"code","43998529":"code","fcb80a13":"code","78ea9520":"code","3b14396f":"code","5ee28d9e":"code","d6a942e5":"code","f9e956ea":"code","bd2dd5ec":"code","49dcd15e":"code","ad77c087":"code","ea1ba07c":"code","b643f95d":"code","86890a3b":"code","26c183e4":"code","9730433d":"code","09612df6":"code","4658df66":"code","ff598468":"code","bf6cf02c":"code","73d1a4a6":"code","6cfb433b":"code","61cb074f":"code","a2197034":"code","77c67b78":"code","4246aa91":"code","50161d78":"code","c00a2ebf":"code","d869ee9c":"code","cebb5162":"code","60bfcf4d":"code","5cf28ed7":"markdown","fb380cab":"markdown","4b683457":"markdown","4f9b7c24":"markdown","72bd93a5":"markdown","0140e313":"markdown","1ebe8ca1":"markdown","d5f8a8a9":"markdown","efd830c3":"markdown","99ed9049":"markdown","cb221f02":"markdown","468584a9":"markdown","431964e5":"markdown","ed3afbd3":"markdown","6da0f547":"markdown","c4f36371":"markdown","9c9ec271":"markdown","89579748":"markdown","87e28efa":"markdown","7277a624":"markdown","3838f102":"markdown","c318ffeb":"markdown","58b3f1f5":"markdown","5c43d765":"markdown","a2d83445":"markdown","ee79b231":"markdown","624abd07":"markdown","cc500ad8":"markdown","ee437ba0":"markdown","ab2c9b59":"markdown"},"source":{"381df760":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n!pip install dataprep\nfrom dataprep.eda import *\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nsns.set_theme(style = \"darkgrid\")\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","6031d779":"conda install -c conda-forge swifter","7d8c138a":"# read data\npath = \"..\/input\/tabular-playground-series-dec-2021\/train.csv\"\ntest_path = \"..\/input\/tabular-playground-series-dec-2021\/test.csv\"\npsuedo_labels_path = '..\/input\/tps12-pseudolabels\/tps12-pseudolabels_v2.csv'\n\ndata = pd.read_csv(path)\npsuedo_labels = pd.read_csv(psuedo_labels_path)","e7558d3e":"# combine given and psuedo labels\ndata = pd.concat([data, psuedo_labels], axis=0)","bb784faa":"# reset index\ndata.reset_index(drop=True, inplace = True)","f0d3bc7c":"# display top 10 rows\ndata.head(10)","25fd1276":"# number of rows and columns in dataset\nrows = data.shape[0]\ncolumns = data.shape[1]\nprint(\"Data has {} rows, {} columns\".format(rows, columns))","528b8758":"data = data.drop(['Id'], axis = 1)","59b51b6c":"from sklearn.model_selection import train_test_split\n\n# set of independent variables\nX = data.drop(['Cover_Type'], axis = 1)\n# dependent variable\ny = data['Cover_Type']\n\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size = 0.3, random_state=15)","15f0fc71":"del X\ndel y\ndel data","85a11e48":"def reduce_mem_usage(df, verbose=True):\n    ''' \n    optimises memory usage \n    \n    Args:\n        df (Pandas DataFrame) : The dataset to optimise\n    \n    Returns:\n        df : Pandas DataFrame\n    '''\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() \/ 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) \/ start_mem))\n    return df","e2b15115":"# reduce memory usage\nX_train = reduce_mem_usage(X_train)\nX_val = reduce_mem_usage(X_val)","8ceab477":"X_train[\"soiltype_label\"] = 0\nX_val[\"soiltype_label\"] = 0\n\nX_train[\"soiltype_label\"] = X_train[\"soiltype_label\"].astype(np.int64)\nX_val[\"soiltype_label\"] = X_val[\"soiltype_label\"].astype(np.int64)\n\nsoil_columns = [x for x in X_train.columns if x.startswith(\"Soil_Type\")]","b7b13c4a":"def make_40_bit_int_from_soiltype(row):\n    ''' \n    creates 40 bit integer for a given\n    \n    Args:\n        Row (Series) : series to create feature for\n    \n    Returns:\n        int : 40bit integer value\n    '''\n    value = 0\n    for column in soil_columns:\n        value |= row[column]\n        value = value << 1\n    return value","a0a0dc89":"import swifter\nX_train[\"soiltype_label\"] = X_train.swifter.apply(make_40_bit_int_from_soiltype, axis=1)","e42a1227":"X_val[\"soiltype_label\"] = X_val.swifter.apply(make_40_bit_int_from_soiltype, axis=1)","8d8a93de":"def make_5_8_bit_ints_from_soiltype(row):\n    integer1 = (np.int64(row[\"soiltype_label\"]) & 0xFF00000000) >> 30\n    integer2 = (np.int64(row[\"soiltype_label\"]) & 0x00FF000000) >> 24\n    integer3 = (np.int64(row[\"soiltype_label\"]) & 0x0000FF0000) >> 16\n    integer4 = (np.int64(row[\"soiltype_label\"]) & 0x000000FF00) >> 8\n    integer5 = (np.int64(row[\"soiltype_label\"]) & 0x00000000FF)\n    return integer1, integer2, integer3, integer4, integer5","5186366c":"X_train[[\"soiltype_int1\", \"soiltype_int2\", \"soiltype_int3\", \"soiltype_int4\", \"soiltype_int5\"]] = X_train.swifter.apply(make_5_8_bit_ints_from_soiltype, axis=1, result_type=\"expand\")\nX_val[[\"soiltype_int1\", \"soiltype_int2\", \"soiltype_int3\", \"soiltype_int4\", \"soiltype_int5\"]] = X_val.swifter.apply(make_5_8_bit_ints_from_soiltype, axis=1, result_type=\"expand\")","94286a8e":"X_train = X_train.drop(['soiltype_label'], axis = 1)\n\nX_val = X_val.drop(['soiltype_label'], axis=1)","d2041891":"pd.set_option('display.max_columns', 100)\n# display descriptive stats\nX_train.describe()","2773be67":"plot(X_train, col1 = 'Elevation')","c22c8ab4":"plot(X_train, col1 = 'Aspect')","4049df12":"# select all rows with incorrect values\nincorrect_aspect_train = ((X_train['Aspect']<0) | (X_train['Aspect']>360))\nincorrect_aspect_val = ((X_val['Aspect']<0) | (X_val['Aspect']>360))\n\n# correct values\nX_train.loc[incorrect_aspect_train, 'Aspect'] = X_train[incorrect_aspect_train]['Aspect']%360\nX_val.loc[incorrect_aspect_val, 'Aspect'] = X_val[incorrect_aspect_val]['Aspect']%360\n\ndel incorrect_aspect_train\ndel incorrect_aspect_val","21cfc973":"plot(X_train, col1 = \"Aspect\")","4ac006c5":"plot(X_train, col1 = 'Slope')","0918c1b2":"plot(X_train, col1 = 'Horizontal_Distance_To_Hydrology')","b06d8cf4":"# correct negative values\nX_train['Horizontal_Distance_To_Hydrology'] = abs(X_train['Horizontal_Distance_To_Hydrology'])\nX_val['Horizontal_Distance_To_Hydrology'] = abs(X_val['Horizontal_Distance_To_Hydrology'])","dcb5e671":"plot(X_train, col1 = 'Horizontal_Distance_To_Hydrology')","4e185b73":"plot(X_train, col1 = 'Vertical_Distance_To_Hydrology')","7792bae5":"# correct negative values\nX_train['Vertical_Distance_To_Hydrology'] = abs(X_train['Vertical_Distance_To_Hydrology'])\nX_val['Vertical_Distance_To_Hydrology'] = abs(X_val['Vertical_Distance_To_Hydrology'])","4b41d752":"plot(X_train, col1 = 'Vertical_Distance_To_Hydrology')","86b0ea76":"plot(X_train, col1 = 'Horizontal_Distance_To_Roadways')","ce1b20de":"# correct negative values\nX_train['Horizontal_Distance_To_Roadways'] = abs(X_train['Horizontal_Distance_To_Roadways'])\nX_val['Horizontal_Distance_To_Roadways'] = abs(X_val['Horizontal_Distance_To_Roadways'])","2c77531e":"plot(X_train, col1 = 'Horizontal_Distance_To_Roadways')","07ba5b54":"plot(X_train, col1 = 'Hillshade_9am')","ee644b19":"# correct negative values\nX_train['Hillshade_9am'] = abs(X_train['Hillshade_9am'])\nX_val['Hillshade_9am'] = abs(X_val['Hillshade_9am'])","8ea07e21":"# selecting incorrect values\nincorrect_hillshade_9am_train = (X_train['Hillshade_9am']>255)\nincorrect_hillshade_9am_val = (X_val['Hillshade_9am']>255)\n\n# correcting values\nX_train.loc[incorrect_hillshade_9am_train, 'Hillshade_9am'] = X_train[incorrect_hillshade_9am_train]['Hillshade_9am']%255\nX_val.loc[incorrect_hillshade_9am_val, 'Hillshade_9am'] = X_val[incorrect_hillshade_9am_val]['Hillshade_9am']%255\n\ndel incorrect_hillshade_9am_train\ndel incorrect_hillshade_9am_val","13cd4ae8":"plot(X_train, col1 = 'Hillshade_9am')","c7c4fc5a":"plot(X_train, col1 = 'Hillshade_Noon')","91c42c49":"# correct negative values\nX_train['Hillshade_Noon'] = abs(X_train['Hillshade_Noon'])\nX_val['Hillshade_Noon'] = abs(X_val['Hillshade_Noon'])","a5ef43b2":"# selecting incorrect values\nincorrect_hillshade_noon_train = (X_train['Hillshade_Noon']>255)\nincorrect_hillshade_noon_val = (X_val['Hillshade_Noon']>255)\n\n# correcting values\nX_train.loc[incorrect_hillshade_noon_train, 'Hillshade_Noon'] = X_train[incorrect_hillshade_noon_train]['Hillshade_Noon']%255\nX_val.loc[incorrect_hillshade_noon_val, 'Hillshade_Noon'] = X_val[incorrect_hillshade_noon_val]['Hillshade_Noon']%255\n\ndel incorrect_hillshade_noon_train\ndel incorrect_hillshade_noon_val","a784d4b6":"plot(X_train, col1='Hillshade_Noon')","84921be7":"plot(X_train, col1 = 'Hillshade_3pm')","5cc9bb91":"# correct negative values\nX_train['Hillshade_3pm'] = abs(X_train['Hillshade_3pm'])\nX_val['Hillshade_3pm'] = abs(X_val['Hillshade_3pm'])","abdc5355":"# selecting incorrect values\nincorrect_hillshade_3pm_train = (X_train['Hillshade_3pm']>255)\nincorrect_hillshade_3pm_val = (X_val['Hillshade_3pm']>255)\n\n# correcting values\nX_train.loc[incorrect_hillshade_3pm_train, 'Hillshade_3pm'] = X_train[incorrect_hillshade_3pm_train]['Hillshade_3pm']%255\nX_val.loc[incorrect_hillshade_3pm_val, 'Hillshade_3pm'] = X_val[incorrect_hillshade_3pm_val]['Hillshade_3pm']%255\n\ndel incorrect_hillshade_3pm_train\ndel incorrect_hillshade_3pm_val","bfe21f6c":"plot(X_train, col1='Hillshade_3pm')","5e05ecab":"plot(X_train, col1 = 'Horizontal_Distance_To_Fire_Points')","8f764a03":"# correct negative values\nX_train['Horizontal_Distance_To_Fire_Points'] = abs(X_train['Horizontal_Distance_To_Fire_Points'])\nX_val['Horizontal_Distance_To_Fire_Points'] = abs(X_val['Horizontal_Distance_To_Fire_Points'])","a725acb6":"plot(X_train, col1 = 'Horizontal_Distance_To_Fire_Points')","43998529":"bin_columns = X_train.columns[10:]\nfor i in bin_columns:\n    plot(X_train, col1=i).show()","fcb80a13":"train = X_train.copy()\ntrain['cover'] = y_train","78ea9520":"train_sample = train.sample(n=100000)\nplot_correlation(train_sample, config = {'height': 800, 'width': 800, })","3b14396f":"del train\ndel train_sample","5ee28d9e":"X_train = reduce_mem_usage(X_train)\nX_val = reduce_mem_usage(X_val)","d6a942e5":"def removeSkew(X, skew_index):\n    ''' \n    Removes columns with skew distribution using \"skew_index\" array\n    \n    Args:\n        X (Numpy Array) : The dataset to remove skew columns from\n        skew_index (Numpy Array) : List of columns to remove from \"X\"\n    \n    Returns:\n        Numpy Array : The data without skew columns\n    '''\n    return np.array(pd.DataFrame(X).drop(skew_index, axis=1))","f9e956ea":"def reportSkewness(X, columns_to_drop):\n    '''\n    Reports columns with skew distribution in given dataset X\n    \n    Args:\n        X (DataFrame) : The dataset to check skewness in\n        columns_to_drop (list) : columns to not use in analyses\n        \n    Returns:\n        list : list of columns with skew distribution\n    '''\n    skew = X.drop(columns_to_drop, axis = 1).skew()\n\n#     columns with asymmetrical distribution\n    skew_index = np.array(skew[~((skew>=-0.5) & (skew <= 0.5))].index)\n\n    print(\"Number of columns with skew distribution : {}\".format(len(skew_index)))\n    return skew_index","bd2dd5ec":"# columns with categorical values types\ncategorical_columns = X_train.columns[10:]\nskew_columns = reportSkewness(X_train, categorical_columns)","49dcd15e":"skew_columns","ad77c087":"for column in skew_columns:\n    if(column!='Slope'):\n#     set up plot\n        f, ax = plt.subplots(nrows=1, ncols=2, figsize = (10, 7))\n#     plot before transformation\n        sns.distplot(X_train[column], ax = ax[0])\n#     apply square root transformation\n        X_train[column] = np.sqrt(X_train[column])\n        X_val[column] = np.sqrt(X_val[column])\n#     plot after transformation\n        sns.distplot(X_train[column], ax = ax[1])\n        plt.show()","ea1ba07c":"def handleOutliers(data, target, to_return = False):\n    ''' \n    Removes outliers from each column and reports the data loss\n    \n    Args:\n        data (DataFrame) : The DataFrame to remove outliers from\n        target : target variable\n        to_return (bool) :  - Default value False\n                            - Whether to return the DataFrame after removing outliers\n    \n    Returns:\n        DataFrame : data free from outliers\n    '''\n#     calculate first quantile\n    Q1 = data.quantile(0.25)\n#     calculate third quantile\n    Q3 = data.quantile(0.75)\n#     calculate inter quartile range\n    IQR1 = Q3-Q1\n\n#     initialise data w\/o outliers (drop outliers)\n    data_c = data[~((data < (Q1-1.5*IQR1))|(data > (Q3+1.5*IQR1))).any(axis = 1)] \n    y_train = target[~((data < (Q1-1.5*IQR1))|(data > (Q3+1.5*IQR1))).any(axis = 1)] \n    \n#     report data loss\n    print('Data loss is {}%'.format(((len(data) - len(data_c))\/len(data))*100))\n    \n    if(to_return):\n        return (data_c, y_train)","b643f95d":"handleOutliers(X_train.drop(bin_columns, axis=1), y_train)","86890a3b":"def countOutliers(data, column):\n    ''' \n    Calculates the number of outliers in given column\n    \n    Args:\n        data (DataFrame) : The dataset in form of Pandas DataFrame\n        column (string) : The column to report number of outliers in\n    \n    Returns:\n        int : percentage of outliers in column\n    '''\n#     calculate first quantile\n    Q1 = data[column].quantile(0.25)\n#     calculate third quantile\n    Q3 = data[column].quantile(0.75)\n#     calculate inter quartile range\n    IQR1 = Q3-Q1\n    \n#     % of outliers in the column\n    return (len(data[((data[column] < (Q1-1.5*IQR1))|(data[column] > (Q3+1.5*IQR1)))])\/len(data))*100","26c183e4":"def columnWiseOutliers(data):\n    ''' \n    Calculates the number of outliers in each column\n    \n    Args:\n        data (DataFrame) : The dataset in form of Pandas DataFrame\n    \n    Returns:\n        DataFrame : percentage of outliers in columns\n    '''\n#     percentage of outliers in each column\n    outliers = []\n\n    for column in data.columns:\n        outliers.append([column, countOutliers(data, column)])\n#     sort in decreasing order\n    outliers.sort(key = lambda x: x[1], reverse = True)\n#     convert to DataFrame\n    df = pd.DataFrame(outliers, columns=['Col', '%Outliers'])\n    return df","9730433d":"columnWiseOutliers(X_train.drop(bin_columns, axis =1))","09612df6":"X_trainc, y_train = handleOutliers(X_train.drop(bin_columns, axis =1), y_train, True)","4658df66":"X_train = X_train.loc[X_trainc.index]","ff598468":"del X_trainc","bf6cf02c":"# columns with single values\nsingle_val_cols = ['Soil_Type7', 'Soil_Type15']\n\n# drop single valued columns\nX_train=X_train.drop(single_val_cols, axis=1)\nX_val=X_val.drop(single_val_cols, axis=1)","73d1a4a6":"# compute Euclidean distance to hydrology\nX_train['Dist_To_Hydro'] = ((X_train['Horizontal_Distance_To_Hydrology'])**2 + (X_train['Vertical_Distance_To_Hydrology'])**2)**0.5\n\n# compute Manhattan distance to hydrology\nX_train['MDist_To_Hydro'] = X_train['Horizontal_Distance_To_Hydrology']+X_train['Vertical_Distance_To_Hydrology']\n\n# applying other operations\nX_train['V_Dist_Hydro_Min_Elev'] = abs(X_train['Vertical_Distance_To_Hydrology']-X_train['Elevation'])\n\nX_train['V_Dist_Hydro_Add_Elev'] = X_train['Vertical_Distance_To_Hydrology']+X_train['Elevation']\n\nX_train['H_Dist_Hydro_Min_Elev'] = abs(X_train['Horizontal_Distance_To_Hydrology']-X_train['Elevation'])\n\nX_train['H_Dist_Hydro_Add_Elev'] = X_train['Horizontal_Distance_To_Hydrology']+X_train['Elevation']\n\nX_train['Slope_Per_Elev'] = X_train['Slope']\/X_train['Elevation']","6cfb433b":"# apply for test data\nX_val['Dist_To_Hydro'] = ((X_val['Horizontal_Distance_To_Hydrology'])**2 + (X_val['Vertical_Distance_To_Hydrology'])**2)**0.5\n\nX_val['MDist_To_Hydro'] = X_val['Horizontal_Distance_To_Hydrology']+X_val['Vertical_Distance_To_Hydrology']\n\nX_val['V_Dist_Hydro_Min_Elev'] = abs(X_val['Vertical_Distance_To_Hydrology']-X_val['Elevation'])\n\nX_val['V_Dist_Hydro_Add_Elev'] = X_val['Vertical_Distance_To_Hydrology']+X_val['Elevation']\n\nX_val['H_Dist_Hydro_Min_Elev'] = abs(X_val['Horizontal_Distance_To_Hydrology']-X_val['Elevation'])\n\nX_val['H_Dist_Hydro_Add_Elev'] = X_val['Horizontal_Distance_To_Hydrology']+X_val['Elevation']\n\nX_val['Slope_Per_Elev'] = X_val['Slope']\/X_val['Elevation']","61cb074f":"# Soil type count\nsoil_features = [x for x in X_train.columns if x.startswith(\"Soil_Type\")]\nX_train[\"Soil_Type_Count\"] = X_train[soil_features].sum(axis=1)\nX_val[\"Soil_Type_Count\"] = X_val[soil_features].sum(axis=1)\n\n# Wilderness area count\nwilderness_features = [x for x in X_train.columns if x.startswith(\"Wilderness_Area\")]\nX_train[\"Wilderness_Area_Count\"] = X_train[wilderness_features].sum(axis=1)\nX_val[\"Wilderness_Area_Count\"] = X_val[wilderness_features].sum(axis=1)","a2197034":"from sklearn.preprocessing import RobustScaler\n\n\ncols = [\n    \"Elevation\",\n    \"Aspect\",\n    \"Dist_To_Hydro\",\n    \"MDist_To_Hydro\",\n    \"Soil_Type_Count\",\n    \"Wilderness_Area_Count\",\n    \"Slope\",\n    \"Horizontal_Distance_To_Hydrology\",\n    'V_Dist_Hydro_Min_Elev',\n    'V_Dist_Hydro_Add_Elev',\n    'H_Dist_Hydro_Min_Elev',\n    'H_Dist_Hydro_Add_Elev',\n    \"Vertical_Distance_To_Hydrology\",\n    \"Horizontal_Distance_To_Roadways\",\n    \"Hillshade_9am\",\n    \"Hillshade_Noon\",\n    \"Hillshade_3pm\",\n    \"Horizontal_Distance_To_Fire_Points\",\n]\n\nscaler = RobustScaler()\nX_train[cols] = scaler.fit_transform(X_train[cols])\nX_val[cols] = scaler.transform(X_val[cols])","77c67b78":"f, ax = plt.subplots(nrows = 1, ncols = 2, figsize = (15, 12)) # set up plot \n\n# visualise distribution in Ytrain\n\nsns.countplot(x = y_train, ax = ax[0]) # plot bar plot\n\n# plot a pie chart\nax[1].pie(x = y_train.value_counts().values, labels = y_train.value_counts().index, autopct = \"%.2f%%\")\nax[1].set_title(\"Percentage distribution\") # set title\n\n# display plots\nplt.show()","4246aa91":"from sklearn.metrics import accuracy_score\n\ndef getScore(clf, X, y):\n    ''' \n    Calculates f1_score for the given data using \"clf\"\n    \n    Args:\n        clf : The instance for classification algorithm\n        X (Numpy Array) : The dependent variables\n        y (Numpy Array) : The independent variables\n    \n    Returns:\n        float : The score calculated\n    '''\n    return accuracy_score(y, clf.predict(X))\n","50161d78":"def printReport(clf, X_train, y_train, X_val, y_val):\n    ''' \n    reports accuracy_score for the training and validation data using \"clf\"\n    \n    Args:\n        clf : The instance for classification algorithm\n        X_train (Numpy Array) : The dependent variables for training\n        y_train (Numpy Array) : The independent variables for training\n        X_val (Numpy Array) : The dependent variables for validation\n        y_val (Numpy Array) : The independent variables for validation\n    \n    Returns:\n        (float, float) : Training & Validation score\n    '''\n#     find scores\n    train_score = getScore(clf, X_train, y_train)\n    val_score = getScore(clf, X_val, y_val)\n    \n    print(\"Training Score : {}\\nValidation Score: {}\".format(train_score, val_score))\n    \n    return (train_score, val_score)","c00a2ebf":"from xgboost import XGBClassifier\n\n# intialise XGB algorithm\nclf_xgb = XGBClassifier(tree_method='gpu_hist', eta = 0.5)\n\n# fit on training data\nclf_xgb.fit(X_train, y_train)\n\nxgb_scores = printReport(clf_xgb, X_train, y_train, X_val, y_val)","d869ee9c":"# preds = clf_xgb.predict(X_val)","cebb5162":"# res = pd.DataFrame(test_id, columns = ['Id'])\n# res['Cover_Type'] = preds","60bfcf4d":"# res.to_csv(\"res6.csv\", index=False)","5cf28ed7":"**SLOPE**\n\nDistribution : Positively Skewed\n\nOutlier : Yes\n\nNull : No\n\nIncorrect Values : No (Assuming negative values denote a down slope)","fb380cab":"- Wilderness Area 1 and 3 are strongly correlated\n- Elevation is a good indicator of Cover_Type, followed by Wilderness_Area4 & 1, 3, Soil_Type39, Horizontal_Distance_To_Roadways\n- Wilderness_Area1 and 3 are correlated to Soil_Type29\n- Wilderness_Area4 is correlated to Soil_Type6\n- Wilderness_Area3 and 4 are correlated\n- Elevation and Wilderness_Area4 are correlated\n- Amongst Soil_Types 39 has highest correlation with Cover_Type\n- Horizontal_Distance_To_Roadways is correlated to Elevation, Wilderness_Area4\n- Wilderness_Area4 is correlated to Horizontal_Distance_To_Firepoints\n- Wilderness_Area1, 3 and 4 are correlated to Soil_Type10\n- Soil_Type3 & Wilderness_Area4","4b683457":"**BINARY COLUMNS**","4f9b7c24":"**Hillshade_3pm**\n\nDistribution : About Normal\n\nOutlier : Yes\n\nNull : No\n\nIncorrect Values : Yes","72bd93a5":"## UNIVARIATE ANALYSIS","0140e313":"**Hillshade_Noon**\n\nDistribution : Negatively Skewed\n\nOutlier : Yes\n\nNull : No\n\nIncorrect Values : Yes","1ebe8ca1":"**Vertical_Distance_To_Hydrology**\n\nDistribution : Positively Skewed\n\nOutlier : Yes\n\nNull : No\n\nIncorrect Values : Yes","d5f8a8a9":"### COMBINING BINARY VALUED SOIL TYPES TO 40 BIT INTEGERS","efd830c3":"Correcting negative values","99ed9049":"## BIVARIATE ANALYSIS","cb221f02":"## SCALE VALUES","468584a9":"## TRAIN - TEST SPLIT","431964e5":"# ****EDA****","ed3afbd3":"# **DATA PREPROCESSING**","6da0f547":"## HANDLE SINGLE VALUED COLUMNS","c4f36371":"**Horizontal_Distance_To_Hydrology**\n\nDistribution : Positively Skewed\n\nOutlier : Yes\n\nNull : No\n\nIncorrect Values : Yes","9c9ec271":"## FEATURE ENGINEERING","89579748":"Since distance as a scalar can't be negative or positive thus taking modulus of negative values","87e28efa":"**Horizontal_Distance_To_Fire_Points**\n\nDistribution : Positively Skewed\n\nOutlier : Yes\n\nNull : No\n\nIncorrect Values : Yes","7277a624":"**ELEVATION**\n\nDistribution : About Normal\n\nOutlier : Yes\n\nNull : No\n\nIncorrect Values : No","3838f102":"**Horizontal_Distance_To_Roadways**\n\nDistribution : Positively Skewed\n\nOutlier : Yes\n\nNull : No\n\nIncorrect Values : Yes","c318ffeb":"## HANDLE SKEWNESS","58b3f1f5":"## HANDLE OUTLIERS","5c43d765":"## OPTIMISE MEMORY","a2d83445":"## CLASS IMBALANCE","ee79b231":"Since the values in Hillshade columns have to be from 0 to 255 thus correcting wherever necessary","624abd07":"**Hillshade_9am**\n\nDistribution : Negatively Skewed\n\nOutlier : Yes\n\nNull : No\n\nIncorrect Values : Yes","cc500ad8":"# MACHINE LEARNING","ee437ba0":"Since Aspect is measured in Degree Azimuth, and it ranges from 0 to 360. Thus considering all values less than 0 as 360+x and all values greater than 360 as x-360.","ab2c9b59":"**ASPECT**\n\nDistribution : About Skewed\n\nOutlier : No\n\nNull : No\n\nIncorrect Values : Yes"}}