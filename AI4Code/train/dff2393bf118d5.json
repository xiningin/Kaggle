{"cell_type":{"099b1c0e":"code","4d2f4ab5":"code","de2613b6":"code","54e3ca5f":"code","f21e8e10":"code","373e9633":"code","059b043b":"code","411c944a":"code","e04a454f":"code","8bebbc50":"code","6814bbfe":"code","30acd089":"code","8b3265fd":"code","be94c7c8":"code","f3eca81a":"code","6752b1c3":"code","31480ab3":"code","f1417622":"code","4ccbe614":"code","4a4bd1af":"code","10f54cd2":"code","15de7c20":"code","e322fd9f":"code","a9578b05":"code","ce2adb4a":"code","f0f305d0":"code","e48fe389":"code","28ca0f3d":"code","4f1e8e4e":"code","a22ccd4f":"code","b23eee28":"code","4f14b0fc":"code","503de07b":"code","7c6f7d46":"code","edf30254":"markdown","bda89638":"markdown","1b8c6097":"markdown","6c23cada":"markdown","8db98cf6":"markdown","25ff958c":"markdown","879f9e95":"markdown","8cdebf7d":"markdown","911d1211":"markdown","95729426":"markdown","9ab538f8":"markdown","9044cd4d":"markdown","25c650f7":"markdown","5698e528":"markdown","747ecce8":"markdown","05ac9efa":"markdown","54e15a1e":"markdown","487b85b8":"markdown","61cf5d4e":"markdown","62d72bed":"markdown","a47ba0e2":"markdown","f33f496d":"markdown","9ebe42fa":"markdown","fb397085":"markdown","72244bdf":"markdown","6ae88d1b":"markdown","0c86c648":"markdown","a98a86a5":"markdown","1c952835":"markdown"},"source":{"099b1c0e":"import numpy as np\nfrom numpy import array\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport string\nimport os\nimport glob\n\nfrom PIL import Image\nfrom time import time\n\nfrom keras import Input, layers\nfrom keras import optimizers\nfrom keras.optimizers import Adam\n\nfrom keras.preprocessing import sequence\nfrom keras.preprocessing import image\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\n\nfrom keras.layers import LSTM, Embedding, Dense, Activation, Flatten, Reshape, Dropout\nfrom keras.layers.wrappers import Bidirectional\nfrom keras.layers.merge import add\n\n\nfrom keras.applications.inception_v3 import InceptionV3\nfrom keras.applications.inception_v3 import preprocess_input\n\nfrom keras.models import Model\nfrom keras.utils import to_categorical","4d2f4ab5":"#We will define all the paths to the files that we require and save the images id and their captions.\ntoken_path = \"..\/input\/flickr8k\/Data\/Flickr8k_text\/Flickr8k.token.txt\"\ntrain_images_path = '..\/input\/flickr8k\/Data\/Flickr8k_text\/Flickr_8k.trainImages.txt'\ntest_images_path = '..\/input\/flickr8k\/Data\/Flickr8k_text\/Flickr_8k.testImages.txt'\nimages_path = '..\/input\/flickr8k\/Data\/Flicker8k_Dataset\/'\nglove_path = '..\/input\/glove6b'","de2613b6":"doc = open(token_path,'r').read()\nprint(doc[:410])","54e3ca5f":"descriptions = dict()\nfor line in doc.split('\\n'):\n        tokens = line.split()\n        if len(line) > 2:\n          image_id = tokens[0].split('.')[0]\n          image_desc = ' '.join(tokens[1:])\n          if image_id not in descriptions:\n              descriptions[image_id] = list()\n          descriptions[image_id].append(image_desc)","f21e8e10":"table = str.maketrans('', '', string.punctuation)\nfor key, desc_list in descriptions.items():\n    for i in range(len(desc_list)):\n        desc = desc_list[i]\n        desc = desc.split()\n        desc = [word.lower() for word in desc]\n        desc = [w.translate(table) for w in desc]\n        desc_list[i] =  ' '.join(desc)","373e9633":"pic = '1000268201_693b08cb0e.jpg'\nx=plt.imread(images_path+pic)\nplt.imshow(x)\nplt.show()\ndescriptions['1000268201_693b08cb0e']","059b043b":"vocabulary = set()\nfor key in descriptions.keys():\n        [vocabulary.update(d.split()) for d in descriptions[key]]\n\nprint('Original Vocabulary Size: %d' % len(vocabulary))","411c944a":"lines = list()\nfor key, desc_list in descriptions.items():\n    for desc in desc_list:\n        lines.append(key + ' ' + desc)\nnew_descriptions = '\\n'.join(lines)","e04a454f":"doc = open(train_images_path,'r').read()\ndataset = list()\nfor line in doc.split('\\n'):\n    if len(line) > 1:\n      identifier = line.split('.')[0]\n      dataset.append(identifier)\n\ntrain = set(dataset)\nprint('Train Dataset size: %d' % len(train))","8bebbc50":"img = glob.glob(images_path + '*.jpg')\ntrain_images = set(open(train_images_path, 'r').read().strip().split('\\n'))\ntrain_img = []\nfor i in img: \n    if i[len(images_path):] in train_images:\n        train_img.append(i)\n        \ntest_images = set(open(test_images_path, 'r').read().strip().split('\\n'))\ntest_img = []\nfor i in img: \n    if i[len(images_path):] in test_images: \n        test_img.append(i)","6814bbfe":"train_descriptions = dict()\nfor line in new_descriptions.split('\\n'):\n    tokens = line.split()\n    image_id, image_desc = tokens[0], tokens[1:]\n    if image_id in train:\n        if image_id not in train_descriptions:\n            train_descriptions[image_id] = list()\n        desc = 'startseq ' + ' '.join(image_desc) + ' endseq'\n        train_descriptions[image_id].append(desc)\n\nprint('Descriptions: train = %d' % len(train_descriptions))","30acd089":"all_train_captions = []\nfor key, val in train_descriptions.items():\n    for cap in val:\n        all_train_captions.append(cap)\nlen(all_train_captions)","8b3265fd":"word_count_threshold = 10\nword_counts = {}\nnsents = 0\nfor sent in all_train_captions:\n    nsents += 1\n    for w in sent.split(' '):\n        word_counts[w] = word_counts.get(w, 0) + 1\n\nvocab = [w for w in word_counts if word_counts[w] >= word_count_threshold]\nprint('Vocabulary = %d' % (len(vocab)))","be94c7c8":"ixtoword = {}\nwordtoix = {}\n\nix = 1\nfor w in vocab:\n    wordtoix[w] = ix\n    ixtoword[ix] = w\n    ix += 1","f3eca81a":"vocab_size = len(ixtoword) + 1 # one for appended 0's\nvocab_size","6752b1c3":"all_desc = list()\nfor key in train_descriptions.keys():\n    [all_desc.append(d) for d in train_descriptions[key]]\nlines = all_desc\n\nmax_length = max(len(d.split()) for d in lines)\nprint('Description Length: %d' % max_length)","31480ab3":"embeddings_index = {} \nf = open(os.path.join(glove_path, 'glove.6B.200d.txt'), encoding=\"utf-8\")\n\nfor line in f:\n    values = line.split()\n    word = values[0]\n    coefs = np.asarray(values[1:], dtype='float32')\n    embeddings_index[word] = coefs","f1417622":"embedding_dim = 200\nembedding_matrix = np.zeros((vocab_size, embedding_dim))\n\nfor word, i in wordtoix.items():\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[i] = embedding_vector","4ccbe614":"model = InceptionV3(weights='imagenet')","4a4bd1af":"model_new = Model(model.input, model.layers[-2].output)","10f54cd2":"def preprocess(image_path):\n    img = image.load_img(image_path, target_size=(299, 299))\n    x = image.img_to_array(img)\n    x = np.expand_dims(x, axis=0)\n    x = preprocess_input(x)\n    return x","15de7c20":"def encode(image):\n    image = preprocess(image) # preprocess the image\n    fea_vec = model_new.predict(image) # Get the encoding vector for the image\n    fea_vec = np.reshape(fea_vec, fea_vec.shape[1]) # reshape from (1, 2048) to (2048, )\n    return fea_vec","e322fd9f":"encoding_train = {}\nfor img in train_img:\n    encoding_train[img[len(images_path):]] = encode(img)\n\ntrain_features = encoding_train","a9578b05":"encoding_test = {}\nfor img in test_img:\n    encoding_test[img[len(images_path):]] = encode(img)","ce2adb4a":"inputs1 = Input(shape=(2048,))\nfe1 = Dropout(0.5)(inputs1)\nfe2 = Dense(256, activation='relu')(fe1)\n\ninputs2 = Input(shape=(max_length,))\nse1 = Embedding(vocab_size, embedding_dim, mask_zero=True)(inputs2)\nse2 = Dropout(0.5)(se1)\nse3 = LSTM(256)(se2)\n\ndecoder1 = add([fe2, se3])\ndecoder2 = Dense(256, activation='relu')(decoder1)\noutputs = Dense(vocab_size, activation='softmax')(decoder2)\n\nmodel = Model(inputs=[inputs1, inputs2], outputs=outputs)\nmodel.summary()","f0f305d0":"model.layers[2].set_weights([embedding_matrix])\nmodel.layers[2].trainable = False","e48fe389":"model.compile(loss='categorical_crossentropy', optimizer='adam')","28ca0f3d":"def data_generator(descriptions, photos, wordtoix, max_length, num_photos_per_batch):\n    X1, X2, y = list(), list(), list()\n    n=0\n    # loop for ever over images\n    while 1:\n        for key, desc_list in descriptions.items():\n            n+=1\n            # retrieve the photo feature\n            photo = photos[key+'.jpg']\n            for desc in desc_list:\n                # encode the sequence\n                seq = [wordtoix[word] for word in desc.split(' ') if word in wordtoix]\n                # split one sequence into multiple X, y pairs\n                for i in range(1, len(seq)):\n                    # split into input and output pair\n                    in_seq, out_seq = seq[:i], seq[i]\n                    # pad input sequence\n                    in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n                    # encode output sequence\n                    out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n                    # store\n                    X1.append(photo)\n                    X2.append(in_seq)\n                    y.append(out_seq)\n            if n==num_photos_per_batch:\n                yield ([array(X1), array(X2)], array(y))\n                X1, X2, y = list(), list(), list()\n                n=0","4f1e8e4e":"epochs = 10\nbatch_size = 3\nsteps = len(train_descriptions)\/\/batch_size","a22ccd4f":"generator = data_generator(train_descriptions, train_features, wordtoix, max_length, batch_size)\nmodel.fit(generator, epochs=epochs, steps_per_epoch=steps, verbose=1)","b23eee28":"def greedySearch(photo):\n    in_text = 'startseq'\n    for i in range(max_length):\n        sequence = [wordtoix[w] for w in in_text.split() if w in wordtoix]\n        sequence = pad_sequences([sequence], maxlen=max_length)\n        yhat = model.predict([photo,sequence], verbose=0)\n        yhat = np.argmax(yhat)\n        word = ixtoword[yhat]\n        in_text += ' ' + word\n        if word == 'endseq':\n            break\n    final = in_text.split()\n    final = final[1:-1]\n    final = ' '.join(final)\n    return final","4f14b0fc":"def beam_search_predictions(image, beam_index = 3):\n    start = [wordtoix[\"startseq\"]]\n    start_word = [[start, 0.0]]\n    while len(start_word[0][0]) < max_length:\n        temp = []\n        for s in start_word:\n            par_caps = sequence.pad_sequences([s[0]], maxlen=max_length, padding='post')\n            preds = model.predict([image,par_caps], verbose=0)\n            word_preds = np.argsort(preds[0])[-beam_index:]\n            # Getting the top <beam_index>(n) predictions and creating a \n            # new list so as to put them via the model again\n            for w in word_preds:\n                next_cap, prob = s[0][:], s[1]\n                next_cap.append(w)\n                prob += preds[0][w]\n                temp.append([next_cap, prob])\n                    \n        start_word = temp\n        # Sorting according to the probabilities\n        start_word = sorted(start_word, reverse=False, key=lambda l: l[1])\n        # Getting the top words\n        start_word = start_word[-beam_index:]\n    \n    start_word = start_word[-1][0]\n    intermediate_caption = [ixtoword[i] for i in start_word]\n    final_caption = []\n    \n    for i in intermediate_caption:\n        if i != 'endseq':\n            final_caption.append(i)\n        else:\n            break\n    \n    final_caption = ' '.join(final_caption[1:])\n    return final_caption","503de07b":"pic = list(encoding_test.keys())[900]\nimage = encoding_test[pic].reshape((1,2048))\nx=plt.imread(images_path+pic)\nplt.imshow(x)\nplt.show()\nprint(\"Greedy:\",greedySearch(image))\nprint(\"Beam Search, K = 3:\",beam_search_predictions(image, beam_index = 3))\nprint(\"Beam Search, K = 5:\",beam_search_predictions(image, beam_index = 5))\nprint(\"Beam Search, K = 7:\",beam_search_predictions(image, beam_index = 7))\nprint(\"Beam Search, K = 10:\",beam_search_predictions(image, beam_index = 10))","7c6f7d46":"pic = list(encoding_test.keys())[999]\nimage = encoding_test[pic].reshape((1,2048))\nx=plt.imread(images_path+pic)\nplt.imshow(x)\nplt.show()\nprint(\"Greedy:\",greedySearch(image))\nprint(\"Beam Search, K = 3:\",beam_search_predictions(image, beam_index = 3))\nprint(\"Beam Search, K = 5:\",beam_search_predictions(image, beam_index = 5))\nprint(\"Beam Search, K = 7:\",beam_search_predictions(image, beam_index = 7))\nprint(\"Beam Search, K = 10:\",beam_search_predictions(image, beam_index = 10))","edf30254":"# Glove Embeddings\nWord vectors map words to a vector space, where similar words are clustered together and different words are separated. The advantage of using Glove over Word2Vec, is that GloVe does not just rely on the local context of words but it incorporates global word co-occurrence to obtain word vectors.\nThe basic premise behind Glove is that we can derive semantic relationships between words from co-occurrence matrix.\nFor our model, we will map all the words in our 38 word long caption to a 200-dimension vector using Glove.","bda89638":"As you have seen from our approach we have opted for transfer learning using InceptionV3 network which is pre-trained on the ImageNet dataset.","1b8c6097":"# Dataset\nA number of datasets are used for training, testing, and evaluation of the image captioning methods. The datasets differ in various perspectives such as the number of images, the number of captions per image, format of the captions, and image size. Three datasets: Flickr8k, Flickr30k, and MS COCO Dataset are popularly used.\n\nIn the Flickr8k dataset each image is associated with five different captions that describe the entities and events depicted in the image that were collected. By associating each image with multiple, independently produced sentences, the dataset captures some of the linguistic variety that can be used to describe the same image.\n\nFlickr8k is a good starting dataset as it is small in size and can be trained easily on low-end laptops\/desktops using a CPU.","6c23cada":"# Model Building and Training","8db98cf6":"Now let's perform some basic text clean to get rid of punctuation and convert our descriptions to lowercase.","25ff958c":"Since we are using InceptionV3 we need to pre-process our input before feeding it into the model. Hence we define a preprocess function to reshape the images to (299 x 299) and feed to the preprocess_input() function of keras.","879f9e95":"Beam Search is where we take top k predictions, feed them again in the model and then sort them using the probabilities returned by the model. So, the list will always contain the top k predictions. In the end, we take the one with the highest probability and go through it till we encounter 'endseq' or reach the maximum caption length.","8cdebf7d":"Next we create a vocabulary of all the unique words present across all the 8000*5 (i.e. 40000) image captions in the data set. We have 8828 unique words across all the 40000 image captions.","911d1211":"Now lets save the image id\u2019s and their new cleaned captions in the same format as the token.txt file:-","95729426":"Now we can go ahead and encode our training and testing images, i.e extract the images vectors of shape (2048, )","9ab538f8":"# Introduction\nImage captioning is a popular research area of Artificial Intelligence that deals with image understanding and a language description for that image. Generating well-formed sentences requires both syntactic and semantic understanding of the language. Being able to describe the content of an image using accurately formed sentences is a very challenging task, but it could also have great impact, by helping visually impaired people better understand the content of images.\n\nThis task is significantly harder in comparison to the image classification or object recognition tasks that have been well researched .\n\nThe biggest challenge is most definitely being able to create a description that must capture not only the objects contained in an image, but also express how these objects relate to each other.","9044cd4d":"# Evaluation\nLet's now test our model on different images and see what captions it generates. We will also look at the different captions generated by Greedy search and Beam search with different k values.","25c650f7":"Next we make the matrix of shape (1660,200) consisting of our vocabulary and the 200-d vector.","5698e528":"Next we load all the 6000 training image id\u2019s in a variable train from the \u2018Flickr_8k.trainImages.txt\u2019 file :-","747ecce8":"Input_3 is the partial caption of max length 34 which is fed into the embedding layer. This is where the words are mapped to the 200-d Glove embedding. It is followed by a dropout of 0.5 to avoid overfitting. This is then fed into the LSTM for processing the sequence.\n\nInput_2 is the image vector extracted by our InceptionV3 network. It is followed by a dropout of 0.5 to avoid overfitting and then fed into a Fully Connected layer.\n\nBoth the Image model and the Language model are then concatenated by adding and fed into another Fully Connected layer. The layer is a softmax layer which provides probabilities to our 1660 word vocabulary.\n\nWe also need to keep in mind that we do not want to retrain the weights in our embedding layer (pre-trained Glove vectors).","05ac9efa":"# Whats Next?\nWhat we have developed today is just the start. There has been a lot of research on this topic and you can make much better Image caption generators.\n\nThings u can implement to improve your model:-\n* Make use of the larger datasets, especially the MS COCO dataset or the Stock3M dataset which is 26 times larger than MS COCO.\n* Making use of an evaluation metric to measure the quality of machine generated text like BLEU (Bilingual evaluation understudy).\n* Implementing an Attention Based model :- Attention based mechanisms are becoming increasingly popular in deep learning because they can address these limitations. They can dynamically focus on the various parts of the input image while the output sequences are being produced.\n* Image-based factual descriptions are not enough to generate high-quality captions. External knowledge can be added in order to generate attractive image captions. Therefore working on Open domain datasets can be an interesting prospect.","54e15a1e":"Now let's define our model.\nWe are creating a Merge model where we combine the image vector and the partial caption. Therefore our model will have 3 major steps:\n\n1. Processing the sequence from the text\n2. Extracting the feature vector from the image\n3. Decoding the output by concatenating the above two layers","487b85b8":"Let\u2019s visualize an example image and its captions :-","61cf5d4e":"As the model generates a 1660 long vector with a probability distribution across all the words in the vocabulary we greedily pick the word with the highest probability to get the next word prediction. This method is called Greedy Search.","62d72bed":"Since our dataset has 6000 images and 40000 captions we will create a function that can train the data in batches.","a47ba0e2":"Now, we load the descriptions of the training images from into a dictionary. However, we will add two tokens in every caption, which are \u2018startseq\u2019 and \u2018endseq\u2019 :-","f33f496d":"Next let's train our model for 30 epochs with batch size of 3 and 2000 steps per epoch.The complete training of the model took 1 hour and 40 minutes.","9ebe42fa":"We must remember that we do not need to classify the images here, we only need to extract an image vector for our images. Hence we remove the softmax layer from the inceptionV3 model.","fb397085":"# Data Loading and PreProcessing","72244bdf":"# Greedy and Beam Search","6ae88d1b":"Now we create two dictionaries to map words to an index and vice versa. Also we append 1 to our vocabulary since we append 0\u2019s to make all captions of equal length.","0c86c648":"So we can see the format in which our image id\u2019s and their captions are stored.\nNext, we create a dictionary named \u201cdescriptions\u201d which contains the name of the image as keys and a list of the 5 captions for the corresponding image as values.","a98a86a5":"To make our model more robust we will reduce our vocabulary to only those words which occur at least 10 times in the entire corpus.","1c952835":"Hence now our total vocabulary size is 1660.\nWe also need to find out what the max length of a caption can be since we cannot have captions of arbitrary length."}}