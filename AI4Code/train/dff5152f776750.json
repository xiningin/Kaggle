{"cell_type":{"26ebf9d5":"code","b5829cbf":"code","3f54aa03":"code","860cdc15":"code","545e1c70":"code","de310cc6":"code","de0dbab6":"code","42700f33":"code","5ab748fe":"code","bed16c77":"code","589b54fc":"code","9ca9ebf1":"code","080b266b":"code","9b54b433":"code","50a8008b":"code","4a902c45":"code","e307cd58":"code","1c1a977d":"code","5ac12b1c":"code","c740078c":"code","d51be126":"code","abd33f32":"code","c0159006":"code","2705220a":"code","2192cc2e":"code","86632604":"code","8ce0952f":"code","7aed393d":"code","f7a19adb":"code","439c0ba9":"code","97d0fc8f":"code","166a952b":"code","43625172":"code","3bb643fe":"code","2791a972":"code","0c61bdec":"code","b51dba17":"code","81280496":"code","b41b9d67":"code","8c75fcb9":"code","3641a25d":"code","93c9e1fc":"code","195ef253":"code","a09f39df":"code","ec976bd1":"code","bff60d9e":"code","60ad9dcb":"code","ae6eaf99":"code","da1c7edb":"code","68036c99":"code","79c07d75":"code","a7dccbb7":"code","b75f2196":"code","bfbaa5f7":"code","5133b963":"code","f14fe827":"code","fe6f9096":"code","159904ec":"code","179429b1":"code","1895f373":"code","d034e6df":"code","54ff4007":"code","22adad93":"code","2d93783c":"code","6dda575d":"code","b6a1b977":"code","e55319bc":"code","cb8af220":"code","ed01fcc9":"code","ea46dfb1":"code","916d4088":"code","fac89349":"code","c4950b85":"code","b05c8a70":"code","b9e87681":"code","a4d673ed":"code","aa340f87":"code","b9c1e800":"code","6b113225":"code","295a63a6":"code","d58dc3ff":"code","e9b8ea68":"code","8337c4c0":"code","617b626a":"code","e8ccd30a":"code","c4b275e6":"code","659861fc":"code","1496d00d":"code","a144e62c":"code","1531a813":"code","6e5f4e46":"markdown","0d0758c7":"markdown","8a5d5e63":"markdown","980996bd":"markdown","2b56af01":"markdown","8b11f870":"markdown","5c9af5ef":"markdown","3ae1e612":"markdown","bbd2e22d":"markdown","12436d4b":"markdown","3625086e":"markdown","e6240c61":"markdown","66dfdb0e":"markdown","ea536038":"markdown","3f7b1a1b":"markdown","ee0487a5":"markdown","ecec23a7":"markdown","a0abe233":"markdown","23959f9c":"markdown","70f8e699":"markdown","a1b51783":"markdown","3dd2b040":"markdown","68847edb":"markdown","b4f74cef":"markdown","23d5aa6a":"markdown","249c776c":"markdown","3b202234":"markdown","57168ff9":"markdown","f627e175":"markdown","0dc06ce5":"markdown","532afdbf":"markdown","db2c26d9":"markdown","57e1e35c":"markdown","07532007":"markdown","71aec425":"markdown","7c60d301":"markdown","311169af":"markdown","33fd1979":"markdown","ae886206":"markdown","f6217c8e":"markdown","3e94da6a":"markdown","67700ab1":"markdown","60773d25":"markdown","705c7cfd":"markdown","161e2116":"markdown","676cf9c0":"markdown","6518497c":"markdown","324ab949":"markdown","fa60cbb0":"markdown","e41b76ed":"markdown","9efcfe6c":"markdown","c5699965":"markdown","51541e78":"markdown","e14a2d2a":"markdown","e99bf38a":"markdown","332ff2d5":"markdown","3354b213":"markdown","90eed85f":"markdown","4a8ea70b":"markdown","6558cdd0":"markdown","575c942d":"markdown","0dead08c":"markdown","00b4764d":"markdown","7b037402":"markdown"},"source":{"26ebf9d5":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","b5829cbf":"import numpy as np   # for numerical calculation\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n","3f54aa03":"train_dir = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntest_dir = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\nprint(\"Training data: {}\".format(train_dir.shape))\nprint(\"Testing data: {}\".format(test_dir.shape))","860cdc15":"train_dir.head(5)","545e1c70":"train_dir.columns","de310cc6":"train_dir.info()","de0dbab6":"train_dir.isnull().any()","42700f33":"train_dir.isnull().sum()","5ab748fe":"test_dir.head(2)","bed16c77":"train_dir.isnull().sum()","589b54fc":"train_dir.head(3)","9ca9ebf1":"updated_train_dir = train_dir.drop(['Ticket', 'PassengerId', 'Name','Cabin'], axis = 1)\nupdated_train_dir.head(3)","080b266b":"updated_test_dir = test_dir.drop(['Ticket', 'PassengerId', 'Name','Cabin'], axis = 1)\nupdated_test_dir.head(3)","9b54b433":"Missing_age = 100 * updated_train_dir['Age'].isnull().sum() \/ updated_train_dir['Age'].shape[0]\n\nprint(\"Age missing value: {}\".format(Missing_age))","50a8008b":"print(\"Median value of Age is {}\".format(updated_train_dir['Age'].median()))\nupdated_train_dir['Age'].fillna(updated_train_dir['Age'].median(), inplace=True)","4a902c45":"updated_train_dir['Age'].isnull().any()","e307cd58":"updated_test_dir['Age'].fillna(updated_test_dir['Age'].median(), inplace=True)\nupdated_test_dir['Age'].isnull().any()","1c1a977d":"# Other missing values\nupdated_train_dir.isnull().any()","5ac12b1c":"updated_train_dir['Embarked'].value_counts()","c740078c":"Missing_embarked = 100 * updated_train_dir['Embarked'].isnull().sum() \/ updated_train_dir['Embarked'].shape[0]\nprint(\"Missing embakred info: {}\".format(Missing_embarked))","d51be126":"updated_train_dir['Embarked'].fillna('S', inplace=True)\nupdated_train_dir.isnull().any()","abd33f32":"# in the test data\nupdated_test_dir['Embarked'].value_counts()\n","c0159006":"updated_test_dir[\"Embarked\"].fillna(updated_test_dir['Embarked'].value_counts().idxmax(), inplace=True)\nupdated_test_dir.isnull().any()","2705220a":"import plotly.graph_objs as go\n\nfig = go.Figure(data=[go.Pie(labels=['Survived','Not Survived'],\n                            values=updated_train_dir['Survived'].value_counts(),\n                            textinfo = 'label + percent')])\nfig.show()","2192cc2e":"# Let calculate the number of survived and not survived values\ntotal_survived_notsurvived = updated_train_dir['Survived'].shape[0]\nnum_survived = updated_train_dir[updated_train_dir['Survived'] == 1].shape[0]\nnot_survived = updated_train_dir[updated_train_dir['Survived'] == 0].shape[0]\n\nprint(\"Survived: {}\".format(100 * (num_survived \/ total_survived_notsurvived)))\nprint(\"Not Survived: {}\".format(100 * (not_survived\/total_survived_notsurvived)))","86632604":"sns.countplot(x='Survived', hue='Sex', data=updated_train_dir)","8ce0952f":"# Let take other columns to learn from the data\n\nupdated_train_dir.columns","7aed393d":"updated_train_dir['Age'].iplot(kind='hist',\n                                      xTitle='Age', title='AGE PLOT'\n                                    )","f7a19adb":"def grouping_Age(x):\n    if x in range(0, 21):\n        return 1\n    elif x in range(21, 41):\n        return 2\n    else:\n        return 3\n    \n    \nupdated_train_dir['Age group']= updated_train_dir['Age'].apply(grouping_Age)","439c0ba9":"updated_test_dir['Age group']= updated_test_dir['Age'].apply(grouping_Age)","97d0fc8f":"age_index = updated_train_dir['Age group'].value_counts().index\nage_values = updated_train_dir['Age group'].value_counts()\nfig = go.Figure(data=[go.Pie(labels=['Age: 0-20','Age: 21-41','Age: 41+'],\n                            values=age_values,\n                            textinfo = 'label + percent')])\nfig.show() ","166a952b":"updated_train_dir['Age group'].value_counts()","43625172":"updated_train_dir.columns","3bb643fe":"plt.figure(figsize=(15,10))\n\nfigure = sns.kdeplot(updated_train_dir['Fare'][updated_train_dir.Survived == 1],\n                    shade=True)\nsns.kdeplot(updated_train_dir['Fare'][updated_train_dir.Survived == 0],\n                 color=\"lightcoral\",   shade=True)\nplt.legend(['Survived', 'Died'])\nplt.title(\"Survival v\/s Died\")\nplt.xlim(-20,200)\n\nfigure.set(xlabel='Fare')\nplt.show()","2791a972":"updated_train_dir['Pclass'].value_counts().iplot(kind='bar',\n                                         bins='10',\n                                        xTitle = 'Ticket class',\n                                        yTitle='Num of Passenger', title='Ticket Class')","0c61bdec":"sns.barplot('Pclass', 'Survived', data=updated_train_dir)\nplt.show()","b51dba17":"sns.barplot('Embarked','Survived', data=updated_train_dir)\nplt.show()","81280496":"updated_train_dir.head(3)","b41b9d67":"# We need to convert Model into the integer based for the model training\n# So we will convert 1 for Male and 0 for female\n\n\ngender = {\n    'male': 1,\n    'female':0\n}\nupdated_train_dir['Sex'] = updated_train_dir['Sex'].apply(lambda x: gender.get(x))\nupdated_train_dir.drop(['Age'], axis=1, inplace=True)\n","8c75fcb9":"updated_test_dir['Sex'] = updated_test_dir['Sex'].apply(lambda x: gender.get(x))\n","3641a25d":"updated_test_dir.drop(['Age'], axis=1, inplace=True)\n","93c9e1fc":"updated_train_dir.head(3)\n# always take a copy of the data\ntrain_Data = updated_train_dir\nupdated_train_dir.head(3)","195ef253":"updated_train_dir.drop(['Fare'], axis=1, inplace=True)","a09f39df":"updated_test_dir.drop(['Fare'], axis=1, inplace=True)","ec976bd1":"updated_train_dir.head(3)","bff60d9e":"traindf = pd.get_dummies(updated_train_dir, columns = [\"Embarked\",\"Age group\", \"Pclass\"],\n                             prefix=[\"Em_type\", \"Age_group\", \"Pclass_\"])","60ad9dcb":"testdf = pd.get_dummies(updated_test_dir, columns = [\"Embarked\",\"Age group\", \"Pclass\"],\n                             prefix=[\"Em_type\", \"Age_group\", \"Pclass_\"])\ntestdf.head(2)\n","ae6eaf99":"traindf.head(2)","da1c7edb":"train_y = traindf['Survived']\ntraindf.drop(['Survived'], axis=1,inplace=True)","68036c99":"print(\"Training shape: {} and Testing shape: {}\\n\\\n      Training Label:{}\".format(traindf.shape,\n                                testdf.shape,\n                                train_y.shape))","79c07d75":"traindf.info()","a7dccbb7":"from sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix","b75f2196":"traindf.head(3)","bfbaa5f7":"train_y[:3]","5133b963":"X_train, X_test, Y_train, Y_test = train_test_split(traindf,\n                                                   train_y,\n                                                test_size=0.3,\n                                                   random_state=42)\nX_train.shape, X_test.shape, Y_train.shape, Y_test.shape","f14fe827":"from sklearn.linear_model import LogisticRegression\nLogisticRegression = LogisticRegression(max_iter=10000)\nLogisticRegression.fit(X_train, Y_train)","fe6f9096":"predictions = LogisticRegression.predict(X_test)\npredictions","159904ec":"Linear_Reg_acc = accuracy_score(predictions, Y_test) * 100","179429b1":"from sklearn.ensemble import RandomForestClassifier\n\nmodel_random = RandomForestClassifier(n_estimators = 700,\n                                     max_features='auto',\n                                     oob_score=True,\n                                     random_state=1,\n                                     n_jobs=1,\n                                      min_samples_leaf=1,\n                                      min_samples_split=10                                  \n                                )","1895f373":"model_random.fit(X_train, Y_train)\npredictions_random = model_random.predict(X_test)","d034e6df":"Random_Forest_Acc = accuracy_score(predictions_random, Y_test) * 100\nprint(\"Random FOrest acc: {}\".format(Random_Forest_Acc))","54ff4007":"from sklearn.svm import SVC, LinearSVC\n\nsvc_model = SVC()\n\nsvc_model.fit(X_train, Y_train)\npredictions_svc = svc_model.predict(X_test)\n","22adad93":"SVC_acc = accuracy_score(predictions_svc, Y_test) * 100\nprint(\"SVC accuracy: {}\".format(SVC_acc))","2d93783c":"from sklearn.neighbors import KNeighborsClassifier\nn_model  = KNeighborsClassifier(n_neighbors = 4)\nn_model.fit(X_train, Y_train)","6dda575d":"predictions_knn = n_model.predict(X_test)","b6a1b977":"accuracy_score(predictions_knn, Y_test) * 100","e55319bc":"knn_data = {}\nfor i in range(1,30):\n    model = KNeighborsClassifier(n_neighbors=i)\n    model.fit(X_train, Y_train)\n    prediction_data = model.predict(X_test)\n    knn_data[i] = accuracy_score(prediction_data, Y_test) * 100\n    ","cb8af220":"knn_data","ed01fcc9":"# max value of neighbors\ndata = max(knn_data, key=knn_data.get)\nprint(data)","ea46dfb1":"n_model  = KNeighborsClassifier(n_neighbors = 14)\nn_model.fit(X_train, Y_train)\npredictions_knn = n_model.predict(X_test)\nknn_Ac = accuracy_score(predictions_knn, Y_test) * 100\nprint(knn_Ac)","916d4088":"knn_acc = knn_data[14]\nprint(\"KNN accuracy: {}\".format(knn_acc))","fac89349":"from sklearn.naive_bayes import GaussianNB\n\nmodel_naive = GaussianNB()\n\nmodel_naive.fit(X_train, Y_train)\n\nprediction_naive = model_naive.predict(X_test)","c4950b85":"gaussian_acc = accuracy_score(prediction_naive, Y_test) * 100\nprint(\"Gaussian Acc: {}\".format(gaussian_acc))","b05c8a70":"from sklearn.tree import DecisionTreeClassifier","b9e87681":"tree_data = {}\nfor i in range(2,40):\n    tree_model = DecisionTreeClassifier(criterion='gini',\n                                    min_samples_split=i,\n                              max_features='auto',\n                              min_samples_leaf=1)\n    tree_model.fit(X_train, Y_train)\n    tree_predict = tree_model.predict(X_test)\n    tree_data[i]=accuracy_score(tree_predict, Y_test)\n    ","a4d673ed":"tree_data","aa340f87":"# we will fetch the larger values\ntree_val = max(tree_data, key=tree_data.get)\nprint(tree_val)","b9c1e800":"tree_model = DecisionTreeClassifier(criterion='gini',\n                                    min_samples_split=12,\n                              max_features='auto',\n                              min_samples_leaf=12)","6b113225":"tree_model.fit(X_train, Y_train)\ntree_predict = tree_model.predict(X_test)\ntree_acc = accuracy_score(tree_predict, Y_test)\nprint(\"Tree accuarcy: {}\".format(tree_acc))","295a63a6":"from sklearn.ensemble import AdaBoostClassifier\nmodel_ada = AdaBoostClassifier(n_estimators=1000, learning_rate=0.1)\nmodel_ada.fit(X_train, Y_train)","d58dc3ff":"prediction_add = model_ada.predict(X_test)\nada_accuracy = accuracy_score(prediction_add, Y_test) * 100\nprint('ada accuarcy: {}'.format(ada_accuracy))","e9b8ea68":"from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nmodel_linear_d= LinearDiscriminantAnalysis()\nmodel_linear_d.fit(X_train,Y_train)\nprediction_lda=model_linear_d.predict(X_test)\nLDA_Accuracy = accuracy_score(prediction_lda, Y_test)\nprint(\"Accuracy LDA: {}\".format(LDA_Accuracy))","8337c4c0":"models = {\n    'LinearDiscriminant':   [LDA_Accuracy, prediction_lda],\n    'ADA': [ada_accuracy, prediction_add],\n    'DecisionTreeClassifier':[tree_acc,tree_predict],\n    'GaussianNB': [gaussian_acc,prediction_naive],\n    'LinearSVC':[SVC_acc,predictions_svc],\n    'RandomForestClassifier':[Random_Forest_Acc,predictions_random],\n    'LogisticRegression':[Linear_Reg_acc,predictions]\n}","617b626a":"models_acc = {\n    'LinearDiscriminant':   LDA_Accuracy,\n    'ADA': ada_accuracy,\n    'DecisionTreeClassifier':tree_acc,\n    'GaussianNB': gaussian_acc,\n    'LinearSVC':SVC_acc,\n    'RandomForestClassifier':Random_Forest_Acc,\n    'LogisticRegression':Linear_Reg_acc}","e8ccd30a":"max_acc = max(models_acc, key=models_acc.get)\nprint(\"MAx accuracy is : {}\".format(max_acc))\nprint(models_acc[max_acc])","c4b275e6":"# We will use LinearSVC model to predict the test datasets\npred_test = svc_model.predict(testdf)\npred_test","659861fc":"len(pred_test)","1496d00d":"submission = pd.DataFrame({\n        \"PassengerId\": test_dir[\"PassengerId\"],\n        \"Survived\": pred_test})","a144e62c":"submission","1531a813":"submission.to_csv(\"submission.csv\",index=False)","6e5f4e46":"<a id=\"age\"><\/a>\n<h3>   \n    <span>\n            4.3 Age Group\n    <\/span>   \n \n<\/h3>","0d0758c7":"* Logistic Regression\n* KNN\n* Support Vector Machines\n* Random Forest\n* Decision Tree\n* Navives Bayes Classifier","8a5d5e63":"<a id=\"nullcal\"><\/a>\n<h3>   \n    <span>\n            3.4 Fixing the Null Values\n    <\/span>   \n \n<\/h3>","980996bd":"So, below are the observations till now\n\n1.  PEople onboarded from  C(Cherbourg) survived more\n2.  Female survived more as compared to Male\n3.  More Fares = Higher TIcket class = More survival chances\n","2b56af01":"There are no missing data in the Test information","8b11f870":"Remaining values is Embarked for missing information\n","5c9af5ef":"The items present in the train directory:\n\n1.  PassengerId:   Id of the passenger who were available in the Titanic\n2.  Survived:   0 for no survived and 1 for survived\n3.  Pclass:  Ticket class: 1 - first class 2- second class\n4. sex: Male\/Female\n5. sibsp: sibling\/ spouses\n\nand many more\n\n\nTHe above features are really important to calculate if they survived or not.","3ae1e612":"**Observations**:\n\nIn Titanic, Age are scattered in various groups. So, we will divide into many groups.\n\nLet group them together into three gtroups:\n\n1.  0 - 20 Age\n2. 20 - 40 Age\n3. 40 + Age","bbd2e22d":"Goal: To predict the survival chances of the passengers  from the datasets.\n\n\n<p>\n    The items present in the datasets:\n\n1.  PassengerId:   Id of the passenger who were available in the Titanic\n2.  Survived:   0 for no survived and 1 for survived\n3.  Pclass:  Ticket class: 1 - first class 2- second class\n4. sex: Male\/Female\n5. sibsp: sibling\/ spouses\n\nand many more\n\n    \n<\/p>    \n\n","12436d4b":"<a id=\"model_Ev\"><\/a>\n<h2>   \n    <span color='red'>\n           6. Model Evaluation\n    <\/span>   \n \n<\/h2>","3625086e":"<a id=\"decision\"><\/a>\n<h3>   \n    <span>\n           5.6 Decision Tree\n    <\/span>   \n \n<\/h3>","e6240c61":"<a id=\"eda\"><\/a>\n<h2>   \n    <span>\n           5. Model Training\n    <\/span>   \n \n<\/h2>","66dfdb0e":"<a id=\"suport\"><\/a>\n<h3>   \n    <span>\n           5.3 Support vector machines\n    <\/span>   \n \n<\/h3>","ea536038":"<a id=\"model_subm\"><\/a>\n<h3>   \n    <span>\n          7.1 Model submission\n    <\/span>   \n \n<\/h3>","3f7b1a1b":"Around 38 % people survived in the Titanic disaster.\n\nAnd 61 % people not survived, which is really not a good thing.","ee0487a5":"We will make the Survived columns as the features value. ","ecec23a7":"<a id=\"eda\"><\/a>\n<h2>   \n    <span>\n            4. EDA (Exploratory Data Analysis)\n    <\/span>   \n \n<\/h2>","a0abe233":"C = Cherbourg, Q = Queenstown, S = Southampton\n\nMajority of the people embarked from Southamtpon port as compared to the other ports. So, we will replace the missing value with the S port.\n\nAbove all are the port information where the passengers embarked. Let calculate the missing values","23959f9c":"<a id=\"adaboost\"><\/a>\n<h3>   \n    <span>\n           5.7 ADABoost\n    <\/span>   \n \n<\/h3>","70f8e699":"<a id=\"python\"><\/a>\n<h3>   \n    <span>\n            2.1 Import Python Libraries\n     <\/span>   \n\n<\/h3>\n","a1b51783":"PassengerId, Name  and Ticket will not play any role in Titanic survival chances. So, we will drop them","3dd2b040":"1.  #### Age ","68847edb":"We fixed all the missing information available in the dataset.","b4f74cef":"The diagram is rightly skewed. So, we can replace the Missing values with the median values","23d5aa6a":"There are some missing values in cabins, Age and Embarked  in the datasets\n\n\nWe will check if the missing values can be handle or not in the datasets.\n","249c776c":"### Test data overview\n","3b202234":"Variable\tDefinition\tKey\nsurvival\tSurvival\t0 = No, 1 = Yes\npclass\tTicket class\t1 = 1st, 2 = 2nd, 3 = 3rd\nsex\tSex\t\nAge\tAge in years\t\nsibsp\t# of siblings \/ spouses aboard the Titanic\t\nparch\t# of parents \/ children aboard the Titanic\t\nticket\tTicket number\t\nfare\tPassenger fare\t\ncabin\tCabin number\t\nembarked\tPort of Embarkation\tC = Cherbourg, Q = Queenstown, S = Southampton","57168ff9":"<a id=\"eda\"><\/a>\n<h3>   \n    <span>\n            4.1 Survival\n    <\/span>   \n \n<\/h3>","f627e175":"We will check the median of the Age Values","0dc06ce5":"<a id=\"random\"><\/a>\n<h3>   \n    <span>\n           5.2 Random Forst Classifier\n    <\/span>   \n \n<\/h3>","532afdbf":"<a id=\"gender\"><\/a>\n<h3>   \n    <span>\n            4.2 Gender\n    <\/span>   \n \n<\/h3>","db2c26d9":" It is very less value. So we will replace the missing value with the most embarked port i.e: S","57e1e35c":"There are no missing values in the Age columns. We will afterwards group all the age to get the better ideas.\n\n\nNow, let focus on the other values of missing information","07532007":"<a id=\"navis\"><\/a>\n<h3>   \n    <span>\n          5.5 Naives Bayes\n    <\/span>   \n \n<\/h3>\n","71aec425":"<a id=\"unwanted\"><\/a>\n<h3>   \n    <span>\n            3.3 Delete Unwanted Columns\n    <\/span>   \n \n<\/h3>","7c60d301":"<a id=\"libraries\"><\/a>\n<h2>   \n    <font  color='red'>\n          <span>\n            2. Python Libraries :\n            <\/span>   \n    <\/font>\n<\/h2>\n","311169af":"![1.png](attachment:1.png)","33fd1979":"<a id=\"ticketclass\"><\/a>\n<h3>   \n    <span>\n           4.5 Ticket class\n    <\/span>   \n \n<\/h3>","ae886206":"We need to understand the problem we are facing and datasets type.\n\nWe want to predict the survival rate for the given test data. Along with that, we have certain titanic datasets information and Survived rate as 0 and 1.\n\nThis is the Regression Problem. And the examples of the Supervised learning. We will verify the Models with the kFOLD and Evaluates the accuracy.","f6217c8e":"Let calculate, gender of the people who survived or not","3e94da6a":"As expected. People who purchased FIrst class Ticket are safest and survived in the disaster.","67700ab1":"There are total 891 Training data nad 418 Testing data. We will go for EDA for the training data","60773d25":"<a id=\"background\"><\/a>\n<h3>   \n          <span>\n            1.1 Background info :\n            <\/span>   \n    \n<\/h3>","705c7cfd":"<a id=\"inspect\"><\/a>\n<h3>   \n    <span>\n            3.2 Overview the dataframes\n    <\/span>   \n \n<\/h3>","161e2116":"<h2>   \n      <span>          \n           Contents\n    <\/span>\n       \n<\/h2>\n<span>\n    <ul>\n        <li><a href='#intro'>1. Introduction<\/a><\/li>\n        <ul>\n            <li><a href='#background'>1.1 Background info<\/a><\/li>\n            <li><a href='#data'>1.2 Dataset information<\/a><\/li>\n        <\/ul>\n        <li><a href='#libraries'>2. Python Libraries<\/a><\/li>\n        <ul>\n            <li><a href='#python'>2.1 Import Python Libraries<\/a><\/li>\n         <\/ul>\n        <li><a href='#understand'>3. Understanding the data<\/a><\/li>\n        <ul>\n            <li><a href='#import'>3.1 Importing the input csv<\/a><\/li>\n            <li><a href='#inspect'>3.2 Overview the dataframes<\/a><\/li>\n            <li><a href='#unwanted'>3.3 Delete Unwanted Columns<\/a><\/li>\n            <li><a href='#nullcal'>3.4 Fixing the Null Values<\/a><\/li>\n        <\/ul>\n              <li><a href='#eda'>4. EDA (Exploratory Data Anslysis)<\/a><\/li>\n        <ul>\n            <li><a href='#survival'>4.1 Survival<\/a><\/li>\n            <li><a href='#gender'>4.2 Gender<\/a><\/li>\n            <li><a href='#age'>4.3 Age Group<\/a><\/li>\n            <li><a href='#fare'>4.4 Fare Values<\/a><\/li>\n            <li><a href='#ticketclass'>4.5 Ticket class<\/a><\/li>   \n            <li><a href='#embark'>4.6 Embarked<\/a><\/li>            \n        <\/ul>\n              <li><a href='#eda'>5. Model Training<\/a><\/li>\n        <ul>\n            <li><a href='#logistic'>5.1 Logistic Regression<\/a><\/li>\n            <li><a href='#random'>5.2 Random Forst Classifier<\/a><\/li>\n            <li><a href='#suport'>5.3 Support vector machines<\/a><\/li>\n            <li><a href='#knn'>5.4 KNN Classifier<\/a><\/li>\n            <li><a href='#navis'>5.5 Naives Bayes<\/a><\/li>   \n            <li><a href='#decision'>5.6 Decision Tree<\/a><\/li>    \n            <li><a href='#decision'>5.7 ADABoost<\/a><\/li> \n            <li><a href='#decision'>5.8 Linear Discrimination<\/a><\/li> \n        <\/ul>\n                <li><a href='#model_Ev'>6. Model Evaluation<\/a><\/li>\n        <ul>\n            <li><a href='#model_compar'>6.1 Model comparison<\/a><\/li>\n        <\/ul>  \n                <li><a href='#submis'>7. Submission<\/a><\/li>\n        <ul>\n            <li><a href='#model_subm'>7.1 Model submission<\/a><\/li>\n                        \n","676cf9c0":"<p>\n    \n RMS Titanic was a British passenger liner operated by the White Star Line that sank in the North Atlantic Ocean in the early morning hours of 15 April 1912, after striking an iceberg during her maiden voyage from Southampton to New York City. Of the estimated 2,224 passengers and crew aboard, more than 1,500 died, making the sinking one of modern history's deadliest peacetime commercial marine disasters.    \n    \n    \nsource:  https:\/\/en.wikipedia.org\/wiki\/RMS_Titanic\n <\/p>","6518497c":"<a id=\"import\"><\/a>\n<h3>   \n    <span>\n            3.1 Importing the input csv\n    <\/span>   \n \n<\/h3>","324ab949":"<a id=\"embark\"><\/a>\n<h3>   \n    <span>\n           4.6 Embarked\n    <\/span>   \n \n<\/h3>","fa60cbb0":"<a id=\"fare\"><\/a>\n<h3>   \n    <span>\n           4.4 Fare Values\n    <\/span>   \n \n<\/h3>","e41b76ed":"<a id=\"logistic\"><\/a>\n<h3>   \n    <span>\n           5.8 Linear Discrimination\n    <\/span>   \n \n<\/h3>","9efcfe6c":"<a id=\"knn\"><\/a>\n<h3>   \n    <span>\n           5.4 KNN Classifier\n    <\/span>   \n \n<\/h3>","c5699965":"Observations\n\nNumber of passengers belong to Third class ticket as compared to the upper class.\n\nSO, we believe that, people likely to book ticket that belongs to economic or third class","51541e78":"**Observations**\n\nPeople who paid low fares are less likely to survive, which relates them to the Ticket class they purchased.\n\nLow Fares == Low Ticket class == Less Survival chances","e14a2d2a":"<a id=\"data\"><\/a>\n<h3>   \n          <span>\n            1.2 Dataset information :\n            <\/span>   \n    \n<\/h3>","e99bf38a":"### Data is ready to train[](http:\/\/)","332ff2d5":"**Observation:**\n\nAmong not survived people , majority are male as compared to female. Let dive deeper in the dataset","3354b213":"**Observations**\n\n1.  Age: 0 - 21    = 171 people\n2.  Age: 21 - 41   = 374 people\n3.  Age: 41+       = 346 people\n\nMAjority of the people in the Titanic were of more than 21+ Age.\n","90eed85f":"<a id=\"model_compar\"><\/a>\n<h3>   \n    <span>\n          6.1 Model comparison\n    <\/span>   \n \n<\/h3>","4a8ea70b":"<a id=\"logistic\"><\/a>\n<h3>   \n    <span>\n           5.1 Logistic Regression\n    <\/span>   \n \n<\/h3>","6558cdd0":"<a id=\"submis\"><\/a>\n<h2>   \n    <span>\n           7. Submission\n    <\/span>   \n \n<\/h2>","575c942d":"<a id=\"intro\"><\/a>\n<h2>   \n    <font  color='red'>\n          <span>\n            1. Introduction :\n            <\/span>   \n    <\/font>\n<\/h2>","0dead08c":"<a id=\"understand\"><\/a>\n<h2>   \n    <font  color='red'>\n          <span>\n            3. Understanding the data:\n            <\/span>   \n    <\/font>\n<\/h2>\n","00b4764d":"#### Embarked Missing info","7b037402":"In the Test data, we have Age, Cabin and Embark as the missing information features"}}