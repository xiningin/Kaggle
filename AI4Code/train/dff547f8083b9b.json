{"cell_type":{"92317dbd":"code","9d2db8cf":"code","0bf5a782":"code","270469d4":"code","121ffb90":"code","c0d0d618":"code","314976ce":"code","a372591e":"code","6b3e2216":"code","795a1e49":"code","8a65700b":"code","50b07778":"code","24a6d1e6":"code","f887a117":"code","f8c816ba":"code","bbe00cc6":"code","082597e0":"code","43adfe67":"code","3c9b1081":"code","5fb8777c":"code","4803284e":"code","ea2e5478":"code","456c56e2":"code","3307324a":"code","27424058":"code","6d56d364":"code","fbf49eee":"code","66904d13":"code","38101622":"code","0129849c":"code","ffff1d6c":"code","dfc5aa5e":"code","92fbc941":"code","e7a1ed1c":"code","01bfa188":"code","60554e17":"code","3d65590f":"code","67d92d55":"code","9cdfeb3c":"code","c8dec4c6":"code","4fa171f2":"code","6c860103":"code","6a3c3ffd":"code","43dc9697":"code","32d66eaa":"code","809e6a3f":"code","bf991626":"code","54c4dea4":"code","4978dd03":"code","f2629dfc":"code","61ca7487":"code","e4bb8d69":"code","9582c3c3":"code","e6d3fd9d":"code","b3fa505c":"code","8275a550":"code","36f50e18":"code","bb062079":"code","309cc2bb":"code","5abe4726":"code","45a07b3f":"code","557b2571":"code","741e4379":"code","26272c49":"code","e277450a":"code","e1f174bb":"code","95166caa":"code","de9cd504":"code","501c3ddc":"code","367c7dc7":"markdown","b68b3abf":"markdown","123502ef":"markdown","15347c91":"markdown","6708decd":"markdown","df2874a7":"markdown","75658fa9":"markdown","448be1a3":"markdown","889908d8":"markdown","b9af799d":"markdown","bb429c27":"markdown","fe1c816f":"markdown","ee6ece23":"markdown","8d4fd5fb":"markdown","76f33893":"markdown","c5902e97":"markdown","4b0982c1":"markdown","10caec28":"markdown","a75e3aa7":"markdown"},"source":{"92317dbd":"# Import all tools we need\n\n# Regular EDA (exploratory data analysis) and plotting libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# we want our plots to appear inside the notebook\n%matplotlib inline \n\n# Models from Scikit-Learn\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Model Evaluations\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.metrics import precision_score, recall_score, f1_score\nfrom sklearn.metrics import plot_roc_curve","9d2db8cf":"df = pd.read_csv('\/kaggle\/input\/heartdisease\/data\/heart-disease.csv', delimiter=',', nrows=None)\ndf.shape ","0bf5a782":"df.head()","270469d4":"df.tail()","121ffb90":"# Let's find out how many of each classes there\ndf.target.value_counts()","c0d0d618":"df.target.value_counts().plot(kind=\"bar\", color=[\"salmon\", \"lightblue\"]);","314976ce":"df.info()","a372591e":"# Are there any missing values?\ndf.isna().sum()","6b3e2216":"df.describe()","795a1e49":"df.sex.value_counts()","8a65700b":"# Compare target column with sex column  \npd.crosstab(df.target, df.sex)","50b07778":"# Create a plot of crosstab\npd.crosstab(df.target, df.sex).plot(kind=\"bar\", \n                                    color=[\"salmon\", \"lightblue\"],\n                                    figsize=(10, 6));\n\nplt.title(\"Heart Disease Frequency for Sex\")\nplt.xlabel(\"0 = No Disease, 1 = Disease\")\nplt.ylabel(\"Amount\")\nplt.legend([\"Female\", \"Male\"]);\nplt.xticks(rotation=0)","24a6d1e6":"df.head()","f887a117":"# Creating another figure\nplt.figure(figsize=(10, 6))\n\n# Scatter with positive examples\nplt.scatter(df.age[df.target == 1],\n            df.thalach[df.target == 1],\n            color = \"salmon\")\n\n# Scatter with negetive examples\nplt.scatter(df.age[df.target == 0],\n            df.thalach[df.target == 0],\n            color = \"lightblue\")","f8c816ba":"df.age[df.target == 1]","bbe00cc6":"# Check the distribution of age column with histogram\ndf.age.plot.hist();","082597e0":"pd.crosstab(df.cp, df.target)","43adfe67":"# Make the crosstab more visual \npd.crosstab(df.cp, df.target).plot(kind=\"bar\",\n                                   figsize=(10, 6),\n                                   color = [\"red\",\"blue\"])\n\n# Add some communication\nplt.title('Heart Disease Freq per chest pain type')\nplt.xlabel('Chest pain type')\nplt.ylabel(\"Amount\")\nplt.legend([\"No Disease\", \"Disease\"])\nplt.xticks(rotation = 0);","3c9b1081":"df.head()","5fb8777c":"# Make a correlation matrix\ndf.corr()","4803284e":"# Let's make our correlation matrix more prettier\ncorrMatrix = df.corr()\nfig, ax = plt.subplots(figsize=(15, 10))\nax = sns.heatmap(corrMatrix,\n                 annot = True,\n                 linewidths = 0.5,\n                 fmt = \".2f\",\n                 cmap = \"YlGnBu\")\n","ea2e5478":"df.head()","456c56e2":"# Split data to X & y\nX = df.drop(\"target\", axis=1)\n\ny  = df.target","3307324a":"X","27424058":"y","6d56d364":"# Split data to train & test sets\nnp.random.seed(44)\n\n# Split into train & test set\nX_train, X_test, y_train, y_test = train_test_split(X,\n                                                    y,\n                                                    test_size=0.2)","fbf49eee":"X_train","66904d13":"y_train, len(y_train)","38101622":"# Put models in a dictionary\nmodels = {\"Logistic Regression\":LogisticRegression(),\n          \"KNN\":KNeighborsClassifier(),\n         \"Random Forest\":RandomForestClassifier()}\n\n# Create a function to fit and scroe models\ndef fit_and_score(models, X_train, X_test, y_train, y_test):\n    \"\"\"\n    Fits and evaluates given machine learning models.\n    models: a dict of different Scikit-Learn machine learning models\n    X_train: training data (no labels)\n    X_test: testing data(no labels)\n    y_train: training labels\n    y_test: test labels\n    \"\"\"\n    # Set random seed\n    np.random.seed(44)\n    # Make a dictionary to keep model scores\n    model_scores = {}\n    # Loop through models\n    for name, model in models.items():\n        # Fit the model to the data\n        model.fit(X_train, y_train)\n        # Evaluate the model and append its score to model_scores\n        model_scores[name] = model.score(X_test, y_test)\n    return model_scores","0129849c":"model_scores = fit_and_score(models=models,\n                             X_train=X_train,\n                             X_test=X_test,\n                             y_train=y_train,\n                             y_test=y_test)\n\nmodel_scores","ffff1d6c":"models_compare = pd.DataFrame(model_scores, index=[\"accuracy\"])\nmodels_compare.T.plot(kind=\"bar\")\n\nplt.xticks(rotation =0);","dfc5aa5e":"# Let's tune KNN\n\ntrain_scores = []\ntest_scores = []\n\n# Create a list of different values for n_neighbors\nneighbors = range(1, 21)\n\n# Setup KNN instance\nknn = KNeighborsClassifier()\n\n# Loop through different n_neighbors \nfor i in neighbors:\n    knn.set_params(n_neighbors=i)\n    \n    # Fit the algorithm\n    knn.fit(X_train, y_train)\n    \n    # Update training scores list\n    train_scores.append(knn.score(X_train, y_train))\n    \n    # Update test scores list\n    test_scores.append(knn.score(X_test, y_test))","92fbc941":"train_scores","e7a1ed1c":"test_scores","01bfa188":"plt.plot(neighbors, train_scores, label=\"Train score\")\nplt.plot(neighbors, test_scores, label=\"Test score\")\nplt.xticks(np.arange(1, 21, 1))\nplt.xlabel(\"No of neighbors\")\nplt.ylabel(\"Model score\")\nplt.legend();\n\nprint(f'Maximum KNN score on test data:{max(test_scores)* 100:.2f}%')","60554e17":"# Create a hyperparameter grid for LogisticRegression\nlog_reg_grid ={\"C\":np.logspace(-4, 4, 20),\n              \"solver\": [\"liblinear\"]}\n\n# Create a hyperparameter grid for RandomForestClassifier\nrf_grid ={\"n_estimators\": np.arange(10, 1000, 50),\n          \"max_depth\":[None, 3, 5, 10],\n          \"min_samples_split\":np.arange(2, 20, 2),\n          \"min_samples_leaf\": np.arange(1, 20, 2)}","3d65590f":"# Tune LogisticRegression\n\nnp.random.seed(34)\n\n# Setup random hyperparameter search for LogisticRegression\nrs_log_reg = RandomizedSearchCV(LogisticRegression(),\n                                param_distributions=log_reg_grid,\n                                cv=5,\n                                n_iter=20,\n                                verbose=True)\n\n# Fit random hyperparameter search model for LogisticRegression\nrs_log_reg.fit(X_train, y_train)","67d92d55":"rs_log_reg.best_params_","9cdfeb3c":"rs_log_reg.score(X_test, y_test)","c8dec4c6":"# Setup random seed\nnp.random.seed(44)\n\n# Setup hyperparameter search for RandomForestClassifier\nrs_rf = RandomizedSearchCV(RandomForestClassifier(),\n                           param_distributions = rf_grid,\n                           cv=5,\n                           n_iter=2,\n                           verbose=True)\n\n# Fit random hyperparamete search for RandomForestClassifier\nrs_rf.fit(X_train, y_train)","4fa171f2":"# Find the best hyperparameters\nrs_rf.best_params_","6c860103":"# Evaluate the randomized search RandomForestClassifier model\nrs_rf.score(X_test, y_test)","6a3c3ffd":"model_scores","43dc9697":"# Different hyperparameter for LogisticRegression model\nlog_reg_grid = {\"C\": np.logspace(-4, 4, 30),\n               \"solver\": ['liblinear']}\n\n# Setup grid hyperparameter search for LogisticRegression\ngs_log_reg = GridSearchCV(LogisticRegression(),\n                          param_grid=log_reg_grid,\n                          cv=5,\n                          verbose=True)\n\n# Fit grid hyperparameter search model\ngs_log_reg.fit(X_train, y_train);","32d66eaa":"# Check the best parameters\ngs_log_reg.best_params_","809e6a3f":"# Evaluate the grid search LogisticRegression model\ngs_log_reg.score(X_test, y_test)","bf991626":"# Make predictions with tuned model\ny_preds = gs_log_reg.predict(X_test)","54c4dea4":"y_preds","4978dd03":"y_test","f2629dfc":"# Plot ROC curve and calculate and AUC metric\nplot_roc_curve(gs_log_reg, X_test, y_test);","61ca7487":"# Confusion matrix\nconfusion_matrix(y_test,y_preds)","e4bb8d69":"sns.set(font_scale=1.5)\n\ndef plot_conf_mat(y_test, y_preds):\n    \"\"\"\n    Plots a nice looking confusion matrix using Seaborn's heatmap()\n    \"\"\"\n    fig, ax = plt.subplots(figsize=(3,3))\n    ax = sns.heatmap(confusion_matrix(y_test, y_preds),\n                     annot=True,\n                     cbar=True)\n    plt.xlabel(\"True label\")\n    plt.ylabel(\"Predicted label\")\n    \nplot_conf_mat(y_test, y_preds)","9582c3c3":"print(classification_report(y_test, y_preds))","e6d3fd9d":"# Check best hyperparameters\ngs_log_reg.best_params_","b3fa505c":"# Create a new classifier with best parameters\nclf = LogisticRegression(C = 4.893900918477489,\n                        solver = \"liblinear\")","8275a550":"# Cross-validated accuracy\ncv_acc = cross_val_score(clf,\n                         X,\n                         y,\n                         cv=5,\n                         scoring = \"accuracy\")\ncv_acc","36f50e18":"cv_acc = np.mean(cv_acc)\ncv_acc","bb062079":"# Cross-validated precision\ncv_precision = cross_val_score(clf,\n                               X,\n                               y,\n                               cv=5,\n                               scoring = \"precision\")\ncv_precision = np.mean(cv_precision)\ncv_precision","309cc2bb":"# Cross-validated recall\ncv_recall = cross_val_score(clf,\n                            X,\n                            y,\n                            cv=5,\n                            scoring = \"recall\")\ncv_recall = np.mean(cv_recall)\ncv_recall","5abe4726":"# Cross-validated f1-score\ncv_f1 = cross_val_score(clf,\n                        X,\n                        y,\n                        cv=5,\n                        scoring = \"f1\")\ncv_f1 = np.mean(cv_f1)\ncv_f1","45a07b3f":"# Visualize cross-validated metrics\ncv_metrics = pd.DataFrame({\"Accuracy\": cv_acc,\n                           \"Precision\": cv_precision,\n                           \"Recall\":cv_recall,\n                           \"F1\":cv_f1},\n                         index= [0])\n\ncv_metrics.T.plot(kind=\"bar\",\n                  title= \"Cross-validated classification metrics\",\n                  legend=False)\nplt.xticks(rotation=0);","557b2571":"# Fit an instance of LogisticRegression\nclf = LogisticRegression(C = 4.893900918477489,\n                        solver='liblinear')\nclf.fit(X_train, y_train);","741e4379":"# Check  coef_\nclf.coef_","26272c49":"X","e277450a":"X.head()","e1f174bb":"# Match coef's of features to columns\nfeature_dict = dict(zip(df.columns, list(clf.coef_[0])))\nfeature_dict","95166caa":"# Visualize feature impoertance\nfeature_df = pd.DataFrame(feature_dict, index=[0])\nfeature_df.T.plot(kind=\"bar\",\n                  title=\"Feature Importance\",\n                  figsize=(6,6),\n                 legend = False)\nplt.xticks(rotation=90);","de9cd504":"pd.crosstab(df.sex, df.target)","501c3ddc":"pd.crosstab(df.slope, df.target)","367c7dc7":"Now we've got our baseline model...and we know a model's first predictions aren't always what we should based our next steps off. What should do?\n\nLet's look at the following:\n* Hyperparameter tuning\n* Feature importance\n* Confusion matrix\n* Cross-vaidation\n* Precision\n* Recall\n* F1 score\n* Classification report\n* ROC curve\n* Area under the curve (AUC)\n\n### Hyperparameter tuning","b68b3abf":"### Preparing the tools\n\nWe 're going to use pandas, Matplotlib and NumPy for data analysis and manipulation.","123502ef":"## Evaluating our tuned machine learning classifier beyond accuracy\n\n* ROC curve and AUC curve\n* Confusion matrix\n* Classification report\n* Precision \n* Recall\n* F1-score \n\n... and it would be great if cross-validation was used where possible.","15347c91":"### Feature Importance\n\nFeature importance is another as asking, \"which feature contributed most to outcomes of the model and how did they contribute?\"\n\nFinding feature importance is different for each machine learning model. One way to find feature importance is to search for '(MODEL NAME) feature importance'.\n\nLets find feature importance for our LogisticRegression model...","6708decd":"Now we've got hyperparameter grids setup for each model, let's tune them RandomizedSearchCV...","df2874a7":"## Load data","75658fa9":"## Heart Disease Frequency per chest pain type\n3. cp - chest pain type\n\n* Typical angina: chest pain related decrease blood supply to the heart\n* Atypical angina: chest pain not related to heart\n* Non-anginal pain: typically esophageal spasms (non heart related)\n* Asymptomatic: chest pain not showing signs of disease","448be1a3":"slope - the slope of the peak exercise ST segment\n\n* 0: Upsloping: better heart rate with excercise (uncommon)\n* 1: Flatsloping: minimal change (typical healthy heart)\n* 2: Downslopins: signs of unhealthy heart\n","889908d8":"## Calculate evaluation metrics using cross-validation\n\nWe're going to calculate accuracy, precision score, recall and f1-score of our modelusing cross-validation and to do so we'll be using  `cross_val_score()`.","b9af799d":"## Model Comparison","bb429c27":"Now we've got our data split into training and test sets, it's time to build a machine learning model\n\nWe'll train it (find the patterns) on the training set.\n\nAnd we'll test it (use the patterns) on the test set.\n\nWe're going to try 3 different machine learning models:\n1. Logistic Regression\n2. K-Nearest Neighbor s Clasifier\n3. Random Forest Classifier","fe1c816f":"## Data Exploration(EDA )\n\nThe goal here is to find out more about the data and become a subject matter expert ondata you're working with. \n\n1. What questions are you trying to solve?\n2. What kind of data do we have and how do we treat different types?\n3. What's missing from the data and how do you deal with it?\n4. Where are the outliers and why should you care about it?\n5. How can you add, change or remove features to get more out of your data? ","ee6ece23":"Now we've tuned LogisticRegression(), lets do the same for RandomForestClassifier()...","8d4fd5fb":"Now we've got a ROC curve, an AUC metric and a confusion matrix, let's get a classification report as well as cross-validated precision, recall and f1-score.","76f33893":"## Hyperparameter Tuning with GridSearchCV\n\nSince our LogisticRegression model provides the best scores so far, we'll try and improve them again using GridSearchCV...","c5902e97":"### Age vs Max Heart Rate for Heart Disease","4b0982c1":"### Heart Disease Frequency according to Sex","10caec28":"## 5. Modelling","a75e3aa7":"## Hyperparameter tuning with RandomizedSearchCV\n\nWe're going to tune:\n* LogisticRegression()\n* RandomForestClassifier()\n\n... using RandomizedSearchCV"}}