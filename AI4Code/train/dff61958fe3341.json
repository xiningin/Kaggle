{"cell_type":{"0dc08bf8":"code","afe9509c":"code","6cc08433":"code","d6d8f65b":"code","06634fb7":"code","1b6784c3":"code","de876f4d":"code","c41e67c5":"code","549184af":"code","421671c2":"markdown","11c6fbd6":"markdown","57530756":"markdown","d1b3423f":"markdown","d48eb8ea":"markdown","9017c805":"markdown"},"source":{"0dc08bf8":"#import libraries\nimport tensorflow as tf \nimport numpy as np \nimport pandas as pd \nimport keras \nimport matplotlib.pyplot as plt\nfrom keras.layers import Dropout","afe9509c":"#Import the  Mnist data (images of handwritten digits and their labels)\n(training_images, training_labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data()","6cc08433":"normaliz = keras.layers.experimental.preprocessing.Normalization()\nnormaliz.adapt(training_images)\n","d6d8f65b":"from tensorflow.keras import regularizers\n\ninputs = keras.Input(shape=(28, 28,1))\nx = normaliz(inputs)\nx = keras.layers.experimental.preprocessing.Normalization()(x)\nx = keras.layers.Conv2D(32, 5, padding='same', activation='relu', input_shape=(28, 28, 1))(x)\nx = keras.layers.Conv2D(64, kernel_size=(3, 3),activation='relu')(x)\nx=  keras.layers.MaxPooling2D(pool_size=(2, 2))(x)\nx = keras.layers.Flatten(input_shape=(28, 28))(x)\nx = keras.layers.Dense(256, activation='relu')(x)\nx = keras.layers.Dense(10, activation='softmax')(x)\nmodel = keras.Model(inputs=inputs, outputs=x)","06634fb7":"model.compile(optimizer=\"adam\",\n                loss=\"sparse_categorical_crossentropy\",\n                metrics=['accuracy'])\nx_train1= training_images.reshape(training_images.shape[0],28,28,1)\nx_test1= test_images.reshape(test_images.shape[0],28,28,1)","1b6784c3":"model.fit(x_train1, training_labels,\n          batch_size=64,\n          epochs=10,\n           validation_data=(x_test1, test_labels))","de876f4d":"#with regularization\ninputs = keras.Input(shape=(28, 28,1))\nx = normaliz(inputs)\nx = keras.layers.experimental.preprocessing.Normalization()(x)\nx = keras.layers.Conv2D(32, 5, padding='same', activation='relu', input_shape=(28, 28, 1))(x)\nx = keras.layers.Conv2D(64, kernel_size=(3, 3),activation='relu')(x)\nx=  keras.layers.MaxPooling2D(pool_size=(2, 2))(x)\nx = keras.layers.Flatten(input_shape=(28, 28))(x)\nx = keras.layers.Dense(256, activation='relu',kernel_regularizer=tf.keras.regularizers.l1(0.01),\n                              activity_regularizer=tf.keras.regularizers.l2(0.01))(x)\nx = keras.layers.Dense(10, activation='softmax')(x)\nmodel = keras.Model(inputs=inputs, outputs=x)","c41e67c5":"model.compile(optimizer=\"adam\",\n                loss=\"sparse_categorical_crossentropy\",\n                metrics=['accuracy'])\nx_train1= training_images.reshape(training_images.shape[0],28,28,1)\nx_test1= test_images.reshape(test_images.shape[0],28,28,1)","549184af":"model.fit(x_train1, training_labels,\n          batch_size=64,\n          epochs=10,\n           validation_data=(x_test1, test_labels))","421671c2":"for L1_L2 function\n\n> tf.keras.regularizers.l1_l2(l1=0.01, l2=0.01","11c6fbd6":"# If you like this Notebook\n# please Vote and comment ,thanks","57530756":"#  L2 regularisation\n\nL2 regularisation technique is called Ridge regression.\n\nthe L2 regularization tries to estimate the mean of the data to avoid overfitting.\n\nL2, on the other hand, is useful when you have collinear\/codependent features. (An example pair of codependent features is gender and ispregnant since, at the current level of medical technology, only females can be ispregnant.) Codependence tends to increase coefficient variance, making coefficients unreliable\/unstable, which hurts model generality. L2 reduces the variance of these estimates, which counteracts the effect of codependencies.\n\n\nLoss function with L2 regularisation :\n\n> Loss=Error(Y\u2212Y\u02c6)+\u03bb\u22111n|wi|**2\n\n> L = y log (wx + b) + (1 - y)log(1 - (wx + b)) + lambda*(w2)** 2\n\n\nor L2 The L2 regularization penalty is computed as: loss = l2 * reduce_sum(square(x))\n\n> tf.keras.regularizers.l2(l2=0.01)\n\n> tf.keras.layers.Dense(3, kernel_regularizer='l2')\n\n\n","d1b3423f":"for L1 The L1 regularization penalty is computed as: loss = l1 * reduce_sum(abs(x))\n\n \n> tf.keras.regularizers.l1(l1=0.01)\n\n> tf.keras.layers.Dense(3, kernel_regularizer='l1')","d48eb8ea":"# L1 Regularization\n\nLasso is another variation, in which the above function is minimized. Its clear that this variation differs from ridge regression only in penalizing the high coefficients. It uses |\u03b2j|(modulus)instead of squares of \u03b2, as its penalty. In statistics, this is known as the L1 norm.\n\n\nL1 regularization tries to estimate the median of the data\n\nL1 tends to shrink coefficients to zero \n\nL1 is therefore useful for feature selection, as we can drop any variables associated with coefficients that go to zero\n\nL1 regularization is sometimes used as a feature selection method. Suppose you have some kind of hard cap on the number of features you can use (because data collection for all features is expensive, or you have tight engineering constraints on how many values you can store, etc.). You can try to tune the L1 penalty to hit your desired number of non-zero features.\n\n\nLoss function with L1 regularisation :\n\n> Loss=Error(Y\u2212Y\u02c6)+\u03bb\u22111n|wi|\n\n> L = y log (wx + b) + (1 - y)log(1 - (wx + b)) + lambda*||w||1 ","9017c805":"# Regularization\n\n\nIt is a form of regression, that constrains\/ regularizes or shrinks the coefficient estimates towards zero. In other words, this technique discourages learning a more complex or flexible model, so as to avoid the risk of overfitting.\n\n\n\n\n\nThe coefficients are chosen, such that they minimize this loss function.\n\nLoss function  :  is the function that computes the distance between the current output of the algorithm and the expected output , so we add a cofficient to it to minimize the error \n\n\n\nthis will adjust the coefficients based on your training data. If there is noise in the training data, then the estimated coefficients won\u2019t generalize well to the future data. This is where regularization comes in and shrinks or regularizes these learned estimates towards zero.\n\n\n\nWe have two types of Refularization : \n1. L1 Regularization or Lasso Regularization\n\n2. L2 Regularization or Ridge Regularization\n "}}