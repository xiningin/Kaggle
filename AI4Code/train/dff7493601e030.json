{"cell_type":{"1f29bf88":"code","ce04a798":"code","77c577c9":"code","17401453":"code","db7b5184":"code","297c0f07":"code","29a6c2a0":"code","0ad0ee04":"code","f1d16ea2":"code","64d6a324":"code","34ad331f":"code","781d402a":"code","83a8ca44":"code","7eb77be6":"code","5574afc0":"code","11d695c2":"code","7808464b":"code","e8d48ba3":"code","7ac39e3d":"code","a5c90f85":"code","daa0f5e8":"code","37ead911":"code","3c2a4e9f":"markdown","0b7e4994":"markdown","457fd658":"markdown","2d7c61fb":"markdown","352702be":"markdown","ba7edf29":"markdown","2065a1d0":"markdown","17f91ee8":"markdown","a5f9bbee":"markdown","3594f989":"markdown","372d74fd":"markdown","70f40ede":"markdown","65c133c4":"markdown","a616273a":"markdown","98895987":"markdown","4f3eafc3":"markdown","b4667fe8":"markdown"},"source":{"1f29bf88":"import numpy as np\nimport pandas as pd\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","ce04a798":"train = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\ny_out = pd.read_csv(\"..\/input\/titanic\/gender_submission.csv\")\nprint(train.shape)\nprint(test.shape)","77c577c9":"print(train.head())\nprint(test.head())","17401453":"train = train.drop(['Name','Ticket',\"Cabin\"],axis = 1)\ntest = test.drop(['Name','Ticket',\"Cabin\"],axis = 1)","db7b5184":"print(train.shape)","297c0f07":"y = train.iloc[:,1]\nx = train.iloc[:,2:]\nx_test = test.iloc[:,1:]","29a6c2a0":"print(x.isnull().sum())\nprint(x_test.isnull().sum())","0ad0ee04":"print(x.mode())\nx.iloc[:,2] = x.iloc[:,2].fillna(value = x.iloc[:,2].mean()) \nx.iloc[:,-1] = x.iloc[:,-1].fillna(value = x.iloc[:,-1].mode()[0])\nprint(x.isnull().sum())\n\nprint(x_test.mode())\nx_test.iloc[:,2] = x_test.iloc[:,2].fillna(value = x_test.iloc[:,2].mean()) \nx_test.iloc[:,-2] = x_test.iloc[:,-2].fillna(value = x_test.iloc[:,-2].mode()[0])\nprint(x_test.isnull().sum())","f1d16ea2":"x = x.values\ny = y.values\nx_test = x_test.values","64d6a324":"from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nx[:,-1] = le2.fit_transform(x[:,-1])\nx_test[:,-1] = le2.transform(x_test[:,-1])","34ad331f":"print(x)\nprint(x_test)","781d402a":"from sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder\nct = ColumnTransformer(transformers=[(\"encoder\",OneHotEncoder(),[1,-1])] , remainder = \"passthrough\")\nx = ct.fit_transform(x)\nx_test = ct.transform(x_test)\nprint(x)","83a8ca44":"from sklearn.preprocessing import StandardScaler\nsc_x = StandardScaler()\nx = sc_x.fit_transform(x)\nx_test = sc_x.transform(x_test)","7eb77be6":"print(x[:5,:])\nprint(x_test[:5,:])","5574afc0":"import tensorflow as tf\nx = tf.convert_to_tensor( x, dtype=tf.float32)","11d695c2":"print(x.shape)","7808464b":"import keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import LeakyReLU,PReLU,ELU\nfrom keras.layers import Dropout\nfrom keras import regularizers","e8d48ba3":"classifier = Sequential()\n\n# Adding the input layer and the first hidden layer\nclassifier.add(Dense(units = 32, kernel_initializer = 'he_uniform', kernel_regularizer = tf.keras.regularizers.l2(0.01), activation='relu',input_dim = 10))\n\n# Adding the second hidden layer\nclassifier.add(Dense(units = 64, kernel_initializer = 'he_uniform', kernel_regularizer = tf.keras.regularizers.l2(0.01), activation='relu'))\n\n# Adding the third hidden layer\nclassifier.add(Dense(units = 128, kernel_initializer = 'he_uniform', kernel_regularizer = tf.keras.regularizers.l2(0.01), activation='relu'))\n\n# Adding the forth hidden layer\nclassifier.add(Dense(units = 64, kernel_initializer = 'he_uniform', kernel_regularizer = tf.keras.regularizers.l2(0.01), activation='relu'))\n\n# Adding the fifth hidden layer\nclassifier.add(Dense(units = 32, kernel_initializer = 'he_uniform', kernel_regularizer = tf.keras.regularizers.l2(0.01), activation='relu'))\n\n# Adding the output layer\nclassifier.add(Dense(units = 1, kernel_initializer = 'glorot_uniform', activation = 'sigmoid'))\n\n# Compiling the ANN\nclassifier.compile(optimizer = 'Adamax', loss = 'binary_crossentropy', metrics = ['accuracy'])\n\n# Fitting the ANN to the Training set\nmodel_history=classifier.fit(x ,y,validation_split=0.1, epochs = 1000, batch_size = 4)","7ac39e3d":"x_test = tf.convert_to_tensor( x_test, dtype=tf.float32)","a5c90f85":"y_pred = classifier.predict(x_test)\n\ny_pred[y_pred > 0.5] = int(1)\ny_pred[y_pred < 0.5] = 0\nprint(type(y_pred))\ny_pred = y_pred.astype(int)","daa0f5e8":"y_pred = pd.DataFrame(y_pred)\ny_out[\"Survived\"] = y_pred","37ead911":"y_out.to_csv(\"submission.csv\", index = False)","3c2a4e9f":"* Importing Tensorflow\n* Converting x dataset into a tensor.","0b7e4994":"We will be using a Neural Network with a input layer, 3 hidden layes and a output layer","457fd658":"Both the datasets are converted into numpy arrays.","2d7c61fb":"Checking for any missing values.","352702be":"Exporting predicted values to csv file by first converting it into dataframe","ba7edf29":"Printing first 5 rows to make sure data is scaled.","2065a1d0":"Splitting the training dataset into x and y:\n* x contains all features that we need to pass to neural network.\n* y contains the output variable that needs to be predicted.","17f91ee8":"* In x dataset (i.e training dataset) \"Age\" and \"Embarked\" columns have missing values.\n* In x_test dataset \"Age\" and \"Fare\" coulmns have missing values.","a5f9bbee":"* Converting x_test to tensor\n* Remember that x_test is already preprocessed (i.e. dealing with missing values and scaling)","3594f989":"Droping the columns that won't be helpful for model to make predictions.","372d74fd":"# For Experimenting\n* You can add or remove layes just dont change input and output layer.\n* You can try using different number of neurons in paricular layers by changing units parameter.\n* If you make any changes to dataset make sure you select right value at ***input_dim***.\n* * You can also experiment with:\n* validation split\n* no of epochs\n* batch size\n","70f40ede":"Making the prediction and converting the probabilities into categories(i.e 1 or 0)","65c133c4":"The \"Embarked\" column contains code names of ports from which people were embarked.\nThere are only 3 of them\nWe have to encode them into number.","a616273a":"Replacing missing values with most frequent values.","98895987":"# Primary Focus will be to get the Neural Network working and getting the data ready.\n# Everything that you can experiment with and fine tune is explained.\n# Just enough Exploratory Data Analysis is performed in this notebook.","4f3eafc3":"* In the next step we are scaling the data.\n* We are using standard scaler it transforms the distribution of data into a normal distribution with mean=0 and standard deviation = 1\n* Scaling should be done cause it makes the computation stable and fast.","b4667fe8":"**Importing the data**\nand getting the shapes of the train and test dataframes"}}