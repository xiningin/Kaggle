{"cell_type":{"4f34fac2":"code","96f77b8c":"code","504fa83d":"code","125f2870":"code","751a7c56":"code","f34c2025":"code","1632ad93":"code","d23eab31":"code","8c8c35c2":"code","782b6804":"code","d16b0464":"code","81604891":"code","bbbded7e":"code","dfbaaa7b":"code","24c839fc":"code","2ba5d0a6":"code","71b0d5dd":"code","39a24a1b":"code","01d37503":"code","81f04ad3":"code","7233baca":"code","c591828f":"code","14e68753":"code","2014d680":"code","53823710":"code","fb0689a9":"code","315b54f8":"code","3314292c":"code","f2f153cb":"code","5b2dee18":"code","2f8a652a":"code","f08cc9ec":"code","5c13c63d":"code","0201cec3":"code","966cecd8":"code","da407b53":"code","e0248046":"code","8dc2bf70":"code","af2d88fa":"code","69d979fb":"code","571356c2":"code","33632488":"code","3c6877e7":"code","a5e3899f":"code","72665154":"code","7d75073c":"code","97abc176":"code","cc1a36d5":"code","2a33273b":"code","e566c006":"code","4d74d1f0":"code","353c0b85":"code","dec4e6e6":"code","38e8fb00":"code","60de5225":"code","059e352b":"code","99812495":"code","749dc864":"code","8b2bc9ec":"code","f73a64f3":"code","1ac0e14d":"code","282b3fe6":"code","35f518ce":"code","08164bb1":"code","e487103a":"code","bc3a0379":"code","568762b0":"code","839fe503":"code","2e3671ee":"code","8c5468cf":"code","0eb2e29c":"code","7ad7a9c5":"code","727b37a7":"code","be778495":"code","a2fa7239":"code","76c81076":"code","1552f4f2":"code","fc5c7a12":"code","ce414ee6":"code","cbb7ff9b":"code","4bd9380e":"markdown","16820d1a":"markdown","e73518bf":"markdown","c5c18329":"markdown","9727292b":"markdown","4fce5489":"markdown","7f393f0e":"markdown","7c7bf9df":"markdown","17f58adc":"markdown","53cd46a2":"markdown","186f318b":"markdown","cdab261a":"markdown","fcca51cd":"markdown","0ce13d24":"markdown","bfb03fa6":"markdown","034d2b56":"markdown","49bd6ece":"markdown","4aec0ba5":"markdown","bcd3c813":"markdown","83bfcd25":"markdown","4a542414":"markdown","fd78366f":"markdown","bfc19913":"markdown","ddff6dd0":"markdown","1ccd9671":"markdown","ef67237a":"markdown","67f7f87a":"markdown","ae789d34":"markdown","1e16e25b":"markdown","07f35af0":"markdown","5a4da394":"markdown","cd7f5d3c":"markdown","553dbdf6":"markdown"},"source":{"4f34fac2":"import pandas as pd\nimport numpy as np\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_selection import VarianceThreshold","96f77b8c":"# load the Santander customer satisfaction dataset from Kaggle\n# I load just a few rows for the demonstration\n\ndata = pd.read_csv('..\/input\/santander-customer-satisfaction\/train.csv', nrows=50000)\ndata.shape","504fa83d":"# check the presence of null data.\n# The snippets below will be able to compare nan values between 2 columns,\n# so in principle missing data are not a problem.\n# in any case, we see that there are no missing data in this dataset\n\n[col for col in data.columns if data[col].isnull().sum() > 0]","125f2870":"data.columns","751a7c56":"# separate dataset into train and test\nX_train, X_test, y_train, y_test = train_test_split(\n    data.drop(labels=['TARGET'], axis=1),\n    data['TARGET'],\n    test_size=0.3,\n    random_state=0)\n\nX_train.shape, X_test.shape","f34c2025":"sel = VarianceThreshold(threshold=0)\nsel.fit(X_train)  # fit finds the features with zero variance","1632ad93":"# get_support is a boolean vector that indicates which features are retained\n# if we sum over get_support, we get the number of features that are not constant\nsum(sel.get_support())","d23eab31":"# another way of finding non-constant features is like this:\nlen(X_train.columns[sel.get_support()])","8c8c35c2":"# finally we can print the constant features\nprint(\n    len([\n        x for x in X_train.columns\n        if x not in X_train.columns[sel.get_support()]\n    ]))\n\n[x for x in X_train.columns if x not in X_train.columns[sel.get_support()]]","782b6804":"# let's visualise the values of one of the constant variables\n# as an example\n\nX_train['ind_var2_0'].unique()","d16b0464":"X_train = sel.transform(X_train)\nX_test = sel.transform(X_test)\n\nX_train.shape, X_test.shape","81604891":"import pandas as pd\nimport numpy as np\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_selection import VarianceThreshold","bbbded7e":"# load the Santander customer satisfaction dataset from Kaggle\n# I load just a few rows for the demonstration\ndata = pd.read_csv(r'..\/input\/santander-customer-satisfaction\/train.csv', nrows=50000)\ndata.shape","dfbaaa7b":"# check the presence of null data.\n# The snippets below will be able to compare nan values between 2 columns,\n# so in principle missing data are not a problem.\n# in any case, we see that there are no missing data in this dataset\n\n[col for col in data.columns if data[col].isnull().sum() > 0]","24c839fc":"# separate train and test sets\nX_train, X_test, y_train, y_test = train_test_split(\n    data.drop(labels=['TARGET'], axis=1),\n    data['TARGET'],\n    test_size=0.3,\n    random_state=0)\n\nX_train.shape, X_test.shape","2ba5d0a6":"# using the code from the previous lecture\n# I remove 58 constant features\n\nconstant_features = [\n    feat for feat in X_train.columns if X_train[feat].std() == 0\n]\n\nX_train.drop(labels=constant_features, axis=1, inplace=True)\nX_test.drop(labels=constant_features, axis=1, inplace=True)\n\nX_train.shape, X_test.shape","71b0d5dd":"sel = VarianceThreshold(\n    threshold=0.01)  # 0.1 indicates 99% of observations approximately\n\nsel.fit(X_train)  # fit finds the features with low variance","39a24a1b":"# get_support is a boolean vector that indicates which features \n# are retained. If we sum over get_support, we get the number\n# of features that are not quasi-constant\nsum(sel.get_support())","01d37503":"# another way of doing the above operation:\nlen(X_train.columns[sel.get_support()])","81f04ad3":"# finally we can print the quasi-constant features\nprint(\n    len([\n        x for x in X_train.columns\n        if x not in X_train.columns[sel.get_support()]\n    ]))\n\n[x for x in X_train.columns if x not in X_train.columns[sel.get_support()]]","7233baca":"# percentage of observations showing each of the different values\nX_train['ind_var31'].value_counts() \/ np.float(len(X_train))","c591828f":"# we can then remove the features like this\nX_train = sel.transform(X_train)\nX_test = sel.transform(X_test)\n\nX_train.shape, X_test.shape","14e68753":"import pandas as pd\nimport numpy as np\n\nfrom sklearn.model_selection import train_test_split","2014d680":"# load the Santander customer satisfaction dataset from Kaggle\n# I load just a few rows for the demonstration\ndata = pd.read_csv(r'..\/input\/santander-customer-satisfaction\/train.csv', nrows=15000)\ndata.shape","53823710":"# check the presence of null data.\n# The snippets below will be able to compare nan values between 2 columns,\n# so in principle missing data are not a problem.\n# in any case, we see that there are no missing data in this dataset\n\n[col for col in data.columns if data[col].isnull().sum() > 0]","fb0689a9":"# separate train and test sets\nX_train, X_test, y_train, y_test = train_test_split(\n    data.drop(labels=['TARGET'], axis=1),\n    data['TARGET'],\n    test_size=0.3,\n    random_state=0)\n\nX_train.shape, X_test.shape","315b54f8":"# transpose the dataframe, so that the columns are the rows of the new dataframe\ndata_t = X_train.T\ndata_t.head()","3314292c":"# check if there are duplicated rows (the columns of the original dataframe)\n# this is a computionally expensive operation, so it might take a while\n# sum indicates how many rows are duplicated\n\ndata_t.duplicated().sum()","f2f153cb":"# visualise the duplicated rows (the columns of the original dataframe)\ndata_t[data_t.duplicated()]","5b2dee18":"# we can capture the duplicated features, by capturing the\n# index values of the transposed dataframe like this:\nduplicated_features = data_t[data_t.duplicated()].index.values\nduplicated_features","2f8a652a":"# alternatively, we can remove the duplicated rows,\n# transpose the dataframe back to the variables as columns\n# keep first indicates that we keep the first of a set of\n# duplicated variables\n\ndata_unique = data_t.drop_duplicates(keep='first').T\ndata_unique.shape","f08cc9ec":"# to find those columns in the original dataframe that were removed:\n\nduplicated_features = [col for col in data.columns if col not in data_unique.columns]\nduplicated_features ","5c13c63d":"import pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nfrom sklearn.model_selection import train_test_split","0201cec3":"# load dataset\ndata = pd.read_csv(r'..\/input\/bnp-paribas-cardif-claims-management\/train.csv.zip', nrows=50000)\ndata.shape","966cecd8":"data.head()","da407b53":"# In practice, feature selection should be done after data pre-processing,\n# so ideally, all the categorical variables are encoded into numbers,\n# and then you can assess whether they are correlated with other features\n\n# here for simplicity I will use only numerical variables\n# select numerical columns:\n\nnumerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\nnumerical_vars = list(data.select_dtypes(include=numerics).columns)\ndata = data[numerical_vars]\ndata.shape","e0248046":"# separate train and test sets\nX_train, X_test, y_train, y_test = train_test_split(\n    data.drop(labels=['target', 'ID'], axis=1),\n    data['target'],\n    test_size=0.3,\n    random_state=0)\n\nX_train.shape, X_test.shape","8dc2bf70":"# visualise correlated features\n# I will build the correlation matrix, which examines the \n# correlation of all features (for all possible feature combinations)\n# and then visualise the correlation matrix using seaborn\n\ncorrmat = X_train.corr()\nfig, ax = plt.subplots()\nfig.set_size_inches(11,11)\nsns.heatmap(corrmat)","af2d88fa":"# with the following function we can select highly correlated features\n# it will remove the first feature that is correlated with anything else\n# without any other insight.\n\ndef correlation(dataset, threshold):\n    col_corr = set()  # Set of all the names of correlated columns\n    corr_matrix = dataset.corr()\n    for i in range(len(corr_matrix.columns)):\n        for j in range(i):\n            if abs(corr_matrix.iloc[i, j]) > threshold: # we are interested in absolute coeff value\n                colname = corr_matrix.columns[i]  # getting the name of column\n                col_corr.add(colname)\n    return col_corr","69d979fb":"corr_features = correlation(X_train, 0.8)\nlen(set(corr_features))","571356c2":"X_train.drop(labels=corr_features, axis=1, inplace=True)\nX_test.drop(labels=corr_features, axis=1, inplace=True)\n\nX_train.shape, X_test.shape","33632488":"import pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\nfrom sklearn.metrics import roc_auc_score\n\nfrom mlxtend.feature_selection import SequentialFeatureSelector as SFS","3c6877e7":"# load dataset\ndata = pd.read_csv(r'..\/input\/bnp-paribas-cardif-claims-management\/train.csv.zip', nrows=50000)\ndata.shape","a5e3899f":"data.head()","72665154":"# In practice, feature selection should be done after data pre-processing,\n# so ideally, all the categorical variables are encoded into numbers,\n# and then you can assess how deterministic they are of the target\n\n# here for simplicity I will use only numerical variables\n# select numerical columns:\n\nnumerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\nnumerical_vars = list(data.select_dtypes(include=numerics).columns)\ndata = data[numerical_vars]\ndata.shape","7d75073c":"# separate train and test sets\nX_train, X_test, y_train, y_test = train_test_split(\n    data.drop(labels=['target', 'ID'], axis=1),\n    data['target'],\n    test_size=0.3,\n    random_state=0)\n\nX_train.shape, X_test.shape","97abc176":"# find and remove correlated features\n# in order to reduce the feature space a bit\n# so that the algorithm takes shorter\n\ndef correlation(dataset, threshold):\n    col_corr = set()  # Set of all the names of correlated columns\n    corr_matrix = dataset.corr()\n    for i in range(len(corr_matrix.columns)):\n        for j in range(i):\n            if abs(corr_matrix.iloc[i, j]) > threshold: # we are interested in absolute coeff value\n                colname = corr_matrix.columns[i]  # getting the name of column\n                col_corr.add(colname)\n    return col_corr\n\ncorr_features = correlation(X_train, 0.8)\nprint('correlated features: ', len(set(corr_features)) )","cc1a36d5":"# removed correlated  features\nX_train.drop(labels=corr_features, axis=1, inplace=True)\nX_test.drop(labels=corr_features, axis=1, inplace=True)\n\nX_train.shape, X_test.shape","2a33273b":"# step forward feature selection\n# I indicate that I want to select 10 features from\n# the total, and that I want to select those features\n# based on the optimal roc_auc\n\nsfs1 = SFS(RandomForestClassifier(n_jobs=4), \n           k_features=2, \n           forward=True, \n           floating=False, \n           verbose=2,\n           scoring='roc_auc',\n           cv=3)\n\nsfs1 = sfs1.fit(np.array(X_train.fillna(0)), y_train)","e566c006":"selected_feat= X_train.columns[list(sfs1.k_feature_idx_)] #it will return index of selected features\nselected_feat","4d74d1f0":"def run_randomForests(X_train, X_test, y_train, y_test):\n    rf = RandomForestClassifier(n_estimators=200, random_state=39, max_depth=4)\n    rf.fit(X_train, y_train)\n    print('Train set')\n    pred = rf.predict_proba(X_train)\n    print('Random Forests roc-auc: {}'.format(roc_auc_score(y_train, pred[:,1])))\n    print('Test set')\n    pred = rf.predict_proba(X_test)\n    print('Random Forests roc-auc: {}'.format(roc_auc_score(y_test, pred[:,1])))","353c0b85":"# evaluate performance of algorithm built\n# using selected features\n\nrun_randomForests(X_train[selected_feat].fillna(0),\n                  X_test[selected_feat].fillna(0),\n                  y_train, y_test)","dec4e6e6":"import pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\nfrom sklearn.metrics import roc_auc_score\n\nfrom mlxtend.feature_selection import ExhaustiveFeatureSelector as EFS","38e8fb00":"# load dataset\ndata = pd.read_csv('..\/input\/santander-customer-satisfaction\/train.csv', nrows=50000)\ndata.shape","60de5225":"data.head()","059e352b":"# In practice, feature selection should be done after data pre-processing,\n# so ideally, all the categorical variables are encoded into numbers,\n# and then you can assess how deterministic they are of the target\n\n# here for simplicity I will use only numerical variables\n# select numerical columns:\n\nnumerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\nnumerical_vars = list(data.select_dtypes(include=numerics).columns)\ndata = data[numerical_vars]\ndata.shape","99812495":"# separate train and test sets\nX_train, X_test, y_train, y_test = train_test_split(\n    data.drop(labels=['TARGET', 'ID'], axis=1),\n    data['TARGET'],\n    test_size=0.3,\n    random_state=0)\n\nX_train.shape, X_test.shape","749dc864":"# find and remove correlated features\n# in order to reduce the feature space a bit\n# so that the algorithm takes shorter\n\ndef correlation(dataset, threshold):\n    col_corr = set()  # Set of all the names of correlated columns\n    corr_matrix = dataset.corr()\n    for i in range(len(corr_matrix.columns)):\n        for j in range(i):\n            if abs(corr_matrix.iloc[i, j]) > threshold: # we are interested in absolute coeff value\n                colname = corr_matrix.columns[i]  # getting the name of column\n                col_corr.add(colname)\n    return col_corr\n\ncorr_features = correlation(X_train, 0.8)\nprint('correlated features: ', len(set(corr_features)) )","8b2bc9ec":"# removed correlated  features\nX_train.drop(labels=corr_features, axis=1, inplace=True)\nX_test.drop(labels=corr_features, axis=1, inplace=True)\n\nX_train.shape, X_test.shape","f73a64f3":"X_train.columns[0:10]","1ac0e14d":"import mlxtend\nmlxtend.__version__","282b3fe6":"# exhaustive feature selection\n# I indicate that I want to select 10 features(4 minimum and 4c2 combination) from\n# the total, and that I want to select those features\n# based on the optimal roc_auc\n\n# in order to shorter search time for the demonstration\n# i will ask the algorithm to try all possible 1,2,3 and 4\n# feature combinations from a dataset of 4 features\n\n# if you have access to a multicore or distributed computer\n# system you can try more greedy searches\n\nefs1 = EFS(RandomForestClassifier(n_estimators=10,n_jobs=4, random_state=0), \n           min_features=1,\n           max_features=4, \n           scoring='roc_auc',\n           print_progress=True,\n           cv=2)\n\nefs1 = efs1.fit(np.array(X_train[X_train.columns[0:4]].fillna(0)), y_train)","35f518ce":"X_train.columns","08164bb1":"def run_randomForests(X_train, X_test, y_train, y_test):\n    rf = RandomForestClassifier(n_estimators=200, random_state=39, max_depth=4)\n    rf.fit(X_train, y_train)\n    print('Train set')\n    pred = rf.predict_proba(X_train)\n    print('Random Forests roc-auc: {}'.format(roc_auc_score(y_train, pred[:,1])))\n    print('Test set')\n    pred = rf.predict_proba(X_test)\n    print('Random Forests roc-auc: {}'.format(roc_auc_score(y_test, pred[:,1])))","e487103a":"efs1.best_idx_","bc3a0379":"selected_feat= X_train.columns[list(efs1.best_idx_)]\nselected_feat","568762b0":"# evaluate performance of classifier using selected features\n\nrun_randomForests(X_train[selected_feat].fillna(0),\n                  X_test[selected_feat].fillna(0),\n                  y_train, y_test)","839fe503":"list(efs1.best_idx_)","2e3671ee":"import pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.feature_selection import RFE\nfrom sklearn.metrics import roc_auc_score","8c5468cf":"# load dataset\ndata = pd.read_csv('..\/input\/santander-customer-satisfaction\/train.csv', nrows=50000)\ndata.shape","0eb2e29c":"data.head()","7ad7a9c5":"# In practice, feature selection should be done after data pre-processing,\n# so ideally, all the categorical variables are encoded into numbers,\n# and then you can assess how deterministic they are of the target\n\n# here for simplicity I will use only numerical variables\n# select numerical columns:\n\nnumerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\nnumerical_vars = list(data.select_dtypes(include=numerics).columns)\ndata = data[numerical_vars]\ndata.shape","727b37a7":"# separate train and test sets\nX_train, X_test, y_train, y_test = train_test_split(\n    data.drop(labels=['TARGET', 'ID'], axis=1),\n    data['TARGET'],\n    test_size=0.3,\n    random_state=0)\n\nX_train.shape, X_test.shape","be778495":"# here I will do the model fitting and feature selection\n# altogether in one line of code\n\n# first I specify the Random Forest instance, indicating\n# the number of trees\n\n# Then I use the selectFromModel object from sklearn\n# to automatically select the features\n\n# RFE will remove one feature at each iteration, the\n# least  important.\n# then it will build another random forest and repeat\n# till a criteria is met.\n\n# in sklearn the criteria to stop is an arbitrary number\n# of features to select, that you need to decide before hand\n# not the best solution, but a solution\n\nsel_ = RFE(RandomForestClassifier(n_estimators=100), n_features_to_select=2)\nsel_.fit(X_train.fillna(0), y_train)","a2fa7239":"# this command let's me visualise those features that were selected.\nsel_.get_support()","76c81076":"# let's add the variable names and order it for clearer visualisation\nselected_feat = X_train.columns[(sel_.get_support())]\nlen(selected_feat)","1552f4f2":"# let's display the list of features\nselected_feat","fc5c7a12":"# create a function to build random forests and compare performance in train and test set\n\ndef run_randomForests(X_train, X_test, y_train, y_test):\n    rf = RandomForestClassifier(n_estimators=200, random_state=39, max_depth=4)\n    rf.fit(X_train, y_train)\n    print('Train set')\n    pred = rf.predict_proba(X_train)\n    print('Random Forests roc-auc: {}'.format(roc_auc_score(y_train, pred[:,1])))\n    print('Test set')\n    pred = rf.predict_proba(X_test)\n    print('Random Forests roc-auc: {}'.format(roc_auc_score(y_test, pred[:,1])))","ce414ee6":"# features selected recursively\nrun_randomForests(X_train[selected_feat].fillna(0),\n                  X_test[selected_feat].fillna(0),\n                  y_train, y_test)","cbb7ff9b":"### The ENdddddd !!","4bd9380e":"### Important\n\nIn all feature selection procedures, it is good practice to select the features by examining only the training set. And this is to avoid overfit.","16820d1a":"### Remove constant features\n\nFirst, I will remove constant features like I did in the previous lecture. This will allow a better visualisation of the quasi-constant ones.","e73518bf":"## Removing quasi-constant features\n### Using variance threshold from sklearn\n\nVariance threshold from sklearn is a simple baseline approach to feature selection. It removes all features which variance doesn\u2019t meet some threshold. By default, it removes all zero-variance features, i.e., features that have the same value in all samples.\n\nHere, I will change the default threshold to remove almost \/ quasi-constant features.","c5c18329":"## 4. Correlation\n\nCorrelation Feature Selection evaluates subsets of features on the basis of the following hypothesis: \"Good feature subsets contain features highly correlated with the target, yet uncorrelated to each other\".\n\n**References**:\n\nM. Hall 1999, [Correlation-based Feature Selection for Machine Learning](http:\/\/www.cs.waikato.ac.nz\/~mhall\/thesis.pdf)\n\nSenliol, Baris, et al. \"Fast Correlation Based Filter (FCBF) with a different search strategy.\" Computer and Information Sciences.\n\n\n\nI will demonstrate how to select features based on correlation using 2 procedures. The first one is a brute force function that finds correlated features without any further insight. The second procedure finds groups of correlated features. Often, more than 2 features are correlated with each other. We can find groups of 3, 4 or more features that are correlated. By identifying these groups, we can then select from each group, which feature we want to keep, and which ones we want to remove.\n\nI will use the Paribas claims dataset from Kaggle.","9727292b":"## 5. Step forward feature selection\n\nSequential feature selection algorithms are a family of greedy search algorithms that are used to reduce an initial d-dimensional feature space to a k-dimensional feature subspace where k < d.\n\nStep forward feature selection starts by evaluating all features individually and selects the one that generates the best performing algorithm, according to a pre-set evaluation criteria. In the second step, it evaluates all possible combinations of the selected feature and a second feature, and selects the pair that produce the best performing algorithm based on the same pre-set criteria.\n\nThe pre-set criteria can be the roc_auc for classification and the r squared for regression for example. \n\nThis selection procedure is called greedy, because it evaluates all possible single, double, triple and so on feature combinations. Therefore, it is quite computationally expensive, and sometimes, if feature space is big, even unfeasible.\n\nThere is a special package for python that implements this type of feature selection: mlxtend.\n\nIn the mlxtend implementation of the step forward feature selection, the stopping criteria is an arbitrarily set number of features. So the search will finish when we reach the desired number of selected features. \n\nThis is somewhat arbitrary because we may be selecting a subopimal number of features, or likewise, a high number of features. \n\nHere I will use the Step Forward feature selection algorithm from mlxtend in a classification (Paribas) and regression (House Price) dataset.","4fce5489":"## Removing duplicate features","7f393f0e":"### I have spent my dear WEEKEND on this !!! \ud83d\ude21 \ud83d\ude21 \ud83d\ude21 : Give me my up-votes (simp)\n\n\n<img src=\"https:\/\/i.pinimg.com\/originals\/fe\/64\/7f\/fe647fc23dec9d0ceed25b94a59d8ae8.jpg\" width=\"400\" height=\"100\" center =True>","7c7bf9df":"### Important\n\nIn all feature selection procedures, it is good practice to select the features by examining only the training set. And this is to avoid overfit.","17f58adc":"### Important\n\nIn all feature selection procedures, it is good practice to select the features by examining only the training set. And this is to avoid overfit.","53cd46a2":"We then use the transform function to reduce the training and testing sets. See below.","186f318b":"## 7.Recursive feature selection using random forests importance\n\nRandom Forests assign equal or similar importance to features that are highly correlated. In addition, when features are correlated, the importance assigned is lower than the importance attributed to the feature itself, should the tree be built without the correlated counterparts.\n\nTherefore, instead of eliminating features based  on importance by brute force , we may get a better selection by removing one feature at a time, and recalculating the importance on each round.\n\nThis method is an hybrid between embedded and wrapper methods: it is based on computation derived when fitting the model, but it also requires fitting several models.\n\nThe cycle is as follows:\n\n- Build random forests using all features\n- Remove least important feature\n- Build random forests and recalculate importance\n- Repeat until a criteria is met\n\nIn this situation, when a feature that is highly correlated to another one is removed, then, the importance of the remaining feature increases. This may lead to a better subset feature space selection. On the downside, building several random forests is quite time consuming, in particular if the dataset contains a high number of features.\n\nI will demonstrate how to select features based random forests importance recursively using sklearn on classification problem. I will use the Paribas claims dataset from Kaggle.","cdab261a":"### Important\n\nIn all feature selection procedures, it is good practice to select the features by examining only the training set. And this is to avoid overfit.","fcca51cd":"### Important\n\nIn all feature selection procedures, it is good practice to select the features by examining only the training set. And this is to avoid overfitting.","0ce13d24":"We can see that 105 columns \/ variables are duplicated. This means that 105 variables are identical to at least another variable within a dataset.","bfb03fa6":"### Important\n\nIn all feature selection procedures, it is good practice to select the features by examining only the training set. And this is to avoid overfit.","034d2b56":"## Hello Everyone,\n\n### *This notebook acts as a tutorial which summarizes the main techniques of feature selection for solving major problems with their short descriptions and implementation.*\n\n### **if you like my work and presentaion, please give a Up-vote to the work book as it will motiate me to share more of my learnings !!**\n\n### **I have Published a Tutorial on Feature Engg too do have look** : [link](https:\/\/www.kaggle.com\/adarshsng\/extensive-advance-feature-engineering-tutorial\/notebook)\n","49bd6ece":"By removing constant and almost constant features, we reduced the feature space from 370 to 261. More than 100 features were removed from the present dataset.","4aec0ba5":"## Removing constant features","bcd3c813":"We can see that 58 columns \/ variables are constant. This means that 58 variables show the same value, just one value, for all the observations of the training set.","83bfcd25":"## 6. Exhaustive feature selection\n\nSequential feature selection algorithms are a family of greedy search algorithms that are used to reduce an initial d-dimensional feature space to a k-dimensional feature subspace where k < d.\n\nIn an exhaustive feature selection the best subset of features is selected, over all possible feature subsets, by optimizing a specified performance metric for a certain machine learning algorithm. For example, if the classifier is a logistic regression and the dataset consists of 4 features, the algorithm will evaluate all **15** feature combinations as follows:\n\n- all possible combinations of 1 feature\n- all possible combinations of 2 features\n- all possible combinations of 3 features\n- all the 4 features\n\nand select the one that results in the best performance (e.g., classification accuracy) of the logistic regression classifier.\n\nThis is another greedy algorithm as it evaluates all possible feature combinations. It is quite computationally expensive, and sometimes, if feature space is big, even unfeasible.\n\nThere is a special package for python that implements this type of feature selection: mlxtend.\n\nIn the mlxtend implementation of the exhaustive feature selection, the stopping criteria is an arbitrarily set number of features. So the search will finish when we reach the desired number of selected features. \n\nThis is somewhat arbitrary because we may be selecting a subopimal number of features, or likewise, a high number of features. \n\nHere I will use the Exhaustive feature selection algorithm from mlxtend in a classification (Paribas).","4a542414":"## 2. Quasi-constant features\n\nQuasi-constant features are those that show the same value for the great majority of the observations of the dataset. In general, these features provide little if any information that allows a machine learning model to discriminate or predict a target. But there can be exceptions. So you should be careful when removing these type of features.\n\nIdentifying and removing quasi-constant features, is an easy first step towards feature selection and more easily interpretable machine learning models.\n\nHere I will demonstrate how to identify quasi-constant features using the Santander Customer Satisfaction dataset from Kaggle. \n\nTo identify constant features, we can use the VarianceThreshold function from sklearn, or we can code it ourselves. I will show 2 snippets of code with both procedures.","fd78366f":"We can see that 55 features are highly correlated with other features in the training set. Very likely, by removing these correlated features, the performance of your machine learning models will drop very little, if at all. We can go ahead and drop the features like we have done in previous lectures.","bfc19913":"We can see immediately how removing duplicated features helps reduce the feature space. We passed from 370 to 265  non-duplicated features.","ddff6dd0":"### Hey there, if you have scrolled this far: do care to Up-vote <3","1ccd9671":"In the plot above, the red squares correspond to highly correlated features (>0.8). We can see that there are quite a few. The diagonal represents the correlation of a feature with itself, therefore the value is 1.\n\n### Brute force approach","ef67237a":"### Using variance threshold from sklearn\n\nVariance threshold from sklearn is a simple baseline approach to feature selection. It removes all features which variance doesn\u2019t meet some threshold. By default, it removes all zero-variance features, i.e., features that have the same value in all samples.","67f7f87a":"Pandas has the function 'duplicated' that evaluates if the dataframe contains duplicated rows. We can use this function to check for duplicated columns if we transpose the dataframe first. By transposing the dataframe, we obtain a new dataframe where the columns are now rows, and with the 'duplicated' method we can go ahead an identify those that are duplicated. \n\nOnce we identify them, we can remove the duplicated rows. See below.\n\n### Code Snippet for small datasets\n\nUsing pandas transpose is computationally expensive, so the computer may run out of memory. That is why we can only use this code block on small datasets. How small will depend of your computer specifications.","ae789d34":"## 1. Constant features\n\nConstant features are those that show the same value, just one value, for all the observations of the dataset. This is, the same value for all the rows of the dataset. These features provide no information that allows a machine learning model to discriminate or predict a target.\n\nIdentifying and removing constant features, is an easy first step towards feature selection and more easily interpretable machine learning models.\n\nHere I will demonstrate how to identify constant features using the Santander Customer Satisfaction dataset from Kaggle. \n\nTo identify constant features, we can use the VarianceThreshold function from sklearn, or we can code it ourselves. I will show 2 snippets of code with both procedures.","1e16e25b":"## 3. Duplicated features\n\nOften datasets contain one or more features that show the same values across all the observations. This means that both features are in essence identical. In addition, it is not unusual to introduce duplicated features after performing **one hot encoding** of categorical variables, particularly when using several highly cardinal variables.\n\nIdentifying and removing duplicated, and therefore redundant features, is an easy first step towards feature selection and more easily interpretable machine learning models.\n\nHere I will demonstrate how to identify duplicated features using the Santander Customer Satisfaction dataset from Kaggle. \n\nThere is no function in python and pandas to find duplicated columns. The code, is computationally costly, so your computer might run out of memory.\n\n**Note**\nFinding duplicated features is a computationally costly operation in Python, therefore depending on the size of your dataset, you might not always be able to perform it.","07f35af0":"<img src=\"https:\/\/ih1.redbubble.net\/image.1600796459.0360\/st,small,507x507-pad,600x600,f8f8f8.jpg\" width=\"600\" height=\"400\" center =True>","5a4da394":"### Important\n\nIn all feature selection procedures, it is good practice to select the features by examining only the training set. And this is to avoid overfit.","cd7f5d3c":"We can see that 50 columns \/ variables are almost constant. This means that 50 variables show predominantly one value for ~99% the observations of the training set. Let's see below.","553dbdf6":"We can see that > 99% of the observations show one value, 0. Therefore, this features is almost constant."}}