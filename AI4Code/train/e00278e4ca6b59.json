{"cell_type":{"f165d8f1":"code","affa68b2":"code","d6e60eb0":"code","ba2f39c9":"code","c17832fd":"code","e00adbc2":"code","4a3b1c31":"code","e9272f74":"code","241c606e":"code","29dba337":"code","bb680583":"code","27e2c961":"code","2fc87560":"code","3030ae5c":"code","768bf80a":"code","0774b0a1":"code","70936176":"code","44ecba3d":"code","50cdc7da":"code","58e64484":"code","7e76ef3f":"code","fe936d8b":"code","cc8a37cc":"code","75ba5f05":"code","9fac80d0":"code","0cace939":"code","cb7deca6":"code","4e6fe874":"code","3de66179":"code","dff0923e":"code","030da40b":"code","a1c866d6":"code","347045d2":"code","99f6153a":"code","5c3ebc2f":"code","231ed116":"code","8ba65292":"code","7348fd1b":"code","8bd5ad83":"code","31199321":"code","37bcf8fd":"code","717a4a42":"code","72065e1d":"code","7461d95c":"code","042a706f":"code","7507e431":"code","0009bf27":"code","01b59204":"code","da7bf40e":"code","3ee6f596":"code","0d854fed":"code","36504f84":"code","92950b35":"code","0a255ba0":"code","93d83764":"code","8e452b06":"code","734b034f":"code","87f9e2c1":"code","8b22eba3":"code","32e109c8":"code","963fd8da":"code","14c05685":"code","c1899aa2":"code","71425e83":"code","32cf6596":"code","d509183a":"code","8d84211d":"code","d51063fc":"code","36dcef12":"code","06de0c3d":"code","09cc7123":"markdown","17a336e3":"markdown","e4f42180":"markdown","e0ce8bb4":"markdown","25b75a89":"markdown","50713a7f":"markdown","1c3f4ce0":"markdown","da1571c4":"markdown","1634c932":"markdown","5d248311":"markdown","9fa03a4d":"markdown","f8b102f7":"markdown","8a48356b":"markdown","01b60cb5":"markdown"},"source":{"f165d8f1":"import os\nimport cv2\nimport keras\nimport pandas             as pd\nimport matplotlib.pyplot  as plt\nimport numpy              as np\nimport tensorflow         as tf\nimport pickle\n\nfrom zipfile import ZipFile\n%tensorflow_version 2.x\nimport tensorflow\ntensorflow.__version__","affa68b2":"from google.colab import drive\ndrive.mount('\/content\/drive')","d6e60eb0":"import os\nprint(os.listdir(\"..\/content\"))","ba2f39c9":"data = np.load('\/content\/drive\/MyDrive\/Part 1- Train data - images.npy', allow_pickle = True)\ndata.shape","c17832fd":"test_image = cv2.imread('\/content\/drive\/MyDrive\/Part 1Test Data - Prediction Image.jpeg')","e00adbc2":"test_image.shape","4a3b1c31":"images = data[:,0]\nmask = data[:,1]\nprint(images.shape, mask.shape)","e9272f74":"plt.imshow(images[300])","241c606e":"plt.imshow(images[408])","29dba337":"print(images[408].shape)","bb680583":"mask[408] ","27e2c961":"display(mask[408])","2fc87560":"IMAGE_WIDTH = 224\nIMAGE_HEIGHT = 224\nALPHA = 1","3030ae5c":"import cv2\nfrom tensorflow.keras.applications.mobilenet import preprocess_input\n\nmasks = np.zeros((int(data.shape[0]), IMAGE_HEIGHT, IMAGE_WIDTH))\nX_train = np.zeros((int(data.shape[0]), IMAGE_HEIGHT, IMAGE_WIDTH, 3))\nfor index in range(data.shape[0]):\n    img = data[index][0]\n    img = cv2.resize(img, dsize=(IMAGE_HEIGHT, IMAGE_WIDTH), interpolation=cv2.INTER_CUBIC)\n    try:\n      img = img[:, :, :3]\n    except:\n      continue\n    X_train[index] = preprocess_input(np.array(img, dtype=np.float32))\n    for i in data[index][1]:\n        x1 = int(i[\"points\"][0]['x'] * IMAGE_WIDTH)\n        x2 = int(i[\"points\"][1]['x'] * IMAGE_WIDTH)\n        y1 = int(i[\"points\"][0]['y'] * IMAGE_HEIGHT)\n        y2 = int(i[\"points\"][1]['y'] * IMAGE_HEIGHT)\n        masks[index][y1:y2, x1:x2] = 1","768bf80a":"X_train.shape","0774b0a1":"masks.shape","70936176":"from matplotlib import pyplot\nn = 10\nprint(X_train[n])\npyplot.imshow(X_train[n])","44ecba3d":"pyplot.imshow(masks[n])","50cdc7da":"from tensorflow.keras.applications.mobilenet import MobileNet\nfrom tensorflow.keras.layers import Concatenate, UpSampling2D, Conv2D, Reshape\nfrom tensorflow.keras.models import Model\nimport tensorflow as tf\n\nHEIGHT_CELLS = 28\nWIDTH_CELLS = 28\nBATCH_SIZE = 1","58e64484":"model = MobileNet(input_shape=(IMAGE_HEIGHT, IMAGE_WIDTH, 3), include_top=False, alpha=ALPHA, weights=\"imagenet\")\nmodel.summary()\n","7e76ef3f":"def create_model(trainable=False):\n    model = MobileNet(input_shape=(IMAGE_HEIGHT, IMAGE_WIDTH, 3), include_top=False, alpha=ALPHA, weights=\"imagenet\")\n\n    for layer in model.layers:\n        layer.trainable = trainable\n        \n    block00 = model.layers[0].input\n    block01 = model.get_layer(\"conv_pw_1_relu\").output\n    block02 = model.get_layer(\"conv_pw_2_relu\").output\n    block03 = model.get_layer(\"conv_pw_3_relu\").output\n    block05 = model.get_layer(\"conv_pw_5_relu\").output\n    block11 = model.get_layer(\"conv_pw_11_relu\").output\n    block13 = model.get_layer(\"conv_pw_13_relu\").output\n\n    decoderBlock = Concatenate()([UpSampling2D()(block13), block11])\n    decoderBlock = Concatenate()([UpSampling2D()(decoderBlock), block05])\n    decoderBlock = Concatenate()([UpSampling2D()(decoderBlock), block03])\n    decoderBlock = Concatenate()([UpSampling2D()(decoderBlock), block01])\n    decoderBlock = Concatenate()([UpSampling2D()(decoderBlock), block00])\n\n    decoderBlock = Conv2D(1, kernel_size=1, activation=\"sigmoid\")(decoderBlock)\n    decoderBlock = Reshape((IMAGE_HEIGHT, IMAGE_WIDTH))(decoderBlock)\n\n    return Model(inputs=model.input, outputs=decoderBlock) \n","fe936d8b":"model = create_model()\n\n# Print summary\nmodel.summary()","cc8a37cc":"def dice_coefficient(y_true, y_pred):\n    numerator = 2 * tf.reduce_sum(y_true * y_pred)\n    denominator = tf.reduce_sum(y_true + y_pred)\n\n    return numerator \/ (denominator + tf.keras.backend.epsilon())","75ba5f05":"from tensorflow.keras.losses import binary_crossentropy\nfrom tensorflow.keras.backend import log, epsilon\ndef loss(y_true, y_pred):\n    return binary_crossentropy(y_true, y_pred) - log(dice_coefficient(y_true, y_pred) + epsilon())","9fac80d0":"model = create_model(False)\nmodel.summary()\noptimizer = tf.keras.optimizers.Adam(lr=1e-4, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\nmodel.compile(loss=loss, optimizer = optimizer, metrics=[dice_coefficient])\n\n","0cace939":"\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\ncheckpoint = ModelCheckpoint(\"model-{loss:.2f}.h5\", monitor=\"loss\", verbose=1, save_best_only=True,\n                             save_weights_only=True, mode=\"min\", save_freq=1)\nstop = EarlyStopping(monitor=\"loss\", patience=5, mode=\"min\")\nreduce_lr = ReduceLROnPlateau(monitor=\"loss\", factor=0.2, patience=1, min_lr=1e-6, verbose=1, mode=\"min\")\n\n","cb7deca6":"model.fit(X_train,masks,epochs = 5,verbose=1,batch_size=3,callbacks=[checkpoint,reduce_lr,stop])","4e6fe874":"plt.imshow(test_image)","3de66179":"sample_image = test_image\nimage = cv2.resize(sample_image, dsize=(IMAGE_HEIGHT, IMAGE_WIDTH), interpolation=cv2.INTER_CUBIC)\nfeat_scaled = preprocess_input(np.array(image, dtype=np.float32))\n\npred_mask = cv2.resize(1.0*(model.predict(x=np.array([feat_scaled]))[0] > 0.2), (IMAGE_WIDTH,IMAGE_HEIGHT))\n\nimage2 = image\nimage2[:,:,0] = pred_mask*image[:,:,0]\nimage2[:,:,1] = pred_mask*image[:,:,1]\nimage2[:,:,2] = pred_mask*image[:,:,2]\n\nout_image = image2\n\nplt.imshow(out_image)","dff0923e":"import matplotlib.pyplot as plt\nplt.imshow(pred_mask, alpha=1)","030da40b":"project_path = '\/content\/'","a1c866d6":"\nimages_zip_path = '\/content\/drive\/MyDrive\/Part 3 - Aligned Face Dataset from Pinterest.zip'\nfrom zipfile import ZipFile\n\nwith ZipFile(images_zip_path, 'r') as z:\n  z.extractall()\n","347045d2":"import numpy as np\nimport os\n\nclass IdentityMetadata():\n    def __init__(self, base, name, file):\n        # print(base, name, file)\n        # dataset base directory\n        self.base = base\n        # identity name\n        self.name = name\n        # image file name\n        self.file = file\n\n    def __repr__(self):\n        return self.image_path()\n\n    def image_path(self):\n        return os.path.join(self.base, self.name, self.file) \n    \ndef load_metadata(path):\n    metadata = []\n    for i in os.listdir(path):\n        for f in os.listdir(os.path.join(path, i)):\n            # Check file extension. Allow only jpg\/jpeg' files.\n            ext = os.path.splitext(f)[1]\n            if ext == '.jpg' or ext == '.jpeg':\n                metadata.append(IdentityMetadata(path, i, f))\n    return np.array(metadata)\n\n# metadata = load_metadata('images')\nmetadata = load_metadata('PINS')","99f6153a":"import cv2\ndef load_image(path):\n    img = cv2.imread(path, 1)\n    # OpenCV loads images with color channels\n    # in BGR order. So we need to reverse them\n    return img[...,::-1]","5c3ebc2f":"load_image('\/content\/PINS\/pins_Aaron Paul\/Aaron Paul101_247.jpg')","231ed116":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import ZeroPadding2D, Convolution2D, MaxPooling2D, Dropout, Flatten, Activation\n\ndef vgg_face():\t\n    model = Sequential()\n    model.add(ZeroPadding2D((1,1),input_shape=(224,224, 3)))\n    model.add(Convolution2D(64, (3, 3), activation='relu'))\n    model.add(ZeroPadding2D((1,1)))\n    model.add(Convolution2D(64, (3, 3), activation='relu'))\n    model.add(MaxPooling2D((2,2), strides=(2,2)))\n    \n    model.add(ZeroPadding2D((1,1)))\n    model.add(Convolution2D(128, (3, 3), activation='relu'))\n    model.add(ZeroPadding2D((1,1)))\n    model.add(Convolution2D(128, (3, 3), activation='relu'))\n    model.add(MaxPooling2D((2,2), strides=(2,2)))\n    \n    model.add(ZeroPadding2D((1,1)))\n    model.add(Convolution2D(256, (3, 3), activation='relu'))\n    model.add(ZeroPadding2D((1,1)))\n    model.add(Convolution2D(256, (3, 3), activation='relu'))\n    model.add(ZeroPadding2D((1,1)))\n    model.add(Convolution2D(256, (3, 3), activation='relu'))\n    model.add(MaxPooling2D((2,2), strides=(2,2)))\n    \n    model.add(ZeroPadding2D((1,1)))\n    model.add(Convolution2D(512, (3, 3), activation='relu'))\n    model.add(ZeroPadding2D((1,1)))\n    model.add(Convolution2D(512, (3, 3), activation='relu'))\n    model.add(ZeroPadding2D((1,1)))\n    model.add(Convolution2D(512, (3, 3), activation='relu'))\n    model.add(MaxPooling2D((2,2), strides=(2,2)))\n    \n    model.add(ZeroPadding2D((1,1)))\n    model.add(Convolution2D(512, (3, 3), activation='relu'))\n    model.add(ZeroPadding2D((1,1)))\n    model.add(Convolution2D(512, (3, 3), activation='relu'))\n    model.add(ZeroPadding2D((1,1)))\n    model.add(Convolution2D(512, (3, 3), activation='relu'))\n    model.add(MaxPooling2D((2,2), strides=(2,2)))\n    \n    model.add(Convolution2D(4096, (7, 7), activation='relu'))\n    model.add(Dropout(0.5))\n    model.add(Convolution2D(4096, (1, 1), activation='relu'))\n    model.add(Dropout(0.5))\n    model.add(Convolution2D(2622, (1, 1)))\n    model.add(Flatten())\n    model.add(Activation('softmax'))\n    return model","8ba65292":"\nmodel = vgg_face()\nmodel.load_weights('\/content\/drive\/MyDrive\/Part 3 - vgg_face_weights.h5')","7348fd1b":"model.layers[0], model.layers[-2]","8bd5ad83":"from tensorflow.keras.models import Model\nvgg_face_descriptor = Model(inputs=model.layers[0].input, outputs=model.layers[-2].output)","31199321":"type(vgg_face_descriptor)","37bcf8fd":"vgg_face_descriptor.inputs, vgg_face_descriptor.outputs","717a4a42":"# Get embedding vector for first image in the metadata using the pre-trained model\nimg_path = metadata[0].image_path()\nimg = load_image(img_path)\n\n# Normalising pixel values from [0-255] to [0-1]: scale RGB values to interval [0,1]\nimg = (img \/ 255.).astype(np.float32)\nimg = cv2.resize(img, dsize = (224,224))\nprint(img.shape)\n\n# Obtain embedding vector for an image\n# Get the embedding vector for the above image using vgg_face_descriptor model and print the shape \nembedding_vector = vgg_face_descriptor.predict(np.expand_dims(img, axis=0))[0]\nprint(embedding_vector.shape)","72065e1d":"embedding_vector[0], type(embedding_vector), type(embedding_vector[0])","7461d95c":"embedding_vector[2], embedding_vector[98], embedding_vector[-2]","042a706f":"total_images = len(metadata)","7507e431":"embeddings = np.zeros((metadata.shape[0], 2622))\nfor i, m in enumerate(metadata):\n    img_path = metadata[i].image_path()\n    img = load_image(img_path)\n    img = (img \/ 255.).astype(np.float32)\n    img = cv2.resize(img, dsize = (224,224))\n    embedding_vector = vgg_face_descriptor.predict(np.expand_dims(img, axis=0))[0]\n    embeddings[i]=embedding_vector","0009bf27":"print('embeddings shape :', embeddings.shape)","01b59204":"embeddings[0], embeddings[988], embeddings[988].shape","da7bf40e":"embeddings[8275]","3ee6f596":"def distance(emb1, emb2):\n    return np.sum(np.square(emb1 - emb2))","0d854fed":"def show_pair(idx1, idx2):\n    plt.figure(figsize=(8,3))\n    plt.suptitle(f'Distance between {idx1} & {idx2}= {distance(embeddings[idx1], embeddings[idx2]):.2f}')\n    plt.subplot(121)\n    plt.imshow(load_image(metadata[idx1].image_path()))\n    plt.subplot(122)\n    plt.imshow(load_image(metadata[idx2].image_path()));    \n\nshow_pair(2, 3)\nshow_pair(30, 31)","36504f84":"show_pair(30, 100)\nshow_pair(70, 72)\nshow_pair(70, 115)","92950b35":"train_idx = np.arange(metadata.shape[0]) % 9 != 0     \ntest_idx = np.arange(metadata.shape[0]) % 9 == 0\n\n# one half as train examples of 10 identities\nX_train = embeddings[train_idx]\n\n# another half as test examples of 10 identities\nX_test = embeddings[test_idx]\ntargets = np.array([m.name for m in metadata])\n\n#train labels\ny_train = targets[train_idx]\n\n#test labels\ny_test = targets[test_idx]\n\nprint('X_train shape : ({0},{1})'.format(X_train.shape[0], X_train.shape[1]))\nprint('y_train shape : ({0},)'.format(y_train.shape[0]))\nprint('X_test shape : ({0},{1})'.format(X_test.shape[0], X_test.shape[1]))\nprint('y_test shape : ({0},)'.format(y_test.shape[0]))","0a255ba0":"y_test[0], y_train[72]","93d83764":"len(np.unique(y_test)), len(np.unique(y_train))","8e452b06":"from sklearn.preprocessing import LabelEncoder\n\nle = LabelEncoder()\ny_train_encoded = le.fit_transform(y_train)","734b034f":"print(le.classes_)\ny_test_encoded = le.transform(y_test)","87f9e2c1":"print('y_train_encoded : ', y_train_encoded)\nprint('y_test_encoded : ', y_test_encoded)","8b22eba3":"# Standarize features\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\nX_train_std = scaler.fit_transform(X_train)","32e109c8":"X_test_std = scaler.transform(X_test)","963fd8da":"print('X_train_std shape : ({0},{1})'.format(X_train_std.shape[0], X_train_std.shape[1]))\nprint('y_train_encoded shape : ({0},)'.format(y_train_encoded.shape[0]))\nprint('X_test_std shape : ({0},{1})'.format(X_test_std.shape[0], X_test_std.shape[1]))\nprint('y_test_encoded shape : ({0},)'.format(y_test_encoded.shape[0]))","14c05685":"from sklearn.decomposition import PCA\n\npca = PCA(n_components=128)\nX_train_pca = pca.fit_transform(X_train_std)\nX_test_pca = pca.transform(X_test_std)","c1899aa2":"from sklearn.svm import SVC\n\nclf = SVC(C=5., gamma=0.001)\nclf.fit(X_train_pca, y_train_encoded)","71425e83":"y_predict = clf.predict(X_test_pca)","32cf6596":"print('y_predict : ',y_predict)\nprint('y_test_encoded : ',y_test_encoded)","d509183a":"y_predict_encoded = le.inverse_transform(y_predict)","8d84211d":"print('y_predict shape : ', y_predict.shape)\nprint('y_test_encoded shape : ', y_test_encoded.shape)","d51063fc":"y_test_encoded[32:49]","36dcef12":"from sklearn.metrics import precision_recall_curve,accuracy_score,f1_score,precision_score,recall_score\naccuracy_score(y_test_encoded, y_predict)","06de0c3d":"\n\nexample_idx = 10\n\nexample_image = load_image(metadata[test_idx][example_idx].image_path())\nexample_prediction = y_predict[example_idx]\nexample_identity =  y_predict_encoded[example_idx]\n\nplt.imshow(example_image)\nplt.title(f'Identified as {example_identity}');","09cc7123":"====================================================","17a336e3":"#Loading the image","e4f42180":"### UNET IS READY, hence Defining Dice Coefficient","e0ce8bb4":"#checking the test image ","25b75a89":"## <font color = Red> <b> Part One","50713a7f":"\nPlot images and get distance between the pairs given below\n\n    2, 3 and 2, 180\n    30, 31 and 30, 100\n    70, 72 and 70, 115\n\n","1c3f4ce0":"#### <font color = teal> Import the dataset.","da1571c4":"\n- <font color = teal> DOMAIN: Entertainment\n- CONTEXT: Company X owns a movie application and repository which caters movie streaming to millions of users who on subscription basis. Company  wants  to  automate  the  process  of  cast  and  crew  information  in  each  scene  from  a  movie  such  that  when  a  user  pauses  on  the movie and clicks on cast information button, the app will show details of the actor in the scene. Company has an in-house computer vision and multimedia experts who need to detect faces from screen shots from the movie scene.\n- DATA DESCRIPTION: The dataset comprises of images and its mask where there is a human face\n- PROJECT OBJECTIVE: Face detection from training images","1634c932":"#### Model designing ","5d248311":"### Plotting few sample images to check the dataset","9fa03a4d":"# <Font Color = Red> <b> Part 2 ","f8b102f7":"- Squared L2 distance was used to calculate the distance between 2 pairs of images.\n- Encoding the target variables, standardizing the features and reducing dimensions using PCA.\n- SVM classifier to predict the celebrity with 96.455% accuracy.","8a48356b":"### Fitting the model for Only 5 Epochs due to system constraints, which should still give is decent test result. ","01b60cb5":"### Even with 5 Epochs we were able to get a decent Dice Coff which is above 50% which is still good to test on the test image. "}}