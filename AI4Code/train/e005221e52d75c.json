{"cell_type":{"8ce62122":"code","ddfd0bee":"code","8f1f7a7c":"code","7c5d15f4":"code","21ff2f48":"code","f74f3d79":"code","5d8500ed":"code","1bc53dcd":"code","42ae54db":"code","e212612e":"code","20de6248":"code","00128eb5":"code","b6444364":"code","b5d308d3":"code","3bbde027":"code","428b0c54":"code","a8e569bc":"code","719618ae":"code","a1db4578":"code","e6260520":"code","b806bc6e":"code","bd2b546f":"code","2cf370a4":"code","6292e7a1":"code","d800ee69":"code","f428f32b":"code","5c13185c":"code","5087e241":"code","7048f204":"markdown","9166bb74":"markdown","c4f8bd0c":"markdown","615927ae":"markdown","19161612":"markdown","82a55963":"markdown","75297ff2":"markdown","2b335611":"markdown","6c1cfd6a":"markdown","fcd599a8":"markdown","3f933025":"markdown","f79aa982":"markdown","63ca9058":"markdown","f7e37043":"markdown","d103a0cf":"markdown","2a4bd4e7":"markdown","775f948d":"markdown","43caf710":"markdown","13f5be4f":"markdown","0014c539":"markdown","2204de32":"markdown","3067c1d1":"markdown","b9051510":"markdown","2ba56e80":"markdown","aa0821b7":"markdown","a465e5e5":"markdown","583ebf83":"markdown","b4d16c76":"markdown","dc38df64":"markdown","8c5a5628":"markdown","9611fd81":"markdown","7d663ff0":"markdown","59c5d176":"markdown","314e610f":"markdown","5b158ea4":"markdown","09d7d6ee":"markdown","e1c4e84e":"markdown","f022a5be":"markdown","caaf54cb":"markdown","5b9c0862":"markdown","404e7b5f":"markdown","26d6b31d":"markdown","bc1c926f":"markdown","3dce224b":"markdown","051d0627":"markdown","70f2e54b":"markdown","a3f25d41":"markdown","d608b12f":"markdown","10e11b48":"markdown","73c348c3":"markdown","4ccc9b18":"markdown","86c4b02f":"markdown","bcd1779d":"markdown","2d96aea1":"markdown","20ddf469":"markdown","c9d1e6ef":"markdown","8fedf3fa":"markdown","5cc06161":"markdown","17aab13a":"markdown","1bb647fe":"markdown"},"source":{"8ce62122":"import pandas as pd             # data analysis\nimport numpy as np              # linear algebra + array handling\nimport sklearn                  # machine learning\nimport matplotlib.pyplot as plt # visualization\nimport seaborn as sns           # visualization\nimport pdpbox                   # partial dependence analysis","ddfd0bee":"data = pd.read_csv('..\/input\/student-alcohol-consumption\/student-mat.csv')\ndata","8f1f7a7c":"cols = ['age', 'address', 'famsize', 'Pstatus', \n        'Medu', 'Fedu', 'studytime', 'schoolsup',\n        'famsup', 'paid', 'activities', 'internet',\n        'romantic', 'famrel', 'freetime', 'goout',\n        'Dalc', 'Walc', 'traveltime', 'G3']\ndata = data[cols]\ndata","7c5d15f4":"mapping = {'address': {'U':0, 'R':1},\n           'famsize': {'LE3':0, 'GT3':1},\n           'Pstatus': {'T':0, 'A':1},\n           'schoolsup':{'no':0,'yes':1},\n           'famsup':{'no':0,'yes':1},\n           'paid':{'no':0,'yes':1},\n           'activities':{'no':0,'yes':1},\n           'internet':{'no':0,'yes':1},\n           'romantic':{'no':0,'yes':1}}\n\nfor column in list(mapping.keys()):\n    data[column] = data[column].map(mapping[column])","21ff2f48":"data","f74f3d79":"plt.figure(figsize=(10, 5), dpi=400)\nsns.countplot(x = data['G3'], palette='magma')\nax = plt.gca()\nax.grid(which='major', axis='y', linestyle='-')\nplt.title('Distribution of Target Variable (Math Grades)')\nplt.show()","5d8500ed":"x = data.drop('G3', axis=1)\ny = data['G3']","1bc53dcd":"from sklearn.model_selection import train_test_split as tts\nx_train, x_test, y_train, y_test = tts(x, y,\n                                       train_size = 0.7,\n                                       random_state = 101)","42ae54db":"from sklearn.linear_model import LinearRegression\nlr = LinearRegression()\nlr.fit(x_train, y_train)","e212612e":"from sklearn.metrics import mean_absolute_error as mae\nprediction = lr.predict(x_test)\nprint(mae(prediction, y_test))","20de6248":"from sklearn.linear_model import Lasso, ARDRegression, ElasticNet\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor\nmodels = {\n          'Decision Tree': DecisionTreeRegressor(),\n          'Random Forest': RandomForestRegressor(),\n          'Gradient Boosting': GradientBoostingRegressor(),\n          'Support Vector Regressor': SVR(),\n          'AdaBoost Regressor': AdaBoostRegressor(),\n          'LASSO': Lasso(),\n          'Bayesian ARD Regressor': ARDRegression(),\n          'ElasticNet': ElasticNet()\n         }\n\nfor name in models:\n    model = models[name]\n    model.fit(x_train, y_train)\n    prediction = model.predict(x_test)\n    print(f'{name}: {np.round(mae(prediction, y_test), 2)}')","00128eb5":"best_model = SVR()\nbest_model.fit(x_train, y_train)\n\n# cont = True\n# while cont:\n#     print('PREDICT YOUR MATH GRADE')\n#     print('-'*50)\n#     print('Enter student data for the following features.')\n#     x_vector = []\n#     for index, col in enumerate(x.columns):\n#         x_vector.append(int(input(f'     ({index+1}\/{len(x.columns)}): {col}  ---  ')))\n        \n#     x_vector = np.array(x_vector).reshape((1, -1))\n#     prediction = model.predict(x_vector)[0]\n    \n#     print(f'Predicted Math Score (0-10): {np.round(prediction, 2)}')\n#     print(f'     Range: {np.round(prediction - 3.15, 2)} to {np.round(prediction + 3.15, 2)}')\n        \n#     print('-'*50)\n#     if input('Would you like to obtain another prediction? (Y\/N)').lower() != 'y':\n#         cont = False\n#     print('-'*50)","b6444364":"import eli5\nfrom eli5.sklearn import PermutationImportance\nperm = PermutationImportance(best_model, random_state=1).fit(x_test, y_test)\neli5.show_weights(perm, \n                  feature_names = x_test.columns.tolist())","b5d308d3":"col_to_ind = {col:ind for ind, col in enumerate(x.columns)}\nsel_cols = ['age', 'Medu', 'Fedu', 'goout', 'Walc', 'studytime',\n            'internet', 'schoolsup', 'address']\nindices = [col_to_ind[col] for col in sel_cols]\nx_train_modified = x_train.iloc[:, indices]\nx_test_modified = x_test.iloc[:, indices]\n\nfor name in models:\n    model = models[name]\n    model.fit(x_train_modified, y_train)\n    prediction = model.predict(x_test_modified)\n    print(f'{name}: {np.round(mae(prediction, y_test), 2)}')","3bbde027":"model = LinearRegression()\nmodel.fit(x_train, y_train)\ncoefs = pd.DataFrame({'Col': x_train.columns,\n                      'Coef': model.coef_})\ncoefs = coefs.sort_values('Coef', ascending=False)\n\nplt.figure(figsize=(10, 5), dpi=400)\nsns.barplot(coefs['Col'], coefs['Coef'],\n            palette='magma')\nplt.gca().yaxis.grid(True)\nplt.xticks(rotation = 90)\nplt.show()","428b0c54":"coefs","a8e569bc":"from pdpbox import pdp\n\nmodel = RandomForestRegressor()\nmodel.fit(x_train, y_train)\n\ndef visualize_pdp(feat_name):\n    base_features = x.columns.values.tolist()\n    pdp_dist = pdp.pdp_isolate(model=model, dataset=x_test, \n                               model_features=base_features, \n                               feature=feat_name)\n    pdp.pdp_plot(pdp_dist, feat_name)\n    plt.show()\n    \nvisualize_pdp('Dalc')\nvisualize_pdp('Walc')","719618ae":"model = RandomForestRegressor()\nmodel.fit(x_train, y_train)\n\nfrom pdpbox import pdp, info_plots\n\nbase_features = x.columns.values.tolist()\n\nfor feat_name in x.columns.drop(['Dalc', 'Walc']):\n    pdp_dist = pdp.pdp_isolate(model=model, dataset=x_test, model_features=base_features, feature=feat_name)\n    pdp.pdp_plot(pdp_dist, feat_name)\n    plt.show()","a1db4578":"# # uncomment and run to visualize distribution of target variables\n\n# plt.figure(figsize=(10, 5), dpi=400)\n# sns.countplot(x = data['Dalc'], palette='magma')\n# ax = plt.gca()\n# ax.grid(which='major', axis='y', linestyle='-')\n# plt.title('Distribution of Weekday Alcohol Consumption in Dataset')\n# plt.show()\n\n# plt.figure(figsize=(10, 5), dpi=400)\n# sns.countplot(x = data['Walc'], palette='magma')\n# ax = plt.gca()\n# ax.grid(which='major', axis='y', linestyle='-')\n# plt.title('Distribution of Weekend Alcohol Consumption in Dataset')\n# plt.show()","e6260520":"model = DecisionTreeRegressor(random_state = 1)\nmodel.fit(x_train, y_train)\n\nfeats = ['Dalc', 'Walc']\ninter = pdp.pdp_interact(model=model, dataset=x_test, model_features=base_features, \n                          features=feats)\npdp.pdp_interact_plot(pdp_interact_out=inter, \n                      feature_names=feats, \n                      plot_type='contour')\nplt.show()","b806bc6e":"model = DecisionTreeRegressor(random_state = 101)\nmodel.fit(x_train, y_train)\nfeats = ['traveltime', 'age']\ninter = pdp.pdp_interact(model=model, dataset=x_test, model_features=base_features, \n                          features=feats)\npdp.pdp_interact_plot(pdp_interact_out=inter, \n                      feature_names=feats, \n                      plot_type='contour')\nplt.show()","bd2b546f":"model = RandomForestRegressor(random_state = 120)\nmodel.fit(x_train, y_train)\nfeats = ['Medu', 'Fedu']\ninter = pdp.pdp_interact(model=model, dataset=x_test, model_features=base_features, \n                          features=feats)\npdp.pdp_interact_plot(pdp_interact_out=inter, \n                      feature_names=feats, \n                      plot_type='contour')\nplt.show()","2cf370a4":"model = RandomForestRegressor(random_state = 120)\nmodel.fit(x_train, y_train)\nfeats = ['Dalc', 'Fedu']\ninter = pdp.pdp_interact(model=model, dataset=x_test, model_features=base_features, \n                          features=feats)\npdp.pdp_interact_plot(pdp_interact_out=inter, \n                      feature_names=feats, \n                      plot_type='contour')\nplt.show()","6292e7a1":"model = RandomForestRegressor(random_state = 120)\nmodel.fit(x_train, y_train)\nfeats = ['goout', 'traveltime']\ninter = pdp.pdp_interact(model=model, dataset=x_test, model_features=base_features, \n                          features=feats)\npdp.pdp_interact_plot(pdp_interact_out=inter, \n                      feature_names=feats, \n                      plot_type='contour')\nplt.show()","d800ee69":"x = data.drop('romantic', axis=1)\ny = data['romantic']\nx_train, x_test, y_train, y_test = tts(x, y, train_size = 0.75)","f428f32b":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\n\nmodels = {\n    'Decision Tree': DecisionTreeClassifier(),\n    'Random Forest': RandomForestClassifier(),\n    'AdaBoost': AdaBoostClassifier(),\n    'SVM': SVC(),\n    'KNN 10': KNeighborsClassifier(n_neighbors = 10),\n    'KNN 100': KNeighborsClassifier(n_neighbors = 100)\n}\n\nfrom sklearn.metrics import f1_score\n\nfor name in models:\n    model = models[name]\n    model.fit(x_train, y_train)\n    prediction = model.predict(x_test)\n    print(f'{name}: {np.round(f1_score(prediction, y_test), 2)}')","5c13185c":"best_model = DecisionTreeClassifier().fit(x_train, y_train)\nperm = PermutationImportance(best_model, random_state=1).fit(x_test, y_test)\neli5.show_weights(perm, \n                  feature_names = x_test.columns.tolist())","5087e241":"model = DecisionTreeClassifier()\nmodel.fit(x_train, y_train)\n\nfrom pdpbox import pdp, info_plots\n\nbase_features = x.columns.values.tolist()\n\nfor feat_name in x.columns:\n    pdp_dist = pdp.pdp_isolate(model=model, dataset=x_test, model_features=base_features, feature=feat_name)\n    pdp.pdp_plot(pdp_dist, feat_name)\n    plt.show()","7048f204":"*This notebook was created for a workshop, presented for the Polling and Open Data Initiative at the University of Washington.*","9166bb74":"When analyzing multi-feature PDPs, imagine a line perpendicular to an axis of focus moving back and forth. How does the contour cross-section change over time?\n- At low week*end* alcohol consumption levels, week*day* alcohol consumption differences on the small scale (1 to 2.5 ish) make a grade difference.\n- The effect of week*day* alcohol consumption doesn't really change that much after a certain threshold, about 2.5. (Also not that many sample points in high-alcohol consumption areas.)\n- At relatively week*day* alchol consumption levels (2-5), changes in the low range of week*end* alcohol consumption make a grade difference.\n","c4f8bd0c":"Great! We can start modeling.","615927ae":"---","19161612":"Let's evaluate on a selection of models. Note that we're using classification models rather than the regression models.","82a55963":"On average, the Linear Regression model's prediction is slightly more than 3 grade points off. Since this is in either direction, this forms a 'range' almost 7 grade points around every prediction, which is very undesirable given the range of 0 to 10.\n\nLet's try some more sophisticated models.","75297ff2":"---","2b335611":"## -1. Importing Libraries","6c1cfd6a":"However, linear regression model coefficient interpretation assumes that the 'partial' effect of an independent variable on the total output is uniform\/linear throughout the range of the independent variable. For instance, the effect of moving from $x = 1$ to $x = 2$ is the same as moving from $x = 100$ to $x = 101$. This is not the case in most problems.\n\nWe will use *partial dependence plots*, which calculate the effect of an independent feature on the dependent variable at regular intervals by navigating the model's decision space. The effect is associated with different confidences to help us gauge the generalizability\/applicability of the trend.","fcd599a8":"The best model, Support Vector Regressor, gives us 3.16 error - marginally better than Linear Regression. When more sophisticated  classical machine learning models perform the same or worse than Linear Regression, we know our dataset is quite noisy!\n\n(Usually we do more complicated things with machine learning models, like fine-tuning internal parameters to optimize performance.)\n\n**Failure to model an $x$-$y$ relationship in data can be informative in and of itself.** A student's math scores in this dataset cannot be reliably\/consistently predicted by their demographic, social, and personal factors. This can be a good thing! - it shows students from different backgrounds and with different behaviors can perform both well and poorly on math exams.","3f933025":"Can we predict if a student is in a relationship or not? What factors contribute most to successful prediction?","f79aa982":"Let's build a (poorly designed and very limited) application to help users get predictions for custom data.\n\n(Example of an interactive ML application.)","63ca9058":"You can also try to do multi-dimensional PDP analysis to find out more.","f7e37043":"Let's try to view the effects of various features on whether a student is engaged in a relationship or not.","d103a0cf":"Let's view the most imporant factors contributing to whether a student is in a relationship or not.","2a4bd4e7":"Some interesting insights:\n- When parents live apart rather than together, the student's grade is expected to increase by roughly 1.28 points. (???)\n- Paying for extra tutoring classes is associated with a 1.08 point increase.\n- Somehow, the mother's education has a greater positive effect on a student's math score than the father's education.\n- Being in a romantic relationship is associated with a 1.42 point decline.\n\nNote that the applicability of these coefficients is dependent on the performance of the model, which in this case isn't great. These are general trends rather than specific 'guarantees'.","775f948d":"---","43caf710":"## 0. Loading and Cleaning the Dataset","13f5be4f":"Loading and viewing raw data:","0014c539":"---","2204de32":"Python libraries handle most of the work for us. Even if you don't code, knowing that these methods exist can help you better plan analytics project pathways.","3067c1d1":"Recommended skills: basic Python knowledge, basic ML knowledge","b9051510":"We can exclude some variables from analysis. We'll foucs on the following features:\n- `age` (15 to 22)\n- `address` (`U` for urban or `R` for rural)\n- `famsize` (`LE3` for $<=3$ or `GT3` for $>3$)\n- `Pstatus` (`T` for living together or `A` for living apart)\n- `Medu` and `Fedu` (0 to 4 for parents' various education levels, none to college)\n- `studytime` (1 to 4 indicating time buckets in hours)\n- `schoolsup` (yes or no, extra educational support)\n- `famsup` (yes or no, family educational support)\n- `paid` (yes or no, extra paid classes in subject)\n- `activities` (yes or no, extracurriculars)\n- `internet` (yes or no, Internet access at home)\n- `romantic` (yes or no, in a romantic relationship)\n- `famrel` (1 to 5, quality of family relationships)\n- `freetime` (1 to 5, free time afterf school)\n- `goout` (1 to 5, going out with friends)\n- `traveltime` (1 to 5, time needed to travel to school)\n- **`Dalc` (1 to 5, workday alcohol consumption)**\n- **`Walc` (1 to 5, weekend alcohol consumption)**\n- **`G3` (0 to 20, final grade)**","2ba56e80":"We need to encode categorical variables appropriately. We'll define dictionaries that map certain text values to quantiative representations.","aa0821b7":"Let's split into training and testing datasets.","a465e5e5":"We're using the F1-score, which better reflects a model's performance on imbalanced datasets. We see Decision Tree performs the best.","583ebf83":"We need to split the dataset into *training* and *testing* datasets.\n\n(P.S.: the \"test set\" is actually the \"validation set\", but we'll stick with \"test set\" terminology because `sklearn` made the frankly poor decision to call the train-validation split function `train_test_split`.)","b4d16c76":"---","dc38df64":"We observe marginally improved model performance by selecting the most important features (and excluding the others).","8c5a5628":"**!!! Correlation is not causation !!!**\n\nBe careful when attempting to make causal inferences.","9611fd81":"First, let's try to understand the distribution of the target variable.","7d663ff0":"We see that as the mother and father's education increases, generally the grades increase as well. There are some interesting trends we notice in how students can achieve different grade contour levels: for instance, a wide range of father education values are possible for the very top 10-10.25 echelon, but the mother's education is strictly between 3.5 and 4.0. Moreover, as the father's education decreases, the mother's education must increase to keep within a tier.","59c5d176":"# Alcohol, Love, and Grades\n\n## PODUW - Machine Learning Modeling and Interpretability Workshop","314e610f":"Let's start off with a very simple linear regression analysis. We'll train a linear regression model and loook at its coefficient values.\n\nA linear regression model is defined as\n\n$$f(x) = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + ...$$\n\nThe coefficients $\\beta_1$, $\\beta_2$, $...$ give us information about how a single independent variable $x_1$, $x_2$, $...$ impacts the result. For instance, if $x_1$ increases by 1 unit, then the predicted output will increase by $\\beta_1 \\cdot 1 = \\beta_1$.\n\n**A higher coefficient value does not necessarily mean that it is more important.** Coefficients may need to be smaller or larger to account for independent variable units. The coefficient derived from linear regression for a feature measuring distance represented in inches would be much smaller than the same feature measuring distance in light-years, even though both are the same in information content. However, coefficient magnitude is comparable among independent variables of the same unit.","5b158ea4":"We want to build a model $f$ that maps an input $x$ to an output $y$. In this case, $x$ is a vector ('list' of features) and $y$ is a scalar (single value).\n\n$$f(x) = y$$\n\nLet's identify our $x$ and $y$.","09d7d6ee":"## 2. Feature Importance","e1c4e84e":"We want to evaluate how well the model did on the testing data. Let's measure the **mean absolute error**, or the average value of how far off the model was from the target.\n\n$$\\text{MAE} = \\sum \\left| \\hat{y} - y \\right|$$","f022a5be":"Our data is now completely quantiative and we can begin modeling.","caaf54cb":"We'll be working with the University of California Irvine **Student Alcohol Consumption**. We will build machine learning models to model the dataset and interpret it to answer some interesting and important questions:\n\n| Section | Question | Technique |\n| --- | --- | --- |\n| 1 | Can we predict a student's grades? | Constructing and training a ML model |\n| 2 | What are the most important predictors of a student's grades? | Feature importance |\n| 3 | How do changes in certain factors affect a student's grades? | Single-feature partial dependence |\n| 4 | How do student weekday and weekend alcohol consumption affect grades? | Multi-feature partial dependence |\n| 5 | Can we predict if students are in a relationship or not, and if so, how? (i.e. how to find love) | Putting it all together! |","5b9c0862":"Let's generate multi-feature interaction plots for more feature combinations:","404e7b5f":"Some interesting observations:\n- Slightly parabolic shape of `goout`\n- Negative trend effect of `romantic`\n- Effect of going to college for mother vs father in `medu` vs `fedu`","26d6b31d":"Attributes for `student-mat.csv` (math course grades):\n\n```\nschool - student's school (binary: 'GP' - Gabriel Pereira or 'MS' - Mousinho da Silveira)\nsex - student's sex (binary: 'F' - female or 'M' - male)\nage - student's age (numeric: from 15 to 22)\naddress - student's home address type (binary: 'U' - urban or 'R' - rural)\nfamsize - family size (binary: 'LE3' - less or equal to 3 or 'GT3' - greater than 3)\nPstatus - parent's cohabitation status (binary: 'T' - living together or 'A' - apart)\nMedu - mother's education (numeric: 0 - none, 1 - primary education (4th grade), 2 \u2013 5th to 9th grade, 3 \u2013 secondary education or 4 \u2013 higher education)\nFedu - father's education (numeric: 0 - none, 1 - primary education (4th grade), 2 \u2013 5th to 9th grade, 3 \u2013 secondary education or 4 \u2013 higher education)\nMjob - mother's job (nominal: 'teacher', 'health' care related, civil 'services' (e.g. administrative or police), 'at_home' or 'other')\nFjob - father's job (nominal: 'teacher', 'health' care related, civil 'services' (e.g. administrative or police), 'at_home' or 'other')\nreason - reason to choose this school (nominal: close to 'home', school 'reputation', 'course' preference or 'other')\nguardian - student's guardian (nominal: 'mother', 'father' or 'other')\ntraveltime - home to school travel time (numeric: 1 - <15 min., 2 - 15 to 30 min., 3 - 30 min. to 1 hour, or 4 - >1 hour)\nstudytime - weekly study time (numeric: 1 - <2 hours, 2 - 2 to 5 hours, 3 - 5 to 10 hours, or 4 - >10 hours)\nfailures - number of past class failures (numeric: n if 1<=n<3, else 4)\nschoolsup - extra educational support (binary: yes or no)\nfamsup - family educational support (binary: yes or no)\npaid - extra paid classes within the course subject (Math or Portuguese) (binary: yes or no)\nactivities - extra-curricular activities (binary: yes or no)\nnursery - attended nursery school (binary: yes or no)\nhigher - wants to take higher education (binary: yes or no)\ninternet - Internet access at home (binary: yes or no)\nromantic - with a romantic relationship (binary: yes or no)\nfamrel - quality of family relationships (numeric: from 1 - very bad to 5 - excellent)\nfreetime - free time after school (numeric: from 1 - very low to 5 - very high)\ngoout - going out with friends (numeric: from 1 - very low to 5 - very high)\nDalc - workday alcohol consumption (numeric: from 1 - very low to 5 - very high)\nWalc - weekend alcohol consumption (numeric: from 1 - very low to 5 - very high)\nhealth - current health status (numeric: from 1 - very bad to 5 - very good)\nabsences - number of school absences (numeric: from 0 to 93)\n```\n\nGrades:\n```\nG1 - first period grade (numeric: from 0 to 20)\nG2 - second period grade (numeric: from 0 to 20)\nG3 - final grade (numeric: from 0 to 20)\n```","bc1c926f":"This is quite interesting! What could explain this phenomenon?","3dce224b":"(Don't worry about the error for now.)","051d0627":"---","70f2e54b":"---","a3f25d41":"## 4. Multi-Feature Partial Dependence","d608b12f":"This is interesting - travel time really only makes a significant difference in grade if the student is within a certain age range, from about 15 to 18.","10e11b48":"## 5. Putting it All Together","73c348c3":"## 1. Constructing and Training a ML Model","4ccc9b18":"*It's hard to interpret the units of the Weight. It's good enough to know that higher values indicate greater importance (i.e. higher impact on model performance), values near 0 indicate neutrality (i.e. the addition of the feature does not help or damage model performance), negative values indicate damage (i.e. removing the feature actually helps performance).*\n\nInteresting observations:\n- Age is the most important factor to predicting one's grade.\n- The education level of a student's parents are roughly equally important in predicting one's grade as well.\n- Weekend consumption of alcohol is a somewhat important factor in predicting student grades.\n- Factors like Internet access, family size, romantic relatioships, and alcohol consumption on weekdays have little predictive value.\n- Factors like free time and family relationship quality are noisy and damage model performance.","86c4b02f":"We can also estimate the partial effect when *multiple* features interact with each other.","bcd1779d":"We use the `RandomForestRegressor` to explain partial dependence instead of `SVR` because the nature of `SVR`'s algorithm causes it to produce rather uninteresting and very linear results, even though it performs slightly better.","2d96aea1":"Let's train a simple linear regression model to predict the *grade* based on the features in $x$.","20ddf469":"**Main Question:** Can we predict a student's math grades using various demographic, social, and personal factors?","c9d1e6ef":"**Permutation Importance** judges the importance of a feature by *how much it impacts model performance when it is removed*.\n\nThe `eli5` library allows us to compute permutation importance. We need to 'fit' a `PermutationImportance` object on `x_test` and `y_test`. We're not actually training the *model* here (that would be a violation of train-test separation, constituting data leakage) -- we're simply allowing `eli5` to judge the impact of a feature's loss on the model's test performance.","8fedf3fa":"If you're interested, you can view the Partial Dependence Plots for other features. Note that although a lot of the dependencies appear to be linear, the dataset is bucketed in a way such that the attribute each feature represents may not actually be linear. Keep this in mind when interpreting the partial effect of an independent variable on a dependent variable.","5cc06161":"## You can do a lot more!\n\nI recommend looking more into the SHAP library, particularly the force plots and cross-sample force plots, which can help demonstrate the effects of different features\/forces on the final outcome. This is especially helpful for complex qualitative analyses that need some specific examples.\n\nGood resources:\n- [SHAP Paper](https:\/\/proceedings.neurips.cc\/paper\/2017\/file\/8a20a8621978632d76c43dfd28b67767-Paper.pdf)\n- [Approachable TDS Article on SHAP](https:\/\/towardsdatascience.com\/shap-shapley-additive-explanations-5a2a271ed9c3)\n- [In-Depth Slides on SHAP](https:\/\/course.ece.cmu.edu\/~ece739\/lectures\/18739-2020-spring-lecture-14-shapley.pdf)","17aab13a":"## 3. Single-Feature Partial Dependence","1bb647fe":"---"}}