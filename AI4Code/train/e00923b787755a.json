{"cell_type":{"f6a4f457":"code","8b0c9371":"code","73f8f580":"code","6baa4899":"code","3ee205d6":"code","bdcbf97e":"code","ef09fc71":"code","434ed210":"code","a8ce22e6":"code","813c2353":"code","545f9120":"code","2dc764d6":"code","5c8001bd":"code","cd2194f5":"code","a249899b":"code","d1b24eca":"code","5f57c366":"code","9cbc2cea":"code","2e25382a":"code","38faa186":"code","21e81356":"code","af8cc700":"code","3e39211e":"code","469828c9":"code","0ade0056":"code","fad51803":"code","26b5c275":"code","1f595153":"code","f8767fc5":"code","ddc01ea0":"code","dda80175":"code","9abe8602":"markdown","9f9ffeba":"markdown","c96e6142":"markdown","ba64902c":"markdown","a9979ee2":"markdown"},"source":{"f6a4f457":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import KFold\nfrom sklearn import model_selection, preprocessing, metrics\n\nimport warnings\nimport datetime\nwarnings.filterwarnings(\"ignore\")\n\nimport os\nprint(os.listdir(\"..\/input\"))","8b0c9371":"#Loading Train and Test Data\ndf_train = pd.read_csv(\"..\/input\/train.csv\", parse_dates=[\"first_active_month\"])\ndf_test = pd.read_csv(\"..\/input\/test.csv\", parse_dates=[\"first_active_month\"])\nprint(\"{} observations and {} features in train set.\".format(df_train.shape[0],df_train.shape[1]))\nprint(\"{} observations and {} features in test set.\".format(df_test.shape[0],df_test.shape[1]))","73f8f580":"df_train.head()","6baa4899":"df_train[\"month\"] = df_train[\"first_active_month\"].dt.month\ndf_test[\"month\"] = df_test[\"first_active_month\"].dt.month\ndf_train[\"year\"] = df_train[\"first_active_month\"].dt.year\ndf_test[\"year\"] = df_test[\"first_active_month\"].dt.year\ndf_train['elapsed_time'] = (datetime.date(2018, 2, 1) - df_train['first_active_month'].dt.date).dt.days\ndf_test['elapsed_time'] = (datetime.date(2018, 2, 1) - df_test['first_active_month'].dt.date).dt.days\ndf_train.head()","3ee205d6":"df_train = pd.get_dummies(df_train, columns=['feature_1', 'feature_2'])\ndf_test = pd.get_dummies(df_test, columns=['feature_1', 'feature_2'])\ndf_train.head()","bdcbf97e":"df_hist_trans = pd.read_csv(\"..\/input\/historical_transactions.csv\")\ndf_hist_trans.head()","ef09fc71":"df_hist_trans = pd.get_dummies(df_hist_trans, columns=['category_2', 'category_3'])\ndf_hist_trans['authorized_flag'] = df_hist_trans['authorized_flag'].map({'Y': 1, 'N': 0})\ndf_hist_trans['category_1'] = df_hist_trans['category_1'].map({'Y': 1, 'N': 0})\ndf_hist_trans.head()","434ed210":"def aggregate_transactions(trans, prefix):  \n    trans.loc[:, 'purchase_date'] = pd.DatetimeIndex(trans['purchase_date']).\\\n                                      astype(np.int64) * 1e-9\n    \n    agg_func = {\n        'authorized_flag': ['sum', 'mean', 'median'],\n        'category_1': ['mean', 'median'],\n        'category_2_1.0': ['mean', 'median'],\n        'category_2_2.0': ['mean', 'median'],\n        'category_2_3.0': ['mean', 'median'],\n        'category_2_4.0': ['mean', 'median'],\n        'category_2_5.0': ['mean', 'median'],\n        'category_3_A': ['mean', 'median'],\n        'category_3_B': ['mean', 'median'],\n        'category_3_C': ['mean', 'median'],\n        'merchant_id': ['nunique'],\n        'purchase_amount': ['sum', 'mean', 'max', 'min', 'std', 'median'],\n        'installments': ['sum', 'mean', 'max', 'min', 'std', 'median'],\n        'purchase_date': [np.ptp],\n        'month_lag': ['min', 'max']\n    }\n    agg_trans = trans.groupby(['card_id']).agg(agg_func)\n    agg_trans.columns = [prefix + '_'.join(col).strip() \n                           for col in agg_trans.columns.values]\n    agg_trans.reset_index(inplace=True)\n    \n    df = (trans.groupby('card_id')\n          .size()\n          .reset_index(name='{}transactions_count'.format(prefix)))\n    \n    agg_trans = pd.merge(df, agg_trans, on='card_id', how='left')\n    \n    return agg_trans","a8ce22e6":"import gc\nmerch_hist = aggregate_transactions(df_hist_trans, prefix='hist_')\ndel df_hist_trans\ngc.collect()\ndf_train = pd.merge(df_train, merch_hist, on='card_id',how='left')\ndf_test = pd.merge(df_test, merch_hist, on='card_id',how='left')\ndel merch_hist\ngc.collect()\ndf_train.head()","813c2353":"df_new_trans = pd.read_csv(\"..\/input\/new_merchant_transactions.csv\")\ndf_new_trans.head()","545f9120":"df_new_trans = pd.get_dummies(df_new_trans, columns=['category_2', 'category_3'])\ndf_new_trans['authorized_flag'] = df_new_trans['authorized_flag'].map({'Y': 1, 'N': 0})\ndf_new_trans['category_1'] = df_new_trans['category_1'].map({'Y': 1, 'N': 0})\ndf_new_trans.head()","2dc764d6":"merch_new = aggregate_transactions(df_new_trans, prefix='new_')\ndel df_new_trans\ngc.collect()\ndf_train = pd.merge(df_train, merch_new, on='card_id',how='left')\ndf_test = pd.merge(df_test, merch_new, on='card_id',how='left')\ndel merch_new\ngc.collect()\ndf_train.head()","5c8001bd":"# TO DO add merchants","cd2194f5":"target = df_train['target']\ndrops = ['card_id', 'first_active_month', 'target']\nuse_cols = [c for c in df_train.columns if c not in drops]\nfeatures = list(df_train[use_cols].columns)\ndf_train[features].head()","a249899b":"print(df_train[features].shape)\nprint(df_test[features].shape) # Validation set","d1b24eca":"import contextlib\nimport xgboost as xgb\nfrom bayes_opt import BayesianOptimization\nfrom sklearn.metrics import mean_squared_error","5f57c366":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(df_train[features],\n                                                    target, test_size=0.2)\ndtrain = xgb.DMatrix(X_train, label=y_train)\ndtest = xgb.DMatrix(X_test)\n\ndel(df_train)\ndel(X_train)\ndel(X_test)","9cbc2cea":"dholdout = xgb.DMatrix(df_test[features])\ntarget_holdout = df_test[\"card_id\"].values\n\ndel(df_test)","2e25382a":"# # https:\/\/www.kaggle.com\/tilii7\/bayesian-optimization-of-xgboost-parameters\/notebook\n# def xgb_evaluate(max_depth, \n#                  gamma,\n#                  min_child_weight,\n#                  max_delta_step,\n#                  subsample,\n#                  colsample_bytree):\n    \n#     global RMSEbest\n#     global ITERbest\n    \n#     params = {'eval_metric': 'rmse',\n#               'nthread' : 4,\n#               'silent' : True,\n#               'max_depth': int(max_depth),\n#               'subsample': max(min(subsample, 1), 0),\n#               'eta': 0.1,\n#               'gamma': gamma,\n#               'colsample_bytree': max(min(colsample_bytree, 1), 0),   \n#               'min_child_weight': min_child_weight ,\n#               'max_delta_step':int(max_delta_step),\n#               'seed' : 16\n#              }\n    \n#     folds = 5\n#     cv_score = 0\n    \n#     print(\"\\n Search parameters (%d-fold validation):\\n %s\" % (folds, params), file=log_file )\n#     log_file.flush()\n    \n#     # Used around 1000 boosting rounds in the full model\n#     cv_result = xgb.cv(params, \n#                        dtrain, \n#                        num_boost_round=2000, # can be increased 1k ~ 20k\n#                        nfold=folds,\n# #                       verbose_eval = 10,\n#                        early_stopping_rounds = 100,\n#                        metrics = 'rmse',\n#                        show_stdv = True\n#                       )    \n    \n#     val_score = cv_result['test-rmse-mean'].iloc[-1]\n#     train_score = cv_result['train-rmse-mean'].iloc[-1]\n#     print(' Stopped after %d iterations with train-rmse = %f val-rmse = %f ( diff = %f ) ' % ( len(cv_result), train_score, val_score, (train_score - val_score) ) )\n#     if ( val_score > RMSEbest ):\n#         RMSEbest = val_score\n#         ITERbest = len(cv_result)\n\n#     # Bayesian optimization only knows how to maximize, not minimize, so return the negative RMSE\n#     return -1.0 * val_score","38faa186":"# log_file = open('Elo-RMSE-5fold-XGB-run-01-v1-full.log', 'a')\n# RMSEbest = -1.\n# ITERbest = 0\n\n# xgb_bo = BayesianOptimization(xgb_evaluate, {\n#                                     'max_depth': (2, 12),\n#                                      'gamma': (0.001, 10.0),\n#                                      'min_child_weight': (0, 20),\n#                                      'max_delta_step': (0, 10),\n#                                      'subsample': (0.4, 1.0),\n#                                      'colsample_bytree' :(0.4, 1.0)})","21e81356":"# xgb_bo.explore({\n#               'max_depth':            [3, 8, 3, 8, 8, 3, 8, 3],\n#               'gamma':                [0.5, 8, 0.2, 9, 0.5, 8, 0.2, 9],\n#               'min_child_weight':     [0.2, 0.2, 0.2, 0.2, 12, 12, 12, 12],\n#               'max_delta_step':       [1, 2, 2, 1, 2, 1, 1, 2],\n#               'subsample':            [0.6, 0.8, 0.6, 0.8, 0.6, 0.8, 0.6, 0.8],\n#               'colsample_bytree':     [0.6, 0.8, 0.6, 0.8, 0.6, 0.8, 0.6, 0.8],\n#               })","af8cc700":"# %%time\n\n# # Use the expected improvement acquisition function to handle negative numbers\n# # Optimally needs quite a few more initiation points 15-20 and number of iterations 25-50\n\n# print('-'*130)\n# print('-'*130, file=log_file)\n# log_file.flush()\n\n# with warnings.catch_warnings():\n#     warnings.filterwarnings('ignore')\n#     #xgb_bo.maximize(init_points=1, n_iter=1, acq='ei', xi=0.0)\n#     xgb_bo.maximize(init_points=10, n_iter=50, acq='ei', xi=0.01)","3e39211e":"# print('-'*130)\n# print('Final Results')\n# print('Maximum XGBOOST value: %f' % xgb_bo.res['max']['max_val'])\n# print('Best XGBOOST parameters: ', xgb_bo.res['max']['max_params'])\n# print('-'*130, file=log_file)\n# print('Final Result:', file=log_file)\n# print('Maximum XGBOOST value: %f' % xgb_bo.res['max']['max_val'], file=log_file)\n# print('Best XGBOOST parameters: ', xgb_bo.res['max']['max_params'], file=log_file)\n# log_file.flush()\n# log_file.close()\n\n# history_df = pd.DataFrame(xgb_bo.res['all']['params'])\n# history_df2 = pd.DataFrame(xgb_bo.res['all']['values'])\n# history_df = pd.concat((history_df, history_df2), axis=1)\n# history_df.rename(columns = { 0 : 'rmse'}, inplace=True)\n# history_df['rmse'] = np.abs(history_df['rmse'])\n# history_df.to_csv('Elo-RMSE-5fold-XGB-run-01-v1-grid.csv')","469828c9":"# pd.read_csv('Elo-RMSE-5fold-XGB-run-01-v1-grid.csv')","0ade0056":"params1 = {'max_depth': 12.0, \n          'gamma': 3.1390339637040787, \n          'min_child_weight': 0.0, \n          'max_delta_step': 10.0, \n          'subsample': 1.0, \n          'colsample_bytree': 1.0}\n\nparams2 = {'colsample_bytree': 0.4,\n          'gamma': 7.438375302732893,\n          'max_delta_step': 3.0732617140069647,\n          'max_depth': 9.926188389437563,\n          'min_child_weight': 20.0,\n          'subsample': 1.0}\n\nparams1['max_depth'] = int(params1['max_depth'])\nparams2['max_depth'] = int(params2['max_depth'])","fad51803":"%%time \n\n# Train a new model with the best parameters from the search\nmodel1 = xgb.train(params1, dtrain, num_boost_round=500)\nmodel2 = xgb.train(params2, dtrain, num_boost_round=500)","26b5c275":"# # Predict on testing and training set\n# y_pred = model2.predict(dtest)\n# y_train_pred = model2.predict(dtrain)\n\n# # Report testing and training RMSE\n# print('Test error:', np.sqrt(mean_squared_error(y_test, y_pred)))\n# print('Train error:', np.sqrt(mean_squared_error(y_train, y_train_pred)))","1f595153":"# # feature importance\n# fig =  plt.figure(figsize = (12,8))\n# axes = fig.add_subplot(111)\n# xgb.plot_importance(model2,ax = axes,height =0.5)\n# sns.despine()\n# plt.tight_layout()","f8767fc5":"predictions1 = model1.predict(dholdout)\npredictions2 = model2.predict(dholdout)","ddc01ea0":"df_sub = pd.DataFrame({'card_id': target_holdout, 'target': predictions1})\ndf_sub.to_csv('p1.csv', index=False)","dda80175":"df_sub = pd.DataFrame({'card_id': target_holdout, 'target': predictions2})\ndf_sub.to_csv('p2.csv', index=False)","9abe8602":"# Prediction ","9f9ffeba":"# Training","c96e6142":"# Testing","ba64902c":"Best model","a9979ee2":"# Feature importance"}}