{"cell_type":{"45de530d":"code","1e247675":"code","3eac22b5":"code","6ab8f6ce":"code","d4b5aa72":"code","3248e989":"code","09c28772":"code","4655a5b8":"code","b15a0c94":"code","5cc10191":"code","98c42a65":"code","57477878":"code","f4482c95":"code","9f2c56f4":"code","f750fa8e":"code","3181c8b0":"code","4ba973f8":"code","10a5ab3e":"code","07bb2fa0":"code","5c34cac9":"code","6e4792f6":"code","3938ad4a":"code","c3111284":"code","99072d8d":"code","6b3efce7":"code","6880f50d":"code","d1cc8459":"code","c99dabc1":"code","ad7b3fa4":"code","9acf1473":"code","4b23fb50":"code","ce9bde33":"code","b5aadcdb":"code","d003ea41":"code","6deaffc1":"code","d867ed50":"code","9c314bae":"code","14448dd1":"code","34d4108a":"code","7fa3fffb":"code","ac6db556":"code","a21c5a71":"code","7adcafff":"code","5b885c33":"code","c1a00a19":"code","f18c6050":"code","ed38af55":"code","61dfd6c9":"code","1f42a47d":"code","79a98a0f":"code","7ee9d548":"code","7a3829cc":"code","f73d43f8":"code","1a72526c":"code","d5050e4f":"code","41bbcf43":"code","42fdd03a":"code","aad7a7ab":"code","f1f41636":"code","bc518ed4":"code","52d94c8f":"code","b66a6887":"code","41ef1cb4":"code","7f232836":"code","3a0bdd40":"code","a9b48ce8":"code","1f6295c1":"code","7c264c6b":"code","41fef81d":"code","7ddff48b":"code","92336b1d":"code","7fdcdaac":"code","df83b099":"code","a3d81f1d":"code","2c4e2718":"code","83026057":"code","8e084e31":"code","30c65216":"code","240797e1":"code","16a477b8":"code","5379ef41":"code","ea351c8a":"code","38ceae96":"code","30eed8db":"code","6000f4a1":"code","523ec6d2":"code","8b4aeace":"code","988f5584":"code","d8b812dd":"code","2d85521a":"code","649f3a1e":"code","7260dc84":"code","34fd80e9":"code","2d412fd6":"code","fc9ad8d2":"code","0c62f371":"code","519b7e1b":"code","43c121dc":"code","4d71a35d":"code","2701c514":"code","6774466c":"code","a6621101":"code","6a758ef4":"code","8ad3f179":"code","a614510b":"code","a9558252":"code","c2224b4a":"code","4837761a":"code","3011e85e":"code","794ad20d":"code","1f08e677":"code","8749b2d1":"code","9964a8cb":"code","9aa787d7":"code","5fa38b29":"code","78b0ffff":"code","837c6bde":"code","13f681a7":"code","c999106b":"code","fe6454a3":"code","f69c8bc4":"code","cae78516":"code","c1c0c2cd":"code","1ab45d5d":"code","122e3894":"code","bca353a2":"code","0c7fb4ff":"code","7b23fb5d":"code","8f414b67":"code","c2fa7525":"code","aa392208":"code","f7cf8576":"code","61aa6942":"code","1d56b655":"code","6ac8d053":"code","1a9c9774":"code","ad1495a9":"markdown","6d3928a1":"markdown","0ac6b77f":"markdown","8f54e9a9":"markdown","c611abfe":"markdown","39cfb0bb":"markdown","8450c3cd":"markdown","97b86ee3":"markdown","9e7747d4":"markdown","4481d3a7":"markdown","6f470ee8":"markdown","9f737daa":"markdown","c6725e28":"markdown","be36bbe4":"markdown","cb412879":"markdown","b151f24c":"markdown","15b52f1c":"markdown","1fb6b9da":"markdown","f8fa5ba7":"markdown","6cf36319":"markdown","6fdcf39c":"markdown","c642aea3":"markdown","33a3e072":"markdown","9d2d302a":"markdown","d245659b":"markdown","76bd465b":"markdown","3ebe5a35":"markdown","eaf5b4ec":"markdown","522ee66e":"markdown","a5388112":"markdown","51693c6c":"markdown","99a9c34a":"markdown","6059a73a":"markdown","477b602f":"markdown","4ab09c15":"markdown","dee08709":"markdown","471b00e5":"markdown","ae64465c":"markdown","6bb315b2":"markdown","39a59716":"markdown","d44b820c":"markdown","ccd6fd86":"markdown","22b07e7f":"markdown","44e432e9":"markdown","37ba5573":"markdown","8d848459":"markdown","db003dab":"markdown","122eb4cb":"markdown","f43c6e12":"markdown","8ecb4ae6":"markdown","14b4c7f9":"markdown","d9481681":"markdown","1cc089dc":"markdown","08e08f5b":"markdown","bdcb4ba2":"markdown","f83af74a":"markdown","52c4623f":"markdown","6b6e5c7a":"markdown","b34bc99c":"markdown","109a172d":"markdown","14d5a52a":"markdown","3bd55d4f":"markdown","b83d2289":"markdown","42c6df26":"markdown","f99da59a":"markdown","aa80ef91":"markdown","69e9eb20":"markdown","3f6eacfa":"markdown","49700e59":"markdown","b8a5ceb9":"markdown","180dcb1f":"markdown","0ca32ce1":"markdown","dc79212b":"markdown","547cb207":"markdown","7fe24514":"markdown","d8a4316b":"markdown","880fc6a8":"markdown","182ec5e4":"markdown","634a807f":"markdown","9c9bc8bd":"markdown","018df4a2":"markdown","95fcd678":"markdown","aaa902a8":"markdown","57c8265c":"markdown","c9e43082":"markdown","e6ad3a32":"markdown","c49c3f10":"markdown","10536bac":"markdown","da73531a":"markdown","a978e7c7":"markdown","17f63d0c":"markdown","2a963334":"markdown","95b6e820":"markdown","eeac523c":"markdown","a0e41e76":"markdown","d4ecacc8":"markdown"},"source":{"45de530d":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","1e247675":"# Importing Basic Datasets\n\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport warnings\nwarnings.filterwarnings('ignore')","3eac22b5":"# Importing the dataset\n\nbikes = pd.read_csv('\/kaggle\/input\/daydata\/day.csv')","6ab8f6ce":"# View the dataframe\n\nbikes.head()","d4b5aa72":"bikes.tail()","3248e989":"# Checking the shape of the dataframe\n\nbikes.shape","09c28772":"# Dropping the above variables\n\nbikes.drop(['instant', 'dteday'], axis=1, inplace=True)\nbikes.shape","4655a5b8":"# Dropping registered and casual\n\nbikes.drop(['registered', 'casual'], axis=1, inplace=True)\nbikes.shape","b15a0c94":"# Checking the dataframe for missing values\n\nbikes.isna().mean()*100","5cc10191":"# Checking the datatypes of the variables\n\nbikes.info()","98c42a65":"# Replacing these values with their string values\n\nbikes.season.replace((1,2,3,4), ('spring', 'summer', 'fall', 'winter'), inplace=True)\nbikes.season.value_counts()","57477878":"# Changing the dtype\n\nbikes.yr = bikes.yr.astype('category')","f4482c95":"# Verifying\n\nbikes.yr.dtypes","9f2c56f4":"# Replacing these values with their string values\n\nbikes.mnth.replace((1,2,3,4,5,6,7,8,9,10,11,12), ('jan', 'feb', 'mar', 'apr', 'may', 'jun', 'jul', 'aug', 'sep', 'oct', 'nov', 'dec'), inplace=True)\nbikes.mnth.value_counts()","f750fa8e":"# Replacing these values with their string values\n\nbikes.weekday.replace((0,1,2,3,4,5,6), ('sun', 'mon', 'tue', 'wed', 'thu', 'fri', 'sat'), inplace=True)\nbikes.weekday.value_counts()","3181c8b0":"# Replacing these values with their string values\n\nbikes.weathersit.replace((1,2,3,4), ('clear', 'misty', 'lrainsnow', 'hrainsnow'), inplace=True)\nbikes.weathersit.value_counts()","4ba973f8":"# Defining a function to return a mutli boxplot\n\ndef multi_boxplot(data, vars_list, y):\n    l = len(vars_list)\n    \n    # Number of columns is fixed\n    no_cols = 4\n    \n    # Number of rows calculated based on the length of the list\n    no_rows = int(np.ceil(l\/4))\n    \n    # Total height of the figure\n    h_row = 4*no_rows\n    \n    # Width of the figure\n    w_row = 16\n    \n    # Define the subplot and the figure size\n    figure, axes = plt.subplots(nrows=no_rows, ncols=no_cols, figsize=(w_row, h_row))\n    \n    # Define a list to contain all the axes indexes\n    axes_list = []\n    \n    # To get an indexes of the boxplots\n    for i in range(no_rows):\n        for j in range(no_cols):\n            axes_list.append([i,j])\n            \n    # To plot the figure\n    for i, t in enumerate(vars_list):\n        ax_val = axes_list[i]\n        a = ax_val[0]\n        b = ax_val[1]\n        sns.boxplot(ax = axes[a, b], data=data, x=t, y=y)\n        \n    plt.tight_layout()\n","10a5ab3e":"# Define a function to return categorical statistics\n\ndef cat_stats(col):\n    cat_df = bikes.groupby(col)['cnt'].agg(['sum', 'mean','count']).sort_values('sum',ascending = False)\n    \n    cat_df['perc_sum']=cat_df['sum']\/bikes.cnt.sum()*100\n    \n    return round(cat_df, 2)","07bb2fa0":"# Define a function to return plots for categorical variables\n\ndef cat_plots( col, x, y):\n    \n    # Defining the size of the plot\n    plt.figure(figsize = (x, y))\n    \n    # First Subplot\n    plt.subplot(1, 2, 1)\n    sns.barplot(x = col, y = 'cnt', data=bikes)\n    \n    # Second Subplot\n    plt.subplot(1, 2, 2)\n    sns.barplot(x = col, y = 'cnt', data = bikes, hue = 'yr', palette = 'cool')\n    \n    # Return\n    \n    return","5c34cac9":"# Running multi-boxplot on our categorical variables\n\nmulti_boxplot(bikes, ['season', 'yr', 'mnth', 'holiday', 'weekday', 'workingday', 'weathersit'], y='cnt')","6e4792f6":"# Descriptive Statistics\n\ncat_stats('season')","3938ad4a":"# Plots\n\ncat_plots('season', 10, 5)","c3111284":"# Descriptive Statistics\n\ncat_stats('mnth')","99072d8d":"# Plots\n\ncat_plots('mnth', 15, 6)","6b3efce7":"# Descriptive Statistics\n\ncat_stats('holiday')","6880f50d":"# Plots\n\ncat_plots('holiday', 10, 5)","d1cc8459":"# Descriptive Statistics\n\ncat_stats('weekday')","c99dabc1":"# Plots\n\ncat_plots('weekday', 15, 5)","ad7b3fa4":"# Descriptive Statistics\n\ncat_stats('workingday')","9acf1473":"# Plots\n\ncat_plots('workingday', 10, 5)","4b23fb50":"# Descriptive Statistics\n\ncat_stats('weathersit')","ce9bde33":"# Plots\n\ncat_plots('weathersit', 10, 5)","b5aadcdb":"# Extract a list of numerical variables\n\nnum_vars = ['temp', 'atemp', 'hum', 'windspeed', 'cnt']\nnum_vars","d003ea41":"# Generating a pairplot to see the relationships among Numerical Variables\n\nsns.pairplot(bikes[num_vars])\nplt.show()","6deaffc1":"# Generating a heatmap to see the correlations\n\nplt.figure(figsize=(15,8))\nsns.heatmap(bikes[num_vars].corr(), linewidth=1, annot=True, cmap='Dark2_r')\nplt.show()","d867ed50":"# To get an overview of all column names\n\nbikes.head()","9c314bae":"# season\n\nseason_d = pd.get_dummies(bikes['season'], drop_first = True)\nseason_d.head()","14448dd1":"# mnth\n\nmnth_d = pd.get_dummies(bikes['mnth'], drop_first = True)\nmnth_d.head()","34d4108a":"# weekday\n\nweekday_d = pd.get_dummies(bikes['weekday'], drop_first = True)\nweekday_d.head()","7fa3fffb":"# weathersit\n\nweathersit_d = pd.get_dummies(bikes['weathersit'], drop_first = True)\nweathersit_d.head()","ac6db556":"# Merging dataframes into a new dataframe called bikes_mod\n\nbikes_mod = pd.concat([bikes, season_d, mnth_d, weekday_d, weathersit_d], axis = 1)\nbikes_mod.head()","a21c5a71":"# Shape of bikes_mod\n\nbikes_mod.shape","7adcafff":"# Dropping the redundant columns\n\nbikes_mod.drop(['season', 'mnth', 'weekday', 'weathersit'], axis=1, inplace=True)\nbikes_mod.shape","5b885c33":"# Importing the packages required for this step\n\nfrom sklearn.model_selection import train_test_split","c1a00a19":"# Setting the seed\n\nnp.random.seed(0)","f18c6050":"# Performing the split\n\nbikes_train, bikes_test = train_test_split(bikes_mod, train_size = 0.7, test_size = 0.3, random_state = 100)","ed38af55":"# Verifying the split\n\n# Shape of training set\n\nbikes_train.shape","61dfd6c9":"# Description of training set\n\nbikes_train.describe()","1f42a47d":"# Shape of test set\n\nbikes_test.shape","79a98a0f":"# Description of test set\n\nbikes_test.describe()","7ee9d548":"# Importing the required package\n\nfrom sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()","7a3829cc":"# Applying scaler to al Numerical columns excluding the binary and dummy variables\n\nscale_list = ['temp', 'atemp', 'hum', 'windspeed', 'cnt']\n\nbikes_train[scale_list] = scaler.fit_transform(bikes_train[scale_list])","f73d43f8":"# Verifying the re-scaling\n\nbikes_train.head()","1a72526c":"bikes_train.describe()","d5050e4f":"# Plotting a heatmap for all the variables in training data\n\nplt.figure(figsize=(20, 20))\nsns.heatmap(round(bikes_train.corr(),2), annot=True, cmap='RdYlBu')\nplt.show()","41bbcf43":"# y_train\ny_train = bikes_train.pop('cnt')\n\n# X_train\nX_train = bikes_train","42fdd03a":"# Verification for y_train\n\ny_train.head()","aad7a7ab":"# Verification for X_train\n\nX_train.head()","f1f41636":"# Importing the required packages\n\nfrom sklearn.feature_selection import RFE\n\nfrom sklearn.linear_model import LinearRegression","bc518ed4":"# Instance of the LinearRegression object\nlm = LinearRegression()\n\n# Fitting the line\nlm.fit(X_train, y_train)\n\n# Running RFE\nrfe = RFE(lm, 20)\nrfe = rfe.fit(X_train, y_train)","52d94c8f":"# Checking parameters that are selected\n\nlist(zip(X_train.columns, rfe.support_, rfe.ranking_))","b66a6887":"# Storing 15 selected variables into a list \n\nsel = X_train.columns[rfe.support_]\nsel","41ef1cb4":"# Which columns has RFE eliminated\n\nX_train.columns[~rfe.support_]","7f232836":"# Create a new dataframe with the selected columns\n\nX_train_rfe = X_train[sel]\nX_train_rfe.head()","3a0bdd40":"# Import required packages\n\n# Importing VIF\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\n# Importing statsmodels as sm\nimport statsmodels.api as sm","a9b48ce8":"# Define a function to calculate VIF\n\ndef cal_vif(train_df):\n    vif = pd.DataFrame()\n    vif['Features'] = train_df.columns\n    vif['VIF'] = [variance_inflation_factor(train_df.values, i) for i in range(train_df.shape[1])]\n    vif['VIF'] = round(vif['VIF'], 2)\n    vif = vif.sort_values(by = \"VIF\", ascending = False)\n    \n    return vif","1f6295c1":"# The train set for first model\n\nX_train0 = X_train_rfe\nX_train0.columns","7c264c6b":"# VIF values of X_train0\n\ncal_vif(X_train0)","41fef81d":"# Creating a new dataframe called X_train_lm0 which we will use to perform the Fine Tuning\n\nX_train_lm0 = sm.add_constant(X_train0)\nX_train_lm0.head(1)","7ddff48b":"# Fitting the model\n\nlr0 = sm.OLS(y_train, X_train_lm0).fit()","92336b1d":"# Summary\n\nlr0.summary()","7fdcdaac":"# Dropping atemp to get X_train1\n\nX_train1 = X_train0.drop(['atemp'], axis=1)\nX_train1.head(1)","df83b099":"# VIF values of X_train1\n\ncal_vif(X_train1)","a3d81f1d":"# Adding constant to get X_train_lm1\n\nX_train_lm1 = sm.add_constant(X_train1)\nX_train_lm1.head(1)","2c4e2718":"# Fitting the model\n\nlr1 = sm.OLS(y_train, X_train_lm1).fit()","83026057":"# Summary\n\nlr1.summary()","8e084e31":"# Dropping workingday to get X_train2\n\nX_train2 = X_train1.drop(['workingday'], axis=1)\nX_train2.head(1)","30c65216":"# VIF values of X_train2\n\ncal_vif(X_train2)","240797e1":"# Adding a constant to get X_train_lm2\n\nX_train_lm2 = sm.add_constant(X_train2)\nX_train_lm2.head(1)","16a477b8":"# Fitting the model\n\nlr2 = sm.OLS(y_train, X_train_lm2).fit()","5379ef41":"# Summary\n\nlr2.summary()","ea351c8a":"# Dropping sat to get X_train3\n\nX_train3 = X_train2.drop(['sat'], axis=1)\nX_train3.head(1)","38ceae96":"# VIF values for X_train3\n\ncal_vif(X_train3)","30eed8db":"# Adding constant to get X_train_lm3\n\nX_train_lm3 = sm.add_constant(X_train3)\nX_train_lm3.head(1)","6000f4a1":"# Fitting the model\n\nlr3 = sm.OLS(y_train, X_train_lm3).fit()","523ec6d2":"# Summary\n\nlr3.summary()","8b4aeace":"# Dropping hum to get X_train4\n\nX_train4 = X_train3.drop(['hum'], axis = 1)\nX_train4.head(1)","988f5584":"# VIF values for X_train4\n\ncal_vif(X_train4)","d8b812dd":"# Adding constant to get X_train_lm4\n\nX_train_lm4 = sm.add_constant(X_train4)\nX_train_lm4.head(1)","2d85521a":"# Fitting the model\n\nlr4 = sm.OLS(y_train, X_train_lm4).fit()","649f3a1e":"# Summary\n\nlr4.summary()","7260dc84":"# Dropping feb to get X_train5\n\nX_train5 = X_train4.drop(['feb'], axis = 1)\nX_train5.head(1)","34fd80e9":"# VIF values for X_train5\n\ncal_vif(X_train5)","2d412fd6":"# Adding constant to get X_train_lm5\n\nX_train_lm5 = sm.add_constant(X_train5)\nX_train_lm5.head(1)","fc9ad8d2":"# Fitting the model\n\nlr5 = sm.OLS(y_train, X_train_lm5).fit()","0c62f371":"# Summary\n\nlr5.summary()","519b7e1b":"# Dropping sun to get X_train6\n\nX_train6 = X_train5.drop(['sun'], axis=1)\nX_train6.head(1)","43c121dc":"# VIF values for X_train6\n\ncal_vif(X_train6)","4d71a35d":"# Adding constant to get X_train_lm6\n\nX_train_lm6 = sm.add_constant(X_train6)\nX_train_lm6.head(1)","2701c514":"# Fitting the model\n\nlr6 = sm.OLS(y_train, X_train_lm6).fit()","6774466c":"# Summary\n\nlr6.summary()","a6621101":"# Heatmap for remaining variables\n\nplt.figure(figsize=(15, 10))\nsns.heatmap(X_train6.corr(), annot=True, cmap='BuGn')\nplt.show()","6a758ef4":"# Dropping jul to get X_train7\n\nX_train7 = X_train6.drop(['jul'], axis=1)\nX_train7.head(1)","8ad3f179":"# VIF values of X_train7\n\ncal_vif(X_train7)","a614510b":"# Adding a constant to get X_train_lm7\n\nX_train_lm7 = sm.add_constant(X_train7)\nX_train_lm7.head(1)","a9558252":"# Fitting the model\n\nlr7 = sm.OLS(y_train, X_train_lm7).fit()","c2224b4a":"# Summary\n\nlr7.summary()","4837761a":"lr7.params","3011e85e":"# y_train_pred is the values predicted using our model\n\ny_train_pred = lr7.predict(X_train_lm7)\ny_train_pred","794ad20d":"# Plotting Residuals vs Temp\n\nsm.graphics.plot_ccpr(lr7, 'temp')\nplt.show()","1f08e677":"# Plotting Resiudals vs lrainsow\n\nsm.graphics.plot_ccpr(lr7, 'windspeed')\nplt.show()","8749b2d1":"# Checking y_train_pred\n\ny_train_pred","9964a8cb":"# Calculating the residuals\n\nres = y_train - y_train_pred\nres.head()","9aa787d7":"# Plotting the residuals\n\nfig = plt.figure()\nsns.distplot(res)\nfig.suptitle('Error Terms')                  \nplt.xlabel('Errors')                         \nplt.show()","5fa38b29":"# Plotting a Normal Q-Q plot\n\nsm.qqplot(res, fit=True, line='45')\nplt.show()","78b0ffff":"# Durbin-Watson value for Final Model\n\nround(sm.stats.stattools.durbin_watson((y_train - y_train_pred)),4)","837c6bde":"# Plotting the scatter plot\n\nplt.figure(figsize=(10,5))\nsns.scatterplot(y_train_pred, res)\nplt.xlabel('Predicted Value')\nplt.ylabel('Residual')\nplt.show()","13f681a7":"# Checking our test set\n\nbikes_test.shape","c999106b":"# Checking columns in test set\n\nbikes_test.columns","fe6454a3":"# Define the numerical variables\n\nnum_vars = ['temp', 'atemp', 'hum', 'windspeed', 'cnt']","f69c8bc4":"# Scaling\n\nbikes_test[num_vars] = scaler.transform(bikes_test[num_vars])\nbikes_test.head()","cae78516":"# Verifying\n\nbikes_test.describe()","c1c0c2cd":"# Separating\n\ny_test = bikes_test.pop('cnt')\nX_test = bikes_test","1ab45d5d":"# Columns in final model\n\ncol = X_train7.columns\ncol","122e3894":"# Re-defining X_test to contain only these columns\n\nX_test = X_test[col]\nX_test.head()","bca353a2":"# Adding constant to get X_test_lm7\n\nX_test_lm7 = sm.add_constant(X_test)\nX_test_lm7.head(1)","0c7fb4ff":"# Obtaining y_pred\n\ny_pred = lr7.predict(X_test_lm7)\ny_pred.head()","7b23fb5d":"# The plot\n\nfig = plt.figure(figsize=(10,6))\nplt.scatter(y_test, y_pred)\nfig.suptitle('y_test vs y_pred', fontsize = 16)               \nplt.xlabel('y_test', fontsize = 14)                          \nplt.ylabel('y_pred', fontsize = 14)   \nplt.show()","8f414b67":"# Importing the required package\n \nfrom sklearn.metrics import r2_score","c2fa7525":"# Rsquared value for Test Set\n\nr2test = round(r2_score(y_test, y_pred),4)\nr2test","aa392208":"# Rsquared value for Test Set\n\nr2train  = round(r2_score(y_train, y_train_pred),4)\nr2train ","f7cf8576":"# Adjusted Rsquared value for Test Set\n\n# Number of rows in test dataset is n\n\nn = X_test.shape[0]\n\n# Number of predictors is the shape along axis 1\n\np = X_test.shape[1]\n\n# Find the Adjusted R-squared using the formula\n\nadjusted_r2 = round(1-(1-r2test)*(n-1)\/(n-p-1),4)\nadjusted_r2","61aa6942":"%%html\n<style>\ntable {float: left}\n<\/style>","1d56b655":"# Importing the required package \n\nimport math\nfrom sklearn.metrics import mean_squared_error","6ac8d053":"# Calculating RMSE\n\nRMSE = round(math.sqrt(mean_squared_error(y_test, y_pred)),4)\nRMSE","1a9c9774":"# To get an overview of the coeffecients\n\nlr7.params.sort_values(ascending=False)","ad1495a9":"### 9.4 Adding constant to X_test to get X_test_lm7","6d3928a1":"#### Observations\nThe VIF and p values are all in the accepted range. The model looks good. There is very low multicollinearity between the predictor variables and the p values for all the predictors is significant.<br>\nFor now, we will consider this as our final model (Unless our test data doesn't yield similar results)\n","0ac6b77f":"<b>Adjusted Rsquared value for Training Set: (As shown in our summary table for lr7)<\/b>\n\nAdj. R-squared for Training Set:\t`0.833` <br>\nAdj. R-squared for Test Set: `0.7997`","8f54e9a9":"### 9.1 Re-scaling the Test Data\nWe will apply `scaler()` to all the numerical variables in the dataset. We will only use scaler.transform, as we want to use the metrics that the model learned from the training data to be applied on the test data. In other words, we want to prevent the information leak from train to test dataset","c611abfe":"### As per our Final Model, the top predictors that influence the bike bookings are:\n- `temp` : A coeffecient value of 0.42 indicates that the temperature has a significant impact on the bike rentals. Higher temperatures see more people renting bikes.\n- `yr` : A coeffecient of 0.23 indicatest that the bike bookings are increasing each year. However, we now know that 2020 would have seen sharp decline in the demand for bikes because of the Covif-19 pandemic. Our model could not have foretold that.\n- `lrainsnow` : A coeffecient of -0.29 indicates that light rain or snow deter people from renting bikes. \n- `windspeed` : A coeffecient of -0.15 indicatese that high wind speeds also deter people from renting bikes.","39cfb0bb":"#### Observations\n- `sun` has a high p value\n\n#### Decision\n`Drop sun`","8450c3cd":"There are 730 rows and 16 columns. <br> <br>\n### What are the different variables present?\n- <b>instant<\/b>: The index of the particular record\/row\n- <b>dteday<\/b>: The date\n- <b>season<\/b>: Seasons. There are 4 categories.\n    - 1 : Spring\n    - 2 : Summer \n    - 3 : Fall \n    - 4: Winter\n- <b>yr<\/b>: Year. We have data for only 2 years - 2018 and 2019.\n    - 0 : 2018\n    - 1 : 2019\n- <b>mnth<\/b>: Month. Values range from 1-12.\n- <b>holiday<\/b>: Weather the day is a holiday.\n    - 0 : Not a Holiday\n    - 1 : Holiday\n- <b>weekday<\/b>: Day of the week. Values range from 0-6\n- <b>workingday<\/b>: If the day is working.\n    - 0 : Neither weekend nor holiday\n    - 1 : Not a working day\n- <b>weathersit<\/b>: The weather on that particular day\n    - 1 : Clear, Few clouds, Partly cloudy, Partly cloudy\n\t- 2 : Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist\n\t- 3 : Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds\n\t- 4 : Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow + Fog\n- <b>temp<\/b>: Temperature in Celsius\n- <b>atemp<\/b>: Feeling temperature in Celsius\n- <b>hum<\/b>: Humidity (in Percentage)\n- <b>windspeed<\/b>: Wind speed\n- <b>casual<\/b>: Count of casual users. Casual users are users who use the service occasionally or don't have a subscription to the service.\n- <b>registered<\/b>: Count of registered users. Registered users are those who register for an annual or 30-day membership and rent more frequently\n- <b>cnt<\/b>: `TARGET` Count of total rental bikes including both casual and registered\n","97b86ee3":"#### To calculate Adjusted Rsquared: <br> <br>\n\n$ R^{2}_{adj} = 1 - \\frac{(1-R^{2})*(n-1)}{(n-p-1)}$","9e7747d4":"#### Observations\n- temp and atemp have the highest VIF values. They are followed by workingday and hum.\n- atemp has the highest p value. sun, sat and workingday also have high p values.\n\n#### Decsision\nSince atemp has a high p value and high VIF value <br>\n`Drop atemp`","4481d3a7":"### 6.3.4 Model 4\n- X_train4 = ['yr', 'holiday', 'temp', 'windspeed', 'spring', 'summer', 'winter', 'dec', 'feb', 'jan', 'jul', 'nov', 'sep', 'sun', 'lrainsnow', 'misty'] \n- X_train_lm4 is the df with the constant\n- lr4 is the linear model","6f470ee8":"Our model has a DW value of `2.0467`. <br>There is almost no auto-correlation. Therefore this suggests that our residuals are indeed independant of each other. <br> <br>\n<b>`Error terms are independant of each other`<\/b>","9f737daa":"We can see that linearity is well preserved in the above plots between the model and predictor variables. <br> <br>\n<b>`There is a Linear Relationship between X and y` <\/b>","c6725e28":"## 6. Building a Linear Model","be36bbe4":"### 8.3 Independance of Error Terms \nThe best test for serial correlation is to look at a residual time series plot (residuals vs. row number) and a table or plot of residual autocorrelations. The Durbin-Watson statistic provides a test for significant residual autocorrelation at lag 1: the DW stat is approximately equal to 2(1-a) where a is the lag-1 residual autocorrelation, so ideally it should be close to 2.0--say, between 1.4 and 2.6 for a sample size of 50. The closer it is 2, the less the auto-correlation between the various variables.","cb412879":"#### Observations\n- The perc_sum for all the weekdays lie in a similar range (13.5% - 14.8%) and all their medians lie between 4000 and 5000.\n- This could indicate that `weekday` could have less or no influence on the dependant variable. ","b151f24c":"### Possible Improvements\n- If we had additional variables describing the `number of the bike rentals by regions`, we could further derive insights about which regions have the highest demand and subsequently take decisions to maximize rentals there.\n- If we had a variables describing the `number of bike rentals by age`, we could derivce insights about our ideal customer profile. We could  identify which age groups use our services the most and then take decisions to improve rentals for the same.","15b52f1c":"#### Fine Tuning","1fb6b9da":"### 6.2 Fitting the Regression Line\nI will be using the `LinearRegression function from SciKit Learn` for it's compatibility with RFE (Recursive Feature Elimination) <br>\nRecursive feature elimination (RFE) is a feature selection method that fits a model and removes the weakest feature (or features) until the specified number of features is reached. Features are ranked by the model\u2019s coef_ or feature_importances_ attributes, and by recursively eliminating a small number of features per loop, RFE attempts to eliminate dependencies and collinearity that may exist in the model.","f8fa5ba7":"#### Observations:\n- `temp` and `atemp` are highly correlated. We cannot use both in our model.\n- `temp` and `cnt` have a positive correlation\n- `cnt` and `temp` have a positive correlation\n- `yr` and `cnt` have a positive correlation\n- `humidity` and `misty` have a positive correlation\n- `spring` and `cnt` have a negative correlation\n- `workingday` and `sat`, `sunday` have a negative correlation\n- `temperatures` and `weather conditions` have correlations with different months","6cf36319":"#### Observations\n- `temp` has a slightly higher VIF value. This is an important predictor variable so we can't afford to drop this. <br> <br>\n\nI will use a heatmap to see which variable still shows a high correlation with `temp` and check if dropping that variable will help reduce the VIF value.","6fdcf39c":"We see that the error terms are normally distributed. <br>\nAdditionally, we can use a Q-Q plot to check if the error terms are normally distributed. If all the points plotted on the graph perfectly lies on a straight line then we can clearly say that this distribution is Normally distribution. (I have elaborated on this concept in the Subjective Questions)","c642aea3":"The mean squared error is `0.0946` which is a very small value in comparison to the range of our dependant variable. This indicates that our model has good fit.","33a3e072":"We can see that there is no clear pattern in the distribution. This affirrms that our error terms have constant variance at different values of X. <br> <br>\n<b>`Error terms are homoscedastic`<\/b>","9d2d302a":"### 6.3.1 Model 1\n- X_train1 =  ['yr', 'holiday', 'workingday', 'temp', 'hum', 'windspeed', 'spring', 'summer', 'winter', 'dec', 'feb', 'jan', 'jul', 'nov', 'sep', 'sat', 'sun', 'lrainsnow', 'misty'] \n- X_train_lm1 is the df with the constant\n- lr1 is the linear model","d245659b":"### 10.1 Plotting y_test vs y_pred to understand the spread","76bd465b":"#### Observations\n- Fall experieces the highest percentage of bookings at `32%` with a median value of above 5000 bookings (for 2 years) \n- Summer and Winter follow with `28%` and `25%` of the total bookings respectively. \n- 2019 had a higher number of bookings as compared to 2018. This suggests that `yr` also can be a good predictor for the dependant variable.\n- This suggests that `season` can be a good predictor of the dependant variable","3ebe5a35":"#### Observations\n- 97% of the bike rentals have occured during working days. This indicates that most of our rentals occur during working days.\n- This definitely suggests that `holiday` is a good predictor for the dependant variable.","eaf5b4ec":"### 6.1 Separating bikes_train into X_train and y_train","522ee66e":"### 4.1 Creating Dummy Variables for Categorical Variables\n- season\n- mnth\n- weekday\n- weathersit","a5388112":"## 9. Making Predictions using Final Model\nNow that we have fitted the model and verified the assumptions of Linear Regression, it's time to go ahead and make predictions using the final, i.e. sevent model - lr7","51693c6c":"We have bikes_train and bikes_test","99a9c34a":"#### Observations:\n- `hum` and `temp` have high VIF values\n- `sun` and `feb` have high p values\n\n#### Decision\nSince `hum` has such a high VIF value <br>\n`Drop hum`","6059a73a":"### 8.4 Homoscedasticity - Error terms have constant variance at different values of X\nA scatterplot of residuals versus predicted values is good way to check for homoscedasticity.  There should be no clear pattern in the distribution; if there is a cone-shaped pattern the data is heteroscedastic.","477b602f":"## 10. Model Evaluation","4ab09c15":"### 6.3.7 Model 7\n- X_train7 = ['yr', 'holiday', 'temp', 'windspeed', 'spring', 'summer', 'winter', 'dec', 'jan', 'nov', 'sep', 'lrainsnow', 'misty'] \n- X_train_lm7 is the df with the constant\n- lr7 is the linear model`","dee08709":"## 2. Data Quality Check","471b00e5":"### Comparing Model Train and Test Rsquared Statistics\n\n<table>\n<thead>\n<tr>\n<th>Measurement<\/th>\n<th>Train Dataset<\/th>\n<th>Test Dataset<\/th>\n<\/tr>\n<\/thead>\n<tbody>\n<tr>\n<td>$R^{2}$<\/td>\n<td>81%<\/td>\n<td>83%<\/td>\n<\/tr>\n<tr>\n<td>$ R^{2}_{adj}$<\/td>\n<td>79%<\/td>\n<td>83%<\/td>\n<\/tr>\n<\/tbody>\n<\/table>","ae64465c":"### yr\nChange it to category type as it is a binary variables","6bb315b2":"### 7.2 Rsquared\nThe Rsquared is a measure of how close the data is to the fitted line. A Rsqaured value of 100% indicates that the model is able to explain 100% of the variance in the data <br>\nOur model has a Rsquared of `83.7` which tells us that the our model can explain 83.7% of the variance in our training data.","39a59716":"### `weekday`","d44b820c":"There are categorical variables that are currently numerical in nature. Some of the variables like 'weathersit' and 'season' have values as 1, 2, 3, 4 which have specific labels associated with them. These numeric values associated with the labels may indicate that there is some order to them - which is actually not the case. Therefore, <br>\n`I will convert such feature values into categorical string values before proceeding further.`","ccd6fd86":"In `statsmodels` we need to explicitly fit a constant using sm.add_constant(X) because if we don't, `statsmodels` fits a regression line passing  through the origin, by default.","22b07e7f":"Why random_state ? <br>\nThis is to check and validate the data when running the code multiple times. Setting random_state a fixed value will guarantee that the same sequence of random numbers is generated each time you run the code.","44e432e9":"<b>The dataframe has no missing values.<\/b>","37ba5573":"### 10.3 Calculating RMSE \n`Root Mean Square Error (RMSE)` is the standard deviation of the residuals (prediction errors). Residuals are a measure of how far from the regression line data points are; RMSE is a measure of how spread out these residuals are. In other words, it tells you how concentrated the data is around the line of best fit. <br>\nLower values of RMSE indicate better fit. ","8d848459":"## 11. Conclusion\n\nOur Final Model is: <br>\n`cnt = 0.239423 + (0.234878*yr) + (-0.090839*holiday) + (0.423611*temp) + (-0.159255*windspeed) + (-0.059974*spring) + (0.048122*summer) + (0.100637*winter) + (-0.045116*dec) + (-0.052486*jan) + (-0.041664*nov) + (0.081832*sep) + (-0.293808*lrainsnow) + (-0.080015*misty)`","db003dab":"#### Observations\n- `hum` has the highest VIF value. `temp` and `spring` also have high VIF values.\n- `sat`, `feb` and `sun` have high p values\n\n#### Decision\nSince `sat` has the highest p value <br>\n`Drop sat`","122eb4cb":"#### Observations\n- 68% of all our bookings occured on clear days with a median value of almost 5000 bookings.\n- This is followed by mist days accounting for 30% of all bookings with a median value of 4000 bookings.\n- This suggestst that `weathersit` is a good predictor for our dependant variables","f43c6e12":"## 7. Final Model Interpretation","8ecb4ae6":"### 10.2 Comparing the Rsquared values & Adjusted Rsquared values","14b4c7f9":"### 9.2 Separating bikes_test to X_test and y_test","d9481681":"### `mnth`","1cc089dc":"### cnt, registered and casual\nWe know that `cnt = registered + casual`. Our objective is to predict the total count of bikes and not by specific category. Therefore we can drop registered and casual. <br>\n<b>Dropping `registered and casual`<\/b>","08e08f5b":"### 5.2 Rescaling the Data in `Training Data`\nScaling or Feature Scaling as it is more commonly known, is a method to standardize the independant variables present in the data and bring them within a fixed range. This is usually done in the data pre-processing stage to deal with data that is in highly varying magnitudes or units.","bdcb4ba2":"### Suggestions based on our Model\nI would recommend giving due importance to these four variables while devising a business plan to maximize rental booking. <br>\n- I have observed that higher temperatures and favourable weather conditions have a positive effect on bike rentals. Therefore, I would recommend increasing availability and promotions during the summer months to further increase bike rentals. ","f83af74a":"#### Observations\n- `temp` has a high correlation with `jul`\n\n#### Decision\nBecause of the high correlation with temp <br>\n`Drop jul`","52c4623f":"### weekday\nValues range from 0-6.","6b6e5c7a":"### 5.3 Checking Correlation Coeffecients","b34bc99c":"### 7.3 F Statistic\nThe F Statistic is an indicator of the overall significance of the model. The F-test of overall significance indicates whether your linear regression model provides a better fit to the data than a model that contains no independent variables. Here the null hypothesis all of the regression coefficients are equal to zero. <br>\n\nThe value of Prob(F) is the probability that the null hypothesis for the full model is true (i.e., that all of the regression coefficients are zero).  For example, if Prob(F) has a value of 0.01000 then there is 1 chance in 100 that all of the regression parameters are zero.  This low a value would imply that at least some of the regression parameters are nonzero and that the regression equation does have some validity in fitting the data. <br>\n<br>\n\nOur model has a F statistic of `196.6` which tells us that the overall model is highly significant. <br>\nOur model also has a very low Prob(F) of `4.31e-186` which indicates that our regression equation does have some validity in fitting the data.","109a172d":"### Do I need all these variables?\nI have judged the following variables to be redundant. The reason for doing so is also mentioned.\n- <b>instant<\/b>: The instant is just the index of the row and hence won't help us in the process of modelling.\n- <b>dteday<\/b>: This is the date of the instance and won't be of any help in the process of modelling. We have variables like mnth and yr which will suffice.\n\n<b>Dropping `instant` and `dteday`<\/b>","14d5a52a":"### 8.2 To check if Error terms are Normally Distributed\nThe best test for normally distributed errors is a normal probability plot or normal quantile plot of the residuals. These are plots of the fractiles of error distribution versus the fractiles of a normal distribution having the same mean and variance. If the distribution is normal, the points on such a plot should fall close to the diagonal reference line.","3bd55d4f":"### 6.3.2 Model 2\n- X_train2 = ['yr', 'holiday', 'temp', 'hum', 'windspeed', 'spring', 'summer', 'winter', 'dec', 'feb', 'jan', 'jul', 'nov', 'sep', 'sat', 'sun', 'lrainsnow', 'misty'] \n- X_train_lm2 is the df with the constant\n- lr2 is the linear model","b83d2289":"What characteristics will we judge our model on? <br>\n- Coeffecients: The coeffecients indicate the effect the variables have on the dependant variable.\n- P-value: We use hypothesis testing to understand the significance of the coeffecients. A value less than 0.05 is ideal.\n- VIF: A variance inflation factor(VIF) detects multicollinearity in regression analysis. Multicollinearity is when there\u2019s correlation between predictors (i.e. independent variables) in a model; it\u2019s presence can adversely affect your regression results. The VIF estimates how much the variance of a regression coefficient is inflated due to multicollinearity in the model. A value under 5 is acceptable.","42c6df26":"### 5.1 Splitting the Data","f99da59a":"## Categorical Variable Analysis\nThe following variables:\n- season\n- yr\n- mnth\n- holiday\n- weekday\n- workingday\n- weathersit","aa80ef91":"## 4. Data Preparation","69e9eb20":"#### Observations\n- The VIF values have dropped to acceptables levels\n- `feb` and `sun` have high p values\n\n#### Decision\nSince `feb` has the highest p value <br>\n`Drop feb`","3f6eacfa":"### weathersit\nThe weather on that particular day\n- 1 : `clear` Clear, Few clouds, Partly cloudy, Partly cloudy\n- 2 : `misty` Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist\n- 3 : `lrainsnow` Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds\n- 4 : `hrainsnow` Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow + Fog","49700e59":"The points plotted on the graph do in fact fall on the straight line. This further confirms that the error terms are normally distributed.\n\n#### `Residuals are Normally Distributed`","b8a5ceb9":"### 6.3.3 Model 3\n- X_train3 = ['yr', 'holiday', 'temp', 'hum', 'windspeed', 'spring', 'summer', 'winter', 'dec', 'feb', 'jan', 'jul', 'nov', 'sep', 'sun', 'lrainsnow', 'misty'] \n- X_train_lm3 is the df with the constant\n- lr3 is the linear model","180dcb1f":"#### Observations\n- Both `temp` and `atemp` have a high correlation with `cnt`. \n- `temp` and `atemp` exhibit a strong linear relationship. We cannot use both these variables in our model as the multicollinearity will make it more difficult to specify the correct model.\n","0ca32ce1":"### `weathersit`","dc79212b":"### 6.3.0 Model 0\n- X_train0 = ['yr', 'holiday', 'workingday', 'temp', 'atemp', 'hum', 'windspeed', 'spring', 'summer', 'winter', 'dec', 'feb', 'jan', 'jul', 'nov', 'sep', 'sat', 'sun', 'lrainsnow', 'misty']\n- X_train_lm0 is the df with the constant\n- lr0 is the linear model","547cb207":"### 6.3.6 Model 6\n- X_train6 = ['yr', 'holiday', 'temp', 'windspeed', 'spring', 'summer', 'winter', 'dec', 'jan', 'jul', 'nov', 'sep', 'lrainsnow', 'misty'] \n- X_train_lm6 is the df with the constant\n- lr6 is the linear model`","7fe24514":"### 7.1 Coeffecients\nThe equation of the best fitted model is: <br>\n`cnt = 0.239423 + (0.234878*yr) + (-0.090839*holiday) + (0.423611*temp) + (-0.159255*windspeed) + (-0.059974*spring) + (0.048122*summer) + (0.100637*winter) + (-0.045116*dec) + (-0.052486*jan) + (-0.041664*nov) + (0.081832*sep) + (-0.293808*lrainsnow) + (-0.080015*misty)`\n<br><br>\nThe coeffecients do represent the relation I observed intuitively in the scatter plot.","d8a4316b":"#### Observations\n- 68% of all our rentals occured on working day. \n- Both working and non-working days had a median close to 5000 bookings (for 2 years)\n- This suggests that `workingday` could be a good predictor of the dependant variable","880fc6a8":"### 9.3 Selecting variables in X_test that were part of our final model","182ec5e4":"### 9.5 Making predictions using Final Model","634a807f":"# Bike Sharing Assignment\n\n### Problem Statement:\n\n#### About the System:\nA bike-sharing system is a service in which bikes are made available for shared use to individuals on a short term basis for a price or free. Many bike share systems allow people to borrow a bike from a \"dock\" which is usually computer-controlled wherein the user enters the payment information, and the system unlocks it. This bike can then be returned to another dock belonging to the same system.\n\n#### Their reasons for approaching us:\nA US bike-sharing provider BoomBikes has recently suffered considerable dips in their revenues due to the ongoing Corona pandemic. The company is finding it very difficult to sustain in the current market scenario. So, it has decided to come up with a mindful business plan to be able to accelerate its revenue as soon as the ongoing lockdown comes to an end, and the economy restores to a healthy state. \n\n\nIn such an attempt, BoomBikes aspires to understand the demand for shared bikes among the people after this ongoing quarantine situation ends across the nation due to Covid-19. They have planned this to prepare themselves to cater to the people's needs once the situation gets better all around and stand out from other service providers and make huge profits.\n\n\nThey have contracted a consulting company to understand the factors on which the demand for these shared bikes depends. Specifically, they want to understand the factors affecting the demand for these shared bikes in the American market. The company wants to know:\n\n- Which variables are significant in predicting the demand for shared bikes.\n- How well those variables describe the bike demands\n\n#### How have they collected data?\nBased on various meteorological surveys and people's styles, the service provider firm has gathered a large dataset on daily bike demands across the American market based on some factors. \n\n### The Business Problem\nYou are required to model the demand for shared bikes with the available independent variables. It will be used by the management to understand how exactly the demands vary with different features. They can accordingly manipulate the business strategy to meet the demand levels and meet the customer's expectations. Further, the model will be a good way for management to understand the demand dynamics of a new market. ","9c9bc8bd":"### 6.3.5 Model 5\n- X_train5 = ['yr', 'holiday', 'temp', 'windspeed', 'spring', 'summer', 'winter', 'dec', 'jan', 'jul', 'nov', 'sep', 'sun', 'lrainsnow', 'misty'] \n- X_train_lm5 is the df with the constant\n- lr5 is the linear model","018df4a2":"### 8.1 To check if there is a Linear Relationship between X and y\nIn multiple regression models, nonlinearity or nonadditivity may be revealed by systematic patterns in plots of the residuals versus individual independent variables. The absence of such features affirms the linear relationship between X and y.","95fcd678":"### season\nSeasons. There are 4 categories. \n- 1 : Spring\n- 2 : Summer \n- 3 : Fall \n- 4: Winter","aaa902a8":"## 3. Data Analysis ","57c8265c":"## 5. Data Splitting and Preparation","c9e43082":"### `holiday`","e6ad3a32":"### by `Aaron Mathew Alex`","c49c3f10":"### `workingday`","10536bac":"## Numerical Variable Analysis\nThe following variables:\n- temp\n- atemp\n- hum\n- windspeed\n- cnt","da73531a":"Why np.random.seed ? <br>\n(pseudo-)Random numbers work by starting with a number (the seed), multiplying it by a large number, adding an offset, then taking modulo of that sum. The resulting number is then used as the seed to generate the next \"random\" number. When you set the seed (every time), it does the same thing every time, giving you the same numbers.","a978e7c7":"## 1. Data Exploration - Understanding the Data\n","17f63d0c":"#### Observations\n- The high VIF values we observed earlier have dropped significantly\n- `workingday`, `hum` and `temp` still have high VIF values\n\n- `sun` has the highest p value\n- `sat` and `workingday` also have high p values\n\n#### Decisions\nSince `workingday` has a high VIF value as well as a high p value <br>\n`Drop workingday`","2a963334":"### `season`","95b6e820":"#### Coarse Tuning","eeac523c":"#### Observations\n- The preiod from May-October witnessed the highest number of bookings. Each month of this period contributes to 10% or more of the total bookings. Their median values are also above 4000 (for 2 years)\n- This suggests that `mnth` could be a good predictor for our dependant variable","a0e41e76":"## 8. Residual Analysis of Training Data\nThere are certain assumptions related to Linear Regression which we need to verify. <br>","d4ecacc8":"### mnth\nMonth. Values in a range from 1-12"}}