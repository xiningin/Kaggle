{"cell_type":{"334a5d8f":"code","3b9f0708":"code","18a64ef9":"code","9fc32798":"code","3b476970":"code","c682af9c":"code","ec07db97":"code","1171f0c0":"code","0ea1ea0b":"code","00da18c4":"code","2a52c491":"code","c6f33859":"code","a92d52ad":"code","35be755a":"markdown","f94c35be":"markdown","484e4007":"markdown","b2e86faf":"markdown","26c88b2a":"markdown","7d67a401":"markdown","d7a1bbd2":"markdown","586ad38e":"markdown","3ab10385":"markdown","239d7522":"markdown","6e7c26a9":"markdown","ee2e01aa":"markdown"},"source":{"334a5d8f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","3b9f0708":"import pandas as pd \nimport numpy as np \nimport matplotlib as mpl\nimport matplotlib.pyplot as plt \nfrom matplotlib.ticker import FuncFormatter\nimport seaborn as sns \n%matplotlib inline\n\nplt.rcParams['figure.dpi'] = 200 #high resolution","18a64ef9":"\"\"\"Data Preparation: Read&Explore\"\"\"\n\n# df_raw = pd.read_csv(r\"C:\\Users\\suhon\\Documents\\Data Scientist Nanodegree\\KaggleSurvey2020-Analysis\\kaggle-survey-2020\\kaggle_survey_2020_responses.csv\") # local \ndf_raw = pd.read_csv(\"\/kaggle\/input\/kaggle-survey-2020\/kaggle_survey_2020_responses.csv\") # kaggle notebook \n\n# check the basic information \nprint(df_raw.info())\n\n# check the number of single choice questions\ncolumns_single = df_raw.filter(regex=\"^(?!.*Part)(?!.*OTHER)\").columns\n#df_raw[columns_single].describe()\nprint(f\"\\nNumber of single choice questions: {columns_single.shape[0]}\") \n\n# check the number of multiple choice questions\ncolumns_multiple = df_raw.filter(regex=\"^(?=.*Part)|(?=.*OTHER)\").columns\n#df_raw[columns_multiple].describe()\nprint(f\"\\nNumber of multiple choice questions: {columns_multiple.shape[0]}\")\n\n\ndf_raw.head()","9fc32798":"\"\"\"Data Preparation: Formatting\"\"\"\n\n# delete the first row and save it into the list \nlist_questions = list(df_raw.iloc[0])\ndf0 = df_raw.drop(df_raw.index[0], axis=0)\n\n# delete the first column \ndf0 = df0.drop(columns=df0.columns[0], axis=1)\n\n# only include current data professionals\nq5_order = [\n    'Research Scientist',\n    'Machine Learning Engineer',\n    'Data Scientist',\n    'Data Analyst', \n    'Data Engineer',\n    'DBA\/Database Engineer',\n    'Software Engineer',\n    'Statistician',\n    'Business Analyst',\n    'Product\/Project Manager',\n    # 'Student', \n    # 'Currently not employed', \n    'Other']\n\n# only include developers in North America: US and Canada \nq3_order = [\n    'United States of America', \n    'Canada']\n\n# DF: drop the unnecessary observations \ndf = df0[df0['Q5'].isin(q5_order) & df0['Q3'].isin(q3_order)]\n\n# DF_STUDENT: only include current students\ndf_std = df0[df0['Q5'].isin(['Student']) & df0['Q3'].isin(q3_order)]\n\n# print the prepared data frame \nprint(df.info())\nprint(df_std.info())\ndf.sample(5)","3b476970":"\"\"\"Wrangling Data for Jobtitle-Activity relationship \"\"\"\n\nq23_map = {'Analysis':'Analyze and understand data to influence product or business decisions',\n            'Data Infrastructure':'Build and\/or run the data infrastructure that my business uses for storing, analyzing, and operationalizing data', \n            'Prototyping':'Build prototypes to explore applying machine learning to new areas', \n            'Machine Learning':'Build and\/or run a machine learning service that operationally improves my product or workflows', \n            'Experiments':'Experimentation and iteration to improve existing ML models', \n            'Research':'Do research that advances the state of the art of machine learning', \n            'None':'None of these activities are an important part of my role at work', \n            'Other':'Other'}\n\nq23_col_map = { 'Q23_Part_1':'Analysis', \n                'Q23_Part_2':'Data Infrastructure', \n                'Q23_Part_3':'Prototyping',\n                'Q23_Part_4':'Machine Learning', \n                'Q23_Part_5':'Experiments', \n                'Q23_Part_6':'Research', \n                'Q23_Part_7':'None', \n                'Q23_OTHER':'Other'}\n\n# change the value of columns related to Q23 to short name and numerical value to count\ndf_q23 = df.filter(regex=\"^(?=.*Q23)\").rename(columns=q23_col_map).copy()\ndf_q23.fillna(0, inplace=True)\ndf_q23.replace(to_replace=list(q23_map.values()), value=1, inplace=True)\n\n# combine Q5 with Q23 \ndf_q5q23 = pd.concat([df[['Q5']], df_q23], axis=1).groupby('Q5').sum().loc[q5_order].T\n# add a total \ndf_q5q23['All Professionals'] = df_q5q23.sum(axis=1)\ndf_q5q23 = df_q5q23[['All Professionals']+q5_order] # change the order \n# combine 'Other' and 'None'\ndf_q5q23.loc['Other'] += df_q5q23.loc['None'] \ndf_q5q23.drop('None', axis=0, inplace=True)","c682af9c":"\"\"\"Figure1: Plotting Multiple Pie Graphs for Jobtitle-Activity \"\"\"\n\n# configure the grid size \nfig1 = plt.figure(figsize=(10,8)) \ngs_w, gs_h = 3, 4\ngs = fig1.add_gridspec(gs_w, gs_h)\ngs.update(wspace=0.1, hspace=0.3) \naxes1 = []\nfor i in range(gs_w):\n    for j in range(gs_h): \n        axes1.append(fig1.add_subplot(gs[i, j]))\n\n# draw a pie chart\ncolor_map = [\"#008294\", \"#d4ac0d\", \"#1f618d\", \"#c0392b\", \"#4b4b4c\", \"#A569BD\",\"#bdbdbd\"]\nfig1.patch.set_facecolor('#fbfbfb')\nfor i in range(gs_w * gs_h): \n    # wedges, texts =  axes1[i].pie(x=df_q5q23.iloc[:, i], colors=color_map, wedgeprops=dict(width=0.2)) # plain\n    wedges, texts, autotexts =  axes1[i].pie(x=df_q5q23.iloc[:, i], colors=color_map, wedgeprops=dict(width=0.2), \n                                            autopct=lambda pct: f\"{pct:5.2f}%\", textprops=dict(color=\"#4a4a4a\", fontsize=4))\n    axes1[i].text(0, 1.2, df_q5q23.columns[i], fontsize=8, fontweight='bold', fontfamily='serif', horizontalalignment='center')\n    axes1[i].set_facecolor('#fbfbfb')\n\n    \n# configure texts in graph \naxes1[0].text(-1., 2.5, 'Different Titles, Different Activities', fontsize=15, fontweight='bold', fontfamily='serif')\naxes1[0].text(-1., 2.1, 'from the Kaggle Survey 2020', fontsize=10, fontweight='light', fontfamily='serif')\naxes1[0].legend(wedges, df_q5q23.index,  bbox_to_anchor=(2.2, 0.05), loc=\"upper center\", ncol=7, fontsize=7)\naxes1[4].legend(wedges, df_q5q23.index,  bbox_to_anchor=(2.2, 0.05), loc=\"upper center\", ncol=7, fontsize=7)\naxes1[8].legend(wedges, df_q5q23.index,  bbox_to_anchor=(2.2, 0.05), loc=\"upper center\", ncol=7, fontsize=7)\n\nfig1.text(0.13, -0.02, \"\\n\".join([f\" - {k} : {q23_map[k]}\" for k in df_q5q23.index]), \n            fontsize=8, fontweight='light', fontfamily='serif', \n            bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.2))\nprint()","ec07db97":"\"\"\" Figure3: Gender Distributions across different job titles \"\"\"\n\n# join jobtitle(Q5) and Gender(Q2) \ndf_q5q2 = df[['Q2','Q5']].groupby('Q5')['Q2'].value_counts().unstack().loc[q5_order]\ndf_q5q2 = df_q5q2.fillna(0)\n\n# group gender categries into ['Man', 'Woman', 'Etc']\netc_columns = [x for x in df['Q2'].unique() if x not in ['Man', 'Woman']]\ndf_q5q2['Etc'] = df_q5q2[etc_columns].sum(axis=1)\ndf_q5q2.drop(columns=etc_columns, axis=1, inplace=True)\n\n# add \"total\" column to annotate the graph \ndf_q5q2['total'] = df_q5q2.sum(axis=1) \n# add ratio columns \ndf_q5q2[['Man_r', 'Woman_r', 'Etc_r']] = (df_q5q2[['Man', 'Woman', 'Etc']].T \/ (df_q5q2['total'] + 0.001)).T # convert it into ratio \n\n# order is upside down when plotting a bar chart\ndf_q5q2 = df_q5q2.iloc[::-1]\n\n# plot a horizontal barchart \nfig3, ax3 = plt.subplots(1,1,figsize=(12,6))\n\nax3.barh(df_q5q2.index, df_q5q2['Man'], \n         color='#004c70', alpha=0.8, label='Man')\nax3.barh(df_q5q2.index, df_q5q2['Woman'], left=df_q5q2['Man'], \n         color='#990000', alpha=0.8, label='Woman')\nax3.barh(df_q5q2.index, df_q5q2['Etc'],left=df_q5q2['Man']+df_q5q2['Woman'],\n         color='#4a4a4a', alpha=0.8, label='Etc')\n\nfor i in df_q5q2.index: \n    ax3.annotate(f\"{df_q5q2['total'][i]:.0f}\",\n                 xy=(df_q5q2['total'][i]+2, i), \n                 va = 'center', ha='left', fontweight='light', fontfamily='serif',\n                 color='#4a4a4a', fontsize=8)\n    if (df_q5q2['Man'][i] > 10):\n        ax3.annotate(f\"{df_q5q2['Man_r'][i]*100:.0f}%\", \n                      xy=(df_q5q2['Man'][i]\/2, i), va = 'center', ha='center', \n                      fontweight='light', fontfamily='serif', color='white', fontsize=8)\n    if (df_q5q2['Woman'][i] > 10):\n        ax3.annotate(f\"{df_q5q2['Woman_r'][i]*100:.0f}%\", \n                     xy=(df_q5q2['Man'][i] + (df_q5q2['Woman'][i]\/2), i), \n                     va = 'center', ha='center', \n                     fontweight='light', fontfamily='serif', color='white', fontsize=8)\n\n\nfor s in ['top', 'left', 'right']:\n    ax3.spines[s].set_visible(False)\n\nax3.set_xlim(0, 460)\nax3.set_ylabel(\"\")\nax3.legend(prop=dict(size=10))\nax3.grid(axis='x', linestyle='-', alpha=0.4)\n\nfig3.text(0.13, 0.95, 'Gender Distribution Across Different Job Titles', \n          fontsize=15, fontweight='bold', fontfamily='serif')\nall_man_ratio = df_q5q2.mean(axis=0)['Man_r']*100\nall_women_ratio = df_q5q2.mean(axis=0)['Woman_r']*100\nall_etc_ratio = df_q5q2.mean(axis=0)['Etc_r']*100\nfig3.text(0.131, 0.91, f'Man({all_man_ratio:.2f}%), Woman({all_women_ratio:.2f}%), Etc({all_etc_ratio:.2f}%) from the Kaggle Survey 2020')\n\nplt.show()\n\n\n","1171f0c0":"\"\"\" Figure2: Age Distributions across different job titles \"\"\"\n\n# join Age and jobtitle data \ndf_q5q1 = df[['Q1','Q5']].groupby('Q5')['Q1'].value_counts().unstack().loc[q5_order]\n\n# add \"All Professionals\" into the first row \ndf_rowsum = pd.DataFrame([df_q5q1.sum(axis=0).tolist()], columns=df_q5q1.columns, index=['All Professionals'])\ndf_q5q1 = pd.concat([df_rowsum, df_q5q1], axis=0)\n\n# convert it into ratio \ndf_q5q1 = (df_q5q1.T \/ (df_q5q1.sum(axis=1) + 0.001)).T \n\n\n\n\n# plot the heatmap \nfig2, ax2 = plt.subplots(1,1,figsize=(12,6))\n\nax_heat = sns.heatmap(ax=ax2, data=df_q5q1, linewidths=.1, square=True, cmap='BuPu',\n                      annot=True, fmt='.0%', annot_kws=dict(size=5, fontfamily='serif'),\n                      cbar_kws=dict(format=FuncFormatter(lambda x,pos: f'{x:.0%}')))\n# resize the ticklabels \nax_heat.set_xticklabels(ax_heat.get_xmajorticklabels(), fontsize=8, fontfamily='serif')\nax_heat.set_yticklabels(ax_heat.get_ymajorticklabels(), fontsize=8, fontfamily='serif')\nax_heat.collections[0].colorbar.ax.tick_params(labelsize=6)\n\n# Change background color\nbackground_color = \"#fbfbfb\"\nfig2.patch.set_facecolor(background_color) # figure background color\nax2.set_facecolor(background_color) # axes background color\n\nax2.set_xlabel(\"\")\nax2.set_ylabel(\"\")\n\nfig2.text(0.37, 0.95, 'Age Distribution Across Different Job Titles', \n          fontsize=15, fontweight='bold', fontfamily='serif')\nfig2.text(0.37, 0.91, 'from the Kaggle Survey 2020',\n          fontsize=10, fontweight='bold', fontfamily='serif')\n\nplt.show()","0ea1ea0b":"\"\"\"Figure4: Compensation Distribution Across Different Job Titles\"\"\"\n\n# The first five compensation ranges seems unrealistic, which might be caused by confusion of monthly and yearly\n# so let's assume that these ranges are faulse information and scrap them out. \n\nq24_order = [\n#     \"$0-999\", \"1,000-1,999\", \"2,000-2,999\", \"3,000-3,999\", \"4,000-4,999\", # fault observations\n    \"5,000-7,499\", \"7,500-9,999\", \"10,000-14,999\", \"15,000-19,999\", \n    \"20,000-24,999\", \"25,000-29,999\", \"30,000-39,999\", \"40,000-49,999\", \n    \"50,000-59,999\", \"60,000-69,999\", \"70,000-79,999\",\n    \"80,000-89,999\", \"90,000-99,999\", \"100,000-124,999\", \"125,000-149,999\", \n    \"150,000-199,999\", \"200,000-249,999\", \"250,000-299,999\", \"300,000-500,000\", \"> $500,000\"]\nq24_map = {\n#     \"$0-999\":\"0~\", \"1,000-1,999\":\"1k~\", \"2,000-2,999\":\"2k~\", \"3,000-3,999\":\"3k~\", \"4,000-4,999\":\"4k~\",\n    \"5,000-7,499\":\"5k~\", \"7,500-9,999\":\"7.5k~\", \"10,000-14,999\":\"10k~\", \"15,000-19,999\":\"15k~\", \n    \"20,000-24,999\":\"20k~\", \"25,000-29,999\":\"25k~\", \"30,000-39,999\":\"30k~\", \"40,000-49,999\":\"40k~\", \n    \"50,000-59,999\":\"50k~\", \"60,000-69,999\":\"60k~\", \"70,000-79,999\":\"70k~\",\n    \"80,000-89,999\":\"80k~\", \"90,000-99,999\":\"90k~\", \"100,000-124,999\":\"100k~\", \n    \"125,000-149,999\":\"125k~\", \"150,000-199,999\":\"150k~\", \"200,000-249,999\":\"200k~\", \n    \"250,000-299,999\":\"250k~\", \"300,000-500,000\":\"300k~\", \"> $500,000\":\"500k~\"}\n\ndf_q5q24 = df[['Q24', 'Q5']].dropna()\ndf_q5q24 = df_q5q24.groupby('Q5')['Q24'].value_counts().unstack()[q24_order].loc[q5_order]\ndf_q5q24 = df_q5q24.rename(columns=q24_map) # to short name\n\n# add 'all professionals' into the first row \ndf_rowsum = pd.DataFrame([df_q5q24.sum(axis=0).tolist()], columns=df_q5q24.columns, index=['All Professionals']) \ndf_q5q24 = pd.concat([df_rowsum, df_q5q24], axis=0)\n\n# convert the scale between 0-100 \ndf_q5q24 = 100*(df_q5q24.T \/ (df_q5q24.sum(axis=1) + 0.001)).T \n\n\n# plot the multiple graph \nfig4, axes4 = plt.subplots(df_q5q24.shape[0],1, sharex=True, figsize=(12,6))\nax_ylim = (0, df_q5q24.max().max())\nfor i, ax in enumerate(axes4): \n    ax.plot(df_q5q24.columns, df_q5q24.iloc[i,:],marker='.')\n    ax.fill_between(df_q5q24.columns, ax_ylim[0], df_q5q24.iloc[i,:], alpha=0.5)\n    ax.set_ylim(ax_ylim)\n    ax.set_ylabel(df_q5q24.index[i], labelpad=20, rotation=0, horizontalalignment='right')\n    ax.set_yticks([])\n\nfig4.text(.12, 0.95, \"Compensastion Distribution Across Different Job Titles\", \n         fontsize=15, fontweight='bold', fontfamily='serif')\n\nfig4.text(.12, 0.91, 'in US dollars per year from the Kaggle Survey 2020', \n         fontsize=10, fontweight='light', fontfamily='serif')\n\nplt.show()","00da18c4":"\"\"\" Data Wrangling for jobtitles-experience(programming&ML)  and jobtitles-education\"\"\"\n\n#df[['Q5', 'Q4','Q6', 'Q15']].describe() # Q4(Education), Q6(Programming Exp.), Q15(ML Exp) \n\n# [DF1] Jobtitles(Q5) ~ (Programming(Q6)+ML(Q15))\n# make a new list of columns to match Q6 and Q15\n# Q6: 0, <1, 1-2, 3-5, 5-10, 10-20, 20+\n# Q15: 0, <1, (1-2, 2-3), (3-4, 4-5), 5-10, 10-20, 20+\nq6q15_order = ['0 year', '< 1 years', '1-2 years', '3-5 years', '5-10 years', '10-20 years', '20+ years']\n# modify Q6\ndf_q6 = df[['Q6']].replace('I have never written code', q6q15_order[0])\n# modify Q15\ndf_q15 = df[['Q15']].replace({'I do not use machine learning methods':q6q15_order[0],\n                                'Under 1 year':q6q15_order[1], \n                                '2-3 years':q6q15_order[2], \n                                '3-4 years':q6q15_order[3], \n                                '4-5 years':q6q15_order[3], \n                                '20 or more years':q6q15_order[6]})\n# combine Q5-Q6\ndf_q5q6 = pd.concat([df[['Q5']], df_q6], axis=1)\ndf_q5q6 = df_q5q6.groupby('Q5')['Q6'].value_counts().unstack().loc[q5_order][q6q15_order]\ndf_q5q6.fillna(0., inplace=True)\n\n# add a row for 'all professionals' (mean)\ndf_q5q6_rowavg = pd.DataFrame([df_q5q6.mean(axis=0).tolist()], index=['All Professionals'], columns=df_q5q6.columns)\ndf_q5q6 = pd.concat([df_q5q6_rowavg, df_q5q6], axis=0)\n\n# Scale the value between 0 to 200  \ndf_q5q6['sum'] = df_q5q6.sum(axis=1)\ndf_q5q6_ratio = 200*(df_q5q6.T \/ df_q5q6['sum']).T\ndf_q5q6_ratio.drop(columns='sum', axis=1, inplace=True)\n\n\n# combine Q5-Q15 \ndf_q5q15 = pd.concat([df[['Q5']], df_q15], axis=1)\ndf_q5q15 = df_q5q15.groupby('Q5')['Q15'].value_counts().unstack().loc[q5_order][q6q15_order]\ndf_q5q15.fillna(0., inplace=True)\n\n# add a row for 'all professionals' (mean)\ndf_q5q15_rowavg = pd.DataFrame([df_q5q15.mean(axis=0).tolist()], index=['All Professionals'], columns=df_q5q15.columns)\ndf_q5q15 = pd.concat([df_q5q15_rowavg, df_q5q15], axis=0)\n\n# Scale the value between 0 to 200 \ndf_q5q15['sum'] = df_q5q15.sum(axis=1)\ndf_q5q15_ratio = 200*(df_q5q15.T \/ df_q5q15['sum']).T\ndf_q5q15_ratio.drop(columns='sum', axis=1, inplace=True)\n\n\n# [DF2] Jobtitles(Q5) ~ Education(Q4)\nq4_order = ['No Formal Edu.', 'Diploma', 'Bachelor', 'Master', 'PhD', 'Professional', 'Not to Answer']\nq4_map = {'No formal education past high school':q4_order[0], \n            'Some college\/university study without earning a bachelor\u2019s degree':q4_order[1], \n            'Bachelor\u2019s degree':q4_order[2], \n            'Master\u2019s degree':q4_order[3], \n            'Doctoral degree':q4_order[4], \n            'Professional degree':q4_order[5], \n            'I prefer not to answer':q4_order[6]}\n\n# combine Q5-Q4 \ndf_q4 = df[['Q4']].replace(q4_map)\ndf_q5q4 = pd.concat([df[['Q5']], df_q4], axis=1)\ndf_q5q4 = df_q5q4.groupby('Q5')['Q4'].value_counts().unstack().loc[q5_order][q4_order]\ndf_q5q4.fillna(0., inplace=True)\n\n# add a row for 'all professionals' (mean)\ndf_q5q4_rowavg = pd.DataFrame([df_q5q4.mean(axis=0).tolist()], index=['All Professionals'], columns=df_q5q4.columns)\ndf_q5q4 = pd.concat([df_q5q4_rowavg, df_q5q4], axis=0)\n\n# calculate the ratio \ndf_q5q4['sum'] = df_q5q4.sum(axis=1)\ndf_q5q4_ratio = (df_q5q4.T \/ df_q5q4['sum']).T\ndf_q5q4_ratio.drop(columns='sum', axis=1, inplace=True)\n\n","2a52c491":"\"\"\"Figure5:  Multiple Plots of both jobtitles-experience(programming&ML) and jobtitles-education\"\"\"\n\n# set figure \nfig5 = plt.figure(figsize=(20, 16), dpi=200)\ngs5 = fig5.add_gridspec(5,5)\n\n# Q5 ~ (Q6,Q15) graph (left)\ndf_q5q6_ratio = df_q5q6_ratio.iloc[::-1] # invert the order of rows (barh graph)\n\nax5p1 = fig5.add_subplot(gs5[1:4, 0:2])\nfor y, q5_idx in enumerate(df_q5q6_ratio.index): \n    for x, q6q15_idx in enumerate(df_q5q6_ratio.columns): \n        sc_q6 = ax5p1.scatter(x, y+0.12, s=df_q5q6_ratio.loc[q5_idx, q6q15_idx], color='#004c70', alpha=0.8, marker='o', edgecolors='none')\n        sc_q15 = ax5p1.scatter(x, y-0.12, s=df_q5q15_ratio.loc[q5_idx, q6q15_idx], color='#ec7063', alpha=0.8, marker='o', edgecolors='none')\n    \nax5p1.grid(linewidth=0.2, zorder=0)\nax5p1.set_yticks(range(len(df_q5q6_ratio.index)))\nax5p1.set_yticklabels(df_q5q6_ratio.index, fontfamily='serif', fontsize=11)\nax5p1.set_xticks(range(len(df_q5q6_ratio.columns)))\nax5p1.set_xticklabels(df_q5q6_ratio.columns, fontfamily='serif', fontsize=11)\nax5p1.legend((sc_q6, sc_q15), ('Years of Programming', 'Years of Using Machine Learning'), scatterpoints=5, \n                bbox_to_anchor=(0.95, 1.08), loc=\"upper right\", borderaxespad=0.2, ncol=2)\nfor s in ['top', 'left', 'right', 'bottom']:\n    ax5p1.spines[s].set_visible(False)\n\n\n# Q5 ~ Q4 graph (right)\ndf_q5q4_ratio = df_q5q4_ratio.iloc[::-1] # invert the order of rows (barh graph)\n\nax5p2 = fig5.add_subplot(gs5[1:4, 2:4], sharey=ax5p1)\ncolor_map = plt.get_cmap('tab10')(np.linspace(0.15, 0.85, df_q5q4_ratio.shape[1]))\ndf_q5q4_cumsum = df_q5q4_ratio.cumsum(axis=1)\n\nfor col, cr in zip(df_q5q4_ratio.columns, color_map):\n    start = df_q5q4_cumsum[col] - df_q5q4_ratio[col]\n    ax5p2.barh(df_q5q4_ratio.index, df_q5q4_ratio[col], left=start, height=0.5, color=cr, alpha=0.6, label=col)\n\nfor i in df_q5q4_ratio.index: \n    for col in ['Bachelor', 'Master', 'PhD']:\n        ax5p2.annotate(f\"{df_q5q4_ratio[col][i]*100:.3}%\", \n                            xy=(df_q5q4_cumsum[col][i]-(df_q5q4_ratio[col][i]\/2), i), \n                            va='center', ha='center', fontsize=9, fontweight='light', \n                            fontfamily='serif', color='white')\n\nax5p2.set_xlim(0,1)\nax5p2.set_xticks([])\nplt.setp(ax5p2.get_yticklabels(), visible=False)\nfor s in ['top', 'left', 'right', 'bottom']:\n    ax5p2.spines[s].set_visible(False)\n    \nax5p2.legend( bbox_to_anchor=(1.01, 1.1), loc=\"upper right\", borderaxespad=0.2, ncol=4)\n\nfig5.text(0.13, 0.83, ' What experience are required? Programming, Machine Learning, or Higher Education?', \n            fontsize=15, fontweight='bold', fontfamily='serif')\nfig5.text(0.131, 0.81, 'with Kaggle Survey 2020 ')\nplt.show()","c6f33859":"\"\"\" Data Wrangling for skillset gap between professionals and students\"\"\"\nfrom collections import OrderedDict\n\n# define the dict of pd.Series \nskillset_map = OrderedDict({'Programming Launguages':'Q7',\n                            'IDEs':'Q9',   \n                            'ML Frameworks':'Q16',\n                            'Learning Platforms':'Q37',\n                            'Hosted Notebooks':'Q10', \n                            'Visualization Tools':'Q14',\n                            'ML Algorithms':'Q17',\n                            'Media Sources':'Q39'\n                           })\nskillset_dict = OrderedDict()\n        \ndef value_counts_multicolumns(mdf): \n    \"\"\"count each value in the multiple columns, which should have single value per single column\"\"\"\n    df_count = pd.Series(dtype='int')\n    for c in mdf.columns: \n        df_count[mdf[c].value_counts().index[0]] = mdf[c].count()\n    return df_count\n\n# cacluate each value_counts \nfor skill, quest in skillset_map.items(): \n    pro_count = value_counts_multicolumns(df.filter(regex=f\"^(?=.*{quest})\"))\n    pro_ratio = 100. * (pro_count \/ pro_count.sum())\n    \n    std_count = value_counts_multicolumns(df_std.filter(regex=f\"^(?=.*{quest})\"))\n    std_ratio = 100. * (std_count \/ std_count.sum())        \n    \n    skillset_dict[skill] = pd.concat([pro_ratio, std_ratio], axis=1).sort_values(0)\n    skillset_dict[skill].rename(columns={0:'Professionals',1:'Students'}, inplace=True)","a92d52ad":"\"\"\" Figure6: multiple plots for skillset gap between professionals and students\"\"\"\n\n# plot a horizontal barchart \nr, c = 4, 2\nfig6, axs6 = plt.subplots(r,c,figsize=(8,16), sharex=True)\n\nfor idx, (skill, df_skill) in enumerate(skillset_dict.items()):\n    y_range = np.arange(len(df_skill.index))\n    bar_width = 0.4\n                               \n    barh_pro = axs6[idx%r][idx\/\/r].barh(y_range + bar_width\/2, df_skill['Professionals'], bar_width, color='#ec7063', label=\"Professionals\") \n    barh_std = axs6[idx%r][idx\/\/r].barh(y_range - bar_width\/2, df_skill['Students'], bar_width, color='#004c70', label=\"Students\")\n\n    axs6[idx%r][idx\/\/r].set_xlim(0, 35)\n    axs6[idx%r][idx\/\/r].set_xticks([])\n    axs6[idx%r][idx\/\/r].set_yticks(y_range)\n    \n    # shorten the index str \n    short_index = df_skill.index.str.replace('(','\\n(')\n    short_index = short_index.str.replace('Notebooks','NB')\n    short_index = short_index.str.replace(' or ',' or \\n')\n    \n    axs6[idx%r][idx\/\/r].set_yticklabels(short_index, fontsize=6)\n    axs6[idx%r][idx\/\/r].set_title(skill)\n#     axs6[idx%r][idx\/\/r].legend(loc='lower center', prop=dict(size=5))\n    \n    for i, sk in enumerate(df_skill.index): \n        if (df_skill['Professionals'][sk] > 3):\n            axs6[idx%r][idx\/\/r].annotate(f\"{df_skill['Professionals'][sk]:.0f}%\",\n                         xy=(df_skill['Professionals'][sk] - 3, i + bar_width\/2), \n                         va = 'center', ha='left', fontweight='light', fontfamily='serif',\n                         color='white', fontsize=6)\n        if (df_skill['Students'][sk] > 3):\n            axs6[idx%r][idx\/\/r].annotate(f\"{df_skill['Students'][sk]:.0f}%\",\n                 xy=(df_skill['Students'][sk] - 3, i - bar_width\/2), \n                 va = 'center', ha='left', fontweight='light', fontfamily='serif',\n                 color='white', fontsize=6)  \n    for s in ['top', 'bottom', 'left', 'right']:\n        axs6[idx%r][idx\/\/r].spines[s].set_visible(False)\n\naxs6[0][0].legend(bbox_to_anchor=(0,1.2), loc='upper left', ncol=2, prop=dict(size=10))\nfig6.text(0.13, 0.95, 'Skillset Gaps Between Professionals and Students', \n          fontsize=15, fontweight='bold', fontfamily='serif')\nfig6.text(0.131, 0.94,'from the Kaggle Survey 2020')\n\nplt.show()\n","35be755a":"As one of the job seekers who want to work in the data science field, I have been struggled to figure out what job title is suited for me. This article is not including all the information or gives you perfect answers but I hope that you can grasp some ideas of how different each data jobs are and what you need to focus on to land your dream job. Don't forget to check takeaways before leaving this article! \n\n> 1. A Master's Degree might be proper the level of education for getting data jobs. \n> 2. Python, Scikit-learn, and Jupyter notebook are the most essential skills in the data science field\n> 3. Check out blogs, Kaggle, and youtube to communicate with working professionals","f94c35be":"[back to top](#table-of-contents)\n\n<a id='References'><\/a>\n# 7. References","484e4007":"[back to top](#table-of-contents)\n\n<a id='takeaways'><\/a>\n# 6. Takeaways","b2e86faf":"<a id=\"introduction\"><\/a>\n[back to top](#table-of-contents)\n\n# 1. Introduction\n\nAre you one of the data science enthusiasts? If so, you probably have questions similar to what I have because there are different kinds of job positions in professional data fields. Without understanding each job title, you would not know where to start. Let's take a look at who really are the data professionals currently working in North America, then build up the list of the required skills and take away some practical tips from them. \n\n","26c88b2a":"[back to top](#table-of-contents)\n\n<a id=\"skills\"><\/a>\n# 5. What skills do you need to stand out your resume? Professionals vs. Students\n\nAs a programmer, skillsets have a great role in matching jobs. Skillsets might differ across various job fields, but I simplified the data to compare what data professionals use on regular basis and what students learn or practice to stand out their resumes. Here is the summary of the analysis from the graph below. \n\n- `Programming Languages`: Top language in DS & ML community is **Python** as expected. For other languages, students prefer to learn R over SQL, while many professionals work with SQL than R. \n- `Hosted Notebooks`: Around 30% of both professionals and students don't use hosted notebooks on regular basis, but the persons who use notebooks regularly favour working in **Colab, Kaggle, and Jupyter**. \n- `IDEs`: Most dominant IDEs used by both group is **Jupyter environment**\n- `Visualization Tools`: Top four tools preferred by both groups are **Matplotlib, Seaborn, Ggplot, and Plotly**.\n- `ML Frameworks`: Top three ML frameworks (**Scikit-learn, TensorFlow, and Keras**) are the same for both groups, but more students use Pytorch than Xgboost while professionals similarly use them both. \n- `ML Algorithms`: While both groups utilize **Linear\/Logistic Regression and DecisionTrees\/Random Forests** regularly, professionals use more Gradient Boosting Machines over Convolutional Neural Networks and Bayesian Approaches in their work.\n- `Learning Platform`: Kaggle Professionals thinks that the best learning platform is **Coursera(21%) followed by Kaggle Learn Courses(12%)** while Students prefers to study through **University Courses(21%) and Coursera(18%)**\n- `Media Sources`: Professionals usually share or report on data science topics via **Blogs(19%), Kaggle(16%) or YouTube(14%)** in order while Students would like to share in **Youtube(19%) the most, followed by Kaggle(17%) and Blogs(14%)** ","7d67a401":"\n# Looking for Data Science Jobs in North America? Here Are Some Things You Should Know\n### from the [Kaggle Machine Learning & Data Sicnece Survey 2020](https:\/\/www.kaggle.com\/c\/kaggle-survey-2020)  \n\n\n\n<!-- ![main-image](magnet-me-LDcC7aCWVlo-unsplash.jpg) -->\n<img src=\"https:\/\/images.unsplash.com\/photo-1598257006626-48b0c252070d?ixlib=rb-1.2.1&ixid=MXwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHw%3D&auto=format&fit=crop&w=1950&q=80\">\n<span>Photo by <a href=\"https:\/\/unsplash.com\/@magnetme?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText\">Magnet.me<\/a> on <a href=\"https:\/\/unsplash.com\/s\/photos\/data-professional?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText\">Unsplash<\/a><\/span>\n\n<a id=\"table-of-contents\"><\/a>\n\n1. [Introduction](#introduction)\n2. [Data Preparation](#preparation)\n3. [What is your option among various data jobs?](#datajobs)\n    * 3.1. [Different job titles, different activities](#datajobs-activities) \n    * 3.2. [Meet the data professionals: Gender, Age and Compenstion](#datajobs-meet) \n4. [What expereicne are required? Programming, Machine Learning, or Higher Education?](#experience)\n5. [What skills do you need to stand out your resume? Professionals vs. Students](#skills)\n6. [Takeaways](#takeaways)\n7. [References](#reference)","d7a1bbd2":"<a id=\"preparation\"><\/a>\n[back to top](#table-of-contents)\n\n# 2. Data Preparation\n\nIn this section, let's shape the original dataset \"[Kaggle Machine Learning & Data Sicnece Survey 2020](https:\/\/www.kaggle.com\/c\/kaggle-survey-2020)\" into our working data frame. Although it is a well-formatted dataset, we need to modify some part of it to draw some findings related to our questions as following points: \n- The survey questions in the data frame are located at the top row --> need to be deleted \n- The first column of the data frame(Survey Duration) is not relevant info --> need to be deleted  \n- Multiple choice responses are already distributed into each columns --> no need to process\n- Note that all columns (survey questions) are categorical variables\n- Data Frame 1 **includs only NorthAmerican Data Professionals** --> need to filter out any observations gathered from students or undemployed ones as well as outside of North America\n- Data Frame 2 **includs only NorthAmerican Data Students** --> need to filter out any observations gathered from professionals or undemployed ones as well as outside of North America\n\nOriginally, the dataset has 20,036 observations and 355 column variables(1 Duration, 18 single-choice-questions, and 336 multiple-choice-questions) but after preparation, the final data frame1 has 1,909 observations and 354 columns and the data frame2 has 395 observations and 354 columns","586ad38e":"<a id=\"datajobs\"><\/a>\n[back to top](#table-of-contents)\n\n# 3. What is your option among various data jobs? \n\nWe are data enthusiasts, so let's meet the working professionals throughout their statistical data! First, you can explore some general job titles along with the comparison of their roles (3.1. [Different job titles, different activities](#datajobs-roles)). Then, you will get a sense of the current trends in those positions with some statistics of the data professionals ( 3.2. [Meet the data professionals: Gender, Age and Compenstion](#datajobs-meet))\n","3ab10385":"1. [\n[Kaggle 2020] Visualization & Analysis](https:\/\/www.kaggle.com\/subinium\/kaggle-2020-visualization-analysis#Q14.-Visualization-Library) written by [Subin An](https:\/\/www.kaggle.com\/subinium)\n2. [Enthusiast to Data Professional - What changes?](https:\/\/www.kaggle.com\/spitfire2nd\/enthusiast-to-data-professional-what-changes#3.-What-should-I-focus-on?) written by [Schubert](https:\/\/www.kaggle.com\/spitfire2nd)","239d7522":"<a id=\"experience\"><\/a>\n[back to top](#table-of-contents)\n\n# 4. What experience are required? Programming, Machine Learning, or Higher Education? \n\nBased on the Kaggle Survey, most data professionals have **3-5 years of programming experience** and **1-2 years of experience using machine learning**. However, both machine learning engineers and data scientists seem to have more experience(3-5 years) in using machine learning than other data jobs. Also, **having a master's degree** is most dominant across most data jobs except research scientists and statisticians. ","6e7c26a9":"[back to top](#table-of-contents)\n\n<a id=\"datajobs-meet\"><\/a>\n## 3.2. Meet the Data Professionals: Gender, Age and Compensation\n\nLet's discover the data professionals currently working in North America. Based on the three graphs below, most of the data professionals using the Kaggle platform are **men(77%)**, **25-40 years old (49%)**, and expected to make **100k-150k US dollars per year**. Please check each figure to see the details of the job title you want to seek. \n\n","ee2e01aa":"[back to top](#table-of-contents)\n\n<a id=\"datajobs-roles\"><\/a>\n## 3.1. Different job titles, different roles\n\nPresumably, you may search only \"Data Scientist\" or \"Machine learning engineer\" in a job search engine, but you might be surprised that many DS&ML professionals in the Kaggle platform defines their professions with many different job titles. Here are short descriptions of some confusing job titles. \n\n\n- **Machine Learning Engineer**: A software engineer who leverages big data tools and programming frameworks to ensure that the raw data gathered from data pipelines are redefined as data science models that are ready to scale as needed. They\u2019re also responsible for taking theoretical data science models and helping scale them out to production-level models that can handle terabytes of real-time data.[springboard.com](https:\/\/www.springboard.com\/blog\/machine-learning-engineer-vs-data-scientist\/)\n\n- **Data Scientist**: A data profesesional who applies statistics, machine learning, and anlaytic approaches to solve critical business problems, and are also expected to have strong programming skills, an ability to design new algorithms, and some expertise in the domain knowledge to handle big data - [cognitiveclass.ai](https:\/\/cognitiveclass.ai\/blog\/data-scientist-vs-data-engineer)\n\n- **Data Analyst**: A data professional in their organization who can query and process data, provide reports, summarize and visualize data, but in most of time, they are not expected to deal with anlyzing big data nor to develop new algorithms\" - [cognitiveclass.ai](https:\/\/cognitiveclass.ai\/blog\/data-scientist-vs-data-engineer)\n\n- **Data Engineer**: A software engineer who prepares the \u201cbig data\u201d infrastructure to be analyzed by Data Scientists, more specifically, design, build, integrate data from various resources, and manage big data to optimize the performance of their company\u2019s big data ecosystem. - [cognitiveclass.ai](https:\/\/cognitiveclass.ai\/blog\/data-scientist-vs-data-engineer)\n\n- **Database Adminimstrator(DBA)\/Database Engineer**: A software engineer who stores and organizes data, which includs some roles such as capacity planning, installation, configuration, database design, migration, performance monitoring, security, troubleshooting, as well as backup and data recovery - [wikipedia.org](https:\/\/en.wikipedia.org\/wiki\/Database_administrator)\n\n- **Business Analyst(BA)**: A data professional who analyzes an organization or business domain (real or hypothetical) and documents its business, processes, or systems, assessing the business model or its integration with technology - [wikipedia.org](https:\/\/en.wikipedia.org\/wiki\/Business_analyst)\n\n- Other Professions working with Data: **Research Scientist, Software Engineer,Statistician and Product\/Project Manager**.\n\n\n\nTo have a better understading of each job title, let's look at the below figure which compares job roles across the different job titles. Here are some `findings` from the comparison\n\n1. Most common roles of all the data professionals are **Data Analysis(26.88%)**, **Prototyping(18.35%)** and **Data Infrastructure(15.64%)**\n2. **Data scientists do more data analysis tasks than ml engineers** while ml engineers are more focusing on prototyping and building machine learning service\n3. Data engineers and DBAs use a third of their working time in Building the data infrastructure\n4. Data analysts and Business analysts spend almost half of their activities in analyzing and understanding data to influence product business decisions   \n\n\n\n<!-- In the Kaggle DS&ML survey, there are seven types of typical data roles.\n\n|Roles | Description|\n|---------------|:-------------------------------------------------------------|\n|**Analysis** | Analyze and understand data to influence product or business decisions|\n|**Data Infrastructure** | Build and\/or run the data infrastructure that my business uses for storing, analyzing, and operationalizing data|\n|**Prototyping** | Build prototypes to explore applying machine learning to new areas|\n|**Machine Learning** | Build and\/or run a machine learning service that operationally improves my product or workflows|\n|**Experiments** | Experimentation and iteration to improve existing ML models|\n|**Research** | Do research that advances the state of the art of machine learning|\n|**None** | None of these activities are an important part of my role at work|\n|**Other** | Other data related roles|\n -->\n\n"}}