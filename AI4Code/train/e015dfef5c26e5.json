{"cell_type":{"f4aea991":"code","02914307":"code","1a2da928":"code","e52bbc31":"code","00ffcc9d":"code","fcc3163b":"code","73ee3bb3":"code","06657d05":"code","29438c1b":"code","08bcdd3b":"markdown","1e55e76f":"markdown","3b045cfb":"markdown","712fd74e":"markdown","380e7254":"markdown","209e08c3":"markdown","39d43920":"markdown","cbb9f027":"markdown","5849994a":"markdown"},"source":{"f4aea991":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom tqdm import tqdm_notebook\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.svm import NuSVR, SVR\nfrom sklearn.metrics import mean_absolute_error\npd.options.display.precision = 15\n\nimport lightgbm as lgb\nimport xgboost as xgb\nimport time\nimport datetime\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import StratifiedKFold, KFold, RepeatedKFold\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\nfrom sklearn.linear_model import Ridge, RidgeCV\nimport gc\nfrom catboost import CatBoostRegressor\nimport seaborn as sns\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n","02914307":"X = pd.read_csv(\"..\/input\/train.csv\", nrows = 600000000, dtype={'acoustic_data': np.int16, 'time_to_failure': np.float64})","1a2da928":"%time\n\nrows = 150_000\ntrain = X\nsegments = int(np.floor(train.shape[0] \/ rows))\n\nX_tr = pd.DataFrame(index=range(segments), dtype=np.float64,\n                       columns=['ave', 'std', 'max', 'min',\n                               'av_change_abs', 'av_change_rate', 'abs_max', 'abs_min',\n                               'std_first_50000', 'std_last_50000', 'std_first_10000', 'std_last_10000',\n                               'avg_first_50000', 'avg_last_50000', 'avg_first_10000', 'avg_last_10000',\n                               'min_first_50000', 'min_last_50000', 'min_first_10000', 'min_last_10000',\n                               'max_first_50000', 'max_last_50000', 'max_first_10000', 'max_last_10000'])\ny_tr = pd.DataFrame(index=range(segments), dtype=np.float64,\n                       columns=['time_to_failure'])\n\ntotal_mean = train['acoustic_data'].mean()\ntotal_std = train['acoustic_data'].std()\ntotal_max = train['acoustic_data'].max()\ntotal_min = train['acoustic_data'].min()\ntotal_sum = train['acoustic_data'].sum()\ntotal_abs_max = np.abs(train['acoustic_data']).sum()\n\nfor segment in tqdm_notebook(range(segments)):\n    seg = train.iloc[segment*rows:segment*rows+rows]\n    x = seg['acoustic_data'].values\n    y = seg['time_to_failure'].values[-1]\n    \n    y_tr.loc[segment, 'time_to_failure'] = y\n    X_tr.loc[segment, 'ave'] = x.mean()\n    X_tr.loc[segment, 'std'] = x.std()\n    X_tr.loc[segment, 'max'] = x.max()\n    X_tr.loc[segment, 'min'] = x.min()\n    \n    \n    X_tr.loc[segment, 'av_change_abs'] = np.mean(np.diff(x))\n    X_tr.loc[segment, 'av_change_rate'] = np.mean(np.nonzero((np.diff(x) \/ x[:-1]))[0])\n    X_tr.loc[segment, 'abs_max'] = np.abs(x).max()\n    X_tr.loc[segment, 'abs_min'] = np.abs(x).min()\n    \n    X_tr.loc[segment, 'std_first_50000'] = x[:50000].std()\n    X_tr.loc[segment, 'std_last_50000'] = x[-50000:].std()\n    X_tr.loc[segment, 'std_first_10000'] = x[:10000].std()\n    X_tr.loc[segment, 'std_last_10000'] = x[-10000:].std()\n    \n    X_tr.loc[segment, 'avg_first_50000'] = x[:50000].mean()\n    X_tr.loc[segment, 'avg_last_50000'] = x[-50000:].mean()\n    X_tr.loc[segment, 'avg_first_10000'] = x[:10000].mean()\n    X_tr.loc[segment, 'avg_last_10000'] = x[-10000:].mean()\n    \n    X_tr.loc[segment, 'min_first_50000'] = x[:50000].min()\n    X_tr.loc[segment, 'min_last_50000'] = x[-50000:].min()\n    X_tr.loc[segment, 'min_first_10000'] = x[:10000].min()\n    X_tr.loc[segment, 'min_last_10000'] = x[-10000:].min()\n    \n    X_tr.loc[segment, 'max_first_50000'] = x[:50000].max()\n    X_tr.loc[segment, 'max_last_50000'] = x[-50000:].max()\n    X_tr.loc[segment, 'max_first_10000'] = x[:10000].max()\n    X_tr.loc[segment, 'max_last_10000'] = x[-10000:].max()","e52bbc31":"scaler = StandardScaler()\nscaler.fit(X_tr)\nX_train_scaled = pd.DataFrame(scaler.transform(X_tr), columns=X_tr.columns)","00ffcc9d":"submission = pd.read_csv('..\/input\/sample_submission.csv', index_col='seg_id')\nX_test = pd.DataFrame(columns=X_tr.columns, dtype=np.float64, index=submission.index)\nplt.figure(figsize=(22, 16))\n\nfor i, seg_id in enumerate(tqdm_notebook(X_test.index)):\n    seg = pd.read_csv('..\/input\/test\/' + seg_id + '.csv')\n    \n    x = seg['acoustic_data'].values\n    X_test.loc[seg_id, 'ave'] = x.mean()\n    X_test.loc[seg_id, 'std'] = x.std()\n    X_test.loc[seg_id, 'max'] = x.max()\n    X_test.loc[seg_id, 'min'] = x.min()\n        \n    X_test.loc[seg_id, 'av_change_abs'] = np.mean(np.diff(x))\n    X_test.loc[seg_id, 'av_change_rate'] = np.mean(np.nonzero((np.diff(x) \/ x[:-1]))[0])\n    X_test.loc[seg_id, 'abs_max'] = np.abs(x).max()\n    X_test.loc[seg_id, 'abs_min'] = np.abs(x).min()\n    \n    X_test.loc[seg_id, 'std_first_50000'] = x[:50000].std()\n    X_test.loc[seg_id, 'std_last_50000'] = x[-50000:].std()\n    X_test.loc[seg_id, 'std_first_10000'] = x[:10000].std()\n    X_test.loc[seg_id, 'std_last_10000'] = x[-10000:].std()\n    \n    X_test.loc[seg_id, 'avg_first_50000'] = x[:50000].mean()\n    X_test.loc[seg_id, 'avg_last_50000'] = x[-50000:].mean()\n    X_test.loc[seg_id, 'avg_first_10000'] = x[:10000].mean()\n    X_test.loc[seg_id, 'avg_last_10000'] = x[-10000:].mean()\n    \n    X_test.loc[seg_id, 'min_first_50000'] = x[:50000].min()\n    X_test.loc[seg_id, 'min_last_50000'] = x[-50000:].min()\n    X_test.loc[seg_id, 'min_first_10000'] = x[:10000].min()\n    X_test.loc[seg_id, 'min_last_10000'] = x[-10000:].min()\n    \n    X_test.loc[seg_id, 'max_first_50000'] = x[:50000].max()\n    X_test.loc[seg_id, 'max_last_50000'] = x[-50000:].max()\n    X_test.loc[seg_id, 'max_first_10000'] = x[:10000].max()\n    X_test.loc[seg_id, 'max_last_10000'] = x[-10000:].max()\n    \nX_test_scaled = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns)","fcc3163b":"model = lgb.LGBMRegressor(n_estimators=1000, n_jobs=-1)\nmodel.fit(X_train_scaled,y_tr)\ny_pred_lgb = model.predict(X_test_scaled)","73ee3bb3":"import xgboost as xgb\n\nmodel = xgb.XGBRegressor(n_estimators=1000)\nmodel.fit(X_train_scaled,y_tr)\ny_pred_xgb = model.predict(X_test_scaled)","06657d05":"sample = pd.read_csv(\"..\/input\/sample_submission.csv\")\nsample['time_to_failure'] = (y_pred_lgb+y_pred_xgb)\/2\nsample.to_csv('submission.csv',index=False)","29438c1b":"%time\n\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import log_loss, roc_auc_score\n\nn_estimators = [10,100,1000]\nfor n_est in  n_estimators:\n    print(n_est)\n    cv = KFold(n_splits=5, shuffle=True,random_state=0)\n    for train, valid in cv.split(X_train_scaled, y_tr):\n        x_train = X_train_scaled.iloc[train]\n        x_valid = X_train_scaled.iloc[valid]\n        y_train = y_tr.iloc[train]\n        y_valid = y_tr.iloc[valid]\n        model_1 = lgb.LGBMRegressor(n_estimators=n_est, n_jobs=-1,random_state=0)\n        model_1.fit(x_train, y_train)\n        y_pred_lgb = model_1.predict(x_valid)\n        model_2 = xgb.XGBRegressor(n_estimators=n_est, n_jobs=-1,random_state=0)\n        model_2.fit(x_train, y_train)\n        y_pred_xgb = model_2.predict(x_valid)\n        y_pred = (y_pred_lgb + y_pred_xgb)\/2\n        print(mean_absolute_error(y_valid, y_pred))  ","08bcdd3b":"Trainig and Predict by XGboosting.\n\n--------------------------------------\nXGboosting\u306b\u3088\u308b\u5b66\u7fd2\u304a\u3088\u3073\u4e88\u6e2c\u3002","1e55e76f":"Loading Data.\n\n-------------------\n\n\u30c7\u30fc\u30bf\u306e\u8aad\u307f\u8fbc\u307f\u3002","3b045cfb":"Blending.\n\n---------------------------\n\n\u6df7\u5408\u3002","712fd74e":"I made too much simple LightGBM & XGboosting Blend model.\nhttps:\/\/www.kaggle.com\/artgor helped me.\n\nI'm beginner, so there may be many strange point.\nPlease give me a advise.\n\n--------------------------------------------------------------\n\nLightGBM\u3068XGboosting\u306e\u6df7\u5408\u306b\u3088\u308b\u30b7\u30f3\u30d7\u30eb\u306a\u56de\u5e30\u30e2\u30c7\u30eb\u3092\u69cb\u7bc9\u3057\u307e\u3057\u305f\u3002\nhttps:\/\/www.kaggle.com\/artgor\u3000\u3055\u3093\u306ekernels\u3092\u53c2\u8003\u306b\u3055\u305b\u3066\u3044\u305f\u3060\u304d\u307e\u3057\u305f\u3002\n\n\u65e5\u672c\u8a9e\u3067\u66f8\u304b\u308c\u305fkernels\u304c\u307b\u3068\u3093\u3069\u306a\u304b\u3063\u305f\u305f\u3081\n\u5c11\u3057\u3067\u3082\u521d\u5fc3\u8005\u306e\u52a9\u3051\u306b\u306a\u308c\u3070\u3068\u3053\u3061\u3089\u306ekernels\u3092\u4f5c\u6210\u3057\u307e\u3059\u3002\n\n\u306a\u304a\u3001\u79c1\u81ea\u8eab\u3082\u521d\u5fc3\u8005\u3067\u3059\u306e\u3067\u7406\u89e3\u306e\u3067\u304d\u3066\u3044\u306a\u3044\u70b9\u304c\u591a\u304f\u3042\u308b\u304b\u3068\u601d\u3044\u307e\u3059\u3002\n\u3054\u6307\u6458\u3084\u30a2\u30c9\u30d0\u30a4\u30b9\u304c\u3042\u308c\u3070\u662f\u975e\u304a\u9858\u3044\u3057\u307e\u3059\u3002","380e7254":"Normalize Features.\n\n------------------------\n\n\u4f5c\u6210\u3057\u305f\u7279\u5fb4\u91cf\u306e\u6b63\u898f\u5316\u3002","209e08c3":"Making Features.\n\n-----------------------\n\n\u7279\u5fb4\u91cf\u306e\u4f5c\u6210\u3002","39d43920":"Making data to predict.\n\n--------------------------------\n\n\u8a13\u7df4\u30c7\u30fc\u30bf\u306e\u4f5c\u6210\u3002","cbb9f027":"Cross Validation.\n\n---------------------------\n\n\u4ea4\u5dee\u691c\u8a3c\u3002","5849994a":"Trainig and Predict by LightGBM.\n\n--------------------------------------\n\nLightGBM\u306b\u3088\u308b\u5b66\u7fd2\u304a\u3088\u3073\u4e88\u6e2c\u3002"}}