{"cell_type":{"fcd4a407":"code","e09025f0":"code","7cee3118":"code","347e0562":"code","4eaccd28":"code","c5178bd6":"code","d89826b5":"code","efd032fe":"code","3c006d7c":"code","c55970b1":"code","949748ee":"code","18d1355b":"code","3b8d0932":"code","7af96336":"code","daf481fd":"code","a35aecb3":"markdown","805f5a65":"markdown","1ba6637d":"markdown","13dd4844":"markdown","b017c069":"markdown","629a1c06":"markdown","ea9a3d41":"markdown","5dad3e7d":"markdown","e240e36b":"markdown","73c71840":"markdown"},"source":{"fcd4a407":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","e09025f0":"import pandas as pd \nimport numpy as np\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n","7cee3118":"df=pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ndff=pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')","347e0562":"df2=df.corr()\nfea=pd.DataFrame(df2.iloc[:,-1][:-1])\n","4eaccd28":"df['LotFrontage'].fillna(df.LotFrontage.mean(),inplace=True)\ndf['GarageYrBlt'].fillna(df.GarageYrBlt.median(),inplace=True)\ndf['MasVnrArea'].fillna(df.MasVnrArea.mean(),inplace=True)","c5178bd6":"sns.distplot(df.LotArea)\nq=df.LotArea.quantile(0.9)\ndat1=df[df.LotArea<q]\nfea1=dat1[fea.index[1:]]\ntar=dat1[\"SalePrice\"]\n","d89826b5":"sns.distplot(dat1.LotArea)","efd032fe":"from sklearn.preprocessing import MinMaxScaler , StandardScaler\nscaler=StandardScaler()\ntar2=np.log(tar)","3c006d7c":"from sklearn.model_selection import train_test_split\nxtrain,xtest,ytrain,ytest=train_test_split(fea1,tar2,test_size=0.3,random_state=42,shuffle=True)","c55970b1":"from sklearn.model_selection import cross_validate,GridSearchCV\nfrom sklearn.metrics import confusion_matrix , accuracy_score\nfrom xgboost import XGBRegressor\nclf=XGBRegressor(n_estimators=5000,eta=0.1,colsample_bytree=1,max_depth=10,\n                 gamma=0,min_child_wieght=4,reg_lambda=4,objective='reg:squarederror')","949748ee":"clf.fit(fea1,tar2)\nfrom sklearn.metrics import mean_squared_error\npred=clf.predict(xtrain)\ntpred=clf.predict(xtest)\nbbb=clf.predict(fea1)\nprint(np.sqrt(mean_squared_error(ytrain,pred)))\nprint(np.sqrt(mean_squared_error(tpred,ytest)))\nprint(np.sqrt(mean_squared_error(bbb,tar2)))","18d1355b":"tfea=dff[fea.index[1:]]","3b8d0932":"tfea['LotFrontage'].fillna(tfea.LotFrontage.mean(),inplace=True)\ntfea['GarageYrBlt'].fillna(tfea.GarageYrBlt.median(),inplace=True)\ntfea['MasVnrArea'].fillna(tfea.MasVnrArea.mean(),inplace=True)\ntfea['BsmtFinSF1'].fillna(tfea.BsmtFinSF1.mean(),inplace=True)\ntfea['BsmtFinSF2'].fillna(tfea.BsmtFinSF2.mean(),inplace=True)\ntfea['BsmtUnfSF'].fillna(tfea.BsmtUnfSF.mean(),inplace=True)\ntfea['TotalBsmtSF'].fillna(tfea.TotalBsmtSF.mean(),inplace=True)\ntfea['BsmtFullBath'].fillna(tfea.BsmtFullBath.mean(),inplace=True)\ntfea['BsmtHalfBath'].fillna(tfea.BsmtHalfBath.mean(),inplace=True)\ntfea['GarageCars'].fillna(tfea.GarageCars.mean(),inplace=True)\ntfea['GarageArea'].fillna(tfea.GarageArea.mean(),inplace=True)       ","7af96336":"\nmypredcs=clf.predict(tfea)\nsubmit=pd.DataFrame(dff['Id'])\nsubmit['SalePrice']=np.exp(mypredcs) \n#since i have scaled the target variable before now using np.exp()\n# for changing into original form.","daf481fd":"submit.to_csv('my_XGBM.csv',index=False)","a35aecb3":"# Removing outliers","805f5a65":"# Data Preprocessing","1ba6637d":"1. Building model with Extreme Gradient Boost Regressor.\n2. I have already tuned the hyperparameters.\n3. I am only using number variables.\n4. You can modify it to improve the score and  for practice.","13dd4844":"**fitting the model and checking the score**","b017c069":"# Data preprocessing for test dataset","629a1c06":"# Scaling the Target variable.","ea9a3d41":"# Filling Missing Values.","5dad3e7d":"# splitting the data in train and test sets","e240e36b":"# Input\n1) df DataFrame is for trainning.\n1) dff DataFrame is for testing.","73c71840":"# Finding correlation for feature engineering "}}