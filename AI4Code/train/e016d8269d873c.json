{"cell_type":{"097e7252":"code","bd4c447a":"code","78b2196d":"code","066b0ab5":"code","17ce4b42":"code","61d387bf":"code","205d5c1d":"code","5dd14dfd":"code","e730dda5":"code","92987180":"code","d66eeab7":"code","35c10c07":"code","e3da5e23":"code","b48f49b6":"code","e55e8d71":"code","3603ed2f":"code","feb20a58":"code","975287b1":"code","22c7edc0":"code","4449ee8d":"code","515c9356":"markdown","2b7da780":"markdown","b81f16d0":"markdown","71db8b99":"markdown","a47a0cca":"markdown","6cf49343":"markdown","01120b98":"markdown","ad3a1b42":"markdown","7b802712":"markdown","9f70a083":"markdown","04f35749":"markdown","7b60924c":"markdown"},"source":{"097e7252":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers as L\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport random","bd4c447a":"BUFFER_SIZE = 1024\nBATCH_SIZE = 256\nINPUT_SHAPE = (32, 32, 3)\nAUTO = tf.data.AUTOTUNE\n\n(x_train, y_train), (x_valid, y_valid) = keras.datasets.cifar100.load_data()\nprint(f\"Training samples: {len(x_train)}\")\nprint(f\"Testing samples: {len(x_valid)}\")\n\ntrain_ds = tf.data.Dataset.from_tensor_slices(x_train)\ntrain_ds = train_ds.shuffle(BUFFER_SIZE).batch(BATCH_SIZE).prefetch(AUTO)\n\nvalid_ds = tf.data.Dataset.from_tensor_slices(x_valid)\nvalid_ds = valid_ds.batch(BATCH_SIZE).prefetch(AUTO)","78b2196d":"IMAGE_SIZE = 48  \nPATCH_SIZE = 6\nNUM_PATCHES = (IMAGE_SIZE \/\/ PATCH_SIZE) ** 2\nMASK_PROPORTION = 0.75","066b0ab5":"def get_train_augmentation_model():\n    model = keras.Sequential(\n        [\n            L.Rescaling(1 \/ 255.0),\n            L.Resizing(INPUT_SHAPE[0] + 20, INPUT_SHAPE[0] + 20),\n            L.RandomCrop(IMAGE_SIZE, IMAGE_SIZE),\n            L.RandomFlip(\"horizontal\"),\n        ],\n        name=\"train_data_augmentation\",\n    )\n    return model\n\n\ndef get_test_augmentation_model():\n    model = keras.Sequential(\n        [L.Rescaling(1 \/ 255.0), L.Resizing(IMAGE_SIZE, IMAGE_SIZE),],\n        name=\"test_data_augmentation\",\n    )\n    return model","17ce4b42":"class Patches(L.Layer):\n    def __init__(self, patch_size=PATCH_SIZE, **kwargs):\n        super().__init__(**kwargs)\n        self.patch_size = patch_size\n        self.resize = L.Reshape((-1, patch_size * patch_size * 3))\n\n    def call(self, images):\n        patches = tf.image.extract_patches(\n            images=images,\n            sizes=[1, self.patch_size, self.patch_size, 1],\n            strides=[1, self.patch_size, self.patch_size, 1],\n            rates=[1, 1, 1, 1],\n            padding=\"VALID\",\n        )\n        patches = self.resize(patches)\n        return patches\n\n    def show_patched_image(self, images, patches):\n        idx = np.random.choice(patches.shape[0])\n        print(f\"Index selected: {idx}.\")\n\n        plt.figure(figsize=(4, 4))\n        plt.imshow(keras.utils.array_to_img(images[idx]))\n        plt.suptitle('Original Image')\n        plt.axis(\"off\")\n        plt.show()\n\n        n = int(np.sqrt(patches.shape[1]))\n        plt.figure(figsize=(4, 4))\n        plt.suptitle('Patches')\n        for i, patch in enumerate(patches[idx]):\n            ax = plt.subplot(n, n, i + 1)\n            patch_img = tf.reshape(patch, (self.patch_size, self.patch_size, 3))\n            plt.imshow(keras.utils.img_to_array(patch_img))\n            plt.axis(\"off\")\n        plt.show()\n\n        return idx\n\n    def reconstruct_from_patch(self, patch):\n        num_patches = patch.shape[0]\n        n = int(np.sqrt(num_patches))\n        patch = tf.reshape(patch, (num_patches, self.patch_size, self.patch_size, 3))\n        rows = tf.split(patch, n, axis=0)\n        rows = [tf.concat(tf.unstack(x), axis=1) for x in rows]\n        reconstructed = tf.concat(rows, axis=0)\n        return reconstructed","61d387bf":"image_batch = next(iter(train_ds))\n\naugmentation_model = get_train_augmentation_model()\naugmented_images = augmentation_model(image_batch)\n\npatch_layer = Patches()\npatches = patch_layer(images=augmented_images)\nrandom_index = patch_layer.show_patched_image(images=augmented_images, patches=patches)","205d5c1d":"LAYER_NORM_EPS = 1e-6\nENC_PROJECTION_DIM = 128\nDEC_PROJECTION_DIM = 64\nENC_NUM_HEADS = 4\nENC_LAYERS = 6\nDEC_NUM_HEADS = 4\nDEC_LAYERS = (\n    2 \n)\nENC_TRANSFORMER_UNITS = [\n    ENC_PROJECTION_DIM * 2,\n    ENC_PROJECTION_DIM,\n] \nDEC_TRANSFORMER_UNITS = [\n    DEC_PROJECTION_DIM * 2,\n    DEC_PROJECTION_DIM,\n]","5dd14dfd":"EPOCHS = 250\nDOWNSTREAM_EPOCHS = 250","e730dda5":"class PatchEncoder(L.Layer):\n    def __init__(\n        self,\n        patch_size=PATCH_SIZE,\n        projection_dim=ENC_PROJECTION_DIM,\n        mask_proportion=MASK_PROPORTION,\n        downstream=False,\n        **kwargs,\n    ):\n        super().__init__(**kwargs)\n        self.patch_size = patch_size\n        self.projection_dim = projection_dim\n        self.mask_proportion = mask_proportion\n        self.downstream = downstream\n\n        self.mask_token = tf.Variable(\n            tf.random.normal([1, patch_size * patch_size * 3]), trainable=True\n        )\n\n    def build(self, input_shape):\n        (_, self.num_patches, self.patch_area) = input_shape\n        self.projection = L.Dense(units=self.projection_dim)\n        self.position_embedding = L.Embedding(\n            input_dim=self.num_patches, output_dim=self.projection_dim\n        )\n        self.num_mask = int(self.mask_proportion * self.num_patches)\n\n    def call(self, patches):\n        batch_size = tf.shape(patches)[0]\n        positions = tf.range(start=0, limit=self.num_patches, delta=1)\n        pos_embeddings = self.position_embedding(positions[tf.newaxis, ...])\n        pos_embeddings = tf.tile(\n            pos_embeddings, [batch_size, 1, 1]\n        )\n        patch_embeddings = (\n            self.projection(patches) + pos_embeddings\n        ) \n        if self.downstream:\n            return patch_embeddings\n        else:\n            mask_indices, unmask_indices = self.get_random_indices(batch_size)\n            unmasked_embeddings = tf.gather(\n                patch_embeddings, unmask_indices, axis=1, batch_dims=1\n            )\n            unmasked_positions = tf.gather(\n                pos_embeddings, unmask_indices, axis=1, batch_dims=1\n            )\n            masked_positions = tf.gather(\n                pos_embeddings, mask_indices, axis=1, batch_dims=1\n            ) \n            mask_tokens = tf.repeat(self.mask_token, repeats=self.num_mask, axis=0)\n            mask_tokens = tf.repeat(\n                mask_tokens[tf.newaxis, ...], repeats=batch_size, axis=0\n            )\n\n            masked_embeddings = self.projection(mask_tokens) + masked_positions\n            return (\n                unmasked_embeddings,\n                masked_embeddings,\n                unmasked_positions,\n                mask_indices,\n                unmask_indices,\n            )\n\n    def get_random_indices(self, batch_size):\n        rand_indices = tf.argsort(\n            tf.random.uniform(shape=(batch_size, self.num_patches)), axis=-1\n        )\n        mask_indices = rand_indices[:, : self.num_mask]\n        unmask_indices = rand_indices[:, self.num_mask :]\n        return mask_indices, unmask_indices\n\n    def generate_masked_image(self, patches, unmask_indices):\n        idx = np.random.choice(patches.shape[0])\n        patch = patches[idx]\n        unmask_index = unmask_indices[idx]\n        new_patch = np.zeros_like(patch)\n        count = 0\n        for i in range(unmask_index.shape[0]):\n            new_patch[unmask_index[i]] = patch[unmask_index[i]]\n        return new_patch, idx","92987180":"def dense_projection(x, dropout_rate, hidden_units):\n    for units in hidden_units:\n        x = L.Dense(units, activation=tf.nn.gelu)(x)\n        x = L.Dropout(dropout_rate)(x)\n    return x","d66eeab7":"def create_encoder(num_heads=ENC_NUM_HEADS, num_layers=ENC_LAYERS):\n    inputs = L.Input((None, ENC_PROJECTION_DIM))\n    x = inputs\n\n    for _ in range(num_layers):\n        x1 = L.LayerNormalization(epsilon=LAYER_NORM_EPS)(x)\n        attention_output = L.MultiHeadAttention(\n            num_heads=num_heads, key_dim=ENC_PROJECTION_DIM, dropout=0.1\n        )(x1, x1)\n        x2 = L.Add()([attention_output, x])\n        x3 = L.LayerNormalization(epsilon=LAYER_NORM_EPS)(x2)\n        x3 = dense_projection(x3, hidden_units=ENC_TRANSFORMER_UNITS, dropout_rate=0.1)\n        x = L.Add()([x3, x2])\n\n    outputs = L.LayerNormalization(epsilon=LAYER_NORM_EPS)(x)\n    return keras.Model(inputs, outputs, name=\"mae_encoder\")","35c10c07":"def create_decoder(\n    num_layers=DEC_LAYERS, num_heads=DEC_NUM_HEADS, image_size=IMAGE_SIZE\n):\n    inputs = L.Input((NUM_PATCHES, ENC_PROJECTION_DIM))\n    x = L.Dense(DEC_PROJECTION_DIM)(inputs)\n\n    for _ in range(num_layers):\n        x1 = L.LayerNormalization(epsilon=LAYER_NORM_EPS)(x)\n        attention_output = L.MultiHeadAttention(\n            num_heads=num_heads, key_dim=DEC_PROJECTION_DIM, dropout=0.1\n        )(x1, x1)\n        x2 = L.Add()([attention_output, x])\n        x3 = L.LayerNormalization(epsilon=LAYER_NORM_EPS)(x2)\n        x3 = dense_projection(x3, hidden_units=DEC_TRANSFORMER_UNITS, dropout_rate=0.1)\n        x = L.Add()([x3, x2])\n\n    x = L.LayerNormalization(epsilon=LAYER_NORM_EPS)(x)\n    x = L.Flatten()(x)\n    pre_final = L.Dense(units=image_size * image_size * 3, activation=\"sigmoid\")(x)\n    outputs = L.Reshape((image_size, image_size, 3))(pre_final)\n\n    return keras.Model(inputs, outputs, name=\"mae_decoder\")","e3da5e23":"class MaskedAutoencoder(keras.Model):\n    def __init__(\n        self,\n        train_augmentation_model,\n        test_augmentation_model,\n        patch_layer,\n        patch_encoder,\n        encoder,\n        decoder,\n        **kwargs,\n    ):\n        super().__init__(**kwargs)\n        self.train_augmentation_model = train_augmentation_model\n        self.test_augmentation_model = test_augmentation_model\n        self.patch_layer = patch_layer\n        self.patch_encoder = patch_encoder\n        self.encoder = encoder\n        self.decoder = decoder\n\n    def calculate_loss(self, images, test=False):\n        if test:\n            augmented_images = self.test_augmentation_model(images)\n        else:\n            augmented_images = self.train_augmentation_model(images)\n\n        patches = self.patch_layer(augmented_images)\n        (\n            unmasked_embeddings,\n            masked_embeddings,\n            unmasked_positions,\n            mask_indices,\n            unmask_indices,\n        ) = self.patch_encoder(patches)\n\n        encoder_outputs = self.encoder(unmasked_embeddings)\n        encoder_outputs = encoder_outputs + unmasked_positions\n        decoder_inputs = tf.concat([encoder_outputs, masked_embeddings], axis=1)\n        decoder_outputs = self.decoder(decoder_inputs)\n        decoder_patches = self.patch_layer(decoder_outputs)\n\n        loss_patch = tf.gather(patches, mask_indices, axis=1, batch_dims=1)\n        loss_output = tf.gather(decoder_patches, mask_indices, axis=1, batch_dims=1)\n\n        total_loss = self.compiled_loss(loss_patch, loss_output)\n\n        return total_loss, loss_patch, loss_output\n\n    def train_step(self, images):\n        with tf.GradientTape() as tape:\n            total_loss, loss_patch, loss_output = self.calculate_loss(images)\n\n        train_vars = [\n            self.train_augmentation_model.trainable_variables,\n            self.patch_layer.trainable_variables,\n            self.patch_encoder.trainable_variables,\n            self.encoder.trainable_variables,\n            self.decoder.trainable_variables,\n        ]\n        grads = tape.gradient(total_loss, train_vars)\n        tv_list = []\n        for (grad, var) in zip(grads, train_vars):\n            for g, v in zip(grad, var):\n                tv_list.append((g, v))\n        self.optimizer.apply_gradients(tv_list)\n        self.compiled_metrics.update_state(loss_patch, loss_output)\n        return {m.name: m.result() for m in self.metrics}\n\n    def test_step(self, images):\n        total_loss, loss_patch, loss_output = self.calculate_loss(images, test=True)\n        self.compiled_metrics.update_state(loss_patch, loss_output)\n        return {m.name: m.result() for m in self.metrics}\n","b48f49b6":"test_images = next(iter(valid_ds))\n\n\nclass TrainMonitor(keras.callbacks.Callback):\n    def __init__(self, epoch_interval=None):\n        self.epoch_interval = epoch_interval\n\n    def on_epoch_end(self, epoch, logs=None):\n        if self.epoch_interval and epoch % self.epoch_interval == 0:\n            test_augmented_images = self.model.test_augmentation_model(test_images)\n            test_patches = self.model.patch_layer(test_augmented_images)\n            (\n                test_unmasked_embeddings,\n                test_masked_embeddings,\n                test_unmasked_positions,\n                test_mask_indices,\n                test_unmask_indices,\n            ) = self.model.patch_encoder(test_patches)\n            test_encoder_outputs = self.model.encoder(test_unmasked_embeddings)\n            test_encoder_outputs = test_encoder_outputs + test_unmasked_positions\n            test_decoder_inputs = tf.concat(\n                [test_encoder_outputs, test_masked_embeddings], axis=1\n            )\n            test_decoder_outputs = self.model.decoder(test_decoder_inputs)\n\n            # Show a maksed patch image.\n            test_masked_patch, idx = self.model.patch_encoder.generate_masked_image(\n                test_patches, test_unmask_indices\n            )\n            print(f\"\\nIdx chosen: {idx}\")\n            original_image = test_augmented_images[idx]\n            masked_image = self.model.patch_layer.reconstruct_from_patch(\n                test_masked_patch\n            )\n            reconstructed_image = test_decoder_outputs[idx]\n\n            fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(15, 5))\n            ax[0].imshow(original_image)\n            ax[0].set_title(f\"Original: {epoch:03d}\")\n\n            ax[1].imshow(masked_image)\n            ax[1].set_title(f\"Masked: {epoch:03d}\")\n\n            ax[2].imshow(reconstructed_image)\n            ax[2].set_title(f\"Resonstructed: {epoch:03d}\")\n\n            plt.show()\n            plt.close()","e55e8d71":"train_augmentation_model = get_train_augmentation_model()\ntest_augmentation_model = get_test_augmentation_model()\npatch_layer = Patches()\npatch_encoder = PatchEncoder()\nencoder = create_encoder()\ndecoder = create_decoder()\n\nmae_model = MaskedAutoencoder(\n    train_augmentation_model=train_augmentation_model,\n    test_augmentation_model=test_augmentation_model,\n    patch_layer=patch_layer,\n    patch_encoder=patch_encoder,\n    encoder=encoder,\n    decoder=decoder,\n)\n\n\nmae_model.compile(\n    optimizer='adam', loss=keras.losses.MeanSquaredError(), metrics=[\"mae\"]\n)","3603ed2f":"keras.utils.plot_model(encoder, show_shapes=True, to_file=\"encoder.png\")\nkeras.utils.plot_model(decoder, show_shapes=True, to_file=\"decoder.png\")\nfig, ax = plt.subplots(1, 2, figsize=(20, 60))\nax[0].imshow(plt.imread('encoder.png'))\nax[0].set_title('Encoder', fontsize=12)\nax[0].axis(\"off\")\nax[1].imshow(plt.imread('decoder.png'))\nax[1].set_title('Decoder', fontsize=12)\nax[1].axis(\"off\");","feb20a58":"es = keras.callbacks.EarlyStopping(min_delta=1e-4, patience=5, verbose=1, restore_best_weights=True)\nrlp = keras.callbacks.ReduceLROnPlateau(patience=2, verbose=1)\n\nhistory = mae_model.fit(\n    train_ds, epochs=EPOCHS, validation_data=valid_ds, callbacks=[TrainMonitor(epoch_interval=5), es, rlp],\n)\n\nloss, mae = mae_model.evaluate(valid_ds)\nprint(f\"Loss: {loss:.2f}\")\nprint(f\"MAE: {mae:.2f}\")","975287b1":"train_augmentation_model = mae_model.train_augmentation_model\ntest_augmentation_model = mae_model.test_augmentation_model\n\npatch_layer = mae_model.patch_layer\npatch_encoder = mae_model.patch_encoder\npatch_encoder.downstream = True\n\nencoder = mae_model.encoder\n\ndownstream_model = keras.Sequential(\n    [\n        L.Input((IMAGE_SIZE, IMAGE_SIZE, 3)),\n        patch_layer,\n        patch_encoder,\n        encoder,\n        L.BatchNormalization(),\n        L.GlobalAveragePooling1D(),\n        L.Dense(10, activation=\"softmax\"),\n    ],\n    name=\"downstream_model\",\n)","22c7edc0":"def prepare_data(images, labels, is_train=True):\n    if is_train:\n        augmentation_model = train_augmentation_model\n    else:\n        augmentation_model = test_augmentation_model\n\n    dataset = tf.data.Dataset.from_tensor_slices((images, labels))\n    if is_train:\n        dataset = dataset.shuffle(BUFFER_SIZE)\n\n    dataset = dataset.batch(BATCH_SIZE).map(\n        lambda x, y: (augmentation_model(x), y), num_parallel_calls=AUTO\n    )\n    return dataset.prefetch(AUTO)\n\n(x_train, y_train), (x_valid, y_valid) = keras.datasets.cifar10.load_data()\ntrain_ds = prepare_data(x_train, y_train)\nvalid_ds = prepare_data(x_valid, y_valid, is_train=False)","4449ee8d":"downstream_model.compile(\n    optimizer='adam', loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n)\n\nes = keras.callbacks.EarlyStopping(min_delta=1e-4, patience=5, verbose=1, restore_best_weights=True)\nrlp = keras.callbacks.ReduceLROnPlateau(patience=2, verbose=1)\n\ndownstream_model.fit(train_ds, validation_data=valid_ds, epochs=DOWNSTREAM_EPOCHS, callbacks=[es, rlp])\n\nloss, accuracy = downstream_model.evaluate(valid_ds)\naccuracy = round(accuracy * 100, 2)\nprint(f\"Accuracy on the test set: {accuracy}%.\")","515c9356":"**A layer for extracting patches from images**\n\nThis layer takes images as input and divides them into patches. ","2b7da780":"**MAE Trainer**","b81f16d0":"**Monitoring Callback**","71db8b99":"**MAE Encoder**","a47a0cca":"# Masked Autoencoder\n\nImplementation of [Masked Autoencoders Are Scalable Vision Learners](https:\/\/arxiv.org\/pdf\/2111.06377.pdf) paper on CIFAR datasets.\nA small ViT(introduced in [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https:\/\/arxiv.org\/pdf\/2010.11929.pdf)) is trained on CIFAR 100 dataset and then evaluated on CIFAR 10 dataset on a downstream classification task.","6cf49343":"# Model","01120b98":"**MAE Decoder**","ad3a1b42":"# Augmentation\n\nThe authors of this paper point out that Masked Autoencoders do not rely on augmentations hence propose a simple augmentation pipeline of:\n* Resizing\n* Random cropping\n* Random horizontal flipping","7b802712":"**Encoder and Patch Masking**\n\nThis layer includes masking and encoding the patches.","9f70a083":"# Evaluation on Downstream Classification","04f35749":"# References\n1. [Masked Autoencoders Are Scalable Vision Learners](https:\/\/arxiv.org\/pdf\/2111.06377.pdf)\n2. [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https:\/\/arxiv.org\/pdf\/2010.11929.pdf)","7b60924c":"**Dense Projection**"}}