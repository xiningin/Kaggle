{"cell_type":{"da1c4062":"code","fddbf8e6":"code","8f3ce956":"code","eba5582c":"code","5b515a1d":"code","4a033156":"code","59019582":"code","d8137197":"code","27ad1bcf":"code","4275affc":"code","d3a72a91":"code","c58ac499":"code","8a653c2a":"code","cc438864":"code","046dc1e7":"code","51511e6b":"code","5a95b276":"code","7b294c6d":"code","aa3136e8":"code","c91cf8d4":"code","d2f330ed":"code","63282ca2":"code","871c459d":"code","5ec1e342":"code","ee2ad177":"code","d804e189":"code","0c25ede9":"code","8360bb7e":"code","ac4ef2c3":"code","a99b1b2b":"code","e65dfbc8":"code","7df5bc25":"code","181faf6a":"code","3adb8f9e":"code","1b760768":"code","10174fd8":"code","94f66083":"markdown","7e18986e":"markdown","4f964e6f":"markdown","4355a966":"markdown","a1296324":"markdown","47569d56":"markdown","c195884f":"markdown","2182d6ca":"markdown","54e41103":"markdown","70e4b08d":"markdown","9d3115eb":"markdown","286371da":"markdown","f8f3bf69":"markdown","a97d80c2":"markdown","cf148f85":"markdown","77ec4725":"markdown","788c3699":"markdown"},"source":{"da1c4062":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os, gc, warnings\nimport random\nimport datetime\nfrom tqdm.notebook import tqdm\n\nfrom scipy import stats\n\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# from pandas.plotting import register_matplotlib_converters\n# register_matplotlib_converters()\n\nimport sklearn\nfrom sklearn.model_selection import KFold, StratifiedKFold\nfrom sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler\n\nimport optuna\nimport lightgbm as lgb\nfrom optuna.integration import LightGBMPruningCallback","fddbf8e6":"path = '..\/input\/tabular-playground-series-sep-2021\/'\n# Input data files are available in the \"..\/input\/\" directory.\nfor dirname, _, filenames in os.walk(path):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","8f3ce956":"def load_data(source, dtypes, path=path):\n    ''' load tables '''\n    assert source in ['train', 'test']\n    df = pd.read_csv(f'{path}\/{source}.csv', index_col=\"id\", dtype= dtypes)\n    return df","eba5582c":"%%time\ntrain = load_data('train', None)\nprint(f\"Data shape: {train.shape}\")\ntrain.sample(4)","5b515a1d":"%%time\ntest = load_data('test', None)\nprint(f\"Data shape: {test.shape}\")\ntest.sample(2)","4a033156":"target_name = \"claim\"\nfeatures = [col for col in train.columns if col not in [target_name]]","59019582":"def missing_statistics(df):    \n    statitics = pd.DataFrame(df.isnull().sum()).reset_index()\n    statitics.columns=['COLUMN NAME',\"MISSING VALUES\"]\n    statitics['TOTAL ROWS'] = df.shape[0]\n    statitics['% MISSING'] = round((statitics['MISSING VALUES']\/statitics['TOTAL ROWS'])*100,2)\n    return statitics","d8137197":"miss = missing_statistics(train)\nmiss","27ad1bcf":"miss[\"% MISSING\"].describe()","4275affc":"del miss","d3a72a91":"train.describe().T","c58ac499":"train[\"std\"] = train[features].std(axis=1)\ntest[\"std\"]  = test[features].std(axis=1)\ntrain[\"n_missing\"] = train[features].isna().sum(axis=1)\ntest[\"n_missing\"]  = test[features].isna().sum(axis=1)\nfeatures += ['std', 'n_missing']\nn_missing = train[\"n_missing\"].copy()","8a653c2a":"train[features] = train[features].fillna(train[features].mean())\ntest[features]  = test[features].fillna(test[features].mean())","cc438864":"scaler = RobustScaler()\n\ntrain[features] = scaler.fit_transform(train[features])\ntest[features]  = scaler.transform(test[features])","046dc1e7":"stats.skew([1,2,3,4,5])","51511e6b":"fig, axes = plt.subplots(11,11,figsize=(16, 16))\naxes = axes.flatten()\n\nfor idx, ax in tqdm(enumerate(axes)):\n    try:\n        idx += 1\n        values = train[f\"f{idx}\"].values\n        sns.kdeplot(data=train, x=f'f{idx}', \n                    fill=True, \n                    ax=ax)\n        sns.kdeplot(data=test, x=f'f{idx}', \n                    fill=True, \n                    ax=ax)\n        ax.set_xticks([])\n        ax.set_yticks([])\n        ax.set_xlabel(f'skew:{round(stats.skew(values), 2)}, kurt:{round(stats.kurtosis(values),2)}')\n        ax.set_ylabel('')\n        ax.spines['left'].set_visible(False)\n        ax.set_title(f'f{idx}', loc='right', weight='bold', fontsize=10)\n    except Exception as e:\n        print(e)\n\nfig.supxlabel('Average by class (by feature)', ha='center', fontweight='bold')\n\nfig.tight_layout()\nplt.show()","5a95b276":"def determine_skewed_columns(df, skew_top_threshold, skew_low_threshold):\n    col_names = df.columns[:-1]\n    skew = stats.skew(df.values)[:-1]\n    mask = (skew >= skew_top_threshold) | (skew <= skew_low_threshold)\n    \n    return col_names[mask]","7b294c6d":"skew_columns_train = determine_skewed_columns(train, 1, -1)","aa3136e8":"skew_columns_test = determine_skewed_columns(test, 1, -1)","c91cf8d4":"list(set(skew_columns_train) - set(skew_columns_test))","d2f330ed":"skew_columns = skew_columns_test","63282ca2":"%%time\nfig, ax = plt.subplots(1, 1, figsize=(12 , 12))\n\ncorr = train.corr()\n\nmask = np.zeros_like(corr, dtype=np.bool_)\nmask[np.triu_indices_from(mask)] = True\n\nsns.heatmap(corr, ax=ax,\n        square=True, center=0, linewidth=1,\n        cmap=sns.diverging_palette(240, 10, as_cmap=True),\n        cbar_kws={\"shrink\": .82},    \n        mask=mask\n       ) \n\nax.set_title(f'Correlation', loc='left', fontweight='bold')     \n\nplt.show()","871c459d":"N_SPLITS = 5\nN_ESTIMATORS = 10000\nEARLY_STOPING_ROUND = 200\nVERBOSE = 1000\nSEED = 2021\n\nN_BINS = 20","5ec1e342":"best_params = {\n    'objective': 'binary',\n    'n_estimators' : N_ESTIMATORS,\n    'random_state' : SEED,\n    'learning_rate': 0.030305148136078583,\n    'subsample'    : 0.5150617351169511,\n    'reg_alpha'    : 0.2491671010019858,\n    'reg_labmda'   : 0.03618390402626644,\n    'subsample_freq': 1,\n    'colsample_bytree': 0.3917166178297055,\n    'min_child_weight': 2,\n    'min_child_sample': 48,\n    'max_depth': 7,\n}","ee2ad177":"def fit_regressor(df, tr_idx, val_idx, features_arr, target_str, params):\n    # train\n    tr_x, tr_y = df[features_arr].iloc[tr_idx], df[target_str][tr_idx]\n    # evaluating (\"test\")\n    vl_x, vl_y = df[features_arr].iloc[val_idx], df[target_str][val_idx]\n    print({'df size':len(tr_x), 'eval size':len(vl_x)})\n\n    clf = lgb.LGBMClassifier(**params)\n    # Metric: Root Mean Square Error (RMSE), it tells you how concentrated the data is around the line of best fit.\n    clf.fit(tr_x, tr_y,\n            eval_set=[(vl_x, vl_y)],\n            early_stopping_rounds=EARLY_STOPING_ROUND,\n            eval_metric=\"auc\",\n            verbose=VERBOSE)\n    #\"l2\"\n    return clf","d804e189":"kf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED) # Provides train\/test indices to split data in train\/test sets.\n# kf = KFold(n_splits=folds, shuffle=True, random_state=seed) #n_splits=folds\n    \n## generating 5 train\/test pair of index_arrays, and analizing wich give the better results.\nmodels = []\nfor tr_idx, val_idx in tqdm(kf.split(X=train, y=n_missing), total=N_SPLITS): # train\/test indices\n    clf = fit_regressor(train, tr_idx, val_idx, features, target_name, best_params)\n    models.append(clf)\n    \ngc.collect() # trigger a manual garbage collection process, cleans up a huge amount of objects.","0c25ede9":"def evaluate(valid_targets, probs, name):\n    from sklearn.metrics import classification_report, accuracy_score, log_loss, roc_auc_score\n    \n    y_pred = np.array(probs > 0.5, dtype=int)\n    acc = accuracy_score(valid_targets, y_pred)\n    loss = log_loss(valid_targets, y_pred)\n    auc = roc_auc_score(valid_targets, probs)\n    print(\"Accuracy score: %.2f\"%(acc))\n    print(\"Log loss: %.2f\"%(loss))\n    print(\"AUC score:\", auc)\n    print(\"Classification report:\")\n    print(classification_report(valid_targets, y_pred))\n    return {\n        \"name\": name, \n        \"accuracy_score\": acc, \n        \"log_loss\": loss, \n        \"auc\": auc\n    }","8360bb7e":"probs = [model.predict_proba(train[features]) for model in models]","ac4ef2c3":"np.shape(probs)","a99b1b2b":"probs = np.mean(probs, axis=0)\nprobs = probs.T[1]","e65dfbc8":"evaluate(train[target_name], probs, \"LGBMClassifier\")","7df5bc25":"_ = lgb.plot_importance(models[0], importance_type='split', figsize=(20,20)) , #\"gain\"","181faf6a":"# preds = [model.predict(test_update) for model in models]\ntest_probs = [model.predict_proba(test[features]) for model in models]\nout_loss = np.mean(test_probs, axis=0) # Using all the models and making the mean between each other.","3adb8f9e":"submission = pd.read_csv(f'{path}\/sample_solution.csv')\nsubmission['claim'] = out_loss.T[1]","1b760768":"submission","10174fd8":"submission.to_csv(f'.\/submission.csv', index=False)\nsubmission.head(9)","94f66083":"## Check if in the data set there is missing data.","7e18986e":"## Distribution Check.","4f964e6f":"# Determine wich columns are skewed\n* If stats.skew is higher than 1 is skewed data with right tail.\n* If stats.skew is less than -1 is skewed data with left tail.","4355a966":"# Feature scalin","a1296324":"# Filling missign data with mean\n* An mean of 1.597059% data is missing in each input column of this data set\n* Due to I have now context about each row and there is less than 2& missing data, I decided to use the mean value of the column to fill each NaN gap.","47569d56":"# Evaluation method in train data","c195884f":"# Confusion matrix","2182d6ca":"# Check in prediction sample","54e41103":"## Discrete features?\n* **All the features are decimal, no categoraical input parameters.**","70e4b08d":"# Algorithm ID3 regresion\n1. Calculate the initial system entropy based on the **objective** variable to predict.\n    * Entropy: Determine wich parameters are more important than others to have a better sort in the tree.","9d3115eb":"## Kurtosis\n* If near 0: the distribuiton is cole to a normal one.\n* If it is positive, the values are really proxim to the central value, the data has no big tails.\n* If it is negative, less values centread in mean and big tails","286371da":"# Important step\n* To create in cross validation a proper missing data distribution check \"n_missing\" dataset implementation.","f8f3bf69":"# LGBMRegressor model prepare","a97d80c2":"## Fisher asymmetry\n* If the value is close to 0, it means: normal distribution\n* If it is more positive: left skeewed distribution\n* If it is more negative: rigth skeewed distribution","cf148f85":"# Some initial analize","77ec4725":"### Note:\n**StratifiedKFold**: We are forcing the model to train with the missing data properly distributed in each train\/test sample.","788c3699":"# Save submision"}}