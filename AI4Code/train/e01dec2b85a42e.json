{"cell_type":{"5686028e":"code","1fd4fb0e":"code","dab0d285":"code","41547e65":"code","d73a6931":"code","cd27e504":"code","f50186cc":"code","f60a599c":"code","7098ebfb":"markdown","32f4d8be":"markdown","540d1831":"markdown"},"source":{"5686028e":"import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.metrics import mean_absolute_error\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom xgboost import XGBRegressor\n\nfrom os.path import join","1fd4fb0e":"dir_path = '..\/input\/home-data-for-ml-course\/'\ntrain_df = pd.read_csv(join(dir_path, 'train.csv'))\ntest_df = pd.read_csv(join(dir_path, 'test.csv'))\n\ny = train_df.SalePrice\nX = train_df.drop(['SalePrice'], axis=1)","dab0d285":"categorical_cols = [ \n    cname\n    for cname in X.columns\n    if X[cname].dtype == 'object'\n    ## normally would keep hotencoding to small categories\n    ## but including neighborhoods seems important\n    # and X[cname].nunique() < 10\n]\n\nnumerical_cols = [\n    cname\n    for cname in X.columns\n    if X[cname].dtype in ['int64', 'float64']    \n]","41547e65":"# split the data\ntrain_X, val_X, train_y, val_y = train_test_split(X, y, train_size=0.8, random_state=0)","d73a6931":"num_transformer = SimpleImputer(strategy='median')\ncat_transformer = Pipeline([\n    ('imputer', SimpleImputer(strategy='most_frequent')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n])\n\n\npreprocessor = ColumnTransformer([\n    ('numerical',   num_transformer, numerical_cols),\n    ('categorical', cat_transformer, categorical_cols),\n])","cd27e504":"model = XGBRegressor(\n    random_state=0,\n    # high n_estimators and low learning_rate as \n    n_estimators=2500,\n    learning_rate=0.04\n)","f50186cc":"final_pipe = Pipeline([\n    ('preprocessor', preprocessor),\n    ('model', model)\n])\n\nfinal_pipe.fit(train_X, train_y)\npredictions = final_pipe.predict(val_X)\n\n# Score it!\nmean_absolute_error(val_y, predictions)","f60a599c":"pred_price = final_pipe.predict(test_df)\n\npd.DataFrame({\n    'Id': test_df.Id,\n    'SalePrice': pred_price\n}).to_csv('submission.csv', index=False)","7098ebfb":"# Preprocessing Pipelines\nHandle:\n- missing data\n- categorical data (One-Hot Encoding)","32f4d8be":"# Submission","540d1831":"# XGBoost Model\nSetting `n_estimators` really high (normally 100-1000) and `learning_rate` lower than the default (0.10), although it takes longer to train, is recommended by the [xgboost tutorial](https:\/\/www.kaggle.com\/alexisbcook\/xgboost#learning_rate)."}}