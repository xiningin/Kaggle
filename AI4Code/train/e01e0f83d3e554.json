{"cell_type":{"6392ef1e":"code","e14c469f":"code","14e0a349":"code","021af362":"code","58412547":"code","987cf0e3":"code","d2eb3b9c":"code","e1e193da":"code","f731f6c2":"code","7d817451":"code","a8917337":"code","d29f2ff4":"code","43de9fc3":"code","a5f85c35":"code","dc30d9e3":"code","84ac30f3":"code","ad7aa9ca":"code","eae5d07e":"code","b07e9363":"code","5e1aef2a":"code","5e13ca4b":"code","54049d30":"code","96c2745e":"code","647859e6":"code","d68a0134":"code","9705f4b4":"code","53fe92d0":"code","156a3185":"code","c5c7e8ae":"code","bf9cd344":"code","958b972d":"code","390c81c1":"code","41b7339d":"code","7580f086":"code","f93b5fbf":"code","da4fb183":"code","7cc95c5e":"code","5d75fc5d":"code","4337afd6":"code","2da7b133":"code","0e1317d4":"code","94b64a90":"code","55b0e809":"code","ce3b56c7":"code","8d6fd8c3":"code","534f711e":"code","dc06625d":"code","26e8cff8":"code","3d7b6ad2":"code","3f244ada":"code","40b0c61a":"code","a4b76f53":"code","f54d4175":"code","0ec82bbf":"code","c756a4d5":"markdown","32e53b5f":"markdown","d66731b6":"markdown","2d99dc7c":"markdown","86c71942":"markdown","a85a6fa0":"markdown","e3e6cdbe":"markdown","82fe4c8f":"markdown","0a6d2fcb":"markdown","2cb1915f":"markdown","629a0a97":"markdown","da338d57":"markdown","226919d0":"markdown","7628e951":"markdown","6e0134db":"markdown","9830e44f":"markdown","c0c5e6e6":"markdown","b6f1047b":"markdown"},"source":{"6392ef1e":"import numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\nimport seaborn as sns\nsns.set()","e14c469f":"raw_data = pd.read_csv('..\/input\/used-cars-price\/Used Car Price.csv')\nraw_data.head()","14e0a349":"raw_data.describe(include='all')","021af362":"data = raw_data.drop(['Model'],axis=1)\ndata.describe(include='all')","58412547":"data.isnull().sum()","987cf0e3":"data_no_mv = data.dropna(axis=0)","d2eb3b9c":"data_no_mv.describe(include='all')","e1e193da":"sns.kdeplot(data_no_mv['Price'], shade = True)","f731f6c2":"q = data_no_mv['Price'].quantile(0.99)\ndata_1 = data_no_mv[data_no_mv['Price']<q]\ndata_1.describe(include='all')","7d817451":"sns.kdeplot(data_1['Price'], shade = True)","a8917337":"sns.kdeplot(data_no_mv['Mileage'], shade = True)","d29f2ff4":"q = data_1['Mileage'].quantile(0.99)\ndata_2 = data_1[data_1['Mileage']<q]","43de9fc3":"sns.kdeplot(data_2['Mileage'], shade = True)","a5f85c35":"sns.kdeplot(data_no_mv['EngineV'], shade = True)","dc30d9e3":"data_3 = data_2[data_2['EngineV']<6.5]","84ac30f3":"sns.kdeplot(data_3['EngineV'], shade = True)","ad7aa9ca":"sns.kdeplot(data_no_mv['Year'], shade = True)","eae5d07e":"q = data_3['Year'].quantile(0.01)\ndata_4 = data_3[data_3['Year']>q]","b07e9363":"sns.kdeplot(data_4['Year'], shade = True)","5e1aef2a":"data_cleaned = data_4.reset_index(drop=True)","5e13ca4b":"data_cleaned.describe(include='all')","54049d30":"fig, (ax1, ax2, ax3) = plt.subplots(1, 3, sharey=True, figsize =(15,3))\n\nax1.scatter(data_cleaned['Year'],data_cleaned['Price'])\nax1.set_title('Price and Year')\n\nax2.scatter(data_cleaned['EngineV'],data_cleaned['Price'])\nax2.set_title('Price and EngineV')\n\nax3.scatter(data_cleaned['Mileage'],data_cleaned['Price'])\nax3.set_title('Price and Mileage')\n\nplt.show()","96c2745e":"log_price = np.log(data_cleaned['Price'])\ndata_cleaned['log_price'] = log_price\ndata_cleaned","647859e6":"fig, (ax1, ax2, ax3) = plt.subplots(1, 3, sharey=False, figsize =(15,3))\n\nax1.scatter(data_cleaned['Year'],data_cleaned['log_price'])\nax1.set_title('Log Price and Year')\n\nax2.scatter(data_cleaned['EngineV'],data_cleaned['log_price'])\nax2.set_title('Log Price and EngineV')\n\nax3.scatter(data_cleaned['Mileage'],data_cleaned['log_price'])\nax3.set_title('Log Price and Mileage')\n\n\nplt.show()","d68a0134":"data_cleaned_2 = data_cleaned.drop(['Price'],axis=1)","9705f4b4":"data_cleaned_2.columns","53fe92d0":"from statsmodels.stats.outliers_influence import variance_inflation_factor\n\nvariables = data_cleaned_2[['Mileage','Year','EngineV']]\n\nvif = pd.DataFrame()\n\nvif[\"VIF\"] = [variance_inflation_factor(variables.values, i) for i in range(variables.shape[1])]\n\nvif[\"features\"] = variables.columns","156a3185":"data_no_multicollinearity = data_cleaned_2.drop(['Year'],axis=1)","c5c7e8ae":"data_with_dummies = pd.get_dummies(data_no_multicollinearity, drop_first = True)\n\ndata_with_dummies","bf9cd344":"data_with_dummies.columns.values","958b972d":"# Now we create a list of our desired columns order as follows:\n\nnew_columns = ['log_price', 'Mileage', 'EngineV', 'Brand_BMW',\n       'Brand_Mercedes-Benz', 'Brand_Mitsubishi', 'Brand_Renault',\n       'Brand_Toyota', 'Brand_Volkswagen', 'Body_hatch', 'Body_other',\n       'Body_sedan', 'Body_vagon', 'Body_van', 'Engine Type_Gas',\n       'Engine Type_Other', 'Engine Type_Petrol', 'Registration_yes']","390c81c1":"data_preprocessed = data_with_dummies[new_columns]\ndata_preprocessed","41b7339d":"targets = data_preprocessed['log_price']\n\ninputs = data_preprocessed.drop(['log_price'],axis=1)","7580f086":"from sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\n\nscaler.fit(inputs)\n\ninputs_scaled = scaler.transform(inputs)","f93b5fbf":"from sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split(inputs_scaled, targets, test_size=0.2, random_state=42)","da4fb183":"reg = LinearRegression()\n\nreg.fit(x_train,y_train)","7cc95c5e":"y_hat = reg.predict(x_train)","5d75fc5d":"plt.scatter(y_train, y_hat)\n\nplt.xlabel('Targets (y_train)',size=15)\nplt.ylabel('Predictions (y_hat)',size=15)\n\nplt.xlim(6,13)\nplt.ylim(6,13)\n\nplt.show()","4337afd6":"sns.kdeplot(y_train - y_hat, shade = True)\nplt.title(\"Residuals PDF\", size=15)","2da7b133":"reg.score(x_train,y_train)","0e1317d4":"reg.intercept_","94b64a90":"reg.coef_","55b0e809":"reg_summary = pd.DataFrame(inputs.columns.values, columns=['Features'])\n\nreg_summary['Weights'] = reg.coef_\n\nreg_summary","ce3b56c7":"y_hat_test = reg.predict(x_test)","8d6fd8c3":"plt.scatter(y_test, y_hat_test)\n\nplt.xlabel('Targets (y_test)',size=15)\nplt.ylabel('Predictions (y_hat_test)',size=15)\n\nplt.xlim(6,13)\nplt.ylim(6,13)\n\nplt.show()","534f711e":"df_performance = pd.DataFrame(np.exp(y_hat_test), columns=['Prediction'])\n\ndf_performance.head()","dc06625d":"df_performance['Target'] = np.exp(y_test)\n\ndf_performance.head()","26e8cff8":"y_test.head()","3d7b6ad2":"y_test = y_test.reset_index(drop=True)\ny_test.head()","3f244ada":"df_performance['Target'] = np.exp(y_test)\ndf_performance","40b0c61a":"df_performance['Residual'] = df_performance['Target'] - df_performance['Prediction']\n\ndf_performance['Percent Error'] = np.absolute(df_performance['Residual']\/df_performance['Target']*100)\n\ndf_performance","a4b76f53":"df_performance.describe()","f54d4175":"pd.options.display.max_rows = 999\n\ndf_performance.sort_values(by=['Percent Error'], inplace=True)\n\nnp.round(df_performance, 3)","0ec82bbf":"np.round(reg_summary, 2)","c756a4d5":"![download (3).png](attachment:f355bb4d-37bf-48ee-a1d3-2064cec2da34.png)\n\n## Step 5: Testing our Model","32e53b5f":"## Step 3: Create dummy Variables\u00b6\nIn session 14 we create a dummy variable by using map\n\ndata = raw_data.copy()\n\ndata['Attendance'] = data['Attendance'].map({'Yes': 1, 'No': 0})\n\n#### Now in what follows, we will create dummies by a simple code of Pandas\n\n#### Note that if we have N categories for a feature, we have to create N-1 dummies\n![download (1).png](attachment:2f4710af-82cc-49d0-8a26-0d4372fce46f.png)","d66731b6":"### 2- No Endogeneity\nWe will talk about this issue later.\n\nKeyWords: Hausman test, IV, 2SLS, GMM, ...\n\nSee the following link:\n\nA good example: https:\/\/python.quantecon.org\/ols.html#Endogeneity\n\nHow we can intrpret our results: https:\/\/stats.stackexchange.com\/questions\/210696\/how-to-interpret-hausman-test-results\n\nDocumentations of linearmodels library: https:\/\/bashtage.github.io\/linearmodels\/doc\/iv\/introduction.html\n\n### 3- Normality and Homoscedasticity\nJust check the above graphs. Since intercept has been included in our model, so the mean of error is ZERO.\n\n### 4- No Autocorrelation\nWe should not be worry, because our data is not a time series data or a panel data.\n\n### 5- No Multicollinearity\nVIF (Variance Inflation Factor) and its application in detecting Multicollinearity\nIf VIF > 10, then multicollinearity is high and we should remove that item.","2d99dc7c":"### 1- Linearity","86c71942":"### Dealing with missing values","a85a6fa0":"Our Model\n\n$\\hat{y} = -0.47\\times Mileage + 0.22\\times EngineV + 0.01\\times Brand\\_BMW + \\ldots + 0.31\\times Registration\\_yes$","e3e6cdbe":"### Exploring the PDFs","82fe4c8f":"### Determining the variables of interest","0a6d2fcb":"**Note:**\n\nThe most common interpretation of r-squared is how well the regression model fits the observed data. For example, an r-squared of 0.60 reveals that 60% of the data fit the regression model. Generally, a higher r-squared indicates a better fit for the model.","2cb1915f":"**Pay Attention Please:**\n\nAn ERROR is the difference between the observed value and the true value (very often unobserved, generated by the data generating process (DGP)). Consider the example of Height and Weight.\n\nA RESIDUAL is the difference between the observed value and the predicted value (by the model).","629a0a97":"## Step 2: Checking the Regression Assumptions\n![download.png](attachment:087237df-65d2-4c82-b3b5-9e797fa05038.png)","da338d57":"## Step 4: Creating the Linear Regression Model","226919d0":"### Dealing with outliers","7628e951":"Percent Error = $\\Big|(Target-Prediction)\\times \\frac{100}{Target}\\Big| = \\frac{Residual}{Target}\\times 100$\n\nSee the following link for more details:\n\nhttps:\/\/www.mathsisfun.com\/data\/percentage-difference-vs-error.html","6e0134db":"## Step 1: Data Cleaning\n\n### Loading the raw data","9830e44f":"Recall that we have:\n1: $ln (e^x) =x$\n\n2: $e^{ln(x)} = x$\n\nSo,\n\n$e^{ln (Price)}= Price$","c0c5e6e6":"![download (2).png](attachment:17729205-1b28-45cb-9658-4acb0a59bda6.png)","b6f1047b":"\nYou can change the values of Target to int:\n\ndf_performance['Target'] = df_performance['Target'].astype(int)\n\nYou can also use of this code to get a rounded two decimal float number:\n\npd.set_option('display.float_format', lambda x: '%.2f' % x)"}}