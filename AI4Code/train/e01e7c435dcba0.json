{"cell_type":{"cd7a5fba":"code","77fa8c3e":"code","38ccdf9e":"code","fffee9b3":"code","2d0dfc92":"code","bdd69b3f":"code","61230500":"code","0fbf6493":"code","8d93fcbb":"code","8fecd88f":"code","4647df7f":"code","795214d5":"code","7582138c":"code","cdea6330":"code","f9e64704":"code","86b1cee6":"code","33c28123":"code","4e32135b":"code","1ee7f42d":"code","10140f3d":"code","60d7170b":"code","29e005ab":"code","d8e32b2f":"code","92b109eb":"code","30824b43":"code","19364b93":"code","03689c81":"code","ad9eeee4":"code","c6b6dd2b":"code","e6c001d6":"code","a9a7ea13":"code","d1ab0fe0":"code","0d5d7eff":"code","f7e86d0e":"code","1dd297f2":"code","b30280ac":"code","2f5d78bf":"code","3f917606":"code","80e9de0e":"code","bcbc2d5b":"code","43c62f5b":"code","c20a17f1":"code","2adad5a1":"code","81eb5e8d":"code","fbe43ca4":"code","497bdcca":"code","066ea460":"code","5783a182":"code","b79ae758":"code","3a8c924c":"code","1f483695":"code","b4cfc71d":"markdown","7f842046":"markdown","801f1657":"markdown","d4104b3e":"markdown","c8b95b1f":"markdown","d3d0088d":"markdown","c9719a7f":"markdown","398358d6":"markdown","43298390":"markdown"},"source":{"cd7a5fba":"import numpy as np\nimport pandas as pd\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","77fa8c3e":"from IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = 'all'\n\nimport random\nfrom collections import Counter\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport warnings\nwarnings.filterwarnings('ignore')\npd.set_option('display.max_rows', 500)","38ccdf9e":"train = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\ntrain.head()\ntrain.shape\ntest.head()\ntest.shape","fffee9b3":"train.info()","2d0dfc92":"train.isnull().sum()","bdd69b3f":"((train.Age.isnull().sum())\/train.shape[0])*100\n((train.Cabin.isnull().sum())\/train.shape[0])*100\n((train.Embarked.isnull().sum())\/train.shape[0])*100","61230500":"train.describe(include='all')","0fbf6493":"#Categorical Features\n\ntrain['Sex'].value_counts()","8d93fcbb":"train['Ticket'].value_counts()","8fecd88f":"train['Embarked'].value_counts()","4647df7f":"train['Cabin'].value_counts()","795214d5":"# Removing unimportant fields since we'll be predicting survivality\n\ntrain.drop(['Name','Ticket','PassengerId'],inplace=True,axis=1)\ntrain.head()","7582138c":"train.shape","cdea6330":"plt.hist(train['Age'],bins=20)","f9e64704":"train.Age.std()","86b1cee6":"train.isnull().sum()","33c28123":"from sklearn.impute import SimpleImputer\n#Numerical Values\nmean_imputer = SimpleImputer(missing_values = np.nan, strategy = 'mean')\nmean_imputer = mean_imputer.fit(train[['Age']])\ntrain['Age'] = mean_imputer.transform(train[['Age']]).ravel()\n#Categorical Values\nmode_imputer = SimpleImputer(missing_values = np.nan, strategy = 'most_frequent')\nmode_imputer = mode_imputer.fit(train[['Embarked']])\ntrain['Embarked'] = mode_imputer.transform(train[['Embarked']]).ravel()","4e32135b":"plt.hist(train['Age'],bins=20)","1ee7f42d":"train.Age.std()","10140f3d":"train.isnull().sum()","60d7170b":"#Age \nage_mean = train.Age.mean()\nage_mean\nage_std = train.Age.std()\nage_std\nage_3std_low = -3*age_std\nll = age_mean + age_3std_low\nll\nage_3std_high = 3*age_std\nhl = age_mean + age_3std_high\nhl","29e005ab":"filt_outliers_train = train[(train['Age'] < ll) & (train['Age'] > hl)]\nfilt_outliers_train","d8e32b2f":"#Fare\nFare_mean = train['Fare'].mean()\nFare_std = train['Fare'].std()\nFare_3std_low = -3*Fare_std\nll = Fare_mean + Fare_3std_low\nFare_3std_high = 3*Fare_std\nhl = Fare_mean + Fare_3std_high\nFare_mean\nFare_std\nll\nhl","92b109eb":"filt_outliers_train = train[(train['Fare'] < ll) & (train['Fare'] > hl)]\nfilt_outliers_train","30824b43":"#IQR Method\ndef out_iqr(s, k=1.5, return_thresholds = False):\n    #Calculating IQR \n    q25, q75 = np.percentile(s,25),np.percentile(s,75)\n    iqr = q75 - q25\n    #Calculating IQR cutoff\n    cut_off = iqr*k\n    lower, upper = q25 - cut_off, q75 + cut_off\n    print(lower, upper)\n    if return_thresholds:\n        return lower, upper\n    else:\n        return [True if x < lower or x > upper else False for x in s]","19364b93":"train['outlier_Age'] = out_iqr(train['Age'])\ntrain[train['outlier_Age'] == True].shape\nsns.boxplot(y = 'Age', data = train, whis = 1.5)","03689c81":"train['outlier_Fare'] = out_iqr(train['Fare'])\ntrain[train['outlier_Fare'] == True].shape\nsns.boxplot(y = 'Fare', data = train, whis = 1.5)","ad9eeee4":"#Pclass vs Fare\nsns.boxplot(x = 'Pclass', y = 'Fare', data = train, whis = 1.5)","c6b6dd2b":"#Binning - Equal with binning\nage_range = train.Age.max() - train.Age.min()\nmin_value = int(np.floor(train.Age.min()))\nmax_value = int(np.ceil(train.Age.max()))\n#Rounding the bin width\ninter_value = int(np.round(age_range\/10))\nmin_value, max_value, inter_value","e6c001d6":"df = pd.read_csv(\"..\/input\/fraud-dataset-chanukyapatnaik\/fraud_data.csv\")","a9a7ea13":"df.shape\ndf.head()","d1ab0fe0":"df.isnull().sum()","0d5d7eff":"df.isFraud.value_counts()\ndf.isFraud.value_counts(normalize=True)*100\nsns.countplot(df.isFraud)","f7e86d0e":"df.isnull().sum() \/ len(df) * 100","1dd297f2":"cat_cols = df.select_dtypes(include='object').columns","b30280ac":"# One hot encoding\ndf.shape\ndf[cat_cols] = df[cat_cols].fillna(df[cat_cols].mode().iloc[0])\ndf = pd.get_dummies(df, columns = cat_cols)\ndf.shape","2f5d78bf":"df.head()","3f917606":"#Transformation\nX = df.drop(columns = ['isFraud'])\ny = df.isFraud","80e9de0e":"from sklearn.preprocessing import StandardScaler\nscaled_features = StandardScaler().fit_transform(X)\nscaled_features = pd.DataFrame(data=scaled_features)\nscaled_features.columns = X.columns\n\nscaled_features.head()","bcbc2d5b":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.30, random_state = 42)","43c62f5b":"# Resampling. Since there is an imbalace of target labels\nfrom sklearn.utils import resample\ntrain_data = pd.concat([X_train, y_train], axis=1)\n\nnot_fraud = train_data[train_data.isFraud == 0]\nfraud = train_data[train_data.isFraud == 1]","c20a17f1":"train_data.shape","2adad5a1":"fraud_unsampled = resample(fraud,\n                            replace = True,\n                            n_samples = len(not_fraud),\n                            random_state = 27)","81eb5e8d":"unsampled = pd.concat([not_fraud, fraud_unsampled])","fbe43ca4":"unsampled.isFraud.value_counts()","497bdcca":"not_fraud_downsampled = resample(not_fraud,\n                                replace = False,\n                                n_samples = len(fraud),\n                                random_state = 27)","066ea460":"downsampled = pd.concat([not_fraud_downsampled, fraud])","5783a182":"downsampled.isFraud.value_counts()","b79ae758":"from imblearn.over_sampling import SMOTE\nsm = SMOTE(random_state = 25, sampling_strategy = 1.0)","3a8c924c":"X_train, y_train = sm.fit_sample(X_train, y_train)","1f483695":"y_train.value_counts()","b4cfc71d":"#### Missing Values","7f842046":"# Imbalanced Class\nDifferent Dataset\n\nSolutions:\n* 1. Resampling Techniques\n    - a. Oversamling Minority Class\n    - b. Undersampling Majority Class\n* 2. Generate Synthetic Samples \n    - a. SMOT\n        - i. Nearest neighbor algorithm","801f1657":"### Undersampling majority class","d4104b3e":"### Data Cleaning\n![image.png](attachment:image.png)","c8b95b1f":"### SMOTE (Synthetic Monority Oversampling Technique)","d3d0088d":"~Incomplete","c9719a7f":"### Resampling minority class","398358d6":"# AIEngineering + DPhi Hands-On Workshop\n> Date: 05\/12\/20\n> Credits: Aarthi, DPhi\nBasics of data preparation for machine learning model building\n\nML process:\n1. Business Objective\n2. Data Requirement \n3. Data Collection\n4. *Exploratory Analysis and Data Preparation*\n- Assessing Quality and Cleaning\n- Transformation\n- Training and Testing splits\n5. Modelling\n6. Evaluation\n7. Deployment\n8. Monitoring\n\n\n## Exploratory Analysis and Data Preparation\nCollecting data from sources then profiling, cleaning, enrich, combining with derived sets for analytical purpose\n\nGIGO = the quality of the data determines the quality of the product\n\nTo assess the quality:\n- Data type\n- Data format\n- Nulls\n- Outliers\n- Redundancies\n\n![image.png](attachment:image.png)\n","43298390":"#### Outliers\n\nSome techniques include:\n- Isolation Forest\n- Minimum Covariance Determinant\n- Local Outlier Factor\n- One-Class SVM\n- DBScan Clustering"}}