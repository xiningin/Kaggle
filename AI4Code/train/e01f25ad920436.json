{"cell_type":{"954d36bf":"code","5ac438f4":"code","c1bbbb8c":"code","b46e388a":"code","8343de60":"code","417923b6":"code","27260acb":"code","e8dccc59":"code","fd810df0":"code","8686c133":"code","bcc0980f":"code","d276e489":"code","1e1426f2":"code","41956ee4":"code","823653c5":"code","54b7a3d2":"code","19c20215":"code","af73e79a":"code","a0fcdf49":"code","42f87558":"code","a5472267":"code","71d3a7e4":"code","0ded44f0":"code","0ad273b3":"code","9ee141d5":"code","7939cd4e":"code","8de14c46":"code","d00f57a8":"code","238c730e":"code","6eca4016":"code","50ca84d1":"code","71fcd8ef":"code","b1348d82":"code","d626ae91":"code","40ec2965":"code","5066b7fe":"code","8c60a82f":"code","1ceda6b9":"code","09944de8":"code","6dd88b93":"code","ea236267":"code","dc9e9baa":"code","fcec8fa7":"code","fc4deae5":"code","0b1a7920":"code","1fd0f446":"code","f206bba6":"code","baad728b":"code","93efcbb8":"code","f2a1d141":"code","1e01a1e2":"markdown","55ef25cc":"markdown","c2b3391f":"markdown","d629d238":"markdown","927713b2":"markdown","6572b699":"markdown","21f7d4c5":"markdown","44c666e1":"markdown","c1153b88":"markdown","ee4ab7f7":"markdown","d8e85116":"markdown","5c1949d1":"markdown","b9dd4dd2":"markdown","ae04532e":"markdown","77477d4a":"markdown","a997023a":"markdown","69ffd023":"markdown","ead82692":"markdown","3eb8adb9":"markdown","91e8fe50":"markdown","cfb6b241":"markdown","6a226040":"markdown","66ae9678":"markdown","8f51edeb":"markdown"},"source":{"954d36bf":"import plotly.express as px\nimport plotly.graph_objects as go\nimport plotly.figure_factory as ff\nfrom plotly.subplots import make_subplots\nimport matplotlib.pyplot as plt\n\nfrom pandas_profiling import ProfileReport\nimport seaborn as sns\nfrom sklearn import metrics\n\nfrom copy import deepcopy\n\nimport h2o","5ac438f4":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c1bbbb8c":"df = pd.read_csv('\/kaggle\/input\/credit-card-customers\/BankChurners.csv')","b46e388a":"# Lets delete those strange variables\ndf = df.drop([\"CLIENTNUM\", \n              \"Naive_Bayes_Classifier_Attrition_Flag_Card_Category_Contacts_Count_12_mon_Dependent_count_Education_Level_Months_Inactive_12_mon_1\", \n              \"Naive_Bayes_Classifier_Attrition_Flag_Card_Category_Contacts_Count_12_mon_Dependent_count_Education_Level_Months_Inactive_12_mon_2\"\n             ], axis=1)\n\ndf.head()","8343de60":"credit_card_report = ProfileReport(df)","417923b6":"credit_card_report","27260acb":"df.isna().sum()","e8dccc59":"from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\ndf['class'] = le.fit_transform(df['Attrition_Flag'])\ndf = df.drop('Attrition_Flag', axis=1)","fd810df0":"# Making a copy of the data to save the original data\noriginal_df = deepcopy(df)","8686c133":"df = deepcopy(original_df)","bcc0980f":"# Encode binary variables\ndf['Gender'] = df['Gender'].map({'M':1, 'F':0})","d276e489":"# Finding all the categorical columns from the data\ncategorical_columns = df.select_dtypes(exclude=['int64','float64']).columns\nnumerical_columns = df.drop('class', axis=1).select_dtypes(include=['int64','float64']).columns\ncategorical_columns","1e1426f2":"# One hot encoding independent variable x\ndef encode_and_bind(original_dataframe, feature_to_encode):\n    dummies = pd.get_dummies(original_dataframe[[feature_to_encode]])\n    res = pd.concat([original_dataframe, dummies], axis=1)\n    res = res.drop([feature_to_encode], axis=1)\n    return(res) ","41956ee4":"for feature in categorical_columns:\n    df = encode_and_bind(df, feature)\n\ndf.head()","823653c5":"df.shape","54b7a3d2":"# Generate x and y sets\nx = df.drop('class', axis=1).values\ny = df['class']","19c20215":"# Splitting the dataset into training set and test set\nfrom sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split(x, df['class'], test_size = 0.2, random_state=1234)","af73e79a":"from xgboost import XGBClassifier\n\nclassifier = XGBClassifier(random_state=1234)\nclassifier.fit(x_train,y_train)","a0fcdf49":"# Predicting the test set\ny_pred = classifier.predict(x_test)\n\n# Making the confusion matrix and calculating accuracy score\naccuracy = metrics.accuracy_score(y_test, y_pred)\nprint(accuracy)\n\nfpr, tpr, thresholds = metrics.roc_curve(y_test, y_pred)\nauc = metrics.auc(fpr, tpr)\nprint(auc)","42f87558":"# Importing packages for SMOTE\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.over_sampling import BorderlineSMOTE\nfrom imblearn.under_sampling import RandomUnderSampler\n\nfrom imblearn.pipeline import Pipeline\n\nfrom collections import Counter","a5472267":"sm = SMOTE(sampling_strategy='auto', random_state=1234)\nx_sm, y_sm = sm.fit_resample(x_train, y_train)","71d3a7e4":"print(Counter(y_train))\nprint(Counter(y_sm))","0ded44f0":"fig = make_subplots(rows=1, cols=2, specs=[[{\"type\": \"pie\"}, {\"type\": \"pie\"}]])\n\nfig.add_trace(\n    go.Pie(labels=list(Counter(y_train).keys()), values=list(Counter(y_train).values()), name='Original data'),\n    row=1, col=1\n)\n\nfig.add_trace(\n    go.Pie(labels=list(Counter(y_sm).keys()), values=list(Counter(y_sm).values()), name='SMOTE data'),\n    row=1, col=2\n)\n\nfig.update_traces(textposition='inside', hole=.4, hoverinfo=\"value+percent+name\")\nfig.update_layout(\n    title_text=\"Class distribution\",\n    # Add annotations in the center of the donut pies.\n    annotations=[dict(text='Original', x=0.16, y=0.5, font_size=12, showarrow=False),\n                 dict(text='SMOTE', x=0.82, y=0.5, font_size=12, showarrow=False)])\nfig.show()","0ad273b3":"over = BorderlineSMOTE(sampling_strategy=0.3)\nunder = RandomUnderSampler(sampling_strategy=0.6)\n\nsteps = [('o', over), ('u', under)]","9ee141d5":"pipeline = Pipeline(steps=steps)\n\n# transform the dataset\nx_sm_us, y_sm_us = pipeline.fit_resample(x_train, y_train)\n\nprint(Counter(y_train))\nprint(Counter(y_sm_us))","7939cd4e":"list(Counter(y_train).keys())","8de14c46":"fig = make_subplots(rows=1, cols=2, specs=[[{\"type\": \"pie\"}, {\"type\": \"pie\"}]])\n\nfig.add_trace(\n    go.Pie(labels=list(Counter(y_train).keys()), values=list(Counter(y_train).values()), name='Original Data'),\n    row=1, col=1\n)\n\nfig.add_trace(\n    go.Pie(labels=list(Counter(y_sm_us).keys()), values=list(Counter(y_sm_us).values()), name='SMOTE and US data'),\n    row=1, col=2\n)\n\nfig.update_traces(textposition='inside', hole=.4, hoverinfo=\"percent+name+value\")\nfig.update_layout(\n    title_text=\"Class distribution\",\n    # Add annotations in the center of the donut pies.\n    annotations=[dict(text='Original', x=0.16, y=0.5, font_size=12, showarrow=False),\n                 dict(text='SMOTE and UnderSample', x=0.9, y=0.5, font_size=12, showarrow=False)])\nfig.show()","d00f57a8":"classifier = XGBClassifier(random_state=1234)\nclassifier.fit(x_sm, y_sm)","238c730e":"# Predicting the test set\ny_pred = classifier.predict(x_test)\n\n# Making the confusion matrix and calculating accuracy score\naccuracy = metrics.accuracy_score(y_test, y_pred)\nprint(accuracy)\n\nfpr, tpr, thresholds = metrics.roc_curve(y_test, y_pred)\nauc = metrics.auc(fpr, tpr)\nprint(auc)","6eca4016":"classifier = XGBClassifier(random_state=1234)\nclassifier.fit(x_sm_us, y_sm_us)","50ca84d1":"# Predicting the test set\ny_pred = classifier.predict(x_test)\n\n# Making the confusion matrix and calculating accuracy score\naccuracy = metrics.accuracy_score(y_test, y_pred)\nprint(accuracy)\n\nfpr, tpr, thresholds = metrics.roc_curve(y_test, y_pred)\nauc = metrics.auc(fpr, tpr)\nprint(auc)","71fcd8ef":"# Names of the independent variables\nfeature_names = list(df.drop('class', axis=1).columns)","b1348d82":"# Concatenate train (with resampling) and test sets to build the new dataframe \nsm_us_x = np.concatenate((x_sm_us, x_test))\nsm_us_y = np.concatenate((y_sm_us, y_test))","d626ae91":"sm_us_df = pd.DataFrame(np.column_stack([sm_us_y, sm_us_x]), columns=['class'] + feature_names)\nsm_us_df.head()","40ec2965":"from sklearn.ensemble import RandomForestClassifier\n\nrf_clf = RandomForestClassifier(random_state=1234)\nrf_clf.fit(x_sm_us, y_sm_us)","5066b7fe":"rf_clf.feature_importances_","8c60a82f":"features_to_plot = 25\n\nimportances = rf_clf.feature_importances_\nindices = np.argsort(importances)\n\nbest_vars = np.array(feature_names)[indices][-features_to_plot:]\nvalues = importances[indices][-features_to_plot:]\nbest_vars","1ceda6b9":"y_ticks = np.arange(0, features_to_plot)\nfig, ax = plt.subplots()\nax.barh(y_ticks, values)\nax.set_yticklabels(best_vars)\nax.set_yticks(y_ticks)\nax.set_title(\"Random Forest Feature Importances\")\nfig.tight_layout()\nplt.show()","09944de8":"best_vars = best_vars[-20:]\nbest_vars","6dd88b93":"from h2o.automl import H2OAutoML\n\nh2o.init()","ea236267":"hf = h2o.H2OFrame(sm_us_df[['class'] + list(best_vars)])\nhf.head()","dc9e9baa":"hf['class'] = hf['class'].asfactor()\npredictors = hf.drop('class').columns\nresponse = 'class'","fcec8fa7":"# Split into train and test\ntrain, valid = hf.split_frame(ratios=[.8], seed=1234)","fc4deae5":"# Add a Stopping Creterias: max number of models and max time\n# We are going to exclude DeepLearning algorithms because they are too slow\naml = H2OAutoML(\n    max_models=20,\n    max_runtime_secs=300,\n    seed=1234,\n    exclude_algos = [\"DeepLearning\"]\n)","0b1a7920":"# Train the model\naml.train(x=predictors,\n        y=response,\n        training_frame=train,\n        validation_frame=valid\n)","1fd0f446":"# View the AutoML Leaderboard\nlb = aml.leaderboard\nlb.head(rows=5)  # Print the first 5 rows","f206bba6":"lb = lb.as_data_frame()\nlb['model_type'] = lb['model_id'].apply(lambda x: x.split('_')[0])\nfig = px.bar(\n    lb, \n    x='model_id',\n    y='auc',\n    color='model_type'\n)\n# fig.update_yaxes(range=[0.999, 1])\nfig.show()","baad728b":"print('The model performance in Accuracy: {}'.format(aml.leader.accuracy(valid=True)))\nprint('The model performance in AUC: {}'.format(aml.leader.auc(valid=True)))","93efcbb8":"aml.leader.varimp_plot(num_of_features=10)","f2a1d141":"aml.leader.shap_summary_plot(valid)","1e01a1e2":"## 2.1 One-Hot Encoding","55ef25cc":"**We have to consider that there are many variables that are categorical and we will have to encode them**","c2b3391f":"![one-hot-encoding.png](attachment:smote.png)","d629d238":"As we can see, we have earned a **0.99 AUC score** and very good parameters!","927713b2":"As we can see in the feature importance plot, the first 4 variables have the most importance in the model, but we are going to retireve the first 20 to be safe about over-fitting and under-fitting.","6572b699":"As we saw in the PandasProfile and here, there are no NaNs at the dataset, so we can go ahead.","21f7d4c5":"As the data is imbalanced, we should consider only the AUC, beacuse accuracy is not a good opion in these cases.","44c666e1":"# 3. Over-Sampling | SMOTE\n\nTo correct the problem of unbalancing the classes of the data set, I will use the SMOTE method.\n","c1153b88":"As we can see in the the Variable Importance plot, the top vars are much the same as they were in the [Feature Selection](#4.-feature-selection) part.","ee4ab7f7":"![banner-credit-card-churning-ig2.jpg](attachment:banner-credit-card-churning-ig2.jpg)","d8e85116":"![one-hot-encoding.png](attachment:one-hot-encoding.png)","5c1949d1":"As we can see, **with SMOTE we have improved the AUC** .","b9dd4dd2":"## 3.2 Generate new DataFrame with SMOTE and UnderSampling data","ae04532e":"As we can see, **with SMOTE and Random Undersaampling we have improved the AUC** .","77477d4a":"## 3.1 Lets try model a model with SMOTE data","a997023a":"# 1. Data Overview","69ffd023":"# 4. Feature Selection\n\nNow we are going to select the top features for the model (because we have done One-Hot-Encoding and there are to many variables at the moment).\nThere are different ways to do this:\n* **PCA**: To select the top features\n* **Train a model**: select the top importance variables for a simple tree\n* etc\n\nWe are going to use the second choice","ead82692":"# 2. Feature Engeneering","3eb8adb9":"## 5.1 H2O AutoML\n\nWe are going to try to execute AutoML with H2O on the resampled dataset and only with the final variables","91e8fe50":"# 5. Machine Learning | H2O\n\nNow, with the resampled dataset and the selected feaures, we are going to train an xgboost model and search the best parameters","cfb6b241":"### SHAP Values\n\nSHAP (SHapley Additive exPlanations) is a game theoretic approach to explain the output of any machine learning model. It connects optimal credit allocation with local explanations using the classic Shapley values from game theory and their related extensions.\n\n> Shapley values calculate the importance of a feature by comparing what a model predicts with and without the feature. However, since the order in which a model sees features can affect its predictions, this is done in every possible order, so that the features are fairly compared.\n\nImagine a sales score model. A customer living in zip code \"A1\" with \"10 purchases\" arrives and its score is 95%, while other from zip code \"A2\" and \"7 purchases\" has a score of 60%.\n\nEach variable had its contribution to the final score. Maybe a slight change in the number of purchases changes the score a lot, while changing the zip code only contributes a tiny amount on that specific customer.\n\nSHAP measures the impact of variables taking into account the interaction with other variables.\n\nBut i think that the best way to understand it is with [Kaggle Own Notebook](https:\/\/www.kaggle.com\/dansbecker\/shap-values) and [Python SHAP implementation Github](https:\/\/github.com\/slundberg\/shap).","6a226040":"### SMOTE\n\nOne approach to addressing imbalanced datasets is to oversample the minority class. The simplest approach involves duplicating examples in the minority class, although these examples don\u2019t add any new information to the model. Instead, new examples can be synthesized from the existing examples. This is a type of data augmentation for the minority class and is referred to as the Synthetic Minority Oversampling Technique, or SMOTE for short.\n\nSMOTE works by selecting examples that are close in the feature space, drawing a line between the examples in the feature space and drawing a new sample at a point along that line.\n\n> SMOTE first selects a minority class instance a at random and finds its k nearest minority class neighbors. The synthetic instance is then created by choosing one of the k nearest neighbors b at random and connecting a and b to form a line segment in the feature space. The synthetic instances are generated as a convex combination of the two chosen instances a and b.\n\nPage 47, [Imbalanced Learning: Foundations, Algorithms, and Applications](https:\/\/www.amazon.com\/dp\/1118074629\/ref=as_li_ss_tl?&linkCode=sl1&tag=inspiredalgor-20&linkId=615e87a9105582e292ad2b7e2c7ea339&language=en_US), 2013\n\n> The combination of SMOTE and under-sampling performs better than plain under-sampling.\n\n[SMOTE: Synthetic Minority Over-sampling Technique](https:\/\/arxiv.org\/abs\/1106.1813), 2011.","66ae9678":"![h2o-automl-logo2.jpg](attachment:h2o-automl-logo2.jpg)","8f51edeb":"## 2.2 Lets try model a model with RAW data"}}