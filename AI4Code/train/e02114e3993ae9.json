{"cell_type":{"fee03aed":"code","75de0b1d":"code","2c446fd0":"code","7f5ff1c1":"code","7b2aa43a":"code","6c322a19":"code","441f567c":"code","38ba288d":"code","737b545c":"code","3e3f3f2e":"code","23384757":"code","322f0bf4":"code","4aba226c":"code","feb29ff2":"code","c05dc977":"code","2d25e91d":"code","db3b009e":"code","f89ee4bf":"code","779cdb3e":"code","a720e2e9":"code","dfe03009":"code","bbe6cee7":"code","db82dba9":"code","7ec36519":"code","41bf00b5":"code","a283181a":"code","32b0cd1d":"code","e8dda1d0":"code","b8e2b5fe":"code","2c465d4f":"code","594df700":"code","030e9aa2":"code","d79a8cba":"code","b053b86f":"code","547c5c4c":"code","cc9d5bfe":"code","231444d3":"code","7afca621":"code","cf08040a":"code","6e18da8f":"code","c5e9eb20":"code","eb253e25":"code","2e7349ab":"code","c9b34add":"code","e0191b55":"code","2e68057f":"code","8e3fae00":"code","57bcd842":"code","834ee153":"code","9be93ecf":"code","df85d331":"code","6229f197":"code","ce06ed13":"code","91b66fad":"code","d54b1f5c":"code","ce2a6574":"code","26f908e3":"code","7ccd8e9d":"code","23d0fe91":"code","9a99c5a3":"code","2593921e":"code","979998e2":"code","2a429eeb":"code","7948cf27":"code","c366a8a1":"code","1d31748c":"code","b9e159a1":"code","970466bb":"code","0ec31b4a":"code","52238869":"code","52ff66c8":"code","b8e0a0ab":"code","bec788de":"code","1f71e7e7":"code","2ffe7f4c":"code","21c7f5a1":"code","7cef810c":"code","3213fd35":"code","60aa1be0":"code","c85578b8":"code","cdce656b":"code","f74cd417":"code","7dd99a61":"code","0af764fb":"code","503950fa":"code","00a59fa8":"code","0c8467fa":"code","d7e96378":"markdown","db27b539":"markdown","e7a0a049":"markdown","0d0332cc":"markdown","d76db547":"markdown","c87e032c":"markdown","30f2177a":"markdown","65714fde":"markdown","c081fdc2":"markdown","d20456e2":"markdown","0553ef41":"markdown","c4c42867":"markdown","2e14eed3":"markdown","0801df93":"markdown","b52174f1":"markdown","724bef97":"markdown","7ec34471":"markdown","5d61c904":"markdown","86d375d7":"markdown","726d52e7":"markdown","47985265":"markdown","a447d5d3":"markdown","aadf40d8":"markdown","f10bae05":"markdown","4cb2995c":"markdown","18ef3ecd":"markdown","afe0a58b":"markdown","6c5bb468":"markdown","5369c8e4":"markdown","23cdcf90":"markdown","a7d12178":"markdown","7b0f15bd":"markdown","2c47e1a4":"markdown","d3d4aa62":"markdown","eb248701":"markdown","89877e45":"markdown","438dc3ad":"markdown","0ffa5c7f":"markdown","e9e34099":"markdown","9fc7cf4d":"markdown","970c64d5":"markdown","fa96ba2a":"markdown","0332449c":"markdown","fcf88745":"markdown","55ae3270":"markdown","401e5b2e":"markdown","b6fa54fa":"markdown","9e9ff031":"markdown","d4ecab65":"markdown","0408b337":"markdown","5cbb9b60":"markdown","348f6d0c":"markdown","df42c02f":"markdown","00a8e014":"markdown","fad981ca":"markdown","0713a237":"markdown","46d9fcaa":"markdown","c9b9d28c":"markdown","3eb3ee47":"markdown","c67d464c":"markdown","2a113be0":"markdown","68bfee86":"markdown","0b39113f":"markdown","855d8782":"markdown","3bd19714":"markdown","e32ca0b7":"markdown","8aba1cf8":"markdown","13a0ce4a":"markdown","1f14c14d":"markdown","2d68609d":"markdown","1d524d89":"markdown","4b136261":"markdown","89420205":"markdown","031f7e71":"markdown","5fc07deb":"markdown","1b86d5a9":"markdown","d2176cff":"markdown","766ac7a0":"markdown","c46f9b33":"markdown","ff767dd7":"markdown","93ec13fd":"markdown","1cd7f4c5":"markdown","14102308":"markdown","af8c9f57":"markdown","6461d94b":"markdown","9722d180":"markdown","3316eb65":"markdown","ef3109b3":"markdown","dbefec1e":"markdown","cceb0b3c":"markdown","33ded8c2":"markdown","0cd3f175":"markdown","0753b265":"markdown","38d115d9":"markdown","a65a9b86":"markdown","8a5cc812":"markdown","4648e4cd":"markdown","461fac58":"markdown","2736727f":"markdown","aa8fb854":"markdown"},"source":{"fee03aed":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# 3\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\n\n# 4\nfrom scipy.spatial.distance import cdist\nfrom sklearn.cluster import KMeans\nfrom plotnine import *\nfrom sklearn.decomposition import PCA\n\n# 5\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import accuracy_score, f1_score, confusion_matrix\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.linear_model import LogisticRegression\n\n# 6\nfrom sklearn.metrics import r2_score, mean_squared_error\nfrom sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor\n\n# seed\nRSEED = 10\n\n# colors\ncmap = 'Blues'","75de0b1d":"train = pd.read_csv('..\/input\/airline-passenger-satisfaction\/train.csv')\ntest = pd.read_csv('..\/input\/airline-passenger-satisfaction\/test.csv')","2c446fd0":"train.head()","7f5ff1c1":"test.head()","7b2aa43a":"train.drop(['Unnamed: 0', 'id'], axis=1, inplace=True)\ntest.drop(['Unnamed: 0', 'id'], axis=1, inplace=True)","6c322a19":"sns.set_theme(style='whitegrid', palette=cmap)","441f567c":"df = train.append(test)","38ba288d":"plt.figure(figsize=(8, 6))\nax = sns.countplot(x='satisfaction', data=df)\nplt.title('Barplot of Satisfaction (Target)', fontsize=14)\nplt.xlabel('Satisfaction (Target)', fontsize=13)\nplt.ylabel('Count', fontsize=13)\nplt.show()","737b545c":"# define categorical columns list\ncat_cols = ['Gender', 'Customer Type', 'Type of Travel', 'Class']","3e3f3f2e":"fig, axs = plt.subplots(nrows=2, ncols=2, figsize=(14, 10))\naxs = axs.ravel()\n\ni=0\nfor col in cat_cols:\n    \n    # define location\n    plt.subplot(2, 2, i+1)\n    \n    # create plot\n    ax = plt.gca()\n    axs[i] = sns.countplot(x = col, hue = 'satisfaction', data = df)\n    \n    # remove legend\n    ax.legend('')\n    \n    i += 1\n\n# add legend\naxs[1].legend(loc='upper right', fontsize=16)\n\nplt.tight_layout()\nplt.show()","23384757":"# create labels for categorical columns\nlabel = LabelEncoder()\n\nlabeled_df = df.copy()\nfor i in cat_cols:\n    labeled_df[i] = label.fit_transform(labeled_df[i])","322f0bf4":"plt.figure(figsize=(15, 12))\nsns.heatmap(labeled_df.corr(), cmap=cmap)\nplt.title('Heatmap of Train Dataset', fontsize=14)\nplt.show()","4aba226c":"cor = df['Departure Delay in Minutes'].corr(df['Arrival Delay in Minutes'])","feb29ff2":"def transform_satisfaction(x):\n    if x == 'satisfied':\n        return 1\n    elif x == 'neutral or dissatisfied':\n        return 0\n    else:\n        return -1","c05dc977":"train['satisfaction'] = train['satisfaction'].apply(transform_satisfaction)\ntest['satisfaction'] = test['satisfaction'].apply(transform_satisfaction)","2d25e91d":"df.isnull().sum()","db3b009e":"train.drop('Arrival Delay in Minutes', axis=1, inplace=True)\ntest.drop('Arrival Delay in Minutes', axis=1, inplace=True)","f89ee4bf":"dummy_train = pd.get_dummies(train, columns=cat_cols, drop_first=True)\ndummy_test = pd.get_dummies(test, columns=cat_cols, drop_first=True)","779cdb3e":"dummy_train.head()","a720e2e9":"train_df = dummy_train.copy()\ntest_df = dummy_test.copy()","dfe03009":"train_cols = train_df[['Age', 'Flight Distance', 'Departure Delay in Minutes']]\ntest_cols = test_df[['Age', 'Flight Distance', 'Departure Delay in Minutes']]","bbe6cee7":"scaler = StandardScaler()\nscaled_train = pd.DataFrame(scaler.fit_transform(train_cols), columns = train_cols.columns)\nscaled_test = pd.DataFrame(scaler.fit_transform(test_cols), columns = test_cols.columns)","db82dba9":"scaled_train.head().style.set_properties(**{'text-align': 'center'}).set_table_styles([dict(selector='th', props=[('text-align', 'center')])])","7ec36519":"cols = ['Age', 'Flight Distance', 'Departure Delay in Minutes']\n\ntrain_df[cols] = scaled_train\ntest_df[cols] = scaled_test","41bf00b5":"train_df.head().style.set_properties(**{'text-align': 'center'}).set_table_styles([dict(selector='th', props=[('text-align', 'center')])])","a283181a":"def elbow_met(df):\n    distortions = []\n    K = range(1,10)\n    for k in K:\n        kmeanModel = KMeans(n_clusters=k, random_state=RSEED).fit(df)\n        kmeanModel.fit(df)\n        distortions.append(sum(np.min(cdist(df, kmeanModel.cluster_centers_, 'euclidean'), axis=1)) \/ df.shape[0])\n\n    # Plot the elbow\n    plt.plot(K, distortions, 'bx-')\n    plt.xlabel('k')\n    plt.ylabel('Within groups sum of squares')\n    plt.title('The Elbow Method showing the optimal k')\n    plt.show()","32b0cd1d":"elbow_met(train_df)","e8dda1d0":"kmeans = KMeans(n_clusters=3, random_state=RSEED)\nkmeans.fit(train_df)","b8e2b5fe":"def get_melted_clusters(labels):\n    result_df = dm.copy()\n    \n    result_df[\"cluster\"] = labels\n    melt_cluster = result_df.groupby(\"cluster\").mean().reset_index().melt(id_vars=\"cluster\")\n    melt_cluster = melt_cluster[\"variable\"].str.split(\"_\" , expand=True).join(melt_cluster)\n    melt_cluster = melt_cluster.rename({0 : \"variable_base\" , 1:\"response\"} , axis=1)\n    melt_cluster[\"cluster\"] = melt_cluster[\"cluster\"].astype(\"category\") \n    return melt_cluster","2c465d4f":"dm = pd.get_dummies(train, columns=['satisfaction'])\nmc = get_melted_clusters(kmeans.labels_)\n\n(\n    ggplot(mc[~mc[\"response\"].isna()], aes(x=\"response\", y=\"value\", fill=\"cluster\"))\n    + geom_col(position=\"fill\")\n    + labs(x='Value', y='Percentage')\n    + coord_flip()\n    + facet_wrap(\"~ variable_base\")\n    + theme(figure_size=(5, 4))\n    + scale_fill_manual(values=('#c6dbef', '#6baed6', '#08306b'))\n)","594df700":"cols = ['Inflight wifi service', 'Departure\/Arrival time convenient', 'Ease of Online booking',\n       'Gate location', 'Food and drink', 'Online boarding', 'Seat comfort', 'Inflight entertainment', 'On-board service',\n       'Leg room service', 'Baggage handling', 'Checkin service', 'Inflight service', 'Cleanliness']\n\ndm = pd.get_dummies(train, columns=cols, drop_first=True)\nmc = get_melted_clusters(kmeans.labels_)\n\n(\n    ggplot(mc[~mc[\"response\"].isna()], aes(x=\"response\", y=\"value\", fill=\"cluster\"))\n    + geom_col(position=\"fill\")\n    + labs(x='Value', y='Percentage')\n    + coord_flip()\n    + facet_wrap(\"~ variable_base\")\n    + theme(figure_size=(12, 10), legend_position = (0.6, 0.2)) \n    + scale_fill_manual(values=('#c6dbef', '#6baed6', '#08306b'))\n)","030e9aa2":"dm = pd.get_dummies(train, columns=['Gender'])\nmc = get_melted_clusters(kmeans.labels_)\n\n(\n    ggplot(mc[~mc[\"response\"].isna()], aes(x=\"response\", y=\"value\", fill=\"cluster\"))\n    + geom_col(position=\"fill\")\n    + labs(x='Value', y='Percentage')\n    + coord_flip()\n    + facet_wrap(\"~ variable_base\")\n    + theme(figure_size=(5, 4))\n    + scale_fill_manual(values=('#c6dbef', '#6baed6', '#08306b'))\n)","d79a8cba":"dm = pd.get_dummies(train, columns=['Customer Type'])\nmc = get_melted_clusters(kmeans.labels_)\n\n(\n    ggplot(mc[~mc[\"response\"].isna()], aes(x=\"response\", y=\"value\", fill=\"cluster\"))\n    + geom_col(position=\"fill\")\n    + labs(x='Value', y='Percentage')\n    + coord_flip()\n    + facet_wrap(\"~ variable_base\")\n    + theme(figure_size=(5, 4))\n    + scale_fill_manual(values=('#c6dbef', '#6baed6', '#08306b'))\n)","b053b86f":"dm = pd.get_dummies(train, columns=['Type of Travel'])\nmc = get_melted_clusters(kmeans.labels_)\n\n(\n    ggplot(mc[~mc[\"response\"].isna()], aes(x=\"response\", y=\"value\", fill=\"cluster\"))\n    + geom_col(position=\"fill\")\n    + labs(x='Value', y='Percentage')\n    + coord_flip()\n    + facet_wrap(\"~ variable_base\")\n    + theme(figure_size=(5, 4))\n    + scale_fill_manual(values=('#c6dbef', '#6baed6', '#08306b'))\n)","547c5c4c":"dm = pd.get_dummies(train, columns=['Class'])\nmc = get_melted_clusters(kmeans.labels_)\n\n(\n    ggplot(mc[~mc[\"response\"].isna()], aes(x=\"response\", y=\"value\", fill=\"cluster\"))\n    + geom_col(position=\"fill\")\n    + labs(x='Value', y='Percentage')\n    + coord_flip()\n    + facet_wrap(\"~ variable_base\")\n    + theme(figure_size=(5, 4))\n    + scale_fill_manual(values=('#c6dbef', '#6baed6', '#08306b'))\n)","cc9d5bfe":"pca = PCA(random_state=RSEED)\npca.fit(train_df)\npca_data = pca.transform(train_df)","231444d3":"per_var = np.round(pca.explained_variance_ratio_* 100, decimals=1)\nlabels = ['PC' + str(x) for x in range(1, len(per_var)+1)]\n\nplt.figure(figsize=(16, 6))\nplt.bar(x=range(1, len(per_var)+1), height=per_var, tick_label=labels, color='#9ecae1')\nplt.ylabel('Percentage of Explained Variance', fontsize=13)\nplt.xlabel('Principal Component', fontsize=13)\nplt.title('Scree Plot', fontsize=14)\nplt.show()","7afca621":"loading_scores = pd.Series(pca.components_[0], index=train_df.columns)\nsorted_loading_scores = loading_scores.abs().sort_values(ascending=False)\ntop_10_features = sorted_loading_scores[0:10].index.values\ntop_features = pd.DataFrame(loading_scores[top_10_features], columns=['Score'])\ntop_features.style.set_properties(**{'text-align': 'center'}).set_table_styles([dict(selector='th', props=[('text-align', 'left')])])","cf08040a":"X_train = train_df.drop(['satisfaction'], axis=1)\ny_train = train_df['satisfaction']\nX_test = test_df.drop(['satisfaction'], axis=1)\ny_test = test_df['satisfaction']","6e18da8f":"rf = RandomForestClassifier(random_state=RSEED)\n\nparameters_grid = {\n    'max_depth': [5, 15],\n    'min_samples_leaf': [2, 8],\n    'n_estimators': [50, 100],\n    'max_features': [5, 10]\n}\n\n# define grid search\ngrid_search = GridSearchCV(estimator=rf, param_grid=parameters_grid, cv=10, n_jobs=-1)\n\n# fit estimator\ngrid_search.fit(X_train, y_train)\n\n# get feature importance\nimp = pd.DataFrame(grid_search.best_estimator_.fit(X_train, y_train).feature_importances_, \n                   index=X_train.columns, columns=['Importance']).sort_values('Importance')\n\nplt.figure(figsize=(8, 6))\nplt.barh(range(0, len(imp)), imp['Importance'], color='#9ecae1')\nplt.grid(axis='x', alpha=0.5, color='lightgrey')\nplt.yticks(range(0, len(imp)), imp.index)\nplt.title('Feature Importance', fontsize=14)\nplt.show()","c5e9eb20":"ab = AdaBoostClassifier(random_state=RSEED)\n\nparameters_grid = {\n    'n_estimators': [50, 100],\n    'learning_rate': [0.001, 0.01, 0.1]\n}\n\n# define grid search\ngrid_search = GridSearchCV(estimator=ab, param_grid=parameters_grid, cv=10, n_jobs=-1)\n\n# fit estimator\ngrid_search.fit(X_train, y_train)\n\n# get feature importance\nimp = pd.DataFrame(grid_search.best_estimator_.fit(X_train, y_train).feature_importances_, \n                   index=X_train.columns, columns=['Importance']).sort_values('Importance')\n\nplt.figure(figsize=(8, 6))\nplt.barh(range(0, len(imp)), imp['Importance'], color='#9ecae1')\nplt.grid(axis='x', alpha=0.5, color='lightgrey')\nplt.yticks(range(0, len(imp)), imp.index)\nplt.title('Feature Importance', fontsize=14)\nplt.show()","eb253e25":"X_train = train_df.drop(['satisfaction'], axis=1)\ny_train = train_df['satisfaction']\nX_test = test_df.drop(['satisfaction'], axis=1)\ny_test = test_df['satisfaction']","2e7349ab":"def fit_predict(X_train, X_test, y_train, model, parameters_grid):\n    \n    # define grid search\n    grid_search = GridSearchCV(estimator=model, param_grid=parameters_grid, cv=10, n_jobs=-1)\n    \n    # fit estimator\n    grid_search.fit(X_train, y_train)\n    \n    # get best estimator\n    best = grid_search.best_estimator_\n    \n    # predict\n    y_pred = best.predict(X_test)\n    \n    return y_pred, grid_search","c9b34add":"def get_class_metrics(y_test, y_pred):\n  \n    # calculate accuracy and f1-score\n    acc = round(accuracy_score(y_test, y_pred), 3)\n    f1 = round(f1_score(y_test, y_pred), 3)\n    scores = [acc, f1]\n    \n    # Get confusion matrix\n    mat = confusion_matrix(y_test, y_pred)\n    \n    # create confusion matrix\n    sns.heatmap(mat, annot=True, cmap=cmap, fmt='d')\n    plt.xlabel('Predicted', fontsize=13)\n    plt.ylabel('True', fontsize=13)\n    plt.title('Confusion Matrix', fontsize=14)\n    \n    return scores, plt","e0191b55":"def display_df(scores, model):\n    df = pd.DataFrame(scores).T\n    df = df.rename(index={0: model}, columns={0: 'Accuracy', 1: 'F1 Score'})\n    return(df)","2e68057f":"rf = RandomForestClassifier(random_state=RSEED)\n\nparameters_grid = {\n    'max_depth': [5, 15],\n    'min_samples_leaf': [2, 8],\n    'n_estimators': [50, 100],\n    'max_features': [5, 10]\n}\n\n# get predictions\ny_pred, grid_search = fit_predict(X_train, X_test, y_train, rf, parameters_grid)\n\n# calculate metrics\nscores, mat = get_class_metrics(y_test, y_pred)","8e3fae00":"rfc_res = display_df(scores, 'Random Forest Classifier')\nrfc_res","57bcd842":"pd.DataFrame.from_dict(grid_search.best_params_, orient='index', columns=['Selected Value']).T","834ee153":"ab = AdaBoostClassifier(random_state=RSEED)\n\nparameters_grid = {\n    'n_estimators': [50, 100],\n    'learning_rate': [0.001, 0.01, 0.1]\n}\n\n# get predictions\ny_pred, grid_search = fit_predict(X_train, X_test, y_train, ab, parameters_grid)\n\n# calculate metrics\nscores, mat = get_class_metrics(y_test, y_pred)","9be93ecf":"abc_res = display_df(scores, 'Ada-Boost Classifier')\nabc_res","df85d331":"pd.DataFrame.from_dict(grid_search.best_params_, orient='index', columns=['Selected Value']).T","6229f197":"mlp = MLPClassifier(max_iter=1000, random_state=RSEED)\n\nparameters_grid = {\n    'hidden_layer_sizes': [(20,), (50,)],\n    'solver': ['sgd', 'adam'],\n    'learning_rate': ['constant', 'adaptive'],\n}\n\n# get predictions\ny_pred, grid_search = fit_predict(X_train, X_test, y_train, mlp, parameters_grid)\n\n# calculate metrics\nscores, mat = get_class_metrics(y_test, y_pred)","ce06ed13":"mlp_res = display_df(scores, 'MLP')\nmlp_res","91b66fad":"grid_search.best_params_","d54b1f5c":"log = LogisticRegression(fit_intercept=True, max_iter=1000, random_state=RSEED)\n\nparameters_grid = {'C': [0.001, 0.01, 0.1, 1, 10]}\n\n# get predictions\ny_pred, grid_search = fit_predict(X_train, X_test, y_train, log, parameters_grid)\n\n# calculate metrics\nscores, mat = get_class_metrics(y_test, y_pred)","ce2a6574":"log_res = display_df(scores, 'Logistic Regression')\nlog_res","26f908e3":"pd.DataFrame.from_dict(grid_search.best_params_, orient='index', columns=['Selected Value']).T","7ccd8e9d":"class_res = rfc_res.copy()\nclass_res = class_res.append(abc_res)\nclass_res = class_res.append(mlp_res)\nclass_res = class_res.append(log_res)\n\nclass_res.style.set_properties(**{'text-align': 'center'}).set_table_styles([dict(selector='th', props=[('text-align', 'left')])])","23d0fe91":"def get_reg_metrics(y_test, y_pred):\n    \n    # calculate R squared\n    R2 = r2_score(y_test, y_pred)\n    \n    # calculate MSE\n    MSE = mean_squared_error(y_test, y_pred)\n    \n    scores = [R2, MSE]\n    \n    # create plot\n    plt.figure(figsize=(8, 6))\n    sns.scatterplot(x=y_test, y=y_pred, color='#9ecae1')\n    plt.title('Actual vs. Predicted', fontsize=14)\n    plt.xlabel('Actual Satisfaction', fontsize=13)\n    plt.ylabel('Predicted Satisfaction', fontsize=13)\n    plt.xticks(np.arange(1, 6))\n    plt.yticks(np.arange(1, 6))\n    \n    return scores, plt","9a99c5a3":"def display_df(scores, model):\n    df = pd.DataFrame(scores).T\n    df = df.rename(index={0: model}, columns={0: 'R2', 1: 'MSE'})\n    return(df)","2593921e":"def feature_importance(imp):\n    plt.figure(figsize=(8, 6))\n    plt.barh(range(0, len(imp)), imp['Importance'], color='#9ecae1')\n    plt.grid(axis='x', alpha=0.5, color='lightgrey')\n    plt.yticks(range(0, len(imp)), imp.index)\n    plt.title('Feature Importance', fontsize=14)\n    return plt","979998e2":"features = ['Gate location', 'Seat comfort', 'Cleanliness']\ncol_train = train_df.loc[:, features]\ncol_test = test_df.loc[:, features]","2a429eeb":"train_df['satisfaction_new'] = col_train.mean(axis=1)\ntest_df['satisfaction_new'] = col_test.mean(axis=1)","7948cf27":"X_train = train_df.drop(['satisfaction', 'satisfaction_new'], axis=1)\nX_train = X_train.drop(features, axis=1)\ny_train = train_df['satisfaction_new']\n\nX_test = test_df.drop(['satisfaction', 'satisfaction_new'], axis=1)\nX_test = X_test.drop(features, axis=1)\ny_test = test_df['satisfaction_new']","c366a8a1":"rf = RandomForestRegressor(random_state=RSEED)\n\nparameters_grid = {\n    'max_depth': [5, 15],\n    'min_samples_leaf': [2, 8],\n    'n_estimators': [50, 100],\n    'max_features': [5, 10]\n}\n\n# get predictions\ny_pred, grid_search = fit_predict(X_train, X_test, y_train, rf, parameters_grid)\n\n# calculate metrics\nscores, plot = get_reg_metrics(y_test, y_pred)","1d31748c":"rfr1_res = display_df(scores, 'Random Forest Regressor')\nrfr1_res","b9e159a1":"pd.DataFrame.from_dict(grid_search.best_params_, orient='index', columns=['Selected Value']).T","970466bb":"imp = pd.DataFrame(grid_search.best_estimator_.fit(X_train, y_train).feature_importances_, \n                   index=X_train.columns, columns=['Importance']).sort_values('Importance')\n\nfeature_importance(imp).show()","0ec31b4a":"ab = AdaBoostRegressor(random_state=RSEED)\n\nparameters_grid = {\n    'n_estimators': [50, 100],\n    'learning_rate': [0.001, 0.01, 0.1]\n}\n\n# get predictions\ny_pred, grid_search = fit_predict(X_train, X_test, y_train, ab, parameters_grid)\n\n# calculate metrics\nscores, plot = get_reg_metrics(y_test, y_pred)","52238869":"abr1_res = display_df(scores, 'Ada-Boost Regressor')\nabr1_res","52ff66c8":"pd.DataFrame.from_dict(grid_search.best_params_, orient='index', columns=['Selected Value']).T","b8e0a0ab":"imp = pd.DataFrame(grid_search.best_estimator_.fit(X_train, y_train).feature_importances_, \n                   index=X_train.columns, columns=['Importance']).sort_values('Importance')\n\nfeature_importance(imp).show()","bec788de":"X_train = train_df.drop(['satisfaction', 'satisfaction_new'], axis=1)\ny_train = train_df['satisfaction_new']\n\nX_test = test_df.drop(['satisfaction', 'satisfaction_new'], axis=1)\ny_test = test_df['satisfaction_new']","1f71e7e7":"rf = RandomForestRegressor(random_state=RSEED)\n\nparameters_grid = {\n    'max_depth': [5, 15],\n    'min_samples_leaf': [2, 8],\n    'n_estimators': [50, 100],\n    'max_features': [5, 10]\n}\n\n# get predictions\ny_pred, grid_search = fit_predict(X_train, X_test, y_train, rf, parameters_grid)\n\n# calculate metrics\nscores, plot = get_reg_metrics(y_test, y_pred)","2ffe7f4c":"rfr2_res = display_df(scores, 'Random Forest Regressor')\nrfr2_res","21c7f5a1":"pd.DataFrame.from_dict(grid_search.best_params_, orient='index', columns=['Selected Value']).T","7cef810c":"imp = pd.DataFrame(grid_search.best_estimator_.fit(X_train, y_train).feature_importances_, \n                   index=X_train.columns, columns=['Importance']).sort_values('Importance')\n\nfeature_importance(imp).show()","3213fd35":"ab = AdaBoostRegressor(random_state=RSEED)\n\nparameters_grid = {\n    'n_estimators': [50, 100],\n    'learning_rate': [0.001, 0.01, 0.1]\n}\n\n# get predictions\ny_pred, grid_search = fit_predict(X_train, X_test, y_train, ab, parameters_grid)\n\n# calculate metrics\nscores, plot = get_reg_metrics(y_test, y_pred)","60aa1be0":"abr2_res = display_df(scores, 'Ada-Boost Regressor')\nabr2_res","c85578b8":"pd.DataFrame.from_dict(grid_search.best_params_, orient='index', columns=['Selected Value']).T","cdce656b":"imp = pd.DataFrame(grid_search.best_estimator_.fit(X_train, y_train).feature_importances_, \n                   index=X_train.columns, columns=['Importance']).sort_values('Importance')\n\nfeature_importance(imp).show()","f74cd417":"reg_res1 = rfr1_res.copy()\nreg_res1 = reg_res1.append(abr1_res)\nreg_res2 = rfr2_res.copy()\nreg_res2 = reg_res2.append(abr2_res)","7dd99a61":"reg_res1.style.set_properties(**{'text-align': 'center'}).set_table_styles([dict(selector='th', props=[('text-align', 'left')])])","0af764fb":"reg_res2.style.set_properties(**{'text-align': 'center'}).set_table_styles([dict(selector='th', props=[('text-align', 'left')])])","503950fa":"class_res.style.set_properties(**{'text-align': 'center'}).set_table_styles([dict(selector='th', props=[('text-align', 'left')])])","00a59fa8":"reg_res1.style.set_properties(**{'text-align': 'center'}).set_table_styles([dict(selector='th', props=[('text-align', 'left')])])","0c8467fa":"reg_res2.style.set_properties(**{'text-align': 'center'}).set_table_styles([dict(selector='th', props=[('text-align', 'left')])])","d7e96378":"According to the Random Forest algorithm, the most important features are:\n- Online boarding\n- Inflight wifi service\n- Type of Travel - Personal Travel ","db27b539":"### Ada-Boost Classifier","e7a0a049":"### Correlation","0d0332cc":"We select $k=3$ and fit the model.","d76db547":"Most features don't seem to be very correlated, except for `Departure Delay in Minutes` and `Arrival Delay in Minutes` which have a correlation score of 0.96.","c87e032c":"## 6.2 Method 1 - Drop Features <a class=\"anchor\" id=\"section6.2\"><\/a>","30f2177a":"We fit a Random Forest and and Ada-Boost model and find the most important features.","65714fde":"We define three functions to avoid code repetition:","c081fdc2":"**Selected Parameters:**","d20456e2":"In the following plots, we present the different features and the percentage of observations that belong to every category, divided by cluster.","0553ef41":"### One-Hot Encoding","c4c42867":"**Selected Parameters:**","2e14eed3":"In this section, we create a new label that consists of the average score given by the costumers for the following features:\n- Gate location\n- Seat comfort\n- Cleanliness\n\nThen we apply two regression algorithms on the data, using two different approaches:\n1. Drop the features - the features that were used to create the label are dropped.\n2. Don't drop the features - the features that were used to create the label are **not** dropped.\n\nTo compare the methods and approaches, we calculate the $\\text{R}^2$ and MSE.\n\n$\\text{R}^2$: the coefficient of determination.\n\nIf $\\hat{y}_i$ is the predicted value of the $i$-th sample and $y_i$ is the corresponding true value for total $n$ samples, the estimated $\\text{R}^2$ is defined as:\n\n$$\\text{R}^2(y, \\hat{y}) = 1 - \\frac{\\sum_{i=1}^n (y_i - \\hat{y}_i)^2}{\\sum_{i=1}^n (y_i - \\bar{y})^2}$$\n\nwhere $\\bar{y} = \\frac{1}{n} \\sum_{i=1}^n y_i$ and $\\sum_{i=1}^n (y_i - \\hat{y}_i)^2 = \\sum_{i=1}^n \\varepsilon_i^2$.\n\n**MSE**: corresponds to the expected value of the squared (quadratic) error or loss.\n\n$$\\text{MSE}(y, \\hat{y}) = \\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{y}_i)^2$$","0801df93":"## 5.3 MLP (Multi-Layer Perceptron) <a class=\"anchor\" id=\"section5.3\"><\/a>","b52174f1":"The best predictions were given by the Random Forest algorithm, closely followed by MLP. Next, the Ada-Boost algorithm had quite good prediction, and lastly, the Logistic Regression had the worst predictions.","724bef97":"We use the K-Means clustering algorithm to identify clusters and find important features in each cluster.","7ec34471":"**Selected Parameters:**","5d61c904":"Next, we create a scree plot to visualize the percentage of explained variance for each PC.","86d375d7":"We define a function `get_melted_clusters` to assign the observations to the clusters created by K-Means.","726d52e7":"# 5 Classification <a class=\"anchor\" id=\"section5\"><\/a>","47985265":"1. The `get_reg_metrics` function calculates the $\\text{R}^2$ and MSE, given the test and predicted label, and creates a scatterplot.","a447d5d3":"3. The `feature_importance` method creates the plot to show the importance of each feature in the model.","aadf40d8":"**Selected Parameters:**","f10bae05":"## 5.1 Random Forest Classifier <a class=\"anchor\" id=\"section5.1\"><\/a>","4cb2995c":"We see a few interesting points:\n- Males and females have similar satisfaction rates.\n- Disployal costumers are more dissatisfied compared to loyal costumers.\n- Personal travel are more dissatisfied compared to buisness travel.\n- Eco class are more dissatisfied compared to buisness class.","18ef3ecd":"## 6.1 New Label <a class=\"anchor\" id=\"section6.1\"><\/a>","afe0a58b":"### Missing Values","6c5bb468":"***","5369c8e4":"# 1 Data <a class=\"anchor\" id=\"section1\"><\/a>","23cdcf90":"## 4.4 Conclusion <a class=\"anchor\" id=\"section4.4\"><\/a>","a7d12178":"![Wallpaper%20Airplane%20Passenger%20Airplanes%20Sky%20Flight%20Clouds%20Aviation.jpg](attachment:Wallpaper%20Airplane%20Passenger%20Airplanes%20Sky%20Flight%20Clouds%20Aviation.jpg)","7b0f15bd":"# 6 Regression <a class=\"anchor\" id=\"section6\"><\/a>","2c47e1a4":"We define three functions to avoid code repetition:","d3d4aa62":"We see that using the second method, the algorithms had extremely higher scores of $\\text{R}^2$, much higher than using the first method, and also lower MSE scores.\n\nIn terms of feature importance, using the second method, i.e. not dropping the features used to create the label, these features had the highest importance scores. In the Ada-Boost model, it seems like they were almost the only features used.","eb248701":"## 4.3 Classification Algorithms <a class=\"anchor\" id=\"section4.3\"><\/a>","89877e45":"2. The `display_df` method is used to create and display the dataframe for the $\\text{R}^2$ and MSE scores.","438dc3ad":"Earlier, we saw that the variable `Arrival Delay in Minutes` is highly correlated with the variable `Departure Delay in Minutes`, and we wish to drop one. As `Arrival Delay in Minutes` has many missing values, we've decided to drop it.","0ffa5c7f":"### Conclusions\n\n**Cluster 0:**\n- Quite neutral satisfaction (a bit more satisfied).\n- Features that had high ratings:\n    - Cleanliness\n    - Food and drink\n    - Inflight entertainment\n- Features that had low ratings: \n    - Departure\/Arrival time\n    - Ease of Online booking\n    - Gate location\n\n\n**Cluster 1:**\n- Not satisfied with their experience.\n- Mostly disloyal costumers in Eco\/Eco Plus class for personal travel.\n- Features that had extremely low ratings: \n    - Cleanliness\n    - Food and drink\n    - Inflight entertainment\n    - Inflight service\n    - Seat comfort\n\n\n**Cluster 2:**\n- Mostly satisfied with their experience.\n- Mostly loyal costumers in buisness class for buisness travel.\n- Features that had extremely high ratings:\n    - Departure\/Arrival time\n    - Ease of Online booking\n    - Gate location\n    - Inflight wifi service","e9e34099":"# 4 Feature Importance <a class=\"anchor\" id=\"section4\"><\/a>","9fc7cf4d":"In the following table we see the 10 most important features for the first principal component.","970c64d5":"***","fa96ba2a":"2. The `get_class_metrics` function calculates the accuracy and F1 Score, given the test and predicted label, and creates a confusion matrix.","0332449c":"***","fcf88745":"The columns `Unnamed: 0` and `id` are irrelevant and thus will be dropped.","55ae3270":"We create a new label that consists of the average score given by the costumers for the following features:\n- Gate location\n- Seat comfort\n- Cleanliness","401e5b2e":"**Feature Importance:**","b6fa54fa":"The $\\text{R}^2$ and MSE scores of the first and second method are presented in the first and second table, respectively.","9e9ff031":"***","d4ecab65":"# 3 Data Wrangling <a class=\"anchor\" id=\"section3\"><\/a>","0408b337":"Next, we used four classification models to predict the satisfaction. The best predictions were given by the Random Forest algorithm, closely followed by MLP.\n\nWe assume that we could improve the performance of the algorithms by increasing our hyperparameter tuning.","5cbb9b60":"According to the Ada-Boost algorithm, the most important features are:\n- Inflight wifi service\n- Type of Travel - Personal Travel\n- Online boarding \n\nThe same features as the Random Forest, but in different order.","348f6d0c":"## 4.2 Visualization Algorithm - PCA <a class=\"anchor\" id=\"section4.2\"><\/a>","df42c02f":"### Random Forest Classifier","00a8e014":"### Random Forest Regressor","fad981ca":"## 4.1 Clustering Algorithm - K-Means <a class=\"anchor\" id=\"section4.1\"><\/a>","0713a237":"***","46d9fcaa":"***","c9b9d28c":"## 6.4 Comparison <a class=\"anchor\" id=\"section6.4\"><\/a>","3eb3ee47":"**Selected Parameters:**","c67d464c":"## 5.2 Ada-Boost Classifier","2a113be0":"# Table of Contents\n\n### 0 [Libraries](#section0)\n\n### 1 [Data](#section1)\n\n### 2 [Data Visualization](#section2)\n\n### 3 [Data Wrangling](#section3)\n\n### 4 [Feature Importance](#section4)\n* 4.1 [Clustering Algorithm - K-Means](#section4.1)\n* 4.2 [Visualization Algorithm - PCA](#section4.2)\n* 4.3 [Classification Algorithms](#section4.3)\n* 4.4 [Conclusions](#section4.4)\n\n### 5 [Classification](#section5)\n* 5.1 [Random Forest Classifier](#section5.1)\n* 5.2 [Ada-Boost Classifier](#section5.2)\n* 5.3 [MLP (Multi-Layer Perceptron)](#section5.3)\n* 5.4 [Logistic Regression](#section5.4)\n* 5.5 [Comparison](#section5.5)\n\n### 6 [Regression](#section6)\n* 6.1 [New Label](#section6.1)\n* 6.2 [Method 1 - Drop Features](#section6.2)\n* 6.3 [Method 2 - Don't Drop Features](#section6.3)\n* 6.4 [Methods Comparison](#section6.4)\n\n### 7 [Discussion and Conclusions](#section7)","68bfee86":"In this project, we attempted to find the features that affect the satisfaction of the clients the most. In addition, we used different algorithms to predict that satisfaction.","0b39113f":"First, we define th `elbow_met` function to find the optimal number of clusters $k$.","855d8782":"1. The `fit_predict` function, uses the input train data to fit a given model, and predicts the label for the test features. A parameters grid is also recieved as input and is used to for hyperparameter optimization.","3bd19714":"### Ada-Boost Regressor","e32ca0b7":"## 5.5 Comparison <a class=\"anchor\" id=\"section5.5\"><\/a>","8aba1cf8":"**Feature Importance:**","13a0ce4a":"# 0 Libraries <a class=\"anchor\" id=\"section0\"><\/a>","1f14c14d":"***","2d68609d":"In this section, we attempt to find the features that affect the clients' satisfaction the most, using different methods.","1d524d89":"We saw that using the second method, the features used to create the label had the highest importance scores and the algorithms gave much better predictions, compared to the first method, where we dropped the features.","4b136261":"# 2 Data Visualization <a class=\"anchor\" id=\"section2\"><\/a>","89420205":"**Selected Parameters:**","031f7e71":"### Ada-Boost Regressor","5fc07deb":"**Feature Importance:**","1b86d5a9":"According to the first two methods, K-Means clustering and PCA, the most important features are:\n- Cleanliness\n- Food and drink\n- Inflight entertainment\n\nAccording to the classification algorithms, the most important features are:\n- Online boarding\n- Inflight wifi service\n- Type of Travel - Personal Travel ","d2176cff":"In the first method, the features that were used to create the label are dropped.\n\nWe apply the Random Forest and Ada-Boost algorithms, and calculate feature importance.","766ac7a0":"3. The `display_df` method is used to create and display the dataframe for the accuracy and F1 scores.","c46f9b33":"### Scaling","ff767dd7":"### Categorical Variables","93ec13fd":"Lastly, we created a new label that consists of the average score given by the costumers for the following features:\n- Gate location\n- Seat comfort\n- Cleanliness\n\nWe applied two regression algorithms on the data, using two different approaches:\n1. Drop the features - the features that were used to create the label are dropped.\n2. Don't drop the features - the features that were used to create the label are **not** dropped.\n\nThe $\\text{R}^2$ and MSE scores of the first and second method are presented in the first and second table, respectively.","1cd7f4c5":"In this section, we apply several classification algorithms on the data. To compare the methods, we calculate the accuracy and F1 score.\n\n**Accuracy**: refers to the ability to correctly predict both positive and negative obseravtions.\n\n$$\\text{Accuracy} = \\frac{\\text{True Positive} + \\text{True Negative}}{\\text{True Positive} + \\text{True Negative} + \\text{False Positive} + \\text{False Negative}}$$\n\n**F1 Score**: conveys the balance between the precision and the recall.\n\n$$F_1 = 2\\cdot \\frac{\\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}}$$\n\nwhere precision and recall and defined by\n$$\\text{Precision} = \\frac{\\text{True Positive}}{\\text{True Positive} + \\text{False Positive}}~, \\quad \\text{Recall} = \\frac{\\text{True Positive}}{\\text{True Positive} + \\text{False Negative}}$$\n\nWe believe that these two measures are good evaluation metrics for our case because the data is balanced and we want both good precision and recall.","14102308":"# Airline Passenger Satisfaction Analysis","af8c9f57":"***","6461d94b":"### Target Variable - Satisfaction\n\nIt seems like the data is quite balanced, with slightly more neutral or dissatisfied costumers.","9722d180":"**Selected Parameters:**","3316eb65":"***","ef3109b3":"We found that the most important features were:\n- Cleanliness\n- Food and drink\n- Inflight entertainment\n- Online boarding\n- Inflight wifi service\n- Type of Travel - Personal Travel ","dbefec1e":"### Labels","cceb0b3c":"The dataset contains an airline passenger satisfaction survey. In this project, we attempt to find the features that affect the satisfaction of the clients the most. In addition, we use different algorithms to predict that satisfaction.\n\n**Content:**\n- Gender: Gender of the passengers (Female, Male)\n- Customer Type: The customer type (Loyal customer, disloyal customer)\n- Age: The actual age of the passengers\n- Type of Travel: Purpose of the flight of the passengers (Personal Travel, Business Travel)\n- Class: Travel class in the plane of the passengers (Business, Eco, Eco Plus)\n- Flight distance: The flight distance of this journey\n- Inflight wifi service: Satisfaction level of the inflight wifi service (0:Not Applicable; 1-5)\n- Departure\/Arrival time convenient: Satisfaction level of departure\/arrival time convenient\n- Ease of Online booking: Satisfaction level of online booking\n- Gate location: Satisfaction level of gate location\n- Food and drink: Satisfaction level of Food and drink\n- Online boarding: Satisfaction level of online boarding\n- Seat comfort: Satisfaction level of seat comfort\n- Inflight entertainment: Satisfaction level of inflight entertainment\n- On-board service: Satisfaction level of on-board service\n- Leg room service: Satisfaction level of leg room service\n- Baggage handling: Satisfaction level of baggage handling\n- Check-in service: Satisfaction level of check-in service\n- Inflight service: Satisfaction level of inflight service\n- Cleanliness: Satisfaction level of Cleanliness\n- Departure Delay in Minutes: Minutes delayed when departure\n- Arrival Delay in Minutes: Minutes delayed when Arrival\n- Satisfaction: Airline satisfaction level (Satisfaction, neutral or dissatisfaction)","33ded8c2":"### Random Forest Regressor","0cd3f175":"**Selected Parameter:**","0753b265":"We use the PCA algorithm to find important features.","38d115d9":"***","a65a9b86":"**Feature Importance:**","8a5cc812":"## 6.3 Method 2 - Don't Drop Features <a class=\"anchor\" id=\"section6.3\"><\/a>","4648e4cd":"# 7 Discussion and Conclusions <a class=\"anchor\" id=\"section7\"><\/a>","461fac58":"In the first method, the features that were used to create the label are **not** dropped.\n\nWe apply the Random Forest and Ada-Boost algorithms, and calculate feature importance.","2736727f":"First, we fit and transform the data using PCA.","aa8fb854":"## 5.4 Logistic Regression <a class=\"anchor\" id=\"section5.4\"><\/a>"}}