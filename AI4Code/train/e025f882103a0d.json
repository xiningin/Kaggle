{"cell_type":{"cf0af09f":"code","53e8909b":"code","8d754e11":"code","d43659aa":"code","08712cfe":"code","f5a47d97":"code","c4de77f6":"code","b28d0d89":"code","640d2e90":"code","b46c0cdb":"code","3006e222":"code","6b4ea3d5":"code","700a55ae":"code","5dbfb5b7":"code","d638c5a5":"code","a6af810d":"code","3e8cfb0b":"code","27912e2c":"code","af4239ae":"code","d4f654e6":"code","57341f4a":"code","c46ac360":"code","ea93a16b":"code","d15903ca":"code","7631a412":"code","8c2b91ac":"code","4c7b4ac8":"code","52c24bfc":"code","c241a7b3":"code","36d0048f":"code","8d056d40":"code","cd4523de":"code","0a2e7cd1":"code","75d591f2":"code","c3c167c4":"code","312435e5":"code","516a8e21":"code","27f24fba":"code","5e1a97c2":"code","2984a3d1":"markdown","58aae653":"markdown","f2f4c19b":"markdown","82b11780":"markdown","32e781b5":"markdown","a50aaf29":"markdown","53787559":"markdown","d7b506b9":"markdown","4105ce95":"markdown","a65c7531":"markdown","55b2313f":"markdown","308df0e1":"markdown","55af5dd9":"markdown"},"source":{"cf0af09f":"# Importing the libraries\n\n# For dealing with warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Data preprocessing and Vectorization\nimport numpy as np \nimport pandas as pd\n\n# Data Visualizations \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.graph_objects as go\nimport plotly.express as px\n\n# Making the NN model\nfrom keras.utils import to_categorical\nfrom keras.models import Sequential \nfrom keras.layers import Dense, LSTM, Dropout\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split","53e8909b":"# Loading the data \ndata = pd.read_csv(\"\/kaggle\/input\/spam-text-message-classification\/SPAM text message 20170820 - Data.csv\")\ndata.head()","8d754e11":"print(\"Total number of messages: \", data.shape[0])","d43659aa":"# General information about the data\ndata.info()","08712cfe":"# Is there any null or missing value?\ndata.isna().sum()","f5a47d97":"# How many data points in each class ?\ndata[\"Category\"].value_counts()","c4de77f6":"sns.set_style(\"whitegrid\")\nsns.countplot(data[\"Category\"])","b28d0d89":"# Let's see the distribution of length of messages in each category.\n# Make a new column saying msg_length\ndata [\"msg_len\"] = data[\"Message\"].apply(len)","640d2e90":"fig = px.violin(data_frame=data, y=\"msg_len\", points=\"all\", color=\"Category\", \n                width=800, height=600)\nfig.show()","b46c0cdb":"sns.set(style=\"whitegrid\")\nsns.violinplot(x=\"Category\", y=\"msg_len\", data=data)","3006e222":"spam_data = data[data[\"Category\"] == \"spam\"]\nham_data = data[data[\"Category\"] == \"ham\"]","6b4ea3d5":"def num_of_outliers(df, col):\n    q1 = np.percentile(df[col].values, q=25)\n    q3 = np.percentile(df[col].values, q=75)\n    iqr = q3 - q1\n    upper_limit = q3 + (iqr * 1.5)\n    lower_limit = q1 - (iqr * 1.5)\n    \n    print(\"Q1: \", q1)\n    print(\"Q3: \", q3)\n    print(\"IQR: \", iqr)\n    print(\"Lower fence: \", lower_limit)\n    print(\"Upper fence: \", upper_limit)\n    print()\n    num_outliers = df[(df[col] < lower_limit) | (df[col] > upper_limit)].shape[0]\n    \n    return num_outliers","700a55ae":"ham_num_out = num_of_outliers(df=ham_data, col=\"msg_len\")\n\nprint(\"Number of outliers in ham data: {}\".format(ham_num_out))","5dbfb5b7":"spam_num_out = num_of_outliers(df=spam_data, col=\"msg_len\")\n\nprint(\"Number of outliers in spam data: {}\".format(spam_num_out))","d638c5a5":"# Let's look at the few spams\nspam_messages = spam_data[\"Message\"]\nrandom_indices = np.random.randint(low=0, high=spam_messages.shape[0], size=5)\nfor idx in random_indices:\n    print(spam_messages.iloc[idx])\n    print()\n    print(\"#\"*70)\n    print()","a6af810d":"# Let's look at some of the ham messages\nham_messages = ham_data[\"Message\"]\nrandom_indices = np.random.randint(low=0, high=ham_messages.shape[0], size=5)\nfor idx in random_indices:\n    print(ham_messages.iloc[idx])\n    print()\n    print(\"#\"*70)\n    print()","3e8cfb0b":"def plot_wordclouds(message_column):\n    msg_string = \"\"\n    for msg in message_column.values:\n        msg_string += msg.lower() + \" \"\n\n    msg_string = msg_string.strip()    \n\n    from wordcloud import WordCloud, STOPWORDS\n    plt.figure(figsize=(20, 10))\n    wordcloud = WordCloud(width=1200, height=800, background_color=\"black\", stopwords=STOPWORDS, \n                          min_font_size=10, include_numbers=True).generate(msg_string)\n    plt.axis(\"off\")\n    plt.grid(None)\n    plt.imshow(wordcloud)","27912e2c":"plot_wordclouds(spam_messages)","af4239ae":"plot_wordclouds(ham_messages)","d4f654e6":"import re\ndef get_numbers(s):\n    return len(re.findall(\"\\d{5,}\", s))","57341f4a":"spam_data[\"number_count\"] = spam_data[\"Message\"].apply(get_numbers)\nspam_data[\"number_count\"].value_counts()","c46ac360":"ham_data[\"number_count\"] = ham_data[\"Message\"].apply(get_numbers)\nham_data[\"number_count\"].value_counts()","ea93a16b":"def get_num_urls(s):\n    return len(re.findall(\"(http|https|www)\\:\\\/\\\/[a-zA-Z0-9\\-\\.]+\\.[a-zA-Z]{2,3}(\\\/\\S*)?\", s))","d15903ca":"spam_data[\"url_count\"] = spam_data[\"Message\"].apply(get_num_urls)\nspam_data[\"url_count\"].value_counts()","7631a412":"ham_data[\"url_count\"] = ham_data[\"Message\"].apply(get_num_urls)\nham_data[\"url_count\"].value_counts()","8c2b91ac":"# Adding number count as feature\ndata[\"number_count\"] = data[\"Message\"].apply(get_numbers)","4c7b4ac8":"# Adding url count as feature\ndata[\"url_count\"] = data[\"Message\"].apply(get_num_urls)","52c24bfc":"# Removing the ourliers\ndata = data[data[\"msg_len\"] < 183]","c241a7b3":"# Data preprocessing \n\nimport re\nimport nltk\nfrom nltk import WordNetLemmatizer\nfrom nltk.corpus import stopwords\n\nstop_words = stopwords.words(\"english\")\n\ndef clean_message(msg):\n    # lowercase\n    msg = msg.lower()\n    # removing special characters\n    msg = re.sub(\"[^a-zA-Z]\", \" \", msg)\n    # word tokenize \n    msg = nltk.word_tokenize(msg)\n    # Lemmatization \n    lemma = WordNetLemmatizer()\n    msg = [lemma.lemmatize(word) for word in msg if word not in stop_words and len(word) > 1]\n    \n    # Joining the words back \n    msg = \" \".join(msg)\n    \n    return msg","36d0048f":"data[\"cleaned_msg\"] = data[\"Message\"].apply(clean_message)","8d056d40":"data[\"is_spam\"] = data[\"Category\"].apply(lambda x: 1 if x == \"spam\" else 0)","cd4523de":"data.drop(labels=[\"Message\", \"Category\"], inplace=True, axis=1)","0a2e7cd1":"data.head()","75d591f2":"# Let's go for tf-idf vectorizer","c3c167c4":"# I am taking very less features so that, it trains faster.\n# You can take something like 200 to 300 features. That should be good.\ntfidf = TfidfVectorizer(max_df=.8, max_features=50)\ntfidf_mat = tfidf.fit_transform(data.iloc[:,3].values)\ny = data[\"is_spam\"].values\n\nX = np.hstack((tfidf_mat.toarray(), data.iloc[:, 0:3].values))\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.3, stratify=y)","312435e5":"X_train_nn = X_train.reshape(-1, X_train.shape[1], 1)\nX_test_nn = X_test.reshape(-1, X_test.shape[1], 1)\ny_train_nn = to_categorical(y_train)\ny_test_nn = to_categorical(y_test)\n\n\nmodel = Sequential()\n\n# Units - outputs dimension \nmodel.add(LSTM(units=50, activation=\"relu\", return_sequences=True))\nmodel.add(Dropout(rate=0.2))\n\nmodel.add(LSTM(units = 50, return_sequences = True))\nmodel.add(Dropout(0.2))\n\n# Adding a third LSTM layer and some Dropout regularisation\nmodel.add(LSTM(units = 50, return_sequences = True))\nmodel.add(Dropout(0.2))\n\n# Adding a fourth LSTM layer and some Dropout regularisation\nmodel.add(LSTM(units = 50))\nmodel.add(Dropout(0.2))\n\nmodel.add(Dense(units=2))\n\nmodel.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n\nhistory = model.fit(X_train_nn, y_train_nn, epochs=10, batch_size=32, verbose=1, validation_data=(X_test_nn, y_test_nn))","516a8e21":"history.history","27f24fba":"plt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('Model accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()","5e1a97c2":"# Plot training & validation loss values\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Model loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()","2984a3d1":"We have two features in the dataset, one is the message body and other is the category whether it's a spam or a ham message. ","58aae653":"Length of messages differ drastically, across two categories.\n\nMedian length of spams and hams are 52 and 149 respectively. That is something we should look upon.","f2f4c19b":"Let' see few of the messages of both the categories.","82b11780":"### 2.1. Outlier Detection in terms of length for both the classes","32e781b5":"If we have a good peek on the data we will find that many spam messages have numbers (mobile number or winning lottery amount etc.) as well as URLs. We will make a features out of them.\n\n**number_count:** count of all the numbers with5 or greater than 5 digits.\n\n**url_count:** number of URL count in the message.\n\nLet's see number_count first for both of the data.","a50aaf29":"## Machine Learning problem formulation\n\n**Problem Statement: ** Make a binary classification model, which will classify given a message, whether that message is a spam or a ham (genuine).\n\n**Model to be used: ** We will use Recurrent Neural Network (RNN) with Long-Short Term Memory (LSTM) model to classify the messages. I will explain what an RNN is, in later part of the kernel.\n\n**Loss Function: ** We will use Categorical CrossEntropy as a loss, to train our RNN model.\n\n**Performance metric :** Which metric to use to evaluate the model? (Is our class label imbalanced ?)","53787559":"<h3>Conclusions: <\/h3> \n1. Total number of data points are 5572.\n2. There are no null values in both the features of data.","d7b506b9":"Overall coversations are less formal, be it ham or spam message. People have used many short words or message slangs.\n\nSpam messages seem to have more numbers, links, or currency signs with large numbers.\n\nLet's see the wordcloud for both the categories.","4105ce95":"## 2. Exploratory Data Analysis","a65c7531":"## Spam \/ Ham Detection \nSpam\/Ham Detection is a classic binary classification problem, which people in Gmail or Outlook must have been working with. Although they must be having complex method with a lot more features than only the mail body. Anyway, the gist remains the same, that is, binary classification of text. \n\nAlhtough spams and hams are popular in mails only, we are here classifying the text messages. Let's get started then.\n\n![spam_ham.jpeg](attachment:spam_ham.jpeg)","55b2313f":"## 1. Overview of Data","308df0e1":"### Data preprocessing, Feature Engineering","55af5dd9":"We will remove the outliers from spam messages as they seem to be very long length for few cases. We will not remove the outliers from spam messages though. They seem to be of comparable length if we see overall data. "}}