{"cell_type":{"44632fb3":"code","4420b683":"code","22a61d1d":"code","8405782f":"code","592742da":"code","e630512b":"code","38e5ce0f":"code","93bb47d4":"code","7d4d5024":"code","a0809216":"code","f1381825":"code","46cc9e2f":"code","e95bb6d0":"code","60e427f4":"code","a4c4fb0d":"code","71ddd09b":"code","f0cf22f9":"code","e430bb3e":"code","a81794b4":"code","13c7883d":"code","8ee25c44":"code","80b78a56":"code","fe7d0662":"code","f14dabe3":"code","2d7c2817":"code","4582734c":"code","c94559b2":"code","f811b88f":"code","13dcadc8":"code","54d0d640":"code","4bed5351":"code","d1d3ab51":"code","74c30517":"code","8e8d743b":"code","2a282893":"code","3885bb82":"code","1ec06d6e":"code","130e0603":"code","f9a60c12":"code","760b5d32":"code","931350b4":"code","326faf7c":"code","f0f5394f":"code","0a432be4":"code","f9ee6603":"markdown","7fb7d89f":"markdown","3a8cf950":"markdown","2ba07b1a":"markdown","93498cfa":"markdown","5bd58687":"markdown","3eb32f88":"markdown","c27bd4ba":"markdown","06beb509":"markdown","62b3633e":"markdown","a7bf8179":"markdown","dbda131a":"markdown","268021e4":"markdown","c9962407":"markdown","d699bea0":"markdown","6394ffab":"markdown","7c62b855":"markdown","08572cbb":"markdown","57396861":"markdown","e99670f5":"markdown","d0ca744f":"markdown","e6464bd4":"markdown","bb0b8954":"markdown","aaa90620":"markdown","4dc5dc8e":"markdown","daf17a0b":"markdown","74129aef":"markdown","ca5d79c0":"markdown","f4a2dace":"markdown","e5837a7c":"markdown","99170d65":"markdown","520004e2":"markdown","4325e325":"markdown","2ec3ec0f":"markdown","96c780e8":"markdown","f8a05799":"markdown","a25a76c6":"markdown","f62ad0dd":"markdown","dc9e63ed":"markdown","80251305":"markdown","a177cc6c":"markdown","3503e06c":"markdown","a4b6083a":"markdown","30658088":"markdown","a8512f51":"markdown","f52112b0":"markdown","3716dd74":"markdown","2c1468c5":"markdown","ea23782e":"markdown","6cab5903":"markdown","cc226379":"markdown","2f3fb4e9":"markdown","d94dd011":"markdown","5cd180ba":"markdown","5e94a081":"markdown","923e0994":"markdown"},"source":{"44632fb3":"!pip install pmdarima","4420b683":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport missingno as msno\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport matplotlib.dates as mdates\nimport scipy.stats\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error\nimport pylab\nsns.set(style='white')\nfrom pmdarima import auto_arima\nfrom statsmodels.tsa.stattools import adfuller\nimport statsmodels.api as sm\nfrom statsmodels.tsa.seasonal import seasonal_decompose\n","22a61d1d":"df=pd.read_csv('..\/input\/nifty50-stock-market-data\/RELIANCE.csv')","8405782f":"df['Date']=pd.to_datetime(df['Date'])\ndf.set_index(['Date'],inplace=True)","592742da":"df.head()","e630512b":"df.describe()","38e5ce0f":"df.shape","93bb47d4":"def missing_values_table(df):\n        # Total missing values\n        mis_val = df.isnull().sum()\n        \n        # Percentage of missing values\n        mis_val_percent = 100 * df.isnull().sum() \/ len(df)\n        \n        # Make a table with the results\n        mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\n        \n        # Rename the columns\n        mis_val_table_ren_columns = mis_val_table.rename(\n        columns = {0 : 'Missing Values', 1 : '% of Total Values'})\n        \n        # Sort the table by percentage of missing descending\n        mis_val_table_ren_columns = mis_val_table_ren_columns[\n            mis_val_table_ren_columns.iloc[:,1] != 0].sort_values(\n        '% of Total Values', ascending=False).round(1)\n        \n        # Print some summary information\n        print (\"Your selected dataframe has \" + str(df.shape[1]) + \" columns.\\n\"      \n            \"There are \" + str(mis_val_table_ren_columns.shape[0]) +\n              \" columns that have missing values.\")\n        \n        # Return the dataframe with missing information\n        return mis_val_table_ren_columns","7d4d5024":"missing_table=missing_values_table(df)\nmissing_table","a0809216":"msno.matrix(df)","f1381825":"df.Trades.plot()","46cc9e2f":"df.Trades[:2850]","e95bb6d0":"# removing missing columns\n\ndf.drop(['Trades','Deliverable Volume','%Deliverble'],axis=1,inplace=True)\n","60e427f4":"fig = go.Figure([go.Scatter(x=df.index, y=df['VWAP'])])\nfig.update_layout(\n    autosize=False,\n    width=1000,\n    height=500,\n    title='VWAP over time',\n    template=\"simple_white\",\n)\nfig.update_xaxes(title=\"Date\")\nfig.update_yaxes(title=\"VWAP\")\nfig.show()","a4c4fb0d":"sns.kdeplot(df['VWAP'],shade=True)","71ddd09b":"fig = go.Figure([go.Scatter(x=df.loc['2019', 'VWAP'].index,y=df.loc['2020', 'VWAP'])])\nfig.update_layout(\n    autosize=False,\n    width=1000,\n    height=500,\n    title='VWAP in 2019',\n    template=\"simple_white\",\n)\nfig.update_xaxes(title=\"Date\")\nfig.update_yaxes(title=\"VWAP\")\n\nfig.show()","f0cf22f9":"fig = go.Figure([go.Scatter(x=df.loc['2020', 'VWAP'].index,y=df.loc['2020', 'VWAP'])])\nfig.update_layout(\n    autosize=False,\n    width=1000,\n    height=500,\n    title='VWAP in 2020',\n    template=\"simple_white\",\n)\nfig.update_xaxes(title=\"Date\")\nfig.update_yaxes(title=\"VWAP\")\nfig.show()","e430bb3e":"cols_plot = ['Open', 'Close', 'High','Low']\naxes = df[cols_plot].plot(figsize=(11, 9), subplots=True)\nfor ax in axes:\n    ax.set_ylabel('Daily trade')","a81794b4":"fig = go.Figure([go.Scatter(x=df.index, y=df['Volume'])])\nfig.update_layout(\n    autosize=False,\n    width=1000,\n    height=500,\n    template='simple_white',\n    title='Volume over time'\n)\nfig.update_xaxes(title=\"Date\")\nfig.update_yaxes(title=\"Volume\")\nfig.show()","13c7883d":"fig = go.Figure([go.Scatter(x=df.loc['2020', 'Volume'].index,y=df.loc['2020', 'Volume'])])\nfig.update_layout(\n    autosize=False,\n    width=1000,\n    height=500,\n    template='simple_white',\n    title='Volume in 2020'\n)\nfig.update_xaxes(title=\"Date\")\nfig.update_yaxes(title=\"Volume\")\nfig.show()","8ee25c44":"scipy.stats.probplot(df.VWAP,plot=pylab)\npylab.show()","80b78a56":"def dicky_fuller_test(x):\n    result = adfuller(x)\n    print('ADF Statistic: %f' % result[0])\n    print('p-value: %f' % result[1])\n    print('Critical Values:')\n    for key, value in result[4].items():\n        print('\\t%s: %.3f' % (key, value))\n    if result[1]>0.05:\n        print(\"Fail to reject the null hypothesis (H0), the data is non-stationary\")\n    else:\n        print(\"Reject the null hypothesis (H0), the data is stationary.\")","fe7d0662":"dicky_fuller_test(df['VWAP'])","f14dabe3":"from statsmodels.tsa.seasonal import seasonal_decompose\nfrom dateutil.parser import parse\n\nplt.rcParams.update({'figure.figsize': (10,10)})\ny = df['VWAP'].to_frame()\n\n\n# Multiplicative Decomposition \nresult_mul = seasonal_decompose(y, model='multiplicative',period = 52)\n\n# Additive Decomposition\nresult_add = seasonal_decompose(y, model='additive',period = 52)\n\n# Plot\nplt.rcParams.update({'figure.figsize': (10,10)})\nresult_mul.plot().suptitle('Multiplicative Decompose', fontsize=22)\nresult_add.plot().suptitle('Additive Decompose', fontsize=22)\nplt.show()","2d7c2817":"df['vwap_diff']=df['VWAP']-df['VWAP'].shift(1)","4582734c":"fig = go.Figure([go.Scatter(x=df.index,y=df.VWAP)])\nfig.update_layout(\n    autosize=False,\n    width=1000,\n    height=500,\n    template='simple_white',\n    title='VWAP over time ')\nfig.show()","c94559b2":"fig = go.Figure([go.Scatter(x=df.index,y=df.vwap_diff)])\nfig.update_layout(\n    autosize=False,\n    width=1000,\n    height=500,\n    template='simple_white',\n    title='difference VWAP over time ')\nfig.show()","f811b88f":"sm.graphics.tsa.plot_acf(df['VWAP'].iloc[1:], lags=40,title='auto correlation of VWAP',zero=False)\nplt.show()","13dcadc8":"sm.graphics.tsa.plot_acf(df['vwap_diff'].iloc[7:], lags=40,title='auto correlation of difference VWAP',zero=False)\nplt.show()","54d0d640":"sm.graphics.tsa.plot_pacf(df['VWAP'].iloc[1:], lags=40,title='partial auto correlation of VWAP',zero=False)\nplt.show()","4bed5351":"sm.graphics.tsa.plot_pacf(df['vwap_diff'].iloc[1:], lags=40,title='partial autocorrelation of difference VWAP  ',zero=False)\nplt.show()","d1d3ab51":"df.head()","74c30517":"df=df.reset_index()","8e8d743b":"lag_features = [\"High\", \"Low\", \"Volume\", \"Turnover\",\"Close\"]\nwindow1 = 3\nwindow2 = 7\nwindow3 = 30\n\ndf_rolled_3d = df[lag_features].rolling(window=window1, min_periods=0)\ndf_rolled_7d = df[lag_features].rolling(window=window2, min_periods=0)\ndf_rolled_30d = df[lag_features].rolling(window=window3, min_periods=0)\n\ndf_mean_3d = df_rolled_3d.mean().shift(1).reset_index().astype(np.float32)\ndf_mean_7d = df_rolled_7d.mean().shift(1).reset_index().astype(np.float32)\ndf_mean_30d = df_rolled_30d.mean().shift(1).reset_index().astype(np.float32)\n\ndf_std_3d = df_rolled_3d.std().shift(1).reset_index().astype(np.float32)\ndf_std_7d = df_rolled_7d.std().shift(1).reset_index().astype(np.float32)\ndf_std_30d = df_rolled_30d.std().shift(1).reset_index().astype(np.float32)\n\nfor feature in lag_features:\n    df[f\"{feature}_mean_lag{window1}\"] = df_mean_3d[feature]\n    df[f\"{feature}_mean_lag{window2}\"] = df_mean_7d[feature]\n    df[f\"{feature}_mean_lag{window3}\"] = df_mean_30d[feature]\n    \n    df[f\"{feature}_std_lag{window1}\"] = df_std_3d[feature]\n    df[f\"{feature}_std_lag{window2}\"] = df_std_7d[feature]\n    df[f\"{feature}_std_lag{window3}\"] = df_std_30d[feature]\n\ndf.fillna(df.mean(), inplace=True)\n\ndf.set_index(\"Date\", drop=False, inplace=True)","2a282893":"\ndf.Date = pd.to_datetime(df.Date, format=\"%Y-%m-%d\")\ndf[\"month\"] = df.Date.dt.month\ndf[\"week\"] = df.Date.dt.week\ndf[\"day\"] = df.Date.dt.day\ndf[\"day_of_week\"] = df.Date.dt.dayofweek\n","3885bb82":"df.head()","1ec06d6e":"\ndf_train = df[df.Date < \"2019\"]\ndf_valid = df[df.Date >= \"2019\"]\n\nexogenous_features = [\"High_mean_lag3\", \"High_std_lag3\", \"Low_mean_lag3\", \"Low_std_lag3\",\n                      \"Volume_mean_lag3\", \"Volume_std_lag3\", \"Turnover_mean_lag3\",\n                      \"Turnover_std_lag3\",\"High_mean_lag7\", \"High_std_lag7\", \"Low_mean_lag7\", \"Low_std_lag7\",\n                      \"Volume_mean_lag7\", \"Volume_std_lag7\", \"Turnover_mean_lag7\",\n                      \"Turnover_std_lag7\",\"High_mean_lag30\", \"High_std_lag30\", \"Low_mean_lag30\", \"Low_std_lag30\",\n                      \"Volume_mean_lag30\", \"Volume_std_lag30\", \"Turnover_mean_lag30\",\n                      \"Close_mean_lag3\", \"Close_mean_lag7\",\"Close_mean_lag30\",\"Close_std_lag3\",\"Close_std_lag7\",\"Close_std_lag30\",\n                      \"Turnover_std_lag30\",\"month\",\"week\",\"day\",\"day_of_week\"]\n","130e0603":"model = auto_arima(df_train.VWAP, exogenous=df_train[exogenous_features], trace=True, error_action=\"ignore\", suppress_warnings=True)\nmodel.fit(df_train.VWAP, exogenous=df_train[exogenous_features])\n\nforecast = model.predict(n_periods=len(df_valid), exogenous=df_valid[exogenous_features])\ndf_valid[\"Forecast_ARIMAX\"] = forecast","f9a60c12":"model.summary()","760b5d32":"df_valid[[\"VWAP\", \"Forecast_ARIMAX\"]].plot(figsize=(14, 7))","931350b4":"residuals=df_valid.VWAP-df_valid.Forecast_ARIMAX","326faf7c":"dicky_fuller_test((residuals))","f0f5394f":"residuals.plot()","0a432be4":"print(\"RMSE of Auto ARIMAX:\", np.sqrt(mean_squared_error(df_valid.VWAP, df_valid.Forecast_ARIMAX)))\nprint(\"\\nMAE of Auto ARIMAX:\", mean_absolute_error(df_valid.VWAP, df_valid.Forecast_ARIMAX))","f9ee6603":"## This notebook will the cover - \n\n* Data Preparation\n* Exploratory Data Analysis\n* Feature Engineering \n* AUTO-ARIMA Model\n* Analyzing residuals \n* Evaluating Model ","7fb7d89f":"## Check for missing values","3a8cf950":"* cyclic patter is shown in every 30 days (monthly)","2ba07b1a":"* we don't have data of Trades on and before 31-05-2011 ","93498cfa":"<font size=\"+3\" color='#780404'><b> Analyzing residuals <\/b><\/font>","5bd58687":"* as you can see there is deep in the month of march and april.\n* Steady increase after month of april .","3eb32f88":"![image.png](attachment:image.png)","c27bd4ba":"**Autocorrelation** and **partial autocorrelation** plots are heavily used in time series analysis and forecasting.\n\nThese are plots that graphically summarize the strength of a relationship with an observation in a time series with observations at prior time steps.\n\n**Statistical correlation** summarizes the strength of the relationship between two variables.\n\nWe can calculate the correlation for time series observations with observations with previous time steps, called lags. Because the correlation of the time series observations is calculated with values of the same series at previous times, this is called a **serial correlation, or an autocorrelation.**\n\nA plot of the autocorrelation of a time series by lag is called the AutoCorrelation Function, or the acronym ACF. This plot is sometimes called a **correlogram or an autocorrelation plot**.\n\n![image.png](attachment:image.png)\n","06beb509":"Adding lag values of High, Low, Volume,Turnover, will use three sets of lagged values, one previous day, one looking back 7 days and another looking back 30 days as a proxy for last week and last month metrics.","62b3633e":"## Plotting ACF and PACF ","a7bf8179":"## Handling missing values ","dbda131a":"* There are two picks in VWAP prices","268021e4":"<font size=\"+3\" color='#780404'><b> Data Preparation <\/b><\/font>","c9962407":"## Visualizing the locations of the missing data","d699bea0":"> Note - If you want to know more about stationarity , you can refer this [link](http:\/\/https:\/\/towardsdatascience.com\/stationarity-in-time-series-analysis-90c94f27322#:~:text=In%20the%20most%20intuitive%20sense,not%20itself%20change%20over%20time.)","6394ffab":"* We don't actually need to convert the time series data into stationary data. For study purpose,I have explained how to check stationarity and how to convert non-stationary data into stationary data ","7c62b855":"## Check Stationarity i.e Dicky Fuller Test","08572cbb":"* data is not normally distributed , however this is what we usually expect from timeseries ","57396861":"<font size=\"+3\" color='#780404'><b> AUTO-ARIMA Model <\/b><\/font>","e99670f5":"Reliance Industries Limited (RIL) is an Indian multinational conglomerate company headquartered in Mumbai, Maharashtra, India. Reliance owns businesses across India engaged in energy, petrochemicals, textiles, natural resources, retail, and telecommunications. Reliance is one of the most profitable companies in India,the largest publicly traded company in India by market capitalization,and the largest company in India as measured by revenue after recently surpassing the government-controlled Indian Oil Corporation.On 22 June 2020, Reliance Industries became the first Indian company to exceed US$150 billion in market capitalization after its market capitalization hit \u20b911,43,667 crore on the BSE.\nThe company is ranked 96th on the Fortune Global 500 list of the world's biggest corporations as of 2020.It is ranked 8th among the Top 250 Global Energy Companies by Platts as of 2016. Reliance continues to be India's largest exporter, accounting for 8% of India's total merchandise exports with a value of \u20b91,47,755 crore and access to markets in 108 countries.Reliance is responsible for almost 5% of the government of India's total revenues from customs and excise duty. It is also the highest income tax payer in the private sector in India.\n\n~ *Source - wikipedia*","d0ca744f":"## data summary","e6464bd4":"## Q-Q plot of VWAP \n\nused to determine whether dataset is distributed a certain way ","bb0b8954":"* There is steady increase in prices upto year 2008 \n* Stock price fell after jan 2008  and attain pick again in may-june 2009  after that it fell again .","aaa90620":"![image.png](attachment:image.png)","4dc5dc8e":"## Volume in 2020","daf17a0b":"## Volume over Time ","74129aef":"* All are following same pattern ","ca5d79c0":"In the most intuitive sense, stationarity means that the statistical properties of a process generating a time series do not change over time. It does not mean that the series does not change over time, just that the way it changes does not itself change over time. The algebraic equivalent is thus a linear function, perhaps, and not a constant one; the value of a linear function changes as \ud835\udc99 grows, but the way it changes remains constant \u2014 it has a constant slope; one value that captures that rate of change.","f4a2dace":"![TEAM%20KAGGLERS.png](attachment:TEAM%20KAGGLERS.png)","e5837a7c":"<font size=\"+3\" color='#780404'><b> Feature Engineering <\/b><\/font>","99170d65":"![image.png](attachment:image.png)","520004e2":"<font size=\"+3\" color='#053c96'><b> Introduction<\/b><\/font>\n\n","4325e325":"## Import dataset","2ec3ec0f":" ## Visualising using KDEs\n Summarizing the data with Density plots to see where the mass of the data is located","96c780e8":"A **partial autocorrelation** is a summary of the relationship between an observation in a time series with observations at prior time steps with the relationships of intervening observations removed.\n\nThe autocorrelation for an observation and an observation at a prior time step is comprised of both the direct correlation and indirect correlations. These indirect correlations are a linear function of the correlation of the observation, with observations at intervening time steps.\n\nIt is these indirect correlations that the partial autocorrelation function seeks to remove. Without going into the math, this is the intuition for the partial autocorrelation.\n\nA **partial autocorrelation** is a summary of the relationship between an observation in a time series with observations at prior time steps with the relationships of intervening observations removed.\n\nThe autocorrelation for an observation and an observation at a prior time step is comprised of both the direct correlation and indirect correlations. These indirect correlations are a linear function of the correlation of the observation, with observations at intervening time steps.\n\nIt is these indirect correlations that the partial autocorrelation function seeks to remove. Without going into the math, this is the intuition for the partial autocorrelation.\n\n![image.png](attachment:image.png)","f8a05799":"## VWAP in 2019","a25a76c6":"## Seasonal Decompose","f62ad0dd":"<font size=\"+3\" color='#053c96'><b> Reliance Industries Limited <\/b><\/font>","dc9e63ed":"* As you can see all the starting values are missing in columns Trades , Deliverable Volume\t and %Deliverble","80251305":"<font size=\"+3\" color='#780404'><b> Evaluating Model <\/b><\/font>","a177cc6c":"## Convert Stationary into Non Stationary","3503e06c":"* There are missing vales in Trades , Deliverable Volumne and % deliverable","a4b6083a":"## Converting Date into DateTime format ","30658088":"## Open,close,High,low prices over time ","a8512f51":"* There was many dips in year 2019 like in May-June , Aug-Sept and in end of the Sept month ","f52112b0":"The dataset used is stock market data of the Nifty-50 index from NSE (National Stock Exchange) India over the last 20 years (2000 - 2019)\n\nThe historic VWAP (Volume Weighted Average Price) is the target variable to predict. VWAP is a trading benchmark used by traders that gives the average price the stock has traded at throughout the day, based on both volume and price.\nRead more about the dataset: https:\/\/www.kaggle.com\/rohanrao\/nifty50-stock-market-data\n\nI am using Reliance stock prices .","3716dd74":"<font size=\"+3\" color='#780404'><b>Exploratory Data Analysis<\/b><\/font>","2c1468c5":"## Import Libraries","ea23782e":"Time series analysis comprises methods for analyzing time series data in order to extract meaningful statistics and other characteristics of the data. Time series forecasting is the use of a model to predict future values based on previously observed values.Whether we wish to predict the trend in financial markets or electricity consumption, time is an important factor that must now be considered in our models. For example, it would be interesting to forecast at what hour during the day is there going to be a peak consumption in electricity, such as to adjust the price or the production of electricity.\n","6cab5903":"<font size=\"+3\" color='#053c96'><b> About Dataset<\/b><\/font>","cc226379":"The Augmented Dickey-Fuller test is a type of statistical test called a unit root test.\n\nThe intuition behind a unit root test is that it determines how strongly a time series is defined by a trend\nIt uses an autoregressive model and optimizes an information criterion across multiple different lag values.\n\nThe null hypothesis of the test is that the time series can be represented by a unit root, that it is not stationary (has some time-dependent structure). The alternate hypothesis (rejecting the null hypothesis) is that the time series is stationary.\n\n**Null Hypothesis (H0)**: If failed to be rejected, it suggests the time series has a unit root, meaning it is non-stationary. It has some time dependent structure.\n\n**Alternate Hypothesis (H1)**: The null hypothesis is rejected; it suggests the time series does not have a unit root, meaning it is stationary. It does not have time-dependent structure.\n\nWe interpret this result using the p-value from the test. A p-value below a threshold (such as 5% or 1%) suggests we reject the null hypothesis (stationary), otherwise a p-value above the threshold suggests we fail to reject the null hypothesis (non-stationary).\n\np-value > 0.05: Fail to reject the null hypothesis (H0), the data has a unit root and is non-stationary.\np-value <= 0.05: Reject the null hypothesis (H0), the data does not have a unit root and is stationary.","2f3fb4e9":"### Differencing","d94dd011":"## Stationarity ","5cd180ba":"Formally, the process {x\u1d62 ; i\u2208\u2124} is weakly stationary if:\n1. The first moment of x\u1d62 is constant; i.e. \u2200t, E[x\u1d62]=\ud835\udf07\n2. The second moment of x\u1d62 is finite for all t; i.e. \u2200t, E[x\u1d62\u00b2]<\u221e (which also implies of course E[(x\u1d62-\ud835\udf07)\u00b2]<\u221e; i.e. that variance is finite for all t)\n3. The cross moment \u2014 i.e. the auto-covariance \u2014 depends only on the difference u-v; i.e. \u2200u,v,a, cov(x\u1d64, x\u1d65)=cov(x\u1d64\u208a\u2090, x\u1d65\u208a\u2090)","5e94a081":"## Plotting VWAP(Volume Weighted Average Price) over time","923e0994":"## VWAP in 2020"}}