{"cell_type":{"e4318ba5":"code","fd8e050b":"code","340eff04":"code","5cf29835":"code","90e54b09":"code","cf2f8788":"markdown","8fadcbec":"markdown","8a152bb3":"markdown","549e1935":"markdown","eb81450b":"markdown"},"source":{"e4318ba5":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","fd8e050b":"x_data = [[0, 0],\n          [0, 1],\n          [1, 0],\n          [1, 1]]\ny_data = [[0],\n          [1],\n          [1],\n          [0]]\n\nplt.scatter(x_data[0][0],x_data[0][1], c='red' , marker='^')\nplt.scatter(x_data[3][0],x_data[3][1], c='red' , marker='^')\nplt.scatter(x_data[1][0],x_data[1][1], c='blue' , marker='^')\nplt.scatter(x_data[2][0],x_data[2][1], c='blue' , marker='^')\n\nplt.xlabel(\"x1\")\nplt.ylabel(\"x2\")\nplt.show()","340eff04":"# 2 Layer Neural Net\uc744 \ud150\uc11c\ud50c\ub85c\uc6b0\ub85c \uad6c\ud604\ndef neural_net(features):\n    layer1 = tf.sigmoid(tf.matmul(features, W1) + b1)\n    layer2 = tf.sigmoid(tf.matmul(features, W2) + b2)\n    hypothesis = tf.sigmoid(tf.matmul(tf.concat([layer1, layer2], -1), W3) + b3)\n    return hypothesis","5cf29835":"tf.random.set_seed(777) # \uc77c\uc815\ud55c \ub370\uc774\ud130 \uacb0\uacfc\uac12\uc744 \uc5bb\uae30 \uc704\ud574 \ub79c\ub364 \uc2dc\ub4dc \uc124\uc815\n\nprint(tf.__version__)\n\n# x, y \ub370\uc774\ud130 \uc815\uc758\n\nx_data = [[0, 0],\n          [0, 1],\n          [1, 0],\n          [1, 1]]\ny_data = [[0],\n          [1],\n          [1],\n          [0]]\n\nplt.scatter(x_data[0][0],x_data[0][1], c='red' , marker='^')\nplt.scatter(x_data[3][0],x_data[3][1], c='red' , marker='^')\nplt.scatter(x_data[1][0],x_data[1][1], c='blue' , marker='^')\nplt.scatter(x_data[2][0],x_data[2][1], c='blue' , marker='^')\n\nplt.xlabel(\"x1\")\nplt.ylabel(\"x2\")\nplt.show()\n\n# \ub370\uc774\ud130\uc14b \uad6c\uc131\ndataset = tf.data.Dataset.from_tensor_slices((x_data, y_data)).batch(len(x_data))\n\nnb_classes = 10\n\nclass wide_deep_nn():\n    def __init__(self, nb_classes):\n        super(wide_deep_nn, self).__init__()\n    \n        # \ubaa8\ub378 \ud559\uc2b5\uc5d0 \uc0ac\uc6a9\ud560 W \uac12\uacfc b \uac12 \uc120\uc5b8\n        self.W1 = tf.Variable(tf.random.normal((2, nb_classes)), name='weight1')\n        self.b1 = tf.Variable(tf.random.normal((nb_classes,)), name='bias1')\n\n        self.W2 = tf.Variable(tf.random.normal((nb_classes, nb_classes)), name='weight2')\n        self.b2 = tf.Variable(tf.random.normal((nb_classes,)), name='bias2')\n\n        self.W3 = tf.Variable(tf.random.normal((nb_classes, nb_classes)), name='weight3')\n        self.b3 = tf.Variable(tf.random.normal((nb_classes,)), name='bias3')\n\n        self.W4 = tf.Variable(tf.random.normal((nb_classes, 1)), name='weight4')\n        self.b4 = tf.Variable(tf.random.normal((1,)), name='bias4')\n\n        self.variables = [self.W1,self.b1,self.W2,self.b2,self.W3,self.b3,self.W4,self.b4]\n        \n    # \uc815\uc218 \ub370\uc774\ud130\ub97c float32\ub85c \ubcc0\ud658\ud558\ub294 \ub370\uc774\ud130 \uc804\ucc98\ub9ac \ud568\uc218\n    def preprocess_data(self, features, labels):\n        features = tf.cast(features, tf.float32)\n        labels = tf.cast(labels, tf.float32)\n        return features, labels\n\n    # \ub274\ub7f4 \ub124\ud2b8\uc6cc\ud06c \uad6c\uc131\n    def deep_nn(self, features):\n        layer1 = tf.sigmoid(tf.matmul(features, self.W1) + self.b1)\n        layer2 = tf.sigmoid(tf.matmul(layer1, self.W2) + self.b2)\n        layer3 = tf.sigmoid(tf.matmul(layer2, self.W3) + self.b3)\n        hypothesis = tf.sigmoid(tf.matmul(layer3, self.W4) + self.b4)\n        return hypothesis\n\n    # \ud559\uc2b5\uc5d0 \uc0ac\uc6a9\ud560 \uc190\uc2e4 \ud568\uc218 \uad6c\ud604\n    def loss_fn(self, hypothesis, labels):\n        cost = -tf.reduce_mean(labels * tf.math.log(hypothesis) + (1 - labels) * tf.math.log(1 - hypothesis))\n        return cost\n\n    # \uc608\uc0c1 \uacb0\uacfc\uac12(predicted)\uacfc \uc2e4\uc81c \uacb0\uacfc\uac12(labels)\uc774 \uc77c\uce58\ud558\ub294\uc9c0 \ud655\uc778\ud558\ub294 \uc815\ud655\ub3c4 \ud568\uc218 \uad6c\ud604\n    def accuracy_fn(self, hypothesis, labels):\n        predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n        accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, labels), dtype=tf.float32))\n        return accuracy\n\n    # \ud559\uc2b5\uc5d0 \uc0ac\uc6a9\ud560 \uacbd\uc0ac\ub3c4 \ud568\uc218 \uad6c\ud604\n    def grad(self, features, labels):\n        with tf.GradientTape() as tape:\n            loss_value = self.loss_fn(self.deep_nn(features), labels)\n        return tape.gradient(loss_value, self.variables)\n\n    # \ud6c8\ub828 \uacfc\uc815 \uad6c\ud604\n    def fit(self, dataset, EPOCHS=20000, verbose=500):\n        # \ud559\uc2b5\uc5d0 \uc0ac\uc6a9\ud560 \uc635\ud2f0\ub9c8\uc774\uc800 \uc120\uc5b8\n        optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)\n        for step in range(EPOCHS):\n            for features, labels in dataset:\n                features, labels = self.preprocess_data(features, labels)\n                grads = self.grad(features, labels)\n                optimizer.apply_gradients(grads_and_vars=zip(grads, self.variables))\n                if step % verbose == 0:\n                    print(\"Iter: {}, Loss: {:.4f}\".format(step, self.loss_fn(self.deep_nn(features), labels)))\n\n    # \ubaa8\ub378 \ud14c\uc2a4\ud2b8 \uacfc\uc815 \uad6c\ud604\n    def test_model(self, x_data, y_data):\n        x_data, y_data = self.preprocess_data(x_data, y_data)\n        test_acc = self.accuracy_fn(self.deep_nn(x_data), y_data)\n        print(\"Testset Accuracy: {:.4f}\".format(test_acc))\n\nmodel = wide_deep_nn(nb_classes)\nmodel.fit(dataset)\nmodel.test_model(x_data, y_data)","90e54b09":"tf.random.set_seed(777) # \uc77c\uc815\ud55c \ub370\uc774\ud130 \uacb0\uacfc\uac12\uc744 \uc5bb\uae30 \uc704\ud574 \ub79c\ub364 \uc2dc\ub4dc \uc124\uc815\n\nprint(tf.__version__)\n\n# x, y \ub370\uc774\ud130 \uc815\uc758\nx_data = [[0, 0],\n          [0, 1],\n          [1, 0],\n          [1, 1]]\ny_data = [[0],\n          [1],\n          [1],\n          [0]]\n\n# \ub370\uc774\ud130\uc14b \uc120\uc5b8\ndataset = tf.data.Dataset.from_tensor_slices((x_data, y_data)).batch(len(x_data))\n\n# \ub370\uc774\ud130 \uc804\ucc98\ub9ac \uacfc\uc815 \uad6c\ud604\ndef preprocess_data(features, labels):\n    features = tf.cast(features, tf.float32)\n    labels = tf.cast(labels, tf.float32)\n    return features, labels\n\nlog_path = \".\/logs\/xor\"\nwriter = tf.summary.create_file_writer(log_path)\n\n# W, b \uac12 \uc124\uc815\nW1 = tf.Variable(tf.random.normal((2, 10)), name='weight1')\nb1 = tf.Variable(tf.random.normal((10,)), name='bias1')\n\nW2 = tf.Variable(tf.random.normal((10, 10)), name='weight2')\nb2 = tf.Variable(tf.random.normal((10,)), name='bias2')\n\nW3 = tf.Variable(tf.random.normal((10, 10)), name='weight3')\nb3 = tf.Variable(tf.random.normal((10,)), name='bias3')\n\nW4 = tf.Variable(tf.random.normal((10, 1)), name='weight4')\nb4 = tf.Variable(tf.random.normal((1,)), name='bias4')\n\n# \ub274\ub7f4 \ub124\ud2b8\uc6cc\ud06c \uc815\uc758\ndef neural_net(features, step):\n    layer1 = tf.sigmoid(tf.matmul(features, W1) + b1)\n    layer2 = tf.sigmoid(tf.matmul(layer1, W2) + b2)\n    layer3 = tf.sigmoid(tf.matmul(layer2, W3) + b3)\n    hypothesis = tf.sigmoid(tf.matmul(layer3, W4) + b4)\n    \n    # \uac01 W\uc640 b\uc758 \ubcc0\ud654\ub418\ub294 \uac12\uc744 \ud150\uc11c\ubcf4\ub4dc\uc758 Histogram\uc5d0 \uae30\ub85d\n    with writer.as_default():\n        tf.summary.histogram('weights1', W1, step=step)\n        tf.summary.histogram('biases1', b1, step=step)\n        tf.summary.histogram('layer1', layer1, step=step)\n        \n        tf.summary.histogram('weights2', W2, step=step)\n        tf.summary.histogram('biases2', b2, step=step)\n        tf.summary.histogram('layer2', layer2, step=step)\n        \n        tf.summary.histogram('weights3', W3, step=step)\n        tf.summary.histogram('biases3', b3, step=step)\n        tf.summary.histogram('layer3', layer3, step=step)\n        \n        tf.summary.histogram('weights4', W4, step=step)\n        tf.summary.histogram('biases4', b4, step=step)\n        tf.summary.histogram('hypothesis', hypothesis, step=step)\n        \n    return hypothesis\n\n# \ube44\uc6a9 \ud568\uc218 \uad6c\ud604\ndef loss_fn(hypothesis, labels):\n    cost = -tf.reduce_mean(labels * tf.math.log(hypothesis) + (1 - labels) * tf.math.log(1 - hypothesis))\n    \n    # \ube44\uc6a9\uc744 \ud150\uc11c\ubcf4\ub4dc\uc758 scalar\uc5d0 \uae30\ub85d\n    with writer.as_default():\n        tf.summary.scalar('loss', cost, step=step)\n    \n    return cost\n\n# \uc635\ud2f0\ub9c8\uc774\uc800 \uc120\uc5b8\noptimizer = tf.keras.optimizers.SGD(learning_rate=0.1)\n\n# \uc815\ud655\ub3c4 \uacc4\uc0b0 \ud568\uc218 \uad6c\ud604\ndef accuracy_fn(hypothesis, labels):\n    predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n    accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, labels), dtype=tf.float32))\n    return accuracy\n\n# \uacbd\uc0ac\ub3c4 \ud568\uc218 \uad6c\ud604\ndef grad(features, labels, step):\n    with tf.GradientTape() as tape:\n        loss_value = loss_fn(neural_net(features, step), labels)\n        \n    return tape.gradient(loss_value, [W1, W2, W3, W4, b1, b2, b3, b4])\n\n# \ud559\uc2b5 \uacfc\uc815 \uad6c\ud604\nEPOCHS = 3000\n\nfor step in range(EPOCHS):\n    for features, labels in dataset:\n        features, labels = preprocess_data(features, labels)\n        grads = grad(features, labels, step)\n        optimizer.apply_gradients(grads_and_vars=zip(grads, [W1, W2, W3, W4, b1, b2, b3, b4]))\n        if step % 50 == 0:\n            loss_value = loss_fn(neural_net(features, step), labels)\n            print(\"Iter: {}, Loss: {:.4f}\".format(step, loss_value))\n    \nx_data, y_data = preprocess_data(x_data, y_data)\ntest_acc = accuracy_fn(neural_net(x_data, step), y_data)\nprint(\"Testset Accuracy: {:.4f}\".format(test_acc))\n        \n","cf2f8788":"## \uc2e4\uc81c \ucf54\ub4dc \uad6c\ud604","8fadcbec":"# Lec 09-1: Neural Net for XOR\n***\n\n## Logistic Regression (Recap)\n- XOR\uc740 \ub450 \uac12\uc774 \uac19\uc744 \ub54c 0, \ub2e4\ub97c \ub54c 1\uc744 \ud45c\uc2dc\ud55c\ub2e4.\n- \ud558\ub098\uc758 Logistic \ud568\uc218\ub85c\ub294 XOR \ubb38\uc81c\ub97c \ud480 \uc218 \uc5c6\ub2e4.\n- \uadf8\ub7ec\ubbc0\ub85c 3\uac1c\uc758 Logistic \ud568\uc218\ub97c \uc5f0\uacb0\ud558\uc5ec \ud47c\ub2e4.","8a152bb3":"## Neural Network\n- 3\uac1c\uc758 Logistic\uc744 2\uac1c\uc758 \ub808\uc774\uc5b4\ub85c \ub098\ub220\uc11c \ubc30\uce58\ud558\uc5ec \ub124\ud2b8\uc6cc\ud06c\ub97c \uad6c\uc131\ud55c\ub2e4. (\uc774\ub97c 2 Layer Neural Net\uc774\ub77c \ud55c\ub2e4.)\n![image.png](attachment:image.png)","549e1935":"## Chart\n![image.png](attachment:image.png)","eb81450b":"# Lec 09-2: TensorBoard for XOR\n***\n## TensorBoard (Jupyter Notebook\uc6a9)\n- \ucf58\uc194\uc5d0 pip install tensorboard \uba85\ub839\uc73c\ub85c \ud150\uc11c\ubcf4\ub4dc \uc124\uce58\n- \ucf58\uc194\uc5d0 %load_ext tensorboard \ubc0f %tensorboard --logdir logs\/xor \uba85\ub839\uc73c\ub85c \ud150\uc11c\ubcf4\ub4dc \uc2e4\ud589\n- \uc774\ud6c4 http:\/\/127.0.0.1:6006 \ub85c \ud150\uc11c\ubcf4\ub4dc \uc811\uadfc \uac00\ub2a5\n"}}