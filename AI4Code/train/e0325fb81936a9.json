{"cell_type":{"315c2774":"code","2b7c7e35":"code","11c01e35":"code","18aa5cd6":"code","1c8e35eb":"code","0e30c05e":"code","c338bf17":"code","088d0f6c":"code","808250eb":"code","f96b8e7e":"code","011d2cee":"code","c13b3a8f":"code","6b3fbbdc":"code","23188b58":"code","2d95919c":"code","52e2da0f":"code","f19c4bbf":"code","f94f1e52":"code","dbef6097":"code","7ff4507b":"code","aa0a616f":"code","37af70f6":"code","1d1a4875":"code","a67f795a":"code","61ebc44f":"code","3e441cb7":"code","e3df28a8":"code","93cb84f8":"code","f27def3a":"code","69ea40f0":"code","615b8681":"code","da3269d3":"code","f0d9ef7c":"code","c8a35d05":"code","1bd44da0":"markdown","e83a3dab":"markdown","505d62db":"markdown","357f12c2":"markdown","3932043a":"markdown","b40ec98f":"markdown","934c521a":"markdown","3934e95f":"markdown","1a9c3330":"markdown","219ffe67":"markdown","6153ea5b":"markdown","7137ef47":"markdown","cd35e94b":"markdown","bce914b4":"markdown"},"source":{"315c2774":"from scipy import misc\nimport matplotlib.pyplot as plt\n\n\nimg = misc.face()  #  1024 x 768, color image of a raccoon face\nplt.imshow(img);","2b7c7e35":"import cv2\nimg = cv2.resize(img, (224, 224))\nplt.imshow(img);","11c01e35":"p = 16\nh, w = img.shape[:2]\n\nfig, ax = plt.subplots(h\/\/p, w\/\/p, figsize=(6,6))\nfor i in range(h\/\/p):\n    for j in range(w\/\/p):\n        patch = img[i*p:(i+1)*p, j*p:(j+1)*p]\n        ax[i, j].imshow(patch)\n        ax[i, j].axis('off')\nplt.show()","18aa5cd6":"patch.shape","1c8e35eb":"patch.flatten().shape","0e30c05e":"import sys\n\npackage_path = '..\/input\/vision-transformer-pytorch\/VisionTransformer-Pytorch'\npackage_path2 = '..\/input\/t2tvit'\npackage_path3 = '..\/input\/timm-pytorch-image-models\/pytorch-image-models-master'\nsys.path.append(package_path)\nsys.path.append(package_path2)\nsys.path.append(package_path3)","c338bf17":"import os\nimport pandas as pd\n\nimport time\nimport datetime\nimport copy\nimport matplotlib.pyplot as plt\nimport json\nimport seaborn as sns\nimport cv2\nimport albumentations as albu\nimport numpy as np\n\nimport torch\nimport torch.nn as nn\nimport torchvision.models as models\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import StratifiedKFold, GroupKFold, KFold, train_test_split\n\n\n# ALBUMENTATIONS\nimport albumentations as albu\n\nfrom albumentations import (\n    HorizontalFlip, VerticalFlip, IAAPerspective, ShiftScaleRotate, CLAHE, RandomRotate90,\n    Transpose, ShiftScaleRotate, Blur, OpticalDistortion, GridDistortion, HueSaturationValue,\n    IAAAdditiveGaussianNoise, GaussNoise, MotionBlur, MedianBlur, IAAPiecewiseAffine, RandomResizedCrop,\n    IAASharpen, IAAEmboss, RandomBrightnessContrast, Flip, OneOf, Compose, Normalize, Cutout, CoarseDropout, ShiftScaleRotate, CenterCrop, Resize\n)\n    \nfrom albumentations.pytorch import ToTensorV2","088d0f6c":"# BASE_DIR=\"..\/input\/plant-pathology-2021-fgvc8\/\"\nBASE_DIR = '..\/input\/plant-pathology-2021-224x224\/'\nTRAIN_IMAGES_DIR = os.path.join(BASE_DIR, 'train_imgs')","808250eb":"train_df = pd.read_csv('..\/input\/plant-pathology-2021-fgvc8\/train.csv')\ntrain_df.head()","f96b8e7e":"print(\"Count of training images {0}\".format(len(os.listdir(TRAIN_IMAGES_DIR))))","011d2cee":"class_name = train_df['labels'].value_counts().index\nclass_count = train_df['labels'].value_counts().values","c13b3a8f":"display(train_df['labels'].nunique())\ntrain_df.labels.value_counts()","6b3fbbdc":"plt.figure(figsize=(8,5))\nsns.countplot(data=train_df, y='labels');","23188b58":"from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\ntrain_df['labels'] = le.fit_transform(train_df.labels)","2d95919c":"def visualize_images(image_ids, labels):\n    plt.figure(figsize=(16, 12))\n    for idx, (image_id, label) in enumerate(zip(image_ids, labels)):\n        plt.subplot(3, 3, idx+1)\n        image = cv2.imread(os.path.join(TRAIN_IMAGES_DIR, image_id))[:, ::-1]  # rgb img\n        plt.imshow(image)\n        plt.title(f\"Class: {label}\", fontsize=12)\n        plt.axis(\"off\")\n    plt.show()\n    \n\ndef plot_augmentation(image_id, transform):\n    plt.figure(figsize=(16, 4))\n    plt.axis('off')\n    img = cv2.imread(os.path.join(TRAIN_IAMGES_DIR, image_id))[:, ::-1] \n    plt.subplot(1, 3, 1)\n    plt.imshow(img)\n#     plt.axis('off')\n    \n    plt.subplot(1, 3, 2)\n    x = transform(image=img)['image']\n    plt.imshow(x)\n#     plt.axis('off')\n    \n    plt.subplot(1, 3, 3)\n    x = transform(image=img)['image']\n    plt.imshow(x)\n    \ndef visualize(images, transform):\n    '''\n    Plot images and their transformations\n    '''\n    fig = plt.figure(figsize=(32, 16))\n    \n    for i, im in enumerate(images):\n        ax = fig.add_subplot(2, 5, i+1, xticks=[], yticks=[])\n        plt.imshow(im)\n        \n    for i, im in enumerate(images):\n        ax = fig.add_subplot(2, 5, i+6, xticks=[], yticks=[])\n        plt.imshow(transform(image=im)['image'])","52e2da0f":"# CUSTOM DATASET CLASS\nclass PlantDataset(Dataset):\n    def __init__(\n        self, df:pd.DataFrame, imfolder:str, train:bool=True, transforms=None\n    ):\n        self.df = df\n        self.imfolder = imfolder\n        self.train = train\n        self.transforms = transforms\n        \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, index):\n        im_path = os.path.join(self.imfolder, self.df.iloc[index]['image'])\n        im = cv2.imread(im_path, cv2.IMREAD_COLOR)\n        im = cv2.cvtColor(im, cv2.COLOR_BGR2RGB)\n        \n        if (self.transforms):\n            im = self.transforms(image=im)['image']\n            \n        if (self.train):\n            label = self.df.iloc[index]['labels']\n            return im, label\n        else:\n            return im","f19c4bbf":"# AUGMENTATIONS\n\ntrain_augs = albu.Compose([\n    albu.RandomResizedCrop(height=224, width=224, p=1.0),\n    albu.HorizontalFlip(p=0.5),\n    albu.VerticalFlip(p=0.5),\n    albu.RandomBrightnessContrast(p=0.5),\n    albu.ShiftScaleRotate(p=0.5),\n    albu.Normalize(    \n        mean=[0.3, 0.3, 0.3],\n        std=[0.1, 0.1, 0.1],),\n    CoarseDropout(p=0.5),\n    Cutout(p=0.5),\n    ToTensorV2(),\n])\n\nvalid_augs = albu.Compose([\n    albu.Resize(height=224, width=224, p=1.0),\n    albu.Normalize(\n        mean=[0.3, 0.3, 0.3],\n        std=[0.1, 0.1, 0.1],),\n    ToTensorV2(),\n])\n","f94f1e52":"# DATA SPLIT\ntrain, valid = train_test_split(\n    train_df,\n    test_size=0.1,\n    random_state=42,\n    stratify=train_df.labels.values\n)\n\n# reset index on both dataframes\ntrain = train.reset_index(drop=True)\nvalid = valid.reset_index(drop=True)\n\n# targets in train,valid datasets\ntrain_targets = train.labels.values\nvalid_targets = valid.labels.values","dbef6097":"# DEFINE PYTORCH CUSTOM DATASET\ntrain_dataset = PlantDataset(\n    df=train,\n    imfolder=TRAIN_IMAGES_DIR,\n    train=True,\n    transforms=train_augs\n)\n\nvalid_dataset = PlantDataset(\n    df=valid,\n    imfolder=TRAIN_IMAGES_DIR,\n    train=True,\n    transforms=valid_augs\n)","7ff4507b":"def plot_image_from_dataset(img_list):\n    image_tensor = img_list[0]\n    target = img_list[1]\n    print(target)\n    image = image_tensor.permute(1, 2, 0)\n    plt.imshow(image)","aa0a616f":"plot_image_from_dataset(train_dataset[7])","37af70f6":"# MAKE PYTORCH DATALOADER\ntrain_loader = DataLoader(\n    train_dataset,\n    batch_size=16,\n    num_workers=4,\n    shuffle = True\n)\n\nvalid_loader = DataLoader(\n    valid_dataset,\n    batch_size=16,\n    num_workers=4,\n    shuffle = False\n)","1d1a4875":"# TRAIN\ndef train_model(datasets, dataloaders, model, criterion, optimizer, scheduler, num_epochs, device):\n    since = time.time()\n    \n    best_model_wts = copy.deepcopy(model.state_dict()) # store best model weights\n    best_acc = 0.0\n    \n    for epoch in range(num_epochs):\n        print(f'Epoch {epoch+1}\/{num_epochs}')\n        print('=' * 10)\n        \n        for phase in ['train', 'valid']:\n            if phase == 'train':\n                model.train()\n            else:\n                model.eval()\n                \n            running_loss = 0.0\n            running_corrects = 0.0\n            running_total = 0.0\n            \n            for step, (inputs, labels) in enumerate(dataloaders[phase]):\n                inputs = inputs.to(device)\n                labels = labels.to(device)\n                model = model.to(device)\n\n                # Zero out the grads\n                optimizer.zero_grad()\n                \n                \n                with torch.set_grad_enabled(phase == 'train'):\n                    # Forward\n                    outputs = model(inputs)\n                    probabilities, preds = torch.max(outputs, 1) \n                    loss = criterion(outputs, labels)\n                    \n                    # Backward\n                    if phase == 'train':\n                        loss.backward()\n                        optimizer.step()\n                \n                # Statistics\n                running_loss += loss.item() * inputs.size(0)\n                running_corrects += torch.sum(preds == labels.data)\n                running_total += len(labels.data)\n                \n                if (step + 1) % 100 == 0:\n                    print(f'[{step + 1}\/{len(dataloaders[phase])}].')\n                    print(f'Loss {running_loss \/ (step + 1)}. Accuracy {running_corrects \/ running_total}')\n            \n            if phase == 'train':\n                scheduler.step()\n                \n            epoch_loss = running_loss \/ len(datasets[phase])\n            epoch_acc = running_corrects.double() \/ len(datasets[phase])\n            \n            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n                phase, epoch_loss, epoch_acc))\n            \n            if phase == 'valid' and epoch_acc > best_acc:\n                best_acc = epoch_acc\n                best_model_wts = copy.deepcopy(model.state_dict())\n        print()\n    \n    time_elapsed = time.time() - since\n    print(f'Training complete in {time_elapsed \/\/ 60:.0f}m {time_elapsed % 60:.0f}s')\n    print(f'Best val Acc: {best_acc:.4f}')\n    \n    model.load_state_dict(best_model_wts)\n    \n    return model","a67f795a":"def load_state_dict(checkpoint_path, use_ema=False, num_classes=1000):\n    if checkpoint_path and os.path.isfile(checkpoint_path):\n        checkpoint = torch.load(checkpoint_path, map_location='cpu')\n        state_dict_key = 'state_dict'\n        if isinstance(checkpoint, dict):\n            if use_ema and 'state_dict_ema' in checkpoint:\n                state_dict_key = 'state_dict_ema'\n        if state_dict_key and state_dict_key in checkpoint:\n            new_state_dict = OrderedDict()\n            for k, v in checkpoint[state_dict_key].items():\n                # strip `module.` prefix\n                name = k[7:] if k.startswith('module') else k\n                new_state_dict[name] = v\n            state_dict = new_state_dict\n        else:\n            state_dict = checkpoint\n#         _logger.info(\"Loaded {} from checkpoint '{}'\".format(state_dict_key, checkpoint_path))\n#         if num_classes != 1000:\n#             # completely discard fully connected for all other differences between pretrained and created model\n#             del state_dict['head' + '.weight']\n#             del state_dict['head' + '.bias']\n\n        return state_dict\n    else:\n#         _logger.error(\"No checkpoint found at '{}'\".format(checkpoint_path))\n        raise FileNotFoundError()\n\n\ndef load_for_transfer_learning(model, checkpoint_path, use_ema=False, strict=False, num_classes=1000):\n    state_dict = load_state_dict(checkpoint_path, use_ema, num_classes)\n    model.load_state_dict(state_dict, strict=strict)\n","61ebc44f":"stat = torch.load('..\/input\/t2tvit\/80.7_T2T_ViT_t_14.pth.tar')\n\ndel stat['state_dict_ema']['head.weight']\ndel stat['state_dict_ema']['head.bias']\n\ntorch.save(stat['state_dict_ema'],'80.7_T2T_ViT_t_14.pth.tar')","3e441cb7":"from models.t2t_vit import *\n# create model\nmodel = T2t_vit_14(img_size=224, num_classes=12)\n# load_for_transfer_learning(model, '80.7_T2T_ViT_t_14.pth.tar', num_classes=12)","e3df28a8":"# # from vision_transformer_pytorch import VisionTransformer\n# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# print(device)\n\n# datasets = {'train': train_dataset,\n#             'valid': valid_dataset}\n\n# dataloaders = {'train': train_loader,\n#                'valid': valid_loader}\n\n# # LOAD PRETRAINED ViT MODEL\n# # model = VisionTransformer.from_pretrained('ViT-B_16', num_classes=12)   \n\n# # OPTIMIZER\n# optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=0.001)\n# # optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay=0.001)\n# # optimizer = AdamP(model.parameters(), lr=1e-4, weight_decay=0.001)\n\n# # LEARNING RATE SCHEDULER\n# scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.1)\n\n# criterion = nn.CrossEntropyLoss()\n# num_epochs = 8","93cb84f8":"# torch.save(model, 'full_plant_pathology_vit.pt')","f27def3a":"# len(dataloaders['train'])","69ea40f0":"# # MODEL TRAIN\n# trained_model = train_model(datasets, dataloaders,\n#                             model, criterion,\n#                             optimizer, scheduler,\n#                             num_epochs, device)","615b8681":"# Save the mode after training\n# torch.save(trained_model, f't2t-vit-14_{num_epochs}_epoch.pth')","da3269d3":"# from vision_transformer_pytorch import VisionTransformer\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(device)\n\n# LOAD PRETRAINED ViT MODEL\nmodel_path = '..\/input\/t2t-vit-14\/t2t-vit-14_3_epoch.pt'\nmodel = torch.load(model_path)\n# model.load_state_dict(torch.load(model_path))\nmodel.eval()","f0d9ef7c":"from pathlib import Path\n\ndf_preds = pd.DataFrame()\nfor path in Path('..\/input\/plant-pathology-2021-fgvc8\/test_images').iterdir():\n    img = cv2.imread(str(path))[:, ::-1]\n    img = valid_augs(image=img)['image'].cuda()\n    pred = model(img[None])\n    \n    df_preds = df_preds.append(\n        {'image': path.parts[-1], 'labels': le.inverse_transform(torch.argmax(pred.cpu(), dim=1))[0]},\n        ignore_index=True)\n    \ndf_preds","c8a35d05":"df_preds.to_csv('submission.csv', index=False)","1bd44da0":"\u0421\u0441\u044b\u043b\u043a\u0438:\n1. [Vision Transformers: A New Computer Vision Paradigm](https:\/\/medium.com\/swlh\/visual-transformers-a-new-computer-vision-paradigm-aa78c2a2ccf2)\n2. [Visual Transformers: Token-based Image Representation and Processing for Computer Vision](https:\/\/arxiv.org\/pdf\/2006.03677.pdf)\n3. [Vision Transformer (ViT) - An image is worth 16x16 words | Paper Explained](https:\/\/www.youtube.com\/watch?v=j6kuz_NqkG0)\n4. [\u041f\u0440\u0438\u043a\u043b\u0430\u0434\u043d\u043e\u0435 \u043c\u0430\u0448\u0438\u043d\u043d\u043e\u0435 \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u0435 4. Self-Attention. Transformer overview](https:\/\/youtu.be\/UETKUIlYE6g)","e83a3dab":"1. \u0418\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u0435 \u043d\u0443\u0436\u043d\u043e \u043f\u043e\u0434\u0435\u043b\u0438\u0442\u044c \u043d\u0430 \u043f\u0430\u0447\u0442\u0438 \u0444\u0438\u043a\u0441\u0438\u0440\u043e\u0432\u0430\u043d\u043d\u043e\u0433\u043e \u0440\u0430\u0437\u043c\u0435\u0440\u0430\n\n\u0415\u0441\u0442\u044c 2D \u043a\u0430\u0440\u0442\u0438\u043d\u043a\u0430 \u0440\u0430\u0437\u043c\u0435\u0440\u0430  H * W, \u043a\u043e\u0442\u043e\u0440\u0430\u044f \u043c\u043e\u0436\u0435\u0442 \u0440\u0430\u0437\u0434\u0435\u043b\u0438\u0442\u044c\u0441\u044f \u043d\u0430 N \u043f\u0430\u0442\u0447\u0435\u0439, \u0433\u0434\u0435 $N=\\frac{H * W}{P^2}$. \u0415\u0441\u043b\u0438 \u043a\u0430\u0440\u0442\u0438\u043d\u043a\u0430 48x48, \u0440\u0430\u0437\u043c\u0435\u0440 \u043f\u0430\u0442\u0447\u0430 16x16, \u0442\u043e \u043f\u043e\u043b\u0443\u0447\u0438\u0442\u0441\u044f $N=\\frac{48 * 48}{16^2} = 9$.","505d62db":"### Self-Attention\n\u0411\u0443\u0434\u0435\u043c \u0440\u0430\u0431\u043e\u0442\u0430\u0442\u044c \u0441\u043e \u0441\u043b\u0435\u0434\u0443\u044e\u0449\u0438\u043c \u043f\u0440\u0435\u0434\u043b\u043e\u0436\u0435\u043d\u0438\u0435\u043c:\n\n\u201dThe animal didn't cross the street because it was too tired\u201d\n\n\u0427\u0442\u043e \u043e\u0437\u043d\u0430\u0447\u0430\u0435\u0442 it \u0432 \u044d\u0442\u043e\u043c \u043f\u0440\u0435\u0434\u043b\u043e\u0436\u0435\u043d\u0438\u0438? It \u043e\u0442\u043d\u043e\u0441\u0438\u0442\u0441\u044f \u043a \u0443\u043b\u0438\u0446\u0435 \u0438\u043b\u0438 \u043a \u0436\u0438\u0432\u043e\u0442\u043d\u043e\u043c\u0443? \u041a\u043e\u0433\u0434\u0430 \u043c\u043e\u0434\u0435\u043b\u044c \u043e\u0431\u0440\u0430\u0431\u0430\u0442\u044b\u0432\u0430\u0435\u0442 \u0441\u043b\u043e\u0432\u043e it, self-attention \u043f\u043e\u0437\u0432\u043e\u043b\u044f\u0435\u0442 \u0435\u0439 \u0430\u0441\u0441\u043e\u0446\u0438\u0438\u0440\u043e\u0432\u0430\u0442\u044c it \u0441 animal, \u043d\u043e it \u0435\u0449\u0435 \u0438 \u0441\u0432\u044f\u0437\u0430\u043d\u043e \u0441 tired.\n\nSelf-attention \u043f\u043e\u0437\u0432\u043e\u043b\u044f\u0435\u0442 \u043d\u0435\u0439\u0440\u043e\u0441\u0435\u0442\u043a\u0435 \u0441\u043c\u043e\u0442\u0440\u0435\u0442\u044c \u043d\u0430 \u0434\u0440\u0443\u0433\u0438\u0435 \u0441\u043b\u043e\u0432\u0430 \u0432\u043e \u0432\u0445\u043e\u0434\u043d\u043e\u0439 \u043f\u043e\u0441\u043b\u0435\u0434\u043e\u0432\u0430\u0442\u0435\u043b\u044c\u043d\u043e\u0441\u0442\u0438 \u0432 \u043f\u043e\u0438\u0441\u043a\u0430\u0445 \u043f\u043e\u0434\u0441\u043a\u0430\u0437\u043e\u043a, \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u043c\u043e\u0433\u0443\u0442 \u043f\u043e\u043c\u043e\u0447\u044c \u043f\u0440\u0438\u0432\u0435\u0441\u0442\u0438 \u043a \u043b\u0443\u0447\u0448\u0435\u043c\u0443 \u043a\u043e\u0434\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u044e \u044d\u0442\u043e\u0433\u043e \u0441\u043b\u043e\u0432\u0430.\n\n\n<img src='http:\/\/jalammar.github.io\/images\/t\/transformer_self-attention_visualization.png' width=400>","357f12c2":"Self-Attention \u043c\u0435\u0436\u0434\u0443 \u043f\u0438\u043a\u0441\u0435\u043b\u044f\u043c\u0438. \u041a \u043f\u0440\u0438\u043c\u0435\u0440\u0443, \u0435\u0441\u043b\u0438 \u043a\u0430\u0440\u0442\u0438\u043d\u043a\u0430 \u0440\u0430\u0437\u043c\u0435\u0440\u0430 640x640, \u043c\u043e\u0434\u0435\u043b\u044c\u043a\u0435 \u043d\u0430\u0434\u043e \u043f\u043e\u0441\u0447\u0438\u0442\u0430\u0442\u044c self-attention \u0434\u043b\u044f 409\u043a \u043a\u043e\u043c\u0431\u0438\u043d\u0430\u0446\u0438\u0439. \u041d\u043e \u0432\u0435\u0440\u043e\u044f\u0442\u043d\u0435\u0439 \u0432\u0441\u0435\u0433\u043e, \u0441\u0430\u043c\u044b\u0439 \u0432\u0435\u0440\u0445\u043d\u0438\u0439 \u043f\u0440\u0430\u0432\u044b\u0439 \u043f\u0438\u043a\u0441\u0435\u043b\u044c \u0432\u0440\u044f\u0434 \u043b\u0438 \u0431\u0443\u0434\u0435\u0442 \u0438\u043c\u0435\u0442\u044c \u0437\u043d\u0430\u0447\u0438\u043c\u043e\u0435 \u0432\u043b\u0438\u044f\u043d\u0438\u0435 \u043d\u0430 \u043d\u0438\u0436\u043d\u0438\u0439 \u043b\u0435\u0432\u044b\u0439 \u043f\u0438\u043a\u0441\u0435\u043b\u044c. ViT \u0441\u043f\u0440\u0430\u0432\u0438\u043b\u0441\u044f \u0441 \u044d\u0442\u043e\u0439 \u043f\u0440\u043e\u0431\u043b\u0435\u043c\u043e\u0439 \u0437\u0430 \u0441\u0447\u0435\u0442 \u0441\u0435\u0433\u043c\u0435\u043d\u0442\u0430\u0446\u0438\u0438 \u043a\u0430\u0440\u0442\u0438\u043d\u043a\u0438 \u043d\u0430 \u043c\u0430\u043b\u0435\u043d\u044c\u043a\u0438\u0435 \u043f\u0430\u0442\u0447\u0438 (\u043a \u043f\u0440\u0438\u043c\u0435\u0440\u0443, 16x16).\n\n![image.png](attachment:cded490a-0e7b-4b0e-99c5-cd91584bb995.png)","3932043a":"## Testing","b40ec98f":"### Multi-Head Self-Attention\n\n\u0423\u043b\u0443\u0447\u0448\u0430\u0435\u0442 \u0440\u0430\u0431\u043e\u0442\u0443 \u0441\u043b\u043e\u044f \u0432\u043d\u0438\u043c\u0430\u043d\u0438\u044f \u0434\u0432\u0443\u043c\u044f \u0441\u043f\u043e\u0441\u043e\u0431\u0430\u043c\u0438:\n\n1. \u0420\u0430\u0441\u0448\u0438\u0440\u044f\u0435\u0442 \u0432\u043e\u0437\u043c\u043e\u0436\u043d\u043e\u0441\u0442\u0438 \u043c\u043e\u0434\u0435\u043b\u0438 \u0444\u043e\u043a\u0443\u0441\u0438\u0440\u043e\u0432\u0430\u0442\u044c\u0441\u044f \u043d\u0430 \u0440\u0430\u0437\u043b\u0438\u0447\u043d\u044b\u0445 \u0441\u043b\u043e\u0432\u0430\u0445.\n\n2. \u0414\u0430\u0435\u0442 \u0441\u043b\u043e\u044e \u0432\u043d\u0438\u043c\u0430\u043d\u0438\u044f \u043d\u0435\u0441\u043a\u043e\u043b\u044c\u043a\u043e \u201c\u043f\u043e\u0434\u043f\u0440\u043e\u0441\u0442\u0440\u0430\u043d\u0441\u0442\u0432 \u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u0435\u043d\u0438\u044f\u201d. \u0421 Multi-Head self-attention \u0435\u0441\u0442\u044c \u043d\u0435\u0441\u043a\u043e\u043b\u044c\u043a\u043e \u043d\u0430\u0431\u043e\u0440\u043e\u0432 \u0432\u0435\u0441\u043e\u0432\u044b\u0445 \u043c\u0430\u0442\u0440\u0438\u0446 query\/key\/value. \u041a\u0430\u0436\u0434\u044b\u0439 \u0438\u0437 \u044d\u0442\u0438\u0445 \u043d\u0430\u0431\u043e\u0440\u043e\u0432 \u0438\u043d\u0438\u0446\u0438\u0430\u043b\u0438\u0437\u0438\u0440\u0443\u0435\u0442\u0441\u044f \u0441\u043b\u0443\u0447\u0430\u0439\u043d\u044b\u043c \u043e\u0431\u0440\u0430\u0437\u043e\u043c. \u0417\u0430\u0442\u0435\u043c, \u043f\u043e\u0441\u043b\u0435 \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f, \u043a\u0430\u0436\u0434\u044b\u0439 \u043d\u0430\u0431\u043e\u0440 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u0442\u0441\u044f \u0434\u043b\u044f \u043f\u0440\u043e\u0435\u0446\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u044f \u0432\u0445\u043e\u0434\u043d\u044b\u0445 \u0432\u043b\u043e\u0436\u0435\u043d\u0438\u0439 \u0432 \u0434\u0440\u0443\u0433\u043e\u0435 \u043f\u043e\u0434\u043f\u0440\u043e\u0441\u0442\u0440\u0430\u043d\u0441\u0442\u0432\u043e \u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u0435\u043d\u0438\u044f.\n\n<img src='http:\/\/jalammar.github.io\/images\/t\/transformer_attention_heads_qkv.png' width=600>\n\n<img src='http:\/\/jalammar.github.io\/images\/t\/transformer_attention_heads_z.png' width=600>\n\n<img src='http:\/\/jalammar.github.io\/images\/t\/transformer_attention_heads_weight_matrix_o.png' width=600>\n\n<img src='http:\/\/jalammar.github.io\/images\/t\/transformer_self-attention_visualization_2.png' width=400>\n\n\n\n\n","934c521a":"This is notebook is based on [CassavaLeaf ViT baseline Notebook](https:\/\/www.kaggle.com\/szuzhangzhi\/vision-transformer-vit-cuda-as-usual) and Adjusted for Plant pathology 2021 dataset.","3934e95f":"\n**\u041f\u0435\u0440\u0432\u044b\u0439 \u044d\u0442\u0430\u043f \u0432 \u0432\u044b\u0447\u0438\u0441\u043b\u0435\u043d\u0438\u0438 self-attention** - \u0441\u043e\u0437\u0434\u0430\u043d\u0438\u0435 \u0442\u0440\u0435\u0445 \u0432\u0435\u043a\u0442\u043e\u0440\u043e\u0432 \u0438\u0437 \u043a\u0430\u0436\u0434\u043e\u0433\u043e \u0432\u0445\u043e\u0434\u043d\u043e\u0433\u043e \u044d\u043c\u0431\u0435\u0434\u0434\u0438\u043d\u0433\u0430. \u0414\u043b\u044f \u043a\u0430\u0436\u0434\u043e\u0433\u043e \u0441\u043b\u043e\u0432\u0430 \u0441\u043e\u0437\u0434\u0430\u0435\u0442\u0441\u044f \u0432\u0435\u043a\u0442\u043e\u0440 Query, Key \u0438 Value. \u042d\u0442\u0438 \u0432\u0435\u043a\u0442\u043e\u0440\u044b \u0441\u043e\u0437\u0434\u0430\u044e\u0442\u0441\u044f \u043f\u0443\u0442\u0435\u043c \u0443\u043c\u043d\u043e\u0436\u0435\u043d\u0438\u044f \u044d\u043c\u0431\u0435\u0434\u0434\u0438\u043d\u0433\u0430 \u043d\u0430 \u0442\u0440\u0438 \u043c\u0430\u0442\u0440\u0438\u0446\u044b, \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u043e\u0431\u0443\u0447\u0430\u044e\u0442\u0441\u044f.\n- Query - \u0441\u043b\u043e\u0432\u043e, \u0441 \u043a\u043e\u0442\u043e\u0440\u043e\u0433\u043e \u0441\u043c\u043e\u0442\u0440\u0438\u043c \u043d\u0430 \u0432\u0441\u0451 \u043e\u0441\u0442\u0430\u043b\u044c\u043d\u043e\u0435\n- Key - \u0441\u043b\u043e\u0432\u043e, \u043d\u0430 \u043a\u043e\u0442\u043e\u0440\u043e\u0435 \u0441\u043c\u043e\u0442\u0440\u0438\u043c\n- Value - \u0437\u0434\u0435\u0441\u044c \u0441\u043e\u0434\u0435\u0440\u0436\u0438\u0442\u0441\u044f \u0441\u043c\u044b\u0441\u043b \u0441\u043b\u043e\u0432\u0430\n\n<img src='http:\/\/jalammar.github.io\/images\/t\/transformer_self_attention_vectors.png' width=500>\n\n**\u0412\u0442\u043e\u0440\u043e\u0439 \u044d\u0442\u0430\u043f \u0432\u044b\u0447\u0438\u0441\u043b\u0435\u043d\u0438\u044f self-attention** \u2013 \u043f\u043e\u043b\u0443\u0447\u0435\u043d\u0438\u0435 \u043a\u043e\u044d\u0444\u0444\u0438\u0446\u0438\u0435\u043d\u0442\u0430 (score). \u041a\u043e\u044d\u0444\u0444\u0438\u0446\u0438\u0435\u043d\u0442 \u043f\u043e\u0434\u0441\u0447\u0438\u0442\u044b\u0432\u0430\u0435\u0442\u0441\u044f \u0441 \u043f\u043e\u043c\u043e\u0449\u044c\u044e \u0441\u043a\u0430\u043b\u044f\u0440\u043d\u043e\u0433\u043e \u043f\u0440\u043e\u0438\u0437\u0432\u0435\u0434\u0435\u043d\u0438\u044f \u0432\u0435\u043a\u0442\u043e\u0440\u0430 Query \u0438 \u0432\u0435\u043a\u0442\u043e\u0440\u0430 Key \u0441\u043e\u043e\u0442\u0432\u0435\u0442\u0441\u0442\u0432\u0443\u044e\u0449\u0435\u0433\u043e \u0441\u043b\u043e\u0432\u0430. \u041f\u0440\u0438 \u043f\u0435\u0440\u0435\u043c\u043d\u043e\u0436\u0435\u043d\u0438\u0438 Query \u043d\u0430 Key \u043c\u044b \u043f\u043e\u043d\u0438\u043c\u0430\u0435\u043c \u043d\u0430\u0441\u043a\u043e\u043b\u044c\u043a\u043e \u0441\u043b\u043e\u0432\u043e \u0441 Key \u0440\u0435\u043b\u0435\u0432\u0430\u043d\u0442\u043d\u043e \u0441\u043b\u043e\u0432\u0443 \u0441 Query.\n\n<img src='http:\/\/jalammar.github.io\/images\/t\/transformer_self_attention_score.png' width=500>\n\n**\u0422\u0440\u0435\u0442\u0438\u0439 \u0438 \u0447\u0435\u0442\u0432\u0435\u0440\u0442\u044b\u0439 \u044d\u0442\u0430\u043f\u044b** \u2013 \u0440\u0430\u0437\u0434\u0435\u043b\u0438\u0442\u044c \u043a\u043e\u044d\u0444\u0444\u0438\u0446\u0438\u0435\u043d\u0442\u044b \u043d\u0430 $\\sqrt{d}$ (\u043a\u0432\u0430\u0434\u0440\u0430\u0442\u043d\u044b\u0439 \u043a\u043e\u0440\u0435\u043d\u044c \u0438\u0437 \u0440\u0430\u0437\u043c\u0435\u0440\u043d\u043e\u0441\u0442\u0438 Key \u0432\u0435\u043a\u0442\u043e\u0440\u0430; \u0434\u043b\u044f \u0442\u043e\u0433\u043e, \u0447\u0442\u043e\u0431\u044b \u0431\u044b\u043b\u0438 \u0431\u043e\u043b\u0435\u0435 \u0441\u0442\u0430\u0431\u0438\u043b\u044c\u043d\u044b\u0435 \u0433\u0440\u0430\u0434\u0438\u0435\u043d\u0442\u044b), \u0430 \u0437\u0430\u0442\u0435\u043c \u043f\u0440\u043e\u043f\u0443\u0441\u0442\u0438\u0442\u044c \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442 \u0447\u0435\u0440\u0435\u0437 \u0444\u0443\u043d\u043a\u0446\u0438\u044e \u0441\u043e\u0444\u0442\u043c\u0430\u043a\u0441 (softmax). \u0414\u0430\u043d\u043d\u0430\u044f \u0444\u0443\u043d\u043a\u0446\u0438\u044f \u043d\u043e\u0440\u043c\u0430\u043b\u0438\u0437\u0443\u0435\u0442 \u043a\u043e\u044d\u0444\u0444\u0438\u0446\u0438\u0435\u043d\u0442\u044b \u0442\u0430\u043a, \u0447\u0442\u043e\u0431\u044b \u043e\u043d\u0438 \u0431\u044b\u043b\u0438 \u043f\u043e\u043b\u043e\u0436\u0438\u0442\u0435\u043b\u044c\u043d\u044b\u043c\u0438 \u0438 \u0432 \u0441\u0443\u043c\u043c\u0435 \u0434\u0430\u0432\u0430\u043b\u0438 1.\n\n<img src='http:\/\/jalammar.github.io\/images\/t\/self-attention_softmax.png' width=500>\n\n**\u041f\u044f\u0442\u044b\u0439 \u044d\u0442\u0430\u043f** \u2013 \u0443\u043c\u043d\u043e\u0436\u0438\u0442\u044c \u043a\u0430\u0436\u0434\u044b\u0439 \u0432\u0435\u043a\u0442\u043e\u0440 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f (Value) \u043d\u0430 \u0441\u043e\u0444\u0442\u043c\u0430\u043a\u0441-\u043a\u043e\u044d\u0444\u0444\u0438\u0446\u0438\u0435\u043d\u0442.\n\n**\u0428\u0435\u0441\u0442\u043e\u0439 \u044d\u0442\u0430\u043f** \u2013 \u0441\u043b\u043e\u0436\u0438\u0442\u044c \u0432\u0437\u0432\u0435\u0448\u0435\u043d\u043d\u044b\u0435 \u0432\u0435\u043a\u0442\u043e\u0440\u044b \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f. \u042d\u0442\u043e \u0438 \u0431\u0443\u0434\u0435\u0442 \u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u044f\u0442\u044c \u0441\u043e\u0431\u043e\u0439 \u0432\u044b\u0445\u043e\u0434 \u0441\u043b\u043e\u044f \u0432\u043d\u0443\u0442\u0440\u0435\u043d\u043d\u0435\u0433\u043e \u0432\u043d\u0438\u043c\u0430\u043d\u0438\u044f \u0432 \u0434\u0430\u043d\u043d\u043e\u0439 \u043f\u043e\u0437\u0438\u0446\u0438\u0438 (\u0434\u043b\u044f \u043f\u0435\u0440\u0432\u043e\u0433\u043e \u0441\u043b\u043e\u0432\u0430).\n\n<img src='http:\/\/jalammar.github.io\/images\/t\/self-attention-output.png' width=500>\n\n\n**\u041c\u0430\u0442\u0440\u0438\u0447\u043d\u0430\u044f \u043a\u0440\u0430\u0442\u043a\u0430\u044f \u0437\u0430\u043f\u0438\u0441\u044c**\n\n<img src='http:\/\/jalammar.github.io\/images\/t\/self-attention-matrix-calculation-2.png' width=400>","1a9c3330":"\n3. \u041f\u043e\u0437\u0438\u0446\u0438\u043e\u043d\u043d\u044b\u0439 \u044d\u043c\u0431\u0435\u0434\u0434\u0438\u043d\u0433 \u0434\u043e\u0431\u0430\u0432\u043b\u044f\u0435\u0442\u0441\u044f \u043a \u043f\u0430\u0442\u0447\u0443, \u0447\u0442\u043e\u0431\u044b \u0441\u043e\u0445\u0440\u0430\u043d\u0438\u0442\u044c \u043f\u043e\u0437\u0438\u0446\u0438\u043e\u043d\u043d\u0443\u044e \u0438\u043d\u0444\u043e\u0440\u043c\u0430\u0446\u0438\u044e\n\n\u042d\u0442\u043e \u043e\u0431\u0443\u0447\u0430\u0435\u043c\u0430\u044f \u0442\u0430\u0431\u043b\u0438\u0446\u0430 \u0441 \u043f\u043e\u0437\u0438\u0446\u0438\u043e\u043d\u043d\u044b\u043c\u0438 \u0432\u0435\u043a\u0442\u043e\u0440\u0430\u043c\u0438.\n\n\n\u0422\u0440\u0430\u043d\u0441\u0444\u043e\u0440\u043c\u0435\u0440\u044b \u043d\u0435 \u0437\u0430\u0432\u0438\u0441\u044f\u0442 \u043e\u0442 \u0441\u0442\u0440\u0443\u043a\u0442\u0443\u0440\u044b \u0432\u0445\u043e\u0434\u043d\u044b\u0445 \u044d\u043b\u0435\u043c\u0435\u043d\u0442\u043e\u0432, \u043f\u043e\u044d\u0442\u043e\u043c\u0443 \u0434\u043e\u0431\u0430\u0432\u043b\u0435\u043d\u0438\u0435 \u043e\u0431\u0443\u0447\u0430\u0435\u043c\u044b\u0445 \u043f\u043e\u0437\u0438\u0446\u0438\u043e\u043d\u043d\u044b\u0445 \u044d\u043c\u0431\u044d\u0434\u0434\u0438\u043d\u0433\u043e\u0432 \u0432 \u043a\u0430\u0436\u0434\u044b\u0439 \u043f\u0430\u0442\u0447 \u043f\u043e\u0437\u0432\u043e\u043b\u0438\u0442 \u043c\u043e\u0434\u0435\u043b\u0438 \u0443\u0437\u043d\u0430\u0442\u044c \u043e \u0441\u0442\u0440\u0443\u043a\u0442\u0443\u0440\u0435 \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u044f.\n\n\n\n\u042d\u0442\u0430 \u043f\u043e\u0441\u043b\u0435\u0434\u043e\u0432\u0430\u0442\u0435\u043b\u044c\u043d\u043e\u0441\u0442\u044c \u0432\u0435\u043a\u0442\u043e\u0440\u043e\u0432 \u043f\u0430\u0442\u0447\u0435\u0439 \u0431\u0443\u0434\u0435\u0442 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c\u0441\u044f \u0432 \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u0435 \u0432\u0445\u043e\u0434\u043d\u043e\u0439 \u043f\u043e\u0441\u043b\u0435\u0434\u043e\u0432\u0430\u0442\u0435\u043b\u044c\u043d\u043e\u0441\u0442\u0438 \u0434\u043b\u044f \u044d\u043d\u043a\u043e\u0434\u0435\u0440\u0430 \u0442\u0440\u0430\u043d\u0441\u0444\u043e\u0440\u043c\u0435\u0440\u0430.","219ffe67":"\u041f\u043e\u043b\u0435\u0437\u043d\u044b\u0435 \u0441\u0441\u044b\u043b\u043a\u0438:\n1. [Swin-Transformer](https:\/\/github.com\/microsoft\/Swin-Transformer)\n2. [VisionTransformer-Pytorch](https:\/\/github.com\/tczhangzhi\/VisionTransformer-PyTorch)\n3. [vit-pytorch](https:\/\/github.com\/lucidrains\/vit-pytorch)","6153ea5b":"### Transformer Encoder\n\n![image.png](attachment:6a08e9ff-e266-493e-880d-5ab34014dc73.png)\n\n\u042d\u043d\u043a\u043e\u0434\u0435\u0440 \u0441\u043e\u0441\u0442\u043e\u0438\u0442 \u0438\u0437\n- Multi-Head Self Attention Layer(MSP) \u0434\u043b\u044f \u043b\u0438\u043d\u0435\u0439\u043d\u043e\u0433\u043e \u043e\u0431\u044a\u0435\u0434\u0438\u043d\u0435\u043d\u0438\u044f \u043d\u0435\u0441\u043a\u043e\u043b\u044c\u043a\u0438\u0445 \u0432\u044b\u0445\u043e\u0434\u043e\u0432 \u0432\u043d\u0438\u043c\u0430\u043d\u0438\u044f \u0432 \u0441\u043e\u043e\u0442\u0432\u0435\u0442\u0441\u0442\u0432\u0438\u0438 \u0441 \u043e\u0436\u0438\u0434\u0430\u0435\u043c\u044b\u043c\u0438 \u0440\u0430\u0437\u043c\u0435\u0440\u0430\u043c\u0438. \u041d\u0435\u0441\u043a\u043e\u043b\u044c\u043a\u043e Multi-Head \u043f\u043e\u043c\u043e\u0433\u0430\u044e\u0442 \u0438\u0437\u0443\u0447\u0430\u0442\u044c \u043b\u043e\u043a\u0430\u043b\u044c\u043d\u044b\u0435 \u0438 \u0433\u043b\u043e\u0431\u0430\u043b\u044c\u043d\u044b\u0435 \u0437\u0430\u0432\u0438\u0441\u0438\u043c\u043e\u0441\u0442\u0438 \u0432 \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u0438.\n- \u041c\u043d\u043e\u0433\u043e\u0441\u043b\u043e\u0439\u043d\u044b\u0435 \u043f\u0435\u0440\u0441\u0435\u043f\u0442\u0440\u043e\u043d\u044b (MLP) \u0441\u043e\u0434\u0435\u0440\u0436\u0430\u0442 \u0434\u0432\u0430 \u0441\u043b\u043e\u044f \u0441 GELU (Gaussian Error Linear Unit)\n- Layer Norm (LN) \u043f\u0440\u0438\u043c\u0435\u043d\u044f\u0435\u0442\u0441\u044f \u043f\u0435\u0440\u0435\u0434 \u043a\u0430\u0436\u0434\u044b\u043c \u0431\u043b\u043e\u043a\u043e\u043c, \u0442\u0430\u043a \u043a\u0430\u043a \u043e\u043d\u0430 \u043d\u0435 \u0432\u0432\u043e\u0434\u0438\u0442 \u043d\u0438\u043a\u0430\u043a\u0438\u0445 \u043d\u043e\u0432\u044b\u0445 \u0437\u0430\u0432\u0438\u0441\u0438\u043c\u043e\u0441\u0442\u0435\u0439 \u043c\u0435\u0436\u0434\u0443 \u043e\u0431\u0443\u0447\u0430\u044e\u0449\u0438\u043c\u0438 \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u044f\u043c\u0438. \u041f\u043e\u043c\u043e\u0433\u0430\u0435\u0442 \u0443\u043b\u0443\u0447\u0448\u0438\u0442\u044c \u0432\u0440\u0435\u043c\u044f \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f \u0438 \u043f\u0440\u043e\u0438\u0437\u0432\u043e\u0434\u0438\u0442\u0435\u043b\u044c\u043d\u043e\u0441\u0442\u044c \u043e\u0431\u043e\u0431\u0449\u0435\u043d\u0438\u044f\n- Residual connections \u043f\u0440\u0438\u043c\u0435\u043d\u044f\u044e\u0442\u0441\u044f \u043f\u043e\u0441\u043b\u0435 \u043a\u0430\u0436\u0434\u043e\u0433\u043e \u0431\u043b\u043e\u043a\u0430\n\n\u0414\u043b\u044f \u043a\u043b\u0430\u0441\u0441\u0438\u0444\u0438\u043a\u0430\u0446\u0438\u0438 \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u0439 \u043a\u043b\u0430\u0441\u0441\u0438\u0444\u0438\u043a\u0430\u0446\u0438\u044f \u0440\u0435\u0430\u043b\u0438\u0437\u043e\u0432\u0430\u043d\u0430 \u0441 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043d\u0438\u0435\u043c MLP \u0441 \u043e\u0434\u043d\u0438\u043c \u0441\u043a\u0440\u044b\u0442\u044b\u043c \u0441\u043b\u043e\u0435\u043c.\n\u0412\u0435\u0440\u0445\u043d\u0438\u0435 \u0441\u043b\u043e\u0438 ViT \u0438\u0437\u0443\u0447\u0430\u044e\u0442 \u0433\u043b\u043e\u0431\u0430\u043b\u044c\u043d\u044b\u0435 \u043e\u0431\u044a\u0435\u043a\u0442\u044b, \u0442\u043e\u0433\u0434\u0430 \u043a\u0430\u043a \u043d\u0438\u0436\u043d\u0438\u0435 \u0441\u043b\u043e\u0438 \u0438\u0437\u0443\u0447\u0430\u044e\u0442 \u043a\u0430\u043a \u0433\u043b\u043e\u0431\u0430\u043b\u044c\u043d\u044b\u0435, \u0442\u0430\u043a \u0438 \u043b\u043e\u043a\u0430\u043b\u044c\u043d\u044b\u0435 \u043e\u0431\u044a\u0435\u043a\u0442\u044b. \u042d\u0442\u043e \u043f\u043e\u0437\u0432\u043e\u043b\u044f\u0435\u0442 ViT \u0438\u0437\u0443\u0447\u0430\u0442\u044c \u0431\u043e\u043b\u0435\u0435 \u043e\u0431\u0449\u0438\u0435 \u043f\u0430\u0442\u0442\u0435\u0440\u043d\u044b.\n","7137ef47":"\u041e\u0431\u044b\u0447\u043d\u043e \u043a\u0430\u0440\u0442\u0438\u043d\u043a\u0443 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u044e\u0442 \u043a\u0430\u043a 3D array (\u0432\u044b\u0441\u043e\u0442\u0430, \u0448\u0438\u0440\u0438\u043d\u0430, \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u043a\u0430\u043d\u0430\u043b\u043e\u0432) \u0438 \u043f\u0440\u0438\u043c\u0435\u043d\u044f\u0435\u0442\u0441\u044f \u043a \u043d\u0438\u043c \u0441\u0432\u0435\u0440\u0442\u043e\u0447\u043d\u044b\u0435 \u0441\u043b\u043e\u0438. \u041d\u043e \u0442\u0443\u0442 \u0435\u0441\u0442\u044c \u0440\u044f\u0434 \u043d\u0435\u0434\u043e\u0441\u0442\u0430\u0442\u043a\u043e\u0432:\n\n- \u043d\u0435 \u0432\u0441\u0435 \u043f\u0438\u043a\u0441\u0435\u043b\u0438 \u043e\u0434\u0438\u043d\u0430\u043a\u043e\u0432\u043e \u043f\u043e\u043b\u0435\u0437\u043d\u044b;\n- \u0441\u0432\u0435\u0440\u0442\u043a\u0438 \u043d\u0435 \u0434\u043e\u0441\u0442\u0430\u0442\u043e\u0447\u043d\u043e \u0445\u043e\u0440\u043e\u0448\u043e \u0440\u0430\u0431\u043e\u0442\u0430\u044e\u0442 \u0441 \u043f\u0438\u043a\u0441\u0435\u043b\u044f\u043c\u0438, \u043d\u0430\u0445\u043e\u0434\u044f\u0449\u0438\u043c\u0438\u0441\u044f \u0434\u0430\u043b\u0435\u043a\u043e \u0434\u0440\u0443\u0433 \u043e\u0442 \u0434\u0440\u0443\u0433\u0430;\n- \u0441\u0432\u0435\u0440\u0442\u043a\u0438 \u043d\u0435\u0434\u043e\u0441\u0442\u0430\u0442\u043e\u0447\u043d\u043e \u044d\u0444\u0444\u0435\u043a\u0442\u0438\u0432\u043d\u044b \u0432 \u043e\u0447\u0435\u043d\u044c \u0433\u043b\u0443\u0431\u043e\u043a\u0438\u0445 \u043d\u0435\u0439\u0440\u043e\u043d\u043d\u044b\u0445 \u0441\u0435\u0442\u044f\u0445.\n\n\u0412 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u0435 \u0430\u0432\u0442\u043e\u0440\u044b \u043f\u0440\u0435\u0434\u043b\u0430\u0433\u0430\u044e\u0442 \u043a\u043e\u043d\u0432\u0435\u0440\u0442\u0438\u0440\u043e\u0432\u0430\u0442\u044c \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u044f \u0432 \u0432\u0438\u0437\u0443\u0430\u043b\u044c\u043d\u044b\u0435 \u0442\u043e\u043a\u0435\u043d\u044b \u0438 \u043f\u043e\u0434\u0430\u0432\u0430\u0442\u044c \u0438\u0445 \u0432 \u0442\u0440\u0430\u043d\u0441\u0444\u043e\u0440\u043c\u0435\u0440.\n\n- \u0412\u043d\u0430\u0447\u0430\u043b\u0435 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u0442\u0441\u044f \u043e\u0431\u044b\u0447\u043d\u044b\u0439 backbone \u0434\u043b\u044f \u043f\u043e\u043b\u0443\u0447\u0435\u043d\u0438\u044f feature maps\n- \u0414\u0430\u043b\u0435\u0435 feature map \u043a\u043e\u043d\u0432\u0435\u0440\u0442\u0438\u0440\u0443\u0435\u0442\u0441\u044f \u0432 \u0432\u0438\u0437\u0443\u0430\u043b\u044c\u043d\u044b\u0435 \u0442\u043e\u043a\u0435\u043d\u044b\n- \u0422\u043e\u043a\u0435\u043d\u044b \u043f\u043e\u0434\u0430\u044e\u0442\u0441\u044f \u0432 \u0442\u0440\u0430\u043d\u0441\u0444\u043e\u0440\u043c\u0435\u0440\u044b\n- \u0412\u044b\u0445\u043e\u0434 \u0442\u0440\u0430\u043d\u0441\u0444\u043e\u0440\u043c\u0435\u0440\u0430 \u043c\u043e\u0436\u0435\u0442 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c\u0441\u044f \u0434\u043b\u044f \u0437\u0430\u0434\u0430\u0447 \u043a\u043b\u0430\u0441\u0441\u0438\u0444\u0438\u043a\u0430\u0446\u0438\u0438\n- \u0410 \u0435\u0441\u043b\u0438 \u043e\u0431\u044a\u0435\u0434\u0438\u043d\u0438\u0442\u044c \u0432\u044b\u0445\u043e\u0434 \u0442\u0440\u0430\u043d\u0441\u0444\u043e\u0440\u043c\u0435\u0440\u0430 \u0441 feature map, \u0442\u043e \u043c\u043e\u0436\u043d\u043e \u043f\u043e\u043b\u0443\u0447\u0438\u0442\u044c \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u044f \u0434\u043b\u044f \u0437\u0430\u0434\u0430\u0447 \u0441\u0435\u0433\u043c\u0435\u043d\u0442\u0430\u0446\u0438\u0438","cd35e94b":"### Training","bce914b4":"\n2. \u0412\u044b\u0442\u044f\u043d\u0443\u0442\u044c 2D \u043f\u0430\u0442\u0447\u0438 \u0432 1D \u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u0435\u043d\u0438\u0435\n\n\u041a\u0430\u0436\u0434\u044b\u0439 \u043f\u0430\u0442\u0447 \u0432\u044b\u0442\u044f\u0433\u0438\u0432\u0430\u0435\u0442\u0441\u044f \u0432 1D \u043f\u0430\u0442\u0447 \u043f\u0443\u0442\u0435\u043c \u043a\u043e\u043d\u043a\u0430\u0442\u0435\u043d\u0430\u0446\u0438\u0438 \u0432\u0441\u0435\u0445 \u043f\u0438\u043a\u0441\u0435\u043b\u0435\u0439 \u0438 \u0437\u0430\u0442\u0435\u043c \u043f\u0440\u0438\u043c\u0435\u043d\u044f\u0435\u0442\u0441\u044f \u043b\u0438\u043d\u0435\u0439\u043d\u0430\u044f \u043f\u0440\u043e\u0435\u043a\u0446\u0438\u044f \u0434\u043e \u0436\u0435\u043b\u0430\u0435\u043c\u043e\u0439 \u0440\u0430\u0437\u043c\u0435\u0440\u043d\u043e\u0441\u0442\u0438 - \u044d\u0442\u043e \u0431\u0443\u0434\u0435\u0442 \u043f\u0430\u0447\u0442 \u044d\u043c\u0431\u0435\u0434\u0434\u0438\u043d\u0433\u0430. (\u042d\u0442\u0430 \u043c\u0430\u0442\u0440\u0438\u0446\u0430 \u043f\u0435\u0440\u0435\u0432\u043e\u0434\u0430 \u0434\u043b\u044f \u0432\u0441\u0435\u0445 \u043f\u0430\u0442\u0447\u0435\u0439 \u043e\u0434\u0438\u043d\u0430\u043a\u043e\u0432\u0430\u044f)"}}