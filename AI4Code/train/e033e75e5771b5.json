{"cell_type":{"f147e8e2":"code","2bd615c6":"code","31273253":"code","4439e620":"code","422d4f18":"code","39baeab7":"code","2abb6daf":"code","e8819cf4":"code","c1e105f8":"code","7f0e932a":"code","94771239":"code","82837388":"code","6ea1c606":"code","b984121f":"code","8e056601":"code","e7b0ec4a":"code","6f1c0e78":"code","e886e6f4":"code","e8c54cf6":"code","04412125":"code","5d5c6639":"code","75c4e9de":"code","9e1b8515":"code","162d0c60":"code","02fa9aaf":"code","7898f0db":"code","46f79c7d":"code","6481afae":"code","9eee1a9d":"code","6fcf0bbc":"code","613b6ffe":"code","a8602fb7":"code","4ed1cd63":"code","7de411ad":"code","5b543eb7":"code","adb8c501":"code","42746754":"code","028a3cb9":"code","989b3240":"code","e40b9ed0":"code","8b893f0a":"code","3f280cec":"code","5cfcc211":"code","8fb9e45c":"code","8200b1ee":"code","30d2b699":"code","578265bc":"code","584022ca":"code","75dc1719":"code","627ebbc3":"code","194ea582":"code","848fcfc9":"code","56176247":"code","ed06920d":"code","adf49988":"code","9a7f7985":"code","17598568":"code","5c4fa218":"code","77c877d3":"markdown","db535ad7":"markdown","2b7e2165":"markdown","0dacd343":"markdown","52918ad1":"markdown","d8f7c8bf":"markdown","2910aac4":"markdown","467d4213":"markdown","fc19e097":"markdown","0bbe3c23":"markdown","2451708b":"markdown","aa080445":"markdown","825e716e":"markdown","126f16a1":"markdown","1494f093":"markdown","d62d18d1":"markdown","44e43389":"markdown","35ded3e6":"markdown","719a0df2":"markdown","6d65c01e":"markdown"},"source":{"f147e8e2":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","2bd615c6":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom scipy import stats\nfrom scipy.stats import randint\n\n# prep\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV, train_test_split\nfrom sklearn import preprocessing\nfrom sklearn.datasets import make_classification\nfrom sklearn.preprocessing import binarize, LabelEncoder, MinMaxScaler\n\n# models\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n\n# Validation libraries\nfrom sklearn import metrics\nfrom sklearn.metrics import accuracy_score, mean_squared_error, precision_recall_curve\nfrom sklearn.model_selection import cross_val_score\n\n#Neural Network\nfrom sklearn.neural_network import MLPClassifier\n\n\n#Bagging\nfrom sklearn.ensemble import BaggingClassifier, AdaBoostClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\n\n#Naive bayes\nfrom sklearn.naive_bayes import GaussianNB \n\n#Stacking\nfrom mlxtend.classifier import StackingClassifier\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"..\/input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output.\n\n#reading in CSV's from a file path\ntrain_df = pd.read_csv('..\/input\/iimsurveyy\/survey.csv')\n\n\n#Pandas: whats the data row count?\nprint(train_df.shape)\n    \n#Pandas: whats the distribution of the data?\nprint(train_df.describe())\n    \n#Pandas: What types of data do i have?\nprint(train_df.info())","31273253":"train_df = train_df.drop(['V27'], axis= 1)\n#train_df = train_df.drop([''], axis= 1)\n#train_df = train_df.drop(['Timestamp'], axis= 1)\n\ntrain_df.isnull().sum().max() #just checking that there's no missing data missing...\ntrain_df.head(5)","4439e620":"defaultInt = 0\ndefaultString = 'NaN'\ndefaultFloat = 0.0\n\nintFeatures = ['V1','V21','V3','V4','V24']\nstringFeatures = ['V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10', 'V11',\n       'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20',\n        'V22', 'V23', 'V25', 'V26', 'V30', 'V31']\nfloatFeatures = []\nfor feature in train_df:\n    if feature in intFeatures:\n        train_df[feature] = train_df[feature].fillna(defaultInt)\n    elif feature in stringFeatures:\n        train_df[feature] = train_df[feature].fillna(defaultString)\n    elif feature in floatFeatures:\n        train_df[feature] = train_df[feature].fillna(defaultFloat)\n    else:\n        print('Error: Feature %s not recognized.' % feature)\ntrain_df.head(5)  ","422d4f18":"train_df['V25'].unique()","39baeab7":"train_df.V25.nunique(dropna = True) ","2abb6daf":"train_df.describe","e8819cf4":"cols=['V3','V4']\ntrain_df[cols] = train_df[cols].applymap(np.int64)\ntrain_df.head(5)  ","c1e105f8":"train_df['V22'].unique()","7f0e932a":"gender = train_df['V25'].str.lower()\ngender = train_df['V25'].unique()\nprint(gender)","94771239":"male_str= ['Male','male', 'M','m', 'male ','Male ', 'Male (cis)','Sex is male', \n          'mail','Male.','man','Cis Male', 'cis man','Cis male' ,'Man',  'cis male', 'Dude',  'cisdude', \n          'Malr', 'mtf', \"I'm a man why didn't you make this a drop down question. You should of asked sex? And I would of answered yes please. Seriously how much text can this take? \"\n            , 'M|','MALE']\ntrans_str= ['Transitioned, M2F','Other\/Transfeminine', 'non-binary', 'Bigender','Genderfluid (born female)', 'male 9:1 female, roughly', 'Other',\n             'genderqueer' 'Human', 'Transgender woman','Genderfluid', 'Male\/genderqueer',  'Nonbinary', 'Genderqueer', 'Androgynous','nb masculine',\n              'Enby','Agender', 'none of your business','genderqueer woman' 'Queer','Male (trans, FtM)', 'Genderflux demi-girl',  'Fluid','human' ,'Unicorn'  ]\nfemale_str= ['F' , 'female', 'Female', 'f', 'woman', 'Cis-woman','female-bodied; no feelings about gender', 'female ', 'fem', ' Female','Female', \n            'I identify as female.','fm','Cis female ','female\/woman', 'Cisgender Female','Woman' , 'Female or Multi-Gender Femme' 'Male.',  \n            'Female (props for making this a freeform field, though)','Female assigned at birth ','AFAB' ]","82837388":"for (row, col) in train_df.iterrows():\n\n    if str.lower(col.V25) in male_str:\n        train_df['V25'].replace(to_replace=col.V25, value='male', inplace=True)\n\n    if str.lower(col.V25) in female_str:\n        train_df['V25'].replace(to_replace=col.V25, value='female', inplace=True)\n\n    if str.lower(col.V25) in trans_str:\n        train_df['V25'].replace(to_replace=col.V25, value='trans', inplace=True)\n\n#Get rid of bullshit","6ea1c606":"train_df['V25'].unique()","b984121f":"train_df.head(7)","8e056601":"train_df['V24'].fillna(train_df['V24'].median(), inplace = True)\ns = pd.Series(train_df['V24'])\ns[s<18] = train_df['V24'].median()\ntrain_df['V24'] = s\ns = pd.Series(train_df['V24'])\ns[s>120] = train_df['V24'].median()\ntrain_df['V24'] = s\n\n#Ranges of Age\ntrain_df['age_range'] = pd.cut(train_df['V24'], [0,20,30,65,100], labels=[\"0-20\", \"21-30\", \"31-65\", \"66-100\"], include_lowest=True)","e7b0ec4a":"fig,ax = plt.subplots(figsize=(8, 6))\nsns.distplot(train_df['V24'].dropna(),ax=ax, kde=False, color='#ffa726')\nplt.title('Age Distribution')\nplt.ylabel(\"Frequency\")","6f1c0e78":"#Age group vs Treatment\nfig,ax = plt.subplots(figsize=(8, 6))\nsns.countplot(data = train_df, x = 'age_range', hue='V21')\nplt.title('Age Group vs Treatment')","e886e6f4":"orig_mental_health_dataset = train_df.copy()\n\ndef understand_variables(dataset):\n    print(\"Type = \" +str(type(dataset))+\"\\n\")\n    print(\"Shape = \"+str(dataset.shape)+\"\\n\")\n    print(\"Head : \\n\\n\"+str(dataset.head())+\"\\n\\n\")\n    print(str(dataset.info())+\"\\n\\n\")\n    print(\"No.of unique values :\\n\\n\"+str(dataset.nunique(axis=0))+\"\\n\\n\")\n    print(\"Description :\\n\\n\"+str(dataset.describe())+\"\\n\\n\")\n    \n    #print(dataset.describe(exclude=[np.number]))\n    #Since no categorical variables, no need to have the above line\n    \n    print(\"Null count :\\n\\n\"+str(dataset.isnull().sum()))\n    \nunderstand_variables(train_df)","e8c54cf6":"def understand_dist(dataset,feature_type):\n    \n    if feature_type == \"Categorical\":\n        \n        categorical_features=[feature for feature in dataset.columns if dataset[feature].dtype=='O']   \n        dataframes=[]\n        for feature in categorical_features:\n            dataframe=dataset[feature].value_counts().rename_axis(feature).reset_index(name='counts')\n            dataframes.append(dataframe)\n            \n            plt.figure(figsize=(10,4))\n            sns.countplot(x=feature,data = dataset)\n            plt.show()\n            print(dataframe,'\\n')\n\n        #for i in range(len(dataframes)):\n        #    print(dataframes[i],'\\n')\n            \n        \n            \n    elif feature_type == \"Numeric\":\n        \n        numerical_features=[feature for feature in dataset.columns if dataset[feature].dtype!='O']\n        \n        for feature in numerical_features:\n            plt.figure(figsize=(10,4))\n            sns.distplot(dataset[feature])\n            plt.show()\n            \n            print(\"Description :\\n\\n\"+str(dataset[feature].describe())+\"\\n\\n\")","04412125":"understand_dist(train_df,\"Categorical\")","5d5c6639":"labelDict = {}\nfor feature in train_df:\n    le = preprocessing.LabelEncoder()\n    le.fit(train_df[feature])\n    le_name_mapping = dict(zip(le.classes_, le.transform(le.classes_)))\n    train_df[feature] = le.transform(train_df[feature])\n    # Get labels\n    labelKey = 'label_' + feature\n    labelValue = [*le_name_mapping]\n    labelDict[labelKey] =labelValue\n    \nfor key, value in labelDict.items():     \n    print(key, value)\n\n#Get rid of 'Country'\ntrain_df = train_df.drop(['V26'], axis= 1)\ntrain_df.head()","75c4e9de":"total = train_df.isnull().sum().sort_values(ascending=False)\npercent = (train_df.isnull().sum()\/train_df.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data.head(20)\nprint(missing_data)","9e1b8515":"corrmat = train_df.corr()\nf, ax = plt.subplots(figsize=(12, 9))\nsns.heatmap(corrmat, vmax=.8, square=True);\nplt.show()\n#TREATMENT HEATMAT\nk = 10 #number of variables for heatmap\ncols = corrmat.nlargest(k, 'V21')['V21'].index\ncm = np.corrcoef(train_df[cols].values.T)\nsns.set(font_scale=1.25)\nhm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values)\nplt.show()","162d0c60":"scaler = MinMaxScaler()\ntrain_df['V24'] = scaler.fit_transform(train_df[['V24']])\ntrain_df.head()","02fa9aaf":"#SPLIT\n\n# define X and y\nfeature_cols = ['V24', 'V25','V15', 'V16', 'V5', 'V6', 'V22','V23', 'V9', 'V10','V17', 'V18','V19','V31'  ]\nX = train_df[feature_cols]\ny = train_df.V21\n\n# split X and y into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=0)\n\n# Create dictionaries for final graph\n# Use: methodDict['Stacking'] = accuracy_score\nmethodDict = {}\nrmseDict = ()","7898f0db":"# Build a forest and compute the feature importances\nforest = ExtraTreesClassifier(n_estimators=250,\n                              random_state=0)\n\nforest.fit(X, y)\nimportances = forest.feature_importances_\nstd = np.std([tree.feature_importances_ for tree in forest.estimators_],\n             axis=0)\nindices = np.argsort(importances)[::-1]\n\nlabels = []\nfor f in range(X.shape[1]):\n    labels.append(feature_cols[f])      \n    \n# Plot the feature importances of the forest\nplt.figure(figsize=(12,8))\nplt.title(\"Feature importances\")\nplt.bar(range(X.shape[1]), importances[indices],\n       color=\"r\", yerr=std[indices], align=\"center\")\nplt.xticks(range(X.shape[1]), labels, rotation='vertical')\nplt.xlim([-1, X.shape[1]])\nplt.show()","46f79c7d":"def evalClassModel(model, y_test, y_pred_class, plot=False):\n    print('Accuracy:', metrics.accuracy_score(y_test, y_pred_class))\n    print('Null accuracy:\\n', y_test.value_counts())\n    print('Percentage of ones:', y_test.mean())  \n    print('Percentage of zeros:',1 - y_test.mean())\n    print('True:', y_test.values[0:25])\n    print('Pred:', y_pred_class[0:25])\n\n    confusion = metrics.confusion_matrix(y_test, y_pred_class)\n    #[row, column]\n    TP = confusion[1, 1]\n    TN = confusion[0, 0]\n    FP = confusion[0, 1]\n    FN = confusion[1, 0]\n    \n    # visualize Confusion Matrix\n    sns.heatmap(confusion,annot=True,fmt=\"d\") \n    plt.title('Confusion Matrix')\n    plt.xlabel('Predicted')\n    plt.ylabel('Actual')\n    plt.show()\n\n    accuracy = metrics.accuracy_score(y_test, y_pred_class)\n    print('Classification Accuracy:', accuracy)\n    print('Classification Error:', 1 - metrics.accuracy_score(y_test, y_pred_class))\n    false_positive_rate = FP \/ float(TN + FP)\n    print('False Positive Rate:', false_positive_rate)\n    \n    #Precision: When a positive value is predicted, how often is the prediction correct?\n    print('Precision:', metrics.precision_score(y_test, y_pred_class))\n    \n    \n    # IMPORTANT: first argument is true values, second argument is predicted probabilities\n    print('AUC Score:', metrics.roc_auc_score(y_test, y_pred_class))\n    \n    # calculate cross-validated AUC\n    print('Cross-validated AUC:', cross_val_score(model, X, y, cv=10, scoring='roc_auc').mean())\n    print('First 10 predicted responses:\\n', model.predict(X_test)[0:10])\n    print('First 10 predicted probabilities of class members:\\n', model.predict_proba(X_test)[0:10])\n    model.predict_proba(X_test)[0:10, 1]\n    y_pred_prob = model.predict_proba(X_test)[:, 1]\n    \n    if plot == True:\n\n        plt.rcParams['font.size'] = 12\n        # 8 bins\n        plt.hist(y_pred_prob, bins=8)\n        \n        # x-axis limit from 0 to 1\n        plt.xlim(0,1)\n        plt.title('Histogram of predicted probabilities')\n        plt.xlabel('Predicted probability of treatment')\n        plt.ylabel('Frequency')\n\n    y_pred_prob = y_pred_prob.reshape(-1,1) \n    y_pred_class = binarize(y_pred_prob, 0.3)[0]\n    \n    # print the first 10 predicted probabilities\n    print('First 10 predicted probabilities:\\n', y_pred_prob[0:10])\n    \n    roc_auc = metrics.roc_auc_score(y_test, y_pred_prob)\n    \n\n    fpr, tpr, thresholds = metrics.roc_curve(y_test, y_pred_prob)\n    if plot == True:\n        plt.figure()\n        \n        plt.plot(fpr, tpr, color='darkorange', label='ROC curve (area = %0.2f)' % roc_auc)\n        plt.plot([0, 1], [0, 1], color='navy', linestyle='--')\n        plt.xlim([0.0, 1.0])\n        plt.ylim([0.0, 1.0])\n        plt.rcParams['font.size'] = 12\n        plt.title('ROC curve for treatment classifier')\n        plt.xlabel('False Positive Rate (1 - Specificity)')\n        plt.ylabel('True Positive Rate (Sensitivity)')\n        plt.legend(loc=\"lower right\")\n        plt.show()\n    def evaluate_threshold(threshold):\n        print('Specificity for ' + str(threshold) + ' :', 1 - fpr[thresholds > threshold][-1])\n\n    predict_mine = np.where(y_pred_prob > 0.50, 1, 0)\n    confusion = metrics.confusion_matrix(y_test, predict_mine)\n    print(confusion)\n    \n    \n    \n    return accuracy","6481afae":"def tuningCV(knn):\n    \n    # search for an optimal value of K for KNN\n    k_range = list(range(1, 31))\n    k_scores = []\n    for k in k_range:\n        knn = KNeighborsClassifier(n_neighbors=k)\n        scores = cross_val_score(knn, X, y, cv=10, scoring='accuracy')\n        k_scores.append(scores.mean())\n    print(k_scores)\n    # plot the value of K for KNN (x-axis) versus the cross-validated accuracy (y-axis)\n    plt.plot(k_range, k_scores)\n    plt.xlabel('Value of K for KNN')\n    plt.ylabel('Cross-Validated Accuracy')\n    plt.show()","9eee1a9d":"def tuningGridSerach(knn):\n    #More efficient parameter tuning using GridSearchCV\n    # define the parameter values that should be searched\n    k_range = list(range(1, 31))\n    print(k_range)\n    \n    # create a parameter grid: map the parameter names to the values that should be searched\n    param_grid = dict(n_neighbors=k_range)\n    print(param_grid)\n    \n    # instantiate the grid\n    grid = GridSearchCV(knn, param_grid, cv=10, scoring='accuracy')\n\n    # fit the grid with data\n    grid.fit(X, y)\n    \n    # view the complete results (list of named tuples)\n    grid.cv_results_\n    \n    # examine the first tuple\n    print(grid.cv_results_[0].parameters)\n    print(grid.cv_results_[0].cv_validation_scores)\n    print(grid.cv_results_[0].mean_validation_score)\n    \n    # create a list of the mean scores only\n    grid_mean_scores = [result.mean_validation_score for result in grid.cv_results_]\n    print(grid_mean_scores)\n    \n    # plot the results\n    plt.plot(k_range, grid_mean_scores)\n    plt.xlabel('Value of K for KNN')\n    plt.ylabel('Cross-Validated Accuracy')\n    plt.show()\n    \n    # examine the best model\n    print('GridSearch best score', grid.best_score_)\n    print('GridSearch best params', grid.best_params_)\n    print('GridSearch best estimator', grid.best_estimator_)","6fcf0bbc":"def tuningRandomizedSearchCV(model, param_dist):\n    #Searching multiple parameters simultaneously\n    # n_iter controls the number of searches\n    rand = RandomizedSearchCV(model, param_dist, cv=10, scoring='accuracy', n_iter=10, random_state=5)\n    rand.fit(X, y)\n    rand.cv_results_\n    \n    # examine the best model\n    print('Rand. Best Score: ', rand.best_score_)\n    print('Rand. Best Params: ', rand.best_params_)\n    \n    # run RandomizedSearchCV 20 times (with n_iter=10) and record the best score\n    best_scores = []\n    for _ in range(20):\n        rand = RandomizedSearchCV(model, param_dist, cv=10, scoring='accuracy', n_iter=10)\n        rand.fit(X, y)\n        best_scores.append(round(rand.best_score_, 3))\n    print(best_scores)","613b6ffe":"def logisticRegression():\n    # train a logistic regression model on the training set\n    logreg = LogisticRegression()\n    logreg.fit(X_train, y_train)\n    \n    # make class predictions for the testing set\n    y_pred_class = logreg.predict(X_test)\n    \n    print('########### Logistic Regression ###############')\n    \n    accuracy_score = evalClassModel(logreg, y_test, y_pred_class, True)\n    \n    #Data for final graph\n    methodDict['Log. Regres.'] = accuracy_score * 100","a8602fb7":"logisticRegression()","4ed1cd63":"def Knn():\n    knn = KNeighborsClassifier(n_neighbors=5)\n    k_range = list(range(1, 31))\n    weight_options = ['uniform', 'distance']\n    param_dist = dict(n_neighbors=k_range, weights=weight_options)\n    tuningRandomizedSearchCV(knn, param_dist)\n    \n    # train a KNeighborsClassifier model on the training set\n    knn = KNeighborsClassifier(n_neighbors=27, weights='uniform')\n    knn.fit(X_train, y_train)\n    \n    # make class predictions for the testing set\n    y_pred_class = knn.predict(X_test)\n    \n    print('########### KNeighborsClassifier ###############')\n    \n    accuracy_score = evalClassModel(knn, y_test, y_pred_class, True)\n\n    #Data for final graph\n    methodDict['KNN'] = accuracy_score * 100","7de411ad":"Knn()","5b543eb7":"def randomForest():\n    # Calculating the best parameters\n    forest = RandomForestClassifier(n_estimators = 20)\n\n    featuresSize = feature_cols.__len__()\n    param_dist = {\"max_depth\": [3, None],\n              \"max_features\": randint(1, featuresSize),\n              \"min_samples_split\": randint(2, 9),\n              \"min_samples_leaf\": randint(1, 9),\n              \"criterion\": [\"gini\", \"entropy\"]}\n    tuningRandomizedSearchCV(forest, param_dist)\n    \n    # Building and fitting my_forest\n    forest = RandomForestClassifier(max_depth = None, min_samples_leaf=8, min_samples_split=2, n_estimators = 20, random_state = 1)\n    my_forest = forest.fit(X_train, y_train)\n    \n    # make class predictions for the testing set\n    y_pred_class = my_forest.predict(X_test)\n    \n    print('########### Random Forests ###############')\n    \n    accuracy_score = evalClassModel(my_forest, y_test, y_pred_class, True)\n\n    #Data for final graph\n    methodDict['R. Forest'] = accuracy_score * 100","adb8c501":"randomForest()","42746754":"def treeClassifier():\n    # Calculating the best parameters\n    tree = DecisionTreeClassifier()\n    featuresSize = feature_cols.__len__()\n    param_dist = {\"max_depth\": [3, None],\n              \"max_features\": randint(1, featuresSize),\n              \"min_samples_split\": randint(2, 9),\n              \"min_samples_leaf\": randint(1, 9),\n              \"criterion\": [\"gini\", \"entropy\"]}\n    tuningRandomizedSearchCV(tree, param_dist)\n    \n    # train a decision tree model on the training set\n    tree = DecisionTreeClassifier(max_depth=3, min_samples_split=8, max_features=6, criterion='entropy', min_samples_leaf=7)\n    tree.fit(X_train, y_train)\n    \n    # make class predictions for the testing set\n    y_pred_class = tree.predict(X_test)\n    \n    print('########### Tree classifier ###############')\n    \n    accuracy_score = evalClassModel(tree, y_test, y_pred_class, True)\n\n    #Data for final graph\n    methodDict['Tree clas.'] = accuracy_score * 100","028a3cb9":"treeClassifier()","989b3240":"import tensorflow as tf\nimport argparse\nfrom sklearn.model_selection import train_test_split,cross_val_score\n\n\nbatch_size = 100\ntrain_steps = 1000\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=0)\n\ndef train_input_fn(features, labels, batch_size):\n    \"\"\"An input function for training\"\"\"\n    # Convert the inputs to a Dataset.\n    dataset = tf.data.Dataset.from_tensor_slices((dict(features), labels))\n\n    # Shuffle, repeat, and batch the examples.\n    return dataset.shuffle(1000).repeat().batch(batch_size)\n\ndef eval_input_fn(features, labels, batch_size):\n    \"\"\"An input function for evaluation or prediction\"\"\"\n    features=dict(features)\n    if labels is None:\n        # No labels, use only features.\n        inputs = features\n    else:\n        inputs = (features, labels)\n\n    # Convert the inputs to a Dataset.\n    dataset = tf.data.Dataset.from_tensor_slices(inputs)\n\n    # Batch the examples\n    assert batch_size is not None, \"batch_size must not be None\"\n    dataset = dataset.batch(batch_size)\n\n    # Return the dataset.\n    return dataset","e40b9ed0":"#['V24', 'V25','V15', 'V16', 'V5', 'V6', 'V22','V23', 'V9', 'V10','V17', 'V18','V19','V31'  ]\nV24 = tf.feature_column.numeric_column(\"V24\")\nV25 = tf.feature_column.numeric_column(\"V25\")\nV16 = tf.feature_column.numeric_column(\"V16\")\nV15 = tf.feature_column.numeric_column(\"V15\")\nV5 = tf.feature_column.numeric_column(\"V5\")\nV6 = tf.feature_column.numeric_column(\"V6\")\nV22 = tf.feature_column.numeric_column(\"V22\")\nV23 = tf.feature_column.numeric_column(\"V23\")\nV9 = tf.feature_column.numeric_column(\"V9\")\nV10 = tf.feature_column.numeric_column(\"V10\")\nV17 = tf.feature_column.numeric_column(\"V17\")\nV19 = tf.feature_column.numeric_column(\"V19\")\nV18 = tf.feature_column.numeric_column(\"V18\")\nV31 = tf.feature_column.numeric_column(\"V31\")\nfeature_columns = [V24, V25,V15,V16,V5,V6,V22,V23,V9,V10,V17,V18,V19,V31]","8b893f0a":"from tensorflow import keras\nfrom tensorflow.keras import layers\nmodel = keras.Sequential([\nlayers.Dense(4, activation='relu', input_shape=[14]),\nlayers.Dense(4, activation='relu'),\nlayers.Dense(1, activation='sigmoid'),\n])","3f280cec":"model.compile(\noptimizer='adam',\nloss='binary_crossentropy',\nmetrics=['binary_accuracy'],\n)","5cfcc211":"early_stopping = keras.callbacks.EarlyStopping(\npatience=5,\nmin_delta=0.001,\nrestore_best_weights=True,\n)\nhistory = model.fit(\nX_train, y_train,\nvalidation_data=(X_test, y_test),\nbatch_size=1000,\nepochs=1000,\nverbose=1,\n)","8fb9e45c":"history_df = pd.DataFrame(history.history)\n# Start the plot at epoch 4\nhistory_df.loc[4:, ['loss', 'val_loss']].plot()\nhistory_df.loc[4:, ['binary_accuracy', 'val_binary_accuracy']].plot()\nprint((\"Best Validation Loss: {:0.4f}\" +\\\n\"\\nBest Validation Accuracy: {:0.4f}\")\\\n.format(history_df['val_loss'].min(),\nhistory_df['val_binary_accuracy'].max()))","8200b1ee":"from sklearn.model_selection import cross_val_predict, cross_val_score, cross_validate\nfrom sklearn.model_selection import StratifiedKFold, StratifiedShuffleSplit\nfrom sklearn.metrics import confusion_matrix, roc_curve\nfrom sklearn.metrics import precision_score, recall_score, f1_score\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.wrappers.scikit_learn import KerasClassifier\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.utils import plot_model\nfrom tensorflow.keras.layers import Input, Dense, Dropout, AlphaDropout, BatchNormalization,Concatenate, concatenate\nfrom tensorflow.keras.optimizers import SGD, RMSprop, Adamax, Adagrad, Adam, Nadam, SGD\nfrom tensorflow.keras.regularizers import l2\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\nfrom tensorflow.keras.metrics import *","30d2b699":"metrics = ['accuracy', \n           Precision(),\n           Recall()]\ndef create_model():\n    model = Sequential()\n    model.add(Input(shape=X_train.shape[1], name='Input_'))\n    model.add(Dense(8, activation='relu', kernel_initializer='glorot_normal', kernel_regularizer=l2(0.00001)))\n    model.add(Dense(16, activation='relu', kernel_initializer='glorot_normal', kernel_regularizer=l2(0.0001)))\n    model.add(Dense(32, activation='relu', kernel_initializer='glorot_normal', kernel_regularizer=l2(0.001)))\n    model.add(Dense(64, activation='relu', kernel_initializer='glorot_normal', kernel_regularizer=l2(0.1)))\n    model.add(Dropout(0.5))\n    model.add(Dense(16, activation='relu', kernel_initializer='glorot_normal', kernel_regularizer=l2(0.1)))\n    model.add(Dropout(0.5))\n    model.add(Dense(1, activation='sigmoid', kernel_initializer='glorot_normal'))\n\n    model.summary()\n    optimize = Adam(lr = 0.0001)\n    model.compile(optimizer = optimize, \n                       loss = 'binary_crossentropy', \n                       metrics = metrics)\n    return model","578265bc":"estimator = KerasClassifier(build_fn = create_model, epochs = 1000, batch_size = 100, verbose = 1)\nkfold = StratifiedKFold(n_splits = 3)\nresults = cross_val_score(estimator, X_train, y_train, cv = kfold)","584022ca":"train_history = estimator.fit(X_train, y_train, epochs = 1000, batch_size = 100)\n","75dc1719":"import seaborn as sns\nsns.set_style('whitegrid')\nimport tensorflow as tf\nfrom sklearn.model_selection import train_test_split, cross_val_predict\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.decomposition import PCA\nfrom xgboost import XGBClassifier\nfrom sklearn.preprocessing import LabelEncoder, MinMaxScaler\nfrom sklearn.metrics import confusion_matrix, precision_score, recall_score,f1_score\nimport time\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n%matplotlib inline","627ebbc3":"xgb = XGBClassifier(n_estimators=100)\ntraining_start = time.perf_counter()\nxgb.fit(X_train, y_train)\ntraining_end = time.perf_counter()\nprediction_start = time.perf_counter()\npreds = xgb.predict(X_test)\nprediction_end = time.perf_counter()\nacc_xgb = (preds == y_test).sum().astype(float) \/ len(preds)*100\nxgb_train_time = training_end-training_start\nxgb_prediction_time = prediction_end-prediction_start\nprint(\"XGBoost's prediction accuracy is: %3.2f\" % (acc_xgb))\nprint(\"Time consumed for training: %4.3f\" % (xgb_train_time))\nprint(\"Time consumed for prediction: %6.5f seconds\" % (xgb_prediction_time))\nmethodDict['XGBoost'] = acc_xgb * 100","194ea582":"xgb_cv = XGBClassifier(n_estimators=100)\nscores = cross_val_score(xgb_cv, X_train, y_train, cv=10, scoring = \"accuracy\")\nprint(\"Scores:\", scores)\nprint(\"Mean:\", scores.mean())\nprint(\"Standard Deviation:\", scores.std())","848fcfc9":"from sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier, VotingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC","56176247":"log_reg = LogisticRegression(random_state=42)\nlog_reg.fit(X_train, y_train)\nprint(\"Accuracy: \", log_reg.score(X_test,y_test))","ed06920d":"knn_clsf = KNeighborsClassifier(n_neighbors=8)\nknn_clsf.fit(X_train, y_train)\nprint(\"Accuracy: \", knn_clsf.score(X_test,y_test))","adf49988":"rf_reg = RandomForestClassifier(random_state=42)\nrf_reg.fit(X_train, y_train)\nprint(\"Accuracy: \", rf_reg.score(X_test,y_test))","9a7f7985":"svm_clsf = SVC()\nsvm_clsf.fit(X_train, y_train)\nprint(\"Accuracy: \", svm_clsf.score(X_test,y_test))","17598568":"voting_classfication = VotingClassifier(estimators = [('knn', knn_clsf),('lg',log_reg), ('rfg', rf_reg), ('svc', svm_clsf)], voting=\"hard\", n_jobs=-1)\nvoting_classfication.fit(X_train, y_train)\nprint(\"Accuracy: \", voting_classfication.score(X_test,y_test))","5c4fa218":"import pickle\nPkl_Filename = \"Modelimp.pkl\"\n\nwith open(Pkl_Filename, 'wb') as file:\n    pickle.dump(voting_classfication, file)\n\n    # some time later...\n# load the model from disk\n\nwith open(Pkl_Filename, 'rb') as file:\n    Pickled_LR_Model = pickle.load(file)\nPickled_LR_Model","77c877d3":"## 6.2 KNN","db535ad7":"# Mental Health in IT: EDA and Prediction","2b7e2165":"## \u270c\ufe0f Please Upvote if you found it useful. It will help me edge further towards my goal of becoming a Data Scientist. \u270c\ufe0f","0dacd343":"## Exporting the model","52918ad1":"# The Dataset \u270b\ud83c\udffc\n \n## This dataset contains the following data:\n\n**Timestamp**\n\n**Age**\n\n**Gender**\n\n**Country**\n\n**state**: If you live in the United States, which state or territory do you live in?\n\n**self_employed**: Are you self-employed?\n\n**family_history**: Do you have a family history of mental illness?\n\n**treatment**: Have you sought treatment for a mental health condition?\n\n**work_interfere**: If you have a mental health condition, do you feel that it interferes with your work?\n\n**no_employees**: How many employees does your company or organization have?\n\n**remote_work**: Do you work remotely (outside of an office) at least 50% of the time?\n\n**tech_company**: Is your employer primarily a tech company\/organization?\n\n**benefits**: Does your employer provide mental health benefits?\n\n**care_options**: Do you know the options for mental health care your employer provides?\n\n**wellness_program**: Has your employer ever discussed mental health as part of an employee wellness program?\n\n**seek_help**: Does your employer provide resources to learn more about mental health issues and how to seek help?\n\n**anonymity**: Is your anonymity protected if you choose to take advantage of mental health or substance abuse treatment resources?\n\n**leave**: How easy is it for you to take medical leave for a mental health condition?\n\n**mentalhealthconsequence**: Do you think that discussing a mental health issue with your employer would have negative consequences?\n\n**physhealthconsequence**: Do you think that discussing a physical health issue with your employer would have negative consequences?\n\n**coworkers**: Would you be willing to discuss a mental health issue with your coworkers?\n\n**supervisor**: Would you be willing to discuss a mental health issue with your direct supervisor(s)?\n\n**mentalhealthinterview**: Would you bring up a mental health issue with a potential employer in an interview?\n\n**physhealthinterview**: Would you bring up a physical health issue with a potential employer in an interview?\n\n**mentalvsphysical**: Do you feel that your employer takes mental health as seriously as physical health?\n\n**obs_consequence**: Have you heard of or observed negative consequences for coworkers with mental health conditions in your workplace?\n\n**comments**: Any additional notes or comments","d8f7c8bf":"# EDA","2910aac4":"## 6.4 Decision Tree classifier","467d4213":"# 5. Tuning \u2699\ufe0f ","fc19e097":"# 6. Models \ud83e\udde0","0bbe3c23":"![Mental Health](https:\/\/contenthub-static.grammarly.com\/blog\/wp-content\/uploads\/2020\/04\/How-Writing-Can-Help-Support-Your-Mental-Health.jpg)","2451708b":"## The original dataset is from Open Sourcing Mental Illness (OSMI) and can be downloaded here. (https:\/\/osmihelp.org\/research\/)","aa080445":"## 6.3 Random Forest","825e716e":"# 1. Importing Libraries and Data Loading \ud83d\udc48\ud83c\udffb ","126f16a1":"## 6.5 Neural Networks","1494f093":"# 4. Scaling \ud83d\udc7e\n","d62d18d1":"## 6.1 Logistic Regression","44e43389":"# 2. Data Cleaning \ud83e\uddf9","35ded3e6":"# 3. Encoding Data \ud83d\ude0e","719a0df2":"## 6.6 XGBoost","6d65c01e":"## Well, this seems like the end to this notebook. \ud83d\ude31\n\n"}}