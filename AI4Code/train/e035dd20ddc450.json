{"cell_type":{"64c61e2b":"code","a4a6e194":"code","dea5dc76":"code","59931c2f":"code","d28e4c60":"code","11bd2f8d":"code","5c50337e":"code","b2b72fb7":"code","c0c14946":"code","82e6bcad":"code","f64e5e3d":"code","f7c1762e":"code","a43b05c3":"code","17049ad1":"code","04a1bc27":"markdown","a8d87666":"markdown","90a0c458":"markdown","69736ebe":"markdown","f8909cf9":"markdown","be9a4d5a":"markdown","aa80bfb4":"markdown","bc327dd4":"markdown","510a952b":"markdown","82566905":"markdown"},"source":{"64c61e2b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","a4a6e194":"!pip install dataprep\n!pip install pyspellchecker","dea5dc76":"from spellchecker import SpellChecker\nfrom dataprep.clean import clean_text\nimport pandas as pd\nfrom nltk import SnowballStemmer\nimport time","59931c2f":"train_full = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/train.csv\")\ntest_full = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/test.csv\")\n\nprint('Training Set Shape = {}'.format(train_full.shape))\nprint('Training Set Memory Usage = {:.2f}MB'.format(train_full.memory_usage().sum()\/2**20))\n\nprint('Test Set Shape = {}'.format(test_full.shape))\nprint('Test Set Memory Usage = {:.2f}MB'.format(test_full.memory_usage().sum()\/2**20))","d28e4c60":"duplicate_data = train_full[train_full.duplicated(['text','target'], keep=False)]","11bd2f8d":"train_full.drop_duplicates(['text','target'], inplace=True, ignore_index=True)\ntrain_full.shape","5c50337e":"custom_pipeline = [\n    {\"operator\": \"fillna\", \"parameters\":{\"value\":\"\"}},\n    {\"operator\": \"lowercase\"},\n    {\"operator\": \"remove_digits\"},\n    {\"operator\": \"remove_html\"},\n    {\"operator\": \"remove_urls\"},\n    {\"operator\": \"remove_accents\"},\n    {\"operator\": \"remove_whitespace\"},\n]","b2b72fb7":"df_train = clean_text(train_full, \"text\", pipeline = custom_pipeline)\ndf_test = clean_text(test_full, \"text\", pipeline= custom_pipeline)","c0c14946":"spell = SpellChecker()\ndef correct_spelling(text):    \n    misspelled_words = spell.unknown(text.split())\n    corrected_text = [spell.correction(w) if w in misspelled_words else w for w in text.split()] \n    return \" \".join(corrected_text)","82e6bcad":"t1 = time.time()\ndf_train.text = df_train.text.apply(correct_spelling)\ndeltaT1 = time.time() - t1","f64e5e3d":"t2 = time.time()\ndf_test.text = df_test.text.apply(correct_spelling)\ndeltaT2 = time.time() - t2","f7c1762e":"stemmer = SnowballStemmer(\"english\")\n\ndef stemming_text(text):\n    return ' '.join(stemmer.stem(w) for w in text.split(' '))\n\ndf_train.text = df_train.text.apply(stemming_text)\ndf_test.text = df_test.text.apply(stemming_text)","a43b05c3":"df_train[['text','target']].to_csv('train_prepared.csv', index=False)\ndf_test.text.to_csv('test_prepared.csv', index=False)","17049ad1":"pd.read_csv('train_prepared.csv') , pd.read_csv('test_prepared.csv')","04a1bc27":"<a id='3'><\/a>\n# <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:150%; text-align:center; border-radius: 15px 50px;\">3. Saving Data \ud83d\udc8e<\/p>","a8d87666":"### We lost arround 1hour to run SpellChecker for Training Data","90a0c458":"<a id=2.3 ><\/a>\n   ## <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:140%; text-align:left; border-radius: 20px 50px;\">2.3 Spelling Checker<\/p>\n   \nSpend so much time to apply Spelling Checker.\n\nSo I will use commited-dataset (df_train\/df_test) applied and saved in the last time) to save time.\n\n[Content](#0)","69736ebe":"<a id='1'><\/a>\n# <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:150%; text-align:center; border-radius: 15px 50px;\">1. Loading Data \ud83d\udc8e<\/p>\n\nJust load the dataset and global variables for colors and so on.\n\n[Content](#0)","f8909cf9":"<a id=2.2 ><\/a>\n## <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:140%; text-align:left; border-radius: 15px 50px;\">2.2 Cleaning text<\/p>\n\n\n[Content](#0)","be9a4d5a":"<a id='2'><\/a>\n# <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:150%; text-align:center; border-radius: 15px 50px;\">2. Data Pre-processing <\/p>\n\nNow we are going to engineering the data to make it easier for the model to clasiffy.\n\nThis section is very important to reduce the dimensions of the problem.\n\n\n\n\n[Content](#0)","aa80bfb4":"<a id=2.1 ><\/a>\n## <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:140%; text-align:left; border-radius: 15px 50px;\">2.1 Remove 92 duplicated Rows<\/p>\n\n\n[Content](#0)","bc327dd4":"<a id=2.4 ><\/a>\n## <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:140%; text-align:left; border-radius: 15px 50px;\">2.4 Stemming<\/p>\n\n\n[Content](#0)","510a952b":"<a id=0><\/a>\n## <p style=\"background-color:lightblue; font-family:newtimeroman; font-size:120%; text-align:left; border-radius: 15px 50px;\">Table of Content<\/p>\n* [1. Loading Data \ud83d\udc8e](#1)\n* [2. Data Preprocessing](#2)\n    * [2.1 Remove 92 duplicated rows](#2.1)\n    * [2.2 Cleaning text](#2.2)\n    * [2.3 Spelling Checker](#2.3)\n    * [2.4 Remove Stemming](#2.4)\n* [3. Saving Data](#3)","82566905":"# In this kernel, I use **dataprep** library to clean data productively. \n    1. fillna: Replace all null values with NaN.\n\n    2. lowercase: Convert all characters to lowercase.\n\n    3. remove_digits: Remove numbers.\n\n    4. remove_html Remove HTML tags.\n\n    5. remove_urls: Remove URLs.\n\n    6. remove_punctuation: Remove punctuation marks. (do not use)\n\n    7. remove_accents: Remove accent marks. \n\n    8. remove_stopwords: Remove stopwords. (do not use)\n\n    9. remove_whitespace: Remove extra spaces, and tabs and newlines."}}