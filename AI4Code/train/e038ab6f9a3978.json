{"cell_type":{"e5d30a4e":"code","9121a92a":"code","c569c808":"code","96b71bbf":"code","cb8a1234":"code","766b74b4":"code","3228a110":"code","d28a2637":"code","c216f6c6":"code","c4698365":"code","ca5115a3":"code","d48086e1":"code","6c697045":"code","2a750f8d":"code","c36dd40b":"code","3b055bd8":"code","90d8b666":"code","b48afe2e":"code","a68d9080":"code","1dc2c37d":"code","c9b12112":"code","0b227c7f":"code","705faa51":"code","14a3ab9a":"code","54f59e8e":"code","a4fa7e22":"code","58b37c85":"code","5715b146":"code","d06006bc":"code","e34d2e1f":"code","c14515d1":"code","466bd874":"code","9e649b1f":"code","9492bd5d":"code","9582deec":"code","f2a98f69":"code","217c942f":"code","7712cbc7":"code","a58fd650":"code","c6e318fb":"code","311e68ae":"code","d920f827":"code","7043e11c":"code","fc38b8a7":"code","0e72329a":"code","35c9d567":"code","4cfa7764":"code","d04a9f8e":"code","9c11dba2":"code","257e69de":"code","2f46e8c1":"code","68a1239f":"code","63109ebf":"code","6fc85d3c":"code","136d4fe3":"code","1f9ebeab":"code","ca9a5389":"code","af2b5533":"code","6d3fb0fe":"code","15390cc9":"code","d32649a0":"code","cc245da3":"code","0909faed":"code","40e60e16":"code","166f5869":"code","cf419f05":"code","c717d882":"code","a8313728":"code","6197c5b2":"code","9d6e74c5":"code","cf3ef340":"code","48439bf6":"code","7db64f86":"code","524ec58e":"code","141f63a6":"code","8390dbd0":"code","dea79594":"code","55df5a31":"code","56cc6baa":"code","544401e1":"code","972b4368":"code","e80043a5":"code","5a4aee7b":"code","30855281":"code","a1f90ced":"code","addd0d94":"code","21918b14":"code","d184f9ab":"code","9731674f":"code","ec53012d":"code","6f8b76b7":"code","1732fa5e":"code","48ed20a0":"code","82e7e6da":"code","a451c37d":"code","3735f381":"code","dce238bd":"code","6c7999ac":"code","9931b81c":"code","ec84d029":"code","2a04865b":"code","6956f93f":"code","50fd34f9":"code","361d22b1":"code","2800328b":"code","b71c4e1f":"code","d75b1186":"code","e9e1daee":"code","04758e38":"code","7fcee3c5":"code","6309eea4":"code","b358fedd":"code","a626621b":"code","c0f88b34":"code","2b1ab0ee":"code","1329b29d":"markdown","4f1492c3":"markdown","e452473b":"markdown","131dcddb":"markdown","040e0deb":"markdown","8e8d2509":"markdown","c6acd04d":"markdown","44db3439":"markdown","14af6102":"markdown","f8446f51":"markdown","adb147c6":"markdown","aa3f1289":"markdown","48a9c338":"markdown","f084f97c":"markdown","dc599f4f":"markdown","f3a82679":"markdown","43a55eae":"markdown","d99038d4":"markdown","0083c8c6":"markdown","16d01636":"markdown","6d8373e6":"markdown","286f81d9":"markdown","f7c89cea":"markdown","3c0696e5":"markdown","c423f52f":"markdown","1d53be07":"markdown","c5bc3744":"markdown","6a9402ea":"markdown","7a53b042":"markdown","53da3567":"markdown","e7154c63":"markdown","25e24f20":"markdown","27764eb2":"markdown","0f707036":"markdown","74914054":"markdown","ee8fb1a2":"markdown","07a54654":"markdown","454e9fbf":"markdown","08590045":"markdown","7073e2bc":"markdown","7d165715":"markdown","684c7b3d":"markdown","d09b0efa":"markdown","5b64bada":"markdown","3b8b7ea0":"markdown","3d7600ce":"markdown","ee5f62bf":"markdown","7f7bf65c":"markdown","caad20d3":"markdown","7de46236":"markdown","8d932994":"markdown","0f0a54af":"markdown","0829bba6":"markdown","78affd25":"markdown","b3f0b770":"markdown","97c64748":"markdown","918fab12":"markdown","3c817cc4":"markdown","f441f494":"markdown","052009de":"markdown","022505f1":"markdown","a287d7cc":"markdown","6e8fb2d3":"markdown","a4ccf1ca":"markdown","a137a1af":"markdown","bdf5b387":"markdown","5ae47213":"markdown","f910efb2":"markdown","b242cb2a":"markdown","84655587":"markdown","1d75e39f":"markdown","05fe7cf6":"markdown","b624a5a5":"markdown","5471d984":"markdown","a43abdfd":"markdown","b937a3a8":"markdown","ff59b34e":"markdown","b55b8c1a":"markdown","040a6021":"markdown","51d0d231":"markdown","58403868":"markdown","cf8f3cc2":"markdown","bfc51bd5":"markdown","d2d832fa":"markdown","5766dcc2":"markdown","08a1ba04":"markdown","63f239d4":"markdown","8deb61d2":"markdown","bd797949":"markdown","605b9767":"markdown","26d58626":"markdown","974b5119":"markdown","cb57651a":"markdown","b219eb73":"markdown","d998b23d":"markdown","addeb778":"markdown","15b63ed3":"markdown","954a31cb":"markdown","fb43bd11":"markdown","f97f0a2d":"markdown","884cd035":"markdown","5664c042":"markdown","561deeac":"markdown","b84adc71":"markdown","af81c5a3":"markdown","dbbc2bdf":"markdown","0f4d7fbd":"markdown","2452c379":"markdown","dbc608c2":"markdown","92a30da9":"markdown","b8aca005":"markdown","f15abab8":"markdown","0d0b9229":"markdown","1ae5a777":"markdown","21c586c7":"markdown","a34be497":"markdown","1fbbf577":"markdown","b270633d":"markdown","465beb5b":"markdown","99581a00":"markdown","0ad07bf9":"markdown","8f407ab1":"markdown","21662e7f":"markdown","4d160fe2":"markdown","7026d84a":"markdown","c53ccdb4":"markdown","9a997412":"markdown","17238eef":"markdown","6dc558db":"markdown","8e99567f":"markdown","b31cedf8":"markdown","00438dc3":"markdown","d0df99cd":"markdown","a1d087da":"markdown","0f037f9e":"markdown","07f795fc":"markdown","a8b8090f":"markdown","53c19873":"markdown","621b2806":"markdown","5aa5dce7":"markdown","95c6fbea":"markdown","edf55947":"markdown","ed771bee":"markdown","f9786b1d":"markdown","a23d3982":"markdown","9f07ddb6":"markdown","7e99c528":"markdown","98120802":"markdown","58bd53f2":"markdown","a97bc497":"markdown","46638412":"markdown","3c6115e3":"markdown","797a3271":"markdown","9f61fad5":"markdown","9fa8c564":"markdown","f353f73a":"markdown","6f0d1e45":"markdown","cc3cfb24":"markdown","0dfbc48a":"markdown","d444a50d":"markdown","ef80fb16":"markdown","b8ae1396":"markdown","f40b5de3":"markdown","68cf342a":"markdown","8c63e1df":"markdown","1026ee74":"markdown","c8cc92a4":"markdown","06a01ddf":"markdown","e03cbe92":"markdown","634347e6":"markdown","352764af":"markdown","2f7c3a7a":"markdown","a24e64af":"markdown","eefab8ef":"markdown","480c8b69":"markdown","d2be6970":"markdown","c3fa301e":"markdown","8574732a":"markdown","45820a03":"markdown","3e3c082c":"markdown","73f788ed":"markdown","af25e6a0":"markdown"},"source":{"e5d30a4e":"#%matplotlib inline\n\n# for seaborn issue:\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport pandas as pd\nimport numpy as np\nfrom scipy import stats\nimport sklearn as sk\nimport itertools\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\nimport seaborn as sns\nfrom statsmodels.graphics.mosaicplot import mosaic\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.linear_model import Perceptron\nfrom sklearn import tree\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn import svm\nimport xgboost as xgb\nimport lightgbm as lgb\nfrom mlxtend.classifier import StackingClassifier\n\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\n\nsns.set(style='white', context='notebook', palette='deep')","9121a92a":"train = pd.read_csv(\"..\/input\/train.csv\")\ntest = pd.read_csv(\"..\/input\/test.csv\")\ncombine = pd.concat([train.drop('Survived',1),test])","c569c808":"train.head(8)","96b71bbf":"train.describe()","cb8a1234":"print(train.isnull().sum())\nprint(test.info())","766b74b4":"surv = train[train['Survived']==1]\nnosurv = train[train['Survived']==0]\nsurv_col = \"blue\"\nnosurv_col = \"red\"\n\nprint(\"Survived: %i (%.1f percent), Not Survived: %i (%.1f percent), Total: %i\"\\\n      %(len(surv), 1.*len(surv)\/len(train)*100.0,\\\n        len(nosurv), 1.*len(nosurv)\/len(train)*100.0, len(train)))","3228a110":"warnings.filterwarnings(action=\"ignore\")\nplt.figure(figsize=[12,10])\nplt.subplot(331)\nsns.distplot(surv['Age'].dropna().values, bins=range(0, 81, 1), kde=False, color=surv_col)\nsns.distplot(nosurv['Age'].dropna().values, bins=range(0, 81, 1), kde=False, color=nosurv_col,\n            axlabel='Age')\nplt.subplot(332)\nsns.barplot('Sex', 'Survived', data=train)\nplt.subplot(333)\nsns.barplot('Pclass', 'Survived', data=train)\nplt.subplot(334)\nsns.barplot('Embarked', 'Survived', data=train)\nplt.subplot(335)\nsns.barplot('SibSp', 'Survived', data=train)\nplt.subplot(336)\nsns.barplot('Parch', 'Survived', data=train)\nplt.subplot(337)\nsns.distplot(np.log10(surv['Fare'].dropna().values+1), kde=False, color=surv_col)\nsns.distplot(np.log10(nosurv['Fare'].dropna().values+1), kde=False, color=nosurv_col,axlabel='Fare')\nplt.subplots_adjust(top=0.92, bottom=0.08, left=0.10, right=0.95, hspace=0.25,\n                    wspace=0.35)\n\nprint(\"Median age survivors: %.1f, Median age non-survivers: %.1f\"\\\n      %(np.median(surv['Age'].dropna()), np.median(nosurv['Age'].dropna())))","d28a2637":"tab = pd.crosstab(train['SibSp'], train['Survived'])\nprint(tab)\n#dummy = tab.div(tab.sum(1).astype(float), axis=0).plot(kind=\"bar\", \n#                         stacked=True, color=[nosurv_col,surv_col])\n#dummy = plt.xlabel('SibSp')\n#dummy = plt.ylabel('Percentage')","c216f6c6":"stats.binom_test(x=5,n=5,p=0.62)","c4698365":"print(\"We know %i of %i Cabin numbers in the training data set and\"\n      %(len(train['Cabin'].dropna()), len(train)))\nprint(\"we know %i of %i Cabin numbers in the testing data set.\"\n      %(len(test['Cabin'].dropna()), len(test)))\ntrain.loc[:,['Survived','Cabin']].dropna().head(8)","ca5115a3":"print(\"There are %i unique ticket numbers among the %i tickets.\" \\\n      %(train['Ticket'].nunique(),train['Ticket'].count()))","d48086e1":"grouped = train.groupby('Ticket')\nk = 0\nfor name, group in grouped:\n    if (len(grouped.get_group(name)) > 1):\n        print(group.loc[:,['Survived','Name', 'Fare']])\n        k += 1\n    if (k>10):\n        break","6c697045":"plt.figure(figsize=(14,12))\nfoo = sns.heatmap(train.drop('PassengerId',axis=1).corr(), vmax=0.6, square=True, annot=True)","2a750f8d":"cols = ['Survived','Pclass','Age','SibSp','Parch','Fare']\ng = sns.pairplot(data=train.dropna(), vars=cols, size=1.5,\n                 hue='Survived', palette=[nosurv_col,surv_col])\ng.set(xticklabels=[])","c36dd40b":"msurv = train[(train['Survived']==1) & (train['Sex']==\"male\")]\nfsurv = train[(train['Survived']==1) & (train['Sex']==\"female\")]\nmnosurv = train[(train['Survived']==0) & (train['Sex']==\"male\")]\nfnosurv = train[(train['Survived']==0) & (train['Sex']==\"female\")]\n\nplt.figure(figsize=[13,5])\nplt.subplot(121)\nsns.distplot(fsurv['Age'].dropna().values, bins=range(0, 81, 1), kde=False, color=surv_col)\nsns.distplot(fnosurv['Age'].dropna().values, bins=range(0, 81, 1), kde=False, color=nosurv_col,\n            axlabel='Female Age')\nplt.subplot(122)\nsns.distplot(msurv['Age'].dropna().values, bins=range(0, 81, 1), kde=False, color=surv_col)\nsns.distplot(mnosurv['Age'].dropna().values, bins=range(0, 81, 1), kde=False, color=nosurv_col,\n            axlabel='Male Age')","3b055bd8":"#foo = combine['Age'].hist(by=combine['Pclass'], bins=np.arange(0,81,1),\n#                          layout=[3,1], sharex=True, figsize=[8,12])\n\n#foo = sns.boxplot(x=\"Pclass\", y=\"Age\", hue=\"Survived\", data=train)\n\n#sns.violinplot(x=\"Pclass\", y=\"Age\", data=combine, inner=None)\n#sns.swarmplot(x=\"Pclass\", y=\"Age\", data=combine, color=\"w\", alpha=.5)\n\nsns.violinplot(x=\"Pclass\", y=\"Age\", hue=\"Survived\", data=train, split=True)\nplt.hlines([0,10], xmin=-1, xmax=3, linestyles=\"dotted\")","90d8b666":"dummy = mosaic(train,[\"Survived\",\"Sex\",\"Pclass\"])","b48afe2e":"g = sns.factorplot(x=\"Pclass\", y=\"Survived\", hue=\"Sex\", col=\"Embarked\",\n                   data=train, aspect=0.9, size=3.5, ci=95.0)\n\n# for some reason in this plot the colours for m\/f are flipped:\n#grid = sns.FacetGrid(train, col='Embarked', size=2.2, aspect=1.6)\n#grid.map(sns.pointplot, 'Pclass', 'Survived', 'Sex', ci=95.0, palette='deep')\n#grid.add_legend()","a68d9080":"tab = pd.crosstab(combine['Embarked'], combine['Pclass'])\nprint(tab)\ndummy = tab.div(tab.sum(1).astype(float), axis=0).plot(kind=\"bar\", stacked=True)\ndummy = plt.xlabel('Port embarked')\ndummy = plt.ylabel('Percentage')","1dc2c37d":"sns.barplot(x=\"Embarked\", y=\"Survived\", hue=\"Pclass\", data=train)","c9b12112":"tab = pd.crosstab(combine['Embarked'], combine['Sex'])\nprint(tab)\ndummy = tab.div(tab.sum(1).astype(float), axis=0).plot(kind=\"bar\", stacked=True)\ndummy = plt.xlabel('Port embarked')\ndummy = plt.ylabel('Percentage')","0b227c7f":"tab = pd.crosstab(combine['Pclass'], combine['Sex'])\nprint(tab)\ndummy = tab.div(tab.sum(1).astype(float), axis=0).plot(kind=\"bar\", stacked=True)\ndummy = plt.xlabel('Pclass')\ndummy = plt.ylabel('Percentage')","705faa51":"sib = pd.crosstab(train['SibSp'], train['Sex'])\nprint(sib)\ndummy = sib.div(sib.sum(1).astype(float), axis=0).plot(kind=\"bar\", stacked=True)\ndummy = plt.xlabel('Siblings')\ndummy = plt.ylabel('Percentage')\n\nparch = pd.crosstab(train['Parch'], train['Sex'])\nprint(parch)\ndummy = parch.div(parch.sum(1).astype(float), axis=0).plot(kind=\"bar\", stacked=True)\ndummy = plt.xlabel('Parent\/Children')\ndummy = plt.ylabel('Percentage')","14a3ab9a":"sns.violinplot(x=\"Embarked\", y=\"Age\", hue=\"Survived\", data=train, split=True)\nplt.hlines([0,10], xmin=-1, xmax=3, linestyles=\"dotted\")","54f59e8e":"plt.figure(figsize=[12,10])\nplt.subplot(311)\nax1 = sns.distplot(np.log10(surv['Fare'][surv['Pclass']==1].dropna().values+1), kde=False, color=surv_col)\nax1 = sns.distplot(np.log10(nosurv['Fare'][nosurv['Pclass']==1].dropna().values+1), kde=False, color=nosurv_col,axlabel='Fare')\nax1.set_xlim(0,np.max(np.log10(train['Fare'].dropna().values)))\nplt.subplot(312)\nax2 = sns.distplot(np.log10(surv['Fare'][surv['Pclass']==2].dropna().values+1), kde=False, color=surv_col)\nax2 = sns.distplot(np.log10(nosurv['Fare'][nosurv['Pclass']==2].dropna().values+1), kde=False, color=nosurv_col,axlabel='Fare')\nax2.set_xlim(0,np.max(np.log10(train['Fare'].dropna().values)))\nplt.subplot(313)\nax3 = sns.distplot(np.log10(surv['Fare'][surv['Pclass']==3].dropna().values+1), kde=False, color=surv_col)\nax3 = sns.distplot(np.log10(nosurv['Fare'][nosurv['Pclass']==3].dropna().values+1), kde=False, color=nosurv_col,axlabel='Fare')\nax3.set_xlim(0,np.max(np.log10(train['Fare'].dropna().values)))\nplt.subplots_adjust(top=0.92, bottom=0.08, left=0.10, right=0.95, hspace=0.25, wspace=0.35)","a4fa7e22":"ax = sns.boxplot(x=\"Pclass\", y=\"Fare\", hue=\"Survived\", data=train);\nax.set_yscale('log')","58b37c85":"print(train[train['Embarked'].isnull()])","5715b146":"combine.where((combine['Embarked'] !='Q') & (combine['Pclass'] < 1.5) & \\\n    (combine['Sex'] == \"female\")).groupby(['Embarked','Pclass','Sex','Parch','SibSp']).size()","d06006bc":"train['Embarked'].iloc[61] = \"C\"\ntrain['Embarked'].iloc[829] = \"C\"","e34d2e1f":"print(test[test['Fare'].isnull()])","c14515d1":"print(test[test['Fare'].isnull()])","466bd874":"test['Fare'].iloc[152] = combine['Fare'][combine['Pclass'] == 3].dropna().median()\nprint(test['Fare'].iloc[152])","9e649b1f":"combine = pd.concat([train.drop('Survived',1),test])\nsurvived = train['Survived']\n\ncombine['Child'] = combine['Age']<=10\ncombine['Cabin_known'] = combine['Cabin'].isnull() == False\ncombine['Age_known'] = combine['Age'].isnull() == False\ncombine['Family'] = combine['SibSp'] + combine['Parch']\ncombine['Alone']  = (combine['SibSp'] + combine['Parch']) == 0\ncombine['Large_Family'] = (combine['SibSp']>2) | (combine['Parch']>3)\ncombine['Deck'] = combine['Cabin'].str[0]\ncombine['Deck'] = combine['Deck'].fillna(value='U')\ncombine['Ttype'] = combine['Ticket'].str[0]\ncombine['Title'] = combine['Name'].str.split(\", \", expand=True)[1].str.split(\".\", expand=True)[0]\ncombine['Fare_cat'] = pd.DataFrame(np.floor(np.log10(combine['Fare'] + 1))).astype('int')\ncombine['Bad_ticket'] = combine['Ttype'].isin(['3','4','5','6','7','8','A','L','W'])\ncombine['Young'] = (combine['Age']<=30) | (combine['Title'].isin(['Master','Miss','Mlle']))\ncombine['Shared_ticket'] = np.where(combine.groupby('Ticket')['Name'].transform('count') > 1, 1, 0)\ncombine['Ticket_group'] = combine.groupby('Ticket')['Name'].transform('count')\ncombine['Fare_eff'] = combine['Fare']\/combine['Ticket_group']\ncombine['Fare_eff_cat'] = np.where(combine['Fare_eff']>16.0, 2, 1)\ncombine['Fare_eff_cat'] = np.where(combine['Fare_eff']<8.5,0,combine['Fare_eff_cat'])\ntest = combine.iloc[len(train):]\ntrain = combine.iloc[:len(train)]\ntrain['Survived'] = survived\n\nsurv = train[train['Survived']==1]\nnosurv = train[train['Survived']==0]","9492bd5d":"g = sns.factorplot(x=\"Sex\", y=\"Survived\", hue=\"Child\", col=\"Pclass\",\n                   data=train, aspect=0.9, size=3.5, ci=95.0)\ntab = pd.crosstab(train['Child'], train['Pclass'])\nprint(tab)\ntab = pd.crosstab(train['Child'], train['Sex'])\nprint(tab)","9582deec":"cab = pd.crosstab(train['Cabin_known'], train['Survived'])\nprint(cab)\ndummy = cab.div(cab.sum(1).astype(float), axis=0).plot(kind=\"bar\", stacked=True)\ndummy = plt.xlabel('Cabin known')\ndummy = plt.ylabel('Percentage')","f2a98f69":"g = sns.factorplot(x=\"Sex\", y=\"Survived\", hue=\"Cabin_known\", col=\"Pclass\",\n                   data=train, aspect=0.9, size=3.5, ci=95.0)","217c942f":"tab = pd.crosstab(train['Deck'], train['Survived'])\nprint(tab)\ndummy = tab.div(tab.sum(1).astype(float), axis=0).plot(kind=\"bar\", stacked=True)\ndummy = plt.xlabel('Deck')\ndummy = plt.ylabel('Percentage')","7712cbc7":"stats.binom_test(x=12,n=12+35,p=24\/(24.+35.))","a58fd650":"g = sns.factorplot(x=\"Deck\", y=\"Survived\", hue=\"Sex\", col=\"Pclass\",\n                   data=train, aspect=0.9, size=3.5, ci=95.0)","c6e318fb":"print(train['Ttype'].unique())\nprint(test['Ttype'].unique())","311e68ae":"tab = pd.crosstab(train['Ttype'], train['Survived'])\nprint(tab)\nsns.barplot(x=\"Ttype\", y=\"Survived\", data=train, ci=95.0, color=\"blue\")","d920f827":"tab = pd.crosstab(train['Bad_ticket'], train['Survived'])\nprint(tab)\ng = sns.factorplot(x=\"Bad_ticket\", y=\"Survived\", hue=\"Sex\", col=\"Pclass\",\n                   data=train, aspect=0.9, size=3.5, ci=95.0)","7043e11c":"tab = pd.crosstab(train['Deck'], train['Bad_ticket'])\nprint(tab)\ndummy = tab.div(tab.sum(1).astype(float), axis=0).plot(kind=\"bar\", stacked=True)\ndummy = plt.xlabel('Deck')\ndummy = plt.ylabel('Percentage')","fc38b8a7":"tab = pd.crosstab(train['Age_known'], train['Survived'])\nprint(tab)\ndummy = tab.div(tab.sum(1).astype(float), axis=0).plot(kind=\"bar\", stacked=True)\ndummy = plt.xlabel('Age known')\ndummy = plt.ylabel('Percentage')","0e72329a":"stats.binom_test(x=424,n=424+290,p=125\/(125.+52.))","35c9d567":"g = sns.factorplot(x=\"Sex\", y=\"Age_known\", hue=\"Embarked\", col=\"Pclass\",\n                   data=train, aspect=0.9, size=3.5, ci=95.0)","4cfa7764":"tab = pd.crosstab(train['Family'], train['Survived'])\nprint(tab)\ndummy = tab.div(tab.sum(1).astype(float), axis=0).plot(kind=\"bar\", stacked=True)\ndummy = plt.xlabel('Family members')\ndummy = plt.ylabel('Percentage')","d04a9f8e":"tab = pd.crosstab(train['Alone'], train['Survived'])\nprint(tab)\nsns.barplot('Alone', 'Survived', data=train)","9c11dba2":"g = sns.factorplot(x=\"Sex\", y=\"Alone\", hue=\"Embarked\", col=\"Pclass\",\n                   data=train, aspect=0.9, size=3.5, ci=95.0)","257e69de":"tab = pd.crosstab(train['Large_Family'], train['Survived'])\nprint(tab)\nsns.barplot('Large_Family', 'Survived', data=train)","2f46e8c1":"g = sns.factorplot(x=\"Sex\", y=\"Large_Family\", col=\"Pclass\",\n                   data=train, aspect=0.9, size=3.5, ci=95.0)","68a1239f":"tab = pd.crosstab(train['Shared_ticket'], train['Survived'])\nprint(tab)\nsns.barplot('Shared_ticket', 'Survived', data=train)","63109ebf":"tab = pd.crosstab(train['Shared_ticket'], train['Sex'])\nprint(tab)\ng = sns.factorplot(x=\"Sex\", y=\"Shared_ticket\", hue=\"Embarked\", col=\"Pclass\",\n                   data=train, aspect=0.9, size=3.5, ci=95.0)","6fc85d3c":"print(combine['Age'].groupby(combine['Title']).count())\nprint(combine['Age'].groupby(combine['Title']).mean())\n\nprint(\"There are %i unique titles in total.\"%(len(combine['Title'].unique())))","136d4fe3":"dummy = combine[combine['Title'].isin(['Mr','Miss','Mrs','Master'])]\nfoo = dummy['Age'].hist(by=dummy['Title'], bins=np.arange(0,81,1))","1f9ebeab":"tab = pd.crosstab(train['Young'], train['Survived'])\nprint(tab)\nsns.barplot('Young', 'Survived', data=train)","ca9a5389":"tab = pd.crosstab(train['Young'], train['Pclass'])\nprint(tab)\ng = sns.factorplot(x=\"Sex\", y=\"Young\", col=\"Pclass\",\n                   data=train, aspect=0.9, size=3.5, ci=95.0)","af2b5533":"plt.figure(figsize=[12,10])\nplt.subplot(311)\nax1 = sns.distplot(np.log10(surv['Fare'][surv['Pclass']==1].dropna().values+1), kde=False, color=surv_col)\nax1 = sns.distplot(np.log10(nosurv['Fare'][nosurv['Pclass']==1].dropna().values+1), kde=False, color=nosurv_col,axlabel='Fare')\nax1.set_xlim(0,np.max(np.log10(train['Fare'].dropna().values+1)))\nplt.subplot(312)\nax2 = sns.distplot(np.log10(surv['Fare'][surv['Pclass']==2].dropna().values+1), kde=False, color=surv_col)\nax2 = sns.distplot(np.log10(nosurv['Fare'][nosurv['Pclass']==2].dropna().values+1), kde=False, color=nosurv_col,axlabel='Fare')\nax2.set_xlim(0,np.max(np.log10(train['Fare'].dropna().values+1)))\nplt.subplot(313)\nax3 = sns.distplot(np.log10(surv['Fare'][surv['Pclass']==3].dropna().values+1), kde=False, color=surv_col)\nax3 = sns.distplot(np.log10(nosurv['Fare'][nosurv['Pclass']==3].dropna().values+1), kde=False, color=nosurv_col,axlabel='Fare')\nax3.set_xlim(0,np.max(np.log10(train['Fare'].dropna().values+1)))\nplt.subplots_adjust(top=0.92, bottom=0.08, left=0.10, right=0.95, hspace=0.25, wspace=0.35)","6d3fb0fe":"pd.DataFrame(np.floor(np.log10(train['Fare'] + 1))).astype('int').head(5)","15390cc9":"tab = pd.crosstab(train['Fare_cat'], train['Survived'])\nprint(tab)\nsns.barplot('Fare_cat', 'Survived', data=train)","d32649a0":"g = sns.factorplot(x=\"Sex\", y=\"Fare_cat\", hue=\"Embarked\", col=\"Pclass\",\n                   data=train, aspect=0.9, size=3.5, ci=95.0)","cc245da3":"combine.groupby('Ticket')['Fare'].transform('std').hist()\nnp.sum(combine.groupby('Ticket')['Fare'].transform('std') > 0)","0909faed":"combine.iloc[np.where(combine.groupby('Ticket')['Fare'].transform('std') > 0)]","40e60e16":"plt.figure(figsize=[12,10])\nplt.subplot(311)\nax1 = sns.distplot(np.log10(surv['Fare_eff'][surv['Pclass']==1].dropna().values+1), kde=False, color=surv_col)\nax1 = sns.distplot(np.log10(nosurv['Fare_eff'][nosurv['Pclass']==1].dropna().values+1), kde=False, color=nosurv_col,axlabel='Fare')\nax1.set_xlim(0,np.max(np.log10(train['Fare_eff'].dropna().values+1)))\nplt.subplot(312)\nax2 = sns.distplot(np.log10(surv['Fare_eff'][surv['Pclass']==2].dropna().values+1), kde=False, color=surv_col)\nax2 = sns.distplot(np.log10(nosurv['Fare_eff'][nosurv['Pclass']==2].dropna().values+1), kde=False, color=nosurv_col,axlabel='Fare')\nax2.set_xlim(0,np.max(np.log10(train['Fare_eff'].dropna().values+1)))\nplt.subplot(313)\nax3 = sns.distplot(np.log10(surv['Fare_eff'][surv['Pclass']==3].dropna().values+1), kde=False, color=surv_col)\nax3 = sns.distplot(np.log10(nosurv['Fare_eff'][nosurv['Pclass']==3].dropna().values+1), kde=False, color=nosurv_col,axlabel='Fare')\nax3.set_xlim(0,np.max(np.log10(train['Fare_eff'].dropna().values+1)))\nplt.subplots_adjust(top=0.92, bottom=0.08, left=0.10, right=0.95, hspace=0.25, wspace=0.35)","166f5869":"print(combine[combine['Fare']>1].groupby('Pclass')['Fare'].std())\nprint(combine[combine['Fare_eff']>1].groupby('Pclass')['Fare_eff'].std())","cf419f05":"combine[(combine['Pclass']==1) & (combine['Fare_eff']>0) & (combine['Fare_eff']<10)]","c717d882":"combine[(combine['Pclass']==3) & (np.log10(combine['Fare_eff'])>1.2)]","a8313728":"ax = sns.boxplot(x=\"Pclass\", y=\"Fare_eff\", hue=\"Survived\", data=train)\nax.set_yscale('log')\nax.hlines([8.5,16],-1,4, linestyles='dashed')","6197c5b2":"tab = pd.crosstab(train['Fare_eff_cat'], train['Survived'])\nprint(tab)\nsns.barplot('Fare_eff_cat', 'Survived', data=train)","9d6e74c5":"g = sns.factorplot(x=\"Sex\", y=\"Fare_eff_cat\", hue=\"Embarked\", col=\"Pclass\",\n                   data=train, aspect=0.9, size=3.5, ci=95.0)","cf3ef340":"combine = pd.concat([train.drop('Survived',1),test])\nsurvived = train['Survived']\n\ncombine[\"Sex\"] = combine[\"Sex\"].astype(\"category\")\ncombine[\"Sex\"].cat.categories = [0,1]\ncombine[\"Sex\"] = combine[\"Sex\"].astype(\"int\")\ncombine[\"Embarked\"] = combine[\"Embarked\"].astype(\"category\")\ncombine[\"Embarked\"].cat.categories = [0,1,2]\ncombine[\"Embarked\"] = combine[\"Embarked\"].astype(\"int\")\ncombine[\"Deck\"] = combine[\"Deck\"].astype(\"category\")\ncombine[\"Deck\"].cat.categories = [0,1,2,3,4,5,6,7,8]\ncombine[\"Deck\"] = combine[\"Deck\"].astype(\"int\")\n\ntest = combine.iloc[len(train):]\ntrain = combine.iloc[:len(train)]\ntrain['Survived'] = survived\n\ntrain.loc[:,[\"Sex\",\"Embarked\"]].head()","48439bf6":"ax = plt.subplots( figsize =( 12 , 10 ) )\nfoo = sns.heatmap(train.drop('PassengerId',axis=1).corr(), vmax=1.0, square=True, annot=True)","7db64f86":"training, testing = train_test_split(train, test_size=0.2, random_state=0)\nprint(\"Total sample size = %i; training sample size = %i, testing sample size = %i\"\\\n     %(train.shape[0],training.shape[0],testing.shape[0]))","524ec58e":"cols = ['Sex','Pclass','Cabin_known','Large_Family','Parch',\n        'SibSp','Young','Alone','Shared_ticket','Child']\ntcols = np.append(['Survived'],cols)\n\ndf = training.loc[:,tcols].dropna()\nX = df.loc[:,cols]\ny = np.ravel(df.loc[:,['Survived']])","141f63a6":"clf_log = LogisticRegression()\nclf_log = clf_log.fit(X,y)\nscore_log = clf_log.score(X,y)\nprint(score_log)","8390dbd0":"pd.DataFrame(list(zip(X.columns, np.transpose(clf_log.coef_))))","dea79594":"cols = ['Sex','Pclass','Cabin_known','Large_Family','Shared_ticket','Young','Alone','Child']\ntcols = np.append(['Survived'],cols)\n\ndf = training.loc[:,tcols].dropna()\nX = df.loc[:,cols]\ny = np.ravel(df.loc[:,['Survived']])\n\ndf_test = testing.loc[:,tcols].dropna()\nX_test = df_test.loc[:,cols]\ny_test = np.ravel(df_test.loc[:,['Survived']])","55df5a31":"clf_log = LogisticRegression()\nclf_log = clf_log.fit(X,y)\nscore_log = cross_val_score(clf_log, X, y, cv=5).mean()\nprint(score_log)","56cc6baa":"clf_pctr = Perceptron(\n    class_weight='balanced'\n    )\nclf_pctr = clf_pctr.fit(X,y)\nscore_pctr = cross_val_score(clf_pctr, X, y, cv=5).mean()\nprint(score_pctr)","544401e1":"clf_knn = KNeighborsClassifier(\n    n_neighbors=10,\n    weights='distance'\n    )\nclf_knn = clf_knn.fit(X,y)\nscore_knn = cross_val_score(clf_knn, X, y, cv=5).mean()\nprint(score_knn)","972b4368":"clf_svm = svm.SVC(\n    class_weight='balanced'\n    )\nclf_svm.fit(X, y)\nscore_svm = cross_val_score(clf_svm, X, y, cv=5).mean()\nprint(score_svm)","e80043a5":"clf_bay = GaussianNB()\nclf_bay.fit(X,y)\nscore_bay = cross_val_score(clf_bay, X, y, cv=5).mean()\nprint(score_bay)","5a4aee7b":"bagging = BaggingClassifier(\n    KNeighborsClassifier(\n        n_neighbors=2,\n        weights='distance'\n        ),\n    oob_score=True,\n    max_samples=0.5,\n    max_features=1.0\n    )\nclf_bag = bagging.fit(X,y)\nscore_bag = clf_bag.oob_score_\nprint(score_bag)","30855281":"clf_tree = tree.DecisionTreeClassifier(\n    #max_depth=3,\\\n    class_weight=\"balanced\",\\\n    min_weight_fraction_leaf=0.01\\\n    )\nclf_tree = clf_tree.fit(X,y)\nscore_tree = cross_val_score(clf_tree, X, y, cv=5).mean()\nprint(score_tree)","a1f90ced":"clf_rf = RandomForestClassifier(\n    n_estimators=1000, \\\n    max_depth=None, \\\n    min_samples_split=10 \\\n    #class_weight=\"balanced\", \\\n    #min_weight_fraction_leaf=0.02 \\\n    )\nclf_rf = clf_rf.fit(X,y)\nscore_rf = cross_val_score(clf_rf, X, y, cv=5).mean()\nprint(score_rf)","addd0d94":"clf_ext = ExtraTreesClassifier(\n    max_features='auto',\n    bootstrap=True,\n    oob_score=True,\n    n_estimators=1000,\n    max_depth=None,\n    min_samples_split=10\n    #class_weight=\"balanced\",\n    #min_weight_fraction_leaf=0.02\n    )\nclf_ext = clf_ext.fit(X,y)\nscore_ext = cross_val_score(clf_ext, X, y, cv=5).mean()\nprint(score_ext)","21918b14":"import warnings\nwarnings.filterwarnings(\"ignore\")\n\nclf_gb = GradientBoostingClassifier(\n            #loss='exponential',\n            n_estimators=1000,\n            learning_rate=0.1,\n            max_depth=3,\n            subsample=0.5,\n            random_state=0).fit(X, y)\nclf_gb.fit(X,y)\nscore_gb = cross_val_score(clf_gb, X, y, cv=5).mean()\nprint(score_gb)","d184f9ab":"clf_ada = AdaBoostClassifier(n_estimators=400, learning_rate=0.1)\nclf_ada.fit(X,y)\nscore_ada = cross_val_score(clf_ada, X, y, cv=5).mean()\nprint(score_ada)","9731674f":"clf_xgb = xgb.XGBClassifier(\n    max_depth=2,\n    n_estimators=500,\n    subsample=0.5,\n    learning_rate=0.1\n    )\nclf_xgb.fit(X,y)\nscore_xgb = cross_val_score(clf_xgb, X, y, cv=5).mean()\nprint(score_xgb)","ec53012d":"clf_lgb = lgb.LGBMClassifier(\n    max_depth=2,\n    n_estimators=500,\n    subsample=0.5,\n    learning_rate=0.1\n    )\nclf_lgb.fit(X,y)\nscore_lgb = cross_val_score(clf_lgb, X, y, cv=5).mean()\nprint(score_lgb)","6f8b76b7":"clf_ext = ExtraTreesClassifier(max_features='auto',bootstrap=True,oob_score=True)\nparam_grid = { \"criterion\" : [\"gini\", \"entropy\"],\n              \"min_samples_leaf\" : [1, 5, 10],\n              \"min_samples_split\" : [8, 10, 12],\n              \"n_estimators\": [20, 50, 100]}\ngs = GridSearchCV(estimator=clf_ext, param_grid=param_grid, scoring='accuracy', cv=3)\ngs = gs.fit(X,y)\nprint(gs.best_score_)\nprint(gs.best_params_)","1732fa5e":"clf_ext = ExtraTreesClassifier(\n    max_features='auto',\n    bootstrap=True,\n    oob_score=True,\n    criterion='gini',\n    min_samples_leaf=5,\n    min_samples_split=8,\n    n_estimators=50\n    )\nclf_ext = clf_ext.fit(X,y)\nscore_ext = clf_ext.score(X,y)\nprint(score_ext)\npd.DataFrame(list(zip(X.columns, np.transpose(clf_ext.feature_importances_))) \\\n            ).sort_values(1, ascending=False)","48ed20a0":"# Taner's code\ndef show_confusion_matrix(cnf_matrix, class_labels):\n    plt.matshow(cnf_matrix,cmap=plt.cm.YlGn,alpha=0.7)\n    ax = plt.gca()\n    ax.set_xlabel('Predicted Label', fontsize=16)\n    ax.set_xticks(range(0,len(class_labels)))\n    ax.set_xticklabels(class_labels,rotation=45)\n    ax.set_ylabel('Actual Label', fontsize=16, rotation=90)\n    ax.set_yticks(range(0,len(class_labels)))\n    ax.set_yticklabels(class_labels)\n    ax.xaxis.set_label_position('top')\n    ax.xaxis.tick_top()\n\n    for row in range(len(cnf_matrix)):\n        for col in range(len(cnf_matrix[row])):\n            ax.text(col, row, cnf_matrix[row][col], va='center', ha='center', fontsize=16)\n\n# sklearn example code\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    print(cm)\n\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, cm[i, j],\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n\nclass_names = [\"Dead\", \"Alive\"]\ncnf_matrix = confusion_matrix(clf_ext.predict(X_test),y_test)\n\n# from: http:\/\/notmatthancock.github.io\/2015\/10\/28\/confusion-matrix.html\ndef show_confusion_matrix2(C,class_labels=['0','1']):\n    \"\"\"\n    C: ndarray, shape (2,2) as given by scikit-learn confusion_matrix function\n    class_labels: list of strings, default simply labels 0 and 1.\n\n    Draws confusion matrix with associated metrics.\n    \"\"\"\n    import matplotlib.pyplot as plt\n    import numpy as np\n    \n    assert C.shape == (2,2), \"Confusion matrix should be from binary classification only.\"\n    \n    # true negative, false positive, etc...\n    tn = C[0,0]; fp = C[0,1]; fn = C[1,0]; tp = C[1,1];\n\n    NP = fn+tp # Num positive examples\n    NN = tn+fp # Num negative examples\n    N  = NP+NN\n\n    fig = plt.figure(figsize=(8,8))\n    ax  = fig.add_subplot(111)\n    ax.imshow(C, interpolation='nearest', cmap=plt.cm.gray)\n\n    # Draw the grid boxes\n    ax.set_xlim(-0.5,2.5)\n    ax.set_ylim(2.5,-0.5)\n    ax.plot([-0.5,2.5],[0.5,0.5], '-k', lw=2)\n    ax.plot([-0.5,2.5],[1.5,1.5], '-k', lw=2)\n    ax.plot([0.5,0.5],[-0.5,2.5], '-k', lw=2)\n    ax.plot([1.5,1.5],[-0.5,2.5], '-k', lw=2)\n\n    # Set xlabels\n    ax.set_xlabel('Predicted Label', fontsize=16)\n    ax.set_xticks([0,1,2])\n    ax.set_xticklabels(class_labels + [''])\n    ax.xaxis.set_label_position('top')\n    ax.xaxis.tick_top()\n    # These coordinate might require some tinkering. Ditto for y, below.\n    ax.xaxis.set_label_coords(0.34,1.06)\n\n    # Set ylabels\n    ax.set_ylabel('True Label', fontsize=16, rotation=90)\n    ax.set_yticklabels(class_labels + [''],rotation=90)\n    ax.set_yticks([0,1,2])\n    ax.yaxis.set_label_coords(-0.09,0.65)\n\n\n    # Fill in initial metrics: tp, tn, etc...\n    ax.text(0,0,\n            'True Neg: %d\\n(Num Neg: %d)'%(tn,NN),\n            va='center',\n            ha='center',\n            bbox=dict(fc='w',boxstyle='round,pad=1'))\n\n    ax.text(0,1,\n            'False Neg: %d'%fn,\n            va='center',\n            ha='center',\n            bbox=dict(fc='w',boxstyle='round,pad=1'))\n\n    ax.text(1,0,\n            'False Pos: %d'%fp,\n            va='center',\n            ha='center',\n            bbox=dict(fc='w',boxstyle='round,pad=1'))\n\n\n    ax.text(1,1,\n            'True Pos: %d\\n(Num Pos: %d)'%(tp,NP),\n            va='center',\n            ha='center',\n            bbox=dict(fc='w',boxstyle='round,pad=1'))\n\n    # Fill in secondary metrics: accuracy, true pos rate, etc...\n    ax.text(2,0,\n            'False Pos Rate: %.2f'%(fp \/ (fp+tn+0.)),\n            va='center',\n            ha='center',\n            bbox=dict(fc='w',boxstyle='round,pad=1'))\n\n    ax.text(2,1,\n            'True Pos Rate: %.2f'%(tp \/ (tp+fn+0.)),\n            va='center',\n            ha='center',\n            bbox=dict(fc='w',boxstyle='round,pad=1'))\n\n    ax.text(2,2,\n            'Accuracy: %.2f'%((tp+tn+0.)\/N),\n            va='center',\n            ha='center',\n            bbox=dict(fc='w',boxstyle='round,pad=1'))\n\n    ax.text(0,2,\n            'Neg Pre Val: %.2f'%(1-fn\/(fn+tn+0.)),\n            va='center',\n            ha='center',\n            bbox=dict(fc='w',boxstyle='round,pad=1'))\n\n    ax.text(1,2,\n            'Pos Pred Val: %.2f'%(tp\/(tp+fp+0.)),\n            va='center',\n            ha='center',\n            bbox=dict(fc='w',boxstyle='round,pad=1'))\n\n\n    plt.tight_layout()\n    plt.show()","82e7e6da":"show_confusion_matrix(cnf_matrix,class_names)\n#show_confusion_matrix2(cnf_matrix,class_names)\n#plot_confusion_matrix(cnf_matrix, classes=class_names, normalize=True,\n#                     title='Normalized confusion matrix')\n#sns.heatmap(cnf_matrix, annot=True)","a451c37d":"clf = clf_ext\nscores = cross_val_score(clf, X, y, cv=5)\nprint(scores)\nprint(\"Mean score = %.3f, Std deviation = %.3f\"%(np.mean(scores),np.std(scores)))","3735f381":"score_ext_test = clf_ext.score(X_test,y_test)\nprint(score_ext_test)","dce238bd":"models = pd.DataFrame({\n    'Model': ['Support Vector Machines', 'KNN', 'Logistic Regression', \n              'Random Forest', 'Gradient Boosting', 'Bagging KNN', \n              'Decision Tree','XGBoost','LightGBM','ExtraTree','Perceptron', 'Naive Bayes'],\n    'Score': [score_svm, score_knn, score_log, score_rf, score_gb, score_bag,\n              score_tree,score_xgb,score_lgb,score_ext,score_pctr, score_bay]})\nmodels.sort_values(by='Score', ascending=False)","6c7999ac":"summary = pd.DataFrame(list(zip(X.columns, \\\n    np.transpose(clf_tree.feature_importances_), \\\n    np.transpose(clf_rf.feature_importances_), \\\n    np.transpose(clf_ext.feature_importances_), \\\n    np.transpose(clf_gb.feature_importances_), \\\n    np.transpose(clf_ada.feature_importances_), \\\n    np.transpose(clf_xgb.feature_importances_), \\\n    np.transpose(clf_lgb.feature_importances_), \\\n    )), columns=['Feature','Tree','RF','Extra','GB','Ada','XGBoost','LightGBM'])\n  \nsummary['Median'] = summary.median(1)\nsummary.sort_values('Median', ascending=False)","9931b81c":"clf_vote = VotingClassifier(\n    estimators=[\n        #('tree', clf_tree),\n        ('knn', clf_knn),\n        ('svm', clf_svm),\n        ('extra', clf_ext),\n       #('gb', clf_gb),\n        ('xgb', clf_xgb),\n        ('percep', clf_pctr),\n        ('logistic', clf_log),\n        #('RF', clf_rf),\n        ],\n    weights=[2,2,3,3,1,2],\n    voting='hard')\nclf_vote.fit(X,y)\n\nscores = cross_val_score(clf_vote, X, y, cv=5, scoring='accuracy')\nprint(\"Voting: Accuracy: %0.2f (+\/- %0.2f)\" % (scores.mean(), scores.std()))\n\n#for clf, label in zip(\n#    [clf_tree,clf_knn,clf_svm,clf_ext,clf_gb,clf_xgb,clf_pctr,clf_log,clf_rf,clf_bag,clf_vote],\n#    ['tree','knn','svm','extra','gb','xgb','percep','logistic','RF','Bag','Ensemble']):\n#    scores = cross_val_score(clf, X, y, cv=5, scoring='accuracy')\n#    print(\"Accuracy: %0.2f (+\/- %0.2f) [%s]\" % (scores.mean(), scores.std(), label))","ec84d029":"# adjust these methods to my notation:\ntrain = X\n\n# training and train\/test split parameters\nntrain = train.shape[0]\nntest = test.shape[0]\nSEED = 0 # for reproducibility\nNFOLDS = 5 # set folds for out-of-fold prediction\nkf = KFold(n_splits=NFOLDS, random_state=SEED)\n\n# Class to extend the Sklearn classifier; this basically unifies the way we call each classifier \nclass SklearnHelper(object):\n    def __init__(self, clf, seed=0, params=None):\n        params['random_state'] = seed\n        self.clf = clf(**params)\n\n    def train(self, x_train, y_train):\n        self.clf.fit(x_train, y_train)\n\n    def predict(self, x):\n        return self.clf.predict(x)\n    \n    def fit(self,x,y):\n        return self.clf.fit(x,y)","2a04865b":"# function for out-of-fold prediction\ndef get_oof(clf, x_train, y_train, x_test):\n    oof_train = np.zeros((ntrain,))\n    oof_test = np.zeros((ntest,))\n    oof_test_skf = np.empty((NFOLDS, ntest))\n    \n    # split data in NFOLDS training vs testing samples\n    for i, (train_index, test_index) in enumerate(kf.split(x_train)):\n        # select train and test sample\n        x_tr = x_train[train_index]\n        y_tr = y_train[train_index]\n        x_te = x_train[test_index]\n        \n        # train classifier on training sample\n        clf.train(x_tr, y_tr)\n        \n        # predict classifier for testing sample\n        oof_train[test_index] = clf.predict(x_te)\n        # predict classifier for original test sample\n        oof_test_skf[i, :] = clf.predict(x_test)\n    \n    # take the median of all NFOLD test sample predictions\n    # (changed from mean to preserve binary classification)\n    oof_test[:] = np.median(oof_test_skf,axis=0)\n    return oof_train.reshape(-1, 1), oof_test.reshape(-1, 1)","6956f93f":"# Put in our parameters for selected classifiers\n# Random Forest parameters\nrf_params = {\n    'n_estimators': 500,\n     'warm_start': True, \n     #'max_features': 0.2,\n    'max_depth': 6,\n    'min_samples_leaf': 2,\n    'max_features' : 'sqrt',\n}\n\n# Extra Trees Parameters\net_params = {\n    'n_estimators':500,\n    #'max_features': 0.5,\n    'max_depth': 8,\n    'min_samples_leaf': 2,\n}\n\n# AdaBoost parameters\nada_params = {\n    'n_estimators': 500,\n    'learning_rate' : 0.75\n}\n\n# Gradient Boosting parameters\ngb_params = {\n    'n_estimators': 500,\n     #'max_features': 0.2,\n    'max_depth': 5,\n    'min_samples_leaf': 2,\n}\n\n# Support Vector Classifier parameters \nsvc_params = {\n    'kernel' : 'linear',\n    'C' : 0.025\n    }","50fd34f9":"# Create objects for each classifier\nrf = SklearnHelper(clf=RandomForestClassifier, seed=SEED, params=rf_params)\net = SklearnHelper(clf=ExtraTreesClassifier, seed=SEED, params=et_params)\nada = SklearnHelper(clf=AdaBoostClassifier, seed=SEED, params=ada_params)\ngb = SklearnHelper(clf=GradientBoostingClassifier, seed=SEED, params=gb_params)\nsvc = SklearnHelper(clf=svm.SVC, seed=SEED, params=svc_params)","361d22b1":"# Create Numpy arrays of train, test and target dataframes to feed into our models\ny_train = y\ntrain = X\nfoo = test.loc[:,cols]\nx_train = train.values \nx_test = foo.values","2800328b":"# Create our OOF train and test predictions. These base results will be used as new features\net_oof_train, et_oof_test = get_oof(et, x_train, y_train, x_test) # Extra Trees\nrf_oof_train, rf_oof_test = get_oof(rf,x_train, y_train, x_test) # Random Forest\nada_oof_train, ada_oof_test = get_oof(ada, x_train, y_train, x_test) # AdaBoost \ngb_oof_train, gb_oof_test = get_oof(gb,x_train, y_train, x_test) # Gradient Boost\nsvc_oof_train, svc_oof_test = get_oof(svc,x_train, y_train, x_test) # Support Vector Classifier\n\nprint(\"Training is complete\")","b71c4e1f":"base_predictions_train = pd.DataFrame( {'RandomForest': rf_oof_train.ravel(),\n     'ExtraTrees': et_oof_train.ravel(),\n     'AdaBoost': ada_oof_train.ravel(),\n      'SVM' : svc_oof_train.ravel(),\n      'GradientBoost': gb_oof_train.ravel()\n    })\nbase_predictions_train.head()","d75b1186":"plt.figure(figsize=(12,10))\nfoo = sns.heatmap(base_predictions_train.corr(), vmax=1.0, square=True, annot=True)","e9e1daee":"x_train = np.concatenate(( et_oof_train, rf_oof_train, ada_oof_train, gb_oof_train, svc_oof_train), axis=1)\nx_test = np.concatenate(( et_oof_test, rf_oof_test, ada_oof_test, gb_oof_test, svc_oof_test), axis=1)","04758e38":"x_train","7fcee3c5":"clf_stack = xgb.XGBClassifier(\n    #learning_rate = 0.02,\n n_estimators= 2000,\n max_depth= 4,\n min_child_weight= 2,\n #gamma=1,\n gamma=0.9,                        \n subsample=0.8,\n colsample_bytree=0.8,\n objective= 'binary:logistic',\n scale_pos_weight=1)\nclf_stack = clf_stack.fit(x_train, y_train)\nstack_pred = clf_stack.predict(x_test)","6309eea4":"scores = cross_val_score(clf_stack, x_train, y_train, cv=5)\nprint(scores)\nprint(\"Mean score = %.3f, Std deviation = %.3f\"%(np.mean(scores),np.std(scores)))","b358fedd":"clf = clf_vote\ndf2 = test.loc[:,cols].fillna(method='pad')\nsurv_pred = clf.predict(df2)","a626621b":"submit = pd.DataFrame({'PassengerId' : test.loc[:,'PassengerId'],\n                       #'Survived': surv_pred.T})\n                       'Survived': stack_pred.T})\nsubmit.to_csv(\"..\/working\/submit.csv\", index=False)\n#submit.to_csv(\"submit.csv\", index=False)","c0f88b34":"submit.head()","2b1ab0ee":"submit.shape","1329b29d":"*Perceptron:* This is a binary classifier that creates a linear decision boundary based on a (hyper-) plane in the parameter space.\n\n[Source](https:\/\/en.wikipedia.org\/wiki\/Perceptron)","4f1492c3":"After studying the relations between the different features let's fill in a few missing values based on what we learned.\n\nIn my opinion, the only training feature for which it makes sense to fill in the NAs is *Embarked*. Too many *Cabin* numbers are missing. And for *Age* we will choose a different approach below. We fill in the 1 missing *Fare* value in the test data frame accordingly.\n\nLet's find the two passengers and assign the most likely port based on what we found so far:","e452473b":"**Naive Bayes**","131dcddb":"*Violin plots* are a modified version of boxplots, where the shape is a \"kernel density estimate\" of the underlying distribution. These estimates are smoothed and therefore extend beyond the actual values (look closely at the dotted zero level). I have also indicated *Age == 10*, which we will use to define children (vs teenagers) in the engineering part below.\n\n**We learn:**\n\n- Age decreases progressively as Pclass decreases from 1st to 3rd\n- Most older passengers are 1st class, but very few children are. This conflates the impact of *Age* and *Pclass* on the survival chances.\n- In 1st class, younger adults had better survival chances than older ones.\n- Most children in 2nd class survived, and the majority in 3rd class did too.","040e0deb":"*eXtreme Gradient Boosting:* It's not just a good name for a band, but XGBoost was also the flavour of the month tool for kaggle competitions in 2016.","8e8d2509":"**We learn:**\n\n- For females the survival chances appear to be higher between 18 and 40, whereas for men in that age range the odds are flipped. This difference between 18-40 yr olds might be a better feature than *Sex* and *Age* by themselves.\n\n- Boys have proportional better survival chances than men, whereas girls have similar chances as women have. Rather small numbers, though. ","c6acd04d":"**Extremely Randomised Trees**","44db3439":"## *This is a work in progress. Comments and critical feedback are always welcome.*","14af6102":"## Outline:\n\n*(Note: the hyper links \"kind of\" work, in that they take you to the corresponding section but create a separate HTML page every time you click one of them. Plus, scrolling only works with arrow keys [at least for me on Firefox]. Feel free to try them out and let me know whether you find them useful.)*\n\n1. [Load Data and Modules](#load) (complete)\n1. [Initial Exploration](#explore) (complete)\n1. [Relations between features](#relations) (complete)\n1. [Missing values](#missing) (complete)\n1. [Derived (engineered) features](#derived) (largely complete)\n1. [Preparing for modelling](#encode) (complete)\n1. [Modelling](#model) (medium completeness; to be extended)\n1. [Preparing our prediction for submission](#submit) (complete)","f8446f51":"We might even be at a stage now where we can investigate the few outliers more in detail:","adb147c6":"In addition, there is some variation between the 1st class male passengers, but it doesn't look overly significant.","aa3f1289":"### *Fare\\_eff\\_cat*","48a9c338":"Just about formally significant (i.e. < 5%). It might be worth our while to include this feature in at least the initial stages of modelling to see how it performs.","f084f97c":"... there were more males among the 3rd class passengers. Possibly travelling alone?","dc599f4f":"# Python walk-through for Titanic data analysis","f3a82679":"We start with an **overview plot of the feature relations:** Here we show a *correlation matrix* for each numerical variable with all the other numerical variables. We excluded *PassengerID*, which is merely a row index. In the plot, stronger correlations have brighter colours in either red (positive correlation) or blue (negative correlation). The closer to white a colour is the weaker the correlation. ","43a55eae":"[Go to the top of the page](#top)","d99038d4":"TODO: Say something about the contributions and follow up with some ANOVA-like analysis","0083c8c6":"**K Nearest Neighbours:**","16d01636":"And that's quite expensive for a 3rd class ticket. Maybe these two actually shared a ticket \/ cabin and we have another transcription \/ data entry error? The ticket numbers are very similar and someone could easily write \"303\" instead of \"304\". Will we ever know? Maybe not. Does it matter much? Probably not.\n\nMore importantly, there is a reasonable argument to be made for this new *Fare_eff* feature to represent the actual fare better than the original feature. For once, it splits much cleaner between the *Pclasses*:","6d8373e6":"### *Cabin\\_known*","286f81d9":"# 3. Relations between features","f7c89cea":"However, we see again that a large part of this effect disappears once we control for *Sex* and *Pclass*. \n\n**We learn:** There remains a potential trend for males and for 3rd class passengers but the uncertainties are large. This feature should be tested in the modelling stage.","3c0696e5":"Anything above 0.05 is usually not significant and therefore solely based on these numbers we cannot say whether the SibSp = 5 sample behaves different than the rest.\n\nFor larger numbers of Parch we have 4 vs 0, 4 vs 1, and 1 vs 0. Just by themselves, the last two are definitely not impressive. Combining them into parch >= 4 gives us 9 vs 1 which is much better.\n\n**We learn:** parch >= 4 and sibsp >= 3 is bad. So is parch + sibsp = 0 (i.e. both 0). Parch in 1-3 and Sibsp in 1-2 is good.","c423f52f":" Passengers with more than 3 children+parents on board had low survival chances. However the corresponding number are not very large. For SibSp we have 15 vs 3, 5 vs 0, and 7 vs 0.\n\nRandom outcomes with 2 possibilities (like *heads or tails* when flipping a coin) follow the [binomial distribution](https:\/\/en.wikipedia.org\/wiki\/Binomial_distribution). We can use a *binomial test* to estimate the probability that 5 non-survivors out of a total 5 passengers with SibSp = 5 happened due to chance assuming the overall 38% survival chance for the entire sample.","1d53be07":"But again the sharing of tickets is more frequent with females and 1st class passengers. This is consistent with the other statistics that show that women were more likely to travel together with larger families.\n\n**We learn:** Several of these derived parameters are strongly correlated with *Sex* and *Pclass*. Whether there is actual signal in them that a model can use to improve the learning accuracy needs to be investigated.","c5bc3744":"Let's summarise briefly what we found in our data exploration:\n\n- sex and ticket class are the main factors\n\n- there seem to be additional impacts from:\n    - age: young men vs young women; (male) children\n    - relatives: parch = 1-3, sibsp = 1-2 (somewhat explained by sex but not completely)\n    - maybe the cabin deck, but not many are known\n\n- other apparent effects appear to be strongly connected to the sex\/class features:\n    - port of embarkation\n    - fare\n    - sharing a ticket\n    - large family\n    - travelling alone\n    - known cabin number\n    - known age","6a9402ea":"For additional insight we compare the *feature\\_importance* output of all the classifiers for which it exists:","7a53b042":"<a id='derived'><\/a>","53da3567":"### *Large\\_Family*","e7154c63":"That seems to be a hopeless variable at first because it just looks like random strings. But in these days, when you were travelling as a group\/family did everyone really get their own ticket? Let's find out how many unique ticket numbers there are:","25e24f20":"Based on this plot we define a new feature called *Bad\\_ticket* under which we collect all the ticket numbers that start with digits which suggest less than 25% survival (e.g. *4*, *5*, or *A*). We are aware that some of the survival fractions we see above are based on small number statistics (e.g. 2 vs 0 for *8*). It is well possible that some of our \"bad tickets\" are merely statistical fluctuations from the base survival rate of 38%.  The barplot shows mean survival fractions and the associated 95% confidence limits, which are large for the sparse samples.\n\nHowever, the significant difference between e.g. *1* and *3* (based on large enough numbers) suggests that this new feature could still contain some useful information. I think that without external information, which we are avoiding in this notebook, we can't do much better in trying to tie the ticket number to the survival statistics.\n\nOf course, it's not the tickets themselves that are \"bad\" for survival, but the possibility that the ticket numbers might encode certain areas of the ship that would have led to higher or lower survival chances.","27764eb2":"**We learn:**\nAgain, we find that having 1-3 family members works best for survival. This feature is a mix of *SibSp* and *Parch*, which increases the overall numbers we can work with, but might smooth out some more subtle effects.","0f707036":"**Perceptron**","74914054":"In our training data set about 60% of the passengers didn't survive. By flat out predicting that everyone in the testing data set died we would get a 60% accuracy. Let's try to do better than that.\n\nHere we also define a consistent colour scheme for the distinguishing between survived \/ not survived. This scheme will (soon) be used throughout this kernel.","ee8fb1a2":"We study the correlation of *Age* with *Pclass* using a *violin plot*, which is also split between survived (right half) and not survived (left half). Check out the other visualisations in your forked copy.","07a54654":"The minimum\/maxim values for pclass, age, sibsp, parch show us the range of these features. Also we see that there's quite a range in fares.","454e9fbf":"*Nearest Neighbours*: a non-parametric classifier that uses the training data closest to each test data point to classify it. *K* is simply the number of neighbours that are making the decision by majority vote. This is a simple yet powerful method that works well for irregular decision boundaries.\n\nImportant parameters:\n\n- n_neighbors: choosing the right *k* depends heavily on the data. Larger values suppress noise but smooth out decision boundaries. Default: 5.\n\n- weights: *uniform* assigns equal weight to each neighbour, whereas *distance* gives more weight to neighbours that are closer.\n\n[Source](http:\/\/scikit-learn.org\/stable\/modules\/neighbors.html)","08590045":"<a id='encode'><\/a>","7073e2bc":"Let's study the relation between *Fare* and *Pclass* in more detail:","7d165715":"With these optimised parameters let's have a look at the feature importance that this classifier gives us:","684c7b3d":"# 6. Preparing for modelling","d09b0efa":"## *Stacking \/ Ensemble methods*","5b64bada":"*Positive vs negative correlation* needs to be understood in terms of whether an increase in one feature leads to an increase (positive) or decrease (negative) in the correlated feature. Perfect correlation would have a correlation index of 1; perfect anti-correlation (= negative correlation) would have -1 (obviously each feature is perfectly correlated with itself; leading to the deep red diagonal). The upper right vs lower left triangle that make up this plot contain the same information, since the corresponding cells show the correlation coefficients of the same features. \n\nThe matrix gives us an overview as to which features are particularly interesting for our analysis. Both strongly positive or negative correlations with the *Survived* feature are valuable. Strong correlations between two other features would suggest that only one of them is necessary for our model (and including the other would in fact induce noise and potentially lead to over-fitting).\n\n**We learn:**\n\n- *Pclass* is somewhat correlated with *Fare* (1st class tickets would be more expensive than 3rd class ones)\n- *SibSp* and *Parch* are weakly correlated (large families would have high values for both; solo travellers would have zero for both)\n- *Pclass* already correlates with *Survived* in a noticeable way","3b8b7ea0":"<a id='load'><\/a>","3d7600ce":"**Support Vector Machine:**","ee5f62bf":"## *Model validation*","7f7bf65c":"But more men were travelling alone than women did. Especially among the 3rd class passengers. Also this feature should be evaluated in our modelling step, to see if it's still significant in the presence of the *Sex* feature.","caad20d3":"Travelling alone appears bad enough to be significant.","7de46236":"### *Missing values*","8d932994":"# 4. Filling in missing values","0f0a54af":"<a id='top'><\/a>","0829bba6":"Here we see that in the testing data set (based on our train\/test split) 12 people who survived were misclassified as dead, whereas 21 who died were misclassified as having survived. That is roughly 20% of the cases that were classified correctly. The confusion matrix plot would allow us to identify significant *imbalances* in our prediction between the false positives and the false negatives. For instance if the off-diagonal elements were 0 and 30. For our case there doesn't seem to be an imbalance.\n\nHere we use Taner's function and also include the \"official\" [sklearn example](http:\/\/scikit-learn.org\/stable\/auto_examples\/model_selection\/plot_confusion_matrix.html) for plotting confusion matrices. The latter one, which you can comment also includes the possibility to plot a *normalised* confusion matrix. In addition, we include the option to use a confusion matrix from [this website](http:\/\/notmatthancock.github.io\/2015\/10\/28\/confusion-matrix.html) which shows more information if we need it. Alternatively, you can use a seaborn heatmap for a quick and easy (but less pretty) plot. Just change the comment tags to switch between the options. Admittedly, 4 different ones are a bit of an overkill, but why not document what we found.","78affd25":"As suspected, it is more likely to know the cabin of a passenger who survived. This could be useful.","b3f0b770":"**Random Forest**","97c64748":"The easiest method to combine different classifiers is through a **Voting Classifier**. It does exactly what the name suggests: each individual classifier makes a certain prediction and then the *majority vote* is used for each row. This majority process can either give all individual votes the same importance or assign different weights to make some classifiers have more impact than others.\n\nVoting can be more powerful when used with weights, so that several weaker classifiers can only successfully vote against one\/two stronger ones if they consistently agree on a specific prediction. This is expected to increase the accuracy of the final prediction. Read more in the extensive [Kaggle Ensemble Guide](https:\/\/mlwave.com\/kaggle-ensembling-guide\/).\n\nBelow, we decide to assign different, somewhat arbitrary weight according to how we think each classifier performs. ","918fab12":"# 5. Derived (engineered) features","3c817cc4":"As far as I can see, there's still quite a bit of variation here.","f441f494":"*Extremely Randomised Trees* is an ensemble classifier similar to random forests. An additional randomness is introduced by selecting random thresholds for each feature and using the best-performing threshold.\n\nHere we also use an \"Out-of-bag score\" (*oob\\_score = True*). This means that we grow our trees from a sub-sample of the training sample (using bootstrapping: *boostrap = True*) and estimate the accuracy based on those entries that were not picked (i.e. \"left out of the bag\"). This gives us a better impression how robust our results are towards generalisation, i.e. how well the classifier that was trained on a particular sample can be applied to new data. \n\nBecause this is ultimately our goal: to apply the classification method we \"learn\" from the training data to any data (in particular the one that is used to judge this competition). There is little use in having a classifier that replicates perfectly the training data by following every random noise feature in that data (called *overfitting*) but doesn't perform well with new data.\n\nThe principles of *bootstrapping* and the *out-of-bag score* can be applied to most classifiers and we already used them in the *bagging* classifier above. Here we just focus a bit on the underlying idea.","052009de":"*Bagging* is a general ensemble method. This means it's a way to average over a (large) number of individual classifiers to improve their accuracy by reducing the variance (= noise). The estimator (above it's a KNN) is used multiple times on *subsets* of the training sample and then it uses the average vote.\n\nBagging for a decision tree classifier should be the same as using a *Random Forest* (see below).\n\nStrictly speaking, bagging is only the correct term if the sub samples are drawn with *replacement* (i.e. put back into the bag, I suppose). Otherwise it's called *Pasting*.\n\nIf sub-samples are used then the remaining samples (the ones not in the bag we're drawing the data from) can be used in *out-of-bag (oob)* estimates (-> *oob\\_score=True*). This is a kind of inbuilt cross-validation step, since the accuracy (score) of the classifier is estimated on data it wasn't trained on.\n\n[Source](http:\/\/scikit-learn.org\/stable\/modules\/ensemble.html#bagging-meta-estimator)","022505f1":"[Go to the top of the page](#top)","a287d7cc":"<a id='explore'><\/a>","6e8fb2d3":"*Decision Tree:* One of the classifiers that's easiest to visualise. Each tree is a series of if-then-else decisions. Example: *if* sex ==  male *then* go left *else* go right. Here, *left* and *right* defines a split at a so called *node* - the decision itself. The first split can be followed up by additional ones to narrow down the decision criteria (based on the subset defined by each previous split).\n\nOne visualisation of this process is a tree trunk *branching off* into successively smaller structures. Hence: decision *tree*. Consequently, the result of the final splits are called *leaf notes* - on a tree, it doesn't get smaller than leafs.\n\nAdvantages of decision trees are that they can deal with both numerical and categorical data, are able to handle multi-output problems, and are easy to follow and interpret.\n\nDisadvantages include:\n\n- Problem: A tendency to overfitting. Solution: pruning, setting maximum depth, or PCA beforehand to find the right number of features. Visualising the tree helps to understand how well it is fitting the data.\n\n- Problem: Unstable to small variations in the data. Solution: ensembles.\n\n- Problem: Creating biased trees if some classes dominate. Solution: balance the data set by either sampling the same number of samples from each class or by adjusting the *sample_weight* parameter to normalise the sum of the class weights to the same value. Following that, parameter *min_weight_fraction_leaf* is less biased towards dominating classes.\n\n- Problem: Being just not easy to fit to certain concepts that don't lend themselves to clear yes-or-no decisions. Solution: Use a different classifier.\n\nAdditional notes:\n\n- Parameters *min_samples_split* and *min_samples_leaf* control the number of samples at a leaf note. min_samples_leaf=5 is a useful initial value. A small number will lead to overfitting, a large number prevents learning.\n\n- For sparse X convert to sparse *csc_matrix* to speed up the learning\n\nAll of the information above is digested from the [sklearn documentation](http:\/\/scikit-learn.org\/stable\/modules\/tree.html)","a4ccf1ca":"### *Shared\\_ticket*","a137a1af":"**We learn:**\n\n- There is a broad distribution between the 1st class passenger fares (rich -> super rich)\n- There's an interesting bimodality in the 2nd class cabins and a long tail in the 3rd class ones. (*TODO: check cumulative fare question*)\n- For each class there is strong evidence that the cheaper cabins were worse for survival. A similar effect can be seen in a *boxplot*:","bdf5b387":"The curious distribution for the \"Q\" survivors somewhat follows the overall trend for 3rd class passengers (which make up the vast majority of \"Q\") but is notably narrower. Not many of the children there survived, but then there were not many children to begin with. Let's come back to this point in discussing the derived features.\n\n**We learn:**\nThere don't seem to be strong differences in *Age* among the *Embarked* categories that would point at an imbalance that goes beyond the influence of *Pclass* and *Sex*. ","5ae47213":"### *Ttype and Bad\\_ticket*","f910efb2":"Based on the first look we define the input columns we'll be working with. We also create our training and testing feature sets.","b242cb2a":"The *Pclass == 1* plot looks interesting at first, but there are only 3 children in this group which makes the apparent pattern just random noise. The other two passenger classes are more interesting, especially for the male children. Note, that since we are selecting by *Age*, which has many missing values, a number of children will be in the *Child == False* group. Nonetheless, this seems useful.\n\n**We learn:** Male children appear to have a survival advantage in 2nd and 3rd class. We should include the *Child* feature in our model testing.","84655587":"Before we start exploring the different models we are modifying the categorical string column types to integer. This is necessary since not all classifiers can handle string input.","1d75e39f":"**We can try out:**\n\nWorking hypothesis: if your group (mostly family) survived then you survived as well, unless you were a man (and presumably helped your wife\/daughter\/lady friend). We could go through the trouble here to identify families by last name. However\n\n 1. Common last names might not be unique in the passenger list\n 2. As we see above a ticket is not always shared by people with a common name.\n\nTherefore, a shared ticket might actually be a stronger predictor. Of course this assumption should be tested by doing the last-name thing too. In addition, we see that the *Fare* was identical for all the passengers in each ticket group. This is something we will explore in more detail below.","05fe7cf6":"### *Family*","b624a5a5":"What can we learn from the titles in the passenger names? These could give us a direct, independent way to estimate the missing age values, so let's look at all the available titles, their frequency, and mean age. For this, we look at the *combined* data set to make sure that we don't miss any titles that might be in *train* or *test* only:","5471d984":"For a final overview before the modelling stage we have another look at the correlation matrix between all old and new features:","a43abdfd":"**eXtreme Gradient Boosting - XGBoost:**","b937a3a8":"In addition, we plot a **Pairplot** of the numerical features. This kind of plot is a more detailed visualisation of relationships between variables. It shows scatter plots for the different feature combinations plus a distribution of each feature on the diagonal. Again, the upper right and lower left triangle contain the same information. This kind of plot is vastly more useful for a set of continuous variables, instead of the categorical or integer values we have here. Nonetheless, it is a valuable exploratory tool that has a place in everyone's toolbox.\n\nThis plot is inspired by, and realised much more aesthetically in, the [comprehensive Ensemble Stacking Kernel by Anisotropic](https:\/\/www.kaggle.com\/arthurtok\/introduction-to-ensembling-stacking-in-python)  ","ff59b34e":"Each of the individual classifiers we have used above has its strengths and weaknesses, and we should always choose the classifier that's best equipped to handle a certain problem and\/or has been found to perform with the highest accuracy. But wouldn't it be nice to combine all these different classifiers to get a more accurate overall prediction? This is possible through an approach called *Ensemble methods*. We have already encountered this strategy in our Random Forests or Bagging estimators above, where the aim was to get a more accurate estimate from combining multiple runs of a single classifier (like a Decision Tree; for instance).\n\nNow, we want to combine the results of *different kinds of classifiers* to improve our prediction.","b55b8c1a":"Coming soon: The next step will use the pre-packaged stacking classifier of the mlxtend package. ","040a6021":"*Support Vector Machine:* This classifier fits a (set of) hyper-plane(s) in the high-dimensional space of the training features so that this plane has the largest distance to any training data points. This is easy to visualise in 2 dimensions as e.g. 1 line that separates 2 classes (see the link below). In higher dimensions only mathematics can save you.\n\nThe *support vectors* are a subset of training data points used in the decision function. For unbalanced problems setting *class\\_weight='balanced'* might be helpful (compare decision tree notes).\n\nAdvantages: Effectiv in high dimensions and versatile with different kernel options.\n\n[Source](http:\/\/scikit-learn.org\/stable\/modules\/svm.html)","51d0d231":"Following a suggestion by [Taner](https:\/\/www.kaggle.com\/kiralt) in the comments we also use a *Confusion Matrix* to evaluate the performance of our classifier. A confusion matrix contains more information than a simple score because it shows how many data points of each class were correctly\/incorrectly classified. It's like a correlation matrix, in a sense. A plot will explain it better than 1000 words. First we define some plotting function; then we plot.","58403868":"# 2. Initial Exploration","cf8f3cc2":"Ok, now from here it looks more like \"S\" is the interesting port since survival is less probably for that one if you are a 3rd class passenger. Otherwise  there is no significant difference within each class.\n\nThere seems to be some impact here that isn't captured by the passenger class. What about the other strong feature, Sex?","bfc51bd5":"[Go to the top of the page](#top)","d2d832fa":"## *Ranking of models and features*","5766dcc2":"**LightGBM:**","08a1ba04":"## *Examining\/Optimising one classifier in more detail:*","63f239d4":"<a id='relations'><\/a>","8deb61d2":"Let's have a look at the ticket numbers and see whether we can extract some additional deck information from them. Above, we created a new feature called *Ttype* which defines the type of a ticket through the first digit of the ticket number.","bd797949":"*AdaBoost*: A boosting classifier that fits sequences of weak learners that are progressively weighted toward those features that the previous weak learners misclassified.","605b9767":"**Finally**, we pick our favourite classifier and **predict** the expected survival for the passengers in the *test* data set. The result is **written to a submission file** according to the competition rules (418 rows; only include the columns *PassengerId* and *Survived*). ","26d58626":"## *Run and describe several different classifiers*","974b5119":"This is a tricky feature because there are so many missing values and the strings don't all have the same number or formatting.","cb57651a":"**We can try out:**\n\n- I suppose one could take the starting letters, which might indicate specific decks on the ship, and use them as an additional classifier. But for less than 25% of cabins known this might not be very useful. We'll see.\n\n- Also, given that so few cabin numbers are know it doesn't seem that there are good records of them. Therefore, one should assume that it's more likely to know someone's cabin number if they survived. Let's see about that in the derived features.\n\n*TODO: Why do some people have multiple cabin numbers? What does that mean?*","b219eb73":"## *Ticket numbers*","d998b23d":"In the next step, we will try to incorporate the information from the great [Introduction to Ensembling\/Stacking in Python by Anisotropic](https:\/\/www.kaggle.com\/arthurtok\/introduction-to-ensembling-stacking-in-python) into our script.\n\nWe start out by copying the relevant parts of the script verbatim (standing on the shoulders of giants, and so on ...) and making it run in our environment. Afterwards, we will try to gradually adapt and simplify the approach, to make use of the work we have already done above for all the individual classifiers. Hopefully, this will result in a better understanding of stacking.\n\n*If you want a step by step overview then have a good look at [Anisotropic's Kernel](https:\/\/www.kaggle.com\/arthurtok\/introduction-to-ensembling-stacking-in-python) and the references therein. Seriously, you should check it out. It's great.*","addeb778":"**Decision Tree:**","15b63ed3":"It actually is. Turns out that we are more likely to know the age of higher class passengers or women, which are the strongest survival predictors we have found, so far. (Of course, the causality might as well go the other way, but that's not really the question here. What we want to find are the best predictors for survival.)\n\n**We learn:** \nThere is a strong impact of *Sex* and *Pclass* on this new feature. This might be enough to explain all the variance in the *Age\\_known* variable. We should test the predictive power in our modelling.","954a31cb":"The factorplot suggests that bad tickets are worse for male passengers, and 3rd class passengers. The individual significances are not overwhelming, but the trend itself might be useful.","fb43bd11":"These are two women that travelled together in 1st class, were 38 and 62 years old, and had no family on board.","f97f0a2d":"Similar to the known Cabin numbers, what about the *passengers for which we know the age*?","884cd035":"Also, we will start to use *factorplots*, i.e. groups of *pointplots*, from the *seaborn* plotting package to visualise the categorical relations:","5664c042":"The last plot doesn't inspire much confidence in a strong correlation between *Deck* and *Bad\\_ticket*, but maybe it will be useful otherwise.\n\n**We learn:** *Bad\\_ticket* might be a lower order effect that could give us some additional accuracy. We should test it out in the modelling stage.","561deeac":"*Gradient boosting:* This is what we call the step-by-step improvement of a weak classifier (like a tree with only 1 node) by successively applying this classifier to the residuals of the previous classifier's results. \n\nFor example: we fit a tree, determine its results (prediction: survived vs not survived), compute the residuals of this prediction vs the real survival numbers (all in the training data, of course), and then fit another tree to these residuals. This tree can now consider the full number of training samples for splitting a node at another feature, instead of having to deal with the decreased sample after the first original node (and the resulting impact of random fluctuations). This can be done again and again for n_estimator number of times.\n\nThe weak classifier itself does not necessarily have to be a tree, but a tree seems to be the favourite approach to use here. Another convention is to initialise this sequence of models with a single prediction value (like the mean of the training survival values).\n\nInstead of reducing the residuals (and the corresponding squared errors) Gradient Boosting focusses on minimising the *Loss Function* by training the classifier on the *gradient* of this function. The Loss Function describes how much the prediction is improved when shifting the predicted values by a certain amount. The method of *Gradient Decent* uses this Loss Function to iteratively move into the direction of its greatest decent (i.e. most negative first derivative). The step sizes can vary from iteration to iteration.\n\nAn additional concept is *Shrinkage*. Here, the size of each step multiplied by a factor (0,1]. In the model parameters, this factor is called the *learning_rate*. Lower learning rates make for a slower decent which seems to be empirically more effective. \n\nOne more step is to provide a sampling of rows and features, like in the random forest discussed above, to increase the diversity in tree splits and thereby a larger amount of information for the method to work with.\n\nThe important parameters are:\n\n- n_estimators: number of boosting stages; more is better\n\n- learning_rate: smaller steps need more stages\n\n- max_depth: tune for best performance; depends on interaction of features\n\n- subsample: only train on a sub sample of the data set drawn without replacement. This is called *Stochastic Gradient Decent*\n\n[Source 1](http:\/\/blog.kaggle.com\/2017\/01\/23\/a-kaggle-master-explains-gradient-boosting\/)\n\n[Source 2](<http:\/\/scikit-learn.org\/stable\/modules\/ensemble.html#gradient-tree-boosting)\n\n\n\n\nIn addition: This is the only instance where we import a module right when it's needed instead of up top. Normally, I would recommend not to ignore warnings but to fix what's causing them. However, here we get 1 warning per n_estimators from a depreciation warning in the inner workings of the classifier, over which we have no control. Therefore: ignore.","b84adc71":"As we would expect intuitively, it appears that we are more likely to know someones age if the survived the disaster. There's a difference of about 30% vs 40% and it should be significant:","af81c5a3":"Above we are creating a kind of summary dashboard, where we collect relevant visualisations to study the distributions of the individual features. We use the matplotlib *subplot* tool to line up the individual plots in a grid. We use overlapping histograms for ordinal features and barplots for categorical features. The barplots show the fraction of people (per group) who survived. There's a lot going on in this figure, so take your time to look at all the details.\n\n**We learn** the following things from studying the individual features:\n\n- *Age:* The medians are identical. However, it's noticeable that fewer young adults have survived (ages 18 - 30-ish) whereas **children younger than 10-ish had a better survival rate.** Also, there are no obvious outliers that would indicate problematic input data. The highest ages are well consistent with the overall distribution. There is a notable shortage of teenagers compared to the crowd of younger kids. But this could have natural reasons.\n\n- *Pclass:* There's a clear trend that **being a 1st class passenger gives you better chances of survival**. Life just isn't fair.\n\n- *SibSp & Parch:* **Having 1-3 siblings\/spouses\/parents\/children on board (SibSp = 1-2, Parch = 1-3) suggests proportionally better survival numbers than being alone (SibSp + Parch = 0) or having a large family travelling with you.**\n\n- *Embarked:* Well, that does look more interesting than expected.  **Embarking at \"C\" resulted in a higher survival rate than embarking at \"S\"**. There might be a correlation with other variables, here though.\n\n- *Fare:* This is case where a linear scaling isn't of much help because there is a smaller number of more extreme numbers. A natural choice in this case is to transform the values logarithmically. For this to work we need to adjust for the zero-fare entries.  The plot tells us that the **survival chances were much lower for the cheaper cabins**. Naively, one would assume that those cheap cabins were mostly located deeper inside the ship, i.e. further away from the life boats.","dbbc2bdf":"Sort of, yes. This goes some way to explain features like better survival for SibSp = 1-3. But I think that it doesn't cover all the signal in the Parch feature.\n\n**We learn:**\n\n- Different percentages of passenger classes and sexes have embarked from different ports, which is reflected in the lower survival rates for \"S\" (more men, fewer 1st class) compared to \"C\" (more women and 1st class).\n\n- It's hard to say at this stage whether there is any real impact left for the *Embarked* feature once we correct for these connections. We will come back to this in the modelling stage when we will study feature importances and significances (soon).","0f4d7fbd":"We want to make sure that our classifiers are not overfitting random data features. One of the most popular ways to check a model for robustness is called *cross validation*.\n\nIt's an approach similar to bootstrapping, where we use smaller samples from our data set to check whether the classifier gives similar results for each of them.\n\nFirst a simple cross-validation using the helper function *cross\\_val\\_score*. By default, the data is divided up into *k* equally sized sub-samples (or *folds*) and the classifier is trained on *k-1* of them and evaluated on the remaining one (e.g. for k = 4 we use 4 samples, leave each of them out once and train on the other 3, then evaluate on the one we've left out). This process is called *K-fold cross validation*.  The parameter *cv* here defines the*number* of folds (or alternatively something more complex as described in the [docs](http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.cross_val_score.html) ).  The method used for computing the scores is by default the native scoring method of the classifier (but can be changed).\n\nMore background info [here](http:\/\/scikit-learn.org\/stable\/modules\/cross_validation.html).\n\n*We've already used this cross-validation above to compute the scores for the individual classifiers.*","2452c379":"Now **that** is interesting. We see that the distributions become significantly narrower and that the tails and bimodality become much weaker (after getting rid of the zero-fare values for both groups). The really expensive *Fares* in *Pclass == 1* are pretty much all gone. Here's how the standard deviations compare:","dbc608c2":"OK, let's go through the features one by one to see what we find. Here we will see how the distributions of survivors and non-survivors compare. Personally, I like histograms for a first look at comparing two or more populations in case of scaled features. For categorical features we will use barplots plus standard deviation bars, to better judge the significance.","92a30da9":"The *feature importance* tells us how much impact an individual feature has on the decisions within the classifier. Alongside the individual features we also compute a *median* importance.\n\nThe overall result is not very surprising: *Sex* and *Pclass* are the dominant features while everything else is of similar, significantly lower importance.\n\nThe devil here is in the details:\n\n- Why is *Sex* so much weaker for the boosting algorithms? And why have features like *Alone* more impact when boosted? Is it because of the lower tree depth?\n- What can we learn from these discrepancies with respect to parameter optimisation for the individual classifiers?","b8aca005":"Finally, we model a fare category, *Fare_cat*, as an ordinal integer variable based on the logarithmic fare values:","f15abab8":"Admittedly, these are quite a few grouping levels, but 30 (\"C\") vs 20 (\"S\") are numbers that are still large enough to be useful in this context. In addition, already a grouping without the *Parch* and *SibSp* features suggests similar numbers for women in 1st class embarking from \"C\" (71) vs \"S\" (69) (in contrast to the larger overall number of all 1st class passengers leaving from \"S\").\n\nAnother recent kernel ([definitely worth checking out](https:\/\/www.kaggle.com\/varimp\/a-mostly-tidyverse-tour-of-the-titanic)) makes a convincing case for predicting *Embarked == \"S\"* for these two passengers (see also the comments). However, in my opinion we have better reasons to impute \"C\" instead. I recommend that you weigh the arguments and make your own decision.\n\n*(How much does it actually matter? Well, in the big picture these are only 2 passengers and their impact on our model accuracy won't be large. However, since the main point of this challenge is to practice data analysis it is certainly worth to take your time to examine the question in a bit more detail.)*","0d0b9229":"## *Test and select the model features*\n\nNow we are ready to model. We start with a *Logistic Regression* to assess the importance of the individual model features. We know that by definition some of our engineered features will have a *high collinearity* (i.e. behave similarly) with other new or existing features. For instance, *Young* was designed to replace *Age* and *Title* as a combination of the two. Other correlations are visible in the heatmap above. The initial modelling will allow us to decide which features are worth to take to the next step.\n\nThis is an iterative process in which you improve your model step by step, until you have found the largest feature combination which still has significant impact. Removing less important features will help you to reduce the noise in your prediction and allow your model to generalise to new data (which is our priority goal in machine learning.)\n\n*TODO: This part is still quite rudimentary and will be expanded in future versions. For now, we just continue with a rather intuitive set of important features.*  ","1ae5a777":"<a id='missing'><\/a>","21c586c7":"Stacking of classifiers that have less correlation gives better results. Intuitively, classifiers that are highly correlated, like *ExtraTrees* and *GradientBoost* above, are already so similar that stacking doesn't change the result in a significant way. This is reflected in the relatively low correlation index of the SVM with everything else.\n\nTherefore, it would be more useful to replace the predominantly tree-based sample of classifiers with a more diverse set. ","a34be497":"To simplify this broad distribution, we decide to classify the fares into *3 fare categories*: 0-10, 10-100, and above 100. This transformation can be easily achieved using the base 10 logarithm:","1fbbf577":"We designed a number of new features, and unsurprisingly several of those are correlated with the original features we used to create them. For instance *Fare\\_cat* and *Fare*. Or *Family* and *SibSp\/Parch*. In the modelling step, we will first determine which of the features carry the most signal (*to be done*) and then use them to train a number of different classifiers.","b270633d":"Ok, so what can we tell from the Deck (derived from the Cabin number)? First of all the overall survival statistics is much better than for the full sample, which is what we found above. Beyond that, the best decks for survival were B, D, and E with about 66% chance. C and F are around 60%. A and G at 50%. The only passenger on deck T died, but that's hardly robust statistics.\n\nThe largest number of cases we have is for B vs C. Let's see whether that's significant:","465beb5b":"<a id='model'><\/a>","99581a00":"In the same way, having a large family appears to be not good for survival.","0ad07bf9":"### *Alone*","8f407ab1":"Interesting. Sharing a ticket number is not uncommon. Let's follow that up a bit.","21662e7f":"[*Naive Bayes*](http:\/\/scikit-learn.org\/stable\/modules\/naive_bayes.html) is a rapid classification method. It uses the famous [Bayes Theorem](https:\/\/en.wikipedia.org\/wiki\/Bayes%27_theorem) under the 'naive' assumption that all predictor features are independent from each other (and only related to the target variable).\n\nDespite this oversimplification Naive Bayes classifiers are performing well in many cases. In addition, they are fast to compute and only require relatively little data to perform well.","4d160fe2":"Now let's study the new features and see how they relate to the survival chances:","7026d84a":"Let's remind ourselves of the distribution of *Fare* with respect to *Pclass*:","c53ccdb4":"### *Child*","9a997412":"# 8. Preparing our prediction for submission","17238eef":"At face value, some classifiers perform better than others. However, the differences between the methods are relatively small and more likely due to more or less over-fitting than anything else. (Except, possibly, for the Perceptron. There a bit more tuning might be appropriate.)","6dc558db":"# **1. Load Data and Modules**","8e99567f":"Because of the larger number of \"Miss\" vs \"Master\" mostly women are classified as \"Young\".  We also recover the age difference between the ticket classes that was already obvious in earlier plots. Both factors mean that the impact of *Young* has to be studied carefully.","b31cedf8":"For each of these various classifiers we can have a closer look to improve their performance and understand their output. As an example we'll be using the *Extremely Randomized Trees*, but any other classifier can be substituted instead.\n\nWe will start with a *grid search algorithm* to find the best parameters to run our classifier. This is called [*tuning of the hyper-parameters*](http:\/\/scikit-learn.org\/stable\/modules\/grid_search.html). The idea is to define a number of possible values for each hyper-parameter. Together, these sets of values define a grid (which is quite easy to visualise in two dimensions). Then, we evaluate the score of the classifier at each grid point and pick the one parameter combination that gives us the best score.","00438dc3":"Almost 100% yes. Above, we extract the standard deviation of the *Fares* among the ticket groups. A standard deviation of zero means that there's no difference. Only 2 values stand out. This is a small number that we could ignore, but we are curious, aren't we?","d0df99cd":"**Gradient Boosting:**","a1d087da":"**Note:** This is not a streamlined analysis, but it contains certain redundancies with the purpose of featuring and exploring different visualisation and modelling tools that can be useful in approaching a binary classification problem. I'm happy to see that my notes are useful for others who are starting out in data analysis and machine learning, and I hope that you will be able to get some inspiration from this kernel.\n\nAs the kernel continues to grow it is branching out in more detail into the different data analysis steps. This adds more depth to the overall content, but it also makes the whole notebook rather extensive. If you're very new to this subject then I recommend to go through each section on it's own (e.g. by playing with a forked copy) rather than reading the whole thing in one go. Of course, for newbies I also recommend the excellent kernels featured in the [Titanic Tutorials](https:\/\/www.kaggle.com\/c\/titanic#tutorials). \n\nFor those of you who prefer *R* over *Python* or want to try out both: I'm currently building a new [R kernel for Titanic](https:\/\/www.kaggle.com\/headsortails\/tidy-titarnic\/) along the same philosophy as this one here. Feel free to check it out and let me know your feedback :-)  ","0f037f9e":"### *Cabin numbers*","07f795fc":"Together with the PassengerId which is just a running index and the indication whether this passenger survived (1) or not (0) we have the following information for each person:\n\n- *Pclass* is the Ticket-class: first (1), second (2), and third (3) class tickets were used. This is an ordinal integer feature. \n\n- *Name* is the name of the passenger. The names also contain titles and some persons might share the same surname; indicating family relations. We know that some titles can indicate a certain age group. For instance *Master* is a boy while *Mr* is a man. This feature is a character string of variable length but similar format.\n\n- *Sex* is an indicator whether the passenger was female or male. This is a categorical text string feature. \n\n- *Age* is the integer age of the passenger. There are NaN values in this column.\n\n- *SibSp* is another ordinal integer feature describing the number of siblings or spouses travelling with each passenger.\n\n- *Parch* is another ordinal integer features that gives the number of parents or children travelling with each passenger.\n\n- *Ticket* is a character string of variable length that gives the ticket number.\n\n- *Fare* is a float feature showing how much each passenger paid for their rather memorable journey.\n\n- *Cabin* gives the cabin number of each passenger. There are NaN in this column. This is another string feature.\n\n- *Embarked* shows the port of embarkation as a categorical character value.\n\nIn summary we have 1 floating point feature (*Fare*), 1 integer variable (*Age*), 3 ordinal integer features (*Plcass, SibSp, Parch*), 2 categorical text features (*Sex, Embarked*), and 3 text string features (*Ticket, Cabin, Name*).","a8b8090f":"Finally, let's check what's going on between *Age* and *Embarked*:","53c19873":"# 7. Modelling","621b2806":"After inspecting the available features individually you might have realised that some of them are likely to be connected. Does the age-dependent survival change with sex? How are pclass and fare related? Are they strongly enough connected so that one of them is superfluous? Let's find out.\n\nNow we are connecting individual clues to get a glimpse of the bigger picture.","5aa5dce7":"### *Deck*","95c6fbea":"[Go to the top of the page](#top)","edf55947":"## *Splitting the train sample into two sub-samples: training and testing*\n\nThis is best practice for evaluating the performance of our models, which should not be tested on the same data they are trained on. This avoids overfitting.","ed771bee":"**Ranking of models.** I've 'borrowed' that one straight from this very nice kernel, because it's a useful summary display of how our models perform:\n<https:\/\/www.kaggle.com\/startupsci\/titanic\/titanic-data-science-solutions>","f9786b1d":"Final validation with the testing data set:","a23d3982":"The file *submit.csv* will now appear in the *Output* tab of this kernel. From there you can download it and submit it by going to \"Leaderboard\" -> \"Submit Predictions\" in the tab list below the competition header. \n\nI recommend to briefly describe the details of your submission (e.g. which classifier, which meta-parameters, ...) in the corresponding text field, so that you remember the model for this score and don't have to re-submit something that you had done already.\n\n&nbsp;\n\n*Best of success and enjoy learning!*","9f07ddb6":"The next idea is to define new features based on the existing ones that allow for a split into survived\/not-survived with higher confidence than the existing features. An example would be \"rich woman\" vs \"poor man\", but this particular distinction should be handled well by most classifiers. We're looking for something a bit more subtle here. This is the part where the detective puts individual clues together to see whether their sum is more than its parts.\n\nThis part of the analysis is called *Feature Engineering*. I prefer the approach to list all the new features that we define together in one place, to keep an overview. Every time we can think of a new feature, we come back here to define it and then study it further down. We compute the new features in the combined data set, to make sure that all feature realisations are complete, and then split the combine data again into train and test.\n\nThe clever way of computing the *Shared\\_ticket* values (using *group\\_by* and *np.where*) was contributed by [GeorgeChou](https:\/\/www.kaggle.com\/georgechou) in the comments. Many thanks! ","7e99c528":"[Go to the top of the page](#top)","98120802":"Now we continue to examine these initial indications in more detail. Earlier, we had a look at the *Survived* statistics of the individual features in the overview figure. Here, we want to look at correlations between the predictor features and how they could affect the target *Survived* behaviour.\n\nUsually it's most interesting to start with the strong signals in the correlation plot and to examine them more in detail.","58bd53f2":"The \"+1\" means that our boundaries are slightly shifted in terms of the \"real\" *Fare*. However, this shift avoids computing issues for the zero-fare passengers and it makes little difference for our understanding of the fare groups. In fact, in the plot above the offset had already been applied as well.\n\nAt the start of this section we define a new feature, *Fare\\_cat*, as fare categories in the same way. Let's try it out:","a97bc497":"As expected, *Pclass* and *Sex* have the most impact, but our engineered features are doing not bad either.","46638412":"**Load Python modules:** The list of modules grows step by step by adding new functionality that is useful for this project. A module could be defined further down once it is needed, but I prefer to have them all in one place to keep an overview.","3c6115e3":"### *Title*","797a3271":"We see that *Master* is capturing the male children\/teenagers very well, whereas *Miss* applies to girls as well as younger women up to about 40. *Mrs* does not contain many teenagers, but has a sizeable overlap with *Miss*; especially in the range of 20-30 years old.\n\nNevertheless, *Miss* is more likely to indicate a younger woman. Overall, there is a certain amount of variance and we're not going to be able to pinpoint a certain age based on the title.\n\nTherefore, we will use 2 *Age Groups*, updating to the *Young* variable we defined above. The idea is to address the issue of missing *Age* values by combining the *Age* and *Title* features into a single feature that should still contain some of the signal regarding survival.\n\nFor this, we define everyone under 30 *or* with a title of *Master*, *Miss*, or *Mlle* (Mademoiselle) as *Young*. All the other titles we group into *Not Young*. This is a bit of a generalisation in terms of how *Miss* and *Mrs* overlap, but it might be a useful starting point. All the other rare titles (like *Don* or *Lady*) have average ages that are high enough to count as *Not Young*.","9f61fad5":"It's Mr Osen and Mr Gustafsson on Ticket 7534. Their *Fares* are close enough, though, to include them in the general treatment.\n\nNow, let's think for a moment: Identical fares could mean that the fare for a cabin was shared equally among the passengers, in which case our previous treatment would have been justified. However, it *could* also mean that the listed value is the *cumulative fare per cabin* and it was simply recorded as the same value for each passenger. Intuitively, this doesn't seem so plausible, since you typically record what is paid for a ticket and not for a cabin. But let's investigate this for a moment and check how it would transform the *Fare* distribution. For this, we create a *Fare_eff* feature above, which we derive by dividing *Fare* by the number of people sharing a ticket (*Ticket_group*; which we also newly created).","9fa8c564":"**We learn:**\n\n- Both the factorplot and the mosaicplot indicate that almost all females that died were 3rd class passengers.\n- For males being in 1st class gives a survival boost, otherwise the proportions look roughly similar.\n- Except for 3rd class, the survival for *Embarked == Q* is close to 100% split between male and female.\n\nLet's follow up the numbers for *Pclass vs Embarked* with a *pandas crosstab plot*:","f353f73a":"Let's investigate the *Fare affair* in more detail. First, we make sure that the passengers in each group really had the same *Fare* values:","6f0d1e45":"Knowing about missing values is important because they indicate how much we don't know about our data.  Making inferences based on just a few cases is often unwise. In addition, many modelling procedures break down when missing values are involved and the corresponding rows will either have to be removed completely or the values need to be estimated somehow.\n\n**We learn:**\n\n- In the **training data** a large majority of *Cabin* numbers are missing, together with 177 *Age* values and 2 *Embarked* values. \n\n- Also, in the **test** data there is one *Fare* missing (cheeky selection), almost 100 *Age* values are unknown, and only 91 *Cabin* numbers were preserved. Best to keep that in mind.","cc3cfb24":"**Logistic Regression** again, this time with only the selected columns","0dfbc48a":"### *Age\\_known*","d444a50d":"**Bagging:**","ef80fb16":"The [LightGBM](https:\/\/github.com\/Microsoft\/LightGBM) is another gradient boosting tool which in 2017 was beginning to eclipse the XGBoost as Kaggle's go-to method for efficient boosting. LightGBM is often signficantly faster than XGBoost and achieves at least a similar accuracy.","b8ae1396":"**Look at your data in as many different ways as possible.** Some properties and connections will be immediately obvious. Others will require you to examine the data, or parts of it, in more specific ways. Metaphorically speaking: this is the part where the detective finds the clues.","f40b5de3":"Our \"usual\" factorplot examination highlights the differences between *Pclass* (as expected) but also shows some interesting variations within the *Sex* feature. This might be related to the fact that women were more likely to share a cabin, and it would therefore indicate that the *Fare* might be a fare per cabin and not per passenger.","68cf342a":"Sharing a ticket appears to be good for survival.","8c63e1df":"### *Fare\\_cat*","1026ee74":"TODO: Expand this section","c8cc92a4":"A 60-yr old 3rd class passenger without family on board. We will base our *Fare* prediction on the median of the 3rd-class fares:","06a01ddf":"[Go to the top of the page](#top)","e03cbe92":"Very much so. However, we have seen before that there might be imbalances in the dominating features *Sex* and *Plcass* that create an apparent signal. Is this another of these cases?","634347e6":"That's really cheap for a 1st class cabin. Maybe a transcription error in the data itself?","352764af":"For a view into *Pclass* vs *Sex* let's use a *mosaic plot* for a 2-dimensional overview.","2f7c3a7a":"*Random Forest:* As the name suggests, this classifier is using a number of decision trees instead of just a single one. Thereby, this is an *ensemble method* which combines the results of individual classifiers to improve the accuracy. Think of it as an average of estimators. An individual estimator may have a poor accuracy but if you combine several of them the resulting mean (or median) average will have a reduced uncertainty. Similar to the standard error of the mean for sampling normal distributions.\n\nThere are two types of ensemble methods: *boosting*, used below, and *averaging* (or *bagging*; see above). A random forest is an averaging classifier for which we train several estimators independently and then average over their individual predictions. Boosting works best for weak learners (e.g. decision stumps) whereas for Bagging\/Averaging to be successful we want to overfit a little\n\nThe *random* in *random forest* comes from the method of training each tree using a random bootstrap sample (i.e. one with replacement) of the original training set. Further randomness is introduced by making the node split dependent on a random subset of features instead of all of them. Here single trees are combined through the average of the prediction probabilities.\n\nIn addition to the tree parameters, the most important settings are:\n\n- n_estimators: number of trees. The larger the better, although improvements become marginal eventually\n\n- max_features: number of random features per subset. Lower numbers decrease variance and increase bias. Rule of thumb for classification: max_features = sqrt(all_features). This is the default setting.\n\nOne suggestion is to use a large number of highly overfitted trees with small split limits and no depth limit.\n\nOnce more, this info was digested from the [sklearn documentation](http:\/\/scikit-learn.org\/stable\/modules\/ensemble.html#forest)","a24e64af":"Ok, so we have 18 different titles, but many of them only apply to a handful of people. The dominating ones are Mr (581), Miss (210), Mrs (170), and Master (53); with the number referring to the combined data. Here are the age distributions for those:","eefab8ef":"A little follow up: For *SibSp* we see in the plot that most of the differences are not very significant (overlapping error bars). Another way of checking the actual numbers are through *cross tables*:","480c8b69":"**Ada Boost:**","d2be6970":"Now this is somewhat expected since it explains the difference between \"S\" and the other ports. Therefore, it seems that between more 1st class passengers embarking at \"C\" and more men at \"S\" there doesn't seem to be much actual influence in the port of embarkation.\n\nHowever, the last plot should also indicate that ...","c3fa301e":"So well, in fact that defining new fare categories seems almost redundant because *Pclass* already captures most of this signal. Nonetheless, we'll try; because we are optimistic people at heart. We use the dashed lines in the plot above for an (empirical) division into 3 classes, which separate the cheaper *Fare_eff* of a *Pclass* group from the more expensive ones of the next one. The new feature is called *Fare_eff_cat* and behaves as follows:","8574732a":"**Load input data.** And combine the available features of train and test data sets. *test* of course doesn't have the column that indicates survival.","45820a03":"**We learn:**\n\n- a high percentage of those embarked at \"C\" were 1st class passengers.\n- almost everyone who embarked at \"Q\" went to 3rd class (this means that the clear separation in the factorplot for \"Q\" isn't very meaningful, unfortunately).\n\nThe 2nd point is somewhat curious, since we recall from above that the survival chances for \"Q\" were actually slightly better than for \"S\". Not significantly so, of course, but certainly not worse even though \"S\" had a higher percentage of 1st and 2nd class passengers.\n\nIt seems that embarking at \"Q\" improved your chances for survival if you were a 3rd class passenger. Let's investigate that a bit more:","3e3c082c":"First a broad overview. What are the types of data and their typical shape and content?","73f788ed":"<a id='submit'><\/a>","af25e6a0":"But most large families were travelling in 3rd class. The tentative imbalance between male and female 3rd class probably reflect the observation we made earlier that men were more likely to travel alone."}}