{"cell_type":{"98131e03":"code","d75bf659":"code","6f76242d":"code","3343cd51":"code","f35d7ed7":"code","0219a1a3":"code","2284fd9d":"code","dfb8b418":"code","ad35751f":"code","7d4d9309":"code","63dfbafe":"code","fe60b173":"code","3c976519":"code","e5fcfcd9":"code","b56d031e":"code","b0f7d8e1":"code","eb7b7c85":"code","311fab7b":"code","6e04208c":"code","842ca779":"code","e8b72c72":"code","6153f542":"code","d15740fa":"code","18d6bf48":"code","20ef02ab":"code","1b0b4749":"code","41d21546":"code","fd7c7af1":"code","4babe65c":"code","582f3177":"code","ac7d3020":"code","034a1eea":"code","662ac2f1":"code","322c1e2d":"code","d0e6f88d":"code","b0d35410":"code","185d3a20":"code","aebed159":"code","f3420a8c":"code","94aa1e00":"code","6c5dc576":"code","4c1f930d":"code","e397541a":"code","bc4d5b22":"code","7a174327":"code","db57c933":"code","9eb72b8f":"code","066bb64d":"code","e583d0ba":"code","13607fb7":"code","61f6ed0e":"code","0466b99d":"code","625122ee":"code","19a9eb84":"code","8ff2f00c":"markdown","856c25b0":"markdown","983bd78e":"markdown","c171163b":"markdown","4ce5bf73":"markdown","46366c2e":"markdown","6f72284e":"markdown","4f50a45b":"markdown","8b198a55":"markdown","f0673634":"markdown","4b8c7625":"markdown","225e3544":"markdown","7b04dc9f":"markdown","b5a63e3e":"markdown","a83ccf03":"markdown"},"source":{"98131e03":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d75bf659":"import matplotlib.pyplot as plt\n%matplotlib inline\n\nimport seaborn as sns\nsns.set_style('whitegrid')\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom tqdm import tqdm","6f76242d":"# load data\ndata = pd.read_csv('\/kaggle\/input\/software-defect-prediction-nasa\/jm1.csv')\ndata.shape","3343cd51":"# import some dependencies to plot\n\nfrom plotly.offline import iplot\n# init_notebook_mode(connected=True)\nimport plotly.graph_objs as go","f35d7ed7":"# check data\ndef show_info(data, is_matrix_transpose=False):\n    # basic shape\n    print('data shape is: {}   sample number {}   attribute number {}\\n'.format(data.shape, data.shape[0], data.shape[1]))\n    # attribute(key)\n    print('data columns number {}  \\nall columns: {}\\n'.format(len(data.columns) ,data.columns))\n    # value's null\n    print('data all attribute count null:\\n', data.isna().sum())\n    # data value analysis and data demo\n    if is_matrix_transpose:\n        print('data value analysis: ', data.describe().T)\n        print('data demo without matrix transpose: ', data.head().T)\n    else:\n        print('data value analysis: ', data.describe())\n        print('data demo without matrix transpose: ', data.head())\n        \nshow_info(data)","0219a1a3":"data.head()","2284fd9d":"# label classification\ndefects_true_false = data.groupby('defects')['b'].apply(lambda x: x.count())\nprint('True: ', defects_true_false[1], 'False: ', defects_true_false[0])\ndata.defects.value_counts().plot.bar()","dfb8b418":"# Attribute relationship -- covariance\ndata.corr()","ad35751f":"# plot columns distribution\ndef plotPerColumnDistribution(df, nGraphShown, nGraphPerRow):\n    nunique = df.nunique()\n    df = df[[col for col in df if nunique[col] > 1 and nunique[col] < 50]] # For displaying purposes, pick columns that have between 1 and 50 unique values\n    nRow, nCol = df.shape\n    columnNames = list(df)\n    nGraphRow = (nCol + nGraphPerRow - 1) \/ nGraphPerRow\n    plt.figure(num = None, figsize = (6 * nGraphPerRow, 8 * nGraphRow), dpi = 80, facecolor = 'w', edgecolor = 'k')\n    for i in range(min(nCol, nGraphShown)):\n        plt.subplot(nGraphRow, nGraphPerRow, i + 1)\n        columnDf = df.iloc[:, i]\n        if (not np.issubdtype(type(columnDf.iloc[0]), np.number)):\n            valueCounts = columnDf.value_counts()\n            valueCounts.plot.bar()\n        else:\n            columnDf.hist()\n        plt.ylabel('counts')\n        plt.xticks(rotation = 90)\n        plt.title(f'{columnNames[i]} (column {i})')\n    plt.tight_layout(pad = 1.0, w_pad = 1.0, h_pad = 1.0)\n    plt.show()\n    \nplotPerColumnDistribution(data, 10, 5)","7d4d9309":"# plot corr\ndef plotCorrelationMatrix(df, graphWidth):\n    df = df.dropna('columns') # drop columns with NaN\n    df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values\n    if df.shape[1] < 2:\n        print(f'No correlation plots shown: The number of non-NaN or constant columns ({df.shape[1]}) is less than 2')\n        return\n    corr = df.corr()\n    plt.figure(num=None, figsize=(graphWidth, graphWidth), dpi=80, facecolor='w', edgecolor='k')\n    corrMat = plt.matshow(corr, fignum = 1)\n    plt.xticks(range(len(corr.columns)), corr.columns, rotation=90)\n    plt.yticks(range(len(corr.columns)), corr.columns)\n    plt.gca().xaxis.tick_bottom()\n    plt.colorbar(corrMat)\n    plt.show()\n\nplotCorrelationMatrix(data, 8)","63dfbafe":"# Scatter and density plots\ndef plotScatterMatrix(df, plotSize, textSize):\n    df = df.select_dtypes(include =[np.number]) # keep only numerical columns\n    # Remove rows and columns that would lead to df being singular\n    df = df.dropna('columns')\n    df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values\n    columnNames = list(df)\n    if len(columnNames) > 10: # reduce the number of columns for matrix inversion of kernel density plots\n        columnNames = columnNames[:10]\n    df = df[columnNames]\n    ax = pd.plotting.scatter_matrix(df, alpha=0.75, figsize=[plotSize, plotSize], diagonal='kde')\n    corrs = df.corr().values\n    for i, j in zip(*plt.np.triu_indices_from(ax, k = 1)):\n        ax[i, j].annotate('Corr. coef = %.3f' % corrs[i, j], (0.8, 0.2), xycoords='axes fraction', ha='center', va='center', size=textSize)\n    plt.suptitle('Scatter and Density Plot')\n    plt.show()\n    \nplotScatterMatrix(data, 20, 10)","fe60b173":"trace1 = go.Box(x=data['uniq_Op'])\nbox_data = [trace1]\niplot(box_data)","3c976519":"# some special columns [type is 'object']\nobject_type_cols = ['uniq_Op', 'uniq_Opnd', 'total_Op', 'total_Opnd', 'branchCount']\n# data.head()\n# data['uniq_Op'] = data['uniq_Op'].astype(np.float64)\n# data['uniq_Opnd'] = data['uniq_Opnd'].astype(np.float64)\n# data['total_Op'] = data['total_Op'].astype(np.float64)\n# data['total_Opnd'] = data['total_Opnd'].astype(np.float64)\n# data['branchCount'] = data['branchCount'].astype(np.float64)","e5fcfcd9":"# extract useful attributions and create new attribution\ndef extract_and_eval(data):\n    '''\n    input: data\n    goal: make an evaluation to every sample and label\n    '''\n    eval = (data.n < 300) & (data.v < 1000) & (data.d < 50) & (data.e < 500000) & (data.t < 5000)\n    data['eval'] = pd.DataFrame(eval)\n    data['eval'] = [1 if e == True else 0 for e in data['eval']]\n\nextract_and_eval(data)\nshow_info(data)","b56d031e":"from sklearn.preprocessing import MinMaxScaler","b0f7d8e1":"scale_v = data[['v']]\nscale_b = data[['b']]\n\nminmax_scaler = MinMaxScaler()\n\nv_scaled = minmax_scaler.fit_transform(scale_v)\nb_scaled = minmax_scaler.fit_transform(scale_b)\ndata['scaled_v'] = pd.DataFrame(v_scaled)\ndata['scaled_b'] = pd.DataFrame(b_scaled)\n\n# check data\nshow_info(data)","eb7b7c85":"tem_data = data.copy()","311fab7b":"from sklearn.model_selection import train_test_split, KFold, cross_val_score\n\n# machine learning model\nfrom sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.tree import DecisionTreeClassifier\n# boosting\nfrom lightgbm import LGBMClassifier\nfrom xgboost import XGBClassifier\nfrom catboost import CatBoostClassifier","6e04208c":"# hyper-parameter\nvalidation_size = 0.2\nrandom_seed=7","842ca779":"# extract target\ndata['target'] = data['defects'].apply(lambda x: 1 if x == True else 0)\ndata = data.drop(['defects'], axis=1)","e8b72c72":"data.info()","6153f542":"# def is_number(s):\n#     try:\n#         float(s)\n#         return True\n#     except ValueError:\n#         pass\n \n#     try:\n#         import unicodedata\n#         unicodedata.numeric(s)\n#         return True\n#     except (TypeError, ValueError):\n#         pass\n \n#     return False\n\n# data type change prework\norigin_data_type_cols = ['uniq_Op', 'uniq_Opnd', 'total_Op', 'total_Opnd', 'branchCount']\ndata = data.drop(origin_data_type_cols, axis=1)\n# for col in origin_data_type_cols:\n#     data[col] = data[data[col].is_number()]\n#     data[col] = data[col].astype(np.float64)\n# data.info()","d15740fa":"target = data['target']\ndata = data.drop(['target'], axis=1)\nX_train, X_val, y_train, y_val = train_test_split(\n    data,\n    target,\n    test_size=validation_size,\n    random_state=random_seed\n)\nX_train.shape, X_val.shape, y_train.shape, y_val.shape","18d6bf48":"# model evaluation calculate and score\nfrom sklearn.metrics import accuracy_score, recall_score, precision_score, roc_auc_score,  mean_squared_error\n# model evaluation \nfrom sklearn.metrics import classification_report, confusion_matrix\n\n# metrics method\ndef metrics_calculate(model_name, y_val, y_pred):\n    '''\n    0. basic metrics values ['accuracy', 'precision', 'recall', 'fpr', 'fnr', 'auc']\n    1. classification report\n    2. confusion matrix\n    '''\n#     y_val = np.reshape(y_val, -1).astype(np.int32)\n#     y_pred = np.where(np.reshape(y_pred, -1) > 0.5, 1, 0)\n#     accuracy = accuracy_score(y_val, y_pred)\n#     precision = precision_score(y_val, y_pred)\n#     recall = recall_score(y_val, y_pred)\n    tn, fp, fn, tp = confusion_matrix(y_val, y_pred).ravel()\n    fpr = fp \/ (tn + fp)\n    fnr = fn \/ (tp + fn)\n#     auc = roc_auc_score(y_val, y_pred)\n#     print('Model:%s Acc:%.8f Prec:%.8f Recall:%.8f FNR:%.8f FPR:%.8f AUC:%.8f' % (model_name, accuracy, precision, recall, fnr, fpr, auc))\n    print(model_name, 'classification report:\\n', classification_report(y_val, y_pred))\n    print(model_name, 'confusion_matrix:\\n', confusion_matrix(y_val, y_pred))\n    print('\\n%s FNR:%.8f FPR:%.8f\\n%s accuracy:%.8f' % (model_name, fnr, fpr, model_name, accuracy_score(y_pred,y_val)))","20ef02ab":"# plot metrics model answer(metrics)\nfrom sklearn.metrics import plot_roc_curve, plot_precision_recall_curve\n\ndef metrics_plot(model_name, model, X_val, y_val):\n    # plot P-R curve\n    disp = plot_precision_recall_curve(model, X_val, y_val)\n#     disp.ax_.set_title('2-class Precision-Recall curve: ''AP={0:0.2f}'.format(average_precision))","1b0b4749":"%%time\n\n# lightgbm\nlgb = LGBMClassifier(\n    max_depth=7,\n    lambda_l1=0.1,\n    lambda_l2=0.01,\n    learning_rate=0.01,\n    n_estimators=500,\n    reg_aplha=1.1,\n    colsample_bytree=0.9,\n    subsample=0.9,\n    n_jobs=5\n)\n# cv = cross_val_score(model, X_train, y_train, cv=kfold, scoring='accuracy')\n# print('lightgbm cv score: ', cv)","41d21546":"# fit\nlgb.fit(X_train, y_train, eval_set=[(X_val, y_val)], eval_metric='accuracy', verbose=True, early_stopping_rounds=50)\n# predict\ny_pred = lgb.predict(X_val)\n# evaluate\nmetrics_calculate('Boosting lightgbm', y_val, y_pred)","fd7c7af1":"%%time\n\n# catboost\ncb = CatBoostClassifier(\n    depth = 9, \n    reg_lambda=0.1,\n    learning_rate = 0.09,\n    iterations = 500\n)\n# cv = cross_val_score(model, X_train, y_train, cv=kfold, scoring='accuracy')","4babe65c":"# fit\ncb.fit(X_train, y_train, eval_set=[(X_val, y_val)],  verbose=True, early_stopping_rounds=50)\n# predict\ncb.predict(X_val)\n# evaluate\nmetrics_calculate('Catboost', y_val, y_pred)","582f3177":"%%time\n\n# xgboost\nxgb = XGBClassifier(\n    max_depth=9,\n    learning_rate=0.01,\n    n_estimators=500,\n    reg_alpha=1.1,\n    colsample_bytree = 0.9, \n    subsample = 0.9,\n    n_jobs = 5\n)\n# cv = cross_val_score(model, X_train, y_train, cv=kfold, scoring='accuracy')\n# print('xgboost cv score: ', cv)","ac7d3020":"# fit\n%time xgb.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=True, early_stopping_rounds=2)\n# pred\ny_pred = xgb.predict(X_val)\n# evaluate\nmetrics_calculate('Boosting xgboost', y_val, y_pred)","034a1eea":"# compare boosting [P-R curve]\nPR_curve = plot_precision_recall_curve(xgb, X_val, y_val)\nPR_curve = plot_precision_recall_curve(lgb, X_val, y_val, ax=PR_curve.ax_)\nPR_curve = plot_precision_recall_curve(cb, X_val, y_val, ax=PR_curve.ax_)\n# compare boosting [ROC curve]\nROC_curve = plot_roc_curve(xgb, X_val, y_val)\nROC_curve = plot_roc_curve(lgb, X_val, y_val, ax=ROC_curve.ax_)\nROC_curve = plot_roc_curve(cb, X_val, y_val, ax=ROC_curve.ax_)","662ac2f1":"from sklearn.model_selection import GridSearchCV\n\ndef grid_search_params(model, parameters, X_train, y_train):\n    gsearch = GridSearchCV(model, param_grid=parameters, scoring='roc_auc', cv=3)\n    gsearch.fit(X_train, y_train)\n    print('Best param value is: {0}\\n'.format(gsearch.best_params_))\n    print('Best score is: {0}\\n'.format(gsearch.best_score_))\n    print(gsearch.cv_results_['mean_test_score'], '\\n')\n#     print(gsearch.cv_results_['params'], '\\n')","322c1e2d":"# %%time\n\n# # xgboost -- gridsearchcv\n# gs_xgb = XGBClassifier(\n#     eta= 0.3, \n#     n_estimators= 500,\n#     gamma= 0,\n#     max_depth= 6, \n#     min_child_weight= 1,\n#     colsample_bytree= 1, \n#     colsample_bylevel= 1, \n#     subsample= 1, \n#     reg_lambda= 1, \n#     reg_alpha= 0,\n#     seed= 33\n# )\n\n# # scale of tree\n# scale_tree_params = {\n#     'max_depth':[3,5,7,9],\n#     'min_child_weight':[1,3,5]\n# }\n\n\n# # control fit degree\n# fit_degree_params = {\n#     'subsample':[i\/10.0 for i in range(6,10)],\n#     'colsample_bytree':[i\/10.0 for i in range(6,10)],\n#     'min_child_weight':[6,8,10,12],\n#     'reg_alpha':[1e-5, 1e-2, 0.1, 1, 100]\n# }\n\n# print('Search for best tree scale parameters')\n# grid_search_params(gs_xgb, scale_tree_params, X_train, y_train)\n\n# print('Search for best fit degree parameters')\n# grid_search_params(gs_xgb, fit_degree_params, X_train, y_train)","d0e6f88d":"# gs_xgb = XGBClassifier(\n#     eta= 0.3, \n#     n_estimators= 500,\n#     gamma= 0,\n#     max_depth= 6, \n#     min_child_weight= 1,\n#     colsample_bytree= 1, \n#     colsample_bylevel= 1, \n#     subsample= 1, \n#     reg_lambda= 1, \n#     reg_alpha= 0,\n#     seed= 33\n# )","b0d35410":"# # test best parameters and origin \n# gs_xgb.fit(X_train, y_train, eval_set=[(X_val, y_val)], eval_metric='accuracy', verbose=True, early_stopping_rounds=5)\n# # predict\n# y_pred = gs_xgb.predict(X_val)\n# # evaluate\n# metrics_calculate('Boosting xgboost after grid-search-cv: ', y_val, y_pred)","185d3a20":"# get data\n# tem_data.head()\nX_train, X_val, y_train, y_val = train_test_split(\n    tem_data.iloc[:, :-10].values, \n    tem_data['eval'].values, \n    test_size=validation_size,\n    random_state=random_seed\n)\nX_train.shape, X_val.shape, y_train.shape, y_val.shape","aebed159":"kfold = KFold(n_splits=10, random_state=random_seed)\n# get model -- GaussianNB\ngaussion_nb = GaussianNB()\n%time cv = cross_val_score(gaussion_nb, X_train, y_train, cv=kfold, scoring='accuracy')\nprint('Naive Bayes GaussianNB cv score: ', cv)","f3420a8c":"# fit\n%time gaussion_nb.fit(X_train, y_train)\n# predict\ny_pred = gaussion_nb.predict(X_val)\n# evaluate\nmetrics_calculate('Naive Bayes - GaussionNB', y_val, y_pred)","94aa1e00":"# get model -- MultinomialNB\nmultinomial_nb = MultinomialNB()\n%time cv = cross_val_score(multinomial_nb, X_train, y_train, cv=kfold, scoring='accuracy')\nprint('Naive Bayes MultinomialNB cv score: ', cv)","6c5dc576":"# fit\n%time multinomial_nb.fit(X_train, y_train)\n# predict\ny_pred = multinomial_nb.predict(X_val)\n# evaluate\nmetrics_calculate('Naive Bayes - MultinomialNB', y_val, y_pred)","4c1f930d":"# get model -- BernoulliNB\nbernoulli_nb = BernoulliNB()\n%time cv = cross_val_score(bernoulli_nb, X_train, y_train, cv=kfold, scoring='accuracy')\nprint('Naive Bayes BernoulliNB cv score: ', cv)","e397541a":"# fit\n%time bernoulli_nb.fit(X_train, y_train)\n# predict\ny_pred = bernoulli_nb.predict(X_val)\n# evaluate\nmetrics_calculate('Naive Bayes - BernoulliNB', y_val, y_pred)","bc4d5b22":"# compare boosting [P-R curve]\nPR_curve_nb = plot_precision_recall_curve(gaussion_nb, X_val, y_val)\nPR_curve_nb = plot_precision_recall_curve(multinomial_nb, X_val, y_val, ax=PR_curve_nb.ax_)\nPR_curve_nb = plot_precision_recall_curve(bernoulli_nb, X_val, y_val, ax=PR_curve_nb.ax_)\n# compare boosting [ROC curve]\nROC_curve_nb = plot_roc_curve(gaussion_nb, X_val, y_val)\nROC_curve_nb = plot_roc_curve(multinomial_nb, X_val, y_val, ax=ROC_curve_nb.ax_)\nROC_curve_nb = plot_roc_curve(bernoulli_nb, X_val, y_val, ax=ROC_curve_nb.ax_)","7a174327":"%%time\n\n# init parameters\ngs_lgb = LGBMClassifier(\n    objective = 'binary',\n    is_unbalance = True,\n    metric = 'binary_logloss,auc',\n    max_depth = 6,\n    num_leaves = 40,\n    learning_rate = 0.1,\n    feature_fraction = 0.7,\n    min_child_samples=21,\n    min_child_weight=0.001,\n    bagging_fraction = 1,\n    bagging_freq = 2,\n    reg_alpha = 0.001,\n    reg_lambda = 8,\n    cat_smooth = 0,\n    num_iterations = 200,\n)\n\n# scale of tree\nscale_tree_params = {\n    'max_depth': [4, 6, 8],\n    'num_leaves': [20, 30, 40],\n    'min_child_samples': [18, 19, 20, 21, 22],\n    'min_child_weight': [0.001, 0.002],\n    'feature_fraction': [0.6, 0.8, 1],\n}\n\n# control fit degree\nfit_degree_params = {\n    \n}\n\nprint('Search for best tree scale parameters')\ngrid_search_params(gs_lgb, scale_tree_params, X_train, y_train)\n\n# print('Search for best fit degree parameters')\n# grid_search_params(gs_lgb, fit_degree_params, X_train, y_train)","db57c933":"# init\ngs_lgb = LGBMClassifier(\n    objective = 'binary',\n    is_unbalance = True,\n    metric = 'binary_logloss,auc',\n    max_depth = 6,\n    num_leaves = 20,\n    learning_rate = 0.1,\n    feature_fraction = 1,\n    min_child_samples=19,\n    min_child_weight=0.001,\n    bagging_fraction = 1,\n    bagging_freq = 2,\n    reg_alpha = 0.001, \n    reg_lambda = 8,\n    cat_smooth = 0,\n    num_iterations = 200,\n)","9eb72b8f":"# fit\n%time gs_lgb.fit(X_train, y_train)\n# predict\ny_pred = gs_lgb.predict(X_val)\n# evaluate\nmetrics_calculate('Boosting lightgbm after grid-search-cv: ', y_val, y_pred)","066bb64d":"# %%time\n\n# tree_model = DecisionTreeClassifier()\n# cv = cross_val_score(tree_model, X_train, y_train, cv=kfold, scoring='accuracy')\n# print('Naive Bayes cv score: ', cv)","e583d0ba":"# # fit\n# %time tree_model.fit(X_train, y_train)\n# # predict\n# y_pred = tree_model.predict(X_val)\n# # evaluate\n# metrics_calculate('Decision Tree', y_val, y_pred)","13607fb7":"# # LR\n# X = data['loc'].values.reshape(-1, 1)\n# X_train, X_val, y_train, y_val = train_test_split(X, data['loc'].values)\n# X_train.shape, X_val.shape, y_train.shape, y_val.shape","61f6ed0e":"# X_train.head()","0466b99d":"# %%time\n\n# model = LinearRegression()\n# model.fit(X_train, y_train)","625122ee":"# # predict\n# y_pred = model.predict(X_val)\n# print('Mean Squared Error (MSE):', mean_squared_error(y_val, y_pred))  \n# print('Root Mean Squared Error (RMSE):', np.sqrt(mean_squared_error(y_val, y_pred)))","19a9eb84":"# X_train.head()","8ff2f00c":"## prework\n\n* import basic dependencies\n* load data","856c25b0":"## EDA\n\n* import some dependencies to plot\n* use plotly to visualization\n    * label classification\n        * count and plot(visualization)\n    * value visualization\n        * use historgram to visualization attribution\n        * relationship\n            * covariance\n            * heatmap\n    * scatter\n    ","983bd78e":"### Lightgbm GridSearchCV\n\n* tree scale [cart regression tree]\n* control fit degree\n","c171163b":"## Model\n\n* hyper-parameter\n* data prepare\n    * extract target(label)\n    * train train split\n    * cross-validation(because of data size only 1w) => use 10-cv\n* build model\n    * naive bayes\n    * LR\n    * Boosting\n        * xgboost\n        * lightgbm\n* fit, predict, evaluate(precision, recall, f1-score, acc, roc, auc)","4ce5bf73":"## Data Normalization\n\n* load importance\n* use Min-Max Normalization","46366c2e":"### Boosting\n\n* xgboost\n* lightgbm\n* catboost\n\n#### Choose a model that performs best => gridSearchCV\n* get best parameter","6f72284e":"**Then show the covariance.**\n -- by coveriance matrix","4f50a45b":"### Decision Tree","8b198a55":"### Other models\n\n* Naive bayes\n    * GaussionNB \u9ad8\u65af\u5206\u5e03\u7684\u6734\u7d20\u8d1d\u53f6\u65af\n    * MultinomialNB \u591a\u9879\u5f0f\u5206\u5e03\u7684\u6734\u7d20\u8d1d\u53f6\u65af\n    * BernoulliNB \u4f2f\u52aa\u5229\u5206\u5e03\u7684\u6734\u7d20\u8d1d\u53f6\u65af","f0673634":"## Feature engineering\n\n* extract some useful attributions and create new attribution","4b8c7625":"## Data cleaning\n\n* fillna\n* remove outliars (by use boxplot to visualization)\n* data type transform\n    * remove ~char","225e3544":"### some special columns \n\n* data cleaning\n* change data type","7b04dc9f":"**get best parameters**\n\n* 'feature_fraction': 0.8, \n* 'min_child_samples': 19, \n* 'min_child_weight': 0.001\n* 'max_depth': 6, \n* 'num_leaves': 20\n","b5a63e3e":"### LR (Linear Regression)","a83ccf03":"## CSU-Spring\u5c0f\u961f Kernel introduction\n<br>\n\n### Introduction\n\n1. \ud83c\udf31 team: Spring\u5c0f\u961f<br><br>\n2. \ud83d\ude04 \u6210\u5458: Aczy156(\u9648\u5189\u98de) Chun yu(\u9673\u4fca\u745c) AFun(\u7eaa\u6631\u5e06)<br><br>\n3. \u26a1 \u9009\u9898: \u4e2d\u5357\u5927\u5b66(Central South University) \u5927\u4e09\u4e0a\u534a\u5b66\u5e74 \u5de5\u7a0b\u7814\u7a76\u4e0e\u5b9e\u4e60 \u8f6f\u4ef6\u7f3a\u9677\u6d4b\u8bd5<br><br>\n4. \ud83d\udd2d dataset source: <br>\n    * Tronclass \u4efb\u8001\u5e08\u7684\u8d44\u6599\u5939\u4e2d\u7684 NASA\u8d44\u6599\u5939\u4e2d\u6587\u4ef6<br>\n    * [NASA open dataset](https:\/\/nasa.github.io\/data-nasa-gov-frontpage\/)<br>\n    * [Kaggle open dataset](https:\/\/www.kaggle.com\/semustafacevik\/software-defect-prediction)<br>\n\n<br>\n### Dataset and Kernel\n\nDataset: \u6574\u5408\u6570\u636e\uff0c\u5e76upload\u81f3Kaggle\uff0c\u6570\u636e\u96c6\u5df2\u5f00\u6e90\u3002<br>\nURL: [https:\/\/www.kaggle.com\/aczy156\/software-defect-prediction-nasa](https:\/\/www.kaggle.com\/aczy156\/software-defect-prediction-nasa)\n<br><br>\n\nKernel\uff1a(\u4e5f\u5c31\u662f\u5f53\u524d\u6b64kernel)\n* URL: [https:\/\/www.kaggle.com\/aczy156\/software-defect-prediction-nasa-eda-naive-bayes](https:\/\/www.kaggle.com\/aczy156\/software-defect-prediction-nasa-eda-naive-bayes)\n* Version\uff1a\n    * Version1: \"build: Baseline(based on machine leanring)\"\n    * Version2: \"feats: Introduction\"\n    * Version3: \"fix: EDA plot method\"\n    * Version4: \"feats: Machine Learning model - Decision Tree\"\n    * Version5: \"fix: evaluate function\"\n    * Version6: \"feats: Machine Learning model - boosting ['xgboost', 'lightgbm', 'catboost']\"\n    * Version7: \"feats: Model Compare by P-R curve and ROC curve\"\n    * Version8: \"feats: GridSearchCV for better parameters\"\n    * Version9: \"fix: model tuning.\"\n    * Versino10: \"styles: format kernel.\""}}