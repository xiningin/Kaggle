{"cell_type":{"7322b73a":"code","e136aded":"code","49313cf7":"code","9cfff2ba":"code","cf51b2eb":"code","be8824ce":"code","552ab6ba":"code","a62b36c1":"code","46648ae5":"code","3299bcaa":"code","378f2da9":"code","4319adf4":"code","5c97f62c":"code","6841a6b7":"code","484e604b":"code","be730dd4":"code","1fc5acf6":"code","6f150a87":"code","9d50613b":"code","7a6653e0":"code","de069c23":"code","5b9374d0":"code","10117d56":"code","050d5766":"code","6be65ef9":"code","b6a00f8f":"code","972c570c":"code","8bff596e":"code","d726a89d":"code","3666f078":"code","39153292":"code","a6f41282":"code","546bc3ac":"code","517de9fc":"code","bed0668d":"code","b00b2d26":"code","74e1d16b":"code","ec5437ed":"code","e77e2f2b":"code","4cca59a6":"markdown","f440629e":"markdown","9ee3ee7c":"markdown","317a3b44":"markdown","e0643ecb":"markdown","d8ad3b7a":"markdown","c7fb1295":"markdown","ac3f6400":"markdown","8db50914":"markdown","30bf61b8":"markdown","f2c381f2":"markdown","b49e1da3":"markdown","8cbd8d44":"markdown","4398896c":"markdown","afd910f4":"markdown","ce10be4c":"markdown","f1af8862":"markdown","839707fb":"markdown"},"source":{"7322b73a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nimport re\nimport nltk\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nimport plotly.graph_objects as go\n\nfrom plotly.subplots import make_subplots\nfrom sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n\nfrom tensorflow.keras.layers import Embedding\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.preprocessing.text import one_hot\nfrom tensorflow.keras.layers import LSTM\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.layers import Bidirectional\nfrom tensorflow.keras.layers import Dropout\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nimport tensorflow as tf\ntf.__version__\n","e136aded":"fake = pd.read_csv(\"\/kaggle\/input\/fake-news-detection\/Fake.csv\", parse_dates=['date'])\ntrue = pd.read_csv(\"\/kaggle\/input\/fake-news-detection\/True.csv\", parse_dates=['date'])","49313cf7":"print(fake.info())\nprint(fake.head())\nprint(fake['subject'].value_counts())","9cfff2ba":"fake[fake['date']==\"https:\/\/100percentfedup.com\/served-roy-moore-vietnamletter-veteran-sets-record-straight-honorable-decent-respectable-patriotic-commander-soldier\/\"]\nfake.loc[9358]['date'] = 'December 31, 2017'\n\n\nfake[fake['date']==\"https:\/\/100percentfedup.com\/video-hillary-asked-about-trump-i-just-want-to-eat-some-pie\/\"]\nfake.loc[15507]['date'] = 'December 29, 2017'\n\nfake[fake['date']==\"https:\/\/100percentfedup.com\/12-yr-old-black-conservative-whose-video-to-obama-went-viral-do-you-really-love-america-receives-death-threats-from-left\/\"]\nfake.loc[15508]['date'] = 'December 30, 2017'\n\n \nfake[fake['date']==\"https:\/\/fedup.wpengine.com\/wp-content\/uploads\/2015\/04\/hillarystreetart.jpg\"]\nfake.loc[15839]['date'] = 'December 30, 2017'\nfake.loc[17432]['date'] = 'December 26, 2017'\nfake.loc[21869]['date'] = 'December 25, 2017'\n\n \nfake[fake['date']==\"https:\/\/fedup.wpengine.com\/wp-content\/uploads\/2015\/04\/entitled.jpg\"]\nfake.loc[15840]['date'] = 'December 29, 2017'\nfake.loc[17433]['date'] = 'December 28, 2017'\nfake.loc[21870]['date'] = 'December 27, 2017'\n\nfake[fake['date']==\"MSNBC HOST Rudely Assumes Steel Worker Would Never Let His Son Follow in His Footsteps\u2026He Couldn\u2019t Be More Wrong [Video]\"]\nfake.loc[18933]['date'] = 'December 24, 2017'\nfake['date'] = pd.to_datetime(fake['date'], dayfirst = True)\nprint(\"Fake News dates: \",fake['date'].min(), fake['date'].max())\nprint(\"True News dates: \",true['date'].min(), true['date'].max())","cf51b2eb":"print(true.info())\nprint(true.head())\nprint(true['subject'].value_counts())","be8824ce":"from nltk.stem.porter import PorterStemmer\nfrom nltk.corpus import stopwords\n\n# Data cleaning\ndef remove_tag(string):\n    text=re.sub('<.*?>','',string)\n    return text\ndef remove_mention(text):\n    line=re.sub(r'@\\w+','',text)\n    return line\ndef remove_hash(text):\n    line=re.sub(r'#\\w+','',text)\n    return line\ndef remove_newline(string):\n    text=re.sub('\\n','',string)\n    return text\ndef remove_url(string): \n    text = re.sub('http[s]?:\/\/(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+','',string)\n    return text\ndef remove_number(text):\n    line=re.sub(r'[0-9]+','',text)\n    return line\ndef remove_punct(text):\n    line = re.sub(r'[!\"\\$%&\\'()*+,\\-.\\\/:;=#@?\\[\\\\\\]^_`{|}~]*','',text)\n    #string=\"\".join(line)\n    return line\ndef text_strip(string):\n    line=re.sub('\\s{2,}', ' ', string.strip())\n    return line   ","552ab6ba":"fake['refine_text']=fake['text'].str.lower()\nfake['refine_text']=fake['refine_text'].apply(lambda x:remove_tag(str(x)))\nfake['refine_text']=fake['refine_text'].apply(lambda x:remove_mention(str(x)))\nfake['refine_text']=fake['refine_text'].apply(lambda x:remove_hash(str(x)))\nfake['refine_text']=fake['refine_text'].apply(lambda x:remove_newline(x))\nfake['refine_text']=fake['refine_text'].apply(lambda x:remove_url(x))\nfake['refine_text']=fake['refine_text'].apply(lambda x:remove_number(x))\nfake['refine_text']=fake['refine_text'].apply(lambda x:remove_punct(x))\nfake['refine_text']=fake['refine_text'].apply(lambda x:text_strip(x))\nfake['text_length']=fake['refine_text'].str.split().map(lambda x: len(x))\n\ntrue['refine_text']=true['text'].str.lower()\ntrue['refine_text']=true['refine_text'].apply(lambda x:remove_tag(str(x)))\ntrue['refine_text']=true['refine_text'].apply(lambda x:remove_mention(str(x)))\ntrue['refine_text']=true['refine_text'].apply(lambda x:remove_hash(str(x)))\ntrue['refine_text']=true['refine_text'].apply(lambda x:remove_newline(x))\ntrue['refine_text']=true['refine_text'].apply(lambda x:remove_url(x))\ntrue['refine_text']=true['refine_text'].apply(lambda x:remove_number(x))\ntrue['refine_text']=true['refine_text'].apply(lambda x:remove_punct(x))\ntrue['refine_text']=true['refine_text'].apply(lambda x:text_strip(x))\ntrue['text_length']=true['refine_text'].str.split().map(lambda x: len(x))\n\nfake['refine_title']=fake['title'].str.lower()\nfake['refine_title']=fake['refine_title'].apply(lambda x:remove_tag(str(x)))\nfake['refine_title']=fake['refine_title'].apply(lambda x:remove_mention(str(x)))\nfake['refine_title']=fake['refine_title'].apply(lambda x:remove_hash(str(x)))\nfake['refine_title']=fake['refine_title'].apply(lambda x:remove_newline(x))\nfake['refine_title']=fake['refine_title'].apply(lambda x:remove_url(x))\nfake['refine_title']=fake['refine_title'].apply(lambda x:remove_number(x))\nfake['refine_title']=fake['refine_title'].apply(lambda x:remove_punct(x))\nfake['refine_title']=fake['refine_title'].apply(lambda x:text_strip(x))\nfake['title_length']=fake['refine_title'].str.split().map(lambda x: len(x))\n\ntrue['refine_title']=true['title'].str.lower()\ntrue['refine_title']=true['refine_title'].apply(lambda x:remove_tag(str(x)))\ntrue['refine_title']=true['refine_title'].apply(lambda x:remove_mention(str(x)))\ntrue['refine_title']=true['refine_title'].apply(lambda x:remove_hash(str(x)))\ntrue['refine_title']=true['refine_title'].apply(lambda x:remove_newline(x))\ntrue['refine_title']=true['refine_title'].apply(lambda x:remove_url(x))\ntrue['refine_title']=true['refine_title'].apply(lambda x:remove_number(x))\ntrue['refine_title']=true['refine_title'].apply(lambda x:remove_punct(x))\ntrue['refine_title']=true['refine_title'].apply(lambda x:text_strip(x))\ntrue['title_length']=true['refine_title'].str.split().map(lambda x: len(x))","a62b36c1":"fig, (ax2) = plt.subplots(1,1,figsize=[17, 10])\nwordcloud2 = WordCloud(background_color='black',colormap=\"terrain_r\",width=800,height=400).generate(\" \".join(fake['title']))\n\nax2.imshow(wordcloud2,interpolation='bilinear')\nax2.axis('off')\nax2.set_title('Fake News - Most Used Words in Title',fontsize=35)","46648ae5":"fig, (ax2) = plt.subplots(1,1,figsize=[17, 10])\nwordcloud2 = WordCloud(background_color='white',colormap=\"spring\", width=800,height=400).generate(\" \".join(fake['title']))\n\nax2.imshow(wordcloud2,interpolation='bilinear')\nax2.axis('off')\nax2.set_title('True News - Most Used Words in Title',fontsize=35)","3299bcaa":"print(\"Average length of True News  : {}\".format(round(true['text_length'].mean(),2)))\nprint(\"Average length of Fake News  : {}\".format(round(fake['text_length'].mean(),2)))\nprint(\"Average title length of True News  : {}\".format(round(true['title_length'].mean(),2)))\nprint(\"Average title length of Fake News  : {}\".format(round(fake['title_length'].mean(),2)))","378f2da9":"fig = go.Figure()\n\nfig.add_trace(go.Violin(y=true['title_length'], box_visible=False, line_color='black', meanline_visible=True, fillcolor='magenta', opacity=0.6,name=\"True\", x0='True News'))\nfig.add_trace(go.Violin(y=fake['title_length'], box_visible=False, line_color='black', meanline_visible=True, fillcolor='skyblue', opacity=0.6,name=\"Fake\", x0='Fake News') )\n\nfig.update_traces(box_visible=False, meanline_visible=True)\nfig.update_layout(title_text=\"Violin - News Title Length\",title_x=0.5)\nfig.show()","4319adf4":"fig = go.Figure()\n\nfig.add_trace(go.Violin(y=true['text_length'], box_visible=False, line_color='black', meanline_visible=True, fillcolor='green', opacity=0.6,name=\"True\", x0='True News'))\nfig.add_trace(go.Violin(y=fake['text_length'], box_visible=False, line_color='black', meanline_visible=True, fillcolor='red', opacity=0.6,name=\"Fake\", x0='Fake News') )\n\nfig.update_traces(box_visible=True, meanline_visible=True)\nfig.update_layout(title_text=\"Violin - News Length\",title_x=0.5)\nfig.show()","5c97f62c":"def ngram_df(corpus,nrange,n=None):\n    vec = CountVectorizer(stop_words = 'english',ngram_range=nrange).fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    total_list=words_freq[:n]\n    df=pd.DataFrame(total_list,columns=['text','count'])\n    return df\nunigram_df=ngram_df(true['refine_title'],(1,1),20)\nbigram_df=ngram_df(true['refine_title'],(2,2),20)\ntrigram_df=ngram_df(true['refine_title'],(3,3),20)\n\nunigram_fake_df=ngram_df(fake['refine_title'],(1,1),20)\nbigram_fake_df=ngram_df(fake['refine_title'],(2,2),20)\ntrigram_fake_df=ngram_df(fake['refine_title'],(3,3),20)","6841a6b7":"fig = make_subplots(\n    rows=3, cols=1,subplot_titles=(\"Unigram\",\"Bigram\",'Trigram'),\n    specs=[[{\"type\": \"scatter\"}],\n           [{\"type\": \"scatter\"}],\n           [{\"type\": \"scatter\"}]\n          ])\n\nfig.add_trace(go.Bar(\n    y=unigram_df['text'][::-1],\n    x=unigram_df['count'][::-1],\n    marker={'color': \"blue\"},  \n    text=unigram_df['count'],\n    textposition = \"outside\",\n    orientation=\"h\",\n    name=\"Months\",\n),row=1,col=1)\n\nfig.add_trace(go.Bar(\n    y=bigram_df['text'][::-1],\n    x=bigram_df['count'][::-1],\n    marker={'color': \"blue\"},  \n    text=bigram_df['count'],\n     name=\"Days\",\n    textposition = \"outside\",\n    orientation=\"h\",\n),row=2,col=1)\n\nfig.add_trace(go.Bar(\n    y=trigram_df['text'][::-1],\n    x=trigram_df['count'][::-1],\n    marker={'color': \"blue\"},  \n    text=trigram_df['count'],\n     name=\"Days\",\n    orientation=\"h\",\n    textposition = \"outside\",\n),row=3,col=1)\n\nfig.update_xaxes(showline=True, linewidth=2, linecolor='black', mirror=True)\nfig.update_yaxes(showline=True, linewidth=2, linecolor='black', mirror=True)\nfig.update_layout(title_text='Top True News N Grams',xaxis_title=\" \",yaxis_title=\" \", showlegend=False,title_x=0.5,height=1200,template=\"plotly_white\")\nfig.show()","484e604b":"fig = make_subplots(\n    rows=3, cols=1,subplot_titles=(\"Unigram\",\"Bigram\",'Trigram'),\n    specs=[[{\"type\": \"scatter\"}],\n           [{\"type\": \"scatter\"}],\n           [{\"type\": \"scatter\"}]\n          ])\n\nfig.add_trace(go.Bar(\n    y=unigram_fake_df['text'][::-1],\n    x=unigram_fake_df['count'][::-1],\n    marker={'color': \"blue\"},  \n    text=unigram_fake_df['count'],\n    textposition = \"outside\",\n    orientation=\"h\",\n    name=\"Months\",\n),row=1,col=1)\n\nfig.add_trace(go.Bar(\n    y=bigram_fake_df['text'][::-1],\n    x=bigram_fake_df['count'][::-1],\n    marker={'color': \"blue\"},  \n    text=bigram_fake_df['count'],\n     name=\"Days\",\n    textposition = \"outside\",\n    orientation=\"h\",\n),row=2,col=1)\n\nfig.add_trace(go.Bar(\n    y=trigram_fake_df['text'][::-1],\n    x=trigram_fake_df['count'][::-1],\n    marker={'color': \"blue\"},  \n    text=trigram_fake_df['count'],\n     name=\"Days\",\n    orientation=\"h\",\n    textposition = \"outside\",\n),row=3,col=1)\n\nfig.update_xaxes(showline=True, linewidth=2, linecolor='black', mirror=True)\nfig.update_yaxes(showline=True, linewidth=2, linecolor='black', mirror=True)\nfig.update_layout(title_text='Top Fake titles N Grams',xaxis_title=\" \",yaxis_title=\" \", showlegend=False,title_x=0.5,height=1200,template=\"seaborn\")\nfig.show()","be730dd4":"true['label'] = 1\nfake['label'] = 0\nnews = pd.concat([true,fake],ignore_index=True)\ny = news['label']\nnews = news.drop(['label'],axis = 1)\nnews\n","1fc5acf6":"def remove_stopwords(text):\n    ps = PorterStemmer()    \n    #review = [ps.stem(word) for word in text.split() if not word in stopwords.words('english')]    \n    review = [word for word in text.split() if not word in stopwords.words('english')]    \n    review = \" \".join(review)\n    return review\n\n\ncorpus = news['refine_text'].apply(lambda x:remove_stopwords(x))\n#corpus = news['refine_text'].values\ncorpus[:3]","6f150a87":"def plot_confusion_matrix(cm, classes, normalize=False, title='Confusion matrix', cmap=plt.cm.Blues):\n    \"\"\"\n    See full source and example: \n    http:\/\/scikit-learn.org\/stable\/auto_examples\/model_selection\/plot_confusion_matrix.html\n    \n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, cm[i, j],\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')","9d50613b":"from sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn import metrics\nimport itertools\n\ncv = CountVectorizer(max_features = 500, ngram_range=(1,3))\nX = cv.fit_transform(corpus).toarray()\nfeature_names = cv.get_feature_names()\nprint(\"Feature Names: \",feature_names[:20])\nprint(\"X shape: \",X.shape)\nprint(\"Get Params: \",cv.get_params())\n\nX_train, X_test, y_train,y_test = train_test_split(X,y, test_size=0.33, random_state=10)\nprint(\"X_train.shape, X_test.shape, y_train.shape, y_test.shape: \",X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n\nclassifier = MultinomialNB()\nclassifier.fit(X_train,y_train)\npred = classifier.predict(X_test)\nscore = metrics.accuracy_score(y_test,pred)\nprint(\"Score: \",score)\n\ncm = metrics.confusion_matrix(y_test,pred)\nplot_confusion_matrix(cm, classes = ['FAKE', 'TRUE'])","7a6653e0":"from prettytable import PrettyTable\npt = PrettyTable()\npt.field_names = [\"S No.\", \"Alpha Value\", \"Score\"]\n\nprevious_score = 0\ni=1\n\n# Hyperparameters with MultinomialNB\nfor alpha in np.arange(0,1,0.1):\n    sclf = MultinomialNB(alpha = alpha)\n    sclf.fit(X_train,y_train)\n    pred = sclf.predict(X_test)\n    score = metrics.accuracy_score(pred,y_test)\n    if score > previous_score:\n        clf = sclf\n    #print(\"Alpha: {}, Score: {} \".format(alpha, score))\n    pt.add_row([i,round(alpha,1),round(score,3)])\n    i = i+1\n    \nprint(pt)","de069c23":"# Most True\nsorted(zip(clf.coef_[0], feature_names),reverse=True)[:20]","5b9374d0":"# Most Fake\nsorted(zip(clf.coef_[0], feature_names),reverse=False)[:20]","10117d56":"from sklearn.linear_model import PassiveAggressiveClassifier\nlinear_clf = PassiveAggressiveClassifier()\n\nlinear_clf.fit(X_train,y_train)\npred = linear_clf.predict(X_test)\nscore = metrics.accuracy_score(y_test,pred)\nprint(\"Score: \",score)\ncm = metrics.confusion_matrix(y_test,pred)\nplot_confusion_matrix(cm, classes = ['FAKE data', 'TRUE data'])","050d5766":"from sklearn.feature_extraction.text import TfidfVectorizer\n\n# create the transform\ntfidf = TfidfVectorizer(max_features=500,ngram_range=(1,3))\n\n# encode document\nX = tfidf.fit_transform(corpus)\n\n## Divide the dataset into Train and Test\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=0)","6be65ef9":"print(tfidf.get_feature_names()[:20])\ntfidf.get_params()\n\nclf_tf = MultinomialNB()\nclf_tf.fit(X_train, y_train)\npred_tf = clf_tf.predict(X_test)\nscore_tf = metrics.accuracy_score(y_test, pred_tf)\nprint(\"accuracy:   %0.3f\" % score_tf)\ncm = metrics.confusion_matrix(y_test, pred_tf)\nplot_confusion_matrix(cm, classes=['FAKE', 'REAL'])","b6a00f8f":"# Most True\nfeature_names_tf = tfidf.get_feature_names()\nsorted(zip(clf_tf.coef_[0], feature_names_tf),reverse=True)[:20]","972c570c":"# Most Fake\nfeature_names_tf = tfidf.get_feature_names()\nsorted(zip(clf_tf.coef_[0], feature_names_tf),reverse=False)[:20]","8bff596e":"pta = PrettyTable()\npta.field_names = [\"S No.\", \"Alpha Value\", \"Score\"]\n\n# Hyperparameters with MultinomialNB (fitted with TF-IDF)\n\nprevious_score = 0\nfor alpha in np.arange(0,1,0.1):\n    sclf = MultinomialNB(alpha = alpha)\n    sclf.fit(X_train,y_train)\n    pred = sclf.predict(X_test)\n    score = metrics.accuracy_score(pred,y_test)\n    if score > previous_score:\n        clf = sclf\n    pta.add_row([i,round(alpha,3),round(score,3)])\n    \nprint(pta)","d726a89d":"from sklearn.linear_model import PassiveAggressiveClassifier\nlinear_clf = PassiveAggressiveClassifier()\n\nlinear_clf.fit(X_train, y_train)\npred = linear_clf.predict(X_test)\nscore = metrics.accuracy_score(y_test, pred)\nprint(\"accuracy:   %0.3f\" % score)\ncm = metrics.confusion_matrix(y_test, pred)\nplot_confusion_matrix(cm, classes=['FAKE Data', 'REAL Data'])","3666f078":"from sklearn.feature_extraction.text import HashingVectorizer\n\n\n# create the transform\nhashVec = HashingVectorizer(n_features=1000, alternate_sign=False)\n\n# encode document\nX = hashVec.fit_transform(corpus.values)\n\n## Divide the dataset into Train and Test\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=0)\n\nclf_hash = MultinomialNB()\nclf_hash.fit(X_train, y_train)\npred_hash = clf_hash.predict(X_test)\nscore_hash = metrics.accuracy_score(y_test, pred_hash)\nprint(\"accuracy:   %0.3f\" % score_hash)\ncm = metrics.confusion_matrix(y_test, pred_hash)\nplot_confusion_matrix(cm, classes=['FAKE', 'REAL'])","39153292":"from prettytable import PrettyTable\nx = PrettyTable()\n\nx.field_names = [\"S No.\", \"Vectorizer\", \"Accuracy\"]\n\nx.add_row([\"1\",\"CountVectorizer\", 0.956])\nx.add_row([\"2\",\"PassiveAggressiveClassifier - CountVectorizer\", 0.994])\nx.add_row([\"3\",\"TfidfVectorizer\", 0.946])\nx.add_row([\"4\",\"PassiveAggressiveClassifier - TfidfVectorizer\", 0.982])\nx.add_row([\"5\",\"HashingVectorizer\", 0.94])\n\nprint(x)","a6f41282":"### Vocabulary size\nvoc_size=5000\n\nonehot_repr=[one_hot(words,voc_size)for words in corpus] \nprint(onehot_repr[0])\nsent_length=20\nembedded_docs=pad_sequences(onehot_repr,padding='pre',maxlen=sent_length)\nprint(embedded_docs[0])\nlen(embedded_docs),y.shape","546bc3ac":"## Creating model\nembedding_vector_features = 40\nmodel = Sequential()\nmodel.add(Embedding(voc_size,embedding_vector_features,input_length=sent_length))\nmodel.add(LSTM(100))\nmodel.add(Dense(1,activation='sigmoid'))\nmodel.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\nprint(model.summary())","517de9fc":"X_final=np.array(embedded_docs)\ny_final=np.array(y)\n\nX_train, X_test, y_train, y_test = train_test_split(X_final, y_final, test_size=0.33, random_state=42)\n\n### Finally Training\nhistory = model.fit(X_train,y_train,validation_data=(X_test,y_test),epochs=10,batch_size=64)\ny_pred = model.predict_classes(X_test)\nprint(\"Confusion Matrix: \\n\",confusion_matrix(y_test,y_pred))\nprint(\"Accuracy Score: \",accuracy_score(y_test,y_pred))\nprint(classification_report(y_test,y_pred))","bed0668d":"## Creating model - Bidirectional, no dropout\nembedding_vector_features=40\nmodel2 = Sequential()\nmodel2.add(Embedding(voc_size,embedding_vector_features,input_length=sent_length))\nmodel2.add(Bidirectional(LSTM(100)))\nmodel2.add(Dense(1,activation='sigmoid'))\nmodel2.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\nprint(model2.summary())","b00b2d26":"print(len(embedded_docs),y.shape)\nX_final=np.array(embedded_docs)\ny_final=np.array(y)\n\nX_train, X_test, y_train, y_test = train_test_split(X_final, y_final, test_size=0.33, random_state=42)\n\n# Training\nhistory2 = model2.fit(X_train,y_train,validation_data=(X_test,y_test),epochs=10,batch_size=64)\ny_pred2=model2.predict_classes(X_test)\n\nprint(\"Confusion Matrix: \\n\",confusion_matrix(y_test,y_pred2))\nprint(\"Accuracy Score: \",accuracy_score(y_test,y_pred2))\nprint(classification_report(y_test,y_pred2))","74e1d16b":"## Creating model\nembedding_vector_features=40\nmodel1=Sequential()\nmodel1.add(Embedding(voc_size,embedding_vector_features,input_length=sent_length))\nmodel1.add(Bidirectional(LSTM(100)))\nmodel1.add(Dropout(0.3))\nmodel1.add(Dense(1,activation='sigmoid'))\nmodel1.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\nprint(model1.summary())","ec5437ed":"print(len(embedded_docs),y.shape)\nX_final=np.array(embedded_docs)\ny_final=np.array(y)\n\nX_train, X_test, y_train, y_test = train_test_split(X_final, y_final, test_size=0.33, random_state=42)\n\n# Training\nmodel1.fit(X_train,y_train,validation_data=(X_test,y_test),epochs=10,batch_size=64)\ny_pred1=model1.predict_classes(X_test)\n\nprint(\"Confusion Matrix: \\n\",confusion_matrix(y_test,y_pred1))\nprint(\"Accuracy Score: \",accuracy_score(y_test,y_pred1))\nprint(classification_report(y_test,y_pred1))","e77e2f2b":"from prettytable import PrettyTable\nx = PrettyTable()\n\nx.field_names = [\"S No.\", \"Deep Learning\", \"Accuracy\"]\n\nx.add_row([\"1\",\"LSTM\", 0.9433])\nx.add_row([\"2\",\"BiDirectional LSTM\", 0.9431])\nx.add_row([\"3\",\"BiDirectional LSTM + Dropout\", 0.9367])\n\nprint(x)","4cca59a6":"<a id=\"6\"><\/a>\n<font size=\"+2\" color=\"blue\"><b>Passive Aggressive Classifier for CountVectorizer<\/b> <\/font><br>","f440629e":"<a id=\"12\"><\/a>\n<font size=\"+2\" color=\"blue\"><b>PassiveAggressiveClassifier for TfidfVectorizer<\/b> <\/font><br>","9ee3ee7c":"<a id=\"8\"><\/a>\n<font size=\"+2\" color=\"blue\"><b>TfidfVectorizer<\/b> <\/font><br>","317a3b44":"<a id=\"3\"><\/a>\n<font size=\"+2\" color=\"blue\"><b>Ngrams of True\/Fake news titles<\/b><\/font><br>","e0643ecb":"<a id=\"15\"><\/a>\n<font size=\"+2\" color=\"blue\"><b>LSTM<\/b> <\/font><br>","d8ad3b7a":"<a id=\"14\"><\/a>\n<font size=\"+2\" color=\"blue\"><b>Comparison of various BOW methods <\/b> <\/font><br>","c7fb1295":"<a id=\"7\"><\/a>\n<font size=\"+2\" color=\"blue\"><b>Hyper Parameterization (MultinomialNB) for CountVectorizer <\/b> <\/font><br>","ac3f6400":"<a id=\"5\"><\/a>\n<font size=\"+2\" color=\"blue\"><b>Count Vectorizer<\/b> <\/font><br>","8db50914":"# **Friends, if you like my notebook. Please upvote this notebook.**","30bf61b8":"<font size=\"+4\" color=teal><u><center>Fake News Classifier <\/center><\/u><\/font>","f2c381f2":"<a id=\"11\"><\/a>\n<font size=\"+2\" color=\"blue\"><b>Hyperparameterization (with MultinomialNB) for TfidfVectorizer<\/b> <\/font><br>","b49e1da3":"<a id=\"1\"><\/a>\n<font size=\"+2\" color=\"blue\"><b>Title Word Clouds<\/b><\/font><br>","8cbd8d44":"<a id=\"data\"><\/a>\n<font size=\"+2\" color=\"blue\"><b>Cleaning data<\/b><\/font><br>","4398896c":"<a id=\"intro\"><\/a>\n<font size=\"+2\" color=\"blue\"><b>Introduction and Imports<\/b><\/font><br>\n\n<font size=\"+1\" color=\"magenta\">\nThere are 2 files one which has true news and the other fake news.\n<\/font>","afd910f4":"<a id=\"4\"><\/a>\n<font size=\"+2\" color=\"blue\"><b>Removal of Stopwords<\/b> <\/font><br>","ce10be4c":"<a id=\"13\"><\/a>\n<font size=\"+2\" color=\"blue\"><b>Hashing Vectorizer<\/b> <\/font><br>","f1af8862":"<a id=\"2\"><\/a>\n<font size=\"+2\" color=\"blue\"><b>Title\/Text Length of True\/Fake news<\/b><\/font><br>","839707fb":"<a id=\"top\"><\/a>\n\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\"  role=\"tab\" aria-controls=\"home\">Table of contents<\/h3>\n\n* [Introduction](#intro)\n* [Data cleaning and Feature extraction](#data)\n* [1.  Title - Word Clouds ](#1)\n* [2.  Length - Title\/Text ](#2)\n* [3.  Ngrams - Title words](#3)\n* [4.  Removal of stopwords](#4)\n* [5.  Count Vectorizer](#5)\n* [6.  Passive Aggressive Classifier Classifier for CountVectorizer](#6)\n* [7.  Hyper Paramterization with Multinomial NB for CountVectorizer](#7)\n* [8.  TfidfVectorizer](#8)\t\n* [9.  Hyperparameterization (with MultinomialNB) for TfidfVectorizer](#11)\n* [11. PassiveAggressiveClassifier for TfidfVectorizer](#12)\n* [12. Hashing Vectorizer](#13)\n* [13. Comparison Table](#14)\n* [14. LSTM](#15)\n    \n"}}