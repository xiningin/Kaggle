{"cell_type":{"40c1bd53":"code","93ef49d1":"code","9619b060":"code","28df0283":"code","dc296f59":"code","ccaffe39":"code","adadb67a":"code","ff64df26":"code","a0b7f865":"code","6beca3ca":"markdown","813df36d":"markdown","4ccce8c5":"markdown","5e5945f8":"markdown","1080ca44":"markdown","c42c19f6":"markdown"},"source":{"40c1bd53":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport re, string\nfrom scipy.special import softmax\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.pipeline import make_union\nfrom scipy.sparse import hstack","93ef49d1":"train = pd.read_csv('..\/input\/jigsaw-toxic-comment-classification-challenge\/train.csv.zip').fillna(' ')\ntest = pd.read_csv('..\/input\/jigsaw-toxic-comment-classification-challenge\/test.csv.zip').fillna(' ')","9619b060":"train_text = train['comment_text']\ntest_text = test['comment_text']\nall_text = pd.concat([train_text, test_text])\nclass_names = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']","28df0283":"word_vec = TfidfVectorizer(\n                    sublinear_tf=True,\n                    strip_accents='unicode',\n                    analyzer='word',\n                    token_pattern=r'\\w{1,}',\n                    ngram_range=(1, 2),\n                    max_features=30000)\n\nchar_vec = TfidfVectorizer(\n                    sublinear_tf=True,\n                    strip_accents='unicode',\n                    analyzer='char',\n                    ngram_range=(1, 3),\n                    max_features=30000)\n\nvec1 = make_union(word_vec, char_vec)\nvec1.fit(all_text)\ntrain_features = vec1.transform(train_text)\ntest_features = vec1.transform(test_text)","dc296f59":"re_tok = re.compile(f'([{string.punctuation}\u201c\u201d\u00a8\u00ab\u00bb\u00ae\u00b4\u00b7\u00ba\u00bd\u00be\u00bf\u00a1\u00a7\u00a3\u20a4\u2018\u2019])')\ndef tokenize(s): \n    return re_tok.sub(r' \\1 ', s).split()\n\nvec2 = TfidfVectorizer(ngram_range=(1,2), \n                      tokenizer=tokenize,\n                      min_df=3, \n                      max_df=0.9, \n                      strip_accents='unicode', \n                      use_idf=1,\n                      smooth_idf=1, \n                      sublinear_tf=1)\ntrain_term_doc = vec2.fit_transform(train_text)\ntest_term_doc = vec2.transform(test_text)","ccaffe39":"# weighted logistic regression\nscores1 = []\nsub1 = pd.DataFrame.from_dict({'id': test['id']})\nfor class_name in class_names:\n    train_target = train[class_name]\n    classifier = LogisticRegression(C = 0.1, solver='saga', class_weight='balanced')\n\n    cv_score = np.mean(cross_val_score(classifier, train_features, train_target, cv=3, scoring='roc_auc'))\n    scores1.append(cv_score)\n    print('CV score for class {} is {:.4%}'.format(class_name, cv_score))\n\n    classifier.fit(train_features, train_target)\n    sub1[class_name] = classifier.predict_proba(test_features)[:, 1]\n\n#print('Total WLR CV score is {:.4%}'.format(np.mean(scores1)))","adadb67a":"# nb-svm\ntrain_x = train_term_doc\ntest_x = test_term_doc\n\ndef pr(y_i, y):\n    p = train_x[y==y_i].sum(0)\n    return (p+1) \/ ((y==y_i).sum()+1)\n\ndef get_mdl(y):\n    y = y.values\n    r = np.log(pr(1,y) \/ pr(0,y))\n    m = LogisticRegression(C=4, solver='liblinear', dual=True)\n    x_nb = train_x.multiply(r)\n    return m.fit(x_nb, y), r\n\nscores2 = []\nsub2 = pd.DataFrame.from_dict({'id': test['id']})\nfor i, j in enumerate(class_names):\n    m,r = get_mdl(train[j])\n    sub2[j] = m.predict_proba(test_x.multiply(r))[:,1]\n    y_pred = m.predict_proba(train_x.multiply(r))[:,1]\n    print('fit score for class {} is {:.4%}'.format(j, roc_auc_score(train_target, y_pred)))\n    scores2.append(roc_auc_score(train_target, y_pred))","ff64df26":"sub = sub1.copy()\nfor i, j in enumerate(class_names):\n    pb = np.array([scores1[i], scores2[i]])\n    weights = lambda x: x\/sum(x)\n    w = weights(pb)\n    print(w)\n    sub[j] = sub1[j]*w[0] + sub2[j]*w[1]","a0b7f865":"sub.to_csv('submission.csv', index=False)","6beca3ca":"## Build and Run model","813df36d":"## Import libraries","4ccce8c5":"## Read data","5e5945f8":"## Submission","1080ca44":"### References\nDo the ensemble and add weights for the logistic regression of the following notebooks:\n* Notebook 1: [Link](https:\/\/www.kaggle.com\/xhlulu\/jigsaw-tpu-xlm-roberta)\n* Notebook 2: [Link](https:\/\/www.kaggle.com\/swannnn\/jigsaw-tpu-xlm-roberta-e3ad07)","c42c19f6":"## Vectorization"}}