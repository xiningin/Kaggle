{"cell_type":{"ceeb0552":"code","788cb4ea":"code","3863bb1b":"code","0f0835fc":"code","43c08f73":"code","1c6b421a":"code","b1c71214":"code","8ea16d2a":"code","37aa0ecb":"code","40f55dfe":"code","8456af56":"code","0992fff7":"code","088d88a3":"code","3e471e93":"code","fbab826e":"markdown","61db78bf":"markdown","c6d6ea67":"markdown","252ef32f":"markdown","02a36e2a":"markdown","21e873ae":"markdown","4dca9246":"markdown","1e21b618":"markdown","41863625":"markdown","3ec220dc":"markdown","c8401fd5":"markdown","dcc88ee6":"markdown","ad9bd22a":"markdown","81163be0":"markdown","0140e039":"markdown","a96c4836":"markdown","1a6f5183":"markdown","e4dffa97":"markdown"},"source":{"ceeb0552":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt # for plots\n#import seaborn as sns #fo\n%matplotlib inline\nimport os\nimport warnings\nwarnings.filterwarnings('ignore')","788cb4ea":"print(os.listdir(\"..\/input\"))\ndata = pd.read_csv('..\/input\/horse.csv')\ndata.head()","3863bb1b":"#print(\"Shape of data (samples, features): \",data.shape)\ndata.dtypes.value_counts()\n\nnan_per=data.isna().sum()\/len(data)*100\nplt.bar(range(len(nan_per)),nan_per)\nplt.xlabel('Features')\nplt.ylabel('% of NAN values')\nplt.plot([0, 25], [40,40], 'r--', lw=1)\nplt.xticks(list(range(len(data.columns))),list(data.columns.values),rotation='vertical')\n\nobj_columns=[]\nnonobj_columns=[]\nfor col in data.columns.values:\n    if data[col].dtype=='object':\n        obj_columns.append(col)\n    else:\n        nonobj_columns.append(col)\n#print(len(obj_columns),\" Object Columns are \\n\",obj_columns,'\\n')\n#print(len(nonobj_columns),\"Non-object columns are \\n\",nonobj_columns)\n\ndata_obj=data[obj_columns]\ndata_nonobj=data[nonobj_columns]\n\n#print(\"Data Size Before Numerical NAN Column(>40%) Removal :\",data_nonobj.shape)\nfor col in data_nonobj.columns.values:\n    if (pd.isna(data_nonobj[col]).sum())>0:\n        if pd.isna(data_nonobj[col]).sum() > (40\/100*len(data_nonobj)):\n            #print(col,\"removed\")\n            data_nonobj=data_nonobj.drop([col], axis=1)\n        else:\n            data_nonobj[col]=data_nonobj[col].fillna(data_nonobj[col].median())\n#print(\"Data Size After Numerical NAN Column(>40%) Removal :\",data_nonobj.shape)\n\nfor col in data_obj.columns.values:\n    data_obj[col]=data_obj[col].astype('category').cat.codes\ndata_merge=pd.concat([data_nonobj,data_obj],axis=1)\n\ntarget=data['outcome']\ntemp=target\n#print(target.value_counts())\ntarget=data_merge['outcome']\n#print(target.value_counts())\n\ntrain_corr=data_merge.corr()\n#sns.heatmap(train_corr, vmax=0.8)\ncorr_values=train_corr['outcome'].sort_values(ascending=False)\ncorr_values=abs(corr_values).sort_values(ascending=False)\n#print(\"Correlation of mentioned features wrt outcome in ascending order\")\n#print(abs(corr_values).sort_values(ascending=False))\n\n#print(\"Data Size Before Correlated Column Removal :\",data_merge.shape)\n\nfor col in range(len(corr_values)):\n        if abs(corr_values[col]) < 0.1:\n            data_merge=data_merge.drop([corr_values.index[col]], axis=1)\n            #print(corr_values.index[col],\"removed\")\n#print(\"Data Size After Correlated Column Removal :\",data_merge.shape)","0f0835fc":"data_merge.head()","43c08f73":"data.describe()","1c6b421a":"from sklearn.preprocessing import StandardScaler\nXstd = StandardScaler().fit_transform(data_merge)","b1c71214":"print('Covariance matrix: \\n', np.cov(Xstd.T))","8ea16d2a":"cov_mat = np.cov(Xstd.T)\neigen_values, eigen_vectors = np.linalg.eig(cov_mat)\nprint('Eigen vectors \\n',eigen_vectors)\nprint('\\nEigen values \\n',eigen_values)","37aa0ecb":"pairs = [(np.abs(eigen_values[i]), eigen_vectors[:,i]) for i in range(len(eigen_values))]\npairs.sort()\npairs.reverse()\n\nprint('Eigen Values in descending order:')\nfor i in pairs:\n    print(i[0])","40f55dfe":"tot = sum(eigen_values)\nvar_per = [(i \/ tot)*100 for i in sorted(eigen_values, reverse=True)]\ncum_var_per = np.cumsum(var_per)\n\nplt.figure(figsize=(10,10))\nx=['PC %s' %i for i in range(1,len(var_per))]\nind = np.arange(len(var_per)) \nplt.bar(ind,var_per)\nplt.xticks(ind,x);\nplt.plot(ind,cum_var_per,marker=\"o\",color='orange')\nplt.xticks(ind,x);\n","8456af56":"N=16\nvalue=10\na = np.ndarray(shape = (N, 0))\nfor x in range(1,value):\n    b=pairs[x][1].reshape(16,1)\n    a = np.hstack((a,b))\nprint(\"Projection Matrix:\\n\",a)","0992fff7":"Y = Xstd.dot(a)","088d88a3":"plt.figure(figsize=(10,5))\nplt.subplot(1,2,1)\nfor name in ('died', 'euthanized', 'lived'):\n    plt.scatter(\n        x=Y[temp==name,3],\n        y=Y[temp==name,4],\n    )\nplt.legend( ('died', 'euthanized', 'lived'))\nplt.title('After PCA')\n\nplt.subplot(1,2,2)\nfor name in ('died', 'euthanized', 'lived'):\n    plt.scatter(\n        x=Xstd[temp==name,3],\n        y=Xstd[temp==name,4],\n    )\nplt.title('Before PCA')\nplt.legend( ('died', 'euthanized', 'lived'))\n","3e471e93":"#add PCA ckitlearn shortcut","fbab826e":"Importing required libraries","61db78bf":"Now comparing data visualization before and after PCA, (1st Principle Component vs 2nd)","c6d6ea67":"## Principal Component Analysis \n\n### Dimensionality Reduction\n\nWhen dealing with high-dimensional data it is difficult to visualize or interpret the data; such data sometimes has data redundancy issue due to multicorrelated features. Also, as the number of dimensions increases, computation time and power increases as well. These issues can be solved using ***Dimensionality Reduction***. \n\nIt is the process of reducing the number of random variables under considertion (also known as features) without losing information. It does so by obtaining a set of principle variables. Thus,\n1. Reducing storage space and compress data \n2. Reduce Computation Time and Power\n3. Deals with multicolinearity, i.e. deals with redundant data\n4. Helps in data visualization and interpretation (e.g. Projection into two dimensions)\n\n### Principal Component Analysis\n\nOne way to perform Dimensionality Reduction is through ***Principal Component Analysis (PCA)*** . It is a linear transformation technique used to identify strong patterns in data by finding out variable correlation. It maps the data to a lower dimensional subspace in a way that data variance is maximized while retaining most of the information.\n\nSteps to perform PCA:\n* Step 1. [Data Standardization](#step1) \n* Step 2: [Covariance Matrix](#step2)\n* Step 3: [Eigen Decomposition of Covariance Matrix](#step3)\n* Step 4: [Projection Onto New Feature Space](#step4)\n","252ef32f":"## <a id='step1'>Step 1 - Data Standardization<\/a>\n\nAs PCA deals with variance maximization of two variables, it is important to have both variables on same scale. \n","02a36e2a":"Plot shows that first component bears almost 20% of information, and first 9 components carry 80% information.","21e873ae":"This notebook discusses Principal Component Analysis in Detail using Horsecohlic DataSet. \nData being used here  is 'Horse Colic Dataset' which predicts whether a horse can survive or not based on past medical conditions.\nData is available via following links.\n1.  [Kaggle](http:\/\/www.kaggle.com\/uciml\/horse-colic)\n2. [UCI Machine Learning Repository](http:\/\/archive.ics.uci.edu\/ml\/datasets\/Horse+Colic)","4dca9246":"The standardized data  is then used to find covariance matrix. We need covariance matrix to find eigen values and vectors in the next step. \n\nA Covariance Matrix is used to analyze linear relationship between two variable, i.e. how the variables change together. \nLets say we have two random variables x and y, \n* if x increases as y increases -> positive linear relationship ***(+ve covariance value)***\n* if x decreases as y decreases -> positive linear relationship ***(+ve covariance value)***\n* if x increases as y decreases -> negative linear relationship  ***(-ve covariance value)***\n* if x decreases as y increases -> negative linear relationship  ***(-ve covariance value)***\n\n![Sketch.png](attachment:Sketch.png)","1e21b618":"To check how much aech Principle component represent variation, let us find the cumulative sum of eigen values and plot them.","41863625":"Reshaping eigen pairs to form a projection matrix, which is then multiplied by samples to transform data to new feature space","3ec220dc":"When a random vector is multiplies by a covarinace matrix, it moves towards the direction of greatest variation. In this way we can extract dimension with greatest \nspread of data. \n\nOne way to do it, is to find the exact vector that doesn't converge (i.e. its direction doesnt change), means it is already giving the maximum data variance.\nIt is done by computing eigen values and vectors where eigen values represent the scale of vector and eigen vector represent the direction, given by\n\n***Covariance Matrix x Vector = Scalar Component x Vector***\n\nwhere, Vector is the eigen vector \n            and Scalar component is the eigen value\n![Sketch_1.png](attachment:Sketch_1.png)\n            ","c8401fd5":"**Reerences:**\n\n[Principle Component Analysis in Python](http:\/\/plot.ly\/ipython-notebooks\/principal-component-analysis\/)","dcc88ee6":"Data is then cleaned and converted to numeric form for processing and returned as **data_merge**.\n\nTutorial link for data cleaning - [Data Cleaning](http:\/\/www.kaggle.com\/sabasiddiqi\/data-examination-and-cleaning).","ad9bd22a":"Reading data from **CSV file** and saving as **Pandas' Dataframe**","81163be0":"# Principal Component Analysis","0140e039":"Now, \nMaking list of eigen values and vectors and sorting the list w.r.t eigen values (Descending order) where highest eigen values represent highest variation. ","a96c4836":"## <a id='step2'>Step 2 - Covariance Matrix<\/a>","1a6f5183":"## <a id='step3'>Step 3 - Eigen Decomposition of Covariance Matrix<\/a>","e4dffa97":"## <a id='step4'>Step 4 - Projection onto New Feature Space<\/a>"}}