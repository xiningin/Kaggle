{"cell_type":{"1579b47f":"code","9f57d9fa":"code","e1ebeb2f":"code","3838633d":"code","92bf4bcb":"code","f252b0b3":"code","6a0c76ee":"code","de898cdd":"code","0f8fa1f9":"code","3e5742e0":"code","64a6e4af":"code","4ab16bd5":"code","c34d794d":"code","53d1b7e3":"code","b505060e":"code","e4f0283b":"code","85db1402":"code","881d718d":"code","f8952f0e":"code","8f277b2b":"code","d88f3d36":"code","87446f72":"code","bd6761d5":"code","6d33b849":"code","2f1ab495":"markdown","7636d5a8":"markdown","5bd5cd9a":"markdown","abdfd9e3":"markdown","3346fef3":"markdown","ed9f373d":"markdown","039093e4":"markdown","06a88fbb":"markdown","a83d6460":"markdown","344a2204":"markdown","439b9ade":"markdown","2c528469":"markdown","8050e29c":"markdown","31238986":"markdown","c81c5f12":"markdown","92d0b027":"markdown","b5109fb7":"markdown","0a1c5700":"markdown","d35b23ac":"markdown","36d8c59e":"markdown","afaa56e3":"markdown","0ae3cddc":"markdown","845b34c4":"markdown","39497e33":"markdown","5118a1ef":"markdown"},"source":{"1579b47f":"!pip install alibi","9f57d9fa":"%matplotlib inline\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom alibi.explainers.ale import ALE, plot_ale","e1ebeb2f":"data = pd.read_csv('..\/input\/heart-disease-cleveland-uci\/heart_cleveland_upload.csv')\n# To display the top 5 rows\ndata.head(5)","3838633d":"heart = data.copy()","92bf4bcb":"target = 'condition'\nfeatures_list = list(heart.columns)\nfeatures_list.remove(target)","f252b0b3":"y = heart.pop('condition')","6a0c76ee":"X_train, X_test, y_train, y_test = train_test_split(heart, y, test_size=0.2, random_state=33)","de898cdd":"lr = LogisticRegression(max_iter=1000)\nlr.fit(X_train, y_train)","0f8fa1f9":"accuracy_score(y_test, lr.predict(X_test))","3e5742e0":"proba_fun_lr = lr.predict_proba","64a6e4af":"proba_ale_lr = ALE(proba_fun_lr, feature_names=features_list, target_names=[0,1])","4ab16bd5":"proba_exp_lr = proba_ale_lr.explain(X_train.values)","c34d794d":"plot_ale(proba_exp_lr, n_cols=3, fig_kw={'figwidth': 12, 'figheight': 15});","53d1b7e3":"plot_ale(proba_exp_lr, features=[2]);","b505060e":"x_train=X_train.to_numpy()\nfig, ax = plt.subplots()\nfor target in range(2):\n    ax.hist(x_train[y_train==target][:,2],label=target);\n\nax.set_xlabel(features_list[2])\nax.legend();","e4f0283b":"from sklearn.ensemble import GradientBoostingClassifier","85db1402":"gb = GradientBoostingClassifier()\ngb.fit(X_train, y_train)","881d718d":"accuracy_score(y_test, gb.predict(X_test))","f8952f0e":"proba_fun_gb = gb.predict_proba","8f277b2b":"proba_ale_gb = ALE(proba_fun_gb, feature_names=features_list, target_names=['0','1'])","d88f3d36":"proba_exp_gb = proba_ale_gb.explain(X_train.values[:,:])","87446f72":"gb.feature_importances_","bd6761d5":"plot_ale(proba_exp_gb, n_cols=2, fig_kw={'figwidth': 15, 'figheight': 15})","6d33b849":"fig, ax = plt.subplots(1, 2, figsize=(8, 4), sharey='row');\nplot_ale(proba_exp_lr, features=[11,2], targets=[1], ax=ax, line_kw={'label': 'LR'});\nplot_ale(proba_exp_gb, features=[11,2], targets=[1], ax=ax, line_kw={'label': 'GB'});","2f1ab495":"Plotting the ALE plots based on feature effects on the probabilities of each class","7636d5a8":"The accuracy is good enough.\n","5bd5cd9a":"By printing out the feature importances, it can be deduced that ca and thal are the most important features fbs and restecg are the least important.","abdfd9e3":"Splitting the data into training and testing set","3346fef3":"In this plot,the ALE lines cross the x-axis at approximately 1.6cm, which suggests that for instances of cp around ~1.6cm, the feature effect on the prediction is the same as the average feature effect. Also, going towards the extreme values of the feature, the model assigns a large positive value for 0 and  large negative penalty towards 3 for classification of \"0\" and vice versa for 1. In other words, the feature effect and the probability is more for instances of higher values of cp when the prediction is 1(having heart disease), and similarly when the cp value is lower, there is more probabolity of no heart disease, as expected.","ed9f373d":"# 2. Using Gradient Boosting Model :","039093e4":"Now, we see the effects of ALE on a non linear model using Gradient Boosting.\n","06a88fbb":"Fitting a logistic regression model","a83d6460":"# Using Logistic Regression Model","344a2204":"Now we plot a histogram for cp","439b9ade":"It is clearly seen that as the cp value increases, more no of patients are likely to get the disease.","2c528469":"ALE, like PDP (Partial Dependence Plots) is a model agnostic technique useful for the explaining and visualizing the effects of each feature on the results of black box models. However, it is better compared to PDP as it addresses the primary drawbacks of the same. PD Plots ignore the correlation between the features and considers them to be independent which leads to erroneous results when the features of the dataset are highly correlated. ALE, on the other hand, produce good results in spite of there being a correlation between features and are also less computationally expensive. They visualise the effect that each feature, isolated from all other features, has on the predictions of the model. \n\nA second order ALE plot can also show the combined effects of multiple features on the outcome. \n","8050e29c":"# ALE explanation using Heart Disease dataset","31238986":"Since gradient boosting is applied, the plots are no longer linear.\nAs seen in the above graphs, fbs and restecg have almost flat lines which clearly indicates that they are the least important features. In contrast, the plot of ca shows a quite increase in feature effect at higher levels of ca.\n","c81c5f12":"Now we calculate the Accumulated Local Effects using Probability space :","92d0b027":"Evaluation the model","b5109fb7":"###Comparing Logistic Regression and Gradient Boosting","0a1c5700":"The units of Y-axis (ALE) relative probability mass, i.e. given a feature value how much probability the model assigns to each class relative to the mean prediction.\n","d35b23ac":"## Probability space","36d8c59e":"Let us consider cp for example.","afaa56e3":"Plotting the graphs for all features towards the probability space","0ae3cddc":"The model gave a quite good accuracy.","845b34c4":"Testing the accuracy.","39497e33":"We have considered 2 features namely, cp and ca. The following conclusions can be made from the above graphs:\n\n1.   In both models, the feature weight of ca for predicting class '1' is high. It is increasing in nature an has a high positive influence at 3. From about 0 to 0.5 ca, the feature has a high negative influence on predicting the class '1'.\n2.   Conversely, the second graph shows that the feature weight of cp has a high importance in the LR Model, but doesn't affect the predictions much on GB Model.\n\n","5118a1ef":"Reading the Heart disease dataset"}}