{"cell_type":{"b7d0663e":"code","c87db848":"code","78273993":"code","ef4ef66c":"code","f5180039":"code","785911a7":"code","6488f62b":"code","974f480f":"code","37db286e":"code","9dda5c98":"code","ecd0b39b":"code","39e923e0":"code","653fe393":"code","d057a991":"code","b80b1f8a":"code","c2186744":"code","5511a096":"code","14939d10":"code","de40e6c7":"code","82a9c18d":"code","ffeb1b81":"code","9ca5f152":"code","c0ec7ba1":"code","c6c01667":"code","d9451d8a":"code","4a7aa928":"code","cda9d022":"code","0c67bcf2":"code","bed0c5a3":"code","1efde8b6":"code","e3ebc0a2":"markdown","3bd12268":"markdown","37084c1a":"markdown","56aff134":"markdown","14fc003d":"markdown","e844b021":"markdown","eec015a6":"markdown","d4e1bced":"markdown","529eb07b":"markdown","0e334e5f":"markdown","3e59c941":"markdown","c7cfd324":"markdown","6db9ecd7":"markdown","bc6d8429":"markdown","ac429e61":"markdown","5567c4a1":"markdown","e55cf3bc":"markdown","4d465e88":"markdown","19e0ef50":"markdown","34087b9b":"markdown","effa892e":"markdown","014bad8c":"markdown","13342e6a":"markdown","9ff6b705":"markdown","4369a0ac":"markdown","8bb2dff5":"markdown","91061d90":"markdown","7f88257c":"markdown"},"source":{"b7d0663e":"\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport random\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.metrics import accuracy_score\nfrom scipy import stats \nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nimport collections\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n","c87db848":"train = pd.read_csv(\"..\/input\/av-healthcare-analytics-ii\/healthcare\/train_data.csv\")\ntrain.head()","78273993":"test = pd.read_csv(\"..\/input\/av-healthcare-analytics-ii\/healthcare\/test_data.csv\")\ntest.head()","ef4ef66c":"train.shape, test.shape","f5180039":"train.info()","785911a7":"plt.rcParams[\"figure.figsize\"] = 12, 8\nsum_ad_deposit = train.groupby(\"Stay\").agg({\"Admission_Deposit\": \"sum\"})\nsum_ad_deposit.plot(kind = \"bar\")\nplt.title(\"Sum of Admission deposit\")\nplt.show()","6488f62b":"collections.Counter(train[\"Stay\"])","974f480f":"sns.boxenplot(train[\"Stay\"], train[\"Visitors with Patient\"])","37db286e":"train_X = train.drop(\"Stay\", axis = 1) #dropping the dependent variable for preprocessing\nfull_data = pd.concat([train_X, test], ignore_index= True)","9dda5c98":"full_data.isnull().sum()","ecd0b39b":"simple_Impute_median = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\nfull_data[\"Bed Grade\"] = simple_Impute_median.fit_transform(full_data[[\"Bed Grade\"]]).ravel()\nfull_data[\"City_Code_Patient\"] = simple_Impute_median.fit_transform(full_data[[\"City_Code_Patient\"]]).ravel()","39e923e0":"full_data.isnull().sum()","653fe393":"Lab_enc = LabelEncoder()\n\nfor i in full_data.columns:\n    if full_data[i].dtype == \"object\":\n        full_data[i] = Lab_enc.fit_transform(full_data[i])","d057a991":"y = Lab_enc.fit_transform(train[\"Stay\"])","b80b1f8a":"def metric(model, pred, y_valid ):\n    if hasattr(model, 'oob_score_'): \n        return (accuracy_score(y_valid, pred)) * 100, model.oob_score_\n    else:\n        return (accuracy_score(y_valid, pred)) * 100\n\ndef get_sample(df,y, number):\n    df = df.sample(number)\n    return df, y[df.index]\n\ndef split(X, y, pct = 0.2):\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=pct, stratify = y )\n    return X_train, X_valid, y_train, y_valid\n\n\ndef feat_imp(model, cols):\n    return pd.DataFrame({\"Col_names\": cols, \"Importance\": model.feature_importances_}).sort_values(\"Importance\", ascending=False)\n\ndef plot_i(fi, x, y):\n    return fi.plot(x, y, \"barh\", figsize = (12,8))\n\ndef create_csv(preds):\n    cols = [\"case_id\", \"Stay\"]\n    sub = pd.DataFrame({\"case_id\": test[\"case_id\"], \"Stay\": preds})\n    sub = sub[cols]\n    \n    return sub.to_csv(\"submission.csv\", index = False)\n","c2186744":"#using the helper functions above\n\nsample_X, sample_y = get_sample(full_data[:train.shape[0]], y, 30000)\nsample_X.shape, sample_y.shape","5511a096":"x_train , x_valid, y_train, y_valid = split(sample_X, sample_y)","14939d10":"%%time\nRf = RandomForestClassifier(oob_score=True) \nmodel = Rf.fit(x_train, y_train)\npreds = model.predict(x_valid)\nprint(metric(model, preds, y_valid))","de40e6c7":"%%time\nRf = RandomForestClassifier(n_estimators=160, max_features=0.5, min_samples_leaf= 5, oob_score=True) \nmodel = Rf.fit(x_train, y_train)\npreds = model.predict(x_valid)\nprint(metric(model, preds, y_valid))","82a9c18d":"feat10 = feat_imp(model, sample_X.columns)\nfeat10[:10]","ffeb1b81":"file4 = pd.read_csv(\"..\/input\/av-healthcare-analytics-ii\/healthcare\/train_data_dictionary.csv\")\nfile4","9ca5f152":"plot_i(feat10, \"Col_names\", \"Importance\")","c0ec7ba1":"#Removing caseid and patient id from sample_x\n\nsample_X = sample_X.drop([\"case_id\", \"patientid\"], axis = 1 )\nsample_X.shape","c6c01667":"x_train , x_valid, y_train, y_valid = split(sample_X, sample_y)","d9451d8a":"%%time\nRf = RandomForestClassifier(n_estimators=160, max_features=0.5, min_samples_leaf= 5, oob_score=True) \nmodel = Rf.fit(x_train, y_train)\npreds = model.predict(x_valid)\nprint(metric(model, preds, y_valid))","4a7aa928":"feat10 = feat_imp(model, sample_X.columns)\nfeat10[:12]","cda9d022":"from lightgbm import LGBMClassifier\ncat= ['Hospital_code', 'Hospital_code', 'Hospital_code', 'Hospital_region_code', 'Department', 'Ward_Type', 'Ward_Facility_Code', \n              'City_Code_Patient', 'Type of Admission', 'Severity of Illness', 'Age']\n\n\nmodel_Lgm = LGBMClassifier(random_state=45)\nmodel_Lgm.fit(x_train, y_train, categorical_feature=cat)\npreds = model_Lgm.predict(x_valid)\nprint(metric(model_Lgm, preds, y_valid))","0c67bcf2":"X = full_data[:train.shape[0]]\ny = y\nX.shape, y.shape","bed0c5a3":"x_train, x_valid, y_train, y_valid = split(X, y)\nx_train.shape, x_valid.shape, y_train.shape, y_valid.shape","1efde8b6":"%%time\nmodel_Lgm = LGBMClassifier(n_estimators=160, num_leaves=32, max_depth=5, reg_lambda= 0.3, random_state=46, n_jobs = -1)\nmodel_Lgm.fit(x_train, y_train, categorical_feature=cat)\npreds = model_Lgm.predict(x_valid)\nprint(metric(model_Lgm, preds, y_valid))","e3ebc0a2":"Lets try building a baseline model and then further develop on it.","3bd12268":"Check if there are any NaN values in the dataset","37084c1a":"## Building our Model\n\nAs we now have a sample size of 30000, we dont know if it represents our dataset in a proper manner but we can surely check and experiment with this size of data.\n\nLets build our first Model.\n\nMy goto model for any Machine Learning problem is always a **RandomForest** because it gives you a great baseline without doing much and gives you good insights of your dataset with which you can build on further.","56aff134":"As we did see some improvement in our Performance metric, also our Oobscore improved we can surely say that it has improved our model to some extent. You can further play with the hyperparameters using a gridsearch maybe and try to get better results at this point.\n\nLets further see some insights from the model:","14fc003d":"Any feedback suggestions would be highly appreciated\n\nWork In Progress ... ","e844b021":"As expected as the stay increases the visitors with patient also increases.","eec015a6":"Lets now check how Visitors with Patients affects the Stay variable","d4e1bced":"Also lets get our dependent variable encoded using Labelencoder","529eb07b":"That looks great, we have beaten our previous score by Random Forest by a nice margin. As we have a good score on this lets try using the model on our complate dataset to see how it performs. We would also use some hyper params to see if we can overall improve the score on our entire dataset. We would also use the regularization param to control overfitting of the data.","0e334e5f":"As we see that our dataset does have some Nan values which we can treat by simply imputing the \"median\" of the column. Though the column is a float, I still consider it as a category hence the imputation is a median. ","3e59c941":"As we can see that removing those 2 columsn we did have much impact on our model performace, though we dropped a bit but it has changed the dynamics of how the feature importance to great extent. ","c7cfd324":"Lets first use the LGBM with all the default parameters to see how it performs on our sample dataset. I prefer specifying the categorical features manually to the model.\n\nFor more information about the algorithm, please look out in this excellent explanation abour the wrking and also hyperparameters:\nhttps:\/\/towardsdatascience.com\/understanding-lightgbm-parameters-and-how-to-tune-them-6764e20c6e5b","6db9ecd7":"As we have dropped the 2 columns lets agains split the dataset and build a model on it.","bc6d8429":"Looking at the dataset we can say that it consist of both numerical and categorical variables, which needs to be treated, so that they can be used in building our model.","ac429e61":"### Sampling the Data\n\nAs we can see that the dataset is of a decent size we should always experiment with data which we can get outputs in matter of seconds rather than training our algorithm on an entire dataset. So we will work on a sample of our dataset.\n\n\nIn this section we are sampling the dataset without thinking much about the \"Balance of dependent variable\" in our sample, further we will try to tackle this stratified sampling. But for now lets keep it simple.","5567c4a1":"### Basic EDA and dataset insights","e55cf3bc":"We can clearly see that the Admission Deposits are very high for certain classes (11-20, 21-30, 31-40)and low for others. Lets check if the classes count has something to do with this.","4d465e88":"Now that our dataset is clear with no Nan Values, we can concentrate on getting the \"object\" dtype to numbers. For this we will use LabelEncoder, we can also use various other techniques like Onehotecoding, pandas.dummies etc but for now we will go with the basic.\n\n## LabelEncoding the categorical variables","19e0ef50":"Yes, the classes count is affecting the admission deposit because as the classes count are more for the above 3 classes (11-20, 21-30, 31-40), the admission deposits also increase with it.","34087b9b":"This notebook is a simple starter where you can get some basic EDA and a Baseline models which can help you crack the 0.42 mark on the leaderboard","effa892e":"#### Special Mention OOB-Score\n***Here the 2nd score that you see is the **\"out of bag score\", which I think is the best tool everybody should use to check check how is your model performing not just on the test data but also a test dataset which your Randomforest algorithm creates while creating an estimator. This helps us a lot in understanding if we are overfitting the data.\nConsider it as a testset which your algorithm creates for every tree. ***\n\nNow that we have our baseline model, lets try tuning in using some Hyperparameters from the algorithm\n","014bad8c":"### Helper Functions","13342e6a":"From the above graph we can see that Admission_Deposit and Visitor with Patient are the most important variables. Surprisingly \"case_id\" and \"patient_id\" are also very important to the model. \n\nBut we can surely say that these would be some unique values in the dataset which can be removed. Lets experiment with the same model hyperparameters by removing these 2 columsn and see how it affects our model performance.","9ff6b705":"- Max_featues: This is a very important hyperparameter which tells the algorithm - what ratio of features in the dataset are available for every tree while its being created. For ex: if our dataset has 20 features, it has a choice from the 10 features to at **every node** for splitting\n  Values which work for me: **1** , **0.5**, **\"sqrt\"**\n\n- Min samples leaf: This is something which tells how many minimum values can a leaf node. Split no further than it. Values which work for me: **1, 3, 5, 10, 25**\n\nFor now these two are sufficient as we want to get a good baseline which can get us in top 40%","4369a0ac":"Wow, this looks great, the model has worked really well on the entire dataset. Maybe we can also tune the parameters more using Gridsearch to obtain better results. This we would try on our sample dataset.","8bb2dff5":"Lets further improve, by trying different models and getting to know how they perform on the dataset.\n\n## LGBM ","91061d90":"Before we build our model there one more important step - removing a small portion from our dataset for vaildation. This helps us to see how is our model performing on unseen data.\n\n\n    ","7f88257c":"Here we are given a problem where we need to classify the saty of the patients in the hospital given some features. As this is a Multiclass classification problem, we might see a imbalanced dataset issue, but we will try to tackle it later.\n\nLets begin by reading our train and test files"}}