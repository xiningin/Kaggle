{"cell_type":{"d1d03bf4":"code","3e83c64c":"code","e77c868c":"code","8040a316":"code","523c7be6":"code","09263430":"code","2d89989c":"code","60cdd108":"code","63160df4":"code","999b5367":"code","e9544b47":"code","0af3ba1e":"code","b7687d72":"code","bd2163ec":"code","87d709b0":"code","c525356e":"code","aa129c57":"code","46f509ec":"code","fd1b0c38":"code","9c59fe8a":"code","e72cc089":"code","3fc5fdcf":"code","6adb9387":"markdown","6bacca91":"markdown","8a9498d0":"markdown","6e0b99c2":"markdown","e01ed66e":"markdown","5140205d":"markdown","efb89b27":"markdown","8c758d48":"markdown","7a7d47a8":"markdown","bb3ad8b7":"markdown","c2a7932b":"markdown","0f37e156":"markdown","38028eb8":"markdown","41ab907d":"markdown","e0fcf89d":"markdown"},"source":{"d1d03bf4":"#importing the necessary libraries\nimport numpy as np\nimport keras\nimport shutil\nimport os\nimport random\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom keras.preprocessing.image import ImageDataGenerator","3e83c64c":"#assigning image paths to variables\nmask_data = \"..\/input\/facemask-dataset\/Mask\/Mask\/\" \nno_mask_data = \"..\/input\/facemask-dataset\/No Mask\/No Mask\/\"","e77c868c":"total_mask_images = os.listdir(mask_data)\nprint(\"no of mask images:: {}\".format(len(total_mask_images)))\ntotal_nonmask_images = os.listdir(no_mask_data)\nprint(\"no of non-mask images:: {}\".format(len(total_nonmask_images)))","8040a316":"os.makedirs('.\/train\/mask')\nos.makedirs('.\/train\/no mask')\nos.makedirs('.\/test\/mask')\nos.makedirs('.\/test\/no mask')","523c7be6":"for images in random.sample(total_mask_images,100):\n    shutil.copy(mask_data+images, '.\/train\/mask')\nfor images in random.sample(total_mask_images,30):\n    shutil.copy(mask_data+images, '.\/test\/mask')\nfor images in random.sample(total_nonmask_images,100):\n    shutil.copy(no_mask_data+images, '.\/train\/no mask')\nfor images in random.sample(total_nonmask_images,30):\n    shutil.copy(no_mask_data+images, '.\/test\/no mask')","09263430":"train_batch = ImageDataGenerator(rescale=1.\/255, zoom_range=0.2, horizontal_flip=True, vertical_flip=True, shear_range=0.2).\\\n            flow_from_directory('.\/train', target_size=(224,224), batch_size=32, class_mode = 'categorical')\ntest_batch = ImageDataGenerator(rescale=1.\/255).\\\n            flow_from_directory('.\/test', target_size = (224,224), batch_size=32, class_mode='categorical')","2d89989c":"train_batch.class_indices","60cdd108":"class_mask = ['mask', 'no mask']","63160df4":"#import vgg16\nfrom keras.applications.vgg16 import VGG16","999b5367":"#vgg16 accepts image size (224,224) only\nIMAZE_SIZE = [224,224]\nvgg = VGG16(input_shape=IMAZE_SIZE+[3], weights='imagenet', include_top=False)","e9544b47":"vgg.summary()","0af3ba1e":"for layers in vgg.layers:\n    layers.trainable = False","b7687d72":"vgg.summary()","bd2163ec":"flatten_layer = keras.layers.Flatten()(vgg.output)\nprediction_layer = keras.layers.Dense(2, activation='softmax')(flatten_layer)","87d709b0":"model = keras.models.Model(inputs = vgg.input, outputs = prediction_layer)","c525356e":"model.summary()","aa129c57":"model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])","46f509ec":"r = model.fit_generator(train_batch, validation_data=test_batch, epochs=5, steps_per_epoch=len(train_batch), validation_steps=len(test_batch))","fd1b0c38":"plt.plot(r.history['loss'], label = 'train loss')\nplt.plot(r.history['val_loss'], label='val loss')\nplt.legend()\n\n\nplt.plot(r.history['accuracy'], label = 'train acc')\nplt.plot(r.history['val_accuracy'], label='val acc')\nplt.legend()","9c59fe8a":"from keras.preprocessing import image\nfrom keras.applications.imagenet_utils import preprocess_input","e72cc089":"img = image.load_img('..\/input\/facemask-dataset\/No Mask\/No Mask\/No Mask109.jpg', target_size=(224,224))\nx=image.img_to_array(img)\nx = np.expand_dims(x,0)\ny = preprocess_input(x)\npred = class_mask[np.argmax(model.predict(y))]\nprint(pred)\nplt.imshow(img)","3fc5fdcf":"img = image.load_img('..\/input\/facemask-dataset\/Mask\/Mask\/Mask214.jpeg', target_size=(224,224))\nx=image.img_to_array(img)\nx = np.expand_dims(x,0)\ny = preprocess_input(x)\npred = class_mask[np.argmax(model.predict(y))]\nprint(pred)\nplt.imshow(img)","6adb9387":"we see that mask indicates '0' while non mask indicates '1. so let us create a list with this info","6bacca91":"We see that trainable parameter is 14,714,688. As the model is already trained we will avoid this.","8a9498d0":"Now vgg16 is ready. We will look at how the model looks","6e0b99c2":"Now the directory is created let us randomly select 100 images for training and 30 images for testing from each mask and non-masked folder","e01ed66e":"# **Here we are going to classify and predict if a face has mask or not**","5140205d":"Lets test our model","efb89b27":"Now our trainable parameter is 0. We can add our layers in the model","8c758d48":"Now let us see the total no of images in each folder","7a7d47a8":"Our model is doing great but due to lack of training image it may not be perfect. Hopefully you find this useful and do give an upvote.\nThank you","bb3ad8b7":"As we have very less images we will directly use a powerful pretrained model VGG16 to get high accuracy","c2a7932b":"Now as we have less images to work with we will use augmented method to get differnt forms of each image.","0f37e156":"Our model is ready. Lets train","38028eb8":"Finally creating the model","41ab907d":"There is very less images to work with.\nSo first we will create directory for training and testing folder","e0fcf89d":"Well we see training and testing folder has 199 and 30 images respectively with 2 classes."}}