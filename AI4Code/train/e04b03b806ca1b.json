{"cell_type":{"3ff5a4c6":"code","372f5f45":"code","2e2435ea":"code","3b3d9726":"code","7d59c6b2":"code","6b0d0442":"code","252f2450":"code","2c83bb97":"code","c10398a8":"code","0986c992":"code","345c76cc":"code","d222b0e9":"code","343f13eb":"code","1c695c24":"code","c5571e11":"code","2f11e240":"code","5df3f169":"code","86f616ec":"code","3953be9b":"code","19ff59a1":"code","aacd76db":"code","a462b2b2":"code","0dc34036":"markdown","173c0605":"markdown","a6504efb":"markdown","1d6faa54":"markdown","f37833e9":"markdown","99d0a05f":"markdown","4e13a475":"markdown","308d68b1":"markdown","28fe5028":"markdown","a9591378":"markdown","dca07f65":"markdown","cc4ebe7a":"markdown","eb1b4004":"markdown","00d8797e":"markdown","e13e8e64":"markdown","b4b661a1":"markdown","4913ac68":"markdown","f30fdc63":"markdown","15335950":"markdown"},"source":{"3ff5a4c6":"import numpy as np\nimport pandas as pd\n\nimport os","372f5f45":"manifest = pd.read_csv('\/kaggle\/input\/youtube-faces-with-facial-keypoints\/youtube_faces_with_keypoints_full.csv')\nmanifest","2e2435ea":"with np.load('\/kaggle\/input\/youtube-faces-with-facial-keypoints\/youtube_faces_with_keypoints_full_3\/youtube_faces_with_keypoints_full_3\/Natasa_Micic_0.npz') as data:\n    colorImages = data['colorImages']\n    boundingBox = data['boundingBox']\n    landmarks2D = data['landmarks2D']\n    landmarks3D = data['landmarks3D']","3b3d9726":"import matplotlib.pyplot as plt","7d59c6b2":"fig, _ = plt.subplots(1,1)\nplt.axis('off')\n\n# use the created array to output your multiple images. In this case I have stacked 4 images vertically\nax = []\n\nax.append(fig.add_subplot(2,2,1))\nplt.imshow(colorImages[:,:,:,0])\nplt.axis('off')\nplt.scatter(pd.DataFrame(boundingBox[:,:,0]).iloc[:,0],pd.DataFrame(boundingBox[:,:,0]).iloc[:,1],color='red')\nplt.scatter(pd.DataFrame(landmarks2D[:,:,0]).iloc[:,0],pd.DataFrame(landmarks2D[:,:,0]).iloc[:,1],color='blue')\nplt.scatter(pd.DataFrame(landmarks3D[:,:,0]).iloc[:,0],pd.DataFrame(landmarks3D[:,:,0]).iloc[:,1],color='orange',s=10)\n\nax.append(fig.add_subplot(2,2,2))\nplt.imshow(colorImages[:,:,:,1])\nplt.axis('off')\nplt.scatter(pd.DataFrame(boundingBox[:,:,1]).iloc[:,0],pd.DataFrame(boundingBox[:,:,1]).iloc[:,1],color='red')\nplt.scatter(pd.DataFrame(landmarks2D[:,:,1]).iloc[:,0],pd.DataFrame(landmarks2D[:,:,1]).iloc[:,1],color='blue')\nplt.scatter(pd.DataFrame(landmarks3D[:,:,1]).iloc[:,0],pd.DataFrame(landmarks3D[:,:,1]).iloc[:,1],color='orange',s=10)\n\nax.append(fig.add_subplot(2,2,3))\nplt.imshow(colorImages[:,:,:,2])\nplt.axis('off')\nplt.scatter(pd.DataFrame(boundingBox[:,:,2]).iloc[:,0],pd.DataFrame(boundingBox[:,:,2]).iloc[:,1],color='red')\nplt.scatter(pd.DataFrame(landmarks2D[:,:,2]).iloc[:,0],pd.DataFrame(landmarks2D[:,:,2]).iloc[:,1],color='blue')\nplt.scatter(pd.DataFrame(landmarks3D[:,:,2]).iloc[:,0],pd.DataFrame(landmarks3D[:,:,2]).iloc[:,1],color='orange',s=10)\n\nax.append(fig.add_subplot(2,2,4))\nplt.imshow(colorImages[:,:,:,3])\nplt.axis('off')\nplt.scatter(pd.DataFrame(boundingBox[:,:,3]).iloc[:,0],pd.DataFrame(boundingBox[:,:,3]).iloc[:,1],color='red')\nplt.scatter(pd.DataFrame(landmarks2D[:,:,3]).iloc[:,0],pd.DataFrame(landmarks2D[:,:,3]).iloc[:,1],color='blue')\nplt.scatter(pd.DataFrame(landmarks3D[:,:,3]).iloc[:,0],pd.DataFrame(landmarks3D[:,:,3]).iloc[:,1],color='orange',s=10)\n#plt.show()","6b0d0442":"from keras.preprocessing.image import load_img, img_to_array\nfrom matplotlib import animation\nfrom IPython.display import HTML\n%matplotlib inline\n\ndef plot_images(img_list):\n  def init():\n    img.set_data(img_list[0])\n    return (img,)\n\n  def animate(i):\n    img.set_data(img_list[i])\n    return (img,)\n\n  fig = plt.figure()\n  ax = fig.gca()\n  img = ax.imshow(img_list[0])\n  anim = animation.FuncAnimation(fig, animate, init_func=init,\n                                 frames=len(img_list), interval=25, blit=True, repeat=False)\n  return anim\n\nimgs = [colorImages[:,:,:,j] for j in range(240)]\n\nHTML(plot_images(imgs).to_html5_video())","252f2450":"import fnmatch\nimport os\n\nmatches = []\nfor root, dirnames, filenames in os.walk('..\/input\/youtube-faces-with-facial-keypoints'):\n    for filename in fnmatch.filter(filenames, '*.npz'):\n        matches.append(os.path.join(root, filename))","2c83bb97":"matches = pd.DataFrame(matches)\nmatches.columns = ['file_name']","c10398a8":"matches = pd.DataFrame([(i,j) for i in manifest.videoID for j in matches.file_name  if i in j])\nmatches.columns = ['videoID','file_name']\nmatches","0986c992":"manifest = pd.merge(matches,manifest)\nmanifest","345c76cc":"from sklearn.model_selection import train_test_split\nX_train, X_test = train_test_split(manifest,test_size=0.3)","d222b0e9":"X_train.reset_index(drop=True, inplace=True)\nX_test.reset_index(drop=True, inplace=True)","343f13eb":"from torch.utils.data import Dataset, DataLoader\nimport torch","1c695c24":"class FaceKeypointsDataset(Dataset):\n    '''Face Keypoints Dataset'''\n    def __init__(self, dataframe):\n        self.dataframe = dataframe\n    \n    def __len__(self):\n        return len(self.dataframe)\n    \n    def __getitem__(self, idx):\n        with np.load(self.dataframe.file_name[idx]) as data:\n            img = torch.from_numpy(data['colorImages']\/255.)\n            labels = torch.from_numpy(data['landmarks2D'])\n        return (img,labels)","c5571e11":"train_data = FaceKeypointsDataset(X_train)\ntest_data = FaceKeypointsDataset(X_test)\n\ntrain_data_loader = DataLoader(train_data,batch_size=1,shuffle=True)\ntest_data_loader = DataLoader(test_data,batch_size=1,shuffle=False)","2f11e240":"import torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.conv1 = nn.Conv2d(3, 32, 5)\n        self.pool1 = nn.MaxPool2d(2, 2)\n\n        self.conv2 = nn.Conv2d(32,64,3)\n        self.pool2 = nn.MaxPool2d(2, 2)\n\n        self.conv3 = nn.Conv2d(64,128,3)\n        self.pool3 = nn.MaxPool2d(2, 2)\n\n        self.conv4 = nn.Conv2d(128,256,3)\n        self.pool4 = nn.MaxPool2d(2, 2)\n\n        self.conv5 = nn.Conv2d(256,512,1)\n        self.pool5 = nn.MaxPool2d(2, 2)\n\n        self.fc1 = nn.Linear(18432, 1024)\n        self.fc2 = nn.Linear(1024, 136)\n        \n        self.drop1 = nn.Dropout(p = 0.1)\n        self.drop2 = nn.Dropout(p = 0.2)\n        self.drop3 = nn.Dropout(p = 0.25)\n        self.drop4 = nn.Dropout(p = 0.25)\n        self.drop5 = nn.Dropout(p = 0.3)\n        self.drop6 = nn.Dropout(p = 0.4)\n    def forward(self, x):\n      \n        x = self.pool1(F.relu(self.conv1(x)))\n        x = self.drop1(x)\n        x = self.pool2(F.relu(self.conv2(x)))\n        x = self.drop2(x)\n        x = self.pool3(F.relu(self.conv3(x)))\n        x = self.drop3(x)\n        x = self.pool4(F.relu(self.conv4(x)))\n        x = self.drop4(x)\n        x = self.pool5(F.relu(self.conv5(x)))\n        x = self.drop5(x)\n        x = x.view(x.size(0), -1)\n        x = F.relu(self.fc1(x))\n        x = self.drop6(x)\n        x = self.fc2(x)\n        return x","5df3f169":"net = Net()","86f616ec":"net = net.double()","3953be9b":"import torch.optim as optim\n\ncriterion = torch.nn.SmoothL1Loss()\n\noptimizer = optim.Adam(net.parameters(), lr = 0.00001)","19ff59a1":"import torch.nn.functional as F","aacd76db":"net.cuda()","a462b2b2":"for epoch in range(5):  # loop over the dataset multiple times\n\n    running_loss = 0.0\n    epoch_loss = []\n    t = 0\n    for i, data in enumerate(train_data_loader, 0):\n        # get the inputs; data is a list of [inputs, labels]\n        inputs, labels = data\n        inputs = inputs.cuda()\n        labels = labels.cuda()\n        \n        for j in range(inputs.shape[4]):\n            t+=1\n            inp = inputs[:,:,:,:,j]\n            inp = inp.permute(0,3,1,2)\n            inp = F.interpolate(inp, size=(224, 224), mode='bicubic', align_corners=False)\n            lab = labels[:,:,:,j]\n\n            # zero the parameter gradients\n            optimizer.zero_grad()\n\n            # forward + backward + optimize\n            outputs = net(inp)\n            loss = criterion(outputs.view(-1), lab.view(-1))\n            loss.backward()\n            optimizer.step()\n\n            # print statistics\n            running_loss += loss.item()\n            epoch_loss.append(loss.item())\n            if t % 5000 == 4999:    # print every 5000 mini-batches\n                \n                samp = int(np.random.randint(low=0,high=len(test_data)-1,size=1))\n                test1 = test_data[samp][0][:,:,:,0].unsqueeze(0).permute(0,3,1,2).cuda()\n                test1 = F.interpolate(test1, size=(224, 224), mode='bicubic', align_corners=False)\n                plt.imshow(np.moveaxis(test1.cpu().detach().numpy()[0,:,:,:],0,-1))\n                plt.scatter(pd.DataFrame(net(test1).cpu().detach().numpy().reshape(68,2)).iloc[:,0],\n                       pd.DataFrame(net(test1).cpu().detach().numpy().reshape(68,2)).iloc[:,1])\n                plt.title(f'epoch {epoch+1}, batch step: {t}, average loss for last 5000 images:{running_loss\/5000}');\n                plt.show()\n                plt.gcf().show()\n                running_loss = 0.0\n    print(f'Epoch {epoch}, loss: ',np.mean(epoch_loss))\n\n    ","0dc34036":"Import some basic libraries to get us started (sorry for not sticking to the convention of having all library imports at the beginning, but I just import them as I develop, I find it tedious to scroll back up every time to add them here... almost as tedious as typing this explanation. I'm sure reading it is also becoming tedious, so just forgive me okay ;)","173c0605":"Define loss and optimizer","a6504efb":"Import plotting library","1d6faa54":"Create a custom dataset class for the facial keypoints data. We will use the 2D landmarks for prediction","f37833e9":"Create the neural network object","99d0a05f":"Reset indices, to allow proper indexing in the Dataset class defined below:","4e13a475":"Define the Convolutional network we're gonna use:","308d68b1":"Move model to GPU","28fe5028":"Getting the filenames for all of the videos from the various subdirectories","a9591378":"# Facial Keypoint Detection from YouTube videos of celebrity faces\n**Gerhard Viljoen - Machine Learning Engineer**","dca07f65":"Cast the network as a double, since the data seems to be double","cc4ebe7a":"We will train for 5 epochs, since our notebook has to compile within 6 hours as per Kaggle limits, printing an example image from the test set with keypoints overlaid, every 5000 batches, to see how the network gets better over time, instead of evaluating at the end.","eb1b4004":"Match the filenames to the Video IDs in the manifest via partial string matching (if the filename contains the string of the video ID, join on that)","00d8797e":"Import the manifest file with metadata about the videos","e13e8e64":"Plot a few example video frames with facial keypoints (2D: blue, 3D: orange) and bounding boxes (red) overlaid","b4b661a1":"Plot a video of the first example (just wanted to teach myself how to display a video in a notebook, not important for the real investigation)","4913ac68":"Define training and test datasets and pop them into a dataloader each for training","f30fdc63":"Split data into training and test sets: 70\/30 split:","15335950":"Data is provided in .npz file format, turns out it is a format belonging to numpy, allowing one to save several arrays in a single file.\n\nBelow I demonstrate how to load and access elements from the archive:"}}