{"cell_type":{"eee0c1cc":"code","10cbf7e5":"code","d90cf35b":"code","b89a228f":"code","e07291da":"code","00ec0fd8":"code","45e656d1":"code","6ba5e0df":"code","0cf82b2b":"code","a8f3b555":"code","7dac577b":"code","c5f36186":"code","f9fb4547":"code","8221aa5d":"code","78fd7f30":"code","587cd51b":"code","b2240688":"code","6637fb67":"code","2a7ab636":"code","0bdcb6a3":"code","2d7bbcab":"code","f8fb58b7":"code","cf1a957d":"code","c033ab6d":"code","f4e007b4":"code","4ddb2067":"code","0b0648a9":"code","2f4a0acd":"code","3397f695":"code","11f8d651":"code","f8b6e66b":"code","67f8a535":"code","24012d91":"code","c0801c79":"markdown","5be0ecd5":"markdown","15a7ccbf":"markdown","92a19ef6":"markdown","8f625136":"markdown","e9a53d15":"markdown","5105b3ec":"markdown","f9bf3c05":"markdown","f9021985":"markdown","21d14ba9":"markdown","a003cef8":"markdown","55909756":"markdown","f168b996":"markdown","bc605aba":"markdown","da5e58ba":"markdown"},"source":{"eee0c1cc":"import numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","10cbf7e5":"df = pd.read_csv('..\/input\/heart-disease-uci\/heart.csv')\nprint(df.shape)","d90cf35b":"df.head()","b89a228f":"df.info()","e07291da":"# Check Missing Values\npd.DataFrame(df.isna().sum(), columns=['Missing_val_counts'])","00ec0fd8":"hd = ['yes' if i == 1 else 'no' for i in df['target']]\ndf['Heart Disease'] = np.array(hd)","45e656d1":"colors = [\"#FF0B04\", \"#4374B3\"]\nsns.set_palette(sns.color_palette(colors))","6ba5e0df":"sns.barplot(x=['Heart Disease', 'No Heart Disease'], y=df['Heart Disease'].value_counts(), palette=sns.color_palette(colors));\nplt.title('Counts of people with heart disease and without heart disease');\nplt.show()","0cf82b2b":"# Age vs heart disease\nsns.histplot(data=df, x='age', hue='Heart Disease', );\nplt.title('Age Distribution for patients and healthy people');\nplt.show()","a8f3b555":"# max Heart rates vs heart disease\nsns.histplot(data=df, x='trestbps', hue='Heart Disease');\nplt.title('max heart beats for patients and healty people');","7dac577b":"# cholesterol vs heart disease\nsns.histplot(data=df, x='chol', hue='Heart Disease');\nplt.title('cholesterol distribution for patients and healty people');","c5f36186":"fig, (ax1,ax2) = plt.subplots(1,2,figsize=(15,6))\n\nax1.bar(df[df['target']==0].cp.value_counts().index, df[df['target']==0].cp.value_counts(), color=\"#4374B3\");\nax1.set_xlabel('Pain level');\nax1.set_ylabel('Counts');\nax1.set_title('chest pain counts for healthy people');\n\nax2.bar(df[df['target']==1].cp.value_counts().index, df[df['target']==1].cp.value_counts(), color=\"#FF0B04\")\nax2.set_xlabel('Pain level');\nax2.set_ylabel('Counts');\nax2.set_title('chest pain counts for patients');","f9fb4547":"# Features\nX = df.drop(['target', 'Heart Disease'], axis=1)\nX.head()","8221aa5d":"# Labels\ny = df['target']","78fd7f30":"X_encoded = pd.get_dummies(X, columns=['cp', 'restecg', 'slope', 'thal'])\nX_encoded.head()","587cd51b":"# Splitting data into train test\nfrom sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split(X_encoded, y, random_state=42)","b2240688":"from sklearn.tree import DecisionTreeClassifier\n\ndt_init = DecisionTreeClassifier(random_state=42)\ndt_init.fit(x_train, y_train)","6637fb67":"from sklearn.tree import plot_tree\n\nplt.figure(figsize=(15,15))\nplot_tree(dt_init, filled=True, rounded=True, class_names=['No hd', 'has hd'], feature_names=x_train.columns);\nplt.show()","2a7ab636":"# Evaluate on test data\nfrom sklearn.metrics import plot_confusion_matrix\n\nplot_confusion_matrix(dt_init, x_test, y_test, display_labels=['No hd', 'has hd'])","0bdcb6a3":"from sklearn.metrics import recall_score\n\nrecall = recall_score(y_test, dt_init.predict(x_test), average='binary')\nprint('Recall: %.3f' % recall)","2d7bbcab":"from sklearn.metrics import precision_score\n\nrecall = precision_score(y_test, dt_init.predict(x_test), average='binary')\nprint('Precision: %.3f' % recall)","f8fb58b7":"dt_init.score(x_test, y_test)","cf1a957d":"paths = dt_init.cost_complexity_pruning_path(x_train, y_train)  \nccp_alphas = paths.ccp_alphas  # get all effective alphas\nccp_alphas = ccp_alphas[:-1] # Exclude maximum ccp_alpha\n\ndt_models = []\nfor ccp_alpha in ccp_alphas:\n    dt = DecisionTreeClassifier(ccp_alpha=ccp_alpha, random_state=42)\n    dt.fit(x_train, y_train)\n    dt_models.append(dt)\n","c033ab6d":"node_counts = [clf.tree_.node_count for clf in dt_models]\ndepth = [clf.tree_.max_depth for clf in dt_models]\nfig, ax = plt.subplots(2, 1, figsize=(10,10))\nax[0].plot(ccp_alphas, node_counts, marker=\"o\", drawstyle=\"steps-post\")\nax[0].set_xlabel(\"alpha\")\nax[0].set_ylabel(\"number of nodes\")\nax[0].set_title(\"Number of nodes vs alpha\")\nax[1].plot(ccp_alphas, depth, marker=\"o\", drawstyle=\"steps-post\")\nax[1].set_xlabel(\"alpha\")\nax[1].set_ylabel(\"depth of tree\")\nax[1].set_title(\"Depth vs alpha\")\nfig.tight_layout()","f4e007b4":"train_scores = [clf.score(x_train, y_train) for clf in dt_models]\ntest_scores = [clf.score(x_test, y_test) for clf in dt_models]\n\nfig, ax = plt.subplots()\nax.set_xlabel(\"alpha\")\nax.set_ylabel(\"accuracy\")\nax.set_title(\"Accuracy vs alpha for training and testing sets\")\nax.plot(ccp_alphas, train_scores, marker=\"o\", label=\"train\", drawstyle=\"steps-post\")\nax.plot(ccp_alphas, test_scores, marker=\"o\", label=\"test\", drawstyle=\"steps-post\")\nax.legend()\nplt.show()","4ddb2067":"from sklearn.model_selection import cross_val_score\n\nalpha_loop_vals = []\n\nfor ccp_alpha in ccp_alphas:\n    dt = DecisionTreeClassifier(random_state=42, ccp_alpha=ccp_alpha)\n    scores = cross_val_score(dt, x_train, y_train, cv=5)\n    alpha_loop_vals.append([ccp_alpha, np.mean(scores), np.std(scores)])\n    \nalpha_results = pd.DataFrame(alpha_loop_vals, columns=['alpha', 'mean_accuracy', 'std'])\n    \nalpha_results.plot('alpha', y='mean_accuracy', yerr='std', marker='o')","0b0648a9":"best_alpha = 0.0156278","2f4a0acd":"dt_final = DecisionTreeClassifier(random_state=42, ccp_alpha=0.0156278)\ndt_final.fit(x_train, y_train)","3397f695":"plt.figure(figsize=(15,15))\nplot_tree(dt_final, filled=True, rounded=True, class_names=['No hd', 'has hd'], feature_names=x_train.columns);\nplt.show()","11f8d651":"plot_confusion_matrix(dt_final, x_test, y_test, display_labels=['No hd', 'has hd'])","f8b6e66b":"Precision = precision_score(y_test, dt_final.predict(x_test), average='binary')\nprint('Precision: %.3f' % recall)","67f8a535":"recall = recall_score(y_test, dt_final.predict(x_test), average='binary')\nprint('recall: %.3f' % recall)","24012d91":"dt_final.score(x_test, y_test)","c0801c79":"# Training","5be0ecd5":"The pruned tree do better than full sized tree.","15a7ccbf":"# Final Model With best ccp_alpha value","92a19ef6":"From the plot above : the best value for alpha is 0.0156278","8f625136":"# Exploratory Data Analysis","e9a53d15":"## One hot Encoding Categorical Data:\n\nDecision tree can handle continuous data, but it can not handle categorical data with multiple values. so we need to one_hot encode them","5105b3ec":"# Reading data","f9bf3c05":"# Preparing data for training\n\nwe will use decision tree classifier. so in the following cells, we will prepare data for that\n","f9021985":"## Tree Pruning","21d14ba9":"as we can see, the best value for alpha that maximizes test Accuracy is 0.0156278","a003cef8":"The tree seems very depth and there is a possibility that overfitting has occured","55909756":"As we can see: our classifier have misclassified multiple test examples\n\ncan we do better?\n* Lets try to prune the decision tree using cost complexity pruning technique and alpha","f168b996":"## Without cross validation","bc605aba":"## With cross validation","da5e58ba":"The tree gets more simpler after pruning!"}}