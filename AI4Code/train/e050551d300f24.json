{"cell_type":{"03f2be4b":"code","190597cb":"code","a73d4c86":"code","229aefa0":"code","64bcbdd1":"code","5180725f":"code","bd7926c7":"code","282229af":"code","ced60ec9":"code","a4e6287b":"code","eadd016d":"code","02eea55e":"code","821a3c38":"code","b36c8258":"code","758fe564":"code","3b59322e":"code","76034b92":"code","1134d983":"code","de8a30d1":"code","71de1d85":"code","2d5a7944":"code","10efb1b0":"code","5882be28":"code","17b726c6":"code","c6f00cb9":"code","5a427f28":"code","83bdfa61":"code","78e69b50":"code","3cd50582":"code","8e66bd35":"code","6a71ffcd":"code","3d814f55":"code","9ff6d382":"code","0d9959ce":"code","6612d544":"code","c9542f43":"code","b58ec4be":"code","0c1eccec":"code","68961d34":"code","59b67510":"code","d8eac634":"code","b6baf8c2":"code","6eb61b98":"code","b85e1f3b":"code","739fda83":"code","27315a0d":"code","5635b00d":"code","c64b9012":"code","118dbac4":"code","625448d6":"code","e5262a36":"code","883ea7c5":"code","6b7d6439":"code","da024757":"code","01001721":"code","56e1cc82":"code","f42be895":"code","1438abbc":"code","9a8d1742":"markdown","dddf6314":"markdown","8ca94506":"markdown","2dc4c7a1":"markdown","eac680d4":"markdown","3a10e9b1":"markdown","84da923e":"markdown","88395d5f":"markdown","f3f1f203":"markdown","82ae8500":"markdown","621cc931":"markdown","d928b2a0":"markdown","073af7ab":"markdown","40ccafff":"markdown","2495a2ee":"markdown","a8bfad7e":"markdown","7f7b4f9c":"markdown","28eda9c5":"markdown","80efe4ae":"markdown","f0bae6ab":"markdown","8607b8d5":"markdown","6f613384":"markdown","febd8900":"markdown","7e65bf4e":"markdown","1d80bedc":"markdown","b3c2dd9b":"markdown","7fb8d421":"markdown","5803c489":"markdown","381b7bb1":"markdown","70a9735f":"markdown","2b71e370":"markdown","a3c71988":"markdown","4ec37dea":"markdown","411b0f8e":"markdown","621725e3":"markdown","19db0d5d":"markdown","c9fd9e70":"markdown","285c6a7b":"markdown","71e63ee6":"markdown","d98a9f15":"markdown","028de2f7":"markdown","29c7ed89":"markdown","65ae0a9c":"markdown","5dd1e382":"markdown","274e1a62":"markdown","8cd040c8":"markdown","25319670":"markdown","377ad4da":"markdown","bf79bf1e":"markdown","35ae530b":"markdown","4aed45c1":"markdown","b23f2973":"markdown","b7326b06":"markdown","5d0847dd":"markdown","c7fe9da0":"markdown","9775d60f":"markdown","93342c06":"markdown","c306f73f":"markdown","8552289f":"markdown","ec256358":"markdown","d69a0815":"markdown","75aa8c1c":"markdown","9801f720":"markdown","f8a2396a":"markdown","20d06391":"markdown"},"source":{"03f2be4b":"class color_class:\n    BOLD_COLOR = '\\033[1m' + '\\033[93m'\n    BOLD = '\\033[1m'\n    END = '\\033[0m'\n\nprint(color_class.BOLD_COLOR + '\\nImporting all the required libraries....\\n\\n'+ color_class.END)\n\nimport warnings \nwarnings.filterwarnings(\"ignore\")\n\n# Base libraries\nimport os\nimport numpy as np\nimport pandas as pd\nimport re\nimport string\nimport glob\nimport math\nfrom IPython.display import display_html\nimport tqdm\nimport wandb\n\n\n## visualization libraries\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nimport matplotlib.patches as patches\nimport seaborn as sns\n!pip install pywaffle\nfrom pywaffle import Waffle\n\n\n# stat tools\nimport statsmodels.api as sm\nfrom scipy.stats import kurtosis, skew\n\n## preprocessing & otherlibraries\nfrom sklearn.model_selection import (train_test_split, \n                                     cross_val_score,\n                                     StratifiedKFold, \n                                     GridSearchCV)\n\n\nfrom sklearn.preprocessing import (StandardScaler,\n                                   MinMaxScaler,\n                                   RobustScaler)\n\n\n## data sampling and outlier detection libraries\n\nfrom sklearn.cluster import DBSCAN\nfrom sklearn.svm import OneClassSVM\nfrom sklearn.ensemble import IsolationForest\nfrom sklearn.covariance import EllipticEnvelope\nfrom sklearn.neighbors import LocalOutlierFactor\n\nfrom umap import UMAP\nfrom sklearn.decomposition import PCA\nfrom imblearn.over_sampling import SMOTE,RandomOverSampler\n\n\n\n# modeling\nfrom sklearn.linear_model import (LinearRegression, \n                                  LogisticRegression) \nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier, plot_importance, early_stopping\nfrom sklearn.ensemble import (AdaBoostClassifier,\n                              ExtraTreesClassifier,\n                              RandomForestClassifier, \n                              GradientBoostingClassifier)\n\n\n# metrics\nfrom sklearn.metrics import (r2_score, \n                             accuracy_score,\n                             roc_auc_score, \n                             f1_score,\n                             recall_score, \n                             precision_score, \n                             recall_score,\n                             confusion_matrix)\n\n\n#feature selection and model interpretaiton\nimport shap\nimport eli5\nfrom eli5.sklearn import PermutationImportance\n\nfrom vecstack import stacking\n\n\n\n## plot settings\n\nsns.set_style('white')\nmpl.rcParams['xtick.labelsize'] = 12\nmpl.rcParams['ytick.labelsize'] = 12\nmpl.rcParams['axes.spines.left'] = False\nmpl.rcParams['axes.spines.right'] = False\nmpl.rcParams['axes.spines.top'] = False\nmpl.rcParams['axes.spines.bottom'] = False\nplt.rcParams.update({'font.size':14})\nplt.rcParams['font.weight']= 'normal'\n\n    \nprint(color_class.BOLD + 'Done!!')","190597cb":"print(color_class.BOLD_COLOR+ '\\nloading the dataset....' + color_class.END)\ndf = pd.read_csv('..\/input\/breast-cancer-wisconsin-data\/data.csv', delimiter = ',', encoding = 'utf-8')\n\nprint(color_class.BOLD )\nprint('Done!')\n","a73d4c86":"print(color_class.BOLD_COLOR + 'Shape of The Dataset:'+ color_class.END + color_class.BOLD + ' {}'.format(df.shape) +2*'\\n' + 'Note: There is {} missing values column in the data'.format(df.isnull().any().sum())  +\n     '\\n')\n\nprint(color_class.BOLD_COLOR + 'Null Value Count Details as Follows'+ color_class.END + '\\n')\nprint(color_class.BOLD)\nprint(df.isnull().sum())\n\n","229aefa0":"df.drop(columns = ['Unnamed: 32'], inplace = True)\ndf['diagnosis'] = df['diagnosis'].map({'M':1, 'B':0})\n\nprint(color_class.BOLD_COLOR + 'Sneak peak into the data...'+ color_class.END)\nprint(color_class.BOLD)\nprint(df.head(2).T)","64bcbdd1":"print(color_class.BOLD_COLOR +'\\nFinal Check for Null Values in data....\\n' + color_class.END)\n\nprint(color_class.BOLD_COLOR + '\\nTotal Number of Null values: \\n' + color_class.END + color_class.BOLD+ str(df.isnull().sum().sum()) + color_class.END)\nprint(color_class.BOLD_COLOR + '\\nAny NUll Value columns: \\n' + color_class.END + color_class.BOLD+ str(df.isnull().sum().any())+ color_class.END)","5180725f":"colors= ['#fe4a49' ,'#2ab7ca' ,'#fed766' ,'#e6e6ea' ,'#f4f4f8']\nstats_df = df.drop(columns  = ['id']).describe().T.reset_index().rename(columns = {'index':'Features'})\nstats_df['count'] = stats_df['count'].astype(int)\n\n\nstyle = stats_df.style.set_table_attributes(\"style='display:inline'\").\\\n                                bar(subset = ['mean', 'std', 'min', '25%','50%','75%', 'max'],axis = 1 ,color = colors[2])\\\n                                .format({\n                                         'mean':\"{:20,.3f}\", \n                                         'std':\"{:20,.3f}\", \n                                         'min':\"{:20,.3f}\", \n                                         '25%':\"{:20,.3f}\",\n                                         '50%':\"{:20,.3f}\",\n                                         '75%':\"{:20,.3f}\", \n                                         'max':\"{:20,.3f}\"})\\\n                                .format({\"Features\": lambda x:  x.upper()},\n                                 )\\\n                                .set_properties(**{'background-color': 'white',                                                   \n                                    'color': 'black'})\nprint(color_class.BOLD_COLOR + '''\\n Descriptive Statistics of The Dataset \\\n\\n''' + color_class.END)\n\ndisplay_html(style._repr_html_(), raw=True)\n\n","bd7926c7":"\ncolors= ['#fe4a49' ,'#2ab7ca' ,'#fed766' ,'#e6e6ea' ,'#f4f4f8']\n\nsns.palplot(colors,size = 3)\n\nplt.gcf().set_size_inches(15,5)\n\nplt.text(-0.75,-0.75, 'Women and Cancer: Color Palette',{'fontfamily':'serif', 'size':24, 'weight':'bold'})\nplt.text(-0.75,-0.68, 'Lets try to stick to these colors throughout presentation.',{'fontfamily':'serif', 'size':16},alpha = 0.9)\nfor idx,values in enumerate(colors):\n    plt.text(idx-0.25,0, colors[idx],{'fontfamily':'serif', 'size':16, 'weight':'bold','color':'black'}, alpha =0.8)\nplt.gcf().set_facecolor('white')\nplt.box(None)\nplt.axis('off')\nplt.text(3.5,0.65,'\u00a9 Made by bhuvanchennoju\/Kaggle',{'fontfamily':'serif', 'size':10,  'color':'black'})\nplt.show()\n","282229af":"# Null accuracy Score for current data\nNUll_acc = round (max(df.diagnosis.values.mean(), 1 - df.diagnosis.values.mean()), 2)\n\nprint(color_class.BOLD +'\\nNull Accuracy Score: '+ color_class.END \\\n      +color_class.BOLD_COLOR + str(NUll_acc) + color_class.END + '\\n' )\n\nprint(color_class.BOLD_COLOR + 'This is the Baseline our model need to cross.\\n'+ color_class.END)\n","ced60ec9":"feat_df = df.drop(columns = ['id', 'diagnosis'])\ntar_df = df['diagnosis']\ncancer_dist = round(tar_df.value_counts(normalize = True),2)*100\n\nfig = plt.figure(FigureClass = Waffle, \n                 constrained_layout = True,\n                 figsize = (8,5),\n                 facecolor = 'white',dpi = 100,\n                 \n                 plots = {'111':\n                          {     \n                           'rows':10,\n                           'columns': 10,\n                           'values' : [cancer_dist.values[0],cancer_dist.values[1]],\n                            'colors' : [colors[2],colors[0]],\n                              'vertical' : True,\n                              'interval_ratio_y': 0.2,\n                              'interval_ratio_x': 0.2,\n                              'icon_legend': False,\n                              'icon_size':5,\n                              'plot_anchor':'C',\n                              \n                          },\n                       \n                         })\n\n## labeling \nfig.text(0.16,0.725, '{}%'.format(cancer_dist.values[1]), {'font':'serif','size':20, 'weight':'bold', 'color':colors[0]})\nfig.text(0.725,0.35, '{}%'.format(cancer_dist.values[0]),{'font':'serif','size':20, 'weight':'bold','color':colors[2]})\n\n## titles and text\nfig.text(-0.1,1.035,'Women and Cancer: How Susceptable Are Women To Breast Cancer?', {'font':'serif','size':18, 'weight':'bold'}, alpha = 1)\nfig.text(-0.1,0.96,'''Its really sad to see nearly 40% of the women are suceptable to cancer.\nLets hope things will change with medical advancements.''',{'font':'serif','size':12, 'weight':'normal'}, alpha = 0.9)\n\nfig.text(0.70,0.95, \"Cancerous\",{'font':'serif','size':14, 'weight':'bold', 'color':colors[0]})\nfig.text(0.85,0.95, '|',{'font':'serif','size':14, 'weight':'bold'})\nfig.text(0.87,0.95, \"Healthy\",{'font':'serif','size':14, 'weight':'bold','color':colors[2]})\n\nfig.text(0.82,0.1,'\u00a9 Made by bhuvanchennoju\/Kaggle',{'font':'serif', 'size':8,'weight':'bold'}, alpha = 0.7)\n\nfig.show()","a4e6287b":"\nfig,ax = plt.subplots(nrows = 10, ncols = 3, figsize = (12,24),dpi=80)\n#fig.patch.set_facecolor(colors[-1])\naxes = ax.ravel()\n\nfor col,ax in zip(feat_df.columns,axes):\n    \n    # skewness and kurtosis\n    if skew(feat_df[col])>1:\n        color = colors[0]\n    else:\n        color = colors[1]\n    \n    ## plots\n    #sns.kdeplot(feat_df[col], ax= ax, fill = True , color = color, alpha = 1, linewidth = 3, ec = 'black')\n    sns.violinplot(feat_df[col], ax =ax, \n                   color = color, cut =0,\n                   inner = 'box',\n                   alpha = 1,linewidth = 3, edgecolor = 'solidblack', saturation =1 )\n    \n    ## plot setting\n    xlabel = ' '.join([value.capitalize() for value in str(col).split('_') ])\n    #ax.set_facecolor(colors[-1])\n    ax.axes.get_yaxis().set_visible(False)\n    ax.axes.set_xlabel(xlabel,{'font':'serif','size':14, 'weight':'bold'}, alpha = 1)\n   \n\n    \nplt.tight_layout(pad= 3,h_pad = 2.5, w_pad = 2.5)\n\n\n## titles and text\nfig.text(0,1.05,'Women and Cancer: Overview of Univariate Feature Distribution', {'font':'serif','size':22, 'weight':'bold'}, alpha = 1)\nfig.text(0,1.02,'''Are there any normally distributed features? It seems most of the features\nare skewed and having high kurtosis, may be a log somekind transformation needed. It seems\nmost of the se features and fractual Dimensions have outliers.  ''',{'font':'serif','size':14, 'weight':'normal'}, alpha = 1)\n\nfig.text(0.65,1, \"Skewed\",{'font':'serif','size':16, 'weight':'bold', 'color':colors[0]})\nfig.text(0.73,1, '|',{'font':'serif','size':16, 'weight':'bold'})\nfig.text(0.74,1, \"Relative Normal\",{'font':'serif','size':16, 'weight':'bold','color':colors[1]})\n\nfig.text(0.73,0,'\u00a9 Made by bhuvanchennoju\/Kaggle',{'font':'serif', 'size':10,'weight':'bold'}, alpha = 0.8)\n\nfig.show()","eadd016d":"\nfig,ax = plt.subplots(nrows = 10, ncols = 3, figsize = (12,24),dpi=80)\n#fig.patch.set_facecolor(colors[-1])\naxes = ax.ravel()\n\nfor col,ax in zip(feat_df.columns,axes):\n    \n    ## plots\n    \n    sns.kdeplot(df[col], ax = ax, shade = True ,\n                palette = [colors[0], colors[2]],\n                alpha = 0.95, linewidth = 3, ec = 'black',\n                hue = df['diagnosis'], hue_order = [1,0],\n                legend = False)\n    \n    ## plot setting\n    xlabel = ' '.join([value.capitalize() for value in str(col).split('_') ])\n    #ax.set_facecolor(colors[-1])\n    ax.axes.get_yaxis().set_visible(False)\n    ax.axes.set_xlabel(xlabel,{'font':'serif','size':14, 'weight':'bold'}, alpha = 1)\n   \n\n    \nplt.tight_layout(pad= 3,h_pad = 1.5, w_pad = 1.5)\n\n\n## titles and text\nfig.text(0,1.03,'Women and Cancer: Distribution of Cancers cells on Feature level', {'font':'serif','size':22, 'weight':'bold'}, alpha = 1)\nfig.text(0,1.01,'''It seems most of the features and targets have similar kind of ditribution, but few\ntarget distributions are morelike normal distribution.''',{'font':'serif','size':14, 'weight':'normal'}, alpha = 1)\n\nfig.text(0.615,1, \"Cancerous\",{'font':'serif','size':16, 'weight':'bold', 'color':colors[0]}, alpha = 1)\nfig.text(0.73,1, '|',{'font':'serif','size':16, 'weight':'bold'})\nfig.text(0.74,1, \"Healthy\",{'font':'serif','size':16, 'weight':'bold','color':colors[2]}, alpha = 1)\n\nfig.text(0.73,0,'\u00a9 Made by bhuvanchennoju\/Kaggle',{'font':'serif', 'size':10,'weight':'bold'}, alpha = 0.8)\n\nfig.show()\n\n","02eea55e":"print(color_class.BOLD_COLOR + '\\nSegregating Features Based On Category....\\n' + color_class.END)\n\n### measurement and characteristics keyword lists\nmeasure_keyword = ['radius','perimeter','area','concavity', 'concave points']\ncharacter_keyword = ['texture','smoothness','compactness','symmetry','fractal']\n\n### mean, standard error, and worst measure feature lists\nmean_measure, mean_character = ['diagnosis'],['diagnosis']\nse_measure, se_character = ['diagnosis'],['diagnosis']\nworst_measure,worst_character = ['diagnosis'],['diagnosis']\n\n### requrired mean, standard errror,and worst measure feature creating loop\nfor col in feat_df.columns:\n    \n    name_list = str(col).split('_')\n    \n    if name_list[0] in measure_keyword:\n        if 'mean' in name_list:\n            mean_measure.append(col)\n            \n        elif 'se' in name_list:\n            se_measure.append(col)\n            \n        else:\n            worst_measure.append(col)           \n            \n    if name_list[0] in character_keyword:\n        if 'mean' in name_list:\n            mean_character.append(col)\n        elif 'se' in name_list:\n            se_character.append(col)\n        else:\n            worst_character.append(col) \n            \n###### descriptions and lists            \nprint(color_class.BOLD + 'Done!' +color_class.END)\nprint(color_class.BOLD_COLOR + '\\nSeperated Features are stored into lists:\\n' +color_class.END)\nprint(color_class.BOLD_COLOR + 'Mean of Measurements: '+color_class.END \\\n      + color_class.BOLD +str(' , '.join(mean_measure[1:])) + color_class.END +'\\n')\nprint(color_class.BOLD_COLOR + 'Mean of Characteristics: '+color_class.END \\\n      + color_class.BOLD +str(' , '.join(mean_character[1:])) + color_class.END  +'\\n')\nprint(color_class.BOLD_COLOR + 'Standard Error of Measurements: '+color_class.END \\\n      + color_class.BOLD +str(' , '.join(se_measure[1:])) + color_class.END  +'\\n')\nprint(color_class.BOLD_COLOR + 'Standard Error of Characteristics: '+color_class.END \\\n      + color_class.BOLD +str(' , '.join(se_character[1:])) + color_class.END  +'\\n')\nprint(color_class.BOLD_COLOR + 'Worst of Measurements: '+color_class.END \\\n      + color_class.BOLD +str(' , '.join(worst_measure[1:])) + color_class.END  +'\\n')\nprint(color_class.BOLD_COLOR + 'Worst of Characteristics: '+color_class.END \\\n      + color_class.BOLD +str(' , '.join(worst_character[1:])) + color_class.END  +'\\n')","821a3c38":"print(color_class.BOLD_COLOR +'\\nFinally helper function to visualize bivariate features....\\n' + color_class.END)\n    \n### bivariate cross relations visualizations function\n            \ndef cust_pairplot(df,var,title, diag_kind = 'kde',corner = True,sign = 'off'):\n    \n    ## plot\n    g = sns.pairplot(data = df[var],\n                 hue= 'diagnosis',hue_order = [1,0],\n                 height = 2.5,aspect = 1,\n                 corner = True, diag_kind= diag_kind, \n                 palette = [colors[0],colors[2]], \n                 plot_kws = {'alpha':1, 'size' : 1, 'linewidth' : 0.5, 'ec':'black'},\n                 diag_kws = {'alpha':0.95,'ec':'black','linewidth':3 });\n    \n    ### plot setting\n    g._legend.remove();\n    \n    plt.gcf().patch.set_facecolor('white');\n    plt.gcf().patch.set_alpha(1)\n    plt.gcf().set_size_inches(12,12);\n    #plt.gcf().set_dpi(55);\n    \n    for ax in plt.gcf().axes:\n        ax.set_facecolor('white')\n        for loc in ['left','right','top','bottom']:\n            ax.spines[loc].set_visible(False)\n        #ax.set_xticks(ticks = [])\n        #ax.set_yticks(ticks = [])\n        ax.set_xlabel(xlabel = ax.get_xlabel(), **{'font':'serif', 'size':12,'weight':'bold'}, alpha = 1)\n        ax.set_ylabel(ylabel = ax.get_ylabel(), **{'font':'serif', 'size':12,'weight':'bold'}, rotation  = 90,alpha = 1)\n\n    ### titles and descriptions\n\n    plt.gcf().text(0.425,0.85, 'Women and Cancer:\\n{}'.format(title),{'font':'serif', 'size':22.,'weight':'bold'}, alpha = 1)\n\n\n    plt.gcf().text(0.425,0.8,'''This visualization shows the bivariate relations among \\nthe {}.'''.format(title),{'font':'serif', 'size':14}, alpha = 1)\n\n    plt.gcf().text(0.44,0.75, \"Cancerous\",{'font':'serif','size':18, 'weight':'bold', 'color':colors[0]}, alpha = 1)\n    plt.gcf().text(0.565,0.75, '|',{'font':'serif','size':18, 'weight':'bold'})\n    plt.gcf().text(0.575,0.75, \"Healthy\",{'font':'serif','size':18, 'weight':'bold','color':colors[2]}, alpha = 1)\n\n    ## legend\n    if sign == 'on':\n        plt.gcf().text(0.75,-0.025,'\u00a9 Made by bhuvanchennoju\/Kaggle',{'font':'serif', 'size':12, 'weight':'bold'},alpha = 1)\n    \n    plt.gca().margins(x =0)\n    plt.gcf().show();","b36c8258":"cust_pairplot(df, mean_measure, 'Mean Measurements of Cancer Cells', sign = 'on')","758fe564":"cust_pairplot(df, mean_character, 'Mean Characteristics of Cancer Cells', sign = 'on')","3b59322e":"\ncust_pairplot(df, se_measure, 'SE in Measurements of Cancer Cells',sign = 'on')","76034b92":"cust_pairplot(df, se_character, 'SE in Characteristics of Cancer Cells',sign = 'on')","1134d983":"cust_pairplot(df, worst_measure, 'Worst of Measurements of Cancer Cells',sign = 'on')","de8a30d1":"cust_pairplot(df, worst_character, 'Worst of Characteristics of Cancer Cells', sign = 'on')","71de1d85":"print(color_class.BOLD_COLOR + '\\nGetting High Positively Correatated and Negatively Coorealated Features \\nfrom cross categorical feature extraction..\\n' + color_class.END)\n\ntemp_df = df.corr().unstack().reset_index()\n\n\n### cross relational positive features\n\npositive_corr_df = (temp_df[(temp_df[0]>0.9) &\n         (temp_df['level_0'] != temp_df['level_1']) & \n         ((temp_df['level_0'].apply(lambda x: str(x).split('_')[-1])) != (temp_df['level_1'].apply(lambda x: str(x).split('_')[-1])))])\n\npositive_corr_df['z'] = positive_corr_df.apply(lambda x: tuple(sorted([x['level_0'],x['level_1']])), axis = 1)\npositive_corr_df.drop_duplicates(subset=\"z\", keep=\"first\" , inplace = True ) \npositive_corr_df.drop(columns = ['z'], inplace = True)\n\n\n\n### cross relational negative features\n\nnegative_corr_df = (temp_df[(temp_df[0]<-0.2) &\n         (temp_df['level_0'] != temp_df['level_1']) & \n         ((temp_df['level_0'].apply(lambda x: str(x).split('_')[-1])) != (temp_df['level_1'].apply(lambda x: str(x).split('_')[-1])))])\n\nnegative_corr_df['z'] = negative_corr_df.apply(lambda x: tuple(sorted([x['level_0'],x['level_1']])), axis = 1)\nnegative_corr_df.drop_duplicates(subset=\"z\", keep=\"first\" , inplace = True ) \nnegative_corr_df.drop(columns = ['z'], inplace = True)\n\nprint(color_class.BOLD +'Done!')","2d5a7944":"print(color_class.BOLD_COLOR + '\\nHelper function to visualize the cross categorical Feature analysis\\n'+ color_class.END)\ndef plot_cross_scatter(corr_df, data =df,title = None,des = None,nrows = 4, ncols = 3, figsize = (12,24), colors = colors):\n    \n    col1_list = corr_df['level_0'].values.tolist()\n    col2_list = corr_df['level_1'].values.tolist()\n    \n    ## plotting\n    fig,axes = plt.subplots(nrows,ncols, figsize = (15,20))\n    \n    # removing the last axes\n    axes.ravel()[-1].axes.get_xaxis().set_visible(False)\n    axes.ravel()[-1].axes.get_yaxis().set_visible(False)\n    \n    for ax,col1,col2 in zip(axes.ravel(), col1_list,col2_list):\n        \n        sns.scatterplot(x= data[col1], y = data[col2], ax = ax,size = 100, \n                        linewidth= 0.5, edgecolor = 'black',\n                        hue = data['diagnosis'], hue_order = [1,0],\n                        palette = [colors[0],colors[2]], legend = False )\n        \n        ## plot setting\n        xlabel = ' '.join([value.capitalize() for value in str(col1).split('_') ])\n        ylabel = ' '.join([value.capitalize() for value in str(col2).split('_') ])\n        \n        ax.axes.set_xlabel(xlabel,{'font':'serif','size':14, 'weight':'bold'}, alpha = 1)\n        ax.axes.set_ylabel(ylabel,{'font':'serif','size':14, 'weight':'bold'}, alpha = 1) \n        \n        ax.set_xticklabels('')\n        ax.set_yticklabels('')\n        \n    \n    ## titles and text\n    fig.text(0.05,0.935,'Women and Cancer: {}'.format(title), {'font':'serif','size':22, 'weight':'bold'}, alpha = 1)\n    fig.text(0.05,0.91,'''{}'''.format(des),{'font':'serif','size':14, 'weight':'normal'}, alpha = 1)\n\n    fig.text(0.63,0.885, \"Cancerous\",{'font':'serif','size':16, 'weight':'bold', 'color':colors[0]}, alpha = 1)\n    fig.text(0.735,0.885, '|',{'font':'serif','size':16, 'weight':'bold'})\n    fig.text(0.745,0.885, \"Healthy\",{'font':'serif','size':16, 'weight':'bold','color':colors[2]}, alpha = 1)\n\n    fig.text(0.73,0.1,'\u00a9 Made by bhuvanchennoju\/Kaggle',{'font':'serif', 'size':10,'weight':'bold'}, alpha = 0.8)\n\n    fig.show()\n    \n    return None\n","10efb1b0":"des = 'Here we are seeing the cancer cell features which are highly correlated with each other and belong to different category. \\nIt seems we have multi-colinear features and they are passing similar information, and This could alter the predictions.'\nplot_cross_scatter(positive_corr_df, title = 'CrossCategorical Positively Related Features', des = des)","5882be28":"des = 'Here we are seeing the cancer cell features which are moderately correlated with each other and belong to different category. \\nIt seems we have multi-colinear features and they are passing similar information, and This could alter the predictions.'\nplot_cross_scatter(negative_corr_df,nrows = 4,ncols = 2, figsize=(12,6)\n                   ,title = 'CrossCategorical Negitively Correlated Features', des = des)","17b726c6":"temp = df.copy()\n\nX_temp = temp.drop(columns = ['id','diagnosis'])\ny_temp = temp['diagnosis']\n\n# fitting on umap\numap = UMAP(random_state=2021)\nmodel_umap = umap.fit_transform(X_temp, y_temp)\n\nfig,ax = plt.subplots(figsize=(7,7),dpi =80)\n\n# plots\nax.scatter(model_umap[temp['diagnosis'] == 0][:,0], model_umap[temp['diagnosis'] == 0][:,1], c= colors[2], alpha=1,s=50, linewidth = 1, ec = 'black')\nax.scatter(model_umap[temp['diagnosis'] == 1][:,0], model_umap[temp['diagnosis'] == 1][:,1], c= colors[0], alpha=1,s=50, linewidth = 1, ec = 'black')\n\n\n## titles and text\n\nax.set_xticklabels('')\nax.set_yticklabels('')\n\nfig.text(0,1.01,'Women and Cancer: Dimensionality Reduction with UMAP', {'font':'serif','size':18, 'weight':'bold'}, alpha = 1)\nfig.text(0,0.95,'''Wow! As data is very less clear clustering of cancer cells can\nbe seen. There are clearly seperable and hope get good results...''',{'font':'serif','size':13, 'weight':'normal'}, alpha = 0.95)\n\nfig.text(0.68,0.85, \"Cancerous\",{'font':'serif','size':14, 'weight':'bold', 'color':colors[0]})\nfig.text(0.85,0.85, '|',{'font':'serif','size':14, 'weight':'bold'})\nfig.text(0.87,0.85, \"Healthy\",{'font':'serif','size':14, 'weight':'bold','color':colors[2]})\n\nfig.text(0.65,0.05,'\u00a9 Made by bhuvanchennoju\/Kaggle',{'font':'serif', 'size':10,'weight':'bold'}, alpha = 0.85)\n\n\nfig.show()\n","c6f00cb9":"print(color_class.BOLD_COLOR + '\\nOutlier Removal,Skewness, Kurtosis helper funtions are here....\\n'+ color_class.END)\n\ndef outlier_detect(algo,data):\n    cols = data.drop(columns = ['id']).columns\n    # creating feature and target numpy arrays\n    feat, tar = data[cols].drop(columns = 'diagnosis').values, data['diagnosis'].values\n    # fitting the features to algo\n    yhat = algo.fit_predict(feat)\n    # masking the features that are not outliers\n    mask = yhat != -1\n    X,y = feat[mask,:], tar[mask]\n    data_inarray = np.append(y.reshape(-1,1),X,axis = 1)\n    return pd.DataFrame(data = data_inarray, columns = cols)\n\ndef skew_sum(data):\n    return skew(data).sum()\n\ndef kurtosis_sum(data):\n    return kurtosis(data).sum()\n\ndef shape(data): \n    return data.shape","5a427f28":"print(color_class.BOLD_COLOR+'\\nOutliers related information with isolation forest, elliptic envelope, localoutlierfactor, dbscan is storing to  dataframes....' + color_class.END)\n\noutlier_algos = [IsolationForest(contamination = 0.05),\\\n                 EllipticEnvelope(contamination = 0.05),\\\n                 LocalOutlierFactor(contamination = 0.05), \\\n                 DBSCAN(eps = 70, min_samples = 10)]\n\n\ndf_list = [df.drop(columns = ['id'])]\nshapes = [df.drop(columns = ['id']).shape[0]]\nskews = [skew(df.drop(columns = ['id']))]\nkurts = [kurtosis(df.drop(columns = ['id']))]\n\nfor algo in outlier_algos:\n    corrected_df = outlier_detect(algo, df)\n    df_list.append(corrected_df)\n    shapes.append(corrected_df.shape[0])\n    skews.append(skew(corrected_df))\n    kurts.append(kurtosis(corrected_df))\n        \n\nalgorithms = ['Original','IsolationForest', 'EllipticEnvelope', 'LocalOutlierFactor', 'DBSCAN']\noutliers_info = pd.DataFrame({'algorithms':algorithms,'df_list':df_list,'shapes':shapes, 'skews':skews, 'kurts': kurts})\n\noutliers_info['skews_sum'] = outliers_info['skews'].apply(lambda x: round(x.sum(),2))\noutliers_info['kurts_sum'] = outliers_info['kurts'].apply(lambda x: round(x.sum(),2))\noutliers_info.sort_values(by = 'shapes').reset_index(drop = True, inplace = True)\n\n\nfor idx, df_ in enumerate(outliers_info['df_list']):\n    from sklearn.metrics import f1_score\n    \n    lr = LinearRegression()\n    X = df_.drop(columns = ['diagnosis'])\n    y = df_['diagnosis']\n    xtrain, xtest,ytrain,ytest = train_test_split(X,y,test_size = 0.2)\n    \n    # linear regression\n    preds = LinearRegression().fit(xtrain.values,ytrain.values).predict(xtest.values)\n        \n    r2 = round(r2_score(ytest,preds),3)\n    #ypred_class = preds > 0.85\n    #acc = round(accuracy_score(ytest, ypred_class),3)\n    #roc_auc = round(roc_auc_score(ytest,ypred_class),3)\n       \n    metric_list = r2\n    \n    outliers_info.loc[idx, 'r2_score'] = metric_list\n    \n\n\nprint(color_class.BOLD + '\\nAll the corrected data stored to' + color_class.END \\\n      + color_class.BOLD_COLOR + str(' Outliers_info DataFrame\\n')+ color_class.END)\n\nprint(color_class.BOLD)\nprint(outliers_info.T)\n","83bdfa61":"print(color_class.BOLD_COLOR +'\\nHelper class to make the outlier and original data comparisions....\\n'+color_class.END)\nclass outlier_viz():\n    \n    def __init__(self,ax,orig_feat = None,corrected_feat = None): \n        self.x_org = orig_feat\n        self.x_corr = corrected_feat\n        self.ax = ax\n\n    def visualize_data(self,name =None, r2 = None,orig_r2 = None):\n        \n        self.ax.set_facecolor('white')\n       # dimension reduction\n        pca1 = PCA(n_components= 2).fit_transform(self.x_org)\n        pca2 = PCA(n_components= 2).fit_transform(self.x_corr)\n        \n        self.ax.scatter(pca1[:,0], pca1[:,1], c = colors[0], s = 50, zorder =0, alpha = 1, linewidth = 1, ec = 'black')\n        self.ax.scatter(pca2[:,0], pca2[:,1], c = colors[1], s = 50, zorder = 3, alpha = 1,linewidth = 1, ec = 'black')\n   \n        self.ax.text(3000,900,'{}'.format(name), {'font':'serif','size':14,'weight':'bold','color':'black'},alpha= 0.9)\n        self.ax.text(3000,800,'R2 Score: {}'.format(r2), {'font':'serif','size':14,'weight':'bold','color':'black'},alpha= 0.9)\n        self.ax.text(3000,700,'Orig R2 Score: {}'.format(orig_r2), {'font':'serif','size':14,'weight':'bold','color':'black'},alpha= 0.9)\n","78e69b50":"fig, ax =plt.subplots(2,2,figsize =(13,9), dpi = 70)\naxes = ax.ravel()\nfor ax in axes:\n    ax.set_xticklabels('')\n    ax.set_yticklabels('')\n\n# plotting \norig = outliers_info['df_list'][0]\n\n(outlier_viz(ax = axes[0] , orig_feat = orig, corrected_feat= outliers_info['df_list'][1])\n            .visualize_data(name = 'Isolation Forest', r2= outliers_info['r2_score'][1],orig_r2 = outliers_info['r2_score'][0]))\n\n(outlier_viz(ax = axes[1], orig_feat = orig, corrected_feat= outliers_info['df_list'][2])\n .visualize_data(name = 'Eclliptic Envelope',r2= outliers_info['r2_score'][2],orig_r2 = outliers_info['r2_score'][0]))\n\n(outlier_viz(ax = axes[2], orig_feat = orig, corrected_feat= outliers_info['df_list'][3])\n .visualize_data(name = 'Local Outlier Factor',r2= outliers_info['r2_score'][3],orig_r2 = outliers_info['r2_score'][0]))\n\n(outlier_viz(ax = axes[3], orig_feat = orig, corrected_feat= outliers_info['df_list'][4])\n .visualize_data(name = 'DBSCAN',r2= outliers_info['r2_score'][4],orig_r2 = outliers_info['r2_score'][0]))\n\n\n# text and labels\n### title and annotations\n## titles and text\nfig.text(-0.05,1.085,'Women and Cancer: Outliers and Original Data', {'font':'serif','size':22, 'weight':'bold'}, alpha = 1)\nfig.text(-0.05,1.0,'''Looks like evey outlier detection algorithm did a good job, butit is not possible to \nselect the best one out of them, without looking at skew and kurtosis values, \nlets dive into that next...''',{'font':'serif','size':14, 'weight':'normal'}, alpha = 1)\n\n\nfig.text(0.59,1, \"Original Data\",{'font':'serif','size':16, 'weight':'bold', 'color':colors[0]}, alpha = 1)\nfig.text(0.73,1, '|',{'font':'serif','size':16, 'weight':'bold'})\nfig.text(0.74,1, \"Corrected Data\",{'font':'serif','size':16, 'weight':'bold','color':colors[1]}, alpha = 1)\n\n\nfig.text(0.7,-0.01,'\u00a9 Made by bhuvanchennoju\/Kaggle',{'font':'serif', 'size':10,'weight':'bold'}, alpha = 0.85)\n\nfig.tight_layout(pad = 1.5, w_pad = 1.5,h_pad = 1.5)\nfig.show()","3cd50582":"fig,ax = plt.subplots(1,2, figsize =(12,6), dpi = 100)\naxes = ax.ravel()\n\n# total skew plot\nfor idx in range(1,outliers_info.shape[0]):\n    axes[0].barh(width = outliers_info['skews_sum'][0],\n                 y = outliers_info['algorithms'][idx], color = colors[0])\naxes[0].barh(width = outliers_info['skews_sum'][1:],\n             y = outliers_info['algorithms'][1:], color = colors[1])\n\n# total kurtosis plot\nfor idx in range(1,outliers_info.shape[0]):\n    axes[1].barh(width = outliers_info['kurts_sum'][0],\n                 y = outliers_info['algorithms'][idx], color = colors[0])\naxes[1].barh(width = outliers_info['kurts_sum'][1:],\n             y = outliers_info['algorithms'][1:], color = colors[2])\n\n# plot ticks and title setting\naxes[1].tick_params(axis = 'y',pad = 95)\naxes[1].set_yticklabels(outliers_info['algorithms'][1:], {'font':'serif','size':16,'weight':'bold'}, ha = 'center')\naxes[1].set_xticklabels('')\naxes[0].set_xticklabels('')\naxes[0].set_yticklabels('')\naxes[0].invert_xaxis()\n\n\n## text and decorations\n\n# skewness annotations\nfor pa in axes[0].patches:\n    axes[0].text(pa.get_width(), pa.get_y()+ pa.get_height()\/2, int(pa.get_width()),\n                 {'color':'white','font':'serif','weight':'bold','size':'12'},alpha= 1, va = 'center')\n    if pa in axes[0].patches[4:]:\n        \n        change = int((int(outliers_info['skews_sum'][0]) - int(pa.get_width())) \/ int(outliers_info['skews_sum'][0]) *100)\n        \n        axes[0].text(pa.get_width()*0, pa.get_y(),'{}% {}'.format(change,u'\\u2193'),\n                     {'color':'black','font':'serif','weight':'bold','size':14},\n                     alpha= 0.8,va = 'bottom', ha='right')\n      \n\n        \n# kurtosis annotations\nfor pa in axes[1].patches:\n    axes[1].text(pa.get_width()-20, pa.get_y()+ pa.get_height()\/2, int(pa.get_width()),\n                 {'color':'white','font':'serif','weight':'bold','size':'12'},alpha= 1, va = 'center')\n    if pa in axes[1].patches[4:]:\n        \n        change = int((int(outliers_info['kurts_sum'][0]) - int(pa.get_width())) \/ int(outliers_info['kurts_sum'][0]) *100)\n        \n        axes[1].text(pa.get_width()*0, pa.get_y(),'{}% {}'.format(change,u'\\u2193'),\n                     {'color':'black','font':'serif','weight':'bold','size':14},alpha= 0.8, va = 'bottom')\n\n        \n### title and annotations\n## titles and text\nfig.text(-0.05,1.18,'Women and Cancer: Comparision of Total Skews and Kurtosis', {'font':'serif','size':22, 'weight':'bold'}, alpha = 1)\nfig.text(-0.05,1.07,'''Total Skews and Total kurtosis means sum of skews,and sum of kurtosis of all features\nrespectively. It seems with default setting of 5% points as outliers, Isolation forest\ndid well in reducing both skew and kurtosis of data.''',{'font':'serif','size':14, 'weight':'normal'}, alpha = 0.95)\n\nfig.text(0.27,0.99, \"Skewness\",{'font':'serif','size':18, 'weight':'bold', 'color':colors[1]})\nfig.text(0.40,0.99, '|',{'font':'serif','size':18, 'weight':'bold'})\nfig.text(0.45,0.99, \"Original\",{'font':'serif','size':18, 'weight':'bold','color':colors[0]})\nfig.text(0.60,0.99, '|',{'font':'serif','size':18, 'weight':'bold'})\nfig.text(0.62,0.99, \"Kurtosis\",{'font':'serif','size':18, 'weight':'bold','color':colors[2]})\n\nfig.text(0.82,0.0,'\u00a9 Made by bhuvanchennoju\/Kaggle',{'font':'serif', 'size':10,'weight':'bold'}, alpha = 0.8)\n\n\nplt.tight_layout(pad = 1, h_pad = 1, w_pad = 1)\n\nfig.show()\n","8e66bd35":"fig, ax = plt.subplots(1,2,figsize = (12,12))\n\naxes = ax.ravel()\n\naxes[0].invert_xaxis()\naxes[0].barh(y = df.drop(columns = ['id']).columns, width=outliers_info['skews'][0].tolist(), color = colors[0],align='center')\naxes[0].barh(y = df.drop(columns = ['id']).columns, width=outliers_info['skews'][1].tolist(), color = colors[1],align='center')\n\naxes[1].barh(y = df.drop(columns = ['id']).columns, width=outliers_info['kurts'][0].tolist(), color = colors[0],align='center')\naxes[1].barh(y = df.drop(columns = ['id']).columns, width=outliers_info['kurts'][1].tolist(), color = colors[2],align='center')\n\n\naxes[0].set_yticklabels('')\naxes[1].set_yticklabels(df.drop(columns = ['id']).columns, {'font':'serif','size':12,'weight':'bold'},rotation = 0,ha= 'center')\naxes[1].tick_params(axis = 'y',pad = 75)\naxes[0].set_xticklabels('')\naxes[1].set_xticklabels('')\n\n### title and annaotations\n### title and annotations\n## titles and text\nfig.text(0,1.09,'Women and Cancer: Isolation Forest Feature Level Stats ', {'font':'serif','size':22, 'weight':'bold'}, alpha = 1)\nfig.text(0,1.05,'''We go to know that Isolation forest does a good job with outlier detection with 10% contamination\nand this butterfly plot shows the feature level change in skewness and kurosis values.''',{'font':'serif','size':14, 'weight':'normal'}, alpha = 0.9)\n\nfig.text(0.27,0.99, \"Skewness\",{'font':'serif','size':18, 'weight':'bold', 'color':colors[1]})\nfig.text(0.40,0.99, '|',{'font':'serif','size':18, 'weight':'bold'})\nfig.text(0.45,0.99, \"Original\",{'font':'serif','size':18, 'weight':'bold','color':colors[0]})\nfig.text(0.60,0.99, '|',{'font':'serif','size':18, 'weight':'bold'})\nfig.text(0.62,0.99, \"Kurtosis\",{'font':'serif','size':18, 'weight':'bold','color':colors[2]})\n\nfig.text(0.75,0.05,'\u00a9 Made by bhuvanchennoju\/Kaggle',{'font':'serif', 'size':10,'weight':'bold'}, alpha = 0.85)\n\nplt.tight_layout(pad = 1, h_pad = 1, w_pad = 1)\nfig.show()","6a71ffcd":"print(color_class.BOLD_COLOR + '\\nSelecting Data without Outliers with Isolation Forest with contamination of 10%\\n' + color_class.END)\n# selecting df with outliers removed\ndf = outliers_info['df_list'][1]\nprint(color_class.BOLD + 'Done!\\n')\n\nprint('Shape of Original Data: '+\\\n      color_class.BOLD_COLOR+ str(outliers_info['df_list'][0].shape) + color_class.END)\nprint(color_class.BOLD + 'Shape of Corrected Data: ' + color_class.END+\\\n      color_class.BOLD_COLOR+ str(df.shape) + color_class.END)\nprint(color_class.BOLD_COLOR+ '\\nAll set for feature selection...\\n' + color_class.END)\n","3d814f55":"print(color_class.BOLD_COLOR+'\\nCutom Correlation matrix values extraction...\\n'+color_class.END)\n## correlation matrix customization \n\ncorr_df = df.corr()\ntemp_df = corr_df.stack().reset_index()\ntemp_df = temp_df[temp_df[0] != 1.0].reset_index(drop = True)\ntemp_df['z'] = temp_df.apply(lambda x: tuple(sorted([x['level_0'],x['level_1']])), axis = 1)\ntemp_df.drop_duplicates(subset=\"z\", keep=\"first\" , inplace = True ) \ntemp_df.drop(columns = ['z'], inplace = True)\ntemp_df.reset_index(drop = True,inplace = True)\ntemp_df['color'] = temp_df[0].apply(lambda x: colors[1] if x <0.25 else ( colors[2]  if ((x > 0.25) & (x<0.85)) else colors[0]))\n\nprint(color_class.BOLD +'Correation Matrix data ready for custom visualization...\\n')\nprint(color_class.BOLD)\nprint(temp_df.head(2))","9ff6d382":"## custom heatmap for correlation matrix\n\nfig, ax = plt.subplots(figsize = (10,6), dpi = 85)\n\n# flipping yaxis \nax.invert_yaxis()\n\n## Creating dop representational plot\nax.scatter(x = temp_df['level_0'], y = temp_df['level_1'],\n           s = temp_df[0]*100, c = temp_df['color'], linewidth = 1, edgecolor = 'black')\n\n## plot setting - ticks and labels\n\nx_vals = temp_df['level_0'].value_counts()\ny_vals = temp_df['level_1'].value_counts().sort_values()\n\nxticklabels =  [ ' '.join((str(col).capitalize()).split('_')) for col in  x_vals.index]\nyticklabels = [ ' '.join((str(col).capitalize()).split('_')) for col in  y_vals.index]\n\n#xticklabels.reverse()\n\n#for x,y,label in zip(x_vals.values, x_vals.values,xticklabels):\n#    ax.text(y-1,x-1.5,label,{'font':'serif','size':10,'weight':'bold','color':'black'},rotation = 90, ha = 'center',va = 'bottom', alpha = 0.75)\n\nax.set_yticklabels(yticklabels,  {'font':'serif','size':10,'weight':'bold','color':'black'}, alpha = 0.75)    \nax.set_xticklabels(xticklabels,  {'font':'serif','size':10,'weight':'bold','color':'black'}, alpha = 0.75,rotation = 90)  \n\n\n## titles and desc\n\n## titles and text\nfig.text(-0.05,0.98,'Women and Cancer: Correlation Matrix and Multi-colinearity', {'font':'serif','size':20, 'weight':'bold'}, alpha = 1)\nfig.text(-0.05,0.92,'''Features could be highly correlated, moderately correalated, and least correlated based on color scheme.\nBlanks spaces indicate negative correlations. Multicolinearity exits in data.''',{'font':'serif','size':12, 'weight':'normal'}, alpha = 0.9)\n\nfig.text(0.37,0.75, \"High\",{'font':'serif','size':14, 'weight':'bold', 'color':colors[0]})\nfig.text(0.45,0.75, '|',{'font':'serif','size':14, 'weight':'bold'})\nfig.text(0.48,0.75, \"Moderate\",{'font':'serif','size':14, 'weight':'bold','color':colors[2]})\nfig.text(0.62,0.75, '|',{'font':'serif','size':14, 'weight':'bold'})\nfig.text(0.65,0.75, \"Least\",{'font':'serif','size':14, 'weight':'bold','color':colors[1]})\n\nfig.text(0.65,-0.2,'\u00a9 Made by bhuvanchennoju\/Kaggle',{'font':'serif', 'size':10,'weight':'bold'}, alpha = 0.7)\n\n\nfig.show()","0d9959ce":"print(color_class.BOLD_COLOR  +'\\nVariance inflation factor algorithm in nutshell...\\n'+color_class.END)\ndef VIF(data):\n    vif_list = list()\n    for col in data.columns:\n        X = data.drop(columns = [col])\n        y = data[col]\n        model = LinearRegression().fit(X.values,y.values)\n        ypreds = model.predict(X.values)\n        r2 = r2_score(y.values,ypreds)\n        VIF = 1 \/(1-r2)\n        vif_list.append(VIF)\n    return vif_list\n","6612d544":"print(color_class.BOLD_COLOR + '\\nVariance Inflation factor implementation for feature selection.... \\n'+ color_class.END)\nVIF_max = 1000\nVIF_dfs = {}\nn = 0\nwhile int(VIF_max) > 10:   \n    try:\n        data = data.drop(columns = [temp['Features'][0]])\n        temp = (pd.DataFrame({'Features':data.columns,'VIF':VIF(data)})\n        .sort_values(by = 'VIF', ascending = False).reset_index(drop = True))\n    except:\n        data = df.drop(columns = ['diagnosis'])\n        temp = (pd.DataFrame({'Features':df.drop(columns = ['diagnosis']).columns,'VIF':VIF(data)})\n        .sort_values(by = 'VIF', ascending = False).reset_index(drop = True))\n    \n    VIF_max = temp['VIF'][0]\n    VIF_dfs['iter_{}'.format(n)] = temp\n    n+=1\ndel temp\n\nprint(color_class.BOLD + '\\nCalcuations are finished! feature and corresponding VIF are stored in VIF_dfs list\\n'+color_class.END)\nprint(color_class.BOLD_COLOR + 'Final Features In Data and Final VIFs...'+ color_class.END)\nprint(color_class.BOLD)\nprint(VIF_dfs['iter_{}'.format(n-1)])","c9542f43":"print(color_class.BOLD_COLOR + '\\nMerging VIF iteration history dataframes to understand how algoirthm worked....\\n'+ color_class.END)\n\n### merging all the dataframes from VIF feature selection implementation\nfor key,value in VIF_dfs.items():\n    if key == 'iter_0':\n        base = value\n    else:\n        base = pd.merge(left = base, right = value, on = 'Features', how = 'outer')\n    base = base\nVIF_matrix = base.fillna(0).set_index('Features',drop = True)\ndel base \nVIF_matrix.columns = VIF_dfs.keys()\n\n## write up\nprint(color_class.BOLD + '\\nDataframes merged and stored data into' + color_class.END + color_class.BOLD_COLOR+ ' VIF_matrix' + color_class.END)\nprint(color_class.BOLD_COLOR+ '\\nExtracting data for custom visualization....\\n' + color_class.END)\n\n\n### Extracting data for custom visualization\ntemp_df = VIF_matrix.apply(lambda x: x\/x.max(), axis = 0).stack().reset_index() # noralizing values and stacking\n\n## there wont be any duplicates, just to make sure do this drop\ntemp_df['z'] = temp_df.apply(lambda x: tuple(sorted([x['Features'],x['level_1']])), axis = 1)\ntemp_df.drop_duplicates(subset=\"z\", keep=\"first\" , inplace = True ) \ntemp_df.drop(columns = ['z'], inplace = True)\ntemp_df.reset_index(drop = True,inplace = True)\ntemp_df['color'] = temp_df[0].apply(lambda x: colors[2] if x <1  else colors[0])\n\n\n## write up\nprint(color_class.BOLD + '\\nDone!!' + color_class.END)","b58ec4be":"## custom heatmap for correlation matrix\n\nfig, ax = plt.subplots(figsize = (10,6), dpi = 85)\n\n# flipping yaxis \nax.invert_yaxis()\n\n## Creating dop representational plot\nax.scatter(y = temp_df['Features'], x = temp_df['level_1'],\n           s = temp_df[0]*120, c = temp_df['color'], linewidth = 1, edgecolor = 'black')\n\n\nax.axvspan(xmin = 13.5, xmax = 14.5, color = colors[1], alpha = 0.25,zorder = 0)\n\n## plot setting - ticks and labels\n\ny_vals = temp_df['Features'].unique()\nx_vals = temp_df['level_1'].unique()\n\nxticklabels =  [ ' '.join((str(col).capitalize()).split('_')) for col in  x_vals]\nyticklabels = [ ' '.join((str(col).capitalize()).split('_')) for col in  y_vals]\n\n#xticklabels.reverse()\n\n#for x,y,label in zip(x_vals.values, x_vals.values,xticklabels):\n#    ax.text(y-1,x-1.5,label,{'font':'serif','size':10,'weight':'bold','color':'black'},rotation = 90, ha = 'center',va = 'bottom', alpha = 0.75)\n\nax.set_yticklabels(yticklabels,  {'font':'serif','size':10,'weight':'bold','color':'black'}, alpha = 0.75)    \nax.set_xticklabels(xticklabels,  {'font':'serif','size':10,'weight':'bold','color':'black'}, alpha = 0.75,rotation = 90)  \n\nax.annotate('This Feature drops \\nin next iteration', xy=(0.,0), xytext=(2, -1),\n             arrowprops=dict(facecolor='white',arrowstyle=\"->\",\n                             connectionstyle=\"arc3,rad=.5\",color='black',linewidth=0.8, alpha = 0.7), \n             #bbox = dict(boxstyle =\"round\", fc =\"white\", pad =0.25,color = 'darkorange'),\n            fontsize=8,fontfamily='serif',fontweight ='bold',ha='center', color='black', zorder = 3,\n            annotation_clip = False, alpha = 0.85)\nax.annotate('Final Features wrt VIF Feature Removal', xy=(14.75,25), xytext=(14.75,25),\n            fontsize=8,fontfamily='serif',fontweight ='bold',ha='center', color='black', zorder = 3,\n            annotation_clip = False, alpha = 0.85, rotation = 90)\n\n\n## titles and desc\n\n## titles and text\nfig.text(-0.05,0.98,'Women and Cancer: Normalized VIF of Features with Iterations', {'font':'serif','size':20, 'weight':'bold'}, alpha = 1)\nfig.text(-0.05,0.92,'''VIF are calculated for eachfeature, and removed the highest VIF feature (RED Bod) for next iteration.\nClearly all the highly correlated features are removed''',{'font':'serif','size':12, 'weight':'normal'}, alpha = 0.9)\n\nfig.text(0.5,0.90, \"Highest VIF\",{'font':'serif','size':14, 'weight':'bold', 'color':colors[0]})\nfig.text(0.65,0.90, '|',{'font':'serif','size':14, 'weight':'bold'})\nfig.text(0.68,0.90, \"Moderate VIF\",{'font':'serif','size':14, 'weight':'bold','color':colors[2]})\n\n\nfig.text(0.65,-0.05,'\u00a9 Made by bhuvanchennoju\/Kaggle',{'font':'serif', 'size':10,'weight':'bold'}, alpha = 0.7)\n\n\nfig.show()","0c1eccec":"\nX_temp = df.drop(columns = ['diagnosis'])\ny_temp = df['diagnosis']\ntemp_X_train,temp_X_val,temp_y_train,temp_y_val = train_test_split(X_temp,y_temp, test_size = 0.2, random_state = 2021)\n\ntemp_model= XGBClassifier(eval_metric='logloss').fit(temp_X_train,temp_y_train)\n\nperm = PermutationImportance(temp_model, scoring = 'r2').fit(temp_X_val,temp_y_val)\neli5_feature_importance1 = (pd.DataFrame({'Features':temp_X_train.columns.tolist(),'Importance':perm.feature_importances_})\n                           .sort_values(by = 'Importance'))\nperm_imp_feats1 = (eli5_feature_importance1.sort_values(by = 'Importance', ascending = False)\n                         .reset_index(drop = True))['Features'][0:15]\n\nprint(color_class.BOLD_COLOR + 'Feature Importance with r2 metric same as VIF....')\neli5.show_weights(perm, feature_names = temp_X_val.columns.tolist())","68961d34":"print(color_class.BOLD_COLOR + '\\nFitting train data on linear regression to get accuracy,r2 ,and roc_aoc scores...\\n' + color_class.END)\nvif_features = VIF_dfs['iter_14']['Features'].values.tolist()\ndef test_linear_features():\n        X_orig = df.drop(columns = ['diagnosis'])\n        y_orig = df['diagnosis']\n        X_train,X_val,y_train,y_val = train_test_split(X_orig,y_orig, test_size = 0.2, random_state = 2021)\n        \n        # linear regression\n        orig_preds = LinearRegression().fit(X_train.values,y_train.values).predict(X_val.values)\n        vif_preds = LinearRegression().fit(X_train[vif_features].values,y_train.values).predict(X_val[vif_features].values)\n        perm_preds = LinearRegression().fit(X_train[perm_imp_feats1].values,y_train.values).predict(X_val[perm_imp_feats1].values)\n        \n        \n        orig_r2 = round(r2_score(y_val, orig_preds),3)\n        vif_r2 = round(r2_score(y_val, vif_preds),3)\n        perm_r2 = round(r2_score(y_val, perm_preds),3)\n        \n        from sklearn.metrics import f1_score\n\n        orig_ypred_class = orig_preds > 0.85\n        vif_ypred_class = vif_preds > 0.85\n        perm_ypred_class = perm_preds > 0.85\n        \n        orig_auc = round(accuracy_score(y_val, orig_ypred_class),3)\n        vif_auc = round(accuracy_score(y_val, vif_ypred_class),3)\n        perm_auc = round(accuracy_score(y_val, perm_ypred_class),3)\n        \n        orig_roc_auc = round(roc_auc_score(y_val, orig_preds),3)\n        vif_roc_auc = round(roc_auc_score(y_val, vif_preds),3)\n        perm_roc_auc = round(roc_auc_score(y_val, perm_preds),3)\n       \n    \n        orig_list = [orig_auc,orig_r2,orig_roc_auc]\n        vif_list = [vif_auc, vif_r2,vif_roc_auc]\n        perm_list = [perm_auc,perm_r2,perm_roc_auc]\n        return orig_list,vif_list,perm_list\n        \norig_list,vif_list, perm_list = test_linear_features()\nprint(color_class.BOLD + '\\nAccuracy score, r2 score, and roc_auc score:\\n ' + color_class.END)\nprint(color_class.BOLD + 'Orginal Features: ' + color_class.END + color_class.BOLD_COLOR + str(orig_list) + color_class.END + '\\n')\nprint(color_class.BOLD + 'Permutation Feature Selection: ' + color_class.END + color_class.BOLD_COLOR + str(vif_list) + color_class.END + '\\n')\nprint(color_class.BOLD + 'Variance Infaltion Factor based Feature Selection: ' + color_class.END + color_class.BOLD_COLOR + str(perm_list) + color_class.END + '\\n')","59b67510":"print(color_class.BOLD_COLOR +'\\nHelper function for the pca visualization with diagnosis hue... \\n'+ color_class.END)\n\nclass pca_viz():\n    \n    def __init__(self,feat,tar,ax): \n        self.feat = feat\n        self.tar = tar\n        self.ax = ax\n\n    def visualize_data(self):\n        \n        temp_y = pd.DataFrame({'y':self.tar})\n        \n        pca = PCA(n_components= 2).fit_transform(self.feat)\n        # plotting\n        self.ax.scatter(pca[temp_y['y'] == 0][:,0], pca[temp_y['y'] == 0][:,1], c = colors[2], s = 50, linewidth =1, ec = 'black')\n        self.ax.scatter(pca[temp_y['y'] == 1][:,0], pca[temp_y['y'] == 1][:,1], c = colors[0], s = 50, linewidth =1,ec ='black')\n   \n        #self.ax.set_xticklabels('')\n        #self.ax.set_yticklabels('')","d8eac634":"fig = plt.figure(figsize =(14,14))\ngs = fig.add_gridspec(10,10)\ngs.update(wspace = 10,hspace = 2)\n\n#ax0 = fig.add_subplot(gs[:,:])\nax1 = fig.add_subplot(gs[1:5, 0:5])\nax2 = fig.add_subplot(gs[1:5, 5:10])\nax3 = fig.add_subplot(gs[6:10, 2:8])\n\n\naxes = [ax1,ax2,ax3]\n\ndata_ = df.copy()\n\n## data with outlier removal \nX_orig = data_.drop(columns = ['diagnosis']).values\nX_perm = data_.drop(columns = ['diagnosis'])[perm_imp_feats1].values\ny_orig = data_['diagnosis'].values\n\n## data with VIF featue selection\n\nX_vif = data_[vif_features].values\n\n# plots\npca_viz(feat = X_perm,tar = y_orig,ax=axes[0]).visualize_data()\npca_viz(feat = X_vif, tar = y_orig,ax=axes[1]).visualize_data()\npca_viz(feat = X_orig,tar = y_orig,ax=axes[2]).visualize_data()\n\n\n# text and title\n## titles and text\naxes[0].text(-50,570, 'Permutation Feature Selection',{'font':'serif','size':14, 'weight':'bold'}, zorder =3)\naxes[1].text(-100,22.5, 'VIF Feature Selection',{'font':'serif','size':14, 'weight':'bold'})\naxes[2].text(0,675, 'Original Features',{'font':'serif','size':14, 'weight':'bold'})\n\n## add scores\n\n#permutation annotations\naxes[0].annotate('Acc: {}'.format(perm_list[0]),(1600,440),(1600,440), zorder =3, annotation_clip = False,\n                 fontsize=12,fontfamily='serif',fontweight ='bold',ha='center', color='black',alpha = 0.85)\naxes[0].annotate(' R2: {}'.format(perm_list[1]),(1600,400),(1600,400), zorder =3, annotation_clip = False,\n                 fontsize=12,fontfamily='serif',fontweight ='bold',ha='center', color='black',alpha = 0.85)\naxes[0].annotate('AUC: {}'.format(perm_list[2]),(1600,350),(1600,350), zorder =3, annotation_clip = False,\n                 fontsize=12,fontfamily='serif',fontweight ='bold',ha='center', color='black',alpha = 0.85)\n\n#VIF annotations\naxes[1].annotate('Acc: {}'.format(vif_list[0]),(1000,18),(1000,18), zorder =3, annotation_clip = False,\n                 fontsize=12,fontfamily='serif',fontweight ='bold',ha='center', color='black',alpha = 0.85)\naxes[1].annotate(' R2: {}'.format(vif_list[1]),(1000,16.5),(1000,16.5), zorder =3, annotation_clip = False,\n                 fontsize=12,fontfamily='serif',fontweight ='bold',ha='center', color='black',alpha = 0.85)\naxes[1].annotate('AUC: {}'.format(vif_list[2]),(1000,15),(1000,15), zorder =3, annotation_clip = False,\n                 fontsize=12,fontfamily='serif',fontweight ='bold',ha='center', color='black',alpha = 0.85)\n\n# Original annotations\naxes[2].annotate('Acc: {}'.format(orig_list[0]),(1900,510),(1900,510), zorder =3, annotation_clip = False,\n                 fontsize=12,fontfamily='serif',fontweight ='bold',ha='center', color='black',alpha = 0.85)\naxes[2].annotate(' R2: {}'.format(orig_list[1]),(1900,470),(1900,470), zorder =3, annotation_clip = False,\n                 fontsize=12,fontfamily='serif',fontweight ='bold',ha='center', color='black',alpha = 0.85)\naxes[2].annotate('AUC: {}'.format(orig_list[2]),(1900,425),(1900,425), zorder =3, annotation_clip = False,\n                 fontsize=12,fontfamily='serif',fontweight ='bold',ha='center', color='black',alpha = 0.85)\n\n\n# text and titles\n\nfig.text(0.05,0.95,'Women and Cancer: Linear model Perforamace with PFI,VIF,and Original Data', {'font':'serif','size':20, 'weight':'bold'}, alpha = 1)\nfig.text(0.05,0.9,'''Though the feature selection is done based on r2 metric, for comparision \nof accuracy,r2, and auc scores among PFI, VIF and Orginal data with LinearRegression.\nVIF based feature selection should give edge here...  ''',{'font':'serif','size':14, 'weight':'normal'}, alpha = 0.9)\n\nfig.text(0.7,0.88, \"Cancerous\",{'font':'serif','size':16, 'weight':'bold', 'color':colors[0]})\nfig.text(0.8,0.88, '|',{'font':'serif','size':16, 'weight':'bold'})\nfig.text(0.81,0.88, \"Healthy\",{'font':'serif','size':16, 'weight':'bold','color':colors[2]})\n\nfig.text(0.75,0.075,'\u00a9 Made by bhuvanchennoju\/Kaggle',{'font':'serif', 'size':9,'weight':'bold'}, alpha = 0.8)\n\nfig.show()","b6baf8c2":"\nX_temp = df.drop(columns = ['diagnosis'])\ny_temp = df['diagnosis']\ntemp_X_train,temp_X_val,temp_y_train,temp_y_val = train_test_split(X_temp,y_temp, test_size = 0.2, random_state = 2021)\n\ntemp_model= XGBClassifier(eval_metric='logloss').fit(temp_X_train,temp_y_train)\n\nperm = PermutationImportance(temp_model, scoring = 'roc_auc').fit(temp_X_val,temp_y_val)\neli5_feature_importance2 = (pd.DataFrame({'Features':temp_X_train.columns.tolist(),'Importance':perm.feature_importances_})\n                           .sort_values(by = 'Importance'))\nperm_imp_feats_auc = (eli5_feature_importance2.sort_values(by = 'Importance', ascending = False)\n                         .reset_index(drop = True))['Features']\n\nprint(color_class.BOLD_COLOR + 'Feature Importance with roc_auc metric....')\neli5.show_weights(perm, feature_names = temp_X_val.columns.tolist())","6eb61b98":"\n## dataframe as per feature selection from permutation importance\ntemp_X_df = df.drop(columns ='diagnosis').copy()\ntemp_X_df = temp_X_df[perm_imp_feats_auc]\ntemp_y_df = df['diagnosis']\n\n## crossvalidation with repeated feature selection\nstratified = StratifiedKFold(n_splits = 5,shuffle = True, random_state = 2021)\n \nfeat_acc = []\nfeat_auc = []\nfeat_f1 = []\nfor idx,feat in enumerate (perm_imp_feats_auc):\n    temp = temp_X_df.iloc[:,:idx]\n    temp['all_other'] = temp_X_df.iloc[:,idx:len(perm_imp_feats_auc)].sum(axis = 1)\n    X_ = temp\n    y_ = temp_y_df\n    \n    if idx == 0:\n        continue\n    else:\n        fold_acc = []\n        fold_auc = []\n        fold_f1 = []\n        for train_idx,valid_idx in stratified.split(X_,y_):\n    \n            xtrain,xvalid = X_.iloc[train_idx],X_.iloc[valid_idx]\n            ytrain,yvalid = y_.iloc[train_idx],y_.iloc[valid_idx]\n            \n            model = XGBClassifier(eval_metric = 'logloss').fit(xtrain.values,ytrain.values)\n            preds = model.predict(xvalid.values)\n            \n            acc_score= accuracy_score(yvalid,preds)\n            auc_score = roc_auc_score(yvalid,preds)\n            f1 = f1_score(yvalid,preds)\n            fold_acc.append(acc_score)\n            fold_auc.append(auc_score)\n            fold_f1.append(f1)\n            \n\n    feat_acc.append(round(np.mean(fold_acc),2))  \n    feat_auc.append(round(np.mean(fold_auc),2))\n    feat_f1.append(round(np.mean(fold_f1),2))","b85e1f3b":"fig,ax = plt.subplots(1,2,figsize =(12,6))\n\n## accuracy vs number of features from permutation importance based feature selection\nax[0].plot(np.arange(0,len(feat_acc),1),feat_acc, color = colors[0], linewidth = 2)\nax[0].scatter(x =np.arange(0,len(feat_acc),1),y=feat_acc, \n              color = colors[1], s = 75,zorder = 3,\n              linewidth = 1,ec = 'black')\nax[0].set_ylabel('Cross-Validation Accuracy Mean',{'font':'serif','size':12, 'weight':'bold'}, alpha = 0.95)\nax[0].set_xlabel('Number of Features',{'font':'serif','size':12, 'weight':'bold'}, alpha = 0.95)\n\n## area under curve vs number of features from permutation importance based feature selection\nax[1].plot(np.arange(0,len(feat_auc),1),feat_auc, color=colors[0], linewidth = 2)\nax[1].scatter(x =np.arange(0,len(feat_auc),1),y=feat_auc,\n              color= colors[2], s= 75,zorder =3,\n               linewidth = 1,ec = 'black')\nax[1].set_ylabel('Cross-Validation AUC Mean',{'font':'serif','size':12, 'weight':'bold'}, alpha = 0.95)\nax[1].set_xlabel('Number of Features',{'font':'serif','size':12, 'weight':'bold'}, alpha = 0.95)\n\n### title and annotations\n## titles and text\nfig.text(-0.05,1.18,'Women and Cancer: Influence of Number of Features on Metric', {'font':'serif','size':22, 'weight':'bold'}, alpha = 1)\nfig.text(-0.05,1.07,'''This plot shows clearly that, even single feature from feature selection is giving\n0.8 accuracy, and with increase in number of features accuracy and auc increased. \nBut not after 10 to 15 featrues theres in nothing much of change.''',{'font':'serif','size':14, 'weight':'normal'}, alpha = 0.95)\n\nfig.text(0.61,0.99, \"Accuracy\",{'font':'serif','size':14, 'weight':'bold', 'color':colors[1]})\nfig.text(0.7,0.99, '|',{'font':'serif','size':14, 'weight':'bold'})\nfig.text(0.72,0.99, \"Area Under Curve\",{'font':'serif','size':14, 'weight':'bold','color':colors[2]})\n\n\nfig.text(0.70,-0.01,'\u00a9 Made by bhuvanchennoju\/Kaggle',{'font':'serif', 'size':10,'weight':'bold'}, alpha = 0.8)\n\nfig.tight_layout(pad = 2.5, w_pad = 2.5)\n\nfig.show()","739fda83":"temp_df = temp_X_df.iloc[:,0:10]\ntemp_df['all_other'] = temp_X_df.iloc[:,10:len(perm_imp_feats_auc)].sum(axis = 1)\ncols = temp_df.columns\ntemp_xtrain,temp_xtest, temp_ytrain,temp_ytest = train_test_split(temp_df, temp_y_df, test_size = 0.2)\n\ntemp_model = XGBClassifier(eval_metric = 'logloss')\ntemp_model.fit(temp_xtrain,temp_ytrain)\n\n### shapvalues \nexplainer = shap.TreeExplainer(temp_model)\n\nshap_values = explainer.shap_values(temp_xtest)\n\ncmap = mpl.colors.LinearSegmentedColormap.from_list(\"\",[colors[1],colors[2],colors[0]])\nshap.summary_plot(shap_values,temp_xtest,\n                  show = False,cmap = cmap)\n\n# plot settings\n## titles and text\nplt.gcf().text(-0.1,1.1,'Women and Cancer: SHAP Values and Features', {'font':'serif','size':22, 'weight':'bold'}, alpha = 1)\nplt.gcf().text(-0.1,0.98,'''This visualizaiton enables us to understand the feature importance\nand global interpretation. I have discussed about SHAP Values in detail\nin my othernotebook...''',{'font':'serif','size':14, 'weight':'normal'}, alpha = 0.95)\n\n\nplt.gcf().text(0.65,-0.01,'\u00a9 Made by bhuvanchennoju\/Kaggle',{'font':'serif', 'size':10,'weight':'bold'}, alpha = 0.8)\n\n\n\nplt.gcf().show()","27315a0d":"### helper function\ndef plot_feat(axes_idx = None, data_ = None, scaler_method = None, color = None):\n    \n    col_names = data_.columns\n  \n    scaled_array = scaler_method.fit_transform( data_)\n    \n    # scaled dataframe\n    scaled_df = pd.DataFrame(scaled_array, columns=col_names)\n     \n    skew_scaler = []\n    for idx, col in zip(axes_idx, col_names):\n        col_skew = skew(scaled_df[col])\n        if col_skew > 1.5: \n            temp = np.log1p(scaled_df[col] + 0.5)\n        else:\n            temp = scaled_df[col]\n       \n        ## plot\n        sns.kdeplot(x = temp, ax = axes[idx],\n                   color = color,fill = True, alpha = 1,\n                   linewidth = 3,ec = 'black')\n        \n        \n        skew_scaler.append(col_skew)\n        \n        xlabel = ' '.join([value.capitalize() for value in str(col).split('_') ])\n        #ax.set_facecolor(colors[-1])\n        axes[idx].axes.get_yaxis().set_visible(False)\n        axes[idx].axes.set_xlabel(xlabel,{'font':'serif','size':14, 'weight':'bold'}, alpha = 1)\n        \n    return skew_scaler\n        \n### Scalers ans axis indicies\n\nscaler_list = [StandardScaler(), MinMaxScaler(), RobustScaler()]\naxes_np_list = [np.arange(0,30,3).tolist(), np.arange(1,30,3).tolist(), np.arange(2,30,3).tolist()]\ncolors_list = [colors[0],colors[1],colors[2]]\ndata = temp_X_df.iloc[:,0:10]\n    \n## plotting \nfig,ax = plt.subplots(10,3, figsize = (15,30))\naxes = ax.ravel()\n\nscaler_skews = []\nfor axes_idx_list,scaler, color in zip(axes_np_list,scaler_list, colors_list):\n    \n    skewness = plot_feat(axes_idx = axes_idx_list, data_ = data, scaler_method = scaler,color=color)\n    scaler_skews.append(skewness)\n    \n    \nplt.tight_layout()\n\n\n## titles and text\nfig.text(0,1.045,'Women and Cancer: Influence of Scaling on Data', {'font':'serif','size':22, 'weight':'bold'}, alpha = 1)\nfig.text(0,1.02,'''Three Common approches for Data Scaling are explored here...As all the\noutliers are removed and we couldnt expect much of change... but it good to know what\nand what of fundamentals...''',{'font':'serif','size':14, 'weight':'normal'}, alpha = 1)\n\nfig.text(0.50,1.005, \"Standardization\\nStandardScaler\",{'font':'serif','size':14, 'weight':'bold', 'color':colors[0]})\nfig.text(0.63,1.01, '|',{'font':'serif','size':27, 'weight':'bold'})\nfig.text(0.64,1.005, \"Normalization\\nMinMaxScaler\",{'font':'serif','size':14, 'weight':'bold','color':colors[1]})\nfig.text(0.75,1.01, '|',{'font':'serif','size':27, 'weight':'bold'})\nfig.text(0.76,1.005, \"OutlierRemoval\\nRobustScaler\",{'font':'serif','size':14, 'weight':'bold','color':colors[2]})\n\n\nfig.text(0.73,0,'\u00a9 Made by bhuvanchennoju\/Kaggle',{'font':'serif', 'size':10,'weight':'bold'}, alpha = 0.8)\n\nfig.show()\n","5635b00d":"print(color_class.BOLD_COLOR + '\\nFinal Data Scaling with StandardScaler.....\\n'+color_class.END)\n\n## final data\nxdata= df.drop(columns = ['diagnosis'])\nxdata = xdata[perm_imp_feats_auc].iloc[:,0:10]\nydata = df['diagnosis']\n\n## final data shapes\nprint(color_class.BOLD + '\\nShape of features Data: '+color_class.END+\\\n      color_class.BOLD_COLOR+ str(xdata.shape) + color_class.END)\nprint(color_class.BOLD + 'Shape of target Data: ' + color_class.END+\\\n      color_class.BOLD_COLOR+ str(ydata.shape) + color_class.END)\nprint(color_class.BOLD_COLOR+ '\\nAll set for final modeling...\\n' + color_class.END)","c64b9012":"classifiers = []\nclassifiers.append(LogisticRegression(random_state = 2021))\nclassifiers.append(SVC(random_state=2021, probability = True))\nclassifiers.append(KNeighborsClassifier())\nclassifiers.append(AdaBoostClassifier(DecisionTreeClassifier(random_state=2021)))\nclassifiers.append(RandomForestClassifier(random_state=2021))\nclassifiers.append(GradientBoostingClassifier(random_state=2021))\nclassifiers.append(ExtraTreesClassifier(random_state= 2021))\nclassifiers.append(XGBClassifier(random_state = 2021,eval_metric = 'logloss'))\nclassifiers.append(LGBMClassifier(random_state = 2021))","118dbac4":"stratified = StratifiedKFold(n_splits = 5, shuffle = True, random_state  = 20)\nAlgorithms = [\"Logistic\",\"SVC\",\"KNeighbors\",\"AdaBoost\",\n              \"RandomForest\",\"GradientBoosting\",\n              \"ExtraTrees\",\"XGBoost\", \"LightGBM\"]\n\n## empty lists to stores values and states\nclass_accuracy = []\nclass_f1 = []\nclass_auc = []\nclass_preds = []\nclass_class_states= []\nclass_valid_truths = []\nclass_valid_features = []\nclass_cm = []\n\nfor classifier,algo in zip(classifiers,Algorithms):\n    \n    fold_accuracy = []\n    fold_f1 = []\n    fold_roc_auc = []\n    fold_preds = []\n    fold_class_states = []\n    fold_valid_truths = []\n    fold_valid_features = []\n    fold_cm = []\n   \n    n = 0\n    \n    print(color_class.BOLD + '*'*17+  color_class.END + color_class.BOLD_COLOR + str(algo) + color_class.END + color_class.BOLD +  '*'*17 + color_class.END)\n    for train_idx,valid_idx in stratified.split(xdata,ydata):\n        xtrain,xvalid = xdata.iloc[train_idx],xdata.iloc[valid_idx]\n        ytrain,yvalid = ydata.iloc[train_idx],ydata.iloc[valid_idx]\n    \n        ## scaling \n        ss = StandardScaler()\n        xtrain = ss.fit_transform(xtrain)\n        xvalid = ss.transform(xvalid)\n    \n        \n        # model \n        model = classifier\n        model.fit(xtrain,ytrain)\n        preds = model.predict(xvalid)\n        \n        ## scores\n        #### fold results, feaures,preds, states\n        accuracy = accuracy_score(yvalid, preds)\n        f1 = f1_score(yvalid,preds)\n        roc_auc = roc_auc_score(yvalid,preds)\n        cm = confusion_matrix(yvalid,preds)\n        \n        fold_accuracy.append(accuracy)\n        fold_f1.append(f1)\n        fold_roc_auc.append(roc_auc)\n        fold_preds.append(preds)\n        fold_class_states.append(model)\n        fold_valid_truths.append(np.array(yvalid).astype(int))\n        fold_valid_features.append(xvalid)\n        fold_cm.append(cm)\n        \n        \n        ## printing results \n        print(color_class.BOLD)\n        print(\"fold{}: Accuracy: {}, F1:{}, Roc_Auc: {} \".format(n, round(accuracy,2),round(f1,2),round(roc_auc,2)))\n        print(color_class.END)\n        \n        n+=1\n        \n    #### class results, feaures,preds, states\n    class_accuracy.append(fold_accuracy)\n    class_f1.append(fold_f1)\n    class_auc.append(fold_auc)\n    class_preds.append(fold_preds)    \n    class_valid_truths.append(fold_valid_truths)\n    class_valid_features.append(fold_valid_features)\n    class_cm.append(fold_cm)\n    class_class_states.append(fold_class_states)\n    \n    ## breif result dynamic prints\n\n    print( color_class.BOLD+ '\\n'+'*'*10 +'Means'+ '*'*10+'\\n' + color_class.END)\n    print(color_class.BOLD_COLOR)\n    print('Accuracy Mean: {}'.format(round(np.mean(fold_accuracy),2)))\n    print('F1 Mean: {}'.format(round(np.mean(fold_f1),2)))\n    print('ROC_AUC Mean: {}'.format(round(np.mean(fold_roc_auc),2)))\n    print(color_class.END)\n    print('\\n'+  color_class.BOLD+'*'*30 + color_class.END +'\\n')","625448d6":"print(color_class.BOLD_COLOR+ 'Storing results into dataframe....\\n'+ color_class.END)\n\nresults_df = (pd.DataFrame({'Algorithms': Algorithms,\n                            'Mean Accuracy':np.row_stack(class_accuracy).mean(axis = 1), \n                            'Mean F1':np.row_stack(class_f1).mean(axis = 1),\n                            'Mean Roc_Auc':np.row_stack(class_accuracy).mean(axis = 1),\n                            'Classifier Preds':class_preds,\n                            'Classifier Valid Truths':class_valid_truths, \n                            'Classifier Valid Features':class_valid_features,\n                            'Classifier CM':class_cm,\n                            'Classifier States':class_class_states})\n              .sort_values(by = 'Mean F1',ascending =True)\n              .reset_index(drop = True))\n\nresults_df['Preds_array'] = results_df['Classifier Preds'].apply(lambda x: np.array(x).ravel())\nresults_df['Truths_array'] = results_df['Classifier Valid Truths'].apply(lambda x: np.array(x).ravel())\n\nprint(color_class.BOLD)\nprint(results_df.head(1).T)","e5262a36":"fig, ax = plt.subplots(1,2,figsize = (18,10))\n\naxes = ax.ravel()\n\naxes[0].invert_xaxis()\ncolor_list = results_df['Mean Accuracy'].apply(lambda x: colors[2])\naxes[0].barh(y = results_df['Algorithms'], width = round(results_df['Mean Accuracy'],3), height = 0.5, color = color_list)\nfor pa in ax[0].patches:\n    ax[0].text(pa.get_width(),pa.get_y()+pa.get_height()\/2, pa.get_width(), ha = 'right', va = 'center',\n              **{'font':'serif','size':10,'weight':'bold'})\n\n\ncolor_list1 = results_df['Mean F1'].apply(lambda x: colors[0] )\naxes[1].barh(y = results_df['Algorithms'], width = round(results_df['Mean F1'],3), height = 0.5, color = color_list1)\nfor pa in ax[1].patches:\n    ax[1].text(pa.get_width(),pa.get_y()+pa.get_height()\/2, pa.get_width(), ha = 'left',va = 'center',\n              **{'font':'serif','size':10,'weight':'bold'})\n\n\naxes[0].set_yticklabels('')\naxes[1].set_yticklabels(results_df['Algorithms'], {'font':'serif','size':12,'weight':'bold'},rotation = 0,ha= 'center')\naxes[1].tick_params(axis = 'y',pad = 75)\naxes[0].set_xticklabels('')\naxes[1].set_xticklabels('')\n\n\n## titles and text\nfig.text(0,0.945,'Women and Cancer: Crossvalidation Fold Means and Classifiers', {'font':'serif','size':22, 'weight':'bold'}, alpha = 1)\nfig.text(0,0.89,'''It seems both Logitsticregression,and SVC classifiers are doing best job. Even F1 score is \ngood for the given models. Adaboost and Decision Tree are kind of over fitted data.''',{'font':'serif','size':14, 'weight':'normal'}, alpha = 0.9)\n\nfig.text(0.15,0.825, \"Fold Accuracy Mean\",{'font':'serif','size':18, 'weight':'bold', 'color':'black'})\nfig.text(0.5,0.825, '|',{'font':'serif','size':24, 'weight':'bold'})\nfig.text(0.65,0.825, \"Fold F1 Score Mean\",{'font':'serif','size':18, 'weight':'bold','color':'black'})\n\n\nfig.text(0.72,0.15,'\u00a9 Made by bhuvanchennoju\/Kaggle',{'font':'serif', 'size':10,'weight':'bold'}, alpha = 0.85)\n\n\n\n\n\nfig.tight_layout(pad = 10,w_pad = 1, h_pad = 10)\nfig.show()","883ea7c5":"my_cmap = mpl.colors.LinearSegmentedColormap.from_list(\"\",[colors[0],colors[1],colors[2]])\n\n\n\nfig,ax = plt.subplots(3,3, figsize=(10,10))\n\nfor truth,pred,axes,algo in zip(results_df['Truths_array'],\n                                results_df['Preds_array'],\n                                ax.ravel(),results_df['Algorithms']):\n\n    \n    cf_mat = confusion_matrix(truth,pred)\n            \n    #### annotations\n    labels = ['True Neg','False Pos','False Neg','True Pos']\n    counts = [\"{0:0.0f}\".format(value) for value in cf_mat.flatten()]\n    percentages = [\"{0:.2%}\".format(value) for value in cf_mat.flatten()\/np.sum(cf_mat)]\n            \n    #### final annotations\n    label = (np.array([f'{v1}\\n{v2}\\n{v3}' for v1,v2,v3 in zip(labels,counts,percentages)])).reshape(2,2)\n            \n    #### scores\n    f1 = f1_score(truth,pred)\n    auc = roc_auc_score(truth,pred)\n    \n    #heatmap\n    sns.heatmap(data = cf_mat, vmin = 0, vmax =330, cmap = my_cmap,linewidth=2,linecolor = 'white',square = True,\n    ax = axes, annot = label, fmt ='', cbar = False, annot_kws = {'font':'serif','size':10, 'color':'black','weight':'bold','alpha':0.8}, alpha =1)\n        \n    axes.text(0,-0,'{}'.format(algo),{'font':'serif','size':12, 'color':'black', 'weight':'bold'})\n    \n    axes.scatter( 1 , 1 , s = 3500, c = 'white')\n    axes.text(0.72,1.1, ' F1: {}\\nAUC: {}'.format(round(f1,2), round(auc,2)),{'font':'serif','size':10, 'color':'black', 'weight':'bold'})\n    \n    ## ticks and labels\n    axes.set_xticklabels('')\n    axes.set_yticklabels('')\n    \n    \n    \n## titles and text\nfig.text(0,1.05,'Women and Cancer: Crossvalidataion Results', {'font':'serif','size':22, 'weight':'bold'}, alpha = 1)\nfig.text(0,1,'''This Visualization show the results of various classifiers and there respective\nresults.''',{'font':'serif','size':14, 'weight':'normal'}, alpha = 0.98)\n\n\n\nfig.text(0.72,0.,'\u00a9 Made by bhuvanchennoju\/Kaggle',{'font':'serif', 'size':10,'weight':'bold'}, alpha = 0.85)\n\n\nfig.tight_layout(pad = 2.5, w_pad = 2.5,h_pad = 2.5)\nfig.show()","6b7d6439":"print(color_class.BOLD + 'Hyperparameters tunning grid...'+ color_class.END)\nclassifiers_params = {\n    \n    LogisticRegression(): {'C':[0.001,0.01,0.05,0.1,0.5,1,10,100,200,1000] , \n                  'penalty': ['l1','l2']} ,\n    \n    LGBMClassifier():     {\n                  'class_weight': [{1:6,0:4},{1:7,0:3},{1:8,0:4}],\n                  'n_estimators': np.arange(100,3000,250),\n                  'num_leaves': np.arange(10,50,10),\n                  'learning_rate': [0.01,0.05,0.1,0.5]},\n                                         \n    RandomForestClassifier() :     {\n                   'class_weight': [{1:6,0:4}],\n                   'max_depth': [2,4,6,8,10],\n                   'max_leaf_nodes': [5,10,15],\n                   'n_estimators': np.arange(100,2000,500)} ,\n    AdaBoostClassifier() :    {\n                   'base_estimator': [DecisionTreeClassifier()],\n                   'learning_rate': [0.01,0.05,0.1],\n                   'n_estimators': np.arange(100,1000,500)} ,\n}","da024757":"print(color_class.BOLD_COLOR+ 'Gridsearch CV implementation with predefined grid params'+ color_class.END +'\\n')\nxtrain,xtest,ytrain,ytest = train_test_split(xdata,ydata,random_state = 2021,shuffle = True,stratify= ydata)\n\n\nss = StandardScaler()\n\nxtrain = ss.fit_transform(xtrain)\nxtest = ss.transform(xtest)\n\nbest_est = []\nbest_pms = []\n\nfor clf,params in classifiers_params.items():\n    print(color_class.BOLD_COLOR + '*'*20 + color_class.END +'\\n')\n    \n    gs = GridSearchCV(estimator= clf, param_grid = params,cv = stratified, verbose= 2,scoring = 'roc_auc',n_jobs = -1)\n    #gs = clf\n    gs.fit(xtrain,ytrain)\n    best_estimator = gs.best_estimator_\n    best_params = gs.best_params_\n    best_est.append(best_estimator)\n    best_pms.append(best_params)\n    preds = best_estimator.predict(xtest)\n    \n    print(color_class.BOLD)\n    print(best_estimator)\n\n    print('Accuracy: {}'.format(accuracy_score(ytest,preds)))\n    print('Roc_Auc: {}'.format(round(roc_auc_score(ytest,preds),8)))\n    ","01001721":"print(color_class.BOLD_COLOR+ 'Results appending from gridsearch...'+'\\n'+ color_class.END)\nacc_list = []\ncm_list = []\nf1_list = []\nauc_list = []\nfor clf in best_est:\n    preds = clf.predict(xtest)\n    cm = confusion_matrix(ytest,preds)\n    acc = accuracy_score(ytest,preds)\n    f1 = f1_score(ytest,preds)\n    auc = roc_auc_score(ytest,preds)\n    cm_list.append(cm)\n    acc_list.append(acc)\n    f1_list.append(f1)\n    auc_list.append(auc)\nhyper_results_df = pd.DataFrame({'Algorithms':['LogisticRegression','LGBMClassifier','RandomForestClassifier','AdaBoostClassifier'],\n                                 'Accuracy':acc_list,\n                                 'f1_score':f1_list,\n                                  'roc_auc_score':auc_list,\n                                  'confusion_matrix':cm_list})\nprint(color_class.BOLD + '\\n')\nprint(hyper_results_df.head())","56e1cc82":"my_cmap = mpl.colors.LinearSegmentedColormap.from_list(\"\",[colors[0],colors[1],colors[2]])\n\n\n\nfig,ax = plt.subplots(2,2, figsize=(8,8))\n\nfor algo,f1,auc,cm,axes in zip(hyper_results_df['Algorithms'],\n                                hyper_results_df['f1_score'],\n                                hyper_results_df['roc_auc_score'],\n                                hyper_results_df['confusion_matrix'],\n                                ax.ravel()):\n\n    \n    cf_mat = cm\n            \n    #### annotations\n    labels = ['True Neg','False Pos','False Neg','True Pos']\n    counts = [\"{0:0.0f}\".format(value) for value in cf_mat.flatten()]\n    percentages = [\"{0:.2%}\".format(value) for value in cf_mat.flatten()\/np.sum(cf_mat)]\n            \n    #### final annotations\n    label = (np.array([f'{v1}\\n{v2}\\n{v3}' for v1,v2,v3 in zip(labels,counts,percentages)])).reshape(2,2)\n            \n    #### scores\n    \n    #heatmap\n    sns.heatmap(data = cf_mat, vmin = 0, vmax =84, cmap = my_cmap,linewidth=2,linecolor = 'white',square = True,\n    ax = axes, annot = label, fmt ='', cbar = False, annot_kws = {'font':'serif','size':10, 'color':'black','weight':'bold','alpha':0.8}, alpha =1)\n        \n    axes.text(0,-0,'{}'.format(algo),{'font':'serif','size':12, 'color':'black', 'weight':'bold'})\n    \n    axes.scatter( 1 , 1 , s = 3500, c = 'white')\n    axes.text(0.72,1.1, ' F1: {}\\nAUC: {}'.format(round(f1,2), round(auc,2)),{'font':'serif','size':10, 'color':'black', 'weight':'bold'})\n    \n    ## ticks and labels\n    axes.set_xticklabels('')\n    axes.set_yticklabels('')\n    \n    \n    \n## titles and text\nfig.text(0,1.05,'Women and Cancer: GridSearch Results', {'font':'serif','size':22, 'weight':'bold'}, alpha = 1)\nfig.text(0,1,'''This Visualization show the results of various classifiers and there respective\nresults.''',{'font':'serif','size':14, 'weight':'normal'}, alpha = 0.98)\n\n\n\nfig.text(0.72,0.,'\u00a9 Made by bhuvanchennoju\/Kaggle',{'font':'serif', 'size':10,'weight':'bold'}, alpha = 0.85)\n\n\nfig.tight_layout(pad = 2.5, w_pad = 2.5,h_pad = 2.5)\nfig.show()","f42be895":"models = best_est\n\nstack_train,stack_test = stacking(models = best_est,\n                                  X_train = xtrain,\n                                  y_train = ytrain,\n                                  X_test = xtest,\n                                  regression = False, \n                                  metric = 'roc_auc',\n                                  n_folds = 5,shuffle = True,\n                                  stratified = True)\n\nfin_model = XGBClassifier(eval_metric='logloss')\n\nfin_model.fit(stack_train, ytrain)\nfin_preds = fin_model.predict(stack_test)\nprint(color_class.BOLD_COLOR+'Stacked Classification...'+'\\n'+ color_class.END)\nprint(color_class.BOLD)\nprint('accuracy: {}'.format(round(accuracy_score(ytest,fin_preds),3)))\nprint('roc_auc: {}'.format(round(roc_auc_score(ytest,fin_preds),3)))\nprint('f1:{}'.format(round(f1_score(ytest,fin_preds),3)))","1438abbc":"# my_cmap = mpl.colors.LinearSegmentedColormap.from_list(\"\",[colors[0],colors[1],colors[2]])\n\n\n\nfig,ax = plt.subplots(figsize=(8,8))\n\ncf_mat = confusion_matrix(ytest,fin_preds)\nf1 = f1_score(ytest,fin_preds)\nauc = roc_auc_score(ytest,fin_preds)\n            \n#### annotations\nlabels = ['True Neg','False Pos','False Neg','True Pos']\ncounts = [\"{0:0.0f}\".format(value) for value in cf_mat.flatten()]\npercentages = [\"{0:.2%}\".format(value) for value in cf_mat.flatten()\/np.sum(cf_mat)]\n            \n#### final annotations\nlabel = (np.array([f'{v1}\\n{v2}\\n{v3}' for v1,v2,v3 in zip(labels,counts,percentages)])).reshape(2,2)\n            \n#### scores\n    \n#heatmap\nsns.heatmap(data = cf_mat, vmin = 0, vmax =84, cmap = my_cmap,linewidth=2,linecolor = 'white',square = True,\n    ax = ax, annot = label, fmt ='', cbar = False, annot_kws = {'font':'serif','size':12, 'color':'black','weight':'bold','alpha':0.8}, alpha =1)\n        \nax.text(0,-0,'{}'.format('Stacked Classification'),{'font':'serif','size':12, 'color':'black', 'weight':'bold'})\n    \nax.scatter( 1 , 1 , s = 5000, c = 'white')\nax.text(0.85,1.05, ' F1: {}\\nAUC: {}'.format(round(f1,2), round(auc,2)),{'font':'serif','size':12, 'color':'black', 'weight':'bold'})\n    \n## ticks and labels\nax.set_xticklabels('')\nax.set_yticklabels('')\n    \n    \n    \n## titles and text\nfig.text(0,1.05,'Women and Cancer: Stacked Classification Results', {'font':'serif','size':22, 'weight':'bold'}, alpha = 1)\nfig.text(0,1,'''This Visualization show the results of stacked classification and there respective\nresults.''',{'font':'serif','size':14, 'weight':'normal'}, alpha = 0.98)\n\nfig.text(0.72,0.,'\u00a9 Made by bhuvanchennoju\/Kaggle',{'font':'serif', 'size':12,'weight':'bold'}, alpha = 0.85)\n\n\nfig.tight_layout(pad = 2.5, w_pad = 2.5,h_pad = 2.5)\nfig.show()","9a8d1742":"<a id = '4.4'><\/a>\n<h2 style = \"font-family: garamond; font-size: 35px; font-style: normal; background-color: #2ab7ca; color : #fed766; border-radius: 5px 5px; padding:5px; text-align:left; font-weight: bold\" >4.4 Stacked Classification<\/h2>","dddf6314":"<a id = '2.1'><\/a>\n<h2 style = \"font-family: garamond; font-size: 35px; font-style: normal; background-color: #2ab7ca; color : #fed766; border-radius: 5px 5px; padding:5px; text-align:left; font-weight: bold\" >2.1 Distribution of Targets<\/h2>","8ca94506":"With analysis of overall feature level overview, its good to see how featuresa are intereacting with reach other. As there are 30 features, it would be mad to attempt all the combinations of feature interactions. So, idea is to segregate the features into 5 or 6 groups and try to understand within those feature category interactions. \n<br><br>\nIdea is to featrue segaregate based on mean, se and worst from the naming of the features....\nlets get started...\n<p style = \"font-size:25px; color:#2ab7ca ;background-color:  ; text-align: left; border-radius: 5px 5px; padding: 5px\" ><strong>  Featues Segregation based on mean,se,and worst....<\/strong><\/p>","2dc4c7a1":"<p style = \"font-size:25px; color:#2ab7ca ;background-color:  ; text-align: left; border-radius: 5px 5px; padding: 5px\" ><strong> Lets have a glimpse on Datapoints...<\/strong><\/p>","eac680d4":"From the above correalation plot, we can seen that there are plenty of highly correlatid functions, so we can safely say we have multi-collinearity in our data. But what is collinearity? if the two features are highly correlated then we say we have collinearity, if same occured for multiple features then it is called multi-colliearity. In general sense, it like two parallel lines, u see both have same slope and never have a intersection point, like-wise here if two features are have same information with somekind of constant multiple or something else, can be called as colinear features.\n\nThis could be a big issue if we take a simple regresion model like linear regression, as the highly correalted features highly influence the model weights and predictions. Though we can take high road to circumvent this issue with better model or other techniques, lets see what are our option here to this kind of issue for general idea.\n\n\n<ul>\n<li>First idea is to use Variance Inflation Factor over the dataset, and remove features have VIF > Threshold. This is kind of iterative in nature, go on till all the features VIF is less than our Threshold. This Threshold varies based on situation and application, and genearal value of theshold is 5.<\/li>\n<li>Second idea to use feature selection or PCA techniques to remove this redundant features.<\/li>\n<li>Third idea to use Lasso, or ridge regression in place of linear regression as it the regularization could control this issue.<\/li>\n<\/ul>\n\n<br>\n<p style = \"font-size:25px; color:#2ab7ca ;background-color:  ; text-align: left; border-radius: 5px 5px; padding: 5px\" ><strong> Variable Inflation Factor and Feature Selection...<\/strong><\/p>\n<br>\n\nWhen there is collinearity in our features, what we do is to calculate VIF. How we do that? for each feature by make that feature itself target, and all other are features as fitting features and calcultae r2 for the curve fitting.\n<br>\nSimply saying, we write a linear equation for one fearture ( which is y) in terms of other features(which are c * x) and calculate the R2 for that curve fitting, and the calcultate the value of 1\/(1-r2) and this value is VIF. \n\nThere are several blogs and articles discussed about mathmatical ideaology and here I am exploring the basic idea of VIF with algorithm itself.\n","3a10e9b1":"<br>\n<h2 style = \"font-size:50px; font-family:Garamond ; font-weight : normal; background-color:#2ab7ca; color :  #fed766 ; text-align: center; border-radius: 5px 5px; padding: 5px\"> Women And Cancer: Analysis and Detection<\/h2> \n<br>\n<div class = 'image'> <img style=\"float:center; width:100%; border:5px solid #fed049;\" align=center src = https:\/\/media2.wnyc.org\/i\/620\/372\/c\/80\/1\/shutterstock_184214636.jpg> \n<\/div>\n<br>\n<br>","84da923e":"<p style = \"font-size:25px; color:#2ab7ca ;background-color:  ; text-align: left; border-radius: 5px 5px; padding: 5px\" ><strong> Cutsom Correlation Matrix Visualization....<\/strong><\/p>","88395d5f":"<br>\n<p style = \"font-size:25px; color:#2ab7ca ;background-color:  ; text-align: left; border-radius: 5px 5px; padding: 5px\" ><strong> Permutation Importance and Feature Selection...<\/strong><\/p>\n<br>\nAs I mentioned before, to cirumvent the collinearity issue, we can take different roads and one such is permutation feature importance. This idea of Permutation feature importance was disccused in great length in my previous notebook, you can find the base idea of permutaiton importance. Lets implement this here with eli5 library and compare VIF feautures as well later...","f3f1f203":"Till now  I have explored all the basic things in machine learning pipeline... Now coming to modeling.... Idea is something like this..\n\n<ol>\n    <li> List append few base classifiers and get crossvalidation scores, then select top three ad bottom two models <\/li>\n    <li> Perform hyperparameter tunning on these 5 models<\/li>\n    <li> Then apply  static weight blending<\/li>\n    <li> if possible apply dynamic weitht blending<\/li>\n<\/ol>","82ae8500":"<a id = '1.0'><\/a>\n<h2 style = \"font-family: garamond; font-size: 40px; font-style: normal;background-color: #2ab7ca; color : #fed766; border-radius: 5px 5px;padding:5px;text-align:center; font-weight: bold\" >1. Introduction<\/h2>\n<br>\n<div class = 'image'> <img style=\" width:60%\" align=center src = https:\/\/github.com\/bhuvan454\/TheSparkFoundation_Internship\/blob\/main\/breast-cancer-awareness-infographic-template-vector.jpg?raw=true><\/div>\n<br>\n<div class = 'image'> <img style=\" width:60%\" align=center src = https:\/\/github.com\/bhuvan454\/TheSparkFoundation_Internship\/blob\/main\/cancer_facts%20(1).jpg?raw=true><\/div>\n<br>\nBreast Cancer is the most common cancer and its very highly reporting skin cancer types in recent times. Above infographics give clear idea about this cancer and its impact on current world. In the world of healthcare, <strong>Breast Cancer is a current hot-buttom issue<\/strong> why u ask? our modern and lathergic lifestyle could be main reason. This type of cancer can occur in both men and women, but as per scientific investigation, <strong>Women are 2X susceptible to Breast Cancer<\/strong>  Which is why it is important to diagnose cancer in early stages.\n<br>\n<div class = 'image'> <img style=\"width:40%\" align=center src = https:\/\/media3.giphy.com\/media\/1428XrTMKTv6rC\/giphy.gif?cid=790b76110d02dc7b200e00d9dd3093e6a61956e53afe8dc7&rid=giphy.gif&ct=g><\/div>\n<br>\nAs most of us know cancer is uncontrolled growth of the cells in a given area,and if that place is breast it causes breast cancer. Based on imaging proceduce called Fine Needle Aspiration proceducre, an expert will classify the cells as malignant,or benign. But <strong>How can we diagnosis breast cancer with machine learning?<\/strong> Thats question of the hour.   \n<br>\nWith image processing techniqes or manual measurements, cell charateristics are measured from Fine Needle aspiration images, and this characteistics will be used to classifiy the cells in to Benign and Cancerous. Few more specifics are given with the datasets...\n    \n<p style = \"font-size:25px; color:#2ab7ca ;background-color:  ; text-align: left; border-radius: 5px 5px; padding: 5px\" ><strong> A Little Description About Dataset...<\/strong><\/p>\n<br>\n    \n<div class = 'image'> <img style=\"width:40%\" align=center src = https:\/\/github.com\/bhuvan454\/TheSparkFoundation_Internship\/blob\/main\/cancer.jpg?raw=true><\/div> \n\n<br>\n\nFeatures are computed from a digitized image (shown above) of a fine needle aspirate (FNA) of a breast mass. They describe characteristics of the cell nuclei present in the image. The mean, standard error and \"worst\" or largest (mean of the three largest values) of these features were computed for each image, resulting in 30 features. \n\n<strong>Attribute Information:<\/strong>\n\n- ID number\n- Diagnosis (M = malignant, B = benign)\n\n<strong>Ten real-valued features are computed for each cell nucleus:<\/strong>\n\n- radius (mean of distances from center to points on the perimeter)\n- texture (standard deviation of gray-scale values)\n- perimeter\n- area\n- smoothness (local variation in radius lengths)\n- compactness (perimeter^2 \/ area - 1.0)\n- concavity (severity of concave portions of the contour)\n- concave points (number of concave portions of the contour)\n- symmetry\n- fractal dimension (\"coastline approximation\" - 1)\n<br>\n\n","621cc931":"<br>\n<p style = \"font-size:25px; color:#2ab7ca ;background-color:  ; text-align: left; border-radius: 5px 5px; padding: 5px\" ><strong> Effect of Number of features from feature importance towards accuracy and f1 score...<\/strong><\/p>\n<br>\nThis helps in finding optimal number of features for modeling... Idea is to use base xgboost model to find the influene of features selected from permuation importance based feature importance ranking....","d928b2a0":"Till now I have explored few things, like data visualization, features, correlations, and feature engineering parts. Now finally to conclude, lets explore the ideas in modeling. In this session, I want to explore few things like, Data Scaling and approches, Crossvalidation and oof predictions, Baseline model settting and Finetunning few models with some hyperoptimization algorithm. \n<br>\n<br>\nLets get started...\n<br>\n<br>","073af7ab":"<a id = '2.6'><\/a>\n<h2 style = \"font-family: garamond; font-size: 35px; font-style: normal; background-color: #2ab7ca; color : #fed766; border-radius: 5px 5px; padding:5px; text-align:left; font-weight: bold\" >2.6 Dimentionality Reduction with UMAP<\/h2>\n\n<br>\nDimentionality reduction technique, could help understand clearly how cancerous and healthy cancer cells are seperated out...","40ccafff":"<a id = '1.1'><\/a>\n<h2 style = \"font-family: garamond; font-size: 35px; font-style: normal; background-color: #2ab7ca; color : #fed766; border-radius: 5px 5px;padding:5px ;text-align:left; font-weight: bold\" > 1.1 Loading Libraries and Utilies<\/h2>","2495a2ee":"<br>\n<p style = \"font-size:25px; color:#2ab7ca ;background-color:  ; text-align: left; border-radius: 5px 5px; padding: 5px\" ><strong> Linear Regression Comparision Based on PFI, VIF, and Original Features...<\/strong><\/p>\n<br>","a8bfad7e":"With the analysis of single category features, lets try to make things more interesting with bringing correlation into the picture. lets try to get the high positively correlated nad negatively correlated features and see how are they correlated to each other. This is crutial for understanding the collinearity of the data...","7f7b4f9c":"<a id = '3.1'><\/a>\n<h2 style = \"font-family: garamond; font-size: 35px; font-style: normal; background-color: #2ab7ca; color : #fed766; border-radius: 5px 5px; padding:5px; text-align:left; font-weight: bold\" >3.1 Outliers And Influential Points<\/h2>\n\nNot every distribution is perfectly normal, and this data is no exception. We see in our dataset, most of the Standarad error features and fractual dimensional features have significant outliers, which can be observed for the feature distribution plots.\n\nWhat are this outliers? these could be anything, like human error or scientific discovery. How to handle these outliers? hmm there are several techniques to handle outliers. Few of such are discussed here, and hoping to document these techniques for future reference. \n\n<ul>Few of outlier handling techniques are given below\n<li>Univariate Feature Outlier Handling Methods\n    <ol>\n    <li>Parametric Methods: \n        <ul><li>Standard Deviation Method - Not in Scope of this notebook<\/li> \n            <li>Inter Quartile Range Method  - Not in Scope of this notebook<\/li>\n        <\/ul>\n   <\/li>\n    <\/ol>\n    <\/li>\n<li>Multi-variate Features Outlier Handling Methods\n     <ol>\n    <li>Parametric Methods:\n        <ul><li><strong>Elliptic Envelope<\/strong><\/li>\n        <\/ul>\n     <\/li>   \n     <li>Non-Parametric Methods: \n        <ul><li><strong>DBSCAN<\/strong><\/li>\n            <li><strong>Local Outlier Factor<\/strong><\/li>\n             <li><strong>Isolation Forest<\/strong><\/li>\n        <\/ul>\n     <\/li>\n      <\/ol>\n <\/li>\n<\/ul> \n\nHere I am interested in going through all automatic or unsupervied outlier detction algorithms and to understand there perfomace, using ordinary simple regression fitting and compare the r2 score ( which is kind of a way to know the influential points and their leverage) on original and cleaned data.... though there are several things that make data influencial, this is the idea I come upwith to way I am moving with outlier detection...","28eda9c5":"<p style = \"font-size:25px; color:#2ab7ca ;background-color:  ; text-align: left; border-radius: 5px 5px; padding: 5px\" ><strong> Visualise the results of outlier detection...<\/strong><\/p>","80efe4ae":"<p style = \"font-size:30px; color:#2ab7ca ;background-color:  ; text-align: left; border-radius: 5px 5px; padding: 5px\" ><strong> Here are my other notebooks....Do checkout if you find my work Helpful....Happy Reading :)<\/strong><\/p>\n<ol>\n<li><a href =\"https:\/\/www.kaggle.com\/bhuvanchennoju\/netflix-is-awesome-why-see-here\" >Netflix is awesome.why? see here<\/a><\/li>\n<li> <a href =\"https:\/\/www.kaggle.com\/bhuvanchennoju\/data-stroytelling-auc-focus-on-strokes\" >Datastorytelling with auc focus on strokes<\/a><\/li>\n<li> <a href = \"https:\/\/www.kaggle.com\/bhuvanchennoju\/s-s-experimets-fun-with-r2-99-21\">Diamonds and Dollars: Experiments and fun r2(99.2)<\/a><\/li>\n<li> <a href = \"https:\/\/www.kaggle.com\/bhuvanchennoju\/ancient-roots-of-agriculture-a-data-overview\">Ancient roots of Agriculture: a data overview<\/a><\/li>\n<li> <a href = \"https:\/\/www.kaggle.com\/bhuvanchennoju\/hey-siri-is-it-a-or-f1-0-992\">Hey siri: cat or dog classificaiton<\/a><\/li>\n    \n<\/ol>","f0bae6ab":"<p style = \"font-size:25px; color:#2ab7ca ;background-color:  ; text-align: left; border-radius: 5px 5px; padding: 5px\" ><strong> Importing all the libraries...<\/strong><\/p>","8607b8d5":"<br>\n<p style = \"font-size:25px; color:#2ab7ca ;background-color:  ; text-align: left; border-radius: 5px 5px; padding: 5px\" ><strong> As we have already takencare of outliers, lets choose the StandardScaler standardization for data scaling....<\/strong><\/p>\n<br>\n","6f613384":"<p style = \"font-family:serif;font-size:25px; color: #fe4a49 ;background-color:  ; text-align: left; border-radius: 5px 5px; padding: 5px\" ><strong>2. Worst of Characteristics of Cancer Cells<\/strong><\/p>","febd8900":"<h3 style = \"font-family: garamond; font-size: 25px; font-style: normal; background-color: #fed766; color : #2ab7ca ; border-radius: 5px 5px; padding:5px; text-align:left; font-weight: bold\" >Worst Values of Cancer Cells<\/h3>","7e65bf4e":"<h3 style = \"font-family: garamond; font-size: 25px; font-style: normal; background-color: #fed766; color : #2ab7ca ; border-radius: 5px 5px; padding:5px; text-align:left; font-weight: bold\" >Positively Correlated CrossCategorical Features<\/h3>","1d80bedc":"<ol>\n<li><a href =\"https:\/\/www.kaggle.com\/residentmario\/variance-inflation-factors-with-nyc-building-sales\" >Variance Inflation Factors with NYC<\/a><\/li>\n<li> <a href =\"https:\/\/www.statisticshowto.com\/variance-inflation-factor\/\" >variance inflation factor<\/a><\/li>\n<li> <a href = \"https:\/\/www.kaggle.com\/bhuvanchennoju\/s-s-experimets-fun-with-r2-99-21\">Diamonds and Dollars: Experiments and fun r2(99.2)<\/a><\/li>\n<li> <a href = \"https:\/\/www.kaggle.com\/serigne\/stacked-regressions-top-4-on-leaderboard\">stacked regression<\/a><\/li>\n\n    \n<\/ol>","b3c2dd9b":"<p style = \"font-size:25px; color:#2ab7ca ;background-color:  ; text-align: left; border-radius: 5px 5px; padding: 5px\" ><strong> Missing Values and NaN values...<\/strong><\/p>","7fb8d421":"\n<br>\n<p style = \"font-size:25px; color:#2ab7ca ;background-color:  ; text-align: left; border-radius: 5px 5px; padding: 5px\" ><strong> what happened so far?<\/strong><\/p>\n<br>\n\n1. Done univariate analysis, bivariate analysis,and multivariate analysis.\n2. Outlier detection methods are discussed\n3. Multicollinearity handling techniques\n4. simple crossvalidation implementation\n5. Gridsearch Cv implementation.\n6. Stacked Classification with basemodels from gridsearch CV\n\n\n\n<br>\n<p style = \"font-size:25px; color:#2ab7ca ;background-color:  ; text-align: left; border-radius: 5px 5px; padding: 5px\" ><strong> Thanks for reading though all the way... I hope my work is helpful :)<\/strong><\/p>\n<br>\n","5803c489":"<a id = '0'><\/a>\n<h2 style = \"font-family: garamond; font-size: 40px; font-style: normal; letter-spcaing: 3px; background-color: #2ab7ca; color : #fed766; border-radius: 5px 5px; padding: 5px;text-align:center; font-weight: bold\" >Table of Contents<\/h2> \n\n* [1. Introduction](#1.0)\n    * [1.1 Libraries and Utilies](#1.1)\n \n* [2. Data Exploration And Explinatory Analysis](#2.0)\n    * [2.1 Distribution of Targets](#2.1)\n    * [2.2 Univariate Analysis of Features](#2.2)\n    * [2.3 Univariate Analysis of Features wrt Targets](#2.3)\n    * [2.4 Multivariate Analysis of Features in Same categories](#2.4)\n    * [2.5 Correlation Based Multivariate Analysis](#2.5)\n    * [2.6 Dimensionality Reduction with UMAP](#2.6)\n\n* [3. Data Cleaning Techniques and Feature Engineering](#3.0)\n    * [3.1 Outliers and Influencial Points ](#3.1)\n    * [3.2 Correlation and Multi-Collinearity](#3.2)\n    * [3.3 Feature Engineering and Feature Contribution](#3.3)\n   \n* [4. Modeling - Idea and General Strategy](#4.0)\n    * [4.1 Influence of Scaling](#4.1)\n    * [4.2 Crossvalidation and OOF predictions](#4.2)\n    * [4.3 Exploring Hyperparameter Tunning](#4.3)\n\n* [5. Summary](#5)\n\n","381b7bb1":"<a id = '5.0'><\/a>\n<h2 style = \"font-family: garamond; font-size: 40px; font-style: normal; background-color: #2ab7ca; color : #fed766; border-radius: 5px 5px; padding:5px; text-align:center; font-weight: bold\" >5. Summary<\/h2>","70a9735f":"<h3 style = \"font-family: garamond; font-size: 25px; font-style: normal; background-color: #fed766; color : #2ab7ca ; border-radius: 5px 5px; padding:5px; text-align:left; font-weight: bold\" >Mean Values of Cancer Cells<\/h3>","2b71e370":"<a id = '2.5'><\/a>\n<h2 style = \"font-family: garamond; font-size: 35px; font-style: normal; background-color: #2ab7ca; color : #fed766; border-radius: 5px 5px; padding:5px; text-align:left; font-weight: bold\" >2.5 Correlation Based Multivariate Analysis<\/h2>","a3c71988":"<a id = '3.2'><\/a>\n<h2 style = \"font-family: garamond; font-size: 35px; font-style: normal; background-color: #2ab7ca; color : #fed766; border-radius: 5px 5px; padding:5px; text-align:left; font-weight: bold\" >3.2 Correlation and Multi-Collinearity<\/h2>","4ec37dea":"<a id = '2.0'><\/a>\n<h2 style = \"font-family: garamond; font-size: 40px; font-style: normal; letter-spcaing: 3px; background-color: #2ab7ca; color : #fed766; border-radius: 5px 5px; padding:5px; text-align:center; font-weight: bold\" >2. Data Exploration and Explinatory Analysis<\/h2>","411b0f8e":"Till this point I have explored the techniques to handle muti-collinearity in the dataset. Muti-collinearity could be very problematic thing if we implement a linear model, but feature selection with permutation feature importance or PCA techniques alternative to VIF. So, lets select features based on metric roc_auc. ","621725e3":"<p style = \"font-size:25px; color:#2ab7ca ;background-color:  ; text-align: left; border-radius: 5px 5px; padding: 5px\" ><strong> Color Palette for visualizaitons....<\/strong><\/p>","19db0d5d":"<p style = \"font-size:25px; color:#2ab7ca ;background-color:  ; text-align: left; border-radius: 5px 5px; padding: 5px\" ><strong> Comparision of  total skews and kurtosis between various algorithms...<\/strong><\/p>","c9fd9e70":"<br>\n<p style = \"font-size:25px; color:#2ab7ca ;background-color:  ; text-align: left; border-radius: 5px 5px; padding: 5px\" ><strong> Confusion Matrix visualizations of total fold results......<\/strong><\/p>\n<br>\n","285c6a7b":"<h3 style = \"font-family: garamond; font-size: 25px; font-style: normal; background-color: #fed766; color : #2ab7ca ; border-radius: 5px 5px; padding:5px; text-align:left; font-weight: bold\" >Standard Error Values of Cancer Cells<\/h3>","71e63ee6":"<a id = '4.3'><\/a>\n<h2 style = \"font-family: garamond; font-size: 35px; font-style: normal; background-color: #2ab7ca; color : #fed766; border-radius: 5px 5px; padding:5px; text-align:left; font-weight: bold\" >4.3 Exploring Hyperparameter Tunning<\/h2>","d98a9f15":"<p style = \"font-size:25px; color:#2ab7ca ;background-color:  ; text-align: left; border-radius: 5px 5px; padding: 5px\" ><strong>As Overall Skewness and Kurtosis are greatly reduced and r2 score improved slightly with Isolation forest, so lets move on with this algo and explore feature level skewness and kutotsis...<\/strong><\/p>","028de2f7":"<p style = \"font-family:serif;font-size:25px; color: #fe4a49 ;background-color:  ; text-align: left; border-radius: 5px 5px; padding: 5px\" ><strong> 2.Standard Errors in Characteristics of Cancer Cells<\/strong><\/p>","29c7ed89":"<h3 style = \"font-family: garamond; font-size: 25px; font-style: normal; background-color: #fed766; color : #2ab7ca ; border-radius: 5px 5px; padding:5px; text-align:left; font-weight: bold\" >Negatively Correlated CrossCategorical Features<\/h3>","65ae0a9c":"<br>\n<p style = \"font-size:25px; color:#2ab7ca ;background-color:  ; text-align: left; border-radius: 5px 5px; padding: 5px\" ><strong> Means of out of the fold predictions........<\/strong><\/p>\n<br>\n","5dd1e382":"<br>\n<p style = \"font-size:25px; color:#2ab7ca ;background-color:  ; text-align: left; border-radius: 5px 5px; padding: 5px\" ><strong> Out of the folds results......<\/strong><\/p>\n<br>\n","274e1a62":" <a id = '3.3'><\/a>\n<h2 style = \"font-family: garamond; font-size: 35px; font-style: normal; background-color: #2ab7ca; color : #fed766; border-radius: 5px 5px; padding:5px; text-align:left; font-weight: bold\" >3.3 Feature Engineering and Feature Contribution<\/h2>","8cd040c8":"<a id = '4.2'><\/a>\n<h2 style = \"font-family: garamond; font-size: 35px; font-style: normal; background-color: #2ab7ca; color : #fed766; border-radius: 5px 5px; padding:5px; text-align:left; font-weight: bold\" >4.2 Cross-Validation and OOF Predictions<\/h2>","25319670":"All The ground work is done for the data exploration!! Right now idea is to space out features based on categories, like mean, standard error, and worst which are various angled exrapolation of the real values. For more idea on this categorization, refer dataset breif....\n<br>\n<br>\nTo understand the hidden patterns, <strong>its always good to see big picture first and then dive deep into data<\/strong>. So, here also, lets begin analysis or visualization of patterns with out target and then move to the feature level understnading....\n<br>\n<br>\nMy strategy for this analysis as follows\n<ul>\n    <li>Target Distribution<\/li>\n    <li>Univariate Analysis<\/li>\n    <li>Binary Feature Analysis<\/li>\n    <li>Multivariate Analysis<\/li>\n    <li>Class Segregation with Dimensionality Reduction <\/li>\n<\/ul>\n","377ad4da":"Why scaling is important? Because model expect a perfet normal distribution for modeling, and most influenical by the outliers in the data. \n<br>\n<br>\nSo, how can we make our data outlier proof and good convergence proof? answer is data scaling. \n<br>\n<br>\nOkay, everything have some mathmatics involved in machine learning,a data scaling in not exception. There are three topics need to know for this scaling, 1) Rescaling, 2) Normalizing 3) Standardizing. Not going to deep... if interested read this [article](https:\/\/towardsai.net\/p\/data-science\/how-when-and-why-should-you-normalize-standardize-rescale-your-data-3f083def38ff#:~:text=Normalization%3A,when%20features%20have%20different%20ranges.).\n\n<br>\nI  have attempted to understanding the scaling approches...\n<ol> \n    <li>Standardization - this is a way to convert a distribution to normal distribution using z score. This can be done with <strong>StandardScaler<\/strong> in here.<\/li>\n    <li>Noramalization - this is a way to bring every value into the range of 0 and 1. This can be done with min, max substraciton from a value, for clear idea, read above article. This can be done with <strong>MinMaxScaler<\/strong> in here.<\/li>\n    <li>Interquantile Scaling: This is a oulier exlimaination kind of scaling, here we do scaling ith IQR of the data. So, the outleirs will be removed from the data. This can be done with <strong>RobustScaler<\/strong><\/li>\n<\/ol>\n\n<strong>As I have done outlier remvoal, feature selction already mostly distributions will have some effect by differ by ranges.<\/strong>\n\n<br>\n<br>\nlets visualize the results...","bf79bf1e":"<p style = \"font-size:25px; color:#2ab7ca ;background-color:  ; text-align: left; border-radius: 5px 5px; padding: 5px\" ><strong>  Ouliers Detection and Helper Functions....<\/strong><\/p>","35ae530b":"<a id = '3.0'><\/a>\n<h2 style = \"font-family: garamond; font-size: 40px; font-style: normal;background-color: #2ab7ca; color : #fed766; border-radius: 5px 5px;padding:5px;text-align:center; font-weight: bold\" >3. Data Cleaning Techniques And Feature Engineering<\/h2>\n<br>\n<br>\nData cleaning and Feature engineering are the part and parcel of machine learnging pipeline. So with understanding of data  on feature level, we know about the existance of few outliers in data, and multi-collinearity in the data.\n<br>\n<br>\nSo what can we do? lets take a general approch rather specific to this dataset,<strong> lets try to explore the various outlier detection alorithms, and ways to handle multi-collinearity in the data and finally few feature engineering ideas maybe....<\/strong>\n\n<br>\n<br>\nLets get started ...","4aed45c1":"<a id = '4.1'><\/a>\n<h2 style = \"font-family: garamond; font-size: 35px; font-style: normal; background-color: #2ab7ca; color : #fed766; border-radius: 5px 5px; padding:5px; text-align:left; font-weight: bold\" >4.1 Influence of Scaling<\/h2>","b23f2973":"<a id = '2.2'><\/a>\n<h2 style = \"font-family: garamond; font-size: 35px; font-style: normal; background-color: #2ab7ca; color : #fed766; border-radius: 5px 5px; padding:5px; text-align:left; font-weight: bold\" >2.2 Univariate Analysis of Features<\/h2>\n<br>\nFrom Traget distribution waffle, we can clearly see that nealy 40% of the diagnostic reports are positive for cancer. So every individual feature, will have the same distribution, but how are they distributed individualy? are they noramlly distributed or skewed. This is quite important for better modeling as ml algos, expect data to be noramally ditributed.  So, lets bring the <strong>sperating perameter of skewness > 1 <\/strong> to seperate out features.\n\n<br>\n<br>\nFrom the below distributions, it is clear that most of the features are skewed and may be <strong>if they are influential outliers, need to operated on them<\/strong> before modeling....","b7326b06":"<a id = '6.0'><\/a>\n<h2 style = \"font-family: garamond; font-size: 40px; font-style: normal; background-color: #2ab7ca; color : #fed766; border-radius: 5px 5px; padding:5px; text-align:center; font-weight: bold\" >5. References<\/h2>","5d0847dd":"<p style = \"font-family:serif;font-size:25px; color: #fe4a49 ;background-color:  ; text-align: left; border-radius: 5px 5px; padding: 5px\" ><strong>1. Worst of Measurements of Cancer Cells<\/strong><\/p>","c7fe9da0":"By now outliers are removed, and not null values exits in our data.. Now lets address the collinearity in this section. collineraity could be problematic in a regression based models so keep accuracy and roc_auc_scores aside try to play round with how to tackle his multicollinearity and feature selection....","9775d60f":"<a id = '2.4'><\/a>\n<h2 style = \"font-family: garamond; font-size: 35px; font-style: normal; background-color: #2ab7ca; color : #fed766; border-radius: 5px 5px; padding:5px; text-align:left; font-weight: bold\" >2.4 Multivariate Analysis of Features In Same Category<\/h2>","93342c06":"<p style = \"font-family:serif;font-size:25px; color: #fe4a49 ;background-color:  ; text-align: left; border-radius: 5px 5px; padding: 5px\" ><strong> 1.Mean Measurements of Cancer Cells<\/strong><\/p>","c306f73f":"<p style = \"font-family:serif;font-size:25px; color: #fe4a49 ;background-color:  ; text-align: left; border-radius: 5px 5px; padding: 5px\" ><strong> 2.Mean Charateristics of Cancer Cells<\/strong><\/p>","8552289f":"<p style = \"font-size:25px; color:#2ab7ca ;background-color:  ; text-align: left; border-radius: 5px 5px; padding: 5px\" ><strong> Null Value Accuracy - Baseline Metric....<\/strong><\/p>","ec256358":"As per data inforgraphic it is clear that, 1 out of 8 women are having breast cancer at some point of time. In 1960s this number used to be 1 out of 11 women, as a lethagic lifestyle could be causation of breast cancer. So, lets see what our data is telling us about woment susceptable to cancer!! \n\n<strong> lets see the distribution of diagnosis of data..<\/strong>","d69a0815":"<p style = \"font-family:serif;font-size:25px; color: #fe4a49 ;background-color:  ; text-align: left; border-radius: 5px 5px; padding: 5px\" ><strong> 1.Standard Errors in Measurements of Cancer Cells<\/strong><\/p>","75aa8c1c":"<p style = \"font-size:25px; color:#2ab7ca ;background-color:  ; text-align: left; border-radius: 5px 5px; padding: 5px\" ><strong> Descriptive Statistics of The Data....<\/strong><\/p>","9801f720":"<a id = '2.3'><\/a>\n<h2 style = \"font-family: garamond; font-size: 35px; font-style: normal; background-color: #2ab7ca; color : #fed766; border-radius: 5px 5px; padding:5px; text-align:left; font-weight: bold\" >2.3 Univariate Analysis of Features wrt Targets<\/h2>\n\n<br>\nWith understanding the distirbution of individual features, it is important to see how each features target level distribution. From the Below visualization is clear that though few features are skewdd towards left tail, correspoinding values of cancerous cells are normally distributed. Okay we can takecare about those extreme points with outlier removal methods and scaling with normalization techniques.","f8a2396a":"<a id = '4.0'><\/a>\n<h2 style = \"font-family: garamond; font-size: 40px; font-style: normal; background-color: #2ab7ca; color : #fed766; border-radius: 5px 5px; padding:5px; text-align:center; font-weight: bold\" >4. Modeling - Idea and Strategy<\/h2>","20d06391":"<p style = \"font-size:25px; color:#2ab7ca ;background-color:  ; text-align: left; border-radius: 5px 5px; padding: 5px\" ><strong> Loading Dataset...<\/strong><\/p>"}}