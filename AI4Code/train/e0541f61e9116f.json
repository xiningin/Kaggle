{"cell_type":{"fe2899ad":"code","3f807348":"code","c9f64ca7":"code","79e0eb8e":"code","007a0df8":"code","3ce76c84":"code","5c4cc926":"code","86e025c5":"code","94665e04":"code","90c4e5dd":"code","f6e041f5":"code","37ed2e84":"code","5a82e665":"code","640b1d01":"code","b75f6d39":"code","28aa2a56":"code","09e9b014":"code","8709f47e":"code","c03283d1":"code","d9b93d2a":"code","282cf847":"code","3ce4c620":"code","002fa947":"code","33fc1d88":"code","acdf2592":"code","86751a8d":"markdown","196e62f5":"markdown","66a9fc19":"markdown","f07b72b5":"markdown","35c3242e":"markdown","0297192c":"markdown","9a2a2b5d":"markdown","2613bb58":"markdown","e8006395":"markdown","28179346":"markdown","9453642e":"markdown"},"source":{"fe2899ad":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom random import shuffle\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport cv2\nimport os \n\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D\nfrom sklearn.model_selection import train_test_split\nimport tensorflow as tf\nfrom sklearn.metrics import classification_report\nimport xgboost as xgb\nfrom sklearn.decomposition import PCA\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n\n# unhide to get the list of files:\n# for dirname, _, filenames in os.walk('\/kaggle\/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","3f807348":"# creating all the constants\nDIRECTORY = '\/kaggle\/input\/microsoft-catsvsdogs-dataset\/PetImages\/'\nCATAGORY = ['Cat', 'Dog']\nIMG_SIZE = 60","c9f64ca7":"\narr = cv2.imread(os.path.join(DIRECTORY,CATAGORY[0],'11590.jpg'), cv2.IMREAD_GRAYSCALE)\nplt.subplot(1,2,1)\nplt.imshow(arr, cmap='gray')\nplt.title('Original')\nplt.xlabel('shape: {}'.format(arr.shape))\nplt.subplot(1,2,2)\n\narr = cv2.resize(arr, (IMG_SIZE,IMG_SIZE))\nplt.imshow(arr, cmap='gray')\nplt.title('Reduced')\nplt.xlabel('shape: {}'.format(arr.shape))","79e0eb8e":"training_data=[]\nclass_list =[]\n\n# there are some broken imgs(corrupted)\n# we will over look them as we encounter them through the try except block\ndef create_training_data():\n    for category in CATAGORY:\n        path = os.path.join(DIRECTORY,category)\n        class_num = CATAGORY.index(category)\n        for img in os.listdir(path):\n            try:\n                img_array = cv2.imread(os.path.join(path,img), cv2.IMREAD_GRAYSCALE)\n                img_array = cv2.resize(img_array, (IMG_SIZE,IMG_SIZE))\n                training_data.append([img_array, class_num])\n                class_list.append(class_num)\n            except Exception as e:\n                pass\n            \ncreate_training_data()","007a0df8":"# check for the total number of images\nprint(len(training_data))","3ce76c84":"# There seems to be no class imbalance\nsns.countplot(pd.Series(class_list).value_counts())","5c4cc926":"# shuffling the data for better results\nshuffle(training_data)","86e025c5":"# check if sample worked\nfor sample in training_data[:10]:\n    print(sample[1])","94665e04":"X = []\ny = []\n\nfor features, labels in training_data:\n    X.append(features)\n    y.append(labels)\n\n    # (number of images, height, width, num of channels)\nX = np.array(X).reshape(-1, IMG_SIZE, IMG_SIZE, 1)\ny = np.array(y)\n\n# normalising the data\nX = X\/255","90c4e5dd":"# train test split\nX_train, X_test, y_train, y_test = train_test_split(X, y,\n                                                    stratify=y, \n                                                    test_size=0.25)","f6e041f5":"model = Sequential()\n\nmodel.add(Conv2D(128, (3, 3), input_shape=X_train.shape[1:]))\nmodel.add(Activation('relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\n\nmodel.add(Conv2D(64, (3, 3)))\nmodel.add(Activation('relu'))\nmodel.add(Conv2D(32, (3, 3)))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\n\nmodel.add(Flatten())  # this converts our 3D feature maps to 1D feature vectors\nmodel.add(Dense(64))\n\nmodel.add(Dense(1))\nmodel.add(Activation('sigmoid'))\n\nmodel.compile(loss='binary_crossentropy',\n              optimizer='adam',\n              metrics=['accuracy'])\n\nmodel.fit(X_train, y_train, batch_size=8, epochs=10, validation_split=0.3, \n          use_multiprocessing=True, workers = 20, class_weight = {0: 3.0,1: 0.5})","37ed2e84":"# model predictions\nmodel.evaluate(X_test,y_test)","5a82e665":"# save model weights:\nmodel.save_weights('.\/my_weights')","640b1d01":"# load the weights to just the feature extraction section of CNN\nmodel = Sequential()\n\nmodel.add(Conv2D(128, (3, 3), input_shape=X_train.shape[1:]))\nmodel.add(Activation('relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\n\nmodel.add(Conv2D(64, (3, 3)))\nmodel.add(Activation('relu'))\nmodel.add(Conv2D(32, (3, 3)))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\n\n# model.add(Flatten())\n# model.compile(loss='binary_crossentropy',\n#               optimizer='adam',\n#               metrics=['accuracy'])\n\nmodel.load_weights('.\/my_weights')","b75f6d39":"# test it on an image to see if it's able to extract the features.\nplt.subplot(1,2,1)\nplt.imshow(X_test[1])\n\nfeatures = model.predict(X_test[1].reshape(-1,60,60,1))\nfeatures = features.reshape(4608,)\n\nplt.subplot(1,2,2)\nplt.imshow(features.reshape(64,72))\nprint('Shape of the array of features extracted: {}'.format(features.shape))","28aa2a56":"# extract features for all images\nx_train_features = model.predict(X_train.reshape(-1,60,60,1))\nx_test_features = model.predict(X_test.reshape(-1,60,60,1))\nx_train_features.shape, x_test_features.shape","09e9b014":"x_train_features = x_train_features.reshape(x_train_features.shape[0],-1)\nx_test_features = x_test_features.reshape(x_test_features.shape[0],-1)\nx_train_features.shape, x_test_features.shape","8709f47e":"# XGB to classify.\nmodel = xgb.XGBClassifier()\nmodel.fit(x_train_features, y_train)","c03283d1":"# Check the accuracy\ny_pred = model.predict(x_test_features)\nprint(classification_report(y_test, y_pred))","d9b93d2a":"# use PCA to reduce the number of features and then use XBG.\n# further reducing the number of features to features 2304\npca = PCA(n_components=2304)\n\nx_train_pca = pca.fit_transform(x_train_features)\nx_test_pca = pca.fit_transform(x_test_features)\n\nmodel.fit(x_train_pca, y_train)\ny_pred = model.predict(x_test_pca)\nprint(classification_report(y_test,y_pred))","282cf847":"# quarter the number of features : 1152\npca = PCA(n_components=1152)\n\nx_train_pca = pca.fit_transform(x_train_features)\nx_test_pca = pca.fit_transform(x_test_features)\n\nmodel.fit(x_train_pca, y_train)\ny_pred = model.predict(x_test_pca)\nprint(classification_report(y_test,y_pred))","3ce4c620":"# further reducing the number of features to features 576\npca = PCA(n_components=576)\n\nx_train_pca = pca.fit_transform(x_train_features)\nx_test_pca = pca.fit_transform(x_test_features)\n\nmodel.fit(x_train_pca, y_train)\ny_pred = model.predict(x_test_pca)\nprint(classification_report(y_test,y_pred))","002fa947":"# further reducing the number of features to features 128\npca = PCA(n_components=128)\n\nx_train_pca = pca.fit_transform(x_train_features)\nx_test_pca = pca.fit_transform(x_test_features)\n\nmodel.fit(x_train_pca, y_train)\ny_pred = model.predict(x_test_pca)\nprint(classification_report(y_test,y_pred))","33fc1d88":"# further reducing the number of features to features 64\npca = PCA(n_components=64)\n\nx_train_pca = pca.fit_transform(x_train_features)\nx_test_pca = pca.fit_transform(x_test_features)\n\nmodel.fit(x_train_pca, y_train)\ny_pred = model.predict(x_test_pca)\nprint(classification_report(y_test,y_pred))","acdf2592":"# further reducing the number of features to features 32\npca = PCA(n_components=32)\n\nx_train_pca = pca.fit_transform(x_train_features)\nx_test_pca = pca.fit_transform(x_test_features)\n\nmodel.fit(x_train_pca, y_train)\ny_pred = model.predict(x_test_pca)\nprint(classification_report(y_test,y_pred))","86751a8d":"# 5. Train Test Split","196e62f5":"# 4. Creating the Dataset","66a9fc19":"# 3. Finalising on the image size","f07b72b5":"# 7. Save and Load model weights","35c3242e":"# 2. Create the necessary constant variables:","0297192c":"### Base Accuracy:","9a2a2b5d":"# 9. Using PCA to reduce the number of dimensions to optimise the process:","2613bb58":"# Conclusion:\n- The CNN model gives us 70% accuracy using all 4608 features.\n- The CNN + XGB gives us 73% accuracy using all 4608 features.\n- The CNN + XGB gives us 65% accuracy using 2304 featues only.\n- The CNN + XGB gives us 64% accuracy using 1152 featues only.\n- The CNN + XGB gives us 63% accuracy using 576 featues only.\n- The CNN + XGB gives us 63% accuracy using 128 featues only.\n- The CNN + XGB gives us 64% accuracy using 32 featues only.\n\n> Consider the 2nd Test for an example where the model performs better by using XGB to classify the images using the same features that the CNN model extracts. \n\n>Considering the last test were we use just 32 featuers to classify the images and we get an accuracy of 64%. Comparing that to your base accuracy of CNN [70%] and CNN + XGB [73%], it performs considering it just uses 32 features. \n\n>The main objective of this code was to show how using a combination of models (stacking) we can get somewhat better accuracy at a lower hardware cost. ","e8006395":"# 1. Load the important Libraries:","28179346":"# 8. Using XGBoost to classify:","9453642e":"# 6. CNN model development for feature extration"}}