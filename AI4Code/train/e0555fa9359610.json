{"cell_type":{"0b6ce570":"code","308e1e3d":"code","beb5efaa":"code","2de7b634":"code","d0eca929":"code","917ac4b3":"code","fb702ca2":"code","6b968099":"code","7236d01c":"code","00c988d0":"code","3b047d75":"code","0ab81c41":"code","46024c6f":"code","94f41d2b":"code","59579b5b":"code","b7b49fe8":"code","85e139c0":"code","8b31966e":"code","9cfba0e8":"code","b99b9440":"code","a8ef0a89":"code","9795ad03":"code","b68a8629":"code","fd03b94f":"code","3721ec06":"code","5d0ff417":"code","101214f6":"code","2cf84245":"code","828d4ecc":"code","867ef9be":"code","e3ff02a3":"code","cd3e92c1":"code","030b2cac":"code","09147042":"code","7e310a12":"code","ba0f04fd":"code","24cbcd95":"code","0b96478f":"code","06881ae3":"code","28f5f10b":"code","faebbcbb":"code","6fd03123":"code","546fca32":"code","f6b51d8c":"code","ccf6191e":"code","3ce42ccb":"code","0146da22":"code","3211b19a":"code","df371639":"code","7f18a639":"code","70e8587d":"markdown","5d796482":"markdown","39f729c3":"markdown","5fa9f04a":"markdown","56568a0b":"markdown","f8c869cb":"markdown","66a67e12":"markdown","faff16d2":"markdown","d3faf1ed":"markdown","fc8d4270":"markdown","bb1b3d0f":"markdown","584ffa9f":"markdown","b14663ab":"markdown","edf91343":"markdown","68e363f0":"markdown","42c861c8":"markdown","aee79337":"markdown","712a8739":"markdown","01da49d2":"markdown","731c8912":"markdown","015888a8":"markdown","743dc9ac":"markdown","558d4bb3":"markdown","57120c3a":"markdown","28378bde":"markdown","8c1a7f25":"markdown","1b08fa51":"markdown","73dd3dde":"markdown","5a989bec":"markdown","023fa1f7":"markdown","af98d295":"markdown","a8deb159":"markdown","b3e5efad":"markdown","f7406f10":"markdown","1f48536e":"markdown"},"source":{"0b6ce570":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score\nfrom datetime import timedelta\nfrom pandas import ExcelWriter","308e1e3d":"!pip install openpyxl","beb5efaa":"df = pd.read_excel(\"..\/input\/retail-customer-data-for-segmentation\/Online Retail.xlsx\")\ndf.head()","2de7b634":"# Check shape of data\ndf.shape","d0eca929":"# Check feature details of data\ndf.info()","917ac4b3":"# Check missing values in data\ndf.isnull().sum()","fb702ca2":"# Calculating the Missing Values % contribution in DF\ndf_null = round(df.isnull().sum()\/len(df)*100,2)\ndf_null","6b968099":"invoice_null_custid = set(df[df['CustomerID'].isnull()]['InvoiceNo'])\ndf[df['InvoiceNo'].isin(invoice_null_custid) & (~df['CustomerID'].isnull())]","7236d01c":"df = df.drop('Description', axis=1)\ndf = df.dropna()\ndf.shape","00c988d0":"df = df.drop_duplicates()\ndf.shape","3b047d75":"# CustomerID is 'float64', changing the datatype of CustomerId to string as Customer ID as numerical data does not make sense\n\ndf['CustomerID'] = df['CustomerID'].astype(str)","0ab81c41":"df.describe(datetime_is_numeric=True)","46024c6f":"df.describe(include=['O'])","94f41d2b":"# Convert to InvoiceDate to Year-Month format\ndf['month_year'] = df['InvoiceDate'].dt.to_period('M')\ndf['month_year'].nunique()","59579b5b":"month_cohort = df.groupby('month_year')['CustomerID'].nunique()\nmonth_cohort","b7b49fe8":"plt.figure(figsize=(10,5))\nsns.barplot(y = month_cohort.index, x = month_cohort.values);\nplt.xlabel(\"Count of customers\")\nplt.title(\"No. of active customers in each month\")","85e139c0":"month_cohort - month_cohort.shift(1)","8b31966e":"retention_rate = round(month_cohort.pct_change(periods=1)*100,2)\nretention_rate","9cfba0e8":"plt.figure(figsize=(10,5))\nsns.barplot(y = retention_rate.index, x = retention_rate.values);\nplt.xlabel(\"Retention (in %)\")\nplt.title(\"Month-wise customer retention rate\");","b99b9440":"df['amount'] = df['Quantity']*df['UnitPrice']\ndf.head()","a8ef0a89":"df_monetary = df.groupby('CustomerID').sum()['amount'].reset_index()\ndf_monetary","9795ad03":"df_frequency = df.groupby('CustomerID').nunique()['InvoiceNo'].reset_index()\n# df_freqency = df.drop_duplicates('InvoiceNo').groupby('CustomerID').count()['InvoiceNo'].reset_index()\ndf_frequency","b68a8629":"# We will fix reference date for calculating recency as last transaction day in data + 1 day\nref_day = max(df['InvoiceDate']) + timedelta(days=1)\ndf['days_to_last_order'] = (ref_day - df['InvoiceDate']).dt.days\ndf.head()","fd03b94f":"df_recency = df.groupby('CustomerID')['days_to_last_order'].min().reset_index()\ndf_recency","3721ec06":"df_rf = pd.merge(df_recency, df_frequency,  on='CustomerID', how='inner')\ndf_rfm = pd.merge(df_rf, df_monetary, on='CustomerID', how='inner')\ndf_rfm.columns = ['CustomerID', 'Recency', 'Frequency', 'Monetary']\ndf_rfm.head()","5d0ff417":"df_rfm['recency_labels'] = pd.cut(df_rfm['Recency'], bins=5,\n                                     labels=['newest', 'newer', 'medium', 'older', 'oldest'])\ndf_rfm['recency_labels'].value_counts().plot(kind='barh');\ndf_rfm['recency_labels'].value_counts()","101214f6":"df_rfm['frequency_labels'] = pd.cut(df_rfm['Frequency'], bins=5, labels=['lowest', 'lower', 'medium', 'higher', 'highest'])\ndf_rfm['frequency_labels'].value_counts().plot(kind='barh');\ndf_rfm['frequency_labels'].value_counts()","2cf84245":"df_rfm['monetary_labels'] = pd.cut(df_rfm['Monetary'], bins=5, labels=['smallest', 'smaller', 'medium', 'larger', 'largest'])\ndf_rfm['monetary_labels'].value_counts().plot(kind='barh');\ndf_rfm['monetary_labels'].value_counts()","828d4ecc":"df_rfm['rfm_segment'] = df_rfm[['recency_labels','frequency_labels','monetary_labels']].agg('-'.join, axis=1)\ndf_rfm.head()","867ef9be":"recency_dict = {'newest': 5, 'newer':4, 'medium': 3, 'older':2, 'oldest':1}\nfrequency_dict = {'lowest':1, 'lower':2, 'medium': 3, 'higher':4, 'highest':5}\nmonetary_dict = {'smallest':1, 'smaller':2, 'medium': 3, 'larger':4, 'largest':5}\n\ndf_rfm['rfm_score'] = df_rfm['recency_labels'].map(recency_dict).astype(int)+ df_rfm['frequency_labels'].map(frequency_dict).astype(int) + df_rfm['monetary_labels'].map(monetary_dict).astype(int)\ndf_rfm.head(10)","e3ff02a3":"df_rfm['rfm_segment'].value_counts().plot(kind='barh', figsize=(10, 5));","cd3e92c1":"df_rfm['rfm_score'].value_counts().plot(kind='barh', figsize=(10, 5));","030b2cac":"print(df_rfm.shape)\ndf_rfm.head()","09147042":"plt.figure(figsize=(12,6))\n\nfor i, feature in enumerate(['Recency', 'Frequency', 'Monetary']):\n    plt.subplot(2,3,i+1)\n    df_rfm[feature].plot(kind='box')\n    plt.subplot(2,3,i+1+3)\n    df_rfm[feature].plot(kind='hist')","7e310a12":"df_rfm = df_rfm[(df_rfm['Frequency']<60) & (df_rfm['Monetary']<40000)]\ndf_rfm.shape","ba0f04fd":"plt.figure(figsize=(12,6))\n\nfor i, feature in enumerate(['Recency', 'Frequency', 'Monetary']):\n    plt.subplot(2,3,i+1)\n    df_rfm[feature].plot(kind='box')\n    plt.subplot(2,3,i+1+3)\n    df_rfm[feature].plot(kind='hist')","24cbcd95":"df_rfm_log_trans = pd.DataFrame()\ndf_rfm_log_trans['Recency'] = np.log(df_rfm['Recency'])\ndf_rfm_log_trans['Frequency'] = np.log(df_rfm['Frequency'])\ndf_rfm_log_trans['Monetary'] = np.log(df_rfm['Monetary']-df_rfm['Monetary'].min()+1)","0b96478f":"scaler = StandardScaler()\n\ndf_rfm_scaled = scaler.fit_transform(df_rfm_log_trans[['Recency', 'Frequency', 'Monetary']])\ndf_rfm_scaled\n\ndf_rfm_scaled = pd.DataFrame(df_rfm_scaled)\ndf_rfm_scaled.columns = ['Recency', 'Frequency', 'Monetary']\ndf_rfm_scaled.head()","06881ae3":"# k-means with some arbitrary k\nkmeans = KMeans(n_clusters=3, max_iter=50)\nkmeans.fit(df_rfm_scaled)","28f5f10b":"kmeans.labels_","faebbcbb":"# Finding the Optimal Number of Clusters with the help of Elbow Curve\/ SSD\nssd = []\nrange_n_clusters = [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\nfor num_clusters in range_n_clusters:\n    kmeans = KMeans(n_clusters=num_clusters, max_iter=100)\n    kmeans.fit(df_rfm_scaled)\n    \n    ssd.append(kmeans.inertia_)\n    \n# plot the SSDs for each n_clusters\nplt.plot(range_n_clusters,ssd);","6fd03123":"# Creating dataframe for exporting to create visualization in tableau later\ndf_inertia = pd.DataFrame(list(zip(range_n_clusters, ssd)), columns=['clusters', 'intertia'])\ndf_inertia","546fca32":"# Finding the Optimal Number of Clusters with the help of Silhouette Analysis\nrange_n_clusters = [2, 3, 4, 5, 6, 7, 8, 9, 10]\n\nfor num_clusters in range_n_clusters:    \n    kmeans = KMeans(n_clusters=num_clusters, max_iter=50)\n    kmeans.fit(df_rfm_scaled)\n    \n    cluster_labels = kmeans.labels_\n    \n    silhouette_avg = silhouette_score(df_rfm_scaled, cluster_labels)\n    print(\"For n_clusters={0}, the silhouette score is {1}\".format(num_clusters, silhouette_avg))","f6b51d8c":"# Final model with k=3\nkmeans = KMeans(n_clusters=3, max_iter=50)\nkmeans.fit(df_rfm_scaled)","ccf6191e":"# assign the label\ndf_rfm['Cluster_Id'] = kmeans.labels_\ndf_rfm.head()","3ce42ccb":"# Box plot to visualize Cluster Id vs Monetary\nsns.boxplot(x='Cluster_Id', y='Monetary', data=df_rfm);","0146da22":"# Box plot to visualize Cluster Id vs Frequency\nsns.boxplot(x='Cluster_Id', y='Frequency', data=df_rfm);","3211b19a":"# Box plot to visualize Cluster Id vs Recency\nsns.boxplot(x='Cluster_Id', y='Recency', data=df_rfm);","df371639":"# Writing dataframe to excel file for creating visualization in tableau\nwriter = pd.ExcelWriter('output_data.xlsx', engine='xlsxwriter')\n\ndf.to_excel(writer, sheet_name='master_data', index=False)\ndf_rfm.to_excel(writer, sheet_name='rfm_data', index=False)\ndf_inertia.to_excel(writer, sheet_name='inertia', index=False)\nwriter.save()","7f18a639":"product_desc = pd.read_excel(\"Online Retail.xlsx\")\nproduct_desc = product_desc[['StockCode', 'Description']]\nproduct_desc = product_desc.drop_duplicates()\nproduct_desc.to_csv('product_desc.csv', index=False)","70e8587d":"26 Customers removed as outlier from out data.","5d796482":"* **InvoiceNo:** Total entries in preprocessed data are 4,01,602 but transactions are 22,190. Most number of entries (count of unique products) are in Invoice No. '576339' and is 542 nos.\n* **StockCode:** There are total 3684 unique products in our data and product with stock code '85123A' appears most frequently (2065 times) in our data.\n* **CustomerID:** There are 4372 unique customers in our final preprocessed data. Customer with ID '17841' appears most frequently in data (7812 times)\n* **Country:** Company has customers across 37 countries. Most entries are from United Kingdom in our dataset (356726)","39f729c3":"**>>>>>--------------------------------------------------------------------------------------------------------------------------------------->>>>>**\n# SOLUTION:\n## Week 1:\n### (A) Data Cleaning\n**(1) Reading Data and Preliminary Data Inspection**","5fa9f04a":"### Project Task: Week 4\n**Data Reporting:**\n\n1. Create a dashboard in tableau by choosing appropriate chart types and metrics useful for the business. The dashboard must entail the following:\n\n    a. Country-wise analysis to demonstrate average spend. Use a bar chart to show the monthly figures\n\n    b. Bar graph of top 15 products which are mostly ordered by the users to show the number of products sold\n\n    c. Bar graph to show the count of orders vs. hours throughout the day\n\n    d. Plot the distribution of RFM values using histogram and frequency charts\n\n    e. Plot error (cost) vs. number of clusters selected\n\n    f. Visualize to compare the RFM values of the clusters using heatmap","56568a0b":"**Calculate RFM metrics:**","f8c869cb":"## *Please refer Dashboard created in Tableau for visualization and graphs*","66a67e12":"### Project Task: Week 1:\n**Data Cleaning:**\n\n1. Perform a preliminary data inspection and data cleaning.\n\n    a. Check for missing data and formulate an apt strategy to treat them.\n\n    b. Remove duplicate data records.\n\n    c. Perform descriptive analytics on the given data.\n\n**Data Transformation:**\n\n2. Perform cohort analysis (a cohort is a group of subjects that share a defining characteristic). Observe how a cohort behaves across time and compare it to other cohorts.\n\n    a. Create month cohorts and analyze active customers for each cohort.\n\n    b. Analyze the retention rate of customers.","faff16d2":"**Build RFM Segments:**","d3faf1ed":"**RFM Score:**","fc8d4270":"**Outliers:** Frequency and Monetary features in above data seem to have lot of outliers. Lets drop them.","bb1b3d0f":"**Monetary analysis:**","584ffa9f":"## Capstone Project: Retail - PGP","b14663ab":"* **(b) Remove duplicate data records:** Since our data is transactional data and it has duplicate entries for InvoiceNo and CustomerID, we will drop only those rows which are completely duplicated, not on the basis of any one particular column such as InvoiceNo or CustomerID etc.","edf91343":"* **Quantity:** Average quantity of each product in transaction is 12.18. Also note that minimum value in **Quantity** column is negative. This implies that some customers had returned the product during our period of analysis.\n* **InvoiceDate:** Our data has transaction between 01-12-2010 to 09-12-2011\n* **UnitPrice:** Average price of each product in transactions is 3.47","68e363f0":"* **(a) Missing values treatment:**","42c861c8":"### Project Task: Week 3\n**Data Modeling :**\n\n1. Create clusters using k-means clustering algorithm.\n\n    a. Prepare the data for the algorithm. If the data is asymmetrically distributed, manage the skewness with appropriate transformation. Standardize the data.\n\n    b. Decide the optimum number of clusters to be formed.\n\n    c. Analyze these clusters and comment on the results.","aee79337":"We can select optimum number of clusters as 3 in our final model","712a8739":"## Week 4: \n**Data Reporting:**\n1. Create a dashboard in tableau by choosing appropriate chart types and metrics useful for the business. The dashboard must entail the following:\n\n        a. Country-wise analysis to demonstrate average spend. Use a bar chart to show the monthly figures\n        b. Bar graph of top 15 products which are mostly ordered by the users to show the number of products sold\n        c. Bar graph to show the count of orders vs. hours throughout the day\n        d. Plot the distribution of RFM values using histogram and frequency charts\n        e. Plot error (cost) vs. number of clusters selected\n        f. Visualize to compare the RFM values of the clusters using heatmap","01da49d2":"We could not find any value to impute null values in **CustomerID** column since all entries for a particular **InvoiceNo** have missing **CustomerID** if that particular **InvoiceNo** has null **CustomerID** in even one entry. So we will drop all rows having null values in **CustomerID**.","731c8912":"**Analyze RFM Segment and Score:**","015888a8":"### Project Task: Week 2\n**Data Modeling :**\n\n1. Build a RFM (Recency Frequency Monetary) model. Recency means the number of days since a customer made the last purchase. Frequency is the number of purchase in a given period. It could be 3 months, 6 months or 1 year. Monetary is the total amount of money a customer spent in that given period. Therefore, big spenders will be differentiated among other customers such as MVP (Minimum Viable Product) or VIP.\n\n2. Calculate RFM metrics.\n\n3. Build RFM Segments. Give recency, frequency, and monetary scores individually by dividing them into quartiles.\n\n    b1. Combine three ratings to get a RFM segment (as strings).\n\n    b2. Get the RFM score by adding up the three ratings.\n\n    b3. Analyze the RFM segments by summarizing them and comment on the findings.\n\n**Note:** Rate \u201crecency\" for customer who has been active more recently higher than the less recent customer, because each company wants its customers to be recent.\n\n**Note:** Rate \u201cfrequency\" and \u201cmonetary\" higher, because the company wants the customer to visit more often and spend more money","743dc9ac":"## Week 2:","558d4bb3":"**c. Analyze these clusters and comment on the results.**","57120c3a":"**Log Transformation:** Now since all three features have right skewed data therefore we will use log transformation of these features in our model.","28378bde":"   **b. Build K-Means Clustering Model and Decide the optimum number of clusters to be formed.**","8c1a7f25":"### Inference:\n\nAs we can observe from above boxplots that our model has nicely created 3 segements of customer with the interpretation as below:\n* Customers with Cluster Id 0 are less frequent buyers with low monetary expenditure and also they have not purchased anything in recent time and hence least important for business.\n* Customers with Cluster Id 1 are the customers having Recency, Frequency and Monetary score in the medium range.\n* Customers with Cluster Id 2 are the most frequent buyers, spending high amount and recently placing orders so they are the most important customers from business point of view.","1b08fa51":"**Recency Analysis:**","73dd3dde":"**Problem Statement:**\n* It is a critical requirement for business to understand the value derived from a customer. RFM is a method used for analyzing customer value.\n* Customer segmentation is the practice of segregating the customer base into groups of individuals based on some common characteristics such as age, gender, interests, and spending habits\n* Perform customer segmentation using RFM analysis. The resulting segments can be ordered from most valuable (highest recency, frequency, and value) to least valuable (lowest recency, frequency, and value).\n\n**Dataset Description:** \n    This is a transnational data set which contains all the transactions that occurred between 01\/12\/2010 and 09\/12\/2011 for a UK-based and registered non-store online retail. The company mainly sells unique and all-occasion gifts.\n\n* **InvoiceNo:** Invoice number. Nominal, a 6-digit integral number uniquely assigned to each transaction. If this code starts with letter 'c', it indicates a cancellation. \n* **StockCode:** Product (item) code. Nominal, a 5-digit integral number uniquely assigned to each distinct product. \n* **Description:** Product (item) name. Nominal. \n* **Quantity:** The quantities of each product (item) per transaction. Numeric. \n* **InvoiceDate:** Invoice Date and time. Numeric, the day and time when each transaction was generated. \n* **UnitPrice:** Unit price. Numeric, Product price per unit in sterling. \n* **CustomerID:** Customer number. Nominal, a 5-digit integral number uniquely assigned to each customer. \n* **Country:** Country name. Nominal, the name of the country where each customer resides.\n","5a989bec":"* **(b) Analyze the retention rate of customers:**","023fa1f7":"**Standard Scalar Transformation:** It is extremely important to rescale the features so that they have a comparable scale.","af98d295":"### (B) Data Transformation\n**(2) Perform Cohort Analysis**\n* **(a) Create month cohort of customers and analyze active customers in each cohort:**","a8deb159":"## Week 3\n### Data Modeling:\n\n1. Create clusters using k-means clustering algorithm.\n\n      **a. Prepare the data for the algorithm. If the data is asymmetrically distributed, manage the skewness with appropriate transformation. Standardize the data.**\n","b3e5efad":"**Frequency Analysis:**","f7406f10":"As we can see two columns in data have missing values.\n* Description - 0.27% (1454 nos.)\n* CustomerID  - 24.93% (135080)\n\n**CustomerID** is important feature of our analysis since our analysis is centered around Customers only so we can not impute null values **CustomerID** with mean\/ median\/ mode in this case. We will check possibility to fill null values in **CustomerID** column by looking up for **InvoiceNo** of the row having null **CustomerID** in other rows where **CustomerID** is present. If there are still any null values in **CustomerID** after this process then we will drop complete row having missing **CustomerID**.\n\nWe can drop **Description** feature from our data since it is not not going to contribute in our model.","1f48536e":"* **(c) Perform descriptive anaylysis on the given data:**"}}