{"cell_type":{"2d09ef92":"code","782c35c4":"code","b436442f":"code","f47e9000":"code","04c3521b":"code","f5ea5d7f":"code","b93fb093":"code","9f274211":"code","c0aa6ea1":"code","d5b536d2":"code","edadcd14":"code","90039d8f":"code","be1d81f9":"code","446cfa43":"code","e15fb6d8":"code","88504b0e":"code","dcfe0348":"code","d34827bd":"code","1489dc2c":"code","1d39462e":"code","255fe1af":"code","1cefdbf2":"code","7ba077dd":"code","b82c750d":"code","f7831510":"code","b85db52d":"code","398c821f":"code","1ddf67ef":"code","40ffe7a4":"code","01a00718":"code","40df0a68":"code","b2cff5a9":"code","7cbb9f21":"code","a92fbe08":"code","7fa1177d":"code","1ab5744f":"code","9f5c6e17":"code","b232b02d":"code","ff010b97":"code","2759cb1b":"code","cd8cfac7":"code","39fd7fe1":"code","12b4f78b":"code","77d2ef64":"code","d9133aeb":"code","1c1152cc":"code","ac9d359e":"code","461400f1":"code","2d0a5fb8":"code","050c7038":"code","70ff7824":"code","8bfac6a3":"code","0c99a436":"code","3e7c7455":"code","0c92f57a":"code","d75c91f4":"code","8088ecdf":"code","2c10f299":"code","5ba3faa5":"code","a4f6f6d1":"code","15e7336b":"code","be520a94":"code","56f8d905":"code","ae6bb5e9":"code","2804dc7f":"code","726eb0e8":"code","417752a9":"code","3082c33d":"code","3288951d":"code","30241697":"code","7e93c14c":"code","4561729e":"code","ccd19a99":"code","fb52e2fd":"code","dee3a688":"code","51498608":"code","29898f45":"code","be807148":"code","3f6fc4da":"code","3177c8c4":"code","28b88449":"code","257f83ad":"code","f2289d00":"code","39e65d9b":"code","101c6aa0":"code","c9962537":"code","3f6d83b4":"code","927372be":"code","5dd1d08d":"code","8b4375f7":"code","ab2b622d":"code","e0ec8efa":"code","cf265276":"code","618f1dc1":"code","0b42bb10":"code","90f0f354":"code","114df972":"code","279b3d5b":"code","b7163248":"code","22624766":"code","bdca937d":"code","74f429ad":"code","4f21d1dd":"code","371195f0":"code","8e8c360d":"code","4a26d4b7":"code","5ae52b41":"code","aad2dffd":"code","a8076af7":"code","90f476f3":"code","90be539a":"code","b532a8dc":"code","65b1e71f":"code","59744203":"code","f02d56d8":"code","7a0c70f1":"code","9d18cb1e":"code","b10c44ed":"code","b3b127ff":"code","3acb8d8a":"code","c63b278d":"code","7a438dff":"code","88add8b3":"code","eb46042d":"code","6ad1dd02":"code","45762c06":"code","94e78bcf":"code","f1d4c00f":"code","3c3be0a3":"code","4a71fbba":"code","44bb726c":"code","9a8ae812":"code","5de9efef":"code","44973bbe":"code","83bc9422":"code","272f80b3":"code","7204f8ab":"code","ce3173af":"code","21b22ada":"code","9e7e889b":"code","d6d8c1c9":"code","6a1ffc03":"code","ac280f19":"code","4b1978e2":"code","32dfe06b":"code","45306f41":"code","668752b7":"code","c482efca":"code","e8b8a50e":"code","2f529cf3":"code","3388c206":"code","01b85f9d":"code","e75a6871":"code","47172ebc":"code","fe768a83":"code","8865b586":"code","2bd0b501":"code","ea660c72":"code","d1dd8fe2":"code","a3f3b9e9":"code","acc07a0e":"code","3f45e7be":"code","8418477b":"code","512a2fcd":"code","7e88a2d5":"code","e1abb6ec":"code","f0f610f6":"code","2a638988":"code","f7624f77":"code","d3017905":"code","5d68977b":"code","be4317d6":"code","263255b5":"code","7c3078d8":"code","b71b4dcd":"code","3de498e7":"code","b70d5518":"code","a821d1f1":"code","01587872":"code","34837e5e":"code","4ebacacc":"code","464bdd3f":"code","67210667":"code","d6b74277":"code","b0d1dc25":"code","8e048673":"code","3fd9bcc1":"code","2ab8d9af":"code","66c3bc41":"code","125a530f":"code","75c840c8":"code","19ecffb2":"code","6c2902d5":"code","755ec6bb":"code","fe8c19fc":"code","05d7ecb7":"code","37eba40f":"code","fbc1ada9":"code","f8fc3845":"code","3a1104a4":"code","639947ed":"code","987b3b63":"code","e602a1d4":"code","5947d220":"code","aebd047f":"code","fa177895":"code","d1f1acae":"code","0d3eb974":"code","825f1e5b":"code","bad8bea6":"code","0b1d5b62":"markdown","f555e76e":"markdown","525ec260":"markdown","9b05f01d":"markdown","e4da255d":"markdown","750adb55":"markdown","800b1598":"markdown","d79dab6d":"markdown","ae73cb7b":"markdown","6d89a274":"markdown","63381593":"markdown","ef67014b":"markdown","bb02f293":"markdown","d95e1422":"markdown","556a0c7c":"markdown","373d9304":"markdown","eb96c6f7":"markdown","fe8e888e":"markdown","3f172906":"markdown","fbe30053":"markdown","b3a68c35":"markdown","a39fde63":"markdown","77fe8716":"markdown","d089c797":"markdown","80ba14b4":"markdown","7579f6a5":"markdown","0ddab123":"markdown","e6af48d1":"markdown","8ec7a952":"markdown","72011435":"markdown","5b1a679b":"markdown","34424984":"markdown","67ae6242":"markdown","c684088b":"markdown","15d61322":"markdown","d8857a90":"markdown","6d472c7c":"markdown","bc12f0d4":"markdown","a0f25466":"markdown","139da22e":"markdown","ed62d9cc":"markdown","f4cfbfdc":"markdown","3fbf5f82":"markdown","8780f7a3":"markdown","71362e60":"markdown","024e242b":"markdown","70181052":"markdown","faa4642c":"markdown","e15facf3":"markdown","1c19be52":"markdown","69f858f4":"markdown","b84bab7f":"markdown","bb5bbaa1":"markdown","f65fe95e":"markdown","21aa32f7":"markdown","bbe79f33":"markdown","7e60f4e6":"markdown","6e72c39b":"markdown","0b060913":"markdown","1159393d":"markdown"},"source":{"2d09ef92":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib as mp\nimport matplotlib.pyplot as plt\n\nimport nltk\nimport re\nimport string\nfrom nltk.corpus import stopwords\nfrom nltk import word_tokenize\nnltk.download('punkt')\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_extraction.text import  TfidfTransformer\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import LinearSVC\nfrom sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import plot_confusion_matrix,classification_report,confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.pipeline import Pipeline,make_pipeline\nimport lime\nfrom lime import lime_text\nfrom lime.lime_text import LimeTextExplainer\nimport spacy\nimport tensorflow as tf\nimport gensim\nfrom gensim.models import Word2Vec\nfrom wordcloud import WordCloud\nfrom nltk.probability import FreqDist\nfrom sklearn.decomposition import LatentDirichletAllocation\nimport gensim.corpora as corpora\nfrom pprint import pprint\n# Plotting tools\nimport pyLDAvis\nimport pyLDAvis.gensim\nimport random\nfrom sklearn.decomposition import NMF\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.layers import Conv1D, Embedding,Dense,Flatten, Input, GlobalMaxPooling1D,MaxPooling1D,Dropout,Activation,BatchNormalization\nfrom tensorflow.keras.models import Model,Sequential\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nimport warnings\nwarnings.filterwarnings('ignore')","782c35c4":"df = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/train.csv\")\ndf.head()","b436442f":"df.shape # 7613 observations and four features and one target","f47e9000":"sns.countplot(data=df,x='target')","04c3521b":"df['target'].value_counts()","f5ea5d7f":"df['target'] = df.target.astype(str)\ndf.dtypes ","b93fb093":"data = df[['text','target']]\ndata.head()","9f274211":"# any null columns\ndata.isnull().sum()","c0aa6ea1":"# remove all characters not number or characters\ndef cleanText(input_string):\n    modified_string = re.sub('[^A-Za-z0-9]+', ' ', input_string)\n    modified_string = re.sub('[0-9]+', ' ', modified_string)\n    modified_string=re.sub(\"[@]\",\"\",modified_string)\n    return(modified_string)\ndata['text'] = data.text.apply(cleanText)\ndata['text'][150]\n\n","d5b536d2":"# Remove non printable characters\ndef remove_not_ASCII(text):\n    text = ''.join([word for word in text if word in string.printable])\n    return text\ndata['text'] = data.text.apply(remove_not_ASCII)\ndata['text'][150]\n","edadcd14":"data.sample(10)","90039d8f":"#converting to lower case\ndata['text']=data['text'].str.lower()","be1d81f9":"#removing punctuations\ndata['text']=data['text'].str.translate(str.maketrans('','',string.punctuation))","446cfa43":" nltk.download('stopwords')","e15fb6d8":"stopWords=stopwords.words('english')+['the', 'a', 'k','h','g','an', 'i', 'he', 'she', 'they', 'to', 'of', 'it', 'n','from','u','w','p','x','c','r','co','http','https','b','e','v','l','q']\ndef removeStopWords(stopWords, rvw_txt):\n    newtxt = ' '.join([word for word in rvw_txt.split() if word not in stopWords])\n    return newtxt\ndata['text'] = [removeStopWords(stopWords,x) for x in data['text']]","88504b0e":"#removing hyperlink\ndata['text']=data['text'].apply(lambda x:re.sub('https?:\/\/\\S+|www\\.\\S+', '', x) )","dcfe0348":"#remove words containing numbers\ndata['text']=data['text'].apply(lambda x:re.sub('\\w*\\d\\w*' , '', x) )","d34827bd":"#removing square brackets\ndata['text']=data['text'].apply(lambda x:re.sub('\\[.*?\\]', '', x) )\ndata['text']=data['text'].apply(lambda x:re.sub('<.*?>+', '', x) )","1489dc2c":"#splitting text into words\ntokenList=[]\nfor indx in range(len(data)):\n       token=word_tokenize(data['text'][indx])\n       tokenList.append(token)\ndata['text_tokens'] = tokenList\ndata.head()","1d39462e":"textcorpus=' ' .join([str(item) for item in data['text_tokens'] ])","255fe1af":"#textcorpus","1cefdbf2":"word_tokens = word_tokenize(textcorpus)\n\n#print(word_tokens)","7ba077dd":"#removing punctuations\nfiltered_words = []\n\nfor w in word_tokens:\n    if w not in string.punctuation:\n        filtered_words.append(w)\n        \n#print(filtered_words)","b82c750d":"freq_dist = FreqDist(filtered_words)\n\nprint(freq_dist)","f7831510":"freq_dist.most_common(20)","b85db52d":"fig, ax = plt.subplots(figsize=(12, 8))\n\nfreq_dist.plot(20, cumulative=False)\n\nplt.show()","398c821f":"wordcloud = WordCloud(max_font_size=50, max_words=100, background_color=\"white\").generate(textcorpus)\nplt.figure(figsize=(15, 10), dpi=80)\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis(\"off\")\nplt.show()","1ddf67ef":"text = data.text[0]\n\n# Create and generate a word cloud image:\nwordcloud = WordCloud().generate(text)\n\n# Display the generated image:\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()","40ffe7a4":"print(data.target[0]) ## disaster tweet","01a00718":"df_1 = data[data.target == '1']","40df0a68":"df_1.head()","b2cff5a9":"text = \" \".join(x for x in df_1.text)\n","7cbb9f21":"len(text)","a92fbe08":"wordcloud = WordCloud(max_font_size=50, max_words=100, background_color=\"white\").generate(text)\nplt.figure(figsize=(15, 10), dpi=80)\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis(\"off\")\nplt.show()","7fa1177d":"df_0 = data[data.target == '0']","1ab5744f":"text = \" \".join(x for x in df_0.text)\nlen(text)","9f5c6e17":"wordcloud = WordCloud(max_font_size=50, max_words=100, background_color=\"white\").generate(text)\nplt.figure(figsize=(15, 10), dpi=80)\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis(\"off\")\nplt.show()","b232b02d":"vectorizer = CountVectorizer()\nbag_of_words = vectorizer.fit_transform(df['text'])\n\nbag_of_words","ff010b97":"print(bag_of_words)","2759cb1b":"vectorizer.vocabulary_.get('disaster')","cd8cfac7":"#vectorizer.vocabulary_","39fd7fe1":"#pd.DataFrame(bag_of_words.toarray(), columns=vectorizer.get_feature_names())","12b4f78b":"vectorizer = TfidfVectorizer()\nbag_of_words = vectorizer.fit_transform(df['text'])\n\nprint(bag_of_words)","77d2ef64":"y = data['target']\nX = data['text']","d9133aeb":"\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=101)","1c1152cc":"\ntfidf = TfidfVectorizer(stop_words='english')","ac9d359e":"tfidf.fit(X_train)","461400f1":"X_train_tfidf = tfidf.transform(X_train)\nX_test_tfidf = tfidf.transform(X_test)","2d0a5fb8":"X_train_tfidf","050c7038":"X_test_tfidf","70ff7824":"#X_test","8bfac6a3":"nb = MultinomialNB()\nnb.fit(X_train_tfidf,y_train)","0c99a436":"log = LogisticRegression(max_iter=500)\nlog.fit(X_train_tfidf,y_train)","3e7c7455":"svc = LinearSVC(penalty=\"l2\")\nsvc.fit(X_train_tfidf,y_train)","0c92f57a":"preds = nb.predict(X_test_tfidf)\npredicted_prob = nb.predict_proba(X_test_tfidf)\nprint(classification_report(y_test,preds))","d75c91f4":"confusion_matrix(y_test,preds)","8088ecdf":"classes = np.unique(y_test)","2c10f299":"cm = confusion_matrix(y_test, preds)\nfig, ax = plt.subplots()\nsns.heatmap(cm, annot=True, fmt='d', ax=ax, cmap=plt.cm.Blues, \n            cbar=False)\nax.set(xlabel=\"Pred\", ylabel=\"True\", xticklabels=classes, \n       yticklabels=classes, title=\"Confusion matrix\")\nplt.yticks(rotation=0)","5ba3faa5":"preds = log.predict(X_test_tfidf)\nprint(classification_report(y_test,preds))\nconfusion_matrix(y_test,preds)\n","a4f6f6d1":"cm = confusion_matrix(y_test, preds)\nfig, ax = plt.subplots()\nsns.heatmap(cm, annot=True, fmt='d', ax=ax, cmap=plt.cm.Blues, \n            cbar=False)\nax.set(xlabel=\"Pred\", ylabel=\"True\", xticklabels=classes, \n       yticklabels=classes, title=\"Confusion matrix\")\nplt.yticks(rotation=0)","15e7336b":"preds = svc.predict(X_test_tfidf)\nprint(classification_report(y_test,preds))\nconfusion_matrix(y_test,preds)","be520a94":"cm = confusion_matrix(y_test, preds)\nfig, ax = plt.subplots()\nsns.heatmap(cm, annot=True, fmt='d', ax=ax, cmap=plt.cm.Blues, \n            cbar=False)\nax.set(xlabel=\"Pred\", ylabel=\"True\", xticklabels=classes, \n       yticklabels=classes, title=\"Confusion matrix\")\nplt.yticks(rotation=0)","56f8d905":"dftest = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/test.csv\")\ndftest.head()","ae6bb5e9":"X = dftest[['text']]\nX.head()","2804dc7f":"X = X.text.apply(cleanText)\nX[150]","726eb0e8":"#converting to lower case\nX = X.str.lower()","417752a9":"#removing punctuations\nX=X.str.translate(str.maketrans('','',string.punctuation))","3082c33d":"X = [removeStopWords(stopWords,x) for x in X]","3288951d":"X_test = tfidf.transform(X)","30241697":"X_test","7e93c14c":"preds = nb.predict(X_test)\n","4561729e":"preds","ccd19a99":"X_test.shape","fb52e2fd":"len(preds)","dee3a688":"len(dftest)","51498608":"df_submission = pd.DataFrame({'id': dftest.id, 'target' : preds} )\ndf_submission.head()\ndf_submission.to_csv('\/kaggle\/working\/submission.csv', index=False) #0.79619\n","29898f45":"\nnlp = spacy.load(\"en_core_web_sm\")\n\ndoc = nlp(data[\"text\"].values[1])\nprint(doc)\n\nfor entity in doc.ents:\n    print(entity.text, entity.label_)","be807148":"doc.ents","3f6fc4da":"from tqdm.auto import tqdm\ntqdm.pandas()\n\ndata[\"ents\"] = data[\"text\"].progress_map(lambda text: [(entity.text, entity.label_) for entity in nlp(text).ents])","3177c8c4":"data.sample(5)","28b88449":"data[\"ent_types\"] = data[\"ents\"].progress_map(lambda x: set(ent[1] for ent in x))","257f83ad":"def replace_text(text, entities):\n    for ent_name, ent_type in entities:\n        text = text.replace(ent_name, ent_type)\n        \n    return text\n\ndata[\"format_text\"] = data.progress_apply(lambda x: replace_text(x[\"text\"], x[\"ents\"]), axis=1)","f2289d00":"data.sample(5)","39e65d9b":"train_df, test_df = train_test_split(data, stratify=data[\"target\"], test_size=0.1)","101c6aa0":"tfidf_vectorizer = TfidfVectorizer(max_df=0.99, \n                                min_df=5,\n                                lowercase=True,\n                                stop_words='english')\ntrain_tfidf = tfidf_vectorizer.fit_transform(train_df[\"format_text\"].values)","c9962537":"train_tfidf","3f6d83b4":"model = LogisticRegression(multi_class=\"multinomial\")\nmodel.fit(train_tfidf, train_df[\"target\"])","927372be":"test_tfidf = tfidf_vectorizer.transform(test_df[\"format_text\"])\ntest_preds = model.predict(test_tfidf)\naccuracy_score(test_df[\"target\"], test_preds)","5dd1d08d":"from nltk.stem import PorterStemmer \nfrom nltk.tokenize import word_tokenize\n\nps = PorterStemmer() \ndef stem_sentence(text):\n    return \" \".join([ps.stem(word) for word in word_tokenize(text)])\n\ntrain_df[\"processed_text\"] = train_df[\"format_text\"].progress_map(stem_sentence)\ntest_df[\"processed_text\"] = test_df[\"format_text\"].map(stem_sentence)","8b4375f7":"tfidf_vectorizer = TfidfVectorizer(max_df=0.99, \n                                min_df=5,\n                                lowercase=True,\n                                stop_words='english')\ntrain_tfidf = tfidf_vectorizer.fit_transform(train_df[\"processed_text\"].values)","ab2b622d":"model = LogisticRegression(multi_class=\"multinomial\")\nmodel.fit(train_tfidf, train_df[\"target\"])\n\ntest_tfidf = tfidf_vectorizer.transform(test_df[\"processed_text\"])\ntest_preds = model.predict(test_tfidf)\naccuracy_score(test_df[\"target\"], test_preds)","e0ec8efa":"train_tfidf","cf265276":"tfidf_vectorizer = TfidfVectorizer(max_df=0.99, \n                                   min_df=5,\n                                   lowercase=True,\n                                   stop_words='english',\n                                   ngram_range=(1, 2) \n                                  )\ntrain_tfidf = tfidf_vectorizer.fit_transform(train_df[\"processed_text\"].values)\n\ntrain_tfidf.shape","618f1dc1":"model = LogisticRegression(solver='sag',multi_class=\"multinomial\")\nmodel.fit(train_tfidf, train_df[\"target\"])\n\n","0b42bb10":"test_tfidf = tfidf_vectorizer.transform(test_df[\"processed_text\"])\ntest_preds = model.predict(test_tfidf)\naccuracy_score(test_df[\"target\"], test_preds)","90f0f354":"data['text length'] = data['text'].apply(len)","114df972":"g = sns.FacetGrid(data,col='target')\ng.map(plt.hist,'text length')","279b3d5b":"sns.boxplot(x='target',y='text length',data=data,palette='rainbow')","b7163248":"X=data['text']\ny=data['target']","22624766":"\ncv = CountVectorizer()","bdca937d":"X = cv.fit_transform(X)","74f429ad":"X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.3,random_state=101)","4f21d1dd":"nb = MultinomialNB()","371195f0":"nb.fit(X_train,y_train)","8e8c360d":"predictions = nb.predict(X_test)","4a26d4b7":"print(confusion_matrix(y_test,predictions))\nprint('\\n')\nprint(classification_report(y_test,predictions))","5ae52b41":"\npipeline = Pipeline([\n    ('bow', CountVectorizer()),  # strings to token integer counts\n    ('tfidf', TfidfTransformer()),  # integer counts to weighted TF-IDF scores\n    ('classifier', MultinomialNB()),  # train on TF-IDF vectors w\/ Naive Bayes classifier\n])","aad2dffd":"X = data['text']\ny = data['target']\nX_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.3,random_state=101)","a8076af7":"pipeline.fit(X_train,y_train)","90f476f3":"predictions = pipeline.predict(X_test)","90be539a":"predicted_prob = pipeline.predict_proba(X_test)","b532a8dc":"predicted_prob","65b1e71f":"print(confusion_matrix(y_test,predictions))\nprint(classification_report(y_test,predictions))","59744203":"cm = confusion_matrix(y_test, predictions)\nfig, ax = plt.subplots()\nsns.heatmap(cm, annot=True, fmt='d', ax=ax, cmap=plt.cm.Blues, \n            cbar=False)\nax.set(xlabel=\"Pred\", ylabel=\"True\", xticklabels=classes, \n       yticklabels=classes, title=\"Confusion matrix\")\nplt.yticks(rotation=0)","f02d56d8":"## select observation\nidx = 0\ntxt_instance = X_test.iloc[idx]\n## check true value and predicted value\nprint(\"True:\", y_test.iloc[idx], \"--> Pred:\", predictions[idx], \"| Prob:\", round(np.max(predicted_prob[idx]),2))\n## show explanation\nexplainer = lime_text.LimeTextExplainer(class_names=\n             np.unique(y_train))\nexplained = explainer.explain_instance(txt_instance, pipeline.predict_proba, num_features=6)\nexplained.show_in_notebook(text=txt_instance, predict_proba=False)","7a0c70f1":"## select observation\nidx = 1\ntxt_instance = X_test.iloc[idx]\nprint(txt_instance)\n## check true value and predicted value\nprint(\"True:\", y_test.iloc[idx], \"--> Pred:\", predictions[idx], \"| Prob:\", round(np.max(predicted_prob[idx]),2))\n## show explanation\nexplainer = lime_text.LimeTextExplainer(class_names=np.unique(y_train))\nexplained = explainer.explain_instance(txt_instance, pipeline.predict_proba, num_features=6)\nexplained.show_in_notebook(text=txt_instance, predict_proba=False)","9d18cb1e":"## select observation\nidx = 2\ntxt_instance = X_test.iloc[idx]\nprint(txt_instance)\n## check true value and predicted value\nprint(\"True:\", y_test.iloc[idx], \"--> Pred:\", predictions[idx], \"| Prob:\", round(np.max(predicted_prob[idx]),2))\n## show explanation\nexplainer = lime_text.LimeTextExplainer(class_names=np.unique(y_train))\nexplained = explainer.explain_instance(txt_instance, pipeline.predict_proba, num_features=6)\nexplained.show_in_notebook(text=txt_instance, predict_proba=False)","b10c44ed":"## select observation\nidx = 3\ntxt_instance = X_test.iloc[idx]\nprint(txt_instance)\n## check true value and predicted value\nprint(\"True:\", y_test.iloc[idx], \"--> Pred:\", predictions[idx], \"| Prob:\", round(np.max(predicted_prob[idx]),2))\n## show explanation\nexplainer = lime_text.LimeTextExplainer(class_names=np.unique(y_train))\nexplained = explainer.explain_instance(txt_instance, pipeline.predict_proba, num_features=6)\nexplained.show_in_notebook(text=txt_instance, predict_proba=False)","b3b127ff":"## select observation\nidx = 4\ntxt_instance = X_test.iloc[idx]\nprint(txt_instance)\n## check true value and predicted value\nprint(\"True:\", y_test.iloc[idx], \"--> Pred:\", predictions[idx], \"| Prob:\", round(np.max(predicted_prob[idx]),2))\n## show explanation\nexplainer = lime_text.LimeTextExplainer(class_names=np.unique(y_train))\nexplained = explainer.explain_instance(txt_instance, pipeline.predict_proba, num_features=6)\nexplained.show_in_notebook(text=txt_instance, predict_proba=False)","3acb8d8a":"X = dftest['text']","c63b278d":"predictions = pipeline.predict(X)","7a438dff":"df_submission = pd.DataFrame({'id': dftest.id, 'target' : predictions} )","88add8b3":"df_submission.head()","eb46042d":"#df_submission.to_csv('\/kaggle\/working\/submissionnew2.csv', index=False)  ## 0.79007 accuracy","6ad1dd02":"# take the text column\ntext = data['text'].values\ntext","45762c06":"# tokenize the words\nnew_vec = [nltk.word_tokenize(textdata) for textdata in text]\nnew_vec[0]","94e78bcf":"# build the model\nmodel = Word2Vec(new_vec, min_count=1,vector_size=32)\nmodel","f1d4c00f":"# predict the output\nmodel.wv.most_similar('fire')\nmodel.wv['fire']","3c3be0a3":"vec = model.wv['disaster']\nmodel.wv.most_similar([vec]) #### fire, emergency similar to disaster","4a71fbba":"vec = model.wv['police']\nmodel.wv.most_similar([vec]) #### people, fire similar to police","44bb726c":"vec = model.wv['emergency']\nmodel.wv.most_similar([vec]) #### crash, fire similar to emergency","9a8ae812":"disaster_tweets_df = data[data['target']==\"1\"]['text']\nnondisaster_tweets_df = data[data['target']==\"0\"]['text']","5de9efef":"LDA = LatentDirichletAllocation(n_components=4,random_state=42) # 4 topics","44973bbe":"vectorizer = TfidfVectorizer()\ndtm = vectorizer.fit_transform(disaster_tweets_df)\ndtm","83bc9422":"LDA.fit(dtm)","272f80b3":"len(vectorizer.get_feature_names())","7204f8ab":"\n\nfor i in range(10):\n    random_id = random.randint(0,len(vectorizer.get_feature_names()))\n    print(vectorizer.get_feature_names()[random_id])","ce3173af":"for i in range(10):\n    random_id = random.randint(0,len(vectorizer.get_feature_names()))\n    print(vectorizer.get_feature_names()[random_id])","21b22ada":"LDA.components_","9e7e889b":"len(LDA.components_[0])","d6d8c1c9":"single_topic = LDA.components_[0]","6a1ffc03":"# Returns the indices that would sort this array.\nsingle_topic.argsort()","ac280f19":"top_word_indices = single_topic.argsort()[-10:]","4b1978e2":"for index in top_word_indices:\n    print(vectorizer.get_feature_names()[index]) # we can find words like bomber, suicide, oil, spill, fire, sandstorm","32dfe06b":"for i,topic in enumerate(LDA.components_):\n    print(f'Top 15 words for topic #{i}:')\n    print([vectorizer.get_feature_names()[i] for i in topic.argsort()[-15:]])\n    print('\\n')","45306f41":"dtm = vectorizer.fit_transform(nondisaster_tweets_df)\ndtm","668752b7":"LDA.fit(dtm) # 4 topics","c482efca":"len(vectorizer.get_feature_names())","e8b8a50e":"for i in range(10):\n    random_id = random.randint(0,len(vectorizer.get_feature_names()))\n    print(vectorizer.get_feature_names()[random_id])\n","2f529cf3":"for i in range(10):\n    random_id = random.randint(0,len(vectorizer.get_feature_names()))\n    print(vectorizer.get_feature_names()[random_id])\n","3388c206":"LDA.components_","01b85f9d":"len(LDA.components_[0])","e75a6871":"single_topic = LDA.components_[0]","47172ebc":"# Returns the indices that would sort this array.\nsingle_topic.argsort()","fe768a83":"# Top 10 words for this topic:\nsingle_topic.argsort()[-10:]","8865b586":"top_word_indices = single_topic.argsort()[-10:]","2bd0b501":"for index in top_word_indices:\n    print(vectorizer.get_feature_names()[index])","ea660c72":"for i,topic in enumerate(LDA.components_):\n    print(f'Top 15 words for topic #{i}:')\n    print([vectorizer.get_feature_names()[i] for i in topic.argsort()[-15:]])\n    print('\\n')","d1dd8fe2":"disaster_tweets_df = data[data['target']==\"1\"]['text_tokens']","a3f3b9e9":"# Create Dictionary\nid2word = corpora.Dictionary(disaster_tweets_df.values)","acc07a0e":"# Term Document Frequency\ncorpus = [id2word.doc2bow(text) for text in disaster_tweets_df]\n\n# View\nprint(corpus[:1])","3f45e7be":"id2word[0]","8418477b":"[[(id2word[id], freq) for id, freq in cp] for cp in corpus[:1]]","512a2fcd":"lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n                                           id2word=id2word,\n                                           num_topics=4, \n                                           random_state=100,\n                                           update_every=1,\n                                           chunksize=100,\n                                           passes=10,\n                                           alpha='auto',\n                                           per_word_topics=True)","7e88a2d5":"# Print the Keyword in the 10 topics\npprint(lda_model.print_topics())\ndoc_lda = lda_model[corpus]","e1abb6ec":"# Visualize the topics\npyLDAvis.enable_notebook()\nvis = pyLDAvis.gensim.prepare(lda_model, corpus, id2word)\nvis","f0f610f6":"\nnondisaster_tweets_df = data[data['target']==\"0\"]['text_tokens']","2a638988":"# Create Dictionary\nid2word = corpora.Dictionary(nondisaster_tweets_df.values)\n","f7624f77":"# Term Document Frequency\ncorpus = [id2word.doc2bow(text) for text in nondisaster_tweets_df]\n\n# View\nprint(corpus[:1])","d3017905":"id2word[0]","5d68977b":"[[(id2word[id], freq) for id, freq in cp] for cp in corpus[:1]]","be4317d6":"lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n                                           id2word=id2word,\n                                           num_topics=4, \n                                           random_state=100,\n                                           update_every=1,\n                                           chunksize=100,\n                                           passes=10,\n                                           alpha='auto',\n                                           per_word_topics=True)","263255b5":"# Print the Keyword in the 10 topics\npprint(lda_model.print_topics())\ndoc_lda = lda_model[corpus]","7c3078d8":"# Visualize the topics\npyLDAvis.enable_notebook()\nvis = pyLDAvis.gensim.prepare(lda_model, corpus, id2word)\nvis","b71b4dcd":"disaster_tweets_df = data[data['target']==\"1\"]['text']","3de498e7":"vectorizer = TfidfVectorizer()\ndtm = vectorizer.fit_transform(disaster_tweets_df)\ndtm","b70d5518":"\n\nnmf = NMF(n_components=4, random_state=42)\nnmf.fit(dtm )","a821d1f1":"for i in range(10):\n    random_id = random.randint(0,len(vectorizer.get_feature_names()))\n    print(vectorizer.get_feature_names()[random_id])","01587872":"first_topic = nmf.components_[0]\ntop_topic_words = first_topic.argsort()[-10:]","34837e5e":"for i in top_topic_words:\n    print(vectorizer.get_feature_names()[i])","4ebacacc":"for i,topic in enumerate(nmf.components_):\n    print(f'Top 15 words for topic #{i}:')\n    print([vectorizer.get_feature_names()[i] for i in topic.argsort()[-15:]])\n    print('\\n')","464bdd3f":"nondisaster_tweets_df = data[data['target']==\"0\"]['text']","67210667":"vectorizer = TfidfVectorizer()\ndtm = vectorizer.fit_transform(nondisaster_tweets_df)\ndtm","d6b74277":"nmf = NMF(n_components=4, random_state=42)\nnmf.fit(dtm )","b0d1dc25":"for i in range(10):\n    random_id = random.randint(0,len(vectorizer.get_feature_names()))\n    print(vectorizer.get_feature_names()[random_id])","8e048673":"first_topic = nmf.components_[0]\ntop_topic_words = first_topic.argsort()[-10:]","3fd9bcc1":"for i in top_topic_words:\n    print(vectorizer.get_feature_names()[i])","2ab8d9af":"for i,topic in enumerate(nmf.components_):\n    print(f'Top 15 words for topic #{i}:')\n    print([vectorizer.get_feature_names()[i] for i in topic.argsort()[-15:]])\n    print('\\n')","66c3bc41":"y = data['target']\nX = data['text']","125a530f":"X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.3,random_state=101)","75c840c8":"# Model constants.\nmax_features = 200000\nembedding_dim = 128","19ecffb2":"tokenizer = Tokenizer(num_words=5000)","6c2902d5":"print(X_train[2])","755ec6bb":"tokenizer.fit_on_texts(X_train)","fe8c19fc":"X_train = tokenizer.texts_to_sequences(X_train)","05d7ecb7":"X_test = tokenizer.texts_to_sequences(X_test)","37eba40f":"vocab_size = len(tokenizer.word_index) + 1  # Adding 1 because of reserved 0 index\nvocab_size","fbc1ada9":"print(X_train[2])","f8fc3845":"for word in ['residents', 'shelter', 'place', 'evacuation','expected','notified']:\n     print('{}: {}'.format(word, tokenizer.word_index[word]))","3a1104a4":"maxlen = 24","639947ed":" X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)","987b3b63":"X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)","e602a1d4":"print(X_train[0, :])","5947d220":"y_train=y_train.astype('int')\ny_test=y_test.astype('int')","aebd047f":"#embedding_dim = 100\n\nmodel = Sequential()\nmodel.add(Embedding(vocab_size, embedding_dim, input_length=maxlen))\nmodel.add(Conv1D(32, 4, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(MaxPooling1D(2))\nmodel.add(Conv1D(64, 4, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(MaxPooling1D(2))\nmodel.add(Flatten())\nmodel.add(Dense(10, activation='relu'))\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.compile(optimizer='adam',\n              loss='binary_crossentropy',\n              metrics=['accuracy'])\nmodel.summary()","fa177895":"history = model.fit(X_train, y_train,\n                    epochs=10,\n                    verbose=True,\n                    validation_data=(X_test, y_test),\n                    batch_size=16)","d1f1acae":"loss, accuracy = model.evaluate(X_train, y_train, verbose=False)\nprint(\"Training Accuracy: {:.4f}\".format(accuracy))\nloss, accuracy = model.evaluate(X_test, y_test, verbose=False)\nprint(\"Testing Accuracy:  {:.4f}\".format(accuracy))","0d3eb974":"preds = model.predict(X_test)","825f1e5b":"preds=tf.squeeze(tf.round(preds))","bad8bea6":"print(confusion_matrix(y_test,preds))\nprint(classification_report(y_test,preds))","0b1d5b62":"### Train Test Split","f555e76e":"#### Extend bag of words with TF-IDF weights","525ec260":"#### we can either use pre trained model or train word2vec from the text corpus","9b05f01d":"#### LDA with gensim\n#### The Dictionary and Corpus needed for Topic Modeling\n#### The two main inputs to the LDA topic model are the dictionary(id2word) and the corpus.","e4da255d":"![pic1.png](attachment:48f99adc-f35d-4cdc-bdea-218fa4a1237e.png)","750adb55":"#### every text is of length 24","800b1598":"### Features and Labels","d79dab6d":"#### words per topic","ae73cb7b":"#### non disaster tweets","6d89a274":"#### non disaster tweets ","63381593":"### Tfidf Vectorization","ef67014b":"# Analysis#4","bb02f293":"# Analysis#3 Understanding Word Embeddings with Gensim ","d95e1422":"![Capture.JPG](attachment:a0de9145-8546-4dca-9a96-3fbfde42b777.JPG)","556a0c7c":"#### how non-negative matrix factorization can be used for topic modeling.","373d9304":"# CNN1D for Text Classification\n","eb96c6f7":"#### Model perplexity and topic coherence provide a convenient measure to judge how good a given topic model is","fe8e888e":"#### Words per topic","3f172906":"### Test data","fbe30053":"# Text data","b3a68c35":"#### alpha and eta are hyperparameters that affect sparsity of the topics.\n","a39fde63":"#### The first topic contains the probabilities of words for topic 1. \n#### To sort the indexes according to probability values, we use the argsort() function. \n#### Once sorted, the 10 words with the highest probabilities will now belong to the last 10 indexes of the array.","77fe8716":"# BoW model Ngrams","d089c797":"#### Topic1 words like survive,disney,love,pretty\n#### Topic2 words like youtube, videos, amazon, work\n#### Topic3 words like good, movie, school, emotional\n#### Topic4 words like stock, new, great, god","80ba14b4":"# Extracting Features from Text\n##### Using Bag of Words, TF-IDF Transformation","7579f6a5":"#### lets check random 10 stored words","0ddab123":"#### Entities","e6af48d1":"# Interpreting text predictions with LIME","8ec7a952":"#### We can view the most frequent terms in the word cloud","72011435":"### Applying the same transformation on the test data as on the train data","5b1a679b":"# Working with NLTK","34424984":"# Analysis #2 Understanding Spacy","67ae6242":"### Evaluation","c684088b":"#### Non-negative matrix factorization is also a supervised learning technique which performs clustering as well as dimensionality reduction. It can be used in combination with TF-IDF scheme to perform topic modeling.","15d61322":"#### The above LDA model is built with 4 different topics where each topic is a combination of keywords and each keyword contributes a certain weightage to the topic.","d8857a90":"### When target equals 1 ","6d472c7c":"#### Topic1 and Topic4 related to nature related catastrophe for example weather, tornado, volcano, thunderstorm, storm, weather, wiildfire.\n#### Topic2 army, crash, bombing, survivors\n#### Topic3 migrants, trauma, earthquake, shots","bc12f0d4":"### Logistic Regression and Naive Bayes do have the same accuracy but differs slightly in the precision and recall","a0f25466":"#### non disaster tweets","139da22e":"### when target equals 0","ed62d9cc":"## Frequency distributions","f4cfbfdc":"#### Documents are probability distributions over topics and topics themselves are the probability distribution over words","3fbf5f82":"#### Get the value to which a word is mapped","8780f7a3":"#### disaster tweets and non disaster tweets","71362e60":"#### disaster tweets","024e242b":"One problem that we have is that each text sequence has in most cases different length of words. To counter this, you can use pad_sequence() which simply pads the sequence of words with zeros. By default, it prepends zeros but we want to append them. Typically it does not matter whether you prepend or append zeros.\n\nA maxlen parameter to specify how long the sequences should be. This cuts sequences that exceed that number.","70181052":"### Support Vector Machine","faa4642c":"#### Word2vec learns the meaning of words by processing a large corpus of unlabelled text. The word embeddings are the weights of the hidden layer that the neural network learns during the training process.The Word2vec model contains information about the relationships between words including similarity. The two ways of training word2vec embeddings are the skip gram approach that predicts the output words from a word of interest and continuous bag of words model predicts the target word from the nearby words.","e15facf3":"### Model training","1c19be52":"#### chunksize is the number of documents to be used in each training chunk. \n#### update_every determines how often the model parameters should be updated and passes is the total number of training passes.","69f858f4":"the Embedding Layer of Keras which takes the previously calculated integers and maps them to a dense vector of the embedding. Will need the following parameters:\n\ninput_dim: the size of the vocabulary output_dim: the size of the dense vector input_length: the length of the sequence","b84bab7f":"# Analysis#5","bb5bbaa1":"### Naive Bayes","f65fe95e":"![pic2.png](attachment:baa4cc08-704a-4594-a02b-cf06c869e499.png)","21aa32f7":"### Logistic Regression","bbe79f33":"# Topic modelling with Latent Dirichlet Allocation","7e60f4e6":"![pic3.png](attachment:a465ac5b-f4dc-41d7-9dbb-60d49c023c86.png)","6e72c39b":"#### Use CountVectorizer to convert a collection of text documents to a \"bag of words\"","0b060913":"# Analysis #1 NLTK and BoW model and Text Classification","1159393d":"# Pipeline"}}