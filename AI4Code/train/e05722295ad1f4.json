{"cell_type":{"4c03939b":"code","179ab5ff":"code","181d88bf":"code","e1867437":"code","05562f82":"code","74e4ddfe":"code","191b0937":"code","74cf3704":"code","eff34dab":"code","c5dae919":"code","24bc7bbf":"code","fee7d1d2":"code","0830b4cc":"code","9aa97179":"code","6273c2b5":"code","f1114ff8":"code","5c10044b":"code","21edd717":"code","8a2b95bf":"code","4c3a8b7a":"code","d8b05953":"code","53236fe9":"code","0b6aae2f":"code","edd911b3":"code","fdb0c434":"code","0024fe42":"code","091deca0":"code","2d5b0520":"code","3499c231":"code","24e75b0a":"code","2c2f5b9f":"code","60961d53":"code","b5a801ac":"markdown","49ae670a":"markdown","b0451ccd":"markdown","441b9613":"markdown","963c8bec":"markdown","719c072a":"markdown","08042bc9":"markdown","3087db73":"markdown","4d0d5c4e":"markdown","65b23176":"markdown","7d9d54ff":"markdown","17f0eae2":"markdown","b2b420f2":"markdown","18de5358":"markdown"},"source":{"4c03939b":"#Desenvolvido por Ronald Albert, 118021192\nimport pandas as pd\nimport numpy as np\nimport time\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn import metrics\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings('ignore')","179ab5ff":"df = pd.read_csv('..\/input\/testes\/dados.csv')\n\ndef returnGender(g):\n    if(g == 1):\n        return \"Masculino\"\n    else:\n        return \"Feminino\"\n    \ndef returnUF(u):\n    return {\n        11: \"Rond\u00f4nia\",\n        12: \"Acre\",\n        13: \"Amazonas\",\n        14: \"Roraima\",\n        15: \"Par\u00e1\",\n        16: \"Amap\u00e1\",\n        17: \"Tocantis\",\n        21: \"Maranh\u00e3o\",\n        22: \"Piau\u00ed\",\n        23: \"Cear\u00e1\",\n        24: \"Rio Grande do Norte\",\n        25: \"Para\u00edba\",\n        26: \"Pernambuco\",\n        27: \"Alagoas\",\n        28: \"Sergipe\",\n        29: \"Bahia\",\n        31: \"Minas Gerais\",\n        32: \"Esp\u00edrito Santo\",\n        33: \"Rio de Janeiro\",\n        35: \"S\u00e3o Paulo\",\n        41: \"Paran\u00e1\",\n        42: \"Santa Catarina\",\n        43: \"Rio Grande do Sul\",\n        50: \"Mato Grosso do Sul\",\n        51: \"Mato Grosso\",\n        52: \"Goias\",\n        53: \"Distrito Federal\"\n    }[u]\n\ndef returnEthnicGroup(e):\n    return {\n        0: \"Ind\u00edgena\",\n        2: \"Branco\",\n        4: \"Negro\",\n        6: \"Asi\u00e1tico\",\n        8: \"Pardo\",\n        9: \"Sem Declara\u00e7\u00e3o\"\n    }[e]\n\ndef returnGruposDeIdade(i):\n    if(i <= 18):\n        return \"Menor de 18 anos\"\n    elif(i <= 25):\n        return \"Entre 18 e 25 anos\"\n    elif(i <= 40):\n        return \"Entre 25 e 40 anos\"\n    elif (i <= 60):\n        return \"Entre 40 e 60 anos\"\n    else:\n        return \"Maior de 60 anos\"\n\ndef returnAnosDeEstudo(e):\n    if(e == 17):\n        return \"N\u00e3o Informado\"\n    return e - 1\n\ndef returnClasseSocial(r):\n    salario_minimo = 1100\n    if(r <= salario_minimo):\n        return \"E\"\n    elif(r <= salario_minimo*3):\n        return \"D\"\n    elif(r <= salario_minimo*5):\n        return \"C\"\n    elif(r <= salario_minimo*15):\n        return \"B\"\n    else:\n        return \"A\"\n\n\ndf['Sexo'] = df['Sexo'].apply(lambda x: returnGender(x))\ndf['UF'] = df['UF'].apply(lambda x: returnUF(x))\ndf['Cor'] = df['Cor'].apply(lambda x: returnEthnicGroup(x))\ndf['Idade'] = df['Idade'].apply(lambda x: returnGruposDeIdade(x))\ndf['Anos de Estudo'] = df['Anos de Estudo'].apply(lambda x: returnAnosDeEstudo(x))\ndf['Renda'] = df['Renda'].apply(lambda x: returnClasseSocial(x))","181d88bf":"def calcularEntropia(df, coluna):\n    values = df[coluna].unique()\n    entropia = 0\n    for i in values:\n        pi = len(df[df[coluna].eq(i)])\/len(df)\n        entropia += pi*np.log2(pi)\n    return -entropia\n    \ndef calcularGanho(df, resultado, coluna):\n    values = df[coluna].unique()\n    ganho = 0\n    for i in values:\n        pi = len(df[df[coluna].eq(i)])\/len(df)\n        ganho += pi * calcularEntropia(df[df[coluna].eq(i)], resultado)\n    ganho = calcularEntropia(df, resultado) - ganho\n    return ganho\n\ndef calcularIndiceDeGiniParaValor(df, resultado, valor, coluna):\n    gini = 0\n    for value in df[resultado].unique():\n        pi = len(df[df[coluna].eq(valor) & df[resultado].eq(value)])\/len(df[df[coluna].eq(valor)])\n        gini += pi * pi\n    gini = 1 - gini\n    return gini\n    \ndef calcularIndiceDeGini(df, resultado, coluna):\n    values = df[coluna].unique()\n    gini = 0\n    for i in values:\n        pi = len(df[df[coluna].eq(i)])\/len(df)\n        gini += calcularIndiceDeGiniParaValor(df, resulado, i, coluna) * pi\n    \n    return gini","e1867437":"class No:\n    def __init__(self, atributo, galhos):\n        self.atributo = atributo\n        self.galhos = galhos\n        \ndef construirArvoreDeDecisao(df, colunas, resultado, resultadoAnterior=0, funcao=calcularGanho, dfOriginal=df):\n    if(len(df[resultado]) == 0):\n        return No(resultadoAnterior, '.')\n    elif(len(df[resultado].unique()) == 1):\n        return No(df[resultado].iloc[0], '.')\n    elif(len(colunas) == 0):\n        return No(df[resultado].value_counts().index[0], '.')\n    \n    maiorGanhoDeColunas = -np.inf\n    for i in colunas:\n        atualGanho = funcao(df, resultado, i)\n        if maiorGanhoDeColunas < atualGanho:\n            colunaEscolhida = i\n            maiorGanhoDeColunas = atualGanho\n    \n    galhos = {}\n    for i in dfOriginal[colunaEscolhida].unique():\n        galhos[i] = construirArvoreDeDecisao(df[df[colunaEscolhida].eq(i)], list(set(colunas) - set([colunaEscolhida])), resultado, df[resultado].value_counts().index[0], funcao, dfOriginal)\n        \n    return No(colunaEscolhida, galhos)","05562f82":"def buscarNaArvore(arvore, individuo):\n    if(arvore.galhos == '.'):\n        return arvore.atributo\n    else:\n        return buscarNaArvore(arvore.galhos[individuo[arvore.atributo]], individuo)","74e4ddfe":"def kFoldValidation(k, df, funcao=calcularGanho):\n    resto = len(df) % k\n    resultadosExperimentos = []\n    resultadosBusca = {}\n    \n    for i in range(0, k):\n        df = df.sample(frac=1)\n        teste = df.iloc[int((i\/k)*len(df)):int(((i+1)\/k)*len(df) + 1)]\n        treinamento = df.iloc[:int((i\/k)*len(df))].append(df.iloc[int(((i+1)\/k)*len(df) + 1):])\n        arvore = construirArvoreDeDecisao(treinamento, ['UF', 'Sexo', 'Idade', 'Cor', 'Anos de Estudo'], 'Renda', funcao)\n        for r in df['Renda'].unique():\n            resultadosBusca[r] = {}\n            for f in df['Renda'].unique():\n                resultadosBusca[r][f] = 0\n        for e in teste.values:\n            v = buscarNaArvore(arvore, {'UF': e[0], 'Sexo': e[1], 'Idade': e[2], 'Cor': e[3], 'Anos de Estudo': e[4]})\n            resultadosBusca[e[5]][v] += 1\n        \n        dfAux = pd.DataFrame.from_dict(resultadosBusca)\n        dfAux = dfAux.sort_index(ascending=True).sort_index(axis=1,ascending=True)\n        dfAux['Total'] = dfAux.sum(axis=1)\n        dfAux.loc['Total'] = dfAux.sum(axis=0)\n        dfAux['Precis\u00e3o'] = np.divide(np.diag(dfAux), dfAux['Total'])\n        dfAux['Precis\u00e3o']['Total'] = np.sum(np.diag(dfAux)[:-1])\/dfAux['Total'].loc['Total']\n        dfAux['Reconhecimento'] = np.divide(np.diag(dfAux), dfAux.loc['Total'][:-1])\n        dfAux['Reconhecimento']['Total'] = np.sum(np.diag(dfAux)[:-1])\/dfAux['Total'].loc['Total']\n        resultadosExperimentos.append(dfAux.copy())\n    \n    return resultadosExperimentos\n\nkfoldResultsEntropia = kFoldValidation(5, df)\nkfoldResultsIndiceDeGini = kFoldValidation(5, df, calcularIndiceDeGini)","191b0937":"pd.DataFrame.from_dict(kfoldResultsEntropia[0])","74cf3704":"pd.DataFrame.from_dict(kfoldResultsEntropia[1])","eff34dab":"pd.DataFrame.from_dict(kfoldResultsEntropia[2])","c5dae919":"pd.DataFrame.from_dict(kfoldResultsEntropia[3])","24bc7bbf":"pd.DataFrame.from_dict(kfoldResultsEntropia[4])","fee7d1d2":"pd.DataFrame.from_dict(kfoldResultsIndiceDeGini[0])","0830b4cc":"pd.DataFrame.from_dict(kfoldResultsIndiceDeGini[1])","9aa97179":"pd.DataFrame.from_dict(kfoldResultsIndiceDeGini[2])","6273c2b5":"pd.DataFrame.from_dict(kfoldResultsIndiceDeGini[3])","f1114ff8":"pd.DataFrame.from_dict(kfoldResultsIndiceDeGini[4])","5c10044b":"sklearnDf = pd.get_dummies(df[['Sexo','UF','Cor','Idade', 'Anos de Estudo']])\nsklearnDf","21edd717":"clf = DecisionTreeClassifier(criterion=\"entropy\")\n\nclf = clf.fit(sklearnDf[:15368*4], df.Renda[:15368*4])","8a2b95bf":"def kFoldValidationScikit(k, df, criterio=\"entropy\"):\n    resto = len(df) % k\n    resultadosExperimentos = []\n    resultadosBusca = {}\n    \n    for i in range(0, k):\n        df = df.sample(frac=1)\n        teste = df.iloc[int((i\/k)*len(df)):int(((i+1)\/k)*len(df) + 1)]\n        treinamento = df.iloc[:int((i\/k)*len(df))].append(df.iloc[int(((i+1)\/k)*len(df) + 1):])\n        arvore = DecisionTreeClassifier(criterion=criterio).fit(pd.get_dummies(treinamento[['Sexo','UF','Cor','Idade', 'Anos de Estudo']]), treinamento['Renda'])\n        for r in df['Renda'].unique():\n            resultadosBusca[r] = {}\n            for f in df['Renda'].unique():\n                resultadosBusca[r][f] = 0\n        prediction = clf.predict(pd.get_dummies(teste[['Sexo','UF','Cor','Idade', 'Anos de Estudo']]))       \n        for e in range(len(prediction)):\n            resultadosBusca[teste['Renda'].iloc[e]][prediction[e]] += 1 \n        \n        dfAux = pd.DataFrame.from_dict(resultadosBusca)\n        dfAux = dfAux.sort_index(ascending=True).sort_index(axis=1,ascending=True)\n        dfAux['Total'] = dfAux.sum(axis=1)\n        dfAux.loc['Total'] = dfAux.sum(axis=0)\n        dfAux['Precis\u00e3o'] = np.divide(np.diag(dfAux), dfAux['Total'])\n        dfAux['Precis\u00e3o']['Total'] = np.sum(np.diag(dfAux)[:-1])\/dfAux['Total'].loc['Total']\n        dfAux['Reconhecimento'] = np.divide(np.diag(dfAux), dfAux.loc['Total'][:-1])\n        dfAux['Reconhecimento']['Total'] = np.sum(np.diag(dfAux)[:-1])\/dfAux['Total'].loc['Total']\n        resultadosExperimentos.append(dfAux.copy())\n    \n    return resultadosExperimentos\n\nkfoldResultsEntropia = kFoldValidationScikit(5, df, 'entropy')\nkfoldResultsIndiceDeGini = kFoldValidationScikit(5, df, 'gini')","4c3a8b7a":"pd.DataFrame.from_dict(kfoldResultsEntropia[0])","d8b05953":"pd.DataFrame.from_dict(kfoldResultsEntropia[1])","53236fe9":"pd.DataFrame.from_dict(kfoldResultsEntropia[2])","0b6aae2f":"pd.DataFrame.from_dict(kfoldResultsEntropia[3])","edd911b3":"pd.DataFrame.from_dict(kfoldResultsEntropia[4])","fdb0c434":"pd.DataFrame.from_dict(kfoldResultsIndiceDeGini[0])","0024fe42":"pd.DataFrame.from_dict(kfoldResultsIndiceDeGini[1])","091deca0":"pd.DataFrame.from_dict(kfoldResultsIndiceDeGini[2])","2d5b0520":"pd.DataFrame.from_dict(kfoldResultsIndiceDeGini[3])","3499c231":"pd.DataFrame.from_dict(kfoldResultsIndiceDeGini[4])","24e75b0a":"path = clf.cost_complexity_pruning_path(pd.get_dummies(df[['Sexo','UF','Cor','Idade', 'Anos de Estudo']]), df['Renda']) \nalphas = path['ccp_alphas']","2c2f5b9f":"accuracy_train, accuracy_test = [], []\nfor i in alphas[alphas>0]:\n    i = i if i >= 0 else i*-1\n    \n    tree = DecisionTreeClassifier(ccp_alpha=i)\n    \n    tree.fit(sklearnDf[:15368*4], df.Renda[:15368*4])\n    y_train_pred = tree.predict(sklearnDf[:15368*4])\n    y_test_pred = tree.predict(sklearnDf[15368:])\n    \n    accuracy_train.append(metrics.accuracy_score(df.Renda[:15368*4], y_train_pred))\n    accuracy_test.append(metrics.accuracy_score(df.Renda[15368:], y_test_pred))","60961d53":"sns.set()\nplt.figure(figsize=(14,7))\nsns.lineplot(y=accuracy_train, x=alphas[alphas>0])\nsns.lineplot(y=accuracy_test, x=alphas[alphas>0])\nplt.xticks(ticks=np.arange(0.00, 0.25, 0.01))\nplt.show()","b5a801ac":"# Resultados Entropia\n\nResultados dos experimento usando a fun\u00e7\u00e3o de ganho como par\u00e2metro para escolher as colunas","49ae670a":"# Pesquisa Nacional por Amostra de Dom\u00edcilios - 2015\nA Pesquisa Nacional por Amostra de Domic\u00edlios - PNAD investiga anualmente, de forma permanente, caracter\u00edsticas gerais da popula\u00e7\u00e3o, de educa\u00e7\u00e3o, trabalho, rendimento e habita\u00e7\u00e3o e outras, com periodicidade vari\u00e1vel, de acordo com as necessidades de informa\u00e7\u00e3o para o pa\u00eds, como as caracter\u00edsticas sobre migra\u00e7\u00e3o, fecundidade, nupcialidade, sa\u00fade, seguran\u00e7a alimentar, entre outros temas. O levantamento dessas estat\u00edsticas constitui, ao longo dos 49 anos de realiza\u00e7\u00e3o da pesquisa, um importante instrumento para formula\u00e7\u00e3o, valida\u00e7\u00e3o e avalia\u00e7\u00e3o de pol\u00edticas orientadas para o desenvolvimento socioecon\u00f4mico e a melhoria das condi\u00e7\u00f5es de vida no Brasil.\n\n---\n\nPara trabalhar com a \u00c1rvore de Decis\u00e3o, os dados num\u00e9ricos que se caracterizam como cont\u00ednuos ser\u00e3o discretizados, e dados que j\u00e1 s\u00e3o categorizados s\u00e3o substituidos exatamente pela categoria que eles representam, por exemplo, no dataset original a coluna 'Sexo', pode assumir os valores 1 e 0, sendo 1 -> Masculino e 0 -> Feminino, dessa forma esses valores ser\u00e3o substitu\u00eddos, pelas strings que, de fato, definem tal categoria.\n\n---\n\nOs grupos de renda ser\u00e3o categorizados pelos grupos de classes sociais do IBGE:<br>\nE: renda menor do que 1 salario m\u00ednimo <br>\nD: renda entre 1 salario m\u00ednimo e 3 sal\u00e1rios m\u00ednimos <br>\nC: renda entre 3 sal\u00e1rios m\u00ednimos e 5 sal\u00e1rios m\u00ednimos <br>\nB: renda entre 5 sal\u00e1rios m\u00ednimos e 15 sal\u00e1rios m\u00ednimos <br>\nA: renda maior do que 15 sal\u00e1rios m\u00ednimos <br>","b0451ccd":"# Buscar na \u00c0rvore\n\nA fun\u00e7\u00e3o de buscar na \u00e1rvore, desce pelos n\u00f3 da \u00e1rvore at\u00e9 encontrar o valor '.' na vari\u00e1vel galhos, o que indica o final da \u00e1rvore, tal busca \u00e9 realizada de maneira recursiva.\n\n---\n\nA entrada para a fun\u00e7\u00e3o s\u00e3o a \u00e1rvore e uma vari\u00e1vel indiv\u00edduo, que \u00e9 um dicion\u00e1rio cujas chaves s\u00e3o as colunas (com exece\u00e7\u00e3o da coluna resultado), e os valores s\u00e3o os valores que determinado indiv\u00edduo possui para aquelas colunas.","441b9613":"# Resultados Indice de Gini\n\nResultados dos experimento usando o Indice de Gini como par\u00e2metro para escolher as colunas","963c8bec":"A seguir ser\u00e1 construido o dataframe para trabalhar com o m\u00e9todo de \u00c1rvore de decis\u00e3o da biblioteca sklearn","719c072a":"# Aplicando o post-prunning \n\nAs seguinte c\u00e9lulas, aplicam o m\u00e9todo da biblioteca sklearn na \u00e1rvore construida de cost_complexity_prunning_path, que nos retorna uma lista de valores de 0 a 1, tais valores que representam o quanto da \u00e1rvore deve ser cortado para um melhor desempenho","08042bc9":"# A \u00e1rvore de decis\u00e3o\n\nA classe n\u00f3 abaixo \u00e9 a defini\u00e7\u00e3o da arv\u00f3re, sendo a vari\u00e1vel 'atributo' o atributo do dataset que representa aquele espec\u00edfico N\u00f3, no caso do dataset em quest\u00e3o os poss\u00edveis atributos de um N\u00f3 s\u00e3o: ('Anos de Estudo', 'Sexo', 'Idade', 'Cor' e 'UF'), todo o n\u00f3 final de uma arv\u00f3res ter\u00e1 como atributo um determinado valor da coluna que se procura prever, no caso do exemplo os atributos no ultimo n\u00f3 de uma \u00e1rvore podem ser ('A', 'B', 'C', 'D' e 'E') e a v\u00e1riavel galho assumir\u00e1 a string '.'<br>\n\nA vari\u00e1vel galhos da classe N\u00f3, representa os pr\u00f3ximos n\u00edveis da \u00e1rvore a partir do corrente n\u00f3, a vari\u00e1vel \u00e9 um dicion\u00e1rio, cuja as chaves s\u00e3o os poss\u00edveis valores do atributo daquele n\u00f3, e os valores de cada uma das chaves do dicion\u00e1rio 'galhos', s\u00e3o os n\u00f3s do seguinte n\u00edvel da \u00e1rvores.\nComo exemplo uma vari\u00e1vel galhos de um N\u00f3, cujo atributo \u00e9 'Sexo', ter\u00e1 como valor:<br>\n`{\n    \"Masculino\": Proximo N\u00f3,\n    \"Feminino\": Proximo N\u00f3\n}`\n\n---\nO algoritmo de implementa\u00e7\u00e3o da \u00c1rvore de Decis\u00e3o, usa a vari\u00e1vel funcao (definidas anteriormente, calcularGanho() e calcularIndiceDeGini()) para decidir qual a melhor coluna para caracterizar o primeiro n\u00f3 da \u00e1rvore, e constroi os seguintes n\u00f3s a partir desse de forma recursiva com os crit\u00e9rios de parada sendo, o dataset se torna vazio, o dataframe ter somente um valor da coluna de resultado (no caso do exemplo, 'Renda'), e n\u00e3o existirem mais colunas para serem avalidas, desse forma a \u00e1rvore retornar\u00e1 o n\u00f3 inicial que por sua vez referencia todos os outros.","3087db73":"# Resultados Entropia\n\nResultados dos experimento usando a fun\u00e7\u00e3o de ganho como par\u00e2metro para escolher as colunas","4d0d5c4e":"# K-Fold Validation\n\nNa seguinte c\u00e9lula s\u00e3o realizados 10 experimentos seguindo o 5-fold validation, s\u00e3o 5 experimentos de k-fold validation com k igual a 5 usando como fun\u00e7\u00e3o o c\u00e1lculo de ganho e outros 5 usando como fun\u00e7\u00e3o o \u00edndice de gini.\n\n---\n\nO resultado de cada um dos experimentos \u00e9 uma tabelas com linhas ['A', 'B', 'C', 'D', 'E', 'Total'] e colunas ['A', 'B', 'C', 'D', 'E', 'Total', 'Precis\u00e3o', 'Reconhecimento'].<br>\nAs colunas referentes as classes e a coluna 'Total' representam todos os indiv\u00edduos do dataset que foram classificados daquela maneira, enquanto as linhas referentes as classes e a linha 'Total' representam todas os indiv\u00edduos do dataset, que foram classificados daquela maneira pela \u00c1rvore de Decis\u00e3o. Como exemplo, o valor na coluna 'B' e linha 'C', representam todos os indivuos que s\u00e3o realmente da classe social 'B', mas que foram classificados como 'C' pela \u00c1rvore de Decis\u00e3o. <br>\n\nA coluna 'Precis\u00e3o' do dataframe na linha 'A', \u00e9 a propor\u00e7\u00e3o de todos os indiv\u00edduos que o algoritmo corretamente classificou como 'A' pela quantidade de indiv\u00edduos classificados, pelo algoritmo como A.<br>\nA coluna 'Reconhecimento' do dataframe na linha 'A', \u00e9 a propor\u00e7\u00e3o de todos os indiv\u00edduos que o algoritmo corretamente classificou como 'A' pela quantidade de indiv\u00edduos que s\u00e3o realmente da classe social 'A'.\n","65b23176":"# Entropia e Indice de Gini\n\nS\u00e3o definidos as fun\u00e7\u00f5es para c\u00e1culo de entropia e ganho de determinada que ser\u00e3o usados na constru\u00e7\u00e3o da \u00c1rvore de Decis\u00e3o.","7d9d54ff":"# Desempenho do Post-Pruning\nO seguinte gr\u00e1fico nos mostra o desempenho do post-pruning para os exemplos de treino e de teste, o gr\u00e1fico \u00e9 um plot da Acur\u00e1ciaXalpha de corte da \u00c1rvore.<br>\nPelo gr\u00e1fico podemos perceber que o melhor desempenho do algoritmo aconteceu, quando o nosso valor para o alpha da \u00e1rvore era igual a 0, ou seja, uma poss\u00edvel poda na \u00c1rvore pioraria o desempenho das suas previs\u00f5es","17f0eae2":"# K-Fold Validation Scikit\nAqui temos a implementa\u00e7\u00e3o do m\u00e9todo k-fold validation, como no exemplo anterior, com a exce\u00e7\u00e3o de que, dessa vez usamos o m\u00e9todo com a \u00e1vore construida pela biblioteca do scikit learn.\n\n---\n\nApesar de usarmos um diferente m\u00e9todo de constru\u00e7\u00e3o de \u00e1rvore o retorno dos experimentos \u00e9 o mesma tabela.","b2b420f2":"Depois de construida a lista de alphas realizamos o teste para cada um deles com as \u00e1rvores diferentes \u00e1rvores para cada, na c\u00e9lula seguinte","18de5358":"# Resultados Indice de Gini\n\nResultados dos experimento usando o Indice de Gini como par\u00e2metro para escolher as colunas"}}