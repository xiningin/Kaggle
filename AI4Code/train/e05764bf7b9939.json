{"cell_type":{"f4030ff3":"code","395a0632":"code","097d51a4":"code","b43e275a":"code","af457b80":"code","0f3738ba":"code","dd823ade":"code","0ce2ab26":"code","c5fbddd2":"code","2648d917":"code","796fce98":"code","d7f58e2e":"code","7c4d800c":"code","97e933e7":"code","92761f63":"code","7af52c56":"code","1b4eab64":"code","80943b3f":"code","a8762207":"code","1b68243d":"code","c7f70b41":"code","358d1edb":"code","80975212":"code","c4c7b677":"code","dff84514":"code","f98bd1f5":"markdown","08d99b33":"markdown","f1800595":"markdown"},"source":{"f4030ff3":"import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.metrics import classification_report\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn.impute import SimpleImputer\nimport xgboost as xgb\nfrom sklearn.metrics import f1_score, recall_score\nfrom sklearn.model_selection import GridSearchCV","395a0632":"#import train data\ndf = pd.read_csv('\/kaggle\/input\/iba-ml1-mid-project\/train.csv')\ndf.head()","097d51a4":"#drop first column and replace dataframe\ndf.drop(['Id'], axis=1, inplace = True)","b43e275a":"#distinct values of column\nprint(df['credit_line_utilization'].unique())\n\n#counts of non-null values\ndf['credit_line_utilization'].value_counts()","af457b80":"#firstly, replace ',' with '.'\ndf['credit_line_utilization'] = df['credit_line_utilization'].str.replace(',','.')\n\n#convert object to float\ndf['credit_line_utilization'] = df['credit_line_utilization'].astype(float)","0f3738ba":"#independent columns\nX = df.iloc[:,:-1]\n\n#target\ny = df['defaulted_on_loan']","dd823ade":"#pipeline\ntransform = Pipeline(steps=[ \n    ('impute', SimpleImputer()),\n    ('polynomial_features', PolynomialFeatures(degree=2, include_bias=False)), \n    ('outlier', RobustScaler(with_centering=False)),\n    ('scaler', StandardScaler()),\n    ('LDA', LinearDiscriminantAnalysis())])\n\nmodel_pipeline = Pipeline(steps=[ ('preprocessing', transform),\n                                 ('Classifier', KNeighborsClassifier())])","0ce2ab26":"X_train, X_test, y_train, y_test = train_test_split(X,y, random_state=42)","c5fbddd2":"#accuracy score of model\nmodel_pipeline.fit(X_train, y_train)\nprint(accuracy_score(y_train, model_pipeline.predict(X_train)))\nprint(accuracy_score(y_test, model_pipeline.predict(X_test)))","2648d917":"#hyperparameters for tuning\nfor param in model_pipeline.get_params():\n    print(param)","796fce98":"param_space = {'Classifier__n_neighbors': [8, 10, 6, 15]}","d7f58e2e":"gridsearch = GridSearchCV(model_pipeline, param_space, cv=3)\ngridsearch.fit(X_train, y_train)\nprint(accuracy_score(y_train, gridsearch.best_estimator_.predict(X_train)))\nprint(accuracy_score(y_test, gridsearch.best_estimator_.predict(X_test)))","7c4d800c":"gridsearch.best_params_","97e933e7":"print(classification_report(y_test, gridsearch.best_estimator_.predict(X_test)))","92761f63":"c_matrix = confusion_matrix(y_test, gridsearch.best_estimator_.predict(X_test))\nc_matrix","7af52c56":"fig = sns.heatmap(c_matrix, annot=True, fmt='d', cmap=\"Blues\")\nfig.set_ylim([0,2])\nplt.show()","1b4eab64":"#counts of target values\ny_test.value_counts()","80943b3f":"model_pipeline = Pipeline(steps=[ ('preprocessing', transform),\n                                  ('polynomial_features', PolynomialFeatures(degree=2, include_bias=False)), \n                                  ('model', LogisticRegression(solver='liblinear')) ])","a8762207":"model_pipeline.fit(X_train, y_train)\nprint(accuracy_score(y_train, model_pipeline.predict(X_train)))\nprint(accuracy_score(y_test, model_pipeline.predict(X_test)))","1b68243d":"model_pipeline = Pipeline(steps=[ ('preprocessing', transform),\n                                  ('polynomial_features', PolynomialFeatures(degree=2, include_bias=False)), \n                                  ('model', GaussianNB()) ])","c7f70b41":"model_pipeline.fit(X_train, y_train)\nprint(accuracy_score(y_train, model_pipeline.predict(X_train)))\nprint(accuracy_score(y_test, model_pipeline.predict(X_test)))","358d1edb":"model_pipeline = Pipeline(steps=[ ('preprocessing', transform),\n                                  ('polynomial_features', PolynomialFeatures(degree=2, include_bias=False)), \n                                  ('model', DecisionTreeClassifier(max_depth=10, random_state=42, min_samples_leaf=15)) ])","80975212":"model_pipeline.fit(X_train, y_train)\nprint(accuracy_score(y_train, model_pipeline.predict(X_train)))\nprint(accuracy_score(y_test, model_pipeline.predict(X_test)))","c4c7b677":"model_pipeline = Pipeline(steps=[ ('preprocessing', transform),\n                                  ('polynomial_features', PolynomialFeatures(degree=2, include_bias=False)), \n                                  ('model', KNeighborsClassifier(n_neighbors=15)) ])","dff84514":"model_pipeline.fit(X_train, y_train)\nprint(accuracy_score(y_train, model_pipeline.predict(X_train)))\nprint(accuracy_score(y_test, model_pipeline.predict(X_test)))","f98bd1f5":"# 2. Model interpretation","08d99b33":"True Positives = 16634 - the number of positive examples that the model correctly classified as positive\nFalse Positives = 179 - the number of negative examples that the model incorrectly classified as positive\nFalse Negatives = 1094 - the number of positive examples that the model incorrectly classified as negative\nTrue Negatives = 134 - the number of negative examples that the model correctly classified as negative","f1800595":"### different classification models - Logistic Regression, Naive Bayes, Decision tree and K-nearest neighbors models"}}