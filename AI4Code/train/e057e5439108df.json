{"cell_type":{"d4c2f7c2":"code","916224e3":"code","a0bf6a0d":"code","8143bcdd":"code","44752648":"code","dbea3164":"code","84bd0d8f":"code","d58ebea4":"code","7cc93fa8":"code","67dfeb00":"code","0b286a41":"code","c1b99e7e":"code","71ef3842":"code","5c66e0e4":"code","483f0000":"code","948fb428":"markdown","36ea94bd":"markdown","ec4fd1f8":"markdown","3bb4968f":"markdown","9ab41381":"markdown","82c3deb3":"markdown","f7a367ef":"markdown","88b33da0":"markdown","aa453885":"markdown","c21a2a2f":"markdown","dfdc4f4f":"markdown","6645c69f":"markdown","89fba882":"markdown","a864e03f":"markdown","a8be97b6":"markdown"},"source":{"d4c2f7c2":"#importing libraries\nimport pandas as pd\nimport numpy as np\n\nimport warnings \nwarnings.filterwarnings('ignore')\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","916224e3":"# Loading Dataset\ntrain=pd.read_csv('..\/input\/titanic\/train.csv')\ntest=pd.read_csv('..\/input\/titanic\/test.csv')\ndataset=[train,test]\ntrain.info()","a0bf6a0d":"# missing values graph\n# for training\nfig = plt.figure(figsize=(12,6))\n\nplt.subplot(121)   #  subplot 1 - female\nplt.title('training datset')\nsns.heatmap(train.isnull(),yticklabels=False,cmap='viridis')#, annot=True, fmt='.2f', square=True, cmap = 'Reds_r')\n\nplt.subplot(122)   #  subplot 2 - male\nplt.title('testing datset')\nsns.heatmap(test.isnull(),yticklabels=False,cmap='viridis' )#,annot=True, fmt='.2f', square=True, cmap = 'Blues_r')\n\nplt.show()","8143bcdd":"# visualize the number of male and female survived or not\n\nsns.countplot(data=train,x='Sex',hue='Survived')","44752648":"## Count of number of family memebers\n#sns.countplot(x = 'Pclass',hue='Sex', data = train,palette='PuBuGn')\nsns.catplot(x=\"Pclass\", hue=\"Sex\", col=\"Survived\",\n                data=train, kind=\"count\",\n                height=4, aspect=.7, palette = 'PuBu');","dbea3164":"sns.boxplot(x='Sex', y='Age', hue = 'Survived',data=train);","84bd0d8f":"sns.countplot(data=train,x='Embarked',hue='Survived')","d58ebea4":"for data in dataset:\n    data['Family']=data['SibSp']+data['Parch']+1\n    \n# Drop columns SibSp and Parch\nfor data in dataset:\n    data.drop(columns=['SibSp','Parch'],inplace=True,axis=1)\n\n# Column Name Ticket and Cabin is not necessary to predict whether passanger will survive or not, So Drop column Name and Ticket and Cabin\nfor data in dataset:\n    data.drop(columns=['Name','Ticket','Cabin',],axis=1,inplace=True)\n    \n# manipulating Fare Column\nfor data in dataset:\n    data.loc[ data['Fare'] <= 7.91, 'Fare'] = 0\n    data.loc[(data['Fare'] > 7.91) & (data['Fare'] <= 14.454), 'Fare'] = 1\n    data.loc[(data['Fare'] > 14.454) & (data['Fare'] <= 31), 'Fare']   = 2\n    data.loc[(data['Fare'] > 31) & (data['Fare'] <= 99), 'Fare']   = 3\n    data.loc[(data['Fare'] > 99) & (data['Fare'] <= 250), 'Fare']   = 4\n    data.loc[ data['Fare'] > 250, 'Fare'] = 5\n    \n","7cc93fa8":"train['Family'].value_counts()","67dfeb00":"sns.barplot(x=\"Family\", y=\"Survived\", data=train)\nplt.show;","0b286a41":"# Convert Categorical Values to Numeric Value \nprint(data['Embarked'].value_counts())\nfor data in dataset:\n    data['Sex']=data['Sex'].map({'female':0,'male':1})\n    data['Embarked']=data['Embarked'].map({'S': 0, 'C': 1, 'Q': 2})","c1b99e7e":"# information about training and testing dataset\nfor data in dataset:\n    data.info()\n    print('========================================')","71ef3842":"# Finding the null columns in train and test datasets\ntrain_null_cols=train.columns[train.isna().any()].to_list()\ntest_null_cols=test.columns[test.isna().any()].to_list()\nprint('train_null_cols : ',train_null_cols)\nprint('test_null_cols : ',test_null_cols)","5c66e0e4":"# imputation of age in traing dataset by using mean\nfor data in dataset:\n    data['Age'].fillna(data['Age'].mean(),inplace=True)\n    \n#imputing Embarked column in traing dataset\ntrain['Embarked'].fillna(train['Embarked'].mode()[0],inplace=True)\n\n#imputing Fare column in testing dataset\ntest['Fare'].fillna(test['Fare'].mode()[0],inplace=True)\n\n#Check After Imputation\nfor data in dataset:\n    data=data.astype(int)\n    data.info()\n    print('========================================')","483f0000":"X_train=train.drop(columns='Survived',axis=1)\nY_train=train['Survived']\n\nfrom xgboost import XGBClassifier\n\nxgb = XGBClassifier(objective='binary:logistic',booster = 'gbtree',eval_metric='logloss', gamma=5,learning_rate = 0.1, max_depth = 5, n_estimators = 100,colsample_bytree=1)\nxgb.fit(X_train, Y_train)\npredictions_xgb=xgb.predict(test)\n\n#feature importance graph\nfrom xgboost import plot_importance\nplot_importance(xgb)\nplt.show()\n\nSurvived=pd.Series(predictions_xgb,name='Survived')\nans=pd.concat([test['PassengerId'],Survived],axis=1)\nans.to_csv('XGB_Ans.csv',index=False)","948fb428":"# **Data Visualization**","36ea94bd":"Let's check the distribution of age and sex with Survival Count","ec4fd1f8":"> We can observe from above feature importance graph that PClass has highest importance.","3bb4968f":"> From above figure , we can see that people","9ab41381":"> Now let's see the catplot which will give detailed distribution of the survival rate between different passenger classes on the Titanic for men and women.","82c3deb3":"# **Applying XGBoost Classifier Model**","f7a367ef":"It shows that most of the women are survived as compared to men","88b33da0":"# **Data Cleaning**","aa453885":"> SibSp and Parch Means number of sibling and Parents\/children respectively. So to get family size we can do operation as adding them to 1. So we will get family size.","c21a2a2f":"# **Imputing the missing values**","dfdc4f4f":"# **Do upvote if you find it useful!**","6645c69f":"> From above graph, we can see that \n> 1. in training dataset, Column **Age, Cabin** and **Embarked** have null values \n> 2. in training dataset, Column **Age**, and **Cabin**  have null values ","89fba882":" We can see that the Average age for both men and women is nearly about 30","a864e03f":"**Distribution of Embarkation Port**","a8be97b6":"#  **Welcome to my notebook**\n\nSteps Involved:\n*     Data Cleaning & Data Manipluation on Train & Test Dataset & Imputing the Missing Values.\n*     Using XGBoost Model with HyperParameter Tuning\n*     Predicting the Class Survived"}}