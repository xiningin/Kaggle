{"cell_type":{"a302479f":"code","5304d696":"code","eaf3606e":"code","8ee7d2a3":"code","f989d08e":"code","00e6f516":"code","305665a9":"code","1f96fa05":"code","0ab453d7":"code","7231c3c9":"code","fe93a029":"code","d6352d58":"code","6efead13":"code","1788ccd9":"code","7acb16ed":"code","b46ba8b1":"code","8c3c0720":"code","6953ebf1":"code","c168f590":"code","483619f7":"code","74631e15":"markdown","9eff2888":"markdown","d7cc4f1e":"markdown","0420e73a":"markdown","300ea855":"markdown","f589b986":"markdown","c584ac2c":"markdown","b7b4f655":"markdown","550a2c02":"markdown","4b811b66":"markdown","f12a63d2":"markdown","8c809b2e":"markdown","83cfb38b":"markdown","c3c75549":"markdown","cbec18b0":"markdown","f694aa86":"markdown","e58b93fb":"markdown","d595994f":"markdown","34ae1cd0":"markdown","75c31d77":"markdown","4eabf7a8":"markdown","cca42452":"markdown"},"source":{"a302479f":"import pandas as pd\npd.set_option('display.max_columns', None)\npd.options.display.float_format = \"{:.2f}\".format\n\nimport numpy as np\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom tqdm import tqdm","5304d696":"data=pd.read_csv('\/kaggle\/input\/wine-dataset-for-clustering\/wine-clustering.csv')","eaf3606e":"data.head()","8ee7d2a3":"data.info()","f989d08e":"data.describe()","00e6f516":"data.skew()","305665a9":"sns.set(style='white',font_scale=1.3, rc={'figure.figsize':(20,20)})\nax=data.hist(bins=20,color='red' )","1f96fa05":"data.plot( kind = 'box', subplots = True, layout = (4,4), sharex = False, sharey = False,color='black')\nplt.show()","0ab453d7":"data.isnull().sum().sort_values(ascending=False).head()","7231c3c9":"from sklearn.preprocessing import StandardScaler\n\nstd_scaler = StandardScaler()\ndata_cluster=data.copy()\ndata_cluster[data_cluster.columns]=std_scaler.fit_transform(data_cluster)","fe93a029":"data_cluster.describe()","d6352d58":"from sklearn.decomposition import PCA\npca_2 = PCA(2)\npca_2_result = pca_2.fit_transform(data_cluster)\n\nprint ('Cumulative variance explained by 2 principal components: {:.2%}'.format(np.sum(pca_2.explained_variance_ratio_)))","6efead13":"sns.set(style='white', rc={'figure.figsize':(9,6)},font_scale=1.1)\n\nplt.scatter(x=pca_2_result[:, 0], y=pca_2_result[:, 1], color='red',lw=0.1)\nplt.xlabel('Principal Component 1')\nplt.ylabel('Principal Component 2')\nplt.title('Data represented by the 2 strongest principal components',fontweight='bold')\nplt.show()","1788ccd9":"import sklearn.cluster as cluster\n\ninertia = []\nfor i in tqdm(range(2,10)):\n    kmeans = cluster.KMeans(n_clusters=i,\n               init='k-means++',\n               n_init=15,\n               max_iter=500,\n               random_state=17)\n    kmeans.fit(data_cluster)\n    inertia.append(kmeans.inertia_)","7acb16ed":"from sklearn.metrics import silhouette_score\n\nsilhouette = {}\nfor i in tqdm(range(2,10)):\n    kmeans = cluster.KMeans(n_clusters=i,\n               init='k-means++',\n               n_init=15,\n               max_iter=500,\n               random_state=17)\n    kmeans.fit(data_cluster)\n    silhouette[i] = silhouette_score(data_cluster, kmeans.labels_, metric='euclidean')","b46ba8b1":"sns.set(style='white',font_scale=1.1, rc={'figure.figsize':(12,5)})\n\nplt.subplot(1, 2, 1)\n\nplt.plot(range(2,len(inertia)+2), inertia, marker='o',lw=2,ms=8,color='red')\nplt.xlabel('Number of clusters')\nplt.title('K-means Inertia',fontweight='bold')\nplt.grid(True)\n\nplt.subplot(1, 2, 2)\n\nplt.bar(range(len(silhouette)), list(silhouette.values()), align='center',color= 'red',width=0.5)\nplt.xticks(range(len(silhouette)), list(silhouette.keys()))\nplt.grid()\nplt.title('Silhouette Score',fontweight='bold')\nplt.xlabel('Number of Clusters')\n\n\nplt.show()","8c3c0720":"kmeans = cluster.KMeans(n_clusters=3,random_state=17,init='k-means++')\nkmeans_labels = kmeans.fit_predict(data_cluster)\n\ncentroids = kmeans.cluster_centers_\ncentroids_pca = pca_2.transform(centroids)\n\npd.Series(kmeans_labels).value_counts()","6953ebf1":"data2=data.copy()\ndata2['Cluster']=kmeans_labels\n\naux=data2.columns.tolist()\naux[0:len(aux)-1]\n\nfor cluster in aux[0:len(aux)-1]:\n    grid= sns.FacetGrid(data2, col='Cluster')\n    grid.map(plt.hist, cluster,color='red')","c168f590":"centroids_data=pd.DataFrame(data=std_scaler.inverse_transform(centroids), columns=data.columns)\ncentroids_data.head()","483619f7":"sns.set(style='white', rc={'figure.figsize':(9,6)},font_scale=1.1)\n\nplt.scatter(x=pca_2_result[:, 0], y=pca_2_result[:, 1], c=kmeans_labels, cmap='autumn')\nplt.scatter(centroids_pca[:, 0], centroids_pca[:, 1],\n            marker='x', s=169, linewidths=3,\n            color='black', zorder=10,lw=3)\nplt.xlabel('Principal Component 1')\nplt.ylabel('Principal Component 2')\nplt.title('Clustered Data (PCA visualization)',fontweight='bold')\nplt.show()","74631e15":"#### We are going to use a K-means algorithm, as it uses the distance as the principal metric to alocate the data in your respective cluster we need to be careful with scale, because we can give more \"relevance\" to large scale features and despite the low scale ones\n#### To prevent that, we can use lot of Scaling methods, in this case i`m going to Satandardize the data: to have a 0 mean and unit variance","9eff2888":"#### As we can see, in K=3 all the metrics indicates that it is the best clusters number. So, we'll be using it","d7cc4f1e":"#### Checking if the Standardization was made correctly, looking for mean=0 and std=1","0420e73a":"## Model Implementation","300ea855":"## Wine data Segmentation\n### A explainable Clustering Analysis\n\n### Content of this Kernel:\n\n* Exploratory Data Analysis with some Data Visualization\n* Data Preprocessing\n* K-Means Clustering with K selection techniques\n* Visualization of Clusters using PCA","f589b986":"#### To reinforce our insights about the data symmetry and their outliers, we can da plot some boxplots:\n#### \"A box plot is a method for graphically depicting groups of numerical data through their quartiles. The box extends from the Q1 to Q3 quartile values of the data, with a line at the median (Q2). The whiskers extend from the edges of box to show the range of the data. The position of the whiskers is set by default to 1.5*IQR (IQR = Q3 - Q1) from the edges of the box. Outlier points are those past the end of the whiskers.\"","c584ac2c":"#### We are using the K-means algorithm, to choose K (number of clusters) were combined two techniques: the Silhouette Score and K-means Inertia (with Elbow analysis)","b7b4f655":"## Importings\n#### Loading all necessary libraries to start the analysis","550a2c02":"## Data Preprocessing","4b811b66":"#### First, we will compute all inertias. Basically, the lower the Inertia the better the clustering ","f12a63d2":"#### Another approach, is looking each cluster centroid to define the cluster characteristics","8c809b2e":"#### Plotting the histogram of each _numerical_ variable (in this case, all features), the main idea here is to visualize the data distribution for each feature. This method can bring fast insights as:\n* Check the kind of each feature distribution\n* Check data symmetry\n* Verify features frequency\n* Identify outliers","83cfb38b":"#### Checking the data head, as we can see all the data is numerical: there are no categorical values","c3c75549":"#### Checking the skewness of our dataset. \n* A normally distribuited data has a skewness close to zero. \n* Skewness greather than zero means that there is more weight in the left side of the data.\n* In another hand, skewness smaller than 0 means that there is more weight in the right side of the data\n\n<img src=\"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/f\/f8\/Negative_and_positive_skew_diagrams_%28English%29.svg\/446px-Negative_and_positive_skew_diagrams_%28English%29.svg.png\">","cbec18b0":"#### As we are dealing with a multi-dimensional dataset, I`ll be using a Principal Component Analysis to reduce it`s dimension and make it \"plottable\" in a cartesian plane. Remebering that we will probably lose some information\/variance in this process but it is just for viualization purposes\n","f694aa86":"## PCA Clusters Visualization","e58b93fb":"#### Checking if there are null values, as we can see our dataset hasn`t null values","d595994f":"## Exploratory Data Analysis (EDA)","34ae1cd0":"#### Here, we can visualize each feature distribution according to each cluster, in this step we can define some characteristics for each group","75c31d77":"#### Reading and transforming the dataset in a pandas DataFrame from a csv file","4eabf7a8":"#### Next, we will compute the silhouette score. Here, the bigger score the better the clustering ","cca42452":"#### Looking for some statistical information about each feature, we can see that the features have very diferrent scales"}}