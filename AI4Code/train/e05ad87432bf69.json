{"cell_type":{"81e381fc":"code","aecd6db9":"code","9195b650":"code","a138e12f":"code","45d6ebba":"code","324e66f6":"code","c774e320":"code","eb66420b":"code","8b0a980e":"code","420c89d3":"code","b3696a48":"code","b26a3112":"code","2582c12e":"code","4fea327b":"code","0b50d9a9":"code","76478f56":"code","62a52bfc":"code","43283b96":"code","c3838b20":"code","e74bd14d":"code","8b76cfe9":"code","0a46ddb1":"code","0822c1ca":"code","08e35eeb":"code","d1d145b1":"code","7a81c76f":"code","ae5c38c3":"code","7d14fb51":"code","e586cd46":"code","b890e92d":"code","213df7c6":"code","1f67b8cd":"code","802df996":"code","4eb5bbcb":"code","38f16505":"code","71ef9f70":"code","d41da19e":"code","7acd7f18":"code","aa484e18":"code","9e22b3c2":"code","7cab8005":"code","6d1bcb66":"code","69ea39c0":"code","1412f7e1":"code","812e5d9f":"code","3b977390":"code","54944c43":"code","d129c512":"code","5016d3b2":"code","9eb13faa":"code","6c3ee4a3":"code","5a476f3e":"code","2ea31691":"code","90654f53":"markdown","5206aad5":"markdown","5aa2f197":"markdown","86767a02":"markdown","5ddc17a5":"markdown","6fedf4f8":"markdown","1b464e29":"markdown","626a133a":"markdown","678aa21e":"markdown","0a3f8cf9":"markdown","e1e7266b":"markdown","55434bb5":"markdown","10383375":"markdown","7954df31":"markdown","3cb9bd7e":"markdown","1d702808":"markdown","63a52aed":"markdown","38ce73f8":"markdown","68cb7311":"markdown","ac6f0a45":"markdown","2a665841":"markdown","46fd95f8":"markdown","edec039b":"markdown","b0fc597b":"markdown","bbf089fc":"markdown","f444d5f8":"markdown","bfd0b872":"markdown","69eacd32":"markdown","86ca6bd8":"markdown","18d3566b":"markdown","a4ff36c0":"markdown","004fdd57":"markdown","56a96f17":"markdown","93469932":"markdown","75f3076e":"markdown"},"source":{"81e381fc":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt # basic visualisation library\nimport seaborn as sns # advanced and nice visualisations\nimport warnings # library to manage warnings\nfrom wordcloud import WordCloud, STOPWORDS # library to create a wordcloud\n\nwarnings.simplefilter(action='ignore', category=FutureWarning) # silencing FutureWarnings out\nwarnings.simplefilter(action='ignore', category=UserWarning)\n\n# printing out version of libraries we use\nprint(\"numpy version: {}\".format(np.__version__))\nprint(\"pandas version: {}\".format(pd.__version__))\nprint(\"seaborn version: {}\".format(sns.__version__))","aecd6db9":"train_data = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/train.csv')\ntest_data = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/test.csv')\n\nprint('There are {} rows and {} columns in a training dataset'.format(train_data.shape[0],train_data.shape[1]))\nprint('and {} rows and {} columns in a testing dataset.'.format(test_data.shape[0],test_data.shape[1]))","9195b650":"train_data.head()","a138e12f":"for i in np.arange(0,10,1):\n    print(i, train_data.text[i])","45d6ebba":"target_counts = train_data['target'].value_counts().div(len(train_data)).mul(100) # calculating percentages of target values\n\nax = sns.barplot(target_counts.index, target_counts.values)\nax.set_xlabel('Target variable')\nax.set_ylabel('Percentage [%]')\nax.set_xticklabels(['False','True'])\n\nax.set_title('Training dataset', fontsize=13)\nplt.show()","324e66f6":"missing_cols = ['keyword', 'location']\n\nfig, ax = plt.subplots(1,2, figsize=(12, 5))\n# calculating percent of missing data in each column\ntrain_nans = train_data[missing_cols].isnull().sum()\/len(train_data)*100  \ntest_nans = test_data[missing_cols].isnull().sum()\/len(test_data)*100\n# creating a barplot\nsns.barplot(x=train_nans.index, y=train_nans.values, ax=ax[0])  \nsns.barplot(x=test_nans.index, y=test_nans.values, ax=ax[1])\n\nax[0].set_ylabel('Missing Values [%]', size=15, labelpad=20)\nax[0].set_yticks(np.arange(0,40,5))\nax[0].set_ylim((0,35))\n\nax[0].set_title('Training dataset', fontsize=13)\nax[1].set_title('Testing dataset', fontsize=13)\nplt.show()","c774e320":"fig, ax = plt.subplots(figsize=(12,6))\ntrue_ratios = train_data.groupby('keyword')['target'].mean().sort_values(ascending=False).mul(100)\nsns.barplot(x=true_ratios.index[:30], y=true_ratios.values[:30], ax=ax)\nplt.xticks(rotation=90)\nplt.title(\"TOP 30 most 'true' keywords\")\nplt.ylabel(\"True ratio [%]\")\nplt.show()","eb66420b":"fig, ax = plt.subplots(figsize=(12,6))\ntrue_ratios = train_data.groupby('keyword')['target'].mean().sort_values(ascending=True).mul(100)\nsns.barplot(x=true_ratios.index[:30], y=true_ratios.values[:30], ax=ax)\nplt.xticks(rotation=90)\nplt.title(\"TOP 30 most 'fake' keywords\")\nplt.ylabel(\"True ratio [%]\")\nplt.show()","8b0a980e":"import re\n# removing contractions\ndef decontracted(tweet):\n    # specific\n    tweet = re.sub(r\"won\\'t\", \"will not\", tweet)\n    tweet = re.sub(r\"can\\'t\", \"can not\", tweet)\n    # general\n    tweet = re.sub(r\"n\\'t\", \" not\", tweet)\n    tweet = re.sub(r\"\\'re\", \" are\", tweet)\n    tweet = re.sub(r\"\\'s\", \" is\", tweet)\n    tweet = re.sub(r\"\\'d\", \" would\", tweet)\n    tweet = re.sub(r\"\\'ll\", \" will\", tweet)\n    tweet = re.sub(r\"\\'t\", \" not\", tweet)\n    tweet = re.sub(r\"\\'ve\", \" have\", tweet)\n    tweet = re.sub(r\"\\'m\", \" am\", tweet)\n    return tweet\n\ndef special_chars(tweet):\n    # Special characters\n    tweet = re.sub(r\"\\x89\u00db_\", \"\", tweet)\n    tweet = re.sub(r\"\\x89\u00db\u00d2\", \"\", tweet)\n    tweet = re.sub(r\"\\x89\u00db\u00d3\", \"\", tweet)\n    tweet = re.sub(r\"\\x89\u00db\u00cfWhen\", \"When\", tweet)\n    tweet = re.sub(r\"\\x89\u00db\u00cf\", \"\", tweet)\n    tweet = re.sub(r\"China\\x89\u00db\u00aas\", \"China's\", tweet)\n    tweet = re.sub(r\"let\\x89\u00db\u00aas\", \"let's\", tweet)\n    tweet = re.sub(r\"\\x89\u00db\u00f7\", \"\", tweet)\n    tweet = re.sub(r\"\\x89\u00db\u00aa\", \"\", tweet)\n    tweet = re.sub(r\"\\x89\u00db\\x9d\", \"\", tweet)\n    tweet = re.sub(r\"\u00e5_\", \"\", tweet)\n    tweet = re.sub(r\"\\x89\u00db\u00a2\", \"\", tweet)\n    tweet = re.sub(r\"\\x89\u00db\u00a2\u00e5\u00ca\", \"\", tweet)\n    tweet = re.sub(r\"from\u00e5\u00cawounds\", \"from wounds\", tweet)\n    tweet = re.sub(r\"\u00e5\u00ca\", \"\", tweet)\n    tweet = re.sub(r\"\u00e5\u00c8\", \"\", tweet)\n    tweet = re.sub(r\"Jap\u00cc_n\", \"Japan\", tweet)    \n    tweet = re.sub(r\"\u00cc\u00a9\", \"e\", tweet)\n    tweet = re.sub(r\"\u00e5\u00a8\", \"\", tweet)\n    tweet = re.sub(r\"Suru\u00cc\u00a4\", \"Suruc\", tweet)\n    tweet = re.sub(r\"\u00e5\u00c7\", \"\", tweet)\n    tweet = re.sub(r\"\u00e5\u00a33million\", \"3 million\", tweet)\n    tweet = re.sub(r\"\u00e5\u00c0\", \"\", tweet)\n    return tweet\n","420c89d3":"# this is taken from notebook of Gunes Evitan \"NLP with Disaster Tweets - EDA, Cleaning and BERT\"\ndef various(tweet):    \n    # Character entity references\n    tweet = re.sub(r\"&gt;\", \">\", tweet)\n    tweet = re.sub(r\"&lt;\", \"<\", tweet)\n    tweet = re.sub(r\"&amp;\", \"&\", tweet)\n    \n    # Typos, slang and informal abbreviations\n    tweet = re.sub(r\"w\/e\", \"whatever\", tweet)\n    tweet = re.sub(r\"w\/\", \"with\", tweet)\n    tweet = re.sub(r\"USAgov\", \"USA government\", tweet)\n    tweet = re.sub(r\"recentlu\", \"recently\", tweet)\n    tweet = re.sub(r\"Ph0tos\", \"Photos\", tweet)\n    tweet = re.sub(r\"amirite\", \"am I right\", tweet)\n    tweet = re.sub(r\"exp0sed\", \"exposed\", tweet)\n    tweet = re.sub(r\"<3\", \"love\", tweet)\n    tweet = re.sub(r\"amageddon\", \"armageddon\", tweet)\n    tweet = re.sub(r\"Trfc\", \"Traffic\", tweet)\n    tweet = re.sub(r\"8\/5\/2015\", \"2015-08-05\", tweet)\n    tweet = re.sub(r\"WindStorm\", \"Wind Storm\", tweet)\n    tweet = re.sub(r\"8\/6\/2015\", \"2015-08-06\", tweet)\n    tweet = re.sub(r\"10:38PM\", \"10:38 PM\", tweet)\n    tweet = re.sub(r\"10:30pm\", \"10:30 PM\", tweet)\n    tweet = re.sub(r\"16yr\", \"16 year\", tweet)\n    tweet = re.sub(r\"lmao\", \"laughing my ass off\", tweet)   \n    tweet = re.sub(r\"TRAUMATISED\", \"traumatized\", tweet)\n    \n    # Hashtags and usernames\n    tweet = re.sub(r\"IranDeal\", \"Iran Deal\", tweet)\n    tweet = re.sub(r\"ArianaGrande\", \"Ariana Grande\", tweet)\n    tweet = re.sub(r\"camilacabello97\", \"camila cabello\", tweet) \n    tweet = re.sub(r\"RondaRousey\", \"Ronda Rousey\", tweet)     \n    tweet = re.sub(r\"MTVHottest\", \"MTV Hottest\", tweet)\n    tweet = re.sub(r\"TrapMusic\", \"Trap Music\", tweet)\n    tweet = re.sub(r\"ProphetMuhammad\", \"Prophet Muhammad\", tweet)\n    tweet = re.sub(r\"PantherAttack\", \"Panther Attack\", tweet)\n    tweet = re.sub(r\"StrategicPatience\", \"Strategic Patience\", tweet)\n    tweet = re.sub(r\"socialnews\", \"social news\", tweet)\n    tweet = re.sub(r\"NASAHurricane\", \"NASA Hurricane\", tweet)\n    tweet = re.sub(r\"onlinecommunities\", \"online communities\", tweet)\n    tweet = re.sub(r\"humanconsumption\", \"human consumption\", tweet)\n    tweet = re.sub(r\"Typhoon-Devastated\", \"Typhoon Devastated\", tweet)\n    tweet = re.sub(r\"Meat-Loving\", \"Meat Loving\", tweet)\n    tweet = re.sub(r\"facialabuse\", \"facial abuse\", tweet)\n    tweet = re.sub(r\"LakeCounty\", \"Lake County\", tweet)\n    tweet = re.sub(r\"BeingAuthor\", \"Being Author\", tweet)\n    tweet = re.sub(r\"withheavenly\", \"with heavenly\", tweet)\n    tweet = re.sub(r\"thankU\", \"thank you\", tweet)\n    tweet = re.sub(r\"iTunesMusic\", \"iTunes Music\", tweet)\n    tweet = re.sub(r\"OffensiveContent\", \"Offensive Content\", tweet)\n    tweet = re.sub(r\"WorstSummerJob\", \"Worst Summer Job\", tweet)\n    tweet = re.sub(r\"HarryBeCareful\", \"Harry Be Careful\", tweet)\n    tweet = re.sub(r\"NASASolarSystem\", \"NASA Solar System\", tweet)\n    tweet = re.sub(r\"animalrescue\", \"animal rescue\", tweet)\n    tweet = re.sub(r\"KurtSchlichter\", \"Kurt Schlichter\", tweet)\n    tweet = re.sub(r\"aRmageddon\", \"armageddon\", tweet)\n    tweet = re.sub(r\"Throwingknifes\", \"Throwing knives\", tweet)\n    tweet = re.sub(r\"GodsLove\", \"God's Love\", tweet)\n    tweet = re.sub(r\"bookboost\", \"book boost\", tweet)\n    tweet = re.sub(r\"ibooklove\", \"I book love\", tweet)\n    tweet = re.sub(r\"NestleIndia\", \"Nestle India\", tweet)\n    tweet = re.sub(r\"realDonaldTrump\", \"Donald Trump\", tweet)\n    tweet = re.sub(r\"DavidVonderhaar\", \"David Vonderhaar\", tweet)\n    tweet = re.sub(r\"CecilTheLion\", \"Cecil The Lion\", tweet)\n    tweet = re.sub(r\"weathernetwork\", \"weather network\", tweet)\n    tweet = re.sub(r\"withBioterrorism&use\", \"with Bioterrorism & use\", tweet)\n    tweet = re.sub(r\"Hostage&2\", \"Hostage & 2\", tweet)\n    tweet = re.sub(r\"GOPDebate\", \"GOP Debate\", tweet)\n    tweet = re.sub(r\"RickPerry\", \"Rick Perry\", tweet)\n    tweet = re.sub(r\"frontpage\", \"front page\", tweet)\n    tweet = re.sub(r\"NewsInTweets\", \"News In Tweets\", tweet)\n    tweet = re.sub(r\"ViralSpell\", \"Viral Spell\", tweet)\n    tweet = re.sub(r\"til_now\", \"until now\", tweet)\n    tweet = re.sub(r\"volcanoinRussia\", \"volcano in Russia\", tweet)\n    tweet = re.sub(r\"ZippedNews\", \"Zipped News\", tweet)\n    tweet = re.sub(r\"MicheleBachman\", \"Michele Bachman\", tweet)\n    tweet = re.sub(r\"53inch\", \"53 inch\", tweet)\n    tweet = re.sub(r\"KerrickTrial\", \"Kerrick Trial\", tweet)\n    tweet = re.sub(r\"abstorm\", \"Alberta Storm\", tweet)\n    tweet = re.sub(r\"Beyhive\", \"Beyonce hive\", tweet)\n    tweet = re.sub(r\"IDFire\", \"Idaho Fire\", tweet)\n    tweet = re.sub(r\"DETECTADO\", \"Detected\", tweet)\n    tweet = re.sub(r\"RockyFire\", \"Rocky Fire\", tweet)\n    tweet = re.sub(r\"Listen\/Buy\", \"Listen \/ Buy\", tweet)\n    tweet = re.sub(r\"NickCannon\", \"Nick Cannon\", tweet)\n    tweet = re.sub(r\"FaroeIslands\", \"Faroe Islands\", tweet)\n    tweet = re.sub(r\"yycstorm\", \"Calgary Storm\", tweet)\n    tweet = re.sub(r\"IDPs:\", \"Internally Displaced People :\", tweet)\n    tweet = re.sub(r\"ArtistsUnited\", \"Artists United\", tweet)\n    tweet = re.sub(r\"ClaytonBryant\", \"Clayton Bryant\", tweet)\n    tweet = re.sub(r\"jimmyfallon\", \"jimmy fallon\", tweet)\n    tweet = re.sub(r\"justinbieber\", \"justin bieber\", tweet)  \n    tweet = re.sub(r\"UTC2015\", \"UTC 2015\", tweet)\n    tweet = re.sub(r\"Time2015\", \"Time 2015\", tweet)\n    tweet = re.sub(r\"djicemoon\", \"dj icemoon\", tweet)\n    tweet = re.sub(r\"LivingSafely\", \"Living Safely\", tweet)\n    tweet = re.sub(r\"FIFA16\", \"Fifa 2016\", tweet)\n    tweet = re.sub(r\"thisiswhywecanthavenicethings\", \"this is why we cannot have nice things\", tweet)\n    tweet = re.sub(r\"bbcnews\", \"bbc news\", tweet)\n    tweet = re.sub(r\"UndergroundRailraod\", \"Underground Railraod\", tweet)\n    tweet = re.sub(r\"c4news\", \"c4 news\", tweet)\n    tweet = re.sub(r\"OBLITERATION\", \"obliteration\", tweet)\n    tweet = re.sub(r\"MUDSLIDE\", \"mudslide\", tweet)\n    tweet = re.sub(r\"NoSurrender\", \"No Surrender\", tweet)\n    tweet = re.sub(r\"NotExplained\", \"Not Explained\", tweet)\n    tweet = re.sub(r\"greatbritishbakeoff\", \"great british bake off\", tweet)\n    tweet = re.sub(r\"LondonFire\", \"London Fire\", tweet)\n    tweet = re.sub(r\"KOTAWeather\", \"KOTA Weather\", tweet)\n    tweet = re.sub(r\"LuchaUnderground\", \"Lucha Underground\", tweet)\n    tweet = re.sub(r\"KOIN6News\", \"KOIN 6 News\", tweet)\n    tweet = re.sub(r\"LiveOnK2\", \"Live On K2\", tweet)\n    tweet = re.sub(r\"9NewsGoldCoast\", \"9 News Gold Coast\", tweet)\n    tweet = re.sub(r\"nikeplus\", \"nike plus\", tweet)\n    tweet = re.sub(r\"david_cameron\", \"David Cameron\", tweet)\n    tweet = re.sub(r\"peterjukes\", \"Peter Jukes\", tweet)\n    tweet = re.sub(r\"JamesMelville\", \"James Melville\", tweet)\n    tweet = re.sub(r\"megynkelly\", \"Megyn Kelly\", tweet)\n    tweet = re.sub(r\"cnewslive\", \"C News Live\", tweet)\n    tweet = re.sub(r\"JamaicaObserver\", \"Jamaica Observer\", tweet)\n    tweet = re.sub(r\"TweetLikeItsSeptember11th2001\", \"Tweet like it is september 11th 2001\", tweet)\n    tweet = re.sub(r\"cbplawyers\", \"cbp lawyers\", tweet)\n    tweet = re.sub(r\"fewmoretweets\", \"few more tweets\", tweet)\n    tweet = re.sub(r\"BlackLivesMatter\", \"Black Lives Matter\", tweet)\n    tweet = re.sub(r\"cjoyner\", \"Chris Joyner\", tweet)\n    tweet = re.sub(r\"ENGvAUS\", \"England vs Australia\", tweet)\n    tweet = re.sub(r\"ScottWalker\", \"Scott Walker\", tweet)\n    tweet = re.sub(r\"MikeParrActor\", \"Michael Parr\", tweet)\n    tweet = re.sub(r\"4PlayThursdays\", \"Foreplay Thursdays\", tweet)\n    tweet = re.sub(r\"TGF2015\", \"Tontitown Grape Festival\", tweet)\n    tweet = re.sub(r\"realmandyrain\", \"Mandy Rain\", tweet)\n    tweet = re.sub(r\"GraysonDolan\", \"Grayson Dolan\", tweet)\n    tweet = re.sub(r\"ApolloBrown\", \"Apollo Brown\", tweet)\n    tweet = re.sub(r\"saddlebrooke\", \"Saddlebrooke\", tweet)\n    tweet = re.sub(r\"TontitownGrape\", \"Tontitown Grape\", tweet)\n    tweet = re.sub(r\"AbbsWinston\", \"Abbs Winston\", tweet)\n    tweet = re.sub(r\"ShaunKing\", \"Shaun King\", tweet)\n    tweet = re.sub(r\"MeekMill\", \"Meek Mill\", tweet)\n    tweet = re.sub(r\"TornadoGiveaway\", \"Tornado Giveaway\", tweet)\n    tweet = re.sub(r\"GRupdates\", \"GR updates\", tweet)\n    tweet = re.sub(r\"SouthDowns\", \"South Downs\", tweet)\n    tweet = re.sub(r\"braininjury\", \"brain injury\", tweet)\n    tweet = re.sub(r\"auspol\", \"Australian politics\", tweet)\n    tweet = re.sub(r\"PlannedParenthood\", \"Planned Parenthood\", tweet)\n    tweet = re.sub(r\"calgaryweather\", \"Calgary Weather\", tweet)\n    tweet = re.sub(r\"weallheartonedirection\", \"we all heart one direction\", tweet)\n    tweet = re.sub(r\"edsheeran\", \"Ed Sheeran\", tweet)\n    tweet = re.sub(r\"TrueHeroes\", \"True Heroes\", tweet)\n    tweet = re.sub(r\"S3XLEAK\", \"sex leak\", tweet)\n    tweet = re.sub(r\"ComplexMag\", \"Complex Magazine\", tweet)\n    tweet = re.sub(r\"TheAdvocateMag\", \"The Advocate Magazine\", tweet)\n    tweet = re.sub(r\"CityofCalgary\", \"City of Calgary\", tweet)\n    tweet = re.sub(r\"EbolaOutbreak\", \"Ebola Outbreak\", tweet)\n    tweet = re.sub(r\"SummerFate\", \"Summer Fate\", tweet)\n    tweet = re.sub(r\"RAmag\", \"Royal Academy Magazine\", tweet)\n    tweet = re.sub(r\"offers2go\", \"offers to go\", tweet)\n    tweet = re.sub(r\"foodscare\", \"food scare\", tweet)\n    tweet = re.sub(r\"MNPDNashville\", \"Metropolitan Nashville Police Department\", tweet)\n    tweet = re.sub(r\"TfLBusAlerts\", \"TfL Bus Alerts\", tweet)\n    tweet = re.sub(r\"GamerGate\", \"Gamer Gate\", tweet)\n    tweet = re.sub(r\"IHHen\", \"Humanitarian Relief\", tweet)\n    tweet = re.sub(r\"spinningbot\", \"spinning bot\", tweet)\n    tweet = re.sub(r\"ModiMinistry\", \"Modi Ministry\", tweet)\n    tweet = re.sub(r\"TAXIWAYS\", \"taxi ways\", tweet)\n    tweet = re.sub(r\"Calum5SOS\", \"Calum Hood\", tweet)\n    tweet = re.sub(r\"po_st\", \"po.st\", tweet)\n    tweet = re.sub(r\"scoopit\", \"scoop.it\", tweet)\n    tweet = re.sub(r\"UltimaLucha\", \"Ultima Lucha\", tweet)\n    tweet = re.sub(r\"JonathanFerrell\", \"Jonathan Ferrell\", tweet)\n    tweet = re.sub(r\"aria_ahrary\", \"Aria Ahrary\", tweet)\n    tweet = re.sub(r\"rapidcity\", \"Rapid City\", tweet)\n    tweet = re.sub(r\"OutBid\", \"outbid\", tweet)\n    tweet = re.sub(r\"lavenderpoetrycafe\", \"lavender poetry cafe\", tweet)\n    tweet = re.sub(r\"EudryLantiqua\", \"Eudry Lantiqua\", tweet)\n    tweet = re.sub(r\"15PM\", \"15 PM\", tweet)\n    tweet = re.sub(r\"OriginalFunko\", \"Funko\", tweet)\n    tweet = re.sub(r\"rightwaystan\", \"Richard Tan\", tweet)\n    tweet = re.sub(r\"CindyNoonan\", \"Cindy Noonan\", tweet)\n    tweet = re.sub(r\"RT_America\", \"RT America\", tweet)\n    tweet = re.sub(r\"narendramodi\", \"Narendra Modi\", tweet)\n    tweet = re.sub(r\"BakeOffFriends\", \"Bake Off Friends\", tweet)\n    tweet = re.sub(r\"TeamHendrick\", \"Hendrick Motorsports\", tweet)\n    tweet = re.sub(r\"alexbelloli\", \"Alex Belloli\", tweet)\n    tweet = re.sub(r\"itsjustinstuart\", \"Justin Stuart\", tweet)\n    tweet = re.sub(r\"gunsense\", \"gun sense\", tweet)\n    tweet = re.sub(r\"DebateQuestionsWeWantToHear\", \"debate questions we want to hear\", tweet)\n    tweet = re.sub(r\"RoyalCarribean\", \"Royal Carribean\", tweet)\n    tweet = re.sub(r\"samanthaturne19\", \"Samantha Turner\", tweet)\n    tweet = re.sub(r\"JonVoyage\", \"Jon Stewart\", tweet)\n    tweet = re.sub(r\"renew911health\", \"renew 911 health\", tweet)\n    tweet = re.sub(r\"SuryaRay\", \"Surya Ray\", tweet)\n    tweet = re.sub(r\"pattonoswalt\", \"Patton Oswalt\", tweet)\n    tweet = re.sub(r\"minhazmerchant\", \"Minhaz Merchant\", tweet)\n    tweet = re.sub(r\"TLVFaces\", \"Israel Diaspora Coalition\", tweet)\n    tweet = re.sub(r\"pmarca\", \"Marc Andreessen\", tweet)\n    tweet = re.sub(r\"pdx911\", \"Portland Police\", tweet)\n    tweet = re.sub(r\"jamaicaplain\", \"Jamaica Plain\", tweet)\n    tweet = re.sub(r\"Japton\", \"Arkansas\", tweet)\n    tweet = re.sub(r\"RouteComplex\", \"Route Complex\", tweet)\n    tweet = re.sub(r\"INSubcontinent\", \"Indian Subcontinent\", tweet)\n    tweet = re.sub(r\"NJTurnpike\", \"New Jersey Turnpike\", tweet)\n    tweet = re.sub(r\"Politifiact\", \"PolitiFact\", tweet)\n    tweet = re.sub(r\"Hiroshima70\", \"Hiroshima\", tweet)\n    tweet = re.sub(r\"GMMBC\", \"Greater Mt Moriah Baptist Church\", tweet)\n    tweet = re.sub(r\"versethe\", \"verse the\", tweet)\n    tweet = re.sub(r\"TubeStrike\", \"Tube Strike\", tweet)\n    tweet = re.sub(r\"MissionHills\", \"Mission Hills\", tweet)\n    tweet = re.sub(r\"ProtectDenaliWolves\", \"Protect Denali Wolves\", tweet)\n    tweet = re.sub(r\"NANKANA\", \"Nankana\", tweet)\n    tweet = re.sub(r\"SAHIB\", \"Sahib\", tweet)\n    tweet = re.sub(r\"PAKPATTAN\", \"Pakpattan\", tweet)\n    tweet = re.sub(r\"Newz_Sacramento\", \"News Sacramento\", tweet)\n    tweet = re.sub(r\"gofundme\", \"go fund me\", tweet)\n    tweet = re.sub(r\"pmharper\", \"Stephen Harper\", tweet)\n    tweet = re.sub(r\"IvanBerroa\", \"Ivan Berroa\", tweet)\n    tweet = re.sub(r\"LosDelSonido\", \"Los Del Sonido\", tweet)\n    tweet = re.sub(r\"bancodeseries\", \"banco de series\", tweet)\n    tweet = re.sub(r\"timkaine\", \"Tim Kaine\", tweet)\n    tweet = re.sub(r\"IdentityTheft\", \"Identity Theft\", tweet)\n    tweet = re.sub(r\"AllLivesMatter\", \"All Lives Matter\", tweet)\n    tweet = re.sub(r\"mishacollins\", \"Misha Collins\", tweet)\n    tweet = re.sub(r\"BillNeelyNBC\", \"Bill Neely\", tweet)\n    tweet = re.sub(r\"BeClearOnCancer\", \"be clear on cancer\", tweet)\n    tweet = re.sub(r\"Kowing\", \"Knowing\", tweet)\n    tweet = re.sub(r\"ScreamQueens\", \"Scream Queens\", tweet)\n    tweet = re.sub(r\"AskCharley\", \"Ask Charley\", tweet)\n    tweet = re.sub(r\"BlizzHeroes\", \"Heroes of the Storm\", tweet)\n    tweet = re.sub(r\"BradleyBrad47\", \"Bradley Brad\", tweet)\n    tweet = re.sub(r\"HannaPH\", \"Typhoon Hanna\", tweet)\n    tweet = re.sub(r\"meinlcymbals\", \"MEINL Cymbals\", tweet)\n    tweet = re.sub(r\"Ptbo\", \"Peterborough\", tweet)\n    tweet = re.sub(r\"cnnbrk\", \"CNN Breaking News\", tweet)\n    tweet = re.sub(r\"IndianNews\", \"Indian News\", tweet)\n    tweet = re.sub(r\"savebees\", \"save bees\", tweet)\n    tweet = re.sub(r\"GreenHarvard\", \"Green Harvard\", tweet)\n    tweet = re.sub(r\"StandwithPP\", \"Stand with planned parenthood\", tweet)\n    tweet = re.sub(r\"hermancranston\", \"Herman Cranston\", tweet)\n    tweet = re.sub(r\"WMUR9\", \"WMUR-TV\", tweet)\n    tweet = re.sub(r\"RockBottomRadFM\", \"Rock Bottom Radio\", tweet)\n    tweet = re.sub(r\"ameenshaikh3\", \"Ameen Shaikh\", tweet)\n    tweet = re.sub(r\"ProSyn\", \"Project Syndicate\", tweet)\n    tweet = re.sub(r\"Daesh\", \"ISIS\", tweet)\n    tweet = re.sub(r\"s2g\", \"swear to god\", tweet)\n    tweet = re.sub(r\"listenlive\", \"listen live\", tweet)\n    tweet = re.sub(r\"CDCgov\", \"Centers for Disease Control and Prevention\", tweet)\n    tweet = re.sub(r\"FoxNew\", \"Fox News\", tweet)\n    tweet = re.sub(r\"CBSBigBrother\", \"Big Brother\", tweet)\n    tweet = re.sub(r\"JulieDiCaro\", \"Julie DiCaro\", tweet)\n    tweet = re.sub(r\"theadvocatemag\", \"The Advocate Magazine\", tweet)\n    tweet = re.sub(r\"RohnertParkDPS\", \"Rohnert Park Police Department\", tweet)\n    tweet = re.sub(r\"THISIZBWRIGHT\", \"Bonnie Wright\", tweet)\n    tweet = re.sub(r\"Popularmmos\", \"Popular MMOs\", tweet)\n    tweet = re.sub(r\"WildHorses\", \"Wild Horses\", tweet)\n    tweet = re.sub(r\"FantasticFour\", \"Fantastic Four\", tweet)\n    tweet = re.sub(r\"HORNDALE\", \"Horndale\", tweet)\n    tweet = re.sub(r\"PINER\", \"Piner\", tweet)\n    tweet = re.sub(r\"BathAndNorthEastSomerset\", \"Bath and North East Somerset\", tweet)\n    tweet = re.sub(r\"thatswhatfriendsarefor\", \"that is what friends are for\", tweet)\n    tweet = re.sub(r\"residualincome\", \"residual income\", tweet)\n    tweet = re.sub(r\"YahooNewsDigest\", \"Yahoo News Digest\", tweet)\n    tweet = re.sub(r\"MalaysiaAirlines\", \"Malaysia Airlines\", tweet)\n    tweet = re.sub(r\"AmazonDeals\", \"Amazon Deals\", tweet)\n    tweet = re.sub(r\"MissCharleyWebb\", \"Charley Webb\", tweet)\n    tweet = re.sub(r\"shoalstraffic\", \"shoals traffic\", tweet)\n    tweet = re.sub(r\"GeorgeFoster72\", \"George Foster\", tweet)\n    tweet = re.sub(r\"pop2015\", \"pop 2015\", tweet)\n    tweet = re.sub(r\"_PokemonCards_\", \"Pokemon Cards\", tweet)\n    tweet = re.sub(r\"DianneG\", \"Dianne Gallagher\", tweet)\n    tweet = re.sub(r\"KashmirConflict\", \"Kashmir Conflict\", tweet)\n    tweet = re.sub(r\"BritishBakeOff\", \"British Bake Off\", tweet)\n    tweet = re.sub(r\"FreeKashmir\", \"Free Kashmir\", tweet)\n    tweet = re.sub(r\"mattmosley\", \"Matt Mosley\", tweet)\n    tweet = re.sub(r\"BishopFred\", \"Bishop Fred\", tweet)\n    tweet = re.sub(r\"EndConflict\", \"End Conflict\", tweet)\n    tweet = re.sub(r\"EndOccupation\", \"End Occupation\", tweet)\n    tweet = re.sub(r\"UNHEALED\", \"unhealed\", tweet)\n    tweet = re.sub(r\"CharlesDagnall\", \"Charles Dagnall\", tweet)\n    tweet = re.sub(r\"Latestnews\", \"Latest news\", tweet)\n    tweet = re.sub(r\"KindleCountdown\", \"Kindle Countdown\", tweet)\n    tweet = re.sub(r\"NoMoreHandouts\", \"No More Handouts\", tweet)\n    tweet = re.sub(r\"datingtips\", \"dating tips\", tweet)\n    tweet = re.sub(r\"charlesadler\", \"Charles Adler\", tweet)\n    tweet = re.sub(r\"twia\", \"Texas Windstorm Insurance Association\", tweet)\n    tweet = re.sub(r\"txlege\", \"Texas Legislature\", tweet)\n    tweet = re.sub(r\"WindstormInsurer\", \"Windstorm Insurer\", tweet)\n    tweet = re.sub(r\"Newss\", \"News\", tweet)\n    tweet = re.sub(r\"hempoil\", \"hemp oil\", tweet)\n    tweet = re.sub(r\"CommoditiesAre\", \"Commodities are\", tweet)\n    tweet = re.sub(r\"tubestrike\", \"tube strike\", tweet)\n    tweet = re.sub(r\"JoeNBC\", \"Joe Scarborough\", tweet)\n    tweet = re.sub(r\"LiteraryCakes\", \"Literary Cakes\", tweet)\n    tweet = re.sub(r\"TI5\", \"The International 5\", tweet)\n    tweet = re.sub(r\"thehill\", \"the hill\", tweet)\n    tweet = re.sub(r\"3others\", \"3 others\", tweet)\n    tweet = re.sub(r\"stighefootball\", \"Sam Tighe\", tweet)\n    tweet = re.sub(r\"whatstheimportantvideo\", \"what is the important video\", tweet)\n    tweet = re.sub(r\"ClaudioMeloni\", \"Claudio Meloni\", tweet)\n    tweet = re.sub(r\"DukeSkywalker\", \"Duke Skywalker\", tweet)\n    tweet = re.sub(r\"carsonmwr\", \"Fort Carson\", tweet)\n    tweet = re.sub(r\"offdishduty\", \"off dish duty\", tweet)\n    tweet = re.sub(r\"andword\", \"and word\", tweet)\n    tweet = re.sub(r\"rhodeisland\", \"Rhode Island\", tweet)\n    tweet = re.sub(r\"easternoregon\", \"Eastern Oregon\", tweet)\n    tweet = re.sub(r\"WAwildfire\", \"Washington Wildfire\", tweet)\n    tweet = re.sub(r\"fingerrockfire\", \"Finger Rock Fire\", tweet)\n    tweet = re.sub(r\"57am\", \"57 am\", tweet)\n    tweet = re.sub(r\"fingerrockfire\", \"Finger Rock Fire\", tweet)\n    tweet = re.sub(r\"JacobHoggard\", \"Jacob Hoggard\", tweet)\n    tweet = re.sub(r\"newnewnew\", \"new new new\", tweet)\n    tweet = re.sub(r\"under50\", \"under 50\", tweet)\n    tweet = re.sub(r\"getitbeforeitsgone\", \"get it before it is gone\", tweet)\n    tweet = re.sub(r\"freshoutofthebox\", \"fresh out of the box\", tweet)\n    tweet = re.sub(r\"amwriting\", \"am writing\", tweet)\n    tweet = re.sub(r\"Bokoharm\", \"Boko Haram\", tweet)\n    tweet = re.sub(r\"Nowlike\", \"Now like\", tweet)\n    tweet = re.sub(r\"seasonfrom\", \"season from\", tweet)\n    tweet = re.sub(r\"epicente\", \"epicenter\", tweet)\n    tweet = re.sub(r\"epicenterr\", \"epicenter\", tweet)\n    tweet = re.sub(r\"sicklife\", \"sick life\", tweet)\n    tweet = re.sub(r\"yycweather\", \"Calgary Weather\", tweet)\n    tweet = re.sub(r\"calgarysun\", \"Calgary Sun\", tweet)\n    tweet = re.sub(r\"approachng\", \"approaching\", tweet)\n    tweet = re.sub(r\"evng\", \"evening\", tweet)\n    tweet = re.sub(r\"Sumthng\", \"something\", tweet)\n    tweet = re.sub(r\"EllenPompeo\", \"Ellen Pompeo\", tweet)\n    tweet = re.sub(r\"shondarhimes\", \"Shonda Rhimes\", tweet)\n    tweet = re.sub(r\"ABCNetwork\", \"ABC Network\", tweet)\n    tweet = re.sub(r\"SushmaSwaraj\", \"Sushma Swaraj\", tweet)\n    tweet = re.sub(r\"pray4japan\", \"Pray for Japan\", tweet)\n    tweet = re.sub(r\"hope4japan\", \"Hope for Japan\", tweet)\n    tweet = re.sub(r\"Illusionimagess\", \"Illusion images\", tweet)\n    tweet = re.sub(r\"SummerUnderTheStars\", \"Summer Under The Stars\", tweet)\n    tweet = re.sub(r\"ShallWeDance\", \"Shall We Dance\", tweet)\n    tweet = re.sub(r\"TCMParty\", \"TCM Party\", tweet)\n    tweet = re.sub(r\"marijuananews\", \"marijuana news\", tweet)\n    tweet = re.sub(r\"onbeingwithKristaTippett\", \"on being with Krista Tippett\", tweet)\n    tweet = re.sub(r\"Beingtweets\", \"Being tweets\", tweet)\n    tweet = re.sub(r\"newauthors\", \"new authors\", tweet)\n    tweet = re.sub(r\"remedyyyy\", \"remedy\", tweet)\n    tweet = re.sub(r\"44PM\", \"44 PM\", tweet)\n    tweet = re.sub(r\"HeadlinesApp\", \"Headlines App\", tweet)\n    tweet = re.sub(r\"40PM\", \"40 PM\", tweet)\n    tweet = re.sub(r\"myswc\", \"Severe Weather Center\", tweet)\n    tweet = re.sub(r\"ithats\", \"that is\", tweet)\n    tweet = re.sub(r\"icouldsitinthismomentforever\", \"I could sit in this moment forever\", tweet)\n    tweet = re.sub(r\"FatLoss\", \"Fat Loss\", tweet)\n    tweet = re.sub(r\"02PM\", \"02 PM\", tweet)\n    tweet = re.sub(r\"MetroFmTalk\", \"Metro Fm Talk\", tweet)\n    tweet = re.sub(r\"Bstrd\", \"bastard\", tweet)\n    tweet = re.sub(r\"bldy\", \"bloody\", tweet)\n    tweet = re.sub(r\"MetrofmTalk\", \"Metro Fm Talk\", tweet)\n    tweet = re.sub(r\"terrorismturn\", \"terrorism turn\", tweet)\n    tweet = re.sub(r\"BBCNewsAsia\", \"BBC News Asia\", tweet)\n    tweet = re.sub(r\"BehindTheScenes\", \"Behind The Scenes\", tweet)\n    tweet = re.sub(r\"GeorgeTakei\", \"George Takei\", tweet)\n    tweet = re.sub(r\"WomensWeeklyMag\", \"Womens Weekly Magazine\", tweet)\n    tweet = re.sub(r\"SurvivorsGuidetoEarth\", \"Survivors Guide to Earth\", tweet)\n    tweet = re.sub(r\"incubusband\", \"incubus band\", tweet)\n    tweet = re.sub(r\"Babypicturethis\", \"Baby picture this\", tweet)\n    tweet = re.sub(r\"BombEffects\", \"Bomb Effects\", tweet)\n    tweet = re.sub(r\"win10\", \"Windows 10\", tweet)\n    tweet = re.sub(r\"idkidk\", \"I do not know I do not know\", tweet)\n    tweet = re.sub(r\"TheWalkingDead\", \"The Walking Dead\", tweet)\n    tweet = re.sub(r\"amyschumer\", \"Amy Schumer\", tweet)\n    tweet = re.sub(r\"crewlist\", \"crew list\", tweet)\n    tweet = re.sub(r\"Erdogans\", \"Erdogan\", tweet)\n    tweet = re.sub(r\"BBCLive\", \"BBC Live\", tweet)\n    tweet = re.sub(r\"TonyAbbottMHR\", \"Tony Abbott\", tweet)\n    tweet = re.sub(r\"paulmyerscough\", \"Paul Myerscough\", tweet)\n    tweet = re.sub(r\"georgegallagher\", \"George Gallagher\", tweet)\n    tweet = re.sub(r\"JimmieJohnson\", \"Jimmie Johnson\", tweet)\n    tweet = re.sub(r\"pctool\", \"pc tool\", tweet)\n    tweet = re.sub(r\"DoingHashtagsRight\", \"Doing Hashtags Right\", tweet)\n    tweet = re.sub(r\"ThrowbackThursday\", \"Throwback Thursday\", tweet)\n    tweet = re.sub(r\"SnowBackSunday\", \"Snowback Sunday\", tweet)\n    tweet = re.sub(r\"LakeEffect\", \"Lake Effect\", tweet)\n    tweet = re.sub(r\"RTphotographyUK\", \"Richard Thomas Photography UK\", tweet)\n    tweet = re.sub(r\"BigBang_CBS\", \"Big Bang CBS\", tweet)\n    tweet = re.sub(r\"writerslife\", \"writers life\", tweet)\n    tweet = re.sub(r\"NaturalBirth\", \"Natural Birth\", tweet)\n    tweet = re.sub(r\"UnusualWords\", \"Unusual Words\", tweet)\n    tweet = re.sub(r\"wizkhalifa\", \"Wiz Khalifa\", tweet)\n    tweet = re.sub(r\"acreativedc\", \"a creative DC\", tweet)\n    tweet = re.sub(r\"vscodc\", \"vsco DC\", tweet)\n    tweet = re.sub(r\"VSCOcam\", \"vsco camera\", tweet)\n    tweet = re.sub(r\"TheBEACHDC\", \"The beach DC\", tweet)\n    tweet = re.sub(r\"buildingmuseum\", \"building museum\", tweet)\n    tweet = re.sub(r\"WorldOil\", \"World Oil\", tweet)\n    tweet = re.sub(r\"redwedding\", \"red wedding\", tweet)\n    tweet = re.sub(r\"AmazingRaceCanada\", \"Amazing Race Canada\", tweet)\n    tweet = re.sub(r\"WakeUpAmerica\", \"Wake Up America\", tweet)\n    tweet = re.sub(r\"\\\\Allahuakbar\\\\\", \"Allahu Akbar\", tweet)\n    tweet = re.sub(r\"bleased\", \"blessed\", tweet)\n    tweet = re.sub(r\"nigeriantribune\", \"Nigerian Tribune\", tweet)\n    tweet = re.sub(r\"HIDEO_KOJIMA_EN\", \"Hideo Kojima\", tweet)\n    tweet = re.sub(r\"FusionFestival\", \"Fusion Festival\", tweet)\n    tweet = re.sub(r\"50Mixed\", \"50 Mixed\", tweet)\n    tweet = re.sub(r\"NoAgenda\", \"No Agenda\", tweet)\n    tweet = re.sub(r\"WhiteGenocide\", \"White Genocide\", tweet)\n    tweet = re.sub(r\"dirtylying\", \"dirty lying\", tweet)\n    tweet = re.sub(r\"SyrianRefugees\", \"Syrian Refugees\", tweet)\n    tweet = re.sub(r\"changetheworld\", \"change the world\", tweet)\n    tweet = re.sub(r\"Ebolacase\", \"Ebola case\", tweet)\n    tweet = re.sub(r\"mcgtech\", \"mcg technologies\", tweet)\n    tweet = re.sub(r\"withweapons\", \"with weapons\", tweet)\n    tweet = re.sub(r\"advancedwarfare\", \"advanced warfare\", tweet)\n    tweet = re.sub(r\"letsFootball\", \"let us Football\", tweet)\n    tweet = re.sub(r\"LateNiteMix\", \"late night mix\", tweet)\n    tweet = re.sub(r\"PhilCollinsFeed\", \"Phil Collins\", tweet)\n    tweet = re.sub(r\"RudyHavenstein\", \"Rudy Havenstein\", tweet)\n    tweet = re.sub(r\"22PM\", \"22 PM\", tweet)\n    tweet = re.sub(r\"54am\", \"54 AM\", tweet)\n    tweet = re.sub(r\"38am\", \"38 AM\", tweet)\n    tweet = re.sub(r\"OldFolkExplainStuff\", \"Old Folk Explain Stuff\", tweet)\n    tweet = re.sub(r\"BlacklivesMatter\", \"Black Lives Matter\", tweet)\n    tweet = re.sub(r\"InsaneLimits\", \"Insane Limits\", tweet)\n    tweet = re.sub(r\"youcantsitwithus\", \"you cannot sit with us\", tweet)\n    tweet = re.sub(r\"2k15\", \"2015\", tweet)\n    tweet = re.sub(r\"TheIran\", \"Iran\", tweet)\n    tweet = re.sub(r\"JimmyFallon\", \"Jimmy Fallon\", tweet)\n    tweet = re.sub(r\"AlbertBrooks\", \"Albert Brooks\", tweet)\n    tweet = re.sub(r\"defense_news\", \"defense news\", tweet)\n    tweet = re.sub(r\"nuclearrcSA\", \"Nuclear Risk Control Self Assessment\", tweet)\n    tweet = re.sub(r\"Auspol\", \"Australia Politics\", tweet)\n    tweet = re.sub(r\"NuclearPower\", \"Nuclear Power\", tweet)\n    tweet = re.sub(r\"WhiteTerrorism\", \"White Terrorism\", tweet)\n    tweet = re.sub(r\"truthfrequencyradio\", \"Truth Frequency Radio\", tweet)\n    tweet = re.sub(r\"ErasureIsNotEquality\", \"Erasure is not equality\", tweet)\n    tweet = re.sub(r\"ProBonoNews\", \"Pro Bono News\", tweet)\n    tweet = re.sub(r\"JakartaPost\", \"Jakarta Post\", tweet)\n    tweet = re.sub(r\"toopainful\", \"too painful\", tweet)\n    tweet = re.sub(r\"melindahaunton\", \"Melinda Haunton\", tweet)\n    tweet = re.sub(r\"NoNukes\", \"No Nukes\", tweet)\n    tweet = re.sub(r\"curryspcworld\", \"Currys PC World\", tweet)\n    tweet = re.sub(r\"ineedcake\", \"I need cake\", tweet)\n    tweet = re.sub(r\"blackforestgateau\", \"black forest gateau\", tweet)\n    tweet = re.sub(r\"BBCOne\", \"BBC One\", tweet)\n    tweet = re.sub(r\"AlexxPage\", \"Alex Page\", tweet)\n    tweet = re.sub(r\"jonathanserrie\", \"Jonathan Serrie\", tweet)\n    tweet = re.sub(r\"SocialJerkBlog\", \"Social Jerk Blog\", tweet)\n    tweet = re.sub(r\"ChelseaVPeretti\", \"Chelsea Peretti\", tweet)\n    tweet = re.sub(r\"irongiant\", \"iron giant\", tweet)\n    tweet = re.sub(r\"RonFunches\", \"Ron Funches\", tweet)\n    tweet = re.sub(r\"TimCook\", \"Tim Cook\", tweet)\n    tweet = re.sub(r\"sebastianstanisaliveandwell\", \"Sebastian Stan is alive and well\", tweet)\n    tweet = re.sub(r\"Madsummer\", \"Mad summer\", tweet)\n    tweet = re.sub(r\"NowYouKnow\", \"Now you know\", tweet)\n    tweet = re.sub(r\"concertphotography\", \"concert photography\", tweet)\n    tweet = re.sub(r\"TomLandry\", \"Tom Landry\", tweet)\n    tweet = re.sub(r\"showgirldayoff\", \"show girl day off\", tweet)\n    tweet = re.sub(r\"Yougslavia\", \"Yugoslavia\", tweet)\n    tweet = re.sub(r\"QuantumDataInformatics\", \"Quantum Data Informatics\", tweet)\n    tweet = re.sub(r\"FromTheDesk\", \"From The Desk\", tweet)\n    tweet = re.sub(r\"TheaterTrial\", \"Theater Trial\", tweet)\n    tweet = re.sub(r\"CatoInstitute\", \"Cato Institute\", tweet)\n    tweet = re.sub(r\"EmekaGift\", \"Emeka Gift\", tweet)\n    tweet = re.sub(r\"LetsBe_Rational\", \"Let us be rational\", tweet)\n    tweet = re.sub(r\"Cynicalreality\", \"Cynical reality\", tweet)\n    tweet = re.sub(r\"FredOlsenCruise\", \"Fred Olsen Cruise\", tweet)\n    tweet = re.sub(r\"NotSorry\", \"not sorry\", tweet)\n    tweet = re.sub(r\"UseYourWords\", \"use your words\", tweet)\n    tweet = re.sub(r\"WordoftheDay\", \"word of the day\", tweet)\n    tweet = re.sub(r\"Dictionarycom\", \"Dictionary.com\", tweet)\n    tweet = re.sub(r\"TheBrooklynLife\", \"The Brooklyn Life\", tweet)\n    tweet = re.sub(r\"jokethey\", \"joke they\", tweet)\n    tweet = re.sub(r\"nflweek1picks\", \"NFL week 1 picks\", tweet)\n    tweet = re.sub(r\"uiseful\", \"useful\", tweet)\n    tweet = re.sub(r\"JusticeDotOrg\", \"The American Association for Justice\", tweet)\n    tweet = re.sub(r\"autoaccidents\", \"auto accidents\", tweet)\n    tweet = re.sub(r\"SteveGursten\", \"Steve Gursten\", tweet)\n    tweet = re.sub(r\"MichiganAutoLaw\", \"Michigan Auto Law\", tweet)\n    tweet = re.sub(r\"birdgang\", \"bird gang\", tweet)\n    tweet = re.sub(r\"nflnetwork\", \"NFL Network\", tweet)\n    tweet = re.sub(r\"NYDNSports\", \"NY Daily News Sports\", tweet)\n    tweet = re.sub(r\"RVacchianoNYDN\", \"Ralph Vacchiano NY Daily News\", tweet)\n    tweet = re.sub(r\"EdmontonEsks\", \"Edmonton Eskimos\", tweet)\n    tweet = re.sub(r\"david_brelsford\", \"David Brelsford\", tweet)\n    tweet = re.sub(r\"TOI_India\", \"The Times of India\", tweet)\n    tweet = re.sub(r\"hegot\", \"he got\", tweet)\n    tweet = re.sub(r\"SkinsOn9\", \"Skins on 9\", tweet)\n    tweet = re.sub(r\"sothathappened\", \"so that happened\", tweet)\n    tweet = re.sub(r\"LCOutOfDoors\", \"LC Out Of Doors\", tweet)\n    tweet = re.sub(r\"NationFirst\", \"Nation First\", tweet)\n    tweet = re.sub(r\"IndiaToday\", \"India Today\", tweet)\n    tweet = re.sub(r\"HLPS\", \"helps\", tweet)\n    tweet = re.sub(r\"HOSTAGESTHROSW\", \"hostages throw\", tweet)\n    tweet = re.sub(r\"SNCTIONS\", \"sanctions\", tweet)\n    tweet = re.sub(r\"BidTime\", \"Bid Time\", tweet)\n    tweet = re.sub(r\"crunchysensible\", \"crunchy sensible\", tweet)\n    tweet = re.sub(r\"RandomActsOfRomance\", \"Random acts of romance\", tweet)\n    tweet = re.sub(r\"MomentsAtHill\", \"Moments at hill\", tweet)\n    tweet = re.sub(r\"eatshit\", \"eat shit\", tweet)\n    tweet = re.sub(r\"liveleakfun\", \"live leak fun\", tweet)\n    tweet = re.sub(r\"SahelNews\", \"Sahel News\", tweet)\n    tweet = re.sub(r\"abc7newsbayarea\", \"ABC 7 News Bay Area\", tweet)\n    tweet = re.sub(r\"facilitiesmanagement\", \"facilities management\", tweet)\n    tweet = re.sub(r\"facilitydude\", \"facility dude\", tweet)\n    tweet = re.sub(r\"CampLogistics\", \"Camp logistics\", tweet)\n    tweet = re.sub(r\"alaskapublic\", \"Alaska public\", tweet)\n    tweet = re.sub(r\"MarketResearch\", \"Market Research\", tweet)\n    tweet = re.sub(r\"AccuracyEsports\", \"Accuracy Esports\", tweet)\n    tweet = re.sub(r\"TheBodyShopAust\", \"The Body Shop Australia\", tweet)\n    tweet = re.sub(r\"yychail\", \"Calgary hail\", tweet)\n    tweet = re.sub(r\"yyctraffic\", \"Calgary traffic\", tweet)\n    tweet = re.sub(r\"eliotschool\", \"eliot school\", tweet)\n    tweet = re.sub(r\"TheBrokenCity\", \"The Broken City\", tweet)\n    tweet = re.sub(r\"OldsFireDept\", \"Olds Fire Department\", tweet)\n    tweet = re.sub(r\"RiverComplex\", \"River Complex\", tweet)\n    tweet = re.sub(r\"fieldworksmells\", \"field work smells\", tweet)\n    tweet = re.sub(r\"IranElection\", \"Iran Election\", tweet)\n    tweet = re.sub(r\"glowng\", \"glowing\", tweet)\n    tweet = re.sub(r\"kindlng\", \"kindling\", tweet)\n    tweet = re.sub(r\"riggd\", \"rigged\", tweet)\n    tweet = re.sub(r\"slownewsday\", \"slow news day\", tweet)\n    tweet = re.sub(r\"MyanmarFlood\", \"Myanmar Flood\", tweet)\n    tweet = re.sub(r\"abc7chicago\", \"ABC 7 Chicago\", tweet)\n    tweet = re.sub(r\"copolitics\", \"Colorado Politics\", tweet)\n    tweet = re.sub(r\"AdilGhumro\", \"Adil Ghumro\", tweet)\n    tweet = re.sub(r\"netbots\", \"net bots\", tweet)\n    tweet = re.sub(r\"byebyeroad\", \"bye bye road\", tweet)\n    tweet = re.sub(r\"massiveflooding\", \"massive flooding\", tweet)\n    tweet = re.sub(r\"EndofUS\", \"End of United States\", tweet)\n    tweet = re.sub(r\"35PM\", \"35 PM\", tweet)\n    tweet = re.sub(r\"greektheatrela\", \"Greek Theatre Los Angeles\", tweet)\n    tweet = re.sub(r\"76mins\", \"76 minutes\", tweet)\n    tweet = re.sub(r\"publicsafetyfirst\", \"public safety first\", tweet)\n    tweet = re.sub(r\"livesmatter\", \"lives matter\", tweet)\n    tweet = re.sub(r\"myhometown\", \"my hometown\", tweet)\n    tweet = re.sub(r\"tankerfire\", \"tanker fire\", tweet)\n    tweet = re.sub(r\"MEMORIALDAY\", \"memorial day\", tweet)\n    tweet = re.sub(r\"MEMORIAL_DAY\", \"memorial day\", tweet)\n    tweet = re.sub(r\"instaxbooty\", \"instagram booty\", tweet)\n    tweet = re.sub(r\"Jerusalem_Post\", \"Jerusalem Post\", tweet)\n    tweet = re.sub(r\"WayneRooney_INA\", \"Wayne Rooney\", tweet)\n    tweet = re.sub(r\"VirtualReality\", \"Virtual Reality\", tweet)\n    tweet = re.sub(r\"OculusRift\", \"Oculus Rift\", tweet)\n    tweet = re.sub(r\"OwenJones84\", \"Owen Jones\", tweet)\n    tweet = re.sub(r\"jeremycorbyn\", \"Jeremy Corbyn\", tweet)\n    tweet = re.sub(r\"paulrogers002\", \"Paul Rogers\", tweet)\n    tweet = re.sub(r\"mortalkombatx\", \"Mortal Kombat X\", tweet)\n    tweet = re.sub(r\"mortalkombat\", \"Mortal Kombat\", tweet)\n    tweet = re.sub(r\"FilipeCoelho92\", \"Filipe Coelho\", tweet)\n    tweet = re.sub(r\"OnlyQuakeNews\", \"Only Quake News\", tweet)\n    tweet = re.sub(r\"kostumes\", \"costumes\", tweet)\n    tweet = re.sub(r\"YEEESSSS\", \"yes\", tweet)\n    tweet = re.sub(r\"ToshikazuKatayama\", \"Toshikazu Katayama\", tweet)\n    tweet = re.sub(r\"IntlDevelopment\", \"Intl Development\", tweet)\n    tweet = re.sub(r\"ExtremeWeather\", \"Extreme Weather\", tweet)\n    tweet = re.sub(r\"WereNotGruberVoters\", \"We are not gruber voters\", tweet)\n    tweet = re.sub(r\"NewsThousands\", \"News Thousands\", tweet)\n    tweet = re.sub(r\"EdmundAdamus\", \"Edmund Adamus\", tweet)\n    tweet = re.sub(r\"EyewitnessWV\", \"Eye witness WV\", tweet)\n    tweet = re.sub(r\"PhiladelphiaMuseu\", \"Philadelphia Museum\", tweet)\n    tweet = re.sub(r\"DublinComicCon\", \"Dublin Comic Con\", tweet)\n    tweet = re.sub(r\"NicholasBrendon\", \"Nicholas Brendon\", tweet)\n    tweet = re.sub(r\"Alltheway80s\", \"All the way 80s\", tweet)\n    tweet = re.sub(r\"FromTheField\", \"From the field\", tweet)\n    tweet = re.sub(r\"NorthIowa\", \"North Iowa\", tweet)\n    tweet = re.sub(r\"WillowFire\", \"Willow Fire\", tweet)\n    tweet = re.sub(r\"MadRiverComplex\", \"Mad River Complex\", tweet)\n    tweet = re.sub(r\"feelingmanly\", \"feeling manly\", tweet)\n    tweet = re.sub(r\"stillnotoverit\", \"still not over it\", tweet)\n    tweet = re.sub(r\"FortitudeValley\", \"Fortitude Valley\", tweet)\n    tweet = re.sub(r\"CoastpowerlineTramTr\", \"Coast powerline\", tweet)\n    tweet = re.sub(r\"ServicesGold\", \"Services Gold\", tweet)\n    tweet = re.sub(r\"NewsbrokenEmergency\", \"News broken emergency\", tweet)\n    tweet = re.sub(r\"Evaucation\", \"evacuation\", tweet)\n    tweet = re.sub(r\"leaveevacuateexitbe\", \"leave evacuate exit be\", tweet)\n    tweet = re.sub(r\"P_EOPLE\", \"PEOPLE\", tweet)\n    tweet = re.sub(r\"Tubestrike\", \"tube strike\", tweet)\n    tweet = re.sub(r\"CLASS_SICK\", \"CLASS SICK\", tweet)\n    tweet = re.sub(r\"localplumber\", \"local plumber\", tweet)\n    tweet = re.sub(r\"awesomejobsiri\", \"awesome job siri\", tweet)\n    tweet = re.sub(r\"PayForItHow\", \"Pay for it how\", tweet)\n    tweet = re.sub(r\"ThisIsAfrica\", \"This is Africa\", tweet)\n    tweet = re.sub(r\"crimeairnetwork\", \"crime air network\", tweet)\n    tweet = re.sub(r\"KimAcheson\", \"Kim Acheson\", tweet)\n    tweet = re.sub(r\"cityofcalgary\", \"City of Calgary\", tweet)\n    tweet = re.sub(r\"prosyndicate\", \"pro syndicate\", tweet)\n    tweet = re.sub(r\"660NEWS\", \"660 NEWS\", tweet)\n    tweet = re.sub(r\"BusInsMagazine\", \"Business Insurance Magazine\", tweet)\n    tweet = re.sub(r\"wfocus\", \"focus\", tweet)\n    tweet = re.sub(r\"ShastaDam\", \"Shasta Dam\", tweet)\n    tweet = re.sub(r\"go2MarkFranco\", \"Mark Franco\", tweet)\n    tweet = re.sub(r\"StephGHinojosa\", \"Steph Hinojosa\", tweet)\n    tweet = re.sub(r\"Nashgrier\", \"Nash Grier\", tweet)\n    tweet = re.sub(r\"NashNewVideo\", \"Nash new video\", tweet)\n    tweet = re.sub(r\"IWouldntGetElectedBecause\", \"I would not get elected because\", tweet)\n    tweet = re.sub(r\"SHGames\", \"Sledgehammer Games\", tweet)\n    tweet = re.sub(r\"bedhair\", \"bed hair\", tweet)\n    tweet = re.sub(r\"JoelHeyman\", \"Joel Heyman\", tweet)\n    tweet = re.sub(r\"viaYouTube\", \"via YouTube\", tweet)\n    return tweet","b3696a48":"# function definitions are in hidden cells above (long)\ndef cleaning_1(data):\n    data['text_no_contr'] = data['text'].apply(decontracted) # removing contractions\n    data['text_clean'] = data['text_no_contr'].apply(special_chars) # correcting special characters\n    data['text'] = data['text_clean'].apply(various) # applying remaining cleaning functions\n\ncleaning_1(train_data)\ncleaning_1(test_data)","b26a3112":"from nltk.tokenize import sent_tokenize, word_tokenize, regexp_tokenize # functions for standard tokenisation\nfrom nltk.tokenize import TweetTokenizer # function for tweets tokenization","2582c12e":"train_data.head(20)","4fea327b":"%%time\n\ntknzr = TweetTokenizer() # initialization of Tweet Tokenizer\n\ndef mean_words_length(text):\n    words = word_tokenize(text)\n    word_lengths = [len(w) for w in words]\n    return round(np.mean(word_lengths),1)\n\ndef features_1(data):\n    # words count\n    data['words_count'] = data['text'].apply(lambda x: len(tknzr.tokenize(x)))\n    # numbers count\n    numbers_regex = r\"(\\d+\\.?,?\\s?\\d+)\"\n    data['numbers_count'] = data['text'].apply(lambda x: len(regexp_tokenize(x, numbers_regex)))\n    # hashtags count\n    hashtags_regex = r\"#\\w+\"\n    data['hashtags_count'] = data['text'].apply(lambda x: len(regexp_tokenize(x, hashtags_regex)))\n    # mentions count\n    mentions_regex = r\"@\\w+\"\n    data['mentions_count'] = data['text'].apply(lambda x: len(regexp_tokenize(x, mentions_regex)))\n    # url count\n    data['url_count'] = data['text'].apply(lambda x: len([w for w in str(x).lower().split() if 'http' in w or 'https' in w]))\n    # mean words length\n    data['mean_words_length'] = data['text'].apply(mean_words_length)\n    # mean words length\n    data['characters_count'] = data['text'].apply(lambda x: len(x))\n\nfeatures_1(train_data)\nfeatures_1(test_data)","0b50d9a9":"train_data.head()","76478f56":"test_data.head()","62a52bfc":"fig, (ax1,ax2,ax3) = plt.subplots(1,3, figsize=(16,5))\nsns.distplot(train_data[train_data['target']==1]['words_count'], label='True', ax=ax1)\nsns.distplot(train_data[train_data['target']==0]['words_count'], label='Fake', ax=ax1)\nax1.legend()\n\nsns.distplot(train_data[train_data['target']==1]['mean_words_length'], label='True', ax=ax2)\nsns.distplot(train_data[train_data['target']==0]['mean_words_length'], label='Fake', ax=ax2)\nax2.legend()\n\nsns.distplot(train_data[train_data['target']==1]['characters_count'], label='True', ax=ax3)\nsns.distplot(train_data[train_data['target']==0]['characters_count'], label='Fake', ax=ax3)\nax3.legend()\n\nplt.show()","43283b96":"fig, (ax1,ax2,ax3,ax4) = plt.subplots(1,4, figsize=(16,5))\n\nsns.countplot(x='numbers_count', hue='target', data=train_data, ax=ax1)\nax1.legend(labels=['Fake','True'], loc=1)\n\nsns.countplot(x='hashtags_count', hue='target', data=train_data, ax=ax2)\nax2.legend(labels=['Fake','True'], loc=1)\nax2.yaxis.label.set_visible(False)\n\nsns.countplot(x='mentions_count', hue='target', data=train_data, ax=ax3)\nax3.legend(labels=['Fake','True'], loc=1)\nax3.yaxis.label.set_visible(False)\n\nsns.countplot(x='url_count', hue='target', data=train_data, ax=ax4)\nax4.legend(labels=['Fake','True'], loc=1)\nax4.yaxis.label.set_visible(False)\n\nplt.show()","c3838b20":"from nltk.corpus import stopwords\nimport string\n\ndef features_2(data):\n    # lowercase tokens\n    data['lowercase_bag_o_w'] = data['text'].apply(lambda x: [w for w in tknzr.tokenize(x.lower())])\n    # stopwords\n    data['stopwords'] = data['text_no_contr'].apply(lambda x: [t for t in x if t in stopwords.words('english')])\n    # stopwords count\n    data['stopwords_count'] = data['stopwords'].apply(lambda x: len(x))\n    # alpha words only (excludes mentions and hashtags)\n    data['alpha_only'] = data['lowercase_bag_o_w'].apply(lambda x: [t for t in x if t.isalpha()])\n    # counts of alpha words only\n    data['alpha_count'] = data['alpha_only'].apply(lambda x: len(x))\n    # counts of punctuation marks only\n    punctuation_regex = r\"[^\\w\\s]\"\n    data['punctuation_count'] = data['text'].apply(lambda x: len(regexp_tokenize(x, punctuation_regex)))\n\nfeatures_2(train_data)\nfeatures_2(test_data)","e74bd14d":"train_data.head()","8b76cfe9":"fig, (ax1,ax2,ax3,ax4) = plt.subplots(1,4, figsize=(16,6))\nsns.distplot(train_data[train_data['target']==1]['stopwords_count'], label='True', ax=ax1)\nsns.distplot(train_data[train_data['target']==0]['stopwords_count'], label='Fake', ax=ax1)\nax1.legend()\n\nsns.distplot(train_data[train_data['target']==1]['alpha_count'], label='True', ax=ax2)\nsns.distplot(train_data[train_data['target']==0]['alpha_count'], label='Fake', ax=ax2)\nax2.legend()\n\nsns.distplot(train_data[train_data['target']==1]['punctuation_count'], label='True', ax=ax3)\nsns.distplot(train_data[train_data['target']==0]['punctuation_count'], label='Fake', ax=ax3)\nax3.legend()\n\nsns.distplot(train_data[train_data['target']==1]['url_count'], label='True', ax=ax4)\nsns.distplot(train_data[train_data['target']==0]['url_count'], label='Fake', ax=ax4)\nax4.legend()\nplt.show()","0a46ddb1":"from PIL import Image\n\nalpha_only_cloud = \" \".join(train_data['alpha_only'].explode())\n\n# defining cloud of words\ncloud_words_raw = \" \".join(review for review in train_data.text)\nstopwords = set(STOPWORDS)\nstopwords.update([\"http\", \"https\",\"co\",\"com\",\"amp\"])\n\n# creating cloud of words\nfig, ax1 = plt.subplots(figsize=(8,6))\nwordcloud = WordCloud(stopwords=stopwords, background_color=\"white\",height=300, contour_width=3).generate(cloud_words_raw)\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()","0822c1ca":"# creating cloud of words\nfig, ax1 = plt.subplots(figsize=(10,6))\nwordcloud = WordCloud(stopwords=stopwords, background_color=\"white\",height=300, contour_color='firebrick', contour_width=3).generate(alpha_only_cloud)\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()","08e35eeb":"from nltk.util import bigrams\n\ndef extract_bigrams(data):\n    # this function extracts bigrams from alphanumeric tweets without stopwords\n    data['alpha_only_clean'] = data['alpha_only'].apply(lambda x: [item for item in x if item not in stopwords])\n    data['bigrams'] = data['alpha_only_clean'].apply(lambda x: list(bigrams(x)))\n    bigrams_list = data['bigrams'].tolist()\n    bigrams_list = list([a for b in bigrams_list for a in b])\n    return bigrams_list\n\ntrain_bigrams = extract_bigrams(train_data)\ntest_bigrams = extract_bigrams(test_data)","d1d145b1":"import collections\n\ndef count_bigrams(bigrams_list):\n    counter_bigrams = collections.Counter(bigrams_list)\n    top30_bigrams = counter_bigrams.most_common(30)\n    labels = [str(r[0]) for r in top30_bigrams]\n    values = [r[1] for r in top30_bigrams]\n    return labels,values\n\ntrain_bgr_labels, train_bgr_values = count_bigrams(train_bigrams)\ntest_bgr_labels, test_bgr_values = count_bigrams(test_bigrams)\n\nfig, ax = plt.subplots(1,2, figsize=(16,6))\nsns.barplot(x=train_bgr_labels, y=train_bgr_values, ax=ax[0])\nsns.barplot(x=test_bgr_labels, y=test_bgr_values, ax=ax[1])\nax[0].tick_params(labelrotation=90)\nax[1].tick_params(labelrotation=90)\nax[0].set_title('Training dataset', fontsize=13)\nax[1].set_title('Testing dataset', fontsize=13)\nplt.show()","7a81c76f":"y_train = train_data.pop('target')\n\ncols_to_drop = ['id','location','bigrams','text','text_no_contr','lowercase_bag_o_w','text_clean','stopwords','alpha_only','alpha_only_clean']\nX_train = train_data.drop(cols_to_drop, axis=1)\nX_train.head()","ae5c38c3":"X_train = pd.get_dummies(X_train, prefix=['key'], columns=['keyword'])\nX_train","7d14fb51":"X_test = test_data.drop(cols_to_drop, axis=1)\nX_test = pd.get_dummies(X_test, prefix=['key'], columns=['keyword'])","e586cd46":"print('After pre-processing there are {} rows and {} columns in a training dataset.'.format(X_train.shape[0],X_train.shape[1]))\nprint('After pre-processing there are {} rows and {} columns in a testing dataset.'.format(X_test.shape[0],X_test.shape[1]))","b890e92d":"from xgboost import XGBClassifier\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import StratifiedKFold\n\nxgb = XGBClassifier(learning_rate=0.02, n_estimators=600, objective='binary:logistic',\n                    silent=True, nthread=1)","213df7c6":"# A parameter grid for XGBoost\nparams_1 = {\n        'min_child_weight': [1, 5, 10],\n        'gamma': [0.5, 1],\n        'subsample': [0.6, 0.8, 1.0],\n        'colsample_bytree': [0.8, 1.0],\n        'max_depth': [5, 6, 7]\n        }","1f67b8cd":"%%time\n\nfolds = 3  # number of folds to be used\n\nskf = StratifiedKFold(n_splits=folds, shuffle = True, random_state = 1)  # define a stratified K-Fold to preserve percentage of each target class\n\nrandom_search_1 = RandomizedSearchCV(xgb, param_distributions=params_1, n_iter=4, scoring=['roc_auc','accuracy','recall','precision'],\n                                   n_jobs=-1, cv=skf.split(X_train,y_train), verbose=2, random_state=1001, refit='roc_auc')\n\nrandom_search_1.fit(X_train, y_train)\nrandom_search_1.best_params_","802df996":"def results_summary(classifier):\n    roc_auc_results = classifier.cv_results_['mean_test_roc_auc']\n    loc = np.where(roc_auc_results == np.amax(roc_auc_results))[0][0]\n\n    rs_roc_auc = classifier.cv_results_['mean_test_roc_auc'][loc]\n    rs_prec = classifier.cv_results_['mean_test_precision'][loc]\n    rs_recall = classifier.cv_results_['mean_test_recall'][loc]\n    rs_accur = classifier.cv_results_['mean_test_accuracy'][loc]\n\n    print(\"ROC_AUC = {:.3f}\".format(rs_roc_auc))\n    print(\"Precision = {:.3f}\".format(rs_prec))\n    print(\"Recall = {:.3f}\".format(rs_recall))\n    print(\"Accuracy = {:.3f}\".format(rs_accur))\n\n    return [rs_roc_auc,rs_prec,rs_recall,rs_accur] # return array for the final summary\n\nxgb_results = results_summary(random_search_1)","4eb5bbcb":"xgb_best = random_search_1.best_estimator_","38f16505":"from xgboost import plot_importance\nplot_importance(xgb_best, max_num_features=15) # top 15 most important features\nplt.show()","71ef9f70":"from sklearn.ensemble import AdaBoostClassifier\n\nada = AdaBoostClassifier()\n\nparams_2 = {\n        'n_estimators': np.arange(100,1200,100),\n        'learning_rate': np.arange(0.1,1.1,0.2),\n        }","d41da19e":"%%time\n\nrandom_search_2 = RandomizedSearchCV(ada, param_distributions=params_2, n_iter=4, scoring=['roc_auc','accuracy','recall','precision'],\n                                   n_jobs=-1, cv=skf.split(X_train,y_train), verbose=3, random_state=1001, refit='roc_auc')\nrandom_search_2.fit(X_train, y_train)\n\nada_best = random_search_2.best_estimator_\nada_results = results_summary(random_search_2)","7acd7f18":"ada_best.feature_importances_[:10]","aa484e18":"X_train.columns[:10]","9e22b3c2":"from sklearn.ensemble import RandomForestClassifier\n\nrfc = RandomForestClassifier(n_jobs=-1)\n\nparams_3 = {\n        'n_estimators': np.arange(100,1000,100),\n        'max_depth': np.arange(30,110,10),\n        'bootstrap': [True, False]\n        }","7cab8005":"random_search_3 = RandomizedSearchCV(rfc, param_distributions=params_3, n_iter=8, scoring=['roc_auc','accuracy','recall','precision'],cv=skf.split(X_train,y_train), verbose=3, random_state=1001, refit='roc_auc',n_jobs=-1)\nrandom_search_3.fit(X_train, y_train)\n\nrfc_best = random_search_3.best_estimator_\nrfc_results = results_summary(random_search_3)","6d1bcb66":"from sklearn.ensemble import ExtraTreesClassifier\n\netc = ExtraTreesClassifier(n_jobs=-1)\n\nparams_4 = {\n        'n_estimators': np.arange(20,1100,100),\n        'max_depth': np.arange(30,130,10),\n        'bootstrap': [True, False]\n        }","69ea39c0":"%%time\n\nrandom_search_4 = RandomizedSearchCV(etc, param_distributions=params_4, n_iter=8, scoring=['roc_auc','accuracy','recall','precision'],\n                                     cv=skf.split(X_train,y_train), verbose=2, random_state=1001, refit='roc_auc', n_jobs=-1)\nrandom_search_4.fit(X_train, y_train)\n\netc_results = results_summary(random_search_4)\netc_best = random_search_4.best_estimator_","1412f7e1":"from sklearn.ensemble import VotingClassifier\nfrom sklearn.model_selection import cross_validate","812e5d9f":"%%time\n\nensemble_1 = VotingClassifier(estimators=[(\"XGB\",xgb_best),(\"ADA\",ada_best)], voting=\"soft\")\n\ncv_results_1 = cross_validate(ensemble_1, X_train, y_train, cv=3, scoring=(\"roc_auc\",\"precision\",\"recall\",\"accuracy\"),\n                              return_train_score=True)\n\nprint(\"ROC_AUC = {:.3f}\".format(cv_results_1[\"test_roc_auc\"].mean()))\nprint(\"Precision = {:.3f}\".format(cv_results_1[\"test_precision\"].mean()))\nprint(\"Recall = {:.3f}\".format(cv_results_1[\"test_recall\"].mean()))\nprint(\"Accuracy = {:.3f}\".format(cv_results_1[\"test_accuracy\"].mean()))\n\nens1_results =[cv_results_1[\"test_roc_auc\"].mean(),cv_results_1[\"test_precision\"].mean(),\n              cv_results_1[\"test_recall\"].mean(), cv_results_1[\"test_accuracy\"].mean()]","3b977390":"from sklearn.tree import DecisionTreeClassifier\n\nclf1 = XGBClassifier(learning_rate=0.02, objective='binary:logistic',  silent=True)\nclf2 = ExtraTreesClassifier()\nclf3 = AdaBoostClassifier()\n\nens_2 = VotingClassifier(estimators = [('xgb', clf1), ('etc', clf2), ('ada', clf3)], voting = 'soft')","54944c43":"%%time\n\nparams = {'xgb__n_estimators': np.arange(500,1200,50), 'xgb__max_depth': [10,20,30],\n         'etc__n_estimators': np.arange(500,2000,100), 'etc__max_depth': np.arange(10,130,10),\n         'ada__n_estimators': np.arange(400,800,50), 'ada__learning_rate': [0.8,1.0]}\n\ncv_results_2 = RandomizedSearchCV(estimator=ens_2, param_distributions=params, n_iter=128, scoring=['roc_auc','accuracy','recall','precision'],\n                          cv=skf.split(X_train,y_train), verbose=3, random_state=10, refit='roc_auc', n_jobs=-1)\n\ncv_results_2.fit(X_train, y_train)\n\nens2_results = results_summary(cv_results_2)","d129c512":"ens2_best = cv_results_2.best_estimator_","5016d3b2":"cv_results_2.best_params_","9eb13faa":"summary = pd.DataFrame({\"XGB\":xgb_results,\"ADA\":ada_results,\"RFC\":rfc_results,\"ETC\":etc_results,\n                        \"Ens_1\":ens1_results,\"Ens_2\":ens2_results},\n                      index=[\"ROC_AUC\",\"Precision\",\"Recall\",\"Accuracy\"])\nsummary","6c3ee4a3":"predictions = ens2_best.predict(X_test)","5a476f3e":"output = pd.DataFrame({'id': test_data.id,\n                       'target': predictions})\noutput.to_csv('submission.csv', index = False)\nprint('submission saved!')","2ea31691":"output.head()","90654f53":"<a id='EDA'><\/a>\n## 2. Exploratory Data Analysis <a href='#Top' style=\"text-decoration: none;\">^<\/a><br>  \n\nThe first and crucial part in any data science project is to understand data we are working with. Therefore in this section we will perform basic Exploratory Data Analysis checking (among others) variables value ranges, types and distributions.","5206aad5":"Histograms above show that there is a difference between number \u00f3f punctuation marks between false and true tweets: false tend to have less of them.","5aa2f197":"Below there is barplot showing true ratio (true to false count) for TOP30 most *true* keywords.","86767a02":"A barchart above shows that fake tweets have bigger count of numbers, hashtags amd mentions. It may be a way to increase the spreading speed of fake news (my theory).","5ddc17a5":"\"XGBoost is an optimized distributed gradient boosting library designed to be highly efficient, flexible and portable. It implements machine learning algorithms under the Gradient Boosting framework. XGBoost provides a parallel tree boosting (also known as GBDT, GBM) that solve many data science problems in a fast and accurate way. The same code runs on major distributed environment (Hadoop, SGE, MPI) and can solve problems beyond billions of examples.\" - XGBoost [documentation](https:\/\/xgboost.readthedocs.io\/en\/latest\/).","6fedf4f8":"Below there are 5 new features that will be created:\n1. words count (all)\n2. numbers count\n3. hashtags count\n4. mentions count\n5. mean length of words  \n\nTo create them we will need tokenizers from the NLTK library. Analysing Tweeter content is so popular task that NLTK contains a dedicated tokenizer for tweets.","1b464e29":"## If you liked this notebook ---> **UPVOTE**.","626a133a":"Below threre are new additional columns that will be created:\n1. `lowercase_bag_o_w` - lowercase bag of words\n2. `stopwords`\n3. `stopwords_count`\n4. `alpha_omly` - bag of word with only alphabetical characters only\n5. `alpha_count` - count of alphabetical characters\n6. `punctuation_count`\nSome of these columns may be dropped later if necessary.  ","678aa21e":"First we will build simple voting ensembles. The idea behind is to use independently-built models for voting in order to predict the labels. This type of ensemble is called bagging and it is aiming to reduce variance (not bias) and it's a good idea if you suspect over-fitting. Let's investigate what will be effect on our calssifiers.\n\nSimple voting ensemble of boosted algorithms","0a3f8cf9":"The cleaing function below can be written as a one-liner but I decomposed it for easier debugging and maintanance.","e1e7266b":"Reading train and test datasets.","55434bb5":"Our target variable is binary and not well balanced but for now, just for simplicity, we will leave it as it is. Alternatively, we can upsample\/downsample target variable groups.  \n\nLet's check how much data is missing in remaining columns.","10383375":"Let's read some tweets.","7954df31":"**BIGRAMS**","3cb9bd7e":"### 4.1 XGBoost","1d702808":"### 4.4 Extra Tree Classifier\n\nExtra Tree Classifier sklearn API documentation: [LINK](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.ExtraTreesClassifier.html?highlight=extra%20tree%20classifier#sklearn.ensemble.ExtraTreesClassifier)","63a52aed":"<a id='Building ensembles'><\/a>\n## 5. Building ensembles<a href='#Top'>^<\/a><br>  ","38ce73f8":"**SUMMARY**\n\nUnder construction....","68cb7311":"Checking target variables (binary) distribution.","ac6f0a45":"Below there is a cloud of words from alphabetical entities only!","2a665841":"<a id='XGB'><\/a>\n## 4. Baseline models<a href='#Top'>^<\/a><br>  \n\nIn this section we will create and optimise a few baseline models, both boosted (XGB, AdaBoost) and bagged (Random Forest).\n\nFirst a basic XGBoost classification model will be created. XGBoost is modern state-of-the art boosting algorithm, very popular in many tasks and has a conveninent Scikit-Learn API. Check this [article](https:\/\/towardsdatascience.com\/getting-started-with-xgboost-in-scikit-learn-f69f5f470a97) on Toward Data Science if you want to learn more details about it.","46fd95f8":"Below we will generate a wordcloud from raw text excluding stopwords.","edec039b":"### 4.2 Ada Boost Classifier\n\nAda Boost Classifier sklearn API documentation: [LINK](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.AdaBoostClassifier.html)","b0fc597b":"Printing all scores for the estimator with the best ROC AUC.","bbf089fc":"Apparently tweets containing keywords **derailment**, **wreckage** and **debris** are assosiated with true tweets.  \n\nThe same for TOP30 most *fake* keywords.","f444d5f8":"Randomized search (alternatively grid search). Number of iteration has beed reduced to 4 to meet notebook maximum runtime on Kaggle (later stages require much more time).","bfd0b872":"<a id='Top'><\/a>\n<center>\n<h1><u>NLP with disaster Tweets - True or Fake?<\/u><\/h1>\n<h3>Preprocessing, XGBoost model and ensemble<\/h3>\n<\/center>\n\n---\n\n\n<!-- Start of Unsplash Embed Code - Centered (Embed code by @BirdyOz)-->\n<div style=\"width:60%; margin: 20px 20% !important;\">\n    <img src=\"https:\/\/images.unsplash.com\/photo-1578652229330-05f320786aa9?ixlib=rb-1.2.1&amp;q=80&amp;fm=jpg&amp;crop=entropy&amp;cs=tinysrgb&amp;w=720&amp;fit=max&amp;ixid=eyJhcHBfaWQiOjEyMDd9\" class=\"img-responsive img-fluid img-med\" alt=\"reflection of light on body of water at night \" title=\"reflection of light on body of water at night \">\n    <div class=\"text-muted\" style=\"opacity: 0.5\">\n        <small><a href=\"https:\/\/unsplash.com\/photos\/nVYEechGqqM\" target=\"_blank\">Photo<\/a> by <a href=\"https:\/\/unsplash.com\/@sippakorn\" target=\"_blank\">@sippakorn<\/a> on <a href=\"https:\/\/unsplash.com\" target=\"_blank\">Unsplash<\/a>, accessed 24\/09\/2020<\/small>\n    <\/div>\n<\/div>\n<!-- End of Unsplash Embed code -->\n\n### INTRODUCTION\n\nThe problem of fake news and disinformation was known much more before the advent on the Internet. It was used to mislead the enemy or to obtain an advantage in politics or economy over competitors. \n\n**Sun Tzu** wrote in his \"The Art of War\":   \n**\u201cThe whole secret lies in confusing the enemy, so that he cannot fathom our real intent\u201d**.  \nIt was written in 5th century BC but the principle stays still valid nowadays. The only difference is that currently, the Internet is a new battlefield. \n\nEasy access and popularity of various social media networks like Facebook or Tweeter give a terrific opportunity for dissemination of fake news. Of course, this applies also to various blogs and web pages. Many of these fake news can then sneak into mainstream news distribution channels like TV or the press. This happens currently more and more frequently \u2013 you can read many interesting examples on [Fighting Fake](http:\/\/www.fightingfake.org.uk\/fake-news#ExamplesOfFakeNews-3) site. The problem became so widespread that this became a topic in cinematography as well \u2013 I recommend you watching for example \u201c[The Hater](https:\/\/www.imdb.com\/title\/tt9506474\/)\u201d (2020) available on Netflix. \n\nThe uncontrolled spread of fake news imposes a real threat not only individual politics and institutions but to all people as a society. Confusing the attacked nation or group of people may have an aim to divide them so they cannot be united. According to old rule *\"divide and conquer\"* this is a remarkably good move fo the attacker.\n\nAnd that the reason it is of the utmost importance to protect us from this danger. There are many ways to achieve that, from the basics like problem awareness and news verification skills to advanced analytics backend systems. \n\nSo here we are. Twitter false news dataset on which we can check or skills and learn how to separate real from fake news. This is a standard supervised classification task:  \n* **Supervised** - the labels are provided and included in a training dataset.\n* **Classification** - the labels are of binary type, 1 (true) and 0 (false).  \n\nWe will be working with text so this task requires NLP - \"Natural Language Processing\".\n\nNote that the aim of this notebook is not to get the highest score in the competition as we will be working with \"classic\" scikit-learn machine learning algorithms. Much better results in this case you can obtain using deep learning approach with GloVe, BERT, etc.\n\n\n### SECTIONS:  \n1. [Reading Data](#Reading)<br>\n2. [Exploratory Data Analysis](#EDA)<br>\n3. [Tokenization and Features Engineering](#Tokenization)<br>\n4. [Baseline models](#XGB)<br>\n    4.1 XGB  \n    4.2 AdaBoost  \n    4.3 Random Forest Classifier  \n    4.4 Extra Tree Classifier  \n5. [Ensemble](#Ensemble)<br>","69eacd32":"A keyword indicating always (in a training set) a fake tweet is *aftershock*.","86ca6bd8":"<a id='Reading'><\/a>\n## 1. Reading data <a href='#Top' style=\"text-decoration: none;\">^<\/a><br>  \n\nDatabase is available in Comma Separated Values (.csv) file and can be easily read with python's pandas library. For visualisation we will use matplotlib, seaborn and wordcloud libraries.","18d3566b":"Histograms above show that a word count of both types of tweets is similar but fake ones tend to have longer words in.","a4ff36c0":"Below there is a long cleaning code taken from [noteboook by Gunes Evitan](https:\/\/www.kaggle.com\/gunesevitan\/nlp-with-disaster-tweets-eda-cleaning-and-bert). It's focused on cleaning this specific dataset - hastags, usernames, etc. It's improving quality of this static database but with a dynamic real-life dataset it's not the recommended approach. However, in the cited notebook the approach to the problem is different - by using vectors created on a specific corpuses (e.g. Wikipedia) and it definitely helps.\n\nI'll use this cleaning section below (hidden cell).","004fdd57":"Parameters to be investigated (arbitrary chosen).","56a96f17":"<a id='Tokenization'><\/a>\n## 3. Tokenization and Features Engineering<a href='#Top'>^<\/a><br>\n\nIn this phase we will pre-process the database by cleaning and feature engineering to generate new and usefull features for ML algorithms. \n\nA text decomposition (like tokenisation, stemminig and lemmatiation) is the most important operation and can be performed in many ways. Here open-source and popular NLTK library will be used (but there are many other like SpaCy, CoreNLP, gensim). The tokenisation in this notebook will use a lot of regex (regular expressions).  \n\nOverall text preprocessing can include operations like:\n* Tokenisation  - splitting text into a list of tokens\n* Stemming - reduction of words to their roots, e.g. \"does\", \"did\", \"done\" will be all reduced to their root: \"do\"\n* Stop Words Removal - many times words like \"a\", \"an\", \"for\", \"from\" are insignifican and can be removed to reduce the noise\n* Features Extraction","93469932":"**Text Cleaning**\n\nNow we will clean tweets by deconstructing contractions like \"I'm\" to \"I am\", \"won't\" to \"will not\", etc. The entire process of cleaning tweets can be very complex and is very well described in [this Kaggle noteboook by Gunes Evitan](https:\/\/www.kaggle.com\/gunesevitan\/nlp-with-disaster-tweets-eda-cleaning-and-bert).","75f3076e":"### 4.3 Random Forest Classifier\n\nRandom Forest Classifier sklearn API documentation: [LINK](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.RandomForestClassifier.html?highlight=random%20forest#sklearn.ensemble.RandomForestClassifier)"}}