{"cell_type":{"72446f4a":"code","c65df656":"code","93771d54":"code","c37c2ef5":"code","7ba5b0ef":"code","40be0cd9":"code","9fda8339":"code","b5cc19ce":"code","74d95133":"code","a17e3ba8":"code","46b8d214":"code","2b839391":"code","2dbed193":"code","679a9f65":"code","0a5216bf":"code","91e26cb2":"markdown","64451b6a":"markdown","f1be56a9":"markdown","d8a0ab1f":"markdown","248018d2":"markdown","7f3f8442":"markdown","b047abb8":"markdown","24096eae":"markdown","8e5816f2":"markdown","67121f5e":"markdown","6ecae354":"markdown","0496df49":"markdown","a6694cc4":"markdown","add334a8":"markdown"},"source":{"72446f4a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom keras.models import Sequential\nimport keras.backend as K\nimport random\nfrom keras import regularizers\nfrom keras.layers import Reshape, multiply ,Activation, Conv2D, Multiply, Input, MaxPooling2D, SpatialDropout2D,BatchNormalization, Flatten, Dense, Lambda, Dropout, LSTM,CuDNNGRU,CuDNNLSTM,GRU,Conv1D\nfrom keras.optimizers import SGD, Adam, RMSprop\nimport os\nprint(os.listdir(\"..\/input\/\"))","c65df656":"data=pd.read_csv('..\/input\/bitstampUSD_1-min_data_2012-01-01_to_2018-11-11.csv', delimiter=',')","93771d54":"data.head(5)","c37c2ef5":"data[data.columns.values] = data[data.columns.values].ffill()\ndata[data.columns.values]=data[data.columns.values].fillna(0)","7ba5b0ef":"def preparedata(data, periodtochange=10, periodtopredict=100):\n    data['change']=(data['Weighted_Price']-data['Weighted_Price'].shift(periodtochange))\/data['Weighted_Price']\n    data['+gain']=(data['Close'].shift(-periodtopredict)-data['Close'])\/data['Weighted_Price']*100\n    data['-gain']=(-data['Close'].shift(-periodtopredict)+data['Close'])\/data['Weighted_Price']*100\n    return data","40be0cd9":"data=preparedata(data)\ndata[data.columns.values]=data[data.columns.values].fillna(0)","9fda8339":"datatrain=data[:1990342]\ndataval=data[1990342:-300]","b5cc19ce":"def data_generator(df, batchsize=10240, length=200):\n    xsez=np.array(df['change'])\n    ysez=np.zeros((len(df),3))\n    ysez[:,:2]=np.array(df[['+gain','-gain']])\n    sequences=np.zeros((batchsize, length, 1))\n    answer=np.zeros((batchsize, 3))\n    j=1\n    k=0\n    while True:\n        k=random.randint(1,xsez.shape[0]-length-2)\n        seq=xsez[k:k+length]\n        ans=ysez[k+length, :]\n        sequences[j%batchsize,:,0]=seq\n        answer[j%batchsize,:]=ans\n        j=j+1\n        if j>len(df)-length-2: j=0\n        if j%batchsize==0:\n            yield sequences, answer","74d95133":"genvalregr=data_generator(dataval, batchsize=10240, length=200)\ntoval=next(genvalregr)","a17e3ba8":"def Gain_loss(y_true, y_pred):\n    y_true_f = K.flatten(y_true)\n    y_pred_f = K.flatten(y_pred)\n    toret = -K.mean(y_true_f * y_pred_f)\n    return toret","46b8d214":"biasl1=0\nkernell1=0\nbiasl2=0.01\nkernell2=0.01","2b839391":"model = Sequential()\nmodel.add(Conv1D(16, 32, activation='sigmoid', input_shape=(None, 1), kernel_regularizer=regularizers.l1_l2(l1=kernell1, l2=kernell2) ,bias_regularizer=regularizers.l1_l2(l1=biasl1, l2=biasl2)))\nmodel.add(BatchNormalization())\nmodel.add(GRU(16, kernel_regularizer=regularizers.l1_l2(l1=kernell1, l2=kernell2) ,bias_regularizer=regularizers.l1_l2(l1=biasl1, l2=biasl2)))\nmodel.add(BatchNormalization())\nmodel.add(Dense(32,  activation='sigmoid',kernel_regularizer=regularizers.l1_l2(l1=kernell1, l2=kernell2) ,bias_regularizer=regularizers.l1_l2(l1=biasl1, l2=biasl2)))\nmodel.add(BatchNormalization())\nmodel.add(Dense(32,  activation='sigmoid',kernel_regularizer=regularizers.l1_l2(l1=kernell1, l2=kernell2) ,bias_regularizer=regularizers.l1_l2(l1=biasl1, l2=biasl2)))\nmodel.add(BatchNormalization())\nmodel.add(Dense(3,  activation='softmax',kernel_regularizer=regularizers.l1_l2(l1=kernell1, l2=kernell2) ,bias_regularizer=regularizers.l1_l2(l1=biasl1, l2=biasl2)))\nmodel.compile(loss=Gain_loss, optimizer=Adam(lr=0.001))","2dbed193":"model.summary()","679a9f65":"from keras.callbacks import LearningRateScheduler\ndef annealing(x):\n    initial_lrate = 0.001\n    return initial_lrate\/(x+1)*np.random.rand()\nlrate = [LearningRateScheduler(annealing)]","0a5216bf":"K.set_value(model.optimizer.lr, 0.0001)\nhist=model.fit_generator(data_generator(datatrain, batchsize=1024, length=200),\n    samples_per_epoch = 10, \n    epochs = 10,\n    validation_data=(toval[0], toval[1]),\n    verbose=1,\n    callbacks=lrate,\n    )","91e26cb2":"Check the number of parameters is not huge to avoid the overfitting","64451b6a":"Create generator in order to save the operative memory during training. \nHere we got the randomisation inside the train and validation data.\nEvery labels have 3 dimensions - 1st - gain in case of 'Buy'  at this moment, second in case of 'Sell', last equals to 0 gain in case of 'Do Nothing'","f1be56a9":"remove the Nan we get after calculations with shift","d8a0ab1f":"Build the function for the Data calculation. (+gain and -gain is the income if we by or sell BTC - model that can be easilly improved by looking at High and Low prices for +-gain instead of Close  in order to take in account the spreads in case of closing the deals \"by Market\" ) +-gains are in percentes related to Weighted_Price","248018d2":"Here is the model for RNN-based Bitcoin price prediction\nThe main steps are the next:\n1. Loading and preprocessing the historical data\nmake training\/validation split\nTrain at first 2\/3 of the samples, validate on the last. Absence of randomization during split is to meet the reality (we can train only on the past)\n2. Build generator for the data tot train RNN\nt is the current time, T is the timeshift\n    Build inputs as a sequences of length L of the Open prices relative change compared to time -T\n    Build outputs as price change at moment +T in future compared to the t\n3, Build RNN\n    First layer is 1D convolutional layer  enhances the learning rate and allow to  find simple dependences \n    Second layer is GRU for detection of more complicated dependences (can be used CuDNNGRU for NVIDIA GPUs)\n    Couple of dense layers\n    Output layer with 3 units and Softmax activation:\n        Output 1 - bet to sell\n        Output 2-  bet to buy\n        Output 3-  bet to do nothing\n    Batch normalization layers speed up the training\n    Loss function is constructed to maximaze the value of (gain) during trading","7f3f8442":"Build the model. Here different activations, length of convolutional core and number of units\/layers can be used","b047abb8":"fill the NaN values","24096eae":" L1 and L2 regularization values for the RNN are necessary in order not to stuck in always 'do nothing' and avoid gradients explosions. L2 regularization used in this version","8e5816f2":"Learning rate can be used as other hyperparameter as well as learning rate change during training is also can be played with, for example","67121f5e":"Create one  batch from validation data ","6ecae354":"make training\/val split","0496df49":"Creation of the Keras Loss as the mean gain per one decicion   ","a6694cc4":"Check the data","add334a8":"Negative loss (-0.0001)  on validation set  can be reached during training with some types of activation\/regularization parameters which means that this model potentially can be used for trading.  \nAnyway in case of  loss>0 on validation, if it is stable, and there is no L1\/L2 regularization, you can just do the opposite of the model output and still use it for decision   (if you are not taking in account spreads during +-gain calculations) o_O"}}