{"cell_type":{"62e8d8ba":"code","3c645e2e":"code","8c158f13":"code","ba0c88e5":"code","54289899":"code","6920e2b4":"code","4d9999d7":"code","c351bc2c":"code","8dc414c5":"code","8fb3121b":"code","972c4dfe":"code","2cd36053":"code","855c3fca":"code","180d8563":"code","068833e3":"code","6402f06a":"code","218eac14":"code","f859ec1e":"code","c1f22cee":"code","d8e2d6d8":"code","5636de64":"code","8997fe07":"code","0ee0375f":"code","f4de089b":"code","4fe6fff3":"code","a54c923e":"code","4006a39e":"code","cddc153c":"code","54333074":"code","e98b0558":"code","2c3a63c9":"markdown","7efcc37d":"markdown","4ce16466":"markdown","d93e6547":"markdown","c453855e":"markdown","fd8b33f5":"markdown","fce15e69":"markdown","cccc7126":"markdown","baf6e3b7":"markdown","6e361863":"markdown","1c07d01d":"markdown","d49d08b2":"markdown","46c883a4":"markdown","243e9571":"markdown","fd99e822":"markdown","cf2295ed":"markdown","40d88664":"markdown","2a6f2dc7":"markdown","4d1b05f0":"markdown","49aabc99":"markdown","c022b554":"markdown","2dc37e7f":"markdown","3fcd4e0a":"markdown","e9dfb323":"markdown"},"source":{"62e8d8ba":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","3c645e2e":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns # data visualization library  \nimport matplotlib.pyplot as plt  # data visualization library  \nimport warnings\nwarnings.filterwarnings(\"ignore\") #ignoring all the warnings","8c158f13":"data=pd.read_csv('\/kaggle\/input\/breast-cancer-wisconsin-data\/data.csv')","ba0c88e5":"data.head() #checking for the data","54289899":"#shape of the data\ndata.shape","6920e2b4":"#datatypes of the columns\ndata.info()","4d9999d7":"# y includes our Target Labels and x includes our features\ny = data.diagnosis                          # M or B \nlist = ['Unnamed: 32','id','diagnosis']\nx = data.drop(list,axis = 1 )\nx.head()","c351bc2c":"x.isnull().sum() #Checking of null values in the data","8dc414c5":"sns.countplot(y)\nB,M=y.value_counts()\nprint('Number of Patient with Malignant Tumor:', M)\nprint('Number of Patient with Benign Tumor:', B)","8fb3121b":"x.describe() #Since we dont know much about the features , we are getting to know about it by the mean and Standard deviation","972c4dfe":"# first ten features\ndata = x\ndata_n_2 = (data - data.mean()) \/ (data.std())              # standardization\ndata = pd.concat([y,data_n_2.iloc[:,0:10]],axis=1)\ndata = pd.melt(data,id_vars=\"diagnosis\",\n                    var_name=\"features\",\n                    value_name='value')\nplt.figure(figsize=(10,10))\nsns.violinplot(x=\"features\", y=\"value\", hue=\"diagnosis\", data=data,split=True, inner=\"quart\")\nplt.xticks(rotation=90)\nplt.show()","2cd36053":"# Second ten features\ndata = pd.concat([y,data_n_2.iloc[:,10:20]],axis=1)\ndata = pd.melt(data,id_vars=\"diagnosis\",\n                    var_name=\"features\",\n                    value_name='value')\nplt.figure(figsize=(10,10))\nsns.violinplot(x=\"features\", y=\"value\", hue=\"diagnosis\", data=data,split=True, inner=\"quart\")\nplt.xticks(rotation=90)\nplt.show()","855c3fca":"# Third ten features\ndata = pd.concat([y,data_n_2.iloc[:,20:31]],axis=1)\ndata = pd.melt(data,id_vars=\"diagnosis\",\n                    var_name=\"features\",\n                    value_name='value')\nplt.figure(figsize=(10,10))\nsns.violinplot(x=\"features\", y=\"value\", hue=\"diagnosis\", data=data,split=True, inner=\"quart\")\nplt.xticks(rotation=90)\nplt.show()","180d8563":"# As an alternative of violin plot, box plot can be used\n# box plots are also useful in terms of seeing outliers\n# In order to show you lets have an example of box plot\n# If you want, you can visualize other features as well.\nplt.figure(figsize=(18,10))\ndata = pd.concat([y,data_n_2],axis=1)\ndata = pd.melt(data,id_vars=\"diagnosis\",\n                    var_name=\"features\",\n                    value_name='value')\nsns.boxplot(x=\"features\", y=\"value\", hue=\"diagnosis\", data=data)\nplt.xticks(rotation=90)\nplt.show()","068833e3":"df = x.loc[:,['radius_worst','perimeter_worst','area_worst']]\ng = sns.pairplot(df)\ng.map_lower(sns.kdeplot, cmap=\"Blues_d\")\ng.map_upper(plt.scatter)\ng.map_diag(sns.kdeplot, lw=3)\nplt.show()","6402f06a":"data = x\ndata_n_2 = (data - data.mean()) \/ (data.std())              # standardization\ndata = pd.concat([y,data_n_2.iloc[:,0:10]],axis=1)\ndata = pd.melt(data,id_vars=\"diagnosis\",\n                    var_name=\"features\",\n                    value_name='value')\nplt.figure(figsize=(10,10))\nsns.swarmplot(x=\"features\", y=\"value\", hue=\"diagnosis\", data=data)\n\nplt.xticks(rotation=60)\nplt.show()","218eac14":"data = pd.concat([y,data_n_2.iloc[:,10:20]],axis=1)\ndata = pd.melt(data,id_vars=\"diagnosis\",\n                    var_name=\"features\",\n                    value_name='value')\nplt.figure(figsize=(10,10))\nsns.swarmplot(x=\"features\", y=\"value\", hue=\"diagnosis\", data=data)\nplt.xticks(rotation=60)\nplt.show()","f859ec1e":"data = pd.concat([y,data_n_2.iloc[:,20:31]],axis=1)\ndata = pd.melt(data,id_vars=\"diagnosis\",\n                    var_name=\"features\",\n                    value_name='value')\nplt.figure(figsize=(10,10))\nsns.swarmplot(x=\"features\", y=\"value\", hue=\"diagnosis\", data=data)\nplt.xticks(rotation=90)\nplt.show()","c1f22cee":"#correlation map\nf,ax = plt.subplots(figsize=(18, 18))\nsns.heatmap(x.corr(), annot=True, linewidths=.5, fmt= '.1f',ax=ax)\nplt.show()","d8e2d6d8":"drop_list1 = ['perimeter_mean','radius_mean','compactness_mean','concave points_mean','radius_se',\n              'perimeter_se','radius_worst','perimeter_worst','compactness_worst','concave points_worst',\n              'compactness_se','concave points_se','texture_worst','area_worst']\nx_1 = x.drop(drop_list1,axis = 1 )        # do not modify x, we will use it later \nx_1.head()","5636de64":"#correlation map\nf,ax = plt.subplots(figsize=(14, 14))\nsns.heatmap(x_1.corr(), annot=True, linewidths=.5, fmt= '.1f',ax=ax)\nplt.show()","8997fe07":"from sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import f1_score,confusion_matrix,classification_report\nfrom sklearn.metrics import accuracy_score\n\n# split data train 70 % and test 30 %\nx_train, x_test, y_train, y_test = train_test_split(x_1, y, test_size=0.3, random_state=42)\n\n#random forest classifier with n_estimators=10 (default)\nclf_rf = RandomForestClassifier(random_state=43)      \nclr_rf = clf_rf.fit(x_train,y_train)\npred=clf_rf.predict(x_test)\nac = accuracy_score(y_test,pred)\nprint('Accuracy is: ',ac)\nprint(\"Classification Report:\\n\",classification_report(y_test,pred))\ncm = confusion_matrix(y_test,pred)\nsns.heatmap(cm,annot=True,fmt=\"d\")\nplt.show()","0ee0375f":"from sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\n# find best scored 5 features\nselect_feature = SelectKBest(chi2, k=5).fit(x_train, y_train)","f4de089b":"print('Score list:', select_feature.scores_)\nprint('Feature list:', x_train.columns)","4fe6fff3":"x_train_2 = select_feature.transform(x_train)\nx_test_2 = select_feature.transform(x_test)\n#random forest classifier with n_estimators=10 (default)\nclf_rf_2 = RandomForestClassifier()      \nclr_rf_2 = clf_rf_2.fit(x_train_2,y_train)\nac_2 = accuracy_score(y_test,clf_rf_2.predict(x_test_2))\nprint('Accuracy is: ',ac_2)\nprint(\"Classification Report:\\n\",classification_report(y_test,clf_rf_2.predict(x_test_2)))\ncm_2 = confusion_matrix(y_test,clf_rf_2.predict(x_test_2))\nsns.heatmap(cm_2,annot=True,fmt=\"d\")\nplt.show()","a54c923e":"from sklearn.feature_selection import RFE\n# Create the RFE object and rank each pixel\nclf_rf_3 = RandomForestClassifier()      \nrfe = RFE(estimator=clf_rf_3, n_features_to_select=5, step=1)\nrfe = rfe.fit(x_train, y_train)","4006a39e":"print('Chosen best 5 feature by rfe:',x_train.columns[rfe.support_])","cddc153c":"from sklearn.feature_selection import RFECV\n\n# The \"accuracy\" scoring is proportional to the number of correct classifications\nclf_rf_4 = RandomForestClassifier() \nrfecv = RFECV(estimator=clf_rf_4, step=1, cv=5,scoring='accuracy')   #5-fold cross-validation\nrfecv = rfecv.fit(x_train, y_train)","54333074":"print('Optimal number of features :', rfecv.n_features_)\nprint('Best features :', x_train.columns[rfecv.support_])","e98b0558":"x_train_2 = rfecv.transform(x_train)\nx_test_2 = rfecv.transform(x_test)\n#random forest classifier with n_estimators=10 (default)\nrfecv_2 = RandomForestClassifier()      \nrfecv_2 = rfecv_2.fit(x_train_2,y_train)\nac_3 = accuracy_score(y_test,rfecv_2.predict(x_test_2))\nprint('Accuracy is: ',ac_2)\nprint(\"Classification Report:\\n\",classification_report(y_test,rfecv_2.predict(x_test_2)))\ncm_3 = confusion_matrix(y_test,rfecv_2.predict(x_test_2))\nsns.heatmap(cm_2,annot=True,fmt=\"d\")\nplt.show()","2c3a63c9":"Accuracy is almost 95% and as it can be seen in confusion matrix, we make few wrong prediction. Now lets see other feature selection methods to find better results.","7efcc37d":"We can see that there are many features which are highly co-related with each other. We can keep one of the feature of such co-relations and drop the others.","4ce16466":"# Conclusion:\n\n Shortly, I tried to show importance of feature selection and data visualization. Default data includes 33 feature but after feature selection we drop this number from 33 to 5 with accuracy 95%. In this kernel I just tried basic things, I am sure with these data visualization and feature selection methods, you can easily exceed the % 95 accuracy. Maybe you can use other classification methods.","d93e6547":"The area_mean feature's max value is 2500 and smoothness_mean features' max 0.16340. Therefore do we need standirdization or normalization before visualization, feature selection, feature extraction or classificaiton","c453855e":"Lets interpret one more thing about plot above, variable of concavity_worst and concave point_worst looks like similar but how can we decide whether they are correlated with each other or not.Even it is the same case with perimeter_worst,radius_worst and area_worst (Not always true but, basically if the features are correlated with each other we can drop one of them)\n \nWe discover one more thing radius_worst, perimeter_worst and area_worst are correlated as it can be seen pair grid plot. We definetely use these discoveries for feature selection.","fd8b33f5":"# Visualization:\n\n In order to visualizate data we are going to use seaborn plots to see for diversity of plots. What I use in real life is mostly violin plot and swarm plot.\n\nBefore violin and swarm plot we need to normalization or standirdization. Because differences between values of features are very high to observe on plot. I plot features in 3 group and each group includes 10 features to observe better.\n\n","fce15e69":"## 2) Univariate feature selection and random forest classification:\n\nIn univariate feature selection, we will use SelectKBest that removes all but the k highest scoring features.\n \nIn this method we need to choose how many features we will use. For example, will k (number of features) be 5 or 10 or 15? The answer is only trying or intuitively. I do not try all combinations but I only choose k = 5 and find best 5 features.","cccc7126":"# **Introduction:**\n\n In this report, I am basically focusing on the Feature Visualization and selection of it. \n We have a Data of patients diagnoised for Breast Cancer.\nBreast cancer is cancer that develops from breast tissue.Signs of breast cancer may include a lump in the breast, a change in breast shape, dimpling of the skin, fluid coming from the nipple, a newly-inverted nipple, or a red or scaly patch of skin.In those with distant spread of the disease, there may be bone pain, swollen lymph nodes, shortness of breath, or yellow skin.\n \n Generally the origin of cancer is from a Tumor,now a person is having cancer if the tunor is Malignant.\n We all know that cancers are basically of two types :\n *  Malignant:If your doctor determines that you have a malignant tumor, that means the mass is cancerous. The word malignant is Latin for \u201cbadly born.\u201d This type of tumor has the ability to multiply uncontrollably, to metastasize (spread) to various parts of the body and invade surrounding tissue.Malignant tumors are formed from abnormal cells that are highly unstable and travel via the blood stream, circulatory system, and lymphatic system. Malignant cells do not have chemical adhesion molecules to anchor  them to the original growth site that benign tumors possess.\n *     Benign:A benign tumor is not a cancerous tumor. Unlike cancer tumors, a non cancerous tumor is unable to spread throughout the body. A non malignant tumor can be serious if they are pressing a primary nerve, a main artery, or compresses brain matter. Overall, benign tumors respond well to treatment and the prognosis is usually favorable.\n\nSome suspected causes of benign tumors include a traumatic injury at the tumor location, chronic inflammation (or long-term stress that leads to inflammation), an undetected infection, or diet.","baf6e3b7":"# Reading and Analysing the Data:","6e361863":"There are only 3 datatypes present in the data, id column is of int64 datatype,one column Diagnosis which is Object and rest are of Float64 datatype.\n\n There are 3 things that take my attention \n * There is an id column that cannot be used for analysis \n * Diagnosis is our Target label \n * Unnamed: 32 feature includes NaN so we do not need it. \n \n So, we can drop id and unnamed:32 column","1c07d01d":"## 4) Recursive feature elimination with cross validation and random forest classification:\n\n Now we will not only find best features but we also find how many features do we need for best accuracy.","d49d08b2":"Best 5 feature to classify is that area_mean, area_se, texture_mean, concavity_worst and concavity_mean. So lets se what happens if we use only these best scored 5 feature.","46c883a4":"Accuracy is almost 93.5% and as it can be seen in confusion matrix, we make few wrong prediction. What we did up to now is that we choose features according to correlation matrix and according to selectkBest method. Although we use 5 features in selectkBest method accuracies look similar. Now lets see other feature selection methods to find better results.","243e9571":"There are 33 columns and 569 rows.","fd99e822":"## 1) Feature selection with correlation and random forest classification:\n\nAs it can be seen in map heat figure radius_mean, perimeter_mean and area_mean are correlated with each other so we will use only area_mean. If you ask how i choose area_mean as a feature to use, well actually there is no correct answer, I just look at swarm plots and area_mean looks like clear for me but we cannot make exact separation among other correlated features without trying. So lets find other correlated features and look accuracy with random forest classifier.\n\nCompactness_mean, concavity_mean and concave points_mean are correlated with each other.Therefore I only choose concavity_mean. Apart from these, radius_se, perimeter_se and area_se are correlated and I only use area_se. radius_worst, perimeter_worst and area_worst are correlated so I use area_worst. Compactness_worst, concavity_worst and concave points_worst so I use concavity_worst. Compactness_se, concavity_se and concave points_se so I use concavity_se. texture_mean and texture_worst are correlated and I use texture_mean. area_worst and area_mean are correlated, I use area_mean.","cf2295ed":"# Importing Libraries:","40d88664":"We can see variance more clear.area_worst in last swarm plot looks like malignant and benign are separated not totaly but mostly. However, smoothness_se in swarm plot 2 looks like malignant and benign are mixed so it is hard to classify while using this feature.","2a6f2dc7":"Finally, we find best 10 features that are texture_mean, area_mean, concavity_mean, area_se, concavity_se, smoothness_se, fractal_dimension_se,smoothness_worst, concavity_worst, symmetry_worst and fractal_dimension_worst for best classification. Lets look at best accuracy with plot.","4d1b05f0":"# Feature Selection and Random Forest Classification:\n\nIn this part we will select feature with different methods that are feature selection with correlation, univariate feature selection, recursive feature elimination (RFE), recursive feature elimination with cross validation (RFECV) and tree based feature selection. We will use random forest classification in order to train our model and predict.","49aabc99":"In swarm plot, I will do three part like violin plot not to make plot very complex appearance. Swarm plot will make it very clear that the festures ae co-related with each other.","c022b554":"After drop correlated features, as it can be seen in below correlation matrix, there are no more correlated features. Actually, I know and you see there is correlation value 0.9 but lets see together what happen if we do not drop it.\n \n Lets use random forest and find accuracy according to chosen features.","2dc37e7f":"Chosen 5 best features by rfe is texture_mean, area_mean, concavity_mean, area_se, concavity_worst. They are exactly similar with previous (selectkBest) method. Therefore we do not need to calculate accuracy again. Shortly, we can say that we make good feature selection with rfe and selectkBest methods. However as you can see there is a problem, okey I except we find best 5 feature with two different method and these features are same but why it is 5. Maybe if we use best 2 or best 15 feature we will have better accuracy. Therefore lets see how many feature we need to use with rfecv method.","3fcd4e0a":"Lets interpret the plot above together. For example, in texture_mean feature, median of the Malignant and Benign looks like separated so it can be good for classification. However, in fractal_dimension_mean feature, median of the Malignant and Benign does not looks like separated so it does not gives good information for classification.","e9dfb323":"## 3) Recursive feature elimination (RFE) with random forest:\n\nBasically, it uses one of the classification methods (random forest in our example), assign weights to each of features. Whose absolute weights are the smallest are pruned from the current set features. That procedure is recursively repeated on the pruned set until the desired number of features\n\nLike previous method, we will use 5 features. However, which 5 features will we use ? We will choose them with RFE method."}}