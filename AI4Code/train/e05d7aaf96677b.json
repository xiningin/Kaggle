{"cell_type":{"691a6392":"code","e4fafc40":"code","feb50d69":"code","b04bc30b":"code","73485d63":"code","372c20cc":"code","c0eacd86":"code","b5fc742f":"code","80657856":"code","56c2fb6c":"code","4123c9e8":"code","b1b38d00":"code","a9e9ec4b":"code","af5d8cde":"code","8af87940":"code","b9a2e6f6":"code","8f1c9867":"code","257eba07":"code","d3a3705a":"code","f650345c":"code","3291413e":"code","7b73ae51":"code","e639b1c5":"code","a5a47e0a":"code","26100419":"code","d9fb0c11":"code","f6583ace":"code","bf26c84a":"code","a86b4ffe":"code","add65bb0":"markdown","d27b709d":"markdown","2c798fec":"markdown","414aa255":"markdown","867c274f":"markdown","902b25fb":"markdown","4ea42774":"markdown","659d11c0":"markdown","6c4fb0e4":"markdown","fabbc5f6":"markdown","edcb1ac9":"markdown","0f4e5dae":"markdown","1f12180c":"markdown","74848086":"markdown","da4eeee8":"markdown","f2429d96":"markdown","a7f4ae5b":"markdown","286cebec":"markdown","4e2282e4":"markdown"},"source":{"691a6392":"#Libraries\nimport os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport tensorflow as tf\nimport tensorflow_datasets as tfds\nfrom tensorflow.keras import Sequential\nfrom tensorflow.keras import layers\n\n#Flags\nholiday_removed = 0\nyear_joined = 0\nscaled = 0\n\n#Labels\ndays_name = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']","e4fafc40":"#Path to data set\npath = '\/kaggle\/input\/electrity-prices\/'\ndataset = 'electricity_prices.csv'\n\n#Data loaded into pandas dataframe. Missing entries denoted by ? in csv.file\ndata = pd.read_csv(path + dataset, na_values='?')\n\n#Overview of dataframe\nprint('#samples  (rows)    =  {}'.format(data.shape[0]))\nprint('#features (columns) =  {}'.format(data.shape[1]))\ndata.head(5)","feb50d69":"time_features = [\n    'PeriodOfDay',\n    'DayOfWeek',\n    'WeekOfYear',\n    'Day',\n    'Month',\n    'Year'   \n]\n\nmetadata_features = [\n    'SMPEP2',\n    'ForecastWindProduction',\n    'SystemLoadEA',\n    'SMPEA',\n    'ORKTemperature',\n    'ORKWindspeed',\n    'CO2Intensity',\n    'ActualWindProduction',\n    'SystemLoadEP2'\n]","b04bc30b":"#Typecast DateTime column into pandas.timestamp\ndata['DateTime'] = pd.to_datetime(data['DateTime'], format='%d\/%m\/%Y %H:%M')\ndates = data['DateTime'].apply(lambda x: x.date()).unique()\n\n#Plot forecasted and actual price from three random days (default seed = 42)\nfig, axes = plt.subplots(1,3, figsize = (12,5))\nfig.subplots_adjust(wspace = 0.25)\n\nrs = np.random.seed(42)\nfor ax in axes:\n    rand_date = dates[np.random.randint(0,len(dates))]\n    price     = data['SMPEP2'][data['DateTime'].apply(lambda x: x.date()) == rand_date]\n    foreprice = data['SMPEA'][data['DateTime'].apply(lambda x: x.date()) == rand_date]\n    \n    sns.lineplot(x = np.array(range(0,len(price)))\/2, y=price,     ax=ax, label='Actual price')\n    sns.lineplot(x = np.array(range(0,len(price)))\/2, y=foreprice, ax=ax, label='Forecasted price')\n        \n    ax.set_xlabel('Hour of day'); ax.set_ylabel('SMPEP2')\n    ax.set(xlim=(0,23), ylim=(0, max(max(price), max(foreprice))*1.10))\n    ax.legend()\n    ax.title.set_text(days_name[rand_date.weekday()] + ' ' + str(rand_date))\n    \n\n#Compute and report evaluation metrics for forecasted price\nmean_target_price = data['SMPEP2'].mean()\n\nRMSE_fore = np.sqrt(1\/len(data['SMPEP2'])*np.sum(np.power(data['SMPEP2'] - data['SMPEA'],2)))\nMAE_fore = 1\/len(data['SMPEP2'])*np.sum(abs(data['SMPEP2'] - data['SMPEA']))\nR2_fore = 1 - np.sum(np.power(data['SMPEA'] - data['SMPEP2'], 2))\/np.sum(np.power(data['SMPEA'] - mean_target_price, 2))\n\nprint('FORECASTED VS. ACTUAL PRICE')\nprint('Mean target price:      {:.2f}'.format(mean_target_price))\nprint('Root mean square error: {:.2f}'.format(RMSE_fore))\nprint('Mean absolute error:    {:.2f}'.format(MAE_fore))\nprint('R-squared:              {:.3f}'.format(R2_fore))","73485d63":"#Entry no. 24769\nprint(data.loc[24769,:])","372c20cc":"#Function to find missing values. Returns dataframe\ndef missing_values(df):\n    feature = []\n    dtype = []\n    missing_values = []\n    unique_values = []\n    holi_missing_rate = []\n    \n    for col in df.columns:\n        feature.append(col)\n        dtype.append(df[col].dtype)\n        missing_values.append(df[col].isnull().sum())\n        unique_values.append(len(df[col].unique()))\n        \n        if data[col].isnull().sum() > 0:\n            holi_missing_rate.append(round(holiday_missing_rate(df, col),2))\n        else:\n            holi_missing_rate.append(0)\n        \n    dataframe_missing_info = pd.DataFrame({\n        'feature': feature,\n        'dtype': dtype,\n        '#missing values': missing_values,\n        'unique values': unique_values,\n        'holiday missing rate (%)': holi_missing_rate\n    })\n        \n    return dataframe_missing_info\n\n\n#Function to find percentage of missing entries on holidays. Returns float.\ndef holiday_missing_rate(dataframe, col):\n    flags = dataframe['HolidayFlag'][dataframe[col].isnull()]\n    nonholi = len(flags[flags == 0])\n    holi = len(flags[flags == 1])\n    holiday_missing_rate = holi\/(nonholi + holi)*100\n    \n    return holiday_missing_rate\n\n#Utilization of above functions to compute missing values dataframe\ndata_missing = missing_values(data)\ndata_missing","c0eacd86":"#Find and report number of holidays\nif holiday_removed == 0:\n    days = round(data.shape[0]\/48)\n    holidays = round(len(data[data['HolidayFlag'] == 1].index)\/48)\n    holiday_removed += 1\n    \n    #Remove holidays from dataframe\n    holiday_idxs = data[data['HolidayFlag'] == 1].index\n    data = data.drop(holiday_idxs)\n    \nprint('Total days: {}'.format(days))\nprint('Total holidays: {}'.format(holidays))\nprint('Holiday rate: {:.3}%'.format(holidays\/days*100))\n\n\nprint('\\nTotal normal days: {}'.format(round(data.shape[0]\/48)))\n\n#Compute missing values dataframe after holiday removal\ndata_missing = missing_values(data)\ndata_missing","b5fc742f":"#Impute missing data. Missing entries will be filled by linear interpolation.\nfor col in data_missing[data_missing['#missing values'] > 0]['feature']:\n    data[col].interpolate(method='linear', inplace=True)\n\ndata_missing = missing_values(data)\ndata_missing","80657856":"#Plot histograms for time features\nfig, axes = plt.subplots(2, 3, figsize=(12,10))\nfig.subplots_adjust(wspace=0.35)\naxes = axes.ravel()\n\nfor i, feature in enumerate(time_features):\n    bins = int(data_missing['unique values'][data_missing['feature'] == feature])\n    sns.histplot(data[feature], bins=bins, ax=axes[i])","56c2fb6c":"#Plot scatter plots for the target price (SMPEP2) versus the different time features\nfig, axes = plt.subplots(2, 3, figsize=(12, 10))\nfig.subplots_adjust(wspace=0.35)\naxes = axes.ravel()\n\nfor i, feature in enumerate(time_features):\n    sns.scatterplot(data=data, x=feature, y='SMPEP2', ax=axes[i])\n\n# There is an outlier in the price column with an exact value of 1000.00. \n# This is manually changed to the mean between bounding values\noutlier_index = data['SMPEP2'][data['SMPEP2'] > 900].index\n\nif outlier_index.shape[0] > 0:\n    outlier_index = outlier_index.values[0]\n    new_val = 0.5*(data.at[outlier_index - 1, 'SMPEP2'] + data.at[outlier_index + 1, 'SMPEP2'])\n    data.at[outlier_index, 'SMPEP2'] = new_val","4123c9e8":"#Compute mean prices for every day of the week\nmean_price = []\nfor day in list(range(0,7)):\n    mean_price.append(data['SMPEP2'][data['DayOfWeek'] == day].mean())\n\n#Seaborn boxen plot to visualize the price distribution for each day of the week\nfig, axes = plt.subplots(2, 1, figsize=(12, 5), gridspec_kw={'height_ratios': [1, 3]}, sharex=True)\nfig.subplots_adjust(hspace=0)\naxes[0].plot(list(range(0, 7)), mean_price, 'k.--')\nsns.boxenplot(data=data, x='DayOfWeek', y='SMPEP2', ax=axes[1])\naxes[0].set(ylim=(59, 71), yticks=[61, 63, 65, 67, 69], ylabel='Mean')\naxes[0].set_title('Price distribution across weekdays');\naxes[1].set(ylim=(-25, 650));\naxes[1].set_xticklabels(days_name)\n\n#One-hot encode to three categories for DayOfWeek\nday_one_hot = pd.get_dummies(data['DayOfWeek'])\n\ndata['monday'] = day_one_hot[0]\ndata['midweek'] = day_one_hot.iloc[:, 1:4].sum(axis=1)\ndata['weekend'] = day_one_hot.iloc[:, 4:7].sum(axis=1)\n\ndata.head(1)","b1b38d00":"#Compute mean prices for every day of the month\nmean_price = []\nfor day in list(range(1,32)):\n    mean_price.append(data['SMPEP2'][(data['Day'] == day) & (data['Year'] != 2011)].mean())\n\n#Seaborn boxen plot to visualize the price distribution for each day of the month\nfig, axes = plt.subplots(2, 1, figsize=(12, 5), gridspec_kw={'height_ratios': [1, 3]}, sharex=True)\nfig.subplots_adjust(hspace=0)\naxes[0].plot(list(range(0, 31)), mean_price, 'k.--')\nsns.boxenplot(data=data[data['Year'] != 2011], x='Day', y='SMPEP2', ax=axes[1])\naxes[0].set(ylim=(60, 68), yticks = [61, 63, 65, 67], ylabel='Mean')\naxes[1].set(ylim=(-25, 650));\naxes[0].set_title('Price distribution across days of the month');","a9e9ec4b":"#Compute mean prices for each month\nmean_price = []\nfor month in list(range(1, 13)):\n    mean_price.append(data['SMPEP2'][(data['Month'] == month) & (data['Year'] != 2011)].mean())\n\nfig, axes = plt.subplots(2, 1, figsize=(12, 5), gridspec_kw={'height_ratios': [1, 3]}, sharex=True)\nfig.subplots_adjust(hspace=0)\naxes[0].plot(np.arange(0, 12), mean_price, 'k.--')\nsns.boxenplot(data=data[data['Year'] != 2011], x='Month', y='SMPEP2', ax=axes[1])\naxes[0].set(ylim=(55, 75), yticks=[57.5, 62.5, 67.5, 72.5], ylabel='Average')\naxes[1].set(ylim=(-25, 650));\naxes[0].set_title('Price distribution across months');\n\n\n#One-hot encoding of the two categories of Month\nmonth_one_hot = pd.get_dummies(data['Month'])\n\ndata['winter'] = month_one_hot[[1, 2, 3, 10, 11, 12]].sum(axis=1)\ndata['summer'] = month_one_hot[[4, 5, 6, 7 , 9 , 10]].sum(axis=1)\n\ndata.head(1)","af5d8cde":"#Compute mean prices for each year\nmean_price = []\nfor year in [2011,2012,2013]:\n    mean_price.append(data['SMPEP2'][data['Year'] == year].mean())\n\nfig, axes = plt.subplots(2, 1, figsize=(8, 5), gridspec_kw={'height_ratios': [1, 3]}, sharex=True)\nfig.subplots_adjust(hspace=0)\naxes[0].plot(np.arange(0, 3), mean_price, 'k.--')\nsns.boxenplot(data=data, x='Year', y='SMPEP2', ax=axes[1])\naxes[0].set(ylim=(55, 75), yticks=[57.5, 62.5, 67.5, 72.5], ylabel='Average')\naxes[1].set(ylim=(-25, 650));\naxes[0].set_title('Price distribution across years');\n\n#One-hot encoding of the two categories of Year\nyear_one_hot = pd.get_dummies(data['Year'])\n\nif year_joined == 0:\n    data = data.join(year_one_hot)\n    year_joined += 1\n\ndata.head(1)","8af87940":"#Compute mean prices for periods of day\nmean_price = []\nfor pday in list(range(0, 48)):\n    mean_price.append(data['SMPEP2'][data['PeriodOfDay'] == pday].mean())\n\nfig, axes = plt.subplots(2, 1, figsize=(12, 5), gridspec_kw={'height_ratios': [1, 3]}, sharex=True)\nfig.subplots_adjust(hspace=0)\naxes[0].plot(np.arange(0, 48), mean_price, 'k.-', markersize=8)\nsns.boxenplot(data=data, x='PeriodOfDay', y='SMPEP2', ax=axes[1])\naxes[0].set(ylim=(30, 150), yticks=[50, 85, 120], ylabel='Average', title='Price distribution across half-hourly intervals')\naxes[1].set(ylim=(-25, 650), xlabel='Hour of day');\naxes[1].set(xticks=list(range(0, 48, 2)))\naxes[1].set_xticklabels(list(range(0, 24)));","b9a2e6f6":"#Find significant frequencies by Fourier transformation\nfft = tf.signal.rfft(data['SMPEP2'])\nfrequencies = np.arange(0, len(fft))\n\n#Change frequencies to 'pr. year'\ndaysinyr = 365.2524\nn_samples_halfh = len(data['SMPEP2'])\nhalfhours_per_year = 2*24*daysinyr\nyears_per_dataset = n_samples_halfh\/(halfhours_per_year)\nfrequencies_per_year = frequencies\/years_per_dataset\n\n\nfig, axes = plt.subplots(1, 3, figsize=(15, 5))\nfig.subplots_adjust(wspace=0.05)\naxes = axes.ravel()\nfor ax in axes[0:2]:\n    ax.plot(frequencies_per_year, np.abs(fft), 'k-')\n    ax.set(ylim=(0, 400000), yticks=[], xticks=[], xlabel='Frequency')\n\naxes[0].set_xscale('log')  \naxes[0].set(xlim=(0.1, max(frequencies_per_year)), xticks=[1, daysinyr\/12, daysinyr])\naxes[0].set_xticklabels(['1\/year', '1\/month', '1\/day']);\n\naxes[1].set(xlim=(200, 2000), xticks=[daysinyr, daysinyr*2, daysinyr*3, daysinyr*4, daysinyr*5])\naxes[1].set_xticklabels(['1\/day', '2\/day', '3\/day', '4\/day', '5\/day']);\n\n#Map the PeriodOfDay feature onto a sine and cosine function with a period of 24 hours\ndata['day_sin'] = np.sin(data['PeriodOfDay']*(2*np.pi\/48))\ndata['day_cos'] = np.cos(data['PeriodOfDay']*(2*np.pi\/48))\n\naxes[2].plot(list(range(0, 48)), data['day_sin'][0:48], 'r.-', label='day sine')\naxes[2].plot(list(range(0, 48)), data['day_cos'][0:48], 'g.-', label='day cosine')\naxes[2].set(yticks=[], xticks=[0, 12, 24, 36, 48], xlabel='Hour of day')\naxes[2].set_xticklabels([0, 6, 12, 18, 24]);\naxes[2].legend();","8f1c9867":"#Define new dataframe with relevant features for modelling.\ndata_model = data[[\n    'DateTime',\n    'SMPEP2',\n    'ForecastWindProduction',\n    'SystemLoadEA',\n    'SMPEA',\n    'ORKTemperature',\n    'ORKWindspeed',\n    'CO2Intensity',\n    'ActualWindProduction',\n    'SystemLoadEP2',\n    'monday',\n    'midweek',\n    'weekend',\n    'winter',\n    'summer',\n     2011,\n     2012,\n     2013,\n    'day_sin',\n    'day_cos'\n        ]]","257eba07":"#Data splitting with a 7:2:1 training-test-validation split\ndef train_test_val_split(dataframe,\n                         trainfrac=0.70,\n                         testfrac=0.20,\n                         valfrac=0.10):\n    \n    rows = len(dataframe)\n    train = dataframe[0:int(rows*trainfrac)].copy().reset_index(drop=True)\n    test  = dataframe[int(rows*trainfrac):int(rows*(trainfrac+testfrac))].copy().reset_index(drop=True)\n    val   = dataframe[int(rows*(trainfrac+testfrac)):int(rows*(trainfrac+testfrac+valfrac))].copy().reset_index(drop=True)\n    \n    return train, test, val\n\ntrain, test, val = train_test_val_split(data_model)\ntrain.head(1)","d3a3705a":"#Standardization of metadata features\nif scaled == 0:\n    train_mean = train[metadata_features].mean()\n    train_std = train[metadata_features].std()\n    \n    train[metadata_features] = (train[metadata_features] - train_mean)\/train_std\n    test[metadata_features] = (test[metadata_features] - train_mean)\/train_std\n    val[metadata_features] = (val[metadata_features] - train_mean)\/train_std\n\n    data_scaled_feat = (data_model[metadata_features] - train_mean)\/train_std\n    data_scaled_feat = data_scaled_feat.melt(var_name='Feature', value_name='Stand. value')\n    \n    scaled += 1\n\nfig, ax = plt.subplots(1, 1, figsize=(12, 5))\nsns.violinplot(x='Feature', y='Stand. value', data=data_scaled_feat, ax=ax)\nax.set_xticklabels(data_model[metadata_features].keys(), rotation=45)\nax.set_title('Normalized metadata features', fontsize=14);","f650345c":"##Save standardized data sets as .csv files\n#SAVING DISABLED\n#train.to_csv(path_or_buf=path + 'electricity_prices_train.csv')\n#test.to_csv(path_or_buf=path + 'electricity_prices_test.csv')\n#val.to_csv(path_or_buf=path + 'electricity_prices_val.csv')","3291413e":"#Class for generating batches of the time-resolved data set. Adjust input_width (# of rows in each batch),\n# target_width (# of price predictions), and target_shift (# of rows between end of input and end of predictions)\nclass BatchGenerator():\n    #Constructor\n    def __init__(self, input_width=1, target_width=1, target_shift=1,\n                 train_df=None, test_df=None, val_df=None,\n                 target_feature=None):\n        \n        #Store data\n        self.train_df = train_df\n        self.test_df  = test_df\n        self.val_df   = val_df\n        \n        #Feature parameters\n        self.target_feature = target_feature\n        self.feature_indices = {name: i for i, name in enumerate(self.train_df.columns)}\n        self.target_feature_index = self.feature_indices[target_feature]\n            \n        #Batch parameters\n        self.input_width = input_width\n        self.target_width = target_width\n        self.target_shift = target_shift\n        \n        self.total_batch_width = input_width + target_shift\n        \n        self.input_slice = slice(0, input_width)\n        self.input_indices = np.arange(self.total_batch_width)[self.input_slice]\n    \n        self.target_start = self.total_batch_width - self.target_width\n        self.target_slice = slice(self.target_start, None)\n        self.target_indices = np.arange(self.total_batch_width)[self.target_slice]\n          \n            \n    #Method to split batches into inputs and targets\n    def split_batches(self, tfdata):\n        inputs = tfdata[:, self.input_slice, :]    \n        targets = tf.stack([tfdata[:, self.target_slice, self.target_feature_index]], axis=-1)\n        \n        #Slicing does not preserve shape information so set the shapes manually\n        inputs.set_shape([None, self.input_width, None])\n        targets.set_shape([None, self.target_width, None])\n\n        return inputs, targets    \n    \n    \n    #Method to make batches from dataframe. Uses self.split_batches()\n    def make_batches(self, df):\n        data = np.array(df, dtype=np.float32)\n        batches = tf.keras.utils.timeseries_dataset_from_array(\n            data=df,\n            targets=None,\n            sequence_length=self.total_batch_width,\n            sequence_stride=1,\n            shuffle=False,\n            batch_size=32\n        )\n        \n        batches = batches.map(self.split_batches)\n        return batches    \n    \n    \n    #Method to plot batches   \n    def plot(self, batch=None, plot_col=None, model=None, max_plots=3):\n        \n        self.batch = batch\n        \n        for inputs_, targets_ in self.batch:\n            inputs = inputs_\n            targets = targets_\n        \n        plot_col_index = self.feature_indices[plot_col]\n        nplots = min(max_plots, len(inputs))\n        \n        fig, axes = plt.subplots(nplots, 1, figsize=(12, 6), sharex=True)\n        fig.subplots_adjust(hspace=0)\n        axes = axes.ravel()\n        axes[-1].set(xlabel='Half-hours')\n        \n        samples = np.random.randint(0, len(targets), size=nplots)\n        \n        for n in range(nplots):\n            axes[n].set(ylabel=plot_col, yticks=[]);\n            axes[n].plot(self.input_indices, inputs[samples[n], :, plot_col_index],\n                         marker='.', markersize=8, label='inputs')\n            if plot_col == self.target_feature:\n                axes[n].scatter(self.target_indices, targets[samples[n], :, :],\n                                ec='k', label='targets', c='r', s=64)\n        \n            if model is not None:\n                predictions = model(inputs)\n                axes[n].scatter(self.target_indices, predictions[samples[n], :],\n                            marker='X', ec='k', label='predictions', c='#ff7f0e', s=64)\n            if n == 0:\n                axes[n].legend()\n    \n    \n    #Object properties to access and make batches of training, test or validation\n    #uses self.make_batches()\n    @property\n    def train(self):\n        return self.make_batches(self.train_df)\n    \n    @property\n    def test(self):\n        return self.make_batches(self.test_df)\n    \n    @property\n    def val(self):\n        return self.make_batches(self.val_df)\n    \n    #Represent method\n    def __repr__(self):\n        return '\\n'.join([\n            f'Total batch width  {self.total_batch_width}',\n            f'Input indices      {self.input_indices}',\n            f'Target indices     {self.target_indices}',\n            f'Target feature     {self.target_feature}'])","7b73ae51":"#Initialize batches as objects of the BatchGenerator class\nwindow24 = BatchGenerator(input_width=24, target_width=1, target_shift=1, target_feature='SMPEP2',\n                           train_df=train.select_dtypes(exclude='datetime64[ns]'),\n                           test_df=test.select_dtypes(exclude='datetime64[ns]'),\n                           val_df=val.select_dtypes(exclude='datetime64[ns]')\n                         )\n\nwindow24","e639b1c5":"#Function to compile, fit, and, save a specified model\ndef compile_and_fit(model=None, data_batches=None, num_epochs=10, model_nm='my_model'):\n    \n    if model is not None:\n        model.compile(loss=tf.losses.MeanSquaredError(),\n                      metrics=[tf.metrics.MeanAbsoluteError()])\n    \n    if data_batches is not None:\n        history = model.fit(data_batches.train, epochs=num_epochs,\n                          validation_data=data_batches.val)\n    \n    # SAVING MODEL DISABLED\n    #if not os.path.exists(path+'\\models\/'):\n    #    os.mkdir(path+'\\models\/')\n    \n    #model.save(path + '\\models\/' + model_nm)\n    #print('Model saved in:', path + '\\models\/' + model_nm)\n        \n    return history","a5a47e0a":"#Build Recurrent Neural Network (RNN)\ndatasetup = window24\nmodel_name = '4layers_100each_drop02_window24'\nnum_epochs = 100\n\nRNN = Sequential()\n\n#1st layer\nRNN.add(layers.LSTM(units=100, return_sequences=True,\n                            input_shape=(datasetup.input_width, len(datasetup.feature_indices))))\nRNN.add(layers.Dropout(0.2))\n\n#2nd layer\nRNN.add(layers.LSTM(units=100, return_sequences=True))\nRNN.add(layers.Dropout(0.2))\n\n#3rd layer\nRNN.add(layers.LSTM(units=100, return_sequences=True))\nRNN.add(layers.Dropout(0.2))\n\n#4th layer\nRNN.add(layers.LSTM(units=100, return_sequences=False))\nRNN.add(layers.Dropout(0.2))\n\n#output layer\nRNN.add(layers.Dense(units=1))","26100419":"#Compile, fit, and save a model with specified dataset\nhistory = compile_and_fit(model=RNN,\n                          data_batches=datasetup,\n                          num_epochs=num_epochs,\n                          model_nm=model_name)","d9fb0c11":"#Save model summary and performance as .txt file\nmodel = RNN\n\nperformance_train = model.evaluate(datasetup.train)\nperformance_test  = model.evaluate(datasetup.test)\nperformance_val   = model.evaluate(datasetup.val)\n\n# SAVING DISABLED\n#with open(path + '\\models\/' + model_name + '.txt', \"a\") as f:\n#    model.summary(print_fn=lambda x: f.write(x + '\\n'))\n#    f.write(f\"\\nEpochs = {num_epochs}\")\n#    f.write('\\n\\nData\\tLoss\\tMAE\\n')\n#    f.write(f\"train\\t{performance_train[0]:.4f}\\t{performance_train[1]:.4f}\\n\")\n#    f.write(f\"test\\t{performance_test[0]:.4f}\\t{performance_test[1]:.4f}\\n\")\n#    f.write(f\"val\\t{performance_val[0]:.4f}\\t{performance_val[1]:.4f}\\n\")\n","f6583ace":"#Plot inputs, tagets and predictions for random data batches in the test data set\ndatasetup.plot(batch=datasetup.test.take(1), plot_col='SMPEP2', model=model)","bf26c84a":"#Make new dataframe with orignal forecast, target prices, and model predictions for the test data set\npredicts = model.predict(datasetup.test)\n\nforecast_df = pd.DataFrame()\nforecast_df.insert(loc=0, column='Date', value=test['DateTime'].apply(lambda x: x.date()))\nforecast_df.insert(loc=1, column='Time', value=test['DateTime'].apply(lambda x: x.time()))\n\nforecast_df.loc[:,'Original'] = test['SMPEA'].apply(lambda x: x*train_std['SMPEA'] + train_mean['SMPEA'])\nforecast_df.loc[datasetup.input_width:,'Target'] = test['SMPEP2'].apply(lambda x: x*train_std['SMPEP2'] + train_mean['SMPEP2'])\nforecast_df.loc[datasetup.input_width:,'Prediction'] = predicts*train_std['SMPEP2'] + train_mean['SMPEP2']\n\n# Plot selected dates with original forecasts, target prices, and model predictions\ndates = forecast_df['Date'].unique()[1::]\nplot_dates = [dates[11], dates[112]]\n\n\nx = list(range(0, 48))\nfor date in plot_dates:\n    fig, ax = plt.subplots(1, 1, figsize=(12,2.5))\n    ax.plot(x, forecast_df['Original'][forecast_df['Date'] == date], c='grey', marker='x', label='original')\n    ax.plot(x, forecast_df['Target'][forecast_df['Date'] == date], c='r', marker='o', label='targets')\n    ax.plot(x, forecast_df['Prediction'][forecast_df['Date'] == date], c='g', marker='s', label='predictions')\n    ax.set(ylabel='Price', xlabel='Time of Day', xticks=list(range(0, 49, 2)))\n    ax.set_xticklabels(list(range(0, 25)))\n    ax.set_title(str(days_name[date.weekday()]) + ' ' + str(date), fontsize=14)\n    ax.legend();","a86b4ffe":"#Compute and plot the MAE for the original forecast and the model predictions\noriginalDailyMAE = []\npredictionsDailyMAE = []\nfor date in dates:      \n    originals = np.array(forecast_df['Original'][forecast_df['Date'] == date])\n    targets = np.array(forecast_df['Target'][forecast_df['Date'] == date])\n    predictions = np.array(forecast_df['Prediction'][forecast_df['Date'] == date])\n    \n    originalDailyMAE.append(np.sum(np.abs(targets - originals))\/len(targets))\n    predictionsDailyMAE.append(np.sum(np.abs(targets - predictions))\/len(targets))\n\nfig, ax = plt.subplots(1, 1, figsize=(12, 5))\nax.plot(list(range(len(dates))), originalDailyMAE, c='r', alpha=0.3, marker='x', label='original MAE')\nax.plot(list(range(len(dates))), predictionsDailyMAE, c='g', marker='x', label='prediction MAE');\n\nax.set(ylabel='Daily MAE of price', xlabel='Date in test set');\nax.set(ylim = (0, 35))\nax.legend()\n\n#Find the total MAE for all dates\noriginalMAE = np.sum(originalDailyMAE)\/len(dates)\npredictionsMAE = np.sum(predictionsDailyMAE)\/len(dates)\n\nprint('Original MAE          {:.2f} ({:.2f}% of mean target price)'.format(originalMAE, originalMAE\/mean_target_price*100))\nprint('Model predictions MAE {:.2f}  ({:.2f}% of mean target price)'.format(predictionsMAE, predictionsMAE\/mean_target_price*100))","add65bb0":"# Electricity Price Forecasting Using an LSTM-RNN\n(Data dowloaded from https:\/\/www.kaggle.com\/salilchoubey\/electrity-prices on December 16th 2021)\n\n*Made-up Case*: A data center in Ireland consumes large amounts of electricity to keep their operation running. So much, in fact, that buying electrical power makes up a significant part of their total spending. Recently, sections of the data center have been upgraded to be put in and out of a standby state. The sections run on significantly reduced power in this state, and they can be switched in and out in only a few minutes. The purpose of this recent upgrade is to reduce overall spending by turning of non-essential equipment in times of peak electricity prices.\n\nThe price of electricity in the area is determined by many different factors but, generally speaking, it will spike when the national demand increases. In afternoon hours, people come home from work, turn on their lights, tellys, tea kettles, and other common household appliances. This causes a large increase demand for power, and consequently, a spike in price. The spike can be several hundreds of percent above the baseline price level. Reducing consumption in these peak hours would lead to reduced spending.\n\nThe price spikes do not appear every day, and sometimes it is not possible to use the standby state due to client demands. To employ the standby-state succesfully, it is therefore nescessary to forecast the electricity price with sufficient accuracy. The data center has recorded the actual price alongside several other parameters in half-hour intervals during the last two years. \n\nThe task at hand is to use the data to train an appropriate Machine Learning Model that can predict the price more accurately than the already available national forecast. Here, we choose to use a Long-Short Term Memory (LSTM) based Recurrent Neural Network (RNN).","d27b709d":"#### DayOfWeek feature\nAs seen below, the mean prices on a given day of the week does actually variy from 69.13 on Mondays to 60.10 on Saturdays. In general, the price on Mondays is significantly higher than on other weekdays. Also, the spike in prices tend to be higher on Tuesdays through Thurdays, but lower in the weekend.\n\nThese observations lead to one-hot encoding of 3 categories (instead of 7) for the DayOfWeek feature:\n- Mondays (monday)\n- Tuesday, Wednesday, Thursday (midweek)\n- Friday, Saturday, Sunday (weekend)","2c798fec":"#### Removal of holidays\nAs seen in the right-most column above, many of the missing entries fall on holidays. There are 32 holidays out of a total of 792 days (see below), which correspond to ~4% of the data set. However, for the missing entries in ORKTemperature, 39.32% fall on holidays. \n\nWe therefore decide to remove samples that fall on holidays. This will exclude many of the missing entries and also reduce the dimensionality of the dataset. Also, consider the fact that the Easter holidays, for instance, only have been sampled twice in the data set. ","414aa255":"### Price Forecasting\nA forecast of the price of electricity is already given by a national institution (the SMPEA feature). The goal of this project is thus to implement a Machine Learning Model that is more accurate than the forecast. Below, the forecasted and actual price of three random days are plotted. The two days to the left both have an afternoon spike in price but the one to the right does not.\n\nThe root mean square error (RMSE) of the forecasted price is 29.77, the mean absolute error (MAE) is 14.95, and the R-squared (R2) value is 0.149. Compare these errors to the mean target price for the entire time period, which is 64.14. The MAE of the forecasted price is thus nearly a quarter of the mean target price, suggesting a relatively inaccurate forecast.","867c274f":"#### Overview of price disitrbution\nWe begin by visualizing the distribution of prices along the different time features. As shown in the figures below, the target price (SMPEP2) depends significantly on the PeriodOfDay with low prices at night and a spike in the afternoon hours (top left). The price also depends on the WeekOfYear with higher price spikes in the Winter (top right). Furthermore, there seems to be a trend of increasing prices over the three years (bottom right). At first glance, there does not seem to be a clear trend for DayOfWeek or Day of the month.\n\nIn the following sections, we shall examine the price distributions accross the different time features in more detail.","902b25fb":"#### Imputation of missing entries\nAfter removal of the holidays, there are still many missing entries, especially in the ORKTemeprature and ORKWindspeed feature. The missing values are imputed by linear interpolation, meaning that one or a series of missing values will be determined by a linear interpolation between the bounding values of the missing value(s).","4ea42774":"#### Data batches\nWe are dealing with a time series forecast so, in order to make a prediction, we have to decide how many previous time steps to feed our model. We will then structure the data into data batches, where every batch will include the input features from the previous time steps plus the target value to be predicted. We shall call this the \"time window\".\n\nBelow, we define a class that generates the desired batches. As input, we can give the \"input_width\" (number of previous time steps), the \"target_width\" (number of time steps to be predicted), and the \"target_shift\", which defines how far out into the future we want to predict. We also define the feature we want to predict by \"target_feature\".","659d11c0":"### Data Preprocessing\nBefore the data can be fed to a Machine Learning Model, it has to be properly cleaned and encoded. Several samples have missing entries and, in some cases, no metadata features have been recorded at all. For instance, entry no. 24769 recorded on March 31st 2013 at 01.30 has all missing metadata features. This sample was recorded on Easter in 2013. Examination of the data set reveals that holidays are often problematic with relatively many missing entries.","6c4fb0e4":"#### Encoding of time features\nTo properly train a Machine Learning Model for time series forecasting, it is important to handle the time features adequately. The task is to filter out non-significant or redundant time features, and encode significant ones in a meaningful way that the Machine Learning Model can interpret.","fabbc5f6":"#### Year feature\nThe price increases year-to-year so the three years are one-hot encoded into three new colums (2011, 2012, and 2013)","edcb1ac9":"#### Month and WeekOfYear features\nThe price spikes seem to be higher in the Winter months (October to April) than in the Summer. Therefore, the month column is one-hot encoded into two categories: winter (Oct, Nov, Dec, Jan, Feb, Mar) and summer (Apr, May, Jun, Jul, Aug,  Sep). These two categories are assumed to also encompass the similar trend observed in the WeekOfYear feature.","0f4e5dae":"#### Sampling distribution\nNow that the data have been cleaned, it is time to get an overview of the distribution of samples across the different time features. The histograms below show that counts for PeriodOfDay (top left) and day-of-month (bottom left) are equally distributed across the bins. The sample count on Mondays (DayOfWeek = 0, top middle) is slightly less due to the exclusion of holidays, which happen to fall inproportionally on Mondays in the time period. Only the end of 2011 has been sampled (bottom right). This leads to more counts for weeks (top right) and months (bottom middle) at the end of the year.","1f12180c":"#### Machine Learning Model: LSTM-based RNN\nNow, we are ready to build the LSTM-based RNN. We choose a 4-LSTM-layered RNN where each layer consists of 100 neurons with a dropout rate of 0.2. We shall train it for 100 epochs. After training, we evaluate the performance on the training, test, and validation data set, and save the results to a text file, alongside the model architecture.","74848086":"### Features\nAs seen above, the data set is comprised of 38014 samples (rows) and 18 features (columns). The first 9 features are identity or time features for a given sample. The following 8 features are metadata and the last is the price to be forecasted. A detailed description of each features is given below:\n\n#### Time features\n- DateTime: (String) Defines date and time of samples\n- Holiday: (String) States the name of the holiday if the date is a bank holiday\n- HolidayFlag: (Integer) Flag for bank holiday; 1 if holiday, 0 if not\n- DayOfWeek: (Integer) Identifier for day of the week; 0 to 6 for Monday through Sunday\n- WeekOfYear: (Integer) Identifier for week of the year; 0 to 52\n- Day: (Integer) Day of the date in DateTime\n- Month: (Integer) Month of the date in DateTime\n- Year: (Integer) Year of the date in DateTime\n- PeriodOfDay: (Interger) Identifier for the half-hour interval of a given day; 0 to 47\n\n#### Metadata features\n- ForecastWindProduction: (Float) Forecasted power production from wind\n- SystemLoadEA: (Float) Forecast of the power grid system load\n- SMPEA: (Float) National forecast of the electricity price\n- ORKTemperature: (Float) Actual temperature measured at Cork Airport\n- ORKWindspeed: (Float) Actual wind speed measured at Cork Airport\n- CO2Intensity: (Float) Actual CO2 intensity of power production in g\/kWh\n- ActualWindProduction: Actual power production from wind\n- SystemLoadEP2: (Float) Actual power grid system load\n- SMPEP2: (Float) The actual price of electricity (price to be forecasted)\n","da4eeee8":"### Model performance and evaluation\nWe have evaluated the model in terms of the mean absolute error (MAE) on the training, test, and validation data sets. To get a visual understanding of the performance, we can use the plot() method from the BatchGenerator() class. Below, three random data batches from the test set have been plotted. \n\nHowever, we are mostly interested in the performance on an entire day, and whether or not the model can forecast a price spike. \n\nWe make a new dataframe where we recombine the date and time from the DateTime feature with the original forecast (SMPEA), the target prices (SMPEP2), and the model predictions. The prices were scaled for training of the model, so we have to remember to rescale to the original values. Now, we can plot the performance on selected dates.\n\nAt first sight, the model predictions seem to be more accurate than the original forecast. We have check this for all dates to be sure, however. Therefore, we compute the daily MAE both for the predictions and the original forecast for every date in the test data set. We can also compute the mean of the daily MAE, i.e. the total MAE. This turns out to be 13.93 for the original forecast versus 6.64 for the model predictions, which tells us that we have more than halfed the error on the original forecast.","f2429d96":"We can now use the BatchGenerator() class to generate batches with a desired time window. Here, we use a window with 24 time steps (corresponding to 12 hours) to predict a single target value one step into the future for the 'SMPEP2' feature. \n\nWe want to be able to experiment with different time windows and model architectures, so we define a function to compile, fit, and save a particular model operated on a particular structure of data.","a7f4ae5b":"#### Day (of month) feature\nThe mean prices on a given day of a month fluctuate between 60 and 68 but do not seem to have any significant trend. For reasons unclear, price spikes seem to be espcially low on the 3rd, 17th, and 31th day of the month. Overall, these observeations lead to the exclusion of the Day of month feature from the modelling.","286cebec":"#### PeriodOfDay feature\nThe PeriodOfDay feature is the most important time feature as the price varies significantly during a single day. The aftenoon spikes lead to a significant increase in the mean half-hourly price between 17.00 and 18.00 hours. One-hot encoding the PeriodOfDay feature would drastically increase the dimensionality of the data set, as it would introduce 48 additional features (one for each time period). Instead, another approach is taken.\n\nA Fourier transformation of the price feature is used to find the most significant frequency of variation. This is found to be exactly once pr. day. To emcompass this trend, the PeriodOfDay feature is encoded as two new columns: A sine and cosine with a period of one day.","4e2282e4":"### Machine Learning Modelling\nNow that the data has been cleaned and properly encoded, it is time to train the model. We use an LSTM-based RNN as our model. For training and evaluation, we use a customary 7:2:1 training-test-validation split. Because we are using a neural network, the metadata features have to be scaled appropriately. We chose standardization over normalization since the former method of scaling is more adequate for data with significant variations, such as the afternoon spikes. It is customary that all three data sets are standardized using the mean and standard deviation of the training data set."}}