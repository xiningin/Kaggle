{"cell_type":{"28573364":"code","6ea839bc":"code","1c679b79":"code","964f3337":"code","d74780f2":"code","4b46130a":"code","376004bf":"code","f926009d":"code","dbfa14fb":"code","fc45f2d5":"code","805e682f":"code","0918c15e":"code","3ec4835f":"code","b0cc6c4e":"code","0671692c":"code","36f51e3e":"code","5bc7fe39":"code","048a3084":"code","c4ffae92":"code","27f8818e":"code","824299be":"code","acfce239":"code","c2cc5b39":"code","d7cbd062":"code","510d6d05":"code","d5bd9973":"code","5749ce68":"code","d342cda7":"code","da7cdc32":"code","e73b882c":"code","e02cb23a":"code","86b8ac59":"code","dee3788f":"code","a8829c4f":"code","ba5a697c":"code","db5bed0b":"code","e4ea80df":"code","f428ec86":"code","88599e8c":"code","240d6f3e":"code","d3f37609":"code","18e7ac9b":"code","00944b7e":"code","89892473":"code","23e93154":"code","ef2929f4":"code","5fc17501":"code","a52f50ab":"code","c6ae5f67":"code","b5743466":"code","4a216bd7":"code","54df3606":"code","c5da4a08":"code","b72f7ffb":"code","45b8a207":"code","a38dbc9e":"code","c4a01f61":"code","f8ac87c6":"code","59f63a1b":"code","07a83b78":"code","1806c83b":"markdown","950e368e":"markdown","c7e6a5db":"markdown","5994b510":"markdown","dd5e9760":"markdown","e7b64879":"markdown","9fcfc4a4":"markdown","c168c99b":"markdown","044c413f":"markdown","f6ea0f35":"markdown","699e3da1":"markdown","b54557cc":"markdown","e456dd59":"markdown","a7e94b3f":"markdown","769aca76":"markdown","1a023e27":"markdown","d7624342":"markdown","2469282b":"markdown","68803a46":"markdown","e81db555":"markdown","bee4d7e4":"markdown","10203e28":"markdown","2a998a72":"markdown","d9a06063":"markdown","718c146e":"markdown","366df918":"markdown","a0d2750f":"markdown","e0126231":"markdown","ea76a32d":"markdown","e8575d36":"markdown","649b6f3e":"markdown","756a5f03":"markdown","33c18fdc":"markdown","3464e91c":"markdown","11b22b55":"markdown","15a27fbc":"markdown","2d355ae0":"markdown","4bbcdbbc":"markdown","021aa1a5":"markdown","4c301168":"markdown","dafd6dd9":"markdown","8f0e4f9f":"markdown","778f6830":"markdown","30c64cb0":"markdown"},"source":{"28573364":"#importing libraries and Reading the Dataset\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\nimport sys\nimport os","6ea839bc":"df=pd.read_csv(\"..\/input\/Admission_Predict.csv\")","1c679b79":"df.head()","964f3337":"print(\"There are\",len(df.columns),\"columns:\")\nfor x in df.columns:\n    sys.stdout.write(str(x)+\",\")","d74780f2":"df=df.rename(columns = {'Chance of Admit ':'Chance of Admit'})","4b46130a":"df.info()","376004bf":"df.head(5)","f926009d":"fig,ax=plt.subplots(figsize=(10,10))\nsns.heatmap(df.corr(),ax=ax,annot=True,linewidths=0.05,fmt='.2f',cmap=\"magma\")\nplt.show()","dbfa14fb":"print(\"Not having Research:\",len(df[df.Research==0]))\nprint(\"Having Research:\",len(df[df.Research==1]))\ny=np.array([len(df[df.Research==0]),len(df[df.Research==1])])\nx=[\"Not Having Research\",\"Having Research\"]\nplt.bar(x,y)\nplt.title(\"Research Experience\")\nplt.xlabel(\"Candidates\")\nplt.ylabel(\"Frequency\")\nplt.show()","fc45f2d5":"y=np.array([df[\"TOEFL Score\"].min(),df[\"TOEFL Score\"].mean(),df[\"TOEFL Score\"].max()])\nx=[\"Worest\",\"Average\",\"Best\"]\nplt.bar(x,y)\nplt.title(\"TOEFL Scores\")\nplt.xlabel(\"Level\")\nplt.ylabel(\"TOEFL Score\")\nplt.show()","805e682f":"df[\"GRE Score\"].plot(kind='hist',bins=200,figsize=(6,6))\nplt.title(\"GRE Scores\")\nplt.xlabel(\"GRE Score\")\nplt.ylabel(\"Frequency\")\nplt.show()","0918c15e":"plt.scatter(df[\"University Rating\"],df.CGPA)\nplt.title(\"CGPA Scores for University Ratings\")\nplt.xlabel(\"University Rating\")\nplt.ylabel(\"CGPA\")\nplt.show()","3ec4835f":"plt.scatter(df[\"GRE Score\"],df.CGPA)\nplt.title(\"CGPA for GRE Scores\")\nplt.xlabel(\"GRE Score\")\nplt.ylabel(\"CGPA\")\nplt.show()","b0cc6c4e":"df[df.CGPA >=8.5].plot(kind='scatter',x='GRE Score',y='TOEFL Score',color='red')\nplt.xlabel(\"GRE Score\")\nplt.ylabel(\"TOEFL  SCORE\")\nplt.title(\"CGPA >=8.5\")\nplt.grid(True)\nplt.show()","0671692c":"s = df[df[\"Chance of Admit\"] >= 0.75][\"University Rating\"].value_counts().head(5)\nplt.title(\"University Ratings of Candidates with an 75% acceptance chance\")\ns.plot(kind='bar',figsize=(20, 10))\nplt.xlabel(\"University Rating\")\nplt.ylabel(\"Candidates\")\nplt.show()","36f51e3e":"plt.scatter(df[\"CGPA\"],df.SOP)\nplt.xlabel(\"CGPA\")\nplt.ylabel(\"SOP\")\nplt.title(\"SOP for CGPA\")\nplt.show()","5bc7fe39":"plt.scatter(df[\"GRE Score\"],df[\"SOP\"])\nplt.xlabel(\"GRE Score\")\nplt.ylabel(\"SOP\")\nplt.title(\"SOP for GRE Score\")\nplt.show()","048a3084":"#reading the dataset\ndf=pd.read_csv(\"..\/input\/Admission_Predict.csv\")\n\n#it may be needed in the future\nserialNo=df['Serial No.'].values\n\ndf.drop(['Serial No.'],axis=1,inplace=True)\n\ndf=df.rename(columns = {'Chance of Admit ':'Chance of Admit'})","c4ffae92":"y = df[\"Chance of Admit\"].values\nx = df.drop([\"Chance of Admit\"],axis=1)\n\n#separating train (80%) and test (20%) sets\nfrom sklearn.model_selection import train_test_split\n\nx_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.20,random_state=42)","27f8818e":"#normalization\nfrom sklearn.preprocessing import MinMaxScaler\nscalerX=MinMaxScaler(feature_range=(0,1))\nx_train[x_train.columns]=scalerX.fit_transform(x_train[x_train.columns])\nx_test[x_test.columns]=scalerX.transform(x_test[x_test.columns])\n","824299be":"from sklearn.linear_model import LinearRegression\nlr=LinearRegression()\nlr.fit(x_train,y_train)\ny_head_lr=lr.predict(x_test)\n\nprint(\"real value of y_test[1]:\"+str(y_test[1]) + \"-> the predict:\" +str(lr.predict(x_test.iloc[[1],:])))\nprint(\"real value of y_test[2]: \" + str(y_test[2]) + \" -> the predict: \" + str(lr.predict(x_test.iloc[[2],:])))\n\nfrom sklearn.metrics import r2_score\nprint(\"r_square score:\",r2_score(y_test,y_head_lr))\n\ny_head_lr_train=lr.predict(x_train)\nprint(\"r_square score (train dataset):\",r2_score(y_train,y_head_lr_train))","acfce239":"from sklearn.ensemble import RandomForestRegressor\nrfr=RandomForestRegressor(n_estimators=100,random_state=42)\nrfr.fit(x_train,y_train)\ny_head_rfr=rfr.predict(x_test)\n\nfrom sklearn.metrics import r2_score\nprint(\"r_square score:\",r2_score(y_test,y_head_rfr))\nprint(\"real value of y_test[1]:\" +str(y_test[1])+\"-> the predict:\"+str(rfr.predict(x_test.iloc[[1],:])))\nprint(\"real value of y_test[2]:\" +str(y_test[2])+\"-> the predict:\"+str(rfr.predict(x_test.iloc[[2],:])))\n\ny_head_rf_train=rfr.predict(x_train)\nprint(\"r_square score (train dataset):\",r2_score(y_train,y_head_rf_train))","c2cc5b39":"from sklearn.tree import DecisionTreeRegressor\ndtr=DecisionTreeRegressor(random_state=42)\ndtr.fit(x_train,y_train)\ny_head_dtr=dtr.predict(x_test)\n\nfrom sklearn.metrics import r2_score\nprint(\"r_square score:\",r2_score(y_test,y_head_dtr))\nprint(\"real value of y_test[1]:\" +str(y_test[1])+ \"-> the predict\" +str(dtr.predict(x_test.iloc[[1],:])))\nprint(\"real value of y_test[2]:\" +str(y_test[2])+ \"-> the predict\"+ str(dtr.predict(x_test.iloc[[2],:])))\n\ny_head_dtr_train=dtr.predict(x_train)\nprint(\"r_square score (train dataset):\",r2_score(y_train,y_head_dtr_train))\n","d7cbd062":"y = np.array([r2_score(y_test,y_head_lr),r2_score(y_test,y_head_rfr),r2_score(y_test,y_head_dtr)])\nx=[\"LinearRegression\",\"RandomForestReg\",\"DecisionTreeReg.\"]\nplt.bar(x,y)\nplt.title(\"Comparision of Regression Algorithms\")\nplt.xlabel(\"Regressor\")\nplt.ylabel(\"r2_score\")\nplt.show()\n","510d6d05":"print(\"real value of y_test[5]:\" +str(y_test[5]) +\"-> the predict:\" +str(lr.predict(x_test.iloc[[5],:])))\nprint(\"real value of y_test[5]:\" +str(y_test[5]) +\"-> the predict:\" +str(rfr.predict(x_test.iloc[[5],:])))\nprint(\"real value of y_test[5]:\" +str(y_test[5]) +\"-> the predict:\" +str(dtr.predict(x_test.iloc[[5],:])))\nprint()\nprint(\"real value of y_test[50]:\" +str(y_test[50]) +\"-> the predict:\" +str(lr.predict(x_test.iloc[[50],:])))\nprint(\"real value of y_test[50]:\" +str(y_test[50]) +\"-> the predict:\" +str(rfr.predict(x_test.iloc[[50],:])))\nprint(\"real value of y_test[50]:\" +str(y_test[50]) +\"-> the predict:\" +str(lr.predict(x_test.iloc[[50],:])))\n","d5bd9973":"red=plt.scatter(np.arange(0,80,5),y_head_lr[0:80:5],color='red')\ngreen=plt.scatter(np.arange(0,80,5),y_head_rfr[0:80:5],color='green')\nblue=plt.scatter(np.arange(0,80,5),y_head_dtr[0:80:5],color=\"blue\")\nblack=plt.scatter(np.arange(0,80,5),y_test[0:80:5],color=\"black\")\nplt.title(\"Comparision of Regression Algorithms\")\nplt.xlabel(\"Index of Candidate\")\nplt.ylabel(\"Chance of Admit\")\nplt.legend((red,green,blue,black),('LR','RFR','DTR','REAL'))\nplt.show()","5749ce68":"df['Chance of Admit'].plot(kind='hist',bins=200,figsize=(6,6))\nplt.title(\"Chance of Admit\")\nplt.xlabel(\"Chance of Admit\")\nplt.ylabel(\"Frequency\")\nplt.show()","d342cda7":"# reading the dataset\ndf = pd.read_csv(\"..\/input\/Admission_Predict.csv\")\n\n# it may be needed in the future.\nserialNo = df[\"Serial No.\"].values\ndf.drop([\"Serial No.\"],axis=1,inplace = True)\n\ndf=df.rename(columns = {'Chance of Admit ':'Chance of Admit'})\ny = df[\"Chance of Admit\"].values\nx = df.drop([\"Chance of Admit\"],axis=1)\n\n# separating train (80%) and test (%20) sets\nfrom sklearn.model_selection import train_test_split\nx_train, x_test,y_train, y_test = train_test_split(x,y,test_size = 0.20,random_state = 42)\n","da7cdc32":"#normalization\nfrom sklearn.preprocessing import MinMaxScaler\nscalerX=MinMaxScaler(feature_range=(0,1))\nx_train[x_train.columns]=scalerX.fit_transform(x_train[x_train.columns])\nx_test[x_test.columns]=scalerX.transform(x_test[x_test.columns])\n\ny_train_01=[1 if each >0.8 else 0 for each in y_train]\ny_test_01=[1 if each > 0.8 else 0 for each in y_test]\n\n\n#List to array\ny_train_01=np.array(y_train_01)\ny_test_01=np.array(y_test_01)","e73b882c":"from sklearn.linear_model import LogisticRegression\nlrc=LogisticRegression()\nlrc.fit(x_train,y_train_01)\nprint(\"score:\",lrc.score(x_test,y_test_01))\nprint(\"real value of y_test_01[1]:\" +str(y_test_01[1]) + \"-> the predict:\" +str(lrc.predict(x_test.iloc[[1],:])))\nprint(\"real value of y_test_01[2]:\"+str(y_test_01[2]) +\"->the predict:\" +str(lrc.predict(x_test.iloc[[2],:])))\n","e02cb23a":"#confusion matrix\nfrom sklearn.metrics import confusion_matrix\ncm_lrc=confusion_matrix(y_test_01,lrc.predict(x_test))\n","86b8ac59":"#cm visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nf,ax=plt.subplots(figsize=(5,5))\nsns.heatmap(cm_lrc,annot=True,linewidths=0.5,linecolor=\"red\",fmt=\".0f\",ax=ax)\nplt.title(\"Test for Test Dataset\")\nplt.xlabel(\"Predicted y values\")\nplt.ylabel(\"Predicted y values\")\nplt.show()\n\n\nfrom sklearn.metrics import precision_score,recall_score\nprint(\"precision_score:\",precision_score(y_test_01,lrc.predict(x_test)))\nprint(\"recall_score:\",recall_score(y_test_01,lrc.predict(x_test)))\n\n\nfrom sklearn.metrics import f1_score\nprint(\"f1_score:\",f1_score(y_test_01,lrc.predict(x_test)))","dee3788f":"cm_lrc_train=confusion_matrix(y_train_01,lrc.predict(x_train))\nf,ax=plt.subplots(figsize=(5,5))\nsns.heatmap(cm_lrc_train,annot=True,linewidths=0.5,linecolor=\"red\",fmt=\".0f\",ax=ax)\nplt.xlabel(\"predicted y values\")\nplt.ylabel(\"real y values\")\nplt.title(\"Test for Train Dataset\")\nplt.show()","a8829c4f":"from sklearn.svm import SVC\nsvm=SVC(random_state=1)\nsvm.fit(x_train,y_train_01)\nprint(\"score:\",svm.score(x_test,y_test_01))\nprint(\"real value of y_test_01[1]:\"+str(y_test_01[1]))\nprint(\"real value of y_test_01[2]:\"+str(y_test_01[2]))","ba5a697c":"#confusion matrix\nfrom sklearn.metrics import confusion_matrix\ncm_svm=confusion_matrix(y_test_01,svm.predict(x_test))","db5bed0b":"#cm visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nf,ax=plt.subplots(figsize=(5,5))\nsns.heatmap(cm_svm,annot=True,linewidths=0.5,linecolor=\"red\",fmt=\".0f\",ax=ax)\nplt.title(\"Test for Test Dataset\")\nplt.xlabel(\"predicted y values\")\nplt.ylabel(\"real y values\")\nplt.show()","e4ea80df":"from sklearn.metrics import precision_score,recall_score\nprint(\"precision_score:\",precision_score(y_test_01,svm.predict(x_test)))\nprint(\"recall_score:\",recall_score(y_test_01,svm.predict(x_test)))\n\n\nfrom sklearn.metrics import f1_score\nprint(\"f1_score\",f1_score(y_test_01,svm.predict(x_test)))","f428ec86":"cm_svm_train=confusion_matrix(y_train_01,svm.predict(x_train))\nf,ax=plt.subplots(figsize=(5,5))\nsns.heatmap(cm_svm_train,annot=True,linewidth=0.5,linecolor=\"red\",fmt=\".0f\",ax=ax)\nplt.xlabel(\"predicted y values\")\nplt.ylabel(\"real y values\")\nplt.title(\"Test for train dataset\")\nplt.show()","88599e8c":"from sklearn.naive_bayes import GaussianNB\nnb=GaussianNB()\nnb.fit(x_train,y_train_01)\nprint(\"score:\",nb.score(x_test,y_test_01))\nprint(\"real value of y_test_01[1]:\"+str(y_test_01[1]))\nprint(\"real value of y_test_01[2]:\" +str(y_test_01[2]))\n","240d6f3e":"#confusian matrix\nfrom sklearn.metrics import confusion_matrix\ncm_nb=confusion_matrix(y_test_01,nb.predict(x_test))","d3f37609":"# cm visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nf, ax = plt.subplots(figsize =(5,5))\nsns.heatmap(cm_nb,annot = True,linewidths=0.5,linecolor=\"red\",fmt = \".0f\",ax=ax)\nplt.title(\"Test for Test Dataset\")\nplt.xlabel(\"predicted y  values\")\nplt.ylabel(\"real y values\")\nplt.show()","18e7ac9b":"from sklearn.metrics import precision_score,recall_score\nprint(\"precision_score\",precision_score(y_test_01,nb.predict(x_test)))\nprint(\"recall_score:\",recall_score(y_test_01,nb.predict(x_test)))\n\nfrom sklearn.metrics import f1_score\nprint(\"f1_score:\",f1_score(y_test_01,nb.predict(x_test)))","00944b7e":"cm_nb_train=confusion_matrix(y_train_01,nb.predict(x_train))\nf,ax=plt.subplots(figsize=(5,5))\nsns.heatmap(cm_nb_train,annot=True,linewidths=0.5,linecolor=\"red\",fmt=\"0.0f\",ax=ax)\nplt.xlabel(\"predicted y values\")\nplt.ylabel(\"predicted y values\")\nplt.title(\"Test for Train Dataset\")\nplt.show()","89892473":"from sklearn.tree import DecisionTreeClassifier\ndtc=DecisionTreeClassifier()\ndtc.fit(x_train,y_train_01)\nprint(\"score:\",dtc.score(x_test,y_test_01))\nprint(\"real value of y_test_01[1]:\"+str(y_test_01[1]))\nprint(\"real value of y_test_01[2]:\" +str(y_test_01[2]))","23e93154":"#confusian matrix\nfrom sklearn.metrics import confusion_matrix\ncm_dtc=confusion_matrix(y_test_01,nb.predict(x_test))","ef2929f4":"# cm visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nf, ax = plt.subplots(figsize =(5,5))\nsns.heatmap(cm_dtc,annot = True,linewidths=0.5,linecolor=\"red\",fmt = \".0f\",ax=ax)\nplt.title(\"Test for Test Dataset\")\nplt.xlabel(\"predicted y  values\")\nplt.ylabel(\"real y values\")\nplt.show()","5fc17501":"from sklearn.metrics import precision_score,recall_score\nprint(\"precision_score\",precision_score(y_test_01,dtc.predict(x_test)))\nprint(\"recall_score:\",recall_score(y_test_01,dtc.predict(x_test)))\n\nfrom sklearn.metrics import f1_score\nprint(\"f1_score:\",f1_score(y_test_01,dtc.predict(x_test)))","a52f50ab":"cm_dtc_train=confusion_matrix(y_train_01,dtc.predict(x_train))\nf,ax=plt.subplots(figsize=(5,5))\nsns.heatmap(cm_dtc_train,annot=True,linewidths=0.5,linecolor=\"red\",fmt=\"0.0f\",ax=ax)\nplt.xlabel(\"predicted y values\")\nplt.ylabel(\"predicted y values\")\nplt.title(\"Test for Train Dataset\")\nplt.show()","c6ae5f67":"from sklearn.ensemble import RandomForestClassifier\nrfc = RandomForestClassifier(n_estimators = 100,random_state = 1)\nrfc.fit(x_train,y_train_01)\nprint(\"score: \", rfc.score(x_test,y_test_01))\nprint(\"real value of y_test_01[1]: \" + str(y_test_01[1]) + \" -> the predict: \" + str(rfc.predict(x_test.iloc[[1],:])))\nprint(\"real value of y_test_01[2]: \" + str(y_test_01[2]) + \" -> the predict: \" + str(rfc.predict(x_test.iloc[[2],:])))\n\n# confusion matrix\nfrom sklearn.metrics import confusion_matrix\ncm_rfc = confusion_matrix(y_test_01,rfc.predict(x_test))\n# print(\"y_test_01 == 1 :\" + str(len(y_test_01[y_test_01==1]))) # 29\n# cm visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nf, ax = plt.subplots(figsize =(5,5))\nsns.heatmap(cm_rfc,annot = True,linewidths=0.5,linecolor=\"red\",fmt = \".0f\",ax=ax)\nplt.title(\"Test for Test Dataset\")\nplt.xlabel(\"predicted y values\")\nplt.ylabel(\"real y values\")\nplt.show()\n\nfrom sklearn.metrics import precision_score, recall_score\nprint(\"precision_score: \", precision_score(y_test_01,rfc.predict(x_test)))\nprint(\"recall_score: \", recall_score(y_test_01,rfc.predict(x_test)))\n\nfrom sklearn.metrics import f1_score\nprint(\"f1_score: \",f1_score(y_test_01,rfc.predict(x_test)))","b5743466":"cm_rfc_train = confusion_matrix(y_train_01,rfc.predict(x_train))\nf, ax = plt.subplots(figsize =(5,5))\nsns.heatmap(cm_rfc_train,annot = True,linewidths=0.5,linecolor=\"red\",fmt = \".0f\",ax=ax)\nplt.xlabel(\"predicted y values\")\nplt.ylabel(\"real y values\")\nplt.title(\"Test for Train Dataset\")\nplt.show()","4a216bd7":"from sklearn.neighbors import KNeighborsClassifier\n\n#finding k value\nscores=[]\nfor each in range(1,50):\n    knn_n=KNeighborsClassifier(n_neighbors=each)\n    knn_n.fit(x_train,y_train_01)\n    scores.append(knn_n.score(x_test,y_test_01))\nplt.plot(range(1,50),scores) \nplt.xlabel(\"k\")\nplt.ylabel(\"accuracy\")\nplt.show()\n\n\n\nknn = KNeighborsClassifier(n_neighbors = 3) # n_neighbors = k\nknn.fit(x_train,y_train_01)\nprint(\"score of 3 :\",knn.score(x_test,y_test_01))\nprint(\"real value of y_test_01[1]: \" + str(y_test_01[1]) + \" -> the predict: \" + str(knn.predict(x_test.iloc[[1],:])))\nprint(\"real value of y_test_01[2]: \" + str(y_test_01[2]) + \" -> the predict: \" + str(knn.predict(x_test.iloc[[2],:])))\n","54df3606":"#confusion matrix\nfrom sklearn.metrics import confusion_matrix\ncm_knn=confusion_matrix(y_test_01,knn.predict(x_test))\n\n# cm visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nf, ax = plt.subplots(figsize =(5,5))\nsns.heatmap(cm_knn,annot = True,linewidths=0.5,linecolor=\"red\",fmt = \".0f\",ax=ax)\nplt.title(\"Test for Test Dataset\")\nplt.xlabel(\"predicted y values\")\nplt.ylabel(\"real y values\")\nplt.show()\n\nfrom sklearn.metrics import precision_score, recall_score\nprint(\"precision_score: \", precision_score(y_test_01,knn.predict(x_test)))\nprint(\"recall_score: \", recall_score(y_test_01,knn.predict(x_test)))\n\nfrom sklearn.metrics import f1_score\nprint(\"f1_score: \",f1_score(y_test_01,knn.predict(x_test)))","c5da4a08":"y = np.array([lrc.score(x_test,y_test_01),svm.score(x_test,y_test_01),nb.score(x_test,y_test_01),dtc.score(x_test,y_test_01),rfc.score(x_test,y_test_01),knn.score(x_test,y_test_01)])\nx=[\"LogisticReg.\",\"SVM\",\"GNB\",\"DEC. Tree\",\"Ran.Forest\",\"KNN\"]\n\nplt.bar(x,y)\nplt.title(\"Comparison of Classification Algorithms\")\nplt.xlabel(\"Classificatin\")\nplt.ylabel(\"Score\")\nplt.show()","b72f7ffb":"df = pd.read_csv(\"..\/input\/Admission_Predict.csv\",sep = \",\")\ndf=df.rename(columns = {'Chance of Admit ':'ChanceOfAdmit'})\nserial = df[\"Serial No.\"]\ndf.drop([\"Serial No.\"],axis=1,inplace = True)\ndf = (df- np.min(df))\/(np.max(df)-np.min(df))\ny = df.ChanceOfAdmit \nx = df.drop([\"ChanceOfAdmit\"],axis=1)","45b8a207":"#for data visualization\nfrom sklearn.decomposition import PCA\npca=PCA(n_components=1,whiten=True) #whitten =normalize\npca.fit(x)\nx_pca=pca.transform(x)\nx_pca=x_pca.reshape(400,)\ndictionary={\"x\":x_pca,\"y\":y}\ndata=pd.DataFrame(dictionary)\nprint(\"data\")\nprint(data.head())\nprint(\"\\ndf:\")\nprint(df.head())","a38dbc9e":"df[\"Serial No.\"] = serial\nfrom sklearn.cluster import KMeans\nwcss = []\nfor k in range(1,15):\n    kmeans = KMeans(n_clusters=k)\n    kmeans.fit(x)\n    wcss.append(kmeans.inertia_)\nplt.plot(range(1,15),wcss)\nplt.xlabel(\"k values\")\nplt.ylabel(\"WCSS\")\nplt.show()\n\nkmeans = KMeans(n_clusters=3)\nclusters_knn = kmeans.fit_predict(x)\n\ndf[\"label_kmeans\"] = clusters_knn\n\n\nplt.scatter(df[df.label_kmeans == 0 ][\"Serial No.\"],df[df.label_kmeans == 0].ChanceOfAdmit,color = \"red\")\nplt.scatter(df[df.label_kmeans == 1 ][\"Serial No.\"],df[df.label_kmeans == 1].ChanceOfAdmit,color = \"blue\")\nplt.scatter(df[df.label_kmeans == 2 ][\"Serial No.\"],df[df.label_kmeans == 2].ChanceOfAdmit,color = \"green\")\nplt.title(\"K-means Clustering\")\nplt.xlabel(\"Candidates\")\nplt.ylabel(\"Chance of Admit\")\nplt.show()\n\ndf[\"label_kmeans\"] = clusters_knn\nplt.scatter(data.x[df.label_kmeans == 0 ],data[df.label_kmeans == 0].y,color = \"red\")\nplt.scatter(data.x[df.label_kmeans == 1 ],data[df.label_kmeans == 1].y,color = \"blue\")\nplt.scatter(data.x[df.label_kmeans == 2 ],data[df.label_kmeans == 2].y,color = \"green\")\nplt.title(\"K-means Clustering\")\nplt.xlabel(\"X\")\nplt.ylabel(\"Chance of Admit\")\nplt.show()","c4a01f61":"df[\"Serial No.\"] = serial\n\nfrom scipy.cluster.hierarchy import linkage, dendrogram\nmerg = linkage(x,method=\"ward\")\ndendrogram(merg,leaf_rotation = 90)\nplt.xlabel(\"data points\")\nplt.ylabel(\"euclidean distance\")\nplt.show()\n\nfrom sklearn.cluster import AgglomerativeClustering\nhiyerartical_cluster = AgglomerativeClustering(n_clusters = 3,affinity= \"euclidean\",linkage = \"ward\")\nclusters_hiyerartical = hiyerartical_cluster.fit_predict(x)\n\ndf[\"label_hiyerartical\"] = clusters_hiyerartical\n\nplt.scatter(df[df.label_hiyerartical == 0 ][\"Serial No.\"],df[df.label_hiyerartical == 0].ChanceOfAdmit,color = \"red\")\nplt.scatter(df[df.label_hiyerartical == 1 ][\"Serial No.\"],df[df.label_hiyerartical == 1].ChanceOfAdmit,color = \"blue\")\nplt.scatter(df[df.label_hiyerartical == 2 ][\"Serial No.\"],df[df.label_hiyerartical == 2].ChanceOfAdmit,color = \"green\")\nplt.title(\"Hierarchical Clustering\")\nplt.xlabel(\"Candidates\")\nplt.ylabel(\"Chance of Admit\")\nplt.show()\n\nplt.scatter(data[df.label_hiyerartical == 0 ].x,data.y[df.label_hiyerartical == 0],color = \"red\")\nplt.scatter(data[df.label_hiyerartical == 1 ].x,data.y[df.label_hiyerartical == 1],color = \"blue\")\nplt.scatter(data[df.label_hiyerartical == 2 ].x,data.y[df.label_hiyerartical == 2],color = \"green\")\nplt.title(\"Hierarchical Clustering\")\nplt.xlabel(\"X\")\nplt.ylabel(\"Chance of Admit\")\nplt.show()","f8ac87c6":"print(df.head())","59f63a1b":"fig,ax=plt.subplots(figsize=(10,10))\nsns.heatmap(df.corr(),ax=ax,annot=True,linewidths=0.05,fmt='.2f',cmap=\"magma\")\nplt.show()","07a83b78":"df = pd.read_csv(\"..\/input\/Admission_Predict.csv\",sep = \",\")\ndf=df.rename(columns = {'Chance of Admit ':'Chance of Admit'})\nnewDF = pd.DataFrame()\nnewDF[\"GRE Score\"] = df[\"GRE Score\"]\nnewDF[\"TOEFL Score\"] = df[\"TOEFL Score\"]\nnewDF[\"CGPA\"] = df[\"CGPA\"]\nnewDF[\"Chance of Admit\"] = df[\"Chance of Admit\"]\n\ny_new = df[\"Chance of Admit\"].values\nx_new = df.drop([\"Chance of Admit\"],axis=1)\n\n# separating train (80%) and test (%20) sets\nfrom sklearn.model_selection import train_test_split\nx_train_new, x_test_new,y_train_new, y_test_new = train_test_split(x_new,y_new,test_size = 0.20,random_state = 42)\n\n# normalization\nfrom sklearn.preprocessing import MinMaxScaler\nscalerX = MinMaxScaler(feature_range=(0, 1))\nx_train[x_train.columns] = scalerX.fit_transform(x_train[x_train.columns])\nx_test[x_test.columns] = scalerX.transform(x_test[x_test.columns])\n\nfrom sklearn.linear_model import LinearRegression\nlr_new = LinearRegression()\nlr_new.fit(x_train_new,y_train_new)\ny_head_lr_new = lr_new.predict(x_test_new)\n\nfrom sklearn.metrics import r2_score\nprint(\"r_square score: \", r2_score(y_test_new,y_head_lr_new))\n","1806c83b":"## <a id='regression'> REGRESSION ALGORITHMS (SUPERVISED MACHINE LEARNING ALGORITHMS)<\/a>","950e368e":"### <a id='hierarchical '>Hierarchical Clustering<\/a>","c7e6a5db":"### <a id=\"DecisionTreeRegression\">Decision Tree Regression <\/a>","5994b510":"### <a id='dtc'>Decision Tree Classification<\/a>","dd5e9760":"## <a id='feature'>THE THREE IMPORTANT FEATURES<\/a>","e7b64879":"GRE Score:\n* This histogram shows the frequency for GRE scores.\n* There is a density between 310 and 330.Being above this range would be a good feature for a candidate to stand out. ","9fcfc4a4":"Test for Train Dataset:","c168c99b":"### <a id='prepareForClassification'>Preparing Data for Classification<\/a>","044c413f":"Test for Train Dataset:","f6ea0f35":"* Candidates with high GRE scores usually have a high SOP score","699e3da1":"### <a id='comparisonOfRegression'>Comparison of Regression Algorithms<\/a>\n\n* Linear regression and random forest regression algorithms were better than decision tree regression algorithm.","b54557cc":"### <a id='comparisonOfClustering'>Comparison of Clustering Algorithms<\/a>","e456dd59":"Test for Train Dataset:","a7e94b3f":"### <a id='prepareForClustering'>Preparing Data for Clustering<\/a>","769aca76":"### <a id='prepareForRegression'>Preparing Data for Regression<\/a>","1a023e27":"CGPA Score for University Ratings:\n* As the quality of the university increases the CGPA score increases    ","d7624342":"Comment:\n* Because most candidates in the data have over 70% chance, many unsuccessful candidates are not well predicted.","2469282b":"### <a id='gnb'>Gaussian Naive Bayes<\/a>","68803a46":"Test for Train Dataset:","e81db555":"### <a id='knnc'>K Nearest Neighbors Classification<\/a>\n","bee4d7e4":"### <a id='lr'>Logistic Regression<\/a>","10203e28":"*Candidates with high CGPA scores usually have a high SOP score","2a998a72":"train_test_split:\n* it split the data into random train (80%) and test (20%) subsets.    ","d9a06063":"* The 3 most important features for admission to the Master: CGPA, GRE SCORE, and TOEFL SCORE\n* The 3 least important features for admission to the Master: Research, LOR, and SOP","718c146e":"### <a id='rfc'>Random Forest Classification<\/a>","366df918":"* If a candidate's Chance of Admit is greater than 80%, the candidate will receive the 1 label.\n* If a candidate's Chance of Admit is less than or equal to 80%, the candidate will receive the 0 label.","a0d2750f":"### <a id='kmeans'>K-means Clustering<\/a>","e0126231":"### <a id =\"randomForestRegression\">Random Forest Regresssion<\/a>","ea76a32d":"TOEFL Score:\n* The lowest TOEFL Score is 92 and the highest Toefl score is 120. The average is 107.41    ","e8575d36":"* These are the regression estimates for samples with 5 and 50 indexes:","649b6f3e":"* Candidates who graduate from good universitiew are more fortunate to be accepted.","756a5f03":"### <a id='pca'>Principal Component Analysis<\/a>","33c18fdc":"* K-means Clustering and Hierarchical Clustering are similarly.","3464e91c":"* All classification algorithms achieved around 90% success. The most successful one is Gaussian Naive Bayes with 96% score.","11b22b55":"### <a id=' linearRegression'>Linear Regression<\/a>","15a27fbc":"### <a id='ThreeLinearRegression'>The Three Features for Linear Regression<\/a>","2d355ae0":"### <a id=\"comparisonofclassificationalgorithms\">Comparison of Classification Algorithms<\/a>","4bbcdbbc":"## <a id='classification'>CLASSIFICATION ALGORITHMS (SUPERVISED MACHINE LEARNING ALGORITHMS)<\/a>","021aa1a5":"## <a id='introduction'>CLUSTERING ALGORITHMS (UNSUPERVISED MACHINE LEARNING ALGORITHMS)<\/a>","4c301168":"### <a id='svm'>Support Vector Machine<\/a>","dafd6dd9":"* Candidates with high GRE scores usually have a high CGPA score","8f0e4f9f":"This is the estimate and the actual acceptance possibilities made with 3 regression algorithms for test samples with 0, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75 indexes:","778f6830":"### <a id='correlationForFeature'>Correlation between All Columns<\/a>","30c64cb0":"* The first results for Linear Regression (7 features):\n<br> r_square score:  0.821208259148699\n\n* The results for Linear Regression now (3 features):                               \nr_square score:  0.8212241793299223\n\n* The two results are very close. If these 3 features (CGPA, GRE SCORE, and TOEFL SCORE) are used instead of all 7 features together, the result is not bad and performance is increased because less calculation is required.                                 \n"}}