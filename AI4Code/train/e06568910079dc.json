{"cell_type":{"a8d0d63e":"code","9e9c7aed":"code","04be045b":"code","30e98b69":"code","dfc2520f":"code","2a5fd0fd":"code","c5c8dd4b":"code","58cb9a14":"code","cfe26e67":"code","27be76ce":"code","28e50c40":"code","0d0532ab":"code","35b2a8c7":"code","ae5c5b39":"code","3c04055a":"code","665da66d":"code","df086a8b":"code","3e66036a":"code","9d576025":"code","03f7d7e3":"code","82b81eec":"code","97332d85":"code","16b075ad":"code","307c8ac7":"code","1d529082":"code","c67889fe":"code","f64981d3":"code","b4673e1e":"code","b0ebcf35":"code","3795adec":"code","b114b75c":"code","bc2dc379":"code","52845519":"code","f5c0fca9":"markdown","85c46445":"markdown","c56ed798":"markdown","a7c3a2fb":"markdown","eccc65ab":"markdown","43c97a80":"markdown","f60f7b15":"markdown","22350fd6":"markdown","0c426f62":"markdown","2c60014c":"markdown","6826e624":"markdown","a8b461a7":"markdown","89c3f5cf":"markdown","f9a9f79a":"markdown","217bf654":"markdown","53da3bc3":"markdown","16d3064d":"markdown","bf87d511":"markdown","91831a5f":"markdown","4fb7ad1a":"markdown"},"source":{"a8d0d63e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session\n\n# import plotting libraries\nimport matplotlib.pyplot as plt\n#import sns for better plots, it is handy to manage subplots\nimport seaborn as sns","9e9c7aed":"data = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ndata.head(5)","04be045b":"data.describe()","30e98b69":"# plotting only one variable\nsns.catplot(x=\"Sex\", y=\"Survived\", kind=\"bar\",data=data)","dfc2520f":"# similarly one can plot any other categorical & Ordinal variable\nsns.catplot(x=\"Pclass\", y=\"Survived\", kind=\"bar\",data=data)","2a5fd0fd":"# plot multiple variables together\nsns.catplot(x=\"Sex\", y=\"Survived\", hue=\"Pclass\", kind=\"bar\", data=data)","c5c8dd4b":"#An alternative style for visualizing the same information is offered by the pointplot() function. \n#This function also encodes the value of the estimate with height on the other axis, \n#but rather than showing a full bar, it plots the point estimate and confidence interval. \n#Additionally, pointplot() connects points from the same hue category. \n#This makes it easy to see how the main relationship is changing as a function of the hue semantic, \n#because your eyes are quite good at picking up on differences of slopes:\nsns.catplot(x=\"Pclass\", y=\"Survived\", hue=\"Sex\", kind=\"point\", data=data)\n# this confirms that there is an interaction effect between sex and pclass","58cb9a14":"data.groupby('Sex')['Survived'].mean()","cfe26e67":"# similarly one can calculate the survival rate for Pclass or any other categorical & ordinal variable\n# or, calculate the survival rate in combination of a few variables\ndata.groupby(['Pclass', 'Sex'])['Survived'].mean()","27be76ce":"# use other features to interpolate the missing age\n# this part is taking from another notebook\n# https:\/\/www.kaggle.com\/ash316\/eda-to-prediction-dietanic\ndata['Initial']=0\nfor i in data:\n    data['Initial']=data.Name.str.extract('([A-Za-z]+)\\.') #lets extract the Salutations\npd.crosstab(data.Initial,data.Sex).T.style.background_gradient(cmap='summer_r') #Checking the Initials with the Sex","28e50c40":"data['Initial'].replace(['Mlle','Mme','Ms','Dr','Major','Lady','Countess','Jonkheer','Col','Rev','Capt','Sir','Don'],\n                        ['Miss','Miss','Miss','Mr','Mr','Mrs','Mrs','Other','Other','Other','Mr','Mr','Mr'],inplace=True)\ndata.groupby('Initial')['Age'].mean() #lets check the average age by Initials","0d0532ab":"## Assigning the NaN Values with the Ceil values of the mean ages\ndata.loc[(data.Age.isnull())&(data.Initial=='Mr'),'Age']=33\ndata.loc[(data.Age.isnull())&(data.Initial=='Mrs'),'Age']=36\ndata.loc[(data.Age.isnull())&(data.Initial=='Master'),'Age']=5\ndata.loc[(data.Age.isnull())&(data.Initial=='Miss'),'Age']=22\ndata.loc[(data.Age.isnull())&(data.Initial=='Other'),'Age']=46\ndata.Age.isnull().any() #So no null values left finally ","35b2a8c7":"df =data[data.Sex == 'male']\nsns.distplot(df['Age'],  kde=False,label='Male')\n\ndf =data[data.Sex == 'female']\nsns.distplot(df['Age'],  kde=False,label='female')\n# Plot formatting\nplt.legend(prop={'size': 12})\nplt.title('Survival count')\nplt.xlabel('Age')\nplt.ylabel('Count')","ae5c5b39":"grid = sns.FacetGrid(data, col='Survived',height=2.2, aspect=1.6)\ngrid.map(plt.hist, 'Age', alpha=.5, bins=20)\ngrid.add_legend()","3c04055a":"grid = sns.FacetGrid(data, col='Survived',row='Sex', height=2.2, aspect=1.6)\ngrid.map(plt.hist, 'Age', alpha=.5, bins=20)\ngrid.add_legend()","665da66d":"# a mix of categorical and continuous varibales makes it difficult to choose proper models\n# also the adjacent ages don't make a big difference (being 20 or 21 doesn't change much) but more the big changes in age (20 vs 80)\n# so lets categorize the age variable into 5 levels\ndata['Age_band']=0\ndata.loc[data['Age']<=16,'Age_band']=0\ndata.loc[(data['Age']>16)&(data['Age']<=32),'Age_band']=1\ndata.loc[(data['Age']>32)&(data['Age']<=48),'Age_band']=2\ndata.loc[(data['Age']>48)&(data['Age']<=64),'Age_band']=3\ndata.loc[data['Age']>64,'Age_band']=4","df086a8b":"# sns.catplot(x=\"Age_band\", y=\"Survived\", row=\"Pclass\", hue='Sex', kind=\"box\", orient=\"h\", data=data)\nsns.catplot(x=\"Age_band\", y=\"Survived\", col=\"Pclass\", hue=\"Sex\", kind=\"point\", data=data)","3e66036a":"data['FamilySize'] = data['SibSp'] + data['Parch'] + 1\ndata['IsAlone'] = 1 #initialize to yes\/1 is alone\ndata['IsAlone'].loc[data['FamilySize'] > 1] = 0 # now update to no\/0 if family size is greater than 1","9d576025":"# turn sex into integers instead of string\ndata['Sex'] = data['Sex'].map( {'female': 0, 'male': 1} ).astype(int)","03f7d7e3":"data.columns","82b81eec":"# drop the rest columns\ndroplist = ['PassengerId', 'Name', 'Age', 'SibSp','Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked', 'Initial', 'FamilySize']\ndata.drop(droplist,axis=1,inplace=True)\nsns.heatmap(data.corr(),annot=True,cmap='RdYlGn')\nfig=plt.gcf()\nplt.xticks()\nplt.yticks()\nplt.show()","97332d85":"#importing all the required ML packages\nfrom sklearn.linear_model import LogisticRegression #logistic regression\nfrom sklearn.ensemble import RandomForestClassifier #Random Forest\nfrom sklearn.tree import DecisionTreeClassifier #Decision Tree\nfrom sklearn.model_selection import train_test_split #training and testing data split\nfrom sklearn import metrics #accuracy measure\nfrom sklearn.metrics import confusion_matrix #for confusion matrix\nfrom sklearn.model_selection import KFold #for K-fold cross validation\nfrom sklearn.model_selection import cross_val_score #score evaluation\nfrom sklearn.model_selection import cross_val_predict #prediction\nfrom sklearn.model_selection import GridSearchCV","16b075ad":"data.head(5)","307c8ac7":"train,val=train_test_split(data,test_size=0.3,random_state=42,stratify=data['Survived'])\ntrain_X=train[train.columns[1:]]\ntrain_Y=train[train.columns[:1]]\nval_X=val[val.columns[1:]]\nval_Y=val[val.columns[:1]]","1d529082":"# first fit a logistic regression\nmodel = LogisticRegression()\nmodel.fit(train_X,train_Y)\nprediction=model.predict(val_X)\nprint('The accuracy of the Logistic Regression is',metrics.accuracy_score(prediction,val_Y))","c67889fe":"# since the dataset is small, having a fixed portion of data as validation data is quite expensive\n# we use k-fold cross validation to alternate the train and validation set\n# split the data into 10 equal parts\nX=data[data.columns[1:]]\nY=data['Survived']\nkfold = KFold(n_splits=10, random_state=22) ","f64981d3":"logistic_cv_result = cross_val_score(LogisticRegression(),X,Y, cv = kfold,scoring = \"accuracy\")\nprint('The mean accuracy of the Logistic Regression under 10-fold validation is: ', np.mean(logistic_cv_result), \n      'std is: ', np.std(logistic_cv_result))","b4673e1e":"# similarly we do the same to decision tree\ntree_cv_result = cross_val_score(DecisionTreeClassifier(),X,Y, cv = kfold,scoring = \"accuracy\")\nprint('The mean accuracy of the decision tree under 10-fold validation is: ', np.mean(tree_cv_result), \n      'std is: ', np.std(tree_cv_result))","b0ebcf35":"# random forest\nforest_cv_result = cross_val_score(RandomForestClassifier(n_estimators=100),X,Y, cv = kfold,scoring = \"accuracy\")\nprint('The mean accuracy of the random forest under 10-fold validation is: ', np.mean(forest_cv_result), \n      'std is: ', np.std(forest_cv_result))","3795adec":"# random forest has a required hyper-pqrqmers: number of trees\n# we can use cross validatio and grid search to find a good number of it\nn_estimators=range(100,1000,100)\nhyper={'n_estimators':n_estimators}\ngd=GridSearchCV(estimator=RandomForestClassifier(random_state=132),param_grid=hyper,verbose=True)\ngd.fit(X,Y)\nprint(gd.best_score_)\nprint(gd.best_estimator_)\nprint(gd.best_params_)","b114b75c":"test_data = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\ntest_data.head()","bc2dc379":"# formulate test data in the same way\ntest_data['Sex'] = test_data['Sex'].map( {'female': 0, 'male': 1} ).astype(int)\ntest_data['Age_band']=0\ntest_data.loc[test_data['Age']<=16,'Age_band']=0\ntest_data.loc[(test_data['Age']>16)&(test_data['Age']<=32),'Age_band']=1\ntest_data.loc[(test_data['Age']>32)&(test_data['Age']<=48),'Age_band']=2\ntest_data.loc[(test_data['Age']>48)&(test_data['Age']<=64),'Age_band']=3\ntest_data.loc[test_data['Age']>64,'Age_band']=4\n\ntest_data['FamilySize'] = test_data['SibSp'] + test_data['Parch'] + 1\ntest_data['IsAlone'] = 1 #initialize to yes\/1 is alone\ntest_data['IsAlone'].loc[test_data['FamilySize'] > 1] = 0 # now update to no\/0 if family size is greater than 1","52845519":"# take features\nfeatures = [\"Pclass\", \"Sex\", \"Age_band\", \"IsAlone\"]\nX_test = pd.get_dummies(test_data[features])\n\n# first re-fit a logistic regression on all the training data\n# model = LogisticRegression()\nmodel = RandomForestClassifier(n_estimators=100)\nmodel.fit(X,Y)\n# make prediction\npredictions = model.predict(X_test)\n\noutput = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions})\noutput.to_csv('my_submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")","f5c0fca9":"## Exploratory Data Analysis","85c46445":"* survivial rate is low (< 40%)\n* age has missing values, and it spreads out widely\n* it is hard to tell much about the categorical\/ordinal variables by this way\n* Ticket feature may be dropped from our analysis as it contains high ratio of duplicates (22%) and there may not be a correlation between Ticket and survival.\n* Cabin feature may be dropped as it is highly incomplete or contains many null values both in training and test dataset.\n* PassengerId may be dropped from training dataset as it does not contribute to survival.\n* Name feature is relatively non-standard, may not contribute directly to survival.\n* ....\nwe look for a better way to explore the relationships --> plots","c56ed798":"### **visualize catgorical & Ordinal variables**","a7c3a2fb":"## apply model on test data and submit","eccc65ab":"lets look at continuous variable (age) now.\nBut remember there was one problem? And what kind of plots do you think is good for visualizing continuous values?","43c97a80":"## Modeling","f60f7b15":"An overview of the dataset by calling describe().\n \nWhat information can be extracted?\nhow many survived?\nIs there any missing values?","22350fd6":"we have seen previously that having siblings, parents, or children also seem to make a difference in the survival rate, \ncan we make that a feature?","0c426f62":"also we should use cross_validation to make sure the fitted model performance is generalisable to test data","2c60014c":"## feature engineering","6826e624":"> ![](https:\/\/annalyzin.files.wordpress.com\/2016\/07\/decision-trees-titanic-tutorial.png)","a8b461a7":"The logistic regression function can be written as:\n![](https:\/\/wikimedia.org\/api\/rest_v1\/media\/math\/render\/svg\/4a5e86f014eb1f0744e280eb0d68485cb8c0a6c3)\nThis way we can keep one side as the conventional linear regression. The left side of the equation is call log-odds, or probit. \nHow do we link this to the binary dependent variable (survival)? We can first reformulate the function a litte bit. By simple algebraic manipulation, the probability that Y=1 is:\n\n![](https:\/\/wikimedia.org\/api\/rest_v1\/media\/math\/render\/svg\/11e03264d56bc270428fae2334fbf3ef11b003c2)\n\nIt means with a logistic function, we can transform the linear regression output to a probability that is bounded between 0 and 1:\n![](https:\/\/res.cloudinary.com\/practicaldev\/image\/fetch\/s--yt2nSddw--\/c_imagga_scale,f_auto,fl_progressive,h_420,q_auto,w_1000\/https:\/\/dev-to-uploads.s3.amazonaws.com\/i\/l7xj9gkzufp00gt2txzu.png)\nWe take 0.5 as cutoff value to turn the probility into categorical values.\n\nThere are many different ways to understand logistic regression, for example, using latent variable:\n","89c3f5cf":"The variables are in different types: categorical (sex, embarked, sibsp, parch, embarked), Ordinal (pclass), and continuous (age). And we visualize them in different ways","f9a9f79a":"### decision tree","217bf654":"### random forest","53da3bc3":"![](https:\/\/i.imgur.com\/AC9Bq63.png)","16d3064d":"### Logistic regression","bf87d511":"there are many different models available, here we explore 3 different models:\n* logistic regression (regression type)\n* decision tree (tree type)\n* random forest (bagging + randomize features) \\\n\\\nother models that are also applicable:\n* SVM\n* KNN\/other clustering models\n* Peceptron\n* ...","91831a5f":"### **visualize continuous variables**","4fb7ad1a":"Age has missing value.                         \nHow to interpolate the missing ages? By mean? By mode?"}}