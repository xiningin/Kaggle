{"cell_type":{"5348230e":"code","93013583":"code","eb79dd29":"code","83c4b6eb":"code","756aee7c":"code","9b972846":"code","e98a9cc3":"code","84714e35":"code","40cb9cb4":"code","82582a37":"code","5b3da4ca":"code","a688fc7b":"code","32a82691":"code","51327d7c":"code","a5252a54":"code","ee2752da":"code","62dfe600":"code","9ad2efdb":"code","9aeeed10":"code","24d75cae":"code","bfa284b9":"markdown","45bdb90d":"markdown","667676d8":"markdown","70d5cac6":"markdown","be07f41d":"markdown","5d0f4662":"markdown","33cdf8b2":"markdown","cfea85e5":"markdown"},"source":{"5348230e":"import pandas as pd\nimport numpy as np\nimport os\nimport librosa\n\nfrom tqdm.notebook import tqdm, trange\nimport subprocess\n\nimport math\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchvision import transforms\nfrom PIL import Image\n\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nPATH = '..\/input\/birdsong-recognition\/'\nIMG = '..\/input\/birdsongspectrograms\/'\nos.listdir(PATH)","93013583":"transformers = transforms.Compose([\n    transforms.RandomCrop((128, 512), pad_if_needed=True, padding_mode=\"reflect\"),\n    transforms.ToTensor(),\n    transforms.Normalize((0.5), (0.5)),\n])\n\ndef load_img(path, rescale=True, normalize=True):\n    img = Image.open(path)\n    img = transformers(img)\n    return img","eb79dd29":"df = pd.read_csv(os.path.join(PATH, 'train.csv'), skiprows=0)\n\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nle.fit(df['ebird_code'].to_numpy())\n\ndf.head()","83c4b6eb":"from csv import writer\ndef append_list_as_row(file_name, list_of_elem):\n    with open(file_name, 'a+', newline='') as write_obj:\n        csv_writer = writer(write_obj)\n        csv_writer.writerow(list_of_elem)","756aee7c":"DURATION = 10\n\nimport os\ntry: \n    os.remove(\"train_val.csv\")\n    print(\"removed successfully\") \nexcept OSError as error: \n    print(error) \n    print(\"File path can not be removed\") \n    \nheader = ['target', 'filepath']\nappend_list_as_row('train_val.csv', header)\n\nfor index, row in tqdm(df.iterrows()):\n    bird = row['ebird_code']\n    audio = row['filename'].replace('mp3', 'jpg')\n    filepath = f'{audio}'\n    \n    target = le.transform([bird])[0]\n    duration = row['duration']\n    \n    if os.path.isfile(f\"{IMG}{filepath}\"):\n        tmp = []\n        tmp.append(target)\n        tmp.append(filepath)\n\n        append_list_as_row('train_val.csv', tmp)\n    \n#     if duration > 10:\n#         now = load_clip(filepath, 0, DURATION)\n#         print(now, now.size())\n#         break","9b972846":"del df\nimport gc\ngc.collect()\n\ndf = pd.read_csv('train_val.csv', skiprows=0)","e98a9cc3":"df.head()","84714e35":"VALIDATION_SIZE = 0.1\n\ndf = df.sample(frac=1).reset_index(drop=True)\n\ntotal_len = len(df)\ntrain_sz = int(total_len * (1-VALIDATION_SIZE))\nval_sz = int(total_len - train_sz)\nprint(train_sz, total_len - train_sz, len(df[:train_sz]), len(df[train_sz:]))\n\n\ndef get_features(train):\n    data = None\n    if train:\n        data = df[:train_sz]\n    else:\n        data = df[train_sz:]\n\n    for index, row in tqdm(data.iterrows()):\n        filepath = row['filepath']\n        spectrogram = load_img(IMG + filepath)\n\n        yield spectrogram, row['target']\n    \ndf.head()","40cb9cb4":"BATCH_SIZE = 128\n\ndef get_batch(data_generator):\n    X, Y = [], []\n    cnt = 0\n    for x, y in data_generator:\n        X.append(x)\n        Y.append(y)\n        cnt += 1\n        if cnt >= BATCH_SIZE:\n            break\n        \n    return torch.stack(X), torch.tensor(Y)","82582a37":"device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n\nprint(device)","5b3da4ca":"class model(nn.Module):\n    def __init__(self):\n        super(model, self).__init__()\n        self.conv1 = nn.Conv2d(1, 2, 3)\n        self.conv2 = nn.Conv2d(2, 4, 3)\n        self.conv3 = nn.Conv2d(4, 8, 3)\n        \n        fn = 6944\n        self.fc1 = nn.Linear(fn, fn*2)\n        self.fc2 = nn.Linear(fn*2, fn)\n        self.fc3 = nn.Linear(fn, fn\/\/2)\n        self.output = nn.Linear(fn\/\/2, 264)\n        \n    def forward(self, x):\n        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n        x = F.relu(F.max_pool2d(self.conv2(x), 2))\n        x = F.relu(F.max_pool2d(self.conv3(x), 2))\n        x = self.flatten(x)\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = F.relu(self.fc3(x))\n        x = self.output(x)\n        return x\n        \n    def flatten(self, x):\n        res = 1\n        for sz in x.size()[1:]:\n            res *= sz\n        return x.view(-1, res)","a688fc7b":"LR=0.0001\n\nnet = model().to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(net.parameters(), lr=LR)","32a82691":"def get_number_of_correct_for_this_batch(y_pred, y):\n    y_pred = torch.nn.Softmax(dim=1)(y_pred)\n    y_pred = torch.argmax(y_pred, dim=1)\n    correct_now = torch.eq(y_pred, y).sum()\n    return correct_now.item()","51327d7c":"EPOCHS = 40\nBEST_MODEL_PATH = 'best_model.pth'\n\nbest_loss = 1000000\npatience = 4\n\nfor epoch in range(EPOCHS):\n    #### Training\n    net.train()\n    gen = get_features(True)\n    steps = math.ceil(train_sz \/ BATCH_SIZE)\n    total_loss = 0\n    total_correct = 0\n    loop = tqdm(range(steps), total=steps)\n    for i, _ in enumerate(loop):\n        X, Y = get_batch(gen)\n        X, Y = X.to(device), Y.to(device)\n\n        # Forward propagation\n        optimizer.zero_grad()\n        y_pred = net(X)\n        loss = criterion(y_pred, Y.view(-1))\n        total_loss += loss.item()\n        \n        # Backward propagation\n        loss.backward()\n        optimizer.step()\n        \n        with torch.no_grad():\n            # Get stats\n            correct_now = get_number_of_correct_for_this_batch(y_pred, Y)\n            total_correct += correct_now\n\n            # Update stats\n            loop.update(1)\n            loop.set_description('Epoch {}\/{}'.format(epoch + 1, EPOCHS))\n            loop.set_postfix(loss=loss.item(), acc=total_correct\/((i+1) * BATCH_SIZE))\n    \n    \n    #### Validation\n    with torch.no_grad():\n        net.eval()\n        gen = get_features(False)\n        steps = math.ceil(val_sz \/ BATCH_SIZE)\n        total_loss = 0\n        total_correct = 0\n        loop = tqdm(range(steps), total=steps)\n        for i, _ in enumerate(loop):\n            X, Y = get_batch(gen)\n            X, Y = X.to(device), Y.to(device)\n\n            y_pred = net(X)\n\n            loss = criterion(y_pred, Y.view(-1))\n            total_loss += loss.item()\n\n            correct_now = get_number_of_correct_for_this_batch(y_pred, Y)\n            total_correct += correct_now\n\n            loop.update(1)\n            loop.set_description('Epoch {}\/{}'.format(epoch + 1, EPOCHS))\n            loop.set_postfix(loss=loss.item(), acc=total_correct\/((i+1) * BATCH_SIZE))\n\n        # Early Stopping\n        if total_loss < best_loss:\n            best_loss = total_loss\n            patience = 4\n            torch.save(net, BEST_MODEL_PATH)\n        else:\n            patience -= 1\n\n        if patience <= 0:\n            print(f\"Early stopping at {epoch}\")\n            break","a5252a54":"best = torch.load(BEST_MODEL_PATH)","ee2752da":"import librosa\nimport cv2\n#from https:\/\/www.kaggle.com\/daisukelab\/creating-fat2019-preprocessed-data\ndef mono_to_color(X, mean=None, std=None, norm_max=None, norm_min=None, eps=1e-6):\n    # Stack X as [X,X,X]\n#     X = np.stack([X, X, X], axis=-1)\n\n    # Standardize\n    mean = mean or X.mean()\n    X = X - mean\n    std = std or X.std()\n    Xstd = X \/ (std + eps)\n    _min, _max = Xstd.min(), Xstd.max()\n    norm_max = norm_max or _max\n    norm_min = norm_min or _min\n    if (_max - _min) > eps:\n        # Normalize to [0, 255]\n        V = Xstd\n        V[V < norm_min] = norm_min\n        V[V > norm_max] = norm_max\n        V = 255 * (V - norm_min) \/ (norm_max - norm_min)\n        V = V.astype(np.uint8)\n    else:\n        # Just zero\n        V = np.zeros_like(Xstd, dtype=np.uint8)\n    return V\n\ndef build_spectrogram(path, offset, duration):\n    y, sr = librosa.load(path, offset=offset, duration=duration)\n    total_secs = y.shape[0] \/ sr\n    M = librosa.feature.melspectrogram(y=y, sr=sr)\n    M = librosa.power_to_db(M)\n    M = mono_to_color(M)\n    \n    filename = path.split(\"\/\")[-1][:-4]\n    path = 'test.jpg'\n    cv2.imwrite(path, M, [int(cv2.IMWRITE_JPEG_QUALITY), 85])\n    return path","62dfe600":"def make_prediction(x):\n    best.eval()\n    y_pred = best(x)\n    y_pred = nn.Softmax(dim=1)(y_pred)\n    y_pred = torch.argmax(y_pred, dim=1)\n    return le.inverse_transform(y_pred)[0]","9ad2efdb":"TEST_FOLDER='..\/input\/birdsong-recognition\/test_audio\/'\n\ntry:\n    preds = []\n    test = pd.read_csv(os.path.join(PATH, 'test.csv'))\n    for index, row in test.iterrows():\n        # Get test row information\n        site = row['site']\n        start_time = row['seconds'] - 5\n        row_id = row['row_id']\n        audio_id = row['audio_id']\n\n        # Get the test sound clip\n        if site == 'site_1' or site == 'site_2':\n            path = build_spectrogram(TEST_FOLDER + audio_id + '.mp3', start_time, 5)\n            y = load_img(path)\n        else:\n            path = build_spectrogram(TEST_FOLDER + audio_id + '.mp3', 0, duration=None)\n            y = load_img(path)\n\n        # Make the prediction\n        pred = make_prediction(y, le, model)\n\n        # Store prediction\n        preds.append([row_id, pred])\nexcept Exception as e:\n    preds = pd.read_csv('..\/input\/birdsong-recognition\/sample_submission.csv')\n    print('why', e)\n        \n# print(preds)\npreds = pd.DataFrame(preds, columns=['row_id', 'birds'])","9aeeed10":"preds.head()","24d75cae":"preds.fillna('nocall', inplace=True)\npreds.to_csv('submission.csv', index=False)","bfa284b9":"## Import all the Libraries","45bdb90d":"## Train","667676d8":"## Data Preprocess","70d5cac6":"#### In this part, \n1. we first write a function to load image and run some transformers from torchvision.  \n2. After that, encode the class name from 0 to `number_of_classes`  \n3. Then make a new csv file named `train_vall.csv` which will have only 2 rows - `target` & `filepath`  \n4. Then from that CSV, we will split the whole dataset into 90% train data and 10% test data  \n5. Then we will write a data generator function to get the batch_size of data  ","be07f41d":"## Test","5d0f4662":"## Model","33cdf8b2":"### At first Thanks to some awesome kernels\n1. https:\/\/www.kaggle.com\/cwthompson\/birdsong-making-a-prediction\n2. https:\/\/www.kaggle.com\/parulpandey\/eda-and-audio-processing-with-python\n3. https:\/\/www.kaggle.com\/seriousran\/mfcc-feature-extraction-for-sound-classification\n4. https:\/\/www.kaggle.com\/hamditarek\/audio-data-analysis-using-librosa\n\n### And some discussions\n1. https:\/\/www.kaggle.com\/c\/birdsong-recognition\/discussion\/158933\n2. https:\/\/www.kaggle.com\/c\/birdsong-recognition\/discussion\/158908\n3. https:\/\/www.kaggle.com\/c\/birdsong-recognition\/discussion\/159001\n4. https:\/\/www.kaggle.com\/c\/birdsong-recognition\/discussion\/159492\n\n## Last but not the Least, Spectrogram Dataset\n1. https:\/\/www.kaggle.com\/ryches\/birdsongspectrograms","cfea85e5":"### Thanks for reading this notebook! If you found this notebook helpful, please give it an upvote. \ud83d\udcaa"}}