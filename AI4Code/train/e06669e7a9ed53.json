{"cell_type":{"fcfbe521":"code","98c649b5":"code","e002b363":"code","f1af8644":"code","036c4abe":"code","c890501d":"code","53d028cc":"code","31491218":"code","8df86425":"code","7b6849e0":"code","f7b546b6":"code","5253123d":"code","d21ce7d6":"code","5ad2fdb2":"code","8da80512":"code","a59e1f68":"code","0671fc30":"code","058e14b9":"code","c19242a9":"code","90c16cb3":"code","2df433ab":"code","69888831":"code","68d58b56":"code","5fc26fd5":"code","28fed596":"code","4dca0417":"code","cd2768ec":"code","63bc3067":"code","5ddf57cc":"code","ea4d0e1f":"code","1e3fb891":"code","d6120c90":"code","9980d4a3":"code","3f183622":"code","8f538c52":"code","ea2333be":"code","a395de95":"code","0325cbf1":"code","fc94c3c8":"code","9b37b3b7":"code","d3ebd07b":"code","07398316":"code","bc5b9e1d":"code","5a2385b1":"code","6ecd75f0":"code","3631865a":"code","709a256e":"code","2208e14e":"code","ac4c635a":"code","9eee1f23":"code","f2577bb0":"code","f3a3bbef":"code","5d7567a0":"code","330b59ef":"code","2e315fa2":"code","991d1422":"code","33af90f8":"code","e0467d41":"code","c1e74659":"markdown","7608cf01":"markdown","f2780587":"markdown","f79fac78":"markdown","1dcd655c":"markdown","b901cd25":"markdown","09a52429":"markdown","da744563":"markdown","7e57caee":"markdown","ad3adaa5":"markdown","8ab085a2":"markdown","c8c01839":"markdown","235d2e05":"markdown","a2d54804":"markdown","ad16b653":"markdown","9288e9a7":"markdown","8dbfeadd":"markdown","1cf60cff":"markdown","0a11582d":"markdown","505c44a6":"markdown","15f8b73e":"markdown","680ca685":"markdown","ffa3ec8c":"markdown","5d000a5a":"markdown","79e109d1":"markdown","425ff5db":"markdown","f60191b7":"markdown","95e03d93":"markdown","54e42016":"markdown","b4a424fa":"markdown","a002ac05":"markdown","69cc4a6b":"markdown","18afd906":"markdown","e7e25364":"markdown","c2d6e378":"markdown","bcea96fd":"markdown","d977a32c":"markdown","92392d4e":"markdown"},"source":{"fcfbe521":"import warnings\nwarnings.filterwarnings('ignore')","98c649b5":"import numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import OneHotEncoder\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()","e002b363":"import multiprocessing\nn_jobs = multiprocessing.cpu_count()-1","f1af8644":"n_jobs","036c4abe":"import os\nFILEDIR = '..\/input\/santander-customer-transaction-prediction-dataset\/'\nos.listdir(FILEDIR)","c890501d":"# Load data\ndf = pd.read_csv(FILEDIR + 'train.csv',\n                 header=0)\n\ndf.head()","53d028cc":"df.shape","31491218":"correlation = df.iloc[:, 1:].corr()\ncorrelation","8df86425":"f = plt.figure(figsize=(20, 18))\nplt.matshow(correlation, fignum=f.number)\n# plt.xticks(range(df.shape[1]), df.columns, fontsize=14, rotation=45)\n# plt.yticks(range(df.shape[1]), df.columns, fontsize=14)\ncb = plt.colorbar()\n# cb.ax.tick_params(labelsize=14)\nplt.title('Correlation Matrix', fontsize=16)\nplt.show()","7b6849e0":"fig = plt.figure(figsize=(15,5))\ncorr = correlation.iloc[1:,1:].values.reshape(-1,)  # drop target\ncorr = corr[corr != 1]\nsns.distplot(corr)\nplt.xlim((-0.02, 0.02))\nplt.show()","f7b546b6":"# distribution of var_0\nfig = plt.figure(figsize=(15,5))\nvar_0_0 = df.loc[df['target']==0,df.columns[2]].values.reshape(-1,)\nvar_0_1 = df.loc[df['target']==1,df.columns[2]].values.reshape(-1,)\nsns.distplot(var_0_0)\nsns.distplot(var_0_1)\nplt.legend([0, 1])\nplt.show()","5253123d":"def getplot_var(df, r=4, c=4):\n    fig, axs = plt.subplots(r, c, figsize=(c*4, r*3))\n    cnt = 0\n    for i in range(r):\n        for j in range(c):\n            sns.distplot(df.loc[df['target']==0,df.columns[cnt+2]].values.reshape(-1,), ax=axs[i,j], axlabel=str(df.columns[cnt+2]))\n            sns.distplot(df.loc[df['target']==1,df.columns[cnt+2]].values.reshape(-1,), ax=axs[i,j], axlabel=str(df.columns[cnt+2]))\n            axs[i,j].legend([0, 1])\n            cnt += 1\n    plt.tight_layout()\n    return","d21ce7d6":"getplot_var(df, 4, 4)","5ad2fdb2":"def missing_value_checker(df):\n    \"\"\"\n    The missing value checker\n\n    Parameters\n    ----------\n    df : dataframe\n    \n    Returns\n    ----------\n    The variables with missing value and their proportion of missing value\n    \"\"\"\n    \n    variable_proportion = [[variable, df[variable].isna().sum() \/ df.shape[0]] \n                           for variable in df.columns \n                           if df[variable].isna().sum() > 0]\n\n    print('%-30s' % 'Variable with missing values', 'Proportion of missing values')\n    for variable, proportion in sorted(variable_proportion, key=lambda x : x[1]):\n        print('%-30s' % variable, proportion)\n        \n    return variable_proportion","8da80512":"variable_proportion = missing_value_checker(df)","a59e1f68":"def categorical_feature_checker(df, target, dtype):\n    \"\"\"\n    The categorical feature checker\n\n    Parameters\n    ----------\n    df : dataframe\n    target : the target\n    dtype : the type of the feature\n    \n    Returns\n    ----------\n    The categorical features and their number of unique value\n    \"\"\"\n    \n    feature_number = [[feature, df[feature].nunique()] \n                      for feature in df.columns \n                      if feature != target and df[feature].dtype.name == dtype]\n    \n    print('%-30s' % 'Categorical feature', 'Number of unique value')\n    for feature, number in sorted(feature_number, key=lambda x : x[1]):\n        print('%-30s' % feature, number)\n    \n    return feature_number","0671fc30":"feature_number = categorical_feature_checker(df, 'target', 'object')","058e14b9":"X_raw = df.drop(['ID_code', 'target'], axis=1)\ny_raw = df['target']","c19242a9":"X, y = X_raw.copy(), y_raw.copy()","90c16cb3":"def plot_2d_space(X, y, label='Classes'):   \n    colors = ['#1F77B4', '#FF7F0E']\n    markers = ['o', '^']\n    for l, c, m in zip(np.unique(y), colors, markers):\n        plt.scatter(\n            X[y==l, 0],\n            X[y==l, 1],\n            c=c, label=l, marker=m\n        )\n    plt.title(label)\n    plt.legend(loc='upper right')\n    plt.show()","2df433ab":"from mpl_toolkits.mplot3d import Axes3D\n\ndef plot_3d_space(X, y):\n    fig = plt.figure(figsize=(10, 8))\n    ax = fig.add_subplot(111, projection='3d')\n\n    for m, c, label in [('o', '#1F77B4', 0), ('^', '#FF7F0E', 1)]:\n        xs = X[y==label].T[0]\n        ys = X[y==label].T[1]\n        zs = y[y==label]\n        ax.scatter(xs, ys, zs, c=c, marker=m)\n\n    ax.set_xlabel('PC1 Label')\n    ax.set_ylabel('PC2 Label')\n    ax.set_zlabel('Target')\n    # rotate the axes and update\n#     for angle in range(0, 360):\n#         ax.view_init(30, angle)\n#         plt.draw()\n#         plt.pause(.001)\n    plt.show()","69888831":"from sklearn.preprocessing import StandardScaler\nss = StandardScaler()\nX = ss.fit_transform(X)\n# X_test = ss.transform(X_test)","68d58b56":"from sklearn.decomposition import PCA\npca = PCA(n_components=2)\npca_X = pca.fit_transform(X)","5fc26fd5":"plot_2d_space(pca_X, y)","28fed596":"plot_3d_space(pca_X, y)","4dca0417":"# %matplotlib qt  #interactive plot\n# plot_3d_space(pca_X, y_test)","cd2768ec":"from sklearn.model_selection import train_test_split\nX, X_test, y, y_test = train_test_split(X_raw, y_raw, test_size=0.2, random_state=0, stratify=y_raw)","63bc3067":"y_raw.value_counts()","5ddf57cc":"# bar chart\ndf.target.value_counts().plot(kind='bar', title='target counts')\nplt.show()","ea4d0e1f":"from imblearn.over_sampling import RandomOverSampler\nfrom imblearn.under_sampling import RandomUnderSampler\n\n# RandomOverSampler (with random_state=0)\n# ros = RandomOverSampler(random_state=0)\n# X, y = ros.fit_sample(X, y)\n\n\n# adjust the weight of undersampling, 2:1 (for target 0,1) is a little bit better in practice.\nnum_class1 = len(y[y==1])\nrus = RandomUnderSampler(random_state=0, ratio={0: int(1*num_class1), 1: num_class1})\n\nX, y = rus.fit_sample(X, y)\npd.DataFrame(data=y, columns=['target'])['target'].value_counts()","1e3fb891":"pd.DataFrame(data=y, columns=['target']).target.value_counts().plot(kind='bar', title='target counts with balanced ')\nplt.show()","d6120c90":"pca_X = pca.fit_transform(X)","9980d4a3":"plot_2d_space(pca_X, y)","3f183622":"plot_3d_space(pca_X, y)","8f538c52":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost.sklearn import XGBClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB","ea2333be":"from sklearn.metrics import roc_auc_score","a395de95":"clfs = {'rf': RandomForestClassifier(random_state=0),\n        'lr': LogisticRegression(random_state=0),\n        'mlp': MLPClassifier(random_state=0),\n        'dt': DecisionTreeClassifier(random_state=0),\n        'rf': RandomForestClassifier(random_state=0),\n        'xgb': XGBClassifier(seed=0),\n#         'svc': SVC(random_state=0),\n#         'knn': KNeighborsClassifier(),\n        'gnb': GaussianNB()}","0325cbf1":"from sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\n\npipe_clfs = {}\n\nfor name, clf in clfs.items():\n    pipe_clfs[name] = Pipeline([('StandardScaler', StandardScaler()),\n                                ('clf', clf)])","fc94c3c8":"# For GridSearchCV\n\nparam_grids = {}\n# ------\nC_range = [10 ** i for i in range(-5, 1)]\nparam_grid = [{'clf__multi_class': ['ovr'],\n               'clf__solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],\n               'clf__C': C_range},\n\n              {'clf__multi_class': ['multinomial'],\n               'clf__solver': ['newton-cg', 'lbfgs', 'sag', 'saga'],\n               'clf__C': C_range}]\n\nparam_grids['lr'] = param_grid\n# ------\nparam_grid = [{'clf__hidden_layer_sizes': [(10,), (10, 10, 10), (100,)],\n               'clf__activation': ['tanh', 'relu']}]\n\nparam_grids['mlp'] = param_grid\n# ------\nparam_grid = [{'clf__min_samples_split': [2, 5, 10],\n               'clf__min_samples_leaf': [20, 60, 150]}]\n\nparam_grids['dt'] = param_grid\n# ------\nparam_grid = [{'clf__n_estimators': [10, 100],\n               'clf__min_samples_split': [5, 10, 30],\n               'clf__min_samples_leaf': [2, 5, 10]}]\n\nparam_grids['rf'] = param_grid\n# ------\nparam_grid = [{'clf__eta': [10 ** i for i in range(-6, -1)],\n               'clf__gamma': [0, 10, 100],\n               'clf__lambda': [10 ** i for i in range(-6, -1)]}]\n\nparam_grids['xgb'] = param_grid\n# ------\nparam_grid = [{'clf__C': [10 ** i for i in range(-4, 5)],\n               'clf__gamma': ['auto', 'scale']}]\n\nparam_grids['svc'] = param_grid\n# ------\nparam_grid = [{'clf__n_neighbors': list(range(1, 11))}]\n\nparam_grids['knn'] = param_grid\n# ------\nparam_grid = [{'clf__var_smoothing': [10 ** i for i in range(-12, -4)]}]\n\nparam_grids['gnb'] = param_grid","9b37b3b7":"# best parameter (selected after GSCV)\n\nparam_grids = {}\n# ------\nC_range = [0.001]\nparam_grid = [{'clf__multi_class': ['ovr'],\n               'clf__solver': ['sag'],\n               'clf__C': C_range}]\n\nparam_grids['lr'] = param_grid\n# ------\nparam_grid = [{'clf__hidden_layer_sizes': [(10,)],\n               'clf__activation': ['relu']}]\n\nparam_grids['mlp'] = param_grid\n# ------\nparam_grid = [{'clf__min_samples_split': [2],\n               'clf__min_samples_leaf': [150]}]\n\nparam_grids['dt'] = param_grid\n# ------\nparam_grid = [{'clf__n_estimators': [200],\n               'clf__min_samples_split': [30],\n               'clf__min_samples_leaf': [2]}]\n\nparam_grids['rf'] = param_grid\n# ------\nparam_grid = [{'clf__eta': [1e-6],\n               'clf__gamma': [10],\n               'clf__lambda': [1e-6]}]\n\nparam_grids['xgb'] = param_grid\n# ------\nparam_grid = [{'clf__C': [0.1],\n               'clf__gamma': ['auto']}]\n\nparam_grids['svc'] = param_grid\n# ------\nparam_grid = [{'clf__n_neighbors': 10}]\n\nparam_grids['knn'] = param_grid\n# ------\nparam_grid = [{'clf__var_smoothing': [1e-12]}]\n\nparam_grids['gnb'] = param_grid\n","d3ebd07b":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import StratifiedKFold\n\n# ------\ngsRF = GridSearchCV(pipe_clfs['rf'], param_grid=param_grids['rf'],\n                    cv=StratifiedKFold(n_splits=4, random_state=0),\n                    scoring=\"roc_auc\", n_jobs=-1, verbose=1)\n\ngsRF.fit(X, y)\nRF_best = gsRF.best_estimator_\nprint(gsRF.best_score_, gsRF.best_estimator_)\n# ------\ngsXGB = GridSearchCV(pipe_clfs['xgb'], param_grid=param_grids['xgb'],\n                    cv=StratifiedKFold(n_splits=4, random_state=0),\n                    scoring=\"roc_auc\", n_jobs=-1, verbose=1)\n\ngsXGB.fit(X, y)\nXGB_best = gsXGB.best_estimator_\nprint(gsXGB.best_score_, gsXGB.best_estimator_)\n# ------\ngsLR = GridSearchCV(pipe_clfs['lr'], param_grid=param_grids['lr'],\n                    cv=StratifiedKFold(n_splits=4, random_state=0),\n                    scoring=\"roc_auc\", n_jobs=-1, verbose=1)\n\ngsLR.fit(X, y)\nLR_best = gsLR.best_estimator_\nprint(gsLR.best_score_, gsLR.best_estimator_)\n# ------\ngsGNB = GridSearchCV(pipe_clfs['gnb'], param_grid=param_grids['gnb'],\n                    cv=StratifiedKFold(n_splits=4, random_state=0),\n                    scoring=\"roc_auc\", n_jobs=-1, verbose=1)\n\ngsGNB.fit(X, y)\nGNB_best = gsGNB.best_estimator_\nprint(gsGNB.best_score_, gsGNB.best_estimator_)\n# ------\n# gsSVC = GridSearchCV(pipe_clfs['svc'], param_grid=param_grids['svc'],\n#                     cv=StratifiedKFold(n_splits=4, random_state=0),\n#                     scoring=\"roc_auc\", n_jobs=-1, verbose=1)\n#\n# gsSVC.fit(X, y)\n# SVC_best = gsSVC.best_estimator_\n# print(gsSVC.best_score_, gsSVC.best_estimator_)\n# ------\n# gsKNN = GridSearchCV(pipe_clfs['knn'], param_grid=param_grids['knn'],\n#                     cv=StratifiedKFold(n_splits=4, random_state=0),\n#                     scoring=\"roc_auc\", n_jobs=-1, verbose=1)\n#\n# gsKNN.fit(X, y)\n# KNN_best = gsKNN.best_estimator_\n# print(gsKNN.best_score_, gsKNN.best_estimator_)\n# ------\ngsMLP = GridSearchCV(pipe_clfs['mlp'], param_grid=param_grids['mlp'],\n                    cv=StratifiedKFold(n_splits=4, random_state=0),\n                    scoring=\"roc_auc\", n_jobs=-1, verbose=1)\n\ngsMLP.fit(X, y)\nMLP_best = gsMLP.best_estimator_\nprint(gsMLP.best_score_, gsMLP.best_estimator_)\n# ------\ngsDT = GridSearchCV(pipe_clfs['dt'], param_grid=param_grids['dt'],\n                    cv=StratifiedKFold(n_splits=4, random_state=0),\n                    scoring=\"roc_auc\", n_jobs=-1, verbose=1)\n\ngsDT.fit(X, y)\nDT_best = gsDT.best_estimator_\nprint(gsDT.best_score_, gsDT.best_estimator_)\n# ------","07398316":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import StratifiedKFold\n\n# The list of [best_score_, best_params_, best_estimator_]\nbest_score_param_estimators = []\n\n# For each classifier\nfor name in pipe_clfs.keys():\n    # Implement me\n    # GridSearchCV\n    gs = GridSearchCV(estimator=pipe_clfs[name],\n                      param_grid=param_grids[name],\n                      scoring='roc_auc',\n                      n_jobs=-1,\n                      verbose=1,\n                      iid=False,\n                      cv=StratifiedKFold(n_splits=10,\n                                         random_state=0),\n                      return_train_score=True)\n    # Implement me\n    # Fit the pipeline\n    gs = gs.fit(X, y)\n\n    # Update best_score_param_estimators\n    best_score_param_estimators.append([gs.best_score_, gs.best_params_, gs.best_estimator_])\n    \n    # Sort cv_results in ascending order of 'rank_test_score' and 'std_test_score'\n    cv_results = pd.DataFrame.from_dict(gs.cv_results_).sort_values(by=['rank_test_score', 'std_test_score'])\n    \n    # Get the important columns in cv_results\n    important_columns = ['rank_test_score',\n                         'mean_test_score', \n                         'std_test_score', \n                         'mean_train_score', \n                         'std_train_score',\n                         'mean_fit_time', \n                         'std_fit_time',                        \n                         'mean_score_time', \n                         'std_score_time']\n    \n    # Move the important columns ahead\n    cv_results = cv_results[important_columns + sorted(list(set(cv_results.columns) - set(important_columns)))]\n\n    # Write cv_results file\n#     cv_results.to_csv(path_or_buf=name + '_cv_results.csv', index=False)","bc5b9e1d":"# Sort best_score_param_estimators in descending order of the best_score_\nbest_score_param_estimators = sorted(best_score_param_estimators, key=lambda x : x[0], reverse=True)\n\n# Print best_score_param_estimators\nfor rank in range(len(best_score_param_estimators)):\n    best_score, best_params, best_estimator = best_score_param_estimators[rank]\n\n    print('Top', str(rank + 1))    \n    print('%-15s' % 'best_score:', best_score)\n    print('%-15s' % 'best_estimator:'.format(20), type(best_estimator.named_steps['clf']))\n    print('%-15s' % 'best_params:'.format(20), best_params, end='\\n\\n')","5a2385b1":"from sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Create the pipeline with StandardScaler and RandomForestClassifier\npipe_rf = Pipeline([('StandardScaler', StandardScaler()),\n                    ('RandomForestClassifier', RandomForestClassifier(class_weight=None, criterion='gini', max_features='auto',\n                            min_samples_leaf=2,\n                            min_samples_split=30,\n                            min_weight_fraction_leaf=0.0,\n                            n_estimators=100, n_jobs=-1, random_state=0,\n                            verbose=1, warm_start=False))])\n\npipe_rf.fit(X, y)","6ecd75f0":"roc_auc_score(y_test, pipe_rf.predict_proba(X_test).T[1])","3631865a":"import matplotlib.pyplot as plt\n\nfeature_value_names = df.columns[2:]\n# Convert the importances into one-dimensional 1darray with corresponding df column names as axis labels\nf_importances = pd.Series(pipe_rf.named_steps['RandomForestClassifier'].feature_importances_, feature_value_names)\n\n# Sort the array in descending order of the importances\nf_importances = f_importances.sort_values(ascending=False)\n\n# Draw the bar Plot from f_importances \nf_importances[:20].plot(x='Features', y='Importance', kind='bar', figsize=(8,5), rot=45, fontsize=14)\n\n# Show the plot\nplt.tight_layout()\nplt.show()","709a256e":"# Select out the more important features\nX_new = X_raw.loc[:, f_importances[f_importances>0.005].index]","2208e14e":"X_new.head()","ac4c635a":"f_importances[f_importances>0.005].sum()","9eee1f23":"X_new, X_test, y_new, y_test = train_test_split(X_new, y_raw, test_size=0.2, random_state=0, stratify=y_raw)","f2577bb0":"# Here just use a simple demo\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import StratifiedKFold\nparam_grid = [{'clf__min_samples_split': [2],\n               'clf__min_samples_leaf': [5000]}]\n\nparam_grids['dt'] = param_grid\n\ngsDT = GridSearchCV(pipe_clfs['dt'], param_grid=param_grids['dt'],\n                    cv=StratifiedKFold(n_splits=4, random_state=0),\n                    scoring=\"roc_auc\", n_jobs=-1, verbose=1)\n\ngsDT.fit(X, y)\nDT_best = gsDT.best_estimator_\nprint(gsDT.best_score_, gsDT.best_estimator_)","f3a3bbef":"# # Draw the tree with original features\n# from pydotplus import graph_from_dot_data\n# from sklearn.tree import export_graphviz\n# from IPython.display import Image\n\n# feature_value_names = df.columns[2:]\n# dot_data = export_graphviz(DT_best.named_steps['clf'],\n#                            filled=True, \n#                            rounded=True,\n#                            class_names=['0', \n#                                         '1'],\n#                            feature_names=feature_value_names,\n#                            out_file=None) \n\n# graph = graph_from_dot_data(dot_data) \n\n# Image(graph.create_png())","5d7567a0":"param_grid = [{'clf__min_samples_split': [2],\n               'clf__min_samples_leaf': [20000]}]\n\nparam_grids['dt'] = param_grid\n\ngsDT = GridSearchCV(pipe_clfs['dt'], param_grid=param_grids['dt'],\n                    cv=StratifiedKFold(n_splits=4, random_state=0),\n                    scoring=\"roc_auc\", n_jobs=-1, verbose=1)\n\ngsDT.fit(X_new, y_new)\nDT_best = gsDT.best_estimator_\nprint(gsDT.best_score_, gsDT.best_estimator_)","330b59ef":"# # Draw the tree with selected features\n# feature_value_names = f_importances[f_importances>0.005].index\n# dot_data = export_graphviz(DT_best.named_steps['clf'],\n#                            filled=True, \n#                            rounded=True,\n#                            class_names=['0', \n#                                         '1'],\n#                            feature_names=feature_value_names,\n#                            out_file=None) \n\n# graph = graph_from_dot_data(dot_data) \n\n# Image(graph.create_png()) ","2e315fa2":"X, X_test, y, y_test = train_test_split(X_raw, y_raw, test_size=0.2, random_state=0, stratify=y_raw)\n# X_test, X_val, y_test, y_val = train_test_split(X_test, y_test, test_size=0.2, random_state=0, stratify=y_test)","991d1422":"# Ensemble - Stacking - Logistic Regression\n# something wrong with the sklearn version. Cannot import StackingClassifier\nfrom sklearn.ensemble import StackingClassifier\nstackingC = StackingClassifier(estimators=[('rf', RF_best), ('lr', LR_best), ('dt', DT_best), #('knn', KNN_best), ('svc', SVC_best),   ##KNN makes model so large\n('mlp', MLP_best), ('xgb', XGB_best), ('gnb', GNB_best)], final_estimator=LogisticRegression(), n_jobs=-1, verbose=1)\nstackingC.fit(X, y)\ny_pred_stacking = stackingC.predict_proba(X_test).T[1]\n# print(confusion_matrix(y_test, y_pred_stacking))\nprint(roc_auc_score(y_test, y_pred_stacking))","33af90f8":"# Ensemble - Stacking - Voting\nfrom sklearn.ensemble import VotingClassifier\nvotingC = VotingClassifier(estimators=[('rf', RF_best), ('lr', LR_best), ('dt', DT_best), #('knn', KNN_best), ('svc', SVC_best),   ##KNN makes model so large\n('mlp', MLP_best), ('xgb', XGB_best), ('gnb', GNB_best)], voting='soft', n_jobs=-1)\nvotingC.fit(X, y)\ny_pred_stacking = votingC.predict_proba(X_test).T[1]\n# print(confusion_matrix(y_test, y_pred_stacking))\nprint(roc_auc_score(y_test, y_pred_stacking))","e0467d41":"# FOR SUBMISSION\ndf_test = pd.read_csv(FILEDIR + 'test.csv',\n                 header=0)\ny_pred_voting = votingC.predict_proba(df_test.iloc[:,1:]).T[1]\ndf_test['target'] = y_pred_voting\ndf_sub = df_test.loc[:,['ID_code', 'target']]\ndf_sub.to_csv('submission.csv', index=False)","c1e74659":"## Resampling","7608cf01":"**ATTENTION. PLEASE AVIOD TO RUN THIS PART. IT WILL TAKE HOURS.**  \nWe run this code in AWS, so no results here.  \n\nSeparately grid search.","f2780587":"The roc_auc_score function computes the area under the receiver operating characteristic (ROC) curve.\n\nCompared to metrics such as the subset accuracy, the Hamming loss, or the F1 score, ROC doesn\u2019t require optimizing a threshold for each label.\n\nAUC-ROC curve is one of the most commonly used metrics to evaluate the performance of machine learning algorithms particularly in the case where we have imbalanced datasets.\n\nNote: this implementation is restricted to the binary classification task or multilabel classification task in label indicator format.\n","f79fac78":"## Handling categorical variable","1dcd655c":"## PCA","b901cd25":"Using PCA to take a glance at the distribution of this high dimensional dataset (without balancing)","09a52429":"Santander, a wholly-owned subsidiary of Spanish Santander Group, commits to helping people and businesses get better in finance. For these reasons, Kagglers was invited by Santander to help them find which clients would be able to make specific trades in the future.\n\nIn this challenge, the data provided by the contest has the same structure as the real data that solved the problem, but the data is anonymized. \n","da744563":"In this project, we aim to solve the binary classification problems. We need to predict the value of target in the testing set.\n\nIn the following, we will explore the dataset, prepare it for several models, train them and find the best model for our dataset.","7e57caee":"# Improvement","ad3adaa5":"## Import models","8ab085a2":"# Data Preprocessing","c8c01839":"## Feature importance","235d2e05":"## Load data","a2d54804":"### Undersampling\nIn this dataset, we have tried oversampling and undersampling. Actually, their results are very close. Here we use undersampling instead of oversampling because the size of dataset is large enough. Using undersampling can make it efficient in hyperparameter tuning.","ad16b653":"## Correlation matrix","9288e9a7":"## Get features and target","8dbfeadd":"Total importance of chosen features: $$\\int = 48.1\\%$$","1cf60cff":"## Create the dictionary of classifiers","0a11582d":"### Without balancing","505c44a6":"### Model  selection\n\nGNB is the best single model for this dataset. \n\n    1. Fast & well performance\n\n    2. This dataset fits to GNB\u2019s assumption of normal distribution.\n\n### Further improvements\n \nEnsemble models is an excellent way to improve the final prediction. \n","15f8b73e":"## Evaluation","680ca685":"## Create the dictionary of pipeline","ffa3ec8c":"## Data split","5d000a5a":"# Objective","79e109d1":"# Conclusion","425ff5db":"Giving an example of Decision Tree. (200 features \/ 54 features)  ","f60191b7":"This dataset contains 200 numeric feature variables and 1 binary target.\n\nBoth training set and testing set have 200 thousands rows. ","95e03d93":"## Ensemble models","54e42016":"$\\mu \\approx 0$ and $\\sigma \\approx 0.0025$","b4a424fa":"**ATTENTION. PLEASE AVIOD TO RUN THIS PART. IT WILL TAKE HOURS.**  \nWe run this code in AWS, so no results here.  \n\nGrid search together.","a002ac05":"<h1>Table of Contents<span class=\"tocSkip\"><\/span><\/h1>\n<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Overview\" data-toc-modified-id=\"Overview-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;<\/span>Overview<\/a><\/span><\/li>\n<li><span><a href=\"#Objective\" data-toc-modified-id=\"Objective-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;<\/span>Objective<\/a><\/span><\/li><li><span><a href=\"#Data-Preprocessing\" data-toc-modified-id=\"Data-Preprocessing-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;<\/span>Data Preprocessing<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Load-data\" data-toc-modified-id=\"Load-data-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;<\/span>Load data<\/a><\/span><\/li><li><span><a href=\"#Correlation-matrix\" data-toc-modified-id=\"Correlation-matrix\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;<\/span>Correlation matrix<\/a><\/span><\/li><li><span><a href=\"#Handling-missing-value\" data-toc-modified-id=\"Handling-missing-value-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;<\/span>Handling missing value<\/a><\/span><li><span><a href=\"#Handling-categorical-variable\" data-toc-modified-id=\"Handling-categorical-variable-3.4\"><span class=\"toc-item-num\">3.4&nbsp;&nbsp;<\/span>Handling categorical variable<\/a><\/span><\/li>\n<li><span><a href=\"#Get-features-and-target\" data-toc-modified-id=\"Get-features-and-target-3.5\"><span class=\"toc-item-num\">3.5&nbsp;&nbsp;<\/span>Get features and target<\/a><\/span>\n<li><span><a href=\"#PCA\" data-toc-modified-id=\"PCA-3.6\"><span class=\"toc-item-num\">3.6&nbsp;&nbsp;<\/span>PCA<\/a><\/span>\n<ul class=\"toc-item\">\n<li><span><a href=\"#Without-balancing\" data-toc-modified-id=\"Without-balancing-3.6.1\"><span class=\"toc-item-num\">3.5.1&nbsp;&nbsp;<\/span>Without balancing<\/a><\/span><\/li>\n<li><span><a href=\"#With-balancing\" data-toc-modified-id=\"With-balancing-3.6.2\"><span class=\"toc-item-num\">3.5.2&nbsp;&nbsp;<\/span>With balancing<\/a><\/span><\/li><\/ul><\/li>\n<li><span><a href=\"#Data-split\" data-toc-modified-id=\"Data-split-3.7\"><span class=\"toc-item-num\">3.7&nbsp;&nbsp;<\/span>Data split<\/a><\/span><\/li>\n<li><span><a href=\"#Resampling\" data-toc-modified-id=\"Resampling-3.8\"><span class=\"toc-item-num\">3.8&nbsp;&nbsp;<\/span>Resampling<\/a><\/span><\/li><\/ul><\/li>\n<li><span><a href=\"#Model-setup\" data-toc-modified-id=\"Model-setup-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;<\/span>Model setup<\/a><\/span>\n<ul class=\"toc-item\">\n<li><span><a href=\"#Import-models\" data-toc-modified-id=\"Import-models-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;<\/span>Import models<\/a><\/span><\/li>\n<li><span><a href=\"#Evaluation\" data-toc-modified-id=\"Evaluation-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;<\/span>Evaluation<\/a><\/span><\/li>\n<li><span><a href=\"#Model-selection\" data-toc-modified-id=\"Model-selection-4.4\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;<\/span>Model selection<\/a><\/span><\/li>\n    <ul class=\"toc-item\">\n<li><span><a href=\"#Create-the-dictionary-of-classifiers\" data-toc-modified-id=\"Create-the-dictionary-of-classifiers-4.3.1\"><span class=\"toc-item-num\">4.3.1&nbsp;&nbsp;<\/span>Create the dictionary of classifiers<\/a><\/span><\/li>\n<li><span><a href=\"#Create-the-dictionary-of-pipeline\" data-toc-modified-id=\"Create-the-dictionary-of-pipeline-4.3.2\"><span class=\"toc-item-num\">4.3.2&nbsp;&nbsp;<\/span>Create the dictionary of pipeline<\/a><\/span><\/li>\n<li><span><a href=\"#Create-the-dictionary-of-parameter-grids\" data-toc-modified-id=\"Create-the-dictionary-of-parameter-grids-4.3.3\"><span class=\"toc-item-num\">4.3.3&nbsp;&nbsp;<\/span>Create the dictionary of parameter grids<\/a><\/span><\/li><\/ul>\n<li><span><a href=\"#Hyperparameter-tuning\" data-toc-modified-id=\"Hyperparameter-tuning-4.4\"><span class=\"toc-item-num\">4.4&nbsp;&nbsp;<\/span>Hyperparameter tuning<\/a><\/span><\/li><\/ul>\n<li><span><a href=\"#Improvement\" data-toc-modified-id=\"Improvement-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;<\/span>Improvement<\/a><\/span>\n<ul class=\"toc-item\">\n<li><span><a href=\"#Feature-importance\" data-toc-modified-id=\"Feature-importance-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;<\/span>Feature importance<\/a><\/span><\/li>\n<li><span><a href=\"#Ensemble-models\" data-toc-modified-id=\"Ensemble-models-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;<\/span>Ensemble models<\/a><\/span><\/li><\/ul><\/li>\n<li><span><a href=\"#Conclusion\" data-toc-modified-id=\"Conclusion-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;<\/span>Conclusion<\/a><\/span><\/li><\/ul><\/div>","69cc4a6b":"# Model selection","18afd906":"### With balancing","e7e25364":"## Handling missing value","c2d6e378":"## Create the dictionary of parameter grids","bcea96fd":"# Model setup","d977a32c":"## Hyperparameter tuning","92392d4e":"- **Stacking**\n- Idea: learn an extra model to aggregate all the results of different models to get a final result.    \n$$f(\\hat{y}_1, \\hat{y}_2, \\hat{y}_3, ...)$$  \n    - Simply, the $f(...)$ can be a vote function.\n    - But stacking uses a more complicated function, like a logistic model.\n    $$\\hat{y} = \\frac{1}{1+e^{-z}}$$  \n    where,  \n    $$z = w_0 + w_1*\\hat{y}_1 + w_2*\\hat{y}_2 + ...$$  \n- When to use: if you have a huge data.  "}}