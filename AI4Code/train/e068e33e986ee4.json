{"cell_type":{"b06a2ab1":"code","97123cd5":"code","52223e7e":"code","a3630ffa":"code","b0d1af83":"code","918eb355":"code","a80300ae":"code","24d29f4d":"code","d13caa00":"code","1e261862":"code","39834b5c":"code","df1f861c":"code","c00f337b":"code","b1444064":"code","d6e4c50e":"code","bca1d5c0":"code","f154e365":"code","19c5031c":"code","e0c0762b":"code","da6c2582":"code","6a20e973":"code","353300b6":"code","23ab77ab":"code","327ce76c":"code","7189df78":"markdown","51c4a81f":"markdown","3fe42a15":"markdown","4bff411d":"markdown","b9f6c837":"markdown","7994cfec":"markdown","cd17fbf8":"markdown","38099fe7":"markdown","b64694c1":"markdown","ebe38776":"markdown","387cd4d5":"markdown","1b0e53bb":"markdown","b307c64a":"markdown","d9f24e74":"markdown"},"source":{"b06a2ab1":"import numpy as np \nimport pandas as pd \n\n# text processing libraries\nimport re\nimport string\nimport nltk\nfrom nltk.corpus import stopwords\n\n# XGBoost\nimport xgboost as xgb\nfrom xgboost import XGBClassifier\n\n# sklearn \nfrom sklearn import model_selection\nfrom sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import f1_score\nfrom sklearn import preprocessing, decomposition, model_selection, metrics, pipeline\nfrom sklearn.model_selection import GridSearchCV,StratifiedKFold,RandomizedSearchCV\n\n# matplotlib and seaborn for plotting\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# File system manangement\nimport os","97123cd5":"root_path = \"..\/input\/nlp-getting-started\"\ntrain_path = os.path.join(root_path,\"train.csv\")\ntrain = pd.read_csv(train_path)\ntrain.head()","52223e7e":"test_path = os.path.join(root_path,\"test.csv\")\ntest = pd.read_csv(test_path)\ntest.head()","a3630ffa":"train['target'].value_counts()","b0d1af83":"sns.barplot(x=(train['target']==1).value_counts(),y=train['target'].value_counts(),palette=\"magma\",data=train)","918eb355":"train['text'][:10]","a80300ae":"# A disaster tweet\ndisaster_tweets = train[train['target']==1]['text']\ndisaster_tweets.values[1]","24d29f4d":"#not a disaster tweet\nnon_disaster_tweets = train[train['target']==0]['text']\nnon_disaster_tweets.values[1]","d13caa00":"from wordcloud import WordCloud\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=[20, 8])\nwordcloud1 = WordCloud( background_color='black',\n                        width=600,\n                        height=400).generate(\" \".join(disaster_tweets))\nax1.imshow(wordcloud1)\nax1.axis('off')\nax1.set_title('Disaster Tweets',fontsize=40);\n\nwordcloud2 = WordCloud( background_color='black',\n                        width=600,\n                        height=400).generate(\" \".join(non_disaster_tweets))\nax2.imshow(wordcloud2)\nax2.axis('off')\nax2.set_title('Non Disaster Tweets',fontsize=40);","1e261862":"def clean_text(text):\n    '''Make text lowercase, remove text in square brackets,remove links,remove punctuation\n    and remove words containing numbers.'''\n    text = text.lower()\n    text = re.sub('\\[.*?\\]', '', text)\n    text = re.sub('https?:\/\/\\S+|www\\.\\S+', '', text)\n    text = re.sub('<.*?>+', '', text)\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n    text = re.sub('\\n', '', text)\n    text = re.sub('\\w*\\d\\w*', '', text)\n    return text\n\n# Applying the cleaning function to both test and training datasets\ntrain['text'] = train['text'].apply(lambda x: clean_text(x))\ntest['text'] = test['text'].apply(lambda x: clean_text(x))\n\n# Let's take a look at the updated text\ntrain['text'].head()","39834b5c":"text = \"Are you coming , aren't you\"\ntokenizer1 = nltk.tokenize.WhitespaceTokenizer()\ntokenizer2 = nltk.tokenize.TreebankWordTokenizer()\ntokenizer3 = nltk.tokenize.WordPunctTokenizer()\ntokenizer4 = nltk.tokenize.RegexpTokenizer(r'\\w+')\n\nprint(\"Example Text: \",text)\nprint(\"------------------------------------------------------------------------------------------------\")\nprint(\"Tokenization by whitespace:- \",tokenizer1.tokenize(text))\nprint(\"Tokenization by words using Treebank Word Tokenizer:- \",tokenizer2.tokenize(text))\nprint(\"Tokenization by punctuation:- \",tokenizer3.tokenize(text))\nprint(\"Tokenization by regular expression:- \",tokenizer4.tokenize(text))","df1f861c":"# Tokenizing the training and the test set\ntokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\ntrain['text'] = train['text'].apply(lambda x: tokenizer.tokenize(x))\ntest['text'] = test['text'].apply(lambda x: tokenizer.tokenize(x))\ntrain['text'].head()","c00f337b":"#Needed While you run in COLAB\nnltk.download('stopwords')","b1444064":"def remove_stopwords(text):\n    \"\"\"\n    Removing stopwords belonging to english language\n    \n    \"\"\"\n    words = [w for w in text if w not in stopwords.words('english')]\n    return words\n\n\ntrain['text'] = train['text'].apply(lambda x : remove_stopwords(x))\ntest['text'] = test['text'].apply(lambda x : remove_stopwords(x))\ntrain.head()","d6e4c50e":"#Needed While you run in COLAB\nnltk.download('wordnet')","bca1d5c0":"# Stemming and Lemmatization examples\ntext = \"feet cats wolves talked\"\n\ntokenizer = nltk.tokenize.TreebankWordTokenizer()\ntokens = tokenizer.tokenize(text)\n\n# Stemmer\nstemmer = nltk.stem.PorterStemmer()\nprint(\"Stemming the sentence: \", \" \".join(stemmer.stem(token) for token in tokens))\n\n# Lemmatizer\nlemmatizer=nltk.stem.WordNetLemmatizer()\nprint(\"Lemmatizing the sentence: \", \" \".join(lemmatizer.lemmatize(token) for token in tokens))","f154e365":"def combine_text(list_of_text):\n    '''Takes a list of text and combines them into one large chunk of text.'''\n    combined_text = ' '.join(list_of_text)\n    return combined_text\n\ntrain['text'] = train['text'].apply(lambda x : combine_text(x))\ntest['text'] = test['text'].apply(lambda x : combine_text(x))\ntrain['text']\ntrain.head()","19c5031c":"def text_preprocessing(text):\n    \"\"\"\n    Cleaning and parsing the text.\n\n    \"\"\"\n    tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n    \n    nopunc = clean_text(text)\n    tokenized_text = tokenizer.tokenize(nopunc)\n    remove_stopwords = [w for w in tokenized_text if w not in stopwords.words('english')]\n    combined_text = ' '.join(remove_stopwords)\n    return combined_text","e0c0762b":"count_vectorizer = CountVectorizer()\ntrain_vectors = count_vectorizer.fit_transform(train['text'])\ntest_vectors = count_vectorizer.transform(test[\"text\"])\n\n## Keeping only non-zero elements to preserve space \nprint(train_vectors[0].todense())","da6c2582":"tfidf = TfidfVectorizer(min_df=2, max_df=0.5, ngram_range=(1, 2))\ntrain_tfidf = tfidf.fit_transform(train['text'])\ntest_tfidf = tfidf.transform(test[\"text\"])","6a20e973":"clf_NB_TFIDF = MultinomialNB()\nscores = model_selection.cross_val_score(clf_NB_TFIDF, train_tfidf, train[\"target\"], cv=5, scoring=\"f1\")\nscores","353300b6":"clf_NB_TFIDF.fit(train_tfidf, train[\"target\"])","23ab77ab":"def submission(submission_file_path,model,test_vectors):\n    sample_submission = pd.read_csv(submission_file_path)\n    sample_submission[\"target\"] = model.predict(test_vectors)\n    sample_submission.to_csv(\"submission.csv\", index=False)","327ce76c":"submission_file_path = os.path.join(root_path,\"sample_submission.csv\")\ntest_vectors=test_tfidf\nsubmission(submission_file_path,clf_NB_TFIDF,test_vectors)","7189df78":"# **Dataset**\nThe dataset are taken from kaggle competition","51c4a81f":"# Real Tweet or Not\n","3fe42a15":"# Removal of Punctuatuation","4bff411d":"# Tokenization","b9f6c837":"# Stemming and Lemmatization\nFor this process we download the wordnet data from nltk","7994cfec":"# Data Preprocessing\nBefore getting our hands in the model we need to clean the data\nSome of the preprocessing methods need to be followed in case of text are given below\n1. Removal of punctuation\n2. Tokenization\n3. Removal of Stopwors\n4. Stemming and Lemmatization\n","cd17fbf8":"# **Test Data**","38099fe7":"# Bag of Words - Countvectorizer Features","b64694c1":"# Removal of Stopwords\nFor that we download the stopword from nltk","ebe38776":"# Preparing Submission CSV","387cd4d5":"# **Train Data**","1b0e53bb":"# Data Visualization","b307c64a":"# Generalization\nThe above steps of data cleaning are combined and made as single function","d9f24e74":"# Simple Naive Bayes on TFIDF"}}