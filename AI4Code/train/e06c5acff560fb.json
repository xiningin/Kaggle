{"cell_type":{"4b4dd996":"code","a55aa84a":"code","cea17633":"code","d9254875":"code","ff1bc9d4":"code","2c367e6b":"code","653f51d9":"code","8ff25288":"code","f517188c":"code","0ebc8993":"code","afdf9f7d":"code","9fc3b364":"code","c6978e85":"code","a624b114":"code","798d5cb8":"code","36f96a08":"code","8f116519":"code","e3f32fab":"code","8b11cc60":"code","9eecd175":"code","1a31fa70":"code","71b930e0":"code","3688983b":"code","6f0d3103":"code","4167aa1e":"code","8e8a7eb3":"code","c46be0bf":"code","645d3e1d":"code","62f8b28a":"code","6d1c73e8":"code","ebc5db73":"code","ccf3f8c7":"code","ff7400ae":"code","300d9250":"code","ba849f61":"code","13ba8b65":"code","331296bf":"code","e5b9677f":"code","7d2d2ed8":"code","15fff734":"code","527df88d":"code","4947a915":"code","9c15e724":"code","a7e54572":"code","ef8160c2":"code","036a6844":"code","6382d0b2":"code","73aebe28":"code","3ced6f41":"code","5957f8ff":"code","47001df8":"code","f674931c":"code","4133a396":"code","227429d8":"code","3356e494":"code","bab3406d":"code","5c1d93ab":"markdown","604679be":"markdown","186cd95b":"markdown","d7e98749":"markdown","4dc676e7":"markdown","aa05fe87":"markdown","0aa6e0b2":"markdown","37208f78":"markdown","e83f207c":"markdown","c1522c75":"markdown","e05b0beb":"markdown","a7ade203":"markdown","1d647f09":"markdown","601a54d7":"markdown","f330d0fc":"markdown"},"source":{"4b4dd996":"import warnings\nwarnings.filterwarnings('ignore')","a55aa84a":"!pip install gensim==1.0.0","cea17633":"from gensim.summarization.summarizer import summarize\nfrom gensim.summarization import keywords\nfrom bs4 import BeautifulSoup\nfrom urllib.request import urlopen","d9254875":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ff1bc9d4":"def scrape(link):\n    website = urlopen(link)\n    s = BeautifulSoup(website)\n    # Capture the paragraph tag <p> in the data\n    # Convert it into text with the map function and join the list elements\n    text = ' '.join(map(lambda x: x.text, s.find_all('p')))\n    print(text)\n    return s.title.text, text","2c367e6b":"link = \"https:\/\/en.wikipedia.org\/wiki\/Natural_language_processing\"\nparagraphs = scrape(link)","653f51d9":"len(''.join(paragraphs))","8ff25288":"paragraphs[:100]","f517188c":"total = str(paragraphs)","0ebc8993":"total","afdf9f7d":"type(total)","9fc3b364":"# Summarizing the text with a ratio of 0.1 - 10% of total words\nsummarize(text, ratio=0.1)","c6978e85":"print(keywords(text, ratio=0.1))","a624b114":"!pip install sumy","798d5cb8":"from sumy.parsers.html import HtmlParser\nfrom sumy.parsers.plaintext import PlaintextParser\nfrom sumy.nlp.tokenizers import Tokenizer\nfrom sumy.summarizers.lsa import LsaSummarizer\nfrom sumy.nlp.stemmers import Stemmer\nfrom sumy.utils import get_stop_words\nfrom sumy.summarizers.luhn import LuhnSummarizer","36f96a08":"# The number of sentences we want our text to be summarized:\nsentences = 15\nlink = \"https:\/\/en.wikipedia.org\/wiki\/SQL_injection\"\nparser = HtmlParser.from_url(link, Tokenizer(\"english\"))","8f116519":"lsa = LsaSummarizer()\nlsa = LsaSummarizer(Stemmer(\"english\"))\nlsa.stop_words = get_stop_words(\"english\")\nfor sentence in lsa(parser.document, sentences):\n    print(sentence)","e3f32fab":"import nltk\nfrom nltk.stem.snowball import SnowballStemmer\nfrom bs4 import BeautifulSoup\nimport re\nimport os\nimport codecs\nfrom sklearn import feature_extraction\nimport mpld3\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport os\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nfrom sklearn.manifold import MDS\nfrom sklearn.cluster import KMeans\nfrom sklearn.feature_extraction.text import TfidfVectorizer","8b11cc60":"df = pd.read_csv('..\/input\/us-consumer-finance-complaints\/consumer_complaints.csv', encoding='latin-1')","9eecd175":"# Extracting the required column\ndf = df[['consumer_complaint_narrative']]\ndf = df[df['consumer_complaint_narrative'].notnull()]","1a31fa70":"df.rename(columns={'consumer_complaint_narrative':'description'}, inplace=True)","71b930e0":"df.head()","3688983b":"df.shape","6f0d3103":"sampling = df.sample(200)","4167aa1e":"# Remove X symbols\ndf['description'] = df['description'].str.replace('XXXX', '')\ndf['description']","8e8a7eb3":"df['description'] = df['description'].str.replace('XX', '')","c46be0bf":"# Conversion to list\ndata = sampling['description'].tolist()","645d3e1d":"ranks = [] # Will be used later\nfor i in range(1, len(data)+1):\n    ranks.append(i)","62f8b28a":"stopwords = nltk.corpus.stopwords.words('english')\nss = SnowballStemmer('english')","6d1c73e8":"# Function to clean up all our data\ndef cleaning(text):\n    tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n    filtered = []\n    for token in tokens:\n        if re.search('[a-zA-Z]', token):\n            filtered.append(token)\n    stem = [ss.stem(t) for t in filtered]\n    return stem","ebc5db73":"def tokenize_only(text):\n    tokens = [word.lower() for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n    filtered = []\n    for token in tokens:\n        if re.search('[a-zA-Z]', token):\n            filtered.append(token)\n    return filtered","ccf3f8c7":"sampling.shape","ff7400ae":"len(data)","300d9250":"tfidf = TfidfVectorizer(min_df=0.2, max_df=0.8,max_features=200000, stop_words='english', use_idf=True, tokenizer=cleaning, ngram_range=(1, 3))\nsampling_tf = tfidf.fit_transform(data)\nterms = tfidf.get_feature_names()","ba849f61":"print(sampling_tf.shape)","13ba8b65":"len(terms)","331296bf":"km = KMeans(n_clusters=6)\nkm.fit(sampling_tf)","e5b9677f":"clusters = km.labels_.tolist()","7d2d2ed8":"complaints_data = { 'rank': ranks, 'complaints': data,\n'cluster': clusters }\nframe = pd.DataFrame(complaints_data, index = [clusters] ,\ncolumns = ['rank', 'cluster'])","15fff734":"frame","527df88d":"frame['cluster'].value_counts().sort_values()","4947a915":"totalvocab_stemmed = []\ntotalvocab_tokenized = []\nfor i in data:\n    a = cleaning(i)\n    totalvocab_stemmed.extend(a)\n    b = tokenize_only(i)\n    totalvocab_tokenized.extend(b)","9c15e724":"vocab_frame = pd.DataFrame({'words':totalvocab_tokenized}, index=totalvocab_stemmed)","a7e54572":"vocab_frame.head()","ef8160c2":"ordering","036a6844":"# Sorting cluster centers by their proximity to entroid\nordering = km.cluster_centers_.argsort()[:, ::-1]\n# Iterating over each cluster\nfor i in range(6):\n    print(\"Cluster %d words\" % i, end=\"\")\n    # Extracting the index of the word\n    for ind in ordering[i, :6]:\n        print(\" At index\", ind, end=\" \")\n        # Using the index to extract the word from tfidf\n        # Using the tfidf word to search using loc in our vocab_frame (which has the index of the stem words)\n        print(' %s'  % vocab_frame.loc[terms[ind].split(' ')].values.tolist()[0][0].encode('utf-8', 'ignore'), end=', ')\n    print()","6382d0b2":"# We search for the term through the stemmed value in the tfidf feature names\nvocab_frame.loc[terms[ind].split(' ')].values.tolist()[0][0]","73aebe28":"# Similarity\nsim_d = 1 - cosine_similarity(sampling_tf)\n# Reducing the features to a 2D space\nmds = MDS(n_components = 2, dissimilarity=\"precomputed\", random_state=1)\npos = mds.fit_transform(sim_d) # shape is of n_components, n_samples)\nxs, ys = pos[: , 0], pos[:, 1]\n\n# Colors to use and cluster names:\ncluster_colors = {0: '#1b9e77', 1: '#d95f02', 2: '#7570b3', 3: '#e7298a', 4: '#66a61e', 5: '#D2691E'}\n#set up cluster names using a dict\ncluster_names = {0: 'property, based, assist',\n 1: 'business, card',\n 2: 'authorized, approved, believe',\n 3: 'agreement, application,business',\n 4: 'closed, applied, additional',\n 5: 'applied, card'}","3ced6f41":"sim_d.shape","5957f8ff":"xs.shape","47001df8":"ys.shape","f674931c":"df1 = pd.DataFrame(dict(x=xs, y=ys, label=clusters)) ","4133a396":"df1.head()","227429d8":"df1.groupby('label').count()","3356e494":"groups = df1.groupby('label')","bab3406d":"fig, ax = plt.subplots(figsize=(17, 9)) \nfor name, group in groups:\n    ax.plot(group.x, group.y, marker='o', linestyle=\"\", ms=20, label=cluster_names[name], color=cluster_colors[name], mec='none')\n    ax.set_aspect('auto')\n    ax.tick_params(axis='x', which='both', bottom='off', top='off', labelbottom='off')\n    ax.tick_params(axis='y', which='both', left='off', top='off', labelleft='off')\n\nax.legend(numpoints=1)\nplt.show()","5c1d93ab":"<h2>Document clustering<\/h2>\n\nThis is also known as text clustering. It is a clustering analysis on text documents. One of the main uses includes document management.\n\nThe process includes several similar steps to basic NLP tasks:\n\n1. Tokenization\n2. Stemming and lemmatization\n3. Removing stop words and punctuation \n4. Counting term frequences or TF-IDF\n5. Clustering through a K-means\/Hierarchitcal technique\n6. Evaluation and final visualizations","604679be":"<h3>TextRank<\/h3>\n\n**Not recommended approach due to lack of support for gensim summarization and its complete removal in the latest versions of gensim**. It is a graph rank algorithmthat uses the core concepts of NLP. It took its foundation from PageRank, used by the popular search engine Google, but designed specifically for text. It will extract the topics from the data, convert them into edge-points\/nodes and capture the relationship between them. Let us capture some data from Wikipedia.","186cd95b":"We will use the finance complaints dataset, which we used in our other notebook for classification purposes:","d7e98749":"Through this, we were able to summarize our documents, in a much more flexible manner. We can further incorporate deep learning techniques to improve our summarization quality.","4dc676e7":"<h2>Feature-based text summarization<\/h2>\n\nIt will extract features form the sentences, check its importance and then rank it. The features may include the length, position of the word, frequency, named entity etc. We can use Luhn's Algorithm for our task.","aa05fe87":"<h2>Clustering using K-means<\/h2>","0aa6e0b2":"<h1>Introduction<\/h1>\n\nThis notebook will focus on some advanced NLP techniques and their various implementations in Python. We will focus on three major tasks\n\n* Summarization of text - feature-based and Text Rank\n* Clustering of documents\n* NLP in search engines","37208f78":"<h3>Identifying cluster behaviour<\/h3>\nWe will find which are the top 5 words nearest to each of the cluster centroids:","e83f207c":"Preprocessing steps","c1522c75":"<h2>Text summarization<\/h2>\n\nThere are a multitude of books, blogs and text material available everyhwere. We may find some good resources to learn NLP, but it's too long to read through. We can summarize the data, to capture all the important information, while also saving time. This process can be implemented in a variety of ways:\n\n* Text Rank - Graph based ranking\n* Feature-based summarization of text\n* Topic linkage\n* Usage of sentence embeddings\n* Encoder-decoders - Deep learning\n\nWe'll be working with the first 2 models in this notebook","e05b0beb":"Let us work with 200 documents for now","a7ade203":"<h2>Plotting clusters","1d647f09":"<h2>NLP in search engines<\/h2>","601a54d7":"Through this, we were able to cluster about 200 different forms of complaints into 6 distinct gorups. We can also use different deep learning techniques such as word-embeddings, to achieve this better. ","f330d0fc":"The major processes in NLP include the following\n\n**Preprocessing**\n* Removal of noise and stop words\n* Tokenization\n* Stemming\n* Lemmatization\n\n**Entity Extraction model** - We can build customized models for this purpose or use libraries such as NLTK and Stanford NER. If we have an ecommerce website, our entity recognition model can work on the following:\n* Gender\n* Color\n* Brand\n* Product category\n* Price\n* Size \n\n\nWe can also build named entity disambiguation using RNNs and LSTMs. This helps in understanding the context and content in which the entities are used e.g. - bank can be a riverbank or financial institution. NERD can help us in this:\n* Data cleaning and preprocessing\n* Training NER model\n* Test and validate\n* Deploy\nThe training of the NERD model can be done through\n* Named entity recongition\/disambiguation\n* RNNs, LSTMs\n* Joint named entity recogntion\n\n**Query enhancnement and expansion** - It is important to understand possible different meanings of entites so that the search results do not miss out on relevance. We can use locally-trained word embeddings such as GloVe or Word2Vec to achieve this.\n\n**The search platform** - Some search platforms have full-text search hit highlighting, elastic stacks, real-time indexing, dynamic clustering etc. This is less on the grounds of NLP but more focused towards end-to-end application featurees.\n\n**Rankigns** - The search results are fetched from Solr or elastic search should be ranked based on user preference and other algorithms."}}