{"cell_type":{"70011078":"code","1008a80d":"code","cf5fa4c3":"code","df90ec22":"code","efca27c4":"code","77820db7":"code","6f5705fb":"code","ce3b48ec":"code","5462ffb7":"code","fdd57f0b":"code","e58ee3dd":"code","fb94a023":"code","ea08d026":"code","762d1111":"code","c4ebabe4":"code","effedfda":"code","1b5a2072":"code","8102cbe4":"code","983a4361":"code","08df7dc3":"code","2f30c75a":"code","c6d1efa9":"code","f391de0a":"code","9172f145":"code","64c1398f":"code","fd7f0290":"code","4b951e93":"code","f7935cad":"code","0a7888bc":"code","b9a189d9":"code","7df13fe0":"code","b601b702":"code","62ca2727":"code","7f7a2187":"code","66491413":"code","f83eced4":"code","9d36fd8a":"code","63bf3ee6":"code","4ae89f18":"code","0f613b02":"code","308ab2c5":"code","729ed1a2":"code","4b9ab5b1":"code","b2aa90f0":"code","2124254d":"code","d3d78f91":"code","7d2655a9":"code","fa97cfdb":"code","69b10864":"code","e67b8e6f":"code","41b668d8":"code","cb99f5ef":"code","79d4a212":"code","83d0872c":"code","fd6172b0":"code","ad17207b":"code","6fbbeb2b":"code","e069095e":"code","711f755a":"code","471f215e":"code","fc407bc5":"code","c75e4895":"code","37ff7781":"code","2b1650db":"code","a62ea8e3":"code","52780299":"code","040ff8ce":"code","31be5d71":"code","f41e03cc":"code","9c8f7be0":"code","013b4992":"code","05005f92":"code","1c813c36":"code","7124494f":"code","df51581d":"code","b63d6c7c":"code","d53f773e":"code","ecb187ae":"code","d38d3049":"code","80df89e0":"code","01a1cc05":"code","92476dd4":"code","984ea966":"code","75786132":"code","3b7b989a":"code","27ae4250":"code","bd05bbc1":"code","22e669d0":"code","565559ab":"code","552b4150":"code","34ee9e4d":"code","603fd826":"code","0ada5d3f":"code","05547d77":"code","0bb136e2":"code","29ae6be2":"code","885f7203":"code","059f91c3":"code","39071701":"code","ce3ef4cc":"code","ea3bcef7":"code","f45c343c":"code","cf6f4667":"code","148b02dc":"code","355df685":"code","04fe4790":"code","ec02e924":"code","ec78dbcd":"code","2e1ab131":"code","9f6b00b7":"code","e2218f6f":"code","e1d5675e":"code","7f4c50ca":"code","adcfdf80":"code","6e03e49b":"code","e45496ef":"code","faaf8b51":"markdown","f1dc07c6":"markdown","92d1382a":"markdown","98a1e2e6":"markdown","3c34144b":"markdown"},"source":{"70011078":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","1008a80d":"import numpy as np\nfrom numpy import array\nimport numpy.polynomial.polynomial as poly\nimport pandas as pd\nimport datetime as dt\nimport seaborn\nimport matplotlib.pylab as plt\nimport os\nimport random\nimport shutil\nimport glob\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Activation, Dense, Flatten, BatchNormalization, Conv2D, MaxPool2D, LSTM\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.metrics import categorical_crossentropy\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom sklearn.metrics import confusion_matrix\nimport itertools\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import r2_score","cf5fa4c3":"seaborn.set(rc={'figure.figsize':(16,8.27)})","df90ec22":"df = pd.read_csv('\/kaggle\/input\/stock-exchange-data\/indexData.csv')","efca27c4":"df.head()","77820db7":"df.info","6f5705fb":"df.describe()","ce3b48ec":"df.isnull().sum()","5462ffb7":"df[\"Date\"] = pd.to_datetime(df[\"Date\"])","fdd57f0b":"df_NYA = df[df[\"Index\"].isin(['NYA'])].sort_values('Date')","e58ee3dd":"df_NYA","fb94a023":"plt.plot(df_NYA[\"Date\"],df_NYA[\"Close\"],label='Row Data')\nplt.legend()","ea08d026":"df_NYA_data_close = df_NYA[[\"Date\",\"Close\"]]","762d1111":"df_NYA_data_close","c4ebabe4":"df_NYA_data_close.isnull().sum()","effedfda":"df_NYA_data_close.index","1b5a2072":"np.where(df_NYA_data_close['Close'].isnull())[0]","8102cbe4":"df_NYA_data_close.iloc[[289]]","983a4361":"df_NYA_data_close_withoutnan = df_NYA_data_close.dropna()","08df7dc3":"df_NYA_data_close_withoutnan.isnull().sum()","2f30c75a":"np.where(df_NYA_data_close_withoutnan['Close'].isnull())[0]","c6d1efa9":"data_train = df_NYA_data_close_withoutnan[df_NYA_data_close_withoutnan.Date < \"2010-01-01\"]\ndata_test = df_NYA_data_close_withoutnan[df_NYA_data_close_withoutnan.Date >= \"2010-01-01\"]","f391de0a":"data_train","9172f145":"data_train.Date = data_train.Date.map(dt.datetime.toordinal)","64c1398f":"X_train = data_train.Date[:, np.newaxis]","fd7f0290":"X_train","4b951e93":"y_train = data_train.Close","f7935cad":"y_train.isnull().sum()","0a7888bc":"tree = DecisionTreeRegressor(criterion='mse',max_depth=50).fit(X_train, y_train)","b9a189d9":"linear_reg = LinearRegression().fit(X_train, y_train)","7df13fe0":"model_rf = RandomForestRegressor(n_estimators=100).fit(X_train, y_train)","b601b702":"X_all = df_NYA_data_close_withoutnan.Date.map(dt.datetime.toordinal)[:, np.newaxis]","62ca2727":"X_all","7f7a2187":"np.where(df_NYA_data_close_withoutnan['Date'] == '1966-01-05')[0]","66491413":"X_all[3]","f83eced4":"pred_tree = tree.predict(X_all)\npred_model_rf = model_rf.predict(X_all)\npred_lr = linear_reg.predict(X_all)\nX = np.array(data_train.Date)\ny = np.array(data_train.Close)\nmodel_coef_polyfit = poly.polyfit(X, y, 3)\nX_new = np.array(df_NYA_data_close_withoutnan.Date.map(dt.datetime.toordinal))\nffit = poly.polyval(X_new, model_coef_polyfit)\nX_new = pd.DataFrame(X_new,columns=['Date'])\nX_new = X_new.Date.map(dt.datetime.fromordinal)","9d36fd8a":"plt.plot(df_NYA_data_close_withoutnan.Date,df_NYA_data_close_withoutnan.Close,label='Row Data')\nplt.plot(df_NYA_data_close_withoutnan.Date,pred_tree,label='Decision Tree')\nplt.plot(df_NYA_data_close_withoutnan.Date,pred_lr,label='Linear Regression')\nplt.plot(df_NYA_data_close_withoutnan.Date,pred_model_rf,label='Random Forest')\nplt.plot(X_new,ffit,label='Fit using polyfit')\nplt.legend()","63bf3ee6":"tree.score(X_all,df_NYA_data_close_withoutnan.Close)","4ae89f18":"linear_reg.score(X_all,df_NYA_data_close_withoutnan.Close)","0f613b02":"model_rf.score(X_all,df_NYA_data_close_withoutnan.Close)","308ab2c5":"df_N100 = df[df[\"Index\"].isin(['N100'])].sort_values('Date')","729ed1a2":"new_index = np.arange(0,len(df_N100.Close),1)","4b9ab5b1":"len(df_N100.Close)","b2aa90f0":"df_N100_data_close = df_N100[[\"Date\",\"Close\"]]\ndf_N100_data_close","2124254d":"df_N100_data_close.index","d3d78f91":"df_N100_data_close.isnull().sum()","7d2655a9":"df_N100_data_close_withoutnan = df_N100_data_close.dropna()","fa97cfdb":"len(df_N100_data_close_withoutnan.Close)","69b10864":"df_N100_data_close_withoutnan.isnull().sum()","e67b8e6f":"data_train_N100 = df_N100_data_close_withoutnan[df_N100_data_close_withoutnan.Date < \"2015-01-01\"]\ndata_test_N100 = df_N100_data_close_withoutnan[df_N100_data_close_withoutnan.Date >= \"2015-01-01\"]","41b668d8":"df_N100_data_close_withoutnan.index","cb99f5ef":"data_train_N100.Date = data_train_N100.Date.map(dt.datetime.toordinal)","79d4a212":"data_train_N100.Close","83d0872c":"X_train_N100 = data_train_N100.Date[:, np.newaxis]\ny_train_N100 = data_train_N100.Close","fd6172b0":"tree_N100 = DecisionTreeRegressor(criterion='mse',max_depth=50).fit(X_train_N100, y_train_N100)\nlinear_reg_N100 = LinearRegression().fit(X_train_N100, y_train_N100)\nmodel_rf_N100 = RandomForestRegressor(n_estimators=10).fit(X_train_N100, y_train_N100)","ad17207b":"X_all_N100 = df_N100_data_close_withoutnan.Date.map(dt.datetime.toordinal)[:, np.newaxis]","6fbbeb2b":"pred_tree_N100 = tree_N100.predict(X_all_N100)\npred_lr_N100 = linear_reg_N100.predict(X_all_N100)\npred_model_rf_N100 = model_rf_N100.predict(X_all_N100)","e069095e":"X_N100 = np.array(data_train_N100.Date)\ny_N100 = np.array(data_train_N100.Close)\nmodel_coef_polyfit = poly.polyfit(X_N100, y_N100, 2)\nX_new_N100 = np.array(df_N100_data_close_withoutnan.Date.map(dt.datetime.toordinal))\nffit_N100 = poly.polyval(X_new_N100, model_coef_polyfit)\nX_new_N100 = pd.DataFrame(X_new_N100,columns=['Date'])\nX_new_N100 = X_new_N100.Date.map(dt.datetime.fromordinal)","711f755a":"plt.plot(df_N100_data_close_withoutnan.Date,df_N100_data_close_withoutnan.Close,label='Row Data')\nplt.plot(df_N100_data_close_withoutnan.Date,pred_tree_N100,label='Decision Tree')\nplt.plot(df_N100_data_close_withoutnan.Date,pred_lr_N100,label='Linear Regression')\nplt.plot(df_N100_data_close_withoutnan.Date,pred_model_rf_N100,label='Random Forest')\nplt.plot(X_new_N100,ffit_N100,label='Fit using Polyfit')\nplt.legend()","471f215e":"# split data into samples\ndef split_sequence(sequence, n_steps_in, n_steps_out):\n    X, y = list(), list()\n    for i in range(len(sequence)):\n        # find the end of this pattern\n        end_ix = i + n_steps_in\n        out_end_ix = end_ix + n_steps_out\n        # check if we are beyond the sequence\n        if out_end_ix > len(sequence):\n            break\n        # gather input and output parts of the pattern\n        seq_x, seq_y = sequence[i:end_ix], sequence[end_ix:out_end_ix]\n        X.append(seq_x)\n        y.append(seq_y)\n    return array(X), array(y)","fc407bc5":"data_train = df_NYA_data_close_withoutnan[df_NYA_data_close_withoutnan.Date < \"2010-01-01\"]\ndata_test = df_NYA_data_close_withoutnan[df_NYA_data_close_withoutnan.Date >= \"2010-01-01\"]\ndata_train","c75e4895":"# choose a number of time steps\nn_steps_in, n_steps_out = 10, 1\n# split data into samples\nX, y = split_sequence(data_train.Close, n_steps_in, n_steps_out)\n# reshape from [samples, timesteps] into [samples, timesteps, features]\nn_features = 1\nX = X.reshape((X.shape[0], X.shape[1], n_features))\nprint(X.shape)","37ff7781":"# model\nmodel = Sequential()\nmodel.add(LSTM(200, activation='relu', return_sequences=True, input_shape=(n_steps_in, n_features)))\nmodel.add(LSTM(200, activation='relu', return_sequences=True))\nmodel.add(LSTM(200, activation='relu'))\nmodel.add(Dense(n_steps_out))\nmodel.compile(optimizer=Adam(learning_rate=0.0001), loss='mse', metrics=['accuracy'])\nmodel.summary()","2b1650db":"# fit model\nhistory = model.fit(X, y, epochs=10, verbose=2, validation_split=0.33)","a62ea8e3":"plt.plot(history.history['loss'],label='Train')\nplt.plot(history.history['val_loss'],label='Validation')\nplt.legend()","52780299":"# demonstrate prediction\n# choose a number of time steps\nn_steps_in, n_steps_out = 10, 1\n# split data into samples\nX, y = split_sequence(data_test.Close, n_steps_in, n_steps_out)\nprint(X.shape)\nx_input = X.reshape((X.shape[0], X.shape[1], n_features))\nyhat_1 = model.predict(x_input, verbose=0)\nprint(yhat_1)","040ff8ce":"test_mean_1 = np.mean(yhat_1,axis=1)\nprint(test_mean_1)","31be5d71":"plt.plot(df_NYA_data_close_withoutnan.Date,df_NYA_data_close_withoutnan.Close,'k',label='Row Data')\nplt.plot(data_test.Date[10:],test_mean_1,label='RNN using LSTM')\nplt.legend()","f41e03cc":"plt.plot(data_test.Date[10:],data_test.Close[10:],'-b',label='Test Data')\nplt.plot(data_test.Date[10:],test_mean_1,'r',label='RNN using LSTM')\nplt.legend()","9c8f7be0":"r2_score(data_test.Close[10:], test_mean_1)","013b4992":"mean_squared_error(data_test.Close[10:],test_mean_1)","05005f92":"((test_mean_1-data_test.Close[10:])**2).sum()\/len(test_mean_1)","1c813c36":"((test_mean_1-data_test.Close[10:])**2).mean()","7124494f":"# choose a number of time steps\nn_steps_in, n_steps_out = 10, 9\n# split into samples\nX, y = split_sequence(data_train.Close, n_steps_in, n_steps_out)\n# reshape from [samples, timesteps] into [samples, timesteps, features]\nn_features = 1\nX = X.reshape((X.shape[0], X.shape[1], n_features))\nprint(X.shape)","df51581d":"# define model\nmodel = Sequential()\nmodel.add(LSTM(200, activation='relu', return_sequences=True, input_shape=(n_steps_in, n_features)))\nmodel.add(LSTM(200, activation='relu', return_sequences=True))\nmodel.add(LSTM(200, activation='relu'))\nmodel.add(Dense(n_steps_out))\nmodel.compile(optimizer=Adam(learning_rate=0.0001), loss='mse', metrics=['accuracy'])\nmodel.summary()","b63d6c7c":"# fit model\nhistory = model.fit(X, y, epochs=10, verbose=2, validation_split=0.33)","d53f773e":"plt.plot(history.history['loss'],label='Train')\nplt.plot(history.history['val_loss'],label='Validation')\nplt.legend()","ecb187ae":"plt.plot(history.history['accuracy'],label='Train')\nplt.plot(history.history['val_accuracy'],label='Validation')\nplt.legend()","d38d3049":"# demonstrate prediction\n# choose a number of time steps\nn_steps_in, n_steps_out = 10, 9\n# split data into samples\nX, y = split_sequence(data_test.Close, n_steps_in, n_steps_out)\nprint(X.shape)\nx_input = X.reshape((X.shape[0], X.shape[1], n_features))\nyhat_2 = model.predict(x_input, verbose=0)\nprint(yhat_2)","80df89e0":"test_mean_2 = np.mean(yhat_2,axis=1)","01a1cc05":"plt.plot(data_test.Date[18:],data_test.Close[18:],'-b',label='Test Data')\nplt.plot(data_test.Date[18:],test_mean_2,'r',label='RNN using LSTM')\nplt.legend()","92476dd4":"r2_score(data_test.Close[18:], test_mean_2)","984ea966":"mean_squared_error(data_test.Close[18:],test_mean_2)","75786132":"# choose a number of time steps\nn_steps_in, n_steps_out = 3, 2\n# split data into samples\nX, y = split_sequence(data_train.Close, n_steps_in, n_steps_out)\n# reshape from [samples, timesteps] into [samples, timesteps, features]\nn_features = 1\nX = X.reshape((X.shape[0], X.shape[1], n_features))\nprint(X.shape)","3b7b989a":"# model\nmodel = Sequential()\nmodel.add(LSTM(200, activation='relu', return_sequences=True, input_shape=(n_steps_in, n_features)))\nmodel.add(LSTM(200, activation='relu', return_sequences=True))\nmodel.add(LSTM(200, activation='relu'))\nmodel.add(Dense(n_steps_out))\nmodel.compile(optimizer=Adam(learning_rate=0.0001), loss='mse', metrics=['accuracy'])\nmodel.summary()","27ae4250":"# fit model\nhistory = model.fit(X, y, epochs=10, verbose=2, validation_split=0.33)","bd05bbc1":"plt.plot(history.history['loss'],label='Train')\nplt.plot(history.history['val_loss'],label='Validation')\nplt.legend()\n#plt.ylim(0,10000)","22e669d0":"plt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])","565559ab":"# demonstrate prediction\n# choose a number of time steps\nn_steps_in, n_steps_out = 3, 2\n# split data into samples\nX, y = split_sequence(data_test.Close, n_steps_in, n_steps_out)\nprint(X.shape)\nx_input = X.reshape((X.shape[0], X.shape[1], n_features))\nyhat_3 = model.predict(x_input, verbose=0)\nprint(yhat_3)","552b4150":"test_mean_3 = np.mean(yhat_3,axis=1)","34ee9e4d":"plt.plot(df_NYA_data_close_withoutnan.Date,df_NYA_data_close_withoutnan.Close,'k',label='Row Data')\nplt.plot(data_test.Date[4:],test_mean_3,label='RNN using LSTM')\nplt.legend()","603fd826":"plt.plot(data_test.Date[4:],data_test.Close[4:],'-b',label='Test Data')\nplt.plot(data_test.Date[4:],test_mean_3,'r',label='RNN using LSTM')\nplt.legend()","0ada5d3f":"r2_score(data_test.Close[4:], test_mean_3)","05547d77":"mean_squared_error(data_test.Close[4:],test_mean_3)","0bb136e2":"data_train_N100 = df_N100_data_close_withoutnan[df_N100_data_close_withoutnan.Date < \"2015-01-01\"]\ndata_test_N100 = df_N100_data_close_withoutnan[df_N100_data_close_withoutnan.Date >= \"2015-01-01\"]","29ae6be2":"# choose a number of time steps\nn_steps_in, n_steps_out = 3, 2\n# split into samples\nX, y = split_sequence(data_train_N100.Close, n_steps_in, n_steps_out)\n# reshape from [samples, timesteps] into [samples, timesteps, features]\nn_features = 1\nX = X.reshape((X.shape[0], X.shape[1], n_features))\nprint(X.shape)","885f7203":"# define model\nmodel = Sequential()\nmodel.add(LSTM(200, activation='relu', return_sequences=True, input_shape=(n_steps_in, n_features)))\nmodel.add(LSTM(200, activation='relu', return_sequences=True))\nmodel.add(LSTM(200, activation='relu'))\nmodel.add(Dense(n_steps_out))\nmodel.compile(optimizer=Adam(learning_rate=0.0001), loss='mse', metrics=['accuracy'])\nmodel.summary()","059f91c3":"# fit model\nhistory = model.fit(X, y, epochs=10, verbose=2, validation_split=0.33)","39071701":"plt.plot(history.history['loss'],label='Train')\nplt.plot(history.history['val_loss'],label='Validation')\nplt.legend()\nplt.ylim(0,1000)","ce3ef4cc":"plt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])","ea3bcef7":"# demonstrate prediction\n# choose a number of time steps\nn_steps_in, n_steps_out = 3, 2\n# split into samples\nX, y = split_sequence(data_test_N100.Close, n_steps_in, n_steps_out)\nprint(X.shape)\nx_input = X.reshape((X.shape[0], X.shape[1], n_features))\nyhat_N100 = model.predict(x_input, verbose=0)\nprint(yhat_N100)","f45c343c":"test_mean_N100 = np.mean(yhat_N100,axis=1)\ntest_mean_N100","cf6f4667":"plt.plot(df_N100_data_close_withoutnan.Date,df_N100_data_close_withoutnan.Close,'k',label='Row Data')\nplt.plot(data_test_N100.Date[4:],test_mean_N100,label='RNN using LSTM')\nplt.legend()","148b02dc":"plt.plot(data_test_N100.Date[4:],data_test_N100.Close[4:],'-b',label='Test Data')\nplt.plot(data_test_N100.Date[4:],test_mean_N100,'r',label='RNN using LSTM')\nplt.legend()","355df685":"r2_score(data_test_N100.Close[4:], test_mean_N100)","04fe4790":"mean_squared_error(data_test_N100.Close[4:],test_mean_N100)","ec02e924":"# choose a number of time steps\nn_steps_in, n_steps_out = 10, 1\n# split into samples\nX, y = split_sequence(data_train_N100.Close, n_steps_in, n_steps_out)\n# reshape from [samples, timesteps] into [samples, timesteps, features]\nn_features = 1\nX = X.reshape((X.shape[0], X.shape[1], n_features))\nprint(X.shape)","ec78dbcd":"# define model\nmodel = Sequential()\nmodel.add(LSTM(200, activation='relu', return_sequences=True, input_shape=(n_steps_in, n_features)))\nmodel.add(LSTM(200, activation='relu', return_sequences=True))\nmodel.add(LSTM(200, activation='relu'))\nmodel.add(Dense(n_steps_out))\nmodel.compile(optimizer=Adam(learning_rate=0.0001), loss='mse', metrics=['accuracy'])\nmodel.summary()","2e1ab131":"# fit model\nhistory = model.fit(X, y, epochs=10, verbose=2, validation_split=0.33)","9f6b00b7":"plt.plot(history.history['loss'],label='Train')\nplt.plot(history.history['val_loss'],label='Validation')\nplt.legend()\nplt.ylim(0,1000)","e2218f6f":"# demonstrate prediction\n# choose a number of time steps\nn_steps_in, n_steps_out = 10, 1\n# split into samples\nX, y = split_sequence(data_test_N100.Close, n_steps_in, n_steps_out)\nprint(X.shape)\nx_input = X.reshape((X.shape[0], X.shape[1], n_features))\nyhat_N100_3 = model.predict(x_input, verbose=0)\nprint(yhat_N100_3)","e1d5675e":"test_mean_N100_3 = np.mean(yhat_N100_3,axis=1)\ntest_mean_N100_3","7f4c50ca":"plt.plot(df_N100_data_close_withoutnan.Date,df_N100_data_close_withoutnan.Close,'k',label='Row Data')\nplt.plot(data_test_N100.Date[10:],test_mean_N100_3,label='RNN using LSTM')\nplt.legend()","adcfdf80":"plt.plot(data_test_N100.Date[10:],data_test_N100.Close[10:],'-b',label='Test Data')\nplt.plot(data_test_N100.Date[10:],test_mean_N100_3,'r',label='RNN using LSTM')\nplt.legend()","6e03e49b":"r2_score(data_test_N100.Close[10:], test_mean_N100_3)","e45496ef":"mean_squared_error(data_test_N100.Close[10:],test_mean_N100_3)","faaf8b51":"Redo it for another index ","f1dc07c6":"In the following section implement and use a Recurrent Neural Network (RNN) to make the predictions. <br>\nI am using a multi-step times forecasting, meaning that I am making a prediction of multiple time steps. The time series needs to be split into samples to train (input) and forecast (output). This is given by the function split_sequence (https:\/\/machinelearningmastery.com\/how-to-develop-lstm-models-for-time-series-forecasting\/). <br>\nIn the following, I will prepare the time series, using input\/output: {10\/1,10\/9,3\/2}, for two indices NYA and N100. Moreover, I will build a RNN using LSTM. <br>","92d1382a":"Prediction of Stock Price using Machine\/Deep Learning <br>\n<br>\nAuthor: jvachier <br>\nCreation date: September 2021 <br>\nPublication date: October 2021 <br>\n<br>\nMy goal is to use Machine Learning to forecast Stock Price. In that purpose, I am comparing different methods, such as Linear Regression, Decision Tree, Random Forest, Polyfit and Deep Learning using a Recurrent Neural Network (RNN) using a Long Short Term Memory (LSTM) architecture.\nThe data set used here can be found on Kaggle (https:\/\/www.kaggle.com\/mattiuzc\/stock-exchange-data). \nThe following analysis is organized as follow. Firstly, I import and pre-process the data. Secondly, I am choosing two indexes NYA and N100 to perform the analysis. Thirdly, I compare different methods in Machine Learning. \nFinally, I build a RNN using a LSTM architecture to forecast the Stock prize for different parameters and visualize the results by plotting the time series.<br>\nReferences:<br>\nhttps:\/\/www.tensorflow.org\/tutorials\/structured_data\/time_series <br>\nhttps:\/\/towardsdatascience.com\/time-series-forecasting-with-deep-learning-and-attention-mechanism-2d001fc871fc <br>\nhttps:\/\/machinelearningmastery.com\/how-to-develop-lstm-models-for-time-series-forecasting\/","98a1e2e6":"Prepare the time series <br>","3c34144b":"Redo the Analysis for the Index N100"}}