{"cell_type":{"3ce797ab":"code","54ec85d7":"code","61c4b3b9":"code","be32d4de":"markdown"},"source":{"3ce797ab":"import pandas as pd\nimport numpy as np\nimport os\nfrom scipy.stats import pearsonr\nimport tensorflow as tf\nimport tensorflow_addons as tfa\nimport lightgbm as lgb\nfrom tensorflow.keras import backend as K\nimport joblib\nimport random\nimport warnings\nwarnings.filterwarnings('ignore')\npd.set_option('display.max_columns', 500)\npd.set_option('display.max_rows', 100)\nimport ubiquant","54ec85d7":"# Calculate pearson correlation coefficient\ndef pearson_coef(data):\n    return data.corr()['target']['prediction']\n\n# Calculate mean pearson correlation coefficient\ndef comp_metric(valid_df):\n    return np.mean(valid_df.groupby(['time_id']).apply(pearson_coef))\n\n# Calculate out of folds score blend\nfc_oof = pd.read_csv('..\/input\/ubiquant-external-models\/ed_mlp.csv')\nlgb_oof = pd.read_csv('..\/input\/ubiquant-lgbm-training-baseline\/simple_lgbm.csv')\nscore_fc = comp_metric(fc_oof)\nscore_lgb = comp_metric(lgb_oof)\nfc_oof['prediction'] = fc_oof['prediction'] * 0.7 + lgb_oof['prediction'] * 0.3\nscore_blend = comp_metric(fc_oof)\nprint(f'Fully connected model score {score_fc}')\nprint(f'Light gradient boosting score {score_lgb}')\nprint(f'Blend score {score_blend}')","61c4b3b9":"# Function to build our model\ndef build_model(shape):\n    def fc_block(x, units, dropout):\n        x = tf.keras.layers.Dropout(dropout)(x)\n        x = tf.keras.layers.Dense(units, activation = 'swish')(x)\n        return x\n    # Input layer\n    inp = tf.keras.layers.Input(shape = (shape))\n    # Encoder block\n    encoder = tf.keras.layers.GaussianNoise(0.015)(inp)\n    encoder = tf.keras.layers.Dense(192)(encoder)\n    encoder = tf.keras.layers.Activation('swish')(encoder)\n    # Decoder block to predict the input to generate more features\n    decoder = tf.keras.layers.Dropout(0.05)(encoder)\n    decoder = tf.keras.layers.Dense(shape, activation = 'linear', name = 'decoder')(decoder)\n    # Autoencoder\n    autoencoder = tf.keras.layers.Dense(256)(decoder)\n    autoencoder = tf.keras.layers.Activation('swish')(autoencoder)\n    autoencoder = tf.keras.layers.Dropout(0.40)(autoencoder)\n    out_autoencoder = tf.keras.layers.Dense(1, activation = 'linear', name = 'autoencoder')(autoencoder)\n    # Concatenate input and encoder output for extra features\n    x = tf.keras.layers.Concatenate()([inp, encoder])\n    x = fc_block(x, units = 1024, dropout = 0.4)\n    x = fc_block(x, units = 512, dropout = 0.4)\n    x = fc_block(x, units = 256, dropout = 0.4)\n    output = tf.keras.layers.Dense(1, activation = 'linear', name = 'mlp')(x)\n    model = tf.keras.models.Model(inputs = [inp], outputs = [decoder, out_autoencoder, output])\n    return model\n\n# Get our features list\ndnn_features = list(np.load('..\/input\/ubiquant-external-models\/ed_mlp_features.npy'))\nlgb_features = list(np.load('..\/input\/ubiquant-lgbm-training-baseline\/features.npy'))\ncorr_features = list(np.load('..\/input\/ubiquant-external-models\/ed_mlp_best_corr.npy'))\n# Build 5 models and load 5 fold weights (tensorflow)\nmodel1 = build_model(len(dnn_features))\nmodel2 = build_model(len(dnn_features))\nmodel3 = build_model(len(dnn_features))\nmodel4 = build_model(len(dnn_features))\nmodel5 = build_model(len(dnn_features))\nmodel1.load_weights('..\/input\/ubiquant-external-models\/ed_mlp_1.h5')\nmodel2.load_weights('..\/input\/ubiquant-external-models\/ed_mlp_2.h5')\nmodel3.load_weights('..\/input\/ubiquant-external-models\/ed_mlp_3.h5')\nmodel4.load_weights('..\/input\/ubiquant-external-models\/ed_mlp_4.h5')\nmodel5.load_weights('..\/input\/ubiquant-external-models\/ed_mlp_5.h5')\n# Load 5 light gradient boosting models\nlgb1 = joblib.load('..\/input\/ubiquant-lgbm-training-baseline\/lgbm_1.pkl')\nlgb2 = joblib.load('..\/input\/ubiquant-lgbm-training-baseline\/lgbm_2.pkl')\nlgb3 = joblib.load('..\/input\/ubiquant-lgbm-training-baseline\/lgbm_3.pkl')\nlgb4 = joblib.load('..\/input\/ubiquant-lgbm-training-baseline\/lgbm_4.pkl')\nlgb5 = joblib.load('..\/input\/ubiquant-lgbm-training-baseline\/lgbm_5.pkl')\nfc_models = [model1, model2, model3, model4, model5]\nlgb_models = [lgb1, lgb2, lgb3, lgb4, lgb5]\n# Predict\nenv = ubiquant.make_env()\niter_test = env.iter_test() \nfor (test_df, sample_prediction_df) in iter_test:\n    for col in corr_features:\n        test_df['time_id'] = test_df['row_id'].str[0:4].astype(np.int64)\n        mapper = test_df.groupby(['time_id'])[col].mean().to_dict()\n        test_df[f'time_id_{col}'] = test_df['time_id'].map(mapper)\n    fc_predictions = []\n    lgb_predictions = []\n    for model in fc_models:\n        fc_predictions.append(model.predict(test_df[dnn_features])[2].reshape(-1))\n    for model in lgb_models:\n        lgb_predictions.append(model.predict(test_df[lgb_features]))\n    # Blend 60% fc, 40% light gradient boosting\n    predictions = np.average(fc_predictions, axis = 0) * 0.7 + np.average(lgb_predictions, axis = 0) * 0.3\n    sample_prediction_df['target'] = predictions\n    env.predict(sample_prediction_df)","be32d4de":"# Quick Notes\n\nThis is a starter package for Ubiquant competition. This is an ensemble of 2 models, one multi layer perceptron and a light gradient boosting model\n\nHere is the link for the training script for the multi layer perceptron (with encoder decoder block for extra features):\nhttps:\/\/www.kaggle.com\/ragnar123\/ubiquant-tf-training-baseline-with-gpu\n\nThis model achives a CV score of 0.1490 and a LB of 0.146. I did some experiments with PCC loss and CV is better, it gain a big boost and achive a CV score of 0.1515 but LB is worst. My best guess is it because using the average to blend folds using PCC loss could not be the correct way.\n\nOn the other hand here is the link for the LGBM model:\nhttps:\/\/www.kaggle.com\/ragnar123\/ubiquant-lgbm-training-baseline\n\nThis model achives a CV score of 0.1395, much worst than DNN. Both models where trained with the same KFold strategy and the same folds so blending the models is leak free. Nevetheless if we blend both models the CV score boost to 0.1556 and a LB to 0.148.\n\n# KFold Strategy\n\nA lof of folks are talking about CV strategy. Let's say we have the following data points which are aligned with time\n\nTrain:\n1, 2, 3, 4, 5, 6, 7, 8, 9, 10\n\nTest:\n11, 12, 13\n\nWe could say that validating with 8, 9, 10 is good because we are using future data, we should not expect any leak. The downside of this validation is that we are asumming that 8, 9, 10 will be similar to 11, 12, 13 and that could not be the case and early sopping will not be perfect. \n\nIn my notebooks im validating all the periods of the train set using GroupKFold. The downside of this validation is that you are using future data and validate on past data, like this.\n\nTrain with 1, 2, 3, 4, 8, 9, 10\nValidate with 5, 6, 7\n\nWe are using big windows to avoid leakage, nevertheless we can't be certain that leak will not occur because we don't know how the features were build.\n\nThe conclusion would be that a correct validation strategy will required more E.D.A, nevertheless im my experiments I have seen a nice correlation between CV and the LB. This does not mean that we are good to go and we have a perfect validation strategy but it is a reasonable start point.\n\n# Update\n\nI improve the encoder decoder multi layer perceptron adding more features and CV got to 0.1512, this boost ensemble CV to 0.1582 and reach LB 0.149. I used a private dataset because I don't have more GPU hours but is almost the same model with a few changes. Obviously their is still much room for improvement."}}