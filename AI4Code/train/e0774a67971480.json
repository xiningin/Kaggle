{"cell_type":{"d720a64e":"code","fbc0717b":"code","d3d43a60":"code","ca35e7b1":"code","d0d9ac76":"code","ed0f2505":"code","ebfa7beb":"code","4910cba7":"code","58e1278a":"code","306715b7":"code","38fa87f0":"code","bdcfc8d1":"code","7939723c":"code","b5080324":"markdown","795b6684":"markdown","e7b9cc0c":"markdown","0a864aa1":"markdown","5013ec46":"markdown","0f272072":"markdown","8d7ae21a":"markdown","13a7cccb":"markdown","06a5f015":"markdown","91f413ae":"markdown"},"source":{"d720a64e":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\npd.set_option('display.max_columns', 500)\n\n# Visualizations\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nplt.rcParams.update({'font.size': 14})\n\n# Encoders\nimport category_encoders as ce\n\n# Model Training\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import GridSearchCV, StratifiedKFold\nfrom sklearn.metrics import log_loss, make_scorer\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\ntrain_df = pd.read_csv(\"..\/input\/tabular-playground-series-may-2021\/train.csv\")\ntest_df = pd.read_csv(\"..\/input\/tabular-playground-series-may-2021\/test.csv\")\n\n# Drop id column\n#train_df.drop('id', axis=1, inplace=True)\n#test_df.drop('id', axis=1, inplace=True)\n\n# Only for visualization purposes\ntemp = train_df.copy()\ntemp.columns = list(['id']) + list([f\"f{c}\" for c in range(50)]) + list(['target'])\nfeature_cols = temp.columns[temp.columns.str.startswith('f')]\n\ndisplay(temp[feature_cols].head(10).style.background_gradient(cmap='Greens_r', vmin=-1, vmax=1))\ndel(temp)","fbc0717b":"# One Hot Encode target variable\nohe = ce.OneHotEncoder(handle_unknown='value', use_cat_names=True)\nOH_target = pd.DataFrame(ohe.fit_transform(train_df['target'].astype(str)))\n\ndisplay(OH_target.head())\n\n# Merge new OH encoded target to train_df \ntrain_df = pd.concat([train_df, OH_target], axis=1)","d3d43a60":"from sklearn.manifold import TSNE\nfeature_cols = train_df.columns[train_df.columns.str.startswith('feature_') | train_df.columns.str.startswith('target_')]\n\nX_embedded = TSNE(n_components=2, init='pca', perplexity = 5, random_state=42).fit_transform(train_df[feature_cols].T.values)\n\nplt.figure(figsize=(10, 10))\nx = X_embedded[:,0]\ny = X_embedded[:,1]\nsns.scatterplot(x[:-4], y[:-4], color='green')\nsns.scatterplot(x[-4:], y[-4:], color='red')\n\nplt.title('Clusters of similar features', fontsize=14)\nplt.grid(True)\n    \nfor i, word in enumerate(feature_cols):\n    if word.startswith('feature'):\n        plt.annotate(f\"f{word.split('_')[1]}\", xy=(x[i], y[i]) , size=10,  alpha=0.8, xytext=(5, 2), \n                 textcoords='offset points', ha='left', va='bottom')\n    else: \n        plt.annotate(f\"c{word.split('_')[2]}\", xy=(x[i], y[i]) , size=13,  alpha=0.8, xytext=(5, 2), \n                 textcoords='offset points', ha='left', va='bottom')\n         \nplt.show()","ca35e7b1":"# Experiment\n\n#drop_cols = ['feature_19', 'feature_38', 'feature_42']\n#train_df.drop(drop_cols, axis=1, inplace=True)\n#test_df.drop(drop_cols, axis=1, inplace=True)","d0d9ac76":"feature_cols = train_df.columns[train_df.columns.str.startswith('feature_')]\n\ntrain_df['feature_number_of_words'] = train_df[feature_cols].sum(axis=1)\ntest_df['feature_number_of_words'] = test_df[feature_cols].sum(axis=1)","ed0f2505":"#train_df['feature_number_of_words'] = np.log(train_df.feature_number_of_words)\nsns.kdeplot(train_df[train_df.target == 'Class_1']['feature_number_of_words'], label='Class_1')\nsns.kdeplot(train_df[train_df.target == 'Class_2']['feature_number_of_words'], label='Class_2')\nsns.kdeplot(train_df[train_df.target == 'Class_3']['feature_number_of_words'], label='Class_3')\nsns.kdeplot(train_df[train_df.target == 'Class_4']['feature_number_of_words'], label='Class_4')\nplt.legend()\nplt.show()","ebfa7beb":"display(train_df[train_df.feature_number_of_words == 0])\ntrain_df = train_df[train_df.feature_number_of_words != 0].reset_index(drop=True)","4910cba7":"idf = np.log(len(train_df) \/ (train_df != 0).sum(axis=0))\nidf.head(5)","58e1278a":"for f in feature_cols:\n    train_df[f\"{f}_tfidf\"] = train_df[f] \/ train_df['feature_number_of_words'] * idf[f]\n    test_df[f\"{f}_tfidf\"] = test_df[f] \/ test_df['feature_number_of_words'] * idf[f]\n    train_df[f\"{f}_tfidf\"] = train_df[f\"{f}_tfidf\"].fillna(0)\n    test_df[f\"{f}_tfidf\"] = test_df[f\"{f}_tfidf\"].fillna(0)\n    \ndisplay(train_df[train_df.columns[train_df.columns.str.endswith('tfidf')]].head(5))","306715b7":"feature_cols = train_df.columns[train_df.columns.str.startswith('feature_')]\nX = train_df[feature_cols]\ny = train_df['target']\nX_test = test_df[feature_cols]\n\nN_SPLITS = 5\n\ndisplay(X.head(5))","38fa87f0":"# Initialize variables\ny_oof_pred = np.zeros((len(X), 4))\ny_test_pred = np.zeros((len(X_test), 4))\n\nkf = StratifiedKFold(n_splits = N_SPLITS)\nfor fold, (train_idx, val_idx) in enumerate(kf.split(X, y)):    \n    # Prepare training and validation data\n    X_train = X.iloc[train_idx].reset_index(drop=True)\n    X_val = X.iloc[val_idx].reset_index(drop=True)\n\n    y_train = y.iloc[train_idx].reset_index(drop=True)\n    y_val = y.iloc[val_idx].reset_index(drop=True)  \n\n    # Define model\n    model = LogisticRegression(random_state = 42, \n                               #C = 0.8, \n                              # max_iter = 100, # default value\n                              )\n    model.fit(X_train, y_train)\n    \n    # Calculate evaluation metric\n    y_val_pred = model.predict_proba(X_val)\n\n    print(f\"Fold {fold + 1} Log Loss: {log_loss(y_val, y_val_pred)}\")\n\n    # Make predictions\n    y_oof_pred[val_idx] = y_val_pred\n    y_test_pred += model.predict_proba(X_test)\n\n\n# Calculate evaluation metric for out of fold validation set\ny_test_pred = y_test_pred \/ N_SPLITS\n\nprint(f\"Overall OOF Log Loss: {log_loss(y, y_oof_pred)}\")","bdcfc8d1":"# Visualize\ny_pred = pd.Series(y_test_pred.argmax(axis=1)).replace({0 : 'Class_1', 1 : 'Class_2', 2 : 'Class_3', 3 : 'Class_4'})\n\nprint(y_pred.value_counts())\n\nfig = plt.figure(figsize=(8,4))\nsns.countplot(y_pred,\n              palette='Greens', \n              order=['Class_1', 'Class_2', 'Class_3', 'Class_4'], )\nplt.show()","7939723c":"submission_df = pd.DataFrame(y_test_pred)\nsubmission_df.columns = ['Class_1', 'Class_2', 'Class_3', 'Class_4']\nsubmission_df['id'] = test_df['id']\nsubmission_df = submission_df[['id', 'Class_1', 'Class_2', 'Class_3', 'Class_4']]\n\nsubmission_df.to_csv(\"submission.csv\", index=False)\ndisplay(submission_df.head())","b5080324":"# Feature Engineering\n\n## Length and Count Features\n\nCommon length and count features for NLP problems are:\n* word count\n* character count\n* sentence count\n* average word length\n* average sentence length\n\nHowever, since we only have the BoW, we can only create a new feature for word count. For the remaining length and count features, we would need the original texts.","795b6684":"# Baseline\n\nThis baseline is copied from my initial notebook for this competition ([Lessons Learned from Previous Cat Competitions](https:\/\/www.kaggle.com\/iamleonie\/lessons-learned-from-previous-cat-competitions))","e7b9cc0c":"## Term Frequency\u2014Inverse Dense Frequency (TF-IDF) \n\nThe following explanation is largely based on this [medium blog post](https:\/\/towardsdatascience.com\/natural-language-processing-feature-engineering-using-tf-idf-e8b9d00e7e76).\n\n### Term Frequency\n\n> The number of times a word appears in a document ($n_{i,j}$) divded by the total number of words ($\\sum_k n_{i,j}$) in the document. \n\n$tf_{i,j} = \\dfrac{n_{i,j}}{\\sum_k n_{i,j}}$\n\n### Inverse Data Frequency\n\n> The log of the number of documents ($N$) divided by the number of documents that contain the word $w$ ($df_t$). \n\n$idf(w)=\\log(\\dfrac{N}{df_t})$","0a864aa1":"So far, this approach is not working very well... \n\nFurther experiments are on going and will be updated shortly.","5013ec46":"# Bag of Words \/ Document Term Matrix\n\nThe BoW or DTM is a way to represent text in a numerical way. Since this data is based on eCommerce listings, I will try to explain it with some eCommerce related examples:\nLet's say you have two product reviews:\n> \"This book was a great read.\" - Review 1\n\n> \"I read this book in a day. Easy read.\" - Review 2\n\nNow you take all unique words from both reviews and make them as features: {'this', 'book', 'was', 'great', 'read', 'i', 'in', 'a', 'day', 'easy'}. And then you assign the number of times this word appears in the review to the feature as follows:\n\n|          | this | book | was | great |read | i | in | a | day |easy |\n|:--------:|------|------|-----|-------|-------|---|----|---|-----|-----|\n| Review 1 | 1    | 1    | 1   | 1     |  1 |0 | 0  | 0 | 0   |0   |\n| Review 2 | 1    | 1    | 0   | 0     | 2 | 1 | 1  | 1 | 1   |1   |\n\nNow this looks quite similar to what we have above. \n\nThis is just a **wild guess but maybe we are trying to classify an eCommerce product given its product reviews**. For example, maybe we are trying to classify whether a book is a thriller, drama, novel or non-fiction based on its reviews. Of course this could be completely wrong but let's try this hypothesis.\n\nSome notes of precaution:\n* One point that does not fit this hypothesis fully at the moment is that we also have some features with negative values. BoW usually do not have negative values (how can a word appear negative times in a text?\"). \n* As [@melanie7744](https:\/\/www.kaggle.com\/melanie7744) has mentioned in the comments, 50 features for BoW seems a little bit too small for a vocabulary.\n* Some \"words\" have a high frequency in the \"sentences\". For example, feature_43 is appearing 21 times in the first sample. I cannot imagine what kind of \"text\" that would be.\n\n**If you have an idea how this could relate to the idea, please share it!**","0f272072":"> Lastly, the TF-IDF is simply the TF multiplied by IDF.\n\nTF-IDF = $tf_{i,j} \\cdot idf(w)$","8d7ae21a":"# Exploratory Data Analysis\nWe have seen many great notebooks in this challenge sofar showcasing dimensionality reductions for the features using UMAP and t-SNE. I want to take a slightly different approach and use dimensionality reduction to see which features are similar to each other.\n\nFrom below plot you can see that most features are close together in one big cluster and that feature_38 and feature_42 are both far off to the sides. This might be a hint to evaluate these features a little bit more in depth.\n\nAlso, we can see that classes 1, 3, 4 are in close approximity to each other while class 2 is a little bit further away from the other three classes.","13a7cccb":"# Just A Theory...\n\nWhen this competition launched first, I though that we were working with pre-encoded categorical data and I created this notebook [Lessons Learned from Previous Cat Competitions](https:\/\/www.kaggle.com\/iamleonie\/lessons-learned-from-previous-cat-competitions). Now a few days have passed and I am beginning to think that we might not be working with categorical data at all.\n\nLet's have a look at the data again. Below you can see that:\n- There are 50 features, which is more than in previous TPS Challenges according to [this disucssion](https:\/\/www.kaggle.com\/c\/tabular-playground-series-may-2021\/discussion\/236128)\n- The data is **sparse**, which means that we mostly have the value zero\n- There are no missing values\n- Most features are positive integers. Some can also be negative.\n\nHm... where have we seen this type of data before? This seems oddly similar to something you might have seen previously in **Natural Language Processing (NLP)** problems. The data almost looks like it is a **Bag of Words (BoW)** or **Document Term Matrix (DTM)** representation.","06a5f015":"As a first experiment, let's see what happens if we drop feature_38 and\/or feature_42.\n* drop feature_19 only: slightly increased performance\n* drop feature_38 only: decreased performance\n* drop feature_42 only: decreased performance\n* drop feature_38 and feature_42: slightly increased performance\n","91f413ae":"There are 2 datapoints in the training data with 0 words. Both are categorized as different classes. Since this does not happen in the test data, we will for now assume that this might be some faulty data and we will drop these from the training data."}}