{"cell_type":{"053fa8dc":"code","c9a150c4":"code","4e798123":"code","7feb11e8":"code","7df284a5":"code","b90a584f":"code","f3124405":"code","47851d09":"code","9c8442b2":"code","a7b14161":"code","835e661b":"code","bef73928":"code","dd0f1625":"code","0e3bf792":"code","be4240ab":"code","f846641d":"code","98aacbe1":"code","0cb1a3b4":"code","dcbeb8f9":"code","b7c5e3e3":"code","1462832c":"code","148e4b35":"code","3ff54bcd":"code","77e3c32b":"code","6dbae73c":"code","48775618":"code","9c5da9de":"code","1e7819a1":"code","45c376ce":"code","649783f5":"code","e0396da6":"code","0eaa17af":"markdown","1cfd66bd":"markdown"},"source":{"053fa8dc":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.model_selection import train_test_split\nfrom collections import Counter # Sort of useful dictionnary counting how many values for each element in a list\nimport seaborn as sns\n%matplotlib inline \n\nfrom scipy.stats import norm\nfrom scipy import stats\nimport matplotlib.pyplot as plt\n# %matplotlib inline so we see graphs within the code in the notebook\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c9a150c4":"# Get data\ndata = pd.read_csv(\"\/kaggle\/input\/new-york-city-airbnb-open-data\/AB_NYC_2019.csv\").set_index('id')\ndata.head(5)","4e798123":"# See essential info about dataset: column names, types, number of entries and non-null entries for each one\ndata.info()","7feb11e8":"# Fill empty and NaNs values with NaN\ndata = data.fillna(np.nan)\n# Check for Null values: only 16 names and 21 host_names missing. \n# 10052 last_review dates and number of reviews per month missing. \ndata.isnull().sum()","7df284a5":"#replacing all NaN values in 'reviews_per_month' with 0\ndata.fillna({'reviews_per_month':0}, inplace=True)\n#examing changes\ndata.reviews_per_month.isnull().sum()\n\n#Although i reckon names and host names and last review will certainly have an influence on demand and thus on prices,\n# I will drop those columns later on for this fast modelling","b90a584f":"# Know for each column: number of entries, mean, standard deviation, min, max, and percentiles.\ndata.describe()","f3124405":"# Analysing the price: our target variable\nsns.distplot(data['price'])","47851d09":"#skewness and kurtosis: looks very high!!!\nprint(\"Skewness: %f\" % data['price'].skew())\nprint(\"Kurtosis: %f\" % data['price'].kurt())","9c8442b2":"# 2 immediate observations about the price:\n# A. Some places price is 0!!!: they have to be taken out.\n# B. Some unusual places have really outrageously high prices: they should be taken out as well since our goal is to \n#model the price\n\n#A. Get rid of the 0$ places\nfree_apts = data.loc[data['price']==0]\nprint(len(free_apts))\nfree_apts_ids = list(free_apts.index)\nprint(free_apts_ids)\n# Drop the free appartments \ndata = data.drop(free_apts_ids, axis = 0)\ndata.head(5)","a7b14161":"# B. Keep only mainstream reasonnable priced appartments representing more than 97% of the data\nmain_data = pd.DataFrame(data.loc[data['price']< 500])\nprint(len(data))\nprint(len(main_data))\nprint(len(main_data)*100\/len(data))\nsns.distplot(main_data['price'])\nprint(\"Skewness: %f\" % main_data['price'].skew())\nprint(\"Kurtosis: %f\" % main_data['price'].kurt())\n","835e661b":"main_data.head(5)","bef73928":"# Feature analysis\n# Numerical features\n\n# Correlation matrix between numerical features: low correlation between the numerical features and the price\n# Inverse correlation between price and longitude: sounds normal, price differs in different regions\ncorr_mat = sns.heatmap(main_data[['latitude', 'longitude', 'price', 'minimum_nights', 'number_of_reviews', 'calculated_host_listings_count',\n'availability_365']].corr(),annot=True, fmt = \".2f\", cmap = \"coolwarm\")","dd0f1625":"# General correlation matrix\n# Positive correlation between number_of_reviews and reviews_per_month\ncorrmat = main_data.corr()\nf, ax = plt.subplots(figsize=(12, 9))\nsns.heatmap(corrmat, square=True);","0e3bf792":"#Relationship with categorical features\n#box plot neighbourhood\/price\n# box plot: Q1 is price below which is 25% of the data, Q3 is 75 percentile, IQR = Q3 - Q1\n# Outlier step above is Q3 +1.5IQR, below Q1 -1.5IQR, anything beyond is an outlier\n# There are still quite a lot of outliers\nvar = 'neighbourhood_group'\nnei_price = pd.concat([main_data['price'], main_data[var]], axis=1)\nf, ax = plt.subplots(figsize=(8, 8))\nfig = sns.boxplot(x=var, y=\"price\", data=nei_price)\nfig.axis(ymin=0, ymax=500);","be4240ab":"#scatter plot minimum_nights\/price\n# Seems like there is no correlation\nvar = 'minimum_nights'\nminnights_to_price = pd.concat([main_data['price'], main_data[var]], axis=1)\nminnights_to_price.plot.scatter(x=var, y='price', ylim=(0,500));","f846641d":"# \u05e2\u05dc \u05d4\u05d3\u05e8\u05da\n# Obviously the minimum_nights number is a mistake for this record, \n#we can replace it maybe with the value of 50% of listings\nmain_data.loc[data['minimum_nights']==1250]\nmain_data.iloc[5767,9]= 3\nmain_data.iloc[5767]","98aacbe1":"#using violinplot to showcase density and distribtuion of prices \n# We can see higher prices in Manhattan of course, of course prices getting the highest for entire homes\/apts\n# In general more private rooms and shared rooms than entire homes on the market\nfig_2=sns.violinplot(data=main_data, x='neighbourhood_group', y='price', hue= 'room_type')\nfig_2.set_title('Density and distribution of prices for each neighberhood_group')","0cb1a3b4":"# Plot price according to latitude\/longitude\nplt.figure(figsize=(10,6))\nfig_3=main_data.plot(kind='scatter', x='longitude',y='latitude',c='price',cmap=plt.get_cmap('jet'),colorbar=True);\nplt.ioff()\n# Plot room types according to latitude\/longitude\nplt.figure(figsize=(10,6))\nfig_4 = sns.scatterplot(main_data.longitude,main_data.latitude,hue=main_data.room_type)\nplt.ioff() # Turns the interactive mode off\n# Plot neighbourhood_group according to latitude\/longitude\nplt.figure(figsize=(10,6))\nfig_4 = sns.scatterplot(main_data.longitude,main_data.latitude,hue=main_data.neighbourhood_group)\nplt.ioff() # Turns the interactive mode off\n","dcbeb8f9":"# Even though i believe name and hostname (wether this is ethical or not) can influence demand and thus the price\n# at this stage i will not use them. All the more so as they have some null values.\n# Will also get rid of last_review because of its null values. host_id also won't help with a regression. So will drop it\nmain_data.drop(['name','host_name','host_id','last_review'],axis=1,inplace=True)\nmain_data.head()\n# The most interesting would be to get into NLP and analyse names to see the influence of the text chosen to describe the place","b7c5e3e3":"# Now i have to label-encode neighborhoud_group, neighbourhood and room_type\n# I am going to go with LabelEncode from Sklearn \n# OneHotEncoder would add as many columns as there are different unique values for each one\n# I am aware label encode can also introduce a wrong meaning using higher numbers for different for ex neighborhoods\n# when there shouldn't be but i'll do it that way though in this exercise\nfrom sklearn.preprocessing import LabelEncoder\n\nlabel = LabelEncoder()\n\ncol_to_encode = ['neighbourhood_group', 'neighbourhood', 'room_type']\nfor col in col_to_encode:\n    main_data[col]= label.fit_transform(main_data[col])\n    \nmain_data.head()","1462832c":"# Get the price: our target variable\nY_data = main_data['price']\nmain_data.drop(['price'],axis=1,inplace=True)\nmain_data.head()\n","148e4b35":"X_train, X_test, y_train, y_test = train_test_split(main_data, Y_data, test_size=0.3, random_state=101)","3ff54bcd":"from sklearn import metrics\n\ndef print_evaluate(true, predicted):  \n    mae = metrics.mean_absolute_error(true, predicted)\n    mse = metrics.mean_squared_error(true, predicted)\n    rmse = np.sqrt(metrics.mean_squared_error(true, predicted))\n    r2_square = metrics.r2_score(true, predicted)\n    print('MAE:', mae)\n    print('MSE:', mse)\n    print('RMSE:', rmse)\n    print('R2 Square', r2_square)","77e3c32b":"from sklearn import linear_model\n\nlin_reg = LinearRegression(normalize=True)\nlin_reg.fit(X_train,y_train)\n","6dbae73c":"print(X_test.index, lin_reg.predict(X_test))\nprint(y_test)","48775618":"submission = pd.DataFrame({'id': X_test.index, 'price': lin_reg.predict(X_test)})\nsubmission.to_csv('submission_airbnb.csv', index=False)\n# Below: for my own use, just because i like to have the true price next to the prediction\n#submission_wTrueResults = pd.DataFrame({'id': X_test.index, 'price': lin_reg.predict(X_test), 'true price': y_test})\n#submission_wTrueResults.to_csv('submission_airbnb_wTrueResults.csv', index=False)","9c5da9de":"print_evaluate(y_test, lin_reg.predict(X_test))","1e7819a1":"# The best results were obtained with the simple linear regression\n# I want to try with a Lasso linear regression = a linear regression with L1 regularization\n# with constant alpha * L1 norm of the weights vector added to the loss\nlasso_reg = linear_model.Lasso(alpha=0.1)\nlasso_reg.fit(X_train,y_train)\nlasso_pred = lasso_reg.predict(X_test)\nprint_evaluate(y_test, lasso_pred)\n# Simple linear regression is still the best!","45c376ce":"from sklearn.tree import DecisionTreeRegressor\nregressor = DecisionTreeRegressor(random_state=0)\nregressor.fit(X_train, y_train)\nprint_evaluate(y_test, regressor.predict(X_test))","649783f5":"import xgboost as xgb\ndata_dmatrix = xgb.DMatrix(data=main_data,label=Y_data)\nxg_reg = xgb.XGBRegressor(objective ='reg:squarederror', colsample_bytree = 0.3, learning_rate = 0.1,max_depth = 5, alpha = 10, n_estimators = 10)\nxg_reg.fit(X_train,y_train)\npreds = xg_reg.predict(X_test)\nprint_evaluate(y_test, preds)","e0396da6":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.datasets import make_regression\nX_train, y_train = make_regression(n_features=10, n_informative=2, random_state=0, shuffle=False)\nregr = RandomForestRegressor(max_depth=4, random_state=0)\nregr.fit(X_train, y_train)\nprint_evaluate(y_test, regr.predict(X_test))","0eaa17af":"DATA EXPLORATION AND PREPARATION","1cfd66bd":"PREDICTION\n\nTime to start predicting the price."}}