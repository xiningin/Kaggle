{"cell_type":{"88544e44":"code","59f68f90":"code","5586df27":"code","ad6692a3":"code","3fc3de67":"code","e42a5d07":"code","efec43a5":"code","0558313d":"code","f77b37f9":"code","4e5af152":"code","90cce18f":"code","be198343":"code","a2d20444":"code","eda74a84":"code","6184f838":"code","1cbe6c78":"code","e9cb151e":"code","f83be58a":"code","947a65ed":"code","0a2d02be":"code","b5fb92a0":"code","ca0b6d97":"code","6f164086":"code","0b267f69":"code","c779cb1e":"code","bb78e23f":"markdown","7c18cb49":"markdown","e3a179a3":"markdown","49c61092":"markdown","e38db9d9":"markdown","411615d2":"markdown","21cd6203":"markdown","9e4c9a10":"markdown","7a59027b":"markdown","0ff6ae21":"markdown","0d0ba1f6":"markdown","df4bde53":"markdown","0f518b62":"markdown","b7da762d":"markdown","6bc544b1":"markdown","b5520f1f":"markdown","8d5ac537":"markdown","6a8f84c2":"markdown","4542f31e":"markdown"},"source":{"88544e44":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","59f68f90":"#Let's just begin by reading our data and displaying it to see how our data looks like and how much cleaning it requires.\n\nsample_prediction = pd.read_csv(\"\/kaggle\/input\/loan-prediction-based-on-customer-behavior\/Sample Prediction Dataset.csv\")\ntraining_data = pd.read_csv(\"\/kaggle\/input\/loan-prediction-based-on-customer-behavior\/Training Data.csv\")\ntest_data = pd.read_csv(\"\/kaggle\/input\/loan-prediction-based-on-customer-behavior\/Test Data.csv\")","5586df27":"print(sample_prediction.head(), training_data.head(), test_data.head(), sep='\\n')","ad6692a3":"training_data.shape","3fc3de67":"#let's see what our data has for us\ntraining_data.describe()","e42a5d07":"training_data.isnull().sum()","efec43a5":"correlation = training_data.corr()\ncorrelation","0558313d":"#let's convert the categorical data to numeric data and find the correlation again on all columns unlike above\nfrom sklearn.preprocessing import LabelEncoder\ncol = [\"Profession\",\"CITY\", \"STATE\",\"Married\/Single\", \"House_Ownership\", \"Car_Ownership\"]\ntrain_data_labelled = training_data.copy()\ntest_data_labelled = test_data.copy()\nfor item in col:\n    if(train_data_labelled[item].dtype=='object' and test_data_labelled[item].dtype=='object'):\n        train_data_labelled[item].fillna('N', inplace=True)\n        test_data_labelled.fillna('N', inplace=True)\n        lbl= LabelEncoder()\n        lbl.fit(train_data_labelled[item].values)\n        train_data_labelled[item] = lbl.transform(train_data_labelled[item].values)\n        \n        lbl.fit(test_data_labelled[item].values)\n        test_data_labelled[item]= lbl.transform(test_data_labelled[item].values)\n        \ntrain_data_labelled.head()","f77b37f9":"train_corr = train_data_labelled.corr()\ntrain_corr","4e5af152":"#let's visualize it\ntrain_data_labelled.hist(figsize=(13,13))\nplt.show()","90cce18f":"#visualize correlation with pairplot\ncols = train_data_labelled.columns.to_list()\nrem = ['Id','Risk_Flag','CITY','STATE']\ncols = [item for item in cols if item not in rem]\nsns.pairplot(train_data_labelled[cols])","be198343":"pip install sweetviz","a2d20444":"import sweetviz as sv","eda74a84":"#using just one line of analyze function we can quickly analyze the report that will be created to us automatically using gml\nreport = sv.analyze(train_data_labelled)\nreport.show_html('train_data_labelled.html')","6184f838":"comparision = sv.compare(train_data_labelled[1000:], train_data_labelled[:1000])\ncomparision.show_html('compare.html')","1cbe6c78":"col = train_data_labelled.columns.to_list() \ncol.remove('Risk_Flag')\n\nx_train= train_data_labelled[col]\ny_train= train_data_labelled['Risk_Flag']\n\n#difference in column name ID in test_data and Id in train data, let's fix it\ntest_data_labelled = test_data_labelled.rename(columns={'ID':'Id'})\n\nx_test = test_data_labelled[test_data_labelled.columns]\ny_test = sample_prediction['risk_flag']\nprint(\"number of test samples:\", x_test.shape[0])\nprint(\"number of training samples:\",x_train.shape[0])","e9cb151e":"from sklearn.linear_model import Ridge\nridge = Ridge(alpha=0.1)\nridge.fit(x_train, y_train)\npred = ridge.predict(x_test)","f83be58a":"from sklearn.metrics import roc_auc_score\nauc = roc_auc_score(y_test, pred)\nprint('The accuracy score based on roc_auc_score is :',auc)","947a65ed":"from sklearn.tree import DecisionTreeClassifier\nDTClassifier= DecisionTreeClassifier(criterion='entropy', random_state=0)\n","0a2d02be":"DTClassifier.fit(x_train,y_train)\ny_pred = DTClassifier.predict(x_test)","b5fb92a0":"#Evaluate based on roc_auc_score\nprint(\"The accuracy score based on roc_auc_score is : \", roc_auc_score(y_test, y_pred))","ca0b6d97":"from sklearn.ensemble import RandomForestClassifier\nrc = RandomForestClassifier(n_estimators=500)\nrc.fit(x_train, y_train)","6f164086":"y_pred = rc.predict(x_test)","0b267f69":"#Evaluate based on roc_auc_score\nprint(\"The accuracy score based on roc_auc_score is : \", roc_auc_score(y_test, y_pred))","c779cb1e":"from sklearn.ensemble import VotingClassifier\nfrom sklearn.linear_model import LogisticRegression\n\nrmd_clf = RandomForestClassifier()\nDTC_clf = DecisionTreeClassifier()\nlog_clf = LogisticRegression()\n\nvoting_clf = VotingClassifier(estimators = [('lr',log_clf),('rf',rmd_clf),('dc',DTC_clf)], voting=\"hard\")\n\nfor clf in [log_clf,rmd_clf,DTC_clf]:\n    clf.fit(x_train, y_train)\n    y_pred = clf.predict(x_test)\n    print(clf.__class__.__name__,roc_auc_score(y_test, y_pred))","bb78e23f":"### Ridge Regression","7c18cb49":"**Evaluate based on ROC_AUC**","e3a179a3":"## Random Forest Classifier","49c61092":"since we know that the models best work with numeric data instead of categorical data, let's convert our data into numeric data","e38db9d9":"As we can see that score is better, let's try with RFC then we will compare all three.","411615d2":"No missing entries means we're good to go with any analysis","21cd6203":"## Preprocessing","9e4c9a10":"## Visualization Through GML SWEETVIZ","7a59027b":"## Decision Tree Classifier","0ff6ae21":"we'll perform our analysis through different Machine Learning Models and see which one works best in terms of accuracy","0d0ba1f6":"let's look if our data has any missing value to avoid any mistakes or we can fix them","df4bde53":"We are also adding Logistic Regression and Support Vector Classifier in the tuple to check if it has greater score than those we used earlier.","0f518b62":"The visualization from GML shows the direct method for finding relations between every columns, numerical columns, and categorical columns.","b7da762d":"let's find pearson correlation of the dataframe to check how strongly the values are related with each other.","6bc544b1":"we need to understand that since our prediction output isn't binary so we can't really use accuracy_score in ridge regression instead we'll use roc test","b5520f1f":"# Conclusion\nWe can see that the Logistic Regression has the highest accuracy score so we can use that model for our prediction analysis.","8d5ac537":"## Comparision with voting classifiers","6a8f84c2":"The pairplot shows every column's correlation with another column in the pictorial form, some of them form the linear line showing positive linear correlation such as experience and current job years.","4542f31e":"## **Modeling, Training, Spliting and Evaluating**"}}