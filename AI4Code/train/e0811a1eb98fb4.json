{"cell_type":{"2ecafc67":"code","3445f7a3":"code","f77a110f":"code","5b7d42af":"code","0a380f95":"code","8effa3a7":"code","f2c73002":"code","3c55cdd8":"code","ca140108":"code","a4c99a50":"code","989d1760":"code","3056ba70":"code","77cca1e2":"code","92d87fe7":"code","8a778d3f":"code","7b2749ed":"code","eac78cec":"code","22734691":"code","cabe4651":"code","b3dbdb00":"code","40e27825":"code","214b4b87":"code","3802c663":"code","ad3ab2a9":"code","7467b7f4":"code","8ab76887":"code","8da71386":"code","4ca984a5":"code","217fbce7":"code","13803146":"code","b5722863":"code","04b669e3":"code","ff1d4b65":"code","c558b4fe":"code","0c2cdd27":"code","90d9f406":"code","542fe522":"code","18a30e30":"code","7a79a2e7":"code","ccfbcaec":"code","ed777a45":"code","f58f5e8b":"code","02be2835":"markdown","82b43348":"markdown","3babd6e4":"markdown","44ba5f60":"markdown","3f801a46":"markdown","360852f3":"markdown","989190f5":"markdown","34afbcb2":"markdown","63d2bfbd":"markdown","3db9d466":"markdown","921eaeb8":"markdown","837075ff":"markdown","41a59689":"markdown","e877a1bf":"markdown","57c209ed":"markdown","2dce8d59":"markdown","d52f33c0":"markdown","0c486f98":"markdown","f7849a91":"markdown","7df439d9":"markdown","4e5c74d7":"markdown","778734bc":"markdown","2bbdcafa":"markdown","518cf308":"markdown","f5f0f998":"markdown"},"source":{"2ecafc67":"!pip install git+https:\/\/github.com\/fastai\/fastai_dev             > \/dev\/null","3445f7a3":"from fastai2.data.all import *\nfrom fastai2.tabular.core import *\nfrom fastai2.tabular.model import *\nfrom fastai2.optimizer import *\nfrom fastai2.learner import *\nfrom fastai2.metrics import *\nfrom fastai2.callback.all import *","f77a110f":"path = Path('\/kaggle\/input\/ashrae-energy-prediction')","5b7d42af":"train = pd.read_csv(path\/'train.csv')\ntrain = train.iloc[:7000]\n\nbldg = pd.read_csv(path\/'building_metadata.csv')\nweather_train = pd.read_csv(path\/\"weather_train.csv\")","0a380f95":"train = train[np.isfinite(train['meter_reading'])]","8effa3a7":"train.head()","f2c73002":"bldg.head()","3c55cdd8":"train = train.merge(bldg, left_on = 'building_id', right_on = 'building_id', how = 'left')","ca140108":"train.head()","a4c99a50":"weather_train.head()","989d1760":"train = train.merge(weather_train, left_on = ['site_id', 'timestamp'], right_on = ['site_id', 'timestamp'])","3056ba70":"del weather_train","77cca1e2":"train[\"timestamp\"] = pd.to_datetime(train[\"timestamp\"])\ntrain[\"hour\"] = train[\"timestamp\"].dt.hour\ntrain[\"day\"] = train[\"timestamp\"].dt.day\ntrain[\"weekend\"] = train[\"timestamp\"].dt.weekday\ntrain[\"month\"] = train[\"timestamp\"].dt.month","92d87fe7":"train.drop('timestamp', axis=1, inplace=True)\ntrain['meter_reading'] = np.log1p(train['meter_reading'])","8a778d3f":"test  = train.copy( deep=True)","7b2749ed":"cat_vars = [\"building_id\", \"primary_use\", \"hour\", \"day\", \"weekend\", \"month\", \"meter\"]\ncont_vars = [\"square_feet\", \"year_built\", \"air_temperature\", \"cloud_coverage\",\n              \"dew_temperature\"]\ndep_var = 'meter_reading'","eac78cec":"procs = [Normalize, Categorify, FillMissing]\nsplits = RandomSplitter()(range_of(train))","22734691":"to = TabularPandas(train, procs, cat_vars, cont_vars, y_names=dep_var, splits=splits, is_y_cat=False)","cabe4651":"to","b3dbdb00":"to.train","40e27825":"dbch = to.databunch()\ndbch.valid_dl.show_batch()","214b4b87":"trn_dl = TabDataLoader(to.train, bs=64, num_workers=0, shuffle=True, drop_last=True)\nval_dl = TabDataLoader(to.valid, bs=128, num_workers=0)","3802c663":"dbunch = DataBunch(trn_dl, val_dl)\ndbunch.valid_dl.show_batch()","ad3ab2a9":"def emb_sz_rule(n_cat): \n    \"Rule of thumb to pick embedding size corresponding to `n_cat`\"\n    return min(600, round(1.6 * n_cat**0.56))","7467b7f4":"def _one_emb_sz(classes, n, sz_dict=None):\n    \"Pick an embedding size for `n` depending on `classes` if not given in `sz_dict`.\"\n    sz_dict = ifnone(sz_dict, {})\n    n_cat = len(classes[n])\n    sz = sz_dict.get(n, int(emb_sz_rule(n_cat)))  # rule of thumb\n    return n_cat,sz","8ab76887":"def get_emb_sz(to, sz_dict=None):\n    \"Get default embedding size from `TabularPreprocessor` `proc` or the ones in `sz_dict`\"\n    return [_one_emb_sz(to.procs.classes, n, sz_dict) for n in to.cat_names]","8da71386":"emb_szs = get_emb_sz(to); print(emb_szs)","4ca984a5":"class TabularModel(Module):\n    \"Basic model for tabular data.\"\n    def __init__(self, emb_szs, n_cont, out_sz, layers, ps=None, embed_p=0., y_range=None, use_bn=True, bn_final=False):\n        ps = ifnone(ps, [0]*len(layers))\n        if not is_listy(ps): ps = [ps]*len(layers)\n        self.embeds = nn.ModuleList([Embedding(ni, nf) for ni,nf in emb_szs])\n        self.emb_drop = nn.Dropout(embed_p)\n        self.bn_cont = nn.BatchNorm1d(n_cont)\n        n_emb = sum(e.embedding_dim for e in self.embeds)\n        self.n_emb,self.n_cont,self.y_range = n_emb,n_cont,y_range\n        sizes = [n_emb + n_cont] + layers + [out_sz]\n        actns = [nn.ReLU(inplace=True) for _ in range(len(sizes)-2)] + [None]\n        _layers = [BnDropLin(sizes[i], sizes[i+1], bn=use_bn and i!=0, p=p, act=a)\n                       for i,(p,a) in enumerate(zip([0.]+ps,actns))]\n        if bn_final: _layers.append(nn.BatchNorm1d(sizes[-1]))\n        self.layers = nn.Sequential(*_layers)\n    \n    def forward(self, x_cat, x_cont):\n        if self.n_emb != 0:\n            x = [e(x_cat[:,i]) for i,e in enumerate(self.embeds)]\n            x = torch.cat(x, 1)\n            x = self.emb_drop(x)\n        if self.n_cont != 0:\n            x_cont = self.bn_cont(x_cont)\n            x = torch.cat([x, x_cont], 1) if self.n_emb != 0 else x_cont\n        x = self.layers(x)\n        if self.y_range is not None:\n            x = (self.y_range[1]-self.y_range[0]) * torch.sigmoid(x) + self.y_range[0]\n        return x","217fbce7":"model = TabularModel(emb_szs, len(to.cont_names), 1, [1000,500]); model","13803146":"opt_func = partial(Adam, wd=0.01, eps=1e-5)\nlearn = Learner(dbunch, model, MSELossFlat(), opt_func=opt_func)","b5722863":"learn.fit_one_cycle(5)","04b669e3":"# preds = learn.get_preds(dl = dbunch.train_dl)","ff1d4b65":"# preds = learn.get_preds(dl = dbunch.valid_dl)","c558b4fe":"# test = pd.read_csv(path\/'test.csv')","0c2cdd27":"test.shape","90d9f406":"to_test = TabularPandas(test, procs, cat_vars, cont_vars, y_names=dep_var ,  is_y_cat=False)","542fe522":"to_test.train.shape","18a30e30":"to_test.valid.shape","7a79a2e7":"dbch_test = to_test.databunch(shuffle_train = False)","ccfbcaec":"preds = learn.get_preds(dl = dbch_test.train_dl)","ed777a45":"trn_dl = TabDataLoader(to_test.train, bs=64, num_workers=0, shuffle=False)\ntdbunch = DataBunch(trn_dl)","f58f5e8b":"preds = learn.get_preds(dl = tdbunch.train_dl)","02be2835":"## Grabbing the Library","82b43348":"Now we can define our optimization function and create our `Learner`","3babd6e4":"Next, just like in fastai v1 we need to declare a few things. Specifically our Categorical and Continuous variables, our preprocessors (Normalization, Categorification, and FillMissing), along with how we want to split our data. `fastai` v2 now includes a `RandomSplitter` which is similar to `.split_by_rand_pct()` but now we can specify a custom range for our data (hence `range_of(train)`)","44ba5f60":"We can then also easily look at our training and validation datasets by calling `.train` or `.valid`","3f801a46":"The last piece of the puzzle we need is our basic `TabularModel`","360852f3":"# Test Section","989190f5":"Hope this helps you get started! :)\n\n- muellerzr","34afbcb2":"Lastly we can create a `DataBunch` object by calling `DataBunch()` and passing in our two `DataLoaders`","63d2bfbd":"Let's make a `Path` object to our data and combine the `train.csv` with the `building_metadata.csv` to grab some more information about these meter readings. For simplicity we will use the first 1000 samples from the training set. For the `DataFrame` preperation please see ryches Kernel [here](https:\/\/www.kaggle.com\/ryches\/simple-lgbm-solution)","3db9d466":"If we look at what `to` actually is, we can see what looks to be a bunch of batches of our data aligned into a dataframe that can easily be read!","921eaeb8":"First we need to enable internet access within this kernel and then `!pip install git+https:\/\/github.com\/fastai\/fastai_dev             > \/dev\/null` the dev repository for us to import from.","837075ff":"As you can see there are a *lot* of ways we can customize our DataBunch's now","41a59689":"## Making the DataBunch","e877a1bf":"# Method_2","57c209ed":"We're going to need a variety of imports, most importantly the `tabular.core` module for building the dataset (the rest deal with training the model)","2dce8d59":"Now we pass in our `TabularPandas` object, `to`","d52f33c0":"## Training the Model","0c486f98":"# fastai v2 Kernel Starter Code\n\nThe goal of this kernel is to show how to train a neural network using fastai 2.0 for this Kaggle Competition","f7849a91":"First let's define our embedding size rule of thumb, along with our `get_emb_sz` function","7df439d9":"Now let's try doing this the second method. We can increase our batch size since the validation set is much smaller than our training dataset. We can also specify a few options with our training dataset too. To do this, we will need to create `TabDataLoaders` to, well, load the data!\n\nWe pass in a dataset, a batch size, our `num_workers`, along with if we want to shuffle our dataset and drop the last batch if it does not evenly split. You should always want to do this with the **training** dataset but not the validation. Defaultly they are both set to `False`","4e5c74d7":"Now that those are defined, we can create a `TabularPandas` object by passing in our dataframe, the `procs`, our variables, what our `y` is, and how we want to split our data. `fastai` v2 is built on a Pipeline structure where first we dictate what we want to do, then we call the databunch (the high-level API is not done yet so we have nothing similar to directly DataBunching an object)","778734bc":"From here we can create our DataBunch object one of two ways. We can either directly do a `dbch = to.databunch()`, *or* we can take it one step further and apply custom works to some dataloaders. First let's look at the basic version","2bbdcafa":"First we need to create a `TabularModel` that needs an embedding matrix size, how many continuous variables to expect, the number of possible outputs (classes), and how big we want our layers. To pass in the embedding matrix sizes, we can use `get_emb_sz` onto a `TabularPandas` object","518cf308":"## Setting Up Our Data","f5f0f998":"If you noticed, most of what changed with the v2 API is focused on the dataloading \/ DataBunch creation. The rest of this Kernel sould look very familiar to fastai users"}}