{"cell_type":{"8694aaa6":"code","82622729":"code","c3d06617":"code","7afdc42a":"code","dba1cc9e":"code","5a6e145f":"code","b3a49a73":"code","5f0a076a":"code","f1280a56":"code","012fdb5c":"code","889a5843":"code","1f6e0bc9":"code","08b19d91":"code","cb5c9663":"code","d9de2cf2":"code","46f1daf1":"code","adc37ce4":"code","24dca75a":"code","15f9cf6f":"code","6cbde647":"code","3ca28f4d":"code","18366998":"code","94f99224":"code","aed5eb4a":"code","bc88b371":"code","4cee4acd":"markdown","b3f2f3e4":"markdown","0a393a25":"markdown","73c9cef2":"markdown","f12b6535":"markdown","efc86720":"markdown","6f5628f1":"markdown","fac31c74":"markdown","160a8920":"markdown","86fcd9c0":"markdown","c2ca7a92":"markdown","d1c01326":"markdown","7253bfc4":"markdown","d5cbd7e8":"markdown","1614eb23":"markdown","09f9fc8e":"markdown","82190004":"markdown","8f3b005e":"markdown","919a8ade":"markdown","a6ea9fc7":"markdown"},"source":{"8694aaa6":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","82622729":"df=pd.read_csv('\/kaggle\/input\/cusersmarildownloadsgermancsv\/german.csv',encoding ='ISO-8859-1',sep=\";\")\ndf.head()","c3d06617":"!pip install evalml","7afdc42a":"X = df.drop(columns=['Creditability'])\ny = df['Creditability']","dba1cc9e":"import evalml\nfrom evalml.pipelines import BinaryClassificationPipeline\n\n#X,y = evalml.demos.load_breast_cancer()\n\npipeline = BinaryClassificationPipeline([\"Simple Imputer\", \"Random Forest Classifier\"])\npipeline.fit(X,y)\n\nprint(pipeline.score(X,y, objectives = ['log loss binary']))","5a6e145f":"pipeline.feature_importance","b3a49a73":"pipeline.graph_feature_importance()","5f0a076a":"from evalml.model_understanding import calculate_permutation_importance\ncalculate_permutation_importance(pipeline, X, y, \"log loss binary\")","f1280a56":"from evalml.model_understanding import graph_permutation_importance\ngraph_permutation_importance(pipeline, X, y, 'log loss binary')","012fdb5c":"from evalml.model_understanding.graphs import partial_dependence\npartial_dependence(pipeline, X, features='Credit_Amount')","889a5843":"from evalml.model_understanding.graphs import graph_partial_dependence\ngraph_partial_dependence(pipeline, X, features='Credit_Amount')","1f6e0bc9":"X_fraud, y_fraud = evalml.demos.load_fraud(100, verbose=False)\nX_fraud.ww.init(logical_types={\"provider\": \"Categorical\", 'region': \"Categorical\"})\nfraud_pipeline = BinaryClassificationPipeline([\"DateTime Featurization Component\",\"One Hot Encoder\", \"Random Forest Classifier\"])\nfraud_pipeline.fit(X_fraud, y_fraud)\n\ngraph_partial_dependence(fraud_pipeline, X_fraud, features='provider')","08b19d91":"partial_dependence(pipeline, X, features=('Account_Balance', 'Duration_of_Credit_monthly'), grid_resolution=10)","cb5c9663":"graph_partial_dependence(pipeline, X, features=('Account_Balance', 'Duration_of_Credit_monthly'), grid_resolution=10)","d9de2cf2":"from evalml.model_understanding.graphs import confusion_matrix\ny_pred = pipeline.predict(X)\nconfusion_matrix(y, y_pred)","46f1daf1":"from evalml.model_understanding.graphs import graph_confusion_matrix\ny_pred = pipeline.predict(X)\ngraph_confusion_matrix(y, y_pred)","adc37ce4":"from evalml.model_understanding.graphs import graph_prediction_vs_actual\nfrom evalml.pipelines import RegressionPipeline\n\n#X_regress, y_regress = evalml.demos.load_diabetes()\nX_train, X_test, y_train, y_test = evalml.preprocessing.split_data(X, y, problem_type='regression')\n\npipeline_regress = RegressionPipeline(['One Hot Encoder', 'Linear Regressor'])\npipeline_regress.fit(X_train, y_train)\n\ny_pred = pipeline_regress.predict(X_test)\ngraph_prediction_vs_actual(y_test, y_pred, outlier_threshold=50)","24dca75a":"pipeline_dt = BinaryClassificationPipeline(['Simple Imputer', 'Decision Tree Classifier'])\npipeline_dt.fit(X, y)","15f9cf6f":"from evalml.model_understanding.graphs import visualize_decision_tree\n\nvisualize_decision_tree(pipeline_dt.estimator, max_depth=2, rotate=False, filled=True, filepath=None)","6cbde647":"from evalml.model_understanding.prediction_explanations import explain_predictions\n\ntable = explain_predictions(pipeline=pipeline, input_features=X, y=None, indices_to_explain=[3],\n                           top_k_features=6, include_shap_values=True)\nprint(table)","3ca28f4d":"from evalml.model_understanding.prediction_explanations import explain_predictions\n\nreport = explain_predictions(pipeline=pipeline, input_features=X, y=y, indices_to_explain=[0, 4, 9], include_shap_values=True,\n                            output_format='text')\nprint(report)","18366998":"from evalml.model_understanding.prediction_explanations import explain_predictions_best_worst\n\nreport = explain_predictions_best_worst(pipeline=pipeline, input_features=X, y_true=y,\n                                        include_shap_values=True, top_k_features=6, num_to_explain=2)\n\nprint(report)","94f99224":"import numpy as np\n\ndef hinge_loss(y_true, y_pred_proba):\n\n    probabilities = np.clip(y_pred_proba.iloc[:, 1], 0.001, 0.999)\n    y_true[y_true == 0] = -1\n\n    return np.clip(1 - y_true * np.log(probabilities \/ (1 - probabilities)), a_min=0, a_max=None)\n\nreport = explain_predictions_best_worst(pipeline=pipeline, input_features=X, y_true=y,\n                                        include_shap_values=True, num_to_explain=5, metric=hinge_loss)\n\nprint(report)","aed5eb4a":"single_prediction_report = explain_predictions(pipeline=pipeline, input_features=X, indices_to_explain=[3],\n                                              y=y, top_k_features=6, include_shap_values=True,\n                                              output_format=\"dataframe\")\nsingle_prediction_report","bc88b371":"report = explain_predictions_best_worst(pipeline=pipeline, input_features=X, y_true=y,\n                                        num_to_explain=1, top_k_features=6,\n                                        include_shap_values=True, output_format=\"dataframe\")\nreport","4cee4acd":"#EvalML \n\nhttps:\/\/github.com\/alteryx\/evalml\n\nEvalML is an AutoML library which builds, optimizes, and evaluates machine learning pipelines using domain-specific objective functions.\n\nKey Functionality\n\nAutomation - Makes machine learning easier. Avoid training and tuning models by hand. Includes data quality checks, cross-validation and more.\n\nData Checks - Catches and warns of problems with your data and problem setup before modeling.\n\nEnd-to-end - Constructs and optimizes pipelines that include state-of-the-art preprocessing, feature engineering, feature selection, and a variety of modeling techniques.\n\nModel Understanding - Provides tools to understand and introspect on models, to learn how they'll behave in your problem domain.\n\nDomain-specific - Includes repository of domain-specific objective functions and an interface to define your own.\n\nFollow along by cloning the demo repository: https:\/\/github.com\/alteryx\/open_sourc...\n\nFor more information on EvalML, visit our GitHub page at https:\/\/github.com\/alteryx\/evalml and our documentation at https:\/\/evalml.alteryx.com\n\nhttps:\/\/github.com\/alteryx\/evalml","b3f2f3e4":"#Two-way partial dependence plots are also possible and invoke the same API.","0a393a25":"#Explaining Best and Worst Predictions\n\nWhen debugging machine learning models, it is often useful to analyze the best and worst predictions the model made. The explain_predictions_best_worst function can help us with this.\n\nThis function will display the output of explain_predictions for the best 2 and worst 2 predictions. By default, the best and worst predictions are determined by the absolute error for regression problems and cross entropy for classification problems.\n\nWe can specify our own ranking function by passing in a function to the metric parameter. This function will be called on y_true and y_pred. By convention, lower scores are better.\n\nAt the top of each table, we can see the predicted probabilities, target value, error, and row index for that prediction. For a regression problem, we would see the predicted value instead of predicted probabilities.","73c9cef2":"#Code by Homayoon Khadivi https:\/\/www.kaggle.com\/homayoonkhadivi\/creative-automation-ml-evalml-model-understanding\/comments","f12b6535":"#Tree Visualization\n\nVisualize the structure of the Decision Tree that was fit to that data, and save it if necessary.","efc86720":"#Permutation Importance\n\nCompute and plot the permutation importance of the pipeline.","6f5628f1":"#Partial Dependence Plots\n\nCalculate the one-way partial dependence plots for a feature. Here, I chose Credit amount.","fac31c74":"#Code by Homayoon Khadivi https:\/\/www.kaggle.com\/homayoonkhadivi\/creative-automation-ml-evalml-model-understanding\/comments","160a8920":"#Why the parenthesis above are empty?","86fcd9c0":"#Compute the partial dependence for a categorical feature. We don't have categorical, hence I used Demos Load fraud, which has nothing to do with my Creditability Dataset. I saved that snippet to apply when I have Categorical features. ","c2ca7a92":"#Predicted Vs Actual Values Graph for Regression Problems\n\nCreate a scatterplot comparing predicted vs actual values for regression problems. Specify an outlier_threshold to color values differently if the absolute difference between the actual and predicted values are outside of a given threshold.","d1c01326":"#Best and worst predictions as a dataframe","7253bfc4":"#The interpretation of the table\n\nThe interpretation of the table is the same for regression problems - but the SHAP value now corresponds to the change in the estimated value of the dependent variable rather than a change in probability. For multiclass classification problems, a table will be output for each possible class.\n\nBelow is an example of how you would explain three predictions with explain_predictions.","d5cbd7e8":"#Create a bar plot of the feature importances","1614eb23":"#Hinge Loss\n\nUse a custom metric (hinge loss) for selecting the best and worst predictions. See this example:","09f9fc8e":"#Explaining Predictions\n\nExplain why the model made certain predictions with the explain_predictions function. This will use the Shapley Additive Explanations (SHAP) algorithm to identify the top features that explain the predicted value.\n\nThis function can explain both classification and regression models - all you need to do is provide the pipeline, the input features, and a list of rows corresponding to the indices of the input features you want to explain. The function will return a table that you can print summarizing the top 3 most positive and negative contributing features to the predicted value.","82190004":"#Feature Importance\n\nWe can get the importance associated with each feature of the resulting pipeline","8f3b005e":"#Confusion Matrix\n\nFor binary or multiclass classification, we can view a confusion matrix of the classifier\u2019s predictions. In the DataFrame output of confusion_matrix(), the column header represents the predicted labels while row header represents the actual labels.","919a8ade":"#Single prediction as a dataframe","a6ea9fc7":"![](https:\/\/warehouse-camo.ingress.cmh1.psfhosted.org\/072e5a48a57a57f8c239cd3fb717f1c9def7f2bd\/68747470733a2f2f6576616c6d6c2d7765622d696d616765732e73332e616d617a6f6e6177732e636f6d2f6576616c6d6c5f686f72697a6f6e74616c2e737667)pypi.org"}}