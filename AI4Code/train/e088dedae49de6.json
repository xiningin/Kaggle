{"cell_type":{"8d9b7e51":"code","127f5163":"code","1143a13e":"code","80d5412a":"code","bf1d8c0b":"code","01f1ad3d":"code","5393ced1":"code","adb8e448":"code","b93f8f3c":"code","00d3e596":"code","11798cbd":"code","e807ec65":"code","7ab621e2":"code","b702902d":"code","b0789075":"code","294992f7":"code","01230bba":"markdown","75e076f2":"markdown","77e3e78e":"markdown","f5c86946":"markdown","2b9ffa20":"markdown","922c8700":"markdown","f9a7a64a":"markdown","cf902714":"markdown","9c1b3e6b":"markdown","707931ec":"markdown"},"source":{"8d9b7e51":"import math\nimport numpy as np\nimport pandas as pd\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import initializers\n\nfrom sklearn.metrics import log_loss\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.metrics import accuracy_score","127f5163":"def get_data():\n\n    df = pd.read_csv('..\/input\/paris-housing-classification\/ParisHousingClass.csv')\n\n    category_to_idx = {v:k for k,v in enumerate(df['category'].unique())}\n    idx_to_category = {k:v for k,v in enumerate(df['category'].unique())}\n\n    features = df.iloc[:,:-2]\n    target = df['category'].replace(category_to_idx)\n    targets = pd.get_dummies(df['category'])\n\n    return df, features, target, targets","1143a13e":"df, features, target, targets = get_data()\ndf.head()","80d5412a":"train, test = train_test_split(df, test_size=0.33, random_state=42)","bf1d8c0b":"class NeuralDecisionTree(keras.Model):\n    def __init__(self, depth, num_features, used_features_rate, num_classes):\n        super(NeuralDecisionTree, self).__init__()\n        self.depth = depth\n        self.num_leaves = 2 ** depth\n        self.num_classes = num_classes\n\n        # Create a mask for the randomly selected features.\n        num_used_features = int(num_features * used_features_rate)\n        one_hot = np.eye(num_features)\n        sampled_feature_indicies = np.random.choice(\n            np.arange(num_features), num_used_features, replace=False\n        )\n        self.used_features_mask = one_hot[sampled_feature_indicies]\n\n        # Initialize the weights of the classes in leaves.\n        self.pi = tf.Variable(\n            initial_value=tf.random_normal_initializer()(\n                shape=[self.num_leaves, self.num_classes]\n            ),\n            dtype=\"float32\",\n            trainable=True,\n        )\n\n        # Initialize the stochastic routing layer.\n        self.decision_fn = layers.Dense(\n            units=self.num_leaves, activation=\"sigmoid\", name=\"decision\"\n        )\n\n    def call(self, features):\n        batch_size = tf.shape(features)[0]\n\n        # Apply the feature mask to the input features.\n        features = tf.matmul(\n            features, self.used_features_mask, transpose_b=True\n        )  # [batch_size, num_used_features]\n        # Compute the routing probabilities.\n        decisions = tf.expand_dims(\n            self.decision_fn(features), axis=2\n        )  # [batch_size, num_leaves, 1]\n        # Concatenate the routing probabilities with their complements.\n        decisions = layers.concatenate(\n            [decisions, 1 - decisions], axis=2\n        )  # [batch_size, num_leaves, 2]\n\n        mu = tf.ones([batch_size, 1, 1])\n\n        begin_idx = 1\n        end_idx = 2\n        # Traverse the tree in breadth-first order.\n        for level in range(self.depth):\n            mu = tf.reshape(mu, [batch_size, -1, 1])  # [batch_size, 2 ** level, 1]\n            mu = tf.tile(mu, (1, 1, 2))  # [batch_size, 2 ** level, 2]\n            level_decisions = decisions[\n                :, begin_idx:end_idx, :\n            ]  # [batch_size, 2 ** level, 2]\n            mu = mu * level_decisions  # [batch_size, 2**level, 2]\n            begin_idx = end_idx\n            end_idx = begin_idx + 2 ** (level + 1)\n\n        mu = tf.reshape(mu, [batch_size, self.num_leaves])  # [batch_size, num_leaves]\n        probabilities = keras.activations.softmax(self.pi)  # [num_leaves, num_classes]\n        outputs = tf.matmul(mu, probabilities)  # [batch_size, num_classes]\n        return outputs","01f1ad3d":"class NeuralDecisionForest(keras.Model):\n    def __init__(self, num_trees, depth, num_features, used_features_rate, num_classes):\n        super(NeuralDecisionForest, self).__init__()\n        self.ensemble = []\n        # Initialize the ensemble by adding NeuralDecisionTree instances.\n        # Each tree will have its own randomly selected input features to use.\n        for _ in range(num_trees):\n            self.ensemble.append(\n                NeuralDecisionTree(depth, num_features, used_features_rate, num_classes)\n            )\n\n    def call(self, inputs):\n        # Initialize the outputs: a [batch_size, num_classes] matrix of zeros.\n        batch_size = tf.shape(inputs)[0]\n        outputs = tf.zeros([batch_size, num_classes])\n\n        # Aggregate the outputs of trees in the ensemble.\n        for tree in self.ensemble:\n            outputs += tree(inputs)\n        # Divide the outputs by the ensemble size to get the average.\n        outputs \/= len(self.ensemble)\n        return outputs","5393ced1":"learning_rate = 0.01\nbatch_size = 265\nnum_epochs = 10\nhidden_units = [64, 64]","adb8e448":"num_trees = 10\ndepth = 10\nused_features_rate = 1.0\nnum_classes = targets.shape[1]\nnum_features = features.shape[1]\n\nforest_model = NeuralDecisionForest(\n    num_trees, depth, num_features, used_features_rate, num_classes\n)","b93f8f3c":"def get_embed_layer():\n    return layers.Embedding(100000, 1, input_length=16)","00d3e596":"def get_model(embed_layer):\n    \n    inputs = layers.Input(shape = (16,))\n    x = embed_layer(inputs)\n    x = layers.Flatten()(x)\n\n    dense = layers.Dense(16, \n                       activation='relu',\n                       kernel_initializer='random_uniform',\n                       bias_initializer=initializers.Constant(0.1))(x)\n\n    x = layers.Dropout(0.3)(dense)\n    x = layers.Dense(50, activation='relu')(x)\n    x = layers.Dropout(0.3)(x)\n    x = layers.Dropout(0.3)(x)\n    x = layers.Dense(20, activation = 'relu')(x)\n    output = layers.Dense(2, activation = 'softmax')(x)\n    \n    return inputs, dense, output","11798cbd":"metrics = [tf.keras.metrics.CategoricalCrossentropy()]\nloss = tf.keras.losses.CategoricalCrossentropy()\n\nes = tf.keras.callbacks.EarlyStopping(\n    monitor='val_loss', min_delta=0.0000001, patience=2, verbose=0,\n    mode='min', baseline=None, restore_best_weights=True)\n\nplateau = tf.keras.callbacks.ReduceLROnPlateau(\n    monitor='val_loss', factor=0.5, patience=2, verbose=0,\n    mode='min', min_delta=0.0000001, cooldown=0, min_lr=10e-7)","e807ec65":"N_FOLDS = 3\nSEED = 2021\n\noof_embedding = np.zeros((train.shape[0],2))\npred_embedding = np.zeros((test.shape[0],2))\n\noof_forest = np.zeros((train.shape[0],2))\npred_forest = np.zeros((test.shape[0],2))\n\nskf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=SEED)","7ab621e2":"for fold, (tr_idx, ts_idx) in enumerate(skf.split(train,train.iloc[:,-1])):\n    print(\"\\n - - - -      Fold: %2d      - - - - \\n\" % (fold + 1))\n       \n    X_train = train.iloc[:,:-2].iloc[tr_idx]\n    y_train = targets.iloc[tr_idx]\n    X_test = train.iloc[:,:-2].iloc[ts_idx]\n    y_test = targets.iloc[ts_idx]\n    \n    test_embeds = get_embed_layer()(test.iloc[:,:-2].values).numpy()\n    \n    inputs, dense, output = get_model(get_embed_layer())\n    \n    model_embedding = tf.keras.Model(inputs,output)\n    model_forest = tf.keras.Model(inputs,forest_model(dense))\n    \n    # ANN\n    model_embedding.compile(tf.keras.optimizers.Adam(learning_rate=0.0001),\n            loss = loss ,\n            metrics = metrics)\n\n    model_embedding.fit(X_train,y_train,\n            validation_data=(X_test,y_test),\n            epochs=50,\n            verbose=0,\n            batch_size = 256,\n            callbacks=[es,plateau])\n    \n    oof_embedding[ts_idx] = model_embedding.predict(X_test)\n    score_embedding = log_loss(y_test, oof_embedding[ts_idx])\n    print(\"Fold %2d, Loss for ANN model: %1.3f\" % ((fold + 1), score_embedding))\n    pred_embedding += model_embedding.predict(test_embeds) \/ N_FOLDS\n    \n    # Decision Forest\n    model_forest.compile(tf.keras.optimizers.Adam(learning_rate=0.001),\n            loss = loss,\n            metrics = metrics)\n    \n    model_forest.fit(X_train,y_train,\n                    validation_data = (X_test,y_test),\n                    batch_size = 256,\n                    epochs = 50,\n                    verbose = 0,\n                    callbacks = [es,plateau])\n    \n    oof_forest[ts_idx] = model_forest.predict(X_test)\n    score_forest = log_loss(y_test, oof_forest[ts_idx])\n    print(\"Fold %2d, Loss for decision forest: %1.3f\" % ((fold + 1), score_forest))\n    \n    pred_forest += model_forest.predict(test_embeds) \/ N_FOLDS","b702902d":"one_hot_test = pd.get_dummies(test.iloc[:,-1])\n\nscore_embedding = log_loss(one_hot_test, pred_embedding)\nprint(\"ANN Model Loss : %1.3f\" % score_embedding)\n\nscore_forest = log_loss(one_hot_test, pred_forest)\nprint(\"Decision Forest Loss: %1.3f\" % score_forest)","b0789075":"def get_accuracy(test, pred):\n    return accuracy_score(np.argmax(test.values, axis=1), np.argmax(pred, axis=1))","294992f7":"acc_embedding = get_accuracy(one_hot_test, pred_embedding)\nprint(\"ANN Model Accuracy : %1.3f\" % acc_embedding)\n\nacc_forest = get_accuracy(one_hot_test, pred_forest)\nprint(\"Decision Forest Accuracy: %1.3f\" % acc_forest)","01230bba":"<h1 id=\"params\" style=\"color:black; background:white; border:0.5px dotted black;\"> \n    <left>Hyperparameters\n        <a class=\"anchor-link\" href=\"#params\" target=\"_self\">\u00b6<\/a>\n    <\/left>\n<\/h1>","75e076f2":"<h1 id=\"result\" style=\"color:black; background:white; border:0.5px dotted black;\"> \n    <left>Results\n        <a class=\"anchor-link\" href=\"#result\" target=\"_self\">\u00b6<\/a>\n    <\/left>\n<\/h1>","77e3e78e":"<h1 id=\"tree\" style=\"color:black; background:white; border:0.5px dotted black;\"> \n    <left>Decision Forest\n        <a class=\"anchor-link\" href=\"#tree\" target=\"_self\">\u00b6<\/a>\n    <\/left>\n<\/h1>","f5c86946":"## Losses","2b9ffa20":"<h1 id=\"dataset\" style=\"color:black; background:white; border:0.5px dotted black;\"> \n    <left>Dataset\n        <a class=\"anchor-link\" href=\"#dataset\" target=\"_self\">\u00b6<\/a>\n    <\/left>\n<\/h1>","922c8700":"## Accuracies","f9a7a64a":"<pre>\n                |\n                |\n                A\n              _\/X\\_\n              \\\/X\\\/\n               |V|\n               |A|\n               |V|\n              \/XXX\\\n              |\\\/\\|\n              |\/\\\/|\n              |\\\/\\|\n              |\/\\\/|\n              |\\\/\\|\n              |\/\\\/|\n             IIIIIII\n             |\\\/_\\\/|\n            \/\\\/\/ \\\\\/\\\n            |\/|   |\\|\n           \/\\X\/___\\X\/\\\n          IIIIIIIIIIIII\n         \/`-\\\/XXXXX\\\/-`\\\n       \/`.-'\/\\|\/I\\|\/\\'-.`\\\n      \/`\\-\/_.-\"` `\"-._ \\-\/\\\n     \/.-'.'           '.'-.\\\n   \/`\\-\/               \\-\/`\\\n _\/`-'\/`_               _`\\'-`\\_\n`\"\"\"\"\"\"\"`                `\"\"\"\"\"\"`\n\nParis Housing ANN + Decision Forest\nby <b>Alin Cijov<\/b>\n<\/pre>","cf902714":"<h1 id=\"train\" style=\"color:black; background:white; border:0.5px dotted black;\"> \n    <left>Training\n        <a class=\"anchor-link\" href=\"#train\" target=\"_self\">\u00b6<\/a>\n    <\/left>\n<\/h1>","9c1b3e6b":"<h1 id=\"add\" style=\"color:black; background:white; border:0.5px dotted black;\"> \n    <left>Additional Functions\n        <a class=\"anchor-link\" href=\"#add\" target=\"_self\">\u00b6<\/a>\n    <\/left>\n<\/h1>","707931ec":"<h1 id=\"model\" style=\"color:black; background:white; border:0.5px dotted black;\"> \n    <left>ANN Model\n        <a class=\"anchor-link\" href=\"#model\" target=\"_self\">\u00b6<\/a>\n    <\/left>\n<\/h1>"}}