{"cell_type":{"943e843e":"code","7aa83807":"code","2f318e0e":"code","816557e9":"code","d4a0ec60":"code","43037aa7":"code","a7e362db":"code","d6dd62c7":"code","995a74c2":"code","909260ee":"code","8adfeadb":"code","6ee78d49":"code","c6dde9e9":"code","14f2bc2c":"code","3480fe5c":"code","64d96264":"code","92605718":"code","1ff23c4c":"code","03999524":"markdown","09a689cf":"markdown"},"source":{"943e843e":"import numpy as np \nimport pandas as pd \nimport os\n\nfrom collections import defaultdict\nfrom glob import glob\nfrom random import choice, sample\nfrom keras.preprocessing import image\nimport cv2\nfrom tqdm import tqdm_notebook\nimport numpy as np\nimport pandas as pd\nfrom keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\nfrom keras.layers import Input, Dense, GlobalMaxPool2D, GlobalAvgPool2D, Concatenate, Multiply, Dropout, Subtract\nfrom keras.models import Model\nfrom keras.optimizers import Adam","7aa83807":" !cp -r ..\/input\/kerasvggface\/* .\/","2f318e0e":"os.chdir(\"..\/input\/kerasvggface\/keras-vggface-master\")","816557e9":"from keras_vggface.utils import preprocess_input\nfrom keras_vggface.vggface import VGGFace","d4a0ec60":"train_file_path = \"\/kaggle\/input\/rfiwsmile\/recognizing-faces-in-the-wild\/train_relationships.csv\"\ntrain_folders_path = \"\/kaggle\/input\/rfiwsmile\/recognizing-faces-in-the-wild\/train\"","43037aa7":"val_famillies_list = [\"F07\", \"F08\", \"F09\"]\n# val_famillies_list = [\"F09\"]","a7e362db":"%%time\nall_images = glob(train_folders_path + \"*\/*\/*\/*\")","d6dd62c7":"def get_train_val(family_name):\n    # Get val_person_image_map\n    val_famillies = family_name\n    train_images = [x for x in all_images if val_famillies not in x] # train_img if val_fam not given\n    val_images = [x for x in all_images if val_famillies in x] # val_img if val_fam given\n\n    train_person_to_images_map = defaultdict(list) # initialises a default dict which will not give key error\n\n    ppl = [x.split(\"\/\")[-3] + \"\/\" + x.split(\"\/\")[-2] for x in all_images] # EXTRACT NAME VALUES\n\n    for x in train_images:\n        train_person_to_images_map[x.split(\"\/\")[-3] + \"\/\" + x.split(\"\/\")[-2]].append(x) \n\n    val_person_to_images_map = defaultdict(list)\n\n    for x in val_images:\n        val_person_to_images_map[x.split(\"\/\")[-3] + \"\/\" + x.split(\"\/\")[-2]].append(x)\n        \n    # Get the train and val dataset\n    relationships = pd.read_csv(train_file_path)\n    relationships = list(zip(relationships.p1.values, relationships.p2.values))\n    relationships = [x for x in relationships if x[0] in ppl and x[1] in ppl]\n\n    train = [x for x in relationships if val_famillies not in x[0]]\n    val = [x for x in relationships if val_famillies in x[0]]\n    \n    return train, val, train_person_to_images_map, val_person_to_images_map","995a74c2":"def read_img(path):\n    img = image.load_img(path, target_size=(197, 197))\n    img = np.array(img).astype(np.float)\n    return preprocess_input(img, version=2)\n\ndef gen(list_tuples, person_to_images_map, batch_size=16):\n    ppl = list(person_to_images_map.keys())\n    while True:\n        batch_tuples = sample(list_tuples, batch_size \/\/ 2)\n        labels = [1] * len(batch_tuples)\n        while len(batch_tuples) < batch_size:\n            p1 = choice(ppl)\n            p2 = choice(ppl)\n\n            if p1 != p2 and (p1, p2) not in list_tuples and (p2, p1) not in list_tuples:\n                batch_tuples.append((p1, p2))\n                labels.append(0)\n\n        for x in batch_tuples:\n            if not len(person_to_images_map[x[0]]):\n                print(x[0])\n\n        X1 = [choice(person_to_images_map[x[0]]) for x in batch_tuples]\n        X1 = np.array([read_img(x) for x in X1])\n\n        X2 = [choice(person_to_images_map[x[1]]) for x in batch_tuples]\n        X2 = np.array([read_img(x) for x in X2])\n\n        yield [X1, X2], labels\n\nimport tensorflow as tf\nfrom keras import backend as K\ndef focal_loss(gamma=2., alpha=.25):\n    def focal_loss_fixed(y_true, y_pred):\n        pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))\n        pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))\n        return -K.sum(alpha * K.pow(1. - pt_1, gamma) * K.log(K.epsilon()+pt_1))-K.sum((1-alpha) * K.pow( pt_0, gamma) * K.log(1. - pt_0 + K.epsilon()))\n    return focal_loss_fixed\n\n\ndef baseline_model():\n    input_1 = Input(shape=(197, 197, 3))\n    input_2 = Input(shape=(197, 197, 3))\n\n    base_model = VGGFace(model='resnet50', include_top=False)\n\n    for x in base_model.layers[:-3]:\n        x.trainable = False\n    for x in base_model.layers[-3:]:\n        x.trainable=True\n\n    x1 = base_model(input_1)\n    x2 = base_model(input_2)\n\n    \n\n    x1 = Concatenate(axis=-1)([GlobalMaxPool2D()(x1), GlobalAvgPool2D()(x1)])\n    x2 = Concatenate(axis=-1)([GlobalMaxPool2D()(x2), GlobalAvgPool2D()(x2)])\n\n    x3 = Subtract()([x1, x2])\n    x3 = Multiply()([x3, x3])\n\n    x1_ = Multiply()([x1, x1])\n    x2_ = Multiply()([x2, x2])\n    x4 = Subtract()([x1_, x2_])\n    x = Concatenate(axis=-1)([x4, x3])\n\n    x = Dense(100, activation=\"relu\")(x)\n    x = Dropout(0.01)(x)\n    out = Dense(1, activation=\"sigmoid\")(x)\n\n    model = Model([input_1, input_2], out)\n    \n    # loss=\"binary_crossentropy\"\n    model.compile(loss=[focal_loss(alpha=.25, gamma=2)], \n                  metrics=['acc'], \n                  optimizer=Adam(0.00003))\n\n    model.summary()\n\n    return model","909260ee":"model = baseline_model()","8adfeadb":"n_val_famillies_list = len(val_famillies_list)","6ee78d49":"n_val_famillies_list","c6dde9e9":"for i in tqdm_notebook(range(n_val_famillies_list)):\n    train, val, train_person_to_images_map, val_person_to_images_map = get_train_val(val_famillies_list[i])\n    file_path = f\"\/kaggle\/workingvgg_face_{i}.h5\"\n    checkpoint = ModelCheckpoint(file_path, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n    reduce_on_plateau = ReduceLROnPlateau(monitor=\"val_acc\", mode=\"max\", factor=0.5, patience=5, verbose=1)\n    es = EarlyStopping(monitor=\"val_acc\", min_delta = 0.001, patience=15, verbose=1)\n    callbacks_list = [checkpoint, reduce_on_plateau, es]\n\n    history = model.fit_generator(gen(train, train_person_to_images_map, batch_size=32), \n                                  use_multiprocessing=True,\n                                  validation_data=gen(val, val_person_to_images_map, batch_size=32), \n                                  epochs=100, verbose=1,\n                                  workers=4, callbacks=callbacks_list, \n                                  steps_per_epoch=400, \n                                  validation_steps=500)","14f2bc2c":"import matplotlib.pyplot as plt\n%matplotlib inline\n\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\n# summarize history for loss\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","3480fe5c":"test_path = \"..\/input\/newtest\/test\/\"\n\nsubmission = pd.read_csv('..\/input\/rfiwsmile\/recognizing-faces-in-the-wild\/sample_submission.csv')\n\ndef chunker(seq, size=32):\n    return (seq[pos:pos + size] for pos in range(0, len(seq), size))","64d96264":"preds_for_sub = np.zeros(submission.shape[0])\nfor i in tqdm_notebook(range(n_val_famillies_list)):\n    file_path = f\"vgg_face_{i}.h5\"\n    model.load_weights(file_path)\n    # Get the predictions\n    predictions = []\n\n    for batch in tqdm_notebook(chunker(submission.img_pair.values)):\n        X1 = [x.split(\"-\")[0] for x in batch]\n        X1 = np.array([read_img(test_path + x) for x in X1])\n\n        X2 = [x.split(\"-\")[1] for x in batch]\n        X2 = np.array([read_img(test_path + x) for x in X2])\n\n        pred = model.predict([X1, X2]).ravel().tolist()\n        predictions += pred\n    preds_for_sub += np.array(predictions) \/ n_val_famillies_list","92605718":"#submission['is_related'] = preds_for_sub\nsubmission.to_csv(\"vgg_face.csv\", index=False)","1ff23c4c":"from IPython.display import HTML\nimport pandas as pd\nimport numpy as np\ndf = pd.DataFrame(submission)\n\ndf.to_csv('submission.csv', index=False)\n\ndef create_download_link(title = \"Download CSV file\", filename = \"data.csv\"):  \n    html = '<a href={filename}>{title}<\/a>'\n    html = html.format(title=title,filename=filename)\n    return HTML(html)\n\n# create a link to download the dataframe which was saved with .to_csv method\ncreate_download_link(filename='submission.csv')","03999524":"## 2. Inference","09a689cf":"## 1. Train"}}