{"cell_type":{"ca77b067":"code","37f13f73":"code","e7f8de4d":"code","9164da55":"code","e77b576c":"code","c0f8447f":"code","e7c20150":"code","b7a45e1a":"code","c9402985":"code","44183a34":"code","23edc02e":"code","20c25188":"code","391b9b20":"code","da21d564":"code","15f62366":"code","f8291063":"code","9a97b631":"code","f575feb7":"code","7cd6f5fe":"code","7f8624d9":"code","35e94ea8":"code","b2f08c31":"code","302d6f9a":"code","24144666":"code","1c6cc489":"code","92c0ebb6":"code","3b8e589f":"code","687df42d":"code","f3ccc76a":"code","db040836":"code","b53bcb33":"code","5bd9f628":"code","d46cfdaa":"markdown","ccdf1a52":"markdown","ae474f43":"markdown","bf734c0a":"markdown","dbecfe98":"markdown","eb356eac":"markdown","53f3faa4":"markdown","2d6d3292":"markdown","e6011386":"markdown","9e454742":"markdown","b2773276":"markdown","9aace910":"markdown","3ec08d29":"markdown","3801a330":"markdown","761751d0":"markdown","fae50b6a":"markdown","ebc32092":"markdown"},"source":{"ca77b067":"import numpy as np\nimport keras\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nfrom scipy.io import loadmat\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelBinarizer\nfrom sklearn.metrics import confusion_matrix\nfrom keras.preprocessing.image import ImageDataGenerator\n%matplotlib inline","37f13f73":"# Set random state\n\nnp.random.seed(20)","e7f8de4d":"# Load the data\n\ntrain_raw = loadmat('..\/input\/svhndataset\/train_32x32.mat')\ntest_raw = loadmat('..\/input\/svhndataset\/test_32x32.mat')","9164da55":"# Load images and labels\n\ntrain_images = np.array(train_raw['X'])\ntest_images = np.array(test_raw['X'])\n\ntrain_labels = train_raw['y']\ntest_labels = test_raw['y']","e77b576c":"# Check the shape of the data\n\nprint(train_images.shape)\nprint(test_images.shape)","c0f8447f":"# Fix the axes of the images\n\ntrain_images = np.moveaxis(train_images, -1, 0)\ntest_images = np.moveaxis(test_images, -1, 0)\n\nprint(train_images.shape)\nprint(test_images.shape)","e7c20150":"# Plot a random image and its label\n\nplt.imshow(train_images[13529])\nplt.show()\n\nprint('Label: ', train_labels[13529])","b7a45e1a":"# Convert train and test images into 'float64' type\n\ntrain_images = train_images.astype('float64')\ntest_images = test_images.astype('float64')","c9402985":"# Convert train and test labels into 'int64' type\n\ntrain_labels = train_labels.astype('int64')\ntest_labels = test_labels.astype('int64')","44183a34":"# Normalize the images data\n\nprint('Min: {}, Max: {}'.format(train_images.min(), train_images.max()))\n\ntrain_images \/= 255.0\ntest_images \/= 255.0","23edc02e":"# One-hot encoding of train and test labels\n\nlb = LabelBinarizer()\ntrain_labels = lb.fit_transform(train_labels)\ntest_labels = lb.fit_transform(test_labels)","20c25188":"# Split train data into train and validation sets\n\nX_train, X_val, y_train, y_val = train_test_split(train_images, train_labels,\n                                                  test_size=0.15, random_state=22)","391b9b20":"y_val.shape","da21d564":"# Data augmentation\n\ndatagen = ImageDataGenerator(rotation_range=8,\n                             zoom_range=[0.95, 1.05],\n                             height_shift_range=0.10,\n                             shear_range=0.15)","15f62366":"# Define auxillary model\n\nkeras.backend.clear_session()\n\naux_model = keras.Sequential([\n    keras.layers.Conv2D(32, (3, 3), padding='same', \n                           activation='relu',\n                           input_shape=(32, 32, 3)),\n    keras.layers.BatchNormalization(),\n    keras.layers.Conv2D(32, (3, 3), padding='same', \n                        activation='relu'),\n    keras.layers.MaxPooling2D((2, 2)),\n    keras.layers.Dropout(0.3),\n    \n    keras.layers.Conv2D(64, (3, 3), padding='same', \n                           activation='relu'),\n    keras.layers.BatchNormalization(),\n    keras.layers.Conv2D(64, (3, 3), padding='same',\n                        activation='relu'),\n    keras.layers.MaxPooling2D((2, 2)),\n    keras.layers.Dropout(0.3),\n    \n    keras.layers.Conv2D(128, (3, 3), padding='same', \n                           activation='relu'),\n    keras.layers.BatchNormalization(),\n    keras.layers.Conv2D(128, (3, 3), padding='same',\n                        activation='relu'),\n    keras.layers.MaxPooling2D((2, 2)),\n    keras.layers.Dropout(0.3),\n    \n    keras.layers.Flatten(),\n    keras.layers.Dense(128, activation='relu'),\n    keras.layers.Dropout(0.4),    \n    keras.layers.Dense(10,  activation='softmax')\n])\n\nlr_schedule = keras.callbacks.LearningRateScheduler(\n              lambda epoch: 1e-4 * 10**(epoch \/ 10))\noptimizer = keras.optimizers.Adam(lr=1e-4, amsgrad=True)\naux_model.compile(optimizer=optimizer,\n                  loss='categorical_crossentropy',\n                 metrics=['accuracy'])","f8291063":"# Fit model in order to determine best learning rate\n\nhistory = aux_model.fit_generator(datagen.flow(X_train, y_train, batch_size=128),\n                              epochs=30, validation_data=(X_val, y_val),\n                              callbacks=[lr_schedule])","9a97b631":"# Plot Learning Rate vs. Loss\n\nplt.semilogx(history.history['lr'], history.history['loss'])\nplt.axis([1e-4, 1e-1, 0, 4])\nplt.xlabel('Learning Rate')\nplt.ylabel('Training Loss')\nplt.show()","f575feb7":"# Define actual model\n\nkeras.backend.clear_session()\n\nmodel = keras.Sequential([\n    keras.layers.Conv2D(32, (3, 3), padding='same', \n                           activation='relu',\n                           input_shape=(32, 32, 3)),\n    keras.layers.BatchNormalization(),\n    keras.layers.Conv2D(32, (3, 3), padding='same', \n                        activation='relu'),\n    keras.layers.MaxPooling2D((2, 2)),\n    keras.layers.Dropout(0.3),\n    \n    keras.layers.Conv2D(64, (3, 3), padding='same', \n                           activation='relu'),\n    keras.layers.BatchNormalization(),\n    keras.layers.Conv2D(64, (3, 3), padding='same',\n                        activation='relu'),\n    keras.layers.MaxPooling2D((2, 2)),\n    keras.layers.Dropout(0.3),\n    \n    keras.layers.Conv2D(128, (3, 3), padding='same', \n                           activation='relu'),\n    keras.layers.BatchNormalization(),\n    keras.layers.Conv2D(128, (3, 3), padding='same',\n                        activation='relu'),\n    keras.layers.MaxPooling2D((2, 2)),\n    keras.layers.Dropout(0.3),\n    \n    keras.layers.Flatten(),\n    keras.layers.Dense(128, activation='relu'),\n    keras.layers.Dropout(0.4),    \n    keras.layers.Dense(10,  activation='softmax')\n])\n\nearly_stopping = keras.callbacks.EarlyStopping(patience=8)\noptimizer = keras.optimizers.Adam(lr=1e-3, amsgrad=True)\nmodel_checkpoint = keras.callbacks.ModelCheckpoint(\n                   '\/kaggle\/working\/best_cnn.h5', \n                   save_best_only=True)\nmodel.compile(optimizer=optimizer,\n              loss='categorical_crossentropy',\n              metrics=['accuracy'])","7cd6f5fe":"model.summary()","7f8624d9":"# Fit model in order to make predictions\n\nhistory = model.fit_generator(datagen.flow(X_train, y_train, batch_size=128),\n                              epochs=70, validation_data=(X_val, y_val),\n                              callbacks=[early_stopping, model_checkpoint])","35e94ea8":"# Evaluate train and validation accuracies and losses\n\ntrain_acc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\n\ntrain_loss = history.history['loss']\nval_loss = history.history['val_loss']","b2f08c31":"# Visualize epochs vs. train and validation accuracies and losses\n\nplt.figure(figsize=(20, 10))\n\nplt.subplot(1, 2, 1)\nplt.plot(train_acc, label='Training Accuracy')\nplt.plot(val_acc, label='Validation Accuracy')\nplt.legend()\nplt.title('Epochs vs. Training and Validation Accuracy')\n    \nplt.subplot(1, 2, 2)\nplt.plot(train_loss, label='Training Loss')\nplt.plot(val_loss, label='Validation Loss')\nplt.legend()\nplt.title('Epochs vs. Training and Validation Loss')\n\nplt.show()","302d6f9a":"# Evaluate model on test data\ntest_loss, test_acc = model.evaluate(x=test_images, y=test_labels, verbose=0)\n\nprint('Test accuracy is: {:0.4f} \\nTest loss is: {:0.4f}'.\n      format(test_acc, test_loss))","24144666":"# Get predictions and apply inverse transformation to the labels\n\ny_pred = model.predict(X_train)\n\ny_pred = lb.inverse_transform(y_pred, lb.classes_)\ny_train = lb.inverse_transform(y_train, lb.classes_)","1c6cc489":"# Plot the confusion matrix\n\nmatrix = confusion_matrix(y_train, y_pred, labels=lb.classes_)\n\nfig, ax = plt.subplots(figsize=(14, 12))\nsns.heatmap(matrix, annot=True, cmap='Greens', fmt='d', ax=ax)\nplt.title('Confusion Matrix for training dataset')\nplt.xlabel('Predicted label')\nplt.ylabel('True label')\nplt.show()","92c0ebb6":"# Ignore the errors in the plots\n\nnp.seterr(all='ignore')","3b8e589f":"# Get convolutional layers\n\nlayers = [model.get_layer('conv2d_1'), \n          model.get_layer('conv2d_2'),\n          model.get_layer('conv2d_3'),\n          model.get_layer('conv2d_4'),\n          model.get_layer('conv2d_5'),\n          model.get_layer('conv2d_6')]","687df42d":"# Define a model which gives the outputs of the layers\n\nlayer_outputs = [layer.output for layer in layers]\nactivation_model = keras.models.Model(inputs=model.input, outputs=layer_outputs)","f3ccc76a":"# Create a list with the names of the layers\n\nlayer_names = []\nfor layer in layers:\n    layer_names.append(layer.name)","db040836":"# Define a function which will plot the convolutional filters\n\ndef plot_convolutional_filters(img):\n    \n    img = np.expand_dims(img, axis=0)\n    activations = activation_model.predict(img)\n    images_per_row = 9\n    \n    for layer_name, layer_activation in zip(layer_names, activations): \n        n_features = layer_activation.shape[-1]\n        size = layer_activation.shape[1]\n        n_cols = n_features \/\/ images_per_row\n        display_grid = np.zeros((size * n_cols, images_per_row * size))\n        for col in range(n_cols): \n            for row in range(images_per_row):\n                channel_image = layer_activation[0,\n                                                 :, :,\n                                                 col * images_per_row + row]\n                channel_image -= channel_image.mean()\n                channel_image \/= channel_image.std()\n                channel_image *= 64\n                channel_image += 128\n                channel_image = np.clip(channel_image, 0, 255).astype('uint8')\n                display_grid[col * size : (col + 1) * size,\n                             row * size : (row + 1) * size] = channel_image\n        scale = 1. \/ size\n        plt.figure(figsize=(scale * display_grid.shape[1],\n                            scale * display_grid.shape[0]))\n        plt.title(layer_name)\n        plt.grid(False)\n        plt.imshow(display_grid, aspect='auto', cmap='plasma')","b53bcb33":"img = X_train[42500]\nplt.imshow(img)\nplt.show()","5bd9f628":"plot_convolutional_filters(img)","d46cfdaa":"After seeing how the algorithm converged, we can now evaluate the model's performance on the test data.","ccdf1a52":"We can see that the loss follows a very specific trajectory: a rapid drop followed by a relatively flat line which shoots back up after a certain point. Thus, it is better to choose a learning rate in the region where the loss is stable; a reasonable choice would be **lr = 0.01** (or 1e-3).","ae474f43":"## 2. Loading and preprocessing","bf734c0a":"In the above, we can see the main idea behind Convolutional Neural Networks, i.e. that as we go deeper into the layers, higher-level features are learned by our models. At first, we can easily understand what is going on but gradually, we find it difficult to keep up with the model's learning process.","dbecfe98":"# Street View House Numbers Classification","eb356eac":"## 4. Visualizations and insights","53f3faa4":"As we can see, the training data consists mostly of 0s, 1s and 2s (in a descending order), while labels '5' up to '9' are underepresented. Furthermore, the confusion matrix can show us the particular problematic cases of our model.","2d6d3292":"## 3. CNN model","e6011386":"The Street View House Numbers (SVHN) dataset is one of the most popular benchmarks for object recognition tasks in academic papers. The images were obtained from house numbers in Google Street View images, are hosted by Stanford University and are very similar in philosophy with the MNIST dataset. However, the original purpose of this dataset is to solve a harder problem: that of recognizing digits and numbers in natural scene images.","9e454742":"## 1. Imports","b2773276":"Not bad at all! This result implies that our error ranges from ~4% to ~4.4% and we should have in mind that the errors reported by state-of-the-art models and methods range from 4.9% to 1.02% ([reference](https:\/\/benchmarks.ai\/svhn)). Of course, there is room for quite a bit of tuning in order to improve performance such as:\n- Change the way the images are transformed in the augmentation process.\n- Change the architecture of our model by adding extra blocks, changing the kernel sizes, making it deeper, etc.\n- Train multiple CNNs and make ensemble predictions.\n- Use some of the extra data which can be found along with the original dataset. ","9aace910":"## 5. Conclusion","3ec08d29":"In order to get more robust results out of our model, we are going to augment the images in the dataset, by randomly rotating them, zooming them in and out, shifting them up and down (**IMPORTANT NOTE:** It is best that we do not shift them horizontally, since there are also distracting digits in the images), shifting their channels and shearing them.","3801a330":"In this kernel, we have trained a Convolutional Neural Network to recognize the digits in the Street View House Numbers dataset (Format 2). In particular, we have performed some minimal preprocessing of the data, we have augmented the data in various ways, we have created an auxillary model in order to find which learning rate we should choose for our optimizer and finally, we have trained the final CNN and evaluated it on the test images data. Furthermore, we have provided two useful visualizations (confusion matrix and feature maps) so as get a sense of how our model actually works and not view it as just a black-box process. Finally, it should be noted that there is quite a bit of room for tuning and different architectures so as to improve the accuracy of the model; nonetheless, our results are pretty good given the simplicity of our approach.\n\nI hope you enjoyed this simple kernel and find it useful, either as a reference for your own projects or as an introduction to the complete process of training CNNs. Also, **PLEASE DO NOT FORGET TO UPVOTE!**","761751d0":"In this section, we can present some nice and useful visualizations which make us understand better how our Convolutional Neural Network actually works. It is my belief that the two following visualizations are the most helpful:\n\n- The **Confusion Matrix** of the model on the training data, so as to get a sense of how it performs on each class label and how the misclassifications are distributed.\n- The **Feature Maps** for a random input image, so as to get a sense of how our model learns the features in each convolutional layer.","fae50b6a":"In order to determine a good learning rate for the optimizer of our model (here, we use the AMSGrad variant of the Adam optimizer), we set a callback in an auxillary model which will gradually increase the learning rate of the optimizer.","ebc32092":"The data of the Street View House Numbers dataset, which can originally be found [here](http:\/\/ufldl.stanford.edu\/housenumbers) are originally in .mat, i.e. files which can be best processed with MATLAB; thus, some preprocessing is required (see section 2). It is important to note that the data are divided into two formats and in this particular kernel we are going to use **Format 2**:\n\n- *Format 1*: The original, variable-resolution colored house-number images with character level bounding boxes.\n- *Format 2*: The cropped digits (32x32 pixels) which follow the philosophy of the MNIST dataset more closely, but also contain some distracting digits to the sides of the digit of interest."}}