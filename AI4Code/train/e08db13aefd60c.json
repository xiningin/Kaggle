{"cell_type":{"c1ff406b":"code","1554d98f":"code","aac11934":"code","7b4093af":"code","3064879c":"code","0a84ad98":"code","387cb5eb":"code","ad76b5a8":"code","85752b5a":"code","cbb015e0":"code","e11b4989":"code","a0467fe5":"code","c59dcbd1":"code","1331d8a7":"code","339acfc0":"code","48390d69":"code","7c6cd994":"code","95365653":"code","6ed4398b":"code","52e1d281":"code","6bc6817b":"code","4d12356f":"code","68b42706":"code","766a159d":"code","50378aaa":"code","8a7823ca":"code","bd1f7ae0":"code","8e4e4520":"code","f445277e":"code","31c35ca1":"code","481fd9c6":"markdown"},"source":{"c1ff406b":"import numpy as np\nimport pandas as pd\n\nfrom pathlib import Path\nfrom typing import *\n\nimport torch\nimport torch.optim as optim\n\nfrom fastai import *\nfrom fastai.vision import *\nfrom fastai.text import *\nfrom fastai.callbacks import *\nfrom pytorch_pretrained_bert import BertTokenizer\nimport warnings\n\nwarnings.filterwarnings(\"ignore\")\n","1554d98f":"class Config(dict):\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n        for k, v in kwargs.items():\n            setattr(self, k, v)\n    \n    def set(self, key, val):\n        self[key] = val\n        setattr(self, key, val)\n","aac11934":"config = Config(\n    testing=False,\n    #bert_model_name=\"bert-base-uncased\",\n    bert_model_name=\"bert-base-multilingual-uncased\",\n    \n    max_lr=3e-5,\n    epochs=2,                   #4,\n    use_fp16=True,\n    bs=64,                      #32,\n    discriminative=False,\n    max_seq_len=192            #256\n)","7b4093af":"class FastAiBertTokenizer(BaseTokenizer):\n    \"\"\"Wrapper around BertTokenizer to be compatible with fast.ai\"\"\"\n    def __init__(self, tokenizer: BertTokenizer, max_seq_len: int=128, **kwargs):\n        self._pretrained_tokenizer = tokenizer\n        self.max_seq_len = max_seq_len\n\n    def __call__(self, *args, **kwargs):\n        return self\n\n    def tokenizer(self, t:str) -> List[str]:\n        \"\"\"Limits the maximum sequence length\"\"\"\n        return [\"[CLS]\"] + self._pretrained_tokenizer.tokenize(t)[:self.max_seq_len - 2] + [\"[SEP]\"]","3064879c":"if config.testing:\n    train = train.head(1024)\n    val = val.head(1024)\n    test = test.head(1024)\n","0a84ad98":"DATA_ROOT = Path(\"..\")\/\"input\"\/ \"jigsaw-multilingual-toxic-comment-classification\/\"\n\ndf1,df2,df3,test,sample = [pd.read_csv(DATA_ROOT \/ fname) for fname in [\"jigsaw-toxic-comment-train.csv\",\n                                                                        \"jigsaw-unintended-bias-train.csv\",\n                                                                        \"validation.csv\",\n                                                                        \"test.csv\",\n                                                                        \"sample_submission.csv\"\n                                                                       ]]\ndf2.toxic = df2.toxic.round().astype(int)\ntrain = pd.concat([\n    df1[['comment_text', 'toxic']],\n    df2[['comment_text', 'toxic']].query('toxic==1'),\n    df2[['comment_text', 'toxic']].query('toxic==0').sample(n=90000, random_state=0)\n])\n\n# rankings_pd.rename(columns = {'test':'TEST', 'odi':'ODI', \n#                               't20':'T20'}, inplace = True) \ntest.rename(columns={\"content\":\"comment_text\"}, inplace = True)\n\nval = df3\n","387cb5eb":"train.head()","ad76b5a8":"test.head()","85752b5a":"val.head()","cbb015e0":"bert_tok = BertTokenizer.from_pretrained(\n    config.bert_model_name,\n)","e11b4989":"fastai_bert_vocab = Vocab(list(bert_tok.vocab.keys()))\nfastai_tokenizer = Tokenizer(tok_func=FastAiBertTokenizer(bert_tok, \n                                                          max_seq_len=config.max_seq_len), pre_rules=[], post_rules=[])","a0467fe5":"databunch = TextDataBunch.from_df(\".\", train, val, test,\n                  tokenizer=fastai_tokenizer,\n                  vocab=fastai_bert_vocab,\n                  include_bos=False,\n                  include_eos=False,\n                  text_cols=\"comment_text\",\n                  label_cols=\"toxic\",\n                  bs=config.bs,\n                  collate_fn=partial(pad_collate, pad_first=False, pad_idx=0),\n             )","c59dcbd1":"databunch.show_batch()","1331d8a7":"# databunch = load_data(path=\"..\/input\/jigsawprocesseddatabunch\/\", file = Path(\"data-jigsaw.pkl\"))","339acfc0":"# databunch.show_batch()","48390d69":"# databunch.device","7c6cd994":"# VERSION = \"20200325\"  #@param [\"1.5\" , \"20200325\", \"nightly\"]\n# !curl https:\/\/raw.githubusercontent.com\/pytorch\/xla\/master\/contrib\/scripts\/env-setup.py -o pytorch-xla-env-setup.py\n# !python pytorch-xla-env-setup.py --version $VERSION","95365653":"# import warnings\n# import torch_xla\n# import torch_xla.debug.metrics as met\n# import torch_xla.distributed.data_parallel as dp\n# import torch_xla.distributed.parallel_loader as pl\n# import torch_xla.utils.utils as xu\n# import torch_xla.core.xla_model as xm\n# import torch_xla.distributed.xla_multiprocessing as xmp\n# import torch_xla.test.test_utils as test_utils","6ed4398b":"# device = xm.xla_device()\n# model = mx.to(device)","52e1d281":"from pytorch_pretrained_bert.modeling import BertConfig, BertForSequenceClassification\nbert_model = BertForSequenceClassification.from_pretrained(config.bert_model_name, num_labels=1)","6bc6817b":"class Loss_fn(nn.BCEWithLogitsLoss):\n  __constants__ = ['weight', 'pos_weight', 'reduction']\n  \n  def __init__(self, weight=None, size_average=None, reduce=None, reduction='mean', pos_weight=None):\n    \n    super().__init__(size_average, reduce, reduction)\n    self.register_buffer('weight', weight)\n    self.register_buffer('pos_weight', pos_weight)\n\n  def forward(self, input, target):\n    # My target is of torch.Size([32])\n    target = target.unsqueeze(1)   # Convert target size  of torch.Size([32, 1])\n    target = target.float()        # BCE loss expects a Tensor of type float\n  \n    return F.binary_cross_entropy_with_logits(input, target,\n                                                  self.weight,\n                                                  pos_weight=self.pos_weight,\n                                                  reduction=self.reduction)","4d12356f":"loss_func = Loss_fn()","68b42706":"from fastai.callbacks import *\n\nlearner = Learner(\n    databunch, bert_model,\n    loss_func=loss_func,\n)\nif config.use_fp16: learner = learner.to_fp16()\n","766a159d":"learner.model_dir = '\/tmp\/'","50378aaa":"learner.lr_find()","8a7823ca":"learner.recorder.plot()","bd1f7ae0":"learner.fit_one_cycle(config.epochs, max_lr=config.max_lr)\n","8e4e4520":"def get_preds_as_nparray(ds_type) -> np.ndarray:\n    \"\"\"\n    the get_preds method does not yield the elements in order by default\n    we borrow the code from the RNNLearner to resort the elements into their correct order\n    \"\"\"\n    preds = learner.get_preds(ds_type)[0].detach().cpu().numpy()\n    sampler = [i for i in databunch.dl(ds_type).sampler]\n    reverse_sampler = np.argsort(sampler)\n    return preds[reverse_sampler, :]","f445277e":"test_preds = get_preds_as_nparray(DatasetType.Test)\n","31c35ca1":"sample_submission = pd.read_csv(\"..\/input\/jigsaw-multilingual-toxic-comment-classification\/sample_submission.csv\")\nif config.testing: sample_submission = sample_submission.head(test.shape[0])\nsample_submission['toxic'] = test_preds\nsample_submission.to_csv(\"predictions.csv\", index=False)","481fd9c6":"**Load the DataBunch**"}}