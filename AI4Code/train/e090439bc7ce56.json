{"cell_type":{"a0944b94":"code","f356b07e":"code","3ce406e7":"code","76638051":"code","810f7e79":"code","92fd389c":"code","80dc2205":"code","a5d3fa5d":"code","1d54ff79":"code","ba2b5e83":"code","42ffa23a":"code","f2d0ab1b":"code","b8c37474":"code","1a5e9314":"code","a032a251":"code","c940cf24":"code","7aad2baf":"code","465d6181":"code","484c88c3":"code","64e82840":"code","275c6ff7":"code","32a2deee":"code","0ff41537":"code","1d2b4131":"code","10ec34e5":"code","613c1641":"code","04bd53b0":"markdown","057e3281":"markdown","7f7b0dc4":"markdown","c90cef9d":"markdown","bb1f3b2c":"markdown","e5a288d0":"markdown","1e3b9703":"markdown","c824ced6":"markdown","68a3c9ac":"markdown","c2b8f00c":"markdown","43a9f424":"markdown","e5a2dc3d":"markdown","a8af368c":"markdown","91567c9f":"markdown","b47c4d7a":"markdown","2ef3cc62":"markdown","53aefdb2":"markdown","3a0f944f":"markdown","9cea89b4":"markdown","a49c4476":"markdown","cd0136a8":"markdown"},"source":{"a0944b94":"import numpy as np\nimport pandas as pd\nimport seaborn as sb\nimport matplotlib.pyplot as plt\nimport string\nimport nltk\nfrom nltk.sentiment import SentimentIntensityAnalyzer\nimport re\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional, SpatialDropout1D\nfrom sklearn.model_selection import train_test_split","f356b07e":"df = pd.DataFrame(pd.read_csv('\/kaggle\/input\/restaurant-reviews-in-dhaka-bangladesh\/reviews.csv'))\npd.set_option('display.max_colwidth',150)\ndf.head()","3ce406e7":"df.isnull().sum()","76638051":"df = df.drop('Review', axis = 1)\ndf = df.drop('Recommends', axis = 1)","810f7e79":"def lower_case(txt):\n    return txt.lower()\n\ndef remove_punctuation(txt):\n    txt_clean = \"\".join([c for c in txt if c not in string.punctuation])\n    return txt_clean\n\ndef remove_non_ascii_chars(txt):\n    txt_fullclean = \"\".join(i for i in txt if ord(i)<128)\n    return txt_fullclean","92fd389c":"df['lower_case_review'] = df['Review Text'].apply(lambda x: lower_case(x))\ndf['punctuation_free_review'] = df['lower_case_review'].apply(lambda x: remove_punctuation(x))\ndf['clean_review'] = df['punctuation_free_review'].apply(lambda x: remove_non_ascii_chars(x))","80dc2205":"df = df[['clean_review']]","a5d3fa5d":"s_i_a = SentimentIntensityAnalyzer()\n\nprint(df['clean_review'][50])\ns_i_a.polarity_scores(df['clean_review'].iloc[50])","1d54ff79":"df['polarity_score'] = df['clean_review'].apply(lambda x: s_i_a.polarity_scores(x))","ba2b5e83":"df['compound_score'] = df['polarity_score'].apply(lambda x: x['compound'])","42ffa23a":"df['sentiment_tag'] = df['compound_score'].apply(lambda x: 'positive' if x>0.20 else 'negative')","f2d0ab1b":"df['sentiment_tag'].value_counts()","b8c37474":"sb.set(rc={'figure.figsize':(7.5,4.27)})\n\nsb.countplot(x = 'sentiment_tag', data = df)","1a5e9314":"df['labels'] = df['sentiment_tag'].apply(lambda x: 0 if x == 'negative' else 1)","a032a251":"df = df.drop('polarity_score', axis = 1)\ndf = df.drop('compound_score', axis = 1)\ndf = df.drop('sentiment_tag', axis = 1)","c940cf24":"(X, y) = (df['clean_review'].values, df['labels'].values)","7aad2baf":"print(\"Type of X: \", type(X), \"\\nType of y: \",type(y))\nprint(\"\\nShape of X: \",X.shape, \"\\nShape of y: \", y.shape)\nprint(\"\\nExample:\", X[0], '=', y[0])","465d6181":"max_length = 100\nembedding_dim = 50\ntrunc_type='post'\npadding_type='post'\noov_token = \"<OOV>\"","484c88c3":"tokenizer = Tokenizer(oov_token = oov_token)\ntokenizer.fit_on_texts(X)\n\nword_index = tokenizer.word_index\nvocab_size = len(word_index)\n\nX_seq = tokenizer.texts_to_sequences(X)\nX_padded = pad_sequences(X_seq, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n\nprint('Vocab Size = ', vocab_size)\nprint(X_padded[0])","64e82840":"train_padded, test_padded, train_label, test_label = train_test_split(X_padded, y, test_size = 0.15, random_state = 10)","275c6ff7":"train_padded = np.array(train_padded)\ntest_padded = np.array(test_padded)\ntrain_label = np.array(train_label)\ntest_label = np.array(test_label)\n\nprint(train_padded.shape, test_padded.shape, train_label.shape, test_label.shape)","32a2deee":"embeddings_index = dict()\nf = open('..\/input\/glove6b50dtxt\/glove.6B.50d.txt')\nfor line in f:\n    values = line.split()\n    word = values[0]\n    coefs = np.asarray(values[1:], dtype='float32')\n    embeddings_index[word] = coefs\nf.close()","0ff41537":"embedding_matrix = np.zeros((vocab_size, 50))\nfor word, index in tokenizer.word_index.items():\n    if index > vocab_size - 1:\n        break\n    else:\n        embedding_vector = embeddings_index.get(word)\n        if embedding_vector is not None:\n            embedding_matrix[index] = embedding_vector","1d2b4131":"model = Sequential()\nmodel.add(Embedding(vocab_size, embedding_dim, input_length = max_length, weights = [embedding_matrix], trainable = False))\nmodel.add(SpatialDropout1D(0.4))\nmodel.add(LSTM(128, return_sequences = True, input_shape = train_padded.shape))\nmodel.add(LSTM(64, dropout = 0.2, recurrent_dropout = 0.2))\nmodel.add(Dense(1, activation = 'sigmoid'))\n\nmodel.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n\nprint(model.summary())","10ec34e5":"num_epochs = 5\n\nhistory = model.fit(train_padded, train_label, epochs=num_epochs, validation_data=(test_padded, test_label), verbose=2)","613c1641":"import matplotlib.pyplot as plt\n\n\nacc=history.history['accuracy']\nval_acc=history.history['val_accuracy']\nloss=history.history['loss']\nval_loss=history.history['val_loss']\n\nepochs=range(len(acc))\n\n\nplt.plot(epochs, acc, 'r')\nplt.plot(epochs, val_acc, 'b')\nplt.title('Training and validation accuracy')\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Accuracy\")\nplt.legend([\"Accuracy\", \"Validation Accuracy\"])\n\nplt.figure()\n\n\nplt.plot(epochs, loss, 'r')\nplt.plot(epochs, val_loss, 'b')\nplt.title('Training and validation loss')\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Loss\")\nplt.legend([\"Loss\", \"Validation Loss\"])\n\nplt.figure()","04bd53b0":"*The compound score is a better indicator of positive and negative sentiment in this case. So I am going to store it in seperate column.*","057e3281":"*Let's import necessary packages and libraries here.*","7f7b0dc4":"*Here I am choosing 0.20 as the threshold value for compound score.*","c90cef9d":"The following functions are for:\n* coverting uppercase to lowercase\n* removing punctuation marks\n* removing non-ascii characters such as emojis, special characters etc.","bb1f3b2c":"*Coverting the reviews and the labels to numpy arrays for further preprocessing.*","e5a288d0":"*Spliting the tokenized values into train and test sets:*","1e3b9703":"*Let's manually label the existing two tags.*","c824ced6":"# Visualizing performance","68a3c9ac":"**The dataset is asymmetric as the number of positive reviews are more than two times of negative reviews.**","c2b8f00c":"This notebook is mainly a practice workbook by me, a machine learning novice. Kindly inform me if you find any errors\/mistakes here.","43a9f424":"*Now let's check the SentimentIntensityAnalyzer tool from nltk and test it on a single review.*","e5a2dc3d":"*As there are too many null values I am dropping those two columns.*","a8af368c":"*As the 'clean_review' column has the most cleaned text I am keeping this column only in the same dataframe.*","91567c9f":"# Tokenization & Padding","b47c4d7a":"*Applying each function at a time and storing the new texts in seperate columns*","2ef3cc62":"*Reading the CSV file and placing it in a dataframe for ease of use.*","53aefdb2":"*The result looks decent.*\n*let's apply it on all the review texts.*","3a0f944f":" # Classification of Restaurant Reviews using two-layers LSTM with GloVe Embedding.","9cea89b4":"# Training Model","a49c4476":"*Declaring all the necessary variables for tokenization:*","cd0136a8":"*Let's convert these values into numpy arrays to feed into neural network.*"}}