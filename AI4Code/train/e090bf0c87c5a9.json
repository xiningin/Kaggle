{"cell_type":{"a0dc6323":"code","8f4ead20":"code","6fd87e5b":"code","07ede276":"code","1661c42f":"code","7b0aa8e1":"code","d5e9166c":"code","913610b0":"code","9ae46669":"code","3c58d7f7":"code","18fec564":"code","edb08727":"code","05ca1451":"code","b6ac264d":"code","995de495":"code","240ae1c7":"code","ea9089b3":"code","776ba781":"code","8d3f20a3":"code","171517a0":"code","4d5c27c9":"code","dcf20b31":"code","45ddd4ab":"code","6d6275fa":"markdown","1fd72aef":"markdown","cb6ab24c":"markdown","93f49f22":"markdown","ca27f9d0":"markdown","4055ef32":"markdown","20b1a43c":"markdown","9efb394d":"markdown","5f1e1f16":"markdown","195bde99":"markdown","cca515dd":"markdown","d8c5e9df":"markdown","a28aecc8":"markdown","29c75dc7":"markdown","c5a4b937":"markdown"},"source":{"a0dc6323":"import pandas as pd \nimport numpy as np \nimport matplotlib.pyplot as plt \nimport seaborn as sns \nimport warnings \nimport gc\n#warnings.filterwarnings('ignore')\n\nfrom sklearn import model_selection\nfrom sklearn import linear_model ,metrics\n%matplotlib inline ","8f4ead20":"#Clear memory from previous run if any \ngc.collect()","6fd87e5b":"import zipfile\nwith zipfile.ZipFile('\/kaggle\/input\/homesite-quote-conversion\/train.csv.zip', 'r') as zip_ref:\n    zip_ref.extractall('.\/')","07ede276":"df=pd.read_csv('.\/train.csv')","1661c42f":"df.head()","7b0aa8e1":"df.shape","d5e9166c":"color=sns.color_palette()[3]\nsns.countplot(x=df['QuoteConversion_Flag'],color=color);\n\n## Inbalanced dataset \n## Something to see :\n## Undersampling\n## Artificial data generation \n## Weights \n","913610b0":"from sklearn import feature_selection\nfrom sklearn.feature_selection import mutual_info_classif\n\nX = df.copy()\ny = X.pop('QuoteConversion_Flag')\n\n# Dropping null columns and columns having only 1 values \nX.drop(columns=['PersonalField7', 'PersonalField84', 'PropertyField3', 'PropertyField4',\n                'PropertyField29', 'PropertyField32', 'PropertyField34',\n                'PropertyField36', 'PropertyField38',\n                'GeographicField10A','PropertyField6'], axis=1,inplace=True)\n\n# Label encoding for categoricals\nfor colname in X.select_dtypes(\"object\"):\n    X[colname], _ = X[colname].factorize()\n\n# Selecting 50 top features using Mutual_info_classif\nkbest_mutual=feature_selection.SelectKBest(score_func=mutual_info_classif,k=50)\nkbest_mutual.fit_transform(X,y)\n\n\ncols = kbest_mutual.get_support(indices=True)\nX_mutual=X.iloc[:,cols]","9ae46669":"from sklearn import ensemble \n\nestimator = ensemble.ExtraTreesClassifier()\nselector = feature_selection.RFE(estimator, n_features_to_select=50, step=10)\n\nselector.fit(X, y)\n\nX_RFE=X.iloc[:,selector.get_support()]\n\n#print(\"Selected Features: %s\" % (X.columns[selector.get_support()]))\nprint(\"Feature Ranking: %s\" % (selector.ranking_))","3c58d7f7":"# Exporting the selected features into dataset for future use \nX_RFE=pd.concat([X_RFE,y],axis=1)\nX_mutual=pd.concat([X_mutual,y],axis=1)\n\nX_RFE.to_csv('.\/RFE_features.csv',index=False,header=True)\nX_mutual.to_csv('.\/mutual_info_features.csv',index=False,header=True)","18fec564":"#Import columns from dataset\n\n#df=pd.read_csv('..\/input\/insurancefeatures-homesite\/train.csv')\n\nRFE_columns=pd.read_csv('..\/input\/insurancefeatures-homesite\/RFE_features.csv').columns\n\n","edb08727":"# Taking just the columns which have been found useful via RFE analyis from the main dataset  \n\ndf_RFE=df.loc[:,RFE_columns]\n\n## Checking the various columns which remain in our dataset \n\nfor col in df_RFE.columns:\n    print(col,\"       :   \",df_RFE[col].nunique())\n    ","05ca1451":"## Visulaize the features \n\ncolumns=[col for col in df_RFE.columns if col not in ['SalesField8','Original_Quote_Date']]\nplt.figure(figsize=(30,30))\nfor i in range(30):\n    plt.subplot(6,5,i+1)\n    sns.countplot(x=df_RFE[columns[i]],color=color);\n    plt.title(columns[i]);\nplt.tight_layout()\n\ndel columns \ngc.collect()\n#most columns look catergorical ","b6ac264d":"#Create Folds\n\ndf_RFE=df_RFE.reset_index(drop=True)\n\ndf_RFE['fold']=-1\nkf=model_selection.StratifiedKFold(n_splits=5)\nfor fold ,(train_idx,val_id) in enumerate(kf.split(X=df_RFE,y=df_RFE['QuoteConversion_Flag'])):\n    df_RFE.loc[val_id,'fold']=fold\n","995de495":"#Preprocess data \n# converting it into one hot vectors , dropping the dummy variable and also the original column\n\ndf_RFE.drop(columns=['Original_Quote_Date','SalesField8'],axis=1,inplace=True)\n\ncolumns=[col for col in df_RFE.columns if col not in ['fold','QuoteConversion_Flag']]\n\nfor column in columns:\n    df_RFE=pd.concat([df_RFE,pd.get_dummies(df_RFE[column],prefix=column,drop_first=True)],axis=1)\n    df_RFE.drop(columns=column,axis=1,inplace=True)","240ae1c7":"## Lets fit a logistic classification model which can also server as our baseline \n\nval_ROC_score=[]\ntrain_ROC_score=[]\n\nfor fold in np.arange(5):\n    \n    \n    print(f' fold {fold}')\n    \n    df_train=df_RFE[df_RFE['fold']!=fold]\n    df_val=df_RFE[df_RFE['fold']==fold]\n    \n    lr=linear_model.LogisticRegression(max_iter=100,n_jobs=-1, random_state=42,\n                                      class_weight='balanced')\n    \n    X_train=df_train.drop(columns=['fold','QuoteConversion_Flag'],axis=1)\n    y_train=df_train['QuoteConversion_Flag']\n    \n    X_val=df_val.drop(axis=1,columns=['fold','QuoteConversion_Flag'])\n    y_val=df_val['QuoteConversion_Flag']\n    \n    \n    lr.fit(X_train,y_train)\n    \n    y_pred_train=lr.predict_proba(X_train)\n    y_pred=lr.predict_proba(X_val)\n    \n    \n    val_ROC_score.append(metrics.roc_auc_score(y_val,y_pred[:,1].reshape(-1,1)))\n    train_ROC_score.append(metrics.roc_auc_score(y_train,y_pred_train[:,1].reshape(-1,1)))\n    \n    print(f'Completed fold {fold}')\n    \n    \nprint(f'Mean_ROC_Score using Logistic Regression is: {np.mean(val_ROC_score)}')\n\n","ea9089b3":"## Prepare the dataset \n\n\n## Label Encode the columns \n\nfrom sklearn import preprocessing\n\nfor column in df_RFE.columns:\n    lb=preprocessing.LabelEncoder()\n    if column not in 'QuoteConversion_Flag':\n        df_RFE.loc[:,column]=lb.fit_transform(df_RFE[column].values)","776ba781":"## Try baseline Light GBM with the above features \n\nimport lightgbm as lgb\nimport logging\nimport optuna\nimport sys\n\n# 1. Define an objective function to be maximized.\ndef objective(trial):\n    \n    \n    data, target = df_RFE.drop('QuoteConversion_Flag',axis=1),df_RFE['QuoteConversion_Flag']\n    train_x, valid_x, train_y, valid_y =model_selection.train_test_split(data, target, test_size=0.15,stratify=target)\n    dtrain = lgb.Dataset(train_x, label=train_y)\n    dvalid=lgb.Dataset(valid_x,label=valid_y)\n\n    # 2. Suggest values of the hyperparameters using a trial object.\n    param = {\n        'objective': 'binary',\n        'metric': 'auc',\n        'device_type':'cpu',\n        'seed':42,\n        'verbosity':-1,\n        'boosting_type': trial.suggest_categorical('boosting_type',['gbdt','rf']),\n        'lambda_l1': trial.suggest_loguniform('lambda_l1', 1e-8, 10.0),\n        'lambda_l2': trial.suggest_loguniform('lambda_l2', 1e-8, 10.0),\n        'num_leaves': trial.suggest_int('num_leaves', 2, 256),\n        'feature_fraction': trial.suggest_uniform('feature_fraction', 0.4, 1.0),\n        'bagging_fraction': trial.suggest_uniform('bagging_fraction', 0.4, 1.0),\n        'bagging_freq': trial.suggest_int('bagging_freq', 1, 7),\n        'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n        'max_bin':trial.suggest_int('max_bin',2,26,step=2),\n        'learning_rate':trial.suggest_float('lr',0.1,1,log=True),\n        'num_iterations':200\n    }\n    \n    pruning_callback = optuna.integration.LightGBMPruningCallback(trial, \"auc\",'valid')\n\n    gbm=lgb.train(param, dtrain,valid_sets=[dvalid],valid_names=['valid'],callbacks=[pruning_callback])\n    \n    auc=gbm.best_score['valid']['auc']\n    \n    return auc\n\n# 3. Create a study object and optimize the objective function.\n\noptuna.logging.get_logger(\"optuna\").addHandler(logging.StreamHandler(sys.stdout))\noptuna.logging.set_verbosity(optuna.logging.WARNING)\n\nstudy = optuna.create_study(direction='maximize',\n                            pruner=optuna.pruners.MedianPruner(n_warmup_steps=10),\n                            sampler=optuna.samplers.TPESampler(),\n                            study_name='Light GBM optimization')\n\nstudy.optimize(objective, n_trials=120)","8d3f20a3":"print(\"Best trial:\")\ntrial=study.best_trial\n\nprint(\"  Value: {}\".format(trial.value))\n\nprint(\"  Params: \")\nfor key, value in trial.params.items():\n    print(\"    {}: {}\".format(key, value))","171517a0":"### Using Dataset generated from mutual_info_features \n\n#Import columns from dataset\n\n#df=pd.read_csv('..\/input\/insurancefeatures-homesite\/train.csv')\n\ndf_mutual=pd.read_csv('..\/input\/insurancefeatures-homesite\/mutual_info_features.csv').columns\n\n# Taking just the columns which have been found useful via RFE analyis from the main dataset  \n\ndf_mutual=df.loc[:,df_mutual]\n\n## Checking the various columns which remain in our dataset \n\nfor col in df_mutual.columns:\n    print(col,\"       :   \",df_mutual[col].nunique())","4d5c27c9":"## Prepare the dataset \n\n## Label Encode the columns \n\nfrom sklearn import preprocessing\n\nfor column in df_mutual.columns:\n    lb=preprocessing.LabelEncoder()\n    if column not in 'QuoteConversion_Flag':\n        df_mutual.loc[:,column]=lb.fit_transform(df_mutual[column].values)","dcf20b31":"## Try baseline Light GBM with the above features \n\nimport lightgbm as lgb\nimport logging\nimport optuna\nimport sys\n\n# 1. Define an objective function to be maximized.\ndef objective(trial):\n    \n    \n    data, target = df_mutual.drop('QuoteConversion_Flag',axis=1),df_mutual['QuoteConversion_Flag']\n    train_x, valid_x, train_y, valid_y =model_selection.train_test_split(data, target, test_size=0.15,stratify=target)\n    dtrain = lgb.Dataset(train_x, label=train_y)\n    dvalid=lgb.Dataset(valid_x,label=valid_y)\n\n    # 2. Suggest values of the hyperparameters using a trial object.\n    param = {\n        'objective': 'binary',\n        'metric': 'auc',\n        'device_type':'cpu',\n        'seed':42,\n        'verbosity':-1,\n        'boosting_type': trial.suggest_categorical('boosting_type',['gbdt','rf']),\n        'lambda_l1': trial.suggest_loguniform('lambda_l1', 1e-8, 10.0),\n        'lambda_l2': trial.suggest_loguniform('lambda_l2', 1e-8, 10.0),\n        'num_leaves': trial.suggest_int('num_leaves', 2, 256),\n        'feature_fraction': trial.suggest_uniform('feature_fraction', 0.4, 1.0),\n        'bagging_fraction': trial.suggest_uniform('bagging_fraction', 0.4, 1.0),\n        'bagging_freq': trial.suggest_int('bagging_freq', 1, 7),\n        'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n        'max_bin':trial.suggest_int('max_bin',2,26,step=2),\n        'learning_rate':trial.suggest_float('lr',0.1,1,log=True),\n        'num_iterations':200\n    }\n    \n    pruning_callback = optuna.integration.LightGBMPruningCallback(trial, \"auc\",'valid')\n\n    gbm=lgb.train(param, dtrain,valid_sets=[dvalid],valid_names=['valid'],callbacks=[pruning_callback])\n    \n    auc=gbm.best_score['valid']['auc']\n    \n    return auc\n\n# 3. Create a study object and optimize the objective function.\n\noptuna.logging.get_logger(\"optuna\").addHandler(logging.StreamHandler(sys.stdout))\noptuna.logging.set_verbosity(optuna.logging.WARNING)\n\nstudy = optuna.create_study(direction='maximize',\n                            pruner=optuna.pruners.MedianPruner(n_warmup_steps=10),\n                            sampler=optuna.samplers.TPESampler(),\n                            study_name='Light GBM optimization')\n\nstudy.optimize(objective, n_trials=120)\n","45ddd4ab":"print(\"Best trial:\")\ntrial=study.best_trial\n\nprint(\"  Value: {}\".format(trial.value))\n\nprint(\"  Params: \")\nfor key, value in trial.params.items():\n    print(\"    {}: {}\".format(key, value))","6d6275fa":"### Selecting features using RFE ","1fd72aef":"### Display the best model's score and hyper-parameters ","cb6ab24c":"Some features look obvious candidates to drop since they have just one value so thay can be dropped ","93f49f22":"# Home Site Quite Conversion Challenge \n\nBefore asking someone on a date or skydiving, it's important to know your likelihood of success. The same goes for quoting home insurance prices to a potential customer. Homesite, a leading provider of homeowners insurance, does not currently have a dynamic conversion rate model that can give them confidence a quoted price will lead to a purchase. \n\nUsing an anonymized database of information on customer and sales activity, including property and coverage information, Homesite is challenging you to predict which customers will purchase a given quote. Accurately predicting conversion would help Homesite better understand the impact of proposed pricing changes and maintain an ideal portfolio of customer segments. \n\n## Main Challenges \n\nThis dataset was huge ~260K rows( aka samples) and 298 (features) and to add to that challenge the data was anonymized so \ndoing feature engineering would be very random and usually brute force . I though of handeling this via feature selection and boosting methodology \n\n__I implemented two feature selection stratergies__ \n\n- __Mutual information:__\nMutual information (MI) between two random variables is a non-negative value, which measures the dependency between the variables. It is equal to zero if and only if two random variables are independent, and higher values mean higher dependency.\n\n- __Reculsive Feature Elimination:__\nGiven an external estimator that assigns weights to features (e.g., the coefficients of a linear model), the goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through any specific attribute or callable. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.\n\nAfter inspecting and performing EDA on the selected features I decided to treat all featues as catergorical. \n\nOnce I have the feature selected to 50 from 298 I triend two model one simple __Logistic regression__ with one-hot encoding and other __LightGBM__ . With logistic regression I Was able to get the ROC-AUC score to 0.95 but the model took a long time to train due to large number of one-hot encoding \n\nI hyper-parameter tuned two Light GBM model with __Optuna__. Optuna is a hyperparameter framework . One feature which I like about it is that it allows us to stop the run for un-promising combination of values . This allows us to run hyper-parameter search for a larger grid.  \n\nFirst model was trained on features obtained using mutual information which gave the ROC-AUC score as 0.93 and the second model was trained with features obtained from RFE which gave me a ROC-AUC score of 0.96+  For the final private test submission I was able to get a score of 0.9627 on the private leader board. \n\nFinally I used Sklearn Pipeline to optimize the prediction workflow for the test set. This allowed me to skip storing all the feature encoding values for 50 feature columns. \n\n## Key Learning \n\n- Feature Selection Techniques \n- Sklearn Pipeline \n\n## Part2 Notebook: Final Implementation \nhttps:\/\/www.kaggle.com\/sumeetsawant\/insurance-quote-xgboost-and-pipeline-auc-0-9627 \n\n\n## Upvote if you like the work \nLinkedIn: https:\/\/www.linkedin.com\/in\/sawantsumeet\/","ca27f9d0":"### Light GBM with Mutual information features and Tuned with Optuna \nso here i select the feature which I got via mutual information","4055ef32":"### Logistic Regression","20b1a43c":"### Trying Various Models ","9efb394d":"### Tune the model ","5f1e1f16":"### Display the best model's score and hyper-parameters ","195bde99":"### Tune the model ","cca515dd":"columns=df.columns.to_list()\n\nplt.figure(figsize=(30,30))\nfor i in range(50):\n    sample=np.random.choice(columns);\n    if(sample not in feat):\n        feat.append(sample)\n    else:\n        while(sample in feat):\n            sample=np.random.choice(columns);\n        \n       \n    plt.subplot(10,5,i+1)\n    sns.countplot(x=df[sample],color=color);\n    plt.title(sample);\nplt.tight_layout()","d8c5e9df":"### Implemention models after feature selections ","a28aecc8":"## Feature Selection articles \nhttps:\/\/towardsdatascience.com\/feature-selection-techniques-in-machine-learning-with-python-f24e7da3f36e\nhttps:\/\/www.analyticsvidhya.com\/blog\/2020\/10\/feature-selection-techniques-in-machine-learning\/\nhttps:\/\/machinelearningmastery.com\/feature-selection-with-real-and-categorical-data\/\n    \n## Usesful functions \nhttps:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.feature_selection.SelectKBest.html\nhttps:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.feature_selection.RFE.html\n\n## Model \nhttps:\/\/towardsdatascience.com\/catboost-vs-light-gbm-vs-xgboost-5f93620723db#:~:text=Unlike%20CatBoost%20or%20LGBM%2C%20XGBoost,supplying%20categorical%20data%20to%20XGBoost\nhttps:\/\/medium.com\/sfu-cspmp\/xgboost-a-deep-dive-into-boosting-f06c9c41349 \n\n## Optuna\nhttps:\/\/neptune.ai\/blog\/optuna-vs-hyperopt","29c75dc7":"### Feature Selection using univariate analysis \n\n- Filter methods  : Using univariate analysis (Mutual information) \n- Wrapper methods : Using Recursive Feature Elimination ","c5a4b937":"### Light GBM with Reculsive Feature Elimination and Tuned with Optuna "}}