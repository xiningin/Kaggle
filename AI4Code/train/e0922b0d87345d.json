{"cell_type":{"838dfee6":"code","5d90218e":"code","e6d48055":"code","8d4c8fc7":"code","154696d8":"code","4ff124e2":"code","72dc28b5":"code","9b5ba9bd":"code","5de8be92":"code","c823fef6":"code","2f8d0173":"code","99ecac29":"code","f92b10e5":"code","c45604ed":"code","16d0c3c8":"code","ecbcb16c":"code","2e667565":"code","83687473":"code","34f3da12":"code","497ce29d":"markdown","c9704489":"markdown","36b9aeda":"markdown"},"source":{"838dfee6":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","5d90218e":"df = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\nprint(df.head())\nprint('Data frame shape',df.shape)\n\nprint(df.columns.values)","e6d48055":"# summarize the dataset\nprint(df.describe())\n","8d4c8fc7":"# summarize the number of unique values in each column\nprint(df.nunique())","154696d8":"# rows with missing values\nprint(df.isnull().sum())","4ff124e2":"# delete unnessesary columns\ndel df['PassengerId']\ndel df['Embarked']\ndel df['Name']\ndel df['Ticket']\n\nprint(df.head())","72dc28b5":"# check if Fare is 0\nprint(df[df['Fare'] < 1])","9b5ba9bd":"from sklearn.preprocessing import OrdinalEncoder\n\n# -> Turning cabin number into Deck\ndf['Deck'] = df['Cabin'].str[:1]\n\ndf['Deck']  = df['Deck'].fillna(0)\n\ndf['Deck'] = df['Deck'].astype('str')\n# define ordinal encoding\nencoder = OrdinalEncoder()\n# transform data\ndf[['Deck']] = encoder.fit_transform(df[['Deck']])\nprint(encoder.categories_)\nprint(df['Deck'].unique())\n\n# replace '0' values with 'nan'\ndf[['Deck']] = df[['Deck']].replace(0, np.NaN)\nprint(df.head())\n\n","5de8be92":"from sklearn.preprocessing import OrdinalEncoder\n\n# define ordinal encoding\nencoder = OrdinalEncoder()\n# transform data\ndf[['Sex']] = encoder.fit_transform(df[['Sex']])\n\n\n# replace '0' values with 'nan'\ndf[['Fare']] = df[['Fare']].replace(0, np.NaN)\n\ndf['Fam'] =  df['SibSp'] + df['Parch']\n\n\nprint(df.head())\n","c823fef6":"del df['Cabin']\n\ndel df['SibSp'] \n\ndel df['Parch']\n\ndel df['Deck']\n# rows with missing values\nprint(df.isnull().sum())","2f8d0173":"from sklearn.metrics import accuracy_score,roc_auc_score\nimport lightgbm as lgb\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer\nfrom imblearn.over_sampling import SMOTE\n\n\n# split into input and output elements\ndata = df.values\nix = [i for i in range(data.shape[1]) if i != 0]\nX, y = data[:, ix], data[:, 0]\n\n# fill nan values\niim = IterativeImputer(imputation_order='random')\nX_imp = iim.fit_transform(X)\n\n# ise imblearn\nsmote = SMOTE(random_state=42)\nX_smote, y_smote = smote.fit_resample(X_imp, y)\n\nX_train, X_valid, y_train, y_valid = train_test_split(X_smote, y_smote, test_size=0.2, random_state=1234)\n\n\n# lightGMB hyperparameter tunning\nlearning_rate = [0.05,0.1,0.2,0.3,0.4,0.5,0.6]\nmax_depth = [2,5,10,15,25]\nnum_leaves = [10,30,60]\nboosting = ['gbdt','rf','dart','goss']\n\nresu=0.5\nfor lr in learning_rate:\n    for md in max_depth:\n        for nl in num_leaves:\n            for bb in boosting:\n                try:\n                    nrounds = 5\n                    lgb_data = lgb.Dataset(X_train, y_train)\n                    # specify your configurations as a dict\n                    params = {'learning_rate':lr,'max_depth':md,'num_leaves':nl,'boosting':bb,'objective': 'binary','feature_fraction': 1,'bagging_fraction': 1, 'verbose': -1}\n                    gmb = lgb.train(params, lgb_data, num_boost_round=nrounds)\n                    # predict the results\n                    y_fitted=gmb.predict(X_valid)\n                    acc = roc_auc_score(y_valid, y_fitted)\n                    if acc > resu:\n                        resu=acc\n                        print('>learning rate=%s max depth=%s num leaves=%s boosting=%s resu=%.3f' % (lr,md,nl,bb,resu))\n                except:\n                    pass\nprint('~~~~~~~')","99ecac29":"from sklearn.metrics import accuracy_score,roc_auc_score\nimport lightgbm as lgb\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer\nfrom imblearn.over_sampling import SMOTE\n\n\n# split into input and output elements\ndata = df.values\nix = [i for i in range(data.shape[1]) if i != 0]\nX, y = data[:, ix], data[:, 0]\n\n# fill nan values\niim = IterativeImputer(imputation_order='random')\nX_imp = iim.fit_transform(X)\n\n# ise imblearn\nsmote = SMOTE(random_state=42)\nX_smote, y_smote = smote.fit_resample(X_imp, y)\n\nX_train, X_valid, y_train, y_valid = train_test_split(X_smote, y_smote, test_size=0.2, random_state=1234)\n\n\n# lightGMB hyperparameter tunning\nlearning_rate = [0.55,0.57,0.59,0.6,0.62,0.64,0.66]\nmax_depth = [10,11,12,13,14,15,16,17,18,19,20]\nnum_leaves = [40,45,50,55,60,65,70]\nboosting = ['gbdt','rf','dart','goss']\n\nresu=0.5\nfor lr in learning_rate:\n    for md in max_depth:\n        for nl in num_leaves:\n            for bb in boosting:\n                try:\n                    nrounds = 5\n                    lgb_data = lgb.Dataset(X_train, y_train)\n                    # specify your configurations as a dict\n                    params = {'learning_rate':lr,'max_depth':md,'num_leaves':nl,'boosting':bb,'objective': 'binary','feature_fraction': 1,'bagging_fraction': 1, 'verbose': -1}\n                    gmb = lgb.train(params, lgb_data, num_boost_round=nrounds)\n                    # predict the results\n                    y_fitted=gmb.predict(X_valid)\n                    acc = roc_auc_score(y_valid, y_fitted)\n                    if acc > resu:\n                        resu=acc\n                        print('>learning rate=%s max depth=%s num leaves=%s boosting=%s resu=%.3f' % (lr,md,nl,bb,resu))\n                except:\n                    pass\nprint('~~~~~~~')","f92b10e5":"from sklearn.metrics import accuracy_score,roc_auc_score\nimport lightgbm as lgb\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer\nfrom sklearn.preprocessing import PowerTransformer\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.metrics import plot_confusion_matrix\n\n\n# split into input and output elements\ndata = df.values\nix = [i for i in range(data.shape[1]) if i != 0]\nX, y = data[:, ix], data[:, 0]\n\n# fill nan values\niim = IterativeImputer(imputation_order='random')\nX_imp = iim.fit_transform(X)\n\n# ise imblearn\nsmote = SMOTE(random_state=42)\nX_smote, y_smote = smote.fit_resample(X_imp, y)\n\nX_train, X_valid, y_train, y_valid = train_test_split(X_smote, y_smote, test_size=0.2, random_state=1234)\n\n# lightGMB hyperparameters\nlr = 0.55\nmd = 12\nnl = 40\nbb = 'gbdt'\n# fit the data\nnrounds = 5\nlgb_data = lgb.Dataset(X_train, y_train)\n# specify your configurations as a dict\nparams = {'learning_rate':lr,'max_depth':md,'num_leaves':nl,'boosting':bb,'objective': 'binary','feature_fraction': 1,'bagging_fraction': 1, 'verbose': -1}\ngmb = lgb.train(params, lgb_data, num_boost_round=nrounds)\n# predict the results\ny_fitted=gmb.predict(X_valid)\nacc = roc_auc_score(y_valid, y_fitted)\nprint('>learning rate=%s max depth=%s num leaves=%s boosting=%s resu=%.3f' % (lr,md,nl,bb,resu))","c45604ed":"from sklearn import metrics\nimport matplotlib.pyplot as plt\n\n######################################################\ndef Find_Optimal_Cutoff(target, predicted):\n    \"\"\" Find the optimal probability cutoff point for a classification\n    model related to the event rate Parameters\n    ----------\n    target: Matrix with dependent or target data, where rows are observations\n    predicted: Matrix with predicted data, where rows are observations\n    Returns\n    -------\n    list type, with optimal cutoff value\n    \"\"\"\n    fpr, tpr, threshold = metrics.roc_curve(target, predicted)\n    i = np.arange(len(tpr))\n    roc = pd.DataFrame({'tf' : pd.Series(tpr-(1-fpr), index=i), 'threshold' :pd.Series(threshold, index=i)})\n    roc_t = roc.iloc[(roc.tf-0).abs().argsort()[:1]]\n    return list(roc_t['threshold'])\n\n\n# extract false positive, true positive rate\nfpr, tpr, thresholds = metrics.roc_curve(y_valid, y_fitted)\nroc_auc = metrics.auc(fpr, tpr)\nprint(\"Area under the ROC curve : %f\" % roc_auc)\ni = np.arange(len(tpr)) # index for df\nroc = pd.DataFrame({'fpr' : pd.Series(fpr, index=i),'tpr' : pd.Series(tpr, index = i),\n'1-fpr' : pd.Series(1-fpr, index = i),\n'tf' : pd.Series(tpr - (1-fpr), index = i),'thresholds' : pd.Series(thresholds,index = i)})\n\nroc.iloc[(roc.tf-0).abs().argsort()[:1]]\n# Plot tpr vs 1-fpr\nfig, ax = plt.subplots()\nplt.plot(roc['tpr'], label='tpr')\nplt.plot(roc['1-fpr'], color = 'red', label='1-fpr')\nplt.legend(loc='best')\nplt.xlabel('1-False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic')\nplt.show()\n\n# Find optimal probability threshold\nthreshold = Find_Optimal_Cutoff(y_valid, y_fitted)\nprint (\"Optimal Probability Threshold: \", threshold)\n# Applying the threshold to the prediction probability\ny_pred_optimal = np.where(y_fitted >= threshold, 1, 0)\n\nprint (\"Optimal Cutoff - Accuracy: \", metrics.accuracy_score(y_valid,y_pred_optimal))\nprint (\"Optimal - Cutoff Confusion Matrix: \\n\", metrics.confusion_matrix(y_valid, y_pred_optimal))","16d0c3c8":"dft = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\nprint(dft.head())\nprint('Data frame shape',dft.shape)","ecbcb16c":"# rows with missing values\nprint(dft.isnull().sum())","2e667565":"cols=['Pclass','Sex','Age','Fare','Fam' ]\n\ndft['Fam'] =  dft['SibSp'] + dft['Parch']\n# define ordinal encoding\nencoder = OrdinalEncoder()\n# transform data\ndf[['Sex']] = encoder.fit_transform(df[['Sex']])\n\n\n# replace '0' values with 'nan'\n#dft[['Fare']] = dft[['Fare']].replace(0, np.NaN)\n\nprint(dft[cols].head())","83687473":"# rows with missing values\nprint(dft.isnull().sum())","34f3da12":"dft[cols] = iim.transform(dft[cols])\n\n\npred = gmb.predict(dft[cols])\n# Applying the threshold to the prediction probability\ny_pred_opt = np.where(pred >= threshold, 1, 0)\n\nresu = pd.DataFrame ({'PassengerId' : dft['PassengerId'], 'Survived': y_pred_opt})\nresu.to_csv('.\/submission_gmb.csv', index=False)\n\nresu.head()","497ce29d":"# The maximum for learning rate is between 0.5 and 0.7, for max depth - between 10 and 20, for num leaves between 50 and 70.","c9704489":"# Define ranges for parameter tuning","36b9aeda":"# The result from analysis is probability. The probability threshold between 0 and 1 is calculated. "}}