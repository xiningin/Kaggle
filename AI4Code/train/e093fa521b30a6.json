{"cell_type":{"f3caea11":"code","f5b26d25":"code","fa774f1f":"code","39ada45e":"code","5e666138":"code","510cb6ff":"code","9bd99a9b":"code","d074bc22":"code","143670e2":"code","2234e625":"code","02535a5b":"code","607ee2dd":"code","a8e41cc1":"code","68ff83c9":"code","706f938a":"code","09bc4e42":"code","f255c77d":"code","16526c75":"code","bdae68af":"code","e562d64a":"code","8271d1ca":"code","68378440":"code","fee6a487":"code","e5859075":"markdown","e3d4c2a7":"markdown","cbb2fc58":"markdown","7417c944":"markdown","88b5954e":"markdown","60856351":"markdown","a84df929":"markdown","386b2d40":"markdown","aeb53934":"markdown","61790562":"markdown","bf169fe4":"markdown","1dd9befb":"markdown","8aa9319d":"markdown","37dd9273":"markdown","9247e21c":"markdown","022c043f":"markdown","e3c83392":"markdown","fce8d788":"markdown","7e9ad377":"markdown","b754c7b4":"markdown","3bb909ec":"markdown","38fcc783":"markdown"},"source":{"f3caea11":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\n#https:\/\/towardsdatascience.com\/machine-learning-kaggle-competition-part-three-optimization-db04ea415507","f5b26d25":"#Required Libraries\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nimport missingno as msno\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import PowerTransformer\nimport category_encoders as ce\nfrom xgboost import XGBRegressor\nfrom sklearn.compose import ColumnTransformer\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom sklearn.model_selection import GridSearchCV\n\npd.set_option('display.max_columns', 300)\npd.set_option('display.max_rows', 11100)","fa774f1f":"# Read the data\nX_full = pd.read_csv('..\/input\/home-data-for-ml-course\/train.csv', index_col='Id')\nprint(\"Train Size \",X_full.shape)\nX_test_full = pd.read_csv('..\/input\/home-data-for-ml-course\/test.csv', index_col='Id')\nprint(\"Test Size \",X_test_full.shape)","39ada45e":"X_full.head()","5e666138":"X_full.columns","510cb6ff":"yrblt = X_full.groupby(['YearBuilt'])['Street'].count().reset_index()\n#print(yrblt)\nimport plotly.graph_objects as go\nfig = go.Figure(data=go.Scatter(x=yrblt.YearBuilt, y=yrblt.Street))\nfig.show()","9bd99a9b":"yrblt = X_full.groupby(['HouseStyle'])['Street'].count().reset_index()\n#print(yrblt)\nimport plotly.graph_objects as go\nfig = go.Figure(data=go.Scatter(x=yrblt.HouseStyle, y=yrblt.Street))\nfig.show()","d074bc22":"yrblt = X_full.groupby(['SaleCondition'])['Street'].count().reset_index()\n#print(yrblt)\nimport plotly.graph_objects as go\nfig = go.Figure(data=go.Scatter(x=yrblt.SaleCondition, y=yrblt.Street))\nfig.show()","143670e2":"yrblt = X_full.groupby(['LotArea'])['Street'].count().reset_index()\n#print(yrblt)\nimport plotly.graph_objects as go\nfig = go.Figure(data=go.Scatter(x=yrblt.LotArea, y=yrblt.Street))\nfig.show()","2234e625":"yrblt = X_full.groupby(['RoofStyle'])['Street'].count().reset_index()\n#print(yrblt)\nimport plotly.graph_objects as go\nfig = go.Figure(data=go.Scatter(x=yrblt.RoofStyle, y=yrblt.Street))\nfig.show()","02535a5b":"yrblt = X_full.groupby(['BldgType'])['Street'].count().reset_index()\n#print(yrblt)\nimport plotly.graph_objects as go\nfig = go.Figure(data=go.Scatter(x=yrblt.BldgType, y=yrblt.Street))\nfig.show()","607ee2dd":"yrblt = X_full.groupby(['GarageCars'])['Street'].count().reset_index()\n#print(yrblt)\nimport plotly.graph_objects as go\nfig = go.Figure(data=go.Scatter(x=yrblt.GarageCars, y=yrblt.Street))\nfig.show()","a8e41cc1":"yrblt = X_full.groupby(['GarageType'])['Street'].count().reset_index()\n#print(yrblt)\nimport plotly.graph_objects as go\nfig = go.Figure(data=go.Scatter(x=yrblt.GarageType, y=yrblt.Street))\nfig.show()","68ff83c9":"yrblt = X_full.groupby(['BldgType'])['SalePrice'].max().reset_index()\n#print(yrblt)\nimport plotly.graph_objects as go\nfig = go.Figure(data=go.Scatter(x=yrblt.BldgType, y=yrblt.SalePrice))\nfig.show()","706f938a":"# Remove rows with missing target, separate target from predictors\nX_full.dropna(axis=0, subset=['SalePrice'])\ny = X_full.SalePrice\nX_full.drop(['SalePrice'], axis=1, inplace=True)","09bc4e42":"# Columns with Null Values and its count\n# this is helpful to identify columns to drop or impute\n[{x: X_full[x].isnull().sum()} for x in X_full.columns[X_full.isna().any()]]","f255c77d":"# Dropping Columns which is having high number of Nulls\nhigh_null_columns = [x for x in X_full.columns if X_full[x].isna().sum()>100]\nprint([high_null_columns])\nX_full.drop(axis = 1, columns = high_null_columns, inplace = True)\nX_test_full.drop(axis = 1, columns = high_null_columns, inplace = True)\nprint(\"Train Size \",X_full.shape)\nprint(\"Test Size \",X_test_full.shape)","16526c75":"cate_columns = X_full.select_dtypes('object').columns\nprint(\"Categorical columns \")\nprint(len(cate_columns),cate_columns)\nnum_columns = X_full.select_dtypes(exclude = ['object']).columns\nprint(\"Numerical columns \")\nprint(len(num_columns), num_columns)","bdae68af":"# Cardinality checks\n# Need to remove high cardinality columns since It will make more computing time after transformation.\nsorted({x:X_full[x].nunique() for x in cate_columns}.items(), key=lambda x: x[1],reverse=True)\n\ncategorical_small_variety_cols = [cname for cname in cate_columns if\n                    X_full[cname].nunique() < 15]\ncategorical_large_variety_cols = [cname for cname in cate_columns if\n                    X_full[cname].nunique() >= 15]\nprint(categorical_small_variety_cols)\nprint(categorical_large_variety_cols)\ncategorical_label_cols =[]\nskew_cols = []","e562d64a":"# Imputation\n\n# Preprocessing for numerical data\nnumerical_transformer = Pipeline(verbose=False,steps=[\n    ('imputer_num', SimpleImputer(strategy='mean'))])\n\n# Preprocessing for categorical data\ncategorical_onehot_transformer = Pipeline(verbose=False,steps=[\n    ('imputer_onehot', SimpleImputer(strategy='most_frequent')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n])\n\ncategorical_label_transformer = Pipeline(verbose=False,steps=[\n    ('imputer_label', SimpleImputer(strategy='most_frequent')),\n    ('label', ce.OrdinalEncoder())\n    \n])\n\ncategorical_count_transformer = Pipeline(verbose=False,steps=[\n    ('imputer_count', SimpleImputer(strategy='most_frequent')),\n    ('count', ce.TargetEncoder(handle_missing='count'))])\n\n# Bundle preprocessing for numerical and categorical data\npreprocessor = ColumnTransformer(verbose=False,\n    transformers=[\n        ('num', numerical_transformer, num_columns),\n        ('cox_box', PowerTransformer(method='yeo-johnson', standardize=False),skew_cols),\n        ('cat_label', categorical_label_transformer, categorical_label_cols),\n        ('cat_onehot', categorical_onehot_transformer, categorical_small_variety_cols),\n        ('cat_count', categorical_onehot_transformer, categorical_large_variety_cols),\n    ])\n\ntrain_pipeline = Pipeline(verbose=False,steps=[\n                    ('preprocessor', preprocessor),   \n                    ('scale', StandardScaler(with_mean=False,with_std=True)),\n                    ('model', XGBRegressor(random_state=42))\n                    ])","8271d1ca":"param_grid = {'model__nthread':[2], #when use hyperthread, xgboost may become slower:2\n              'model__learning_rate': [0.04, 0.05], #so called `eta` value\n              'model__max_depth': range(3,5,1), # 3,5,1\n              \"model__colsample_bytree\" : [ 0.2 ],\n              'model__silent': [1],\n              'model__n_estimators': [800], #number of trees:800\n             }\nsearched_model = GridSearchCV(estimator=train_pipeline,param_grid = param_grid, scoring=\"neg_mean_absolute_error\", cv=5, error_score='raise', verbose = 1)\nsearched_model.fit(X_full,y)\nprint(searched_model.best_score_)\n# -14495.6467\n# -14443.2930 : 900\n# -14446.1679 : 1000\n# -14438.5401 : 950,4,0.04,0.05 or 950,2, [0.04, 0,05]\n# -14435.5775 : 950, 2, [0.04, 0,05], \n# -14435.4461 : 860","68378440":"preds_test = searched_model.predict(X_test_full)","fee6a487":"output = pd.DataFrame({'Id': X_test_full.index,'SalePrice': preds_test})\noutput.to_csv('submission.csv', index=False)\noutput","e5859075":"# EDA","e3d4c2a7":"## 1. Data Loading","cbb2fc58":"### We have two dataset for training and testing.\n> Our Target varibale is **SalePrice**","7417c944":"## Gable -  supports for cold or temperate climates","88b5954e":"## 1 Family houses are too high","60856351":"Imputation is done based on Numerical and Catecorical features. Mostly for numerical features filled by 'Median' values. Categorcial values filled by most frequent values of a particular feature.\n\nPipeline is an amazing tool to integrate the imputation and model. It will reduce a lot of manual typed codes.","a84df929":"## 5. Prediction","386b2d40":"## 3. Imputation and Pipeline building","aeb53934":"# Happy Learning :)","61790562":"# House Price Prediction\nJust a try with Kaggle Machine Learning Course.\n\nI used many others code for my reference.\n\nThis is about predicting the house price based on the details like size of the house, number of rooms, number of bed rooms, number of Garages, location of the house, age of the house and etc. Normally In realestate, this value is fixed based on these parameters manually. we are trying to teach an algorithm to predict the price for us.\n\nLet go into the data. :)","bf169fe4":"you like it plz **upvote**, \n\nyou have suggestions to improve, please comment :) ","1dd9befb":"## 6. Submission","8aa9319d":"## 1 Family houses selling price is too high","37dd9273":"## Even most of the houses are 1 family, but it contains 2 car parking garages.","9247e21c":"## 4. Model Buidling","022c043f":"Generally dropping the columns is not adviceable, but in our case the following columns are having too many null values. if the null values is less than 10% for a particular feature, we can try to impute to get better result. *(Imputing a large null based on our assumption may lead us in the wrong direction.)*","e3c83392":"## Mostly 1 Story houses followed by 2 Story houses.","fce8d788":"## Attached Car parking is high followed by Separate Garage","7e9ad377":"## After 2000 number of house construction gone peak.","b754c7b4":"## Most of the house built area is less than 20K Sq.Ft","3bb909ec":"Training a model is like teaching a kid. need a lot of attention. especially in Learning_Rate and Estimators. You can change these paramters and see the prediction. \n\nInitially, I have fixed the estimator as 500, the MAE(Mean Absolute Error) was fine. While changing the value little bit, MAE started reducing. finally I fixed this value.","38fcc783":"> ## 2. Feature Engineering"}}