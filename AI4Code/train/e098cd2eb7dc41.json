{"cell_type":{"7747a3c1":"code","3c76fab1":"code","d72a9ddb":"code","70eb025d":"code","e47bdb18":"code","f4314f02":"code","f89a07f1":"code","9b1da57c":"code","79de33e9":"code","eca9f662":"code","6535a5db":"code","3c94415c":"code","ca969b88":"code","a6d0bcd2":"code","6e2499b3":"code","7c9d8249":"code","f2ca5242":"code","90064d1e":"markdown","cfc44f6b":"markdown","fdef13e7":"markdown","001f7f78":"markdown","1e335bbf":"markdown","9337df3b":"markdown","eadac588":"markdown","b4750466":"markdown","9711098e":"markdown","5cb69be9":"markdown","bf026da1":"markdown","bc352e4e":"markdown","cb2ed7b3":"markdown","c013201c":"markdown","999d7192":"markdown"},"source":{"7747a3c1":"import pandas as pd\nimport numpy as np\nimport dateutil.easter as easter\nimport matplotlib.pyplot as plt\nfrom matplotlib.ticker import MaxNLocator, FormatStrFormatter, PercentFormatter\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.compose import TransformedTargetRegressor","3c76fab1":"train_df = pd.read_csv('..\/input\/tabular-playground-series-jan-2022\/train.csv')\ntest_df = pd.read_csv('..\/input\/tabular-playground-series-jan-2022\/test.csv')\n\n# The dates are read as strings and must be converted\nfor df in [train_df, test_df]:\n    df['date'] = pd.to_datetime(df.date)\n    df.set_index('date', inplace=True, drop=False)\ntrain_df","d72a9ddb":"print(train_df.groupby(['country', 'store', 'product']).date.count())\n\nprint(\"First day:\", train_df.date.min(), \"   Last day:\", train_df.date.max())\nprint(\"Number of days in four years:\", 365 * 4 + 1) # four years including one leap year\nprint(18 * 1461, train_df.shape, train_df.date.isna().sum())","70eb025d":"train_df.groupby(['country', 'store', 'product']).num_sold.agg(['min', 'max', 'mean'])","e47bdb18":"test_df.date.min(), test_df.date.max()","f4314f02":"kk = train_df.groupby(['country', 'store', 'product']).num_sold.mean().unstack(level='store')\nkk['KaggleRama:KaggleMart'] = kk.KaggleRama \/ kk.KaggleMart\nkk","f89a07f1":"# Group by year\nkk = train_df.groupby(['country', 'store', 'product', train_df.date.dt.year]).num_sold.mean().unstack(level='product')\nkk['Mugs\/Sticker'] = kk['Kaggle Mug'] \/ kk['Kaggle Sticker']\nkk['Hats\/Sticker'] = kk['Kaggle Hat'] \/ kk['Kaggle Sticker']\nkk","9b1da57c":"# Group by month\nkk = train_df.groupby(['product', train_df.date.dt.month]).num_sold.mean().unstack(level='product')\nkk['Mugs\/Sticker'] = kk['Kaggle Mug'] \/ kk['Kaggle Sticker']\nkk['Hats\/Sticker'] = kk['Kaggle Hat'] \/ kk['Kaggle Sticker']\nkk","79de33e9":"plt.figure(figsize=(18, 12))\nfor i, (combi, df) in enumerate(train_df.groupby(['country', 'store', 'product'])):\n    ax = plt.subplot(6, 3, i+1, ymargin=0.5)\n    ax.hist(df.num_sold, bins=50, color='pink')\n    #ax.set_xscale('log')\n    ax.set_title(combi)\nplt.suptitle('Histograms of num_sold', y=1.03)\nplt.tight_layout(h_pad=3.0)\nplt.show()","eca9f662":"plt.figure(figsize=(18, 12))\nfor i, (combi, df) in enumerate(train_df.groupby(['country', 'store', 'product'])):\n    ax = plt.subplot(6, 3, i+1, ymargin=0.5)\n    #print(df.num_sold.values.shape, df.num_sold.values)\n    ax.plot(df.num_sold)\n    ax.set_title(combi)\n    #if i == 6: break\nplt.tight_layout(h_pad=3.0)\nplt.suptitle('Daily sales for 2015-2018', y=1.03)\nplt.show()","6535a5db":"plt.figure(figsize=(18, 12))\nfor i, (combi, df) in enumerate(train_df.groupby(['country', 'store', 'product'])):\n    ax = plt.subplot(6, 3, i+1, ymargin=0.5)\n    ax.bar(range(1, 32),\n           df.num_sold[df.date.dt.month==12].groupby(df.date.dt.day).mean(),\n           color=['b'] * 25 + ['orange'] * 6)\n    ax.set_title(combi)\n    ax.set_xticks(ticks=range(5, 31, 5))\nplt.tight_layout(h_pad=3.0)\nplt.suptitle('Daily sales for December', y=1.03)\nplt.show()","3c94415c":"plt.figure(figsize=(18, 12))\nfor i, (combi, df) in enumerate(train_df.groupby(['country', 'store', 'product'])):\n    ax = plt.subplot(6, 3, i+1, ymargin=0.5)\n    #print(df.resample('MS').num_sold.sum())\n    resampled = df.resample('MS').num_sold.sum()\n    ax.bar(range(len(resampled)), resampled)\n    ax.set_title(combi)\n    ax.set_ylim(resampled.min(), resampled.max())\n    ax.set_xticks(range(0, 48, 12), [f\"Jan {y}\" for y in range(2015, 2019)])\nplt.suptitle('Monthly sales for 2015-2018', y=1.03)\nplt.tight_layout(h_pad=3.0)\nplt.show()","ca969b88":"plt.figure(figsize=(18, 12))\nfor i, (combi, df) in enumerate(train_df.groupby(['country', 'store', 'product'])):\n    ax = plt.subplot(6, 3, i+1, ymargin=0.5)\n    resampled = df.resample('MS').sum()\n    resampled = resampled.groupby(resampled.index.month).mean()\n    ax.bar(range(1, 13), resampled.num_sold)\n    ax.set_xticks(ticks=range(1, 13), labels='JFMAMJJASOND')\n    ax.set_title(combi)\n    ax.set_ylim(resampled.num_sold.min(), resampled.num_sold.max())\nplt.suptitle('Monthly sales for 2015-2018', y=1.03)\nplt.tight_layout(h_pad=3.0)\nplt.show()","a6d0bcd2":"plt.figure(figsize=(18, 12))\nfor i, (combi, df) in enumerate(train_df.groupby(['country', 'store', 'product'])):\n    ax = plt.subplot(6, 3, i+1, ymargin=0.5)\n    resampled = df.resample('AS').sum()\n    ax.bar(range(2015, 2019), resampled.num_sold, color='brown')\n    ax.set_title(combi)\n    ax.xaxis.set_major_locator(MaxNLocator(integer=True)) # only integer labels\n    ax.set_ylim(0, resampled.num_sold.max())\nplt.suptitle('Growth of yearly sales for 2015-2018', y=1.03)\nplt.tight_layout(h_pad=3.0)\nplt.show()","6e2499b3":"plt.figure(figsize=(12, 90))\nfor i, (combi, df) in enumerate(train_df.groupby(['country', 'product', 'store'])):\n    ax = plt.subplot(18, 1, i+1, ymargin=0.5)\n\n    # Bar charts (scaled so that 2015 is 1.0)\n    resampled = df[(df.date.dt.month<12) | (df.date.dt.day<25)].resample('AS').num_sold.sum()\n    resampled \/= resampled.iloc[0]\n    resampled_end_of_year = df[(df.date.dt.month==12) & (df.date.dt.day>=25)].resample('AS').num_sold.sum()\n    resampled_end_of_year \/= resampled_end_of_year.iloc[0]\n    ax.bar(range(2015, 2019), resampled, color='brown')\n    ax.bar(range(2015, 2019), resampled_end_of_year, color='orange', width=0.4)\n    \n    # Fit exponential growth curves and determine percent growth per year\n    X = np.arange(2015, 2019).reshape(-1, 1)\n    lr = TransformedTargetRegressor(LinearRegression(), func=np.log, inverse_func=np.exp)\n    lr.fit(X, resampled)\n    ax.plot(range(2015, 2019), lr.predict(X), color='brown', label=f\"whole year: {lr.predict([[2016]]).squeeze() - 1:.1%}\")\n    lr.fit(X, resampled_end_of_year)\n    ax.plot(range(2015, 2019), lr.predict(X), color='orange', label=f\"end of year: {lr.predict([[2016]]).squeeze() - 1:.1%}\")\n    \n    ax.legend()\n    ax.set_title(f\"Yearly sales for {combi}\")\n    ax.xaxis.set_major_locator(MaxNLocator(integer=True)) # only integer labels\nplt.tight_layout(h_pad=3.0)\nplt.show()\n","7c9d8249":"plt.figure(figsize=(18, 12))\nfor i, (combi, df) in enumerate(train_df.groupby(['country', 'store', 'product'])):\n    ax = plt.subplot(6, 3, i+1, ymargin=0.5)\n    resampled = df.groupby(df.index.dayofweek).mean()\n    ax.bar(range(7), resampled.num_sold, \n           color=['b']*4 + ['g'] + ['orange']*2)\n    ax.set_title(combi)\n    ax.set_xticks(ticks=range(7), labels=['M', 'T', 'W', 'T', 'F', 'S', 'S'])\n    ax.set_ylim(0, resampled.num_sold.max())\nplt.suptitle('Sales per day of the week', y=1.03)\nplt.tight_layout(h_pad=3.0)\nplt.show()","f2ca5242":"plt.figure(figsize=(18, 12))\nfor i, (year, df) in enumerate(train_df.groupby(train_df.date.dt.year)):\n    df = df.reset_index(drop=True)\n    ax = plt.subplot(4, 1, i+1, ymargin=0.5)\n    april = df.num_sold[(df.date.dt.month==4)].groupby(df.date.dt.day).mean()\n    date_range = pd.date_range(start=f'{year}-04-01', end=f'{year}-04-30', freq='D')\n    easter_date = easter.easter(year)\n    color = ['r' if d == easter_date else 'lightblue' if (d.date() - easter_date).days in range(6) else 'b' if d.dayofweek < 5 else 'orange' for d in date_range]\n    ax.bar(range(1, 31),\n           april,\n           color=color)\n    ax.set_title(str(year))\n    ax.set_xticks(ticks=range(5, 31, 5))\nplt.tight_layout(h_pad=3.0)\nplt.suptitle('Daily sales for April', y=1.03)\nplt.show()","90064d1e":"# KaggleRama sells more\n\nFor every country and product, KaggleRama on average sells 1.74 times as much as KaggleMart.\n\n**Insight:** Maybe it suffices to model KaggleMart and multiply all predictions by 1.74 to get the KaggleRama predictions.","cfc44f6b":"## Monthly sales and seasonal variation\n\nA plot of the monthly totals shows the seasonal variation and a growing trend. The growth looks more pronounced fo the stickers than for the hats.\n\n**Insight**\n- We must ensure that our models can extrapolate the growth to the fifth year.","fdef13e7":"## Growth\n\nAggregating the sales per year shows the growth trend. All country-store-product combinations show growth, but there are subtle differences:\n- In Norway, 2016 was a bad year with lower sales than 2015.\n- Sweden had no growth from 2017 to 2018.\n- Almost everywhere, the end-of-year rush grew more than the rest of the year.\n\n**Insight**\n- We have to model a growth rate which depends on the country.\n- The growth is neither linear nor exponential.\n- We have to model a different growth rate for the end-of-year rush (and maybe other seasonal effects).\n\nFor a broader analysis of the topic, see [this discussion](https:\/\/www.kaggle.com\/c\/tabular-playground-series-jan-2022\/discussion\/298318). The outcome was that the Kaggle sales figures depend on the country's GDP.","001f7f78":"# Products\n\nIf we group the data by country, store, product and year, the ratio Sticker:Mug:Hat is always 1:1.97:3.5 and depends neither on country nor on store nor on year. If we group the data by month, however, the ratio is not constant. This implies that the products have different seasonal variations.\n\n**Insight:**\nWe have to model seasonal effects which depend on the product.","1e335bbf":"# Time series\n\n## Daily sales and the year-end peak\n\nA plot of the daily values of the 18 four_year time series clearly shows high peaks at the end of every year. If we look at the diagrams closely, we see slight waveforms and discern more seasonal effects:\n- (left column:) Kaggle sells more hats in the first half of the year than in the second half (maybe because buyers want to wear Kaggle hats during summer).\n- (middle column:) Demand for mugs is higher in the (northern hemisphere) winter than in summer.\n- (right column:) Sticker sales don't depend on season, except for some small spikes. (All three products have these spikes in the second quarter.)\n\n**Insight**\n- We have to model seasonal effects which depend on the product.\n- We have to model waves with a wavelength of a year as well as short spikes.","9337df3b":"Let's zoom in on the year-end peak. We plot only the 31 days of December, averaged over the four years. The plots show that sales start growing after Christmas and peak on the 30th of December:\n","eadac588":"# Completeness of the data\n\nThere are three countries, two stores and three products. For all 18 combinations of these, we have the sales data for 1461 days. The 1461 days are all days of the four years 2015, 2016, 2017, 2018. There are no missing values.","b4750466":"# An EDA which makes sense\n\nAmong others, this EDA shows:\n- variation per season and day of the week\n- the effect of Easter\n- zoom-in on end-of-year peak\n- yearly growth (which is higher for the end-of-year peak than for the rest of the year)\n- why this competition is scored by SMAPE\n\nWhat next? Look at [the notebook](https:\/\/www.kaggle.com\/ambrosm\/tpsjan22-03-linear-model) which presents feature engineering (based on the insights of this EDA) and a linear model which makes use of the features.","9711098e":"# Histograms and SMAPE\n\nThe histograms for every country-store-product combination show that all histograms are skewed. For every product, there are some days with sales far above the mean. For these outliers, predictions will be much less accurate than for the regular days. This is why the competition is scored by Symmetric mean absolute percentage error ([SMAPE](https:\/\/en.wikipedia.org\/wiki\/Symmetric_mean_absolute_percentage_error)) rather than MAE or MSE.\n\nOf course, in a competition scored by SMAPE, we have to carefully choose a loss function for training our models. MSE or MAE are not the ideal loss functions here.\n\nEvery product's histogram has a slightly different shape. The histograms of the Kaggle Hat looks wider than the histograms of the other two products.\n\n**Insight**\n- Choosing a suitable loss function is important.\n- It may be advantageous to predict log(num_sold) rather than num_sold directly.\n- We need more analysis to understand why the histograms have different shapes. Maybe it is because of the seasonal variations, maybe there is something else.","5cb69be9":"# Easter\n\nThe following diagram shows that during the week after Easter, sales are higher than normal. The diagram shows daily sales for April of the four years. Weekends are colored orange, Easter Sunday is marked red, the week after Easter is colored light blue.\n\nEaster of 2016 was on the 27th of March; the diagram shows only the last days of the week after Easter.\n\n**Insight**\n- The model must know the date of Easter and account for higher demand in the week after Easter.","bf026da1":"We can see the seasonal variation more clearly if we average over the four years and show only 12 bars for the 12 months:\n- Hats have the maximum in April or May and the minimum in September. They have another (local) maximum in December \/ January.\n- Mugs have the maximum in December \/ January and the minimum in June or July. They have a small local maximum in March.\n- Stickers have their maximum in December \/ January, minimum in February and second maximum May.","bc352e4e":"Whereas the training data covers the year 2015 through 2018, the test data requires us to predict the year 2019:","cb2ed7b3":"## Weekdays\n\nSaturdays and Sundays are the best days (highest sales) for all products. Friday seems to be better than Monday through Thursday.\n\n**Insight**\n- Our model needs to distinguish at least three phases of the week: Mon-Thu, Fri, Sat-Sun.","c013201c":"What next? Look at [a notebook](https:\/\/www.kaggle.com\/ambrosm\/tpsjan22-03-linear-model) which presents feature engineering (based on the insights of this EDA) and a linear model which makes use of the features.","999d7192":"Every product is sold in every store on every day. num_sold is always positive:"}}