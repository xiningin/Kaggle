{"cell_type":{"b84e52bc":"code","a1e4fba5":"code","daf51a36":"code","6ef1f15d":"code","3f01ac07":"code","4862e52f":"code","9e2fdb7c":"code","1d2b1340":"code","e4652d9c":"code","6be22eae":"code","23112e47":"code","aeda7a9d":"code","04ef00c7":"code","005edb0d":"markdown","b5302113":"markdown","7ef8d70d":"markdown","a3678625":"markdown","99126e0c":"markdown","105e39bb":"markdown","56762675":"markdown","00cba818":"markdown","ae4798d7":"markdown","fb2e5f89":"markdown","07bbdb66":"markdown","cdb021fc":"markdown","20a77206":"markdown"},"source":{"b84e52bc":"import numpy as np\nimport pandas as pd\n\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\n\nimport os\nimport glob\nimport cv2","a1e4fba5":"import tensorflow as tf\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.layers import Input, Add, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D, AveragePooling2D, MaxPooling2D\nfrom tensorflow.keras.models import Model, load_model\nfrom tensorflow.keras.initializers import glorot_uniform\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau\nfrom tensorflow.keras.optimizers import SGD","daf51a36":"foods = list(os.walk('..\/input\/food41\/images\/'))[0][1]\nnp.random.shuffle(foods)","6ef1f15d":"# select how many types of foods to train on\nnr_foods = 5","3f01ac07":"idx_to_name = {i:x for (i,x) in enumerate(foods[:nr_foods])}\nname_to_idx = {x:i for (i,x) in enumerate(foods[:nr_foods])}\n\nidx_to_name","4862e52f":"data = []\nlabels = []\nimg_size = (112, 112)\n\nfor food in idx_to_name.values():\n    path = '..\/input\/food41\/images\/'\n    imgs = [cv2.resize(cv2.imread(img), img_size, interpolation=cv2.INTER_AREA) for img in glob.glob(path + food + '\/*.jpg')]\n    for img in imgs:\n        labels.append(name_to_idx[food])\n        data.append(img)\n        \n# Normalize data\ndata = np.array(data)\ndata = data \/ 255.0\ndata = data.astype('float32')\n\n# Create one hot encoding for labels\nlabels = np.array(labels)\nlabels = np.eye(len(idx_to_name.keys()))[list(labels)]","9e2fdb7c":"# check out the shapes\ndata.shape, labels.shape","1d2b1340":"# split training, labels\nX_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.3, random_state=42)","e4652d9c":"def bottleneck_residual_block(X, kernel_size, filters, reduce=False, s=2):\n    # unpack the tuple to retrieve Filters of each CONV layer\n    F1, F2, F3 = filters\n    # Save the input value to use it later to add back to the main path.\n    X_shortcut = X\n    # if condition if reduce is True\n    if reduce:\n        # if we are to reduce the spatial size, apply a 1x1 CONV layer to the shortcut path\n        # to do that, we need both CONV layers to have similar strides\n        X_shortcut = Conv2D(filters = F3, kernel_size = (1, 1), strides = (s,s))(X_shortcut)\n        X_shortcut = BatchNormalization(axis = 3)(X_shortcut)\n        # if reduce, we will need to set the strides of the first conv to be similar to the shortcut strides\n        X = Conv2D(filters = F1, kernel_size = (1, 1), strides = (s,s), padding = 'valid')(X)\n        X = BatchNormalization(axis = 3)(X)\n        X = Activation('relu')(X)\n    else:\n        # First component of main path\n        X = Conv2D(filters = F1, kernel_size = (1, 1), strides = (1,1), padding = 'valid')(X)\n        X = BatchNormalization(axis = 3)(X)\n        X = Activation('relu')(X)\n        \n    # Second component of main path\n    X = Conv2D(filters = F2, kernel_size = kernel_size, strides = (1,1), padding = 'same')(X)\n    X = BatchNormalization(axis = 3)(X)\n    X = Activation('relu')(X)\n\n    # Third component of main path\n    X = Conv2D(filters = F3, kernel_size = (1, 1), strides = (1,1), padding = 'valid')(X)\n    X = BatchNormalization(axis = 3)(X)\n\n    # Final step: Add shortcut value to main path, and pass it through a RELU activation\n    X = Add()([X, X_shortcut])\n    X = Activation('relu')(X)\n    return X","6be22eae":"def ResNet50(input_shape, classes):\n    # Define the input as a tensor with shape input_shape\n    X_input = Input(input_shape)\n\n    # Stage 1\n    X = Conv2D(64, (7, 7), strides=(2, 2), name='conv1')(X_input)\n    X = BatchNormalization(axis=3, name='bn_conv1')(X)\n    X = Activation('relu')(X)\n    X = MaxPooling2D((3, 3), strides=(2, 2))(X)\n\n    # Stage 2\n    X = bottleneck_residual_block(X, 3, [64, 64, 256], reduce=True, s=1)\n    X = bottleneck_residual_block(X, 3, [64, 64, 256])\n    X = bottleneck_residual_block(X, 3, [64, 64, 256])\n\n    # Stage 3\n    X = bottleneck_residual_block(X, 3, [128, 128, 512], reduce=True, s=2)\n    X = bottleneck_residual_block(X, 3, [128, 128, 512])\n    X = bottleneck_residual_block(X, 3, [128, 128, 512])\n    X = bottleneck_residual_block(X, 3, [128, 128, 512])\n\n    # Stage 4\n    X = bottleneck_residual_block(X, 3, [256, 256, 1024], reduce=True, s=2)\n    X = bottleneck_residual_block(X, 3, [256, 256, 1024])\n    X = bottleneck_residual_block(X, 3, [256, 256, 1024])\n    X = bottleneck_residual_block(X, 3, [256, 256, 1024])\n    X = bottleneck_residual_block(X, 3, [256, 256, 1024])\n    X = bottleneck_residual_block(X, 3, [256, 256, 1024])\n\n    # Stage 5\n    X = bottleneck_residual_block(X, 3, [512, 512, 2048], reduce=True, s=2)\n    X = bottleneck_residual_block(X, 3, [512, 512, 2048])\n    X = bottleneck_residual_block(X, 3, [512, 512, 2048])\n\n    # AVGPOOL\n    X = AveragePooling2D((1,1))(X)\n\n    # output layer\n    X = Flatten()(X)\n    X = Dense(classes, activation='softmax', name='fc' + str(classes))(X)\n    # Create the model\n    model = Model(inputs = X_input, outputs = X, name='ResNet50')\n\n    return model","23112e47":" # min_lr: lower bound on the learning rate\n# factor: factor by which the learning rate will be reduced\nreduce_lr= ReduceLROnPlateau(monitor='val_loss',factor=np.sqrt(0.1),patience=5, min_lr=0.5e-6)\n\nsgd = SGD(lr=0.01, momentum=0.9, nesterov=False)","aeda7a9d":"# create model\nmodel = ResNet50(input_shape = (112, 112, 3), classes = nr_foods)\n\n# compile the model\nmodel.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n \n# train the model\n# call the reduce_lr value using callbacks in the training method\nhistory = model.fit(X_train, y_train, batch_size=16, validation_data=(X_test, y_test), epochs=10, verbose=0, callbacks=[reduce_lr])","04ef00c7":"plt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'val'], loc='upper left')\nplt.show()","005edb0d":"<img align=left src='https:\/\/storage.googleapis.com\/kagglesdsdata\/datasets%2F771810%2F1329910%2Fidblock.png?GoogleAccessId=databundle-worker-v2@kaggle-161607.iam.gserviceaccount.com&Expires=1594745721&Signature=E119ItRSjruMuhgZoe87NPh1X5M%2FEFH4DZp%2BxAXZZZU4I1l7z1%2FoTwW4oVrW40P6XK06sT%2FUKGGc%2BM5IAZvVKoEjEGEIuqLW%2FuT95u7Rnhnedm4pRbfiuAzajNwgM3TRYYVZgKPstu3jYqhfdqZ5Q9GgJ3jy01xw3GXpnc5EkXxfcLa2K4aJi8ZQc%2Bqlow8a5k33JXxGNsRPm1rtuqfm06uW9CcUMKnYLSMv8wuh%2FU6ZMYTkShUDcWQjxENHfNmmSi7Bxkb5MHsBn0GfQ7Yr01D2Y9tHyxrcAUrsUtzgBq8zLCrZIayApnvJlSDz9t4y7dIbP3nZpTX4aKymExFMdA%3D%3D'>","b5302113":"## Bottleneck residual without reduce","7ef8d70d":"<a id='training'><\/a>\n# Training","a3678625":"## Bottleneck Residual Block with reduce","99126e0c":"<a id='residual'><\/a>\n# Residual Block","105e39bb":"<img align=left src='https:\/\/storage.googleapis.com\/kagglesdsdata\/datasets%2F771810%2F1329919%2Freduceblock.png?GoogleAccessId=databundle-worker-v2@kaggle-161607.iam.gserviceaccount.com&Expires=1594746011&Signature=a1DgPh9VPvb8MjDNg8dQfHYKHOhTzTZbAib4GNO9h0KeBYVAw3gNRxyQO%2Bib7%2F%2Fxa6zRSJKx63xsKSTyXYFbbvKp0cyBi92ifjFRbTAyUh35lOsDleH1UufyE92M98T%2Br90QwEXFlsbQ%2FdKGW6JdhcuGPR4M8akKGaGO%2FpTS2V84w3dy8%2FsC4sj2uiYSbkTTWTO0HQJyx%2FuOFaSPCOkrDfP7FVWFwfbV3VxvnRmM2byz2LA1dK6j39SLdDyGqO4zvEeghnfAHwqrShgLQi9jZRhAX1UStk33o03vCp8B3Bge5%2FgPD%2BsXMz%2BoAWERytvG8z79dPVMLU0hfcYKQXkz%2BA%3D%3D'>","56762675":"# CNN Food-101 Residual Block (ResNet)","00cba818":"<a id='architecture'><\/a>\n# Architecture","ae4798d7":"<img align=left src='https:\/\/storage.googleapis.com\/kagglesdsdata\/datasets%2F771810%2F1329857%2Fresnet.png?GoogleAccessId=databundle-worker-v2@kaggle-161607.iam.gserviceaccount.com&Expires=1594744705&Signature=lgq2WzbFyl25w9lCgKULB%2BquMGZER2JdIOmYWH8mN2u%2F%2BqDSsFGjo2mHszKeN0pysQx45NwhNyUrJHepH1YL5cS79KXin%2BCNRcTnC6zTLlGVvdwhCH%2BPdWW0bw7rCMhNywli1NW8oaF32zB31W67EIWrcb1E3mQEEz%2FjhMFN%2BKrAm1mgXUdMjf8P3sJzT6CZjaL%2F3MI0K9QdekMSGanE1sSH1RxSPuqGeZ8QUS7RUAaZLeEqiubSGXjKJ2DNw5CeFxPEN5fbodvUQvgPwML2eKBRW%2FWbVpqpiPZnOby1WHJFenORUjycIgcRNKGthjRmiiuvVXOH8cd2IxNGHVozow%3D%3D'>","fb2e5f89":"<a id='split'><\/a>\n# Split Data","07bbdb66":"<img width=550 align=left src='https:\/\/storage.googleapis.com\/kagglesdsdata\/datasets%2F771810%2F1329942%2Ffood.jpg?GoogleAccessId=databundle-worker-v2@kaggle-161607.iam.gserviceaccount.com&Expires=1594746835&Signature=JJhjnZC8AKMqVR%2BLSyyUcIXoM9XPJcK0NsKhLCSb8wTc%2BDnX3a7R3yrqrOfg3cuyO8xGsRzMHnsYNdYuVuUbIrUPGuOWUO6G8lz%2FkmOaMEwWTibi6A4SQHB3r8Jh4LilQ4z0CDTAe%2FOC2Ul0vpvc9WcCOFAQxGOV6sE%2B9ucMF7VXuxMzxgcnb4pm8brpwojfvfrHBKzYFlEl%2BI3Wh53FGKN0Bcys0qFZd1P4LTQ5BWZ%2BpJT5%2Bo3q3mKO5b3Rh6FGK0ZODNC8rZ2CoemZEX9ixVMz2NIBi%2BNy0fIu20JjW70vnxaymbfPVq1zc59pFJYOw6FA0mLnH68LDJsVHAVx7Q%3D%3D'>","cdb021fc":"## Content\n\n* [Split Data](#split)\n* [Residual Block](#residual)\n* [Architecture](#architecture)\n* [Training](#training)","20a77206":"# Reference\n\nArchitecture images<br>\nhttps:\/\/www.manning.com\/books\/deep-learning-for-vision-systems"}}