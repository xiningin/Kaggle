{"cell_type":{"2a4a22f5":"code","b15fa29e":"code","ede51283":"code","1dec197e":"code","9a6776ac":"code","eb326cd6":"code","d7ed1edd":"code","3d922481":"code","77a09b3c":"code","7a7623fe":"code","e0f212ea":"code","c85bb2e3":"markdown","536b2e57":"markdown","64884a45":"markdown","1e3cc920":"markdown","8270cf79":"markdown","75539f1c":"markdown","f3ed4454":"markdown","aee85d61":"markdown","5c317f13":"markdown","24ddb029":"markdown","c50ed812":"markdown","9e679429":"markdown"},"source":{"2a4a22f5":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \nimport math\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.svm import SVR\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import r2_score","b15fa29e":"df = pd.read_csv('\/kaggle\/input\/new-york-city-airbnb-open-data\/AB_NYC_2019.csv')","ede51283":"df.head(10)","1dec197e":"df.isnull().sum()","9a6776ac":"df.drop(['name','host_name'],axis = 1, inplace = True)\ndf.dropna(axis = 0,inplace = True)","eb326cd6":"new_df = pd.get_dummies(df,columns = ['neighbourhood_group','neighbourhood','room_type'])\nnew_df","d7ed1edd":"X = new_df.drop(['price'], axis = 1)\ny = new_df.price","3d922481":"X = new_df.drop(['id','host_id','last_review'],axis =1)","77a09b3c":"X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.3, random_state = 43)","7a7623fe":"scaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)","e0f212ea":"models =[LinearRegression(),SVR(),DecisionTreeRegressor(),RandomForestRegressor(),GradientBoostingRegressor()]\na,b,c,d,e = [],[],[],[],[]\nfor i in models:\n    model = i.fit(X_train,y_train)\n    y_pred = model.predict(X_test)\n    a.append(mean_squared_error(y_test,y_pred))\n    b.append(mean_absolute_error(y_test,y_pred))\n    c.append(r2_score(y_test,y_pred))\n    d.append(math.sqrt(mean_squared_error(y_test,y_pred)))\n    e.append(1 - (1-r2_score(y_test,y_pred)) * (len(X_test)-1)\/(len(X_test)-len(X.columns)-1))\n\npd.DataFrame([a,b,c,d,e],index = ['MSE','MAE','R2','RMSE','Adjusted R2'], columns = ['Linear Reg','SVR','Decision Tree Reg','Random Forest Reg','Gradient Boosting Regressor'])","c85bb2e3":"Adjusted R\u00b2\n> It is similar to R\u00b2 except that it increases only when the number of predictors adds value to the model.\n![](https:\/\/miro.medium.com\/max\/495\/0*WkdWEm2993yhYvUA.png)","536b2e57":"**R\u00b2 (Co-efficient of Determination)**\n> It is used for evaluating the performance of the regression model. R\u00b2 increases when the number of predictors increase. R\u00b2 can lie between 0 to 1. R\u00b2  can be negative when the model performs worse than the average regression line. \n![](https:\/\/miro.medium.com\/max\/783\/0*_Bk3m941thWlveS3.png)","64884a45":"### **Importing the libraries**","1e3cc920":"### **Import the dataset**","8270cf79":"## **I worked with this dataset to explore on with the Regression performance metrics in depth.**","75539f1c":"**Mean Absolute Error**\n> It is the absolute difference between the actual and predicted values. It is more robust to outliers.\n![](https:\/\/miro.medium.com\/proxy\/0*zX9jlpZ8k0CuEpFE.jpg)","f3ed4454":"### Dropping the columns not required and rows with NA values","aee85d61":"### Conclusion\n\n**> But when comparing to RMSE, I would say that Linear Regression performs the best as it has very less error 1.58 when compared to others. This is the first and best preferred error estimate. If you feel the need to play with the outliers, you can go with MAE.**\n\n**> Overall among all the models applied, Linear Regression and Grandient Boosting Regressor works the best with more than 99% Adjusted R\u00b2 and the Support Vector Machine Regressor works the worst with 11% Adjusted R\u00b2.**","5c317f13":"**Root Mean Squared Error**\n> RMSE is the most widely used metric for regression tasks and is the square root of the averaged squared difference between the target value and the value predicted by the model. It is preferred more in some cases because the errors are first squared before averaging which poses a high penalty on large errors. This implies that RMSE is useful when large errors are undesired.\n![](https:\/\/miro.medium.com\/max\/650\/0*TO7BkvQwtnvVzkK4.png)","24ddb029":"### **Splitting the dataset**","c50ed812":"### **Applying the Machine Learning models**","9e679429":"**Mean Squared Error**\n> It is the average of the squared difference between the actual and the predicted values.\n![](http:\/\/miro.medium.com\/max\/875\/0*aTUPK_ILg7-n0znw.jpg)"}}