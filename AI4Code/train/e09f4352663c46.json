{"cell_type":{"d0e21724":"code","1bb2f106":"code","a8461983":"code","4629f6a5":"code","5f3167aa":"code","3410c8f0":"code","be7bee86":"code","167c2248":"code","96a72359":"markdown","df81f99e":"markdown","497e505a":"markdown","456c4dc9":"markdown","0ac7029b":"markdown","726e60a9":"markdown","97628178":"markdown","24476ca0":"markdown","e23fb4c0":"markdown"},"source":{"d0e21724":"import numpy as np","1bb2f106":"np.bincount([1,2,3,3,3])\n\n# number of\n# 0s : 0\n# 1s : 1\n# 2s : 1\n# 3s : 3","a8461983":"def _replaceZeroes(data, reduction_factor=0.999999):\n  \"\"\"\n  avoid log(0) undefined error becasue `p_xis` can be 0\n  and  we have to calculate log(p_xis)\n  \n  Replace all zeroes by a number lower than the lowest\n  \"\"\"\n  min_nonzero = np.min(data[np.nonzero(data)])\n  data[data == 0] = min_nonzero - (reduction_factor*min_nonzero)\n  return data \n\n\ndef _get_pxis(X):\n    \"\"\"\n    - classes labels must be whole numbers. 0, 1, 2 ....\n        + because np.bincount works like that\n    - len(output) = num_classes\n    \"\"\"\n    num_samples = len(X)\n    count_xis   = np.bincount(X)\n    p_xis       = count_xis \/ num_samples\n    \n    return _replaceZeroes(p_xis) # avoid log error\n\n\ndef entropy(X):\n    \"\"\"\n    - X is a 1-D vec\n    \"\"\"\n    p_xis = _get_pxis(X)\n    return -1 * np.sum(p_xis * np.log2(p_xis))\n\n\ndef gini_impurity(X):\n    \"\"\"\n    - X is a 1-D vec\n    \"\"\"\n    p_xis = _get_pxis(X)\n    return 1 - np.sum(p_xis * p_xis)\n\ndef neg_lg_ps(X):\n    \"\"\"\n    - X is a 1-D vec\n    \"\"\"\n    p_xis = _get_pxis(X)\n    return -1 * np.sum(np.log2(p_xis))","4629f6a5":"#2 classes\na = [1,1,1,1,1,1,1,1,1,1] # zero randomness\nb = [0,1,1,1,0,1,1,1,1,0] # some randomness\nc = [0,1,0,1,0,1,0,1,0,1] # extreme randomness","5f3167aa":"H_a = entropy(a)\nH_b = entropy(b)\nH_c = entropy(c)\n\nG_a = gini_impurity(a)\nG_b = gini_impurity(b)\nG_c = gini_impurity(c)\n\nL_a = neg_lg_ps(a)\nL_b = neg_lg_ps(b)\nL_c = neg_lg_ps(c)\n\nprint(f\"H_a: {H_a:.2f}\\tH_b: {H_b:.2f}\\tH_c: {H_c:.2f}\")\nprint(f\"G_a: {G_a:.2f}\\tG_b: {G_b:.2f}\\tG_c: {G_c:.2f}\")\nprint(f\"L_a: {L_a:.2f}\\tL_b: {L_b:.2f}\\tL_b: {L_b:.2f}\")","3410c8f0":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\nfig, axarr = plt.subplots(1,3)\nfig.set_size_inches(15,3)\n\nx_labels = [\"zero randomness\", \"some randomness\", \"extreme randomness\"]\n\naxarr[0].barh(x_labels, [H_a, H_b, H_c])\naxarr[1].barh(x_labels, [G_a, G_b, G_c])\naxarr[2].barh(x_labels, [L_a, L_b, L_c])\n\naxarr[0].set_title(\"Entropy\")\naxarr[1].set_title(\"Gini Impurity\")\naxarr[2].set_title(\"Neg Log Sum of Ps\\nNote: works oppositely\")\n\n\nfig.tight_layout(pad=3.0)\nplt.show()","be7bee86":"#3 classes\na = [3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3] # zero randomness\nb = [0,1,1,1,2,1,1,1,3,2,1,1,1,0,1,1] # some randomness\nc = [0,1,2,3,0,1,2,3,0,1,2,3,0,1,2,3] # extreme randomness","167c2248":"H_a = entropy(a)\nH_b = entropy(b)\nH_c = entropy(c)\n\nG_a = gini_impurity(a)\nG_b = gini_impurity(b)\nG_c = gini_impurity(c)\n\nprint(f\"H_a: {H_a:.2f}\\tH_b: {H_b:.2f}\\tH_c: {H_c:.2f}\")\nprint(f\"G_a: {G_a:.2f}\\tG_b: {G_b:.2f}\\tG_c: {G_c:.2f}\")","96a72359":"# Other Interpretations\n\n- Taking probabs square instead. **Gini Impurity = $1 - \\sum{P^2_{x_1}}$** (Note subtraction from 1)\n- Taking log of probabs only\n\n# Disadvantages\n- `Log` is compute expensive. Hence use **Gini Impurity**","df81f99e":"**Simple Implementation**","497e505a":"# Entropy $(H)$\n\n> The degree of randomness in data (use $log_2$)\n\n$$H(X) = \\sum{- P(x_i) \\log_2{P(x_i)}}$$\n\nExcept for zero probability, probability makes sure that input to log is positive. Trick is to replace zero by value smaller than min_probab\n\nExample, \n\n```\na = [3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3] # zero randomness\nb = [0,1,1,1,2,1,1,1,3,2,1,1,1,0,1,1] # some randomness\nc = [0,1,2,3,0,1,2,3,0,1,2,3,0,1,2,3] # extreme randomness\n```\n\n$H(a) < H(b) < H(c)$ \n\n> H is greater for random data\n\n\n# Intuiton\n\n> We are working w\/ probabilities. and sum of probabs will be 1 because computed on same `data` (`a` or `b` or `c`)\n>\n> Find `case` where *pdf of pdfs* is extrememly skewed (ideal)\n\nFor data w\/ `3 classes`, if computed probabilites are:\n\n**Case 1:** `[0.33, 0.33, 0.33]`<br>\n**Case 2:** `[0.05, 0.05, 0.9]` or `[0.05, 0.9, 0.05]` or `[0.9, 0.05, 0.05]`\n\n- **case 1:** all same $\\Rightarrow$ Extremely random \n- **case 2:** one high, other small $\\Rightarrow$ ideal (not random)\n\n#### How to find `case` where *pdf of pdfs* is extrememly skewed (ideal) ?\n\n- method 1: use neg **sum** of `P_xi * log(P_xi)` (entropy)\n    - Greater value $\\Rightarrow$ less skewed $\\Rightarrow$ ideal\n- method 2: use `1` minus **sum** of `P_xi * P_xi` (gini impurity) \n    - Greater value $\\Rightarrow$ less skewed $\\Rightarrow$ ideal\n    \n> Log(x) where x<1 (here probabs hence always < 1) is negative hence we use `-` in `-log(x)` \n>\n> so that overall value is positive (BUT NOT EXACTLY FOR ONLY THIS REASON)\n>\n> Another reason: as $P \\uparrow \\Rightarrow value \\downarrow$","456c4dc9":"> Note how limits of scale incresse\n\nmaximum = $-\\frac{1}{4} \\bigg(lg{\\frac{1}{4^4}} \\bigg)$ = 2","0ac7029b":"> - Note scale limits\n> - NLL (Neg Log Liklihood) is estimated **ONLY** for **CORRECT CLASSES** (known while training). Here, Neg Logs is being computed for **all Probabilities**","726e60a9":"# Range of entropy\n\n### For `k` num of classes,<br>\n\n**Maximum** when Probability of each class is same.<br>\n$P_{c1} = P_{c2} = \\,...\\, = P_{ck} = \\frac{1}{k}$<br>\n\nSubstituting in entropy formula,<br>\n$\\Rightarrow \\,\\, -P_{c1} * lg(P_{c1}) - P_{c2} * lg(P_{c2}) \\,\\, - \\,\\, ... \\,\\, - P_{ck} * lg(P_{ck})$<br> \n$\\Rightarrow \\,\\, -\\frac{1}{k} * lg(\\frac{1}{k}) - \\frac{1}{k} * lg(\\frac{1}{k}) \\,\\, - \\,\\, ... \\,\\, - \\frac{1}{k} * lg(\\frac{1}{k})$<br>\n$\\Rightarrow -\\frac{1}{k}\\bigg( k lg(\\frac{1}{k}) \\bigg)$<br>\n$\\Rightarrow -\\frac{1}{k} \\bigg(lg{\\frac{1}{k^k}} \\bigg)$\n\n**Minimum** value will be 0 (as probability cannot go below it )<br>\n\n### For `2` classes,<br>\n\nmaximum = $-\\frac{1}{2} \\bigg(lg{\\frac{1}{2^2}} \\bigg)$ = 1 <br>\nminimum = 0\n","97628178":"> Scale limits change w\/ num classes\n\nFor 3 classes","24476ca0":"## Significance in machine learning\n\n- Model should be able to divide $X_{train}$ such that $H_{labels}$ is less\n    - key idea behind decision trees\n    - [Information gain](link) is used on top of entropy\n    \n    \n- Try to reduce entropy **of only correct class([NLL](LINK))** **i.e** decrease randomness in **given** class's prediction \n    - Key idea in [Multinomial Logloss \/ cross entropy loss \/ log-loss](link)\n    - *According to formula* $\\sum{- y_{true} \\log{|P_{y_{true}}|}}$ &nbsp;,&nbsp;&nbsp;&nbsp;&nbsp; *$y_{true}$ is `1` only for correct class*\n        - if $P \\uparrow \\Rightarrow L\\downarrow$ because of **neg symbol**. Hence minimizes loss.","e23fb4c0":"#### For 2 classes\n\n![image.png](attachment:image.png)\n\n[image source](https:\/\/www.quora.com\/What-is-difference-between-Gini-Impurity-and-Entropy-in-Decision-Tree)"}}