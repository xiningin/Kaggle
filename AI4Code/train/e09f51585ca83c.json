{"cell_type":{"d59dab10":"code","a17572e4":"code","344516a2":"code","2bb51b77":"code","facecc2d":"code","db3df223":"code","e81cdf1b":"code","8d968100":"code","8c13380b":"code","84365d24":"code","7dfa9dff":"code","77eaa2b7":"code","7eb387a1":"code","051ae941":"code","210c1f0a":"code","6c56e5d7":"code","d2633406":"code","e83c5581":"code","5f89cc41":"code","3718c8e9":"code","d0de10ab":"code","2e3cb3ca":"code","f1301241":"code","a5a55b3e":"code","bf601a05":"code","463a5ba1":"code","20b2b545":"code","096e38a1":"code","7d0df23f":"code","ebe1885c":"code","e54f0d56":"code","95b11aae":"code","09aa9c1c":"code","bce26ce7":"code","6d0b30e5":"code","6220500c":"code","9b25883c":"code","65946469":"code","7ab1d50b":"code","a253c536":"code","89a39635":"code","cc77f4fa":"code","62101de8":"code","db7579c5":"code","3bf83c5a":"code","dfa52f79":"code","b4ec4f23":"code","86744859":"code","9077bc25":"code","6efe7546":"code","44469296":"code","2208bc2e":"code","230a6ecc":"code","5a1ec9e6":"code","480d2f8d":"code","9f041678":"code","19844544":"code","debce707":"code","abaa2ee9":"code","3adf649d":"code","0132395e":"code","c8f6d142":"code","85ff41c1":"code","4651930a":"code","90d9a631":"code","f2fdfdaf":"code","ea106962":"code","ffe58bc0":"code","a6e6dd1b":"code","8efd7809":"code","50ebd349":"code","0e03f74b":"code","9e993734":"code","c137dce9":"code","1c2e5103":"code","68d25552":"code","ceefa76a":"code","6bb1d5be":"code","01caeb5d":"code","1f7db84b":"code","d4249665":"code","21032c29":"code","ec25f2e6":"code","20e84e3f":"code","020c90a4":"markdown","1285c0ea":"markdown","f4936911":"markdown","53a7532c":"markdown"},"source":{"d59dab10":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","a17572e4":"#importing libraries\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport matplotlib  \nimport statsmodels.formula.api as smf    \nimport statsmodels.api as sm  \nfrom sklearn.preprocessing import robust_scale\nfrom sklearn import model_selection, preprocessing, feature_selection, ensemble, linear_model, metrics, decomposition\n## for explainer    \nfrom lime import lime_tabular\nfrom mlxtend.preprocessing import minmax_scaling\nfrom scipy import stats\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import minmax_scale\nfrom sklearn.preprocessing import MaxAbsScaler\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.preprocessing import Normalizer\nfrom sklearn.preprocessing.data import QuantileTransformer\nfrom scipy.stats import skew\npd.set_option('display.max_rows', 1000)\n## for data\nimport pandas as pd\nimport numpy as np\nfrom pylab import rcParams\nrcParams['figure.figsize'] = 8, 5\n## for plotting\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nkf = KFold(n_splits = 5, random_state = 2)\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\n\n## for statistical tests\nimport scipy\nimport statsmodels.formula.api as smf\nimport statsmodels.api as sm\n## for machine learning\nfrom sklearn import model_selection, preprocessing, feature_selection, ensemble, linear_model, metrics, decomposition\n## for explainer\nfrom lime import lime_tabular\n## for machine learning\nfrom sklearn import model_selection, preprocessing, feature_selection, ensemble, linear_model, metrics, decomposition\n\nfrom IPython.core.interactiveshell import InteractiveShell \nInteractiveShell.ast_node_interactivity = \"all\"","344516a2":"df_train_main = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ndf_test_main = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\n","2bb51b77":"df_test_main.shape","facecc2d":"# outlier imputation \ndf_train_main.loc[df_train_main['PassengerId'] == 631, 'Age'] = 48","db3df223":"# Outlier detector (make worse for our algo)\nfrom collections import Counter\n\ndef detect_outliers(df,n,features):\n    \"\"\"\n    Takes a dataframe df of features and returns a list of the indices\n    corresponding to the observations containing more than n outliers according\n    to the Tukey method.\n    \"\"\"\n    outlier_indices = []\n    \n    # iterate over features(columns)\n    for col in features:\n        # 1st quartile (25%)\n        Q1 = np.percentile(df[col], 25)\n        # 3rd quartile (75%)\n        Q3 = np.percentile(df[col],75)\n        # Interquartile range (IQR)\n        IQR = Q3 - Q1\n        \n        # outlier step\n        outlier_step = 1.5 * IQR\n        \n        # Determine a list of indices of outliers for feature col\n        outlier_list_col = df[(df[col] < Q1 - outlier_step) | (df[col] > Q3 + outlier_step )].index\n        \n        # append the found outlier indices for col to the list of outlier indices \n        outlier_indices.extend(outlier_list_col)\n        \n    # select observations containing more than 2 outliers\n    outlier_indices = Counter(outlier_indices)        \n    multiple_outliers = list( k for k, v in outlier_indices.items() if v > n )\n    \n    return multiple_outliers   \n\n#detect outliers from Age, SibSp , Parch and Fare\nOutliers_to_drop = detect_outliers(df_train_main,2,[\"Age\",\"SibSp\",\"Parch\",\"Fare\"])\ndf_train_main.loc[Outliers_to_drop] # Show the outliers rows\n# Drop outliers\ndf_train_main = df_train_main.drop(Outliers_to_drop, axis = 0).reset_index(drop=True)\n","e81cdf1b":"# # Installing and loading the library\n# !pip install dabl\n\n# import dabl","8d968100":"# dabl.plot(df_train_main, target_col=\"Survived\")","8c13380b":"# ec = dabl.SimpleClassifier(random_state=0).fit(df_train_main, target_col=\"Survived\") ","84365d24":"df = pd.concat((df_train_main.loc[:,'Pclass':'Embarked'], df_test_main.loc[:,'Pclass':'Embarked']))","7dfa9dff":"#import pandas_profiling \n#profile = df.profile_report(title='Profiling Report')\n#profile.to_file(output_file=\"Data profiling.html\")","77eaa2b7":"# credit: https:\/\/www.kaggle.com\/willkoehrsen\/start-here-a-gentle-introduction. \n# One of the best notebooks on getting started with a ML problem.\n\n# def missing_values_table(df):\n#         # Total missing values\n#         mis_val = df.isnull().sum()\n        \n#         # Percentage of missing values\n#         mis_val_percent = 100 * df.isnull().sum() \/ len(df)\n        \n#         # Make a table with the results\n#         mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\n        \n#         # Rename the columns\n#         mis_val_table_ren_columns = mis_val_table.rename(\n#         columns = {0 : 'Missing Values', 1 : '% of Total Values'})\n        \n#         # Sort the table by percentage of missing descending\n#         mis_val_table_ren_columns = mis_val_table_ren_columns[\n#             mis_val_table_ren_columns.iloc[:,1] != 0].sort_values(\n#         '% of Total Values', ascending=False).round(1)\n        \n#         # Print some summary information\n#         print (\"Your selected dataframe has \" + str(df.shape[1]) + \" columns.\\n\"      \n#             \"There are \" + str(mis_val_table_ren_columns.shape[0]) +\n#               \" columns that have missing values.\")\n        \n#         # Return the dataframe with missing information\n#         return mis_val_table_ren_columns","7eb387a1":"# missing= missing_values_table(df)\n# missing","051ae941":"# !pip install missingno\n# import missingno as msno","210c1f0a":"# msno.matrix(df)","6c56e5d7":"## some feature engeneering\n\n# NaNs:\n# # Handling missing values\n\n# from sklearn.impute import SimpleImputer\n# #setting strategy to 'most frequent' to impute by the mean\n# imputer = SimpleImputer(strategy='most_frequent')# strategy can also be mean or median \n# df.iloc[:,:] = imputer.fit_transform(df)\n\n## Age tuning:\ndf['Age'] = df.groupby(\"Pclass\")['Age'].transform(lambda x: x.fillna(x.median()))\ndf[\"Age\"] = df[\"Age\"].astype(int)\n\n## Fare tuning:\ndf['Fare'] = df.groupby(\"Pclass\")['Fare'].transform(lambda x: x.fillna(x.median()))\n#df[\"Fare\"] = df[\"Fare\"].map(lambda i: np.log(i) if i > 0 else 0) #worse\n\n## Fare Zero tuning:\ndf['Fare'] = df['Fare'].replace(0, df['Fare'].median())\n\n# new Fare_cat feature:\n\ndef fare_category(fr): ## worse\n    if fr <= 18:\n        return 1\n    elif fr <= 33 and fr > 18:\n        return 2\n    elif fr <= 48 and fr > 33:\n        return 3\n    elif fr <= 63 and fr > 48:\n        return 4\n    elif fr <= 78 and fr > 63:\n        return 5\n    elif fr <= 93 and fr > 78:\n        return 6\n\n    return 7\n\n\n\n\ndf['Fare_cat'] = df['Fare'].apply(fare_category) \ndf[\"Fare_cat\"] = df[\"Fare_cat\"].astype(int)\n\n# Embarked tuning\ndf[\"Embarked\"] = df[\"Embarked\"].fillna(\"C\")\ndf[\"Embarked\"][df[\"Embarked\"] == \"S\"] = 1\ndf[\"Embarked\"][df[\"Embarked\"] == \"C\"] = 2\ndf[\"Embarked\"][df[\"Embarked\"] == \"Q\"] = 2\ndf[\"Embarked\"] = df[\"Embarked\"].astype(int)\n\n# New 'familySize' feature & dripping 2 features:\ndf['FamilySize'] = df['SibSp'] + df['Parch'] + 1\ndf['FamilySize'] = [1 if i < 5 else 0 for i in df['FamilySize']]\n\n# def family_category(n):\n#     if n == 1:\n#         return 1\n#     elif n == 2 :\n#         return 2\n#     elif n == 3  :\n#         return 3\n#     elif n >= 4  :\n#         return 3\n    \n# df['FamilySize_cat'] = df['FamilySize'].apply(family_category) \n\ndf = df.drop(['Parch'], axis=1)\ndf = df.drop(['SibSp'], axis=1)\n\n# Other\n#df = df.drop(['Ticket'], axis=1)\n\ntickets = []\nfor i in list(df.Ticket):\n    if not i.isdigit():\n        tickets.append(i.replace(\".\",\"\").replace(\"\/\",\"\").strip().split(\" \")[0])\n    else:\n        tickets.append(\"x\")\ndf[\"Ticket\"] = tickets\n\ndf = pd.get_dummies(df, columns= [\"Ticket\"], prefix = \"T\")\n\n# Convert 'Sex' variable to integer form!\ndf[\"Sex\"][df[\"Sex\"] == \"male\"] = 0\ndf[\"Sex\"][df[\"Sex\"] == \"female\"] = 1\ndf[\"Sex\"] = df[\"Sex\"].astype(int)\n\n# New Title feature\ndf['Title'] = df.Name.str.extract(' ([A-Za-z]+)\\.', expand = False)\n# Bundle rare salutations: 'Other' category\ndf['Title'] = df['Title'].replace(['Rev', 'Dr', 'Col', 'Ms', 'Mlle', 'Major', 'Countess', \n                                       'Capt', 'Dona', 'Jonkheer', 'Lady', 'Sir', 'Mme', 'Don'], 'Other')\ntitle_category = {'Mr':1, 'Miss':2, 'Mrs':3, 'Master':4, 'Other':4}\n# Mapping 'Title' to group\ndf['Title'] = df['Title'].map(title_category)\n##dropping Name feature\ndf = df.drop(['Name'], axis=1)\n\n# Cabin feature tuning:\n\n# Replace missing values with 'U' for Cabin\ndf['Cabin'] = df['Cabin'].fillna('U')\nimport re\n# Extract first letter\ndf['Cabin'] = df['Cabin'].map(lambda x: re.compile(\"([a-zA-Z]+)\").search(x).group())\ncabin_category = {'A':9, 'B':8, 'C':7, 'D':6, 'E':5, 'F':4, 'G':3, 'T':2, 'U':1}\n# Mapping 'Cabin' to group\ndf['Cabin'] = df['Cabin'].map(cabin_category)\n#df = df.drop(['Cabin'], axis=1) ## worse\n\ndf[\"Pclass\"][df[\"Pclass\"] == 3] = 4\ndf[\"Pclass\"][df[\"Pclass\"] == 2] = 5\ndf[\"Pclass\"][df[\"Pclass\"] == 1] = 6\ndf[\"Pclass\"] = df[\"Pclass\"].astype(int)\n\n## new features creation\n\ndf['FareCat_Sex'] = df['Fare_cat']*df['Sex']\n#df['Fare_Sex'] = df['Fare']*df['Sex']\ndf['Pcl_Sex'] = df['Pclass']*df['Sex']\n#df['Pcl_Age'] = df['Pclass']*df['Age']\n#df['Pcl_FareCat'] = df['Pclass']*df['Fare_cat']\ndf['Pcl_Title'] = df['Pclass']*df['Title']\n#df['Age_Sex'] = df['Age']*df['Sex']\n# df['FareCat_3'] = df['Fare_cat']**3\n# df['FareCat_2'] = df['Fare_cat']**2\n# df['FareCat_Sq'] = np.sqrt(df['Fare_cat'])\n# df['FareCat_Log'] = np.log1p(df['Fare_cat'])\n# df['Age_3'] = df['Age']**3\n# df['Age_2'] = df['Age']**2\n# df['Age_Sq'] = np.sqrt(df['Age'])\n#df['Age_Log'] = - np.log1p(df['Age'])\n# df['Cab_Title'] = df['Cabin']*df['Title']\n# df['Cab_Pcl'] = df['Cabin']*df['Pclass']\n# df['Cab_Emb'] = df['Cabin']*df['Embarked']\n# df['Cab_Fare'] = df['Cabin']*df['Fare']\n\n\n# def age_category(age): ## worse\n#     if age <= 19:\n#         return 5\n#     elif age <= 26 and age > 19:\n#         return 4\n#     elif age <= 38 and age > 26:\n#         return 3\n#     elif age <= 57 and age > 38:\n#         return 2\n#     elif age <= 76 and age > 57:\n#         return 1\n\ndef age_category(age):\n    if age <= 6:\n        return 4\n    elif age <= 15 and age > 6:\n        return 3\n\n    elif age <= 80 and age > 15:\n        return 2\n#     elif age <= 80 and age > 50:\n#         return 1\n    return 1\ndf['Age_cat'] = df['Age'].apply(age_category) \n\ndf['Age_cat_Sex'] = df['Age_cat']*df['Sex']\ndf['Age_cat_Pclass'] = df['Age_cat']*df['Pclass']\n\n\n#df = df.drop(['FamilySize'], axis=1)\n#df = df.drop(['Age'], axis=1) ##worse\n#df = df.drop(['Fare'], axis=1) ##worse\n#df['Emb_Sex'] = df['Embarked']*df['Sex'] #worse\n#df['Emb_Fare_cat'] = df['Embarked']*df['Fare_cat'] #worse\ndf['Title_Sex'] = df['Title']*df['Sex']\n#df['Fare_cat_Sex'] = df['Sex']*df['Fare_cat']\n#df['Age_Fare'] = df['Age_cat']*df['Fare_cat'] #worse\ndf['Age_Fare'] = df['Age_cat']*df['Fare_cat']\n#df['Title_Age_cat'] = df['Title']*df['Age_cat'] #worse\n#df['FamilySize_Title'] = df['FamilySize']*df['Title'] #worse\n\n# Create new feature of family size\n#df['NotAlone'] = df['FamilySize'].map(lambda s: 0 if s == 1 else 1)\n# df['SmallF'] = df['FamilySize'].map(lambda s: 1 if  s == 2  else 0)\n# df['MedF'] = df['FamilySize'].map(lambda s: 1 if 3 <= s <= 4 else 0)\n# df['LargeF'] = df['FamilySize'].map(lambda s: 0 if s >= 5 else 1)\n\ndf = df.drop(['Age'], axis=1)\n# df = df.drop(['FamilySize'], axis=1)\n# df = df.drop(['Fare'], axis=1)","d2633406":"#df['Fare'].describe()","e83c5581":"#df['Fare'].value_counts(bins=5)","5f89cc41":"#df[df['Fare']<=105]['Fare'].value_counts(bins=6)","3718c8e9":"#df[df['Fare']<=18]['Fare'].value_counts(bins=3)","d0de10ab":"#df['Age'].value_counts(bins=4)","2e3cb3ca":"#df[(df['Age']>19) & (df['Age']<=38)]['Age'].value_counts(bins=3)","f1301241":"df.describe(include='all').transpose()","a5a55b3e":"df.info()","bf601a05":"df.isnull().sum().sort_values(ascending = False).head()","463a5ba1":"for column in df.columns:\n    print(f\"{column}: {df[column].nunique()}\")\n    if df[column].nunique() < 10:\n        print(f\"{df[column].value_counts()}\")\n    print(\"====================================\")","20b2b545":"\n# numerical_features = df.select_dtypes(exclude = [\"object\"]).columns\n# num = df[numerical_features]\n# # Log transform of the skewed numerical features to lessen impact of outliers\n# # Inspired by Alexandru Papiu's script : https:\/\/www.kaggle.com\/apapiu\/house-prices-advanced-regression-techniques\/regularized-linear-models\n# # As a general rule of thumb, a skewness with an absolute value > 0.5 is considered at least moderately skewed\n# skewness = num.apply(lambda x: skew(x))\n# skewness = skewness[abs(skewness) > 0.5]\n# print(str(skewness.shape[0]) + \" skewed numerical features to log transform\")\n# skewed_features = skewness.index\n# df[skewed_features] = np.log1p(df[skewed_features])\n\n# ## scale for numerical features\n# scaler = StandardScaler()\n# df[skewed_features] = scaler.fit_transform(df[skewed_features])\n\n","096e38a1":"#creating matrices for feature selection:\nX_train = df[:df_train_main.shape[0]]\nX_test_fin = df[df_train_main.shape[0]:]\ny = df_train_main.Survived\nX_train['Y'] = y\ndf = X_train\ndf.head(20) ## DF for Model training","7d0df23f":"def bar_plot(variable):\n    \"\"\"\n        input: variable ex: \"Sex\"\n        output: bar plot & value count\n    \"\"\"\n    # get feature\n    var = df[variable]\n    # count number of categorical variable(value\/sample)\n    varValue = var.value_counts()\n    \n    # visualize\n    plt.figure(figsize = (9,3))\n    plt.bar(varValue.index, varValue)\n    plt.xticks(varValue.index, varValue.index.values)\n    plt.ylabel(\"Frequency\")\n    plt.title(variable)\n    plt.show()\n    print(\"{}: \\n {}\".format(variable,varValue))","ebe1885c":"category1 = [\"Y\",\"Sex\",\"Pclass\",\"Embarked\"]\nfor c in category1:\n    bar_plot(c)","e54f0d56":"def plot_hist(variable):\n    # for numerical features\n    plt.figure(figsize = (9,3))\n    plt.hist(df[variable], bins = 50)\n    plt.xlabel(variable)\n    plt.ylabel(\"Frequency\")\n    plt.title(\"{} distribution with hist\".format(variable))\n    plt.show()","95b11aae":"numericVar = [\"Fare\"]\nfor n in numericVar:\n    plot_hist(n)","09aa9c1c":"# Plcass vs Survived\ndf[[\"Pclass\",\"Y\"]].groupby([\"Pclass\"], as_index = False).mean().sort_values(by=\"Y\",ascending = False)","bce26ce7":"categorical_val = []\ncontinous_val = []\nfor column in df.columns:\n    print('==============================')\n    print(f\"{column} : {df[column].unique()}\")\n    if len(df[column].unique()) <= 10:\n        categorical_val.append(column)\n    else:\n        continous_val.append(column)","6d0b30e5":"# categorical_val.remove('Y')\n# df = pd.get_dummies(df, columns = categorical_val)","6220500c":"# ## sample of interactive plots\n# #importing plotly and cufflinks in offline mode\n# import cufflinks as cf\n# import plotly.offline\n# cf.go_offline()\n# cf.set_config_file(offline=False, world_readable=True)\n# df[['Pclass', 'Y']].groupby(['Pclass'], as_index=False).mean().iplot(kind='bar')\n","9b25883c":"#Correlation with output variable\ncor = df.corr()\ncor_target = (cor['Y'])\n#Selecting highly correlated features (8% level)\nrelevant_features = cor_target[(cor_target<=-0.08) | (cor_target>=0.08) ]\nrelevant_features.sort_values(ascending = False).head(60)","65946469":"##feature selection according to its correlation to key feature\nfeatures = relevant_features.keys().tolist()\ndf = df[features]\ndf.head()","7ab1d50b":"\n# Import module for dataset splitting\nfrom sklearn.model_selection import train_test_split\n\nX = df.drop('Y', axis=1)\ny = df.Y\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\nscaler = RobustScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\nX = scaler.transform(X)\n\n#Here is out local validation scheme!\ndf_train, df_test = model_selection.train_test_split(df, test_size=0.2, random_state=1)\n## print info    \nprint(\"X_train shape:\", df_train.drop(\"Y\",axis=1).shape, \"| X_test shape:\", df_test.drop(\"Y\",axis=1).shape)    \nprint(\"y_train mean:\", round(np.mean(df_train[\"Y\"]),2), \"| y_test mean:\", round(np.mean(df_test[\"Y\"]),2))    \nprint(df_train.shape[1], \"features:\", df_train.drop(\"Y\",axis=1).columns.to_list())","a253c536":"from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier, VotingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.linear_model import LogisticRegression","89a39635":"logreg = LogisticRegression()\nlogreg.fit(X_train, y_train)\nacc_log_train = round(logreg.score(X_train, y_train)*100,2) \nacc_log_test = round(logreg.score(X_test,y_test)*100,2)\nprint(\"Training Accuracy: % {}\".format(acc_log_train))\nprint(\"Testing Accuracy: % {}\".format(acc_log_test))","cc77f4fa":"random_state = 42\nclassifier = [DecisionTreeClassifier(random_state = random_state),\n             SVC(random_state = random_state),\n             RandomForestClassifier(random_state = random_state),\n             LogisticRegression(random_state = random_state),\n             KNeighborsClassifier()]\n\ndt_param_grid = {\"min_samples_split\" : range(10,500,20),\n                \"max_depth\": range(1,20,2)}\n\nsvc_param_grid = {\"kernel\" : [\"rbf\"],\n                 \"gamma\": [0.001, 0.01, 0.1, 1],\n                 \"C\": [1,10,50,100,200,300,1000]}\n\nrf_param_grid = {\"max_features\": [1,3,10],\n                \"min_samples_split\":[2,3,10],\n                \"min_samples_leaf\":[1,3,10],\n                \"bootstrap\":[False],\n                \"n_estimators\":[100,300],\n                \"criterion\":[\"gini\"]}\n\nlogreg_param_grid = {\"C\":np.logspace(-3,3,7),\n                    \"penalty\": [\"l1\",\"l2\"]}\n\nknn_param_grid = {\"n_neighbors\": np.linspace(1,19,10, dtype = int).tolist(),\n                 \"weights\": [\"uniform\",\"distance\"],\n                 \"metric\":[\"euclidean\",\"manhattan\"]}\nclassifier_param = [dt_param_grid,\n                   svc_param_grid,\n                   rf_param_grid,\n                   logreg_param_grid,\n                   knn_param_grid]","62101de8":"cv_result = []\nbest_estimators = []\nfor i in range(len(classifier)):\n    clf = GridSearchCV(classifier[i], param_grid=classifier_param[i], cv = StratifiedKFold(n_splits = 10), scoring = \"accuracy\", n_jobs = -1,verbose = 1)\n    clf.fit(X_train,y_train)\n    cv_result.append(clf.best_score_)\n    best_estimators.append(clf.best_estimator_)\n    print(cv_result[i])","db7579c5":"cv_results = pd.DataFrame({\"Cross Validation Means\":cv_result, \"ML Models\":[\"DecisionTreeClassifier\", \"SVM\",\"RandomForestClassifier\",\n             \"LogisticRegression\",\n             \"KNeighborsClassifier\"]})\n\ng = sns.barplot(\"Cross Validation Means\", \"ML Models\", data = cv_results)\ng.set_xlabel(\"Mean Accuracy\")\ng.set_title(\"Cross Validation Scores\")","3bf83c5a":"votingC = VotingClassifier(estimators = [(\"dt\",best_estimators[0]),\n                                        (\"rfc\",best_estimators[2]),\n                                        (\"lr\",best_estimators[3])],\n                                        voting = \"soft\", n_jobs = -1)\nvotingC = votingC.fit(X_train, y_train)\nprint(accuracy_score(votingC.predict(X_test),y_test))","dfa52f79":"# ## optimized params below\n\n# from sklearn.model_selection import GridSearchCV\n# # set model. max_iter - Maximum number of iterations taken for the solvers to converge.\n# lr = LogisticRegression(random_state = 64, max_iter = 1000)\n\n# # set parameters values we are going to check\n# optimization_dict = {'class_weight':['balanced', None],\n#                      'solver':['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],\n#                      'C': [0.01, 0.05, 0.07, 0.1, 0.5, 1, 2, 4, 5, 10, 15, 20, 50, 100, 200, 500, 1000]\n#                      }\n# # set GridSearchCV parameters\n# GS = GridSearchCV(lr, optimization_dict, \n#                      scoring='accuracy', n_jobs = -1, cv = 10)\n\n# # use training features\n# GS.fit(X_train, y_train)\n\n# # print result\n# print(GS.best_score_)\n# print(GS.best_params_)","b4ec4f23":"from sklearn.linear_model import LogisticRegression\n# set best parameters to the model (as per optimized earlier)\nlr_tuned_model =  LogisticRegression(solver = 'newton-cg',\n                                     C = 200,\n                                     random_state = 64,\n                                     class_weight = None,\n                                     n_jobs = -1)","86744859":"# train our model with training data\nimport numpy as np\nlr_tuned_model.fit(X_train, y_train)\n\n# calculate importances based on coefficients.\nimportances = abs(lr_tuned_model.coef_[0])\nimportances = 100.0 * (importances \/ importances.max())\n# sort \nindices = np.argsort(importances)[::-1]\n\n# Rearrange feature names so they match the sorted feature importances\nnames = [df.columns[i] for i in indices]\n\n# visualize\nplt.figure(figsize = (12, 5))\nsns.set_style(\"whitegrid\")\nchart = sns.barplot(x = names, y = importances[indices])\nplt.xticks(\n    rotation=45, \n    horizontalalignment='right',\n    fontweight='light'  \n)\nplt.title('Logistic regression. Feature importance')\nplt.tight_layout()","9077bc25":"## USE W\/O SCALER\n# # Calculating and Displaying importance using the eli5 library\n# import eli5\n# from eli5.sklearn import PermutationImportance\n\n# perm = PermutationImportance(lr_tuned_model, random_state=1).fit(X_test,y_test)\n# eli5.show_weights(perm, feature_names = X_test.columns.tolist())","6efe7546":"# from sklearn.model_selection import GridSearchCV\n\n# # set model\n# rf = RandomForestClassifier(oob_score = True, n_jobs = -1, random_state = 64)\n# # create a dictionary of parameters values we want to try\n# optimization_dict = {'criterion':['gini', 'entropy'],\n#                      'n_estimators': [50,100, 200, 500],\n#                      'max_depth': [3, 7, 10, 15],\n#                      'min_samples_split': [6, 7, 8],\n#                      'min_samples_leaf': [2, 3, 4, 6]\n#                      }\n\n# # set GridSearchCV parameters\n# GS = GridSearchCV(rf, optimization_dict, \n#                      scoring='accuracy', verbose = 1, n_jobs = -1, cv = 5)\n\n# # use training data\n# GS.fit(X_train, y_train)\n\n# # print best score and best parameters combination\n# print(GS.best_score_)\n# print(GS.best_params_)\n","44469296":"# set best parameters to the model\nrf_tuned_model =  RandomForestClassifier(criterion = 'gini',\n                                       n_estimators = 50,\n                                       max_depth = 15,\n                                       min_samples_split = 6,\n                                       min_samples_leaf = 3,\n                                       max_features = 'auto',\n                                       oob_score = True,\n                                       random_state = 64,\n                                       n_jobs = -1)\n# train model using training dataset\nrf_tuned_model.fit(X_train, y_train)\n\n# Calculate feature importances\nimportances = rf_tuned_model.feature_importances_\n\n# Visualize Feature Importance\n# Sort feature importances in descending order\nindices = np.argsort(importances)[::-1]\n\n# Rearrange feature names so they match the sorted feature importances\nnames = [df.columns[i] for i in indices]\n\nplt.figure(figsize = (12, 5))\nsns.set_style(\"whitegrid\")\nchart = sns.barplot(x = names, y=importances[indices])\nplt.xticks(\n    rotation=45, \n    horizontalalignment='right',\n    fontweight='light'  \n)\nplt.title('Random forest. Feature importance')\nplt.tight_layout()","2208bc2e":"\n# from sklearn.model_selection import GridSearchCV\n# # set model\n# xgb_model = XGBClassifier(random_state = 64)\n# # create a dictionary of parameters values we want to try\n# optimization_dict = {'n_estimators': [1000, 1500, 2000],\n#                      'max_depth': [4, 6, 8],\n#                      'learning_rate': [0.1, 0.5, 0.9],\n#                      'gamma': [1, 5, 7],\n#                      'min_child_weight':[5, 7, 9],\n#                      'subsample': [0.8, 0.9, 1.0]\n#                      }\n# # set GridSearchCV parameters\n# GS = GridSearchCV(xgb_model, optimization_dict, \n#                      scoring='accuracy', verbose = 1, n_jobs = -1, cv = 5)\n\n# # use training data\n# GS.fit(X_train, y_train)\n# print(GS.best_score_)\n# print(GS.best_params_)\n","230a6ecc":"# set model with best parameters\nxgb =  XGBClassifier(n_estimators = 1000,\n                               max_depth = 6,\n                               learning_rate = 0.1,\n                               gamma = 1,\n                               min_child_weight = 5,\n                               subsample = 1.0,\n                               random_state = 64)\n# train model with training dataset\nxgb.fit(X_train, y_train)\n\n# Calculate feature importances\nimportances = xgb.feature_importances_\n\n# Visualize Feature Importance\n# Sort feature importances in descending order\nindices = np.argsort(importances)[::-1]\n\n# Rearrange feature names so they match the sorted feature importances\nnames = [df.columns[i] for i in indices]\n\nplt.figure(figsize = (12, 5))\nsns.set_style(\"whitegrid\")\nchart = sns.barplot(x = names, y=importances[indices])\nplt.xticks(rotation=45, horizontalalignment='right', fontweight='light')\nplt.title('XGBoost. Feature importance')\nplt.tight_layout()","5a1ec9e6":"# ## call model RandomizedSearchCV\n# gb = ensemble.GradientBoostingClassifier()\n# ## define hyperparameters combinations to try\n# optimization_dict = {'learning_rate':[0.1,0.05,0.01],      #weighting factor for the corrections by new trees when added to the model\n# 'n_estimators':[750,1000,1250],  #number of trees added to the model\n# 'max_depth':[5,6,7],    #maximum depth of the tree\n# 'min_samples_split':[8,10,20],    #sets the minimum number of samples to split\n# 'min_samples_leaf':[5,7,9],     #the minimum number of samples to form a leaf\n# 'max_features':[4,5,6],     #square root of features is usually a good starting point\n# 'subsample':[0.85,0.9,0.95]}       #the fraction of samples to be used for fitting the individual base learners. Values lower than 1 generally lead to a reduction of variance and an increase in bias.\n# ## random search\n# RS = model_selection.RandomizedSearchCV(gb, \n#        param_distributions = optimization_dict, n_iter=1000, \n#        scoring=\"accuracy\", verbose = 1, n_jobs = -1, cv = 5).fit(X_train, y_train)\n# print(\"Best Model parameters:\", RS.best_params_)\n# print(\"Best Model mean accuracy:\", RS.best_score_)\n# model = RS.best_estimator_\n","480d2f8d":"gb = ensemble.GradientBoostingClassifier(subsample= 0.95, n_estimators= 1000, min_samples_split= 10, min_samples_leaf= 5, max_features= 6, max_depth= 5, learning_rate= 0.1)\n# train model with training dataset\ngb.fit(X_train, y_train)\n\n# Calculate feature importances\nimportances = gb.feature_importances_\n\n# Visualize Feature Importance\n# Sort feature importances in descending order\nindices = np.argsort(importances)[::-1]\n\n# Rearrange feature names so they match the sorted feature importances\nnames = [df.columns[i] for i in indices]\n\nplt.figure(figsize = (12, 5))\nsns.set_style(\"whitegrid\")\nchart = sns.barplot(x = names, y=importances[indices])\nplt.xticks(rotation=45, horizontalalignment='right', fontweight='light')\nplt.title('GBoost. Feature importance')\nplt.tight_layout()","9f041678":"models = []\n# add our tuned models into list\nmodels.append(('Logistic Regression', lr_tuned_model))\nmodels.append(('Random Forest', rf_tuned_model))\nmodels.append(('XGBoost', xgb))\nmodels.append(('GBoost', gb))\n#models.append(('KNN', sq))\n\n\nresults = []\nnames = []\n\n# evaluate each model in turn\nfor name, model in models:\n    kfold = model_selection.KFold(n_splits=10, shuffle = True, random_state = 64)\n    cv_results = model_selection.cross_val_score(model, X, \n                                                 y, \n                                                 cv = 10, scoring = 'accuracy')\n    results.append(cv_results)\n    names.append(name)\n    # print mean accuracy and standard deviation\n    print(\"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std()))","19844544":"from keras.models import Sequential\nfrom keras.layers import Dense, Activation, Dropout, BatchNormalization\nimport keras\nfrom keras.optimizers import SGD\nimport graphviz\nfrom numpy.random import seed\n\nimport tensorflow","debce707":"X_train = np.array(X_train)\nX_test = np.array(X_test)\ny_train = np.array(y_train)\ny_test = np.array(y_test)","abaa2ee9":"# Initialising the NN\nsq = Sequential()\n\n# layers\nsq.add(Dense(units = 8, kernel_initializer = 'he_normal', activation = 'relu', input_dim = 18))\nsq.add(Dropout(0.50))\n# sq.add(BatchNormalization())\nsq.add(Dense(units = 32, kernel_initializer = 'he_normal', activation = 'relu'))\nsq.add(Dropout(0.50))\n# sq.add(BatchNormalization())\nsq.add(Dense(units = 64, kernel_initializer = 'he_normal', activation = 'relu'))\nsq.add(Dropout(0.50))\nsq.add(Dense(units = 1, kernel_initializer = 'he_normal', activation = 'sigmoid'))\n\n#optimizers list\n#optimizers['SGD', 'RMSprop', 'Adagrad', 'Adadelta', 'Adam', 'Adamax', 'Nadam']\n\n# Compiling the ANN\nsq.compile(optimizer='Adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n\n\n\n# Train the ANN\nr = sq.fit(X_train, y_train, batch_size = 9, epochs = 50, verbose=0,  validation_data=(X_test, y_test))\nscores = sq.evaluate(X_test, y_test, batch_size=100)\nprint(\"%s: %.2f%%\" % (sq.metrics_names[1], scores[1]*100))","3adf649d":"# def create_model(lyrs=[8], act='linear', opt='Adam', dr=0.0):\n    \n#     # set random seed for reproducibility\n#     seed(42)\n#     tensorflow.random.set_seed(42)\n    \n#     model = Sequential()\n    \n#     # create first hidden layer\n#     model.add(Dense(lyrs[0], input_dim=X_train.shape[1], activation=act))\n    \n#     # create additional hidden layers\n#     for i in range(1,len(lyrs)):\n#         model.add(Dense(lyrs[i], activation=act))\n    \n#     # add dropout, default is none\n#     model.add(Dropout(dr))\n    \n#     # create output layer\n#     model.add(Dense(1, activation='sigmoid'))  # output layer\n    \n#     model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n    \n#     return model","0132395e":"# model = create_model()\n# print(model.summary())","c8f6d142":"# r = model.fit(X_train, y_train, epochs=100, batch_size=32, validation_split=0.2, verbose=0)\n# val_acc = np.mean(r.history['accuracy'])\n# print(\"\\n%s: %.2f%%\" % ('accuracy', val_acc*100))","85ff41c1":"plt.figure(figsize=(10, 6))\nplt.plot(r.history['loss'], label='loss')\nplt.plot(r.history['val_loss'], label='val_loss')\nplt.legend()","4651930a":"plt.plot(r.history['accuracy'])\nplt.plot(r.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.show()","90d9a631":"# from keras.wrappers.scikit_learn import KerasClassifier\n# from sklearn.model_selection import GridSearchCV\n# # create model\n# model = KerasClassifier(build_fn=create_model, verbose=0)\n\n# # define the grid search parameters\n# batch_size = [16, 32, 64, 128]\n# epochs = [25, 50, 100, 150, 200, 250, 300]\n# param_grid = dict(batch_size=batch_size, epochs=epochs)\n\n# # search the grid\n# grid = GridSearchCV(estimator=model, \n#                     param_grid=param_grid,\n#                     cv=3,\n#                     verbose=2,\n#                    n_jobs=-1)  # include n_jobs=-1 if you are using CPU\n\n# grid_result = grid.fit(X_train, y_train)","f2fdfdaf":"# # summarize results\n# print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n# means = grid_result.cv_results_['mean_test_score']\n# stds = grid_result.cv_results_['std_test_score']\n# params = grid_result.cv_results_['params']\n# for mean, stdev, param in zip(means, stds, params):\n#     print(\"%f (%f) with: %r\" % (mean, stdev, param))","ea106962":"# # create model\n# model = KerasClassifier(build_fn=create_model, epochs=150, batch_size=16, verbose=0)\n\n# # define the grid search parameters\n# optimizer = ['SGD', 'RMSprop', 'Adagrad', 'Adadelta', 'Adam', 'Nadam']\n# param_grid = dict(opt=optimizer)\n\n# # search the grid\n# grid = GridSearchCV(estimator=model, param_grid=param_grid, verbose=2)\n# grid_result = grid.fit(X_train, y_train)","ffe58bc0":"# # summarize results\n# print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n# means = grid_result.cv_results_['mean_test_score']\n# stds = grid_result.cv_results_['std_test_score']\n# params = grid_result.cv_results_['params']\n# for mean, stdev, param in zip(means, stds, params):\n#     print(\"%f (%f) with: %r\" % (mean, stdev, param))","a6e6dd1b":"# seed(42)\n# tensorflow.random.set_seed(42)\n\n# # create model\n# model = KerasClassifier(build_fn=create_model, \n#                         epochs=150, batch_size=16, verbose=0)\n\n# # define the grid search parameters\n# layers = [(8),(10),(10,5),(12,6),(12,8,4)]\n# param_grid = dict(lyrs=layers)\n\n# # search the grid\n# grid = GridSearchCV(estimator=model, param_grid=param_grid, verbose=2)\n# grid_result = grid.fit(X_train, y_train)","8efd7809":"# print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n# means = grid_result.cv_results_['mean_test_score']\n# stds = grid_result.cv_results_['std_test_score']\n# params = grid_result.cv_results_['params']\n# for mean, stdev, param in zip(means, stds, params):\n#     print(\"%f (%f) with: %r\" % (mean, stdev, param))","50ebd349":"# # create model\n# model = KerasClassifier(build_fn=create_model, \n#                         epochs=150, batch_size=16, verbose=0)\n\n# # define the grid search parameters\n# drops = [0.0, 0.01, 0.05, 0.1, 0.2, 0.5]\n# param_grid = dict(dr=drops)\n# grid = GridSearchCV(estimator=model, param_grid=param_grid, verbose=2)\n# grid_result = grid.fit(X_train, y_train)","0e03f74b":"# # summarize results\n# print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n# means = grid_result.cv_results_['mean_test_score']\n# stds = grid_result.cv_results_['std_test_score']\n# params = grid_result.cv_results_['params']\n# for mean, stdev, param in zip(means, stds, params):\n#     print(\"%f (%f) with: %r\" % (mean, stdev, param))","9e993734":"# # create final model\n# model = create_model(lyrs=[10,5], dr=0.0, opt='RMSprop')\n\n# print(model.summary())","c137dce9":"# # train model on full train set, with 80\/20 CV split\n# training = model.fit(X_train, y_train, epochs=150, batch_size=16, \n#                      validation_split=0.2, verbose=0)\n\n# # evaluate the model\n# scores = model.evaluate(X_train, y_train)\n# print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))","1c2e5103":"# #save result\n# df_test_main['Survived'] = sq.predict_classes(X_test_fin)\n# df_test_main[['PassengerId', 'Survived']].to_csv('submission.csv', index = False)\n# print(\"Your submission was successfully saved!\")\n","68d25552":"# #save result for neuro model\n# y_pred = sq.predict_classes(X_test_fin)\n# output = pd.DataFrame({'PassengerId': df_test_main.PassengerId, 'Survived': y_pred[:, 0]})\n# output.to_csv('my_submission.csv', index=False)\n# print(\"Your submission was successfully saved!\")\n","ceefa76a":"from keras.wrappers.scikit_learn import KerasClassifier\nn_folds = 5\ncv_score_lg = cross_val_score(estimator=lr_tuned_model, X=X_train, y=y_train, cv=n_folds, n_jobs=-1)\ncv_score_xgb = cross_val_score(estimator=xgb, X=X_train, y=y_train, cv=n_folds, n_jobs=-1)\ncv_score_gb = cross_val_score(estimator=gb, X=X_train, y=y_train, cv=n_folds, n_jobs=-1)\ncv_score_rf = cross_val_score(estimator=rf_tuned_model, X=X_train, y=y_train, cv=n_folds, n_jobs=-1)\ncv_score_sq = cross_val_score(estimator=KerasClassifier(build_fn=sq, batch_size=16, epochs=50, verbose=0),\n                                 X=X_train, y=y_train, cv=n_folds, n_jobs=-1)","6bb1d5be":"cv_result = {'lr': cv_score_lg, 'xgb': cv_score_xgb, 'gb': cv_score_gb, 'rf': cv_score_rf, 'sq': cv_score_sq}\ncv_data = {model: [score.mean(), score.std()] for model, score in cv_result.items()}\ncv_df = pd.DataFrame(cv_data, index=['Mean_accuracy', 'Variance'])\ncv_df","01caeb5d":"fig = plt.figure(figsize=(6,4))\nplt.boxplot(results)\nplt.title('Algorithm Comparison')\nplt.xticks([1,2,3,4], names)\nplt.show()","1f7db84b":"## for best model xgb\nxgb.fit(X_train, y_train)\npredicted_prob = xgb.predict_proba(X_test)[:,1]\npredicted = xgb.predict(X_test)\n## Accuracy  AUC\naccuracy = metrics.accuracy_score(y_test, predicted)\nauc = metrics.roc_auc_score(y_test, predicted_prob)\nprint(\"Accuracy (overall correct predictions):\",  round(accuracy,2))\nprint(\"Auc:\", round(auc,2))\n    \n## Precision e Recall\nrecall = metrics.recall_score(y_test, predicted)\nprecision = metrics.precision_score(y_test, predicted)\nprint(\"Recall (all 1s predicted right):\", round(recall,2))\nprint(\"Precision (confidence when predicting a 1):\", round(precision,2))\nprint(\"Detail:\")\nprint(metrics.classification_report(y_test, predicted, target_names=[str(i) for i in np.unique(y_test)]))\n\n","d4249665":"classes = np.unique(y_test)\nfig, ax = plt.subplots()\ncm = metrics.confusion_matrix(y_test, predicted, labels=classes)\nsns.heatmap(cm, annot=True, fmt='d', cmap=plt.cm.Blues, cbar=False)\nax.set(xlabel=\"Pred\", ylabel=\"True\", title=\"Confusion matrix\")\nax.set_yticklabels(labels=classes, rotation=0)\nplt.show()","21032c29":"cross_val_score(gb, X_test, y_test, cv = kf).mean()","ec25f2e6":"gb = ensemble.GradientBoostingClassifier(n_estimators=1000, learning_rate = 0.5, min_samples_split = 8, min_samples_leaf = 4,   max_features=6, max_depth = 6, random_state = 0)\ngb.fit(X_train, y_train)\npredicted_prob = gb.predict_proba(X_test)[:,1]\npredicted = gb.predict(X_test)\n## Accuray e AUC\naccuracy = metrics.accuracy_score(y_test, predicted)\nauc = metrics.roc_auc_score(y_test, predicted_prob)\nprint(\"Accuracy (overall correct predictions):\",  round(accuracy,2))\nprint(\"Auc:\", round(auc,2))\n    \n## Precision e Recall\nrecall = metrics.recall_score(y_test, predicted)\nprecision = metrics.precision_score(y_test, predicted)\nprint(\"Recall (all 1s predicted right):\", round(recall,2))\nprint(\"Precision (confidence when predicting a 1):\", round(precision,2))\nprint(\"Detail:\")\nprint(metrics.classification_report(y_test, predicted, target_names=[str(i) for i in np.unique(y_test)]))","20e84e3f":"cross_val_score(gb, X_test, y_test, cv = kf).mean()","020c90a4":"<div class=\"alert alert-block alert-success\">  \nUse green box only when necessary like to display links to related content.  \n<\/div>","1285c0ea":"<div class=\"alert alert-block alert-warning\">  \n<b>Example:<\/b> Yellow Boxes are generally used to include additional examples or mathematical formulas.  \n<\/div>","f4936911":"<div class=\"alert alert-block alert-info\">\n<b>Tip:<\/b> Use blue boxes (alert-info) for tips and notes. \nIf it\u2019s a note, you don\u2019t have to include the word \u201cNote\u201d.\n<\/div>","53a7532c":"<div class=\"alert alert-block alert-danger\">  \nIt is good to avoid red boxes but can be used to alert users to not delete some important part of code etc.   \n<\/div>"}}