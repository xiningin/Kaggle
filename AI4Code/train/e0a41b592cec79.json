{"cell_type":{"eddfd680":"code","950c37ae":"code","97ea1c15":"code","8be1f6ea":"code","8a897394":"code","961b4fa9":"code","aaa0b1a5":"code","bf085378":"code","ae198772":"code","68956ba0":"code","23935e99":"code","69e38db0":"code","0a8709bf":"code","88b65462":"code","7acef913":"code","8c674044":"code","09074f50":"code","643459f3":"code","1df789a3":"code","6648c3bf":"code","3a46db61":"code","b1e5a40c":"code","f45a2787":"code","78c20d67":"code","65318179":"code","9e2da373":"code","e891762b":"code","e6a5f0f8":"code","8c431404":"code","01ca7052":"code","cb8f4bad":"code","b25ee12c":"code","fd9f65a8":"code","05296d20":"code","b22c7017":"code","07db5a9d":"code","99a5f658":"markdown","1d0d869e":"markdown","95645d50":"markdown","3cb5c4f9":"markdown","678e6657":"markdown","e0397aa3":"markdown","1d4523f9":"markdown","e6b932f4":"markdown","bd6b3816":"markdown","63718561":"markdown","17c664f4":"markdown","be45c501":"markdown","3f6b0009":"markdown","09984286":"markdown","0eeb43a8":"markdown","ed97078b":"markdown","26db3985":"markdown"},"source":{"eddfd680":"# packages\n\n# standard\nimport numpy as np\nimport pandas as pd\nimport time\n\n# plots\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom statsmodels.graphics.mosaicplot import mosaic\n\n# machine learning tools\nimport h2o\nfrom h2o.estimators import H2ORandomForestEstimator\nfrom h2o.estimators import H2OGradientBoostingEstimator","950c37ae":"# load data + first glance\ndf_train = pd.read_csv('..\/input\/tabular-playground-series-mar-2021\/train.csv')\ndf_test = pd.read_csv('..\/input\/tabular-playground-series-mar-2021\/test.csv')\ndf_sub = pd.read_csv('..\/input\/tabular-playground-series-mar-2021\/sample_submission.csv')\n\n# first glance (training data)\ndf_train.head()","97ea1c15":"# dimensions\ndf_train.shape","8be1f6ea":"df_train.info()","8a897394":"# basic stats\nprint(df_train.target.value_counts())\ndf_train.target.value_counts().plot(kind='bar')\nplt.grid()\nplt.show()","961b4fa9":"features_num = ['cont0', 'cont1', 'cont2', 'cont3', \n                'cont4', 'cont5', 'cont6', 'cont7',\n                'cont8', 'cont9', 'cont10']","aaa0b1a5":"# plot distribution of numerical features\nfor f in features_num:\n    plt.figure(figsize=(8,4))\n    df_train[f].plot(kind='hist', bins=100)\n    plt.title(f)\n    plt.grid()\n    plt.show()","bf085378":"corr_pearson = df_train[features_num].corr(method='pearson')\ncorr_spearman = df_train[features_num].corr(method='spearman')\n\nfig = plt.figure(figsize = (10,8))\nsns.heatmap(corr_pearson, annot=True, cmap='RdYlGn', vmin=-1, vmax=+1)\nplt.title('Pearson Correlation')\nplt.show()\n\nfig = plt.figure(figsize = (10,8))\nsns.heatmap(corr_spearman, annot=True, cmap='RdYlGn', vmin=-1, vmax=+1)\nplt.title('Spearman Correlation')\nplt.show()","ae198772":"# example of scatter plot - we pick pair having highest (Pearson) correlation\nsns.jointplot(data=df_train, x='cont1', y='cont2', kind='hex')\nplt.show()","68956ba0":"features_cat = ['cat0', 'cat1', 'cat2', 'cat3',\n                'cat4', 'cat5', 'cat6', 'cat7',\n                'cat8', 'cat9', 'cat10', 'cat11',\n                'cat12', 'cat13', 'cat14', 'cat15',\n                'cat16', 'cat17', 'cat18']","23935e99":"# plot distribution of categorical features\nfor f in features_cat:\n    plt.figure(figsize=(14,4))\n    df_train[f].value_counts().plot(kind='bar')\n    plt.title(f)\n    plt.grid()\n    plt.show()","69e38db0":"# count different values\/levels\ncat10_freq = df_train.cat10.value_counts()\nprint(cat10_freq)\n\n# and plot frequency distribution using log scale\nfig, ax = plt.subplots(figsize=(12,4))\nax.plot(np.log10(cat10_freq))\nax.xaxis.set_major_locator(plt.MaxNLocator(20)) # reduce number of x-axis labels\nplt.title('cat10 - Frequencies')\nplt.ylabel('log10(Frequency)')\nplt.grid()\nplt.show()","0a8709bf":"# evaluate mean of target by level\ncat10_target = df_train.groupby(['cat10']).agg({\n    'target' : ['mean','count']})\n# ... and sort by frequency of level\ncat10_target = cat10_target.sort_values([('target','count')], ascending=False)\n\n# plot mean of target by level; bubble area ~ frequency\nfig, ax = plt.subplots(figsize=(12,6))\nax.scatter(cat10_target.index, cat10_target[('target','mean')],\n           s=2*np.sqrt(cat10_target[('target','count')]),\n           alpha = 0.5)\nax.xaxis.set_major_locator(plt.MaxNLocator(20)) # reduce number of x-axis labels\nplt.title('cat10 - Average target by level (bubble area ~ frequency)')\nplt.grid()\nplt.show()","88b65462":"# let's give it a try: define levels to be kept\nn_keep = 201\ncat10_keep = cat10_freq[0:n_keep].index.tolist()\nprint(cat10_keep)","7acef913":"# add new column with reduced number of levels\ndf_train['cat10_reduced'] = df_train.cat10.where(df_train.cat10.isin(cat10_keep), '_OTHER_')\ndf_train.cat10_reduced.value_counts()","8c674044":"# check frequency of _OTHER_ category\ndf_train[df_train.cat10_reduced=='_OTHER_'].cat10_reduced.value_counts()","09074f50":"# same for test set!\ndf_test['cat10_reduced'] = df_test.cat10.where(df_test.cat10.isin(cat10_keep), '_OTHER_')\ndf_test.cat10_reduced.value_counts()","643459f3":"# update feature list accordingly\nfeatures_cat = ['cat0', 'cat1', 'cat2', 'cat3',\n                'cat4', 'cat5', 'cat6', 'cat7',\n                'cat8', 'cat9', 'cat10_reduced', 'cat11',\n                'cat12', 'cat13', 'cat14', 'cat15',\n                'cat16', 'cat17', 'cat18']","1df789a3":"# plot target vs binned numerical features using mosaic plot\nplt_para_save = plt.rcParams['figure.figsize'] # remember plot settings\n\nfor f in features_num:\n    \n    # add binned version of each numerical feature first\n    new_var = f + '_bin'\n    df_train[new_var] = pd.qcut(df_train[f], 10)\n    \n    # then create mosaic plot\n    plt.rcParams[\"figure.figsize\"] = (16,7) # increase plot size for mosaics\n    mosaic(df_train, [new_var, 'target'], title='Target vs ' + f + ' [binned]')\n    plt.show()\n    \n# reset plot size again\nplt.rcParams['figure.figsize'] = plt_para_save","6648c3bf":"# plot target vs features using mosaic plot\nplt_para_save = plt.rcParams['figure.figsize'] # remember plot settings\n\nfor f in features_cat:\n    plt.rcParams[\"figure.figsize\"] = (16,7) # increase plot size for mosaics\n    mosaic(df_train, [f, 'target'], title='Target vs ' + f + ' [binned]')\n    plt.show()\n    \n# reset plot size again\nplt.rcParams['figure.figsize'] = plt_para_save","3a46db61":"# select predictors\npredictors = features_num + features_cat\nprint('Number of predictors: ', len(predictors))\nprint(predictors)","b1e5a40c":"# start H2O\nh2o.init(max_mem_size='12G', nthreads=4) # Use maximum of 12 GB RAM and 4 cores","f45a2787":"# upload data frames in H2O environment\nt1 = time.time()\ntrain_hex = h2o.H2OFrame(df_train)\ntest_hex = h2o.H2OFrame(df_test)\nt2 = time.time()\nprint('Elapsed time [s]: ', np.round(t2-t1,2))\n\n# force categorical target\ntrain_hex['target'] = train_hex['target'].asfactor()","78c20d67":"# define Gradient Boosting model\nn_cv = 10\n\nfit_1 = H2OGradientBoostingEstimator(ntrees = 200,\n                                     max_depth=6,\n                                     min_rows=100,\n                                     learn_rate=0.05, # default: 0.1\n                                     sample_rate=1,\n                                     col_sample_rate=0.5,\n                                     nfolds=n_cv,\n                                     seed=999)","65318179":"# train model\nt1 = time.time()\nfit_1.train(x=predictors,\n            y='target',\n            training_frame=train_hex)\nt2 = time.time()\nprint('Elapsed time [s]: ', np.round(t2-t1,2))","9e2da373":"# show cross validation metrics\nfit_1.cross_validation_metrics_summary()","e891762b":"# show scoring history - training vs cross validations\nfor i in range(n_cv):\n    cv_model_temp = fit_1.cross_validation_models()[i]\n    df_cv_score_history = cv_model_temp.score_history()\n    my_title = 'CV ' + str(1+i) + ' - Scoring History [AUC]'\n    plt.scatter(df_cv_score_history.number_of_trees,\n                y=df_cv_score_history.training_auc, \n                c='blue', label='training')\n    plt.scatter(df_cv_score_history.number_of_trees,\n                y=df_cv_score_history.validation_auc, \n                c='darkorange', label='validation')\n    plt.title(my_title)\n    plt.xlabel('Number of Trees')\n    plt.ylabel('AUC')\n    plt.ylim(0.8,1)\n    plt.legend()\n    plt.grid()\n    plt.show()","e6a5f0f8":"# basic version\nfit_1.varimp_plot(-1)","8c431404":"# variable importance using shap values => see direction as well as severity of feature impact\nt1 = time.time()\nfit_1.shap_summary_plot(train_hex);\nt2 = time.time()\nprint('Elapsed time [s]: ', np.round(t2-t1,2))","01ca7052":"# training performance\nperf_train = fit_1.model_performance(train=True)\nperf_train.plot()","cb8f4bad":"# cross validation performance\nperf_cv = fit_1.model_performance(xval=True)\nperf_cv.plot()","b25ee12c":"# predict on test set (extract probabilities only)\npred_test = fit_1.predict(test_hex)['p1']\npred_test = pred_test.as_data_frame().p1","fd9f65a8":"# let's quickly check the frequency of those \"exotic\" levels\ndf_test.cat10.value_counts()[['BU','BW','CA','DG','EJ','JM','KE','KM']]","05296d20":"# plot test set predictions (probabilities)\nplt.figure(figsize=(7,5))\nplt.hist(pred_test, bins=100)\nplt.title('Predictions on Test Set')\nplt.grid()\nplt.show()","b22c7017":"# prepare submission\ndf_sub.target = pred_test\ndf_sub.head(10)","07db5a9d":"# save to file for submission\ndf_sub.to_csv('submission.csv', index=False)","99a5f658":"<a id='6'><\/a>\n# Predict on Test Set and prepare Submission","1d0d869e":"<a id='2'><\/a>\n# Numerical Features","95645d50":"#### Well, \"cat10\" has lots of different values, this might require a closer look...","3cb5c4f9":"### Variable Importance","678e6657":"#### => The \"cat10\" feature does not only challenge us with its many levels but also has a few levels that occur only in the test set. By using cat10_reduced now instead of cat10 this problem is no longer relevant...","e0397aa3":"<a id='1'><\/a>\n# Target Exploration","1d4523f9":"<a id='5'><\/a>\n# Build model","e6b932f4":"# Table of Contents\n* [Target Exploration](#1)\n* [Numerical Features](#2)\n* [Categorical Features](#3)\n* [Target vs Features](#4)\n* [Build Model](#5)\n* [Predict on Test Set and prepare Submission](#6)","bd6b3816":"#### => It could be beneficial to group the less frequent levels (e. g. right of \"CP\") into a group \"other\".","63718561":"## Numerical Features","17c664f4":"## Feature Correlations","be45c501":"## Categorical Features","3f6b0009":"### This time we have a categorical (binary) target!","09984286":"## Check performance on training data \/ cross validations","0eeb43a8":"<a id='3'><\/a>\n# Categorical Features","ed97078b":"#### We are lucky, no missing values!","26db3985":"<a id='4'><\/a>\n# Target vs Features"}}