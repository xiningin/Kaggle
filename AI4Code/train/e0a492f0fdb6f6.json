{"cell_type":{"d2ebfd95":"code","ae8d992c":"code","f1979e0c":"code","571b258b":"code","42ee7319":"code","a22c4a10":"code","885f156a":"code","3327d08b":"code","d203ef16":"code","3b7a65e2":"code","fb427788":"code","e66b4eb6":"code","725f9a3e":"code","e9ab1e53":"code","406a68a1":"code","263c0c8f":"code","7065daf1":"code","2888f21b":"code","5122328e":"code","207e5556":"code","6e6865e3":"code","d5ad91f6":"code","e5e3088c":"code","410f018e":"code","1adc0a5f":"code","c17b40d1":"code","bd41c41f":"code","6d4fb27c":"code","9f708d9b":"code","35c40f7e":"code","efd4adbc":"markdown","85c9fcb2":"markdown","6a758c5c":"markdown","43705983":"markdown","bd0d0fe2":"markdown","d8a82664":"markdown","bddea458":"markdown","3cedb94f":"markdown","bb04b4f2":"markdown","1e5ea721":"markdown","12cd8d17":"markdown","98f97349":"markdown","58887668":"markdown","92a46ac5":"markdown","4bbba59a":"markdown","73a3193f":"markdown","9dadd4c0":"markdown","62ec45e2":"markdown","1a54121d":"markdown","dbd84943":"markdown","05d598a5":"markdown","26c01b53":"markdown","ae646326":"markdown","dcfe8d76":"markdown","7eae6e96":"markdown","a331b002":"markdown","de42c83c":"markdown","7d657b40":"markdown","46b8029e":"markdown","aae84d60":"markdown","c4452a22":"markdown","14f84cad":"markdown","2881b5c4":"markdown","3ec592f9":"markdown","54f049e2":"markdown"},"source":{"d2ebfd95":"import gc\nimport os\nimport logging\nimport datetime\nimport warnings\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport lightgbm as lgb\nfrom tqdm import tqdm_notebook\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import roc_auc_score, roc_curve\nfrom sklearn.model_selection import StratifiedKFold\nwarnings.filterwarnings('ignore')\n\nimport json\nimport subprocess\nfrom pandas.io.json import json_normalize\n\nfrom IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = \"all\"\n\npd.options.display.max_columns = 999\npd.options.display.max_rows = 999","ae8d992c":"PATH=\"..\/input\/\"\nos.listdir(PATH)","f1979e0c":"json_path = PATH + 'text.data.jsonl.xz'","571b258b":"import lzma\n\ndef sample(json_path, sample_size=0.25, num_lines=None, drop=[]):\n    \"\"\"\n    Sample json file\n    \"\"\"\n        \n    def file_len(fname):\n        \"\"\"\n        Given filename, return number of lines\n        Credits: \u00d3lafur Waage, https:\/\/stackoverflow.com\/a\/845069\/3096104\n        \"\"\"\n        p = subprocess.Popen(['wc', '-l', fname], stdout=subprocess.PIPE, \n                                                  stderr=subprocess.PIPE)\n        result, err = p.communicate()\n        if p.returncode != 0:\n            raise IOError(err)\n        return int(result.strip().split()[0])\n    \n    sample_size = sample_size * file_len(json_path)\n    \n    with lzma.open(json_path) as in_file:\n        lines = 0\n        sample = []\n        line = in_file.readline()\n        if num_lines:\n            while (line and lines < num_lines):\n                case = json.loads(str(line, 'utf8'))\n                        \n                # drop elements \n                for to_drop in drop:\n                    temp = case\n                    for obj in to_drop.split('.')[:-1]:\n                        if obj in temp:\n                            temp = temp[obj]\n                    if to_drop.split('.')[-1] in temp:\n                        del temp[to_drop.split('.')[-1]]\n                                \n                sample.append(case)\n                lines = lines + 1\n                line = in_file.readline()\n        else:\n            while (line and lines < sample_size):\n                case = json.loads(str(line, 'utf8'))\n                # drop elements \n                for to_drop in drop:\n                    temp = case\n                    for obj in to_drop.split('.')[:-1]:\n                        if obj in temp:\n                            temp = temp[obj]\n                    if to_drop.split('.')[-1] in temp:\n                        del temp[to_drop.split('.')[-1]]\n                        \n                sample.append(case)\n                lines = lines + 1\n                line = in_file.readline()\n        return sample","42ee7319":"illinois_sample = sample(json_path, sample_size=0.01) # sample the data","a22c4a10":"illinois_sample = json_normalize(illinois_sample)\nillinois_sample.head(2)\nillinois_sample.shape","885f156a":"def expand(df):\n    \"\"\"\n    Expand list and dict columns of Pandas dataframe.\n    \n    Given a df where certain columns contains list of JSON object or python dictionary, return expanded df.\n    \n    Columns with NAN values are not expanded.\n    \"\"\"\n    \n    # expand list\n    for col in df:\n        # check if all columns contain dictionary df\n        is_list = df[col].apply(lambda x: isinstance(x, list)).any()\n        has_na = df[col].isna().any()\n        \n        # if it's a list, expand the df, merge and delete legacy column\n        if is_list and not has_na:\n            temp = df[col].apply(pd.Series)\n            temp.columns = temp.columns.astype(str)\n            temp.columns = col + '_' + temp.columns\n            df = df.drop(columns=[col]).join(temp, lsuffix='.' + col )\n            \n    # expand dict\n    for col in df:\n        # check if all columns contain dictionary df\n        is_dict = df[col].apply(lambda x: isinstance(x, dict)).any()\n        has_na = df[col].isna().any()\n        \n        # if it's a dict, expand the df, merge and delete legacy column\n        if is_dict and not has_na:       \n            normalized = json_normalize(df[col])\n            normalized.columns = col + '.' + normalized.columns\n            df = df.drop(columns=[col]).join(normalized, how='outer')\n    return df\n\nillinois_sample = expand(illinois_sample)\nillinois_sample.head(1)","3327d08b":"def lowercase(df):\n    \"\"\"Return a df with all object columns lowercased\"\"\"\n    object_columns = df.select_dtypes('object')\n    for col in object_columns:\n        df[col] = df[col].str.lower()\n    return df\n\nillinois_sample = lowercase(illinois_sample)\nillinois_sample.head(1)","d203ef16":"illinois_sample = illinois_sample.loc[:,~illinois_sample.isna().all()]\nillinois_sample.head(1)","3b7a65e2":"def is_list(series):\n    \"\"\"Return true if all columns of pd.Series are list\"\"\"\n    return series.apply(lambda x: isinstance(x, list)).any()\n\ndef is_dict(series):\n    \"\"\"Return true if all columns of pd.Series are dict\"\"\"\n    return series.apply(lambda x: isinstance(x, dict)).any()\n\ndef columns_list_dict(df):\n    \"\"\"Return all columns of df that are either list or dict\"\"\"\n    columns_list_dict = []\n    for col in df:\n        series = df[col]\n        if is_list(series) or is_dict(series):\n            columns_list_dict.append(col)\n    return columns_list_dict\n\nunhashable_columns = columns_list_dict(illinois_sample)\nunhashable_columns","fb427788":"illinois_sample, hashable_data = illinois_sample.drop(columns=unhashable_columns), illinois_sample[unhashable_columns]","e66b4eb6":"illinois_sample.nunique()","725f9a3e":"print(\"'id' is unique?:\", illinois_sample['id'].is_unique)","e9ab1e53":"illinois_sample = illinois_sample.loc[ :, (illinois_sample.nunique() != 1) ]","406a68a1":"illinois_sample.head(1)\nillinois_sample.loc[0, 'casebody.data.head_matter']","263c0c8f":"illinois_whole = sample(json_path, sample_size=1, drop=['casebody.data.head_matter', 'casebody.data.opinions']) \n\nillinois_whole = json_normalize(illinois_whole)\n\nillinois_whole.head(2)","7065daf1":"illinois_whole = expand(illinois_whole)","2888f21b":"illinois_whole = lowercase(illinois_whole)","5122328e":"illinois_whole = illinois_whole.loc[:,~illinois_whole.isna().all()]","207e5556":"illinois_whole = illinois_whole.loc[ :, (illinois_whole.nunique() != 1) ]","6e6865e3":"illinois_whole.head(1)\n\nprint(\"There are \", illinois_whole.shape[0], \"law cases\")","d5ad91f6":"illinois_whole['court.name'].value_counts()","e5e3088c":"illinois_whole[['court.name', 'court.name_abbreviation']].drop_duplicates()","410f018e":"illinois_whole = illinois_whole.drop(columns=['court.name_abbreviation'])","1adc0a5f":"def plot_nan(df):\n    from matplotlib import cm\n\n    some_nan = df.isnull().sum() != 0\n\n    nan_percentage = df.isna().sum() \/ df.shape[0]\n    nan_percentage.sort_values(inplace=True)\n\n    cmap = cm.get_cmap('coolwarm')\n    colors = cmap(nan_percentage)\n\n    nan_percentage.plot(kind='bar', figsize=(20,8), title=\"Percentage of NaN\", color=colors, fontsize=14)\n\nplot_nan(illinois_whole)","c17b40d1":"attorney_columns = [col for col in illinois_whole.columns.tolist() if 'attorneys' in col]\nillinois_whole['all_attorneys'] = illinois_whole[attorney_columns].apply(lambda x: x.dropna().tolist(), axis=1)\nillinois_whole = illinois_whole.drop(columns=attorney_columns)","bd41c41f":"judges_columns = [col for col in illinois_whole.columns.tolist() if 'judges' in col]\nillinois_whole['all_judges'] = illinois_whole[judges_columns].apply(lambda x: x.dropna().tolist(), axis=1)\nillinois_whole = illinois_whole.drop(columns=judges_columns)","6d4fb27c":"plot_nan(illinois_whole)","9f708d9b":"assigned_parties = illinois_whole.loc[:,(illinois_whole.isna().sum() > 0)].dropna()\n\nassigned_parties","35c40f7e":"illinois_whole.head(10)","efd4adbc":"For a better interpretability, we drop all columns with constant values","85c9fcb2":"<h1><center><font size=\"6\">Caselaw EDA Illinois<\/font><\/center><\/h1>\n\n<h2><center><font size=\"5\">Part 1: data parsing<\/font><\/center><\/h2>\n\n<h3><center><font size=\"4\">Dataset used: Caselaw Dataset (Illinois)<\/font><\/center><\/h3>\n\n<br>\n\n# <a id='0'>Content<\/a>\n\n- <a href='#1'>1. Introduction<\/a> \n- <a href='#2'>2. Sample data<\/a>\n - <a href='#21'>2.1 Data parsing<\/a>\n - <a href='#22'>2.2 Data cleaning<\/a>\n - <a href='#23'>2.3 Data exploration<\/a>\n- <a href='#3'>3. Whole data<\/a>  \n - <a href='#31'>3.1 Data parsing and cleaning<\/a>\n - <a href='#31'>3.2 Data exploration and data understanding<\/a>\n- <a href='#3'>4. Conclusion<\/a>  ","6a758c5c":"Merge all judges columns:","43705983":"## Unique values\n\nPandas is not always the best tools to analzye text-structured data. For example ```pd.DataFrame.nunique()``` does not work when columns have unhashable data type like dict or list.\n\nIn our specific case, there may be some columns with unhashable data:","bd0d0fe2":"There are two available files, one in text and the other in xml format. For this analysis we will use the *text json*  file. \n","d8a82664":"## <a id='23'>2.3 Data Exploration<\/a>  \n\nEventually, data are in a human-readable format and ready to be interpreted. \n\n1% of the file data correspond to approximately 18k cases, already a good staring point to explore and understand the data.","bddea458":"we are now left with a single dataframe containing all columns except *'casebody.data.head_matter'* and  *'casebody.data.opinions'*.","3cedb94f":"Drop columns full of NAN:","bb04b4f2":"There are only three entries with assigned parties columns. ","1e5ea721":"### Not assigned values","12cd8d17":"# <a id='3'>3. Whole data <\/a>  \n\n## <a id='31'>3.1 Data parsing and cleaning<\/a>\n\nThe *head_matter* columns contains a lot of text. We deduce that the large file size is due to that column. \n\nTo being able to load all the data into a single dataframe, temporarly dropping two columns that may contains many text-data.","98f97349":"We can check again the distribution of NAN values:","58887668":"After parsing, loading and cleaning the data we were able to produce a single pandas dataframe.\n\nWe made the basis to start a more sophisticated analysis of the different cases. In the next kernel, we will continue cleaning the data with the final goal of finding relations between different cases and multiple parties.\n\n__Feedback, suggestions and questions in comments are more than welcome!__\n","92a46ac5":"apparently, even the parties columns have many missing data. Let's try to figure out more insights:","4bbba59a":"## Analysis of a single case\n\nLet's analyze in the details a single case, in order to understand the different columns value.","73a3193f":"#### Load data   ","9dadd4c0":"There are still some columns that have cells with JSON-like data. Apparently, json_normalize is not able to expand list of JSON object.\n\nWe define another helper function that extract this kind of data. This  function is far to be universal. __Suggestions and advices in comments are more than welcome!__\n\nThe operation takes around 30 seconds to be executed.","62ec45e2":"## <a id='22'>2.2 Data Cleaning<\/a>  \n\nTransform all text in lowercase:\n","1a54121d":"Now, we can load the dataframe. To this end we use a pandas built-in function , *json_normalize*, that normalize semi-structured JSON data into a flat table.","dbd84943":"Court Name abbreviation","05d598a5":"Because there are just a few of *unhashable columns*, we drop it, temporarly. In future we may want to come back and explore also this data.","26c01b53":"drop constant columns","ae646326":"# <a id='1'>1. Introduction<\/a>  \n\nThe[ Library Innovation Lab](http:\/\/https:\/\/lil.law.harvard.edu\/) at the Harvard Law School Library recently published the full corpus of available U.S. case law. Two jurisdiction, Arkansas and Illinois are freely available online. The following Kaggle kernel analyze the content of the __Illinois__ dataframe.\n\nAs we will see, the dataframe is not yet particularly clean.The goal of this first notebook is to define helper functions to make it easier to load the data into a Pandas dataframe, making it easier to proceed with further analysis. \n\nThis is the first part of a series of notebooks. In future kernels, we will try to understand the link between the different parties of the cases and analyze the cases content to find pattern and insights.\n","dcfe8d76":"### Not assigned values: parties","7eae6e96":"Since there is a one-to-one corrispondency between *court name* and *court name abbreviation*, we drop such data.","a331b002":"drop nan columns:","de42c83c":"We sample 1% of the data:","7d657b40":"# <a id='4'>4. Conclusion<\/a>  ","46b8029e":"as we did before, we expand all columns:","aae84d60":"lowercase all text:","c4452a22":"The size of the data is very large, more than 900 MB. It's computationally infeasible to load the whole dataframe into pandas, we need therefore an alternative solution.\n\nAs we can read [here](https:\/\/case.law\/bulk\/), the compressed files contains multiple json objects, one for each line. \n\nWe define an helper function that sample the data.","14f84cad":"## <a id='32'>3.2 Data exploration and data understanding<\/a>\n\n### Court Name","2881b5c4":"# <a id='2'>2. Sample data <\/a>  \n\n## <a id='21'>2.1 Data parsing <\/a>  \n\nLoad packages:","3ec592f9":"\nColumns with *judges* or *attorneys* have many missing data. For a better data interpretability, we merge all columns with a similar contents in a single colum. \n\nMerge all attorneys columns:","54f049e2":"different *court name* are available in the df, the most relevant are:\n\n- supreme court: highest court in the United States.\n- appellate court: there are 13 appellate courts that sit below the U.S. Supreme Court, and they are called the U.S. Courts of Appeals\n- circiut court: the core concept of circuit courts requires judges to travel to different locales to ensure wide visibility and understanding of cases in a region\n\nsources: [Circuit court](https:\/\/en.wikipedia.org\/wiki\/Circuit_court) and [Court Role and Structure](https:\/\/www.uscourts.gov\/about-federal-courts\/court-role-and-structure) \n"}}