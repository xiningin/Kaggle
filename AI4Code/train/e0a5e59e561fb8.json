{"cell_type":{"8815fd6e":"code","80aa8b3b":"code","5b9a8f8c":"code","ba68676f":"code","1025665f":"code","90077314":"code","56e31c4e":"code","d27316ed":"code","8cad6b66":"code","c63cd92e":"code","d1e87ef9":"code","3c22f03c":"code","4278b020":"code","0977e7a2":"code","3a66414e":"code","75bc55c8":"code","04276909":"code","f6fa97f6":"code","e577595b":"code","c70c6c64":"code","09ccbc1c":"code","fb4d4a3c":"code","ba5cbca7":"code","d321005b":"code","9d435b80":"code","6c9e536d":"markdown","1d534960":"markdown","c67ccdba":"markdown","379408da":"markdown","7014843d":"markdown","ae9a393d":"markdown","db27d714":"markdown","5f1e2ec4":"markdown","c1a0f847":"markdown","c89839c2":"markdown","2b35973b":"markdown","a694b623":"markdown","10080b0e":"markdown","cee7b510":"markdown","bb2769ef":"markdown","c84b0ba9":"markdown","9615af06":"markdown","69b3c279":"markdown","0f921f4f":"markdown","fb7c921a":"markdown","f9582e68":"markdown"},"source":{"8815fd6e":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nplt.rcParams.update({'font.size': 14})\n\nimport re\n\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\ndistricts_info = pd.read_csv(\"..\/input\/learnplatform-covid19-impact-on-digital-learning\/districts_info.csv\")\nproducts_info = pd.read_csv(\"..\/input\/learnplatform-covid19-impact-on-digital-learning\/products_info.csv\")\n","80aa8b3b":"# Engagement data structure\npd.set_option('display.max_colwidth', None)  \nengagement_dict = {\n    \"time\":\t\"date in \\\"YYYY-MM-DD\\\"\",\n    \"lp_id\" :\t\"The unique identifier of the product\",\n    \"pct_access\" :\t\"Percentage of students in the district have at least one page-load event of a given product and on a given day\",\n    \"engagement_index\"\t: \"Total page-load events per one thousand students of a given product and on a given day\"\n}\npd.DataFrame.from_dict(engagement_dict, orient=\"index\", columns=[\"description\"])","5b9a8f8c":"# title District csv structure\ndistricts_info_desc = {\n    \"district_id\"\t: \"The unique identifier of the school district\",\n    \"state\" :\t\"The state where the district resides in\",\n    \"locale\" :\t\"NCES locale classification that categorizes U.S. territory into four types of areas: City, Suburban, Town, and Rural. See Locale Boundaries User's Manual for more information.\",\n    \"pct_black\/hispanic\" :\t\"Percentage of students in the districts identified as Black or Hispanic based on 2018-19 NCES data\",\n    \"pct_free\/reduced\" : \t\"Percentage of students in the districts eligible for free or reduced-price lunch based on 2018-19 NCES data\",\n    \"countyconnectionsratio\" :\t\"ratio (residential fixed high-speed connections over 200 kbps in at least one direction\/households) based on the county level data from FCC From 477 (December 2018 version). See FCC data for more information.\",\n    \"pptotalraw\" :\t\"Per-pupil total expenditupd.set_option('display.max_rows', 500)re (sum of local and federal expenditure) from Edunomics Lab's National Education Resource Database on Schools (NERD$) project. The expenditure data are school-by-school, and we use the median value to represent the expenditure of a given school district.\"\n}\npd.DataFrame.from_dict(districts_info_desc, orient=\"index\", columns=[\"description\"])","ba68676f":"# District info visualization\ndistricts_info.dtypes\nfor column in districts_info.columns:\n  print(column)\n  if column != 'district_id':\n    y = districts_info[column].value_counts()\n    ax = sns.barplot(y.index, y.values)\n    ax.tick_params(axis='x', rotation=90)\n    plt.show(ax)","1025665f":"# product info structure\nproducts_info_desc = {\n    \"LP ID\"\t: \"The unique identifier of the product\",\n    \"URL\":\t\"Web Link to the specific product\",\n\"Product Name\" :\t\"Name of the specific product\",\n\"Provider\/Company Name\" :\t\"Name of the product provider\",\n\"Sector(s)\"\t: \"Sector of education where the product is used\",\n\"Primary Essential Function\" :\t\"The basic function of the product. There are two layers of labels here. Products are first labeled as one of these three categories: LC = Learning & Curriculum, CM = Classroom Management, and SDO = School & District Operations. Each of these categories have multiple sub-categories with which the products were labeled\"\n}\npd.DataFrame.from_dict(products_info_desc, orient=\"index\", columns=[\"description\"])#@title Product info structure\nproducts_info_desc = {\n    \"LP ID\"\t: \"The unique identifier of the product\",\n    \"URL\":\t\"Web Link to the specific product\",\n\"Product Name\" :\t\"Name of the specific product\",\n\"Provider\/Company Name\" :\t\"Name of the product provider\",\n\"Sector(s)\"\t: \"Sector of education where the product is used\",\n\"Primary Essential Function\" :\t\"The basic function of the product. There are two layers of labels here. Products are first labeled as one of these three categories: LC = Learning & Curriculum, CM = Classroom Management, and SDO = School & District Operations. Each of these categories have multiple sub-categories with which the products were labeled\"\n}\npd.DataFrame.from_dict(products_info_desc, orient=\"index\", columns=[\"description\"])","90077314":"products_info.head()","56e31c4e":"# Products info visualization\nproducts_info.dtypes\nfor column in products_info.columns:\n  print(column)\n  if column not in ['LP ID','URL','Product Name', 'Provider\/Company Name']:\n    y = products_info[column].value_counts()\n    ax = sns.barplot(y.index, y.values)\n    ax.tick_params(axis='x', rotation=90)\n    # plt.rcparams # figure size in inches\n    plt.rcParams['figure.figsize'] = 11.7,8.27\n    plt.show(ax)","d27316ed":"# Dropping Districts with NaN States\ndistricts_info = districts_info[districts_info.state.notna()].reset_index(drop=True)","8cad6b66":"# One-Hot Encoding the Product Sectors\ntemp_sectors = products_info['Sector(s)'].str.get_dummies(sep=\"; \")\ntemp_sectors.columns = [f\"sector_{re.sub(' ', '', c)}\" for c in temp_sectors.columns]\nproducts_info = products_info.join(temp_sectors)\nproducts_info.drop(\"Sector(s)\", axis=1, inplace=True)\n\ndel temp_sectors","c63cd92e":"products_info['primary_function_main'] = products_info['Primary Essential Function'].apply(lambda x: x.split(' - ')[0] if x == x else x)\nproducts_info['primary_function_sub'] = products_info['Primary Essential Function'].apply(lambda x: x.split(' - ')[1] if x == x else x)\n\n# Synchronize similar values\nproducts_info['primary_function_sub'] = products_info['primary_function_sub'].replace({'Sites, Resources & References' : 'Sites, Resources & Reference'})\nproducts_info.drop(\"Primary Essential Function\", axis=1, inplace=True)","d1e87ef9":"districts_info.shape","3c22f03c":"PATH = '..\/input\/learnplatform-covid19-impact-on-digital-learning\/engagement_data' \n\ntemp = []\n\nfor district in districts_info.district_id.unique():\n    df = pd.read_csv(f'{PATH}\/{district}.csv', index_col=None, header=0)\n    df[\"district_id\"] = district\n    temp.append(df)\n    \n    \nengagement = pd.concat(temp)\nengagement = engagement.reset_index(drop=True)","4278b020":"engagement.head()","0977e7a2":"fig, ax = plt.subplots(1, 1, figsize=(8,4))\n\nsns.histplot(engagement.groupby('district_id').time.nunique(), bins=30)\nax.set_title('Unique Days of Engagement Data per District')\nplt.show()","3a66414e":"# Delete previously created engagement dataframe and create a new one\ndel engagement\n\ntemp = []\n\nfor district in districts_info.district_id.unique():\n    df = pd.read_csv(f'{PATH}\/{district}.csv', index_col=None, header=0)\n    df[\"district_id\"] = district\n    if df.time.nunique() == 366:\n        temp.append(df)\n\nengagement = pd.concat(temp)\nengagement = engagement.reset_index(drop=True)\n\n# Only consider districts with full 2020 engagement data\ndistricts_info = districts_info[districts_info.district_id.isin(engagement.district_id.unique())].reset_index(drop=True)\nproducts_info = products_info[products_info['LP ID'].isin(engagement.lp_id.unique())].reset_index(drop=True)","75bc55c8":"engagement.time = engagement.time.astype('datetime64[ns]')","04276909":"# Number of school districts per state\nus_state_abbrev = {\n    'Alabama': 'AL',\n    'Alaska': 'AK',\n    'American Samoa': 'AS',\n    'Arizona': 'AZ',\n    'Arkansas': 'AR',\n    'California': 'CA',\n    'Colorado': 'CO',\n    'Connecticut': 'CT',\n    'Delaware': 'DE',\n    'District Of Columbia': 'DC',\n    'Florida': 'FL',\n    'Georgia': 'GA',\n    'Guam': 'GU',\n    'Hawaii': 'HI',\n    'Idaho': 'ID',\n    'Illinois': 'IL',\n    'Indiana': 'IN',\n    'Iowa': 'IA',\n    'Kansas': 'KS',\n    'Kentucky': 'KY',\n    'Louisiana': 'LA',\n    'Maine': 'ME',\n    'Maryland': 'MD',\n    'Massachusetts': 'MA',\n    'Michigan': 'MI',\n    'Minnesota': 'MN',\n    'Mississippi': 'MS',\n    'Missouri': 'MO',\n    'Montana': 'MT',\n    'Nebraska': 'NE',\n    'Nevada': 'NV',\n    'New Hampshire': 'NH',\n    'New Jersey': 'NJ',\n    'New Mexico': 'NM',\n    'New York': 'NY',\n    'North Carolina': 'NC',\n    'North Dakota': 'ND',\n    'Northern Mariana Islands':'MP',\n    'Ohio': 'OH',\n    'Oklahoma': 'OK',\n    'Oregon': 'OR',\n    'Pennsylvania': 'PA',\n    'Puerto Rico': 'PR',\n    'Rhode Island': 'RI',\n    'South Carolina': 'SC',\n    'South Dakota': 'SD',\n    'Tennessee': 'TN',\n    'Texas': 'TX',\n    'Utah': 'UT',\n    'Vermont': 'VT',\n    'Virgin Islands': 'VI',\n    'Virginia': 'VA',\n    'Washington': 'WA',\n    'West Virginia': 'WV',\n    'Wisconsin': 'WI',\n    'Wyoming': 'WY'\n}\n\ndistricts_info['state_abbrev'] = districts_info['state'].replace(us_state_abbrev)\ndistricts_info_by_state = districts_info['state_abbrev'].value_counts().to_frame().reset_index(drop=False)\ndistricts_info_by_state.columns = ['state_abbrev', 'num_districts']\n\nfig = go.Figure()\nlayout = dict(\n    title_text = \"Number of Available School Districts per State\",\n    geo_scope='usa',\n)\n\nfig.add_trace(\n    go.Choropleth(\n        locations=districts_info_by_state.state_abbrev,\n        zmax=1,\n        z = districts_info_by_state.num_districts,\n        locationmode = 'USA-states', # set of locations match entries in `locations`\n        marker_line_color='white',\n        geo='geo',\n        colorscale=px.colors.sequential.Teal, \n    )\n)\n            \nfig.update_layout(layout)   \nfig.show()","f6fa97f6":"# locale and pp_total_raw of districts\ndistricts_info.pp_total_raw.unique()\ntemp = districts_info.groupby('locale').pp_total_raw.value_counts().to_frame()\ntemp.columns = ['amount']\n\ntemp = temp.reset_index(drop=False)\n\ntemp = temp.pivot(index='locale', columns='pp_total_raw')['amount']\ntemp = temp[['[4000, 6000[', '[6000, 8000[', '[8000, 10000[', '[10000, 12000[',\n       '[12000, 14000[', '[14000, 16000[', '[16000, 18000[', \n       '[18000, 20000[', '[20000, 22000[', '[22000, 24000[', ]]\n\n\nfig, ax = plt.subplots(1, 2, figsize=(24,4))\n\nsns.countplot(data=districts_info, x='locale', ax=ax[0], palette='GnBu')\n\nsns.heatmap(temp, annot=True,  cmap='GnBu', ax=ax[1])\nax[1].set_title('Heatmap of Districts According To locale and pp_total_raw')\nplt.show()","e577595b":"# Histogram of remaining data points in districts data\nfig, ax = plt.subplots(2, 2, figsize=(16,8))\n\nsns.countplot(data=districts_info, x='pct_black\/hispanic', order=['[0, 0.2[', '[0.2, 0.4[', '[0.4, 0.6[', '[0.6, 0.8[','[0.8, 1[', ], palette='GnBu', ax=ax[0,0])\nax[0,0].set_ylim([0,135])\nsns.countplot(data=districts_info, x='pct_free\/reduced', order=['[0, 0.2[', '[0.2, 0.4[', '[0.4, 0.6[', '[0.6, 0.8[','[0.8, 1[', ], palette='GnBu', ax=ax[0,1])\nax[0,1].set_ylim([0,135])\n\nsns.countplot(data=districts_info, x='county_connections_ratio', palette='GnBu', ax=ax[1,0])\nax[1,0].set_ylim([0,135])\nsns.countplot(data=districts_info, x='pp_total_raw', order=['[4000, 6000[', '[6000, 8000[', '[8000, 10000[', '[10000, 12000[',\n       '[12000, 14000[', '[14000, 16000[', '[16000, 18000[', \n       '[18000, 20000[', '[20000, 22000[', '[22000, 24000[', ], palette='GnBu', ax=ax[1,1])\nax[1,1].set_ylim([0,135])\nax[1,1].set_xticklabels(ax[1,1].get_xticklabels(), rotation=90)\n\nplt.tight_layout()\nplt.show()","c70c6c64":"def replace_ranges_pct(range_str):\n    if range_str == '[0, 0.2[':\n        return 0.1\n    elif range_str == '[0.2, 0.4[':\n        return 0.3\n    elif range_str == '[0.4, 0.6[':\n        return 0.5\n    elif range_str == '[0.6, 0.8[':\n        return 0.7\n    elif range_str == '[0.8, 1[':\n        return 0.9\n    else:\n        return np.nan\n    \ndef replace_ranges_raw(range_str):\n    if range_str == '[4000, 6000[':\n        return 5000\n    elif range_str == '[6000, 8000[':\n        return 7000\n    elif range_str == '[8000, 10000[':\n        return 9000\n    elif range_str == '[10000, 12000[':\n        return 11000\n    elif range_str ==  '[12000, 14000[':\n        return 13000\n    elif range_str ==  '[14000, 16000[':\n        return 15000\n    elif range_str == '[16000, 18000[':\n        return 17000\n    elif range_str ==  '[18000, 20000[':\n        return 19000\n    elif range_str ==  '[20000, 22000[':\n        return 21000\n    elif range_str ==  '[22000, 24000[':\n        return 21000\n    else: \n        return np.nan\n    \ndistricts_info['pct_black_hispanic_num'] = districts_info['pct_black\/hispanic'].apply(lambda x: replace_ranges_pct(x))\ndistricts_info['pct_free_reduced_num'] = districts_info['pct_free\/reduced'].apply(lambda x: replace_ranges_pct(x))\ndistricts_info['pp_total_raw_num'] = districts_info['pp_total_raw'].apply(lambda x: replace_ranges_raw(x))\n\ndef plot_state_mean_for_var(col):\n    temp = districts_info.groupby('state_abbrev')[col].mean().to_frame().reset_index(drop=False)\n\n    fig = go.Figure()\n    layout = dict(\n        title_text = f\"Mean {col} per State\",\n        geo_scope='usa',\n    )\n\n    fig.add_trace(\n        go.Choropleth(\n            locations=temp.state_abbrev,\n            zmax=1,\n            z = temp[col],\n            locationmode = 'USA-states', # set of locations match entries in `locations`\n            marker_line_color='white',\n            geo='geo',\n            colorscale=px.colors.sequential.Teal, \n        )\n    )\n\n    fig.update_layout(layout)   \n    fig.show()\n\nplot_state_mean_for_var('pct_black_hispanic_num')\nplot_state_mean_for_var('pct_free_reduced_num')\nplot_state_mean_for_var('pp_total_raw_num')","09ccbc1c":"# Products category and sub category histogram\nfig, ax = plt.subplots(1, 2, figsize=(16,4))\nsns.countplot(data=products_info, x='primary_function_main', palette ='GnBu', ax=ax[0])\nax[0].set_title('Main Categories in Primary Functions')\n\nsns.countplot(data=products_info[products_info.primary_function_main == 'LC'], x='primary_function_sub', palette ='GnBu', ax=ax[1])\nax[1].set_title('Sub-Categories in Primary Function LC')\nax[1].set_xticklabels(ax[1].get_xticklabels(), rotation=90)\nplt.show()","fb4d4a3c":"virtual_classroom_lp_id = products_info[products_info.primary_function_sub == 'Virtual Classroom']['LP ID'].unique()\n\n# Remove weekends from the dataframe\nengagement['weekday'] = pd.DatetimeIndex(engagement['time']).weekday\nengagement_without_weekends = engagement[engagement.weekday < 5]\n\n# Figure 1\nf, ax = plt.subplots(nrows=1, ncols=1, figsize=(24, 6))\nfor virtual_classroom_product in virtual_classroom_lp_id:\n    temp = engagement_without_weekends[engagement_without_weekends.lp_id == virtual_classroom_product].groupby('time').pct_access.mean().to_frame().reset_index(drop=False)\n    sns.lineplot(x=temp.time, y=temp.pct_access, label=products_info[products_info['LP ID'] == virtual_classroom_product]['Product Name'].values[0])\nplt.legend()\nplt.show()\n\n# Figure 2\nf, ax = plt.subplots(nrows=1, ncols=1, figsize=(24, 6))\nfor virtual_classroom_product in virtual_classroom_lp_id:\n    temp = engagement_without_weekends[engagement_without_weekends.lp_id == virtual_classroom_product].groupby('time').engagement_index.mean().to_frame().reset_index(drop=False)\n    sns.lineplot(x=temp.time, y=temp.engagement_index, label=products_info[products_info['LP ID'] == virtual_classroom_product]['Product Name'].values[0])\nplt.legend()\nplt.show()","ba5cbca7":"products_info['lp_id'] = products_info['LP ID'].copy()\n\nf, ax = plt.subplots(nrows=3, ncols=3, figsize=(18, 8))\n\ni = 0\nj = 0\nfor subfunction in products_info[products_info.primary_function_main == 'LC'].primary_function_sub.unique():\n    lp_ids = products_info[products_info.primary_function_sub == subfunction]['LP ID'].unique()\n\n    temp = engagement_without_weekends[engagement_without_weekends.lp_id.isin(lp_ids)]\n    temp = temp.groupby('lp_id').pct_access.mean().sort_values(ascending=False).to_frame().reset_index(drop=False)\n    temp = temp.merge(products_info[['lp_id', 'Product Name']], on='lp_id').head()\n    \n    sns.barplot(data=temp, x='pct_access', y='Product Name', palette='GnBu', ax=ax[i, j])\n    \n    ax[i, j].set_title(f'Top 5 in \\n{subfunction}', fontsize=12)\n    ax[i, j].set_xlim([0, 20])\n    j = j + 1\n    if j == 3:\n        i = i + 1\n        j = 0\n        \nf.delaxes(ax[2, 1])\nf.delaxes(ax[2, 2])\n\nplt.tight_layout()\nplt.show()\n","d321005b":"products_info[products_info['Product Name'] == 'Among Us']","9d435b80":"engagement['quarter'] = pd.DatetimeIndex(engagement['time']).quarter.astype(str)\n\ntemp = engagement.merge(products_info[['lp_id', 'Product Name', 'primary_function_main', 'primary_function_sub']], on='lp_id')\ntemp = temp[temp.primary_function_sub.notna()]\ntemp = temp.groupby(['quarter', 'primary_function_sub'])['pct_access', 'engagement_index'].mean().reset_index(drop=False)\n\ntemp = temp.pivot(index='primary_function_sub', columns='quarter')[['pct_access', 'engagement_index']].fillna(0)\n\ntemp.columns = [\"_\".join(a) for a in temp.columns.to_flat_index()]\n\ntemp['pct_access_delta'] = temp['pct_access_4'] - temp['pct_access_1']\ntemp['engagement_index_delta'] = temp['engagement_index_4'] - temp['engagement_index_1']\ntemp=temp.reset_index(drop=False)\n#temp = temp.merge(products_info[['lp_id', 'Product Name', 'primary_function_sub']], on='lp_id')\n\nf, ax = plt.subplots(nrows=1, ncols=2, figsize=(20, 10))\n\ndf = temp.sort_values(by='pct_access_delta', ascending=False)#.head(5)\n\nsns.barplot(data=df, x='pct_access_delta', y='primary_function_sub', palette='GnBu', ax=ax[0])\n\ndf = temp.sort_values(by='engagement_index_delta', ascending=False)#.head(5)\n\nsns.barplot(data=df, x='engagement_index_delta', y='primary_function_sub', palette='GnBu', ax=ax[1])\nplt.tight_layout()\nplt.show()","6c9e536d":"### In above example to showcase the difference between pct_access and engagement_index, we intuitively chose products from the subfunction 'Virtual Classroom' based on our \"domain knowledge\" that in-person education does not need video conferencing tools but during the pandemic these tools gained a lot of popularity. Let's do a quick analysis and see whether the avaiable data reflects the assumption we had made before.\n\n### For this purpose, we will average the pct_access and engagement_index data over each quarter in 2020 and then take the difference between the last quarter of 2020 and the first quarter of 2020. Based on this approach, we should be able to see which subfunction category that gained the most engagement over the course of the pandemic in 2020.\n\n### As you can see in the below left figure, the data reflects our assumption that products of the subfunction 'Virtual Classroom' gained the most change in pct_access. At second place, we have Learning Management Systems (LMS). These top two categories are also the top two categories from the engagement_index point of view.","1d534960":"## Engagement Data\n### The engagement data are aggregated at school district level, and each file in the folder engagement_data represents data from one school district. The 4-digit file name represents district_id which can be used to link to district information in district_info.csv. The lp_id can be used to link to product information in product_info.csv.","c67ccdba":"## Preprocessing","379408da":"## This notebook builds on How To Approach Analytics Challengez notebook","7014843d":"### Furthermore, we will concatenate the engagement data from all remaining districts in one dataframe by adding the key column district_id to each engagement file","ae9a393d":"To make the data easier to compare, we will only consider distrcits with engagement data for everyday in 2020.","db27d714":"### If we look at the distributions of the remaining columns in districts_info, you can quickly see that county_connections_ratio only has one unique value which is [0.18, 1[. To be fair, before dropping all rows without any state information, this column had another value of [1, 2[ but only for one data point. So, this column does not really contain any valuable information. Maybe we can get this sort of information from an external dataset.","5f1e2ec4":"### Just on a side note because I saw this on accident: 'Among Us', which is a multiplayer game that was quite popular during 2020, is also listed as a product under the main category LC. So, we need to take the list of products with a grain of salt when we are evaluating them as 'digital learning products'.","c1a0f847":"\n### Now we will try to understand that meaning behind pct_access and engagement_index.\n\n### * pct_access: % of students in the district have at least one page-load event of a given product and on a given day\n### * engagement_index: Total page-load events per 1000 students of a given product and on a given day\n### In the first figure below, you can see the overall mean pct_access of all products of the category 'Virtual Classroom'. For better understanding, weekends are removed from this visualization as students will not attend classes on weekends and this would add a disturbing visual effect and make the plot more difficult to understand. Let's summarize what we see:\n\n### * the home schooling phase starts at the beginning of March\n### * there is a bell-shape between March and July\n### * during July and August there are summer holidays and therefore no classes to attend\n### * after the summer holidays the pct_access increases to a higher level as observed at the beginning of the pandemic and it stays somewhat constant\n### * there are a few drops in pct_access visible throughout the year - these might be national holidays or other holidays\n### * Zoom and Meet seem to be the two most popular products for virtual classrooms\n### Over the last quarter of 2020, we can see a pct_access of roughly 15. What does this mean? 15 % of students in the district have at least one page-load event of Zoom or Meet. To be honest, that seems a little bit low from the home schooling point of view given that every student needs to attend classes on a school day. However, this seems to hint at the fact that not all students had to attend classes virtually but were able to physically go to school. Judging from this State-by-State Map of Where School Buildings Are Opened or Closed it seems like a lot of schools offered in-person education in 2020. That means, when looking at digital learning, we should probably focus our analysis on districts where digital learning was actually applied to get some key insights.\n\n### While for pct_access Zoom and Meet seem to have roughly similiar values, we can see in the lower graph that Meet has more than 4 times the value of Zoom for engagement_index in the last quarter of 2020. What does this mean? If we have 1000 page-load events per 1000 students for Zoom on a given day that means that one student uses Zoom once a day. In contrast, Meet is used 4 or 5 times daily on average per student.\n\n### To make things a little bit clearer: If every students has two applications on their phone, the pct_access indicates how many of the students access this app on a daily basis but the engagement_index tells you how much the students engag","c89839c2":"### After preprocessing, we are left with a reduced districts_info dataframe with 176 districts","2b35973b":"### The most common category in the column Primary Essential Function is Learning & Curriculum (LC) as shown on the below left figure. For the categories Classroom Management (CM) and School & District Operations (SDO) there are far fewer tool options available.","a694b623":"### Note that at this stage, we have removed quite a bit of data. This obviously can lead to loss of information. However, on the other hand this makes the data easier to compare and therefore, it can help us in finding insights more quickly. To summarize, we have removed districts without any information on the location and we have removed districts with incomplete data in 2020.","10080b0e":"### Below we can see the top 5 most accessed products for each LC sub-category sorted by the mean pct_access for 2020 over all districts. We can see that most of the products are on average accessed by less than 5 % of students on a daily basis. Exceptions are YouTube, Google Docs, and Canvas. YouTube in this case is a difficult one to evaluate since it can be used for leisure in addition to education, so we need to be careful here. Google Docs seems to make a lot of sense since students can use Google Docs. According to Canvas it is\n\n### Canvas is a web-based learning management system, or LMS. It is used by learning institutions, educators, and students to access and manage online course learning materials and communicate about skill development and learning achievement.\n\n### so it also makes sense that this one is quite often accessed.\n\n### Regarding the sub-category 'Career Planning & Job Search' the average pct_access is very low. This is probably due top the fact that career planning is only relevant to older students. Therefore, we can probably exclude this subcategory when inspecting the digital learning aspect.","cee7b510":"### Finally, we will convert the time column to the type datetime64[ns] for easier handling.","bb2769ef":"## Review of the Problem Statement\n### Next, review the problem statement and brainstorm some ideas for questions you would like to answer with your analysis. For this, you could try something like a mindmap as follows.\n\n### Problem Statement\n\n### The COVID-19 Pandemic has disrupted learning for more than 56 million students in the United States. In the Spring of 2020, most states and local governments across the U.S. closed educational institutions to stop the spread of the virus. In response, schools and teachers have attempted to reach students remotely through distance learning tools and digital platforms. Until today, concerns of the exacaberting digital divide and long-term learning loss among America\u2019s most vulnerable learners continue to grow. Challenge\n\n### We challenge the Kaggle community to explore (1) the state of digital learning in 2020 and (2) how the engagement of digital learning relates to factors such as district demographics, broadband access, and state\/national level policies and events.\n\n### We encourage you to guide the analysis with questions that are related to the themes that are described above (in bold font). Below are some examples of questions that relate to our problem statement:\n\n### What is the picture of digital connectivity and engagement in 2020?\n### What is the effect of the COVID-19 pandemic on online and distance learning, and how might this also evolve in the future?\n### How does student engagement with different types of education technology change over the course of the pandemic?\n### How does student engagement with online learning platforms relate to different geography? Demographic context (e.g., race\/ethnicity, ESL, learning disability)? Learning context? Socioeconomic status?\n### Do certain state interventions, practices or policies (e.g., stimulus, reopening, eviction moratorium) correlate with the increase or decrease online engagement?\n### Based on the above problem statement from the challenge's description, you could start with a mindmap as shown below. learnplatform.005.jpeg\n\n### From here you should continue your brainstorming and extend the above mindmap with further items.","c84b0ba9":"### Splitting up the Primary Essential Function","9615af06":"## Initial Exploratory Data Analysis (EDA)","69b3c279":"## Product information data\n### The product file products_info.csv includes information about the characteristics of the top 372 products with most users in 2020. The categories listed in this file are part of LearnPlatform's product taxonomy. Data were labeled by our team. Some products may not have labels due to being duplicate, lack of accurate url or other reasons.","0f921f4f":"## District Data\n### The district file districts_info.csv includes information about the characteristics of school districts, including data from NCES (2018-19), FCC (Dec 2018), and Edunomics Lab. In this data set, we removed the identifiable information about the school districts. We also used an open source tool ARX (Prasser et al. 2020) to transform several data fields and reduce the risks of re-identification. For data generalization purposes some data points are released with a range where the actual value falls under. Additionally, there are many missing data marked as 'NaN' indicating that the data was suppressed to maximize anonymization of the dataset.","fb7c921a":"### For the majority of school districts (133) there are 366 unique days available. However, for 43 school districts there are less than 366 unique days of data available. For example for district 3670 there is only data available from 2020-02-15 to 2020-03-02 or for district 2872 there is only data available for January 2020 and then two more single days in Feburary and March.","f9582e68":"### Let's begin with a simple EDA. First of all, I am interested how diverse the available school districts are. As you can see in below plot, the available data does not cover all the states in the U.S. (19\/50). The states with the most available school districts are CT (29) and UT (24) while there are also states with only one school district (FL, TN, NY, AZ)."}}