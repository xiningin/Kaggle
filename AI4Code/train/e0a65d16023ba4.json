{"cell_type":{"d603040e":"code","de4c8303":"code","db46edb6":"code","dc855d62":"code","7a57db92":"code","1d8e1a52":"code","cf70c7fb":"code","4217ea40":"code","c96c62bf":"code","e866f529":"code","289b3cac":"code","c88dd8c2":"code","a5b6a4dc":"code","5262ebfc":"code","b1f7c805":"code","755ffe62":"code","25c68239":"code","8755e8c9":"code","bf869f6a":"code","c9373629":"code","f9808b3f":"code","2b9f3db5":"code","1430eb7d":"code","0b0a26c3":"code","ad2aa019":"code","1f95c093":"code","8ed90d70":"code","559e12bd":"code","02b0bf78":"code","702d6d78":"code","c307670e":"code","b0af0d67":"code","fdf63fec":"code","c6dc274a":"code","415e6570":"code","b898f241":"code","5d47a720":"code","ed3a5f29":"code","538bfa9b":"markdown","2ddecfd8":"markdown","463d133a":"markdown","70136cf3":"markdown","c52ed5c3":"markdown","f5f15311":"markdown","db71cd5b":"markdown","ac3a571b":"markdown","6cd9c1b9":"markdown","b5552fef":"markdown","debe9d15":"markdown","1232ca36":"markdown","91cee9de":"markdown","9c6cf07f":"markdown","deebd2e8":"markdown","366cf9cf":"markdown","6922b03a":"markdown","e67af8d5":"markdown","ec4c33a9":"markdown","a3241c89":"markdown","e81a2212":"markdown","1e628a9a":"markdown","94e69b70":"markdown","39e583ac":"markdown","2ce3762c":"markdown","8b98c79f":"markdown","a84928b6":"markdown","105c9668":"markdown","5cf25511":"markdown"},"source":{"d603040e":"import pandas as pd\nimport numpy as np\n\ndata = pd.read_csv(\"\/kaggle\/input\/pubmed-abstracts\/pubmed_abstracts.csv\", index_col=0)\ndata.head(5)","de4c8303":"data.iloc[1]","db46edb6":"# slice data columns that contain only text data.\ncols = data.columns.tolist()[:8]\nsample = data[cols].dropna().sample(1)\n\nfor colname, value in zip(sample.columns, sample.values[0]):\n    value = eval(value)\n    print(\"\\n\\nColumn name:\\n\", colname.upper(), \n          \"\\n\\nTitle:\\n\", value[1],\n         \"\\n\\nAbstract:\\n\", value[0])","dc855d62":"import re\n\nimport nltk\nfrom nltk.tokenize import word_tokenize, sent_tokenize","7a57db92":"# function for removing html tags\ndef preprocess_text(text):\n    try:\n        text = re.sub('[^a-zA-Z\u0430-\u044f\u0410-\u042f1-9]+', ' ', text)\n        text = re.sub(' +', ' ', text)\n    except:\n        print(text)\n    return text\n\n\n# Data\ndl = data.deep_learning.dropna()\ncovid_19 = data.covid_19.dropna()\nhuman_connectome = data.human_connectome.dropna()\nvirtual_reality = data.virtual_reality.dropna()\nbrain_machine_interfaces = data.brain_machine_interfaces.dropna()\nelectroactive_polymers = data.electroactive_polymers.dropna()\npedot_electrodes = data.pedot_electrodes.dropna()\nneuroprosthetics = data.neuroprosthetics.dropna()\n\ntexts = list(pd.concat([dl, covid_19, \n                        human_connectome, virtual_reality, \n                        brain_machine_interfaces, electroactive_polymers, \n                        pedot_electrodes, neuroprosthetics]).unique())\n\n\"\"\"texts = list(pd.concat([data.deep_learning, data.covid_19, \n                        data.human_connectome, data.virtual_reality, \n                        data.brain_machine_interfaces, data.electroactive_polymers, \n                        data.pedot_electrodes, data.neuroprosthetics]).unique())\"\"\"\n\nprint(\"Data:\\n\", texts[:1], \"\\n\")\n\n\n# Cleared Data\ntexts = [preprocess_text(x) for x in texts]\n\nprint(\"Cleared Data:\\n\", texts[:1])","1d8e1a52":"%%time\n\ntokenized_texts = [word_tokenize(text.lower()) for text in texts]\n\nprint(\"Tokenized text:\\n\", [row for row in tokenized_texts[5]])\nprint(\"Source text:\\n\", [' '.join(tokenized_texts[5])])","cf70c7fb":"import gensim\nfrom gensim.models import Word2Vec","4217ea40":"%%time\n\nmodel = Word2Vec(tokenized_texts, \n                 size=32,      # embedding vector size\n                 min_count=5,  # consider words that occured at least 5 times\n                 window=5).wv  # define context as a 5-word window around the target word","c96c62bf":"model.get_vector('brain')","e866f529":"model.most_similar('trump')","289b3cac":"model.most_similar(['pytorch', 'cnn'])","c88dd8c2":"model.most_similar(positive=['coronavirus', 'money'])","a5b6a4dc":"%%time\n\nfrom sklearn.manifold import TSNE\n\n#DATA\ndef get_tsne_projection(word_vectors):\n    x = TSNE(n_components=2).fit_transform(word_vectors)\n    x = (x - np.mean(x, axis=0)) \/ np.std(x, axis=0)\n    return x\n\n\n\n\nwords = sorted(model.vocab.keys(), \n               key=lambda word: model.vocab[word].count,\n               reverse=True)[:10000]# only 10k\n\nprint(words[::1000])\nword_vectors = model.vectors[[model.vocab[word].index for word in words]]\nword_tsne = get_tsne_projection(word_vectors)","5262ebfc":"import bokeh.models as bm, bokeh.plotting as pl\nfrom bokeh.io import output_notebook\n\ndef draw_vectors(x, y, radius=10, alpha=0.25, color='blue',\n                 width=600, height=400, show=True, **kwargs):\n    \"\"\" draws an interactive plot for data points with auxilirary info on hover \"\"\"\n    output_notebook()\n    \n    if isinstance(color, str): \n        color = [color] * len(x)\n    data_source = bm.ColumnDataSource({ 'x' : x, 'y' : y, 'color': color, **kwargs })\n\n    fig = pl.figure(active_scroll='wheel_zoom', width=width, height=height)\n    fig.scatter('x', 'y', size=radius, color='color', alpha=alpha, source=data_source)\n\n    fig.add_tools(bm.HoverTool(tooltips=[(key, \"@\" + key) for key in kwargs.keys()]))\n    if show: \n        pl.show(fig)\n    return fig\n\n# PLOT\ndraw_vectors(word_tsne[:, 0], word_tsne[:, 1], color='teal', token=words)","b1f7c805":"import matplotlib.colors as mcolors\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport random\n\nsns.set_context(\"poster\", font_scale=0.6, rc={\"lines.linewidth\": 0.7})\n\ndef tsne_plot_similar_words(title,\n                            labels,\n                            embedding_clusters,\n                            word_clusters,\n                            a):\n    plt.figure(figsize=(13, 10))\n    colors = random.choices(list(mcolors.CSS4_COLORS.keys()),k=len(labels))\n    idx_color = 0\n    for label, embeddings, words, color in zip(labels, embedding_clusters, word_clusters, colors):\n        x = embeddings[:, 0]\n        y = embeddings[:, 1]\n\n        plt.scatter(x, y, c=colors[idx_color], alpha=a, label=label)\n        for i, word in enumerate(words):\n            plt.annotate(word, alpha=0.5, xy=(x[i], y[i]), xytext=(5, 2),\n                         textcoords='offset points', ha='right', va='bottom', size=8)\n        idx_color += 1\n\n    plt.legend(loc=4, fontsize=12)\n    plt.title(title, fontsize=18)\n    plt.grid(True)\n    plt.axis(\"off\")\n    plt.show()","755ffe62":"words = ['cnn', 'patient', 'mri', \n             \"leukemia\", 'nanotube', 'machine',\n             'muscle','treatment' ,'rendering' ,\n             'biomaterials']\nembedds = []\nsimilarities_words = []\n\n\ndef most_similar(word):\n    words = []\n    embedds = []\n    for similar_word, _ in model.most_similar(word, topn=10):\n        words.append(similar_word)\n        embedds.append(model[similar_word])\n    return np.array(embedds), words\n\n\n\nfor word in words:\n    vectors, sim_words = most_similar(word)\n    embedds.append(vectors)\n    similarities_words.append(sim_words)\n    \n#print(np.array(embedds).shape, similarities_words)\nembedding_clusters = np.array([get_tsne_projection(np.array(embedds[i])) \n          for i in range(len(embedds))])","25c68239":"tsne_plot_similar_words(title='Similar words from PubMed',\n                        labels=words,\n                        embedding_clusters=embedding_clusters,\n                        word_clusters=similarities_words,\n                        a=0.7)","8755e8c9":"%%time\n\nimport gensim.downloader as api\n\n#print(api.info())\nmodel = api.load('fasttext-wiki-news-subwords-300')","bf869f6a":"def get_phrase_embedding(model, phrase):    \n    vector = np.zeros([model.vector_size], dtype='float32')\n    \n    phrase = phrase.lower()\n    counter = 0\n    for word in word_tokenize(phrase):\n        if word in model.vocab:\n            vector += model.get_vector(word.lower())\n            counter += 1   \n    vector = vector \/ (counter if counter > 0 else 1)\n    return vector\n\n\n#Let's count the vectors of all phrases\/abstracts.\ntext_vectors = np.array([get_phrase_embedding(model, \" \".join(phrase)) for phrase in tokenized_texts])","c9373629":" def find_knn(model, vectors, texts, query, k=10):\n\n     query = get_phrase_embedding(model, query)\n     cossims = np.matmul(vectors, query)\n     norms = np.sqrt((query**2).sum() * (vectors**2).sum(axis=1))\n     cossims = cossims\/norms\n\n     result_i = np.argpartition(-cossims, range(k))[0:k]\n     results = [texts[i] for i in result_i]\n     return results","f9808b3f":"results = find_knn(model, text_vectors, texts, query=\"convolutional neural network for semantic segmentation.\", k=10)\n\nprint('\\n'.join(results))","2b9f3db5":"find_knn(model, text_vectors, texts, query=\"machine learning can help in the fight against coronavirus?\", k=10)","1430eb7d":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\n\nfrom torchvision import transforms","0b0a26c3":"from collections import Counter\n\nMIN_COUNT = 5\n\nwords_counter = Counter(token for tokens in tokenized_texts for token in tokens)\nword2index = {\n    '<unk>': 0\n}\n\nfor word, count in words_counter.most_common():\n    if count < MIN_COUNT:\n        break\n        \n    word2index[word] = len(word2index)\n    \nindex2word = [word for word, _ in sorted(word2index.items(), key=lambda x: x[1])]# sort by count\n    \nprint('Vocabulary size:', len(word2index))\nprint('Tokens count:', sum(len(tokens) for tokens in tokenized_texts))\nprint('Unknown tokens appeared:', sum(1 for tokens in tokenized_texts for token in tokens if token not in word2index))\nprint('Most freq words:', index2word[1:21])","ad2aa019":"def build_contexts(tokenized_texts, window_size):\n    contexts = []\n    for tokens in tokenized_texts:\n        for i in range(len(tokens)):\n            central_word = tokens[i]\n            context = [tokens[i + delta] for delta in range(-window_size, window_size + 1) \n                       if delta != 0 and i + delta >= 0 and i + delta < len(tokens)]\n\n            contexts.append((central_word, context))\n            \n    return contexts","1f95c093":"contexts = build_contexts(tokenized_texts, window_size=2)\ncontexts[:5]","8ed90d70":"contexts = [(word2index.get(central_word, 0), [word2index.get(word, 0) for word in context]) \n            for central_word, context in contexts]\n\ncontexts[:5]","559e12bd":"import math\n\ndef make_skip_gram_batchs_iter(contexts, window_size, num_skips, batch_size, epochs=2):\n    assert batch_size % num_skips == 0\n    assert num_skips <= 2 * window_size\n    \n    central_words = [word for word, context in contexts if len(context) == 2 * window_size and word != 0]\n    contexts = [context for word, context in contexts if len(context) == 2 * window_size and word != 0]\n    \n    batch_size = int(batch_size \/ num_skips)\n    batchs_count = int(math.ceil(len(contexts) \/ batch_size))\n    \n    print('Initializing batchs generator with {} batchs per epoch'.format(batchs_count))\n    \n    for epoch in range(epochs):\n        indices = np.arange(len(contexts))\n        np.random.shuffle(indices)\n\n        for i in range(batchs_count):\n            batch_begin, batch_end = i * batch_size, min((i + 1) * batch_size, len(contexts))\n            batch_indices = indices[batch_begin: batch_end]\n\n            batch_data, batch_labels = [], []\n\n            for data_ind in batch_indices:\n                central_word, context = central_words[data_ind], contexts[data_ind]\n                \n                words_to_use = random.sample(context, num_skips)\n                batch_data.extend(words_to_use)\n                batch_labels.extend([central_word] * num_skips)\n            \n            yield batch_data, batch_labels\n            \nbatch, labels = next(make_skip_gram_batchs_iter(contexts, window_size=2, num_skips=2, batch_size=32))","02b0bf78":"model = nn.Sequential(\n    nn.Embedding(len(word2index), 32),\n    nn.Linear(32, len(word2index))\n)\nmodel = model.cuda()","702d6d78":"import time\n\nloss_every_nsteps = 1000\ntotal_loss = 0\nstart_time = time.time()\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\nloss_function = nn.CrossEntropyLoss().cuda()\n\nfor step, (batch, labels) in enumerate(make_skip_gram_batchs_iter(contexts, window_size=2,\n                                                                  num_skips=4, batch_size=256,\n                                                                  epochs=2)):\n    batch = torch.LongTensor(batch).cuda()\n    labels = torch.LongTensor(labels).cuda()\n\n    batch_res = model(batch)\n    logits = F.log_softmax(batch_res)\n \n    loss = loss_function(logits, labels)\n    loss.backward()\n    \n    optimizer.step()\n    optimizer.zero_grad()\n    total_loss += loss.item()\n    \n    if step != 0 and step % loss_every_nsteps == 0:\n        print(\"Step = {}, Avg Loss = {:.4f}, Time = {:.2f}s\".format(step, total_loss \/ loss_every_nsteps, \n                                                                    time.time() - start_time))\n        total_loss = 0\n        start_time = time.time()","c307670e":"embeddings = model[0].weight.cpu().data.numpy()","b0af0d67":"from sklearn.metrics.pairwise import cosine_similarity\n\ndef most_similar(embeddings, index2word, word2index, word):\n    word_emb = embeddings[word2index[word]]\n    \n    similarities = cosine_similarity([word_emb], embeddings)[0]\n    top10 = np.argsort(similarities)[-10:]\n    embedds = [embeddings[e] for e in top10 ]\n    \n    return embedds, [index2word[index] for index in reversed(top10)]\n","fdf63fec":"vectors, words, = most_similar(embeddings, index2word, word2index, 'unet')\nwords","c6dc274a":"vectors, words, = most_similar(embeddings, index2word, word2index, 'virus')\nwords","415e6570":"vectors, words, = most_similar(embeddings, index2word, word2index, 'people')\nwords","b898f241":"def visualize_embeddings(embeddings, index2word, word_count):\n    word_vectors = embeddings[1: word_count + 1]\n    words = index2word[1: word_count + 1]\n    \n    word_tsne = get_tsne_projection(word_vectors)\n    draw_vectors(word_tsne[:, 0], word_tsne[:, 1], color='purple', token=words)\n    \n    \nvisualize_embeddings(embeddings, index2word, 10000)","5d47a720":"embedds = []\nsimilarities_words = []\nwords = ['cnn', 'patient', 'mri', \n             \"leukemia\", 'nanotube', 'machine',\n             'muscle','treatment' ,'rendering' ,\n             'biomaterials']\n\n\nfor word in words:\n    vectors, sim_words = most_similar(embeddings, index2word, word2index, word)\n    embedds.append(vectors)\n    similarities_words.append(sim_words)\n\nembedding_clusters = np.array([get_tsne_projection(np.array(embedds[i])) \n          for i in range(len(embedds))])","ed3a5f29":"tsne_plot_similar_words(title='Similar words from PubMed',\n                        labels=words,\n                        embedding_clusters=embedding_clusters,\n                        word_clusters=similarities_words,\n                        a=0.7)","538bfa9b":"Skip-Grams","2ddecfd8":"First, we will teach Gensim Word2vec on these texts, then custom network on pyhorch","463d133a":"as a phrase embeddings, there will be an averaged vector of word embeddings.","70136cf3":"most similar words","c52ed5c3":"TSNE for k nearest words","f5f15311":"# TSNE","db71cd5b":"TSNE for k nearest words","ac3a571b":"### DataGenerator","6cd9c1b9":"Data samples may not always contain both a title and an abstract, so this should be taken into account when pre-processing the data.","b5552fef":"+ https:\/\/www.codetd.com\/ru\/article\/7884527\n+ http:\/\/mccormickml.com\/2016\/04\/19\/word2vec-tutorial-the-skip-gram-model\/\n\n\nModel structure     |  Skip-Gram\n:------------------------:|:-------------------------:\n![](https:\/\/img2018.cnblogs.com\/i-beta\/1252882\/201911\/1252882-20191117110351675-977787758.png) | ![](http:\/\/mccormickml.com\/assets\/word2vec\/training_data.png)","debe9d15":"# Word Embeddings Gensim\n\n+ https:\/\/radimrehurek.com\/gensim\/models\/word2vec.html","1232ca36":"vector in embeddings space","91cee9de":"## Word Embeddings\nmost similar words","9c6cf07f":"# What does data look like?","deebd2e8":"# Data Preprocessing","366cf9cf":"### Encoding words into numeric skip-grams","6922b03a":"### Word2Vec\n\nKey idea is to teach embedding vectors so that they can be used to predict the context of the corresponding words as best as possible.\nA word in vector space will be defined as:\n**w(some word) = u(some word) + v(some word)**  where U and V are some weight matrices.\n\nSo, we need to train 2 weight matrices U and V that will define the embedding space of the words, so we must implement a two-layer linear model, and a skip-gram data generator.\n","e67af8d5":"Sample of text data","ec4c33a9":"Now let's write a function to search for k nearest vectors using cosine similarity.\n\n\n+ https:\/\/medium.com\/@adriensieg\/text-similarities-da019229c894\n\n![alt](https:\/\/miro.medium.com\/max\/852\/1*5J8YlnfnZlzFobQC9cGk-w.png)","a3241c89":"most similar words","e81a2212":"### Steps:\n\n+ What does data look like?\n+ Data Preprocessing\n+ Gensim:\n  + Word Embeddings\n  + Phrase Embeddings\n  + TSNE\n+ Pytorch:\n  + Word2Vec \n  + Word Embeddings \n  + TSNE","1e628a9a":"Data tokenization","94e69b70":"# Pytorch \n# Word2Vec from scratch","39e583ac":"# TSNE","2ce3762c":"it is expected that since they are close in the embedding space they will be semantically close as well","8b98c79f":"### train model","a84928b6":"# Phrase Embeddings\nLet's count the vectors of all phrases\/abstracts using predtrained model(fasttext) on [wiki-news-subwords-300](https:\/\/fasttext.cc\/docs\/en\/english-vectors.html)","105c9668":"Sample of all data","5cf25511":"### fun visualization"}}