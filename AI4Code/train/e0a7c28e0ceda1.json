{"cell_type":{"6036f6f2":"code","2f5beb29":"code","e6c437b7":"code","43538132":"code","df80d2f9":"code","135bb5a4":"code","09e52742":"code","3c57ed5f":"code","9808aa04":"code","9544309a":"code","31b95860":"code","b725b1dc":"code","8f765933":"code","a3b8ba82":"code","9215fe2f":"code","94e50fc7":"code","2c3667f3":"code","46f3e40a":"code","c7926c2d":"code","83ff8a3c":"code","031b0711":"code","af5c180b":"code","d9fbecff":"code","77822c2d":"code","3dd0caf1":"code","de143545":"code","0ee4191a":"code","6801ec51":"code","66b06c3a":"code","1f1212a9":"code","a2776daf":"code","473a137b":"code","245f9a53":"code","bba1a1db":"code","f5cce263":"code","27c7c9c1":"code","b1ef888d":"code","c5496cba":"code","25d8fb23":"code","0dc78e90":"code","c9f1fb04":"code","89547e39":"code","aefb0919":"code","137df63a":"code","08aa1f4f":"code","40181c46":"code","8e5eb3b0":"code","f14776d1":"code","2a6d18f0":"code","5f043e08":"code","7077ab74":"code","1376d4b2":"code","5eafb26e":"code","62f28c58":"code","84d5f41e":"code","b23064f1":"code","60821e04":"code","26378e76":"code","056ec361":"code","00313ca1":"code","47cb0e83":"code","b8449f32":"code","889f713d":"code","c631f4b6":"code","cf38775c":"code","ef5cbe94":"code","4dc4b12e":"code","ade4ecfd":"code","1e37a851":"code","8c796f5b":"code","775cf850":"code","1ff993b8":"code","e1e8c5f1":"code","5ba1e1de":"code","a7d0e66e":"code","85e07fdb":"markdown","03446ae8":"markdown","a3dca35f":"markdown","79564e38":"markdown","a1477648":"markdown","18ea97ce":"markdown","962200f8":"markdown","dd03040f":"markdown","6d1fc210":"markdown","48620f8a":"markdown","ad11151c":"markdown","42f9c5ad":"markdown","01ff4914":"markdown","149153fd":"markdown","3809d638":"markdown","5179bc5d":"markdown","4cb1702f":"markdown","82757923":"markdown","0d47d23e":"markdown","900df7c4":"markdown","915f622a":"markdown","862900a5":"markdown","fa2aed23":"markdown","932a5543":"markdown","6cc0e2e5":"markdown","c440ee3b":"markdown","25ffaaef":"markdown","be4f17e0":"markdown","1279a14a":"markdown","f3b02fb4":"markdown","4a2ef77d":"markdown","209c7f01":"markdown","854fd15e":"markdown","f67d5e7a":"markdown","9945779b":"markdown","4a5d2363":"markdown"},"source":{"6036f6f2":"import numpy as np \nimport matplotlib.pyplot as plt \nimport pandas as pd \nimport random\nimport math\nimport time\nfrom sklearn.model_selection import RandomizedSearchCV, train_test_split\nfrom sklearn.svm import SVR\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\nimport datetime\n\nimport torch","2f5beb29":"#Use this to fetch latest data\n\nconfirmed_df = pd.read_csv('https:\/\/raw.githubusercontent.com\/CSSEGISandData\/COVID-19\/master\/csse_covid_19_data\/csse_covid_19_time_series\/time_series_covid19_confirmed_global.csv')\ndeaths_df = pd.read_csv('https:\/\/raw.githubusercontent.com\/CSSEGISandData\/COVID-19\/master\/csse_covid_19_data\/csse_covid_19_time_series\/time_series_covid19_deaths_global.csv')\nrecoveries_df = pd.read_csv('https:\/\/raw.githubusercontent.com\/CSSEGISandData\/COVID-19\/master\/csse_covid_19_data\/csse_covid_19_time_series\/time_series_covid19_recovered_global.csv')\ncsse_daily_df = pd.read_csv('https:\/\/raw.githubusercontent.com\/CSSEGISandData\/COVID-19\/master\/csse_covid_19_data\/csse_covid_19_daily_reports\/08-16-2020.csv')","e6c437b7":"confirmed_df\n# confirmed_df has time series data of cases over time","43538132":"recoveries_df\n# same as confirmed_df, recoveries_df also has time series data of recoveries over time","df80d2f9":"all(confirmed_df.keys()[4:]==deaths_df.keys()[4:])","135bb5a4":"all(deaths_df.keys()[4:]==recoveries_df.keys()[4:])","09e52742":"csse_daily_df","3c57ed5f":"csse_daily_df['Country_Region'].value_counts()\n# this is the most recent data of Covid19 cases!","9808aa04":"%pip install geopandas","9544309a":"# let's look at the csse daily data\nimport seaborn as sns\nfrom shapely.geometry import Point\nimport geopandas as gpd\nfrom geopandas import GeoDataFrame\n\ngeometry = [Point(xy) for xy in zip(csse_daily_df['Long_'], csse_daily_df['Lat'])]\ngdf = GeoDataFrame(csse_daily_df[['Lat','Long_']], geometry=geometry)   \n\n#this is a simple map that goes with geopandas\nworld = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))\ngdf.plot(ax=world.plot(figsize=(15, 10)), marker='o', color='red', markersize=15);","31b95860":"# Top 7 countries\ncsse_daily_df['Country_Region'].value_counts()[:7]\n# US has over 3000 entries in data","b725b1dc":"top_c = csse_daily_df.groupby(['Country_Region']).Confirmed.sum().sort_values(ascending=False)\ntop_c","8f765933":"# Top 50 Country wise Confirmed cases acc to given data\n\nfig, ax = plt.subplots(figsize=(10, 13))\nsns.barplot(y=top_c.index[:50], x=top_c.values[:50])","a3b8ba82":"csse_daily_df_us = csse_daily_df[csse_daily_df['Country_Region']=='US'] # get US only data\n\nfig, ax = plt.subplots(figsize=(17, 7))\nsns.distplot(csse_daily_df_us['Confirmed'].dropna())","9215fe2f":"# Drop Unnecessary Columns\ncsse_daily_df_us.drop(['Country_Region','Province_State','Long_','Last_Update','Combined_Key'], axis=1, inplace=True)\n\ncsse_daily_df_us.Recovered.value_counts()","94e50fc7":"# since recovered has only 1 statis value, we will simply drop it too\n\ncsse_daily_df_us.drop(['Recovered'], axis=1, inplace=True)","2c3667f3":"csse_daily_df_us.info()","46f3e40a":"print(\"{} \\nNan values found\".format(csse_daily_df_us.isna().sum()))\ncsse_daily_df_us.dropna(inplace=True) # drop na","c7926c2d":"sns.pairplot(csse_daily_df_us[['Confirmed','Deaths','Active','Incidence_Rate','Case-Fatality_Ratio']])","83ff8a3c":"fig, ax = plt.subplots(figsize=(12, 8))\ncorr = csse_daily_df_us.corr()\nsns.heatmap(corr, linewidths=.5, cbar_kws={\"shrink\": .5},annot=True)","031b0711":"# Label Encode Admin2 columns\n\nfrom sklearn.preprocessing import LabelEncoder\n\nadmin2 = LabelEncoder()\n\ncsse_daily_df_us['Admin2'] = admin2.fit_transform(csse_daily_df_us['Admin2'])\ncsse_daily_df_us","af5c180b":"from scipy.stats import pearsonr\n\nshould_drop = []\nfor idx,i in enumerate(['FIPS', 'Admin2', 'Lat', 'Active','Incidence_Rate', 'Case-Fatality_Ratio']):\n    for j in ['Confirmed','Deaths']: # go through columns and find corr\n        corr, _ = pearsonr(csse_daily_df_us[i],csse_daily_df_us[j]) # use scipy\n        print(i,\"has corr value =\",corr,\"with\",j)\n        if corr > 0.85: \n            should_drop.append(i)\n#             csse_daily_df_us.drop(j,axis=1,inplace = True) # drop it\n            \n    \nshould_drop","d9fbecff":"# LEt's keep Confirmed a target variable\nX_conf = csse_daily_df_us.drop(['Confirmed'],axis=1)\ny_conf = csse_daily_df_us['Confirmed']\n\nfrom sklearn.feature_selection import SelectKBest, chi2\n\nbest_feature = SelectKBest(score_func= chi2, k = 'all')\nbest_feature = best_feature.fit(X_conf,y_conf)\n\ncol_scores = pd.DataFrame(best_feature .scores_)\ncol_names = pd.DataFrame(X_conf.columns)\n\nfeature_score = pd.concat([col_names, col_scores], axis=1)\nfeature_score.columns = ['attribute', 'score']\nfeature_score","77822c2d":"# get a list of dates \ndates = confirmed_df.keys()[4:]\n\nconfirmed = confirmed_df.loc[:, dates]\ndeaths = deaths_df.loc[:, dates]\nrecoveries = recoveries_df.loc[:, dates]\n\nconfirmed","3dd0caf1":"total_cases = confirmed.sum(axis=0).values\ntotal_deaths = deaths.sum(axis=0).values\ntotal_recoveries = recoveries.sum(axis=0).values\n\n# let's find how many are active still, those who didn't die or recovered but were daignozed\ntotal_active = total_cases - total_deaths - total_recoveries","de143545":"print(\"total cases accumulated = {} \\noverall total_cases = {}\".format(max(total_cases),max(confirmed['7\/13\/20'])))","0ee4191a":"# get the unique countries\ncountries = confirmed_df['Country\/Region'].unique()\nlen(countries)","6801ec51":"def p(x):\n    return x.loc[:,dates].sum(axis=1).values[0]\n\nworst_countries = confirmed_df.groupby('Country\/Region').apply(p).sort_values(ascending = False)[:50]\nworst_countries[:10]","66b06c3a":"fig, ax = plt.subplots(figsize=(10, 10))\nsns.barplot(y=worst_countries.index[:25], x=worst_countries.values[:25])","1f1212a9":"# This method is imp because we have data in accumulative format\ndef daily_values(data):\n    d = [] \n    d.append(data[0])\n    for i in range(1,len(data)):\n        d.append(data[i]-data[i-1]) # get unique date for the day, since it is accumulative\n    return d \n\ndef weekly_average(data):\n    weekly_average = []\n    for i in range(len(data)):\n        if i + 7 < len(data):\n            weekly_average.append(np.mean(data[i:i+7]))\n        else:\n            weekly_average.append(np.mean(data[i:len(data)]))\n    return weekly_average\n\n\n# mortality rate\nmortality_rate = np.array(daily_values(total_deaths))\/np.array(daily_values(total_cases))\n\n#recovery rate\nrecovery_rate = np.array(daily_values(total_recoveries))\/np.array(daily_values(total_cases))","a2776daf":"#days array\ndays = np.array(range(len(dates))).reshape(-1, 1)\n\n# confirmed cases\nglobal_daily_values = daily_values(total_cases)\nglobal_daily_increase_avg = weekly_average(global_daily_values)","473a137b":"fig, ax = plt.subplots(figsize=(12, 7))\n\nsns.barplot(x = list(range(len(global_daily_values))),y = global_daily_values)\nplt.plot(days, global_daily_increase_avg, linestyle='dashed', color='orange')\nplt.title('# of Coronavirus Cases Per Day', size=20)\nplt.xlabel('Days Since 27\/09\/2020', size=15)\nplt.ylabel('# of Cases', size=15)\nplt.legend(['Weekly Average'], prop={'size': 15})\nplt.xticks(size=15)\nplt.yticks(size=15)\nplt.show()\n","245f9a53":"# Daily Deaths\nglobal_daily_deaths = daily_values(total_deaths)\nglobal_daily_death_avg = weekly_average(global_daily_deaths)\n\nfig, ax = plt.subplots(figsize=(12, 7))\nsns.barplot(x = list(range(len(global_daily_deaths))),y=global_daily_deaths)\nplt.plot(days, global_daily_death_avg, linestyle='dashed', color='orange')\nplt.title('# of Coronavirus Cases Deaths Per Day', size=20)\nplt.xlabel('Days Since 27\/09\/2020', size=15)\nplt.ylabel('# of Deaths', size=15)\nplt.legend(['Weekly Average'], prop={'size': 15})\nplt.xticks(size=15)\nplt.yticks(size=15)\nplt.show()","bba1a1db":"# Daily Recoveries\nglobal_daily_recoveries = daily_values(total_recoveries)\nglobal_daily_recoveries_avg = weekly_average(global_daily_recoveries)\n\n\nfig, ax = plt.subplots(figsize=(12, 7))\nsns.barplot(x = list(range(len(global_daily_recoveries))),y=global_daily_recoveries)\nplt.plot(days, global_daily_recoveries_avg, linestyle='dashed', color='orange')\nplt.title('# of Coronavirus Cases Recoveries Per Day', size=20)\nplt.xlabel('Days Since 27\/09\/2020', size=15)\nplt.ylabel('# of Recoveries', size=15)\nplt.legend(['Weekly Average'], prop={'size': 15})\nplt.xticks(size=15)\nplt.yticks(size=15)\nplt.show()","f5cce263":"#Active Cases\nglobal_daily_active = daily_values(total_active)\nglobal_active_avg = weekly_average(global_daily_active)\n\n\nfig, ax = plt.subplots(figsize=(12, 7))\nsns.barplot(x = list(range(len(global_daily_active))),y=global_daily_active)\nplt.plot(days, global_active_avg, linestyle='dashed', color='orange')\nplt.title('# of Active Coronavirus Cases', size=20)\nplt.xlabel('Days Since 27\/09\/2020', size=15)\nplt.ylabel('# of Active Cases', size=15)\nplt.legend(['Weekly Average'], prop={'size': 15})\nplt.xticks(size=15)\nplt.yticks(size=15)\nplt.show()","27c7c9c1":"plt.figure(figsize=(12, 7))\nsns.pointplot(x = days[:,0] , y = np.log(global_daily_values))\nplt.title('Log of # of Coronavirus Cases Per Day', size=20)\nplt.xlabel('Days Since 27\/09\/2020', size=15)\nplt.ylabel('# of Cases', size=15)\nplt.xticks(size=15)\nplt.yticks(size=15)\nplt.grid()","b1ef888d":"plt.figure(figsize=(12, 7))\nsns.pointplot(x = days[:,0] , y = np.log(global_daily_deaths))\nplt.title('Log of # of Coronavirus Deaths Per Day', size=20)\nplt.xlabel('Days Since 27\/09\/2020', size=15)\nplt.ylabel('# of Cases', size=15)\nplt.xticks(size=15)\nplt.yticks(size=15)\nplt.grid()","c5496cba":"plt.figure(figsize=(12, 7))\nsns.pointplot(x = days[:,0] , y = np.log(global_daily_recoveries))\nplt.title('Log of # of Coronavirus Recoveries Per Day', size=20)\nplt.xlabel('Days Since 27\/09\/2020', size=15)\nplt.ylabel('# of Cases', size=15)\nplt.xticks(size=15)\nplt.yticks(size=15)\nplt.grid()","25d8fb23":"plt.figure(figsize=(12, 7))\n\nsns.pointplot(days[:,0], mortality_rate, color='Green')\nplt.axhline(y = np.mean(mortality_rate),linestyle='-', color='red')\nplt.title('Mortality Rate of Coronavirus = {0:2f}'.format(np.mean(mortality_rate)), size=25)\nplt.legend(['mortality rate', 'y='+str(np.mean(mortality_rate))], prop={'size': 15})\nplt.xlabel('Days Since 27\/09\/2020', size=15)\nplt.ylabel('Case Mortality Rate', size=15)\nplt.xticks(size=15)\nplt.yticks(size=15)\nplt.grid()","0dc78e90":"plt.figure(figsize=(12, 7))\n\nsns.pointplot(days[:,0], recovery_rate, color='red')\nplt.axhline(y = np.mean(recovery_rate),linestyle='-', color='black')\nplt.title('Recovery Rate of Coronavirus = {0:2f}'.format(np.mean(recovery_rate)), size=25)\nplt.legend(['recovery rate', 'y='+str(np.mean(recovery_rate))], prop={'size': 15})\nplt.xlabel('Days Since 27\/09\/2020', size=15)\nplt.ylabel('Case Recovery Rate', size=15)\nplt.xticks(size=15)\nplt.yticks(size=15)\nplt.grid()","c9f1fb04":"#Draw a joint reg plot of mortality rate and recovery rate\n\nsns.jointplot(mortality_rate, recovery_rate , kind=\"reg\", size=7)\nplt.show()","89547e39":"!pip install autoviz \n\nfrom autoviz.AutoViz_Class import AutoViz_Class\nfrom IPython.display import display # display from IPython.display\n\nAV = AutoViz_Class()","aefb0919":"# Let's now visualize the plots generated by AutoViz.\nreport_2 = AV.AutoViz('https:\/\/raw.githubusercontent.com\/CSSEGISandData\/COVID-19\/master\/csse_covid_19_data\/csse_covid_19_daily_reports\/08-16-2020.csv')","137df63a":"con_wise_confirm_cases = {}\ncon_wise_recovered_cases = {}\ncon_wise_death_cases = {}\n\nfor c in worst_countries.index:\n    con_wise_confirm_cases[c] = confirmed_df[confirmed_df['Country\/Region']==c].loc[:,dates].sum(axis = 0).values\n    con_wise_death_cases[c] = deaths_df[deaths_df['Country\/Region']==c].loc[:,dates].sum(axis = 0).values\n    con_wise_recovered_cases[c] = recoveries_df[recoveries_df['Country\/Region']==c].loc[:,dates].sum(axis = 0).values\n    \n# Let's look at confirmed cases in India\ncon_wise_confirm_cases['India']","08aa1f4f":"# list of all the countries!!\ncon_wise_confirm_cases.keys()","40181c46":"india_daily = daily_values(con_wise_confirm_cases['India'])\nindia_avg = weekly_average(india_daily)\n\nindia_daith_daily = daily_values(con_wise_death_cases['India'])\nindia_death_avg = weekly_average(india_daith_daily)\n\nindia_recovery_daily = daily_values(con_wise_recovered_cases['India'])\nindia_recovert_avg = weekly_average(india_recovery_daily)\n\nplt.figure(figsize=(13, 7))\nsns.barplot(days[:,0], india_daily)\nplt.title('India Confirmed Cases', size=20)\nplt.plot(days, india_avg, color='orange', linestyle='dashed')\nplt.legend(['Weekly Avg'], prop={'size': 15})\nplt.xlabel('Days Since 27\/09\/2020', size=15)\nplt.ylabel('# of Cases', size=15)\nplt.xticks(size=15)\nplt.yticks(size=15)\nplt.show()\n\nplt.figure(figsize=(13, 7))\nsns.barplot(days[:,0], india_daith_daily)\nplt.title('India Death Cases', size=20)\nplt.plot(days, india_death_avg, color='orange', linestyle='dashed')\nplt.legend(['Weekly Avg'], prop={'size': 15})\nplt.xlabel('Days Since 27\/09\/2020', size=15)\nplt.ylabel('# of Cases', size=15)\nplt.xticks(size=15)\nplt.yticks(size=15)\nplt.show()\n\nplt.figure(figsize=(13, 7))\nsns.barplot(days[:,0], india_recovery_daily)\nplt.title('India Recovery Cases', size=20)\nplt.plot(days, india_recovert_avg, color='orange', linestyle='dashed')\nplt.legend(['Weekly Avg'], prop={'size': 15})\nplt.xlabel('Days Since 27\/09\/2020', size=15)\nplt.ylabel('# of Cases', size=15)\nplt.xticks(size=15)\nplt.yticks(size=15)\nplt.show()","8e5eb3b0":"con_daily = daily_values(con_wise_confirm_cases['Italy'])\ncon_avg = weekly_average(con_daily)\n\ncon_daith_daily = daily_values(con_wise_death_cases['Italy'])\ncon_death_avg = weekly_average(con_daith_daily)\n\ncon_recovery_daily = daily_values(con_wise_recovered_cases['Italy'])\ncon_recovert_avg = weekly_average(con_recovery_daily)\n\nplt.figure(figsize=(13, 7))\nsns.barplot(days[:,0], con_daily)\nplt.title('Italy Confirmed Cases', size=20)\nplt.plot(days, con_avg, color='orange', linestyle='dashed')\nplt.legend(['Weekly Avg'], prop={'size': 15})\nplt.xlabel('Days Since 27\/09\/2020', size=15)\nplt.ylabel('# of Cases', size=15)\nplt.xticks(size=15)\nplt.yticks(size=15)\nplt.show()\n\nplt.figure(figsize=(13, 7))\nsns.barplot(days[:,0], con_daith_daily)\nplt.title('Italy Death Cases', size=20)\nplt.plot(days, con_death_avg, color='orange', linestyle='dashed')\nplt.legend(['Weekly Avg'], prop={'size': 15})\nplt.xlabel('Days Since 27\/09\/2020', size=15)\nplt.ylabel('# of Cases', size=15)\nplt.xticks(size=15)\nplt.yticks(size=15)\nplt.show()\n\nplt.figure(figsize=(13, 7))\nsns.barplot(days[:,0], con_recovery_daily)\nplt.title('Italy Recovery Cases', size=20)\nplt.plot(days, con_recovert_avg, color='orange', linestyle='dashed')\nplt.legend(['Weekly Avg'], prop={'size': 15})\nplt.xlabel('Days Since 27\/09\/2020', size=15)\nplt.ylabel('# of Cases', size=15)\nplt.xticks(size=15)\nplt.yticks(size=15)\nplt.show()","f14776d1":"con_daily = daily_values(con_wise_confirm_cases['US'])\ncon_avg = weekly_average(con_daily)\n\ncon_daith_daily = daily_values(con_wise_death_cases['US'])\ncon_death_avg = weekly_average(con_daith_daily)\n\ncon_recovery_daily = daily_values(con_wise_recovered_cases['US'])\ncon_recovert_avg = weekly_average(con_recovery_daily)\n\nplt.figure(figsize=(13, 7))\nsns.barplot(days[:,0], con_daily)\nplt.title('US Confirmed Cases', size=20)\nplt.plot(days, con_avg, color='orange', linestyle='dashed')\nplt.legend(['Weekly Avg'], prop={'size': 15})\nplt.xlabel('Days Since 27\/09\/2020', size=15)\nplt.ylabel('# of Cases', size=15)\nplt.xticks(size=15)\nplt.yticks(size=15)\nplt.show()\n\nplt.figure(figsize=(13, 7))\nsns.barplot(days[:,0], con_daith_daily)\nplt.title('US Death Cases', size=20)\nplt.plot(days, con_death_avg, color='orange', linestyle='dashed')\nplt.legend(['Weekly Avg'], prop={'size': 15})\nplt.xlabel('Days Since 27\/09\/2020', size=15)\nplt.ylabel('# of Cases', size=15)\nplt.xticks(size=15)\nplt.yticks(size=15)\nplt.show()\n\nplt.figure(figsize=(13, 7))\nsns.barplot(days[:,0], con_recovery_daily)\nplt.title('US Recovery Cases', size=20)\nplt.plot(days, con_recovert_avg, color='orange', linestyle='dashed')\nplt.legend(['Weekly Avg'], prop={'size': 15})\nplt.xlabel('Days Since 27\/09\/2020', size=15)\nplt.ylabel('# of Cases', size=15)\nplt.xticks(size=15)\nplt.yticks(size=15)\nplt.show()","2a6d18f0":"## Cannot use Area Plot with -ve values, so let's go with Line Plot","5f043e08":"plt.figure(figsize=(13, 9))\nsns.lineplot(days[:,0], daily_values(con_wise_confirm_cases['India']))\nsns.lineplot(days[:,0], daily_values(con_wise_confirm_cases['Italy']))\nsns.lineplot(days[:,0], daily_values(con_wise_confirm_cases['US']))\nsns.lineplot(days[:,0], daily_values(con_wise_confirm_cases['Russia']))\nsns.lineplot(days[:,0], daily_values(con_wise_confirm_cases['Spain']))\nsns.lineplot(days[:,0], daily_values(con_wise_confirm_cases['Brazil']))\n\nplt.title('# of Confirmed Coronavirus Cases', size=25)\nplt.xlabel('Days Since 27\/09\/2020', size=15)\nplt.ylabel('# of Cases', size=15)\nplt.legend(['India', 'Italy', 'US', 'Russia', 'Spain','Brazil'], prop={'size': 12})\nplt.xticks(size=15)\nplt.yticks(size=15)\nplt.show()\n","7077ab74":"plt.figure(figsize=(13, 7))\nsns.lineplot(days[:,0], daily_values(con_wise_recovered_cases['India']))\nsns.lineplot(days[:,0], daily_values(con_wise_recovered_cases['Italy']))\nsns.lineplot(days[:,0], daily_values(con_wise_recovered_cases['US']))\nsns.lineplot(days[:,0], daily_values(con_wise_recovered_cases['Russia']))\nsns.lineplot(days[:,0], daily_values(con_wise_recovered_cases['Spain']))\nsns.lineplot(days[:,0], daily_values(con_wise_recovered_cases['Brazil']))\n\nplt.title('# of Recovered Coronavirus Cases', size=25)\nplt.xlabel('Days Since 27\/09\/2020', size=15)\nplt.ylabel('# of Cases', size=15)\nplt.legend(['India', 'Italy', 'US', 'Russia', 'Spain','Brazil'], prop={'size': 12})\nplt.xticks(size=15)\nplt.yticks(size=15)\nplt.show()","1376d4b2":"plt.figure(figsize=(13, 7))\nsns.lineplot(days[:,0], daily_values(con_wise_death_cases['India']))\nsns.lineplot(days[:,0], daily_values(con_wise_death_cases['Italy']))\nsns.lineplot(days[:,0], daily_values(con_wise_death_cases['US']))\nsns.lineplot(days[:,0], daily_values(con_wise_death_cases['Russia']))\nsns.lineplot(days[:,0], daily_values(con_wise_death_cases['Spain']))\nsns.lineplot(days[:,0], daily_values(con_wise_death_cases['Brazil']))\n\nplt.title('# of Coronavirus Deaths', size=25)\nplt.xlabel('Days Since 27\/09\/2020', size=15)\nplt.ylabel('# of Cases', size=15)\nplt.legend(['India', 'Italy', 'US', 'Russia', 'Spain','Brazil'], prop={'size': 12})\nplt.xticks(size=15)\nplt.yticks(size=15)\nplt.show()","5eafb26e":"latest_data_df = pd.read_csv('https:\/\/raw.githubusercontent.com\/CSSEGISandData\/COVID-19\/master\/csse_covid_19_data\/csse_covid_19_daily_reports_us\/09-26-2020.csv')\nlatest_data_df.head() # Latest data has data of only 1 country, USA","62f28c58":"plt.figure(figsize=(10,13))\nsns.barplot(x = latest_data_df['Confirmed'],y=latest_data_df['Province_State'])\nplt.title('USA State wise cases', size=20)\nplt.xlabel('# of case', size=15)\nplt.ylabel('States', size=15)\nplt.show()","84d5f41e":"x = latest_data_df['Confirmed'].sort_values(ascending=False)[:10]\ny = []\nfor i in x.index:\n    y.append(latest_data_df['Province_State'][i])\n    \nx = list(x)\ntemp = sum(latest_data_df['Confirmed'].sort_values(ascending=False)[10:])\nx.append(temp)\ny.append('Others')","b23064f1":"colors = ['grey','blue','red','yellow','green','brown']\nexplode = [0,0,0,0,0,0,0,0,0,0,0.1]\n\n# visual\nplt.figure(figsize = (10,10))\nplt.pie(x, labels=y, explode = explode,colors=colors, autopct='%1.1f%%')\nplt.title('% of cases based on US states',color = 'blue',fontsize = 15)","60821e04":"total_cases_X = np.array(daily_values(total_cases)).reshape(-1, 1)\ntotal_deaths_X = np.array(daily_values(total_deaths)).reshape(-1, 1)\ntotal_recoveries_X = np.array(daily_values(total_recoveries)).reshape(-1, 1)","26378e76":"X_train_confirmed, X_test_confirmed, y_train_confirmed, y_test_confirmed = train_test_split(days, total_cases_X, test_size=0.11, shuffle=False)\nprint(\"size of X_train = {} \\nsize of X_test = {}\".format(len(X_train_confirmed),len(X_test_confirmed)))","056ec361":"from sklearn.metrics import auc, accuracy_score, confusion_matrix, mean_squared_error\nfrom sklearn.model_selection import cross_val_score, GridSearchCV, KFold, RandomizedSearchCV\n\nimport xgboost as xgb\n\nxgb_model = xgb.XGBRegressor(objective=\"reg:linear\")\nxgb_model.fit(X_train_confirmed, y_train_confirmed)","00313ca1":"xg_pred = xgb_model.predict(X_test_confirmed)\nprint(mean_absolute_error(y_test_confirmed, xg_pred))","47cb0e83":"#Try Linear Regression\n\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\nLreg = LinearRegression(normalize=True).fit(X_train_confirmed, y_train_confirmed)\n\n# check against testing data\nm_pred = Lreg.predict(X_test_confirmed)\nplt.plot(y_test_confirmed)\nplt.plot(m_pred)\nplt.legend(['Test Data', 'Linear Predictions'])\nprint('MAE:', mean_absolute_error(xg_pred, y_test_confirmed))\nprint('MSE:',mean_squared_error(xg_pred, y_test_confirmed))","b8449f32":"#Try Ridge Regression\n\nfrom sklearn.linear_model import Ridge\n\nreg = Ridge(alpha=1.0).fit(X_train_confirmed, y_train_confirmed)\n\n# check against testing data\nm_pred = reg.predict(X_test_confirmed)\nplt.plot(y_test_confirmed)\nplt.plot(m_pred)\nplt.legend(['Test Data', 'Ridge Predictions'])\nprint('MAE:', mean_absolute_error(m_pred, y_test_confirmed))\nprint('MSE:',mean_squared_error(m_pred, y_test_confirmed))","889f713d":"# Let's create a formatted date column to do prediction\nstart = '1\/22\/2020'\nstart_date = datetime.datetime.strptime(start, '%m\/%d\/%Y')\nfuture_forcast_dates = []\nfor i in range(len(days)):\n    future_forcast_dates.append((start_date + datetime.timedelta(days=i)).strftime('%Y-%m-%d'))\nfuture_forcast_dates","c631f4b6":"time_df = pd.DataFrame({'date':future_forcast_dates,'Confirmed':daily_values(total_cases)})\ntime_df = time_df.sort_values('date')\ntime_df = time_df.groupby('date')['Confirmed'].sum().reset_index()\ntime_df = time_df.set_index('date')\ntime_df.index = pd.to_datetime(time_df.index)\ntime_df","cf38775c":"temp = time_df['Confirmed'].resample('D').mean()\ntemp","ef5cbe94":"temp.plot(figsize=(15, 6))\nplt.show()","4dc4b12e":"import warnings\nwarnings.filterwarnings(\"ignore\")\nimport statsmodels.api as sm\nfrom pylab import rcParams\nimport itertools\n\nrcParams['figure.figsize'] = 16, 12\n\n# LEt's use statsmodel api to get some visualization and pre-built models\n\ndecomposition = sm.tsa.seasonal_decompose(temp, model='additive')\nfig = decomposition.plot()\nplt.show()","ade4ecfd":"import sklearn.metrics as metrics\n\n# Let's define a function to get all the metrics\n\ndef regression_results(y_true, y_pred):# Regression metrics\n    \n    explained_variance = metrics.explained_variance_score(y_true, y_pred)\n    mean_absolute_error = metrics.mean_absolute_error(y_true, y_pred) \n    mse = metrics.mean_squared_error(y_true, y_pred) \n    mean_squared_log_error = metrics.mean_squared_log_error(y_true, y_pred)\n    median_absolute_error = metrics.median_absolute_error(y_true, y_pred)\n    r2 = metrics.r2_score(y_true, y_pred)\n    \n    print('explained_variance: ', round(explained_variance,4))    \n    print('mean_squared_log_error: ', round(mean_squared_log_error,4))\n    print('r2: ', round(r2,4))\n    print('MAE: ', round(mean_absolute_error,4))\n    print('MSE: ', round(mse,4))\n    print('RMSE: ', round(np.sqrt(mse),4))","1e37a851":"# we will ad a yesterday column basedon which we will predict Confirmed cases\ntime_df.loc[:,'Yesterday'] = time_df.loc[:,'Confirmed'].shift()# inserting another column with day before yesterday's values.\ntime_df = time_df.dropna()\ntime_df","8c796f5b":"X_train_confirmed = time_df[:'2020-06'].drop(['Confirmed'], axis = 1)\ny_train_confirmed = time_df.loc[:'2020-06', 'Confirmed']\n\nX_test_confirmed = time_df['2020-07':].drop(['Confirmed'], axis = 1)\ny_test_confirmed = time_df.loc['2020-07':, 'Confirmed']\n\nX_train_confirmed , y_train_confirmed","775cf850":"from sklearn.model_selection import TimeSeriesSplit\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nrcParams['figure.figsize'] = 17, 9\n    \nmodels = []\nmodels.append(('LR', LinearRegression()))\nmodels.append(('KNN', KNeighborsRegressor())) \nmodels.append(('RF', RandomForestRegressor(n_estimators = 10))) # Ensemble method - collection of many decision trees\nmodels.append(('SVR', SVR(gamma='auto'))) # kernel = linear# Evaluate each model in turn\nresults = []\nnames = []\n\nfor name, model in models:\n    # TimeSeries Cross validation\n    tscv = TimeSeriesSplit(n_splits=22)\n    \n    cv_results = cross_val_score(model, X_train_confirmed, y_train_confirmed, cv=tscv, scoring='r2')\n    results.append(cv_results)\n    names.append(name)\n    print('%s: %f (%f)' % (name, cv_results.mean(), cv_results.std()))\n    \n# Compare Algorithms\nplt.boxplot(results, labels=names)\nplt.title('Algorithm Comparison')\nplt.show()","1ff993b8":"from sklearn.model_selection import GridSearchCV\n\nmodel = RandomForestRegressor()\n\nfrom sklearn.metrics import make_scorer\n\ndef rmse(actual, predict):\n    predict = np.array(predict)\n    actual = np.array(actual)\n    distance = predict - actual\n    square_distance = distance ** 2\n    mean_square_distance = square_distance.mean()\n    score = np.sqrt(mean_square_distance)\n    return score\n\nrmse_score = make_scorer(rmse, greater_is_better = False)\n\nparam_search = { \n    'n_estimators': [25, 50, 100],\n    'max_features': ['auto', 'sqrt', 'log2'],\n    'max_depth' : [i for i in range(5,15)]\n}\ntscv = TimeSeriesSplit(n_splits=5)\ngsearch = GridSearchCV(estimator=model, cv=tscv, param_grid = param_search, scoring = rmse_score)\ngsearch.fit(X_train_confirmed, y_train_confirmed)\nbest_score = gsearch.best_score_\nbest_model = gsearch.best_estimator_","e1e8c5f1":"best_model","5ba1e1de":"y_true = y_test_confirmed.values\ny_pred = best_model.predict(X_test_confirmed)\n\nregression_results(y_true, y_pred)","a7d0e66e":"plt.figure(figsize=(10,7))\nplt.plot(y_true)\nplt.plot(y_pred)\nplt.legend(['Test Data', 'Random Forest'])\nprint('MAE:', mean_absolute_error(y_pred, y_test_confirmed))\nprint('MSE:',mean_squared_error(y_pred, y_test_confirmed))","85e07fdb":"## Worst affected counutries till date, take top 50 worst","03446ae8":"### A very large chi square test statistic means that the data does not fit very well. In other words, there isn't a relationship.\n\n. In feature selection, we aim to select the features which are highly dependent on the response.\n\nWhen two features are independent, the observed count is close to the expected count, thus we will have smaller Chi-Square value. So high Chi-Square value indicates that the hypothesis of independence is incorrect. In simple words, higher the Chi-Square value the feature is more dependent on the response and it can be selected for model training.\n\n\nHere we have very high values, which doesn't seem like any variable is good to predict Confirmed case. Let's first look at time series data and see what we can dig out\n\n","a3dca35f":"**Confirm, death, and Active have high positive corelation, where as rest has negative corelation**\n\n### We should find pearson correlation to better understand how much the correlation is and what we want to keep, or should we spend time on this data","79564e38":"## With this we can see that data is accumulated over dates, which means it is not showing us new cases of every day, but accumulative cases till date. \n\n## We will need to find daily average of the cases to get a better idea","a1477648":"### Notice that I've not Normalized the data, which probably I should have... rihgt? or...","18ea97ce":"## Let's have a brief look at csse daily data","962200f8":"**There for sure is some trend going on**","dd03040f":"### But we can use our daily cases data to pretend a trend if any through Time Series Prediction\n\n**We can use LSTM and XGBoost as our predictive Networks but the data is too small, i,e little over 150. Using such ensemble and large models would lead to bad results only, so let's try to do Regression on this. Let's try simple Linear Regression Since data is very much Linear**","6d1fc210":"- Cross validation could help in understanding and fine tunning the model,\n- let's use cross-validation this time to predict Confirmed Cases\n- Notice we can't use simple cross validation when dealing with TimeSeriesData. A better alternative for cross validation on time series data (than K-fold CV) is Forward Chaining strategy.","48620f8a":"# Let's Use AutoViz module on csse_daily df to see if it can bring in some useful info","ad11151c":"## We can study relationship between pairs of columns and deduce the relationship and significance of Variables","42f9c5ad":"## Let's look at the Recovery and mortality rate ","01ff4914":"**Worst hit state in US is California, followed by Texas, Florida, and New York**","149153fd":"### hmm... not what we expected.. now is it?\n- Hold on for the improvised notebook ;)\n- Deal Breaker is.... We can improve the way we are doing Time Series Analysis!!\n\n### We can easily do the same with recoveries and deaths trends too :)","3809d638":"## Testing of Columns Relationships**\n\n1.) Plot the heatmap of correlation\n\n2.) get pearson correlation and drop columns with certain values\n\n3.) Do a chi2 test to get the significance level if necessary\n\n4.) Do a z-value or IQR test to see outliers if necessary","5179bc5d":"### Some values in Recoveries data shows -ve recoveries. This is the inconsostency and Outliers in Data which needs to be fixed. There are 4 faulty values.","4cb1702f":"### Ridge Regression and Linear Regression both yield similar results. It is obvious as there was not much of an overfitting!\n#### and Mostly Because of the lack of data availablity and the way we are predicting.","82757923":"---\n## We cannot use days as we are currently using, so let's change the approach","0d47d23e":"# Let's look at Time series data","900df7c4":"### Let's do a sanity check on the dates and see if all have same dates","915f622a":"hmmmmm.......","862900a5":"#### pearsonr= if it is 1, there is positive correlation and if it is, -1 there is negative correlation.\n#### If it is zero, there is no correlation between variables","fa2aed23":"---\nLet's get excited because there's so muh to learn!\n### We will do Intensive EDA, AutoViz, Pie Charts and all, along with TimeSeries Predictions!\n---","932a5543":"## WAIT!!\n\n\n# Looking for 100 Data Science Interview Questions?\n## I've got you: *https:\/\/www.linkedin.com\/posts\/alaapdhall_day-7-of-100-data-science-interview-questions-activity-6712629560569069568-yQbn*\n\n## If you're Interested in Deep Learning with PyTorch, visit https:\/\/www.aiunquote.com for 100 project in Deep Learning Series!!\n---","6cc0e2e5":"It is showing is a good amount of Correlation between variables!! There's so much to extract form these plots","c440ee3b":"### We can see that Latest Data has only US as country, and CSSE data has no variable that can be used to do our predictions, well except Independence_rate, as proven by Pearson corr and Chi sq test.","25ffaaef":"**Recovery df has some missing rows**","be4f17e0":"## Number of samples per country in our data","1279a14a":"## Let's store data of countries in a dictionary ","f3b02fb4":"## Prediction of cases are improved by using TimeSeriesSplit() \nWe will continue to work on this notebook!!\n\n#### Please visit my website wherein I am doing 100 Projects in Deep Learning.\n---\n### [AI Unquote](https:\/\/www.aiunquote.com)\n### Follow me on LinkedIn where I post DS related Stuff everyday!!  [Alaap Dhall](https:\/\/www.linkedin.com\/in\/alaapdhall\/)\n***\n\n<br>\n\n**Thank you**","4a2ef77d":"## It looks like most of the data is very much skewed towards US, with 3000+ entries\n\nIn general you fix this skewness by removing outliers, UnderSampling, taking Log of Values etc. But that depends on the data, we basically will only use US to do our Analysis!!\n\n*So, let's focus on US only and see what columns have significance in the result or what are the correlations, to see whether we should do time series prediction only or should also look at other variables to consider.*","209c7f01":"## Total cases country wise","854fd15e":"### I will do Time-Series Split later when we predict using TimeSeries currently I am using simple split. We could do some Feature Engineering and make some new feature but then anything I do will be baised because of cases in US in our data provided. I'll simply do some normal split and try if regression works","f67d5e7a":"### Let's get Log Values","9945779b":"**Active has very high corr value, which is obvious but rest actually have corr value very close to 0, i.e 0.01,0.04 and values like these are never good to use in modelling, and will yield for lower p-values. \nWe can remove them and will be left with \"Incidence_Rate\"**\n\nLet's do a **chi sq test** and find sifnificance if we have any, I am not yet normalizing the data tho. \n\n[*We can use p-value test as well to get significance, if any*]","4a5d2363":"**This shows us how the virus is spread out through out the world acc to given data. Seems like Data is skewed towards US more**"}}