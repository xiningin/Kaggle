{"cell_type":{"b6a207f7":"code","2b52fe9c":"code","8005bcc9":"code","9c24c46c":"code","5e1d99c4":"code","a73573fb":"code","f15e363e":"code","0de35521":"code","ee450dfd":"code","07cd6e7c":"code","80adee53":"code","92008223":"code","9c7d681e":"code","0a4e5914":"code","e17b3c3b":"code","add4b9e4":"code","93506c23":"code","af289557":"code","8ba534c8":"code","ccfbca50":"code","dac4059b":"code","46949a29":"code","c6226bb8":"code","b441271f":"code","d1ee7249":"code","5fc295ce":"code","8dcc4230":"code","edf6a3c4":"code","eff9916d":"code","9e222787":"code","ccb351c5":"code","925c3708":"code","d226cf9e":"code","f417b7cd":"code","4eb5ba58":"code","1f5b5308":"code","40d6322e":"code","cffe8e50":"code","49390907":"code","0cb34f8c":"code","48b29333":"code","db1f9eda":"code","782155ed":"code","6f28bea3":"code","26621ba7":"code","1f0ac3a3":"code","93ac4192":"code","3af8799c":"code","2110bcef":"code","6ed0f371":"code","4eff366c":"code","64825545":"code","e2b866d1":"code","fbc52e25":"code","74bf452d":"code","2e245559":"code","db2b61c7":"markdown","c7ecc478":"markdown","44a7032e":"markdown","9ac0b1f3":"markdown","b4c7f635":"markdown","5951548b":"markdown","6b64256e":"markdown","1229c5bc":"markdown","f9441272":"markdown","98173e2d":"markdown","f7b5bb67":"markdown","4e04d80b":"markdown","86fdf35e":"markdown","43002b1f":"markdown","ce5c6e34":"markdown","738df4ad":"markdown","a4c39bd0":"markdown","9fd3f3c7":"markdown","b048bee7":"markdown","c7d2835f":"markdown","5038c282":"markdown","911c9004":"markdown","a3d14318":"markdown","f37ca2a6":"markdown","c3ace543":"markdown","e1c2995f":"markdown"},"source":{"b6a207f7":"# Import the necessary packages used in this notebook\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.graph_objects as go\nimport plotly.express as px\nimport plotly\nimport os\nimport os\n\nfrom sklearn.preprocessing import LabelEncoder\nprint(os.listdir(\"..\/input\"))\nimport warnings\n# Remove any warning messages\nwarnings.filterwarnings(\"ignore\")\n# Any results you write to the current directory are saved as output.","2b52fe9c":"datafr = pd.read_csv(\"..\/input\/mushroom-classification\/mushrooms.csv\", error_bad_lines=False)","8005bcc9":"datafr.shape","9c24c46c":"datafr.head()","5e1d99c4":"# Replace ? with NaN\ndatafr = datafr.replace(r'^\\?$', np.nan, regex=True)","a73573fb":"# Check the missing values in the column\nmissing_data = datafr.isnull().sum().sort_values(ascending=False)","f15e363e":"# Check for Data Duplication\nduplicateRowsDF = datafr[datafr.duplicated()]\nduplicateRowsDF","0de35521":"missing_data = missing_data.reset_index(drop=False)\nmissing_data = missing_data.rename(columns={\"index\": \"Columns\", 0: \"Value\"})\nmissing_data['Proportion'] = (missing_data['Value']\/len(datafr))*100","ee450dfd":"sample = missing_data[missing_data['Proportion']>10]\nfig = px.pie(sample, names='Columns', values='Proportion',\n             color_discrete_sequence=px.colors.sequential.Viridis_r,\n             title='Columns with a percentage of Missing values over 10%')\nfig.update_traces(textposition='inside', textinfo='label')\nfig.update_layout(paper_bgcolor='rgba(0,0,0,0)',\n                  plot_bgcolor='rgba(0,0,0,0)',\n                  font=dict(family='Cambria, monospace', size=12, color='#000000'))\nfig.show()","07cd6e7c":"# Fill Missing Value\ndatafr = datafr.fillna(method='ffill')","80adee53":"fig = px.pie(datafr, names='class',\n             color_discrete_sequence=px.colors.sequential.Viridis_r,\n             title='Proportion of data for Class column')\nfig.update_traces(textposition='inside', textinfo='percent+label')\nfig.update_layout(paper_bgcolor='rgba(0,0,0,0)',\n                  plot_bgcolor='rgba(0,0,0,0)',\n                  font=dict(family='Cambria, monospace', size=12, color='#000000'))\nfig.show()","92008223":"datafr.columns","9c7d681e":"fig = px.pie(datafr, names='cap-shape',\n             color_discrete_sequence=px.colors.sequential.Viridis_r,\n             title='Proportion of data for Cap-Shape column ')\nfig.update_traces(textposition='inside', textinfo='percent+label')\nfig.update_layout(paper_bgcolor='rgba(0,0,0,0)',\n                  plot_bgcolor='rgba(0,0,0,0)',\n                  font=dict(family='Cambria, monospace', size=12, color='#000000'))\nfig.show()","0a4e5914":"fig = px.pie(datafr, names='cap-color',\n             color_discrete_sequence=px.colors.sequential.Viridis_r,\n             title='Proportion of data for Cap-Color column ')\nfig.update_traces(textposition='inside', textinfo='percent+label')\nfig.update_layout(paper_bgcolor='rgba(0,0,0,0)',\n                  plot_bgcolor='rgba(0,0,0,0)',\n                  font=dict(family='Cambria, monospace', size=12, color='#000000'))\nfig.show()","e17b3c3b":"fig = px.pie(datafr, names='bruises',\n             color_discrete_sequence=px.colors.sequential.Viridis_r,\n             title='Proportion of data for Bruises column ')\nfig.update_traces(textposition='inside', textinfo='percent+label')\nfig.update_layout(paper_bgcolor='rgba(0,0,0,0)',\n                  plot_bgcolor='rgba(0,0,0,0)',\n                  font=dict(family='Cambria, monospace', size=12, color='#000000'))\nfig.show()","add4b9e4":"fig = px.pie(datafr, names='gill-attachment',\n             color_discrete_sequence=px.colors.sequential.Viridis_r,\n             title='Proportion of data for Gill Attachment column ')\nfig.update_traces(textposition='inside', textinfo='percent+label')\nfig.update_layout(paper_bgcolor='rgba(0,0,0,0)',\n                  plot_bgcolor='rgba(0,0,0,0)',\n                  font=dict(family='Cambria, monospace', size=12, color='#000000'))\nfig.show()","93506c23":"fig = px.pie(datafr, names='gill-size',\n             color_discrete_sequence=px.colors.sequential.Viridis_r,\n             title='Proportion of data for Gill Size column ')\nfig.update_traces(textposition='inside', textinfo='percent+label')\nfig.update_layout(paper_bgcolor='rgba(0,0,0,0)',\n                  plot_bgcolor='rgba(0,0,0,0)',\n                  font=dict(family='Cambria, monospace', size=12, color='#000000'))\nfig.show()","af289557":"fig = px.pie(datafr, names='population',\n             color_discrete_sequence=px.colors.sequential.Viridis_r,\n             title='Proportion of data for Population column ')\nfig.update_traces(textposition='inside', textinfo='percent+label')\nfig.update_layout(paper_bgcolor='rgba(0,0,0,0)',\n                  plot_bgcolor='rgba(0,0,0,0)',\n                  font=dict(family='Cambria, monospace', size=12, color='#000000'))\nfig.show()","8ba534c8":"from sklearn.metrics import roc_curve\nfrom sklearn.metrics import roc_auc_score\n\nfrom sklearn.model_selection import cross_val_score\n# Using 10 folds cross-validation\ndef CrossVal(trainX,trainY,model):\n    accuracy=cross_val_score(model,trainX , trainY, cv=10, scoring='accuracy')\n    return(accuracy)","ccfbca50":"'''\nfrom sklearn.ensemble import RandomForestClassifier\nforest = RandomForestClassifier(random_state = 1)\nmodelF = forest.fit(X_train, Y_train)\ny_predF = modelF.predict(X_test)\n\nfrom sklearn.model_selection import GridSearchCV\nn_estimators = [100, 200, 300, 400, 500]\nmax_depth = [5, 8, 15, 25, 30]\nmin_samples_split = [2, 5, 10, 15, 100]\nmin_samples_leaf = [1, 2, 5, 10] \n\nhyperF = dict(n_estimators = n_estimators, max_depth = max_depth,  \n              min_samples_split = min_samples_split, \n             min_samples_leaf = min_samples_leaf)\n\ngridF = GridSearchCV(forest, hyperF, cv = 3, verbose = 1, \n                      n_jobs = -1)\nbestF = gridF.fit(X_train, Y_train)\n'''","dac4059b":"from sklearn.ensemble import RandomForestClassifier\ndef random_forest(X_train,Y_train, X_test):\n    # Next we take Random Forest Model (Ensemble) for Binary Classification\n    rf = RandomForestClassifier(n_estimators = 200,random_state = 40)\n    # Creare a model with X_train and Y_train data\n    rf.fit(X_train,Y_train)\n    # predict probabilities\n    probs = rf.predict_proba(X_test)\n    # keep probabilities for the positive outcome only\n    probs = probs[:, 1]\n    return rf, probs","46949a29":"sample_1 = datafr.copy()\n\nlabel = LabelEncoder()\nfor col in sample_1.columns:\n    sample_1[col] = label.fit_transform(sample_1[col])\n# Print Updated Data\nsample_1.head(10)","c6226bb8":"# Splitting the dataset\n# Predictor variables\nX = sample_1.drop('class',axis=1)\n# Target or Class variable\nY = sample_1['class']","b441271f":"# Let's using scikit learn to split our dataset\nfrom sklearn.model_selection import train_test_split\n# Using 70:30 ratio for train:test\nX_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size=.3,random_state=400)","d1ee7249":"X_train.shape","5fc295ce":"X_test.shape","8dcc4230":"# Call Random Forest Classifier\nrf1, probs = random_forest(X_train,Y_train, X_test)","edf6a3c4":"sorted_idx = rf1.feature_importances_.argsort()\nplt.barh(X.columns[sorted_idx], rf1.feature_importances_[sorted_idx])\nplt.xlabel(\"Random Forest Feature Importance\")","eff9916d":"import shap\nexplainer = shap.TreeExplainer(rf1)\nshap_values = explainer.shap_values(X_test)\nshap.summary_plot(shap_values, X_test, plot_type=\"bar\")","9e222787":"# Run the model on X_test to predict the target labels. Use cross-validation accuracy to check if model overfits or underfits\npredict1 = rf1.predict(X_test)\nrf1 = RandomForestClassifier(n_estimators=200, n_jobs=-1, random_state=40)\nscore_rf = CrossVal(X_train,Y_train,rf1)\nprint('Cross-Validation accuracy is {:.2f}%'.format(score_rf.mean()*100))","ccb351c5":"# Compare the predicted target labels with Y_test\nfrom sklearn.metrics import accuracy_score,confusion_matrix, f1_score\nprint(\"Accuracy using Random Forest Model: {:.2f}%\".format(accuracy_score(Y_test,predict1)*100))\n# assign cnf_matrix with result of confusion_matrix array\ncnf_matrix = confusion_matrix(Y_test,predict1)\n\n# calculate AUC\nauc_rf = roc_auc_score(Y_test, probs)\n#print('AUC: %.3f' % auc)\n# calculate roc curve\nfpr, tpr, thresholds = roc_curve(Y_test, probs)\n# plot no skill\nplt.plot([0, 1], [0, 1], linestyle='--')\n# plot the roc curve for the model\nplt.plot(fpr, tpr, marker='.')\nplt.title(\"ROC Curve for Random Forest with AUC Score: {:.3f}\".format(auc_rf))\n# show the plot\nplt.show()\n\n#create a heat map\nsns.heatmap(pd.DataFrame(cnf_matrix), annot = True, cmap = 'Purples', fmt = 'd')\nrf_f1=f1_score(Y_test,predict1)\nplt.title('F1 Score for Random Forest model is {:.2f}'.format(rf_f1))","925c3708":"sample_2 = datafr.copy()\nle = LabelEncoder()\nsample_2['Class Encoded'] = le.fit_transform(sample_2['class'])\n# Predictor variables\nX = sample_2.drop(['class', 'Class Encoded'],axis=1)\n\nnew_df = pd.DataFrame()\nfor col in X.columns:\n    y = pd.get_dummies(X[col], prefix=col)\n    new_df = pd.concat([new_df, y], axis=1)\n# Print Updated Data\nnew_df.head(10)","d226cf9e":"# Principal Component Analysis\nfrom sklearn.decomposition import PCA\npca = PCA(n_components=5)\nprincipal_component = pca.fit_transform(new_df)\nprincipalDf = pd.DataFrame(data = principal_component,\n                           columns = ['principal component 1', 'principal component 2',\n                                     'principal component 3', 'principal component 4',\n                                     'principal component 5'])","f417b7cd":"principalDf.head(10)","4eb5ba58":"# Splitting the dataset\n# Predictor variables\nX = principalDf\n# Target or Class variable\nY = sample_2['Class Encoded']","1f5b5308":"# Let's using scikit learn to split our dataset\nfrom sklearn.model_selection import train_test_split\n# Using 70:30 ratio for train:test\nX_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size=.3,random_state=400)","40d6322e":"X_train.shape","cffe8e50":"X_test.shape","49390907":"# Call Random Forest Classifier\nrf2, probs = random_forest(X_train,Y_train, X_test)","0cb34f8c":"sorted_idx = rf2.feature_importances_.argsort()\nplt.barh(X.columns[sorted_idx], rf2.feature_importances_[sorted_idx])\nplt.xlabel(\"Random Forest Feature Importance\")","48b29333":"import shap\nexplainer = shap.TreeExplainer(rf2)\nshap_values = explainer.shap_values(X_test)\nshap.summary_plot(shap_values, X_test, plot_type=\"bar\")","db1f9eda":"# Run the model on X_test to predict the target labels. Use cross-validation accuracy to check if model overfits or underfits\npredict2 = rf2.predict(X_test)\nrf2 = RandomForestClassifier(n_estimators=200, n_jobs=-1, random_state=40)\nscore_rf = CrossVal(X_train,Y_train,rf2)\nprint('Cross-Validation accuracy is {:.2f}%'.format(score_rf.mean()*100))","782155ed":"# Compare the predicted target labels with Y_test\nfrom sklearn.metrics import accuracy_score,confusion_matrix, f1_score\nprint(\"Accuracy using Random Forest Model: {:.2f}%\".format(accuracy_score(Y_test,predict2)*100))\n# assign cnf_matrix with result of confusion_matrix array\ncnf_matrix = confusion_matrix(Y_test,predict2)\n\n# calculate AUC\nauc_rf = roc_auc_score(Y_test, probs)\n#print('AUC: %.3f' % auc)\n# calculate roc curve\nfpr, tpr, thresholds = roc_curve(Y_test, probs)\n# plot no skill\nplt.plot([0, 1], [0, 1], linestyle='--')\n# plot the roc curve for the model\nplt.plot(fpr, tpr, marker='.')\nplt.title(\"ROC Curve for Random Forest with AUC Score: {:.3f}\".format(auc_rf))\n# show the plot\nplt.show()\n\n#create a heat map\nsns.heatmap(pd.DataFrame(cnf_matrix), annot = True, cmap = 'Purples', fmt = 'd')\nrf2_f1 = f1_score(Y_test,predict2)\nplt.title('F1 Score for Random Forest model is {:.2f}'.format(rf2_f1))","6f28bea3":"sample_3 = datafr.copy()\nle = LabelEncoder()\nsample_3['Class Encoded'] = le.fit_transform(sample_3['class'])\nsample_3 = sample_3.drop('class', axis=1)\nsample_3.head()","26621ba7":"# Source: https:\/\/maxhalford.github.io\/blog\/target-encoding-done-the-right-way\/\ndef calc_smooth_mean(df1, df2, cat_name, target, weight):\n    # Compute the global mean\n    mean = sample_3[target].mean()\n\n    # Compute the number of values and the mean of each group\n    agg = sample_3.groupby(cat_name)[target].agg(['count', 'mean'])\n    counts = agg['count']\n    means = agg['mean']\n\n    # Compute the \"smoothed\" means\n    smooth = (counts * means + weight * mean) \/ (counts + weight)\n\n    # Replace each value by the according smoothed mean\n    if df2 is None:\n        return df1[cat_name].map(smooth)\n    else:\n        return df1[cat_name].map(smooth),df2[cat_name].map(smooth.to_dict())","1f0ac3a3":"# Target Encode all the columns in sample_3 dataset except Class variable\nWEIGHT = 5\nnew_df = pd.DataFrame()\nfor col in sample_3.columns[:-1]:\n    new_df[col] = calc_smooth_mean(df1=sample_3, df2=None, cat_name=col, target='Class Encoded', weight=WEIGHT)","93ac4192":"# Target Encoded columns\nnew_df.head(10)","3af8799c":"# Splitting the dataset\n# Predictor variables\nX = new_df\n# Target or Class variable\nY = sample_3['Class Encoded']","2110bcef":"# Let's using scikit learn to split our dataset\nfrom sklearn.model_selection import train_test_split\n# Using 70:30 ratio for train:test\nX_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size=.3,random_state=400)","6ed0f371":"X_train.shape","4eff366c":"X_test.shape","64825545":"# Call Random Forest Classifier\nrf3, probs = random_forest(X_train,Y_train, X_test)","e2b866d1":"sorted_idx = rf3.feature_importances_.argsort()\nplt.barh(X.columns[sorted_idx], rf3.feature_importances_[sorted_idx])\nplt.xlabel(\"Random Forest Feature Importance\")","fbc52e25":"import shap\nexplainer = shap.TreeExplainer(rf3)\nshap_values = explainer.shap_values(X_test)\nshap.summary_plot(shap_values, X_test, plot_type=\"bar\")","74bf452d":"# Run the model on X_test to predict the target labels. Use cross-validation accuracy to check if model overfits or underfits\npredict3 = rf3.predict(X_test)\nrf3 = RandomForestClassifier(n_estimators=200, n_jobs=-1, random_state=40)\nscore_rf = CrossVal(X_train,Y_train,rf3)\nprint('Cross-Validation accuracy is {:.2f}%'.format(score_rf.mean()*100))","2e245559":"# Compare the predicted target labels with Y_test\nfrom sklearn.metrics import accuracy_score,confusion_matrix, f1_score\nprint(\"Accuracy using Random Forest Model: {:.2f}%\".format(accuracy_score(Y_test,predict3)*100))\n# assign cnf_matrix with result of confusion_matrix array\ncnf_matrix = confusion_matrix(Y_test,predict3)\n\n# calculate AUC\nauc_rf = roc_auc_score(Y_test, probs)\n#print('AUC: %.3f' % auc)\n# calculate roc curve\nfpr, tpr, thresholds = roc_curve(Y_test, probs)\n# plot no skill\nplt.plot([0, 1], [0, 1], linestyle='--')\n# plot the roc curve for the model\nplt.plot(fpr, tpr, marker='.')\nplt.title(\"ROC Curve for Random Forest with AUC Score: {:.3f}\".format(auc_rf))\n# show the plot\nplt.show()\n\n#create a heat map\nsns.heatmap(pd.DataFrame(cnf_matrix), annot = True, cmap = 'Purples', fmt = 'd')\nrf3_f1 = f1_score(Y_test,predict3)\nplt.title('F1 Score for Random Forest model is {:.2f}'.format(rf3_f1))","db2b61c7":"## 1. Label Encoding\nIn this encoding technique, the categorical data is assigned a value from 1 to N (N is the number for different categories present in the data). This kind of an encoding technique is applied to the ordinal data. The assigning of the value from 1 to N happens either in an increasing or a decreasing order. Once if the order is chosen to be ascending or descending it is fixed throughout for all the values in the column and cannot be changed randomly or in between. ","c7ecc478":"### Define Random Forest Model Function","44a7032e":"#### 2) Feature Importance computed with SHAP values\nThe SHAP interpretation can be used (it is model-agnostic) to compute the feature importances from the Random Forest. It is using the Shapley values from game theory to estimate the how does each feature contribute to the prediction.","9ac0b1f3":"## 3) Target Encoding\n\n\u201cfeatures are replaced with a blend of posterior probability of the target given particular categorical value and the prior probability of the target over all the training data.\u201d","b4c7f635":"### Exploratory Data Analysis","5951548b":"### Define Cross Validation Function","6b64256e":"### Display Results","1229c5bc":"### Check the proportion of data for each class","f9441272":"### Random Forest Feature Importance","98173e2d":"### Attribute Information:\n![](https:\/\/i.pinimg.com\/originals\/ab\/0d\/d4\/ab0dd459bfeeb16857224738e7919da1.gif)\n* **classes**: edible=e, poisonous=p\n* **cap-shape**: bell=b,conical=c,convex=x,flat=f, knobbed=k,sunken=s\n* **cap-surface**: fibrous=f,grooves=g,scaly=y,smooth=s\n* **cap-color**: brown=n,buff=b,cinnamon=c,gray=g,green=r,pink=p,purple=u,red=e,white=w,yellow=y\n* **bruises**: bruises=t,no=f\n* **odor**: almond=a,anise=l,creosote=c,fishy=y,foul=f,musty=m,none=n,pungent=p,spicy=s\n* **gill-attachment**: attached=a,descending=d,free=f,notched=n\n* **gill-spacing**: close=c,crowded=w,distant=d\n* **gill-size**: broad=b,narrow=n\n* **gill-color**: black=k,brown=n,buff=b,chocolate=h,gray=g, green=r,orange=o,pink=p,purple=u,red=e,white=w,yellow=y\n* **stalk-shape**: enlarging=e,tapering=t\n* **stalk-root**: bulbous=b,club=c,cup=u,equal=e,rhizomorphs=z,rooted=r,missing=?\n* **stalk-surface-above-ring**: fibrous=f,scaly=y,silky=k,smooth=s\n* **stalk-surface-below-ring**: fibrous=f,scaly=y,silky=k,smooth=s\n* **stalk-color-above-ring**: brown=n,buff=b,cinnamon=c,gray=g,orange=o,pink=p,red=e,white=w,yellow=y\n* **stalk-color-below-ring**: brown=n,buff=b,cinnamon=c,gray=g,orange=o,pink=p,red=e,white=w,yellow=y\n* **veil-type**: partial=p,universal=u\n* **veil-color**: brown=n,orange=o,white=w,yellow=y\n* **ring-number**: none=n,one=o,two=t\n* **ring-type**: cobwebby=c,evanescent=e,flaring=f,large=l,none=n,pendant=p,sheathing=s,zone=z\n* **spore-print-color**: black=k,brown=n,buff=b,chocolate=h,green=r,orange=o,purple=u,white=w,yellow=y\n* **population**: abundant=a,clustered=c,numerous=n,scattered=s,several=v,solitary=y\n* **habitat**: grasses=g,leaves=l,meadows=m,paths=p,urban=u,waste=w,woods=d","f7b5bb67":"#### 1) Built In Feature Importance","4e04d80b":"#### 2) Feature Importance computed with SHAP values\nThe SHAP interpretation can be used (it is model-agnostic) to compute the feature importances from the Random Forest. It is using the Shapley values from game theory to estimate the how does each feature contribute to the prediction.","86fdf35e":"In this kernel, we've explored different ways of identifying the feature importance in a tree based algorithm (Random Forest) also we have tried 3 different ways of encoding 'categorical' data to 'numeric' data.\n\nI hope it was a helpful kernel for those trying out these algorithms and beginning their journey in Data Science domain.","43002b1f":"#### 2) Feature Importance computed with SHAP values\nThe SHAP interpretation can be used (it is model-agnostic) to compute the feature importances from the Random Forest. It is using the Shapley values from game theory to estimate the how does each feature contribute to the prediction.","ce5c6e34":"## 2. One Hot Encoding (with PCA)\nOne-hot encoding is easier to conceptually understand. This type of encoding simply \u201cproduces one feature per category, each binary.\u201d Or for the example above, creating a new feature for cat, dog, and hamster. In the column cat, for example, we show that a cat exists with a 1, and it doesn\u2019t exist with a 0.","738df4ad":"### Random Forest Feature Importance","a4c39bd0":"![](https:\/\/i.insider.com\/5c3fad1a5241471eae53d303?width=1100&format=jpeg&auto=webp)","9fd3f3c7":"**Cons:** <br>\nThe problem using the number is that they introduce relation\/comparison between them. Apparently, there is no\nrelation between various bridge type. The algorithm might misunderstand that data has some kind of hierarchy\/order 0 < 1 < 2 \u2026 < 6.","b048bee7":"#### 1) Built In Feature Importance","c7d2835f":"#### 1) Built In Feature Importance","5038c282":"### Display Results","911c9004":"### Hyperparameter Tuning for Random Forest","a3d14318":"### Random Forest Feature Importance","f37ca2a6":"**Problem Statement**: Mushroom hunting, mushrooming, mushroom picking, mushroom foraging, and similar terms describe the activity of gathering mushrooms in the wild, typically for culinary use. This practice is popular throughout most of Europe, Australia, Japan, Korea, parts of the Middle East, and the Indian subcontinent, as well as the temperate regions of Canada and the United States.","c3ace543":"In this kernel we're gonna explore an ensemble based model called Random Forest and further dig down to the following: <br>\n*  Random Forest with Tuning\n* Identifying ways to estimate Feature Importance\n    * 1. Built-in\n    * 2. SHAP values\n* Try 3 different types of Encoding (Categorical data -> Numeric data)\n    * 1. Label Encoding\n    * 2. One Hot Encoding\n    * 3. Target Encoding\n\n[Reference 1](https:\/\/medium.com\/analytics-vidhya\/target-encoding-vs-one-hot-encoding-with-simple-examples-276a7e7b3e64)\n[Reference 2](https:\/\/www.analyticsvidhya.com\/blog\/2020\/08\/types-of-categorical-data-encoding\/)","e1c2995f":"### Display Results"}}