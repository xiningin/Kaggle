{"cell_type":{"0b7fcf7a":"code","ba7c1904":"code","c4d97492":"code","b519ada9":"code","281f36df":"code","48ee09c3":"code","264bd395":"code","79a78fea":"code","3725ac90":"code","a913cf1e":"code","20279505":"code","c088818a":"code","69bdc8e4":"code","8d3ca3d0":"code","a028a995":"code","4ccd5108":"code","37c0313f":"code","797b1e07":"code","68e40462":"code","c712f179":"code","f92c59ad":"code","4667baf4":"code","c7805012":"code","e3aee3e0":"code","740aa15d":"code","924d97d4":"code","4cd83056":"code","bca65204":"code","9d02194c":"code","b8333b06":"code","00ab7ee3":"code","bb78caeb":"code","fb46d252":"code","473bfa55":"code","f1eb372d":"code","6f2ead45":"code","70fd993d":"code","11296c5c":"code","6112f04c":"code","340f3c8f":"code","d8b40a47":"code","5830bd9f":"code","217c837d":"markdown","7562ecbd":"markdown","0244aeb9":"markdown","77eecfd6":"markdown","3c86a4b2":"markdown","f2eab880":"markdown","7e585eeb":"markdown","9130be98":"markdown"},"source":{"0b7fcf7a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ba7c1904":"# Let's import the data\ndf = pd.read_csv('..\/input\/red-wine-quality-cortez-et-al-2009\/winequality-red.csv')\ndf","c4d97492":"df.describe()","b519ada9":"#Now, we will review the using histogram as\ndf.hist(figsize=(20,18), bins= 50)\n","281f36df":"df.corr()\nplt.figure(figsize=(15,12))\nsns.heatmap(df.corr(), annot=True)","48ee09c3":"df.isnull().sum()","264bd395":"plt.figure(figsize=(12,10))\nsns.boxplot(df['quality'], df['fixed acidity'], orient='v', color='grey')","79a78fea":"plt.figure(figsize=(12,10))\nsns.boxplot(df['quality'], df['volatile acidity'], orient='v', color='red')","3725ac90":"plt.figure(figsize=(12,10))\nsns.boxplot(df['quality'], df['citric acid'], orient='v', color='purple')","a913cf1e":"plt.figure(figsize=(12,10))\nsns.boxplot(df['quality'], df['citric acid'], orient='v', color='gold')","20279505":"plt.figure(figsize=(12,10))\nsns.boxplot(df['quality'], df['residual sugar'], orient='v', color='blue')","c088818a":"plt.figure(figsize=(12,10))\nsns.boxplot(df['quality'], df['chlorides'], orient='v', color='green')","69bdc8e4":"plt.figure(figsize=(12,10))\nsns.boxplot(df['quality'], df['free sulfur dioxide'], orient='v', color='red')","8d3ca3d0":"plt.figure(figsize=(12,10))\nsns.boxplot(df['quality'], df['total sulfur dioxide'], orient='v', color='violet')","a028a995":"plt.figure(figsize=(12,10))\nsns.boxplot(df['quality'], df['density'], orient='v', color='indigo')","4ccd5108":"plt.figure(figsize=(12,10))\nsns.boxplot(df['quality'], df['pH'], orient='v', color='khaki')","37c0313f":"plt.figure(figsize=(12,10))\nsns.boxplot(df['quality'], df['sulphates'], orient='v', color='lime')","797b1e07":"plt.figure(figsize=(12,10))\nsns.boxplot(df['quality'], df['alcohol'], orient='v', color='tomato')","68e40462":"from scipy import stats\nz = np.abs(stats.zscore(df))\nz","c712f179":"threshold = 3\nprint(np.where(z > 3))","f92c59ad":"df_new= df[(z < 3).all(axis=1)]","4667baf4":"print(df.shape)\nprint(df_new.shape)","c7805012":"df = df_new.copy()\ndf.shape","e3aee3e0":"#For simplicity lets make variable quality into binomial variable\ndf['quality'].value_counts()","740aa15d":"bins=[2,6,8]\nlabels=[0,1]\ndf['quality']=pd.cut(x=df['quality'], bins=bins, labels=labels)","924d97d4":"df['quality'].value_counts()","4cd83056":"#Now, lest split the data into target variable\ny = df['quality']\nx = df.drop(['quality'], axis=1)\nprint(x.shape)\nprint(y.shape)","bca65204":"#Now, split the data into training and testing sets\nfrom sklearn.model_selection import train_test_split\nx_tr, x_tst, y_tr, y_tst = train_test_split(x, y, test_size=0.2, random_state=3)","9d02194c":"#The splitted data is imbalanced hence we need to balance it properly, we can done it using SMOTE as\nfrom imblearn.over_sampling import SMOTE\nsm = SMOTE(random_state=3)\nx_tr_sm, y_tr_sm = sm.fit_resample(x_tr, y_tr)","b8333b06":"import collections\nprint(\"Before SMOTE:\", collections.Counter(y_tr))\nprint(\"After SMOTE:\", collections.Counter(y_tr_sm))","00ab7ee3":"#Now, we standardize the data using standard scaler function\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nx_tr_sm = scaler.fit_transform(x_tr_sm)\nx_tst = scaler.transform(x_tst)","bb78caeb":"#Now, we import Random Forest classifer to predict the output\nfrom sklearn.ensemble import RandomForestClassifier\nrf = RandomForestClassifier(max_depth=18, bootstrap=False)\nrf.fit(x_tr_sm, y_tr_sm)","fb46d252":"y_pred_rf = rf.predict(x_tst)\ny_pred_rf","473bfa55":"results = []","f1eb372d":"#Now lets measure accuracy of the model\nfrom sklearn.metrics import confusion_matrix, classification_report, accuracy_score, plot_roc_curve, roc_curve\ncm = confusion_matrix(y_tst, y_pred_rf)\n\nacc = accuracy_score(y_tst, y_pred_rf)\nscore = rf.score(x_tst, y_tst)\nresults.append(acc)\n\nprint(\"Score : \", score)\nprint(\"RandomForestClassifier Acc : \", acc)\n\nfrom sklearn.metrics import plot_confusion_matrix\nplot_confusion_matrix(rf, x_tst, y_tst, cmap= \"pink\")  \nplt.show()","6f2ead45":"print(\" \\t \\t  RandomForestClassifier Classification Report\")\nprint(classification_report(y_tst, y_pred_rf))","70fd993d":"#Now, we will import another classification algorithm i.e. Logistic Regression\nfrom sklearn.linear_model import LogisticRegression\nlr = LogisticRegression()\nlr.fit(x_tr_sm, y_tr_sm)","11296c5c":"y_pred_lr = lr.predict(x_tst)\ny_pred_lr","6112f04c":"from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\ncm_lr = confusion_matrix(y_tst, y_pred_lr)\n\nacc_lr = accuracy_score(y_tst, y_pred_lr)\nscore_lr = lr.score(x_tst, y_tst)\nresults.append(acc_lr)\n\nprint(\"Score : \", score_lr)\nprint(\"Logistic Regression Classifier Acc : \", acc_lr)\n\nfrom sklearn.metrics import plot_confusion_matrix\nplot_confusion_matrix(lr, x_tst, y_tst, cmap= \"hot\")  \nplt.show()","340f3c8f":"print(\" \\t \\t  Logistic Regression  Classification Report\")\nprint(classification_report(y_tst, y_pred_lr))","d8b40a47":"#ROC AUC Curve for Random Forest Classifier\nfrom sklearn.metrics import plot_roc_curve\nroc_rf = plot_roc_curve(rf, x_tst, y_tst)\nroc_rf","5830bd9f":"#ROC AUC Curev for Logistic Regression Classifer\nroc_lr = plot_roc_curve(lr, x_tst, y_tst)\nroc_lr","217c837d":"**Result**:\nFrom the above, we can see that Random Forest Classifier has highest success among two algorithms, hence concludes.\n","7562ecbd":"From the above correlation matrix, we can see that variable alcohol, sulphates, citric acid and fixed acidity have more correlations than other varibles.","0244aeb9":"We have imported the data successfully. The dataset contains 1599 rows and 12 columns including target variable i.e. quality. Now, we will explore the dataset.","77eecfd6":"There is no null in dataset. Now we will check for the outlier.","3c86a4b2":"We will check if there is any null value is present in the dataset or not.","f2eab880":"**Lets remove the outliers using Z-score**","7e585eeb":"In above histograms, we can see that mostly features are skewed to right and symmetric.\nNow, we will check the correlation of the data","9130be98":"Let's check dataset, i.e. mean. meadian. 25%, 50%, max, min etc."}}