{"cell_type":{"68ecf30c":"code","0b06f47e":"code","cecaf650":"code","67637c48":"code","758529e7":"markdown"},"source":{"68ecf30c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \nfrom sklearn.preprocessing import LabelEncoder\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport pickle\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nimport os, re, csv, math, codecs\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input, Dense, Embedding, SpatialDropout1D, concatenate, Masking\nfrom tensorflow.keras.layers import LSTM, Bidirectional, GlobalMaxPooling1D, Dropout\nimport tensorflow as tf\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nfrom tqdm import tqdm\nPATH = '..\/input\/google-quest-challenge\/'\ndf_train = pd.read_csv(PATH+'train.csv')\ndf_test = pd.read_csv(PATH+'test.csv')\nsample_submission = pd.read_csv('..\/input\/google-quest-challenge\/sample_submission.csv').fillna(' ')\nsample_submission.head()\nclass_names = list(sample_submission.columns[1:])\nclass_question = class_names[:21]\nclass_answer = class_names[21:]\ny1 = df_train[class_question]\ny2 = df_train[class_answer]\ndf_train","0b06f47e":"le = LabelEncoder()\ncategoria = df_train.category\ntrain_categoria = le.fit_transform(categoria)\ntrain_categoria = tf.keras.utils.to_categorical(train_categoria, num_classes=5)\nx = df_train.columns[[1,2,5]]\nx = df_train[x]\ntokenizer = Tokenizer(num_words=1000, oov_token=\"<OOV>\")\ntokenizer.fit_on_texts(x.question_title)\ntokenizer.fit_on_texts(x.question_body)\ntokenizer.fit_on_texts(x.answer)\nword_index = tokenizer.word_index\ntrain_title = tokenizer.texts_to_sequences(x.question_title)\ntrain_body = tokenizer.texts_to_sequences(x.question_body)\ntrain_answer = tokenizer.texts_to_sequences(x.answer)\ntrain_title = pad_sequences(train_title)\ntrain_body = pad_sequences(train_body)\ntrain_answer = pad_sequences(train_answer)\n\nprint('loading word embeddings...')\nembeddings_index = {}\nf = codecs.open('..\/input\/fasttext\/wiki.simple.vec', encoding='utf-8')\nfor line in tqdm(f):\n    values = line.rstrip().rsplit(' ')\n    word = values[0]\n    coefs = np.asarray(values[1:], dtype='float32')\n    embeddings_index[word] = coefs\nf.close()\nprint('found %s word vectors' % len(embeddings_index))\nemb_matrix = np.zeros((1000 + 1, 300))\nfor word, i in tokenizer.word_index.items():\n    if i>= tokenizer.num_words - 1:\n        break\n    embedding_vector = embeddings_index.get(word)\n    if (embedding_vector is not None) and len(embedding_vector) > 0:\n        # words not found in embedding index will be all-zeros.\n        emb_matrix[i] = embedding_vector\n        \n","cecaf650":"def build_model(embedding_matrix):\n    embedding = Embedding(\n        *embedding_matrix.shape, \n        weights=[embedding_matrix], \n        trainable=False, \n        mask_zero=True\n    )\n    \n    q_in = Input(shape=(None,))\n    q = embedding(q_in)\n    q = tf.keras.layers.Conv1D(64 \/\/ 2,5, padding='valid')(q)\n    q = SpatialDropout1D(0.2)(q)\n    q = GlobalMaxPooling1D()(q)\n    q = tf.keras.layers.Flatten()(q)\n    \n    t_in = Input(shape=(None,))\n    t = embedding(t_in)\n    t = tf.keras.layers.Conv1D(64 \/\/ 2,5, padding='valid')(t)\n    t = SpatialDropout1D(0.2)(t)\n    t = GlobalMaxPooling1D()(t)\n    t = tf.keras.layers.Flatten()(t)\n    a_in = Input(shape=(None,))\n    a = embedding(a_in)\n    a = tf.keras.layers.Conv1D(64 \/\/ 2,5, padding='valid')(a)\n    a = SpatialDropout1D(0.2)(a)\n    a = GlobalMaxPooling1D()(a)\n    a = tf.keras.layers.Flatten()(a)\n    c_in = Input(shape=(train_categoria.shape[1]))\n    c = Dense(50, activation='relu')(c_in)\n    hidden = concatenate([q, t, c])    \n    hidden = Dense(300, activation='sigmoid')(hidden)\n    out1 = Dense(21, activation='sigmoid')(hidden)\n    hidden2 = concatenate([q , a, c])\n    hidden2 = Dense(300, activation='sigmoid')(hidden2)\n    out2 = Dense(9, activation='sigmoid')(hidden2)\n    model = Model(inputs=[t_in, q_in, a_in, c_in], outputs=[out1,out2])\n    \n    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n\n    return model\n\nmodel = build_model(emb_matrix)\nmodel.summary()","67637c48":"train_target1 = np.array(y1)\ntrain_target2 = np.array(y2)\nhistory = model.fit(\n    [train_title, train_body, train_answer, train_categoria],\n    [train_target1,train_target2],\n    epochs=10,\n    validation_split=0.2,\n    batch_size=64\n)\nwith open('tokenizer.pickle', 'wb') as handle:\n    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\nmodel.save('modelo.h5')\n","758529e7":"<h1>Simple implementation with Keras using Word Embeddings<\/h1>\n<p>This kernel is based on \"keras-cnn-with-fasttext-embeddings\" and \"TF2 QA: LSTM for long answers predictions\" kernels.\n<p>You can find the predictions of this model in my notebook \"Kernel with Word Embeddings\"(https:\/\/www.kaggle.com\/jessormazaespin\/kernel-with-word-embeddings), I would appreciate your upvote"}}