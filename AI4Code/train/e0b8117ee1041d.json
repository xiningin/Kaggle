{"cell_type":{"e2e38174":"code","d4e4b96f":"code","8f869755":"code","8a060d47":"code","19b01b1c":"code","e04167ea":"code","4339136f":"code","2c894d97":"code","a1992438":"code","55ffe9f0":"code","f12079a1":"code","ae26e6fb":"code","49464962":"code","b07a8968":"code","5bd6d4bd":"code","c8345924":"code","531b8bc8":"code","3039b33d":"code","b49493de":"code","8181a2e8":"code","a8e8cf1b":"markdown","9cc38757":"markdown","7dcbdbd3":"markdown","664e42bc":"markdown","c55fee6c":"markdown","d9a6a185":"markdown","1d1f729b":"markdown"},"source":{"e2e38174":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d4e4b96f":"%reload_ext autoreload\n%autoreload 2\n%matplotlib inline","8f869755":"MODEL_NAME = 'sentiment'\n\nACTIVATION = 'sigmoid'\nBATCH_SIZE = 32\n\nCLASSES = ['negative', 'positive']","8a060d47":"df = pd.read_csv('\/kaggle\/input\/stockmarket-sentiment-dataset\/stock_data.csv')\nprint(df.shape)\nprint(df.columns)\n# df.head()","19b01b1c":"def tanh_to_sigmoid(x):\n    return 0 if x == -1 else 1\n\n\ndf['Y'] = df['Sentiment'].apply(lambda x: tanh_to_sigmoid(x))","e04167ea":"from sklearn.model_selection import train_test_split\n\ndf_train, df_test = train_test_split(df, test_size=0.02, random_state=1)\n\nX_train, X_test = df_train['Text'].values, df_test['Text'].values\nY_train, Y_test = np.asarray(df_train['Y'].tolist()), np.asarray(df_test['Y'].tolist())\n\nprint('X:', X_train.shape, X_test.shape)\nprint(X_train[0])\nprint('Y:', Y_train.shape, Y_test.shape)\nprint(Y_train[0])","4339136f":"! pip install tensorflow_text","2c894d97":"import tensorflow as tf\nimport tensorflow_hub as hub\nimport tensorflow.keras.layers as layers\nfrom tensorflow.keras.models import Model\nimport tensorflow_text\n\n# Enable to modify cache location with PATH variable: TFHUB_CACHE_DIR=\/Users\/Cache\nuse_layer = hub.KerasLayer(\"https:\/\/tfhub.dev\/google\/universal-sentence-encoder-multilingual-large\/3\", input_shape=[], dtype=tf.string, trainable=False)\n\n\nclass PreprocessText(layers.Layer):\n    def __init__(self, name=\"preprocess_text\", **kwargs):\n        super(PreprocessText, self).__init__(name=name, **kwargs)\n\n    def call(self, input):\n        data_preprocessor.process_text(input)\n        return self.preprocess(input)\n\n\n\ndef create_model(out_shape, activation='softmax'):\n    # Functional\n    inputs = layers.Input(shape=(1,), dtype=tf.string)\n    \n    X = use_layer(tf.squeeze(tf.cast(inputs, tf.string)))\n    X = layers.Dense(512, activation='relu')(X)\n    X = layers.Dropout(0.3)(X)\n    X = layers.Dense(512, activation='relu')(X)\n    X = layers.Dropout(0.3)(X)\n\n    outputs = layers.Dense(out_shape, activation=activation)(X)\n\n    model = Model(inputs=inputs, outputs=outputs) \n    return model","a1992438":"model = create_model(Y_train.shape[1] if ACTIVATION == 'softmax' else 1, activation=ACTIVATION)\n    \nmodel.summary()","55ffe9f0":"from tensorflow.keras.optimizers import Adam\n\nmodel.compile(loss = 'binary_crossentropy', \n              optimizer = Adam(lr=3e-4, decay=1e-6, beta_1=0.9, beta_2=0.999), \n              metrics = ['accuracy'])","f12079a1":"BEST_WEIGHTS = (f'{MODEL_NAME}_use_best_weights.hdf5')\nMODEL_FILE = (f'{MODEL_NAME}_use_model.h5')","ae26e6fb":"from datetime import datetime\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard\n\nmonitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=10, verbose=1, mode='auto')\ncheckpoint = ModelCheckpoint(filepath=BEST_WEIGHTS, verbose=1, save_best_only=True)   # Save the best model\ntensorboard = TensorBoard(log_dir='logs\/{} - {}'.format(MODEL_NAME, datetime.now().strftime('%Y%m%d%H%M%S')))","49464962":"hist = model.fit(\n    X_train,\n    Y_train,\n    batch_size = BATCH_SIZE, \n    epochs = 30, \n    validation_data=(X_test, Y_test),\n    validation_split=0.02,\n    callbacks = [monitor, checkpoint, tensorboard],\n    shuffle=True,\n    verbose=1\n)","b07a8968":"model.load_weights(BEST_WEIGHTS)\nmodel.save(MODEL_FILE)","5bd6d4bd":"import matplotlib.pyplot as plt\n\ndef plot_train_history(history):\n    # plot the cost and accuracy \n    loss_list = history['loss']\n    val_loss_list = history['val_loss']\n    accuracy_list = history['accuracy']\n    val_accuracy_list = history['val_accuracy']\n    # epochs = range(len(loss_list))\n\n    # plot the cost\n    plt.plot(loss_list, 'b', label='Training cost')\n    plt.plot(val_loss_list, 'r', label='Validation cost')\n    plt.ylabel('cost')\n    plt.xlabel('iterations')\n    plt.title('Training and validation cost')\n    plt.legend()\n    \n    plt.figure()\n    \n    # plot the accuracy\n    plt.plot(accuracy_list, 'b', label='Training accuracy')\n    plt.plot(val_accuracy_list, 'r', label='Validation accuracy')\n    plt.ylabel('accuracy')\n    plt.xlabel('iterations')\n    plt.title('Training and validation accuracy')\n    plt.legend()\n\n\nplot_train_history(hist.history)","c8345924":"score = model.evaluate(X_test, Y_test)\n\nprint (\"Test Loss = \" + str(score[0]))\nprint (\"Test Accuracy = \" + str(score[1]))","531b8bc8":"Y_test_pred = model.predict(X_test, verbose=1)","3039b33d":"# Get threshold is binary classifation\n\nif ACTIVATION == 'sigmoid':\n    from sklearn.metrics import roc_curve, auc\n\n    def calculate_optimal_threshold(Y, Y_pred):\n        # ROC Curve\n        fpr, tpr, thresholds = roc_curve(Y, Y_pred)\n        roc_auc = auc(fpr, tpr)\n        plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC Curve')\n        plt.plot([0, 1], [0, 1], color='navy', lw=1, linestyle='--')\n        plt.xlim([-0.025, 1.025])\n        plt.ylim([-0.025, 1.025])\n        plt.xlabel('False Positive Rate')\n        plt.ylabel('True Positive Rate')\n        plt.title('RoC Curve')\n        print(\"AUC: \", roc_auc)\n\n        # Calculate the optimal threshold\n        i = np.arange(len(tpr)) # index for df\n        roc_df = pd.DataFrame({'threshold' : pd.Series(thresholds, index = i), \n                               'fpr': pd.Series(fpr, index=i), \n                               '1-fpr' : pd.Series(1-fpr, index = i), \n                               'tpr': pd.Series(tpr, index = i), \n                               'diff': pd.Series(tpr - (1-fpr), index = i) })\n        opt_threshold = roc_df.iloc[roc_df['diff'].abs().argsort()[:1]]\n        print(opt_threshold)\n\n        return opt_threshold['threshold'].values[0]\n\n\n    threshold = calculate_optimal_threshold(Y_test, Y_test_pred)","b49493de":"from sklearn.metrics import accuracy_score, mean_squared_error, confusion_matrix, precision_score, recall_score, classification_report\n\ndef analyze(Y, Y_pred, classes, activation=\"softmax\"):\n    if activation == \"sigmoid\":\n        Y_cls = Y\n        Y_pred_cls = (Y_pred > threshold).astype(float)\n    elif activation == \"softmax\":\n        Y_cls = np.argmax(Y, axis=1)\n        Y_pred_cls = np.argmax(Y_pred, axis=1)\n    \n    \n    accuracy = accuracy_score(Y_cls, Y_pred_cls)\n    print(\"Accuracy score: {}\\n\".format(accuracy))\n    \n    \n    rmse = np.sqrt(mean_squared_error(Y, Y_pred))\n    print(\"RMSE score: {}\\n\".format(rmse))\n\n    \n    # plot Confusion Matrix\n    print(\"Confusion Matrix:\")\n    cm = confusion_matrix(Y_cls, Y_pred_cls)\n    print(cm)\n    # Plot the confusion matrix as an image.\n    plt.matshow(cm)\n    # Make various adjustments to the plot.\n    num_classes = len(classes)\n    plt.colorbar()\n    tick_marks = np.arange(num_classes)\n    plt.xticks(tick_marks, range(num_classes))\n    plt.yticks(tick_marks, range(num_classes))\n    plt.xlabel('Predicted')\n    plt.ylabel('True')\n    \n    \n    # plot Classification Report\n    print(\"Classification Report:\")\n    print(classification_report(Y_cls, Y_pred_cls, target_names=classes))\n\n\n\nanalyze(Y_test, Y_test_pred, CLASSES, ACTIVATION)","8181a2e8":"def show_mislabeled(X, Y, Y_pred, classes, activation=\"softmax\", num_show = None):\n    num_col = 5\n    \n    if activation == \"sigmoid\":\n        Y_cls = Y\n        Y_pred_cls = np.squeeze((Y_pred > threshold).astype(float))\n    elif activation == \"softmax\":\n        Y_cls = np.argmax(Y, axis=1)\n        Y_pred_cls = np.argmax(Y_pred, axis=1)\n    \n    mislabeled_indices = np.where(Y_cls != Y_pred_cls)[0]\n    print(f'{len(mislabeled_indices)} mislabeled\\n')\n    \n    if num_show is None or num_show > len(mislabeled_indices):\n        num_show = len(mislabeled_indices)\n        \n    for i, index in enumerate(mislabeled_indices[:num_show]):   \n        print(\"{}\\nPrediction: {}\\nLabel: {}\\n\\n\".format(X[index], \n                                                         classes[int(Y_pred_cls[index])], \n                                                         classes[int(Y_cls[index])]))\n\n\n\nshow_mislabeled(X_test, Y_test, Y_test_pred, CLASSES, ACTIVATION, 10)","a8e8cf1b":"\n# Preprocess Data","9cc38757":"# Show Mislabeled","7dcbdbd3":"# Train","664e42bc":"# Model","c55fee6c":"# Test","d9a6a185":"# Analyze","1d1f729b":"# Load Data"}}