{"cell_type":{"03913b15":"code","bcfd8c31":"code","fb7e6663":"code","eec28f69":"code","5bee972f":"code","a8fc2715":"code","8e101ef0":"code","93eb4a61":"code","8d5802d6":"code","fcba9cf5":"code","0e31658e":"code","361b7162":"code","9a94bb36":"code","6d859868":"code","48200b87":"code","d10caf08":"code","8442df78":"code","15933704":"code","9a519033":"code","5c5dfc32":"code","ef5b4e66":"code","9766bfa9":"code","cff89999":"code","e0c41a2f":"code","554c3b9a":"code","091b192f":"code","1962c821":"code","8c90a9dd":"code","247e0dc1":"code","eeebdcfc":"code","29c5551a":"code","4b4313c2":"code","7deff7c1":"code","f11f1c6d":"code","36d2b5df":"code","b858c82e":"code","84638d04":"code","84646551":"code","182e5653":"code","2dea6530":"code","b8846983":"code","a8549be7":"code","88312ea1":"code","81ce3e52":"code","e499fe6b":"code","501f6d66":"code","e5000c28":"code","ac2cb8de":"code","5dd65556":"code","2d4cff91":"code","28774515":"code","baa4dbb3":"code","f587095c":"code","e2534b16":"code","e7af3425":"code","5dd3974b":"code","6e591c6d":"code","7fa2ff4f":"code","736e76a3":"code","31596ad0":"code","3cbc8a10":"code","1948609a":"code","45a75335":"code","8baf1b4a":"code","8efcae65":"code","f35471df":"code","bcd123c8":"code","b4e561f3":"code","1d5dee93":"code","1c8ba264":"code","c1b739a9":"code","a418c2ed":"code","d5e18dc7":"code","fc8d63db":"code","e4e4c797":"code","4269b2c0":"code","16e9d3d5":"code","548174f5":"code","a92bd78c":"code","111706b0":"code","5bdfc8df":"code","4c3d873b":"code","37470983":"code","876abbc4":"code","250901b3":"code","ef7c6454":"code","071bbe5c":"code","1f43b10f":"code","f961abac":"code","cbb7fdc7":"code","05bbfe4b":"code","b1d65e03":"code","51da56d7":"code","cfb30460":"code","a26463e9":"code","476c5a08":"code","da30b8d8":"code","d722bc01":"code","bba1e770":"code","15a4c583":"code","5f3928ea":"code","c30ccfa7":"code","03f5c602":"code","942eea0c":"code","67179f07":"code","ed1f8326":"code","54699dfe":"code","ce2081d2":"code","86a4f64f":"code","c7ab7118":"code","dc5c005f":"code","b2d1f911":"code","9580f799":"code","e7ec339f":"code","f08d9877":"code","bc68b08f":"code","6ed6090f":"code","83611ad6":"code","656ba460":"code","0eddda6e":"code","3502e503":"code","2d4212a4":"code","2e5f132b":"code","b7286569":"code","9c14566d":"code","9038c238":"code","199fa15c":"code","f0854b15":"code","7a9c86df":"code","fcbd7f5a":"markdown","c58c13ea":"markdown","6f0ab19f":"markdown","478fdb9a":"markdown","8f5bb609":"markdown","da142d73":"markdown","a714351e":"markdown","7b88404a":"markdown","3e352097":"markdown","7ecd06f4":"markdown","58d2bff3":"markdown","4b48e73a":"markdown","e296a5e5":"markdown","f05ec87e":"markdown","bcad573c":"markdown","b0422f0a":"markdown","11e0b42a":"markdown","dc60b452":"markdown","1fb10db7":"markdown","a5ce81d4":"markdown","d0e91af4":"markdown","39fb0ed4":"markdown","10a1191f":"markdown","3ba95f10":"markdown","0d0a0da8":"markdown","8ae96446":"markdown","78f6a11f":"markdown","3a6ac37d":"markdown","66801f79":"markdown","58c40167":"markdown","91b6cb80":"markdown","776bd17b":"markdown","05549c26":"markdown","27e679e7":"markdown","f4b5104d":"markdown","fca49188":"markdown","d2149467":"markdown","67af4e93":"markdown","210ba72d":"markdown","15e56eb5":"markdown","462c69af":"markdown","d58a8b9b":"markdown","d0be5775":"markdown","cd044758":"markdown","c4aea8f2":"markdown","a7221a91":"markdown","99985595":"markdown","06a455c7":"markdown","3931f6c2":"markdown","1ccd7406":"markdown","a26e31bb":"markdown","5984b29d":"markdown","9f339f8e":"markdown","4033be57":"markdown","5ef1624f":"markdown","a5c34cdc":"markdown","9d0d97e3":"markdown","6d55e2c2":"markdown","156ed0dc":"markdown","4374b1b8":"markdown","a3733568":"markdown","0ef78b02":"markdown","9871b8f3":"markdown"},"source":{"03913b15":"# Data wrangling\nimport pandas as pd\nimport numpy as np\nfrom collections import Counter\n# Data visualisation\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n# Machine learning models\nfrom sklearn.linear_model import LogisticRegression, Perceptron, SGDClassifier\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom catboost import CatBoostClassifier\n# Model evaluation\nfrom sklearn.model_selection import cross_val_score\n# Hyperparameter tuning\nfrom sklearn.model_selection import GridSearchCV\n# Remove warnings\nimport warnings\nwarnings.filterwarnings('ignore')","bcfd8c31":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","fb7e6663":"# import data\ntrain = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntest=pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\nsub = pd.read_csv('\/kaggle\/input\/titanic\/gender_submission.csv')","eec28f69":"train.head()","5bee972f":"print(\"Training set shape: \", train.shape)\nprint(\"Test set shape: \", test.shape)\nprint(\"Sample submission shape: \", sub.shape)","a8fc2715":"# data types\ntrain.info()","8e101ef0":"test.info()","93eb4a61":"# Missing data in training set \ntrain.isnull().sum().sort_values(ascending = False)","8d5802d6":"# Missing data in testing set \ntest.isnull().sum().sort_values(ascending = False)","fcba9cf5":"train.describe()","0e31658e":"train.describe(include=\"O\")","361b7162":"train.loc[train.Cabin.notnull(),'Cabin']=1\ntrain.loc[train.Cabin.isnull(),'Cabin']=0","9a94bb36":"test.loc[test.Cabin.notnull(),'Cabin']=1\ntest.loc[test.Cabin.isnull(),'Cabin']=0","6d859868":"train.Cabin.isnull().sum()","48200b87":"test.Cabin.isnull().sum()","d10caf08":"def detect_outliers(df, n, features):\n\n    outlier_indices = [] \n    for col in features: \n        Q1 = np.percentile(df[col], 25)\n        Q3 = np.percentile(df[col], 75)\n        IQR = Q3 - Q1\n        outlier_step = 1.5 * IQR \n        outlier_list_col = df[(df[col] < Q1 - outlier_step) | (df[col] > Q3 + outlier_step)].index\n        outlier_indices.extend(outlier_list_col) \n    outlier_indices = Counter(outlier_indices)\n    multiple_outliers = list(key for key, value in outlier_indices.items() if value > n) \n    return multiple_outliers","8442df78":"outliers_to_drop = detect_outliers(train, 2, ['Age', 'SibSp', 'Parch', 'Fare'])\nprint(\"The {} indices for the outliers to drop are: \".format(len(outliers_to_drop)), outliers_to_drop)","15933704":"# Outliers in numerical variables\ntrain.loc[outliers_to_drop, :]","9a519033":"# Drop outliers and reset index\nprint(\"Before: {} rows\".format(len(train)))\ntrain = train.drop(outliers_to_drop, axis = 0).reset_index(drop = True)\nprint(\"After: {} rows\".format(len(train)))","5c5dfc32":"test.head()","ef5b4e66":"outliers_to_drop_to_test = detect_outliers(test, 2, ['Age', 'SibSp', 'Parch', 'Fare'])","9766bfa9":"# Outliers in numerical variables\ntest.loc[outliers_to_drop_to_test, :]","cff89999":"sns.heatmap(train[['Survived', 'SibSp', 'Parch', 'Age', 'Fare', 'Pclass']].corr(), annot = True, fmt = '.2f', cmap = 'Reds')","e0c41a2f":"# Value counts of the SibSp column \ntrain['SibSp'].value_counts(dropna = False)","554c3b9a":"# Mean of survival by SibSp\ntrain[['SibSp', 'Survived']].groupby('SibSp', as_index = False).mean().sort_values(by = 'Survived', ascending = False)","091b192f":"sns.barplot(x = 'SibSp', y ='Survived', data = train)\nplt.ylabel('Survival Probability')\nplt.title('Survival Probability by SibSp')","1962c821":"# Value counts of the Parch column \ntrain['Parch'].value_counts(dropna = False)","8c90a9dd":"# Mean of survival by Parch\ntrain[['Parch', 'Survived']].groupby('Parch', as_index = False).mean().sort_values(by = 'Survived', ascending = False)","247e0dc1":"sns.barplot(x = 'Parch', y ='Survived', data = train)\nplt.ylabel('Survival Probability')\nplt.title('Survival Probability by Parch')","eeebdcfc":"# Null values in Age column \ntrain['Age'].isnull().sum()","29c5551a":"# Passenger age distribution\nsns.distplot(train['Age'], label = 'Skewness: %.2f'%(train['Age'].skew()))\nplt.legend(loc = 'best')\nplt.title('Passenger Age Distribution')","4b4313c2":"# Age distribution by survival\ng = sns.FacetGrid(train, col = 'Survived')\ng.map(sns.distplot, 'Age')","7deff7c1":"sns.kdeplot(train['Age'][train['Survived'] == 0], label = 'Died')\nsns.kdeplot(train['Age'][train['Survived'] == 1], label = 'Survived')\nplt.xlabel('Age')\nplt.title('Passenger Age Distribution by Survival')","f11f1c6d":"# Null values of Fare column \ntrain['Fare'].isnull().sum()","36d2b5df":"# Passenger fare distribution\nsns.distplot(train['Fare'], label = 'Skewness: %.2f'%(train['Fare'].skew()))\nplt.legend(loc = 'best')\nplt.ylabel('Passenger Pclass Distribution')","b858c82e":"# Value counts of the SibSp column \n\ntrain['Pclass'].value_counts(dropna = False)","84638d04":"# Mean of survival by class\ntrain[['Pclass', 'Survived']].groupby('Pclass', as_index = False).mean().sort_values(by = 'Survived', ascending = False)","84646551":"sns.barplot(x = 'Pclass', y ='Survived', data = train)\nplt.ylabel('Survival Probability')\nplt.title('Survival Probability by Pclass')","182e5653":"# Survival by gender and passenger class\n\ng = sns.factorplot(x = 'Pclass', y = 'Survived', hue = 'Sex', data = train, kind = 'bar')\ng.despine(left = True)\nplt.ylabel('Survival Probability')\nplt.title('Survival Probability by Sex and Passenger Class')","2dea6530":"# Passenger fare distribution\nsns.distplot(train['Pclass'], label = 'Skewness: %.2f'%(train['Pclass'].skew()))\nplt.legend(loc = 'best')\nplt.ylabel('Passenger Fare Distribution')","b8846983":"# Age distribution by survival\ng = sns.FacetGrid(train, col = 'Survived')\ng.map(sns.distplot, 'Pclass')","a8549be7":"# Value counts of the sex column\ntrain['Sex'].value_counts(dropna = False)","88312ea1":"# Mean of survival by sex\ntrain[['Sex', 'Survived']].groupby('Sex', as_index = False).mean().sort_values(by = 'Survived', ascending = False)","81ce3e52":"sns.barplot(x = 'Sex', y ='Survived', data = train)\nplt.ylabel('Survival Probability')\nplt.title('Survival Probability by Gender')","e499fe6b":"# Value counts of the Embarked column \ntrain['Embarked'].value_counts(dropna = False)","501f6d66":"# Mean of survival by point of embarkation\ntrain[['Embarked', 'Survived']].groupby(['Embarked'], as_index = False).mean().sort_values(by = 'Survived', ascending = False)","e5000c28":"sns.barplot(x = 'Embarked', y ='Survived', data = train)\nplt.ylabel('Survival Probability')\nplt.title('Survival Probability by Point of Embarkation')","ac2cb8de":"# Survival probability by all categorical variables\ngrid = sns.FacetGrid(train, row = 'Embarked', size = 2.2, aspect = 1.6)\ngrid.map(sns.pointplot, 'Pclass', 'Survived', 'Sex', palette = 'deep')\ngrid.add_legend()","5dd65556":"pd.pivot_table(train,index=['Cabin'],values=['Survived']).plot.bar(figsize=(8,5))\nplt.title('Survival Rate')","2d4cff91":"cabin=pd.crosstab(train.Cabin,train.Survived)\ncabin.rename(index={0:'no cabin',1:'cabin'},columns={0.0:'Died',1.0:'Survived'},inplace=True)\ncabin","28774515":"cabin.plot.bar(figsize=(8,5))\nplt.xticks(rotation=0,size='xx-large')\nplt.title('Survived Count')\nplt.xlabel('')\nplt.legend()","baa4dbb3":"# Drop ticket  feature from training and test set\ntrain = train.drop(['Ticket'], axis = 1)\ntest = test.drop(['Ticket'], axis = 1)","f587095c":"# Missing values in training set \ntrain.isnull().sum().sort_values(ascending = False)","e2534b16":"# Compute the most frequent value of Embarked in training set\nmode = train['Embarked'].dropna().mode()[0]\nmode","e7af3425":"# Fill missing value in Embarked with mode\ntrain['Embarked'].fillna(mode, inplace = True)","5dd3974b":"# Missing values in test set\ntest.isnull().sum().sort_values(ascending = False)","6e591c6d":"# Compute median of Fare in test set \nmedian = test['Fare'].dropna().median()\nmedian","7fa2ff4f":"# Fill missing value in Fare with median\ntest['Fare'].fillna(median, inplace = True)","736e76a3":"# Combine training set and test set\ncombine = pd.concat([train, test], axis = 0).reset_index(drop = True)\ncombine.head()","31596ad0":"# Missing values in the combined dataset\ncombine.isnull().sum().sort_values(ascending = False)","3cbc8a10":"# Convert Sex into numerical values where 0 = male and 1 = female\ncombine['Sex'] = combine['Sex'].map({'male': 0, 'female': 1})","1948609a":"sns.factorplot(y = 'Age', x = 'Sex', hue = 'Pclass', kind = 'box', data = combine)\nsns.factorplot(y = 'Age', x = 'Parch', kind = 'box', data = combine)\nsns.factorplot(y = 'Age', x = 'SibSp', kind = 'box', data = combine)","45a75335":"sns.heatmap(train.drop(['Survived', 'Name', 'Fare'], axis = 1).corr(), annot = True, cmap = 'Reds')","8baf1b4a":"# Check number of missing ages \nage_nan_indices = list(combine[combine['Age'].isnull()].index)\nlen(age_nan_indices)","8efcae65":"# Loop through list and impute missing ages\nfor index in age_nan_indices:\n    median_age = combine['Age'].median()\n    predict_age = combine['Age'][(combine['SibSp'] == combine.iloc[index]['SibSp']) \n                                 & (combine['Parch'] == combine.iloc[index]['Parch'])\n                                 & (combine['Pclass'] == combine.iloc[index][\"Pclass\"])].median()\n    if np.isnan(predict_age):\n        combine['Age'].iloc[index] = median_age\n    else:\n        combine['Age'].iloc[index] = predict_age","f35471df":"# Make sure there is no more missing ages \ncombine['Age'].isnull().sum()","bcd123c8":"# Passenger fare distribution\nsns.distplot(combine['Fare'], label = 'Skewness: %.2f'%(combine['Fare'].skew()))\nplt.legend(loc = 'best')\nplt.title('Passenger Fare Distribution')","b4e561f3":"# Apply log transformation to Fare column to reduce skewness\ntrain['Fare'] = train['Fare'].map(lambda x: np.log(x) if x > 0 else 0)","1d5dee93":"# Passenger fare distribution after log transformation\nsns.distplot(train['Fare'], label = 'Skewness: %.2f'%(train['Fare'].skew()))\nplt.legend(loc = 'best')\nplt.title('Passenger Fare Distribution After Log Transformation')","1c8ba264":"combine.head()","c1b739a9":"# Get title from name\ncombine['Title'] = [name.split(',')[1].split('.')[0].strip() for name in combine['Name']]\ncombine[['Name', 'Title']].head()","a418c2ed":"# Value counts of Title\ncombine['Title'].value_counts()","d5e18dc7":"# Number of unique Title\ncombine['Title'].nunique()","fc8d63db":"# Simplify title\ncombine['Title'] = combine['Title'].replace(['Dr', 'Rev', 'Col', 'Major', 'Capt'], 'Officer')\ncombine['Title'] = combine['Title'].replace(['Lady', 'Jonkheer', 'Don','the Countess','Sir', 'Dona'], 'Royalty')\ncombine['Title'] = combine['Title'].replace(['Mlle', 'Miss'], 'Miss')\ncombine['Title'] = combine['Title'].replace(['Mme','Mrs','Ms'], 'Mrs')\ncombine['Title'] = combine['Title'].replace('Mr', 'Mr')\ncombine['Title'] = combine['Title'].replace('Master', 'Master')","e4e4c797":"figure = plt.figure(figsize=(25, 7))\nsns.countplot(combine['Title'])","4269b2c0":"# Mean of survival by name title\ncombine[['Title', 'Survived']].groupby(['Title'], as_index = False).mean().sort_values(by = 'Survived', ascending = False)","16e9d3d5":"sns.factorplot(x = 'Title', y = 'Survived', data = combine, kind = 'bar')\nplt.ylabel('Survival Probability')\nplt.title('Mean of survival by Title')","548174f5":"# Drop name column\ncombine = combine.drop('Name', axis = 1)\ncombine.head()","a92bd78c":"# Calculate family size from SibSp and Parch\ncombine['Family_Size'] = combine['SibSp'] + combine['Parch'] + 1\ncombine[['SibSp', 'Parch', 'Family_Size']].head(10)","111706b0":"# Mean of survival by family_size\ncombine[['Family_Size', 'Survived']].groupby('Family_Size', as_index = False).mean().sort_values(by = 'Survived', ascending = False)","5bdfc8df":"# Create Alone feature\ncombine['Alone'] = 0\ncombine.loc[combine['Family_Size'] == 1, 'Alone'] = 1","4c3d873b":"# Mean of survival by Alone\ncombine[['Alone', 'Survived']].groupby('Alone', as_index = False).mean().sort_values(by = 'Survived', ascending = False)","37470983":"# Drop SibSp, Parch and FamilySize features from combine dataframe\ncombine = combine.drop(['SibSp', 'Parch', 'Family_Size'], axis = 1)\ncombine.head()","876abbc4":"combine['Minor'] = combine['Age'] <= 17\ncombine['Major'] = 1 - combine['Minor']  ","250901b3":"combine[['Major', 'Survived']].groupby('Major', as_index = False).mean().sort_values(by = 'Survived', ascending = False)","ef7c6454":"combine.loc[(combine['Age'] <= 17), 'Major'] = 0\ncombine.loc[(combine['Age'] > 17), 'Major'] = 1","071bbe5c":"combine.head()","1f43b10f":"# Drop Age and Minor from combine dataframe\ncombine = combine.drop(['Age', 'Minor'], axis = 1)\ncombine.head()","f961abac":"combine.head()","cbb7fdc7":"# Encode Title and Embarked feature\ncombine = pd.get_dummies(combine, columns = ['Title'])\ncombine = pd.get_dummies(combine, columns = ['Embarked'], prefix = 'Em')\ncombine.head()","05bbfe4b":"# Divide Fare into four bands\ncombine['Fare_Band'] = pd.cut(combine['Fare'], 4)\ncombine[['Fare_Band', 'Survived']].groupby(['Fare_Band'], as_index=False).mean().sort_values(by = 'Fare_Band')","b1d65e03":"# Assign ordinal to each fare band\ncombine.loc[combine['Fare'] <= 1.56, 'Fare'] = 0\ncombine.loc[(combine['Fare'] > 1.56) & (combine['Fare'] <= 3.119), 'Fare'] = 1\ncombine.loc[(combine['Fare'] > 3.119) & (combine['Fare'] <= 4.679), 'Fare'] = 2\ncombine.loc[combine['Fare'] > 4.679, 'Fare'] = 3","51da56d7":"# Convert Fare into integer\ncombine['Fare'] = combine['Fare'].astype('int')","cfb30460":"# Drop FareBand feature\ncombine = combine.drop('Fare_Band', axis = 1)","a26463e9":"combine.head()","476c5a08":"# Separate training and test set from the combined dataframe\ntrain = combine[:len(train)]\ntest = combine[len(train):]","da30b8d8":"# Drop passenger ID column from and training set\n\ntrain = train.drop('PassengerId', axis = 1)\ntrain.head()","d722bc01":"# Convert survived back to integer in the training set\ntrain['Survived'] = train['Survived'].astype('int')\ntrain.head()","bba1e770":"# Drop passenger survived column from test set\ntest = test.drop('Survived', axis = 1)\ntest.head()","15a4c583":"train['Cabin'] = train['Cabin'].astype('int')","5f3928ea":"test['Cabin'] = test['Cabin'].astype('int')","c30ccfa7":"train.info()","03f5c602":"test.info()","942eea0c":"X_train = train.drop('Survived', axis = 1)\nY_train = train['Survived']\nX_test = test.drop('PassengerId', axis = 1).copy()\nprint(\"X_train shape: \", X_train.shape)\nprint(\"Y_train shape: \", Y_train.shape)\nprint(\"X_test shape: \", X_test.shape)","67179f07":"logreg = LogisticRegression()\nlogreg.fit(X_train, Y_train)\nY_pred = logreg.predict(X_test)\nacc_log = round(logreg.score(X_train, Y_train) * 100, 2)\nacc_log","ed1f8326":"svc = SVC()\nsvc.fit(X_train, Y_train)\nY_pred = svc.predict(X_test)\nacc_svc = round(svc.score(X_train, Y_train) * 100, 2)\nacc_svc","54699dfe":"knn = KNeighborsClassifier(n_neighbors = 5)\nknn.fit(X_train, Y_train)\nY_pred = knn.predict(X_test)\nacc_knn = round(knn.score(X_train, Y_train) * 100, 2)\nacc_knn","ce2081d2":"gaussian = GaussianNB()\ngaussian.fit(X_train, Y_train)\nY_pred = gaussian.predict(X_test)\nacc_gaussian = round(gaussian.score(X_train, Y_train) * 100, 2)\nacc_gaussian","86a4f64f":"perceptron = Perceptron()\nperceptron.fit(X_train, Y_train)\nY_pred = perceptron.predict(X_test)\nacc_perceptron = round(perceptron.score(X_train, Y_train) * 100, 2)\nacc_perceptron","c7ab7118":"linear_svc = LinearSVC()\nlinear_svc.fit(X_train, Y_train)\nY_pred = linear_svc.predict(X_test)\nacc_linear_svc = round(linear_svc.score(X_train, Y_train) * 100, 2)\nacc_linear_svc","dc5c005f":"sgd = SGDClassifier()\nsgd.fit(X_train, Y_train)\nY_pred = sgd.predict(X_test)\nacc_sgd = round(sgd.score(X_train, Y_train) * 100, 2)\nacc_sgd","b2d1f911":"decision_tree = DecisionTreeClassifier()\ndecision_tree.fit(X_train, Y_train)\nY_pred = decision_tree.predict(X_test)\nacc_decision_tree = round(decision_tree.score(X_train, Y_train) * 100, 2)\nacc_decision_tree","9580f799":"random_forest = RandomForestClassifier(n_estimators = 100)\nrandom_forest.fit(X_train, Y_train)\nY_pred = random_forest.predict(X_test)\nacc_random_forest = round(random_forest.score(X_train, Y_train) * 100, 2)\nacc_random_forest","e7ec339f":"catboost = CatBoostClassifier(logging_level='Silent')\ncatboost.fit(X_train, Y_train)\nY_pred = catboost.predict(X_test)\nacc_catboost = round(catboost.score(X_train, Y_train) * 100, 2)","f08d9877":"models = pd.DataFrame({'Model': ['Support Vector Machines', 'KNN', 'Logistic Regression', \n                                 'Random Forest', 'Naive Bayes', 'Perceptron', 'Stochastic Gradient Decent', \n                                 'Linear SVC', 'Decision Tree', 'CatBoost'],\n                       'Score': [acc_svc, acc_knn, acc_log, acc_random_forest, acc_gaussian, acc_perceptron,\n                                 acc_sgd, acc_linear_svc, acc_decision_tree, acc_catboost]})\n\nmodels.sort_values(by = 'Score', ascending = False, ignore_index = True)","bc68b08f":"# Create a list which contains classifiers \n\nclassifiers = []\nclassifiers.append(LogisticRegression())\nclassifiers.append(SVC())\nclassifiers.append(KNeighborsClassifier(n_neighbors = 5))\nclassifiers.append(GaussianNB())\nclassifiers.append(Perceptron())\nclassifiers.append(LinearSVC())\nclassifiers.append(SGDClassifier())\nclassifiers.append(DecisionTreeClassifier())\nclassifiers.append(RandomForestClassifier())\nclassifiers.append(CatBoostClassifier())\n\nlen(classifiers)","6ed6090f":"# Create a list which contains cross validation results for each classifier\ncv_results = []\nfor classifier in classifiers:\n    cv_results.append(cross_val_score(classifier, X_train, Y_train, scoring = 'accuracy', cv = 10))","83611ad6":"# Mean and standard deviation of cross validation results for each classifier  \ncv_mean = []\ncv_std = []\nfor cv_result in cv_results:\n    cv_mean.append(cv_result.mean())\n    cv_std.append(cv_result.std())","656ba460":"cv_res = pd.DataFrame({'Cross Validation Mean': cv_mean, 'Cross Validation Std': cv_std, 'Algorithm': ['Logistic Regression', 'Support Vector Machines', 'KNN', 'Gausian Naive Bayes', 'Perceptron', 'Linear SVC', 'Stochastic Gradient Descent', 'Decision Tree', 'Random Forest', 'CatBoost']})\ncv_res.sort_values(by = 'Cross Validation Mean', ascending = False, ignore_index = True)","0eddda6e":"sns.barplot('Cross Validation Mean', 'Algorithm', data = cv_res, order = cv_res.sort_values(by = 'Cross Validation Mean', ascending = False)['Algorithm'], palette = 'Set3', **{'xerr': cv_std})\nplt.ylabel('Algorithm')\nplt.title('Cross Validation Scores')","3502e503":"# Create the parameter grid based on the results of random search \nparam_grid = {'learning_rate': [0.03, 0.1],\n        'depth': [4, 6, 10],\n        'l2_leaf_reg': [1, 3, 5, 7, 9]}\n\n\n# Instantiate the grid search model\ngrid = GridSearchCV(CatBoostClassifier(), param_grid = param_grid, \n                          cv = 3, refit=True, verbose = True)\ngrid.fit(X_train, Y_train)","2d4212a4":"print(\"Best parameters: \", grid.best_params_) \nprint(\"Best estimator: \", grid.best_estimator_)","2e5f132b":"catboost = CatBoostClassifier(depth=10, l2_leaf_reg=5, learning_rate=0.1)\ncatboost.fit(X_train, Y_train)\nY_pred = catboost.predict(X_test)\nacc_catboost = round(catboost.score(X_train, Y_train) * 100, 2)","b7286569":"# Mean cross validation score\ncross_val_score(catboost, X_train, Y_train, scoring = 'accuracy', cv = 10).mean()","9c14566d":"# Survival predictions by CatBoost classifier\nY_pred","9038c238":"len(Y_pred)","199fa15c":"# Create submission dataframe\noutput = pd.DataFrame({'PassengerId': test['PassengerId'], 'Survived': Y_pred})","f0854b15":"# Create and save csv file \noutput.to_csv(\"submission_titanic.csv\", index = False)","7a9c86df":"print(\"Your submission was successfully saved!\")","fcbd7f5a":"### Numerical variables correlation with survival","c58c13ea":"that is 177 values are missing in the Age column.\nOne solution is to fill in the null values with the median age. We could also impute with the mean age but the median is more robust to outliers.","6f0ab19f":"### 5.2.7 Stochastic gradient descent ","478fdb9a":"In this section, I will construct 3 new features:\n\nTitle\nAlone\nMajor","8f5bb609":"### 6.3.1 Training accuracy","da142d73":"### 5.2.10 CatBoost","a714351e":"## 3.2 Feature analysis","7b88404a":"# Conclusion\n","3e352097":"Previously, we have encoded the sex column such that 0 = male and 1 = female. We need to repeat this process for Title and Embarked.","7ecd06f4":"## 4. Feature encoding","58d2bff3":"There are a lot of missing values in 'Cabin', maybe there is difference between the survival rate of people who has Cabin number and those who hasn't.","4b48e73a":"Pandas allows you to a have a high-level simple statistical description of the numerical features. This can be done using the describe method.","e296a5e5":"Recall that our passenger fare column has a very high positive skewness. Therefore, we will apply a log transformation to address this issue.","f05ec87e":"# 4. Data preprocessing","bcad573c":"As we can see, CatBoost has the highest cross validation mean and thus we will proceed with this model.","b0422f0a":"## 5.2 Fit model to data and make predictions","11e0b42a":"The 'Age', 'Cabin', 'Fare' columns have missing values.","dc60b452":"I achieved a submission score of 0.7799. In other words, I have correctly predicted 77.99 of the test set. \nI drew inspirations from the following notebooks in the making of this notebook:\n* https:\/\/www.kaggle.com\/startupsci\/titanic-data-science-solutions Titanic Data Science Solutions by Manav Sehgal\n* https:\/\/www.kaggle.com\/yassineghouzam\/titanic-top-4-with-ensemble-modeling Titanic Top 4% With Emsemble Modelling by Yassin Ghouzam\n* https:\/\/www.kaggle.com\/catadanna\/titanic Titanesque by Catadanna","1fb10db7":"### Categorical variable: Embarked","a5ce81d4":"### Numerical variable: Pclass","d0e91af4":"### 4.3.2 Alone","39fb0ed4":"### Numerical variable: SibSp","10a1191f":"## 5.3.3 Major","3ba95f10":"## 5.1 Split training data","0d0a0da8":"### 5.2.3 K-nearest neighbours (KNN)","8ae96446":"In this section, we will perform the following preprocessing steps:\n\nDrop and fill missing values\nData trasformation (log transformation)\nFeature engineering\nFeature encoding","78f6a11f":"I will also transform Fare into an ordinal variable rather than a continuous variable.","3a6ac37d":"### 3.2.1 Numerical variables\nThe numerical variables are : SibSp, Parch, Age, Pclass and Fare.","66801f79":"## 3.2.2 Categorical variables","58c40167":"In this section, we will analyse the features in our dataset individually and see how they correlate with survival probability.","91b6cb80":" In this notebook we create a model that predicts which passengers survived the Titanic shipwreck.","776bd17b":"To create  Major and Minor features,  to  transform Age ","05549c26":"The Survived column is the target variable. If Suvival = 1 the passenger survived, otherwise he's dead. The is the variable we're going to predict.","27e679e7":"# 2. Import and read data\n\nWe import and read the 3 datasets : Training set,Test set and Sample submission.\nWe'll be using the training set to build our predictive model and the testing set to score it and generate an output file to submit on the Kaggle evaluation system.","f4b5104d":"### 5.2.5 Perceptron","fca49188":"### 6.3.2 K-fold cross validation","d2149467":"### Numerical variable: Fare","67af4e93":"The goal is to use features that are most correlated with Age to predict the values for Age. But first, we need to convert Sex into numerical values where 0 = male and 1 = female. This process is known as encoding and we will further explore this later in the notebook.","210ba72d":"Loop through each missing age in the list to locate the rows that have the same SibSp, Parch and PClass values and fill the missing age with the median of those rows. If rows are not found, simply fill the missing age with the median of the entire Age column.","15e56eb5":"### 5.2.9 Random forest","462c69af":"### Detect and drop the outliers","d58a8b9b":"### 5.2.6 Linear SVC","d0be5775":"# 7. Preparing data for submission","cd044758":"## 4.2 Data transformation","c4aea8f2":"# 3. Exploratory Data Analysis (EDA)","a7221a91":"### 5.2.4 Gaussian naive bayes","99985595":"Hyperparameter tuning is the process of tuning the parameters of a model. Here I will tune the parameters of CatBost classifier using GridSearchCV.","06a455c7":"### 4.3.1 Title","3931f6c2":"Age is not correlated with Sex but is negatively correlated with SibSp, Parch and Pclass.","1ccd7406":"### 6.3.3 Hyperparameter tuning for CatBoost","a26e31bb":"# 5. Modelling","5984b29d":"### Numerical variable: Parch","9f339f8e":"### 6.3 Model evaluation and hyperparameter tuning","4033be57":"The 'Age', 'Cabin', 'Embarked' columns have missing values.","5ef1624f":"## 4.3 Feature engineering","a5c34cdc":"### Categorical variable: Sex","9d0d97e3":"### 5.2.1 Logistic regression","6d55e2c2":"### 5.2.2 Support vector machines","156ed0dc":"# 1. Import libraries","4374b1b8":"### Numerical variable: Age","a3733568":"### Categorical variable: Cabin","0ef78b02":"### 5.2.8 Decision tree","9871b8f3":"there are some variables that have nothing to do with the probability of survival like PassengerId, the ticket number, the cabin number (but not maybe if they have a cabin or not, to be checked ). So we can file them safely before building our ml model. In addition, we also have to manage the missing values. So, all these tasks are data preprocessing."}}