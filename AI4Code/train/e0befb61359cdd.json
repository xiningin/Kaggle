{"cell_type":{"1cef6952":"code","f00f8b0e":"code","e9a1541e":"code","d0f2151f":"code","f6da4467":"code","353a775d":"code","a67847dd":"code","21849e9b":"markdown"},"source":{"1cef6952":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n%matplotlib inline\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","f00f8b0e":"df = pd.read_csv('\/kaggle\/input\/fetal-health-classification\/fetal_health.csv')\ndf","e9a1541e":"def data_cleaner(data):\n    data.fillna(0, inplace=True)\n    return data","d0f2151f":"df = data_cleaner(df)\ndf","f6da4467":"df.columns","353a775d":"from sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom xgboost.sklearn import XGBClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\n\ntarget = ['fetal_health']\nfeatures = ['baseline value', 'accelerations', 'fetal_movement',\n       'uterine_contractions', 'light_decelerations', 'severe_decelerations',\n       'prolongued_decelerations', 'abnormal_short_term_variability',\n       'mean_value_of_short_term_variability',\n       'percentage_of_time_with_abnormal_long_term_variability',\n       'mean_value_of_long_term_variability', 'histogram_width',\n       'histogram_min', 'histogram_max', 'histogram_number_of_peaks',\n       'histogram_number_of_zeroes', 'histogram_mode', 'histogram_mean',\n       'histogram_median', 'histogram_variance', 'histogram_tendency']\n\nX = df[features]\ny = df[target]\n\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42, shuffle=True)\n\nmodels = []\nmodels.append(('LogisticRegression', LogisticRegression()))\nmodels.append(('DecisionTreeClassifier', DecisionTreeClassifier()))\nmodels.append(('XGBClassifier', XGBClassifier()))\nmodels.append(('GradientBoostingClassifier', GradientBoostingClassifier()))\nmodels.append(('KNeighborsClassifier', KNeighborsClassifier()))\nmodels.append(('RandomForestClassifier', RandomForestClassifier()))\n\nresults = []\nnames = []\nfor name, model in models:\n    kfold = KFold(n_splits=10, random_state=0)\n    cv_results = cross_val_score(model, X, y, cv=kfold, scoring='accuracy')\n    results.append(cv_results)\n    names.append(name)\n    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n    print(msg)","a67847dd":"model=XGBClassifier(random_state=0)\nmodel.fit(X_train,y_train)\ny_pred=model.predict(X_test)\n\nfrom sklearn import metrics\n\ncm = metrics.confusion_matrix(y_test, y_pred)\nplt.figure(figsize=(9,9))\nsns.heatmap(cm, annot=True, fmt=\".3f\", linewidths=.5, square = True, cmap = 'Blues_r');\nplt.ylabel('Actual label');\nplt.xlabel('Predicted label');\nall_sample_title = 'Confusion Matrix - score:'+str(metrics.accuracy_score(y_test,y_pred))\nplt.title(all_sample_title, size = 15);\nplt.show()\nprint(metrics.classification_report(y_test,y_pred))","21849e9b":"# LAZY PROGRAMMER WARNING.."}}