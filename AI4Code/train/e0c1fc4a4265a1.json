{"cell_type":{"c654be96":"code","aeb4eb85":"code","002dbf0c":"code","69ec696f":"code","a9c0859a":"code","9d26c202":"code","1662e720":"code","7d992d7a":"code","ecdb149f":"code","88a88cfb":"code","7554aa06":"code","93970834":"code","9a6f6918":"code","d276bfde":"code","94bfd3e3":"code","880a5284":"code","36e9f2c1":"code","82a30b44":"code","9197807b":"code","f8576aa7":"code","3ab478b3":"code","a8251400":"code","b67ebc2e":"code","d7df3d84":"code","502b6250":"code","268b6f77":"code","9c1a1d65":"code","35638c2f":"code","89f6423c":"code","ccb75af8":"code","661be467":"code","3e838fa7":"code","c32fab85":"code","9f005594":"code","c2005cc0":"code","793d5b36":"code","dcd1d857":"code","cdb8ff54":"code","e1feefca":"code","e367d0df":"code","60161378":"code","54f6215a":"code","4465aaa0":"code","9e2ecd79":"code","77af7185":"code","6d68cb89":"code","ae6e4eaa":"code","d6ec6d89":"code","496011e1":"code","5c413143":"code","bc37e91f":"code","b080bad5":"code","318201e0":"code","abb57617":"code","628d1a68":"code","59d1055f":"code","521cdd8d":"code","9f62ea52":"code","7e557e98":"code","fa4320ee":"code","a1976139":"code","cc173ba1":"code","5f57182f":"code","3d9252bc":"markdown","3cce7d97":"markdown","df4db286":"markdown","d71c3669":"markdown","502ed0fb":"markdown","aad5eded":"markdown","f29175d4":"markdown","9ecd1eec":"markdown","b038be69":"markdown","b5a86eef":"markdown","10aecd60":"markdown","6810e50f":"markdown","cc6e0cfd":"markdown","c0fbba81":"markdown","dbafb3cd":"markdown","b9e8a81e":"markdown","5e2c32c8":"markdown","3ebbd570":"markdown","7c39f99e":"markdown","9326389e":"markdown","b4e4ddb7":"markdown","78423bdf":"markdown","f615f877":"markdown","6bb0499c":"markdown","0635d629":"markdown","67edcb39":"markdown","e5d8cb6c":"markdown","ec513f2b":"markdown","46761e4b":"markdown","9f8012ec":"markdown"},"source":{"c654be96":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n#For plotting graphs\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n#Library for getting mutual info\nfrom sklearn.feature_selection import mutual_info_regression\n\n# for doing randomized or grid search of best estimation in model parameter\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n\n#for scalling the data\nfrom sklearn.preprocessing import MinMaxScaler\n\n# for calculating all scores\nfrom sklearn.metrics import accuracy_score,recall_score,precision_score,f1_score,confusion_matrix,roc_auc_score, r2_score\n\n# all different models used here\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom xgboost import XGBClassifier\n\n# to plot confusion matrix of predicted and Y_test value\nfrom sklearn.metrics import confusion_matrix\n\n#to ignore warning\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","aeb4eb85":"heart=pd.read_csv(\"\/kaggle\/input\/heartcsv\/Heart.csv\")\nheart.head()","002dbf0c":"heart.info()","69ec696f":"heart.shape","a9c0859a":"heart.describe()","9d26c202":"#to check only those column which have null in them\nheart[[i for i in heart.columns if heart[i].isnull().sum()>0]].isnull().sum()","1662e720":"# Removing the NaN\nheart.Thal.value_counts()","7d992d7a":"# As the maximum number of cases have 'Normal' as the Thal, replacing the missing value to 'Normal'\nheart.Thal = heart.Thal.fillna('normal')\nheart.Thal.value_counts()","ecdb149f":"heart.Ca.value_counts()","88a88cfb":"# As the maximum number of cases have 0.0 as the Ca, replacing the missing value to 0.0\nheart.Ca = heart.Ca.fillna(0.0)\nheart.Ca.value_counts()","7554aa06":"print(heart.ChestPain.unique())\nprint(heart.Thal.unique())\nprint(heart.AHD.unique())","93970834":"#nominal encoding technique\nheart_encoding = pd.get_dummies(heart[['ChestPain', 'Thal', 'AHD']])\nheart_final = pd.concat([heart, heart_encoding],1)\nheart_final = heart_final.drop(['ChestPain', 'Thal', 'AHD'], axis = 1)\nheart_final.head(2)","9a6f6918":"# checking no. of males(1) and female (0)\nheart_final.Sex.value_counts()","d276bfde":"# Checking AHD wrt sex\nheart_final.Sex[heart_final.AHD_Yes ==1].value_counts()","94bfd3e3":"pd.crosstab(heart_final.AHD_Yes,heart_final.Sex)","880a5284":"pd.crosstab(heart_final.AHD_Yes,heart_final.Sex).plot(kind='bar',figsize=(20,10),color=[\"blue\",\"green\"])\nplt.title(\"Frequency of Heart Disease vs Sex\")\nplt.xlabel(\"0= AHD_Yes, 1= AHD_NO\")\nplt.ylabel(\"No. of people with heart disease\")\nplt.legend([\"Female\",\"Male\"])\nplt.xticks(rotation=0);","36e9f2c1":"heart_final.Sex[heart_final.AHD_Yes==1].value_counts().plot(kind='bar',figsize=(10,6),color=['green','blue'])\nplt.title(\"males vs females with heart disease\")","82a30b44":"# Finding co-relation between all column\nheart_final.corr()","9197807b":"cor_mat=heart_final.corr()\nfig,ax=plt.subplots(figsize=(15,10))\nsns.heatmap(cor_mat,annot=True,linewidths=0.5,fmt=\".3f\")","f8576aa7":"# Conclusion:- from above co-relation and heatmap we can say that AHD_NO and AHD_yes are giving the same information, so removing one of them (AHD_No)\n# droping AHD_No as its giving the same value as AHD_Yes, also droping 'Unnamed: 0' as its doesnot have any benift\nheart_final = heart_final.drop(['AHD_No', 'Unnamed: 0'], axis = 1)\nheart_final.head(2)","3ab478b3":"# getting all final column name\nheart_final.columns","a8251400":"heart_scaled = heart_final","b67ebc2e":"# Scaling all values except the target variable = AHD_Yes\nMMscal=MinMaxScaler()\nfeatures=['Age', 'Sex', 'RestBP', 'Chol', 'Fbs', 'RestECG', 'MaxHR', 'ExAng',\n       'Oldpeak', 'Slope', 'Ca', 'ChestPain_asymptomatic',\n       'ChestPain_nonanginal', 'ChestPain_nontypical', 'ChestPain_typical',\n       'Thal_fixed', 'Thal_normal', 'Thal_reversable']\nheart_scaled[features] = MMscal.fit_transform(heart_final[features])\nheart_scaled.head()","d7df3d84":"# Creating Features and Target variable\nX = heart_scaled.drop('AHD_Yes', axis=1)\nY = heart_scaled.AHD_Yes","502b6250":"#splitting the data into training and testing data sets\nfrom sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test=train_test_split(X,Y,test_size=0.2 ,random_state=1)","268b6f77":"mutual_info = mutual_info_regression(X_train, y_train)\nmutual_info = pd.Series(mutual_info)\nmutual_info.index = X_train.columns\nmutual_info = mutual_info.sort_values(ascending=False)\nmutual_info","9c1a1d65":"#Considering the columns for training the model which are more than 0% of information shared with dependent variable\/feature\nReq_Columns = list(mutual_info[mutual_info>0].index)\nReq_Columns","35638c2f":"X_train_final = X_train[Req_Columns]\nX_train_final.head(1)","89f6423c":"X_test_final = X_test[Req_Columns]\nX_test_final.head(1)","ccb75af8":"#X_test_final = X_test\n#X_train_final = X_train","661be467":"def evaluation(Y_test,Y_pred):\n    acc=accuracy_score(Y_test,Y_pred)\n    rcl=recall_score(Y_test,Y_pred)\n    f1=f1_score(Y_test,Y_pred)\n    auc_score=roc_auc_score(Y_test,Y_pred)\n    prec_score=precision_score(Y_test,Y_pred)\n    \n    metric_dict={'accuracy': round(acc*100,2),\n               'recall': round(rcl*100,2),\n               'F1 score': round(f1*100,2),\n               'auc score': round(auc_score*100,2),\n               'precision': round(prec_score*100,2)\n                }\n    \n    return print(metric_dict)","3e838fa7":"np.random.seed(42)\nKNC_model=  KNeighborsClassifier()\nKNC_model.fit(X_train_final,y_train)\nKNC_model_y_pred=KNC_model.predict(X_test_final)\nKNC_model_r2_score=round(r2_score(y_test,KNC_model_y_pred)*100,2)\nprint(\"R2 Score for predicted value: \",KNC_model_r2_score)\nprint(\"Accuracy on Training set: \",round(KNC_model.score(X_train_final,y_train)*100,2))\nKNC_model_score = round(KNC_model.score(X_test_final,y_test)*100,2)\nprint(\"Accuracy on Testing set: \",KNC_model_score)\nevaluation(y_test,KNC_model_y_pred)","c32fab85":"np.random.seed(42)\nLR_model=LogisticRegression()\nLR_model.fit(X_train_final,y_train)\nLR_model_y_pred=LR_model.predict(X_test_final)\nLR_model_r2_score=round(r2_score(y_test,LR_model_y_pred)*100,2)\nprint(\"R2 Score for predicted value: \",LR_model_r2_score)\nprint(\"Accuracy on Training set: \",round(LR_model.score(X_train_final,y_train)*100,2))\nLR_model_score = round(LR_model.score(X_test_final,y_test)*100,2)\nprint(\"Accuracy on Testing set: \",LR_model_score)\nevaluation(y_test,LR_model_y_pred)","9f005594":"np.random.seed(42)\nRFC_model=RandomForestClassifier()\nRFC_model.fit(X_train_final,y_train)\nRFC_model_y_pred=RFC_model.predict(X_test_final)\nRFC_model_r2_score=round(r2_score(y_test,RFC_model_y_pred)*100,2)\nprint(\"R2 Score for predicted value: \",RFC_model_r2_score)\nprint(\"Accuracy on Training set: \",round(RFC_model.score(X_train_final,y_train)*100,2))\nRFC_model_score = round(RFC_model.score(X_test_final,y_test)*100,2)\nprint(\"Accuracy on Testing set: \",RFC_model_score)\nevaluation(y_test,RFC_model_y_pred)","c2005cc0":"np.random.seed(42)\nSVC_model=SVC()\nSVC_model.fit(X_train_final,y_train)\nSVC_model_y_pred=SVC_model.predict(X_test_final)\nSVC_model_r2_score=round(r2_score(y_test,SVC_model_y_pred)*100,2)\nprint(\"R2 Score for predicted value: \",SVC_model_r2_score)\nprint(\"Accuracy on Training set: \",round(SVC_model.score(X_train_final,y_train)*100,2))\nSVC_model_score = round(SVC_model.score(X_test_final,y_test)*100,2)\nprint(\"Accuracy on Testing set: \",SVC_model_score)\nevaluation(y_test,SVC_model_y_pred)","793d5b36":"XGB_model=XGBClassifier()\nXGB_model.fit(X_train_final,y_train)\nXGB_model_y_pred=XGB_model.predict(X_test_final)\nXGB_model_r2_score=round(r2_score(y_test,XGB_model_y_pred)*100,2)\nprint(\"R2 Score for predicted value: \",XGB_model_r2_score)\nprint(\"Accuracy on Training set: \",round(XGB_model.score(X_train_final,y_train)*100,2))\nXGB_model_score = round(XGB_model.score(X_test_final,y_test)*100,2)\nprint(\"Accuracy on Testing set: \",XGB_model_score)\nevaluation(y_test,XGB_model_y_pred)","dcd1d857":"all_model_score = pd.DataFrame({'Model_Name': ['Logistic Regression', 'Random Forest', 'K-Nearest Neighbour', 'Support Vector Machine', \"Extreme Gradient Boost\"], \n                                'Accuracy_Score': [LR_model_score, RFC_model_score, KNC_model_score, SVC_model_score, XGB_model_score]})\nall_model_score = all_model_score.sort_values(by=['Accuracy_Score'], ascending=False)\nall_model_score","cdb8ff54":"j = range(1, 31) # 1 to 30\n\n# Setup algorithm\nknc_model = KNeighborsClassifier()\nknc_count=1\nscore_final = 0\n# Loop through different neighbors values\nfor i in j:\n    knc_model.set_params(n_neighbors = i) # set neighbors value\n    score = round(knc_model.fit(X_train_final, y_train).score(X_test_final,y_test)*100,2)\n    if score > score_final:\n        score_final = score\n        knc_count = i\n    # Fit the algorithm\n    #print(f\"Accuracy with {i} no. of neighbors: {score}%\")\nprint('Best n_neighbors is: ' + str(knc_count) + ', with accuracy score of ' + str(score_final) +'%')","e1feefca":"np.random.seed(42)\nfrom sklearn.neighbors import KNeighborsClassifier\nKNC_model=  KNeighborsClassifier(n_neighbors=knc_count)\nKNC_model.fit(X_train_final,y_train)\nKNC_model_y_pred=KNC_model.predict(X_test_final)\nKNC_model_r2_score=round(r2_score(y_test,KNC_model_y_pred)*100,2)\nprint(\"R2 Score for predicted value: \",KNC_model_r2_score)\nprint(\"Accuracy on Training set: \",round(KNC_model.score(X_train_final,y_train)*100,2))\nKNC_model_score_mannual = round(KNC_model.score(X_test_final,y_test)*100,2)\nprint(\"Accuracy on Testing set: \",KNC_model_score_mannual)\nevaluation(y_test,KNC_model_y_pred)","e367d0df":"knn_param_grid={'n_neighbors': np.arange(1,31,1),\n          'leaf_size': np.arange(1,31,1)}\n\nknn_gs_model=GridSearchCV(KNeighborsClassifier(),param_grid=knn_param_grid,cv=5,verbose=True)\n\nknn_gs_model.fit(X_train_final, y_train)","60161378":"knn_gs_model.best_params_","54f6215a":"best_results = pd.DataFrame(knn_gs_model.best_params_, index=[0])\nbest_results","4465aaa0":"np.random.seed(42)\nfrom sklearn.neighbors import KNeighborsClassifier\nKNC_model=  KNeighborsClassifier(n_neighbors=best_results.n_neighbors[0], leaf_size = best_results.leaf_size[0] )\nKNC_model.fit(X_train_final,y_train)\nKNC_model_y_pred=KNC_model.predict(X_test_final)\nKNC_model_r2_score=round(r2_score(y_test,KNC_model_y_pred)*100,2)\nprint(\"R2 Score for predicted value: \",KNC_model_r2_score)\nprint(\"Accuracy on Training set: \",round(KNC_model.score(X_train_final,y_train)*100,2))\nKNC_model_score_gs = round(KNC_model.score(X_test_final,y_test)*100,2)\nprint(\"Accuracy on Testing set: \",KNC_model_score_gs)\nevaluation(y_test,KNC_model_y_pred)","9e2ecd79":"# getting maximum of both tunning method\nKNC_model_score2 =max(KNC_model_score_gs, KNC_model_score_mannual)\nKNC_model_score2","77af7185":"np.random.seed(42)\nrf_count=1\nscore_final = 0\nrfc_model = RandomForestClassifier()\n#checked once till 50, still found the best estimator at 7, so decresed the range to 10 for future runs\nfor i in range(1,10,1):\n    #print(f\"With {i} estimators:\")\n    rfc_model=rfc_model.set_params(n_estimators=i*10,max_depth=i,random_state=i)\n    score = round(rfc_model.fit(X_train_final, y_train).score(X_test_final,y_test)*100,2)\n    if score > score_final:\n        score_final = score\n        rf_count = i*10\n    #print(f\"Accuracy: {clf2.score(X_test_final,y_test)*100:2f}%\")\nprint('Best n_estimators is: ' + str(rf_count) + ', with accuracy score of ' + str(score_final) +'%')","6d68cb89":"np.random.seed(42)\nRFC_model=RandomForestClassifier(n_estimators=rf_count, max_depth=(rf_count\/10),random_state=int(rf_count\/10))\nRFC_model.fit(X_train_final,y_train)\nRFC_model_y_pred=RFC_model.predict(X_test_final)\nRFC_model_r2_score=round(r2_score(y_test,RFC_model_y_pred)*100,2)\nprint(\"R2 Score for predicted value: \",RFC_model_r2_score)\nprint(\"Accuracy on Training set: \",round(RFC_model.score(X_train_final,y_train)*100,2))\nRFC_model_score2 = round(RFC_model.score(X_test_final,y_test)*100,2)\nprint(\"Accuracy on Testing set: \",RFC_model_score2)\nevaluation(y_test,RFC_model_y_pred)","ae6e4eaa":"learning_rate = [0.01, 0.1]\nmax_depth = [int(x) for x in np.linspace(5, 40, num = 6)]\nmin_child_weight = [int(x) for x in np.linspace(1, 20, num = 6)]\nsubsample =  [0.5, 0.7]\ncolsample_bytree = [0.5, 0.7]\nobjective = ['reg:squarederror']\nn_estimators = [int(x) for x in np.linspace(start = 20, stop = 2000, num = 40)]\ngamma = [0.6, 0.7]\nseed = [27]\nreg_lambda = [2]\nbooster = ['dart']\ncolsample_bylevel = [0.6]\ncolsample_bynode = [0.5]\n\nrandom_grid_param = {'learning_rate': learning_rate,\n               'max_depth': max_depth,\n               'min_child_weight': min_child_weight,\n               'subsample': subsample,\n               'colsample_bytree': colsample_bytree,\n               'objective': objective,\n               'n_estimators': n_estimators,\n               'gamma': gamma,\n               'seed' : seed,\n               'reg_lambda' : reg_lambda,\n               'booster' : booster,\n               'colsample_bylevel' : colsample_bylevel,\n               'colsample_bynode' : colsample_bynode}\n\n\nXGB_rg_model = RandomizedSearchCV(estimator = XGB_model, param_distributions = random_grid_param,scoring='neg_mean_squared_error',\n                               n_iter = 10, cv = 5, verbose=2, random_state=42, n_jobs = 1)\n\nXGB_rg_model.fit(X_train_final,y_train)","d6ec6d89":"XGB_rg_model.best_params_","496011e1":"best_results_XGB = pd.DataFrame(XGB_rg_model.best_params_, index=[0])\nbest_results_XGB","5c413143":"XGB_model=XGBClassifier(subsample= best_results_XGB.subsample[0], \n                        seed= best_results_XGB.seed[0], \n                        reg_lambda= best_results_XGB.reg_lambda[0],\n                        objective= best_results_XGB.objective[0],\n                        n_estimators= best_results_XGB.n_estimators[0],\n                        min_child_weight= best_results_XGB.min_child_weight[0],\n                        max_depth= best_results_XGB.max_depth[0],\n                        learning_rate= best_results_XGB.learning_rate[0],\n                        gamma= best_results_XGB.gamma[0],\n                        colsample_bytree= best_results_XGB.colsample_bytree[0],\n                        colsample_bynode= best_results_XGB.colsample_bynode[0],\n                        colsample_bylevel= best_results_XGB.colsample_bylevel[0],\n                        booster= best_results_XGB.booster[0])\n\nXGB_model.fit(X_train_final,y_train)\nXGB_model_y_pred=XGB_model.predict(X_test_final)\nXGB_model_r2_score=round(r2_score(y_test,XGB_model_y_pred)*100,2)\nprint(\"R2 Score for predicted value: \",XGB_model_r2_score)\nprint(\"Accuracy on Training set: \",round(XGB_model.score(X_train_final,y_train)*100,2))\nXGB_model_score2 = round(XGB_model.score(X_test_final,y_test)*100,2)\nprint(\"Accuracy on Testing set: \",XGB_model_score2)\nevaluation(y_test,XGB_model_y_pred)","bc37e91f":"# defining parameter range \nparam_grid = {'C': [0.1, 1,2, 10, 100, 1000],  \n              'gamma': [1, 0.1, 0.01, 0.001, 0.0001], \n              'kernel': ['rbf','linear']}  \n  \nsvc_gs_model = GridSearchCV(SVC(), param_grid,cv=5, refit = True, verbose = 3) \n  \n# fitting the model for grid search \nsvc_gs_model.fit(X_train_final, y_train)","b080bad5":"print(svc_gs_model.best_params_)\nprint(f\"Accuracy score:{svc_gs_model.score(X_test_final,y_test)}%\")","318201e0":"best_results_SVC = pd.DataFrame(svc_gs_model.best_params_, index=[0])\nbest_results_SVC","abb57617":"np.random.seed(42)\nSVC_model=SVC(C= best_results_SVC.C[0], gamma= best_results_SVC.gamma[0], kernel= best_results_SVC.kernel[0])\nSVC_model.fit(X_train_final,y_train)\nSVC_model_y_pred=SVC_model.predict(X_test_final)\nSVC_model_r2_score=round(r2_score(y_test,SVC_model_y_pred)*100,2)\nprint(\"R2 Score for predicted value: \",SVC_model_r2_score)\nprint(\"Accuracy on Training set: \",round(SVC_model.score(X_train_final,y_train)*100,2))\nSVC_model_score2 = round(SVC_model.score(X_test_final,y_test)*100,2)\nprint(\"Accuracy on Testing set: \",SVC_model_score2)\nevaluation(y_test,SVC_model_y_pred)","628d1a68":"# creating dataframe with accuracy score after tunning\nall_model_score_tunned = pd.DataFrame({'Model_Name': ['Logistic Regression', 'Random Forest_Tunned', 'K-Nearest Neighbour_Tunned', \n                                                      'Support Vector Machine_Tunned',\"Extreme Gradient Boost_Tunned\"], \n                                'Accuracy_Score': [LR_model_score, RFC_model_score2, KNC_model_score2, SVC_model_score2, XGB_model_score2]})\nall_model_score_tunned = all_model_score_tunned.sort_values(by=['Accuracy_Score'], ascending=False)\nall_model_score_tunned","59d1055f":"# dataframe with accuracy score before tunning\nall_model_score","521cdd8d":"# concatinating both the before and after tunning model accuracy\nall_model_score_final = pd.concat([all_model_score_tunned, all_model_score],0)\nall_model_score_final = all_model_score_final.sort_values(by=['Accuracy_Score'], ascending=False)\nall_model_score_final","9f62ea52":"#removing duplicate value of Logistic Regression\nall_model_score_final = all_model_score_final.drop_duplicates()\nall_model_score_final","7e557e98":"print(\"Best evaluation parameters achieved with Random Forest and XGBoost model:\")\nprint(\"Evaluation parameters achieved with Random Forest:\")\nevaluation(y_test,RFC_model_y_pred)\nprint(\"Evaluation parameters achieved with XGBoost model:\")\nevaluation(y_test,XGB_model_y_pred)","fa4320ee":"XGB_final_metrics={'Accuracy': XGB_model.score(X_test_final,y_test),\n                   'Precision': precision_score(y_test,XGB_model_y_pred),\n                   'Recall': recall_score(y_test,XGB_model_y_pred),\n                   'F1': f1_score(y_test,XGB_model_y_pred),\n                   'AUC': roc_auc_score(y_test,XGB_model_y_pred)}\n\nXGB_metrics=pd.DataFrame(XGB_final_metrics,index=[0])\nXGB_metrics.T.plot.bar(title='XGBoost metric evaluation',legend=False);","a1976139":"fig,ax=plt.subplots()\nax=sns.heatmap(confusion_matrix(y_test,XGB_model_y_pred),annot=True,cbar=True)","cc173ba1":"RFC_final_metrics={'Accuracy': RFC_model.score(X_test_final,y_test),\n                   'Precision': precision_score(y_test,RFC_model_y_pred),\n                   'Recall': recall_score(y_test,RFC_model_y_pred),\n                   'F1': f1_score(y_test,RFC_model_y_pred),\n                   'AUC': roc_auc_score(y_test,RFC_model_y_pred)}\n\nRFC_metrics=pd.DataFrame(RFC_final_metrics,index=[0])\nRFC_metrics.T.plot.bar(title='Random Forest Classifier metric evaluation',legend=False);","5f57182f":"fig,ax=plt.subplots()\nax=sns.heatmap(confusion_matrix(y_test,RFC_model_y_pred),annot=True,cbar=True)","3d9252bc":"<h2 style=\"text-align: left; font-family: 'Garamond'; font-size:25px; color:purple\">3. RandomForestClassifier<\/h2>","3cce7d97":"<h2 style=\"text-align: left; font-family: 'Garamond'; font-size:25px; color:purple\">3.Tuning XGBClassifier(XG Boost\/ Extreme Gradient Boost) using hyperparameter (RandomizedSearchCV) <\/h2>","df4db286":"<h3 style=\"text-align: left; font-family: 'Garamond'; font-size:20px; color:teal\">Hyperparameter tunning for  KNeighborsClassifier\/(K-Nearest Neighbour) using GridSearchCV<\/h3>","d71c3669":"<h3 style=\"text-align: left; font-family: 'Garamond'; font-size:20px; color:teal\">Manual Fine Tunning<\/h3>","502ed0fb":"<h2 style=\"text-align: left; font-family: 'Garamond'; font-size:25px; color:purple\">1. KNeighborsClassifier (K-Nearest Neighbour)<\/h2> ","aad5eded":"<h2 style=\"text-align: left; font-family: 'Garamond'; font-size:25px; color:purple\">4. SVC (Support Vector Machine)<\/h2>","f29175d4":"<h1 style=\"text-align: left; font-family: 'Garamond'; font-size:30px; color:blue\">Training the Data woth different Models<\/h1>","9ecd1eec":"<h1 style=\"text-align: left; font-family: 'Garamond'; font-size:30px; color:blue\">Get Mutual Information<\/h1>","b038be69":"<h1 style=\"text-align: left; font-family: 'Garamond'; font-size:40px; color:blue\"> Start <\/h1>","b5a86eef":"<h2 style=\"text-align: left; font-family: 'Garamond'; font-size:25px; color:purple\">2. LogisticRegression<\/h2> ","10aecd60":"<h1 style=\"text-align: left; font-family: 'Garamond'; font-size:30px; color:blue\">Fine Tunning the Model<\/h1>","6810e50f":"<h3 style=\"text-align: left; font-family: 'Garamond'; font-size:20px; color:teal\">Cretating a dataframe with all accuracy score of models<\/h3>","cc6e0cfd":"<h1 style=\"text-align: left; font-family: 'Garamond'; font-size:30px; color:blue\">Create a function for evaluating metrics <\/h1>","c0fbba81":"<h2 style=\"text-align: left; font-family: 'Garamond'; font-size:25px; color:purple\">2. RandomForestClassifier<\/h2>","dbafb3cd":"<h2 style=\"text-align: left; font-family: 'Garamond'; font-size:25px; color:purple\">Evalution matics for XGB Model<\/h2>","b9e8a81e":"<h1 style=\"text-align: left; font-family: 'Garamond'; font-size:30px; color:blue\"> Conclusion<\/h1>","5e2c32c8":"<h1 style=\"text-align: left; font-family: 'Garamond'; font-size:30px; color:blue\"> Feature Engineering - Data Cleaning and Creating an ADS for Analysis<\/h1>","3ebbd570":"<p style=\"-moz-border-radius: 16px;\n         -webkit-border-radius: 16px;\n         background-color: #f0f7fb;\n         background-image: url(https:\/\/f1.madcapsoftware.com\/blogImages\/2017\/08\/css-box-icon-3.png);\n         background-position: 0px 0px;\n         background-repeat: no-repeat;\n         border: solid 1px #3498db;\n         border-radius: 10px;\n         line-height: 38px;\n         overflow: hidden;\n         padding: 15px 60px;\n          color:green;\n          font-size: 14px\"><strong><B>Problem Statement:<\/B><\/strong> With the heart dataset,trying to predict if someone will get Acute heart disease(AHD) with other health parameter<\/p>\n","7c39f99e":"<h2 style=\"text-align: left; font-family: 'Garamond'; font-size:25px; color:purple\">Evalution matics for Random Forest Model<\/h2>","9326389e":"<h2 style=\"text-align: left; font-family: 'Garamond'; font-size:25px; color:purple\">4.Hyper parameter tuning SVC(Support Vector Machine) using GridSearchCV<\/h2>","b4e4ddb7":"<h2 style=\"text-align: left; font-family: 'Garamond'; font-size:25px; color:purple\">1. KNeighborsClassifier (K-Nearest Neighbour)<\/h2> ","78423bdf":"<h1 style=\"text-align: left; font-family: 'Garamond'; font-size:40px; color:blue\"> END <\/h1>","f615f877":"<p style=\"-moz-border-radius: 16px;\n         -webkit-border-radius: 16px;\n         background-color: #f0f7fb;\n         background-image: url(https:\/\/f1.madcapsoftware.com\/blogImages\/2017\/08\/css-box-icon-3.png);\n         background-position: 0px 0px;\n         background-repeat: no-repeat;\n         border: solid 1px #3498db;\n         border-radius: 10px;\n         line-height: 38px;\n         overflow: hidden;\n         padding: 15px 60px;\n          color:green;\n          font-size: 14px\"><strong><B>Conclusion Statement:<\/B><\/strong> Using Random Forest classfier model and Extreme Gradiant Boosting classifier model we are getting around 87% accuracy.<\/p>\n\n<p style=\"-moz-border-radius: 16px;\n         -webkit-border-radius: 16px;\n         background-color: #f0f7fb;\n         background-image: url(https:\/\/f1.madcapsoftware.com\/blogImages\/2017\/08\/css-box-icon-3.png);\n         background-position: 0px 0px;\n         background-repeat: no-repeat;\n         border: solid 1px #3498db;\n         border-radius: 10px;\n         line-height: 38px;\n         overflow: hidden;\n         padding: 15px 60px;\n          color:green;\n          font-size: 14px\"><strong><B>Business Impact:<\/B><\/strong> Using this model Hospitals\/Doctors can predict the likelyhood of someone getting a acute heart disease by getting the pateints health inputs\/parameters.<\/p>\n          \n<p class=\"alert alert-danger\" style=\"-moz-border-radius: 16px;\n         -webkit-border-radius: 16px;\n         background-image: url(https:\/\/f1.madcapsoftware.com\/blogImages\/2017\/08\/css-box-icon-4.png);\n         background-position: 2px 0px;\n         background-repeat: no-repeat;\n         border: solid 1px;\n         border-radius: 10px;\n         line-height: 38px;\n         overflow: hidden;\n         padding: 15px 60px;\n         font-size: 14px\"><strong>Important:<\/strong> Please do upvote if you find this resourceful and leave comments<\/p>\n\n<p style=\"-moz-border-radius: 16px;\n         -webkit-border-radius: 16px;\n         background-color: #f0f7fb;\n         background-position: 0px 0px;\n         background-repeat: no-repeat;\n         border: solid 1px #3498db;\n         border-radius: 10px;\n         line-height: 38px;\n         overflow: hidden;\n         padding: 15px 60px;\n          color:darkblue;\n          font-size: 14px\"><strong><B>Keep kaggling :) !!!!!!!!<\/B><\/strong><\/p>\n","6bb0499c":"<h1 style=\"text-align: left; font-family: 'Garamond'; font-size:30px; color:blue\"> Loading\/Viewing Data <\/h1>","0635d629":"<p style=\"-moz-border-radius: 16px;\n         -webkit-border-radius: 16px;\n         background-color: #f0f7fb;\n         background-image: url(https:\/\/f1.madcapsoftware.com\/blogImages\/2017\/08\/css-box-icon-3.png);\n         background-position: 0px 0px;\n         background-repeat: no-repeat;\n         border: solid 1px #3498db;\n         border-radius: 10px;\n         line-height: 38px;\n         overflow: hidden;\n         padding: 15px 60px;\n          color:green;\n          font-size: 14px\"><strong>Message:<\/strong>Please suggest any improvement<\/p>\n<p style=\"-moz-border-radius: 16px;\n         -webkit-border-radius: 16px;\n         background-color: #f0f7fb;\n         background-image: url(https:\/\/f1.madcapsoftware.com\/blogImages\/2017\/08\/css-box-icon-3.png);\n         background-position: 0px 0px;\n         background-repeat: no-repeat;\n         border: solid 1px #3498db;\n         border-radius: 10px;\n         line-height: 38px;\n         overflow: hidden;\n         padding: 15px 60px;\n          color:green;\n          font-size: 14px\"><strong>Note:<\/strong>  Please do note as the target variable is categorical, we are using Logistic regression and \nclassifier model of all other models<\/p>\n","67edcb39":"<h1 style=\"text-align: left; font-family: 'Garamond'; font-size:30px; color:blue\">Looking at the evaluation metrics for our best model (Visual confirmation)<\/h1>","e5d8cb6c":"<h1 style=\"text-align: center; font-family: 'Garamond'; font-size:50px; color:red\">\ud83d\udc96 Heart Disease Prediction \ud83d\udc96<\/h1>","ec513f2b":"<h2 style=\"text-align: left; font-family: 'Garamond'; font-size:25px; color:purple\">5. XGBClassifier (XG Boost\/ Extreme Gradient Boost)<\/h2>","46761e4b":"##### Conclusion is that Male are more prone toward AHD","9f8012ec":"<h1 style=\"text-align: left; font-family: 'Garamond'; font-size:30px; color:blue\"> Imports of required Python Libraries <\/h1>"}}