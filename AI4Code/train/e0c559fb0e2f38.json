{"cell_type":{"f01d8fed":"code","dfdccefd":"code","30219402":"code","357c9e2a":"code","4fdfece5":"code","ddbe6932":"code","bd8cf52d":"code","13b672d4":"code","988c184e":"code","897178ab":"code","072405ca":"code","7fd847d2":"code","cad2f52d":"code","ac3e5fc4":"code","3f558d6d":"code","a417b8f9":"code","6974c3c0":"code","eca34253":"code","52a9ba0a":"code","767eb8fa":"code","82212498":"code","a6a4ba99":"code","7bd783ba":"code","7be3f719":"code","81ac9be1":"code","d5a48f9a":"code","422710b3":"code","0d7035b5":"code","9768f9b8":"markdown","8f798561":"markdown","136f8ed0":"markdown","9f02b348":"markdown","21c0c8e8":"markdown","7bbebdb4":"markdown","e4ba6785":"markdown","e12e8586":"markdown","445fbf7b":"markdown","754e763a":"markdown","1563866d":"markdown","fd6e7f76":"markdown","637dce30":"markdown","70a9871d":"markdown","bf687e57":"markdown","788e6a2b":"markdown","e8d2eec1":"markdown","0d72e7a5":"markdown","6a08654e":"markdown","38f1d0d2":"markdown","3407382a":"markdown"},"source":{"f01d8fed":"# import the important libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\n\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPool2D, Dropout","dfdccefd":"# Input data files are available in the \"..\/input\/\" directory.\ntrain = pd.read_csv(\"..\/input\/digit-recognizer\/train.csv\")\ntest = pd.read_csv(\"..\/input\/digit-recognizer\/test.csv\")","30219402":"train.shape, test.shape","357c9e2a":"X = train.drop(labels=[\"label\"], axis=1)\ny = train[\"label\"]\n\n# check the shape\nX.shape, y.shape","4fdfece5":"# checking manually\ny.value_counts()","ddbe6932":"# Checking by plotting the same\nplt.subplots(figsize = (10,8))\nplt.title('Counts in numbers to their labels ')\nsns.countplot(x=y, data=train)\nplt.show()","bd8cf52d":"X_train , X_test , y_train , y_test = train_test_split(X,y, test_size = 0.1 , random_state = 99)\n# check the shape now\nX_train.shape,X_test.shape,y_train.shape,y_test.shape,test.shape","13b672d4":"X_train=X_train.values.astype('float32')\nX_test=X_test.values.astype('float32')\ntest=test.values.astype('float32')","988c184e":"# changing the shape of X_train and y_train and test also\nX_train=X_train.reshape(X_train.shape[0], 28, 28, 1)\nX_test=X_test.reshape(X_test.shape[0], 28, 28, 1)\ntest=test.reshape(test.shape[0] , 28 , 28 , 1)","897178ab":"X_train.shape,X_test.shape,test.shape","072405ca":"# check the maximum values in the dataset\nX_train.max(),X_train.min()","7fd847d2":"X_train=X_train\/255\nX_test=X_test\/255\ntest=test\/255","cad2f52d":"# check the maximum values in the dataset\nX_train.max(),X_train.min()","ac3e5fc4":"input_shape=X_train[0].shape\ninput_shape","3f558d6d":"# import model\nmodel=Sequential()\n# layers\nmodel.add(Conv2D(filters=32, kernel_size=(3,3), activation='relu', input_shape=input_shape))\nmodel.add(Conv2D(filters=64, kernel_size=(3,3), activation='relu'))\nmodel.add(MaxPool2D(pool_size=(2,2)))\nmodel.add(Dropout(0.25))\n\n# simple ANN now\n\nmodel.add(Flatten())\nmodel.add(Dense(128, activation='relu'))\nmodel.add(Dropout(0.50))\nmodel.add(Dense(10, activation='softmax'))","a417b8f9":"model.summary()","6974c3c0":"# compile the model\nmodel.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])","eca34253":"%%time\nhistory=model.fit(X_train, y_train, batch_size=128, epochs=10, verbose=1, validation_data=(X_test,y_test))","52a9ba0a":"# evaluating the model with testing data\nloss, accuracy=model.evaluate(X_test,y_test)\nloss, accuracy","767eb8fa":"# plot the figure now\npd.DataFrame(history.history).plot(figsize=(8,5))\nplt.grid(True)\nplt.show()","82212498":"# plot confusion matrix\nfrom mlxtend.plotting import plot_confusion_matrix\nfrom sklearn.metrics import confusion_matrix","a6a4ba99":"y_pred = model.predict_classes(test)\n","7bd783ba":"class_names=['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\nclass_names","7be3f719":"mat=confusion_matrix(y_test, y_pred[:4200])\nplot_confusion_matrix(conf_mat=mat, class_names= class_names,show_normed=True, figsize=(7,7))","81ac9be1":"results = pd.Series(y_pred,name=\"Label\")","d5a48f9a":"results","422710b3":"submission = pd.concat([pd.Series(range(1,28001),name = \"ImageId\"),results],axis = 1)\n","0d7035b5":"submission.to_csv(\"submission.csv\",index=False)","9768f9b8":"check the shape of the train and test data","8f798561":"We can easily conclude from the above that the data is not unbalanced\n\nNow we will split the data into training and testing","136f8ed0":"Splitting the data into X and y(i.e independent and dependent varaible in simple terms)","9f02b348":"Evaluate the model","21c0c8e8":"Here above we converted the values of the data into float32, by which the above three dataframes got converted into a numpy array","7bbebdb4":"Fitting the model","e4ba6785":"Now we will compile the model,by using the optimizer as adam and loss as 'binary_categorical_crossentropy' and metris as ['accuracy']","e12e8586":"We can easily that the data range is between 0 to 255, here we need to normalize the data to bring it into the range of 0 to 1 so that our model predicts the data more efficiently","445fbf7b":"We will plot the data","754e763a":"Here the shape of our data is not according to the CNN architecture, so we will reshape the data into CNN architecture that is (images,rows,cols,channels)\n\nHere the images will be the no of the images used , rows and columns will be the pixels of the images mentioned in the dataset descriptions which are 28 * 28 . since all images are gray scale so it will only use '1' channel","1563866d":"**Now we will Build the Model**","fd6e7f76":"Before building the model here we will need to pass the input shape","637dce30":"Import the important libraries","70a9871d":"Import the data in the train and test from the input directory","bf687e57":"Now let us check whether the target variable is imbalanced or not in the training data","788e6a2b":"This is an example of using CNN on MNIST Dataset in the simplest way possible by me.\n\nHope everyone watching this likes it.","e8d2eec1":"Now our data has been normalized, we can also scale the data by using MinMaxScaler as well as Standard Scaler","0d72e7a5":"Making the prediction using the model","6a08654e":"Now we will check the range values of the data.","38f1d0d2":"Now check the shape again","3407382a":"Now we will check the summary to see how many parameters are we passing in this model"}}