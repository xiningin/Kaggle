{"cell_type":{"988e63b4":"code","4f25ae06":"code","69a05f74":"code","103233dc":"code","e9755053":"code","1ba93312":"code","4b4fd25f":"code","ef463677":"code","9e1edb4a":"code","6c7d6955":"code","f7a4badd":"code","68428edd":"code","7ed67844":"code","32efb145":"code","ab45a1ba":"code","f43e4298":"code","06b2432e":"code","4ee7e518":"code","7578a4e1":"code","3b9b6e68":"code","4eb4f0aa":"code","2033618f":"code","bda4154d":"code","6275dec3":"markdown","8820a8b1":"markdown","7a865399":"markdown","f28b5b00":"markdown","6ce26e0b":"markdown","2e39e5e9":"markdown","af7115bb":"markdown","0db79d72":"markdown","81b41f91":"markdown","4a8d6bd0":"markdown","ede3dbb9":"markdown","5bd42462":"markdown"},"source":{"988e63b4":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nimport tensorflow as tf\nimport tensorflow_addons as tfa\nfrom sklearn.preprocessing import LabelEncoder\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n%matplotlib inline","4f25ae06":"initial = pd.read_csv(\"..\/input\/prepaired-data-of-customer-revenue-prediction\/test_filtered.csv\", low_memory=False)\ninitial.head()","69a05f74":"initial.device_deviceCategory.value_counts()","103233dc":"desktop = initial[initial[\"device_deviceCategory\"] == \"desktop\"].sample(n=34000, random_state=42)\nmobile = initial[initial[\"device_deviceCategory\"] == \"mobile\"].sample(n=34000, random_state=42)\ntablet = initial[initial[\"device_deviceCategory\"] == \"tablet\"].sample(n=34000, random_state=42)\nsample = pd.concat([desktop, mobile, tablet])\nsample.head()","e9755053":"sample.info()","1ba93312":"how_many_ats = {}\nfor col in sample.columns:\n    if \"@\" in sample[col].value_counts().index:\n        how_many_ats[col] = sample[col].value_counts()[\"@\"]\n    else:\n        how_many_ats[col] = 0\n\npd.DataFrame(how_many_ats.items()).sort_values(1, ascending=False)","4b4fd25f":"to_be_dropped = [key for key in how_many_ats if how_many_ats[key] > 1000]\nclean = sample.drop(columns=to_be_dropped)\nclean.info()\nto_be_dropped","ef463677":"ats_left = {}\nfor col in clean.columns:\n    if \"@\" in clean[col].value_counts().index:\n        ats_left[col] = clean[col].value_counts()[\"@\"]\n    else:\n        ats_left[col] = 0\n        \npd.DataFrame(ats_left.items()).sort_values(1, ascending=False)","9e1edb4a":"cols_to_work_on = [\"device_operatingSystem\", \"geoNetwork_subContinent\", \"geoNetwork_country\", \"geoNetwork_continent\"]\nfiller = {col: clean[col].value_counts().index[0] for col in cols_to_work_on}\n\nfor col in cols_to_work_on:\n    clean[col].replace(\"@\", filler[col], inplace=True)\n    print(clean[col].value_counts())","6c7d6955":"clean.info()","f7a4badd":"categoricals = [\"channelGrouping\", \"device_browser\", \"device_operatingSystem\", \"geoNetwork_continent\", \"geoNetwork_country\", \"geoNetwork_subContinent\", \"trafficSource_source\"]\nfor cat in categoricals:\n    print(cat + \": \" + str(clean[cat].value_counts().shape[0]))","68428edd":"final = clean.copy()\nfinal = pd.get_dummies(final, columns=categoricals)\n\nprint(final.columns[:20])","7ed67844":"unused = final[[\"fullVisitorId\", \"device_isMobile\", \"date\", \"visitStartTime\"]].copy()\nfinal.drop(columns=[\"fullVisitorId\", \"device_isMobile\", \"date\", \"visitStartTime\"], inplace=True)\nfinal.head()","32efb145":"final.info()","ab45a1ba":"final.rename({\"device_deviceCategory\": \"Y\"}, axis=1, inplace=True)\nfinal.Y.value_counts()","f43e4298":"label_encoder = LabelEncoder()\nfinal[\"Y\"] = label_encoder.fit_transform(final[\"Y\"])\nfinal[\"Y\"].value_counts()","06b2432e":"Y = final.pop(\"Y\")\nY = pd.get_dummies(Y)\nY.head()","4ee7e518":"X_t_v, X_test, Y_t_v, Y_test = train_test_split(final, Y, test_size=0.2, random_state=42)\nX_train, X_val, Y_train, Y_val = train_test_split(X_t_v, Y_t_v, test_size=0.2, random_state=42)\nX_train.shape","7578a4e1":"input_size = X_train.shape[1]\noutput_size = Y.shape[1]\n\nmodel = tf.keras.Sequential([\n                            tf.keras.Input(shape=(input_size,)),\n                            tf.keras.layers.Dense(42, activation=\"relu\"),\n                            tf.keras.layers.Dense(12, activation=\"relu\"),\n                            tf.keras.layers.Dense(output_size, activation=\"softmax\")\n                            ])","3b9b6e68":"model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=\"AUC\")","4eb4f0aa":"NUM_EPOCHS = 7\nBATCH_SIZE = 100\n\nmodel.fit(x=X_train, y=Y_train, epochs=NUM_EPOCHS, batch_size=BATCH_SIZE, validation_data=(X_val, Y_val), verbose=2)","2033618f":"test_loss, test_accuracy = model.evaluate(X_test, Y_test)\n\nprint(\"\\nTest loss: \" + str(test_loss) + \". Test accuracy: \" + str(test_accuracy*100.) + \"%.\")","bda4154d":"Y_pred = pd.DataFrame(model.predict(X_test))\nY_pred.head()","6275dec3":"# Predicting Customer Device Types\n\nIn this project, we will be predicting what type of device the customers are using to connect to an e-commerce site. We will be using deep learning to predict customers' probabilities of using each of the three unique device categories.\n\nData used can be found [here](https:\/\/www.kaggle.com\/lipann\/prepaired-data-of-customer-revenue-prediction?select=test_filtered.csv). Let's take a look at it.\n\n# Reading the data in","8820a8b1":"# Preprocessing\n\nWe will begin with ditching the columns that we will not be fitting into our model.","7a865399":"We are now left with very few values to be cleaned. We will be filling these values with the most frequent value for each corresponding column.","f28b5b00":"As we can see, some of the columns are full of @ as a placeholder for missing values. We will be dropping columns that have more than 1000 @s and replacing the ones that are left with the most frequent value of the columns.","6ce26e0b":"The data currently has more than 800k rows. We'll do a quick sampling to speed up the process. However, as you can see above, the classes are very imbalanced. To prevent having an imbalanced sample, we will create 3 unique samples for each of these categories.","2e39e5e9":"As we can see, some columns have much more unique values compared to the others. Still, there's not much. We will be One-Hot encoding all of them.","af7115bb":"We will be using Label Encoding on our target column.","0db79d72":"# Cleaning the data\n\nAlthough we do not have missing values, we can see that some places were filled by \"@\". Real life organizational data is nowadays better structured than this. We will be dropping some of these columns so that they won't disrupt the algorithm. Let's see how many of these columns are problematic.","81b41f91":"Let's split our data into train, validation and test sets.","4a8d6bd0":"# Conclusions\n\nWe have predicted customers' probabilities to be using one of the three different devices in this project. Problem in hand was a Multi-Class classification problem. We had highly imbalanced classes and had to sample. At the end, our model has performed well on the test set according to our evaluation.","ede3dbb9":"Now that out dataset is clean, we will initiate encoding process on Categorical Data.\n\n# Handling Categorical Data\n\nWe will be using One-Hot encoding to deal with categorical data.","5bd42462":"# Training & evaluation of the model\n\nIn this step, we will be training and evaluating our model. We will use Rectified Linear Unit (hidden layers - helps avoid vanishing gradient) & Softmax as our activation functions. Since we're doing a multi-class classification, we will be using Categorical Cross-Entropy loss. We will be evaluating our results using ROC-AUC."}}