{"cell_type":{"13fb530d":"code","15827935":"code","3e6e9670":"code","7cb262a0":"code","e6e3f4c8":"markdown"},"source":{"13fb530d":"# Default imports from Kaggle\nimport numpy as np\nimport pandas as pd\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Custom imports\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import MinMaxScaler\n\n# Handy in order to display maps inside notebook\n%matplotlib inline","15827935":"def show_correlation(df, feature_name, target_name, plot=True, plot_kind='bar'):\n    ''' Shows the \"correlation\" (in a not necessarily statistic meaning) between a feature and its target. '''\n    try:\n        print('corr', df[feature_name].corr(df[target_name]))\n    except:\n        pass\n\n    gb = df[[feature_name, target_name]].groupby(feature_name).agg('mean')\n    print(gb)\n    \n    if plot:\n        gb.plot(kind=plot_kind)\n\n\ndef onehot(df, column_name):\n    ''' Transforms a particular categorical field in \"onehot\" so it can be used for the classification models. '''\n    return pd.concat([df, pd.get_dummies(df[column_name], prefix='_OneHot{}'.format(column_name))], axis=1)","3e6e9670":"df = pd.read_csv('..\/input\/train.csv')\ndf['_NumRelatives'] = df['Parch'] + df['SibSp']\ndf['_AgeBin'] = pd.cut(df['Age'], 8)\n\ndel df['Cabin']  # Just because it contains a lot of NaN\/null values\ndf.dropna(inplace=True)\n\ndf = onehot(df, 'Sex')\ndf = onehot(df, 'Pclass')  # Despite being numeric this is a categorical feature (numeric value per se doesn't make sense here; after all values act more like labels) \n\n# Brief overview of our dataframe\nprint('*' * 10)\nprint(df.info())\n\n# Elaborate about our dataframe so we can decide what to use as our model features\nprint('*' * 10)\nshow_correlation(df, 'Pclass', 'Survived')\n\nprint('*' * 10)\nshow_correlation(df, 'Sex', 'Survived')\n\nprint('*' * 10)\nshow_correlation(df, '_AgeBin', 'Survived')\n\nprint('*' * 10)\nshow_correlation(df, '_NumRelatives', 'Survived')","7cb262a0":"relevant_features = [\n    '_OneHotPclass_1', '_OneHotPclass_2', '_OneHotPclass_3',\n    '_OneHotSex_male', '_OneHotSex_female',\n    'Age',\n    '_NumRelatives'\n]\n\nX = df[relevant_features]\ny = df['Survived']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)  # We have too few data, so prioritize this for the training set.\n\ninner_clf = LogisticRegression(penalty='l1', solver='liblinear')\nclf = make_pipeline(\n    MinMaxScaler(),  # Scale our features to be within 0..1\n    SelectFromModel(inner_clf),  # Remove unnecessary features (mitigating risk of overfitting)\n    inner_clf,  # Apply the classifier per se\n)\nclf.fit(X_train, y_train)\n\nscores = cross_val_score(clf, X_test, y_test, cv=10)\nprint('Accuracy score: ~{:.2f}%'.format(np.mean(scores) * 100))","e6e3f4c8":"# 80% accuracy in a dead-simple, fair and non-overfitting model\n\n### If you like it, upvote it.  =]"}}