{"cell_type":{"042ed243":"code","bb90cf8d":"code","8e02e5b5":"code","9a1e40b8":"code","8f6f9bdc":"code","3f989915":"code","bd50b8b8":"code","19854b96":"code","326b9b27":"code","13e96073":"code","1ef1e3c6":"code","3488cbc6":"code","2d392283":"code","cba7cc55":"code","51a8a97c":"code","0097c383":"code","ac0704d6":"markdown","de4457fc":"markdown","45187549":"markdown","1551ce85":"markdown","b20327ac":"markdown","1b7cba8b":"markdown","61e6ad97":"markdown","01278314":"markdown"},"source":{"042ed243":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n## Importing sklearn packages!!\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.svm import SVC\nfrom sklearn.datasets import load_iris\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score,recall_score, confusion_matrix\nfrom sklearn.model_selection import GridSearchCV,RandomizedSearchCV\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn import preprocessing\nfrom sklearn import decomposition\nfrom sklearn.decomposition import PCA","bb90cf8d":"iris = load_iris()","8e02e5b5":"iris.data","9a1e40b8":"dir(iris)","8f6f9bdc":"df = pd.DataFrame(iris.data,columns=iris.feature_names,)\ndf.head()","3f989915":"df['target'] = iris.target\ndf.head()","bd50b8b8":"X = df.iloc[:,:-1].values###Features \ny = df.iloc[:,-1].values## labels\nX","19854b96":"X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3,random_state=0)","326b9b27":"pipelines_dt = Pipeline([('scaler1',StandardScaler()),\n                         ('pca1',PCA(n_components=2)),\n                         ('dt_classifier',DecisionTreeClassifier())])","13e96073":"pipelines_rfc = Pipeline([('scaler2',StandardScaler()),\n                          ('pca2',PCA(n_components=2)),\n                          ('rf_classifier',RandomForestClassifier())])","1ef1e3c6":"pipelines = [pipelines_dt,pipelines_rfc]","3488cbc6":"best_accuracy=0.0\nbest_classifier=0\nbest_pipeline=\"\"","2d392283":"pipe_dict={0:'Dicision Tree',1:'Random Forest'}\n\n# Fit the pipelines\nfor pipe in pipelines: \n\tpipe.fit(X_train, y_train)\n","cba7cc55":"## You can implement the other Classifier as you can see that both give same accuracy but after tuning you can get better result from any of the classifier!!\nfor i,model in enumerate(pipelines):\n  print(\"{} Test accuracy is {}\".format(pipe_dict[i],model.score(X_test,y_test)))\n","51a8a97c":"for i,model in enumerate(pipelines):\n    if model.score(X_test,y_test)>best_accuracy:\n        best_accuracy=model.score(X_test,y_test)\n        best_pipeline=model\n        best_classifier=i\nprint('Classifier with best accuracy:{}'.format(pipe_dict[best_classifier]))","0097c383":" pipe = Pipeline([('classifier',RandomForestClassifier())])\n## You can also the other classifier like by creating another dictionary inside grid_params.\ngrid_params = [{'classifier':[RandomForestClassifier()],\n                'classifier__n_estimators' : [10,20,30,40,50,60,70,80],\n                'classifier__criterion' : [\"gini\",\"entropy\"],\n                'classifier__max_depth' : [2,4,6,8,10],\n                'classifier__min_samples_split':[5,4,6,7,8],\n                'classifier__max_features':['auto', 'sqrt', 'log2']\n                }]\n\ngrid_search = GridSearchCV(pipe, grid_params, cv=5, verbose=0,n_jobs=-1,scoring=\"accuracy\")\n\ngrid_search.fit(X_train,y_train)\nbest_accuracy = grid_search.best_score_\nbest_parameters = grid_search.best_params_\nprint(\"Best Accuracy: {:.2f} %\".format(best_accuracy*100))\nprint(\"Best Parameters:\", best_parameters)","ac0704d6":"### Tuning our classifier using Pipelines","de4457fc":"### Creating the pipelines for differnt classifier's\n1. Fit and Transform data using Standard Scaler\n2. Dimension Reduction using PCA\n3. Classifier","45187549":" ## Upvote if you like my Notebook!! Stay Tuned\n \n ### Further I will be adding more best and optimize techniques for params tuning:\n \n * Part 2: Optuna\n \n * Part 3: Hyperopt","1551ce85":"## Hyperparameter Tuning Using Pipeline!!!","b20327ac":"![pipe.webp](attachment:pipe.webp)","1b7cba8b":"### **In this notebook I have written about one of the way to perform params tuning using Pipeline and GridSearchCV , using pipelines you can test different model accuracy with a clean and maintainable code.**","61e6ad97":"### Loading And Preprocessing the Data","01278314":"## Part 1 : Hyperparameter tuning using Sklearn Pipelines"}}