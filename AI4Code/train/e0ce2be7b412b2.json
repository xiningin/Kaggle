{"cell_type":{"36f1bc4d":"code","bbeca87f":"code","0a46b4de":"code","15a8608e":"code","fa85a0f1":"code","1dc88951":"code","f7b32564":"code","517409a9":"code","71b2759b":"code","46cd9b0b":"code","4a5c895d":"code","c3fd3d1c":"code","4a0ef909":"code","efb19549":"code","da2201b6":"markdown","31ae3182":"markdown","922578d2":"markdown","594d853b":"markdown","e7d39a2a":"markdown","953f833d":"markdown","ab5bd887":"markdown"},"source":{"36f1bc4d":"import numpy as np\nimport pandas as pd\npd.set_option('max_columns', None)\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\nimport tensorflow as tf\n\nfrom sklearn.metrics import confusion_matrix, classification_report","bbeca87f":"data = pd.read_csv('..\/input\/apartment-rental-offers-in-germany\/immo_data.csv')","0a46b4de":"data","15a8608e":"data.info()","fa85a0f1":"def onehot_encode(df, column):\n    df = df.copy()\n    dummies = pd.get_dummies(df[column], prefix=column)\n    df = pd.concat([df, dummies], axis=1)\n    df = df.drop(column, axis=1)\n    return df","1dc88951":"def preprocess_inputs(df):\n    df = df.copy()\n    \n    # Drop high-cardinality categorical columns\n    df = df.drop(['houseNumber', 'street', 'streetPlain', 'regio3', 'description', 'facilities'], axis=1)\n    \n    # Drop columns with more than 25% missing values\n    missing_value_columns = df.loc[:, df.isna().mean() > 0.25].columns\n    df = df.drop(missing_value_columns, axis=1)\n    \n    # Drop examples with missing label values\n    missing_label_rows = df.loc[df['typeOfFlat'].isna(), :].index\n    df = df.drop(missing_label_rows, axis=0).reset_index(drop=True)\n    \n    # Construct label column\n    df['isApartment'] = df['typeOfFlat'].apply(lambda x: 1 if x == 'apartment' else 0)\n    df = df.drop('typeOfFlat', axis=1)\n    \n    # Get columns with remaining missing values\n    remaining_na_columns = df.loc[:, df.isna().sum() > 0]\n    categorical_na_columns = remaining_na_columns.select_dtypes('object').columns\n    numeric_na_columns = remaining_na_columns.drop(categorical_na_columns, axis=1).columns\n    \n    # Fill numeric missing values with column mean\n    for column in numeric_na_columns:\n        df[column] = df[column].fillna(df[column].mean())\n    \n    # Fill categorical missing values with \"missing\"\n    for column in categorical_na_columns:\n        df[column] = df[column].fillna(\"missing\")\n    \n    # Convert booleans columns to int columns\n    for column in df.columns:\n        if df[column].dtype == 'bool':\n            df[column] = df[column].astype(np.int)\n    \n    # Extract date features\n    df['date'] = pd.to_datetime(df['date'], format='%b%y')\n    df['year'] = df['date'].apply(lambda x: x.year)\n    df['month'] = df['date'].apply(lambda x: x.month)\n    df = df.drop('date', axis=1)\n    \n    # One-hot encode\n    for column in df.select_dtypes('object'):\n        df = onehot_encode(df, column)\n    \n    # Split df into X and y\n    y = df['isApartment']\n    X = df.drop('isApartment', axis=1)\n    \n    # Train-test split\n    X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, shuffle=True, random_state=1)\n    \n    # Scale X\n    scaler = StandardScaler()\n    scaler.fit(X_train)\n    X_train = pd.DataFrame(scaler.transform(X_train), columns=X.columns)\n    X_test = pd.DataFrame(scaler.transform(X_test), columns=X.columns)\n    \n    return X_train, X_test, y_train, y_test","f7b32564":"def evaluate_model(model, X_test, y_test):\n    \n    results = model.evaluate(X_test, y_test, verbose=0)\n    print(\"    Test Loss: {:.4f}\".format(results[0]))\n    print(\"Test Accuracy: {:.2f}%\".format(results[1] * 100))\n    print(\"     Test AUC: {:.4f}\".format(results[2]))\n    \n    y_pred = np.squeeze(np.array(model.predict(X_test) >= 0.5, dtype=np.int))\n    \n    cm = confusion_matrix(y_test, y_pred)\n    clr = classification_report(y_test, y_pred, target_names=[\"NOT APARTMENT\", \"APARTMENT\"])\n    \n    plt.figure(figsize=(6, 6))\n    sns.heatmap(cm, annot=True, fmt='g', cmap='Blues', cbar=False)\n    plt.xticks(np.arange(2) + 0.5, [\"NOT APARTMENT\", \"APARTMENT\"])\n    plt.yticks(np.arange(2) + 0.5, [\"NOT APARTMENT\", \"APARTMENT\"])\n    plt.xlabel(\"Predicted\")\n    plt.ylabel(\"Actual\")\n    plt.title(\"Confusion Matrix\")\n    plt.show()\n    \n    print(\"Classification Report:\\n----------------------\\n\", clr)","517409a9":"X_train, X_test, y_train, y_test = preprocess_inputs(data)","71b2759b":"X_train","46cd9b0b":"y_train","4a5c895d":"inputs = tf.keras.Input(shape=(X_train.shape[1],))\nx = tf.keras.layers.Dense(128, activation='relu')(inputs)\nx = tf.keras.layers.Dense(128, activation='relu')(x)\noutputs = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n\nmodel = tf.keras.Model(inputs=inputs, outputs=outputs)\n\nprint(model.summary())","c3fd3d1c":"model.compile(\n    optimizer='adam',\n    loss='binary_crossentropy',\n    metrics=[\n        'accuracy',\n        tf.keras.metrics.AUC(name='auc')\n    ]\n)\n\nhistory = model.fit(\n    X_train,\n    y_train,\n    validation_split=0.2,\n    batch_size=32,\n    epochs=100,\n    callbacks=[\n        tf.keras.callbacks.EarlyStopping(\n            monitor='val_loss',\n            patience=5,\n            restore_best_weights=True\n        )\n    ]\n)","4a0ef909":"epochs_range = range(len(history.history['loss']))\n\nplt.figure(figsize=(16, 10))\nplt.plot(epochs_range, history.history['loss'], label=\"Training Loss\")\nplt.plot(epochs_range, history.history['val_loss'], label=\"Validation Loss\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\nplt.title(\"Training and Validation Loss Over Time\")\nplt.legend()\nplt.show()","efb19549":"evaluate_model(model, X_test, y_test)","da2201b6":"# Preprocessing","31ae3182":"# Helper Functions","922578d2":"# Training","594d853b":"# Getting Started","e7d39a2a":"# Task for Today  \n\n***\n\n## Apartment Type Prediction  \n  \nGiven *data about home rentals in Germany*, let's try to predict if a given home is **an apartment** or not.  \n  \nWe will use a TensorFlow\/Keras neural network to make our predictions.","953f833d":"# Data Every Day  \n\nThis notebook is featured on Data Every Day, a YouTube series where I train models on a new dataset each day.  \n\n***\n\nCheck it out!  \nhttps:\/\/youtu.be\/bX6A5S7V_e4","ab5bd887":"# Results"}}