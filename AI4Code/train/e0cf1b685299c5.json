{"cell_type":{"126c1f1d":"code","5497105e":"code","a2bfab9f":"code","b7bae555":"code","417c2e0d":"code","8e9b49cb":"code","7f6a3910":"code","ee436130":"code","901b901f":"code","d3142d11":"code","7945569f":"code","10e9b97b":"code","517b41e3":"code","917fb4aa":"code","ccd494c9":"code","aaaf4055":"code","8ed84f90":"code","bdc17c34":"code","f2d9834e":"code","d534ec6e":"code","905bd525":"code","f089ac20":"code","06d8857c":"code","6af9c505":"code","162d4e96":"code","ab15c266":"code","d8a3b505":"code","68c30d65":"code","078c9a0a":"code","3054b31d":"code","9ab374e4":"code","012f0119":"code","3fd7f479":"code","b4ede414":"code","8521da09":"code","cbb1b167":"code","9b8bba46":"code","993cb966":"code","e072ae56":"code","97658f5b":"code","3af7df62":"code","465b2623":"code","d8f88de3":"code","6aed9a60":"code","e1593acf":"code","79d150bd":"code","ef75b5e8":"code","44d2d406":"code","7297c906":"code","f1901793":"code","3863cda2":"code","b3a87d02":"markdown","e43f7063":"markdown","3a277f04":"markdown","8c2cbcbd":"markdown","0f5993e1":"markdown","a504ec89":"markdown","7ec36a65":"markdown","7f5f044f":"markdown","733cc8a2":"markdown","cf297db0":"markdown","8c471204":"markdown","6a7c1af9":"markdown","80fac77a":"markdown","8adbe874":"markdown","84c3674c":"markdown","d76dcf71":"markdown","70e433a2":"markdown","927c9eb6":"markdown","bc13d27d":"markdown","591177c0":"markdown","7213f204":"markdown","c3370512":"markdown","fa68891a":"markdown","b03a0924":"markdown","0438768d":"markdown","eacd7ef0":"markdown","64bbfa13":"markdown","a4381dc6":"markdown","23a27df9":"markdown","5ba5e975":"markdown","064d132f":"markdown","45feae5f":"markdown"},"source":{"126c1f1d":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport re\nimport string\nimport missingno as msno\nimport nltk\n\nimport warnings   #Warnings\nwarnings.filterwarnings(\"ignore\")\n\nfrom wordcloud import WordCloud           \nfrom itertools import chain\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import roc_curve, auc\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.model_selection import GridSearchCV\nfrom nltk.stem.snowball import SnowballStemmer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n%matplotlib inline \n\nprint(\"Libraries Imported\")","5497105e":"data=pd.read_csv(\"..\/input\/twitter-60k\/twitter.csv\",encoding='latin-1' ) \ndata.head(5)","a2bfab9f":"print(data.info())\nprint(\"-\"*80)\nprint(\"The Dataset contains class level count as\\n\", data[\"class_label\"].value_counts())","b7bae555":"data['class_label'] = data['class_label'].replace([4],1)","417c2e0d":"#Checking for Null values \nprint(msno.matrix(data))\nprint(data.isnull().sum())","8e9b49cb":"at=data[data['tweet'].str.contains(\"@\")]\nprint(len(data.tweet))\nprint(len(data.tweet.unique()))","7f6a3910":"data.drop_duplicates(subset=['tweet', 'userid'],keep = False, inplace = True)\n","ee436130":"for index,text in enumerate(data['tweet'][18:20]):\n  print('tweet %d:\\n'%(index+1),text)","901b901f":"def rem(i):\n    j=\" \".join(filter(lambda x:x[0]!='@', i.split())) #removing words starting with '@'\n    j1=re.sub(r\"http\\S+\", \"\", j)                       # removing urls\n    j2=\" \".join(filter(lambda x:x[0]!='&', j1.split()))  #the \"&\" was found to be used in many misspelled words and usernames\n    return j2\n       ","d3142d11":"data[\"tweet\"]=data[\"tweet\"].apply(rem) #applying the function","7945569f":"for index,text in enumerate(data['tweet'][18:20]):\n  print('tweet %d:\\n'%(index+1),text)","10e9b97b":"def smiley(a):\n    x1=a.replace(\":\u2011)\",\"happy\")\n    x2=x1.replace(\";)\",\"happy\")\n    x3=x2.replace(\":-}\",\"happy\")\n    x4=x3.replace(\":)\",\"happy\")\n    x5=x4.replace(\":}\",\"happy\")\n    x6=x5.replace(\"=]\",\"happy\")\n    x7=x6.replace(\"=)\",\"happy\")\n    x8=x7.replace(\":D\",\"happy\")\n    x9=x8.replace(\"xD\",\"happy\")\n    x10=x9.replace(\"XD\",\"happy\")\n    x11=x10.replace(\":\u2011(\",\"sad\")    #using 'replace' to convert emoticons\n    x12=x11.replace(\":\u2011[\",\"sad\")\n    x13=x12.replace(\":(\",\"sad\")\n    x14=x13.replace(\"=(\",\"sad\")\n    x15=x14.replace(\"=\/\",\"sad\")\n    x16=x15.replace(\":[\",\"sad\")\n    x17=x16.replace(\":{\",\"sad\")\n  \n    x18=x17.replace(\":P\",\"playful\")\n    x19=x18.replace(\"XP\",\"playful\")\n    x20=x19.replace(\"xp\",\"playful\")\n  \n    \n    x21=x20.replace(\"<3\",\"love\")\n    x22=x21.replace(\":o\",\"shock\")\n    x23=x22.replace(\":-\/\",\"sad\")\n    x24=x23.replace(\":\/\",\"sad\")\n    x25=x24.replace(\":|\",\"sad\")\n    return x25","517b41e3":"data['tweet']=data['tweet'].apply(smiley)","917fb4aa":"data['tweet']=data['tweet'].apply(lambda x: x.lower())","ccd494c9":"def decontracted(phrase):\n    # specific\n    phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n\n    # general\n    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n    phrase = re.sub(r\"\\'s\", \" is\", phrase)   #using regular expressions to expand the contractions\n    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n    return phrase","aaaf4055":"data['tweet']=data['tweet'].apply(decontracted)","8ed84f90":"data['tweet']=data['tweet'].apply(lambda x: re.sub('\\w*\\d\\w*','', x))","bdc17c34":"data['tweet']=data['tweet'].apply(lambda x: re.sub('[%s]' % re.escape(string.punctuation), '', x))","f2d9834e":"def odd(a):\n    words = ['wi','ame','quot','ti','im']\n    querywords = a.split()\n\n    resultwords  = [word for word in querywords if word.lower() not in words]\n    result = ' '.join(resultwords)\n    return result","d534ec6e":"data[\"tweet\"]=data[\"tweet\"].apply(odd)","905bd525":"data[\"tweet\"]=data[\"tweet\"].apply(lambda x: re.sub(' +', ' ', x))","f089ac20":"stop=[\"i\", \"me\", \"my\", \"myself\", \"we\", \"our\",\"will\", \"go\",\"got\",\"ours\", \"ourselves\", \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\", \"herself\", \"it\", \"its\", \"itself\", \"they\", \"them\", \"their\", \"theirs\", \"themselves\", \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\", \"these\", \"those\", \"am\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \"having\", \"do\", \"does\", \"did\", \"doing\", \"a\", \"an\", \"the\", \"and\", \"but\", \"if\", \"or\", \"because\", \"as\", \"until\", \"while\", \"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \"against\", \"between\", \"into\", \"through\", \"during\", \"before\", \"after\", \"above\", \"below\", \"to\", \"from\", \"up\", \"down\", \"in\", \"out\", \"on\", \"off\", \"over\", \"under\", \"again\", \"further\", \"then\", \"once\", \"here\", \"there\", \"when\", \"where\", \"why\", \"how\", \"all\", \"any\", \"both\", \"each\", \"few\", \"more\", \"most\", \"other\", \"some\", \"such\",  \"nor\", \"only\", \"own\", \"same\", \"so\", \"than\", \"too\", \"very\", \"s\", \"t\", \"just\", \"don\", \"should\", \"now\"]","06d8857c":"print(stop)","6af9c505":"data[\"tweet\"]=data['tweet'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))","162d4e96":"data[\"tweet\"]=data[\"tweet\"].apply(lambda x: re.sub(' +', ' ', x))","ab15c266":"data[\"word_count\"] = data['tweet'].apply(lambda x: len(str(x).split()))","d8a3b505":"sns.distplot(data.word_count, kde=False, rug=True)","68c30d65":"data.drop(\"word_count\", axis=1, inplace=True)","078c9a0a":"pos = data[data.class_label==1]\ncloud= (' '.join(pos['tweet']))\nwcloud = WordCloud(width = 1000, height = 500).generate(cloud)\nplt.figure(figsize=(15,5))\nplt.imshow(wcloud)\nplt.axis('off')","3054b31d":"pos = data[data.class_label==0]\ncloud= (' '.join(pos['tweet']))\nwcloud = WordCloud(width = 1000, height = 500).generate(cloud)\nplt.figure(figsize=(15,5))\nplt.imshow(wcloud)\nplt.axis('off')","9ab374e4":"data[\"word_count\"] = data['tweet'].apply(lambda x: len(str(x).split()))","012f0119":"X=data.drop(['class_label',\"id\",\"date\",\"flag\",\"userid\"], axis = 1)  # seperating the class label\ny=data[\"class_label\"].values","3fd7f479":"X.head()","b4ede414":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42)","8521da09":"#Vectorizing\nvectorizer = TfidfVectorizer() #Using TFIDF to vectorize the text","cbb1b167":"vectorizer = TfidfVectorizer(min_df=10,ngram_range=(1,3)) \nvectorizer.fit(X_train['tweet'].values)           # Training the TFIDF model\nx_tr=vectorizer.transform(X_train['tweet'].values)\nx_te=vectorizer.transform(X_test['tweet'].values)","9b8bba46":"x_tr.shape   #We get 4191 features after vectorizing ","993cb966":"model = MultinomialNB()  \nparameters = {'alpha':[0.00001, 0.00005, 0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1, 5,\n10, 50, 100]}\nclf = GridSearchCV(model, parameters, cv=10,scoring='roc_auc',return_train_score=True)\nclf.fit(x_tr, y_train)","e072ae56":"results = pd.DataFrame.from_dict(clf.cv_results_)   # converting the results in to a dataframe\nresults = results.sort_values(['param_alpha'])  \nresults.head()","97658f5b":"train_auc= results['mean_train_score'].values  #extracting the auc scores \ncv_auc = results['mean_test_score'].values","3af7df62":"a1=[]\nfor i in parameters.values():\n    a1.append(i)\nalphas = list(chain.from_iterable(a1))","465b2623":"plt.plot(alphas, train_auc, label='Train AUC')\nplt.plot(alphas, cv_auc, label='CV AUC')\nplt.scatter(alphas, train_auc, label='Train AUC points')\nplt.scatter(alphas, cv_auc, label='CV AUC points')\n\nplt.legend()\nplt.xlabel(\"Alpha: hyperparameter\")\nplt.ylabel(\"AUC\")\nplt.title(\"Hyper parameter Vs AUC plot\")  \nplt.grid()\nplt.show()","d8f88de3":"bestparam=clf.best_params_['alpha']   #extracting the best hyperparameter\nprint(\"The best Alpha=\",bestparam)","6aed9a60":"mul_model = MultinomialNB(alpha=bestparam) #Building a Naive Bayes model with the best alpha\nmul_model.fit(x_tr,y_train)               #Training the model","e1593acf":"y_train_pred = mul_model.predict_proba(x_tr)[:,1]  #Prediction using the model(log probability of each class)\ny_test_pred = mul_model.predict_proba(x_te)[:,1]\ntrain_fpr, train_tpr, tr_thresholds = roc_curve(y_train, y_train_pred)\ntest_fpr, test_tpr, te_thresholds = roc_curve(y_test, y_test_pred)   \nplt.plot(train_fpr, train_tpr, label=\"train AUC =\"+str(auc(train_fpr, train_tpr)))\nplt.plot(test_fpr, test_tpr, label=\"test AUC =\"+str(auc(test_fpr, test_tpr)))\nplt.legend()\nplt.title(\"AUC PLOTS\")             #Plotting train and test AUC \nplt.grid()\nplt.show()","79d150bd":"trauc=round(auc(train_fpr, train_tpr),3)\nteauc=round(auc(test_fpr, test_tpr),3)\nprint('Train AUC=',trauc)\nprint('Test AUC=',teauc)\n","ef75b5e8":"def find_best_threshold(threshould, fpr, tpr):\n    t = threshould[np.argmax(tpr*(1-fpr))]      #finding the best threashold \n    print(\"the maximum value of tpr*(1-fpr)\", max(tpr*(1-fpr)), \"for threshold\", np.round(t,3))\n    return t\n\ndef predict_with_best_t(proba, threshould):\n    predictions = []\n    for i in proba:\n        if i>=threshould:\n            predictions.append(1)\n        else:                                 #building a confusion matrix with the best threashold \n            predictions.append(0)\n    return predictions","44d2d406":"best_t = find_best_threshold(tr_thresholds, train_fpr, train_tpr)\nTRCM=confusion_matrix(y_train, predict_with_best_t(y_train_pred, best_t))\nTECM=confusion_matrix(y_test, predict_with_best_t(y_test_pred, best_t))","7297c906":"def CM(x,y):\n    labels = ['TN','FP','FN','TP']\n    group_counts = [\"{0:0.0f}\".format(value) for value in x.flatten()]\n                    \n    labels = [f\"{v1}\\n{v2}\" for v1, v2 in\n    zip(labels,group_counts)]\n    labels = np.asarray(labels).reshape(2,2)       #Building a design for the confusion matrix\n    sns.heatmap(x, annot=labels, fmt='', cmap='BuPu')\n    plt.xlabel(\"Predicted\")\n    plt.ylabel(\"Actual\")\n    plt.title(y)\n    plt.plot()","f1901793":"CM(TRCM,'Train Confusion Matrix')","3863cda2":"CM(TECM,'Test Confusion Matrix')","b3a87d02":"### Remove usernames and URL's","e43f7063":"### Cleaning other oddities that were found","3a277f04":"### Converting common emoticons to their appropriate sentiment","8c2cbcbd":"- Using Naive bayes as a base model for the Gridsearch .\n- Gridsearch is performed to find the best hyperparameter i.e Alpha","0f5993e1":"- The above word cloud gives the words that are present in the negative tweets\n- Bigger the word ,more times it appears in the negative tweets","a504ec89":"- By using the naive bayes model we were able to get an AUC score >0.8\n- Building a confusion matrix gave a better understanding of the performance \n- There were more True positives and True negatives which means more points were classified properly \n- Hence, our model worked fairly well and is ready for deployment\n- Model is being deployed using **Streamlit** on **AWS EC2 Instance** [Link](http:\/\/)","7ec36a65":"- The  plot gives the distribution of length of words in a tweet\n- There are more tweets around the length 6\n- And the least are of length 30+","7f5f044f":"### Converting all the tweets to lower case ","733cc8a2":"- We have more true positives and true negatives\n- which means more points were classified correctly\n- The same can be observed in the test confusion matrix","cf297db0":"* Source of Data: [Data Link](https:\/\/www.kaggle.com\/kazanova\/sentiment140)\n* It contains **1,600,000 tweets** extracted using the twitter api\n\n**It contains the following 6 fields:**\n* target: the polarity of the tweet (0 = negative, 2 = neutral, 4 = positive)\n* ids: The id of the tweet ( 2087)\n* date: the date of the tweet (Sat May 16 23:58:44 UTC 2009)\n* flag: The query (lyx). If there is no query, then this value is NO_QUERY.\n* user: the user that tweeted (robotickilldozr)\n* text: the text of the tweet (Lyx is cool)\n \nGithub Link: [Github](https:\/\/github.com\/Sanjay-Suthraye\/twitter_sentiment_analysis)  |  Streamlit Link : [Streamlit](http:\/\/52.15.124.164:8501\/)\n","8c471204":"### Removing stopwords","6a7c1af9":"## Null values and Duplicates","80fac77a":"## Confusion Matrix","8adbe874":"## Importing the Libraries","84c3674c":"- The above word cloud gives the words that are present in the positive tweets\n- Bigger the word ,more times it appears in the positive tweets","d76dcf71":"### Hyperparameter tuning & Cross Validation","70e433a2":"### Modelling","927c9eb6":"# *Twitter Sentiment Analysis* ","bc13d27d":"- We have an auc score for both test and train >0.8\n- This means our model has performed well\n- And since the difference between train and test scores is very little , the model is not overfitting","591177c0":"* Changing **4 to 1** to make it easy for modelling \n* Dataset contains 32000 negative tweets and 29830 positive tweets\n* 0 being negative and 1 being positive\n* There are 6 features in total, this notebook only considers the tweet for Analysis","7213f204":"## Data upload and Analysis\n\n**Note:** The data is Huge to compute so, for the distributed Class label, data is reduced already from **1,600,000 rows to 60,000**","c3370512":"- Plotting a graph between the hyperparameter and the AUC(Train,CV)\n- From the graph , the best hyperparameter could be from **3 to 5**\n- Lets confirm this by extracting the best alpha","fa68891a":"## Data cleaning\n* Removing Usernames\n* Removing URL's\n* Converting Emoticons\n* Expanding Contractions\n* Lower Case\n* Removing Digits\n* Removing extra punctuations\n* Removing Extra Spaces\n* Removing Stop words","b03a0924":"### Removing Digits","0438768d":"### Removing extra spaces ","eacd7ef0":"## Splitting and Transformation","64bbfa13":"![head_image.png](attachment:head_image.png)\n[Source](https:\/\/www.google.com\/url?sa=i&url=https%3A%2F%2Ftowardsdatascience.com%2Ftwitter-sentiment-analysis-classification-using-nltk-python-fa912578614c&psig=AOvVaw1Psxyvku6YtgL9RfKfJjVV&ust=1597049069037000&source=images&cd=vfe&ved=0CAIQjRxqFwoTCMC-ysbdjesCFQAAAAAdAAAAABAQ)","a4381dc6":"### Expanding Contractions","23a27df9":"### Remove punctuations","5ba5e975":"## Visualization","064d132f":"### Conclusion","45feae5f":"- Dataset contains no null values\n- Dataset contains 209 duplicates\n- Removing the duplicates"}}