{"cell_type":{"f4327a7f":"code","e1a45974":"code","c6202c65":"code","ffa1b3b3":"code","11480047":"code","b6495dcc":"code","53ce0aa9":"code","8b3c17ff":"code","c16f6eb4":"code","5eaf1ac4":"code","42286418":"code","7b13171f":"code","bdcb0b8c":"markdown","8384c32e":"markdown","7f2a7ad3":"markdown"},"source":{"f4327a7f":"import numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\n%matplotlib inline \n\nimport cv2\n\nimport os","e1a45974":"os.listdir(\"..\/input\")","c6202c65":"from fastai.imports import *\nfrom fastai.transforms import *\nfrom fastai.conv_learner import *\nfrom fastai.model import *\nfrom fastai.dataset import *\nfrom fastai.sgdr import *\nfrom fastai.plots import *","ffa1b3b3":"PATH = \"..\/input\/\"\nTMP_PATH = \"\/tmp\/tmp\"\nMODEL_PATH = \"\/tmp\/model\/\"\nsz=224","11480047":"fnames = np.array([f'train\/{f}' for f in sorted(os.listdir(f'{PATH}train'))])\nlabels = np.array([(0 if 'cat' in fname else 1) for fname in fnames])","b6495dcc":"arch=resnet50\ndata = ImageClassifierData.from_names_and_array(\n    path=PATH, \n    fnames=fnames, \n    y=labels, \n    classes=['dogs', 'cats'], \n    test_name='test', \n    tfms=tfms_from_model(arch, sz)\n)\nlearn = ConvLearner.pretrained(arch, data, precompute=True, tmp_name=TMP_PATH, models_name=MODEL_PATH)","53ce0aa9":"###\n### Search for suitable, i.e., best Learning Rate for our-newly-added-Last Layer (as we have used 'precompute=True', i.e., ResNet50-minus-its-last-layer weights are being re-used as is)\n###\n#lrf=learn.lr_find()\n#learn.sched.plot_lr()\n#learn.sched.plot()\n\n###\n### Use the identified best Learning Rate for our-newly-added-Last Layer\n### Note that even without running above 3 lines of Learning Rate Finder, it is well known that best learning rate is 0.01 for Cats & Dogs images with 224x224 size\n### Kaggle Score obtained is 0.38683 (v7)\n###\n#learn.fit(0.01, 2)","8b3c17ff":"###\n### SGDR (SGD with warm Resrart): fast.ai uses half Cosine shape decay (start with 0.01 & decay till 0) of LR during each epoch and then it restarts with 1e-02\n### Kaggle score obtained is 0.37578 (v8)\n###\nlearn.fit(1e-2, 10, cycle_len=1)\nlearn.sched.plot_lr()","c16f6eb4":"###\n### Continue from Last Layer learned model with PreCompute=TRUE\n### Unfreeze all layers (all weights learned so far are retained) => it sets PreCompute=FALSE making all layers learnable\n### Effectively, the network weights are intialized as (ResNet-minus-last-layer with its original pre-trained weight & Last Layer as per above model learning while keeping ResNet as frozen)\n### Now, all layers are FURTHER learnable\n### Kaggle score obtained is 0.34815 (v9)\n###\nlearn.unfreeze()\n\n# Differential LR (above identified best LR for last layer, x0.1 to middle layer, x0.01 to inner layer)\nlr=np.array([1e-4,1e-3,1e-2])\n\nlearn.fit(lr, 3, cycle_len=1, cycle_mult=2)\n\nlearn.sched.plot_lr()","5eaf1ac4":"temp = learn.predict(is_test=True)\npred = np.argmax(temp, axis=1)","42286418":"import cv2\n\n# learn.predict works on unsorted os.listdir, hence listing filenames without sorting\nfnames_test = np.array([f'test\/{f}' for f in os.listdir(f'{PATH}test')])\n\nf, ax = plt.subplots(5, 5, figsize = (15, 15))\n\nfor i in range(0,25):\n    imgBGR = cv2.imread(f'{PATH}{fnames_test[i]}')\n    imgRGB = cv2.cvtColor(imgBGR, cv2.COLOR_BGR2RGB)\n    \n    # a if condition else b\n    predicted_class = \"Dog\" if pred[i] else \"Cat\"\n\n    ax[i\/\/5, i%5].imshow(imgRGB)\n    ax[i\/\/5, i%5].axis('off')\n    ax[i\/\/5, i%5].set_title(\"Predicted:{}\".format(predicted_class))    \n\nplt.show()","7b13171f":"results_df = pd.DataFrame(\n    {\n        'id': pd.Series(fnames_test), \n        'label': pd.Series(pred)\n    })\nresults_df['id'] = results_df.id.str.extract('(\\d+)')\nresults_df['id'] = pd.to_numeric(results_df['id'], errors = 'coerce')\nresults_df.sort_values(by='id', inplace = True)\n\nresults_df.to_csv('submission.csv', index=False)\nresults_df.head()","bdcb0b8c":"### References\n\n1. [Kaggle kernel about implementing Cats & Dogs images classification in Keras](https:\/\/www.kaggle.com\/suniliitb96\/tutorial-keras-transfer-learning-with-resnet50)","8384c32e":"## Fast.ai Learning through Cats & Dogs Image Classification\n\n#### Sunil Kumar","7f2a7ad3":"This kernel can be used to submit to Cats & Dogs Kaggle competition. One can try 1st option for training just the last layer, i,e., Dense FC layer while re-using pretrained ResNet* layers. Other option is to try SGDR. Then another approach given below is to try SGDR with Differential Learning Rate. Observe the validation loss, validation accuracy and Kaggle score. NOTE that try this with GPU ON and Internet CONNECTED in your Kaggle kernel. One can try fast.ai Images Augmentation while preparing the ConvNet model before model learning and it is one of the ways of better generalizing our deep learning - refer to https:\/\/becominghuman.ai\/data-augmentation-using-fastai-aefa88ca03f1 .\n\nEven before moving to lectures beyond Lesson #1, I got curious to explore learning performance with various options given in fast.ai Lesson #1 notebook and explored if all fast.ai tricks can be implemented in Keras & TensorFlow. Refer to tutorial on [Tutorial Keras: Transfer Learning with ResNet50 for image classification on Cats & Dogs dataset](https:\/\/www.kaggle.com\/suniliitb96\/tutorial-keras-transfer-learning-with-resnet50).\n\nNotes about special observations noticed while practicing with fast.ai on Cats & Dogs dataset from an old Kaggle competition with reference to fast.ai Lesson#1: - \n* Refer to http:\/\/ruder.io\/deep-learning-optimization-2017\/ for a comprehensive practical background in Deep Learning Optimizations with respect to Optimizers, Learning Rate schedule, etc.\n* Refer to SGDR in https:\/\/abdel.me\/2018\/01\/05\/fastai-part1\/ for a better understanding as what fast.ai is doing under the hood.\n* SGDR\n    * Refer to the below plot of learning schedule for cycle_len=1 (notice iteration count as ~313, i.e., 10k\/32)\n    * It is based on the concept of Cyclical Learning, a.k.a., Learning Rate Annealing with Warm Restart. SGD with Learning Rate Annealing schedule is a common practice to compete with adaptive Adam optimizer.\n    * Learning Rate decays as per half cosine with each Epoch Step (ref model.py >> step fn) and it restarts with specified Learning Rate value\n*  Differential Learning Rate for Fine Tuning PreTrained ResNet*    \n    * It is not enough to just train and re-train the Dense FC last layer while keeping ResNet layers frozen.\n    * We idenity best Learning Rate for Dense FC last layer and apply sclaed down Learning Rate for ResNet* grouped layers (Fast.ai seem to have treated ResNet* in 2 grouped layers & I'm yet to figure out as how this grouping has been done).\n    * SGDR + DLR + cyclt_mult affects in stretching Learning Rate Annealing to span across epochs. Refer to below plot for DLR.    \n    "}}