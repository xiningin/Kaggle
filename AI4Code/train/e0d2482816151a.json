{"cell_type":{"66bb4d67":"code","b5356f1e":"code","b509adeb":"code","d33d3743":"code","e80f34de":"code","34bbdc64":"code","321e2948":"code","7bc84c2a":"code","7cdad849":"code","2faa7b23":"code","e3e76f69":"code","6c6c2390":"code","8067b081":"code","5052d353":"code","66e4336b":"code","1cf65cdb":"code","0ff50b36":"code","7514b986":"code","402267d6":"code","3d1bb82e":"code","ced9bd5a":"code","2f29c1c5":"markdown","13b2a2e7":"markdown","bfaeae42":"markdown","946829ee":"markdown","cb0b0d5e":"markdown"},"source":{"66bb4d67":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","b5356f1e":"import re\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\nimport nltk\nnltk.download(\"stopwords\")\nnltk.download('punkt')\nnltk.download('wordnet')\nfrom nltk.corpus import stopwords\nfrom nltk.stem import \tWordNetLemmatizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix\nfrom keras.preprocessing.text import one_hot\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Model, load_model\nfrom keras.layers import LSTM, Activation, Dense, Dropout, Input, Embedding,Bidirectional\nfrom keras.optimizers import Adam\nfrom keras import layers\nfrom keras.models import Sequential\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n# nltk.download()","b509adeb":"train=pd.read_csv(r'\/kaggle\/input\/nlp-getting-started\/train.csv')\ntest=pd.read_csv(r'\/kaggle\/input\/nlp-getting-started\/test.csv')","d33d3743":"train.head()","e80f34de":"train[\"text\"]=train[\"text\"].str.lower()\ntest[\"text\"]=test[\"text\"].str.lower()\ntrain[\"keyword\"].fillna(\"\",inplace=True)\ntest[\"keyword\"].fillna(\"\",inplace=True)\ntrain[\"text\"]=train[\"text\"].str.lower()+\" \"+train[\"keyword\"]\ntest[\"text\"]=test[\"text\"].str.lower()+\" \"+test[\"keyword\"]","34bbdc64":"wordnet_lemmatizer = WordNetLemmatizer()\ndef cleaning(text):\n  text=text.replace(\"\\n\",\" \")\n  text = nltk.word_tokenize(text)\n  text= [re.sub('[()?.><$^''\\&%!]+', '', i) for i in text]\n  text=[re.sub('[\\\\:@#].*|[\\d]+',\"\",i) for i in text]\n  text=[i for i in text if i not in stopwords.words('english') and len(i)>3 and i.startswith((\"#\",\"@\",\"http\",\"\/\/\"))==False]#\"#\" not in i and \"@\" not in i and i.startswith(\"http:\/\/\")==False ]\n  text=[wordnet_lemmatizer.lemmatize(i) for i in text ]\n  text=[i.strip() for i in text]\n  return text","321e2948":"train[\"Clean_text\"]=[\" \".join(cleaning(i)) for i in train[\"text\"]]\ntest[\"Clean_text\"]=[\" \".join(cleaning(i)) for i in test[\"text\"]]","7bc84c2a":"train.head()","7cdad849":"test.head()","2faa7b23":"target_one=train[train[\"target\"]==1]\ntarget_zero=train[train[\"target\"]==0]","e3e76f69":"wc=WordCloud(background_color=\"white\",stopwords=STOPWORDS,max_words=500, width=600, height=600,random_state=1)\nwc.generate(\" \".join(target_one[\"Clean_text\"].tolist()))\nplt.figure(figsize=(10,15))\nplt.imshow(wc)","6c6c2390":"wc=WordCloud(background_color=\"white\",stopwords=STOPWORDS,max_words=500, width=600, height=600,random_state=1)\nwc.generate(\" \".join(target_zero[\"Clean_text\"].tolist()))\nplt.figure(figsize=(15,25))\nplt.imshow(wc)\n","8067b081":"voc_size = 50000\nsent_length = 30\nembedding_dim = 300\n\nX_train = [one_hot(words, voc_size) for words in train['Clean_text']]\nX_train = pad_sequences(X_train, padding='pre', maxlen=sent_length)\nY_train = train['target']\nX_test = [one_hot(words, voc_size) for words in test['Clean_text']]\nX_test = pad_sequences(X_test, padding='pre', maxlen=sent_length)\n\nprint(X_train.shape, Y_train.shape)\nprint(X_test.shape)","5052d353":"input_dim=X_train.shape[1]\ny_train=np.array(train[\"target\"].tolist())","66e4336b":"model = Sequential()\nmodel.add(layers.Dense(16, input_dim=input_dim, activation='relu'))\nmodel.add(layers.Dropout(.5))\nmodel.add(layers.Dense(16, input_dim=input_dim, activation='relu'))\nmodel.add(layers.Dropout(.2))\nmodel.add(layers.Dense(8, input_dim=input_dim, activation='relu'))\nmodel.add(layers.Dropout(.2))\nmodel.add(layers.Dense(4, input_dim=input_dim, activation='relu'))\nmodel.add(layers.Dense(1, activation='sigmoid'))\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel.summary()","1cf65cdb":"history = model.fit(X_train,y_train,epochs=50, verbose=False, batch_size=100, shuffle = True)","0ff50b36":"check_submission=pd.read_csv(r'\/kaggle\/input\/nlp-getting-started\/sample_submission.csv')","7514b986":"check_submission.head()","402267d6":"pred=model.predict(X_test)\ncheck_submission[\"target\"]=pred","3d1bb82e":"check_submission[check_submission[\"target\"]<0.5][\"target\"]=0\ncheck_submission[check_submission[\"target\"]>=0.5][\"target\"]=1\ncheck_submission[\"target\"]=check_submission['target'].astype(int)","ced9bd5a":"check_submission.to_csv('prediction.csv', index=False)\ncheck_submission.head()","2f29c1c5":"Prediction","13b2a2e7":"Import Libraries","bfaeae42":"Data cleaning","946829ee":"Deep Neural Network","cb0b0d5e":"Plot word cloud "}}