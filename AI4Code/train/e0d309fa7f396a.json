{"cell_type":{"f5cdf2cc":"code","f4eb1872":"code","41684140":"code","98e54616":"code","b12adf9b":"code","877dd3fb":"code","8ed1590e":"code","76e5ab35":"code","816e9a13":"code","5e02245a":"code","efa3980c":"code","454fda3f":"code","5a51f42b":"code","d92eac95":"code","3aaa39fd":"code","7e94a633":"code","b7b58c3d":"code","99d52cfb":"code","909d67d0":"code","4ab53fed":"code","5beb93c2":"code","5e77cd5a":"code","0eb1ec0d":"code","d7992645":"code","f587d114":"code","630ca59a":"code","03bb696c":"code","cdea6062":"code","46a305a1":"code","681720b7":"code","28d2a1e9":"code","2be8ad80":"code","b8270f60":"code","d50c75a3":"code","c2fae236":"code","897ae0d2":"code","070a3b0f":"code","9f0af726":"code","9f833835":"code","fdbcb0f3":"code","64b03867":"code","b497ac5e":"code","5f7340cb":"code","6b2cd2ba":"code","0b5018f6":"code","f920d2c4":"code","6b4d7857":"code","efad6dd4":"code","d59e27db":"code","f683e221":"code","02cbfa35":"code","fe1a143d":"code","1547ed7a":"code","c97e60a3":"code","1b0de134":"code","715cbb4b":"code","93a7b9d0":"code","2ffe256e":"code","9f030122":"code","d2f42789":"code","7d382260":"code","39fdd687":"code","50507f84":"code","a280d5cb":"code","9b057158":"code","898421a2":"code","5ee56286":"code","ae627def":"code","21b5d1e5":"code","e83aedf4":"code","f16ebdc6":"code","a4addd4e":"code","7ebbfacc":"code","39755745":"code","2bcfdcbb":"code","be3d2427":"code","e4b0c175":"code","37f04be9":"code","fbf8ef5d":"code","1c1c93c9":"code","473332ee":"code","9ef5443d":"code","e5905fc9":"code","d571700c":"code","d01b4ac5":"code","79f4329c":"code","a68e78ce":"code","c823e149":"code","c097dbf1":"code","3472ba2f":"code","bbc10c96":"code","3dc40bf5":"markdown","40531030":"markdown","80eaa304":"markdown","6d988ecd":"markdown","be5c4143":"markdown","a2d51891":"markdown","aca8db57":"markdown","342411c1":"markdown","09733b26":"markdown","2ad1a7d5":"markdown","d913ff38":"markdown","f07cde34":"markdown","0209920d":"markdown","2fcb5c50":"markdown","24c0337b":"markdown","5e17e892":"markdown","45d3f61b":"markdown","e86c8b32":"markdown","b08dc611":"markdown","f0e514fe":"markdown","c43050b4":"markdown","a420f566":"markdown","64f9643e":"markdown","10d734a8":"markdown","2be414d3":"markdown","d10d310b":"markdown","03ef5002":"markdown","e93834e1":"markdown","c69e0878":"markdown","163a85d5":"markdown","f9593c5e":"markdown","791d1a2c":"markdown","5587e814":"markdown","0ca98f69":"markdown","135b82ab":"markdown","11bd84ab":"markdown","edc3da52":"markdown","0343410c":"markdown","92ce36eb":"markdown","06894c58":"markdown","6f7bd2b1":"markdown","ef7c0724":"markdown","8c690eff":"markdown","061f65bb":"markdown","19a4cbdc":"markdown","c7645dd7":"markdown","cc4cd187":"markdown","813fffbb":"markdown","516aa8a2":"markdown","832a24f2":"markdown","38212b63":"markdown","406e5aaf":"markdown"},"source":{"f5cdf2cc":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nfrom pandas.plotting import scatter_matrix\nwarnings.filterwarnings('ignore')\n%matplotlib inline\nimport os","f4eb1872":"dataset = pd.read_csv(\"..\/input\/train.csv\")","41684140":"test_data = pd.read_csv(\"..\/input\/test.csv\")","98e54616":"dataset.info()","b12adf9b":"dataset.describe()","877dd3fb":"dataset.head()","8ed1590e":"def dropColumns(dataset, columns):\n    return dataset.drop(columns,axis=1)","76e5ab35":"dataset = dropColumns(dataset=dataset, columns=[\"PassengerId\", \"Name\"])\ndataset.head()","816e9a13":"plt.figure(figsize=(20,20))\np=sns.heatmap(dataset.corr(), annot=True,cmap='RdYlGn',square=True)  ","5e02245a":"p=sns.pairplot(dataset)","efa3980c":"cat_features = dataset.select_dtypes(include=['object']).copy()\ncat_features.head()","454fda3f":"cat_features.isnull().sum()","5a51f42b":"cat_features['Cabin'] = cat_features['Cabin'].fillna(cat_features['Cabin'].value_counts().index[0])","d92eac95":"cat_features['Embarked'] = cat_features['Embarked'].fillna(cat_features['Embarked'].value_counts().index[0])","3aaa39fd":"cat_features.head()","7e94a633":"dataset.dtypes","b7b58c3d":"non_cat_features = dataset.select_dtypes(include=['int64', 'float64']).copy()\nnon_cat_features.head()","99d52cfb":"non_cat_features.isnull().sum()","909d67d0":"non_cat_features['Age'].fillna(non_cat_features['Age'].mean(), inplace=True)","4ab53fed":"non_cat_features.isnull().sum()","5beb93c2":"clean_dataset = pd.concat([non_cat_features, cat_features], axis=1)","5e77cd5a":"clean_dataset.head()","0eb1ec0d":"# for the submission\npassengerId = test_data.PassengerId","d7992645":"test_data = dropColumns(dataset=test_data, columns=[\"PassengerId\", \"Name\"])\ntest_data.head()","f587d114":"cat_features_test = test_data.select_dtypes(include=['object']).copy()\ncat_features_test.head()","630ca59a":"cat_features_test.isnull().sum()","03bb696c":"cat_features_test['Cabin'] = cat_features_test['Cabin'].fillna(cat_features_test['Cabin'].value_counts().index[0])","cdea6062":"non_cat_features_test = test_data.select_dtypes(include=['int64', 'float64']).copy()\nnon_cat_features_test.head()","46a305a1":"non_cat_features_test.isnull().sum()","681720b7":"non_cat_features_test['Age'].fillna(non_cat_features_test['Age'].mean(), inplace=True)\nnon_cat_features_test['Fare'].fillna(non_cat_features_test['Fare'].mean(), inplace=True)","28d2a1e9":"clean_dataset_test = pd.concat([non_cat_features_test, cat_features_test], axis=1)\nclean_dataset_test.head()","2be8ad80":"# histogram\nclean_dataset.hist(figsize=(20, 20))\nplt.show()","b8270f60":"sm = scatter_matrix(clean_dataset, alpha=0.2, figsize=(20, 20), diagonal='kde')\n\n#Change label rotation\n[s.xaxis.label.set_rotation(45) for s in sm.reshape(-1)]\n[s.yaxis.label.set_rotation(0) for s in sm.reshape(-1)]\n\n#May need to offset label when rotating to prevent overlap of figure\n[s.get_yaxis().set_label_coords(-0.3,0.5) for s in sm.reshape(-1)]\n\n#Hide all ticks\n[s.set_xticks(()) for s in sm.reshape(-1)]\n[s.set_yticks(()) for s in sm.reshape(-1)]\n\nplt.show()","d50c75a3":"def displayBarPlot(feature):\n    sex_count = clean_dataset[feature].value_counts()\n    sns.set(style=\"darkgrid\")\n    sns.barplot(sex_count.index, sex_count.values, alpha=0.9)\n    plt.title('Frequency Distribution of %s' %feature)\n    plt.ylabel('Number of Occurrences', fontsize=12)\n    plt.xlabel(feature, fontsize=12)\n    \ndef displayPieChart(feature):\n    labels = clean_dataset[feature].astype('category').cat.categories.tolist()\n    counts = clean_dataset[feature].value_counts()\n    sizes = [counts[var_cat] for var_cat in labels]\n    fig1, ax1 = plt.subplots()\n    ax1.pie(sizes, labels=labels, autopct='%1.1f%%', shadow=True) #autopct is show the % on plot\n    ax1.axis('equal')\n    plt.show()\n    \ndef barPlotFeatureTarget(feature):\n    f,ax=plt.subplots(1,2,figsize=(18,8))\n    clean_dataset[[feature,'Survived']].groupby([feature]).mean().plot.bar(ax=ax[0])\n    ax[0].set_title('Survived vs %s' %feature)\n    sns.countplot(feature,hue='Survived',data=clean_dataset,ax=ax[1])\n    ax[1].set_title('%s:Survived vs Dead' %feature)\n    plt.show()\n    \ndef displayKdePlot(feature, hue):\n    sns.FacetGrid(clean_dataset, hue=hue, size=5).map(sns.kdeplot, feature).add_legend()\n    plt.show()\n    \ndef displayJointPlot(x, y):\n    sns.jointplot(x=x,y=y, data=clean_dataset)\n","c2fae236":"displayBarPlot('Sex')","897ae0d2":"displayPieChart('Sex')","070a3b0f":"p = sns.factorplot(x='Sex', y='Survived', data=clean_dataset, kind='box' ,aspect=2.5 )","9f0af726":"barPlotFeatureTarget('Sex')","9f833835":"p = sns.factorplot(x='Embarked', data=clean_dataset , kind='count',aspect=2.5 )","fdbcb0f3":"displayBarPlot(feature='Embarked')","64b03867":"displayPieChart(feature='Embarked')","b497ac5e":"barPlotFeatureTarget(feature='Embarked')","5f7340cb":"displayKdePlot(feature='Age',hue='Survived')","6b2cd2ba":"clean_dataset.head()","0b5018f6":"# Number of distinct Ticket\nclean_dataset['Ticket'].value_counts().count()","f920d2c4":"# Number of distinct Cabin\nclean_dataset['Cabin'].value_counts().count()","6b4d7857":"# pd.options.display.max_rows = 4000\nclean_dataset.groupby(\"Ticket\")['Fare'].mean().sort_values()","efad6dd4":"\"\"\"\n0 - 50 = T1\n50 - 100 = T2\n100 - 150 = T3\n150 - 200 = T4\n200 - 250 = T5\n250 - 300 = T6\n300 - ... = T7\n\"\"\"\ndef ticket_group(i):\n    group = 0\n    if i<100:\n        group = \"T1\"\n    elif i>=100 and i<150:\n        group = \"T2\"\n    elif i>=150 and i<200:\n        group = \"T3\"\n    elif i>=200 and i<250:\n        group = \"T4\"\n    elif i>= 250 and i<300:\n        group = \"T5\"\n    else:\n        group = \"T6\"\n    return group\n    ","d59e27db":"clean_dataset['Ticket'] = clean_dataset.Fare.apply(lambda x: ticket_group(x))\nclean_dataset.head()","f683e221":"clean_dataset.groupby(\"Ticket\")['Fare'].mean().sort_values()","02cbfa35":"clean_dataset['Ticket'].value_counts()","fe1a143d":"clean_dataset['Cabin'] = [i[0] for i in clean_dataset.Cabin]\nclean_dataset.head()","1547ed7a":"dataset[dataset.Cabin == 'T'].count()","c97e60a3":"test_data[test_data.Cabin == 'T'].count()","1b0de134":"clean_dataset.columns","715cbb4b":"i = clean_dataset[clean_dataset.Cabin == 'T'].index\nclean_dataset = clean_dataset.drop(i)\n#clean_dataset = clean_dataset.drop(\"Cabin_T\", axis=1)","93a7b9d0":"clean_dataset.head()","2ffe256e":"clean_dataset_test['Ticket'].value_counts().count()","9f030122":"clean_dataset_test['Cabin'].value_counts().count()","d2f42789":"clean_dataset_test['Ticket'] = clean_dataset_test.Fare.apply(lambda x: ticket_group(x))","7d382260":"clean_dataset_test.groupby(\"Ticket\")['Fare'].mean().sort_values()","39fdd687":"clean_dataset_test['Cabin'] = [i[0] for i in clean_dataset_test.Cabin]","50507f84":"def oneHotEncode(dataset, feature):\n    return pd.get_dummies(dataset[feature])","a280d5cb":"encoded_embarked = oneHotEncode(dataset=clean_dataset, feature='Embarked')\nencoded_embarked.head()","9b057158":"encoded_sex =  oneHotEncode(dataset=clean_dataset, feature='Sex')\nencoded_sex.head()","898421a2":"encoded_ticket =  oneHotEncode(dataset=clean_dataset, feature='Ticket')\nencoded_ticket.head()","5ee56286":"encoded_cabin =  oneHotEncode(dataset=clean_dataset, feature='Cabin')\n# Rename the columns to distinguish from the \"C\" cabin from the \"C\" value in Embarked\nencoded_cabin = encoded_cabin.rename(columns={'A':'Cabin_A', 'B': 'Cabin_B', \n                                                         'C': 'Cabin_C', 'D': 'Cabin_D', \n                                                         'E': 'Cabin_E', 'F': 'Cabin_F', \n                                                         'G': 'Cabin_G', 'T': 'Cabin_T'})\n#encoded_cabin = encoded_cabin.rename_axis({'A': 'Cabin_A'})\nencoded_cabin.head()","ae627def":"clean_dataset = pd.concat([clean_dataset, encoded_sex, encoded_ticket, encoded_cabin, encoded_embarked], axis=1)\n# Drop column\nclean_dataset = clean_dataset.drop(['Sex', 'Ticket', 'Cabin', 'Embarked'], axis=1)\nclean_dataset.head()","21b5d1e5":"clean_dataset.columns","e83aedf4":"# Embarked Feature\nencoded_embarked_test = oneHotEncode(dataset=clean_dataset_test, feature='Embarked')\n# Sex feature\nencoded_sex_test =  oneHotEncode(dataset=clean_dataset_test, feature='Sex')\n# Ticket\nencoded_ticket_test =  oneHotEncode(dataset=clean_dataset_test, feature='Ticket')\n# Cabin\nencoded_cabin_test =  oneHotEncode(dataset=clean_dataset_test, feature='Cabin')\n# Rename the columns to distinguish from the \"C\" cabin from the \"C\" value in Embarked\nencoded_cabin_test = encoded_cabin_test.rename(columns={'A':'Cabin_A', 'B': 'Cabin_B', \n                                                         'C': 'Cabin_C', 'D': 'Cabin_D', \n                                                         'E': 'Cabin_E', 'F': 'Cabin_F', \n                                                         'G': 'Cabin_G', 'T': 'Cabin_T'})\n\nclean_dataset_test = pd.concat([clean_dataset_test, encoded_sex_test, encoded_ticket_test, encoded_cabin_test, encoded_embarked_test], axis=1)\n# Drop column\nclean_dataset_test = clean_dataset_test.drop(['Sex', 'Ticket', 'Cabin', 'Embarked'], axis=1)\nclean_dataset_test.head()\n","f16ebdc6":"clean_dataset_test.columns","a4addd4e":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\n\ncolumns = clean_dataset.columns\ncolumns = columns.drop(['Survived'])\nsurvived = clean_dataset.Survived\ndata = clean_dataset.drop(['Survived'], axis=1)\n\n\nclean_dataset_scaled = pd.DataFrame(sc.fit_transform(data),columns=columns,index=clean_dataset.index)\nclean_dataset_scaled = pd.concat([clean_dataset_scaled, survived], axis=1)\nclean_dataset_scaled.head()","7ebbfacc":"# No 'Survived' column in test set\ncolumns_test = clean_dataset_test.columns\n\nclean_dataset_test_scaled = pd.DataFrame(sc.fit_transform(clean_dataset_test),columns=columns_test,index=clean_dataset_test.index)\nclean_dataset_test_scaled.head()","39755745":"X = clean_dataset_scaled.drop([\"Survived\"],axis=1)\ny = clean_dataset_scaled.Survived\nprint(X.shape)\nprint(y.shape)","2bcfdcbb":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y,random_state = 2,test_size=0.3)","be3d2427":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import make_scorer\nfrom sklearn.metrics import fbeta_score\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import confusion_matrix","e4b0c175":"def checkPerformances(classifier, best_clf):\n    # Make predictions using the unoptimized and optimized and model\n    predictions = (classifier.fit(X_train, y_train)).predict(X_test)\n    best_predictions = best_clf.predict(X_test)\n\n\n    # Report the before-and-afterscores\n    print(\"Unoptimized model\\n------\")\n    print(classifier)\n    print(\"Accuracy score on testing data: {:.4f}\".format(accuracy_score(y_test, predictions)))\n    print(\"F-score on testing data: {:.4f}\".format(fbeta_score(y_test, predictions, beta = 0.5, average=\"micro\")))\n    print(\"\\nOptimized Model\\n------\")\n    print(best_clf)\n    print(\"\\nFinal accuracy score on the testing data: {:.4f}\".format(accuracy_score(y_test, best_predictions)))\n    print(\"Final F-score on the testing data: {:.4f}\".format(fbeta_score(y_test, best_predictions, beta = 0.5,  average=\"micro\")))","37f04be9":"scorer = make_scorer(fbeta_score, beta=0.5, average=\"micro\")","fbf8ef5d":"DT_clf = DecisionTreeClassifier()\nDT_parameters = {\n    'criterion': ['gini', 'entropy'],\n    'min_samples_leaf': [1, 2, 3, 4]\n    }\n\n# Run the grid search\ngrid_obj = GridSearchCV(DT_clf, DT_parameters, scoring=scorer)\ngrid_obj = grid_obj.fit(X_train, y_train)\n\n# Set the regressor to the best combination of parameters\nbest_DT_clf = grid_obj.best_estimator_\n\ncheckPerformances(DT_clf, best_DT_clf)","1c1c93c9":"from sklearn.ensemble import RandomForestClassifier\nRF_clf = RandomForestClassifier(max_depth=None, random_state=None)\n\nparameters = {'n_estimators': [10, 20, 30, 100], 'max_features':[3,4,5, None], 'max_depth': [5,6,7, None], 'criterion': ['gini', 'entropy']}\ngrid_obj = GridSearchCV(RF_clf, parameters, scoring=scorer)\ngrid_fit = grid_obj.fit(X_train, y_train)\n\n# Get the estimator\nbest_RF_clf = grid_fit.best_estimator_\n\ncheckPerformances(classifier=RF_clf, best_clf=best_RF_clf)","473332ee":"LR_clf = LogisticRegression(random_state = 0)\nLR_parameters = {\n    'penalty': ['l1', 'l2'],\n    'C': np.logspace(0, 4, 10)\n    }\n\n# Run the grid search\ngrid_obj = GridSearchCV(LR_clf, LR_parameters, scoring=scorer)\ngrid_obj = grid_obj.fit(X_train, y_train)\n\n# Set the regressor to the best combination of parameters\nbest_LR_clf = grid_obj.best_estimator_\n\nbest_LR_clf\ncheckPerformances(LR_clf, best_LR_clf)","9ef5443d":"NB_clf = GaussianNB()\nNB_clf.fit(X_train, y_train)\npredictions = (NB_clf.fit(X_train, y_train)).predict(X_test)\n\nprint(\"Accuracy score on testing data: {:.4f}\".format(accuracy_score(y_test, predictions)))\nprint(\"F-score on testing data: {:.4f}\".format(fbeta_score(y_test, predictions, beta = 0.5, average=\"micro\")))","e5905fc9":"SVM_clf = SVC()\nSVM_parameters = {\n    'kernel': ['linear', 'poly', 'rbf'],\n    'degree': [1, 2, 3, 4],\n    'shrinking' : [True, False]\n    }\n\n# Run the grid search\ngrid_obj = GridSearchCV(SVM_clf, SVM_parameters, scoring='accuracy')\ngrid_obj = grid_obj.fit(X_train, y_train)\n\n# Set the regressor to the best combination of parameters\nbest_SVM_clf = grid_obj.best_estimator_\n\ncheckPerformances(SVM_clf, best_SVM_clf)","d571700c":"from sklearn import model_selection\n# 10-fold cross validation\n# Test options and evaluation metric\nseed = 7\n# Using metric accuracy to measure performance\nscoring = 'accuracy' ","d01b4ac5":"# Spot Check Algorithms\nmodels = []\nmodels.append(('LR', best_LR_clf))\nmodels.append(('DT', best_DT_clf))\nmodels.append(('RF', best_RF_clf))\n#models.append(('KNN', best_KNN_clf))\nmodels.append(('NB', NB_clf))\nmodels.append(('SVM', best_SVM_clf))\n\n# evaluate each model in turn\nresults = []\nnames = []\nfor name, model in models:\n\tkfold = model_selection.KFold(n_splits=10, random_state=seed)\n\tcv_results = model_selection.cross_val_score(model, X_train, y_train, cv=kfold, scoring=scoring)\n\tresults.append(cv_results)\n\tnames.append(name)\n\tmsg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n\tprint(msg)","79f4329c":"fig = plt.figure()\nfig.suptitle('Algorithm Comparison')\nax = fig.add_subplot(111)\nplt.boxplot(results)\nax.set_xticklabels(names)\nplt.show()","a68e78ce":"y_pred = best_RF_clf.predict(X_test)\n# Confusion matrix\nlabels = [0, 1]\ncm = confusion_matrix(y_test, y_pred, labels)\nprint(cm)\nfig = plt.figure()\nax = fig.add_subplot(111)\ncax = ax.matshow(cm)\nplt.title('Confusion matrix of the RF classifier')\nfig.colorbar(cax)\nax.set_xticklabels([''] + labels)\nax.set_yticklabels([''] + labels)\nplt.xlabel('Predicted')\nplt.ylabel('True')\nplt.show()","c823e149":"'''\nsubmission = pd.DataFrame({\n        \"PassengerId\": dataset[\"PassengerId\"],\n        \"Survived\": y_pred\n    })\nsubmission.to_csv('submission.csv', index=False)\n'''","c097dbf1":"\nX_train.columns","3472ba2f":"clean_dataset_test_scaled.columns","bbc10c96":"best_RF_clf.fit(X_train, y_train)\npred = best_RF_clf.predict(clean_dataset_test_scaled)\noutput=pd.DataFrame({'PassengerId':passengerId,'Survived':pred})\noutput.to_csv('Submission.csv', index=False)","3dc40bf5":"Encoding data","40531030":"## Categorical features\n\n* Sex\n* Cabin\n* Embarked\n* Ticket","80eaa304":"#### Note\nOnly one line contains the T value for the cabin. However, in the test set, there is no such value. If we train the model on this basis, it will not be able to make predictions on the test set because the number of characteristics will no longer be the same. Since it's only one line, we'll get rid of it. ","6d988ecd":"# Titanic analysis","be5c4143":"Let's check the result","a2d51891":"## Splitting the dataset","aca8db57":"## Confusion matrix with Random Forest","342411c1":"### Ticket","09733b26":"## Sex","2ad1a7d5":"## One hot encoding","d913ff38":"## Naive Bayes","f07cde34":"### Cabin","0209920d":"![ahmed-zayan-sRg9N_0pn1Q-unsplash%20%281%29.jpg](attachment:ahmed-zayan-sRg9N_0pn1Q-unsplash%20%281%29.jpg)","2fcb5c50":"### Drop unecessary columns","24c0337b":"## Non categorical features","5e17e892":"## Embarked","45d3f61b":"#### Cabin","e86c8b32":"# Visualize data","b08dc611":"## Categorical Data Essense","f0e514fe":"We've got too many tickets. Let's group them by the mean of fare.","c43050b4":"## Compare algorithms","a420f566":"## Operations on test set","64f9643e":"Let's filter categorical features to deal with them","10d734a8":"Preprocessing, feature scaling and other actions performed on train set.","2be414d3":"## Handle missing values","d10d310b":"## Logistic Regression","03ef5002":"## Random Forest","e93834e1":"## Operations on test set","c69e0878":"#### Cabin feature","163a85d5":"#### Ticket","f9593c5e":"## Data Exploration","791d1a2c":"# Model Training","5587e814":"### Submission","0ca98f69":"Filter non categorical features","135b82ab":"## Decision tree","11bd84ab":"Let's take a look at missing values and replace them by the mean for each column","edc3da52":"### Sex","0343410c":"#### Embarked","92ce36eb":"#### About the values of the cabins","06894c58":"# Data handling","6f7bd2b1":"We have missing values only in the age column.","ef7c0724":"There is too much missing values in Cabin feature. If the dataset was big enough, we could just drop the rows with those missing values. Let's replace NaN values by the mode.","8c690eff":"## Getting sense of the data","061f65bb":"Get first letter for each cabin","19a4cbdc":"## Data Scaling ","c7645dd7":"### Embarked","cc4cd187":"### SVM","813fffbb":"This would create too many features if we use one hot encoding on this two categorical features. So we will put them in custom categories base on range of the fares","516aa8a2":"### Categorical features","832a24f2":"We can add all of these to the dataset and drop the columns","38212b63":"The ticket may be related to the occupied cabin and therefore to the passenger's chances of survival. So we leave this column.","406e5aaf":"# K- fold cross validation"}}