{"cell_type":{"172168ef":"code","727754fd":"code","d1d82f88":"code","35efe808":"code","3c78de8d":"code","be5b10d5":"code","935ff093":"code","1869f8f3":"code","7f4ccf9c":"code","9ff3207a":"code","48c01051":"code","d91ebbed":"code","09c6cab4":"code","87c4ca65":"code","8a8e550e":"code","5df82eb5":"code","fa050a21":"code","b8abfd43":"code","aa6aa340":"code","884af7b9":"code","da7bbd23":"code","a808e546":"code","0737abc7":"code","a65b8fbb":"code","624dce46":"code","ad658d0d":"code","921dd3b8":"code","88c55368":"code","6e0be560":"code","d29fddef":"code","96990f1a":"code","b3145446":"code","5ef9139c":"code","14cec7d8":"code","5b7f4760":"code","6e588867":"code","c79810e8":"code","444a043d":"code","062bcd7f":"code","d23e06dc":"code","12be7fcb":"code","08f845f1":"code","5a63a681":"code","c04d9f68":"code","7f0edf2d":"code","e264fd75":"code","f4b4f387":"markdown","9e221562":"markdown","2ccd7458":"markdown"},"source":{"172168ef":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","727754fd":"df = pd.read_csv('\/kaggle\/input\/first-gop-debate-twitter-sentiment\/Sentiment.csv')\ndf.head(3)","d1d82f88":"columns_to_drop = ['candidate_gold','relevant_yn_gold','sentiment_gold',\n                   'subject_matter_gold','tweet_coord','tweet_location','user_timezone',\n                   'id','tweet_created','tweet_id','name']\ndf.drop(labels=columns_to_drop,axis=1,inplace=True)\ndf.head(3)","35efe808":"df.dropna(inplace=True)\ndf.isnull().sum()","3c78de8d":"df['sentiment'].unique()","be5b10d5":"import nltk\nfrom nltk.corpus import stopwords\n\n# tweets = []\nstopwords_set = set(stopwords.words(\"english\"))\n\ndef remove_stopwords(doc):\n    words_filtered = [e.lower() for e in doc.split()]\n    words_cleaned = [word for word in words_filtered\n        if 'http' not in word\n        and not word.startswith('@')\n        and not word.startswith('#')\n        and word != 'rt']\n    doc_without_stopwords = ' '.join([word for word in words_cleaned if not word in stopwords_set])\n    \n    return doc_without_stopwords","935ff093":"df['text'] = df['text'].apply(remove_stopwords)","1869f8f3":"#this function is used to remove the punctuation in the text data\ndef remove_punctuations(doc):\n    punctuations = \"\"\"!()-[]{};:'\"\\,\u201c\u201d<>.\/?@#$%^&*_~\"\"\"\n    #we add one more punctuation to our list as this punctuation mark was used multiple times in the text data\n    punctuations += '\ufffd' \n    for p in punctuations:\n      if p in doc:\n        doc = doc.replace(p,\"\")\n    return doc","7f4ccf9c":"df['text'] = df['text'].apply(remove_punctuations)","9ff3207a":"#this function will remove all the tokens which are not alphabatic\ndef remove_digits(doc):\n    tokens = doc.split()\n    result = ' '.join([i for i in tokens if i.isalpha()])\n    return result","48c01051":"df['text'] = df['text'].apply(remove_digits)","d91ebbed":"#importing libraries for stemming\nimport re\nimport nltk\nfrom nltk.stem import SnowballStemmer #general stemmer\nprint(\" \".join(SnowballStemmer.languages))","09c6cab4":"#we will select the dutch language stemmer as out text is in dutch language\nstemmer = SnowballStemmer(\"english\")\n# stemmer.stem(df['text'].iloc[0])\ndf['text'] = df['text'].apply(stemmer.stem)","87c4ca65":"# selecting tweets with positive and negative sentiment\ndf_final = df[df['sentiment'] != 'Neutral']","8a8e550e":"df_final['sentiment'] = df_final['sentiment'].apply(lambda x : 1 if x == 'Positive' else 0)","5df82eb5":"#creating pradictor and target variable \nX = df_final['text']\ny = df_final['sentiment']","fa050a21":"X","b8abfd43":"y","aa6aa340":"# tokenizing the proprocessed text data\nsent = [row.split() for row in df_final['text']]","884af7b9":"sent[0]","da7bbd23":"from gensim.models.phrases import Phrases, Phraser","a808e546":"phrases = Phrases(sent, min_count=30, progress_per=10000)","0737abc7":"bigram = Phraser(phrases)","a65b8fbb":"sentences = bigram[sent]","624dce46":"from gensim.models import Word2Vec","ad658d0d":"w2v_model = Word2Vec(min_count=20,\n                     window=2,\n                     vector_size=300, \n                     alpha=0.03, \n                     min_alpha=0.0007)","921dd3b8":"w2v_model.build_vocab(sentences, progress_per=10000)","88c55368":"w2v_model.train(sentences, total_examples=w2v_model.corpus_count, epochs=30, report_delay=1)","6e0be560":"w2v_model.wv.vectors","d29fddef":"w2v_model","96990f1a":"# w2v_model.wv.get_vecattr(word, \"count\")  #gives the count of words\/occurence of the word.\ndef word2token(sentence):\n#     print(sentence)\n    words = sentence.split()\n#     print(words)\n    vec = []\n    for word in words:\n        try:\n            vec.append(w2v_model.wv.key_to_index[word])\n        # If word is not in index return 0. I realize this means that this\n        # is the same as the word of index 0 (i.e. most frequent word), but 0s\n        # will be padded later anyway by the embedding layer (which also\n        # seems dirty but I couldn't find a better solution right now)\n        except KeyError:\n            vec.append(0)\n    return vec","b3145446":"temp = df_final['text'].apply(word2token)","5ef9139c":"from keras.preprocessing.sequence import pad_sequences\nX = pad_sequences(temp)","14cec7d8":"X.shape","5b7f4760":"X[0:10]","6e588867":"from keras.models import Sequential\nfrom keras.layers import Dense, Embedding, LSTM","c79810e8":"vocab_size,embedding_size = w2v_model.wv.vectors.shape","444a043d":"pretrained_weights = w2v_model.wv.vectors","062bcd7f":"pretrained_weights.shape","d23e06dc":"model = Sequential()\nmodel.add(Embedding(input_dim=vocab_size, output_dim=embedding_size, weights=[pretrained_weights]))\nmodel.add(LSTM(32))\nmodel.add(Dense(1,activation='sigmoid'))\nmodel.compile(loss = 'binary_crossentropy', optimizer='adam',metrics = ['accuracy'])\nmodel.summary()","12be7fcb":"# spliting the dataset into test and train set\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=101)","08f845f1":"# output size = 32\nbatch_size = 64\nepochs = 100\nmodel.fit(X_train, y_train, epochs = epochs, batch_size=batch_size)","5a63a681":"y_pred = model.predict(X_test)","c04d9f68":"for i in range(10):\n    print(y_pred[i])","7f0edf2d":"for i,yp in enumerate(y_pred):\n    if yp >= 0.5:\n        y_pred[i] = 1\n    else:\n        y_pred[i] = 0","e264fd75":"from sklearn.metrics import confusion_matrix\ncf_mat = confusion_matrix(y_test, y_pred,labels=[0,1])\ncf_mat","f4b4f387":"https:\/\/stackoverflow.com\/questions\/42064690\/using-pre-trained-word2vec-with-lstm-for-word-generation\n\nhttps:\/\/www.kaggle.com\/guichristmann\/lstm-classification-model-with-word2vec\n\nhttps:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/layers\/LSTM","9e221562":"# **Preprocessing Text Data**","2ccd7458":"# **Preparing data for word2vec**\nhttps:\/\/www.kaggle.com\/pierremegret\/gensim-word2vec-tutorial"}}