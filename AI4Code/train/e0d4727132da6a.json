{"cell_type":{"1419761b":"code","3f95df82":"code","c50af13c":"code","3ddad28f":"code","7b3144c3":"code","6eec9c9a":"code","6222e42b":"code","7f0ce0e4":"code","aae6f415":"code","42c71a50":"code","8a6272a4":"code","2be110b7":"code","94714efc":"code","2d11c46d":"code","a338980b":"code","d65a958d":"code","3047aed8":"code","afd70a57":"code","4b0dd448":"code","82eab9e1":"code","e8cc885a":"code","82ffe72c":"code","3b419cc1":"code","e7362b59":"code","5ab6a8e5":"code","9219898d":"code","b40f629d":"code","f1d463a2":"code","d55bd352":"code","532af08e":"code","3083b753":"code","c76ddf86":"code","be2c8aa2":"code","b6176ed0":"code","601feb6a":"code","9a1281b6":"code","d71aa765":"code","d91be43a":"code","51dbfc4b":"code","7ec045f5":"code","6a010a83":"code","3881bf46":"code","13afed56":"code","fdbfdeca":"code","b138ec50":"code","7ef899b4":"code","8f28e973":"code","eba0d8c3":"code","c3563c3e":"code","c502477b":"code","6c2cdcf1":"code","9cc9868d":"code","51e12623":"code","852b4955":"code","f616368b":"code","fc1a56e7":"code","e3fe480b":"code","3a5950f1":"code","4ed82749":"markdown","11024ec4":"markdown","ce191024":"markdown","38d80492":"markdown","5e71e79d":"markdown","3fdd11bb":"markdown"},"source":{"1419761b":"import numpy as np # linear algebra\nimport pandas as pd\nimport cv2\nimport os\nimport math\nimport copy\nimport imageio\nimport seaborn as sns\nfrom glob import glob\nfrom pathlib import Path\nfrom collections import deque\nimport matplotlib.pyplot as plt\nfrom keras.utils import np_utils\nfrom IPython.display import Image\nfrom sklearn.utils import shuffle\nfrom sklearn.model_selection import train_test_split","3f95df82":"import seaborn as sns\nimport plotly.express as px\nfrom IPython.display import SVG\nimport plotly.graph_objects as go\nimport plotly.figure_factory as ff\nfrom plotly.subplots import make_subplots\n\n%matplotlib inline","c50af13c":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import models\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras.datasets import mnist\nfrom keras.layers.advanced_activations import LeakyReLU\nfrom keras.losses import categorical_crossentropy\nfrom tensorflow.keras.utils import plot_model\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.preprocessing import image\nfrom tensorflow.keras.applications.vgg16 import VGG16\nfrom tensorflow.keras.applications.vgg16 import preprocess_input\nfrom tensorflow.keras.applications import VGG16,inception_v3\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.optimizers import RMSprop,Adam,Optimizer,Optimizer, SGD\nfrom tensorflow.keras.layers import Input, Lambda,Dense, Dropout, Flatten, Conv2D, MaxPool2D, BatchNormalization,MaxPooling2D,BatchNormalization,\\\n                                    Permute,concatenate, TimeDistributed, Bidirectional,GRU, SimpleRNN,\\\n                                    LSTM, GlobalAveragePooling2D, SeparableConv2D, ZeroPadding2D, Convolution2D, ZeroPadding2D,Reshape,\\\n                                    Conv2DTranspose, LeakyReLU, Conv1D, AveragePooling1D, MaxPooling1D,Activation,ConvLSTM2D,MaxPooling3D\n\n","3ddad28f":"Main_Video_Path = Path(\"..\/input\/real-life-violence-situations-dataset\/Real Life Violence Dataset\")\nVideo_Path = list(Main_Video_Path.glob(r\"*\/*.mp4\"))\nVideo_Labels = list(map(lambda x: os.path.split(os.path.split(x)[0])[1],Video_Path))\nVideo_Path_Series = pd.Series(Video_Path,name=\"MP4\").astype(str)\nVideo_Labels_Series = pd.Series(Video_Labels,name=\"CATEGORY\")\nMain_MP4_Data = pd.concat([Video_Path_Series,Video_Labels_Series],axis=1)","7b3144c3":"Violence_Data = Main_MP4_Data[Main_MP4_Data[\"CATEGORY\"] == \"Violence\"]\nNonViolence_Data = Main_MP4_Data[Main_MP4_Data[\"CATEGORY\"] == \"NonViolence\"]\n\nViolence_Data = Violence_Data.reset_index()\nNonViolence_Data = NonViolence_Data.reset_index()","6eec9c9a":"Violence_Data","6222e42b":"Main_Video_Path = Path(\"..\/input\/violencedetectionsystem\")\nVideo_Path = list(Main_Video_Path.glob(r\"*\/*.mp4\"))\nVideo_Labels = list(map(lambda x: os.path.split(os.path.split(x)[0])[1],Video_Path))\nVideo_Path_Series = pd.Series(Video_Path,name=\"MP4\").astype(str)\nVideo_Labels_Series = pd.Series(Video_Labels,name=\"CATEGORY\")\nMain_MP4_Data = pd.concat([Video_Path_Series,Video_Labels_Series],axis=1)","7f0ce0e4":"Main_MP4_Data[\"CATEGORY\"].replace({'fight':'Violence','noFight':'NonViolence'}, inplace=True)\nVD = Main_MP4_Data[Main_MP4_Data[\"CATEGORY\"] == \"Violence\"]\nNVD = Main_MP4_Data[Main_MP4_Data[\"CATEGORY\"] == \"NonViolence\"]\n\n","aae6f415":"Violence_Data = Violence_Data.append(VD,ignore_index=True, sort=False)\nNonViolence_Data = NonViolence_Data.append(NVD,ignore_index=True, sort=False)","42c71a50":"Violence_Data","8a6272a4":"NonViolence_Data","2be110b7":"Main_Video_Path3 = Path(\"..\/input\/ucf-crime-full\/Normal_Videos_for_Event_Recognition\")\nVideo_Path = list(Main_Video_Path3.glob(r\"*.mp4\"))\n# Video_Labels = list(map(lambda x: os.path.split(os.path.split(x)[0])[1],Video_Path))\nVideo_Path_Series = pd.Series(Video_Path,name=\"mp4\").astype(str)\nVideo_Labels_Series = pd.Series('NonViolence',name=\"CATEGORY\")\nMain_MP4_Data = pd.concat([Video_Path_Series,Video_Labels_Series],axis=1)","94714efc":"Main_MP4_Data['CATEGORY'] = Main_MP4_Data['CATEGORY'].fillna(\"NonViolence\")","2d11c46d":"Main_MP4_Data.rename(columns = {'mp4':'MP4'}, inplace = True)","a338980b":"Main_MP4_Data","d65a958d":"NonViolence_Data = NonViolence_Data.append(Main_MP4_Data,ignore_index=True, sort=False)","3047aed8":"NonViolence_Data","afd70a57":"Main_Video_Path3 = Path(\"..\/input\/ucf-crime-full\/Fighting\")\nVideo_Path = list(Main_Video_Path3.glob(r\"*.mp4\"))\n# Video_Labels = list(map(lambda x: os.path.split(os.path.split(x)[0])[1],Video_Path))\nVideo_Path_Series = pd.Series(Video_Path,name=\"mp4\").astype(str)\nVideo_Labels_Series = pd.Series('Violence',name=\"CATEGORY\")\nMain_MP4_Data = pd.concat([Video_Path_Series,Video_Labels_Series],axis=1)","4b0dd448":"Main_MP4_Data['CATEGORY'] = Main_MP4_Data['CATEGORY'].fillna(\"Violence\")","82eab9e1":"Main_MP4_Data.rename(columns = {'mp4':'MP4'}, inplace = True)","e8cc885a":"Main_MP4_Data","82ffe72c":"Violence_Data = Violence_Data.append(Main_MP4_Data,ignore_index=True, sort=False)","3b419cc1":"Violence_Data","e7362b59":"NonViolence_Data","5ab6a8e5":"\nCdata = NonViolence_Data","9219898d":"Cdata = Cdata.append(Violence_Data,ignore_index=True, sort=False)","b40f629d":"Cdata","f1d463a2":"Cdata[\"CATEGORY\"].replace({'Violence':1,'NonViolence':0}, inplace=True)","d55bd352":"# Cdata.CATEGORY[Cdata.CATEGORY == 'Violence']","532af08e":"Violence_Data.MP4[1]","3083b753":"! mkdir .\/Frames\n","c76ddf86":"v_train,v_valid = train_test_split(Violence_Data.MP4, test_size=0.1,random_state=17)\nprint(\"v_train {} samples , v_valid {} samples\".format(len(v_train),len(v_valid)))","be2c8aa2":"nv_train,nv_valid = train_test_split(NonViolence_Data.MP4, test_size=0.1,random_state=17)\nprint(\"nv_train {} samples , nv_valid {} samples\".format(len(nv_train),len(nv_valid)))","b6176ed0":"os.mkdir(\".\/Frames\/train\")\nos.mkdir(\".\/Frames\/test\")\nos.mkdir(\".\/Frames\/train\/nonviolence\")\nos.mkdir(\".\/Frames\/train\/violence\")\nos.mkdir(\".\/Frames\/test\/nonviolence\")\nos.mkdir(\".\/Frames\/test\/violence\")","601feb6a":"def make_frames(df,tag,sset):\n    v = 0\n    for file_video in df:\n        Video_File_Path = file_video\n\n        Video_Caption = cv2.VideoCapture(Video_File_Path)\n        Frame_Rate = 15\n        count = 0\n        temp = []\n        if Video_Caption.isOpened():\n            os.mkdir('.\/Frames\/{}\/{}\/v{}'.format(sset,tag,v)) \n        while Video_Caption.isOpened():\n\n            Current_Frame_ID = Video_Caption.get(1)\n\n            ret,frame = Video_Caption.read()\n\n            if ret != True:\n                break\n\n            if Current_Frame_ID % math.floor(Frame_Rate) == 0:\n                image = cv2.resize(frame,(256,256))\n                cv2.imwrite(\".\/Frames\/{}\/{}\/v{}\/frame{}.jpg\".format(sset,tag,v,count), image)\n                count += 1\n        v += 1\n\n        Video_Caption.release()\n    \n    \n        \n        \n        \n    ","9a1281b6":"make_frames(v_train,tag='violence',sset='train')","d71aa765":"make_frames(v_valid,tag='violence',sset='test')","d91be43a":"make_frames(nv_train,tag='nonviolence',sset='train')","51dbfc4b":"make_frames(nv_valid,tag='nonviolence',sset='test')","7ec045f5":"sorted(os.listdir(\".\/Frames\/test\"))","6a010a83":"class Config():\n    def __init__(self):\n        pass\n    \n    num_classes=2\n    labels_to_class = {0:'nonviolence',1:'violence'}\n    class_to_labels = {'nonviolence':0,'violence':1}\n    resize = 224\n    num_epochs =10\n    batch_size =10","3881bf46":"train_data_path = '.\/Frames\/train'\ntest_data_path = \".\/Frames\/test\"","13afed56":"if not os.path.exists('data_files'):\n    os.mkdir('data_files')\nif not os.path.exists('data_files\/train'):\n    os.mkdir('data_files\/train') \nif not os.path.exists('data_files\/test'):\n    os.mkdir('data_files\/test') ","fdbfdeca":"num_classes = 2\nlabels_name={'nonviolence':0,'violence':1}","b138ec50":"data_dir_list = os.listdir(train_data_path)\ndata_dir_list","7ef899b4":"def make_train(train_data_path):\n    data_dir_list = os.listdir(train_data_path)\n    for data_dir in data_dir_list: # looping over every activity\n        label = labels_name[str(data_dir)]\n        video_list = os.listdir(os.path.join(train_data_path,data_dir))\n        for vid in video_list: # looping over every video within an activity\n            train_df = pd.DataFrame(columns=['FileName', 'Label', 'ClassName'])\n            img_list = os.listdir(os.path.join(train_data_path,data_dir,vid))\n            for img in img_list:# looping over every frame within the video\n                img_path = os.path.join(train_data_path,data_dir,vid,img)\n                train_df = train_df.append({'FileName': img_path, 'Label': label,'ClassName':data_dir },ignore_index=True)\n            file_name='{}_{}.csv'.format(data_dir,vid)\n            train_df.to_csv('data_files\/train\/{}'.format(file_name))\n            \nmake_train(train_data_path)","8f28e973":"def make_test(test_data_path):\n    data_dir_list = os.listdir(test_data_path)\n    for data_dir in data_dir_list: # looping over every activity\n        label = labels_name[str(data_dir)]\n        video_list = os.listdir(os.path.join(test_data_path,data_dir))\n        for vid in video_list: # looping over every video within an activity\n            test_df = pd.DataFrame(columns=['FileName', 'Label', 'ClassName'])\n            img_list = os.listdir(os.path.join(test_data_path,data_dir,vid))\n            for img in img_list: # looping over every frame within the video\n                img_path = os.path.join(test_data_path,data_dir,vid,img)\n                test_df = test_df.append({'FileName': img_path, 'Label': label,'ClassName':data_dir },ignore_index=True)\n            file_name='{}_{}.csv'.format(data_dir,vid)\n            test_df.to_csv('data_files\/test\/{}'.format(file_name))\n\nmake_test(test_data_path)","eba0d8c3":"class ActionDataGenerator(object):\n    \n    def __init__(self,root_data_path,temporal_stride=1,temporal_length=16,resize=224):\n        \n        self.root_data_path = root_data_path\n        self.temporal_length = temporal_length\n        self.temporal_stride = temporal_stride\n        self.resize=resize\n    def file_generator(self,data_path,data_files):\n        '''\n        data_files - list of csv files to be read.\n        '''\n        for f in data_files:       \n            tmp_df = pd.read_csv(os.path.join(data_path,f))\n            label_list = list(tmp_df['Label'])\n            total_images = len(label_list) \n            if total_images>=self.temporal_length:\n                num_samples = int((total_images-self.temporal_length)\/self.temporal_stride)+1\n                print ('num of samples from vid seq-{}: {}'.format(f,num_samples))\n                img_list = list(tmp_df['FileName'])\n            else:\n                print ('num of frames is less than temporal length; hence discarding this file-{}'.format(f))\n                continue\n            \n            start_frame = 0\n            samples = deque()\n            samp_count=0\n            for img in img_list:\n                samples.append(img)\n                if len(samples)==self.temporal_length:\n                    samples_c=copy.deepcopy(samples)\n                    samp_count+=1\n                    for t in range(self.temporal_stride):\n                        samples.popleft() \n                    yield samples_c,label_list[0]\n\n    def load_samples(self,data_cat='train'):\n        data_path = os.path.join(self.root_data_path,data_cat)\n        csv_data_files = os.listdir(data_path)\n        file_gen = self.file_generator(data_path,csv_data_files)\n        iterator = True\n        data_list = []\n        while iterator:\n            try:\n                x,y = next(file_gen)\n                x=list(x)\n                data_list.append([x,y])\n            except Exception as e:\n                print ('the exception: ',e)\n                iterator = False\n                print ('end of data generator')\n        return data_list\n    \n    def shuffle_data(self,samples):\n        data = shuffle(samples,random_state=2)\n        return data\n    \n    def preprocess_image(self,img):\n        img = cv2.resize(img,(self.resize,self.resize))\n        img = img\/255\n        return img\n    \n    def data_generator(self,data,batch_size=10,shuffle=True):              \n        \"\"\"\n        Yields the next training batch.\n        data is an array [[img1_filename,img2_filename...,img16_filename],label1], [image2_filename,label2],...].\n        \"\"\"\n        num_samples = len(data)\n        if shuffle:\n            data = self.shuffle_data(data)\n        while True:   \n            for offset in range(0, num_samples, batch_size):\n                #print ('startring index: ', offset) \n                # Get the samples you'll use in this batch\n                batch_samples = data[offset:offset+batch_size]\n                # Initialise X_train and y_train arrays for this batch\n                X_train = []\n                y_train = []\n                # For each example\n                for batch_sample in batch_samples:\n                    # Load image (X)\n                    x = batch_sample[0]\n                    y = batch_sample[1]\n                    temp_data_list = []\n                    for img in x:\n                        try:\n                            img = cv2.imread(img)\n                            #apply any kind of preprocessing here\n                            #img = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n                            img = self.preprocess_image(img)\n                            temp_data_list.append(img)\n    \n                        except Exception as e:\n                            print (e)\n                            print ('error reading file: ',img)  \n    \n                    # Read label (y)\n                    #label = label_names[y]\n                    # Add example to arrays\n                    X_train.append(temp_data_list)\n                    y_train.append(y)\n        \n                # Make sure they're numpy arrays (as opposed to lists)\n                X_train = np.array(X_train)\n                #X_train = np.rollaxis(X_train,1,4)\n                y_train = np.array(y_train)\n                y_train = np_utils.to_categorical(y_train, 2)\n\n                # The generator-y part: yield the next training batch            \n                yield X_train, y_train\n","c3563c3e":"root_data_path='data_files'\n\ndata_gen_obj=ActionDataGenerator(root_data_path,temporal_stride=2,temporal_length=5)","c502477b":"train_data = data_gen_obj.load_samples(data_cat='train')\n\ntest_data = data_gen_obj.load_samples(data_cat='test')","6c2cdcf1":"\nprint('num of train_samples: {}'.format(len(train_data)))\n\nprint('num of train_samples: {}'.format(len(test_data)))\n","9cc9868d":"\ntrain_generator = data_gen_obj.data_generator(train_data,batch_size=10,shuffle=True)\n\ntest_generator = data_gen_obj.data_generator(test_data,batch_size=10,shuffle=True)","51e12623":"Callback_Stop_Early = tf.keras.callbacks.EarlyStopping(monitor=\"loss\",patience=3,mode='auto')","852b4955":"def My_ConvLSTM_Model(frames, pixels_x, pixels_y, channels,categories):\n  \n    trailer_input  = Input(shape=(frames, pixels_x, pixels_y,channels)\n                    , name='trailer_input')\n    \n    first_ConvLSTM = ConvLSTM2D(filters=20, kernel_size=(3, 3)\n                       , recurrent_activation='hard_sigmoid'\n                       , activation='tanh'\n                       , padding='same', return_sequences=True)(trailer_input)\n    first_BatchNormalization = BatchNormalization()(first_ConvLSTM)\n    first_Pooling = MaxPooling3D(pool_size=(1, 2, 2), padding='same')(first_BatchNormalization)\n    \n    second_ConvLSTM = ConvLSTM2D(filters=10, kernel_size=(3, 3)\n                        , padding='same', return_sequences=True)(first_Pooling)\n    second_BatchNormalization = BatchNormalization()(second_ConvLSTM)\n    second_Pooling = MaxPooling3D(pool_size=(1, 3, 3), padding='same')(second_BatchNormalization)\n    \n    outputs = [branch(second_Pooling, 'cat_{}'.format(category)) for category in categories]\n    \n    seq = Model(inputs=trailer_input, outputs=outputs, name='Model')\n    \n    return seq\n\ndef branch(last_convlstm_layer, name):\n  \n    branch_ConvLSTM = ConvLSTM2D(filters=5, kernel_size=(3, 3)\n                        , stateful = False\n                        , kernel_initializer='random_uniform'\n                        , padding='same', return_sequences=True)(last_convlstm_layer)\n    branch_Pooling = MaxPooling3D(pool_size=(1, 2, 2), padding='same')(branch_ConvLSTM)\n    flat_layer = TimeDistributed(Flatten())(branch_Pooling)\n    \n    first_Dense = TimeDistributed(Dense(512,))(flat_layer)\n    second_Dense = TimeDistributed(Dense(32,))(first_Dense)\n    ff = Flatten()(second_Dense)\n    target =Dense(2,activation = 'softmax',name=name)(ff)\n    \n    return target","f616368b":"model = My_ConvLSTM_Model(5,224, 224,3, ['violence','nonviolence'])\nmodel.compile(loss=categorical_crossentropy,\n                  optimizer=Adam(), metrics=['accuracy'])\nmodel.summary()","fc1a56e7":"history = model.fit_generator(train_generator, \n                steps_per_epoch=len(train_data)\/\/10,epochs=10,\n                              callbacks=[Callback_Stop_Early,tf.keras.callbacks.ModelCheckpoint(\".\/Checkpoint.h5\", save_best_only = True)],\n                             validation_data = test_generator ,\n                              validation_steps = len(test_data)\/\/10)","e3fe480b":"model.save(\".\/convolstm.hdf5\")","3a5950f1":"print(history.history.keys())\nsns.set()\n# summarize history for accuracy\nplt.plot(history.history['cat_violence_accuracy'])\nplt.plot(history.history['val_cat_violence_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('cat_1_accuracy')\nplt.xlabel('epoch')\nplt.legend(['cat_violence_accuracy', 'val_cat_violence_accuracy'], loc='upper left')\nplt.show()\n\nplt.plot(history.history['cat_nonviolence_accuracy'])\nplt.plot(history.history['val_cat_nonviolence_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('cat_0_accuracy')\nplt.xlabel('epoch')\nplt.legend(['cat_nonviolence_accuracy', 'val_cat_nonviolence_accuracy'], loc='upper left')\nplt.show()\n\n# summarize history for loss\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('total_loss')\nplt.xlabel('epoch')\nplt.legend(['train_loss', 'valid_loss'], loc='upper left')\nplt.show()\n\nplt.plot(history.history['cat_violence_loss'])\nplt.plot(history.history['val_cat_violence_loss'])\nplt.title('model loss')\nplt.ylabel('cat_1_loss')\nplt.xlabel('epoch')\nplt.legend(['cat_violence_loss', 'val_cat_violence_loss'], loc='upper left')\nplt.show()\n\n\nplt.plot(history.history['cat_nonviolence_loss'])\nplt.plot(history.history['val_cat_nonviolence_loss'])\nplt.title('model loss')\nplt.ylabel('cat_0_loss')\nplt.xlabel('epoch')\nplt.legend(['cat_nonviolence_loss', 'val_cat_nonviolence_loss'], loc='upper left')\nplt.show()","4ed82749":"### **Merging all different Data**","11024ec4":"# Part-4  VDG + Convolstm ","ce191024":"## **Data Importing from different datasets**","38d80492":"##  **Import Libraries**","5e71e79d":"## **Video Preprocessing**","3fdd11bb":"<a href=\".\/convolstm.hdf5\"> Download File <\/a>"}}