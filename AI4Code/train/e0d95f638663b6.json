{"cell_type":{"5872b0b0":"code","f50fca80":"code","c43cf3ef":"code","61e88919":"code","e9edcc79":"code","0e2acce3":"code","2944a397":"code","fe041526":"code","82f19149":"code","4618ca8a":"code","d93f02fc":"code","3b5ca82f":"code","a90d020a":"code","9e5ff523":"code","814e779d":"code","4230b386":"code","abb965fd":"code","5239a237":"code","e33f6e6a":"code","80bce58c":"code","bb3e9a73":"code","1837a67d":"code","59494fe7":"code","b074460f":"code","88f1739a":"code","164490b8":"code","e76b06eb":"code","c7721a76":"code","ce8ca988":"code","4a8b7d3b":"code","61466733":"code","7fcc6a15":"markdown","63d89b8b":"markdown","e044212e":"markdown","aac69c90":"markdown","71d9dd5a":"markdown","a7bdafb8":"markdown","9fe5dd72":"markdown","f463378b":"markdown","3f0323c9":"markdown","5ee7acfa":"markdown","469b5c0e":"markdown","8e2e0947":"markdown","b3bc681c":"markdown","afee6493":"markdown","d48121d9":"markdown","3e4e61eb":"markdown","3c6e83a0":"markdown","d89c2da9":"markdown","82313197":"markdown","b3b2e71e":"markdown"},"source":{"5872b0b0":"# ! pip install -q kaggle\n# from google.colab import files\n# files.upload()\n# ! mkdir ~\/.kaggle\n# ! cp kaggle.json ~\/.kaggle\/\n# ! chmod 600 ~\/.kaggle\/kaggle.json\n\n# ! kaggle datasets list\n# ! pip install --upgrade --force-reinstall --no-deps kaggle\n\n# !kaggle competitions download -c nlp-getting-started -p 'dataset'","f50fca80":"# cd dataset","c43cf3ef":"# ! unzip nlp-getting-started.zip","61e88919":"import pandas as pd\n\ntrain = pd.read_csv('..\/input\/nlp-getting-started\/train.csv')\ntest = pd.read_csv('..\/input\/nlp-getting-started\/test.csv')\nsample_submission = pd.read_csv('..\/input\/nlp-getting-started\/sample_submission.csv')","e9edcc79":"train","0e2acce3":"test","2944a397":"sample_submission","fe041526":"train.info()","82f19149":"train.isna().sum()","4618ca8a":"test.info()","d93f02fc":"test.isna().sum()","3b5ca82f":"from sklearn.feature_extraction.text import CountVectorizer\n\ncntv = CountVectorizer()\nvectxt_train = cntv.fit_transform(train['text'])","a90d020a":"vectxt_train.toarray().shape","9e5ff523":"vecdf_train = pd.DataFrame(data = vectxt_train.toarray(), columns = cntv.get_feature_names())\nvecdf_train","814e779d":"words_cnt = cntv.vocabulary_\nsorted(words_cnt.items(), key=lambda x: x[1], reverse = True)  ","4230b386":"vectxt_test = cntv.transform(test['text'])","abb965fd":"vectxt_test.toarray().shape","5239a237":"vecdf_test = pd.DataFrame(data = vectxt_test.toarray(), columns = cntv.get_feature_names())\nvecdf_test","e33f6e6a":"from sklearn.naive_bayes import GaussianNB\n\ngnb_model = GaussianNB()","80bce58c":"from sklearn import model_selection\n\nscores = model_selection.cross_val_score(gnb_model, vectxt_train.toarray(), train[\"target\"], cv=10, scoring=\"f1\")\nscores","bb3e9a73":"from sklearn import linear_model\n\nrc_model = linear_model.RidgeClassifier()","1837a67d":"from sklearn import model_selection\n\nscores = model_selection.cross_val_score(rc_model, vectxt_train, train[\"target\"], cv=10, scoring=\"f1\")\nscores","59494fe7":"rc_model.fit(vectxt_train, train['target'])","b074460f":"predictions = rc_model.predict(vectxt_test)\npredictions","88f1739a":"sample_submission['target'] = predictions\nsample_submission['target'].value_counts()","164490b8":"sample_submission","e76b06eb":"sample_submission.to_csv('ss.csv', index = False)","c7721a76":"ss = pd.read_csv('ss.csv')\nss","ce8ca988":"test['disaster_check'] = predictions\ncheck = test[['text','disaster_check']]","4a8b7d3b":"from random import randint\n\nrand_int = []\n\nfor i in range(5):\n  rand_int.append(randint(0,len(test)))\n\nrand_int","61466733":"pd.set_option('display.max_colwidth', None) # to display full content\n\nfor i in rand_int:\n  print(check.iloc[i,:])\n  print('\\n')","7fcc6a15":"# Import dataset","63d89b8b":"Let's transform our test 'text' column. But be aware we are using transform only since we want to use same model on test set and not to create second model what wouldn't have any sens.","e044212e":"# Curiosity check:)","aac69c90":"# EDA","71d9dd5a":"we vectorize text column of train dataset","a7bdafb8":"Let's check how our model perfoms first","9fe5dd72":"ridge regression performance","f463378b":"# Submission","3f0323c9":"# Train","5ee7acfa":"It seems that the algorithm was able to predict correctly all of above examples. Of course it is arguable whether e.g. second sentence is a disaster. I mean this event is something that is probably already known by either police or medics. I think better example of (unexpected) disaster is 3rd sentence about terrorism because it can be unknown yet by the officials. Also quite cool example is 4th sentence where, despite the fact user used 'crushed' word in a tweet, algorithm correctly labeled it as 0.","469b5c0e":"I will use ridge regression since it managed to predict up to 69% accurate","8e2e0947":"we do have 21637 unique strings in whole tweets bag","b3bc681c":"Let's see how our strings perform","afee6493":"# CountVectorizer","d48121d9":"As we can see not every cell is filled. But what we are interested in is the text and target rows which are fully filled","3e4e61eb":"let's create a list of random 5 indexes from test.","3c6e83a0":"Ridge regression instance","d89c2da9":"display tweets and anwsers ","82313197":"Naive Bayes instance","b3b2e71e":"# Naive Bayes vs Ridge Regression"}}