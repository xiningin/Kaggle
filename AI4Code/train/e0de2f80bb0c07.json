{"cell_type":{"a84c3ed8":"code","0cbb25e5":"code","6ee1b954":"code","094483b5":"code","f4827e48":"code","039e90cb":"code","a08450cf":"code","be072bf5":"code","d45b20ab":"code","f3b3841e":"code","58417e13":"code","7304113c":"code","8f8b0bd3":"code","6c7e8d88":"code","88957069":"code","a5f6b808":"code","82e6036f":"code","77b55a2b":"code","2c17b431":"code","8b155808":"code","a7097d8f":"code","9d843182":"markdown","7f4fc7d9":"markdown","9651c862":"markdown","c56071b3":"markdown"},"source":{"a84c3ed8":"import joblib\nimport torch\nimport torch.nn as nn\nimport transformers\nimport os\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn import preprocessing\nfrom sklearn import model_selection\n\nfrom tqdm import tqdm\nfrom transformers import AdamW\nfrom transformers import get_linear_schedule_with_warmup","0cbb25e5":"! git clone https:\/\/github.com\/pranav-ust\/BERT-keyphrase-extraction.git","6ee1b954":"class config:\n    MAX_LEN = 128\n    TRAIN_BATCH_SIZE = 32\n    VALID_BATCH_SIZE = 8\n    EPOCHS = 3\n    BASE_MODEL_PATH = \"bert-base-uncased\"\n    MODEL_PATH = \"model.bin\"\n    TOKENIZER = transformers.BertTokenizer.from_pretrained(\n        BASE_MODEL_PATH,\n        do_lower_case=True\n    )","094483b5":"class EntityDataset:\n    def __init__(self, texts, tags):\n        self.texts = texts\n        self.tags = tags\n    \n    def __len__(self):\n        return len(self.texts)\n    \n    def __getitem__(self, item):\n        text = self.texts[item]\n        tags = self.tags[item]\n\n        ids = []\n        target_tag =[]\n\n        for i, s in enumerate(text):\n            inputs = config.TOKENIZER.encode(\n                s,\n                add_special_tokens=False\n            )\n            # abhishek: ab ##hi ##sh ##ek\n            input_len = len(inputs)\n            ids.extend(inputs)\n            target_tag.extend([tags[i]] * input_len)\n\n        ids = ids[:config.MAX_LEN - 2]\n        target_tag = target_tag[:config.MAX_LEN - 2]\n\n        ids = [101] + ids + [102]\n        target_tag = [0] + target_tag + [0]\n\n        mask = [1] * len(ids)\n        token_type_ids = [0] * len(ids)\n\n        padding_len = config.MAX_LEN - len(ids)\n\n        ids = ids + ([0] * padding_len)\n        mask = mask + ([0] * padding_len)\n        token_type_ids = token_type_ids + ([0] * padding_len)\n        target_tag = target_tag + ([0] * padding_len)\n\n        return {\n            \"ids\": torch.tensor(ids, dtype=torch.long),\n            \"mask\": torch.tensor(mask, dtype=torch.long),\n            \"token_type_ids\": torch.tensor(token_type_ids, dtype=torch.long),\n            \"target_tag\": torch.tensor(target_tag, dtype=torch.long),\n        }","f4827e48":"def train_fn(data_loader, model, optimizer, device, scheduler):\n    model.train()\n    final_loss = 0\n    for data in tqdm(data_loader, total=len(data_loader)):\n        for k, v in data.items():\n            data[k] = v.to(device)\n        optimizer.zero_grad()\n        _, loss = model(**data)\n        loss.backward()\n        optimizer.step()\n        scheduler.step()\n        final_loss += loss.item()\n    return final_loss \/ len(data_loader)\n\n\ndef eval_fn(data_loader, model, device):\n    model.eval()\n    final_loss = 0\n    for data in tqdm(data_loader, total=len(data_loader)):\n        for k, v in data.items():\n            data[k] = v.to(device)\n        _,loss = model(**data)\n        final_loss += loss.item()\n    return final_loss \/ len(data_loader)","039e90cb":"def loss_fn(output, target, mask, num_labels):\n    lfn = nn.CrossEntropyLoss()\n    active_loss = mask.view(-1) == 1\n    active_logits = output.view(-1, num_labels)\n    active_labels = torch.where(\n        active_loss,\n        target.view(-1),\n        torch.tensor(lfn.ignore_index).type_as(target)\n    )\n    loss = lfn(active_logits, active_labels)\n    return loss","a08450cf":"class EntityModel(nn.Module):\n    def __init__(self, num_tag):\n        super(EntityModel, self).__init__()\n        self.num_tag = num_tag\n        self.bert = transformers.BertModel.from_pretrained(\n            config.BASE_MODEL_PATH\n        )\n        self.bert_drop_1 = nn.Dropout(0.3)\n        self.bert_drop_2 = nn.Dropout(0.3)\n        self.out_tag = nn.Linear(768, self.num_tag)\n    \n    def forward(\n        self, \n        ids, \n        mask, \n        token_type_ids,  \n        target_tag\n    ):\n        o1, _ = self.bert(\n            ids, \n            attention_mask=mask, \n            token_type_ids=token_type_ids\n        )\n\n        bo_tag = self.bert_drop_1(o1)\n\n        tag = self.out_tag(bo_tag)\n\n        loss_tag = loss_fn(tag, target_tag, mask, self.num_tag)\n\n        loss = loss_tag\n        return tag, loss","be072bf5":"! pip install pytorch_pretrained_bert","d45b20ab":"\nfrom pytorch_pretrained_bert import BertTokenizer\nbert_model_dir='bert-base-uncased'","f3b3841e":"tokenizer = BertTokenizer.from_pretrained(bert_model_dir, do_lower_case=True)","58417e13":"\ndef data_preprocess(data_path,data_type):\n    sentences=[]\n    sentence_word=[]\n    tags=[]\n    \n    sentences_file=os.path.join(data_path,data_type,'sentences.txt')\n    tag_file=os.path.join(data_path,data_type,'tags.txt')\n    \n    with open(sentences_file, 'r') as file:\n        for line in file:\n            # replace each token by its index\n            \n            tokens = line.strip().split()\n#             print(len(tokens))\n            input_len=len(tokens)\n            sentences.extend([line]*input_len)\n            sentence_word.extend(tokens)\n            \n    with open(tag_file, 'r') as file:\n        for line in file:\n            \n            tag=line.strip().split()\n            tags.extend(tag)\n    final_data=pd.DataFrame({'Sentence':sentences,'Sentence_words':sentence_word,'tag':tags,})\n    \n    enc_tag = preprocessing.LabelEncoder()\n\n    final_data.loc[:, \"tag\"] = enc_tag.fit_transform(final_data[\"tag\"])\n    sentences = final_data.groupby(\"Sentence\")[\"Sentence_words\"].apply(list).values\n    \n    tag = final_data.groupby(\"Sentence\")[\"tag\"].apply(list).values\n    return sentences,enc_tag,tag\n","7304113c":"sentences_file='.\/BERT-keyphrase-extraction\/data\/task1\/train\/sentences.txt'\nsentences = []\ntags = []\n\nwith open(sentences_file, 'r') as file:\n    for line in file:\n        # replace each token by its index\n        tokens = line.strip().split()\n        print(tokens)\n        s=config.TOKENIZER.convert_tokens_to_ids(tokens)\n#         print(s)\n        sentences.append(s)\n#         print(tokens)\n","8f8b0bd3":"print(len(sentences))\n\nprint(sentences[0])\n","6c7e8d88":"data_path='.\/BERT-keyphrase-extraction\/data\/task1'\ndata_type='train'","88957069":"sentences,enc_tag,tag=data_preprocess(data_path,data_type)\n\n# sentences=data_preprocess(data_path,data_type)","a5f6b808":"print(len(sentences))\nprint(len(tag))","82e6036f":"meta_data = {\n    \"enc_tag\": enc_tag\n}\n\n\nnum_tag = len(list(enc_tag.classes_))\n\n","77b55a2b":"(\n    train_sentences,\n    test_sentences,\n    train_tag,\n    test_tag\n) = model_selection.train_test_split(\n    sentences, \n    tag, \n    random_state=42, \n    test_size=0.1\n)\n\ntrain_dataset = EntityDataset(\n    texts=train_sentences, tags=train_tag\n)\n\ntrain_data_loader = torch.utils.data.DataLoader(\n    train_dataset, batch_size=config.TRAIN_BATCH_SIZE, num_workers=4\n)\n\nvalid_dataset = EntityDataset(\n    texts=test_sentences, tags=test_tag\n)\n\nvalid_data_loader = torch.utils.data.DataLoader(\n    valid_dataset, batch_size=config.VALID_BATCH_SIZE, num_workers=1\n)","2c17b431":"device = torch.device(\"cuda\")\nmodel = EntityModel(num_tag=num_tag)\nmodel.to(device)\n\nparam_optimizer = list(model.named_parameters())\nno_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\noptimizer_parameters = [\n    {\n        \"params\": [\n            p for n, p in param_optimizer if not any(\n                nd in n for nd in no_decay\n            )\n        ],\n        \"weight_decay\": 0.001,\n    },\n    {\n        \"params\": [\n            p for n, p in param_optimizer if any(\n                nd in n for nd in no_decay\n            )\n        ],\n        \"weight_decay\": 0.0,\n    },\n]\n\nnum_train_steps = int(\n    len(train_sentences) \/ config.TRAIN_BATCH_SIZE * config.EPOCHS\n)\noptimizer = AdamW(optimizer_parameters, lr=3e-5)\nscheduler = get_linear_schedule_with_warmup(\n    optimizer, \n    num_warmup_steps=0, \n    num_training_steps=num_train_steps\n)\n\nbest_loss = np.inf\nfor epoch in range(config.EPOCHS):\n    train_loss = train_fn(\n        train_data_loader, \n        model, \n        optimizer, \n        device, \n        scheduler\n    )\n    test_loss = eval_fn(\n        valid_data_loader,\n        model,\n        device\n    )\n    print(f\"Train Loss = {train_loss} Valid Loss = {test_loss}\")\n    if test_loss < best_loss:\n        torch.save(model.state_dict(), config.MODEL_PATH)\n        best_loss = test_loss","8b155808":"# enc_tag = meta_data[\"enc_tag\"]\n# enc_tag","a7097d8f":"### Testing model on a sample_txt\n\n# meta_data = joblib.load(\"meta.bin\")\nenc_tag = meta_data[\"enc_tag\"]\n\n\nnum_tag = len(list(enc_tag.classes_))\n\nsentence = \"\"\"\n\ncomplex lange ##vin ( cl ) [ 1 , 2 ] sign problem numerical simulations of lattice field theories weight , sampling . nonzero chemical potential , lower and four - dimensional field theories sign problem in the thermodynamic limit [ 3 \u2013 8 ] ( for reviews , e . g . refs . [ 9 , 10 ] ) . however , inc ##epti ##on , [ 11 \u2013 16 ] . improved understanding , relying on the combination of analytical and numerical insight . past , probability distribution complex ##ified configuration space , lange ##vin process , [ 17 , 18 ] . distribution local ##ised cl results . importantly , non ##abel ##ian gauge theories , sl ( n , c ) gauge cooling [ 8 , 10 ] .\nnuclear theory thermal ##ization nuclear reactions , semi - classical methods [ 13 , 14 , 10 ] , quantum liquids [ 15 , 16 ] . improved molecular dynamics methods combining quantum features semi classical treatment of dynamical correlations [ 17 , 18 ] . still , clear - cut quantum approach yet , [ 19 , 20 , 10 ] . field of clusters and nano structures lasers imaging techniques . semic ##lass ##ical [ 21 , 22 ] qualitatively describe dynamical processes . simple metals with sufficiently del ##ocal ##ized wave functions , justify ##ing semic ##lass ##ical approximations . organic systems , celebr ##ated c ##60 [ 4 , 23 ] , way . classical , approaches , very intense laser pulses [ 2 ] . blow ##n quantum mechanical features anym ##ore . scenarios , quantum shell effects ignored .\ndirac equation . the cre ##utz model [ 32 ] treatment , objects hopping on a lattice instead of particles moving in a space - time continuum . \n\"\"\"\ntokenized_sentence = config.TOKENIZER.encode(sentence)\n\nsentence = sentence.split()\nprint(sentence)\nprint(tokenized_sentence)\n\ntest_dataset = EntityDataset(\n    texts=[sentence], \n    tags=[[0] * len(sentence)]\n)\n\ndevice = torch.device(\"cuda\")\nmodel = EntityModel(num_tag=num_tag)\nmodel.load_state_dict(torch.load(config.MODEL_PATH))\nmodel.to(device)\n\nwith torch.no_grad():\n    data = test_dataset[0]\n    for k, v in data.items():\n        data[k] = v.to(device).unsqueeze(0)\n    tag, _ = model(**data)\n\n    print(\n        enc_tag.inverse_transform(\n            tag.argmax(2).cpu().numpy().reshape(-1)\n        )[:len(tokenized_sentence)]\n    )","9d843182":"## Training and evaluation functions","7f4fc7d9":"## Dataset","9651c862":"## Data processing","c56071b3":"## Loss function and model"}}