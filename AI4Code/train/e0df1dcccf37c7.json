{"cell_type":{"2ad3f4a3":"code","e6861373":"code","52a820cb":"code","8af3eba9":"code","59748915":"code","fe78f6a9":"code","742b485f":"code","f254f524":"code","ed3c8679":"code","deff4564":"code","09224522":"code","1f791d5c":"code","06984709":"code","c6a4d6d0":"code","ed66ae37":"code","136b09e6":"code","3c238fc7":"code","8da16b6c":"code","65ed6afc":"code","e7e64140":"code","ea8d04be":"code","854f5f55":"code","a95a90f2":"code","f94769ef":"code","b809f18f":"code","0df7aae5":"code","ea15de1d":"code","e9f635ab":"code","f98657c8":"code","9d98e860":"markdown","06b5aebb":"markdown","bc3b6749":"markdown","4af6d2a0":"markdown","5ab50fdb":"markdown","480c6902":"markdown","5690e802":"markdown","94f310b0":"markdown"},"source":{"2ad3f4a3":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","e6861373":"import random\nrandom.seed(42)","52a820cb":"train_data = pd.read_csv('..\/input\/train\/train.csv', nrows=1900000)","8af3eba9":"train_data = train_data.rename(index=str, columns={\"Unnamed: 0\": \"id\"})\ntrain_data.head(20)","59748915":"def get_info(data):\n    info = pd.DataFrame(data.count() \/ data['target'].size, columns=['not_null'])\n    info['type'] = data.dtypes\n    info['unique'] = data.nunique()\n    mapping = { 'feature_40': data['feature_40'].unique(),\n                'feature_38': data['feature_38'].unique(),\n                'feature_30': data['feature_30'].unique(),\n                'feature_11': data['feature_11'].unique(),\n                'feature_4': data['feature_4'].unique()}\n    info['values'] = info.index.map(mapping)\n    return info\n    \nget_info(train_data)","fe78f6a9":"bad_factors = ['feature_13', 'feature_16', 'feature_32', 'feature_34', 'feature_35']\ntrain_data = train_data.drop(bad_factors, axis=1)","742b485f":"numeric = train_data.select_dtypes(include=[np.number]).drop(['id','group_id','target'], axis=1).columns.tolist()","f254f524":"train_data = pd.get_dummies(train_data)","ed3c8679":"train_means = train_data.min(axis=0, numeric_only=True)","deff4564":"train_means = train_means - train_means * 2 - 999","09224522":"train_data = train_data.fillna(train_means)","1f791d5c":"grouped = train_data.groupby(by='group_id', axis=0, sort=False)","06984709":"for col in numeric:\n    train_data[col + '_max'] = grouped[col].transform(max)\n    train_data[col + '_min'] = grouped[col].transform(min)\n    train_data[col + '_p_max'] = train_data[col] \/ (train_data[col + '_max'] + 0.001)\n    train_data[col + '_p_min'] = train_data[col] \/ (train_data[col + '_min'] + 0.001)","c6a4d6d0":"train_data.head(20)","ed66ae37":"import xgboost as xgb","136b09e6":"# import numpy as np\n# from catboost import CatBoostClassifier, Pool\n","3c238fc7":"# limit = int(train_data.id.size * 2 \/ 3)\n# tr = train_data.iloc[:limit, :]\n# tv = train_data.iloc[limit:, :]","8da16b6c":"num_round = 30000\ndef model(xtr, xtv):\n       ## fixed parameters\n    scale_pos_weight = sum(train_data['target']==0)\/sum(train_data['target']==1)  \n    param = {'verbosity': 1,\n             'eta': 0.025,\n              \"min_eta\"               : 0.001,\n               \"eta_decay\"             : 0.3,\n                \"max_fails\"             : 3,\n                \"early_stopping_rounds\" : 20,\n             'subsample': 0.7,\n             'tree_method': 'gpu_hist',\n             'max_depth': 4,\n             'min_child_weight':1, ## unbalanced dataset\n             'objective':'binary:logistic',\n             'eval_metric':'map@1', \n             'scale_pos_weight':scale_pos_weight}\n    evallist = [(xtv, 'eval'), (xtr, 'train')] \n    return xgb.train(param, xtr, num_round, evallist)\n    \n    ","65ed6afc":"def my_func(data):\n    val = data['group_id'].values\n    res=[]\n    last = val[0]\n    cur = 0\n    for t in val:\n        if t == last:\n            cur = cur + 1\n        else:\n            res.append(cur)\n            cur = 1\n            last = t\n    res.append(cur)\n    return res","e7e64140":"def to_xgb_matrix(p_data):  \n    label = p_data['target']\n    counts = my_func(p_data)\n    data = p_data.drop(['id', 'group_id', 'target'], axis=1)\n    dtrain = xgb.DMatrix(data, label=label)\n    dtrain.set_group(counts)\n    return dtrain\n\ndef best(data, target):\n    length = len(target)\n    index = 0\n    rid_map = {}\n    while index < length:\n        tmp = data.group_id.iloc[index]\n        if tmp in rid_map:\n            if rid_map[tmp][0] < target[index]:\n                rid_map[tmp] = [target[index], data.id.iloc[index]]\n        else:\n            rid_map[tmp] = [target[index], data.id.iloc[index]]\n        index = index + 1\n    return dict([(k, v[1]) for (k, v) in rid_map.items()])\n\ndef validate(x_tr, test):\n    limit2 = int(x_tr.id.size \/ 2)\n    tr2 = x_tr.iloc[:limit2, :]\n    tv2 = x_tr.iloc[limit2:, :]\n\n    xtr2 = to_xgb_matrix(tr2)\n    xtv2 = to_xgb_matrix(tv2)\n\n    m = model(xtr2, xtv2)\n    ty = test['target']\n    dd = xgb.DMatrix(test.drop(['id', 'group_id', 'target'], axis=1))\n\n    pred = m.predict(dd, ntree_limit=m.best_ntree_limit)\n    pred_val = best(test.drop(['target'], axis=1), pred)\n\n    real_val = best(test.drop(['target'], axis=1), list(ty))\n    duplicates = dict(real_val.items() & pred_val.items())\n\n    return len(duplicates) \/ len(real_val)\n               \n#validate(tr, tv)","ea8d04be":"test_data = pd.read_csv('..\/input\/test\/test.csv')\ntest_data = test_data.rename(index=str, columns={\"Unnamed: 0\": \"id\"})","854f5f55":"test_data = test_data.drop(bad_factors, axis=1)\ntest_data = pd.get_dummies(test_data)\ntest_data = test_data.fillna(train_means)\n","a95a90f2":"grouped2 = test_data.groupby(by='group_id', axis=0, sort=False)\nfor col in numeric:\n    test_data[col + '_max'] = grouped2[col].transform(max)\n    test_data[col + '_min'] = grouped2[col].transform(min)\n    test_data[col + '_p_max'] = test_data[col] \/ (test_data[col + '_max'] + 0.001)\n    test_data[col + '_p_min'] = test_data[col] \/ (test_data[col + '_min'] + 0.001)","f94769ef":"train_data['feature_39_applicableByName'] = 0\ntest_data['feature_37_castVariable'] = 0\ntest_data['feature_37_qualifiedWithField'] = 0\ntest_data['feature_37_accessibleFieldGetter'] = 0\ntest_data = test_data.reindex(sorted(test_data.columns), axis=1)\ntrain_data = train_data.reindex(sorted(train_data.columns), axis=1)\ntest_data.head(20)","b809f18f":"limit = int(train_data.id.size * 4 \/ 5)\ntr = train_data.iloc[:limit, :]\ntv = train_data.iloc[limit:, :]","0df7aae5":"xtr2 = to_xgb_matrix(tr)\nxtv2 = to_xgb_matrix(tv)\n","ea15de1d":"m = model(xtr2, xtv2)","e9f635ab":"tdd = xgb.DMatrix(test_data.drop(['id', 'group_id'], axis=1))\npred = m.predict(tdd, ntree_limit=m.best_ntree_limit)","f98657c8":"mysubmission = pd.DataFrame({'Id': test_data.id, 'target': pred})\nmysubmission.to_csv('submission.csv', index=False)","9d98e860":"\u0412\u044b\u0432\u0435\u0434\u0435\u043c \u0438\u043d\u0444\u043e\u0440\u043c\u0430\u0446\u0438\u044e \u043e \u043a\u043e\u043b\u043e\u043d\u043a\u0430\u0445","06b5aebb":"\u0423\u0434\u0430\u043b\u0438\u043c \u043f\u043b\u043e\u0445\u0438\u0435 \u0444\u0430\u043a\u0442\u043e\u0440\u044b","bc3b6749":"\u0424\u0443\u043d\u043a\u0446\u0438\u044f \u0434\u043b\u044f \u0433\u0440\u0443\u043f\u043f\u0438\u0440\u043e\u0432\u043a\u0438","4af6d2a0":"\u0414\u043e\u0431\u0430\u0432\u0438\u043c \u0433\u0440\u0443\u043f\u043f\u043e\u0432\u044b\u0445 \u0444\u0430\u043a\u0442\u043e\u0440\u043e\u0432","5ab50fdb":"One-hot","480c6902":"\u0417\u0430\u043f\u043e\u043b\u043d\u0438\u043c \u043d\u0443\u043b\u044b","5690e802":"\u0414\u043e\u0431\u0430\u0432\u0438\u043c \u043e\u0442\u0441\u0443\u0442\u0441\u0432\u0443\u044e\u0449\u0438\u0435 \u043a\u043e\u043b\u043e\u043d\u043a\u0438 \u0432 \u043e\u0431\u0430 \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u0430","94f310b0":"\u0414\u043e\u043b\u044c\u043a\u043e \u0447\u0438\u0441\u043b\u043e\u0432\u044b\u0435 \u0441\u0442\u043e\u043b\u0431\u0446\u044b"}}