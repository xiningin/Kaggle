{"cell_type":{"fbe71cf2":"code","70fcfa97":"code","7d34ec52":"code","39bf8ec6":"code","690504b2":"code","dbc81380":"code","9358f6e6":"code","e9039ed2":"code","bcd947cb":"code","ea74ec39":"code","813c859a":"code","2816db08":"code","80c74513":"code","338428bf":"code","eb4cc7b1":"code","5cf277eb":"code","310dad27":"code","f24f0300":"code","fb6fb415":"markdown"},"source":{"fbe71cf2":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session\n\nimport warnings\nwarnings.filterwarnings('ignore')","70fcfa97":"## MAin aim is to understand more about the data\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n\nfrom scipy import stats\nfrom scipy.stats import norm, skew\nfrom sklearn.preprocessing import LabelEncoder\nfrom scipy import stats\nfrom scipy.stats import norm, skew\nfrom scipy.special import boxcox1p\n\nfrom sklearn import model_selection\n\n## Display all the columns of the dataframe\npd.pandas.set_option('display.max_columns',None)\n","7d34ec52":"from sklearn.linear_model import ElasticNetCV, LassoCV, RidgeCV\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.metrics import mean_squared_error, mean_squared_log_error, mean_absolute_error\nfrom mlxtend.regressor import StackingCVRegressor\nfrom xgboost import XGBRegressor\n","39bf8ec6":"#elasticnet\nelasticnet_alphas = [5e-5, 1e-4, 5e-4, 1e-3]\nelasticnet_l1ratios = [0.8, 0.85, 0.9, 0.95, 1]\n#lasso\nlasso_alphas = [5e-5, 1e-4, 5e-4, 1e-3]\n#ridge\nridge_alphas = [13.5, 14, 14.5, 15, 15.5]\n\n\nMODELS = { \n    \"elasticnet\" : make_pipeline(RobustScaler(), ElasticNetCV(max_iter=1e7, alphas=elasticnet_alphas, l1_ratio=elasticnet_l1ratios)),\n     \"lasso\" : make_pipeline(RobustScaler(), LassoCV(max_iter=1e7, alphas=lasso_alphas, random_state=42)),\n     \"ridge\" : make_pipeline(RobustScaler(), RidgeCV(alphas=ridge_alphas)),\n     \"gradb\" : GradientBoostingRegressor(n_estimators=6000, learning_rate=0.01,\n                                  max_depth=4, max_features='sqrt',\n                                  min_samples_leaf=15, min_samples_split=10,\n                                  loss='huber', random_state=42),\n\n    \"svr\" : make_pipeline(RobustScaler(),\n                    SVR(C=20, epsilon=0.008, gamma=0.0003)),\n\n    \"xgboost\" : XGBRegressor(learning_rate=0.01, n_estimators=6000,\n                       max_depth=3, min_child_weight=0,\n                       gamma=0, subsample=0.7,\n                       colsample_bytree=0.7,\n                       objective='reg:squarederror', nthread=-1,\n                       scale_pos_weight=1, seed=27,\n                       reg_alpha=0.00006, random_state=42)}\n\nMODELS_stack = StackingCVRegressor(regressors=(MODELS['elasticnet'], MODELS['gradb'], MODELS['lasso'], \n                                          MODELS['ridge'], MODELS['svr'], MODELS['xgboost']),\n                              meta_regressor=MODELS['xgboost'],\n                              use_features_in_secondary=True)","690504b2":"def feature_engineering(df):\n    #lib\n\n    temporal_features = [feature for feature in df.columns if 'Yr' in feature or 'Year' in feature or 'Mo' in feature]\n    numeric_features = [feature for feature in df.columns if df[feature].dtype != 'O' and feature not in temporal_features and feature not in (\"Id\", \"kfold\",\"SalePrice\")]\n    categorical_features = [feature for feature in df.columns if df[feature].dtype == 'O' and feature not in temporal_features]\n\n    \n    #feature-eng on temporal-dataset\n\n    for feature in temporal_features:\n        if feature == 'YrSold' or feature == 'MoSold':\n            pass\n        else:\n            df[feature] = df['YrSold'] - df[feature]\n\n    df['YrSold'] = df['YrSold'].astype(str)\n    df['MoSold'] = df['MoSold'].astype(str) \n    df['MSSubClass'] = df['MSSubClass'].apply(str)\n    \n    \n    #fill-na\n\n    for feature in numeric_features:\n        df[feature] = df.groupby(\"Neighborhood\")[feature].transform(lambda x: x.fillna(x.median()))\n\n    for feature in categorical_features:\n        df[feature] = df[feature].fillna(\"Missing\")\n\n    for feature in temporal_features:\n        if feature == 'YrSold' or feature == 'MoSold':\n            df[feature] = df[feature].fillna(\"Missing\")\n        else:\n            df[feature] = df[feature].fillna(0)\n\n    #feature-generation\n\n    df['TotalHouseSF'] = df['TotalBsmtSF'] + df['1stFlrSF'] + df['2ndFlrSF']\n\n    df['TotalLot'] = df['LotFrontage'] + df['LotArea']\n\n    df['TotalBsmtFin'] = df['BsmtFinSF1'] + df['BsmtFinSF2']\n    \n    df['TotalBath'] = df['FullBath'] + df['HalfBath']\n\n    df['TotalPorch'] = df['OpenPorchSF'] + df['EnclosedPorch'] + df['ScreenPorch']\n\n    #feature-selection (multi-correnality)\n\n    #df.drop(['TotalBsmtFin','LotArea','TotalBsmtSF','GrLivArea','GarageYrBlt','GarageArea'],axis=1,inplace=True)\n\n    cols = ('FireplaceQu', 'BsmtQual', 'BsmtCond', 'GarageQual', 'GarageCond', \n        'ExterQual', 'ExterCond','HeatingQC', 'PoolQC', 'KitchenQual', 'BsmtFinType1', \n        'BsmtFinType2', 'Functional', 'Fence', 'BsmtExposure', 'GarageFinish', 'LandSlope',\n        'LotShape', 'PavedDrive', 'Street', 'Alley', 'CentralAir', 'MSSubClass', 'YrSold', 'MoSold')\n    # process columns, apply LabelEncoder to categorical features\n    for c in cols:\n        lbl = LabelEncoder() \n        lbl.fit(list(df[c].values)) \n        df[c] = lbl.transform(list(df[c].values))\n\n    #some more-feature engineering:\n\n    df[\"TotalGarageQual\"] = df[\"GarageQual\"] * df[\"GarageCond\"]\n    df[\"TotalExteriorQual\"] = df[\"ExterQual\"] * df[\"ExterCond\"]\n    \n\n    #df.drop([\"PoolQC\"],axis=1,inplace=True)\n\n    # box_cox\n\n    numeric_feats = [feature for feature in df.columns if df[feature].dtype != \"object\" and feature not in (\"Id\", \"kfold\",\"SalePrice\")]\n    # Check the skew of all numerical features\n    skewed_feats = df[numeric_feats].apply(lambda x: skew(x)).sort_values(ascending=False)\n    skewness = pd.DataFrame({'Skew' :skewed_feats})\n    \n    skewness = skewness[abs(skewness) > 0.75]\n    \n    skewed_features = skewness.index\n    lam = 0.15\n    for feat in skewed_features:\n        df[feat] = boxcox1p(df[feat], lam)\n\n\n    #rare features \n    features = [feature for feature in df.columns if df[feature].dtype == 'O']   \n\n    for feature in features:\n        abc = df[feature].value_counts().to_dict()\n        for key, value in abc.items():\n            if value\/len(df[feature]) < 0.01:\n                df.loc[:,feature][df[feature]==key]=\"rare\"\n\n    return df","dbc81380":"def run(fold,model):\n\n    #loading\n    df = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv')\n    df_test = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv')\n\n    #K-foldding\n    \n    df[\"kfold\"] = -1\n    kf = model_selection.StratifiedKFold(n_splits=4, shuffle=True, random_state=42)\n\n    for f, (train_idx, val_idx) in enumerate(kf.split(X=df, y=df.SalePrice.values)):\n        df.loc[val_idx, 'kfold'] = f\n\n    #function run\n    df = feature_engineering(df)\n    df_test = feature_engineering(df_test)  \n\n     #Some missing values still-were comming!! (please fix it in the above feature-eng(function) otherwise this also works fine)\n    df_test.fillna(0,inplace=True) \n\n    df.SalePrice = np.log1p(df.SalePrice)\n\n    #concat\n    df = pd.concat([df,df_test],axis=0)\n    \n    df = pd.get_dummies(df)\n    \n    #Feature_selection\n\n    for feature in df.columns:\n        all_value_counts = df[feature].value_counts()\n        zero_value_counts = all_value_counts.iloc[0]\n        if zero_value_counts \/ len(df) > 0.99:\n            df.drop(feature,axis=1,inplace=True)\n\n    #split back\n    \n    df_test = df.loc[df[\"Id\"].between(1461,2919)]\n\n    df =  df.loc[df[\"Id\"].between(1,1460)]\n    \n\n    \n    #train-tests-split\n    df_train = df[df.kfold != fold].reset_index(drop=True)\n\n    df_valid = df[df.kfold == fold].reset_index(drop=True)\n\n    numeric_features = [feature for feature in df_train.columns if feature not in (\"Id\", \"kfold\",\"SalePrice\")]\n    \n    x_train = df_train[numeric_features].values\n    x_valid = df_valid[numeric_features].values\n\n    \n    #regressor models\n\n    reg = MODELS[model]\n\n    reg.fit(x_train,df_train.SalePrice.values)\n    valid_preds = reg.predict(x_valid)\n    test_preds = reg.predict(df_test[numeric_features].values)\n    \n\n    # scoring\n    \n    rmse = np.sqrt(mean_squared_error(df_valid.SalePrice.values, valid_preds))\n    mae = mean_absolute_error(df_valid.SalePrice.values, valid_preds)\n    \n    print(f\"FOLD={fold}, MODEL = {model}, RMSE = {rmse}, MAE ={mae} \")\n    return(test_preds)\n","9358f6e6":"preds_df = pd.DataFrame()\n\nfor fold in range(3):\n    for keys,items in MODELS.items():    \n        preds_df[\"fold\"+str(fold)+keys] = run(fold,keys)\n        ","e9039ed2":"#if you want to run,stacked regressor also. (it takes time, but results are good)\n\ndef run(fold,model):\n\n    #loading\n    df = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv')\n    df_test = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv')\n\n    #K-foldding\n    \n    df[\"kfold\"] = -1\n    kf = model_selection.StratifiedKFold(n_splits=4, shuffle=True, random_state=42)\n\n    for f, (train_idx, val_idx) in enumerate(kf.split(X=df, y=df.SalePrice.values)):\n        df.loc[val_idx, 'kfold'] = f\n\n    #function run\n    df = feature_engineering(df)\n    df_test = feature_engineering(df_test)  \n\n    #Some missing values still-were comming!! (please fix it in the above feature-eng(function) otherwise this also works fine)\n    df_test.fillna(0,inplace=True) \n\n    df.SalePrice = np.log1p(df.SalePrice)\n\n    #concat\n    df = pd.concat([df,df_test],axis=0)\n    \n    df = pd.get_dummies(df)\n    \n    #Feature_selection\n\n    for feature in df.columns:\n        all_value_counts = df[feature].value_counts()\n        zero_value_counts = all_value_counts.iloc[0]\n        if zero_value_counts \/ len(df) > 0.99:\n            df.drop(feature,axis=1,inplace=True)\n\n    #split back\n    \n    df_test = df.loc[df[\"Id\"].between(1461,2919)]\n\n    df =  df.loc[df[\"Id\"].between(1,1460)]\n    \n\n    \n    #train-tests-split\n    df_train = df[df.kfold != fold].reset_index(drop=True)\n\n    df_valid = df[df.kfold == fold].reset_index(drop=True)\n\n    numeric_features = [feature for feature in df_train.columns if feature not in (\"Id\", \"kfold\",\"SalePrice\")]\n    \n    x_train = df_train[numeric_features].values\n    x_valid = df_valid[numeric_features].values\n\n    \n    #regressor models\n\n    reg = MODELS_stack\n\n    reg.fit(x_train,df_train.SalePrice.values)\n    valid_preds = reg.predict(x_valid)\n    test_preds = reg.predict(df_test[numeric_features].values)\n    \n\n    # scoring\n    \n    rmse = np.sqrt(mean_squared_error(df_valid.SalePrice.values, valid_preds))\n    mae = mean_absolute_error(df_valid.SalePrice.values, valid_preds)\n    \n    print(f\"FOLD={fold}, MODEL = {model}, RMSE = {rmse}, MAE ={mae} \")\n    return(test_preds)\n\n\nfor fold in range(3):\n    preds_df[\"fold\"+str(fold)+\"stack\"] = run(fold,keys)","bcd947cb":"preds_df","ea74ec39":"for cols in preds_df.columns:\n    preds_df[cols] = np.expm1(preds_df[cols])\n","813c859a":"preds_df","2816db08":"#Weighted_average aka blending # adjust if you did not use stacked-regressor. \n\nfor e,col in enumerate(preds_df.columns):\n    if e == 0:\n        preds_df['SalePrice'] = preds_df[col]\n    elif col in ['fold0stack','fold1stack','fold2stack']:\n        preds_df['SalePrice'] += preds_df[col]*4\n    else:\n        preds_df['SalePrice'] += preds_df[col]    \n","80c74513":" preds_df['SalePrice'] =  preds_df['SalePrice']\/30","338428bf":"df_test = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv',usecols=[\"Id\"])","eb4cc7b1":"preds_df[\"id\"] = df_test.values.flatten()","5cf277eb":"submission  = preds_df[['id','SalePrice']]","310dad27":"submission","f24f0300":"submission.to_csv(\"..\/..\/kaggle\/working\/submission.csv\", index=False)","fb6fb415":"### Upvote if you like!!!!!! Cheers"}}