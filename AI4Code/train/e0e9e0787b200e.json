{"cell_type":{"8f999bcd":"code","6fdb910f":"code","2e638ba6":"code","51d87ea9":"code","69557b53":"code","3ad7adcc":"code","da962d90":"code","17537637":"code","20602e6a":"code","50ce9010":"code","a65162e7":"code","96b728f0":"code","70ed1fda":"code","67f3c779":"code","72bd6fed":"code","bc4bc4a1":"code","612bf007":"code","bbd136db":"code","517af848":"code","33aa59e2":"code","f44a55d4":"code","dd86fef7":"code","b89586c4":"code","1572f9a4":"code","9c833e77":"code","462cdefb":"code","3dfc0c54":"code","2f5ff13d":"code","75a5e6eb":"code","2c3fd135":"code","e82501f9":"code","d95bf3c3":"code","e970d055":"code","a123bb54":"code","caba5f45":"code","2ad82d28":"code","2414bfb6":"code","d6a264d1":"code","dc1f906f":"code","14734abf":"code","2e7e4514":"code","08cc8843":"code","13c7ed53":"code","78c68983":"code","50726b1e":"code","e57b5be6":"code","025ab18e":"code","f7d5577b":"code","0a016df1":"code","52a5bc0a":"code","f47b0779":"code","ecc133f3":"code","e07d6205":"markdown","44f1a085":"markdown","80af08a3":"markdown","49ba1ddd":"markdown","19855e7a":"markdown"},"source":{"8f999bcd":"from __future__ import print_function, division\nfrom builtins import range, input\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom zipfile import ZipFile\nfrom pathlib import Path\nimport numpy as np\nimport pandas as pd\nimport pickle\nimport matplotlib.pyplot as plt\n#from sklearn.preprocessing import LabelEncoder","6fdb910f":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        os.path.join(dirname, filename)\ndf = pd.read_csv('\/kaggle\/input\/book-crossing-pre-processing\/pre_processed.csv')\n","2e638ba6":"%%capture\n'''\nbookcrossing_data_file_url = (\n    \"http:\/\/www2.informatik.uni-freiburg.de\/~cziegler\/BX\/BX-CSV-Dump.zip\"\n)\nBX_zipped_file = keras.utils.get_file(\n    \"BX-CSV-Dump.zip\", bookcrossing_data_file_url, extract=False\n)\nkeras_datasets_path = Path(BX_zipped_file).parents[0]\nBX_dir = keras_datasets_path \/ 'book-crossing'\n\n# Only extract the data the first time the script is run.\nif not BX_dir.exists():\n    with ZipFile(BX_zipped_file, \"r\") as zip:\n         #Extract files\n        print(\"Extracting all the files now...\")\n        zip.extractall(path=BX_dir)\n        print(\"Done!\")\n\nratings_file = BX_dir \/ 'BX-Book-Ratings.csv'\nr_cols = ['user_id', 'book_id', 'rating']\ndf = pd.read_csv(ratings_file, sep=';', names=r_cols, encoding='latin-1', low_memory=False)\ndf = df.drop(df.index[0])\n\n#transforma ratings em valor float\ndf[\"rating\"] = df[\"rating\"].values.astype(np.float32)\n\n\n#------apenas para o dataset do bookcrossing\n# manter apenas as avalia\u00e7\u00f5es explicitas - nota 0 signfica que o usuario apenas interagiu com o produto de alguma forma\ndf=df.loc[df['rating']>0]\n\n#colocar ratings que sao de 1-10 para serem de 0.5 a 5\n#df['rating']=df[\"rating\"].apply(lambda x: x\/2)\n#-------------------------------------------------\n\n#tira uma amostra menor do dataset\n#df = df.sample(frac=0.01, random_state=42)\n\n\n#transforma os ids dos usuarios e livros \nuser_ids = df[\"user_id\"].unique().tolist()\nuser2user_encoded = {x: i for i, x in enumerate(user_ids)}\nuserencoded2user = {i: x for i, x in enumerate(user_ids)}\nbook_ids = df[\"book_id\"].unique().tolist()\nbook2book_encoded = {x: i for i, x in enumerate(book_ids)}\nbook_encoded2book = {i: x for i, x in enumerate(book_ids)}\ndf[\"user\"] = df[\"user_id\"].map(user2user_encoded)\ndf[\"book\"] = df[\"book_id\"].map(book2book_encoded)\n\nn_users = len(user2user_encoded)\nn_books = len(book_encoded2book)\n\n# min and max ratings will be used to normalize the ratings later\nmin_rating = min(df[\"rating\"])\nmax_rating = max(df[\"rating\"])\n\nprint(\n    \"Number of users: {}, Number of books: {}, Min rating: {}, Max rating: {}\".format(\n        n_users, n_books, min_rating, max_rating\n    )\n)\n\n'''","51d87ea9":"%%capture\n'''\ng = df.groupby('user_id')['rating'].count()\ntop_users = g.sort_values(ascending=False)[:15]\n\ng = df.groupby('book_id')['rating'].count()\ntop_books = g.sort_values(ascending=False)[:15]\n\ntop_r = df.join(top_users, rsuffix='_r', how='inner', on='user_id')\ntop_r = top_r.join(top_books, rsuffix='_r', how='inner', on='book_id')\n\nprint(pd.crosstab(top_r.user_id, top_r.book_id, top_r.rating, aggfunc=np.sum))\n'''\n","69557b53":"df.head()","3ad7adcc":"df.dtypes","da962d90":"#criando uma lista com os user e livros unicos do dataset\nusers=set(df['user_id'].unique())\nitems=set(df['book_id'].unique())\n#armazenando a quantidade de itens e users unicos\nn_users = len(users)\nn_books = len(items)\n\n#armazenando as avalia\u00e7\u00f5es minimas e m\u00e1ximas\nmin_rating = min(df[\"rating\"])\nmax_rating = max(df[\"rating\"])\n\nprint(\n    \"Number of users: {}, Number of books: {}, Min rating: {}, Max rating: {}\".format(\n        n_users, n_books, min_rating, max_rating\n    )\n)","17537637":"#faz uma copia do dataset\ndf_original=df.copy()","20602e6a":"from sklearn.model_selection import train_test_split","50ce9010":"#verificar distribui\u00e7\u00e3o de ratings\ndf['rating'].value_counts()","a65162e7":"#droxar user e item index (vou fazer de novo depois)\ndf = df.drop(['user', 'book'], axis=1)\ndf.head()","96b728f0":"#dataset with unique users\nusers_unique = df['user_id'].unique()\n#permutar users\nusers_unique = np.random.permutation(users_unique)\n#dividir dataset em propor\u00e7\u00e3o 0.9-0.1\nnormalfrac=0.9\nusers_normal = users_unique[:np.int(np.round(len(users_unique)*normalfrac))]\ndfnormal=df.loc[df['user_id'].isin(users_normal)]\ndfsimulate=df.loc[~df['user_id'].isin(users_normal)]\ndfnormal.shape, dfsimulate.shape","70ed1fda":"# split dfnormal into train and validation\ntemptrain,tempvalid = train_test_split(dfnormal,test_size=0.1,train_size=0.9,random_state=11,shuffle=True)","67f3c779":"#etiquetar os datasets com o tipo deles\ntemptrain['random_dstype']='train'\ntempvalid['random_dstype']='test'","72bd6fed":"#concatenar de novo\ndfnormal=pd.concat([temptrain,tempvalid],axis=0)\ndfnormal['random_dstype'].value_counts()","bc4bc4a1":"#permutar o outro dataset que tinha 0.1 do orignal\nsimulateusers=dfsimulate['user_id'].unique()\nsimulateusers=np.random.permutation(simulateusers)\n#pegar 0.9 para ser o dataset de train do simulate e dividir em train e validation de novo\nsimulateusers_train=simulateusers[:np.int(np.round(len(simulateusers)*normalfrac))]\ndfsimulate_train=dfsimulate.loc[dfsimulate['user_id'].isin(simulateusers_train)]\ndfsimulate_test=dfsimulate.loc[~dfsimulate['user_id'].isin(simulateusers_train)]","612bf007":"#etiquetar os datasets com o tipo deles\ndfsimulate_train['random_dstype']='train'\ndfsimulate_test['random_dstype']='test'","bbd136db":"#concatenar de novo\ndfsimulate=pd.concat([dfsimulate_train,dfsimulate_test],axis=0)","517af848":"#concatenar o dfsimulate com o dfnormal e formar o dataset completo de novo\ndf=pd.concat([dfnormal,dfsimulate],axis=0)","33aa59e2":"#dividir dataset completo em train e validation\ndftrain=df.loc[df['random_dstype']=='train']\ndfvalid=df.loc[df['random_dstype']!='train']\n\ndftrain.shape, dfvalid.shape","f44a55d4":"dftrain.head()","dd86fef7":"#mapear users e itens de novo\nuser_to_idx={j:i+1 for i, j in enumerate(df['user_id'].unique())}\nitem_to_idx={j:i+1 for i,j in enumerate(df['book_id'].unique())}\nidx_to_user={i+1:j for i, j in enumerate(df['user_id'].unique())}\nidx_to_item={i+1:j for i,j in enumerate(df['book_id'].unique())}\n\ndftrain['user_idx']=[user_to_idx[i] for i in dftrain['user_id']]\ndftrain['item_idx']=[item_to_idx[i] for i in dftrain['book_id']]\ndfvalid['user_idx']=[user_to_idx[i] for i in dfvalid['user_id']]\ndfvalid['item_idx']=[item_to_idx[i] for i in dfvalid['book_id']]","b89586c4":"#dataset apenas com user id, book id e rating\ndfinp_train=dftrain[['user_idx','item_idx','rating']]\ndfinp_test=dfvalid[['user_idx','item_idx','rating']]","1572f9a4":"dfinp_train.head()","9c833e77":"dftrain.reset_index(inplace=True)\ndfvalid.reset_index(inplace=True)\n\ndftrain.head()","462cdefb":"#colocar uma flag de train e test nos datasets\ndftrain['flag_train']=1\ndfvalid['flag_train']=0","3dfc0c54":"#average do dataset de validacao\navg = dfvalid['rating'].mean()\nprint(avg)","2f5ff13d":"dftrain.isnull().sum()","75a5e6eb":"#criar uma coluna int bin\u00e1ria para indicar os valores nulos da coluna country\nfor X in ['country']:\n    dftrain[X+'_NA']=dftrain[X].isnull().astype(int)\n    dfvalid[X+'_NA']=dfvalid[X].isnull().astype(int)\n#substitui os valores null por -1\nfor X in ['country']:\n    dftrain[X].fillna('-1',inplace=True)\n    dfvalid[X].fillna('-1',inplace=True)","2c3fd135":"#dataset after cross validation \ndf_final = pd.concat([dftrain,dfvalid],axis=0)\ndf_final.head()","e82501f9":"#indexing side info columns \nPublisher_to_idx={j:i+1 for i,j in enumerate(df_final['publisher'].unique())}\nidx_to_Publisher={i+1:j for i,j in enumerate(df_final['publisher'].unique())}\nYearOfPublication_to_idx={j:i+1 for i,j in enumerate(df_final['year_of_publication'].unique())}\nidx_to_YearOfPublication={i+1:j for i,j in enumerate(df_final['year_of_publication'].unique())}\nBookAuthor_to_idx={j:i+1 for i,j in enumerate(df_final['book_author'].unique())}\nidx_to_BookAuthor={i+1:j for i,j in enumerate(df_final['book_author'].unique())}\nCountry_to_idx={j:i+1 for i,j in enumerate(df_final['country'].unique())}\nidx_to_Country={i+1:j for i,j in enumerate(df_final['country'].unique())}\nLanguage_to_idx={j:i+1 for i,j in enumerate(df_final['Language'].unique())}\nidx_to_Language={i+1:j for i,j in enumerate(df_final['Language'].unique())}\nCategory_to_idx={j:i+1 for i,j in enumerate(df_final['Category'].unique())}\nidx_to_Category={i+1:j for i,j in enumerate(df_final['Category'].unique())}\nAge_to_idx={j:i+1 for i,j in enumerate(df_final['age'].unique())}\nidx_to_Age={i+1:j for i,j in enumerate(df_final['age'].unique())}","d95bf3c3":"dftrain['Publisher_idx']=[Publisher_to_idx.get(i) for i in dftrain['publisher']]\ndfvalid['Publisher_idx']=[Publisher_to_idx.get(i) for i in dfvalid['publisher']]","e970d055":"dftrain['YearOfPublication_idx']=[YearOfPublication_to_idx.get(i) for i in dftrain['year_of_publication']]\ndfvalid['YearOfPublication_idx']=[YearOfPublication_to_idx.get(i) for i in dfvalid['year_of_publication']]","a123bb54":"dftrain['BookAuthor_idx']=[BookAuthor_to_idx.get(i) for i in dftrain['book_author']]\ndfvalid['BookAuthor_idx']=[BookAuthor_to_idx.get(i) for i in dfvalid['book_author']]","caba5f45":"dftrain['Country_idx']=[Country_to_idx.get(i) for i in dftrain['country']]\ndfvalid['Country_idx']=[Country_to_idx.get(i) for i in dfvalid['country']]","2ad82d28":"dftrain['Language_idx']=[Language_to_idx.get(i) for i in dftrain['Language']]\ndfvalid['Language_idx']=[Language_to_idx.get(i) for i in dfvalid['Language']]","2414bfb6":"dftrain['Category_idx']=[Category_to_idx.get(i) for i in dftrain['Category']]\ndfvalid['Category_idx']=[Category_to_idx.get(i) for i in dfvalid['Category']]","d6a264d1":"dftrain['Age_idx']=[Age_to_idx.get(i) for i in dftrain['age']]\ndfvalid['Age_idx']=[Age_to_idx.get(i) for i in dfvalid['age']]","dc1f906f":"#side info columns\nsideCols=['Publisher_idx','YearOfPublication_idx','BookAuthor_idx','Country_idx', 'Language_idx','Category_idx','Age_idx']","14734abf":"sideCols","2e7e4514":"dftrain.columns","08cc8843":"dftrain[sideCols].head()","13c7ed53":"dftrain[sideCols].max()","78c68983":"#OLD CODE FROM NOW ON\n#this is an on going project","50726b1e":"# split into train and test\n\nx = df[[\"user\", \"book\"]].values\n# Normalize the targets between 0 and 1. Makes it easy to train.\ny = df[\"rating\"].apply(lambda x: (x - min_rating) \/ (max_rating - min_rating)).values\n#y = df[\"rating\"].values\n# Assuming training on 90% of the data and validating on 10%.\nX_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.5, random_state=42)\n\nX_train_array = [X_train[:, 0], X_train[:, 1]]\nX_test_array = [X_test[:, 0], X_test[:, 1]]\n\n#media = y_train.mean()\n\nprint(X_train.shape, X_test.shape, y_train.shape, y_test.shape, media)\n","e57b5be6":"from tensorflow import keras\nimport tensorflow as tf\nfrom keras.models import Model\nfrom keras.layers import Input, Reshape, Dot\nfrom keras.layers import Add, Activation, Lambda, BatchNormalization, Flatten\nfrom keras.layers import Concatenate, Dense, Dropout\nfrom keras.layers.embeddings import Embedding\nfrom keras.optimizers import Adam, SGD\nfrom keras.regularizers import l2,l1","025ab18e":"n_factors = 10\n\n\nclass EmbeddingLayer:\n    def __init__(self, n_items, n_factors, embs):\n        self.n_items = n_items\n        self.n_factors = n_factors\n        self.embs = embs\n        \n    \n    def __call__(self, x):\n        x = Embedding(self.n_items, self.n_factors, embeddings_initializer='he_normal',\n                      embeddings_regularizer=l2(1e-6))(x)\n        x = Reshape((self.n_factors,))(x)\n        return x\n\n\ndef RecommenderNet(n_users, n_books, n_factors, min_rating, max_rating):\n    user = Input(shape=(1,))\n    u = EmbeddingLayer(n_users, n_factors)(user)\n    \n    book = Input(shape=(1,))\n    m = EmbeddingLayer(n_books, n_factors)(book)\n\n    x = Concatenate()([u, m])\n    x = Dropout(0.05)(x)\n\n    x = Dense(256, kernel_initializer='he_normal', kernel_regularizer=l1(1e-4), bias_regularizer=l2(1e-4))(x)\n    x = Activation('relu')(x)\n    x = Dropout(0.25)(x)\n    #x = BatchNormalization()(x)\n    \n    x = Dense(128, kernel_initializer='he_normal', kernel_regularizer=l2(1e-4), bias_regularizer=l2(1e-4))(x)\n    x = Activation('relu')(x)\n    x = Dropout(0.25)(x)\n    #x = BatchNormalization()(x)\n\n    x = Dense(10, kernel_initializer='he_normal', kernel_regularizer=l2(1e-4))(x)\n    x = Activation('relu')(x)\n    x = Dropout(0.25)(x)\n    #x = BatchNormalization()(x)\n    \n    x = Dense(1, kernel_initializer='he_normal')(x)\n    #x = Activation('relu')(x)\n    x = Activation('sigmoid')(x)\n    #x = Lambda(lambda x: x * (max_rating - min_rating) + min_rating)(x)\n    \n    lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n        initial_learning_rate=1e-2,\n        decay_steps=10000,\n        decay_rate=0.9)\n    opt = keras.optimizers.SGD(learning_rate=lr_schedule)\n\n    model = Model(inputs=[user, book], outputs=x)\n    #opt = Adam(lr=0.001)\n    #opt=SGD(lr=0.0025)\n    model.compile(loss='mean_squared_error', optimizer=opt)\n    \n    return model","f7d5577b":"model = RecommenderNet(n_users, n_books, n_factors, min_rating, max_rating)\nmodel.summary()","0a016df1":"callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n    # This callback will stop the training when there is no improvement in  \n    # the validation loss for three consecutive epochs.  \n    \nhistory = model.fit(x=X_train_array, \n                    y=y_train, \n                    batch_size=64, \n                    epochs=100,\n                    callbacks=[callback],\n                    verbose=1, \n                    validation_data=(X_test_array, y_test)\n                    )","52a5bc0a":"# plot losses\nplt.plot(history.history['loss'], label=\"train loss\")\nplt.plot(history.history['val_loss'], label=\"test loss\")\nplt.legend()\nplt.show()","f47b0779":"# plot mae\nplt.plot(history.history['loss'], label=\"train loss\")\nplt.plot(history.history['val_loss'], label=\"test loss\")\nplt.legend()\nplt.show()","ecc133f3":"# plot rmse\nplt.plot(history.history['loss'], label=\"train loss\")\nplt.plot(history.history['val_loss'], label=\"test loss\")\nplt.legend()\nplt.show()","e07d6205":"### **Deal with NULL values**","44f1a085":"## **Cross-Validation**","80af08a3":"### **Create Embeddings**","49ba1ddd":"### **Index side information columns**","19855e7a":"# Book-Crossing: Hibrid Recommendation System with Deep Neural Networks"}}