{"cell_type":{"8590065f":"code","46723aad":"code","e55023fa":"code","1ce8620a":"code","8105986f":"code","8d1dafb9":"code","0cbfed02":"code","2b653609":"code","3caead5f":"code","3fd1b211":"code","5ef73004":"code","a62219ab":"code","a15821c7":"code","b9e6a475":"code","31cc198c":"code","e914badb":"code","0e28e690":"code","325a7d65":"code","b0df217f":"code","1f7f1fe8":"code","fa993a98":"code","9ce6b08e":"markdown","5b4836dc":"markdown","5b3fc217":"markdown","48f88469":"markdown","545739ce":"markdown","ffb6b2b1":"markdown","02de2943":"markdown","24a83910":"markdown","c2b710a0":"markdown","93f32fcd":"markdown","94e59a28":"markdown"},"source":{"8590065f":"import numpy as np \nimport torch\nfrom torch import nn\nfrom torch import optim\nimport torchvision\nimport torchvision.transforms as transforms\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport torchvision.models as models     \nfrom collections import defaultdict, namedtuple","46723aad":"#set device and seed\ndevice = 'cuda'\ntorch.manual_seed(1)\ntorch.cuda.manual_seed(1)\nnp.random.seed(1)\ntorch.backends.cudnn.deterministic = True","e55023fa":"transform = transforms.Compose(\n    [transforms.Resize(224),\n     transforms.ToTensor(),\n     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n\ntrainset = torchvision.datasets.CIFAR10(root='.\/data', train=True,\n                                        download=True, transform=transform)\n\ntrainloader = torch.utils.data.DataLoader(trainset, batch_size=256,\n                                          shuffle=True, num_workers=0)\n\ntestset = torchvision.datasets.CIFAR10(root='.\/data', train=False,\n                                       download=True, transform=transform)\n\ntestloader = torch.utils.data.DataLoader(testset, batch_size=256,\n                                         shuffle=False, num_workers=0)","1ce8620a":"def getmodel():\n    model = models.resnet34(pretrained=False)\n    model.fc.out_features = 10\n    return model.to(device)","8105986f":"class stat:\n    '''\n    Store a single-valued descriptor of the gradients, as mean or std,\n    after every minibatch\n    '''\n    def __init__(self,fn):\n        self.values = defaultdict(list)\n        self.fn = fn\n        \n    def update(self,grads):\n        for k,v in grads.items():\n            self.values[k].append(self.fn(v))\n            \n    def plot(self,labels):\n        fig, ax = plt.subplots(1,1)\n        x = [i for i in range(len(self.values[0]))]\n        for k,v in self.values.items():\n            ax.plot(x,v,label = labels[k])\n        ax.legend()\n        plt.show()","8d1dafb9":"class histogram:\n    '''\n    Store a histogram of the gradients after every minibatch\n    '''\n    def __init__(self,bins = None, bound = 5e-4):\n        self.values = defaultdict(list)\n        self.bins = bins\n        self.bound = bound\n        \n    def update(self,grads):\n        if not self.bins:\n            self.bins = {k: int(1 + np.log(len(v))\/np.log(2)) for k,v in grads.items()}\n        for k,v in grads.items():\n            h = v.histc(bins=self.bins[k],min=-self.bound,max=self.bound).detach().tolist()\n            self.values[k].append(h)\n            \n    def plot(self,labels,**kwargs):\n        rows, cols = len(self.values)\/\/2, 2\n        fig, axs = plt.subplots(rows,cols,figsize = (15,5))\n        axs = axs.flatten()\n        fig.patch.set_facecolor('w')\n        for (k,v),ax in zip(self.values.items(),axs):\n            ax.imshow(np.array(self.values[k]).T,extent = (0,200,-15,15),**kwargs)\n            ax.set_title(labels[k])\n            ax.set_yticklabels('')\n        fig.suptitle('Gradients Histograms over one epoch')\n        plt.show()","0cbfed02":"class modelgrads:\n    '''\n    Main module. Get the gradients, update the stats, and it had some \n    other utility functions\n    \n    model: a torch resnet\n    stats: a list of objects with an update method implemented \n    '''\n    def __init__(self,model,stats=None):\n        self.model = model\n        self.stats = stats\n        self._set_groups()\n        \n    def layers(self): \n        for m in self.model.modules():\n            if hasattr(m,'weight'):\n                yield m\n        \n    def _set_groups(self):\n        layer_type = defaultdict(int,{nn.Linear: 1, nn.Conv2d: 2}) \n        for m in self.layers():\n            setattr(m,'group',layer_type[type(m)])  \n            if isinstance(m,nn.Conv2d) and m.in_channels != m.out_channels and m.kernel_size == (1,1):\n                layer_type[nn.Conv2d] += 1\n        self._nconv = layer_type[nn.Conv2d]\n    \n    def layer_groups(self):\n        layer_groups = defaultdict(list)\n        for m in self.layers(): layer_groups[m.group].append(m)\n        return layer_groups\n    \n    def labels(self):\n        non_conv = {0: 'BatchNorm Layers', 1: 'Linear Layers'}\n        conv = {k: f'Conv Layer {k-1}' for k in range(2,self._nconv+1)}\n        return {**non_conv,**conv}\n        \n    def _get_grads(self):\n        grads = defaultdict(torch.cuda.FloatTensor)\n        for m in self.layers(): \n            grads[m.group] = torch.cat((grads[m.group], m.weight.grad.flatten()))\n        return grads\n\n    def update_stats(self):\n        grads = self._get_grads()\n        for stat in self.stats:\n            stat.update(grads)","2b653609":"def train_loop(model, optimizer, loss_fn, epochs, grads=None):\n    \n    for _ in range(epochs):\n        model.train()\n        current = 0\n        for img, lab in trainloader:\n            optimizer.zero_grad()\n            out = model(img.float().to(device))\n            loss = loss_fn(out, lab.cuda().to(device))\n            loss.backward()\n            optimizer.step()\n            current += loss.item()\n            if grads:\n                grads.update_stats()\n                \n        train_loss = current \/ len(trainloader)\n                \n        with torch.no_grad():\n            current, acc = 0, 0\n            model.eval()\n            for img, lab in testloader:\n                out  = model(img.float().to(device))\n                loss = loss_fn(out, lab.to(device))\n                current += loss.item() \n                _, pred = nn.Softmax(-1)(out).max(-1)\n                acc += (pred == lab.cuda()).sum().item()\n\n            valid_loss = current \/ len(testloader)\n            accuracy   = 100 * acc \/ len(testset)\n\n        print(f'Train loss: {train_loss:.2f}, Validation loss: {valid_loss:.2f}, Accuracy: {accuracy}')","3caead5f":"resnet = getmodel()","3fd1b211":"absmean = stat(fn = lambda v: v.abs().mean().detach().item())\nmean = stat(fn = lambda v: v.mean().detach().item())\nstd = stat(fn = lambda v: v.std().detach().item())\nhist = histogram()","5ef73004":"resnet_grads = modelgrads(resnet, stats = [absmean, mean, std, hist])","a62219ab":"optimizer = optim.Adam(resnet.parameters(), lr =1e-3)\nloss_fn = nn.CrossEntropyLoss()","a15821c7":"train_loop(model = resnet, optimizer = optimizer, loss_fn = loss_fn, epochs = 1, grads = resnet_grads)","b9e6a475":"labels = resnet_grads.labels()","31cc198c":"hist.plot(labels, cmap = 'rainbow')","e914badb":"absmean.plot(labels)","0e28e690":"mean.plot(labels)","325a7d65":"std.plot(labels)","b0df217f":"def optim_diff(layer_groups, lr, cr):\n    opt = []\n    for k,group in groups.items():\n        if k == 1:\n            opt.append({'params': [par for l in group for par in l.parameters()], \n                        'lr': 1e-2})\n        else:\n            opt.append({'params': [par for l in group for par in l.parameters()], \n                        'lr': lr*(1+cr)**(k-1)})  \n    return optim.Adam(opt, lr = lr)","1f7f1fe8":"resnet = getmodel()\ngroups = modelgrads(resnet).layer_groups()\noptimizer = optim_diff(groups, 1e-3,.1)\ntrain_loop(model = resnet, optimizer = optimizer, loss_fn = loss_fn, epochs = 5)","fa993a98":"resnet = getmodel()\ngroups = modelgrads(resnet).layer_groups()\noptimizer = optim.Adam(resnet.parameters(), lr =1e-3)\ntrain_loop(model = resnet, optimizer = optimizer, loss_fn = loss_fn, epochs = 5)","9ce6b08e":"We'll train the net with a constant learning rate and with differential learning rates and see which one performs better.","5b4836dc":"We will train the model just for one epoch","5b3fc217":"### Setting the learning rates\n\nWe'll set the learning rates in a way that they grow exponentially, starting at the layers near to the input and increasing it as we go to the ones near to the output. ","48f88469":"Next we have the training loop. It's nothing special, it just have a call back to the update method.","545739ce":"In the next notebook we will explore the effects of setting different learning rates for different layers.\n\nThe first time i heard about this idea of picking different learning rates for different layers was on the fast-ai course in the context of transer-learning, i.e, using a pre-trained model on a related task. The intuition behind that is pre-trained weights are already good so there's no need to change them a lot. It's still unclear if this true for every transfer learning application, but the same question could be done to the whole deep learning world. It is worth to set different learning rates for different layers? Let's see if we can answer that question. But first we need to know which layers should get higher learning rates. In order to do that we'll investigate more about the gradients of a classical resnet model","ffb6b2b1":"We we'll work with the cifar10 dataset, which consist in images of 10 different classes. I took the next code from the pytorch tutorials","02de2943":"The way we are going to group the layers it's quite arbitrary and simple. All batchnorm layers are togheter, convolutional layers with the same kernel size get in the same group and finally the last linear layer alone.","24a83910":"Let's talk about the gradients. It's imposible for me to save them for every minibatch, but you don't want to have access to every specific gradient weight all the time. Instead you want to know how do they behave in general, so, after grabing the gradients at the end of the batch, we'll store some descriptors on the fly. \n\nWe won't compute those statistics for every single layer of the architecture, we'll group of layers that will be defined below.","c2b710a0":"Looking at the results, it's clear that as we go deeper in the net the gradients get smaller and smaller. The gradients of the last group of the final linear layer are almost zero after a few batches. To be honest, to this point I really thought layers near the input should have lower gradients because of the chain rule (backpropagation).\n\nSay $L$ is the loss function; $l_1,l_2$ are the two last layers which depends on the parameters $\\alpha,\\beta$ respectively and $v = l_1(\\alpha,u), w = l_2(\\beta,v)$. Then we have\n\n\\begin{align*}\n\\frac{\\partial L}{\\partial \\beta} =  \\frac{\\partial L}{\\partial w}\\frac{\\partial w}{\\partial \\beta}\\\\\n\\frac{\\partial L}{\\partial \\alpha} =  \\frac{\\partial L}{\\partial w}\\frac{\\partial w}{\\partial v}\\frac{\\partial v}{\\partial \\alpha}\n\\end{align*}\n\nAs we go more layers back, more therms appear into the product above. If they are small (less than one), the product goes to zero. Doesn't make much sense when we look at the plots so we can only expect at some point those derivatives to be greater than one. Take a moment to think about what they are: functions on the activations and weights. Activations (we hope) have mean zero and deviation one. Weights, according to torch docs are drawn from \n\n\\begin{align*}\n\\mathcal{U}(-\\sqrt{\\nu},\\sqrt{\\nu})\\\\\n\\nu = \\frac{1}{C_{in}\\cdot k^2}\\\\\n\\end{align*}\n\nwhere $C_{in}$ are the input channels and $k$ the kernel size, so weights are small too, at least at the beginning of the training. What happened here? The only explanation i find to this phenomenon is gradients functions at some point are huge sums and products and considering at some layers all negative activations are nullified by relu, it makes some sense those products add enough to be greater than one.\n\nAnyway, I'm not going to go deeper into this topic, although it seems interesting. Let's move onto the next section.","93f32fcd":"Our model to use will be a non pre-trained resnet 34. This little function it's for re-download the model and reset its parameters","94e59a28":"After 5 epochs, I can't say which optimization method performs better. I actually tried with several different increments (5,10,15 and 20\\%) and none of those gives a reliable boosting in validation loss or accuracy.  \nA few reasons this is happening could be:\n\n* Adam already take into account the gradient size and it doesn't substract the raw gradients \n* The change or increase rate parameter needs to be much more carefully fine tuned, along the learning rate, to really see improvements \n\nAnyway, it was super interesting to see how the gradients of model evolved during the training. I'm pretty sure this behavior it's the same for all the resnet architectures and it was kind of surprising the deeper layers have smaller gradients. "}}