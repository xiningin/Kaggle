{"cell_type":{"7e194367":"code","4c667a0a":"code","b7c58855":"code","5edbf2fb":"code","3ff00ec7":"code","578f68c7":"code","53d7a360":"code","52dfebb7":"code","af6e0860":"code","6059d85e":"code","35c31e0f":"code","98025134":"code","2cb8ba52":"code","e036dc57":"code","9e77d45d":"code","a6af9a3a":"code","32ec1982":"code","dbaa9204":"code","88195d52":"code","2a6a6bbf":"markdown","0e4ccbef":"markdown","7531efd5":"markdown","23ea6aaa":"markdown","64419046":"markdown","8e39010c":"markdown","5aef5910":"markdown","b9106693":"markdown","34a92c72":"markdown","673be6a9":"markdown","17880e22":"markdown","b71480c4":"markdown","992ca47c":"markdown","f70c296d":"markdown","6ef8adba":"markdown","a6695d33":"markdown"},"source":{"7e194367":"import pandas as pd\nimport numpy as np\nimport gc\nimport warnings\nwarnings.filterwarnings(\"ignore\")\napplication_train = pd.read_csv('..\/input\/application_train.csv')","4c667a0a":"application_sample1 = application_train.loc[application_train.TARGET==1].sample(frac=0.1, replace=False)\nprint('label 1 sample size:', str(application_sample1.shape[0]))\napplication_sample0 = application_train.loc[application_train.TARGET==0].sample(frac=0.1, replace=False)\nprint('label 0 sample size:', str(application_sample0.shape[0]))\napplication = pd.concat([application_sample1, application_sample0], axis=0).sort_values('SK_ID_CURR')","b7c58855":"categorical_list = []\nnumerical_list = []\nfor i in application.columns.tolist():\n    if application[i].dtype=='object':\n        categorical_list.append(i)\n    else:\n        numerical_list.append(i)\nprint('Number of categorical features:', str(len(categorical_list)))\nprint('Number of numerical features:', str(len(numerical_list)))","5edbf2fb":"from sklearn.preprocessing import Imputer\napplication[numerical_list] = Imputer(strategy='median').fit_transform(application[numerical_list])","3ff00ec7":"del application_train; gc.collect()\napplication = pd.get_dummies(application, drop_first=True)\nprint(application.shape)","578f68c7":"X = application.drop(['SK_ID_CURR', 'TARGET'], axis=1)\ny = application.TARGET\nfeature_name = X.columns.tolist()","53d7a360":"def cor_selector(X, y):\n    cor_list = []\n    # calculate the correlation with y for each feature\n    for i in X.columns.tolist():\n        cor = np.corrcoef(X[i], y)[0, 1]\n        cor_list.append(cor)\n    # replace NaN with 0\n    cor_list = [0 if np.isnan(i) else i for i in cor_list]\n    # feature name\n    cor_feature = X.iloc[:,np.argsort(np.abs(cor_list))[-100:]].columns.tolist()\n    # feature selection? 0 for not select, 1 for select\n    cor_support = [True if i in cor_feature else False for i in feature_name]\n    return cor_support, cor_feature","52dfebb7":"cor_support, cor_feature = cor_selector(X, y)\nprint(str(len(cor_feature)), 'selected features')","af6e0860":"from sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\nfrom sklearn.preprocessing import MinMaxScaler\nX_norm = MinMaxScaler().fit_transform(X)\nchi_selector = SelectKBest(chi2, k=100)\nchi_selector.fit(X_norm, y)","6059d85e":"chi_support = chi_selector.get_support()\nchi_feature = X.loc[:,chi_support].columns.tolist()\nprint(str(len(chi_feature)), 'selected features')","35c31e0f":"from sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LogisticRegression\nrfe_selector = RFE(estimator=LogisticRegression(), n_features_to_select=100, step=10, verbose=5)\nrfe_selector.fit(X_norm, y)","98025134":"rfe_support = rfe_selector.get_support()\nrfe_feature = X.loc[:,rfe_support].columns.tolist()\nprint(str(len(rfe_feature)), 'selected features')","2cb8ba52":"from sklearn.feature_selection import SelectFromModel\nfrom sklearn.linear_model import LogisticRegression\n\nembeded_lr_selector = SelectFromModel(LogisticRegression(penalty=\"l1\"), '1.25*median')\nembeded_lr_selector.fit(X_norm, y)","e036dc57":"embeded_lr_support = embeded_lr_selector.get_support()\nembeded_lr_feature = X.loc[:,embeded_lr_support].columns.tolist()\nprint(str(len(embeded_lr_feature)), 'selected features')","9e77d45d":"from sklearn.feature_selection import SelectFromModel\nfrom sklearn.ensemble import RandomForestClassifier\n\nembeded_rf_selector = SelectFromModel(RandomForestClassifier(n_estimators=100), threshold='1.25*median')\nembeded_rf_selector.fit(X, y)","a6af9a3a":"embeded_rf_support = embeded_rf_selector.get_support()\nembeded_rf_feature = X.loc[:,embeded_rf_support].columns.tolist()\nprint(str(len(embeded_rf_feature)), 'selected features')","32ec1982":"from sklearn.feature_selection import SelectFromModel\nfrom lightgbm import LGBMClassifier\n\nlgbc=LGBMClassifier(n_estimators=500, learning_rate=0.05, num_leaves=32, colsample_bytree=0.2,\n            reg_alpha=3, reg_lambda=1, min_split_gain=0.01, min_child_weight=40)\n\nembeded_lgb_selector = SelectFromModel(lgbc, threshold='1.25*median')\nembeded_lgb_selector.fit(X, y)","dbaa9204":"embeded_lgb_support = embeded_lgb_selector.get_support()\nembeded_lgb_feature = X.loc[:,embeded_lgb_support].columns.tolist()\nprint(str(len(embeded_lgb_feature)), 'selected features')","88195d52":"pd.set_option('display.max_rows', None)\n# put all selection together\nfeature_selection_df = pd.DataFrame({'Feature':feature_name, 'Pearson':cor_support, 'Chi-2':chi_support, 'RFE':rfe_support, 'Logistics':embeded_lr_support,\n                                    'Random Forest':embeded_rf_support, 'LightGBM':embeded_lgb_support})\n# count the selected times for each feature\nfeature_selection_df['Total'] = np.sum(feature_selection_df, axis=1)\n# display the top 100\nfeature_selection_df = feature_selection_df.sort_values(['Total','Feature'] , ascending=False)\nfeature_selection_df.index = range(1, len(feature_selection_df)+1)\nfeature_selection_df.head(100)","2a6a6bbf":"# <a id='1'>Prepare<\/a>","0e4ccbef":"# <a id='3'>Summary<\/a>","7531efd5":"Enjoy this kenel by creating a model with the above features selected.","23ea6aaa":"- <a href='#1'>Prepare<\/a>  \n- <a href='#2'>Feature Selection<\/a>\n    - <a href='#2-1'>1. Filter<\/a>\n        - <a href='#2-1-1'>1.1 Pearson Correlation<\/a>\n        - <a href='#2-1-2'>1.2 Chi-2<\/a>\n    - <a href='#2-2'>2. Wrapper<\/a>\n    - <a href='#2-3'>3. Embeded<\/a>\n        - <a href='#2-3-1'>3.1 Logistics Regression L1<\/a>\n        - <a href='#2-3-2'>3.2 Random Forest<\/a>\n        - <a href='#2-3-3'>3.3 LightGBM<\/a>\n- <a href='#3'>Summary<\/a>","64419046":"## <a id='2-1'>1 Filter<\/a>\n- documentation for **SelectKBest**: http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.feature_selection.SelectKBest.html\n\n###  <a id='2-1-1'>1.1 Pearson Correlation<\/a>\n**Note**\n- Normalization: no\n- Impute missing values: yes","8e39010c":"# <a id='2'>Feature Selection<\/a>\n- select **100** features from 226\n- **xxx_support**: list to represent select this feature or not\n- **xxx_feature**: the name of selected features","5aef5910":"# Feature Selection\n- This kernel is dedicated to <b>feature selection<\/b> and some common ways to perform feature selection.\n- We'll be using the data from the competition <a href='https:\/\/www.kaggle.com\/c\/home-credit-default-risk'>Home Credit Default Risk<\/a>.","b9106693":"### Feature matrix and target","34a92c72":"###  <a id='2-3-2'>3.2 Random Forest<\/a>\n**Note**\n- Normalization: No\n- Impute missing values: Yes","673be6a9":"###  <a id='2-3-3'>3.3 LightGBM<\/a>\n**Note**\n- Normalization: No\n- Impute missing values: No","17880e22":"## <a id='2-3'>3. Embeded<\/a>\n- documentation for **SelectFromModel**: http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.feature_selection.SelectFromModel.html\n###  <a id='2-3-1'>3.1 Logistics Regression L1<\/a>\n**Note**\n- Normalization: Yes\n- Impute missing values: Yes","b71480c4":"### Deal with Categorical features: OneHotEncoding","992ca47c":"## <a id='2-2'>2. Wrapper<\/a>\n- documentation for **RFE**: http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.feature_selection.RFE.html\n\n**Note**\n- Normalization: depend on the used model; yes for LR\n- Impute missing values: depend on the used model; yes for LR\n","f70c296d":"### Stratified Sampling (ratio = 0.1)","6ef8adba":"### Impute missing values","a6695d33":"###  <a id='2-1-2'>1.2 Chi-2<\/a>\n\n**Note**\n- Normalization: MinMaxScaler (values should be bigger than 0)\n- Impute missing values: yes"}}