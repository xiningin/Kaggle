{"cell_type":{"8bff30bb":"code","1c915724":"code","ca7b2862":"code","f0f3845b":"code","46be1587":"code","e0cf256c":"code","9a8c117b":"code","bbff7abb":"code","761dcac3":"code","3e009f5e":"code","d3d1d2df":"code","89f61743":"code","3c8cc234":"code","7e76a75a":"code","f9d8943b":"code","7bc3a575":"code","0ac72557":"markdown"},"source":{"8bff30bb":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","1c915724":"!pip install iterative-stratification","ca7b2862":"import matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler\n\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n\nfrom keras.layers import Input, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D\nfrom keras import regularizers\nfrom keras.models import Model\n\nfrom tqdm import tqdm","f0f3845b":"test_features = pd.read_csv(\"\/kaggle\/input\/lish-moa\/test_features.csv\")\ntrain_features = pd.read_csv(\"\/kaggle\/input\/lish-moa\/train_features.csv\")\ntrain_targets_scored = pd.read_csv(\"\/kaggle\/input\/lish-moa\/train_targets_scored.csv\")\ntrain_targets_nonscored = pd.read_csv(\"\/kaggle\/input\/lish-moa\/train_targets_nonscored.csv\")\nsample_submission = pd.read_csv(\"\/kaggle\/input\/lish-moa\/sample_submission.csv\")","46be1587":"# Getting glimpse of the training data\ntrain_features.head()","e0cf256c":"# Getting glimpse of the scores. \n# The structure of labelled data is slightly unconventional in this contest since it is separate out into two different dataframes, train_features and train_targets_scored\ntrain_targets_scored.head()","9a8c117b":"train_features.sort_values(by=['sig_id'], inplace=True)\ntrain_targets_scored.sort_values(by=['sig_id'], inplace=True)","bbff7abb":"# Validating each row is a unique sig_id. Usually first step in any EDA to \nprint(train_features.shape)\nprint(train_targets_scored.shape)\nprint(test_features.shape)\n\nprint(\"All ids are unique\" if train_features['sig_id'].nunique() == train_features.shape[0] else \"Redundant Ids\")","761dcac3":"# Looking at the distribution of categorical features\nfg, ax = plt.subplots(1,3, figsize=(20,5))\nax[0].hist(train_features.cp_type)\nax[0].set_title('cp_type')\nax[1].hist(train_features.cp_time)\nax[1].set_title('cp_time')\nax[2].hist(train_features.cp_dose)\nax[2].set_title('cp_dose')","3e009f5e":"# Plotting distribution of labels\nclasses = sorted(train_targets_scored.sum(axis=0, numeric_only=True))\nplt.plot(classes)","d3d1d2df":"# Look at range, mean and median of the g_col and c_col data\n\ng_min_values = []\nc_min_values = []\ng_max_values = []\nc_max_values = []\ng_median_values = []\nc_median_values = []\ng_mean_values = []\nc_mean_values = []\n\nfor i in range(872):\n    g_col = 'g-' + str(i)\n    c_col = 'c-' + str(i)\n    if g_col in train_features.columns:\n        g_min_values.append(train_features[g_col].min())\n        g_max_values.append(train_features[g_col].max())\n        g_median_values.append(train_features[g_col].median())\n        g_mean_values.append(train_features[g_col].mean())\n    if c_col in train_features.columns:\n        c_min_values.append(train_features[c_col].min())\n        c_max_values.append(train_features[c_col].max())\n        c_median_values.append(train_features[c_col].median())\n        c_mean_values.append(train_features[c_col].mean())\n    \ng_min_sorted = sorted(g_min_values)\ng_max_sorted = sorted(g_max_values)\nc_min_sorted = sorted(c_min_values)\nc_max_sorted = sorted(c_max_values)\ng_median_sorted = sorted(g_median_values)\ng_mean_sorted = sorted(g_mean_values)\nc_median_sorted = sorted(c_median_values)\nc_mean_sorted = sorted(c_mean_values)\n\nfg, ax = plt.subplots(2, 4, figsize=(35, 15))\n\nax[0,0].plot(g_min_sorted)\nax[0,0].set_title('min g_col')\nax[0,1].plot(g_max_sorted)\nax[0,1].set_title('max g_col')\nax[0,2].plot(g_median_sorted)\nax[0,2].set_title('median g_col')\nax[0,3].plot(g_mean_sorted)\nax[0,3].set_title('mean g_col')\n\n\nax[1,0].plot(c_min_sorted)\nax[1,0].set_title('min c_col')\nax[1,1].plot(c_max_sorted)\nax[1,1].set_title('max c_col')\nax[1,2].plot(c_median_sorted)\nax[1,2].set_title('median c_col')\nax[1,3].plot(c_mean_sorted)\nax[1,3].set_title('mean c_col')\n","89f61743":"# Look at some sample g-col and c-col data distribution\nrows = 2\ncols = 5\nfig, ax = plt.subplots(rows, cols, figsize=(20,8))\nfor row in range(rows):\n    for col in range(cols):\n        index = row * 2 + col\n        data = sorted(train_features['g-' + str(index)])\n        ax[row, col].plot(data)\n        ax[row, col].set_title('g-' + str(index))\n","3c8cc234":"# Begin Feature transformation.\n# Convert the following to OHE\n# cp_type\n# cp_time\n# cp_dose\n\ndef one_hot_encode(data, column_name):    \n    encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\n    encoded_data = encoder.fit_transform(data[column_name].values.reshape(-1, 1))\n    encoded_df = pd.DataFrame(data=encoded_data, columns=train_features[column_name].unique())\n    merged_data = data.merge(encoded_df, left_index=True, right_index=True)\n    merged_data.drop(columns=[column_name], inplace=True)\n    return encoder, merged_data\n\ncp_type_encoder, train_features = one_hot_encode(train_features, 'cp_type')\ncp_time_encoder, train_features = one_hot_encode(train_features, 'cp_time')\ncp_dose_encoder, train_features = one_hot_encode(train_features, 'cp_dose')\n\ncategorical_feature_columns = []\ncategorical_feature_columns.extend(cp_type_encoder.categories_[0])\ncategorical_feature_columns.extend(cp_dose_encoder.categories_[0])\ncategorical_feature_columns.extend(cp_time_encoder.categories_[0])\n\nprint(categorical_feature_columns)\n\ntrain_features.head()","7e76a75a":"# Convert all g-* and c-* columns to unit variance in the range 0-1\n\ndef standardize_and_normalize(data, columns):\n    \"\"\"\n    Returns a map of column name to a tuple containing the standard scaler and the min max scaler which should be applied to the test data in that order.\n    \"\"\"\n    response = {}\n    \n    for column in tqdm(columns):\n        standard_scaler = StandardScaler()\n        min_max_scaler = MinMaxScaler((-1, 1))\n        \n        transformed = min_max_scaler.fit_transform(data[column].values.reshape(-1, 1))\n        transformed = standard_scaler.fit_transform(transformed)\n\n        transformed_df = pd.DataFrame(data=transformed, columns=[column])\n        data = data.drop(columns=[column])\n        data = data.merge(transformed_df, left_index=True, right_index=True)\n        \n        response[column] = (standard_scaler, min_max_scaler)\n        \n    return data, response\n    \n\nfeature_columns = train_features.columns.tolist()\n\n# Remove all categorical columns for feature scaling\nfor column in categorical_feature_columns:\n        feature_columns.remove(column)\n        \n# Remove sig_id columns for feature scaling\nfeature_columns.remove('sig_id')\n\n# Performing feature standardization and normalization on all numeric features\ntrain_features, std_norm_scaler_map = standardize_and_normalize(train_features, feature_columns)\n\n# Verification\nprint(min(train_features['c-96']))\nprint(max(train_features['c-96']))\n\n# Looking at final transformed features\ntrain_features.head()","f9d8943b":"# Creating kfolds of the data for training multiple models\n\nkfolds = 10\nmskf = MultilabelStratifiedKFold(n_splits=kfolds, random_state=42)\n\nfor fold, (train_index, val_index) in enumerate(mskf.split(train_features, train_targets_scored)):\n    train_features.loc[val_index, 'fold'] = fold\n    train_targets_scored.loc[val_index, 'fold'] = fold\n\ntrain_features.head(10)","7bc3a575":"# Dump to a CSV for sharing\ntrain_features.to_csv(\"transformed_features.csv\", index=False)\n","0ac72557":"![Medicines.jpg](attachment:Medicines.jpg)\n\n# Overview\nThis is a basic notebook containing fundamental EDA and data transformation for beginners. Most of the observations and transformation are generic and typical for data of this nature. I am hoping that I can save someone 30 minutes by sharing this. If you use it or find it useful, please leave a like :).\n\n# EDA\nThe following observations were realized:\n1. There are 3 categorical features cp_type, cp_time and cp_dose containing 2, 3 and 2 categories each. The distribution across each category is fairly even except for cp_type.\n2. Distribution of prediction labels is highly imbalanced for some labels. We may use class_weights to over sample some classes however, given that data augmentation is not possible, re-sampling the same data will not yield great results. Can simply scrape some really low scoring samples.\n3. Looked at min, max, median and mean values of all numerical features. They are mostly confined in the range -8 to 10 with vast majority of values (>80%) being closer to 0.\n4. Looked at some sample feature distributions. The distributions looks great candidates for standard and min-max scaler.\n\n# Transformations\nPerformed the following transformations:\n1. Convert the categorical features to OH encoded features. Since cardinality is small for all categorical features, this works well.\n2. Apply Standard and MinMaxScaler to all numerical features.\n\nNote that if you are using these transformers, you can find the encoders during test in the following way:\n1. cp_type_encoder: Encoder for cp_type categorical variable.\n2. cp_time_encoder: Encoder for cp_time categorical variable.\n3. cp_dose_encoder: Encoder for cp_dose categorical variable.\n4. std_norm_scaler_map: Map where keys are numeric feature column names like g-0,g-1... and c-0,c-1... and the values are a tuple containing the StandardScaler and MinMaxScaler. Use it in that order only.\n\n"}}