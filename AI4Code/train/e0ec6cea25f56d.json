{"cell_type":{"a6512fa7":"code","ce7f9692":"code","df46d2a4":"code","4b400410":"code","d6b310ea":"code","07bbe6b2":"code","383d1e02":"code","aac9724c":"code","9d50b74a":"code","3c78588d":"code","3d1d15b8":"code","8a2aca6c":"code","617cfc3a":"code","10176b06":"code","cd96467e":"code","73ba70f9":"code","449187c0":"code","2c2cc0f4":"code","ba05a985":"code","0e0f7030":"code","cc6b5f85":"code","5efba581":"code","2e1186a5":"code","2ac5787a":"code","48ce513f":"code","0882bb4f":"code","457306f6":"code","bccfec6a":"code","35f598fa":"code","7e896dbf":"code","9eb4d997":"code","db4e3362":"code","e7f4da93":"code","96ed5ed5":"code","df85fd0b":"code","2f358b5b":"code","d6be748b":"code","78b3f3ff":"code","6cbc7f04":"code","ab9fd44b":"code","73037e2b":"code","53944c3c":"code","a88013f3":"code","632332ee":"code","1bb8b4e4":"code","78d4a5f5":"code","eed1aba0":"code","b0241c7a":"code","748f35d4":"code","9a778c62":"code","706f06e8":"code","c036f3b8":"code","726d7c51":"code","64329196":"markdown","f6584c4a":"markdown","aee9c3d6":"markdown","d300a60b":"markdown","35db5a67":"markdown","1379ec74":"markdown","81db4082":"markdown","ef742fe7":"markdown","0a807142":"markdown","16499d3f":"markdown","8ff59d6c":"markdown","c5ff15f5":"markdown","5b3e5c1a":"markdown","9dcd532c":"markdown","cf4f819f":"markdown","0a7c8e90":"markdown","7a6383f2":"markdown","2bfbca97":"markdown","a0e52733":"markdown","826ca534":"markdown","efdc43d1":"markdown","3f4ad0a9":"markdown","84499897":"markdown","f8414ede":"markdown","068cace1":"markdown","98044764":"markdown","6202badd":"markdown","cf012cd4":"markdown","0145ada4":"markdown","737b3da5":"markdown","1feef479":"markdown","6e3e9c7b":"markdown","c2fc0514":"markdown","07edb349":"markdown","d67c9cca":"markdown","23cb37db":"markdown","19993c0a":"markdown","d5abcb32":"markdown","5aadef77":"markdown","ed9509fd":"markdown","917838c3":"markdown","9bab8894":"markdown","eac402fb":"markdown","849b4957":"markdown","d99873dd":"markdown","ea452a31":"markdown","6636e561":"markdown"},"source":{"a6512fa7":"import numpy as np\nimport pandas as pd\nimport re\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn import metrics\nfrom sklearn.metrics import accuracy_score\nfrom sklearn import model_selection\nfrom sklearn import linear_model, neighbors, ensemble, neural_network, svm\n\nimport os\nimport warnings\nwarnings.filterwarnings('ignore') # to avoid seeing warnings, you might wanna set it to 'once' or 'always'","ce7f9692":"data_train = pd.read_csv('\/kaggle\/input\/titanic\/train.csv',sep=',')\ndata_test = pd.read_csv('\/kaggle\/input\/titanic\/test.csv',sep=',')","df46d2a4":"print(\"---- Data Size ----\")\nprint(\"Training # Rows: \", data_train.shape[0], \" | Columns: \", data_train.shape[1])\nprint(\"Test #     Rows: \", data_test.shape[0], \" | Columns: \", data_test.shape[1])","4b400410":"data_train.head()","d6b310ea":"data_train.info()","07bbe6b2":"data_test.info()","383d1e02":"for col in ['PassengerId', 'Pclass', 'Sex', 'Ticket', 'Cabin', 'Embarked']:\n    data_train[col] = data_train[col].astype('category')\n    data_test[col] = data_test[col].astype('category')\n    \n#data_train['Survived'] = data_train['Survived'].astype('category')","aac9724c":"data_train.describe(include='all')","9d50b74a":"data_test.describe(include='all')","3c78588d":"# Let's see how Sex is correlated with Survival of people\ng = sns.catplot(x=\"Sex\", y=\"Survived\", data=data_train,aspect=1, height=4, kind=\"bar\", palette=\"Set2\")\ng.despine(left=True)\ng.set_ylabels(\"Survival Probability\")","3d1d15b8":"# Let's see how Pclass (Class of travel) is correlated with Survival of people\ng = sns.catplot(x=\"Pclass\", y=\"Survived\", data=data_train,aspect=1.5, height=4, kind=\"bar\", palette=\"pastel\")\ng.despine(left=True)\ng.set_ylabels(\"Survival Probability\")","8a2aca6c":"# Let's see how Embarked Port is correlated with Survival of people\ng = sns.catplot(x=\"Embarked\", y=\"Survived\", data=data_train,aspect=1.5, height=4, kind=\"bar\", palette=\"Set3\")\ng.despine(left=True)\ng.set_ylabels(\"Survival Probability\")","617cfc3a":"data_train['Age_bins'] = pd.cut(data_train['Age'], bins=[5,15,25,35,45,55,65,75,85,95,150])","10176b06":"g = sns.catplot(x=\"Age_bins\", y=\"Survived\",hue=\"Sex\", data=data_train,aspect=2.7, height=5, kind=\"bar\", palette=\"Set2\")\ng.set_ylabels(\"Survival Probability\")","cd96467e":"data_train['Fare_bins'] = pd.cut(data_train['Fare'], bins=[0,10,50,100,150,200,250,300,400,500,600])\ng = sns.catplot(x=\"Fare_bins\", y=\"Survived\", hue=\"Sex\", data=data_train,aspect=2.5, kind=\"bar\", palette=\"Set3\")\ng.set_ylabels(\"Survival Probability\")","73ba70f9":"sns.catplot(x=\"Survived\", y=\"Fare\", hue=\"Sex\", kind=\"violin\", col=\"Embarked\", col_wrap=3,\n               split=True, inner=\"quart\", data=data_train, orient=\"v\")","449187c0":"g = sns.catplot(x=\"SibSp\", y=\"Survived\", data=data_train, kind=\"bar\", palette=\"Set2\")\ng.despine(left=True)\ng.set_ylabels(\"Survival Probability\")","2c2cc0f4":"g = sns.catplot(x=\"Parch\", y=\"Survived\", data=data_train, kind=\"bar\", palette=\"Set3\")\ng.despine(left=True)\ng.set_ylabels(\"Survival Probability\")","ba05a985":"data_train['Family_size'] = data_train['SibSp']+data_train['Parch'] + 1  # adding 1 to count the person currently considered too\ng = sns.catplot(x=\"Family_size\", y=\"Survived\", data=data_train, kind=\"bar\", palette=\"pastel\")\ng.despine(left=True)\ng.set_ylabels(\"Survival Probability\")","0e0f7030":"sns.catplot(y=\"Age\", kind=\"violin\", inner=\"box\", data=data_train[data_train.Age >= 0])","cc6b5f85":"sns.catplot(y=\"Family_size\", kind=\"violin\", inner=\"box\", data=data_train[data_train.SibSp.notna()])","5efba581":"sns.catplot(x=\"Pclass\", y=\"Survived\", hue=\"Sex\", kind=\"point\", col=\"Embarked\", col_wrap=3,\n               data=data_train)","2e1186a5":"sns.catplot(x=\"Pclass\", y=\"Fare\", kind=\"violin\", hue='Sex', split=True, col=\"Embarked\", col_wrap=3, data=data_train)","2ac5787a":"# Name format is Lastname, Prefix. Firstname Middle Name ..\ndata_train['Name_Prefix'] = data_train['Name'].str.split(',', expand=True)[1].str.strip().str.split('.', expand=True)[0].str.lower()\ndata_train['Name_Prefix'].value_counts()","48ce513f":"# re-assign some of the titles\ndata_train['Name_Prefix'] = data_train['Name_Prefix'].apply(lambda x: 'miss' if x in ['mlle','ms'] else x)\ndata_train['Name_Prefix'] = data_train['Name_Prefix'].apply(lambda x: 'mrs' if x in ['mme'] else x)\n\n# replace rarely occuring prefixes\nprefixes = (data_train['Name_Prefix'].value_counts() < 10)\ndata_train['Name_Prefix'] = data_train['Name_Prefix'].apply(lambda x: 'misc' if prefixes.loc[x] == True else x)\ndata_train['Name_Prefix'].value_counts()","0882bb4f":"g = sns.catplot(x=\"Name_Prefix\", y=\"Survived\", data=data_train, kind=\"bar\", palette=\"Set3\", aspect = 1.1)\ng.despine(left=True)\ng.set_ylabels(\"Survival Probability\")","457306f6":"sns.catplot(x=\"Name_Prefix\", y=\"Age\", kind=\"box\", data=data_train)","bccfec6a":"data_train.shape, data_test.shape","35f598fa":"data_train = data_train[['PassengerId', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp', 'Parch','Ticket', 'Fare', 'Cabin', 'Embarked','Survived']]\ndata_all = data_train.append(data_test, ignore_index=True, sort=False)\ndata_all.shape","7e896dbf":"data_all[['PassengerId', 'Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']].isna().sum()","9eb4d997":"data_all[data_all['Embarked'].isna()]","db4e3362":"data_all.at[data_all[data_all['Embarked'].isna()].index,'Embarked'] = 'C'","e7f4da93":"data_all[data_all['Fare'].isna()]","96ed5ed5":"fare = np.mean(data_all[(data_all['Pclass']==3) & (data_all['Embarked']=='S') & (data_all['Sex']=='male') & (data_all['SibSp']==0) & (data_all['Parch']==0) & (data_all['Age'] > 50) & (data_all['PassengerId'] != 1044)]['Fare'].values)","df85fd0b":"data_all.at[data_all[data_all['Fare'].isna()].index,'Fare'] = fare","2f358b5b":"data_all['Name_Prefix'] = data_all['Name'].str.split(',', expand=True)[1].str.strip().str.split('.', expand=True)[0].str.lower()\ndata_all['Name_Prefix'] = data_all['Name_Prefix'].apply(lambda x: 'miss' if x in ['mlle','ms'] else x)\ndata_all['Name_Prefix'] = data_all['Name_Prefix'].apply(lambda x: 'mrs' if x in ['mme'] else x)\n# replace rarely occuring prefixes\nprefixes = (data_all['Name_Prefix'].value_counts() < 10)\ndata_all['Name_Prefix'] = data_all['Name_Prefix'].apply(lambda x: 'misc' if prefixes.loc[x] == True else x)","d6be748b":"for title in data_all['Name_Prefix'].unique():\n    for sex in data_all['Sex'].unique():\n        mean_age = np.mean(data_all[(~data_all['Age'].isna()) & (data_all['Sex'] == sex) & (data_all['Name_Prefix'] == title)]['Age'].values)\n        data_all.at[data_all[(data_all['Age'].isna()) & (data_all['Sex'] == sex) & (data_all['Name_Prefix'] == title)].index,'Age'] = mean_age","78b3f3ff":"data_all[['PassengerId', 'Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked','Name_Prefix']].isna().sum()","6cbc7f04":"data_all['Family_size'] = data_all['SibSp']+data_all['Parch'] + 1","ab9fd44b":"data_all = pd.get_dummies(data_all, columns=['Pclass','Sex','Embarked','Name_Prefix'])","73037e2b":"age_scaler = MinMaxScaler()\ndata_all['Age'] = age_scaler.fit_transform(data_all['Age'].values.reshape(-1,1))\nfare_scaler = MinMaxScaler()\ndata_all['Fare'] = fare_scaler.fit_transform(data_all['Fare'].values.reshape(-1,1))","53944c3c":"data_all[['PassengerId','Age','Fare']].head(2)","a88013f3":"data_train = data_all.iloc[:891]\ndata_test = data_all.iloc[891:]\ndata_train.shape, data_test.shape","632332ee":"columns_to_not_use = ['PassengerId', 'Survived', 'Ticket', 'Cabin', 'Name']","1bb8b4e4":"cv_splits = model_selection.ShuffleSplit(n_splits=10, test_size=0.3, train_size=0.7, random_state=42)","78d4a5f5":"kwargs = {\n    'logistic_regression':{'class_weight': None,  # default is None\n                           'random_state': 42\n                          },\n    \n    'random_forest': {'n_estimators': 10,              # default is 100\n                     'max_depth': 6,                   # default is None\n                     'max_features': 'auto',           \n                     'max_leaf_nodes': 16,             # default is None\n                     'class_weight': None,             # default is None\n                     'criterion': 'entropy',           # default is gini\n                     'oob_score': True,                # default is False\n                     'random_state': 42\n                     },\n    \n    'k_neighbours': {'n_neighbors': 10,                  # default is 5\n                    'p': 2                               # default is 2 (euclidean_distance), p =1 (manhattan_distance)\n                    },\n    \n    'mlp_classifier': {'hidden_layer_sizes': (16,8),      # default is (100,)\n                      'activation': 'tanh',               # default is 'relu'\n                      'solver': 'adam',                 \n                      'max_iter': 150,                    # default is 200\n                      'random_state': 42},\n    \n    'svm': {'C': 2,                       # default is 1\n           'kernel':'rbf',                \n           'random_state': 42}\n}","eed1aba0":"algos = {\n    'logistic_regression':linear_model.LogisticRegression(**kwargs['logistic_regression']),\n    'random_forest':ensemble.RandomForestClassifier(**kwargs['random_forest']),\n    'k_neighbours':neighbors.KNeighborsClassifier(**kwargs['k_neighbours']),\n    'mlp_classifier':neural_network.MLPClassifier(**kwargs['mlp_classifier']),\n    'svm':svm.SVC(**kwargs['svm'])\n}","b0241c7a":"cv_results = {'Algorithm':[],                     # algorithm name\n              'Mean Train Accuracy':[],           # Mean of training accuracy on all splits\n              'Mean Test Accuracy':[],            # Mean of test accuracy on all splits\n              'Test Standard deviation':[],       # Standard deviation of test accuracy on all splits \n                                                  # (this is to know how worse the algorithm can perform)\n              'Fit Time': []}                     # how fast the algorithm converges","748f35d4":"for alg_name,alg in algos.items():\n    cv_results['Algorithm'].append(alg_name)\n    \n    cross_val = model_selection.cross_validate(alg, \n                                               data_train.loc[:, ~data_train.columns.isin(columns_to_not_use)], \n                                               data_train['Survived'],\n                                               cv  = cv_splits,\n                                               return_train_score=True,\n                                               return_estimator=False\n                                              )\n    \n    cv_results['Mean Train Accuracy'].append(cross_val['train_score'].mean())\n    cv_results['Mean Test Accuracy'].append(cross_val['test_score'].mean())\n    cv_results['Test Standard deviation'].append(cross_val['test_score'].std()*3)\n    cv_results['Fit Time'].append(cross_val['fit_time'].mean())\n    ","9a778c62":"cv_results_df = pd.DataFrame.from_dict(cv_results)\ncv_results_df.sort_values(by=['Mean Test Accuracy'], inplace=True, ascending=False)\ncv_results_df","706f06e8":"# store the predictions in a dictionary\ny_predicted = {}","c036f3b8":"for alg_name,alg in algos.items():\n    \n    alg.fit(data_train.loc[:, ~data_train.columns.isin(columns_to_not_use)], data_train['Survived'])\n    y_predicted[alg_name] = alg.predict(data_test.loc[:, ~data_test.columns.isin(columns_to_not_use)])\n    ","726d7c51":"# create a dataframe and write to a csv file\nfor alg_name in algos.keys():\n    results_dict = {'PassengerId':data_test['PassengerId'].values.tolist(), 'Survived':list(map(int, y_predicted[alg_name]))}\n    results_df = pd.DataFrame.from_dict(results_dict)\n    results_df.to_csv(alg_name+'.csv', index=False)","64329196":"**Observation 2:**  \n_Class of Travel is correlated with Survival, Higher class of travel (class 1) means more probability of surviving_","f6584c4a":"**Observation 3:**  \n_People embarked at port C are more likely to Survive_","aee9c3d6":"Let's inspect further and check the number of unique, min, max values to catch any outliers","d300a60b":"Loading data and then doing inital data exploration","35db5a67":"### Data Analysis\n\n#### Let's check datatypes of each column and how many Not NA values are present","1379ec74":"**Observation 7**  \n_Males embarked at port Q in class 1, died (contrary to other ports)_","81db4082":"We can directly use the **mode** for imputation. BUT we can see that both passengers are females, both Survived, traveling class 1, so from above visualizations, we can see that they _mostly_ embarked at port **C**","ef742fe7":"#### Insights \nPassengerId - All unique - Not useful for predicting survival chance - drop the column  \nName - All Unique - drop the column  \nTicket - 681 (~76%) unique values -  \nCabin - 147 unique values - 77% null values, we will not use this column  \nAge - between 0.42 to 80 yrs (all positive, seems fine)  \nFare - 0 to 512 (all positive, seems fine)  ","0a807142":"Splitting the data in training and test again","16499d3f":"Let's first combine both training and test data sets  ","8ff59d6c":"For making submission to this competition, we will use all _five_ algorithms to make predictions and find out the max accuracy achieved","c5ff15f5":"Before that let's first quickly convert few of them to correct datatypes (category)","5b3e5c1a":"#### <font color=green><u>Information about data from Kaggle challenge webpage<\/u><\/font>\n<table align=left>\n    <tr>\n        <td>PassengerId<\/td>\n        <td>ID for each passenger<\/td>\n    <\/tr>\n    <tr>\n        <td>Survived<\/td>\n        <td>Survived or Not (0 or 1)<\/td>\n    <\/tr>\n    <tr>\n        <td>Pclass<\/td>\n        <td>Class of Travel<\/td>\n    <\/tr>\n    <tr>\n        <td>Name<\/td>\n        <td>Name of Passenger<\/td>\n    <\/tr>\n    <tr>\n        <td>Sex<\/td>\n        <td>Gender of passenger<\/td>\n    <\/tr>\n    <tr>\n        <td>Age<\/td>\n        <td>Age in years (fractional)<\/td>\n    <\/tr>\n    <tr>\n        <td>SibSp<\/td>\n        <td>Number of Sibling\/Spouse aboard<\/td>\n    <\/tr>\n    <tr>\n        <td>Parch<\/td>\n        <td>Number of Parent\/Child aboard<\/td>\n    <\/tr>\n    <tr>\n        <td>Ticket<\/td>\n        <td>Ticket Number<\/td>\n    <\/tr>\n    <tr>\n        <td>Fare<\/td>\n        <td>Ticket Fare<\/td>\n    <\/tr>\n    <tr>\n        <td>Cabin<\/td>\n        <td>Cabin Number<\/td>\n    <\/tr>\n    <tr>\n        <td>Embarked<\/td>\n        <td>The port in which a passenger has embarked.<br>C - Cherbourg, S - Southampton, Q = Queenstown<\/td>\n    <\/tr>\n<\/table>","9dcd532c":"Scaling Age and Fare data  \nScaling features isn't necessary for algorithms like Logistic regression or random forest but is required if we want to use SVM\n\nWe will use scikit-learn `MinMaxScaler` here.  \n  \nA good read for feature scaling - https:\/\/sebastianraschka.com\/Articles\/2014_about_feature_scaling.html","cf4f819f":"### Let's get started","0a7c8e90":"### Data Preprocessing and Feature Engineering","7a6383f2":"Due to some restrictions in my current python environment, I can not install and use fancyimpute or impyute library. So, for this exercise we will manually group the data and take mean","2bfbca97":"<font size=4> From initial inspection,  Age, Cabin, Embarked and Fare (in test set) fields seem to have null values <\/font>","a0e52733":"**Imputation**  \n<u>Categorical<\/u> - Mode  \n<u>Numerical<\/u> - Mean\/Median\n\n**Better imputation Suggestion**  \n<u>Embarked<\/u> - Should be same for a Family? Checking if SibSp is one, then clustering on last name  ?  \n<u>Fare<\/u> - Should be dependent on Pclass, Sex, Embarked, Age, Sex ?  \n<u>Age<\/u> - Should be correlated with Fare, Emabrked, ~ same as SibSp if present (hard to find though)  \n  \nA good references for ways to impute missing values  \nhttps:\/\/towardsdatascience.com\/6-different-ways-to-compensate-for-missing-values-data-imputation-with-examples-6022d9ca0779  \nhttps:\/\/www.hilarispublisher.com\/open-access\/a-comparison-of-six-methods-for-missing-data-imputation-2155-6180-1000224.pdf","826ca534":"**Observation 6:**  \n_lesser group size, more chances of Survival_","efdc43d1":"**Can we use names ?**  \nNames seem to contain individual prefixes, can they help with predicting survival?","3f4ad0a9":"Let's take a peek at the data","84499897":"Let's create one more column - Family_size and encode categorical columns before we move on to Data Modeling step  \n  \nWhile we used bins to visualize Age, Fare vs Survival and other attributes, it is (IMO) generally not a good idea to bin continuous variables while modeling.   \nA good refernce - https:\/\/stats.stackexchange.com\/a\/68839  \n> _If you use a few bins you throw away a lot of information in the predictors; if you use many you tend to fit wiggles in what should be a smooth, if not linear, relationship, & use up a lot of degrees of freedom. Generally better to use polynomials or splines (piecewise polynomials that join smoothly) for the predictors. Binning's really only a good idea when you'd expect a discontinuity in the response at the cut-points_","f8414ede":"Let's check Test dataset too","068cace1":"**Columns to keep** - PassengerId, Pclass, Sex, Age, SibSp, Parch, Fare, Embarked","98044764":"#### <font color=green><u>Observations:-<\/u><\/font>\n\n* All algorithms with little hyperparameter tuning performs almost equally same on dataset\n* random forest is performing well with least standard deviation\n* SVM gives the highest test accuracy\n* K-neighbours takes the least amount of time in fitting the dataset but performs bad (comparing to others)\n* mlp_classifier (neural network) performs well too but also takes huge anmount of time in fitting","6202badd":"PClass, Sex, Age, SibSp, Parch, Fare, Embarked seem to be fine","cf012cd4":"**Observation 8:**  \n_Based on above two plots, Females (prefixes - Mrs, Miss, Ms) and children (prefixes - Master) have higher chances of survival as compared to Males (Mr.)_","0145ada4":"<font color=blue><b>Maximum accuracy achieved on kaggle is by using Random Forest `0.79904`<\/b><\/font>  \n  \nThis can further be increased by doing more **feature engineering** (e.g. exploring Ticket, Cabin and other features ?), **hyperparameter tuning** (using GridSearchCV), and **other algorithms** can also be tried to see if they provide more accuracy\n\nThis was my first competition and I had fun learning many new things. Hope you had fun too while going through this notebook","737b3da5":"Now let's define the algorithms and their hyperparameters (for performance tuning)","1feef479":"**Observation 4:**  \n_Males only less than 15 yrs old or >= 75 yrs old are more likely to  Survive, females are in general more likely to survive than males but women > 15 year old are more likly to survive as compared to any one else on the ship_","6e3e9c7b":"<u>Steps:-<\/u>\n* We will use ShuffleSplit (for training, cross-validation)\n* We will train using _5_ most common algorithms and compare the test and train accuracy and their fit time-\n>- Logistic Regression \n>- Random Foreast\n>- K-Neighbours\n>- MLPClassifier (neural network)\n>- Support vector machines\n  \nYou can experiment with other algorithms of your choice in the similar way.","c2fc0514":"## Making Predictions","07edb349":"**Observation 5:**  \n_Men are more likely to survive as fare goes up  \nPeople in Fare range 150-200 have survived less_","d67c9cca":"**Overview**  \nIn this challenge, we try to predict the survial chances of a passesnger onbaord Titanic by utilizing the information available to us e.g. Passenger class, ticket fare, age, sex, Pclass etc.  ","23cb37db":"Why random_state = `42`? Well that's the answer to `the life the universe and everything!`. _**Hint:** Hitchhiker's Guide to the Galaxy_  \n\nJoke aside, you can use any value for random_state. It simply sets a seed to the random number generator, so that your results are always deterministic. Please check algorithm's scikit-learn learn webpage for more details on the hyper-parameters","19993c0a":"Let's create Name Prefix (Title) column before imputing Age as name title may help in grouping records and then calculating Age  \nAnother way for large datasets is to use **kNN or MICE for imputing** missing values (see _fancyimpute_ python package)","d5abcb32":"Let's Create Age and Fare bins (groups) to visalize them","5aadef77":"### Data Modeling","ed9509fd":"### Data Visualization\n\n_Let's visalize some of the features to see correlation with target (Survival)_","917838c3":"**Observation 1:**  \n_Females are more likely to Survive as compared to Males_","9bab8894":"### <u>Content<\/u>\n1. [Importing required libraries and loading data](#Let's-get-started)\n2. [Data Analysis](#Data-Analysis)\n3. [Data Visualization](#Data-Visualization)\n4. [Feature Engineering and Data Preprocessing](#Data-Preprocessing-and-Feature-Engineering)\n5. [Data Modeling](#Data-Modeling)\n7. [Making Predictions](#Making-Predictions)\n8. [Final thoughts!](#Final-thoughts)","eac402fb":"Importing required modules and libraries for for data preparation, visualization, modeling and making predictions","849b4957":"before we move on, let's create an empty dict to store results","d99873dd":"## Final thoughts","ea452a31":"\n# Titanic: Machine Learning from Disaster","6636e561":"Performing similar imputation for fare, using other fields to get more exact mean fare value"}}