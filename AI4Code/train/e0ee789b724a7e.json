{"cell_type":{"76cc20e3":"code","ce152203":"code","9719036c":"code","2c2d49dc":"code","12254b90":"code","cd5777d4":"code","4e0728ee":"code","22f86093":"code","bf918ec5":"code","53ef0792":"code","74e04704":"code","52532e03":"code","e9c8dd1f":"code","d92be26a":"markdown","cd95fe6e":"markdown","dca21237":"markdown","49c80672":"markdown","ca3becc3":"markdown","2cbdd116":"markdown","c4e69abb":"markdown","4260b805":"markdown"},"source":{"76cc20e3":"# Generic and sound-processing libs\nimport numpy as np\nimport pandas as pd\nimport os\n!pip install librosa\nimport librosa\n\n# Rapids and plotting libs\nimport cupy as cp\nimport cudf, cuml\nfrom cuml.manifold import TSNE\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\n%matplotlib inline","ce152203":"# Inputs\nCLIP_SIZE = 6 # How long to make each clip (s). Centered at the midpoint of the song\nF_MIN = 90 # Of all the rows of true positive data, this is the lowest f_min frequency\nF_MAX = 14000 # Of all the rows of true positive data, this is the highest f_max frequency\nNfft = 2048 # FFT Parameter\nhop_len = 512 # FFT Parameter\nsr = 44100 # FFT Parameter","9719036c":"train_tp = pd.read_csv('\/kaggle\/input\/rfcx-species-audio-detection\/train_tp.csv')\ntrain_fp = pd.read_csv('\/kaggle\/input\/rfcx-species-audio-detection\/train_fp.csv')\n\n# Combine datasets, after labeling which are TP and which are FP\ntrain_tp['tp'] = 'tp'\ntrain_fp['tp'] = 'fp'\n\ntrain_fp = train_fp.sample(frac=1, random_state=234)\ntrain_fp = train_fp.head(len(train_tp)) # Too many fp rows, so will only keep as many as I have of tp rows\n\ntrain = pd.concat([train_tp, train_fp])\n\n# Create identification labels\ntrain['species_song'] = train['species_id'].astype(str) + '-' + train['songtype_id'].astype(str)\ntrain['species_song_tp'] = train['species_song'].astype(str) + '-' + train['tp'].astype(str)\n\ndef define_data_clips(df):\n    ''' \n    Returns the same dataframe, but indicates 10 second clips to use for t-SNE, centered at location of song. \n    Removes rows that extend above\/below allowed thresholds.  \n    '''\n    df['t_center'] = df['t_min'] + (df['t_max'] - df['t_min'])\/2\n    df['t_clip_lower'] = df['t_center'] - CLIP_SIZE\/2\n    df['t_clip_upper'] = df['t_center'] + CLIP_SIZE\/2\n    df = df[(df['t_clip_lower'] > 0) & (df['t_clip_upper'] < 60)] # Bounds must be within a minute, so all clips have same shape (10s)\n    return df\n\ntrain = define_data_clips(train)\nprint(len(train))","2c2d49dc":"# Determine bins to keep\nfreqs = librosa.fft_frequencies(sr=sr, n_fft=Nfft)\nbin_size = freqs[1]-freqs[0]\n# freq[i] is the lower bound of the bin. freq[i+1] is the upper bound of bin i\n\n# Initialize bins to keep\ni_low = 0\ni_high = len(freqs)\nfor i in range(len(freqs)):\n    if freqs[i]+ bin_size < F_MIN:\n        i_low = i+1 # At end of loop, will return the bin number for the first bin to exceed low threshold\n    if freqs[i] > F_MAX:\n        i_high = i\n        break # Break loop once finding first bin above threshold\n","12254b90":"%%time\ndef create_sfft(row, sr, Nfft, hop_len, i_low, i_high):\n    '''\n    For each row in the DB, create the 10s clip around the data and store the results in the dataframe.\n    '''\n    offset = row['t_clip_lower']\n    duration = row['t_clip_upper'] - row['t_clip_lower']\n    fname = f\"..\/input\/rfcx-species-audio-detection\/train\/{row['recording_id']}.flac\"\n    x , sr = librosa.load(fname, sr=sr, offset=offset, duration=duration)\n    \n    X = librosa.stft(x, n_fft= Nfft, hop_length=hop_len)\n    Xdb = librosa.amplitude_to_db(abs(X)).astype(np.float16) # Need to reduce from float32 to float16 to fit in memory\n    Xdb = Xdb[i_low:i_high]\n    \n    row['sfft'] = Xdb\n    return row\n\ntrain = train.apply(lambda row: create_sfft(row, sr, Nfft, hop_len, i_low, i_high), axis=1)\ntrain['sfft'].iloc[0].shape","cd5777d4":"train['sfft_n_freq_bins'] = train['sfft'].apply(lambda x: x.shape[0])\ntrain['sfft_n_time_bins'] = train['sfft'].apply(lambda x: x.shape[1])\n\n# As expected, all data has the same shape. Good!\nprint(f\"med freq bins: {train['sfft_n_freq_bins'].median()}\")\nprint(f\"min freq bins: {train['sfft_n_freq_bins'].min()}\")\nprint(f\"max freq bins: {train['sfft_n_freq_bins'].max()}\")\nprint(f\"med time bins: {train['sfft_n_time_bins'].median()}\")\nprint(f\"min time bins: {train['sfft_n_time_bins'].min()}\")\nprint(f\"max time bins: {train['sfft_n_time_bins'].max()}\")   ","4e0728ee":"# Create numpy array\ntrain_sfft = train['sfft'].to_list()\ntrain_sfft = np.array(train_sfft)\ntrain_sfft = train_sfft.reshape(train_sfft.shape[0], -1)\nprint(train_sfft.shape)","22f86093":"# Build t-SNE\ntsne = TSNE(n_components=2)\ntrain_sfft_2D = tsne.fit_transform(train_sfft)","bf918ec5":"# Put parameters back in the dataframe\ntrain_sfft_2D = cp.asnumpy(train_sfft_2D)\ntrain['tsneX'] = train_sfft_2D[:, 0]\ntrain['tsneY'] = train_sfft_2D[:, 1]","53ef0792":"fig = px.scatter(train, x=\"tsneX\", y=\"tsneY\", color=\"species_song_tp\",\n                 labels={\n                     \"tsneX\": \"X\",\n                     \"tsneY\": \"Y\",\n                     \"species_song_tp\": \"Species + Song + TP\"\n                 },\n                 opacity = 0.5, \n                 title=f'Rainforest t-SNE: {CLIP_SIZE} second clip')\nfig.update_yaxes(matches=None, showticklabels=False, visible=True)\nfig.update_xaxes(matches=None, showticklabels=False, visible=True)\nfig.show()","74e04704":"train_tp = train[train['tp']=='tp']\nfig = px.scatter(train_tp, x=\"tsneX\", y=\"tsneY\", color=\"species_song\",\n                 labels={\n                     \"tsneX\": \"X\",\n                     \"tsneY\": \"Y\",\n                     \"species_song\": \"Species + Song\"\n                 },\n                 opacity = 0.5, \n                 title=f'Rainforest t-SNE, True Positives Only: {CLIP_SIZE} second clip')\nfig.update_yaxes(matches=None, showticklabels=False, visible=True)\nfig.update_xaxes(matches=None, showticklabels=False, visible=True)\nfig.show()","52532e03":"train_tp['species_id_str'] = train_tp['species_id'].apply(lambda x: str(x))\nfig = px.scatter(train_tp, x=\"tsneX\", y=\"tsneY\", color=\"species_id_str\",\n                 labels={\n                     \"tsneX\": \"X\",\n                     \"tsneY\": \"Y\",\n                     \"species_id_str\": \"Species\"\n                 },\n                 opacity = 0.5, \n                 title=f'Rainforest t-SNE, True Positives Only: : {CLIP_SIZE} second clip')\nfig.update_yaxes(matches=None, showticklabels=False, visible=True)\nfig.update_xaxes(matches=None, showticklabels=False, visible=True)\nfig.show()","e9c8dd1f":"train.head()\ntrain.drop(columns=['sfft', 'sfft_n_freq_bins', 'sfft_n_time_bins'], inplace=True)\ntrain.to_csv('train_output.csv', index=False)","d92be26a":"## Verify all clips have the same size\nAs shown below, all clips are 1025x862. We want all the clips to have the same shape, so this is good. Otherwise, we'd need to pad\/clip bins as needed.","cd95fe6e":"## Generate SFFTs for each clip\nI use a pandas apply function to load the recording, clip it down to the times of interest, and convert it to an SFFT.\n\nI actually considered using Rapids to do all of the FFT calculations, but decided against it when I realized I would have to re-write the FFT calculations. Additionally, I don't think Rapids can parallelize loading the data from the .flac files, which was also time consuming.\n\n**If that's not the case, someone let me know in the comments!**","dca21237":"## Determining clip locations\nI don't want to use the entire 1-minute clip for each song for a few reasons:\n* Most songs are only 1-2 seconds long, and I'm afraid they'll get lost in the noise of a 60 second clip.\n* The 60 second recording may have multiple songs. I want to label each clip as having 1 **main** song. I realize even in a 6 second clip, there may be multiple songs. However, it'll happen less often.\n\nTo account for this, I will be taking 10 second clips, centered at the center of the song in question. This means a recording will have multiple rows: One for each song in the recording. This is consistent with the train_tp and train_fp files.","49c80672":"# Rainforest t-SNE with Colored Labels\nThis post was inspired by and based off of [Bojan's Notebook](https:\/\/www.kaggle.com\/tunguz\/visualizing-fft-features-with-t-sne-and-umap), which showed how to use [Rapids](https:\/\/rapids.ai\/) to more quickly create t-SNE visualizations with a GPU. I found the post interesting, but really wanted to see colors for each species in the final plot.\n\nInstead of forking the original notebook, I decided to start from scratch. It allowed me more control over how I processed the data, which I prefer.","ca3becc3":"# Takeaways\nThe output data (train_output.csv) is a great resource for exploring true positives versus false positives, and how similar they may look. Additionally, the t-SNE features *might* be good for training predictive models.\n\nWhile developing this notebook, I found myself running into CPU and RAM limits. I had to make a few adjustments to solve these issues. Specifically:\n* Converting the SFFT outputs from float32 to float16\n* Taking a 6-second clip length instead of a 10-second length\n* Removing frequencies outside of the global range of f_min and f_max\n\nI could have also adjusted the SFFT parameters, such as lowering the sampling rate, but didn't need to.\n\nFeel free to leave your comments!","2cbdd116":"## t-SNE\nCreate a numpy array as an input to the t-SNE algorithm, then run t-SNE using Rapids.","c4e69abb":"# Plots\nExlore the data, with colored labels. Possibly good features for training?","4260b805":"## Save Training Data with t-SNE labels"}}