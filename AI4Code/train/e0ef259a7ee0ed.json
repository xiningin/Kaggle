{"cell_type":{"6addb8f6":"code","2dd00ced":"code","bd5b0925":"code","b6e07901":"code","d3e43ca4":"code","1faef0b7":"code","f9d368f7":"code","f0b7fa8f":"code","c88b1bc5":"code","80cd0317":"markdown","bbb40e1e":"markdown","24103882":"markdown","5fac1787":"markdown","4c3c4371":"markdown","c8f5f5a5":"markdown","feeddc14":"markdown"},"source":{"6addb8f6":"#Import necessay libraries\nimport pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot as plt\n%matplotlib inline\nimport matplotlib\nmatplotlib.rcParams[\"figure.figsize\"] = (20,10)\nimport seaborn as sns\n\n#Preprocessing\nfrom sklearn import model_selection\nfrom sklearn.preprocessing import OrdinalEncoder,StandardScaler\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split\n#Model\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error,roc_auc_score\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\nimport lightgbm as lgb\nimport xgboost as xgb\n","2dd00ced":"#import the data and shape\ntrain = pd.read_csv(\"..\/input\/tabular-playground-series-nov-2021\/train.csv\")\ntest = pd.read_csv(\"..\/input\/tabular-playground-series-nov-2021\/test.csv\")\nsample=pd.read_csv(\"..\/input\/tabular-playground-series-nov-2021\/sample_submission.csv\")\nprint(train.shape,test.shape,sample.shape)\nprint(train.isnull().sum())\ntrain.describe()","bd5b0925":"train = pd.read_csv(\"..\/input\/tps-nov-21-crossvalidationdata\/trainfold_5.csv\")\n#store the final_prediction data and score\nfinal_predictions = []\nscore= []\n\n#features(categorical and numerical datas separate)\nuseful_features = [c for c in train.columns if c not in (\"id\",\"target\",\"kfold\")]\nnumerical_cols = [col for col in useful_features if 'f' in col]\n\ntest = test[useful_features]\n\nfor fold in range(5):\n    xtrain = train[train.kfold != fold].reset_index(drop=True)\n    xvalid = train[train.kfold == fold].reset_index(drop=True)\n    xtest = test.copy()\n    \n    ytrain = xtrain.target\n    yvalid = xvalid.target\n    \n    xtrain = xtrain[useful_features]\n    xvalid = xvalid[useful_features]\n    \n    scaler = preprocessing.StandardScaler()\n    xtrain[numerical_cols] = scaler.fit_transform(xtrain[numerical_cols])\n    xvalid[numerical_cols] = scaler.transform(xvalid[numerical_cols])\n    xtest[numerical_cols] = scaler.transform(xtest[numerical_cols])\n    \n    params_b = {\n        'eta':0.0006409492043348976,\n        'gamma': 0.44055233553496753,\n        'reg_lambda':0.000669870160395597,\n        'reg_alpha':9570.52928031902,\n        'subsmaple':0.6048519345751173,\n        'learning_rate':0.01,\n        'n_estimators':10000,\n        \n        }\n    model= XGBRegressor(**params_b,\n                       tree_method='gpu_hist',\n                       predictor='gpu_predictor',\n                       gpu_id=0)\n    model.fit(xtrain,ytrain,early_stopping_rounds=300,eval_set=[(xvalid,yvalid)],verbose=2000)\n    preds_valid = model.predict(xvalid)\n    \n    #Training model apply the test data and predict the output\n    test_pre = model.predict(xtest)\n    final_predictions.append(test_pre)\n    \n    roc=roc_auc_score(yvalid,preds_valid)\n    score.append(roc)\n    #way of output is display\n    print(f\"fold:{fold},roc:{roc}\")\n\n#mean of repeation of fold data and identify the  mean and standard deviation \nprint(np.mean(score),np.std(score))\n","b6e07901":"#prediction of data\npreds = np.mean(np.column_stack(final_predictions),axis=1)\nprint(preds)\nsample.target = preds\nsample.to_csv(\"submission_XG.csv\",index=False)\nprint(\"success\")","d3e43ca4":"#Optuna\nimport optuna\nfrom optuna.samplers import TPESampler\nfrom hyperopt import STATUS_OK,Trials,fmin,hp,tpe\n\n\ndef objective(trial):\n    params = {'max_depth':hp.quniform(\"max_depth\",3,10,2),\n              'eta': trial.suggest_loguniform('eta',1e-5,0.2),\n              'gamma':trial.suggest_loguniform('gamma',1e-8,1.0),\n              'reg_lambda': trial.suggest_loguniform('reg_lambda',1e-4,1e4),\n              'reg_alpha': trial.suggest_loguniform('reg_alpha',1e-4,1e4),\n              'colsample_bytree': hp.uniform('colsample_bytree',0.1,1.1),\n              'min_child_weight':hp.quniform('min_child_weight',100,1000,1),\n              'subsample': trial.suggest_float('subsample',1e-8,1),\n              'learning_rate': trial.suggest_loguniform(\"learning_rate\",0.001,0.3),\n              'max_depth': trial.suggest_int(\"max_depth\", 1, 10),\n              'n_estimators':10000,\n              'seed':0,\n              'tree_method':'gpu_hist',\n              'gpu_id': 0,\n              'predictor': 'gpu_predictor'\n            }\n    \n    model = xgb.XGBRegressor(**params)\n\n    model.fit(\n        xtrain, \n        ytrain,\n        early_stopping_rounds=100,\n        eval_set=[(xvalid, yvalid)], \n        verbose=False\n    )\n\n    yhat = model.predict(xvalid)\n    return roc_auc_score(yvalid, yhat)\n\nstudy = optuna.create_study(direction=\"maximize\")\nstudy.optimize(objective, n_trials=100)\nprint(study.best_params)","1faef0b7":"params= study.best_params","f9d368f7":"model= XGBRegressor(**params,\n                    tree_method='gpu_hist',\n                    predictor='gpu_predictor',\n                    gpu_id=0)\nmodel.fit(xtrain,ytrain,early_stopping_rounds=300,eval_set=[(xvalid,yvalid)],verbose=2000)\npreds_valid = model.predict(xvalid)\n    \n#Training model apply the test data and predict the output\ntest_pre = model.predict(xtest)\n\nsample.target = test_pre\nsample.to_csv(\"submission_XG1.csv\",index=False)\nprint(\"success\")","f0b7fa8f":"from catboost import CatBoostClassifier\ntrain = pd.read_csv(\"..\/input\/tabular-playground-series-nov-2021\/train.csv\")\n#store the final_prediction data and score\nfinal_prediction1 = []\nscore= []\n\nfeatures = [f for f in train.columns if f not in(\"id\",\"target\")]\nX = train[features]\ny = train.target\ntest = test[features]\n\nss = StandardScaler()\nX = ss.fit_transform(X)\nxtest = ss.transform(test)\n\nxtrain,xvalid,ytrain,yvalid = train_test_split(X,y,test_size=0.2,random_state=42)\nprint(xtrain.shape,ytrain.shape,xvalid.shape,yvalid.shape)\n\nc_para = {\n    'iterations':25000,\n    'loss_function':'Logloss',\n    'use_best_model':True,\n    'early_stopping_rounds':1000,\n    'learning_rate':0.0513589,\n    'border_count':32,\n    'verbose':1000,\n    'random_state':228,\n    'subsample': 0.95312,\n    \"max_depth\": 2,\n    \"min_data_in_leaf\":77,\n    'rsm':0.5,\n    'l2_leaf_reg': 0.02247766515106271\n}\ncat_boost = CatBoostClassifier(**c_para)\ncat_boost.fit(xtrain,ytrain,eval_set=[(xvalid,yvalid)])\n\npred = cat_boost.predict(xvalid)\ntest_pred = cat_boost.predict(xtest)\nfinal_prediction1.append(test_pred)\n\nroc=roc_auc_score(yvalid,pred)\nscore.append(roc)\n#way of output is display\nprint(f\"fold:{fold},roc:{roc}\")\n\n#prediction of data\npreds = np.column_stack(final_prediction1)\nprint(preds)\nsample.target = preds\nsample.to_csv(\"submission_CAT.csv\",index=False)\nprint(\"success\")","c88b1bc5":"prediction1=[]\nscore = []\n#lgb parameters\nparams_lgb = {\n    \"task\": \"train\",\n    \"boosting_type\": \"gbdt\",\n    \"objective\": \"regression\",\n    'subsample': 0.95312,\n    'learning_rate': 0.0051635,\n    \"max_depth\": 4,\n    \"feature_fraction\": 0.2256038826485174,\n    \"bagging_fraction\": 0.7705303688019942,\n    \"min_child_samples\": 290,\n    \"reg_alpha\": 14.68267919457715,\n    \"reg_lambda\": 66.156,\n    \"max_bin\": 772,\n    \"min_data_per_group\": 177,\n    \"bagging_freq\": 1,\n    \"cat_smooth\": 96,\n    \"cat_l2\": 17,\n    \"verbosity\": -1,\n    'random_state':288,\n    'n_estimators':8000,\n    'colsample_bytree':0.1107,\n    'njobs':4\n}\n    \nlgb_train = lgb.Dataset(xtrain, ytrain)\nlgb_val = lgb.Dataset(xvalid, yvalid)\n    \nmodel = lgb.train(params=params_lgb,\n                      train_set=lgb_train,\n                      valid_sets=lgb_val,\n                      early_stopping_rounds=300,\n                      verbose_eval=1000)\n    \npreds_valid = model.predict(xvalid,num_iteration=model.best_iteration)\ntest_predict = model.predict(xtest,num_iteration=model.best_iteration)\nprediction1.append(test_predict)\n#ROC\nroc=roc_auc_score(yvalid,preds_valid)\n#Score \nscore.append(roc)\nprint(np.mean(score),np.std(score))\n\n#prediction of data\npreds = np.column_stack(prediction1)\nprint(preds)\nsample.target = preds\nsample.to_csv(\"submission_LGB.csv\",index=False)\nprint(\"success\")","80cd0317":"## **Import Necessary libraries** ","bbb40e1e":"## **CatBoostClassifier** ","24103882":"## **LGBM** ","5fac1787":"## **XGBoostRegressor_KFold_5 data** \n**I'm trying optuna and tune the hyperparameters the value of output is params_b**\n","4c3c4371":"## ***Bulid the Model***","c8f5f5a5":"## **Thankyou**  \ud83d\ude0a \ud83d\ude0a\n\n**I am trying different Boosting techniques and various hyperparameters** \ud83d\ude0a \ud83d\ude0a \ud83d\ude0a","feeddc14":"# **Tabular playground Series _ Nov 2021**\n\n## **Baseline :  STEPS**\n\n### ***First stage apporach of Problem --> Notebook_Boosting techniques***\n\n1. Import CV-KFOLD-Split5 Data\n1. Read the data\n2. Identify the null data\n3. Feature Selection\n4. Preprocessing the data\n5. Split the data(Train_Test_Split)\n6. Build the Model(I'm try XGBRegressor)\n7. XGBRegressor\n8. Catboost classifier\n9. LGBM\n10. ROC and Save csv file"}}