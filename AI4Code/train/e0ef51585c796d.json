{"cell_type":{"8e63cd40":"code","ed3adc8c":"code","1f28a166":"code","a744a1d0":"code","b625a7ba":"code","ee485cbb":"code","6994e31f":"code","dda9c92c":"code","1d9a5c93":"code","99cfdf86":"code","d5c44759":"code","83d4ebf4":"code","26cc7a0f":"code","c5d97a0c":"code","e44ffb40":"code","be768cf8":"code","4efae69f":"code","7c694024":"code","f52d7554":"code","1617a753":"code","17e031e3":"code","85530c97":"code","5792d8ab":"code","9d0ef5fc":"code","016301e3":"code","90464d3c":"code","68eeb157":"code","c197986a":"code","b16da10a":"code","ae697144":"code","56ff71ee":"code","7ada94d0":"code","bc100eea":"code","91164975":"code","8b2bb940":"code","9a4a472c":"code","48ecd103":"code","9d50745b":"code","a06c0cc9":"code","47aaeb9f":"code","d54d38b7":"code","4903bc63":"code","9005d648":"code","96a9033d":"code","fb526613":"code","dd6f0ef2":"code","d975eadb":"code","022b0b5c":"code","aa4b90d9":"code","995a12c9":"code","99863cdf":"code","8b66d394":"code","10c68f15":"code","d69f4d19":"code","197c4926":"code","5197a7a2":"code","db04df7f":"code","a0ebdc05":"code","1a68414a":"code","89f3a3a0":"code","2dea418d":"code","2cdaf182":"markdown","35e66b89":"markdown","9103926d":"markdown","25b8662a":"markdown","16afad1e":"markdown","4a0e6d20":"markdown","98e9b21c":"markdown","6049f56e":"markdown","1d5a6f61":"markdown","f8b556ac":"markdown","a391b3a4":"markdown","f5b47c0d":"markdown","857e681d":"markdown","82ce54b9":"markdown","c6106689":"markdown","6735e9d4":"markdown","60205ce2":"markdown","b9031601":"markdown","1f142b05":"markdown","6a1d5133":"markdown","ff34309b":"markdown","5d21a398":"markdown","8ad9e0d2":"markdown","aa3967e6":"markdown","c92ce171":"markdown","a74fbe6c":"markdown","c42e18cc":"markdown","9c9e124d":"markdown","7048e503":"markdown","6d90d4bf":"markdown","8ead48b9":"markdown","ee33d7e6":"markdown","8b56f489":"markdown","09ec1170":"markdown","d82b6089":"markdown","3a918f32":"markdown","189d3de8":"markdown","d130fb3a":"markdown","c52efaf9":"markdown","6b71fb54":"markdown"},"source":{"8e63cd40":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ed3adc8c":"# Part 1 - Data Preprocessing\n\n# Importing the libraries\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n# Importing the data set\nX = pd.read_csv('\/kaggle\/input\/vaxorno\/Flu_Shot_Learning_Predict_H1N1_and_Seasonal_Flu_Vaccines_-_Training_Features.csv',index_col=\"respondent_id\")\ny = pd.read_csv('\/kaggle\/input\/vaxorno\/Flu_Shot_Learning_Predict_H1N1_and_Seasonal_Flu_Vaccines_-_Training_Labels.csv',index_col=\"respondent_id\")       ","1f28a166":"X.info()","a744a1d0":"X.isnull().sum().sort_values(ascending=False)","b625a7ba":"for col in X.columns:\n    print('NANs    ',X[col].isnull().sum()) \n    print(X[col].value_counts())\n    print('')","ee485cbb":"import seaborn as sns\n# function to see how many missing data points there are for each row\/participant, it may be worth it to \n# remove every row with > 10 missing data points.\n# However there may be a relationship between missing data and vaccine status\nmissing = X.apply(lambda x: x.count(), axis=1)\nsns.histplot(data=missing)","6994e31f":"X['full_count'] = X.apply(lambda x: x.count(), axis=1)\nX['full_count'] = (X['full_count'] - min(X['full_count'])) \/ (max(X['full_count'])- min(X['full_count']))","dda9c92c":"ax = sns.regplot(x=\"full_count\", y=y[\"h1n1_vaccine\"], data=X, logistic=True)","1d9a5c93":"from scipy import stats\nstats.spearmanr(X['full_count'], list(y.h1n1_vaccine), nan_policy='omit')","99cfdf86":"ax = sns.regplot(x=\"full_count\", y=y[\"seasonal_vaccine\"], data=X, logistic=True)","d5c44759":"stats.spearmanr(X['full_count'], list(y.seasonal_vaccine), nan_policy='omit')","83d4ebf4":"y.seasonal_vaccine.value_counts()","26cc7a0f":"y.h1n1_vaccine.value_counts()","c5d97a0c":"baselinedf = X.fillna(X.mode().iloc[0])\ndummy_cols = list(baselinedf.select_dtypes(include=['object']).columns)\nbaselinedf = pd.get_dummies(baselinedf, columns=dummy_cols)","e44ffb40":"from sklearn.model_selection import train_test_split\nX_train, X_valid, y_train, y_valid = train_test_split(baselinedf, y.h1n1_vaccine, stratify=y.h1n1_vaccine,\n                                                      train_size=0.80,random_state=9) \n\nfrom sklearn.linear_model import LogisticRegression\nbaseh1 = LogisticRegression(solver='liblinear').fit(X_train, y_train)\nbaseh1.score(X_valid,y_valid)","be768cf8":"X_train, X_valid, y_train, y_valid = train_test_split(baselinedf, y.seasonal_vaccine, stratify=y.seasonal_vaccine,\n                                                      train_size=0.80,random_state=9) \n\nbaseseas = LogisticRegression(solver='liblinear').fit(X_train, y_train)\nbaseseas.score(X_valid,y_valid)","4efae69f":"X[['opinion_h1n1_vacc_effective','opinion_h1n1_risk',\n 'opinion_h1n1_sick_from_vacc','opinion_seas_vacc_effective','opinion_seas_risk',\n 'opinion_seas_sick_from_vacc']]=X[['opinion_h1n1_vacc_effective','opinion_h1n1_risk',\n 'opinion_h1n1_sick_from_vacc','opinion_seas_vacc_effective','opinion_seas_risk',\n 'opinion_seas_sick_from_vacc']].fillna(value=3)","7c694024":"X[['household_adults','household_children','h1n1_concern','h1n1_knowledge','behavioral_avoidance','behavioral_face_mask','behavioral_wash_hands',\n'behavioral_large_gatherings','behavioral_outside_home','behavioral_touch_face','chronic_med_condition','behavioral_antiviral_meds',\n'child_under_6_months','health_worker']] = X[['household_adults','household_children','h1n1_concern','h1n1_knowledge',\n'behavioral_avoidance','behavioral_face_mask','behavioral_wash_hands',\n'behavioral_large_gatherings','behavioral_outside_home','behavioral_touch_face','chronic_med_condition','behavioral_antiviral_meds',\n'child_under_6_months','health_worker']].fillna(X.mode().iloc[0])\n","f52d7554":"X.fillna(-1,inplace=True)","1617a753":"to_change=['household_adults','household_children','h1n1_concern','h1n1_knowledge','opinion_h1n1_vacc_effective',\n          'opinion_h1n1_risk','opinion_h1n1_sick_from_vacc','opinion_seas_vacc_effective','opinion_seas_risk',\n          'opinion_seas_sick_from_vacc']\nfor col in to_change:\n    X[col] = (X[col] -min(X[col])) \/ (max(X[col])- min(X[col]))","17e031e3":"X.hist(grid = False, figsize=(25,80), layout=(29, 5), bins=50)","85530c97":"non_dummy = ['h1n1_concern','h1n1_knowledge','opinion_h1n1_vacc_effective','opinion_h1n1_risk',\n 'opinion_h1n1_sick_from_vacc','opinion_seas_vacc_effective','opinion_seas_risk',\n 'opinion_seas_sick_from_vacc','full_count','household_adults','household_children','behavioral_avoidance',\n'behavioral_face_mask','behavioral_wash_hands','behavioral_large_gatherings','behavioral_outside_home',\n'behavioral_touch_face','chronic_med_condition','child_under_6_months','health_worker']\n\ndummy_cols = list(set(X.columns) - set(non_dummy))\nnewdf = pd.DataFrame()\nnewdf = pd.get_dummies(X, columns=dummy_cols) ","5792d8ab":"#Add targets to dataset\n\nnewdf = newdf.join(y)\n\nsns.set(rc={'figure.figsize':(15,15)})\nsns.heatmap(newdf.corr())","9d0ef5fc":"for col in newdf.columns:\n    c,d = stats.spearmanr(list(newdf[col]), list(y.h1n1_vaccine), nan_policy='omit')\n    if c < -.1 or c>.1:\n        print(col)\n        print(stats.spearmanr(list(newdf[col]), list(y.h1n1_vaccine), nan_policy='omit'))\n        print('')\n","016301e3":"for col in newdf.columns:\n    c,p = stats.spearmanr(list(newdf[col]), list(y.seasonal_vaccine), nan_policy='omit')\n    if c < -.1 or c>.1:\n        print(col)\n        print(stats.spearmanr(list(newdf[col]), list(y.seasonal_vaccine), nan_policy='omit'))\n        print('')\n        ","90464d3c":"#Remove the targets from the dataset\n\nnewdf.drop(['h1n1_vaccine','seasonal_vaccine'], axis=1, inplace=True)","68eeb157":"X_train, X_valid, y_train, y_valid = train_test_split(newdf, y.h1n1_vaccine, stratify=y.h1n1_vaccine, \n                                                      train_size=0.80, random_state=9) \n\nclfh1n1 = LogisticRegression(solver='liblinear').fit(X_train, y_train)\nclfh1n1.score(X_valid,y_valid)\n","c197986a":"# Importing the Keras libraries and packages\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nimport tensorflow as tf\n#from tensorflow.keras import models\n\n\nh1n1nn = keras.Sequential([\nlayers.BatchNormalization(input_shape= [newdf.shape[1]]),\nlayers.Dense(512, activation='relu'),\nlayers.BatchNormalization(),\nlayers.Dropout(.6),\nlayers.Dense(1024, activation='relu'),    \nlayers.BatchNormalization(),\nlayers.Dropout(.6),\nlayers.Dense(256, activation='relu'),    \nlayers.BatchNormalization(),\nlayers.Dropout(.6),\nlayers.Dense(1, activation='sigmoid'),\n])\n\nopt = keras.optimizers.Adam(learning_rate=0.0001)\nh1n1nn.compile(\n    optimizer= opt,\n    loss='binary_crossentropy',\n    metrics=['binary_accuracy'],\n)\n","b16da10a":"early_stopping = keras.callbacks.EarlyStopping(\n    patience=30,\n    min_delta=0.001,\n    restore_best_weights=True,\n    monitor='val_binary_accuracy'\n)\n\nhistory = h1n1nn.fit(\n    X_train, y_train,\n    validation_data=(X_valid, y_valid),\n    batch_size=80,\n    epochs=50,\n    callbacks=[early_stopping],\n)\n\nhistory_df = pd.DataFrame(history.history)\nhistory_df.loc[:, ['loss', 'val_loss']].plot(title=\"Cross-entropy\")\nhistory_df.loc[:, ['binary_accuracy', 'val_binary_accuracy']].plot(title=\"binary_accuracy\")\n\n","ae697144":"### Function to evaluate model\n\ndef model_eval(model,X_valid,y_valid):\n    final = model.predict(X_valid)\n    c = 0\n    yy = y_valid.tolist()\n    f = final.tolist()\n    ff = []\n    # create list of 0,1 from .xx\n    for i in range(len(f)):\n        if f[i] > [.5001]:\n            ff.append(1)\n        else:\n            ff.append(0)\n\n    #compare prediction with actual\n    for i in range(len(ff)):\n        if ff[i]==yy[i]:\n            c+=1\n    print(c\/len(ff))\n    ","56ff71ee":"model_eval(h1n1nn,X_valid,y_valid)","7ada94d0":"\nX_train, X_valid, y_train, y_valid = train_test_split(newdf, y.seasonal_vaccine, stratify=y.seasonal_vaccine,\n                                                      train_size=0.80,random_state=9) \n\nclfseas = LogisticRegression(solver='liblinear').fit(X_train, y_train)\nclfseas.score(X_valid,y_valid)","bc100eea":"# Importing the Keras libraries and packages\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nimport tensorflow as tf\n#from tensorflow.keras import models\n\n\nseasnn = keras.Sequential([\nlayers.BatchNormalization(input_shape= [newdf.shape[1]]),\nlayers.Dense(512, activation='relu'),\nlayers.BatchNormalization(),\nlayers.Dropout(.6),\nlayers.Dense(1024, activation='relu'),    \nlayers.BatchNormalization(),\nlayers.Dropout(.6),\nlayers.Dense(256, activation='relu'),    \nlayers.BatchNormalization(),\nlayers.Dropout(.6),\nlayers.Dense(1, activation='sigmoid'),\n])\n\nopt = keras.optimizers.Adam(learning_rate=0.0005)\nseasnn.compile(\n    optimizer= opt,\n    loss='binary_crossentropy',\n    metrics=['binary_accuracy'],\n)","91164975":"early_stopping = keras.callbacks.EarlyStopping(\n    patience=30,\n    min_delta=0.001,\n    restore_best_weights=True,\n    monitor='val_binary_accuracy'\n)\n\nhistory = seasnn.fit(\n    X_train, y_train,\n    validation_data=(X_valid, y_valid),\n    batch_size=80,\n    epochs=50,\n    callbacks=[early_stopping],\n)\n\nhistory_df = pd.DataFrame(history.history)\nhistory_df.loc[:, ['loss', 'val_loss']].plot(title=\"Cross-entropy\")\nhistory_df.loc[:, ['binary_accuracy', 'val_binary_accuracy']].plot(title=\"binary_accuracy\")","8b2bb940":"model_eval(seasnn,X_valid,y_valid)","9a4a472c":"X_train, X_valid, y_train, y_valid = train_test_split(newdf, y.h1n1_vaccine, stratify=y.h1n1_vaccine, \n                                                      train_size=0.80, random_state=9) \nimport eli5\nfrom eli5.sklearn import PermutationImportance\n\nperm = PermutationImportance(clfh1n1, random_state=1).fit(X_valid, y_valid)\neli5.show_weights(perm, feature_names = X_valid.columns.tolist(),top=20)\n","48ecd103":"X_train, X_valid, y_train, y_valid = train_test_split(newdf, y.seasonal_vaccine, stratify=y.seasonal_vaccine,\n                                                      train_size=0.80,random_state=9) \n\n\nperm = PermutationImportance(clfseas, random_state=1).fit(X_valid, y_valid)\neli5.show_weights(perm, feature_names = X_valid.columns.tolist(),top=20)","9d50745b":"from pdpbox import pdp, get_dataset, info_plots\nfeatures_to_plot = ['opinion_h1n1_risk', 'doctor_recc_h1n1_1.0']\ninter1  =  pdp.pdp_interact(model=clfh1n1, dataset=X_valid, model_features=newdf.columns, features=features_to_plot)\n\npdp.pdp_interact_plot(pdp_interact_out=inter1, feature_names=features_to_plot, plot_type='contour')\nplt.show()","a06c0cc9":"features_to_plot = ['opinion_h1n1_risk', 'opinion_h1n1_vacc_effective']\ninter1  =  pdp.pdp_interact(model=clfh1n1, dataset=X_valid, model_features=newdf.columns, features=features_to_plot)\n\npdp.pdp_interact_plot(pdp_interact_out=inter1, feature_names=features_to_plot, plot_type='contour')\nplt.show()","47aaeb9f":"X_train, X_valid, y_train, y_valid = train_test_split(newdf, y.seasonal_vaccine, stratify=y.seasonal_vaccine,\n                                                      train_size=0.80,random_state=9) ","d54d38b7":"features_to_plot = ['opinion_seas_sick_from_vacc', 'chronic_med_condition']\ninter1  =  pdp.pdp_interact(model=clfseas, dataset=X_valid, model_features=newdf.columns, features=features_to_plot)\n\npdp.pdp_interact_plot(pdp_interact_out=inter1, feature_names=features_to_plot, plot_type='contour')\nplt.show()","4903bc63":"features_to_plot = ['opinion_seas_risk', 'opinion_seas_vacc_effective']\ninter1  =  pdp.pdp_interact(model=clfseas, dataset=X_valid, model_features=newdf.columns, features=features_to_plot)\n\npdp.pdp_interact_plot(pdp_interact_out=inter1, feature_names=features_to_plot, plot_type='contour')\nplt.show()\n\n","9005d648":"newdf['season_risk_effective'] = (newdf['opinion_seas_risk']* newdf['opinion_seas_vacc_effective'])\nnewdf['h1n1_risk_effective'] = (newdf['opinion_h1n1_risk']* newdf['opinion_h1n1_vacc_effective'])\n\nnewdf['nodoc_or_insur_h1n1'] = newdf['doctor_recc_h1n1_-1.0']+newdf['health_insurance_-1.0']\nnewdf['nodoc_or_insur_seas'] = newdf['doctor_recc_seasonal_-1.0']+newdf['health_insurance_-1.0']\n\nnewdf['afriad_sick'] = newdf['opinion_seas_sick_from_vacc']+newdf['chronic_med_condition']\n\n\n## Min max scaler, keep data in 0-1 range.\n\nto_change2 = ['afriad_sick','nodoc_or_insur_h1n1','nodoc_or_insur_seas',\n              'season_risk_effective','h1n1_risk_effective']\nfor col in to_change2:\n    newdf[col] = (newdf[col] -min (newdf[col])) \/ (max(newdf[col])- min (newdf[col]))\n","96a9033d":"X_train, X_valid, y_train, y_valid = train_test_split(newdf, y.seasonal_vaccine, stratify=y.seasonal_vaccine,\n                                                      train_size=0.80,random_state=9) \n\nclfseas2 = LogisticRegression(solver='liblinear').fit(X_train, y_train)\nclfseas2.score(X_valid,y_valid)","fb526613":"X_train, X_valid, y_train, y_valid = train_test_split(newdf, y.h1n1_vaccine, stratify=y.h1n1_vaccine, \n                                                      train_size=0.80, random_state=9) \n\nclfh1n2 = LogisticRegression(solver='liblinear').fit(X_train, y_train)\nclfh1n2.score(X_valid,y_valid)","dd6f0ef2":"from sklearn.cluster import KMeans\n\nfeatures = newdf[['chronic_med_condition','opinion_h1n1_vacc_effective','opinion_seas_risk','doctor_recc_seasonal_0.0','behavioral_wash_hands',\n'behavioral_touch_face','health_worker','household_children','doctor_recc_seasonal_1.0','health_insurance_1.0']]\n\n#cluster\nkmeans = KMeans(n_clusters=9, random_state=0)\nnewdf[\"Cluster\"] = kmeans.fit_predict(features)\n\nnewdf = pd.get_dummies(newdf, columns=[\"Cluster\"])","d975eadb":"from sklearn.feature_selection import mutual_info_classif\n\ndef make_mi_scores(X, y):\n    mi_scores = mutual_info_classif(X, y)\n    mi_scores = pd.Series(mi_scores, name=\"MI Scores\", index=X.columns)\n    mi_scores = mi_scores.sort_values(ascending=False)\n    return mi_scores\n\nseasmi_scores = make_mi_scores(newdf, y.seasonal_vaccine)\nseasmi_scores[::3]","022b0b5c":"# Now for H1N1 \nh1n1mi_scores = make_mi_scores(newdf, y.h1n1_vaccine)\nh1n1mi_scores[::3]  # MI scores","aa4b90d9":"to_remove = []\nfor index, value in zip(h1n1mi_scores.items(),seasmi_scores.items()):\n    if value[1] < .001 and index[1] < .001:\n        to_remove.append(index[0])\nnewdf.drop(columns = to_remove,inplace=True)\nto_remove","995a12c9":"sns.set(rc={'figure.figsize':(15,15)})\nsns.heatmap(newdf.corr())","99863cdf":"X_train, X_valid, y_train, y_valid = train_test_split(newdf, y.seasonal_vaccine, stratify=y.seasonal_vaccine,\n                                                      train_size=0.80,random_state=9) \n\nclfseas3 = LogisticRegression(solver='liblinear').fit(X_train, y_train)\nclfseas3.score(X_valid,y_valid)","8b66d394":"X_train, X_valid, y_train, y_valid = train_test_split(newdf, y.h1n1_vaccine, stratify=y.h1n1_vaccine,\n                                                      train_size=0.80,random_state=9) \n\nclfh1n3 = LogisticRegression(solver='liblinear').fit(X_train, y_train)\nclfh1n3.score(X_valid,y_valid)","10c68f15":"X_train, X_valid, y_train, y_valid = train_test_split(newdf, y.h1n1_vaccine, stratify=y.h1n1_vaccine, \n                                                      train_size=0.80, random_state=9) ","d69f4d19":"# Importing the Keras libraries and packages\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nimport tensorflow as tf\n#from tensorflow.keras import models\n\n\nh1n2nn = keras.Sequential([\nlayers.BatchNormalization(input_shape= [newdf.shape[1]]),\nlayers.Dense(256, activation='relu'),\nlayers.BatchNormalization(),\nlayers.Dropout(.6),\nlayers.Dense(512, activation='relu'),    \nlayers.BatchNormalization(),\nlayers.Dropout(.6),\nlayers.Dense(1024, activation='relu'),    \nlayers.BatchNormalization(),\nlayers.Dropout(.6),\nlayers.Dense(256, activation='relu'),    \nlayers.BatchNormalization(),\nlayers.Dropout(.6),\nlayers.Dense(1, activation='sigmoid'),\n])\n\nopt = keras.optimizers.Adam(learning_rate=0.0001)\nh1n2nn.compile(\n    optimizer= opt,\n    loss='binary_crossentropy',\n    metrics=['binary_accuracy'],\n)","197c4926":"early_stopping = keras.callbacks.EarlyStopping(\n    patience=100,\n    min_delta=0.001,\n    restore_best_weights=True,\n    monitor='val_binary_accuracy'\n)\n\nhistory = h1n2nn.fit(\n    X_train, y_train,\n    validation_data=(X_valid, y_valid),\n    batch_size=16,\n    epochs=150,\n    callbacks=[early_stopping],\n)\n\nhistory_df = pd.DataFrame(history.history)\nhistory_df.loc[:, ['loss', 'val_loss']].plot(title=\"Cross-entropy\")\nhistory_df.loc[:, ['binary_accuracy', 'val_binary_accuracy']].plot(title=\"binary_accuracy\")\n","5197a7a2":"model_eval(h1n2nn,X_valid,y_valid)","db04df7f":"yhat = h1n2nn.predict(newdf)\nnewdf['h1n1_vaccine_hat'] = yhat","a0ebdc05":"X_train, X_valid, y_train, y_valid = train_test_split(newdf, y.seasonal_vaccine, stratify=y.seasonal_vaccine,\n                                                      train_size=0.80,random_state=9) ","1a68414a":"# Importing the Keras libraries and packages\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nimport tensorflow as tf\n#from tensorflow.keras import models\n\n\nseas2nn = keras.Sequential([\nlayers.BatchNormalization(input_shape= [newdf.shape[1]]),\nlayers.Dense(128, activation='relu'),\nlayers.BatchNormalization(),\nlayers.Dropout(.5),\nlayers.Dense(512, activation='relu'),    \nlayers.BatchNormalization(),\nlayers.Dropout(.4),\nlayers.Dense(2048, activation='relu'),    \nlayers.BatchNormalization(),\nlayers.Dropout(.5),\nlayers.Dense(512, activation='relu'),    \nlayers.BatchNormalization(),\nlayers.Dropout(.4),\nlayers.Dense(256, activation='relu'),    \nlayers.BatchNormalization(),\nlayers.Dropout(.4),\nlayers.Dense(1, activation='sigmoid'),\n])\n\nopt = keras.optimizers.Adam(learning_rate=0.0001)\nseas2nn.compile(\n    optimizer= opt,\n    loss='binary_crossentropy',\n    metrics=['binary_accuracy'],\n)","89f3a3a0":"early_stopping = keras.callbacks.EarlyStopping(\n    patience=100,\n    min_delta=0.001,\n    restore_best_weights=True,\n    monitor='val_binary_accuracy'\n)\n\nhistory = seas2nn.fit(\n    X_train, y_train,\n    validation_data=(X_valid, y_valid),\n    batch_size=32,\n    epochs=150,\n    callbacks=[early_stopping],\n)\n\nhistory_df = pd.DataFrame(history.history)\nhistory_df.loc[:, ['loss', 'val_loss']].plot(title=\"Cross-entropy\")\nhistory_df.loc[:, ['binary_accuracy', 'val_binary_accuracy']].plot(title=\"binary_accuracy\")","2dea418d":"model_eval(seas2nn,X_valid,y_valid)","2cdaf182":"There are 2 target variables. Let's look at those.","35e66b89":"# **Load data**  **-ETL-** \n\nThis will be trivial because data are already in csv form","9103926d":"Now for the H1N1 target.","25b8662a":"Accuracy only increased slightly but this looks much better in terms of overfitting.","16afad1e":"For the rest of the missing data fill with -1. This will effectivly create create a new column of data when one-hot encoding takes place.","4a0e6d20":"# **Model Evaluation**","98e9b21c":"Take aways. All features are categorical or nominal. Most data are here, there are some concerns.  \n* Employment_industry and occupation have alot of missing data but this is probably due to ~12000 people are unemployed+not in work force. Will create another variable N\/A for these nans.\n* 13000 NaNs for health insurance could be problematic. \n* 2160 NaNs for doctor_recc_seasonal and doctor_recc_h1n1.\n* 4423 NaNs for income_poverty \n\nWill probably want to address nans differently for each feature. Most will need to be One-Hot-Encoded. Next let's check the missing values per row.","6049f56e":"# **Model Evaluation**","1d5a6f61":"These plots reaffirms our intuition that if someone thinks H1N1 is dangerous and the vaccine is effective they will be much more likley to be vaccinated.","f8b556ac":"The heatmap looks cool and there seems to be some structure but it is difficult to see exactly what is going on. We may need to remove some columns if the models are suffering from multicollinearity. For now, lets run some quick correlations to see what is correlating with the targets. First H1N1","a391b3a4":"Our baseline was ~83% this is a solid improvement. Next lets create a neural network.","f5b47c0d":"This reinforces the inital importance testing we did with regression after the heatmap. Now we can play around with some variables and see if there are any interaction effects to create new features with.","857e681d":"Next we will remove all of the features that do not contribute to the seasonsal or H1N1 prediction.","82ce54b9":"**Final result**\n\nThis is the end. We started with 2 logistic regression models with minimal feature creation\/engineering which scored h1n1=.832, seasonal =.783. The first round of neural networks scored h1n1=.848 and seasonal =.793. After 2 rounds of feature engineering the final neural networks accuracy are h1n1=.85 and seasonal =.79.\n\nIn the future it may be worth splitting the datasets per target, minimizing the noise. Spend more time with PCA to reduce dimensions. Also to look more into the stat collection process and try to get a better understanding of the missing data.","c6106689":"Lets do the same thing with the other target, seasonal_vaccine.","6735e9d4":"This notebook for the IBM Advanced Data Science Capstone project. It creates 2 models using the same dataset. One to predict if someone is vaccinated for H1N1, the other for the seasonal flu. The data was collected by the NIH.\n\nIt starts by importing the data, checking the data, cleaning the data, building and testing a model, a round of feature creation and a final test of the model.","60205ce2":"# **Model Training\/Evaluation**\n\nTime to build the model. We will start with a logistic regression model to compare to the baseline. After this we will build a neural network model.","b9031601":"# **Model Evaluation**","1f142b05":"The first round of the process is complete. So far, our models acheived a 85% accuracy for predicting if someone had a H1N1 vaccine and 79% for prediciting if they recieved the seasonal flu vaccine. Unfortantly the neural networks overfit and did not perform much better than the logistic regression. \n\nHow can we improve this?\n* We can go back and rethink some of the imputation strategies. \n* More feature engineering\n* Remebering the heatmap, there were some variables that corrlated with eachother, we could apply a reduction technique like PCA to address this\n* Over fitting will have to be dealt with in the model, adjust layers and neurons. Removing features that have little predicitive power can help too.\n\nFirst we will start another round of feature engineering and investigate the features with permutation importance and mutual information. \n\nFirst for H1N1","6a1d5133":"Normalize data rest of data","ff34309b":"The network was overfitting lets try to reduce the number of neurons to try to fix this. We will start with the h1n1 model.","5d21a398":"Lets rerun all of this for the seasonal_vaccine","8ad9e0d2":"Lets see if these created features helped to improve the accuracy. First for the seasonal target.","aa3967e6":"First let's put minimal thought into filling NaNs to develop a baseline","c92ce171":"Most rows have most data. Let's create our first new feature of missing data. This may help if there is a systematic reason data is missing or if missing data can help to predict target variable.","a74fbe6c":"# **Model Evaluation**","c42e18cc":"This is the accuracy we are looking to beat by implementing better feature engineering and data imputation.","9c9e124d":"**Feature Engineering**\n\nNext let's start data imputation.\nThere are 6 features of opinions with responses from 1-5. These make up most of the non binary\/categorical features in this dataset.\n# Example\nopinion_h1n1_vacc_effective - Respondent's opinion about H1N1 vaccine effectiveness.\n* 1 = Not at all effective; \n* 2 = Not very effective; \n* 3 = Don't know; \n* 4 = Somewhat effective; \n* 5 = Very effective.\n\nFor these I will fill nans with 3. This could be problematic because 3 is often the least frequent selection.\n","7048e503":"Now lets look at seasonal","6d90d4bf":"Look at all the unique values with the missing data","8ead48b9":"# **Feature Creation**","ee33d7e6":"Seasonal vaccines are evenly distrubted. There is a significant disproportion of h1n1 targets.","8b56f489":"# **Data Exploration**","09ec1170":"Next use mode imputation for features missing minimal data.","d82b6089":"Next lets check mutual information. Mutual information will allow us to see more than just linear interactions and detect how much impact each feature has. But, before we do that lets add a k-means variable to see if we can get any additional information.","3a918f32":"Lets take a look at the distribution of data before dummies","189d3de8":"First Mutual information scores for seasonal","d130fb3a":"Check quality of data. Questions to ask. \n* How much data is there? \n* What are the types of data(integers,strings)?\n* Are there missing values?\n","c52efaf9":"Now lets check some for Seasonal vaccine","6b71fb54":"There seems to be a modest relationship between H1N1 vaccine and missing values and no relationship with seasonal vaccine. I will leave the full_count feature for now. An option will be to remove that feature and all rows missing too much data later."}}