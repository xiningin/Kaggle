{"cell_type":{"a70fbe91":"code","59a2c065":"code","fac30b10":"code","98d05a28":"code","bef280ca":"code","0d097813":"code","6aca7315":"code","43165ff4":"code","3db3a9f0":"code","faa0c52f":"code","9019ecd3":"code","8f2c3d0c":"code","ede07305":"code","ab9a3304":"code","b1a88caf":"code","31a9ddb1":"code","9e0e4d09":"code","fc1d9c99":"code","c4e0ee1c":"code","64b4e7d8":"code","01987b48":"markdown","c0a3a585":"markdown","25a087a7":"markdown","575c1e74":"markdown","2436530e":"markdown","70ee302c":"markdown","92c3b25e":"markdown","3def3124":"markdown","435313f8":"markdown","45d511e8":"markdown","ca70223d":"markdown","7a2b230f":"markdown","31ecd393":"markdown","c13ec5f9":"markdown","4669dc1a":"markdown","40d0323e":"markdown","5e3112c5":"markdown","dd989e52":"markdown","a106b042":"markdown","6b89c1f8":"markdown","b297431a":"markdown","c31804ee":"markdown","a33ea126":"markdown","950c26f8":"markdown","bc97808c":"markdown","a87146da":"markdown","9e532d41":"markdown","03b34455":"markdown","8948eb70":"markdown","50deb9b3":"markdown","57372480":"markdown","cbef7285":"markdown","71658fba":"markdown","02a4d94d":"markdown","208ecb91":"markdown","01040468":"markdown","5bcbbb85":"markdown","91e3ef0e":"markdown","52a4cc20":"markdown","5d485654":"markdown","dd7b1de0":"markdown","81206470":"markdown","3f9695a9":"markdown"},"source":{"a70fbe91":"import pandas  # for dataframes\nimport matplotlib.pyplot as plt # for plotting graphs\nimport seaborn as sns # for plotting graphs","59a2c065":"data=pandas.read_csv('HR_comma_sep.csv')","fac30b10":"data.head() # \u201chead()\u201dfunction of pandas library which returns first five observations.\n#Here, Original data is separated by comma delimiter(\u201c , \u201c) in given data set.","98d05a28":"data.tail() #\u201ctail()\u201d returns last five observations.","bef280ca":"data.info() # check attributes names and datatypes using info().","0d097813":"left = data.groupby('left')\nleft.mean()","6aca7315":"data.describe()","43165ff4":"left_count=data.groupby('left').count()\nplt.bar(left_count.index.values, left_count['satisfaction_level'])\nplt.xlabel('Employees Left Company')\nplt.ylabel('Number of Employees')\nplt.show()","3db3a9f0":"data.left.value_counts()","faa0c52f":"num_projects=data.groupby('number_project').count()\nplt.bar(num_projects.index.values, num_projects['satisfaction_level'])\nplt.xlabel('Number of Projects')\nplt.ylabel('Number of Employees')\nplt.show()","9019ecd3":"time_spent=data.groupby('time_spend_company').count()\nplt.bar(time_spent.index.values, time_spent['satisfaction_level'])\nplt.xlabel('Number of Years Spend in Company')\nplt.ylabel('Number of Employees')\nplt.show()","8f2c3d0c":"features=['number_project','time_spend_company','Work_accident','left', 'promotion_last_5years','Departments ','salary']\nfig=plt.subplots(figsize=(10,15))\nfor i, j in enumerate(features):\n    plt.subplot(4, 2, i+1)\n    plt.subplots_adjust(hspace = 1.0)\n    sns.countplot(x=j,data = data)\n    plt.xticks(rotation=90)\n    plt.title(\"No. of employee\")","ede07305":"fig=plt.subplots(figsize=(10,15))\nfor i, j in enumerate(features):\n    plt.subplot(4, 2, i+1)\n    plt.subplots_adjust(hspace = 1.0)\n    sns.countplot(x=j,data = data, hue='left')\n    plt.xticks(rotation=90)\n    plt.title(\"No. of employee\")","ab9a3304":"#import module\nfrom sklearn.cluster import KMeans\n# Filter data\nleft_emp =  data[['satisfaction_level', 'last_evaluation']][data.left == 1]\n# Create groups using K-means clustering.\nkmeans = KMeans(n_clusters = 3, random_state = 0).fit(left_emp)","b1a88caf":"# Add new column \"label\" annd assign cluster labels.\nleft_emp['label'] = kmeans.labels_\n# Draw scatter plot\nplt.scatter(left_emp['satisfaction_level'], left_emp['last_evaluation'], c=left_emp['label'],cmap='Accent')\nplt.xlabel('Satisfaction Level')\nplt.ylabel('Last Evaluation')\nplt.title('3 Clusters of employees who left')\nplt.show()","31a9ddb1":"# Import LabelEncoder\nfrom sklearn import preprocessing\n#creating labelEncoder\nle = preprocessing.LabelEncoder()\n# Converting string labels into numbers.\ndata['salary']=le.fit_transform(data['salary'])\ndata['Departments ']=le.fit_transform(data['Departments '])","9e0e4d09":"#Spliting data into Feature and\nX=data[['satisfaction_level', 'last_evaluation', 'number_project',\n       'average_montly_hours', 'time_spend_company', 'Work_accident',\n       'promotion_last_5years', 'Departments ', 'salary']]\ny=data['left']","fc1d9c99":"# Import train_test_split function\nfrom sklearn.model_selection import train_test_split\n\n# Split dataset into training set and test set\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)  # 70% training and 30% test","c4e0ee1c":"#Import Gradient Boosting Classifier model\nfrom sklearn.ensemble import GradientBoostingClassifier\n\n#Create Gradient Boosting Classifier\ngb = GradientBoostingClassifier()\n\n#Train the model using the training sets\ngb.fit(X_train, y_train)\n\n#Predict the response for test dataset\ny_pred = gb.predict(X_test)","64b4e7d8":"#Import scikit-learn metrics module for accuracy calculation\nfrom sklearn import metrics\n# Model Accuracy, how often is the classifier correct?\nprint(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n# Model Precision\nprint(\"Precision:\",metrics.precision_score(y_test, y_pred))\n# Model Recall\nprint(\"Recall:\",metrics.recall_score(y_test, y_pred))","01987b48":"# iii. Model Building","c0a3a585":"# i.Loading Dataset","25a087a7":"# Importing Modules","575c1e74":"# 1. Data loading and understanding feature","2436530e":"Cover the following topics:\n\n* Data loading and understanding feature\n* Exploratory data analysis \n* Data visualization\n* Cluster analysis\n* Building prediction model\n* Evaluating model performance","70ee302c":"This is how you can analyze the features one by one, but it will be time-consuming. The better option is here to use Seaborn library and plot all the graphs in a single run using subplots.","92c3b25e":"# 5. Building a Prediction Model using Gradient Boosting Tree.","3def3124":"# 4. Cluster Analysis:","435313f8":"To understand model performance, dividing the dataset into a training set and a test set is a good strategy.\n\nLet's split dataset by using function train_test_split(). You need to pass 3 parameters features, target, and test_set size. Additionally, you can use random_state to select records randomly.","45d511e8":"Here, Employee who left the company can be grouped into 3 type of employees:\n\n* High Satisfaction and High Evaluation(Shaded by green color in the graph), you can also call them Winners.\n* Low Satisfaction and High Evaluation(Shaded by blue color(Shaded by green color in the graph), you can also call them Frustrated.\n* Moderate Satisfaction and moderate Evaluation (Shaded by grey color in the graph), you can also call them 'Bad match'.","ca70223d":"Lots of machine learning algorithms require numerical input data, so you need to represent categorical columns in a numerical column.\n\nIn order to encode this data, you could map each value to a number. e.g. Salary column's value can be represented as low:0, medium:1, and high:2.\n\nThis process is known as label encoding, and sklearn conveniently will do this for you using LabelEncoder","7a2b230f":"# ii. Expressing data","31ecd393":"The describe() function in pandas is convenient in getting various summary statistics. This function returns the count, mean, standard deviation, minimum and maximum values and the quantiles of the data.","c13ec5f9":"# iv. Subplots using Seaborn","4669dc1a":"# iii. Data Insights","40d0323e":"Well, got a classification rate of 97%, considered as good accuracy.\n\n* Precision: Precision is about being precise, i.e., how precise your model is. In other words, you can say, when a model makes a prediction, how often it is correct. In your prediction case, when your Gradient Boosting model predicted an employee is going to leave, that employee actually left 95% of the time.\n\n* Recall: If there is an employee who left present in the test set and your Gradient Boosting model can identify it 92% of the time.","5e3112c5":"Here, Dataset is broken into two parts in ratio of 70:30. It means 70% data will used for model training and 30% for model testing.","dd989e52":"# ii. Split Train and Test Set","a106b042":"# 2. Exploratory Analysis","6b89c1f8":"Let's build employee an churn prediction model.\n\nHere, you are going to predict churn using Gradient Boosting Classifier.\n\nFirst, import the GradientBoostingClassifier module and create Gradient Boosting classifier object using GradientBoostingClassifier() function.\n\nThen, fit your model on train set using fit() and perform prediction on the test set using predict().","b297431a":"# 3. Data Visualization","c31804ee":"Most of the employee is doing the project from 3-5.","a33ea126":"Observated the following points in the above visualization:\n\n* Most of the employee is doing the project from 3-5.\n* There is a huge drop between 3 years and 4 years experienced employee.\n* The no of employee left is 23 % of the total employment.\n* A decidedly less number of employee get the promotion in the last 5 year.\n* The sales department is having maximum no.of employee followed by technical and support\n* Most of the employees are getting salary either medium or low.","950c26f8":"Similarly, plot a bar graph to count the number of employees have based on how much experience","bc97808c":"Check how many employees were left?\n\nHere, a bar graph is plotted using Matplotlib. The bar graph is suitable for showing discrete variable counts.","a87146da":"Here, out of 15,000 approx 3,571 were left, and 11,428 stayed. The no of employee left is 23 % of the total employment.","9e532d41":"# i. Employees Left","03b34455":"Let's find out the groups of employees who left. The most important factor for any employee to stay or leave is satisfaction and performance in the company. So let's bunch them in the group of people using cluster analysis.","8948eb70":"There are two types of employee one who stayed and another who left the company. So, you can divide data into two groups and compare their characteristics. Here, you can find the average of both the groups using groupby() and mean() function.","50deb9b3":"* In Research, it was found that employee churn will be affected by age, tenure, pay, job satisfaction, salary, working conditions, growth potential and employee\u2019s perceptions of fairness. \n* Some other variables such as age, gender, ethnicity, education, and marital status, were essential factors in the prediction of employee churn. \n* In some cases such as the employee with niche skills are harder to replace. It affects the ongoing work and productivity of existing employees. \n* Acquiring new employees as a replacement has its costs such as hiring costs and training costs. \n* Also, the new employee will take time to learn skills at the similar level of technical or business expertise knowledge of an older employee. \n* Organizations tackle this problem by applying machine learning techniques to predict employee churn, which helps them in taking necessary actions.","57372480":"Above can interpret, Employees who left the company had low satisfaction level, low promotion rate, low salary, and worked more compare to who stayed in the company.","cbef7285":"Following features are most influencing a person to leave the company:\n\n* Promotions: Employees are far more likely to quit their job if they haven't received a promotion in the last 5 years.\n* Time with Company: Here, The three-year mark looks like a time to be a crucial point in an employee's career. Most of them quit their job around the three-year mark. Another important point is 6-years point, where the employee is very unlikely to leave.\n* Number Of Projects: Employee engagement is another critical factor to influence the employee to leave the company. Employees with 3-5 projects are less likely to leave the company. The employee with less and more number of projects are likely to leave.\n* Salary: Most of the employees that quit among the mid or low salary groups.","71658fba":"# i. Pre-Processing Data","02a4d94d":"# ii. Number of Projects","208ecb91":"Here, you imported preprocessing module and created Label Encoder object. Using this LabelEncoder object you fit and transform \"salary\" and \"Departments \" column into numeric column.","01040468":"# iii. Time Spent in Company","5bcbbb85":"# 6. Evaluating Model Performance","91e3ef0e":"# EMPLOYEE CHURN PREDICTION","52a4cc20":"Similarly, plot a bar graph to count the number of employees deployed on How many projects?","5d485654":"Most of the employee experience between 2-4 years. Also, there is a massive gap between 3 years and 4 years experienced employee.","dd7b1de0":"# v. Data Analysis and Visualization Summary:","81206470":"This dataset has 14,999 samples, and 10 attributes(6 integer, 2 float, and 2 objects).\nNo variable column has null\/missing values.\nYou can describe 10 attributes in detail as:\n\n* satisfaction_level: It is employee satisfaction point, which ranges from 0-1.\n* last_evaluation: It is evaluated performance by the employer, which also ranges from 0-1.\n* number_projects: How many numbers of projects assigned to an employee?\n* average_monthly_hours: How many average numbers of hours worked by an employee in a month?\n* time_spent_company: time_spent_company means employee experience. The number of years spent by an employee in the company.\n* work_accident: Whether an employee has had a work accident or not.\n* promotion_last_5years: Whether an employee has had a promotion in the last 5 years or not.\n* Departments: Employee's working department\/division.\n* Salary: Salary level of the employee such as low, medium and high.\n* left: Whether the employee has left the company or not.","3f9695a9":"Observed the following points in the above visualization:\n\n* Those employees who have the number of projects more than 5 were left the company.\n* The employee who had done 6 and 7 projects, left the company it seems to like that they were overloaded with work.\n* The employee with five-year experience is leaving more because of no promotions in last 5 years and more than 6 years experience are not leaving because of affection with the company.\n* Those who promotion in last 5 years they didn't leave, i.e., all those left they didn't get the promotion in the previous 5 years."}}