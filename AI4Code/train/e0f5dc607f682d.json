{"cell_type":{"5a29479d":"code","21f1b24a":"code","91fc539a":"code","4a408593":"code","81e24c3a":"code","74b85f28":"code","fb6f5862":"code","c5dd01ef":"code","8196bd66":"code","e4321d01":"code","6ca5e959":"code","f2bddf32":"code","4df0e64e":"code","a74d3939":"code","108ed234":"code","f85fd690":"code","6484e670":"code","514138a0":"code","ae98c4fa":"code","78d83d51":"code","390d454a":"code","dbe3058c":"code","1342eeb8":"code","4ac52f5a":"code","9addfd5d":"code","24ee6edc":"code","a76bf197":"code","eaa403eb":"code","59a481c6":"markdown","85d07802":"markdown","bac19b5b":"markdown","80142bea":"markdown","a0e6f143":"markdown","d086546b":"markdown","bbc08fdc":"markdown"},"source":{"5a29479d":"import os\nimport numpy as np\nimport cv2\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nfrom glob import glob\nfrom tqdm.auto import tqdm\nimport imgaug.augmenters as iaa\nimport imgaug as ia\nimport tensorflow as tf\nfrom tensorflow.keras import *\nimport tensorflow.keras.backend as K\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd","21f1b24a":"#!pip install gdown","91fc539a":"#!conda install -y gdown","4a408593":"# download dataset from https:\/\/drive.google.com\/file\/d\/1JILW10sr40CRTLiuA1mf__5GBtMDF9xc\/view?usp=sharing\n#!gdown --id 1JILW10sr40CRTLiuA1mf__5GBtMDF9xc --output vessel_seg.zip","81e24c3a":"# unzip file\n#!unzip -q vessel_seg.zip","74b85f28":"#find the path\n#!find ..\/","fb6f5862":"# read img and mask\nimg_paths = glob('..\/input\/sai-vessel-segmentation2\/all\/train\/*.tif')\nimg_path = np.random.choice(img_paths, size=1)[0]\nmask_path = img_path.replace('_training.tif', '_manual1.gif')\n\nprint('img path: ', img_path)\nprint('mask path:',  mask_path)\n\nimg = cv2.imread(img_path)[:, :, ::-1]\nmask = Image.open(mask_path)\nmask = np.array(mask)","c5dd01ef":"# show image\nplt.figure(figsize=(20, 5))\nplt.subplot(1,2,1)\nplt.imshow(img)\nplt.subplot(1,2,2)\nplt.imshow(mask)\nplt.show()","8196bd66":"img.shape, mask.shape","e4321d01":"IMG_SIZE = 512\nBS = 4","6ca5e959":"class DataGenerator(utils.Sequence):\n    def __init__(self, img_paths, batch_size, img_size, shuffle=True, mode='train', aug=False):\n        self.batch_size = batch_size\n        self.shuffle = shuffle\n        self.img_size = img_size\n        self.mode = mode\n        self.img_paths = img_paths\n        self.mask_paths = [p.replace('_training.tif', '_manual1.gif') for p in self.img_paths]\n        self.aug = aug        \n        self.seq = iaa.Sequential([\n            iaa.Fliplr(0.5), # 50% horizontal flip\n            iaa.Affine(\n                rotate=(-45, 45), # random rotate -45 ~ +45 degree\n                shear=(-16,16), # random shear -16 ~ +16 degree\n                scale={\"x\": (0.8, 1.2), \"y\": (0.8, 1.2)} # scale x, y: 80%~120%\n            ),\n        ])\n        self.indexes = np.arange(len(self.mask_paths))\n        # data augmentation\n        self.on_epoch_end()\n\n    def __len__(self):\n        return int(np.ceil(len(self.mask_paths) \/ self.batch_size)) # batches per epoch\n\n    def __getitem__(self, index):\n        # Generate indexes of the batch\n        idxs = self.indexes[index * self.batch_size:(index + 1) * self.batch_size]\n        # Find list of IDs\n        batch_img_paths = [self.img_paths[i] for i in idxs]\n        batch_mask_paths = [self.mask_paths[i] for i in idxs]\n\n        # Generate data\n        X, y = self.__data_generation(batch_img_paths, batch_mask_paths)\n        if self.mode != 'test':\n            return X, y\n        else:\n            return X\n\n    def on_epoch_end(self):\n        # Updates indexes after each epoch\n        if self.shuffle:\n            np.random.shuffle(self.indexes)\n\n    def __data_generation(self, img_paths, mask_paths):\n        # Generates data containing batch_size samples\n        x = np.empty((len(img_paths), self.img_size, self.img_size, 3), dtype=np.float32)\n        y = np.empty((len(img_paths), self.img_size, self.img_size, 1), dtype=np.float32)\n\n        for i, (img_path, mask_path) in enumerate(zip(img_paths, mask_paths)):\n            img = cv2.imread(img_path)[:, :,::-1]\n            img = self.preprocess(img)\n            x[i] = img\n            if self.mode != 'test':\n                mask = np.array(Image.open(mask_path))\n                mask = self.preprocess(mask)\n                y[i] = np.expand_dims(mask, axis=-1)\n        return x, y\n\n    def preprocess(self, img):\n        data = cv2.resize(img, (self.img_size, self.img_size))\n        data = data \/ 255. # normalize to 0~1\n        return data","f2bddf32":"all_paths = glob('..\/input\/sai-vessel-segmentation2\/all\/train\/*.tif')\ntest_paths = sorted(glob('..\/input\/sai-vessel-segmentation2\/all\/test\/*.tif'))\ntrain_paths, val_paths = train_test_split(all_paths, test_size=0.2)\n\ntrain_gen = DataGenerator(train_paths, BS, IMG_SIZE, shuffle=True,  mode='train', aug=True)\nval_gen   = DataGenerator(val_paths,   BS, IMG_SIZE, shuffle=False, mode='val' )\ntest_gen  = DataGenerator(test_paths,  BS, IMG_SIZE, shuffle=False, mode='test')\n\n\n\n#train_gen1 = DataGenerator(train_paths, BS, IMG_SIZE, shuffle=True,  mode='train', aug=True)\n#train_gen2 = DataGenerator(train_paths, BS, IMG_SIZE, shuffle=True,  mode='train', aug=True)\n#val_gen1   = DataGenerator(val_paths,   BS, IMG_SIZE, shuffle=False, mode='val' )\n#val_gen2   = DataGenerator(val_paths,   BS, IMG_SIZE, shuffle=False, mode='val' )\n#test_gen1  = DataGenerator(test_paths,  BS, IMG_SIZE, shuffle=False, mode='test')\n#test_gen2  = DataGenerator(test_paths,  BS, IMG_SIZE, shuffle=False, mode='test')\n#train_gen  = pd.concat([train_gen1, train_gen2])\n#val_gen    = pd.concat([val_gen1  , val_gen2])\n#test_gen   = pd.concat([test_gen1 , test_gen2])","4df0e64e":"# Check output\n\nbatch_x, batch_y = train_gen[0]\n\nbatch_i = np.random.choice(len(batch_x))\nplt.figure(figsize=(20, 5))\nplt.subplot(1,2,1)\nplt.imshow(batch_x[batch_i], cmap='gray')\nplt.subplot(1,2,2)\nplt.imshow(batch_y[batch_i, :, :, 0])\nplt.show()","a74d3939":"input_scalar_w1 = Input(shape = (256, 256,1))\ninput_scalar_w2 = Input(shape = (128, 128,1))\ninput_scalar_w3 = Input(shape = (64, 64,1))\nw1=tf.eye(256)*1\nw2=tf.eye(128)*1\nw3=tf.eye(64)*1\nw  =2.\nw_1=2.2\nw_2=2.\nw_3=1.8","108ed234":"# TODO\ninput_layer = layers.Input(shape=(IMG_SIZE, IMG_SIZE, 3))\nc1 = layers.Conv2D(filters=8, kernel_size=(3,3), activation='relu', padding='same')(input_layer)\nl = layers.MaxPool2D(strides=(2,2))(c1)\nc2 = layers.Conv2D(filters=16, kernel_size=(3,3), activation='relu', padding='same')(l)\nl = layers.MaxPool2D(strides=(2,2))(c2)\nc3 = layers.Conv2D(filters=32, kernel_size=(3,3), activation='relu', padding='same')(l)\nl = layers.MaxPool2D(strides=(2,2))(c3)\nc4 = layers.Conv2D(filters=32, kernel_size=(3,3), activation='relu', padding='same')(l)\n\nl = layers.concatenate([layers.UpSampling2D(size=(2,2))(c4),c3*w_3], axis=-1)\nl = layers.Conv2D(filters=32, kernel_size=(2,2), activation='relu', padding='same')(l)\nl = layers.concatenate([layers.UpSampling2D(size=(2,2))(l), c2*w_2], axis=-1)\nl = layers.Conv2D(filters=24, kernel_size=(2,2), activation='relu', padding='same')(l)\nl = layers.concatenate([layers.UpSampling2D(size=(2,2))(l), c1*w_1], axis=-1)\nl = layers.Conv2D(filters=16, kernel_size=(2,2), activation='relu', padding='same')(l)\nl = layers.Conv2D(filters=64, kernel_size=(1,1), activation='relu')(l)\noutput_layer = layers.Conv2D(filters=1, kernel_size=(1,1), activation='sigmoid')(l)\n                                                         \nmodel = models.Model(input_layer, output_layer)","f85fd690":"utils.plot_model(model, show_shapes=True)","6484e670":"# Customize Dice coefficient\ndef dice_coef(y_true, y_pred):\n    y_true_f = K.flatten(y_true)\n    y_pred_f = K.flatten(y_pred)\n    intersection = K.sum(y_true_f * y_pred_f)\n    return (2. * intersection + K.epsilon()) \/ (K.sum(y_true_f) + K.sum(y_pred_f) + K.epsilon())","514138a0":"model.compile(optimizer=optimizers.Adam(), loss='binary_crossentropy', metrics=[dice_coef])","ae98c4fa":"weight_saver = callbacks.ModelCheckpoint('seg.h5', monitor='val_loss', save_best_only=True)","78d83d51":"logs = model.fit(train_gen,\n                 validation_data = val_gen,\n                 epochs=5000,\n                 callbacks = [weight_saver])","390d454a":"history = logs.history\nplt.plot(history['loss'])\nplt.plot(history['val_loss'])\nplt.title('loss')\nplt.show()\nplt.plot(history['dice_coef'])\nplt.plot(history['val_dice_coef'])\nplt.title('Dice')\nplt.show()","dbe3058c":"history = logs.history\nplt.plot(history['loss'])\nplt.plot(history['val_loss'])\nplt.title('loss')\nplt.show()\nplt.plot(history['dice_coef'])\nplt.plot(history['val_dice_coef'])\nplt.title('Dice')\nplt.show()","1342eeb8":"model_final = models.load_model('seg.h5', compile=False)","4ac52f5a":"# Sample 1 batch\nbatch_idx = np.random.randint(len(val_gen))\nprint(batch_idx)\ndata = val_gen[batch_idx]\nimgs, mask = data # (bs, 512, 512, 3), (bs, 512, 512, 1)\nmask_pred = model_final.predict(imgs)\n\n# show inputs\nimg_idx = np.random.randint(len(imgs)) # sample 1 image from batch\nplt.figure(figsize=(20, 5))\nplt.subplot(1,3,1)\nplt.imshow(imgs[img_idx])\n\n# show ground truth & model prediction\nplt.subplot(1, 3, 2)\nplt.imshow(mask[img_idx, :, :, 0])\nplt.subplot(1, 3, 3)\nplt.imshow(mask_pred[img_idx, :, :, 0])\nplt.show()\n# plt.imshow(mask_pred[img_idx, :, :, 0], cmap='gray')","9addfd5d":"# Displayed with different threshold\nmask_pred_raw = mask_pred[img_idx, :, :, 0]\nmask_pred_raw","24ee6edc":"plt.imshow(mask[img_idx, :, :, 0])\nplt.title('Ground Truth')\nplt.show()\n\nplt.figure(figsize=(20, 10))\nfor i in range(1, 10):\n    plt.subplot(2, 5, i)\n    threshold = i * 0.1\n    mask_threshold = mask_pred_raw.copy()\n    mask_threshold[mask_threshold <= threshold] = 0.\n    mask_threshold[mask_threshold > threshold] = 1.\n    plt.imshow(mask_threshold)\n    plt.title(f'threshold: {threshold:.1f}')\nplt.show()","a76bf197":"# make prediction\nthreshold = 0.5\noutputs = []\nfor i in range(len(test_gen)):\n    x_test = test_gen[i]\n    y_preds = model_final.predict(x_test)\n    y_preds[y_preds <= threshold] = 0\n    y_preds[y_preds > threshold] = 1\n    for y_pred in y_preds:\n        dots = np.where(y_pred.flatten() == 1)[0]\n        run_lengths = []\n        prev = -2\n        for b in dots:\n            if (b > prev +1):\n                run_lengths.extend((b+1,0))\n            run_lengths[-1] += 1\n            prev = b\n        output = ' '.join([str(r) for r in run_lengths])\n        outputs.append(output)","eaa403eb":"df = pd.DataFrame(columns=['Id', 'Predicted'])\ndf['Id'] = [str(i) for i in range(20)]\ndf['Predicted'] = outputs\ndf.to_csv('submission.csv', index=None)\ndf","59a481c6":"w=1 no rotation","85d07802":"#### Data Analysis","bac19b5b":"#### Evaluate","80142bea":"#### Training","a0e6f143":"#### Download data","d086546b":"#### Build model","bbc08fdc":"#### Data processing"}}