{"cell_type":{"e2962072":"code","c9c2fad8":"code","941a25f6":"markdown","522c6dc8":"markdown","c759fd3c":"markdown","68fedab8":"markdown","fce053c9":"markdown","bb6c0593":"markdown"},"source":{"e2962072":"from collections import deque\n\nimport numpy as np\n\n\nMAP_SIZE = 21\nCONVERT_COST = 500\nSPAWN_COST = 500\n\n\nclass Helper:\n    \"\"\"This class is a template for agent helpers.\n    \"\"\"\n\n    def __init__(self):\n        self.state_shape = None\n        self.action_shape = None\n        self.memory = {}\n\n    def convert_obs(self, uid, obs):\n        return np.array([0])\n\n    def convert_action(self, uid, obs, action):\n        return 'NONE'\n\n    def reset(self):\n        pass\n\n    def step(self):\n        pass\n\n\nclass BasicShipyardHelper(Helper):\n    \"\"\"This class provides basic provisions for helping a shipyard agent.\n    \"\"\"\n\n    def __init__(self):\n        # steps, phalite, occupied, num ships\n        self.state_shape = (4,)\n        # None, spawn\n        self.action_shape = (2,)\n\n        self.reset()\n\n    def convert_obs(self, uid, obs):\n        phalite, shipyards, ships = obs.players[obs.player]\n        occupied = shipyards[uid] in [x[0] for x in ships.values()]\n        occupied = 1 if occupied else 0\n        return np.array([obs.step, phalite, occupied, len(ships)])\n\n    def convert_action(self, uid, obs, action):\n        phalite, shipyards, ships = obs.players[obs.player]\n        occupied = shipyards[uid] in [x[0] for x in ships.values()]\n        if action == 1 and phalite >= SPAWN_COST and not occupied:\n            return 'SPAWN'\n        return 'NONE'\n\n\nclass BasicShipHelper(Helper):\n    \"\"\"This class provides basic provisions for helping a ship agent.\n    \"\"\"\n\n    def __init__(self, stack=2, radius=5):\n        self.stack = stack\n        self.radius = radius\n        self.frame_state_shape = (self.radius*2+1, self.radius*2+1, 2)\n        self.state_shape = ((self.radius*2+1)**2 * 2 * self.stack,)\n        self.action_shape = (6,)\n        self.reset()\n\n    def convert_obs(self, uid, obs):\n        phalite, shipyards, ships = obs.players[obs.player]\n        if uid not in self.state:\n            self.state[uid] = deque(maxlen=self.stack)\n            for _ in range(self.stack-1):\n                self.state[uid].append(np.zeros(self.frame_state_shape))\n        if uid in ships:\n            entities_map = np.zeros((MAP_SIZE, MAP_SIZE))\n            for player, (_, sy, s) in enumerate(obs.players):\n                for shipyard in sy.values():\n                    if obs.player == player:\n                        entities_map[shipyard \/\/ MAP_SIZE,\n                                     shipyard % MAP_SIZE] += 2\n                    else:\n                        entities_map[shipyard \/\/ MAP_SIZE,\n                                     shipyard % MAP_SIZE] -= 2\n                for ship in s.values():\n                    if obs.player == player:\n                        entities_map[ship[0] \/\/ MAP_SIZE,\n                                     ship[0] % MAP_SIZE] += 1\n                    else:\n                        entities_map[ship[0] \/\/ MAP_SIZE,\n                                     ship[0] % MAP_SIZE] -= 1\n            halite_map = np.reshape(obs.halite, (MAP_SIZE, MAP_SIZE))\n        \n            state_map = np.stack([halite_map, entities_map], axis=2)\n            state_map = np.tile(state_map, (3, 3, 1))\n\n            spos, shalite = ships[uid]\n            y = spos \/\/ MAP_SIZE + MAP_SIZE\n            x = spos % MAP_SIZE + MAP_SIZE\n            r = self.radius\n            self.state[uid].append(state_map[y-r:y+r+1, x-r:x+r+1])\n            return np.dstack(self.state[uid]).flatten()\n        self.state[uid].append(np.zeros(self.frame_state_shape))        \n        return np.dstack(self.state[uid]).flatten()\n\n    def convert_action(self, uid, obs, action):\n        phalite, shipyards, ships = obs.players[obs.player]\n        pos, shalite = ships[uid]\n        sy, sx = pos \/\/ MAP_SIZE, pos % MAP_SIZE\n\n        if action == 0:\n            return 'NONE'\n        if action == 1:\n            if phalite > CONVERT_COST and pos not in shipyards.values():\n                return 'CONVERT'\n            return 'NONE'\n        if action == 2:\n            return 'NORTH'\n        if action == 3:\n            return 'SOUTH'\n        if action == 4:\n            return 'WEST'\n        if action == 5:\n            return 'EAST'\n\n    def reset(self):\n        self.state = {}","c9c2fad8":"# Code below is incomplete\nimport os\nfrom copy import deepcopy\n\nfrom kaggle_environments import make\n\n\nclass HaliteEnv:\n    def __init__(self, opponents, shipyard_helper, ship_helper,\n                 replay_save_dir='', **kwargs):\n        self.shipyard_helper = shipyard_helper\n        self.ship_helper = ship_helper\n        self.env = make('halite', **kwargs)\n        self.trainer = self.env.train([None, *opponents])\n        self.replay_save_dir = replay_save_dir\n\n    def update_obs(self, uid, action):\n        \"\"\"Simulate environment step forward and update observation\"\"\"\n        pass\n    \n    def reset(self):\n        \"\"\"Reset trainner environment\"\"\"\n        self.obs = deepcopy(self.trainer.reset())\n        return self.obs\n    \n    def step(self, actions):\n        \"\"\"Step forward in actual environment\"\"\"\n        self.obs, reward, terminal, info = self.trainer.step(actions)\n        self.obs = deepcopy(self.obs)\n        return self.obs, reward, terminal\n    \n    def play_episdoe(self, shipyard_agent, ship_agent, max_steps=400):\n        \"\"\"Play one episode\"\"\"\n        self.shipyard_helper.reset()\n        self.ship_helper.reset()\n        obs = self.reset()\n        states = {}\n        total_reward = 0\n        for step in range(1, max_steps + 1):\n            actions = {}\n            phalite, shipyards, ships = obs.players[obs.player]\n            init_shipyards = deepcopy(shipyards)\n            init_ships = deepcopy(ships)\n            \n            for uid in init_shipyards.keys():\n                # implementations vary\n                # convert observation\n                # select action\n                # convert action\n                # update simulated environment\n                # Add experience\/memory\n                pass\n            \n            for uid in init_ships.keys():\n                # implementations vary\n                # convert observation\n                # select action\n                # convert action\n                # update simulated environment\n                # Add experience\/memory\n                pass\n                \n            obs, ep_reward, ep_terminal = self.step(actions)\n            self.shipyard_helper.step()\n            self.ship_helper.step()\n            \n            if ep_terminal:\n                break\n\n        # For sparse rewards we revise all ships and\n        # shipyards last memory to be terminal and have the same reward\n        # Important to note that training ships and shipyards at the same\n        # time may produce poor results as they will get a reward that may\n        # or may not represent that agents effectiveness.\n        phalite, shipyards, ships = obs.players[obs.player]\n        if obs.step < 399:\n            if len(shipyards) == len(ships) == 0:\n                total_reward = -1.0\n            else:\n                total_reward = 1.0\n        else:\n            halites = [halite for (halite, _, _) in obs.players]\n            max_halite = np.max(halites)\n            if max_halite == 0:\n                max_halite = 1\n            del halites[obs.player]\n            total_reward = (phalite - np.max(halites)) \/ max_halite\n        \n        # Revise the last memory of each ship\/shipyard\n        # reward to be total_reward\n        pass\n    \n        with open(os.path.join(self.replay_save_dir, 'out.html'), 'w') as file:\n            file.write(self.env.render(mode='html'))\n        \n        ","941a25f6":"# Conclusion\nHopefully, this notebook has answered the questions from [here](https:\/\/www.kaggle.com\/c\/halite\/discussion\/148923). The design and code were specifically made to work with [PAI-Utils](https:\/\/pypi.org\/project\/paiutils\/), so implementation with other RL packages may be different.","522c6dc8":"# Agent Helper\nAn agent helper does two things. First, it converts observations into specific states. Second, it converts actions into raw actions, which allows for the potential to check invalid actions before we do them. For ships we have an agent helper and for shipyards we have one also.\n\nAn example of the type of state a ship would need is a map of halite, a map of ships, and a map of shipyards all with the ship in the center. This becomes a large state with a size of 675 for a map of 15 units, so reducing the map to just a reasonable sized radius around the ship can be efficent, such as 5 making the state size 363.\nAn example of the type of state a shipyard would need is a list with the player halite, number of ships, current step, and if a ship is on the shipyard.","c759fd3c":"The purpose of this notebook is to answer some questions about how to approach making an RL Agent for Halite.\nAn important note: great results have not been reached with this design, but this is mainly due to short training times, untuned hyperparameters, and a sparse reward system.\nThe design that we will be looking at contains three parts: an agent, an agent helper, and an environment. \n![design.png](attachment:design.png)\n*Simulated Environment does not necessarily provide the reward","68fedab8":"# Agent\nOne of the first problems encountered when designing a way to attack Halite is how to handle multiple ships (and shipyards) that can be created (also destroyed) with no concrete population limit. One solution is to make a limit, say 20 ships, and 5 shipyards, meaning we will have 25 agents that depending on the availability of ships and shipyards, will be used. However, this solution can be difficult to train and inefficient. The best solution is to create two agents one for ships and the other for shipyards, and for each ship and shipyard, we give a specific state (ex. a 5x5 matrix of halite values with the ship being in the center) to the ship and shipyard agents to predict actions for. This solution is much more efficient but can be somewhat difficult to implement for an RL Agent depending on the algorithm (DQN vs. A2C). The reason this is difficult is experience\/memory for some algorithms (PG, A2C, etc.) must be temporally organized to calculate the discounted rewards, but we have multiple ships contributing to the same memory buffer. This difficulty is overcome by organizing experiences to be separately contained through a dictionary with UIDs for keys, and then upon a ship's destruction or the game-ending, they are orderly appended to the memory buffer.\n\nThe next problem we have just created with the above solution is that we need to make observations into specific states for each ship and shipyard. To handle this, we introduced agent helpers.","fce053c9":"# Potential Improvements\n**Reward System** The sparse reward system suggested in this design is not optimal. The same reward is given to each ship, and although the ships all have the same \"brain,\" there is usually some stochasticity in the decision (for exploration). Thus, to some degree, the rewards are poorly correlated with an individual ship's performance. Therefore, one proposed solution is to add another reward on top of the winning\/losing reward, which determines if the ship was proactive in helping to achieve the goal of winning. Proactiveness can be defined in different ways. The simplest is if the ship contributed to the player's halite, it is proactive. However, this will most likely lead to a greedy strategy, so taking into count the destruction of enemy entities may also be important in determining proactiveness. The idea of rewarding proactiveness can also be applied to shipyards, but of course, it needs to be defined differently than ship proactiveness.\n\n","bb6c0593":"# Environment\nThe goal of making an environment class is to wrap the halite kaggle environment in order to make it more friendly to RL agents. An RL agent needs to be able to get an environment state, manipulate an environment, get rewards, and check if the environment is complete. So, to fulfill what the agent needs, we make a step method that takes all the ship and shipyard actions and returns a new state, a reward, and a terminal. However, we have run into some problems. The first problem is that we need all the actions before we get a new state. The second problem is that the reward given by the halite kaggle environment is not the best and can be None in some circumstances. The third problem, specifically for ships, is that ships can be destroyed, which means the terminal for a ship differs from the terminal given by the halite kaggle environment.\n\n**Solving the first problem**\nIn order to remove the need for all ships and shipyard actions to act before we get a result, we must implement a simulation of the halite kaggle environment. This simulation will take a ship or shipyard with an action and manipulate the observation as if only that action took place (the simulation does not need to be perfect; ex. not checking ship to ship collisions).\n\n**Solving the second problem**\nThere are a couple ways to solve the problem of having a poor reward function. One could reward each ship based on its efficiency in gathering halite, but the solution we have focused on is the use of a sparse reward function where the agent in an episode is given a reward between -1 to 1 based on how much halite the player has compared to the best opponent. A reward for completing games early can also be given, where if the agent has won sooner than the max_step it gets a reward of 1, and if it has loss sooner a reward of -1.\n\n**Solving the third problem**\nThis problem is solved by checking if a ship has been destroyed and if so adjusting the last terminal to True. It is important to avoid using the simulatated environment to check the destrustion as it is not guaranted, so if able use the actual halite kaggle environment observation to make the check."}}