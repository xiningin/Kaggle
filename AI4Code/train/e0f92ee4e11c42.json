{"cell_type":{"4e351f02":"code","3d722de7":"code","7293573c":"code","aeb25b73":"code","3d67d181":"code","5c003c6d":"code","e2d0392a":"code","2fff2877":"code","f2fb1f40":"code","16ac8b91":"code","04400467":"code","33aca059":"code","d0281af8":"code","3bbf67f7":"code","79eb492b":"code","d6c95ff3":"code","c8ce990a":"code","4438e0fe":"code","4cddeaa9":"code","1da13cae":"code","f228f67b":"code","75db96dc":"code","9c4ffd04":"code","15c2f6fe":"code","4a94cee9":"code","faea9df5":"code","57b0a071":"code","805ca471":"code","bbe34a4f":"code","bc86b3d7":"code","706e9d6a":"code","ad7d48eb":"code","c22ac8a7":"code","c1bc948c":"code","2aa12631":"code","22df96ea":"code","184b0034":"code","c10c469c":"code","a0bad221":"code","f926b249":"code","bef6f931":"code","cb9ce0cd":"code","7832b814":"code","4cc45537":"code","94f79f24":"code","5b2fcc93":"code","45886a9f":"code","68077949":"code","f8884d76":"code","1ba08fa8":"code","c7efec98":"code","17b1e27f":"code","69f95ad7":"code","06df3897":"code","c03c14a8":"code","de85c3df":"code","1834c590":"code","a7115f6f":"code","58452c39":"code","b7215d4b":"code","96938cab":"code","69f6c9d5":"code","e75e1225":"code","2a64cadc":"code","ad336cec":"code","eac79001":"code","0fe48f11":"code","c5751698":"code","edbf800e":"code","45eb6092":"code","15dd0db3":"code","07711e90":"code","38db0eaa":"code","8de78cad":"code","1eed21ed":"code","d34994bf":"code","ca8a1de1":"code","0a33e818":"code","45d5b241":"code","5c7afb5b":"code","e6f8304b":"code","ba81ff91":"code","32a05210":"markdown","9be2f665":"markdown","51b62b31":"markdown","f9666b83":"markdown","80fa21ad":"markdown","9c2d9d04":"markdown","1a9ac90a":"markdown","f2dc015a":"markdown","671ddcf3":"markdown","a90aad4f":"markdown","317f2f76":"markdown","963de34f":"markdown","7c37783b":"markdown","0b142460":"markdown","3baa1aa5":"markdown","9751d008":"markdown","e5fd4edf":"markdown","04a05553":"markdown","f54f3b4a":"markdown","bbe32248":"markdown","f981162b":"markdown","21ebd5f5":"markdown","8cd60551":"markdown","5fb7b5b4":"markdown","327652ba":"markdown","c274486c":"markdown","6171b994":"markdown","f457f1a8":"markdown","bf4f60ec":"markdown","92890d15":"markdown","c654ba74":"markdown","38a5de36":"markdown","35724fcc":"markdown","f4f59e6f":"markdown","ced1d97f":"markdown","78eeb038":"markdown","55cd170e":"markdown","e7810af7":"markdown","81dc9e19":"markdown","1d41a601":"markdown","4aa27741":"markdown","008c8af2":"markdown","8695f600":"markdown"},"source":{"4e351f02":"# Libaries import\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom copy import copy","3d722de7":"data = pd.read_csv(\"\/kaggle\/input\/uci-online-news-popularity-data-set\/OnlineNewsPopularity.csv\")\ndata.head(n=4)\norigianl_data = copy(data)\ndata.columns\n\n","7293573c":"# Here we drop the two non-preditive (url and timedelta) attributes. They won't contribute anything\ndata.drop(labels=['url', ' timedelta'], axis = 1, inplace=True)\ndata.head(n=4)","aeb25b73":"# describing the data\ndata.describe()\n# from the data, there will be need to normailze the data if their will be need for condersing any machine learning model.","3d67d181":"# creating a grading criteria for the shares\nshare_data = data[' shares']\ndata[' shares'].describe()","5c003c6d":"# create label grades for the classes\nshare_label = list()\nfor share in share_data:\n    if share <= 645:\n        share_label.append('Very Poor')\n    elif share > 645 and share <= 861:\n        share_label.append('Poor')\n    elif share > 861 and share <= 1400:\n        share_label.append('Average')\n    elif share > 1400 and share <= 31300:\n        share_label.append('Good')\n    elif share > 31300 and share <= 53700:\n        share_label.append('Very Good')\n    elif share > 53700 and share <= 77200:\n        share_label.append('Excellent')\n    else:\n        share_label.append('Exceptional')\n\n# Update this class label into the dataframe\ndata = pd.concat([data, pd.DataFrame(share_label, columns=['popularity'])], axis=1)\ndata.head(4)\n","e2d0392a":"# Merging the weekdays columns channels as one single column\npublishdayMerge=data[[' weekday_is_monday',' weekday_is_tuesday',' weekday_is_wednesday', \n                      ' weekday_is_thursday', ' weekday_is_friday',' weekday_is_saturday' ,' weekday_is_sunday' ]]\ntemp_arr=[]\nfor r in list(range(publishdayMerge.shape[0])):\n    for c in list(range(publishdayMerge.shape[1])):\n        if ((c==0) and (publishdayMerge.iloc[r,c])==1):\n            temp_arr.append('Monday')\n        elif ((c==1) and (publishdayMerge.iloc[r,c])==1):\n            temp_arr.append('Tueday')\n        elif ((c==2) and (publishdayMerge.iloc[r,c])==1):\n            temp_arr.append('Wednesday')\n        elif ((c==3) and (publishdayMerge.iloc[r,c])==1):\n            temp_arr.append('Thursday')\n        elif ((c==4) and (publishdayMerge.iloc[r,c])==1):\n            temp_arr.append('Friday')\n        elif ((c==5) and (publishdayMerge.iloc[r,c])==1):\n            temp_arr.append('Saturday') \n        elif ((c==6) and (publishdayMerge.iloc[r,c])==1):\n            temp_arr.append('Sunday')\n            \n# Merging the data channels as one single column\nDataChannelMerge=data[[' data_channel_is_lifestyle',' data_channel_is_entertainment' ,' data_channel_is_bus',\n                        ' data_channel_is_socmed' ,' data_channel_is_tech',' data_channel_is_world' ]]\n#logic to merge data channel\nDataChannel_arr=[]\nfor r in list(range(DataChannelMerge.shape[0])):\n    if (((DataChannelMerge.iloc[r,0])==0) and ((DataChannelMerge.iloc[r,1])==0) and ((DataChannelMerge.iloc[r,2])==0) and ((DataChannelMerge.iloc[r,3])==0) and ((DataChannelMerge.iloc[r,4])==0) and ((DataChannelMerge.iloc[r,5])==0)):\n        DataChannel_arr.append('Others')\n    for c in list(range(DataChannelMerge.shape[1])):\n        if ((c==0) and (DataChannelMerge.iloc[r,c])==1):\n            DataChannel_arr.append('Lifestyle')\n        elif ((c==1) and (DataChannelMerge.iloc[r,c])==1):\n            DataChannel_arr.append('Entertainment')\n        elif ((c==2) and (DataChannelMerge.iloc[r,c])==1):\n            DataChannel_arr.append('Business')\n        elif ((c==3) and (DataChannelMerge.iloc[r,c])==1):\n            DataChannel_arr.append('Social Media')\n        elif ((c==4) and (DataChannelMerge.iloc[r,c])==1):\n            DataChannel_arr.append('Tech')\n        elif ((c==5) and (DataChannelMerge.iloc[r,c])==1):\n            DataChannel_arr.append('World')","2fff2877":"# merge the the new data into the dataframe\ndata.insert(loc=11, column='weekdays', value=temp_arr)\ndata.insert(loc=12, column='data_channel', value=DataChannel_arr)\n\n# Now I drop the old data\ndata.drop(labels=[' data_channel_is_lifestyle',' data_channel_is_entertainment' ,' data_channel_is_bus',\n                        ' data_channel_is_socmed' ,' data_channel_is_tech',' data_channel_is_world', \n                 ' weekday_is_monday',' weekday_is_tuesday',' weekday_is_wednesday', \n                      ' weekday_is_thursday', ' weekday_is_friday',' weekday_is_saturday' ,' weekday_is_sunday'], axis = 1, inplace=True)\nprint(data.shape)\ndata.head(n=4)","f2fb1f40":"data.columns","16ac8b91":"# Evaluating features (sensors) contribution towards the label\nfig = plt.figure(figsize=(15,5))\nax = sns.countplot(x='popularity',data=data,alpha=0.5)\n","04400467":"# Fetch the counts for each class\nclass_counts = data.groupby('popularity').size().reset_index()\nclass_counts.columns = ['Popularity','No of articles']\nclass_counts\n","33aca059":"# Visualizaing the \"low\" expectation hypothesis\n# n_non_stop_words\nprint(data[' n_non_stop_words'].describe())\n# Comment - Visualizing the n_non_stop_words data field shows that the present of a record with 1042 value, \n# futher observation of that data shows that it belongs to entertainment which is not actually. It belongs to world news or others.\n# this particluar also contains 0 on a lot of attributes. This record is classifed as a noise and will be remove.\ndata = data[data[' n_non_stop_words'] != 1042]\n# Here, we will go ahead and drop the field of ' n_non_stop_words'\ndata.drop(labels=[' n_non_stop_words'], axis = 1, inplace=True)\n","d0281af8":"# remove noise from n_tokens_content. those equals to 0\ndata  = data[data[' n_tokens_content'] != 0]\nprint (\"After noise removal - \",data.shape)","3bbf67f7":"# n_non_stop_unique_tokens\ndata[' n_non_stop_unique_tokens'].describe()\n# a lot of unique words, it is better to use a different plot from bar plots\n# line plot\ntemp_data = data[data[' shares'] <= 100000]\nfig, axes = plt.subplots(figsize=(10,10))\n# box plot\nsns.boxplot(x='popularity', y=' n_non_stop_unique_tokens', data=data, ax=axes)\n# box plot of the dataset shows majority (75%) of the data inrespective of their shares is in the range of 0.6 - 0.8.\n# So does it offers any uniques? No, it doesn't.","79eb492b":"#kw_min_min and related kw_ terms\ndata[' kw_min_min'].describe()\ntemp_data = data[data[' shares'] <= 100000]\n# running a pair plot for the kw__terms\nkw_cols = [' kw_min_min', ' kw_max_min', ' kw_avg_min', ' kw_min_max', ' kw_max_max', ' kw_avg_max', ' kw_min_avg', \n            ' kw_max_avg', ' kw_avg_avg', ' shares']\n# run a pairplot\n#sns.pairplot(temp_data, vars=kw_cols, hue='popularity', diag_kind='kde')\n","d6c95ff3":"#Finding relationship between 'rate_positive_words', 'rate_negative_words', 'global_rate_positive_words', 'global_rate_negative_words', and 'shares'\ntemp_data = data[data[' shares'] <= 100000]\n# running a pair plot for the these terms\nkw_cols = [' rate_positive_words', ' rate_negative_words', ' global_rate_positive_words', ' global_rate_negative_words', ' shares']\n# run a pairplot\nsns.pairplot(temp_data, vars=kw_cols, hue='popularity', diag_kind='kde')\n\n'''\nThere is a linear relationship between rate_positive_words and rate_negative_words (it is expected)\nrate_positive_words = No special relationship or observable trait was observed for this variable. Although most of articles tends \nto be on falls towards the 0.3 - 1\nrate_negative_words = No special relationship or observable trait was observed for this variable. Although most of articles tends \nto be on falls towards the 0.8 - 0 = Note the articles with popularity less than \"average\" have the lowest negative score rate.\nglobal_rate_positive_words - There is a slight relationship with shares. - Medium\nglobal_rate_negative_words - There is a slight relationship with shares. - Medium\n\n'''","c8ce990a":"# attempt polartiy\ntemp_data = data[data[' shares'] <= 100000]\nsns.lmplot(x=' avg_positive_polarity', y=' shares', col='popularity', data=temp_data)","4438e0fe":"# attempt polartiy\ntemp_data = data[data[' shares'] <= 100000]\nfig, axes = plt.subplots(figsize=(10,10))\nsns.scatterplot(x=' avg_positive_polarity', y=' shares', hue='popularity', data=temp_data, ax=axes)","4cddeaa9":"#Finding relationship between 'rate_positive_words', 'rate_negative_words', 'global_rate_positive_words', 'global_rate_negative_words', and 'shares'\ntemp_data = data[data[' shares'] <= 100000]\n# running a pair plot for the terms\nkw_cols = [' avg_positive_polarity', ' min_positive_polarity', ' max_positive_polarity', ' avg_negative_polarity', ' min_negative_polarity', ' max_negative_polarity', ' shares']\n# run a pairplot\nsns.pairplot(temp_data, vars=kw_cols, hue='popularity', diag_kind='kde')\n\n'''\navg_positive_polarity and avg_negative_polarity are good features with some clear observation \n'''","1da13cae":"# attempt title_subjectivity\ntemp_data = data[data[' shares'] <= 100000]\nfig, axes = plt.subplots(figsize=(15,15))\nsns.scatterplot(x=' title_sentiment_polarity', y=' shares', hue='popularity', data=temp_data, ax=axes)","f228f67b":"# attempt title_subjectivity\ntemp_data = data[data[' shares'] <= 100000]\nfig, axes = plt.subplots(figsize=(15,15))\nsns.relplot(x=' title_subjectivity', y=' shares', hue='popularity', data=temp_data, ax=axes)","75db96dc":"temp_data = data[data[' shares'] <= 100000]\n# running a pair plot for the kw__terms\nkw_cols = [' title_sentiment_polarity', ' abs_title_sentiment_polarity', ' title_subjectivity', ' abs_title_subjectivity', ' shares']\n# run a pairplot\nsns.pairplot(temp_data, vars=kw_cols, hue='popularity', diag_kind='kde')","9c4ffd04":"# attempt self_reference_min_shares\ntemp_data = data[(data[' shares'] <= 100000) & (data[' self_reference_min_shares'] <= 30000)]\nfig, axes = plt.subplots(figsize=(15,15))\nsns.scatterplot(x=' self_reference_min_shares', y=' shares', hue= 'popularity', data=temp_data, ax=axes)","15c2f6fe":"temp_data = data[data[' shares'] <= 100000]\n# running a pair plot for the kw__terms\nkw_cols = [' self_reference_min_shares', ' self_reference_max_shares', ' self_reference_avg_sharess', ' shares']\n# run a pairplot\nsns.pairplot(temp_data, vars=kw_cols, hue='popularity', diag_kind='kde')","4a94cee9":"#### LDA - 0: 5\ntemp_data = data[data[' shares'] <= 100000]\n# running a pair plot for the kw__terms\nkw_cols = [' LDA_00', ' LDA_01', ' LDA_02', ' LDA_03', ' LDA_04', ' shares']\n# run a pairplot\nsns.pairplot(temp_data, vars=kw_cols, hue='popularity', diag_kind='kde')","faea9df5":"# extact the weekdays articles distrubution\nweekdays_data = data.groupby('weekdays').size().reset_index()\nweekdays_data.columns = ['weekdays','count']\nweekdays_data","57b0a071":"# shows the days when articles are usually posted\nfig, axes = plt.subplots(figsize=(10,10))\nax = sns.countplot(x='weekdays',data=data,alpha=0.5, ax=axes)","805ca471":"# shows relationship with the number of shares and the weekdays\ntemp_data = data[(data['popularity'] == 'Very Poor') | (data['popularity'] == 'Poor') | (data['popularity'] == 'Average') | (data['popularity'] == 'Good')]\nax = sns.catplot(x='weekdays', col=\"popularity\", data=temp_data, kind=\"count\", height=10, aspect=.7)","bbe34a4f":"# shows relationship with the number of shares and the weekdays (compare only the best three popularity)\ntemp_data = data[(data['popularity'] == 'Exceptional') | (data['popularity'] == 'Excellent') | (data['popularity'] == 'Very Good')]\nax = sns.catplot(x='weekdays', col=\"popularity\", data=temp_data, kind=\"count\", height=20, aspect=.7)\n\n'''\n'''","bc86b3d7":"temp_data = data[data[' shares'] <= 100000]\n# running a pair plot for the kw__terms\nkw_cols = [' average_token_length', ' num_keywords', ' global_subjectivity', ' global_sentiment_polarity', ' shares']\n# run a pairplot\nsns.pairplot(temp_data, vars=kw_cols, hue='popularity', diag_kind='kde')","706e9d6a":"## Seeing the distribution of the articles across the data channels\n# extact the weekdays articles distrubution\ndata_channel_data = data.groupby('data_channel').size().reset_index()\ndata_channel_data.columns = ['Data Channels','No of articles']\ndata_channel_data\n","ad7d48eb":"# Shows the distribution of the articles across the channels\nsns.catplot(x='data_channel', data=data, kind=\"count\", height=10, aspect=.7)","c22ac8a7":"#the ranking of the channels in regards to the shares popularity\ntemp_data = data[(data['popularity'] == 'Very Poor') | (data['popularity'] == 'Poor') | (data['popularity'] == 'Average') | (data['popularity'] == 'Good')]\nax = sns.catplot(x='data_channel', col=\"popularity\", data=temp_data, kind=\"count\", height=8, aspect=.7)","c1bc948c":"# shows relationship with the number of shares and the ranking of the channels (compare only the best three popularity)\ntemp_data = data[(data['popularity'] == 'Exceptional') | (data['popularity'] == 'Excellent') | (data['popularity'] == 'Very Good')]\nax = sns.catplot(x='data_channel', col=\"popularity\", data=temp_data, kind=\"count\", height=10, aspect=.7)\n","2aa12631":"fig,ax = plt.subplots(figsize=(10,10))\ntemp_data = data[data[' num_imgs'] <= 25]\nsns.boxplot(x='popularity',y=' num_imgs', hue='data_channel', data=temp_data, showfliers=False)","22df96ea":"#n_tokens_content\nsns.scatterplot(x=' n_tokens_content',y='popularity', data=data)","184b0034":"#n_tokens_title\ntemp_data = data[data[' shares'] <= 200000]\nsns.scatterplot(x=' n_tokens_title',y=' shares', hue='popularity', data=temp_data)","c10c469c":"#n_tokens_title\ntemp_data = data[data[' shares'] <= 200000]\nplt.figure(figsize=(10,10))\nsns.scatterplot(x=' n_unique_tokens',y=' shares', hue='popularity', data=data)","a0bad221":"#num_hrefs\ntemp_data = data[data[' shares'] <= 100000]\nsns.scatterplot(x=' num_hrefs',y=' shares', hue='popularity', data=temp_data)","f926b249":"#num_imgs\ntemp_data = data[data[' shares'] <= 100000]\n#plt.figure(figsize=(30,10))\n#sns.barplot(x=' num_imgs',y=' shares', hue='popularity', data=temp_data)\nsns.lmplot(x=' num_imgs', y=' shares', col='popularity', data=temp_data)","bef6f931":"#num_videos\ntemp_data = data[data[' shares'] <= 100000]\nnoise_data  = data[data[' num_videos'] == 0]\nprint (noise_data.shape)\n#plt.figure(figsize=(30,10))\n#sns.barplot(x=' num_imgs',y=' shares', hue='popularity', data=temp_data)\nsns.lmplot(x=' num_videos', y=' shares', col='popularity', data=temp_data)","cb9ce0cd":"#average_token_length\ntemp_data = data[data[' shares'] <= 100000]\nnoise_data  = data[data[' average_token_length'] == 0]\nprint (noise_data.shape)\n#plt.figure(figsize=(30,10))\nsns.scatterplot(x=' average_token_length',y=' shares', hue='popularity', data=temp_data)","7832b814":"#num_keywords\ntemp_data = data[data[' shares'] <= 100000]\nnoise_data  = data[data[' num_keywords'] == 0]\nprint (noise_data.shape)\n#plt.figure(figsize=(30,10))\nsns.scatterplot(x=' num_keywords',y=' shares', hue='popularity', data=temp_data)","4cc45537":"#self_reference_avg_sharess\ntemp_data = data[data[' shares'] <= 100000]\nnoise_data  = data[data[' self_reference_avg_sharess'] == 0]\nprint (noise_data.shape)\n#plt.figure(figsize=(30,10))\nsns.scatterplot(x=' self_reference_avg_sharess',y=' shares', hue='popularity', data=temp_data)\nsns.lmplot(x=' self_reference_avg_sharess', y=' shares', col='popularity', data=temp_data)","94f79f24":"print(\"Skewness: %f\" % data[' shares'].skew())\nprint(\"Kurtosis: %f\" % data[' shares'].kurt())\n","5b2fcc93":"from scipy.stats import norm, probplot\n\n#histogram and normal probability plot\ntemp_data = data[data[' shares'] <= 100000]\nfig,ax = plt.subplots(figsize=(10,10))\nsns.distplot(data[' shares'], fit=norm);\nfig = plt.figure()\nres = probplot(data[' shares'], plot=plt)\n'''\n'Shares' doesn't have a normal distribution. It shows 'peakedness', positive skewness and does not follow the diagonal line.\nThus some statistic analysis might not be suitable for it\n\n'''","45886a9f":"from copy import copy","68077949":"#applying log transformation\nnew_shares_data = copy(data)\n\nnew_shares_data.loc[new_shares_data[' shares'] > 0, ' shares'] = np.log(data.loc[data[' shares'] > 0, ' shares'])\nnew_shares_log = new_shares_data[' shares']\n#transformed histogram and normal probability plot\nfig,ax = plt.subplots(figsize=(10,10))\nsns.distplot(new_shares_log, fit=norm);\nfig = plt.figure()\nres = probplot(new_shares_log, plot=plt)","f8884d76":"#Check for missing data\ntotal = new_shares_data.isnull().sum().sort_values(ascending=False)\npercent = (new_shares_data.isnull().sum()\/new_shares_data.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data.head()","1ba08fa8":"# shares data is not needed for classification\ndata_without_shares = data.drop(labels=[' shares'], axis = 1, inplace=False)\n\ntraining_set = data_without_shares.iloc[:, :(data_without_shares.shape[1]-1)]\n# convert categorical variables into dummy - it use one-hot encoding\ntraining_set = pd.get_dummies(training_set)\n\n# extract the label data in this case popularity\nlabel_set = data_without_shares.iloc[:, (data_without_shares.shape[1]-1):].values\n\ndata_feature2 = copy(training_set)","c7efec98":"# only the best observed features are extracted here\ndata_feature1 =training_set[[' n_tokens_title',' n_tokens_content',' n_unique_tokens',' num_hrefs',\n                       ' num_self_hrefs',' num_imgs',' num_videos',' average_token_length',' num_keywords',\n                       ' kw_avg_avg',' self_reference_avg_sharess',' global_subjectivity',\n                       ' global_sentiment_polarity',' global_rate_positive_words',' global_rate_negative_words',' avg_positive_polarity',\n                       ' avg_negative_polarity',' title_sentiment_polarity','weekdays_Friday', 'weekdays_Monday', 'weekdays_Saturday',\n       'weekdays_Sunday', 'weekdays_Thursday', 'weekdays_Tueday',\n       'weekdays_Wednesday', 'data_channel_Business',\n       'data_channel_Entertainment', 'data_channel_Lifestyle',\n       'data_channel_Others', 'data_channel_Social Media', 'data_channel_Tech',\n       'data_channel_World']]","17b1e27f":"data_feature1.columns","69f95ad7":"# helper function for evalating the fisher ndex\ndef fisher_index_calc(trainingSet, labelSet):\n    (dim1_T, dim2_T) = trainingSet.shape\n    (dim1_L, dim2_L) = labelSet.shape\n\n    # create the fisher output variable - A vector of all the features\n    fisher_ratios = np.zeros((1, dim2_T), dtype=float).flatten()\n    # It's expected that the dim1_T and dim1_L be of the same size, else this input parameters is nulled.\n    if dim1_L != dim1_T:\n        return fisher_ratios\n\n    # First extract out the number of features available.\n    # grouped both data together, and create a pandas dataframe from it.\n    train1 = pd.DataFrame(trainingSet)\n    label1 = pd.DataFrame(labelSet, columns=['LABEL'])\n    grouped = pd.concat([train1, label1], axis=1)\n\n    # fetch the number of classes\n    (no_classes, demo) = grouped.groupby('LABEL').count()[[0]].shape\n    #print grouped\n\n    # loop through all features\n    for j in range(dim2_T):\n        # the variance of the feature j\n        j_variance = np.var(trainingSet[:,j])\n        j_mean = np.mean(trainingSet[:,j])\n        j_summation = 0\n        for k in range(no_classes):\n            output = grouped.groupby('LABEL').count()[[j]]\n            k_feature_count = output.iloc[k,0]\n            # mean for class k of feature j\n            output = grouped.groupby('LABEL').mean()[[j]]\n            k_feature_mean = output.iloc[k,0]\n            currentSum = k_feature_count * np.square((k_feature_mean - j_mean))\n            j_summation = j_summation + currentSum\n        fisher_ratios[j] = j_summation \/ np.square(j_variance)\n\n    return fisher_ratios","06df3897":"# calculates the fisher score of the features\nfisher_scores = fisher_index_calc(training_set.values, label_set)\n\ndf = pd.DataFrame({'Fisher Ratio For All Features': fisher_scores})\n","c03c14a8":"# plot the fisher analysis score\nax = df.plot.bar(figsize=(20,10))\nplt.show()","de85c3df":"# feature selection based on fisher score\n# Fisher Index Ratio Filter - Remove features with low score\n# indices of features to remove based on fisher ratios\nto_remove = []\nfor i in range((len(fisher_scores))):\n    if fisher_scores[i] < 3000:\n        # we mark for removal\n        to_remove.append(i)\n\n# remove features with low fisher score\ndata_feature_fisher = training_set.drop(training_set.columns[to_remove], axis=1, inplace=False)\nprint (\"fisher based features - \", data_feature_fisher.shape)\ndata_feature_fisher.columns","1834c590":"# use log transformation to transform each features to a normal distribution\ntraining_set_normal = copy(training_set)\n\n# note log transformation can only be performed on data without zero value\nfor col in training_set_normal.columns:\n    #applying log transformation\n    temp = training_set_normal[training_set_normal[col] == 0]\n    # only apply to non-zero features\n    if temp.shape[0] == 0:\n        training_set_normal[col] = np.log(training_set_normal[col])\n        print (col)\n    else:\n        # attempt to only transform the positive values alone\n        training_set_normal.loc[training_set_normal[col] > 0, col] = np.log(training_set_normal.loc[training_set_normal[col] > 0, col])\n","a7115f6f":"# only the best observed features are extracted here\ndata_feature1_normal =training_set_normal[[' n_tokens_title',' n_tokens_content',' n_unique_tokens',' num_hrefs',\n                       ' num_self_hrefs',' num_imgs',' num_videos',' average_token_length',' num_keywords',\n                       ' kw_avg_avg',' self_reference_avg_sharess',' global_subjectivity',\n                       ' global_sentiment_polarity',' global_rate_positive_words',' global_rate_negative_words',' avg_positive_polarity',\n                       ' avg_negative_polarity',' title_sentiment_polarity','weekdays_Friday', 'weekdays_Monday', 'weekdays_Saturday',\n       'weekdays_Sunday', 'weekdays_Thursday', 'weekdays_Tueday',\n       'weekdays_Wednesday', 'data_channel_Business',\n       'data_channel_Entertainment', 'data_channel_Lifestyle',\n       'data_channel_Others', 'data_channel_Social Media', 'data_channel_Tech',\n       'data_channel_World']]\n","58452c39":"# calculates the fisher score of the features\nfisher_scores_normal = fisher_index_calc(training_set_normal.values, label_set)\n\ndf = pd.DataFrame({'Fisher Ratio For All Features - Normal Distribution': fisher_scores_normal})\n# plot the fisher analysis score\nax = df.plot.bar(figsize=(20,10))\nplt.show()","b7215d4b":"# feature selection based on fisher score\n# Fisher Index Ratio Filter - Remove features with low score\n# indices of features to remove based on fisher ratios\nto_remove = []\nfor i in range((len(fisher_scores_normal))):\n    if fisher_scores_normal[i] < 1000:\n        # we mark for removal\n        to_remove.append(i)\n\n# remove features with low fisher score\ndata_feature_fisher_normal = training_set_normal.drop(training_set_normal.columns[to_remove], axis=1, inplace=False)\n# ihave about 25 features left.\nprint (\"fisher based features : Normal distributions - \", data_feature_fisher_normal.shape)\ndata_feature_fisher_normal.columns","96938cab":"data_feature2_normal = copy(training_set_normal)","69f6c9d5":"temp_data_normal = pd.concat([training_set_normal, pd.DataFrame(new_shares_log, columns=[' shares'])], axis=1)\nlabel_set6 = data_without_shares.iloc[:, (data_without_shares.shape[1]-1):]\ntemp_data_normal = pd.concat([temp_data_normal, label_set6], axis=1)\n","e75e1225":"temp_data = temp_data_normal[temp_data_normal[' shares'] <= 100000]\n# running a pair plot for the kw__terms\nkw_cols = [' average_token_length', ' num_keywords', ' n_tokens_title', ' global_sentiment_polarity', ' shares']\n# run a pairplot\nsns.pairplot(temp_data_normal, vars=kw_cols, hue='popularity', diag_kind='kde')","2a64cadc":"#n_tokens_title\nfig, axs = plt.subplots(figsize=(20,10), nrows=1,ncols=2)\nsns.scatterplot(x=' num_keywords',y=' shares', hue='popularity', data=temp_data_normal, ax=axs[0])\nsns.scatterplot(x=' num_keywords',y=' shares', hue='popularity', data=data, ax=axs[1])","ad336cec":"#n_tokens_content\nfig, axs = plt.subplots(figsize=(20,10), nrows=1,ncols=2)\nsns.scatterplot(x=' n_tokens_content',y=' shares', hue='popularity', data=temp_data_normal, ax=axs[0])\nsns.scatterplot(x=' n_tokens_content',y=' shares', hue='popularity', data=data, ax=axs[1])","eac79001":"#title_subjectivity\nfig, axs = plt.subplots(figsize=(20,10), nrows=1,ncols=2)\nsns.scatterplot(x=' title_subjectivity',y=' shares', hue='popularity', data=temp_data_normal, ax=axs[0])\nsns.scatterplot(x=' title_subjectivity',y=' shares', hue='popularity', data=data, ax=axs[1])","0fe48f11":"# normalizaling the data with standard scaler\n# we will normalized all the features selections\n\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\n\n# scalled all the feature selections\ndata_feature1_nor = scaler.fit_transform(data_feature1.values)\ndata_feature2_nor = scaler.fit_transform(data_feature2.values)\ndata_feature_fisher_nor = scaler.fit_transform(data_feature_fisher.values)\ndata_feature1_normal_nor = scaler.fit_transform(data_feature1_normal.values)\ndata_feature_fisher_normal_nor = scaler.fit_transform(data_feature_fisher_normal.values)\ndata_feature2_normal_nor = scaler.fit_transform(data_feature2_normal.values)\n\nfeatures_selection = [data_feature1_nor, data_feature2_nor, data_feature_fisher_nor, data_feature1_normal_nor, \n                     data_feature_fisher_normal_nor, data_feature2_normal_nor]\n\nfeatures_selection_labels = ['Features on Hypothesis', 'All Features', 'Fisher based Features', \n                             'Features on Hypothesis - Normal Distribution', 'Fisher based Features - Normal Distribution',\n                             'All Features - Normal Distribution']\n","c5751698":"data.head(n=5)","edbf800e":"# encoding the label set with a label encoder\nfrom sklearn.preprocessing import LabelEncoder\n\nlabelEn = LabelEncoder()\nencoded_labels = labelEn.fit_transform(data.loc[:, 'popularity'].values)\nclass_names = labelEn.classes_\nclass_names","45eb6092":"# Splitting the data for Training and Testing\nfrom sklearn.model_selection import train_test_split, GridSearchCV\n\n# train and test for a feature selections\nX_train_1, X_test_1, y_train_1, y_test_1 = train_test_split(data_feature1_nor, encoded_labels, test_size=0.3, shuffle=False)\nX_train_2, X_test_2, y_train_2, y_test_2 = train_test_split(data_feature2_nor, encoded_labels, test_size=0.3, shuffle=False)\nX_train_3, X_test_3, y_train_3, y_test_3 = train_test_split(data_feature_fisher_nor, encoded_labels, test_size=0.3, shuffle=False)\nX_train_4, X_test_4, y_train_4, y_test_4 = train_test_split(data_feature1_normal_nor, encoded_labels, test_size=0.3, shuffle=False)\nX_train_5, X_test_5, y_train_5, y_test_5 = train_test_split(data_feature_fisher_normal_nor, encoded_labels, test_size=0.3, shuffle=False)\nX_train_6, X_test_6, y_train_6, y_test_6 = train_test_split(data_feature2_normal_nor, encoded_labels, test_size=0.3, shuffle=False)","15dd0db3":"from sklearn.metrics import accuracy_score, make_scorer","07711e90":"features_selection[4].shape","38db0eaa":"encoded_labels.shape","8de78cad":"# function for confusion matrix\n\nimport matplotlib.pyplot as plt\n\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.utils.multiclass import unique_labels\n\ndef plot_confusion_matrix(y_true, y_pred, classes,\n                          normalize=False,\n                          title=None,\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    if not title:\n        if normalize:\n            title = 'Normalized confusion matrix'\n        else:\n            title = 'Confusion matrix, without normalization'\n\n    # Compute confusion matrix\n    cm = confusion_matrix(y_true, y_pred)\n    # Only use the labels that appear in the data\n    classes = classes[unique_labels(y_true, y_pred)]\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n\n    print(cm)\n\n    fig, ax = plt.subplots(figsize=(10,10))\n    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n    ax.figure.colorbar(im, ax=ax)\n    # We want to show all ticks...\n    ax.set(xticks=np.arange(cm.shape[1]),\n           yticks=np.arange(cm.shape[0]),\n           # ... and label them with the respective list entries\n           xticklabels=classes, yticklabels=classes,\n           title=title,\n           ylabel='True label',\n           xlabel='Predicted label')\n\n    # Rotate the tick labels and set their alignment.\n    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n             rotation_mode=\"anchor\")\n\n    # Loop over data dimensions and create text annotations.\n    fmt = '.32f' if normalize else 'd'\n    thresh = cm.max() \/ 2.\n    for i in range(cm.shape[0]):\n        for j in range(cm.shape[1]):\n            ax.text(j, i, format(cm[i, j], fmt),\n                    ha=\"center\", va=\"center\",\n                    color=\"white\" if cm[i, j] > thresh else \"black\")\n    fig.tight_layout(pad=5, rect= (0, 0, 1, 1))\n    return ax","1eed21ed":"# defining the model\nfrom sklearn.neighbors import KNeighborsClassifier\n\n\nk_range = np.arange(1,100)\naccuracy = []\n\nfor n in k_range:    \n    neigh = KNeighborsClassifier(n_neighbors=n, n_jobs=-1)\n\n    neigh.fit(X_train_2, y_train_2)  \n\n    # predict the result\n    y_pred = neigh.predict(X_test_2)\n    #print (\"Random Forest Classifer Result\")\n    #print (\"Performance - \" + str(100*accuracy_score(y_pred, y_test_2)) + \"%\")\n    accuracy.append(100*accuracy_score(y_pred, y_test_2))\n\n","d34994bf":"plt.figure(figsize=(20,13))\nplt.plot(k_range, accuracy, 'r-', label='KNN Accuracy Vs KNN Neighbors size')\nplt.plot(k_range, accuracy, 'bx')\nplt.xlabel('KNN Neighbors size')\nplt.ylabel('KNN Accuracy')\nplt.legend()\nplt.grid()\nplt.title('KNN Accuracy Vs Neighbors size')\nplt.show()","ca8a1de1":"from sklearn.ensemble import RandomForestClassifier\n\nnns = [1, 5, 10, 50, 100, 200, 500, 1000, 2000, 3000]\naccuracy = []\n\nfor n in nns:    \n    clf = RandomForestClassifier(n_estimators=n, n_jobs=5, max_depth=50,\n                                 random_state=0)\n    clf.fit(X_train_2, y_train_2)  \n\n    # predict the result\n    y_pred = clf.predict(X_test_2)\n    #print (\"Random Forest Classifer Result\")\n    #print (\"Performance - \" + str(100*accuracy_score(y_pred, y_test_2)) + \"%\")\n    accuracy.append(100*accuracy_score(y_pred, y_test_2))\n","0a33e818":"from sklearn.ensemble import RandomForestClassifier\n\nclf = RandomForestClassifier(n_estimators=1000, n_jobs=-1, max_depth=50,\n                             random_state=0)\nclf.fit(X_train_2, y_train_2)  \n\n# predict the result\ny_pred = clf.predict(X_test_2)\nprint (\"Random Forest Classifer Result\")\nprint (\"Performance - \" + str(100*accuracy_score(y_pred, y_test_2)) + \"%\")\n","45d5b241":"# Plot non-normalized confusion matrix\nplot_confusion_matrix(y_test_2, y_pred, classes=class_names,\n                      title='Confusion matrix For Random Forest')","5c7afb5b":"plt.figure(figsize=(10,7))\nplt.plot(nns, accuracy, 'r-', label='Random Forest Accuracy Vs Number of Tress')\nplt.plot(nns, accuracy, 'bx')\nplt.xlabel('Random Forest Tree Sizes')\nplt.ylabel('Random Forest Accuracy')\nplt.legend()\nplt.grid()\nplt.title('Random Forest Accuracy Vs Number of Tress')\nplt.show()","e6f8304b":"# iterating through all the possible features\n\nclf = RandomForestClassifier(n_estimators=1000, n_jobs=5, max_depth=50,\n                                 random_state=0)\n\nfor i in range(len(features_selection)):\n    X_train, X_test, y_train, y_test = train_test_split(features_selection[i], encoded_labels, test_size=0.3, shuffle=False)\n        \n    # commence training - NOTE: It takes hours to be complete\n    clf.fit(X_train, y_train)\n\n    # predict the result\n    y_pred = clf.predict(X_test)\n    \n    print(\"Result for using Feature Selection - \", features_selection_labels[i])\n    print (\"Random Forest Classifer Result\")\n    print (\"Performance - \" + str(100*accuracy_score(y_pred, y_test)) + \"%\")\n   ","ba81ff91":"from sklearn.svm import SVC\n\nsvc_grid = SVC(gamma='auto')\n\n# iterating through all the possible features\n\nfor i in range(len(features_selection)):\n    X_train, X_test, y_train, y_test = train_test_split(features_selection[i], encoded_labels, test_size=0.3, shuffle=False)\n        \n    # commence training - NOTE: It takes hours to be complete\n    svc_grid.fit(X_train, y_train)\n\n    # predict the result\n    y_pred = svc_grid.predict(X_test)\n    \n    print(\"Result for using Feature Selection - \", features_selection_labels[i])\n    print (\"SVC Classifer Result\")\n    print (\"Performance - \" + str(100*accuracy_score(y_pred, y_test)) + \"%\")\n   ","32a05210":"#### Data Channel Evaluation\nHere, it can be seen that the best articles with highest share popularity belongs to the \"Others\" channel. For a more concrete channel,\nThe \"Business\" and \"Entertaiment\" channel are great for the best popularity. Coming in third position will be the \"World\" and \"Tech\". \nTech channels performed generally okay. \nOne important observation is also that \"Entertaiment\" channel based articls seems to be persistent in all popularity types. Meaning they might\nnot always be the best channel to publish for. ","9be2f665":"#### Feature selection based on best hypothesis observed - Normal distribution\n","51b62b31":"In this project we analyzed the given online news data set and was able to clearly observed some interesting patterns that good articles do have in common. We initially carried out a subjective analysis which was based on our own intuition and because we understand it is easily possible for human intuition to be biased or crowded from past experience, and use a quantitative analysis to confirm our initial hypothesis by doing univariate analysis using scatter plot, boxplot, and barplot of each feature with the shares feature.\n\nAlthough our main gain is to build important insight about how popularity of articles are defined, we also went ahead in seeing how to predict the popularity of an article. Seven popularity classes was derived from the shares class and three machine learning model was built to be able to predict the popularity class of the model. In order to be able to tune the models for better performances we consider different feature selection techniques, but these feature selections technique didn\u2019t really made much influence to the performance of the machine learning models.\n\nThe best machine learning model was the **Random Forest** which was able to attain an accuracy of 51.4% on the testing data-set. Some of the reasons for this low accuracy score is as a result of the large variance in the data set and also the imbalance in the class distribution which drives the prediction models to be bias towards popularity classes with more articles.\n\nFrom the insight analysis carried out on the data-set the following are some of the things we recommend to improve the popularity of an article:\n- The number of words in the article should be less than 1500 words. The lesser the better.\n- Article title shouldn\u2019t be too long or too short. 6 \u2013 17 words is the ideal number of words to have for titles.\n- Articles should have good amount of images. Between 1 \u2013 40 images is great.\n- Also having a couple of videos is also nice for article popularity, but not too much. The higher the lower the odds.\n- Easy to read words helps to improve article popularity.\n- The number of keywords in the metadata really influences the shares to a margin. The higher the value the better the shares chances. A value upward of 5 is recommend.\n- Articles referencing popular articles have a higher chance of improving their own popularity.\n- Increase the number of popular unique words in the article to increase the chances of having better popularity.\n- Avoid the use of longer words in the articles.\n- Best popular articles are usually posted on Mondays and Wednesday (and a bit of Tuesdays). Sundays and Saturdays (Weekends generally) are the worsts days to publish an article. \n- Articles that talks about current trending tends to have higher popularity.\n- Increase the amount of subjectivity in the title and content. \n- The \"Business\" and \"Entertainment\" channel are great for the best popularity. Coming in third position will be the \"World\" and\/or \"Tech\" channels.","f9666b83":"#### Data channels vs Num_images vs popularity\nHere we compared data channels impact with num images in regards to article popularity - \nEarlier we said, good articles tend to have high visuals (num_images) in them but it is not always the case everytime. \nThose rare cases where the high visuals or low visuals doesn't change anything is in the Business channel. From the plot below,\nwe can see that Business channels generally don't get influnece by the num_images in them. They are generally low inrespective of the popularity.\nThis is peculiar pattern. \nEntertainment channels generally tend to have high visuals as their popularity increases, with the only exception in 'Average' popularity.\n","80fa21ad":" ## Normal Distribution analysis for 'Shares'\n  - Normality what we mean is that the data should look like a normal distribution. This is important because several statistic tests rely on this (e.g. t-statistics) and some machine learning models. We evaluated the impact of this normaility on our models","9c2d9d04":"# Exploratory Analysis For Online News Popularity - A deep insight analysis\n\n- by [Ayo Ayibiowu](https:\/\/www.linkedin.com\/in\/ayoayibiowu\/)\n\nIn this project, the goal is the explore the dataset given and be able to find critical insights that can be used to influence potential article popularity. Also, machine learning models was built to be able to predict the popularity of a given article. \n\nThe process followed is highlighted below:\n - Data Cleaning - Noise detection and removal\n - Subjective analysis - Using our intuition to evaluate a data variable\/feature and decide whether a variable influences the popularity of the article or not.\n - Quantitative Analysis - How correct is our intuition? Here we carry our several analysis to accept or debunk our initial hypothesis\n - Normal Distribuiton Observation on the dataset\n - Feature Selection and Evaluation\n - Machine Learning Classification\n - Summary and Conclusion. \n \n","1a9ac90a":"## Data Visualization \nEvaluating our hypothesis\n","f2dc015a":"## Open Book - Ideas to consider\n\nSome ideas to consider\n - What is the effect of number of images\/number of videos on the article popularity?\n - Is their a relationship between the number of words in the content and\/or  \n number of words in the title in the article popularity\n - Is their a concrete relationship between average length of words in the content to the popularity\n - Create a grading\/rank for the popularity: Excellent, Good, Okay, Poor, Very Poor\n - How is the ranking of the channels in regards to the shares popularity\n - What data channel has the most popularity and what feature in that particular data channel\n contributes towards that asserction. Is this also observed in the other data channels?\n - What about the effects of the Worst, Best, and Avg keywords\n \n - Is their an influence on the min, max or avg shares on each article referenced articles. Does\n the number of shares in those referenced articles also influence the number of shares in the main\n article?\n \n - At what point in the weekend do people share articles the most? Can that means people read those\n articles the most on those days?\n \n - What is the effect of LDA analysis on the article popularity\n \n - Does article with more text sentiment influence the popularity\n \n - what the is relationship between the text sentiment and the article publish day? Are more sentiment \n on a particlar day?\n \n - How about the influence of postive\/negative words in the text sentiment and popularity. Do people\n favours postive worded article. What is ratio of postive to negative word articles in the dataset? \n Are the mutually balanced and can we make a judgment based on that alone?\n \n - What about the effect of subjectivity on the title and test in the popularity\n ","671ddcf3":"# To Be Continued","a90aad4f":"### Noise Removal\n - We observed some noise from the dataset coming from different features. \n ","317f2f76":"## Grading the Shares\n\n* Exceptional = Top 95%\n* Excellent = Top 90%\n* Very Good = Top 80%\n* Good = Top 60%\n* Average = Top 50%\n* Poor = Top 35%\n* Very Poor = Rest\n","963de34f":"### Random Forest Classifier\n","7c37783b":"# Machine Learning - Supervised Learning Classification\n - Here, we are going to apply some machine learing models on our dataset for classifying an article popularity","0b142460":"The maximum accuracy observed with SVM was 50,58% . An observation with SVM is that training start becoming increasing as the number of polynomial degree increases, training examples increases, C value rises and also the number of features increase which basically makes the model become more complex to draw an hyper plane for separating the classes. ","3baa1aa5":"Although shares doesn't have a normal distrubition, we can do a log transformation to give us a normal distrubition data","9751d008":"### Variables of our features selection are listed below:\n - Feature selection based on best hypothesis observed - **data_feature1**\n - Feature Selection on the whole dataset - **data_feature2**\n - Feature selection using fisher discriminal analysis - **data_feature_fisher**\n - Feature selection based on the best hypothesis observed but with a normal distribution (log transformation) - **data_feature1_normal**\n - Feature selection using fisher discriminate analysis on normal distribution dataset - **data_feature_fisher_normal**\n - Feature Selection on the whole dataset - **data_feature2_normal** \n\n","e5fd4edf":"### Here we check the class balance\n","04a05553":"#### Weekdays Variable Effect\n","f54f3b4a":"### Evaluating Expectations","bbe32248":"## Varaibles Summary Observation","f981162b":"Random Forest has the best result for this classification task reaching an accuracy of 51.4%. Due to the nature of Random forest being able to set different number of decision trees, features, tree depth, splitting criteria, and others it tends to require a lot of parameter tuning. ","21ebd5f5":"> > From the scatterplot below, it can be seen than good articles will generally tend to have n_tokens_content less than 2000 and greater than 100 words","8cd60551":"# Feature Extraction & Selection\n - Here we will be extracting some of the best features we observed out from the data. The below criteria will be considered:\n  - Feature selection based on best hypothesis observed\n  - Feature Selection on the whole dataset\n  - Feature selection using fisher discriminal analysis\n  - Feature selection based on the best hypothesis observed but with a normal distribution (log transformation)\n  - Feature selection using fisher discriminate analysis on normal distribution dataset\n  - Feature Selection on the whole dataset - Normal Distribution","5fb7b5b4":"This project is continued [here](https:\/\/www.kaggle.com\/thehapyone\/online-news-popularity-a-classification-problem). In the v2 of this project, the knowledge gained so far is used to address a classification problem thus improving the prediction accuracy by large margin.","327652ba":"# Data Processing\n","c274486c":"#### Feature Selection Based on Fisher Discrimating Analysis","6171b994":"## Reading the data","f457f1a8":"# Making Recommendations For Good Articles\n - n_tokens_content should be less than 1500 words. The lesser the better.\n - n_tokens_title should be between 6 - 17 words. \n - n_unique_tokens should be between 0.3 - 0.8\n - num_hrefs is between 1 and 40 referrence links\n - num_imgs should between 1 - 40 images\n - num_videos should be between 0 - 25 vidoes. The higher the lower the odds.\n - average_token_length should be between 4 - 6\n - The number of keywords in the metedata really influences the shares to a margin. The higher the value the better the shares chances. A value upward of 5 is recommend.\n - Here, it can be seen that the best articles with highest share popularity belongs to the \"Others\" channel. For a more concrete channel, The \"Business\" and \"Entertaiment\" channel are great for the best popularity. Coming in third position will be the \"World\" and \"Tech\".\n - Best popular articles are usually posted on Mondays and Wednesday (and a bit of tuesdays). Sundays and Saturdays (Weekends generally) are the worsts days to publish an articles. \n - Articles that talks about current trending are better for shares","bf4f60ec":"#### Finding the normal distrubution of the dataset","92890d15":"## Visulazing the impact of normal distribution on the data\n In the new transformation, the features observation is more clear than before.\n","c654ba74":"It seems the best popular articles are usually posted on Mondays and Wednesday (and a bit of tuesdays)\nSundays and Saturdays (Weekends generally) are the worsts days to publish an articles. Your chances are low\n","38a5de36":"#### Feature Selection on the whole dataset - Normal Distribution\n","35724fcc":"> > From the scatterplot below, it can be seen than good articles will generally tend to have n_tokens_title between 6 and 17 words","f4f59e6f":"# Summary and Conclusion","ced1d97f":"The KNN model which gave the best accuracy of 49.11% was based on using the all the data-set feature and number of neighbor of 71. Although, this was the best accuracy discovered, there wasn\u2019t much difference with the other models. For example, using all the features gave an accuracy less than 1 % of the highest accuracy observed. ","78eeb038":"#### Feature Selection on the whole dataset\n","55cd170e":"![image.png](attachment:image.png)","e7810af7":"#### Feature Selection Based on Fisher Discrimating Analysis - Normal Distribution\n","81dc9e19":"#### Feature selection based on best hypothesis observed","1d41a601":"### Evaluating the Observerd Hypothesis","4aa27741":">> similar to the above. ","008c8af2":"### SVM - Support Vector Machines\n","8695f600":"### KNN Classifier"}}