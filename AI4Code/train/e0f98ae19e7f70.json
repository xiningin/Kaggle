{"cell_type":{"561bc9e8":"code","22bd85bb":"code","c177c75c":"code","80c85f1e":"code","2592725c":"code","8ea6db2b":"code","1daa5f46":"code","b696c68a":"code","cd76a289":"code","8d2e0528":"code","ddb55560":"code","ce07ea1d":"code","be4c8daf":"code","dde39381":"code","f1c908a9":"code","f60cfa87":"code","6b38d875":"code","36b58f9b":"code","910e7e48":"code","3a90805d":"code","48865d70":"code","1d59309f":"code","90272258":"code","90641560":"code","d3bc03a6":"code","e584d566":"markdown","70d076de":"markdown","bc653f18":"markdown","9038974e":"markdown","3281a58e":"markdown","a40abedc":"markdown","a883dac1":"markdown","cecf7147":"markdown","a897da00":"markdown","aec6690a":"markdown","3d54b85b":"markdown","4b50be89":"markdown","08cb9c78":"markdown","42f5f4e8":"markdown","509e23c7":"markdown","539a5e1d":"markdown","1d7c1541":"markdown","e30e865c":"markdown","de5fed08":"markdown","90e770c5":"markdown","db7183ba":"markdown","8aa3011f":"markdown","aa5f0398":"markdown","7eef8f1e":"markdown","8a8fa40e":"markdown","14566751":"markdown","647d8c08":"markdown"},"source":{"561bc9e8":"import numpy as np\nimport pandas as pd\n\ndata = pd.read_excel('\/kaggle\/input\/final_data.xlsx', parse_dates=True, index_col='date')\ndata=data[:-1]\ndata_raw=data.copy()\n\ndata.head()","22bd85bb":"data = data.interpolate(method='polynomial', order=1)\n\ndata.head()","c177c75c":"data_usd = pd.read_csv('\/kaggle\/input\/exchange_rate.csv')\ndata_usd_raw = data_usd.copy()\n\ndata_usd.head()","80c85f1e":"data_usd['date'] = pd.to_datetime(data_usd['date'],\n                    format='%d.%m.%Y', errors='ignore')\ndata_usd = data_usd.set_index('date')\ndata_usd = data_usd['exrate']\n\ndata_usd.head()","2592725c":"import matplotlib.pyplot as plt\nfrom pandas.plotting import register_matplotlib_converters\nregister_matplotlib_converters()\n\nplt.plot(data_usd)\nplt.show()","8ea6db2b":"data_usd = data_usd.resample('M').mean()\n\nend_date = '2019-12-31'\ndata_usd = data_usd[:end_date]\n\ndata = data.assign(exrate = data_usd.values)\n\nstart_date = '2012-01-01'\ndata = data[start_date:]\n\ndata.head()","1daa5f46":"data_interbank = pd.read_excel('\/kaggle\/input\/interbank.xlsx')\n\ndata_interbank.head()","b696c68a":"import re\n \ndef regexp(reg):\n\n    res = re.findall(r'\\d{2}.\\d{2}.\\d{4}', reg)\n    return res[0]     \n\ndata_interbank['date'] = data_interbank['date'].apply(regexp)\n\ndata_interbank['date'].head()","cd76a289":"def replace(rep):\n\n    rep = rep.replace(',', '.') \n    return rep\n\ndef to_float(fl):\n\n    fl = float(fl)\n    return fl\n\ndata_interbank['total_amount_usd'] = data_interbank['total_amount_usd'].apply(replace)\ndata_interbank['total_amount_usd'] = data_interbank['total_amount_usd'].apply(to_float)\n\ndata_interbank['total_amount_usd'].head()","8d2e0528":"data_interbank['date'] = pd.to_datetime(data_interbank['date'],\n                    format='%d.%m.%Y', errors='ignore')\ndata_interbank = data_interbank.set_index('date')\ndata_interbank = data_interbank['total_amount_usd']\ndata_interbank = data_interbank.resample('M').sum()\ndata_interbank = data_interbank[start_date:]\n\ndata_interbank.head()\n\ndata = data.assign(interbank = data_interbank.values)","ddb55560":"from sklearn.preprocessing import MinMaxScaler\n\nscaler_X = MinMaxScaler(feature_range = (0, 1))\n\nX = data.drop(labels=['exrate'], axis=1)\nX = pd.DataFrame(scaler_X.fit_transform(X), columns = X.columns)\n\nX.head()","ce07ea1d":"scaler_y = MinMaxScaler(feature_range = (0, 1))\n\ny = np.array(data['exrate'])\ny = np.reshape(y, (len(y),-1))\ny = pd.DataFrame(scaler_y.fit_transform(y))\n\ny.head()","be4c8daf":"def raw_plot(data, column_name):\n\n    plt.plot(data.index, data[column_name], label=column_name)\n    plt.legend()\n    plt.show()    \n    \nraw_plot(X, 'ppi')","dde39381":"def box(feat):\n\n    plt.boxplot(x=X[feat])\n    plt.title(feat)\n    plt.show()     \n    \nbox('ppi')","f1c908a9":"features = list(X.columns)\nprint(features)","f60cfa87":"def fix_outliers(column):\n    \n    learning_rate = 0.35\n    \n    q1 = X[column].quantile(0.25)\n    q3 = X[column].quantile(0.75)\n    iqr = q3-q1\n    fence_low  = q1-1.5*iqr\n    fence_high = q3+1.5*iqr\n    \n    X[column].loc[(X[column] >= fence_high)] = X[column].quantile(1-learning_rate)\n    X[column].loc[(X[column] <= fence_low)] = X[column].quantile(learning_rate)\n        \nfor col in features:\n    fix_outliers(col)  ","6b38d875":"raw_plot(X, 'ppi')\nbox('ppi')","36b58f9b":"def feature_lag(features):\n\n    for feature in features:\n        X[feature + '-lag1'] = X[feature].shift(1)\n        X[feature + '-lag2'] = X[feature].shift(2)\n        X[feature + '-lag3'] = X[feature].shift(3)\n        X[feature + '-lag6'] = X[feature].shift(6)\n        X[feature + '-lag12'] = X[feature].shift(12)\n    \nfeature_lag(features)  \nX.drop(features, axis=1, inplace=True)\n\nprint(X.columns)","910e7e48":"X.head()\n\nreal_X_size = len(X)\nX = X.dropna()\ndropna_X_size = len(X)\ny = y[real_X_size-dropna_X_size:]","3a90805d":"train_size = 0.78\nseparator = round(len(X.index)*train_size)\n\nX_train, y_train = X.iloc[0:separator], y.iloc[0:separator]\nX_test, y_test = X.iloc[separator:], y.iloc[separator:]","48865d70":"from keras.wrappers.scikit_learn import KerasRegressor\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout\nfrom keras.optimizers import Adam\nfrom keras.constraints import maxnorm\n\ndef build_model():\n\n    model = Sequential([\n    Dense(128, activation='relu', input_shape=[len(X.columns)],\n                        kernel_constraint=maxnorm(5)),\n    Dropout(0.3),\n    Dense(1, kernel_initializer='normal', activation='sigmoid')])\n    optimizer = Adam(lr=0.01)\n    model.compile(optimizer=optimizer, loss='mean_squared_error',\n                  metrics=['accuracy'])\n    return model\n\nmodel = KerasRegressor(build_fn=build_model, epochs=200, batch_size=10, verbose=0)","1d59309f":"history = model.fit(X_train, y_train)\npreds = model.predict(X_test)\n\nprint(preds)","90272258":"predictions = scaler_y.inverse_transform([preds])\npreds_real = [x for x in predictions[0]]\n\nprint(preds_real)","90641560":"def predict_plot():\n\n    ind_preds = data['2018-07-01':'2019-12-31']\n    fig, axs = plt.subplots(1, figsize=(9,7))\n    fig.suptitle('Predictions\/real values')\n    axs.plot(data.index, data.exrate, 'b-', label='real')\n    axs.plot(ind_preds.index, preds_real, 'r-', label='prediction')\n    axs.legend(loc=2)\n\npredict_plot()  ","d3bc03a6":"from sklearn.metrics import mean_absolute_error\n\ndef errors(y_true, y_pred, r):\n    \n    y_true, y_pred = np.array(y_true), np.array(y_pred)\n    mape = np.mean(np.abs( (y_true - y_pred)\/y_true))*100\n    print('MAPE: {}%'.format(mape.round(r)))\n    print('MAE: {}'.format(mean_absolute_error(y_true, y_pred).round(r)))  \n\nerrors(data_usd['2018-7-01':'2019-12-31'], preds_real, 3)","e584d566":"# Ukrainian hryvnia exchange rate prediction using Keras\n\n**by Tsepa Oleksii, Samoshin Andriy and Mysak Yuriy**\n\nIn this notebook we will walk through time series forecasting using Keras Neural Network, also the same problem was solved by XGBoost and ARIMA, but they were not as good as Keras. This notebook was made for NBU IT Challenge on topic \"Building a neural network for forecasting exchange rates\".\nFull pack of datasets and solutions you can find [here](https:\/\/github.com\/imgremlin\/NBU-IT-Challenge).","70d076de":"# Data Exploration\n\n**Main dataframe**\n\nSo let's turn to the code. First of all import main file with our data about which you can read in \"data_tutorial.txt\".","bc653f18":"Plot our predictions to compare them with real values.","9038974e":"Finally, we can do the same things with interbank data as with usd data and then concatenate 3 datasets to make a single whole.","3281a58e":"Now we have to transform our predictions from 0-1 scope to real values.","a40abedc":"# Modeling\n\nDefine training size, then splitting X and y on train and test sets.","a883dac1":"As I said, we have got daily usd data and other data is given monthly, so we've to reshape usd data by taking a mean across the month. Then we're cropping our dataset due to inconsistencies in date limits. Soon we'll work with interbank dataset, which beginning from 2012.","cecf7147":"Fit model and make predictions. ","a897da00":"# Data Preprocessing\n\n**Scaling**\n\nDivide dataset to X and y. Because of using Neural Networks it is a must to scale all data. We used MinMaxScaler. If you want then decode our predictions, you have to use two separate scalers - for X and y.","aec6690a":"But it easily to detect outliers by using box plots. Above plot shows point equals to 1, that is outlier as that is not included in the box of other observation i.e no where near the quartiles.","3d54b85b":"Here do the same things, but to scale 1D dataset, we need to reshape it.","4b50be89":"So, let's clean the dataset: we need only date columns, which will be used as an index and column 'exrate'.","08cb9c78":"**Feature lags**\n\nDue to the fact that our task is to predict the exchange rate, we can not use the data from the same month to predict on itself, so we created feature lags. Feature lags are new variables which shows impact of a certain feature a few months later. Then drop our non-lagged features.","42f5f4e8":"You can see that now we haven't got outliers on both plots.","509e23c7":"Make a list of our columns in order to further work with it.","539a5e1d":"Build a model using Keras Regressor. Almost all parameters were taking by using GridSearch, it's not hard, so you can do it on your own, but it takes some time. To make the process faster - you can some other tools such as Random Search or Keras Tuner.","1d7c1541":"**USD dataframe**\n\nThen we're importing exchange rate data which is given daily","e30e865c":"Build a plot to understand our exchange rate.","de5fed08":"Calculate error to evaluate model and improve in future.","90e770c5":"**Fixing outliers**\n\nNow, we've to work with each column in X and fix outliers - extreme values that deviate from other observations on data. As an example let's take 'ppi' column, where you can see outlier right before 40s index.","db7183ba":"That's all, thank you for reading! Leave your comments and suggestion what would you change in this model.","8aa3011f":"There are a lot of missing data, but we can deal with them using interpolation. We choose linear interpolation.\n\nMissing data appears due to the fact that some data is given quarterly, other annually, other monthly.","aa5f0398":"Because of we are using lags, we get new missing values. Remove them in X dataframe and then remove the same amount of rows in y dataframe.","7eef8f1e":"If we want to convert this dataset to the Series with date index and value, we need to clean it. Firstly let's use regexp to get rid of last to cyrillic characters and adapt to the datetime format.","8a8fa40e":"We write a function that removes outliers by equating them to a certain quartile, the quartile is chosen manually.","14566751":"**Interbank dataframe**\n\nImport one more dataset, where we can find how much currency sold NBU on a given day","647d8c08":"Then, interbank data has got another problem - last two columns are not numerical, so we have to replace all ' , ' to ' . ' and then convert them to float."}}