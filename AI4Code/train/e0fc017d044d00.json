{"cell_type":{"3cc99b9d":"code","0c722573":"code","7338c8d3":"code","818a6e42":"code","a8b28ff5":"code","12ce2836":"code","ea18fba9":"code","51ea926d":"code","c6fcc493":"code","4e15f093":"code","593d55a0":"code","1adc707f":"code","678e3dbf":"code","d7f3d7db":"code","857d6538":"code","6332bc39":"code","6d03f97c":"code","efa7f612":"code","aa22dc22":"code","dbf9c816":"code","e7c61e4f":"code","34ef529b":"code","38851bd8":"code","868d5eb3":"markdown","93ca1674":"markdown","34c6ff22":"markdown","51bf5feb":"markdown"},"source":{"3cc99b9d":"import numpy as np #linear algebra\nimport pandas as pd # data processing\n\nimport os\nfor dirname, _ , filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","0c722573":"from transformers import BertTokenizer, TFBertModel\nimport matplotlib.pyplot as plt\nimport tensorflow as tf","7338c8d3":"os.environ[\"WANDB_API_KEY\"] = \"0\" ## to silence warning","818a6e42":"# Setting up TPU\n\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nexcept ValueError:\n    strategy = tf.distribute.get_strategy() # for CPU and single GPU\n    print('Number of replicas:', strategy.num_replicas_in_sync)","a8b28ff5":"# Downloading Data\n# The training set contains a premise, a hypothesis, a label (0 = entailment, 1 = neutral, 2 = contradiction), and the language of the text.\n\ntrain = pd.read_csv(\"..\/input\/contradictory-my-dear-watson\/train.csv\")","12ce2836":"train.head()","ea18fba9":"train.premise.values[1]","51ea926d":"train.hypothesis.values[1]","c6fcc493":"train.label.values[1]","4e15f093":"# look at the distribution of languages in the training set.\n\nlabels, frequencies = np.unique(train.language.values, return_counts = True)\n\nplt.figure(figsize = (10,10))\nplt.pie(frequencies, labels = labels, autopct = \"%1.1f%%\")\nplt.show()","593d55a0":"# Preparing Data for Input\n# To start out, we can use a pretrained model. Here, we'll use a multilingual\n# BERT model from huggingface. \n# More info at: https:\/\/github.com\/google-research\/bert\/blob\/master\/multilingual.md\n\nmodel_name = 'bert-base-multilingual-cased'\ntokenizer = BertTokenizer.from_pretrained(model_name)","1adc707f":"# Tokenizers turn sequences of words into arrays of numbers. Let's look at an example:\n\ndef encode_sentence(s):\n    tokens = list(tokenizer.tokenize(s))\n    tokens.append('[SEP]')\n    return tokenizer.convert_tokens_to_ids(tokens)","678e3dbf":"encode_sentence(\"I love natural language processing\")","d7f3d7db":"def bert_encode(hypotheses, premises, tokenzier):\n    \n    num_examples = len(hypotheses)\n    \n    sentence1 = tf.ragged.constant([encode_sentence(s) for s in np.array(hypotheses)])\n    sentence2 = tf.ragged.constant([encode_sentence(s) for s in np.array(premises)])\n    \n    cls = [tokenizer.convert_tokens_to_ids(['[CLS]'])]*sentence1.shape[0]\n    input_word_ids = tf.concat([cls, sentence1, sentence2], axis=-1)\n    \n    input_mask = tf.ones_like(input_word_ids).to_tensor()\n    \n    type_cls = tf.zeros_like(cls)\n    type_s1 = tf.zeros_like(sentence1)\n    type_s2 = tf.ones_like(sentence2)\n\n    input_type_ids = tf.concat([type_cls, type_s1, type_s2], axis=-1).to_tensor()\n    \n    inputs = {'input_word_ids' : input_word_ids.to_tensor(),\n             'input_mask': input_mask,\n             'input_type_ids': input_type_ids}\n    \n    return inputs","857d6538":"train_input = bert_encode(train.premise.values, train.hypothesis.values, tokenizer)","6332bc39":"max_len = 50\n\ndef build_model():\n    bert_encoder = TFBertModel.from_pretrained(model_name)\n    input_word_ids = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n    input_mask = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n    input_type_ids = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"input_type_ids\")\n    \n    embedding = bert_encoder([input_word_ids, input_mask, input_type_ids])[0]\n    output = tf.keras.layers.Dense(3, activation='softmax')(embedding[:,0,:])\n    \n    model = tf.keras.Model(inputs=[input_word_ids, input_mask, input_type_ids], outputs=output)\n    model.compile(tf.keras.optimizers.Adam(lr=1e-5), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n    \n    return model","6d03f97c":"with strategy.scope():\n    model = build_model()\n    model.summary()","efa7f612":"model.fit(train_input, train.label.values, epochs=5, verbose=1, batch_size=64, validation_split=0.2)","aa22dc22":"test = pd.read_csv(\"..\/input\/contradictory-my-dear-watson\/test.csv\")\ntest_input = bert_encode(test.premise.values, test.hypothesis.values, tokenizer)","dbf9c816":"test.head()","e7c61e4f":"predictions = [np.argmax(i) for i in model.predict(test_input)]\n\nsubmission = test.id.copy().to_frame()\nsubmission['prediction'] = predictions","34ef529b":"submission.head()","38851bd8":"# submission file generated for the competition\nsubmission.to_csv(\"submission.csv\", index = False)","868d5eb3":"BERT uses three kind of input data - input word IDs, input masks and input typoe IDs.\n\nThese allow the model to know that the premise and hypothesis are distinct sentences and also to ignore any padding from the tokenizer.\n\nWe add a [CLS] token to denote the beginning of the inputs, and a [SEP] token to denote the separation between the premise and the hypothesis. We also need to pad all of the inputs to be the same size. For more information about BERT inputs, see: https:\/\/huggingface.co\/transformers\/model_doc\/bert.html#tfbertmodel\n\nNow, we're going to encode all of our premise\/hypothesis pairs for input into BERT.","93ca1674":"**Creating & Training Model**\n\nNow, we can incorporate the BERT transformer into a Keras Functional Model. For more information about the Keras Functional API, see: https:\/\/www.tensorflow.org\/guide\/keras\/functional.\n\nThis model was inspired by the model in this notebook: https:\/\/www.kaggle.com\/tanulsingh077\/deep-learning-for-nlp-zero-to-transformers-bert#BERT-and-Its-Implementation-on-this-Competition, which is a wonderful introduction to NLP!","34c6ff22":"**Natural Language Inferencing (NLI)**\n\nNatural language Inferencing is a classic NLP (Natural Language Processing) problem that involves taking two sentences (the premise and the hypothesis), and deciding how they are related - If the premise entails the hypothesis, contradicts it, or neither\n\nDataset - Contradictory, My Dear Watson\n\nTools - Tensorflow 2, Keras and BERT","51bf5feb":"**Generating & Submitting Predictions**"}}