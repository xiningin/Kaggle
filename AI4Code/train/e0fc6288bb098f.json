{"cell_type":{"a1eaac9e":"code","fa7780bf":"code","6403c03c":"code","39ee97e7":"code","9ef28c96":"code","fcfb785b":"code","2a6ca0dc":"code","2b12d300":"code","e0fd4216":"code","004f6b23":"code","02bb6a6a":"code","d6e27955":"code","8e4b202d":"code","c0d5b710":"code","6f10816a":"code","9a6b9817":"code","96a06a2f":"code","82f0f8f9":"code","59b03443":"code","396affb6":"code","841fa5ad":"code","b43ed6d9":"code","55daf413":"code","38b255c5":"code","ba3ccdcb":"code","a9311534":"code","5dfa86a2":"code","7049511a":"code","3d534caf":"code","e75c785a":"code","23013f27":"code","e4655ef4":"code","bf8bf379":"code","9708e63d":"code","f8e27fe5":"code","ffce9c5b":"markdown","cee98a91":"markdown","0ec06657":"markdown","ab57d31e":"markdown","0ae5a40a":"markdown","dadf0ec7":"markdown","ce112ac4":"markdown","1389f442":"markdown","e490e97d":"markdown","00e8bd8f":"markdown","d08d8674":"markdown","7600f3d4":"markdown"},"source":{"a1eaac9e":"# import required libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom collections import Counter\nimport re\npd.set_option('display.max_colwidth', -1)\nfrom nltk.corpus import stopwords\nfrom nltk.stem.wordnet import WordNetLemmatizer","fa7780bf":"# read the data from drive\ndf = pd.read_json('News_Category_Dataset_v2.json', lines=True)\ndf.head()","6403c03c":"# descriptive analysis of the dataset\ndf.describe().T","39ee97e7":"df.drop_duplicates(inplace=True)","9ef28c96":"# check for missing values if any\ndf.isna().sum()","fcfb785b":"df1 = df.copy()\ndf1.head()","2a6ca0dc":"# df1.aut_name + ' ' + \ndf1['text'] = (df1.headline + ' ' + df1.short_description)\ndf1 = df1[(df1.text != ' ') | (df1.text != '')]","2b12d300":"# Removing Punctuation\ndf1['text'] = df1['text'].str.replace('[^\\w\\s]','')","e0fd4216":"df1.text = [x.lower() for x in df1.text]","004f6b23":"df1.category[df1.category=='THE WORLDPOST'] = 'WORLDPOST'\ndf1.category[df1.category=='GREEN'] = 'ENVIRONMENT'\ndf1.category[df1.category=='CULTURE & ARTS'] = 'ARTS'\ndf1.category[df1.category=='COMEDY'] = 'ENTERTAINMENT'\ndf1.category[(df1.category=='BLACK VOICES') | (df1.category=='LATINO VOICES') | (df1.category=='QUEER VOICES')] = 'VOICES'\ndf1.category[df1.category=='STYLE'] = 'STYLE & BEAUTY'\ndf1.category[df1.category=='ARTS & CULTURE'] = 'ARTS'\ndf1.category[df1.category=='COLLEGE'] = 'EDUCATION'\ndf1.category[df1.category=='SCIENCE'] = 'TECH'\ndf1.category[df1.category=='WEDDINGS'] = 'GOOD NEWS'\ndf1.category[df1.category=='TASTE'] = 'FOOD & DRINK'\ndf1.category[(df1.category=='PARENTING') | (df1.category=='FIFTY')] = 'PARENTS'\ndf1.category[df1.category=='WORLD NEWS'] = 'WORLDPOST'\n","02bb6a6a":"df1.head()","d6e27955":"# distribution of categories in dataset\nplt.figure(figsize=(16,8))\nsns.countplot(df1.category, order=df1.category.value_counts().index, color='c')\nplt.xticks(rotation=90)\nplt.xlabel('Category',fontsize=12)\nplt.ylabel('Count', fontsize=12)\nplt.title('Distribution of Categories', fontsize=15)\nplt.show()","8e4b202d":"#Popular category per month\n# a = df1.groupby(pd.Grouper(key='date', freq='M'))['category'].agg(lambda x:x.value_counts().index[0])\na = df1.category.value_counts()\n\nimport squarify\nimport matplotlib\n\nMEDIUM_SIZE = 12.5\nBIGGER_SIZE = 23\n\nplt.rc('font', size=MEDIUM_SIZE)\nplt.rc('figure', titlesize=BIGGER_SIZE)\n# #Utilise matplotlib to scale our goal numbers between the min and max, then assign this scale to our values.\n\nnorm = matplotlib.colors.Normalize(vmin=a.values.min(), vmax=a.values.max())\ncolors = [matplotlib.cm.Blues(norm(value)) for value in a.values]\n\nlbl= np.array(a.index)+ \" \\n \" + a.values.astype(\"str\")\n\nplt.figure(figsize=(12,8))\nsquarify.plot(sizes=a.values[:20], label=lbl[0:20], alpha=0.7, color=colors)\nplt.axis('off')\nplt.title(\"News Category TreeMap\")\nplt.tight_layout()\nplt.show()\n\n","c0d5b710":"all_words = ' '.join([text for text in df1['text']])","6f10816a":"from wordcloud import WordCloud\n\nplt.figure(figsize=(12,10))\nwordcloud = WordCloud(width=800, height=500, random_state=21, \n                      max_font_size=110, background_color='white').generate(all_words)\n\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis('off')\nplt.show()","9a6b9817":"from sklearn.feature_extraction.text import CountVectorizer, ENGLISH_STOP_WORDS, TfidfVectorizer\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\nfrom sklearn.metrics import confusion_matrix, classification_report, cohen_kappa_score","96a06a2f":"vect = CountVectorizer(min_df=5, stop_words='english')\nX = vect.fit_transform(df1.text)","82f0f8f9":"le = LabelEncoder()\ny = le.fit_transform(df1.category)","59b03443":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=12)","396affb6":"nb = MultinomialNB()\nnb.fit(X_train, y_train)","841fa5ad":"y_pred = nb.predict(X_test)\nprint(f'train score: {nb.score(X_train, y_train):.4f}')\nprint(f'test score: {nb.score(X_test, y_test):.4f}')","b43ed6d9":"print('KAPPA SCORE: ',cohen_kappa_score(y_test,y_pred))","55daf413":"def predict_cat(title):\n    stop = stopwords.words('english')\n    if title:\n        title = title.replace('[^\\w\\s]','')\n        title = ' '.join(x for x in  title.split(' ') if x not in stop)\n        cod = nb.predict(vect.transform([title]))\n        return le.inverse_transform(cod)[0]\n    else:\n        print('text cannot be blank')\n","38b255c5":"predict_cat(\"India\u2019s largest ever \u2018eye in the sky\u2019 will take on its neighbours\")","ba3ccdcb":"%%time\nsvc = SVC(kernel='linear')\nsvc.fit(X_train, y_train)","a9311534":"y_pred = svc.predict(X_test)\nprint(f'train score: {svc.score(X_train, y_train):.4f}')\nprint(f'test score: {svc.score(X_test, y_test):.4f}')","5dfa86a2":"logreg = LogisticRegression()\nlogreg.fit(X_train, y_train)\n","7049511a":"y_pred_logreg = logreg.predict(X_test)\nprint(f'train score: {logreg.score(X_train, y_train):.4f}')\nprint(f'test score: {logreg.score(X_test, y_test):.4f}')","3d534caf":"print('KAPPA SCORE: ',cohen_kappa_score(y_test,y_pred_logreg))","e75c785a":"tfidf = TfidfVectorizer()\nX = tfidf.fit_transform(df1.text)","23013f27":"le = LabelEncoder()\ny = le.fit_transform(df1.category)","e4655ef4":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=15)","bf8bf379":"nb_tfidf = MultinomialNB()\nnb_tfidf.fit(X_train, y_train)","9708e63d":"%%time\nsvc = SVC(kernel='linear')\nsvc.fit(X_train, y_train)y_pred = svc.predict(X_test)\nprint(f'train score: {svc.score(X_train, y_train):.4f}')\nprint(f'test score: {svc.score(X_test, y_test):.4f}')y_pred_tfidf = nb_tfidf.predict(X_test)\nnb_tfidf.score(X_test, y_test)","f8e27fe5":"print('KAPPA SCORE: ',cohen_kappa_score(y_test,y_pred_tfidf))","ffce9c5b":"cm_nb = confusion_matrix(y_test, y_pred)\nplt.figure(figsize=(10,8))\nax = sns.heatmap(cm_nb, annot=True, fmt='0.0f', annot_kws={'fontsize':12})\nax.xaxis.set_ticklabels(df1.category.unique(), rotation=10, fontsize=12)\nax.yaxis.set_ticklabels(df1.category.unique(), rotation=0, fontsize=12)\nplt.show()","cee98a91":"## EDA","0ec06657":"### Naive Bayes","ab57d31e":"# nb_gs = MultinomialNB().fit(X_train, y_train)\n\n\nfrom sklearn.pipeline import Pipeline\n\ntext_clf = Pipeline([('vect', CountVectorizer()),\n                     ('tfidf', TfidfTransformer()),\n                     ('clf', MultinomialNB())])\n\ntext_clf = text_clf.fit(X_train, y_train)\n\n\nparameters = {'vect__ngram_range': [(1, 1), (1, 2)],\n              'tfidf__use_idf': (True, False),\n              'clf__alpha': (1e-2, 1e-3)}\n\n# nb_gs = MultinomialNB()\ngs_nb = GridSearchCV(text_clf, parameters, n_jobs=-1)\ngs_nb = gs_nb.fit(X_train, y_train)\ngs_nb.best_score_\ngs_nb.best_params_","0ae5a40a":"### Count Vectorizer","dadf0ec7":"### Logistic Regression","ce112ac4":"### Naive Bayes with TF-IDF Vectorizer","1389f442":"### GridSearchCV","e490e97d":"print(classification_report(y_test, y_pred))","00e8bd8f":"### SVM","d08d8674":"### TF-IDF Vectorizer","7600f3d4":" The dataset timeline starts at 28-01-2012 and ends at 26-05-2018"}}