{"cell_type":{"d0aaf12c":"code","33f02684":"code","deba0bb7":"code","b2f3dd52":"code","c44c0b61":"code","fd80555f":"code","77421ee6":"code","400ba100":"code","4e419d9d":"code","84eace86":"code","b049963b":"code","ef391ca4":"markdown","9114a047":"markdown"},"source":{"d0aaf12c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\nimport pandas as pd\nimport numpy as np\nimport math\npd.options.display.max_columns = 999\npd.options.display.max_rows = 112\nfrom pandas_profiling import ProfileReport\nimport matplotlib.pyplot as plt\nimport missingno as msno\nimport seaborn as sn","33f02684":"\"\"\" \"\"\"\ndf00= pd.read_excel(\"\/kaggle\/input\/covid19\/dataset.xlsx\")\n\ndf00.reset_index(drop=True, inplace=True)\ndf00.rename(columns={\"Patient ID\":\"id\",\"SARS-Cov-2 exam result\":\"target\"},inplace=True)\ndf00['target'].replace(\"negative\",0,inplace=True)\ndf00['target'].replace(\"positive\",1,inplace=True)\n\ndf01_missing = df00.isnull().sum()\n\ndf00.reset_index(drop=True, inplace=True)\ndf00.rename(columns={\"Patient ID\":\"id\",\"SARS-Cov-2 exam result\":\"target\"},inplace=True)\ndf00['target'].replace(\"negative\",0,inplace=True)\ndf00['target'].replace(\"positive\",1,inplace=True)\n\ndf01_missing = df00.isnull().sum()\nqt_rows = df00.shape[0]\n\n#Creating a missing count df\n# Gerando data frame com quantidade de missings por variavel\ndf_pct_missing = pd.DataFrame(df01_missing,columns=['qt_missing'])\ndf_pct_missing = pd.DataFrame(df01_missing,columns=['qt_missing'])\ndf_pct_missing['Features'] = df_pct_missing.index\ndf_pct_missing['pc_miss'] = (100*df_pct_missing['qt_missing'].divide(qt_rows)).astype(int)\ndf_pct_missing['qt_rows'] = qt_rows\ndf_pct_missing.reset_index(drop = True, inplace = True)\ndf_pct_missing = df_pct_missing[[\"Features\",\"qt_rows\",\"qt_missing\",\"pc_miss\"]]\n\ndf01 = df00.copy()\n\n\n#drop uselles features for now\ndf01.drop(['id', 'Patient age quantile', 'target',\n       'Patient addmited to regular ward (1=yes, 0=no)',\n       'Patient addmited to semi-intensive unit (1=yes, 0=no)',\n       'Patient addmited to intensive care unit (1=yes, 0=no)'],axis=1,inplace=True)\n\ndf01['soma'] = df01.count(axis=1)\n\ndf01 = df01[df01['soma']>1]\n\ndf01.reset_index(inplace=True,drop=False)\n\ndf01 = df01['index'].to_frame()\ndf00.reset_index(drop=False, inplace=True)\ndf00 = pd.merge(df01,df00, how=\"left\", on=\"index\")\ndf00.set_index(\"index\", inplace=True)\n\n#This function is not perfect for this but will help our analisys\n#vamos verificar os metadados com o aux\u00edlio dessa fun\u00e7\u00e3o\ndef AjusteMetadados(dataframe): \n\n    train = dataframe\n    # Verifica os tipos de vari\u00e1veis presentes na tabela de treino\n    t = []\n    for i in train.columns:\n            t.append(train[i].dtype)\n\n    n = []\n    for i in train.columns:\n            n.append(i)\n\n    aux_t = pd.DataFrame(data=t,columns=[\"Tipos\"])\n    aux_n = pd.DataFrame(data=n,columns=[\"Features\"])\n    df_tipovars = pd.concat([aux_n, aux_t], axis=1, join_axes=[aux_n.index])\n\n    data = []\n    for f in train.columns:\n\n        # Definindo o papel das vari\u00e1veis:\n        if f == 'target':\n            role = 'target'\n        elif f == 'id':\n            role = 'id'\n        else:\n            role = 'input'\n        # Definindo o tipo das vari\u00e1veis: nominal, ordinal, binary ou interval\n        if f == 'target':\n            level = 'binary'\n        elif train[f].dtype == 'object' or f == 'id': \n            level = 'nominal'\n        elif train[f].dtype in ['float','float64'] :\n            level = 'interval'\n        elif train[f].dtype in ['int','int64'] :\n            level = 'ordinal'\n\n        # Todas vari\u00e1veis s\u00e3o incializadas com keep exceto o id\n        keep = True\n        if f == 'id':\n            keep = False\n\n        # Definindo o tipo das vari\u00e1veis da tabela de entrada\n        dtype = train[f].dtype\n\n        # Criando a lista com todo metadados\n        f_dict = {\n            'Features': f,\n            'Role': role,\n            'Level': level,\n            'Keep': keep,\n            'Tipo': dtype\n        }\n        data.append(f_dict)\n\n    meta = pd.DataFrame(data, columns=['Features', 'Role', 'Level', 'Keep', 'Tipo'])\n\n    # Quantidade de dom\u00ednios distintos para cada cari\u00e1vel do tipo ordinal e nominal\n    card = []\n\n    v = train.columns\n    for f in v:\n        dist_values = train[f].value_counts().shape[0]\n        f_dict = {\n                'Features': f,\n                'Cardinality': dist_values\n            }\n        card.append(f_dict)\n\n    card = pd.DataFrame(card, columns=['Features', 'Cardinality'])\n\n    metadados_train = pd.merge(meta, card, on='Features')\n\n    return metadados_train \n\nmetadados = AjusteMetadados(df00)\nmetadados = metadados[metadados['Cardinality']>0]\n\nvars_nominais = metadados[(metadados.Level  == 'nominal') & (metadados.Role == 'input')]\nvars_nominais_df = df00[vars_nominais['Features']]\n\n#DATAFRAME + NUMERO DE LINHAS\n#DEVOLVE UM DF COM OS VALORES \u00daNICOS POR FEATURE EM UM DF\ndef unique_df(df,x):    \n    rows = np.arange(x).tolist()\n    unique = pd.DataFrame(rows,columns={\"index\"})\n    for i in df.columns:\n        u = df[i].unique()\n        a = pd.DataFrame(u,columns=[i])\n        try:\n            a.sort_values(i,inplace=True,ascending=True)\n            a.reset_index(drop=True,inplace=True)\n        except:\n            a = a\n        unique=unique.merge(a,how=\"left\",left_index=True,right_index=True)\n    unique.fillna(\"\",inplace=True)\n    unique.drop(\"index\",axis=1,inplace=True)\n    return unique\n\nunique_df(df00[vars_nominais_df.columns],5)\n\ndf00['Urine - Esterase'].replace('not_done', np.nan, inplace=True)\ndf00['Urine - pH'].replace('N\u00e3o Realizado', np.nan, inplace=True)\ndf00['Urine - Hemoglobin'].replace('not_done', np.nan, inplace=True)\ndf00['Urine - Bile pigments'].replace('not_done', np.nan, inplace=True)\ndf00['Urine - Ketone Bodies'].replace('not_done', np.nan, inplace=True)\ndf00['Urine - Nitrite'].replace('not_done', np.nan, inplace=True)\ndf00['Urine - Urobilinogen'].replace('not_done', np.nan, inplace=True)\ndf00['Urine - Protein'].replace('not_done', np.nan, inplace=True)\ndf00['Urine - Leukocytes'].replace(\"<1000\", \"900\", inplace=True)\n\nunique_df(df00[vars_nominais_df.columns],3)\ndf00['Urine - pH'] = df00['Urine - pH'].astype(\"float64\")\ndf00['Urine - Leukocytes'] = df00['Urine - Leukocytes'].astype(\"float64\")\n\nmetadados = AjusteMetadados(df00)\nvars_nominais = metadados[(metadados.Level  == 'nominal') & (metadados.Role == 'input')]\nnom = df00[vars_nominais['Features'].tolist()]\n\ndf_nom = pd.get_dummies(\n                        nom,\n                        prefix=nom.columns,\n                        prefix_sep='_',\n                        dummy_na=False,\n                        columns=nom.columns,\n                        sparse=False,\n                        drop_first=False,\n                        dtype=None,\n                        )\n\nvars_ordinais = metadados[(metadados.Level  == 'ordinal') & (metadados.Role == 'input')]\nord_ = df00['Patient age quantile'].to_frame()\n\nvars_interval = metadados[(metadados.Level  == 'interval') & (metadados.Role == 'input')]\nvars_interval_bc = vars_interval[vars_interval['Cardinality']<10]\ninterval_bc=df00[vars_interval_bc['Features'].tolist()]\n\ninterval_bc.drop([\"Fio2 (venous blood gas analysis)\",\"Myeloblasts\"],axis=1,inplace=True)\ninterval_bc.fillna(0,inplace=True)\n\nvars_interval_ac = vars_interval[vars_interval['Cardinality']>=10]\ninterval_ac=df00[vars_interval_ac['Features'].tolist()]\ninterval_ac.fillna(0,inplace=True)\n\nnormalizar_ = pd.merge(ord_,interval_ac,right_index=True,left_index=True)\nnormalizar = pd.merge(normalizar_,interval_bc,right_index=True,left_index=True)\n\nfrom sklearn.preprocessing import MinMaxScaler\n\n# Classe respon\u00e1vel pela normaliza\u00e7\u00e3o\nsc = MinMaxScaler(feature_range=(0, 1))\n\nscaled_features = sc.fit_transform(normalizar)\nnum_normalizadas = pd.DataFrame(scaled_features, columns=normalizar.columns)\n\ndf_nom.reset_index(drop=True,inplace=True)\ndf00.reset_index(inplace=True,drop=True)\nid_=df00['id'].to_frame()\ntarget=df00['target'].to_frame()\n\nabt_ = pd.merge(df_nom,num_normalizadas, left_index=True,right_index=True)\nabt__ = pd.merge(id_,abt_, left_index=True,right_index=True)\nabt = pd.merge(abt__,target,left_index=True,right_index=True)\n\ndf00 = abt.copy()\ndf00.rename(columns={\"Urine - Crystals_Oxalato de C\u00e1lcio +++\":\"Urine_Crystals_Oxalato_de_Calcio_PPP\",\n                     \"Urine - Crystals_Oxalato de C\u00e1lcio -++\":\"Urine_Crystals_Oxalatode_Calcio_NPP\",\n                    \"Urine - Crystals_Urato Amorfo +++\":\"Urine - Crystals_Urato_Amorfo_PPP\",\n                    \"Urine - Crystals_Urato Amorfo --+\":\"Urine - Crystals_Urato_Amorfo_NNP\"},inplace=True)\n\ndf00.columns = [\"\".join (c if c.isalnum() else \"_\" for c in str(x)) for x in df00.columns]\ndf00.drop(\"id\",inplace=True,axis=1)\n\n\nfrom sklearn.model_selection import train_test_split\nexplicativas = df00.drop(['target'], axis=1)\nresposta = df00[\"target\"]\n\nx_train, x_test, y_train, y_test = train_test_split(explicativas, resposta, test_size = 0.2, random_state = 0)\n\nfrom lightgbm import LGBMClassifier\nlgbm=LGBMClassifier(boosting_type='gbdt',\n                        num_leaves=10,\n                        max_depth=1,\n                        learning_rate=0.05,\n                        n_estimators=200,\n                        subsample_for_bin=200000,\n                        objective='binary',\n                        class_weight=None,\n                        is_unbalance = True,\n                        min_split_gain=1,\n                        min_child_weight=0.0001,\n                        min_child_samples=1,\n                        subsample=1.0, \n                        subsample_freq=0,\n                        colsample_bytree=1.0, \n                        reg_alpha=0.0,\n                        reg_lambda=0.0, \n                        random_state=37,\n                        n_jobs=3,\n                        silent=True,\n                        importance_type='gain'\n                        )\n\nlgbm.fit(x_train, y_train)\n\nfi = pd.DataFrame(lgbm.feature_importances_, columns={\"importances\"})\n\n#normalizar a import\u00e2ncia das vari\u00e1veis\nfrom sklearn.preprocessing import MinMaxScaler\n\n# Classe respon\u00e1vel pela normaliza\u00e7\u00e3o\nscf = MinMaxScaler(feature_range=(0, 1))\n\nscaled_features = scf.fit_transform(fi)\nfi = pd.DataFrame(scaled_features, columns=fi.columns)\n\nfeat = pd.DataFrame(df00.columns,columns={\"Features\"})\nFeat_imp = pd.merge(feat,fi,left_index=True,right_index=True)\nFeat_imp=Feat_imp.sort_values(\"importances\",ascending=False)\n\n\nfrom sklearn.model_selection import GridSearchCV\nfrom lightgbm import LGBMClassifier\n\nparametros = {'num_leaves':[20,30,40],\n              'learning_rate':[0.1],\n              'n_estimators':[150],\n              'n_jobs':[4],\n              'max_depth':[1],\n              'verbose':[0]}\n\nGS_LGBM = GridSearchCV(estimator=LGBMClassifier(),\n             param_grid=parametros, scoring='roc_auc', verbose=0)\n\nGS_LGBM.fit(x_train,y_train)\n\nfrom lightgbm import LGBMClassifier\nfrom sklearn.feature_selection import RFE\n\n#vamos utilizar os par\u00e2metros encontrados pelo Grid Search\n#GridSearch best parameter\nlgbm2=GS_LGBM.best_estimator_\n\nfi_2 = pd.DataFrame(lgbm2.feature_importances_, columns={\"importances\"})\n\n#normalizar a import\u00e2ncia das vari\u00e1veis\nfrom sklearn.preprocessing import MinMaxScaler\n\n# Classe respon\u00e1vel pela normaliza\u00e7\u00e3o\nscf2 = MinMaxScaler(feature_range=(0, 1))\n\nscaled_features = scf2.fit_transform(fi)\nfi_2 = pd.DataFrame(scaled_features, columns=fi_2.columns)\n\nfeat2 = pd.DataFrame(df00.columns,columns={\"Features\"})\n\nFeat_imp2 = pd.merge(feat2,fi_2,left_index=True,right_index=True)\n\nFeat_imp2=Feat_imp.sort_values(\"importances\",ascending=False)\n","deba0bb7":"\"\"\" Now this is our code. Deep learning applied to the data pre-processed using the previous code \"\"\"","b2f3dd52":"\"\"\" Get only the 20 most significant features \"\"\"\nxns = list(Feat_imp2.head(20)['Features'])\nxx_train = x_train[xns]\nxx_test = x_test[xns]\n\n\"\"\" Create training\/validation datasets \"\"\"\nxx_train = {'in': np.array(xx_train.to_numpy())}\nxx_test = {'in': np.array(xx_test.to_numpy())}\nyy_train = {'out': y_train.to_numpy()}\nyy_test = {'out': y_test.to_numpy()}","c44c0b61":"\"\"\" Build our deep dense model \"\"\" \nmodel.compile('adam', loss = LOSSES, metrics = METRICS)\n\nfrom tensorflow.keras import layers, models\n\ninputs = xx_train\noutputs = yy_train\n\nresidual = False\nunits = [32] #[64, 128, 256, 512]\nreps = [1] #[2, 2, 3, 3]\ndropout_rate = .1\nepochs = 200\nbatch_size = 10\n\nfrom keras import backend as K\ndef weighted_binary_crossentropy( y_true, y_pred, weight=1. ) :\n    y_true = K.clip(K.cast(y_true,tf.float32), K.epsilon(), 1.-K.epsilon())\n    y_pred = K.clip(y_pred, K.epsilon(), 1.-K.epsilon())\n    logloss = -(y_true * K.log(y_pred) * weight + (1. - y_true) * K.log(1. - y_pred))\n    return K.mean( logloss, axis=-1)\n\nINPUTS = {}\nINPUT_STREAM = []\nfor xn in inputs:\n    ip = layers.Input(shape = inputs[xn].shape[1:],\n                      name = xn)\n\n    x = ip\n    for i,(u,r) in enumerate(zip(units,reps)):\n\n        for _r in range(r):\n            res = x\n            if i != 0:\n                x = layers.Dropout(dropout_rate)(x)\n            x = layers.Dense(u)(x)\n            x = layers.LeakyReLU(alpha=.2)(x)\n            x = layers.BatchNormalization()(x)\n            if residual and _r != 0:\n                x = layers.Add()([x,res])\n\n    INPUTS[xn] = ip\n    INPUT_STREAM.append(x)\n\n\"\"\" CORE \"\"\"\nif len(INPUT_STREAM) > 1:\n    CORE_STREAM = layers.Concatenate(axis=1)(INPUT_STREAM)\nelse:\n    CORE_STREAM = INPUT_STREAM[0]\n\n\"\"\" BOTTLENECK \"\"\"\nbottleneck_units = []\nfor u in bottleneck_units:\n    CORE_STREAM = layers.Dropout(dropout_rate)(CORE_STREAM)\n    CORE_STREAM = layers.Dense(u)(CORE_STREAM)\n    CORE_STREAM = layers.LeakyReLU(alpha=.2)(CORE_STREAM)\n    CORE_STREAM = layers.BatchNormalization()(CORE_STREAM)\n\n\"\"\" Out \"\"\"\nOUTPUTS = {}\nLOSSES = {}\nMETRICS = {}\nfor yn in outputs:\n    y = layers.Dense(1)(CORE_STREAM)\n    y = layers.Activation('sigmoid', name = yn)(y)\n    OUTPUTS[yn] = y\n    LOSSES[yn] = weighted_binary_crossentropy#'binary_crossentropy'\n    METRICS[yn] = ['accuracy']\n\n\"\"\" Build & Compile model \"\"\"\nmodel = models.Model([INPUTS[xn] for xn in INPUTS],\n                     [OUTPUTS[yn] for yn in OUTPUTS])\n\n\n\"\"\" compile \"\"\"\nmodel.compile('adam', loss = LOSSES, metrics = METRICS)","fd80555f":"\"\"\" Train \"\"\"\nfit_history = model.fit(xx_train, yy_train,\n                        validation_data = (xx_test, yy_test),\n                        epochs = epochs,\n                        batch_size = batch_size,\n                        verbose = True)\n","77421ee6":"\"\"\" Plot training history \"\"\"\nmetrics = np.unique([ln.replace('val_','') for ln in fit_history.history])\nfig = plt.figure(figsize=(10*len(metrics),10))\nfor i,ln in enumerate(metrics):\n    plt.subplot(1,len(metrics),i+1)\n    plt.plot(fit_history.epoch, fit_history.history[ln] if 'loss' not in ln \\\n        else np.array(fit_history.history[ln]),\n             label='train')\n    if 'val_'+ln in fit_history.history:\n        plt.plot(fit_history.epoch, fit_history.history['val_'+ln],\n                 label = 'validation')\n    plt.legend()\n    if 'acc' in ln:\n        plt.ylim([0,1])\nplt.show()\n#fig.savefig('{}\/losses.png'.format(out_dir))","400ba100":" \"\"\" Predict data \"\"\"\nyy_train_pred = model.predict(xx_train, verbose = True)\nif not isinstance(yy_train_pred,list):\n    yy_train_pred = [yy_train_pred]\nif len(yy_train_pred) != len(outputs):\n    yy_train_pred = [yy_train_pred]\nif not isinstance(yy_train_pred,dict):\n    yy_train_pred = {yn: yy_train_pred[i] for i,yn in enumerate(model.output_names)}\n\nyy_test_pred = model.predict(xx_test, verbose = True)\nif not isinstance(yy_test_pred,list):\n    yy_test_pred = [yy_test_pred]\nif len(yy_test_pred) != len(outputs):\n    yy_test_pred = [yy_test_pred]\nif not isinstance(yy_test_pred,dict):\n    yy_test_pred = {yn: yy_test_pred[i] for i,yn in enumerate(model.output_names)}\n","4e419d9d":"\"\"\" ROCs \"\"\"\nfrom sklearn.metrics import roc_curve, roc_auc_score\n\nrocs_train = {yn: roc_curve(yy_train[yn], yy_train_pred[yn]) for yn in yy_train}\nrocs_val = {yn: roc_curve(yy_test[yn], yy_test_pred[yn]) for yn in yy_test}\n\n\n\"\"\" Calculate AUCs \"\"\"\naucs_train = {yn: roc_auc_score(yy_train[yn], yy_train_pred[yn]) for yn in yy_train}\naucs_val = {yn: roc_auc_score(yy_test[yn], yy_test_pred[yn]) for yn in yy_test}\n\n\n\"\"\" Calculate optimal thresholds \"\"\"\ndef optimal_point(fpr,tpr,thresholds):\n    imin = np.argmin((fpr)**2 + (1-tpr)**2)\n    return thresholds[imin], imin\n\nopt_thresholds_train = {yn: optimal_point(rocs_train[yn][0],\n                                          rocs_train[yn][1],\n                                          rocs_train[yn][2])\n                        for yn in rocs_train}\nopt_thresholds_val = {yn: optimal_point(rocs_val[yn][0],\n                                        rocs_val[yn][1],\n                                        rocs_val[yn][2])\n                      for yn in rocs_val}\n\n\n\"\"\" Plot \"\"\"\nfig = plt.figure(figsize = (10*2,10*len(rocs_train)))\nfor i,yn in enumerate(rocs_train):\n\n    \"\"\" Train ROC \"\"\"\n    plt.subplot(len(rocs_train), 2, 2*i + 1)\n    opt_thr, opt_i = opt_thresholds_train[yn]\n    plt.plot(rocs_train[yn][0], rocs_train[yn][1], label = 'infected (AUC = {})'.format(aucs_train[yn]))\n    #plt.plot(rocs_train[yn][0][0][opt_i], rocs_train[yn][0][1][opt_i], '.', markersize = 20)\n    #plt.plot(rocs_train[yn][1][0], rocs_train[yn][1], label='infected (AUC = {})'.format(aucs_train[yn][1]))\n    #plt.plot(rocs_train[yn][1][0][opt_i], rocs_train[yn][1][1][opt_i], '.', markersize = 20)\n    plt.plot([0,1],[0,1],'--')\n    plt.xlim([0,1])\n    plt.ylim([0,1])\n    plt.title('{} - Train'.format(yn))\n    plt.legend()\n\n\n    \"\"\" Val ROC \"\"\"\n    plt.subplot(len(rocs_train), 2, 2*i + 2)\n    opt_thr, opt_i = opt_thresholds_val[yn]\n    plt.plot(rocs_val[yn][0], rocs_val[yn][1], label = 'not_infected (AUC = {})'.format(aucs_val[yn]))\n    #plt.plot(rocs_val[yn][0][0][opt_i], rocs_val[yn][0][1][opt_i], '.', markersize = 20)\n    #plt.plot(rocs_val[yn][1][0], rocs_val[yn][1][1], label='infected (AUC = {})'.format(aucs_val[yn][1]))\n    #plt.plot(rocs_val[yn][1][0][opt_i], rocs_val[yn][1][1][opt_i], '.', markersize = 20)\n    plt.plot([0,1],[0,1],'--')\n    plt.xlim([0,1])\n    plt.ylim([0,1])\n    plt.title('{} - Val'.format(yn))\n    plt.legend()\n\nplt.show()\n#fig.savefig('{}\/rocs.png'.format(out_dir))","84eace86":"from sklearn.metrics import confusion_matrix\n\nCM_train = {yn: confusion_matrix(yy_train[yn], yy_train_pred[yn] > .5) #1.*(Y_pred[yn][:ntrain] == np.hstack((Y_pred[yn][:ntrain].max(1)[:,None],Y_pred[yn][:ntrain].max(1)[:,None]))))\n            for yn in yy_train}\nCM_val = {yn: confusion_matrix(yy_test[yn], yy_test_pred[yn] > .5) #1.*(Y_pred[yn][ntrain:] == np.hstack((Y_pred[yn][ntrain:].max(1)[:,None],Y_pred[yn][ntrain:].max(1)[:,None]))))\n            for yn in yy_test}\n\ndef plot_confusion_matrix(cm,\n                          target_names,\n                          title='Confusion matrix',\n                          cmap=None,\n                          normalize=True,\n                          filename = 'confusion_matrix.png'):\n    \"\"\"\n    given a sklearn confusion matrix (cm), make a nice plot\n\n    Arguments\n    ---------\n    cm:           confusion matrix from sklearn.metrics.confusion_matrix\n\n    target_names: given classification classes such as [0, 1, 2]\n                  the class names, for example: ['high', 'medium', 'low']\n\n    title:        the text to display at the top of the matrix\n\n    cmap:         the gradient of the values displayed from matplotlib.pyplot.cm\n                  see http:\/\/matplotlib.org\/examples\/color\/colormaps_reference.html\n                  plt.get_cmap('jet') or plt.cm.Blues\n\n    normalize:    If False, plot the raw numbers\n                  If True, plot the proportions\n\n    Usage\n    -----\n    plot_confusion_matrix(cm           = cm,                  # confusion matrix created by\n                                                              # sklearn.metrics.confusion_matrix\n                          normalize    = True,                # show proportions\n                          target_names = y_labels_vals,       # list of names of the classes\n                          title        = best_estimator_name) # title of graph\n\n    Citiation\n    ---------\n    http:\/\/scikit-learn.org\/stable\/auto_examples\/model_selection\/plot_confusion_matrix.html\n\n    \"\"\"\n    import matplotlib.pyplot as plt\n    import numpy as np\n    import itertools\n\n    accuracy = np.trace(cm) \/ float(np.sum(cm))\n    misclass = 1 - accuracy\n\n    if cmap is None:\n        cmap = plt.get_cmap('Blues')\n\n    fig = plt.figure(figsize=(8, 6))\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n\n    if target_names is not None:\n        tick_marks = np.arange(len(target_names))\n        plt.xticks(tick_marks, target_names, rotation=45)\n        plt.yticks(tick_marks, target_names)\n\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n\n\n    thresh = cm.max() \/ 1.5 if normalize else cm.max() \/ 2\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        if normalize:\n            plt.text(j, i, \"{:0.4f}\".format(cm[i, j]),\n                     horizontalalignment=\"center\",\n                     color=\"white\" if cm[i, j] > thresh else \"black\")\n        else:\n            plt.text(j, i, \"{:,}\".format(cm[i, j]),\n                     horizontalalignment=\"center\",\n                     color=\"white\" if cm[i, j] > thresh else \"black\")\n\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label\\naccuracy={:0.4f}; misclass={:0.4f}'.format(accuracy, misclass))\n    plt.show()\n    #fig.savefig(filename)\n\n\"\"\" Plot \"\"\"\nout_dir = '.'\n_ = [plot_confusion_matrix(CM_train[yn],\n                           normalize = False,\n                           target_names = ['not_infected','infected'],\n                           title = 'Train_{}'.format(yn),\n                           filename = '{}\/conf_matrix_{}_train.png'.format(out_dir,yn))\n     for yn in CM_train]\n_ = [plot_confusion_matrix(CM_val[yn],\n                           normalize = False,\n                           target_names = ['not_infected','infected'],\n                           title = 'Val_{}'.format(yn),\n                           filename = '{}\/conf_matrix_{}_val.png'.format(out_dir,yn))\n     for yn in CM_val]","b049963b":"# contact info: mbvalentin@cbpf.br","ef391ca4":"CBPF AI Team \n\nThe CBPF AI Team applies techniques of Artificial Intelligence in problems like Astrophysics, Astroparticles, Cosmology, Geophysics, Petrophysics and now for COVID19 forecasting. In CBPF, AI became an important tool to find gravitational lenses phenomenon or to characterize reservoirs and estimate, with higher precision, physical properties like permeability and absolute porosity for the oil and gas industry. \n\nThe CBPF Image Processing and Artificial Intelligence Research Group have developed a computer technology, in the Project called Sci.Mind. This new computer will provide the institution (and its collaborators) the cutting-edge technologies for AI exploration of scientific data. The main goal is to build a high-performance computer with small energy consumption and low noise that can be used in an office desk. This processing power is being used in several ways to increase the computational capacity in many areas of knowledge like scientific research, pattern recognition, signal and image processing, linear algebra, statistics analysis, 3Ds reconstructions, finance modeling, oil and gas exploration, etc.\n\nCBPF Kaggle Team\n\u2022\tManuel Blanco Valent\u00edn. \n\u2022\tPatrick Schubert de Souza Fuchs\n\u2022\tCl\u00e9cio R. De Bom \n\u2022\tMarcelo Portes de Albuquerque\n\u2022\tM\u00e1rcio Portes de Albuquerque","9114a047":"The following code for pre-processing the data was obtained from https:\/\/www.kaggle.com\/rodrigonanosys\/covid19-task1-gini-63-rocauc-81-acc-92"}}