{"cell_type":{"13d2f807":"code","c3821174":"code","be9de7ae":"code","27bc3a02":"code","b8c15421":"code","444d248b":"code","81c631ba":"code","6cf53d74":"code","141b7afd":"code","0618f21e":"code","62026cbb":"code","bdcdb1ef":"code","3110e001":"code","7d01a62c":"code","dedc13d3":"markdown","bc5b0dfb":"markdown"},"source":{"13d2f807":"!pip install transformers","c3821174":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\nfrom transformers import BertTokenizer,BertModel\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader,Dataset\nfrom torch.nn.utils.rnn import pack_padded_sequence\nfrom torch.optim import AdamW\nfrom tqdm import tqdm\nfrom argparse import ArgumentParser\nfrom ignite.engine import Events, create_supervised_trainer, create_supervised_evaluator\nfrom ignite.metrics import Accuracy, Loss\nfrom ignite.engine.engine import Engine, State, Events\nfrom ignite.handlers import EarlyStopping\nfrom ignite.contrib.handlers import TensorboardLogger, ProgressBar\nfrom ignite.utils import convert_tensor\nfrom torch.optim.lr_scheduler import ExponentialLR\nimport warnings  \nwarnings.filterwarnings('ignore')","be9de7ae":"def readfiles():\n    path = '\/kaggle\/input\/nlp-getting-started'\n    train = pd.read_csv(os.path.join(path,'train.csv'))\n    test = pd.read_csv(os.path.join(path,'test.csv'))\n    sample_subs = pd.read_csv(os.path.join(path,'sample_submission.csv'))\n    \n    return train,test,sample_subs\n\ntrain,test,sample_subs = readfiles()","27bc3a02":"from transformers import BertTokenizer\ndef Bert_Tokenizer(model_name):\n    device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n    tokenizer = BertTokenizer.from_pretrained(model_name)\n    return tokenizer\ntokenizer = Bert_Tokenizer('bert-base-uncased')","b8c15421":"class TextDataset(Dataset):\n    def __init__(self,df,tokenizer,max_len):\n        \n        self.bert_encode = tokenizer\n        self.texts = df.text.values\n        self.labels = df.target.values\n        self.max_len = max_len\n        \n    def __len__(self):\n        \n        return len(self.texts)\n    \n    def __getitem__(self,idx):\n        \n        tokens,mask,tokens_len = self.get_token_mask(self.texts[idx],self.max_len)\n        label = self.labels[idx]\n        return [torch.tensor(tokens),torch.tensor(mask),torch.tensor(tokens_len)],label\n        \n    def get_token_mask(self,text,max_len):\n        \n        tokens = []\n        mask = []\n        text = self.bert_encode.encode(text)\n        size = len(text)\n        pads = self.bert_encode.encode(['PAD']*(max(0,max_len-size)))\n        tokens[:max(max_len,size)] = text[:max(max_len,size)]\n        tokens = tokens + pads[1:-1]\n        mask = [1]*size+[0]*len(pads[1:-1])\n        tokens_len = len(tokens)\n        \n        return tokens,mask,tokens_len","444d248b":"def get_data_loaders():\n    from sklearn.model_selection import train_test_split\n    x_train , x_valid = train_test_split(train, test_size=0.1,random_state=2020)\n    train_dataset = TextDataset(x_train,tokenizer=tokenizer,max_len=120)\n    train_loader = torch.utils.data.DataLoader(train_dataset,batch_size=1,shuffle=True)\n    valid_dataset = TextDataset(x_valid,tokenizer=tokenizer,max_len=120)\n    valid_loader = torch.utils.data.DataLoader(valid_dataset,batch_size=1,shuffle=True)\n    \n    return train_loader , valid_loader","81c631ba":"class MixedBertModel(nn.Module):\n    def __init__(self,pre_trained='bert-base-uncased'):\n        super().__init__()\n        \n        self.bert = BertModel.from_pretrained(pre_trained)\n        self.hidden_size = self.bert.config.hidden_size\n        self.LSTM = nn.LSTM(self.hidden_size,self.hidden_size,bidirectional=True)\n        self.clf = nn.Linear(self.hidden_size*2,1)\n        \n    def forward(self,inputs):\n        \n        encoded_layers, pooled_output = self.bert(input_ids=inputs[0],attention_mask=inputs[1])\n        encoded_layers = encoded_layers.permute(1, 0, 2)\n        enc_hiddens, (last_hidden, last_cell) = self.LSTM(pack_padded_sequence(encoded_layers, inputs[2]))\n        output_hidden = torch.cat((last_hidden[0], last_hidden[1]), dim=1)\n        output_hidden = F.dropout(output_hidden,0.2)\n        output = self.clf(output_hidden)\n        \n        return F.sigmoid(output)","6cf53d74":"def _prepare_batch(batch, device=None, non_blocking=False):\n\n    x, y = batch\n    return (convert_tensor(x, device=device, non_blocking=non_blocking),\n            convert_tensor(y, device=device, non_blocking=non_blocking))\ndef create_supervised_trainer1(model, optimizer, loss_fn, metrics={}, device=None):\n\n    def _update(engine, batch):\n        model.train()\n        optimizer.zero_grad()\n        x, y = _prepare_batch(batch, device=device)\n        y_pred = model(x)\n        loss = loss_fn(y_pred, y.float())\n        loss.backward()\n        optimizer.step()\n        return loss.item(), y_pred, y\n\n    def _metrics_transform(output):\n        return output[1], output[2]\n\n    engine = Engine(_update)\n\n    for name, metric in metrics.items():\n        metric._output_transform = _metrics_transform\n        metric.attach(engine, name)\n\n    return engine\n\ndef create_supervised_evaluator1(model, metrics=None,\n                                device=None, non_blocking=False,\n                                prepare_batch=_prepare_batch,\n                                output_transform=lambda x, y, y_pred: (y_pred, y,)):\n\n    metrics = metrics or {}\n\n    if device:\n        model\n\n    def _inference(engine, batch):\n        model.eval()\n        with torch.no_grad():\n            x, y = prepare_batch(batch, device=device, non_blocking=non_blocking)\n            y_pred = model(x)\n            return output_transform(x, y.float(), y_pred)\n\n    engine = Engine(_inference)\n\n    for name, metric in metrics.items():\n        metric.attach(engine, name)\n\n    return engine","141b7afd":"def run(log_interval=100,epochs=2,lr=0.000006):\n    train_loader ,valid_loader = get_data_loaders()\n    model = MixedBertModel()\n    device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n\n    criterion = nn.BCELoss()\n    optimizer = AdamW(model.parameters(), lr=lr)\n    lr_scheduler = ExponentialLR(optimizer, gamma=0.90)\n    trainer = create_supervised_trainer1(model.to(device), optimizer, criterion, device=device)\n    evaluator = create_supervised_evaluator1(model.to(device), metrics={'BCELoss': Loss(criterion)}, device=device)\n\n    if log_interval is None:\n        e = Events.ITERATION_COMPLETED\n        log_interval = 1\n    else:\n        e = Events.ITERATION_COMPLETED(every=log_interval)\n        \n    desc = \"loss: {:.4f} | lr: {:.4f}\"\n    pbar = tqdm(\n        initial=0, leave=False, total=len(train_loader),\n        desc=desc.format(0, lr)\n    )\n    @trainer.on(e)\n    def log_training_loss(engine):\n        pbar.refresh()\n        lr = optimizer.param_groups[0]['lr']\n        pbar.desc = desc.format(engine.state.output[0], lr)\n        pbar.update(log_interval)\n        \n    @trainer.on(Events.EPOCH_COMPLETED)\n    def update_lr_scheduler(engine):\n        lr_scheduler.step()\n        \n\n            \n    @trainer.on(Events.EPOCH_COMPLETED)\n    def log_training_results(engine):\n        evaluator.run(train_loader)\n        metrics = evaluator.state.metrics\n        avg_loss = metrics['BCELoss']\n        tqdm.write(\n            \"Train Epoch: {} BCE loss: {:.2f}\".format(engine.state.epoch, avg_loss)\n        )\n    \n    @trainer.on(Events.EPOCH_COMPLETED)\n    def log_validation_results(engine):\n        pbar.refresh()\n        evaluator.run(valid_loader)\n        metrics = evaluator.state.metrics\n        avg_loss = metrics['BCELoss']\n        tqdm.write(\n            \"Valid Epoch: {} BCE loss: {:.2f}\".format(engine.state.epoch, avg_loss)\n        )\n        pbar.n = pbar.last_print_n = 0\n    \n    \n    try:\n        trainer.run(train_loader, max_epochs=epochs)\n    \n    except Exception as e:\n        import traceback\n        print(traceback.format_exc())\n    return model","0618f21e":"model =run()","62026cbb":"class TestTextDataset(Dataset):\n    def __init__(self,df,tokenizer,max_len):\n        \n        self.bert_encode = tokenizer\n        self.texts = df.text.values\n        self.max_len = max_len\n        \n    def __len__(self):\n        \n        return len(self.texts)\n    \n    def __getitem__(self,idx):\n        \n        tokens,mask,tokens_len = self.get_token_mask(self.texts[idx],self.max_len)\n        return [torch.tensor(tokens),torch.tensor(mask),torch.tensor(tokens_len)]\n        \n    def get_token_mask(self,text,max_len):\n        \n        tokens = []\n        mask = []\n        text = self.bert_encode.encode(text)\n        size = len(text)\n        pads = self.bert_encode.encode(['PAD']*(max(0,max_len-size)))\n        tokens[:max(max_len,size)] = text[:max(max_len,size)]\n        tokens = tokens + pads[1:-1]\n        mask = [1]*size+[0]*len(pads[1:-1])\n        tokens_len = len(tokens)\n        \n        return tokens,mask,tokens_len","bdcdb1ef":"device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\nmodel.eval()\npredictions = []\ntest_dataset = TestTextDataset(test,tokenizer=tokenizer,max_len=120)\ntest_loader = torch.utils.data.DataLoader(test_dataset,batch_size=32,shuffle=False)\nwith torch.no_grad():\n    for idx , (inputs) in tqdm(enumerate(test_loader),total=len(test_loader)):\n        inputs = [a.to(device) for a in inputs]\n        preds = model(inputs)\n        predictions.append(preds.cpu().detach().numpy())\n        \npredictions = np.vstack(predictions)","3110e001":"sample_subs.target = np.round(np.vstack(predictions)).astype(int)\nsample_subs.head(20)","7d01a62c":"sample_subs.to_csv('submission.csv', index = False)","dedc13d3":"#### Inference","bc5b0dfb":"Reference : https:\/\/web.stanford.edu\/class\/cs224n\/reports\/custom\/15785631.pdf"}}