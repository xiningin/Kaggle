{"cell_type":{"7c3e3b3f":"code","89d8b286":"code","bc706768":"code","f9e801d9":"code","f6d2e225":"code","31d84c60":"code","6a5c31ea":"code","d2bd6762":"code","5dcc6491":"code","ed234732":"code","39a44ace":"code","29540a5a":"code","b2c009bb":"code","ca8fe0a7":"code","093d1b0c":"markdown","b37c68e6":"markdown","2c636961":"markdown","8da7f37f":"markdown","2a500d58":"markdown"},"source":{"7c3e3b3f":"import matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_error\nimport torch\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader, TensorDataset\nimport torch.nn as nn\nimport torchvision.transforms as transforms\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler","89d8b286":"data_path = '\/kaggle\/input\/persian-artists-on-spotfiy\/Spotfiy_Persian_Artists.csv'\ndf = pd.read_csv(data_path)\ndf.head()","bc706768":"df.describe()","f9e801d9":"df.isna().sum()","f6d2e225":"selected_features = df.select_dtypes(exclude=[object, bool]).columns\nfinal_df = df[selected_features].dropna()","31d84c60":"features_to_see_correlation = ['valence', 'tempo', 'liveness', 'instrumentalness', 'acousticness', 'speechiness', \\\n                              'mode', 'loudness', 'energy', 'danceability', 'key']\nsns.pairplot(final_df[features_to_see_correlation])","6a5c31ea":"X, y = final_df.drop('tempo', axis=1), final_df['tempo']\ntrain_X, test_X, train_y, test_y = train_test_split(X, y, test_size=0.2, random_state=10)\nprint(f'train size: {train_X.shape[0]}, test size: {test_X.shape[0]}')","d2bd6762":"rf_model = RandomForestRegressor(random_state=5)\nrf_model.fit(train_X, train_y)\npredicted = rf_model.predict(test_X)\nmae_on_test = mean_absolute_error(predicted, test_y)\nprint(f'MAE on out-of-sample data: {mae_on_test}')","5dcc6491":"feat_importances = pd.Series(rf_model.feature_importances_, index=train_X.columns)\nfeat_importances.nlargest(5).plot(kind='barh', color=['blue', 'orange', 'green', 'red', 'yellow'])\nplt.show()","ed234732":"features_selected_by_rf = feat_importances.nlargest().keys()\nfeatures_selected_by_rf = features_selected_by_rf.values.tolist() + ['tempo']","39a44ace":"final_df = final_df[features_selected_by_rf]\nX, y = final_df.drop('tempo', axis=1), final_df['tempo']\ntrain_X, test_X, train_y, test_y = train_test_split(X, y, test_size=0.2, random_state=10)","29540a5a":"class CustomTensorDataset(Dataset):\n    def __init__(self, X, y):\n        self.size = X.shape[0]\n        self.X = torch.from_numpy(X)\n        self.y = torch.from_numpy(y)\n        \n    def __getitem__(self, index):\n        return self.X[index], self.y[index]\n    \n    def __len__(self):\n        return self.size\n\nsd = StandardScaler()\ntransformed_train_X = sd.fit_transform(train_X.values.astype(np.float32))\ntransformed_test_X = sd.transform(test_X.values.astype(np.float32))\n\ntrain_dataset = CustomTensorDataset(transformed_train_X, train_y.values.astype(np.float32))\ntest_dataset = CustomTensorDataset(transformed_test_X, test_y.values.astype(np.float32))\n\nbatch_size = 8\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n\nfeatures, labels = next(iter(train_loader))\nfor feature, label in zip(features, labels):\n    print(feature, label)\n    break","b2c009bb":"class MultiLayerPerceptron(nn.Module):\n    def __init__(self, input_size, output_size, hidden_size):\n        super().__init__()\n        self.linear1 = nn.Linear(input_size, hidden_size)\n        self.linear2 = nn.Linear(hidden_size, output_size)\n    \n    def forward(self, x):\n        out = F.relu(self.linear1(x))\n        out = self.linear2(out)\n        return out\n\n# device\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# hyperparameters\ninput_size, output_size = len(features_selected_by_rf) -1 , 1\nhidden_size = 50\nmlp_model = MultiLayerPerceptron(input_size, output_size, hidden_size)\ncriterion = nn.MSELoss()\noptimizer = torch.optim.Adam(mlp_model.parameters(), lr=2e-3)\nn_epochs = 100\nnum_samples = len(train_loader)\n\n# training loop\nfor epoch in range(n_epochs):\n    for i, (inputs, labels) in enumerate(train_loader):\n        inputs, labels = inputs.to(device), labels.to(device)\n        # forward pass\n        predicted = mlp_model(inputs)\n        loss = criterion(predicted, labels)\n        # backward pass\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        \n        if i % 100 == 0:\n            print(f'epoch {epoch}\/{n_epochs}, step {i+1}\/{num_samples}, loss={loss.item():.4f}')","ca8fe0a7":"mae = []\nwith torch.no_grad():\n    for inputs, labels in test_loader:\n        inputs, labels = inputs.to(device), labels.to(device)\n        predicted = mlp_model(inputs)\n        mae.append(torch.abs(predicted - labels).mean())\n    print(f'MAE on out-of-sample data: {sum(mae)\/24:.4f}')\n","093d1b0c":"### Train\/Test Split","b37c68e6":"### MLP Model","2c636961":"### CustomDataset & DataLoader","8da7f37f":"### Random Forest Regressor & Feature Importance","2a500d58":"### Feature Selection"}}