{"cell_type":{"41f687ad":"code","735e06d8":"code","07a8635f":"code","2f62666a":"code","21f1c45f":"code","9513a506":"code","f3fbbd79":"code","c0509aad":"code","f199b384":"code","f400a14f":"code","22bfab56":"code","2cb6fbd8":"markdown"},"source":{"41f687ad":"print(\"\\n... IMPORTS STARTING ...\\n\")\nprint(\"\\n\\tVERSION INFORMATION\")\n# Machine Learning and Data Science Imports\nimport tensorflow as tf; print(f\"\\t\\t\u2013 TENSORFLOW VERSION: {tf.__version__}\");\nimport tensorflow_addons as tfa; print(f\"\\t\\t\u2013 TENSORFLOW ADDONS VERSION: {tfa.__version__}\");\nimport pandas as pd; pd.options.mode.chained_assignment = None;\nimport numpy as np; print(f\"\\t\\t\u2013 NUMPY VERSION: {np.__version__}\");\nimport sklearn; print(f\"\\t\\t\u2013 SKLEARN VERSION: {sklearn.__version__}\");\nfrom sklearn.preprocessing import RobustScaler, PolynomialFeatures\nfrom sklearn.model_selection import GroupKFold, KFold;\n\n!pip install -q keras-cv-attention-models\nimport keras_cv_attention_models\n\n# Built In Imports\nfrom kaggle_datasets import KaggleDatasets\nfrom multiprocessing import cpu_count\nfrom collections import Counter\nfrom datetime import datetime\nfrom glob import glob\nimport warnings\nimport requests\nimport imageio\nimport IPython\nimport sklearn\nimport urllib\nimport zipfile\nimport pickle\nimport random\nimport shutil\nimport string\nimport math\nimport time\nimport gzip\nimport ast\nimport sys\nimport io\nimport os\nimport gc\nimport re\n\n# Visualization Imports\nfrom matplotlib.colors import ListedColormap\nimport matplotlib.patches as patches\nimport plotly.graph_objects as go\nimport matplotlib.pyplot as plt\nfrom tqdm.notebook import tqdm; tqdm.pandas();\nimport plotly.express as px\nimport seaborn as sns\nfrom PIL import Image\nimport matplotlib; print(f\"\\t\\t\u2013 MATPLOTLIB VERSION: {matplotlib.__version__}\");\nimport plotly\nimport PIL\nimport cv2\n\n\ndef seed_it_all(seed=7):\n    \"\"\" Attempt to be Reproducible \"\"\"\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    random.seed(seed)\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\n\n    \nprint(\"\\n\\n... IMPORTS COMPLETE ...\\n\")\n    \nprint(\"\\n... SEEDING FOR DETERMINISTIC BEHAVIOUR ...\\n\")\nseed_it_all()","735e06d8":"print(f\"\\n... ACCELERATOR SETUP STARTING ...\\n\")\n\n# Detect hardware, return appropriate distribution strategy\ntry:\n    # TPU detection. No parameters necessary if TPU_NAME environment variable is set. On Kaggle this is always the case.\n    TPU = tf.distribute.cluster_resolver.TPUClusterResolver()  \nexcept ValueError:\n    TPU = None\n\nif TPU:\n    print(f\"\\n... RUNNING ON TPU - {TPU.master()}...\")\n    tf.config.experimental_connect_to_cluster(TPU)\n    tf.tpu.experimental.initialize_tpu_system(TPU)\n    strategy = tf.distribute.experimental.TPUStrategy(TPU)\nelse:\n    print(f\"\\n... RUNNING ON CPU\/GPU ...\")\n    # Yield the default distribution strategy in Tensorflow\n    #   --> Works on CPU and single GPU.\n    strategy = tf.distribute.get_strategy() \n\n# What Is a Replica?\n#    --> A single Cloud TPU device consists of FOUR chips, each of which has TWO TPU cores. \n#    --> Therefore, for efficient utilization of Cloud TPU, a program should make use of each of the EIGHT (4x2) cores. \n#    --> Each replica is essentially a copy of the training graph that is run on each core and \n#        trains a mini-batch containing 1\/8th of the overall batch size\nN_REPLICAS = strategy.num_replicas_in_sync\n    \nprint(f\"... # OF REPLICAS: {N_REPLICAS} ...\\n\")\n\nprint(f\"\\n... ACCELERATOR SETUP COMPLTED ...\\n\")","07a8635f":"print(\"\\n... DATA ACCESS SETUP STARTED ...\\n\")\n\nif TPU:\n    # Google Cloud Dataset path to training and validation images\n    DATA_DIR = KaggleDatasets().get_gcs_path('petfinder-pawpularity-score')\n    CAT_DIR = KaggleDatasets().get_gcs_path('cat-breeds-dataset')\n    DOG_DIR = KaggleDatasets().get_gcs_path('stanford-dogs-dataset')\n    save_locally = tf.saved_model.SaveOptions(experimental_io_device='\/job:localhost')\nelse:\n    # Local path to training and validation images\n    DATA_DIR = \"\/kaggle\/input\/petfinder-pawpularity-score\"\n    CAT_DIR = \"\/kaggle\/input\/cat-breeds-dataset\"\n    DOG_DIR = \"\/kaggle\/input\/stanford-dogs-dataset\"\n    save_locally = None\n    \nprint(f\"\\n... DATA DIRECTORY PATH IS:\\n\\t--> {DATA_DIR}\")\nprint(f\"\\n... CAT DIRECTORY PATH IS:\\n\\t--> {CAT_DIR}\")\nprint(f\"\\n... DOG DIRECTORY PATH IS:\\n\\t--> {DOG_DIR}\")\n\nprint(f\"\\n... IMMEDIATE CONTENTS OF DATA DIRECTORY IS:\")\nfor file in tf.io.gfile.glob(os.path.join(DATA_DIR, \"*\")): print(f\"\\t--> {file}\")\n\nprint(f\"\\n... IMMEDIATE CONTENTS OF CAT DIRECTORY IS:\")\nfor file in tf.io.gfile.glob(os.path.join(CAT_DIR, \"*\")): print(f\"\\t--> {file}\")\n\nprint(f\"\\n... IMMEDIATE CONTENTS OF DOG DIRECTORY IS:\")\nfor file in tf.io.gfile.glob(os.path.join(DOG_DIR, \"*\")): print(f\"\\t--> {file}\")\n\nprint(\"\\n\\n... DATA ACCESS SETUP COMPLETED ...\\n\")","2f62666a":"print(f\"\\n... XLA OPTIMIZATIONS STARTING ...\\n\")\n\nprint(f\"\\n... CONFIGURE JIT (JUST IN TIME) COMPILATION ...\\n\")\n# enable XLA optmizations (10% speedup when using @tf.function calls)\ntf.config.optimizer.set_jit(True)\n\nprint(f\"\\n... XLA OPTIMIZATIONS COMPLETED ...\\n\")","21f1c45f":"print(\"\\n... BASIC DATA SETUP STARTING ...\\n\\n\")\n\nprint(\"\\n... TRAIN DATAFRAME ..\\n\")\nTRAIN_CSV = os.path.join(DATA_DIR, \"train.csv\")\ntrain_df = pd.read_csv(TRAIN_CSV)\ntrain_df[\"img_path\"] = train_df.Id.apply(lambda x: os.path.join(DATA_DIR, \"train\", x+\".jpg\"))\ndisplay(train_df)\n\nprint(\"\\n... TEST DATAFRAME ..\\n\")\nTEST_CSV = os.path.join(DATA_DIR, \"test.csv\")\ntest_df = pd.read_csv(TEST_CSV)\ntest_df[\"img_path\"] = test_df.Id.apply(lambda x: os.path.join(DATA_DIR, \"test\", x+\".jpg\"))\n\ndisplay(test_df)\n\nprint(\"\\n... SAMPLE SUBMISSION DATAFRAME ..\\n\")\nSS_CSV = os.path.join(DATA_DIR, \"sample_submission.csv\")\nss_df = pd.read_csv(SS_CSV)\ndisplay(ss_df)\n\n# Set Other Variables\nprint(\"\\n... SETTING OTHER VARIABLES ..\\n\")\n\nINPUT_SHAPE = (224, 224, 3)\nN_CLASSES = train_df.Pawpularity.nunique()\nREPLICA_BATCH_SIZE = 32\nOVERALL_BATCH_SIZE = REPLICA_BATCH_SIZE * N_REPLICAS\nAUTO = tf.data.experimental.AUTOTUNE\n\nprint(\"\\n\\n... BASIC DATA SETUP FINISHING ...\\n\")","9513a506":"effnetb0_imagenet = tf.keras.applications.EfficientNetB0()\neffnetb0_imagenet.summary()","f3fbbd79":"def flatten_l_o_l(nested_list):\n    \"\"\" Flatten a list of lists \"\"\"\n    return [item for sublist in nested_list for item in sublist]\n\ndef tf_load_image(image, resize_to=INPUT_SHAPE):\n    image = tf.image.decode_jpeg(tf.io.read_file(image), channels=INPUT_SHAPE[-1])\n    image = tf.image.resize(image, size=resize_to[:-1])\n    return image\n\ndef rotate_and_crop(images):\n    \"\"\"Rotate the given image with the given rotation degree and crop for the black edges if necessary\n    Args:\n        image: A `Tensor` representing an image(s) of arbitrary size.\n    \n    Returns:\n        A rotated image.\n    \"\"\"\n    \n    \n    def _largest_rotated_rect(w, h, angle):\n        \"\"\"\n        \n        Given a rectangle of size wxh that has been rotated by 'angle' (in\n        radians), computes the width and height of the largest possible\n        axis-aligned rectangle within the rotated rectangle.\n        Original JS code by 'Andri' and Magnus Hoff from Stack Overflow\n        Converted to Python by Aaron Snoswell\n        \n        Source: http:\/\/stackoverflow.com\/questions\/16702966\/rotate-image-and-crop-out-black-borders\n        \n        \"\"\"\n        \n        quadrant = tf.cast(tf.math.floor(angle \/ (math.pi\/2)), dtype=tf.uint8)\n        quadrant = tf.bitwise.bitwise_and(quadrant, tf.constant(3, dtype=quadrant.dtype))\n        sign_alpha = tf.cond(tf.bitwise.bitwise_and(quadrant, tf.constant(1, dtype=quadrant.dtype))==tf.constant(0, dtype=quadrant.dtype), lambda: angle, lambda: math.pi-angle)\n        alpha = (sign_alpha % math.pi + math.pi) % math.pi\n\n        bb_w = w * tf.math.cos(alpha) + h * tf.math.sin(alpha)\n        bb_h = w * tf.math.sin(alpha) + h * tf.math.cos(alpha)\n\n        gamma = tf.cond(w<h, lambda: tf.math.atan2(bb_w, bb_w), lambda: tf.math.atan2(bb_w, bb_w))\n\n        delta = math.pi - alpha - gamma\n\n        length = tf.cond(w<h, lambda: h, lambda: w)\n\n        d = length * tf.math.cos(alpha)\n        a = d * tf.math.sin(alpha) \/ tf.math.sin(delta)\n\n        y = a * tf.math.cos(gamma)\n        x = y * tf.math.tan(gamma)\n\n        return (bb_w - 2 * x, bb_h - 2 * y)\n  \n    # Get desired output dimensions\n    output_height, output_width = tf.constant(INPUT_SHAPE[0], dtype=tf.float32), tf.constant(INPUT_SHAPE[1], dtype=tf.float32)\n\n    rotation_degree = (math.pi\/180)*tf.random.normal(shape=(), stddev=10)\n    images = tfa.image.rotate(images, rotation_degree, interpolation='BILINEAR')\n\n    # Center crop to ommit black noise on the edges\n    lrr_width, lrr_height = _largest_rotated_rect(output_height, output_width, rotation_degree)\n    lrr_offset_w, lrr_offset_h = tf.cast(tf.math.round((output_width-lrr_width)\/2), dtype=tf.int32), tf.cast(tf.math.round((output_height-lrr_height)\/2), dtype=tf.int32)\n    lrr_width, lrr_height = tf.cast(tf.math.round(lrr_width), dtype=tf.int32), tf.cast(tf.math.round(lrr_height), dtype=tf.int32)\n\n    images = tf.image.crop_to_bounding_box(images, lrr_offset_h, lrr_offset_w, target_height=lrr_height, target_width=lrr_width)\n    images = tf.image.resize(images, (output_height, output_width))\n    \n    return images\n\n\ndef simple_augmentation(images, labels):\n    # Random Horizontal Flip\n    images = tf.image.random_flip_left_right(images)\n    \n    # images = rotate_and_crop(images)\n    \n    images = tfa.image.random_cutout(images, mask_size=(2*(INPUT_SHAPE[0]\/\/30),2*(INPUT_SHAPE[0]\/\/30)))\n    \n    # Random Saturation\n    images = tf.image.random_saturation(images, 0.975, 1.025)\n\n    # Random Hue\n    images = tf.image.random_hue(images, 0.0125)\n    \n    # Random Brightness\n    images = tf.image.random_brightness(images, 0.125)\n    \n    return images, labels","c0509aad":"train_img_ds = tf.data.Dataset.from_tensor_slices(train_df.img_path.values)\ntrain_ds = train_img_ds.map(lambda x: (tf_load_image(x),x), num_parallel_calls=tf.data.AUTOTUNE).cache().batch(OVERALL_BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n\ntest_img_ds = tf.data.Dataset.from_tensor_slices(test_df.img_path.values)\ntest_ds = test_img_ds.map(lambda x: (tf_load_image(x),x), num_parallel_calls=tf.data.AUTOTUNE).batch(OVERALL_BATCH_SIZE).prefetch(tf.data.AUTOTUNE)","f199b384":"# all_train_preds = effnetb0_imagenet.predict(train_ds)\n# all_train_preds = tf.argmax(all_train_preds, axis=-1)\n","f400a14f":"all_train_preds = []\nall_train_paths = []\nfor img, path, in tqdm(train_ds):\n    all_train_preds.append(tf.cast(tf.argmax(effnetb0_imagenet.predict(img), axis=-1), tf.int16))\n    all_train_paths.append(path)\n    \nall_test_preds = []\nall_test_paths = []\nfor img, path, in tqdm(train_ds):\n    all_test_preds.append(tf.cast(tf.argmax(effnetb0_imagenet.predict(img), axis=-1), tf.int16))\n    all_test_paths.append(path)","22bfab56":"train_df.corr()[\"Pawpularity\"]","2cb6fbd8":"## **As we can see... imagenet classes are more correlated than any other given feature**"}}