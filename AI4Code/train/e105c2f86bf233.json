{"cell_type":{"5d2bd329":"code","1aab1496":"code","31564a64":"code","34097347":"code","c1fa33ed":"code","badfee75":"code","238a8f5a":"code","cdabb0c8":"code","7f867689":"code","94801dd4":"code","1839bba4":"code","ad3370b1":"code","96b4e588":"code","77c62309":"code","57eb6221":"code","21ac531f":"code","a1a583be":"code","d0c42e6a":"code","0a376a81":"code","5c7f137f":"code","4c1bb5e7":"code","c4c24085":"code","68009add":"code","fae49a4d":"code","fd8ac1b6":"code","a382d31e":"markdown","0df3e12f":"markdown","e452cd6b":"markdown","6cbb749e":"markdown","af419afd":"markdown","b86fbfcb":"markdown","0a185e9c":"markdown","d033dff5":"markdown","fb28878d":"markdown","73ce4f5d":"markdown","f4e8ce3f":"markdown","a986b527":"markdown"},"source":{"5d2bd329":"import numpy as np \nimport pandas as pd\nimport matplotlib\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline \nfrom wordcloud import WordCloud, STOPWORDS\nfrom nltk.sentiment import SentimentIntensityAnalyzer\nfrom textblob import TextBlob\nimport warnings\nwarnings.simplefilter(\"ignore\")","1aab1496":"data_df = pd.read_csv(\"..\/input\/sub-reddit-of-the-day-posts\/subredditoftheday_reddit.csv\")","31564a64":"data_df.head()","34097347":"data_df.info()","c1fa33ed":"def missing_data(data):\n    total = data.isnull().sum()\n    percent = (data.isnull().sum()\/data.isnull().count()*100)\n    tt = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n    types = []\n    for col in data.columns:\n        dtype = str(data[col].dtype)\n        types.append(dtype)\n    tt['Types'] = types\n    return(np.transpose(tt))","badfee75":"missing_data(data_df)","238a8f5a":"def show_wordcloud(data, title=\"\"):\n    text = \" \".join(t for t in data.dropna())\n    stopwords = set(STOPWORDS)\n    stopwords.update([\"t\", \"co\", \"https\", \"amp\", \"U\", \"fuck\", \"fucking\"])\n    wordcloud = WordCloud(stopwords=stopwords, scale=4, max_font_size=50, max_words=500,background_color=\"black\").generate(text)\n    fig = plt.figure(1, figsize=(16,16))\n    plt.axis('off')\n    fig.suptitle(title, fontsize=20)\n    fig.subplots_adjust(top=2.3)\n    plt.imshow(wordcloud, interpolation='bilinear')\n    plt.show()","cdabb0c8":"show_wordcloud(data_df['title'], title = 'Prevalent words in titles')","7f867689":"show_wordcloud(data_df['body'], title = 'Prevalent words in post bodies')","94801dd4":"# borrowed from https:\/\/www.kaggle.com\/pashupatigupta\/sentiments-transformer-vader-embedding-bert\nsia = SentimentIntensityAnalyzer()\ndef find_sentiment(post):\n    if sia.polarity_scores(post)[\"compound\"] > 0:\n        return \"Positive\"\n    elif sia.polarity_scores(post)[\"compound\"] < 0:\n        return \"Negative\"\n    else:\n        return \"Neutral\"       ","1839bba4":"def plot_sentiment(df, feature, title):\n    counts = df[feature].value_counts()\n    percent = counts\/sum(counts)\n\n    fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(12, 5))\n\n    counts.plot(kind='bar', ax=ax1, color='green')\n    percent.plot(kind='bar', ax=ax2, color='blue')\n    ax1.set_ylabel(f'Counts : {title} sentiments', size=12)\n    ax2.set_ylabel(f'Percentage : {title} sentiments', size=12)\n    plt.suptitle(f\"Sentiment analysis: {title}\")\n    plt.tight_layout()\n    plt.show()","ad3370b1":"data_df['title_sentiment'] = data_df['title'].apply(lambda x: find_sentiment(x))\nplot_sentiment(data_df, 'title_sentiment', 'Title')","96b4e588":"show_wordcloud(data_df.loc[data_df['title_sentiment']=='Positive', 'title'], title = 'Prevalent words in titles (Positive sentiment)')","77c62309":"show_wordcloud(data_df.loc[data_df['title_sentiment']=='Negative', 'title'], title = 'Prevalent words in titles (Negative sentiment)')","57eb6221":"show_wordcloud(data_df.loc[data_df['title_sentiment']=='Neutral', 'title'], title = 'Prevalent words in titles (Neutral sentiment)')","21ac531f":"df = data_df.loc[~data_df.body.isna()]\ndf['body_sentiment'] = df['body'].apply(lambda x: find_sentiment(x))\nplot_sentiment(df, 'body_sentiment', 'Body')","a1a583be":"show_wordcloud(df.loc[df['body_sentiment']=='Positive', 'body'], title = 'Prevalent words in body (Positive sentiment)')","d0c42e6a":"show_wordcloud(df.loc[df['body_sentiment']=='Negative', 'body'], title = 'Prevalent words in body (Negative sentiment)')","0a376a81":"show_wordcloud(df.loc[df['body_sentiment']=='Neutral', 'body'], title = 'Prevalent words in body (Neutral sentiment)')","5c7f137f":"def find_sentiment_polarity_textblob(post):\n    blob = TextBlob(post)\n    polarity = 0\n    for sentence in blob.sentences:\n        polarity += sentence.sentiment.polarity\n    return polarity\n\ndef find_sentiment_subjectivity_textblob(post):\n    blob = TextBlob(post)\n    subjectivity = 0\n    for sentence in blob.sentences:\n        subjectivity += sentence.sentiment.subjectivity\n    return subjectivity","4c1bb5e7":"data_df['title_sentiment_polarity'] = data_df['title'].apply(lambda x: find_sentiment_polarity_textblob(x))\ndata_df['title_sentiment_subjectivity'] = data_df['title'].apply(lambda x: find_sentiment_subjectivity_textblob(x))","c4c24085":"def plot_sentiment_textblob(df, feature, title):\n    polarity = df[feature+'_sentiment_polarity']\n    subjectivity = df[feature+'_sentiment_subjectivity']\n\n    fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(12, 5))\n\n    polarity.plot(kind='kde', ax=ax1, color='magenta')\n    subjectivity.plot(kind='kde', ax=ax2, color='green')\n    ax1.set_ylabel(f'Sentiment polarity : {title}', size=12)\n    ax2.set_ylabel(f'Sentiment subjectivity: {title}', size=12)\n    plt.suptitle(f\"Sentiment analysis (polarity & subjectivity): {title}\")\n    plt.tight_layout()\n    plt.show()","68009add":"plot_sentiment_textblob(data_df, \"title\", 'Title')","fae49a4d":"df['body_sentiment_polarity'] = df['body'].apply(lambda x: find_sentiment_polarity_textblob(x))\ndf['body_sentiment_subjectivity'] = df['body'].apply(lambda x: find_sentiment_subjectivity_textblob(x))","fd8ac1b6":"plot_sentiment_textblob(df, \"body\", 'Body')","a382d31e":"# Analysis preparation\n\nWe initialize the packages that we will use in the analysis.","0df3e12f":"## Title","e452cd6b":"We also look to things like data quality, for example missing data.","6cbb749e":"Body of posts is missing in approximatively half of the data.","af419afd":"# Data visualization\n\n\nWe will use wordclouds to identify the most frequent words in the titles and body of the posts.\n\nFor understanding some of the frequent used terms, it will be useful to consult this resource: [r\/subredditoftheday stats](https:\/\/subredditstats.com\/r\/subredditoftheday)\n\nSome examples (from the resource mentioned above):  \n\n1\t  pancakeswap\t\u00d73717\n2\t  girlsyouknowirl\t\u00d72709\n3\t  amazing_architecture\t\u00d72047\n4\t  BaddieHermOnlyFans\t\u00d71709\n5\t  SmartGamingDeals\t\u00d71315\n6\t  GrandPieceOnline\t\u00d7844\n7\t  thesongofachilles\t\u00d7668\n8\t  WKHS\t\u00d7537\n9\t  mangacoloring\t\u00d7466\n10\t  RushRoyale\t\u00d7451\n11\t  FranceDetendue\t\u00d7410\n12\t  RustConsole\t\u00d7395\n13\t  t3ddyyyy\t\u00d7383\n14\t  Ranboo\t\u00d7356\n15\t  WANDAVISION\t\u00d7348\n16\t  NFT\t\u00d7336\n17\t  SpecialHumor\t\u00d7313\n18\t  MakersPlace\t\u00d7296\n19\t  NarakaBladePoint\t\u00d7295\n20\t  pantiesandsocks\t\u00d7270\n21\t  Timberborn\t\u00d7242\n22\t  LesliePecas\t\u00d7227\n23\t  sekulermilliyetciturk\t\u00d7223\n24\t  TollbugataBets\t\u00d7221\n25\t  GenshinTrades\t\u00d7213\n26\t  epoxyhotdog\t\u00d7200\n27\t  FuckPierre\t\u00d7185\n28\t  InsideJob\t\u00d7185\n29\t  AxieInfinity\t\u00d7182\n30\t  BinanceSmartChain\t\u00d7181","b86fbfcb":"We read and glimpse the data.","0a185e9c":"## With TextBlob","d033dff5":"### Body","fb28878d":"## Body","73ce4f5d":"### Title","f4e8ce3f":"# Sentiment analysis\n\n## With nltk SentimentIntensityAnalyzer","a986b527":"# Introduction\n\nFrom Wikipedia: [r\/subredditoftheday](https:\/\/www.reddit.com\/r\/subredditoftheday\/), .. is a celebration of the interesting communities on reddit.com. Once a day we shine a spotlight on the small, the big, the new and the old. Our mission is to spotlight unique reddit communities and bring the awesome, every damn day.\n\n<center><img src=\"https:\/\/styles.redditmedia.com\/t5_2sgno\/styles\/communityIcon_2wot6zdwqem01.png\" width=600><\/img><\/center>\n\n### Inspiration comes from Gabriel Preda Analysis https:\/\/www.kaggle.com\/gpreda\/wallstreetbets-reddit-posts-analysis. Thanks to give him credits. \ud83d\udc4f"}}