{"cell_type":{"19e7e1de":"code","90062c15":"code","3da7ea95":"code","83722e0e":"code","2a306ad3":"code","70f3c279":"code","00958a27":"code","63b60cad":"code","0d3d19ae":"code","34341d8d":"code","18edb864":"code","e0d738c1":"code","13d7fa55":"code","c4408513":"code","5fe69264":"code","f52d3a15":"code","d5a11ffc":"code","076008c3":"markdown","4f82b2d7":"markdown","1dde17b6":"markdown","52e2263c":"markdown","e1a16412":"markdown","ca4a8e95":"markdown","8bb75712":"markdown","7bca139d":"markdown","1f53c3f0":"markdown","0963bbed":"markdown","373459fc":"markdown","70d57d5a":"markdown","853adcaa":"markdown","09e8c61f":"markdown","ac11933c":"markdown","8128a4d8":"markdown"},"source":{"19e7e1de":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.dates as mdates\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout,GRU\nfrom keras import optimizers \nfrom sklearn.preprocessing import MinMaxScaler\n\nseed = 1234\nnp.random.seed(seed)\nplt.style.use('ggplot')","90062c15":"dataraw = pd.read_csv('..\/input\/bitcoin-data-from-2014-to-2020\/BTC-USD.csv',index_col='Date', parse_dates=['Date'])\ndataraw","3da7ea95":"# use feature 'Date' & 'Close'\ndataset = pd.DataFrame(dataraw['Close'])\nprint(dataset.describe())\nprint(' Count row of data: ',len(dataset))","83722e0e":"#Min-Max Normalization\ndataset_norm = dataset.copy()\ndataset[['Close']]\nscaler = MinMaxScaler()\ndataset_norm['Close'] = scaler.fit_transform(dataset[['Close']])\ndataset_norm","2a306ad3":"fig = plt.figure(figsize=(10, 4))\nplt.plot(dataset_norm)\nplt.xlabel('Date')\nplt.gca().xaxis.set_major_formatter(mdates.DateFormatter(\"%Y-%m\"))\nplt.title('Data Normalized')\nplt.show()","70f3c279":"# Partition data into data train, val & test\ntotaldata = dataset.values\ntotaldatatrain = int(len(totaldata)*0.7)\ntotaldataval = int(len(totaldata)*0.1)\ntotaldatatest = int(len(totaldata)*0.2)\n\n# Store data into each partition\ntraining_set = dataset_norm[0:totaldatatrain]\nval_set=dataset_norm[totaldatatrain:totaldatatrain+totaldataval]\ntest_set = dataset_norm[totaldatatrain+totaldataval:]","00958a27":"# graph data training\nfig = plt.figure(figsize=(10, 4))\nplt.plot(training_set)\nplt.xlabel('Date')\nplt.gca().xaxis.set_major_formatter(mdates.DateFormatter(\"%Y-%m\"))\nplt.title('Data Training')\nplt.show()","63b60cad":"# graph data validation\nfig = plt.figure(figsize=(10, 4))\nplt.plot(val_set)\nplt.xlabel('Date')\nplt.gca().xaxis.set_major_formatter(mdates.DateFormatter(\"%Y-%m\"))\nplt.title('Data Validation')\nplt.show()","0d3d19ae":"# graph data test\nfig = plt.figure(figsize=(10, 4))\nplt.plot(test_set)\nplt.xlabel('Date')\nplt.gca().xaxis.set_major_formatter(mdates.DateFormatter(\"%Y-%m\"))\nplt.title('Data Test')\nplt.show()","34341d8d":"# Initiaton value of lag\nlag = 2\n# Sliding windows function\ndef create_sliding_windows(data,len_data,lag):\n    x=[]\n    y=[]\n    for i in range(lag,len_data):\n        x.append(data[i-lag:i,0])\n        y.append(data[i,0]) \n    return np.array(x),np.array(y)\n\n# Formating data into array for create sliding windows\narray_training_set = np.array(training_set)\narray_val_set = np.array(val_set)\narray_test_set = np.array(test_set)\n\n# Create sliding windows into training data\nx_train, y_train = create_sliding_windows(array_training_set,len(array_training_set), lag)\nx_train = np.reshape(x_train, (x_train.shape[0],x_train.shape[1],1))\n# Create sliding windows into validation data\nx_val,y_val = create_sliding_windows(array_val_set,len(array_val_set),lag)\nx_val = np.reshape(x_val, (x_val.shape[0],x_val.shape[1],1))\n# Create sliding windows into test data\nx_test,y_test = create_sliding_windows(array_test_set,len(array_test_set),lag)\nx_test = np.reshape(x_test, (x_test.shape[0],x_test.shape[1],1))","18edb864":"# Hyperparameters\nlearning_rate = 0.0001\nhidden_unit = 64\nbatch_size=256\nepoch = 100\n\n# Architecture Gated Recurrent Unit\nregressorGRU = Sequential()\n\n# First GRU layer with dropout\nregressorGRU.add(GRU(units=hidden_unit, return_sequences=True, input_shape=(x_train.shape[1],1), activation = 'tanh'))\nregressorGRU.add(Dropout(0.2))\n# Second GRU layer with dropout\nregressorGRU.add(GRU(units=hidden_unit, return_sequences=True, activation = 'tanh'))\nregressorGRU.add(Dropout(0.2))\n# Third GRU layer with dropout\nregressorGRU.add(GRU(units=hidden_unit, return_sequences=False, activation = 'tanh'))\nregressorGRU.add(Dropout(0.2))\n\n# Output layer\nregressorGRU.add(Dense(units=1))\n\n# Compiling the Gated Recurrent Unit\nregressorGRU.compile(optimizer=optimizers.Adam(lr=learning_rate),loss='mean_squared_error')\n\n# Fitting ke data training dan data validation\npred = regressorGRU.fit(x_train, y_train, validation_data=(x_val,y_val), batch_size=batch_size, epochs=epoch)","e0d738c1":"# Graph model loss (train loss & val loss)\nfig = plt.figure(figsize=(10, 4))\nplt.plot(pred.history['loss'], label='train loss')\nplt.plot(pred.history['val_loss'], label='val loss')\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(loc='upper right')\nplt.show()","13d7fa55":"# Tabel value of training loss & validation loss\nlearningrate_parameter = learning_rate\ntrain_loss=pred.history['loss'][-1]\nvalidation_loss=pred.history['val_loss'][-1]\nlearningrate_parameter=pd.DataFrame(data=[[learningrate_parameter, train_loss, validation_loss]],\n                                    columns=['Learning Rate', 'Training Loss', 'Validation Loss'])\nlearningrate_parameter.set_index('Learning Rate')","c4408513":"# Implementation model into data test\ny_pred_test = regressorGRU.predict(x_test)\n\n# Invert normalization min-max\ny_pred_invert_norm = scaler.inverse_transform(y_pred_test)\n","5fe69264":"# Comparison data test with data prediction\ndatacompare = pd.DataFrame()\ndatatest=np.array(dataset['Close'][totaldatatrain+totaldataval+lag:])\ndatapred= y_pred_invert_norm\n\ndatacompare['Data Test'] = datatest\ndatacompare['Prediction Results'] = datapred\ndatacompare","f52d3a15":"# Calculatre value of Root Mean Square Error \ndef rmse(datatest, datapred):\n    return np.round(np.sqrt(np.mean((datapred - datatest) ** 2)), 4)\nprint('Root Mean Square Error :',rmse(datatest, datapred))\n\ndef mape(datatest, datapred): \n    return np.round(np.mean(np.abs((datatest - datapred) \/ datatest) * 100), 4)\n    \nprint('Mean Absolute Percentage Error : ', mape(datatest, datapred), '%')","d5a11ffc":"# Create graph data test and prediction result\nplt.figure(num=None, figsize=(10, 4), dpi=80,facecolor='w', edgecolor='k')\nplt.title('Graph Comparison Data Actual and Data Prediction')\nplt.plot(datacompare['Data Test'], color='red',label='Data Test')\nplt.plot(datacompare['Prediction Results'], color='blue',label='Prediction Results')\nplt.xlabel('Day')\nplt.ylabel('Price')\nplt.legend()\nplt.show()\n","076008c3":"# Feature Selection","4f82b2d7":"# Data Partition","1dde17b6":"## Evaluation Prediction Results","52e2263c":"# Preprocessing Data","e1a16412":"# Comparison data test with prediction results","ca4a8e95":"### Normalization Min-Max","8bb75712":"# Grafik Data Test","7bca139d":"# Model GRU","1f53c3f0":"# Grafik Data Validation","0963bbed":"# Graph Training Data","373459fc":"## Graph training loss & validation loss","70d57d5a":"# Import Data","853adcaa":"# Sliding Windows","09e8c61f":"# Tabel Training Loss & Validation Loss","ac11933c":"# Implementation model into data test","8128a4d8":"### Graph Data Normalized"}}