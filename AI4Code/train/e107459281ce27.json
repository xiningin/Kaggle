{"cell_type":{"252e517a":"code","832d4a19":"code","7896a97a":"code","04ca8260":"code","bd80f03a":"code","4f11c25f":"code","04299655":"code","90a0fd5d":"code","d1203c65":"code","10f63461":"code","be4550b4":"code","75eba8dc":"code","1401de95":"code","b19280c2":"code","7dca325a":"code","26835be3":"code","30784657":"code","e9f074e9":"code","6035ecb8":"code","7167c0e9":"code","4cbf05b0":"code","48e6b9dc":"code","f6811f16":"code","82aa8f27":"code","0fa980b9":"code","49bd7a3e":"code","3f3cd9fc":"code","05251ca2":"code","6433b724":"code","251f7f12":"code","2e697ffa":"code","d70d26ec":"code","e019a669":"markdown","210b1b80":"markdown","51460a5f":"markdown","80adb879":"markdown","95d21cb5":"markdown","ae6a70b0":"markdown","145ea392":"markdown","bb44e540":"markdown","6497209e":"markdown","0c7c9f5f":"markdown","c3b65400":"markdown"},"source":{"252e517a":"import pandas as pd # data processing, CSV file I\/O \nfrom matplotlib import pyplot as plt # data visualisation\nfrom sklearn.preprocessing import StandardScaler # preprocessing stuff\nfrom sklearn.ensemble import RandomForestClassifier as rfc # model!","832d4a19":"train = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")","7896a97a":"# Create dataframe both to control all data\n# I will extract train and test from both at the end\\\n# By looking at where the \"Survived\" column is null\nboth = pd.concat([train, test], ignore_index=True)\nboth.head()","04ca8260":"# Get rid of useless columns\nboth.drop([\"Ticket\", \"Cabin\"], axis=1, inplace=True)","bd80f03a":"# Is there a significant number of null values?\nplt.plot(both.columns, [both[column].isnull().sum() for column in both])","4f11c25f":"# Make sure no column has any null values except survived\n# (Survived doesn't matter because it's just the testing\\\n# data which isn't supposed to have survived values)\nfor column in both.drop(\"Survived\", axis=1, errors=\"ignore\"):\n    if both[column].isnull().any():\n        both[column].fillna(both[column].mode()[0], inplace=True)\n# Not the most effective way of doing it but I couldn't\\\n# be bothered to think of a better way","04299655":"# Just checking that worked\n# The 418 at the start is the \"Survived\" column\nprint(\"Null values in columns:\", [both[column].isnull().sum() for column in both])","90a0fd5d":"# Name is useless however title could give information\\\n# About how important each person is\n# I will create a VIP column showing importance\\\n# Of each person\nboth[\"Title\"] = [name.split(\",\")[1].split(\".\")[0] for name in both[\"Name\"]] # Get part of name between , and . as it will be the title","d1203c65":"# Checking what titles there are\nboth[\"Title\"].unique()","10f63461":"# Evaluate whether people are likely to be VIP\\\n# Based on their title\n# Higher number = more important\neval_title_to = {\n    \" Mr\": 0,\n    \" Mrs\": 0,\n    \" Miss\": 0,\n    \" Master\": 0,\n    \" Ms\": 0,\n    \" Major\": 0,\n    \" Dr\": 0,\n    \" Mme\": 0,\n    \" Mlle\": 0,\n    \" Rev\": 0.5,\n    \" Don\": 1,\n    \" Lady\": 1,\n    \" Sir\": 1,\n    \" Col\": 1,\n    \" Capt\": 1,\n    \" the Countess\": 1,\n    \" Jonkheer\": 1,\n    \" Dona\": 1\n}\nboth[\"VIP\"] = [eval_title_to[title] for title in both[\"Title\"]]","be4550b4":"both[\"Embarked\"].unique()","75eba8dc":"# Just checking whether \"Embarked\" is actually useful,\\\n# But it turns out it does seem to have some effect\\\n# On survival rates\ntrain = both.dropna()\nplt.plot(\n    train[\"Embarked\"].unique(),\n    [train[train[\"Embarked\"] == embarked][\"Survived\"].mean() for embarked in train[\"Embarked\"].unique()]\n)","1401de95":"# The parch column shows how many parents and children\\\n# A person has on board with them, and the sibsp\\\n# Column shows how many siblings. I will simply combine\\\n# These into a single column showing family size\nboth[\"Family\"] = both[\"Parch\"] + both[\"SibSp\"]","b19280c2":"# Based on age and parch I can evaluate whether or not\\\n# The person is likely to be a parent\n# It won't be fully accurate but it will give a rough idea\nparent = [0 for row in range(len(both[\"Age\"]))]\nfor row in range(len(both[\"Age\"])):\n    if both[\"Age\"][row] > 20:\n        if both[\"Parch\"][row] >= 1:\n            parent[row] = 1\nboth[\"Parent\"] = parent","7dca325a":"both[\"Parent\"].unique()","26835be3":"both.head()","30784657":"both[\"Parent\"].values.sum() \/ len(both[\"Parent\"]) # Proportion of people who are parents","e9f074e9":"both[\"Sex\"] = [{\"male\": 0, \"female\": 1}[sex] for sex in both[\"Sex\"]]\nboth[\"C\"] = [{\"C\": 1, \"S\": 0, \"Q\": 0}[embarked] for embarked in both[\"Embarked\"]]\nboth[\"S\"] = [{\"C\": 0, \"S\": 1, \"Q\": 0}[embarked] for embarked in both[\"Embarked\"]]\nboth[\"Q\"] = [{\"C\": 0, \"S\": 0, \"Q\": 1}[embarked] for embarked in both[\"Embarked\"]]\nboth.drop([\"Embarked\", \"Name\", \"Title\"], axis=1, inplace=True)","6035ecb8":"both.head()","7167c0e9":"plt.plot(both.columns, abs(both.corr(method=\"spearman\")[\"Survived\"]))","4cbf05b0":"# Correlation of VIP and Age and (obviously) PassengerId are low\\\n# So I will drop them\nboth.drop([\"PassengerId\", \"Age\", \"VIP\"], axis=1, inplace=True)","48e6b9dc":"train = both.dropna()\ntrain_main = train.drop(\"Survived\", axis=1)\ntrain_target = train[\"Survived\"]\ntest = both[both[\"Survived\"].isna()].drop(\"Survived\", axis=1)","f6811f16":"ss = StandardScaler()\ntrain_main = pd.DataFrame(ss.fit_transform(train_main))\ntest = pd.DataFrame(ss.fit_transform(test))","82aa8f27":"train_main.head()","0fa980b9":"test.head()","49bd7a3e":"train_target.head()","3f3cd9fc":"# Get predictions from random forest classifier\npredictions_rfc = rfc().fit(train_main, train_target).predict(test).astype(int)","05251ca2":"final_predictions = predictions_rfc","6433b724":"survived_count = 0\nfor i in final_predictions:\n    if i == 1:\n        survived_count += 1\nprint(\"Proportion predicted to have survived:\", survived_count \/ len(final_predictions))","251f7f12":"final_predictions_df = pd.DataFrame({\"PassengerId\": [i for i in range(892, 1310)], \"Survived\": final_predictions})","2e697ffa":"final_predictions_df","d70d26ec":"final_predictions_df.to_csv(\"\/kaggle\/working\/my_predictions.csv\", index=False)","e019a669":"# **1. Feature engineering \/ preprocessing**","210b1b80":"**1.2b Feature engineering - family size**","51460a5f":"# **2. Time to get some results**","80adb879":"**1.5 Extract train and test data back out of both**","95d21cb5":"# **0. Summary and contents**\n\n**0.1 Contents**\n\n    0 - Summary and contents\n        0.1 - Contents\n        0.2 - Summary\n    1 - Feature engineering \/ preprocessing\n        1.1 - Getting rid of useless columns and filling in null values\n        1.2 - Feature engineering\n        1.3 - Making columns numerical\n        1.4 - Correlating data and deciding which columns to keep\n        1.5 - Extracting train and test data from all data\n        1.6 - Scaling data\n    2 - Getting results\n\n**0.2 Summary**\n\nThe main part of this project is the processing of data and feature engineering with less of the focus on the model itself. I tried out a bunch of models and the random forest classifier (rfc) model performed significantly higher than any other in my first test, so I've continued to use it.\n\nThe data processing \/ feature enginerring is split into parts and is performed mainly with the pandas module. In addition, I use matplotlib for data visualisation and sklearn for data scaling and the model iself.","ae6a70b0":"**1.6 Scaling all columns so they are on an equal scale; that way no model will see one column as of higher value than another and become biased**","145ea392":"**1.4 Correlating data and deciding which columns to keep**","bb44e540":"**1.1 Getting rid of useless columns and filling in null values (with modes)**","6497209e":"**1.2c Feature engineering - parent**","0c7c9f5f":"**1.2a Feature engineering - VIP column**","c3b65400":"**1.3 Making all columns numerical**"}}