{"cell_type":{"b6656069":"code","4392a892":"code","1eccd160":"code","4f78d102":"code","f6b61481":"code","a5065d83":"code","736c6019":"code","b74bf23b":"code","168d3874":"code","d73dafd3":"code","3c160540":"code","2de60a16":"code","38c28444":"code","541b5656":"code","83866270":"code","3a021444":"code","f0384dd6":"code","52de75af":"code","b27795cc":"code","1ca1d99a":"code","697ea8d4":"code","448b0b64":"code","48002a15":"code","05343327":"code","4bf9b439":"code","e1ff342e":"code","0e40a389":"markdown","808bf4eb":"markdown","1288befa":"markdown","c1d6871e":"markdown","6981b73d":"markdown","ba606983":"markdown","b6d9bba2":"markdown","ef19bf27":"markdown","68258b57":"markdown","44ae28cd":"markdown","3d35cff2":"markdown","60717efd":"markdown","52fdae0b":"markdown","f0cd0070":"markdown","072c1caf":"markdown","8d3849a7":"markdown","610e7fb9":"markdown","23710de4":"markdown","fcaafaf9":"markdown","b339eb51":"markdown"},"source":{"b6656069":"# Downloading the necessary files and the dataset\nfrom IPython.display import clear_output\n\n!wget -cq -O problem_unittests.py https:\/\/raw.githubusercontent.com\/udacity\/deep-learning-v2-pytorch\/master\/project-face-generation\/problem_unittests.py\n!wget -cq -O processed_celeba_small.zip https:\/\/s3.amazonaws.com\/video.udacity-data.com\/topher\/2018\/November\/5be7eb6f_processed-celeba-small\/processed-celeba-small.zip\nclear_output()  \nprint(\"Downloaded Successfully\")","4392a892":"# Extractting the dataset\n!unzip -n processed_celeba_small.zip\nclear_output()\nprint(\"Extracted Successfully\")","1eccd160":"data_dir = 'processed_celeba_small\/'\n\nimport pickle as pkl\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport problem_unittests as tests\n#import helper\n\n%matplotlib inline","4f78d102":"# necessary imports\nimport torch\nfrom torchvision import datasets\nfrom torchvision import transforms\nfrom torch.utils.data import DataLoader","f6b61481":"def get_dataloader(batch_size, image_size, data_dir='processed_celeba_small\/'):\n    \"\"\"\n    Batch the neural network data using DataLoader\n    :param batch_size: The size of each batch; the number of images in a batch\n    :param img_size: The square size of the image data (x, y)\n    :param data_dir: Directory where image data is located\n    :return: DataLoader with batched data\n    \"\"\"\n    \n    # resize and normalize the images\n    transform = transforms.Compose([transforms.Resize(image_size),\n                                    transforms.ToTensor()])\n\n    # define datasets using ImageFolder\n    dataset = datasets.ImageFolder(data_dir, transform)\n\n    # create and return DataLoader\n    data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers = 4)\n\n    return data_loader","a5065d83":"# Define function hyperparameters\nbatch_size = 32\nimg_size = 32\n\n# Call your function and get a dataloader\nceleba_train_loader = get_dataloader(batch_size, img_size)","736c6019":"def imshow(img):\n    npimg = img.numpy()\n    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n\n# obtain one batch of training images\ndataiter = iter(celeba_train_loader)\nimages, _ = dataiter.next() # _ for no labels\n\n# plot the images in the batch, along with the corresponding labels\nfig = plt.figure(figsize=(20, 4))\nplot_size=20\nfor idx in np.arange(plot_size):\n    ax = fig.add_subplot(2, plot_size\/2, idx+1, xticks=[], yticks=[])\n    imshow(images[idx])","b74bf23b":"def scale(x, feature_range=(-1, 1)):\n    ''' Scale takes in an image x and returns that image, scaled\n       with a feature_range of pixel values from -1 to 1. \n       This function assumes that the input x is already scaled from 0-1.'''\n    min, max = feature_range\n    x = x * (max - min) + min\n    \n    return x","168d3874":"# checking scaled range\n# should be close to -1 to 1\nimg = images[0]\nscaled_img = scale(img)\n\nprint('Min: ', scaled_img.min())\nprint('Max: ', scaled_img.max())","d73dafd3":"import torch.nn as nn\nimport torch.nn.functional as F","3c160540":"# helper conv function -> a convolutional layer + an optional batch norm layer\ndef conv(in_channels, out_channels, kernel_size=4, stride=2, padding=1, batch_norm=True):\n    \"\"\"Creates a convolutional layer, with optional batch normalization.\n    \"\"\"\n    layers = []\n    conv_layer = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, \n                           kernel_size=kernel_size, stride=stride, padding=padding, bias=False)\n    \n    #appending convolutional layer\n    layers.append(conv_layer)\n    \n    #appending batch norm layer\n    if batch_norm:\n        layers.append(nn.BatchNorm2d(out_channels))\n        \n    return nn.Sequential(*layers)","2de60a16":"class Discriminator(nn.Module):\n\n    def __init__(self, conv_dim):\n        \"\"\"\n        Initialize the Discriminator Module\n        :param conv_dim: The depth of the first convolutional layer\n        \"\"\"\n        super(Discriminator, self).__init__()\n\n        # complete init function\n        self.conv_dim = conv_dim\n        \n        #leaky relu negative slope\n        self.leaky_relu_slope = 0.2\n        \n        # Convolutional layers, increasing in depth\n        # first layer has *no* batchnorm\n        \n       \n        self.conv1 = conv(3, conv_dim, batch_norm=False)  \n        self.conv2 = conv(conv_dim, conv_dim*2)           \n        self.conv3 = conv(conv_dim*2, conv_dim*4)         \n                \n        \n        # Classification layer -- final, fully-connected layer\n        self.fc = nn.Linear(conv_dim*4*4*4, 1)\n    \n    \n    def forward(self, x):\n        \"\"\"\n        Forward propagation of the neural network\n        :param x: The input to the neural network     \n        :return: Discriminator logits; the output of the neural network\n        \"\"\"\n        # define feedforward behavior\n        \n        # relu applied to all conv layers but last\n        out = F.leaky_relu(self.conv1(x), self.leaky_relu_slope)\n        out = F.leaky_relu(self.conv2(out), self.leaky_relu_slope)\n        out = F.leaky_relu(self.conv3(out), self.leaky_relu_slope)\n        \n        \n        out = out.view(-1, self.conv_dim*4*4*4) #flattening\n        \n        # last, classification layer\n        out = self.fc(out) \n        \n        return out","38c28444":"#deconv helper function, which creates a transpose convolutional layer + an optional batchnorm layer\ndef deconv(in_channels, out_channels, kernel_size=4, stride=2, padding=1, batch_norm=True):\n    \"\"\"Creates a transpose convolutional layer, with optional batch normalization.\n    \"\"\"\n    layers = []\n    \n    # append transpose conv layer -- we are not using bias terms in conv layers\n    layers.append(nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride, padding, bias=False))\n    \n    # optional batch norm layer\n    if batch_norm:\n        layers.append(nn.BatchNorm2d(out_channels))\n        \n    return nn.Sequential(*layers)","541b5656":"class Generator(nn.Module):\n    \n    def __init__(self, z_size, conv_dim):\n        \"\"\"\n        Initialize the Generator Module\n        :param z_size: The length of the input latent vector, z\n        :param conv_dim: The depth of the inputs to the *last* transpose convolutional layer\n        \"\"\"\n        super(Generator, self).__init__()\n\n        # complete init function\n        \n        self.conv_dim = conv_dim\n        \n        # first, fully-connected layer\n        self.fc = nn.Linear(z_size, conv_dim*4*4*4)\n\n        \n        # transpose conv layers\n        self.deconv1 = deconv(conv_dim*4, conv_dim*2)\n        self.deconv2 = deconv(conv_dim*2, conv_dim)\n        self.deconv3 = deconv(conv_dim, 3, batch_norm=False)\n        \n        \n        \n\n    def forward(self, x):\n        \"\"\"\n        Forward propagation of the neural network\n        :param x: The input to the neural network     \n        :return: A 32x32x3 Tensor image as output\n        \"\"\"\n        # define feedforward behavior\n        \n        ## fully-connected\n        x = self.fc(x)\n        \n        #retrieve the batch size\n        batch_size = x.shape[0]\n        \n        #reshape\n        x = x.view(-1, self.conv_dim*4, 4, 4)\n        \n        \n        # hidden transpose conv layers + relu\n        x = F.relu(self.deconv1(x))\n        x = F.relu(self.deconv2(x))\n       \n        \n        # last layer + tanh activation\n        x = self.deconv3(x)\n        x = torch.tanh(x)\n            \n        return x","83866270":"def weights_init_normal(m):\n    \"\"\"\n    Applies initial weights to certain layers in a model .\n    The weights are taken from a normal distribution \n    with mean = 0, std dev = 0.02.\n    :param m: A module or layer in a network    \n    \"\"\"\n    # classname will be something like:\n    # `Conv`, `BatchNorm2d`, `Linear`, etc.\n    classname = m.__class__.__name__\n        \n    # for every Linear layer and convolutional in a model..\n    if classname.find('Linear') != -1 or classname.find('Convo2d') != -1:\n        \n        #Fills the input Tensor with values drawn from the normal distribution\n        #mean =0 and standard deviation=0.02\n        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n        \n        #The bias terms or set to 0\n        m.bias.data.fill_(0)","3a021444":"def build_network(d_conv_dim, g_conv_dim, z_size):\n    # define discriminator and generator\n    D = Discriminator(d_conv_dim)\n    G = Generator(z_size=z_size, conv_dim=g_conv_dim)\n\n    # initialize model weights\n    D.apply(weights_init_normal)\n    G.apply(weights_init_normal)\n\n    print(D)\n    print()\n    print(G)\n    \n    return D, G","f0384dd6":"# Define model hyperparams\nd_conv_dim = 32\ng_conv_dim = 32\nz_size = 100\n\nD, G = build_network(d_conv_dim, g_conv_dim, z_size)","52de75af":"import torch\n\n# Check for a GPU\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\ntrain_on_gpu = torch.cuda.is_available()\nif not train_on_gpu:\n    print('No GPU found. Please use a GPU to train your neural network.')\nelse:\n    print('Training on GPU!')","b27795cc":"def real_loss(D_out, smooth=False):\n    '''Calculates how close discriminator outputs are to being real.\n       param, D_out: discriminator logits\n       return: real loss'''\n   \n    #retrieve the batch size\n    batch_size = D_out.size(0)\n    \n    # smooth, real labels = 0.9\n   \n    if smooth:\n        labels = torch.ones(batch_size)*0.9\n    else:\n        labels = torch.ones(batch_size)    # real labels = 1\n    \n    # move labels to GPU if available \n    if train_on_gpu:\n        labels = labels.cuda()\n        \n    # binary cross entropy with logits loss\n    criterion = nn.BCEWithLogitsLoss()\n    # calculate loss\n    loss = criterion(D_out.squeeze(), labels)\n    \n    \n    return loss\n\ndef fake_loss(D_out):\n    \n    '''Calculates how close discriminator outputs are to being fake.\n       param, D_out: discriminator logits\n       return: fake loss'''\n    \n    batch_size = D_out.size(0)\n    \n    labels = torch.zeros(batch_size) # fake labels = 0\n    \n    if train_on_gpu:\n        labels = labels.cuda()\n    \n    # The losses will by binary cross entropy loss with logits, which we can get with \n    # BCEWithLogitsLoss. This combines a sigmoid activation function and and binary \n    #cross entropy loss in one function.\n    criterion = nn.BCEWithLogitsLoss()\n    # calculate loss\n    loss = criterion(D_out.squeeze(), labels)\n    \n    \n    return loss","1ca1d99a":"import torch.optim as optim\n\nlr = 0.005\nbeta1 = 0.3\nbeta2 = 0.999 # default value\n\n# Create optimizers for the discriminator D and generator G\nd_optimizer = optim.Adam(D.parameters(), lr, betas=(beta1, beta2))\ng_optimizer = optim.Adam(G.parameters(), lr, betas=(beta1, beta2))","697ea8d4":"def train(D, G, n_epochs, print_every=2500):\n    '''Trains adversarial networks for some number of epochs\n       param, D: the discriminator network\n       param, G: the generator network\n       param, n_epochs: number of epochs to train for\n       param, print_every: when to print and record the models' losses\n       return: D and G losses'''\n    \n    # move models to GPU\n    if train_on_gpu:\n        D.cuda()\n        G.cuda()\n\n    # keep track of loss and generated, \"fake\" samples\n    samples = []\n    losses = []\n\n    # Get some fixed data for sampling. These are images that are held\n    # constant throughout training, and allow us to inspect the model's performance\n    sample_size=16\n    fixed_z = np.random.uniform(-1, 1, size=(sample_size, z_size))\n    fixed_z = torch.from_numpy(fixed_z).float()\n    # move z to GPU if available\n    if train_on_gpu:\n        fixed_z = fixed_z.cuda()\n\n    # epoch training loop\n    for epoch in range(n_epochs):\n\n        # batch training loop\n        for batch_i, (real_images, _) in enumerate(celeba_train_loader):\n\n            batch_size = real_images.size(0)\n            real_images = scale(real_images)\n            \n            \n\n            # ===============================================\n            #               TRAIN THE NETWORKS\n            # ===============================================\n            \n            d_optimizer.zero_grad()\n            \n            \n            # ---------TRAIN THE DISCRIMINATOR ----------------\n            \n            # Train the discriminator on real and fake images\n            \n            # Compute the discriminator losses on real images \n            if train_on_gpu:\n                real_images = real_images.cuda()\n                \n            D_real = D(real_images)\n            d_real_loss = real_loss(D_real)\n            \n            \n            # Train with fake images\n            # Generate fake images\n            z = np.random.uniform(-1, 1, size=(batch_size, z_size))\n            z = torch.from_numpy(z).float()\n            # move x to GPU, if available\n            if train_on_gpu:\n                z = z.cuda()\n            fake_images = G(z)  #generate fake images from generator\n            \n            \n            # Compute the discriminator losses on fake images\n            D_fake = D(fake_images)\n            d_fake_loss = fake_loss(D_fake)\n            \n            \n            # add up loss and perform backprop\n            # For the discriminator, the total loss is the sum of the losses for \n            # real and fake images, d_loss = d_real_loss + d_fake_loss.\n            d_loss = d_real_loss + d_fake_loss\n            d_loss.backward()\n            d_optimizer.step()\n            \n            \n            \n             # ---------TRAIN THE GENERATOR ----------------\n             \n             # Train the generator with an adversarial loss     \n             \n            g_optimizer.zero_grad()\n            \n            # Generate fake images\n            z = np.random.uniform(-1, 1, size=(batch_size, z_size))\n            z = np.random.uniform(-1, 1, size=(batch_size, z_size))\n            z = torch.from_numpy(z).float()\n            if train_on_gpu:\n                z = z.cuda()\n            \n            fake_images = G(z)\n            \n            # Compute the discriminator losses on fake images \n            # using flipped labels!\n            D_fake = D(fake_images) # pass the fake images to the discriminator as input\n            \n            g_loss = real_loss(D_fake) # use real loss to flip labels\n            \n            # perform backprop and optimization steps\n            g_loss.backward()\n            g_optimizer.step()\n            \n            # Print some loss stats\n            if batch_i % print_every == 0:\n                # append discriminator loss and generator loss\n                losses.append((d_loss.item(), g_loss.item()))\n                # print discriminator and generator loss\n                print('Epoch [{:5d}\/{:5d}] | d_loss: {:6.4f} | g_loss: {:6.4f}'.format(\n                        epoch+1, n_epochs, d_loss.item(), g_loss.item()))\n\n\n        ## AFTER EACH EPOCH##    \n        # generate and save sample, fake images\n        G.eval() # for generating samples\n        samples_z = G(fixed_z)\n        samples.append(samples_z)\n        G.train() # back to training mode\n\n    # Save training generator samples\n    with open('train_samples.pkl', 'wb') as f:\n        pkl.dump(samples, f)\n    \n    # finally return losses\n    return losses","448b0b64":"# set number of epochs \nn_epochs = 25\n\n# call training function\nlosses = train(D, G, n_epochs=n_epochs)","48002a15":"fig, ax = plt.subplots()\nlosses = np.array(losses)\nplt.plot(losses.T[0], label='Discriminator', alpha=0.5)\nplt.plot(losses.T[1], label='Generator', alpha=0.5)\nplt.title(\"Training Losses\")\nplt.legend()","05343327":"# helper function for viewing a list of passed in sample images\ndef view_samples(epoch, samples):\n    fig, axes = plt.subplots(figsize=(16,4), nrows=2, ncols=8, sharey=True, sharex=True)\n    for ax, img in zip(axes.flatten(), samples[epoch]):\n        img = img.detach().cpu().numpy()\n        img = np.transpose(img, (1, 2, 0))\n        img = ((img + 1)*255 \/ (2)).astype(np.uint8)\n        ax.xaxis.set_visible(False)\n        ax.yaxis.set_visible(False)\n        im = ax.imshow(img.reshape((32,32,3)))","4bf9b439":"# Load samples from generator, taken while training\nwith open('train_samples.pkl', 'rb') as f:\n    samples = pkl.load(f)","e1ff342e":"_ = view_samples(-1, samples)","0e40a389":"# **Visualize the CelebA Data**\n\nThe CelebA dataset contains over 200,000 celebrity images with annotations. These are color images with 3 color channels (RGB)#RGB_Images each.\n\n**Pre-process and Load the Data**\n\nEach of the CelebA images has been cropped to remove parts of the image that don't include a face, then resized down to 64x64x3 NumPy images. This pre-processed dataset is a smaller subset of the very large CelebA data.\n","808bf4eb":"# Deep Convolutional GANs\nA GAN using convolutional layers in the generator and discriminator is called a Deep Convolutional GAN, or DCGAN for short. The DCGAN architecture was first explored in 2016 and has seen impressive results in generating new images; you can read the [original paper, here](https:\/\/arxiv.org\/pdf\/1511.06434.pdf).\n\nSpecifically, we'll use a series of convolutional or transpose convolutional layers in the discriminator and generator. It's also necessary to use batch normalization to get these convolutional networks to train.","1288befa":"# **Training loss**\nPlotting the training losses for the generator and discriminator, recorded after each epoch.","c1d6871e":"## Creating the DataLoader","6981b73d":"# Face Generation\nIn this notebook, we will generate realistic fake faces, by defining and training a DCGAN. Our main objective is to get a generator network to generate new images of faces that look as realistic as possible!\n\n**Get Data**\nI'll be using the CelebFaces Attributes Dataset (CelebA) to train your adversarial networks.\n\n**Pre processed Data**\nEach of the CelebA images has been cropped to remove parts of the image that don't include a face, then resized down to 64x64x3 NumPy images. \n\nSource - The data has been taken from udacity's deep learning [github repository](https:\/\/github.com\/udacity).\n","ba606983":"# Building the whole network\nNow we'll call all the functions and make out network","b6d9bba2":"# Optimizers \nWe'll use the Adam optimizer with the right parameters.","ef19bf27":"# Training on GPU\nWe'll check if GPU is available or not and train our model in GPU if it is.","68258b57":"## Function to create DataLoader","44ae28cd":"# Visualising the pre-processed data\nFunction to visualise the images present in the data.","3d35cff2":"# **Discriminator and Generator Losses**\nNow we need to calculate the losses for both types of adversarial networks.\n\n## **Discriminator Losses**\n\nFor the discriminator, the total loss is the sum of the losses for real and fake images, d_loss = d_real_loss + d_fake_loss.\n\n## **Generator Loss**\nThe generator loss will look similar only with flipped labels. The generator's goal is to get the discriminator to think its generated images are real.\n","60717efd":"# **Generator samples from training**\nNow the faces we wanted to see that doesn't even exist in this world and are 100% fake created by the machine.","52fdae0b":"# **Define the Model**\nA GAN is comprised of two adversarial networks, a discriminator and a generator.\n\n# **Discriminator**\nWe will define the discriminator. This is a convolutional classifier. It is suggested we define this with normalization.\n\nThe inputs to the discriminator are 32x32x3 tensor images\n\nThe output is a single value that will indicate whether a given image is real or fake","f0cd0070":"## Discriminator Model","072c1caf":"# **Generator**\nThe generator should upsample an input and generate a new image of the same size as our training data 32x32x3. This should be mostly transpose convolutional layers with normalization applied to the outputs.\n\n\nThe inputs to the generator are vectors of some length z_size\n\nThe output is a image of shape 32x32x3.\n","8d3849a7":"### **Scaling the data**\n\nThe output of a tanh activated generator will contain pixel values in a range from -1 to 1,so we need to rescale our training images to range -1 to 1, which was 0 to 1 now.","610e7fb9":"# **Initialize the weights of your networks**\n\nTo help your models converge, we should initialize the weights of the convolutional and linear layers in your model. From reading the original DCGAN paper, they say:\n\n**All weights were initialized from a zero-centered Normal distribution with standard deviation 0.02.**\n","23710de4":"## Generator Model","fcaafaf9":"# **Training**\nTraining will involve alternating between training the discriminator and the generator. You'll use your functions real_loss and fake_loss to help you calculate the discriminator losses.\n\nWe are training the discriminator by alternating on real and fake images.\n\nThen the generator, which tries to trick the discriminator and should have an opposing loss function.","b339eb51":"Helper function to create convolutional layers with batch normalization."}}