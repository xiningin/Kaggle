{"cell_type":{"e075507b":"code","ed279fee":"code","5e105218":"code","caafb8f9":"code","2016e98e":"code","3cf4e1f7":"code","13fa0b2e":"code","89c2e44b":"code","b04666e3":"code","d97f7088":"code","2197a6e9":"code","791f9117":"code","d29e7b81":"code","00bf46d6":"code","c2b23d73":"code","59fca72e":"code","ae6ef70b":"code","63f9b73c":"code","3199f937":"code","f00f5a59":"code","ef3ae7b5":"code","d0730f84":"code","d90c7921":"code","15b06164":"code","85aeef91":"code","74d8d9b7":"code","1b7292cf":"code","5275b392":"code","bde2fa65":"code","d9075c97":"code","bd1b6733":"code","aeeef1a8":"code","cc8ab653":"code","6c1cccba":"code","c06541b1":"code","c95025b1":"code","a3f895fc":"code","3056c25a":"code","bbd1023e":"code","6663bb5a":"code","5db8a820":"code","db2d33d0":"code","cd4371ba":"code","fcfe4864":"code","d656405d":"code","e41f09ee":"code","4c1be0b7":"code","ead84ec1":"code","748f0e90":"code","aa4cad73":"code","e3a9085d":"code","a8409fa1":"code","7f21fb3c":"markdown","39a4ce9f":"markdown","4561fbab":"markdown","89e7c798":"markdown","f5f4132e":"markdown","1b260d97":"markdown","982977da":"markdown","bb439ada":"markdown","0bd340a2":"markdown","f29b35ed":"markdown","f7d6db41":"markdown","1fbd0064":"markdown","7e36bb8c":"markdown","728f2cde":"markdown","637cbd4b":"markdown","12c527e1":"markdown","dccb61cf":"markdown","7714a278":"markdown","8fe83ba3":"markdown","9628464e":"markdown","2ac7988c":"markdown","2322bc5f":"markdown","cce586d4":"markdown","eb3f5cdd":"markdown","5d560b59":"markdown","413e6ea9":"markdown","b4ab3777":"markdown","8ec82b40":"markdown","ea62321e":"markdown","3911ce56":"markdown","ddaea010":"markdown","4594b666":"markdown","9e70c734":"markdown"},"source":{"e075507b":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\npd.set_option('display.float_format', lambda x: '%.3f' % x)\npd.set_option('display.max_rows', 100)\npd.set_option('display.max_columns', 100)\nimport seaborn as sns\nimport gc\nimport pickle\nimport time\nfrom itertools import product\nimport optuna\nimport lightgbm as lgb\nfrom lightgbm import plot_importance\nimport sklearn.metrics\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import mean_squared_error\nprint('Done')","ed279fee":"# check data files\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","5e105218":"path = '..\/input\/competitive-data-science-predict-future-sales\/'\n\n# train files\nDF_sales       = pd.read_csv(path + 'sales_train.csv')\nDF_items       = pd.read_csv(path + 'items.csv')\nDF_item_cat    = pd.read_csv(path + 'item_categories.csv')\nDF_shops       = pd.read_csv(path + 'shops.csv')\n\n# test files\nDF_test        = pd.read_csv(path + 'test.csv')\nDF_sample_subs = pd.read_csv(path + 'sample_submission.csv')","caafb8f9":"print(f'there are {DF_sales.shape[0]} rows and {DF_sales.shape[1]} columns')\nDF_sales","2016e98e":"print(f'there are {DF_items.shape[0]} rows and {DF_items.shape[1]} columns')\nDF_items.head(5)","3cf4e1f7":"print(f'there are {DF_item_cat.shape[0]} rows and {DF_item_cat.shape[1]} columns')\nDF_item_cat.head(5)","13fa0b2e":"print(f'there are {DF_shops.shape[0]} rows and {DF_shops.shape[1]} columns')\nDF_shops.head(5)","89c2e44b":"print('---' * 10)\nprint('sales_train check\\n')\nprint(DF_sales.isna().sum())\nprint('---' * 10)\nprint('item_cat check\\n')\nprint(DF_items.isna().sum())\nprint('---' * 10)\nprint('item check\\n')\nprint(DF_item_cat.isna().sum())\nprint('---' * 10)\nprint('shops check\\n')\nprint(DF_shops.isna().sum())","b04666e3":"DF_sales.describe()","d97f7088":"sns.boxplot( x= DF_sales.item_cnt_day )","2197a6e9":"sns.boxplot( x= DF_sales.item_price )","791f9117":"## Drop item_price and item_cnt_day too high\nDF_sales = DF_sales[DF_sales.item_price<100000] # drop 1\nDF_sales = DF_sales[DF_sales.item_cnt_day<1000] # drop 2\n\n## Drop negative price \nDF_sales = DF_sales[DF_sales.item_price > 0].reset_index(drop=True) # drop 1\n\n## The item has been returned \nDF_sales.loc[DF_sales.item_cnt_day < 0, 'item_cnt_day'] = 0","d29e7b81":"DF_shops","00bf46d6":"print('shops_id 0 and 57\\n')\nprint(f'Shop_id 0  : Date = {DF_sales[DF_sales.shop_id == 0].date.min()} - {DF_sales[DF_sales.shop_id == 0].date.max()}')\nprint(f'Shop_id 57 : Date = {DF_sales[DF_sales.shop_id == 57].date.min()} - {DF_sales[DF_sales.shop_id == 57].date.max()}')\n\nprint('----'*10)\n\nprint('shops_id 1 and 58\\n')\nprint(f'Shop_id 1  : Date = {DF_sales[DF_sales.shop_id == 1].date.min()} - {DF_sales[DF_sales.shop_id == 1].date.max()}')\nprint(f'Shop_id 58 : Date = {DF_sales[DF_sales.shop_id == 58].date.min()} - {DF_sales[DF_sales.shop_id == 58].date.max()}')\n\nprint('----'*10)\n\nprint('shops_id 10 and 11\\n')\nprint(f'Shop_id 10  : Date = {DF_sales[DF_sales.shop_id == 10].date.min()} - {DF_sales[DF_sales.shop_id == 10].date.max()}')\nprint(f'Shop_id 11  : Date = {DF_sales[DF_sales.shop_id == 11].date.min()} - {DF_sales[DF_sales.shop_id == 11].date.max()}')","c2b23d73":"a = DF_sales.shop_id.unique()\nb = DF_test.shop_id.unique()\nprint(f'shop_id which only exists in one of the data ( train or test )\\n{set(a) - set(b)}')","59fca72e":"# \u042f\u043a\u0443\u0442\u0441\u043a \u041e\u0440\u0434\u0436\u043e\u043d\u0438\u043a\u0438\u0434\u0437\u0435, 56\nDF_sales.loc[DF_sales.shop_id == 0, 'shop_id'] = 57\n\n# \u042f\u043a\u0443\u0442\u0441\u043a \u0422\u0426 \"\u0426\u0435\u043d\u0442\u0440\u0430\u043b\u044c\u043d\u044b\u0439\"\nDF_sales.loc[DF_sales.shop_id == 1, 'shop_id'] = 58\n\n# \u0416\u0443\u043a\u043e\u0432\u0441\u043a\u0438\u0439 \u0443\u043b. \u0427\u043a\u0430\u043b\u043e\u0432\u0430 39\u043c\u00b2\nDF_sales.loc[DF_sales.shop_id == 11, 'shop_id'] = 10","ae6ef70b":"# extract city and category location from shop name\n\nDF_shops.loc[DF_shops.shop_name == '\u0421\u0435\u0440\u0433\u0438\u0435\u0432 \u041f\u043e\u0441\u0430\u0434 \u0422\u0426 \"7\u042f\"', 'shop_name'] = '\u0421\u0435\u0440\u0433\u0438\u0435\u0432\u041f\u043e\u0441\u0430\u0434 \u0422\u0426 \"7\u042f\"'  # fix name\nDF_shops['shop_city'] = DF_shops['shop_name'].str.split(' ').map(lambda x: x[0])\nDF_shops['shop_cat'] = DF_shops['shop_name'].str.split(' ').map(lambda x: x[1])\nDF_shops.head(5)","63f9b73c":"# label encoder for shops\n\nDF_shops['shop_city']  = LabelEncoder().fit_transform(DF_shops['shop_city'] )\nDF_shops['shop_cat'] = LabelEncoder().fit_transform(DF_shops['shop_cat'])\nDF_shops.drop(['shop_name'], axis=1, inplace= True)\nDF_shops.head(5)","3199f937":"# merge item data and item category data\n\nDF_items = pd.merge(DF_items, DF_item_cat, on = 'item_category_id')\nDF_items","f00f5a59":"# split item categories into several parts\n\nDF_items['item_sub_cat_1'] = np.select(\n    [DF_items.item_category_id.isin(range(0,8)),\n    DF_items.item_category_id.isin(range(10,18)),\n    DF_items.item_category_id.isin(range(18,32)),\n    DF_items.item_category_id.isin(range(32,37)),\n    DF_items.item_category_id.isin(range(37,42)),\n    DF_items.item_category_id.isin(range(42,55)),\n    DF_items.item_category_id.isin(range(55,61)),\n    DF_items.item_category_id.isin(range(61,73)),\n    DF_items.item_category_id.isin(range(73,79)),\n    DF_items.item_category_id.isin([8,80]),\n    DF_items.item_category_id==83, \n    DF_items.item_category_id==9,\n    DF_items.item_category_id==79,  \n    DF_items.item_category_id.isin([81,82])],\n\n    ['accessories','consoles','games','payment_cards','Cinema','books','music',\n     'gifts','programs','tickets','batteries','delivery','office','discs'])\n\nDF_items","ef3ae7b5":"# label encoder for item_category and drop some colomns\n\nDF_items['item_sub_cat_1'] = LabelEncoder().fit_transform(DF_items['item_sub_cat_1'])\nDF_items.drop(['item_name','item_category_name'], axis=1, inplace= True)\nDF_items","d0730f84":"# prepare train data\n\nDF_all = []\ncols = ['date_block_num','shop_id','item_id']\nfor i in range(34):\n    sales = DF_sales[DF_sales.date_block_num==i]\n    DF_all.append(np.array(list(product([i], sales.shop_id.unique(), sales.item_id.unique())), dtype='int16'))\n    \nDF_all = pd.DataFrame(np.vstack(DF_all), columns=cols)\nDF_all['date_block_num'] = DF_all['date_block_num'].astype(np.int8)\nDF_all['shop_id'] = DF_all['shop_id'].astype(np.int8)\nDF_all['item_id'] = DF_all['item_id'].astype(np.int16)\nDF_all.sort_values(cols,inplace=True)\nDF_all","d90c7921":"# prepare test data\n\nDF_test.drop(['ID'], axis=1, inplace = True)\nDF_test['date_block_num'] = 34\nDF_test['date_block_num'] = DF_test['date_block_num']\nDF_test['shop_id'] = DF_test['shop_id']\nDF_test['item_id'] = DF_test['item_id']\nDF_test.head()","15b06164":"# merger into data into one dataframe\n\nDF_all = pd.concat([DF_all, DF_test], ignore_index=True, sort=False, keys=cols)\nDF_all = pd.merge(DF_all, DF_shops, on=['shop_id'], how='left')\nDF_all = pd.merge(DF_all, DF_items, on=['item_id'], how='left')\nDF_all.fillna(0, inplace=True)\nDF_all","85aeef91":"# change type column to reduce memory use \n\nDF_all.date_block_num    = DF_all.date_block_num.astype(np.int8)\nDF_all.shop_id           = DF_all.shop_id.astype(np.int8)\nDF_all.item_id           = DF_all.item_id.astype(np.int16)\nDF_all.shop_city         = DF_all.shop_city.astype(np.int8)\nDF_all.shop_cat          = DF_all.shop_cat.astype(np.int8)\nDF_all.item_category_id  = DF_all.item_category_id.astype(np.int8)\nDF_all.item_sub_cat_1    = DF_all.item_sub_cat_1.astype(np.int8)","74d8d9b7":"def lag_feature(df, lags, col):\n    tmp = df[['date_block_num','shop_id','item_id',col]]\n    for i in lags:\n        shifted = tmp.copy()\n        shifted.columns = ['date_block_num','shop_id','item_id', col+'_lag_'+str(i)]\n        shifted['date_block_num'] += i\n        df = pd.merge(df, shifted, on=['date_block_num','shop_id','item_id'], how='left')\n    return df\n\ndef fill_na_test(df):\n    for col in df.columns:\n        if ('_lag' in col) & (df[col].isnull().any()):\n            print(col)\n            if ('month' in col):\n                df[col].fillna(0, inplace=True)         \n    return df","1b7292cf":"temp = DF_sales.groupby(['shop_id','item_id','date_block_num']).agg(item_cnt_month=('item_cnt_day',sum))\ntemp.columns = ['item_cnt_month']\ntemp.reset_index(inplace=True)\nDF_all = pd.merge(DF_all, temp, on=cols, how='left')\nDF_all['item_cnt_month'] = (DF_all['item_cnt_month']\n                                .fillna(0)\n                                .clip(0,20)\n                                .astype(np.float16))","5275b392":"ts = time.time()\n\n# item_cnt_month\n\nDF_all = lag_feature(DF_all, [1, 2, 3], 'item_cnt_month')\n\n\n# Monthly - item_cnt_month\n\ntemp = DF_all.groupby(['date_block_num']).agg({'item_cnt_month' : ['mean']})\ntemp.columns = ['avg_month']\ntemp.reset_index(inplace=True)\nDF_all = pd.merge(DF_all, temp, on=['date_block_num'], how='left')\nDF_all = lag_feature(DF_all, [1, 2, 3], 'avg_month')\nDF_all.drop(['avg_month'], axis=1, inplace= True)\n\n# Monthly item - item_cnt_month\n\ntemp = DF_all.groupby(['date_block_num', 'item_id']).agg({'item_cnt_month' : ['mean']})\ntemp.columns = ['avg_item_month']\ntemp.reset_index(inplace=True)\nDF_all = pd.merge(DF_all, temp, on=['date_block_num', 'item_id'], how='left')\nDF_all = lag_feature(DF_all, [1, 2, 3], 'avg_item_month')\nDF_all.drop(['avg_item_month'], axis=1, inplace= True)\n\n# Monthly shops - item_cnt_month\n\ntemp = DF_all.groupby(['date_block_num', 'shop_id']).agg({'item_cnt_month' : ['mean']})\ntemp.columns = ['avg_shop_month']\ntemp.reset_index(inplace=True)\nDF_all = pd.merge(DF_all, temp, on=['date_block_num', 'shop_id'], how='left')\nDF_all = lag_feature(DF_all, [1, 2, 3], 'avg_shop_month')\nDF_all.drop(['avg_shop_month'], axis=1, inplace= True)\n\n# Monthly item_category - item_cnt_month\n\ntemp = DF_all.groupby(['date_block_num', 'item_category_id']).agg({'item_cnt_month' : ['mean']})\ntemp.columns = ['avg_item_cat_month']\ntemp.reset_index(inplace=True)\nDF_all = pd.merge(DF_all, temp, on=['date_block_num', 'item_category_id'], how='left')\nDF_all = lag_feature(DF_all, [1,2], 'avg_item_cat_month')\nDF_all.drop(['avg_item_cat_month'], axis=1, inplace= True)\n\n# Monthly shops item_category - item_cnt_month\n\ntemp = DF_all.groupby(['date_block_num', 'shop_id', 'item_category_id']).agg({'item_cnt_month' : ['mean']})\ntemp.columns = ['avg_shops_itemcat_month']\ntemp.reset_index(inplace=True)\nDF_all = pd.merge(DF_all, temp, on=['date_block_num', 'shop_id', 'item_category_id'], how='left')\nDF_all = lag_feature(DF_all, [1,2], 'avg_shops_itemcat_month')\nDF_all.drop(['avg_shops_itemcat_month'], axis=1, inplace= True)\n\n# Monthly shops item - item_cnt_month\n\ntemp = DF_all.groupby(['date_block_num', 'shop_id', 'item_id']).agg({'item_cnt_month' : ['mean']})\ntemp.columns = ['avg_shops_item_month']\ntemp.reset_index(inplace=True)\nDF_all = pd.merge(DF_all, temp, on=['date_block_num', 'shop_id', 'item_id'], how='left')\nDF_all = lag_feature(DF_all, [1,2], 'avg_shops_item_month')\nDF_all.drop(['avg_shops_item_month'], axis=1, inplace= True)\n\n# Monthly shops subs_item category - item_cnt_month\n\ntemp = DF_all.groupby(['date_block_num', 'shop_id', 'item_sub_cat_1']).agg({'item_cnt_month' : ['mean']})\ntemp.columns = ['avg_shops_item_sub_month']\ntemp.reset_index(inplace=True)\nDF_all = pd.merge(DF_all, temp, on=['date_block_num', 'shop_id', 'item_sub_cat_1'], how='left')\nDF_all = lag_feature(DF_all, [1,2], 'avg_shops_item_sub_month')\nDF_all.drop(['avg_shops_item_sub_month'], axis=1, inplace= True)\n\n# Monthly shops_city - item_cnt_month\n\ntemp = DF_all.groupby(['date_block_num', 'shop_city']).agg({'item_cnt_month' : ['mean']})\ntemp.columns = ['avg_shopscity_month']\ntemp.reset_index(inplace=True)\nDF_all = pd.merge(DF_all, temp, on=['date_block_num', 'shop_city'], how='left')\nDF_all = lag_feature(DF_all, [1,2], 'avg_shopscity_month')\nDF_all.drop(['avg_shopscity_month'], axis=1, inplace= True)\n\nDF_all\n\nprint('Use time:', time.time() - ts)\n","bde2fa65":"# days, month, and year features\n\nDF_all['month'] = DF_all['date_block_num'] % 12\ndays = pd.Series([31,28,31,30,31,30,31,31,30,31,30,31])\nDF_all['days'] = DF_all['month'].map(days)\nDF_all['years'] = np.select(\n    [DF_all.date_block_num.isin(range(0,12)),\n     DF_all.date_block_num.isin(range(12,25)),\n     DF_all.date_block_num.isin(range(25,35))],\n    ['13','14','15'])\n\nDF_all['month'] = DF_all['month'] + 1  # fix month\n\n# change type column to reduce memory use\n\nDF_all['days'] = DF_all['days'].astype(np.int8)\nDF_all['month'] = DF_all['month'].astype(np.int8)\nDF_all['years'] = DF_all['years'].astype(np.int8)\nDF_all","d9075c97":"# The first month when one item is on sale\n\nDF_all['item_shop_first_sale'] = \\\nDF_all['date_block_num'] - DF_all.groupby(['item_id','shop_id'])['date_block_num'].transform('min')\n\nDF_all['item_first_sale'] = \\\nDF_all['date_block_num'] - DF_all.groupby('item_id')['date_block_num'].transform('min')\nDF_all","bd1b6733":"# check null data \nDF_all.isnull().any()","aeeef1a8":"# fill null data with zero (0)\nDF_all = fill_na_test(DF_all)\nDF_all.isnull().any()","cc8ab653":"DF_all.info()","6c1cccba":"DF_all.to_pickle('dataset.pkl')","c06541b1":"del DF_all\ndel temp\ndel DF_sales\ndel DF_items\ndel DF_item_cat\ndel DF_shops\ngc.collect()","c95025b1":"# read pickle data \n\ndata = pd.read_pickle('dataset.pkl')","a3f895fc":"data.columns","3056c25a":"data = data[[\n       \n       'date_block_num', 'shop_id', 'item_id', 'shop_city', 'shop_cat',\n       'item_category_id', 'item_sub_cat_1', 'item_cnt_month',\n       'item_cnt_month_lag_1', 'item_cnt_month_lag_2', 'item_cnt_month_lag_3',\n       'avg_month_lag_1', 'avg_month_lag_2', 'avg_month_lag_3',\n       'avg_item_month_lag_1', 'avg_item_month_lag_2', 'avg_item_month_lag_3',\n       'avg_shop_month_lag_1', 'avg_shop_month_lag_2', 'avg_shop_month_lag_3',\n       'avg_item_cat_month_lag_1', 'avg_item_cat_month_lag_2',\n       'avg_shops_itemcat_month_lag_1', 'avg_shops_itemcat_month_lag_2',\n       'avg_shops_item_month_lag_1', 'avg_shops_item_month_lag_2',\n       'avg_shops_item_sub_month_lag_1', 'avg_shops_item_sub_month_lag_2',\n       'avg_shopscity_month_lag_1', 'avg_shopscity_month_lag_2', 'month',\n       'days', 'years', 'item_shop_first_sale', 'item_first_sale'\n]]","bbd1023e":"data","6663bb5a":"# create train and validation data\n\nX_train = data[data.date_block_num < 33].drop(['item_cnt_month'], axis=1)\nY_train = data[data.date_block_num < 33]['item_cnt_month']\n\nX_valid = data[data.date_block_num == 33].drop(['item_cnt_month'], axis=1)\nY_valid = data[data.date_block_num == 33]['item_cnt_month']\n\nX_test  = data[data.date_block_num == 34].drop(['item_cnt_month'], axis=1)","5db8a820":"def rmsle(y, y_pred):\n    return np.sqrt(mean_squared_error(y, y_pred))\n\n\ndef plot_features(booster, figsize):    \n    fig, ax = plt.subplots(1,1,figsize=figsize)\n    return plot_importance(booster=booster, ax=ax)\n","db2d33d0":"lgb_train = lgb.Dataset(X_train, Y_train)\nlgb_eval  = lgb.Dataset(X_valid, Y_valid, reference=lgb_train)","cd4371ba":"def objective(trial):\n    \n    # choose parameters that you want\n    \n    param = {\n        \"objective\": \"regression\",\n        \"metric\": \"rmse\",\n        \"verbosity\": -1,\n        \"boosting_type\": \"gbdt\",\n        \"lambda_l1\": trial.suggest_float(\"lambda_l1\", 1e-8, 10.0, log=True),\n        \"lambda_l2\": trial.suggest_float(\"lambda_l2\", 1e-8, 10.0, log=True),\n        \"num_leaves\": trial.suggest_int(\"num_leaves\", 2, 256),\n        \"feature_fraction\": trial.suggest_float(\"feature_fraction\", 0.4, 1.0),\n        \"bagging_fraction\": trial.suggest_float(\"bagging_fraction\", 0.4, 1.0),\n        \"bagging_freq\": trial.suggest_int(\"bagging_freq\", 1, 7),\n        \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 5, 100),\n    }\n\n    model = lgb.train(param, \n                      lgb_train,\n                      valid_sets=[lgb_train,lgb_eval],\n                      early_stopping_rounds=15, #10,\n                      verbose_eval=1)\n    \n    y_pred = model.predict(X_valid)\n    accuracy = rmsle(Y_valid, y_pred)\n\n    return accuracy","fcfe4864":"study = optuna.create_study(direction='minimize')\nstudy.optimize(objective, n_trials=5)\n \nprint('Number of finished trials:', len(study.trials))\nprint('Best trial:', study.best_trial.params)","d656405d":"best_params = study.best_trial.params\nprint(f'Best trial parameters\\n{best_params}')","e41f09ee":"# add some fix params\n\nx = {\"objective\": \"regression\",\n     \"metric\"   : \"rmse\",\n     \"verbosity\": -1,\n     \"boosting_type\": \"gbdt\"}\n\nbest_params.update(x)\nbest_params","4c1be0b7":"evals_result = {} \n\nmodel = lgb.train(best_params,\n                  lgb_train,\n                  valid_sets=[lgb_train,lgb_eval],\n                  evals_result=evals_result,\n                  early_stopping_rounds=30, # 20\n                  verbose_eval=1,\n                  )","ead84ec1":"y_pred = model.predict(X_valid)\nrmsle(Y_valid, y_pred)","748f0e90":"print('feature importans')\nplot_features(model, (10,14))","aa4cad73":"print('Plot metrics recorded during training...')\nax = lgb.plot_metric(evals_result, figsize=(10, 5))","e3a9085d":"# predicting model\n\nY_test = model.predict(X_test).clip(0, 20)\n\nsubmission = pd.DataFrame({\n    \"ID\": DF_test.index, \n    \"item_cnt_month\": Y_test\n})\nsubmission.head(10)","a8409fa1":"submission.to_csv('submission.csv', index=False)","7f21fb3c":"## Time","39a4ce9f":"<h1> Introduction <\/h1>\n\n****\n\nIn this competition, we have to predict the total sales for each product and store in the next month. To solved this problem, we are given daily historical sales data from several stores in 33 months ( January 2013 to October 2015 ).\n\n****\n\nHello, this is my second notebook in this competition, if you want to see my other notebook, simple eda for this competition, its here [predict-future-sales-exploring-data](https:\/\/www.kaggle.com\/dhiiyaur\/predict-future-sales-exploring-data). The main idea of this notebook its to predict future sales with lgbm model and using optuna to find the best sets hyperparameters for model\n","4561fbab":"Aggregating item_cnt_month to a monthly level and clip variable, because target values are clipped into [0,20] range.","89e7c798":"## Optuna hyperparameters","f5f4132e":"***\n\n# References\n\n* https:\/\/github.com\/Microsoft\/LightGBM\n* https:\/\/github.com\/optuna\/optuna\n* https:\/\/www.kaggle.com\/corochann\/optuna-tutorial-for-hyperparameter-optimization\n* https:\/\/www.kaggle.com\/dlarionov\/feature-engineering-xgboost\n* https:\/\/www.kaggle.com\/kyakovlev\/1st-place-solution-part-1-hands-on-data\n","1b260d97":"## Load Data","982977da":"![download.jpg](attachment:download.jpg)","bb439ada":"## Extract Information\n\nin this section, I want to extract some information from data that will be useful in the prediction model\n* Shops\n* Items\n* Time ( lag features, mean encoding )","0bd340a2":"The store names are the same, but they don't open and close at the same time, let's check if store_id is in the test data.","f29b35ed":"****","f7d6db41":"Check missing values, and outliers from data","1fbd0064":"## Create a model","7e36bb8c":"****","728f2cde":"Optuna is an automatic hyperparameter optimization software framework, particularly designed for machine learning. It features an imperative, define-by-run style user API. with optuna, we can choose whatever parameters that we want to optimization","637cbd4b":"Some shops have the same name (shops_id 0 - 57, 1 - 58, and 11 - 10), let's check again based on the opening and closing dates of the store, and check on the test data.","12c527e1":"shops_id 0, 1, and 11 are not in the test data, I think we should merge the shop id to solve that problem","dccb61cf":"### Prepare data for model","7714a278":"## Items","8fe83ba3":"![download%20%281%29.jpg](attachment:download%20%281%29.jpg)","9628464e":"![d4754858e7376a236f5b8594c0e61677.jpg](attachment:d4754858e7376a236f5b8594c0e61677.jpg)","2ac7988c":"### Create Lag Features and Mean-Encodings\n\n* item_cnt_month\n* Monthly - item_cnt_month \n* Monthly item - item_cnt_month\n* Monthly shops - item_cnt_month\n* Monthly item_category - item_cnt_month\n* Monthly shops item_category - item_cnt_month\n* Monthly shops item - item_cnt_month\n* Monthly shops subs_item category - item_cnt_month\n* Monthly shops_city - item_cnt_month","2322bc5f":"Based on this data, it seems like there is no data missing","cce586d4":"<h1><center>Thank you for reading my notebook, upvote if you like this notebook :)<\/center><\/h1>\n","eb3f5cdd":"### Additional features\n\n* days, month, and year features\n* The first month when one item is on sale","5d560b59":"## Import necessary libraries","413e6ea9":"****","b4ab3777":"### File descriptions\n\n> * sales_train.csv - the training set. Daily historical data from January 2013 to October 2015.\n> * test.csv - the test set. You need to forecast the sales for these shops and products for November 2015.\n> * sample_submission.csv - a sample submission file in the correct format.\n> * items.csv - supplemental information about the items\/products.\n> * item_categories.csv  - supplemental information about the items categories.\n> * shops.csv- supplemental information about the shops.\n\n### Data fields\n\n> * ID - an Id that represents a (Shop, Item) tuple within the test set\n> * shop_id - unique identifier of a shop\n> * item_id - unique identifier of a product\n> * item_category_id - unique identifier of item category\n> * item_cnt_day - number of products sold. You are predicting a monthly amount of this measure\n> * item_price - current price of an item\n> * date - date in format dd\/mm\/yyyy\n> * date_block_num - a consecutive month number, used for convenience. January 2013 is 0, February 2013 is 1,..., October 2015 is 33\n> * item_name - name of item\n> * shop_name - name of shop\n> * item_category_name - name of item category","8ec82b40":"there are outliers from data","ea62321e":"Let's look at the train data file","3911ce56":"<h1><center> Predict Future Sales - Lightgbm hyperparameter Optuna <\/center><\/h1>\n\n****","ddaea010":"![a6253442820727.57d8e71562cb9.jpg](attachment:a6253442820727.57d8e71562cb9.jpg)","4594b666":"![optuna-logo.png](attachment:optuna-logo.png)","9e70c734":"## Shops"}}