{"cell_type":{"e3847a65":"code","57e8c495":"code","bbf3a799":"code","9af5c35f":"code","d34ca51e":"code","423509d5":"code","ca0422cf":"code","99619a9b":"code","0a106fbb":"code","424344bd":"code","08736349":"code","8b75a715":"code","bc445caf":"code","072248e9":"code","475b92e0":"code","cc349433":"code","e8d4008b":"code","c989b6a8":"code","e52ce837":"code","17853914":"code","1b015c97":"code","06962fba":"code","b8cd5995":"code","d77e6d0c":"code","a983c880":"code","14b167ce":"markdown","f3488bfa":"markdown","4fe22998":"markdown","eb7a2aa8":"markdown","fc637324":"markdown","567f29e9":"markdown","f44aea6f":"markdown","fc984128":"markdown","5b796e1d":"markdown","c72add70":"markdown","566afd00":"markdown","840487bb":"markdown","76f456f4":"markdown","3d856515":"markdown","2c8a7087":"markdown","7067ab84":"markdown","15495476":"markdown"},"source":{"e3847a65":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport math\nfrom collections import Counter\n\nimport os\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.preprocessing import StandardScaler","57e8c495":"train_data = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ngeneder_data = pd.read_csv('\/kaggle\/input\/titanic\/gender_submission.csv')\ntest_data = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')","bbf3a799":"train_data.head()","9af5c35f":"train_data.shape","d34ca51e":"def detect_outliers(data,n,features):\n    outlier_indices = []  \n    for col in features:\n        Q1 = np.percentile(data[col], 25)\n        Q3 = np.percentile(data[col],75)\n        IQR = Q3 - Q1  \n        outlier_step = 1.5 * IQR\n        outlier_list_col = data[(data[col] < Q1 - outlier_step) | (data[col] > Q3 + outlier_step )].index\n        outlier_indices.extend(outlier_list_col)\n    outlier_indices = Counter(outlier_indices)        \n    multiple_outliers = list( k for k, v in outlier_indices.items() if v > n )\n    return multiple_outliers   \n\nOutliers_to_drop = detect_outliers(train_data,2,[\"Age\",\"SibSp\",\"Parch\",\"Fare\"])\ntrain_data = train_data.drop(Outliers_to_drop, axis = 0).reset_index(drop=True)","423509d5":"train_data.isnull().sum()","ca0422cf":"#Removing 'Cabin' and organizing by dtype\noriginal_columns = ['PassengerId', 'Survived', 'Pclass', 'SibSp', 'Parch', 'Fare', 'Age', 'Sex', 'Embarked', 'Ticket', 'Name', 'Cabin']\nreorg_columns = ['PassengerId', 'Survived', 'Pclass', 'SibSp', 'Parch', 'Fare', 'Age', 'Sex', 'Embarked', 'Ticket', 'Name'] \ntrain_data_reorg = train_data[reorg_columns]\ntrain_data_reorg.head(20)","99619a9b":"def FillNumerical(data, column_names):\n    mean_per_column = data[column_names].apply(lambda x: x.mean(),axis=0)\n    numerical_fill = data[column_names].fillna(mean_per_column, axis=0)\n    return numerical_fill\n\ndef GetAgeMeans(data, age_column_name, name_column_name):\n    miss_list = []\n    mr_list = []\n    mrs_list = []\n    other_list = []\n    for row in data.index:\n        if 'Miss' in data[name_column_name][row]:\n            miss_list.append(data[age_column_name][row])\n        elif 'Mrs' in data[name_column_name][row]:\n            mrs_list.append(data[age_column_name][row])\n        elif 'Mr' in data[name_column_name][row]:\n            mr_list.append(data[age_column_name][row])\n        else: \n            other_list.append(data[age_column_name][row])\n    mean_dict = {'miss_mean': np.nanmean(miss_list), 'mrs_mean': np.nanmean(mrs_list), 'mr_mean': np.nanmean(mr_list), 'other_mean': np.nanmean(other_list)}\n    return mean_dict\n\ndef FillAges(data, age_column_name, name_column_name):\n    mean_dict = GetAgeMeans(data, age_column_name, name_column_name)\n    for row in data.index:\n        if math.isnan(data[age_column_name][row]):\n            if 'Miss' in data[name_column_name][row]:\n                data.loc[row, age_column_name] = mean_dict.get('miss_mean')\n            elif 'Mrs' in data[name_column_name][row]:\n                data.loc[row, age_column_name] = mean_dict.get('mrs_mean')\n            elif 'Mr' in data[name_column_name][row]:\n                data.loc[row, age_column_name] = mean_dict.get('mr_mean')\n            else: \n                data.loc[row, age_column_name] = mean_dict.get('other_mean')\n        else:\n            continue\n    age_data = data[age_column_name]\n    return age_data\n\ndef get_most_frequent_value(columns):\n    return columns.value_counts().index[0]\n\ndef FillCategorical(data, column_names):\n    most_common_val = data[column_names].apply(get_most_frequent_value,axis=0)\n    categorical_fill = data[column_names].fillna(most_common_val,axis=0)\n    return categorical_fill","0a106fbb":"numerical_cols = FillNumerical(train_data_reorg,reorg_columns[:6])\nage_col = FillAges(train_data_reorg, 'Age', 'Name')\ncat_cols = FillCategorical(train_data_reorg, reorg_columns[7:])\ncleaned_data = pd.concat((numerical_cols, age_col, cat_cols), sort=False, axis=1)\n\ncleaned_data.tail()","424344bd":"#Confirm that there are no zero values\ncleaned_data.isnull().sum()","08736349":"cleaned_data = pd.get_dummies(cleaned_data, columns=reorg_columns[7:9])\ncleaned_data.head()","8b75a715":"final_columns = [column for column in cleaned_data.columns]\ncleaned_data[final_columns].nunique()","bc445caf":"correlation = cleaned_data.corr()\nf, axes = plt.subplots(nrows=1, ncols=2, figsize = (16,6))\nx1 = sns.heatmap(correlation, center=0, vmin=-1, vmax=1, ax=axes[0]).set_title('All Data Correlations')\nx2 = sns.heatmap(correlation[['Survived']], center=0, vmin=-1, vmax=1, ax=axes[1]).set_title('Survival Correlations')","072248e9":"survived_by_pclass = cleaned_data.groupby('Pclass')['Survived'].agg([np.sum])\nf,axes = plt.subplots(1,3, figsize=(16,6))\naxes[0].pie(cleaned_data['Pclass'].value_counts()\/cleaned_data.shape[0]*100, labels=['Pclass3','Pclass1','Pclass2'], colors=['lightsteelblue','cornflowerblue','royalblue'], autopct='%1.1f%%')\naxes[0].axis('equal')\naxes[0].set_title('Distribution of Passengers\\n By Pclass')\naxes[1].hist(cleaned_data['Pclass'], bins=np.arange(5), align='left', rwidth=.9, color='cornflowerblue', stacked=True, label='Total Onboard')\naxes[1].bar(survived_by_pclass.index, survived_by_pclass[survived_by_pclass.columns[0]], color='navy', label='Survived')\naxes[1].set_title('PClass Distrobution and Survival Rate')\naxes[1].set_xlabel('Pclass')\naxes[1].set_ylabel('Number of Passengers')\naxes[1].legend(loc='upper_left')\naxes[1].set_xticks(range(4))\naxes[1].set_xlim(.5,3.5)\naxes[2].bar(survived_by_pclass.index, cleaned_data.groupby('Pclass')['Survived'].mean()*100, color='royalblue', alpha=.6)\naxes[2].set_title('Survival Percentage by Pclass')\naxes[2].set_xlabel('Pclass')\naxes[2].set_ylabel('Percentage of Surviving Passengers')\naxes[2].set_xticks(range(4))\naxes[2].set_xlim(.5,3.5)\nplt.show()","475b92e0":"pclass_distro = cleaned_data['Pclass'].value_counts()\/cleaned_data.shape[0]*100\nprint('Percentage of passengers in Pclass 1:', round(pclass_distro[1],2))\nprint('Percentage of passengers in Pclass 2:', round(pclass_distro[2],2))\nprint('Percentage of passengers in Pclass 3:', round(pclass_distro[3],2))","cc349433":"survived_by_gender = cleaned_data.groupby('Sex_female')['Survived'].agg([np.sum])\nf,axes = plt.subplots(1,3, figsize=(16,6))\naxes[0].pie(cleaned_data['Sex_female'].value_counts()\/cleaned_data.shape[0]*100, labels=['Male','Female'], colors=['olivedrab','yellowgreen'], autopct='%1.1f%%')\naxes[0].axis('equal') \naxes[0].set_title('Distribution of Passengers\\n By Sex')\naxes[1].hist(cleaned_data['Sex_female'], bins=np.arange(5), align='left', rwidth=.9, color='darkgreen', alpha=.4, stacked=True, label='Total Onboard')\naxes[1].bar(survived_by_gender.index, survived_by_gender[survived_by_gender.columns[0]], color='darkgreen', label='Survived')\naxes[1].set_title('Sex Distrobution and Survival Rate')\naxes[1].set_xlabel('Sex')\naxes[1].set_ylabel('Number of Passengers')\naxes[1].legend(loc='upper_left')\naxes[1].set_xticks([0,1])\naxes[1].set_xticklabels(['male', 'female'])\naxes[1].set_xlim(-1,2)\naxes[2].bar(survived_by_gender.index, cleaned_data.groupby('Sex_female')['Survived'].mean()*100, color='darkseagreen', alpha=.6)\naxes[2].set_title('Survival Percentage by Sex')\naxes[2].set_xlabel('Sex')\naxes[2].set_ylabel('Percentage of Surviving Passengers')\naxes[2].set_xticks([0,1])\naxes[2].set_xticklabels(['male', 'female'])\naxes[2].set_xlim(-1,2)\nplt.show()","e8d4008b":"class1_female = cleaned_data.groupby(['Sex_female','Pclass'])['Survived'].agg([np.mean])\nclass1_female","c989b6a8":"test_columns = ['PassengerId', 'Pclass', 'SibSp', 'Parch', 'Fare', 'Age', 'Sex', 'Embarked', 'Ticket', 'Name'] \ntest_data_reorg = test_data[test_columns]\ntest_data_reorg.dtypes","e52ce837":"numerical_cols_test = FillNumerical(test_data_reorg,test_columns[:5])\nage_col_test = FillAges(test_data_reorg, 'Age', 'Name')\ncat_cols_test = FillCategorical(test_data_reorg, test_columns[6:])\ncleaned_test_data = pd.concat((numerical_cols_test, age_col_test, cat_cols_test), sort=False, axis=1)\n\ncleaned_test_data.head()","17853914":"cleaned_test_data.isnull().sum()","1b015c97":"y = cleaned_data[\"Survived\"]\nfeatures = ['Pclass', 'Age', 'SibSp', 'Parch', 'Fare', 'Sex_female', 'Sex_male', 'Embarked_C', 'Embarked_Q', 'Embarked_S']\nX = cleaned_data[features]\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X).astype(float)","06962fba":"# Number of trees in random forest\nn_estimators = [int(x) for x in np.linspace(start = 100, stop = 2000, num = 10)]\n# Number of features to consider at every split\nmax_features = ['auto', 'sqrt']\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(1, 100, num = 10)]\nmax_depth.append(None)\n# Minimum number of samples required to split a node\nmin_samples_split = [8, 10, 12]\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 2]\n# Method of selecting samples for training each tree\nbootstrap = [True, False]\n# Create the random grid\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'bootstrap': bootstrap}\nprint(random_grid)","b8cd5995":"rf = RandomForestClassifier()\n\nrf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, scoring = 'accuracy', n_iter = 100, cv = 6, verbose=2, random_state=1234, n_jobs = -1)\nrf_random.fit(X_scaled, y)\n\nprint(rf_random.best_params_)\nprint(rf_random.best_score_)","d77e6d0c":"# Create the parameter grid based on the results of random search \nparam_grid = {\n    'bootstrap': [True],\n    'max_depth': [50, 100, 200],\n    'max_features': [2, 3],\n    'min_samples_leaf': [2, 3],\n    'min_samples_split': [10, 12],\n    'n_estimators': [300, 600, 2000, 3000]\n}\n# Create a based model\nrf = RandomForestClassifier()\n# Instantiate the grid search model\ngrid_search = GridSearchCV(estimator = rf, param_grid = param_grid, \n                          cv = 10, n_jobs = -1, verbose = 2)\ngrid_search.fit(X_scaled, y)\nprint(grid_search.best_params_)\nprint(grid_search.best_score_)","a983c880":"X_test = pd.get_dummies(cleaned_test_data[['Pclass', 'Age', 'SibSp', 'Parch', 'Fare','Sex', 'Embarked']])\ntest_scaler = StandardScaler()\nX_test_scaled = test_scaler.fit_transform(X_test).astype(float)\n\npredictions = grid_search.predict(X_test_scaled)\n\noutput = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions})\noutput.to_csv('my_submission_5.csv', index=False)\nprint(\"Your submission was successfully saved!\")","14b167ce":"## Reading & Cleaning The Data  \n\n**Importing Data & Packages**  \nTo explore the data before building a model, data processing and visualization packages are installed and all files are read into the document.","f3488bfa":"**Cleaning Test Data**  \nCalling the same funcitons used on the training data, missing values in the test data are filled in, dummy variables are created, and values are scaled. ","4fe22998":"***********","eb7a2aa8":"********","fc637324":"**Filling Missing Values**  \nWe can see below that of our 891 rows, the Cabin column is missing values in 687, which makes analysis of the column unimpactful and potentially even misleading. Given the missing data and the fact that Cabin, Fare, and Pclass are all indicative of eachother, we will therefore remove that column.  \n\nWe also see that the age column is missing 177 rows. Because we do have 80% of the data in that column, and there are not other columns that contain the same information, rather than removing that column, we will replace the null values with the average age of the rest of the column.\n\nWith only two rows missing from embarked, we will fill those with the most common value for embarked.","567f29e9":"## EXPLORATORY DATA ANALYSIS  \n\n\nFirst, taking a look at heatmap below, we can see that several features are strongly coorrelated which tells us a bit more about our data overall.  \n\n**Pclass and Sex are the strongest indicators of survival.**  \n\n1. Pclass: The Pclass, indicates the ticket class of the passenger and receives a value of 1, 2, or 3. The lower the Pclass, the more elite the ticket is (first class, second class, third class). It intuitively makes sense therefore that we see a negative correlation between Pclass and Fare as well as Pclass and Age. The pattern shows that younger passengers generally paid less money and are in a higher Pclass where as the older passengers were able to spend more money to be in a lower Pclass.  \n\n2. Survival: In the graph on the right, we look specifically at correlations between passenger features and survival. Fromt this view, we can see that being females has a strong positive correlation with surviving while being male greatly decreases the likelyhood of surviving. We also see a fairly strong negative correlation between Pclass and surviving indicating that those in the more elite class had a stronger chance of surviving. Lastly, we see a slight positive correlation etween Embarked_C and survival which mirrors the slight correlation we see in the first graph between Embarc_c and Pclass, indicating that more of the elite passengers boarded at Embark_c. ","f44aea6f":"## Cleaning Test Data & Building A Model","fc984128":"**Women were likely to survive the sinking of the Titanic.**  \n\nFollowing the same process as above, we can examine the distrobution of men and women onboard the Titanic and their relative survival risks. As seen in the left figure below, more than 2\/3 of passengers onboard the Titanic were males. However, we can see in the following two graphs that women were prioritized on the safety boats leading to roughly a 75% survival rate for womenn onboard. \n\nAgain in the middle graph displays the drastic death rate in the number of males. With only a 20% surivival rate, the number of males drops from nearly 600 to only about 100. ","5b796e1d":"**Random Search with Cross Validation**","c72add70":"###  Titanic Survival Analysis \n\n**Challenge**  \nPredict who will survive the sinking of the Titanic.   \n\n**Background**  \nOn April 15, 1912, during her maiden voyage, the widely considered \u201cunsinkable\u201d RMS Titanic ship sank after colliding with an iceberg. Without enough lifeboats onboard for all passengers to travel safely to short, 1502 out of 2224 passengers and crew died. Data collected on the ship's passengers provides visibility into the aspects that made some people more likely to survive.  \n\n**Goal**  \nUse relevant passenger data to build a predictive model that answers the question: \u201cwhat sorts of people were more likely to survive?\u201d  ","566afd00":"**Most deaths on the Titanic were from people in Pclass3.**  \n\nAs seen in the left figure below, most passengers (55%) were in Pclass3, while only 24% were in Pclass1, and 21% in Pclass2. However, the right graph shows that Pclass3 had the lowest survival rates at around 25% compared to Pclass1 and Pclass2 which had survival rates around 63% and 48% respectively. \n\nTo visualize the drastic disparity in number of deaths within each class, the number of survivors is overlayed onto the graph in the middle. The distance between the tall bar and short bar shows the number of deaths for that Pclass. Visually, the 75% death rate within Pclass3 is evident and contrasted to the 37% death rate in Pclass1.","840487bb":"This is my first kernel and an exciting step in my data science journey. I would love any pointers and recommendations. Thanks!","76f456f4":"**Grid Search with Cross Validation**","3d856515":"**Dealing With Outliers**  \nLeveraging the Q3, Q3, IRQ model, the code below identifies outlier, whihc are then dropped from the training data. ","2c8a7087":"**Random Forests & Pruning**  \nAfter the initial submission with this kernel which included a simple tree with set testing values, I've explored two pruning techniquues below, randomized search with cross validation and grid search with cross validation. Results from the two techniques were similar at 83% - 85% accuracy. However, the additional feature pruning did not have a significant impact on the test data accuracy. ","7067ab84":"*******","15495476":"As the final stage of the cleaning process, we will use one hot encoding to create dummy variables for our Sex and Embarked columns so that they are easier to analyze within the data set. "}}