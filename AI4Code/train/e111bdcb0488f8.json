{"cell_type":{"0676a103":"code","9f9cca1b":"code","0b83942c":"code","db9ac004":"code","b458fcc4":"code","631d59ae":"code","d89a10cb":"code","5fb5ea39":"code","5ae9f242":"code","54ea9c96":"code","bb872f0b":"code","b4dba15f":"code","9e0abb3c":"code","67e1f2d7":"code","999e21ef":"code","37acb0db":"code","f55c180a":"code","b3c7c7d8":"code","2a96d2df":"code","ae824b5a":"code","44d833ab":"code","1011d2ab":"code","51632a03":"markdown","bdf16a38":"markdown","112e3dbb":"markdown","103dad08":"markdown","40a40dbe":"markdown","973f1232":"markdown","73cd9e0c":"markdown","b79cad39":"markdown","da9787a8":"markdown","491bd51f":"markdown","2cd3e41c":"markdown","631d9fe1":"markdown"},"source":{"0676a103":"import cv2\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image","9f9cca1b":"image = Image.open('\/kaggle\/input\/5-celebrity-faces-dataset\/data\/train\/elton_john\/httpwwwlautdeEltonJohneltonjohnjpg.jpg')\n\nimage = image.convert('RGB')\n\nplt.imshow(image, cmap = 'gray', interpolation = 'bicubic')\nplt.show()","0b83942c":"from numpy import asarray\n# convert to array\npixels = asarray(image)","db9ac004":"!pip install mtcnn","b458fcc4":"# confirm mtcnn was installed correctly\nimport mtcnn\n# print version\nprint(mtcnn.__version__)","631d59ae":"from mtcnn.mtcnn import MTCNN\n# create the detector, using default weights\ndetector = MTCNN()","d89a10cb":"# detect faces in the image\nresults = detector.detect_faces(pixels)","5fb5ea39":"# extract the bounding box from the first face\nx1, y1, width, height = results[0]['box']\nx2, y2 = x1 + width, y1 + height","5ae9f242":"# extract the face\nface = np.array(image)[y1:y2, x1:x2]","54ea9c96":"plt.imshow(face)","bb872f0b":"from PIL import Image\n# resize pixels to the model size\nimage = Image.fromarray(face)\nimage = image.resize((160, 160))\nface_array = np.asarray(image)","b4dba15f":"plt.imshow(face_array)","9e0abb3c":"from keras.models import load_model\nmodel = load_model('\/kaggle\/input\/facenet\/facenet_keras.h5')\nprint('Loaded Model')\nprint(model.inputs)\nprint(model.outputs)","67e1f2d7":"import os\n\ndef extract_face(filename, required_size=(160, 160)):\n    # load image from file\n    image = Image.open(filename)\n    # convert to RGB, if needed\n    image = image.convert('RGB')\n    # convert to array\n    pixels = np.asarray(image)\n    # create the detector, using default weights\n    detector = MTCNN()\n    # detect faces in the image\n    results = detector.detect_faces(pixels)\n    # extract the bounding box from the first face\n    x1, y1, width, height = results[0]['box']\n    # deal with negative pixel index\n    x1, y1 = abs(x1), abs(y1)\n    x2, y2 = x1 + width, y1 + height\n    # extract the face\n    face = pixels[y1:y2, x1:x2]\n    # resize pixels to the model size\n    image = Image.fromarray(face)\n    image = image.resize(required_size)\n    face_array = np.asarray(image)\n    return face_array\n\ndef load_face(dir):\n    faces = list()\n    # enumerate files\n    for filename in os.listdir(dir):\n        path = dir + filename\n        face = extract_face(path)\n        faces.append(face)\n    return faces\n\ndef load_dataset(dir):\n    # list for faces and labels\n    X, y = list(), list()\n    for subdir in os.listdir(dir):\n        path = dir + subdir + '\/'\n        faces = load_face(path)\n        labels = [subdir for i in range(len(faces))]\n        print(\"loaded %d sample for class: %s\" % (len(faces),subdir) ) # print progress\n        X.extend(faces)\n        y.extend(labels)\n    return np.asarray(X), np.asarray(y)\n\n\n# load train dataset\ntrainX, trainy = load_dataset('\/kaggle\/input\/5-celebrity-faces-dataset\/data\/train\/')\nprint(trainX.shape, trainy.shape)\n# load test dataset\ntestX, testy = load_dataset('\/kaggle\/input\/5-celebrity-faces-dataset\/data\/val\/')\nprint(testX.shape, testy.shape)\n\n","999e21ef":"from numpy import expand_dims\n\n# get the face embedding for one face\ndef get_embedding(model, face_pixels):\n    # scale pixel values\n    face_pixels = face_pixels.astype('float32')\n    # standardize pixel values across channels (global)\n    mean, std = face_pixels.mean(), face_pixels.std()\n    face_pixels = (face_pixels - mean) \/ std\n    # transform face into one sample\n    samples = expand_dims(face_pixels, axis=0)\n    # make prediction to get embedding\n    yhat = model.predict(samples)\n    return yhat[0]","37acb0db":"# convert each face in the train set to an embedding\nnewTrainX = list()\nfor face_pixels in trainX:\n    embedding = get_embedding(model, face_pixels)\n    newTrainX.append(embedding)\nnewTrainX = asarray(newTrainX)\nprint(newTrainX.shape)\n# convert each face in the test set to an embedding\nnewTestX = list()\nfor face_pixels in testX:\n    embedding = get_embedding(model, face_pixels)\n    newTestX.append(embedding)\nnewTestX = asarray(newTestX)\nprint(newTestX.shape)","f55c180a":"df = pd.DataFrame(newTrainX)\ndf[\"target\"] = trainy\ndf.head()","b3c7c7d8":"import seaborn as sns\nfrom sklearn.decomposition import PCA\n# Create a PCA instance:\npca = PCA(n_components=2) \n# Fit pca to 'X'\npca_features = pca.fit_transform(newTrainX)\nprint (pca_features.shape)\n\ndf_plot = pd.DataFrame(pca_features)\ndf_plot[\"target\"] = trainy\n\nplt.figure(figsize=(16, 6))\nsns.scatterplot(x=df_plot[0] , y= df_plot[1], data = df_plot,  hue = \"target\" )","2a96d2df":"from sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import Normalizer","ae824b5a":"newTrainX.shape , newTestX.shape , testy.shape\n\nnormaliser = Normalizer()\nemdTrainX_norm = normaliser.fit_transform(newTrainX)\nemdTestX_norm = normaliser.transform(newTestX)\n\nencoder = LabelEncoder()\ntrainy_enc = encoder.fit_transform(trainy)\ntesty_enc = encoder.transform(testy)","44d833ab":"from sklearn.metrics import accuracy_score\nfrom sklearn.svm import SVC\nmodel = SVC(kernel='linear', probability=True)\nmodel.fit(emdTrainX_norm, trainy_enc)\n# predict\nyhat_train = model.predict(emdTrainX_norm)\nyhat_test = model.predict(emdTestX_norm)\n# score\nscore_train = accuracy_score(trainy_enc, yhat_train)\nscore_test = accuracy_score(testy_enc, yhat_test)\n# summarize\nprint('Accuracy: train=%.3f, test=%.3f' % (score_train*100, score_test*100))","1011d2ab":"from random import choice\n# select a random face from test set\nselection = choice([i for i in range(testX.shape[0])])\nrandom_face = testX[selection]\nrandom_face_emd = emdTestX_norm[selection]\nrandom_face_class = testy_enc[selection]\nrandom_face_name = encoder.inverse_transform([random_face_class])\n\n# prediction for the face\nsamples = np.expand_dims(random_face_emd, axis=0)\nyhat_class = model.predict(samples)\nyhat_prob = model.predict_proba(samples)\n# get name\nclass_index = yhat_class[0]\nclass_probability = yhat_prob[0,class_index] * 100\npredict_names = encoder.inverse_transform(yhat_class)\nall_names = encoder.inverse_transform([0,1,2,3,4])\n#print('Predicted: %s (%.3f)' % (predict_names[0], class_probability))\nprint('Predicted: \\n%s \\n%s' % (all_names, yhat_prob[0]*100))\nprint('Expected: %s' % random_face_name[0])\n# plot face\nplt.imshow(random_face)\ntitle = '%s (%.3f)' % (predict_names[0], class_probability)\nplt.title(title)\nplt.show()","51632a03":"* So Jerry Seinfeld is very different from Mindy Kaling, Elton John is less different from Ben Affleck than to Mindy Kaling. Female and Male seem separated by the First Principle Component of the PCA.","bdf16a38":"# Prepare dataset","112e3dbb":"# Data Visualisation with PCA","103dad08":"it is a good practice to normalize the face embedding vectors. It is a good practice because the vectors are often compared to each other using a distance metric.\n\nIn this context, vector normalization means scaling the values until the length or magnitude of the vectors is 1 or unit length. ","40a40dbe":"# prediction","973f1232":"## Resize to 160 x 160","73cd9e0c":"![](https:\/\/scikit-learn.org\/stable\/_images\/sphx_glr_plot_all_scaling_010.png)\nThe Normalizer rescales the vector for each sample to have unit norm, independently of the distribution of the samples. It can be seen on both figures below where all samples are mapped onto the unit circle. In our example the two selected features have only positive values; therefore the transformed data only lie in the positive quadrant. This would not be the case if some original features had a mix of positive and negative values.","b79cad39":"# Face Recognition","da9787a8":"# Classification using the features from the embedding","491bd51f":"## Face Detection\n\n* AKA drawing a bounding box around faces on an image\n* https:\/\/arxiv.org\/abs\/1604.02878 Zhang, Kaipeng et al. \u201cJoint Face Detection and Alignment Using Multitask Cascaded Convolutional Networks.\u201d IEEE Signal Processing Letters 23.10 (2016): 1499\u20131503. Crossref. Web.","2cd3e41c":"FaceNet is a face recognition system developed in 2015 by researchers at Google - and pre-trained models. It learns a mapping from face images to a compact Euclidean space where distances directly correspond to a measure of face similarity. The model is a deep convolutional neural network trained via a triplet loss function that encourages vectors for the same identity to become more similar (smaller distance), whereas vectors for different identities are expected to become less similar (larger distance). \n\n     * Schroff, Florian, Dmitry Kalenichenko, and James Philbin. \u201cFaceNet: A Unified Embedding for Face Recognition and Clustering.\u201d 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2015): n. pag. Crossref. Web.\n\n* **Face Verification** (Authentication) A one-to-one mapping of a given face against a known identity (e.g. is this the person?).\n* **Face Identification** (Recognition). A one-to-many mapping for a given face against a database of known faces (e.g. who is this person?).","631d9fe1":"# Create Face Embeddings\n\nA face embedding is a vector that represents the features extracted from the face. This can then be compared with the vectors generated for other faces. For example, another vector that is close (by some measure) may be the same person, whereas another vector that is far (by some measure) may be a different person.\n\nSource: https:\/\/machinelearningmastery.com\/how-to-develop-a-face-recognition-system-using-facenet-in-keras-and-an-svm-classifier\/\n\n\n* The FaceNet model can be used as part of the classifier itself, or we can use the FaceNet model to pre-process a face to create a face embedding that can be stored and used as input to our classifier model. This latter approach is preferred as the FaceNet model is both large and slow to create a face embedding."}}