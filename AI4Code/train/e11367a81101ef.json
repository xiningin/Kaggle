{"cell_type":{"5ba223c3":"code","39d126da":"code","33414319":"code","f57fe7b8":"code","a50bd8d4":"code","e3f2a554":"code","7a9f199d":"code","e3dd7e78":"code","22eb82c7":"code","c91b4e95":"code","cb04bb5e":"code","43930643":"code","bce5d215":"code","f3961d16":"code","ac403d45":"code","278fb532":"markdown","469c7e81":"markdown","f0eb6d14":"markdown","27e00fc5":"markdown","8ba2f41e":"markdown","6ef1db0e":"markdown","d23ba708":"markdown","ba7a6572":"markdown","87f1f09d":"markdown","0d1cac53":"markdown","bd6bad7b":"markdown","4158ad5e":"markdown"},"source":{"5ba223c3":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","39d126da":"data = pd.read_csv('..\/input\/ex1data1.txt' , header = None) #read from dataset\ndata.head(5)","33414319":"data.size","f57fe7b8":"X = np.array(data.iloc[:,0]) #read first column\ny = np.array(data.iloc[:,1]) #read second column\n\nm = len(y)\nprint(m)","a50bd8d4":"plt.scatter(X,y)\nplt.title('Population vs Profit')\nplt.xlabel('Population of City in 10,000s')\nplt.ylabel('Profit in $10,000s')\nplt.show()","e3f2a554":"# adding the intercept term\nones = np.ones((m,1)) \n# Add a column of ones to X. The numpy function stack joins arrays along a given axis. \n# The first axis (axis=0) refers to rows (training examples) \n# and second axis (axis=1) refers to columns (features).\nX = np.stack([np.ones(m), X], axis=1)\n#X","7a9f199d":"#Testing the cost function\ndef computeCost(X,y,theta):\n    J = (np.sum(np.power((np.dot(X, theta) - y),2)))\/(2*m)\n    return J","e3dd7e78":"#compute and display initial cost\ntheta = np.zeros(2)\nJ = computeCost(X, y, theta)\nprint('With theta = [0, 0] \\nCost computed = %.2f' % J)","22eb82c7":"theta = np.array([-1,2])\nJ = computeCost(X, y, theta)\nprint('With theta = [-1, 2]\\nCost computed = %.2f' % J)","c91b4e95":"\"\"\"\n    Performs gradient descent to learn `theta`. Updates theta by taking `num_iters`\n    gradient steps with learning rate `alpha`.\n    \n    Parameters\n    ----------\n    X : array_like\n        The input dataset of shape (m x n+1).\n    \n    y : arra_like\n        Value at given features. A vector of shape (m, ).\n    \n    theta : array_like\n        Initial values for the linear regression parameters. \n        A vector of shape (n+1, ).\n    \n    alpha : float\n        The learning rate.\n    \n    num_iters : int\n        The number of iterations for gradient descent. \n    \n    Returns\n    -------\n    theta : array_like\n        The learned linear regression parameters. A vector of shape (n+1, ).\n    \n    J_history : list\n        A python list for the values of the cost function after each iteration.\n    \n    Instructions\n    ------------\n    Peform a single gradient step on the parameter vector theta.\n\n    While debugging, it can be useful to print out the values of \n    the cost function (computeCost) and gradient here.\n    \"\"\"\ndef gradientDescent(X,y,theta,alpha,iterations):\n    m = y.size\n    J_history = np.zeros(iterations)\n    \n    for i in np.arange(iterations):\n        h = X.dot(theta)\n        theta = theta - alpha * (1\/m) * (X.T.dot(h-y))\n        J_history[i] = computeCost(X,y,theta)\n    return(theta, J_history)","cb04bb5e":"print('\\nRunning Gradient Descent ...\\n')\n#Some gradient descent settings\niterations = 1500\nalpha = 0.01\n\ntheta, Cost_J = gradientDescent(X, y, theta, alpha, iterations)\nprint('theta:', theta.ravel())\n\nplt.plot(Cost_J)\nplt.xlabel('Iterations')\nplt.ylabel('Cost_J')\nplt.show()","43930643":"J = computeCost(X, y, theta);\nprint('cost function with optimized value of theta: ',J)","bce5d215":"plt.scatter(X[:,1],y, label = 'Training Data')\nplt.title('Population vs Profit')\nplt.xlabel('Population of City in 10,000s')\nplt.ylabel('Profit in $10,000s')\nplt.plot(X[:,1], X.dot(theta),color='red',label='Hypothesis: h(x) = %0.2f + %0.2fx'%(theta[0],theta[1]))\nplt.legend(loc='best')\nplt.show()","f3961d16":"#% Predict values for population sizes of 35,000 and 70,000\n\npredict1 = np.dot([1, 3.5],theta) # takes inner product to get y\npredict2 = np.dot([1, 7],theta) # takes inner product to get y\n\nprint('For population = 35,000, we predict a profit of ', predict1*10000)\nprint('For population = 70,000, we predict a profit of ', predict2*10000)","ac403d45":"# grid over which we will calculate J\ntheta0_vals = np.linspace(-10, 10, 100)\ntheta1_vals = np.linspace(-1, 4, 100)\n\n# initialize J_vals to a matrix of 0's\nJ_vals = np.zeros((theta0_vals.shape[0], theta1_vals.shape[0]))\n\n# Fill out J_vals\nfor i, theta0 in enumerate(theta0_vals):\n    for j, theta1 in enumerate(theta1_vals):\n        J_vals[i, j] = computeCost(X, y, [theta0, theta1])\n        \n# Because of the way meshgrids work in the surf command, we need to\n# transpose J_vals before calling surf, or else the axes will be flipped\nJ_vals = J_vals.T\n\n# surface plot\nfig = plt.figure(figsize=(12, 5))\nax = fig.add_subplot(121, projection='3d')\nax.plot_surface(theta0_vals, theta1_vals, J_vals, cmap='viridis')\nplt.xlabel('theta0')\nplt.ylabel('theta1')\nplt.title('Surface')\n\n# contour plot\n# Plot J_vals as 15 contours spaced logarithmically between 0.01 and 100\nax = plt.subplot(122)\nplt.contour(theta0_vals, theta1_vals, J_vals, linewidths=2, cmap='viridis', levels=np.logspace(-2, 3, 20))\nplt.xlabel('theta0')\nplt.ylabel('theta1')\nplt.plot(theta[0], theta[1], 'ro', ms=10, lw=2)\nplt.title('Contour, showing minimum')\n\n","278fb532":"The purpose of these graphs is to show you how $J(\\theta)$ varies with changes in $\\theta_0$ and $\\theta_1$. The cost function $J(\\theta)$ is bowl-shaped and has a global minimum. (This is easier to see in the contour plot than in the 3D surface plot). This minimum is the optimal point for $\\theta_0$ and $\\theta_1$, and each step of gradient descent moves closer to this point.","469c7e81":"Predict profit for a city with population of 35000 and 70000 hypothesis equation h\u03b8","f0eb6d14":"**Visualizing J(theta_0, theta_1) **\n\nI have refered [github python code for machine learning](https:\/\/render.githubusercontent.com\/view\/ipynb?commit=441d6b601878bffded0aa5c0349c7edda77ed8db&enc_url=68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f64696267657267652f6d6c2d636f7572736572612d707974686f6e2d61737369676e6d656e74732f343431643662363031383738626666646564306161356330333439633765646461373765643864622f4578657263697365312f6578657263697365312e6970796e62&nwo=dibgerge%2Fml-coursera-python-assignments&path=Exercise1%2Fexercise1.ipynb&repository_id=121845101&repository_type=Repository#section2)","27e00fc5":"I have enrolled in coursera Machine learning by Prof. Andrew Ng, there is programming exercises in Octave. However I was curious to do the same exercises in Python. so here is my efforts. Hope it will help me to revise my concepts as well will help many new aspirant datascientists or someone who is learning machine learning.","8ba2f41e":"**2.2 Gradient Descent**\n\nIn this part, we will fit the linear regression parameters \u03b8 to our dataset using gradient descent.\nThe objective of linear regression is to minimize the cost function\n\n$$ J(\\theta) = \\frac{1}{2m} \\sum_{i=1}^m \\left( h_{\\theta}(x^{(i)}) - y^{(i)}\\right)^2$$ \nwhere the hypothesis $h_\\theta(x)$ is given by the linear model \n$$ h_\\theta(x) = \\theta^Tx = \\theta_0 + \\theta_1 x_1$$ \n\nRecall that the parameters of our model are the $\\theta_j$ values. These are \nthe values we will adjust to minimize cost $J(\\theta)$. One way to do this is to\nuse the batch gradient descent algorithm. In batch gradient descent, each \niteration performs the update\n\n$$ \\theta_j = \\theta_j - \\alpha \\frac{1}{m} \\sum_{i=1}^m \\left( h_\\theta(x^{(i)}) - y^{(i)}\\right)x_j^{(i)} \\qquad \\text{simultaneously update } \\theta_j \\text{ for all } j$$ \n\nWith each step of gradient descent, our parameters $\\theta_j$ come closer to the optimal values that will achieve the lowest cost J($\\theta$). \n\n**Implementation Note:** We store each example as a row in the the $X$ matrix in Python `numpy`. To take into account the intercept term ($\\theta_0$), we add an additional first column to $X$ and set it to all ones. This allows us to treat $\\theta_0$ as simply another 'feature'.,","6ef1db0e":"**2.2.3 Computing the cost J(\u03b8)**\n\nAs we perform gradient descent to learn minimize the cost function J(\u03b8), it is helpful to monitor the convergence by computing the cost. In this section, we will implement a function to calculate J(\u03b8) so we can check the convergence of our gradient descent implementation.\n\nFirst of all lets calculate Cost. remember that the variables X and y are not scalar values, but matrices whose rows represent the examples from the training set. Once we have completed the function, the next step is to run computeCost once using \u03b8 initialized to zeros, and we will see the cost printed to the screen.\n\nYou should expect to see a cost of 32.07","d23ba708":"If you like my efforts, please upvote. I value your suggesstions too. Thanks for stoping by. Stay Tuned for Linear Regression with Multiple Variables","ba7a6572":"**2.2.4 Gradient descent**\n\nNext calculate gradiantDecent. That will help us finding values for theta to reduce cost function..\n\nAs we program, Keep in mind that the cost J(\u03b8) is parameterized by the vector \u03b8, not X and y. That is, we minimize the value of J(\u03b8) by changing the values of the vector \u03b8, not by changing X or y. Refer to the equations in this handout and to the video lectures if you are uncertain from coursera machine learning by Prof. ANdrew Ng. A good way to verify that gradient descent is working correctly is to look at the value of J(\u03b8) and check that it is decreasing with each step.\n\nAssuming we have implemented gradient descent and computeCost correctly, our value of J(\u03b8) should never increase, and should converge to a steady value by the end of the algorithm.","87f1f09d":"Now compute costfunction with optimized value of theta.","0d1cac53":"**New cost function value is 4.48.. which is much better then 32.07**\n\nLets use these final\/optimized parameters to plot the linear fit. ","bd6bad7b":"**Linear regression with one variable**\n\nIn this part of this exercise, we will implement linear regression with one variable to predict profits for a food truck. Suppose you are the CEO of a restaurant franchise and are considering different cities for opening a new outlet. \nThe chain already has trucks in various cities and you have data for profits and populations from the cities. You would like to use this data to help you select which city to expand to next. \n\nThe file ex1data1.txt contains the dataset for our linear regression problem. The first column is the population of a city and the second column is the profit of a food truck in that city. A negative value for profit indicates a loss.","4158ad5e":"**2.1 Plotting the Data :**\n\nBefore starting on any task, it is often useful to understand the data by visualizing it. For this dataset, we can use a scatter plot to visualize the data.\nIt has only two properties to plot (profit and population). "}}