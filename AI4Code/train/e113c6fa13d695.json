{"cell_type":{"fda3a9fb":"code","d9a25e88":"code","d787b138":"code","cb5ed553":"code","312329d5":"code","ac8183ec":"code","7e61782c":"code","0a4df18f":"code","ac87a2d3":"code","3a339024":"code","adf71ee1":"code","039972e7":"code","5c19e258":"code","a3103754":"code","4effdf30":"code","5dd7d697":"code","c94fb6b0":"code","aecd9494":"code","3f928595":"code","7833303f":"code","6608603e":"code","2328ce0b":"markdown","5eba2638":"markdown","e479eb94":"markdown","de102424":"markdown","80916244":"markdown","65b82a3f":"markdown","7c5a680b":"markdown","8d38dfc2":"markdown","7bff86d3":"markdown","7e5b9871":"markdown","a9a5b168":"markdown","7e528d31":"markdown","3818a024":"markdown","faa97066":"markdown","3f89fe99":"markdown","d8989d43":"markdown","3f5012d6":"markdown","48f39b75":"markdown","9fb07b2b":"markdown","426e39da":"markdown","35834071":"markdown","e286af7c":"markdown"},"source":{"fda3a9fb":"import os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nfrom sklearn.preprocessing import LabelEncoder, MinMaxScaler\nfrom sklearn.model_selection import cross_val_score, StratifiedKFold\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nfrom scipy import stats\nimport math\n\nfrom sklearn.linear_model import LogisticRegression, Ridge, Lasso, ElasticNet\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier, ExtraTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, BaggingClassifier, ExtraTreesClassifier, GradientBoostingClassifier, VotingClassifier\nimport lightgbm as lgb\nimport xgboost as xgb\n\nimport warnings\nwarnings.simplefilter('ignore')\n\nimport gc\nimport itertools","d9a25e88":"train = pd.read_csv(\"..\/input\/X_train.csv\")\ntest = pd.read_csv(\"..\/input\/X_test.csv\")\nlabel = pd.read_csv(\"..\/input\/y_train.csv\")\nsub = pd.read_csv(\"..\/input\/sample_submission.csv\")","d787b138":"def reduce_mem_usage(df):\n    # iterate through all the columns of a dataframe and modify the data type\n    #   to reduce memory usage.        \n    \n    start_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n\n    for col in df.columns:\n        col_type = df[col].dtype\n\n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n\n    end_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) \/ start_mem))\n\n    return df\n\ndef plot_confusion_matrix(truth, pred, classes, normalize=False, title=''):\n    cm = confusion_matrix(truth, pred)\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n    \n    plt.figure(figsize=(10, 10))\n    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n    plt.title('Confusion matrix', size=15)\n    plt.colorbar(fraction=0.046, pad=0.04)\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.grid(False)\n    plt.tight_layout()","cb5ed553":"train = reduce_mem_usage(train)\ntest = reduce_mem_usage(test)","312329d5":"plt.figure(figsize=(15, 5))\nsns.countplot(label['surface'], order=label.surface.value_counts().index)\nplt.show()","ac8183ec":"# This is a dictionary consisting of the weights of the distributions for each target class from the below discussion thread.\n# https:\/\/www.kaggle.com\/c\/career-con-2019\/discussion\/85204#latest-496648\n\ndef create_valid_set(label):\n    # Lets try creating a validation set of 10% of the total size.\n    ldict = {\n        'concrete': 0.16,\n        'soft_pvc': 0.18,\n        'wood': 0.06,\n        'tiled': 0.03,\n        'fine_concrete': 0.10,\n        'hard_tiles_large_space': 0.12,\n        'soft_tiles': 0.23,\n        'carpet': 0.05,\n        'hard_tiles': 0.07,\n    }\n    score = 0\n    print(\"Required count of target classes for the Valid Set :: \")\n    for key, value in ldict.items():\n        score += value\n        print(key, int(value * 380)) # Multiplying by 380 i.e 10% of 3810 for our validation size of 10%.\n        ldict[key] = int(value * 380)\n    print(\"\\nTotal Weights of class :: \", score)\n    \n    # Grouping surface with group_id and the count attached to each surface.\n    ser = label.groupby(['surface'])['group_id'].value_counts()\n    ser = pd.DataFrame(ser)\n    ser.columns = ['count']\n    \n    # Maually creating the valid set using the counts using the required count and the count we have in the train set.\n    # This dictionary consists of the group_id for the required valid set. \n    cv_set = {\n        'concrete': [0],\n        'soft_pvc': [69],\n        'wood': [2],\n        'tiled': [28],\n        'fine_concrete': [36],\n        'hard_tiles_large_space': [16],\n        'soft_tiles': [4, 17],\n        'carpet': [52],\n        'hard_tiles': [27],\n    }\n\n    cv_size = 0\n    for key, value in cv_set.items():\n        print(key)\n        for i in value:\n            cv_size += label[label['group_id'] == i].shape[0]\n            print(\"\\nGot shape :: \", label[label['group_id'] == i].shape[0])\n        print(\"Expected shape :: \", ldict[key])\n    \n    val_df = pd.DataFrame()\n    for key, value in cv_set.items():\n        for i in value:\n            val_df = pd.concat([val_df, label[label['group_id'] == i]])\n    print(\"Valid Set Size :: \", val_df.shape[0])\n    \n    # We have only 1 group_id for the hard_tiles and it consists of only 21 records.\n    # So we have added the same group_id in the train as well as valid set. GROUP_ID = 27(for \"hard_tiles\")\n    hard_tiles_index = label[(label['surface'] == 'hard_tiles') & (label['group_id'] == 27)].index\n    \n    # Therefore train set = Total Set series_id - Valid Set series_id + Hard_Tiles.index\n    trn_series_id_list = list(set(label.series_id.unique()) - set(val_df.series_id.unique())) + hard_tiles_index.tolist()\n    \n    print(\"Train Set Distribution\")\n    print(label['surface'].iloc[trn_series_id_list].value_counts())\n    \n    print(\"Valid Set Distribution\")\n    print(label['surface'].iloc[val_df.index].value_counts())\n    \n    trn_df = label.iloc[trn_series_id_list]\n    \n    trn_df.set_index(['series_id'], inplace=True)\n    val_df.set_index(['series_id'], inplace=True)\n    \n    return trn_df, val_df","7e61782c":"# https:\/\/stackoverflow.com\/questions\/53033620\/how-to-convert-euler-angles-to-quaternions-and-get-the-same-euler-angles-back-fr?rq=1\ndef quaternion_to_euler(x, y, z, w):\n\n    t0 = +2.0 * (w * x + y * z)\n    t1 = +1.0 - 2.0 * (x * x + y * y)\n    X = math.atan2(t0, t1)\n\n    t2 = +2.0 * (w * y - z * x)\n    t2 = +1.0 if t2 > +1.0 else t2\n    t2 = -1.0 if t2 < -1.0 else t2\n    Y = math.asin(t2)\n\n    t3 = +2.0 * (w * z + x * y)\n    t4 = +1.0 - 2.0 * (y * y + z * z)\n    Z = math.atan2(t3, t4)\n    \n    return X, Y, Z","0a4df18f":"def FE(data):\n    df = pd.DataFrame()\n    \n    data['norm_quat'] = (data['orientation_X']**2 + data['orientation_Y']**2 + data['orientation_Z']**2 + data['orientation_W']**2)\n    data['mod_quat'] = (data['norm_quat'])**0.5\n    \n    data['norm_X'] = data['orientation_X'] \/ data['mod_quat']\n    data['norm_Y'] = data['orientation_Y'] \/ data['mod_quat']\n    data['norm_Z'] = data['orientation_Z'] \/ data['mod_quat']\n    data['norm_W'] = data['orientation_W'] \/ data['mod_quat']\n    \n    data['total_angular_velocity'] = (data['angular_velocity_X'] ** 2 + data['angular_velocity_Y'] ** 2 +\n                             data['angular_velocity_Z'] ** 2) ** 0.5\n    data['total_linear_acceleration'] = (data['linear_acceleration_X'] ** 2 + data['linear_acceleration_Y'] ** 2 +\n                             data['linear_acceleration_Z'] ** 2) ** 0.5\n    data['total_orientation'] = (data['orientation_X'] ** 2 + data['orientation_Y'] ** 2 +\n                             data['orientation_Z'] ** 2) ** 0.5\n    \n    data['acc_vs_vel'] = data['total_linear_acceleration'] \/ data['total_angular_velocity']\n    \n    x, y, z, w = data['orientation_X'].tolist(), data['orientation_Y'].tolist(), data['orientation_Z'].tolist(), data['orientation_W'].tolist()\n    nx, ny, nz = [], [], []\n    \n    for i in range(len(x)):\n        xx, yy, zz = quaternion_to_euler(x[i], y[i], z[i], w[i])\n        nx.append(xx)\n        ny.append(yy)\n        nz.append(zz)\n    \n    data['euler_x'] = nx\n    data['euler_y'] = ny\n    data['euler_z'] = nz\n    \n    data['total_angle'] = (data['euler_x'] ** 2 + data['euler_y'] ** 2 + data['euler_z'] ** 2) ** 0.5\n    data['angle_vs_acc'] = data['total_angle'] \/ data['total_linear_acceleration']\n    data['angle_vs_vel'] = data['total_angle'] \/ data['total_angular_velocity']\n    \n    def f1(x):\n        return np.mean(np.diff(np.abs(np.diff(x))))\n    \n    def f2(x):\n        return np.mean(np.abs(np.diff(x)))\n    \n    # Deriving more feature, since we are reducing rows now, we should know min, max, mean values\n    for col in data.columns:\n        if col in ['row_id', 'series_id', 'measurement_number']:\n            continue\n            \n        df[col + '_mean'] = data.groupby(['series_id'])[col].mean()\n        df[col + '_max'] = data.groupby(['series_id'])[col].max()\n        df[col + '_min'] = data.groupby(['series_id'])[col].min()\n        df[col + '_std'] = data.groupby(['series_id'])[col].std()\n        df[col + '_maxtoMin'] = df[col + '_max'] \/ df[col + '_min']\n        \n        df[col + '_abs_min'] = data.groupby(['series_id'])[col].apply(lambda x: np.min(np.abs(x)))\n        df[col + '_abs_max'] = data.groupby(['series_id'])[col].apply(lambda x: np.max(np.abs(x)))\n#         df[col + '_abs_std'] = data.groupby(['series_id'])[col].apply(lambda x: np.std(np.abs(x)))\n        df[col + '_abs_avg'] = (df[col + '_abs_min'] + df[col + '_abs_max'])\/2\n        \n        # Change. 1st order.\n        df[col + '_mean_abs_change'] = data.groupby('series_id')[col].apply(f2)\n        \n        # Change of Change. 2nd order.\n        df[col + '_mean_change_of_abs_change'] = data.groupby('series_id')[col].apply(f1)\n        \n    return df","ac87a2d3":"%%time\n\ntrn_df, val_df = create_valid_set(label)\ntrain = FE(train)\ntest = FE(test)","3a339024":"train.shape","adf71ee1":"le = LabelEncoder()\nlabel['surface'] = le.fit_transform(label['surface'])","039972e7":"train.fillna(0,inplace=True)\ntrain.replace(-np.inf,0,inplace=True)\ntrain.replace(np.inf,0,inplace=True)\ntest.fillna(0,inplace=True)\ntest.replace(-np.inf,0,inplace=True)\ntest.replace(np.inf,0,inplace=True)","5c19e258":"x_train = train.iloc[trn_df.index]\ny_train = label['surface'].iloc[trn_df.index]\n\nx_val = train.iloc[val_df.index]\ny_val = label['surface'].iloc[val_df.index]\n\nprint(x_train.shape, y_train.shape, x_val.shape, y_val.shape)","a3103754":"def lb_dist(model):\n    model.fit(x_train, y_train)\n    print(\"Train Acc :: \", accuracy_score(y_train, model.predict(x_train)))\n    print(\"Valid Acc :: \", accuracy_score(y_val, model.predict(x_val)))\n    print(\"CV Accuracy :: \", cross_val_score(rand, train, label['surface'], cv=5).mean())\n\n    return model","4effdf30":"rand = RandomForestClassifier(n_estimators=500, random_state=13)\nrand = lb_dist(rand)","5dd7d697":"plot_confusion_matrix(y_val, rand.predict(x_val), classes=le.classes_)","c94fb6b0":"print(\"Accuracy Score :: \", accuracy_score(label['surface'], rand.predict(train)))\nplot_confusion_matrix(label['surface'], rand.predict(train), classes=le.classes_)","aecd9494":"folds = StratifiedKFold(n_splits=10, shuffle=True, random_state=20)\npredicted = np.zeros((test.shape[0],9))\nmeasured= np.zeros((train.shape[0]))\nscore = 0","3f928595":"for times, (trn_idx, val_idx) in enumerate(folds.split(train.values, label['surface'].values)):\n    model = RandomForestClassifier(n_estimators=500, random_state=13)\n    model.fit(train.iloc[trn_idx], label['surface'][trn_idx])\n    measured[val_idx] = model.predict(train.iloc[val_idx])\n    predicted += model.predict_proba(test)\/folds.n_splits\n    score += model.score(train.iloc[val_idx], label['surface'][val_idx])\n    print(\"Fold: {} score: {}\".format(times, model.score(train.iloc[val_idx], label['surface'][val_idx])))\n    gc.collect()\n","7833303f":"sub['surface'] = le.inverse_transform(predicted.argmax(axis=1))\nsub.to_csv('rand_sub_10.csv', index=False)\nsub.head()","6608603e":"sub.surface.value_counts()","2328ce0b":"**Label Encoding our target classes**","5eba2638":"**Filling missing and infinite data by zeroes**","e479eb94":"This was my first kernel, so please spare me and everybody is welcome for their suggestions in the comments.\n\n** **CONCLUSION** :: Understanding the confusion matrix and solving the problem of the imbalance in the target classes is defintly the way to go for increasing the Accuracy Score.**","de102424":"**Checking our predictions for the Test Set**","80916244":"**The function below check the Train, Test, and CV Scores of the trained model.**\n\n**Lets Check.**","65b82a3f":"**We have very less records for \"hard_tiles\".**\n\nSo it would most probably be very difficult for our RandomForest, LightGBM to detect and correctly classify the hard_tiles.","7c5a680b":"**Importing Tools**","8d38dfc2":"**Submitting our Predictions**","7bff86d3":"**Thanks** for your time and **UPVOTE** if you liked it and decide to try the same. ","7e5b9871":"Well those three lines state my pain of matching local CV to LeaderBoard.\n\nLets understand what those 3 lines say:\n1. Our Classifier is overfitting like HELL.\n2. Cross Validation is also misleading as : \n            a. Test Distribution is different from the Train Distribution.\n            b. In CV some splits might end up not even considering the \"hard_tiles\" class as it has only 21 records.\n            c. The \"hard_tiles\" and \"soft_tiles\" hold quite a lot in the leaderboard.\n\nSo we must find features which could help us classifying classes like  :  \"hard_tiles\", \"soft_tiles\", \"tiled\", \"wood\" as they have very different Train and LeaderBoard distributions.","a9a5b168":"**This submission scores 0.71 on LeaderBoard and we got a Validation Accuracy of 0.64. I think that might be good enough.**","7e528d31":"**Lets first check the Train Target Distribution** ","3818a024":"<img src=\"https:\/\/i.imgur.com\/DoFc3mW.png\" \/>","faa97066":"We can see that the leaderboard has quite different distibition than in our test.\n\nThe different distributions lie in : \n\n**1. High Local Predictions for Wood but less on the Leaderboard set.**\n\n**2. High Local Predictions for Tiled but less on the Leaderboard set.**\n\n**3. Low Local Predictions for Soft_Tiles but high on the Leaderboard set.**\n\n**4. Many records of type hard_tiles in the Leaderboard set. **\n\nLets instigate more. :\/","3f89fe99":"**Lets check Confusion Matrix and see how our Classifier is doing.**","d8989d43":"**Understanding the Confusion Matrix**\n\nThis plot states so much that is very valuable in what to do next to **increase our SCORE.**\n\nWhat I learnt from this plot : \n\n**1. The diagonal in confusion matrix should always look bright i.e more populated in the whole matrix which means that the True Labels and the Predicted Labels must be same (or we should maximise that).**\n\nExample of a **Good Confusion Matrix **\n\n<img src=\"https:\/\/scikit-learn.org\/stable\/_images\/sphx_glr_plot_confusion_matrix_002.png\" \/>\n\n**2. Worst Classified Classes are :**\n\n        a. Carpet - 1 correctly classified out of 11.\n        b. Concrete - 57 out of 57 (Might be Overfit).\n        c. Fine_Concrete - 0 out of 36 (WHAT!!!)\n        d. Hard_Tiles - 21 out of 21 (Might be Overfit).\n        e. Hard_Tiles_Large_Space - 0 out of 45 (WHAT!!!)\n        f. Soft_PVC - 66 out of 70 (GREAT) \n        g. Soft_Tiles - 53 ou tof 69 (I can live with that.)\n        h. Tiled - 33 out of 36 (GREAT)\n        i. Wood - 0 out of 18 (You kidding?)\n\n\n**This Confusion Matrix reveals the major faults in our RandomForestClassifier.**\n\n**So fixing these faults would\/should mean an increase in score. (FOR SURE)**","3f5012d6":"**We can see that the Classifier is clearly overfitting by getting 96% in local CV and check the matrix!!**\n\n**That is one clean and bright diagonal.** (Just if I had that for the Leaderboard  **** *******__*******  )","48f39b75":"**Reading Data**","9fb07b2b":"References :: \n\nFeature Engineering : \n\n1. https:\/\/www.kaggle.com\/prashantkikani\/help-humanity-by-helping-robots\/\n\n2. https:\/\/www.kaggle.com\/jesucristo\/my-best-helping-robots-0-72\n\nLeaderBoard Probing\n\n1. https:\/\/www.kaggle.com\/c\/career-con-2019\/discussion\/84760\n\n2. https:\/\/www.kaggle.com\/c\/career-con-2019\/discussion\/85204\n \n\n","426e39da":"**Highly Influenced by the kernels : **\n\n1. https:\/\/www.kaggle.com\/prashantkikani\/help-humanity-by-helping-robots\/\n\n2. https:\/\/www.kaggle.com\/jesucristo\/my-best-helping-robots-0-72","35834071":"The below is a distribution of the leaderboard probe given in the thread : https:\/\/www.kaggle.com\/c\/career-con-2019\/discussion\/84760","e286af7c":"**Leaderboard Distribution**\n\nI have been doing this competetion for 4 days or so. \n\nThe main problem that everybody is (including me hitting my head through the WALL) facing is the **local cross validation is not matching the LeaderBoard.**\n\nIn this kernel I try to create a Validation Set which matches the leaderboard using LeaderBoard Probing done by **@donkeys and @ninoko.**\n\nThe leaderboard distributions are given in the discussion threads : \n\n1. https:\/\/www.kaggle.com\/c\/career-con-2019\/discussion\/84760\n2. https:\/\/www.kaggle.com\/c\/career-con-2019\/discussion\/85204\n\nA **big thank you** for wasting your 9 submissions for the greater GOOD. \n\nSo lets get into it."}}