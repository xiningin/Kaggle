{"cell_type":{"4dabecce":"code","178eaab9":"code","bdbe9218":"code","2edcdcde":"code","d618f459":"code","9b129022":"code","48c2c533":"code","e7f627cd":"code","0094ae49":"code","f2f05d0c":"code","146a57fc":"code","db9eb1a6":"code","f0c27f72":"code","bb97c610":"code","18443536":"code","d55e9a04":"code","d98421f2":"code","10173bf7":"code","f53c7337":"code","1f097d84":"code","c5262fda":"code","75af2b75":"code","13165d7d":"code","ae7741dc":"code","4d72fb30":"code","30b69c81":"code","6eef84d7":"code","725f7197":"code","22e7cc4a":"code","13f0c5c9":"code","d940e046":"code","3323543a":"code","f73b31f0":"code","e6d0dd8f":"code","24b2e077":"code","5d498b0b":"code","e9a8235e":"code","a0d34fed":"code","25659a9a":"code","4bd113de":"code","dae08e4e":"code","a23930aa":"code","b8f729b9":"code","8bbb0e91":"code","876e9c0f":"code","5096ecfe":"code","98ed9a99":"code","1a3c92b5":"code","c40babe4":"code","f4fd1519":"code","08702c0d":"code","d325cadc":"code","207af5c4":"code","eb303743":"code","c8b1cd3c":"code","209b7d31":"markdown","e23f2f04":"markdown","0aef4d26":"markdown","eee972c1":"markdown","2fac7c9e":"markdown","3f93231d":"markdown","03c33f00":"markdown","a655c2d0":"markdown","b50ac759":"markdown","8d357c71":"markdown","528e4b3e":"markdown","a1ebb696":"markdown","7519cd57":"markdown","c4021f49":"markdown","f200b36f":"markdown","6d0d8cf8":"markdown","af488de1":"markdown","bd82fdda":"markdown","c2d9edd4":"markdown","bc8339e1":"markdown","1affa296":"markdown","cf2d09e7":"markdown","a0802eff":"markdown","dc6e292b":"markdown","23e09ae0":"markdown","f7adf8c6":"markdown","ef62a089":"markdown","07ea9ed6":"markdown","161fb28a":"markdown","6c7124a4":"markdown","ddc3f237":"markdown","992f76b1":"markdown","08ac4aa9":"markdown","da395ee6":"markdown","34129453":"markdown"},"source":{"4dabecce":"import os\nimport gc\nimport sys\nimport time\nimport tqdm\nimport random\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport datatable as dt\nimport matplotlib.pyplot as plt\nplt.style.use('fivethirtyeight')\n\nimport plotly.express as px\nimport plotly.graph_objs as go\nimport plotly.figure_factory as ff\n\nfrom sklearn.decomposition import PCA\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.manifold import TSNE\n\nfrom colorama import Fore, Back, Style\ny_ = Fore.YELLOW\nr_ = Fore.RED\ng_ = Fore.GREEN\nb_ = Fore.BLUE\nm_ = Fore.MAGENTA\nc_ = Fore.CYAN\nsr_ = Style.RESET_ALL\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport lightgbm as lgb\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.utils.data import TensorDataset, DataLoader, Dataset\n\nfrom sklearn.metrics import roc_auc_score, mean_squared_error\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.model_selection import KFold, StratifiedKFold\nfrom sklearn.pipeline import FeatureUnion\nfrom sklearn.decomposition import PCA, TruncatedSVD\nfrom sklearn.preprocessing import (StandardScaler,PowerTransformer,\n                                   QuantileTransformer,LabelEncoder, \n                                   OneHotEncoder, OrdinalEncoder,\n                                  RobustScaler)","178eaab9":"folder_path = '..\/input\/tabular-playground-series-feb-2021'\ntrain_data = pd.read_csv(f'{folder_path}\/train.csv')\ntest_data = pd.read_csv(f'{folder_path}\/test.csv')\nsample = pd.read_csv(f'{folder_path}\/sample_submission.csv')","bdbe9218":"print(\"{0}Number of rows in train data: {1}{2}\\n{0}Number of columns in train data: {1}{3}\".format(y_,r_,train_data.shape[0],train_data.shape[1]))\nprint(\"{0}Number of rows in test data: {1}{2}\\n{0}Number of columns in test data: {1}{3}\".format(m_,r_,test_data.shape[0],test_data.shape[1]))\nprint(\"{0}Number of rows in sample : {1}{2}\\n{0}Number of columns in sample : {1}{3}\".format(c_,r_,sample.shape[0],sample.shape[1]))","2edcdcde":"train_data.head()","d618f459":"test_data.head()","9b129022":"sample.head()","48c2c533":"cont_features = [f'cont{i}' for i in range(14)]\ncat_features = [f'cat{i}' for i in range(10)]\nall_features = cont_features + cat_features","e7f627cd":"train_data.isnull().sum().sum()","0094ae49":"plt.style.use('fivethirtyeight')\ndef distribution1(feature,color1,color2,df=train_data):\n    plt.figure(figsize=(15,7))\n    \n    plt.subplot(121)\n    dist = sns.distplot(df[feature],color=color1)\n    a = dist.patches\n    xy = [(a[i].get_x() + a[i].get_width() \/ 2,a[i].get_height()) \\\n          for i in range(1,len(a)-1) if (a[i].get_height() > a[i-1].get_height() and a[i].get_height() > a[i+1].get_height())]\n    \n    for i,j in xy:\n        dist.annotate(\n            s=f\"{i:.3f}\",\n            xy=(i,j), \n            xycoords='data',\n            ha='center', \n            va='center', \n            fontsize=11, \n            color='black',\n            xytext=(0,7), \n            textcoords='offset points',\n        )\n    \n    qnt = df[feature].quantile([.25, .5, .75]).reset_index(level=0).to_numpy()\n    plt.subplot(122)\n    box = sns.boxplot(df[feature],color=color2)\n    for i,j in qnt:\n        box.annotate(str(j)[:4],xy= (j-.05,-0.01),horizontalalignment='center')\n        \n    print(\"{}Max value of {} is: {} {:.2f} \\n{}Min value of {} is: {} {:.2f}\\n{}Mean of {} is: {}{:.2f}\\n{}Standard Deviation of {} is:{}{:.2f}\"\\\n      .format(y_,feature,r_,df[feature].max(),g_,feature,r_,df[feature].min(),b_,feature,r_,df[feature].mean(),m_,feature,r_,df[feature].std()))","f2f05d0c":"distribution1('cont0','yellow','red')","146a57fc":"distribution1('target','red','blue')","db9eb1a6":"plt.figure(figsize=(20,15))\ncolors = ['#8ECAE6','#219EBC','#023047',\n          '#023047','#023047','#0E402D',\n          '#023047','#023047','#F77F00',\n          '#D62828','#4285F4','#EA4335',\n          '#FBBC05','#34A853']\nfor i,feature in enumerate(cont_features):\n    plt.subplot(2,7,i+1)\n    sns.distplot(train_data[feature],color=colors[i])","f0c27f72":"corr = train_data[all_features+['target']].corr()\nfig = px.imshow(corr)\nfig.show()","bb97c610":"def plot_grid(data,color1,color2,color3):\n    f = sns.PairGrid(data);\n    plt.figure(figsize=(10,10));\n    f.map_upper(plt.scatter,color = color1);\n    f.map_lower(sns.kdeplot,color = color2);\n    #f.map_diag(sns.histplot,color = c3 );\n    f.map_diag(sns.kdeplot, lw=3, legend=False,color = color3);","18443536":"plot_grid(train_data.loc[:1000,all_features+['target']],'#EA4335','#FBBC05','#34A853');","d55e9a04":"def pca_plot1(features,n_components,target,nrows=10**4):\n    pca = PCA(n_components=n_components)\n    train_d = train_data.sample(n=nrows).fillna(train_data.mean())\n    train_g_pca = pca.fit_transform(train_d[features])\n\n    total_var = pca.explained_variance_ratio_.sum()*100\n    labels = {str(i): f\"PC {i+1}\" for i in range(n_components)}\n\n    fig = px.scatter_matrix(\n        train_g_pca,\n        dimensions=range(n_components),\n        labels=labels,\n        title=f\"Total explained variance ratio{total_var:.2f}%\",\n        color=train_d[target].values\n    )\n\n    fig.update_traces(diagonal_visible=True,opacity=0.5)\n    fig.show()","d98421f2":"pca_plot1(cont_features,4,'target')","10173bf7":"def pca_plot_3d(features,target,nrows=10**4):\n    pca = PCA(n_components=3)\n    train_d = train_data.sample(n=nrows).fillna(train_data.mean())\n    train_g_pca = pca.fit_transform(train_d[features])\n\n    total_var = pca.explained_variance_ratio_.sum()*100\n\n    fig = px.scatter_3d(\n        train_g_pca,x=0,y=1,z=2,\n        title=f\"Total explained variance ratio{total_var:.2f}%\",\n        color=train_d[target].values,\n        labels={'0': 'PC 1', '1': 'PC 2', '2': 'PC 3'}\n    )\n\n    fig.show()","f53c7337":"pca_plot_3d(cont_features,'target')","1f097d84":"def plot_exp_var(features,nrows=10**4):\n    pca = PCA()\n    train_d = train_data.sample(n=nrows).fillna(train_data.mean())\n    pca.fit(train_d[features])\n    exp_var_cumul = np.cumsum(pca.explained_variance_ratio_)\n\n    fig = px.area(\n        x=range(1, exp_var_cumul.shape[0] + 1),\n        y=exp_var_cumul,\n        labels={\"x\": \"# Components\", \"y\": \"Explained Variance\"},\n    )\n    fig.show()","c5262fda":"plot_exp_var(cont_features)","75af2b75":"plt.style.use(\"ggplot\")\nplt.figure(figsize=(25,20))\nfor i,feature in enumerate(cat_features):\n    plt.subplot(2,5,i+1)\n    sns.countplot(train_data[feature])","13165d7d":"def distribution3(feature,category,df=train_data):\n    plt.subplots(figsize=(15, 7))\n    sns.histplot(train_data,x=feature,hue=category)","ae7741dc":"distribution3('cont0','cat9')","4d72fb30":"def boxploting1(feature,category,df=train_data,figure_size=(15,7)):\n    plt.subplots(figsize=figure_size)\n    sns.boxplot(x=feature, y=category, data=df,whis=[0, 100], width=.6, palette=\"vlag\")","30b69c81":"boxploting1('cont0','cat9')","6eef84d7":"def boxploting2(feature,category,df=train_data,figure_size=(15,7)):\n    plt.subplots(figsize=figure_size)\n    sns.boxenplot(y=feature, x=category,color=\"pink\",scale=\"linear\", data=df)","725f7197":"boxploting2('cont0','cat8',figure_size=(10,7))","22e7cc4a":"def distribution3(feature,category1,category2,df=train_data,figure_size=(15,15)):\n    sns.set_theme(style=\"dark\")\n    sns.displot(\n        data=df, x=feature, y=category1, col=category2,\n        log_scale=(True, False), col_wrap=4, height=4, aspect=.7,\n    )","13f0c5c9":"distribution3('cont5','cat8','cat9')","d940e046":"def swarmplot(feature,category1,category2,df=train_data,figure_size=(15,7)):\n    sns.set_theme(style=\"whitegrid\", palette=\"muted\")\n    plt.figure(figsize=figure_size)\n    ax = sns.swarmplot(data=df, x=feature, y=category1, hue=category2)\n    ax.set(ylabel=\"\")","3323543a":"swarmplot('target','cat8','cat9',df=train_data.sample(n=10000))","f73b31f0":"def scatterplot1(feature1,feature2,category,df=train_data):\n    fig = px.scatter(train_data, x=feature1, y=feature2, color=category, marginal_y=\"violin\",\n               marginal_x=\"box\", trendline=\"ols\", template=\"simple_white\")\n    fig.show()","e6d0dd8f":"scatterplot1('cont6','cont7','cat1')","24b2e077":"def errorbar(feature1,feature2,feature3,category,df=train_data):\n    df['e'] = df[feature3]\/100\n    fig = px.scatter(df, x=feature1, y=feature2, color=category, error_x=\"e\", error_y=\"e\")\n    fig.show()","5d498b0b":"errorbar('cont1','cont2','cont3','cat9',df=train_data.sample(n=1000))","e9a8235e":"def parallelcat(feature,df=train_data):\n    fig = px.parallel_categories(df, color=feature, color_continuous_scale=px.colors.sequential.Inferno)\n    fig.show()","a0d34fed":"parallelcat('cont2',df=train_data.sample(n=1000))","25659a9a":"def parallellines(feature,df=train_data):\n    fig = px.parallel_coordinates(df, color=feature,\n                    color_continuous_scale=px.colors.diverging.Tealrose, color_continuous_midpoint=2)\n    fig.show()","4bd113de":"parallellines('target',df=train_data.sample(n=1000))","dae08e4e":"def sunburst(category1,category2,df=train_data):\n    fig = px.sunburst(df, path=[category1, category2],\n                  color='target')\n    fig.show()","a23930aa":"sunburst('cat8','cat9',df=train_data.sample(n=1000))","b8f729b9":"folder_path = '..\/input\/tabular-playground-series-feb-2021'\ntrain_data = pd.read_csv(f'{folder_path}\/train.csv')\ntest_data = pd.read_csv(f'{folder_path}\/test.csv')\nsample = pd.read_csv(f'{folder_path}\/sample_submission.csv')\n\ncont_features = [f'cont{i}' for i in range(14)]\ncat_features = [f'cat{i}' for i in range(10)]\n# cat_features = ['cat1','cat3','cat5','cat8','cat9']\nall_features =   cat_features + cont_features\ntarget_feature = 'target'\n\nnum_bins = int(1 + np.log2(len(train_data)))\ntrain_data.loc[:,'bins'] = pd.cut(train_data['target'].to_numpy(),bins=num_bins,labels=False)\nbins = train_data['bins'].to_numpy()\n\nfor feat in cat_features:\n    le = LabelEncoder()\n    train_data.loc[:,feat] = le.fit_transform(train_data[feat].fillna(\"-1\"))\n    test_data.loc[:,feat] = le.transform(test_data[feat].fillna(\"-1\"))\n\n\nqt = QuantileTransformer()\ntrain_data.loc[:,cont_features] = qt.fit_transform(train_data.loc[:,cont_features])\ntest_data.loc[:,cont_features] = qt.transform(test_data.loc[:,cont_features])\n\nemb_c = {cat: int(train_data[cat].nunique()) for cat in cat_features if int(train_data[cat].nunique()) >2}\nemb_cols = emb_c.keys()\nembedding_sizes = [(c, min(50, (c+1)\/\/2)) for _,c in emb_c.items()]\ncont_features = cont_features + [cat for cat in cat_features if cat not in emb_cols]\ncat_features = emb_cols\n\n# target = train_data[target_feature].to_numpy()\n# train_data = train_data[all_features].to_numpy()\n# test_data = test_data[all_features].to_numpy()\n\ntrain_data.shape, test_data.shape","8bbb0e91":"def rmse_score(y_true,y_pred):\n    return np.sqrt(mean_squared_error(y_true,y_pred))","876e9c0f":"seed = 2021\n\ndef seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    \nseed_everything(seed=seed)","5096ecfe":"config = {\n    \"epochs\":15,\n    \"train_batch_size\":1024,\n    \"valid_batch_size\":1024,\n    \"test_batch_size\":1024,\n    \"nfolds\":5, \n    \"learning_rate\":0.001,\n    \n    \"input_size\":len(all_features), \n    'cont_size':len(cont_features),\n    'hidden_sizes':[128,64,32,16],\n    'output_size':1\n}","98ed9a99":"class TPSDataset(Dataset):\n    def __init__(self,df,cat_features,cont_features):\n        self.cat_data = df.loc[:,cat_features].to_numpy()\n        self.cont_data = df.loc[:,cont_features].to_numpy()\n        self.target = df.loc[:,target_feature].to_numpy()\n    \n    def __getitem__(self,idx):\n        input1 = torch.tensor(self.cat_data[idx],dtype=torch.long)\n        input2 = torch.tensor(self.cont_data[idx],dtype=torch.long)\n        target = torch.tensor(self.target[idx],dtype=torch.float)\n        return input1,input2, target\n    \n    def __len__(self):\n        return len(self.target)","1a3c92b5":"class Model(nn.Module):\n    def __init__(self,cont_size,output_size,hidden_sizes,embedding_sizes):\n        super().__init__()\n        self.embeddings = nn.ModuleList([nn.Embedding(cat,size) for (cat,size) in embedding_sizes])\n        self.emb_size = sum(e.embedding_dim for e in self.embeddings)\n        self.cont_size = cont_size\n            \n        self.layer1 = self.batch_linear_drop(self.emb_size+self.cont_size,hidden_sizes[0],0.1,activation=nn.ReLU)\n        self.layer2 = self.batch_linear_drop(hidden_sizes[0],hidden_sizes[1],0.1,activation=nn.ReLU)\n        self.layer3 = self.batch_linear_drop(hidden_sizes[1],hidden_sizes[2],0.1,activation=nn.ReLU)\n        self.layer4 = self.batch_linear_drop(hidden_sizes[2],hidden_sizes[3],0.1,activation=nn.ReLU)\n        self.layer5 = self.batch_linear(hidden_sizes[3],output_size)\n        \n    def batch_linear_drop(self,inp,out,drop,activation=None):\n        if activation:\n            return nn.Sequential(nn.BatchNorm1d(inp),nn.Dropout(drop),nn.Linear(inp,out),activation())\n        else:\n            return nn.Sequential(nn.BatchNorm1d(inp),nn.Dropout(drop),nn.Linear(inp,out))\n            \n    def batch_linear(self,inp,out,activation=None):\n        if activation:\n            return nn.Sequential(nn.BatchNorm1d(inp),nn.Linear(inp,out),activation())\n        else:\n            return nn.Sequential(nn.BatchNorm1d(inp),nn.Linear(inp,out))\n    \n    def forward(self,input1,input2):\n        x = [e(input1[:,i]) for i,e in enumerate(self.embeddings)]\n        x = torch.cat(x,1)\n        x = torch.cat([x,input2],1)\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n        x = self.layer5(x)\n        return x","c40babe4":"def run(plot_losses=True,verbose=True):\n    \n    def loss_fn(outputs,targets):\n        loss = nn.MSELoss()(outputs,targets)\n        return loss\n  \n    def train_loop(train_loader,model,loss_fn,device,optimizer,lr_scheduler=None):\n        model.train()\n        total_loss = 0\n        for i, (inputs1,inputs2, targets) in enumerate(train_loader):\n            inputs1 = inputs1.to(device)\n            inputs2 = inputs2.to(device)\n            targets = targets.to(device)\n            optimizer.zero_grad()\n            \n            outputs = model(inputs1,inputs2)\n            loss = loss_fn(outputs,targets)\n            loss.backward()\n                \n            total_loss += loss.item()\n\n            optimizer.step()\n                    \n        total_loss \/= len(train_loader)\n        return total_loss\n    \n    def valid_loop(valid_loader,model,loss_fn,device):\n        model.eval()\n        total_loss = 0\n        predictions = list()\n        \n        for i, (inputs1,inputs2,targets) in enumerate(valid_loader):\n            inputs1 = inputs1.to(device)\n            inputs2 = inputs2.to(device)\n            targets = targets.to(device)\n            \n            outputs = model(inputs1,inputs2)\n            loss = loss_fn(outputs,targets)\n\n            predictions.extend(outputs.detach().cpu().numpy())\n            \n            total_loss += loss.item()\n        total_loss \/= len(valid_loader)\n            \n        return total_loss,np.array(predictions)\n    \n    \n    fold_train_losses = list()\n    fold_valid_losses = list()\n    \n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    print(f\"{device} is used\")\n    \n#     kfold = KFold(n_splits=config['nfolds'])\n    kfold = StratifiedKFold(n_splits=config['nfolds'])\n    for k , (train_idx,valid_idx) in enumerate(kfold.split(X=train_data,y=bins)):\n        x_train,x_valid = train_data.iloc[train_idx,:],train_data.iloc[valid_idx,:]\n\n        input_size = config['cont_size']\n        hiddens_sizes = config['hidden_sizes']\n        output_size = 1\n\n        model = Model(input_size,output_size,hiddens_sizes,embedding_sizes)\n        model.to(device)\n\n        train_ds = TPSDataset(x_train,cat_features,cont_features)\n        train_dl = DataLoader(train_ds,\n                             batch_size = config[\"train_batch_size\"],\n                              shuffle=True,\n                              num_workers = 4,\n                              pin_memory=True\n                             )\n\n        valid_ds = TPSDataset(x_valid,cat_features,cont_features)\n        valid_dl = DataLoader(valid_ds,\n                              batch_size =config[\"valid_batch_size\"],\n                              shuffle=False,\n                              num_workers = 4,\n                              pin_memory=True,\n                             )\n        \n        optimizer = optim.Adam(model.parameters(),lr=config['learning_rate'])\n        lr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.8, patience=2, verbose=True)\n#         lr_scheduler = optim.lr_scheduler.StepLR(optimizer,step_size=10,gamma=0.8)\n#         lr_scheduler = None\n\n        print(f\"Fold {k}\")\n        best_loss = 999\n        \n        train_losses = list()\n        valid_losses = list()\n        start = time.time()\n        for i in range(config[\"epochs\"]):\n            train_loss = train_loop(train_dl,model,loss_fn,device,optimizer,lr_scheduler=lr_scheduler)\n            valid_loss,predictions = valid_loop(valid_dl,model,loss_fn,device)\n            \n            if lr_scheduler:\n                lr_scheduler.step(valid_loss)\n\n            train_losses.append(train_loss)\n            valid_losses.append(valid_loss)\n            \n            end = time.time()\n            epoch_time = end - start\n            start = end\n            \n            score = rmse_score(x_valid[target_feature],predictions)\n                          \n            if verbose:\n                print(f\"epoch:{i} Training loss:{train_loss} | Validation loss:{valid_loss}| Score: {score}  | epoch time {epoch_time:.2f}s \")\n\n            if valid_loss <= best_loss:\n                if verbose:\n                    print(f\"{g_}Validation loss Decreased from {best_loss} to {valid_loss}{sr_}\")\n                best_loss = valid_loss\n                torch.save(model.state_dict(),f'model{k}.bin')\n                \n        fold_train_losses.append(train_losses)\n        fold_valid_losses.append(valid_losses)\n#         break\n      \n        \n    if plot_losses == True:\n        plt.figure(figsize=(20,14))\n        for i, (t,v) in enumerate(zip(fold_train_losses,fold_valid_losses)):\n            plt.subplot(2,5,i+1)\n            plt.title(f\"Fold {i}\")\n            plt.plot(t,label=\"train_loss\")\n            plt.plot(v,label=\"valid_loss\")\n            plt.legend()\n        plt.show()","f4fd1519":"run()","08702c0d":"class TPSDataset(Dataset):\n    def __init__(self,df,cat_features,cont_features):\n        self.cat_data = df.loc[:,cat_features].to_numpy()\n        self.cont_data = df.loc[:,cont_features].to_numpy()\n    \n    def __getitem__(self,idx):\n        input1 = torch.tensor(self.cat_data[idx],dtype=torch.long)\n        input2 = torch.tensor(self.cont_data[idx],dtype=torch.long)\n        return input1,input2\n    \n    def __len__(self):\n        return len(self.cat_data)","d325cadc":"models = list()\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ninput_size = config['cont_size']\nhiddens_sizes = config['hidden_sizes']\noutput_size = 1\n\nfor i in range(config['nfolds']):\n    model = Model(input_size,output_size,hiddens_sizes,embedding_sizes)\n    model.load_state_dict(torch.load(f\".\/model{i}.bin\",map_location=device))\n    model.to(device)\n    model.eval()\n    models.append(model)","207af5c4":"def inference(test):\n    all_prediction = np.zeros((test.shape[0],1))\n    test_ds = TPSDataset(test,cat_features,cont_features)\n    test_dl =  DataLoader(test_ds,batch_size = config[\"test_batch_size\"],shuffle=False,num_workers = 4,pin_memory=True)\n    \n    for model in models:\n        prediction = list()\n        for inputs1,inputs2 in test_dl:\n            inputs1 = inputs1.to(device)\n            inputs2 = inputs2.to(device)\n            outputs = model(inputs1,inputs2) \n            prediction.extend(outputs.detach().cpu().numpy())\n        all_prediction += prediction\n\n    return all_prediction\/config['nfolds']","eb303743":"predictions = inference(test_data)\nsample.target = predictions\nsample.to_csv('submission.csv',index=False)\nsample.head()","c8b1cd3c":"plt.figure(figsize=(15,7))\nplt.subplot(131)\nsns.distplot(sample.target)\nplt.title(\"Distribution of test target\")\nplt.subplot(132)\nsns.distplot(train_data[target_feature])\nplt.title(\"Distribution of train target\")\nplt.subplot(133)\nsns.distplot(sample.target,label='test')\nsns.distplot(train_data[target_feature],label='train')\nplt.legend()\nplt.title(\"Distribution of train-test target\");","209b7d31":"<h4 style=\"border:2px solid purple; border-radius:10px; color:black; background-color:lightblue\"><center> 4.7.2 How good are 3 components of pca at seprating data (3d plot)<\/center><\/h4>","e23f2f04":"<h2 style=\"border:2px solid purple; border-radius:10px; color:black; background-color:yellow\"><center> 1. Given Data \ud83d\udcbd <\/center><\/h2>\n\nwe are provided with 3 files.<br\/>\n1. sample_submission.csv\n2. test.csv\n3. train.csv\n\nTrain data contains 10 categorical featues and 14 continous features<br\/>\nwe have to make prediction on test data and make submission using sample_submission's format<br\/>","0aef4d26":" <h2 style=\"border:2px solid purple; border-radius:10px; color:black; background-color:pink\"><center> 4.9 cont0 distribution based on cat9<\/center><\/h2>\nAs cat9 has most categories let us see distribution of one feature based on cat9.","eee972c1":"<h4 style=\"border:2px solid purple; border-radius:10px; color:black; background-color:lightblue\"><center> 4.8 Countplot of all categorical fearues <\/center><\/h4>","2fac7c9e":"<h2 style=\"border:2px solid purple; border-radius:10px; color:white; background-color:green\"><center> Tabular Playground Series(feb) <\/center><\/h2>\n\nTable Playground Series are beginner friendly monthly competitions organised by kaggle.<br\/>\n    \nIn this competition we have to make a regrssion model based on categorical and continous features provided<br\/>\n\nThis notebook is beginner friendly guide for creating supercool EDA and making baseline model.\n    \n**Feel free to ask any question and if you find any mistake tell me in the comments**","3f93231d":"<h3 style=\"border:2px solid purple; border-radius:10px; color:black; background-color:pink\"><center>4.2 Distribution of cont0<\/center><\/h3>","03c33f00":"It is clear that all the features have different disbtributions and none of them looks like normal distribution<br\/>\n\ncont1 is very interesting as there are gaps in cont1 as if it is partly continous and partly categorical<br\/>\n\nMaybe we should look into correlations between this features to get better insight","a655c2d0":"<h3 style=\"border:2px solid purple; border-radius:10px; color:black; background-color:pink\"><center> 4.3 Distribution of all the continous features. <\/center><\/h3>","b50ac759":"<h2 style=\"border:2px solid red; border-radius:10px; color:white; background-color:orange\"><center> Inference <\/center><\/h2>","8d357c71":"<h3 style=\"border:2px solid purple; border-radius:10px; color:black; background-color:pink\"><center> 4.14 Error bar <\/center><\/h3>","528e4b3e":"<h3 style=\"border:2px solid purple; border-radius:10px; color:black; background-color:pink\"><center>4.5 Pairplot of features<\/center><\/h3>","a1ebb696":"<h3 style=\"border:2px solid purple; border-radius:10px; color:black; background-color:pink\"><center> 4.12 Trivariate histogram with 2 category<\/center><\/h3>","7519cd57":"<h2 style=\"border:2px solid purple; border-radius:10px; color:black; background-color:brown\"><center> 5. Data Preprocessing<\/center><\/h2>\nI am using StratifiedKFold for regression by dividing target into bins\n","c4021f49":"Target ranges from 0 to 10.31<br\/>\nbut distribution really starts from target values >4<br\/>\n\nWe can see 2 peaks in target distribution<br\/>\nOne may assume that data is taken from 2 different distribution and then combined.<br\/>\nso we are seeing two peaks<br\/>\n\nBut judging from TPS(jan) actually target is from same distribution but some values of targets<br\/>\nbelong to test data so we will most probably see the distribution of predictions with one peak around value 7<br\/>","f200b36f":"\n<h3 style=\"border:2px solid purple; border-radius:10px; color:black; background-color:pink\"><center> 4.16 Parallel lines<\/center><\/h3>","6d0d8cf8":"It is clear that distribution of categories are evry skewed<br\/>\nIt would be good to remove some of the cat features like<br\/>\ncat4,cat0, cat2, cat6, cat7, <br\/>\nThe reason for this is that almost all rows will have same values for these columns<br\/>\nso it will not provide any usefull information to model.","af488de1":"## Importing Libraries \ud83d\udcd7.","bd82fdda":"<h4 style=\"border:2px solid purple; border-radius:10px; color:black; background-color:lightblue\"><center> 4.7.3 Ploting explained variance <\/center><\/h4>","c2d9edd4":"<h3 style=\"border:2px solid purple; border-radius:10px; color:black; background-color:pink\"><center> 4.3 Distribution of targets<\/center><\/h3>","bc8339e1":"<h3 style=\"border:2px solid purple; border-radius:10px; color:black; background-color:pink\"><center> 4.1 Checking for Null Values\n<\/center><\/h3>","1affa296":"<h3 style=\"border:2px solid purple; border-radius:10px; color:black; background-color:pink\"><center> 4.11 Boxenplot for cont1 and cat8\n <\/center><\/h3>","cf2d09e7":"<h2 style=\"border:2px solid purple; border-radius:10px; color:black; background-color:yellow\"><center> 3. Loading Data \ud83d\udcbd<\/center><\/h2>\n\nWe will use pandas to load data<br\/>\n ","a0802eff":"<h2 style=\"border:2px solid purple; border-radius:10px; color:black; background-color:yellow\"><center> 4. Exploratory Data Analysis \ud83d\udcca\ud83d\udcc8\ud83d\udcc9\ud83d\udc40<\/center><\/h2>","dc6e292b":"<h2 style=\"border:2px solid red; border-radius:10px; color:black; background-color:orange\"><center> 6. Pytorch Model<\/center><\/h2>","23e09ae0":"We see that there is some correlation between      featues cont5 to cont12 but they are not very high.\n    \nOne interesting thing is no features have any    correlation with target","f7adf8c6":"<h3 style=\"border:2px solid purple; border-radius:10px; color:black; background-color:pink\"><center> 4.7 Let's play with PCA<\/center><\/h3>","ef62a089":"Pair plot gives very good idea about relationship between the features<br\/>\nall the features have a square relationship means for every value in one feature<br\/>\nthere are enough values in entire range of other feature which makes this data little challanging<br\/>","07ea9ed6":"<h2 style=\"border:2px solid purple; border-radius:10px; color:black; background-color:yellow\"><center> 2. Metric \ud83d\udcd0<\/center><\/h2>\n\nHere metric used is very simple which is root mean squared error.<br\/>\n\nrmse = root(1\/n (yit-yip)^2)\n\nHere yit is true target values<br\/>\nand yip is predicted target values.<br\/>","161fb28a":"<h4 style=\"border:2px solid purple; border-radius:10px; color:black; background-color:lightblue\"><center> 4.7.1 How good are 4 components of pca at seprating data<\/center><\/h4>","6c7124a4":"<h3 style=\"border:2px solid purple; border-radius:10px; color:black; background-color:pink\"><center> 4.15 parellel categories<\/center><\/h3>","ddc3f237":"<h3 style=\"border:2px solid purple; border-radius:10px; color:black; background-color:pink\"><center> 4.17 sunburst Chart<\/center><\/h3>","992f76b1":"<h3 style=\"border:2px solid purple; border-radius:10px; color:black; background-color:pink\"><center> 4.10 cont0 box-plot based on cat9<\/center><\/h3>\nI think box-plot will give use better idea than histplot","08ac4aa9":"<h3 style=\"border:2px solid purple; border-radius:10px; color:black; background-color:pink\"><center> 4.13 swarmplot of target<\/center><\/h3>","da395ee6":"<h3 style=\"border:2px solid purple; border-radius:10px; color:black; background-color:pink\"><center>4.13 Trend lines and templates plotly<\/center><\/h3>\nNow I will use plotly for plotting","34129453":"<h3 style=\"border:2px solid purple; border-radius:10px; color:black; background-color:pink\"><center> 4.4 Correlation Matrix<\/center><\/h3>"}}