{"cell_type":{"d4bfe237":"code","74490fe5":"code","d2214ef5":"code","2c9a8e50":"code","49db73dd":"code","b5b20888":"code","7bc5f7bf":"code","0978d41b":"code","f06007a3":"code","4bb3817e":"code","f706f940":"code","0d9d98d2":"code","a592d79f":"code","35566256":"code","bb10c1b0":"code","2d7b3e31":"code","537fc999":"code","5b06757e":"code","96bd16d0":"code","4139bd43":"code","403f65ce":"code","8dd62ba0":"code","86835b38":"code","8482b0b0":"code","3b68fbfa":"code","d5e59fcc":"code","c4de9fb9":"code","caf8d07a":"markdown","fa19944f":"markdown","22bca7db":"markdown","511c0827":"markdown","ebe0c1e3":"markdown","97a7c5d4":"markdown","b27a5384":"markdown","94da4c82":"markdown","469eeb89":"markdown","7257544b":"markdown","43c32a1a":"markdown","1f3b878e":"markdown","cd6e1668":"markdown","9cd70569":"markdown","ea6794b3":"markdown","11c718ef":"markdown","a00fc6c9":"markdown","516e45de":"markdown","9a79276c":"markdown","cc19068e":"markdown","d2649893":"markdown","465e320e":"markdown","71f02b2d":"markdown"},"source":{"d4bfe237":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport sklearn\nimport xgboost as xgb\nfrom sklearn.preprocessing import StandardScaler,LabelEncoder\nfrom sklearn.ensemble import GradientBoostingRegressor,RandomForestRegressor\nfrom sklearn.model_selection import cross_val_score, KFold,GridSearchCV,train_test_split\nfrom lightgbm import LGBMRegressor\nfrom sklearn.feature_selection import SelectKBest,f_regression,RFECV\nfrom skopt import dummy_minimize\nfrom sklearn.linear_model import ElasticNet\nfrom sklearn.linear_model import Lasso \nfrom sklearn.linear_model import Ridge\nfrom sklearn.metrics import mean_squared_error","74490fe5":"#Read data\ntrain = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv')\ntest['SalePrice'] = np.nan\ndf = pd.concat([test,train],sort=False)\ndf.head()","d2214ef5":"#Deal with null values\nmissing = pd.DataFrame({'types':df.dtypes, 'percetange_of_missing': df.isna().sum()\/len(df)*100})\nmissing = missing[missing['percetange_of_missing'] != 0]\nmissing.sort_values(by='percetange_of_missing', ascending=False)","2c9a8e50":"#Drop coluns with more than 50% of null\ndf = df.drop(['PoolQC','MiscFeature','Alley','Fence'],axis=1)          \n\n#Convert categoric features to object\ndf[['MSSubClass','OverallQual','OverallCond']] = df[['MSSubClass','OverallQual','OverallCond']].astype(object)","49db73dd":"cols_num = missing[missing.types==\"float64\"].index\ncols_num = cols_num.drop(['SalePrice'])\nprint(cols_num)\nfor i in cols_num:\n    df[i].fillna(df[i].median(), inplace = True)","b5b20888":"print(missing[missing.types==\"object\"][missing.percetange_of_missing<50])\ncols_cat = missing[missing.types==\"object\"][missing.percetange_of_missing<50].index\nfor i in cols_cat:\n    df[i].fillna(df[i].mode()[0], inplace = True)","7bc5f7bf":"df2 = df.drop(['Id'],axis=1)\n# Threshold for removing correlated variables\nthreshold = 0.7\ncorr_matrix = df2.corr().abs()\n\n# Upper triangle of correlations\nupper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\nupper.head()","0978d41b":"# Select columns with correlations above threshold\nto_drop = [column for column in upper.columns if any(upper[column] > threshold)]\nprint('There are %d columns to remove.' % (len(to_drop)))\ndataset = df.drop(columns = to_drop)","f06007a3":"categorical= df.select_dtypes(include='object')\n\nfig, axes = plt.subplots(round(len(categorical.columns) \/ 3), 2, figsize=(22, 80))\n\nfor i, ax in enumerate(fig.axes):\n    if i < len(categorical.columns):\n        sns.countplot(x=categorical.columns[i], data=categorical, ax=ax)\n\nfig.tight_layout()","4bb3817e":"numerics = df.select_dtypes(include=['float64','float32','int64'])\nnumerics = numerics.drop(['Id'],axis=1)\nfig, ax = plt.subplots(17,2,figsize=(22, 80))\nfor i, col in enumerate(numerics):\n    plt.subplot(17,2,i+1)\n    plt.xlabel(col, fontsize=10)\n    sns.kdeplot(numerics[col].values, bw=0.5)\nplt.show() ","f706f940":"fig, ax = plt.subplots(18,2,figsize=(22,80))\nfor i, col in enumerate(numerics):\n    plt.subplot(17,2,i+1)\n    plt.xlabel(col, fontsize=10)\n    sns.scatterplot(x=numerics[col].values, y='SalePrice', data=numerics)\nplt.show() ","0d9d98d2":"import warnings\nwarnings.filterwarnings(\"ignore\")\n#Plot Categoric features \ncategorical = df.select_dtypes(include=['object'])\ncategorical['SalePrice'] = train.SalePrice\nfig, ax = plt.subplots(22,2,figsize=(22,150))\nfor i, col in enumerate(categorical):\n    plt.subplot(22,2,i+1)\n    plt.xlabel(col, fontsize=10)\n    sns.boxplot(x=categorical.columns[i], y=\"SalePrice\", data=categorical, dodge=False)\nplt.show() ","a592d79f":"#Check and print the correlation between features and target (top 10 positive correlations)\n#Weak positive correlation\ndf2 = df2.select_dtypes(['float64','int64'])\n\ncorr = df2.corr()\nsns.heatmap(corr, \n            xticklabels=corr.columns.values,\n            yticklabels=corr.columns.values)\nplt.title('Correlation Matrix')\nplt.gcf().set_size_inches(11,9)\nplt.show()","35566256":"print(df.SalePrice.quantile([0.01,0.99]))\nprint('---')\nprint(df.SalePrice.describe())\ndf.SalePrice = np.where(df.SalePrice>442567.0,442567.0,df.SalePrice)\ndf.SalePrice = np.where(df.SalePrice<61815.97,61815.97,df.SalePrice)","bb10c1b0":"num_col = numerics.columns\nfor col in df[num_col]:\n    df[col]= np.where(df[col]>df[col].quantile(0.99),df[col].quantile(0.99),df[col])\n    df[col]= np.where(df[col]<df[col].quantile(0.01),df[col].quantile(0.01),df[col])","2d7b3e31":"df['OverallQual'] = LabelEncoder().fit_transform(df['OverallQual'])\n\n#Create new features\ndf['TotalSF'] = df['TotalBsmtSF'] + df['1stFlrSF'] + df['2ndFlrSF']\ndf['TotalBath'] = df['FullBath']+df['BsmtFullBath']+(df['BsmtHalfBath']+df['HalfBath'])*0.5\ndf['TotalPorchSF'] = df['OpenPorchSF'] + df['3SsnPorch'] + df['EnclosedPorch'] + df['ScreenPorch']\ndf['HasPool'] = df['PoolArea'].apply(lambda x: 1 if x > 0 else 0)\ndf['HasFireplace'] = df['Fireplaces'].apply(lambda x: 1 if x > 0 else 0)\ndf['isNewer'] = df['MSSubClass'].apply(lambda x: 1 if x in [20, 60, 120] else 0)\n\n#Encoder in object columsn\nfor col in df.columns[df.dtypes == 'object']:\n    le = LabelEncoder()\n    le.fit(df[col])\n    df[col] = le.transform(df[col])\n\n#standard features\nnum_col = df2.select_dtypes(include=['float64','float32','int64']).columns\nnum_col = num_col.drop(['SalePrice'])\nscaler = sklearn.preprocessing.StandardScaler().fit(df[num_col])\ndf[num_col]= scaler.transform(df[num_col])","537fc999":"#train\ntrain = df[df['SalePrice'].isna()==False]\n\n#Valid + submission\nx_valid = df[df['SalePrice'].isna()==True]\nsubmission = x_valid['Id'].to_frame()\nx_valid = x_valid.drop(['Id'],axis=1)\n\n#x e y\ny =train.loc[:,['SalePrice','Id']]\nx =train.drop(['SalePrice','Id'], axis = 1)\n\nx_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.3,random_state=42)","5b06757e":"mdl1 = GradientBoostingRegressor(n_estimators = 100)\nmdl1.fit(x_train,y_train['SalePrice'])\np = mdl1.predict(x_test) \nprint((mean_squared_error(y_test['SalePrice'], p))**0.5)","96bd16d0":"mdl2 = RandomForestRegressor(n_estimators = 100)\nmdl2.fit(x_train,y_train['SalePrice'])\np2 = mdl2.predict(x_test) \nprint((mean_squared_error(y_test['SalePrice'], p2))**0.5)","4139bd43":"mdl3 = Ridge() \nmdl3.fit(x_train,y_train['SalePrice'])\np3 = mdl3.predict(x_test) \nprint((mean_squared_error(y_test['SalePrice'], p3))**0.5)","403f65ce":"mdl4 = Lasso()\nmdl4.fit(x_train,y_train['SalePrice'])\np4 = mdl4.predict(x_test) \nprint((mean_squared_error(y_test['SalePrice'], p4))**0.5)","8dd62ba0":"mdl5 = ElasticNet()\nmdl5.fit(x_train,y_train['SalePrice'])\np5 = mdl5.predict(x_test) \nprint((mean_squared_error(y_test['SalePrice'], p5))**0.5)","86835b38":"select_feature = SelectKBest(f_regression, k=30).fit(x_train, y_train['SalePrice'])\nx_train2 = select_feature.transform(x_train)\nx_test2 = select_feature.transform(x_test)\n\nmdl6 = GradientBoostingRegressor(n_estimators = 100,random_state=42)\nmdl6.fit(x_train2,y_train['SalePrice'])\np = mdl6.predict(x_test2) \nprint((mean_squared_error(y_test['SalePrice'], p))**0.5)","8482b0b0":"mdl7 = GradientBoostingRegressor(n_estimators = 100)\nrfecv = RFECV(estimator=mdl7, step=1, cv=5,scoring='neg_mean_squared_error')  \nrfecv = rfecv.fit(x_train, y_train['SalePrice'])\n\nprint('Optimal number of features :', rfecv.n_features_)\nprint('Best features :', x_train.columns[rfecv.support_])\nbest_features = list(x_train.columns[rfecv.support_])\n\n# Plot number of features VS. cross-validation scores\nplt.figure()\nplt.xlabel(\"Number of features selected\")\nplt.ylabel(\"Cross validation score of number of selected features\")\nplt.plot(range(1, len(rfecv.grid_scores_) + 1), rfecv.grid_scores_)\nplt.show()","3b68fbfa":"x_train3 = x_train[best_features]\nx_test3 = x_test[best_features]\n\nmdl7 = GradientBoostingRegressor(n_estimators = 100,random_state=42)\nmdl7.fit(x_train3,y_train['SalePrice'])\np = mdl7.predict(x_test3) \nprint((mean_squared_error(y_test['SalePrice'], p))**0.5)","d5e59fcc":"parameters = {\n              'max_features': [0.7,0.8, 0.9],\n              'min_samples_leaf' :[1,3,7],\n              'learning_rate' : [0.01,0.03,0.1],\n               'subsample': [0.8,0.9],\n               'max_depth': [5,6,7]\n              }\n\n\nmdl8 = GradientBoostingRegressor(n_estimators = 100,random_state=42)\ngrid_search2 = GridSearchCV(mdl8, parameters, cv=5,n_jobs=-1,scoring='neg_root_mean_squared_error')\ngrid_search2.fit(x_train3,y_train['SalePrice'])\n\nprint(grid_search2.best_params_)\nprint(grid_search2.best_score_)","c4de9fb9":"x_valid = x_valid[best_features]\nSalePrice = mdl7.predict(x_valid)\nsubmission['SalePrice'] = SalePrice  \nsubmission['SalePrice'] = submission['SalePrice']\nsubmission.to_csv('submission.csv' , index=False)","caf8d07a":"### Try some feature selection methods in my beste model (Gradient boosting)","fa19944f":"### 5) Elastic Net regression","22bca7db":"### 4) Lasso regression (L1)","511c0827":"Plotting relationship between categorical features and SalePrice","ebe0c1e3":"### **Steps:**\n\nDeal with missing data\n\nDeal with Multicollinearity\n\nExploratory Data Analysis\n\nFeature Engineering\n\nTest some machine learning models\n\nTest some feature selection methods\n\nTuning Hyperparamater using Gridsearch\n\nMake submission","97a7c5d4":"### 3) Ridge regression (L2)","b27a5384":"### Split data in test and train ","94da4c82":"### Feature Engineering","469eeb89":"Plotting relationship between numerics features and SalePrice","7257544b":"### Tuning Hyperparamater using Gridsearch","43c32a1a":"### Challenge: Predict the final price of each home","1f3b878e":"Plotting distribution of numeric features","cd6e1668":"## Input median in numerics null values","9cd70569":"### Input mode in categorical null values","ea6794b3":"### 1) Gradient Boosting Regressor","11c718ef":"### Deal with Multicollinearity","a00fc6c9":"Using SelectKbest","516e45de":"### Make submission","9a79276c":"Using recursive feature elimination","cc19068e":"* Plotting frequency of categorical features","d2649893":"### 2) Random Forest Regressor","465e320e":"### Exploratory Data Analysis","71f02b2d":"### Deal with outliers"}}