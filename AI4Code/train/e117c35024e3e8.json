{"cell_type":{"04d77aa6":"code","311b5d9d":"code","5d539dab":"code","661ca1c0":"code","d6754043":"code","5b99541a":"code","15f80dab":"code","820d2ea4":"code","b307b0d3":"code","740ff925":"code","2a3e079b":"code","a984e95b":"code","633ab381":"code","52cc69d1":"code","52a10839":"code","02dce41c":"code","865d0a42":"code","4bfa7b9d":"code","a37d62c1":"code","9f0def0c":"markdown","f5266115":"markdown","822aec83":"markdown","46395549":"markdown","8bbfa495":"markdown","53fb2616":"markdown","ec9cf8bc":"markdown","bdd49645":"markdown","1de89905":"markdown","a642f79b":"markdown","55e6811a":"markdown","e17499e2":"markdown","016a8717":"markdown","7c43502f":"markdown","87215d94":"markdown","9d436e68":"markdown","80ef0d09":"markdown","5e67231e":"markdown","e1338975":"markdown","7e38e2af":"markdown"},"source":{"04d77aa6":"import pandas as pd \nimport numpy as np\n\ndata = pd.read_csv('..\/input\/customer-segmentation-tutorial-in-python\/Mall_Customers.csv')\ndata.head()","311b5d9d":"data.info()","5d539dab":"data.describe()","661ca1c0":"import seaborn as sns \nimport matplotlib.pyplot as plt \n\ndata.columns = ['CustomerID', 'Gender', 'Age', 'AnnualIncome', 'SpendingScore']\nplot = sns.pairplot(data, corner = True)\nplot.fig.suptitle(\"Pairplot of the data\", y = 1, fontsize = 20) \nplt.show()","d6754043":"plot = sns.pairplot(data, hue = 'Gender', corner = True)\nplot.fig.suptitle(\"Pairplot of the data by Gender\", y = 1.05, fontsize = 20) \nplt.show()","5b99541a":"sns.histplot(data = data, x = 'Age', hue = 'Gender', bins = 15, kde = True, multiple = 'dodge').set_title('Histogram analysis for Age by Gender')\nplt.show()","15f80dab":"data['AgeRange'] = pd.cut(data[\"Age\"],bins=[0, 15, 30, 45, 60, np.inf],labels=['<15', '15-30', '30-45', '45-60', '>60'])\n\nfig, axes = plt.subplots(1, 2, figsize = (13, 4))\nfig.suptitle('Strata Comparison for Age and Age Range')\n\nsns.histplot(ax=axes[0], x='Age', data = data, hue = 'Gender', multiple = 'dodge')\naxes[0].set_title('Age')\nsns.histplot(ax=axes[1], x='AgeRange', data = data, hue = 'Gender', multiple = 'dodge')\naxes[1].set_title('AgeRange')\n\nplt.show()","820d2ea4":"plot = sns.pairplot(data, hue = 'AgeRange', corner = True)\nplot.fig.suptitle(\"Pairplot of the data by Age Range\", y = 1.05, fontsize = 20) \nplt.show()","b307b0d3":"fig, axes = plt.subplots(1, 2, figsize = (13, 4))\nfig.suptitle('Spending Scores by Age Range and Gender')\n\nsns.barplot(ax=axes[0], data = data, x='AgeRange', order = ['15-30', '30-45','45-60', '>60'] , y = 'SpendingScore', hue = 'Gender')\naxes[0].set_title('Age Range vs Spending Score by Gender')\nsns.barplot(ax=axes[1], data = data, x='Gender', y = 'SpendingScore', hue = 'AgeRange')\naxes[1].set_title('Gender vs Spending Score by Age Range')\n            \nplt.show()","740ff925":"pip install pycaret","2a3e079b":"from pycaret.clustering import *\ndata.columns = data.columns.str.strip()\nclassifier = setup(data, silent = True, preprocess = True, profile = True)","a984e95b":"kmeans = create_model('kmeans')\nplot_model(kmeans, plot = 'cluster')","633ab381":"evaluate_model(kmeans)","52cc69d1":"plot_model(kmeans, plot = 'tsne')","52a10839":"plot_model(kmeans, plot = 'elbow')","02dce41c":"plot_model(kmeans, plot = 'silhouette')","865d0a42":"kmeans_optimized = create_model('kmeans', num_clusters = 5)\nplot_model(kmeans_optimized, plot = 'cluster')","4bfa7b9d":"plot_model(kmeans_optimized, plot = 'tsne')","a37d62c1":"plot_model(kmeans_optimized, plot = 'silhouette')","9f0def0c":"## 3. k-Means\nBefore heading down to the analysis, let's try to capture the depth of this beautiful and simple algorithm. The idea of this algorithm is as follows:\u00a0\n1. You start with a centroid randomly and provide the number of clusters to be initialized. This value is called 'k', hence the name k-Means.\u00a0\n2. Using the distance measure to this centroid, you group an incoming instance.\n3. Based on step 2, you update the centroid again of each cluster based on their distance.\n4. Iteratively perform 2 and 3 until the centroids stop changing.\n\nUsing the Pycaret package on python, lets run the k-Means model and see what the results look like. When we setup the data we can provide an optional parameter which asks whether we would like a profiling to be performed or not. The profiling results are a part of the Kaggle notebook, if you would like to further know more interesting information from the data. The profiling too confirms the non significance of gender within this dataset based on correlation matrices. The model by default is assigned k = 4. Let's visualize how this model looks like on our dataset:","f5266115":"As you can see, age seems to have a significant relationship with the dataset. But how do we visualize this? Age is a numerical field in the dataset and to visualize the same we would need to take a page from stratified sampling to form multiple stratas whilst not affecting the distribution too much.\u00a0\nBased on this distribution on a cumulative manner, we can create stratas to have a definitive understanding on how many of these customers fall into these various stratas. As seen from the image below, I used, 15\u201330, 30\u201345, 45\u201360 and Above 60 as the stratas based on the distribution shown. The two plots follow roughly the same proportion within each bins","822aec83":"What can we infer from these plots? There seems to be a definitive interesting pattern or groupings between spending score and annual income, age and annual income, as expected. Spending score seems to follow roughly a normal distribution whereas annual income tends to have a skewed nature. Does the scale of these differ by a large amount? Not really, although feature scaling can definitely benefit any model. We need to ensure there isn't any significant outlier affect on the same when we consider feature scaling methods.","46395549":"Now the patterns are visibly changing more and more. However, does gender really seem to have an impact of significance with this dataset? Lets take a look at the histogram of age by gender to identify if there is indeed any significant relationship for this set.","8bbfa495":"## 2. Exploratory Data Analysis\nWithout further ado, lets analyze a dataset to dive into a better understanding on clustering analysis. Mall customer segmentation data!\u200a-\u200a[2]","53fb2616":"From this visualization, cluster 2 seems to be well segregated from the other clusters. However there seems to be a spread of cluster 1. The marginal borders between cluster 1, cluster 3 and cluster 4, respectively seems to be quite thin. Also do you notice an instance to be what seems like an anomaly from the 3D plot? As you can see, there is a heavy dependency on the value of k for this algorithm which requires you to run the same couple of times in order to arrive at a solution that you are comfortable with.\n\n### How do I know what model to be comfortable with if I don't have labels to begin\u00a0with?\n\nAs we all know, with any analysis, we need to define the performance measure, or ways to evaluate it in order for us to know how this model would perform for our use cases. We definitely want to limit any surprises after we deploy the model! This algorithm provides you with a guaranteed solution. But is it the most optimal solution? Let's find out by understanding how to evaluate a cluster model.","ec9cf8bc":"Visually the cluster grouping seems to be performing better with k = 5. However there seems to be some marginal instances between cluster 1 and cluster 3. What about the silhouette diagram?","bdd49645":"## 1. Introduction\nHas it ever happened to you that when you are in the middle of a conversation, and you suddenly forget what an object is called? If you speak multiple languages and constantly shift between them, then you probably have dealt with this situation. At this point, you would try to recollect and well, lets admit it most of the time we would fail only to try and describe that object by associating it with something similar.","1de89905":"The spending score is based on parameters such as customer engagement and purchasing behavior. It ranges from 1\u2013100. This dataset has 200 such customers and there exists no missing values for any fields. Before we begin with any clustering analysis its always a good idea to understand the shape and distribution of the features you are dealing with. Let's take a look at the pairplot of this dataset.","a642f79b":"But what about gender? Gender is a categorical variable and what if we could try and visualize the same plot but specific to the gender distributions?","55e6811a":"### But, what if I don't know what that object is to begin\u00a0with?\nWhen you don't have a label to work with, you are dealing with an unsupervised problem. If you have some labels, then it would be a semi-supervised problem. Clusters cannot be accurately defined as opposed to a definitive label which is why there are many algorithms that significantly differ on how these clusters are defined.\n\nSome ways how a cluster is defined are as follows:\u00a0\n1. Distance: How far is an instance away from each other?\n2. Centroids: How far is an instance away from the center of each cluster?\n3. Density: How dense is each cluster?\n4. Distribution: How does the shape impact the cluster?\u00a0\n\nWhen the margin is definitive in the sense that either the instance belongs to the cluster or it does not, then this type of clustering is termed as Hard Clustering. Whereas, when an instance can belong to a particular cluster to a certain degree it is termed as Soft Clustering. The latter provides a score for each instance per cluster. This score could be based on a distance measure or a similarity measure such as RBF.","e17499e2":"Now that we have created the stratas, let's visualize the impact of age range on this dataset using the pairplot.","016a8717":"Even though the elbow plot suggested to approach with k =  5, model seems to be performing worse than before as majority  instances do not lie beyond the average silhouette score. There is a tendency in cluster 3 towards -1 which suggests that there are some instances which may belong to the neighboring groups. There is definitely room for improvement. As you probably guessed, the influence of the k value, the number of instances within each cluster and the shape of the cluster all has significance on the choice of k-Means to fit with the data.","7c43502f":"Based on the above information, our model has an average silhouette score of 0.4 which shows room for improvement which is also suggested by the elbow plot with k = 5. Most of the instances do not lie beyond the dashed line. Cluster 1 seems to be performing the worst as some instances tends towards -1 and indicates that they probably belong to their neighboring clusters. Cluster 1 has heavy room for improvement.\u00a0\nAs suggested by these plots let's re-run the algorithm with k = 5. Let's take a look at their 3D and 2D plots first.","87215d94":"## 5. Resources\n[1] Cluster analysis, Wikipedia \n[2] Mall customer segmentation data, Vijay Choudhary, Kaggle \n[3] Clustering analysis, Annette Catherine Paul, Kaggle\u00a0\n[4] Unsupervised learning techniques, Aurelien Geron, Hands-On Machine Learning with Scikit Learn, Keras & Tensorflow, O'Reilly\u00a0\n[5] k-Means advantages and disadvantages\u200a-\u200aClustering in Machine Learning, Google developer courses","9d436e68":"# Introduction to Clustering Algorithms and its Use Cases\n\n##### This analysis is a part of this medium blog: https:\/\/medium.com\/delvify\/introduction-to-clustering-algorithms-and-its-use-cases-35c1655c91e7\n\n","80ef0d09":"### Inertia \nIt is the mean squared distance between each instance and its closest centroid. The lower this value, the the more denser the cluster is. But is this always a good measure? As the number of clusters increases this value will always decrease. Hence inertia is heavily dependent on the value of k as well. How do we decide on a value for k?\n\n### Elbow plot\nPlotting inertia vs number of clusters will reveal a much more informative plot. We can see that there is an elbow at k = 5. After k = 5, the distortion score approaches saturation and this suggests that an optimal k could be k = 5. But this approach is sort of brute force and of course computationally expensive as you have to run the model multiple times with different k.\n\n### Silhouette score\nA better measure is the mean of silhouette coefficient across all the instances. This coefficient measures the mean distance between intra cluster distance, i.e. the mean distance of the instance with other instances within the same cluster and the mean distance of the nearest cluster over the maximum value between these two measures. This measure varies from -1 to 1 and can be depicted as follows:\u00a0\n1. Value ~ 1 \u2192 The instance is well packed within its defined cluster\u00a0\n2. Value ~ 0 \u2192 The instance is closer to the boundary of its defined cluster\u00a0\n3. Value ~ -1 \u2192 The instance most probably does not belong in its defined cluster\u00a0\n\nWhen we use this information and plot every instance's silhouette coefficient we can obtain a much more informative evaluation of the cluster, and we get a silhouette diagram. Let's take a look at the significance of various elements within this plot. \n\n* Height: Each cluster label has a knife looking formation. The height of this depicts the density or the number of instances within that cluster.\u00a0\n* Width: The width of these formations are the respective silhouette coefficients. The wider it is, the better.\u00a0\n* Dashed line: Furthermore, the dashed line is the average silhouette coefficient. If majority of the instances within a cluster belong to a lower coefficient than this line, we can confidently say that the cluster formations have room for improvement as its mean would be closer to other clusters. Hence majority of the instances should be existent beyond this line and closer to 1.\n\nIn this manner Silhouette Score and it's diagram act as a good source of choice for  optimal k.","5e67231e":"## 3. Beyond k-Means\n\nWhat are the limitations of k-Means?\u00a0\n* Dependency on k and on initial value assignment: There is a heavy dependency of the algorithm on k value which means we need to run the algorithm multiple times in order to evaluate and choose a model that's best fitting. Additionally, there is a dependency of the model on the initial assignment of these centroids, which is randomized.\u00a0\n* Numerical influence: k-Means can only handle numerical data. Although the purpose of this analysis was to introduce clustering analysis and means of evaluation of the same, as with any algorithm with appropriate data preprocessing, categorical variable handling and feature engineering can improve the performance of the model.\u00a0\n* Shape of the cluster: k-Means is heavily shape dependent. The algorithm has an assumption that we are mainly dealing with spherical formations. When the shape of the data and its distribution do not align with this assumption we would need to look for other algorithms more suiting in terms of the shape of the clusters. But k-Means act as a great first step in understanding a possible underlying shape of each clusters.\n* Number of members: The number of members within each cluster is an important parameter for this algorithm. The algorithm assumes roughly that each cluster would have the same number of instances which may be a very idealistic thought process for various use cases in the real world.\u00a0\n* Dimensions: We worked with very limited features. There may be numerous other factors impacting the spending score. When dealing with higher dimensions especially with a distance based similarity measure, it always saturates and converges to a constant value which will heavily bias your model.\u00a0\n* Outliers: As we saw in our analysis above, outliers can have a significant impact on the centroid as it drags the shape of the cluster. Sometimes, outliers may even get their own cluster!\n* Convergence to local minima: k-Means will give a guaranteed convergence. But this may be a local minima and not global minima. Hence not always will the solution be an optimal solution.\n\n## 4. How can I use clustering analysis in my business use\u00a0cases?\n1. Exploratory data analysis\u200a-\u200aWhat to do with this data? We have asked this question so many times whenever dealing with a new dataset. Clustering forms an integral part to analyzing data especially in the exploratory data analysis stage. It can help you understand the nuances between your features to further give you a better understanding on how to take it from there. After all, any good analysis should be able to increase the number of questions to ask!\n2. Customer segmentation\u200a-\u200aAs shown in the example above, understanding a buyer persona and grouping it can help attain various growth objectives, be it, marketing, retargeting audience, demand planning and even customer retention. When a customer lands on your page for instance, based on their interaction we would be able to map them back to a cluster and take strategic decision on how to guide the customer towards your objective KPI. Whether that is improving recommendations in this manner for an uplift in your conversion or whether it is to engage them better and achieve a higher retention rate, clustering analysis to the rescue!\n3. Visual search\u200a-\u200aHas it ever happened to you that you are taking a stroll down the park and noticed some amazing shoes? You suspect that its from your favorite brand, so you snap a photo and try to search online where you would get all similar looking shoes if not for the same one, for you to choose from. Clustering algorithms can group images into various clusters based on their similarity and when shown with a new image, it would simply try to identify which group does this image resemble the most to?\n4. Fraud detection\u200a-\u200aAn anomaly is anything that deviates from a what we define as a standard or anything that is not expected. When an instance is shown to a cluster model wherein it performs strange in the sense that it belongs to none of the clusters, then that instance could be something to watch out for or simply might need your attention on! Flagging these instances could potentially save you from a danger. But make sure the instance is an anomaly and not a novelty instance (curious about this? More on that later!","e1338975":"This plot reveals the possibility of cluster analysis and the impact already! If we deep dive into the spending score vs annual income plot the cluster formations although vague at this point is visibly significant. Now its time for us to model our clusters!","7e38e2af":"We tried to identify similar objects or associations between this object and other perceivably similar objects to group them together together. This concept is typically what a clustering algorithm does.\n\nCluster analysis or clustering is the task of group a set of objects in such a way that objects in the same group called a cluster are more similar to each other than those in other clusters.\u200a-\u200a[1]"}}