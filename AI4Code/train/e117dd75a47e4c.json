{"cell_type":{"413eb9fa":"code","8ad7f1b5":"code","94e9dcaf":"code","73b9e2b8":"code","9b44b52f":"code","67687ea4":"code","2eb1e255":"code","34bf8fb8":"code","f3a73f60":"code","d3b7538b":"markdown","7e86cb9e":"markdown"},"source":{"413eb9fa":"%matplotlib inline\nimport matplotlib.pyplot as plt\nimport seaborn as sns; sns.set()\nimport numpy as np","8ad7f1b5":"from sklearn.datasets import load_digits\ndigits = load_digits()\ndigits.data.shape","94e9dcaf":"digits = load_digits()\nplt.imshow(digits.data[0].reshape(8, 8), cmap='binary')\ndigits.target[0]","73b9e2b8":"def plot_digits(data):\n    fig, ax = plt.subplots(10, 10, figsize=(8, 8),\n                           subplot_kw=dict(xticks=[], yticks=[]))\n    fig.subplots_adjust(hspace=0.05, wspace=0.05)\n    for i, axi in enumerate(ax.flat):\n        im = axi.imshow(data[i].reshape(8, 8), cmap='binary')\n        im.set_clim(0, 16)\nplot_digits(digits.data)","9b44b52f":"# use a straightforward PCA, asking it to preserve 99% of the variance in the projected data\nfrom sklearn.decomposition import PCA\n\npca = PCA(0.99, whiten=True)\ndata = pca.fit_transform(digits.data)\ndata.shape","67687ea4":"# Use of AIC to get a gauge of number of GMM components to use\nfrom sklearn.mixture import GaussianMixture\n\nn_components = np.arange(50, 210, 10)\nmodels = [GaussianMixture(n, covariance_type='full', random_state=0)\n          for n in n_components]\naics = [model.fit(data).aic(data) for model in models]\nplt.plot(n_components, aics);","2eb1e255":"# Use of 110 as components number\ngmm = GaussianMixture(110, covariance_type='full', random_state=0)\ngmm.fit(data)\nprint(gmm.converged_)","34bf8fb8":"# Generate new data\ndata_new = gmm.sample(100)\ndata_new[0].shape","f3a73f60":"# use the inverse transform of the PCA object to construct the new digits\ndigits_new = pca.inverse_transform(data_new[0])\nplot_digits(digits_new)","d3b7538b":"## Generative Model with Gaussian Mixture ","7e86cb9e":"### Loading data"}}