{"cell_type":{"16cc66fd":"code","fa18ad2b":"code","bba4bf05":"code","58f12eaf":"code","453912e7":"code","70691324":"code","df8880cc":"code","7b7a192f":"code","6937ab52":"code","a2ad0950":"code","3cae1979":"code","847b74d9":"code","d898bf46":"code","63b6a2e4":"code","0938d17d":"code","03b25b81":"code","70a92f82":"code","e459495c":"code","ba50a7ee":"code","c2802ea4":"code","6a917d6b":"code","1771dc39":"code","7987e28b":"code","8277975e":"code","163cf448":"code","3513c54e":"code","9de531dc":"code","a4c7a3d3":"code","bd832fd3":"code","dae7fb1b":"code","0318e344":"code","3ab758fb":"code","dcd22ff4":"code","346bfff2":"code","ad0d9b86":"code","f17b03b2":"code","98c5bf4d":"code","6dcdd572":"code","5b7a6720":"code","c24b6596":"code","b4806864":"code","abdf4fd4":"code","01d27348":"code","d2ecfb52":"code","4b0a6730":"code","39cd3b5c":"code","1c69e1fa":"code","74942a54":"code","181f3df7":"code","34028a7c":"code","80e1bc2e":"code","af4b1ee7":"code","f1663f3a":"code","57bb542e":"code","33f53298":"code","6b06f9c6":"code","fd1e6072":"code","90fbc416":"code","c4b8ebb7":"code","01f9d859":"code","f331ef5b":"code","6676f4d7":"code","1e348395":"code","1d6cbd2e":"code","549d3a93":"code","11109262":"code","97451172":"code","8bce7da3":"code","0310674d":"code","5844f6c1":"code","0eb19625":"markdown","6fee9bad":"markdown","e32ed82e":"markdown","4b02a2ad":"markdown","ae052417":"markdown","125f4664":"markdown","bcb768c3":"markdown","d9edfb13":"markdown","46aab49e":"markdown","51434656":"markdown","d3ef7379":"markdown","25a8dd9a":"markdown","37b46807":"markdown","a2ca21fe":"markdown","bc5aa57d":"markdown","abe9b08e":"markdown","acee16c5":"markdown","c1666dc4":"markdown","901a27e7":"markdown","14e1c446":"markdown","29a7068f":"markdown","a338dd45":"markdown","adcdbded":"markdown","820d261c":"markdown","873a2f24":"markdown","5b8e43af":"markdown","29b66569":"markdown","1c5365ac":"markdown","e22a9794":"markdown","949ea319":"markdown","4f21740f":"markdown","8745214d":"markdown","6f9e0d65":"markdown","e20c94cc":"markdown","31332d69":"markdown","c2682f62":"markdown","05b6ef96":"markdown","ca2e6723":"markdown","754b86c4":"markdown","17dc0a62":"markdown","b63b81b0":"markdown","ae004aa4":"markdown"},"source":{"16cc66fd":"import pandas as pd\nimport numpy as np\nimport time\nimport lightgbm as lgb\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import KFold\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nplt.style.use('seaborn')\nsns.set(font_scale=2)\n\nimport warnings \nwarnings.filterwarnings('ignore')","fa18ad2b":"import os\nimport json\nimport numpy as np\nimport pandas as pd\nfrom pandas.io.json import json_normalize\n\ndef load_df(csv_path='..\/input\/train.csv', nrows=None):\n    JSON_COLUMNS = ['device', 'geoNetwork', 'totals', 'trafficSource']\n    \n    df = pd.read_csv(csv_path, \n                     converters={column: json.loads for column in JSON_COLUMNS}, \n                     dtype={'fullVisitorId': 'str'}, # Important!!\n                     nrows=nrows)\n    \n    for column in JSON_COLUMNS:\n        column_as_df = json_normalize(df[column])\n        column_as_df.columns = [f\"{column}.{subcolumn}\" for subcolumn in column_as_df.columns]\n        df = df.drop(column, axis=1).merge(column_as_df, right_index=True, left_index=True)\n    print(f\"Loaded {os.path.basename(csv_path)}. Shape: {df.shape}\")\n    return df\n\nprint(os.listdir(\"..\/input\"))","bba4bf05":"%%time\ntrain_df = load_df()\ntest_df = load_df(\"..\/input\/test.csv\")","58f12eaf":"target = train_df['totals.transactionRevenue'].fillna(0).astype(float)\ntarget = target.apply(lambda x: np.log(x) if x > 0 else x)\ndel train_df['totals.transactionRevenue']","453912e7":"columns = [col for col in train_df.columns if train_df[col].nunique() > 1]\n#____________________________\ntrain_df = train_df[columns]\ntest_df = test_df[columns]","70691324":"train_df.head()\n\npercent = (100 * train_df.isnull().sum() \/ train_df.shape[0]).sort_values(ascending=False)\n\npercent[:10]","df8880cc":"percent = (100 * test_df.isnull().sum() \/ test_df.shape[0]).sort_values(ascending=False)\npercent[:10]","7b7a192f":"drop_cols = ['trafficSource.referralPath', 'trafficSource.adContent', 'trafficSource.adwordsClickInfo.slot', 'trafficSource.adwordsClickInfo.page',\n            'trafficSource.adwordsClickInfo.adNetworkType']","6937ab52":"train_df.drop(drop_cols, axis=1, inplace=True)\ntest_df.drop(drop_cols, axis=1, inplace=True)","a2ad0950":"train_df['trafficSource.keyword'].fillna('nan', inplace=True)\ntest_df['trafficSource.keyword'].fillna('nan', inplace=True)","3cae1979":"# for ele in train_df['trafficSource.keyword'].vablue_counts().index:\n#     print(ele)\n\n# Save your page","847b74d9":"def add_new_category(x):\n    x = str(x).lower()\n    if x == 'nan':\n        return 'nan'\n    \n    x = ''.join(x.split())\n    \n    if 'youtube' in x or 'you' in x or 'yo' in x or 'tub' in x:\n        return 'youtube'\n    elif 'google' in x or 'goo' in x or 'gle' in x:\n        return 'google'\n    else:\n        return 'other'","d898bf46":"train_df['trafficSource.keyword'] = train_df['trafficSource.keyword'].apply(add_new_category)\ntest_df['trafficSource.keyword'] = test_df['trafficSource.keyword'].apply(add_new_category)","63b6a2e4":"train_df['trafficSource.keyword'].value_counts().sort_values(ascending=False).plot.bar()\nplt.yscale('log')\nplt.show()","0938d17d":"categorical_feats = ['trafficSource.keyword']","03b25b81":"train_df['totals.pageviews'].fillna(0, inplace=True)\ntest_df['totals.pageviews'].fillna(0, inplace=True)","70a92f82":"train_df['totals.pageviews'] = train_df['totals.pageviews'].astype(int)\ntest_df['totals.pageviews'] = test_df['totals.pageviews'].astype(int)","e459495c":"train_df['totals.pageviews'].plot.hist(bins=10)\nplt.yscale('log')\nplt.show()","ba50a7ee":"features_object = [col for col in train_df.columns if train_df[col].dtype == 'object']","c2802ea4":"features_object","6a917d6b":"train_df['channelGrouping'].value_counts().plot.bar()\nplt.show()","1771dc39":"categorical_feats.append('channelGrouping')","7987e28b":"plt.figure(figsize=(20, 10))\ntrain_df['device.browser'].value_counts().plot.bar()\nplt.yscale('log')\nplt.show()","8277975e":"categorical_feats.append('device.browser')","163cf448":"# plt.figure(figsize=(10, 10))\ntrain_df['device.deviceCategory'].value_counts().plot.bar()\n# plt.yscale('log')\nplt.show()","3513c54e":"categorical_feats.append('device.deviceCategory')","9de531dc":"# plt.figure(figsize=(10, 10))\ntrain_df['device.operatingSystem'].value_counts().plot.bar()\nplt.yscale('log')\nplt.show()","a4c7a3d3":"categorical_feats.append('device.operatingSystem')","bd832fd3":"train_df['geoNetwork.city'].value_counts()","dae7fb1b":"categorical_feats.append('geoNetwork.city')","0318e344":"train_df['geoNetwork.continent'].value_counts()","3ab758fb":"categorical_feats.append('geoNetwork.continent')","dcd22ff4":"train_df['geoNetwork.country'].value_counts()[:10].plot.bar()\nplt.show()","346bfff2":"categorical_feats.append('geoNetwork.country')","ad0d9b86":"train_df['geoNetwork.metro'].value_counts()[:10].plot.bar()","f17b03b2":"categorical_feats.append('geoNetwork.metro')","98c5bf4d":"train_df['geoNetwork.networkDomain'].value_counts()","6dcdd572":"categorical_feats.append('geoNetwork.networkDomain')","5b7a6720":"train_df['geoNetwork.region'].value_counts()","c24b6596":"categorical_feats.append('geoNetwork.region')","b4806864":"train_df['geoNetwork.subContinent'].value_counts().plot.bar()\nplt.yscale('log')\nplt.show()","abdf4fd4":"categorical_feats.append('geoNetwork.subContinent')","01d27348":"train_df['totals.hits'].value_counts()","d2ecfb52":"train_df['totals.hits'] = train_df['totals.hits'].astype(int)\ntest_df['totals.hits'] = test_df['totals.hits'].astype(int)","4b0a6730":"train_df['trafficSource.adwordsClickInfo.gclId'].value_counts()","39cd3b5c":"train_df.drop('trafficSource.adwordsClickInfo.gclId', axis=1, inplace=True)\ntest_df.drop('trafficSource.adwordsClickInfo.gclId', axis=1, inplace=True)","1c69e1fa":"train_df['trafficSource.campaign'].value_counts().plot.bar()\nplt.yscale('log')\nplt.show()","74942a54":"categorical_feats.append('trafficSource.campaign')","181f3df7":"train_df['trafficSource.medium'].value_counts().plot.bar()\nplt.yscale('log')\nplt.show()","34028a7c":"categorical_feats.append('trafficSource.medium')","80e1bc2e":"# for value in train_df['trafficSource.source'].value_counts().index:\n#     print(value)\n# save your page","af4b1ee7":"def add_new_category(x):\n    x = str(x).lower()\n    if 'google' in x:\n        return 'google'\n    elif 'baidu' in x:\n        return 'baidu'\n    elif 'facebook' in x:\n        return 'facebook'\n    elif 'reddit' in x:\n        return 'reddit'\n    elif 'yahoo' in x:\n        return 'yahoo'\n    elif 'bing' in x:\n        return 'bing'\n    elif 'yandex' in x:\n        return 'yandex'\n    else:\n        return 'other'","f1663f3a":"train_df['trafficSource.source'] = train_df['trafficSource.source'].apply(add_new_category)\ntest_df['trafficSource.source'] = test_df['trafficSource.source'].apply(add_new_category)","57bb542e":"train_df['trafficSource.source'].value_counts().sort_values(ascending=False).plot.bar()\nplt.yscale('log')\nplt.show()","33f53298":"categorical_feats.append('trafficSource.source')","6b06f9c6":"train_df['device.isMobile'] = train_df['device.isMobile'].astype(int)\ntest_df['device.isMobile'] = test_df['device.isMobile'].astype(int)","fd1e6072":"len_train = train_df.shape[0]\n\ndf_all = pd.concat([train_df, test_df])","90fbc416":"def change_date_to_datetime(x):\n    str_time = str(x)\n    date = '{}-{}-{}'.format(str_time[:4], str_time[4:6], str_time[6:])\n    return date\n\ndef add_time_feature(data):\n    data['date'] = pd.to_datetime(data['date'])\n    data['Year'] = data.date.dt.year\n    data['Month'] = data.date.dt.month\n    data['Day'] = data.date.dt.day\n    data['WeekOfYear'] = data.date.dt.weekofyear\n    return data\n\ndf_all['date'] = df_all['date'].apply(change_date_to_datetime)\ndf_all = add_time_feature(df_all)","c4b8ebb7":"categorical_feats += ['Year', 'Month', 'Day', 'WeekOfYear']","01f9d859":"df_all.drop('date', axis=1, inplace=True)","f331ef5b":"from sklearn.preprocessing import LabelEncoder","6676f4d7":"for col in categorical_feats:\n    lbl = LabelEncoder()\n    df_all[col] = lbl.fit_transform(df_all[col])","1e348395":"train_df = df_all[:len_train]\ntest_df = df_all[len_train:]","1d6cbd2e":"train_fullVisitorId = train_df['fullVisitorId']\ntrain_sessionId = train_df['sessionId']\ntrain_visitId = train_df['visitId']\n\ntest_fullVisitorId = test_df['fullVisitorId']\ntest_sessionId = test_df['sessionId']\ntest_visitId = test_df['visitId']\n\ntrain_df.drop(['fullVisitorId', 'sessionId', 'visitId'], axis=1, inplace=True)\ntest_df.drop(['fullVisitorId', 'sessionId', 'visitId'], axis=1, inplace=True)","549d3a93":"train_df.head()","11109262":"param = {'num_leaves':48,\n         'min_data_in_leaf': 300, \n         'objective':'regression',\n         'max_depth': -1,\n         'learning_rate':0.005,\n         \"min_child_samples\":40,\n         \"boosting\":\"gbdt\",\n         \"feature_fraction\":0.8,\n         \"bagging_freq\":1,\n         \"bagging_fraction\":0.8 ,\n         \"bagging_seed\": 3,\n         \"metric\": 'rmse',\n         \"lambda_l1\": 1,\n         'lambda_l2': 1,\n         \"verbosity\": -1}","97451172":"folds = KFold(n_splits=5, shuffle=True, random_state=15)\noof = np.zeros(len(train_df))\npredictions = np.zeros(len(test_df))\nstart = time.time()\nfeatures = list(train_df.columns)\nfeature_importance_df = pd.DataFrame()\n\nfor fold_, (trn_idx, val_idx) in enumerate(folds.split(train_df.values, target.values)):\n    trn_data = lgb.Dataset(train_df.iloc[trn_idx], label=target.iloc[trn_idx], categorical_feature=categorical_feats)\n    val_data = lgb.Dataset(train_df.iloc[val_idx], label=target.iloc[val_idx], categorical_feature=categorical_feats)\n    \n    num_round = 10000\n    clf = lgb.train(param, trn_data, num_round, valid_sets = [trn_data, val_data], verbose_eval=400, early_stopping_rounds = 500, categorical_feature=categorical_feats)\n    oof[val_idx] = clf.predict(train_df.iloc[val_idx].values, num_iteration=clf.best_iteration)\n    \n    fold_importance_df = pd.DataFrame()\n    fold_importance_df[\"feature\"] = features\n    fold_importance_df[\"importance\"] = clf.feature_importance()\n    fold_importance_df[\"fold\"] = fold_ + 1\n    feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n    \n    predictions += clf.predict(test_df.values, num_iteration=clf.best_iteration) \/ folds.n_splits","8bce7da3":"print(\"CV score: {:<8.5f}\".format(mean_squared_error(oof, target)**0.5))","0310674d":"cols = feature_importance_df[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(\n    by=\"importance\", ascending=False)[:1000].index\n\nbest_features = feature_importance_df.loc[feature_importance_df.feature.isin(cols)]\n\nplt.figure(figsize=(14,10))\nsns.barplot(x=\"importance\", y=\"feature\", data=best_features.sort_values(by=\"importance\", ascending=False))\nplt.title('LightGBM Features (avg over folds)')\nplt.tight_layout()\nplt.savefig('lgbm_importances.png')","5844f6c1":"submission = pd.DataFrame()\n\nsubmission['fullVisitorId'] = test_fullVisitorId\n\nsubmission['PredictedLogRevenue'] = predictions\n\ngrouped_test = submission[['fullVisitorId', 'PredictedLogRevenue']].groupby('fullVisitorId').sum().reset_index()\ngrouped_test.to_csv('submit.csv',index=False)","0eb19625":"### trafficSource.medium","6fee9bad":"## 3.3 Training model","e32ed82e":"### totals.pageviews","4b02a2ad":"### device.operatingSystem","ae052417":"## 3.2 Drop features","125f4664":"- THis feature is boolean type. So let's do astype(int)","bcb768c3":"### Remove features with NaN percents larger than 70%","d9edfb13":"# 2. Feature engineering","46aab49e":"### device.deviceCategory","51434656":"- I don't have any idea for this complex feature. For now, I want to remove this feature.","d3ef7379":"### geoNetwork.subContinent","25a8dd9a":"### channelGrouping","37b46807":"### geoNetwork.networkDomain","a2ca21fe":"### trafficSource.keyword ","bc5aa57d":"# 1. Read dataset","abe9b08e":"## 3.4 Submission","acee16c5":"### geoNetwork.continent","c1666dc4":"## 3.1 Label encoding","901a27e7":"-  US(California, New York), UK(England) and Thailand(Bangkok), Vietnam(Ho Chi Minh), Turkey(Istanbul) are top 5 region","14e1c446":"- There are so many cities.","29a7068f":"## 2.3 Time feature","a338dd45":"### trafficSource.campaign","adcdbded":"### geoNetwork.region","820d261c":"- After looking the feature, simply I think that the category feature can be divided into youtube, google, other categories.","873a2f24":"# 3. Model development","5b8e43af":"### \tdevice.isMobile","29b66569":"### device.browser","1c5365ac":"- There are so many domains. How can we deal with it?\n- One-hot is not good choice. It will generate 28064 featues...\n- Just remove this feature? or use this feature in efficient way?","e22a9794":"### totals.hits","949ea319":"- This feature could be considered as continuous feature","4f21740f":"- For now, Let's remove the columns with NaN percents larger than 70%","8745214d":"### geoNetwork.city","6f9e0d65":"- At first, I want to say thanks to many kernel authors.\n- I referred to below kernels.\n- https:\/\/www.kaggle.com\/fabiendaniel\/lgbm-starter-lb-1-70\n- https:\/\/www.kaggle.com\/julian3833\/1-quick-start-read-csv-and-flatten-json-fields\n- https:\/\/www.kaggle.com\/astrus\/entity-embedding-neural-network-keras-lb-0-748","e20c94cc":"- In this dataset, many user ares in US","31332d69":"- google, baidu, facebook, reddit, yahoo, bing, yandex, ","c2682f62":"- NaN values in this feature could be relplaced with 0 value becuase nan view would mean no view.","05b6ef96":"## 2.1 Check null data","ca2e6723":"## 2.2 Object features","754b86c4":"### geoNetwork.country","17dc0a62":"### geoNetwork.metro","b63b81b0":"### trafficSource.adwordsClickInfo.gclId","ae004aa4":"### trafficSource.source"}}