{"cell_type":{"ed108511":"code","f44be56d":"code","46fe7c1b":"code","4cd822f9":"code","061efcff":"code","d45a3f17":"code","c9765f27":"code","3b4b8fb4":"code","e0e06539":"code","b2d2c0d0":"code","7291fb2d":"code","db3ba2f5":"code","0f535786":"code","0ab23185":"code","5b4bdf01":"code","dc951f05":"code","c2b08041":"code","61ad4d5f":"code","36798f5b":"code","0ee2cf82":"code","71b59e71":"code","7d4a667d":"code","0cc802ef":"code","8b2c8a4d":"code","c17a4a32":"code","a1b34906":"code","e2ee08f4":"code","07a8dd4d":"code","91353346":"code","5a748122":"code","d3286cba":"code","a32886ef":"code","79e1ffd6":"code","5e121dd6":"code","8e75da0a":"code","6fab75b4":"code","2a2fea16":"code","7af8bd58":"code","756c7922":"code","4edab113":"code","e0f090a6":"code","febcb501":"code","c999cdc3":"code","89ee022d":"code","c261aab5":"code","1e660597":"code","5a90688a":"code","e6f6e7c7":"code","2771c175":"code","1f599618":"code","1353a7d4":"code","a83649dd":"code","1cf78eb7":"code","e73e98a8":"code","41a61607":"code","c2e0d7bc":"code","db2fa301":"code","6194a4c7":"markdown","7d3765bd":"markdown","4566268d":"markdown","6c6d1116":"markdown","be4bea43":"markdown","9762e912":"markdown","a966fe07":"markdown","690631c3":"markdown","38613872":"markdown","d81e8043":"markdown","a6f48a67":"markdown","d7b93a7c":"markdown","4a25d58e":"markdown","a7a56526":"markdown","1971a038":"markdown","f24bf25f":"markdown","0fd825e7":"markdown","3ada010b":"markdown","32a6b166":"markdown","00b1bdec":"markdown","8c8074f5":"markdown","5c6c6d8d":"markdown","591bd14c":"markdown","49ca5358":"markdown","2ade2d28":"markdown","f2fb78f7":"markdown","16baab58":"markdown","c5861763":"markdown","a18322b0":"markdown","817647c6":"markdown","65b3292c":"markdown","0ff0f055":"markdown","0e012f9d":"markdown","4ab068e2":"markdown","7c830f96":"markdown","8c81e62c":"markdown","c556b38d":"markdown","79f6958d":"markdown","2a2b3c95":"markdown","42badb4a":"markdown","2f629c63":"markdown","2a0ae621":"markdown","df95a352":"markdown","170a7c17":"markdown","dcc5744d":"markdown","72660821":"markdown","4b536c1f":"markdown","54337aa2":"markdown","6d48fb7a":"markdown","fa8baf2e":"markdown","8df51a03":"markdown","7aaccca1":"markdown","21fd8479":"markdown"},"source":{"ed108511":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","f44be56d":"import pandas as pd \nfrom IPython.display import display\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import f1_score\nfrom sklearn.utils import shuffle\nimport numpy as np\nfrom sklearn.metrics import precision_score, recall_score\nfrom sklearn.metrics import precision_recall_curve\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import roc_curve","46fe7c1b":"    data = pd.read_csv('..\/input\/bank-customer-churn-modeling\/Churn_Modelling.csv')\n   \n","4cd822f9":"data.head()","061efcff":"data.columns = data.columns.str.lower()\ndata.columns","d45a3f17":"data.columns = ['row_number', 'customer_id', 'surname', 'creditscore', 'geography',\n       'gender', 'age', 'tenure', 'balance', 'num_of_products', 'has_crcard',\n       'isactive_member', 'estimated_salary', 'exited']\ndata.columns","c9765f27":"data.info()","3b4b8fb4":"data.shape","e0e06539":"data.describe()","b2d2c0d0":"data['tenure'].describe()","7291fb2d":"data['tenure'].hist()","db3ba2f5":"display(data.isna().sum())","0f535786":"data.duplicated().sum()","0ab23185":"data['exited'].value_counts()","5b4bdf01":"print('Percentage of positive ratings: {:.2%}'.format(data['exited'].mean()))\n\n","dc951f05":"data.hist(bins=50, figsize=(20,15), edgecolor='black', linewidth=2)\nplt.show()","c2b08041":"data1 = data.drop([\"row_number\",\"customer_id\",\"surname\"],axis=1).copy()\ndisplay(data1.head())","61ad4d5f":"exited_yes= data1[data1['exited'] == 1]['geography']\nexited_no = data1[data1['exited'] == 0]['geography']\n\nplt.hist([exited_yes,exited_no],label=(\"Yes\",\"No\"))\nplt.xlabel(\"Country\")\nplt.ylabel(\"Number of People\")\nplt.legend();","36798f5b":"tenure_yes = data1[data1.exited ==1].tenure\ntenure_no = data1[data1.exited ==0].tenure\n\nplt.hist([tenure_yes, tenure_no], label = (\"Yes\", \"No\"))\nplt.xlabel(\"Tenure\")\nplt.ylabel(\"Numper of People\")\nplt.legend();","0ee2cf82":"sns.countplot(data=data1,x=data1['exited'], hue=data1['gender'])","71b59e71":"sns.countplot(x = data1['isactive_member'], hue = data1['exited'])","7d4a667d":"sns.countplot(x = data1['has_crcard'], hue = data1['exited'])","0cc802ef":"geography_ohe = pd.get_dummies(data['geography'], drop_first = True)\ngender_ohe = pd.get_dummies(data['gender'], drop_first=True)\n\ndata1.drop(['gender', 'geography'], axis = 1 , inplace = True)\n\ndata_ohe = pd.concat([data1, geography_ohe, gender_ohe], axis = 1)\n\ndata_ohe.head()","8b2c8a4d":"data_ohe.info()","c17a4a32":"display(data_ohe.isna().sum())","a1b34906":"data.corr()","e2ee08f4":"pd.plotting.scatter_matrix(data, figsize = (30, 30))","07a8dd4d":"features = data_ohe.drop(['exited'], axis = 1)\ntarget = data_ohe['exited']","91353346":"from sklearn.model_selection import train_test_split\n\nfeatures_train , features_remain , target_train , target_remain = train_test_split(features, target, \n                                                                                  test_size = 0.4, random_state=12345)\n\nfeatures_valid , features_test , target_valid , target_test = train_test_split(features_remain, target_remain,\n                                                                              test_size = 0.5, random_state=12345)","5a748122":"features_train.shape\n\n","d3286cba":"features_valid.shape","a32886ef":"features_test.shape\n","79e1ffd6":"from sklearn.preprocessing import StandardScaler\n\nnumeric = ['creditscore', 'age', 'balance', 'estimated_salary']\nscaler = StandardScaler()\nscaler.fit(features_train[numeric])\n\npd.options.mode.chained_assignment = None\n\nfeatures_train[numeric] = scaler.transform(features_train[numeric])\n\nfeatures_train.head()\n\n","5e121dd6":"features_valid[numeric] = scaler.transform(features_valid[numeric])\nfeatures_valid.head()","8e75da0a":"features_test[numeric] = scaler.transform(features_test[numeric])\nfeatures_test.head()","6fab75b4":"model = LogisticRegression(random_state =12345, solver='liblinear')\nmodel.fit(features_train, target_train)\npredicted_valid = model.predict(features_valid)\nprint(\"F1:\", f1_score(target_valid, predicted_valid))","2a2fea16":"model = LogisticRegression(random_state =12345, solver='liblinear', class_weight = 'balanced')\nmodel.fit(features_train, target_train)\npredicted_valid = model.predict(features_valid)\nprint(\"F1:\", f1_score(target_valid, predicted_valid))","7af8bd58":"model = RandomForestClassifier(random_state = 12345, n_estimators = 10)\nmodel.fit(features_train, target_train)\npredicted_valid = model.predict(features_valid)\nprint(\"F1:\", f1_score(target_valid, predicted_valid))","756c7922":"model = RandomForestClassifier(random_state = 12345, n_estimators = 10,\n                              class_weight = 'balanced')\nmodel.fit(features_train, target_train)\npredicted_valid = model.predict(features_valid)\nprint(\"F1:\", f1_score(target_valid, predicted_valid))","4edab113":"data['exited'].value_counts().to_frame()","e0f090a6":"def upsample(features, target, repeat):\n    features_zeros = features[target == 0]\n    features_ones = features[target == 1]\n    target_zeros = target[target == 0]\n    target_ones = target[target == 1]\n    \n    features_upsample = pd.concat([features_zeros] + [features_ones] * repeat )\n    target_upsample  = pd.concat([target_zeros] + [target_ones] * repeat)\n    \n    features_upsample, target_upsample = shuffle(features_upsample, target_upsample, \n                                                random_state = 12345)\n    return features_upsample, target_upsample\n\nfeatures_upsample, target_upsample = upsample(features_train , target_train, 5)\n\n","febcb501":"target_upsample.shape\n\n","c999cdc3":"features_upsample.shape\n\n","89ee022d":"model = LogisticRegression(random_state = 12345, solver = 'liblinear')\nmodel.fit(features_upsample, target_upsample)\npredicted_valid = model.predict(features_valid)\nprint('F1:', f1_score(target_valid, predicted_valid))","c261aab5":"model = RandomForestClassifier(random_state = 12345, n_estimators = 10)\nmodel.fit(features_upsample, target_upsample)\npredicted_valid = model.predict(features_valid)\nprint('F1:', f1_score(target_valid, predicted_valid))\n\n","1e660597":"def downsample(features, target, fraction):\n    features_zeros = features[target == 0]\n    features_ones = features[target == 1]\n    target_zeros = target[target == 0]\n    target_ones = target[target == 1]\n    \n    features_downsample = pd.concat(\n        [features_zeros.sample(frac = fraction, random_state = 12345)] + [features_ones])\n    \n    target_downsample = pd.concat(\n    [target_zeros.sample(frac = fraction, random_state = 12345)] + [target_ones])\n    \n    features_downsample, target_downsample = shuffle(features_downsample, target_downsample,\n                                                    random_state = 12345)\n    return features_downsample, target_downsample\n\nfeatures_downsample, target_downsample = downsample(features_train, target_train, 0.1)\n                                   \n    ","5a90688a":"features_downsample.shape","e6f6e7c7":"target_downsample.shape","2771c175":"model = LogisticRegression(random_state = 12345, solver = 'liblinear')\nmodel.fit(features_downsample, target_downsample)\npredicted_valid = model.predict(features_valid)\nprint('F1:', f1_score(target_valid, predicted_valid))","1f599618":"model = RandomForestClassifier(random_state = 12345, n_estimators = 10)\nmodel.fit(features_downsample, target_downsample)\npredicted_valid = model.predict(features_valid)\nprint('F1:', f1_score(target_valid, predicted_valid))","1353a7d4":"model = LogisticRegression(random_state = 12345, solver = 'liblinear')\nmodel.fit(features_train, target_train)\nprobabilities_valid = model.predict_proba(features_valid) \nprobabilities_one_valid = probabilities_valid[:, 1]\n\nfor threshold in np.arange(0, 0.95, 0.05):\n    predicted_valid = probabilities_one_valid > threshold\n    precision = precision_score(target_valid, predicted_valid)\n    recall = recall_score(target_valid, predicted_valid)\n    f1 = f1_score(target_valid, predicted_valid)\n    print(\"Threshold = {:.2f} | Accuracy = {:.3f}, Completeness = {:.3f} | F1 = {:.3f}\".format(\n        threshold, precision, recall, f1))\n    \nprecision, recall, threshold = precision_recall_curve(target_valid, probabilities_one_valid)\n    \nplt.figure(figsize=(10,10))\nplt.step(recall, precision, where = 'post')\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.ylim([0.0, 1.05])\nplt.xlim([0.0, 1.0])\nplt.title(' Precision-Recall')\nplt.show() \n\n","a83649dd":"best_model = None\nbest_result = 0\n\nfor est in range(1, 20):\n    model = RandomForestClassifier(random_state = 12345, n_estimators = est)\n    model.fit(features_train, target_train)\n    probabilities_valid = model.predict_proba(features_valid) \n    probabilities_one_valid = probabilities_valid[:, 1]\n    \n    for threshold in np.arange(0, 0.95, 0.05):  \n        predicted_valid = probabilities_one_valid > threshold\n        precision = precision_score(target_valid, predicted_valid)\n        recall = recall_score(target_valid, predicted_valid)\n        f1 = f1_score(target_valid, predicted_valid)\n                \n        if precision > best_result :\n            best_model = model\n            best_result = precision\n        \n        if recall > best_result :\n            best_model = model\n            best_result = recall\n       \n        if threshold > best_result :\n            best_model = model\n            best_result = threshold \n          \n        if f1 > best_result :\n            best_model = model\n            best_result = f1\n            \n        print('est = {:.0f}'.format(est))\n        print(\"Threshold = {:.2f} | Accuracy = {:.3f}, Completeness = {:.3f} | F1 = {:.3f}\".format(\n        threshold, precision, recall, f1))\n        print('_________________________________________')\nprecision, recall, threshold = precision_recall_curve(target_valid, probabilities_one_valid)\n    \nplt.figure(figsize=(10,10))\nplt.step(recall, precision, where = 'post')\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.ylim([0.0, 1.05])\nplt.xlim([0.0, 1.0])\nplt.title('Precision-Recall')\nplt.show() ","1cf78eb7":"final_model = RandomForestClassifier(random_state = 12345, \n                                      n_estimators = 45,\n                                    max_depth = 50)\nfinal_model.fit(features_upsample, target_upsample)\nprediction_test = final_model.predict(features_test)\n\n","e73e98a8":"conf_matrix = confusion_matrix(target_test, prediction_test)\nprint(conf_matrix)","41a61607":"%%time\naccuracy = accuracy_score(target_test, prediction_test)\nprecision = precision_score(target_test, prediction_test)\nrecall = recall_score(target_test, prediction_test)\nf1 = f1_score(target_test, prediction_test)\n\nprint('Accuracy:', accuracy)\nprint('Precision:', precision)\nprint('Recall:', recall)\nprint('F1:', f1)","c2e0d7bc":"probabilities = final_model.predict_proba(features_test)[:, 1] \n\nfpr, tpr, thresholds = roc_curve(target_test, probabilities)\n\nplt.figure()\nplt.plot(fpr, tpr)\nplt.plot([0,1],[0,1], linestyle= '--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.0])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC-\u043a\u0440\u0438\u0432\u0430\u044f')\nplt.show()","db2fa301":"auc_roc = roc_auc_score(target_test, probabilities)\nprint('AUC:',auc_roc)","6194a4c7":"- Calculate AUC","7d3765bd":"- Let's look  and compare the fact of leaving customers with their activity","4566268d":"- To assess the quality, we visualize the error matrix","6c6d1116":"#### Random Forest :","be4bea43":"There is an improvement here too.","9762e912":"#### Random Forest","a966fe07":"1. Let's create hists where :\n- Let's look  and compare the fact of customer exit in different countries","690631c3":"Missing values are not observed","38613872":"## Model testing","d81e8043":"We can conclude that most customers leave with a ten-year term in the bank. Unexpectedly","a6f48a67":"Downsampling shows worse results than Upsampling for all algorithms.\nLet's try to change the threshold and see what the indicators will be - this time we will turn to recall and precision.","d7b93a7c":"3. Let's do a correlation analysis of the data:","4a25d58e":"#### Logistic Regression\n\n- the threshold value is specified from 0 to 0.95 with a step of 0.02","a7a56526":"# Conclusion:\nOur goal was to create a model that can predict customer churn.\n\nThe main task was:\n\n- create a model in which F1 is higher than 0.59;\n- comparison of different methods of balancing target attribute classes;\n- testing different models and choosing the best one.\n- At the first step, research and data preprocessing were performed:\n  * gaps in the Tenure column have been replaced with median values;\n  * categorical features were converted to numerical ones using One-Hot Encoding\n- As models for study, we used Logistic Regression and Random Forest (which performed best)\n- Three methods were used to balance target attribute classes:\n  * class_weight\n  * upsampling\n  * downsampling\n- A random forest was chosen as the final model, the method of dealing with imbalance is upsampling\n- An error matrix was built: True Positive (TP) - 229 True Negative (TN) - 1454 False Positive (FP) - 123 False Negative (FN) - 194\n- The error matrices were calculated with the following metrics:\n  * Accuracy: 0.8415\n  * Precision: 0.6505681818181818\n  * Recall: 0.541371158392435\n  * F1: 0.590967741935484\n- We built an ROC curve. AUC value - 0.8440795657433765","1971a038":"- train \"test\"","f24bf25f":"5. Let's do feature scaling. Scaling features across the entire dataset can lead to data leakage.\n-  We will train only 'train'.","0fd825e7":"We get a test sample of 20% and again divide the remaining 80% to get a test sample. We will train on 60% of the data","3ada010b":"2. Columns of geography and gender will be converted to numeric using OHE, we need quantitative characteristics for greater accuracy","32a6b166":"### Dowmsampling","00b1bdec":"- divide the training sample into negative and positive objects\n- randomly discard some of the negative items\n- taking into account the received data, we will create a new training sample\n- shuffle the data","8c8074f5":"- Random forest showed the best result. Let's also try with class_weight = 'balanced'","5c6c6d8d":"### Data preparation","591bd14c":"#### Random Forest","49ca5358":"#### Logistic Regression :","2ade2d28":"It seems that the share of people with cards is approximately the same in the group of those who left and those who did not.","f2fb78f7":"To test the model, we use the Random Forest model, and to combat the imbalance, we use upsampling, since it showed the best result.","16baab58":"#### Logistic Regression","c5861763":"No correlation is observed.","a18322b0":"- Let's look at \"Exited\" :","817647c6":"We do not observe anomalies in the dataset and significant outliers. For example, the minimum age is 18 and the maximum is 92, this is normal.","65b3292c":"We can conclude that the clients of France and Germany left the bank most of all, but in France there are twice as many clients as in Germany, therefore, in percentage terms, most of the clients left in Germany.","0ff0f055":"- Now it is better . But now we will not choose hyperparameters, let's move on to the next algorithm","0e012f9d":"We observe a slight increase in the metric, close to the one we received by specifying the class_weight parameter","4ab068e2":"- Let's train \"valid\"","7c830f96":"## Take a look into this problem :","8c81e62c":"The highest score is achieved at a threshold of 0.3.","c556b38d":"- Let's look  and compare the fact of leaving clients with different age in the bank","79f6958d":"In the process of developing functions for our model, we will remove three columns - customer_id, row_number and surname, which in our case do not carry a payload.","2a2b3c95":"### Combat imbalanced :","42badb4a":"We see that inactive clients leave the most, as expected.","2f629c63":"We can see more women leaving the bank.","2a0ae621":"- Pretty bad figure, repeat with `class_weight = 'balanced' '","df95a352":"For a threshold of 0, recall is 1 - all responses are positive. At a threshold of 0.85, the model stops giving correct answers. The highest value of F1 is observed at a threshold of 0.25.","170a7c17":"- Calculate metrics","dcc5744d":"- Let's build a ROC curve","72660821":"7. Random Forest","4b536c1f":"- Let's look and compare the fact of leaving customers with the presence of a credit card.","54337aa2":"6. Let's start from Logistic Regression","6d48fb7a":"4. Let's break the data into samples:","fa8baf2e":"### Threshold change","8df51a03":"- We will also not choose hyperparameters now until we fix the problem with the imbalance.","7aaccca1":"### Upsampling :\n- Divide the training sample into negative and positive\n- Duplicate the objects of the positive class and merge them with the - Objects of the negative class\n- Mix data","21fd8479":"- Let's look and compare the fact of care for men and women"}}