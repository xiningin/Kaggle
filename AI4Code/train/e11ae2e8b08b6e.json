{"cell_type":{"19cf6b77":"code","6afdad85":"code","788f2eeb":"code","d3dfc85e":"code","c469033d":"code","f04c3231":"code","0752c55b":"code","a74fc7a9":"code","d4fc862d":"code","c9fefda1":"code","9134a909":"code","395ea570":"code","d193f1e3":"code","4b232143":"code","93212388":"code","5d30535c":"code","46f0d8a6":"code","6c1108da":"code","fed89d02":"code","8656b1a2":"code","dfd1abc6":"code","51f34297":"code","08c9fe5f":"code","58aa8e87":"code","d1cb8d49":"code","c8a53b52":"code","f2002fc9":"code","63002b95":"code","7de8db61":"code","0983cf9c":"code","90244df0":"code","f8b3a390":"code","96fb23bd":"code","287b9284":"code","62181fb4":"code","4b3fc9bd":"code","57f9b6fd":"code","bb47badf":"code","a302324f":"code","55078187":"code","19abac9c":"code","3e1383ef":"code","ecf3c7ce":"code","81fd5b8a":"code","241095a4":"code","f062b56c":"code","b7df6b06":"code","93b4cc6b":"code","0f349c5b":"code","6cb19fc8":"code","19afd672":"code","36a40661":"code","eb7ddbe5":"code","14a513f0":"markdown","772a539e":"markdown","3012ba47":"markdown","aa2fe024":"markdown","5210f677":"markdown","51c30486":"markdown","4f9c5b54":"markdown"},"source":{"19cf6b77":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport pickle\nimport seaborn as sns\nfrom statsmodels.tsa.stattools import adfuller\nimport matplotlib.pyplot as plt\n\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost.sklearn import XGBRegressor\n\nimport keras\nfrom keras.layers import Dense\nfrom keras.models import Sequential\nfrom keras.layers import LSTM\n\nimport statsmodels.api as sm\nimport statsmodels.tsa.api as smt\n\n\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","6afdad85":"def downcast(df):\n    dtypes = df.dtypes\n    cols = dtypes.index.tolist()\n    types = dtypes.values.tolist()\n    \n    for col, typ in zip(cols, types):\n        \n        if 'int' in str(typ):\n            if df[col].min() > np.iinfo(np.int8).min and \\\n                df[col].max() < np.iinfo(np.int8).max:\n                df[col] = df[col].astype(np.int8)\n            \n            elif df[col].min() > np.iinfo(np.int16).min and \\\n                df[col].max() < np.iinfo(np.int16).max:\n                df[col] = df[col].astype(np.int16)\n                \n            elif df[col].min() > np.iinfo(np.int32).min and \\\n                df[col].max() < np.iinfo(np.int32).max:\n                df[col] = df[col].astype(np.int32)\n                \n            else:\n                df[col] = df[col].astype(np.int64)\n                \n        elif 'float' in str(typ):\n            if df[col].min() > np.finfo(np.float16).min and \\\n                df[col].max() < np.finfo(np.float16).max:\n                df[col] = df[col].astype(np.float16)\n                \n            elif df[col].min() > np.finfo(np.float32).min and \\\n                df[col].max() < np.finfo(np.float32).max:\n                df[col] = df[col].astype(np.float32)\n                \n            else:\n                df[col] = df[col].astype(np.float64)\n                \n        elif typ == np.object:\n            if col == 'date':\n                df[col] = pd.to_datetime(df[col], format='%Y-%m-%d')\n                \n            else:\n                df[col] = df[col].astype('category')\n    \n    return df","788f2eeb":"#sales = pd.read_csv('..\/input\/m5-forecasting-accuracy\/sales_train_validation.csv')\n#price_df = pd.read_csv(\"..\/input\/m5-forecasting-accuracy\/sell_prices.csv\")\ncal_df = pd.read_csv(\"..\/input\/m5-forecasting-accuracy\/calendar.csv\")","d3dfc85e":"sales = downcast(sales)","c469033d":"df = pd.melt(\n    sales,\n    id_vars=[\n        'id',\n        'item_id',\n        'dept_id',\n        'cat_id',\n        'store_id',\n        'state_id'\n    ],\n    var_name='d',\n    value_name='sold')\n\ndf.head()","f04c3231":"df[\"d\"] = df[\"d\"].apply(lambda x: x.split('_')[1]).astype(np.int16)","0752c55b":"df_level_1=pd.DataFrame(df.groupby([\"d\"])['sold'].sum())","a74fc7a9":"from datetime import date\n\ndf_level_1.set_index(pd.to_datetime(pd.date_range(date(2011, 1,29), periods=1913).tolist()), inplace=True, drop=True)\n\n","d4fc862d":"df_level_1 = df_level_1.rename({'sold': 'sales'}, axis=1)","c9fefda1":"os.remove(\"\/kaggle\/input\/m5uploaded\/level_1_diff.pkl\")","9134a909":"df_level_1=pd.DataFrame()\nwith open('\/kaggle\/input\/m5upload\/m5_sales_agg_level_1.pkl', 'rb') as f:\n        df_level_1= pickle.load(f)","395ea570":"\nwith open('\/kaggle\/input\/m5upload\/m5_level_1_diff.pkl', 'rb') as f:\n        df_level_1_diff= pickle.load(f)","d193f1e3":"\n#with open('\/kaggle\/working\/m5uploaded\/level_1_diff.pkl', 'wb') as f:\n#        pickle.dump(df_level_1, f)","4b232143":"def test_stationarity(timeseries):\n    \n    #Determing rolling statistics\n    rolmean = timeseries.rolling(window=5).mean()\n    rolstd = timeseries.rolling(window=5).std()\n    \n    #Plot rolling statistics:\n    \n    plt.figure(figsize=(25,10))\n    \n    orig = plt.plot(timeseries, color='c',label='Original')\n    mean = plt.plot(rolmean, color='g', label='Rolling Mean')\n    std = plt.plot(rolstd, color='black', label = 'Rolling Std')\n    \n    \n    plt.legend(loc='best')\n    plt.title('Rolling Mean & Standard Deviation')\n    plt.show(block=False)\n    \n    #Perform Dickey-Fuller test:\n    print ('Results of Dickey-Fuller Test:')\n    dftest = adfuller(timeseries, autolag='AIC')\n    dfoutput = pd.Series(dftest[0:4], index=['Test Statistic','p-value','#Lags Used','Number of Observations Used'])\n    for key,value in dftest[4].items():\n        dfoutput['Critical Value (%s)'%key] = value\n    print (dfoutput)","93212388":"plt.figure(figsize=(25,10))\n\nplt.plot(df_level_1.index.values, df_level_1.values, linestyle='solid')\nplt.xlabel('Day')\nplt.ylabel('Unit Sold')\nplt.title('Unit Sold Over time')","5d30535c":"#trending, visually mean is not constant, p-value bigger than 10%, Dickey-Fuller Test fails to reject null hypothesis, series has a unit root, is non-stationary\ntest_stationarity(df_level_1)","46f0d8a6":"from statsmodels.tsa.seasonal import seasonal_decompose\ndecomposition = seasonal_decompose(df_level_1, period=365)\n\ntrend = decomposition.trend\nseasonal = decomposition.seasonal\nresidual = decomposition.resid\n\nplt.figure(figsize=(40,30))\nplt.subplot(411)\nplt.plot(df_level_1, label='Original')\nplt.legend(loc='best')\nplt.subplot(412)\nplt.plot(trend, label='Trend')\nplt.legend(loc='best')\nplt.subplot(413)\nplt.plot(seasonal,label='Seasonality')\nplt.legend(loc='best')\nplt.subplot(414)\nplt.plot(residual, label='Residuals')\nplt.legend(loc='best')\nplt.tight_layout()","6c1108da":"#first level differencing \ndf_level_1_diff = df_level_1-df_level_1.shift()\ndf_level_1_diff.dropna(inplace=True)\n\ndf_level_1_diff = df_level_1_diff.rename({'sales': 'sales_diff'}, axis=1)\n\n","fed89d02":"test_stationarity(df_level_1_diff)","8656b1a2":"#from statsmodels.graphics.tsaplots import plot_acf\n#from statsmodels.graphics.tsaplots import plot_pacf\n#plot_acf(df_level_1)\n#plot_pacf(df_level_1, lags=50)","dfd1abc6":"import statsmodels.api as sm\n\nfig = plt.figure(figsize=(12,8))\nax1 = fig.add_subplot(211)\nfig = sm.graphics.tsa.plot_acf(df_level_1.sales, lags=40, ax=ax1) # \nax2 = fig.add_subplot(212)\nfig = sm.graphics.tsa.plot_pacf(df_level_1.sales, lags=40, ax=ax2)# , lags=40","51f34297":"fig = plt.figure(figsize=(12,8))\nax1 = fig.add_subplot(211)\nfig = sm.graphics.tsa.plot_acf(df_level_1_diff, lags=40, ax=ax1)\nax2 = fig.add_subplot(212)\nfig = sm.graphics.tsa.plot_pacf(df_level_1_diff, lags=40, ax=ax2)","08c9fe5f":"#with open('\/kaggle\/working\/level_1_diff.pkl', 'wb') as f:\n#        pickle.dump(df_level_1_diff, f)","58aa8e87":"def generate_supervised(data):\n    \"\"\"Generates a dataframe where each row represents a day and columns\n    include sales, the dependent variable, and prior sales for each lag 7 lag features are generated. Data is used for regression modeling.\n    Output df:\n    d1  sales_diff  lag1  lag2  lag3 ... lag7 \n    d2  sales_diff  lag1  lag2  lag3 ... lag7 \n    \"\"\"\n    supervised_df = data.copy()\n\n    #create column for each lag\n    for i in range(1,8):\n        col_name = 'lag_' + str(i)\n        supervised_df[col_name] = supervised_df['sales_diff'].shift(i)\n\n    #drop null values\n    supervised_df = supervised_df.dropna().reset_index(drop=True)\n    return supervised_df\n    \n","d1cb8d49":"    \nreg_model_df = generate_supervised(df_level_1_diff)\n","c8a53b52":"with open('\/kaggle\/working\/regmodel_df.pkl', 'wb') as f:\n        pickle.dump(reg_model_df, f)","f2002fc9":"def tts(data):\n    \"\"\"Splits the data into train and test. Test set consists of the last 28\n    days of data.\n    \"\"\"\n    #data = data.drop(['sales', 'date'], axis=1)\n    train, test = data[0:-28], data[-28:]\n\n    return train, test","63002b95":"def scale_data(train_set, test_set):\n    \"\"\"Scales data using MinMaxScaler and separates data into X_train, y_train,\n    X_test, and y_test.\n    Keyword Arguments:\n    -- train_set: dataset used to train the model\n    -- test_set: dataset used to test the model\n    \"\"\"\n\n    #apply Min Max Scaler\n    scaler = MinMaxScaler(feature_range=(-1, 1))\n    scaler = scaler.fit(train_set)\n\n    # reshape training set\n\n    train_set = train_set.values.reshape(train_set.shape[0], train_set.shape[1])\n    train_set_scaled = scaler.transform(train_set)\n\n    # reshape test set\n    test_set = test_set.values.reshape(test_set.shape[0], test_set.shape[1])\n    test_set_scaled = scaler.transform(test_set)\n\n    X_train, y_train = train_set_scaled[:, 1:], train_set_scaled[:, 0:1].ravel()\n    X_test, y_test = test_set_scaled[:, 1:], test_set_scaled[:, 0:1].ravel()\n\n    return X_train, y_train, X_test, y_test, scaler","7de8db61":"def undo_scaling(y_pred, x_test, scaler_obj, lstm=False):\n    \"\"\"For visualizing and comparing results, undoes the scaling effect on\n    predictions.\n    Keyword arguments:\n    -- y_pred: model predictions\n    -- x_test: features from the test set used for predictions\n    -- scaler_obj: the scaler objects used for min-max scaling\n    -- lstm: indicate if the model run is the lstm. If True, additional\n             transformation occurs\n    \"\"\"\n\n    #reshape y_pred\n    y_pred = y_pred.reshape(y_pred.shape[0], 1, 1)\n\n    if not lstm:\n        x_test = x_test.reshape(x_test.shape[0], 1, x_test.shape[1])\n\n    #rebuild test set for inverse transform\n    pred_test_set = []\n    for index in range(0, len(y_pred)):\n        pred_test_set.append(np.concatenate([y_pred[index], x_test[index]],\n                                            axis=1))\n\n    #reshape pred_test_set\n    pred_test_set = np.array(pred_test_set)\n    pred_test_set = pred_test_set.reshape(pred_test_set.shape[0],\n                                          pred_test_set.shape[2])\n\n    #inverse transform\n    pred_test_set_inverted = scaler_obj.inverse_transform(pred_test_set)\n\n    return pred_test_set_inverted\n","0983cf9c":"def predict_df(unscaled_predictions, original_df, is_arima=False):\n    \"\"\"Generates a dataframe that shows the predicted sales for each day\n    for plotting results.\n    Keyword arguments:\n    -- unscaled_predictions: the model predictions that do not have min-max or\n                             other scaling applied\n    -- original_df: the original daily sales dataframe\n    \"\"\"\n    \n    #create dataframe that shows the predicted sales\n    result_list = []\n    #sales_dates = list(original_df[-29:].date)\n    sales_dates = list(original_df[-29:].index)\n    \n    act_sales = list(original_df[-29:].sales)\n\n    for index in range(0, len(unscaled_predictions)):\n        result_dict = {}\n        \n        if not is_arima:\n            result_dict['pred_value'] = int(unscaled_predictions[index][0] +\n                                        act_sales[index])\n        else:\n            result_dict['pred_value'] = int(unscaled_predictions.iloc[index][0] +\n                                        act_sales[index])\n            \n        result_dict['date'] = sales_dates[index+1]\n        result_list.append(result_dict)\n\n    df_result = pd.DataFrame(result_list)\n    df_result.set_index([\"date\"],drop=True, inplace=True)\n    return df_result","90244df0":"model_scores = {}\n\ndef get_scores(unscaled_df, original_df, model_name):\n    \"\"\"Prints the root mean squared error, mean absolute error, and r2 scores\n    for each model. Saves all results in a model_scores dictionary for\n    comparison.\n    Keyword arguments:\n    -- unscaled_predictions: the model predictions that do not have min-max or\n                             other scaling applied\n    -- original_df: the original daily sales dataframe\n    -- model_name: the name that will be used to store model scores\n    \"\"\"\n    rmse = np.sqrt(mean_squared_error(original_df.sales[-28:], unscaled_df.pred_value[-28:]))\n    mae = mean_absolute_error(original_df.sales[-28:], unscaled_df.pred_value[-28:])\n    r2 = r2_score(original_df.sales[-28:], unscaled_df.pred_value[-28:])\n\n    rmsse_score = rmsse(np.array(ground_truth), np.array(unscaled_df), np.array(train_series), 0)\n    \n    model_scores[model_name] = [rmse, mae, r2, rmsse_score]\n\n    print(f\"RMSE: {rmse}\")\n    print(f\"MAE: {mae}\")\n    print(f\"R2 Score: {r2}\")\n    print(f\"rmsse: {rmsse_score}\")","f8b3a390":"def plot_results(results, original_df, model_name):\n    \"\"\"Plots predictions over original data to visualize results. Saves each\n    plot as a png.\n    Keyword arguments:\n    -- results: a dataframe with unscaled predictions\n    -- original_df: the original daily sales dataframe\n    -- model_name: the name that will be used in the plot title\n    \"\"\"\n    fig, ax = plt.subplots(figsize=(25, 7))\n\n    original_df_plot = original_df.iloc[-28:, :]\n    sns.lineplot(original_df_plot.index, original_df_plot.sales, data=original_df_plot, ax=ax,\n                 label='Original', color='mediumblue')\n\n    sns.lineplot(results.index, results.pred_value, data=results, ax=ax,\n                 label='Predicted', color='red')\n    ax.set(xlabel=\"Date\",\n           ylabel=\"Sales\",\n           title=f\"{model_name} Sales Forecasting Prediction\")\n    ax.legend()\n    sns.despine()\n\n    plt.savefig(f'\/kaggle\/working\/{model_name}_forecast.png')","96fb23bd":"def regressive_model(train_data, test_data, model, model_name, original_df):\n    \n    # Call helper functions to create X & y and scale data\n    X_train, y_train, X_test, y_test, scaler_object =  scale_data(train_data, test_data)\n    \n    # Run regression model\n    mod = model\n    mod.fit(X_train, y_train)\n    predictions = mod.predict(X_test)\n    # Call helper functions to undo scaling & create prediction df\n    unscaled = undo_scaling(predictions, X_test, scaler_object)\n    unscaled_df = predict_df(unscaled, original_df, False)\n    # Call helper functions to print scores and plot results\n    get_scores(unscaled_df, original_df, model_name)\n    plot_results(unscaled_df, original_df, model_name)\n","287b9284":"h = 28\nn = 1885\ndef rmsse(ground_truth, forecast, train_series, axis=1):\n    # assuming input are numpy array or matrices\n    assert axis == 0 or axis == 1\n    assert type(ground_truth) == np.ndarray and type(forecast) == np.ndarray and type(train_series) == np.ndarray\n    \n    if axis == 1:\n        # using axis == 1 we must guarantee these are matrices and not arrays\n        assert ground_truth.shape[1] > 1 and forecast.shape[1] > 1 and train_series.shape[1] > 1\n    \n    numerator = ((ground_truth - forecast)**2).sum(axis=axis)\n    if axis == 1:\n        denominator = 1\/(n-1) * ((train_series[:, 1:] - train_series[:, :-1]) ** 2).sum(axis=axis)\n    else:\n        denominator = 1\/(n-1) * ((train_series[1:] - train_series[:-1]) ** 2).sum(axis=axis)\n    return (1\/h * numerator\/denominator) ** 0.5","62181fb4":"\n#forecast = df_level_1_diff.iloc[-28:, 1:]\n#rmsse_level_1 = rmsse(np.array(ground_truth), np.array(forecast), np.array(train_series), 0)","4b3fc9bd":"# Separate data into train and test sets\ntrain, test = tts(reg_model_df)\ntrain_series = df_level_1[:-28]\nground_truth = df_level_1[-28:]","57f9b6fd":"#print(regmodel_df.shape) (1905, 8), 7 lags and 1 diff remove 8 rows from 1903  \n#print(train.shape) (1877, 8)\n#print(test.shape)(28, 8)\nregressive_model(train, test, LinearRegression(), 'LinearRegression', df_level_1)\n\n","bb47badf":"regressive_model(train, test, RandomForestRegressor(n_estimators=100, max_depth=20), \n          'RandomForest', df_level_1)","a302324f":"regressive_model(train, test, XGBRegressor( n_estimators=100, \n                                    learning_rate=0.2, \n                                    objective='reg:squarederror'), 'XGBoost', df_level_1)","55078187":"def lstm_model(train_data, test_data, epochs, original_df):\n    \n    X_train, y_train, X_test, y_test, scaler_object = scale_data(train_data, test_data)\n    \n    X_train = X_train.reshape(X_train.shape[0], 1, X_train.shape[1])\n    X_test = X_test.reshape(X_test.shape[0], 1, X_test.shape[1])\n   \n    model = Sequential()\n    model.add(LSTM(4, batch_input_shape=(1, X_train.shape[1], X_train.shape[2]), \n                   stateful=True))\n    model.add(Dense(1))\n    model.add(Dense(1))\n    model.compile(loss='mean_squared_error', optimizer='adam')\n    model.fit(X_train, y_train, epochs=epochs, batch_size=1, verbose=1, \n              shuffle=False)\n    predictions = model.predict(X_test,batch_size=1)\n    \n    unscaled = undo_scaling(predictions, X_test, scaler_object, lstm=True)\n    unscaled_df = predict_df(unscaled, original_df)\n    \n    get_scores(unscaled_df, original_df, 'LSTM'+str(epochs))\n    \n    plot_results(unscaled_df, original_df, 'LSTM'+str(epochs))","19abac9c":"lstm_model(train, test, 2, df_level_1)","3e1383ef":"lstm_model(train, test, 50, df_level_1)","ecf3c7ce":"def sarimax_model(data, original_df, count):\n    # Model  \n    if count==1:\n        sar = sm.tsa.statespace.SARIMAX(data.sales_diff, order=(1, 0, 0), seasonal_order=(1, 1, 1, 7), trend='c').fit()\n    elif count==2:\n        sar = sm.tsa.statespace.SARIMAX(data.sales_diff, order=(7, 0, 0), seasonal_order=(0, 1, 0, 7), trend='c').fit()\n    elif count==3:\n        sar = sm.tsa.statespace.SARIMAX(data.sales_diff, order=(1, 0, 1), seasonal_order=(2, 1, 2, 7), trend='c').fit()\n    elif count==4:\n        sar = sm.tsa.statespace.SARIMAX(data.sales_diff, order=(1, 0, 1), seasonal_order=(3, 1, 3, 7), trend='c').fit()    \n    elif count==5:\n        sar = sm.tsa.statespace.SARIMAX(data.sales_diff, order=(1, 0, 1), seasonal_order=(4, 2, 4, 7), trend='c').fit()\n    elif count==6:\n        sar = sm.tsa.statespace.SARIMAX(data.sales_diff, order=(1, 0, 1), seasonal_order=(5, 2, 5, 7), trend='c').fit()\n    elif count==7:\n        sar = sm.tsa.statespace.SARIMAX(data.sales_diff, order=(2, 0, 2), seasonal_order=(4, 2, 4, 7), trend='c').fit()\n        \n    # Generate predictions    \n    start, end, dynamic = 1884, 1912, 7    \n    data['pred_value'] = sar.predict(start=start, end=end)     \n    # Call helper functions to undo scaling & create prediction df   \n    \n    \n    unscaled_df = predict_df(data.iloc[-28:, 1:], original_df, is_arima=True)\n    #print(unscaled_df)\n    \n    # Call helper functions to print scores and plot results   \n    get_scores(unscaled_df, original_df, 'SARIMA'+str(count)) \n    plot_results(unscaled_df, original_df, 'SARIMA'+str(count))","81fd5b8a":"#order=(1, 0, 0), seasonal_order=(1, 1, 1, 7),\nsarimax_model(df_level_1_diff,  df_level_1, 1)  ","241095a4":"#order=(7, 0, 0), seasonal_order=(0, 1, 0, 7),\nsarimax_model(df_level_1_diff,  df_level_1, 2)  ","f062b56c":"#order=(1, 0, 1), seasonal_order=(2, 1, 2, 7),\nsarimax_model(df_level_1_diff,  df_level_1, 3) ","b7df6b06":"sarimax_model(df_level_1_diff,  df_level_1, 4) ","93b4cc6b":"sarimax_model(df_level_1_diff,  df_level_1, 5) ","0f349c5b":"sarimax_model(df_level_1_diff,  df_level_1, 6) ","6cb19fc8":"sarimax_model(df_level_1_diff,  df_level_1, 7) ","19afd672":"pickle.dump(model_scores, open( \"model_scores.p\", \"wb\" ) )","36a40661":"def create_results_df():\n    results_dict = pickle.load(open(\"\/kaggle\/working\/model_scores.p\", \"rb\"))\n    #print(results_dict)\n    #results_dict.update(pickle.load(open(\"\/kaggle\/working\/arima_model_scores.p\", \"rb\")))\n    \n    restults_df = pd.DataFrame.from_dict(results_dict, orient='index', \n                                        columns=['RMSE', 'MAE','R2', 'RMSSE'])\n    \n    restults_df = restults_df.sort_values(by='RMSE', ascending=False).reset_index()\n    \n    return restults_df","eb7ddbe5":"results = create_results_df()\nresults","14a513f0":"The yearly pattern is very obvious. and also we can see a upwards trend. Which means this data is not stationary.\n\nTo get a stationary data, there's many techiniques. We use first differencing here.","772a539e":" p-value is much smaller than 1%, we can reject null hypothesis. The data after first difference is stationary.\n \n From the autocorrelation plot we can tell whether or not we need to add MA terms. \n \n From the partial autocorrelation plot we know we need to add AR terms.","3012ba47":"We are generating data for regressive models. Lag 7 periods as features, later will add events. ","aa2fe024":"ToDo:\n1. grid search SARIMA for p, d, q\n2. add external varialbes to SARIMA\n\next_var_list=['wday', 'month', 'year','event_name_1', 'event_type_1', 'event_name_2', 'event_type_2','snap_CA', 'snap_TX', 'snap_WI']","5210f677":"1. Estimate level1 total unit sales\n","51c30486":"Here we can see the acf and pacf both has a recurring pattern every 7 periods. Indicating a weekly pattern exists. \n\nWe should start to consider SARIMA to take seasonality into account","4f9c5b54":"\nThere are two ways you can check the stationarity of a time series. The first is by looking at the data. By visualizing the data it should be easy to identify a changing mean or variation in the data. \n\nFor a more accurate assessment there is the Dickey-Fuller test."}}