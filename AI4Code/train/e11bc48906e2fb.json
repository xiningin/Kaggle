{"cell_type":{"270a5cb7":"code","130001ab":"code","6c33ff59":"code","9041dd35":"code","3365dacb":"code","c8a22c55":"code","9f322164":"code","7eb40e22":"code","2917acce":"code","7acff9ee":"code","6a4a6377":"code","320f4520":"code","cb74f3a9":"code","59687075":"code","bef6f29b":"code","6ab41ee7":"code","a8b426ec":"code","e1b8e2c0":"code","edeea483":"code","df69b4ef":"code","e47fec24":"code","e3ded19d":"code","f3fbd3b5":"code","ce0f25d2":"code","01cdfff1":"code","d2d33c45":"code","fe082b30":"code","3d745a4f":"code","99575f24":"code","35def482":"code","1dff55ae":"code","982480d9":"code","dc85db68":"code","26053c5d":"code","50f33210":"code","92781f18":"code","03e3a55f":"code","e3564bb6":"code","69982ecb":"code","1f2ed171":"markdown","d87db0bf":"markdown","b658aa3d":"markdown","4d38d57f":"markdown","d2f3b935":"markdown","60723da8":"markdown","7b5ac086":"markdown","5b8c9d39":"markdown","f261d7a8":"markdown","2d6f5434":"markdown","bed3b3f1":"markdown","25ec3935":"markdown","a4473ea2":"markdown","4c78dcb8":"markdown","3fbc42a0":"markdown","5b245e87":"markdown","8f0d8200":"markdown","b2adae49":"markdown","dd0921b5":"markdown","c6726d5d":"markdown","8f63fc46":"markdown","e0458f3f":"markdown","20cbc8fa":"markdown","6703e94a":"markdown","8232cc36":"markdown"},"source":{"270a5cb7":"# libraries\nimport numpy as np\nimport pandas as pd\nimport os\nimport cv2\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score\nimport torch\nfrom torch.utils.data import TensorDataset, DataLoader,Dataset\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision\nimport torchvision.transforms as transforms\nimport torch.optim as optim\nfrom torch.optim import lr_scheduler\nimport time \nfrom PIL import Image\ntrain_on_gpu = True\nfrom torch.utils.data.sampler import SubsetRandomSampler\nfrom torch.optim.lr_scheduler import StepLR, ReduceLROnPlateau, CosineAnnealingLR\nfrom sklearn.metrics import accuracy_score\nimport cv2","130001ab":"!pip install albumentations > \/dev\/null 2>&1\n!pip install pretrainedmodels > \/dev\/null 2>&1\n!pip install kekas > \/dev\/null 2>&1\n!pip install adabound > \/dev\/null 2>&1","6c33ff59":"# more imports\nimport albumentations\nfrom albumentations import torch as AT\nimport pretrainedmodels\nimport adabound\n\nfrom kekas import Keker, DataOwner, DataKek\nfrom kekas.transformations import Transformer, to_torch, normalize\nfrom kekas.metrics import accuracy\nfrom kekas.modules import Flatten, AdaptiveConcatPool2d\nfrom kekas.callbacks import Callback, Callbacks, DebuggerCallback\nfrom kekas.utils import DotDict","9041dd35":"labels = pd.read_csv('..\/input\/train_labels.csv')\nfig = plt.figure(figsize=(25, 4))\n# display 20 images\ntrain_imgs = os.listdir(\"..\/input\/train\")\nfor idx, img in enumerate(np.random.choice(train_imgs, 20)):\n    ax = fig.add_subplot(2, 20\/\/2, idx+1, xticks=[], yticks=[])\n    im = Image.open(\"..\/input\/train\/\" + img)\n    plt.imshow(im)\n    lab = labels.loc[labels['id'] == img.split('.')[0], 'label'].values[0]\n    ax.set_title(f'Label: {lab}')","3365dacb":"test_img = os.listdir('..\/input\/test\/')\ntest_df = pd.DataFrame(test_img, columns=['id'])\ntest_df['label'] = -1\ntest_df['data_type'] = 'test'\ntest_df['id'] = test_df['id'].apply(lambda x: x.split('.')[0])\n\nlabels['label'] = labels['label'].astype(int)\nlabels['data_type'] = 'train'\n\nlabels.head()","c8a22c55":"labels.shape, test_df.shape","9f322164":"train_short, _ = train_test_split(labels, stratify=labels.label, test_size=0.95)","7eb40e22":"train_short.shape","2917acce":"# splitting data into train and validation\ntrain, valid = train_test_split(train_short, stratify=train_short.label, test_size=0.1)","7acff9ee":"def reader_fn(i, row):\n    image = cv2.imread(f\"..\/input\/{row['data_type']}\/{row['id']}.tif\")[:,:,::-1] # BGR -> RGB\n    label = torch.Tensor([row[\"label\"]])\n    return {\"image\": image, \"label\": label}","6a4a6377":"def augs(p=0.5):\n    return albumentations.Compose([\n        albumentations.HorizontalFlip(),\n        albumentations.RandomRotate90(),\n        albumentations.Transpose(),\n        albumentations.Flip(),\n    ], p=p)","320f4520":"def get_transforms(dataset_key, size, p):\n\n    PRE_TFMS = Transformer(dataset_key, lambda x: cv2.resize(x, (size, size)))\n\n    AUGS = Transformer(dataset_key, lambda x: augs()(image=x)[\"image\"])\n\n    NRM_TFMS = transforms.Compose([\n        Transformer(dataset_key, to_torch()),\n        Transformer(dataset_key, normalize())\n    ])\n    \n    train_tfms = transforms.Compose([PRE_TFMS, AUGS, NRM_TFMS])\n    val_tfms = transforms.Compose([PRE_TFMS, NRM_TFMS])\n    \n    return train_tfms, val_tfms","cb74f3a9":"# getting transformations\ntrain_tfms, val_tfms = get_transforms(\"image\", 224, 0.5)","59687075":"train_dk = DataKek(df=train, reader_fn=reader_fn, transforms=train_tfms)\nval_dk = DataKek(df=valid, reader_fn=reader_fn, transforms=val_tfms)\n\nbatch_size = 32\nworkers = 0\n\ntrain_dl = DataLoader(train_dk, batch_size=batch_size, num_workers=workers, shuffle=True, drop_last=True)\nval_dl = DataLoader(val_dk, batch_size=batch_size, num_workers=workers, shuffle=False)","bef6f29b":"test_dk = DataKek(df=test_df, reader_fn=reader_fn, transforms=val_tfms)\ntest_dl = DataLoader(test_dk, batch_size=batch_size, num_workers=workers, shuffle=False)","6ab41ee7":"# create a simple neural network using pretrainedmodels library\n# https:\/\/github.com\/Cadene\/pretrained-models.pytorch\n\nclass Net(nn.Module):\n    def __init__(\n            self,\n            num_classes: int,\n            p: float = 0.2,\n            pooling_size: int = 2,\n            last_conv_size: int = 81536,\n            arch: str = \"densenet169\",\n            pretrained: str = \"imagenet\") -> None:\n        \"\"\"A simple model to finetune.\n        \n        Args:\n            num_classes: the number of target classes, the size of the last layer's output\n            p: dropout probability\n            pooling_size: the size of the result feature map after adaptive pooling layer\n            last_conv_size: size of the flatten last backbone conv layer\n            arch: the name of the architecture form pretrainedmodels\n            pretrained: the mode for pretrained model from pretrainedmodels\n        \"\"\"\n        super().__init__()\n        net = pretrainedmodels.__dict__[arch](pretrained=pretrained)\n        modules = list(net.children())[:-1]  # delete last layer\n        # add custom head\n        modules += [nn.Sequential(\n            # AdaptiveConcatPool2d is a concat of AdaptiveMaxPooling and AdaptiveAveragePooling \n            AdaptiveConcatPool2d(size=pooling_size),\n            Flatten(),\n            nn.BatchNorm1d(13312),\n            nn.Dropout(p),\n            nn.Linear(13312, num_classes)\n        )]\n        self.net = nn.Sequential(*modules)\n\n    def forward(self, x):\n        logits = self.net(x)\n        return logits","a8b426ec":"dataowner = DataOwner(train_dl, val_dl, None)","e1b8e2c0":"model = Net(num_classes=1)\n\ncriterion = nn.BCEWithLogitsLoss()","edeea483":"def step_fn(model: torch.nn.Module,\n            batch: torch.Tensor) -> torch.Tensor:\n    \"\"\"Determine what your model will do with your data.\n\n    Args:\n        model: the pytorch module to pass input in\n        batch: the batch of data from the DataLoader\n\n    Returns:\n        The models forward pass results\n    \"\"\"\n    \n    inp = batch[\"image\"]\n    return model(inp)","df69b4ef":"def bce_accuracy(target: torch.Tensor,\n                 preds: torch.Tensor) -> float:\n    thresh = 0.5\n    target = target.cpu().detach().numpy()\n    preds = (torch.sigmoid(preds).cpu().detach().numpy() > thresh).astype(int)\n    return accuracy_score(target, preds)","e47fec24":"keker = Keker(model=model,\n              dataowner=dataowner,\n              criterion=criterion,\n              step_fn=step_fn,\n              target_key=\"label\",\n              metrics={\"acc\": bce_accuracy},\n              opt=torch.optim.SGD,\n              opt_params={\"momentum\": 0.99})","e3ded19d":"keker.unfreeze(model_attr=\"net\")\n\n# If you want to freeze till some layer:\nlayer_num = -1\nkeker.freeze_to(layer_num, model_attr=\"net\")","f3fbd3b5":"keker.kek_lr(final_lr=0.001, logdir=\"logs\")","ce0f25d2":"# !wget https:\/\/bin.equinox.io\/c\/4VmDzA7iaHb\/ngrok-stable-linux-amd64.zip\n# !unzip ngrok-stable-linux-amd64.zip\n# LOG_DIR = 'logs' # Here you have to put your log directory\n# get_ipython().system_raw(\n#     'tensorboard --logdir {} --host 0.0.0.0 --port 6006 &'\n#     .format(LOG_DIR)\n# )\n# get_ipython().system_raw('.\/ngrok http 6006 &')\n# ! curl -s http:\/\/localhost:4040\/api\/tunnels | python3 -c \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\"\n#!tensorboard --logdir logs --host 0.0.0.0 --port 6007","01cdfff1":"keker.plot_kek_lr('logs')","d2d33c45":"# simply training the model with pre-defined parameters.\nkeker.kek(lr=2e-5, epochs=1, logdir='train_logs0')","fe082b30":"# we plot all three graphs by default, but we can also choose which of them will be shown.\nkeker.plot_kek('train_logs0', metrics=['loss', 'acc'])","3d745a4f":"keker.kek(lr=2e-5, epochs=3,\n         sched=torch.optim.lr_scheduler.StepLR,\n              sched_params={\"step_size\":1, \"gamma\": 0.9},# optimizer class. if note specifiyng, \n                                                  # an SGD is using by default\n              opt_params={\"weight_decay\": 1e-5},\n              early_stop_params={\n              \"patience\": 2,   \n              \"metric\": \"acc\", \n              \"mode\": \"min\",   \n              \"min_delta\": 0\n          }, logdir='train_logs')","99575f24":"keker.plot_kek('train_logs')","35def482":"keker.kek_one_cycle(max_lr=1e-4,                  # the maximum learning rate\n                    cycle_len=3,                  # number of epochs, actually, but not exactly\n                    momentum_range=(0.95, 0.85),  # range of momentum changes\n                    div_factor=25,                # max_lr \/ min_lr\n                    increase_fraction=0.3,        # the part of cycle when learning rate increases\n                    logdir='train_logs1')","1dff55ae":"keker.plot_kek('train_logs1')","982480d9":"train, valid = train_test_split(labels, stratify=labels.label, test_size=0.1)\n\ntrain_dk = DataKek(df=train, reader_fn=reader_fn, transforms=train_tfms)\nval_dk = DataKek(df=valid, reader_fn=reader_fn, transforms=val_tfms)\n\ntrain_dl = DataLoader(train_dk, batch_size=batch_size, num_workers=workers, shuffle=True, drop_last=True)\nval_dl = DataLoader(val_dk, batch_size=batch_size, num_workers=workers, shuffle=False)\n\ntest_dk = DataKek(df=test_df, reader_fn=reader_fn, transforms=val_tfms)\ntest_dl = DataLoader(test_dk, batch_size=batch_size, num_workers=workers, shuffle=False)\n\ndataowner = DataOwner(train_dl, val_dl, None)\n\nkeker = Keker(model=model,\n              dataowner=dataowner,\n              criterion=criterion,\n              step_fn=step_fn,\n              target_key=\"label\",\n              metrics={\"acc\": bce_accuracy},\n              opt=adabound.AdaBound,\n              opt_params={'final_lr': 0.001})\n\nkeker.unfreeze(model_attr=\"net\")\n\nlayer_num = -3\nkeker.freeze_to(layer_num, model_attr=\"net\")","dc85db68":"keker.kek_one_cycle(max_lr=1e-4,\n                    cycle_len=6,\n                    momentum_range=(0.95, 0.85),\n                    div_factor=25,\n                    increase_fraction=0.3)","26053c5d":"# keker.kek(lr=2e-5, epochs=3,\n#          sched=ReduceLROnPlateau,\n#               sched_params={\"patience\":2, \"factor\": 0.1},# optimizer class. if note specifiyng, \n#                                                   # an SGD is using by default\n#               early_stop_params={\n#               \"patience\": 2,   \n#               \"metric\": \"acc\", \n#               \"mode\": \"min\",   \n#               \"min_delta\": 0\n#           }, logdir='train_logs_fin')","50f33210":"keker.plot_kek_lr('train_logs_fin')","92781f18":"# simple prediction\npreds = keker.predict_loader(loader=test_dl)","03e3a55f":"# TTA\nflip_ = albumentations.Flip(always_apply=True)\ntranspose_ = albumentations.Transpose(always_apply=True)\n\ndef insert_aug(aug, dataset_key=\"image\", size=224):    \n    PRE_TFMS = Transformer(dataset_key, lambda x: cv2.resize(x, (size, size)))\n    \n    AUGS = Transformer(dataset_key, lambda x: aug(image=x)[\"image\"])\n    \n    NRM_TFMS = transforms.Compose([\n        Transformer(dataset_key, to_torch()),\n        Transformer(dataset_key, normalize())\n    ])\n    \n    tfm = transforms.Compose([PRE_TFMS, AUGS, NRM_TFMS])\n    return tfm\n\nflip = insert_aug(flip_)\ntranspose = insert_aug(transpose_)\n\ntta_tfms = {\"flip\": flip, \"transpose\": transpose}\n\n# third, run TTA\nkeker.TTA(loader=test_dl,                # loader to predict on \n          tfms=tta_tfms,                # list or dict of always applying transforms\n          savedir=\"tta_preds1\",  # savedir\n          prefix=\"preds\")               # (optional) name prefix. default is 'preds'\n\n# it will saves predicts for each augmentation to savedir with name\n#  - {prefix}_{name_from_dict}.npy if tfms is a dict\n#  - {prefix}_{index}.npy          if tfms is a list","e3564bb6":"prediction = np.zeros((test_df.shape[0], 1))\nfor i in os.listdir('tta_preds1'):\n    pr = np.load('tta_preds1\/' + i)\n    prediction += pr\nprediction = prediction \/ len(os.listdir('tta_preds1'))","69982ecb":"test_preds = pd.DataFrame({'imgs': test_df.id.values, 'preds': prediction.reshape(-1,)})\ntest_preds.columns = ['id', 'label']\ntest_preds.to_csv('sub.csv', index=False)","1f2ed171":"## Keker\n\nNow we can define the Keker - the core Kekas class for training the model.\n\nHere we define everything which is necessary for training:\n* the model which was defined earlier;\n* dataowner containing the data for training and validation;\n* criterion;\n* step function;\n* the key of labels, which was defined in the reader function;\n* the dictionary with metrics (there can be several of them);\n* The optimizer and its parameters;\n\nThere are more advanced parameters, you can read more about them in the docstring: https:\/\/github.com\/belskikh\/kekas\/blob\/9f9fbb00ded7e12e79ba3d2edaa3a8f82b368722\/kekas\/keker.py","d87db0bf":"### General information\n\nThere are many frameworks for Deep Learning. Recently I started using Pytorch and liked it, but sometimes it requires too much coding, so I started looking for wrappers over Pytorch.\n\nRecently a new library emerged: Kekas https:\/\/github.com\/belskikh\/kekas\n\nIt provides a simple API for training neural nets for images and good visualizations of training process. There are also nice tricks from fast.ai - learning rate finder, one cycle policy and others.\n\nIn this kernel I'll show what this library can do. At first I'll take a small subset of data and demonstrate the general functionality, after this we'll try training model on the full data.  Sadly, kaggle kernels have some problems currently, so I won't be able to show everything.\n","b658aa3d":"## Training model on all data\n\nNow, let's train model on all data.\n\nI use AdaBound: https:\/\/github.com\/Luolc\/AdaBound","4d38d57f":"### Training the model","d2f3b935":"## Various ways to train a model","60723da8":"Next we define model and loss. As I choose `BCEWithLogitsLoss`, we can set the number of classes for output to 1.","7b5ac086":"For now let's take a subset of data for faster training.","5b8c9d39":"### Learning Rate Find\nIn fast.ai Jeremy offered and idea of learning rate finder to find an optimal learning rate. Let's do it! We can simply define the final LR and the directory where logs are written.","f261d7a8":"### Data transformation\n\nNext step is defining data transformations and augmentations. This differs from standard PyTorch way. We define resizing, augmentations and normalizing separately, this allows to easily create separate transformers for train and valid\/test data.\n\nAt first we define augmentations. We create a function with a list of augmentations (I prefer albumentation library: https:\/\/github.com\/albu\/albumentations)","2d6f5434":"And now we define what will the model do with the data. For example we could slice the output and take only a part of it. For now we will simply return the output of the model.","bed3b3f1":"## Predicting and TTA\n\nSimply predicting on test data is okay, but it is better to use TTA - test time augmentation. Let's see how it can be done with Kekas.\n\n* define augmentations;\n* define augmentation function;\n* create objects with these augmentations;\n* put these objects into a single dictionary;","25ec3935":"Kekas accepts pandas DataFrame as an input and iterates over it to get image names and labels","a4473ea2":"Also we can define new parameters or even override old ones.","4c78dcb8":"Some of good libraries for DL aren't available in Docker with GPU by default, so it is necessary to install them. (don't forget to turn on internet connection in kernels).","3fbc42a0":"There are much more features, you can read the tutorial: https:\/\/github.com\/belskikh\/kekas\/blob\/master\/Tutorial.ipynb or the code itself","5b245e87":"### Reader function\n\nAt first it is necessary to create a reader function, which will open images. It accepts i and row as input (like from pandas iterrows). The function should return a dictionary with image and label.\n\n`[:,:,::-1]` - is a neat trick which converts BGR images to RGB, it works faster that converting to RGB by usual means.","8f0d8200":"Now we create a transforming function. It heavily uses Transformer from kekas.\n* The first step is defining resizing. You can change arguments of function if you want images to have different height and width, otherwis you can leave it as it is.\n* Next step is defining atgmentations. Here we provide the key of image which is defined in `reader_fn`;\n* The third step is defining final transformation to tensor and normalizing;\n* After this we can compose separate transformations for train and valid\/test data;","b2adae49":"Now we can create a DataKek, which is similar to creating dataset in Pytorch. We define the data, reader function and transformation.Then we can define standard PyTorch DataLoader.","dd0921b5":"One of the ways to plot the training curves is to use tensorboard. Sadly it isn't supported well by kaggle. If you are in edit mode, you can try the next cell. Or you can simply use the keker method to plot the logs from the defined directory.","c6726d5d":"### Freezing and unfreezing\nFreezing and unfreezing layers is quite important if you want to achieve high score. It can be done easily with Kekas. Af first you unfreeze the model and then freeze up to a necessary layer.","8f63fc46":"One more cool feature is a possibility to use one cycle!","e0458f3f":"### Defining custom metrics\n\nCurrently Kekas has only simple accuracy (see here: https:\/\/github.com\/belskikh\/kekas\/blob\/9f9fbb00ded7e12e79ba3d2edaa3a8f82b368722\/kekas\/metrics.py), but we can define any metric we like. It should accept tensors of target and prediction as an imput and return the metric value as an output.\n\nLet's define an accuracy for out net with BCE.","20cbc8fa":"## Preparing data","6703e94a":"The data for training needs to be transformed one more time - we define DataOwner, which contains all the data. For now let's define it for train and valid.","8232cc36":"## Building a neural net\n\nHere we define the architecture of the neural net.\n\n* Pre-trained backbone is taken from pretrainedmodels: https:\/\/github.com\/Cadene\/pretrained-models.pytorch Here I take `densenet169`\n* We also define changes to the architecture. For example, we take off the last layer and add a custom head with `nn.Sequential`. `AdaptiveConcatPool2d` is a layer in kekas, which concats `AdaptiveMaxPooling` and `AdaptiveAveragePooling` "}}