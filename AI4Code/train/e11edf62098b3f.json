{"cell_type":{"151412a1":"code","a0545445":"code","9b098fa6":"code","9cbfdafc":"code","7439dd5b":"code","fe7b3c81":"code","e6ceca54":"code","e40db3df":"code","0a72feb4":"code","028f1b9c":"code","f32cc629":"code","4fe38d7d":"code","88562354":"code","4029e276":"code","db00c828":"code","69eb2d5a":"code","82bc6d5f":"code","02412489":"code","bfb9f159":"code","0955ea39":"code","2adae3ed":"code","7b36dfa7":"code","81c238f1":"code","5f9998a0":"code","cdcca012":"code","30ee8003":"code","12cce016":"code","e3a90341":"code","714d423a":"code","08f0ea39":"code","b5752545":"code","4ccc2ded":"code","59feb153":"code","4e4b4159":"code","13089751":"code","0a15e5ff":"code","f7e66f5f":"code","c774f5c1":"code","8975efd1":"code","f51ded2c":"code","4a7203a9":"code","1db090ae":"code","e080c0cd":"code","a8b6be7a":"code","4a80ae1e":"code","b8190fc3":"code","3b68bf93":"code","3943b87b":"code","ee8687d1":"code","21cfe586":"code","25ff6f38":"code","da6a184f":"code","8420d39d":"code","aa86b500":"code","560e43d1":"code","8ee37923":"code","8d0ad1df":"code","9c28fe59":"code","1764b640":"code","b0dad47d":"code","e4a2f815":"code","ab3ceca3":"code","2937f51e":"code","5d190132":"code","60906246":"code","1442f66b":"code","8ea31daf":"code","c96bf69f":"code","d20d24d0":"code","0d8df00d":"code","759cbb00":"code","d57c632e":"code","351e552f":"code","22749874":"code","d71b15f9":"code","97685abc":"code","5697795f":"code","659ced6c":"code","a35e851a":"code","77b4cba9":"code","638cae61":"code","a9fe7f49":"code","0b359cbd":"code","3c66ce0f":"code","baf10fbe":"code","da85d408":"code","81e0cb5a":"code","640fbc86":"code","5e7cc237":"code","66abbf6f":"code","6185565d":"code","dca1a1cb":"code","6f217696":"markdown","6174e00a":"markdown","bb8032c0":"markdown","16b08e35":"markdown","55ffd57b":"markdown","60a0e1b9":"markdown","776e6d19":"markdown","10c26b1e":"markdown","d42f8f70":"markdown","e177bd03":"markdown","6ff2f2d7":"markdown","a2b7fe35":"markdown","5c47209f":"markdown","664537da":"markdown","739f4f30":"markdown","32f922df":"markdown","b9e51116":"markdown","e5ba8b75":"markdown","9010cb17":"markdown","d4bfd4d7":"markdown","b8be000b":"markdown","05923a74":"markdown","b95aa3d1":"markdown","f582d4fa":"markdown","c2178007":"markdown","7b14421f":"markdown","1018faa4":"markdown","ec4cb799":"markdown","6c11811f":"markdown","fd36a145":"markdown","c9d5fff1":"markdown","f0f15f50":"markdown","35ac83d9":"markdown","0f596737":"markdown","56b135f8":"markdown","d7b0bed8":"markdown","4c517f8b":"markdown","faa3c43d":"markdown","f3daebbe":"markdown","98e499e2":"markdown","06c2e511":"markdown","53691288":"markdown","02ed05db":"markdown","e6bff8b9":"markdown","40f2e90e":"markdown","ae338dea":"markdown","153e4933":"markdown","bafc5bf1":"markdown","639ea0b0":"markdown","5146187e":"markdown","49b9cdd6":"markdown","bc3f6925":"markdown","605c81b4":"markdown","f062a86a":"markdown","78c72a88":"markdown","75887511":"markdown","1d670cf9":"markdown","37ac6a2b":"markdown","d892ab34":"markdown","6627d085":"markdown","45efd39f":"markdown","3a40055b":"markdown","6d1ce35d":"markdown"},"source":{"151412a1":"# install pyspark\n!pip install pyspark","a0545445":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport sys\nimport itertools\n\nfrom fbprophet import Prophet\nfrom fbprophet.plot import add_changepoints_to_plot\nfrom fbprophet.diagnostics import cross_validation, performance_metrics\nfrom fbprophet.plot import add_changepoints_to_plot, plot_cross_validation_metric\n\nfrom pyspark.ml.classification import RandomForestClassifier, LogisticRegression\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\nfrom sklearn.metrics import classification_report, confusion_matrix\n\n# Import Sparksession\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import *\nfrom pyspark.sql import functions as F\nfrom pyspark.sql.types import IntegerType\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nspark=SparkSession.builder.appName(\"Data_Wrangling\").getOrCreate()","9b098fa6":"# Print PySpark and Python versions\nprint('Python version: '+sys.version)\nprint('Spark version: '+spark.version)","9cbfdafc":"# Read data\nfile_location = \"..\/input\/pakistans-largest-ecommerce-dataset\/Pakistan Largest Ecommerce Dataset.csv\"\nfile_type = \"csv\"\ninfer_schema = \"false\"\nfirst_row_is_header = \"true\"\ndelimiter = \",\"\n\ndf = spark.read.format(file_type)\\\n.option(\"inferSchema\", infer_schema)\\\n.option(\"header\", first_row_is_header)\\\n.option(\"sep\", delimiter)\\\n.load(file_location)","7439dd5b":"df.show()","fe7b3c81":"# Print Metadata\ndf.printSchema()","e6ceca54":"print('The shape of ecommerce dataset is', (df.count(), len(df.columns)))","e40db3df":"df.describe().toPandas().T","0a72feb4":"plt.figure(figsize=(12, 6))\nsns.distplot([row['price'] for row in df.select('price').collect()], color='g');","028f1b9c":"# Identifying missing values\ndf.select([count(when((col(c)=='') | col(c).isNull() |isnan(c), c)).alias(c) for c in df.columns]).show()","f32cc629":"#  Drop Columns\ndrop_columns_list=[\"_c21\",\"_c22\",\"_c23\",\"_c24\",\"_c25\"]\ndf1 = df.drop(*drop_columns_list)","4fe38d7d":"df1.show()","88562354":"df1.columns","4029e276":"df1.orderBy(asc(\"item_id\")).show(5)","db00c828":"df1 = df1.na.drop(how = \"all\")","69eb2d5a":"# Identifying missing values\ndf1.select([count(when((col(c)=='') | col(c).isNull() |isnan(c), c)).alias(c) for c in df1.columns]).show()","82bc6d5f":"## computing mode value of status column for fill missing value\nmode_status = df1.groupby(\"status\").count().orderBy(\"count\", ascending=False).first()[0]\nmode_status","02412489":"## mode value of status column is complete \ndf1 = df1.fillna(mode_status, subset=['status'])","bfb9f159":"## computing mode value of category_name_1 column for fill missing value\nmode_category_name_1 = df1.groupby(\"category_name_1\").count().orderBy(\"count\", ascending=False).first()[0]\nmode_category_name_1","0955ea39":"df1 = df1.fillna(mode_category_name_1, subset=['category_name_1'])","2adae3ed":"# Identifying missing values\ndf1.select([count(when((col(c)=='') | col(c).isNull() |isnan(c), c)).alias(c) for c in df1.columns]).show()","7b36dfa7":"df1 = df1.na.drop(subset=['Working Date', 'sku', 'Customer ID'])","81c238f1":"# Identifying missing values\ndf1.select([count(when((col(c)=='') | col(c).isNull() |isnan(c), c)).alias(c) for c in df1.columns]).show()","5f9998a0":"df1 = df1.withColumn('created_at', F.to_date(F.unix_timestamp('created_at', 'M\/d\/y').cast('timestamp')))\ndf1 = df1.withColumn('Working Date', F.to_date(F.unix_timestamp('Working Date', 'M\/d\/y').cast('timestamp')))","cdcca012":"df1.printSchema()","30ee8003":"df1.show()","12cce016":"df1 = df1.withColumn(\"qty_ordered\", df1[\"qty_ordered\"].cast(IntegerType()))\ndf1 = df1.withColumn(\"price\", df1[\"price\"].cast(IntegerType()))\ndf1 = df1.withColumn(\"grand_total\", df1[\"grand_total\"].cast(IntegerType()))\ndf1 = df1.withColumn(\"discount_amount\", df1[\"discount_amount\"].cast(IntegerType()))\ndf1 = df1.withColumn(\"Month\", df1[\"Month\"].cast(IntegerType()))\ndf1 = df1.withColumn(\"Year\", df1[\"Year\"].cast(IntegerType()))","e3a90341":"df1 = df1.withColumnRenamed(\" MV \", \"MV\")\ndf1 = df1.withColumnRenamed(\"created_at\", \"order_date\")","714d423a":"df1 = df1.drop_duplicates()","08f0ea39":"print('The shape of ecommerce dataset is', (df1.count(), len(df1.columns)))","b5752545":"df1.printSchema()","4ccc2ded":"df1.describe().toPandas().T","59feb153":"from pyspark.ml.stat import Correlation\ndata = df1.select('price', 'qty_ordered', 'grand_total', 'discount_amount', 'Year', 'Month')\nvector_col = \"corr_features\"\nassembler = VectorAssembler(inputCols=data.columns, \n                            outputCol=vector_col)\nmyGraph_vector = assembler.transform(data).select(vector_col)\nmatrix = Correlation.corr(myGraph_vector, vector_col).collect()[0][0]","4e4b4159":"matrix = Correlation.corr(myGraph_vector, vector_col).collect()[0][0]\ncorrmatrix = matrix.toArray().tolist()\nprint(corrmatrix)","13089751":"correlation_dataframe = spark.createDataFrame(corrmatrix, data.columns)\ncorrelation_dataframe.show()","0a15e5ff":"plt.figure(figsize = (12,10))\nax = sns.heatmap(correlation_dataframe.toPandas(), annot=True)\nax.set_yticklabels(correlation_dataframe.columns)\nplt.show()","f7e66f5f":"## all categories\ndf1.select(\"category_name_1\").distinct().show()","c774f5c1":"best_category = df1.groupby(\"category_name_1\").count().sort(col(\"count\").desc()).toPandas()\nbest_category","8975efd1":"best_category.set_index('category_name_1', inplace=True)\nax = best_category.plot(kind='pie', y='count', autopct='%1.1f%%', figsize=(8,8), title=\"Best Selling Categories\")\nax.legend(bbox_to_anchor=(1.5, 1.0))\nplt.show()","f51ded2c":"best_category.plot(kind='bar', y='count', figsize=(12,6),\\\n                   title=\"Best Selling Categories\")\nplt.ylabel(\"Total Order\")\nplt.show()","4a7203a9":"status_count = df1.groupby(\"status\").count().sort(col(\"count\").desc()).toPandas()\nstatus_count","1db090ae":"status_count.set_index('status', inplace=True)\nstatus_count.plot(kind='bar', y='count', figsize=(12,6),\\\n                  title=\"Order Status Frequency\", color='red')\nplt.xlabel(\"status category\")\nplt.ylabel(\"total counts\")\nplt.show()","e080c0cd":"df1.select(\"Year\").distinct().sort(col(\"Year\")).show()","a8b6be7a":"n = 1\nplt.figure(figsize=(22 , 10))\nfor year in df1.select(\"Year\").distinct().sort(col(\"Year\")).collect():\n  data = [val['status'] for val in df.filter(F.col(\"Year\") == year['Year'])\\\n          .select('status').collect() if val['status'] != None]\n  plt.subplot(1,3,n)\n  plt.title('Order Status in {0}'.format(year['Year']))\n  plt.xlabel('Order status')\n  plt.ylabel('Total no of orders')\n  plt.xticks(rotation = 90)\n  plt.hist(data)\n  n+=1\n\nplt.show()","4a80ae1e":"df1.groupby(\"payment_method\").count().show(20, False)","b8190fc3":"df1 = df1.withColumn('payment_method', regexp_replace('payment_method', 'cod', 'cash_on_delivery'))","3b68bf93":"payment_method_count = df1.groupby('payment_method').count().sort(col(\"count\").desc()).toPandas()\npayment_method_count","3943b87b":"payment_method_count.set_index('payment_method', inplace=True)\npayment_method_count.plot(kind='bar', figsize=(12,6) , color='green' , title=\"Payment method vs order\")\nplt.xlabel(\"payment category\")\nplt.ylabel(\"total counts\")\nplt.show()","ee8687d1":"order_per_month_year = df1.groupby('M-Y').count().sort(col(\"count\").desc()).toPandas()\norder_per_month_year","21cfe586":"order_per_month_year.set_index('M-Y', inplace=True)\norder_per_month_year.plot(kind='bar', figsize=(12,6) , color='Blue' , title=\"Order count per month of each year\")\nplt.xlabel(\"Month-Year\")\nplt.ylabel(\"total counts\")\nplt.show()","25ff6f38":"oreder_completed = df1.filter(F.col('status').isin(['complete','paid', 'received', 'cash_on_delivery']))\noreder_completed_by_category = oreder_completed.groupby('category_name_1').count()\\\n.sort(col(\"count\").desc()).toPandas()\n\noreder_completed_by_category","da6a184f":"oreder_completed_by_category.set_index('category_name_1', inplace=True)\noreder_completed_by_category.plot(kind='bar', figsize=(12,6), color='purple',\n                                  title=\"Best Cateory w.r.t Order Completion\")\nplt.xlabel(\"Category\")\nplt.ylabel(\"total counts\")\nplt.show()","8420d39d":"oreder_not_completed = df1.filter(~F.col('status').isin(['complete','paid', 'received', 'cash_on_delivery']))\noreder_not_completed_by_category = oreder_not_completed.groupby('category_name_1').count()\\\n.sort(col(\"count\").desc()).toPandas()\n\noreder_not_completed_by_category","aa86b500":"oreder_not_completed_by_category.set_index('category_name_1', inplace=True)\noreder_not_completed_by_category.plot(kind='bar', figsize=(12,6), color='red',\n                                  title=\"Worst Cateory w.r.t not Order Completion\")\nplt.xlabel(\"Category\")\nplt.ylabel(\"total counts\")\nplt.show()","560e43d1":"data = df1.groupby(\"order_date\").count().sort(col(\"order_date\")).toPandas()\ndata.rename(columns={ \"order_date\": 'ds', \"count\":\"y\"}, inplace=True)\ndata.head()","8ee37923":"plot_data = data.copy()\nplot_data.index = plot_data.ds\nplot_data.drop(['ds'], axis=1, inplace=True)\nplot_data.plot(kind='line', figsize=(16, 10), title='Order Count w.r.t Date')","8d0ad1df":"model = Prophet(daily_seasonality=True)\nmodel.fit(data)","9c28fe59":" # predicting out of sample for next year\nfuture = model.make_future_dataframe(periods=365, freq='d', include_history=True)\nfuture.tail(4)","1764b640":"forecast = model.predict(future)\nforecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']].tail()\nfig1 = model.plot(forecast,xlabel=\"Date\",ylabel=\"Number of Orders\")","b0dad47d":"# Execute cross validation\ndf_cv = cross_validation(model, initial='365 days', period='90 days', horizon = '60 days')\npm = performance_metrics(df_cv)\ndisplay(pm.head(),pm.tail())\nfig = plot_cross_validation_metric(df_cv, metric='mape')\nplt.show()","e4a2f815":"df_cv","ab3ceca3":"df.select('category_name_1').distinct().show()","2937f51e":"mbl_cat_data = df1.filter(col(\"category_name_1\") == 'Mobiles & Tablets')","5d190132":"data1 = mbl_cat_data.groupby(\"order_date\").count().sort(col(\"order_date\")).toPandas()\ndata1.rename(columns={ \"order_date\": 'ds', \"count\":\"y\"}, inplace=True)\ndata1.head()","60906246":"model_1 = Prophet(daily_seasonality=True)\nmodel_1.fit(data1)","1442f66b":" # predicting out of sample for next year\nfuture = model_1.make_future_dataframe(periods=60, freq='d', include_history=True)\nfuture.tail(4)","8ea31daf":"forecast = model_1.predict(future)\nforecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']].tail()\nfig1 = model_1.plot(forecast, xlabel=\"Date\",ylabel=\"Number of Orders\")","c96bf69f":"# Execute cross validation\ndf_cv = cross_validation(model_1, initial='500 days', period='60 days', horizon = '60 days')\npm = performance_metrics(df_cv)\ndisplay(pm.head(),pm.tail())\nfig = plot_cross_validation_metric(df_cv, metric='mape')\nplt.show()","d20d24d0":"df_cv","0d8df00d":"h_s_data = df1.filter(col(\"category_name_1\") == 'Health & Sports')","759cbb00":"data2 = h_s_data.groupby(\"order_date\").count().sort(col(\"order_date\")).toPandas()\ndata2.rename(columns={ \"order_date\": 'ds', \"count\":\"y\"}, inplace=True)\ndata2.head()","d57c632e":"model_2 = Prophet()\nmodel_2.fit(data2)","351e552f":" # predicting out of sample for next year\nfuture = model_2.make_future_dataframe(periods=60, freq='d', include_history=True)\nfuture.tail(4)","22749874":"forecast = model_2.predict(future)\nforecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']].tail()\nfig1 = model_2.plot(forecast, xlabel=\"Date\",ylabel=\"Number of Orders\")","d71b15f9":"# Execute cross validation\ndf_cv = cross_validation(model_2, initial='500 days', period='60 days', horizon = '60 days')\npm = performance_metrics(df_cv)\ndisplay(pm.head(),pm.tail())\nfig = plot_cross_validation_metric(df_cv, metric='mape')\nplt.show()","97685abc":"df_cv","5697795f":"# Graphical Confusion Matrix function\ndef plot_confusion_matrix(cm, classes,\n                          normalize=True,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')","659ced6c":"categorical_columns= ['category_name_1', 'payment_method', 'BI Status', 'Customer Since']\n\nstage_strings= [StringIndexer(inputCol= c, outputCol= c+\"_string_encoded\") for c in categorical_columns]\nstage_string = [StringIndexer(inputCol= 'status', outputCol= \"status_string_encoded\")]\nstage_one_hot = [OneHotEncoder(dropLast=True, inputCol= c+\"_string_encoded\", outputCol= c+ \"_one_hot\") for c in categorical_columns]\n\nppl = Pipeline(stages= stage_strings + stage_string + stage_one_hot)\ncategorical_data = df1.select(*categorical_columns)\ndf = ppl.fit(df1).transform(df1)","a35e851a":"df = df.select([c for c in df.columns if c not in ['item_id', 'status',\n                                                   'order_date', 'sku',\n                                                   'price', 'increment_id',\n                                                   'sales_commission_code', \n                                                   'Working Date', 'MV', \n                                                   'Year', 'M-Y', 'FY',\n                                                   'Customer ID', 'status']+categorical_columns])","77b4cba9":"df.show(5)","638cae61":"vector_assembler = VectorAssembler(inputCols=df.columns, outputCol= \"features\")\ndata_training_and_test = vector_assembler.transform(df)","a9fe7f49":"# 70% data for training and 30% data for testing \n(training_data, test_data) = data_training_and_test.randomSplit([0.7, 0.3], 2021)","0b359cbd":"rf = RandomForestClassifier(labelCol=\"status_string_encoded\",\n                            featuresCol=\"features\", numTrees=20, seed=2021)\nmodel = rf.fit(training_data)","3c66ce0f":"predictions = model.transform(test_data)\n\npredictions.select(\"status_string_encoded\",\"prediction\", \"probability\")\\\n.show(n = 20, truncate = 30)","baf10fbe":"evaluator = MulticlassClassificationEvaluator(labelCol='status_string_encoded',\n                                              predictionCol=\"prediction\",\n                                              metricName= 'accuracy')\nf1= MulticlassClassificationEvaluator(labelCol='status_string_encoded',\n                                      predictionCol= 'prediction',\n                                      metricName= 'f1')\nprint('Accuracy', evaluator.evaluate(predictions))\nprint('F1 Score', f1.evaluate(predictions))","da85d408":"y_true = predictions.select(['status_string_encoded']).collect()\ny_pred = predictions.select(['prediction']).collect()\nprint(classification_report(y_true, y_pred))","81e0cb5a":"cnf_matrix = confusion_matrix(y_true, y_pred)\nnp.set_printoptions(precision=2)\n\n# Plot non-normalized confusion matrix\nplt.figure(figsize=(14, 8))\nplot_confusion_matrix(cnf_matrix, classes=range(0, 16)\n                      ,normalize= False,  title='Confusion matrix')","640fbc86":"lr = LogisticRegression(labelCol='status_string_encoded',\n                        featuresCol='features', predictionCol= 'prediction')\nlrModel = lr.fit(training_data)","5e7cc237":"predictions = lrModel.transform(test_data)\n\npredictions.select(\"status_string_encoded\", \"prediction\", \"probability\")\\\n.show(n=20, truncate=30)","66abbf6f":"evaluator = MulticlassClassificationEvaluator(labelCol='status_string_encoded',\n                                              predictionCol=\"prediction\",\n                                              metricName= 'accuracy')\nf1= MulticlassClassificationEvaluator(labelCol='status_string_encoded',\n                                      predictionCol= 'prediction',\n                                      metricName= 'f1')\nprint('Accuracy', evaluator.evaluate(predictions))\nprint('F1 Score', f1.evaluate(predictions))","6185565d":"y_true = predictions.select(['status_string_encoded']).collect()\ny_pred = predictions.select(['prediction']).collect()\nprint(classification_report(y_true, y_pred))","dca1a1cb":"cnf_matrix = confusion_matrix(y_true, y_pred)\nnp.set_printoptions(precision=2)\n\n# Plot non-normalized confusion matrix\nplt.figure(figsize=(14, 8))\nplot_confusion_matrix(cnf_matrix, classes=range(0, 16)\n                      ,normalize= False,  title='Confusion matrix')","6f217696":"### 5.9 Data types of data after type casting","6174e00a":"## 5. Fix Data Qulaity Problems","bb8032c0":"### 5.10 Summary Statistics of processed data","16b08e35":"### 8.4 Split data to Training and Testing","55ffd57b":"fill some null rows with mode value of a column.","60a0e1b9":"### 8.1 Graphical Confusion Matrix Code","776e6d19":"Note: For further Exploratory Data Analysis I need to fix the problems in data after fixing the data I can coninue the Exploratory Data Analysis","10c26b1e":"### 5.2 Remove null columns","d42f8f70":"Data contains 1048574 rows but 464051 rows have no records. 584524 rows contain records","e177bd03":"## 1. Install Required Libraries","6ff2f2d7":"#### 8.5.2 Performance Evaluation","a2b7fe35":"**Pakistan Largest Ecommerce Dataset EDA, Time Series Forecasting, Classification**","5c47209f":"## 6. Exploratory Data Analysis(EDA) Continue's","664537da":"## 4. Exploratory Data Analysis(EDA)","739f4f30":"### 5.1 Find missing values from data","32f922df":"Summary statistic of data and to easily see all the columns clearly without scroll I take the Transpose of statistics result.","b9e51116":"### 5.6 Fix the Data Types of Columns by performaing type casting","e5ba8b75":"### 8.6 Logistic Regression Classifier","9010cb17":"### 8.5 Random Forest Classifier","d4bfd4d7":"### 5.4 Fill null values","b8be000b":"# Table of Contents\n\n- 1. Install Required Libraries\n- 2. Import Libraries\n- 3. Load Data\n- 4. Exploratory Data Analysis (EDA)\n   - 4.1 Head of Dataset\n   - 4.2 Schema of data\n   - 4.3 Shape of data\n   - 4.4 Summary statistics of data\n   - 4.5 Price Distribution\n- 5. Fix Data Quality Problems\n   - 5.1 Find missing values from data\n   - 5.2 Remove null columns\n   - 5.3 Drop rows where all values are null\n   - 5.4 Fill null values\n   - 5.5 Removing null rows on the basis of Working Date and sku column\n   - 5.6 Fix the Data Types of Columns by performaing type casting\n   - 5.7 Remove data duplications if any exist in data\n   - 5.8 Shape of data after preprocessing\n   - 5.9 Data types of data after type casting\n   - 5.10 Summary Statistics of processed data\n- 6. Exploratory Data Analysis (EDA)\n   - 6.1 Correlation Between Columns\n   - 6.2 Best Selling category\n   - 6.3 Visulize Payment Methods versus Order Status\n   - 6.4 Order Status per Year\n   - 6.5 Payment method vs Orders\n   - 6.6 No. of Orders per Month of each year\n   - 6.7 Best Category by Completion order\n   - 6.8 Worst Category by not Completion order\n- 7. Time Series Forecasting of Daily Sales\n   - 7.1 Extracting feature day wise order count\n   - 7.2 Order count graph w.r.t date\n   - 7.3 Facebook Prophet Model for Daily Order Count Forecasting\n   - 7.4 Performance Evaluation\n- 8. ML Model for Classification of Order Status\n   - 8.1 Graphical Confusion Matrix Code\n   - 8.2 Data Prepration for Model Input\n   - 8.3 Converting Data to Features\n   - 8.4 Split data to Training and Testing\n   - 8.5 Random Forest Classifier\n       - 8.5.1 Predictions\n       - 8.5.2 Performance Evaluation\n   - 8.6 Logistic Regression Classifier\n       - 8.5.1 Predictions\n       - 8.6.2 Performance Evaluation","05923a74":"check null values","b95aa3d1":"## 2. Import Libraries","f582d4fa":"### 4.2 Schema of data","c2178007":"## Forecasting for Health & Sports category","7b14421f":"### 4.5 Price Distribution","1018faa4":"### 8.3 Converting Data to Features","ec4cb799":"Cod replace with his full name cash_on_Delivery","6c11811f":"### 7.3 Facebook Prophet Model for Daily Order Count Forecasting","fd36a145":"## Forecasting for Mobiles & Tablets category","c9d5fff1":"only left those column where are records","f0f15f50":"#### 8.6.1 Predictions","35ac83d9":"### 5.8 Shape of data after preprocessing","0f596737":"### 6.5 Payment method vs Orders","56b135f8":"### 7.1 Extracting feature day wise order count","d7b0bed8":"## 7. Time Series Forecasting of Daily Sales","4c517f8b":"I observe from the graphs in years 2016, 2017, 2017 almost completed order quantities are as follows 74000, 123400, 35600. But if we see the canclecd orders in years 2016, 2017, 2017 are as follows almost 40000, 95000, 67000. Also the refunded order of years 2016, 2017, 2017 as follow almost 14000, 3400, 1300.   \n\nI observe that order completion is decreasing and order canceletion is increasing which is not good sign for Ecommerece. Ecommerece website should have to make a policy which help them to maintain thier customer and order completion is successful.  ","faa3c43d":"Remove extra space from column name MV and rename column created_at to order_date","f3daebbe":"### 6.6 No. of Orders per Month of each Year","98e499e2":"### 6.2 Best Selling category\n","06c2e511":"## 8. ML Model for Classification of Order Status","53691288":"### 5.3 Drop rows where all values are null","02ed05db":"### 8.2 Data Prepration for Model Input","e6bff8b9":"Now I will remove where all rows are empty. If we see the tail of data we will see few of rows where all columns are empty.","40f2e90e":"#### 8.5.1 Predictions","ae338dea":"### 7.4 Performance Evaluation","153e4933":"### 4.1 Head of dataset","bafc5bf1":"### 7.2 Order count graph w.r.t date","639ea0b0":"### 6.1 Correlation Between Columns","5146187e":"## Forecasting of order count by category","49b9cdd6":"### 5.7 Remove data duplications if any exist in data","bc3f6925":"## 3. Load Data","605c81b4":"### 6.7 Best Category by Completion order","f062a86a":"### 5.5 Removes null rows on the basis of Working Date and sku column","78c72a88":"#### 8.6.2 Performance Evaluation","75887511":"By analysing data I observe that almost half of the rows are completely empty. I also observe that there are five column which are totally empty so i removed that columns from data. By further analysing I observe that i can't drop alll Null values rows because some actual data columns have null values so I need to keep them. I will only drop those rows where all values are null.","1d670cf9":"### 4.3 Shape of data","37ac6a2b":"### 6.8 Worst Category by not Completion order","d892ab34":"### 6.3 Visualize Payment Methods versus Order Status","6627d085":"Filling missing values of status and category_name_1 column by mode of column values","45efd39f":"Mobiles & Tablets are Best Selling category in Ecommerce.  \n70% of the items sell from these seven categories , Mobiles & Tablets  ,  Men's Fashion ,  Women's Fashion , Appliances  , Superstore  , Beauty & Grooming  , soghaat  contributed.","3a40055b":"### 6.4 Order Status per Year","6d1ce35d":"### 4.4 Summary statistics of data"}}