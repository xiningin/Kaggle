{"cell_type":{"84ab8ccd":"code","544a1fc8":"code","eb206451":"code","4ae65026":"code","92ed168e":"code","1370434f":"code","52808be0":"code","ab66ddbb":"code","7da65270":"code","acfaa078":"code","edef3b34":"code","fbc1527d":"code","8732d5b3":"code","9dbd4f6f":"code","c6ad283d":"code","2d337fce":"code","7664313e":"code","5d181ecf":"code","c646cd0c":"code","efec23e0":"code","d3a704b6":"code","21a6f1b4":"code","de44c03c":"code","ebd022ec":"code","c4017964":"code","43611295":"code","1ca13bce":"code","ae35c535":"code","ba9c94a6":"code","7360730e":"code","b172dd65":"code","188bebf4":"code","d1860db1":"code","8bc31761":"code","0d97874b":"code","c9f542ae":"code","1aac4f13":"code","93951f59":"code","49d439c8":"code","5656f7c7":"code","43f2038d":"code","868d77ad":"code","d5f5bf73":"code","79f89f5d":"code","fcce5783":"code","c6399de0":"code","277a151b":"code","f8cc1ffb":"code","1ef059c0":"code","3b9e4893":"code","8bc3fa5f":"code","87de360b":"code","71a9e871":"code","0e2e873c":"code","d07972e9":"code","45d19599":"code","c8e73de8":"code","987b7361":"code","bfd3f411":"code","c7539583":"code","6af6e0bb":"code","a899f498":"code","936ef337":"code","7c9afa7c":"code","01402b9d":"code","d08c78b8":"code","0c07d855":"code","21284dc7":"code","0d9dd489":"code","f673bea0":"code","6fb54a62":"code","3526590e":"code","bc601690":"code","817bbac9":"code","b17d08e8":"code","51767992":"code","1de9412b":"code","0824e971":"code","fa4f1547":"code","ef2b0f01":"code","d4e8fb3b":"code","1c30e280":"code","787e5763":"markdown","f512f752":"markdown","4e2e5f91":"markdown","5a005e2f":"markdown","129b4d51":"markdown","c239eed3":"markdown","fa068c64":"markdown","1920e177":"markdown","a41145ba":"markdown","02543f80":"markdown","7e969f1c":"markdown","bc2cc21d":"markdown","31794e41":"markdown","4c1d218b":"markdown","81c1e17a":"markdown","9f35cbb4":"markdown","62d53174":"markdown","5609d63e":"markdown","e8436fa0":"markdown","d22d416e":"markdown","35970c4a":"markdown","96e5f93b":"markdown","9ce15498":"markdown","5fce18da":"markdown","039f52c6":"markdown","a3e80a59":"markdown","0f802da1":"markdown","495f3dd7":"markdown","e827800b":"markdown","bdbb6c3d":"markdown","290fdb76":"markdown","8c2f7093":"markdown","e861523b":"markdown","d7c4b865":"markdown","8655ba85":"markdown","37553051":"markdown","0f3f2c89":"markdown"},"source":{"84ab8ccd":"# Supressing the warning messages\nimport warnings\nwarnings.filterwarnings('ignore')","544a1fc8":"# Reading the dataset\nimport pandas as pd\nimport numpy as np\nSalaryData=pd.read_csv('..\/input\/salaryband\/train.csv', encoding='latin')\nprint('Shape before deleting duplicate values:', SalaryData.shape)\n\n# Removing duplicate rows if any\nSalaryData=SalaryData.drop_duplicates()\nprint('Shape After deleting duplicate values:', SalaryData.shape)\n\n# Printing sample data\n# Start observing the Quantitative\/Categorical\/Qualitative variables\nSalaryData.head(10)","eb206451":"SalaryData.columns = SalaryData.columns.str.lower()\nSalaryData['income'].replace({'<=50K':0, '>50K':1}, inplace=True)\nSalaryData.head(10)","4ae65026":"%matplotlib inline\n# Creating Bar chart as the Target variable is Categorical\nGroupedData=SalaryData.groupby('income').size()\nGroupedData.plot(kind='bar', figsize=(4,3))","92ed168e":"# Looking at sample rows in the data\nSalaryData.head()","1370434f":"SalaryData.info()","52808be0":"SalaryData.describe(include='all')","ab66ddbb":"SalaryData.nunique()","7da65270":"# Plotting multiple bar charts at once for categorical variables\n# Since there is no default function which can plot bar charts for multiple columns at once\n# we are defining our own function for the same\n\ndef PlotBarCharts(inpData, colsToPlot):\n    %matplotlib inline\n    \n    import matplotlib.pyplot as plt\n    \n    # Generating multiple subplots\n    fig, subPlot=plt.subplots(nrows=1, ncols=len(colsToPlot), figsize=(40,6))\n    fig.suptitle('Bar charts of: '+ str(colsToPlot))\n\n    for colName, plotNumber in zip(colsToPlot, range(len(colsToPlot))):\n        inpData.groupby(colName).size().plot(kind='bar',ax=subPlot[plotNumber])","acfaa078":"# Calling the function\nPlotBarCharts(inpData=SalaryData, colsToPlot=['workclass', 'education', 'maritalstatus','occuptaion'])","edef3b34":"# Calling the function\nPlotBarCharts(inpData=SalaryData, colsToPlot=['relationship', 'race', 'gender', 'country'])","fbc1527d":"# Plotting histograms of multiple columns together\n# Observe that ApplicantIncome and CoapplicantIncome has outliers\nSalaryData.hist(['age','capitalgain','capitalloss','hoursperweek'], figsize=(18,10))","8732d5b3":"# Finding nearest values to 40000 mark\nSalaryData['capitalgain'][SalaryData['capitalgain']>40000].sort_values()","9dbd4f6f":"# Replacing outliers with nearest possibe value\nSalaryData['capitalgain'][SalaryData['capitalgain']>40000] = 41310","c6ad283d":"# Finding nearest values to 1000 mark\nSalaryData['capitalloss'][SalaryData['capitalloss']<1000].sort_values(ascending=False)","2d337fce":"# Replacing outliers with nearest possibe value\nSalaryData['capitalloss'][SalaryData['capitalloss']>1000] = 974","7664313e":"SalaryData.hist(['capitalgain','capitalloss'], figsize=(18,5))","5d181ecf":"# Finding how many missing values are there for each column\nSalaryData.isnull().sum()","c646cd0c":"#SalaryData['workclass'].fillna(SalaryData['workclass'].mode()[0], inplace=True)\n#SalaryData['occuptaion'].fillna(SalaryData['occuptaion'].mode()[0], inplace=True)\n#SalaryData['country'].fillna(SalaryData['country'].mode()[0], inplace=True)","efec23e0":"SalaryData.isnull().sum()","d3a704b6":"# Box plots for Categorical Target Variable \"income\" and continuous predictors\nContinuousColsList=['age','hoursperweek','capitalgain','capitalloss']\n\nimport matplotlib.pyplot as plt\nfig, PlotCanvas=plt.subplots(nrows=1, ncols=len(ContinuousColsList), figsize=(18,5))\n\n# Creating box plots for each continuous predictor against the Target Variable \"income\"\nfor PredictorCol , i in zip(ContinuousColsList, range(len(ContinuousColsList))):\n    SalaryData.boxplot(column=PredictorCol, by='income', figsize=(5,5), vert=True, ax=PlotCanvas[i])\n","21a6f1b4":"# Defining a function to find the statistical relationship with all the categorical variables\ndef FunctionAnova(inpData, TargetVariable, ContinuousPredictorList):\n    from scipy.stats import f_oneway\n\n    # Creating an empty list of final selected predictors\n    SelectedPredictors=[]\n    \n    print('##### ANOVA Results ##### \\n')\n    for predictor in ContinuousPredictorList:\n        CategoryGroupLists=inpData.groupby(TargetVariable)[predictor].apply(list)\n        AnovaResults = f_oneway(*CategoryGroupLists)\n        \n        # If the ANOVA P-Value is <0.05, that means we reject H0\n        if (AnovaResults[1] < 0.05):\n            print(predictor, 'is correlated with', TargetVariable, '| P-Value:', AnovaResults[1])\n            SelectedPredictors.append(predictor)\n        else:\n            print(predictor, 'is NOT correlated with', TargetVariable, '| P-Value:', AnovaResults[1])\n    \n    return(SelectedPredictors)","de44c03c":"# Calling the function to check which categorical variables are correlated with target\nContinuousVariables=['age','hoursperweek','capitalgain','capitalloss']\nFunctionAnova(inpData=SalaryData, TargetVariable='income', ContinuousPredictorList=ContinuousVariables)","ebd022ec":"# Cross tablulation between two categorical variables\nCrossTabResult=pd.crosstab(index=SalaryData['maritalstatus'], columns=SalaryData['income'])\nCrossTabResult","c4017964":"#Visual Inference using Grouped Bar charts\nCategoricalColsList=['workclass', 'education', 'maritalstatus','occuptaion',\n                    'relationship', 'race', 'gender']\n\nimport matplotlib.pyplot as plt\nfig, PlotCanvas=plt.subplots(nrows=len(CategoricalColsList), ncols=1, figsize=(10,70))\n\n# Creating Grouped bar plots for each categorical predictor against the Target Variable \"income\"\nfor CategoricalCol , i in zip(CategoricalColsList, range(len(CategoricalColsList))):\n    CrossTabResult=pd.crosstab(index=SalaryData[CategoricalCol], columns=SalaryData['income'])\n    CrossTabResult.plot.bar(color=['lightblue','green'], ax=PlotCanvas[i], title=CategoricalCol+' Vs '+'income')","43611295":"# Writing a function to find the correlation of all categorical variables with the Target variable\ndef FunctionChisq(inpData, TargetVariable, CategoricalVariablesList):\n    from scipy.stats import chi2_contingency\n    \n    # Creating an empty list of final selected predictors\n    SelectedPredictors=[]\n\n    for predictor in CategoricalVariablesList:\n        CrossTabResult=pd.crosstab(index=inpData[TargetVariable], columns=inpData[predictor])\n        ChiSqResult = chi2_contingency(CrossTabResult)\n        \n        # If the ChiSq P-Value is <0.05, that means we reject H0\n        if (ChiSqResult[1] < 0.05):\n            print(predictor, 'is correlated with', TargetVariable, '| P-Value:', ChiSqResult[1])\n            SelectedPredictors.append(predictor)\n        else:\n            print(predictor, 'is NOT correlated with', TargetVariable, '| P-Value:', ChiSqResult[1])        \n            \n    return(SelectedPredictors)","1ca13bce":"CategoricalVariables=['workclass', 'education', 'maritalstatus','occuptaion',\n                    'relationship', 'race', 'gender']\n\n# Calling the function\nFunctionChisq(inpData=SalaryData, \n              TargetVariable='income',\n              CategoricalVariablesList= CategoricalVariables)","ae35c535":"SelectedColumns=['workclass', 'education', 'maritalstatus', 'occuptaion', \n                 'relationship', 'race', 'gender','age', 'hoursperweek',\n                 'capitalgain', 'capitalloss']\n\n# Selecting final columns\nDataForML=SalaryData[SelectedColumns]\nDataForML.head()","ba9c94a6":"# Saving this final data for reference during deployment\nDataForML.to_pickle('DataForML.pkl')","7360730e":"DataForML['education'].unique()\nDataForML['gender'].unique()","b172dd65":"# Converting the binary nominal variable sex to numeric\nDataForML['gender'].replace({' Female':0, ' Male':1}, inplace=True)","188bebf4":"from sklearn import preprocessing\nlabel_encoder = preprocessing.LabelEncoder()\nDataForML['education_num']= label_encoder.fit_transform(DataForML['education'])","d1860db1":"# Treating all the nominal variables at once using dummy variables\nDataForML_Numeric=pd.get_dummies(DataForML)\n\n# Adding Target Variable to the data\nDataForML_Numeric['income']=SalaryData['income']\n\n# Printing sample rows\nDataForML_Numeric.head()","8bc31761":"pd.set_option(\"display.max.columns\", None)\npd.set_option(\"display.precision\", 2)\nDataForML_Numeric.head()","0d97874b":"# Printing all the column names for our reference\nDataForML_Numeric.columns","c9f542ae":"TargetVariable='income'\nPredictors=['education_num', 'age', 'hoursperweek', 'capitalgain', 'capitalloss',\n       'workclass_?', 'workclass_Federal-gov', 'workclass_Local-gov',\n       'workclass_Never-worked', 'workclass_Private', 'workclass_Self-emp-inc',\n       'workclass_Self-emp-not-inc', 'workclass_State-gov',\n       'workclass_Without-pay', 'maritalstatus_Divorced',\n       'maritalstatus_Married-AF-spouse', 'maritalstatus_Married-civ-spouse',\n       'maritalstatus_Married-spouse-absent', 'maritalstatus_Never-married',\n       'maritalstatus_Separated', 'maritalstatus_Widowed', 'occuptaion_?',\n       'occuptaion_Adm-clerical', 'occuptaion_Armed-Forces',\n       'occuptaion_Craft-repair', 'occuptaion_Exec-managerial',\n       'occuptaion_Farming-fishing', 'occuptaion_Handlers-cleaners',\n       'occuptaion_Machine-op-inspct', 'occuptaion_Other-service',\n       'occuptaion_Priv-house-serv', 'occuptaion_Prof-specialty',\n       'occuptaion_Protective-serv', 'occuptaion_Sales',\n       'occuptaion_Tech-support', 'occuptaion_Transport-moving',\n       'relationship_Husband', 'relationship_Not-in-family',\n       'relationship_Other-relative', 'relationship_Own-child',\n       'relationship_Unmarried', 'relationship_Wife',\n       'race_Amer-Indian-Eskimo', 'race_Asian-Pac-Islander', 'race_Black',\n       'race_Other', 'race_White', 'gender_Female', 'gender_Male']","1aac4f13":"X=DataForML_Numeric[Predictors].values\ny=DataForML_Numeric[TargetVariable].values","93951f59":"# Split the data into training and testing set\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=428)","49d439c8":"### Sandardization of data ###\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\n# Choose either standardization or Normalization\n# On this data Min Max Normalization produced better results\n\n# Choose between standardization and MinMAx normalization\n#PredictorScaler=StandardScaler()\nPredictorScaler=MinMaxScaler()\n\n# Storing the fit object for later reference\nPredictorScalerFit=PredictorScaler.fit(X)\n\n# Generating the standardized values of X\nX=PredictorScalerFit.transform(X)\n\n# Split the data into training and testing set\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)","5656f7c7":"# Sanity check for the sampled data\nprint(X_train.shape)\nprint(y_train.shape)\nprint(X_test.shape)\nprint(y_test.shape)","43f2038d":"# Logistic Regression\nfrom sklearn.linear_model import LogisticRegression\n# choose parameter Penalty='l1' or C=1\n# choose different values for solver 'newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'\nclf = LogisticRegression(C=1,penalty='l2', solver='newton-cg')\n\n# Printing all the parameters of logistic regression\n# print(clf)\n\n# Creating the model on Training Data\nLOG=clf.fit(X_train,y_train)\nprediction=LOG.predict(X_test)\n\n# Measuring accuracy on Testing Data\nfrom sklearn import metrics\nprint(metrics.classification_report(y_test, prediction))\nprint(metrics.confusion_matrix(y_test, prediction))\n\n# Printing the Overall Accuracy of the model\nF1_Score=metrics.f1_score(y_test, prediction, average='weighted')\nprint('Accuracy of the model on Testing Sample Data:', round(F1_Score,2))\n\n# Importing cross validation function from sklearn\nfrom sklearn.model_selection import cross_val_score\n\n# Running 10-Fold Cross validation on a given algorithm\n# Passing full data X and y because the K-fold will split the data and automatically choose train\/test\nAccuracy_Values=cross_val_score(LOG, X , y, cv=10, scoring='f1_weighted')\nprint('\\nAccuracy values for 10-fold Cross Validation:\\n',Accuracy_Values)\nprint('\\nFinal Average Accuracy of the model:', round(Accuracy_Values.mean(),2))\n","868d77ad":"from sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import roc_curve\ny_pred_proba = clf.predict_proba(X_test)[::,1]\nfpr, tpr, _ = roc_curve(y_test,prediction)\nauc = roc_auc_score(y_test,prediction)\nplt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc))\nplt.legend(loc=4)\nplt.show()","d5f5bf73":"#Decision Trees\nfrom sklearn import tree\n#choose from different tunable hyper parameters\nclf = tree.DecisionTreeClassifier(max_depth=6,criterion='entropy')\n\n# Printing all the parameters of Decision Trees\nprint(clf)\n\n# Creating the model on Training Data\nDTree=clf.fit(X_train,y_train)\nprediction=DTree.predict(X_test)\n\n# Measuring accuracy on Testing Data\nfrom sklearn import metrics\nprint(metrics.classification_report(y_test, prediction))\nprint(metrics.confusion_matrix(y_test, prediction))\n\n# Printing the Overall Accuracy of the model\nF1_Score=metrics.f1_score(y_test, prediction, average='weighted')\nprint('Accuracy of the model on Testing Sample Data:', round(F1_Score,2))\n\n# Plotting the feature importance for Top 10 most important columns\n%matplotlib inline\nfeature_importances = pd.Series(DTree.feature_importances_, index=Predictors)\nfeature_importances.nlargest(10).plot(kind='barh')\n\n# Importing cross validation function from sklearn\nfrom sklearn.model_selection import cross_val_score\n\n# Running 10-Fold Cross validation on a given algorithm\n# Passing full data X and y because the K-fold will split the data and automatically choose train\/test\nAccuracy_Values=cross_val_score(DTree, X , y, cv=10, scoring='f1_weighted')\nprint('\\nAccuracy values for 10-fold Cross Validation:\\n',Accuracy_Values)\nprint('\\nFinal Average Accuracy of the model:', round(Accuracy_Values.mean(),2))","79f89f5d":"from sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import roc_curve\ny_pred_proba = clf.predict_proba(X_test)[::,1]\nfpr, tpr, _ = roc_curve(y_test,prediction)\nauc = roc_auc_score(y_test,prediction)\nplt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc))\nplt.legend(loc=4)\nplt.show()","fcce5783":"# Random Forest (Bagging of multiple Decision Trees)\nfrom sklearn.ensemble import RandomForestClassifier\n# Choose different hyperparameter values of max_depth, n_estimators and criterion to tune the model\nclf = RandomForestClassifier(max_depth=5, n_estimators=100,criterion='gini')\n\n# Printing all the parameters of Random Forest\nprint(clf)\n\n# Creating the model on Training Data\nRF=clf.fit(X_train,y_train)\nprediction=RF.predict(X_test)\n\n# Measuring accuracy on Testing Data\nfrom sklearn import metrics\nprint(metrics.classification_report(y_test, prediction))\nprint(metrics.confusion_matrix(y_test, prediction))\n\n# Printing the Overall Accuracy of the model\nF1_Score=metrics.f1_score(y_test, prediction, average='weighted')\nprint('Accuracy of the model on Testing Sample Data:', round(F1_Score,2))\n\n# Importing cross validation function from sklearn\nfrom sklearn.model_selection import cross_val_score\n\n# Running 10-Fold Cross validation on a given algorithm\n# Passing full data X and y because the K-fold will split the data and automatically choose train\/test\nAccuracy_Values=cross_val_score(RF, X , y, cv=10, scoring='f1_weighted')\nprint('\\nAccuracy values for 10-fold Cross Validation:\\n',Accuracy_Values)\nprint('\\nFinal Average Accuracy of the model:', round(Accuracy_Values.mean(),2))\n\n\n# Plotting the feature importance for Top 10 most important columns\n%matplotlib inline\nfeature_importances = pd.Series(RF.feature_importances_, index=Predictors)\nfeature_importances.nlargest(10).plot(kind='barh')","c6399de0":"from sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import roc_curve\ny_pred_proba = clf.predict_proba(X_test)[::,1]\nfpr, tpr, _ = roc_curve(y_test,prediction)\nauc = roc_auc_score(y_test,prediction)\nplt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc))\nplt.legend(loc=4)\nplt.show()","277a151b":"# Adaboost \nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n\n# Choosing Decision Tree with 1 level as the weak learner\nDTC=DecisionTreeClassifier(max_depth=1)\nclf = AdaBoostClassifier(n_estimators=100, base_estimator=DTC ,learning_rate=0.1)\n\n# Printing all the parameters of Adaboost\nprint(clf)\n\n# Creating the model on Training Data\nAB=clf.fit(X_train,y_train)\nprediction=AB.predict(X_test)\n\n# Measuring accuracy on Testing Data\nfrom sklearn import metrics\nprint(metrics.classification_report(y_test, prediction))\nprint(metrics.confusion_matrix(y_test, prediction))\n\n# Printing the Overall Accuracy of the model\nF1_Score=metrics.f1_score(y_test, prediction, average='weighted')\nprint('Accuracy of the model on Testing Sample Data:', round(F1_Score,2))\n\n# Importing cross validation function from sklearn\nfrom sklearn.model_selection import cross_val_score\n\n# Running 10-Fold Cross validation on a given algorithm\n# Passing full data X and y because the K-fold will split the data and automatically choose train\/test\nAccuracy_Values=cross_val_score(AB, X , y, cv=10, scoring='f1_weighted')\nprint('\\nAccuracy values for 10-fold Cross Validation:\\n',Accuracy_Values)\nprint('\\nFinal Average Accuracy of the model:', round(Accuracy_Values.mean(),2))\n\n# Plotting the feature importance for Top 10 most important columns\n%matplotlib inline\nfeature_importances = pd.Series(AB.feature_importances_, index=Predictors)\nfeature_importances.nlargest(10).plot(kind='barh')","f8cc1ffb":"from sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import roc_curve\ny_pred_proba = clf.predict_proba(X_test)[::,1]\nfpr, tpr, _ = roc_curve(y_test,prediction)\nauc = roc_auc_score(y_test,prediction)\nplt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc))\nplt.legend(loc=4)\nplt.show()","1ef059c0":"# Xtreme Gradient Boosting (XGBoost)\nfrom xgboost import XGBClassifier\nclf=XGBClassifier(max_depth=2, learning_rate=0.1, n_estimators=200, objective='binary:logistic', booster='gbtree')\n\n# Printing all the parameters of XGBoost\nprint(clf)\n\n# Creating the model on Training Data\nXGB=clf.fit(X_train,y_train)\nprediction=XGB.predict(X_test)\n\n# Measuring accuracy on Testing Data\nfrom sklearn import metrics\nprint(metrics.classification_report(y_test, prediction))\nprint(metrics.confusion_matrix(y_test, prediction))\n\n# Printing the Overall Accuracy of the model\nF1_Score=metrics.f1_score(y_test, prediction, average='weighted')\nprint('Accuracy of the model on Testing Sample Data:', round(F1_Score,2))\n\n# Importing cross validation function from sklearn\nfrom sklearn.model_selection import cross_val_score\n\n# Running 10-Fold Cross validation on a given algorithm\n# Passing full data X and y because the K-fold will split the data and automatically choose train\/test\nAccuracy_Values=cross_val_score(XGB, X , y, cv=10, scoring='f1_weighted')\nprint('\\nAccuracy values for 10-fold Cross Validation:\\n',Accuracy_Values)\nprint('\\nFinal Average Accuracy of the model:', round(Accuracy_Values.mean(),2))\n\n# Plotting the feature importance for Top 10 most important columns\n%matplotlib inline\nfeature_importances = pd.Series(XGB.feature_importances_, index=Predictors)\nfeature_importances.nlargest(10).plot(kind='barh')","3b9e4893":"from sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import roc_curve\ny_pred_proba = clf.predict_proba(X_test)[::,1]\nfpr, tpr, _ = roc_curve(y_test,prediction)\nauc = roc_auc_score(y_test,prediction)\nplt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc))\nplt.legend(loc=4)\nplt.show()","8bc3fa5f":"# K-Nearest Neighbor(KNN)\nfrom sklearn.neighbors import KNeighborsClassifier\nclf = KNeighborsClassifier(n_neighbors=3)\n\n# Printing all the parameters of KNN\nprint(clf)\n\n# Creating the model on Training Data\nKNN=clf.fit(X_train,y_train)\nprediction=KNN.predict(X_test)\n\n# Measuring accuracy on Testing Data\nfrom sklearn import metrics\nprint(metrics.classification_report(y_test, prediction))\nprint(metrics.confusion_matrix(y_test, prediction))\n\n# Printing the Overall Accuracy of the model\nF1_Score=metrics.f1_score(y_test, prediction, average='weighted')\nprint('Accuracy of the model on Testing Sample Data:', round(F1_Score,2))\n\n# Importing cross validation function from sklearn\nfrom sklearn.model_selection import cross_val_score\n\n# Running 10-Fold Cross validation on a given algorithm\n# Passing full data X and y because the K-fold will split the data and automatically choose train\/test\nAccuracy_Values=cross_val_score(KNN, X , y, cv=10, scoring='f1_weighted')\nprint('\\nAccuracy values for 10-fold Cross Validation:\\n',Accuracy_Values)\nprint('\\nFinal Average Accuracy of the model:', round(Accuracy_Values.mean(),2))","87de360b":"from sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import roc_curve\ny_pred_proba = clf.predict_proba(X_test)[::,1]\nfpr, tpr, _ = roc_curve(y_test,prediction)\nauc = roc_auc_score(y_test,prediction)\nplt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc))\nplt.legend(loc=4)\nplt.show()","71a9e871":"# Support Vector Machines(SVM)\nfrom sklearn import svm\nclf = svm.SVC(C=3, kernel='rbf', gamma=0.1)\n\n# Printing all the parameters of KNN\nprint(clf)\n\n# Creating the model on Training Data\nSVM=clf.fit(X_train,y_train)\nprediction=SVM.predict(X_test)\n\n# Measuring accuracy on Testing Data\nfrom sklearn import metrics\nprint(metrics.classification_report(y_test, prediction))\nprint(metrics.confusion_matrix(y_test, prediction))\n\n# Printing the Overall Accuracy of the model\nF1_Score=metrics.f1_score(y_test, prediction, average='weighted')\nprint('Accuracy of the model on Testing Sample Data:', round(F1_Score,2))\n\n# Importing cross validation function from sklearn\nfrom sklearn.model_selection import cross_val_score\n\n# Running 10-Fold Cross validation on a given algorithm\n# Passing full data X and y because the K-fold will split the data and automatically choose train\/test\nAccuracy_Values=cross_val_score(SVM, X , y, cv=10, scoring='f1_weighted')\nprint('\\nAccuracy values for 10-fold Cross Validation:\\n',Accuracy_Values)\nprint('\\nFinal Average Accuracy of the model:', round(Accuracy_Values.mean(),2))","0e2e873c":"test=pd.read_csv('..\/input\/salaryband\/test.csv', encoding='latin')\nids = test['Id']","d07972e9":"test.columns = test.columns.str.lower()","45d19599":"SelectedColumns=['workclass', 'education', 'maritalstatus', 'occuptaion', \n                 'relationship', 'race', 'gender','age', 'hoursperweek',\n                 'capitalgain', 'capitalloss']\n\n# Selecting final columns\nDataFortest=test[SelectedColumns]\nDataFortest.head()","c8e73de8":"DataFortest['gender'].replace({' Female':0, ' Male':1}, inplace=True)","987b7361":"from sklearn import preprocessing\nlabel_encoder = preprocessing.LabelEncoder()\nDataFortest['education_num']= label_encoder.fit_transform(DataFortest['education'])\nDataFortest_Numeric=pd.get_dummies(DataFortest)","bfd3f411":"DataFortest_Numeric.head()","c7539583":"X=DataFortest_Numeric.values","6af6e0bb":"### Sandardization of data ###\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\n# Choose either standardization or Normalization\n# On this data Min Max Normalization produced better results\n\n# Choose between standardization and MinMAx normalization\n#PredictorScaler=StandardScaler()\nPredictorScaler=MinMaxScaler()\n\n# Storing the fit object for later reference\nPredictorScalerFit=PredictorScaler.fit(X)\n\n# Generating the standardized values of X\nX=PredictorScalerFit.transform(X)\n","a899f498":"DataFortest_Numeric.head()","936ef337":"DataForML_Numeric.columns","7c9afa7c":"TargetVariable='income'\nPredictors=['education_num', 'age', 'hoursperweek', 'capitalgain', 'capitalloss',\n       'workclass_?', 'workclass_Federal-gov', 'workclass_Local-gov',\n       'workclass_Never-worked', 'workclass_Private', 'workclass_Self-emp-inc',\n       'workclass_Self-emp-not-inc', 'workclass_State-gov',\n       'workclass_Without-pay', 'maritalstatus_Divorced',\n       'maritalstatus_Married-AF-spouse', 'maritalstatus_Married-civ-spouse',\n       'maritalstatus_Married-spouse-absent', 'maritalstatus_Never-married',\n       'maritalstatus_Separated', 'maritalstatus_Widowed', 'occuptaion_?',\n       'occuptaion_Adm-clerical', 'occuptaion_Armed-Forces',\n       'occuptaion_Craft-repair', 'occuptaion_Exec-managerial',\n       'occuptaion_Farming-fishing', 'occuptaion_Handlers-cleaners',\n       'occuptaion_Machine-op-inspct', 'occuptaion_Other-service',\n       'occuptaion_Priv-house-serv', 'occuptaion_Prof-specialty',\n       'occuptaion_Protective-serv', 'occuptaion_Sales',\n       'occuptaion_Tech-support', 'occuptaion_Transport-moving',\n       'relationship_Husband', 'relationship_Not-in-family',\n       'relationship_Other-relative', 'relationship_Own-child',\n       'relationship_Unmarried', 'relationship_Wife',\n       'race_Amer-Indian-Eskimo', 'race_Asian-Pac-Islander', 'race_Black',\n       'race_Other', 'race_White', 'gender_Female', 'gender_Male']","01402b9d":"X_train=DataForML_Numeric[Predictors].values\ny_train=DataForML_Numeric[TargetVariable].values\nX_test=DataFortest_Numeric[Predictors].values","d08c78b8":"print(X_train.shape)\nprint(y_train.shape)\nprint(X_test.shape)","0c07d855":"# Xtreme Gradient Boosting (XGBoost)\nfrom xgboost import XGBClassifier\nclf=XGBClassifier(max_depth=2, learning_rate=0.1, n_estimators=200, objective='binary:logistic', booster='gbtree')\n\n# Printing all the parameters of XGBoost\nprint(clf)\n\n# Printing all the parameters of logistic regression\n# print(clf)\n\n# Creating the model on Training Data\nXGB=clf.fit(X_train,y_train)\nprediction=XGB.predict(X_test)","21284dc7":"prediction","0d9dd489":"submission = pd.DataFrame({'Id' : ids ,'Income Band': prediction })","f673bea0":"submission.head(30)","6fb54a62":"submission['Income Band'].replace({0 :'<=50K', 1:'>50K'}, inplace=True)","3526590e":"submission.to_csv(r'C:\\Users\\SAYAN\\Desktop\\New folder (2)\\sayanayaksubmission.csv', header=True, index=False)","bc601690":"# Separate Target Variable and Predictor Variables\nTargetVariable='income'\n\n# Selecting the final set of predictors for the deployment\n# Based on the variable importance charts of multiple algorithms above\nPredictors=['education_num', 'age', 'hoursperweek', 'capitalgain', 'capitalloss',\n       'workclass_?', 'workclass_Federal-gov', 'workclass_Local-gov',\n       'workclass_Never-worked', 'workclass_Private', 'workclass_Self-emp-inc',\n       'workclass_Self-emp-not-inc', 'workclass_State-gov',\n       'workclass_Without-pay', 'maritalstatus_Divorced',\n       'maritalstatus_Married-AF-spouse', 'maritalstatus_Married-civ-spouse',\n       'maritalstatus_Married-spouse-absent', 'maritalstatus_Never-married',\n       'maritalstatus_Separated', 'maritalstatus_Widowed', 'occuptaion_?',\n       'occuptaion_Adm-clerical', 'occuptaion_Armed-Forces',\n       'occuptaion_Craft-repair', 'occuptaion_Exec-managerial',\n       'occuptaion_Farming-fishing', 'occuptaion_Handlers-cleaners',\n       'occuptaion_Machine-op-inspct', 'occuptaion_Other-service',\n       'occuptaion_Priv-house-serv', 'occuptaion_Prof-specialty',\n       'occuptaion_Protective-serv', 'occuptaion_Sales',\n       'occuptaion_Tech-support', 'occuptaion_Transport-moving']\n\nX=DataForML_Numeric[Predictors].values\ny=DataForML_Numeric[TargetVariable].values\n\n### Sandardization of data ###\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\n# Choose either standardization or Normalization\n# On this data Min Max Normalization produced better results\n\n# Choose between standardization and MinMAx normalization\n#PredictorScaler=StandardScaler()\nPredictorScaler=MinMaxScaler()\n\n# Storing the fit object for later reference\nPredictorScalerFit=PredictorScaler.fit(X)\n\n# Generating the standardized values of X\nX=PredictorScalerFit.transform(X)\n\nprint(X.shape)\nprint(y.shape)\n","817bbac9":"from sklearn.linear_model import LogisticRegression\n# choose parameter Penalty='l1' or C=1\n# choose different values for solver 'newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'\n# Using the Logistic Regression algorithm with final hyperparamters\nclf = LogisticRegression(C=1,penalty='l2', solver='newton-cg')\n\n# Training the model on 100% Data available\nFinalLogisticModel=clf.fit(X,y)","b17d08e8":"from sklearn.model_selection import cross_val_score\n\n# Running 10-Fold Cross validation on a given algorithm\n# Passing full data X and y because the K-fold will split the data and automatically choose train\/test\nAccuracy_Values=cross_val_score(FinalLogisticModel, X , y, cv=10, scoring='f1_weighted')\nprint('\\nAccuracy values for 10-fold Cross Validation:\\n',Accuracy_Values)\nprint('\\nFinal Average Accuracy of the model:', round(Accuracy_Values.mean(),2))","51767992":"import pickle\nimport os\n\n# Saving the Python objects as serialized files can be done using pickle library\n# Here let us save the Final model\nwith open('FinalLogisticModel.pkl', 'wb') as fileWriteStream:\n    pickle.dump(FinalLogisticModel, fileWriteStream)\n    # Don't forget to close the filestream!\n    fileWriteStream.close()\n    \nprint('pickle file of Predictive Model is saved at Location:',os.getcwd())","1de9412b":"# This Function can be called from any from any front end tool\/website\ndef PredictSalaryBand(InputData):\n    import pandas as pd\n    Num_Inputs=InputData.shape[0]\n    \n    # Making sure the input data has same columns as it was used for training the model\n    # Also, if standardization\/normalization was done, then same must be done for new input\n    \n    # Appending the new data with the Training data\n    DataForML=pd.read_pickle('DataForML.pkl')\n    InputData=InputData.append(DataForML)\n    \n    # Generating dummy variables for rest of the nominal variables\n    InputData=pd.get_dummies(InputData)\n            \n    # Maintaining the same order of columns as it was during the model training\n    Predictors=['education_num', 'age', 'hoursperweek', 'capitalgain', 'capitalloss',\n       'workclass_?', 'workclass_Federal-gov', 'workclass_Local-gov',\n       'workclass_Never-worked', 'workclass_Private', 'workclass_Self-emp-inc',\n       'workclass_Self-emp-not-inc', 'workclass_State-gov',\n       'workclass_Without-pay', 'maritalstatus_Divorced',\n       'maritalstatus_Married-AF-spouse', 'maritalstatus_Married-civ-spouse',\n       'maritalstatus_Married-spouse-absent', 'maritalstatus_Never-married',\n       'maritalstatus_Separated', 'maritalstatus_Widowed', 'occuptaion_?',\n       'occuptaion_Adm-clerical', 'occuptaion_Armed-Forces',\n       'occuptaion_Craft-repair', 'occuptaion_Exec-managerial',\n       'occuptaion_Farming-fishing', 'occuptaion_Handlers-cleaners',\n       'occuptaion_Machine-op-inspct', 'occuptaion_Other-service',\n       'occuptaion_Priv-house-serv', 'occuptaion_Prof-specialty',\n       'occuptaion_Protective-serv', 'occuptaion_Sales',\n       'occuptaion_Tech-support', 'occuptaion_Transport-moving']\n    \n    # Generating the input values to the model\n    X=InputData[Predictors].values[0:Num_Inputs]\n    \n    # Generating the standardized values of X since it was done while model training also\n    X=PredictorScalerFit.transform(X)\n    \n    # Loading the Function from pickle file\n    import pickle\n    with open('FinalLogisticModel.pkl', 'rb') as fileReadStream:\n        LogisticModel=pickle.load(fileReadStream)\n        # Don't forget to close the filestream!\n        fileReadStream.close()\n            \n    # Genrating Predictions\n    Prediction=LogisticModel.predict(X)\n    PredictedStatus=pd.DataFrame(Prediction, columns=['Predicted Status'])\n    return(PredictedStatus)","0824e971":"NewEmployeeDetails=pd.DataFrame(\ndata=[[39,11,40,15024,0,'State-gov','Never-married','Sales'],\n     [40,11,40,2174,0,'Private','Never-married','Tech-support']],\ncolumns=['age', 'education_num', 'hoursperweek','capitalgain', \n         'capitalloss', 'workclass', 'maritalstatus','occuptaion'])\n\nprint(NewEmployeeDetails)\n\n# Calling the Function for prediction\nPredictSalaryBand(InputData= NewEmployeeDetails)","fa4f1547":"# Creating the function which can take inputs and return predictions\ndef FunctionSalaryBandPrediction(inp_age, inp_education_num , inp_hours_per_week, inp_capital_gain,\n                               inp_capital_loss, inp_workclass, inp_marital_status,inp_occuptaion):\n    \n    # Creating a data frame for the model input\n    SampleInputData=pd.DataFrame(\n     data=[[inp_age, inp_education_num , inp_hours_per_week, inp_capital_gain,\n           inp_capital_loss, inp_workclass, inp_marital_status,inp_occuptaion]],\n     columns=['age', 'education_num', 'hoursperweek','capitalgain', \n         'capitalloss', 'workclass', 'maritalstatus','occuptaion'])\n\n    # Calling the function defined above using the input parameters\n    Predictions=PredictSalaryBand(InputData= SampleInputData)\n\n    # Returning the predictions\n    return(Predictions.to_json())\n\n# Function call\nFunctionSalaryBandPrediction(inp_age=39,\n                             inp_education_num =13,\n                             inp_hours_per_week=40,\n                             inp_capital_gain=15024,\n                             inp_capital_loss=0,\n                             inp_workclass='State-gov',\n                             inp_marital_status='Never-married',\n                             inp_occuptaion='Tech-support',\n                            )","ef2b0f01":"from flask import Flask, request, jsonify\nimport pickle\nimport pandas as pd\nimport numpy","d4e8fb3b":"app = Flask(__name__)\n\n@app.route('\/get_salary_band_prediction', methods=[\"GET\"])\ndef get_salary_band_prediction():\n    try:\n        # Getting the paramters from API call\n        age_value = float(request.args.get('age'))\n        education_num_value = float(request.args.get('education_num'))\n        hours_per_week_value=float(request.args.get('hoursperweek'))\n        capital_gain_value=float(request.args.get('capitalgain'))\n        capital_loss_value = float(request.args.get('capitalloss'))\n        workclass_value = request.args.get('workclass')\n        marital_status_value = request.args.get('maritalstatus')\n        occuptaion_value=request.args.get('occuptaion')\n                \n        # Calling the funtion to get predictions\n        prediction_from_api=FunctionSalaryBandPrediction(\n                             inp_age=age_value,\n                             inp_education_num =education_num_value,\n                             inp_hours_per_week=hours_per_week_value,\n                             inp_capital_gain=capital_gain_value,\n                             inp_capital_loss=capital_loss_value,\n                             inp_workclass=workclass_value,\n                             inp_marital_status=marital_status_value,\n                             inp_occuptaion=occuptaion_value\n                             )\n\n        return (prediction_from_api)\n    \n    except Exception as e:\n        return('Something is not right!:'+str(e))","1c30e280":"import os\nif __name__ ==\"__main__\":\n    \n    # Hosting the API in localhost\n    app.run(host='127.0.0.1', port=8080, threaded=True, debug=True, use_reloader=False)\n    # Interrupt kernel to stop the API","787e5763":"# Visual Exploratory Data Analysis\n\nCategorical variables: Bar plot\nContinuous variables: Histogram\n\n# Visualize distribution of all the Categorical Predictor variables in the data using bar plots","f512f752":"## Visualizing distribution after outlier treatment","4e2e5f91":"# Basic Data Exploration Results\n\nBased on the basic exploration above, you can now create a simple report of the data, noting down your observations regaring each column. Hence, creating a initial roadmap for further analysis.\n\nThe selected columns in this step are not final, further study will be done and then a final list will be created\n\nage: Continuous. Selected.\n\nworkclass: Categorical. Selected.\n\neducation: Categorical. Selected.\n\nmarital_status: Categorical. Selected.\n\noccupation: Categorical. Selected.\n\nrelationship: Categorical. Selected.\n\nrace: Categorical. Selected.\n\ngender: Categorical. Selected.\n\ncapital_gain: Continuous. Selected.\n\ncapital.loss: Continuous. Selected.\n\nhours_per_week: Continuous. Selected.\n\ncountry: Categorical. Selected.\n\nincome: Categorical. Selected. This is the Target Variable!","5a005e2f":"# Defining the problem statement:\n\nCreate a Classification model which can tell if a person deserves a salary greater than 50,000 dollars or not?\n\nTarget Variable: income\n\nPredictors: age, workclass, education, marital_status, occupation etc.\n\nincome=0 The employee earns less than or equal to 50,000 dollars in a year\n\nincome=1 The employee earns more than 50,000 dollars in a year","129b4d51":"# Machine Learning: Splitting the data into Training and Testing sample","c239eed3":"# Looking at the distribution of Target variable & Basic Data Exploration","fa068c64":"## Relationship exploration: Categorical Vs Continuous -- Box Plots","1920e177":"# Adaboost","a41145ba":"### Converting the binary nominal variable to numeric using 1\/0 mapping","02543f80":"# Submission and Data Export","7e969f1c":"### If the distribution looks similar for each category(Boxes are in the same line), that means the the continuous variable has NO effect on the target variable. Hence, the variables are not correlated to each other.\n\n\n\n\n\n# Statistical Feature Selection (Categorical Vs Continuous) using ANOVA test\n\nAnalysis of variance(ANOVA) is performed to check if there is any relationship between the given continuous and categorical variable\n\nAssumption(H0): There is NO relation between the given variables (i.e. The average(mean) values of the numeric Predictor variable is same for all the groups in the categorical Target variable)\nANOVA Test result: Probability of H0 being true","bc2cc21d":"# http:\/\/127.0.0.1:8080\/get_salary_band_prediction?age=25&education_num=11&hoursperweek=40&capitalgain=0&capitalloss=0&workclass='State-gov'&maritalstatus='Never-married'&occuptaion='Tech-support'","31794e41":"#  Test Data for XGboost prediction","4c1d218b":"# Logistic Regression","81c1e17a":"# Data Pre-processing for Machine Learning\n\nList of steps performed on predictor variables before data can be used for machine learning\n\n    1. Converting each Ordinal Categorical columns to numeric\n    2. Converting Binary nominal Categorical columns to numeric using 1\/0 mapping\n    3. Converting all other nominal categorical columns to numeric using pd.get_dummies()\n    4. Data Transformation (Optional): Standardization\/Normalization\/log\/sqrt. Important if you are using distance based algorithms like KNN, or Neural Networks","9f35cbb4":"### The data distribution of the target variable is satisfactory to proceed further. There are sufficient number of rows for each category to learn from.\n\nThere are four commands which are used for Basic data exploration in Python\n\nhead() : This helps to see a few sample rows of the data\n\ninfo() : This provides the summarized information of the data\n\ndescribe() : This provides the descriptive statistical details of the data\n\nnunique(): This helps us to identify if a column is categorical or continuous","62d53174":"# This case study is about apt salary band determination. When an organization decides to hire a new employee and the question is how much salary does this person deserves based on their credentials, demographic\/anagraphic details and experience?\n\nBased on the \"Census Income\" data of 32,561 professionals from around the world. We will try to learn when a candidate deserves a salary greater than $50K and when they does not.\n\nIn below case study I will discuss the step by step approach to create a Machine Learning predictive model in such scenarios. You can use this flow as a template to solve any supervised ML classification problem.\n\nThe flow of the case study is as below:\n\nReading the data in python\nDefining the problem statement\nIdentifying the Target variable\nLooking at the distribution of Target variable\nBasic Data exploration\nRejecting useless columns\nVisual Exploratory Data Analysis for data distribution (Histogram and Barcharts)\nFeature Selection based on data distribution\nOutlier treatment\nMissing Values treatment\nVisual correlation analysis\nStatistical correlation analysis (Feature Selection)\nConverting data to numeric for ML\nSampling and K-fold cross validation\nTrying multiple classification algorithms\nSelecting the best Model\nDeploying the best model in production","5609d63e":"### Converting the Ordinal categorical variable","e8436fa0":"# Deployment by FLASK API","d22d416e":"# Standardization\/Normalization of data\n\n","35970c4a":"## Selected Continuous Variables:\n\nage : Selected.\n\ncapital_gain: Selected. Outliers seen beyond 40000, need to treat them.\n\ncapital.loss: Selected. Outliers seen beyond 1000, need to treat them.\n\nhours_per_week: Selected. Distribution looks good.\n\n# Outlier treatment\n\nOutliers are extreme values in the data which are far away from most of the values. You can see them as the tails in the histogram.\n\nOutlier must be treated one column at a time. As the treatment will be slightly different for each column.","96e5f93b":"# Decision Trees","9ce15498":"### Converting the nominal variable to numeric using get_dummies()","5fce18da":"# XGBoost","039f52c6":"# SVM","a3e80a59":"# Random Forest","0f802da1":"# I am choosing XGBOOST as the final model since it is fast and accuarcy is better for this data!  >>>>>>>>>>>>>>>                and                 <<<<<<<<<<<<<<  I am choosing Logistic Regression as the final model for production since it is very fast for this data!","495f3dd7":"# Relationship exploration: Categorical Vs Categorical -- Grouped Bar Charts","e827800b":"# Missing values treatment\n\nMissing values are treated for each column separately.\n\nIf a column has more than 30% data missing, then missing value treatment cannot be done. That column must be rejected because too much information is missing.","bdbb6c3d":"## Bar Charts Interpretation \n\nIf there is a column which shows too skewed distribution like \"country\" where there is only one dominant bar and the other categories are present in very low numbers. These kind of columns may not be very helpful in machine learning. \n\nSelected Categorical Variables: All the categorical variables are selected except \"country\".\n\n'workclass', 'education', 'marital_status','occupation', 'relationship', 'race', 'sex'","290fdb76":"# Feature Selection\n\nNow its time to finally choose the best columns(Features) which are correlated to the Target variable. This can be done directly by measuring the correlation values or ANOVA\/Chi-Square tests. However, it is always helpful to visualize the relation between the Target variable and each of the predictors to get a better sense of data.\n\nI have listed below the techniques used for visualizing relationship between two variables as well as measuring the strength statistically.\n\nVisual exploration of relationship between variables\n\n    Continuous Vs Continuous ---- Scatter Plot\n    Categorical Vs Continuous---- Box Plot\n    Categorical Vs Categorical---- Grouped Bar Plots\n\nStatistical measurement of relationship strength between variables\n\n    Continuous Vs Continuous ---- Correlation matrix\n    Categorical Vs Continuous---- ANOVA test\n    Categorical Vs Categorical--- Chi-Square test\n\nIn this case study the Target variable is categorical, hence below two scenarios will be present\n\n    Categorical Target Variable Vs Continuous Predictor\n    Categorical Target Variable Vs Categorical Predictor","8c2f7093":"# Selecting final predictors for Machine Learning","e861523b":"# Data description\nThe business meaning of each column in the data is as below\n\nid: index\n\nage: Age of the employee\n\nworkclass: Which type of organization the employee works in? State-gov\/Private etc.\n\neducation: The highest education of the employee\n\nmarital_status: The marital status of the employee\n\noccupation: The type of job\n\nrelationship: Type of relationship in? Husband, wife etc.\n\nrace: Which race the employee belongs to\n\ngender: Gender of the employee\n\ncapital_gain: How much capital gains does the employee gets in an year\n\ncapital.loss: How much capital loss does the employee bears in an year\n\nhours_per_week: How many hours the employee works in a week?\n\ncountry: Which country the employee is working?\n\nincome: Is the salary greater than $50,000K or not","d7c4b865":"### Visualize distribution of all the Continuous Predictor variables in the data using histograms","8655ba85":"# Loading Test data and then Data preprocessing","37553051":"### These grouped bar charts show the frequency in the Y-Axis and the category in the X-Axis. If the ratio of bars is similar across all categories, then the two columns are not correlated.\n\n## Statistical Feature Selection (Categorical Vs Categorical) using Chi-Square Test\n\nChi-Square test is conducted to check the correlation between two categorical variables\n\n    Assumption(H0): The two columns are NOT related to each other\n    Result of Chi-Sq Test: The Probability of H0 being True","0f3f2c89":"# KNN"}}