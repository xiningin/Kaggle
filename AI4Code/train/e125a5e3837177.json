{"cell_type":{"1150083e":"code","e8152eb5":"code","05e6a48d":"code","65ea4728":"code","9bd492da":"code","23f9ac9e":"code","a00a42e6":"code","827aa637":"code","09d1ffb5":"code","73cad618":"code","581c6663":"code","6e9f103d":"code","646f3046":"code","1d34331e":"code","03005035":"code","22826c7e":"code","3a8a6bf6":"code","11d6ea30":"code","0bea2386":"code","a84d6228":"code","ecef5126":"code","5cef801b":"code","5675ef53":"code","211bd4cc":"code","48333be1":"code","5db12703":"code","25bea294":"markdown","7560abc6":"markdown","de8deaaa":"markdown","408bdfea":"markdown","29aadd76":"markdown","2ca3fbb2":"markdown","e7357a24":"markdown"},"source":{"1150083e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","e8152eb5":"df = pd.read_csv('..\/input\/laptop-prices\/laptops.csv',encoding='latin-1')","05e6a48d":"df.corr()","65ea4728":"df.info()","9bd492da":"df.drop(columns=['Unnamed: 0','Company','Inches','ScreenResolution','TypeName','Weight'],inplace=True)\ndf.head()\n\n# these feature are unique and does nothing in our predictions, So, we drop it !!","23f9ac9e":"# = = = = =  == = = =  = = =  = = = =  = = = = = = = = = =  == = = = =  = = =  = =  = = = = = = = =  = = = = == =  = ==  \n\ndf.reset_index(drop=True,inplace=True) # it is important do it anyways before encoder, then you will get no error\n\n\n# = = = = = = = = = = = == = = = = = =  == = = = = =  == = =  = = = = = = = = = == = = = = = = = = = = = = = = = = = ==\n\n# HELLO ! I AM HIGHLIGHTED !! LOL\n\n# This will reset your index, so that, if there is any situation where you index got skipped,\n# Then, it will again make it a continous sequence of no.s","a00a42e6":"df.info()","827aa637":"col = [feature for feature in df.columns if df[feature].dtype == 'O']\n\n# Using List comprehension for extracting categorical column names ","09d1ffb5":"for feature in col:\n    labels_ordered= df.groupby([feature])['Price_euros'].mean().sort_values().index\n    labels_ordered={k:i for i,k in enumerate(labels_ordered,0)}\n    df[feature]=df[feature].map(labels_ordered)","73cad618":"df.head()","581c6663":"y = df['Price_euros']\nX = df.drop(columns='Price_euros')","6e9f103d":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler() # setting with_mean False, is for a reason !\n\ntemp = scaler.fit_transform(X)\nX = pd.DataFrame(temp)","646f3046":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42,shuffle = True)","1d34331e":"from sklearn.linear_model import LinearRegression\nlr = LinearRegression() ","03005035":"lr.fit(X_train,y_train)","22826c7e":"lr.score(X_train,y_train) # train-set score","3a8a6bf6":"lr.score(X_test,y_test) # test-set score","11d6ea30":"y_pred_lr = lr.predict(X_test) # prediction ","0bea2386":"from sklearn.ensemble import RandomForestRegressor\nrfr = RandomForestRegressor(random_state=1)","a84d6228":"rfr.fit(X_train,y_train)","ecef5126":"rfr.score(X_train,y_train) # train-set score ","5cef801b":"rfr.score(X_test,y_test) # test-set score","5675ef53":"y_pred_rfr = rfr.predict(X_test) #prediction","211bd4cc":"from sklearn.metrics import mean_absolute_error","48333be1":"mae = mean_absolute_error(y_pred_lr,y_test)\nmean = y_test.mean()\npercentage_mae = mae\/mean*100\npercentage_mae\n\n# we are having a mean_absolute_error of 13.36%","5db12703":"mae = mean_absolute_error(y_pred_rfr,y_test)\nmean = y_test.mean()\npercentage_mae = mae\/mean*100\npercentage_mae\n\n# we are having a mean_absolute_error of 13.36%","25bea294":"# Linear Regression","7560abc6":"# Random Forest Regressor","de8deaaa":"# Feature Engineering","408bdfea":"# Changing catigorical Feature to labels","29aadd76":"# Mean Absolute Error","2ca3fbb2":"# Scaling","e7357a24":"# Encoding\n\n#### I have not used OneHotEncoder here, as our dataset is having a lot of categorical features. Thus, when we encode them with OneHot, it will end up making a lot of features which will result your model to Overfit !!\n#### Trust me, I have used that before, got score of test data in negative !! Isn't it funny!\n> Working of below code\n* We are going to assign each and every value of a categorical feature to a no.\n* This assignment of no.s will be done by how much it worths as target feature\n* ..................................................................................................................................\n* we are going to group dataset by feature.\n* Then, we will take out mean of target feature('Price_euros') of the grouped data.\n* Now, Sort the values and get the index.\n* ..................................................................................................................................\n* Now, make a dictionary, and assign index, to value 0 to n.\n* So, Now we can actually get changes to our original dataset"}}