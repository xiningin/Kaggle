{"cell_type":{"9a0f65fe":"code","724cfd96":"code","940f22a8":"code","6fb31218":"code","33a0690b":"code","10839685":"code","b4005d75":"code","6e3293dc":"code","76be9180":"code","bf3e9570":"code","94b4381d":"code","05007d41":"code","11ae0480":"code","4afb3133":"code","408fa098":"code","de2c0993":"code","87765d89":"code","a3a9b226":"code","6511281f":"code","4c85b4cb":"code","6602d85d":"markdown","bef92cbc":"markdown","ec823c47":"markdown","54aac151":"markdown"},"source":{"9a0f65fe":"# -*- coding: utf-8 -*-\n\nimport tensorflow as tf\nimport numpy as np\nimport urllib\nimport tarfile\nimport os\nfrom tqdm import tqdm_notebook\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom imageio import imread, imsave, mimsave\nfrom PIL import Image\nimport glob\nimport shutil","724cfd96":"import numpy as np # linear algebra\nimport xml.etree.ElementTree as ET # for parsing XML\nimport matplotlib.pyplot as plt # to show images\nfrom PIL import Image # to read images\nimport os\nimport glob\n\nroot_images=\"..\/input\/all-dogs\/all-dogs\/\"\nroot_annots=\"..\/input\/annotation\/Annotation\/\"","940f22a8":"all_images=os.listdir(\"..\/input\/all-dogs\/all-dogs\/\")\nprint(f\"Total images : {len(all_images)}\")\n\nbreeds = glob.glob('..\/input\/annotation\/Annotation\/*')\nannotation=[]\nfor b in breeds:\n    annotation+=glob.glob(b+\"\/*\")\nprint(f\"Total annotation : {len(annotation)}\")\n\nbreed_map={}\nfor annot in annotation:\n    breed=annot.split(\"\/\")[-2]\n    index=breed.split(\"-\")[0]\n    breed_map.setdefault(index,breed)\n    \nprint(f\"Total Breeds : {len(breed_map)}\")","6fb31218":"def bounding_box(image):\n    bpath=root_annots+str(breed_map[image.split(\"_\")[0]])+\"\/\"+str(image.split(\".\")[0])\n    tree = ET.parse(bpath)\n    root = tree.getroot()\n    objects = root.findall('object')\n    for o in objects:\n        bndbox = o.find('bndbox') # reading bound box\n        xmin = int(bndbox.find('xmin').text)\n        ymin = int(bndbox.find('ymin').text)\n        xmax = int(bndbox.find('xmax').text)\n        ymax = int(bndbox.find('ymax').text)\n        \n    return (xmin,ymin,xmax,ymax)\n\ndef get_crop_image(image):\n    bbox=bounding_box(image)\n    im=Image.open(os.path.join(root_images,image))\n    im=im.crop(bbox)\n    return im","33a0690b":"%%time\nplt.figure(figsize=(10,10))\nfor i,image in enumerate(all_images):\n    im=get_crop_image(image)\n    \n    plt.subplot(3,3,i+1)\n    plt.axis(\"off\")\n    plt.imshow(im)    \n    if(i==8):\n        break","10839685":"%%time\npath = '..\/input\/all-dogs'\ndataset = 'all-dogs'\ndata_path = os.path.join(path, dataset)\nimages = glob.glob(os.path.join(data_path, '*.*')) \nprint(len(images))","b4005d75":"images[0]","6e3293dc":"z_dim = 1000\n\nWIDTH = 64\nHEIGHT = 64\n\nOUTPUT_DIR = 'samples_dogs'\nif not os.path.exists(OUTPUT_DIR):\n    os.mkdir(OUTPUT_DIR)\n\nGEN_DIR = 'generated_dogs'\nif not os.path.exists(GEN_DIR):\n    os.mkdir(GEN_DIR)\n    \nX = tf.placeholder(dtype=tf.float32, shape=[None, HEIGHT, WIDTH, 3], name='X')\nnoise = tf.placeholder(dtype=tf.float32, shape=[None, z_dim], name='noise')\nis_training = tf.placeholder(dtype=tf.bool, name='is_training')\n\ndef lrelu(x, leak=0.2):\n    return tf.maximum(x, leak * x)\n\ndef sigmoid_cross_entropy_with_logits(x, y):\n    return tf.nn.sigmoid_cross_entropy_with_logits(logits=x, labels=y)","76be9180":"def discriminator(image, reuse=None, is_training=is_training):\n    momentum = 0.9\n    with tf.variable_scope('discriminator', reuse=reuse):\n        h0 = lrelu(tf.layers.conv2d(image, kernel_size=5, filters=64, strides=2, padding='same'))\n        \n        h1 = tf.layers.conv2d(h0, kernel_size=5, filters=128, strides=2, padding='same')\n        h1 = lrelu(tf.layers.batch_normalization(h1, training=is_training, momentum=momentum))\n        \n        h2 = tf.layers.conv2d(h1, kernel_size=5, filters=256, strides=2, padding='same')\n        h2 = lrelu(tf.layers.batch_normalization(h2, training=is_training, momentum=momentum))\n        \n        h3 = tf.layers.conv2d(h2, kernel_size=5, filters=512, strides=2, padding='same')\n        h3 = lrelu(tf.layers.batch_normalization(h3, training=is_training, momentum=momentum))\n\n        h4 = tf.layers.flatten(h3)\n        h4 = tf.layers.dense(h4, units=1)\n        return tf.nn.sigmoid(h4), h4","bf3e9570":"def generator(z, is_training=is_training):\n    momentum = 0.9\n    with tf.variable_scope('generator', reuse=None):\n        d = 4\n        h0 = tf.layers.dense(z, units=d * d * 512)\n        h0 = tf.reshape(h0, shape=[-1, d, d, 512])\n        h0 = tf.nn.relu(tf.layers.batch_normalization(h0, training=is_training, momentum=momentum))\n        \n        h1 = tf.layers.conv2d_transpose(h0, kernel_size=5, filters=256, strides=2, padding='same')\n        h1 = tf.nn.relu(tf.layers.batch_normalization(h1, training=is_training, momentum=momentum))\n        \n        h2 = tf.layers.conv2d_transpose(h1, kernel_size=5, filters=128, strides=2, padding='same')\n        h2 = tf.nn.relu(tf.layers.batch_normalization(h2, training=is_training, momentum=momentum))\n        \n        h3 = tf.layers.conv2d_transpose(h2, kernel_size=5, filters=64, strides=2, padding='same')\n        h3 = tf.nn.relu(tf.layers.batch_normalization(h3, training=is_training, momentum=momentum))\n        \n        h4 = tf.layers.conv2d_transpose(h3, kernel_size=5, filters=3, strides=2, padding='same', activation=tf.nn.tanh, name='g')\n        return h4","94b4381d":"g = generator(noise)\nd_real, d_real_logits = discriminator(X)\nd_fake, d_fake_logits = discriminator(g, reuse=True)\n\nvars_g = [var for var in tf.trainable_variables() if var.name.startswith('generator')]\nvars_d = [var for var in tf.trainable_variables() if var.name.startswith('discriminator')]\n\nloss_d_real = tf.reduce_mean(sigmoid_cross_entropy_with_logits(d_real_logits, tf.ones_like(d_real)))\nloss_d_fake = tf.reduce_mean(sigmoid_cross_entropy_with_logits(d_fake_logits, tf.zeros_like(d_fake)))\nloss_g = tf.reduce_mean(sigmoid_cross_entropy_with_logits(d_fake_logits, tf.ones_like(d_fake)))\nloss_d = loss_d_real + loss_d_fake","05007d41":"update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\nwith tf.control_dependencies(update_ops):\n    optimizer_d = tf.train.AdamOptimizer(learning_rate=0.0002, beta1=0.5).minimize(loss_d, var_list=vars_d)\n    optimizer_g = tf.train.AdamOptimizer(learning_rate=0.0002, beta1=0.5).minimize(loss_g, var_list=vars_g)","11ae0480":"def read_image(image_name, height, width):\n    image = get_crop_image(image_name)\n    \n    h = image.size[0]\n    w = image.size[1]\n    \n    image = np.array(image.resize((height, width)))\n    return image \/ 255.","4afb3133":"def montage(images):    \n    if isinstance(images, list):\n        images = np.array(images)\n    img_h = images.shape[1]\n    img_w = images.shape[2]\n    n_plots = int(np.ceil(np.sqrt(images.shape[0])))\n    if len(images.shape) == 4 and images.shape[3] == 3:\n        m = np.ones(\n            (images.shape[1] * n_plots + n_plots + 1,\n             images.shape[2] * n_plots + n_plots + 1, 3)) * 0.5\n    elif len(images.shape) == 4 and images.shape[3] == 1:\n        m = np.ones(\n            (images.shape[1] * n_plots + n_plots + 1,\n             images.shape[2] * n_plots + n_plots + 1, 1)) * 0.5\n    elif len(images.shape) == 3:\n        m = np.ones(\n            (images.shape[1] * n_plots + n_plots + 1,\n             images.shape[2] * n_plots + n_plots + 1)) * 0.5\n    else:\n        raise ValueError('Could not parse image shape of {}'.format(images.shape))\n    for i in range(n_plots):\n        for j in range(n_plots):\n            this_filter = i * n_plots + j\n            if this_filter < images.shape[0]:\n                this_img = images[this_filter]\n                m[1 + i + i * img_h:1 + i + (i + 1) * img_h,\n                  1 + j + j * img_w:1 + j + (j + 1) * img_w] = this_img\n    return m","408fa098":"batch_size = 4\nepochs = 500000","de2c0993":"# Tensorflow is dead when running a little time.\nsv = tf.train.Supervisor()\nsaver = sv.saver\nwith sv.managed_session() as sess:\n    z_samples = np.random.uniform(-1.0, 1.0, [batch_size, z_dim]).astype(np.float32)\n    samples = []\n    loss = {'d': [], 'g': []}\n\n    offset = 0\n    for i in tqdm_notebook(range(epochs)):\n        n = np.random.uniform(-1.0, 1.0, [batch_size, z_dim]).astype(np.float32)\n\n        offset = (offset + batch_size) % len(images)\n        batch = np.array([read_image(img, HEIGHT, WIDTH) for img in all_images[offset: offset + batch_size]])\n        batch = (batch - 0.5) * 2\n\n        d_ls, g_ls = sess.run([loss_d, loss_g], feed_dict={X: batch, noise: n, is_training: True})\n        loss['d'].append(d_ls)\n        loss['g'].append(g_ls)\n\n        sess.run(optimizer_d, feed_dict={X: batch, noise: n, is_training: True})\n        sess.run(optimizer_g, feed_dict={X: batch, noise: n, is_training: True})\n        sess.run(optimizer_g, feed_dict={X: batch, noise: n, is_training: True})\n\n        if i % 10000 == 0:\n            print(i, d_ls, g_ls)\n            gen_imgs = sess.run(g, feed_dict={noise: z_samples, is_training: False})\n            gen_imgs = (gen_imgs + 1) \/ 2\n            imgs = [img[:, :, :] for img in gen_imgs]\n            gen_imgs = montage(imgs)\n            plt.axis('off')\n            plt.imshow(gen_imgs)\n            plt.show()\n\n    plt.plot(loss['d'], label='Discriminator')\n    plt.plot(loss['g'], label='Generator')\n    plt.legend(loc='upper right')\n    plt.show()\n    \n    graph = tf.get_default_graph()\n    g = graph.get_tensor_by_name('generator\/g\/Tanh:0')\n    noise = graph.get_tensor_by_name('noise:0')\n    is_training = graph.get_tensor_by_name('is_training:0')\n\n    n = np.random.uniform(-1.0, 1.0, [batch_size, z_dim]).astype(np.float32)\n    gen_imgs = sess.run(g, feed_dict={noise: n, is_training: False})\n    gen_imgs = (gen_imgs + 1) \/ 2\n    imgs = [img[:, :, :] for img in gen_imgs]\n    gen_imgs = montage(imgs)\n    gen_imgs = np.clip(gen_imgs, 0, 1)\n    plt.figure(figsize=(8, 8))\n    plt.axis('off')\n    plt.imshow(gen_imgs)\n    plt.show()\n    \n    # Store data\n    n_batches = 10000 \/\/ batch_size\n    last_batch_size = 10000 % batch_size\n\n    for i in tqdm_notebook(range(n_batches)):\n        n = np.random.uniform(-1.0, 1.0, [batch_size, z_dim]).astype(np.float32)\n        gen_imgs = sess.run(g, feed_dict={noise: n, is_training: False})\n        gen_imgs = (gen_imgs + 1) \/ 2\n        for j in range(batch_size):\n            imsave(os.path.join(GEN_DIR, f'sample_{i}_{j}.png'), gen_imgs[j])\n    \n    for i in range(last_batch_size):\n        n = np.random.uniform(-1.0, 1.0, [batch_size, z_dim]).astype(np.float32)\n        gen_imgs = sess.run(g, feed_dict={noise: n, is_training: False})\n        gen_imgs = (gen_imgs + 1) \/ 2\n        imsave(os.path.join(GEN_DIR, f'sample_{n_batches}_{i}.png'), gen_imgs[i])","87765d89":"%%time\nshutil.make_archive('images', 'zip', GEN_DIR)","a3a9b226":"len(os.listdir(GEN_DIR))","6511281f":"!rm -rf generated_dogs\/\n!rm -rf samples_dogs\/","4c85b4cb":"!ls -l .","6602d85d":"# 2 DCGAN","bef92cbc":"# 1 Crop image","ec823c47":"The script is based on the book **deep interesing**","54aac151":"The code below is based on [the amazing script.](https:\/\/www.kaggle.com\/whizzkid\/crop-images-using-bounding-box)"}}