{"cell_type":{"52225cfd":"code","11f73e81":"code","46ac43e0":"code","5857f1fc":"code","aeb2a0f6":"code","07d10348":"code","07791efc":"code","dcb45369":"code","e714efa1":"code","2f980e36":"code","ca9d0008":"code","b1f7fabe":"code","383a13e1":"code","ee5edd4e":"code","34e9393a":"code","4a9e38fb":"code","8815e159":"code","5df059f6":"code","ddf1fa32":"code","f8b1f3d3":"code","45fd7e50":"code","d10b7f98":"code","9a4d35ee":"code","530cfa3e":"code","67877d82":"code","43688ded":"code","232089e5":"code","8d574be7":"code","84c1ef08":"code","93650a40":"code","b52fa181":"code","8230e3f2":"code","e5d5f4bf":"code","5c97efac":"code","a17821d0":"code","ffbe3d7f":"code","7e623882":"code","1e20a022":"code","e5a3e8d3":"code","082945b7":"code","0175906f":"code","82db79ae":"code","0823b6c1":"code","f73f398c":"code","b2832c50":"code","264addae":"code","9685ba94":"code","344a3010":"code","2c8c8c14":"code","f42bbfd1":"code","0b4a6734":"code","0407011e":"code","4af36bc0":"code","eb603abd":"code","5cfde841":"markdown","7c6f8d70":"markdown","a96c4c30":"markdown","ff0a300f":"markdown","767934cd":"markdown","d273b63e":"markdown","073353cb":"markdown","7974dcb2":"markdown","db0582f3":"markdown","2c1d1fcf":"markdown","03220169":"markdown","0f8f0ae3":"markdown","6e343744":"markdown","396ace3f":"markdown"},"source":{"52225cfd":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","11f73e81":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport collections\nimport re, string\nimport sys\nimport time\nfrom nltk.corpus import stopwords\nfrom wordcloud import WordCloud\nfrom mpl_toolkits.basemap import Basemap\n\nfrom subprocess import check_output\n%matplotlib inline","46ac43e0":"import json\n\n\ndef read_json(file):\n    def init_ds(json):\n        ds= {}\n        keys = json.keys()\n        for k in keys:\n            ds[k]= []\n        return ds, keys\n\n    dataset = {}\n    keys = []\n    with open(file) as file_lines:\n        for count, line in enumerate(file_lines):\n            data = json.loads(line.strip())\n            if count ==0:\n                dataset, keys = init_ds(data)\n            for k in keys:\n                dataset[k].append(data[k])\n                \n        return pd.DataFrame(dataset)","5857f1fc":"%%time\n\nyelp_business= read_json('..\/input\/yelp-dataset\/yelp_academic_dataset_business.json')","aeb2a0f6":"yelp_business.head()","07d10348":"# distribution of ratings\nx=yelp_business['stars'].value_counts()\nx=x.sort_index()\n#plot\nfig = plt.figure(figsize=(10,5))\nax= sns.barplot(x.index, x.values, alpha=0.8)\nplt.title(\"Distribution of Star Ratings\")\nplt.ylabel('number of businesses', fontsize=12)\nplt.xlabel('Star Ratings ', fontsize=12)\n\n#adding the text labels\nrects = ax.patches\nlabels = x.values\nfor rect, label in zip(rects, labels):\n    height = rect.get_height()\n    ax.text(rect.get_x() + rect.get_width()\/2, height + 5, label, ha='center', va='bottom')\n#ax.set_facecolor('#269cbc')\n#fig.patch.set_facecolor('#269cbc')\nfig.savefig('stars_general.png', bbox_inches='tight')\nplt.show()\n","07791efc":"#popular business categories\n\nbusiness_cats=' '.join(yelp_business['categories'].dropna())\n\ncats=pd.DataFrame(business_cats.split(','),columns=['category'])\nx=cats.category.value_counts()\nprint(\"There are \",len(x),\" different types\/categories of Businesses in Yelp!\")\n#prep for chart\nx=x.sort_values(ascending=False)\nx=x.iloc[0:20]\n\nfig = plt.figure(figsize=(10,4))\nax = sns.barplot(x.index, x.values, alpha=0.8)#,color=color[5])\nplt.title(\"Top categories\",fontsize=25)\nlocs, labels = plt.xticks()\nplt.setp(labels, rotation=80)\nplt.ylabel('Number of businesses', fontsize=12)\nplt.xlabel('Category', fontsize=12)\n\n#adding the text labels\nrects = ax.patches\nlabels = x.values\nfor rect, label in zip(rects, labels):\n    height = rect.get_height()\n    ax.text(rect.get_x() + rect.get_width()\/2, height + 5, label, ha='center', va='bottom')\n\nplt.show()\nfig.savefig('popular_categories2.png', bbox_inches='tight')","dcb45369":"%%time\n\n#yelp_business= read_json('..\/input\/yelp-dataset\/yelp_academic_dataset_business.json')\n#restaurants = yelp_business[yelp_business['categories'].str.contains('Restaurant') == True]\nmedical = yelp_business[yelp_business['categories'].str.contains('Medical') == True]\ndel(yelp_business)","e714efa1":"medical.describe()","2f980e36":"print('Median and average review count',medical['review_count'].median(), medical['review_count'].mean())\n#print('Median over 1000: ', medical[medical['review_count'] >1000]['review_count'].median())\nplt.figure(figsize = (10,10))\nsns.barplot(medical[medical['review_count'] >320]['review_count'],medical[medical['review_count'] >320]['name'],\n           palette = 'summer')\nplt.xlabel('')\nplt.title('Top review count');\nplt.savefig(\"top_reviewed_places.png\", bbox_inches='tight')","ca9d0008":"# distribution of ratings in medical and healt care\nx=medical['stars'].value_counts()\nx=x.sort_index()\n#plot\nplt.figure(figsize=(10,5))\nax= sns.barplot(x.index, x.values, alpha=0.8)\nplt.title(\"Average Star Ratings in Medical and Health care\")\nplt.ylabel('number of businesses', fontsize=12)\nplt.xlabel('star ratings ', fontsize=12)\n\n#adding the text labels\nrects = ax.patches\nlabels = x.values\nfor rect, label in zip(rects, labels):\n    height = rect.get_height()\n    ax.text(rect.get_x() + rect.get_width()\/2, height + 5, label, ha='center', va='bottom')\n#ax.set_facecolor((1.0, 0.47, 0.42))\nplt.show()","b1f7fabe":"%%time\n\nyelp_review= read_json('..\/input\/yelp-dataset\/yelp_academic_dataset_review.json')\n","383a13e1":"#review_restaurants = yelp_review[yelp_review.business_id.isin(restaurants['business_id']) == True]\n\nreviews = yelp_review[yelp_review.business_id.isin(medical['business_id']) == True]\ndel(yelp_review)","ee5edd4e":"# reviews =review_restaurants.drop(['review_id','user_id','business_id','date','useful','funny','cool'],axis=1)\nreviews = reviews.drop(['review_id','user_id','business_id','date','useful','funny','cool'],axis=1)\nreviews.head(10)\nreviews.describe()\n","34e9393a":"reviews.count()","4a9e38fb":"reviews['stars'].value_counts()\n","8815e159":"cloud_texts = ' '.join(reviews['text'][:10].astype(str))\nstopwords = {'one','some','is','a','at','is','he','back', 'if', 'the'}\ncloud_texts  = ' '.join([word for word in re.split(\"\\W+\",cloud_texts) if word.lower() not in stopwords])","5df059f6":"cloud = WordCloud(width=1440, height= 1080,max_words= 200, background_color='white').generate(cloud_texts)","ddf1fa32":"plt.figure(figsize=(20, 15))\nplt.imshow(cloud)\nplt.axis('off');\nplt.savefig(\"cloud_text.png\", bbox_inches='tight')","f8b1f3d3":"# distribution of ratings in reviews on medical and healt care\nx=reviews['stars'].value_counts()\nx=x.sort_index()\n#plot\nplt.figure(figsize=(10,5))\nax= sns.barplot(x.index, x.values, alpha=0.8)\nplt.title(\"Distribution of Star Ratings in Medical and Health care reviews\")\nplt.ylabel('number of reviews', fontsize=12)\nplt.xlabel('star ratings ', fontsize=12)\n\n#adding the text labels\nrects = ax.patches\nlabels = x.values\nfor rect, label in zip(rects, labels):\n    height = rect.get_height()\n    ax.text(rect.get_x() + rect.get_width()\/2, height + 5, label, ha='center', va='bottom')\n\nplt.show()\n\n#sns.countplot(reviews.stars)","45fd7e50":"reviews.count()","d10b7f98":"# reviews = reviews[reviews.stars!=3]\n# reviews['label'] = reviews['stars'].apply(lambda x: 1 if x>3 else 0)\n# reviews = reviews.drop('stars',axis=1)\n\n# negative\/neutral\/positive\nreviews = reviews[reviews['stars'].isin([1,3,5])]\nreviews['label'] = reviews['stars'].apply(lambda x: 0 if x==1 else x)\nreviews['label'] = reviews['label'].apply(lambda x: 1 if x==3 else x)\nreviews['label'] = reviews['label'].apply(lambda x: 2 if x==5 else x)\n\nprint(reviews['stars'].value_counts())","9a4d35ee":"from sklearn.utils import shuffle\nreviews2 = reviews.groupby('label')[['text', 'label']].apply(lambda s: s.sample(10000))\nreviews2.reset_index(drop=True, inplace=True)\nreviews2 = shuffle(reviews2)\n#reviews2 = reviews[:100000]","530cfa3e":"print(reviews2.head())\nprint(reviews2['label'].value_counts())","67877d82":"neg_reviews = reviews2[reviews2['label'] == 0].sample(1) \nprint(\"Negative reviews:\", neg_reviews['text'].values)\nneutr_reviews = reviews2[reviews2['label'] == 1].sample(1) \nprint(\"Neutral reviews:\", neutr_reviews['text'].values)\npos_reviews = reviews2[reviews2['label'] == 2].sample(1) \nprint(\"Positive reviews:\", pos_reviews['text'].values)","43688ded":"import pandas as pd,numpy as np,seaborn as sns\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.utils import to_categorical\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport spacy\nfrom keras.layers import Bidirectional,GlobalMaxPool1D,Conv1D\nfrom keras.layers import LSTM,Input,Dense,Dropout,Activation\nfrom keras.models import Model","232089e5":"texts = reviews2[\"text\"].values\nlabels = reviews2[\"label\"].values","8d574be7":"texts.shape, labels.shape","84c1ef08":"max_num_words = 1000\ntokenizer = Tokenizer(num_words=max_num_words)\ntokenizer.fit_on_texts(texts)\nsequences = tokenizer.texts_to_sequences(texts)\nword_index = tokenizer.word_index\nlen(word_index)","93650a40":"#print(tokenizer.word_counts)\n# number of input texts\n# print(tokenizer.document_count)\n# print(tokenizer.word_index)\n# print(tokenizer.word_docs)","b52fa181":"seq_sample = tokenizer.texts_to_sequences([\"hello everybody, how are you doing SACSjk\"])\nprint(seq_sample)","8230e3f2":"lengths = np.array([len(i) for i in sequences])\nprint(\"Average lenngth:\", np.mean(lengths), \"Median of length:\", np.median(lengths))","e5d5f4bf":"from sklearn.model_selection import train_test_split\n# maximal sequence length based on the median of the review word lengths\nmax_seq_length = 80\ndata = pad_sequences(sequences, maxlen=max_seq_length, truncating='pre') #'post'\nlabels = to_categorical(np.asarray(labels))","5c97efac":"print(data.shape, labels.shape)","a17821d0":"print(len(texts[1]), texts[1])\nprint(len(data[0]), data[0])\ntext_sample = tokenizer.sequences_to_texts([data[1]])\nprint(text_sample)","ffbe3d7f":"X_train, X_test, Y_train, Y_test = train_test_split(data, labels, test_size=0.2)\nprint(X_train.shape)\nprint(Y_train.shape)\nprint(X_test.shape)\nprint(Y_test.shape)","7e623882":"unique_elements, counts_elements = np.unique(np.argmax(Y_train,axis=1), return_counts=True)\nprint(\"Frequency of unique values of Y_train:\",unique_elements, counts_elements)","1e20a022":"def read_glove_vecs(glove_file):\n    embedding_index = {}\n\n    with open(glove_file, 'r') as f:\n        for line in f:\n            values = line.split()\n            word = values[0]\n            coefs = np.asarray(values[1:],dtype='float32')\n            embedding_index[word] = coefs\n        \n    return embedding_index","e5a3e8d3":"embedding_index = read_glove_vecs('..\/input\/glove6b50dtxt\/glove.6B.50d.txt')\nprint('found word vecs: ',len(embedding_index))","082945b7":"def set_embedding_matrix(word_index, embedding_dim): \n    #embedding_dim = 50\n    embedding_matrix = np.zeros((len(word_index)+1,embedding_dim))\n    # embedding_matrix.shape \n    for word,i in word_index.items():\n        embedding_vector = embedding_index.get(word)\n        if embedding_vector is not None:\n            embedding_matrix[i] = embedding_vector\n    return embedding_matrix","0175906f":"embedding_dim = 50\nembedding_matrix = set_embedding_matrix(word_index, embedding_dim)","82db79ae":"print(max_seq_length)","0823b6c1":"from keras.layers import Embedding\nembedding_layer = Embedding(len(word_index)+1,embedding_dim,weights=[embedding_matrix],input_length=max_seq_length,trainable=False)","f73f398c":"from keras.layers import Bidirectional,GlobalMaxPooling1D\nfrom keras.layers import LSTM,Input,Dense,Dropout,Activation\nfrom keras.models import Model","b2832c50":"def LSTM_Model(max_seq_length):\n    inp = Input(shape=(max_seq_length,))\n    x = embedding_layer(inp)\n    x = Bidirectional(LSTM(64,return_sequences=True,dropout=0.1,recurrent_dropout=0.1))(x)    \n    x = GlobalMaxPooling1D()(x)    \n    x = Dense(64,activation='relu')(x)\n    x = Dropout(0.1)(x)\n    \n    # positive \/negative\n    #x = Dense(2,activation='sigmoid')(x) \n    # positive \/negative \/neutral \n    x = Dense(3,activation='sigmoid')(x)  \n    x = Activation('softmax')(x)\n    \n    model = Model(inputs=inp,outputs=x)\n    return model\n","264addae":"model = LSTM_Model(max_seq_length)\n#model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\nmodel.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\nmodel.summary()","9685ba94":"history = model.fit(X_train,Y_train,validation_data=(X_test,Y_test),epochs=20,batch_size=128);","344a3010":"import pickle\n# save the model to disk\nfilename_nn = 'nn_model.pkl'\npickle.dump(model, open(filename_nn, 'wb'))\n# load the model from disk\n#loaded_model = pickle.load(open(filename_nn, 'rb'))\n","2c8c8c14":"import matplotlib.pyplot as plt\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('Model accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.savefig(\"learning_curve.png\", bbox_inches='tight')\nplt.show()","f42bbfd1":"import numpy as np\n\n\ndef plot_confusion_matrix(cm,\n                          target_names,\n                          title='Confusion matrix',\n                          cmap=None,\n                          normalize=True):\n    \"\"\"\n    given a sklearn confusion matrix (cm), make a nice plot\n\n    Arguments\n    ---------\n    cm:           confusion matrix from sklearn.metrics.confusion_matrix\n\n    target_names: given classification classes such as [0, 1, 2]\n                  the class names, for example: ['high', 'medium', 'low']\n\n    title:        the text to display at the top of the matrix\n\n    cmap:         the gradient of the values displayed from matplotlib.pyplot.cm\n                  see http:\/\/matplotlib.org\/examples\/color\/colormaps_reference.html\n                  plt.get_cmap('jet') or plt.cm.Blues\n\n    normalize:    If False, plot the raw numbers\n                  If True, plot the proportions\n\n    Usage\n    -----\n    plot_confusion_matrix(cm           = cm,                  # confusion matrix created by\n                                                              # sklearn.metrics.confusion_matrix\n                          normalize    = True,                # show proportions\n                          target_names = y_labels_vals,       # list of names of the classes\n                          title        = best_estimator_name) # title of graph\n\n    Citiation\n    ---------\n    http:\/\/scikit-learn.org\/stable\/auto_examples\/model_selection\/plot_confusion_matrix.html\n\n    \"\"\"\n    import matplotlib.pyplot as plt\n    import numpy as np\n    import itertools\n\n    accuracy = np.trace(cm) \/ float(np.sum(cm))\n    misclass = 1 - accuracy\n\n    if cmap is None:\n        cmap = plt.get_cmap('Blues')\n\n    plt.figure(figsize=(8, 6))\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n\n    if target_names is not None:\n        tick_marks = np.arange(len(target_names))\n        plt.xticks(tick_marks, target_names, rotation=45)\n        plt.yticks(tick_marks, target_names)\n\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n\n\n    thresh = cm.max() \/ 1.5 if normalize else cm.max() \/ 2\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        if normalize:\n            plt.text(j, i, \"{:0.4f}\".format(cm[i, j]),\n                     horizontalalignment=\"center\",\n                     color=\"white\" if cm[i, j] > thresh else \"black\")\n        else:\n            plt.text(j, i, \"{:,}\".format(cm[i, j]),\n                     horizontalalignment=\"center\",\n                     color=\"white\" if cm[i, j] > thresh else \"black\")\n\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label\\naccuracy={:0.4f}; misclass={:0.4f}'.format(accuracy, misclass))\n    plt.show()","0b4a6734":"from sklearn.metrics import confusion_matrix\nimport itertools\npred = np.argmax(model.predict(X_test),axis=1)\ntrue_label = np.argmax(Y_test, axis=1)\n\n\ncm = confusion_matrix(y_true=true_label, y_pred=pred)\nplot_confusion_matrix(cm, target_names=['neg', 'neutr', 'pos'], normalize=True) #,cmap=plt.cm.Blues):","0407011e":"from sklearn.metrics import classification_report\nprint(classification_report(true_label, pred))","4af36bc0":"for i in range(100):\n    text_sample = tokenizer.sequences_to_texts([data[i]])\n    \n    #print(np.argmax(labels[i]))\n    sample = data[i].reshape(1,80)\n    #print(type(data[i]), data[i].shape)\n    true_label = np.argmax(labels[i])\n    pred_label = np.argmax(model.predict(sample))\n    if true_label != pred_label:\n        print(text_sample)\n        print(\"True label: \", true_label,\"Predicted label: \", pred_label)\n\n","eb603abd":"my_text_neg = \"In one word it was terrible.\"\nmy_text_neutral = \"Eventhough not very warm the doctor treated me nicely and there were no problems in the end\"\nmy_text_pos = \"Absolutely stunning experience! I am looking forward to next visit.\"\nfor my_text in [my_text_neg, my_text_neutral, my_text_pos]:\n    my_seq = tokenizer.texts_to_sequences([my_text])\n    print(\"Recontructed text: \", tokenizer.sequences_to_texts(my_seq))\n    my_seq = pad_sequences(my_seq, maxlen=max_seq_length, truncating='pre') \n    print(\"Predicted probabilities: \", model.predict(my_seq.reshape(1,80)))\n    print(\"Predicted label: \", np.argmax(model.predict(my_seq.reshape(1,80))))","5cfde841":"# Set the ML model","7c6f8d70":"# Train\/test split","a96c4c30":"# Plot model results","ff0a300f":"# Analysis of medical and healthcare reviews in yelp dataset","767934cd":"# Read business and review datasets","d273b63e":"# Wrongly predicted reviews","073353cb":"# Medical reviews ","7974dcb2":"# Text preprocessing","db0582f3":"# Popular business categories","2c1d1fcf":"# Distribution of star ratings","03220169":"1. filter medical and healthcare services from the table with bussineses and reviews\n2. LSTM based NN labeling reviews as negativ\/neutral\/positive learned on the star scores","0f8f0ae3":"# Load GloVe for word embeddings","6e343744":"# Label new reviews","396ace3f":"# Make positive, neutral and negative labels"}}