{"cell_type":{"6668631f":"code","096b27af":"code","2a328aa1":"code","990b51bc":"code","da81248a":"code","bca3aab4":"code","b822d2f3":"code","e6ed18b2":"code","1998f083":"code","fa5767e7":"code","58cfa394":"code","b07bc27f":"code","3db9d76b":"code","e420cbc6":"code","b3e90d61":"code","176bc274":"code","fdd76fa2":"code","73a20a62":"code","2c942e4e":"code","53105a02":"code","8e0c63eb":"code","5ef1412c":"code","84d6db71":"code","6840abe9":"code","7aaf71ef":"code","4977a155":"code","77d14606":"code","4e3bce26":"code","92131d0b":"code","0e1acb47":"code","98140d01":"code","0b749b3d":"code","f05af0b9":"code","d1126d75":"code","49c4672a":"code","2dad5d6e":"code","82fdb3b1":"code","c1f1f256":"code","5a369d2f":"code","74e983ad":"code","75e19faf":"code","6c34e630":"code","835c8510":"code","4e76e936":"code","07ae8424":"code","1c33d4d9":"code","fd35ee7d":"code","201e7be5":"code","831973d7":"code","0b83f513":"code","2963821c":"code","2066a5cc":"code","6198daa7":"code","09f2ec5e":"code","cc50fe17":"code","6a429e8a":"code","583cd53d":"code","4b4e8c78":"code","0ccb3465":"code","1c60e200":"code","9e2cc8c4":"code","3a669712":"code","3d129e51":"code","d0e29b9e":"code","67de0062":"code","7a783e08":"code","2996c9c9":"code","2f3fa4d1":"code","45cd3e11":"code","f237d293":"code","1e452197":"code","c8afbcdb":"code","0445a644":"code","f31b6806":"code","71099644":"code","84f47397":"code","e2775e38":"code","58c57883":"code","29937171":"code","4b699f22":"code","d3863d4e":"code","5592b246":"code","8c88ff6e":"code","442c10f1":"code","b83f43e5":"code","adcaaf9d":"code","470fb576":"code","dcc1c98f":"code","d526df3a":"code","d3bbe6c1":"code","7c80e7c2":"code","3026844b":"code","496ec477":"code","653f8511":"code","f2c1d0d8":"code","1775ccce":"code","f42d973e":"code","1f3e4c8e":"code","b2a13303":"code","58657766":"code","3aca7404":"code","0f6c1775":"code","8680fc5a":"code","ea950808":"code","285dc1aa":"code","f5251ebb":"code","350aa030":"code","fa68d977":"code","3d9a0fc6":"code","4633d6d4":"code","6dd66d0b":"code","4a3da191":"code","0a3d0e07":"code","2adfd056":"code","78e530d2":"code","7b566444":"code","331a3631":"code","a499c413":"markdown","1cacfe9b":"markdown","e66c25ef":"markdown","d4a92559":"markdown","f9f1feae":"markdown","8b033dad":"markdown","d653ec47":"markdown","963a049b":"markdown","e87ea16c":"markdown","76b8f67b":"markdown","49905b5e":"markdown","2ca2de5e":"markdown","1bc316f2":"markdown","dd134709":"markdown","33c563d6":"markdown","11802f0f":"markdown","931dd301":"markdown","b9d0ce2b":"markdown","30557e00":"markdown","bcb1e2f0":"markdown","e38e5405":"markdown","704f7a8a":"markdown","dd885d5f":"markdown","67ef0dd7":"markdown","2259d8f7":"markdown","44e8645d":"markdown","aa34def1":"markdown","378b0840":"markdown","f4e7ffbe":"markdown","f9cdd60f":"markdown"},"source":{"6668631f":"import numpy as np\nimport pandas as pd\nimport datetime\nimport matplotlib\nimport matplotlib.pyplot as plt\nfrom matplotlib import colors\nimport plotly.express as px\nimport seaborn as sns\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\n#from yellowbrick.cluster import KElbowVisualizer\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt, numpy as np\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom sklearn.cluster import AgglomerativeClustering\nfrom matplotlib.colors import ListedColormap\nfrom sklearn import metrics","096b27af":"df = pd.read_csv('..\/input\/customer-personality-analysis\/marketing_campaign.csv',sep=\"\\t\")\ndf","2a328aa1":"\n# Exploratory Data Analysis\ndef libraries():\n    global pd,np\n    import pandas as pd\n    import numpy as np\ndef load():\n    global df\n    df=pd.read_csv('..\/input\/customer-personality-analysis\/marketing_campaign.csv',sep=\"\\t\")\n    \ndef top_rows(value):\n    print('\\033[1m'+ 'displaying the', value, 'rows from top'+'\\033[0m')\n    a=df.head(value)\n    print(a,'\\n')\n    \ndef bottom_rows(value):\n    print('\\033[1m'+'displaying the', value, 'rows from bottom'+'\\033[0m')\n    b=df.tail(value)\n    print(b,'\\n')\n    \ndef rows_columns():\n    print('\\033[1m'+'Shape of the Data set'+'\\033[0m')\n    c=df.shape\n    print(c,'\\n')\n    \ndef col_names():\n    print('\\033[1m'+'Column Names in the Data set'+'\\033[0m')\n    d=df.columns\n    print(d,'\\n')\n    \ndef information():\n    print('\\033[1m'+'Quick Overview of DataSet(info)'+'\\033[0m')\n    e = df.info()\n    print(e,'\\n')\n\ndef sizee():\n    print('\\033[1m'+'No.of Elements in the DataSet'+'\\033[0m')\n    f = df.size\n    print(f,'\\n')\n\ndef ndimension():\n    print('\\033[1m'+'Dimensions in your dataframe'+'\\033[0m')\n    g = df.ndim\n    print(g,'\\n')\n    \ndef stats_summary():\n    print('\\033[1m'+'Staistical Summary of DataSet'+'\\033[0m')\n    h = df.describe()\n    print(h,'\\n')\n    \ndef null_values():\n    print('\\033[1m'+'Number of Missing values in each column'+'\\033[0m')\n    i = df.isnull().sum()\n    print(i,'\\n')\n    \ndef n_unique():\n    print('\\033[1m'+'Number of unique elements'+'\\033[0m')\n    j = df.nunique()\n    print(j,'\\n')\n    \ndef memory_use():\n    print('\\033[1m'+'Memory used by all colomns in bytes'+'\\033[0m')\n    k = df.memory_usage()\n    print(k,'\\n')\n    \ndef is_na(value):\n    print('\\033[1m'+'Dataframe filled with boolean values with true indicating missing values'+'\\033[0m')\n    l = df.isna().head(value)\n    print(l,'\\n')\n    \ndef duplicate():\n    print('\\033[1m'+'Boolean Series denoting duplicate rows'+'\\033[0m')\n    m = df.duplicated().sum()\n    print(m,'\\n')\n    \ndef valuecounts():\n    print('\\033[1m'+'Series containing count of unique values'+'\\033[0m')\n    n = df.value_counts()\n    print(n,'\\n')\n\ndef datatypes():\n    print('\\033[1m'+'Datatype of each column'+'\\033[0m')\n    o = df.dtypes\n    print(o,'\\n')\n    \ndef correlation():\n    print('\\033[1m'+'Correalation between all columns in DataFrame'+'\\033[0m')\n    p = df.corr()\n    print(p,'\\n')\n    \ndef nonnull_count():\n    print('\\033[1m'+'Count of non-null values'+'\\033[0m')\n    q = df.count()\n    print(q,'\\n')\n    \ndef eda():\n    load()\n    value= 5 \n    datatypes()\n    top_rows(value)\n    bottom_rows(value)\n    rows_columns()\n    col_names()\n    information()\n    sizee()\n    ndimension()\n    stats_summary()\n    null_values()\n    n_unique()\n    memory_use()\n    is_na(value)\n    nonnull_count()\n    duplicate()\n    valuecounts()\n    correlation()\n    \n    \n    \n        \ndef stats_u(data,col):\n    if data[col].dtype == \"float64\":\n        print(col,\"has Quantitative data\")\n        mean_value=data[col].mean()\n        print('mean of',col,'column',mean_value)\n        max_value = data[col].max()\n        print('Maximum value of',col,'column',max_value)\n        min_value = data[col].min()\n        print('Minimum value of',col,'column',min_value)\n        median_value = data[col].median(skipna = True)\n        print('median of',col,'column',median_value)\n        std_value = data[col].std()\n        print('standard deviation of',col,'column',std_value)\n        q1 = data[col].quantile(0.25,interpolation='nearest')\n        print('quartile 1 of',col,'column is',q1)\n        q2 = data[col].quantile(0.5,interpolation='nearest')\n        print('quartile 2 of',col,'column is',q2)\n        q3 = data[col].quantile(0.75,interpolation='nearest')\n        print('quartile 3 of',col,'column is',q3)\n        q4 = data[col].quantile(1,interpolation='nearest')\n        print('quartile 4 of',col,'column is',q4)\n        IQR = q3 -q1\n        LLP = q1 - 1.5*IQR\n        ULP = q3 + 1.5*IQR\n        print('Lower Limit Point:',LLP)\n        print('Upper Limit Point:',ULP)\n        if data[col].min() > LLP and data[col].max() < ULP:\n            print(\"No outliers\")\n        else:\n            print(\"There are outliers\")\n            print(data[data[col]<LLP][col])\n            print(data[data[col]>ULP][col])\n            \n    elif data[col].dtype == \"int64\":\n        print(col,\"has Quantitative data\")\n        mean_value=data[col].mean()\n        print('mean of',col,'column',mean_value)\n        median_value = data[col].median(skipna = True)\n        print('median of',col,'column',median_value)\n        std_value = data[col].std()\n        print('standard deviation of',col,'column',std_value)\n        q1 = data[col].quantile(0.25,interpolation='nearest')\n        print('quartile 1 of',col,'column is',q1)\n        q2 = data[col].quantile(0.5,interpolation='nearest')\n        print('quartile 2 of',col,'column is',q2)\n        q3 = data[col].quantile(0.75,interpolation='nearest')\n        print('quartile 3 of',col,'column is',q3)\n        q4 = data[col].quantile(1,interpolation='nearest')\n        print('quartile 4 of',col,'column is',q4)\n        IQR = q3 -q1\n        LLP = q1 - 1.5*IQR\n        ULP = q3 + 1.5*IQR\n        print('Lower Limit Point:',LLP)\n        print('Upper Limit Point:',ULP)\n        if data[col].min() > LLP and data[col].max() < ULP:\n            print(\"No outliers\")\n        else:\n            print(\"There are outliers\")\n            print(\"Outliers are:\")\n            print(data[data[col]<LLP][col])\n            print(data[data[col]>ULP][col])\n    else:\n        print(col,'has Qualitative Data')\n        z = df[col].mode()\n        print('mode of',col,'column:\\n',z)\n        print('Count of mode is:\\n',df[col].value_counts())\n        print('Unique strings in',col,'are',data[col].nunique())\n        if(data[col].nunique() == 1):\n            print(col,'has same string')\n        elif(data[col].nunique() == 2):\n            print(col,'has binary strings')\n        else:\n            print(col,'has multi stings')\n\n\nlibraries()\neda()\n\nprint(\"----------------------------------------------------------------------------------------------------------------------\")\nprint('\\033[1m'+'Summary Of DataSet'+'\\033[0m')\nprint('\\033[1m'+'DataTypes in the DataSet:\\n'+'\\033[0m',df.dtypes)\nprint('\\033[1m'+'Columns in DataSet:'+'\\033[0m',df.columns)\nprint('\\033[1m'+'Shape of DataSet:'+'\\033[0m',df.shape)\nprint('\\033[1m'+'Size of DataSet:'+'\\033[0m',df.size)\nprint('\\033[1m'+'Dimension of DataSet:'+'\\033[0m',df.ndim)\nprint('\\033[1m'+'Total Memory used in DataSet:'+'\\033[0m',df.memory_usage().sum())\nprint('\\033[1m'+'Total Number of missing values in DataSet:'+'\\033[0m',df.isnull().sum().sum())\nprint('\\033[1m'+'Total Number of Unique values in DataSet:'+'\\033[0m',df.nunique().sum())\nprint('\\033[1m'+'Total Number of non null values in DataSet:'+'\\033[0m',df.count().sum())\nprint('\\033[1m'+'Total Number of duplicate rows in DataSet:'+'\\033[0m',df.duplicated().sum())\nprint(\"----------------------------------------------------------------------------------------------------------------------\")\nprint('\\033[1m'+'Summary Of Each Colomn'+'\\033[0m')\nprint(\"\\n\")\ncols=df.columns\ncols\nfor i in cols:\n    print('\\033[1m'+i+'\\033[0m')\n    stats_u(df,i)\n    print(\"\\n\")\n            ","990b51bc":"df.head()","da81248a":"df.tail()","bca3aab4":"df.dtypes","b822d2f3":"df.columns","e6ed18b2":"df.shape","1998f083":"df.size","fa5767e7":"df.info()","58cfa394":"df.describe()","b07bc27f":"df.isnull().sum()\n## income has null values","3db9d76b":"df.duplicated().sum()","e420cbc6":"df.skew()","b3e90d61":"x = df.drop(['Z_CostContact','Z_Revenue'],axis = 1)\nx.corr()","176bc274":"df.dropna(inplace = True)","fdd76fa2":"df[\"Year_Birth\"] = 2021-df[\"Year_Birth\"]","73a20a62":"df[\"Spent\"] = df[\"MntWines\"]+ df[\"MntFruits\"]+ df[\"MntMeatProducts\"]+ df[\"MntFishProducts\"]+ df[\"MntSweetProducts\"]+ df[\"MntGoldProds\"]","2c942e4e":"df.drop(['Z_CostContact','Z_Revenue'],axis = 1,inplace = True)","53105a02":"df.rename(columns = {'Year_Birth':'Age'},inplace = True)\ndf","8e0c63eb":"!pip install autoviz","5ef1412c":"! pip install xlrd","84d6db71":"from autoviz.AutoViz_Class import AutoViz_Class\nAV = AutoViz_Class()\ndf_av = AV.AutoViz('..\/input\/customer-personality-analysis\/marketing_campaign.csv',sep=\"\\t\")","6840abe9":"plt.figure(figsize=(16,9))\nax = sns.heatmap(x.corr(),annot = True,cmap = 'viridis')\nplt.show()","7aaf71ef":"''' Plot a Shifted Correlation Matrix '''\n# Diagonal correlation is always unity & less relevant, shifted variant shows only relevant cases\ndef corrMat(df,id=False):\n    \n    corr_mat = df.corr().round(2)\n    f, ax = plt.subplots(figsize=(12,7))\n    mask = np.triu(np.ones_like(corr_mat, dtype=bool))\n    mask = mask[1:,:-1]\n    corr = corr_mat.iloc[1:,:-1].copy()\n    sns.heatmap(corr,mask=mask,vmin=-0.3,vmax=0.3,center=0, \n                cmap='RdPu_r',square=False,lw=2,annot=True,cbar=False)\n#     bottom, top = ax.get_ylim() \n#     ax.set_ylim(bottom + 0.5, top - 0.5) \n    ax.set_title('Shifted Linear Correlation Matrix')\n    \ncorrMat(df)","4977a155":"'''Plot Correlation to Target Variable only'''\ndef corrMat2(df,target='Spent',figsize=(9,0.5),ret_id=False):\n    \n    corr_mat = df.corr().round(2);shape = corr_mat.shape[0]\n    corr_mat = corr_mat.transpose()\n    corr = corr_mat.loc[:, df.columns == target].transpose().copy()\n    if(ret_id is False):\n        f, ax = plt.subplots(figsize=figsize)\n        sns.heatmap(corr,vmin=-0.3,vmax=0.3,center=0, \n                     cmap='RdPu_r',square=False,lw=2,annot=True,cbar=False)\n        plt.title(f'Feature Correlation to {target}')\n    \n    if(ret_id):\n        return corr\ncorrMat2(df.drop(['Education','Marital_Status','Dt_Customer'],axis = 1))","77d14606":"sns.pairplot(df[['Income','Age','Recency','Spent','Marital_Status']])","4e3bce26":"df['Education'].value_counts()","92131d0b":"sns.countplot(x = 'Education' , data = df)\nplt.show()\n# basic are less in numbered","0e1acb47":"df['Marital_Status'].value_counts()","98140d01":"sns.countplot( x = 'Marital_Status',data = df)\nplt.show()\n# Widow,Alone,Absurd,Yolo are less in numbered","0b749b3d":"obj = ['Education','Marital_Status']","f05af0b9":"for i in obj:\n    data=df.copy()\n    data.groupby(i)['Age'].mean().plot.bar()\n    plt.xlabel(i)\n    plt.ylabel('Age')\n    plt.title(i)\n    plt.show()\n\n# average age is above 50 for people doing masters,PhD,Graduation\n# average age of basic is lessthan all other features below 50\n# average age of wiodows are high above 60\n# average age of absurd yolo single and alone below 50","d1126d75":"# I found no relations\nfor i in range(len(obj)):\n    for j in range(2):\n        x = obj[i]\n        if obj[j] != x:\n            sns.barplot(x= x,y='Age',hue=obj[j],data=df)\n            plt.legend(bbox_to_anchor=(1.1, 1.05))\n            plt.show()","49c4672a":"df7 = df.copy()","2dad5d6e":"for i in range(len(obj)):\n    x='Marital_Status'\n    for j in range(1):\n        if obj[i] != x:\n            sns.barplot(x= x,y='Age',hue=obj[i],data=df7)\n            sns.set(rc={'figure.figsize':(11,12)})\n            plt.show()","82fdb3b1":"for i in range(len(obj)):\n    x='Marital_Status'\n    for j in range(1):\n        if obj[i] != x:\n            sns.barplot(x= x,y='Income',hue=obj[i],data=df7)\n            sns.set(rc={'figure.figsize':(11,12)})\n            plt.show()","c1f1f256":"y = df.drop(['Education','Marital_Status','Dt_Customer'],axis = 1)\nfor i in y.columns:\n    sns.boxplot(x = i, data = y,color = 'yellowgreen')   \n    plt.xlabel(i)\n    plt.show()","5a369d2f":"def count_outliers(data,col):\n        q1 = data[col].quantile(0.25,interpolation='nearest')\n        q2 = data[col].quantile(0.5,interpolation='nearest')\n        q3 = data[col].quantile(0.75,interpolation='nearest')\n        q4 = data[col].quantile(1,interpolation='nearest')\n        IQR = q3 -q1\n        global LLP\n        global ULP\n        LLP = q1 - 1.5*IQR\n        ULP = q3 + 1.5*IQR\n        if data[col].min() > LLP and data[col].max() < ULP:\n            print(\"No outliers in\",i)\n        else:\n            print(\"There are outliers in\",i)\n            x = data[data[col]<LLP][col].size\n            y = data[data[col]>ULP][col].size\n            a.append(i)\n            print('Count of outliers are:',x+y)\nglobal a\na = []\nfor i in y.columns:\n    count_outliers(df,i)","74e983ad":"LE=LabelEncoder()\nfor i in obj:\n    df[i]=df[[i]].apply(LE.fit_transform)\ndf['Dt_Customer']=df[['Dt_Customer']].apply(LE.fit_transform)","75e19faf":"df.dtypes","6c34e630":"df1 = df.groupby('Age').agg({ 'Education' : 'count', 'Marital_Status' : 'count', 'Income':'mean', 'Kidhome' : 'count',\n       'Teenhome' : 'count',  'Recency':'mean', \n        'NumDealsPurchases' : 'mean', 'NumWebPurchases' : 'mean',\n       'NumCatalogPurchases' : 'mean', 'NumStorePurchases' : 'mean', 'NumWebVisitsMonth' : 'mean',\n        'Complain' : 'sum', 'Response' : 'sum', 'Spent' : 'mean'})\ndf1","835c8510":"# gropubu analysis using education\ndf2 = df.groupby('Education').agg({ 'Age' : 'mean', 'Education' : 'count', 'Marital_Status' : 'sum', 'Income':'mean', 'Kidhome' : 'count',\n       'Teenhome' : 'count',  'Recency':'mean', \n        'NumDealsPurchases' : 'mean', 'NumWebPurchases' : 'mean',\n       'NumCatalogPurchases' : 'mean', 'NumStorePurchases' : 'mean', 'NumWebVisitsMonth' : 'mean',\n        'Complain' : 'sum', 'Response' : 'sum', 'Spent' : 'mean'})\ndf2\n\n#Graduation     2\n#PhD            4\n#Master         3\n#2n Cycle       0\n#Basic          1","4e76e936":"px.bar(data_frame=df2.drop(['Income','Marital_Status'],axis = 1), barmode='group',\n       title = \"<b>Education wise Analyzing<\/b>\",template=\"plotly_dark\")\n# droping income,marital status  since they have lagrge value\n# PhD spent more tha any other in last 2 years","07ae8424":"px.bar(data_frame=df2[['Income','Marital_Status']], barmode='group',\n       title = \"<b>Education wise Analyzing<\/b>\",template=\"plotly_dark\")\n# income for phD are more","1c33d4d9":"df3 = df.groupby('Marital_Status').agg({ 'Age' : 'mean', 'Education' : 'count', 'Marital_Status' : 'sum', 'Income':'mean', 'Kidhome' : 'count',\n       'Teenhome' : 'count',  'Recency':'mean', \n        'NumDealsPurchases' : 'mean', 'NumWebPurchases' : 'mean',\n       'NumCatalogPurchases' : 'mean', 'NumStorePurchases' : 'mean', 'NumWebVisitsMonth' : 'mean',\n        'Complain' : 'sum', 'Response' : 'sum', 'Spent' : 'mean'})\ndf3\n\n#Married       3\n#Together      5\n#Single        4\n#Divorced      2\n#Widow         6\n#Alone         1\n#YOLO          7\n#Absurd        0","fd35ee7d":"px.bar(data_frame=df3.drop(['Income','Marital_Status','Spent'],axis = 1), barmode='group',\n       title = \"<b>Marital status wise Analyzing<\/b>\",template=\"plotly_dark\")\n# more number of kids at home for married and they are doing education\n# average age of widow is more\n# average recency of absurd is more","201e7be5":"px.bar(data_frame=df3[['Income','Spent']], barmode='group',\n       title = \"<b>Marital_Status wise Analyzing<\/b>\",template=\"plotly_dark\")\n# average income of widow is more\n# money spent by absurd is more","831973d7":"del_cols = ['AcceptedCmp3', 'AcceptedCmp4', 'AcceptedCmp5', 'AcceptedCmp1','AcceptedCmp2', 'Complain', 'Response']\nds = df.drop(del_cols, axis=1)\nscaler = StandardScaler()\nscaler.fit(ds)\nscaled_features = pd.DataFrame(scaler.transform(ds),columns= ds.columns )","0b83f513":"scaled_features.head()","2963821c":"#Principal component analysis (PCA) is a technique for reducing the dimensionality of such datasets, \n#increasing interpretability but at the same time minimizing information loss.\npca = PCA(n_components=3)\npca.fit(scaled_features)\nPCA_df = pd.DataFrame(pca.transform(scaled_features), columns=([\"Education\",\"Income\", \"Kidhome\"]))\nPCA_df.describe().T","2066a5cc":"#A 3D Projection Of Data In The Reduced Dimension\nx =PCA_df[\"Education\"]\ny =PCA_df[\"Income\"]\nz =PCA_df[\"Kidhome\"]\n#To plot\nfig = plt.figure(figsize=(10,8))\nax = fig.add_subplot(111, projection=\"3d\")\nax.scatter(x,y,z, c=\"magenta\", marker=\"o\" )\nax.set_title(\"A 3D Projection Of Data In The Reduced Dimension\")\nplt.show()","6198daa7":"from sklearn.cluster import KMeans\nfrom sklearn.datasets import make_blobs\n\nfrom yellowbrick.cluster import KElbowVisualizer","09f2ec5e":"model = KMeans()\nvisualizer = KElbowVisualizer(model, k=10)\n\nvisualizer.fit(PCA_df)        # Fit the data to the visualizer\nvisualizer.show()        # Finalize and render the figure","cc50fe17":"#Initiating the Agglomerative Clustering model \nAC = AgglomerativeClustering(n_clusters=4)\n# fit model and predict clusters\nAC_df = AC.fit_predict(PCA_df)\nPCA_df[\"Clusters\"] = AC_df\n#Adding the Clusters feature to the orignal dataframe.\ndf[\"Clusters\"]= AC_df","6a429e8a":"#Plotting the clusters\nfig = plt.figure(figsize=(10,8))\nax = plt.subplot(111, projection='3d', label=\"bla\")\nax.scatter(x, y, z, s=40, c=PCA_df[\"Clusters\"], marker='o', cmap = 'viridis' )\nax.set_title(\"The Plot Of The Clusters\")\nplt.show()","583cd53d":"#Plotting countplot of clusters\npl = sns.countplot(x=df[\"Clusters\"])\npl.set_title(\"Distribution Of The Clusters\")\nplt.show()","4b4e8c78":"pl = sns.scatterplot(data = df,x=df[\"Spent\"], y=df[\"Income\"],hue=df[\"Clusters\"])\npl.set_title(\"Cluster's Profile Based On Income And Spending\")\nplt.legend()\nplt.show()","0ccb3465":"plt.figure()\npl=sns.swarmplot(x=df[\"Clusters\"], y=df[\"Spent\"], color= \"#CBEDDD\", alpha=0.5 )\npl=sns.boxenplot(x=df[\"Clusters\"], y=df[\"Spent\"])\nplt.show()","1c60e200":"#Creating a feature to get a sum of accepted promotions \ndf[\"Total_Promos\"] = df[\"AcceptedCmp1\"]+ df[\"AcceptedCmp2\"]+ df[\"AcceptedCmp3\"]+ df[\"AcceptedCmp4\"]+ df[\"AcceptedCmp5\"]\n#Plotting count of total campaign accepted.\nplt.figure()\npl = sns.countplot(x=df[\"Total_Promos\"],hue=df[\"Clusters\"])\npl.set_title(\"Count Of Promotion Accepted\")\npl.set_xlabel(\"Number Of Total Accepted Promotions\")\nplt.show()","9e2cc8c4":"#Plotting the number of deals purchased\nplt.figure()\npl=sns.boxenplot(y=df[\"NumDealsPurchases\"],x=df[\"Clusters\"])\npl.set_title(\"Number of Deals Purchased\")\nplt.show()","3a669712":"from sklearn.cluster import AffinityPropagation\n#Initiating the Affinity Clustering model \nAP = AffinityPropagation(damping=0.9)\n# fit model and predict clusters\nAP_df = AP.fit_predict(PCA_df)\nPCA_df[\"Clusters\"] = AP_df\n#Adding the Clusters feature to the orignal dataframe.\ndf[\"Clusters\"]= AP_df","3d129e51":"#Plotting the clusters\nfig = plt.figure(figsize=(10,8))\nax = plt.subplot(111, projection='3d', label=\"bla\")\nax.scatter(x, y, z, s=40, c=PCA_df[\"Clusters\"], marker='o', cmap = 'viridis' )\nax.set_title(\"The Plot Of The Clusters\")\nplt.show()","d0e29b9e":"#Plotting countplot of clusters\npl = sns.countplot(x=df[\"Clusters\"])\npl.set_title(\"Distribution Of The Clusters\")\nplt.show()","67de0062":"pl = sns.scatterplot(data = df,x=df[\"Spent\"], y=df[\"Income\"],hue=df[\"Clusters\"])\npl.set_title(\"Cluster's Profile Based On Income And Spending\")\nplt.legend()\nplt.show()","7a783e08":"plt.figure()\npl=sns.swarmplot(x=df[\"Clusters\"], y=df[\"Spent\"], color= \"#CBEDDD\", alpha=0.5 )\npl=sns.boxenplot(x=df[\"Clusters\"], y=df[\"Spent\"])\nplt.show()","2996c9c9":"#Creating a feature to get a sum of accepted promotions \ndf[\"Total_Promos\"] = df[\"AcceptedCmp1\"]+ df[\"AcceptedCmp2\"]+ df[\"AcceptedCmp3\"]+ df[\"AcceptedCmp4\"]+ df[\"AcceptedCmp5\"]\n#Plotting count of total campaign accepted.\nplt.figure()\npl = sns.countplot(x=df[\"Total_Promos\"],hue=df[\"Clusters\"])\npl.set_title(\"Count Of Promotion Accepted\")\npl.set_xlabel(\"Number Of Total Accepted Promotions\")\nplt.show()","2f3fa4d1":"#Plotting the number of deals purchased\nplt.figure()\npl=sns.boxenplot(y=df[\"NumDealsPurchases\"],x=df[\"Clusters\"])\npl.set_title(\"Number of Deals Purchased\")\nplt.show()","45cd3e11":"from sklearn.cluster import Birch\n#Initiating the Birch Clustering model \nBP = Birch(threshold=0.01, n_clusters=4)\n# fit model and predict clusters\nBP_df = BP.fit_predict(PCA_df)\nPCA_df[\"Clusters\"] = BP_df\n#Adding the Clusters feature to the orignal dataframe.\ndf[\"Clusters\"]= BP_df","f237d293":"#Plotting the clusters\nfig = plt.figure(figsize=(10,8))\nax = plt.subplot(111, projection='3d', label=\"bla\")\nax.scatter(x, y, z, s=40, c=PCA_df[\"Clusters\"], marker='o', cmap = 'viridis' )\nax.set_title(\"The Plot Of The Clusters\")\nplt.show()","1e452197":"#Plotting countplot of clusters\npl = sns.countplot(x=df[\"Clusters\"])\npl.set_title(\"Distribution Of The Clusters\")\nplt.show()","c8afbcdb":"pl = sns.scatterplot(data = df,x=df[\"Spent\"], y=df[\"Income\"],hue=df[\"Clusters\"])\npl.set_title(\"Cluster's Profile Based On Income And Spending\")\nplt.legend()\nplt.show()","0445a644":"plt.figure()\npl=sns.swarmplot(x=df[\"Clusters\"], y=df[\"Spent\"], color= \"#CBEDDD\", alpha=0.5 )\npl=sns.boxenplot(x=df[\"Clusters\"], y=df[\"Spent\"])\nplt.show()","f31b6806":"#Creating a feature to get a sum of accepted promotions \ndf[\"Total_Promos\"] = df[\"AcceptedCmp1\"]+ df[\"AcceptedCmp2\"]+ df[\"AcceptedCmp3\"]+ df[\"AcceptedCmp4\"]+ df[\"AcceptedCmp5\"]\n#Plotting count of total campaign accepted.\nplt.figure()\npl = sns.countplot(x=df[\"Total_Promos\"],hue=df[\"Clusters\"])\npl.set_title(\"Count Of Promotion Accepted\")\npl.set_xlabel(\"Number Of Total Accepted Promotions\")\nplt.show()","71099644":"#Plotting the number of deals purchased\nplt.figure()\npl=sns.boxenplot(y=df[\"NumDealsPurchases\"],x=df[\"Clusters\"])\npl.set_title(\"Number of Deals Purchased\")\nplt.show()","84f47397":"from sklearn.cluster import DBSCAN\n#Initiating the BBSCAN Clustering model \nDP = DBSCAN(eps=0.30, min_samples=9)\n# fit model and predict clusters\nDP_df = DP.fit_predict(PCA_df)\nPCA_df[\"Clusters\"] = DP_df\n#Adding the Clusters feature to the orignal dataframe.\ndf[\"Clusters\"]= DP_df","e2775e38":"#Plotting the clusters\nfig = plt.figure(figsize=(10,8))\nax = plt.subplot(111, projection='3d', label=\"bla\")\nax.scatter(x, y, z, s=40, c=PCA_df[\"Clusters\"], marker='o', cmap = 'viridis' )\nax.set_title(\"The Plot Of The Clusters\")\nplt.show()","58c57883":"#Plotting countplot of clusters\npl = sns.countplot(x=df[\"Clusters\"])\npl.set_title(\"Distribution Of The Clusters\")\nplt.show()","29937171":"pl = sns.scatterplot(data = df,x=df[\"Spent\"], y=df[\"Income\"],hue=df[\"Clusters\"])\npl.set_title(\"Cluster's Profile Based On Income And Spending\")\nplt.legend()\nplt.show()","4b699f22":"plt.figure()\npl=sns.swarmplot(x=df[\"Clusters\"], y=df[\"Spent\"], color= \"#CBEDDD\", alpha=0.5 )\npl=sns.boxenplot(x=df[\"Clusters\"], y=df[\"Spent\"])\nplt.show()","d3863d4e":"#Creating a feature to get a sum of accepted promotions \ndf[\"Total_Promos\"] = df[\"AcceptedCmp1\"]+ df[\"AcceptedCmp2\"]+ df[\"AcceptedCmp3\"]+ df[\"AcceptedCmp4\"]+ df[\"AcceptedCmp5\"]\n#Plotting count of total campaign accepted.\nplt.figure()\npl = sns.countplot(x=df[\"Total_Promos\"],hue=df[\"Clusters\"])\npl.set_title(\"Count Of Promotion Accepted\")\npl.set_xlabel(\"Number Of Total Accepted Promotions\")\nplt.show()","5592b246":"#Plotting the number of deals purchased\nplt.figure()\npl=sns.boxenplot(y=df[\"NumDealsPurchases\"],x=df[\"Clusters\"])\npl.set_title(\"Number of Deals Purchased\")\nplt.show()","8c88ff6e":"from sklearn.cluster import MiniBatchKMeans\n#Initiating the MiniBatchKMeans Clustering model \nMP = MiniBatchKMeans(n_clusters=4)\n# fit model and predict clusters\nMP_df = MP.fit_predict(PCA_df)\nPCA_df[\"Clusters\"] = MP_df\n#Adding the Clusters feature to the orignal dataframe.\ndf[\"Clusters\"]= MP_df","442c10f1":"#Plotting the clusters\nfig = plt.figure(figsize=(10,8))\nax = plt.subplot(111, projection='3d', label=\"bla\")\nax.scatter(x, y, z, s=40, c=PCA_df[\"Clusters\"], marker='o', cmap = 'viridis' )\nax.set_title(\"The Plot Of The Clusters\")\nplt.show()","b83f43e5":"#Plotting countplot of clusters\npl = sns.countplot(x=df[\"Clusters\"])\npl.set_title(\"Distribution Of The Clusters\")\nplt.show()","adcaaf9d":"pl = sns.scatterplot(data = df,x=df[\"Spent\"], y=df[\"Income\"],hue=df[\"Clusters\"])\npl.set_title(\"Cluster's Profile Based On Income And Spending\")\nplt.legend()\nplt.show()","470fb576":"plt.figure()\npl=sns.swarmplot(x=df[\"Clusters\"], y=df[\"Spent\"], color= \"#CBEDDD\", alpha=0.5 )\npl=sns.boxenplot(x=df[\"Clusters\"], y=df[\"Spent\"])\nplt.show()","dcc1c98f":"#Creating a feature to get a sum of accepted promotions \ndf[\"Total_Promos\"] = df[\"AcceptedCmp1\"]+ df[\"AcceptedCmp2\"]+ df[\"AcceptedCmp3\"]+ df[\"AcceptedCmp4\"]+ df[\"AcceptedCmp5\"]\n#Plotting count of total campaign accepted.\nplt.figure()\npl = sns.countplot(x=df[\"Total_Promos\"],hue=df[\"Clusters\"])\npl.set_title(\"Count Of Promotion Accepted\")\npl.set_xlabel(\"Number Of Total Accepted Promotions\")\nplt.show()","d526df3a":"#Plotting the number of deals purchased\nplt.figure()\npl=sns.boxenplot(y=df[\"NumDealsPurchases\"],x=df[\"Clusters\"])\npl.set_title(\"Number of Deals Purchased\")\nplt.show()","d3bbe6c1":"from sklearn.cluster import MeanShift\n#Initiating the MeanShift Clustering model \nMSP = MeanShift()\n# fit model and predict clusters\nMSP_df = MSP.fit_predict(PCA_df)\nPCA_df[\"Clusters\"] = MSP_df\n#Adding the Clusters feature to the orignal dataframe.\ndf[\"Clusters\"]= MSP_df","7c80e7c2":"#Plotting the clusters\nfig = plt.figure(figsize=(10,8))\nax = plt.subplot(111, projection='3d', label=\"bla\")\nax.scatter(x, y, z, s=40, c=PCA_df[\"Clusters\"], marker='o', cmap = 'viridis' )\nax.set_title(\"The Plot Of The Clusters\")\nplt.show()","3026844b":"#Plotting countplot of clusters\npl = sns.countplot(x=df[\"Clusters\"])\npl.set_title(\"Distribution Of The Clusters\")\nplt.show()","496ec477":"pl = sns.scatterplot(data = df,x=df[\"Spent\"], y=df[\"Income\"],hue=df[\"Clusters\"])\npl.set_title(\"Cluster's Profile Based On Income And Spending\")\nplt.legend()\nplt.show()","653f8511":"plt.figure()\npl=sns.swarmplot(x=df[\"Clusters\"], y=df[\"Spent\"], color= \"#CBEDDD\", alpha=0.5 )\npl=sns.boxenplot(x=df[\"Clusters\"], y=df[\"Spent\"])\nplt.show()","f2c1d0d8":"#Creating a feature to get a sum of accepted promotions \ndf[\"Total_Promos\"] = df[\"AcceptedCmp1\"]+ df[\"AcceptedCmp2\"]+ df[\"AcceptedCmp3\"]+ df[\"AcceptedCmp4\"]+ df[\"AcceptedCmp5\"]\n#Plotting count of total campaign accepted.\nplt.figure()\npl = sns.countplot(x=df[\"Total_Promos\"],hue=df[\"Clusters\"])\npl.set_title(\"Count Of Promotion Accepted\")\npl.set_xlabel(\"Number Of Total Accepted Promotions\")\nplt.show()","1775ccce":"#Plotting the number of deals purchased\nplt.figure()\npl=sns.boxenplot(y=df[\"NumDealsPurchases\"],x=df[\"Clusters\"])\npl.set_title(\"Number of Deals Purchased\")\nplt.show()","f42d973e":"from sklearn.cluster import OPTICS\n#Initiating the OPTICS Clustering model \nOP = OPTICS(eps=0.8, min_samples=10)\n# fit model and predict clusters\nOP_df = OP.fit_predict(PCA_df)\nPCA_df[\"Clusters\"] = OP_df\n#Adding the Clusters feature to the orignal dataframe.\ndf[\"Clusters\"]= OP_df","1f3e4c8e":"#Plotting the clusters\nfig = plt.figure(figsize=(10,8))\nax = plt.subplot(111, projection='3d', label=\"bla\")\nax.scatter(x, y, z, s=40, c=PCA_df[\"Clusters\"], marker='o', cmap = 'viridis' )\nax.set_title(\"The Plot Of The Clusters\")\nplt.show()","b2a13303":"#Plotting countplot of clusters\npl = sns.countplot(x=df[\"Clusters\"])\npl.set_title(\"Distribution Of The Clusters\")\nplt.show()","58657766":"pl = sns.scatterplot(data = df,x=df[\"Spent\"], y=df[\"Income\"],hue=df[\"Clusters\"])\npl.set_title(\"Cluster's Profile Based On Income And Spending\")\nplt.legend()\nplt.show()","3aca7404":"plt.figure()\npl=sns.swarmplot(x=df[\"Clusters\"], y=df[\"Spent\"], color= \"#CBEDDD\", alpha=0.5 )\npl=sns.boxenplot(x=df[\"Clusters\"], y=df[\"Spent\"])\nplt.show()","0f6c1775":"#Creating a feature to get a sum of accepted promotions \ndf[\"Total_Promos\"] = df[\"AcceptedCmp1\"]+ df[\"AcceptedCmp2\"]+ df[\"AcceptedCmp3\"]+ df[\"AcceptedCmp4\"]+ df[\"AcceptedCmp5\"]\n#Plotting count of total campaign accepted.\nplt.figure()\npl = sns.countplot(x=df[\"Total_Promos\"],hue=df[\"Clusters\"])\npl.set_title(\"Count Of Promotion Accepted\")\npl.set_xlabel(\"Number Of Total Accepted Promotions\")\nplt.show()","8680fc5a":"#Plotting the number of deals purchased\nplt.figure()\npl=sns.boxenplot(y=df[\"NumDealsPurchases\"],x=df[\"Clusters\"])\npl.set_title(\"Number of Deals Purchased\")\nplt.show()","ea950808":"from sklearn.cluster import SpectralClustering\n#Initiating the SpectralClustering Clustering model \nSP = SpectralClustering(n_clusters=4)\n# fit model and predict clusters\nSP_df = SP.fit_predict(PCA_df)\nPCA_df[\"Clusters\"] = SP_df\n#Adding the Clusters feature to the orignal dataframe.\ndf[\"Clusters\"]= SP_df","285dc1aa":"#Plotting the clusters\nfig = plt.figure(figsize=(10,8))\nax = plt.subplot(111, projection='3d', label=\"bla\")\nax.scatter(x, y, z, s=40, c=PCA_df[\"Clusters\"], marker='o', cmap = 'viridis' )\nax.set_title(\"The Plot Of The Clusters\")\nplt.show()","f5251ebb":"#Plotting countplot of clusters\npl = sns.countplot(x=df[\"Clusters\"])\npl.set_title(\"Distribution Of The Clusters\")\nplt.show()","350aa030":"pl = sns.scatterplot(data = df,x=df[\"Spent\"], y=df[\"Income\"],hue=df[\"Clusters\"])\npl.set_title(\"Cluster's Profile Based On Income And Spending\")\nplt.legend()\nplt.show()","fa68d977":"plt.figure()\npl=sns.swarmplot(x=df[\"Clusters\"], y=df[\"Spent\"], color= \"#CBEDDD\", alpha=0.5 )\npl=sns.boxenplot(x=df[\"Clusters\"], y=df[\"Spent\"])\nplt.show()","3d9a0fc6":"#Creating a feature to get a sum of accepted promotions \ndf[\"Total_Promos\"] = df[\"AcceptedCmp1\"]+ df[\"AcceptedCmp2\"]+ df[\"AcceptedCmp3\"]+ df[\"AcceptedCmp4\"]+ df[\"AcceptedCmp5\"]\n#Plotting count of total campaign accepted.\nplt.figure()\npl = sns.countplot(x=df[\"Total_Promos\"],hue=df[\"Clusters\"])\npl.set_title(\"Count Of Promotion Accepted\")\npl.set_xlabel(\"Number Of Total Accepted Promotions\")\nplt.show()","4633d6d4":"#Plotting the number of deals purchased\nplt.figure()\npl=sns.boxenplot(y=df[\"NumDealsPurchases\"],x=df[\"Clusters\"])\npl.set_title(\"Number of Deals Purchased\")\nplt.show()","6dd66d0b":"from sklearn.mixture import GaussianMixture\n#Initiating the GaussianMixture Clustering model \nGP = GaussianMixture(n_components=4)\n# fit model and predict clusters\nGP_df = GP.fit_predict(PCA_df)\nPCA_df[\"Clusters\"] = GP_df\n#Adding the Clusters feature to the orignal dataframe.\ndf[\"Clusters\"]= GP_df","4a3da191":"#Plotting the clusters\nfig = plt.figure(figsize=(10,8))\nax = plt.subplot(111, projection='3d', label=\"bla\")\nax.scatter(x, y, z, s=40, c=PCA_df[\"Clusters\"], marker='o', cmap = 'viridis' )\nax.set_title(\"The Plot Of The Clusters\")\nplt.show()","0a3d0e07":"#Plotting countplot of clusters\npl = sns.countplot(x=df[\"Clusters\"])\npl.set_title(\"Distribution Of The Clusters\")\nplt.show()","2adfd056":"pl = sns.scatterplot(data = df,x=df[\"Spent\"], y=df[\"Income\"],hue=df[\"Clusters\"])\npl.set_title(\"Cluster's Profile Based On Income And Spending\")\nplt.legend()\nplt.show()","78e530d2":"plt.figure()\npl=sns.swarmplot(x=df[\"Clusters\"], y=df[\"Spent\"], color= \"#CBEDDD\", alpha=0.5 )\npl=sns.boxenplot(x=df[\"Clusters\"], y=df[\"Spent\"])\nplt.show()","7b566444":"#Creating a feature to get a sum of accepted promotions \ndf[\"Total_Promos\"] = df[\"AcceptedCmp1\"]+ df[\"AcceptedCmp2\"]+ df[\"AcceptedCmp3\"]+ df[\"AcceptedCmp4\"]+ df[\"AcceptedCmp5\"]\n#Plotting count of total campaign accepted.\nplt.figure()\npl = sns.countplot(x=df[\"Total_Promos\"],hue=df[\"Clusters\"])\npl.set_title(\"Count Of Promotion Accepted\")\npl.set_xlabel(\"Number Of Total Accepted Promotions\")\nplt.show()","331a3631":"#Plotting the number of deals purchased\nplt.figure()\npl=sns.boxenplot(y=df[\"NumDealsPurchases\"],x=df[\"Clusters\"])\npl.set_title(\"Number of Deals Purchased\")\nplt.show()","a499c413":"## UPVOTE IF U LIKE","1cacfe9b":"# OPTICS\n#### OPTICS clustering (where OPTICS is short for Ordering Points To Identify the Clustering Structure) is a modified version of DBSCAN described above.\n\n#### It is implemented via the OPTICS class and the main configuration to tune is the \u201ceps\u201d and \u201cmin_samples\u201d hyperparameters.","e66c25ef":"# Importing the Libraries","d4a92559":"# Loading Datset","f9f1feae":"# Mean Shift\n#### Mean shift clustering involves finding and adapting centroids based on the density of examples in the feature space.\n\n#### It is implemented via the MeanShift class and the main configuration to tune is the \u201cbandwidth\u201d hyperparameter.","8b033dad":"## From the above plot, it can be clearly seen that cluster 2 is our biggest set of customers closely followed by cluster 1. We can explore what each cluster is spending on for the targeted marketing strategies.","d653ec47":"# (Important) Analysis Using Groupby","963a049b":"# Spectral Clustering\n#### Spectral Clustering is a general class of clustering methods, drawn from linear algebra.\n\n#### It is implemented via the SpectralClustering class and the main Spectral Clustering is a general class of clustering methods, drawn from linear algebra. to tune is the \u201cn_clusters\u201d hyperparameter used to specify the estimated number of clusters in the data.","e87ea16c":"# Data visualisation","76b8f67b":"## Encoding","49905b5e":"## from above groupby agg we can say that,\n## many number of them in graduatrion and avereage monry spent by them is also high\n## average money spent by basic is low among all and their income is also low\n## income of PhD is more among all","2ca2de5e":"# Feature Scaling","1bc316f2":"### for all who do basic income is less\n### Income for single is alomost eual for 2nCycle,master,PhD less for basic\n### income for graduation,masters, PhD is averagely above 40k\n### average income for basic is 10k to 20k","dd134709":"# Exploratory Data Analysis","33c563d6":"# Data Preprocessing","11802f0f":"# Data visualisation using Autoviz","931dd301":"# Gaussian Mixture Model\n#### A Gaussian mixture model summarizes a multivariate probability density function with a mixture of Gaussian probability distributions as its name suggests.\n\n\n#### It is implemented via the GaussianMixture class and the main configuration to tune is the \u201cn_clusters\u201d hyperparameter used to specify the estimated number of clusters in the data.","b9d0ce2b":"# Basic data cleaning","30557e00":"# Exploratory Data Analysis using userdefined function","bcb1e2f0":"# Mini-Batch K-Means\n#### Mini-Batch K-Means is a modified version of k-means that makes updates to the cluster centroids using mini-batches of samples rather than the entire dataset, which can make it faster for large datasets, and perhaps more robust to statistical noise.\n\n#### It is implemented via the MiniBatchKMeans class and the main configuration to tune is the \u201cn_clusters\u201d hyperparameter set to the estimated number of clusters in the data.","e38e5405":"# Affinity Clustering Model\n\n#### Affinity Propagation involves finding a set of exemplars that best summarize the data.\n#### It is implemented via the AffinityPropagation class and the main configuration to tune is the \u201cdamping\u201d set between 0.5 and 1, and perhaps \u201cpreference.\u201d","704f7a8a":"# Clustering","dd885d5f":"# BIRCH\n#### BIRCH Clustering (BIRCH is short for Balanced Iterative Reducing and Clustering using Hierarchies) involves constructing a tree structure from which cluster centroids are extracted.\n#### It is implemented via the Birch class and the main configuration to tune is the \u201cthreshold\u201d and \u201cn_clusters\u201d hyperparameters, the latter of which provides an estimate of the number of clusters.","67ef0dd7":"### Calculating age by year of birth since age is useful in classification","2259d8f7":"# Dimensionality reduction using PCA","44e8645d":"# DBSCAN\n#### DBSCAN Clustering (where DBSCAN is short for Density-Based Spatial Clustering of Applications with Noise) involves finding high-density areas in the domain and expanding those areas of the feature space around them as clusters.\n\n#### It is implemented via the DBSCAN class and the main configuration to tune is the \u201ceps\u201d and \u201cmin_samples\u201d hyperparameters.","aa34def1":"### combining all amount spent in entire 2 years","378b0840":"## Count of ouliers using using IQR method","f4e7ffbe":"### in single there are more number of people who do PhD less number of people who do basic and average age is between 40-50\n### in together it is alomost equal number of doing PhD and Masters less number of people who do 2n cycle and average age is between 50-60\n### in married it is alomost equal number of doing PhD and Masters less number of people who do basic and average age is between 40-50\n### in divorced it is alomost equal number of doing PhD and 2ncylce less number of people who do basic and average age is between 50 - 60\n### in widow more number of people do masters and less number of people do graduation and average age is between 60-70\n### in alone there is only 3 categories graduation,PhD,masters and they do masters more and 2ncycle less average age is 40-50\n### in absurd there is only 2 categories masters,graduation and people doing masters are more average age is 40-50\n### in yolo only in category PhD\n### This is useful for classification","f9cdd60f":"# Evaluating models"}}