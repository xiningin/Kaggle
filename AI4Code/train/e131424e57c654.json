{"cell_type":{"7c904f94":"code","7b466448":"code","88b364bd":"code","095aae48":"code","e0bceba0":"code","4aacb860":"code","22bb9aea":"code","20b812af":"code","d4c91d05":"code","102c13a9":"code","3fcbf715":"code","622f22a6":"code","ab5c95a9":"code","cdca9b58":"code","6928dd56":"code","3e827bf0":"code","d16f48ca":"code","f4a375d4":"code","e90da175":"code","4e65edcc":"code","7c1245a2":"code","d1ee5b12":"markdown","3e47fcd9":"markdown","95634ebf":"markdown","4205a789":"markdown","6b370689":"markdown","f581595f":"markdown","6b54c85c":"markdown","172e48b6":"markdown","4124b1da":"markdown","0c5cc629":"markdown","3e075ff0":"markdown","8bb24483":"markdown","fe3d1a57":"markdown","698a4a0b":"markdown","634693a1":"markdown","d281a092":"markdown"},"source":{"7c904f94":"!git clone https:\/\/github.com\/WiDSTexas2021\/datathon-code.git\n!git clone https:\/\/github.com\/ishita1983\/LSTM_Electricity_Load_Forecast","7b466448":"#@title All the libraries required for this notebook are imported\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom matplotlib.pylab import rcParams\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nimport tensorflow as tf\nimport pytz\nimport matplotlib.image as mpimg\nimport datetime as dt\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, LSTM, Dropout, RepeatVector, TimeDistributed\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\n%matplotlib inline\n\nsns.set(style='whitegrid', palette='muted')\nrcParams['figure.figsize'] = 14, 8\nnp.random.seed(1)\ntf.random.set_seed(1)\n\nprint('Tensorflow version:', tf.__version__)","88b364bd":"cityname_dict = {'South':'Corpus Christi',\n                 'North':'Wichita Falls',\n                 'Coast':'Houston',\n                 'Far West':'Midland',\n                 'South Central':'Austin',\n                 'East':'Tyler',\n                 'West':'Abilene',\n                 'North Central':'Dallas'}\n\n\ndef show_image(im):\n    ''' Shows the image im in the notebook'''\n    plt.imshow(mpimg.imread(im))\n    plt.grid(False)  \n    plt.xticks([])\n    plt.yticks([])\n    plt.show()\n\ndef create_dataset(X2, y, time_steps_in, time_steps_out,lag_steps):\n    '''create segmentation of the input and the output data according to time steps and time lags'''\n    Xs, ys = [], []\n    for i in range(len(y) - time_steps_out-time_steps_in+1):\n        if i ==0:\n             startindex = i\n        else:\n             startindex = startindex +   lag_steps\n        if startindex <  (len(y) - time_steps_out- time_steps_in  ):\n             \n             v2 = X2[(startindex + time_steps_in):(startindex+ time_steps_in + time_steps_out)] # weather in furture\n             v = v2\n             v = np.reshape(v,(time_steps_in,1))\n             Xs.append(v)        \n             ys.append(y[(startindex + time_steps_in):(startindex+ time_steps_in + time_steps_out)])\n        else:    \n             break\n    return np.array(Xs),np.array(ys)    \n\ndef create_data_frame_for_ml(region, df_weather_history,df_weather_forecast,df_hr_load):    \n      '''from region name, hourly load, weather forecast and history: geneartes the dataframe to be used in training and the weather prediction in that region'''\n      cityname = cityname_dict[region]\n      df_weather_region_history = df_weather_history[df_weather_history['city']==cityname]\n      df_weather_region_forecast = df_weather_forecast[df_weather_forecast['city']==cityname]\n      df_hr_load['Hour_Ending']=df_hr_load['Hour_Ending'].apply(lambda x:x.replace(tzinfo=None)  )\n      df_to_ml=df_hr_load.merge(df_weather_region_history,  how = 'left', left_on='Hour_Ending', right_on='timestamp')\n      df_to_ml['tempF'] = df_to_ml['tempF'].interpolate(method='linear')\n      df = df_to_ml[['Hour_Ending','tempF',region]]\n      print('Complete...created and merged data sets for weather region ' + region)\n      return df, df_weather_region_forecast\n\ndef plot_timeseries(dates,weather,hourly_usage,region):\n      fig = make_subplots(specs=[[{\"secondary_y\": True}]])\n      fig.add_trace(go.Scatter(x=dates,y=weather,name = 'Temperature (degF) '+cityname_dict[region]))\n      fig.add_trace(go.Scatter(x=dates,y=hourly_usage,name='Hourly Electricity Use (KWH) for '+region),secondary_y=True)\n      fig.update_layout(showlegend=True)\n      fig.update_xaxes(title_text=\"Date\")\n      fig.update_yaxes(title_text=\"Temperature (degF)\", secondary_y=False)\n      fig.update_yaxes(title_text=\"Hourly Electricity Use (KWH)\", secondary_y=True)\n      fig.show()      \n\ndef str_to_dt(timestr,hr,min):\n      '''converts date time string to datetime'''\n      return dt.datetime.strptime(timestr,'%m-%d-%Y').replace(hour=hr,minute=min)\n\ndef filter_training_data(df,day_training_starts, day_training_stops):\n      '''filters the data based on training start and stop dates'''\n      index_training = (df['Hour_Ending']>day_training_starts) & (df['Hour_Ending']<=day_training_stops)\n      train_df = df[index_training]\n      num_row_tar = ((day_training_stops-day_training_starts).days)*24\n      num_row_ac = train_df.shape[0]\n      if num_row_tar==num_row_ac:\n        print('The number of data entries match hourly requirement')\n      else:\n        print('The number of data entries mismatch hourly requirement by '+ str(num_row_ac-num_row_tar))\n      print('Complete... filtered data according to the start and end days')    \n      return train_df  \n\ndef transform_data(train_df, region):\n      \n      train_data= train_df[[region,'tempF']].values\n\n      scaler1 = MinMaxScaler()\n      scaler2 = MinMaxScaler()\n      X1 = train_data[:,0].reshape(train_df.shape[0],1)\n      X2 = train_data[:,1].reshape(train_df.shape[0],1)\n\n      scaler_use_el = scaler1.fit(X1)\n      scaler_weather = scaler2.fit(X2)\n      train_X1 = scaler_use_el.transform(X1)\n      train_X2 = scaler_weather.transform(X2)\n\n      print(' Train data shape (before transformtion) for load and temperature: '+str (train_X1.shape)+str (train_X2.shape))\n      print('Complete... transformation and scaling of traing data')\n      return (train_X1,train_X2, scaler_use_el,scaler_weather)\n\n          \ndef train_model(X_train,Y_train):\n     '''defines the LSTM model and trains it on the training data set'''\n     num_features = X_train.shape[2]\n     time_steps_in = X_train.shape[1]\n     time_steps_out = Y_train.shape[1]\n     model = Sequential([\n       LSTM(200, activation = 'relu', input_shape=(time_steps_in, num_features),return_sequences=True),\n       LSTM(200, return_sequences=True),\n       LSTM(100, return_sequences=True),\n       LSTM(100, activation = 'relu'),\n       Dense(time_steps_out)])\n     model.compile(loss='mse', optimizer='adam')\n     es = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=5, mode='min',restore_best_weights=True,verbose = 1)\n     print('Training LSTM Model...')\n     history=model.fit(X_train,Y_train, epochs = 50,callbacks = es, validation_split = 0.1, batch_size = 128,verbose = 0 )\n     print('Complete... Training the Model')\n     print('Plotting loss over epochs...')\n     plt.plot(history.history['loss'], label='Training Loss')\n     plt.plot(history.history['val_loss'], label='Validation Loss')\n     plt.grid(True)\n     plt.legend()\n     plt.xlabel('Epochs')\n     plt.ylabel('Loss')\n     plt.show()\n     return model\n\ndef create_datestamp_df(prediction_start_date, prediction_end_date):\n    df_ts_test = pd.DataFrame({'Hours': pd.date_range(prediction_start_date, prediction_end_date, freq='1H')} )\n    return df_ts_test\n\ndef forecast(model,df_ts_test,df_weather_region_forecast,region,scaler_use_el,scaler_weather):\n  df_test=df_ts_test.merge(df_weather_region_forecast,  how = 'left', left_on='Hours', right_on='timestamp')\n  df_test['tempF'] = df_test['tempF'].interpolate(method='linear')\n  df_test = df_test.drop([0])\n  df_test = df_test[['Hours','tempF']]\n  X_test =  df_test['tempF'].values\n  X_test = scaler_weather.transform(X_test.reshape(X_test.shape[0],1))\n  Xtest = X_test.reshape(1,X_test.shape[0],1)\n  y_pred = model(Xtest)\n  predicted_use = scaler_use_el.inverse_transform(y_pred)[0]\n  print('Complete ... making predictions')\n  return df_test, predicted_use      ","095aae48":"show_image('datathon-code\/data\/ercotWeatherZoneMap.png')","e0bceba0":"df_hr_load= pd.read_csv('datathon-code\/data\/ercot_hourly_load.csv',parse_dates=['Hour_Ending'])","4aacb860":"for i in range(1,len(df_hr_load.columns)):\n fig = go.Figure()\n fig.add_trace(go.Scatter(x=df_hr_load[df_hr_load.columns[0]], y=df_hr_load[df_hr_load.columns[i]],\n                    mode='lines',name=df_hr_load.columns[i]))\n fig.update_layout(showlegend=True)\n fig.show()","22bb9aea":"print('This is the summary of how many NaN entries are there:')\nprint(df_hr_load.isnull().sum())\nprint('This is where the Nan Values are:')\nprint(df_hr_load[df_hr_load['East'].isnull()])\ndf_hr_load=df_hr_load.fillna(method='ffill')\nprint('This is the summary of how many NaN entries are there after filling:')\nprint(df_hr_load.isnull().sum())","20b812af":"df_weather_forecast= pd.read_csv('datathon-code\/data\/weather_forecast.csv')\ndf_weather_history= pd.read_csv('datathon-code\/data\/weather_history.csv')\n","d4c91d05":"time_dict = {0:'00:00', 300:'3:00', 600:'6:00', 900:'9:00', 1200:'12:00', 1500:'15:00', 1800:'18:00', 2100:'21:00'}\ndf_weather_forecast= df_weather_forecast.replace({'time':time_dict})\ndf_weather_history= df_weather_history.replace({'time':time_dict})\ndf_weather_forecast['timestamp'] = df_weather_forecast['date'] + ' ' + df_weather_forecast['time'] \ndf_weather_history['timestamp'] = df_weather_history['date'] + ' ' + df_weather_history['time']\ndf_weather_history['timestamp']= pd.to_datetime(df_weather_history['timestamp'])\ndf_weather_forecast['timestamp']= pd.to_datetime(df_weather_forecast['timestamp'])","102c13a9":"print('Number of Nan entries in weather forecast: '+str(df_weather_forecast.isnull().values.sum()))\nprint('Number of Nan entries in weather history: '+str(df_weather_history.isnull().values.sum()))\n","3fcbf715":"show_image('LSTM_Electricity_Load_Forecast\/wids1.PNG')\nshow_image('LSTM_Electricity_Load_Forecast\/wids2.PNG')","622f22a6":"def main(region,start_training_day,stop_training_day,prediction_start_date,prediction_end_date,numdays,lagdays ):\n  \n   time_steps_in  = numdays*24\n   time_steps_out  = numdays*24\n   lag_steps = lagdays*24\n\n   df, df_weather_region_forecast=create_data_frame_for_ml(region, df_weather_history,df_weather_forecast,df_hr_load)\n   plot_timeseries(df['Hour_Ending'],df['tempF'],df[region],region)\n   train_df = filter_training_data(df,str_to_dt(start_training_day,0,0), str_to_dt(stop_training_day,0,0))\n   train_X1,train_X2, scaler_use_el,scaler_weather=transform_data(train_df, region)\n   X_train,Y_train = create_dataset(train_X2, train_X1,time_steps_in, time_steps_out,lag_steps)\n   print('Training data shape (after transformtion): '+str(X_train.shape) +str(Y_train.shape))\n   model = train_model(X_train,Y_train)\n   df_ts_test = create_datestamp_df(prediction_start_date, prediction_end_date)\n   df_test,predicted_use = forecast(model,df_ts_test,df_weather_region_forecast,region,scaler_use_el,scaler_weather)\n   plot_timeseries(df_test['Hours'],df_test['tempF'],predicted_use,region)\n   return df_test, predicted_use ","ab5c95a9":"region = 'Coast'\nstart_training_day =  '11-02-2014'  #starting on a sunday\nstop_training_day =  '06-05-2021'\nprediction_start_date = '2021-06-13'  #starting on a sunday\nprediction_end_date = '2021-06-20'\nnumdays = 7\nlagdays = 7                          # for Far West we cannot do the lagging to 7 days due to non-stationarity of usage data\n\ndf_test, predicted_use = main(region,start_training_day,stop_training_day,prediction_start_date,prediction_end_date,numdays,lagdays )\n\n## only keep this for the first run\ndf_to_submission=df_test['Hours'].apply(lambda x:str(x)+'-05:00'  )\ndf_to_submission = pd.DataFrame(df_to_submission).rename(columns={\"Hours\": \"Hour_Ending\"})\n## comment out the above two lines for susequent region runs\n\ndf_to_submission[region] =predicted_use ","cdca9b58":"region = 'East'\nstart_training_day =  '11-02-2014'  #starting on a sunday\nstop_training_day =  '06-05-2021'\nprediction_start_date = '2021-06-13'  #starting on a sunday\nprediction_end_date = '2021-06-20'\nnumdays = 7\nlagdays = 7                          # for Far West we cannot do the lagging to 7 days due to non-stationarity of usage data\n\ndf_test, predicted_use = main(region,start_training_day,stop_training_day,prediction_start_date,prediction_end_date,numdays,lagdays )\n\n\ndf_to_submission[region] =predicted_use ","6928dd56":"region = 'Far West'\nstart_training_day =  '05-02-2019'  #starting on a sunday\nstop_training_day =  '06-05-2021'\nprediction_start_date = '2021-06-13'  #starting on a sunday\nprediction_end_date = '2021-06-20'\nnumdays = 7\nlagdays = 2                          # for Far West we cannot do the lagging to 7 days due to non-stationarity of usage data\n\ndf_test, predicted_use = main(region,start_training_day,stop_training_day,prediction_start_date,prediction_end_date,numdays,lagdays )\n\n\ndf_to_submission[region] =predicted_use ","3e827bf0":"region = 'North'\nstart_training_day =  '11-02-2014'  #starting on a sunday\nstop_training_day =  '06-05-2021'\nprediction_start_date = '2021-06-13'  #starting on a sunday\nprediction_end_date = '2021-06-20'\nnumdays = 7\nlagdays = 7                          # for Far West we cannot do the lagging to 7 days due to non-stationarity of usage data\n\ndf_test, predicted_use = main(region,start_training_day,stop_training_day,prediction_start_date,prediction_end_date,numdays,lagdays )\n\n\ndf_to_submission[region] =predicted_use ","d16f48ca":"region = 'North Central'\nstart_training_day =  '06-03-2018'  #starting on a sunday\nstop_training_day =  '06-05-2021'\nprediction_start_date = '2021-06-13'  #starting on a sunday\nprediction_end_date = '2021-06-20'\nnumdays = 7\nlagdays = 7                          # for Far West we cannot do the lagging to 7 days due to non-stationarity of usage data\n\ndf_test, predicted_use = main(region,start_training_day,stop_training_day,prediction_start_date,prediction_end_date,numdays,lagdays )\n\n\ndf_to_submission[region] =predicted_use ","f4a375d4":"region = 'South'\nstart_training_day =  '11-02-2014'  #starting on a sunday\nstop_training_day =  '06-05-2021'\nprediction_start_date = '2021-06-13'  #starting on a sunday\nprediction_end_date = '2021-06-20'\nnumdays = 7\nlagdays = 7                          # for Far West we cannot do the lagging to 7 days due to non-stationarity of usage data\n\ndf_test, predicted_use = main(region,start_training_day,stop_training_day,prediction_start_date,prediction_end_date,numdays,lagdays )\n\n\ndf_to_submission[region] =predicted_use ","e90da175":"region = 'South Central'\nstart_training_day =  '11-02-2014'  #starting on a sunday\nstop_training_day =  '06-05-2021'\nprediction_start_date = '2021-06-13'  #starting on a sunday\nprediction_end_date = '2021-06-20'\nnumdays = 7\nlagdays = 7                          # for Far West we cannot do the lagging to 7 days due to non-stationarity of usage data\n\ndf_test, predicted_use = main(region,start_training_day,stop_training_day,prediction_start_date,prediction_end_date,numdays,lagdays )\n\n\ndf_to_submission[region] =predicted_use ","4e65edcc":"region = 'West'\nstart_training_day =  '11-02-2014'  #starting on a sunday\nstop_training_day =  '06-05-2021'\nprediction_start_date = '2021-06-13'  #starting on a sunday\nprediction_end_date = '2021-06-20'\nnumdays = 7\nlagdays = 7                          # for Far West we cannot do the lagging to 7 days due to non-stationarity of usage data\n\ndf_test, predicted_use = main(region,start_training_day,stop_training_day,prediction_start_date,prediction_end_date,numdays,lagdays )\n\n\ndf_to_submission[region] =predicted_use ","7c1245a2":"df_to_submission.to_csv('submission.csv',index=False)","d1ee5b12":"### Description: Model Training (LSTM)","3e47fcd9":"## WIDS Texas Datathon 2021: Prediction of Electricity Load \n\nBy [Ishita Chakraborty](https:\/\/www.linkedin.com\/in\/ishitachakrabortyphd\/)\n\n\nThis notebook explores the prediction of electrical load for 7 days into the future using historical data of Texas and predicted weather for the 7 days. A multi-layer LSTM model is trained for forecasting hourly electricity usage.\n\nThis work is submitted as an entry to the WiDS Texas Datathon 2021. The input data to the model is stored in a git repo and updated every day. In the current state of the notebook, the forecasting is for June 13 to June 20,2021.\n","95634ebf":"* The first step in model building is creating the proper training data set. \n\n* We will start here with a multi time step LSTM model that predicts the output only from the weather (temerature) at that time stamp. The model will be trained on the past usage as output with historical weather as the input. In the test set (prediction) the weather forecast will be used as the input for the electricity usage forecast. The two schematics below show the scheme of training data selections. \n\n* The second figure shows the schematic of the RNN (LSTM) model used here for the sequence data.\n\n* The model will be trained with the stationary time series data (last 6 years data, 2 years for far west)\n\n* 10% of the training data will be kept as validation set. Early stopping will be used to reduce overtraining.\n\n* The test data here is the 7 days forecast. Checks have been done by keeping aside last few days of known data as test set. In the final run all the data is used to train the model.","4205a789":"#### Import Data and libraries\nSome exploration of the data is done through plotting.\n","6b370689":"Next we will look at other data. Although covid data is available, it does not look like there is any thing special in the 2020 hourly usage data due to covid. So the covid data is not used in the model. The weather history and weather prediction will be used in building the model.\n\n\n","f581595f":"In the hourly usage data, electricity usage is recorded for every hour for eight weather zones of Texas. All timeseries data is plotted to take a closer look at the variation. The plots can be zoomed in to reveal a more granular detail. As the plots are examined (zoom) in at a monthly, weekly, and daily level and few details come out:\n\n*   There is hourly variation in the usage with more use in the day time\n*   There is some variation in summer and winter months \n*   For most weather zones except for Far West, the usage data is stationary for the past 5 years\n*    However in the Far West region, usage has been up significantly in the past 2 years, most likely due to the population growth in the Midland area around the Permian Basin\n*    There is some variation during weekends and weekdays too\n\n","6b54c85c":"### Explore data\n\nExplore the available data with plots and charts.\n\nThe map below shows the different weather zones in Texas. We will use this to track the cities related to the weather zones and pull out the zonal weather history and weather prediction.\n\n","172e48b6":"### Train model for East region. Scroll down to see logs and plots.","4124b1da":"### All the helper functions needed for forecastingn model is stored below. The descriptions of the helper functions are:\n\n\n\n*   `show_image`: shows an image in the notebook\n*   `create_dataset`: create segmentation of the input and the output data according to time steps and time lags\n*    `create_data_frame_for_ml`: from region name, hourly load, weather forecast and history generates the dataframe to be used in training and that includes the weather and the hourly loads\n*    `filter_training_data`: filters a data frame between a start day and an end day\n*     `transform_data`: scales the wether and the hourly usage data and also returns the scalers\n*     `train_model`: defines the LSTM model and trains it on the trainig data set\n*     `create_datestamp_ds`: creates the date and time stamps for the forecast \n*     `forecast`: uses the trained model to predict data in to the future\n","0c5cc629":"### Description: Preprocessing and Feature Engineering","3e075ff0":"FInd out if there are any missing values in the hourly load data. Only one entry was found. The missing values are replaced with preceding values.","8bb24483":"### Train model for Coast region. Scroll down to see logs and plots.","fe3d1a57":"Make the values of the datetime column into datetime data format. For that a bit of pre-processing is necessary.","698a4a0b":"Check if there are any Nan Values in the weather data. Looks like not!","634693a1":"Prior to building the predictive model, the data needs to be prepared. The data sets to be used for training are the hourly electricity usage data and the weather history data (3-hourly). In order to have the correct sequence for training, these data sets need to be merged. The function `create_data_for_ml` creates the merged data set by:  \n\n\n1.   matching the time stamp between the two data frames (electricity use and weather)\n2.   interpolating (linear) in the missing hourly values in the 3 hourly weather data to match with hourly electricity usage data\n\nAfter the dataframes are merged, the weather and the electricity usage data are plotted together.\n\n* For training, the data will need to be scaled and the scaling parameters \non the training data is saved for use during predictions.\n\n* Another important thing is to set the **start of training date at the same day of the week we would like the predictions to start (which is a Sunday in this case)**. The shift to the data is 7 days for each training example, so that the training process captures the correct sequence of days of the week. This can be easily done when we have a stationary training set for a long period, which is true for all zones except Far West.\n\n","d281a092":"### Now Lets get everything to work!\n\nHere is the main function that calls in on all the helper functions to build, train and make prediction."}}