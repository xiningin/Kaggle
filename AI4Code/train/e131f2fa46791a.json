{"cell_type":{"7af50e2e":"code","a5148416":"code","85f4c9cd":"code","97ccf3f9":"code","dc8f970a":"code","3c90630d":"code","4a5153c1":"code","2222704f":"code","f784571a":"code","36c76d08":"code","7096bf71":"code","87387542":"code","76eb6aa7":"code","4466b66a":"code","d3e277b6":"code","bdff86e5":"markdown"},"source":{"7af50e2e":"data_train = '\/kaggle\/input\/g-research-crypto-forecasting\/train.csv'\ndata_asset_details = '\/kaggle\/input\/g-research-crypto-forecasting\/asset_details.csv'\ndata_supplemental_train = '\/kaggle\/input\/g-research-crypto-forecasting\/supplemental_train.csv'","a5148416":"import random\nimport pandas as pd\nimport numpy as np\nimport lightgbm as lgb\nimport time\nimport datetime\nimport plotly.graph_objects as go","85f4c9cd":"%%time\ndf_train = pd.read_csv(data_train, \n                       dtype={'Asset_ID': 'int8', 'Count': 'int32', 'row_id': 'int32', 'Count': 'int32', \n                              'Open': 'float64', 'High': 'float64', 'Low': 'float64', 'Close': 'float64', \n                              'Volume': 'float64', 'VWAP': 'float64'\n                             }\n                      )\ndf_train.head()","97ccf3f9":"import gresearch_crypto","dc8f970a":"df_assets = pd.read_csv(data_asset_details).sort_values(by='Asset_ID')\ndf_assets.head()","3c90630d":"for i in range(14):\n \n    dfcrop=df_train[df_train['Asset_ID']==i]\n    print('Percentage of values not nan',(1-(np.sum((dfcrop['Target'].isnull()).astype(int))\/dfcrop.shape[0]))*100)\n  ","4a5153c1":"import xgboost as xgb\n\ndef upper_shadow(df):\n    return df['High'] - np.maximum(df['Close'], df['Open'])\n\ndef lower_shadow(df):\n    return np.minimum(df['Close'], df['Open']) - df['Low']\n\n\ndef get_features(df):\n    df_feat = df[['Count', 'Open', 'High', 'Low', 'Close', 'Volume', 'VWAP']].copy()\n    df_feat['Upper_Shadow'] = upper_shadow(df_feat)\n    df_feat['Lower_Shadow'] = lower_shadow(df_feat)\n    \n    df_feat[\"Close\/Open\"] = df_feat[\"Close\"] \/ df_feat[\"Open\"] \n    df_feat[\"Close-Open\"] = df_feat[\"Close\"] - df_feat[\"Open\"] \n    df_feat[\"High-Low\"] = df_feat[\"High\"] - df_feat[\"Low\"] \n    df_feat[\"High\/Low\"] = df_feat[\"High\"] \/ df_feat[\"Low\"]\n    \n    df_feat['Mean'] = df_feat[['Open', 'High', 'Low', 'Close']].mean(axis=1)\n\n    df_feat[\"Median\"] = df_feat[[\"Open\", \"High\", \"Low\", \"Close\"]].median(axis=1)\n\n    return df_feat","2222704f":"from sklearn.preprocessing import StandardScaler","f784571a":"def get_Xy_and_model_for_asset(df_train, asset_id):\n    df = df_train[df_train[\"Asset_ID\"] == asset_id]\n    \n    df_proc = get_features(df)\n    df_proc['y'] = df['Target']\n    df_proc = df_proc.drop(labels=np.where(np.isinf(df_proc))[0], axis=0)\n    df_proc = df_proc.dropna(how=\"any\")\n    #df_proc = df_proc.reset_index()\n    \n    X = df_proc.drop(\"y\", axis=1)\n    y = df_proc[\"y\"]\n    \n    #scaler = StandardScaler()\n    #X = scaler.fit_transform(X)\n    \n    if asset_id == 0:\n        model = xgb.XGBRegressor(\n            n_estimators=317,\n            max_depth=8,\n            learning_rate= 0.008967159857886885,\n            subsample=0.8074685834714562,\n            colsample_bytree=0.6156249507619749,\n            missing=-999,\n            random_state=2022,\n            tree_method='gpu_hist'\n        )\n    elif asset_id == 1:\n        model = xgb.XGBRegressor(\n            n_estimators=637,\n            max_depth=13,\n            learning_rate= 0.09253396014321574,\n            subsample=0.700624738784116,\n            colsample_bytree=0.73896289605807,\n            missing=-999,\n            random_state=2022,\n            tree_method='gpu_hist'\n        )\n    elif asset_id == 2:\n        model = xgb.XGBRegressor(\n            n_estimators=452,\n            max_depth=8,\n            learning_rate=  0.4167642609563461,\n            subsample=0.582006239504628,\n            colsample_bytree=0.6829989973347863,\n            missing=-999,\n            random_state=2022,\n            tree_method='gpu_hist'\n        )\n    elif asset_id == 3:\n        model = xgb.XGBRegressor(\n            n_estimators=452,\n            max_depth=8,\n            learning_rate= 0.4167642609563461,\n            subsample=0.5820062395046286,\n            colsample_bytree=0.6829989973347863,\n            missing=-999,\n            random_state=2022,\n            tree_method='gpu_hist'\n        )\n    elif asset_id == 4:\n        model = xgb.XGBRegressor(\n            n_estimators=471,\n            max_depth=9,\n            learning_rate= 0.05918488024797159,\n            subsample=0.7693819367697938,\n            colsample_bytree=0.7266084230952958,\n            missing=-999,\n            random_state=2022,\n            tree_method='gpu_hist'\n        )\n    elif asset_id == 5:\n        model = xgb.XGBRegressor(\n            n_estimators=452,\n            max_depth=8,\n            learning_rate= 0.4167642609563461,\n            subsample=0.582006239504628,\n            colsample_bytree=0.6829989973347863,\n            missing=-999,\n            random_state=2022,\n            tree_method='gpu_hist'\n        )\n    elif asset_id == 6:\n        model = xgb.XGBRegressor(\n            n_estimators=476,\n            max_depth=8,\n            learning_rate= 0.4202769113980745,\n            subsample=0.5563315209270074,\n            colsample_bytree=0.6951993738259458,\n            missing=-999,\n            random_state=2022,\n            tree_method='gpu_hist'\n        )\n    elif asset_id == 7:\n        model = xgb.XGBRegressor(\n            n_estimators=520,\n            max_depth=15,\n            learning_rate=0.19184853364231427,\n            subsample=0.8869731830313443,\n            colsample_bytree=0.6855158027999262,\n            missing=-999,\n            random_state=2022,\n            tree_method='gpu_hist'\n        )\n    elif asset_id == 8:\n        model = xgb.XGBRegressor(\n            n_estimators=471,\n            max_depth=9,\n            learning_rate= 0.05918488024797159,\n            subsample=0.7693819367697938,\n            colsample_bytree=0.7266084230952958,\n            missing=-999,\n            random_state=2022,\n            tree_method='gpu_hist'\n        )\n    elif asset_id == 9:\n        model = xgb.XGBRegressor(\n            n_estimators=229,\n            max_depth=9,\n            learning_rate= 0.23016519709096778,\n            subsample=0.7928998128269837,\n            colsample_bytree=0.5299924747454009,\n            missing=-999,\n            random_state=2022,\n            tree_method='gpu_hist'\n        )\n    elif asset_id == 10:\n        model = xgb.XGBRegressor(\n            n_estimators=229,\n            max_depth=9,\n            learning_rate=0.23016519709096778,\n            subsample=0.7928998128269837,\n            colsample_bytree=0.5299924747454009,\n            missing=-999,\n            random_state=2022,\n            tree_method='gpu_hist'\n        )\n    elif asset_id == 11:\n        model = xgb.XGBRegressor(\n            n_estimators=229,\n            max_depth=9,\n            learning_rate=0.23016519709096778,\n            subsample=0.7928998128269837,\n            colsample_bytree=0.5299924747454009,\n            missing=-999,\n            random_state=2022,\n            tree_method='gpu_hist'\n        )\n    elif asset_id == 12:\n        model = xgb.XGBRegressor(\n            n_estimators=229,\n            max_depth=9,\n            learning_rate=0.23016519709096778,\n            subsample=0.7928998128269837,\n            colsample_bytree=0.5299924747454009,\n            missing=-999,\n            random_state=2022,\n            tree_method='gpu_hist'\n        )\n    elif asset_id == 13:\n        model = xgb.XGBRegressor(\n            n_estimators=121,\n            max_depth=6,\n            learning_rate=0.3062149270836522,\n            subsample=0.7361485971162751,\n            colsample_bytree=0.685244452888315,\n            missing=-999,\n            random_state=2022,\n            tree_method='gpu_hist'\n        )\n    model.fit(X, y)\n    del X\n    del y\n    #return X, y, model\n    return model","36c76d08":"def get_Xy_and_model_for_asset_with_reset_index(df_train, asset_id):\n    df = df_train[df_train[\"Asset_ID\"] == asset_id]\n    \n    df_proc = get_features(df)\n    df_proc['y'] = df['Target']\n    df_proc = df_proc.reset_index(drop=True)\n    df_proc = df_proc.drop(labels=np.where(np.isinf(df_proc))[0], axis=0)\n    df_proc = df_proc.dropna(how=\"any\")\n    df_proc = df_proc.reset_index(drop=True)\n    \n    X = df_proc.drop(\"y\", axis=1)\n    y = df_proc[\"y\"]\n    \n    #scaler = StandardScaler()\n    #X = scaler.fit_transform(X)\n    \n    model = xgb.XGBRegressor(\n        n_estimators=229,\n        max_depth=9,\n        learning_rate=0.23016519709096778,\n        subsample=0.7928998128269837,\n        colsample_bytree=0.5299924747454009,\n        missing=-999,\n        random_state=2022,\n        tree_method='gpu_hist'\n    )\n    model.fit(X, y)\n    del X\n    del y\n    #return X, y, model\n    return model","7096bf71":"%%time\nXs = {}\nys = {}\nmodels = {}\n\nfor asset_id, asset_name in zip(df_assets['Asset_ID'], df_assets['Asset_Name']):\n    print(f\"Training model for {asset_name:<16} (ID={asset_id:<2})\")\n    try:\n        if asset_id == 10:\n            models[asset_id] = get_Xy_and_model_for_asset_with_reset_index(df_train, asset_id)\n            continue\n        #X, y, model = get_Xy_and_model_for_asset(df_train, asset_id) \n        model = get_Xy_and_model_for_asset(df_train, asset_id)\n        models[asset_id] = model\n        #Xs[asset_id], ys[asset_id], models[asset_id] = X, y, model\n    except:         \n        models[asset_id] = None\n        #Xs[asset_id], ys[asset_id], models[asset_id] = None, None, None    ","87387542":"import traceback","76eb6aa7":"models","4466b66a":"for asset_id, asset_name in zip(df_assets['Asset_ID'], df_assets['Asset_Name']):\n    print(f\"Feature importances for {asset_name:<16} (ID={asset_id:<2}) - {models[asset_id].feature_importances_}\")","d3e277b6":"%%time\nenv = gresearch_crypto.make_env()\niter_test = env.iter_test()\n#scaler = StandardScaler()\nfor i, (df_test, df_pred) in enumerate(iter_test):\n    for j , row in df_test.iterrows():\n        if models[row['Asset_ID']] is not None:\n            try:\n                model = models[row['Asset_ID']]\n                x_test = get_features(pd.DataFrame([row]))\n                #x_test = scaler.fit_transform(x_test)\n                y_pred = model.predict(x_test)[0]\n                df_pred.loc[df_pred['row_id'] == row['row_id'], 'Target'] = y_pred\n            except:\n                df_pred.loc[df_pred['row_id'] == row['row_id'], 'Target'] = 0\n                traceback.print_exc()\n        else: \n            df_pred.loc[df_pred['row_id'] == row['row_id'], 'Target'] = 0\n    env.predict(df_pred)","bdff86e5":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session"}}