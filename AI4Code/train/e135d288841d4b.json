{"cell_type":{"be309916":"code","338870eb":"code","fac9df76":"code","79546259":"code","80c818d9":"code","6d4f1008":"code","74eb7a9b":"code","9b67354f":"code","60151ba8":"code","41c128c8":"code","26eb07cd":"code","eb1fcf3c":"code","fdd5ae5d":"code","c016b308":"code","1fbeb0fa":"code","d0477279":"code","361ef2d3":"code","63c0a3ac":"code","bce85322":"markdown","2dd88687":"markdown","b880e235":"markdown","8a96672f":"markdown","91b60186":"markdown","faaa2289":"markdown","bafbe1c5":"markdown","b3d69c4c":"markdown"},"source":{"be309916":"DATA_PATH = '..\/input\/shopee-product-matching\/'\n!ls ..\/input\/shopee-pytorch-xlmroberta-doubles-relativebinclf\/xlm-roberta-large_128_fold0_min_val_loss.pth","338870eb":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport cv2, matplotlib.pyplot as plt\nfrom   tqdm import tqdm_notebook\nimport pickle\nimport gc\nimport torch\nimport torch.nn as nn\nfrom   torch.utils.data import Dataset, DataLoader\n\nfrom   sklearn.preprocessing import normalize\nfrom   transformers import BertConfig, AutoTokenizer, AutoModelForSequenceClassification\nfrom   transformers import RobertaTokenizer, RobertaForSequenceClassification, XLMRobertaModel\nimport cudf, cuml, cupy\nfrom   cuml.feature_extraction.text import TfidfVectorizer\nfrom   cuml.neighbors import NearestNeighbors\n\ndef getMetric(col):\n    def f1score(row):\n        n = len( np.intersect1d(row.target,row[col]) )\n        return 2*n \/ (len(row.target)+len(row[col]))\n    return f1score\n\nCOMPUTE_CV = True\ntest = pd.read_csv (DATA_PATH + 'test.csv')\nif len(test)>3: COMPUTE_CV = False\nelse: print('this submission notebook will compute CV score, but commit notebook will not')","fac9df76":"# COMPUTE_CV = False\n\nif COMPUTE_CV:\n    train = pd.read_csv(DATA_PATH + 'train.csv')\n    train['image'] = DATA_PATH + 'train_images\/' + train['image']\n    tmp = train.groupby('label_group').posting_id.agg('unique').to_dict()\n    train['target'] = train.label_group.map(tmp)\n    # train_gf = cudf.read_csv(DATA_PATH + 'train.csv')\nelse:\n    train = pd.read_csv(DATA_PATH + 'test.csv')\n    train['image'] = DATA_PATH + 'test_images\/' + train['image']\n    # train_gf = cudf.read_csv(DATA_PATH + 'test.csv')\n    \nprint('train shape is', train.shape )\ntrain.head()","79546259":"DEVICE = torch.device ('cuda' if torch.cuda.is_available () else 'cpu')\n\"\"\"\ntest_df = None\nif COMPUTE_CV:\n    test_df = pd.read_csv('..\/input\/shopee-folds\/train_fold.csv')\n    print('Using train as test to compute CV (since commit notebook). Shape is', test_df.shape )\nelse:\n    test_df = pd.read_csv('..\/input\/shopee-product-matching\/test.csv')\n    print('Test shape is', test_df.shape )\n\"\"\"","80c818d9":"if COMPUTE_CV:\n    train_gf = cudf.read_csv(DATA_PATH + 'train.csv')\nelse:\n    train_gf = cudf.read_csv(DATA_PATH + 'test.csv')","6d4f1008":"# title TFIDF\n\n# from sklearn.feature_extraction.text import TfidfVectorizer\nmodel = TfidfVectorizer (stop_words=None, binary=True, max_features=25000)\ntext_embeddings = model.fit_transform (train_gf.title).toarray()\nprint ('text embeddings shape',text_embeddings.shape)\n\npreds = []\nCHUNK = 1024*4\n\nprint('Finding similar titles...')\nCTS = len(train)\/\/CHUNK\nif len(train)%CHUNK!=0: CTS += 1\nfor j in range( CTS ):\n    \n    a = j*CHUNK\n    b = (j+1)*CHUNK\n    b = min(b,len(train))\n    print('chunk',a,'to',b)\n    \n    # COSINE SIMILARITY DISTANCE\n    # cts = np.dot( text_embeddings, text_embeddings[a:b].T).T\n    cts = cupy.matmul(text_embeddings, text_embeddings[a:b].T).T\n    \n    for k in range(b-a):\n        # IDX = np.where(cts[k,]>0.7)[0]\n        IDX = cupy.where(cts[k,]>0.75)[0]\n        o = train.iloc[cupy.asnumpy(IDX)].posting_id.values\n        preds.append(o)        \ndel model, text_embeddings\n\ntrain['oof_text'] = preds\n\nif COMPUTE_CV:\n    train['f1'] = train.apply(getMetric('oof_text'),axis=1)\n    print('CV score for tfidf baseline =',train.f1.mean())","74eb7a9b":"tmp = train.groupby('image_phash').posting_id.agg('unique').to_dict()\ntrain['oof_hash'] = train.image_phash.map(tmp)","9b67354f":"if COMPUTE_CV:\n    train['f1'] = train.apply(getMetric('oof_hash'),axis=1)\n    print('CV score for phash baseline =',train.f1.mean())\n# imagehash","60151ba8":"!ls ..\/input\/shopee-siamese-effnetb6-img224\/efficientnet_b6_224_fold0_min_val_loss.pth\nIMG_MODEL_PATH = \"..\/input\/shopee-siamese-effnetb6-img224\/efficientnet_b6_224_fold0_min_val_loss.pth\"","41c128c8":"import sys\nsys.path.append ('\/kaggle\/input\/timm-pytorch-image-models\/pytorch-image-models-master')\nimport timm\nimport albumentations as A\nfrom   albumentations.pytorch import ToTensorV2\n\nIMG_SIZE = 224\nvalid_transforms = A.Compose ([\n    A.Resize (IMG_SIZE, IMG_SIZE),\n    A.Normalize (),\n    ToTensorV2 (p=1.0),\n])\n\nclass SiameseImageDataset (Dataset):    \n    def __init__(self, img_paths, transform=valid_transforms):\n        self.img_paths = img_paths\n        self.transform = transform\n        \n    def __getitem__(self, index):\n        \n        img_path = self.img_paths[index]\n        img      = self.transform (image=cv2.imread (img_path))['image']        \n        return img\n    \n    def __len__(self):\n        return len (self.img_paths)\n    \nclass ShopeeNet (nn.Module):\n\n    def __init__(self,\n                 n_classes=512,\n                 model_name='efficientnet_b6',\n                 use_fc=False,\n                 fc_dim=0,\n                 dropout=0.1,\n                 pretrained=False):\n        \"\"\"\n        :param n_classes:\n        :param model_name: name of model from pretrainedmodels\n            e.g. resnet50, resnext101_32x4d, pnasnet5large\n        \"\"\"\n        super(ShopeeNet, self).__init__()\n        print('Building Model Backbone for {} model'.format(model_name))\n        \n        self.backbone = timm.create_model (model_name, pretrained=pretrained)\n        final_in_features = self.backbone.classifier.in_features\n        \n        self.backbone.classifier = nn.Identity()\n        self.backbone.global_pool = nn.Identity()\n        \n        self.pooling =  nn.AdaptiveAvgPool2d(1)\n        \n        self.use_fc = use_fc\n        if use_fc:\n            self.dropout = nn.Dropout(p=dropout)\n            self.fc = nn.Linear(final_in_features, fc_dim)\n            self.bn = nn.BatchNorm1d(fc_dim)\n            self._init_params()\n            final_in_features = fc_dim\n        \n        self.final = nn.Linear (final_in_features, n_classes)\n    \n    def freeze (self):\n        \n        for param in self.parameters ():\n            param.requires_grad = False        \n        for param in self.backbone.parameters ():\n            param.requires_grad = False\n        \n        for param in self.final.parameters ():\n            param.requires_grad = True\n        if self.use_fc:\n            for param in self.fc.parameters ():\n                param.requires_grad = True\n            for param in self.bn.parameters ():\n                param.requires_grad = True\n        return\n    \n    def unfreeze (self):\n        \n        for param in self.backbone.parameters ():\n            param.requires_grad = True\n        for param in self.parameters ():\n            param.requires_grad = True\n            \n        for param in self.final.parameters ():\n            param.requires_grad = True\n        if self.use_fc:\n            for param in self.fc.parameters ():\n                param.requires_grad = True\n            for param in self.bn.parameters ():\n                param.requires_grad = True\n        return\n    \n    def _init_params (self):\n        nn.init.xavier_normal_(self.fc.weight)\n        nn.init.constant_(self.fc.bias, 0)\n        nn.init.constant_(self.bn.weight, 1)\n        nn.init.constant_(self.bn.bias, 0)\n    \n    def forward(self, x):\n        feature = self.extract_feat (x)\n        logits = self.final (feature)\n        return logits\n    \n    def extract_feat (self, x):\n        batch_size = x.shape[0]\n        x = self.backbone (x)\n        x = self.pooling (x).view (batch_size, -1)\n\n        if self.use_fc:\n            x = self.dropout (x)\n            x = self.fc (x)\n            x = self.bn (x)\n        return x\n    \nclass TripletModel (nn.Module):\n    \n    def __init__(self, embeddingModel):\n        \n        super (TripletModel, self).__init__()\n        self.embeddingModel = embeddingModel\n    \n    def forward (self, i1, i2, i3):\n        \n        E1 = self.embeddingModel (i1)\n        E2 = self.embeddingModel (i2)\n        E3 = self.embeddingModel (i3)\n        return E1, E2, E3\n    \n    def freeze (self):        \n        self.embeddingModel.freeze ()\n        return\n    \n    def unfreeze (self):\n        self.embeddingModel.unfreeze ()\n        return\n    \nimgmodel = ShopeeNet ()\nimgmodel = TripletModel (imgmodel)\nimgmodel.load_state_dict (torch.load (IMG_MODEL_PATH, map_location=torch.device ('cpu'))['model_state_dict'])\nimgmodel = imgmodel.embeddingModel","26eb07cd":"imagedataset = SiameseImageDataset (\n    train['image'].values,\n    valid_transforms)\n\nimageloader = torch.utils.data.DataLoader(\n    imagedataset,\n    batch_size=256, shuffle=False, num_workers=8\n)","eb1fcf3c":"DEVICE = 'cuda'\n\nimgmodel = imgmodel.to (DEVICE)\n\nimagefeat = []\nwith torch.no_grad():\n    for data in tqdm_notebook(imageloader):\n        data = data.to(DEVICE)\n        feat = imgmodel(data)\n        feat = feat.reshape(feat.shape[0], feat.shape[1])\n        feat = feat.data.cpu().numpy()        \n        imagefeat.append(feat)","fdd5ae5d":"# l2 norm to kill all the sim in 0-1\n# 3.8w * 512\n# \u5f52\u4e00\u5316\nimagefeat = np.vstack (imagefeat)\n# imagefeat = normalize(imagefeat)","c016b308":"gc.collect ()\n\nprint('Finding similar titles...')\n\n# Find nearest neighbours e.g using Siamese embeddings\nmodel = NearestNeighbors (n_neighbors = 50)\nmodel.fit (imagefeat)\ndistances, indices = model.kneighbors (imagefeat)\nthresholds = list (np.arange (1, 10, 1))  # 0.04\nscores = []\nfor threshold in thresholds:\n\n    preds = []\n    for k in range (imagefeat.shape[0]):\n        idx = np.where (distances[k,] < threshold)[0]\n        ids = indices[k,idx]\n        posting_ids = train.iloc[cupy.asnumpy(ids)].posting_id.values\n        preds.append (posting_ids)\n    train['oof_bert'] = preds\n    train['f1'] = train.apply (getMetric ('oof_bert'), axis=1)\n    score = train.f1.mean ()\n    print (f'Our f1 score for threshold {threshold} is {score}')\n    scores.append (score)\nthresholds_scores = pd.DataFrame ({'thresholds': thresholds, 'scores': scores})\nmax_score = thresholds_scores[thresholds_scores['scores'] == thresholds_scores['scores'].max()]\nbest_threshold = max_score['thresholds'].values[0]\nbest_score = max_score['scores'].values[0]\nprint(f'Our best score is {best_score} and has a threshold {best_threshold}')","1fbeb0fa":"train['oof_cnn'] = preds\n\nif COMPUTE_CV:\n    train['f1'] = train.apply(getMetric('oof_cnn'),axis=1)\n    print('CV score for cnn baseline =',train.f1.mean())","d0477279":"def combine_for_sub(row):\n    x = np.concatenate([row.oof_text, row.oof_cnn]) #, row.oof_hash])\n    return ' '.join( np.unique(x) )\n\ndef combine_for_cv(row):\n    x = np.concatenate([row.oof_text, row.oof_cnn]) #, row.oof_hash])\n    return np.unique(x)","361ef2d3":"if COMPUTE_CV:\n    tmp = train.groupby('label_group').posting_id.agg('unique').to_dict()\n    train['target'] = train.label_group.map(tmp)\n    train['oof'] = train.apply(combine_for_cv,axis=1)\n    train['f1'] = train.apply(getMetric('oof'),axis=1)\n    print('CV tfidf+cnn+phash Score =', train.f1.mean() )\n\ntrain['matches'] = train.apply(combine_for_sub,axis=1)","63c0a3ac":"train[['posting_id','matches']].to_csv('submission.csv',index=False)\nsub = pd.read_csv('submission.csv')\nsub.head()","bce85322":"> # image hash ","2dd88687":"from scipy.spatial.distance import cdist\n\n# Load data (deserialize)\nwith open('..\/input\/k\/sapthrishi007\/shopee-img-txt-knn-inf-v2\/text_embeddings.np', 'rb') as handle:\n    text_embeddings = pickle.load (handle)\n\ntext_embeddings = cupy.array (text_embeddings)\ngc.collect ()\nfor thresh in [0.9993]:\n    \n    preds = []\n    CHUNK = 1024*4\n    print('Finding similar titles...')\n\n    \"\"\"\n    # Find nearest neighbours e.g using Siamese embeddings\n    model = NearestNeighbors (n_neighbors = 10)\n    model.fit (text_embeddings)\n    distances, indices = model.kneighbors (text_embeddings)\n    thresholds = list (np.arange (0.5, 1.5, 0.1))\n    scores = []\n    for threshold in thresholds:\n\n        preds = []\n        for k in range (text_embeddings.shape[0]):\n            idx = np.where (distances[k,] < threshold)[0]\n            ids = indices[k,idx]\n            posting_ids = train.iloc[cupy.asnumpy(ids)].posting_id.values\n            preds.append (posting_ids)\n        train['oof_bert'] = preds\n        train['f1'] = train.apply (getMetric ('oof_bert'), axis=1)\n        score = train.f1.mean ()\n        print (f'Our f1 score for threshold {threshold} is {score}')\n        scores.append (score)\n    thresholds_scores = pd.DataFrame ({'thresholds': thresholds, 'scores': scores})\n    max_score = thresholds_scores[thresholds_scores['scores'] == thresholds_scores['scores'].max()]\n    best_threshold = max_score['thresholds'].values[0]\n    best_score = max_score['scores'].values[0]\n    print(f'Our best score is {best_score} and has a threshold {best_threshold}')\n    # OR :-\n    \"\"\"\n    \n    CTS = len (train) \/\/ CHUNK\n    if len(train)%CHUNK!=0: CTS += 1\n    for j in range( CTS ):\n\n        a = j*CHUNK\n        b = (j+1)*CHUNK\n        b = min(b,len(train))\n        print('chunk',a,'to',b)\n\n        # COSINE SIMILARITY DISTANCE\n        # cts = np.dot( text_embeddings, text_embeddings[a:b].T).T\n        cts = cupy.matmul(text_embeddings, text_embeddings[a:b].T).T\n\n        for k in range(b-a):\n            # IDX = np.where(cts[k,]>0.7)[0]\n            IDX = cupy.where(cts[k,] > thresh)[0]\n            o = train.iloc[cupy.asnumpy(IDX)].posting_id.values\n            preds.append(o)\n\n    train['oof_bert'] = preds\n    if COMPUTE_CV:\n        train['f1'] = train.apply (getMetric ('oof_bert'), axis=1)\n        print(f'CV score for bert-gap thresh {thresh} baseline =', train.f1.mean ())\n\n        # del txt_model, text_embeddings, text_dataset, text_dl, test_df\ndel text_embeddings; gc.collect ()","b880e235":"# Text TFIDF","8a96672f":"## CV score for bert-gap thresh 0.9993 baseline = 0.5149385335423771","91b60186":"bert_model_name = '..\/input\/xlm-roberta-large'\nmax_len         = 128\ntokenizer       = AutoTokenizer.from_pretrained (bert_model_name)\nTXT_MODEL_PATH  = \"..\/input\/shopee-pytorch-xlmroberta-doubles-relativebinclf\/xlm-roberta-large_128_fold0_min_val_loss.pth\"\n\ndef encode (premise):\n    \n    encoded_dict = tokenizer (\n        premise,                   # 1st of the Sentence pair to encode.\n        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n        truncation=True,           # just max_len will not automatically truncate\n        max_length = max_len,      # Pad & truncate all sentences.\n        padding='max_length',\n        return_attention_mask = True,   # Construct attn. masks.\n        return_tensors = 'pt',     # Return pytorch tensors.\n    ) \n    # print ('encoded_dict =', encoded_dict)\n    # 1-D tensors are expected for a sample. Hence squeeze these 2-D tensors e.g [1,256] shaped tensors to 1-D [256] shape \n    for k in encoded_dict:\n        encoded_dict[k] = torch.squeeze (encoded_dict[k])\n    return encoded_dict\n\nclass TextDataset (Dataset):\n    \n    def __init__(self, df):\n        self.df = df\n        return\n    \n    def __getitem__(self, index):\n        \n        title = self.df['title'][index]\n        title = encode (title)\n        return title\n    \n    def __len__(self):\n        return self.df.shape[0]\n\n\nclass MyGAPModelForSeqClf (nn.Module):\n    \n    def __init__(self, bert_model_name=bert_model_name, outputCount=1, drop_prob=0.2, nonlin=nn.SiLU ()):\n        \n        super (MyGAPModelForSeqClf, self).__init__()\n        self.model       = AutoModelForSequenceClassification.from_pretrained (bert_model_name).base_model  # adding .base_model if using pretrained XLMRobertaForSequenceClassification\n        \"\"\"\n        self.drop_prob   = drop_prob\n        self.nonlin      = nonlin\n        self.outputCount = outputCount\n        hidden_size      = self.model.config.hidden_size\n        self.dense       = nn.Linear (hidden_size, hidden_size)\n        self.batchnorm   = nn.BatchNorm1d (hidden_size)\n        self.outDense    = nn.Linear (hidden_size, outputCount)\n        self.dropout     = nn.Dropout (drop_prob) \"\"\"\n        return\n    \n    def freeze (self):\n        \n        for param in self.model.base_model.parameters ():\n            param.requires_grad = False\n        return\n    \n    def unfreeze (self):\n        \n        for param in self.model.base_model.parameters ():\n            param.requires_grad = True\n        return\n    \n    def forward (self, input_ids, attention_mask, token_type_ids=None, labels=None, **kwargs):\n        \n        last_hidden_states = None\n        \n        # The base bert model do not take labels as input\n        if token_type_ids is None:\n            moutput = self.model (input_ids=input_ids, attention_mask=attention_mask)\n            last_hidden_states = moutput[0]\n        else:\n            moutput = self.model (input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n            last_hidden_states = moutput[0]\n        #print('last_hidden_states.size=', last_hidden_states.size())\n        \n        # GAP: last_hidden_states shape = batch_size * max_seq_len * emb_dim(1024?)\n        # output shape = batch_size * emb_dim(1024?)  i.e avg across the sequence\n        last_hidden_states = torch.mean (last_hidden_states, 1)             #;print('GAP last_hidden_states.size=', last_hidden_states.size())\n        # OR\n        # Use just the 1st [CLS] embedding\n        # last_hidden_states = last_hidden_states[:, 0, :]                      #;print ('last_hidden_states.shape =', last_hidden_states.shape)\n        return last_hidden_states\n    \ntext_model = MyGAPModelForSeqClf (bert_model_name)\n# load the pretrained model which was trained this code only (by commenting out these 4 line)\n# try:\n#     text_model.load_state_dict (torch.load (TXT_MODEL_PATH)['model_state_dict'])\n# except:\n#     text_model.load_state_dict (torch.load (TXT_MODEL_PATH, map_location='cpu')['model_state_dict'])\n    \nimport warnings\nwarnings.filterwarnings (\"ignore\")\ntext_model.to (DEVICE)\n\ntext_dataset = TextDataset (train)\ntext_dl = DataLoader (text_dataset, batch_size=10, num_workers=4)\ntext_embeddings = []\n\nfor batch in text_dl:\n    with torch.no_grad ():        \n        for k in batch:\n            batch[k] = batch[k].to (DEVICE)        \n        emb = text_model (**batch)\n        text_embeddings.append (emb.cpu ().numpy ())\n\ntext_embeddings = np.vstack (text_embeddings)\ntext_embeddings = normalize (text_embeddings)\n# Store data (serialize)\nwith open ('text_embeddings.np', 'wb') as handle:\n    pickle.dump (text_embeddings, handle)\n\ndel text_model\ngc.collect ()\ntorch.cuda.empty_cache ()","faaa2289":"# image Siamese","bafbe1c5":"# Text BERT","b3d69c4c":"[](http:\/\/)!ls ..\/input\/shopee-img-txt-knn-inf-v2\/text_embeddings.np"}}