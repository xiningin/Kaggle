{"cell_type":{"193b0bce":"code","5482e167":"code","b1f71de4":"code","3dd85c80":"code","7c59179f":"code","e3e3b388":"code","f908e962":"code","b48f0fc2":"code","735b6f2f":"code","d8a240eb":"code","ec248227":"code","e4b5362a":"code","dac1190c":"code","3f1f667f":"code","73f25e8c":"code","1d430e23":"code","82420b87":"markdown","fe4ae55b":"markdown","6361c388":"markdown","f07ef7da":"markdown","e6432d08":"markdown","4318fdb4":"markdown","8d7e533e":"markdown","90a06894":"markdown","54da6bd4":"markdown","1afa9f50":"markdown","d4a1b502":"markdown","4e1fe746":"markdown","1308232c":"markdown","3f44cdba":"markdown","242b9444":"markdown"},"source":{"193b0bce":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session\n","5482e167":"\ndataset_path = '..\/input\/netflix-shows\/netflix_titles.csv'\ndata = pd.read_csv(dataset_path)\ndata.head()","b1f71de4":"import urllib.parse\nfrom bs4 import BeautifulSoup\nimport requests\nimport numpy as np \nimport pandas as pd \nfrom threading import Thread\nimport threading\nimport math\nimport time\nfrom IPython.display import display # If you want to render pandas local, you should replace data.head() to display(data.head())\n\ndataset_path = '..\/input\/netflix-shows\/netflix_titles.csv'\n# suggestion_show_path = '..\/output\/netflix-shows\/suggestion.csv'\n# imdb_dataset_path = '..\/output\/netflix-shows\/imdb_dataset.csv'\n\ndata = pd.read_csv(dataset_path)\n","3dd85c80":"headers = {'User-Agent': 'Mozilla\/5.0 (X11; Linux x86_64) AppleWebKit\/537.36 (KHTML, like Gecko) Chrome\/47.0.2526.106 Safari\/537.36'}","7c59179f":"thread_num = 8 # Base on your computer.\n\nsuggest_imdb = 0 # That variable used to determine what rate are useful for user. between 0 and 10\n\ndef main():\n    print(\"Starting crawl data\")\n    num_shows = int(len(data.index))\n    print(num_shows)\n    imdb_list = list(range(0,num_shows))\n    steps = []\n    start = 0\n    end  = 0\n    start_time = time.time()\n\n    condition = num_shows%thread_num == 0\n    loop = thread_num if condition else thread_num -1\n    \n    each_row = int(num_shows\/thread_num if condition else math.ceil(num_shows\/thread_num))\n    for index in range(0,loop):\n        end = start+each_row-1\n        steps.append((start,end))\n        start = end+1\n        \n    if not condition:\n        steps.append((start,num_shows))\n    print(steps)\n\n    # Create thread, and then execute them, we will talk about that section below.\n    lst_thread = []\n    for thread in steps:\n        print('Start Thread')\n        th = threading.Thread(target=execute_analyze_data,args=(thread[0],thread[1],imdb_list,))\n        th.start()\n        lst_thread.append(th)\n    \n    for th in lst_thread:\n        th.join()\n\n    data.insert(2, \"IMDb\", imdb_list, True) \n    data.to_csv('data_with_imdb.csv', sep=',', encoding='utf-8', index=False)\n    print(\"Done crawl data: \", time.time() - start_time, \"s\")\n    select_top_imdb(data, suggest_imdb)","e3e3b388":"def select_top_imdb(data, suggest_imdb):\n    print(\"Starting select suggestion show\")\n    select = data.loc[data[\"IMDb\"] >= suggest_imdb]\n    select.to_csv('suggest_movie.csv', sep=',', encoding='utf-8', index=False)\n    # print(\"Done: \", time.time() - now, \"s\")","f908e962":"def execute_analyze_data(start,end, imdb_list):\n    href_link = get_list_id(data,start,end)\n    get_imdb_rate(imdb_list, data,href_link,start,end)","b48f0fc2":"def get_list_id(data, start, end):\n    href_link = []\n    for index in range(start,end):\n        print(index,\"-Process get title id: \", data[\"title\"][index], end=\"\\n\")\n\n        values = {'q':data[\"title\"][index]} \n        query = urllib.parse.urlencode(values) \n        query_find = 'https:\/\/imdb.com\/find?{}'.format(query)\n        response = requests.get(query_find, headers=headers)\n        resp = response.content\n        html = BeautifulSoup(resp,'html.parser')\n        result = html.findAll(\"td\", {\"class\": \"result_text\"})\n        if len(result) > 0:\n            isNew = False\n            for item in result:\n                if item.text.find(\"({})\".format(data[\"release_year\"])):\n                    href_link.append(item.a[\"href\"])\n                    isNew = True\n                    break;\n            if not isNew:\n                href_link.append(None)\n                continue\n        else:\n            href_link.append(None)\n    return href_link","735b6f2f":"def get_imdb_rate(imdb_list, data, href_link, start, end):\n    i = 0\n    for index in range(start,end):\n        print(index,\"Process get imdb: \", data[\"title\"][index],  end=\"\\n\")\n        if href_link[i]:\n            query_find = 'https:\/\/imdb.com{}'.format(href_link[i])\n            response = requests.get(query_find, headers=headers)\n            resp = response.content\n            html = BeautifulSoup(resp,'html.parser')\n            result = html.findAll(\"div\", {\"class\": \"ratingValue\"})[0]\n            imdb_list[index] = float(result.strong.span.text)\n        else:\n            imdb_list[index] = -1\n        i +=1\n    return imdb_list","d8a240eb":"# if __name__ == \"__main__\":\n#     main()","ec248227":"# After crawl dataset.\ndataset_path = '..\/input\/netflix-title-imdb\/netflix_titles_imdb.csv'\ndata = pd.read_csv(dataset_path)\ndata.head()","e4b5362a":"#Drop unexpected dataset ||| IF need\ndata.info()","dac1190c":"data = data.loc[data[\"IMDb\"]<=10] # To ensure.\ndata = data.dropna(subset=['IMDb'])\ndata.info()","3f1f667f":"from collections import Counter\nfrom matplotlib import gridspec\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nt_data = data.dropna(subset=['country'])\n\npivot_columns = \"country\"\n\n# Query the movie type and get it duration\npivot_data = t_data[pivot_columns]\n\n# Because some data have invalid token space, so that make pandas count wrong. We need to strip that illegular and then sort\npivot_count = pd.Series(dict(Counter(','.join(pivot_data).replace(' ,',',').replace(', ',',').split(',')))).sort_values(ascending=False)\ntop15 = pivot_count.head(15)\n\nIMDb_data = []\nfor country in top15.index:\n    IMDb_data.append(t_data[t_data[pivot_columns].str.contains(country)].mean()[\"IMDb\"])\n    \n    \nfig = plt.figure(figsize=(20, 10))\ngs = gridspec.GridSpec(nrows=1, ncols=2, height_ratios=[8], width_ratios=[10, 5])\nax1 = sns.set_style(style=None, rc=None )\nfig, ax1 = plt.subplots(figsize=(30,10))\nax1.set_ylabel('Number of contributions', fontsize=18)\nax2 = ax1.twinx()\nax2.set_ylabel('IMDb', fontsize=18)\nsns.lineplot(x=top15.index, y = IMDb_data, marker='o', sort = False, ax=ax2)\nsns.barplot(data = t_data, x=top15.index, y=top15, alpha=0.5, ax=ax1, palette=\"mako\")","73f25e8c":"# Next code, we're going to groupby the duration, that's why we create more column is use to measure the data\ndata['duration_int'] = data.apply(lambda row: int(row[\"duration\"].split(' ')[0]) , axis=1)\ndata.head()","1d430e23":"pivot_columns = 'duration'\npivot_data = data[data[\"type\"] == \"Movie\"][pivot_columns]\npivot_data.head(100)\nfig, ax1 = plt.subplots(figsize=(30,10))\n\nsns.distplot(pivot_data.astype(str).str.extract('(\\d+)'),kde=False,color=['red'])\npivot_data = data[data[\"type\"] == \"Movie\"].groupby(by=pivot_columns).agg(['mean'])[['duration_int','IMDb']]\nnframe =  pd.DataFrame()\n# nframe['duration'] = pivot_data['duration_int']['mean']\nnframe['IMDb'] = pivot_data['IMDb']['mean']\nnframe['duration_'] = pivot_data['duration_int']['mean']\nnframe = nframe.sort_values('duration_',ascending=True)\nnframe = nframe.drop(columns=['duration_'])\nnframe.head()\nax1.set_ylabel('Number of contributions', fontsize=18)\nax2 = ax1.twinx()\nax2.set_ylabel('IMDb', fontsize=18)\nsns.lineplot(data=nframe, marker='o', sort = False, ax=ax2)\nax2.set(xticklabels=[])","82420b87":"I have splice task of each process into 2 stage:\n1. Get hyper link stage: I want to catch all link from imdb.com. You know, after crawling data then we can take the link to access main page to get IMDb. **But, please note that not all TV shows and Movies in Netflix has IMDb rate**","fe4ae55b":"Firstly, we should render the relatively connection of country and IMDb. Maybe we can catch some information here.","6361c388":"After all, crawl one more times to catch IMDb score. All my access is base all BeautifulSoup parsing, and the key is on the HTML side.","f07ef7da":"First, we have to import some useful library to connect with website","e6432d08":"Luckily, the not null value of duration is equal with IMDb.","4318fdb4":"Because the dataset is very large, so if you want to connect and crawl data more quickly, u have to understand multi-threading to improve performance. Moreover, if you want to get more speed, you can create more processes to run. For me, I take about 15 minutes to catch all IMDb from imdb.com. Actually, the request take not too much performance, but the waiting time is too long. Therefore, it's very good for me to reduce waste time.","8d7e533e":"As we can see at that dataset, show_id and Movie is not belonging to anything we can get IMDb from, but that's helpful to suggest to the client. More over, realse_year can be used to determine exactly movie in the action connect to imdb.com and search movie by **title**.","90a06894":"Header of the request is important too. If your header is unreliable, your request will take long time to be response","54da6bd4":"Oops, how about the duration of each **movie**. Does it important for watcher to rate the movies ???. Let see.\n\nBefore doing anything, i will create a duration with int-type (minute first) for movie","1afa9f50":"Fortunately, It's working and we get IMDb here.","d4a1b502":"After crawling data, the system should analyze movies that satisfy user conditions in the upper. But don't worry about data which is not eligible for condition. Because i have create one more file to save all record \"**data_with_imdb**\"","4e1fe746":"Now, let create a middle method to link from base process to child task. Each process has its crawl assignment between *start* and *end* variable.","1308232c":"Ok, that's everything we need. \n**Note that, because kaggle is prevent request in\/out. So if you want to catch the IMDb, you should do it in your local computer. With my computer, it take about 15 minutes to crawl all IMDb**","3f44cdba":"Horay, let check our new dataset.","242b9444":"Humn, about 7372\/7787 row have IMDb, that's good for me."}}