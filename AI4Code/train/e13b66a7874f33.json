{"cell_type":{"f2cfd7f0":"code","62f3294e":"code","72d94d18":"code","282e143b":"code","0d223072":"code","6c9fdd90":"code","465842ea":"code","4d188315":"code","71f2195d":"code","3ef0c7ad":"code","cab619ef":"code","c349d1d6":"code","2337f185":"code","2b7fb5b5":"code","0710295f":"code","4f888bd7":"code","8c82896f":"code","7e244ba0":"code","49d86e2d":"code","809baa3f":"code","302f2fb1":"code","0d5540d1":"code","6067c7bf":"code","8d010fe9":"code","774ba411":"code","d71e5d45":"code","967a5fee":"code","2178de1a":"code","5f71ee98":"code","8bda19c4":"code","eb6dd33b":"code","ef500895":"code","f5cfe353":"code","1775ad97":"code","d912c07b":"code","600bf2ea":"code","41ab004f":"code","1f6b3e66":"code","b27a7511":"code","0dcf426f":"code","0f7b6f24":"code","1077e8fb":"code","b785fec9":"code","8abab9a5":"code","b1dc76b8":"code","f32a2f38":"code","e6b00a29":"code","c42bdf0d":"code","98491116":"code","83be940c":"code","a3f48aff":"code","e878c43e":"markdown","c06672e9":"markdown","ce7ef647":"markdown","bbdb2812":"markdown","b7fa7026":"markdown","4d34836e":"markdown","3608e707":"markdown","dfbfe34f":"markdown","fd8ceae5":"markdown","38f1813d":"markdown","6a6c9b60":"markdown","42ad5d91":"markdown","e17264ee":"markdown","18f8b2d4":"markdown","93410738":"markdown","2b1fb916":"markdown","229fa6fc":"markdown","248c7085":"markdown","39efdd5f":"markdown","8d4da6eb":"markdown","3c6f9cfb":"markdown","72aae1d8":"markdown","5d7ea5e7":"markdown"},"source":{"f2cfd7f0":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns #plotting package\nimport matplotlib.pyplot as plt  # Matlab-style plotting\nfrom sklearn.preprocessing import LabelEncoder\n\nfrom scipy import stats\nfrom scipy.stats import norm, skew \n\n\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\")) \n\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n\n# Any results you write to the current directory are saved as output.","62f3294e":"# download .csv file from Kaggle Kernel\n\nfrom IPython.display import HTML\nimport pandas as pd\nimport numpy as np\nimport base64\n\n# function that takes in a dataframe and creates a text link to  \n# download it (will only work for files < 2MB or so)\ndef create_download_link(df, title = \"Download CSV file\", filename = \"data.csv\"):  \n    csv = df.to_csv()\n    b64 = base64.b64encode(csv.encode())\n    payload = b64.decode()\n    html = '<a download=\"{filename}\" href=\"data:text\/csv;base64,{payload}\" target=\"_blank\">{title}<\/a>'\n    html = html.format(payload=payload,title=title,filename=filename)\n    return HTML(html)\n\n# create a random sample dataframe\ndf = pd.DataFrame(np.random.randn(50, 4), columns=list('ABCD'))\n\n# create a link to download the dataframe","72d94d18":"# Read data \ntrain_data = pd.read_csv(\"..\/input\/train.csv\")\ntest_data = pd.read_csv(\"..\/input\/test.csv\")","282e143b":"# Take a quick look at the data structure\ntrain_data.head()","0d223072":"# Take a quick look at the data structure\ntest_data.head()","6c9fdd90":"# Take a quick look at the data structure\nprint(\"The size of train data\", train_data.shape)\nprint(\"The size of test data\", test_data.shape)\n\n# From the above, you can notice that train and test data almost \n# share the same number of columns, except \"Sale Price\" in the training set. \n\n# total number of rows in training data set\n# ntrain = train_data.shape[0]","465842ea":"# The `info()` method is useful to get a quick description of the data, in particular \n# the total number of rows and each attributes types and number of non-null values.\ntrain_data.info()","4d188315":"#correlation matrix\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ncorrmat = train_data.corr()\nf, ax = plt.subplots(figsize=(20, 9))\nsns.heatmap(corrmat, vmax=.8, annot=True);","71f2195d":"# Correlation matrix (heatmap style).\n# correlation matri\n\ncorrmat = train_data.corr()\nf, ax = plt.subplots(figsize=(12, 9))\nsns.heatmap(corrmat, vmax=.8, square=True); ","3ef0c7ad":"# most correlated features\ncorrmat = train_data.corr()\ntop_corr_features = corrmat.index[abs(corrmat[\"SalePrice\"])>0.5]\nplt.figure(figsize=(10,10))\ng = sns.heatmap(train_data[top_corr_features].corr(),annot=True,cmap=\"RdYlGn\")","cab619ef":"# Scatter Plot for numerical variables\n\nli_cat_feats = [ 'TotalBsmtSF', '1stFlrSF', 'GrLivArea', 'GarageArea', 'LotFrontage', 'LotArea', 'MasVnrArea']   \ntarget = 'SalePrice'\nnr_rows = 2\nnr_cols = 4\n\nfig, axs = plt.subplots(nr_rows, nr_cols, figsize=(nr_cols*4,nr_rows*3))\n\nfor r in range(0,nr_rows):\n    for c in range(0,nr_cols):  \n        i = r*nr_cols+c\n        if i < len(li_cat_feats):\n            sns.scatterplot(x=li_cat_feats[i], y=target, data=train_data, ax = axs[r][c])\n    \nplt.tight_layout()    \nplt.show()  ","c349d1d6":"#li_cat_feats = list(categorical_feats)\n\n# Box plot for categorical variables\n\nli_cat_feats = ['OverallQual','GarageCars', 'YearBuilt', 'YearRemodAdd', 'Fireplaces', 'GarageYrBlt', 'TotRmsAbvGrd']\ntarget = 'SalePrice'\nnr_rows = 2\nnr_cols = 4\n\nfig, axs = plt.subplots(nr_rows, nr_cols, figsize=(nr_cols*4,nr_rows*3))\n\nfor r in range(0,nr_rows):\n    for c in range(0,nr_cols):  \n        i = r*nr_cols+c\n        if i < len(li_cat_feats):\n            sns.boxplot(x=li_cat_feats[i], y=target, data=train_data, ax = axs[r][c])\nplt.tight_layout()    \nplt.show()","2337f185":"# Copy\/Paste the the ID of both training and testing dataset for future use\n# We will be using ID, when we make submission to the compitions\ntrain_ID = train_data['Id']\ntest_ID = test_data['Id']","2b7fb5b5":"# Drop the ID as they are not necessary for SalePrice prediction\ntrain_data.drop(\"Id\", axis = 1, inplace = True)\ntest_data.drop(\"Id\", axis = 1, inplace = True)","0710295f":"def plot_dist_norm(dist, title):\n    sns.distplot(dist, fit=norm);\n    (mu, sigma) = norm.fit(dist);\n    plt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)], loc='best')\n    plt.ylabel('Frequency')\n    plt.title(title)\n    fig = plt.figure()\n    res = stats.probplot(dist, plot=plt)\n    plt.show()","4f888bd7":"# Plit SalePrice Distribution\nplot_dist_norm(train_data['SalePrice'], 'SalePrice Distribution')\n","8c82896f":"# transform the SalePrice\ntransform_log = np.log1p(train_data[\"SalePrice\"])\nplot_dist_norm(transform_log, 'log(SalePrice) Distribution') \n\n# Copy back the SalePrice to train_data \ntrain_data[\"SalePrice\"] = transform_log","7e244ba0":"var = 'GrLivArea'\ndata = pd.concat([train_data['SalePrice'], train_data[var]], axis=1)\ndata.plot.scatter(x=var, y='SalePrice')","49d86e2d":"# Show me ourliers\ntrain_data[train_data.GrLivArea > 4500] \n\n# The outlier exists at 523 and 1298 index \n# To handle the outlier, we need to do two things\n# (1) Remove the 523 and 1298 rows from train_data\n# (2) Remove the 523 and 1298 rows from SalePrice","809baa3f":"# (1)\n# As you can see that there are some outliers in GrLivArea. That may harm ML model. \n# Let's remove these outliers \n# Remove GrLivArea value greater than 4500 (See the above image)\ntrain_data = train_data[train_data.GrLivArea < 4500]","302f2fb1":"# (2)\n# Remove the 523 and 1298 rows from SalePrice as the corresponding \n# train_data  are outlier and they are removed \n\ntransform_log = transform_log.drop([523 , 1298], axis=0)","0d5540d1":"all_data = pd.concat((train_data, test_data)).reset_index(drop=True)","6067c7bf":"# Drop \"SalePrice\" as it is our target variable\nall_data.drop(['SalePrice'], axis=1, inplace=True)","8d010fe9":"# get information about missing values \ntotal = all_data.isnull().sum().sort_values(ascending=False)\npercent = (all_data.isnull().sum()\/all_data.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data[total > 0]","774ba411":"# Imputation\n# Our stratergy is to fill NA values with Mode of the column for some columns (Not All columnns!!!) as that value is appearing frequently\n# This is \"Data Scientist's Call!!\" (There is no rule of thumb)\n\nfor col in ['SaleType', 'KitchenQual' , 'Exterior2nd' , 'Exterior1st' , 'Electrical' , 'Functional' , 'MSZoning' ] :\n    all_data[col] = all_data[col].fillna(all_data[col].mode()[0]) \n    \n# Fill NA with Zero\nfor col in ['BsmtFinSF1', 'BsmtFinSF2' , 'GarageCars' , 'GarageArea' , 'TotalBsmtSF' , 'BsmtUnfSF' , 'BsmtHalfBath' , 'BsmtFullBath' , 'MasVnrArea' ]:\n    all_data[col]=all_data[col].fillna(0)\n\n# fill NA with None\nfor col in ['MasVnrType','FireplaceQu' , 'Fence' , 'PoolQC' , 'MiscFeature' ,'Alley' , 'BsmtFinType1', 'BsmtFinType2', 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'GarageType', 'GarageFinish' , 'GarageCond' , 'GarageQual'] :\n    all_data[col]=all_data[col].fillna('None')","d71e5d45":"# Drop the columns\nall_data.drop(['Utilities', 'Street' , 'PoolQC'], axis=1, inplace=True)","967a5fee":"# We can create a new feature that may be related more directly to SalePrice (in the hope of better linearity).\n\nall_data['Total_sqr_footage'] = (all_data['BsmtFinSF1'] + all_data['BsmtFinSF2'] + all_data['1stFlrSF'] + all_data['2ndFlrSF'])\nall_data['Total_Bathrooms'] = (all_data['FullBath'] + (0.5 * all_data['HalfBath']) + all_data['BsmtFullBath'] + (0.5 * all_data['BsmtHalfBath']))\nall_data['Total_porch_sf'] = (all_data['OpenPorchSF'] + all_data['3SsnPorch'] + all_data['EnclosedPorch'] + all_data['ScreenPorch'] + all_data['WoodDeckSF'])\n","2178de1a":"all_data['haspool'] = all_data['PoolArea'].apply(lambda x: 1 if x > 0 else 0)\nall_data['has2ndfloor'] = all_data['2ndFlrSF'].apply(lambda x: 1 if x > 0 else 0)\nall_data['hasgarage'] = all_data['GarageArea'].apply(lambda x: 1 if x > 0 else 0)\nall_data['hasbsmt'] = all_data['TotalBsmtSF'].apply(lambda x: 1 if x > 0 else 0)\nall_data['hasfireplace'] = all_data['Fireplaces'].apply(lambda x: 1 if x > 0 else 0)","5f71ee98":"# There are some wrong values in GarageYrBlt\nall_data['GarageYrBlt'][all_data['GarageYrBlt']>2150]","8bda19c4":"all_data['GarageYrBlt']=all_data['GarageYrBlt'].fillna(all_data['YearBuilt'][ all_data['GarageYrBlt'].isnull()])\nall_data['GarageYrBlt'][all_data['GarageYrBlt']>2018] = all_data['YearBuilt'][all_data['GarageYrBlt']>2018]","eb6dd33b":"# I think it is reasonable to guess the values according to \"Neighborhood\", as suggested by others.\n\nall_data[\"LotFrontage\"] = all_data.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(lambda x: x.fillna(x.median()))","ef500895":"all_data['MSSubClass'] = all_data['MSSubClass'].astype(str)\nall_data['OverallCond'] = all_data['OverallCond'].astype(str)\nall_data['BsmtFullBath'] = all_data['BsmtFullBath'].astype(str)\nall_data['BsmtHalfBath'] = all_data['BsmtHalfBath'].astype(str)\nall_data['YrSold'] = all_data['YrSold'].astype(str)\nall_data['MoSold'] = all_data['MoSold'].astype(str)\nall_data['GarageYrBlt'] = all_data['GarageYrBlt'].astype(str)\nall_data['YearBuilt'] = all_data['YearBuilt'].astype(str)\nall_data['YearRemodAdd'] = all_data['YearRemodAdd'].astype(str)\nall_data['GarageCars'] = all_data['GarageCars'].astype(str)","f5cfe353":"# We can create a new feature that may be related more directly to SalePrice (in the hope of better linearity).\nall_data['TotalSF']=all_data['TotalBsmtSF']+all_data['1stFlrSF']+all_data['2ndFlrSF']","1775ad97":"cols = ('ExterCond','HeatingQC', 'BsmtFinType1', \n        'BsmtFinType2', 'Functional', 'Fence', 'BsmtExposure', 'GarageFinish', 'LandSlope',\n        'LotShape', 'PavedDrive', 'CentralAir', 'MSSubClass', 'OverallCond',\n        'YrSold', 'MoSold','GarageYrBlt','YearBuilt','YearRemodAdd', 'BsmtHalfBath','BsmtFullBath', 'GarageCars')\n    \n# process columns, apply LabelEncoder to categorical features\nfor c in cols:\n    lbl = LabelEncoder() \n    lbl.fit(list(all_data[c].values)) \n    all_data[c] = lbl.transform(list(all_data[c].values))","d912c07b":"all_data.FireplaceQu = all_data.FireplaceQu.astype('category', ordered=True, categories=['None','Po','Fa','TA','Gd','Ex']).cat.codes\nall_data.BsmtQual = all_data.BsmtQual.astype('category', ordered=True, categories=['None','Fa','TA','Gd','Ex']).cat.codes\nall_data.BsmtCond = all_data.BsmtCond.astype('category', ordered=True, categories=['None','Po','Fa','TA','Gd']).cat.codes\nall_data.GarageQual = all_data.GarageQual.astype('category', ordered=True, categories=['None','Po','Fa','TA','Gd','Ex']).cat.codes\nall_data.GarageCond = all_data.GarageCond.astype('category', ordered=True, categories=['None','Po','Fa','TA','Gd','Ex']).cat.codes\nall_data.ExterQual = all_data.ExterQual.astype('category', ordered=True, categories=['Fa','TA','Gd','Ex']).cat.codes\n#all_data.PoolQC = all_data.PoolQC.astype('category', ordered=True, categories=['None','Fa','Gd','Ex']).cat.codes\nall_data.KitchenQual = all_data.KitchenQual.astype('category', ordered=True, categories=['Fa','TA','Gd','Ex']).cat.codes","600bf2ea":"skewed_feats = all_data[all_data.dtypes[all_data.dtypes != \"object\"].index].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\nskewness = pd.DataFrame({'Skew' :skewed_feats})\nf, ax = plt.subplots(figsize=(15, 12))\nplt.xticks(rotation='90')\nsns.barplot(x=skewed_feats.index, y=skewed_feats)\nplt.xlabel('Features', fontsize=15)\nplt.ylabel('Skewness', fontsize=15)\nplt.title('Skewness by feature', fontsize=15)","41ab004f":"from scipy.special import boxcox1p #for Box Cox transformation\n\nfor feat in skewness[abs(skewness)>0.5].index:\n    all_data[feat] = boxcox1p(all_data[feat], 0.15)","1f6b3e66":"# Dummification \nall_data = pd.get_dummies(all_data)\n\n# To avoid dummy variable trap\nall_data.drop([\"Neighborhood_Veenker\", \"RoofMatl_WdShngl\", \"RoofStyle_Shed\" , \"SaleCondition_Partial\" , \"SaleType_WD\"], axis=1, inplace=True)\n\n\n# Now our training and testing dataset is imputed now, let's now separate as it.\n# ntrain variable indicates the total number of rows in the training dataset\n\n# We will use this dataset to train our model and test the accuracy of the model \ntrain_data = all_data[:train_data.shape[0]]\n\n# Test data set - this data set will be used for submissions to the competitions\n# We will keep this dataset aside now (DO NOT TOUCH!!!)\n# We will use this dataset once, we finalize our model.\ntest_data = all_data[train_data.shape[0]:]\n\n# target variable - SalePrice\ny_train = transform_log","b27a7511":"y_train.shape ,  train_data.shape","0dcf426f":"# Import libraries\nfrom sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC\nfrom sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split\nfrom sklearn.metrics import mean_squared_error\nimport xgboost as xgb\nimport lightgbm as lgb","0f7b6f24":"#Cross-validation function\nn_folds = 10\n\ndef rmsle_cv(model):\n    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(train_data.values)\n    rmse= np.sqrt(-cross_val_score(model, train_data.values, y_train, scoring=\"neg_mean_squared_error\", cv = kf))\n    return(rmse)","1077e8fb":"KRR = KernelRidge(alpha=0.6, kernel='polynomial', degree=2, coef0=2.5)\nscore = rmsle_cv(KRR)\nprint(\"Kernel Ridge score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","b785fec9":"lasso = make_pipeline(RobustScaler(), Lasso(alpha =0.0005, random_state=1))\nscore = rmsle_cv(lasso)\nprint(\"Lasso score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","8abab9a5":"ENet = make_pipeline(RobustScaler(), ElasticNet(alpha=0.0005, l1_ratio=.9, random_state=3))\nscore = rmsle_cv(ENet)\nprint(\"ElasticNet score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","b1dc76b8":"GBoost = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05,\n                                   max_depth=4, max_features='sqrt',\n                                   min_samples_leaf=15, min_samples_split=10, \n                                   loss='huber', random_state =5)\nscore = rmsle_cv(GBoost)\nprint(\"Gradient Boosting score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","f32a2f38":"model_xgb = xgb.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, \n                             learning_rate=0.05, max_depth=3, \n                             min_child_weight=1.7817, n_estimators=2200,\n                             reg_alpha=0.4640, reg_lambda=0.8571,\n                             subsample=0.5213, silent=1,\n                             random_state =7, nthread = -1)\nscore = rmsle_cv(model_xgb)\nprint(\"Xgboost score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","e6b00a29":"model_lgb = lgb.LGBMRegressor(objective='regression',num_leaves=5,\n                              learning_rate=0.05, n_estimators=720,\n                              max_bin = 55, bagging_fraction = 0.8,\n                              bagging_freq = 5, feature_fraction = 0.2319,\n                              feature_fraction_seed=9, bagging_seed=9,\n                              min_data_in_leaf =6, min_sum_hessian_in_leaf = 11)\nscore = rmsle_cv(model_lgb)\nprint(\"LGBM score: {:.4f} ({:.4f})\\n\" .format(score.mean(), score.std()))","c42bdf0d":"LassoMd = lasso.fit(train_data.values,y_train)\nENetMd = ENet.fit(train_data.values,y_train)\nKRRMd = KRR.fit(train_data.values,y_train)\nGBoostMd = GBoost.fit(train_data.values,y_train)\nXGBMd = model_xgb.fit(train_data.values, y_train)\nLGBMd = model_lgb.fit(train_data.values, y_train)","98491116":"finalMd = (np.expm1(LassoMd.predict(test_data.values)) + \n           np.expm1(ENetMd.predict(test_data.values)) + \n           np.expm1(KRRMd.predict(test_data.values)) + \n           np.expm1(GBoostMd.predict(test_data.values))  + \n           np.expm1(XGBMd.predict(test_data.values))  +\n           np.expm1(LGBMd.predict(test_data.values)) ) \/ 6\nfinalMd","83be940c":"# Code to submit your data to the competition\n\n\nmy_submission = pd.DataFrame({'Id': test_ID, 'SalePrice': finalMd}) \n\n# you could use any filename. We choose submission here\nmy_submission.to_csv('submission.csv', index=False)","a3f48aff":"my_submission","e878c43e":"![](https:\/\/www.safaribooksonline.com\/library\/view\/clojure-for-data\/9781784397180\/graphics\/7180OS_01_180.jpg)\n\n* For more info  click [Here](https:\/\/www.google.co.in\/url?sa=t&rct=j&q=&esrc=s&source=web&cd=3&cad=rja&uact=8&ved=0ahUKEwi7i-jN-K7XAhWJKo8KHbIHAV4QFgguMAI&url=http%3A%2F%2Fwhatis.techtarget.com%2Fdefinition%2Fskewness&usg=AOvVaw1LJhHdq4KFEYIpfdXjOlF-) \n\n* Learn more about [skeness](https:\/\/whatis.techtarget.com\/definition\/skewness)","c06672e9":"![](http:\/\/intranet.tdmu.edu.ua\/data\/kafedra\/internal\/distance\/classes_stud\/english\/1course\/Medical%20statistics\/08.%20Types%20of%20correlation.files\/image013.gif)\n","ce7ef647":"### We are now ready for ML\n\n[Reference Kernel](https:\/\/www.kaggle.com\/ashirahama\/blending) ","bbdb2812":"#### Ridge Regresssion","b7fa7026":"It seems Box Cox transformation does give better results. The thresholds of transformation doesn't seem to matter much.\n\n","4d34836e":"#### Data imputation","3608e707":"Combining training and testing sets into one dataset would impute and transform the features in both sets simultaneously. It is much more convenient and mistake-proof than doing it seperately.","dfbfe34f":"It is noticed that a lot of the features are not normally distributed. We want to check the skewness and transform them accordingly.","fd8ceae5":"#### Lasso Regression\n","38f1813d":"### Observations based on heatmap\n1. The variable `OverallQual` has white color and it seems that it has strong co-relation with `SalePrice` target variable. \n\n2. The variable `TotalBsmtSF` and `1stFlrSF` show good correlation with `SalePrice` variable. \n\n3. The variable `GrLiveArea` shows good correlation with `SalePrice` variable.\n\n4. The GarageX variables such as `GarageCars` and `GarageArea`  also show good correlation with `SalePrice` variable.  \n\n5. Apartfrom the above, feature variables such as **Lot** Variable (`LotFrontage`, `LotArea`), `YearBuilt`, `YearRemodAdd`, `MasVnrArea`, `TotalRmsAbvGrd`, `Fireplaces`, `GarageYrBlt` shows some correlation with target variable `SalePrice`. ","6a6c9b60":"#### XGBoost","42ad5d91":"We will analyze correlation using the following \n(Reference: [Comprehensive data exploration with Python](https:\/\/www.kaggle.com\/pmarcelino\/comprehensive-data-exploration-with-python))\n\n* Correlation matrix (heatmap style).\n* `SalePrice` correlation matrix (zoomed heatmap style).\n* Scatter plots between the most correlated variables (move like Jagger style).","e17264ee":"#### LightGBM","18f8b2d4":"![skewness](https:\/\/i.stack.imgur.com\/7iSYs.png)","93410738":"Now we see that the distribution is far from normal. A popular treatment is to take a log(1+x) transformation. ","2b1fb916":"### Relationship with Numerical variable \n","229fa6fc":"our goal is to predict SalePrice, we need to check whether it more or less distributes normally. If not, a transformation is needed. The regressors work best on normal distributions. \n\n[Reference Kernel](https:\/\/www.kaggle.com\/rickychwong\/a-beginner-s-kernel) \n\nAs we see, the target variable `Sale Price` is not normally distributed.  This can reduce the performance of the ML regression models because some ML assumes normal distribution. Therefore we make a log transformation, the resulting distrbution looks much better. \n\nReference: **[House Prices: EDA to ML (Beginner)](https:\/\/www.kaggle.com\/dejavu23\/house-prices-eda-to-ml-beginner#Plots-of-relation-to-target-for-all-numerical-features) **","248c7085":"It is most likely that 2207 should be 2007, but I would rather treat it like Nan. I would assume Nan means it was built in the same year as the house. One can also assume Nan means no Garage and assign a new value, say 0. (if we choose to convert into strings, it is ok to assign some new value). However we see that the distribution is almost a normal, it is unwise to destroy that. No garage can also have a GarageYrBlt as long as GarageCars is 0.","39efdd5f":"Now all Nan's are filled in. We have to convert some features from numbers to strings so that the regressors don't treat them as continuous real numbers, but categories instead.","8d4da6eb":"#### Elastic Net Regression","3c6f9cfb":"#### Outlier\n\nAs observed previosly, `GrLivArea` and `SalePrice` are linearly dependent. We are trying to closly observe the relationships among these two variables by plotting scatter graph.","72aae1d8":"#### Exploratory Data Anlaysis\n\nBefore we do data preprocessing. Let's first do 'Exploratory data analysis\".\n\nThe goal of Exploratory Data Analysis is: \n\n*  Understand the overall data set, \n*  What is the target variables\n*  Which variables are dependent on the target variable most?\n*  Identify variable that are not at all dependent on the target variable. So, we can drop them. \n* Remove the outlier ","5d7ea5e7":"#### Gradient Boosting Regression\n"}}