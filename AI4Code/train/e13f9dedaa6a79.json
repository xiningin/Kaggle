{"cell_type":{"a1a1a6da":"code","7ae3b198":"code","08e274fe":"code","36cd1cc4":"code","547f5795":"code","1e876b2a":"code","ca9d7ea6":"code","a9885fd7":"code","bf785f84":"code","bf1b218a":"code","0e3543cf":"markdown","8f497258":"markdown","92b43dc4":"markdown","d1bc7e83":"markdown","994c2b84":"markdown","df02746a":"markdown","05970dee":"markdown","b3bc0a7c":"markdown","dd141db2":"markdown","9ebc3821":"markdown"},"source":{"a1a1a6da":"# Directive pour afficher les graphiques dans Jupyter\n%matplotlib inline","7ae3b198":"# Pandas : librairie de manipulation de donn\u00e9es\n# MatPlotLib : librairie de visualisation et graphiques\n# sklearn : librairie de machine learning\n\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nfrom sklearn import ensemble, metrics, preprocessing, neighbors","08e274fe":"df = pd.read_csv('..\/input\/pima-indians-diabetes-database\/diabetes.csv')\n\ndf.head()","36cd1cc4":"data_train = df.sample(frac=0.8, random_state=1)     # 80% des donn\u00e9es avec frac=0.8\ndata_test = df.drop(data_train.index)                # le reste des donn\u00e9es pour le test\n\nprint (\"Taille data_train:\", len(data_train))\nprint (\"Taille data_test:\", len(data_test))","547f5795":"X_train = data_train.drop(['Outcome'], axis=1)\ny_train = data_train['Outcome']\nX_test = data_test.drop(['Outcome'], axis=1)\ny_test = data_test['Outcome']\n\nX_train.head(3)","1e876b2a":"rf = ensemble.RandomForestClassifier()\nrf.fit(X_train, y_train)\ny_rf = rf.predict(X_test)","ca9d7ea6":"rf_score = metrics.accuracy_score(y_test, y_rf)\nprint(rf_score)","a9885fd7":"ajusteur = preprocessing.MinMaxScaler()\najusteur.fit(X_train)\n\nX_train = ajusteur.transform(X_train)\nX_test  = ajusteur.transform(X_test)\n\nprint(X_train)","bf785f84":"rf.fit(X_train, y_train)\ny_rf = rf.predict(X_test)\n\nrf_score = metrics.accuracy_score(y_test, y_rf)\nprint(rf_score)","bf1b218a":"classifier = neighbors.KNeighborsClassifier(n_neighbors=15)\n\nclassifier = classifier.fit(X_train,y_train)\n\ny_knn = classifier.predict(X_test)\n\nknn_score = metrics.accuracy_score(y_test, y_knn)\nprint(knn_score)","0e3543cf":"R\u00e9p\u00e9ter les \u00e9tapes pr\u00e9c\u00e9dentes","8f497258":"Utilizer le m\u00e9thode de Random Forest pour faire la pr\u00e9diction","92b43dc4":"Cr\u00e9er le dataframe principal en lisant le base de don\u00e9es et les afficher","d1bc7e83":"On peut concluire que c'est un probl\u00e8me de classification, une fois que la variable cible est binaire","994c2b84":"Le resultat n'est pas bon, donc on peut am\u00e9liorer les donn\u00e9es d'entr\u00e9. Pour exemple, l'\u00e9chelle des variables","df02746a":"Pour les deux groupes cr\u00e9es, separer la variable cible de les donn\u00e9es d'entr\u00e9e","05970dee":"V\u00e9rifier la pr\u00e9cision de la pr\u00e9diction","b3bc0a7c":"Separer au hasard les donn\u00e9es d'entra\u00eenement et de test","dd141db2":"\u00c9tonnamment, le r\u00e9sultat c'est inf\u00e9rieur au pr\u00e9cedent, donc on peut conclure que les variables plus importants pour la pr\u00e9diction sont laquelles qu'ont un valeur absolue sup\u00e9rieur ","9ebc3821":"Apr\u00e9s un petit recherche, j'ai tester un autre type de m\u00e9thode, mais le r\u00e9sultat n'a chang\u00e9 beaucoup"}}