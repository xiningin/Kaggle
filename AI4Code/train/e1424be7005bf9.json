{"cell_type":{"173f3d51":"code","24a18a8b":"code","4cd1517d":"code","75accf29":"code","c7f33fb1":"code","6dca6fc2":"code","a5dda11a":"code","0f257037":"code","d40c131c":"code","85527a4a":"code","5e018135":"code","a166388b":"code","9efd2817":"code","e2020c31":"code","e83030f0":"code","69bd1f0f":"code","01152555":"code","e9aae20d":"code","6496d2e2":"code","27ad1b30":"code","a1881025":"code","9c5e7da0":"code","4278dc91":"code","854dbd20":"code","06fe9363":"code","2c21b12c":"code","2949a6d9":"code","3e34fa47":"code","8a6d8afc":"code","5277f567":"code","b0ada3d5":"code","72c17d52":"code","6bfcdcd5":"code","39cfccdd":"code","9e614408":"code","9ca5ceb4":"code","a570ffc8":"code","ac2ab349":"code","80d20345":"code","ff25ddc5":"code","a016a6d7":"code","f68ff3eb":"code","46e43d9c":"code","95035aad":"code","6bf477d5":"code","c2fbb3a7":"code","15a684b8":"markdown","2ed957ac":"markdown","a0770f90":"markdown","d2d5b5ea":"markdown","0f2c862c":"markdown","fb7b8464":"markdown","265fb0c3":"markdown","9c737bed":"markdown","8d97f7c4":"markdown","f20848d2":"markdown","77cff53d":"markdown","645ed6ca":"markdown","9c382002":"markdown","b7e7543f":"markdown","48b977bf":"markdown","ecf48ee8":"markdown","71a3c44b":"markdown","4f56c2c0":"markdown","67ba6266":"markdown","d71e6376":"markdown","5d42c1ce":"markdown","f5de89eb":"markdown","1efb4d62":"markdown","e3c5aa46":"markdown","bb730d52":"markdown","33cb27e0":"markdown","c38e19a7":"markdown","f5fc1131":"markdown","30fe502b":"markdown","c7d55103":"markdown","8cd76e8b":"markdown","509f94a0":"markdown","5f7266f4":"markdown","9effffa3":"markdown","d90b93dd":"markdown","7a49439a":"markdown","efcc26b6":"markdown"},"source":{"173f3d51":"import numpy as np\n\nimport pandas as pd\nfrom pandas.plotting import scatter_matrix\nimport geopandas as gpd\n\nimport matplotlib.pyplot as plt\n%config InlineBackend.figure_format = 'retina'\nimport seaborn as sns\n\nfrom IPython.display import HTML\n\nimport warnings\nwarnings.filterwarnings('ignore')","24a18a8b":"!pip install gif\nimport gif","4cd1517d":"pd.options.display.float_format = '{:,.2f}'.format\npd.set_option('precision', 2)\nfont_size = 17","75accf29":"df_m = pd.read_csv('..\/input\/housing-in-london\/housing_in_london_monthly_variables.csv', parse_dates = ['date'])\n\nprint ('This dataset contains {} rows and {} columns.'.format(df_m.shape[0], df_m.shape[1]))\ndf_m.head()","c7f33fb1":"df_m.info()","6dca6fc2":"null_df_m = df_m.isnull().sum().sort_values(ascending = False)\npercent = (df_m.isnull().sum()\/df_m.isnull().count()).sort_values(ascending = False)*100\n\nnull_df_m = pd.concat([null_df_m, percent], axis = 1, keys = ['Counts', '% Missing'])\nprint ('Missing: ')\nnull_df_m.head()","a5dda11a":"df_m.drop('no_of_crimes', axis = 1, inplace = True)   # drop the 'no_of_crimes column\n\ndf_m['houses_sold'].fillna(df_m.groupby('area')['houses_sold'].transform('mean'), inplace = True) # fill NaN values with the mean of that particular area","0f257037":"df_m['year'] = df_m['date'].dt.year\ndf_m.iloc[[0, -1]]","d40c131c":"df_m = df_m[df_m['year'] < 2020]\ndf_m['year'].max()","85527a4a":"lnd_boroughs = df_m[df_m['borough_flag'] == 1]['area'].unique()\nlen(lnd_boroughs)","5e018135":"df_m[df_m['borough_flag'] == 0]['area'].nunique()","a166388b":"df_m[df_m['borough_flag'] == 0]['area'].unique()","9efd2817":"eng_regions = ['south west', 'south east', 'east of england', 'west midlands', 'east midlands', 'yorks and the humber', 'north west', 'north east']","e2020c31":"lnd = df_m[df_m['area'].isin(lnd_boroughs)]\neng = df_m[df_m['area'].isin(eng_regions)]","e83030f0":"lnd_pr = lnd.groupby('date')['average_price'].mean()\neng_pr = eng.groupby('date')['average_price'].mean()","69bd1f0f":"plt.figure(figsize = (9, 5))\n\nlnd_pr.plot(y = 'average_price', color = 'royalblue', lw = 2, label = 'London')\neng_pr.plot(y = 'average_price', color = 'firebrick', lw = 2, label = 'England')\n\nplt.axvspan('2007-12-21', '2009-06-21', alpha = 0.5, color = '#E57715')\nplt.text(x = '2008-04-01', y = 390000, s = 'Recession', rotation = 90, fontsize = font_size-2)\nplt.axvline(x = '2016-06-23', lw = 2, color = '#E57715', linestyle = '--')\nplt.text(x = '2015-08-01', y = 210000, s = 'Brexit Referendum', rotation = 90, fontsize = font_size-2)\n\nplt.title('Time evolution of the average house price', size = font_size)\nplt.ylabel('Average Price', size = font_size)\nplt.xticks(size = font_size - 3)\nplt.xlabel('Date', size = font_size)\nplt.yticks(size = font_size - 3)\nplt.legend(fontsize = font_size - 3);","01152555":"@gif.frame\ndef plot(df_lnd, df_eng, date):\n    \n    ### select a sub-dataframe from the start until date ###\n    d_ln = df_lnd.loc[df_lnd.index[0]:date]\n    d_eng = df_eng.loc[df_eng.index[0]:date]\n    \n    fig = plt.figure(figsize = (9, 5))\n    plt.xlim(pd.Timestamp('1994-12-01'), pd.Timestamp('2020-01-01'))\n    plt.ylim(47000, 550000)\n    \n    ### for the vertical orange rectangle and the vertical dashed line ###\n    if (date > pd.Timestamp('2007-12-22') and date < pd.Timestamp('2009-06-21')):\n        plt.axvspan(pd.Timestamp('2007-12-21'), date, alpha = 0.5, color = '#E57715') \n    elif (date > pd.Timestamp('2009-06-21')):\n        plt.axvspan(pd.Timestamp('2007-12-21'), pd.Timestamp('2009-06-21'), alpha = 0.5, color = '#E57715')\n        plt.text(x = pd.Timestamp('2008-04-29'), y = 390000, s = 'Recession', rotation = 90, fontsize = font_size-2)\n    if (date > pd.Timestamp('2016-06-23')):\n        plt.axvline(x = pd.Timestamp('2016-06-23'), lw = 2, color = '#E57715', linestyle = '--')\n        plt.text(x = pd.Timestamp('2015-08-01'), y = 210000, s = 'Brexit Referendum', rotation = 90, fontsize = font_size-2)\n    ############################################################################################################\n    \n    plt.plot(d_ln, color = 'royalblue', lw = 2, label = 'London')\n    plt.plot(d_eng, color = 'firebrick', lw = 2, label = 'England')\n    \n    plt.title('Time evolution of the average house price', size = font_size)\n    plt.ylabel('Average Price', size = font_size)\n    plt.xticks(size = font_size - 3)\n    plt.xlabel('Date', size = font_size)\n    plt.yticks(size = font_size - 3)\n    plt.legend(loc = 2, fontsize = font_size - 3);","e9aae20d":"frames = []\nfor months in pd.date_range(start = lnd_pr.index[0], end = lnd_pr.index[-1], freq = '3MS'): # 3MS --> every three months\n    frame = plot(lnd_pr, eng_pr, months)\n    frames.append(frame)\n    \ngif.save(frames, 'Price-Lnd_vs_Eng.gif', duration = 1, unit = 's', between = 'startend')","6496d2e2":"HTML('<img src=\".\/Price-Lnd_vs_Eng.gif\" \/>')","27ad1b30":"lnd_b_prices = lnd.groupby('area')['average_price'].mean()\nlnd_top10_pr = lnd_b_prices.sort_values(ascending = False).to_frame()\n\nprint ('\\nThe 10 most expensive boroughs in London are:')\nlnd_top10_pr.head(10)","a1881025":"lnd_top10_pr.head(10).sort_values(by = 'average_price', ascending = True).plot(kind = 'barh', figsize = (9, 5), \n                                                                               color = 'steelblue', edgecolor = 'firebrick',\n                                                                               legend = False)\n\nplt.title('Average price in the most expensive London boroughs (1995-2019)', size = font_size, y = 1.05)\nplt.ylabel('London Borough', size = font_size)\nplt.yticks(size = font_size - 3)\nplt.xlabel('Average Price', size = font_size)\nplt.xticks([0, 200_000, 400_000, 600_000], size = font_size - 3);","9c5e7da0":"top5_indeces = lnd_top10_pr.head().index\ncolors = ['#e74c3c', '#3498db', '#95a5a6', '#34495e', '#2ecc71']\n\nplt.figure(figsize = (9, 5))\n\nfor index, i in enumerate(top5_indeces):\n    df_ = lnd[lnd['area'] == i]\n    df_ = df_.groupby('date')['average_price'].mean()\n    \n    df_.plot(y = 'average_price', label = i, color = colors[index])\n       \nplt.title('Average price in the most expensive boroughs', y = 1.04, size = font_size)\nplt.xlabel('Date', size = font_size)\nplt.xticks(size = font_size - 3)\nplt.ylabel('Average Price', size = font_size)\nplt.yticks([0.2*1E+6, 0.6*1E+6, 1.0*1E+6, 1.4*1E+6], size = font_size - 3)\nplt.legend(fontsize = font_size - 5);","4278dc91":"eng_prices = eng.groupby('area')['average_price'].mean()\neng_top3_pr = eng_prices.sort_values(ascending = False).to_frame()\n\nprint('The top 3 most expensive regions in England are:')\neng_top3_pr.head(3)","854dbd20":"top3_indeces = eng_top3_pr.head(3).index\ncolors = ['darkorange', '#8EB8E5', 'forestgreen', ]\n\nplt.figure(figsize = (9, 5))\n\nfor index, i in enumerate(top3_indeces):\n    df_ = eng[eng['area'] == i]\n    df_ = df_.groupby('date')['average_price'].mean()\n    df_.plot(y = 'average_price', label = i, color = colors[index])\n\nplt.title('Average price in the most expensive English regions by date', size = font_size, y = 1.04)\nplt.xlabel('Date', size = font_size)\nplt.xticks(size = font_size - 3)\nplt.ylabel('Average Price', size = font_size)\nplt.yticks([100_000, 200_000, 300_000], size = font_size - 3)\nplt.legend(fontsize = font_size - 3);","06fe9363":"plt.figure(figsize = (9, 5))\n\nfor index, i in enumerate(top3_indeces):\n    df_ = eng[eng['area'] == i]\n    df_ = df_.groupby('date')['average_price'].mean()\n    df_.plot(y = 'average_price', label = i, color = colors[index])\n\nlnd_bng_pr = lnd[lnd['area'] == 'barking and dagenham'].groupby('date')['average_price'].mean()\nlnd_bng_pr.plot(y = 'average_price', lw = 2, linestyle = '--', color = '#A30015', label = 'barking and dagenham')\n\nplt.title('3 expensive English regions VS cheapest London borough', size = font_size, y = 1.06)\nplt.xlabel('Date', size = font_size)\nplt.xticks(size = font_size - 3)\nplt.ylabel('Average Price', size = font_size)\nplt.yticks([0.1*1E+6, 0.2*1E+6, 0.3*1E+6], size = font_size - 3)\nplt.legend(labels = ['South East (Eng)', 'East of England (Eng)', 'South West (Eng)', 'Barking and Dagenham (L)'], \n           fontsize = font_size - 3);","2c21b12c":"lnd_houses = lnd.groupby('date')['houses_sold'].sum()\nlnd_houses.plot(figsize = (9, 5), lw = 2, y = 'houses_sold', color = '#00072D')\n\nplt.axvspan('2007-12-21', '2009-06-21', alpha = 0.5, color = '#F08700')\nplt.text(x = '2008-04-01', y = 10700, s = 'Recession', rotation = 90, fontsize = font_size-2)\nplt.axvspan('2016-01-1', '2016-05-01', alpha = 0.7, color = '#FFCAAF')\n\n# plt.axvline(x = '2016-06-23', lw = 2, color = '#E57715', linestyle = '--')\nplt.text(x = '2016-06-01', y = 10000, s = 'New tax legislation', rotation = 90, fontsize = font_size-2)\n\nplt.title('Houses sold in London by date', size = font_size)\nplt.xlabel('Date', size = font_size)\nplt.xticks(size = font_size - 3)\nplt.ylabel('Houses sold', size = font_size)\nplt.yticks([4000, 8000, 12000, 16000], size = font_size - 3);","2949a6d9":"lnd_b_houses = lnd.groupby('area')['houses_sold'].sum()\nlnd_top5_h = lnd_b_houses.sort_values(ascending = False).to_frame()\nlnd_top5_h.head(5)","3e34fa47":"london_map = gpd.read_file('..\/input\/london-borough-and-ward-boundaries-up-to-2014\/London_Wards\/Boroughs\/London_Borough_Excluding_MHW.shp')\nlondon_map.columns = london_map.columns.str.lower()\nlondon_map.head()","8a6d8afc":"london_map['name'] = london_map['name'].str.lower()\nlondon_map.rename(columns = {'name': 'area'}, inplace = True)\nlondon_map.rename(columns = {'gss_code': 'code'}, inplace = True)\n\nlondon_map = london_map[['area', 'code', 'hectares', 'geometry']]\nlondon_map.head()","5277f567":"lnd_m = lnd.groupby('area').agg({'average_price': ['mean'], 'houses_sold': 'sum'})\n\nlnd_m.columns = ['average_price', 'houses_sold']\nlnd_m.reset_index(inplace = True)\nlnd_m.head()","b0ada3d5":"np.intersect1d(lnd_m['area'], london_map['area']).size","72c17d52":"lnd_m_map = pd.merge(london_map, lnd_m, how = 'inner', on = ['area'])\nlnd_m_map.head()","6bfcdcd5":"type(lnd_m_map)","39cfccdd":"fig, ax = plt.subplots(1, 2, figsize = (12, 12))\n\nlnd_m_map.plot(ax = ax[0], column = 'average_price', cmap = 'Reds', edgecolor = 'maroon', legend = True, legend_kwds = {'label': 'Average Price', 'orientation' : 'horizontal'})\n\nlnd_m_map.plot(ax = ax[1], column = 'houses_sold', cmap = 'Blues', edgecolor = 'maroon', legend = True, legend_kwds = {'label': 'Houses Sold', 'orientation' : 'horizontal'})\n\nax[0].axis('off')\nax[0].set_title('Average House Price (All years)', size = font_size)\nax[1].axis('off')\nax[1].set_title('Houses Sold (All years)', size = font_size);","9e614408":"df_y = pd.read_csv('..\/input\/housing-in-london\/housing_in_london_yearly_variables.csv', parse_dates = ['date'])\ndf_y = df_y[df_y['area'].isin(lnd_boroughs)] # select only London boroughs\n\nprint ('This dataset contains {} rows and {} columns.'.format(df_y.shape[0], df_y.shape[1]))\ndf_y.head()","9ca5ceb4":"null_df_y = df_y.isnull().sum().sort_values(ascending = False)\npercent = (df_y.isnull().sum()\/df_y.isnull().count()).sort_values(ascending = False)*100\n\nnull_df = pd.concat([null_df_y, percent], axis = 1, keys = ['Counts', '%'])\nnull_df.head(10)","a570ffc8":"# import missingno as msno\n# msno.matrix(df_y)","ac2ab349":"df_y[~df_y['mean_salary'].str.isnumeric()]['mean_salary'].value_counts()","80d20345":"df_y['mean_salary'] = df_y['mean_salary'].replace(['#'], np.NaN)\ndf_y['mean_salary'] = df_y['mean_salary'].astype(float)","ff25ddc5":"df_y['year'] = df_y['date'].dt.year\n\nprint ('yearly_variables dataset')\nprint ('\\tFirst date: ', df_y['year'].min())\nprint ('\\tFinal date: ', df_y['year'].max())","a016a6d7":"lnd_m_group = lnd.groupby(['area', 'year']).mean().reset_index()  # group based on area and year (take mean)\nlnd_m_group = lnd_m_group[lnd_m_group['year'] >= 1999]            # select all years after 1999 (included)\n\nprint ('monthly_variables dataset')\nprint ('\\tFirst date: ', lnd_m_group['year'].min())\nprint ('\\tFinal date: ', lnd_m_group['year'].max())","f68ff3eb":"lnd_y_group = df_y.groupby(['area', 'year']).mean().reset_index() # group it based on area and year\nlnd_y_group.head()","46e43d9c":"lnd_total = pd.merge(lnd_y_group, lnd_m_group, on = ['area', 'year'], how = 'left')\nlnd_total.drop(['borough_flag_x', 'borough_flag_y'], axis = 1, inplace = True)\n\nlnd_total.head()","95035aad":"corr_table = lnd_total.corr()\ncorr_table['average_price'].sort_values(ascending = False)","6bf477d5":"plt.figure(figsize = (10, 8))\n\nmask = np.triu(np.ones_like(corr_table, dtype = np.bool))\n\nax = sns.heatmap(corr_table, mask = mask, annot = True, cmap = 'YlGnBu_r')\nbottom, top = ax.get_ylim()\nax.set_ylim(bottom + 0.5, top - 0.5);","c2fbb3a7":"columns = ['average_price', 'median_salary', 'mean_salary', 'number_of_jobs']\n\nscatter_matrix(lnd_total[columns], figsize = (12, 12), color = '#D52B06', alpha = 0.3, \n               hist_kwds = {'color':['bisque'], 'edgecolor': 'firebrick'});","15a684b8":"# Future Developement\n\nThis notebook could be further improved by making some additions:\n\n- Use maps to visualise how variables in the 'yearly_dataset' vary from borough to borough,\n- Compare English regions with London (all boroughs) with respect to the other variables (maps could be useful),\n- Use Folium for plotting interactive maps.\n    \n<br>\nPlease <font color=\"red\" size=+0><b>upvote<\/b><\/font> if you liked this notebook! Thank you! \ud83d\ude09","2ed957ac":"- It doesn't come as a surpise that apart from the year, the average price in London has a **significant positive correlation** with the **median and mean salary** along with the **number of jobs** in a borough. \n- There is **small positive correlation** with **life satisfaction**. More data for this attribute would be useful.\n- As we discussed earlier, the most affluent boroughs happen to be small (see map) which explains the **negative correlation** with the **size** of an area. \n- The average price is also affected by the **number of houses sold**, since the former goes up when the later decreases (people sell higher when less houses are sold?).\n\nWe can summarise these correlations (along with all pair-wise correlations) with the following heatmap:","a0770f90":"## Correlations","d2d5b5ea":"### London VS England\n\nWe can now split the dataset into two: one for boroughs in London and one for the other regions of England:","0f2c862c":"Two attributes, 'houses_sold' and 'no_of_crimes', have missing values. We will drop the whole 'no_of_crimes' attribute since almost 50% of its instances have NaN values. \n\nIt's easier to replace missing values in the 'houses_sold' attribute as only a small portion is missing. We will use the mean value for the same area for all years in each case. Of course, someone could argue that this number would change over the years, but we could assume that the final results won't be affected due to small number of values being changed.","fb7b8464":"# Datasets\n\nWe are going to use the [Housing in London](https:\/\/www.kaggle.com\/justinas\/housing-in-london) dataset which is provided by [Justinas Cirtautas](https:\/\/www.kaggle.com\/justinas). It contains two csv files ('**housing_in_london_monthly_variables.csv**' and '**housing_in_london_yearly_variables.csv'**) with a lot of relevant information such as the monthly average house prices, yearly mean and median salary for residents of each area, etc.\n\nThe data is split by areas of London called boroughs, but some of the instances correspond to other UK regions (like North East, West Midlands, etc.).\n\nAdditionally, we are going to use a third dataset for plotting maps ([London Borough and Ward Boundaries up to 2014](https:\/\/www.kaggle.com\/csobral\/london-borough-and-ward-boundaries-up-to-2014\/notebooks), provided by [Cayo Costa Sobral](https:\/\/www.kaggle.com\/csobral)).","265fb0c3":"<br>\n\nThe average price in each borough fluctuates through time. However, we can calculate its mean which can give us a rough indication of how expensive each area is.","9c737bed":"To better understand these numbers we need more information such as the size of the borough, its population, crime rates etc. Some of these information are available in the other dataset ('housing_in_london_yearly_variables').","8d97f7c4":"# Housing over the Years\n\nWe are going to focus on the **average price** and the **number of houses sold** in each area. \n\nFor this task, we need two datasets:\n- '**housing_in_london_monthly_variables**' which contains monthly information about the London boroughs and the other UK regions, and\n- '**London_Borough_Excluding_MHW.shp**' for plotting maps.\n\n## Data Preprocessing\n\nLet's import the first dataset and take a quick look at the data:","f20848d2":"# Conclusions\n\nThe main conclusions are the following:\n\n- The average price in **London boroughs** is **higher** compared to the rest of England. It has been **affected** by major financial and political events (such the Recession and Brexit), but has **significantly increased** from 1995 to 2019 (by a factor of 5),\n\n- **Affluent regions** such as Kensighton and Westiminister have the **highest average price**, \n\n- The number of houses sold **plummeted** after the recession, **streadily increased** until 2016 but then again **dropped** after the referendum, \n\n- As anyone could predict, the main factor that influences the average price is the **financial prosperity** of the corresponding borough. Higher salaries and more jobs result in higher prices.","77cff53d":"The dataset contains samples from January 1995 to January 2020. We will discard all samples from 2020 since the year is not complete:","645ed6ca":"It does, so we are good to go.","9c382002":"We can manually isolate the ones that correspond to a region in England: ","b7e7543f":"---\n\n# Factors Affecting Housing\n\n## Preparing the Final Dataset\n\nFor this question we need to import the third and final dataset ('**housing_in_london_yearly_variables**') which we are going to merge with the monthly dataset.","48b977bf":"We focus on the 5 most expensive boroughs and see the evolution of the average price through time:","ecf48ee8":"<font size=+3 color=\"#3B3534\"><center><b>Housing in London \ud83c\udfd8\ufe0f\ud83d\udcb7\ud83d\udc82<\/b><\/center><\/font>\n\n<img src=\"https:\/\/images.unsplash.com\/photo-1501683567677-568cfd15361c?ixlib=rb-1.2.1&ixid=eyJhcHBfaWQiOjEyMDd9&auto=format&fit=crop&w=1414&q=80\" width=\"750\">\n<center>Photo by Nigel Tadyanehondo on Unsplash<\/center>\n\n<br>\n\n# Introduction\n\nHello readers! The topic of this notebook is the housing market of London in the years between 1995 and 2020. We are going to perform exploratory data analysis with the goal of discovering new information and answering the following questions:\n\n- How has the housing market (average house price and number of houses sold) changed over the years in different boroughs of London? How does it compare to England?\n- What factors affected it the most?\n\n<br>\n\n**Table of Contents**\n\n1. [Introduction](#Introduction)\n2. [Libraries](#Libraries)\n3. [Meet the Datasets](#Datasets)\n4. [London Housing over the Years](#Housing-over-the-Years)\n5. [Factors Affecting Housing](#Factors-Affecting-Housing)\n6. [Conclusions](#Conclusions)\n7. [Future Developement](#Future-Developement)","71a3c44b":"We can insert our gif in the notebook:","4f56c2c0":"## Data Exploration\n\nAccording to [Wikipedia](https:\/\/en.wikipedia.org\/wiki\/List_of_London_boroughs), there are 33 boroughs in London (32 + the City of London). Is that the case in our dataset?","67ba6266":"The merged dataset has a new type, **geodataframe**, which is what we need for plotting the map.","d71e6376":"The 'date' attribute is of datatime type, hence we can extract the corresponding year of each instance (month and day are not important):","5d42c1ce":"It is! How many and which regions are outside of London?","f5de89eb":"## Map of London\n\nWe are going to use the **second dataset** which contains geospatial information about London (specifically, the 'geometry' attribute). This [notebook](https:\/\/www.kaggle.com\/justinas\/house-prices-in-london) helped me a lot for the analysis.\n\nWe start by merging the two datasets:","1efb4d62":"There are 7 attributes in total. An instance represents a London borough if 'borough_flag' equals 1. The meaning of the rest of the attributes can be easily inferred from their name.\n\nThe method `info()` can give us valuable information such as the type and the number of missing values in each attribute:","e3c5aa46":"It is evident that even the cheapest borough in London is **comparable** to the most expensive regions of England!\n\n### Houses Sold\n\nLet's explore the 'houses_sold' attribute for London.","bb730d52":"- **Kensington and Chelsea** is the **most expensive borough** in London between 1995 and 2019. We can read in [wikipedia](https:\/\/en.wikipedia.org\/wiki\/Royal_Borough_of_Kensington_and_Chelsea) that it holds royal status and it includes many affluent areas, hence the increased price.\n- **Westministe**r, which is adjacent to Kensighton, comes **second**. Again this is not a surpise since it is the location of the national government and includes many sites commonly associated with London (e.g. the Buckingham Palace, the Houses of Parliament, 10 Downing Street, and Trafalgar Square) ([Source](https:\/\/en.wikipedia.org\/wiki\/City_of_Westminster)).\n\nWe can perform a similar analysis for England:","33cb27e0":"The three **most expensive English regions** are all located in the **South of England** (two of them border London). It would be interesting to plot the same graph but with the cheapest borough of London (which we can find is Barking and Dagenham):","c38e19a7":"- The number of houses sold in London **dropped sharply** during the **financial recession** and it seems that it **hasn't completely recovered** since,  \n- Interestingly, there is **spike in March 2016** which at first sight looks like a artefact. That is not the case, because people were actually trying to avoid a **new legislation** that came into effect in April 2016 and imposed an increase in the tax bill on buying a second home (Credits to [Marcel Fellipe](https:\/\/www.kaggle.com\/marcelfellipe\/housing-in-london-exploratory-data-analysis) for finding this). \n- Again, the **referendum** resulted in a **downward trend** starting from July 2016.\n\n<br>\n\nThe boroughs with the highest number of sold houses between 1995 and 2019 are:","f5fc1131":"We can see some obvious correlations such as median-mean salary, houses sold-no_of_houses, no_of_houses-population_size, etc.\n\nLastly, a linear relationship between attributes can be visualised with the `scatter_matrix` method:","30fe502b":"Remember that London has 33 boroughs, same as the lnd_tot dataset. Let's check that the london_map dataset has the same names:","c7d55103":"- We can now see Kensighton and Westminister as the two dark boroughs in the first map. We can roughly say that boroughs **closer** to these two areas are **more expensive**.\n- As we hinted earlier, the number of houses sold is probably affected by many factors with one of them being the **total area** of each borough. We can definetely see that larger boroughs have more houses sold.","8cd76e8b":"- In overall, the averace price follows an **upward trend** during the studied time frame, with London always having a higher average price. \n- This upward trend was disrupted by the the **[Great Recession](https:\/\/en.wikipedia.org\/wiki\/Great_Recession)** (2007-209), hence the significant **decline starting from 2007**. London apparently **recovered quickly** and the average price increased rapidly until 2016. **England** followed a similar behaviour but with a **moderate rise** in the same period.\n- Arguably, the **[Brexit referendum](https:\/\/en.wikipedia.org\/wiki\/2016_United_Kingdom_European_Union_membership_referendum)** had an impact in London housing since the average price hit a **plateau after 2016**. It seems the the rest of England wasn't affected as much from the referendum.\n\n<br>\n\n<font color=\"#842A16\" size=+0><b>Gifs<\/b><\/font> \n\nI really like adding another dimension in my graphs, by making **gifs**. I recently came across the [**gif**](https:\/\/github.com\/maxhumber\/gif) library by Max Humber, which allows us to make gifs simply by building a bunch of \"frames\" with a standard for loop.\n\nThe steps are the following:\n\n- Create a 'plot' function which plots the average price for both London (df_lnd) and England (df_eng) until the specified date (third argument). It doesn't have to return something.\n- Decorate the plot function with @gif.frame, \n- Use a for loop to create a frame for each month, and append the result to a list (called 'frames' in our case). Finally,\n- Use the `save()` method from the gif library to save it.","509f94a0":"# Libraries\n\nWe start by importing the necessary libraries and setting some parameters for the whole notebook (e.g. the display format for the pandas library).","5f7266f4":"This dataset has more information for each borough and is ideal for answering the second question. Let's see how many missing values each attrbitutes has:","9effffa3":"#### Average Price\n\nThe `groupby()` method allows us to calculate the mean 'average price' for each 'date':","d90b93dd":"It seems that all new attributes have missing values. Pandas' `corr()` function, which we are going to use later, ignores a pairwise correlation if there is a NaN value in one of the observations, so we shouldn't worry about it for now.\n\n'median_salary' is not currently numeric. If we try to convert it to numeric (with the `astype()` method), we'll get an error. That's because some instances contain symbols and need to be replaced first:","7a49439a":"This dataset starts four years later compared to the monthly dataset. Therefore, we need to merge them so that there is no mismatch in the 'year' column:","efcc26b6":"The two datasets are ready to be merged:"}}