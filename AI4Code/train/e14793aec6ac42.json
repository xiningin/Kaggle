{"cell_type":{"16c4d59c":"code","6e653473":"code","bc4925e7":"code","5a191236":"code","d8f2f583":"code","a0020b30":"code","05117a8b":"code","76f05d61":"code","d03bba45":"code","853dda57":"code","c34e7f44":"code","22dc1363":"code","6cd1587a":"code","6c15ecd8":"code","664ee75a":"code","7d45dd8c":"code","51b57520":"code","4764725b":"code","e6979581":"code","85ab607c":"code","bc0d1e3a":"code","9bddb479":"code","a20be0be":"code","93ffbb62":"code","26f24946":"code","88943ff7":"code","176bde6c":"code","ce78a206":"code","30fe1cbc":"code","9f20de74":"code","6c15e665":"code","59d60c64":"code","baad5290":"code","f5050a6b":"code","38732d51":"code","5c916523":"code","30e20b8a":"code","311c05c2":"code","990bd566":"code","b4b9cf61":"code","07d74433":"code","885bd6f5":"code","a3ea8fd4":"code","de719ec3":"code","77c2ea67":"code","cd1bec9b":"code","5f21d91d":"code","cb1c91ec":"code","dfc68a63":"code","dbe64b9d":"code","72ec28ee":"code","d1a885a1":"code","727e8c83":"code","9b82ae13":"code","2329eb9d":"code","8ae4e9f9":"code","ef95c71e":"code","34c8ea31":"code","6fec4a5c":"code","9f65260f":"code","ac07c704":"code","c8a1bb6a":"code","a0de3d03":"code","aa954802":"markdown","a451c209":"markdown","3bddb4fb":"markdown","522b0ec7":"markdown","2fe27d08":"markdown","0f0099d6":"markdown","ec931bc3":"markdown","9a1da0d7":"markdown","5d7f7f04":"markdown","49e31a98":"markdown","01630267":"markdown","55ba9a85":"markdown","45df1a81":"markdown","785e72df":"markdown","588ee5c8":"markdown","4ef1210a":"markdown","8c557047":"markdown","0fbd6d28":"markdown","663da061":"markdown","6aed9800":"markdown","0fc8a844":"markdown","560c579c":"markdown","16c90e21":"markdown","32ff4882":"markdown","36e2c30f":"markdown","b7fc58c3":"markdown","bd8bd507":"markdown","5edaf854":"markdown","05efccfa":"markdown","3c59be25":"markdown","67d50d56":"markdown","e0a2bec1":"markdown","465cdac3":"markdown","316d21e9":"markdown","afd61c81":"markdown","45b39a6d":"markdown","9272d1dc":"markdown"},"source":{"16c4d59c":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom plotnine import *\n\n\n%matplotlib inline","6e653473":"df = pd.read_csv('..\/input\/ikea-sa-furniture-web-scraping\/IKEA_SA_Furniture_Web_Scrapings_sss.csv',  index_col=0)","bc4925e7":"df.head()","5a191236":"df.describe()","d8f2f583":"df.info()","a0020b30":"df.nunique()","05117a8b":"df.duplicated().sum()","76f05d61":"df['category'].value_counts()","d03bba45":"df.category.unique()","853dda57":"df.other_colors.unique()","c34e7f44":"df.sellable_online.unique()","22dc1363":"df.old_price.unique()[:20]","6cd1587a":"def fix_old_price(df):\n    '''modify old_price feature'''\n    \n    if df['old_price']  == 'No old price':\n        return df['price']\n\n    elif df['old_price'][-4:] != 'pack':\n        return float(str(df['old_price'])[3:].replace(',',''))\n        \n    else:\n        return np.nan\n\ndf['discounted'] = (df['old_price'] != 'No old price').astype(int)   \ndf['old_price'] = df.apply(fix_old_price, axis=1)\ndf[['price', 'old_price', 'discounted']].head()","6c15ecd8":"df[['depth', 'height', 'width']].isna().head(5)","664ee75a":"ggplot(df, aes(xmin = 0, ymin = 0, xmax = 'width', ymax = 'height', colour = 'category')) + \\\ngeom_rect(alpha = 0.05, fill = \"#FFFFFF\", size = 1) + \\\nscale_x_continuous(limits = (0, 200)) + \\\nscale_y_continuous(limits = (0, 200)) + \\\nfacet_wrap('category', ncol = 3) + \\\nguides()+ \\\ncoord_fixed() +\\\ntheme(figure_size=(9, 9)) ","7d45dd8c":"ggplot(df, aes(xmin = 0, ymin = 0, xmax = 'width', ymax = 'height', colour = 'price', size='price',  fill = 'sellable_online')) + \\\ngeom_rect(alpha = 0.05, fill = \"#FFFFFF\", size = 1) + \\\nscale_x_continuous(limits = (0, 200)) + \\\nscale_y_continuous(limits = (0, 200)) + \\\nfacet_wrap('category', ncol = 3) + \\\nguides()+ \\\ncoord_fixed() +\\\ntheme(figure_size=(9, 9)) ","51b57520":"df.groupby('category')[['width', 'height', 'depth']].apply(lambda x: x.notnull().sum())","4764725b":"df['width_d'] = (df['width'].notnull()).astype(int)\ndf['height_d'] = (df['height'].notnull()).astype(int)\ndf['depth_d'] = (df['depth'].notnull()).astype(int)\ndf[['width', 'height', 'depth', 'width_d', 'height_d', 'depth_d']].head(5)","e6979581":"df[['width', 'height', 'depth']] = df.groupby(['category'])['width', 'height', 'depth'].transform(lambda x: x.fillna(x.mean()))","85ab607c":"df.groupby('category')[['width', 'height', 'depth']].apply(lambda x: x.notnull().sum())","bc0d1e3a":"cols = ['item_id', 'name','link', 'short_description',\n        'designer']\ndf2 = df.drop(cols, axis=1)\ndf2.columns","9bddb479":"df2.isna().sum()","a20be0be":"df2.dropna(inplace=True)\ndf2.isna().sum()","93ffbb62":"df2.head()","26f24946":"order = df['category'].value_counts().index\ncolor0 = sns.color_palette()[0]\ncolor1 = sns.color_palette()[1]\n\nplt.figure(figsize=[10, 6])\n\n\nsns.countplot(data=df2, y='category', order=order, color=color1)","88943ff7":"binsize = 500\n\nplt.figure(figsize=[16, 5])\nplt.hist(data=df2, x='price',bins=binsize, color=color1)\n\nplt.xlabel('Price');","176bde6c":"binsize = 500\n\nplt.figure(figsize=[16, 5])\nplt.hist(data=df2, x='price',bins=binsize, color=color1)\nplt.xlim(0,1500)\n\nplt.xlabel('Price');","ce78a206":"binsize = 500\n\nplt.figure(figsize=[16, 5])\nplt.hist(data=df2, x='old_price',bins=binsize, color=color0)\nplt.xlim(0,1500)\n\nplt.xlabel('Old Price');","30fe1cbc":"binsize = 500\n\nplt.figure(figsize=[16, 5])\nplt.hist(data=df2, x='old_price',bins=binsize)\nplt.hist(data=df2, x='price',bins=binsize)\n\nplt.xlim(0,1500)\n\nplt.xlabel('Price vs Old Price');","9f20de74":"selable_online_count = df2['sellable_online'].value_counts()\n\nplt.figure(figsize=[6, 6])\nexplode = (0, 0.4)\n\nplt.pie(selable_online_count, explode=explode, autopct='%1.1f%%');\nplt.legend(df2['sellable_online'].unique())","6c15e665":"other_colors_count = df2['other_colors'].value_counts()\n\nplt.figure(figsize=[6, 6])\nexplode = (0, 0.1)\n\nplt.pie(other_colors_count, autopct='%1.1f%%')\nplt.legend(df2['other_colors'].unique());","59d60c64":"other_colors_count = df2['discounted'].value_counts()\n\nplt.figure(figsize=[6, 6])\nexplode = (0, 0.1)\n\nplt.pie(other_colors_count, autopct='%1.1f%%')\nplt.legend(df2['other_colors'].unique());","baad5290":"binsize = 30\n\nmeasures = ['width', 'height', 'depth']\n\nfig, ax = plt.subplots(nrows=3, figsize = [6,8])\nfor index, measure in enumerate(measures): \n    ax[index].hist(data=df2, x=measure, bins=binsize, color=color0)\n    ax[index].set_ylabel(measure);\n    ax[index].set_xlabel('');","f5050a6b":"plt.figure(figsize=[16, 6])\n\nsns.scatterplot(data=df2, x=\"old_price\", y=\"price\", alpha=0.3);","38732d51":"plt.figure(figsize=[16, 6])\n\nsns.scatterplot(data=df2.query('old_price < 500'), x=\"old_price\", y=\"price\", alpha=0.3)","5c916523":"plt.figure(figsize=[16, 6])\n\ndf2['discount_amount'] = df2['old_price'] - df2['price']\n\nsns.scatterplot(data=df2, x=\"price\", y=\"discount_amount\", alpha=0.4)","30e20b8a":"plt.figure(figsize=[16, 6])\n\nsns.scatterplot(data=df2.query('price < 3000'), x=\"price\", y=\"discount_amount\", alpha=0.5)","311c05c2":"plt.figure(figsize=[16, 6])\nresult = df.groupby([\"category\"])['price'].aggregate(np.mean).reset_index().sort_values('price', ascending=False)\n\nsns.barplot(data=df2, y='price', x='category', color=color0, order=result['category'])\n\nplt.xticks(rotation=90);","990bd566":"plt.figure(figsize=[6, 4])\n\nsns.barplot(data=df2, y='price', x='other_colors', color=color1)\nplt.xticks(rotation=90);","b4b9cf61":"plt.figure(figsize=[6, 4])\n\nsns.barplot(data=df2, y='price', x='sellable_online', color=color0)\nplt.xticks(rotation=90);","07d74433":"order = df['category'].value_counts().index\n\nsns.catplot(data=df2, x=\"category\", hue='other_colors', kind=\"count\", order=order, height=8, aspect=12\/10)\n\nplt.xticks(rotation=90);","885bd6f5":"binsize = 30\n\nmeasures = ['width', 'height', 'depth']\n\nfig, ax = plt.subplots(nrows=3, figsize = [6,8])\nfor index, measure in enumerate(measures): \n    sns.scatterplot(data=df2, x=\"price\", y=measure, alpha=0.5, ax = ax[index])\n    ax[index].set_ylabel(measure);\n    ax[index].set_xlabel('');","a3ea8fd4":"df2['size'] = (np.where(df2['depth_d'] == 1, df2['depth'],1)) *\\\n(np.where(df2['width_d'] == 1, df2['width'],1)) *\\\n(np.where(df2['height_d'] == 1, df2['height'],1))\n\n\ndf2[['size', 'width', 'height', 'depth', 'width_d', 'height_d', 'depth_d']].head(10)","de719ec3":"plt.figure(figsize=[16, 6])\n\nsns.scatterplot(data=df2, x=\"price\", y=\"size\", alpha=0.5)","77c2ea67":"plt.figure(figsize=[16, 6])\n\nsns.scatterplot(data=df2.query('price < 3000'), x=\"price\", y=\"size\", hue='discounted', alpha=0.5)\n","cd1bec9b":"result = df.groupby([\"category\"])['price'].aggregate(np.mean).reset_index().sort_values('price', ascending=False)\n\nsns.catplot(data=df2, x=\"category\", hue='other_colors', kind=\"bar\", y='price', order=result['category'], height=10, aspect=12\/9)\n\nplt.xticks(rotation=90);","5f21d91d":"result = df.groupby([\"category\"])['price'].aggregate(np.mean).reset_index().sort_values('price', ascending=False)\n\nsns.catplot(data=df2, x=\"category\", hue='discounted', kind=\"bar\", y='price', order=result['category'], height=10, aspect=12\/9)\n\nplt.xticks(rotation=90);","cb1c91ec":"plt.figure(figsize=[16, 10])\n\nsns.scatterplot(data=df2, x=\"width\", y=\"height\", size='price', hue='price')","dfc68a63":"df2.head()","dbe64b9d":"df2.to_csv('clean_IKEA_dataset.csv', index=False)","72ec28ee":"from sklearn.preprocessing import MinMaxScaler, LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import utils","d1a885a1":"df = df2.copy()\ndf.head()","727e8c83":"scaler = MinMaxScaler()\n\ndf[['size', 'width', 'height', 'depth', 'discount_amount','price']] = scaler.fit_transform(df[['size', 'width', 'height', 'depth', 'discount_amount','price']])\ndf.head()","9b82ae13":"# encode class values as integers\nencoder = LabelEncoder()\nencoded_Y = encoder.fit_transform(df.category)\n# convert integers to dummy variables (i.e. one hot encoded)\ndummy_y = utils.to_categorical(encoded_Y)\n","2329eb9d":"df_train , df_test, dummy_y_train, dummy_y_test = train_test_split(df, dummy_y, shuffle=True, test_size=0.3)","8ae4e9f9":"feature_columns = []\n\ndiscount_amount = tf.feature_column.numeric_column(\"discount_amount\")\nfeature_columns.append(discount_amount)\n\nsize = tf.feature_column.numeric_column(\"size\")\nfeature_columns.append(size)\n\nwidth = tf.feature_column.numeric_column(\"width\")\nfeature_columns.append(width)\n\nheight = tf.feature_column.numeric_column(\"height\")\nfeature_columns.append(height)\n\ndepth = tf.feature_column.numeric_column(\"depth\")\nfeature_columns.append(depth)\n\nwidth_d = tf.feature_column.numeric_column(\"width_d\")\nfeature_columns.append(width_d)\n\nheight_d = tf.feature_column.numeric_column(\"height_d\")\nfeature_columns.append(height_d)\n\ndepth_d = tf.feature_column.numeric_column(\"depth_d\")\nfeature_columns.append(depth_d)\n\nother_colors = tf.feature_column.categorical_column_with_vocabulary_list(\n    key='other_colors', vocabulary_list=('Yes', 'No'), default_value=0)\nfeature_columns.append(tf.feature_column.indicator_column(other_colors))\n\ncategory = tf.feature_column.categorical_column_with_vocabulary_list(\n    key='category', vocabulary_list=('Bar furniture', 'Beds', 'Bookcases & shelving units',\n                                     'Cabinets & cupboards', 'Caf\u00e9 furniture', 'Chairs',\n                                     'Chests of drawers & drawer units', \"Children's furniture\",\n                                     'Nursery furniture', 'Outdoor furniture', 'Room dividers',\n                                     'Sideboards, buffets & console tables', 'Sofas & armchairs',\n                                     'Tables & desks', 'Trolleys', 'TV & media furniture', 'Wardrobes'),\n    default_value=0)\nfeature_columns.append(tf.feature_column.indicator_column(category))\n\nmy_feature_layer = tf.keras.layers.DenseFeatures(feature_columns)","ef95c71e":"#@title Define the plotting function.\n\ndef plot_the_loss_curve(epochs, mse):\n  \"\"\"Plot a curve of loss vs. epoch.\"\"\"\n\n  plt.figure()\n  plt.xlabel(\"Epoch\")\n  plt.ylabel(\"Mean Squared Error\")\n\n  plt.plot(epochs, mse, label=\"Loss\")\n  plt.legend()\n  plt.ylim([mse.min()*0.95, mse.max() * 1.03])\n  plt.show()  \n\nprint(\"Defined the plot_the_loss_curve function.\")","34c8ea31":"def create_model(my_learning_rate, my_feature_layer):\n  \"\"\"Create and compile a simple linear regression model.\"\"\"\n  # Most simple tf.keras models are sequential.\n  model = tf.keras.models.Sequential()\n\n  # Add the layer containing the feature columns to the model.\n  model.add(my_feature_layer)\n\n  # Describe the topography of the model by calling the tf.keras.layers.Dense\n  # method once for each layer. We've specified the following arguments:\n  #   * units specifies the number of nodes in this layer.\n  #   * activation specifies the activation function (Rectified Linear Unit).\n  #   * name is just a string that can be useful when debugging.\n\n  # Define the first hidden layer with 20 nodes.   \n  model.add(tf.keras.layers.Dense(units=20, \n                                  activation='relu', \n                                  kernel_regularizer=tf.keras.regularizers.l2(l=0.0),\n                                  name='Hidden1'))\n  \n  # Define the second hidden layer with 10 nodes. \n  model.add(tf.keras.layers.Dense(units=10, \n                                  activation='relu', \n                                  kernel_regularizer=tf.keras.regularizers.l2(l=0.0),\n                                  name='Hidden2'))\n\n  \n  # Define the output layer.\n  model.add(tf.keras.layers.Dense(units=1,  \n                                  name='Output'))                              \n  \n  model.compile(optimizer=tf.keras.optimizers.Adam(lr=my_learning_rate),\n                loss=\"mean_squared_error\",\n                metrics=[tf.keras.metrics.MeanSquaredError()])\n\n  return model\n\n\ndef train_model(model, dataset, epochs, label_name,\n                batch_size=None):\n  \"\"\"Train the model by feeding it data.\"\"\"\n\n  # Split the dataset into features and label.\n  features = {name:np.array(value) for name, value in dataset.items()}\n  label = np.array(features.pop(label_name))\n  history = model.fit(x=features, y=label, batch_size=batch_size,\n                      epochs=epochs, shuffle=True) \n\n  # The list of epochs is stored separately from the rest of history.\n  epochs = history.epoch\n  \n  # To track the progression of training, gather a snapshot\n  # of the model's mean squared error at each epoch. \n  hist = pd.DataFrame(history.history)\n  mse = hist[\"mean_squared_error\"]\n\n  return epochs, mse","6fec4a5c":"# The following variables are the hyperparameters.\nlearning_rate = 0.01\nepochs = 20\nbatch_size = 2\n\n\n# Specify the label\nlabel_name = \"price\"\n\n# Establish the model's topography.\nmy_model = create_model(learning_rate, my_feature_layer)\n\n# Train the model on the normalized training set. We're passing the entire\n# normalized training set, but the model will only use the features\n# defined by the feature_layer.\nepochs, mse = train_model(my_model, df_train, epochs, \n                          label_name, batch_size)\nplot_the_loss_curve(epochs, mse)\n\n# After building a model against the training set, test that model\n# against the test set.\ntest_features = {name:np.array(value) for name, value in df_test.items()}\ntest_label = np.array(test_features.pop(label_name)) # isolate the label\nprint(\"\\n Evaluate the new model against the test set:\")\nmy_model.evaluate(x = test_features, y = test_label, batch_size=batch_size)","9f65260f":"feature_columns = []\n\nprice = tf.feature_column.numeric_column(\"price\")\nfeature_columns.append(price)\n\ndiscount_amount = tf.feature_column.numeric_column(\"discount_amount\")\nfeature_columns.append(discount_amount)\n\nsize = tf.feature_column.numeric_column(\"size\")\nfeature_columns.append(size)\n\nwidth = tf.feature_column.numeric_column(\"width\")\nfeature_columns.append(width)\n\nheight = tf.feature_column.numeric_column(\"height\")\nfeature_columns.append(height)\n\ndepth = tf.feature_column.numeric_column(\"depth\")\nfeature_columns.append(depth)\n\nwidth_d = tf.feature_column.numeric_column(\"width_d\")\nfeature_columns.append(width_d)\n\nheight_d = tf.feature_column.numeric_column(\"height_d\")\nfeature_columns.append(height_d)\n\ndepth_d = tf.feature_column.numeric_column(\"depth_d\")\nfeature_columns.append(depth_d)\n\nother_colors = tf.feature_column.categorical_column_with_vocabulary_list(\n    key='other_colors', vocabulary_list=('Yes', 'No'), default_value=0)\nfeature_columns.append(tf.feature_column.indicator_column(other_colors))\n\n\nmy_feature_layer = tf.keras.layers.DenseFeatures(feature_columns)","ac07c704":"#@title Define the plotting function\ndef plot_curve(epochs, hist, list_of_metrics):\n  \"\"\"Plot a curve of one or more classification metrics vs. epoch.\"\"\"  \n  # list_of_metrics should be one of the names shown in:\n  # https:\/\/www.tensorflow.org\/tutorials\/structured_data\/imbalanced_data#define_the_model_and_metrics  \n\n  plt.figure()\n  plt.xlabel(\"Epoch\")\n  plt.ylabel(\"Value\")\n\n  for m in list_of_metrics:\n    x = hist[m]\n    plt.plot(epochs[1:], x[1:], label=m)\n\n  plt.legend()\n\nprint(\"Loaded the plot_curve function.\")","c8a1bb6a":"def create_model(my_learning_rate, my_feature_layer):\n  \"\"\"Create and compile a deep neural net.\"\"\"\n  \n  # All models in this course are sequential.\n  model = tf.keras.models.Sequential()\n\n  # The features are stored in a two-dimensional 28X28 array. \n  # Flatten that two-dimensional array into a a one-dimensional \n  # 784-element array.\n  model.add(my_feature_layer)\n\n  # Define the first hidden layer.   \n  model.add(tf.keras.layers.Dense(units=500, activation='relu'))\n  model.add(tf.keras.layers.Dropout(rate=0.2))\n\n  model.add(tf.keras.layers.Dense(units=200, activation='relu'))\n  model.add(tf.keras.layers.Dropout(rate=0.2))\n\n  model.add(tf.keras.layers.Dense(units=20, activation='relu'))\n  model.add(tf.keras.layers.Dropout(rate=0.2))\n\n\n  # Output Layer\n  model.add(tf.keras.layers.Dense(units=17, activation='softmax'))     \n                           \n  # Construct the layers into a model that TensorFlow can execute.  \n  # Notice that the loss function for multi-class classification\n  # is different than the loss function for binary classification.  \n  model.compile(optimizer=tf.keras.optimizers.Adam(lr=my_learning_rate),\n                loss=\"categorical_crossentropy\",\n                metrics=['accuracy'])\n  \n  return model    \n\n\ndef train_model(model, dataset, train_label, epochs,\n                batch_size=None):\n  \"\"\"Train the model by feeding it data.\"\"\"\n\n  # Split the dataset into features and label.\n  features = {name:np.array(value) for name, value in dataset.items()}\n  label = np.array(features.pop(label_name))\n\n\n  history = model.fit(x=features, y=train_label, batch_size=batch_size,\n                      epochs=epochs, shuffle=True) \n\n  # The list of epochs is stored separately from the rest of history.\n  epochs = history.epoch\n  \n  # To track the progression of training, gather a snapshot\n  # of the model's mean squared error at each epoch. \n  hist = pd.DataFrame(history.history)\n\n  return epochs, hist","a0de3d03":"# The following variables are the hyperparameters.\nlearning_rate = 0.001\nepochs = 200\nbatch_size = 5\n\n\nlabel_name = \"category\"\n# Establish the model's topography.\nmy_model = create_model(learning_rate, my_feature_layer)\n\n# Train the model on the normalized training set.\nepochs, hist = train_model(my_model, df_train, dummy_y_train, \n                           epochs, batch_size)\n\n# Plot a graph of the metric vs. epochs.\nlist_of_metrics_to_plot = ['accuracy']\nplot_curve(epochs, hist, list_of_metrics_to_plot)\n\n# Evaluate against the test set.\nprint(\"\\n Evaluate the new model against the test set:\")\nfeatures = {name:np.array(value) for name, value in df_test.items()}\n\nmy_model.evaluate(x=features, y=dummy_y_test, batch_size=batch_size)","aa954802":"<a id='intro'><\/a>\n## Introduction\n\n### Context:\nThis dataset is a practice of web scraping techniques. The web scraping has been applied on IKEA Saudi Arabian website for the furniture category. The scraped website link: https:\/\/www.ikea.com\/sa\/en\/cat\/furniture-fu001\/\n\nThe data requested by 4\/20\/2020. <br>\ndataset: https:\/\/www.kaggle.com\/ahmedkallam\/ikea-sa-furniture-web-scraping\n\n### Content:\n\n* item_id : item id wich can be used later to merge with other IKEA dataframes\n* name: the commercial name of items\n* category:the furniture category that the item belongs to (Sofas, beds, chairs, Trolleys,\u2026)\n* Price: the current price in Saudi Riyals as it is shown in the website by 4\/20\/2020\n* old_price: the price of item in Saudi Riyals before discount\n* Short_description: a brief description of the item\n* full_Description: a very detailed description of the item. Because it is long, it is dropped from the final dataframe, but it   is available in the code in case it needs to be analyzed.\n* designer: The name of the designer who designed the item. this is extracted from the full_description column.\n* size: the dimensions of the item including a lot of details.As a lot of dimensions mentioned and they vary from item to item,\n  the most common dimensions have been extracted which are: Height, Wideh, and Depth. This column is dropped from the final       dataframe, but it is available in the code in case it is needed.\n* width: Width of the item in Centimeter\n* height: Height of the item in Centimeter\n* depth: Depth of the item in Centimeter\n* sellable_Online: if the item is available for online purchasing or in-stores only (Boolean)\n* other_colors: if other colors are available for the item, or just one color as displayed in the website (Boolean)\n* link: the web link of the item\n\n### Licences:\nThe scraped website link: https:\/\/www.ikea.com\/sa\/en\/cat\/furniture-fu001\/","a451c209":"* old price as expected follow the price shape","3bddb4fb":"<a id='prep'><\/a>\n## Imports and preperations","522b0ec7":"* max value of price might be an indication of an outlier, a visualization will indicate more.\n","2fe27d08":"* old_price feature needs some modifications:\n    1. remove the \"SR \" string\n    2. Change the \"No old price\" to the same price as now\n    3. make it float","0f0099d6":"<a id='wrangling'><\/a>\n## Data Wrangling","ec931bc3":"* looks like there is a peak in prices every 100 SR and much noticeable at the 1000 SR mark","9a1da0d7":"* price looks kinda log shape, lets zome in a little bit","5d7f7f04":"\n# 1. Investigating IKEA Furniture Dataset\n\n## Table of Contents\n<ul>\n<li><a href=\"#intro\">Introduction<\/a><\/li>\n<li><a href=\"#wrangling\">Data Wrangling<\/a><\/li>\n<li><a href=\"#eda\">Exploratory Data Analysis<\/a><\/li>\n<li><a href=\"#conclusions\">Conclusions<\/a><\/li>\n<\/ul>\n\n# 2. Predicting IKEA's features\n\n## Table of Contents\n    \n<ul>\n<li><a href=\"#intro\">Introduction<\/a><\/li>\n<li><a href=\"#prep\">Imports and preperations<\/a><\/li>\n<li><a href=\"#model_1\">Model 1<\/a><\/li>\n<li><a href=\"#model_2\">Model 2<\/a><\/li>\n<li><a href=\"#conclusions\">Conclusions<\/a><\/li>\n<\/ul>","49e31a98":"### Investigating depth, height, width","01630267":"* hug amount of null values in the depth height and width features\n* old_price is an object, might required more looking","55ba9a85":"### Dropping unused columns and the 10 weird old_price's raw","45df1a81":"## Bivariate Exploration\n\n### 1. Price vs Old Price","785e72df":"* No Duplicates","588ee5c8":"<a id='eda'><\/a>\n## Exploratory Data Analysis\n\n\n* now after we did our cleaning, let's look at the data and look for any interesting insights \n\n\n## Univariate Exploration","4ef1210a":"* All features have a variance of unique values but category, sellable_online, and other_colors could be useful in the analysis","8c557047":"### Measures vs Price","0fbd6d28":"* items with other colors are more expensive","663da061":"# Add new column for if an Item discounted or not (todo)","6aed9800":"### Investigating Old Price","0fc8a844":"## Importing and Loading data","560c579c":"### 2. Price vs Categorical Variables","16c90e21":"* Nice, looks like each category have almost similar shapes\n* now let's count the valid (not null) values of each measure","32ff4882":"<a id='model_2'><\/a>\n## Model 2: Category classifier","36e2c30f":"<a id='model_1'><\/a>\n## Model 1: Item's price prediction","b7fc58c3":"### Investigating Quantitative values","bd8bd507":"* from this visualization, we found:\n    1. most of the items don't have any discount on it\n    2. for low prices there is two line relations, one that shares the same line with the high prices and one limited only for low prices\n    3. this relation is roughly 25% discount\n    4. items from 8k to 10k SR don't have any discount that follows this relation, 200 SR discount only","5edaf854":"* some patterns found in some categories, like for example most of the Trolleys don't have depth\n<br><br>\n* now I will fill the null values with its category mean but first I will make 3 new columns indicating if it was available before or not (for other purposes)","05efccfa":"* looks like the null values can be one of the three or more\n* before fixing those values let's plot them.\n    - from: https:\/\/www.kaggle.com\/pozdniakov\/ikea-furniture\n    - Inspiration: https:\/\/twitter.com\/henrywrover2\/status\/1323626098924621825","3c59be25":"* And here brighter colors (yellowish) means its pricer than others","67d50d56":"* here I found 10 values with the ward pack in it, so I decided to drop them (later)\n* and I applied the modifications mentioned above\n\n<hr>\n","e0a2bec1":"* items that are sellable online are more expensive than those are local only","465cdac3":"* interesting relation between old price and price, here we can see a linear increase in the value of discounts the more the price increases\n* maybe we can see more if we look at the relation between prices and the discount amount","316d21e9":"\n<hr>","afd61c81":"<hr>","45b39a6d":"* here we see that most items are sellable online (99.2)\n* and only 40% of items have other colors","9272d1dc":"<a id='conclusions'><\/a>\n## Conclusions"}}