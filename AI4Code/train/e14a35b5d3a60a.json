{"cell_type":{"2dfe2034":"code","ce6d8875":"code","6de5b0ef":"code","4a13842b":"code","1cf646c0":"code","2b4a4fb0":"code","137cf7e0":"code","9417d7e8":"code","9e25e053":"code","4ba824d8":"code","304cdf8c":"code","ab4318dd":"code","7f322ea1":"code","53d4d86f":"code","411ad1e6":"code","f44313ea":"code","544c7afe":"code","e0cd9cc1":"code","449c05d9":"code","ff579782":"code","41ff9fef":"code","c7067001":"code","68278678":"code","f1e70430":"code","4b53142b":"code","484d7b6a":"code","1fa638b4":"code","454350f2":"code","516dc3d9":"code","116abce9":"code","c9e5e7c7":"code","5b73b7c5":"code","35b1db4b":"code","0b8075af":"code","5aa3de14":"code","5c296baf":"code","6c1e5ddb":"code","4af3d924":"code","de9680f6":"code","db896cbb":"code","8f58e05c":"markdown","334b9917":"markdown","0781be3b":"markdown","ad20ce6f":"markdown","7f8e9ae1":"markdown","00349976":"markdown","276330d7":"markdown"},"source":{"2dfe2034":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os, cv2, random, time, shutil, csv\nimport tensorflow as tf\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nfrom tqdm import tqdm\nnp.random.seed(42)\n%matplotlib inline \nimport json\nimport os\nimport cv2\nimport keras\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.models import Model\nfrom keras.layers import BatchNormalization, Dense, GlobalAveragePooling2D, Lambda, Dropout, InputLayer, Input\nfrom keras.utils import to_categorical\nfrom tensorflow.keras.preprocessing.image import load_img","ce6d8875":"#Data Paths\ntrain_dir = '\/kaggle\/input\/dog-breed-identification\/train\/'\ntest_dir = '\/kaggle\/input\/dog-breed-identification\/test\/'\n#Count\/Print train and test samples.\n","6de5b0ef":"#Read train labels.\nlabels_dataframe = pd.read_csv('\/kaggle\/input\/dog-breed-identification\/labels.csv')\n#Read sample_submission file to be modified by pridected labels.\n# sample_df = pd.read_csv('\/kaggle\/input\/dog-breed-identification\/sample_submission.csv')\n#Incpect labels_dataframe.\nlabels_dataframe.head(5)","4a13842b":"#Create list of alphabetically sorted labels.\ndog_breeds = sorted(list(set(labels_dataframe['breed'])))\nn_classes = len(dog_breeds)\nprint(n_classes)\ndog_breeds[:10]","1cf646c0":"# with open('\/kaggle\/working\/dog_breeds.json', 'w') as fp:\n#     json.dump(dog_breeds,fp)","2b4a4fb0":"#Map each label string to an integer label.\nclass_to_num = dict(zip(dog_breeds, range(n_classes)))\nclass_to_num","137cf7e0":"class_to_num['toy_poodle']","9417d7e8":"# with open('\/kaggle\/working\/class_to_num.json', 'w') as fp:\n#     json.dump(class_to_num,fp)","9e25e053":"labels_dataframe['file_name'] = labels_dataframe['id'].apply(lambda x:train_dir+f\"{x}.jpg\")","4ba824d8":"labels_dataframe.head(5)['file_name'][0]","304cdf8c":"labels_dataframe.head()","ab4318dd":"codes = []\nfor index, data in labels_dataframe.iterrows():\n    breed = data['breed']\n    codes.append(class_to_num[breed])","7f322ea1":"codes","53d4d86f":"y = to_categorical(codes) # encoded our features ","411ad1e6":"y","f44313ea":"# from keras.applications.inception_resnet_v2 import InceptionResNetV2, preprocess_input as resnet_preprocess\n# from keras.applications.inception_v3 import InceptionV3, preprocess_input as inception_preprocess\n# from keras.applications.xception import Xception, preprocess_input as xception_preprocess\n# from keras.applications.nasnet import NASNetLarge, preprocess_input as nasnet_preprocess\n# from keras.layers.merge import concatenate\n\n# input_shape = (331,331,3)\n# input_layer = Input(shape=input_shape)\n\n\n# #first extractor inception_resnet\n# preprocessor_resnet = Lambda(resnet_preprocess)(input_layer)\n# inception_resnet = InceptionResNetV2(weights = 'imagenet',\n#                                      include_top = False,input_shape = input_shape,pooling ='avg')(preprocessor_resnet)\n\n# # second extractor InceptionV3\n# preprocessor_inception = Lambda(inception_preprocess)(input_layer)\n# inception_v3 = InceptionV3(weights = 'imagenet',\n#                                      include_top = False,input_shape = input_shape,pooling ='avg')(preprocessor_inception)\n# # Third extractor Xception\n# preprocessor_xception = Lambda(xception_preprocess)(input_layer)\n# xception = Xception(weights = 'imagenet',\n#                                      include_top = False,input_shape = input_shape,pooling ='avg')(preprocessor_xception)\n\n# # fourth extractor nasnet large\n# preprocessor_nasnet = Lambda(nasnet_preprocess)(input_layer)\n# nasnet = NASNetLarge(weights = 'imagenet',\n#                                      include_top = False,input_shape = input_shape,pooling ='avg')(preprocessor_nasnet)\n\n\n# merge = concatenate([inception_v3, xception,nasnet,inception_resnet])\n# model = Model(inputs = input_layer, output = merge)\n\n\n# from keras.applications.inception_resnet_v2 import InceptionResNetV2, preprocess_input as resnet_preprocess\n# from keras.applications.inception_v3 import InceptionV3, preprocess_input as inception_preprocess\nfrom keras.applications.xception import Xception, preprocess_input as xception_preprocess\nfrom keras.applications.nasnet import NASNetMobile, preprocess_input as nasnet_preprocess\nfrom keras.applications.densenet import DenseNet121, preprocess_input as densenet_preprocess\n# from keras.applications.mobilenet_v2 import MobileNetV2, preprocess_input as mobilenet_preprocess\nfrom keras.applications.resnet_v2 import ResNet50V2, preprocess_input as resnet_v2_preprocess\nfrom keras.layers.merge import concatenate\nfrom keras.layers import Input,Lambda\nfrom keras.models import Model\n\ninput_shape = (224,224,3)\ninput_layer = Input(shape=input_shape)\n\n\n#first extractor resnet50v2\npreprocessor_resnet = Lambda(resnet_v2_preprocess)(input_layer)\nresnet = ResNet50V2(weights = 'imagenet',\n                                     include_top = False,input_shape = input_shape,pooling ='avg')(preprocessor_resnet)\n\n# second extractor densenet\npreprocessor_inception = Lambda(densenet_preprocess)(input_layer)\ndensenet = DenseNet121(weights = 'imagenet',\n                                     include_top = False,input_shape = input_shape,pooling ='avg')(preprocessor_inception)\n# # Third extractor nasnet\npreprocessor_xception = Lambda(nasnet_preprocess)(input_layer)\nnasnet = NASNetMobile(weights = 'imagenet',\n                                     include_top = False,input_shape = input_shape,pooling ='avg')(preprocessor_xception)\n# Third extractor Xception\n# preprocessor_xception = Lambda(xception_preprocess)(input_layer)\n# xception = Xception(weights = 'imagenet',\n#                                      include_top = False,input_shape = input_shape,pooling ='avg')(preprocessor_xception)\n# # # # fourth extractor nasnet large\n# preprocessor_nasnet = Lambda(nasnet_preprocess)(input_layer)\n# nasnet = NASNetLarge(weights = 'imagenet',\n#                                      include_top = False,input_shape = input_shape,pooling ='avg')(preprocessor_nasnet)\n\n\nmerge = concatenate([resnet,nasnet,densenet])\nmodel = Model(inputs = input_layer, outputs = merge)","544c7afe":"model.save('\/kaggle\/working\/feature_extractor.h5')","e0cd9cc1":"v= model.output.shape[1:]\nprint(v)","449c05d9":"print(v)","ff579782":"# for feature_extraction dataframe must have to contain file_name and  breed columns\ndef feature_extractor(df):\n    img_size = (224,224,3)\n    data_size = len(df)\n    batch_size = 20\n    X = np.zeros([data_size,4128], dtype=np.uint8)\n#     y = np.zeros([data_size,120], dtype=np.uint8)\n    datagen = ImageDataGenerator()\n    generator = datagen.flow_from_dataframe(df,\n    x_col = 'file_name', class_mode = None, \n    batch_size=20, shuffle = False,target_size = (img_size[:2]),color_mode = 'rgb')\n    i = 0\n    for input_batch in tqdm(generator):\n        input_batch = model.predict(input_batch)\n        X[i * batch_size : (i + 1) * batch_size] = input_batch\n        i += 1\n        if i * batch_size >= data_size:\n           break\n    return X\n        \n    ","41ff9fef":"X = feature_extractor(labels_dataframe)","c7067001":"labels_dataframe","68278678":"X.shape","f1e70430":"from keras.callbacks import EarlyStopping,ModelCheckpoint, ReduceLROnPlateau\n#Prepare call backs\nEarlyStop_callback = keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\ncheckpoint = ModelCheckpoint('\/kaggle\/working\/checkpoing',\n                             monitor = 'val_loss',mode = 'min',save_best_only= True)\nlr = ReduceLROnPlateau(monitor = 'val_loss',factor = 0.5,patience = 3,min_lr = 0.00001)\nmy_callback=[EarlyStop_callback,checkpoint]","4b53142b":"dnn = keras.models.Sequential([\n    InputLayer(X.shape[1:]),\n    Dropout(0.7),\n    Dense(n_classes, activation='softmax')\n])\n\ndnn.compile(optimizer='adam',\n              loss='categorical_crossentropy',\n              metrics=['accuracy'])\n\n#Train simple DNN on extracted features.\nh = dnn.fit(X , y,\n            batch_size=128,\n            epochs=60,\n            validation_split=0.1 ,\n           callbacks = my_callback)","484d7b6a":"fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 12))\n\nax1.plot(h.history['loss'],color = 'b',label = 'loss')\nax1.plot(h.history['val_loss'],color = 'r',label = 'val_loss')\nax1.set_xticks(np.arange(1, 60, 1))\nax1.set_yticks(np.arange(0, 1, 0.1))\nax1.legend(['loss','val_loss'],shadow = True)\n\n\nax2.plot(h.history['accuracy'],color = 'green',label = 'accuracy')\nax2.plot(h.history['val_accuracy'],color = 'red',label = 'val_accuracy')\nax2.legend(['accuracy','val_accuracy'],shadow = True)\n# ax2.set_xticks(np.arange(1, 60, 1))\n# ax2.set_yticks(np.arange(0, 60, 0.1))\nplt.show()","1fa638b4":"\nfrom keras.models import load_model\ndnn.save('\/kaggle\/working\/dogbreed.h5')\n\n# saved_model = load_model('\/kaggle\/working\/dogbreed.h5')\n","454350f2":"\ntest_data = []\nfor pic in os.listdir(test_dir):\n    test_data.append(test_dir+pic)","516dc3d9":"test_data","116abce9":"test_dataframe = pd.DataFrame({'file_name':test_data})","c9e5e7c7":"test_dataframe","5b73b7c5":"test_features = feature_extractor(test_dataframe)","35b1db4b":"y_pred = dnn.predict(test_features)","0b8075af":"y_pred.shape","5aa3de14":"def get_key(val): \n        for key, value in class_to_num.items(): \n         if val == value: \n             return key \n            \npred_codes = np.argmax(y_pred, axis = 1)\npred_codes\n    ","5c296baf":"predictions = []\nfor i in pred_codes:\n    predictions.append(get_key(i))","6c1e5ddb":"predictions","4af3d924":"test_dataframe['breed'] = predictions","de9680f6":"test_dataframe","db896cbb":"\nplt.figure(figsize=(6,6))\n\nfor index , data in test_dataframe[:10].iterrows():\n    img = data['file_name']\n    label = data['breed']\n    img = cv2.imread(img)\n#     plt.subplot(2,5, index+1)\n    plt.imshow(img)\n    plt.xlabel(label,fontsize = (15))\n    plt.tight_layout()\n    plt.show()\n    \n","8f58e05c":"#### Model","334b9917":"### Saving and loading the model ","0781be3b":"## Feature extraction of data not labels","ad20ce6f":"### Plot the Result ","7f8e9ae1":"## Prediction of the Test Data","00349976":"### Drawing some images ","276330d7":"**Introduction**\n\n* This kernel is a detailed guide for transfer learning on Dog Breeds problem, it's all about learning a new technique, evaluate it using only Kaggle training set without cheating.\n\n* The aim of this kernel is to show you how to use pre-trained CNN models as feature extractors, which one of the most effective transfer learning techniques.\n\n* A reasonable question comes to your mind, 'Wait, why do we have to use this technique, why don't we just use regular transfer learning ?', if you try to do so, you will figure out that the problem is pretty hard for a single model to handle (you would get higher loss and less accuracy).\n\n* It's even hard for humankind to distinguish between 120 dog breeds!, single poor CNN would struggle.\n\n**Explanation**\n\n* Take look at general CNN architecture for image classification in two main parts, \u201cfeature extractor\u201d that based on conv-layers, and \u201cclassifier\u201d which usually based on fully connected layers:\n\n![image.png](attachment:image.png)\n\n* Simply, feature extractor could be created as follow > (Feature Extractor = Pretrained Model - Late Fully Connected Layers)\n\n* For example, InceptionV3 feature extractor (without last FC layer) outputs 2048 vector for each image sample, each value represent a certain feature of dog image (Coded in numerical values of course), like Dog color?, How big is his head?, Shape of the eyes?, length of the tale?, Size? .. etc\n\n* Hence, more \"different\" feature extractors mean more features to be used to determine which breed does this dog belong.\n\n* So our strategy goes as the following,\n  1. Create 4 feature extractor using different pre-trained CNN models\n  2. Extract features from raw data and stacks the features together.\n  3. Use a simple DNN with one dense layer and a heavy dropout layer to figure out patterns in the feature extracted from the data.\n     \n     \n* The code is simple, concise and fully-commented. Feel free to ask for help \/ more info \/ more explanation in the comments.\n\n* Finally if this kernel helps you somehow, kindly don't forget to leave a little upvote.\n\n* ENJOY.\n"}}