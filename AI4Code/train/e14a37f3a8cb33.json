{"cell_type":{"e26e8324":"code","3ed91870":"code","619b6a09":"code","de178433":"code","89e49942":"code","d698c499":"code","e95ae1ad":"code","df4f6099":"code","5915114d":"code","c56b8488":"code","27faa6c6":"code","cc29d8da":"code","184a058e":"code","cf1135ce":"code","f0f93f3b":"code","aea5d421":"code","e78751f5":"code","26d59533":"code","c3ebd25c":"code","63508e27":"code","acfbe635":"code","061f8a76":"code","26c477e3":"code","e8bb3ae0":"code","657e58fc":"code","61b1c19a":"code","1ffd8195":"code","019923f1":"code","9b4c69ab":"code","e63faeb6":"code","640110ed":"code","17ed793c":"code","769b5942":"code","1570d62a":"code","490787cc":"code","62af1a5f":"code","f94be2ec":"code","1e9a34be":"code","a6a175ea":"code","82a19000":"code","0846a371":"code","6b9f043f":"code","3d353f53":"code","9f1709ce":"code","d3bc7bb7":"code","6d1b032b":"code","75b003c1":"code","3942ec77":"code","59a24b91":"code","3f1c9acf":"code","a7fd2256":"code","b74f5b0c":"code","f6c80923":"code","c8839451":"code","c4b6b522":"code","e2704af6":"code","e07cf2c9":"code","6f307184":"code","f1adc642":"code","6072350d":"code","dd00dc5d":"code","4ba705bc":"code","8eaec017":"code","a86d6b6b":"code","0014a662":"code","b51712d1":"markdown","88809d21":"markdown","c1acbdf4":"markdown","83021f93":"markdown","bc4f3c66":"markdown","f60aac89":"markdown","ed95c055":"markdown","256dba13":"markdown"},"source":{"e26e8324":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport plotly.express as px\n\nfrom sklearn.cluster import KMeans, AgglomerativeClustering\nfrom sklearn.mixture import GaussianMixture\nfrom sklearn.preprocessing import StandardScaler, normalize\nfrom sklearn.metrics import silhouette_score\nimport scipy.cluster.hierarchy as shc","3ed91870":"# reading the ecommerce.csv\ntrans_df = pd.read_csv('\/kaggle\/input\/high-value-customers-identification\/Ecommerce.csv',encoding='ISO-8859-1')\ntrans_df.drop(columns='Unnamed: 8', axis=1, inplace=True)\ntrans_df.head()","619b6a09":"# Total Transaction: 541909\ntrans_df.shape","de178433":"# Null values\ntrans_df.isnull().sum()","89e49942":"# Triming all leading\/trailing whitespaces of all the string-based columns.\ntrans_df[trans_df.select_dtypes(['object']).columns] = \\\ntrans_df.select_dtypes(['object']).apply(lambda x: x.str.strip())\ntrans_df.head()","d698c499":"# No. of unique items\ntrans_df.StockCode.nunique()","e95ae1ad":"# No. of unique transaction\ntrans_df.InvoiceNo.nunique()","df4f6099":"# No. of unique customers\ntrans_df.CustomerID.nunique()","5915114d":"# No. of unique Country\ntrans_df.Country.nunique()","c56b8488":"# total sales per item per transaction.\ntrans_df['sales'] = trans_df.Quantity * trans_df.UnitPrice\ntrans_df.head()","27faa6c6":"trans_df[trans_df['CustomerID'].isnull()].head()","cc29d8da":"trans_df[trans_df['UnitPrice']==0].head()","184a058e":"# There are 9291 return & debt adjustment based invoice transactions. \n# And starts with 'C' or 'A' as prefix to invoice no.\ntrans_df[trans_df['InvoiceNo'].str.len() > 6].InvoiceNo.count()","cf1135ce":"# Any return or debt adjustment transactions, which purchase sales > 0\ntrans_df[(trans_df['InvoiceNo'].str.len() > 6) & (trans_df.sales>0)].InvoiceNo","f0f93f3b":"trans_df[trans_df['StockCode']=='B']","aea5d421":"# Need to remove all the return & debt adjustment transactions, as it doesn't state any info about HVC.\n# Striping off 'C', 'A' from the invoice no.: 'C': Return Transactions; 'A': Debt Adjustment\n# Some of the transaction after aggregating at invoice no, will become 0 as total sales per invoice.\n# yes, there are chances return transactions are not mapped to sales transactions.\n# Length of Normal sales invoice no = 6.\ntrans_df['InvoiceNo'] = trans_df['InvoiceNo'].apply(lambda x: x[1:] if len(str(x)) > 6 else x)","e78751f5":"temp_trans_df = trans_df.groupby('InvoiceNo', as_index=False).sales.sum()\ntemp_trans_df.rename(columns = {'sales':'tot_sales_per_invoice'}, inplace = True) \ntemp_trans_df.head()","26d59533":"# 5940 are return & debt adjustment transactions with negative sales\ntemp_trans_df[temp_trans_df.tot_sales_per_invoice<=0].count()","c3ebd25c":"# Valid transactions: 25900 - 5940 = 19960 \ntemp_trans_df= temp_trans_df[temp_trans_df.tot_sales_per_invoice>0]\ntemp_trans_df.shape","63508e27":"temp_trans_df.describe()","acfbe635":"# Mapping out with Valid Transactions only.\nderived_trans_df = pd.merge(trans_df.groupby(['InvoiceNo','CustomerID','Country'], as_index=False).sales.sum(),\\\n                            temp_trans_df, on='InvoiceNo')\n\nderived_trans_df.drop('sales', axis=1, inplace=True)\n\nderived_trans_df.rename(columns = {'InvoiceNo':'invoice_no', 'CustomerID':'customer_id',\\\n                                   'sales':'tot_sales_per_invoice'}, inplace = True) \n\nderived_trans_df.head()","061f8a76":"# There are still some anomalies on max tot_sales_per_invoice side.\nderived_trans_df.describe()","26c477e3":"derived_trans_df.shape","e8bb3ae0":"# Removing Outliers\nsns.boxplot(derived_trans_df.tot_sales_per_invoice) ","657e58fc":"derived_trans_df.boxplot(column='tot_sales_per_invoice')","61b1c19a":"'''\n# We could have done outlier removal beforehand rather than cleaning it manually.\n# But in that case min quantile was still considering negative sales transactions i.e., return transactions.\n# Below are the quantile values without removing manually.\n\nQ1:87.9625, Q3:418.0224999999999, IQR:330.05999999999995\nMin Quantile: -407.12749999999994\nMax Quantile: 913.1124999999998\nSkew Value: 0.5472467444617237\n'''\n\nQ1 = derived_trans_df.tot_sales_per_invoice.quantile(0.25)\nQ3 = derived_trans_df.tot_sales_per_invoice.quantile(0.75)\nIQR = Q3 - Q1\nprint(\"Q1:{}, Q3:{}, IQR:{}\".format(Q1, Q3, IQR))\nprint(\"Min Quantile:\", Q1 - 1.5 * IQR)\nprint(\"Max Quantile:\", Q3 + 1.5 * IQR)\nprint(\"Skew Value:\",derived_trans_df.tot_sales_per_invoice.skew())","1ffd8195":"derived_trans_df = derived_trans_df[~((derived_trans_df.tot_sales_per_invoice  < (Q1 - 1.5 * IQR)) |\\\n                                      (derived_trans_df.tot_sales_per_invoice  > (Q3 + 1.5 * IQR)))]\n\nderived_trans_df.head()","019923f1":"sns.boxplot(derived_trans_df.tot_sales_per_invoice) ","9b4c69ab":"derived_trans_df.shape","e63faeb6":"derived_trans_df.describe()","640110ed":"# total purchases per customer\nderived_trans_df = derived_trans_df.groupby(['customer_id'], as_index=False).agg({'tot_sales_per_invoice':'sum',\\\n                                                                                  'invoice_no':'count'})\nderived_trans_df.rename(columns = {'invoice_no':'tot_no_of_trans','tot_sales_per_invoice':'tot_purchases'}, \\\n                        inplace = True) \n\nderived_trans_df.head()","17ed793c":"derived_trans_df['avg_purchases_per_trans'] = \\\nderived_trans_df['tot_purchases'] \/ derived_trans_df['tot_no_of_trans']\nderived_trans_df.head()","769b5942":"derived_trans_df.describe().T","1570d62a":"# Validating customer ids, by looking for null values, \n# although it's been taken care already by using \"group by clause on customer id\" earlier.\nderived_trans_df.isnull().sum()","490787cc":"# No. of unique customers 4234 wrt to previously identified no. of unique customers 4372\n# ~138 customers removed.36 customers posed -ive purchases & 102 customers are outliers \nderived_trans_df.shape","62af1a5f":"# Standardisation of Dataset, to address any scaling issues. \nx = StandardScaler().fit_transform(derived_trans_df[['tot_purchases', 'tot_no_of_trans', \\\n                                                     'avg_purchases_per_trans']].values)\nx.shape","f94be2ec":"cost=[]\nscore=[]\nfor k in range(2,10):\n    kmeans=KMeans(n_clusters=k, init='k-means++',random_state=42)\n    labels = kmeans.fit_predict(x)\n    cost.append(kmeans.inertia_)\n    score.append(silhouette_score(x, labels, metric='euclidean'))","1e9a34be":"# Sklearn Kmeans inertia api.. works on Sum of squared distances of samples to their closest cluster center.\n# As per inertia_score(SSE) K=4\nplt.figure(figsize=(25,10))\nplt.plot(range(2,10), cost, label='Sum of squared error')\nplt.xlabel(\"Number of cluster\")\nplt.ylabel(\"Inertia\")\nplt.legend()\nplt.show()","a6a175ea":"# Silhouette Score\nscore","82a19000":"# As per silhouette_score K=4. We could pick K=2, as per silhouette_score, but then it could be a local optima.\nplt.figure(figsize=(25,10))\nplt.plot(range(2,10), score, label='Silhouette Score')\nplt.xlabel(\"Number of cluster\")\nplt.ylabel(\"Silhouette\")\nplt.legend()\nplt.show()","0846a371":"kmeans=KMeans(n_clusters=4, init='k-means++',random_state=0)\nlabels = kmeans.fit_predict(x)","6b9f043f":"derived_trans_df['kmeans_labels'] = pd.DataFrame(labels)\nderived_trans_df.head()","3d353f53":"# Cluster Centroids\nkmeans.cluster_centers_","9f1709ce":"plt.figure(figsize=(25,10))\nplt.scatter(x[:,1],x[:,2], c=labels)\nplt.scatter(kmeans.cluster_centers_[:, 1], kmeans.cluster_centers_[:, 2], c = 'green', label = 'Centroids')\nplt.legend()\nplt.show()","d3bc7bb7":"fig = px.scatter_3d(x, x=x[:,0], y=x[:,1], z=x[:,2], color=labels, symbol=labels)\nfig.show()","6d1b032b":"fig = px.scatter(x, x=x[:,1], y=x[:,2], color=labels, symbol=labels)\nfig.show()","75b003c1":"plt.figure(figsize=(25, 5))  \nplt.title(\"Dendrograms\")  \ndend = shc.dendrogram(shc.linkage(x, method='ward'))\nplt.axhline(y=45, color='r', linestyle='--')","3942ec77":"# From above denogram, we choose k=4 as no. of cluster.\ncluster = AgglomerativeClustering(n_clusters=4, affinity='euclidean', linkage='ward')  \nlabels = cluster.fit_predict(x)","59a24b91":"derived_trans_df['hierarchical_labels'] = pd.DataFrame(labels)\nderived_trans_df.head()","3f1c9acf":"plt.figure(figsize=(10, 7))  \nplt.scatter(x[:,1],x[:,2], c=labels)","a7fd2256":"fig = px.scatter(x=x[:,1], y=x[:,2], color=labels, symbol=labels)\nfig.show()","b74f5b0c":"gmm = GaussianMixture(n_components=4, verbose=1, random_state=42)\nlabels = gmm.fit_predict(x)","f6c80923":"derived_trans_df['gmm_labels'] = pd.DataFrame(labels)\nderived_trans_df.head()","c8839451":"plt.figure(figsize=(10, 7))  \nplt.scatter(x[:,1],x[:,2], c=labels)\nplt.xlabel(\"standardised tot_no_of_trans\")\nplt.ylabel(\"standardised avg_purchases_per_trans\")\n#plt.legend()\nplt.show()","c4b6b522":"fig = px.scatter(x=x[:,1], y=x[:,2], color=labels, symbol=labels)\nfig.show()","e2704af6":"fig = px.scatter_3d(x, x=x[:,0], y=x[:,1], z=x[:,2], color=labels, symbol=labels)\nfig.show()","e07cf2c9":"derived_trans_df.head()","6f307184":"def cluster_stats(data):\n    print(data[['tot_purchases','tot_no_of_trans','avg_purchases_per_trans']].describe())","f1adc642":"# kmeans\nfor k in range(0, derived_trans_df.kmeans_labels.nunique()):\n    print(\"*\"*50)\n    print(\"K:\",k)\n    cluster_stats(derived_trans_df[derived_trans_df.kmeans_labels==k])\n    print(\"\")","6072350d":"# hierarchical\nfor k in range(0, derived_trans_df.hierarchical_labels.nunique()):\n    print(\"*\"*50)\n    print(\"K:\",k)\n    cluster_stats(derived_trans_df[derived_trans_df.hierarchical_labels==k])\n    print(\"\")","dd00dc5d":"# gmm\nfor k in range(0, derived_trans_df.gmm_labels.nunique()):\n    print(\"*\"*50)\n    print(\"K:\",k)\n    cluster_stats(derived_trans_df[derived_trans_df.gmm_labels==k])\n    print(\"\")","4ba705bc":"def visualize_cluster(data, labels):\n    f, axes = plt.subplots(nrows=1, ncols=3, figsize=(20,5))\n    sns.boxplot(x=data[labels], y=data.tot_purchases, hue=data[labels], data=data, orient='v', ax=axes[0])\n    sns.boxplot(x=data[labels], y=data.tot_no_of_trans, hue=data[labels], data=data, orient='v', ax=axes[1])\n    sns.boxplot(x=data[labels], y=data.avg_purchases_per_trans, hue=data[labels], data=data, orient='v', ax=axes[2])","8eaec017":"visualize_cluster(derived_trans_df[['customer_id', 'tot_purchases', 'tot_no_of_trans',\\\n                                   'avg_purchases_per_trans', 'kmeans_labels']], 'kmeans_labels')","a86d6b6b":"visualize_cluster(derived_trans_df[['customer_id', 'tot_purchases', 'tot_no_of_trans',\\\n                                   'avg_purchases_per_trans', 'hierarchical_labels']], 'hierarchical_labels')","0014a662":"visualize_cluster(derived_trans_df[['customer_id', 'tot_purchases', 'tot_no_of_trans',\\\n                                   'avg_purchases_per_trans', 'gmm_labels']], 'gmm_labels')","b51712d1":"### Semantic Understanding of Clusters","88809d21":"### Hierarchical Clustering","c1acbdf4":"Kaggle : https:\/\/www.kaggle.com\/vik2012kvs\/high-value-customers-identification\n\nProblem Statement Type: Customer Segmentation: Clustering \n\nDESCRIPTION\nBackground of Problem Statement:\nA UK-based online retail store has captured the sales data for different products for the period of one year \n(Nov 2016 to Dec 2017). The organization sells gifts primarily on the online platform. \nThe customers who make a purchase consume directly for themselves. There are small businesses that buy \nin bulk and sell to other customers through the retail outlet channel.\n\nProject Objective:\nFind significant customers for the business who make high purchases of their favourite products. \nThe organization wants to roll out a loyalty program to the high-value customers after identification of segments. \nUse the clustering methodology to segment customers into groups:\n\nDataset Description:\nThis is a transnational dataset that contains all the transactions occurring between Nov-2016 to Dec-2017\nfor a UK-based online retail store.\n\nAttribute \t\t\t\t\t\tDescription\nInvoiceNo \t\t\t\t\t\tInvoice number (A 6-digit integral number uniquely assigned to each transaction)\nStockCode \t\t\t\t\t\tProduct (item) code\nDescription \t\t\t\t\tProduct (item) name\nQuantity \t\t\t\t\t\tThe quantities of each product (item) per transaction\nInvoiceDate \t\t\t\t\tThe day when each transaction was generated\nUnitPrice \t\t\t\t\t\tUnit price (Product price per unit)\nCustomerID \t\t\t\t\t\tCustomer number (Unique ID assigned to each customer)\nCountry \t\t\t\t\t\tCountry name (The name of the country where each customer resides)\n\n\nDefinition of High Value Customers [HVC]\nhttps:\/\/nectarom.com\/2017\/02\/24\/high-value-customers-high-value-results\/#:~:text=High%2Dvalue%20customers%20are%20those,in%20times%20of%20financial%20duress.\n\n\nBased on Business Objective: \nMy takeaway to go either for Hierarchical or EM Clustering technique, as it's more robust for this dataset.\nBelow analysis are my own subjective overview, I might be wrong in first place, would love to hear feedback & learn from you.","83021f93":"### EM Clustering","bc4f3c66":"#### My inferences about the dataset:","f60aac89":"### Kmeans Clustering","ed95c055":"Each record maps to an invoice no. and each invoice no. maps to multiple items, as part of market basket, i.e., in one single transactions, customer brought multiple items.\n\nThere is 1:1 mapping of invoice no. to customer-id, but there are instances where the same customer brought items from multiple locations(Country).Some invoice no. transactions has no customer-id mapping also.\n\nWe might want to drop \"country\" feature from clustering because there are 1:Many mapping between customer_id -country, which is trival from business objective, as we are only interested to understand HVC(high value customer) rather than country of origin of purchases.But based on business, this can be leveraged.\n\nThere are some transactions where\n    unitprice=0.\n    adjustment of bad debt and not mapped to actual purchases of items made by the customer.\n    return transaction are also not properly mapped to purchase\/sell transactions invoice no.\n    \nBut above scenarios are all quite expected in the real world.\n\n\nBelow are some sample transaction taken directly from the input file.\n\nInvoiceNo\tStockCode\tDescription\tQuantity\tInvoiceDate\tUnitPrice\tCustomerID\t\tCountry\n536545\t\t21134\t\t\t\t\t\t1\t\t29-Nov-16\t\t0\t\tUnited Kingdom\t\t0\n536546\t\t22145\t\t\t\t\t\t1\t\t29-Nov-16\t\t0\t\tUnited Kingdom\t\t0\n536547\t\t37509\t\t\t\t\t\t1\t\t29-Nov-16\t\t0\t\tUnited Kingdom\t\t0\n581422\t\t23169\t\tsmashed\t\t  -235\t\t06-Dec-17\t\t0\t\tUnited Kingdom\t\t0\n\nInvoiceNo\tStockCode\tDescription\t\tQuantity\tInvoiceDate\tUnitPrice\tCustomerID\tCountry\t\t\tsales\nA563185\t        B\t    Adjust bad debt\t    1\t    10-Aug-17\t 11062.06\t\t        United Kingdom\t11062.06\nA563186\t\t\tB\t\tAdjust bad debt\t\t1\t\t10-Aug-17\t-11062.06\t\t\t\tUnited Kingdom\t-11062.06\nA563187\t\t\tB\t\tAdjust bad debt\t\t1\t\t10-Aug-17\t-11062.06\t\t\t\tUnited Kingdom\t-11062.06\n\nInvoiceNo\tStockCode\tDescription\t\t\t\t\tQuantity\tInvoiceDate\tUnitPrice\tCustomerID\tCountry\t\t\t\n541431\t\t23166\tMEDIUM CERAMIC TOP STORAGE JAR\t74215\t\t16-Jan-17\t\t1.04\t12346\t\tUnited Kingdom\nC541433\t\t23166\tMEDIUM CERAMIC TOP STORAGE JAR\t-74215\t\t16-Jan-17\t\t1.04\t12346\t\tUnited Kingdom\n581483\t\t23843\tPAPER CRAFT , LITTLE BIRDIE\t\t80995\t\t07-Dec-17\t\t2.08\t16446\t\tUnited Kingdom\nC581484\t\t23843\tPAPER CRAFT , LITTLE BIRDIE\t\t-80995\t\t07-Dec-17\t\t2.08\t16446\t\tUnited Kingdom\n\n\nThere are 9291 return & debt adjustment based invoice transactions. \nAnd these invoices either starts with 'C' or 'A' as prefix before original sales invoice transactions.\n'A': Debt Adjustment\n'C': Return Transactions\n\nThis is my own understanding that in order to analyse HVC(High Value Customers), would like to have to below derived metrics.\n\ncustomer_id                : Unique Customer ID.\ntot_purchases              : Total purchases ever made by the customer.\ntot_no_of_trans            : Total no. of transactions made by the customer.\navg_purchases_per_trans    : Average purchases made by the customer per transaction [purchasing power]","256dba13":"## UK-High value Customers Identification "}}