{"cell_type":{"2d044dcf":"code","3598b8d9":"code","18a688ed":"code","5ff11598":"code","a0a3852e":"code","5a755d5e":"code","aba44501":"code","d5136dd2":"code","7434ebbb":"code","9610b083":"code","8cf6a65f":"code","2500dd8d":"code","cce25b09":"code","3a3fa524":"code","4f9aa763":"code","05f876b5":"code","768b76c1":"code","f0b030a7":"code","5652485c":"code","f760ca16":"code","18889cd4":"code","46f2180e":"markdown","de967827":"markdown","9ab073f8":"markdown","94d2adf2":"markdown","8a5cff60":"markdown","ec2991af":"markdown","40bbbf21":"markdown","770ef8a0":"markdown","7169e9f6":"markdown","f15a86e1":"markdown","29376caa":"markdown","5e680739":"markdown","1a403a37":"markdown","53455493":"markdown","1397f910":"markdown","a2e17cd8":"markdown","db8f2ec1":"markdown","586ab6e8":"markdown","2bc35cda":"markdown"},"source":{"2d044dcf":"#importing necessary libraries\nimport os\nimport numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt \nimport scipy\nfrom scipy.stats import entropy\nfrom sklearn.decomposition import PCA\nfrom scipy.optimize import curve_fit\n","3598b8d9":"#Data paths\ndataset_path_1st = '..\/input\/bearing-dataset\/1st_test\/1st_test'\ndataset_path_2nd = '..\/input\/bearing-dataset\/2nd_test\/2nd_test'\ndataset_path_3rd = '..\/input\/bearing-dataset\/3rd_test\/4th_test\/txt'","18a688ed":"# Test for the first file\ndataset = pd.read_csv('..\/input\/bearing-dataset\/1st_test\/1st_test\/2003.10.22.12.06.24', sep='\\t')\nax = dataset.plot(figsize = (24,6), title= \"Bearing Vibration\" , legend = True)\nax.set(xlabel=\"cycle(n)\", ylabel=\"vibration\/acceleration(g)\")\nplt.show()","5ff11598":"# Root Mean Squared Sum\ndef calculate_rms(df):\n    result = []\n    for col in df:\n        r = np.sqrt((df[col]**2).sum() \/ len(df[col]))\n        result.append(r)\n    return result\n\n# extract peak-to-peak features\ndef calculate_p2p(df):\n    return np.array(df.max().abs() + df.min().abs())\n\n# extract shannon entropy (cut signals to 500 bins)\ndef calculate_entropy(df):\n    ent = []\n    for col in df:\n        ent.append(entropy(pd.cut(df[col], 500).value_counts()))\n    return np.array(ent)\n# extract clearence factor\ndef calculate_clearence(df):\n    result = []\n    for col in df:\n        r = ((np.sqrt(df[col].abs())).sum() \/ len(df[col]))**2\n        result.append(r)\n    return result\n\ndef time_features(dataset_path, id_set=None):\n    time_features = ['mean','std','skew','kurtosis','entropy','rms','max','p2p', 'crest', 'clearence', 'shape', 'impulse']\n    cols1 = ['B1_x','B1_y','B2_x','B2_y','B3_x','B3_y','B4_x','B4_y']\n    cols2 = ['B1','B2','B3','B4']\n    \n    # initialize\n    if id_set == 1:\n        columns = [c+'_'+tf for c in cols1 for tf in time_features]\n        data = pd.DataFrame(columns=columns)\n    else:\n        columns = [c+'_'+tf for c in cols2 for tf in time_features]\n        data = pd.DataFrame(columns=columns)\n\n        \n        \n    for filename in os.listdir(dataset_path):\n        # read dataset\n        raw_data = pd.read_csv(os.path.join(dataset_path, filename), sep='\\t')\n        \n        # time features\n        mean_abs = np.array(raw_data.abs().mean())\n        std = np.array(raw_data.std())\n        skew = np.array(raw_data.skew())\n        kurtosis = np.array(raw_data.kurtosis())\n        entropy = calculate_entropy(raw_data)\n        rms = np.array(calculate_rms(raw_data))\n        max_abs = np.array(raw_data.abs().max())\n        p2p = calculate_p2p(raw_data)\n        crest = max_abs\/rms\n        clearence = np.array(calculate_clearence(raw_data))\n        shape = rms \/ mean_abs\n        impulse = max_abs \/ mean_abs\n        \n        if id_set == 1:\n            mean_abs = pd.DataFrame(mean_abs.reshape(1,8), columns=[c+'_mean' for c in cols1])\n            std = pd.DataFrame(std.reshape(1,8), columns=[c+'_std' for c in cols1])\n            skew = pd.DataFrame(skew.reshape(1,8), columns=[c+'_skew' for c in cols1])\n            kurtosis = pd.DataFrame(kurtosis.reshape(1,8), columns=[c+'_kurtosis' for c in cols1])\n            entropy = pd.DataFrame(entropy.reshape(1,8), columns=[c+'_entropy' for c in cols1])\n            rms = pd.DataFrame(rms.reshape(1,8), columns=[c+'_rms' for c in cols1])\n            max_abs = pd.DataFrame(max_abs.reshape(1,8), columns=[c+'_max' for c in cols1])\n            p2p = pd.DataFrame(p2p.reshape(1,8), columns=[c+'_p2p' for c in cols1])\n            crest = pd.DataFrame(crest.reshape(1,8), columns=[c+'_crest' for c in cols1])\n            clearence = pd.DataFrame(clearence.reshape(1,8), columns=[c+'_clearence' for c in cols1])\n            shape = pd.DataFrame(shape.reshape(1,8), columns=[c+'_shape' for c in cols1])\n            impulse = pd.DataFrame(impulse.reshape(1,8), columns=[c+'_impulse' for c in cols1])\n            \n        else:\n            mean_abs = pd.DataFrame(mean_abs.reshape(1,4), columns=[c+'_mean' for c in cols2])\n            std = pd.DataFrame(std.reshape(1,4), columns=[c+'_std' for c in cols2])\n            skew = pd.DataFrame(skew.reshape(1,4), columns=[c+'_skew' for c in cols2])\n            kurtosis = pd.DataFrame(kurtosis.reshape(1,4), columns=[c+'_kurtosis' for c in cols2])\n            entropy = pd.DataFrame(entropy.reshape(1,4), columns=[c+'_entropy' for c in cols2])\n            rms = pd.DataFrame(rms.reshape(1,4), columns=[c+'_rms' for c in cols2])\n            max_abs = pd.DataFrame(max_abs.reshape(1,4), columns=[c+'_max' for c in cols2])\n            p2p = pd.DataFrame(p2p.reshape(1,4), columns=[c+'_p2p' for c in cols2])\n            crest = pd.DataFrame(crest.reshape(1,4), columns=[c+'_crest' for c in cols2])\n            clearence = pd.DataFrame(clearence.reshape(1,4), columns=[c+'_clearence' for c in cols2])\n            shape = pd.DataFrame(shape.reshape(1,4), columns=[c+'_shape' for c in cols2])\n            impulse = pd.DataFrame(impulse.reshape(1,4), columns=[c+'_impulse' for c in cols2])\n            \n        mean_abs.index = [filename]\n        std.index = [filename]\n        skew.index = [filename]\n        kurtosis.index = [filename]\n        entropy.index = [filename]\n        rms.index = [filename]\n        max_abs.index = [filename]\n        p2p.index = [filename]\n        crest.index = [filename]\n        clearence.index = [filename]\n        shape.index = [filename]\n        impulse.index = [filename] \n        \n        # concat\n        merge = pd.concat([mean_abs, std, skew, kurtosis, entropy, rms, max_abs, p2p,crest,clearence, shape, impulse], axis=1)\n        data = data.append(merge)\n        \n    if id_set == 1:\n        cols = [c+'_'+tf for c in cols1 for tf in time_features]\n        data = data[cols]\n    else:\n        cols = [c+'_'+tf for c in cols2 for tf in time_features]\n        data = data[cols]\n        \n    data.index = pd.to_datetime(data.index, format='%Y.%m.%d.%H.%M.%S')\n    data = data.sort_index()\n    return data                                  ","a0a3852e":"set1 = time_features(dataset_path_1st, id_set=1)\nset1.to_csv('set1_timefeatures.csv')","5a755d5e":"set1 = pd.read_csv(\".\/set1_timefeatures.csv\")\nset1 = set1.rename(columns={'Unnamed: 0':'time'})\n#set1 = set1.set_index('time')\nlast_cycle = int(len(set1))","aba44501":"features = set1.copy()\n#simple moving average SMA\nma = pd.DataFrame()\nma['B4_x_mean'] = features['B4_x_mean']\nma['SMA'] = ma['B4_x_mean'].rolling(window=5).mean()\nma['time'] = features['time']","d5136dd2":"#Cumulative Moving Average\nma['CMA'] = ma[\"B4_x_mean\"].expanding(min_periods=10).mean()","7434ebbb":"#Exponantial Moving Average\nma['EMA'] = ma['B4_x_mean'].ewm(span=40,adjust=False).mean()","9610b083":"ma.plot(x=\"time\", y= ['B4_x_mean','SMA','CMA','EMA'])","8cf6a65f":"def health_indicator(bearing_data,use_filter=False):    \n    data = bearing_data.copy()\n    if use_filter:\n        for ft in data.columns:\n            data[ft] = data[ft].ewm(span=40,adjust=False).mean()\n    pca = PCA()\n    X_pca = pca.fit_transform(data)\n    component_names = [f\"PC{i+1}\" for i in range(X_pca.shape[1])]\n    X_pca = pd.DataFrame(X_pca, columns=component_names)\n    print(\"Explained variance of Pricincipal Component 1 is:\"+str(pca.explained_variance_ratio_[0]))\n    health_indicator = np.array(X_pca['PC1'])\n    degredation = pd.DataFrame(health_indicator,columns=['PC1'])\n    degredation['cycle'] = degredation.index\n    degredation['PC1'] = degredation['PC1']-degredation['PC1'].min(axis=0)\n    \n    return degredation\n\ndef fit_exp(df,base=500,print_parameters=False):\n    x =np.array(df.cycle)\n    x = x[-base:].copy()\n    y = np.array(degredation.PC1)\n    y = y[-base:].copy()\n    def exp_fit(x,a,b):\n        y = a*np.exp(abs(b)*x)\n        return y\n    #initial parameters affect the result\n    fit = curve_fit(exp_fit,x,y,p0=[0.01,0.001],maxfev=10000)\n    if print_parameters:\n        print(fit)\n    return fit\n\ndef predict(X_df,p):\n    x =np.array(X_df.cycle)\n    a,b = p[0]\n    fit_eq = a*np.exp(abs(b)*x)\n    return fit_eq\nlog = [[],[]]","2500dd8d":"#variable for incrementing index\nprediction_cycle = 600\n#variable for keeping intial value\ninit_cycle = prediction_cycle","cce25b09":"#selected_features = ['mean','std','kurtosis','skew','entropy',\n#        'rms','max','p2p','crest','shape','impulse']\nselected_features = ['max','p2p','rms']\n\nbearing = 3\nB_x = [\"B{}_x_\".format(bearing)+i for i in selected_features]\nearly_cycles = set1[B_x][:init_cycle]\nearly_cycles_pca = health_indicator(early_cycles,use_filter=True)","3a3fa524":"#run this again, again to simulate life-cycle of a bearing\ndata = set1[B_x][:prediction_cycle]\nind=data.index\ndegredation = health_indicator(data,use_filter=True)\n#degredation.plot(y='PC1',x='cycle')\nfit = fit_exp(degredation,base=250)\n\nprediction = predict(degredation,fit)\nm,n = fit[0]\nthres = 2\n#print(prediction_cycle)\nfail_cycle = (np.log(thres\/m))\/abs(n)\nlog[0].append(prediction_cycle)\nlog[1].append(fail_cycle)\n\nprint(m,n)\nprint('failed at'+str(fail_cycle))\n\nfig =plt.figure('Cycle: '+str(prediction_cycle))\nax =fig.subplots()\n\nax.plot([0,prediction_cycle],[2,2])\nax.scatter(degredation['cycle'],degredation['PC1'],color='b',s=5)\nax.plot(degredation['cycle'],prediction,color='r',alpha=0.7)\nax.set_title('Cycle: '+str(prediction_cycle))\nax.legend(['threshold','prediction'])\nfig.savefig('output.png')\nplt.show()\nincrement_cycle =25\nprediction_cycle += increment_cycle","4f9aa763":"#True labels represent Warnings!\nd = {'time':set1['time'][init_cycle::increment_cycle],'cycle': log[0], 'prediction': (np.array(log[1]))}\ndf = pd.DataFrame(data=d)\ndf['isvalid'] = df['prediction']<2156\n#adding real labels for analysing results\ndf['real'] = np.zeros(len(set1['time'][init_cycle::increment_cycle]))\n\nfor row in set1['time'][init_cycle::increment_cycle].index:\n    if bearing == 1:\n        if row<=151:\n            df['real'][row] = 'early'\n        if 151 < row <=600:\n            df['real'][row] = \"suspect\"\n        if 600 < row <=1499:\n            df['real'][row] = \"normal\"\n        if 1499 < row <=2098:\n            df['real'][row] = \"suspect\"\n        if 2098 < row <= 2156:\n            df['real'][row] =(\"imminent_failure\")\n    if bearing == 2:\n        if row<=500:\n            df['real'][row] =(\"early\")\n        if 500 < row <=2000:\n            df['real'][row] =(\"normal\")\n        if 2000 < row <=2120:\n            df['real'][row] =(\"suspect\")\n        if 2120< row <=2156:\n            df['real'][row] =(\"imminet_failure\")\n\n    if bearing == 3:\n        if row<=500:\n            df['real'][row] =(\"early\")\n        if 500 < row <= 1790:\n            df['real'][row] =(\"normal\")\n        if 1790 < row <=2120:\n            df['real'][row] =(\"suspect\")\n        if 2120 < row <=2156:\n            df['real'][row] =(\"Inner_race_failure\")\n    if bearing == 4:\n        if row<=200:\n            df['real'][row] =(\"early\")\n        if 200 < row <=1000:\n            df['real'][row] =(\"normal\")\n        if 1000 < row <= 1435:\n            df['real'][row] =(\"suspect\")\n        if 1435 < row <=1840:\n            df['real'][row] =(\"Inner_race_failure\")\n        if 1840 < row <=2156:\n            df['real'][row] =(\"Stage_two_failure\")\n\nprint(bearing)\nprint(df.tail(60))\n","05f876b5":"set2 = time_features(dataset_path_2nd, id_set=2)\nset2.to_csv('set2_timefeatures.csv')","768b76c1":"set2 = pd.read_csv(\".\/set2_timefeatures.csv\")\nset2 = set2.rename(columns={'Unnamed: 0':'time'})\nset2.head()","f0b030a7":"log = [[],[]]\n#variable for incrementing index\nprediction_cycle = 550\n#variable for keeping intial value\ninit_cycle = prediction_cycle","5652485c":"selected_features = ['max','p2p']\nbearing = 1\nB_x = [\"B{}_\".format(bearing)+i for i in selected_features]\nearly_cycles = set2[B_x][:init_cycle]\nearly_cycles_pca = health_indicator(early_cycles,use_filter=True)","f760ca16":"#run this again, again to simulate life-cycle of a bearing\ndata = set2[B_x][:prediction_cycle]\ndegredation = health_indicator(data,use_filter=True)\nfit = fit_exp(degredation,base=250)\n\nprediction = predict(degredation,fit)\nm,n = fit[0]\nthres = 2\nfail_cycle = (np.log(thres\/m))\/abs(n)\nlog[0].append(prediction_cycle)\nlog[1].append(fail_cycle)\n\n#print(m,n)\nprint('failed at'+str(fail_cycle))\n\nfig =plt.figure()\nax =fig.subplots()\nax.plot([0,prediction_cycle],[2,2])\nax.set_title('Cycle: '+str(prediction_cycle))\nax.scatter(degredation['cycle'],degredation['PC1'],color='b',s=5)\nax.plot(degredation['cycle'],prediction,color='r',alpha=0.7)\nax.legend(['threshold','prediction'])\nplt.show()\nincrement_cycle = 25\nprediction_cycle += increment_cycle","18889cd4":"#True labels represent alerts which are given before real end cycle!\nd = {'time':set2['time'][init_cycle::increment_cycle],'cycle': log[0], 'prediction': (np.array(log[1]))}\ndf = pd.DataFrame(data=d)\ndf['is valid'] = df['prediction']<984\ndf.head(30)","46f2180e":"# RUL Prediction","de967827":"# Reference\n1. https:\/\/www.mathworks.com\/help\/predmaint\/ug\/wind-turbine-high-speed-bearing-prognosis.html\n2. https:\/\/www.mathworks.com\/company\/newsletters\/articles\/three-ways-to-estimate-remaining-useful-life-for-predictive-maintenance.html\n3. http:\/\/emilygraceripka.com\/blog\/14\n4. https:\/\/acadpubl.eu\/hub\/2018-119-15\/4\/745.pdf\n5. https:\/\/www.kaggle.com\/ryanholbrook\/principal-component-analysis\n6. https:\/\/www.datasklr.com\/principal-component-analysis-and-factor-analysis\/principal-component-analysis","9ab073f8":"**Now**, Let's testing this approach on dataset2.","94d2adf2":"The first 550 cycle is assumed to be healthy","8a5cff60":"1.  Calling feature extraction function defined above to merge extracted features      \n2.  Saving as .csv file","ec2991af":"Moving average filter (also known as rolling average, running average) is a time series filter which calculates running weighted sum of time series. Simple moving average, cumulative moving average ,and exponantiel moving average methods are applied to feature data for smoothing. After comparing the results on a graph, it is concluded that EMA(40) is best option for filtering because it best reflecting the actual moves of the real data.","40bbbf21":"# Feature Extraction #","770ef8a0":"**NOTE:**                                                                                                      \nThis work is carried out during my Data Science Internship in SensHero Predictive Maintenance Solutions.\n\nI would be very glad if you can leave any comment for improvement.","7169e9f6":"# Additional Resources\n1. https:\/\/arxiv.org\/pdf\/1812.03315.pdf\n","f15a86e1":"* **PCA**\n\nIn order to build a degredation model, a feature that represents the information of base features is extracted using Principal Component Analysis. The principal component that have highest variance is selected and further investigations are conducted on that component. Dimensionalty of redunduncy of the data is reduced and a principle feature is obtained.\n\n* **Model Fitting**\n\nIn the time series prediction, it is assumed that degredations occurs exponantially in many natural phenomena, such as decay of a aradioactive element. Therefore, an exponantial model which have always positive slope is created as a function and it is fitted with scipy.optimize library. In order not to miss the specific movement that may help to forecast the future, exponential model is not fitted to whole data that is available. Instead, exponential function is fitted to last n data.","29376caa":"In this section several signal-based statistical features has been extracted from the data                                 \n**Definition and formula of the features:**\n* ***Absolute Mean*** $$\\overline{x} = \\frac{1}{N}\\sum_{i=1}^{N}|x_i| $$\n\n* ***Standart Deviation:*** $$\\sigma         = \\sqrt{\\frac{1}{N}\\sum_{i=1}^{N}(x_i - \\overline{x})^2}$$\n* ***Skewness:*** \nAsymmetry of a signal distribution. Faults can impact distribution symmetry and therefore increase the level of skewness.\n$$\\mathrm{Sk} = \\frac{1}{N}\\sum_{i=1}^{N}\\frac{(x_i-\\overline{x})^3}{\\sigma^3}$$\n* ***Kurtosis:***\nLength of the tails of a signal distribution, or equivalently, how outlier prone the signal is. Developing faults can increase the number of outliers, and therefore increase the value of the kurtosis metric.\n$$\\mathrm{K} = \\frac{1}{N}\\sum_{i=1}^{N}\\frac{(x_i-\\overline{x})^4}{\\sigma^4}$$\n* ***Entropy:***$$ H(X) = -\\sum_{i=1}^{N} P(x_i)\\log{P(x_i)} $$\n                            \n* ***RMS:*** $$x_{rms} =\\sqrt{(\\frac{1}{N})\\sum_{i=1}^{N}(x)^{2}}$$\n\n* ***Peak to Peak:*** $$ x_p = \\max \\mathrm{value} - \\min \\mathrm{value}$$\n\n* ***Crest Factor:*** \nPeak value divided by the RMS. Faults often first manifest themselves in changes in the peakiness of a signal before they manifest in the energy represented by the signal root mean squared. The crest factor can provide an early warning for faults when they first develop. \n$$x_{crest} =\\frac{\\max \\mathrm{value}}{\\mathrm{x_{rms}}}$$\n\n* ***Clearence Factor:*** \nPeak value divided by the squared mean value of the square roots of the absolute amplitudes. For rotating machinery, this feature is maximum for healthy bearings and goes on decreasing for defective ball, defective outer race, and defective inner race respectively. The clearance factor has the highest separation ability for defective inner race faults.\n$$ x_{clear} = \\frac{x_p}{(\\frac{1}{N}\\sum_{i=1}^{N}\\sqrt{|x_i|})^2}  $$\n\n* ***Shape Factor:*** \nRMS divided by the mean of the absolute value. Shape factor is dependent on the signal shape while being independent of the signal dimensions.\n$$\\frac{x_{rms}}{\\overline{x}}$$\n\n* ***Impulse:*** \nCompare the height of a peak to the mean level of the signal.\n$$\\frac{\\max \\mathrm{value}}{\\overline{x}}  $$\n","5e680739":"# Conclusion\n In this notebook basic approach have been implemented for predicting remaining useful life of bearings. Firstly, condition indicating features are extracted and the dimensionality of these features are increased to 1 feature with Principal Component Analysis. Then, exponantial function is fitted to available data to predict cycle at which fault occurs. In the future, more useful filtering can be applied and different time series prediction models can be tried. Also, a performance evaluating metric can be defined and the hyperparameters can be tuned.","1a403a37":"# Merging Data","53455493":"Reading Data again","1397f910":"**Life Cycle of Bearing 3 in data set1 with prediction curve**\n\n![ezgif.com-gif-maker.gif](attachment:5e3da065-2ba0-4e6a-bea2-d459c0d29ad8.gif)","a2e17cd8":"**DataSet2**","db8f2ec1":"According to the dataset description, at the end of the test-to-failure experiment, inner race defect occurred in\nbearing 3 and roller element defect in bearing 4.","586ab6e8":"According to the dataset description, at the end of the test-to-failure experiment, outer race failure occurred in\nbearing 1 in dataset 2.'is valid' column represents whether the predicted cycle is less than the maximum cycle of the bearing. ","2bc35cda":"Predictive maintenance allows early detection of failures based on historical data. This type of maintenance reduces unscheduled maintenance costs and incresases the lifespan of a machine. Generally 3 types of approaches exist in literature:\n1. Using Life Time Data: Failures are anticipated before they occurs thanks to proportional hazard models and probability distributions of component failure times\n2. Run-to-Failure Data: If run-to-failure data of many machines exists, then health state of a particular machine can be estimated by comparing the others.\n3. Threshold Data: When historical or failure data does not exist, a degredation model can be created and failure time can be estimated with a threshold value.\n\nIn this work 'threshold data' approach is utilized because limited amount of run-to-failure data exist. Also bearings' features are suitable to create a degredation model. \n"}}