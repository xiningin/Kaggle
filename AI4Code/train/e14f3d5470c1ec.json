{"cell_type":{"53081c63":"code","91906f86":"code","715810ba":"code","7d48cb75":"code","9a80068d":"code","c5bbe14f":"code","e8429cf3":"code","07446967":"code","20b28fba":"code","d986a0ab":"code","b1235654":"code","6cdd40a6":"code","feda0407":"code","0f22a24c":"code","0c4da27a":"code","ed1a611b":"code","174a0260":"markdown","36a7bf06":"markdown","b05eb4fd":"markdown","01c4365f":"markdown","a291e938":"markdown","a9d61fd0":"markdown","0c5ffb45":"markdown"},"source":{"53081c63":"# Ignore  the warnings\nimport warnings\nwarnings.filterwarnings('always')\nwarnings.filterwarnings('ignore')\n\nimport pandas as pd\nimport numpy as np\nimport PIL.Image\n\ndaisy_path = \"\/kaggle\/input\/AIAxTainanDL\/flower\/flower_classification\/train\/daisy\/\"\ndandelion_path = \"\/kaggle\/input\/AIAxTainanDL\/flower\/flower_classification\/train\/dandelion\/\"\nrose_path = \"\/kaggle\/input\/AIAxTainanDL\/flower\/flower_classification\/train\/rose\/\"\nsunflower_path = \"\/kaggle\/input\/AIAxTainanDL\/flower\/flower_classification\/train\/sunflower\/\"\ntulip_path = \"\/kaggle\/input\/AIAxTainanDL\/flower\/flower_classification\/train\/tulip\/\"\ntest_path=\"\/kaggle\/input\/AIAxTainanDL\/flower\/flower_classification\/test\/\"\nsubmission = pd.read_csv('\/kaggle\/input\/AIAxTainanDL\/submission.csv')","91906f86":"from os import listdir\nimport cv2\n\n\n\nimg_data = []\nlabels = []\n\nsize = 64,64\ndef iter_images(images,directory,size,label):\n    try:\n        for i in range(len(images)):\n            img = cv2.imread(directory + images[i])\n            img = cv2.resize(img,size)\n            img_data.append(img)\n            labels.append(label)\n    except:\n        pass\n\niter_images(listdir(daisy_path),daisy_path,size,0)\niter_images(listdir(dandelion_path),dandelion_path,size,1)\niter_images(listdir(rose_path),rose_path,size,2)\niter_images(listdir(sunflower_path),sunflower_path,size,3)\niter_images(listdir(tulip_path),tulip_path,size,4)","715810ba":"len(img_data),len(labels)","7d48cb75":"test_data = []\n\nsize = 64,64\ndef test_images(images,directory,size):\n    try:\n        for i in range(len(images)):\n            img = cv2.imread(directory + submission['id'][i]+\".jpg\")\n            img = cv2.resize(img,size)\n            test_data.append(img)\n    except:\n        pass\n\n\ntest_images(listdir(test_path),test_path,size)","9a80068d":"len(test_data)","c5bbe14f":"import numpy as np\ndata = np.asarray(img_data).reshape(len(img_data), 64*64*3)\ntestData=np.asarray(test_data).reshape(len(test_data), 64*64*3)\n\n#div by 255\ndata = data \/ 255.0\ntestData=testData\/255.0\n\nlabels = np.asarray(labels)\n","e8429cf3":"data.shape,labels.shape","07446967":"from sklearn.model_selection import train_test_split\n\n# Split the data\nX_train, X_validation, Y_train, Y_validation = train_test_split(data, labels, test_size=0.02, shuffle= True)\nprint(\"Length of X_train:\", len(X_train), \"Length of Y_train:\", len(Y_train))\nprint(\"Length of X_validation:\",len(X_validation), \"Length of Y_validation:\", len(Y_validation))","20b28fba":"from keras.utils import np_utils\nY_train_one_hot = np_utils.to_categorical(Y_train, 5)\nY_validation_one_hot = np_utils.to_categorical(Y_validation, 5)","d986a0ab":"# TensorFlow\nimport tensorflow as tf\nfrom tqdm import tqdm\nfrom sklearn.utils import shuffle\n\ntf.reset_default_graph()\n\nwith tf.name_scope('placeholder'):\n    input_data = tf.placeholder(tf.float32, shape=[None, 64*64*3], name='X')\n    y_true = tf.placeholder(tf.float32, shape=[None, 5], name='y')\n    \nwith tf.variable_scope('network'):\n    h1 = tf.layers.dense(input_data, 256, activation=tf.nn.sigmoid, name='hidden1')  \n    h2 = tf.layers.dense(h1, 128, activation=tf.nn.relu, name='hidden2') \n    h3 = tf.layers.dense(h2, 64, activation=tf.nn.relu, name='hidden3')\n    out = tf.layers.dense(h3, 5, name='output')\n    \nwith tf.name_scope('loss'):\n    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=y_true, logits=out), name='loss')\n    \nwith tf.name_scope('accuracy'):\n    correct_prediction = tf.equal(tf.argmax(tf.nn.softmax(out), 1), tf.argmax(y_true, 1))\n    compute_acc = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n    \nwith tf.name_scope('opt'):\n    update = tf.train.GradientDescentOptimizer(learning_rate=0.4).minimize(loss) \n\ninit = tf.global_variables_initializer()","b1235654":"tf.global_variables() # \u67e5\u770b\u6240\u6709tf\u8b8a\u6578","6cdd40a6":"# \u8a08\u7b97\u7e3d\u5171\u6709\u5e7e\u7d44weight\nvar_sum = 0\nfor tensor in tf.global_variables(scope='network'):\n    if 'Adam' not in tensor.name:\n        var_sum += np.product(tensor.shape.as_list())\n    \nprint('the total number of weights is', var_sum) ","feda0407":"bs = 64\n\ntrain_loss_epoch, train_acc_epoch = [], []\ntest_loss_epoch, test_acc_epoch = [], []\n\nsess = tf.Session()\nsess.run(init)\n\nfor i in tqdm(range(101)):\n    \n#     training part\n    train_loss_batch, train_acc_batch = [], []\n    \n    total_batch = len(X_train) \/\/ bs\n    \n    for j in range(total_batch):\n        \n        X_batch = X_train[j*bs : (j+1)*bs]\n        y_batch = Y_train_one_hot[j*bs : (j+1)*bs]\n        batch_loss, batch_acc, _ = sess.run([loss, compute_acc, update], \n                                            feed_dict={input_data: X_batch, y_true: y_batch})\n        \n        train_loss_batch.append(batch_loss)\n        train_acc_batch.append(batch_acc)\n        \n    train_loss_epoch.append(np.mean(train_loss_batch))\n    train_acc_epoch.append(np.mean(train_acc_batch))\n    \n#     testing part\n    batch_loss, batch_acc = sess.run([loss, compute_acc], \n                                    feed_dict={input_data: X_validation, y_true: Y_validation_one_hot})\n\n    test_loss_epoch.append(batch_loss)\n    test_acc_epoch.append(batch_acc)\n    \n    X_train, Y_train_one_hot = shuffle(X_train, Y_train_one_hot)\n    \n    if i%10 == 0:\n        print('step: {:2d}, train loss: {:.3f}, train acc: {:.3f}, test loss: {:.3f}, test acc: {:.3f}'\n             .format(i, train_loss_epoch[i], train_acc_epoch[i], test_loss_epoch[i], test_acc_epoch[i]))","0f22a24c":"import matplotlib.pyplot as plt\nplt.plot(train_loss_epoch, 'b', label='train')\nplt.plot(test_loss_epoch, 'r', label='test')\nplt.legend()\nplt.title(\"Loss\")\nplt.show()\n\nplt.plot(train_acc_epoch, 'b', label='train')\nplt.plot(test_acc_epoch, 'r', label='test')\nplt.legend()\nplt.title(\"Accuracy\")\nplt.show()","0c4da27a":"pred =  np.argmax(sess.run(out,feed_dict={input_data: testData}), axis=1)\nnewsSbmission=submission\nnewsSbmission[\"class\"]=pred\nnewsSbmission.to_csv(\"submission.csv\", index=False)","ed1a611b":"pred","174a0260":"## 1 ) Load data\n### 1.1) Making the functions to get the training and testing set","36a7bf06":"## 4) Output Testing Data","b05eb4fd":"### 2.3 ) Splitting into Training and Validation Sets","01c4365f":"### 3 ) Modelling\n### 3.1 ) build network","a291e938":"## 2 ) Data preprocessing\n### 2.1) scaled down to values between 0 to 1","a9d61fd0":"### 3.2 ) tranning model","0c5ffb45":"### 2.4 ) One Hot Encoding"}}