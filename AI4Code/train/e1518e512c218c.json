{"cell_type":{"613f8691":"code","dc52e0cc":"code","4089fbc5":"code","7670f9c3":"code","1b46df8f":"code","76d56f1f":"code","067d8938":"code","6166b39b":"code","6678a2b0":"code","989af9db":"code","5ac75db0":"code","488bec4e":"code","22c9e39e":"code","16559edb":"code","70ee746b":"code","f06afb9a":"code","c0baf336":"code","648e0a71":"code","406ac89e":"code","03d93bfc":"code","e7dd0282":"code","1672f996":"code","55911553":"code","f6c1491d":"code","786c8036":"code","76bea115":"code","a482c984":"code","2816c3f7":"code","741d6400":"code","90620cf3":"code","7451d708":"code","f173a316":"code","041a0de0":"code","eb316aba":"code","8472749e":"code","e29ba5fd":"code","d2a5f6b5":"code","ae3ecc23":"code","47b4594f":"code","6d3fe37f":"code","821004cb":"code","d4d1e554":"code","04adc092":"code","40895856":"code","10840110":"code","c0baad31":"code","5a9f6b77":"code","90ac9d74":"code","b1ba2aee":"code","7613b173":"code","7666ed84":"code","3e4db554":"code","7be6822e":"code","7c9c6b1a":"code","681b7c18":"code","a095e222":"code","219e352c":"code","070af93e":"code","178f934a":"code","a29091ae":"code","1cb82939":"code","5ea8ba3d":"code","92c536e9":"code","a59517e1":"code","09305983":"code","0f6e7f00":"code","5982d09b":"code","acee3dd3":"code","e4d630a8":"code","ce23d170":"code","e334df01":"code","c1d3c888":"code","1cb223ec":"code","66a22b5c":"code","326f398a":"code","da66dcd2":"code","603cdcaa":"code","fc256a05":"code","ba122cd1":"code","64c4216b":"code","47f88b04":"code","8c52e0ac":"code","c4cb952a":"code","809a959b":"code","f394a5b7":"code","19fe89d4":"code","c5bfaf5a":"markdown","1dd7f05b":"markdown","74eae354":"markdown","2a823191":"markdown","744b5ffc":"markdown","ca92b4f0":"markdown","359ead57":"markdown","d87aa210":"markdown","2f673bc2":"markdown","91b1eac8":"markdown","5a5912b3":"markdown","dee3cca5":"markdown","8bfbe4ec":"markdown"},"source":{"613f8691":"# Importing the Required Libraries\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom keras.models import Model, load_model\nfrom keras.preprocessing import image\nimport time\nimport pickle\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.utils import to_categorical\nfrom keras.layers import *\nimport pandas as pd\nfrom nltk.translate.bleu_score import sentence_bleu","dc52e0cc":"dataset = pd.read_csv('..\/input\/flickr8k\/captions.txt',delimiter=',',header=None)\ndataset=dataset[1:]","4089fbc5":"print(dataset.shape)","7670f9c3":"dataset.head()","1b46df8f":"print(dataset[0][3])\nprint(dataset[1][3])","76d56f1f":"# Libraries to be used for data cleaning\n\nimport re\nimport nltk\n\n\ncorpus = []\nfor i in range(1, dataset.shape[0]):\n    cap = re.sub('[^a-zA-Z]', ' ', dataset[1][i])  #Removed all other characters except alphabets\n    cap = cap.lower()                              #Converted to lower case\n    cap = cap.split()                               \n    cap=[word for word in cap if len(word)>1]      #Removed single letter words\n    cap = ' '.join(cap)                            # Joined with spaces\n    cap= '<startseq> '+cap+' <endseq>'\n    corpus.append(cap)                             #list of captions","067d8938":"for i in range(50): print(corpus[i:i+1])\nprint(len(corpus))","6166b39b":"i=1\nwhile i < dataset.shape[0]:\n  dataset[0][i]=dataset[0][i].split('.')[0]        #Removed characters after '.'\n  i=i+1","6678a2b0":"print(dataset.head)","989af9db":"type(dataset)","5ac75db0":"# Joins the Images to its captions using a dictionary \n\nfrom collections import defaultdict\nnew_dict= defaultdict(lambda : [])          #Created a dictionary with image ids as key and captions as value\ni=1\nwhile i <len(corpus):\n  # print(dataset[0][i])\n  new_dict[dataset[0][i]].append(corpus[i-1])\n  i=i+1                                                     ","488bec4e":"print(len(new_dict))","22c9e39e":"all_vocab = []              #A list of all the words in the captions\n\nfor key in new_dict.keys():\n    [ all_vocab.append(i) for des in new_dict[key] for i in des.split()]","16559edb":"print(\"total words appearing : \" , len(all_vocab))","70ee746b":"from collections import Counter\n\ncounter = Counter(all_vocab)","f06afb9a":"dic_ = dict(counter)","c0baf336":"sorted_dic = sorted(dic_.items(), key = lambda x: x[1], reverse=True)   #List with words and their corresponding frequency","648e0a71":"print(len(sorted_dic))","406ac89e":"\nfor i in range(len(sorted_dic)):print(sorted_dic[i]) ","03d93bfc":"# we decide upon a threshold value which helps in selecting the words that occur more than others in the corpus\n# Here we choose a threshold of 10, so that words that occur more than 10 times in the entire corpus are chosen\n\nthreshold_value = 10\n\nd = [(x) for x in sorted_dic if x[1]>threshold_value]","e7dd0282":"len(d)","1672f996":"all_vocab = [x[0] for x in d]                       #Updating all_vocab","55911553":"len(all_vocab)","f6c1491d":"f = open('new_dict.txt', 'w')\nf.write(str(new_dict))\nf.close()","786c8036":"image_ids=[]\nj=1\nfor i in range(0,len(dataset)\/\/5 ):\n    image_ids.append(dataset[0][j])\n    j=j+5\n    ","76bea115":"print(image_ids)","a482c984":"new_dict[image_ids[0]]","2816c3f7":"split_train_to_test_ratio=0.8\ntrain_end=int(split_train_to_test_ratio*len(image_ids))\n\ntrain=image_ids[0:train_end]\ntest=image_ids[train_end:]","741d6400":"train","90620cf3":"train_descriptions = {}               #dictionary with keys as image ids and values as corresponding captions\nfor t in train:\n    train_descriptions[t] = []\n    for cap in new_dict[t]:\n        train_descriptions[t].append(cap)","7451d708":"test_descriptions = {}             #dictionary with keys as image ids and values as corresponding captions\nfor t in test:\n    test_descriptions[t] = []\n    for cap in new_dict[t]:\n        test_descriptions[t].append(cap)","f173a316":"train_descriptions","041a0de0":"test_descriptions","eb316aba":"i=0\nfor keys,values in train_descriptions.items():\n i=i+1\n \n print(keys)\n print(values)\n if i>50:break","8472749e":"from keras.applications.resnet50 import ResNet50, preprocess_input","e29ba5fd":"model = ResNet50(weights = 'imagenet', input_shape = (224,224,3))","d2a5f6b5":"model.summary()","ae3ecc23":"model_new = Model(inputs = model.input, outputs =  model.layers[-2].output)","47b4594f":"# Function to preprocess images\n\ndef preprocess_image(img):\n    img = image.load_img(img, target_size=(224,224))\n    img = image.img_to_array(img)\n    img = preprocess_input(img)\n    img = np.expand_dims(img, axis = 0)\n\n    return img\n","6d3fe37f":"# Function for obtaining feature vectors (encodings) from images\n\ndef encode_image(img):\n    img = preprocess_image(img)\n    fea_vec = model_new.predict(img)\n    fea_vec = fea_vec.reshape(fea_vec.shape[1], )\n    return fea_vec","821004cb":"images='..\/input\/flickr8k\/Images\/'","d4d1e554":" # Obtaining Feature vector from training images\n \nstart = time.time()\n\nencoding_train = {}                #Dictionary with keys as image ids and values as encoding corresponding to the images                        \n\nfor ix, img in enumerate(train):\n    \n    img = images+train[ix]+\".jpg\"\n    \n    p = encode_image(img)\n    \n    encoding_train[ img[len(images):] ] = p\n    \n    \n    if ix%100 == 0:\n        print(\"Encoding image :\" + str(ix))\n    \nprint(\"Time taken in sec - \" + str(time.time() - start))","04adc092":"len(encoding_train)","40895856":"encoding_train[train[0]+\".jpg\"]","10840110":"print(\"Time taken in sec - \" + str(time.time() - start))","c0baad31":" # Obtaining Feature vector from test images\n\nstart = time.time()\n\nencoding_test = {}\n\nfor ix, img in enumerate(test):\n    \n    img = images+test[ix]+\".jpg\"\n    \n    p = encode_image(img)\n    \n    encoding_test[ img[len(images):] ] = p\n    \n    \n    if ix%100 == 0:\n        print(\"Encoding image :\" + str(ix))\n    \nprint(\"Time taken in sec - \" + str(time.time() - start))","5a9f6b77":"# saving features to disk\n\nwith open(\".\/encoded_train_images.pkl\", 'wb') as f:\n    pickle.dump(encoding_train, f )","90ac9d74":"with open(\".\/encoded_test_images.pkl\", 'wb') as f:\n    pickle.dump(encoding_test, f )","b1ba2aee":"with open(\".\/encoded_train_images.pkl\", 'rb') as f:\n    encoding_train = pickle.load(f)","7613b173":"with open(\".\/encoded_test_images.pkl\", 'rb') as f:\n    encoding_test = pickle.load(f)","7666ed84":"word_to_idx = {}\nidx_to_word = {}\n\nix = 1\n\nfor e in all_vocab:\n    #print(ix,e)\n    word_to_idx[e] = ix\n    idx_to_word[ix] = e\n    ix +=1","3e4db554":"i=0\nfor keys,values in word_to_idx.items():\n i=i+1\n \n print(keys,values)\n \n if i>50:break","7be6822e":"for i in range(51):print(all_vocab[i])","7c9c6b1a":"len(all_vocab)","681b7c18":"vocab_size = len(idx_to_word) + 1\nprint(vocab_size)\n","a095e222":"all_caption_len = []\n\nfor key in train_descriptions.keys():\n    for cap in train_descriptions[key]:\n        all_caption_len.append(len(cap.split()))","219e352c":"print(len(all_caption_len))\nprint(all_caption_len[:50])","070af93e":"max_len = max(all_caption_len)\nprint(max_len) ","178f934a":"def data_generator(train_descriptions, encoding_train, word_to_idx, max_len,  num_photos_per_batch ):\n    X1, X2, y = [], [], []\n    \n    n=0\n    cnt = 0\n    all_items = list(train_descriptions.keys())\n    \n    while True:\n      n+=1\n      # print(cnt)\n      key = all_items[cnt]\n      desc_list = train_descriptions[key]              \n      cnt+=1\n      cnt= (cnt%len(all_items))\n        \n      photo = encoding_train[key+\".jpg\"]          #feature vector\n            #print(photo.shape)\n      for desc in desc_list:                       #desc : iterates through the 5 captions\n        seq = [word_to_idx[word] for word in desc.split() if word in word_to_idx]                \n                \n        for i in range(1, len(seq)): \n            in_seq = seq[0:i]\n            out_seq = seq[i]\n            \n            in_seq = pad_sequences( [in_seq], maxlen=max_len, value= 0, padding='post')[0]\n        \n            out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n            \n            X1.append(photo)\n            X2.append(in_seq)\n            y.append(out_seq)\n\n      if n == num_photos_per_batch:\n          yield [np.array(X1), np.array(X2)] , np.array(y)\n          X1, X2, y = [], [], []\n          n = 0         ","a29091ae":"key=\"1000268201_693b08cb0e\"\nprint(key)\nprint(encoding_train[key+\".jpg\"])","1cb82939":"encoding_train","5ea8ba3d":"for i in data_generator(train_descriptions, encoding_train, word_to_idx, max_len, 3):\n    X, y = i\n    print(X[0].shape) \n    print(X[1].shape)\n    print(y.shape)\n    break","92c536e9":"embeddings = {}\n\nwith open(\"..\/input\/glove6b200d\/glove.6B.200d.txt\", 'r', encoding='utf-8') as f:\n    for line in f:\n        values = line.split()\n        word = values[0]\n        coeffs = np.array(values[1:], dtype=\"float32\")\n        \n        embeddings[word] = coeffs\n","a59517e1":"i=0\nfor keys,values in embeddings.items():\n i=i+1\n \n print(keys,values)\n \n if i>10:break","09305983":"print(len(embeddings))","0f6e7f00":"def getOutputEmbeddings():\n\n    emb_dim = 200\n    embedding_matrix_output = np.zeros((vocab_size, emb_dim ))\n    \n    for word, idx in word_to_idx.items():\n        \n        emb_vec = embeddings.get(word)\n        \n        if emb_vec is not None:\n            embedding_matrix_output[idx] = emb_vec\n            \n    return embedding_matrix_output","5982d09b":"embedding_output = getOutputEmbeddings()","acee3dd3":"embedding_output.shape","e4d630a8":"print(embedding_output[5:7])","ce23d170":"input_img_fea = Input(shape=(2048,))\ninp_img1 = Dropout(0.3)(input_img_fea)\ninp_img2 = Dense(256, activation='relu')(inp_img1)","e334df01":"print(inp_img2.shape)","c1d3c888":"input_cap = Input(shape=(max_len,))\ninp_cap1 = Embedding(input_dim= vocab_size, output_dim=200, mask_zero=True)(input_cap)\n#print(inp_cap1)\ninp_cap2 = Dropout(0.3)(inp_cap1)\ninp_cap3 = LSTM(256)(inp_cap2)","1cb223ec":"print(inp_cap3.shape)","66a22b5c":"decoder1 = add([inp_img2, inp_cap3])\nprint(decoder1.shape)\ndecoder2 = Dense(256, activation='relu')(decoder1)\noutput = Dense(vocab_size, activation='softmax')(decoder2)\n\n\nmodel = Model(inputs = [input_img_fea, input_cap]  , outputs =  output )","326f398a":"model.summary()","da66dcd2":"model.layers[2].set_weights([embedding_output])\nmodel.layers[2].trainable = False","603cdcaa":"model.compile(loss=\"categorical_crossentropy\", optimizer='adam') ","fc256a05":"epochs = 30\nnumber_photos_per_batch = 3\nsteps = len(train_descriptions)\/\/number_photos_per_batch\n\nmytraingen = data_generator(train_descriptions, encoding_train, word_to_idx, max_len, number_photos_per_batch)\n\nmodel.fit(mytraingen,steps_per_epoch=steps,epochs = epochs)\nmodel.save(filepath=\".\/best_model1.h5\")","ba122cd1":"model.save_weights(\".\/weights1.h5\") ","64c4216b":"def predict(photo_enc,model):\n    in_text = \"<startseq>\"\n    \n    for i in range(max_len):\n        sequence = [word_to_idx[word] for word in in_text.split() if word in word_to_idx]\n        #print(sequence)\n        sequence = pad_sequences([sequence], maxlen=max_len, padding='post')\n        \n        y_pred = model.predict([photo_enc, sequence])\n        y_pred = np.argmax(y_pred)\n        word = idx_to_word[y_pred]\n        \n        in_text += \" \"+word\n        \n        if word == '<endseq>':\n            break\n        \n        \n    final_caption = in_text.split()\n    final_caption = final_caption[1:-1]\n    final_caption = \" \".join(final_caption)\n    return final_caption","47f88b04":"model=load_model(\".\/best_model1.h5\")","8c52e0ac":"len(encoding_test)","c4cb952a":"scores=[]\nfor rn in range(5):\n    img_id = list(encoding_test.keys())[rn]\n    photo_enc = encoding_test[img_id].reshape((1,2048))\n    sh_img_id=img_id[:-4]\n    reference=new_dict[sh_img_id]\n    references=[]\n    print(img_id)\n\n    for ref in reference:\n        ref=ref[11:-9]\n        ref=ref.split(' ')\n        references.append(ref)\n    pred = predict(photo_enc,model)\n    translation=pred.split(' ')\n    score1 = sentence_bleu(references, translation)\n    scores.append(score1)\n    print(\"BLEU Score for image id : \"+img_id+\"is : \"+str(score1))\n    print('1 '+ pred)\n    path = images + img_id\n    img = plt.imread(path)\n    plt.imshow(img)\n    plt.show()\n\n   ","809a959b":"scores=[]\nfor rn in range(1619):\n    img_id = list(encoding_test.keys())[rn]\n    photo_enc = encoding_test[img_id].reshape((1,2048))\n    sh_img_id=img_id[:-4]\n    reference=new_dict[sh_img_id]\n    references=[]\n\n    for ref in reference:\n        ref=ref[11:-9]\n        ref=ref.split(' ')\n        references.append(ref)\n    pred = predict(photo_enc,model)\n    translation=pred.split(' ')\n    score1 = sentence_bleu(references, translation)\n    scores.append(score1)","f394a5b7":"scores","19fe89d4":"from statistics import mean\n\nprint(\"Max : \"+str(max(scores)))\nprint(\"Min : \"+str(min(scores)))\nprint(\"Average : \"+str(mean(scores)))\n","c5bfaf5a":"### Predictor Function\nThis function takes an image and predicts the corresponding caption for it by pasiing it into the model","1dd7f05b":"### Splitting image captions dataset into training and testing dataset and appending it all in 2 lists ","74eae354":"### Generator Function\nThe Data Generator maps the Image encodings with the training captions for the training of the model","2a823191":"### Data Preprocessing","744b5ffc":"###  Word Embedding\nOf GloVe embeddings","ca92b4f0":"### Model Architecture\nDevelopment and stacking of the model's layers","359ead57":"### Importing and Loading the model","d87aa210":"# Image Captioning Project\n\nThis project involved the use of a modified **pre-trained ResNet50 model** to map english words to the input image\u2019s features to produce a caption through text generators (of GloVe word embedding).\n\n**Concepts Used:** Image Processing, Word Embeddings, GAN modelling. \n\n*   Trained on 8kFlickr dataset. \n*   Coded using keras with tensorflow backend.\n*   ResNet50 model is used to extract features from the images.\n*   BLEU score as its evaluation metric.","2f673bc2":"Creating a word counter for the corpus","91b1eac8":"Step 0: Importing the libaries required for the project","5a5912b3":"#### Image Preprocessing\nWe take the train & test image datasets and preprocess them.","dee3cca5":"**Captions**","8bfbe4ec":"### Model Training"}}