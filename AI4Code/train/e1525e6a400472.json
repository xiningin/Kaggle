{"cell_type":{"85dfae1f":"code","d3cca5b3":"code","1b792001":"code","d552cc89":"code","7ad41fa3":"code","b7ea1bdc":"code","f29f1bda":"code","08496e03":"code","a2e826f7":"code","34e8c943":"code","4df9fb01":"code","ed2f7dc7":"code","c2f5ddcd":"code","1627706b":"code","992c1c56":"code","291f8959":"code","2901221f":"code","f1007bbf":"code","a41e069e":"code","cc0528c0":"code","681db95a":"code","92d46088":"code","3b47c621":"code","d5ca0915":"code","7ef70276":"code","d8d1ae28":"code","7341f4ca":"code","6706ffb7":"code","3d21c412":"code","e8f564a2":"code","19e6996f":"code","48f608f6":"code","e626531c":"code","0bfc890e":"code","6f2a4679":"code","f4576083":"code","6523b3a7":"code","2b3890a8":"code","a694a4cc":"code","68673fb0":"code","ef97971e":"code","57fb0523":"code","da8abf1a":"code","40ed121f":"code","46d2f7da":"code","6538e619":"code","13bef943":"code","67e12bd1":"code","5707e9a8":"code","0d173475":"code","e7411628":"code","3c3b86e0":"code","750bd1df":"code","d92f291c":"code","f541a66d":"code","463de609":"code","fa0cd614":"code","daa7d908":"code","2b571b4d":"code","6051021b":"code","9c0668cd":"code","aad53f99":"code","86a52ace":"code","ea2916c8":"code","fc96c7c4":"code","9de8f966":"code","dec428f9":"code","8fa4ac91":"code","d8bfefa6":"code","3edfcbff":"code","3b2438cd":"code","c01ab679":"code","badd3ac7":"code","5ce1af07":"code","efc2ec47":"code","e49e423d":"code","4cb16dd5":"code","b1c33212":"code","49645a37":"code","7a7b6a3b":"code","cc252958":"code","59f7ede0":"code","c05a3896":"code","00acb3c3":"code","d7df8827":"code","60901e99":"code","1c517983":"code","43f2b5a7":"code","b188c252":"code","c1c6afa0":"code","c021658e":"code","7d9eabcc":"code","93ce93cc":"code","905a7386":"code","fb6857f5":"code","5cacc8b0":"code","fce73add":"code","9498f5f5":"code","877ffe59":"code","47a108ba":"code","1f91dd6f":"code","b365dfa8":"code","ee2dbc40":"code","0e50510c":"code","2e0b97f8":"code","e9139a37":"code","3b8662af":"code","8a2986ff":"code","76c35c10":"code","a94b125d":"code","80ac9990":"code","16c71d34":"code","c042ee35":"code","8230517a":"code","f340fe07":"code","894e1cd0":"code","199be078":"code","e2ef7c3c":"code","38733df4":"code","f71c3ecb":"code","bc8385cd":"code","0be71b38":"code","d9b26ace":"code","54a6aacb":"code","b1ca01b7":"code","67d7fd09":"code","94780f52":"code","4833700a":"code","11638fba":"code","ac437d1e":"code","0fef7c2b":"code","690ce2c9":"code","1a234a50":"code","004ba739":"code","1b6a4aff":"code","b62a4d51":"code","44c56c8a":"code","81fd6c00":"code","32b04584":"code","7f32de23":"code","51b153f8":"code","86de65f5":"code","310977b9":"code","7d8700b0":"code","a774aab6":"code","ac324a3c":"code","bfb5e11d":"code","cacc01f3":"code","4a274432":"code","26aae998":"code","c83edb65":"code","25a00d61":"code","3aec1815":"code","eb6b773c":"code","90cd53d3":"code","05ae8d71":"code","19ce5ae6":"code","1e34ad7e":"code","3cb7f4d5":"code","9014bf9c":"code","f2170f43":"code","ae36ee1e":"code","bb9be9ee":"code","392534d9":"code","4a4422a7":"code","301e8433":"code","92af8d77":"code","b5c11f61":"code","9e0949b2":"code","bbeebd9e":"code","3fec2789":"code","1e178d9e":"code","6ec74698":"code","c3b7a768":"code","e4de6f46":"code","078589a4":"code","429f5516":"code","195b381f":"code","988a41de":"code","9725823e":"code","027b4bfc":"code","53f0fb42":"code","8dec81d5":"code","e976ec6e":"code","33f13853":"code","48e5c802":"code","9ae1c38e":"code","a31922d3":"code","994f5af0":"code","3298fa8b":"code","47f7af71":"code","46cc1ad5":"markdown","0456555c":"markdown","f4cbbe2e":"markdown","1dbe1b4e":"markdown","5ba1efc9":"markdown","722c771b":"markdown","d54a17bb":"markdown","15a75ba5":"markdown","9fa4c8e6":"markdown","f898fcd0":"markdown","be104834":"markdown","dceec909":"markdown","9a030b85":"markdown","87535d0b":"markdown","0dbc29b8":"markdown","d70d2b75":"markdown","e6d5b775":"markdown","4939a6b4":"markdown","2229c7c1":"markdown","062c7f4d":"markdown","ad3161a8":"markdown","2a800562":"markdown","45dca43c":"markdown","ea7bb64a":"markdown","c5779843":"markdown","f819bac5":"markdown","949f8904":"markdown","b9882fe5":"markdown","9aba728f":"markdown","389edc5f":"markdown","0972d7cd":"markdown","1d7badb4":"markdown","56bceae4":"markdown","c7856121":"markdown","58e0905f":"markdown","8299de21":"markdown","06d335b6":"markdown","b172b7d4":"markdown","82c1229c":"markdown","ce14d412":"markdown","d8519d3d":"markdown","b05ebe06":"markdown","8b802550":"markdown","38a41e09":"markdown","a88771e0":"markdown","61a338f8":"markdown","955c21ce":"markdown","2d1b3d67":"markdown","ee59456a":"markdown","3858ed84":"markdown","07608401":"markdown","b4d416c1":"markdown","399203cb":"markdown","6ce51ca8":"markdown","f10e9a92":"markdown","9d3b465c":"markdown","573867d2":"markdown","077568cd":"markdown","fa538d3c":"markdown","aea12bd1":"markdown","91375992":"markdown","baff7566":"markdown","f6b5b946":"markdown","bc6286e8":"markdown","ae91ca67":"markdown","72d49fbf":"markdown","340aee57":"markdown","5383e7ee":"markdown","7916a8c4":"markdown","99b17cdf":"markdown","6d645cf7":"markdown","a504b666":"markdown","3a73e630":"markdown","e4823297":"markdown","6df33842":"markdown","cef61e9a":"markdown","81eb128d":"markdown","ef0736a6":"markdown","77778ef8":"markdown","6474cb67":"markdown","5eb96dc4":"markdown","43c5247f":"markdown","dbca33bf":"markdown","ceb289b8":"markdown","d5180e1a":"markdown","3a9093a5":"markdown","561f0bb8":"markdown","2850bec7":"markdown","0a8d601f":"markdown","886d9891":"markdown","f9a2fb20":"markdown","bbd0f480":"markdown","5d17d39c":"markdown","f8758b36":"markdown","6f92a035":"markdown","7eb8048c":"markdown","fb41133c":"markdown","fd0f2a6c":"markdown","c7a03825":"markdown","b9586844":"markdown","3ae33cb1":"markdown","a9219d0c":"markdown","1f8f38e0":"markdown","e308ff2a":"markdown","a21d8607":"markdown","9109e0e1":"markdown","09655634":"markdown","aaf50339":"markdown","b813afae":"markdown","e5393d31":"markdown","0945859c":"markdown","889766a3":"markdown","0541a80e":"markdown","c6ff55a4":"markdown","d88dd3a3":"markdown","7c374d9d":"markdown","21bbf47c":"markdown"},"source":{"85dfae1f":"import pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport numpy as np\nimport statsmodels.api as sm\nimport scipy.stats as st\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import LabelEncoder\nimport seaborn as sns\nfrom sklearn.metrics import confusion_matrix\nimport matplotlib.mlab as mlab\nimport random as rnd\nfrom operator import add\nfrom sklearn.pipeline import Pipeline\nfrom imblearn.over_sampling import SMOTE \nimport plotly.express as ex\nfrom sklearn.metrics import accuracy_score,recall_score,precision_score,f1_score,confusion_matrix,roc_auc_score, r2_score\nfrom sklearn.model_selection import GridSearchCV\n\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nimport pandas_profiling\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom keras.layers import Dropout\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import BatchNormalization\n\n\n%matplotlib inline\nimport warnings\nwarnings.filterwarnings('ignore')","d3cca5b3":"# Importing the data and checking the first five row of the data\nstroke_raw = pd.read_csv('..\/input\/stroke-prediction-dataset\/healthcare-dataset-stroke-data.csv')\nstroke_raw.head()","1b792001":"# Showing the last 5 rows of the data\nstroke_raw.tail()","d552cc89":"# checking the shape of the raw dataset\nstroke_raw.shape","7ad41fa3":"# checking the size of the raw dataset\nstroke_raw.size","b7ea1bdc":"# Profiling of Data\nx=pandas_profiling.ProfileReport(stroke_raw)\nx","f29f1bda":"# print data characteristics, usings pandas built-in describe() function\nstroke_raw.describe()","08496e03":"# converting some data type to object\nstroke_raw = stroke_raw.astype({'hypertension': 'int', 'heart_disease': 'int'})","a2e826f7":"# crosscheck the dataset to identify the feature with missing value\nprint(stroke_raw.info())\nmissing_values = (stroke_raw.isnull().sum() \/ len(stroke_raw)) * 100\nprint(\"\\nFeatures with missing values: \\n\", missing_values[missing_values > 0])","34e8c943":"# Explore the unique values of categorical features\nprint(np.char.center(\" The Unique values of categorical variables \", 60, fillchar = \"*\"))\nprint(\"\\nGender: \", stroke_raw.gender.unique())\nprint(\"Heart Disease: \", sorted(stroke_raw.heart_disease.unique()))\nprint(\"Smoking Status: \", sorted(stroke_raw.smoking_status.unique()))\nprint(\"Ever married: \", sorted(stroke_raw.ever_married.unique()))\nprint(\"work type: \", sorted(stroke_raw.work_type.unique()))\nprint(\"hypertension: \", sorted(stroke_raw.hypertension.unique()))\nprint(\"Residence type: \", sorted(stroke_raw.Residence_type.unique()))\nprint(\"Stroke: \", sorted(stroke_raw.stroke.unique()))","4df9fb01":"# Checking for the features we have in the dataset to see if there are useless features\nstroke_raw.columns","ed2f7dc7":"# Drop the Id colomn since it is the only feature that won't be needed for the data processing\nstroke_raw.drop('id', inplace=True, axis=1)","c2f5ddcd":"# Check for any null values\nstroke_raw.isnull().sum()","1627706b":"# Finding out the percentage of missing values \n# In order to know if we can ignore, fill or eliminate the missing values\ncount=0\nfor i in stroke_raw.isnull().sum(axis=1):\n    if i>0:\n        count=count+1\nprint('Total number of rows with missing values is ', count)\nprint('Since the percentage of missing value is',round((count\/len(stroke_raw.index))*100),'percent', 'I shouldnt drop them')\n\nplt.title('Missing Value in Stroke Dataset',fontweight='bold')\nax = sns.heatmap(stroke_raw.isna().sum().to_frame(),annot=True,fmt='d',cmap='vlag')\nax.set_xlabel('Amount Missing')\nplt.show()","992c1c56":"# We have known the colomn with missing value, let's Find the specific rows with missing data\nstroke_raw.isnull().sum(axis = 1)","291f8959":"# visualize the missing data in the dataset\nsns.heatmap(stroke_raw.isnull(),yticklabels=False,cbar=False,cmap='viridis')","2901221f":"# Constructing decision tree pipe for Bmi with missing data\n# Setting the random_state to 42\n# Then fill the missing values with the accurate randomized values\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler,LabelEncoder\nfrom sklearn.tree import DecisionTreeRegressor,DecisionTreeClassifier\n\npipe_bmi = Pipeline( steps=[ \n                               ('scale',StandardScaler()),\n                               ('lr',DecisionTreeRegressor(random_state=42))\n                              ])\nX = stroke_raw[['avg_glucose_level','gender','bmi']].copy()\nX.gender = X.gender.replace({'Male':0,'Female':1,'Other':-1}).astype(np.uint8)\n\nMissing = X[X.bmi.isna()]\nX = X[~X.bmi.isna()]\nY = X.pop('bmi')\npipe_bmi.fit(X,Y)\npredicted_bmi = pd.Series(pipe_bmi.predict(Missing[['avg_glucose_level','gender']]),index=Missing.index)\nstroke_raw.loc[Missing.index,'bmi'] = predicted_bmi","f1007bbf":"# Plot to check if the data is now clean\nsns.heatmap(stroke_raw.isnull(), cbar=False,cmap='viridis')","a41e069e":"# changing dataset name with a dummy dropna\nstroke = stroke_raw.dropna()","cc0528c0":"# plot for missing value status\nplt.title('Missing Value Status',fontweight='bold')\nax = sns.heatmap(stroke.isna().sum().to_frame(),annot=True,fmt='d',cmap='vlag')\nax.set_xlabel('Amount Missing')\nplt.show()","681db95a":"# Rename the stroke colomn to target so as to avoid confusion when predicting\nstroke.rename(columns={'stroke':'target'},inplace=True)","92d46088":"# Check the head and tail of the dataset before proceeding to visualization\nprint(stroke.head())\nprint (stroke.tail())","3b47c621":"# Compare the stroke chances by counting the dependent variable and plot using pyplot\n# Also, Calculate the percentage that had stroke or not\n\ncolors = ['darkturquoise', 'darkorange']\nplt.style.use('default')\nplt.rcParams['figure.figsize']=(12,6)\n\nax = sns.countplot(x='stroke', data=stroke_raw, palette=colors, alpha=0.9, edgecolor=('white'), linewidth=2)\nax.set_ylabel('count', fontsize=12)\nax.set_xlabel('target', fontsize=12)\nax.grid(b=True, which='major', color='grey', linewidth=0.2)\nplt.title('The Stroke Chance', fontsize=18)\nplt.show()\n\nstroke_0 = len(stroke_raw[stroke_raw.stroke == 0])\nstroke_1 = len(stroke_raw[stroke_raw.stroke == 1])\nprint(\"Percentage not Having stroke: {:.2f}%\".format((stroke_0 \/ (len(stroke_raw.stroke))*100)))\nprint(\"Percentage Having stroke: {:.2f}%\".format((stroke_1 \/ (len(stroke_raw.stroke))*100)))","d5ca0915":"# Using Histogram chart to Check the distribution of data across the features by implementing the function below:\n\ndef draw_histograms(dataframe, features, rows, cols):\n    fig=plt.figure(figsize=(20,20))\n    for i, feature in enumerate(features):\n        ax=fig.add_subplot(rows,cols,i+1)\n        dataframe[feature].hist(bins=20,ax=ax,facecolor='green')\n        ax.set_title(feature+\" Distribution\",color='red')\n        \n    fig.tight_layout() \n    plt.style.use('default')\n    plt.show()\ndraw_histograms(stroke,stroke.columns,6,3)","7ef70276":"# Now let's use a pie chart to get the exact percentage of the distribution \n\n# (1) First, Define the properties of the chart below, \n\ndef draw_semi_pie_chart(data, column, fig, renamed_index_dict, title):\n    default_colors = ['#66b3ff', '#ff9999', '#99ff99', '#ffcc99', '#c2c2f0', '#ffb3e6', '#ff6666']\n    rnd.shuffle(default_colors)\n    ax = stroke[column].value_counts().rename(index = renamed_index_dict).plot.pie(colors = default_colors, autopct='%1.1f%%', startangle=90, title = title)\n    ax.set_ylabel('')\n    for item in ([ax.title, ax.xaxis.label, ax.yaxis.label] + ax.get_xticklabels() + ax.get_yticklabels()):\n        item.set_fontsize(20)\n        \n    centre_circle = plt.Circle((0,0), 0.70, fc='white')\n    fig.gca().add_artist(centre_circle)","d8d1ae28":"# (2) Then, plot the chart one by one\n\nfig = plt.gcf()\nfig.set_size_inches(18, 17)\ngrid_rows = 3\ngrid_cols = 3\n\n# Plot Gender Pie chart\nplt.subplot(grid_rows, grid_cols, 1)\ndraw_semi_pie_chart(stroke, 'gender', fig, {0: 'Female', 1: 'Male'}, 'Gender Distribution')\n\n# Plot Hypertension Pie chart\nplt.subplot(grid_rows, grid_cols, 2)\ndraw_semi_pie_chart(stroke, 'hypertension', fig, {0: 'NO', 1: 'YES'}, 'Hypertension Distribution')\n\n# Plot Marriage Pie chart\nplt.subplot(grid_rows, grid_cols, 3)\ndraw_semi_pie_chart(stroke, 'ever_married', fig, {0: 'Not married', 1: 'Married'}, 'Married or Not')\n\n# Plot Work Type Pie chart\nplt.subplot(grid_rows, grid_cols, 4)\ndraw_semi_pie_chart(stroke, 'work_type', fig, {0: 'Never Worked', 1: 'Childeren', 2: 'Govt work', 3: 'self employed', 4: 'private' }, 'Work Type')\n\n# Plot Residence Type Pie chart\nplt.subplot(grid_rows, grid_cols, 5)\ndraw_semi_pie_chart(stroke, 'Residence_type', fig, {0: 'Urban', 1: 'Rural'}, 'Type of Residence')\n\n# Plot Smoking Status Pie chart\nplt.subplot(grid_rows, grid_cols, 6)\ndraw_semi_pie_chart(stroke, 'smoking_status', fig, {0: 'Unknown', 1: 'Smokes', 2: 'Never smokes', 3: 'Formally smokes', 4: 'private' }, 'smoking_status')\nfig.tight_layout()\nplt.show()","7341f4ca":"fig = ex.histogram(stroke['bmi'],nbins=100, height=500, width=700, template='ggplot2')\nfig.show()","6706ffb7":"fig = ex.histogram(stroke['avg_glucose_level'],nbins=100, height=500, width=700, template='ggplot2')\nfig.show()","3d21c412":"# Performing Label encoding on 'Residence_type' ,'ever_married' and 'gender'\nstroke[\"Residence_type\"] = stroke[\"Residence_type\"].apply(lambda x: 1 if x==\"Urban\" else 0)\nstroke[\"ever_married\"] = stroke[\"ever_married\"].apply(lambda x: 1 if x==\"Yes\" else 0)\nstroke[\"gender\"] = stroke[\"gender\"].apply(lambda x: 1 if x==\"Male\" else 0)","e8f564a2":"# Then plot the distribution of the encoded features\nsns.distplot(stroke['gender'],rug=True)\nplt.show()\n\nsns.distplot(stroke['Residence_type'],rug=True)\nplt.show()\n\nsns.distplot(stroke['ever_married'],rug=True)\nplt.show()","19e6996f":"# Plot the Gender vs target\nimport plotly\nimport plotly.graph_objs as go\nfrom plotly.offline import init_notebook_mode, iplot\ninit_notebook_mode(connected=True)\ncol='gender'\nd1=stroke[stroke['target']==0]\nd2=stroke[stroke['target']==1]\nv1=d1[col].value_counts().reset_index()\nv1=v1.rename(columns={col:'count','index':col})\nv1['percent']=v1['count'].apply(lambda x : 100*x\/sum(v1['count']))\nv1=v1.sort_values(col)\nv2=d2[col].value_counts().reset_index()\nv2=v2.rename(columns={col:'count','index':col})\nv2['percent']=v2['count'].apply(lambda x : 100*x\/sum(v2['count']))\nv2=v2.sort_values(col)\ntrace1 = go.Bar(x=v1[col], y=v1[\"count\"], name=0, marker=dict(color=\"#a678de\"))\ntrace2 = go.Bar(x=v2[col], y=v2[\"count\"], name=1, marker=dict(color=\"#6ad49b\"))\ndata = [trace1, trace2]\nlayout={'title':\"target over the gender(male or female)\",'xaxis':{'title':\"target\"}}\n#layout = go.Layout(title=\"Content added over the years\", legend=dict(x=0.1, y=1.1, orientation=\"h\"))\nfig = go.Figure(data, layout=layout)\niplot(fig)","48f608f6":"# plot the target vs work type\ncol='work_type'\nd1=stroke[stroke['target']==0]\nd2=stroke[stroke['target']==1]\nv1=d1[col].value_counts().reset_index()\nv1=v1.rename(columns={col:'count','index':col})\nv1['percent']=v1['count'].apply(lambda x : 100*x\/sum(v1['count']))\nv1=v1.sort_values(col)\nv2=d2[col].value_counts().reset_index()\nv2=v2.rename(columns={col:'count','index':col})\nv2['percent']=v2['count'].apply(lambda x : 100*x\/sum(v2['count']))\nv2=v2.sort_values(col)\ntrace1 = go.Bar(x=v1[col], y=v1[\"count\"], name=0, marker=dict(color=\"#a678de\"))\ntrace2 = go.Bar(x=v2[col], y=v2[\"count\"], name=1, marker=dict(color=\"#6ad49b\"))\ndata = [trace1, trace2]\nlayout={'title':\"target over the work type\",'xaxis':{'title':\"target\"}}\n#layout = go.Layout(title=\"Content added over the years\", legend=dict(x=0.1, y=1.1, orientation=\"h\"))\nfig = go.Figure(data, layout=layout)\niplot(fig)","e626531c":"# Plot the target vs the average glucose level\ncol='avg_glucose_level'\nd1=stroke[stroke['target']==0]\nd2=stroke[stroke['target']==1]\nv1=d1[col].value_counts().reset_index()\nv1=v1.rename(columns={col:'count','index':col})\nv1['percent']=v1['count'].apply(lambda x : 100*x\/sum(v1['count']))\nv1=v1.sort_values(col)\nv2=d2[col].value_counts().reset_index()\nv2=v2.rename(columns={col:'count','index':col})\nv2['percent']=v2['count'].apply(lambda x : 100*x\/sum(v2['count']))\nv2=v2.sort_values(col)\ntrace1 = go.Scatter(x=v1[col], y=v1[\"count\"], name=0, marker=dict(color=\"#a678de\"))\ntrace2 = go.Scatter(x=v2[col], y=v2[\"count\"], name=1, marker=dict(color=\"#6ad49b\"))\ndata = [trace1, trace2]\nlayout={'title':\"Target over the person's average glucose level\"}\nfig = go.Figure(data, layout=layout)\niplot(fig)","0bfc890e":"# Checking the relationship between Average glucose level, Bmi and ever_married\nsns.lmplot(x=\"avg_glucose_level\", y=\"bmi\",data=stroke,hue=\"ever_married\")\nplt.show()","6f2a4679":"# Tabulize the categorical vs Target\nstroke_cat = stroke[['gender','Residence_type','ever_married','smoking_status','work_type', 'hypertension','heart_disease','target']]\ncat_vs_target = pd.concat([pd.crosstab(stroke_cat[x], stroke_cat.target) for x in stroke_cat.columns[:-1]], keys = stroke_cat.columns[:-1])\ncat_vs_target","f4576083":"# (1) Plot the stroke frequency across the age range\npd.crosstab(stroke.age,stroke.target).plot(kind=\"bar\",figsize=(20,6))\nplt.title('Stroke Frequency for Ages')\nplt.xlabel('Age')\nplt.ylabel('Frequency')\nplt.show()\n\n# The chat below shows that stroke chances generally increases with age within the range of 45 to 82 years of age\n# We can also see that at 78 years old, there are more stroke patients than any other age range","6523b3a7":"# (2) Plot the stroke frequency in relation to BMI\n\nstroke.loc[stroke['target'] == 0,\n                 'bmi'].hist(label='No Stroke')\nstroke.loc[stroke['target'] == 1,\n                 'bmi'].hist(label=' There is Stroke')\nplt.xlabel('BMI')\nplt.ylabel('Stroke')\nplt.legend()\n\n# chances of stroke increses with bmi 20-40, and the rate at which there is no stroke is highest at BMI 19 to 38","2b3890a8":"plt.subplot(2,1,1)\nplt.title('Stroke Sample Distribution Based On Bmi And Glucose Level')\nsns.scatterplot(x=stroke['avg_glucose_level'],y=stroke['bmi'],hue=stroke['target'])\nplt.show()","a694a4cc":"plt.subplot(2,1,2)\nplt.title('Stroke Sample Distribution Based On Bmi And Age')\nsns.scatterplot(x=stroke['age'],y=stroke['bmi'],hue=stroke['target'])\nplt.show()","68673fb0":"fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(15, 5))\nstroke.plot(kind='scatter', x='age', y='avg_glucose_level', alpha=0.5, color='blue', ax=axes[0], title=\"Age vs. avg_glucose_level\")\nstroke.plot(kind='scatter', x='bmi', y='avg_glucose_level', alpha=0.5, color='orange', ax=axes[1], title=\"bmi vs. avg_glucose_level\")\nplt.show()","ef97971e":"# Performing 'one hot encoding' on 'work_type' and 'smoking_status'\nwork = pd.get_dummies(stroke['work_type'],prefix='work_type',drop_first=True)\nstroke=pd.concat([stroke,work],axis=1)\nstroke.drop(['work_type'],axis=1,inplace=True)\n\nsmoking = pd.get_dummies(stroke['smoking_status'],prefix='smoking_status',drop_first=True)\nstroke=pd.concat([stroke,smoking],axis=1)\nstroke.drop(['smoking_status'],axis=1,inplace=True)\n\n","57fb0523":"# Re-explore the dataset to check the one hot encoding\nstroke.head()","da8abf1a":"# checking correlation by plotting\n%matplotlib inline\nplt.figure(figsize=(10,10))\nsns.heatmap(stroke.corr(),annot=True,fmt='.1f')\nplt.show()","40ed121f":"# More deeper correlation check\nstroke.corr()","46d2f7da":"# Rename Stroke\nstroke_scaled = stroke","6538e619":"# Scaling all values except the target variable = target\n\nfrom sklearn.preprocessing import MinMaxScaler\nMMscal=MinMaxScaler()\nfeatures=['age', 'gender', 'hypertension', 'heart_disease', 'ever_married',\n       'Residence_type', 'avg_glucose_level', 'bmi', 'work_type_Never_worked',\n       'work_type_Private', 'work_type_Self-employed', 'work_type_children',\n       'smoking_status_formerly smoked', 'smoking_status_never smoked', 'smoking_status_smokes']\nstroke_scaled[features] = MMscal.fit_transform(stroke[features])\n\n#Check first five rows after scaling\nstroke_scaled.head()","13bef943":"# Creating Features and Target variable as X and Y\nX = stroke_scaled.drop('target', axis=1)\nY = stroke_scaled.target","67e12bd1":"# Splitting the data into training and testing data sets\nfrom sklearn.model_selection import train_test_split\nX_train1,X_test1,y_train1,y_test1=train_test_split(X,Y,test_size=0.2 ,random_state=1)","5707e9a8":"# Collecting Mutual Information for feature selection\nfrom sklearn.feature_selection import mutual_info_regression\nmutual_info1 = mutual_info_regression(X_train1, y_train1)\nmutual_info1 = pd.Series(mutual_info1)\nmutual_info1.index = X_train1.columns\nmutual_info1 = mutual_info1.sort_values(ascending=False)\nmutual_info1","0d173475":"# Considering the columns for training the model which are more than 0% of information shared with dependent variable\/feature\nReq_Columns1 = list(mutual_info1[mutual_info1>0].index)\nReq_Columns1","e7411628":"# Checking the first 5 row of training data\nX_train_final_1 = X_train1[Req_Columns1]\nX_train_final_1.head(5)","3c3b86e0":"# Checking the last 3 rows of X test data\nX_test_final_1 = X_test1[Req_Columns1]\nX_test_final_1.head(3)","750bd1df":"# Check Stroke Proportion of the Imbalanced Data\nfig = ex.pie(stroke,names='target')\nfig.update_layout(title='<b>Imbalance stroke proportion<b>')\nfig.show()","d92f291c":"# First, Define the first evaluation function for the first batches of algorithms as 'evaluation_1'\n\ndef evaluation_1(Y_test1,Y_pred1):\n    acc_1=accuracy_score(Y_test1,Y_pred1)\n    rcl_1=recall_score(Y_test1,Y_pred1)\n    f1_1=f1_score(Y_test1,Y_pred1)\n    auc_score_1 = roc_auc_score(Y_test1,Y_pred1)\n    prec_score_1=precision_score(Y_test1,Y_pred1)\n    \n    metric_dict={'accuracy': round(acc_1*100,2),\n               'recall': round(rcl_1*100,2),\n               'F1 score': round(f1_1*100,2),\n               'auc score': round(auc_score_1*100,2),\n               'precision': round(prec_score_1*100,2)\n                }\n    \n    return print(metric_dict)","f541a66d":"# Training KNN model \nnp.random.seed(42) # Set the pseudo random number\nKNC_model_1 =  KNeighborsClassifier() # Define the classifier\n\n# Fit the model for prediction\nKNC_model_1.fit(X_train_final_1,y_train1)\n\n# make prediction on testing data   \nKNC_model_y_pred_1=KNC_model_1.predict(X_test_final_1) \nKNC_model_r2_score_1=round(r2_score(y_test1,KNC_model_y_pred_1)*100,2)\n\n# Print the scores and the evaluation\nprint(\"R2 Score for predicted value: \",KNC_model_r2_score_1)\nprint(\"Accuracy on Training set: \",round(KNC_model_1.score(X_train_final_1,y_train1)*100,2))\nKNC_model_score_1 = round(KNC_model_1.score(X_test_final_1,y_test1)*100,2)\nprint(\"Accuracy on Testing set: \",KNC_model_score_1)\nevaluation_1(y_test1,KNC_model_y_pred_1)","463de609":"# Plot the confusion matrix\nfig,ax=plt.subplots()\nax=sns.heatmap(confusion_matrix(y_test1,KNC_model_y_pred_1),annot=True,cbar=True)","fa0cd614":"from sklearn.linear_model import LogisticRegression\n\n# Training Logistics model \nnp.random.seed(42) # Set the pseudo random number\nLR_model_1 = LogisticRegression() # Define the classifier\n\n# Fit the model for prediction\nLR_model_1.fit(X_train_final_1,y_train1)\n\n# make prediction on testing data   \nLR_model_y_pred_1 = LR_model_1.predict(X_test_final_1) \nLR_model_r2_score_1=round(r2_score(y_test1,KNC_model_y_pred_1)*100,2)\n\n# Print the scores and the evaluation\nprint(\"R2 Score for predicted value: \",LR_model_r2_score_1)\nprint(\"Accuracy on Training set: \",round(LR_model_1.score(X_train_final_1,y_train1)*100,2))\nLR_model_score_1 = round(LR_model_1.score(X_test_final_1,y_test1)*100,2)\nprint(\"Accuracy on Testing set: \",LR_model_score_1)\nevaluation_1(y_test1,KNC_model_y_pred_1)","daa7d908":"# Plot the confusion matrix\nfig,ax=plt.subplots()\nax=sns.heatmap(confusion_matrix(y_test1,LR_model_y_pred_1),annot=True,cbar=True)","2b571b4d":"# Training Random Forest model \nnp.random.seed(42) # Set the pseudo random number\nRFC_model_1=RandomForestClassifier() # Define the classifier\n\n# Fit the model for prediction\nRFC_model_1.fit(X_train_final_1,y_train1)\n\n# make prediction on testing data   \nRFC_model_y_pred_1 = RFC_model_1.predict(X_test_final_1) \nRFC_model_r2_score_1=round(r2_score(y_test1,RFC_model_y_pred_1)*100,2)\n\n# Print the scores and the evaluation\nprint(\"R2 Score for predicted value: \",RFC_model_r2_score_1)\nprint(\"Accuracy on Training set: \",round(RFC_model_1.score(X_train_final_1,y_train1)*100,2))\nRFC_model_score_1 = round(RFC_model_1.score(X_test_final_1,y_test1)*100,2)\nprint(\"Accuracy on Testing set: \",RFC_model_score_1)\nevaluation_1(y_test1,RFC_model_y_pred_1)","6051021b":"# Plot the confusion matrix\nfig,ax=plt.subplots()\nax=sns.heatmap(confusion_matrix(y_test1,RFC_model_y_pred_1),annot=True,cbar=True)","9c0668cd":"from sklearn.svm import SVC\n\n# Training SVM model \nnp.random.seed(42) # Set the pseudo random number\nSVC_model_1=SVC() # Define the classifier\n\n# Fit the model for prediction\nSVC_model_1.fit(X_train_final_1,y_train1)\n\n# make prediction on testing data   \nSVC_model_y_pred_1 = SVC_model_1.predict(X_test_final_1) \nSVC_model_r2_score_1=round(r2_score(y_test1,SVC_model_y_pred_1)*100,2)\n\n# Print the scores and the evaluation\nprint(\"R2 Score for predicted value: \",SVC_model_r2_score_1)\nprint(\"Accuracy on Training set: \",round(SVC_model_1.score(X_train_final_1,y_train1)*100,2))\nSVC_model_score_1 = round(SVC_model_1.score(X_test_final_1,y_test1)*100,2)\nprint(\"Accuracy on Testing set: \",SVC_model_score_1)\nevaluation_1(y_test1,SVC_model_y_pred_1)","aad53f99":"# Plot the confusion matrix\nfig,ax=plt.subplots()\nax=sns.heatmap(confusion_matrix(y_test1,SVC_model_y_pred_1),annot=True,cbar=True)","86a52ace":"np.random.seed(42) # Set the pseudo random number\nfrom xgboost import XGBClassifier\nXGB_model_1 = XGBClassifier()\nXGB_model_1.fit(X_train_final_1,y_train1)\nXGB_model_y_pred_1=XGB_model_1.predict(X_test_final_1)\nXGB_model_r2_score_1=round(r2_score(y_test1,XGB_model_y_pred_1)*100,2)\nprint(\"R2 Score for predicted value: \",XGB_model_r2_score_1)\nprint(\"Accuracy on Training set: \",round(XGB_model_1.score(X_train_final_1,y_train1)*100,2))\nXGB_model_score_1 = round(XGB_model_1.score(X_test_final_1,y_test1)*100,2)\nprint(\"Accuracy on Testing set: \",XGB_model_score_1)\nevaluation_1(y_test1,XGB_model_y_pred_1)","ea2916c8":"# Plot the confusion matrix\nfig,ax=plt.subplots()\nax=sns.heatmap(confusion_matrix(y_test1,XGB_model_y_pred_1),annot=True,cbar=True)","fc96c7c4":"from sklearn.neural_network import MLPClassifier\n\n# Define the classification parameters\nmlp_1 = MLPClassifier(hidden_layer_sizes=(300,300,300), max_iter=1000, alpha=0.00001,\n                     solver='adam', verbose=10,  random_state=21)\n# Fit the models\nmlp_1.fit(X_train_final_1, y_train1)\nmlp_pred_1= mlp_1.predict(X_test_final_1)\n\nmlp_score_1 = mlp_1.score(X_train_final_1, y_train1)\nmlp_test_1 = mlp_1.score(X_test_final_1, y_test1)\nmlp_model_r2_score_1=round(r2_score(y_test1,mlp_pred_1)*100,2)\n\n# Make predictions\ny_pred_1 =mlp_1.predict(X_test_final_1)\n\n#evaluation\ncm = confusion_matrix(y_test1,y_pred_1)\nprint(\"R2 Score for predicted value: \",mlp_model_r2_score_1)\nprint('Training Score',mlp_score_1)\nprint('Testing Score \\n',mlp_test_1)\nprint(cm)\nevaluation_1(y_test1,mlp_pred_1)","9de8f966":"# Plot the confusion matrix\nfig,ax=plt.subplots()\nax=sns.heatmap(confusion_matrix(y_test1,mlp_pred_1),annot=True,cbar=True)","dec428f9":"# First Concatenate the accuracy Accuracy scores\nsummary_score1 = pd.DataFrame({'Applied_Model': ['Logistic Regression Raw', 'Random Forest Raw', 'K-Nearest Neighbour Raw', 'Support Vector Machine Raw', \"Extreme Gradient Boost Raw\", 'Multilayer Perceptron Raw'], \n                                'Accuracy_Score': [LR_model_score_1, RFC_model_score_1, KNC_model_score_1, SVC_model_score_1, XGB_model_score_1,mlp_test_1*100]})\nsummary_score1 = summary_score1.sort_values(by = ['Accuracy_Score'], ascending = False)\nsummary_score1","8fa4ac91":"# Define evaluation metrics for Logistic Regression as the best performing algorithm\nLGR_final_metrics1 ={'Accuracy': LR_model_1.score(X_test_final_1,y_test1),\n                   'Precision': precision_score(y_test1,LR_model_y_pred_1),\n                   'Recall': recall_score(y_test1,LR_model_y_pred_1),\n                   'F1': f1_score(y_test1,LR_model_y_pred_1),\n                   'AUC': roc_auc_score(y_test1,LR_model_y_pred_1)}\n\nLGR_metrics=pd.DataFrame(LGR_final_metrics1,index=[0])\nLGR_metrics.T.plot.bar(title='Logistic Regression Metric Evaluation', color = 'black', legend=False);","d8bfefa6":"# Find the Area under curve\nfrom sklearn import metrics\ny_pred_rfc_proba1 = RFC_model_1.predict_proba(X_test_final_1)[::,1]\nfpr_rfc, tpr_rfc, _ = metrics.roc_curve(y_test1,  y_pred_rfc_proba1)\nauc_rfc = metrics.roc_auc_score(y_test1, y_pred_rfc_proba1)\nprint(\"AUC RFC :\", auc_rfc)","3edfcbff":"# plot the ROC curve of the AUC\nplt.plot(fpr_rfc,tpr_rfc,label=\"RFC, auc={:.3f})\".format(auc_rfc))\nplt.plot([0, 1], [0, 1], 'k--')\nplt.xlabel('False positive rate')\nplt.ylabel('True positive rate')\nplt.title('Random Forest ROC curve')\nplt.legend(loc=4)\nplt.show()","3b2438cd":"# Define the dictionary for listing\naccuracies_under = dict()\nrecall_under = dict()","c01ab679":"# Splitting the data into training and testing data sets\nfrom sklearn.model_selection import train_test_split\nX_train2,X_test2,y_train2,y_test2=train_test_split(X,Y,test_size=0.2 ,random_state=1)","badd3ac7":"# Show the count of the target Before Applying Random UNDER Sampling\n\none_count = 0\nzero_count = 0\n\nfor i in y_train2:\n    if i == 1:\n        one_count +=1\n    else:\n        zero_count +=1\n        \nprint('Number of one count before applying RandomUnder Sampler is: ', one_count)\nprint('Number of zero count before applying RandomUnder Sampler is: ', zero_count)","5ce1af07":"# Import the package and perform undersampling on the data \nfrom imblearn.under_sampling import RandomUnderSampler\nunder = RandomUnderSampler(sampling_strategy= 0.6)\n\nX_train2, y_train2 = under.fit_resample(X_train2, y_train2)","efc2ec47":"#  Now, let us check the data After Applying Random UnderSampling\none_count = 0\nzero_count = 0\n\nfor i in y_train2:\n    if i == 1:\n        one_count +=1\n    else:\n        zero_count +=1\n        \nprint('Number of one count after applying RandomUnder Sampler is: ', one_count)\nprint('Number of zero count after applying RandomUnder Sampler is: ', zero_count)","e49e423d":"# plot to show target after Random Undersampling \nfig = ex.pie(y_train2,names='target')\nfig.update_layout(title='<b>Stroke Proportion After Random Undersampling<b>')\nfig.show()","4cb16dd5":"# Collect Mutual Information for the data to be used for Train and Test\nfrom sklearn.feature_selection import mutual_info_regression\nmutual_info2 = mutual_info_regression(X_train2, y_train2)\nmutual_info2 = pd.Series(mutual_info1)\nmutual_info2.index = X_train2.columns\nmutual_info2 = mutual_info2.sort_values(ascending=False)\nmutual_info2","b1c33212":"# Considering the columns for training the model which are more than 0% of information shared with dependent variable\/feature\nReq_Columns2 = list(mutual_info2[mutual_info2>0].index)\nReq_Columns2","49645a37":"# Define the training set \nX_train_final_2 = X_train2[Req_Columns2]\nX_train_final_2.head(5)","7a7b6a3b":"# Define the testing set\nX_test_final_2 = X_test2[Req_Columns1]\nX_test_final_2.head(3)","cc252958":"# Define the evaluation function for the  new Algorithm implementation\n\ndef evaluation_2(Y_test2,Y_pred2):\n    acc_2=accuracy_score(Y_test2,Y_pred2)\n    rcl_2=recall_score(Y_test2,Y_pred2)\n    f1_2=f1_score(Y_test2,Y_pred2)\n    auc_score_2 = roc_auc_score(Y_test2,Y_pred2)\n    prec_score_2=precision_score(Y_test2,Y_pred2)\n    \n    metric_dict={'accuracy': round(acc_2*100,2),\n               'recall': round(rcl_2*100,2),\n               'F1 score': round(f1_2*100,2),\n               'auc score': round(auc_score_2*100,2),\n               'precision': round(prec_score_2*100,2)\n                }\n    \n    return print(metric_dict)","59f7ede0":"# Training KNN model \nnp.random.seed(42) # Set the pseudo random number\nKNC_model_2 =  KNeighborsClassifier() # Define the classifier\n\n# Fit the model for prediction\nKNC_model_2.fit(X_train_final_2,y_train2)\n\n# make prediction on testing data   \nKNC_model_y_pred_2=KNC_model_2.predict(X_test_final_2) \nKNC_model_r2_score_2=round(r2_score(y_test2,KNC_model_y_pred_2)*100,2)\n\n# Print the scores and the evaluation\nprint(\"R2 Score for predicted value: \",KNC_model_r2_score_2)\nprint(\"Accuracy on Training set: \",round(KNC_model_2.score(X_train_final_2,y_train2)*100,2))\nKNC_model_score_2 = round(KNC_model_2.score(X_test_final_2,y_test2)*100,2)\nprint(\"Accuracy on Testing set: \",KNC_model_score_2)\nevaluation_2(y_test2,KNC_model_y_pred_2)","c05a3896":"# Plot the confusion matrix\nfig,ax=plt.subplots()\nax=sns.heatmap(confusion_matrix(y_test2,KNC_model_y_pred_2),annot=True,cbar=True)","00acb3c3":"# Training Logistics model \nnp.random.seed(42) # Set the pseudo random number\nLR_model_2 = LogisticRegression() # Define the classifier\n\n# Fit the model for prediction\nLR_model_2.fit(X_train_final_2,y_train2)\n\n# make prediction on testing data   \nLR_model_y_pred_2 = LR_model_2.predict(X_test_final_2) \nLR_model_r2_score_2=round(r2_score(y_test2,LR_model_y_pred_2)*100,2)\n\n# Print the scores and the evaluation\nprint(\"R2 Score for predicted value: \",LR_model_r2_score_2)\nprint(\"Accuracy on Training set: \",round(LR_model_2.score(X_train_final_2,y_train2)*100,2))\nLR_model_score_2 = round(LR_model_2.score(X_test_final_2,y_test2)*100,2)\nprint(\"Accuracy on Testing set: \",LR_model_score_2)\nevaluation_2(y_test2,LR_model_y_pred_2)","d7df8827":"# Plot the confusion matrix\nfig,ax=plt.subplots()\nax=sns.heatmap(confusion_matrix(y_test2,LR_model_y_pred_2),annot=True,cbar=True)","60901e99":"# Training Random Forest model \nnp.random.seed(42) # Set the pseudo random number\nRFC_model_2 = RandomForestClassifier() # Define the classifier\n\n# Fit the model for prediction\nRFC_model_2.fit(X_train_final_2,y_train2)\n\n# make prediction on testing data   \nRFC_model_y_pred_2 = RFC_model_2.predict(X_test_final_2) \nRFC_model_r2_score_2=round(r2_score(y_test2,RFC_model_y_pred_2)*100,2)\n\n# Print the scores and the evaluation\nprint(\"R2 Score for predicted value: \",RFC_model_r2_score_2)\nprint(\"Accuracy on Training set: \",round(RFC_model_2.score(X_train_final_2,y_train2)*100,2))\nRFC_model_score_2 = round(RFC_model_2.score(X_test_final_2,y_test2)*100,2)\nprint(\"Accuracy on Testing set: \",RFC_model_score_2)\nevaluation_2(y_test2,RFC_model_y_pred_2)","1c517983":"# Plot the confusion matrix\nfig,ax=plt.subplots()\nax=sns.heatmap(confusion_matrix(y_test2,RFC_model_y_pred_2),annot=True,cbar=True)","43f2b5a7":"# Training SVM model \nnp.random.seed(42) # Set the pseudo random number\nSVC_model_2=SVC() # Define the classifier\n\n# Fit the model for prediction\nSVC_model_2.fit(X_train_final_2,y_train2)\n\n# make prediction on testing data   \nSVC_model_y_pred_2 = SVC_model_2.predict(X_test_final_2) \nSVC_model_r2_score_2=round(r2_score(y_test2,SVC_model_y_pred_2)*100,2)\n\n# Print the scores and the evaluation\nprint(\"R2 Score for predicted value: \",SVC_model_r2_score_2)\nprint(\"Accuracy on Training set: \",round(SVC_model_2.score(X_train_final_2,y_train2)*100,2))\nSVC_model_score_2 = round(SVC_model_2.score(X_test_final_2,y_test2)*100,2)\nprint(\"Accuracy on Testing set: \",SVC_model_score_2)\nevaluation_2(y_test2,SVC_model_y_pred_2)","b188c252":"# Plot the confusion matrix\nfig,ax=plt.subplots()\nax=sns.heatmap(confusion_matrix(y_test2,SVC_model_y_pred_2),annot=True,cbar=True)","c1c6afa0":"# Define the parameters\nmlp_2 = MLPClassifier(hidden_layer_sizes=(300,300,300), max_iter=1000, alpha=0.00001,\n                     solver='adam', verbose=10,  random_state=21)\n# Fit the models\nmlp_2.fit(X_train_final_2, y_train2)\nmlp_pred_2= mlp_2.predict(X_test_final_2)\n\nmlp_score_2 = round(mlp_2.score(X_train_final_2, y_train2)*100,2)\nmlp_test_2 = mlp_2.score(X_test_final_2, y_test2)\nmlp_model_r2_score_2=round(r2_score(y_test2,mlp_pred_2)*100,2)\n\n\n# Make predictions\ny_pred_2 =mlp_2.predict(X_test_final_2)\n\n#evaluation\nprint(\"R2 Score for predicted value: \",mlp_model_r2_score_2)\nprint('Training Score',mlp_score_2)\nprint('Testing Score \\n',mlp_test_2)\n\nevaluation_2(y_test2,mlp_pred_2)","c021658e":"# Plot the confusion matrix\nfig,ax=plt.subplots()\nax=sns.heatmap(confusion_matrix(y_test2,mlp_pred_2),annot=True,cbar=True)","7d9eabcc":"# Concatenate the accuracies of the algorithms\nsummary_score2 = pd.DataFrame({'Applied_Model': ['Logistic Regression Rand', 'Random Forest Rand', 'K-Nearest Neighbour Rand', 'Support Vector Machine Rand', 'Multilayer Perceptron Rand'], \n                                'Accuracy_Score': [LR_model_score_2, RFC_model_score_2, KNC_model_score_2, SVC_model_score_2,mlp_test_2*100]})\nsummary_score2 = summary_score2.sort_values(by=['Accuracy_Score'], ascending=False)\nsummary_score2","93ce93cc":"# Define evaluation metrics for Logistic Regression as the best performing algorithm\nLGR_final_metrics2 ={'Accuracy': LR_model_2.score(X_test_final_2,LR_model_y_pred_2),\n                   'Precision': precision_score(y_test2,LR_model_y_pred_2),\n                   'Recall': recall_score(y_test2,LR_model_y_pred_2),\n                   'F1': f1_score(y_test2,LR_model_y_pred_2),\n                   'AUC': roc_auc_score(y_test2,LR_model_y_pred_2)}\n\nLGR_metrics2 = pd.DataFrame(LGR_final_metrics2,index=[0])\nLGR_metrics2.T.plot.bar(title='Logistic Regression Metric Evaluation', color = 'blue', legend=False);","905a7386":"# Find the Area under curve\nfrom sklearn import metrics\ny_pred_LR_proba2 = LR_model_2.predict_proba(X_test_final_2)[::,1]\nfpr_LR, tpr_LR, _ = metrics.roc_curve(y_test2,  y_pred_LR_proba2)\nauc_LR = metrics.roc_auc_score(y_test2, y_pred_LR_proba2)\nprint(\"AUC LR :\", auc_LR)","fb6857f5":"# plot the ROC curve of the AUC\nplt.plot(fpr_rfc,tpr_rfc,label=\"LR, auc={:.3f})\".format(auc_rfc))\nplt.plot([0, 1], [0, 1], 'k--')\nplt.xlabel('False positive rate')\nplt.ylabel('True positive rate')\nplt.title('Logistics Regression ROC curve')\nplt.legend(loc=4)\nplt.show()","5cacc8b0":"# Performing smote on the original data\nsm = SMOTE(random_state=123)\nX_sm , Y_sm = sm.fit_resample(X,Y)\n\nprint(f'''Shape of X before SMOTE:{X.shape}\nShape of X after SMOTE:{X_sm.shape}''',\"\\n\\n\")\n\nprint(f'''Target Class distributuion before SMOTE:\\n{Y.value_counts(normalize=True)}\nTarget Class distributuion after SMOTE :\\n{Y_sm.value_counts(normalize=True)}''')","fce73add":"# plot to show target after performing SMOTE \nfig = ex.pie(Y_sm,names='target')\nfig.update_layout(title='<b>Stroke Proportion After Smote Sampling<b>')\nfig.show()","9498f5f5":"# splitting the data into training and testing data sets\nfrom sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test=train_test_split(X_sm,Y_sm,test_size=0.2 ,random_state=1)","877ffe59":"# Getting Mutual Informations\nmutual_info = mutual_info_regression(X_train, y_train)\nmutual_info = pd.Series(mutual_info)\nmutual_info.index = X_train.columns\nmutual_info = mutual_info.sort_values(ascending=False)\nmutual_info","47a108ba":"# Considering the columns for training the model which are more than 0% of information shared with dependent variable\/feature\nReq_Columns = list(mutual_info[mutual_info>0].index)\nReq_Columns","1f91dd6f":"# Get the final training set\nX_train_final = X_train[Req_Columns]\nX_train_final.head(5)","b365dfa8":"# Get the final Test set\nX_test_final = X_test[Req_Columns]\nX_test_final.head(3)","ee2dbc40":"# Define the Evaluation metrics for the Algorthm\ndef evaluation(Y_test,y_pred):\n    acc=accuracy_score(y_test,y_pred)\n    rcl=recall_score(y_test,y_pred)\n    f1=f1_score(y_test,y_pred)\n    auc_score=roc_auc_score(y_test,y_pred)\n    prec_score=precision_score(y_test,y_pred)\n    \n    metric_dict={'accuracy': round(acc*100,2),\n               'recall': round(rcl*100,2),\n               'F1 score': round(f1*100,2),\n               'auc score': round(auc_score*100,2),\n               'precision': round(prec_score*100,2)\n                }\n    \n    return print(metric_dict)","0e50510c":"# Training KNN model\nnp.random.seed(42) # Set the pseudo random number\nKNC_model=  KNeighborsClassifier() # Define the classifier\nKNC_model.fit(X_train_final,y_train) # Fit the model\nKNC_model_y_pred=KNC_model.predict(X_test_final) # make prediction on testing data\nKNC_model_r2_score=round(r2_score(y_test,KNC_model_y_pred)*100,2)\n\n# Print the scores and the evaluation\nprint(\"R2 Score for predicted value: \",KNC_model_r2_score)\nprint(\"Accuracy on Training set: \",round(KNC_model.score(X_train_final,y_train)*100,2))\nKNC_model_score = round(KNC_model.score(X_test_final,y_test)*100,2)\nprint(\"Accuracy on Testing set: \",KNC_model_score)\nevaluation(y_test,KNC_model_y_pred)","2e0b97f8":"# Plot the confusion matrix\nfig,ax=plt.subplots()\nax=sns.heatmap(confusion_matrix(y_test,KNC_model_y_pred),annot=True,cbar=True)","e9139a37":"from sklearn.linear_model import LogisticRegression\nnp.random.seed(42)\nLR_model=LogisticRegression()\nLR_model.fit(X_train_final,y_train)\nLR_model_y_pred=LR_model.predict(X_test_final)\nLR_model_r2_score=round(r2_score(y_test,LR_model_y_pred)*100,2)\nprint(\"R2 Score for predicted value: \",LR_model_r2_score)\nprint(\"Accuracy on Training set: \",round(LR_model.score(X_train_final,y_train)*100,2))\nLR_model_score = round(LR_model.score(X_test_final,y_test)*100,2)\nprint(\"Accuracy on Testing set: \",LR_model_score)\nevaluation(y_test,LR_model_y_pred)","3b8662af":"# plot the confusion matrix\nfig,ax=plt.subplots()\nax=sns.heatmap(confusion_matrix(y_test,LR_model_y_pred),annot=True,cbar=True)","8a2986ff":"# Check the relevance of feature by checking the coeficients\ncoef = LR_model.coef_[0]\ncoef = [abs(number) for number in coef]\nprint(coef)","76c35c10":"# First, arrange the colomn\ncols = list(stroke_scaled.columns)\ncols.index('target')\n\n# Remove the target label \ndel cols[8]\ncols","a94b125d":"# Then arrange the colomn based on importance\nsorted_index = sorted(range(len(coef)), key = lambda k: coef[k], reverse = True)\nfor idx in sorted_index:\n    print(cols[idx])","80ac9990":"# Training the model\nnp.random.seed(42)\nRFC_model=RandomForestClassifier()\nRFC_model.fit(X_train_final,y_train)\nRFC_model_y_pred=RFC_model.predict(X_test_final)\nRFC_model_r2_score=round(r2_score(y_test,RFC_model_y_pred)*100,2)\n\n# Print the accuracy and evaluation\nprint(\"R2 Score for predicted value: \",RFC_model_r2_score)\nprint(\"Accuracy on Training set: \",round(RFC_model.score(X_train_final,y_train)*100,2))\nRFC_model_score = round(RFC_model.score(X_test_final,y_test)*100,2)\nprint(\"Accuracy on Testing set: \",RFC_model_score)\nevaluation(y_test,RFC_model_y_pred)","16c71d34":"# plot the confusion matrix\nfig,ax=plt.subplots()\nax=sns.heatmap(confusion_matrix(y_test,RFC_model_y_pred),annot=True,cbar=True)","c042ee35":"from sklearn.svm import SVC\nnp.random.seed(42)\nSVC_model=SVC()\nSVC_model.fit(X_train_final,y_train)\nSVC_model_y_pred=SVC_model.predict(X_test_final)\nSVC_model_r2_score=round(r2_score(y_test,SVC_model_y_pred)*100,2)\nprint(\"R2 Score for predicted value: \",SVC_model_r2_score)\nprint(\"Accuracy on Training set: \",round(SVC_model.score(X_train_final,y_train)*100,2))\nSVC_model_score = round(SVC_model.score(X_test_final,y_test)*100,2)\nprint(\"Accuracy on Testing set: \",SVC_model_score)\nevaluation(y_test,SVC_model_y_pred)","8230517a":"# plot the confusion matrix\nfig,ax=plt.subplots()\nax=sns.heatmap(confusion_matrix(y_test,SVC_model_y_pred),annot=True,cbar=True)","f340fe07":"from xgboost import XGBClassifier\nXGB_model=XGBClassifier()\nXGB_model.fit(X_train_final,y_train)\nXGB_model_y_pred=XGB_model.predict(X_test_final)\nXGB_model_r2_score=round(r2_score(y_test,XGB_model_y_pred)*100,2)\nprint(\"R2 Score for predicted value: \",XGB_model_r2_score)\nprint(\"Accuracy on Training set: \",round(XGB_model.score(X_train_final,y_train)*100,2))\nXGB_model_score = round(XGB_model.score(X_test_final,y_test)*100,2)\nprint(\"Accuracy on Testing set: \",XGB_model_score)\nevaluation(y_test,XGB_model_y_pred)","894e1cd0":"# plot the confusion matrix\nfig,ax=plt.subplots()\nax=sns.heatmap(confusion_matrix(y_test,XGB_model_y_pred),annot=True,cbar=True)","199be078":"from sklearn.neural_network import MLPClassifier\n\n# Define the parameters\nmlp=MLPClassifier(hidden_layer_sizes=(300,300,300), max_iter=1000, alpha=0.00001,\n                     solver='adam', verbose=10,  random_state=21)\n# Fit the models\nmlp.fit(X_train_final, y_train)\nmlp_pred= mlp.predict(X_test_final)\n\nmlp_score = mlp.score(X_train_final, y_train)\nmlp_test = mlp.score(X_test_final, y_test)\n\n# Make predictions\ny_pred =mlp.predict(X_test_final)\n\n#evaluation\ncm = confusion_matrix(y_test,y_pred)\nprint('Training Score',mlp_score)\nprint('Testing Score \\n',mlp_test)\nprint(cm)\nevaluation(y_test,mlp_pred)","e2ef7c3c":"# plot the confusion matrix\nfig,ax=plt.subplots()\nax=sns.heatmap(confusion_matrix(y_test,mlp_pred),annot=True,cbar=True)","38733df4":"# Compare the Accuracy scores\nsummary_score = pd.DataFrame({'Applied_Model': ['Logistic Regression smote', 'Random Forest smote', 'K-Nearest Neighbour smote', 'Support Vector Machine smote', \"Extreme Gradient Boost smote\", 'Multilayer Perceptron smote'], \n                                'Accuracy_Score': [LR_model_score, RFC_model_score, KNC_model_score, SVC_model_score, XGB_model_score,mlp_test*100]})\nsummary_score = summary_score.sort_values(by=['Accuracy_Score'], ascending=False)\nsummary_score","f71c3ecb":"# Find the Area under curve\nfrom sklearn import metrics\ny_pred_xgb_proba = XGB_model.predict_proba(X_test_final)[::,1]\nfpr_xgb, tpr_xgb, _ = metrics.roc_curve(y_test,  y_pred_xgb_proba)\nauc_xgb = metrics.roc_auc_score(y_test, y_pred_xgb_proba)\nprint(\"AUC XGB :\", auc_xgb)","bc8385cd":"# plot the ROC curve\nplt.plot(fpr_xgb,tpr_xgb,label=\"XGB, auc={:.3f})\".format(auc_xgb))\nplt.plot([0, 1], [0, 1], 'k--')\nplt.xlabel('False positive rate')\nplt.ylabel('True positive rate')\nplt.title('Extreme Gradient Boost ROC curve')\nplt.legend(loc=4)\nplt.show()","0be71b38":"from sklearn.metrics import precision_recall_curve\nxgb_precision, xgb_recall, _ = precision_recall_curve(y_test, y_pred_xgb_proba)\nno_skill = len(y_test[y_test==1]) \/ len(y_test)\nplt.plot([0, 1], [no_skill, no_skill], linestyle='--', color='black', label='No Skill')\nplt.plot(xgb_recall, xgb_precision, color='orange', label='XGB')\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.title('Precision-Recall curve')\nplt.legend()\nplt.show()","d9b26ace":"# Checking the cross validation score ON XGB to get best score\nfrom sklearn.model_selection import cross_val_score\nscores = cross_val_score(XGB_model, X_train_final, y_train, cv = 10)\nprint(\"Scores: \", scores)\nprint(\"Accuracy: \", round(scores.mean(), 2) * 100, \"%\")\nprint(\"Standard Deviation: +\/-\", scores.std())","54a6aacb":"j = range(1, 31) # Set range from 1 to 30\n\n# Setup algorithm\nknc_model = KNeighborsClassifier()\nknc_count=1\nscore_final = 0\n# Loop through different neighbors values\nfor i in j:\n    knc_model.set_params(n_neighbors = i) # set neighbors value\n    score = round(knc_model.fit(X_train_final, y_train).score(X_test_final,y_test)*100,2)\n    if score > score_final:\n        score_final = score\n        knc_count = i\n    # Fit the algorithm\n    #print(f\"Accuracy with {i} no. of neighbors: {score}%\")\nprint('Best n_neighbors is: ' + str(knc_count) + ', with accuracy score of ' + str(score_final) +'%')","b1ca01b7":"# Train and test after manual Fine tuning\nnp.random.seed(42)\nfrom sklearn.neighbors import KNeighborsClassifier\nKNC_model=  KNeighborsClassifier(n_neighbors=knc_count)\nKNC_model.fit(X_train_final,y_train)\nKNC_model_y_pred=KNC_model.predict(X_test_final)\nKNC_model_r2_score=round(r2_score(y_test,KNC_model_y_pred)*100,2)\nprint(\"R2 Score for predicted value: \",KNC_model_r2_score)\nprint(\"Accuracy on Training set: \",round(KNC_model.score(X_train_final,y_train)*100,2))\nKNC_model_score_mannual = round(KNC_model.score(X_test_final,y_test)*100,2)\nprint(\"Accuracy on Testing set: \",KNC_model_score_mannual)\nevaluation(y_test,KNC_model_y_pred)","67d7fd09":"from sklearn.model_selection import GridSearchCV\nknn_param_grid={'n_neighbors': np.arange(1,31,1),\n          'leaf_size': np.arange(1,31,1)}\n\nknn_gs_model=GridSearchCV(KNeighborsClassifier(),param_grid=knn_param_grid,cv=3,verbose=True)\n\nknn_gs_model.fit(X_train_final, y_train)","94780f52":"# Fit the best parameter from the grid search\nknn_gs_model.best_params_","4833700a":"# Print the best result\nbest_results = pd.DataFrame(knn_gs_model.best_params_, index=[0])\nbest_results","11638fba":"# Train and testing after GridsearchCV\nnp.random.seed(42)\nfrom sklearn.neighbors import KNeighborsClassifier\nKNC_model=  KNeighborsClassifier(n_neighbors=best_results.n_neighbors[0], leaf_size = best_results.leaf_size[0] )\nKNC_model.fit(X_train_final,y_train)\nKNC_model_y_pred=KNC_model.predict(X_test_final)\nKNC_model_r2_score=round(r2_score(y_test,KNC_model_y_pred)*100,2)\nprint(\"R2 Score for predicted value: \",KNC_model_r2_score)\nprint(\"Accuracy on Training set: \",round(KNC_model.score(X_train_final,y_train)*100,2))\nKNC_model_score_gs = round(KNC_model.score(X_test_final,y_test)*100,2)\nprint(\"Accuracy on Testing set: \",KNC_model_score_gs)\nevaluation(y_test,KNC_model_y_pred)","ac437d1e":"# Picking the best between the manual and gridsearchcv optimization\nKNC_model_score2 =max(KNC_model_score_gs, KNC_model_score_mannual)\nKNC_model_score2","0fef7c2b":"# Find the best N estimator for manual fine tunning of Random Forest\nnp.random.seed(42)\nrf_count=1\nscore_final = 0\nrfc_model = RandomForestClassifier()\n#checked once till 50, still found the best estimator at 7, so decresed the range to 10 for future runs\nfor i in range(1,10,1):\n    #print(f\"With {i} estimators:\")\n    rfc_model=rfc_model.set_params(n_estimators=i*10,max_depth=i,random_state=i)\n    score = round(rfc_model.fit(X_train_final, y_train).score(X_test_final,y_test)*100,2)\n    if score > score_final:\n        score_final = score\n        rf_count = i*10\n    #print(f\"Accuracy: {clf2.score(X_test_final,y_test)*100:2f}%\")\nprint('Best n_estimators is: ' + str(rf_count) + ', with accuracy score of ' + str(score_final) +'%')","690ce2c9":"# Train and Test with the best n estimator\nnp.random.seed(42)\nRFC_model=RandomForestClassifier(n_estimators=rf_count, max_depth=(rf_count\/10),random_state=int(rf_count\/10))\nRFC_model.fit(X_train_final,y_train)\nRFC_model_y_pred=RFC_model.predict(X_test_final)\nRFC_model_r2_score=round(r2_score(y_test,RFC_model_y_pred)*100,2)\nprint(\"R2 Score for predicted value: \",RFC_model_r2_score)\nprint(\"Accuracy on Training set: \",round(RFC_model.score(X_train_final,y_train)*100,2))\nRFC_model_score2 = round(RFC_model.score(X_test_final,y_test)*100,2)\nprint(\"Accuracy on Testing set: \",RFC_model_score2)\nevaluation(y_test,RFC_model_y_pred)","1a234a50":"# Fitting 5 folds for each of 10 candidates, totalling 50 fits to Find the best parameters\nfrom sklearn.model_selection import RandomizedSearchCV\nlearning_rate = [0.01, 0.1]\nmax_depth = [int(x) for x in np.linspace(5, 40, num = 6)]\nmin_child_weight = [int(x) for x in np.linspace(1, 20, num = 6)]\nsubsample =  [0.5, 0.7]\ncolsample_bytree = [0.5, 0.7]\nobjective = ['reg:squarederror']\nn_estimators = [int(x) for x in np.linspace(start = 20, stop = 2000, num = 40)]\ngamma = [0.6, 0.7]\nseed = [27]\nreg_lambda = [2]\nbooster = ['dart']\ncolsample_bylevel = [0.6]\ncolsample_bynode = [0.5]\nrandom_grid_param = {'learning_rate': learning_rate,\n               'max_depth': max_depth,\n               'min_child_weight': min_child_weight,\n               'subsample': subsample,\n               'colsample_bytree': colsample_bytree,\n               'objective': objective,\n               'n_estimators': n_estimators,\n               'gamma': gamma,\n               'seed' : seed,\n               'reg_lambda' : reg_lambda,\n               'booster' : booster,\n               'colsample_bylevel' : colsample_bylevel,\n               'colsample_bynode' : colsample_bynode}\n\n\nXGB_rg_model = RandomizedSearchCV(estimator = XGB_model, param_distributions = random_grid_param,scoring='neg_mean_squared_error',\n                               n_iter = 10, cv = 5, verbose=2, random_state=42, n_jobs = 1)\n\nXGB_rg_model.fit(X_train_final,y_train)","004ba739":"# Revealing the best parameters for the optimized extreme gradient boost\nXGB_rg_model.best_params_","1b6a4aff":"# creating dataframe for the best parameters\nbest_results_XGB = pd.DataFrame(XGB_rg_model.best_params_, index=[0])\nbest_results_XGB","b62a4d51":"# Train and test with the parameters\nXGB_model=XGBClassifier(subsample= best_results_XGB.subsample[0], \n                        seed= best_results_XGB.seed[0], \n                        reg_lambda= best_results_XGB.reg_lambda[0],\n                        objective= best_results_XGB.objective[0],\n                        n_estimators= best_results_XGB.n_estimators[0],\n                        min_child_weight= best_results_XGB.min_child_weight[0],\n                        max_depth= best_results_XGB.max_depth[0],\n                        learning_rate= best_results_XGB.learning_rate[0],\n                        gamma= best_results_XGB.gamma[0],\n                        colsample_bytree= best_results_XGB.colsample_bytree[0],\n                        colsample_bynode= best_results_XGB.colsample_bynode[0],\n                        colsample_bylevel= best_results_XGB.colsample_bylevel[0],\n                        booster= best_results_XGB.booster[0])\nXGB_model.fit(X_train_final,y_train)\nXGB_model_y_pred=XGB_model.predict(X_test_final)\nXGB_model_r2_score=round(r2_score(y_test,XGB_model_y_pred)*100,2)\nprint(\"R2 Score for predicted value: \",XGB_model_r2_score)\nprint(\"Accuracy on Training set: \",round(XGB_model.score(X_train_final,y_train)*100,2))\nXGB_model_score2 = round(XGB_model.score(X_test_final,y_test)*100,2)\nprint(\"Accuracy on Testing set: \",XGB_model_score2)\nevaluation(y_test,XGB_model_y_pred)","44c56c8a":"# defining parameter range \nparam_grid = {'C': [0.1, 1,2, 10, 100, 1000],  \n              'gamma': [1, 0.1, 0.01, 0.001, 0.0001], \n              'kernel': ['rbf','linear']}  \n  \nsvc_gs_model = GridSearchCV(SVC(), param_grid,cv=5, refit = True, verbose = 3) \n  \n# fitting the model for grid search \nsvc_gs_model.fit(X_train_final, y_train)","81fd6c00":"# print the tunning parameters and accuracy score\nprint(svc_gs_model.best_params_)\nprint(f\"Accuracy score:{svc_gs_model.score(X_test_final,y_test)}%\")","32b04584":"# Create dataframe for the parameter above for fitting prediction\nbest_results_SVC = pd.DataFrame(svc_gs_model.best_params_, index=[0])\nbest_results_SVC","7f32de23":"# Train and test the new prediction after tunning SVC by supplying the parameters\nnp.random.seed(42)\nSVC_model=SVC(C= best_results_SVC.C[0], gamma= best_results_SVC.gamma[0], kernel= best_results_SVC.kernel[0])\nSVC_model.fit(X_train_final,y_train)\nSVC_model_y_pred=SVC_model.predict(X_test_final)\nSVC_model_r2_score=round(r2_score(y_test,SVC_model_y_pred)*100,2)\nprint(\"R2 Score for predicted value: \",SVC_model_r2_score)\nprint(\"Accuracy on Training set: \",round(SVC_model.score(X_train_final,y_train)*100,2))\nSVC_model_score2 = round(SVC_model.score(X_test_final,y_test)*100,2)\nprint(\"Accuracy on Testing set: \",SVC_model_score2)\nevaluation(y_test,SVC_model_y_pred)","51b153f8":"# dataframe with accuracy score before tunning\nsummary_score","86de65f5":"# creating dataframe with accuracy score after tunning\nall_model_score_tunned = pd.DataFrame({'Applied_Model': ['Logistic Regression sm_tunned', 'Random Forest sm_Tunned', 'K-Nearest Neighbour sm_Tunned', \n                                                      'Support Vector Machine sm_Tunned',\"Extreme Gradient Boost sm_Tunned\"], \n                                'Accuracy_Score': [LR_model_score, RFC_model_score2, KNC_model_score2, SVC_model_score2, XGB_model_score2]})\nall_model_score_tunned = all_model_score_tunned.sort_values(by=['Accuracy_Score'], ascending=False)\nall_model_score_tunned","310977b9":"# Concatinating both the before and after tunning model accuracy\nall_model_score_final = pd.concat([all_model_score_tunned, summary_score],0)\nall_model_score_final = all_model_score_final.sort_values(by=['Accuracy_Score'], ascending=False)\nall_model_score_final","7d8700b0":"# Removing duplicate value of Logistic Regression\nall_model_score_final = all_model_score_final.drop_duplicates()\nall_model_score_final","a774aab6":"# Define some dictionaries\naccuracies_tomek = dict()\nrecall_tomek = dict()","ac324a3c":"# Split the dataset into new training and testing data\nX_train3, X_test3, y_train3, y_test3 = train_test_split(X,Y, random_state=22, test_size=0.2, shuffle=True)","bfb5e11d":"# Target count Before applying SMOTE Tomek\n\none_count = 0\nzero_count = 0\n\nfor i in y_train3:\n    if i==1:\n        one_count +=1\n    else:\n        zero_count +=1\n        \nprint('Number of one after applying SMOTE Tomek is: ', one_count)\nprint('Number of zero after applying SMOTE Tomek is: ', zero_count)","cacc01f3":"# Import and fit SMote tomek\nfrom imblearn.combine import SMOTETomek\ncombine = SMOTETomek()\n\nX_train3, y_train3 = combine.fit_resample(X_train3, y_train3)","4a274432":"# Target count After applying SMOTE Tomek\n\none_count = 0\nzero_count = 0\n\nfor i in y_train3:\n    if i==1:\n        one_count +=1\n    else:\n        zero_count +=1\n        \nprint('Number of one after applying SMOTE Tomek is: ', one_count)\nprint('Number of zero after applying SMOTE Tomek is: ', zero_count)","26aae998":"# plot to show target after SMOTE Tomek\nfig = ex.pie(y_train3,names='target')\nfig.update_layout(title='<b>Stroke Proportion After SmoteTOTEK<b>')\nfig.show()","c83edb65":"# Get Mutual Information \nfrom sklearn.feature_selection import mutual_info_regression\nmutual_info_3 = mutual_info_regression(X_train3, y_train3)\nmutual_info_3 = pd.Series(mutual_info_3)\nmutual_info_3.index = X_train.columns\nmutual_info_3 = mutual_info_3.sort_values(ascending=False)\nmutual_info_3","25a00d61":"# Considering the columns for training the model which are more than 0% of information shared with dependent variable\/feature\nReq_Columns_3 = list(mutual_info_3[mutual_info_3 > 0].index)\nReq_Columns_3","3aec1815":"# Create the final Train data and check first five rows\nX_train_final_3 = X_train3[Req_Columns_3]\nX_train_final_3.head(5)","eb6b773c":"# Create the final Test data and check first 3 rows\nX_test_final_3 = X_test3[Req_Columns_3]\nX_test_final_3.head(3)","90cd53d3":"# Define the evaluation metrics\ndef evaluation3(Y_test3,y_pred3):\n    acc=accuracy_score(y_test3,y_pred3)\n    rcl=recall_score(y_test3,y_pred3)\n    f1=f1_score(y_test3,y_pred3)\n    auc_score=roc_auc_score(y_test3,y_pred3)\n    prec_score=precision_score(y_test3,y_pred3)\n    \n    metric_dict={'accuracy': round(acc*100,2),\n               'recall': round(rcl*100,2),\n               'F1 score': round(f1*100,2),\n               'auc score': round(auc_score*100,2),\n               'precision': round(prec_score*100,2)\n                }\n    \n    return print(metric_dict)","05ae8d71":"# Training KNN model\nnp.random.seed(42) # Set the pseudo random number\nKNC_model_3=  KNeighborsClassifier() # Define the classifier\nKNC_model_3.fit(X_train_final_3,y_train3) # Fit the model\nKNC_model_y_pred_3=KNC_model_3.predict(X_test_final_3) # make prediction on testing data\nKNC_model_r2_score_3=round(r2_score(y_test3,KNC_model_y_pred_3)*100,2)\n\n# Print the scores and the evaluation\nprint(\"R2 Score for predicted value: \",KNC_model_r2_score)\nprint(\"Accuracy on Training set: \",round(KNC_model_3.score(X_train_final_3,y_train3)*100,2))\nKNC_model_score_3 = round(KNC_model_3.score(X_test_final_3,y_test3)*100,2)\nprint(\"Accuracy on Testing set: \",KNC_model_score_3)\nevaluation3(y_test3,KNC_model_y_pred_3)","19ce5ae6":"from sklearn.model_selection import GridSearchCV\nparameters = {'n_neighbors': np.arange(1, 10)}\ngrid_search = GridSearchCV(estimator = KNC_model_3, param_grid = parameters, scoring = 'accuracy', cv = 10, n_jobs = -1)\ngrid_search = grid_search.fit(X_train_final_3, y_train3)\nbest_accuracy = grid_search.best_score_\nbest_parameters = grid_search.best_params_\n\nprint(\"Best Score: \", best_accuracy)\nprint(\"Best Params: \", best_parameters)","1e34ad7e":"# plot the confusion matrix\nfig,ax=plt.subplots()\nax=sns.heatmap(confusion_matrix(y_test3,KNC_model_y_pred_3),annot=True,cbar=True)","3cb7f4d5":"# Training Logistics model \nnp.random.seed(42) # Set the pseudo random number\nLR_model_3 = LogisticRegression() # Define the classifier\n\n# Fit the model for prediction\nLR_model_3.fit(X_train_final_3,y_train3)\n\n# make prediction on testing data   \nLR_model_y_pred_3 = LR_model_3.predict(X_test_final_3) \nLR_model_r2_score_3=round(r2_score(y_test3,KNC_model_y_pred_3)*100,2)\n\n# Print the scores and the evaluation\nprint(\"R2 Score for predicted value: \",LR_model_r2_score_3)\nprint(\"Accuracy on Training set: \",round(LR_model_3.score(X_train_final_3,y_train3)*100,2))\nLR_model_score_3 = round(LR_model_3.score(X_test_final_3,y_test3)*100,2)\nprint(\"Accuracy on Testing set: \",LR_model_score_3)\nevaluation3(y_test3,LR_model_y_pred_3)","9014bf9c":"# plot the confusion matrix\nfig,ax=plt.subplots()\nax=sns.heatmap(confusion_matrix(y_test3,LR_model_y_pred_3),annot=True,cbar=True)","f2170f43":"# Training Random Forest model \nnp.random.seed(42) # Set the pseudo random number\nRFC_model_3 = RandomForestClassifier() # Define the classifier\n\n# Fit the model for prediction\nRFC_model_3.fit(X_train_final_3,y_train3)\n\n# make prediction on testing data   \nRFC_model_y_pred_3 = RFC_model_3.predict(X_test_final_3) \nRFC_model_r2_score_3=round(r2_score(y_test3,RFC_model_y_pred_3)*100,2)\n\n# Print the scores and the evaluation\nprint(\"R2 Score for predicted value: \",RFC_model_r2_score_3)\nprint(\"Accuracy on Training set: \",round(RFC_model_3.score(X_train_final_3,y_train3)*100,3))\nRFC_model_score_3 = round(RFC_model_3.score(X_test_final_3,y_test3)*100,2)\nprint(\"Accuracy on Testing set: \",RFC_model_score_3)\nevaluation3(y_test3,RFC_model_y_pred_3)","ae36ee1e":"# plot the confusion matrix\nfig,ax=plt.subplots()\nax=sns.heatmap(confusion_matrix(y_test3,RFC_model_y_pred_3),annot=True,cbar=True)","bb9be9ee":"# Training SVM model \nnp.random.seed(42) # Set the pseudo random number\nSVC_model_3=SVC() # Define the classifier\n\n# Fit the model for prediction\nSVC_model_3.fit(X_train_final_3,y_train3)\n\n# make prediction on testing data   \nSVC_model_y_pred_3 = SVC_model_3.predict(X_test_final_3) \nSVC_model_r2_score_3=round(r2_score(y_test3,SVC_model_y_pred_3)*100,2)\n\n# Print the scores and the evaluation\nprint(\"R2 Score for predicted value: \",SVC_model_r2_score_3)\nprint(\"Accuracy on Training set: \",round(SVC_model_3.score(X_train_final_3,y_train3)*100,2))\nSVC_model_score_3 = round(SVC_model_3.score(X_test_final_3,y_test3)*100,2)\nprint(\"Accuracy on Testing set: \",SVC_model_score_3)\nevaluation3(y_test3,SVC_model_y_pred_3)","392534d9":"# plot the confusion matrix\nfig,ax=plt.subplots()\nax=sns.heatmap(confusion_matrix(y_test3,SVC_model_y_pred_3),annot=True,cbar=True)","4a4422a7":"XGB_model_3=XGBClassifier()\nXGB_model_3.fit(X_train_final_3,y_train3)\nXGB_model_y_pred_3=XGB_model_3.predict(X_test_final_3)\nXGB_model_r2_score_3=round(r2_score(y_test,XGB_model_y_pred)*100,2)\nprint(\"R2 Score for predicted value: \",XGB_model_r2_score_3)\nprint(\"Accuracy on Training set: \",round(XGB_model_3.score(X_train_final_3,y_train3)*100,2))\nXGB_model_score_3 = round(XGB_model_3.score(X_test_final_3,y_test3)*100,2)\nprint(\"Accuracy on Testing set: \",XGB_model_score_3)\nevaluation3(y_test3,XGB_model_y_pred_3)","301e8433":"# plot the confusion matrix\nfig,ax=plt.subplots()\nax=sns.heatmap(confusion_matrix(y_test3,XGB_model_y_pred_3),annot=True,cbar=True)","92af8d77":"\n# Define the parameters\nmlp_3 = MLPClassifier(hidden_layer_sizes=(300,300,300), max_iter=1000, alpha=0.00001,\n                     solver='adam', verbose=10,  random_state=21)\n\n# Fit the models\nmlp_3.fit(X_train_final_3, y_train3)\nmlp_pred_3= mlp_3.predict(X_test_final_3)\n\nmlp_score_3 = round(mlp_3.score(X_train_final_3, y_train3)*100,2)\nmlp_test_3 = mlp_3.score(X_test_final_3, y_test3)\nmlp_3_r2_score_1=round(r2_score(y_test3,mlp_pred_3)*100,2)\n\n# Make predictions\ny_pred_3 =mlp_3.predict(X_test_final_3)\n\n#evaluation\ncm = confusion_matrix(y_test3,y_pred_3)\nprint(\"R2 Score for predicted value: \",mlp_3_r2_score_1)\nprint('Training Score',mlp_score_3)\nprint('Testing Score \\n',mlp_test_3)\nprint(cm)\nevaluation3(y_test3,mlp_pred_3)","b5c11f61":"# plot the confusion matrix\nfig,ax=plt.subplots()\nax=sns.heatmap(confusion_matrix(y_test3,mlp_pred_3),annot=True,cbar=True)","9e0949b2":"summary_score3 = pd.DataFrame({'Applied_Model': ['Logistic Regression Tomek', 'Random Forest Tomek', 'K-Nearest Neighbour Tomek', 'Support Vector Machine Tomek', \"Extreme Gradient Boost Tomek\", 'Multilayer Perceptron Tomek'], \n                                'Accuracy_Score': [LR_model_score_3, RFC_model_score_3, KNC_model_score_3, SVC_model_score_3, XGB_model_score_3,mlp_test_3*100]})\nsummary_score3 = summary_score3.sort_values(by=['Accuracy_Score'], ascending=False)\nsummary_score3","bbeebd9e":"# A plot to compare the accuracy of the models applied\nf, ax = plt.subplots(figsize=(8, 6))\nsns.set_color_codes('pastel')\nsns.barplot(y ='Applied_Model', x ='Accuracy_Score', data = summary_score1, color='gold', edgecolor='black')\nplt.title('Accuracy Score Before Using Any Imbalance Data Handling Technique', fontsize=18)\nplt.show()","3fec2789":"# A plot to compare the accuracy of the models applied\nf, ax = plt.subplots(figsize=(8, 6))\nsns.set_color_codes('pastel')\nsns.barplot(y ='Applied_Model', x ='Accuracy_Score', data = summary_score2, color='Red', edgecolor='black')\nplt.title('Accuracy Score After using Random Undersampling', fontsize=18)\nplt.show()","1e178d9e":"# A plot to compare the accuracy of the models applied\nf, ax = plt.subplots(figsize=(8, 6))\nsns.set_color_codes('pastel')\nsns.barplot(y ='Applied_Model', x ='Accuracy_Score', data = summary_score, color='Green', edgecolor='black')\nplt.title('Accuracy Score After using Smote', fontsize=18)\nplt.show()","6ec74698":"# A plot to compare the accuracy of the models applied\nf, ax = plt.subplots(figsize=(8, 6))\nsns.set_color_codes('pastel')\nsns.barplot(y ='Applied_Model', x ='Accuracy_Score', data = summary_score3, color='purple', edgecolor='black')\nplt.title('Accuracy Score After using SmoteTomek', fontsize=18)\nplt.show()","c3b7a768":"# Concatinating all model accuracy\nresults = pd.concat([all_model_score_final,summary_score1,summary_score2,summary_score3],0)","e4de6f46":"# Arrange Models in descending order\nresults.sort_values(by='Accuracy_Score', ascending=False)","078589a4":"# Plot evaluation metrics for  best performing Xgboost\nxgb_final_metrics ={'Accuracy': XGB_model.score(X_test_final,y_test),\n                     'Precision': precision_score(y_test,XGB_model_y_pred),\n                     'Recall': recall_score(y_test,XGB_model_y_pred),\n                     'F1': f1_score(y_test,XGB_model_y_pred),\n                     'AUC': roc_auc_score(y_test,XGB_model_y_pred)}\n\nxgb_metrics=pd.DataFrame(xgb_final_metrics,index=[0])\nxgb_metrics.T.plot.bar(title='XG boost Metric Evaluation', color = '#0a417a', legend=False);","429f5516":"# Find the Area under curve\ny_pred_xgb_proba2 = XGB_model_3.predict_proba(X_test_final_3)[::,1]\nfpr_xgb2, tpr_xgb2, _ = metrics.roc_curve(y_test3,  y_pred_xgb_proba2)\nauc_xgb2 = metrics.roc_auc_score(y_test3, y_pred_xgb_proba2)\nprint(\"AUC XGB_Tomek :\", auc_xgb2)","195b381f":"# plot the ROC curve\nplt.plot(fpr_xgb2,tpr_xgb2,label=\"XGB, auc={:.3f})\".format(auc_xgb2))\nplt.plot([0, 1], [0, 1], 'k--')\nplt.xlabel('False positive rate')\nplt.ylabel('True positive rate')\nplt.title('Extreme Gradient Boost for Smote Tomek ROC curve')\nplt.legend(loc=4)\nplt.show()","988a41de":"# Using the SMOTE data, Check the shape of the data\nX_train_final.shape","9725823e":"# Build the layers and train the model\nmodel = Sequential()\nmodel.add(Dense(32, input_shape=(15,), activation='relu')),\nmodel.add(Dropout(0.2)),\nmodel.add(Dense(16, activation='relu')),\nmodel.add(Dropout(0.2)),\nmodel.add(Dense(8, activation='relu')),\nmodel.add(Dropout(0.2)),\nmodel.add(Dense(4, activation='relu')),\nmodel.add(Dropout(0.2)),\nmodel.add(Dense(1, activation='sigmoid'))","027b4bfc":"# Compile the Model\nopt = tf.keras.optimizers.Adam(learning_rate=0.001) #optimizer\n\nmodel.compile(optimizer=opt, loss=tf.keras.losses.BinaryCrossentropy(), metrics=['accuracy']) #metrics","53f0fb42":"# Set early stopper \nearlystopper = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', min_delta=0, patience=15, verbose=1,mode='auto', baseline=None, restore_best_weights=False)","8dec81d5":"# Fit the Model\nhistory = model.fit(X_train_final.values, y_train.values, epochs = 6, batch_size=5, validation_split = 0.15, verbose = 0,\n                    callbacks = [earlystopper])\nhistory_dict = history.history","e976ec6e":"# Plot the training loss and the validation training loss\nloss_values = history_dict['loss']\nval_loss_values=history_dict['val_loss']\nplt.plot(loss_values,'b',label='training loss')\nplt.plot(val_loss_values,'r',label='val training loss')\nplt.legend()\nplt.xlabel(\"Epochs\")","33f13853":"# Plot the accuracy and the validation Accuracy\naccuracy_values = history_dict['accuracy']\nval_accuracy_values=history_dict['val_accuracy']\nplt.plot(val_accuracy_values,'-r',label='val_accuracy')\nplt.plot(accuracy_values,'-b',label='accuracy')\nplt.legend()\nplt.xlabel(\"Epochs\")","48e5c802":"# Make predictions on the test set\ny_pred_nn = model.predict_classes(X_test_final)","9ae1c38e":"# Print the prediction scores\nprint(\"Accuracy Neural Net:\",metrics.accuracy_score(y_test, y_pred_nn))\nprint(\"Precision Neural Net:\",metrics.precision_score(y_test, y_pred_nn))\nprint(\"Recall Neural Net:\",metrics.recall_score(y_test, y_pred_nn))\nprint(\"F1 Score Neural Net:\",metrics.f1_score(y_test, y_pred_nn))\n","a31922d3":"# Plot the Confusion Matrix\nmatrix_nn = confusion_matrix(y_test, y_pred_nn)\ncm_nn = pd.DataFrame(matrix_nn, index=['no_stroke', 'stroke'], columns=['no_stroke', 'stroke'])\n\nsns.heatmap(cm_nn, annot=True, cbar=None, cmap=\"Blues\", fmt = 'g')\nplt.title(\"Confusion Matrix Neural Network\"), plt.tight_layout()\nplt.ylabel(\"True Class\"), plt.xlabel(\"Predicted Class\")\nplt.show()","994f5af0":"# Calculate the AUC Score\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import auc\ny_pred_nn_proba = model.predict_proba(X_test_final)\nfpr_keras, tpr_keras, thresholds_keras = roc_curve(y_test,y_pred_nn_proba)\nauc_keras = auc(fpr_keras, tpr_keras)\nprint('AUC Neural Net: ', auc_keras)","3298fa8b":"# Plot the ROC\nplt.figure(1)\nplt.plot([0, 1], [0, 1], 'k--')\nplt.plot(fpr_keras, tpr_keras, label='Keras (area = {:.3f})'.format(auc_keras))\nplt.xlabel('False positive rate')\nplt.ylabel('True positive rate')\nplt.title('Neural Net ROC curve')\nplt.legend(loc='best')\nplt.show()","47f7af71":"nn_precision, nn_recall, _ = precision_recall_curve(y_test, y_pred_nn_proba)\nno_skill = len(y_test[y_test==1]) \/ len(y_test)\nplt.plot([0, 1], [no_skill, no_skill], linestyle='--', color='black', label='No Skill')\nplt.plot(nn_recall, nn_precision, color='orange', label='Keras NN')\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.title('Precision-Recall curve')\nplt.legend()\nplt.show()","46cc1ad5":"Yikes. Only around 100 of patients are classified as stroke patients. This is a great way to make high-accuracy low-recall models and further inaccurate predictions.\nTherefore, I can\u2019t pass this type of dataset to a machine learning algorithm. ","0456555c":"### <h2 style=\"text-align: left; font-family: 'Garamond'; font-size:20px; color:purple\">Evaluation After Tunning<\/h2>","f4cbbe2e":"# <center> How XGBoost works :<\/center>\n\n![](https:\/\/d1rwhvwstyk9gu.cloudfront.net\/2020\/02\/XG-Boost-FINAL-01.png)\n\nThe sequential ensemble methods, also known as \u201cboosting\u201d, creates a sequence of models that attempt to correct the mistakes of the models before them in the sequence. The first model is built on training data, the second model improves the first model, the third model improves the second, and so on. \nXGBoost provides an efficient and effective implementation of the gradient boosting algorithm","1dbe1b4e":"From above, XGB used with SMOTE achieved a feat accuracy of 95.8%","5ba1efc9":"- The Xtreme Gradient Boost has the best performance when used with SMOTE according to my four most important classification metrics which are Accuracy, R2 score, F1-score and AUC. XGboost also performed very well.\n\n- The KNN from sklearn is the one that minimizes the most the false negatives.  so I decided to keep this model to predict Stroke. It's very important that a physician do not miss strokes so minimizing false negatives rate is very essential. \n","722c771b":"### <font color=CornflowerBlue>3.1 Univariate Visualization:<font>","d54a17bb":"##### <h4 style=\"text-align: left; font-family: 'Garamond'; font-size:20px; color:green\"> Investigating the best performing AUC (Random Forest) <\/h4> ","15a75ba5":"\n\nThe last step before modelling is to split the data into train and test samples. The test set will be composed of 20% of the data.\n\nI will use the train dataset to train the models and then evaluate them of the test set : \n<center><img src= \"https:\/\/data-flair.training\/blogs\/wp-content\/uploads\/sites\/2\/2018\/08\/1-16.png\">\n\nTo split the data, I will use train_test_split function from sklearn","9fa4c8e6":"#   <font color=RoyalBlue>5. Modelling and Feature Importance <font>","f898fcd0":"Highly negative R2 and poor training accuracy value shows that model does not fit for the dataset. Therefore, there is no need for hyperparameter","be104834":"#### <h3 style=\"text-align: left; font-family: 'Garamond'; font-size:20px; color:teal\">Manual Fine Tunning of KNN <\/h3>","dceec909":"   ![Stroke-1024x463.jpeg](attachment:Stroke-1024x463.jpeg)\n","9a030b85":"## <font color=Brown >5.4 Modelling Using SMOTETomek Data :<font>","87535d0b":"#### <h4 style=\"text-align: left; font-family: 'Garamond'; font-size:20px; color:green\"> III. Applying Random Forest on Oversampled Data <\/h4>  ","0dbc29b8":"### <font color=CornflowerBlue>5.1.1 Splitting the data into Train and Test :<font>","d70d2b75":"Only XGboost and KNN fit the data perfectly. All others are pooly fit. Let us check MLP algorithms","e6d5b775":"### <font color=CornflowerBlue>4.3 Scaling the Data :<font>","4939a6b4":"#### <h4 style=\"text-align: left; font-family: 'Garamond'; font-size:20px; color:green\"> V. Applying Extreme Gradient Boost on unbalanced Data <\/h4> ","2229c7c1":"# <h2 style=\"text-align: left; font-family: 'Garamond'; font-size:20px; color:green\">4. Applying Support Vector Machine after SmoteTotek <\/h2> ","062c7f4d":"# <h3 style=\"text-align: left; font-family: 'Garamond'; font-size:20px; color:Brown\"> Explore Feature Importance using Logistic Regression<\/h3> ","ad3161a8":"- Instead of filling the missing data with the mean or median or mode which might slightly affect the prediction, \n- I will use a simple Decision tree to plot get a more accurate randomized values,\n- I will be using 3 features which are average glucose level, gender and the BMI whose datas are missing.","2a800562":" # <h2 style=\"text-align: left; font-family: 'Garamond'; font-size:30px; color:Purple\">Handling Imbalanced Dataset<\/h2>","45dca43c":"\n\nSMOTE (Synthetic Minority Oversampling technique) is a method that generates synthetic samples from the minority class. It is used to obtain a synthetically class-balanced or nearly class-balanced training set, which is then used to train the classifier.","ea7bb64a":"## <font color=Brown >5.3 Modelling Using SMOTE Data :<font>","c5779843":"Highly negative R2 but higher training accuracy value shows that model does not fit for the dataset. Therefore, there is no need for hyperparameter tuning","f819bac5":"### <font color=CornflowerBlue>5.1.2 Collect Mutual Information :<font>","949f8904":"![sphx_glr_plot_smote_tomek_001.png](attachment:sphx_glr_plot_smote_tomek_001.png)\nThis method shown in the diagram above combines the SMOTE ability to generate synthetic data for minority class and Tomek Links ability to remove the data that are identified as Tomek links from the majority class (that is, samples of data from the majority class that is closest with the minority class data)","b9882fe5":"From Above, the size of the data is 479.2kb.\nThe dataset is made up of datatypes: floating point, Integer and object\nWe can see that BMI has some missing values. Then something need to be done about this later.","9aba728f":"The k-nearest neighbors (KNN) algorithm is a basic supervised machine learning algorithm that can be used to solve both classification and regression problems. KNN works by exploiting the distances between a features and all the examples in the data, and select the specified number examples (K) closest to the feature, then votes for the most frequent label (in the case of classification) or averages the labels (in the case of regression).","389edc5f":"## <font color=Purple>5.1.4 Extraction from the Algorithm Implementation :<font>","0972d7cd":"### <font color=CornflowerBlue>2.3 Handling Missing Data:<font>","1d7badb4":"#### <h4 style=\"text-align: left; font-family: 'Garamond'; font-size:20px; color:green\"> II. Applying Logistics Regression on Random Undersampled Data<\/h4> ","56bceae4":"# <h2 style=\"text-align: left; font-family: 'Garamond'; font-size:20px; color:green\">6. Applying MLP after SmoteTotek <\/h2> ","c7856121":"The fundamental idea behind a random forest is to combine the predictions made by many decision trees into a single model. Random forests or random decision forests are an ensemble learning method for classification, regression and other tasks that operates by constructing a multitude of decision trees at training time and outputting the class that is the mode of the classes or mean prediction of the individual trees","58e0905f":"Since we couldn't properly generate the best features for the training using correlation matrix, we will be using mutual information to generate the set of the best performing data for training. \n\nHence, the mutual information (MI) of any two random variables is the measure of the mutual dependence between the two variables. ![download.png](attachment:download.png)\n\nFrome the picture above, the violet area is the mutual information shared","8299de21":"When compared to how MLP performed on undersampled data, this is a great progress. ","06d335b6":"Either married or not, there are more people with lower average glucose leveland high Bmi than people with higher average glucose level and Low Bmi. ","b172b7d4":"#### <h4 style=\"text-align: left; font-family: 'Garamond'; font-size:20px; color:green\"> VI. Applying MLP on Oversampled Data <\/h4>   ","82c1229c":"From above, the coefficients ranges from app (0.23 to 7.37). Next step is to find the specific feature with the highest to lowest.","ce14d412":"#### <h4 style=\"text-align: left; font-family: 'Garamond'; font-size:20px; color:green\"> IV. Applying SVM on Oversampled Data <\/h4>   ","d8519d3d":"![GridVRandom.png](attachment:GridVRandom.png)\nhyperparameter optimization or tuning is the problem of choosing a set of optimal hyperparameters for a learning algorithm (Wikipaedia)\n\n- Grid search is arguably the most basic hyperparameter tuning method. With this technique, we simply build a model for each possible combination of all of the hyperparameter values provided, evaluating each model, and selecting the architecture which produces the best results.(Kdnudgets)\n\n\n","b05ebe06":"Only the 8 features above will be selected for training and testing the algorithms based on mutual information collected after persorming Random Undersampling","8b802550":"### <h2 style=\"text-align: left; font-family: 'Garamond'; font-size:25px; color:purple\">5. Fine Tunning SVM on Oversampled Data<\/h2> ","38a41e09":"### <h2 style=\"text-align: left; font-family: 'Garamond'; font-size:25px; color:purple\">2. Fine Tunning RandomForest on Oversampled Data<\/h2> ","a88771e0":"## <h1 style=\"text-align: left; font-family: 'Garamond'; font-size:25px; color:brown\"> 5.5 Fine Tunning other Models<\/h1>","61a338f8":"#### <h4 style=\"text-align: left; font-family: 'Garamond'; font-size:20px; color:green\"> VI. Applying Multilayer Perceptron on unbalanced Data <\/h4> ","955c21ce":"# <font color=RoyalBlue>2. Exploratory Data Analysis<font>","2d1b3d67":"From the shape above, we can see that we have 5110 rows (values) and 12 columns (features)","ee59456a":"## <font color=Brown>5.1 Modelling Using Raw Data :<font>","3858ed84":"#### <h4 style=\"text-align: left; font-family: 'Garamond'; font-size:20px; color:green\"> II. Applying LGR on Oversampled Data <\/h4>  ","07608401":"Highly negative R2 and lower training accuracy value shows that model does not fit for the dataset. Therefore, there is no need for hyperparameter optimization","b4d416c1":"# <font color=RoyalBlue>6. Conclusion<font>","399203cb":"With Manual Fine Tunning, KNN increased from 89.7 to 92.8%. That is awesome","6ce51ca8":"### <center> How Neural network works : <\/center>\n\n<center><img src= \"https:\/\/victorzhou.com\/27cf280166d7159c0465a58c68f99b39\/network3.svg\">\n\nThe h1, h2 and o1 which are the layers of the neural network are nodes. The function of a node is to pass the input from data and combine with set of coefficients and bias, that either amplify or dampen the input, then fix a  significance to  the inputsdepending on the type of algorithm the task is trying to learn. \n    \nThe input-weight products are summed and then passed to the activation function, to determine whether and to what extent that signal should progress further through the network to affect the ultimate outcome, which is the  classification mechanics. If the signals successfully passes through, the neuron has been \u201cactivated.\u201d\n    \nMLP is a deep ANN and supervised algorithm which is made up of more than one perceptron. It is a feedforward model that generates output from an input data by training the network with backpropagation.","f10e9a92":"#### <h4 style=\"text-align: left; font-family: 'Garamond'; font-size:20px; color:green\"> III. Applying Random Forest on Random Undersampled Data <\/h4> ","9d3b465c":"As shown above, Logistic Regression performed best of all the algorithms. Also, all the algorithms achieved a negative R2 score including our best performing MLP. Logit regression and SVM achieved the same accuracy. Random forest has the least accuracy.\n\n- Let's talk about the Negative R2 scores:\n\n Generally, \ud835\udc452  is computed as 1\u2212\ud835\udc46\ud835\udc46res divided by \ud835\udc46\ud835\udc46tot. (where the \ud835\udc46\ud835\udc46\ud835\udc5f\ud835\udc52\ud835\udc60 = residual error.) \n\nWhat \ud835\udc452 does is that it  basically compares the fit of the chosen model with that of a horizontal straight line (the null hypothesis). If the chosen model fits worse than a horizontal line, then \ud835\udc452 is negative\n\nThis means that most of the algorithms are fitting worse than the horizontal line. Which means the \ud835\udc46\ud835\udc46res is greater than the \ud835\udc46\ud835\udc46tot\n\n- Bottom line: \n\nThe negative \ud835\udc452 is not mathematically impossible. It just means that the chosen model (with its constraints) fits the data really poorly. \n\n- So what can I do?\n\nI will investigate the best AUC score  \n\nI will see if handling of Imbalance Data can solve the issue of poorly fit","573867d2":"### <font color=CornflowerBlue>4.2 Check for Correlation:<font>","077568cd":"#### <h4 style=\"text-align: left; font-family: 'Garamond'; font-size:20px; color:green\"> I. Applying K-Nearest Neighbour on unbalanced Data <\/h4> ","fa538d3c":"From the table above, we can see that the ranges between the values of the non-categorical features are wide. Then, Feature scaling has to be  performed. I will do this later*\n\nSince Hypertension and heart disease are in binary classification, they shouldn't be shown among the description above. So, I will convert it to an object data type.\n\nThis is also necessary to decide the necessity for feature scaling","aea12bd1":"### <font color=CornflowerBlue>3.3 Multivariate Visualization:<font>","91375992":"# <font color=RoyalBlue>7. Comparing our best Accuracy with Deep Neural Network<font>","baff7566":"Only the 8 features above are fit for training based on the mutual information greater than zero shared with the target","f6b5b946":"![Oversampling.jpeg](attachment:Oversampling.jpeg)","bc6286e8":"#### <h4 style=\"text-align: left; font-family: 'Garamond'; font-size:20px; color:green\"> II. Applying Logistics Regression on unbalanced Data <\/h4> ","ae91ca67":"#### <h4 style=\"text-align: left; font-family: 'Garamond'; font-size:20px; color:green\"> V. Applying MLP on Random Undersampled Data <\/h4> ","72d49fbf":"#### <h4 style=\"text-align: left; font-family: 'Garamond'; font-size:20px; color:green\"> I. Applying K-Nearest Neighbour on Random Undersampled Data <\/h4> ","340aee57":"# <h2 style=\"text-align: left; font-family: 'Garamond'; font-size:20px; color:green\">5. Applying Xgboost after SmoteTotek <\/h2> ","5383e7ee":"From above,\n- the target class before Smote is 0.9:0.1\n- After SMOTE, we have 0.5:0.5","7916a8c4":"#### <h4 style=\"text-align: left; font-family: 'Garamond'; font-size:20px; color:green\"> I. Applying KNN on Oversampled Data <\/h4> ","99b17cdf":"From the correlation matrix, we can see that none of the features arrive at a significant correlation with the target feature. Only age has the highest with 0.2. \nTherefore, this matrix is not enough to arrive at a conclusion of features that can help in predicting stroke.\nWe will implement another method ","6d645cf7":"### <font color=CornflowerBlue>3.2 Bivariate Visualization:<font>","a504b666":"# <h2 style=\"text-align: left; font-family: 'Garamond'; font-size:20px; color:green\">3. Applying Random Forest after SmoteTotek <\/h2> ","3a73e630":"### <font color=CornflowerBlue>Dataset:<font>\n\nIn this project, I am going to use the *healthcare-dataset-stroke-data.csv* dataset from kaggle. The dataset is publically available on the Kaggle website, and it is from an ongoing cardiovascular study.  The classification goal is to predict whether the patient has Stroke or not. The dataset provides the patients\u2019 information. It includes over 5,000 records and 12 attributes. The data contains 5110 observations with 12 attributes.\n\n The dataset contains 13 independent features and 1 target feature as described below.<br\/>\n> 1. age \n> 2. gender \n> 3. Hypertension (2 values) \n> 4. Heart Disease (2 Values)\n> 5. Ever Married (2 values)\n> 6. Work Type (4 values) \n> 7. Resident Type (values 0,1)\n> 8. Average Glucose Level \n> 9. Body Mass Index\n> 10. Smoking Status\n> 11. Stroke - is the binary target variable, 0 indicates that the patient has stroke, the value is 1 if not.   ","e4823297":"### <font color=RoyalBlue>6.2. How Does XGB from SMOTE performed best?<font>","6df33842":"\n\n### <font color=CornflowerBlue>2.2 Prepare Data:<font>\n\n","cef61e9a":"Logistic regression has an accuracy of 78.56% Accuracy of the logistic regression is poor compared to KNN","81eb128d":"### <center> How SVM works : <\/center>\n\n<center><img src= \"https:\/\/vitalflux.com\/wp-content\/uploads\/2020\/07\/Screenshot-2020-07-07-at-3.44.38-PM.png\">\n\nA support vector machine (SVM) is a supervised machine learning model that uses classification algorithms for two-group classification problems. The objective of the SVM algorithm is to find a hyperplane in an N-dimensional space(N \u2014 the number of features) that distinctly classifies the data points.","ef0736a6":"### <h2 style=\"text-align: left; font-family: 'Garamond'; font-size:25px; color:purple\">3. Fine Tunning XGboost on Oversampled Data using Randomized SearchCV<\/h2> ","77778ef8":"#### <h4 style=\"text-align: left; font-family: 'Garamond'; font-size:20px; color:green\"> IV. Applying Support Vector Machine on unbalanced Data <\/h4> ","6474cb67":"From above, gender has the highest coefficient while bmi has the lowest","5eb96dc4":"Accuracy didn't improve above 92.8%","43c5247f":"Classification metrics for Multi Layer Perceptron (rounded down) :\n- Accuracy : 96.0\n- AUC : 97.9\n- Standard deviation: +\/- 0.0045356875389769204\n\nI will Check If any of the Algorithms can be tuned to perform better than this accuracy","dbca33bf":"### <h2 style=\"text-align: left; font-family: 'Garamond'; font-size:25px; color:purple\">1. Fine Tunning KNN on Oversampled Data<\/h2> ","ceb289b8":"### <font color=CornflowerBlue>Objective:<font>\n\nA stroke is a serious life-threatening medical condition that happens when the blood supply to part of the brain is cut off. Strokes are a medical emergency and urgent treatment is essential. The sooner a person receives treatment for a stroke, the less damage is likely to happen.(NHS)\n\nLike all organs, the brain needs the oxygen and nutrients provided by blood to function properly. If the supply of blood is restricted or stopped, brain cells begin to die. This can lead to brain injury, disability and possibly death. (NHS). This classification model will predict whether the patient has heart disease or not based on various conditions\/symptoms of their body.\n\n\n","d5180e1a":"Deduction From the Pie chart above :\n\n1) We have more females than male\n2) only 9.7% of the population has hypertension. \n3) More than 60% of the population is married\n4) More than 50% of the population works in private sector\n5) Urban and rural distribution of residency is almost shared equally\n6) More than 30% of the population do not declare their smoking status but more people has never smoked","3a9093a5":"\nFrom above, we can see that the target is highly imbalance. This is a challenge because our classifiers will always be forced predict the most common class without performing any accurate prediction of the features and it will have a h. So what is the way forward? Random undersampling.  \n\nAlthough this method is simple and effective, the limitation of RU is that Some features are removed without any concern for their importance and their effect in determining the decision boundary between the classes. This means it is possible, or even likely, that useful information will be deleted.\n\n### <center>How undersampling works :<\/center>\n<center><img src= \"https:\/\/miro.medium.com\/max\/335\/1*YH_vPYQEDIW0JoUYMeLz_A.png\">\n\n\n\nTo perform random undersample, we can use the package imblearn with RandomUnderSampler function !","561f0bb8":"From above, Only \"Bmi\" has null value with 201 missing values","2850bec7":"Bmi seems like a very important features. So we must treat the missing values with care i.e., we can't drop it","0a8d601f":"## <font color=Purple> 5.2.4 Extraction from the Algorithm Implementation :<font>","886d9891":"###  <font color=RoyalBlue>6.1. Compare The Accuracies Across the Steps using Bar plot<font>","f9a2fb20":"#### <h4 style=\"text-align: left; font-family: 'Garamond'; font-size:20px; color:green\"> IV. Applying SVN on Random Undersampled Data <\/h4> ","bbd0f480":"# <font color=RoyalBlue>1. Introduction<font>\n","5d17d39c":"### <font color=CornflowerBlue>2.1 Import Library:<font>","f8758b36":"- Bottom line: \n\nThe negative \ud835\udc452 is still obvious after undersampling. It just means that the chosen model (with its constraints) fits the data really poorly even after handling the imbalance. \n\n- So what can I do?\n\nI will see if SMOTE can solve the issue of poorly fit","6f92a035":"With GridSearchCV, KNN algorithm increase to 95.6%. Impressive!","7eb8048c":"  With a positive r2 score, KNN is showing a prospect of having a good performance. let us try to perform some tuning using GridsearchCV","fb41133c":"### <font color=CornflowerBlue>5.1.3 Run the Algorithms :<font>","fd0f2a6c":"## <font color=Brown>5.2 Modelling Using Random Undersampled Data :<font>","c7a03825":"### <font color=CornflowerBlue>4.1 Change Data Type:<font>","b9586844":"# <h2 style=\"text-align: left; font-family: 'Garamond'; font-size:20px; color:green\">1. Applying KNeighborsClassifier (K-Nearest Neighbour) after SmoteTotek <\/h2> ","3ae33cb1":"Only the features above are useful for the algorthm","a9219d0c":"The Accuracy with  K Nearest Neighbor is 91.11%. This is a good performance but lower than the taining accuracy. Also note the positivity in the R2 score. Which means a good fitting","1f8f38e0":"#   <font color=RoyalBlue>3. Data Visualization <font>","e308ff2a":"The Random forest Algorithm perform better with an accuracy of 94.6%","a21d8607":"# <h2 style=\"text-align: left; font-family: 'Garamond'; font-size:20px; color:green\">2. Applying Logistics Regression after SmoteTotek <\/h2> ","9109e0e1":"It is important to see here that the gender is not only male and female. There are also some people who never disclosed their smoking status. ","09655634":"### <center> How Logistic regression works :\n\n<center><img src= \"https:\/\/saedsayad.com\/images\/LogReg_1.png\">\n\nIn Logit Regression, input variable (x) are combined linearly using weights or coefficient values to predict an output variable (y). \nThe key difference from linear regression is that the output value is a binary values (0 or 1) rather than a numeric value. Logistic regression is a linear method, but the predictions are transformed using the logistic function.\n    \nLogistic regression (LR) is a classification and regression analysis in statistics used for prediction of outcome of a categorical dependent variable from a set of independent variables. Therefore, it is a supervised learning algorithm.\nLogistic regression relies on probability of success using the logistic (sigmoid) function. The term regression is very simple and it means any process that attempts to find relationships between variables. It is logistic because it uses logistic function as a link function.","aaf50339":"In this section, I will be performing 4 major steps which are \n- Modelling Using Imbalanced Raw stroke Data\n- Modelling Using Balanced Stroke data (Random Undersampling)\n- Modelling Using Balanced Stroke date (SMOTE)\n- modelling Using Balanced Stroke Data (SMOTETomek)\nThis 4 Steps will be compared to pick the best algorithm that fit with the best Data","b813afae":"#### <h4 style=\"text-align: left; font-family: 'Garamond'; font-size:20px; color:green\"> III. Applying Random Forest Classifier on unbalanced Data <\/h4> ","e5393d31":"# <h3 style=\"text-align: left; font-family: 'Garamond'; font-size:20px; color:teal\">Hyperparameter tunning for  KNeighborsClassifier\/(K-Nearest Neighbour) using GridSearchCV<\/h3>","0945859c":"#   <font color=RoyalBlue>4. Feature Engineering <font>","889766a3":"From the numbers above, there is no significant correlation. Hence, I will have to generate important features with mutual information later","0541a80e":"#### <h4 style=\"text-align: left; font-family: 'Garamond'; font-size:20px; color:green\"> V. Applying Xgboost on Oversampled Data <\/h4>   ","c6ff55a4":"<div class=\"alert alert-info\">\n<h1><center><font color=darkblue> Exploring the Importance of Class Imbalance Handling Techniques on the prediction of Stroke .<font><\/center><\/h1>\n\n\n<\/div>","d88dd3a3":"From the bar plots above, SMote MLP Algorithm performed best","7c374d9d":"- Random Search sets up a grid of hyperparameter values and selects random combinations to train the model and score. This allows to explicitly control the number of parameter combinations that are attempted. The number of search iterations is set based on time or resources. Scikit Learn offers the RandomizedSearchCV function for this process.(Kdnudgets)","21bbf47c":"## <h2 style=\"text-align: left; font-family: 'Garamond'; font-size:25px; color:Brown\"> 5.4 Investigating why XGB performed better than others <\/h2> "}}