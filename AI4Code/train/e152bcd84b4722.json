{"cell_type":{"6b595246":"code","04d9e069":"code","0beb52b9":"code","56a3f023":"code","8fb5e2c8":"code","408d7848":"code","c60c8e91":"code","d0ca1ae1":"code","7c557e88":"code","ebb82208":"code","ba9e71c6":"code","87886c9e":"code","1238c47a":"code","89386b64":"code","cd12774c":"code","1ffded4c":"code","ad44c2ca":"code","0645b62c":"markdown","ed2fae79":"markdown","8fe07edc":"markdown","cb79e502":"markdown","3a2c2c1b":"markdown","90bbccd3":"markdown","5721fb87":"markdown","b3728e5f":"markdown","51c97dab":"markdown","13e95610":"markdown","73cf6510":"markdown","4887d741":"markdown","5bafb774":"markdown","0dc27b82":"markdown","3a35da31":"markdown","8fa34da8":"markdown","bbcb6c05":"markdown"},"source":{"6b595246":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.metrics import mean_squared_error, r2_score\nimport os\n\n\ndata = pd.read_csv(\"\/kaggle\/input\/predict-price-data\/imports-85-car.data\")\ncolumns = ['symboling', 'normalized-losses', 'make', 'fuel-type', 'aspiration', 'num-of-doors', 'body-style',\n        'drive-wheels', 'engine-location', 'wheel-base', 'length', 'width', 'height', 'curb-weight', 'engine-type',\n        'num-of-cylinders', 'engine-size', 'fuel-system', 'bore', 'stroke', 'compression-rate', 'horsepower',\n        'peak-rpm', 'city-mpg', 'highway-mpg', 'price']\ndata = pd.read_csv(\"\/kaggle\/input\/predict-price-data\/imports-85-car.data\", names=columns)\ndata.head()","04d9e069":"continuous_columns = ['normalized-losses', 'wheel-base', 'length', 'width', 'height','curb-weight', 'bore', 'stroke',\n           'compression-rate', 'horsepower','peak-rpm', 'city-mpg', 'highway-mpg', 'price']\nfiltered_data = data[continuous_columns]\nfiltered_data.head()","0beb52b9":"filtered_data = filtered_data.replace('?', np.nan)\nfiltered_data.head()\n","56a3f023":"filtered_data = filtered_data.astype('float')\nrp_data = filtered_data.dropna(subset=['price'])\nrp_data.isnull().sum()","8fb5e2c8":"cleaned_data = rp_data.fillna(rp_data.mean())\ncleaned_data.isnull().sum()","408d7848":"cleaned_data = cleaned_data.astype(float)\ntarget = cleaned_data['price']\ncleaned_data = (cleaned_data - cleaned_data.min())\/(cleaned_data.max() - cleaned_data.min())\ncleaned_data['price'] = target\ncleaned_data.describe()\n","c60c8e91":"train_col=cleaned_data['horsepower']\ntarget_col=['price']\ndef knn_train_test(train_col, target_col, cleaned_data):\n    knn = KNeighborsRegressor()\n    np.random.seed(41)\n\n    shuffled_index = np.random.permutation(cleaned_data.index)\n    rand_data = cleaned_data.reindex(shuffled_index)\n    last_train_row = int(len(rand_data) \/ 2)\n\n    # Select the first half and set as training set\n    # Select the second half and set as test set\n    train_data = rand_data.iloc[0:last_train_row]\n    test_data = rand_data.iloc[last_train_row:]\n\n    # Fit a KNN model with default k value\n    knn.fit(train_data[[train_col]], train_data[target_col])\n\n    # Make predictions using model\n    prediction = knn.predict(test_data[[train_col]])\n\n    # Calculate r2 score\n    r2 = r2_score(test_data[target_col], prediction)\n\n    return r2\n","d0ca1ae1":"spr_results = dict()\ntrain_cols = cleaned_data.columns.drop('price')\n\nfor col in train_cols:\n    rmse_val = knn_train_test(col, 'price', cleaned_data)\n    spr_results[col] = rmse_val\n\nspr_results_series = pd.Series(spr_results)\nprint(spr_results_series.sort_values(ascending=False))","7c557e88":"np.corrcoef(cleaned_data['price'], cleaned_data['horsepower'])\nhorse_price= plt.scatter(cleaned_data['horsepower'], cleaned_data['price'])\nplt.xlabel(\"horsepower\")\nplt.ylabel(\"price\")\nplt.title(\"Price vs Horsepower\")\nplt.show()\n\nnp.corrcoef(cleaned_data['price'], cleaned_data['length'])\nplt.scatter(cleaned_data['length'], cleaned_data['price'])\nplt.xlabel(\"length\")\nplt.ylabel(\"price\")\nplt.title(\"Price vs Length\")\nplt.show()","ebb82208":"lst_k= [1,3,5,7,9,11,13,15,20]\ndef knn_train_test_with_k(train_col, target_col, cleaned_data, lst_k):\n    np.random.seed(41)\n    shuffled_index = np.random.permutation(cleaned_data.index)\n    rand_data = cleaned_data.reindex(shuffled_index)\n    last_train_row = int(len(rand_data) \/ 2)\n    train_data = rand_data.iloc[0:last_train_row]\n    test_data = rand_data.iloc[last_train_row:]\n    k_r2 = {}\n    for k in lst_k:\n        # Fit model using k nearest neighbors\n        knn = KNeighborsRegressor(n_neighbors=k)\n        knn.fit(train_data[[train_col]], train_data[target_col])\n\n        # Make predictions using model with parameter k\n        predicted_labels = knn.predict(test_data[[train_col]])\n\n        # Calculate and return r2 score\n        r2 = r2_score(test_data[target_col], predicted_labels)\n        k_r2[k] = r2\n    return k_r2","ba9e71c6":"k_r2_results = {}\ntrain_cols = cleaned_data.columns.drop('price')\nfor col in train_cols:\n    r2_val = knn_train_test_with_k(col, 'price', cleaned_data, lst_k)\n    k_r2_results[col] = r2_val\nprint(k_r2_results)\n\n    ","87886c9e":"   {\n   \"normalized-losses\":{\n      \"1\":-0.20347877150311544,\n      \"3\":0.17182725146471145,\n      \"5\":0.18153843512468892,\n      \"7\":0.10083702893110358,\n      \"9\":0.182362509395823,\n      \"11\":0.18031169148017767,\n      \"13\":0.16974371713072933,\n      \"15\":0.1341773639203342,\n      \"20\":0.0667826878105201\n   },\n   \"wheel-base\":{\n      \"1\":0.5901895470391019,\n      \"3\":0.3446114130407425,\n      \"5\":0.29361993510864925,\n      \"7\":0.25849269186507307,\n      \"9\":0.24116785925974116,\n      \"11\":0.2411499427601267,\n      \"13\":0.21732985177662112,\n      \"15\":0.22993509426628334,\n      \"20\":0.21530706065441718\n   },\n   \"length\":{\n      \"1\":0.44811624375935755,\n      \"3\":0.47755642151821065,\n      \"5\":0.5008398379078013,\n      \"7\":0.4494274006708693,\n      \"9\":0.4158077529868751,\n      \"11\":0.3978756122950492,\n      \"13\":0.3999642309399496,\n      \"15\":0.3873973361782471,\n      \"20\":0.3741386387457387\n   },\n   \"width\":{\n      \"1\":0.7138978548128321,\n      \"3\":0.5917872893952195,\n      \"5\":0.5422211099152492,\n      \"7\":0.5087479104832227,\n      \"9\":0.49400488832784184,\n      \"11\":0.49544659225107635,\n      \"13\":0.4748648828312273,\n      \"15\":0.46589314269340854,\n      \"20\":0.4181362493867924\n   },\n   \"height\":{\n      \"1\":-0.34750232697831507,\n      \"3\":0.012670204039874844,\n      \"5\":-0.046938163034020874,\n      \"7\":0.015282983353519075,\n      \"9\":0.021379205655576827,\n      \"11\":-0.04193631550265353,\n      \"13\":-0.060119408003041386,\n      \"15\":-0.0445003798244612,\n      \"20\":-0.04789718105661289\n   },\n   \"curb-weight\":{\n      \"1\":0.4344317601261617,\n      \"3\":0.5541364995438716,\n      \"5\":0.6276543235169999,\n      \"7\":0.6458119033491309,\n      \"9\":0.6374280311872131,\n      \"11\":0.618411233439343,\n      \"13\":0.6100309915211192,\n      \"15\":0.5879079639235119,\n      \"20\":0.5802663501280053\n   },\n   \"bore\":{\n      \"1\":0.23293061869426546,\n      \"3\":0.2918144896891518,\n      \"5\":0.24874273417857606,\n      \"7\":0.2651695758532888,\n      \"9\":0.3644448735243053,\n      \"11\":0.36670690168638365,\n      \"13\":0.34030771479121713,\n      \"15\":0.2886539505109288,\n      \"20\":0.2738662454075663\n   },\n   \"stroke\":{\n      \"1\":-0.009557791310919495,\n      \"3\":0.17098084694098215,\n      \"5\":0.049243335637251695,\n      \"7\":0.023151602905364332,\n      \"9\":0.09238977228449086,\n      \"11\":0.0723641454882813,\n      \"13\":0.1100917046465093,\n      \"15\":0.09412363120075362,\n      \"20\":0.04881488321768279\n   },\n   \"compression-rate\":{\n      \"1\":0.12400206165224748,\n      \"3\":0.36354890008040386,\n      \"5\":0.24453644015304088,\n      \"7\":0.1687898838618337,\n      \"9\":0.11544680405161034,\n      \"11\":0.02281339505648461,\n      \"13\":-0.012994180381237008,\n      \"15\":0.0026816296142917473,\n      \"20\":0.028819341490647687\n    },\n   \"horsepower\":{\n      \"1\":0.6082793673131655,\n      \"3\":0.7651409653928744,\n      \"5\":0.7612838636248621,\n      \"7\":0.7313486732278749,\n      \"9\":0.690430467471112,\n      \"11\":0.6654523386913755,\n      \"13\":0.6284018293519005,\n      \"15\":0.6145724997468027,\n      \"20\":0.5756592846678703\n   },\n   \"peak-rpm\":{\n      \"1\":-0.0917138518970706,\n      \"3\":-0.06944703003948538,\n      \"5\":0.0016078100318174249,\n      \"7\":-0.02156727473143727,\n      \"9\":-0.04861466756370936,\n      \"11\":-0.0694891503769286,\n      \"13\":-0.025762258400936044,\n      \"15\":-0.05119654534547169,\n      \"20\":-0.06804677424265737\n   },\n   \"city-mpg\":{\n      \"1\":0.645086326024572,\n      \"3\":0.6708318091986238,\n      \"5\":0.6684244119005729,\n      \"7\":0.6129819574543349,\n      \"9\":0.6019164298058262,\n      \"11\":0.6009925105849733,\n      \"13\":0.6109863403949236,\n      \"15\":0.5849554577391467,\n      \"20\":0.5554490691485593\n   },\n   \"highway-mpg\":{\n      \"1\":0.5552104902091517,\n      \"3\":0.7002064962010035,\n      \"5\":0.6954033408422096,\n      \"7\":0.6780040850502695,\n      \"9\":0.6668544550377844,\n      \"11\":0.6339638556552454,\n      \"13\":0.5926790372812158,\n      \"15\":0.5653690854498343,\n      \"20\":0.5300989657617556\n   }\n}","1238c47a":"for col in train_cols:\n     for k,v in k_r2_results.items():\n        x = list(v.keys())\n        y = list(v.values())\n        plt.plot(x,y)\n        plt.xlabel('k value')\n        plt.ylabel('R2')\n        plt.title(\"Multiple K\")      \nplt.show()","89386b64":"feature_avg_r2 = {}\nfor k,v in k_r2_results.items():\n    avg_r2 = np.mean(list(v.values()))\n    feature_avg_r2[k] = avg_r2\n    series_avg_r2 = pd.Series(feature_avg_r2)\nprint(series_avg_r2.sort_values(ascending=False))","cd12774c":"def knn_train_test_several_features(train_col, target_col, cleaned_data, lst_k):\n    np.random.seed(41)\n    shuffled_index = np.random.permutation(cleaned_data.index)\n    rand_data = cleaned_data.reindex(shuffled_index)\n    last_train_row = int(len(rand_data) \/ 2)\n    train_data = rand_data.iloc[0:last_train_row]\n    test_data = rand_data.iloc[last_train_row:]\n    k_r2 = {}\n    for k in lst_k:\n        knn = KNeighborsRegressor(n_neighbors=k)\n        knn.fit(train_data[train_cols], train_data[target_col])\n\n        predictions = knn.predict(test_data[train_cols])\n\n        r2 = r2_score(test_data[target_col], predictions)\n        k_r2[k] = r2\n\n    return k_r2   \n     \n    ","1ffded4c":"k_r2_results = {}\ntwo_best_features = ['horsepower', 'highway-mpg']\nr2 = knn_train_test_several_features(two_best_features, 'price', cleaned_data, lst_k=range(1,25))\nk_r2_results[\"two best features\"] = r2\n\nthree_best_features = ['horsepower', 'highway-mpg', 'city-mpg']\nr3 = knn_train_test_several_features(three_best_features, 'price', cleaned_data, lst_k=range(1,25))\nk_r2_results[\"three best features\"] = r3\n\nfour_best_features = ['horsepower', 'highway-mpg', 'city-mpg', 'curb-weight']\nr4 = knn_train_test_several_features(four_best_features, 'price', cleaned_data, lst_k=range(1,25))\nk_r2_results[\"four best features\"] = r4\n\nfive_best_features = ['horsepower', 'highway-mpg', 'city-mpg', 'curb-weight', 'width']\nr5 = knn_train_test_several_features(five_best_features, 'price', cleaned_data, lst_k=range(1,25))\nk_r2_results[\"five best features\"] = r5\n\nsix_best_features = ['horsepower', 'highway-mpg', 'city-mpg', 'curb-weight', 'width', 'length']\nr6 = knn_train_test_several_features(six_best_features, 'price', cleaned_data, lst_k=range(1,25))\nk_r2_results[\"six best features\"] = r6\nplt.show()\nprint(k_r2_results)\n\n","ad44c2ca":"{\n   \"normalized-losses\":{\n      \"1\":-0.20347877150311544,\n      \"3\":0.17182725146471145,\n      \"5\":0.18153843512468892,\n      \"7\":0.10083702893110358,\n      \"9\":0.182362509395823,\n      \"11\":0.18031169148017767,\n      \"13\":0.16974371713072933,\n      \"15\":0.1341773639203342,\n      \"20\":0.0667826878105201\n   },\n   \"wheel-base\":{\n      \"1\":0.5901895470391019,\n      \"3\":0.3446114130407425,\n      \"5\":0.29361993510864925,\n      \"7\":0.25849269186507307,\n      \"9\":0.24116785925974116,\n      \"11\":0.2411499427601267,\n      \"13\":0.21732985177662112,\n      \"15\":0.22993509426628334,\n      \"20\":0.21530706065441718\n   },\n   \"length\":{\n      \"1\":0.44811624375935755,\n      \"3\":0.47755642151821065,\n      \"5\":0.5008398379078013,\n      \"7\":0.4494274006708693,\n      \"9\":0.4158077529868751,\n      \"11\":0.3978756122950492,\n      \"13\":0.3999642309399496,\n      \"15\":0.3873973361782471,\n      \"20\":0.3741386387457387\n   },\n   \"width\":{\n      \"1\":0.7138978548128321,\n      \"3\":0.5917872893952195,\n      \"5\":0.5422211099152492,\n      \"7\":0.5087479104832227,\n      \"9\":0.49400488832784184,\n      \"11\":0.49544659225107635,\n      \"13\":0.4748648828312273,\n      \"15\":0.46589314269340854,\n      \"20\":0.4181362493867924\n   },\n   \"height\":{\n      \"1\":-0.34750232697831507,\n      \"3\":0.012670204039874844,\n      \"5\":-0.046938163034020874,\n      \"7\":0.015282983353519075,\n      \"9\":0.021379205655576827,\n      \"11\":-0.04193631550265353,\n      \"13\":-0.060119408003041386,\n      \"15\":-0.0445003798244612,\n      \"20\":-0.04789718105661289\n   },\n   \"curb-weight\":{\n      \"1\":0.4344317601261617,\n      \"3\":0.5541364995438716,\n      \"5\":0.6276543235169999,\n      \"7\":0.6458119033491309,\n      \"9\":0.6374280311872131,\n      \"11\":0.618411233439343,\n      \"13\":0.6100309915211192,\n      \"15\":0.5879079639235119,\n      \"20\":0.5802663501280053\n   },\n   \"bore\":{\n      \"1\":0.23293061869426546,\n      \"3\":0.2918144896891518,\n      \"5\":0.24874273417857606,\n      \"7\":0.2651695758532888,\n      \"9\":0.3644448735243053,\n      \"11\":0.36670690168638365,\n      \"13\":0.34030771479121713,\n      \"15\":0.2886539505109288,\n      \"20\":0.2738662454075663\n   },\n   \"stroke\":{\n      \"1\":-0.009557791310919495,\n      \"3\":0.17098084694098215,\n      \"5\":0.049243335637251695,\n      \"7\":0.023151602905364332,\n      \"9\":0.09238977228449086,\n      \"11\":0.0723641454882813,\n      \"13\":0.1100917046465093,\n      \"15\":0.09412363120075362,\n      \"20\":0.04881488321768279\n   },\n   \"compression-rate\":{\n      \"1\":0.12400206165224748,\n      \"3\":0.36354890008040386,\n      \"5\":0.24453644015304088,\n      \"7\":0.1687898838618337,\n      \"9\":0.11544680405161034,\n      \"11\":0.02281339505648461,\n      \"13\":-0.012994180381237008,\n      \"15\":0.0026816296142917473,\n      \"20\":0.028819341490647687\n    },\n   \"horsepower\":{\n      \"1\":0.6082793673131655,\n      \"3\":0.7651409653928744,\n      \"5\":0.7612838636248621,\n      \"7\":0.7313486732278749,\n      \"9\":0.690430467471112,\n      \"11\":0.6654523386913755,\n      \"13\":0.6284018293519005,\n      \"15\":0.6145724997468027,\n      \"20\":0.5756592846678703\n   },\n   \"peak-rpm\":{\n      \"1\":-0.0917138518970706,\n      \"3\":-0.06944703003948538,\n      \"5\":0.0016078100318174249,\n      \"7\":-0.02156727473143727,\n      \"9\":-0.04861466756370936,\n      \"11\":-0.0694891503769286,\n      \"13\":-0.025762258400936044,\n      \"15\":-0.05119654534547169,\n      \"20\":-0.06804677424265737\n   },\n   \"city-mpg\":{\n      \"1\":0.645086326024572,\n      \"3\":0.6708318091986238,\n      \"5\":0.6684244119005729,\n      \"7\":0.6129819574543349,\n      \"9\":0.6019164298058262,\n      \"11\":0.6009925105849733,\n      \"13\":0.6109863403949236,\n      \"15\":0.5849554577391467,\n      \"20\":0.5554490691485593\n   },\n   \"highway-mpg\":{\n      \"1\":0.5552104902091517,\n      \"3\":0.7002064962010035,\n      \"5\":0.6954033408422096,\n      \"7\":0.6780040850502695,\n      \"9\":0.6668544550377844,\n      \"11\":0.6339638556552454,\n      \"13\":0.5926790372812158,\n      \"15\":0.5653690854498343,\n      \"20\":0.5300989657617556\n   }\n}","0645b62c":"**EVALUATION**\n\nUsing scatterplot to see how the relationship between horsepower, length and price","ed2fae79":"**PREPARATION**\n\nTo build a model, we only use continuous variables. So, I'll filter these columns","8fe07edc":"**BUSINESS UNDERSTANDING**\n\nFrom the data,I want to predict the price of the car based on its attributes. There are 3 business quesstions that I need to answer here:\n* Is the horsepower related to the cars' price?\n* Is the length of the car related to the cars' price?\n* Whether the predicted price will be reasonable accuracy with these attributes? \n","cb79e502":"**MODELLING**\n\nIn this project, I'll not use linear regress model due to having many outliers which is linear regressions' drawback, but K-nearest neighbor model.\n\nFirstly, I will modelize for each feature seperately. (k default) \n\nLogic:\nUsed 50% of the data to train a knn-mmodel and computes the r2 score based on the remaining 50% of the data\n\n   + train_col: column which should be used as feature\n   + target_col: the target column which should be predicted\n   + cleaned_data: pandas dataframe which contains the data\n\n","3a2c2c1b":"Then visualize the above result ","90bbccd3":"The chart indicates if k belongs to the range 3-5, the Knn model seems more accuracy. (The closer the r2-score is to 1, the better model)\n","5721fb87":"Similarly, now we use this function for different k then visualize the result ","b3728e5f":"Alright, data is clean now.\n\nThe next step is feature scale","51c97dab":"Now, I build the model based on several features to define how many features will optimize the Knn model and which features should be used to predict the price to answer the last question\n\nFirstly, I will compute the average r2-score across different k value for each feature in the above result.","13e95610":"Using a metric called r2 score to evaluate the performance of the knn model. The closer the r2-score is to 1, the better model.\nAs the above result, we can answer first 2 questions:\n\n* The feature horsepower has the highest r2-score (76%) among all considered features. Hence, it is definitively related to the price.\n* There exists also a relationship between the price and the length but not strong (50%)\n\n","73cf6510":"From the above result, I can conclude that the optimal model is with k=4 and the number of features = 2 simply because, at this point, the r2 score is highest with 85%. \n\nIn conclusion, when I want to predict the price of the cars, I'll input data on this trained knn model, and based on attributes in the new input data, the model will compare with its historical data, then suggest us the predicted price with the accuracy level approximately 85%.","4887d741":"In case of the remaining columns, using column mean to replace missing values then check again","5bafb774":"Take number of features into account","0dc27b82":"In the following step, let remove rows having missing price value because \"Price\" is the predicted value. Then, check whether the remaining columns have any missing values.\n","3a35da31":"\nNow we can use the above function to train a knn model for each feature seperate and collect the results:","8fa34da8":"Knn model is known as sensitive with bias when k is not enough large. For that reason, I modify the knn_train_test function by including a list of different k values.\n* cleaned_data(obj): pandas dataframe which contains the data\n* lst_k(obj): list of different k-values","bbcb6c05":"**CLEANING DATA**\n\nLooking at our data, we can see there are some \"?\" which preresented for missing value. So, I'll clean these question mark"}}