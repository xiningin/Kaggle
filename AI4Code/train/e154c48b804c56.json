{"cell_type":{"8d557d5c":"code","7698e0ea":"code","fd9ecc54":"code","c611bd05":"code","d5ad638c":"code","ca08f028":"code","1ad6fddd":"code","0383b605":"code","6bb76ebf":"code","ffa2470a":"code","ada97a4d":"code","7439b7d8":"code","e4834238":"code","3770d457":"code","f0213041":"code","38a627f3":"code","5c8890bf":"code","92b48092":"code","76c4fe1d":"code","b4db8681":"code","fbb4a4a5":"code","a905ae23":"code","5915bf42":"code","b6b3934f":"code","b2352a66":"code","786f65bc":"code","941bd5d0":"code","3a48cce1":"code","abf89390":"code","dbfd7eef":"code","dbb1c193":"code","90d433cc":"code","1dd4b40b":"code","398b9dad":"code","251b6f57":"code","77181325":"code","4caced85":"code","e070d87a":"code","a0ea6a68":"code","64eccf59":"code","590e2620":"code","5270ff56":"code","21eeda66":"code","efb32ece":"code","3719aadb":"code","e68e2213":"code","2788883c":"code","4afcc76b":"code","c3c0b847":"code","71a8aed3":"code","ff4e7dc5":"code","66957783":"code","78c0ea83":"code","c71f8532":"code","f33e72d9":"code","7fb90b0f":"code","22dba25e":"markdown","f6931a31":"markdown","9e668273":"markdown","1078a7bf":"markdown","c36cade8":"markdown","8d52dafc":"markdown","e8852599":"markdown","b32f0d2d":"markdown","7b7e6bea":"markdown","c02b76a6":"markdown","f6af0013":"markdown","d3320af7":"markdown","f9c997f1":"markdown","6f3b790f":"markdown","bf19eb92":"markdown","68586818":"markdown","aeaa4e5b":"markdown","31b7ef3d":"markdown","18a8a92a":"markdown","070a6369":"markdown","3444550b":"markdown","198bea32":"markdown","d7ca5138":"markdown","1cca4e39":"markdown","64095fd3":"markdown"},"source":{"8d557d5c":"import numpy as np\nimport pandas as pd\nfrom fastai.callbacks import *\nfrom fastai.callback import *\nfrom fastai.basic_train import *\nfrom fastai.basic_data import *\nfrom fastai.basics import *\nfrom fastai.metrics import *","7698e0ea":"path = Path('..\/input')\npath.ls()","fd9ecc54":"df_train = pd.read_csv(path\/'train.csv')","c611bd05":"df_train.head()","d5ad638c":"y = df_train['label'].values\nX = df_train.drop(columns='label').values","ca08f028":"from sklearn.model_selection import train_test_split","1ad6fddd":"x_train,x_valid,y_train,y_valid = train_test_split(X,y)\nx_train.shape, x_valid.shape,y_train.max(),y_train.min()","0383b605":"x_train,y_train,x_valid,y_valid = map(tensor,(x_train,y_train,x_valid,y_valid))","6bb76ebf":"def normalize_data(train,valid):\n    train = train.float()\n    valid = valid.float()\n    m = train.mean()\n    s = train.std()\n    return (train - m)\/s, (valid - m)\/s","ffa2470a":"x_train, x_valid = normalize_data(x_train,x_valid)\nx_train.shape,x_valid.shape","ada97a4d":"plt.imshow(x_train[0].view(28,28),cmap='gray')\nplt.title(str(y_train[0]))","7439b7d8":"train_ds = TensorDataset(x_train,y_train)\nvalid_ds = TensorDataset(x_valid,y_valid)","e4834238":"train_dl = DataLoader(\n    dataset=train_ds,\n    batch_size=64,\n    shuffle=True,\n    num_workers=2\n)\nvalid_dl = DataLoader(\n    dataset=valid_ds,\n    batch_size=128,\n    num_workers=2\n)","3770d457":"data = DataBunch(train_dl,valid_dl)\ndata.c = len(y_train.unique())","f0213041":"plt.imshow(data.train_ds[0][0].view(28,28),cmap='gray')\nplt.title(str(data.train_ds[0][1]))","38a627f3":"def flatten(x):\n    return x.view(x.shape[0],-1)\n\nclass Lambda(nn.Module):\n    def __init__(self,func): \n        super().__init__()\n        self.func = func\n        \n    def forward(self,xb): return self.func(xb)\n\nclass SubRelu(nn.Module):\n    def __init__(self,sub=0.4):\n        super().__init__()\n        self.sub = sub\n    \n    def forward(self,xb):\n        xb = F.relu(xb)\n        xb.sub_(self.sub)\n        return xb\n\ndef subConv2d(ni,nf,ks=3,stride=2):\n    return nn.Sequential(nn.Conv2d(ni,nf,ks,padding=ks\/\/2,stride=stride),SubRelu())\n\ndef get_subRelu_model():\n    model = nn.Sequential(\n        subConv2d(1,8),\n        subConv2d(8,16),\n        subConv2d(16,32),\n        subConv2d(32,32),\n        nn.AdaptiveAvgPool2d(1),\n        Lambda(flatten),\n        nn.Linear(32,10),\n    )\n    return model","5c8890bf":"model = get_subRelu_model()\nmodel","92b48092":"def init_model(model):\n    for layer in model:\n        if isinstance(layer,nn.Sequential):\n            nn.init.kaiming_normal_(layer[0].weight)\n            layer[0].bias.detach().zero_()","76c4fe1d":"model[0][0].bias","b4db8681":"init_model(model)\nmodel[0][0].bias #check the model is initialized","fbb4a4a5":"class BatchTransFormXCallback(Callback):\n    _order = 2 \n    def __init__(self,tfm):\n        #super().__init__(learn)\n        self.tfm = tfm\n    \n    def on_batch_begin(self,**kwargs):\n        xb = self.tfm(kwargs['last_input'])\n        return {'last_input': xb.float()}","a905ae23":"# wrap learner creation step, for easy re-use during build up this notebook\ndef get_learner(model):\n    opt_func = optim.SGD\n    loss_func = nn.CrossEntropyLoss()\n    return Learner(data,model.cuda(),opt_func=opt_func,loss_func=loss_func,metrics=accuracy)","5915bf42":"learn = get_learner(model)","b6b3934f":"learn.callbacks.append(BatchTransFormXCallback(lambda x: x.view(-1,1,28,28)))","b2352a66":"learn.callback_fns.append(ActivationStats) #fastai build in hook to grab layer activation stats","786f65bc":"learn.split(lambda m: m[4])","941bd5d0":"#double check the model is split\nlearn.layer_groups","3a48cce1":"learn.lr_find()\nlearn.recorder.plot()","abf89390":"#apply cos sched and discriminative lrs\nlearn.fit_one_cycle(8,slice(1e-1,4e-1),pct_start=0.3)","dbfd7eef":"means = learn.activation_stats.stats[0]\nfor i in range(4):\n    plt.plot(means[i][:800])\nplt.legend(range(4))","dbb1c193":"std = learn.activation_stats.stats[1]\nfor i in range(4):\n    plt.plot(std[i][:800])\nplt.legend(range(4))","90d433cc":"learn.recorder.plot_lr()","1dd4b40b":"learn.recorder.plot_losses()","398b9dad":"class BatchNorm_layer(nn.Module):\n    def __init__(self,nf,mom=0.1,eps=1e-6):\n        super().__init__()\n        self.nf = nf\n        self.mom = mom\n        self.eps = eps\n        self.gamma = nn.Parameter(torch.ones(nf,1,1))\n        self.beta = nn.Parameter(torch.zeros(nf,1,1))\n        self.register_buffer('vars',torch.ones(1,nf,1,1))\n        self.register_buffer('means',torch.zeros(1,nf,1,1))\n    \n    def batch_norm(self,xb):\n        m = xb.mean(dim=(0,2,3),keepdim=True)\n        #var = xb.var(dim=(0,2,3),keepdim=True)\n        var = xb.detach().cpu().numpy() #kaggle torch.var only takes int for dim, not tuple\n        var = var.var((0,2,3),keepdims=True)\n        var = torch.from_numpy(var).cuda()\n        self.means.lerp_(m,self.mom)\n        self.vars.lerp_(var,self.mom)\n        return m,var\n    \n    def forward(self,xb):\n        if self.training:\n            with torch.no_grad(): m,v = self.batch_norm(xb)\n        else:\n            m,v = self.means,self.vars\n        xb = (xb - m) \/ (v+self.eps).sqrt()\n        return self.gamma * xb + self.beta","251b6f57":"class RunningBatchNorm(nn.Module):\n    def __init__(self, nf, mom=0.1, eps=1e-5):\n        super().__init__()\n        self.mom, self.eps = mom, eps\n        self.mults = nn.Parameter(torch.ones (nf,1,1))\n        self.adds  = nn.Parameter(torch.zeros(nf,1,1))\n        self.register_buffer('sums', torch.zeros(1,nf,1,1))\n        self.register_buffer('sqrs', torch.zeros(1,nf,1,1))\n        self.register_buffer('count', tensor(0.))\n        self.register_buffer('factor', tensor(0.))\n        self.register_buffer('offset', tensor(0.))\n        self.batch = 0\n        \n    def update_stats(self, x):\n        bs,nc,*_ = x.shape\n        self.sums.detach_()\n        self.sqrs.detach_()\n        dims = (0,2,3)\n        s    = x    .sum(dims, keepdim=True)\n        ss   = (x*x).sum(dims, keepdim=True)\n        c    = s.new_tensor(x.numel()\/nc)\n        mom1 = s.new_tensor(1 - (1-self.mom)\/math.sqrt(bs-1))\n        self.sums .lerp_(s , mom1)\n        self.sqrs .lerp_(ss, mom1)\n        self.count.lerp_(c , mom1)\n        self.batch += bs\n        means = self.sums\/self.count\n        varns = (self.sqrs\/self.count).sub_(means*means)\n        if bool(self.batch < 20): varns.clamp_min_(0.01)\n        self.factor = self.mults \/ (varns+self.eps).sqrt()\n        self.offset = self.adds - means*self.factor\n        \n    def forward(self, x):\n        if self.training: self.update_stats(x)\n        return x*self.factor + self.offset","77181325":"def Conv2d_BN(ni,nf,ks=3,stride=2,BN=True):\n    if BN:\n        return nn.Sequential(nn.Conv2d(ni,nf,ks,padding=ks\/\/2,stride=stride),SubRelu(),BatchNorm_layer(nf))\n    else:\n        return nn.Sequential(nn.Conv2d(ni,nf,ks,padding=ks\/\/2,stride=stride),SubRelu(),RunningBatchNorm(nf))\n\ndef get_batchNorm_model(BN=True):\n    model = nn.Sequential(\n        #Lambda(lambda x: x.view(-1,1,28,28).float()),\n        Conv2d_BN(1,8,BN=BN),\n        Conv2d_BN(8,16,BN=BN),\n        Conv2d_BN(16,32,BN=BN),\n        Conv2d_BN(32,32,BN=BN),\n        nn.AdaptiveAvgPool2d(1),\n        Lambda(flatten),\n        nn.Linear(32,10),\n    )\n    return model","4caced85":"bn_model = get_batchNorm_model()\nopt_func = optim.SGD\nloss_func = nn.CrossEntropyLoss\nlearn = Learner(data,bn_model.cuda(),opt_func=opt_func,loss_func=loss_func(),metrics=accuracy)\ncb = BatchTransFormXCallback(tfm=lambda x: x.view(-1,1,28,28))\nlearn.callbacks.append(cb)\nlearn.callback_fns.append(ActivationStats)\ninit_model(learn.model)","e070d87a":"learn.split(lambda m: m[4])","a0ea6a68":"learn.fit_one_cycle(12,slice(1e-1,2.),pct_start=0.3)","64eccf59":"learn.recorder.plot_lr()","590e2620":"learn.recorder.plot_losses()","5270ff56":"means = learn.activation_stats.stats[0]\nfor i in range(4):\n    plt.plot(means[i][:800])\nplt.legend(range(4))","21eeda66":"std = learn.activation_stats.stats[1]\nfor i in range(4):\n    plt.plot(std[i][:800])\nplt.legend(range(4))","efb32ece":"bn_model = get_batchNorm_model(BN=False)\nopt_func = optim.SGD\nloss_func = nn.CrossEntropyLoss\nlearn = Learner(data,bn_model.cuda(),opt_func=opt_func,loss_func=loss_func(),metrics=accuracy)\ncb = BatchTransFormXCallback(tfm=lambda x: x.view(-1,1,28,28))\nlearn.callbacks.append(cb)\nlearn.callback_fns.append(ActivationStats)\ninit_model(learn.model)","3719aadb":"learn.split(lambda m: m[4])","e68e2213":"learn.fit_one_cycle(12,slice(1e-1,2.),pct_start=0.3)","2788883c":"df_test = pd.read_csv(path\/'test.csv')\ndf_test = df_test.values","4afcc76b":"test_train = torch.from_numpy(X) #just to get m and std\ntest = torch.from_numpy(df_test)\ntest.shape","c3c0b847":"_,test = normalize_data(test_train,test)","71a8aed3":"dummy_y = torch.ones(test.shape[0])\ndummy_y.shape","ff4e7dc5":"test_ds = TensorDataset(test,dummy_y)","66957783":"test_dl = DataLoader(\n    dataset = test_ds,\n    batch_size = 64,\n    num_workers = 2\n)","78c0ea83":"learn.model.eval()","c71f8532":"def get_preds(test_dl,model):\n    preds = []\n    model.cpu()\n    for dl in test_dl:\n        pred_batch = torch.argmax(model(dl[0].view(-1,1,28,28)),dim=1)\n        preds += pred_batch.detach().tolist()\n    return preds","f33e72d9":"preds = get_preds(test_dl,learn.model)\nlen(preds)","7fb90b0f":"final = pd.Series(preds,name='Label')\nsubmission = pd.concat([pd.Series(range(1,28001),name='ImageId'),final],axis=1)\nsubmission.to_csv('fastai-pytorch-0.99.csv',index=False)","22dba25e":"This code silightly modifies from fastai 2019 part 2 class. check the [link here](https:\/\/github.com\/fastai\/fastai_docs\/blob\/master\/dev_course\/dl2\/07_batchnorm.ipynb)","f6931a31":"Since not applying transfer learning, make sure the model is initialized\nhttps:\/\/arxiv.org\/abs\/1502.01852","9e668273":"\n# Introduction\n","1078a7bf":"check if the data is correct","c36cade8":"# Problem\nHowever, if one were trying to use transfer learning on MNIST, the first issue is how to clean the data. The most nerual network arch accepting 3 channels (RBG), but for MNIST dataset, it has 784 pixels in a row for single image. \n\nFastai vision datablock API doesn't support this kind of data. It needs image files, one work around will be provide customized itemlist on top of ImageItemList. This can be done in couple line of codes, [FastAI 1.0 with customized Itemlist](https:\/\/www.kaggle.com\/heye0507\/fastai-1-0-with-customized-itemlist)","8d52dafc":"Fastai Callback to handle outshape tensor.\n\nProblem: \n\nHave input data shape (64,784), first conv layer has filter size (1,8,5,5). The input data needs to be reshaped to (64,1,28,28)\n\nResolve:\n\ncallback to do transformation on every batch. The process here is the following:\n\n1. grab a batch\n2. apply transformation on training\n\nNotice here, the transformation is done on run time, the data remains unchanged. This can also be applied to use other data argumentaion tenique other than the default fastai library. \n\nAlso, fastai callbacks are controlled by _order\n\nIt will sort the callback list based on order, so make sure you use order in the right sequence\nData during callback is packed in dict in fastai, check [fastai callback docs](https:\/\/docs.fast.ai\/callback.html#Classes-for-callback-implementors)","e8852599":"Tensor dataset requires pair of input and output, so we will provide a dummy output to wrap the Dataset. ","b32f0d2d":"# Another approach\n\nHere I will explain another approach to hybrid pytorch and fastai. This approach has the following advantages:\n\n1. The model doesn't tight with fastai API's, one doesn't have to know how to use Datablock API. Therefore you can add fastai to the existing model. \n\n2. Hybrid models, as described in the 2018 part 2, a object detection model can be done with 1 classifier model with another regression model (The classifier is to find right class in the image and the regression model is to work on bounding box coordinates). Therefore, ImageItemList datablock API won't suit for this case\n\n3. More control of the model. You can apply different customized layers into existing model, you can replace one layer with another (Later I will show how to use a subRelu() instead fo original Pytorch Relu())\n\nHowever, this approach also has some downsides\n\n1. One has to know the boundries, since part of the work is done on fastai, part is done on Pytorch. Without proper support of the existing ItemList, some of the API is not going to work, for instance: show_batch(), predict()\n\n2. Hard to debug, it requires more time to figure out the root cause of the issus as this approach hybrids two things together. \n\nTherefore, this notebook is serverd as demonstration purpose, you can also wrap ItemList behavior to best suit the purpose. ","7b7e6bea":"As discusssed in 2019 fastai part 2, Kaiming indicates that we want to have 0 mean and 1 std for the initialization, however, during class we noticed that if you have 0 mean and std 1, after relu() layer, the less than 0 part are set to 0. The relu() layer will cause the activations to have roughtly 0.5 mean and 1 std, we can fix this by subtract 0.5 after relu()","c02b76a6":"# Create databunch using Pytorch","f6af0013":"# Things we want to take from fastai\n\n1. learning rate finder https:\/\/arxiv.org\/abs\/1803.09820\n2. cos scheduler,discriminative learing rate https:\/\/arxiv.org\/abs\/1506.01186\n3. fastai callback systems\n","d3320af7":"Special thanks to Jeremy for the great class. Lots of ideas in this notebook is from fastai part2 2019.\n\nIn fastai part 1 2019, Lesson 5. We have seen that how Jeremey is wrapped the dataset in the dataloader, and used a signel linear layer nerual network to train MNIST set [Lesson 5: SGD with MNIST](https:\/\/github.com\/fastai\/course-v3\/blob\/master\/nbs\/dl1\/lesson5-sgd-mnist.ipynb). This can be easily done using pure pytorch.\n\n\nFastai tranditional routine would be similar to the following:\n\n1. Create databunch using datablock API\nThis ensures data argumentation, labeling \/ spilting data.\n\n2. Create Leaner with optim and arch, apply transfer learning, cut the model and attach with adapitveconcatpooling layer to maximize the gain from the CNN, then follow by couple linear layer to extract information before loss function (in MNIST the loss will be cross entropy loss, which is softmax + negative log likelyhood)\nBehind the scence, this is all done by the fastai library.\n\n3. Train your model using cos scheduler with high lr to have super convergence. ","f9c997f1":"Initialization seems working fine, in the first few epochs we have 0 mean and 1std. We can also check losses and lrs to see if the model is behave as we expected","6f3b790f":"# Prepare Learner","bf19eb92":"This can be done easily just following Pytorch tutorial\n\n1. Wrap test data into Dataloader\n2. Set the model to eval() mode to prevent updating weights\n3. Grab data from dataloader, and use the model to predict","68586818":"We can still train longer, but lets add batchnorm layer","aeaa4e5b":"Before applying discriminative learnring rate, one have to tell fastai how to split the model to apply different lrs (this are done behind the scence in the fastai supported arch)\nFollowing the fastai way, we cut the model at the pooling layer, so CNN gets smaller lr, and Linear layer gets larger lr","31b7ef3d":"We peaked the learning rate to 2.0, and the model still converges. \n\nAlso, from the mean and std we can see BatchNorm really smooth the activations cross all layers, therefore you can train the model in a very high learning rate and it still converges.\n\n[How Does Batch Normalization Help Optimization](https:\/\/arxiv.org\/pdf\/1805.11604.pdf)\n\nNow let's try using running batch norm from fastai class","18a8a92a":"# Fin\nNow we just need to use our model to predict. Unfortunately, we didn't supply learner with a proper ItemList, therefore, we can't call learn.predict(). We will just use the model to do prediction, a possible work around would be supply the DataBunch with test_dl,then learn.predict() might work.","070a6369":"30% up time, 70% down time, this ensures the model trains with high lr","3444550b":"# Prepare own model","198bea32":"Running Batch Norm seems really wroking well even for larger batch size (64), which Jeremey showed in class that you can use it with small BS, such as BS=2","d7ca5138":"# BatchNorm V.S. Running Norm\nhttps:\/\/arxiv.org\/pdf\/1502.03167.pdf","1cca4e39":"# Prepare Data in Pytorch","64095fd3":"The simple CNN model built here always have 1 conv, 1 subRelu(), I simply pack them together.\n\nUsed Lambda layer to flatten the model, this could also have done using Lambda layer to reshape the input, but this can also be done using fastai callbacks, therefore in the future if we ever need to reshape the input, there is no need to change the model arch, using callbacks to do transformation will be much easier."}}