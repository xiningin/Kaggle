{"cell_type":{"61111b54":"code","19335ef0":"code","8770b338":"code","1b80acaf":"code","d80856ed":"code","6bbf6f33":"code","41f1b2db":"code","8c1740fd":"code","79e5ea8c":"code","0a35d05c":"code","eb894a72":"code","56724055":"code","68ac2003":"code","94d3b757":"code","87398b57":"code","4529e9a4":"code","36ed76fc":"code","ab81cda2":"markdown","e4c51197":"markdown","8275cc14":"markdown","a4621278":"markdown"},"source":{"61111b54":"from IPython.core.display import display, HTML\n\nimport pandas as pd\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport glob\nimport os\nimport gc\nimport psutil\nimport pickle\nfrom tqdm import tqdm\nfrom joblib import Parallel, delayed\n\nfrom sklearn import preprocessing, model_selection\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import QuantileTransformer\nfrom sklearn.feature_selection import RFECV\nfrom sklearn.metrics import r2_score\nfrom sklearn.model_selection import GroupKFold, KFold\nimport lightgbm as lgb\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn import utils\n\nimport matplotlib.pyplot as plt \nimport seaborn as sns\nimport numpy.matlib\n\nimport warnings\nwarnings.filterwarnings('ignore')\npath_submissions = '\/'\ntarget_name = 'target'\nscores_folds = {}\n","19335ef0":"train = pd.read_feather('..\/input\/preprocessing-rapids-finish-in-3-mins\/feature_train.feather')","8770b338":"le = LabelEncoder()\nle.fit(list(train['stock_id'].astype(str).values))\ntrain['stock_id'] = le.transform(list(train['stock_id'].astype(str).values))","1b80acaf":"# F\ndef get_memory_usage():\n    return np.round(psutil.Process(os.getpid()).memory_info()[0]\/2.**30, 2) \ndef reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                       df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() \/ 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) \/ start_mem))\n    return df","d80856ed":"get_memory_usage()","6bbf6f33":"train = reduce_mem_usage(train)","41f1b2db":"features = [col for col in train.columns if col not in {'time_id','target','row_id'}]","8c1740fd":"train.fillna(-999, inplace=True)","79e5ea8c":"gc.collect()","0a35d05c":"\ndef rmspe(y_true, y_pred):\n    return np.sqrt(np.mean(np.square((y_true - y_pred) \/ y_true)))\n\ndef feval_rmspe(y_pred, lgb_train):\n    y_true = lgb_train.get_label()\n    return 'RMSPE', rmspe(y_true, y_pred), False\nparams = {\n    'objective': 'rmse',\n    'boosting_type': 'gbdt',\n    'max_depth': -1,\n    'max_bin':255,\n    'min_data_in_leaf':750,\n    'learning_rate': 0.1,\n    'subsample': 0.72,\n    'subsample_freq': 3,\n    'lambda_l1': 0.5,\n    'lambda_l2': 1.0,\n    'categorical_column':[0],\n    'seed':2021,\n    'n_jobs':-1,\n    'verbose': -1,\n}","eb894a72":"%time\nclf = lgb.LGBMRegressor(**params)\nrfe = RFECV(estimator=clf, step=20, cv=GroupKFold(n_splits=5), scoring= 'neg_mean_squared_error', verbose=2)\nrfe.fit(X = train[features], y = train['target'], groups = train['time_id'])","56724055":"gc.collect()","68ac2003":"print('Optimal number of features:', rfe.n_features_)","94d3b757":"plt.figure(figsize=(14, 8))\nplt.xlabel(\"Number of features selected\")\nplt.ylabel(\"Cross validation score\")\nplt.plot(range(1, len(rfe.grid_scores_) + 1), rfe.grid_scores_)\nplt.show()","87398b57":"features = train[features].columns[rfe.ranking_ == 1]","4529e9a4":"train = pd.read_feather('..\/input\/preprocessing-rapids-finish-in-3-mins\/feature_train.feather')\n# Function to early stop with root mean squared percentage error\ndef rmspe(y_true, y_pred):\n    return np.sqrt(np.mean(np.square((y_true - y_pred) \/ y_true)))\n\ndef feval_rmspe(y_pred, lgb_train):\n    y_true = lgb_train.get_label()\n    return 'RMSPE', rmspe(y_true, y_pred), False\n\ndef train_and_evaluate_lgb(train, params, features):\n    # Hyperparammeters (just basic)\n    \n    if 'stock_id' not in features:\n        features.insert(0,'stock_id') \n    #features = [col for col in features if col not in nnn]\n    y = train['target']\n    #train[features] = train[features].fillna(-999)\n    # Create out of folds array\n    oof_predictions = np.zeros(train.shape[0])\n    \n    # Create a KFold object\n    gfold = GroupKFold(n_splits = 5)\n    kfold = KFold(n_splits = 5, shuffle = True, random_state = 42)\n    groups = train['time_id']\n    models = []\n    counter = 0\n    # Iterate through each fold\n    NUM_FOLDS =5\n    for fold, (train_idx, val_idx) in enumerate(gfold.split(train, train['target'], train['time_id'])):\n        print('CV {}\/{}'.format(fold+1,5)) \n\n        x_train = train.loc[train_idx,features]\n        y_train = train.loc[train_idx, target_name]\n        x_val = train.loc[val_idx, features]\n        y_val = train.loc[val_idx, target_name]\n    ##################################################################\n        # Root mean squared percentage error weights\n        train_weights = 1 \/ np.square(y_train)\n        val_weights = 1 \/ np.square(y_val)\n\n        \n        train_dataset = lgb.Dataset(x_train, y_train, weight = train_weights, categorical_feature = ['stock_id'])\n        val_dataset = lgb.Dataset(x_val, y_val, weight = val_weights ,categorical_feature = ['stock_id']  )\n        model = lgb.train(params = params,\n                          num_boost_round=1000,\n                          train_set = train_dataset, \n                          valid_sets = [train_dataset, val_dataset], \n                          verbose_eval = 50,\n                          early_stopping_rounds=20,\n                          feval = feval_rmspe)\n        # Add predictions to the out of folds array\n        lgb.plot_importance(model,max_num_features=20, importance_type = 'gain')\n        models.append(model)\n        oof_predictions[val_idx] = model.predict(x_val[features])\n        # Predict the test set\n        #test_predictions += model.predict(test[features]) \/ 5\n         \n    rmspe_score = rmspe(y, oof_predictions)\n    _ = gc.collect()\n    print(f'Our out of folds RMSPE is {rmspe_score}')\n    \n    \n    # Return test predictions\n    return models\n# Traing and evaluate\nmodels= train_and_evaluate_lgb(train,params, features)","36ed76fc":"pickle.dump(models, open('.\/models.pkl','wb'))","ab81cda2":"Get preprocessing data from https:\/\/www.kaggle.com\/res1235\/preprocessing-rapids-finish-in-3-mins","e4c51197":"## Recursive feature elimination cross validation","8275cc14":"## Modeling","a4621278":"## Prepare data"}}