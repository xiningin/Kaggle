{"cell_type":{"f6fdfb78":"code","a08ed4e2":"code","d7f70446":"code","ae3e2323":"code","dc2f904e":"code","1ffa2090":"code","774fa000":"code","e81dde3a":"code","fe1afbd2":"code","afed72e9":"code","c54ee51e":"code","db7d380e":"code","d572d490":"code","d4259d1a":"code","58fcf484":"code","783dab63":"code","71f63e0c":"code","dcde490e":"code","7e95c338":"code","c075f261":"code","214591ee":"code","5630858e":"code","85782af4":"code","5b90f3e7":"code","b242ba45":"code","79b9fdef":"code","c6b1fa6a":"code","ea483167":"code","bad0cfad":"code","73891d56":"code","117c7c1f":"code","7c54bff4":"code","76c179b4":"code","0af87828":"code","b85ffbda":"code","575927ac":"code","a3f2fce0":"code","819eedd9":"code","cabd581b":"markdown","b97dc472":"markdown","137cadfa":"markdown","a5df0ffc":"markdown","ea326ddd":"markdown","55a293eb":"markdown","252f7158":"markdown","b7f816c4":"markdown","80b5c031":"markdown","bb57146f":"markdown","db51ccf6":"markdown","a85ad7a1":"markdown","f9ded08d":"markdown","f0e08090":"markdown","5fe04fac":"markdown","05396eb8":"markdown","a9aa0843":"markdown","1487e5f4":"markdown","c9efef5e":"markdown","d9ad8816":"markdown","7be9f67c":"markdown","6280c9d3":"markdown","db3e1510":"markdown","ae5f820e":"markdown","9fa0a195":"markdown","b5142236":"markdown","3e28d2b0":"markdown","5c7a2f7f":"markdown","472ef07e":"markdown","4057bd3b":"markdown","e67d8cea":"markdown","80f6c9eb":"markdown","3f1dfa44":"markdown","1e8a26d0":"markdown","6fd216f3":"markdown","93a5e097":"markdown","6477f6be":"markdown"},"source":{"f6fdfb78":"# Work with Data - the main Python libraries\nimport numpy as np\nimport pandas as pd\nimport pandas_profiling as pp\n\n# Visualization\nimport matplotlib.pyplot as plt\n\n# Preprocessing\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split, KFold, ShuffleSplit, GridSearchCV\n\n# Modeling\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nimport xgboost as xgb\nfrom xgboost.sklearn import XGBRegressor\n\n# Metrics\nfrom sklearn.metrics import r2_score","a08ed4e2":"# Download training data\ntrain = pd.read_csv('\/kaggle\/input\/ammonium-prediction-in-river-water\/train.csv')","d7f70446":"# Display the first 5 rows of the training dataframe.\ntrain.head()","ae3e2323":"# Information for training data\ntrain.info()","dc2f904e":"# Download test data\ntest = pd.read_csv('..\/input\/ammonium-prediction-in-river-water\/test.csv')","1ffa2090":"# Display the 7 last rows of the training dataframe\ntest.tail()","774fa000":"test.info()","e81dde3a":"# Select the stations with the most data in training dataset\ntrain = train.drop(['Id','3','4','5','6','7'], axis = 1)\ntrain = train.dropna().reset_index(drop=True)\ntrain.info()","fe1afbd2":"# Display the statistics for training data\ntrain.describe()","afed72e9":"# EDA with Pandas Profiling\npp.ProfileReport(train)","c54ee51e":"# Selecting a target featute and removing it from training dataset\ntarget = train.pop('target')","db7d380e":"# Select the stations with the most data in test dataset\ntest = test.drop(['Id','3','4','5','6','7'], axis = 1)\ntest = test.dropna().reset_index(drop=True)","d572d490":"# EDA with Pandas Profiling\npp.ProfileReport(test)","d4259d1a":"# Display basic information about the test data\ntest.info()","58fcf484":"# Standartization data\nscaler = StandardScaler()\ntrain = pd.DataFrame(scaler.fit_transform(train), columns = train.columns)\n\n# Display training data\ntrain","783dab63":"# Display the statistics for training data\ntrain.describe()","71f63e0c":"# Standartization data\nscaler = StandardScaler()\ntest = pd.DataFrame(scaler.fit_transform(test), columns = test.columns)\n# Display test\ntest","dcde490e":"# Display the statistics for training data\ntest.describe()","7e95c338":"# Training data splitting to new training (part of the all training) and validation data\ntrain_all = train.copy()\ntarget_all = target.copy()\ntrain, valid, target_train, target_valid = train_test_split(train_all, target_all, test_size=0.2, random_state=0)","c075f261":"# Display information about new training data\ntrain.info()","214591ee":"# Display information about validation data\nvalid.info()","5630858e":"# Cross-validation of training data with shuffle\ncv_train = ShuffleSplit(n_splits=5, test_size=0.2, random_state=0)","85782af4":"# Creation the dataframe with the resulting score of all models\nresult = pd.DataFrame({'model' : ['Decision Tree Regressor', 'Random Forest Regressor', 'XGBoost Regressor'], \n                       'train_score': 0, 'valid_score': 0})\nresult","5b90f3e7":"# Decision Tree Regressor\ndecision_tree = DecisionTreeRegressor()\nparam_grid = {'min_samples_leaf': [i for i in range(5,10)], 'max_depth': [i for i in range(3,12)]}\n\n# Training model\ndecision_tree_CV = GridSearchCV(decision_tree, param_grid=param_grid, cv=cv_train, verbose=False)\ndecision_tree_CV.fit(train, target_train)\nprint(decision_tree_CV.best_params_)\n\n# Prediction for training data\ny_train_decision_tree = decision_tree_CV.predict(train)\n\n# Accuracy of model\nr2_score_acc = round(r2_score(target_train, y_train_decision_tree)*100, 1)\nprint(f'Accuracy of DecisionTreeRegressor model training is {r2_score_acc}')\n\n# Save to result dataframe\nresult.loc[result['model'] == 'Decision Tree Regressor', 'train_score'] = r2_score_acc","b242ba45":"# Print rounded r2_score_acc to 2 decimal values after the text\ny_val_decision_tree = decision_tree_CV.predict(valid)\nr2_score_acc_valid = round(r2_score(target_valid, y_val_decision_tree)*100,1)\nresult.loc[result['model'] == 'Decision Tree Regressor', 'valid_score'] = r2_score_acc_valid\nprint(f'Accuracy of DecisionTreeRegressor model prediction for valid dataset is {r2_score_acc_valid}')","79b9fdef":"# Random Forest Regressor\nrf = RandomForestRegressor()\nparam_grid = {'n_estimators': [10, 100, 500], 'min_samples_leaf': [i for i in range(5,10)], \n              'max_features': ['auto'], 'max_depth': [i for i in range(4,6)], \n              'criterion': ['mse'], 'bootstrap': [False]}\n\n# Training model\nrf_CV = GridSearchCV(rf, param_grid=param_grid, cv=cv_train, verbose=False)\nrf_CV.fit(train, target_train)\nprint(rf_CV.best_params_)\n\n# Prediction for training data\ny_train_rf = rf_CV.predict(train)\n\n# Accuracy of model\nr2_score_acc = round(r2_score(target_train, y_train_rf)*100,1)\nprint(f'Accuracy of RandomForestRegressor model training is {r2_score_acc}')\n\n# Save to result dataframe\nresult.loc[result['model'] == 'Random Forest Regressor', 'train_score'] = r2_score_acc","c6b1fa6a":"# Print rounded r2_score_acc to 2 decimal values after the text\ny_val_rf = rf_CV.predict(valid)\nr2_score_acc_valid = round(r2_score(target_valid, y_val_rf)*100,1)\nresult.loc[result['model'] == 'Random Forest Regressor', 'valid_score'] = r2_score_acc_valid\nprint(f'Accuracy of RandomForestRegressor model prediction for valid dataset is {r2_score_acc_valid}')","ea483167":"# XGBoost Regressor\nxgb = XGBRegressor()\nparam_grid = {'max_depth': [5, 6, 7],\n              'min_child_weight': [4],\n              'subsample': [0.7],\n              'colsample_bytree': [0.7],\n              'n_estimators': [500]}\n\n# Training model\nxgb_CV = GridSearchCV(xgb, param_grid=param_grid, cv=cv_train, verbose=False)\nxgb_CV.fit(train, target_train)\nprint(xgb_CV.best_params_)\n# Prediction for training data\ny_train_xgb = xgb_CV.predict(train)\n# Accuracy of model\nr2_score_acc = round(r2_score(target_train, y_train_xgb)*100, 1)\nprint(f'Accuracy of XGBRegressor model training is {r2_score_acc}')\nresult.loc[result['model'] == 'XGBoost Regressor', 'train_score'] = r2_score_acc","bad0cfad":"# Print rounded r2_score_acc to 2 decimal values after the text\ny_val_xgb = xgb_CV.predict(valid)\nr2_score_acc_valid = round(r2_score(target_valid, y_val_xgb)*100,1)\nresult.loc[result['model'] == 'XGBoost Regressor', 'valid_score'] = r2_score_acc_valid\nprint(f'Accuracy of XGBRegressor model prediction for valid dataset is {r2_score_acc_valid}')","73891d56":"# Prediction of target for test data for all models\ny_test_decision_tree = decision_tree_CV.predict(test)\ny_test_rf = rf_CV.predict(test)\ny_test_xgb = xgb_CV.predict(test)","117c7c1f":"# Building plot for prediction for the training data \nx = np.arange(len(train))\nplt.figure(figsize=(16,10))\nplt.scatter(x, target_train, label = \"Target data\", color = 'g')\nplt.scatter(x, y_train_decision_tree, label = \"Decision Tree prediction\", color = 'b')\nplt.scatter(x, y_train_rf, label = \"Random Forest prediction\", color = 'y')\nplt.plot(x, np.full(len(train), 0.5), label = \"Maximum allowable value\", color = 'r')\nplt.title('Prediction for the training data')\nplt.legend(loc='best')\nplt.grid(True)","7c54bff4":"# Building plot for prediction for the valid data \nx = np.arange(len(valid))\nplt.figure(figsize=(16,10))\nplt.scatter(x, target_valid, label = \"Target data\", color = 'g')\nplt.scatter(x, y_val_decision_tree, label = \"Decision Tree prediction\", color = 'b')\nplt.scatter(x, y_val_rf, label = \"Random Forest prediction\", color = 'y')\nplt.scatter(x, y_val_xgb, label = \"XGBoost prediction\", color = 'm')\nplt.plot(x, np.full(len(valid), 0.5), label = \"Maximum allowable value\", color = 'r')\nplt.title('Prediction for the valid data')\nplt.legend(loc='best')\nplt.grid(True)","76c179b4":"# Building plot for prediction for the test data \nx = np.arange(len(test))\nplt.figure(figsize=(16,10))\nplt.scatter(x, y_test_decision_tree, label = \"Decision Tree prediction\", color = 'b')\nplt.scatter(x, y_test_rf, label = \"Random Forest prediction\", color = 'y')\nplt.scatter(x, y_test_xgb, label = \"XGBoost prediction\", color = 'c')\nplt.plot(x, np.full(len(test), 0.5), label = \"Maximum allowable value\", color = 'r')\nplt.title('Prediction for the test data')\nplt.legend(loc='best')\nplt.grid(True)","0af87828":"# Display results of modeling\nresult.sort_values(by=['valid_score', 'train_score'], ascending=False)","b85ffbda":"# Select models with minimal overfitting\nresult_best = result[(result['train_score'] - result['valid_score']).abs() < 5]\nresult_best.sort_values(by=['valid_score', 'train_score'], ascending=False)","575927ac":"# Select the best model\nresult_best.nlargest(1, 'valid_score')","a3f2fce0":"# Find a name of the best model (with maximal valid score)\nbest_model_name = result_best.loc[result_best['valid_score'].idxmax(result_best['valid_score'].max()), 'model']","819eedd9":"print(f'The best model is \"{best_model_name}\"')","cabd581b":"## 2. Download data<a class=\"anchor\" id=\"2\"><\/a>\n\n[Back to Table of Contents](#0.1)","b97dc472":"**TASK:** Display the statistics for test data","137cadfa":"**ADDITIONAL TASKS:** \n1. Add to dataframe result also calculated array: y_train, y_val.\n2. Creation the function with all commands and output information (in each section of this chapter 4) for all models:\n\n        result = get_model(train, valid, target_train, target_valid, model_name, param_grid, cv_train, result)","a5df0ffc":"**ADDITIONAL TASKS:** \n1. Set number of splitting = 5, 7, 10 and to compare of results.\n2. Try use another method for cross-validation of training data (without shuffle):\n\n        KFold(n_splits=5, shuffle=False, random_state=0)","ea326ddd":"**TASK:** Make EDA for the test dataset by Pandas Profiling","55a293eb":"## 1. Import libraries<a class=\"anchor\" id=\"1\"><\/a>\n\n[Back to Table of Contents](#0.1)","252f7158":"### It is recommended to start studying this course from notebooks:\n* [AI-ML-DS Training. L1T : Titanic - Decision Tree](https:\/\/www.kaggle.com\/vbmokin\/ai-ml-ds-training-l1t-titanic-decision-tree)\n* [AI-ML-DS Training. L1T : NH4 - linear regression](https:\/\/www.kaggle.com\/vbmokin\/ai-ml-ds-training-l1t-nh4-linear-regression)\n* [BOD prediction in river - 15 regression models](https:\/\/www.kaggle.com\/vbmokin\/bod-prediction-in-river-15-regression-models)\n\nand then move on to this notebook.","b7f816c4":"<a class=\"anchor\" id=\"0.1\"><\/a>\n## Table of Contents\n\n1. [Import libraries](#1)\n1. [Download data](#2)\n1. [EDA & FE & Preprocessing data](#3)\n    - [Statistics & FE](#3.1)\n    - [Data standartization](#3.2)\n    - [Training data splitting](#3.3)\n    - [Cross-validation of training data](#3.4)\n1. [Modeling](#4)\n    - [Decision Tree Regressor](#4.1)\n    - [Random Forest Regressor](#4.2)\n    - [XGBoost Regressor](#4.3)    \n1. [Test prediction](#5)\n1. [Results visualization](#6)\n1. [Select the best model](#6)","80b5c031":"**It is important to make sure** that all features in the training and test datasets:\n* do not have missing values (number of non-null values = number of entries of index) \n* all features have a numeric data type (int8, int16, int32, int64 or float16, float32, float64).","bb57146f":"## 5. Test prediction<a class=\"anchor\" id=\"5\"><\/a>\n\n[Back to Table of Contents](#0.1)","db51ccf6":"## 6. Visualization<a class=\"anchor\" id=\"6\"><\/a>\n\n[Back to Table of Contents](#0.1)","a85ad7a1":"### 3.3. Training data splitting<a class=\"anchor\" id=\"3.3\"><\/a>\n\n[Back to Table of Contents](#0.1)","f9ded08d":"**ADDITIONAL TASKS:** \n1. Add to dataframe result also calculated array: y_test.\n2. Add the line with XGBRegressor model prediction (train, valid, test take from the dataframe result).\n3. Creation the function with all commands and output information for all models (for type_plot = 'training', 'valid' or 'test'):\n\n        plot_prediction(result, type_plot='training')","f0e08090":"### 3.4. Cross-validation of training data<a class=\"anchor\" id=\"3.4\"><\/a>\n\n[Back to Table of Contents](#0.1)","5fe04fac":"### 3.1. Statistics & FE<a class=\"anchor\" id=\"3.1\"><\/a>\n\n[Back to Table of Contents](#0.1)","05396eb8":"**TASK:** Display information about validation data","a9aa0843":"**ADDITIONAL TASK:** Add the XGBRegressor model (the same commands as in 4.1 and 4.2 adapted to the library xgb). Please see example in the notebooks: \n* [BOD prediction in river - 15 regression models](https:\/\/www.kaggle.com\/vbmokin\/bod-prediction-in-river-15-regression-models)\n* [XGBRegressor with GridSearchCV](https:\/\/www.kaggle.com\/jayatou\/xgbregressor-with-gridsearchcv)","1487e5f4":"### 4.3. XGBoost Regressor<a class=\"anchor\" id=\"4.3\"><\/a>\n\n[Back to Table of Contents](#0.1)","c9efef5e":"**TASK:** Building plot for prediction for the valid data.","d9ad8816":"## Acknowledgements\n* [Data Science for tabular data: Advanced Techniques](https:\/\/www.kaggle.com\/vbmokin\/data-science-for-tabular-data-advanced-techniques)\n* [EDA for tabular data: Advanced Techniques](https:\/\/www.kaggle.com\/vbmokin\/eda-for-tabular-data-advanced-techniques)\n* [Datasets for river water quality prediction](https:\/\/www.kaggle.com\/vbmokin\/datasets-for-river-water-quality-prediction)\n* [Heart Disease - Automatic AdvEDA & FE & 20 models](https:\/\/www.kaggle.com\/vbmokin\/heart-disease-automatic-adveda-fe-20-models)\n* [BOD prediction in river - 15 regression models](https:\/\/www.kaggle.com\/vbmokin\/bod-prediction-in-river-15-regression-models)\n* [The system \"MONITORING AND ENVIRONMENTAL ASSESSMENT OF WATER RESOURCES OF UKRAINE\", State Agency of Water Resources of Ukraine](http:\/\/monitoring.davr.gov.ua\/EcoWaterMon\/GDKMap\/Index)","7be9f67c":"![image.png](attachment:image.png)\n* 1 - the source of the river (see at the station first on the left), \n* ....\n* 8 (target) - the place of water intake in Vinnytsia (see at the station in the lower right corner)","6280c9d3":"I hope you find this notebook useful and enjoyable.\n\nYour comments and feedback are most welcome.\n\n[Go to Top](#0)","db3e1510":"### 4.2. Random Forest Regressor<a class=\"anchor\" id=\"4.2\"><\/a>\n\n[Back to Table of Contents](#0.1)","ae5f820e":"**TASK:** Building plot for prediction for the test data.","9fa0a195":"### 4.1. Decision Tree Regressor<a class=\"anchor\" id=\"4.1\"><\/a>\n\n[Back to Table of Contents](#0.1)","b5142236":"## 4. Modeling<a class=\"anchor\" id=\"4\"><\/a>\n\n[Back to Table of Contents](#0.1)","3e28d2b0":"### Possible Tasks:\n\n* Analysis of data dependences, including EDA.\n\n* Prediction the target data (water quaity in the target station) with the highest accuracy.\n\n* Analysis of impact on the prediction accuracy in target station from the different number of stations (1, 2, ... 7).","5c7a2f7f":"The analysis showed that many values are only available in stations 1 and 2, while others have much less data. I propose select only these two stations.","472ef07e":"### 3.2. Data standartization<a class=\"anchor\" id=\"3.2\"><\/a>\n\n[Back to Table of Contents](#0.1)","4057bd3b":"**ADDITIONAL TASK:** Try use RobustScaler or MinMaxScaler instead of StandardScaler and to analyze what is the difference for accuracy of models will be below.","e67d8cea":"## 7. Select the best model <a class=\"anchor\" id=\"7\"><\/a>\n\n[Back to Table of Contents](#0.1)","80f6c9eb":"**ADDITIONAL TASK:** Try use other values in the parameter test_size above: 0.1, 0.15, 0.3, 0.5 and to analyze what is the difference for accuracy of models will be below.","3f1dfa44":"## Dataset [Ammonium prediction in river water](https:\/\/www.kaggle.com\/vbmokin\/ammonium-prediction-in-river-water)","1e8a26d0":"Dataset has data of the Ammonium ions concentration in river water (the maximum permissible value in Ukraine is 0.5 mg\/cub. dm).\n\nAmmonium ions (NH4) concentration is measured in mg\/cub. dm (ie milligrams in the cubic decimeter).\n\nDatasets has data of river water quality from 8 consecutive stations of the state water monitoring system for Pivdennyi Bug river (from the source of the river to the water intake of the city of Vinnytsia).\n\nTarget is a NH4 concentration in the river crossection with the water intake of the Vinnytsia city.\n\nData for the 1997-2019.","6fd216f3":"**TASK:** Standardize the test dataset with the same scaler and display it","93a5e097":"## 3. EDA & FE & Preprocessing data<a class=\"anchor\" id=\"3\"><\/a>\n\n[Back to Table of Contents](#0.1)","6477f6be":"### Map of the stations:\nhttp:\/\/monitoring.davr.gov.ua\/EcoWaterMon\/GDKMap\/Index\n\n![image.png](attachment:image.png)\n\nThe upper reaches of the Pivdennyi Bug river"}}