{"cell_type":{"08b4a7a4":"code","d2c3942c":"code","6b9c8d6f":"code","3c8904db":"code","d24d19f9":"code","9aaae77b":"code","1b72fe4a":"code","b40afdf0":"code","78335e83":"code","f409ede9":"code","4f3f5ee8":"code","0a35ad49":"code","f888c16c":"markdown","6fda733e":"markdown","05b7bddc":"markdown","578c3ab9":"markdown","5fbefa72":"markdown","528168e4":"markdown","8f268f92":"markdown","dc2dee67":"markdown"},"source":{"08b4a7a4":"import math, sys, functools, os, codecs, gc, time\nimport importlib\nfrom pathlib import Path\nimport numpy as np\nimport numpy.random as rd\nimport pandas as pd\nfrom datetime import  datetime as dt\nfrom collections import Counter\nimport traceback\n\nimport lightgbm as lgb\nfrom sklearn import preprocessing\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split, KFold, StratifiedKFold\nfrom sklearn.decomposition import LatentDirichletAllocation\n","d2c3942c":"\ndef current_time():\n    return dt.strftime(dt.now(),'%Y-%m-%d %H:%M:%S')\n\n\ndef pred(training, testing, y, lgbm_params, submit_file_name):\n    # Init predictions\n    oof_preds = np.zeros(training.shape[0])\n    sub_preds = np.zeros(testing.shape[0])\n\n    # Run KFold\n    clf_list = []\n    auc_list = []\n    feature_importance_df = pd.DataFrame()\n\n    folds = KFold(n_splits=FOLD_NUM, shuffle=True, random_state=SEED)\n    fold_iter = folds.split(training)\n\n    for n_fold, (trn_idx, val_idx) in enumerate(fold_iter):\n        print(f\"============ start {n_fold+1} fold training ============\")\n\n        X_train = training.iloc[trn_idx]\n        X_valid = training.iloc[val_idx]\n        y_train = np.log1p(y.iloc[trn_idx])\n        y_valid = np.log1p(y.iloc[val_idx])\n        \n        # Lgbm Dataset\n        lgtrain = lgb.Dataset(X_train, y_train,\n                          feature_name=training.columns.tolist())\n        lgvalid = lgb.Dataset(X_valid, y_valid,\n                          feature_name=training.columns.tolist())\n        # Start fitting\n        gbdt_reg = lgb.train(\n            lgbm_params,\n            lgtrain,\n            num_boost_round=20000,\n            early_stopping_rounds=100,\n            verbose_eval=100,\n            valid_sets=[lgtrain, lgvalid],\n            valid_names=['train', 'valid']\n        )\n\n        # evaluation\n        preds_train = gbdt_reg.predict(X_train)\n        rmse_train = np.sqrt(mean_squared_error(y_train, preds_train))\n        print(\"RMSE train cv{}:\".format(n_fold+1), rmse_train)\n\n        preds_valid = gbdt_reg.predict(X_valid)\n        rmse_valid = np.sqrt(mean_squared_error(y_valid, preds_valid))\n        print(\"RMSE valid cv{}:\".format(n_fold+1), rmse_valid)\n        oof_preds[val_idx] = np.expm1(preds_valid)\n\n        preds_test = gbdt_reg.predict(testing)\n        sub_preds += np.expm1(preds_test) \/ FOLD_NUM\n\n        del lgtrain\n        del lgvalid\n        gc.collect()\n    print(\"=+\"*30)\n    rmse_valid = np.sqrt(mean_squared_error(np.log1p(y), np.log1p(oof_preds)))\n    print(\"RMSE valid FULL cv{}:\".format(n_fold+1), rmse_valid)\n    \n    submit = pd.DataFrame(preds_test, columns=[\"target\"], index=testdex)\n    submit.to_csv(submit_file_name, index=True, header=True)\n    \n    return rmse_valid","6b9c8d6f":"# Parameter\nDATA_PATH = Path('..\/input\/')\nSEED = 71\nTOPIC_COMP = 20 # number of topics\nFOLD_NUM = 5\n#===================\n# Data Loading\nprint(\"data loading\")\nnrows = None #100 #None\n# parse_dates=[\"activation_date\"],\ntraining = pd.read_csv(str(DATA_PATH\/'train.csv'), index_col=\"ID\",  nrows=nrows)\ntraindex = training.index\ntesting = pd.read_csv(str(DATA_PATH\/'test.csv'), index_col=\"ID\",  nrows=nrows)\ntestdex = testing.index\nprint(\"loading finished.\")","3c8904db":"# target\ny = training.target.copy()\ndel training['target']\ny_log = np.log1p(y)","d24d19f9":"# removing duplicated columns, highly correlated columns\nduplicate_cols = [\"d60ddde1b\", \"912836770\", \"acc5b709d\", \"f8d75792f\", \"f333a5f60\"]\nhigh_corr_cols = [\n\"04e06920e\", \"e90ed19da\", \"7d72d6787\", \"4c256f2f9\", \"871617f50\",\n\"4a3248e89\", \"15bba6b9e\", \"3c29aec1e\", \"4647da55a\", \"083640132\",\n\"c4ed18259\", \"8966b4eee\", \"45713ba5f\", \"9a3f53be7\", \"1d0affea2\",\n\"2306bf286\", \"62d2a813b\", \"acd155589\", \"5d26f4d92\", \"28b21c1d2\",\n\"6dcac05e7\", \"bfde2aa61\", \"34d3974de\", \"598ae7ea9\", \"e851264a5\",\n\"5619c1297\", \"0c5eaf8a7\", \"bacadce94\", \"22b3e64c8\", \"224a28832\",\n\"07cfb1624\", \"8c1e20670\", \"49131c9e6\", \"1de1fda2c\", \"a04f3e320\",\n\"dcc181073\", \"2e648ce4b\", \"3c556d78f\", \"869a169f9\", \"99258443a\"]\n\nprint(\"training.shape\", training.shape)\nprint(\"testing.shape\", testing.shape)\n\ncols = [c for c in training.columns if c not in duplicate_cols + high_corr_cols]\ntraining = training[cols]\ntesting = testing[cols]\n\nprint(\"training.shape\", training.shape)\nprint(\"testing.shape\", testing.shape)\n\n############################################################################\nprint(\"data preprocessing\")\n\n# remove constant cols\nprint(\"training.shape\", training.shape)\nprint(\"testing.shape\", testing.shape)\nnuniq = training.nunique()\nconstant_col = nuniq.index[nuniq==1].values\ntraining.drop(constant_col, axis=1, inplace=True)\ntesting.drop(constant_col, axis=1, inplace=True)\nprint(\"removed constant columns: {}\".format(len(constant_col)))\n\nprint(\"training.shape\", training.shape)\nprint(\"testing.shape\", testing.shape)\ndf_cols = training.columns.tolist()\nprint(\"data preprocessing finished\")","9aaae77b":"lgbm_params = { \n        'objective': 'regression',\n        'num_leaves': 60,\n        'subsample': 0.61,\n        'colsample_bytree': 0.64,\n        'min_split_gain': 0.00259,\n        'reg_alpha': 0.00514,\n        'reg_lambda': 57.148,\n        'min_child_weight': 0.7117,\n        'verbose': -1,\n        'seed': 3,\n        'boosting_type': 'gbdt',\n        'max_depth': -1,\n        'learning_rate': 0.05,\n        'metric': 'rmse',\n    }","1b72fe4a":"valid_full_rmse = pred(training, testing, y, lgbm_params, \"submit_plane_pred.csv\")","b40afdf0":"print(f\"RMSLE with only plane features: {valid_full_rmse}\")","78335e83":"# Data concatenation\nprint(\"Data concatenation.\")\ndf = pd.concat([training, testing], axis=0)\n\n# Converting values to ratio of max value of each cols\nprint(\"convert df to percentage value\")\ndf_max = df.max(axis=0)\ndf_ratio = pd.DataFrame(np.divide(df.values, df_max[np.newaxis, :]))\ndf_ratio.index = df.index\ndf_ratio.columns = df.columns\n\n# topic model features\nprint(\"start topic modeling\")\n# assuming occurence count of a word(columns) for each column valuees. (i.e. \"5%\"\" means 5 times occuring on a document)\n# \u5404\u9805\u76eemax\u306b\u5bfe\u3059\u308b\u6bd4\u7387\u3092\u51fa\u73fe\u56de\u6570\u3068\u307f\u306a\u3057\u3066LDA\u306e\u5bfe\u8c61\u30c7\u30fc\u30bf\u3092\u7b97\u51fa(ex: max\u306b\u5bfe\u30575%\u306e\u5024\u306f5\u56de\u51fa\u73fe\u3057\u305f\u3068\u307f\u306a\u3059)\ndf_ratio_100 = (df_ratio.fillna(0)*100).astype(int)\n\n# Run LDA\nprint(current_time(), 'Run LDA')\nlda = LatentDirichletAllocation(n_components=TOPIC_COMP, max_iter=10, learning_method='online',\n                                learning_offset=50.,random_state=SEED).fit(df_ratio_100)\ntopic_result = lda.transform(df_ratio_100)\n\ndf_topic_result = pd.DataFrame(topic_result, columns=[\"{0}_{1:02d}\".format('tp', i) for i in range(TOPIC_COMP)])\ndf_topic_result.index = df_ratio_100.index\nprint(current_time(), \"finished topic modeling\")\n\ndf = df.join(df_topic_result, on=\"ID\", how='left')\n\n# split train and test dataset\ntraining = df.loc[traindex]\ntesting = df.loc[testdex]","f409ede9":"valid_full_rmse_with_tp = pred(training, testing, y, lgbm_params, \"submit_topic_pred.csv\")","4f3f5ee8":"print(f\"RMSLE with topic features: {valid_full_rmse_with_tp}\")","0a35ad49":"print(f\"ratio of RMSLE with topic feature vs plane{valid_full_rmse_with_tp\/valid_full_rmse}\")","f888c16c":"# No Topic Modela","6fda733e":"[RESULT]  \n* RMSLE with plane features:             **1.4181**\n* RMSLE with topic model features:  **1.3967**  \nThe score becomes better!!!\n\n[Number of Topic]  \nBasically, the more number of topics, the better score. However, there is trade off between training speed and good score.","05b7bddc":"RMSE is about **1.3967** with Topic Model features. The score is improving compared with only plane features!  \n\u30c8\u30d4\u30c3\u30af\u30e2\u30c7\u30eb\u3092\u9069\u7528\u3057\u305f\u6642\u306eValidation set\u306eRMSLE\u306f\u7d04**1.3967**\u3067\u3001\u7d20\u306e\u7279\u5fb4\u91cf\u306e\u307f\u306e\u6642\u3088\u308a\u30b9\u30b3\u30a2\u304c\u6539\u5584\u3057\u3066\u3044\u308b\uff01","578c3ab9":"Since the dataset of this competition is sparse data, so I regarded each row as a document and each column as a word for applying Topic Model.\nUnfortunately, since this idea was not applied since the leaked case was discovered,  this idea was not included in my final submission.\nIn this Kernel, I introduce this Topic Modeling idea which is effective to gain the score.\nFirst, applying LightGBM model to only the plane features to check original score of RMSLE, and then applying the topic model features to verify whether the score becomes better or not.\n  \n\u4eca\u56de\u306e\u30c7\u30fc\u30bf\u306f\u30b9\u30d1\u30fc\u30b9\u306a\u30c7\u30fc\u30bf\u3060\u3063\u305f\u305f\u3081\u3001\u5404\u884c\u3092\u6587\u66f8\u3068\u307f\u306a\u3057\u3001\u5404\u5217\u3092\u5358\u8a9e\u3068\u307f\u306a\u3057\u305f\u30c8\u30d4\u30c3\u30af\u30e2\u30c7\u30eb \u304c\u9069\u7528\u3067\u304d\u308b\u306e\u3067\u306f\u3068\u8003\u3048\u3001\u9069\u7528\u3057\u3066\u307f\u305f\u3002\u6b8b\u5ff5\u306a\u304c\u3089\u30ea\u30fc\u30af\u306e\u4ef6\u304c\u767a\u899a\u3057\u3066\u4ee5\u964d\u3001\u3053\u306e\u30a2\u30a4\u30c7\u30a3\u30a2\u3092\u9069\u7528\u3057\u3066\u3044\u306a\u304b\u3063\u305f\u305f\u3081\u3001\u6700\u7d42\u30b5\u30d6\u30df\u30c3\u30c8\u306b\u306f\u3053\u306e\u30a2\u30a4\u30c7\u30a3\u30a2\u3092\u76db\u308a\u8fbc\u3093\u3067\u3044\u306a\u304b\u3063\u305f\u304c\u3001\u3053\u306eKernel\u3067\u306f\u3001\u7d20\u306e\u30c7\u30fc\u30bf\u306b\u30c8\u30d4\u30c3\u30af\u30e2\u30c7\u30eb \u306e\u7279\u5fb4\u91cf\u3060\u3051\u8ffd\u52a0\u3057\u305f\u6642\u306b\u7cbe\u5ea6\u304c\u4e0a\u304c\u308b\u304b\u3069\u3046\u304b\u3092\u691c\u8a3c\u3059\u308b\u3002","5fbefa72":"# WithTopic Model\n### making topic model features","528168e4":"RMSE is about **1.4181** with only plane features.   \n\u30c8\u30d4\u30c3\u30af\u30e2\u30c7\u30eb \u3092\u9069\u7528\u3057\u306a\u304b\u3063\u305f\u6642\u306eValidation set\u306eRMSLE\u306f\u7d04 **1.4181**\n","8f268f92":"# Predict with LGBM","dc2dee67":"[Idea]  \nConverting the values of dataset as ratio with max values of each column,(i.e applying MinMaxScaler for each column). In that case, with multiplying 100 and cast with `int`, the values can be interpretating as a percent compared with max values. Then I regards the values as a word count.(Bag of words) So I can apply Topic Model for this dataset.\n\n[\u30a2\u30a4\u30c7\u30a3\u30a2]\n\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306e\u5404\u5024\u3092\u305d\u306e\u5217\u306e\u6700\u5927\u5024\u3067\u5272\u308b\u3053\u3068\u306b\u3088\u3063\u3066\u3001\u6700\u5927\u5024\u304b\u3089\u6bd4\u8f03\u3057\u305f\u6bd4\u7387\u306b\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u308b\u3002\u3053\u308c\u306b100\u3092\u304b\u3051\u3066int\u3067\u30ad\u30e3\u30b9\u30c8\u3059\u308b\u3053\u3068\u306b\u3088\u3063\u3066\u6574\u6570\u306b\u3059\u308b\u3002\u3053\u306e\u5024\u3092\u5404\u5217\u3092\u5358\u8a9e\u3068\u307f\u306a\u3057\u3066\u305d\u306e\u51fa\u73fe\u56de\u6570\u3068\u307f\u306a\u3059\u3053\u3068\u304c\u3067\u304d\u308b\u3002\u5404\u884c\u306f\u6587\u66f8\u306b\u898b\u7acb\u3066\u3066\u5404\u5217\u306b\u5bfe\u5fdc\u3059\u308b\u5358\u8a9e\u304c\u305d\u306e\u6587\u66f8\u306b\u4f55\u56de\u51fa\u73fe\u3057\u305f\u304b\u3068\u8003\u3048Bag of words\u3067\u3042\u308b\u3068\u3057\u3066\u53d6\u308a\u6271\u3048\u308b\u3002\u3053\u306e\u3088\u3046\u306a\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306b\u30c8\u30d4\u30c3\u30af\u30e2\u30c7\u30eb \u3092\u9069\u7528\u3057\u3066\u307f\u305f\u3002"}}