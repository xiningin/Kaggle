{"cell_type":{"bb827c4f":"code","64108b5a":"code","607ec139":"code","9b61a448":"code","7f920ddf":"code","cd25ea71":"code","fc30afac":"code","705b84e6":"code","5d04ae89":"code","77e7ea6a":"code","a467bc5d":"code","559531ae":"code","6e1c6a0e":"code","56573899":"code","b289700d":"code","156729ac":"code","f612d052":"code","d595984d":"code","b93878eb":"code","df21c6c8":"code","36068176":"code","cfbdb035":"code","1ea69afa":"code","a92e8697":"code","9cd595eb":"code","3255493b":"code","e1145683":"code","c852acbe":"code","77d9d87f":"code","6ae1410b":"code","06d70061":"code","809edf63":"code","06f02e93":"code","8557c5d9":"markdown","07912951":"markdown","ef987b42":"markdown","40c34f7b":"markdown","f6e8b1bf":"markdown","447cdec3":"markdown","f724754f":"markdown","88b0bb22":"markdown","4c921c35":"markdown","96bb9944":"markdown","aca72af8":"markdown","64bd62a1":"markdown","3531daf9":"markdown","3af8ef02":"markdown","204f50c7":"markdown","919fb360":"markdown","67ed51b8":"markdown","db896b09":"markdown","23ae0eb6":"markdown","9f20782d":"markdown","075b9372":"markdown","03bad681":"markdown","4fba81e4":"markdown","49a778ca":"markdown","0fb668d5":"markdown","a78236e3":"markdown"},"source":{"bb827c4f":"# Usual Imports\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nimport string\nimport random\nimport operator\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.decomposition import NMF, LatentDirichletAllocation, TruncatedSVD\nfrom statistics import *\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport concurrent.futures\nimport time\nimport pyLDAvis.sklearn\nfrom pylab import bone, pcolor, colorbar, plot, show, rcParams, savefig\nimport textstat\nimport warnings\nimport nltk\nwarnings.filterwarnings('ignore')\n\n%matplotlib inline\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Plotly based imports for visualization\nfrom plotly import tools\nimport plotly.plotly as py\nfrom plotly.offline import init_notebook_mode, iplot\ninit_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.figure_factory as ff\n\n# spaCy based imports\nimport spacy\nfrom spacy.lang.en.stop_words import STOP_WORDS\nfrom spacy.lang.en import English\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","64108b5a":"quora_train = pd.read_csv(\"..\/input\/train.csv\")\nquora_train.head()","607ec139":"# SpaCy Parser for questions\npunctuations = string.punctuation\nstopwords = list(STOP_WORDS)\n\nparser = English()\ndef spacy_tokenizer(sentence):\n    mytokens = parser(sentence)\n    mytokens = [ word.lemma_.lower().strip() if word.lemma_ != \"-PRON-\" else word.lower_ for word in mytokens ]\n    mytokens = [ word for word in mytokens if word not in stopwords and word not in punctuations ]\n    mytokens = \" \".join([i for i in mytokens])\n    return mytokens","9b61a448":"tqdm.pandas()\nsincere_questions = quora_train[\"question_text\"][quora_train[\"target\"] == 0].progress_apply(spacy_tokenizer)\ninsincere_questions = quora_train[\"question_text\"][quora_train[\"target\"] == 1].progress_apply(spacy_tokenizer)","7f920ddf":"# One function for all plots\ndef plot_readability(a,b,title,bins=0.1,colors=['#3A4750', '#F64E8B']):\n    trace1 = ff.create_distplot([a,b], [\"Sincere questions\",\"Insincere questions\"], bin_size=bins, colors=colors, show_rug=False)\n    trace1['layout'].update(title=title)\n    iplot(trace1, filename='Distplot')\n    table_data= [[\"Statistical Measures\",\"Sincere questions\",\"Insincere questions\"],\n                [\"Mean\",mean(a),mean(b)],\n                [\"Standard Deviation\",pstdev(a),pstdev(b)],\n                [\"Variance\",pvariance(a),pvariance(b)],\n                [\"Median\",median(a),median(b)],\n                [\"Maximum value\",max(a),max(b)],\n                [\"Minimum value\",min(a),min(b)]]\n    trace2 = ff.create_table(table_data)\n    iplot(trace2, filename='Table')","cd25ea71":"syllable_sincere = np.array(quora_train[\"question_text\"][quora_train[\"target\"] == 0].progress_apply(textstat.syllable_count))\nsyllable_insincere = np.array(quora_train[\"question_text\"][quora_train[\"target\"] == 1].progress_apply(textstat.syllable_count))\nplot_readability(syllable_sincere,syllable_insincere,\"Syllable Analysis\",5)\n    ","fc30afac":"lexicon_sincere = np.array(quora_train[\"question_text\"][quora_train[\"target\"] == 0].progress_apply(textstat.lexicon_count))\nlexicon_insincere = np.array(quora_train[\"question_text\"][quora_train[\"target\"] == 1].progress_apply(textstat.lexicon_count))\nplot_readability(lexicon_sincere,lexicon_insincere,\"Lexicon Analysis\",4,['#C65D17','#DDB967'])","705b84e6":"length_sincere = np.array(quora_train[\"question_text\"][quora_train[\"target\"] == 0].progress_apply(len))\nlength_insincere = np.array(quora_train[\"question_text\"][quora_train[\"target\"] == 1].progress_apply(len))\nplot_readability(length_sincere,length_insincere,\"Question Length\",40,['#C65D17','#DDB967'])","5d04ae89":"spw_sincere = np.array(quora_train[\"question_text\"][quora_train[\"target\"] == 0].progress_apply(textstat.avg_syllables_per_word))\nspw_insincere = np.array(quora_train[\"question_text\"][quora_train[\"target\"] == 1].progress_apply(textstat.avg_syllables_per_word))\nplot_readability(spw_sincere,spw_insincere,\"Average syllables per word\",0.2,['#8D99AE','#EF233C'])","77e7ea6a":"lpw_sincere = np.array(quora_train[\"question_text\"][quora_train[\"target\"] == 0].progress_apply(textstat.avg_letter_per_word))\nlpw_insincere = np.array(quora_train[\"question_text\"][quora_train[\"target\"] == 1].progress_apply(textstat.avg_letter_per_word))\nplot_readability(lpw_sincere,lpw_insincere,\"Average letters per word\",2,['#8491A3','#2B2D42'])","a467bc5d":"fre_sincere = np.array(quora_train[\"question_text\"][quora_train[\"target\"] == 0].progress_apply(textstat.flesch_reading_ease))\nfre_insincere = np.array(quora_train[\"question_text\"][quora_train[\"target\"] == 1].progress_apply(textstat.flesch_reading_ease))\nplot_readability(fre_sincere,fre_insincere,\"Flesch Reading Ease\",20)","559531ae":"fkg_sincere = np.array(quora_train[\"question_text\"][quora_train[\"target\"] == 0].progress_apply(textstat.flesch_kincaid_grade))\nfkg_insincere = np.array(quora_train[\"question_text\"][quora_train[\"target\"] == 1].progress_apply(textstat.flesch_kincaid_grade))\nplot_readability(fkg_sincere,fkg_insincere,\"Flesch Kincaid Grade\",4,['#C1D37F','#491F21'])","6e1c6a0e":"fog_sincere = np.array(quora_train[\"question_text\"][quora_train[\"target\"] == 0].progress_apply(textstat.gunning_fog))\nfog_insincere = np.array(quora_train[\"question_text\"][quora_train[\"target\"] == 1].progress_apply(textstat.gunning_fog))\nplot_readability(fog_sincere,fog_insincere,\"The Fog Scale (Gunning FOG Formula)\",4,['#E2D58B','#CDE77F'])","56573899":"ari_sincere = np.array(quora_train[\"question_text\"][quora_train[\"target\"] == 0].progress_apply(textstat.automated_readability_index))\nari_insincere = np.array(quora_train[\"question_text\"][quora_train[\"target\"] == 1].progress_apply(textstat.automated_readability_index))\nplot_readability(ari_sincere,ari_insincere,\"Automated Readability Index\",10,['#488286','#FF934F'])","b289700d":"cli_sincere = np.array(quora_train[\"question_text\"][quora_train[\"target\"] == 0].progress_apply(textstat.coleman_liau_index))\ncli_insincere = np.array(quora_train[\"question_text\"][quora_train[\"target\"] == 1].progress_apply(textstat.coleman_liau_index))\nplot_readability(cli_sincere,cli_insincere,\"The Coleman-Liau Index\",10,['#8491A3','#2B2D42'])","156729ac":"lwf_sincere = np.array(quora_train[\"question_text\"][quora_train[\"target\"] == 0].progress_apply(textstat.linsear_write_formula))\nlwf_insincere = np.array(quora_train[\"question_text\"][quora_train[\"target\"] == 1].progress_apply(textstat.linsear_write_formula))\nplot_readability(lwf_sincere,lwf_insincere,\"Linsear Write Formula\",2,['#8D99AE','#EF233C'])","f612d052":"dcr_sincere = np.array(quora_train[\"question_text\"][quora_train[\"target\"] == 0].progress_apply(textstat.dale_chall_readability_score))\ndcr_insincere = np.array(quora_train[\"question_text\"][quora_train[\"target\"] == 1].progress_apply(textstat.dale_chall_readability_score))\nplot_readability(dcr_sincere,dcr_insincere,\"Dale-Chall Readability Score\",1,['#C65D17','#DDB967'])","d595984d":"def consensus_all(text):\n    return textstat.text_standard(text,float_output=True)\n\ncon_sincere = np.array(quora_train[\"question_text\"][quora_train[\"target\"] == 0].progress_apply(consensus_all))\ncon_insincere = np.array(quora_train[\"question_text\"][quora_train[\"target\"] == 1].progress_apply(consensus_all))\nplot_readability(con_sincere,con_insincere,\"Readability Consensus based upon all the above tests\",2)","b93878eb":"def word_generator(text):\n    word = list(text.split())\n    return word\ndef bigram_generator(text):\n    bgram = list(nltk.bigrams(text.split()))\n    bgram = [' '.join((a, b)) for (a, b) in bgram]\n    return bgram\ndef trigram_generator(text):\n    tgram = list(nltk.trigrams(text.split()))\n    tgram = [' '.join((a, b, c)) for (a, b, c) in tgram]\n    return tgram\nsincere_words = sincere_questions.progress_apply(word_generator)\ninsincere_words = insincere_questions.progress_apply(word_generator)\nsincere_bigrams = sincere_questions.progress_apply(bigram_generator)\ninsincere_bigrams = insincere_questions.progress_apply(bigram_generator)\nsincere_trigrams = sincere_questions.progress_apply(trigram_generator)\ninsincere_trigrams = insincere_questions.progress_apply(trigram_generator)\n","df21c6c8":"color_brewer = ['#57B8FF','#B66D0D','#009FB7','#FBB13C','#FE6847','#4FB5A5','#8C9376','#F29F60','#8E1C4A','#85809B','#515B5D','#9EC2BE','#808080','#9BB58E','#5C0029','#151515','#A63D40','#E9B872','#56AA53','#CE6786','#449339','#2176FF','#348427','#671A31','#106B26','#008DD5','#034213','#BC2F59','#939C44','#ACFCD9','#1D3950','#9C5414','#5DD9C1','#7B6D49','#8120FF','#F224F2','#C16D45','#8A4F3D','#616B82','#443431','#340F09']\n\ndef ngram_visualizer(v,t):\n    X = v.values\n    Y = v.index\n    trace = [go.Bar(\n                y=Y,\n                x=X,\n                orientation = 'h',\n                marker=dict(color=color_brewer, line=dict(color='rgb(8,48,107)',width=1.5,)),\n                opacity = 0.6\n    )]\n    layout = go.Layout(\n        title=t,\n        margin = go.Margin(\n            l = 200,\n            r = 400\n        )\n    )\n\n    fig = go.Figure(data=trace, layout=layout)\n    iplot(fig, filename='horizontal-bar')\n    \ndef ngram_plot(ngrams,title):\n    ngram_list = []\n    for i in tqdm(ngrams.values, total=ngrams.shape[0]):\n        ngram_list.extend(i)\n    random.shuffle(color_brewer)\n    ngram_visualizer(pd.Series(ngram_list).value_counts()[:20],title)","36068176":"# Top Sincere words\nngram_plot(sincere_words,\"Top Sincere Words\")","cfbdb035":"# Top Insincere words\nngram_plot(insincere_words,\"Top Insincere Words\")","1ea69afa":"# Sincere Bigrams\nngram_plot(sincere_bigrams,\"Top 20 Sincere Bigrams\")","a92e8697":"# Insincere Bigrams\nngram_plot(insincere_bigrams,\"Top 20 Insincere Bigrams\")","9cd595eb":"# Sincere Trigrams\nngram_plot(sincere_trigrams,\"Top 20 Sincere Trigrams\")","3255493b":"# Insincere Trigrams\nngram_plot(insincere_trigrams,\"Top 20 Insincere Trigrams\")","e1145683":"vectorizer_sincere = CountVectorizer(min_df=5, max_df=0.9, stop_words='english', lowercase=True, token_pattern='[a-zA-Z\\-][a-zA-Z\\-]{2,}')\nsincere_questions_vectorized = vectorizer_sincere.fit_transform(sincere_questions)\nvectorizer_insincere = CountVectorizer(min_df=5, max_df=0.9, stop_words='english', lowercase=True, token_pattern='[a-zA-Z\\-][a-zA-Z\\-]{2,}')\ninsincere_questions_vectorized = vectorizer_insincere.fit_transform(insincere_questions)","c852acbe":"# Latent Dirichlet Allocation Model\nlda_sincere = LatentDirichletAllocation(n_components=10, max_iter=5, learning_method='online',verbose=True)\nsincere_lda = lda_sincere.fit_transform(sincere_questions_vectorized)\nlda_insincere = LatentDirichletAllocation(n_components=10, max_iter=5, learning_method='online',verbose=True)\ninsincere_lda = lda_insincere.fit_transform(insincere_questions_vectorized)","77d9d87f":"# Functions for printing keywords for each topic\ndef selected_topics(model, vectorizer, top_n=10):\n    for idx, topic in enumerate(model.components_):\n        print(\"Topic %d:\" % (idx))\n        print([(vectorizer.get_feature_names()[i], topic[i])\n                        for i in topic.argsort()[:-top_n - 1:-1]]) ","6ae1410b":"# Keywords for topics clustered by Latent Dirichlet Allocation\nprint(\"Sincere questions LDA Model:\")\nselected_topics(lda_sincere, vectorizer_sincere)","06d70061":"print(\"Insincere questions LDA Model:\")\nselected_topics(lda_insincere, vectorizer_insincere)","809edf63":"pyLDAvis.enable_notebook()\ndash = pyLDAvis.sklearn.prepare(lda_sincere, sincere_questions_vectorized, vectorizer_sincere, mds='tsne')\ndash","06f02e93":"pyLDAvis.enable_notebook()\ndash = pyLDAvis.sklearn.prepare(lda_insincere, insincere_questions_vectorized, vectorizer_insincere, mds='tsne')\ndash","8557c5d9":"## 6. Readability features\nThis basically returns the readability statistics for given text. ","07912951":"### Show your appreciation by UPVOTES. I welcome suggestions to improve this kernel further.","ef987b42":"# Visualizing LDA results of insincere questions","40c34f7b":"So, the sincere questions mostly deal with topics like education, relationships, work life, product reviews, elements of life etc.","f6e8b1bf":"# Statistics for the given data\nWe will use the ```textstat``` package by kaggler Shivam Bansal(@shivamb) for this purpose. ","447cdec3":"## 6.5 The Coleman-Liau Index\nReturns the grade level of the text using the Coleman-Liau Formula. This is a grade formula in that a score of 9.3 means that a ninth grader would be able to read the document.\n\nRead More: [Wikipedia](https:\/\/en.wikipedia.org\/wiki\/Coleman%E2%80%93Liau_index)","f724754f":"## Printing keywords","88b0bb22":"### 6.1 The Flesch Reading Ease formula\nThe following table can be helpful to assess the ease of readability in a document.\n###    Score\t- Difficulty\n* 90-100 - Very Easy\n* 80-89 -\tEasy\n* 70-79 -\tFairly Easy\n* 60-69 -\tStandard\n* 50-59 -\tFairly Difficult\n* 30-49 -\tDifficult\n* 0-29 -\tVery Confusing\n\nRead More: [Wikipedia](https:\/\/en.wikipedia.org\/wiki\/Flesch%E2%80%93Kincaid_readability_tests)","4c921c35":"## Count Vectorizers for the data","96bb9944":"# About the dataset\nAn existential problem for any major website today is how to handle toxic and divisive content. Quora wants to tackle this problem head-on to keep their platform a place where users can feel safe sharing their knowledge with the world.\n\nQuora is a platform that empowers people to learn from each other. On Quora, people can ask questions and connect with others who contribute unique insights and quality answers. A key challenge is to weed out insincere questions -- those founded upon false premises, or that intend to make a statement rather than look for helpful answers.\n\nIn this competition, Kagglers will develop models that identify and flag insincere questions. To date, Quora has employed both machine learning and manual review to address this problem. With your help, they can develop more scalable methods to detect toxic and misleading content.\n\nHere's your chance to combat online trolls at scale. Help Quora uphold their policy of \u201cBe Nice, Be Respectful\u201d and continue to be a place for sharing and growing the world\u2019s knowledge.","aca72af8":"## 2. Lexicon Analysis","64bd62a1":"## 4. Average syllables per word in a question ","3531daf9":"## Applying Latent Dirichlet Allocation(LDA) models","3af8ef02":"## 3. Question length","204f50c7":"## 1. Syllable Analysis","919fb360":"# What is topic-modelling?\nIn machine learning and natural language processing, a topic model is a type of statistical model for discovering the abstract \"topics\" that occur in a collection of documents. Topic modeling is a frequently used text-mining tool for discovery of hidden semantic structures in a text body. Intuitively, given that a document is about a particular topic, one would expect particular words to appear in the document more or less frequently: \"dog\" and \"bone\" will appear more often in documents about dogs, \"cat\" and \"meow\" will appear in documents about cats, and \"the\" and \"is\" will appear equally in both. A document typically concerns multiple topics in different proportions; thus, in a document that is 10% about cats and 90% about dogs, there would probably be about 9 times more dog words than cat words.\n\nThe \"topics\" produced by topic modeling techniques are clusters of similar words. A topic model captures this intuition in a mathematical framework, which allows examining a set of documents and discovering, based on the statistics of the words in each, what the topics might be and what each document's balance of topics is. It involves various techniques of dimensionality reduction(mostly non-linear) and unsupervised learning like LDA, SVD, autoencoders etc.","67ed51b8":"## 6.3 The Fog Scale (Gunning FOG Formula)\nReturns the FOG index of the given text. This is a grade formula in that a score of 9.3 means that a ninth grader would be able to read the document.\n\nRead More: [Wikipedia](https:\/\/en.wikipedia.org\/wiki\/Gunning_fog_index)","db896b09":"## 6.7 Dale-Chall Readability Score\nDifferent from other tests, since it uses a lookup table of the most commonly used 3000 English words. Thus it returns the grade level using the New Dale-Chall Formula.\n\n**Score** - **Understood by**\n* 4.9 or lower - average 4th-grade student or lower\n* 5.0\u20135.9\t- average 5th or 6th-grade student\n* 6.0\u20136.9\t- average 7th or 8th-grade student\n* 7.0\u20137.9\t- average 9th or 10th-grade student\n* 8.0\u20138.9\t- average 11th or 12th-grade student\n* 9.0\u20139.9\t- average 13th to 15th-grade (college) student\n\nRead More: [Wikipedia](https:\/\/en.wikipedia.org\/wiki\/Dale%E2%80%93Chall_readability_formula)","23ae0eb6":"## 5. Average letters per word in a question ","9f20782d":"The insincere questions, however deal with racism(there is a lot of mention of race here), homosexuality, politics, American elections, religion, terrotism and sex etc.\n\nBut words related to India and America appear in both sincere and insincere questions. Kaggle or quora, we are everywhere. ","075b9372":"## 6.8 Readability Consensus based upon all the above tests\nBased upon all the above tests, returns the estimated school grade level required to understand the text.","03bad681":"## 6.2 The Flesch-Kincaid Grade Level\nReturns the Flesch-Kincaid Grade of the given text. This is a grade formula in that a score of 9.3 means that a ninth grader would be able to read the document.\n\nRead More: [Wikipedia](https:\/\/en.wikipedia.org\/wiki\/Flesch%E2%80%93Kincaid_readability_tests)","4fba81e4":"## 6.4 Automated Readability Index\nReturns the ARI (Automated Readability Index) which outputs a number that approximates the grade level needed to comprehend the text.For example if the ARI is 6.5, then the grade level to comprehend the text is 6th to 7th grade.\n\nRead More: [Wikipedia](https:\/\/en.wikipedia.org\/wiki\/Automated_readability_index)","49a778ca":"# Visualizing LDA results of sincere questions with pyLDAvis","0fb668d5":"# N-gram analysis\nIn the fields of computational linguistics and probability, an n-gram is a contiguous sequence of n items from a given sample of text or speech. The items can be phonemes, syllables, letters, words or base pairs according to the application. The n-grams typically are collected from a text or speech corpus. When the items are words, n-grams may also be called shingles.","a78236e3":"## 6.6 Linsear Write Formula\nReturns the grade level of the text using the Coleman-Liau Formula. This is a grade formula in that a score of 9.3 means that a ninth grader would be able to read the document.\n\nRead More: [Wikipedia](https:\/\/en.wikipedia.org\/wiki\/Linsear_Write)"}}