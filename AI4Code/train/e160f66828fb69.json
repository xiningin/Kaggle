{"cell_type":{"fb99cb4f":"code","bb5b0a73":"code","2d6d791c":"code","ac830b0d":"code","0ad9e953":"code","0a3b943a":"code","c696ce12":"code","d18f59bb":"code","a47f1aca":"code","1ebbc1e6":"code","023e905c":"code","04f7562e":"code","afff931e":"code","0198eafb":"code","2345060b":"code","ac7014df":"code","863cda98":"code","370865b1":"code","6463976d":"code","409b80d1":"code","e08f05cc":"code","98c1411a":"code","6f8799d0":"code","5f55e0fe":"code","b2aa5061":"code","30c62f76":"code","fd1d4c8e":"code","f1d5268a":"code","e99f8599":"code","c8352195":"code","c4e8f870":"code","fc633be4":"code","31465ff7":"code","ee0c5187":"code","39bd48dc":"code","025a519d":"code","a01be1a4":"code","7da632a9":"code","368fee6b":"code","1fe49c26":"code","fa55fdf4":"code","a37b1f43":"code","84a1db09":"code","4de3077b":"code","c68d47b6":"code","98dcee6c":"code","268335a9":"code","67a3b634":"code","017654dc":"code","55c593fd":"code","3fa18171":"code","f1a3ebf6":"code","481b2724":"code","1b71575d":"code","96afc258":"code","e824da62":"code","93b30c71":"code","d33c0d16":"code","e7295d3e":"code","e1affb39":"code","145df3ec":"code","eeecc344":"code","e77e90b8":"code","5dd177cd":"code","3816cbbc":"code","ed384f0f":"code","a2d3daac":"code","7ebfe4a1":"code","51b05570":"code","db2832f8":"code","71b50947":"code","8b14a082":"code","4d973916":"markdown","0b6bbb6e":"markdown","deda07a2":"markdown","4800271e":"markdown","ae8eefc1":"markdown","016f5c52":"markdown","7e18dbf6":"markdown","86b95f3f":"markdown","3d92e872":"markdown","ad3df5ec":"markdown","5074d92e":"markdown","acfc41aa":"markdown","51d64280":"markdown","36529d22":"markdown","34190b4b":"markdown","eafccef5":"markdown","b4db123d":"markdown","b35e50ba":"markdown","fc632d5b":"markdown","c92f2475":"markdown","a1d3224b":"markdown","ceca8bb5":"markdown","7c795205":"markdown","5ce16301":"markdown","968652f4":"markdown","978db73d":"markdown","987e1756":"markdown","65ff3faa":"markdown","109552ca":"markdown","1b035643":"markdown","d14d2577":"markdown","ec0e9427":"markdown","6b013d40":"markdown","9f8a2055":"markdown","82cb375c":"markdown","11f36d39":"markdown","fe3e3459":"markdown","944be3cf":"markdown","0403d706":"markdown"},"source":{"fb99cb4f":"import time\nstart = time.time()\nimport os\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.feature_selection import VarianceThreshold\nfrom sklearn.preprocessing import StandardScaler\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\nfrom textblob import TextBlob\n\n#plotting\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n#statistics & econometrics\nimport statsmodels.tsa.api as smt\nimport statsmodels.api as sm\n\n#model fiiting and selection\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import make_scorer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import TimeSeriesSplit\nfrom sklearn.linear_model import Lasso, Ridge\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost.sklearn import XGBRegressor, XGBClassifier","bb5b0a73":"df = pd.read_csv(\"..\/input\/stocknews\/Combined_News_DJIA.csv\",low_memory=False,\n                    parse_dates=[0])\n#use date as index\ndf.index = df.Date\ndf = df.drop([\"Date\"], axis=1)\n\nfull_stock = pd.read_csv(\"..\/input\/stocknews\/DJIA_table.csv\",low_memory=False,\n                    parse_dates=[0])\nfull_stock.index = full_stock.Date\nfull_stock = full_stock.drop([\"Date\"], axis=1)\n\n#calculate the difference between opening and closing stock value\nfull_stock['Diff'] = full_stock.Close - full_stock.Open\nfl_cols = list(full_stock.columns)\nfl_cols = fl_cols[0:6]\nfull_stock = full_stock.drop(fl_cols, axis=1)\n\n#merge the headlines together into one text\nheadlines = []\nfor row in range(0,len(df.index)):\n   headlines.append(' '.join(str(x) for x in df.iloc[row,2:27]))\n\ndf['Headlines'] = headlines\n\n#add the difference between opening and closing stock value to the df - this will be the y variable\ndf = df.merge(full_stock, left_index=True, right_index=True)\n\n#drop the Label column and Top1-Top25\ndrop_it = df.columns\ndrop_it = drop_it[0:26]\ndf = df.drop(drop_it, axis=1)\n\n#show how the dataset looks like\ndf.head(5)","2d6d791c":"df = df.replace(np.nan, ' ', regex=True)\n\n#sanity check\ndf.isnull().sum().sum()","ac830b0d":"df = df.replace('b\\\"|b\\'|\\\\\\\\|\\\\\\\"|\\'', '', regex=True)\ndf = df.replace('[0-9]', '', regex=True)\ndf.head(5)","0ad9e953":"df.shape","0a3b943a":"def ts_train_test_split(X, y, test_size):\n    \"\"\"\n        Perform train-test split with respect to time series structure\n    \"\"\"\n    \n    # get the index after which test set starts\n    test_index = int(len(X)*(1-test_size))\n    \n    X_train = X.iloc[:test_index]\n    y_train = y.iloc[:test_index]\n    X_test = X.iloc[test_index:]\n    y_test = y.iloc[test_index:]\n    \n    return X_train, X_test, y_train, y_test","c696ce12":"X = df.copy()\nX = X.drop(['Diff'],axis=1)\ny = df.Diff","d18f59bb":"mean = np.mean(y)\nsd = np.std(y)\ny = (y-mean)\/sd","a47f1aca":"X_train, X_test, y_train, y_test = ts_train_test_split(X, y, test_size = 0.1)\n\n#remove first 7 rows of training set to acocunt for rows that will be removed later due to lagging\nX_train = X_train.drop(X_train.index[[np.arange(0,7)]])\ny_train = y_train[7:len(y_train)]\n\n\n#save the train-test indeces\ntest_idx = y_test.index","1ebbc1e6":"Anakin = SentimentIntensityAnalyzer()\n\nAnakin.polarity_scores(\" \")","023e905c":"def detect_subjectivity(text):\n    return TextBlob(text).sentiment.subjectivity\n\ndetect_subjectivity(\" \") #should return 0","04f7562e":"start_vect=time.time()\nprint(\"ANAKIN: 'Intializing the process..'\")\ncol=\"Headlines\"\n\n\ndf[col] = df[col].astype(str) # Make sure data is treated as a string\ndf[col+'_pos']= df[col].apply(lambda x:Anakin.polarity_scores(x)['pos'])\ndf[col+'_neg']= df[col].apply(lambda x:Anakin.polarity_scores(x)['neg'])\ndf[col+'_comp']= df[col].apply(lambda x:Anakin.polarity_scores(x)['compound'])\ndf[col+'_sub'] = df[col].apply(detect_subjectivity)\n    \nprint(\"VADER: Vaderization completed after %0.2f Minutes\"%((time.time() - start_vect)\/60))","afff931e":"from sklearn.feature_extraction.text import CountVectorizer\n\nngrammer = CountVectorizer(ngram_range=(1, 2), lowercase=True)\nn_grams_train = ngrammer.fit_transform(X_train.Headlines)\nn_grams_test = ngrammer.transform(X_test.Headlines)","0198eafb":"n_grams_train.shape","2345060b":"#the text isn't required anymore\ndf = df.drop(col,axis=1)\ndf.head(5)","ac7014df":"fig1 = go.Figure()\nfig1.add_trace(go.Scatter(x=df.index, y=df.Diff,\n                    mode='lines'))\ntitle = []\ntitle.append(dict(xref='paper', yref='paper', x=0.0, y=1.05,\n                              xanchor='left', yanchor='bottom',\n                              text='Aug, 2008 - Jun, 2016',\n                              font=dict(family='Arial',\n                                        size=30,\n                                        color='rgb(37,37,37)'),\n                              showarrow=False))\nfig1.update_layout(xaxis_title='Date',\n                   yaxis_title='Difference between opening and closing value (in $)',\n                  annotations=title)\nfig1.show()","863cda98":"pairplot = sns.pairplot(df)","370865b1":"df.describe()","6463976d":"def unique_ratio (col):\n    return len(np.unique(col))\/len(col)\n\ncols = list(df.columns)\ncols = cols[1:len(cols)]\n\nur = []\nvar = []\nfor col in cols:\n    ur.append(unique_ratio(df[col]))\n    var.append(np.var(df[col]))\n    \nfeature_sel = pd.DataFrame({'Column': cols, \n              'Unique': ur,\n              'Variance': var})\nfeature_sel","409b80d1":"uniq_fig = go.Figure(data=go.Scatter(\n    x=feature_sel.Column,\n    y=feature_sel.Unique,\n    mode='markers'\n))\nuniq_fig.update_layout( yaxis_title='Unique ratio')\nuniq_fig.show()","e08f05cc":"var_fig = go.Figure(data=go.Scatter(\n    x=feature_sel.Column,\n    y=feature_sel.Variance,\n    mode='markers'\n))\nvar_fig.update_layout( yaxis_title='Variance')\nvar_fig.show()","98c1411a":"drop = ['Headlines_pos']\nclean_df = df.copy()\nclean_df = clean_df.drop(drop, axis=1)","6f8799d0":"lag_df = clean_df.copy()\nlag_df.head(3)","5f55e0fe":"to_lag = list(lag_df.columns)\nto_lag_7 = to_lag[0]\nto_lag_3 = to_lag[1:len(to_lag)]","b2aa5061":"#lagging text features two days back\nfor col in to_lag_3:\n    for i in range(1, 4):\n        new_name = col + ('_lag_{}'.format(i))\n        lag_df[new_name] = lag_df[col].shift(i)\n    \n#lagging closing values 7 days back\nfor i in range(1, 8):\n    new_name = to_lag_7 + ('_lag_{}'.format(i))\n    lag_df[new_name] = lag_df[to_lag_7].shift(i)","30c62f76":"lag_df.head(10)","fd1d4c8e":"lag_df = lag_df.drop(lag_df.index[[np.arange(0,7)]])\n\n#sanity check for NaNs\nlag_df.isnull().sum().sum()","f1d5268a":"lag_df.head(10)","e99f8599":"lag_df[\"Day\"] = lag_df.index.dayofweek\nlag_df[\"Month\"] = lag_df.index.month\nlag_df[\"Year\"] = lag_df.index.year\nlag_df[\"Quarter\"] = lag_df.index.quarter","c8352195":"# for time-series cross-validation set 5 folds \ntscv = TimeSeriesSplit(n_splits=5)","c4e8f870":"lag_df = lag_df.drop(['Diff'], axis=1)\nprint(min(test_idx))\nX_train = lag_df[lag_df.index < min(test_idx)]\nX_test = lag_df[lag_df.index >= min(test_idx)]","fc633be4":"#classifier 1 - already prepared from before\n#n_grams_train\n#n_grams_test\nX_train.columns\n#classifier 2 - the same as X_train\nX_train_c2 = X_train\nX_test_c2 = X_test\n\n#Econometric\ndrop_e = list(X_train.columns)\ndrop_e = drop_e[0:12]\nX_train_e = X_train.drop(drop_e, axis=1)\nX_test_e = X_test.drop(drop_e, axis=1)\n\n#NLP\nX_train_n = X_train[drop_e]\nX_test_n = X_test[drop_e]","31465ff7":"y_train_dir = []\nfor i in range(0,len(y_train)):\n    if y_train[i]<0: y_train_dir.append(0)\n    else: y_train_dir.append(1)\n        \ny_test_dir = []\nfor i in range(0,len(y_test)):\n    if y_test[i]<0: y_test_dir.append(0)\n    else: y_test_dir.append(1)","ee0c5187":"from sklearn.metrics import balanced_accuracy_score\nscorer_class = make_scorer(balanced_accuracy_score)","39bd48dc":"class_perf = pd.DataFrame(columns=['Model','Acc', 'SD'])","025a519d":"from sklearn.naive_bayes import MultinomialNB\nNB = MultinomialNB()\n\nnb_param = {'alpha': list(np.arange(0,1,0.01))}\nsearch_nb = GridSearchCV(estimator=NB,\n                          param_grid = nb_param,\n                          scoring = scorer_class,\n                          cv = tscv,\n                          n_jobs=4,\n                          verbose=2\n                         )\nsearch_nb.fit(X=n_grams_train, y=y_train_dir)","a01be1a4":"nb_c1 = search_nb.best_estimator_\n\n#get cv results of the best model + confidence intervals\nfrom sklearn.model_selection import cross_val_score\ncv_score = cross_val_score(nb_c1, n_grams_train, y_train_dir, cv=tscv, scoring=scorer_class)\nclass_perf = class_perf.append({'Model':'NB_c1', 'Acc':np.mean(cv_score), 'SD':(np.std(cv_score))}, ignore_index=True)\nnb_c1","7da632a9":"from sklearn.linear_model import LogisticRegression\nlr = LogisticRegression(class_weight='balanced') #using l2\n\nlr_param = {'C': list(np.arange(0.1,1,0.1))}\nsearch_lr = GridSearchCV(estimator=lr,\n                          param_grid = lr_param,\n                          scoring = scorer_class,\n                          cv = tscv,\n                          n_jobs=4,\n                          verbose=2\n                         )\nsearch_lr.fit(n_grams_train, y_train_dir)","368fee6b":"lr_c1 = search_lr.best_estimator_\n\n#get cv results of the best model + confidence intervals\nfrom sklearn.model_selection import cross_val_score\ncv_score = cross_val_score(lr_c1, n_grams_train, y_train_dir, cv=tscv, scoring=scorer_class)\nclass_perf = class_perf.append({'Model':'Logistic Regression_c1', 'Acc':np.mean(cv_score), 'SD':(np.std(cv_score))}, ignore_index=True)\nlr_c1","1fe49c26":"scaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)","fa55fdf4":"from sklearn.naive_bayes import GaussianNB\nnb_c2 = GaussianNB()\n\nnb_c2.fit(X=X_train, y=y_train_dir)\n\n#get cv results of the best model + confidence intervals\nfrom sklearn.model_selection import cross_val_score\ncv_score = cross_val_score(nb_c2, X_train, y_train_dir, cv=tscv, scoring=scorer_class)\nclass_perf = class_perf.append({'Model':'NB_c2', 'Acc':np.mean(cv_score), 'SD':(np.std(cv_score))}, ignore_index=True)\nnb_c2","a37b1f43":"lr = LogisticRegression(class_weight='balanced') #using l2\n\nlr_param = {'C': list(np.arange(0.1,1,0.1))}\nsearch_lr = GridSearchCV(estimator=lr,\n                          param_grid = lr_param,\n                          scoring = scorer_class,\n                          cv = tscv,\n                          n_jobs=4,\n                          verbose=2\n                         )\nsearch_lr.fit(X_train, y_train_dir)\n\nlr_c2 = search_lr.best_estimator_\n\n#get cv results of the best model + confidence intervals\nfrom sklearn.model_selection import cross_val_score\ncv_score = cross_val_score(lr_c2, X_train, y_train_dir, cv=tscv, scoring=scorer_class)\nclass_perf = class_perf.append({'Model':'Logistic Regression_c2', 'Acc':np.mean(cv_score), 'SD':(np.std(cv_score))}, ignore_index=True)\nlr_c2","84a1db09":"lr_c2","4de3077b":"print(class_perf)","c68d47b6":"from sklearn.model_selection import cross_val_predict\nstack = LogisticRegression(class_weight='balanced')\n\nstack_train = pd.DataFrame(pd.DataFrame(columns=['nb_c2', 'lr_c2']))\nstack_train['nb_c2'] = nb_c2.predict(X_train)\nstack_train['lr_c2'] = lr_c2.predict(X_train)\n\nstack_test = pd.DataFrame(pd.DataFrame(columns=['nb_c2', 'lr_c2']))\nstack_test['nb_c2'] = nb_c2.predict(X_test)\nstack_test['lr_c2'] = lr_c2.predict(X_test)\n\n\nstack.fit(stack_train, y_train_dir)\n\npred_dir_train = cross_val_predict(stack, stack_train, y_train_dir)\npred_dir_test = stack.predict(stack_test)\n\n#add the stack to the classifier performance table\ncv_score = cross_val_score(stack, stack_train, y_train_dir, cv=tscv, scoring=scorer_class)\nclass_perf = class_perf.append({'Model':'Stack', 'Acc':np.mean(cv_score), 'SD':(np.std(cv_score))}, ignore_index=True)","98dcee6c":"class_perf","268335a9":"X_train_n.loc[:,'Direction'] = pred_dir_train\nX_test_n.loc[:,'Direction'] = pred_dir_test","67a3b634":"def mape(y_true, y_pred): \n    return np.mean(np.abs((y_true - y_pred) \/ y_true)) * 100\nscorer = make_scorer(mean_squared_error)\nscaler = StandardScaler()","017654dc":"def plotCoef(model,train_x):\n    \"\"\"\n        Plots sorted coefficient values of the model\n    \"\"\"\n    \n    coefs = pd.DataFrame(model.coef_, train_x.columns)\n    coefs.columns = [\"coef\"]\n    coefs[\"abs\"] = coefs.coef.apply(np.abs)\n    coefs = coefs.sort_values(by=\"abs\", ascending=False).drop([\"abs\"], axis=1)\n    \n    plt.figure(figsize=(15, 7))\n    coefs.coef.plot(kind='bar')\n    plt.grid(True, axis='y')\n    plt.hlines(y=0, xmin=0, xmax=len(coefs), linestyles='dashed');","55c593fd":"econ_perf = pd.DataFrame(columns=['Model','MSE', 'SD'])","3fa18171":"ridge_param = {'model__alpha': list(np.arange(1,10,0.1))}\nridge = Ridge(max_iter=5000)\npipe = Pipeline([\n    ('scale', scaler),\n    ('model', ridge)])\nsearch_ridge = GridSearchCV(estimator=pipe,\n                          param_grid = ridge_param,\n                          scoring = scorer,\n                          cv = tscv,\n                          n_jobs=4,\n                          verbose=2\n                         )\nsearch_ridge.fit(X_train_e, y_train)","f1a3ebf6":"ridge_e = search_ridge.best_estimator_\n\n#get cv results of the best model + confidence intervals\nfrom sklearn.model_selection import cross_val_score\ncv_score = cross_val_score(ridge_e, X_train_e, y_train, cv=tscv, scoring=scorer)\necon_perf = econ_perf.append({'Model':'Ridge', 'MSE':np.mean(cv_score), 'SD':(np.std(cv_score))}, ignore_index=True)\nridge_e","481b2724":"plotCoef(ridge_e['model'], X_train_e)","1b71575d":"coefs = ridge_e['model'].coef_\nridge_coefs = pd.DataFrame({'Coef': coefs,\n                           'Name': list(X_train_e.columns)})\nridge_coefs[\"abs\"] = ridge_coefs.Coef.apply(np.abs)\nridge_coefs = ridge_coefs.sort_values(by=\"abs\", ascending=False).drop([\"abs\"], axis=1)\nridge_coefs","96afc258":"rf_param = {'model__n_estimators': [10, 100, 300],\n            'model__max_depth': [10, 20, 30, 40],\n            'model__min_samples_split': [2, 5, 10],\n            'model__min_samples_leaf': [1, 2, 3],\n            'model__max_features': [\"auto\", 'sqrt']}\n\nrf = RandomForestRegressor()\npipe = Pipeline([\n    ('scale', scaler),\n    ('model', rf)])\ngridsearch_rf = GridSearchCV(estimator=pipe,\n                          param_grid = rf_param,\n                          scoring = scorer,\n                          cv = tscv,\n                          n_jobs=4,\n                          verbose=3\n                         )\ngridsearch_rf.fit(X_train_e, y_train)","e824da62":"rf_e = gridsearch_rf.best_estimator_\n\n#get cv results of the best model + confidence intervals\ncv_score = cross_val_score(rf_e, X_train_e, y_train, cv=tscv, scoring=scorer)\necon_perf = econ_perf.append({'Model':'RF', 'MSE':np.mean(cv_score), 'SD':(np.std(cv_score))}, ignore_index=True)","93b30c71":"xgb_param = {'model__lambda': list(np.arange(0.1,3, 0.1)), #L2 regularisation\n             'model__alpha': list(np.arange(0.1,3, 0.1)),  #L1 regularisation\n            }\n\nxgb = XGBRegressor(booster='gblinear', feature_selector='shuffle', objective='reg:squarederror')\n\npipe = Pipeline([\n    ('scale', scaler),\n    ('model', xgb)])\ngridsearch_xgb = GridSearchCV(estimator=pipe,\n                          param_grid = xgb_param,\n                          scoring = scorer,\n                          cv = tscv,\n                          n_jobs=4,\n                          verbose=3\n                         )\ngridsearch_xgb.fit(X_train_e, y_train)","d33c0d16":"xgb_e = gridsearch_xgb.best_estimator_\n\n#get cv results of the best model + confidence intervals\ncv_score = cross_val_score(xgb_e, X_train_e, y_train, cv=tscv, scoring=scorer)\necon_perf = econ_perf.append({'Model':'XGB', 'MSE':np.mean(cv_score), 'SD':(np.std(cv_score))}, ignore_index=True)\nxgb_e","e7295d3e":"plotCoef(xgb_e['model'], X_train_e)\n\ncoefs = xgb_e['model'].coef_\nxgb_coefs = pd.DataFrame({'Coef': coefs,\n                           'Name': list(X_train_e.columns)})\nxgb_coefs[\"abs\"] = xgb_coefs.Coef.apply(np.abs)\nxgb_coefs = xgb_coefs.sort_values(by=\"abs\", ascending=False).drop([\"abs\"], axis=1)\nxgb_coefs","e1affb39":"print(econ_perf)\necon_fig = px.scatter(econ_perf, x=\"Model\", y='MSE', color='Model', error_y=\"SD\")\necon_fig.show()","145df3ec":"nlp_perf = pd.DataFrame(columns=['Model','MSE', 'SD'])","eeecc344":"ridge_param = {'model__alpha': list(np.arange(1,10,0.1))}\nridge = Ridge(max_iter=5000)\npipe = Pipeline([\n    ('scale', scaler),\n    ('model', ridge)\n])\nsearch_ridge = GridSearchCV(estimator=pipe,\n                          param_grid = ridge_param,\n                          scoring = scorer,\n                          cv = tscv,\n                          n_jobs=4\n                         )\nsearch_ridge.fit(X_train_n, y_train)","e77e90b8":"ridge_n = search_ridge.best_estimator_\n\n#get cv results of the best model + confidence intervals\ncv_score = cross_val_score(ridge_n, X_train_n, y_train, cv=tscv, scoring=scorer)\nnlp_perf = nlp_perf.append({'Model':'Ridge', 'MSE':np.mean(cv_score), 'SD':(np.std(cv_score))}, ignore_index=True)\nridge_n","5dd177cd":"plotCoef(ridge_n['model'], X_train_n)\n\ncoefs = ridge_n['model'].coef_\nridge_coefs = pd.DataFrame({'Coef': coefs,\n                           'Name': list(X_train_n.columns)})\nridge_coefs[\"abs\"] = ridge_coefs.Coef.apply(np.abs)\nridge_coefs = ridge_coefs.sort_values(by=\"abs\", ascending=False).drop([\"abs\"], axis=1)\nridge_coefs","3816cbbc":"rf_param = {'model__n_estimators': [10, 100, 300],\n            'model__max_depth': [10, 20, 30, 40],\n            'model__min_samples_split': [2, 5, 10],\n            'model__min_samples_leaf': [1, 2, 3],\n            'model__max_features': [\"auto\", 'sqrt']}\nrf = RandomForestRegressor()\npipe = Pipeline([\n    ('scale', scaler),\n    ('model', rf)])\ngridsearch_rf = GridSearchCV(estimator=pipe,\n                          param_grid = rf_param,\n                          scoring = scorer,\n                          cv = tscv,\n                          n_jobs=4,\n                          verbose=3\n                         )\ngridsearch_rf.fit(X_train_n, y_train)","ed384f0f":"rf_n = gridsearch_rf.best_estimator_\n\n#get cv results of the best model + confidence intervals\ncv_score = cross_val_score(rf_n, X_train_n, y_train, cv=tscv, scoring=scorer)\nnlp_perf = nlp_perf.append({'Model':'RF', 'MSE':np.mean(cv_score), 'SD':(np.std(cv_score))}, ignore_index=True)","a2d3daac":"xgb_param = {'model__lambda': list(np.arange(1,10, 1)), #L2 regularisation\n             'model__alpha': list(np.arange(1,10, 1)),  #L1 regularisation\n            }\nxgb = XGBRegressor(booster='gblinear', feature_selector='shuffle', objective='reg:squarederror')\n\npipe = Pipeline([\n    ('scale', scaler),\n    ('model', xgb)])\ngridsearch_xgb = GridSearchCV(estimator=pipe,\n                          param_grid = xgb_param,\n                          scoring = scorer,\n                          cv = tscv,\n                          n_jobs=4,\n                          verbose=3\n                         )\ngridsearch_xgb.fit(X_train_n, y_train)","7ebfe4a1":"xgb_n = gridsearch_xgb.best_estimator_\n\n#get cv results of the best model + confidence intervals\ncv_score = cross_val_score(xgb_n, X_train_n, y_train, cv=tscv, scoring=scorer)\nnlp_perf = nlp_perf.append({'Model':'XGB', 'MSE':np.mean(cv_score), 'SD':(np.std(cv_score))}, ignore_index=True)\nxgb_n","51b05570":"plotCoef(xgb_n['model'], X_train_n)\n\ncoefs = xgb_n['model'].coef_\nxgb_coefs = pd.DataFrame({'Coef': coefs,\n                           'Name': list(X_train_n.columns)})\nxgb_coefs[\"abs\"] = xgb_coefs.Coef.apply(np.abs)\nxgb_coefs = xgb_coefs.sort_values(by=\"abs\", ascending=False).drop([\"abs\"], axis=1)\nxgb_coefs","db2832f8":"print(nlp_perf)\n\nnlp_fig = px.scatter(nlp_perf, x=\"Model\", y='MSE', color='Model', error_y=\"SD\")\nnlp_fig.show()","71b50947":"prediction_compare = pd.DataFrame(pd.DataFrame(columns=['y_true', 'econ_r', 'econ_rf', 'econ_x', 'nlp_r', 'nlp_rf', 'nlp_x']))\nprediction_compare['y_true'] = y_test\nprediction_compare['econ_r'] = ridge_e.predict(X_test_e)\nprediction_compare['econ_rf'] = rf_e.predict(X_test_e)\nprediction_compare['econ_x'] = xgb_e.predict(X_test_e)\nprediction_compare['nlp_r'] = ridge_n.predict(X_test_n)\nprediction_compare['nlp_rf'] = rf_n.predict(X_test_n)\nprediction_compare['nlp_x'] = xgb_n.predict(X_test_n)\n\nprediction_compare.sample(5)","8b14a082":"class_perf.to_csv('class_perf.csv')\necon_perf.to_csv(\"econ_perf.csv\")\nnlp_perf.to_csv(\"nlp_perf.csv\")\nprediction_compare.to_csv('compare_predictions.csv')\n\nX_test = pd.DataFrame(data=X_test[1:,1:], \n                      index=X_test[1:,0],\n                      columns=X_test[0,1:])\nX_test.to_csv(\"X_test.csv\")","4d973916":"# 5. Lag the extracted features\nTo allow the models to look into the past, we'll add features which are essentially just copies of rows from n-steps back. In order to not create too many new features we'll add only features from 1 week prior to the current datapoint.","0b6bbb6e":"Both c2 models seem to be the most promissing, so we'll stack just these 2.","deda07a2":"Function for plotting coeficients of models (lasso and XGBoost)","4800271e":"Write a function to save the subjectivity score directly from TextBlob function's output. Subjectivity score might detect direct quotes in the headlines and positive stuff is rarely quoted in the headline.","ae8eefc1":"## Stack the classifiers with logistic regression\n\nFirst we evaluate all the classifiers and decide which to use for stacking.","016f5c52":"## Classifiers 2\n\n### Naive Bayes","7e18dbf6":"### Ridge","86b95f3f":"# Explorative Data Analysis","3d92e872":"# Sentiment and subjectivity score extraction\nNow I run the sentiment analysis extracting the compound score that goes from -0.5 (most negative) to 0.5 (most positive). I'm going to use the \"dirty\" texts in this part because VADER can utilize the information such as ALL CAPS, punctuation, etc. I'll also calculate the subjectivity of each headline using the TextBlob package.\n\nInitialise the VADER analyzer.","ad3df5ec":"# Time features","5074d92e":"In this process, rows with NAs were created. Unfortunately these rows will have to be removed since we simply don't have the data from the future.","acfc41aa":"Add these direction prediction to the data for NLP models","51d64280":"### Random Forest","36529d22":"### Logistic regression","34190b4b":"## Classifiers 1 - ngram","eafccef5":"### Ridge regression","b4db123d":"We need to convert the y variable to binary. 1=up or same, 0=down","b35e50ba":"### XGBoost","fc632d5b":"And define a scorer for classifiers, we'll use balanced accuracy since there's slight imbalance in the data, no-information rate is around 0.52.","c92f2475":"The cost function to minimize is mean squared error because this function assigns cost proportionally to the error size. The mean absolute percentage error will be used for plotting and easier interpretation. It's much easier to understand the errors of a model in terms of percentage.\nEach training set is scaled (normalized) independently to minimize data leakage.","a1d3224b":"# N-grams, n=1:2","ceca8bb5":"# Model training\nLet's train 3(+1) ML models. We'll do this in 3 stages. First, using the econometric features alone (7 lags of y). Second, train classifiers using n_grams and topics to predict direction. Third, including the information extracted from the headlines (compound, subjectivity and their lags) and direction of the movement predicted by the classifiers from second stage.\n\n**Models**\n- Ridge regression - punish model for using too many features but doesn't allow the coeficients drop to zero completely\n- SVM\n- XGBoost\n- Naive Bayes (only for n-grams and topics)\n\nWe'll score all models by mean squared error as it gives higher penalty to larger mistakes.\nAnd before each model training we'll standardize the training data.\n\nThe first step will be creating folds for cross-validation. We'll use the same folds for all models in order to allow for creating a meta-model. Since we're working with timeseries the folds cannot be randomly selected. Instead a fold will be a sequence of data so that we don't lose the time information.","7c795205":"# 8.1 Econometric models\nFirst let's train models using only the lags of the y variable (i.e. diff).","5ce16301":"And we also prepare the feature sets for each of the models. \n - Classifier 1 - n-grams\n - Classifier 2 - sentiment and lags of y\n - Econometric - using only lags of y\n - NLP - using only sentiment and subjectivity","968652f4":"Now, we're ready to fit the models","978db73d":"### Logistic regression","987e1756":"### XGBoost","65ff3faa":"Next we split the dataset into training and testing using the indeces saved at the begining.","109552ca":"# Data cleaning\n\n### NA treatment\nWe'll simply fill the NAs in the numerical features (Date, Close). \nIn the text features we'll fill the missing values with ''.","1b035643":"### Random Forest","d14d2577":"# Feature selection","ec0e9427":"### Remove the HTML tags and digits\nThere are several non-word tags in the headlines that would just bias the sentiment analysis so we need to remove them and replace with ''. This can be done with regex.","6b013d40":"### Naive Bayes","9f8a2055":"# Train-test split\nSome of the feature extraction methods require to use separate train-test sets.","82cb375c":"We have ~344k n-grams.","11f36d39":"Next we'll look at some descriptive statistics about the data.","fe3e3459":"## NLP models\nLet's try now predict the stock value using only information from the news headlines.","944be3cf":"### Save predictions to compare the performance and interpretation","0403d706":"# Classifiers to predict direction of stock change"}}