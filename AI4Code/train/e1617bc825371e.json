{"cell_type":{"5b2c9ebd":"code","8aee949e":"code","2748d68c":"code","07082047":"code","abcff98d":"code","d583d1cb":"code","eb0437b3":"code","8f51257a":"code","13a46feb":"code","6809df9c":"code","8ae1d03d":"code","9d885df1":"code","45fae227":"code","c5143975":"code","c137867e":"code","e5219fd5":"code","641a8414":"code","869e1d14":"code","62ff99da":"code","38365af4":"code","c84f1f58":"code","cd376571":"code","a43f1f6f":"code","9654ce0b":"code","b94aac9b":"code","bbc4bb95":"code","fc7a6cfe":"code","5d83b548":"code","2c0419d4":"code","3493fcaf":"code","0aa1e094":"code","42a89467":"code","43dacf98":"code","dee61f91":"code","8915e6b4":"code","6a0f55a4":"code","a43148e2":"code","435c8337":"code","b4dcb2be":"code","28d1df25":"code","02ca319d":"code","0fe4e23b":"code","c53e6f76":"code","91ecf1f3":"code","214b3b83":"code","e66c3230":"code","ae6f0c7b":"code","9ebe6a1e":"code","cdbdc647":"code","5e0cbcf3":"code","96210a4a":"code","c362a095":"code","48f48494":"code","2a1a4b6f":"code","2cb27692":"code","d6d63492":"code","44bf8c8f":"code","cb5ae197":"code","7225b9be":"code","97520bd6":"code","ff33156f":"code","795887a3":"code","a4f4efc3":"code","b3acba3e":"code","b8844e6e":"code","2122af64":"code","536c0dad":"code","25aeb459":"code","3f3b3b1b":"code","e5728b1e":"code","72a46f84":"code","fcb2afd0":"code","af65900a":"code","401d72a7":"code","034f6047":"code","49593e69":"code","7404b65b":"code","5aa050a7":"code","7f884ec9":"code","83a2655c":"code","0b43a052":"code","926995ce":"code","5c8395bb":"code","f40b7cc3":"code","52928dac":"code","832fd313":"code","82c800d5":"code","b6fe5764":"code","f28b8e76":"code","e4c835aa":"code","c9be9be8":"code","9139c4ca":"code","2f0bc748":"code","211f636e":"code","f568987b":"code","d654f112":"code","f3af11dc":"code","f5018096":"markdown","c39f232a":"markdown","97438fd6":"markdown","4ffc6410":"markdown","46da68e8":"markdown","65d4b199":"markdown","f07766f6":"markdown","4714fa38":"markdown","f02732c4":"markdown","287dc11c":"markdown","47feb64a":"markdown","5980a448":"markdown","236a1fd7":"markdown","c6876d10":"markdown","40df54d2":"markdown","2816b40f":"markdown","8a6a7686":"markdown","09f0cdd2":"markdown","e6c46e4e":"markdown","93b1d8e9":"markdown","aa762936":"markdown","fcfa800a":"markdown","a52bdba3":"markdown","97f6ddf3":"markdown","06e0ea42":"markdown","6a4e5091":"markdown","c47769d2":"markdown","60432b46":"markdown","600bb68c":"markdown","90002cb4":"markdown","c1312ddf":"markdown","a94d27e8":"markdown","40a07db9":"markdown","60c7df92":"markdown","d8d09f3a":"markdown","765ce772":"markdown","c5449d55":"markdown","f0a82d02":"markdown","ae5cd9e6":"markdown","e652aa67":"markdown","897f1c99":"markdown","12f68c96":"markdown","72521bea":"markdown","b67fc283":"markdown","3bf8ba99":"markdown","71bb4394":"markdown","e4d6088f":"markdown","81a213f7":"markdown","c55d34cf":"markdown","ae4010ea":"markdown","0db0faea":"markdown","d4215274":"markdown","105fcac6":"markdown","d7eeceb2":"markdown","39d68570":"markdown","3b771f2c":"markdown","c3d24e65":"markdown","adffc9e4":"markdown","a51e740b":"markdown","8dcc9673":"markdown","37d545db":"markdown","98dc7a5c":"markdown","ff74b20f":"markdown","4b7c1b63":"markdown","9a0c81d4":"markdown","1a57c6a3":"markdown","d94afcc5":"markdown","1e8db3dd":"markdown","71319ff4":"markdown","9898f8f7":"markdown","641a0b35":"markdown","52a28aaa":"markdown","db115770":"markdown","cabdf6eb":"markdown","fad47eea":"markdown","f94e0fab":"markdown","3b3b29c6":"markdown"},"source":{"5b2c9ebd":"#https:\/\/anaconda.org\/conda-forge\/mlxtend\n#conda install -c conda-forge mlxtend","8aee949e":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib\n\nimport matplotlib.pyplot as plt\nfrom scipy.stats import skew\nfrom scipy.stats.stats import pearsonr\n\n\n%config InlineBackend.figure_format = 'retina' #set 'png' here when working on notebook\n%matplotlib inline\n\nfrom xgboost import XGBRegressor\n\nfrom sklearn.linear_model import LinearRegression, SGDRegressor, Lasso, Ridge, LassoCV, RidgeCV ,ElasticNetCV, PassiveAggressiveRegressor, HuberRegressor, TheilSenRegressor\n\nfrom sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor, AdaBoostRegressor, GradientBoostingRegressor\n\nfrom mlxtend.regressor import StackingCVRegressor\n\nimport missingno as msno","2748d68c":"#Let's import our data\n\ntrain = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/train.csv\")\ntest = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/test.csv\")","07082047":"train.head()","abcff98d":"test.head()","d583d1cb":"train.select_dtypes(exclude = 'object').columns","eb0437b3":"train.select_dtypes(include = 'object').columns","8f51257a":"num_correlation = train.select_dtypes(exclude='object').corr()\nplt.figure(figsize=(20,20))\nplt.title('High Correlation')\nsns.heatmap(num_correlation > 0.8, annot=True, square=True);","13a46feb":"corr = num_correlation.corr()\nprint(corr['SalePrice'].sort_values(ascending=False))","6809df9c":"corr[corr['SalePrice']>0.3].index","8ae1d03d":"train.drop(columns=['GarageArea','TotRmsAbvGrd','GarageYrBlt','1stFlrSF'],axis=1,inplace=True) \ntest.drop(columns=['GarageArea','TotRmsAbvGrd','GarageYrBlt','1stFlrSF'],axis=1,inplace=True)","9d885df1":"# Useless Columns...\ntrain=train.drop(columns=['Street','Utilities']) \ntest=test.drop(columns=['Street','Utilities']) ","45fae227":"data = pd.concat([train['SalePrice'], train['OverallQual']], axis=1)\nf, ax = plt.subplots(figsize=(8, 6))\nfig = sns.boxplot(x=train['OverallQual'], y=\"SalePrice\", data=data)\nfig.axis(ymin=0, ymax=800000);","c5143975":"data = pd.concat([train['SalePrice'], train['TotalBsmtSF']], axis=1)\ndata.plot.scatter(x='TotalBsmtSF', y='SalePrice', alpha=0.3, ylim=(0,800000));","c137867e":"data = pd.concat([train['SalePrice'], train['LotArea']], axis=1)\ndata.plot.scatter(x='LotArea', y='SalePrice', alpha=0.3, ylim=(0,800000));","e5219fd5":"data = pd.concat([train['SalePrice'], train['GrLivArea']], axis=1)\ndata.plot.scatter(x='GrLivArea', y='SalePrice', alpha=0.3, ylim=(0,800000));","641a8414":"sns.set_style(\"white\")\nsns.set_color_codes(palette='deep')\nf, ax = plt.subplots(figsize=(8, 7))\n#Check the new distribution \nsns.distplot(train['SalePrice'], color=\"b\");\nax.xaxis.grid(False)\nax.set(ylabel=\"Frequency\")\nax.set(xlabel=\"SalePrice\")\nax.set(title=\"SalePrice distribution\")\nsns.despine(trim=True, left=True)\nplt.show()","869e1d14":"matplotlib.rcParams['figure.figsize'] = (12.0, 6.0)\nprices = pd.DataFrame({\"price\":train[\"SalePrice\"], \"log(price + 1)\":np.log1p(train[\"SalePrice\"])})\nprices.hist(bins = 50);","62ff99da":"#log transform the target:\ntrain[\"SalePrice\"] = np.log1p(train[\"SalePrice\"])","38365af4":"# Remove outliers\ntrain.drop(train[(train['OverallQual']<5) & (train['SalePrice']>200000)].index, inplace=True)\ntrain.drop(train[(train['GrLivArea']>4500) & (train['SalePrice']<300000)].index, inplace=True)\ntrain.reset_index(drop=True, inplace=True)","c84f1f58":"all_data = pd.concat((train.loc[:,'MSSubClass':'SaleCondition'],\n                      test.loc[:,'MSSubClass':'SaleCondition']))","cd376571":"((all_data.isnull().sum().sort_values(ascending = False)\/len(all_data))*100).head(30)","a43f1f6f":"def handle_missing(features):\n    # the data description states that NA refers to typical ('Typ') values\n    features['Functional'] = features['Functional'].fillna('Typ')\n    # Replace the missing values in each of the columns below with their mode\n    features['Electrical'] = features['Electrical'].fillna(\"SBrkr\")\n    features['KitchenQual'] = features['KitchenQual'].fillna(\"TA\")\n    features['Exterior1st'] = features['Exterior1st'].fillna(features['Exterior1st'].mode()[0])\n    features['Exterior2nd'] = features['Exterior2nd'].fillna(features['Exterior2nd'].mode()[0])\n    features['SaleType'] = features['SaleType'].fillna(features['SaleType'].mode()[0])\n    features['MSZoning'] = features.groupby('MSSubClass')['MSZoning'].transform(lambda x: x.fillna(x.mode()[0]))\n    features['GarageCars'] = features['GarageCars'].fillna(0)\n    \n    # the data description stats that NA refers to \"No Pool\"\n    features[\"PoolQC\"] = features[\"PoolQC\"].fillna(\"None\")\n\n    # Replacing the missing values with None\n    for col in ['GarageType', 'GarageFinish', 'GarageQual', 'GarageCond']:\n        features[col] = features[col].fillna('None')\n    # NaN values for these categorical basement features, means there's no basement\n    for col in ('BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2'):\n        features[col] = features[col].fillna('None')\n        \n    # Group the by neighborhoods, and fill in missing value by the median LotFrontage of the neighborhood\n    features['LotFrontage'] = features.groupby('Neighborhood')['LotFrontage'].transform(lambda x: x.fillna(x.median()))\n\n    # We have no particular intuition around how to fill in the rest of the categorical features\n    # So we replace their missing values with None\n    objects = []\n    for i in features.columns:\n        if features[i].dtype == object:\n            objects.append(i)\n    features.update(features[objects].fillna('None'))\n        \n    # And we do the same thing for numerical features, but this time with 0s\n    numeric_dtypes = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    numeric = []\n    for i in features.columns:\n        if features[i].dtype in numeric_dtypes:\n            numeric.append(i)\n    features.update(features[numeric].fillna(0))    \n    return features\n\nall_data = handle_missing(all_data)","9654ce0b":"all_data.isnull().sum().sort_values(ascending = False).head(5)","b94aac9b":"#log transform skewed numeric features:\nnumeric_feats = all_data.dtypes[all_data.dtypes != 'object'].index\n\n\nskewed_feats = train[numeric_feats].apply(lambda x: skew(x.dropna())) #compute skewness\nskewed_feats = skewed_feats[skewed_feats > 0.75]\nskewed_feats = skewed_feats.index","bbc4bb95":"skewed_feats","fc7a6cfe":"#skewed_feats = skewed_feats.drop(['BsmtFinSF2', 'LowQualFinSF', 'BsmtHalfBath', \n #                                 'KitchenAbvGr', 'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea', 'MiscVal'])\n\nfor x in skewed_feats:\n\n    print('Skewed features:', x)\n    print('\\n')\n    print('-'*10, '\\n')\n    sns.distplot(all_data[x], color=\"b\");\n    ax.xaxis.grid(False)\n    ax.set(ylabel=\"Frequency\")\n    ax.set(xlabel=\"SalePrice\")\n    ax.set(title=\"SalePrice distribution\")\n    plt.show();\n    print('-'*10, '\\n')","5d83b548":"all_data[skewed_feats] = np.log1p(all_data[skewed_feats])","2c0419d4":"#skewed_feats = skewed_feats.drop(['BsmtFinSF2', 'LowQualFinSF', 'BsmtHalfBath', \n #                                 'KitchenAbvGr', 'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea', 'MiscVal'])\n\n\nfor x in skewed_feats:\n\n    print('Skewed features:', x)\n    print('\\n')\n    print('-'*10, '\\n')\n    sns.distplot(all_data[x], color=\"b\");\n    ax.xaxis.grid(False)\n    ax.set(ylabel=\"Frequency\")\n    ax.set(xlabel=\"SalePrice\")\n    ax.set(title=\"SalePrice distribution\")\n    plt.show();\n    print('-'*10, '\\n')","3493fcaf":"all_data = all_data.replace({'MSSubClass': {20: 'SubClass_20', 30: 'SubClass_30',40: 'SubClass_40',\n45: 'SubClass_45',50: 'SubClass_50',60: 'SubClass_60',70: 'SubClass_70',\n75: 'SubClass_75',80: 'SubClass_80',85: 'SubClass_85',90: 'SubClass_90',\n120: 'SubClass_120',150: 'SubClass_150',160: 'SubClass_160',180: 'SubClass_180',\n190: 'SubClass_190'}})","0aa1e094":"all_data = all_data.replace({'MSSubClass': {20: 'SubClass_20', 30: 'SubClass_30',40: 'SubClass_40',\n45: 'SubClass_45',50: 'SubClass_50',60: 'SubClass_60',70: 'SubClass_70',\n75: 'SubClass_75',80: 'SubClass_80',85: 'SubClass_85',90: 'SubClass_90',\n120: 'SubClass_120',150: 'SubClass_150',160: 'SubClass_160',180: 'SubClass_180',\n190: 'SubClass_190'}})","42a89467":"# Some of the non-numeric predictors are stored as numbers; convert them into strings \nall_data['MSSubClass'] = all_data['MSSubClass'].apply(str)\nall_data['YrSold'] = all_data['YrSold'].astype(str)\nall_data['MoSold'] = all_data['MoSold'].astype(str)","43dacf98":"from pandas.api.types import CategoricalDtype\n\n# Categorical values\n# Ordered\nall_data[\"BsmtCond\"] = pd.Categorical(all_data[\"BsmtCond\"], categories=['No','Po','Fa','TA','Gd','Ex'],ordered=True)\nall_data[\"BsmtExposure\"] = pd.Categorical(all_data[\"BsmtExposure\"], categories=['No','Mn','Av','Gd'],ordered=True)\nall_data[\"BsmtFinType1\"] = pd.Categorical(all_data[\"BsmtFinType1\"],categories=['No','Unf','LwQ','Rec','BLQ','ALQ','GLQ'],ordered=True)\nall_data[\"BsmtFinType2\"] = pd.Categorical(all_data[\"BsmtFinType2\"],categories=['No','Unf','LwQ','Rec','BLQ','ALQ','GLQ'],ordered=True)\nall_data[\"BsmtQual\"] = pd.Categorical(all_data[\"BsmtQual\"],categories=['No','Po','Fa','TA','Gd','Ex'],ordered=True)\nall_data[\"ExterCond\"] = pd.Categorical(all_data[\"ExterCond\"],categories=['Po','Fa','TA','Gd','Ex'],ordered=True)\nall_data[\"ExterQual\"] = pd.Categorical(all_data[\"ExterQual\"],categories=['Po','Fa','TA','Gd','Ex'],ordered=True)\nall_data[\"Fence\"] = pd.Categorical(all_data[\"Fence\"],categories=['No','MnWw','GdWo','MnPrv','GdPrv'],ordered=True)\nall_data[\"FireplaceQu\"] = pd.Categorical(all_data[\"FireplaceQu\"],categories=['No','Po','Fa','TA','Gd','Ex'],ordered=True)\nall_data[\"Functional\"] = pd.Categorical(all_data[\"Functional\"],categories=['Sal','Sev','Maj2','Maj1','Mod','Min2','Min1','Typ'],ordered=True)\nall_data[\"GarageCond\"] = pd.Categorical(all_data[\"GarageCond\"],categories=['No','Po','Fa','TA','Gd','Ex'],ordered=True)\nall_data[\"GarageFinish\"] = pd.Categorical(all_data[\"GarageFinish\"],categories=['No','Unf','RFn','Fin'],ordered=True)\nall_data[\"GarageQual\"] = pd.Categorical(all_data[\"GarageQual\"],categories=['No','Po','Fa','TA','Gd','Ex'],ordered=True)\nall_data[\"HeatingQC\"] = pd.Categorical(all_data[\"HeatingQC\"],categories=['Po','Fa','TA','Gd','Ex'],ordered=True)\nall_data[\"KitchenQual\"] = pd.Categorical(all_data[\"KitchenQual\"],categories=['Po','Fa','TA','Gd','Ex'],ordered=True)\nall_data[\"PavedDrive\"] = pd.Categorical(all_data[\"PavedDrive\"],categories=['N','P','Y'],ordered=True)\nall_data[\"PoolQC\"] = pd.Categorical(all_data[\"PoolQC\"],categories=['No','Fa','TA','Gd','Ex'],ordered=True)","dee61f91":"all_data['TotalSF']=all_data['TotalBsmtSF']  + all_data['2ndFlrSF']","8915e6b4":"all_data['TotalBath']=all_data['BsmtFullBath'] + all_data['FullBath'] + (0.5*all_data['BsmtHalfBath']) + (0.5*all_data['HalfBath'])","6a0f55a4":"all_data['YrBltAndRemod']=all_data['YearBuilt']+(all_data['YearRemodAdd']\/2)","a43148e2":"all_data['Porch_SF'] = (all_data['OpenPorchSF'] + all_data['3SsnPorch'] + all_data['EnclosedPorch'] + all_data['ScreenPorch'] + all_data['WoodDeckSF'])","435c8337":"all_data['Has2ndfloor'] = all_data['2ndFlrSF'].apply(lambda x: 1 if x > 0 else 0)\nall_data['HasBsmt'] = all_data['TotalBsmtSF'].apply(lambda x: 1 if x > 0 else 0)\nall_data['HasFirePlace'] = all_data['Fireplaces'].apply(lambda x: 1 if x > 0 else 0)\nall_data['Has2ndFlr']=all_data['2ndFlrSF'].apply(lambda x: 1 if x > 0 else 0)\nall_data['HasBsmt']=all_data['TotalBsmtSF'].apply(lambda x: 1 if x > 0 else 0)","b4dcb2be":"all_data['LotArea'] = all_data['LotArea'].astype(np.int64)\nall_data['MasVnrArea'] = all_data['MasVnrArea'].astype(np.int64)","28d1df25":"all_data = pd.get_dummies(all_data,drop_first=True)\n\nall_data.shape","02ca319d":"X = all_data[:train.shape[0]]\ny = train.SalePrice\n    \nTest = all_data[train.shape[0]:]","0fe4e23b":"print(X.shape)\nprint(Test.shape)\nprint(y.shape)","c53e6f76":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y,random_state = 0,test_size=0.25)","91ecf1f3":"from sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import r2_score\nfrom sklearn import linear_model\nimport math\nimport sklearn.metrics as sklm\n\nregr = linear_model.LinearRegression()\nregr.fit(X_train,y_train)\ny_pred = regr.predict(X_train)","214b3b83":"residuals = y_train.values-y_pred\nmean_residuals = np.mean(residuals)\nprint(\"Mean of Residuals {}\".format(mean_residuals))","e66c3230":"p = sns.scatterplot(y_pred,residuals)\nplt.xlabel('y_pred\/predicted values')\nplt.ylabel('Residuals')\nplt.ylim(-2.5,2.5)\nplt.xlim(11,13)\np = sns.lineplot([10,26],[0,0],color='blue')\np = plt.title('Residuals vs fitted values plot for homoscedasticity check')","ae6f0c7b":"import statsmodels.stats.api as sms\nfrom statsmodels.compat import lzip\nname = ['F statistic', 'p-value']\ntest = sms.het_goldfeldquandt(residuals, X_train)\nlzip(name, test)","9ebe6a1e":"p = sns.distplot(residuals,kde=True)\np = plt.title('Normality of error terms\/residuals')","cdbdc647":"plt.figure(figsize=(30,30))  # on this line I just set the size of figure to 12 by 10.\np=sns.heatmap(train.corr(), annot=True,cmap='RdYlGn',square=True)  # seaborn has very simple solution for heatmap","5e0cbcf3":"from sklearn.linear_model import Ridge, RidgeCV, Lasso, ElasticNet, LassoCV, LassoLarsCV\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.svm import SVR\n\nimport sys\n\nif not sys.warnoptions:\n    import warnings\n    warnings.simplefilter(\"ignore\")","96210a4a":"def rmse_cv(model):\n    rmse = np.sqrt(-cross_val_score(model, X, y, scoring = \"neg_mean_squared_error\" , cv=5))\n    return (rmse)","c362a095":"scores = {}\n\nscores['xgboost'] = rmse_cv(XGBRegressor())\n\nscores['LinearRegression'] = rmse_cv(LinearRegression())\n\nscores['SGDRegressor'] = rmse_cv(SGDRegressor())\n\nscores['Lasso'] = rmse_cv(Lasso())\n\nscores['Ridge'] = rmse_cv(Ridge())\n\nscores['PassiveAggressiveRegressor'] = rmse_cv(PassiveAggressiveRegressor())\n\nscores['HuberRegressor'] = rmse_cv(HuberRegressor())\n\nscores['TheilSenRegressor'] = rmse_cv(TheilSenRegressor())\n\nscores['RandomForestRegressor'] = rmse_cv(RandomForestRegressor())\n\nscores['ExtraTreesRegressor'] = rmse_cv(ExtraTreesRegressor())\n\nscores['AdaBoostRegressor'] = rmse_cv(AdaBoostRegressor())\n\nscores['GradientBoostingRegressor'] = rmse_cv(GradientBoostingRegressor())\n\nscores['DecisionTreeRegressor'] = rmse_cv(DecisionTreeRegressor())\n\nscores['ElasticNet'] = rmse_cv(ElasticNet())\n\nscores['SVR'] = rmse_cv(SVR())\n","48f48494":"model_rmse = pd.DataFrame(scores).mean()\nmodel_rmse = model_rmse.sort_values(ascending=True)\nprint('Model scores\\n{}'.format(model_rmse))","2a1a4b6f":"model_ridge = Ridge()","2cb27692":"alphas = [0.05, 0.1, 0.3, 1, 3, 5, 10, 15, 30, 50, 75]\ncv_ridge = [rmse_cv(Ridge(alpha = alpha)).mean() for alpha in alphas]","d6d63492":"cv_ridge = pd.Series(cv_ridge, index = alphas)\ncv_ridge.plot(title = \"rmse mean in fonction of alpha\")\nplt.xlabel('alpha')\nplt.ylabel('rmse (mean)');","44bf8c8f":"cv_ridge.min()","cb5ae197":"model_ridge = Ridge(alpha = 5).fit(X_train, y_train)","7225b9be":"coef = pd.Series(model_ridge.coef_, index = X_train.columns)","97520bd6":"print(\"Ridge picked \" + str(sum(coef != 0)) + \" variables and eliminated the other \" +  str(sum(coef == 0)) + \" variables\")","ff33156f":"imp_coef = pd.concat([coef.sort_values().head(10),\n                    coef.sort_values().tail(10)])","795887a3":"matplotlib.rcParams['figure.figsize'] = (8.0, 10.0)\nimp_coef.plot(kind = \"barh\")\nplt.title(\"Coefficients in the Ridge Model\");","a4f4efc3":"\nmatplotlib.rcParams['figure.figsize'] = (6.0, 6.0)\n\npreds = pd.DataFrame({\"preds\":model_ridge.predict(X_train), \"true\":y_train})\npreds[\"residuals\"] = preds[\"true\"] - preds[\"preds\"]\npreds.plot(x = \"preds\", y = \"residuals\",kind = \"scatter\");","b3acba3e":"from sklearn.model_selection import train_test_split","b8844e6e":"X_tr, X_val, y_tr, y_val = train_test_split(X, y, random_state = 3)","2122af64":"my_model = XGBRegressor(n_estimators=1000, learning_rate=0.05)\nmy_model.fit(X_tr, y_tr, early_stopping_rounds=5, \n             eval_set=[(X_val, y_val)], verbose=False)","536c0dad":"my_model.get_params","25aeb459":"rmse_cv(XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n       colsample_bytree=1, gamma=0, importance_type='gain',\n       learning_rate=0.05, max_delta_step=0, max_depth=3,\n       min_child_weight=1, missing=None, n_estimators=1000, n_jobs=1,\n       nthread=None, objective='reg:linear', random_state=0, reg_alpha=0,\n       reg_lambda=1, scale_pos_weight=1, seed=None, silent=True,\n       subsample=1)).mean()","3f3b3b1b":"from sklearn.model_selection import GridSearchCV\n\n\nparam_grid={'n_estimators':[100], \n            'learning_rate': [0.1, 0.05, 0.02, 0.01],\n            'max_depth':[4,6], \n            'min_samples_leaf':[3,5,9,17],\n            'max_features':[1.0,0.3,0.1] }\n\nGB = GradientBoostingRegressor()\nclf = GridSearchCV(GB, param_grid, cv=5)\nclf.fit(X, y)","e5728b1e":"print(clf.best_params_)","72a46f84":"\nrmse_cv(GradientBoostingRegressor\n        (learning_rate= 0.1, max_depth= 4, max_features= 0.3, min_samples_leaf= 3, n_estimators= 100)).mean()","fcb2afd0":"#GradientBoostingRegressor \n\nGB = GradientBoostingRegressor(learning_rate= 0.1, max_depth= 4, max_features= 0.3, min_samples_leaf= 3, n_estimators= 100)\nGB.fit(X, y)\ngb_preds = np.expm1(GB.predict(Test))\n\n\n# XGBRegressor\n\nmodel_xgb = XGBRegressor(n_estimators=1000, learning_rate=0.05)\nmodel_xgb.fit(X_tr, y_tr, early_stopping_rounds=5, \n             eval_set=[(X_val, y_val)], verbose=False)\n\nxgb_preds = np.expm1(model_xgb.predict(Test))\n\n#RIDGE\n\nmodel_ridge = Ridge(alpha = 10)\nmodel_ridge.fit(X, y)\n\nridge_preds = np.expm1(model_ridge.predict(Test))","af65900a":"preds = 0.4*gb_preds + 0.3*xgb_preds + 0.3* ridge_preds","401d72a7":"solution = pd.DataFrame({\"id\":test.Id, \"SalePrice\":preds})\nsolution.to_csv(\"solution.csv\", index = False)","034f6047":"from sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import r2_score\nfrom sklearn import linear_model\nimport math\nimport sklearn.metrics as sklm\n\ndef function(model, model_name):\n\n    train = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/train.csv\")\n    test = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/test.csv\")\n    \n    #skew\n    train[\"SalePrice\"] = np.log1p(train[\"SalePrice\"])\n    \n    # Remove outliers\n    train.drop(train[(train['OverallQual']<5) & (train['SalePrice']>200000)].index, inplace=True)\n    train.drop(train[(train['GrLivArea']>4500) & (train['SalePrice']<300000)].index, inplace=True)\n    train.reset_index(drop=True, inplace=True)\n    \n    all_data = pd.concat((train.loc[:,'MSSubClass':'SaleCondition'],\n                      test.loc[:,'MSSubClass':'SaleCondition']))\n    \n    #handle missing value\n    all_data = handle_missing(all_data)\n    \n    #log transform skewed numeric features:\n    numeric_feats = all_data.dtypes[all_data.dtypes != \"object\"].index\n\n    skewed_feats = train[numeric_feats].apply(lambda x: skew(x.dropna())) #compute skewness\n    skewed_feats = skewed_feats[skewed_feats > 0.75]\n    skewed_feats = skewed_feats.index\n\n    all_data[skewed_feats] = np.log1p(all_data[skewed_feats])\n    \n    all_data = all_data.replace({'MSSubClass': {20: 'SubClass_20', 30: 'SubClass_30',40: 'SubClass_40',\n    45: 'SubClass_45',50: 'SubClass_50',60: 'SubClass_60',70: 'SubClass_70',\n    75: 'SubClass_75',80: 'SubClass_80',85: 'SubClass_85',90: 'SubClass_90',\n    120: 'SubClass_120',150: 'SubClass_150',160: 'SubClass_160',180: 'SubClass_180',\n    190: 'SubClass_190'}})\n    \n    all_data = all_data.replace({'MSSubClass': {20: 'SubClass_20', 30: 'SubClass_30',40: 'SubClass_40',\n    45: 'SubClass_45',50: 'SubClass_50',60: 'SubClass_60',70: 'SubClass_70',\n    75: 'SubClass_75',80: 'SubClass_80',85: 'SubClass_85',90: 'SubClass_90',\n    120: 'SubClass_120',150: 'SubClass_150',160: 'SubClass_160',180: 'SubClass_180',\n    190: 'SubClass_190'}})\n    \n    all_data['MSSubClass'] = all_data['MSSubClass'].apply(str)\n    all_data['YrSold'] = all_data['YrSold'].astype(str)\n    all_data['MoSold'] = all_data['MoSold'].astype(str)\n    \n    all_data[\"BsmtCond\"] = pd.Categorical(all_data[\"BsmtCond\"], categories=['No','Po','Fa','TA','Gd','Ex'],ordered=True)\n    all_data[\"BsmtExposure\"] = pd.Categorical(all_data[\"BsmtExposure\"], categories=['No','Mn','Av','Gd'],ordered=True)\n    all_data[\"BsmtFinType1\"] = pd.Categorical(all_data[\"BsmtFinType1\"],categories=['No','Unf','LwQ','Rec','BLQ','ALQ','GLQ'],ordered=True)\n    all_data[\"BsmtFinType2\"] = pd.Categorical(all_data[\"BsmtFinType2\"],categories=['No','Unf','LwQ','Rec','BLQ','ALQ','GLQ'],ordered=True)\n    all_data[\"BsmtQual\"] = pd.Categorical(all_data[\"BsmtQual\"],categories=['No','Po','Fa','TA','Gd','Ex'],ordered=True)\n    all_data[\"ExterCond\"] = pd.Categorical(all_data[\"ExterCond\"],categories=['Po','Fa','TA','Gd','Ex'],ordered=True)\n    all_data[\"ExterQual\"] = pd.Categorical(all_data[\"ExterQual\"],categories=['Po','Fa','TA','Gd','Ex'],ordered=True)\n    all_data[\"Fence\"] = pd.Categorical(all_data[\"Fence\"],categories=['No','MnWw','GdWo','MnPrv','GdPrv'],ordered=True)\n    all_data[\"FireplaceQu\"] = pd.Categorical(all_data[\"FireplaceQu\"],categories=['No','Po','Fa','TA','Gd','Ex'],ordered=True)\n    all_data[\"Functional\"] = pd.Categorical(all_data[\"Functional\"],categories=['Sal','Sev','Maj2','Maj1','Mod','Min2','Min1','Typ'],ordered=True)\n    all_data[\"GarageCond\"] = pd.Categorical(all_data[\"GarageCond\"],categories=['No','Po','Fa','TA','Gd','Ex'],ordered=True)\n    all_data[\"GarageFinish\"] = pd.Categorical(all_data[\"GarageFinish\"],categories=['No','Unf','RFn','Fin'],ordered=True)\n    all_data[\"GarageQual\"] = pd.Categorical(all_data[\"GarageQual\"],categories=['No','Po','Fa','TA','Gd','Ex'],ordered=True)\n    all_data[\"HeatingQC\"] = pd.Categorical(all_data[\"HeatingQC\"],categories=['Po','Fa','TA','Gd','Ex'],ordered=True)\n    all_data[\"KitchenQual\"] = pd.Categorical(all_data[\"KitchenQual\"],categories=['Po','Fa','TA','Gd','Ex'],ordered=True)\n    all_data[\"PavedDrive\"] = pd.Categorical(all_data[\"PavedDrive\"],categories=['N','P','Y'],ordered=True)\n    all_data[\"PoolQC\"] = pd.Categorical(all_data[\"PoolQC\"],categories=['No','Fa','TA','Gd','Ex'],ordered=True)\n    \n    all_data['TotalSF']=all_data['TotalBsmtSF']  + all_data['2ndFlrSF']\n    \n    all_data['TotalBath']=all_data['BsmtFullBath'] + all_data['FullBath'] + (0.5*all_data['BsmtHalfBath']) + (0.5*all_data['HalfBath'])\n    \n    all_data['YrBltAndRemod']=all_data['YearBuilt']+(all_data['YearRemodAdd']\/2)\n    \n    all_data['Porch_SF'] = (all_data['OpenPorchSF'] + all_data['3SsnPorch'] + all_data['EnclosedPorch'] + all_data['ScreenPorch'] + all_data['WoodDeckSF'])\n    \n    all_data['Has2ndfloor'] = all_data['2ndFlrSF'].apply(lambda x: 1 if x > 0 else 0)\n    all_data['HasBsmt'] = all_data['TotalBsmtSF'].apply(lambda x: 1 if x > 0 else 0)\n    all_data['HasFirePlace'] = all_data['Fireplaces'].apply(lambda x: 1 if x > 0 else 0)\n    all_data['Has2ndFlr']=all_data['2ndFlrSF'].apply(lambda x: 1 if x > 0 else 0)\n    all_data['HasBsmt']=all_data['TotalBsmtSF'].apply(lambda x: 1 if x > 0 else 0)\n    \n    all_data['LotArea'] = all_data['LotArea'].astype(np.int64)\n    all_data['MasVnrArea'] = all_data['MasVnrArea'].astype(np.int64)\n\n    all_data = pd.get_dummies(all_data).reset_index(drop=True)\n    \n    # Remove any duplicated column names\n    all_data = all_data.loc[:,~all_data.columns.duplicated()]\n    \n    X = all_data[:train.shape[0]]\n    y = train.SalePrice\n    \n    Test = all_data[train.shape[0]:]\n\n\n    from sklearn.model_selection import train_test_split\n    X_train, X_test, y_train, y_test = train_test_split(X, y,test_size = .3, random_state=0)\n    \n\n    #modeling\n    \n    if model == Ridge:\n   \n        model=model(alpha = 10).fit(X_train,y_train)\n\n        y_pred= model.predict(X_test)\n\n    \n        print(\"The  RMSE score achieved with {} is {}\".format(model_name, str(math.sqrt(sklm.mean_squared_error(y_test, y_pred)))))\n        \n        coef = pd.Series(model.coef_, index = X_train.columns)\n        \n        print()\n        print('Our most valuable features with {} are :'.format(model_name))\n        print()\n        print(coef.sort_values(ascending = False).head(10))\n        print()\n        print(coef.sort_values(ascending = False).tail(10))\n        \n        #submission\n        model_preds = np.expm1(model.predict(Test))\n        \n        solution = pd.DataFrame({\"id\":test.Id, \"SalePrice\":model_preds})\n        solution.to_csv(\"solution.csv\", index = False)\n        print()\n        print(\"Submission file created !\")\n\n        \n    \n    elif model == XGBRegressor:\n\n        model = model(n_estimators=1000, learning_rate=0.05).fit(X_train, y_train)\n        \n        y_pred=model.predict(X_test)\n        \n        print(\"The  RMSE score achieved with {} is {}\".format(model_name, str(math.sqrt(sklm.mean_squared_error(y_test, y_pred)))))\n        \n        \n        coef = pd.Series(model.feature_importances_, index = X_train.columns)\n        \n        print()\n        print('Our most valuable features with {} are :'.format(model_name))\n        print()\n        print(coef.sort_values(ascending = False).head(10))\n        \n        #submission\n        model_preds = np.expm1(model.predict(Test))\n        \n        solution = pd.DataFrame({\"id\":test.Id, \"SalePrice\":model_preds})\n        solution.to_csv(\"solution.csv\", index = False)\n        \n        print()\n        print(\"Submission file created !\")\n        \n    elif model == GradientBoostingRegressor:\n\n        model = model(learning_rate= 0.1, max_depth= 4, max_features= 0.3, min_samples_leaf= 3, n_estimators= 100).fit(X_train, y_train)\n        \n        y_pred=model.predict(X_test)\n        \n        print(\"The  RMSE score achieved with {} is {}\".format(model_name, str(math.sqrt(sklm.mean_squared_error(y_test, y_pred)))))\n        \n        \n        coef = pd.Series(model.feature_importances_, index = X_train.columns)\n        \n        print()\n        print('Our most valuable features with {} are :'.format(model_name))\n        print()\n        print(coef.sort_values(ascending = False).head(10))\n        \n        #submission\n        model_preds = np.expm1(model.predict(Test))\n        \n        solution = pd.DataFrame({\"id\":test.Id, \"SalePrice\":model_preds})\n        solution.to_csv(\"solution.csv\", index = False)\n        \n        print()\n        print(\"Submission file created !\")","49593e69":"function(Ridge, 'Ridge')","7404b65b":"function(XGBRegressor, 'XGBRegressor')","5aa050a7":"function(GradientBoostingRegressor, 'GradientBoostingRegressor')","7f884ec9":"from lightgbm import LGBMRegressor\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom mlxtend.regressor import StackingCVRegressor\n\n# Setup cross validation folds\nkf = KFold(n_splits=12, random_state=42, shuffle=True)\n\n# Light Gradient Boosting Regressor\nlightgbm = LGBMRegressor(objective='regression', \n                       num_leaves=6,\n                       learning_rate=0.01, \n                       n_estimators=7000,\n                       max_bin=200, \n                       bagging_fraction=0.8,\n                       bagging_freq=4, \n                       bagging_seed=8,\n                       feature_fraction=0.2,\n                       feature_fraction_seed=8,\n                       min_sum_hessian_in_leaf = 11,\n                       verbose=-1,\n                       random_state=42)\n\n# XGBoost Regressor\nxgboost = XGBRegressor(learning_rate=0.01,\n                       n_estimators=6000,\n                       max_depth=4,\n                       min_child_weight=0,\n                       gamma=0.6,\n                       subsample=0.7,\n                       colsample_bytree=0.7,\n                       objective='reg:linear',\n                       nthread=-1,\n                       scale_pos_weight=1,\n                       seed=27,\n                       reg_alpha=0.00006,\n                       random_state=42)\n\n# Ridge Regressor\nridge_alphas = [1e-15, 1e-10, 1e-8, 9e-4, 7e-4, 5e-4, 3e-4, 1e-4, 1e-3, 5e-2, 1e-2, 0.1, 0.3, 1, 3, 5, 10, 15, 18, 20, 30, 50, 75, 100]\nridge = make_pipeline(RobustScaler(), RidgeCV(alphas=ridge_alphas, cv=kf))\n\n# Support Vector Regressor\nsvr = make_pipeline(RobustScaler(), SVR(C= 20, epsilon= 0.008, gamma=0.0003))\n\n# Gradient Boosting Regressor\ngbr = GradientBoostingRegressor(n_estimators=6000,\n                                learning_rate=0.01,\n                                max_depth=4,\n                                max_features='sqrt',\n                                min_samples_leaf=15,\n                                min_samples_split=10,\n                                loss='huber',\n                                random_state=42)  \n\n# Random Forest Regressor\nrf = RandomForestRegressor(n_estimators=1200,\n                          max_depth=15,\n                          min_samples_split=5,\n                          min_samples_leaf=5,\n                          max_features=None,\n                          oob_score=True,\n                          random_state=42)\n\n# Stack up all the models above, optimized using xgboost\nstack_gen = StackingCVRegressor(regressors=(xgboost, lightgbm, svr, ridge, gbr, rf),\n                                meta_regressor=xgboost,\n                                use_features_in_secondary=True)","83a2655c":"print('stack_gen')\nstack_gen_model = stack_gen.fit(np.array(X), np.array(y))","0b43a052":"print('lightgbm')\nlgb_model_full_data = lightgbm.fit(X, y)","926995ce":"print('xgboost')\nxgb_model_full_data = xgboost.fit(X, y)","5c8395bb":"print('Svr')\nsvr_model_full_data = svr.fit(X, y)","f40b7cc3":"print('Ridge')\nridge_model_full_data = ridge.fit(X, y)","52928dac":"print('RandomForest')\nrf_model_full_data = rf.fit(X, y)","832fd313":"print('GradientBoosting')\ngbr_model_full_data = gbr.fit(X, y)","82c800d5":"# Blend models in order to make the final predictions more robust to overfitting\ndef blended_predictions(X):\n    return ((0.1 * np.expm1(ridge_model_full_data.predict(X))) + \\\n            (0.2 * np.expm1(svr_model_full_data.predict(X))) + \\\n            (0.1 * np.expm1(gbr_model_full_data.predict(X))) + \\\n            (0.1 * np.expm1(xgb_model_full_data.predict(X))) + \\\n            (0.1 * np.expm1(lgb_model_full_data.predict(X))) + \\\n            (0.05 * np.expm1(rf_model_full_data.predict(X))) + \\\n            (0.35 * np.expm1(stack_gen_model.predict(np.array(X)))))\n\nsolution2 = pd.DataFrame({\"id\":test.Id, \"SalePrice\":blended_predictions(Test)})\n\nsolution2.to_csv(\"solution2.csv\", index = False)","b6fe5764":"import eli5\nfrom IPython.display import display, HTML","f28b8e76":"function(Ridge, 'Ridge')","e4c835aa":"def function(train, test):\n    \n    #skew\n    train[\"SalePrice\"] = np.log1p(train[\"SalePrice\"])\n    \n    # Remove outliers\n    train.drop(train[(train['OverallQual']<5) & (train['SalePrice']>200000)].index, inplace=True)\n    train.drop(train[(train['GrLivArea']>4500) & (train['SalePrice']<300000)].index, inplace=True)\n    train.reset_index(drop=True, inplace=True)\n    \n    all_data = pd.concat((train.loc[:,'MSSubClass':'SaleCondition'],\n                      test.loc[:,'MSSubClass':'SaleCondition']))\n    \n    #handle missing value\n    all_data = handle_missing(all_data)\n    \n    #log transform skewed numeric features:\n    numeric_feats = all_data.dtypes[all_data.dtypes != \"object\"].index\n\n    skewed_feats = train[numeric_feats].apply(lambda x: skew(x.dropna())) #compute skewness\n    skewed_feats = skewed_feats[skewed_feats > 0.75]\n    skewed_feats = skewed_feats.index\n\n    all_data[skewed_feats] = np.log1p(all_data[skewed_feats])\n    \n    all_data = pd.get_dummies(all_data).reset_index(drop=True)\n    \n    # Remove any duplicated column names\n    all_data = all_data.loc[:,~all_data.columns.duplicated()]\n    \n    X = all_data[:train.shape[0]]\n    y = train.SalePrice\n    \n    Test = all_data[train.shape[0]:]\n    \n    return X, y, Test\n","c9be9be8":"train = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/train.csv\")\ntest = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/test.csv\")\n\nX, y, Test = function(train, test)","9139c4ca":"X = X[['GrLivArea', '1stFlrSF', 'OverallQual', 'Neighborhood_Crawfor', 'LotArea',\n       'Functional_Typ', 'Neighborhood_StoneBr', 'Exterior1st_BrkFace', 'MSZoning_FV', 'KitchenQual_Ex',\n       'OverallQual', 'KitchenAbvGr', 'Condition1_Artery', 'Street_Grvl', 'SaleType_WD',\n       'Neighborhood_NWAmes', 'Neighborhood_IDOTRR', 'SaleCondition_Abnorml', 'Neighborhood_Edwards', 'Functional_Maj2', 'MSZoning_C (all)']]","2f0bc748":"Test = Test[['GrLivArea', '1stFlrSF', 'OverallQual', 'Neighborhood_Crawfor', 'LotArea',\n       'Functional_Typ', 'Neighborhood_StoneBr', 'Exterior1st_BrkFace', 'MSZoning_FV', 'KitchenQual_Ex',\n       'OverallQual', 'KitchenAbvGr', 'Condition1_Artery', 'Street_Grvl', 'SaleType_WD',\n       'Neighborhood_NWAmes', 'Neighborhood_IDOTRR', 'SaleCondition_Abnorml', 'Neighborhood_Edwards', 'Functional_Maj2', 'MSZoning_C (all)']]","211f636e":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y,test_size = .3, random_state=0)","f568987b":"model_ridge = Ridge(alpha = 10).fit(X_train,y_train)","d654f112":"y_pred=model_ridge.predict(X_test)\n\nprint(\"The  RMSE score achieved with is \", str(math.sqrt(sklm.mean_squared_error(y_test, y_pred))))       ","f3af11dc":"model_preds = np.expm1(model_ridge.predict(Test))\n        \nsolution = pd.DataFrame({\"id\":test.Id, \"SalePrice\":model_preds})\nsolution.to_csv(\"solution.csv\", index = False)","f5018096":"That's quite close to zero, so that's good !","c39f232a":"#### GradientBoostingRegressor","97438fd6":"Let's first see which type of features we have here :","4ffc6410":"Also We create extra features in order to categorize data with and without a feature\n\nFor example:\n\n'HasPool' is 1 if you have pool and 0 if you don't have a pool","46da68e8":"Like preprocessing, this is a crucial step.\n\nFeature Engineering is one of the most important steps to complete before starting a Machine Learning analysis.Most of the basic Feature Engineering techniques consist of finding inconsistencies in the data and of creating new features by combining\/diving existing ones. Creating the best possible Machine Learning\/Deep Learning model can certainly help to achieve good results, but choosing the right features in the right format to feed in a model can by far boost performances","65d4b199":"**Ordered categorical features**\n\n(\"Very bad\", \"Bad\", \"okay\", \"Good\", \"Amazing\") need to be encoded as numerical ordered values (0->4 for example).","f07766f6":"For now let's create our Ridge model\n\n#### Ridge","4714fa38":"Let's try a method which I've seen in a Kernel on Kaggle, and see if I could increase my position on the leaderboard. If not, too bad, I could enhanced my skill at least !","f02732c4":"Since p value is more than 0.05 in Goldfeld Quandt Test, we can't reject it's null hypothesis that error terms are homoscedastic. Good.","287dc11c":"<a id=\"4\"><\/a> <br>\n## Modeling\nNow we are going to visualize which model seems performs well with our function rmse_cv, let's try first the most well known : Linear Regression :","47feb64a":"Highly-Correlated Features:\n\n- YearBuilt vs GarageYrBlt\n- 1stFlrSF vs TotalBsmtSF\n- GrLivArea vs TotRmsAbvGrd\n- GarageCars vs GarageArea","5980a448":"So let's focus on GradientBoostingRegressor, XGBoost and Ridge !","236a1fd7":"**Create new meta features**","c6876d10":"Let's look at the residuals as well:","40df54d2":"The residual plot looks pretty good.","2816b40f":"Finally, with my best model, I placed top 15% on Kaggle, I'm quite happy with the way things have gone :)","8a6a7686":"<a id=\"3\"><\/a> <br>\n## Data preprocessing: ","09f0cdd2":"The most important positive feature is GrLivArea - the above ground area by area square feet. This definitely make sense. Then a few other location and quality features contributed positively. Some of the negative features make less sense and would be worth looking into more - it seems like they might come from unbalanced categorical variables.\n\nAlso note that unlike the feature importance you'd get from a random forest these are actual coefficients in your model - so you can say precisely why the predicted price is what it is. The only issue here is that we log_transformed both the target and the numeric features so the actual magnitudes are a bit hard to interpret.","e6c46e4e":"**Linear Regression**","93b1d8e9":"That's seems good !","aa762936":"Here, these two variables doesn't seems related.","fcfa800a":"So for the Ridge regression we get a rmse of about 0.116","a52bdba3":"Let's find the highly-correlated (correlations higher than 0.8)","97f6ddf3":"These two variable seems highly related !","06e0ea42":"<a id=\"7\"><\/a> <br>\n## Features engineering","6a4e5091":"Let's make sure we handled all the missing values","c47769d2":"Let's try our function !","60432b46":"It seems to have a little tendance","600bb68c":"That's look better right ? The SalePrice is now normally distributed, great !","90002cb4":"GridSearchCV improve slightly our score. Sometime it's not worth the time, but sometime it is.","c1312ddf":"Let's check the variable with the most missing values","a94d27e8":"**Numerical values\/ Reduce skewness**","40a07db9":"**Create training and test set**","60c7df92":"Note the U-ish shaped curve above. When alpha is too large the regularization is too strong and the model cannot capture all the complexities in the data. If however we let the model be too flexible (alpha small) the model begins to overfit. A value of alpha = 10 is about right based on the plot above.","d8d09f3a":"Correlation within dependent variables is what we need to look for and avoid. This data doesn't contain perfect multicollinearity among independent variables. In case there was any then we would try to remove one of the correlated variables depending on which was more important to our regression model","765ce772":"The SalePrice is skewed to the right. This is a problem because most ML models don't do well with non-normally distributed data. We can apply a log(1+x) tranform to fix the skew.","c5449d55":"Let's try to stack and blend our model, in Kaggle's competition, this methods work wery well sometimes.","f0a82d02":"We obtained with these models a 0.12144 submission RMSE, which is quite good, we are well place on the leaderboard but maybe we can do better ?\n\n","ae5cd9e6":"*No perfect multicollinearity*\n\nIn regression, multicollinearity refers to the extent to which independent variables are correlated. Multicollinearity affects the coefficients and p-values, but it does not influence the predictions, precision of the predictions, and the goodness-of-fit statistics. If your primary goal is to make predictions, and you don\u2019t need to understand the role of each independent variable, you don\u2019t need to reduce severe multicollinearity.","e652aa67":"The residual terms are pretty much normally distributed for the number of test points we took. That's good !","897f1c99":"**Categorical values**","12f68c96":"Now we will be applying tests.\nA tip is to keep in mind that if we want 95% confidence on our findings and tests then the p-value should be less than 0.05 to be able to reject the null hypothesis. Remember, a researcher or data scientist would always aim to reject the null hypothesis.\n\n*Goldfeld Quandt Test*\n\nChecking heteroscedasticity : Using Goldfeld Quandt we test for heteroscedasticity.\n\nNull Hypothesis: Error terms are homoscedastic\nAlternative Hypothesis: Error terms are heteroscedastic.","72521bea":"<a id=\"6\"><\/a> <br>\n## Setup our next models","b67fc283":"Many times it makes sense to take a weighted average of uncorrelated results - this usually imporoves the score.","3bf8ba99":"*Check for Homoscedasticity*\n\nHomoscedasticity means that the residuals have equal or almost equal variance across the regression line. By plotting the error terms with predicted terms we can check that there should not be any pattern in the error terms\n\nFirst of all, let's see with a graph.","71bb4394":"Linear Regression seems to be well fitted to our data. We will keep that in mind. Now we will try a bunch of model and see which one perform well.\n\n**All models**","e4d6088f":"We can clearly see that those feature are skewed, we need to deal with that ! Let's do it.","81a213f7":"We can also take a look directly at what the most important coefficients are:","c55d34cf":"*Check for Normality of error terms\/residuals*","ae4010ea":"Let's try with only those variables !","0db0faea":"That's seems quite correlated !","d4215274":"What about non ordered categorical features ? Let's transform them with into dummies variables.\n\nSince we are working on a regression problem, we have to watch out for multicollinearities. That's why i choosed to set the argument drop_firt=True, to derive n-1 features from a feature with n different categories.\n\nThis help to reduce the collinearity .","105fcac6":" Another neat thing about the Ridge is that it does feature selection for you - setting coefficients of features it deems unimportant to zero. Let's take a look at the coefficients:","d7eeceb2":"**all models**","39d68570":"That's slightly decrease our performances but that's also hugely decrease our calcul time because we have drastically reduced our database. So that's a trade off. If you favor the performances against the time, so go for the entirely database, but if you favor the time against the performances so go for the reduce database.","3b771f2c":"At first glance, it appears that we will have a lot of work to deal with missing values.\n\nBut don't worry, after reading carrefully the data_description.txt file attached, most of the missing values correponds to None values, they aren't missing value.\n\nFor most of these missing values, it appears that the NA correspond to the absence of the feature, so actually it is an information (for example , a house that doesn't have an Alley access, a Pool or a Fence ...).\n\nExcept for some features (LotFrontage espacially), all we need is to rename the NA category to 'None' or 'No' (in my case).","c3d24e65":"The main tuning parameter for the Ridge model is alpha - a regularization parameter that measures how flexible our model is. The higher the regularization the less prone our model will be to overfit. However it will also lose flexibility and might not capture all of the signal in the data. We need to find a compromise !","adffc9e4":"<a id=\"1\"><\/a> <br>\n## The Goal\n- Each row in the dataset describes the characteristics of a house.\n- Our goal is to predict the SalePrice, given these features.\n- Our models are evaluated on the Root-Mean-Squared-Error (RMSE) between the log of the SalePrice predicted by our model, and the log of the actual SalePrice. Converting RMSE errors to a log scale ensures that errors in predicting expensive houses and cheap houses will affect our score equally.","a51e740b":"We need to transform MSSubClass, MoSold and YrSold into categorical features.","8dcc9673":"Preprocessing is a crucial step in Data Science. You can make the most optimal model with the most optimal hyperparamter, if you haven't process in a right way your data, you will rarely get anything at all. So let's do this properly.","37d545db":"**Blending and stacking**","98dc7a5c":"Let's see for all of the known model the rmse mean :","ff74b20f":"<a id=\"5\"><\/a> <br>\n## Function to wrap up\n\nBefore moving to the next section of this work, I would like to introduce a function that does all the work we did above in details just with one line of code. The function does all the regression pipeline:\n\n- Import the data\n- Log transform our target variable\n- Drop the outliers\n- Handle the missing data\n- Deal with skewness\n- Deal with categorical ordered\/non ordered\n- Split the data to train\/test\n- Compute metrics\n- Return features importances\n- Predict the target\n- Return a file for submit to Kaggle\n\nAnd you just need to precise one of the three models we implemented earlier (Ridge, XgbRegressor, GradientBoostingRegressor)","4b7c1b63":"We have 37 numerical features and 43 categorical features.","9a0c81d4":"**Fill missing values**","1a57c6a3":"<a id=\"2\"><\/a> <br>\n## Data Exploration","d94afcc5":"So now I would like to test my models but with a less variable, to gain some time and see if it decrease my performances.","1e8db3dd":"#### XGBoost","71319ff4":"We can see the most correlated variable to the target SalePrice.","9898f8f7":"That's doesn't improve our submission RMSE, too bad. It stay approximatively the same. At least, we tried ! Maybe our model is too specific and overfit ?","641a0b35":"**Models with less variable**","52a28aaa":"After these log transformation, most of our features have a smaller skewness.","db115770":"*Mean of Residuals*\n\nResiduals as we know are the differences between the true value and the predicted value. One of the assumptions of linear regression is that the mean of the residuals should be zero. So let's find out.","cabdf6eb":"1. [The goal](#1)\n1. [Data Exploration](#2)\n1. [Data preprocessing](#3)\n    - Fill missing values\n\n1. [Features engineering](#7)\n    - Encoding our categorical variables\n    - Create training and test set\n1. [Modeling](#4)\n    - Linear Regression\n    - All models\n    - Ridge\n    - XGBoost\n    - GradientBoostingRegressor\n1. [Function to wrap up](#5)\n1. [Setup our next models](#6)\n    - Blending and stacking\n    - Models with less variable","fad47eea":"Let's plot how SalePrice relates to some of the features in the dataset","f94e0fab":"Our results are enhanced with our hyperparameters tuning","3b3b29c6":"In order to perform Linear Regression, we need to check some assumptions like :\n    \n- Mean of Residuals\n- Check for Homoscedasticity\n- Check for Normality of error terms\/residuals\n\nAnd so on, that's why I made a entire section just for Linear Regression. To took the time and do things right !\n\nThose assumptions require us to perform the regression before we can even check for them. So let's perform regression on it"}}