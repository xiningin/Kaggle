{"cell_type":{"a2da847d":"code","836b54f4":"code","a4db6ef9":"code","819661ed":"code","2f5444e7":"code","59cc92c7":"code","62565768":"code","8e08a14b":"code","16de30da":"code","bef13c77":"code","48cefb67":"code","8537254f":"code","e4ba7edc":"code","9217de39":"code","7504174d":"code","7cc20920":"code","d0ae8e48":"code","c419fcbc":"code","cb7e41bc":"code","0efa31a7":"code","d7c045a5":"code","7d37648e":"code","983683e0":"code","3dc37f33":"code","c815bf0a":"code","061fb78a":"code","a5d3eb70":"code","a5423836":"code","e0d85981":"code","c29fba60":"code","0dfb556d":"code","82e3de31":"code","e8b02ca3":"code","64e7b137":"code","9fcbc44e":"code","f29b12ab":"code","5e9365b8":"code","7c7c77a6":"code","fa480637":"code","005d0891":"code","016be791":"code","94fd1aff":"code","378fd256":"code","e8733915":"code","351562d4":"code","5adf480d":"code","88014bbb":"code","2ce5b77c":"code","830c941b":"code","7a057ee0":"code","5a11936f":"code","8007c004":"code","1372afb6":"markdown","fe4e7766":"markdown","6f5e0dd5":"markdown","ac75da83":"markdown","0b314b8f":"markdown","5f327f8d":"markdown","b468b857":"markdown","76dd41a0":"markdown"},"source":{"a2da847d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","836b54f4":"import matplotlib.pylab as plt","a4db6ef9":"from sklearn import metrics\nmetrics.homogeneity_score([0, 0, 1, 1], [1, 1, 0, 0])","819661ed":"print(\"%.3f\" % metrics.homogeneity_score([0, 0, 1, 1], [0, 0, 1, 2]))","2f5444e7":"print(\"%.3f\" % metrics.homogeneity_score([0, 0, 1, 1], [0, 0, 1, 2]))","59cc92c7":"print(\"%.3f\" % metrics.homogeneity_score([0, 0, 1, 1], [0, 1, 2, 3]))","62565768":"print(\"%.3f\" % metrics.homogeneity_score([0, 0, 1, 1], [0, 1, 0, 1]))\nprint(\"%.3f\" % metrics.homogeneity_score([0, 0, 1, 1], [0, 0, 0, 0]))\nprint (metrics.completeness_score([0, 0, 1, 1], [1, 1, 0, 0]))\nprint(metrics.completeness_score([0, 0, 1, 1], [0, 0, 0, 0]))\nprint(metrics.completeness_score([0, 1, 2, 3], [0, 0, 1, 1]))\nprint(metrics.completeness_score([0, 0, 1, 1], [0, 1, 0, 1]))\nprint(metrics.completeness_score([0, 0, 0, 0], [0, 1, 2, 3]))","8e08a14b":"print (metrics.v_measure_score([0, 0, 1, 1], [0, 0, 1, 1]))\nprint (metrics.v_measure_score([0, 0, 1, 1], [1, 1, 0, 0]))","16de30da":"print(\"%.3f\" % metrics.completeness_score([0, 1, 2, 3], [0, 0, 0, 0]))\nprint(\"%.3f\" % metrics.homogeneity_score([0, 1, 2, 3], [0, 0, 0, 0]))\nprint(\"%.3f\" % metrics.v_measure_score([0, 1, 2, 3], [0, 0, 0, 0]))\nprint(\"%.3f\" % metrics.v_measure_score([0, 0, 1, 2], [0, 0, 1, 1]))\nprint(\"%.3f\" % metrics.v_measure_score([0, 1, 2, 3], [0, 0, 1, 1]))","bef13c77":"print(\"%.3f\" % metrics.v_measure_score([0, 0, 1, 1], [0, 0, 1, 2]))\nprint(\"%.3f\" % metrics.v_measure_score([0, 0, 1, 1], [0, 1, 2, 3]))","48cefb67":"print(\"%.3f\" % metrics.v_measure_score([0, 0, 0, 0], [0, 1, 2, 3]))","8537254f":"print(\"%.3f\" % metrics.v_measure_score([0, 0, 1, 1], [0, 0, 0, 0]))","e4ba7edc":"#Create some data\nMAXN=40\nX = np.concatenate([1.25*np.random.randn(MAXN,2), 5+1.5*np.random.randn(MAXN,2)])\nX = np.concatenate([X,[8,3]+1.2*np.random.randn(MAXN,2)])\nX.shape","9217de39":"#Just for visualization purposes, create the labels of the 3 distributions\ny = np.concatenate([np.ones((MAXN,1)),2*np.ones((MAXN,1))])\ny = np.concatenate([y,3*np.ones((MAXN,1))])\n\nplt.subplot(1,2,1)\nplt.scatter(X[(y==1).ravel(),0],X[(y==1).ravel(),1],color='r')\nplt.scatter(X[(y==2).ravel(),0],X[(y==2).ravel(),1],color='b')\nplt.scatter(X[(y==3).ravel(),0],X[(y==3).ravel(),1],color='g')\nplt.title('Data as were generated')\n\nplt.subplot(1,2,2)\nplt.scatter(X[:,0],X[:,1],color='r')\nplt.title('Data as the algorithm sees them')\nplt.savefig('myfig.png',dpi=300, bbox_inches='tight')\nfrom sklearn import cluster\nfrom sklearn.cluster import KMeans\n\nK=3 # Assuming to be 3 clusters!\n\nclf = cluster.KMeans(init='random', n_clusters=K)\nclf.fit(X)\n","7504174d":"print (clf.labels_) # or\nprint (clf.predict(X)) # equivalent","7cc20920":"print (X[(y==1).ravel(),0]) #numpy.ravel() returns a flattened array\nprint (X[(y==1).ravel(),1])","d0ae8e48":"plt.scatter(X[(y==1).ravel(),0],X[(y==1).ravel(),1],color='r')\nplt.scatter(X[(y==2).ravel(),0],X[(y==2).ravel(),1],color='b')\nplt.scatter(X[(y==3).ravel(),0],X[(y==3).ravel(),1],color='g')\n\nfig = plt.gcf()\nfig.set_size_inches((6,5))","c419fcbc":"x = np.linspace(-5,15,200)\nXX,YY = np.meshgrid(x,x)\nsz=XX.shape\ndata=np.c_[XX.ravel(),YY.ravel()]\n# c_ translates slice objects to concatenation along the second axis.","cb7e41bc":"Z=clf.predict(data) # returns the labels of the data\nprint (Z)\nn=len(Z)\nprint('the number of the labels is:', n)","0efa31a7":"# Visualize space partition\nplt.imshow(Z.reshape(sz), interpolation='bilinear', origin='lower',\nextent=(-5,15,-5,15),alpha=0.3, vmin=0, vmax=K-1)\nplt.title('Space partitions', size=14)\nplt.scatter(X[(y==1).ravel(),0],X[(y==1).ravel(),1],color='r')\nplt.scatter(X[(y==2).ravel(),0],X[(y==2).ravel(),1],color='b')\nplt.scatter(X[(y==3).ravel(),0],X[(y==3).ravel(),1],color='g')\n\nfig = plt.gcf()\nfig.set_size_inches((6,5))\n\nplt.savefig('myfig2.png',dpi=300, bbox_inches='tight')","d7c045a5":"clf = cluster.KMeans(n_clusters=K, random_state=0)\n#initialize the k-means clustering\nclf.fit(X) #run the k-means clustering\n\ndata=np.c_[XX.ravel(),YY.ravel()]\nZ=clf.predict(data) # returns the clustering labels of the data\nb=len(Z)\nprint('the number of the clustering labels is :', b)","7d37648e":"plt.title('Final result of K-means', size=14)\n\nplt.scatter(X[(y==1).ravel(),0],X[(y==1).ravel(),1],color='r')\nplt.scatter(X[(y==2).ravel(),0],X[(y==2).ravel(),1],color='b')\nplt.scatter(X[(y==3).ravel(),0],X[(y==3).ravel(),1],color='g')\n\nplt.imshow(Z.reshape(sz), interpolation='bilinear', origin='lower',\nextent=(-5,15,-5,15),alpha=0.3, vmin=0, vmax=K-1)\n\nx = np.linspace(-5,15,200)\nXX,YY = np.meshgrid(x,x)\nfig = plt.gcf()\nfig.set_size_inches((6,5))\n\nplt.savefig('myfig3.png',dpi=300, bbox_inches='tight')","983683e0":"clf = cluster.KMeans(init='random', n_clusters=K, random_state=0)\n#initialize the k-means clustering\nclf.fit(X) #run the k-means clustering\nZx=clf.predict(X)\n\nplt.subplot(1,3,1)\nplt.title('Original labels', size=14)\nplt.scatter(X[(y==1).ravel(),0],X[(y==1).ravel(),1],color='r')\nplt.scatter(X[(y==2).ravel(),0],X[(y==2).ravel(),1],color='b') # b\nplt.scatter(X[(y==3).ravel(),0],X[(y==3).ravel(),1],color='g') # g\nfig = plt.gcf()\nfig.set_size_inches((12,3))\n\nplt.subplot(1,3,2)\nplt.title('Data without labels', size=14)\nplt.scatter(X[(y==1).ravel(),0],X[(y==1).ravel(),1],color='r')\nplt.scatter(X[(y==2).ravel(),0],X[(y==2).ravel(),1],color='r') # b\nplt.scatter(X[(y==3).ravel(),0],X[(y==3).ravel(),1],color='r') # g\nfig = plt.gcf()\nfig.set_size_inches((12,3))\n\nplt.subplot(1,3,3)\nplt.title('Clustering labels', size=14)\nplt.scatter(X[(Zx==1).ravel(),0],X[(Zx==1).ravel(),1],color='r')\nplt.scatter(X[(Zx==2).ravel(),0],X[(Zx==2).ravel(),1],color='b')\nplt.scatter(X[(Zx==0).ravel(),0],X[(Zx==0).ravel(),1],color='g')\nfig = plt.gcf()\nfig.set_size_inches((12,3))","3dc37f33":"from sklearn import metrics\n\nclf = cluster.KMeans(n_clusters=K, init='k-means++', random_state=0,\nmax_iter=300, n_init=10)\n#initialize the k-means clustering\nclf.fit(X) #run the k-means clustering\n\nprint ('Final evaluation of the clustering:')\n\nprint('Inertia: %.2f' % clf.inertia_)\n\nprint('Adjusted_rand_score %.2f' % metrics.adjusted_rand_score(y.ravel(),\nclf.labels_))\n\nprint('Homogeneity %.2f' % metrics.homogeneity_score(y.ravel(),\nclf.labels_))\n\nprint('Completeness %.2f' % metrics.completeness_score(y.ravel(),\nclf.labels_))\n\nprint('V_measure %.2f' % metrics.v_measure_score(y.ravel(), clf.labels_))\n\nprint('Silhouette %.2f' % metrics.silhouette_score(X, clf.labels_,\nmetric='euclidean'))\n\nclf1 = cluster.KMeans(n_clusters=K, init='random', random_state=0,\nmax_iter=2, n_init=2)\n#initialize the k-means clustering\nclf1.fit(X) #run the k-means clustering\n\nprint ('Final evaluation of the clustering:')\n\nprint ('Inertia: %.2f' % clf1.inertia_)\n\nprint ('Adjusted_rand_score %.2f' % metrics.adjusted_rand_score(y.ravel(),\nclf1.labels_))\n\nprint ('Homogeneity %.2f' % metrics.homogeneity_score(y.ravel(),\nclf1.labels_))\n\nprint ('Completeness %.2f' % metrics.completeness_score(y.ravel(),\nclf1.labels_))\n\nprint ('V_measure %.2f' % metrics.v_measure_score(y.ravel(),\nclf1.labels_))\n\nprint ('Silhouette %.2f' % metrics.silhouette_score(X, clf1.labels_,\nmetric='euclidean'))","c815bf0a":"#Read and check the dataset downloaded from the EuroStat\n\nimport pandas as pd\nimport numpy as np\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn import cluster\n\nedu=pd.read_csv('\/kaggle\/input\/ense3ict2020\/files\/ch07\/educ_figdp_1_Data.csv',na_values=':')\nedu.head()","061fb78a":"edu.tail()","a5d3eb70":"#Pivot table in order to get a nice feature vector representation with dual indexing by TIME and GEO\npivedu=pd.pivot_table(edu, values='Value', index=['TIME', 'GEO'], columns=['INDIC_ED'])\npivedu.head()","a5423836":"print ('Let us check the two indices:\\n')\nprint ('\\nPrimary index (TIME): \\n' + str(pivedu.index.levels[0].tolist()))\nprint ('\\nSecondary index (GEO): \\n' + str(pivedu.index.levels[1].tolist()))","e0d85981":"#Extract 2010 set of values\nedu2010=pivedu.query('TIME==[2010]')\nedu2010.head()","c29fba60":"#Store column names and clear them for better handling. Do the same with countries\nedu2010 = edu2010.rename(index={'Euro area (13 countries)': 'EU13',\n'Euro area (15 countries)': 'EU15',\n'European Union (25 countries)': 'EU25',\n'European Union (27 countries)': 'EU27',\n'Former Yugoslav Republic of Macedonia, the': 'Macedonia',\n'Germany (until 1990 former territory of the FRG)': 'Germany'\n})\nfeatures = edu2010.columns.tolist()\nprint(features)\ncols=edu2010.columns\ncountries = edu2010.index.tolist()\n\nedu2010.columns=range(12)\nedu2010.head()","0dfb556d":"# import packages\n#import pandas as pd\n#import numpy as np\n#import seaborn as sns\n\n#import matplotlib.pyplot as plt\n#import matplotlib.mlab as mlab\n#import matplotlib\n#plt.style.use('ggplot')\n#from matplotlib.pyplot import figure\n\n#%matplotlib inline\n#matplotlib.rcParams['figure.figsize'] = (12,8)\n\n#pd.options.mode.chained_assignment = None\n\n\n\n# read the data\n#df = edu2010\n\n# shape and data types of the data\n#print(df.shape)\n#print(df.dtypes)\n\n# select numeric columns\n#df_numeric = df.select_dtypes(include=[np.number])\n#numeric_cols = df_numeric.columns.values\n#print(numeric_cols)\n\n# select non numeric columns\n#df_non_numeric = df.select_dtypes(exclude=[np.number])\n#non_numeric_cols = df_non_numeric.columns.values\n#print(non_numeric_cols)","82e3de31":"#Check what is going on in the NaN data\nnan_countries=np.sum(np.where(edu2010.isnull(),1,0),axis=1)#numeric missing\nprint(nan_countries)\n#yes_countries=np.sum(np.where(edu2010.isnull(),0,1),axis=1)\n#print(yes_countries)\nplt.bar(np.arange(nan_countries.shape[0]),nan_countries)\nplt.xticks(np.arange(nan_countries.shape[0]),countries,rotation=90,horizontalalignment='left',\nfontsize=12)\nfig = plt.gcf()\nfig.set_size_inches((12,5))\n\n# if it's a larger dataset and the visualization takes too long can do this.\n# % of missing.\nfor col in edu2010.columns:\n    pct_missing = np.mean(edu2010[col].isnull())\n    print('{} - {}%'.format(col, round(pct_missing*100)))\n","e8b02ca3":"# first create missing indicator for features with missing data\n#for col in df.columns:\n    #missing = df[col].isnull()\n    #num_missing = np.sum(missing)\n    \n    #if num_missing > 0:  \n        #print('created missing indicator for: {}'.format(col))\n        #df['{}_ismissing'.format(col)] = missing\n\n\n# then based on the indicator, plot the histogram of missing values\n#ismissing_cols = [countries for countries in features if 'ismissing' in countries]\n#df['num_missing'] = df[ismissing_cols].sum(axis=1)\n\n#df['num_missing'].value_counts().reset_index().sort_values(by='index').plot.bar(x='index', y='num_missing')","64e7b137":"# drop rows with a lot of missing values.\n#ind_missing = df[df['num_missing'] > 4].index\n#df_less_missing_rows = df.drop(ind_missing, axis=0)","9fcbc44e":"# drop rows with a lot of missing values.\n#wrk_countries = nan_countries<4\n\n#educlean=index.edu2010['wrk_countries']\n\n#educlean=edu2010.ix[wrk_countries][wrk_countries]\n\n#ind_missing = df[df['nan_countries'] > 4].index\n#df_less_missing_rows = df.drop(ind_missing, axis=0)\n    \n    #Remove non info countries\n#T = df.append({\"TIME\": 2010, \"GEO\": 'Turkey'}, ignore_index = True)\n#b=T.drop((T.index), axis = 0, inplace = True)\n#print(b)\n#L=df.append({\"TIME\": 2010, \"GEO\": ''},ignore_index = True)\n#c=L.drop((L.index), axis = 0, inplace = True)\n#Li=df.append({\"TIME\": 2010, \"GEO\": 'Liechtenstein'},ignore_index = True)\n#d=Li.drop((Li.index), axis = 0, inplace = True)\n#df1=df[(df.)]\n#wrk_countries = nan_countries<4\n#print(wrk_countries)\n#educlean=df.dropna(axis=1, how='any', thresh=None, subset=None, inplace=False),\n#Let us check the features we have\n#na_features = np.sum(np.where(df.isnull(),1,0),axis=0)\n#print (na_features)\n\n#plt.bar(np.arange(na_features.shape[0]),na_features)\n#plt.xticks(fontsize=12)\n#fig = plt.gcf()\n#fig.set_size_inches((8,4))\n\n#educlean=edu2010-[edu2010.drop(edu2010.drop())\n#ind_missing = edu2010[edu2010['nan_countries'] > 4].index\n#df_less_missing_rows = edu2010.drop(ind_missing, axis=0)\n# If we want to drop.\n\n\n#wrk_countries = nan_countries<4\n#edu2010.head()\n#educlean=np.ix_([wrk_countries])\n#educlean=edu2010.drop((edu2010.index[edu2010.]), axis = 0, inplace = True)&(edu2010.index), axis = 0, inplace = True)&)[wrk_countries]\n#df.drop(df.index[(df.income=='>50K\\n') & (df['age']>df['age'].median() +35) & (df['age'] > df['age'].median()-15)])\n#educlean=edu2010.drop(edu2010.index[(edu2010.TIME=='2010')&(edu2010['GEO']=='Turkey')])#.ix - Construct an open mesh from multiple sequences.\n\n#Let us check the features we have\n#na_features = np.sum(np.where(edu2010.isnull(),1,0),axis=0)\n#print (na_features)\n\n#plt.bar(np.arange(na_features.shape[0]),na_features)\n#plt.xticks(fontsize=12)\n#fig = plt.gcf()\n#fig.set_size_inches((8,4))              ","f29b12ab":"#Remove non info countries\nwrk_countries = nan_countries<4\n\neduclean=edu2010.loc[wrk_countries] #.ix - Construct an open mesh from multiple sequences.\n\n#Let us check the features we have\nna_features = np.sum(np.where(educlean.isnull(),1,0),axis=0)\nprint (na_features)\n\nplt.bar(np.arange(na_features.shape[0]),na_features)\nplt.xticks(fontsize=12)\nfig = plt.gcf()\nfig.set_size_inches((8,4))","5e9365b8":"#Option A fills those features with some value, at risk of extracting wrong information\n#Constant filling : edufill0=educlean.fillna(0)\nedufill=educlean.fillna(educlean.mean())\nprint ('Filled in data shape: ' + str(edufill.shape))\n\n#Option B drops those features\nedudrop=educlean.dropna(axis=1)\n#dropna: Return object with labels on given axis omitted where alternately any or\n# all of the data are missing\nprint ('Drop data shape: ' + str(edudrop.shape))","7c7c77a6":"scaler = StandardScaler() #Standardize features by removing the mean and scaling to unit variance\n\nX_train_fill = edufill.values\nX_train_fill = scaler.fit_transform(X_train_fill)\n\nclf = cluster.KMeans(init='k-means++', n_clusters=3, random_state=42)\n\nclf.fit(X_train_fill) #Compute k-means clustering.\n\ny_pred_fill = clf.predict(X_train_fill)\n#Predict the closest cluster each sample in X belongs to.\n\nidx=y_pred_fill.argsort()","fa480637":"plt.plot(np.arange(35),y_pred_fill[idx],'ro')\nwrk_countries_names = [countries[i] for i,item in enumerate(wrk_countries) if item ]\n\nplt.xticks(np.arange(len(wrk_countries_names)),[wrk_countries_names[i] for i in idx],\nrotation=90,horizontalalignment='left',fontsize=12)\nplt.title('Using filled in data', size=15)\nplt.yticks([0,1,2])\nfig = plt.gcf()\n\nfig.set_size_inches((12,5))","005d0891":"X_train_drop = edudrop.values\nX_train_drop = scaler.fit_transform(X_train_drop)\n\nclf.fit(X_train_drop) #Compute k-means clustering.\ny_pred_drop = clf.predict(X_train_drop) #Predict the closest cluster of each sample in X.","016be791":"idx=y_pred_drop.argsort()\nplt.plot(np.arange(35),y_pred_drop[idx],'ro')\nwrk_countries_names = [countries[i] for i,item in enumerate(wrk_countries) if item ]\n\nplt.xticks(np.arange(len(wrk_countries_names)),[wrk_countries_names[i] for i in idx],\nrotation=90,horizontalalignment='left',fontsize=12)\nplt.title('Using dropped missing values data',size=15)\nfig = plt.gcf()\nplt.yticks([0,1,2])\nfig.set_size_inches((12,5))","94fd1aff":"plt.plot(y_pred_drop+0.2*np.random.rand(35),y_pred_fill+0.2*np.random.rand(35),'bo')\nplt.xlabel('Predicted clusters for the filled in dataset.')\nplt.ylabel('Predicted clusters for the dropped missing values dataset.')\nplt.title('Correlations')\nplt.xticks([0,1,2])\nplt.yticks([0,1,2])\nplt.savefig('myfig7.png',dpi=300, bbox_inches='tight')","378fd256":"print ('Cluster 0: \\n' + str([wrk_countries_names[i] for i,item in enumerate(y_pred_fill)\nif item==0]))\nprint ('Cluster 0: \\n' + str([wrk_countries_names[i] for i,item in enumerate(y_pred_drop)\nif item==0]))\nprint ('\\n')\nprint ('Cluster 1: \\n' + str([wrk_countries_names[i] for i,item in enumerate(y_pred_fill)\nif item==1]))\nprint ('Cluster 1: \\n' + str([wrk_countries_names[i] for i,item in enumerate(y_pred_drop)\nif item==1]))\nprint ('\\n')\nprint ('Cluster 2: \\n' + str([wrk_countries_names[i] for i,item in enumerate(y_pred_fill)\nif item==2]))\nprint ('Cluster 2: \\n' + str([wrk_countries_names[i] for i,item in enumerate(y_pred_drop)\nif item==2]))\nprint ('\\n')","e8733915":"width=0.3\np1 = plt.bar(np.arange(8),scaler.inverse_transform(clf.cluster_centers_[1]),width,color='b')\n# Scale back the data to the original representation\np2 = plt.bar(np.arange(8)+width,scaler.inverse_transform(clf.cluster_centers_[2]),\nwidth,color='yellow')\np0 = plt.bar(np.arange(8)+2*width,scaler.inverse_transform(clf.cluster_centers_[0]),\nwidth,color='r')\n\nplt.legend( (p0[0], p1[0], p2[0]), ('Cluster 0', 'Cluster 1', 'Cluster 2') ,loc=9)\nplt.xticks(np.arange(8) + 0.5, np.arange(8),size=12)\nplt.yticks(size=12)\nplt.xlabel('Economical indicators')\nplt.ylabel('Average expanditure')\nfig = plt.gcf()\n\nplt.savefig('myfig9.png',dpi=300, bbox_inches='tight')","351562d4":"from scipy.spatial import distance\np = distance.cdist(X_train_drop[y_pred_drop==0,:],[clf.cluster_centers_[1]],'euclidean')\n#the distance of the elements of cluster 0 to the center of cluster 1\n\nfx = np.vectorize(np.int)\n\nplt.plot(np.arange(p.shape[0]),\nfx(p)\n)\n\nwrk_countries_names = [countries[i] for i,item in enumerate(wrk_countries) if item ]\nzero_countries_names = [wrk_countries_names[i] for i,item in enumerate(y_pred_drop)\nif item==0]\nplt.xticks(np.arange(len(zero_countries_names)),zero_countries_names,rotation=90,\nhorizontalalignment='left',fontsize=12)","5adf480d":"from scipy.spatial import distance\np = distance.cdist(X_train_drop[y_pred_drop==0,:],[clf.cluster_centers_[1]],'euclidean')\npown = distance.cdist(X_train_drop[y_pred_drop==0,:],[clf.cluster_centers_[0]],'euclidean')\n\nwidth=0.45\np0=plt.plot(np.arange(p.shape[0]),fx(p),width)\np1=plt.plot(np.arange(p.shape[0])+width,fx(pown),width,color = 'red')\n\nwrk_countries_names = [countries[i] for i,item in enumerate(wrk_countries) if item ]\nzero_countries_names = [wrk_countries_names[i] for i,item in enumerate(y_pred_drop)\nif item==0]\nplt.xticks(np.arange(len(zero_countries_names)),zero_countries_names,rotation=90,\nhorizontalalignment='left',fontsize=12)\nplt.legend( (p0[0], p1[0]), ('d -> 1', 'd -> 0') ,loc=1)\nplt.savefig('myfig11.png',dpi=300, bbox_inches='tight')","88014bbb":"X_train = edudrop.values\nclf = cluster.KMeans(init='k-means++', n_clusters=4, random_state=0)\nclf.fit(X_train)\ny_pred = clf.predict(X_train)\n\nidx=y_pred.argsort()\nplt.plot(np.arange(35),y_pred[idx],'ro')\nwrk_countries_names = [countries[i] for i,item in enumerate(wrk_countries) if item ]\n\nplt.xticks(np.arange(len(wrk_countries_names)),[wrk_countries_names[i] for i in idx],rotation=90,\nhorizontalalignment='left',fontsize=12)\nplt.title('Using drop features',size=15)\nplt.yticks([0,1,2,3])\nfig = plt.gcf()\nfig.set_size_inches((12,5))","2ce5b77c":"width=0.2\np0 = plt.bar(np.arange(8)+1*width,clf.cluster_centers_[0],width,color='r')\np1 = plt.bar(np.arange(8),clf.cluster_centers_[1],width,color='b')\np2 = plt.bar(np.arange(8)+3*width,clf.cluster_centers_[2],width,color='yellow')\np3 = plt.bar(np.arange(8)+2*width,clf.cluster_centers_[3],width,color='pink')\n\nplt.legend( (p0[0], p1[0], p2[0], p3[0]), ('Cluster 0', 'Cluster 1', 'Cluster 2',\n'Cluster 3') ,loc=9)\nplt.xticks(np.arange(8) + 0.5, np.arange(8),size=12)\nplt.yticks(size=12)\nplt.xlabel('Economical indicator')\nplt.ylabel('Average expenditure')\nfig = plt.gcf()\nfig.set_size_inches((12,5))\nplt.savefig('myfig12.png',dpi=300, bbox_inches='tight')","830c941b":"print ('Cluster 0: \\n' + str([wrk_countries_names[i] for i,item in enumerate(y_pred) if item==0]))\n\nprint ('Cluster 1: \\n' + str([wrk_countries_names[i] for i,item in enumerate(y_pred) if item==1]))\n\nprint ('Cluster 2: \\n' + str([wrk_countries_names[i] for i,item in enumerate(y_pred) if item==2]))\n\nprint ('Cluster 3: \\n' + str([wrk_countries_names[i] for i,item in enumerate(y_pred) if item==3]))\n\n#Save data for future use.\nimport pickle\nofname = open('edu2010.pkl', 'wb')\ns = pickle.dump([edu2010, wrk_countries_names,y_pred ],ofname)\nofname.close()","7a057ee0":"from scipy.cluster.hierarchy import linkage, dendrogram\nfrom scipy.spatial.distance import pdist\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.neighbors import kneighbors_graph\nfrom sklearn.metrics import euclidean_distances\n\nX = StandardScaler().fit_transform(edudrop.values)\n\ndistances = euclidean_distances(edudrop.values)\n\nspectral = cluster.SpectralClustering(n_clusters=4, affinity=\"nearest_neighbors\")\nspectral.fit(edudrop.values)\n\ny_pred = spectral.labels_.astype(np.int)","5a11936f":"idx=y_pred.argsort()\n\nplt.plot(np.arange(35),y_pred[idx],'ro')\nwrk_countries_names = [countries[i] for i,item in enumerate(wrk_countries) if item ]\n\nplt.xticks(np.arange(len(wrk_countries_names)),[wrk_countries_names[i]\nfor i in idx],rotation=90,horizontalalignment='left',fontsize=12)\n\nplt.yticks([0,1,2,3])\n\nplt.title('Applying Spectral Clustering on the drop features',size=15)\nfig = plt.gcf()\nfig.set_size_inches((12,5))","8007c004":"X_train = edudrop.values\ndist = pdist(X_train,'euclidean')\nlinkage_matrix = linkage(dist,method = 'complete');\nplt.figure() # we need a tall figure\nfig = plt.gcf()\nfig.set_size_inches((12,12))\ndendrogram(linkage_matrix, orientation=\"right\", color_threshold = 4,labels = wrk_countries_names, leaf_font_size=20);\n\nplt.savefig('myfig14.png',dpi=300, bbox_inches='tight')\nplt.show()\n\n#plt.tight_layout() # fixes margins","1372afb6":"model.predict(): given a trained model, predict the label of a new set of data. This method accepts one argument, the new data X_new (e.g. model.predict(X_new)), and returns the learned label for each object in the array, we have 40000 lables ","fe4e7766":"Perfect labelings are both homogeneous and complete, hence have score 1.0\nLabelings that assign all classes members to the same clusters are complete but not homogeneous","6f5e0dd5":"we have 3 countries with 3 missing values, 1 country with one missing value,1 country(liechtenstein) with 6 missing values and the most missing value belong to luxumburg and turkey with 9 missing values. and we should improve them","ac75da83":"Labelings that have pure clusters with members coming from the same classes are homogeneous but un-necessary splits harms completeness and thus penalize V-measure as well","0b314b8f":"#Extract 2010 set of values\nedu2010=pivedu.query('TIME==[\"2020\"]')\nedu2010.head()","5f327f8d":"If classes members are completely split across different clusters, the assignment is totally incomplete, hence the V-Measure is null","b468b857":"Clusters that include samples from totally different classes totally destroy the homogeneity of the labeling","76dd41a0":"The KMeans algorithm clusters data by trying to separate samples in n groups of equal variance, minimizing a criterion known as the inertia or within-cluster sum-of-squares. This algorithm requires the number of clusters to be specified. It scales well to large number of samples and has been used across a large range of application areas in many different fields.\n\nThe k-means algorithm divides a set of N samples X into K disjoint clusters C , each described by the mean Mj of the samples in the cluster. The means are commonly called the cluster \u201ccentroids\u201d; note that they are not, in general, points from X, although they live in the same space.\n\nThe K-means algorithm aims to choose centroids that minimise the inertia, or within-cluster sum-of-squares criterion."}}