{"cell_type":{"e228730c":"code","96fdf4c9":"code","843d14b4":"code","b65be46c":"code","23b21d8b":"code","61df0f6a":"code","762cd0e3":"code","c851c2e8":"code","64e0de83":"code","8620fb35":"code","fbdfec53":"code","c5fd60d3":"code","adef47d9":"code","fc6f7ed5":"code","2cfff23a":"code","18f4f423":"markdown","8d08e05e":"markdown","62927d96":"markdown","165306d0":"markdown","e3214eef":"markdown","e99a89d2":"markdown","e1b74d15":"markdown","adae7161":"markdown","e7c6bf5d":"markdown"},"source":{"e228730c":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Nueral network\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import OneHotEncoder\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import Dropout\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.optimizers import SGD\nfrom keras.regularizers import l1\nfrom sklearn.cluster import KMeans\n\ndata = pd.read_csv('..\/input\/weather-dataset-rattle-package\/weatherAUS.csv')\n\ndata.shape","96fdf4c9":"# pre-processing\ndata.drop(\"Date\", axis=1, inplace=True)\n\n# NA value handling\ndef print_na(data):\n  for column in data.columns:\n    print(f'{column} with NA values sum: {data[column].isna().sum()}')\n\nprint_na(data)","843d14b4":"# dictionary used to fill na values\nfill_dictionary = {\n    'MinTemp': data['MinTemp'].mean(),\n    'MaxTemp': data['MaxTemp'].mean(),\n    'Rainfall': 0,\n    'Evaporation': 0,\n    'Sunshine': 0,\n    'WindGustDir' : 'NO',\n    'WindGustSpeed' : 0,\n    'WindDir9am' : 'NO',\n    'WindDir3pm' : 'NO',\n    'WindSpeed9am' : 0,\n    'WindSpeed3pm' : 0,\n    'Humidity9am': data['Humidity9am'].mean(),\n    'Humidity3pm': data['Humidity3pm'].mean(),\n    'Pressure9am': data['Pressure9am'].mean(),\n    'Pressure3pm': data['Pressure3pm'].mean(),\n    'Temp9am': data['Temp9am'].mean(),\n    'Temp3pm': data['Temp3pm'].mean(),\n    'Cloud9am': 0,\n    'Cloud3pm': 0,\n  \n}\n\nfor key, value in fill_dictionary.items():\n  data[key] = data[key].fillna(value)\n\ndata = data.dropna()\n\nprint_na(data)\ndata","b65be46c":"# Need to one hot encode the locations column\n\nencoder = LabelEncoder()\nlocations = encoder.fit_transform(data['Location'])\n\nonehot_encoder = OneHotEncoder(sparse=False)\nlocations = locations.reshape(len(locations), 1)\nlocations = onehot_encoder.fit_transform(locations)\n#print(locations.shape)\n\n# one hot RainToday and only label encode RainTomorrow Column\nrain_today = encoder.fit_transform(data['RainToday'])\n\nrain_tmr = encoder.fit_transform(data['RainTomorrow'])\n\nrain_today = rain_today.reshape(len(rain_today), 1)\n\nrain_today = onehot_encoder.fit_transform(rain_today)\n\n\n# join and finalize data\nLocations = pd.DataFrame(locations)\nRainToday = pd.DataFrame(rain_today, columns=['no', 'yes'])\n\n# Going to drop these categorical columns now\n# the wind directions were dropped because they might not be good predictors\n# plus, we already have wind speed as a feature\ndata = data.drop('Location', axis = 1)\ndata = data.drop('RainToday', axis = 1)\ndata = data.drop('RainTomorrow', axis = 1)\ndata = data.drop('WindGustDir', axis = 1)\ndata = data.drop('WindDir9am', axis = 1)\ndata = data.drop('WindDir3pm', axis = 1)\ndata = data.reset_index(drop=True)\n\n# normalization\ndata=(data-data.mean())\/data.std()\n\ndata = data.join(Locations)\ndata = data.join(RainToday)\ndata['Rain_Tomorrow'] = rain_tmr\n\ndata","23b21d8b":"# set the data\n# Y will be the last column so we set that aside\nX = data.iloc[:, 0:67]\nY = data.iloc[:, 67]\n\n# here to stratify Y so that the proportions of \"yes\" \"no\" labels in Y will be preserved when splitting\nX_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=0, stratify = Y)\n\n# The model\ndef create_model():\n\n\t# create model\n    # Regularizers and dropout were added because previous models were very overfit\n\tmodel = Sequential()\n\tmodel.add(Dense(64, input_dim=67, activation='relu'))\n\tmodel.add(Dropout(0.4))\n\tmodel.add(Dense(32, activation='relu', kernel_regularizer='l2'))\n\tmodel.add(Dropout(0.4))\n\tmodel.add(Dense(16, activation='relu', kernel_regularizer='l2'))\n\tmodel.add(Dense(1, activation='sigmoid'))\n\topt = SGD(learning_rate=0.01, momentum=0.8)\n\t# Compile model\n\tmodel.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n\treturn model\n\n# Did not use k-fold for this because it might run too long\nnetwork = create_model()\nhistory = network.fit(X_train, y_train, epochs=50, batch_size=32, validation_split=0.2)\n\n","61df0f6a":"history_dict = history.history\nprint(history_dict.keys())","762cd0e3":"# plotting the \"history\"\n# the history of the model's accuracy and validation set accuracy\n# from this graph we could check for overfitting\nplt.plot(history_dict['accuracy'])\nplt.plot(history_dict['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","c851c2e8":"test_loss, test_acc = network.evaluate(X_test, y_test)\nprint('Test accuracy:', test_acc)\n\nfrom sklearn.metrics import confusion_matrix\n\nprint(\"Confusion matrix: \")\n# for this confusion matrix, the rows are \"predicted\" and the columns are \"actual\"\ny_pred = network.predict(X_test)\nNN_FN = confusion_matrix(np.round(y_pred), y_test)\nNN_FN","64e0de83":"NN_FN[0][1] \/ (NN_FN[0][1] + NN_FN[1][1])","8620fb35":"from sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler","fbdfec53":"# select the data that we want to cluster\n# This cluster is without PCA and only have average temperature and rainfall accounted for\nclustering_data = pd.read_csv('..\/input\/weather-dataset-rattle-package\/weatherAUS.csv')\n\nclustering_data = clustering_data[['Location', 'MinTemp','MaxTemp','Rainfall']]\n\n# here we are calculating the average temperature ourselves\nclustering_data['AvgTemp'] = (clustering_data['MaxTemp'] - clustering_data['MinTemp']) \/ 2.0 + clustering_data['MinTemp']\nclustering_data = clustering_data.dropna()\nclustering_data.drop(['MinTemp','MaxTemp'], axis=1, inplace=True)\n\n# we groupby the location column to get the average values per location\n# this way we can analyze by location\nclustering_data = clustering_data.groupby(['Location']).mean()\n# standardize\nclustering_data = (clustering_data-clustering_data.mean())\/clustering_data.std()\nclustering_data","c5fd60d3":"# The clustering\nkmeans = KMeans(n_clusters= 5)\n \n#predict the labels of clusters.\nlabel = kmeans.fit_predict(clustering_data)\n\n#Getting unique labels\nlabels = np.unique(label)","adef47d9":"#plot clustering\nfig = plt.gcf()\nfig.set_size_inches(14, 10)\n\nnp_array_cluster_data = np.array(clustering_data)\nannotations = clustering_data.index # the index are location names\n\n#plotting the results:\nfor i in labels:\n    plt.scatter(np_array_cluster_data[label == i , 0], np_array_cluster_data[label == i , 1], label = i)\n\n# add the location name as an annotation for each point\nfor i, label in enumerate(annotations):\n    plt.annotate(label, (np_array_cluster_data[i, 0], np_array_cluster_data[i, 1]))\n\nplt.xlabel('Average Rainfall (cm)', fontweight='bold', color = 'orange', fontsize='17', horizontalalignment='center')\nplt.ylabel('Average Temperature', fontweight='bold', color = 'orange', fontsize='17', horizontalalignment='center')\nplt.legend()\nplt.axhline(0, color='red', linestyle='dashed')\nplt.axvline(0, color='red', linestyle='dashed')\nplt.show()\n\n\n#fig.savefig('test2png.png', dpi=100)","fc6f7ed5":"# The following cluster contains all numerical\/continuous values \n# This is then passed into PCA\nfrom sklearn.preprocessing import StandardScaler\n\npca_data = pd.read_csv('..\/input\/weather-dataset-rattle-package\/weatherAUS.csv')\n\n# These are categorical and non-continuous values\n# which would not be that good for PCA\n# as such they are dropped\nto_drop = [\n    'Date',\n    'WindDir9am',\n    'WindDir3pm',\n    'WindGustDir',\n    'RainToday',\n    'RainTomorrow'\n]\n\npca_data.drop(to_drop, axis = 1, inplace = True)\n\n# deal with NA values\nfor column in pca_data.columns:\n    if column == 'Location':\n        continue    \n    pca_data[column] = pca_data[column].fillna(pca_data[column].mean())\n\n# we want to group the data into its \"location\" column first\n# calculating all the means for each location\n# This way we can analyze the overall weather per location\npca_data = pca_data.groupby(['Location']).mean()\nprint(pca_data.shape)","2cfff23a":"pca = PCA(2)\n\npca_results = pca.fit_transform(pca_data)\n\nexplained_var = list(pca.explained_variance_ratio_)\n\npca_data = pd.DataFrame(pca_results, index = pca_data.index)\n# standardize\npca_data = (pca_data-pca_data.mean())\/pca_data.std()\n\npca_data_array = np.array(pca_data)\n\n# The clustering\nkmeans = KMeans(n_clusters= 5)\n\n#get the labels of clusters.\nlabel = kmeans.fit_predict(pca_data)\n\n#Getting unique labels\nlabels = np.unique(label)\n\n#plot clustering\nfig = plt.gcf()\nfig.set_size_inches(14, 10)\n\nannotations = pca_data.index # the index are location names\n\n#plotting the results:\nfor i in labels:\n    plt.scatter(pca_data_array[label == i , 0], pca_data_array[label == i , 1], label = i)\n\n# add the location name as an annotation for each point\nfor i, label in enumerate(annotations):\n    plt.annotate(label, (pca_data_array[i, 0], pca_data_array[i, 1]))\n\nplt.xlabel(f\"PCA 0, explained var {round(explained_var[0],2) * 100}%\", fontweight='bold', color = 'orange', fontsize='17', horizontalalignment='center')\nplt.ylabel(f\"PCA 1, explained var {round(explained_var[1],2) * 100}%\", fontweight='bold', color = 'orange', fontsize='17', horizontalalignment='center')\nplt.axhline(0, color='red', linestyle='dashed')\nplt.axvline(0, color='red', linestyle='dashed')\nplt.legend()\nplt.show()","18f4f423":"## **Table of Content**\n* ### [Project Summary](#id1)\n* ### [Data Accessing & Cleaning](#id4)\n* ### [Neural Network](#id2) \n* ### [False Negatives](#id5) \n* ### [Kmeans Clustering with PCA](#id3)","8d08e05e":"The second clustering model depicts 80% of the data\u2019s variance. For this model, we only included continuous numerical features therefore features such as WindDirection were not included. The goal of the second model was to visualize the general weather conditions for each location, not just average rainfall and temperature, but also the clouds, sunshine, wind speed, humidity etc. The purpose of PCA was to reduce the dimensionality to 2D so that it could be graphed. The two components describe 80% of the data\u2019s variance, which is still a great representative of the overall weather properties. It is clear that the locations in these clusters were more spread out, which makes sense because as more weather features are introduced, there would be more distinctions for each location. ","62927d96":"### <a id=\"id5\"> False Negatives<\/a>\nConsering false negative erros for neural network","165306d0":"### <a id=\"id4\"> Data Accessing & Cleaning<\/a>","e3214eef":"### <a id=\"id3\">Kmeans Clustering with PCA<\/a>\nTwo K-means clustering models were created. The first of which was created from the average rainfall and temperature of each location. The second was created from the two most significant principle components. The team chose the number of clusters \u201ck\u201d to be five because we wanted to visit 5 different areas in Australia, one from each cluster which hopefully displays different weather patterns. Because all clusters in the model were well-formed, we didn\u2019t have to adjust the value of k. The x and y axis values are after normalization.","e99a89d2":"### <a id=\"id2\">Neural Network<\/a>\nThe neural network model was written in python and the training and testing data were stratified to maintain the split of the binary target feature. The number of layers and neurons in each layer, the learning rate and number of epochs were all tuned however with minimal difference in accuracy. It is suspected that the model was overfitting since the testing accuracy was a lot lower than training accuracy, therefore regularization and dropouts were added for mitigation. The final model produced a misclassification rate of 14.47% and a false negative rate of 50.7%.","e1b74d15":"# **Data Science Final Project: Rain in Australia [PART 2]**\n## Team 3\n### Michael Chen\n### Michelle Peng\n### Kristin Fang\n### Peter Wu\n### Vanessa Han\n#### link to PART 1: https:\/\/www.kaggle.com\/peter0228\/ds-final-project-rain-in-australia-part-1","adae7161":"### <a id=\"id1\">  Project Summary<\/a>\nA team of five HBA2 students are planning for their graduation road trip to Australia, and are trying to decide the exact dates for the trip. They found that the weather forecast in Australia is sometimes inaccurate and therefore, to optimize the travel experience, the team decided to leverage the data models learned from their data science course, and create a model that can accurately predict the next day\u2019s weather.\n\nThe objective of this project is to predict if the next day is going to rain based on today\u2019s weather indicators in Australia. The dataset contains information about Australia\u2019s weather observation for ten years gathered from various weather stations in Australia. The weather is defined as \u201craining\u201d if the precipitation is 1mm or more. The data is gathered from the Australian Government Bureau of Meteorology.  \n\n\nThis part covers Neutral network for predicting if it wll rain in certain regions of australia\nAlso includes kmeans clustering with PCA <br>","e7c6bf5d":"The first clustering model was well-formed in that the clusters are distinct, with minimal overlaps. The goal was to visualize the two most basic weather properties which were the average temperature and rainfall. It was clear that some clusters contained outliers such as the green and purple clusters. Overall, we can see that for most locations, the average temperature and rainfall are pretty similar, as locations in the red and orange clusters are pretty close together."}}