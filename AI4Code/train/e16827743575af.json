{"cell_type":{"4826ec51":"code","fb8abc48":"code","28e0936f":"code","a0377e7f":"code","7fca3de5":"code","c620b66f":"code","6e7f6a38":"code","be276e2d":"code","98afab37":"code","b24df659":"code","feaf7b9f":"code","7c66660a":"code","a7dcaf49":"code","747c5320":"code","afdaf0e1":"code","51299fa9":"code","9df4ebde":"code","4cf74236":"code","be260bf5":"code","4366cbb5":"code","5feb8719":"code","48cb66b1":"code","5d50173e":"code","c60ebbd5":"code","025fd1ef":"code","502d039d":"code","e1d8ac4c":"code","7aa5efe4":"code","ceef8922":"code","1495dab9":"code","5805c20e":"code","472acac3":"code","80cf6979":"code","886050ab":"code","ac973831":"code","7980659d":"code","3ea93ebd":"code","e52e9f7d":"code","08ffcf38":"code","95817d93":"code","f4b4cb1e":"code","2e634928":"code","bc79c9c6":"code","841c305f":"code","b4dd84c0":"code","e3c2acf5":"code","d5cf5505":"code","cd9412e7":"code","c56e80ed":"code","8aad227e":"code","e934e446":"code","96eea17f":"code","2b6d9079":"code","6fa0f7e1":"code","0283e738":"code","fb4c4270":"code","5627e572":"code","84e47865":"code","139b1564":"code","422cf53e":"code","f6a00ae0":"code","d63a40d5":"code","dc78e3b5":"code","ac951ecc":"code","6be2cb0e":"code","02a15a21":"code","ff5d2b7c":"code","61628446":"code","bc820302":"code","82e25afb":"code","e4b4b0c0":"code","6ee6a33e":"code","4e805256":"code","12965335":"code","349955b3":"code","b1344fbe":"code","7745758a":"code","e38fa561":"code","a12eb2ce":"code","0810f8fb":"code","2ca18d5e":"code","dae83144":"code","710cc76f":"code","a81a6f52":"code","c8f993bf":"code","b122d816":"code","b088100e":"code","34cdc454":"code","63a158bb":"code","9d05e874":"code","cba6c927":"markdown","a11facee":"markdown","8d7e3a09":"markdown","98a62630":"markdown","dfee9e4e":"markdown","34fb1d3b":"markdown","12273c63":"markdown","cf535874":"markdown","e610a7db":"markdown","44c2819f":"markdown","b5545e1b":"markdown","5bfb3792":"markdown","3b7604c6":"markdown","123c149b":"markdown","ed4afae0":"markdown","6ad477fd":"markdown","a1d7f14d":"markdown","281a959d":"markdown","08da9fd9":"markdown","9b0bac7a":"markdown","b7736204":"markdown","773d1484":"markdown","44b90d81":"markdown","2bb6a587":"markdown","11e54ea4":"markdown","167d6a6b":"markdown","80ff6392":"markdown","8a7de765":"markdown","148b25f7":"markdown","ffde6ecb":"markdown","2b6c3482":"markdown","2c28f43d":"markdown","0c1ccbd0":"markdown","8e61c66e":"markdown","a869aadd":"markdown","cb1c01c9":"markdown","2256077f":"markdown","ad66abfa":"markdown","bc998315":"markdown","7bb5422b":"markdown","f989d0ec":"markdown","a37055ed":"markdown","4aef5cfd":"markdown","b7727540":"markdown","71a56691":"markdown","49ffead6":"markdown","ac11912d":"markdown","38b6efd7":"markdown","b1b3c3e3":"markdown","c8bf19e4":"markdown","a0d8fe1f":"markdown","6a05120a":"markdown","05891977":"markdown","469d4954":"markdown","96181660":"markdown","6335e75f":"markdown","28060a05":"markdown","3ac3e218":"markdown","fa486e47":"markdown","ae43ae7d":"markdown","c101855d":"markdown","8033c372":"markdown","828b42db":"markdown","070029d2":"markdown","14214185":"markdown","60a2541c":"markdown","55353e01":"markdown","33eb3352":"markdown","3fed2754":"markdown","40ef843e":"markdown","afd1e024":"markdown","905dbd46":"markdown","ef3c7458":"markdown","7c42c384":"markdown","fec0bb6b":"markdown","db2face7":"markdown","73bf85f8":"markdown","015abb58":"markdown","4326e51e":"markdown","c1f063c3":"markdown","e7be015d":"markdown","7c0e5e99":"markdown","81141d2b":"markdown","673e2ab8":"markdown","4ca17d19":"markdown","b7453ffb":"markdown","43763992":"markdown","ad5048dd":"markdown","97da25b7":"markdown","bcc1427a":"markdown","c54bf6fa":"markdown","2c5c45bc":"markdown","616228e9":"markdown","2f8cf007":"markdown","7cc50289":"markdown","b812ba4f":"markdown","f003cd49":"markdown","dd27d795":"markdown","6ca7d9d6":"markdown","03d74bb1":"markdown","e72cdc4f":"markdown","3ef4d343":"markdown","9069b93d":"markdown","00ba3f16":"markdown","33d4d82d":"markdown","691d2789":"markdown","46bbc32d":"markdown","635b0912":"markdown","20353e7e":"markdown","8a4b431a":"markdown","d32e11e5":"markdown","164145c1":"markdown","bc509212":"markdown","acc2f068":"markdown","7cbe0d3c":"markdown","2de722fa":"markdown","09d705e4":"markdown","228404bb":"markdown","3a25a4f7":"markdown","2024e69f":"markdown","89485537":"markdown","ff0388eb":"markdown","2a44d3e3":"markdown","ee256974":"markdown","d0fdbef4":"markdown","6dfae7c1":"markdown","5e1ee288":"markdown","30ea1060":"markdown","dfdea0be":"markdown","1ecffd15":"markdown","cd01dae2":"markdown","f87cf9f5":"markdown","acb4d662":"markdown","c10d588c":"markdown","f72d059b":"markdown","bfb49ccc":"markdown","992a0a14":"markdown","94fc9521":"markdown","424f633a":"markdown"},"source":{"4826ec51":"from sklearn.dummy import DummyClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import KBinsDiscretizer\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.metrics import accuracy_score, confusion_matrix, recall_score\nfrom sklearn.ensemble import IsolationForest\nfrom sklearn.preprocessing import FunctionTransformer, OneHotEncoder\nfrom imblearn import FunctionSampler\nimport pandas as pd\nimport numpy as np\nimport plotly.graph_objects as go\nimport plotly.express as px\n\n\nimport utilidad_grupor_practica1 as utils\n\nseed = 27912","fb8abc48":"filepath = \"..\/input\/pima-indians-diabetes-database\/diabetes.csv\" \n\nindex = None\ntarget = \"Outcome\"\n\ndata = utils.load_data(filepath, index, target)","28e0936f":"(X, y) = utils.divide_dataset(data, target=\"Outcome\")\n\ntrain_size = 0.7\n\n(X_train, X_test, y_train, y_test) = train_test_split(X, y,\n                                                      stratify=y,\n                                                      random_state=seed,\n                                                      train_size=train_size)","a0377e7f":"train = X_train.copy()\ntrain[target] = y_train","7fca3de5":"train.sample(5, random_state=seed)","c620b66f":"train.info()","6e7f6a38":"train.describe().T","be276e2d":"train.describe(include='category')","98afab37":"utils.plot_barplot(train)","b24df659":"utils.plot_histogram(train)","feaf7b9f":"vs = [\"Glucose\", \"BloodPressure\", \"SkinThickness\", \"Insulin\", \"BMI\"]\nntrain = train.replace({v: {0: np.NaN} for v in vs})\nutils.not_valid_values_plot(ntrain, vs)","7c66660a":"train_clean = utils.imputar_valores(train)\nutils.plot_histogram(train_clean)","a7dcaf49":"f = go.Figure(data=[{'type': 'box', 'y': train_clean[v], 'name': v} for v in set(train_clean.columns) - {\"Outcome\"}])\nf.show()","747c5320":"fig = utils.plot_pairplot(train_clean, 'Outcome')\nfig.update_layout(width=1200, height=1000)","afdaf0e1":"px.imshow(train_clean.corr())","51299fa9":"disc = KBinsDiscretizer(n_bins=3, strategy='uniform')","9df4ebde":"mea_imp = SimpleImputer(missing_values=0, strategy='mean')\npreproc = ColumnTransformer([('', 'drop', ['Insulin', 'SkinThickness']), \n                             ('mea_inp', mea_imp, [\"BMI\", \"Glucose\", \"BloodPressure\"])], remainder='passthrough')","4cf74236":"zeror = DummyClassifier(strategy='most_frequent', random_state=seed)\n\nclean_zeror = make_pipeline(preproc, zeror)\nutils.evaluate(clean_zeror, X_train, X_test, y_train, y_test)\n\ncfs = [confusion_matrix(y_test, clean_zeror.predict(X_test))]\n\n\n","be260bf5":"ndt = DecisionTreeClassifier(random_state=seed)\n\ncln_ndt = make_pipeline(preproc, ndt)\n\nutils.evaluate(cln_ndt, X_train, X_test, y_train, y_test)\n\ncfs.append(confusion_matrix(y_test, cln_ndt.predict(X_test)))","4366cbb5":"cln_disc_ndt = make_pipeline(preproc, disc, ndt)\n\nutils.evaluate(cln_disc_ndt, X_train, X_test, y_train, y_test)\n\ncfs.append(confusion_matrix(y_test, cln_disc_ndt.predict(X_test)))","5feb8719":"go.Figure([go.Scatter(x=[0, 1], y=[0, 1], line={'dash': 'dash'}, name='Clasificador aleatorio'), go.Scatter(x=[cfs[i][0, 1] \/ (cfs[i][0, 0] + cfs[i][0, 1]) for i in range(len(cfs))], y=[cfs[i][1, 1] \/ (cfs[i][1, 1] + cfs[i][1, 0]) for i in range(len(cfs))], mode='markers', hovertext=['ZeroR', '\u00c1rbol sin discretizar', '\u00c1rbol discretizando'], name='Clasificadores propuestos')], layout={'title': 'Espacio ROC', 'xaxis': {'title': '1-specificity'}, 'yaxis': {'title': 'sensitivity'}})","48cb66b1":"filepath = \"..\/input\/breast-cancer-wisconsin-data\/data.csv\"\n\nindex = \"id\"\ntarget = \"diagnosis\"\n\ndata = utils.load_data(filepath, index, target)","5d50173e":"data.sample(5, random_state=seed)","c60ebbd5":"del data['Unnamed: 32']","025fd1ef":"data.sample(5, random_state=seed)","502d039d":"data.diagnosis.unique()","e1d8ac4c":"(X, y) = utils.divide_dataset(data, target=\"diagnosis\")","7aa5efe4":"X.sample(5, random_state=seed)","ceef8922":"y.sample(5, random_state=seed)","1495dab9":"train_size = 0.7\n(X_train, X_test, y_train, y_test) = train_test_split(X, y,\n                                                      stratify=y,\n                                                      random_state=seed,\n                                                      train_size=train_size)","5805c20e":"train = utils.join_dataset(X_train, y_train)\ntest = utils.join_dataset(X_test, y_test)","472acac3":"train.sample(5, random_state=seed)","80cf6979":"test.sample(5, random_state=seed)","886050ab":"data.shape","ac973831":"train.shape","7980659d":"test.shape","3ea93ebd":"train.info(memory_usage=False)","e52e9f7d":"y_train.cat.categories","08ffcf38":"train.describe(include=\"category\")","95817d93":"utils.plot_barplot(train)","f4b4cb1e":"utils.plot_histogram(train)","2e634928":"train_mean = train.loc[:,'radius_mean':'fractal_dimension_mean']\ntrain_mean[\"diagnosis\"] = train[\"diagnosis\"]\n\nfig = utils.plot_pairplot(train_mean, 'diagnosis')\nfig.update_layout(width=1600, height=1400)","bc79c9c6":"X_train_mean = X_train.loc[:,'radius_mean':'fractal_dimension_mean']\n\nfig = px.imshow(X_train_mean.corr(),title=\"Mapa de Correlaci\u00f3n variables 'mean'\")\nfig.show()\n\nX_train_mean.corr()","841c305f":"X_train_se = X_train.loc[:,'radius_se':'fractal_dimension_se']\n\nfig = px.imshow(X_train_se.corr(),title=\"Mapa de Correlaci\u00f3n variables 'se'\")\nfig.show()\n\nX_train_se.corr()","b4dd84c0":"X_train_worst = X_train.loc[:,'radius_worst':'fractal_dimension_worst']\n\nfig = px.imshow(X_train_worst.corr(),title=\"Mapa de Correlaci\u00f3n variables 'worst'\")\nfig.show()\n\nX_train_worst.corr()","e3c2acf5":"f = go.Figure(data=[{'type': 'box', 'y': train[v], 'name': v} for v in set(train.columns)- {\"diagnosis\"}])\nf.show()","d5cf5505":"from sklearn.pipeline import make_pipeline\nfrom sklearn.compose import ColumnTransformer\n\npreproc = ColumnTransformer([(\"\", \"drop\", [\"concavity_mean\", \"compactness_mean\", \"concavity_se\", \"compactness_se\", \"concavity_worst\", \"compactness_worst\", \"area_mean\", \"perimeter_mean\", \"area_se\", \"perimeter_se\", \"area_worst\", \"perimeter_worst\"])], remainder=\"passthrough\")","cd9412e7":"discretizer = KBinsDiscretizer(n_bins=3, strategy=\"uniform\")","c56e80ed":"zero_r_model = DummyClassifier(strategy=\"most_frequent\")","8aad227e":"pipeline = make_pipeline(preproc, zero_r_model)","e934e446":"cfs = [] # Matriz de confusi\u00f3n para curva ROC","96eea17f":"utils.evaluate1(pipeline,\n               X_train, X_test,\n               y_train, y_test)\n\ncfs.append(confusion_matrix(y_test, pipeline.predict(X_test)))","2b6d9079":"tree_model = DecisionTreeClassifier(random_state=seed)","6fa0f7e1":"preprocessed_tree_model = make_pipeline(preproc, tree_model)\npreprocessed_discretized_tree_model = make_pipeline(preproc, discretizer, tree_model)","0283e738":"utils.evaluate1(preprocessed_tree_model,\n               X_train, X_test,\n               y_train, y_test)\n\ncfs.append(confusion_matrix(y_test, preprocessed_tree_model.predict(X_test)))","fb4c4270":"utils.evaluate1(preprocessed_discretized_tree_model,\n               X_train, X_test,\n               y_train, y_test)\n\ncfs.append(confusion_matrix(y_test, preprocessed_discretized_tree_model.predict(X_test)))","5627e572":"go.Figure([go.Scatter(x=[0, 1], y=[0, 1], line={'dash': 'dash'}, name='Clasificador aleatorio'), go.Scatter(x=[cfs[i][0, 1] \/ (cfs[i][0, 0] + cfs[i][0, 1]) for i in range(len(cfs))], y=[cfs[i][1, 1] \/ (cfs[i][1, 1] + cfs[i][1, 0]) for i in range(len(cfs))], mode='markers', hovertext=['ZeroR', '\u00c1rbol sin discretizar', '\u00c1rbol discretizando'], name='Clasificadores propuestos')], layout={'title': 'Espacio ROC', 'xaxis': {'title': '1-specificity'}, 'yaxis': {'title': 'sensitivity'}})","84e47865":"filepath = \"..\/input\/titanic\/train.csv\"\n\nindex = \"PassengerId\"\ntarget = \"Survived\"\n\ndata = utils.load_data(filepath, index, target)","139b1564":"data.sample(5, random_state=seed)","422cf53e":"data.Survived.unique()","f6a00ae0":"(X, y) = utils.divide_dataset(data, target=\"Survived\")","d63a40d5":"X.sample(5, random_state=seed)","dc78e3b5":"y.sample(5, random_state=seed)","ac951ecc":"train_size = 0.7\n(X_train, X_test, y_train, y_test) = train_test_split(X, y,\n                                                      stratify=y,\n                                                      random_state=seed,\n                                                      train_size=train_size)","6be2cb0e":"train = utils.join_dataset(X_train, y_train)\ntest = utils.join_dataset(X_test, y_test)","02a15a21":"train.sample(5, random_state=seed)","ff5d2b7c":"data.shape","61628446":"train.shape","bc820302":"test.shape","82e25afb":"train.info(memory_usage=False)","e4b4b0c0":"train.isnull().sum()","6ee6a33e":"y_train.cat.categories","4e805256":"train.describe(include=\"category\")","12965335":"utils.plot_barplot(train)","349955b3":"utils.plot_histogram(train)","b1344fbe":"fig = utils.plot_pairplot(train, 'Survived')\nfig.update_layout(width=1200, height=1000)","7745758a":"px.imshow(train.corr())","e38fa561":"train.corr()","a12eb2ce":"f = go.Figure(data=[{'type': 'box', 'y': train[v], 'name': v} for v in set(train.columns)- {\"Survived\"}])\nf.show()","0810f8fb":"from sklearn.pipeline import make_pipeline\nfrom sklearn.compose import ColumnTransformer\n\nmoda_imp = SimpleImputer(strategy='most_frequent')\npreproc = ColumnTransformer([(\"\", \"drop\", [\"Name\", \"Cabin\", \"Ticket\"]),('moda_inp', moda_imp, [\"Age\", \"Embarked\"])], remainder=\"passthrough\")","2ca18d5e":"def eliminar_outliers(X, y=None, random_state=0):\n    model = IsolationForest(random_state=random_state)\n    model.fit(X)\n    y_pred = model.predict(X)\n    if y!=None:\n        return X[y_pred == 1], y[y_pred == 1]\n    else:\n        return X[y_pred == 1]\n\neliminar_outliers = FunctionTransformer(func=eliminar_outliers, kw_args={\"random_state\":seed}, accept_sparse=True)\n","dae83144":"encoder = OneHotEncoder(handle_unknown='ignore')","710cc76f":"zero_r_model = DummyClassifier(strategy=\"most_frequent\")","a81a6f52":"pipeline = make_pipeline(preproc, encoder, eliminar_outliers, zero_r_model)","c8f993bf":"cfs = [] # Matriz de confusi\u00f3n para curva ROC","b122d816":"utils.evaluate(pipeline,\n               X_train, X_test,\n               y_train, y_test)\n\ncfs.append(confusion_matrix(y_test, pipeline.predict(X_test)))","b088100e":"tree_model = DecisionTreeClassifier(random_state=seed)","34cdc454":"preprocessed_tree_model = make_pipeline(preproc, encoder, eliminar_outliers, tree_model)","63a158bb":"utils.evaluate(preprocessed_tree_model,\n               X_train, X_test,\n               y_train, y_test)\n\ncfs.append(confusion_matrix(y_test, preprocessed_tree_model.predict(X_test)))","9d05e874":"go.Figure([go.Scatter(x=[0, 1], y=[0, 1], line={'dash': 'dash'}, name='Clasificador aleatorio'), go.Scatter(x=[cfs[i][0, 1] \/ (cfs[i][0, 0] + cfs[i][0, 1]) for i in range(len(cfs))], y=[cfs[i][1, 1] \/ (cfs[i][1, 1] + cfs[i][1, 0]) for i in range(len(cfs))], mode='markers', hovertext=['ZeroR', '\u00c1rbol sin discretizar', '\u00c1rbol discretizando'], name='Clasificadores propuestos')], layout={'title': 'Espacio ROC', 'xaxis': {'title': '1-specificity'}, 'yaxis': {'title': 'sensitivity'}})","cba6c927":"Lo siguiente que haremos es ver informaci\u00f3n sobre las variables y sus tipos.","a11facee":"Mostremos ahora las distribuciones de las distintas variables.","8d7e3a09":"Podemos ver que `Age` y `Pregnancies` presentan cierta correlaci\u00f3n (0.55) aunque no es lo suficientemente significativa para eliminar alguna de las dos. Tambi\u00e9n observamos que las variables predictoras son en general bastante independientes de la variable clase ya que no se puede observar una diferencia clara en la distribuci\u00f3n de los puntos etiquetados como 0 y como 1.\n\nAl no poder observar puntos de corte claros, tomaremos la decisi\u00f3n de realizar la discretizaci\u00f3n en tres intervalos de igual anchura.\n\n","98a62630":"# Pima Diabetes\n\nEl objetivo de este _dataset_ es predecir si un paciente tiene diabetes bas\u00e1ndose en diversos factores como la cantidad de glucosa, la edad, o el n\u00famero de embarazos. Todos los pacientes que se incluyen son mujeres de descendencia pima india.","dfee9e4e":"Como puede resultar evidente, el \u00e1rbol de decisi\u00f3n que hemos implementado obtiene mejores resultados que el algoritmo `Zero-R` puesto que no solo se quedan con la clase mayoritaria.\n\nA su vez, es importante comentar que el \u00e1rbol de decisi\u00f3n entrenado con el conjunto de datos sin discretizar nos proporciona un recall no excesivamente bueno para el problema que estamos tratando. Lo que si podemos ver es que empleando un modelo predictivo no muy complejo obtendremos unas predicciones mejores que sin \u00e9l, lo que nos demuestra la importancia de preprocesar nuestro conjunto de datos.","34fb1d3b":"Seguimos con el Mapa de Correlaci\u00f3n de las variables de tipo `se`:","12273c63":"## Aprendizaje *Zero-R*","cf535874":"Estudiando el mapa de correlaci\u00f3n podemos sacar una clara conclusi\u00f3n: Ninguna variable depende directamente de otra, como ya hab\u00edamos observado en las matrices de dispersi\u00f3n por parejas.\n\nEn el posterior preprocesamiento, esto implicar\u00e1 que si deseamos eliminar alguna variable no ser\u00e1 debido a su dependencia de otra, si no a otras caracter\u00edsticas que comentaremos m\u00e1s adelante.","e610a7db":"Mediante un **Boxplot** veremos de mejor forma como est\u00e1n distribuidos los datos de cada una, y as\u00ed ademas identificar de mejor forma aquellas que tengan valores at\u00edpicos (*outliers*), no solo fij\u00e1ndonos en el histograma. Ahora, podremos fijarnos en cada variable de forma individual para comprobar si cuentan con alg\u00fan outlier.","44c2819f":"Podemos observar que tenemos lo que parecen distribuciones normales en las variables `Glucose`, `BMI`, `BloodPressure` y `DiabetesPedigreeFunction`, teniendo esta \u00faltima una asimetr\u00eda a la derecha destacable. En las variables `Pregnacies` y `Age` podemos observar que los datos siguen una tendencia descendente conforme aumenta el valor de la variable, por lo que se alejan m\u00e1s que el resto de la distribuci\u00f3n normal y se acercan a una exponencial.\n\nAdem\u00e1s, tambi\u00e9n podemos observar la existencia de _outliers_ y valores ruidosos en algunas variables como es el caso de `Pregnancies` cuyos valores llegan a alcanzar el valor 17 y que tambi\u00e9n posee una cantidad considerable entre 6 y 14; lo cual es poco veros\u00edmil.","b5545e1b":"### An\u00e1lisis multivariado\nAhora realizaremos un an\u00e1lisis multivariado para intentar encontrar correlaciones entre las variables","5bfb3792":"## Evaluaci\u00f3n *Arbol de Decisi\u00f3n*","3b7604c6":"Comenzamos cargando el conjunto de datos `titanic`:\n\nComo es un conjunto de datos para una competici\u00f3n, nos quedaremos solo con la parte de train para el an\u00e1lisis exploratorio de los datos, puesto que es la \u00fanica que contiene la variable predictora `Survived` que ahora comentaremos.","123c149b":"Empezaremos entrenando un Zero-R. Para cada clasificador mostraremos la matriz de confusi\u00f3n y analizaremos los resultados al final.","ed4afae0":"## Aprendizaje *Arbol de Decisi\u00f3n*","6ad477fd":"Por \u00faltimo, discretizaremos antes de entrenar el \u00e1rbol","a1d7f14d":"Antes de comenzar el an\u00e1lisis exploratorio de los datos, dividiremos nuestro conjunto de datos en otros dos subconjuntos, uno de entrenamiento y otro de prueba, con los siguientes porcentajes:\n\n* Conjunto de entrenamiento: **70%**\n* Conjunto de prueba: **30%**\n\nMediante este proceso, nos aseguraremos de que los resultados posteriores del proceso de validaci\u00f3n han sido obtenidos de una manera correcta.","281a959d":"A continuaci\u00f3n, separaremos en dos subconjuntos nuestro conjunto de datos inicial, uno con las variables predictoras (`X`) y otro con la variable objetivo (`y`). ","08da9fd9":"Ahora realizaremos un an\u00e1lisis multivariado para intentar encontrar correlaciones entre las variables.\n\nPara ello, obtenemos del conjunto de entrenamiento solo las variables de tipo `mean`, puesto que son las m\u00e1s representativas. Con ellas, realizaremos una matriz de dispersi\u00f3n por parejas para comprobar las relaciones entre ellas.\n\nComo podemos ver, la variable clase se diferencia tambi\u00e9n en nuestra gr\u00e1fica, siendo el color naranja el diagn\u00f3stico `M`, y su hom\u00f3logo azul el diagn\u00f3stico `B`:","9b0bac7a":"Comenzamos cargando el conjunto de datos `wisconsin`:","b7736204":"Nuevamente, vamos a asegurarnos de que el conjunto de datos se ha dividido correctamente. Para ello, visualizaremos las variables del conjunto de datos de entrenamiento, observando que la variable clase tambi\u00e9n aparece al final del conjunto:","773d1484":"## Evaluaci\u00f3n *Arbol de Decisi\u00f3n*","44b90d81":"Comprobaremos que se hayan separado correctamente, empezamos mostrando las variables predictoras:","2bb6a587":"### Visualizaci\u00f3n de las variables","11e54ea4":"Observando los resultados de los dos clasificadores, podemos decir que los \u00e1rboles de decisi\u00f3n son superiores al ZeroR en *accuracy*, adem\u00e1s de alcanzar un mejor compromiso entre *sensitivity* y *specificity*. Como en los conjuntos de datos estudiados previamente en esta libreta, hemos optado por eliminar la discretizaci\u00f3n previa puesto que no hemos encontrado que haya sido significativamente \u00fatil.","167d6a6b":"Seguidamente, lo que haremos ser\u00e1 unir los conjuntos `X_train` e `y_train` para obtener el conjunto de datos de entrenamiento.\nHaremos los mismo para `X_test` e `y_test`, juntando as\u00ed el conjunto de datos de test.","80ff6392":"Comenzaremos mostrando un **histograma** con todas las variables, para mostrar la densidad de ejemplos para los distintos valores de las variables num\u00e9ricas y analizar las tendencias que estas toman:","8a7de765":"Podemos ver que casi un 50% de los valores de la variable `Insulin` y un 30% de la variable `SkinThickness` no son v\u00e1lidos, por lo que hemos decidido eliminarlas al tener demasiado sesgo. Para el resto de variables imputaremos los valores perdidos, por la media en el caso en los tres casos al ser variables que admiten valores reales. Tras esto mostraremos los histogramas otra vez para ver c\u00f3mo son realmente las distribuciones sin que se vean influenciadas por los valores perdidos.","148b25f7":"Comprobamos lo mismo para nuestro nuevo conjunto de prueba:","ffde6ecb":"A la hora de visualizar las variables, comenzaremos comprobando la distribuci\u00f3n de la variable clase de nuestro conjunto de entrenamiento. Como vemos, hay 384 instancias de la clase mayoritaria `0`, es decir que no sobrevivieron, y 239 de la clase `1`, es decir que sobrevivieron.","2b6c3482":"De acuerdo con esto, encotramos variedad entre las variables predictoras del conjunto de datos. Tenemos variables de tipo `object`, `int64`, y `float`. Sin embargo, la variable clase (`Survived`) es categ\u00f3rica y contiene dos estados, cuyos valores ser\u00e1n `0`y`1` como ya hemos comentado anteriormente:","2c28f43d":"Se puede observar mejor de la siguiente manera, comprobando gr\u00e1ficamente qu\u00e9 clase es la que predomina:","0c1ccbd0":"Primero recopilaremos el preprocesamiento que hemos decidido que queremos hacer en un pipeline.\n\nEn primer lugar, aplicaremos una imputaci\u00f3n para las variables con valores perdidos que se vieron en el an\u00e1lisis exploratorio y que permanecer\u00e1n en el conjunto de datos como `BMI, Glucose y BloodPressure`. El m\u00e9todo de imputaci\u00f3n a seguir ser\u00e1 por la media, de modo que estos valores perdidos se ver\u00e1n sustituidos por la media.\n\nAdem\u00e1s, eliminaremos las variables que se han comentado previamente en el an\u00e1lisis exploratorio, como son `Insulin y SkinThickness`.","8e61c66e":"Mostremos que el proceso ha sido satisfactorio mostrando el conjunto de train.","a869aadd":"Para ver claramente la posible existencia de _outliers_ en las distribuciones, las visualizaremos mediante un diagrama de cajas.","cb1c01c9":"Para conocer cu\u00e1l es el tipo de las variables, recurrimos al m\u00e9todo `info`:","2256077f":"A continuaci\u00f3n, separaremos en dos subconjuntos nuestro conjunto de datos inicial, uno con las variables predictoras (`X`) y otro con la variable objetivo (`y`). ","ad66abfa":"Comprobaremos que se hayan separado correctamente:","bc998315":"Aplicamos nuestro preprocesamiento al conjunto de entrenamiento y al conjunto de test:","7bb5422b":"---","f989d0ec":"A la hora de visualizar las variables, comenzaremos comprobando la distribuci\u00f3n de la variable clase de nuestro conjunto de entrenamiento. Como vemos, hay 250 instancias de la clase mayoritaria `B`, y 148 de la clase `M`.","a37055ed":"Una vez tenemos bien definidos nuestros conjuntos de entrenamiento y prueba, podemos pasar con el an\u00e1lisis exploratorio de datos.","4aef5cfd":"### Codificaci\u00f3n de valores","b7727540":"##\u00a03. Aprendizaje y Evaluaci\u00f3n *Zero-R*","71a56691":"Mediante el estudio de este histograma en el que se han representado todas las variables, podremos extraer diversas conclusiones que nos ser\u00e1n \u00fatiles a la hora de preprocesar los datos de nuestro conjunto:\n* Mirando las variables independientemente, encontramos outliers en las variables `radius, area, y perimeter` en sus tres variantes, lo que ser\u00e1 algo a tener en cuenta a la hora de preprocesar los datos si queremos eliminarlos.\n* Estimaci\u00f3n de las distribuciones:\n    * Si analizamos las variables `mean`, veremos que `radius, perimeter y area` siguen una distribuci\u00f3n normal al igual que la mayor\u00eda de variables, excepto las relacionadas con la concavidad, que pueden llegar a tomar una distribuci\u00f3n exponencial.\n    * Si analizamos las variables `se`, veremos que `radius, perimeter y area` siguen ahora una distribuci\u00f3n exponencial, al igual que las relacionadas con la concavidad.\n    * Si analizamos las variables `worst`, veremos que `radius, perimeter y area` vuelven a seguir una distribuci\u00f3n normal, y las relacionadas con la concavidad pasan a ser distintas, ahora solo `concavity` sigue una tendencia exponencial, mientras que `concave_points` pasa a tomar una clara distribuci\u00f3n normal.\n* Tras este paso, vemos tambi\u00e9n que todas las distribuciones tienden a ir hacia la **derecha**.","49ffead6":"Estudiando la matriz de dispersi\u00f3n de las variables predictoras ordenadas por parejas, podemos sacar como conclusi\u00f3n que las variables `perimeter y area` dependen fuertemente de `radius`.\n\nIgualmente, podemos apreciar c\u00f3mo las variables `concavity y compactness` dependen tambi\u00e9n en gran medida de `concave points`.\n\nA continuaci\u00f3n, realizaremos diversas observaciones de estos datos para ver si realmente las dependencias ya mencionadas se ven reflejadas en el conjunto de datos.","ac11912d":"Nuevamente, vamos a asegurarnos de que el conjunto de datos se ha dividido correctamente. Comenzamos con las variables del conjunto de datos de entrenamiento, observando que la variable clase tambi\u00e9n aparece al final del conjunto:","38b6efd7":"Como vemos, encontramos valores nulos en ciertas variables. Ve\u00e1moslo mejor de la siguiente manera:","b1b3c3e3":"Como podemos observar, tenemos una columna, la \u00faltima, cuyo nombre es `Unnamed` y todo el contenido de sus filas `NaN`. Esto significa que antes de continuar trabajando con nuestra base de datos, debemos borrarla.","c8bf19e4":"## Aprendizaje *Arbol de Decisi\u00f3n*","a0d8fe1f":"## Evaluaci\u00f3n *Zero-R*","6a05120a":"Como observamos antes, existen muchos valores perdidos que vienen representados con 0 en variables en las que este valor no est\u00e1 entre los valores razonables para la variable. Es el caso de `Glucose`, `BloodPressure`, `SkinThickness`, `Insulin` y `BMI`. Antes de hacer nada m\u00e1s, veamos qu\u00e9 fracci\u00f3n del total de valores est\u00e1n perdidos por variable.","05891977":"Aplicamos el pipeline al \u00e1rbol de decisi\u00f3n creado. Diferenciaremos en dos:\n* Solo preprocesamiento (eliminaci\u00f3n de variables que no son necesarias)\n* Preprocesamiento y discretizaci\u00f3n","469d4954":"Aplicamos nuestro preprocesamiento al conjunto de entrenamiento y al conjunto de test:","96181660":"### Descripci\u00f3n del conjunto de datos\nEl n\u00famero de casos y variables (respectivamente) del conjunto de datos se puede obtener consultando el atributo `shape`:\nObservamos como de los 891 registros iniciales, tenemos **623** de ellos para el entrenamiento (un 70%), y **268** (un 30%) para el conjunto de prueba, ambos con 11 variables, las 10 predictoras y la variable objetivo.","6335e75f":"# Wisconsin","28060a05":"Se puede observar mejor de la siguiente manera, comprobando gr\u00e1ficamente qu\u00e9 clase es la que predomina:","3ac3e218":"Para la detecci\u00f3n y eliminaci\u00f3n de los outliers emplearemos un ensemble llamado `IsolationForest` de scikit-learn. Cuando \u00e9ste detecte un outlier, borrar\u00e1 toda la instancia.","fa486e47":"Otra cosa que observamos gracias a aplicar el m\u00e9todo `info` sobre nuestro conjunto de datos de entrenamiento es que en realidad solo contamos con **10 variables** reales, pero que se convierten en 30 puesto que est\u00e1n divididas en 3 categor\u00edas `mean`, `se` y `worst`. Es importante considerar esto cuando pasemos a visualizar las variables posteriormente, puesto que al haber tal cantidad de variables ser\u00e1 mejor una representaci\u00f3n dividida por estas 3 categor\u00edas.","ae43ae7d":"Ahora realizaremos un **mapa de correlaci\u00f3n** para poder visualizar de una manera m\u00e1s clara las correlaciones entre las variables num\u00e9ricas de nuestro conjunto.","c101855d":"Una vez hemos cargado el conjunto de datos, mostraremos 5 registros aleatorios mediante la funci\u00f3n `sample` para comprobar que el proceso ha sido realizado correctamente","8033c372":"## 2. Preprocesamiento de datos","828b42db":"Empezamos mostrando las variables predictoras:","070029d2":"Ahora es el momento de entrenar y validar nuestros clasificadores. Para ello, vamos a usar una matriz de confusi\u00f3n y tasa de acierto.","14214185":"Como puede resultar evidente, los \u00e1rboles de decisi\u00f3n que hemos implementado obtienen mejores resultados que el algoritmo `Zero-R` puesto que no solo se quedan con la clase mayoritaria.\n\nA su vez, es importante comentar que el \u00e1rbol de decisi\u00f3n entrenado con el conjunto de datos discretizado (tras realizar las modificaciones necesarias que aprendimos en el an\u00e1lisis exploratorio) obtiene una tasa de acierto similar al \u00e1rbol de decisi\u00f3n con el conjunto de datos sin discretizar para este problema concreto.\n\nLa conclusi\u00f3n que podemos obtener es que con un sencillo preprocesamiento y empleando un modelo predictivo no muy complejo obtendremos unas predicciones con una precisi\u00f3n de un 92%, lo que nos demuestra la importancia de preprocesar nuestro conjunto de datos.","60a2541c":"Una vez hemos realizado nuestro preprocesamiento de datos, es hora de pasar a aprender los modelos y posteriormente evaluarlos haciendo uso del procesamiento que hemos llevado a cabo, lo que implica a\u00f1adir al **pipeline** el preprocesamiento (eliminaci\u00f3n de variables e imputaci\u00f3n de valores), la codificaci\u00f3n one-hot y la eliminaci\u00f3n de outliers.","55353e01":"Tenemos una cantidad considerable de *outliers* sobre todo en las variables `BloodPressure`, `DiabetesPedigreeFunction` y `Age`.","33eb3352":"Aplicamos el pipeline al \u00e1rbol de decisi\u00f3n creado con el preprocesamiento aplicado.","3fed2754":"Y con el conjunto de datos discretizado:","40ef843e":"## 2. Preprocesamiento de datos","afd1e024":"De acuerdo con esto, todas las variables predictoras del conjunto de datos son num\u00e9ricas (`float64`). Sin embargo, la variable clase (`diagnosis`) es categ\u00f3rica y contiene dos estados, cuyos valores ser\u00e1n `M`y`B` como ya hemos comentado anteriormente:","905dbd46":"Ahora realizaremos 3 distintos **Mapas de Correlaci\u00f3n**. La respuesta a esto es que separaremos los 3 tipos de variables `mean, se y worst` en mapas distintos, para poder visualizar de una manera m\u00e1s clara las correlaciones entre las variables de nuestro conjunto.\n\nComenzamos con el Mapa de Correlaci\u00f3n de las variables de tipo `mean`:","ef3c7458":"Como podemos observar tenemos una sola variable categ\u00f3rica (la clase) mientras que las predictoras son todas n\u00famericas (6 que solo toman valores enteros en el conjunto de datos y 2 con valores reales).\n\nLas variables con las que trabajaremos son:\n\n* Pregnancies: el n\u00famero de embarazos.\n* Glucose: la concentraci\u00f3n de glucosa en plasma.\n* BloodPressure: la presi\u00f3n arterial diast\u00f3lica (mmHg).\n* SkinThickness: el espesor del pliegue del tr\u00edceps (mm).\n* Insulin: la insulina en suero.\n* BMI: el \u00edndice de masa corporal ($\\frac{kg}{m^2}$)\n* DiabetesPedigreeFunction: funci\u00f3n de pedigree de diabetes.\n* Age: la edad.\n* Outcome: si tiene diabetes o no.","7c42c384":"### Descripci\u00f3n del conjunto de datos\nEl n\u00famero de casos y variables (respectivamente) del conjunto de datos se puede obtener consultando el atributo `shape`:\nObservamos como de los 569 registros iniciales, tenemos **398** de ellos para el entrenamiento (un 70%), y **171** (un 30%) para el conjunto de prueba, ambos con 31 variables, las 30 predictoras y la variable objetivo.","fec0bb6b":"Comenzaremos mostrando un **histograma** con todas las variables, para mostrar la densidad de ejemplos para los distintos valores de las variables num\u00e9ricas y analizar las tendencias que estas toman:","db2face7":"Seguidamente, lo que haremos ser\u00e1 unir los conjuntos `X_train` e `y_train` para obtener el conjunto de datos de entrenamiento.\nHaremos los mismo para `X_test` e `y_test`, juntando as\u00ed el conjunto de datos de test.","73bf85f8":"Estudiando la matriz de dispersi\u00f3n de las variables predictoras ordenadas por parejas, podemos sacar como conclusi\u00f3n que a primera vista ninguna de las variables est\u00e1 directamente relacionada con otra, por lo que cada una aporta una informaci\u00f3n distinta.\n\nA continuaci\u00f3n, realizaremos diversas observaciones de estos datos para ver si se nos hubiesen escapado dependencias en el conjunto de datos.","015abb58":"----","4326e51e":"Las conclusiones que podemos obtener de este diagrama es que encontramos **outliers** en las variables `radius, area, y perimeter` en sus tres variantes (como ya hab\u00edamos comentado en el histograma), y adem\u00e1s las variables `smoothness y concavity` cuentan con valores at\u00edpicos.\n\nEsto ser\u00e1 algo que ser\u00e1 algo a tener en cuenta a la hora de preprocesar los datos si queremos eliminarlos.","c1f063c3":"Ahora es el momento de entrenar y validar nuestros clasificadores. Para ello, vamos a usar una matriz de confusi\u00f3n y tasa de acierto.","e7be015d":"##\u00a03. Aprendizaje y Evaluaci\u00f3n *Zero-R*","7c0e5e99":"## Conclusi\u00f3n","81141d2b":"Una vez tenemos bien definidos nuestros conjuntos de entrenamiento y prueba, podemos pasar con el an\u00e1lisis exploratorio de datos.","673e2ab8":"El an\u00e1lisis exploratorio de datos es un paso fundamental a la hora de comprender los datos con los que vamos a trabajar.\n\nEl objetivo de este an\u00e1lisis es explorar, describir y visualizar la naturaleza de los datos recogidos mediante la aplicaci\u00f3n de t\u00e9cnicas simples de resumen de datos y m\u00e9todos gr\u00e1ficos, para observar las posibles relaciones entre las variables de nuestro conjunto de datos.\n\nPara comenzar, veremos una descripci\u00f3n del conjunto de datos que vamos a emplear.","4ca17d19":"Como era de esperar, el modelo *Zero-R* obtiene malos resultados, pues solo predice la clase mayoritaria en el conjunto de entrenamiento, en este caso 0 (B).","b7453ffb":"Antes de comenzar el an\u00e1lisis exploratorio de los datos, dividiremos nuestro conjunto de datos en otros dos subconjuntos, uno de entrenamiento y otro de prueba, con los siguientes porcentajes:\n\n* Conjunto de entrenamiento: **70%**\n* Conjunto de prueba: **30%**\n\nMediante este proceso, nos aseguraremos de que los resultados posteriores del proceso de validaci\u00f3n han sido obtenidos de una manera correcta.","43763992":"Sin embargo la cantidad de datos que existen de cada clase no es la misma, por lo que estamos ante un problema desbalanceado.","ad5048dd":"El an\u00e1lisis exploratorio de datos es un paso fundamental a la hora de comprender los datos con los que vamos a trabajar.\n\nEl objetivo de este an\u00e1lisis es explorar, describir y visualizar la naturaleza de los datos recogidos mediante la aplicaci\u00f3n de t\u00e9cnicas simples de resumen de datos y m\u00e9todos gr\u00e1ficos, para observar las posibles relaciones entre las variables de nuestro conjunto de datos.\n\nPara comenzar, veremos una descripci\u00f3n del conjunto de datos que vamos a emplear.","97da25b7":"## 2. Preprocesamiento de datos","bcc1427a":"## 1. An\u00e1lisis exploratorio de datos\n\n### An\u00e1lisis univariado\n\nPrimero veamos c\u00f3mo se distribuyen las diferentes variables.","c54bf6fa":"Observando los resultados de los tres clasificadores, podemos decir que los \u00e1rboles de decisi\u00f3n son superiores al ZeroR en *accuracy*, adem\u00e1s de alcanzar un mejor compromiso entre *sensitivity* y *specificity*. Podemos observar estos valores individualmente tambi\u00e9n y vemos que si nos interesa optimizar el *recall*, que suele ser el caso con ciertos problemas m\u00e9dicos como este, el que mejor valor obtiene en esta m\u00e9trica son los \u00e1rbol entrenado sin discretizar previamente (puesto que el \u00e1rbol discretizado nos aporta el mismo resultado). Por tanto en este problema podemos observar que la discretizaci\u00f3n previa no ha sido significativamente \u00fatil.","2c5c45bc":"Lo primero que haremos es cargar los datos y dividirlos en conjunto de entrenamiento y de test estratificando. En este conjunto de datos no tenemos index por lo que lo indicaremos con `None`.","616228e9":"Si nos centramos en cada variable de manera aislada, podemos observar lo siguiente:\n\nEn cuanto a las variables como `Cabin, Sex, Name, Pclass y Ticket`, no podemos considerar outliers puesto que son variables con valores totalmente distintos y por ello no podr\u00e1n contar con valores at\u00edpicos.\n\nOtra de las conclusiones que podemos obtener de este diagrama es que encontramos **outliers** en las variables `SibSp, Fare, y Embarked` como ya hab\u00edamos comentado en el histograma. Esto es interesante, puesto que nos indica que la mayor\u00eda de gente embarc\u00f3 en Southampton, y se considera a los otros lugares de embarque como valores at\u00edpicos.\n\nEsto ser\u00e1 algo que ser\u00e1 algo a tener en cuenta a la hora de preprocesar los datos si queremos eliminarlos.","2f8cf007":"En la variable clase no parece haber anomal\u00edas en cuanto a su dominio.","7cc50289":"Lo que podemos observar es que las clases de la variable objetivo del problema no tienen el mismo n\u00famero de casos, es decir, el problema est\u00e1 desbalanceado (las frecuencias de las combinaciones de estados no aparecen en la misma proporci\u00f3n).\n\nAhora que sabemos que el problema es **desbalanceado**, es el turno de visualizar las variables predictoras del conjunto de datos.","b812ba4f":"Estudiando los tres mapas de correlaci\u00f3n podemos sacar una clara conclusi\u00f3n: **Las variables `perimeter y area` dependen directamente de `radius`.** Esto significa que m\u00e1s adelante en el preprocesamiento de datos podremos eliminar las variables `perimeter y area` sin problema.\n\nDe la misma manera, vemos c\u00f3mo las variables `concavity y compactness` dependen tambi\u00e9n en gran medida de `concave points`, por lo que ambas variables podr\u00e1n tambier ser eliminadas posteriormente, debido a su dependencia con esta \u00faltima.","f003cd49":"Con el objeto de obtener unos datos m\u00e1s simples y convertir las variables num\u00e9ricas en intervalos vamos a discretizar esas variables, siendo en este caso, todas las variables predictoras.\n\nPara simplificar los datos realizaremos un proceso de discretizaci\u00f3n de los datos. Esto quiere decir que convertiremos las variables que son num\u00e9ricas (todas las variables predictoras que tengamos **despu\u00e9s de la selecci\u00f3n de variables** que hemos hecho previamente) en variables agrupadas por intervalos, lo que restar\u00e1 complejidad al modelo.\n\nAl no poder observar puntos de corte claros, tomaremos arbitrariamente la decisi\u00f3n de realizar la discretizaci\u00f3n en tres intervalos de igual anchura.","dd27d795":"## Evaluaci\u00f3n *Zero-R*","6ca7d9d6":"En esta etapa limpiaremos y organizaremos los datos de manera adecuada para entrenar a nuestro modelo bas\u00e1ndonos en las observaci\u00f3n que hemos realizado en el an\u00e1lisis exploratorio de datos previo. Por ello, en este conjunto de datos nos centraremos en la selecci\u00f3n de variables adecuadas para conseguir reducir el n\u00famero de estas.\n\n\nPara realizar este proceso, haremos uso de un **Pipeline**. Este Pipeline ser\u00e1 el encargado de aplicar las transformaciones que hemos decidido a nuestro conjunto de datos.\n\nComo hemos decidido anteriormente graci\u00e1s al an\u00e1lisis exploratorio de los datos, debemos eliminar las columnas (variables) seleccionadas previamente:\n* Aquellas relacionadas con los `concave points`: `concavity_mean, compactness_mean, concavity_se, compactness_se, concavity_worst y compactness_worst`.\n* Aquellas relacionadas con `radius`, es decir: `area_mean, perimeter_mean, area_se, perimeter_se, area_worst y perimeter_worst`.\n\nLa respuesta a por qu\u00e9 estas columnas son las mencionadas previamente, porque son variables que dependen directamente de las 2 que vamos a dejar en nuestro conjunto de datos: `radius y concave points`.","03d74bb1":"##\u00a04. Aprendizaje y Evaluaci\u00f3n *\u00c1rbol de Decisi\u00f3n*","e72cdc4f":"Una vez estudiado el algoritmo `Zero-R` probaremos un nuevo m\u00e9todo, la creaci\u00f3n de un \u00e1rbol de decisi\u00f3n.\n\nPara obtener este arbol de decisi\u00f3n, usaremos el estimador `DecisionTreeClassifier` de `scikit-learn`, sin olvidar fijar la semilla que definimos al principio de la libreta para asegurar que los experimentos sean reproducibles:","3ef4d343":"## Conclusi\u00f3n","9069b93d":"Mediante el estudio de este histograma en el que se han representado todas las variables, podremos extraer diversas conclusiones que nos ser\u00e1n \u00fatiles a la hora de preprocesar los datos de nuestro conjunto:\n* Mirando las variables independientemente, encontramos outliers en las variables `SibSp`, ya que ciertas personas contaban con 8 familiares en el barco, lo que se aleja mucho de lo com\u00fan. Tambi\u00e9n la variable `Fare` tiene outliers, puesto que varias personas se alejan mucho de la tarifa media (unos 15), llegando hasta valores de m\u00e1s de 500. Esto ser\u00e1 algo a tener en cuenta a la hora de preprocesar los datos si queremos eliminarlos.\n* Estimaci\u00f3n de las distribuciones:\n    * Si analizamos la variable `age`, veremos que sigue una distribuci\u00f3n normal con tendencia a la derecha (mayor edad).\n    * Si analizamos las variables `SibSp y Parch`, veremos que siguen una distribuci\u00f3n exponencial.","00ba3f16":"### Visualizaci\u00f3n de las variables","33d4d82d":"## 4. Aprendizaje y Evaluaci\u00f3n *Arbol de Decisi\u00f3n*","691d2789":"Una vez estudiado el algoritmo `Zero-R` probaremos un nuevo m\u00e9todo, la creaci\u00f3n de un \u00e1rbol de decisi\u00f3n.\n\nPara obtener este arbol de decisi\u00f3n, usaremos el estimador `DecisionTreeClassifier` de `scikit-learn`, sin olvidar fijar la semilla que definimos al principio de la libreta para asegurar que los experimentos sean reproducibles:","46bbc32d":"## 4. Aprendizaje y Evaluaci\u00f3n *Arbol de Decisi\u00f3n*","635b0912":"Como era de esperar, el modelo *Zero-R* obtiene malos resultados, pues solo predice la clase mayoritaria en el conjunto de entrenamiento, en este caso 0 (No supervivientes).","20353e7e":"Mediante un codificaci\u00f3n **one-hot**, podremos trabajar c\u00f3modamente con las variables categ\u00f3ricas y objetos a la hora de crear los algoritmos con los que evaluaremos el conjunto de datos.","8a4b431a":"###\u00a0Discretizaci\u00f3n","d32e11e5":"No se puede observar ninguna partici\u00f3n clara de los puntos con ninguna de las variables que nos ayude a decidirnos sobre hacer alguna discretizaci\u00f3n.","164145c1":"Nos ser\u00e1 \u00fatil juntar el conjunto de entrenamiento en una sola variable que contenga ambas las variables predictoras y la variable clase.","bc509212":"Es muy \u00fatil disponer del conjunto de datos separado dos subconjuntos, uno con las variables predictoras (`X`) y otro con la variable objetivo (`y`). Se puede utilizar el siguiente fragmento de c\u00f3digo para dividirlo: ","acc2f068":"Este conjunto de datos est\u00e1 compuesto por 10 variables predictoras que recogen datos acerca de los pasajeros del Titanic y la variable clase `Survived` que nos indicar\u00e1 si el pasajero muri\u00f3 (0) o sobrevivi\u00f3 (1).\nLas variables predictoras, por tanto, son:\n* `Pclass`: Estatus socio-econ\u00f3mico de cada pasajero\n* `Name`: Los nombres de los pasajeros\n* `Sex`: Sexo de los pasajeros\n* `Age`: Edad de los pasajeros\n* `SibSp`: Siblings & Spouses. N\u00famero de hermanos y parejas que viajaban en el barco\n* `Parch`: N\u00famero de parientes que cada pasajero ten\u00eda en el barco\n* `Ticket`: N\u00famero de su ticket de embarque\n* `Fare`: Precio de la tarifa que cada pasajero pag\u00f3\n* `Cabin`: Camarote en los que se alojaban los pasajeros\n* `Embarked`: Ciudad de embarque (Southampton, Cherburgo y Queenstown)\n\nEl conocimiento de estas variables nos ayudar\u00e1 a entender mejor como tratar los datos a la hora del an\u00e1lisis.","7cbe0d3c":"Lo que este algoritmo har\u00e1 ser\u00e1 aprender un clasificador que asigne a los casos del conjunto de test la clase predominante en el conjunto de entrenamiento (ya vimos que el problema era desbalanceado). Como veremos, en nuestro conjunto de datos concreto no destaca por su efectividad.","2de722fa":"En esta etapa limpiaremos y organizaremos los datos de manera adecuada para entrenar a nuestro modelo bas\u00e1ndonos en las observaci\u00f3n que hemos realizado en el an\u00e1lisis exploratorio de datos previo. Por ello, en este conjunto de datos nos centraremos en la selecci\u00f3n de variables adecuadas para conseguir reducir el n\u00famero de estas.\n\n\nPara realizar este proceso, haremos uso de un **Pipeline**. Este Pipeline ser\u00e1 el encargado de aplicar las transformaciones que hemos decidido a nuestro conjunto de datos.\n\nGracias al an\u00e1lisis exploratorio de los datos, podemos eliminar diversas columnas que no aportan apenas informaci\u00f3n relevante:\n* `Name, Cabin, Ticket`.\nLa respuesta a por qu\u00e9 estas columnas, es porque son variables que aportan informaci\u00f3n diferente para cada instancia del conjunto de datos, por lo que no aporta informaci\u00f3n relevamente. Adem\u00e1s como hemos comentado previamente, `Cabin` cuenta con multitud de valores nulos.\n\nAdem\u00e1s, aplicaremos una imputaci\u00f3n para las variables con valores perdidos que se vieron en el an\u00e1lisis exploratorio y que permanecer\u00e1n en el conjunto de datos como `Age y Embarked`. El m\u00e9todo de imputaci\u00f3n a seguir ser\u00e1 por moda (most_frequent), de modo que los valores perdidos se ver\u00e1n sustituidos por la moda.","09d705e4":"### Eliminaci\u00f3n de outliers","228404bb":"Usaremos un tama\u00f1o del conjunto de entrenamiento y de test del 70% y 30% respectivamente.","3a25a4f7":"Observando los resultados de los tres clasificadores, podemos decir que los \u00e1rboles de decisi\u00f3n son superiores al ZeroR en *accuracy*, adem\u00e1s de alcanzar un mejor compromiso entre *sensitivity* y *specificity*. Podemos observar estos valores individualmente tambi\u00e9n y vemos que si nos interesa optimizar el *recall*, que suele ser el caso con ciertos problemas m\u00e9dicos como este, el que mejor valor obtiene en esta m\u00e9trica es \u00e1rbol entrenado sin discretizar previamente. Por tanto en este problema podemos observar que la discretizaci\u00f3n previa no ha sido significativamente \u00fatil.\n\nSin embargo, entre los dos \u00e1rboles, la discretizaci\u00f3n previa parece que mejora ligeramente las medidas consideradas cuando consideramos ambas _sensitivity_ y _specificity_ con respecto a no realizarla.","2024e69f":"Lo que este algoritmo har\u00e1 ser\u00e1 aprender un clasificador que asigne a los casos del conjunto de test la clase predominante en el conjunto de entrenamiento (ya vimos que el problema era desbalanceado). Como veremos, en nuestro conjunto de datos concreto no destaca por su efectividad.","89485537":"Vamos a ver los resultados del \u00e1rbol de decisi\u00f3n:","ff0388eb":"## Aprendizaje *Zero-R*","2a44d3e3":"Ahora realizaremos un an\u00e1lisis multivariado para intentar encontrar correlaciones entre las variables.\n\nPara ello, obtenemos del conjunto de entrenamiento todas las variables pese a que algunas son m\u00e1s representativas que otras. Con ellas, realizaremos una matriz de dispersi\u00f3n por parejas para comprobar las relaciones entre ellas.\n\nComo podemos ver, la variable clase se diferencia tambi\u00e9n en nuestra gr\u00e1fica, siendo el color naranja los no supervivientes `0`, y su hom\u00f3logo azul los supervivientes `1`:","ee256974":"Lo que podemos observar es que las clases de la variable objetivo del problema no tienen el mismo n\u00famero de casos, es decir, el problema est\u00e1 desbalanceado (las frecuencias de las combinaciones de estados no aparecen en la misma proporci\u00f3n).\n\nAhora que sabemos que el problema es **desbalanceado**, es el turno de visualizar las variables predictoras del conjunto de datos.","d0fdbef4":"##\u00a03. Aprendizaje y Evaluaci\u00f3n *Zero-R*","6dfae7c1":"La variable `Cabin` contiene una gran cantidad de valores nulos, por lo que a la hora de visualizar las variables no debemos tenerla muy en cuenta, y en el posterior procesamiento ser\u00e1 eliminda puesto que a primera vista no aporta informaci\u00f3n relevante.","5e1ee288":"# Pr\u00e1ctica 1: An\u00e1lisis exploratorio de datos, preprocesamiento y validaci\u00f3n de modelos de clasificaci\u00f3n\n\n####\u00a0Miner\u00eda de Datos: Curso acad\u00e9mico 2020-2021\n\n### Profesorado:\n\n* Juan Carlos Alfaro Jim\u00e9nez\n* Jos\u00e9 Antonio G\u00e1mez Mart\u00edn\n\n### Realizado por:\n\n* Antonio Beltr\u00e1n Navarro\n* Ram\u00f3n Jes\u00fas Mart\u00ednez S\u00e1nchez","30ea1060":"## 1. An\u00e1lisis exploratorio de datos","dfdea0be":"Vamos a ver los resultados del \u00e1rbol de decisi\u00f3n sin el conjunto de datos discretizado:","1ecffd15":"# Titanic","cd01dae2":"Y a continuaci\u00f3n la variable objetivo:","f87cf9f5":"Y a continuaci\u00f3n la variable objetivo:","acb4d662":"Aqu\u00ed ya podemos observar ciertas anomal\u00edas, como por ejemplo que existe el valor 0 en variables donde no tiene sentido como `Glucose`, `SkinThickness` o `Insulin`. Adem\u00e1s en las dos \u00faltimas este valor tambi\u00e9n marca el primer cuartil lo que indica una gran cantidad de datos err\u00f3neos. Por el otro extremo, los valores m\u00e1ximos de algunas variables parecen tambi\u00e9n ser poco veros\u00edmiles y probablemente _outliers_ como es el caso de 17 en `Pregnancies`.","c10d588c":"Comprobamos que se ha borrado correctamente haciendo uso del m\u00e9todo `sample` de nuevo:","f72d059b":"Ahora usaremos un \u00e1rbol de clasificaci\u00f3n sin discretizaci\u00f3n previa y con los hiperpar\u00e1metros por defecto:\n","bfb49ccc":"Una vez hemos cargado el conjunto de datos, mostraremos 5 registros aleatorios mediante la funci\u00f3n `sample` para comprobar que el proceso ha sido realizado correctamente","992a0a14":"Mediante un **Boxplot** veremos de mejor forma como est\u00e1n distribuidos los datos de cada variable predictora, y as\u00ed ademas identificar de mejor forma aquellas que tengan valores at\u00edpicos (*outliers*), no solo fij\u00e1ndonos en el histograma. Ahora, podremos fijarnos en cada variable de forma individual para comprobar si cuentan con alg\u00fan outlier.","94fc9521":"## 1. An\u00e1lisis exploratorio de datos","424f633a":"Para conocer cu\u00e1l es el tipo de las variables, recurrimos al m\u00e9todo `info`:"}}