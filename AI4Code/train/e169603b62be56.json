{"cell_type":{"3aeea1da":"code","4af034d4":"code","a59aa61d":"code","ede03414":"code","83cb0cdc":"code","2c9b1210":"code","94441ac7":"code","ac4cab47":"code","9bbe0ca4":"code","3b38431c":"code","53bbe977":"code","9345ac0c":"code","3502f82b":"code","796476f4":"code","8b4db7fc":"code","cd069ec7":"code","1ca5eff0":"code","5a2eedc1":"code","454b438e":"code","3d6a23aa":"code","641d184b":"code","801fc9d9":"code","4345207d":"code","912265da":"code","5b4007de":"code","388a0f59":"code","cc1d9f72":"code","63b1de6c":"code","af0d6c2e":"code","156b63b9":"code","62d16ca9":"code","7f95617b":"code","762c8528":"code","4c636b1f":"markdown","d6662e12":"markdown","70762718":"markdown","e1f67b97":"markdown","6f1135d2":"markdown","82bcb278":"markdown"},"source":{"3aeea1da":"import numpy as np\nimport pandas as pd\nimport math\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.linear_model import Ridge\nfrom sklearn.model_selection import GridSearchCV ,RepeatedKFold\nfrom sklearn.model_selection import train_test_split,cross_val_score, KFold \nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import roc_auc_score, roc_curve\nimport sklearn.metrics as sklm\nimport matplotlib.pyplot as plt","4af034d4":"data_train = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ndata_test = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')\ntrain= data_train.copy()\ntest = data_test.copy()\ntrain = train.drop('Id', axis=1)\ntest = test.drop('Id', axis=1)","a59aa61d":"train.head(5)\n\n","ede03414":"train.info()","83cb0cdc":"test.info()","2c9b1210":"# columns int tain set have more than 80% of nan \nnan_values_train = train.isnull().sum()\npourcentage_train =pd.DataFrame((np.array(nan_values_train)\/1459)*100 ,index=nan_values_train.index)\npourcentage_train[pourcentage_train[0]>80]","94441ac7":"# columns int test set have more than 80% of nan values\n\nnan_values_test = test.isnull().sum()\npourcentage_test = pd.DataFrame((np.array(nan_values_train)\/1459)*100 ,index=nan_values_train.index)\npourcentage_test[pourcentage_test[0]>80]","ac4cab47":"train = train.drop(['Alley','PoolQC','Fence','MiscFeature'], axis=1)\n\ntest = test.drop(['Alley','PoolQC','Fence','MiscFeature'], axis=1)\n","9bbe0ca4":"# filling the nan for tain set\nmissing_value_train = train.isnull().sum()\n\nfor col in list(missing_value_train.index):\n    if train[col].dtype == 'object':\n        train[col].fillna(train[col].value_counts().index[0], inplace=True)\n    else:\n        train[col].fillna(train[col].mean(), inplace=True)\n        \n# filling the nan for test set        \nmissing_value_test = test.isnull().sum()\n\nfor col in list(missing_value_test.index):\n    if test[col].dtype == 'object':\n        test[col].fillna(test[col].value_counts().index[0], inplace=True)\n    else:\n        test[col].fillna(test[col].mean(), inplace=True)\n","3b38431c":"# Encoding categorical train data\n\ntrain=train.apply(LabelEncoder().fit_transform)","53bbe977":"test=test.apply(LabelEncoder().fit_transform)","9345ac0c":"test=sc.fit_transform(test)","3502f82b":"test","796476f4":"print(\"Train: \\n{}\".format(train.dtypes.value_counts()))\n","8b4db7fc":"#split the train data to feautures X and target y\ny = np.ravel(np.array(train[['SalePrice']]))            \nX = train.drop(['SalePrice'], axis=1)\n             \nprint(y.shape)\nprint(X.shape)\n","cd069ec7":" #Splitting the dataset into the Training set and Test set\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0)\n","1ca5eff0":"# Feature Scaling\n\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)","5a2eedc1":"\n#createing Root Mean Squared Logarithmic Error (RMSLE) function\n\ndef rmsle (y_test, y_pred):\n    return round(np.sqrt(sklm.mean_squared_error(y_test, y_pred)),10)","454b438e":"#set the KFold function\nkfolds = KFold(n_splits=10, shuffle=True, random_state=42)\n","3d6a23aa":"\n# define model\nmodel = Ridge()\n# define model evaluation method\ncv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n# define grid\ngrid = dict()\ngrid['alpha'] = np.arange(0, 1, 0.01)\n# define search\nsearch = GridSearchCV(model, grid, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1)\n# perform the search\nresults = search.fit(X_train, y_train)\n# summarize\nprint('MAE: %.3f' % results.best_score_)\nprint('Config: %s' % results.best_params_)","641d184b":"\n## set Ridge Regulariztion (L2)\n## Find Alpha\n\nridge = Ridge()\nparam = {'alpha': [a for a in range(50, 70)]}\n\nridge_reg = GridSearchCV(ridge, param_grid=param, scoring='neg_mean_squared_error'\n                     , cv=10)\n\nridge_reg.fit(X_train, y_train)\nprint(f\"The best value in Alpha: {ridge_reg.best_params_}\")\nprint(f\"The best score: {math.sqrt(-ridge_reg.best_score_)}\")","801fc9d9":"ridge_mod = Ridge(alpha=58)\nridge_mod.fit(X_train, y_train)\nridge_mod_train = ridge_mod.predict(X_train)\nridge_mod_test = ridge_mod.predict(X_test)\n\nprint(f'Root Mean Square Error train =  {rmsle(y_train, ridge_mod_train)}')\nprint(f'Root Mean Square Error test =  {rmsle(y_test, ridge_mod_test)}')   \n\nMSEs = cross_val_score(ridge_mod, X, y, \n                       scoring='neg_mean_squared_error', \n                       cv=kfolds)\n\nfor i,j in enumerate(MSEs):\n    j= math.sqrt(np.mean(-j))\n    print(f'Fold {i}: {round(j,4)}')\n    \nprint(f'Mean RMSE in Ridge: {round(math.sqrt(np.mean(-MSEs)),10)}')","4345207d":"from sklearn.linear_model import Lasso\n\nlasso = Lasso()\nparams = {'alpha': [0.1, 1, 10]}\n\nlasso_reg = GridSearchCV(lasso, param_grid=param, cv=kfolds, scoring='neg_mean_squared_error')\n\nlasso_reg.fit(X_train, y_train)\nprint(f'The best value of lasso: {lasso_reg.best_params_}')","912265da":"\nlasso_mod = Lasso(alpha=50)\nlasso_mod.fit(X_train, y_train)\nlasso_mod_train = lasso_mod.predict(X_train)\nlasso_mod_test = lasso_mod.predict(X_test)\n\nprint(f'Root Mean Square Error train =  {str(rmsle(y_train, lasso_mod_train))}')\nprint(f'Root Mean Square Error test =  {rmsle(y_test, lasso_mod_test)}')\n\nLasso_CV = Lasso(alpha=1)\nMSEs = cross_val_score(lasso_mod, X, y, scoring='neg_mean_squared_error', cv=kfolds)\n\nfor i,j in enumerate(MSEs):\n    j= math.sqrt(np.mean(-j))\n    print(f'Fold {i}: {round(j,4)}')\n\nprint(f'Mean Lasso: {round(math.sqrt(np.mean(-MSEs)),10)}')","5b4007de":"## Set RandomForest model\nfrom sklearn.ensemble import RandomForestRegressor\nrandom_forest = RandomForestRegressor(n_estimators=1400,\n                                      max_depth=13,\n                                      min_samples_split=5,\n                                      min_samples_leaf=5,\n                                      max_features=None,\n                                      random_state=42,\n                                      oob_score=True\n                                     )\n\nrandom_for = random_forest.fit(X_train, y_train)\nrandom_for_mod = random_for.predict(X_test)\n\nprint(f'Root Mean Square Error test = {rmsle(y_test, random_for_mod)}')","388a0f59":"from sklearn.ensemble import VotingRegressor\n\n## Voting in esemble\n\nvote_mod = VotingRegressor([('Ridge', ridge_mod), ('Lasso', lasso_mod), \n                             ('Random_forest', random_forest)])\nvote= vote_mod.fit(X_train, y_train.ravel())\nvote_pred=vote.predict(X_test)\n\nprint(f'Root Mean Square Error test = {rmsle(y_test, vote_pred)}')","cc1d9f72":"## Set XGBRegressor model\nfrom xgboost import XGBRegressor\nxgb_regress = XGBRegressor(learning_rate=0.01,\n                         n_estimators=3600,\n                         max_depth=4, min_child_weight=1,\n                         gamma=0.6, scale_pos_weight=1, \n                         seed=27, reg_alpha=0.00006 )\n\nxg_mod = xgb_regress.fit(X_train, y_train)\nxg_pred = xg_mod.predict(X_test)\n\nprint(f'Root Mean Square Error test =  {rmsle(y_test, xg_pred)}') ","63b1de6c":"from mlxtend.regressor import StackingCVRegressor\nmodel = [ridge_mod, lasso_mod, vote_mod]\n\n## Stacking in ensemble\n\nstack = StackingCVRegressor(regressors=model, \n                           meta_regressor=xgb_regress, \n                            use_features_in_secondary=True)\n\nstack_mod=stack.fit(X_train, y_train.ravel())\nstacking_pred=stack_mod.predict(X_test)\n\nprint(f'Root Mean Square Error test = {rmsle(y_test, stacking_pred)}')","af0d6c2e":"lasso_mod_test_data_pred=ridge_mod.predict(test)","156b63b9":"\nsubmission = pd.DataFrame({\n    \"Id\": data_test[\"Id\"],\n    \"SalePrice\": lasso_mod_test_data_pred\n    })","62d16ca9":"submission","7f95617b":"submission.to_csv(\"submission_test_set.csv\", index=False)\nsubmission.head()","762c8528":"#comapre between pridcted result and data result\nsale_price_compare = pd.DataFrame({\n    \"train sale price desc\": train.describe().iloc[:, -1],\n    \"test sale price desc\": submission.describe().iloc[:, -1]\n    },index=['count','mean','std','min','25%','50%','75%','max' ])\nsale_price_compare","4c636b1f":"**> Finging the alpha parameter fo Ridge **","d6662e12":"the \"info\"  function shows that there is a lot of missing data so as we calculate the percentage of the NaN value we found that **Alley** **PoolQC** **Fence** **MiscFeature** have more than 80% of data is NaN so the feature is not reliable we gonna exclude them","70762718":"Exploring and cleanig the data","e1f67b97":"for other columns, we will change by the most **frequent** if the type of data **object** and with the **mean** if the type of data does **not object (int float...)**","6f1135d2":"importing traing data and test data and drop Id","82bcb278":"**Importing Libraries**"}}