{"cell_type":{"71958eb1":"code","f6a93e4c":"code","dad40d6d":"code","47480bb9":"code","e69ab352":"code","55e0f63e":"code","81a8b887":"code","7a14d1f8":"code","e843ff0b":"code","15768bb4":"code","1bd63b33":"code","e96ddec1":"code","df6416ab":"code","dc79efff":"code","157ba97b":"code","fa166969":"code","5b8c9c69":"code","a4636d89":"code","8ddf581b":"code","1900d2d7":"code","bbfb3eb1":"code","5f9be107":"code","2f3ec63a":"code","f69149d2":"code","0c514da2":"code","708586c2":"code","5287b68e":"code","0e2d8d34":"code","7a826bd2":"code","185b8a19":"code","af80abdc":"code","c063e263":"code","4d70fa15":"code","bb7c7e33":"code","890a9257":"code","778f155a":"code","202c4708":"code","4f06bea4":"code","c0a6b478":"code","8397e9f1":"code","62fdbe35":"code","00adaddd":"code","0807db65":"code","8179928e":"code","20b9dd57":"code","d436d404":"code","a141266c":"code","52e5a5f6":"code","d5a0975f":"markdown","90d341f2":"markdown"},"source":{"71958eb1":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt","f6a93e4c":"from tensorflow.python.keras.models import Sequential\nfrom tensorflow.python.keras.layers import Dense, GRU, Embedding, CuDNNGRU\nfrom tensorflow.python.keras.optimizers import Adam\nfrom tensorflow.python.keras.preprocessing.text import Tokenizer\nfrom tensorflow.python.keras.preprocessing.sequence import pad_sequences","dad40d6d":"df = pd.read_csv(\"..\/input\/hepsiburada.csv\")\ndf.head()","47480bb9":"X = df[\"Review\"].values.tolist()\ny = df[\"Rating\"].values.tolist()","e69ab352":"cutoff = int(len(df) *.8)\nX_train, X_test = X[:cutoff], X[cutoff:]\ny_train, y_test = y[:cutoff], y[cutoff:]\n#cutoff our threshold value. we divide, as before cutoff and after cutoff.","55e0f63e":"X_train[800] # example of positive comment(turkish).","81a8b887":"y_train[800] # positive number is 1, negative number is 0.","7a14d1f8":"num_words = 10000\ntokenizer = Tokenizer(num_words = num_words) #numwords default is \"None\"","e843ff0b":"tokenizer.fit_on_texts(X)\n# X was comments, now we converting to token.","15768bb4":"counter = 0\nfor word in tokenizer.word_index:\n    counter = counter + 1\n    print(counter, \"-> \", word)\n    if(counter == 20):\n        break\n# this code shows most using first 20 token word.","1bd63b33":"X_train_tokens = tokenizer.texts_to_sequences(X_train)\nX_test_tokens = tokenizer.texts_to_sequences(X_test)","e96ddec1":"X_train[800]","df6416ab":"np.array(X_train_tokens[800])","dc79efff":"num_tokens = [len(tokens) for tokens in X_train_tokens + X_test_tokens]\nnum_tokens = np.array(num_tokens)\nnum_tokens","157ba97b":"np.mean(num_tokens)","fa166969":"np.max(num_tokens)","5b8c9c69":"np.argmax(num_tokens)","a4636d89":"print(\"The longest comment: \", X_train[21941], \"\\n\\n\", \"Token of the longest comment :\", X_train_tokens[21941])","8ddf581b":"import math\nmax_tokens = np.mean(num_tokens) + 2*np.std(num_tokens)\nmax_tokens = math.ceil(max_tokens)\nmax_tokens","1900d2d7":"np.sum(num_tokens < max_tokens) \/ len(num_tokens)","bbfb3eb1":"X_train_pad = pad_sequences(X_train_tokens, maxlen = max_tokens)\nX_test_pad = pad_sequences(X_test_tokens, maxlen = max_tokens)","5f9be107":"print(\"X Train Shape : \", X_train_pad.shape, \"\\nX Test Shape : \", X_test_pad.shape)","2f3ec63a":"np.array(X_train_tokens[800])","f69149d2":"X_train_pad[800]","0c514da2":"idx = tokenizer.word_index\ninverse_map = dict(zip(idx.values(), idx.keys()))","708586c2":"def tokens_to_string(tokens):\n    words = (inverse_map[token] for token in tokens if token != 0)\n    text = \" \".join(words)\n    return text","5287b68e":"X_train[800]","0e2d8d34":"tokens_to_string(X_train_tokens[800])","7a826bd2":"model = Sequential()","185b8a19":"embedding_size = 50 #vector size for each word.","af80abdc":"model.add(Embedding(input_dim = num_words,\n                   output_dim = embedding_size,\n                   input_length = max_tokens,\n                   name = \"embedding_layer\"))","c063e263":"model.add(CuDNNGRU(units = 16, return_sequences = True))\nmodel.add(CuDNNGRU(units = 8, return_sequences = True))\nmodel.add(CuDNNGRU(units = 4))\nmodel.add(Dense(1, activation = \"sigmoid\"))","4d70fa15":"optimizer = Adam(lr = 1e-3)\nmodel.compile(loss=\"binary_crossentropy\",\n             optimizer = optimizer,\n             metrics = [\"accuracy\"])","bb7c7e33":"model.summary()","890a9257":"model.fit(X_train_pad, y_train, epochs=5, batch_size = 256)","778f155a":"result = model.evaluate(X_test_pad, y_test)","202c4708":"result # loss value and hit rate","4f06bea4":"y_pred = model.predict(x = X_test_pad[0:1000])\ny_pred = y_pred.T[0]","c0a6b478":"cls_pred = np.array([1.0 if p > 0.5 else 0.0 for p in y_pred])","8397e9f1":"cls_true = np.array(y_test[0:1000])","62fdbe35":"incorrect = np.where(cls_pred != cls_true)\nincorrect = incorrect[0]\nlen(incorrect)","00adaddd":"idx = incorrect[0]\nidx","0807db65":"text = X_test[idx]\ntext","8179928e":"y_pred[idx]","20b9dd57":"text1 = \"bu \u00fcr\u00fcn \u00e7ok iyi herkese tavsiye ederim\"\ntext2 = \"kargo \u00e7ok h\u0131zl\u0131 ayn\u0131 g\u00fcn elime ge\u00e7ti\"\ntext3 = \"tam bir fiyat performans \u00fcr\u00fcn\u00fc\"\ntext4 = \"m\u00fckemmel\"\ntext5 = \"k\u00f6t\u00fc yorumlar g\u00f6z\u00fcm\u00fc korkutmu\u015ftu ancak hi\u00e7bir sorun ya\u015famad\u0131m te\u015fekk\u00fcrler\"\ntext6 = \"bu \u00fcr\u00fcn\u00fc tasarlayan m\u00fchendis izin vaktinde \u00f6zene bezene yapm\u0131\u015f \"\ntext7 = \"b\u00fcy\u00fck bir hayal k\u0131r\u0131kl\u0131\u011f\u0131 ya\u015fad\u0131m bu \u00fcr\u00fcn bu markaya yak\u0131\u015fmam\u0131\u015f\"\ntext8 = \"tasar\u0131m\u0131 harika ancak kargo \u00e7ok ge\u00e7 geldi ve \u00fcr\u00fcn a\u00e7\u0131lm\u0131\u015ft\u0131 tavsiye etmem\"\ntext9 = \"hi\u00e7 resimde g\u00f6sterildi\u011fi gibi de\u011fil\"\ntext10 = \"hi\u00e7 bu kadar k\u00f6t\u00fc bir sat\u0131c\u0131ya denk gelmemi\u015ftim \u00fcr\u00fcn\u00fc geri iade ediyorum\"\ntext11 = \"bekledi\u011fim gibi \u00e7\u0131kmad\u0131\"\ntext12 = \"i\u00e7inden h\u0131yar \u00e7\u0131kt\u0131, biliydim beyle elaca\u011fn\u0131\"\n\ntexts = [text1, text2, text3, text4, text5, text6, text7, text8, text9, text10, text11, text12]","d436d404":"tokens = tokenizer.texts_to_sequences(texts)","a141266c":"tokens_pad = pad_sequences(tokens, maxlen=max_tokens)\ntokens_pad.shape","52e5a5f6":"z_pred = model.predict(tokens_pad)\npred = np.array([\"a positive comment.\" if p > 0.5 else \"a negative comment.\" for p in z_pred])\ncounter = 1\nfor i in pred:\n    print(counter, \". comment is \",i)\n    counter = counter +1","d5a0975f":"# Model","90d341f2":"# Tokenization"}}