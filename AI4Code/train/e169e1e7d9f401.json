{"cell_type":{"49881b1f":"code","35cc3868":"code","0e35d59d":"code","30c9605c":"code","a7ffd454":"code","46a56376":"code","8b458b6f":"code","b4821dfb":"code","fbf8ed8b":"code","65a286c1":"code","697dc59c":"code","405b2ada":"code","1e065b8d":"code","0477d94b":"code","662f4e01":"code","240310ab":"code","d47b7759":"code","111c8f36":"code","24e8ac83":"code","895162bd":"code","82c223ef":"code","e0c3336e":"code","339c71ac":"code","b20aaa63":"markdown","57ecdf56":"markdown","fa3b89fb":"markdown"},"source":{"49881b1f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","35cc3868":"import seaborn as sns\nimport matplotlib.pyplot as plt\nimport PIL\nfrom sklearn.model_selection import train_test_split\nimport cv2\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.layers import Input, Dense, Dropout, Lambda, Flatten, BatchNormalization, Convolution2D , MaxPooling2D\nfrom tensorflow.keras.optimizers import Adam, RMSprop\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom sklearn.metrics import confusion_matrix\nimport itertools\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras.utils import to_categorical\nfrom keras.preprocessing.image import load_img, img_to_array","0e35d59d":"#Among all the content we got from the input folder we choose to keep only the train and test folders . \n#The other files will be used later on\nfichier=os.listdir(\"..\/input\")\nfichier.remove('train.csv')\nfichier.remove('sample_submission.csv')\nfichier  ","30c9605c":"#The folders train  and test countain images refered to by their 'id' followed by the type of the file (jpg).\nimgtype = '\/*.jpg'\ntrain = sorted([x for x in os.listdir(\"..\/input\" + '\/'+ 'train' + '\/train'  ) if x.endswith(imgtype[2:])])\ntest = sorted([x for x in os.listdir(\"..\/input\" + '\/'+ 'test' + '\/test') if x.endswith(imgtype[2:])])\ntrain","a7ffd454":"#We open the CSV file which countains two columns: 'id'and 'has cactus'\ncsv_train = pd.read_csv(\"..\/input\/\" + 'train.csv')\nsns.countplot('has_cactus', data=csv_train)\nplt.title('Classes', fontsize=15)\nplt.show()\ncsv_train.has_cactus=csv_train.has_cactus.astype(str) #This variable is turned to string to match the data format \n                                                      #required by the augmentation function we'll use\n","46a56376":"#The function ImageDataGenerator allows here to multiply the data by the value provided.\n#Batch Size corresponds to the number of samples that will be passed through our network at one time. \ndatagen=ImageDataGenerator(rescale=1.\/255)\nbatch_size=140 #so that the number of iterations will be 100 since the training sample countains 14000 images","8b458b6f":"#--------The Data Augmentation\n#The initial train sample is split between Training sample and validation sample following the rule of 80%\/20%\n#The function flow_from_dataframe will provide a new sample with images randomly changed following the parameters fixed\n\ntrain_generator=datagen.flow_from_dataframe(dataframe=csv_train[:14001],directory=\"..\/input\" + '\/'+ 'train' + '\/train',x_col='id',\n                                            y_col='has_cactus',class_mode='binary',batch_size=batch_size,\n                                            target_size=(150,150))\nvalidation_generator=datagen.flow_from_dataframe(dataframe=csv_train[14000:],directory=\"..\/input\" + '\/'+ 'train' + '\/train',x_col='id',\n                                                y_col='has_cactus',class_mode='binary',batch_size=50,\n                                                target_size=(150,150))\ny_train=csv_train[:14001]\ny_val=csv_train[14000:]","b4821dfb":"nb_train_samples = 14001\nnb_validation_samples = 3500","fbf8ed8b":"#--------Convolutionnal Neural Network model\n#The model we chose consists in three steps each time: convolution, pooling, and dropout.\n#1-Convolution2D function consists in applying a Convolutional kernel, of size 3x3. \n#2-MaxPooling uses a kernel 2x2 to downsize the information countained in the image and keep only the pixel with max value\n#3-Dropout\nmodel = Sequential() #Creation of an empty neural network\nmodel.add(Convolution2D(32, (3, 3), input_shape = (150, 150, 3), activation = 'relu')) #followed by an activation layer\nmodel.add(MaxPooling2D(pool_size = (2, 2)))\nmodel.add(Dropout(rate = 0.1))\n\nmodel.add(Convolution2D(64, (3, 3), activation = 'relu'))\nmodel.add(MaxPooling2D(pool_size = (2, 2)))\nmodel.add(Dropout(rate = 0.1))\n\nmodel.add(Convolution2D(128, (3, 3), activation = 'relu'))\nmodel.add(MaxPooling2D(pool_size = (2, 2)))\nmodel.add(Dropout(rate = 0.1))\n\n#Flatten 3d feature maps to 1D\nmodel.add(Flatten())\nmodel.add(Dense(128, activation = 'relu'))\nmodel.add(Dense(64, activation = 'relu'))\nmodel.add(Dense(1, activation = 'sigmoid'))\n\n#compile the model\nmodel.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n\n#generate a summary of the model\nmodel.summary()","65a286c1":"#Stops the fit generator when the value of the accuaracy stagnates\ncallbacks = [EarlyStopping(monitor='val_loss', patience=4)]","697dc59c":"#--------Learning for the training set\nhistory = model.fit_generator(\n    train_generator, \n    steps_per_epoch=nb_train_samples \/\/ batch_size,\n    epochs=20,\n    callbacks=callbacks,\n    validation_data=validation_generator,\n    validation_steps=nb_validation_samples \/\/ batch_size,\n)","405b2ada":"# plot training history\nplt.plot(history.history['loss'], label='train')\nplt.plot(history.history['val_loss'], label='test')\nplt.plot(history.history['val_acc'], label='acc_test')\nplt.plot(history.history['acc'], label='acc_train')\nplt.legend()\nplt.show()\n#the accuaracy of the model is 98%.","1e065b8d":"def plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, cm[i, j],\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')","0477d94b":"Y_pred = model.predict_classes(validation_generator)\nconfusion_mtx = confusion_matrix(validation_generator.classes, Y_pred) \nplot_confusion_matrix(confusion_mtx, classes = range(2))\n\n#Its seems that the CNN built recognizes well when an image contains a Cactus, \n#but the  big number of real '1's is because they represented a biggest share of the training set.","662f4e01":"datagen1=ImageDataGenerator(rotation_range=30, width_shift_range=0.2, \n                             height_shift_range=0.2, zoom_range=0.2, \n                             horizontal_flip=True, vertical_flip=True, \n                             validation_split=0.1,rescale=1.\/255)\nbatch_size=140","240310ab":"train_generator1=datagen1.flow_from_dataframe(dataframe=csv_train[:14001],directory=\"..\/input\" + '\/'+ 'train' + '\/train',x_col='id',\n                                            y_col='has_cactus',class_mode='binary',batch_size=batch_size,\n                                            target_size=(150,150))\n\n\nvalidation_generator1=datagen.flow_from_dataframe(dataframe=csv_train[14000:],directory=\"..\/input\" + '\/'+ 'train' + '\/train',x_col='id',\n                                                y_col='has_cactus',class_mode='binary',batch_size=50,\n                                                target_size=(150,150))\n\ny_train=csv_train[:14001]\ny_val=csv_train[14000:]","d47b7759":"model1 = Sequential()\nmodel1.add(Convolution2D(32, (3, 3), input_shape = (150, 150, 3), activation = 'relu'))\nmodel1.add(MaxPooling2D(pool_size = (2, 2)))\nmodel1.add(Dropout(rate = 0.1))\nmodel1.add(Convolution2D(64, (3, 3), activation = 'relu'))\nmodel1.add(MaxPooling2D(pool_size = (2, 2)))\nmodel1.add(Dropout(rate = 0.1))\n#flatten 3d feature maps to 1D\nmodel1.add(Flatten())\nmodel1.add(Dense(64, activation = 'relu'))\nmodel1.add(Dense(1, activation = 'sigmoid'))\n\n#compile the model\nmodel1.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n\nmodel1.summary()\n\ncallbacks = [EarlyStopping(monitor='val_loss', patience=4)]\n\nhistory1 = model1.fit_generator(\n    train_generator1, \n    steps_per_epoch=nb_train_samples \/\/ batch_size,\n    epochs=20,\n    callbacks=callbacks,\n    validation_data=validation_generator1,\n    validation_steps=nb_validation_samples \/\/ batch_size,\n)","111c8f36":"Y_pred1 = model1.predict_classes(train_generator)\nconfusion_mtx = confusion_matrix(train_generator.classes, Y_pred1) \nplot_confusion_matrix(confusion_mtx, classes = range(2))","24e8ac83":"#Images of the test folder are loaded into X_tst and resized so they can  match the format for which we did the training\nX_tst = []\nTest_imgs = []\nfor img_id in os.listdir(\"..\/input\" + '\/'+ 'test' + '\/test'):\n    img = cv2.imread(\"..\/input\" + '\/'+ 'test' +'\/' +'test' +'\/' + img_id)\n    img2 = cv2.resize(img, (150,150))\n    X_tst.append(img2)     \n    \n    Test_imgs.append(img_id)\nX_tst = np.asarray(X_tst)\nX_tst = X_tst.astype('float32')\nX_tst \/= 255\n","895162bd":"#We apply the final weights on the test sample... \ntest_predictions = model.predict_classes(X_tst)\ntest_predictions","82c223ef":"#... and save the result in csv file where the first column is the 'id' and the second is the prediction\nsub_df = pd.DataFrame(test_predictions, columns=['has_cactus'])\nsub_df['has_cactus'] = sub_df['has_cactus'].apply(lambda x: 1 if x > 0.75 else 0)\nsub_df['id'] = ''\ncols = sub_df.columns.tolist()\ncols = cols[-1:] + cols[:-1]\nsub_df=sub_df[cols]\nfor i, img in enumerate(Test_imgs):\n    sub_df.set_value(i,'id',img)","e0c3336e":"sub_df.head()","339c71ac":"sub_df.to_csv('submission.csv',index=False)","b20aaa63":"**Model 1**","57ecdf56":"**Mod\u00e8le 2**\n\nThis model has less layers than the first one and provides unsatisfaisant results. \nWe hence keep the first model.","fa3b89fb":"**Test**"}}