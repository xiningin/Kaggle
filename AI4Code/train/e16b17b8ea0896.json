{"cell_type":{"89ddf39f":"code","d4a66d88":"code","b05aba35":"code","3ff3835b":"code","d97daaef":"code","306d6ff9":"code","14848ba3":"code","5e3021aa":"code","3ce127a5":"code","e35a1328":"code","0c02a51c":"code","1a8b929c":"code","c56369fb":"code","c9858223":"code","570b3d2a":"code","c127646c":"code","53d90ffb":"code","0f158889":"code","636ce8b6":"code","f069b001":"code","f31a46d0":"code","f10e8e9f":"code","bd5d1079":"code","dd7cc839":"code","63e96f6b":"code","0b970174":"code","1b0cdbd0":"code","83efc53a":"code","4035c5a4":"code","26c75a22":"code","806a3e64":"code","b7d47a0b":"code","310e4e64":"code","b3f42011":"code","5f191df9":"code","3bd46b06":"code","87f30d31":"code","04e21b3f":"code","f42e1510":"code","796c3b25":"code","5dbec2e7":"code","06aa01de":"code","05a38fab":"code","46a4b030":"code","31459273":"code","7aa44aaf":"code","391691f9":"code","332102ab":"code","b3fc04cd":"code","eac6aaef":"code","8ed12b82":"code","530e00dd":"code","3f5c6dd5":"code","ee0ff6eb":"code","73180b75":"code","953e9d22":"markdown","97688a49":"markdown","a88b019e":"markdown","b48545e9":"markdown","8a20049f":"markdown","d21249ff":"markdown","9aeb074f":"markdown","729a84fe":"markdown","3a045a53":"markdown","a6ac5ecc":"markdown","cfa52106":"markdown","4a4b8d63":"markdown","3f6c33a2":"markdown","79419870":"markdown","766e63e2":"markdown","f0793c4f":"markdown","2b33518a":"markdown","ae589e9c":"markdown","eed8fe38":"markdown","8aa0fbcb":"markdown","97f38b8c":"markdown","05246a40":"markdown","b8266b40":"markdown","c57f7ab5":"markdown","43c6ee09":"markdown","9b695eb3":"markdown","67c17b02":"markdown","f27185c2":"markdown","225ee3a9":"markdown","cd7de20a":"markdown","af0897d7":"markdown","f214e918":"markdown","d4bec569":"markdown","f6a55839":"markdown","faf7118c":"markdown","86216af6":"markdown","4696fdf4":"markdown","4a641650":"markdown","27256f17":"markdown","0dbd6632":"markdown","bd46105b":"markdown"},"source":{"89ddf39f":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport random\nimport statistics \nfrom pandas import get_dummies\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.decomposition import PCA\nfrom sklearn.metrics import roc_auc_score\nfrom xgboost import XGBClassifier\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import normalize\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import classification_report,confusion_matrix,accuracy_score,mean_squared_error\nfrom sklearn.metrics import roc_auc_score,roc_curve\nfrom imblearn.pipeline import Pipeline as imbPipe\nfrom xgboost import  XGBClassifier\nimport warnings\nwarnings.filterwarnings(\"ignore\")","d4a66d88":"df_train = pd.read_csv(\"..\/input\/hr-analytics-job-change-of-data-scientists\/aug_train.csv\")\ndf_test = pd.read_csv(\"..\/input\/hr-analytics-job-change-of-data-scientists\/aug_test.csv\")","b05aba35":"df_train","3ff3835b":"df_train.info()","d97daaef":"df_train.isnull().sum()","306d6ff9":"df_test.isnull().sum()","14848ba3":"display(df_train[['city','city_development_index','relevent_experience','gender','education_level','major_discipline','experience','company_size','company_type','target']].groupby(['gender','education_level','experience','company_size']).agg([\"max\",'mean',\"min\"]).style.background_gradient(cmap=\"viridis\"))","5e3021aa":"#Countplots showing the frequency of each category with respect to education level \nplt.figure(figsize=[15,17])\nplot=[\"relevent_experience\", \"education_level\",\"major_discipline\", \"experience\",\"company_size\",\"company_type\", \"training_hours\",\"target\"]\nn=1\nfor f in plot:\n    plt.subplot(4,2,n)\n    sns.countplot(x=f, hue='education_level', edgecolor=\"black\", alpha=0.7, data=df_train)\n    sns.despine()\n    plt.title(\"Countplot of {}  by education_level\".format(f))\n    n=n+1\nplt.tight_layout()\nplt.show()","3ce127a5":"import plotly.express as px\nfrom plotly.subplots import make_subplots\nimport plotly.graph_objs as go\n\nel = df_train['education_level'].value_counts().reset_index()\nel.columns = [\n    'education_level', \n    'percent'\n]\nel['percent'] \/= len(df_train)\n\nfig = px.pie(\n    el, \n    names='education_level', \n    values='percent', \n    title='Education_level', \n    width=800,\n    height=500 \n)\n\nfig.show()","e35a1328":"sns.pairplot(df_train)","0c02a51c":"# Experience\n# Enrolee total experience in years\nimport plotly.express as px\nfrom plotly.subplots import make_subplots\nimport plotly.graph_objs as go\n\nep = df_train['experience'].value_counts().reset_index()\nep.columns = [\n    'experience', \n    'percent'\n]\nep['percent'] \/= len(df_train)\n\nfig = px.pie(\n    ep, \n    names='experience', \n    values='percent', \n    title='Experience', \n    width=800,\n    height=500 \n)\n\nfig.show()","1a8b929c":"def clean_experience(df):\n    for i in df[\"experience\"]:\n        if(i==\">20\"):\n            df[\"experience\"][df[\"experience\"]==i]=27\n        if(i == \"<1\"):\n            df[\"experience\"][df[\"experience\"]==i]=0\nclean_experience(df_train)\nclean_experience(df_test)\n\ndf_train[\"experience\"] = df_train[\"experience\"].fillna(0)\ndf_train[\"experience\"] = df_train['experience'].astype('int')\ndf_test[\"experience\"] = df_test[\"experience\"].fillna(0)\ndf_test[\"experience\"] = df_test['experience'].astype('int')","c56369fb":"# Taining_hours\nf, axes = plt.subplots(1,1, figsize = (16, 5))\ng1 = sns.distplot(df_train[\"training_hours\"], color=\"blue\",ax = axes)\nplt.title(\"Distributional of training_hours\")","c9858223":"# education_level:training_hours\net = df_train.sort_values(by='training_hours', ascending=True)[:7000]\nfigure = plt.figure(figsize=(10,6))\nsns.barplot(y=et.education_level, x=et.training_hours)\nplt.xticks()\nplt.xlabel('training_hours')\nplt.ylabel('education_level')\nplt.title('education_level:training_hours ')\nplt.show()","570b3d2a":"def clean_NAN(df):\n    df[\"gender\"] = df[\"gender\"].fillna(\"Unknown\")\n    df[\"education_level\"]=df[\"education_level\"].fillna(\"Unknown\")\n    df[\"major_discipline\"].fillna(value=\"Unknown\", inplace=True)\n    df[\"experience\"] = df[\"experience\"].fillna(df[\"experience\"].mean())\n    df[\"company_type\"] = df[\"company_type\"].fillna(\"Unknown\")\nclean_NAN(df_train)\nclean_NAN(df_test)","c127646c":"def clean_company_size_1(df):\n    converted_list_1 = []\n    converted_list_2 = []\n    converted_list_3 = []\n    for i in df[\"company_size\"]:\n        if i == \"10\/49\":\n            i = \"10-49\"\n            converted_list_1.append(i)\n        converted_list_1.append(i)\n        if i == \"<10\":\n            i = '1-9'\n            converted_list_2.append(i)\n        converted_list_2.append(i)\n        if i == \"10000+\":\n            i = '10000-20000'\n            converted_list_3.append(i)\n        converted_list_3.append(i)\n    df[\"company_size\"]=pd.Series(converted_list_1)\n    df[\"company_size\"]=pd.Series(converted_list_2)\n    df[\"company_size\"]=pd.Series(converted_list_3)\n    new = df['company_size'].str.split(\"-\", n = 1, expand = True) \n    df['company_size_min']= new[0]\n    df['company_size_max']= new[1] \n    df[\"company_size_max\"] = df['company_size_max'].astype('int')\n    df[\"company_size_min\"] = df['company_size_min'].astype('int')\ndf_train[\"company_size\"]=df_train[\"company_size\"].fillna(\"0-0\")\ndf_test[\"company_size\"]=df_test[\"company_size\"].fillna(\"0-0\")\nclean_company_size_1(df_train)\nclean_company_size_1(df_test)","53d90ffb":"def clean_last_new_job(df):\n    converted_list_1 = []\n    converted_list_2 = []\n    for i in df[\"last_new_job\"]:\n        if i == \"never\" or i == np.NaN:\n            i = 0\n            converted_list_1.append(i)\n        converted_list_1.append(i)\n        if i == \">4\":\n            i = 6\n            converted_list_2.append(i)\n        converted_list_2.append(i)\n    df[\"last_new_job\"]=pd.Series(converted_list_1)\n    df[\"last_new_job\"]=pd.Series(converted_list_2)\nclean_last_new_job(df_train)\nclean_last_new_job(df_test)","0f158889":"def clean_city(df):\n    converted_list_1 = []\n    for i in range(len(df[\"city\"])):\n        j = df[\"city\"][i].replace(\"city_\",\"\")\n        converted_list_1.append(j)\n    df[\"city\"]=pd.Series(converted_list_1)\nclean_city(df_train)\nclean_city(df_test)","636ce8b6":"def clean_relevent_experience(df):\n    converted_list_1 = []\n    converted_list_2 = []\n    for i in df[\"relevent_experience\"]:\n        if i == \"Has relevent experience\":\n            i = 1\n            converted_list_1.append(i)\n        converted_list_1.append(i)\n        if i == \"No relevent experience\":\n            i = 0\n            converted_list_2.append(i)\n        converted_list_2.append(i)\n    df[\"relevent_experience\"]=pd.Series(converted_list_1)\n    df[\"relevent_experience\"]=pd.Series(converted_list_2)\nclean_relevent_experience(df_train)\nclean_relevent_experience(df_test)","f069b001":"def one_hot_encoding(df):\n    enrolled_dummies = pd.get_dummies(df[\"enrolled_university\"], dummy_na=True)\n    gender_dummies = pd.get_dummies(df[\"gender\"], dummy_na=True)\n    education_dummies = pd.get_dummies(df[\"education_level\"],dummy_na=True)\n    stream_dummies = pd.get_dummies(df[\"major_discipline\"],dummy_na=True)\n    company_dummies = pd.get_dummies(df[\"company_type\"],dummy_na=True)\n    df[\"Type_no_enrollment\"] = enrolled_dummies[\"no_enrollment\"]\n    df[\"Type_Full_time_course\"] = enrolled_dummies[\"Full time course\"]\n    df[\"Type_Part_time_course\"]=enrolled_dummies[\"Part time course\"]\n    df[\"Gender_Male\"] = gender_dummies[\"Male\"]\n    df[\"Gender_Female\"] =gender_dummies[\"Female\"]\n    df[\"Gender_Unknown\"]=gender_dummies[\"Unknown\"]\n    df[\"Gender_Other\"]=gender_dummies[\"Other\"]\n    df[\"Education_Graduate\"] = education_dummies[\"Graduate\"]\n    df[\"Education_Masters\"] = education_dummies[\"Masters\"]\n    df[\"Education_High_School\"] = education_dummies[\"High School\"]\n    df[\"Education_Primary_School\"] = education_dummies[\"Primary School\"]\n    df[\"Education_Phd\"] = education_dummies[\"Phd\"]\n    df[\"Education_Unknown\"] = education_dummies[\"Unknown\"]\n    df[\"Stream_STEM\"] = stream_dummies[\"STEM\"]\n    df[\"Stream_Humanities\"] = stream_dummies[\"Humanities\"]\n    df[\"Stream_Other\"] = stream_dummies[\"Other\"]\n    df[\"Stream_Business_Degree\"] = stream_dummies[\"Business Degree\"]\n    df[\"Stream_Arts\"] = stream_dummies[\"Arts\"]\n    df[\"Stream_No_Major\"] = stream_dummies[\"No Major\"]\n    df[\"Stream_Unknown\"] = stream_dummies[\"Unknown\"]\n    df[\"Company_Pvt_Ltd\"] = company_dummies[\"Pvt Ltd\"]\n    df[\"Company_Funded_Startup\"] = company_dummies[\"Funded Startup\"]\n    df[\"Company_Public_Sector\"]=company_dummies[\"Public Sector\"]\n    df[\"Company_Early_Stage_Startup\"] = company_dummies[\"Early Stage Startup\"]\n    df[\"Company_NGO\"] = company_dummies[\"NGO\"]\n    df[\"Company_Other\"] = company_dummies[\"Other\"]\n    df[\"Company_Unknown\"] = company_dummies[\"Unknown\"]\none_hot_encoding(df_train)\none_hot_encoding(df_test)","f31a46d0":"df_train = df_train.dropna(subset=['enrolled_university',\"last_new_job\"])\ndf_test = df_test.dropna(subset=['enrolled_university',\"last_new_job\"])","f10e8e9f":"def clean_company_size_2(df):\n    converted_list_1 = []\n    converted_list_2 = []\n    for i in df[\"company_size_min\"]:\n        if i == 0:\n            i = int(df[\"company_size_min\"].mean())\n            converted_list_1.append(i)\n        converted_list_1.append(i)\n    for i in df[\"company_size_max\"]:\n        if i == 0:\n            i = int(df[\"company_size_max\"].mean())\n            converted_list_2.append(i)\n        converted_list_2.append(i)\n    df[\"company_size_min\"]=pd.Series(converted_list_1)\n    df[\"company_size_max\"]=pd.Series(converted_list_2)\ndf_train[\"company_size_min\"] = df_train[\"company_size_min\"].fillna(int(df_train[\"company_size_min\"].mean()))\ndf_train[\"company_size_max\"] = df_train[\"company_size_max\"].fillna(int(df_train[\"company_size_max\"].mean()))\n\ndf_test[\"company_size_min\"] = df_test[\"company_size_min\"].fillna(int(df_test[\"company_size_min\"].mean()))\ndf_test[\"company_size_max\"] = df_test[\"company_size_max\"].fillna(int(df_test[\"company_size_max\"].mean()))\n\nclean_company_size_2(df_test)\nclean_company_size_2(df_train)","bd5d1079":"df_test.isnull().sum()","dd7cc839":"# Target\n# 0 \u2013 Not looking for job change,\n# 1 \u2013 Looking for a job change\n# As you can see, here we have imbalanced data, the number of 1 ( Looking for a job change) < 0 (Not looking for job change)\nmnj = df_train['target'].value_counts()  \nplt.figure(figsize=(6,4))\nsns.barplot(mnj.index, mnj.values, alpha=0.8)\nplt.ylabel('Number of Data', fontsize=12)\nplt.xlabel('target', fontsize=9)\nplt.xticks(rotation=90)\nplt.show();","63e96f6b":"df_test.index = np.arange(0,len(df_test))","0b970174":"df_test_copy = df_test.copy()\ndf_test","1b0cdbd0":"df_train = df_train.drop(['enrollee_id','gender','enrolled_university','education_level','major_discipline','company_type','company_size'],axis=1)\ndf_test = df_test.drop(['enrollee_id','gender','enrolled_university','education_level','major_discipline','company_type',\"company_size\"],axis=1)","83efc53a":"X = df_train.drop(\"target\",axis=1)\nY = pd.DataFrame(df_train[\"target\"])","4035c5a4":"smote = SMOTE()\nX, Y = smote.fit_resample(X, Y)","26c75a22":"X","806a3e64":"Y[\"target\"].value_counts()","b7d47a0b":"df_train_final = X.copy()\ndf_train_final['target'] = Y\ndf_test_final = df_test.copy()","310e4e64":"cols_to_be_normalized = [\"city\",\"city_development_index\",\"experience\",\"last_new_job\",\"training_hours\",\"company_size_min\",\"company_size_max\"]\ncols_not_to_be_normalized = [\"relevent_experience\",\"Type_no_enrollment\",\"Type_Full_time_course\",\"Type_Part_time_course\",\"Gender_Male\",\"Gender_Female\",\"Gender_Unknown\",\n                            \"Gender_Other\",\"Education_Graduate\",\"Education_Masters\",\"Education_High_School\",\"Education_Primary_School\",\"Education_Phd\",\n                            \"Education_Unknown\",\"Stream_STEM\",\"Stream_Humanities\",\"Stream_Other\",\"Stream_Business_Degree\",\"Stream_Arts\",\"Stream_No_Major\",\n                            \"Stream_Unknown\",\"Company_Pvt_Ltd\",\"Company_Funded_Startup\",\"Company_Public_Sector\", \"Company_Early_Stage_Startup\", \"Company_NGO\",\n                            \"Company_Other\", \"Company_Unknown\", \"target\"]","b3f42011":"train_normalized = normalize(df_train_final[cols_to_be_normalized])\ntrain_boolean = df_train_final[cols_not_to_be_normalized]\ndf_train_normalized = pd.DataFrame(train_normalized,columns = cols_to_be_normalized)\ndf_train_boolean = pd.DataFrame(train_boolean,columns=cols_not_to_be_normalized)","5f191df9":"cols_to_be_normalized = [\"city\",\"city_development_index\",\"experience\",\"last_new_job\",\"training_hours\",\"company_size_min\",\"company_size_max\"]\ncols_not_to_be_normalized = [\"relevent_experience\",\"Type_no_enrollment\",\"Type_Full_time_course\",\"Type_Part_time_course\",\"Gender_Male\",\"Gender_Female\",\"Gender_Unknown\",\n                            \"Gender_Other\",\"Education_Graduate\",\"Education_Masters\",\"Education_High_School\",\"Education_Primary_School\",\"Education_Phd\",\n                            \"Education_Unknown\",\"Stream_STEM\",\"Stream_Humanities\",\"Stream_Other\",\"Stream_Business_Degree\",\"Stream_Arts\",\"Stream_No_Major\",\n                            \"Stream_Unknown\",\"Company_Pvt_Ltd\",\"Company_Funded_Startup\",\"Company_Public_Sector\", \"Company_Early_Stage_Startup\", \"Company_NGO\",\n                            \"Company_Other\", \"Company_Unknown\"]","3bd46b06":"test_normalized = normalize(df_test_final[cols_to_be_normalized])\ntest_boolean = df_test_final[cols_not_to_be_normalized]\ndf_test_normalized = pd.DataFrame(test_normalized,columns = cols_to_be_normalized)\ndf_test_boolean = pd.DataFrame(test_boolean,columns=cols_not_to_be_normalized)","87f30d31":"df_train_final = df_train_normalized.merge(df_train_boolean,left_index=True, right_index=True)\ndf_test_final = df_test_normalized.merge(df_test_boolean,left_index=True, right_index=True)\ndf_test_final.index = np.arange(0,len(df_test_final))\ndf_test_final","04e21b3f":"df_test_final","f42e1510":"X = df_train_final.drop(\"target\",axis = 1)\nY = df_train_final[\"target\"]","796c3b25":"X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.33, random_state=42,shuffle=True, stratify = Y)","5dbec2e7":"logitsic_model = LogisticRegression()","06aa01de":"logitsic_model.fit(X_train,Y_train)","05a38fab":"Y_pred = logitsic_model.predict(X_test)","46a4b030":"print(classification_report(Y_test,Y_pred))","31459273":"confusion_matrix(Y_test,Y_pred)","7aa44aaf":"print(mean_squared_error(Y_test,Y_pred))","391691f9":"print (\"Accuracy : \", accuracy_score(Y_test, Y_pred)) ","332102ab":"from sklearn.ensemble import GradientBoostingClassifier\n# Now we can try setting different learning rates, so that we can compare the performance of the classifier's \n#performance at different learning rates. Let's see what the performance was for different learning rates:\n\nlr_list = [0.005, 0.0075, 0.01, 0.025, 0.05,0.1,0.25,0.5,1,0.88,0.9,1]\n\nfor learning_rate in lr_list:\n    gb_clf = GradientBoostingClassifier(n_estimators=50, learning_rate=learning_rate, max_features=2, max_depth=2, random_state=0)\n    gb_clf.fit(X_train, Y_train)\n\n    print(\"Learning rate: \", learning_rate)\n    print(\"Accuracy score (training): {0:.3f}\".format(gb_clf.score(X_train, Y_train)))\n    print(\"Accuracy score (validation): {0:.3f}\".format(gb_clf.score(X_test, Y_test)))","b3fc04cd":"import warnings\nwarnings.filterwarnings(\"ignore\")\nXGBoost_pipe = imbPipe([\n    (\"XGBoost\", XGBClassifier(random_state=42,n_jobs=-1,tree_method=\"hist\"))\n])\n\nparams={\n    \"XGBoost__max_depth\": [20,21],\n    \"XGBoost__min_child_weight\":[22,23],\n    \"XGBoost__n_estimators\":[25,27],\n    \"XGBoost__subsample\":[0.4,0.5,0.6],\n    \"XGBoost__colsample_bytree\":[0.4,0.5,0.6],\n    \"XGBoost__gamma\":[1,2,3],\n    \n}\n\nXGBoost_grid = GridSearchCV(XGBoost_pipe, params, n_jobs=-1,cv=3,scoring=\"roc_auc\")\nXGBoost_grid.fit(X_train, Y_train)\nprint(\"Best Parameters for Model:  \",XGBoost_grid.best_params_)\nY_pred=XGBoost_grid.predict(X_train)\nprint(\"\\n\")\nprint(classification_report(Y_train, Y_pred))\n","eac6aaef":"Y_pred=XGBoost_grid.predict(X_test)  \nprint(classification_report(Y_test, Y_pred))","8ed12b82":"accuracy_score(Y_test,Y_pred)","530e00dd":"X_test = df_test_final.copy()","3f5c6dd5":"Y_pred = XGBoost_grid.predict(X_test)  ","ee0ff6eb":"submission = pd.DataFrame(df_test_copy['enrollee_id'])\nsubmission[\"target\"] = Y_pred","73180b75":"filename = 'submission.csv'\nsubmission.to_csv(filename,index=False)","953e9d22":"### Exploring the Distribution of df.experience","97688a49":"# <font color = \"red\">XGBoost<\/font>\nXGBoost is a decision-tree-based ensemble Machine Learning algorithm that uses a gradient boosting framework. In prediction problems involving unstructured data (images, text, etc.) artificial neural networks tend to outperform all other algorithms or frameworks.","a88b019e":"### More NA Values \ud83d\ude2b","b48545e9":"## Accuracy score","8a20049f":"# Normalization\n### Noramalizing the train & test data for better accuracy","d21249ff":"# Thank You \ud83e\udd17\n### I hope you had a good time reading my notebook. Pls do support and comment! \ud83d\ude0e","9aeb074f":"# Analysing Data","729a84fe":"### Train Test Split ","3a045a53":"#### Accuracy Score","a6ac5ecc":"## Importing Data","cfa52106":"### Based on the above models , XGBoost has higher accuracy  \ud83d\udca5 ","4a4b8d63":"#### NAN Values are replaced with unknown","3f6c33a2":"### Evaluation Metrics\n","79419870":"## One Hot encoding\nIt is a process by which categorical variables are converted into a form that could be provided to ML algorithms to do a better job in prediction.","766e63e2":"### Data Balanced Successfully \ud83e\udd18","f0793c4f":"# <font color = \"red\">Gradient Boosting Classifier<\/font>\nGradient boosting is a machine learning technique for regression and classification problems, which produces a prediction model in the form of an ensemble of weak prediction models, typically decision trees.","2b33518a":"## Data Upscaling","ae589e9c":"#### Cleaning company_size to attain the required format for both test and train","eed8fe38":"### Education Level Vs Training Hours","8aa0fbcb":"#### Confusion Matrix","97f38b8c":"# Data Preprocessing ","05246a40":"#### Classification Report","b8266b40":"#### Cleaning company_size to attain the required format and split them into min and max company_size ","c57f7ab5":"### Replacing the NaN values with the average","43c6ee09":"### Imputing our model on given test data","9b695eb3":"### Fitting the data into Logistic Model","67c17b02":"#### Mean Squared Error","f27185c2":"<center><img src=\"https:\/\/media1.giphy.com\/media\/2vq9I9HGKrpjaHNLVb\/giphy.gif\"><\/img><\/center><br>","225ee3a9":"#### Countplots with respect to educational level \n##### Education Level\n##### This dataset contains 5 education level:\n\n<li>Graduate<br><\/li>\n<li>Masters<br><\/li>\n<li>High School<br><\/li>\n<li>PhD<br><\/li>\n<li>Primary School<\/li>\n","cd7de20a":"<center><h1 class=\"list-group-item list-group-item-success\">HR Analytics: Job Change Prediction<\/center>\n<img src = \"https:\/\/www.valamis.com\/documents\/10197\/605345\/hr-analytics.png\">\n\n### Context\nA company which is active in Big Data and Data Science wants to hire data scientists among people who successfully pass some courses which conduct by the company. Many people signup for their training. Company wants to know which of these candidates are really wants to work for the company after training or looking for a new employment because it helps to reduce the cost and time as well as the quality of training or planning the courses and categorization of candidates. Information related to demographics, education, experience are in hands from candidates signup and enrollment.\n\nThis dataset designed to understand the factors that lead a person to leave current job for HR researches too. By model(s) that uses the current credentials,demographics,experience data you will predict the probability of a candidate to look for a new job or will work for the company, as well as interpreting affected factors on employee decision.\n\nThe whole data divided to train and test . Target isn't included in test but the test target values data file is in hands for related tasks. A sample submission correspond to enrollee_id of test set provided too with columns : enrollee _id , target\n\n### Contents:\n<font size = 3.5 color = \"blue\">\n<li>Importing Packages<\/li>\n<li>Importing Data<\/li>\n<li>Analysing Data<\/li>\n<li>Data Overview<\/li>\n<li>Transforming data to required format<\/li>\n<li>Visualization<\/li>\n<li>Data Preprocessing<\/li>\n<li>One Hot encoding<\/li>\n<li>Filling NA Values<\/li>\n<li>Data Upscaling<\/li>\n<li>Training Models<\/li>\n<li>Evaluation Metrics<\/li>\n","af0897d7":"### Distribution of Training Hours","f214e918":"## Classisification report of the XGBoost model","d4bec569":"# <font color = \"red\">Logistic Regression<\/font>\nLogistic Regression is a Machine Learning classification algorithm that is used to predict the probability of a categorical dependent variable. In logistic regression, the dependent variable is a binary variable that contains data coded as 1 (yes, success, etc.) or 0 (no, failure, etc.).'","f6a55839":"# Data Overview","faf7118c":"# Visualization","86216af6":"### Splitting  Independent & dependent varaiables ","4696fdf4":"### Transforming data to required format","4a641650":"## Importing Packages","27256f17":"### Pairplot with numerical values","0dbd6632":"<center><img src=\"https:\/\/media.tenor.com\/images\/aa37ff519d18dc4b51b8a55fb36e27e7\/tenor.gif\"><\/img><\/center><br>\n<center><font size = 4 color = \"red\">Data Cleaning done successfully \u2728<\/font><\/center>","bd46105b":"### Predicting x_test with trained model"}}