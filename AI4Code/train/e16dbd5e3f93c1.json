{"cell_type":{"7dfe5c1f":"code","263cd20c":"code","a228e3d6":"code","4133c16f":"code","b1ef7684":"code","80572b7c":"code","a3d1ea45":"code","b77e0574":"code","5ba742ab":"code","f13b009f":"code","d57fa12a":"markdown","3a9956ab":"markdown","858f8b47":"markdown","884c59a2":"markdown"},"source":{"7dfe5c1f":"import warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport numpy as np\nimport pandas as pd\n\nfrom pathlib import Path\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import GroupKFold\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import KBinsDiscretizer\nfrom sklearn.impute import SimpleImputer \nfrom sklearn.impute import MissingIndicator\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\nfrom catboost import CatBoostRegressor","263cd20c":"data_folder = Path(\"..\/input\/TTiDS20\/\")\nsubmissions_folder = Path.cwd()\n\ntrain_df = pd.read_csv(data_folder \/ \"train.csv\", index_col=0)\ntest_df = pd.read_csv(data_folder \/ \"test_no_target.csv\", index_col=0)\nzipcodes_df = pd.read_csv(data_folder \/ \"zipcodes.csv\", index_col=0)\n\ntrain_df = pd.merge(train_df.reset_index(), zipcodes_df.drop_duplicates(\"zipcode\"), on=\"zipcode\", how=\"left\")\ntest_df = pd.merge(test_df.reset_index(), zipcodes_df.drop_duplicates(\"zipcode\"), on=\"zipcode\", how=\"left\")","a228e3d6":"cat_features = [\"type\", \"gearbox\", \"model\", \"fuel\", \"brand\", \"city\"]\ncont_missing_features = [\"engine_capacity\", \"damage\", \"insurance_price\", \"latitude\", \"longitude\"]\ncat_missing_features = [\"type\", \"gearbox\", \"model\", \"fuel\", \"city\"]","4133c16f":"def mape(y_true, y_pred):\n    y_true, y_pred = np.array(y_true), np.array(y_pred)\n    return np.mean(np.abs((y_true - y_pred) \/ y_true)) * 100\n\ndef zip_dataframes(*dataframes):\n    for idx, dataframe in enumerate(dataframes):\n        dataframe[\"df_order\"] = idx\n    return pd.concat(dataframes)\n\ndef unzip_dataframes(dataframe):\n    dataframes = []\n    for n in dataframe[\"df_order\"].unique().tolist():\n        dataframes.append(dataframe[dataframe[\"df_order\"] == n].drop(columns=\"df_order\"))\n    return dataframes\n    \n\ndef create_submit_df(test_df, preds):\n    submit_df = pd.DataFrame({\n        \"Id\": test_df[\"index\"],\n        \"Predicted\": preds,\n    })\n    return submit_df","b1ef7684":"def preprocessing(train_df, test_df, funcs):\n    train_df = train_df.copy()\n    test_df = test_df.copy()\n    for func in funcs:\n        train_df, test_df = func(train_df, test_df)\n    return train_df, test_df","80572b7c":"def impute_nan_with_zero(train_df, test_df):\n    for cat_feature in cat_features:\n        train_df[cat_feature] = train_df[cat_feature].fillna(\"nan\")\n        test_df[cat_feature] = test_df[cat_feature].fillna(\"nan\")\n    train_df = train_df.fillna(0)\n    test_df = test_df.fillna(0)\n    return train_df, test_df\n\ndef impute_nan(train_df, test_df):\n    for cont_missing_feature in cont_missing_features:\n        imp = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\n        imp.fit(pd.concat([train_df, test_df])[[cont_missing_feature]])\n        train_df[cont_missing_feature] = imp.transform(train_df[[cont_missing_feature]])\n        test_df[cont_missing_feature] = imp.transform(test_df[[cont_missing_feature]])\n\n    for cat_missing_feature in cat_missing_features:\n        imp = SimpleImputer(missing_values=np.nan, strategy='constant', fill_value=\"nan\")\n\n        imp.fit(pd.concat([train_df, test_df])[[cat_missing_feature]])\n        train_df[cat_missing_feature] = imp.transform(train_df[[cat_missing_feature]])\n        test_df[cat_missing_feature] = imp.transform(test_df[[cat_missing_feature]])\n    return train_df, test_df\n\ndef drop_columns(train_df, test_df):\n    drop_columns = [\"index\"]\n    train_df = train_df.drop(columns=drop_columns)\n    test_df = test_df.drop(columns=drop_columns)\n    return train_df, test_df\n\ndef drop_price_outliers(train_df, test_df):\n    upper_bound = np.quantile(train_df.price, 0.95)\n    train_df = train_df[train_df.price <= upper_bound]\n    return train_df, test_df\n\n\ndef drop_insurance_price_outliers(train_df, test_df):\n    upper_bound = np.quantile(train_df.insurance_price, 0.99)\n    train_df = train_df[train_df.insurance_price <= upper_bound]\n    return train_df, test_df\n\ndef fill_insurance_price(train_df, test_df):\n    train_df.loc[train_df.insurance_price.isna(), \"insurance_price\"] = train_df.insurance_price.mean()\n    return train_df, test_df\n    \ndef fix_registration_year(train_df, test_df):\n    train_df.loc[train_df.registration_year < 100, \"is_fixed_reg_year\"] = 1.0\n    train_df.registration_year = train_df.registration_year.apply(lambda y : 2000 + y if y < 21 else y)\n    train_df.registration_year = train_df.registration_year.apply(lambda y : 1900 + y if y < 100 else y)\n    \n    test_df.loc[test_df.registration_year < 100, \"is_fixed_reg_year\"] = 1.0\n    test_df.registration_year = test_df.registration_year.apply(lambda y : 2000 + y if y < 21 else y)\n    test_df.registration_year = test_df.registration_year.apply(lambda y : 1900 + y if y < 100 else y)\n    return train_df, test_df\n\ndef cat_encode(train_df, test_df):\n    for cat_feature in cat_features:\n        le = LabelEncoder()\n        le.fit(pd.concat([train_df, test_df])[cat_feature])\n        train_df[cat_feature] = le.transform(train_df[cat_feature])\n        test_df[cat_feature] = le.transform(test_df[cat_feature])\n        \n    return train_df, test_df\n\ndef indicate_missing(train_df, test_df):\n    for missing_feature in cont_missing_features+cat_missing_features:\n        imp = MissingIndicator(missing_values=np.nan)\n        imp.fit(pd.concat([train_df, test_df])[[missing_feature]])\n        train_df[\"is_missing_\" + missing_feature] = imp.transform(train_df[[missing_feature]])\n        test_df[\"is_missing_\" + missing_feature] = imp.transform(test_df[[missing_feature]])\n    return train_df, test_df","a3d1ea45":"def cross_validate(\n    model,\n    train_df,\n    kfold,\n    metric,\n    preproc_funcs,\n    target=\"price\",\n    test_df=None,\n    log_target=False,\n    *args,\n    **kwargs\n):\n    val_scores = []\n    test_preds = []\n    \n    if isinstance(kfold, GroupKFold):\n        splits = kfold.split(train_df, groups=kwargs[\"groups\"])\n    elif isinstance(kfold, StratifiedKFold):\n        target_values = train_df[[target]]\n        est = KBinsDiscretizer(n_bins=50, encode='ordinal', strategy='quantile')\n        stratify_on = est.fit_transform(target_values).T[0]\n        splits = kfold.split(train_df, stratify_on)\n    else:\n        splits = kfold.split(train_df)\n\n    for idx, (tr_idx, val_idx) in enumerate(splits):\n        tr_df = train_df.iloc[tr_idx]\n        val_df = train_df.iloc[val_idx]\n        \n        if test_df is not None:\n            tr_df, zip_df = preprocessing(tr_df, zip_dataframes(val_df, test_df), preproc_funcs)\n            val_df, ts_df = unzip_dataframes(zip_df)\n        else:\n            tr_df, val_df = preprocessing(tr_df, val_df, preproc_funcs)\n        \n        x_tr = tr_df.drop(columns=target).values\n        y_tr = tr_df[target].values\n        x_val = val_df.drop(columns=target).values\n        y_val = val_df[target].values\n        \n        if log_target:\n            y_tr = np.log(y_tr)\n            y_val = np.log(y_val)\n        \n        model.fit(x_tr, y_tr)\n        preds = model.predict(x_val)\n        \n        preds = np.exp(preds) if log_target else preds\n        y_val = np.exp(y_val) if log_target else y_val\n        \n        fold_score = metric(y_val, preds)\n        val_scores.append(fold_score)\n        \n        print(f\"fold {idx+1} score: {fold_score}\")\n\n        if test_df is not None:\n            x_ts = ts_df.drop(columns=target).values\n            test_fold_preds = model.predict(x_ts)\n            test_fold_preds = np.exp(test_fold_preds) if log_target else test_fold_preds\n            test_preds.append(test_fold_preds)\n            \n    print(f\"mean score: {np.mean(val_scores)}\")\n    print(f\"score variance: {np.var(val_scores)}\")\n\n    if test_df is not None:\n        return val_scores, test_preds\n    \n    return val_scores","b77e0574":"# model = XGBRegressor(\n#     random_state=42,\n#     n_estimators=500,\n#     max_depth=5,\n#     objective=\"reg:gamma\"\n# )\n\n# model = CatBoostRegressor(\n#     random_state=42,\n#     depth=10,\n#     loss_function=\"MAE\",\n#     cat_features=[1, 3, 5, 7, 8, 12],\n#     verbose=False,\n# )","5ba742ab":"model = LGBMRegressor(\n    random_state=42,\n    objective='mape',\n    num_leaves=100,\n    feature_fraction=0.9,\n    max_depth=-1,\n    learning_rate=0.03,\n    num_iterations=1300,\n    subsample=0.5,\n)\n\nkfold = StratifiedKFold(n_splits=4, random_state=42, shuffle=True)\npreproc_funcs = [\n    indicate_missing,\n    impute_nan_with_zero,\n    drop_columns,\n    cat_encode,\n]\n\nval_scores, test_preds = cross_validate(\n    model, \n    train_df,\n    kfold,\n    mape,\n    preproc_funcs,\n    test_df=test_df,\n    log_target=True,\n)","f13b009f":"submit_df = create_submit_df(test_df, np.mean(test_preds, axis=0))\nsubmit_df.to_csv(submissions_folder \/ \"lgbm-logtarget-stratkfold.csv\", index=False)\nsubmit_df","d57fa12a":"## Utils","3a9956ab":"## Cross-validation predict","858f8b47":"## Preprocessing","884c59a2":"## Data loading"}}