{"cell_type":{"2a7b83a2":"code","edb91291":"code","46c5694d":"code","9f78b26d":"code","5cb28b05":"code","e0e626f3":"code","c5148723":"code","abb54b86":"code","b565b4d3":"code","4d46e26c":"code","d912338d":"code","f21ccbb0":"code","45a3653a":"code","bfc04f65":"code","e75a942e":"code","1f5037a9":"code","3cbeac11":"code","fa18f7b7":"code","9d878f23":"code","332cb714":"markdown","255ff03e":"markdown","a964fd2f":"markdown","9725401a":"markdown","ab0f0482":"markdown","3b45a9f3":"markdown","f1442c10":"markdown"},"source":{"2a7b83a2":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline","edb91291":"def initialize(dim):\n    w = np.zeros((dim,1))\n    assert(w.shape == (dim, 1))\n    return w","46c5694d":"def calc_yhat(w,X):\n    y_hat = np.dot(w.T,X)\n    return y_hat","9f78b26d":"def calc_cost(w,y_hat,Y):\n    m = Y.shape[1]\n    cost = 1\/m * np.sum(np.power(y_hat-Y,2))\n    return cost","5cb28b05":"def calculate_gradient(w,X,Y): \n    m = X.shape[1]\n    y_hat = calc_yhat(w,X)\n    grad = 2 \/ m * np.dot(X,(y_hat-Y).T)\n    return grad","e0e626f3":"def gradient_descent(X,Y,learning_rate,epochs,details):\n    costs = []\n    w = initialize(X.shape[0])\n    for i in range(epochs+1):\n        if i % details == 0:\n            y_hat = calc_yhat(w,X)\n            cost = calc_cost(w,y_hat,Y)\n            costs.append(cost)\n            print(\"Cost after {} iterations is {}\".format(i,cost))\n        \n        grad = calculate_gradient(w,X,Y)\n        w = w - learning_rate * grad\n        \n    return w,costs","c5148723":"def run_model(X,Y,learning_rate,epochs, details):\n    '''Input Shape:\n        X : (1,M)\n        Y : (1,M)\n        details : epoch interval after which cost is printed''' \n    ones = np.ones((1,X.shape[1]))                    \n    X = np.append(ones,X,axis = 0)                 # To match with both b0 and b1 in w     (Y = b0 + b1X)      \n\n    w,costs = gradient_descent(X,Y,learning_rate,epochs,details)\n    d = {\n        'Intercept' : w[0,0],\n        'Slope'     : w[1,0],\n        'Costs'     : costs\n        }\n    return d                                       # Return a dictionary of intercept,slope and Cost minimization details","abb54b86":"X = np.random.rand(1,2000)\nY = 5*X + np.random.randn(1,2000)*0.1\nplt.scatter(X,Y)","b565b4d3":"result = run_model(X,Y,0.01,1000,100)","4d46e26c":"result['Slope']","d912338d":"result['Intercept']","f21ccbb0":"result['Costs']","45a3653a":"plt.plot(result['Costs'])","bfc04f65":"df = pd.read_csv('..\/input\/salary-data1\/Salary_Data.csv')\ndf.head(10)","e75a942e":"X = df[['YearsExperience']].values\nY = df[['Salary']].values\nplt.scatter(X,Y)","1f5037a9":"from sklearn.preprocessing import MinMaxScaler\nmmc = MinMaxScaler()\nX[:] = mmc.fit_transform(X[:])\nY[:] = mmc.fit_transform(Y[:])","3cbeac11":"result = run_model(X,Y,0.01,20,2)","fa18f7b7":"plt.plot(result['Costs'])","9d878f23":"for key,value in result.items():\n    print(key, value)","332cb714":"### 2. Calculating Y_hat and cost function","255ff03e":"### 1. Initializing the parameters","a964fd2f":"# Linear Regression From Scratch\n### Steps:\n 1. Initialize the parametes\n 2. Repeat until convergence or given number of iterations <br>\n     i. Calculate Predicted values for dependent variable as per current parameters (y_hat)<br>\n     ii. Calculate current loss (Cost function)<br>\n     iii. Calculate gradients (Derivative of cost function wrt X)<br>\n     iv. Update Parameters (Gradient Descent)<br>","9725401a":"### 4. Combining all functions to create a model","ab0f0482":"### 3.Optimization using gradient descent","3b45a9f3":"## Training on a dataset","f1442c10":"### Training the model on simple ndarrays"}}