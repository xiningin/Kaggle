{"cell_type":{"93faaaf1":"code","418c35de":"code","466a9570":"code","ebd11178":"code","c8d72a76":"code","9083398c":"code","49253f0d":"code","bf3eaf68":"code","0dc42857":"code","257c12d5":"code","b34a6355":"code","a1718259":"code","3d48ef59":"code","1b5eb462":"code","96b5d67c":"code","06aece6e":"code","ab0b9c23":"code","501d1c5f":"code","763b0369":"code","285849ae":"code","b1e02bc8":"code","592cec10":"code","6891c229":"code","2510e604":"code","9a3e66a3":"code","ef9ccfe0":"code","10a5ad4a":"code","567ea41d":"code","659786d2":"code","6b5bdabd":"code","2380c19a":"code","ef1c2b9b":"code","3a3c6aeb":"code","35e28efb":"code","30d877c3":"code","2ff08db0":"code","45d7113a":"code","3eb842ed":"code","102173ea":"code","927dcd4a":"code","a79b4242":"code","56f334b9":"code","64b67229":"code","5da48db0":"code","23f29424":"markdown","03c0efed":"markdown","10187900":"markdown","b5736a50":"markdown","962af4e5":"markdown","39e73987":"markdown","65c68db3":"markdown","3ae1a895":"markdown","60b8649a":"markdown","a495f8fc":"markdown","c1b59481":"markdown","c58a2b79":"markdown","ca1fc0e7":"markdown","13ca571b":"markdown","b96af33b":"markdown","55ff24d1":"markdown","b7fc0b38":"markdown"},"source":{"93faaaf1":"!pip install plotly\n\n!pip install seaborn\n!pip install nltk\n!pip install gensim\n!pip install yellowbrick\n","418c35de":"#importing libraries\nimport pandas as pd \nimport numpy as np\nimport nltk \nimport plotly.express as px\nimport gensim\nimport gc\nimport string\nimport re\nimport yellowbrick\n#import plotly.plotly as py\nimport plotly.graph_objs as go\n#from plotly.offline import iplot, init_notebook_mode\nfrom plotly.subplots import make_subplots\n\n#cufflinks.go_offline(connected=True)\n#init_notebook_mode(connected=True)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\n\n\n\npd.set_option('display.max_colwidth', None)\npd.options.display.max_columns = None","466a9570":"df_ny = pd.read_csv('..\/input\/glassdoor-data-science-job-data\/Data_Job_NY.csv')\ndf_sf = pd.read_csv('..\/input\/glassdoor-data-science-job-data\/Data_Job_SF.csv')\ndf_tx = pd.read_csv('..\/input\/glassdoor-data-science-job-data\/Data_Job_TX.csv')\ndf_wa = pd.read_csv('..\/input\/glassdoor-data-science-job-data\/Data_Job_WA.csv')\n#Concatenating the data files\n\ndata_df = pd.concat([df_ny , df_sf , df_tx,df_wa] , axis = 0 , ignore_index = True)\n","ebd11178":"del df_ny , df_sf , df_tx ,df_wa\ngc.collect()","c8d72a76":"#Beginning the Cleaning and analysis of the data\n#data_df.head(1)","9083398c":"#data_df.tail(1)","49253f0d":"data_df.shape","bf3eaf68":"data_df.info()","0dc42857":"#First let's convert min_salary and max_salary columns to int\ndata_df['Min_Salary'] = data_df['Min_Salary'].apply(lambda x : int(x))\ndata_df['Max_Salary'] =data_df['Max_Salary'].apply(lambda x : int(x))","257c12d5":"#Extracting date and day from Date_Posted : data is the format y-m-d\nimport calendar\ndata_df['Month'] = data_df['Date_Posted'].apply(lambda x : calendar.month_abbr[int(str(x).split('-')[1])]) \n#data_df['Month'] = data_df['Month'].apply(lambda x : calendar.month_abbr[int(x)])\n","b34a6355":"def Convert_to_Day(x):\n    sl = x.split('-')\n    \n    return calendar.day_abbr[int(calendar.weekday(int(sl[0]) , int(sl[1]) , int(sl[2])))]\n\ndata_df['Day'] = data_df['Date_Posted'].apply(lambda x : Convert_to_Day(x))","a1718259":"#While collecting the data if no salary is found I replaced the value by -1 so lets store that data in different data frame\nindex_missing = data_df[(data_df['Min_Salary'] == -1)].index\ntest_df = data_df.iloc[index_missing, :].reset_index(drop = True)\ndata_df.drop(index_missing , axis = 0 , inplace = True)\ndata_df = data_df.reset_index(drop = True)\n#We will use this data as our test set.","3d48ef59":"#Now that we have train and test set there are duplicates in the data becasue our scraper was not perfect and could havea assimilated multiple entries\ncols = [col for col in data_df.columns if col not in ['Day' , 'Month']]\n#For training data \ntrain_series = data_df.duplicated(cols , keep = 'first')\ndata_df =data_df[~train_series].reset_index(drop = True)\ntest_series = test_df.duplicated(cols , keep = 'first')\ntest_df = test_df[~test_series].reset_index(drop = True)","1b5eb462":"#Unique States\n\nprint(data_df['State'].unique())","96b5d67c":"#Let's explore the top 5 cites in which most job lisitngs are there\nfor state in data_df['State'].unique():\n    print(f\"State of {state}\")\n    print(data_df[data_df['State'] == state]['City'].value_counts()[:5])\n\n","06aece6e":"#Pie Chart of CA and NY\n\nmax_state = ['CA' , 'TX']\nfig = make_subplots(rows = 1 , cols =2 , specs=[[{'type':'domain'}, {'type':'domain'}]])\nfor i,state in enumerate(max_state,1):\n    cities = data_df[data_df['State'] == state]['City'].value_counts()[:5].index.to_list()\n    counts = data_df[data_df[\"State\"] == state]['City'].value_counts()[:5].to_list()\n    fig.add_trace(go.Pie(labels = cities ,values = counts  ,name = state),1,i)\nfig.update_traces(hole=.4, hoverinfo=\"label+percent+name\")\nfig.update_layout(\n    title_text=\"States with most number of jobs\",\n    # Add annotations in the center of the donut pies.\n    annotations=[dict(text= 'CA', x=0.20, y=0.5, font_size=25, showarrow=False),\n                 dict(text='TX', x=0.82, y=0.5, font_size=25, showarrow=False)])\nfig.show()","ab0b9c23":"#Dropping the states where value of number of jobs will be one as they'll be outliers\nindex = data_df[(data_df['State'] =='NC') | (data_df['State'] =='TN') | (data_df['State'] =='KY')].index\ndata_df.drop(index , inplace = True)","501d1c5f":"#Let's the avg minimal salaries for states \nimport numpy as np\nstates = data_df['State'].unique().tolist()\nfig = go.Figure()\nmin_sal =  data_df.groupby('State')['Min_Salary']\nmax_sal =  data_df.groupby('State')['Max_Salary']\nfig.add_trace(go.Bar(x = states,\n                    y = min_sal.mean(),\n                    name = 'Min Salary' , marker_color = 'Magenta'))\n\nfig.add_trace(go.Bar(x = states,\n                    y = max_sal.mean(),\n                    name = 'Max Salary' , marker_color = 'SkyBlue'))\nfig.update_layout(template = 'ggplot2', barmode = 'group')\nfig.show()\n","763b0369":"#Let's see avg minimal salaries according to top  5 cities \nstates = [\"CA\",'TX','DC']\nfig = make_subplots(rows = 3 , cols = 1,specs = [[{\"type\": \"xy\"}],[{\"type\": \"xy\"}],[{\"type\": \"xy\"}]])\ncolors = ['#2e9dd4' ,'#e76969' ,'#4fd882' ,'#f22dea' , '#e7468f']\nfor i,state in enumerate(states,1):\n\n    cities = data_df[data_df['State'] == state]['City'].value_counts()[:5].index.to_list()\n    avg_min_sals = []\n    for city in cities:\n        \n\n        avg_min_sals.append(int(data_df[data_df['City'] == city]['Min_Salary'].mean()))\n    fig.add_trace(go.Bar(x = cities , y = avg_min_sals  ,marker_color = colors ,name = state),i,1)\nfig.update_layout(template = 'ggplot2' , title = \"Average Minimal Salaries per city in states with most number of Jobs\")\nfig.show()\n\n\n","285849ae":"#Job Types in States with Max number of Jobs\nfor state in states:\n    print(f\"Type of Jobs in state of {state}\")\n    print(data_df[data_df['State'] == state]['Job_Type'].value_counts())","b1e02bc8":"#Let's see the day on which most number of jobs are posted\nday_fig = go.Figure([go.Bar(x = data_df['Day'].value_counts().index.to_list() ,\n                    y = data_df['Day'].value_counts().to_list() , marker_color = 'skyblue')])\nday_fig.update_layout(template = 'ggplot2' , title = 'Days with max number of jobs')","592cec10":"#Now's  let's explore the industry column\n#This column has Nan Values\n\nind = data_df[~data_df['Industry'].isnull()]\nprint(f\"Number of Unique Industries : {ind.Industry.nunique()}\")","6891c229":"ind.Industry.value_counts()","2510e604":"#top 8 industries with max number of jobs\n\nfig = go.Figure()\nfig.add_traces(go.Pie(values = ind.Industry.value_counts()[:8].to_list(),\n                    labels= ind.Industry.value_counts()[:8].index.to_list(),\n                    name = 'Industry',textposition = 'inside' , textinfo = 'percent+label'))\nfig.update_layout(template = 'plotly_white',title = 'Industries with most number of Data Science Related jobs' )\nfig.show()","9a3e66a3":"#Let's see which industries dominate the states \nfor state in ind.State.unique():\n    print(f\"State of {state}\")\n    print(ind[ind['State'] == state]['Industry'].value_counts()[:8])\n\n\n","ef9ccfe0":"#Lets take a look at minimal average salary for the top 8 industries\nfig = go.Figure()\nfig.add_trace(go.Bar(x = ind.groupby(\"Industry\")['Min_Salary'].mean().to_list(),\ny = ind.groupby(\"Industry\")['Min_Salary'].mean().index.to_list(), marker_color = 'goldenrod',\norientation = 'h' , name = \"Min Avg Salary\"\n))\nfig.add_trace(go.Bar(x = ind.groupby(\"Industry\")['Max_Salary'].mean().to_list(),\ny = ind.groupby(\"Industry\")['Max_Salary'].mean().index.to_list(), marker_color = 'deepskyblue'\n,orientation = 'h' ,name = \"Max Avg Salary\"))\nfig.update_layout( template = 'plotly_dark',\n    title = 'Minimal And Maximal Average Annual Salaries according to industries' ,barmode = 'group')\nfig.show()","10a5ad4a":"#Now let's explore companies \n\nprint(f\"Number of Unique Company Names : {data_df['Company'].nunique()}\")\n","567ea41d":"# Companies which have most number of job postings\n\nfig = go.Figure()\nfig.add_trace(go.Bar(y = data_df['Company'].value_counts()[:20].to_list(),\nx= data_df['Company'].value_counts()[:20].index.to_list(),\nmarker_color = 'deepskyblue' , name = \"Company\"))\nfig.update_layout(title= 'Companies with Max Number of Job Postings related to data science',\n                template = 'plotly_dark')\nfig.show()","659786d2":"#Let's take a look at Avg Minimal and Maximal salaries for companies \ndef Plot_Company_salaries(companies,title):\n    fig = go.Figure()\n    min_sal = []\n    max_sal = []\n    for company in companies:\n        min_sal.append(data_df[data_df['Company'] == company]['Min_Salary'].mean())\n        max_sal.append(data_df[data_df['Company'] == company]['Max_Salary'].mean())\n\n\n\n    fig.add_trace(go.Bar(x = min_sal ,y = companies , marker_color = 'deepskyblue' \n    , name  = 'Minimal Salary' , orientation = 'h'))\n    fig.add_trace(go.Bar( x= max_sal,y = companies , marker_color = 'red' , \n    name = 'Maximal Salary', orientation = 'h'))\n\n    fig.update_layout(title = title,\n    barmode = 'group' , template = 'plotly_dark')\n    fig.show()\n    \n    ","6b5bdabd":"#Top 5 companies in CA\nstates = ['CA' ,'TX' ,'DC' ,'MD' ,'VA']\ncompanies = []\ntitles = []\nfor state in states:\n    companies.append(data_df[data_df['State'] == state]['Company'].value_counts()[:5].index.to_list())\n    titles.append(f'{state} : Minimal And Maximal Annual Average Salaries for top 5 companies')\n\nfor i in range(len(states)):\n    Plot_Company_salaries(companies[i] , titles[i])\n","2380c19a":"#Distribution of ratings of companies\n\nratings =data_df[~data_df['Rating'].isnull()]['Rating']\nsns.distplot(ratings,kde = True , rug = True)\nplt.axvline(np.median(ratings),color='r', linestyle='--')\nplt.grid(True)\nplt.title(\"Distribution of Ratings\")\nplt.show()","ef1c2b9b":"#Minimal Salaries distribution\nsns.distplot(data_df['Min_Salary'] , kde = True , rug = True)\nplt.axvline(np.median(data_df['Min_Salary']),color='r', linestyle='--')\nplt.axvline(np.mean(data_df['Min_Salary']),color='g', linestyle='--')\nplt.grid(True)\nplt.title(\"Distribution of minimal Salaries\")\nplt.show()\n\n","3a3c6aeb":"#Box plot for minimal salaries\nfig = px.box(data_df , y = 'Min_Salary' ,points = 'all')\nfig.show()","35e28efb":"#Maximal Salaries distribution\nsns.distplot(data_df['Max_Salary'] , kde = True , rug = True)\nplt.axvline(np.median(data_df['Max_Salary']),color='r', linestyle='--')\nplt.axvline(np.mean(data_df['Max_Salary']),color='g', linestyle='--')\nplt.grid(True)\n#plt.figure(figsize=(100,100))\nplt.title(\"Distribution of Maximum Salaries\")\nplt.show()","30d877c3":"#Box plot for maximal salaries\nfig = px.box(data_df , y = 'Max_Salary' ,points = 'all')\nfig.show()","2ff08db0":"#unique Job titles\ndata_df['Job_title'].nunique()","45d7113a":"#Top 8 job titles with max jobs\nfig = go.Figure()\nfig.add_traces(go.Pie(values = data_df.Job_title.value_counts()[:8].to_list(),\n                    labels= data_df.Job_title.value_counts()[:8].index.to_list(),\n                    name = 'Job Title',textposition = 'inside' , textinfo = 'percent+label'))\nfig.update_layout(template = 'plotly_white',title = 'Job Titles with most number of  jobs',\n                showlegend = False )\nfig.show()\n","3eb842ed":"titles = ['Data Scientist' ,'Data Analyst' ,'Data Engineer']\nmin_sal = []\nmax_sal = []\nfor title in titles:\n    min_sal.append(data_df[data_df['Job_title'] == title]['Min_Salary'].mean())\n    max_sal.append(data_df[data_df['Job_title'] == title]['Max_Salary'].mean())\n\nfig = go.Figure()\nfig.add_trace(go.Bar(x = min_sal ,y = titles , marker_color = 'deepskyblue',\norientation = 'h' , name = 'Min Salary'))\nfig.add_trace(go.Bar(x = max_sal ,y = titles , marker_color = 'magenta',\norientation = 'h' , name = 'Max Salary'))\nfig.update_layout(title = 'Annual Avergae Salaries for Job titles having most jobs',\nbarmode = 'group' ,template = 'plotly_white')\nfig.show()","102173ea":"#Let's See how the description actually looks\nx = data_df.Job_Desc[0].replace('\\n\\n' , '\\n')\nx = x.split('\\n')\n\nprint(*x , sep = '\\n')","927dcd4a":"#Let's clean \\n\ndata_df['Job_Desc'] = data_df['Job_Desc'].replace('\\n\\n' , \" \" , regex = True)\ndata_df['Job_Desc'] = data_df['Job_Desc'].replace('\\n' , \" \" , regex = True)\n\ntest_df['Job_Desc'] = test_df['Job_Desc'].replace('\\n\\n' , \" \" , regex = True)\ntest_df['Job_Desc'] = test_df['Job_Desc'].replace('\\n' , \" \" , regex = True)\n#Let's remove punctuation and Stopwords\n\nfrom gensim.parsing.preprocessing import remove_stopwords\ndef Remove_puncutations_stopwords(s):\n\n    s = ''.join([i for i in s if i not in string.punctuation])\n    s = remove_stopwords(s)\n    return s\n\ndata_df['Job_Desc'] = data_df['Job_Desc'].apply(lambda x : Remove_puncutations_stopwords(x))\n\ntest_df['Job_Desc'] = test_df['Job_Desc'].apply(lambda x : Remove_puncutations_stopwords(x))","a79b4242":"#Let's try to visualize counts of the tokens\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom yellowbrick.text import FreqDistVisualizer\n\nvec = CountVectorizer(min_df= 2 , stop_words = 'english' , ngram_range = (2,2))\ndocs = vec.fit_transform(data_df.Job_Desc)\nfeatures = vec.get_feature_names()\n\nvisualizer = FreqDistVisualizer(features=features, orient='h' , size = (800,800))\nvisualizer.fit(docs)\nvisualizer.show()","56f334b9":"#Let's test it out for job_titles\n\nvec_title = CountVectorizer(min_df= 2 , stop_words = 'english' , ngram_range = (2,2))\ndocs_titles = vec.fit_transform(data_df.Job_title)\nfeatures_title = vec.get_feature_names()\n\nvisualizer = FreqDistVisualizer(features=features_title, orient='h' , size = (800,800))\nvisualizer.fit(docs_titles)\nvisualizer.show()","64b67229":"#Now let's take average of minimal and maximal salary find its median\n\ndata_df['avg_sal'] = (data_df['Min_Salary'] + data_df['Max_Salary'])\/\/2\n","5da48db0":"#Median avg annual salary\nprint(f\"Median average annual salary is {data_df['avg_sal'].median()}\")\nmedian_sal = data_df['avg_sal'].median()\ndata_df['is_higher'] = [1 if i > median_sal else 0 for i in data_df.avg_sal]\ndata_df.to_csv(\"train_data.csv\" , index = False)\ntest_df.to_csv('test_data.csv' , index = False)","23f29424":"* The city with the highest avg minimal salary is South San Fransico.","03c0efed":"* Apparently Friday is the day in which max number of jobs were posted.","10187900":"## ***Information About the columns present in the Data***\n\n1. There are 12 columns in the data they are as follows:\n    * ***Job_title*** : The title of job which you are applying to\n    * ***Company*** : Company name\n    * ***State\/City*** : State\/City in which the companies job posting is listed.\n    * ***Min_Salary*** : Minimum yearly salary in USD.\n    * ***Max_Salary*** : Maximum yearly salary in USD.\n    * ***Job_Desc*** : The job description which included skills,requirements,etc\n    * ***Industry*** : The industry in which the company works.\n    * ***Date_posted*** : The date  on which the job was posted on glassdoor\n    * ***Valid_until*** : The last date of applying to the job.\n    * ***Job_Type*** : Type of job full-time , part-time,etc.\n2. All the data collected is about job lisitng for Data Scientist and related roles. The job listings are scraped from ***Glassdoor.co.in***","b5736a50":"## State Column\n\n* United States of America has 50 states but we have collected data mainly by using city search like San Fransico, New York ,etc\n* Here is a map of states in USA\n\n![map](http:\/\/ontheworldmap.com\/usa\/us-state-abbreviations-map-max.jpg)\n","962af4e5":"* According to our data and the above plot **Genentech**,**Booz Allen Hamilton Inc.**,**Amazon** are the companies with the most number of openings.","39e73987":"## Extracting Features out of Job Description ","65c68db3":"## About Glassdoor\n\n![glass](https:\/\/upload.wikimedia.org\/wikipedia\/commons\/e\/e1\/Glassdoor_logo.svg)\n\nGlassdoor is a website where current and former employees anonymously review companies. Glassdoor also allows users to anonymously submit and view salaries as well as search and apply for jobs on its platform.Glassdoor launched its site in 2008 , as a site that \u201ccollects company reviews and real salaries from employees of large companies and displays them anonymously for all members to see,\u201d according to TechCrunch. The company then averaged the reported salaries, posting these averages alongside the reviews employees made of the management and culture of the companies they worked for\u2014including some of the larger tech companies like Google and Yahoo. The site also allows the posting of office photographs and other company-relevant media.","3ae1a895":"* Agriculture and Forestry is an outlier if because in whole of the data there is only one example of it.\n* IT industry pays the best avg minimal and maximal yearly salary.","60b8649a":"# Glassdoor Jobs Data-Analysis \n\nI came up with this personal personal project to test my skills to the fullest and learn new things. In this project I scraped job postings related to the position of 'Data Scientist' from glassdoor.com,analysed the gathered data and framed a machine learning problem out of it. In the below write up I'll mention the details on what I learned. \nI selected states of California,Washington,New York as major areas to find the roles.\n\nYou can find the code for scraping and Modelling in my git link.\nGithub Link : https:\/\/github.com\/Atharva-Phatak\/Glassdoor-Jobs_Data-Analysis","a495f8fc":"## What's the Inference about salaries ?\n* Minimal Average salary for NY is greater CA & DC, that can be because of the less data points for NY state indicating it is an outlier.\n* Both DC and California offer almost the same average salaries both minimal and maximal.","c1b59481":"## Some Observation on Job Description\n\nHere's what I observe \n* There are '\\n' which need to removed\n* The descriprion is mostly structred as follows though the order of it can be different\n\n    * Info About Company\n    * Qualifications\n    * Duties\n    * Requirements\n","c58a2b79":"## Exploring Industries\n\n* Data science,Machine Learning,Deep Learning is being used in varied fields from natural science all the way to finance and banking sectors.\n* Naturally, our collected data will contain some of the industries but may not contain all of them.","ca1fc0e7":"* State of DC is dominated by Business Service and the state of Maryland by Biotech industry atleast for this collected data.","13ca571b":"* As expected industries which come under IT sector have max number of jobs followed by industries which come under Business services.\n* Though one intersting for me was Biotech requires more amount data science related individual than finance atleast for this data set.","b96af33b":"##  Exploratory Data Analysis\n\n* It's time to explore features of the data and get our hands dirty.\n\n","55ff24d1":"* No Doubt Silicon valley Hub, San Fransico is dominating with most number of job openings","b7fc0b38":"* As you can see count vecotrizer mentions the most repeating bi-grams in the Job_Desc. We can create features out it."}}