{"cell_type":{"2da0f10d":"code","bd2dd893":"code","da712494":"code","eb98fe2b":"code","c8f4754d":"code","c6801426":"code","ecfd5364":"code","53def1bb":"code","4f49973b":"code","baafb034":"code","782a2265":"code","5162167e":"code","2d655d3e":"code","134200e6":"code","deec3f0b":"code","ff06725e":"code","fd15f0fe":"code","32144024":"code","0257b5a0":"code","33928e1a":"code","a05892c2":"code","33cf9511":"code","7c55efaf":"code","872c51d6":"code","2037657e":"code","b6335920":"code","be789510":"code","d0fae4fc":"code","e61b39ff":"markdown","adbdcafe":"markdown","09d5e006":"markdown","7b50efe7":"markdown","99b6d6a2":"markdown","2cb34d2c":"markdown","9cabdc32":"markdown","9f33e470":"markdown","96019ee7":"markdown","1427da6f":"markdown","05350b5c":"markdown","07c9e1da":"markdown","5265337a":"markdown","3cb35bbc":"markdown","304ecbde":"markdown","0ba1e32e":"markdown","345e2b0d":"markdown","58c97644":"markdown"},"source":{"2da0f10d":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","bd2dd893":"import scipy.optimize as opt\nimport matplotlib.pyplot as plt\nfrom datetime import datetime, timedelta\nfrom tqdm import tqdm_notebook \nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import mean_squared_log_error\nimport warnings; warnings.filterwarnings('ignore')","da712494":"train = pd.read_csv('\/kaggle\/input\/covid19-global-forecasting-week-2\/train.csv')\ntrain['Date'] = pd.to_datetime(train['Date'])\n\n# So we can keep track of when the training set ends, eval set starts, etc.\n# Could be a better way to handle this, but this is spaghetti code from my iterations to handle offsets when predicting dates\nfirst_test_date = np.datetime64('2020-03-19')\nlast_train_date = np.datetime64('2020-03-31')\neval_set = train[train['Date'] > first_test_date]\n\n# overlap_days keeps track of how many training dates are also in the test dates, for offset purposes\noverlap_days = last_train_date - first_test_date\noverlap_days = int(overlap_days.astype('timedelta64[D]') \/ np.timedelta64(1, 'D'))\n\ntrain = train[train['Date'] <= last_train_date]\n\ntrain","eb98fe2b":"test = pd.read_csv('\/kaggle\/input\/covid19-global-forecasting-week-2\/test.csv')\ntest['Date'] = pd.to_datetime(test['Date'])\ntest","c8f4754d":"sub = pd.read_csv('\/kaggle\/input\/covid19-global-forecasting-week-2\/submission.csv')\nsub = sub.set_index('ForecastId', drop=True)\nsub","c6801426":"def model(parameters, time):\n    y_pred = parameters[0] * (1 - np.exp(-parameters[1] * (time - parameters[3])))**parameters[2] + np.maximum(time*parameters[4], 0)\n    return np.nan_to_num(y_pred).clip(0, np.inf)\n\ndef residual(parameters, time, data):\n    y_pred = model(parameters, time)\n    return mean_squared_error(data, y_pred)","ecfd5364":"def fitModel(time, data, guess):\n    params = opt.minimize(residual, guess, args=(time, data), method='Nelder-Mead', tol=1e-7)\n    return params.x","53def1bb":"def trainModels(data):\n    model_params = {}\n\n    for country in tqdm_notebook(data['Country_Region'].unique()):\n        country_data = data[data['Country_Region'] == country]\n        for province in country_data['Province_State'].unique():  \n            province_data = country_data[country_data['Province_State'] == province]\n            if pd.isnull(province):\n                province = None\n                province_data = country_data[country_data['Province_State'].isnull()]\n            for measure in ('ConfirmedCases', 'Fatalities'):\n                filtered_data = province_data[measure]\n                time_samples = len(filtered_data)\n                try: \n                    start_date = filtered_data.nonzero()[0][0]\n                    # guess offset is log(day of first reported case), assuming exponential spread prior to proper test documented the case\n                    guess_offset = -np.log(filtered_data[start_date])\n                except:\n                    start_date = 0\n                    guess_offset = 0\n                guess_params = [filtered_data.max()*2, 0.1, 5, guess_offset, 0]\n                fit_params = fitModel(range(time_samples-start_date), filtered_data.iloc[start_date:], guess = guess_params)\n                identifier = (country, province, measure)\n                model_params[identifier] = {'params': fit_params, 'num_samples': time_samples, 'start_date': start_date, 'max_value': filtered_data.max()}\n    \n    return model_params","4f49973b":"model_params = trainModels(train)","baafb034":"def forecast(data, model_params, sub):\n    model_predictions = {}\n    for country in tqdm_notebook(data['Country_Region'].unique()):\n        country_data = data[data['Country_Region'] == country]\n        for province in country_data['Province_State'].unique():  \n            province_data = country_data[country_data['Province_State'] == province]\n            if pd.isnull(province):\n                province = None\n                province_data = country_data[country_data['Province_State'].isnull()]\n            for measure in ('ConfirmedCases', 'Fatalities'):\n                filtered_data = province_data['ForecastId']\n                \n                identifier = (country, province, measure)\n                params = model_params[identifier]['params']\n                num_samples = model_params[identifier]['num_samples']\n                start_date = model_params[identifier]['start_date']\n                \n                predictions = model(params, range(num_samples-start_date-overlap_days, num_samples-start_date-overlap_days + len(filtered_data)))\n                \n                model_predictions[identifier] = predictions\n                sub.loc[filtered_data, measure] = predictions\n    return model_predictions","782a2265":"model_predictions = forecast(test, model_params, sub)","5162167e":"identifier = ('Taiwan*', None, 'ConfirmedCases')\n\nsamples_train = train[(train['Country_Region'] == identifier[0]) & train['Province_State'].isnull()][identifier[2]]\nsamples_eval = eval_set[(eval_set['Country_Region'] == identifier[0]) & eval_set['Province_State'].isnull()][identifier[2]]\nif identifier[1] is not None:\n    samples_train = train[(train['Country_Region'] == identifier[0]) & (train['Province_State'] == identifier[1])][identifier[2]]\n    samples_eval = eval_set[(eval_set['Country_Region'] == identifier[0]) & (eval_set['Province_State'] == identifier[1])][identifier[2]]\nparams = model_params[identifier]['params']\nprint(params)\nnum_samples = model_params[identifier]['num_samples']\nstart_date = model_params[identifier]['start_date']\npredictions = model_predictions[identifier]\nplt.scatter(range(num_samples), samples_train)\n# Plots the eval set points\n# Note that when using full training set before final submission, the eval set overlaps with training set, so the graph is misleading in terms of fit\nplt.scatter(range(num_samples-overlap_days, num_samples-overlap_days+len(samples_eval)), samples_eval)\nplt.plot(range(start_date, num_samples + len(predictions)), model(params, range(num_samples-start_date + len(predictions))))","2d655d3e":"eval_with_id = pd.merge(eval_set, test, on=['Date', 'Country_Region', 'Province_State'])\nsub = sub.replace([np.inf, -np.inf], np.nan)\nsub = sub.fillna(0)\nmerged_eval = pd.merge(eval_with_id, sub, left_on='ForecastId', right_index=True)\nmerged_eval","134200e6":"# Evaluation score, not accurate if using full dataset where training and eval set overlap\nscore_confirmed = np.sqrt(mean_squared_log_error(merged_eval['ConfirmedCases_x'].values, merged_eval['ConfirmedCases_y'].values))\nscore_fatality = np.sqrt(mean_squared_log_error(merged_eval['Fatalities_x'].values, merged_eval['Fatalities_y'].values))\nprint(f'Confirmed Cases Score: {score_confirmed}\\nFatality Score: {score_fatality}\\nAverage Score: {np.mean([score_confirmed, score_fatality])}')","deec3f0b":"sub.to_csv('submission.csv')\nsub","ff06725e":"def p2f(x):\n    try:\n        return float(x.strip('%'))\/100\n    except:\n        return None\n    \ndef s2f(x):\n    try:\n        return float(x)\n    except:\n        return None\n\npop_set = pd.read_csv('\/kaggle\/input\/population-by-country-2020\/population_by_country_2020.csv', converters={'Med. Age': s2f, 'Urban Pop %': p2f})\npop_set.rename(columns={ pop_set.columns[0]: \"Country\" , pop_set.columns[1]: \"Population\" , pop_set.columns[4]: \"Density\", pop_set.columns[8]: 'Median Age', pop_set.columns[9]: \"Urban Pop\"}, inplace = True)\npop_set = pop_set.iloc[:, [0, 1, 4, 8, 9]]\npop_set = pop_set.replace(regex={\n    'Czech Republic (Czechia)': 'Czechia', \n    \"C\u00f4te d'Ivoire\": \"Cote d'Ivoire\", \n    'St. Vincent & Grenadines': 'Saint Vincent and the Grenadines', \n    'Saint Kitts & Nevis': 'Saint Kitts and Nevis', \n    'Taiwan': 'Taiwan*', \n    'South Korea': 'Korea, South', \n    'United States': 'US'})\npop_set","fd15f0fe":"freedom_set = pd.read_csv('\/kaggle\/input\/cato-2017-human-freedom-index\/cato_2017_hfi_by_year_summary.csv')\nfreedom_set = freedom_set[freedom_set['Year'] == 2015]\nfreedom_set.rename(columns={ freedom_set.columns[2]: \"Country\" , freedom_set.columns[3]: \"Personal Freedom\" , freedom_set.columns[4]: \"Economic Freedom\", freedom_set.columns[5]: 'Human Freedom'}, inplace = True)\nfreedom_set = freedom_set.iloc[:, [2, 3, 4, 5]]\nfreedom_set = freedom_set.replace(regex={\n    'Czech Republic': 'Czechia', \n    'Taiwan': 'Taiwan*', \n    'Korea, Republic of': 'Korea, South', \n    'United States': 'US'})\nfreedom_set","32144024":"country_set = pd.read_csv('\/kaggle\/input\/countries-of-the-world\/countries of the world.csv', decimal=\",\")\ncountry_set.rename(columns={ country_set.columns[7]: \"Infant Mortality\" , country_set.columns[8]: \"GDP\" , country_set.columns[9]: \"Literacy\"}, inplace = True)\ncountry_set = country_set.iloc[:, [0,7,8,9]]\ncountry_set['Country'] = country_set['Country'].str.strip()\ncountry_set = country_set.replace(regex={\n    'Czech Republic': 'Czechia', \n    'Taiwan': 'Taiwan*', \n    'Korea, Republic of': 'Korea, South', \n    'United States': 'US'})\ncountry_set","0257b5a0":"from scipy import stats\ndef analyzeParams(model_params, index, measurement, external_set, external_column, threshold= 1000, label=False, xmin=None, xmax=None, ymin=None, ymax=None):\n    \n    external_vals = []\n    params_vals = []\n    names = []\n    \n    for identifier in model_params:\n        \n        if identifier[0] not in ['US', 'China'] and identifier[2] == measurement and model_params[identifier]['max_value'] > threshold:\n            try:\n                val = external_set.loc[external_set['Country'] == identifier[0], external_column].values[0]\n                if np.isnan(val):\n                    continue\n                external_vals.append(val)\n                if identifier[1] is None:\n                    names.append(identifier[0])\n                else:\n                    names.append(identifier[1] + ', ' + identifier[0])\n\n                params = model_params[identifier]['params']\n                params_vals.append(params[index])\n            except:\n                continue\n    \n\n    R = np.corrcoef(external_vals, params_vals)\n    plt.figure(figsize=(8, 6))\n    plt.scatter(external_vals, params_vals)\n    plt.xlim(xmin, xmax)\n    plt.ylim(ymin, ymax)\n    plt.title(\"Correlation R = \" + str(R[0, 1]))\n\n    if label:\n        for i, name in enumerate(names):\n            if (xmax is None or external_vals[i] <= xmax) and (xmin is None or external_vals[i] >= xmin) and (ymax is None or yparams_vals[i] <= ymax) and (ymin is None or params_vals[i] >= ymin):\n                plt.text(external_vals[i], params_vals[i], name, size=12)\n    ","33928e1a":"analyzeParams(model_params, 1, 'ConfirmedCases', pop_set, 'Density', 500, True)","a05892c2":"analyzeParams(model_params, 1, 'Fatalities', pop_set, 'Median Age', 10, True)","33cf9511":"analyzeParams(model_params, 1, 'ConfirmedCases', pop_set, 'Median Age', 500, True)","7c55efaf":"analyzeParams(model_params, 1, 'ConfirmedCases', freedom_set, 'Economic Freedom', 500, True)","872c51d6":"analyzeParams(model_params, 1, 'ConfirmedCases', freedom_set, 'Personal Freedom', 500, True)","2037657e":"analyzeParams(model_params, 1, 'ConfirmedCases', freedom_set, 'Human Freedom', 500, True)","b6335920":"analyzeParams(model_params, 1, 'Fatalities', country_set, 'Infant Mortality', 10, True)","be789510":"analyzeParams(model_params, 1, 'ConfirmedCases', country_set, 'Literacy', 500, True)","d0fae4fc":"analyzeParams(model_params, 1, 'ConfirmedCases', country_set, 'GDP', 500, True)","e61b39ff":"For documentation purposes, the average RMSLE for the models on the eval set during public evaluation period was about 0.85","adbdcafe":"There are several limitations to this model:\n* The model is relatively simplistic, so it will not effectively model multiple spikes or trends that may occur due to inconsistent testing, reporting, or policy changes\n* In cases like South Korea, in which the infection rate has \"plateaued\", the model predicts a flat saturation. In reality, South Korea managed to pull the curve down and the rate of infection after the initial exponential growth is now a linear growth, but it is not a flat plateau. Therefore, in regions in which the virus has already been slowed to linear growth, the model does not effectively reflect that. However, these situations are arguably not the main priority, because the focus is on analyzing the more dangerous exponential spread observed in other regions.","09d5e006":"During the first week of the forecasting effort, we focused on finding a general model that decently forecasts and models the virus spread. We eventually settled on an exponential decay in increasing form model. This model function is a function of time $t$ defined as: \n### $$y = C * (1 - e^{-\\alpha(t-d)})^{p}$$\n\nwhere \n* $C$ is some upper bound as the spread of the virus saturates the population \n* $\\alpha$ represents some growth factor\n* $d$ is some time offset\n* $p$ is a power factor we added that we noticed creates an exponential growth in the initial stages of the curve, which effectively acts like a delay of when the main curve spike occurs relative to the first few reported cases","7b50efe7":"# Covid-19 Forecast and Analysis","99b6d6a2":"Function to iterate through all countries, provinces, and measurement type (confirmed cases or fatalities) and learns fitted parameters for the above defined model","2cb34d2c":"Assuming the above model is sufficient, given enough data, the second part of this notebook will explore how the fitted parameters correlate with external data features for each region. For example, initial intuition would suggest that the growth rate $\\alpha$ would be positively negatively correlated with population density. This would hopefully shed some light to see what factors might contribute to different infection curves observed in different locations. \n","9cabdc32":"### Part 2: Analysis","9f33e470":"We load all of the data provided from the forecasting competition. \n\nNote: For training, we create an eval set for the purpose of developing and fine-tuning the model. This set is empty \/ disregarded when re-ran for submission purposes.","96019ee7":"Finally, write submissions.","1427da6f":"Compare predictions for the eval set with the ground truth, get preliminary score for the models.\n\nNote: The next two cells are irrelevant \/ useless when re-ran right before submission, because eval set would overlap with training set and not be used.","05350b5c":"Imports.","07c9e1da":"### Introduction","5265337a":"Given the model parameters learned for each tuple of (country, province, measurement type), we predict the values for test cases.","3cb35bbc":"The below cell prints out the training points, eval points, and fitted model curve for a specified country & province.\n\nUsed for debugging, visualization, sanity check purposes.","304ecbde":"All correlations found are incredibly weak...\n\nMight need to rethink model.\n\nCurrent model performs well for forecasting, but is not entirely interpretable (e.g. no singular growth factor, curve slope dependent on multiple parameters)\n\nPerhaps with more data the learned parameters would be more informative.\n\nAlternatively, either switch models or look into using external data as parameters for learning as well... (but intuitively I don't think that would help)","0ba1e32e":"### Part 1: Forecast and Prediction Model","345e2b0d":"Define the model, cost function, and learner.\n\nTo reiterate, model is defined as:\n$$y = C * (1 - e^{-\\alpha(t-d)})^{p}$$","58c97644":"### tl;dr\nWe create a simple model to sufficiently model infection curve, using forecasting as the metric. The simple model would ideally forecast decently well.\n\nNext, using the learned model parameters, analyze its correlation with external features for each location."}}