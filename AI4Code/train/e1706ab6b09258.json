{"cell_type":{"d03842c7":"code","6949a4eb":"code","258cde14":"code","27a96346":"code","0ba49ec4":"code","9a0f4f74":"code","70a4ab6b":"code","155d9921":"code","a79ace28":"code","508a07c4":"code","f2f36641":"code","bac80afa":"code","00267e0d":"code","ce292002":"code","1e0c238e":"code","c421990e":"code","26b4f83f":"code","35574bbe":"code","a25860d3":"code","90d06594":"code","f438d3ed":"code","d2089f6b":"code","d7fbcf9e":"code","af6c99d0":"code","3f1ac8e0":"code","a7487566":"code","fa3c961c":"code","24fcf034":"code","47dcfcf0":"code","764bee3a":"code","5d3eb93b":"code","f822b7e2":"code","b724eca0":"code","4af9dbe2":"code","d4c94e83":"code","8d6d2d5f":"code","81fcf609":"code","f1a1e8ef":"code","7355e01a":"code","f71c3ded":"code","5857b296":"code","0bbcce46":"code","315c87a8":"code","bc4f2348":"code","55c3847a":"code","588b5c22":"code","8c98678e":"markdown","de9e7da5":"markdown","1219169a":"markdown","9e01608c":"markdown","3fdacf24":"markdown","3010645a":"markdown","2dab470a":"markdown","13205cc1":"markdown","080bc7d7":"markdown","48bd02cc":"markdown","5bf058a5":"markdown","ed15c651":"markdown","7447a8f8":"markdown","892161e3":"markdown","960f8518":"markdown","d6b27cfc":"markdown","c61cb2da":"markdown","140b3f13":"markdown","626871de":"markdown","6c45a194":"markdown","6f55e5a6":"markdown","32c0e92f":"markdown","b89637a5":"markdown"},"source":{"d03842c7":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# importing libraries\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport plotly.express as xp\nsns.set()\n\nfrom scipy import stats\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport os\n\n\nfrom sklearn.metrics import accuracy_score\n\n#ML algoritms\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom xgboost import XGBClassifier\nimport xgboost as xgb\n\n\n\n#Performance metrics\nfrom sklearn.metrics import classification_report, confusion_matrix, plot_confusion_matrix","6949a4eb":"for dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","258cde14":"df = pd.read_csv('\/kaggle\/input\/fetal-health-classification\/fetal_health.csv')","27a96346":"df.info()","0ba49ec4":"df.describe().T","9a0f4f74":"# Plot fetal health with box plots, and present actual counts on each box. \nlabels = ['normal',\n         'suspect',\n         'pathological']\nfig, ax = plt.subplots(figsize=(12,6))\n_ = sns.countplot(df.fetal_health)\n_ = ax.set_xlabel('Fetal Health')\n_ = ax.set_xticklabels(labels)\n_ = plt.title('Distribution of Fetal Health Conditions')\n_ = plt.text(s = f\"n = {df.fetal_health.value_counts()[1]}\", x = -0.25, y = 800)\n_ = plt.text(s = f\"n = {df.fetal_health.value_counts()[2]}\", x = 1, y = 800)\n_ = plt.text(s = f\"n = {df.fetal_health.value_counts()[3]}\", x = 2, y = 800)\n\n","70a4ab6b":"plt.figure(figsize=(20,20))\ncorr = df.corr()\ncmap = sns.color_palette(\"light:b\", as_cmap=True)\nsns.heatmap(corr, annot=True, cmap=cmap)","155d9921":"def getall_visuals(data,column,xlabel, bins = 50):\n    \"\"\" This function plots histograms for 3 different fetal health classes\n    and one for all data. \n    \n    Column name should be str.\n    \"\"\"\n    fig = plt.figure(figsize=(12, 10))\n\n    plt.subplots_adjust(wspace= 0.25, hspace= 0.25)\n\n    sub1 = fig.add_subplot(2,3,1) # two rows, two columns, fist cell\n\n\n    # Create second axes, the top-left plot with orange plot\n    sub2 = fig.add_subplot(2,3,2) \n\n    #third ax, top right\n    sub3 = fig.add_subplot(2,3,3) \n\n    # Create forth ax\n    sub4 = fig.add_subplot(2,2,(3,4)) \n\n    sns.histplot(data[data['fetal_health']==1][column] , ax=sub1, color = 'g')\n    sns.histplot(data[data['fetal_health']==2][column] , ax=sub2, color = 'r')\n    sns.histplot(data[data['fetal_health']==3][column] , ax=sub3, color = 'y')\n    \n    sub1.set_xlabel(xlabel + ' of Normal')\n    sub2.set_xlabel(xlabel + ' of Suspicious')\n    sub3.set_xlabel(xlabel + ' of Pathological')\n    sub4.set_xlabel(xlabel + ' of All')\n    \n    \n    sns.histplot(data[column] , ax=sub4, bins=bins)","a79ace28":"getall_visuals(df, 'accelerations', 'Accelerations', 20 )","508a07c4":"getall_visuals(df, 'prolongued_decelerations', 'Prolonged Descelerations', 20 )","f2f36641":"getall_visuals(df, 'abnormal_short_term_variability', 'Abnormal Short Term Variability \\n', 50)","bac80afa":"getall_visuals(df,'percentage_of_time_with_abnormal_long_term_variability', 'Abnormal Long Term Variability \\n %', 50)","00267e0d":"from scipy.stats import kruskal, mannwhitneyu","ce292002":"normal = df[df['fetal_health']==1]\nsuspicious = df[df['fetal_health']==2]\npathological = df[df['fetal_health']==3]","1e0c238e":"for x in df.columns:\n    print(f\"p values comparing fetal health for {x} : {kruskal(normal[x],suspicious[x],pathological[x])[1]:.2f}\")","c421990e":"!pip install scikit_posthocs","26b4f83f":"import scikit_posthocs as sp\n\nfor col in df.columns:\n    print(col)\n    display(abs(sp.posthoc_dunn([normal[col], suspicious[col],pathological[col]], p_adjust = 'bonferroni')))\n    print('*'*50)","35574bbe":"# Visualize distributions of features\ndef bayesian_dist(column, data):\n    \"\"\"this function gets column name (str) and dataframe (str), \n    returns distribution plot of the column, skewness and kurtosis\"\"\"\n    sns.distplot(data[column])\n    plt.title(x)\n    plt.show()\n    plt.show()\n    print('skewness: ', stats.skew(data[column]))\n    print('kurtosis: ', stats.kurtosis(data[column]))\n    \n\ndef normal_visual(column, df):\n    \"\"\"This function gets column and dataframe as str.\n    Return \n    Shapiro Wilk test and Kolmogorov-Smirnov test results,\n    distplot, skewness and kurtosis of the column\n    \"\"\"\n    bayesian_dist(column, df)\n    print('*'* 30)\n    print(column, 'Shapiro-Wilk test t score: ', \"{:.2f}\".format(stats.shapiro(df[x])[0]))\n    print(column, 'Shapiro-Wilk test p value: ', \"{:.2f}\".format(stats.shapiro(df[x])[1]))\n    print('*'*30)\n    print(column, 'Kolmogorov-Smirnov t score: ', \"{:.2f}\".format(stats.kstest(df[x],'norm', args=(df[x].mean(),\n                                                                                                   df[x].std()))[0]))\n    print(column, 'Kolmogorov-Smirnov t score: ', \"{:.2f}\".format(stats.kstest(df[x],'norm', args=(df[x].mean(),\n                                                                                                   df[x].std()))[1]))\n    ","a25860d3":"for x in df.columns:\n    if x != 'fetal_health':\n        normal_visual(x, df)","90d06594":"from sklearn.preprocessing import StandardScaler\nss = StandardScaler()","f438d3ed":"# To use optuna for hyperparameter tuning, I need to change target values as 0,1,2. To avoid any mistakes, I will do it in the cell below.\ndf['fetal_health'].replace({1:0,2:1,3:2}, inplace = True)\n\n","d2089f6b":"# for test purposes, I'll seperate 10% of the data\nunseen_data = df.sample(n=210)\nremaining_data = df.drop(unseen_data.index)\nunseen_data.reset_index(inplace=True)\nremaining_data.reset_index(inplace=True)","d7fbcf9e":"remaining_data.drop('index',axis=1, inplace=True)\nunseen_data.drop('index',axis=1, inplace=True)","af6c99d0":"unseen_data['fetal_health'].value_counts()","3f1ac8e0":"remaining_data","a7487566":"scaled_data = remaining_data.copy()\nscaled_data.drop('fetal_health',axis=1,inplace=True)\n\nscaled_data = ss.fit_transform(scaled_data)","fa3c961c":"scaled_data = pd.DataFrame(scaled_data , columns = df.drop('fetal_health',axis=1).columns)\nscaled_data","24fcf034":"from sklearn.model_selection import train_test_split\ntarget = remaining_data['fetal_health']\npredictors = scaled_data.copy()\nx_train, x_test, y_train, y_test = train_test_split(predictors,target, train_size=0.7,\n                                                    random_state= 42, stratify = target.values)\n\n\ndisplay(x_train.shape)\ndisplay(y_train.shape)\ndisplay(x_test.shape)\ndisplay(y_test.shape)","47dcfcf0":"model_accuracy = pd.DataFrame(columns=['Model','Accuracy'])\nmodels = {\"LR\": LogisticRegression(),\n          \"KNN\" : KNeighborsClassifier(),\n          \"DT\" : DecisionTreeClassifier(),\n          'RFC' : RandomForestClassifier(),\n          'ABC' : AdaBoostClassifier(),\n          'GBC' : GradientBoostingClassifier(),\n          'DTC' : DecisionTreeClassifier(),\n          'XGB' : XGBClassifier(early_stopping_rounds = 300)\n          }\n\nfor test, clf in models.items():\n    clf.fit(x_train, y_train)\n    y_pred = clf.predict(x_test)\n    acc = accuracy_score(y_test,y_pred)\n    train_pred = clf.predict(x_train)\n    train_acc = accuracy_score(y_train, train_pred)\n    print( test + ' scores')\n    print(acc)\n    print(classification_report(y_test,y_pred))\n    print(confusion_matrix(y_test,y_pred))\n    print('*' * 100)\n    model_accuracy = model_accuracy.append({'Model': test, 'Accuracy': acc, 'Train_acc': train_acc}, ignore_index=True)","764bee3a":"model_accuracy.sort_values(ascending=False, by = 'Accuracy')","5d3eb93b":"target_unseen= unseen_data['fetal_health']\nfeatures_unseen = unseen_data.drop('fetal_health',axis=1)\nscaled_features = ss.transform(features_unseen)","f822b7e2":"XGBC = XGBClassifier(early_stopping_rounds=300)\nXGBC.fit(x_train, y_train)\ny_pred = XGBC.predict(x_test)","b724eca0":"confusion_matrix(y_test,y_pred)","4af9dbe2":"plt.subplots(figsize=(12,8))\ncf_matrix = confusion_matrix(y_test,y_pred)\nsns.heatmap(cf_matrix\/np.sum(cf_matrix), cmap=cmap,annot = True, annot_kws = {'size':15})","d4c94e83":"confusion_matrix(XGBC.predict(scaled_features), target_unseen)","8d6d2d5f":"plt.subplots(figsize=(12,8))\ncf_matrix = confusion_matrix(XGBC.predict(scaled_features), target_unseen)\nsns.heatmap(cf_matrix\/np.sum(cf_matrix), cmap=cmap,annot = True, annot_kws = {'size':15})","81fcf609":"import optuna\nxgbc = XGBClassifier()\ndef objective(trial):\n    train_x, test_x, train_y, test_y = x_train, x_test, y_train, y_test\n    dtrain = xgb.DMatrix(train_x, label=train_y)\n    dtest = xgb.DMatrix(test_x, label=test_y)\n\n    list_bins = [25, 50, 75, 100, 125, 150, 175, 200, 225, 250]\n    param = {\n        'verbosity' : 0,\n        'eta' : trial.suggest_uniform('eta', 0.1, 0.8),\n        'max_depth' : trial.suggest_int('max_depth', 3, 20),\n        'colsample_bytree' : trial.suggest_discrete_uniform('colsample_bytree',0.5,1.0,0.05),\n        \"booster\": trial.suggest_categorical(\"booster\", [\"gbtree\", \"gblinear\", \"dart\"]),\n        \"lambda\": trial.suggest_loguniform(\"lambda\", 1e-8, 1.0),\n        \"alpha\": trial.suggest_loguniform(\"alpha\", 1e-8, 1.0),\n        'subsample' : trial.suggest_discrete_uniform('subsample', 0.6, 0.9, 0.1),\n        'max_bin' : trial.suggest_categorical('max_bin', list_bins),\n        'objective' : 'multi:softmax',\n        'num_class' : 3,\n         \"eval_metric\": \"mlogloss\"\n    }\n         \n\n    # Add a callback for pruning.\n    pruning_callback = optuna.integration.XGBoostPruningCallback(trial, \"validation-mlogloss\")\n    bst = xgb.train(param, dtrain, early_stopping_rounds = 300, evals=[(dtest, \"validation\")], callbacks=[pruning_callback])\n    preds = bst.predict(dtest)\n    pred_labels = np.rint(preds)\n    accuracy = accuracy_score(test_y, pred_labels)\n    return accuracy","f1a1e8ef":"study = optuna.create_study()\nstudy.optimize(objective, n_trials=2)","7355e01a":"study.best_params","f71c3ded":"# These parameters always changes, however I tried various option. Each of them overfit and misclassified unseen data. So I leave this one as it is. \nXGBC_new = XGBClassifier(eta= 0.345,\n                    max_depth = 9,\n                    colsample_bytree = 0.7,\n                    booster = 'gblinear',\n                    reg_lambda = 6.280816614235506e-06,\n                    reg_alpha = 4.725045071821984e-08,\n                    subsamle = 0.6,\n                    max_bin = 250)","5857b296":"scaled_features = pd.DataFrame(scaled_features, columns = x_train.columns)","0bbcce46":"XGBC_new.fit(x_train,y_train)","315c87a8":"#\u00a0X_test prediction\ny_pred = XGBC_new.predict(x_test)\nconfusion_matrix(y_pred, y_test)","bc4f2348":"y_unseen = XGBC_new.predict(scaled_features)\nconfusion_matrix(y_unseen, target_unseen)","55c3847a":"print(classification_report(y_unseen, target_unseen))","588b5c22":"plt.subplots(figsize=(12,8))\ncf_matrix = confusion_matrix(target_unseen, y_unseen)\nsns.heatmap(cf_matrix\/np.sum(cf_matrix), cmap=cmap,annot = True, annot_kws = {'size':15})","8c98678e":"# Fetal Health Prediction \n\nA normal pregnancy lasts 9 months. Each three months period is called trimester. During this 9 months (and 3 trimesters) the fetus grows and gets ready for birth. However, due to various reasons the growth of fetus stops or disturbed. These various reasons may cause disability or mortality of the newborn. \n\nOne of the main assessment methods of fetal healt is cardiotocagraphy (CTG) which evaluates fetal heart beat and uterine contractions of mother. CTG monitoring is widely used to assess fetal wellbeing by identifying babies at risk of hypoxia (lack of oxygen). CTG is mainly used during labour.\n\nHearing your babies heart is one of the most beautiful things in the world, which comes after grabing your newborn. Me as a father of 3 months old baby boy, it was anxious thing each time whether it will be normal or not. Therefore, a good prediction model for fetal health will be a stress reliever for parents or parent to be's.\n\nIn this notebook I will try to build a model to predict whether a fetus is normal, suspicious or problematic with given CTG values.\n\nThis notebook includes;\n\n* Importing Libraries\n* Loading Data\n* Exploratory Data Analysis\n* Machine Learning","de9e7da5":"### Information About Data\n\nThis dataset contains 2126 records of features extracted from Cardiotocogram exams, which were then classified by expert obstetrician into 3 classes:\n\n1. Normal\n1. Suspect\n1. Pathological\n\n**Features**\n\nData has 21 features. Each feature is gathered from CTG scans. Further information about the data features https:\/\/www.ncbi.nlm.nih.gov\/pmc\/articles\/PMC6822315\/\n\n**Target**\n\n'fetal_health' Tagged as 1 (Normal), 2 (Suspect) and 3 (Pathological)","1219169a":"# Importing Libraries","9e01608c":"If you like my work, please consider giving an upvote:)","3fdacf24":"#### Observations\n\nThe data has 2126 rows, and 21 columns. Has no missings. Columns including information about Histograms seems a bit problem; the distance between min, 25% and 50% is not so close. I might say they have non-normal distributions. ","3010645a":"## Visuals","2dab470a":"#### Observation\n\nNormal values are gathered between 20-60; and the amount of variability of suspicious and pathologicals are gathered above 60's.","13205cc1":"# Exploratory Data Analysis","080bc7d7":"#### Observation\n\nMost of the normals are gathered at 0. Suspicious babies' values are gathered betweeen 0 to 70, pathological babies gathered aboce 75 if not in 0. ","48bd02cc":"#### Observation\n\nMost of the data were gathered aroun 0.  However, values above 0.005 is only on Normal cardigrams. ","5bf058a5":"There are significant differences for each feature between 3 groups. Let's see which group caused the differences. I need to perform a post hoc analysis. For kruskall wallis test, dunn test can be used. I will use scikit_posthocs library for dunn test.","ed15c651":"As we can see the differences I found with Kruskall Wallis test are caused by all groups' median values except Severe Decelerations and Prolonged Decelerations.\n\nThe difference in both groups are caused by Problematic fetuses (p<0.0017)","7447a8f8":"### Hyperparameter Optimization with Optuna \n\nXGBoostClassifier gets the best results here. I will use optuna to tune parameters. But first I need to change my target labels as 0 ,1 ,2 to avoid num_classes error in XGB. ","892161e3":"#### Interpretation\n\nThe distribution of the target variable is like real world; not all births are unhealthy or resulted with death of the newborn.  According to worldbank data, the mortality rate in 2019 28.2 per 1000 births. So we could expect a distribution like this one. However due to machine learning purposes, this is not good. I will work on this after finishing EDA.","960f8518":"# Machine Learning","d6b27cfc":"#### Observation\n\nAgain most of the data is at 0, however the amount of 0.001's and higher increasing in suspicious and pathologicals.\n\n","c61cb2da":"There are 20 features which might affect fetal health. Before investigating all of them (plotting every 20 of them with histograms might be over-plotting) First let's investigate heatmap.","140b3f13":"### Target Variable : Fetal_Health\n\nLet's investigate the target variable first. How it is distributed, is there any class imbalance?\n\nBest way to see these, in my humble opinion is to plot them. ","626871de":"## A\/B Testing\n\nWe have one categorical feature which includes three classes. In healthcare, we would like to know whether if there is a difference between groups or not. If we found a significant difference, we would search for which groups caused the difference.","6c45a194":"## Scaling, Machine Learning Implementation and Prediction","6f55e5a6":"# Loading Data","32c0e92f":"## Comparison of Basic model and Hyperparameter Tuned Model\nI tried various hyperparameters with optuna. However, it always kept resulting bad outcomes. I would use basic model here, that is because the basic model has good metrics and with the time consumed to hypertune the parameters I would get minimal increase or same result. ","b89637a5":"According to heatmap; accelerations, prolonged decelerations,  abnormal short term variability, percentage of abnormal long term variability have effects on fetal health. It's better to investigate these. Also, histogram median, mode and mean has high correlations in between. I need to choose one of them for my prediction. I will later investigate VIF scores of features.\n"}}