{"cell_type":{"6d8cf8b5":"code","e12e4cc6":"code","c5e1ae39":"code","65bb31e5":"code","3adfc66a":"code","c6155900":"code","16ac0bba":"code","bacae419":"code","dec6a055":"code","4cbd8938":"code","7ee3c92e":"code","4878c3f6":"code","0ca1760d":"code","26e50ab4":"code","5e4038a0":"code","0f96ae8d":"code","6141f7f2":"code","560455ab":"code","f37a79bb":"code","79407206":"code","f6e2ed17":"code","74d70eab":"code","10561938":"code","3de3ee26":"code","b52a9f79":"code","4b3b0962":"code","ff2aec5d":"markdown","260d7414":"markdown","5f2fea0a":"markdown","3d9d78bc":"markdown","df1c9abb":"markdown","023dd8e0":"markdown","742ee545":"markdown","6c685622":"markdown"},"source":{"6d8cf8b5":"!pip install -q --no-deps ..\/input\/fasthugs","e12e4cc6":"from fastai.text.all import *\nfrom fastai.callback.wandb import *\nfrom fasthugs.data import TransformersTextBlock, TextGetter\nfrom fasthugs.learner import TransLearner\n\nfrom transformers import AutoModelForSequenceClassification\nfrom sklearn.model_selection import StratifiedKFold, KFold\nimport gc\nimport wandb","c5e1ae39":"from kaggle_secrets import UserSecretsClient\n\nwandb_key = UserSecretsClient().get_secret(\"wandb_api_key\")","65bb31e5":"%env WANDB_ENTITY=arampacha\n%env WANDB_PROJECT=commonlit\n%env WANDB_SILENT=true","3adfc66a":"wandb.login(key=wandb_key)","c6155900":"path = Path('..\/input\/commonlitreadabilityprize')\noutput_path = Path('.\/')\npath.ls()","16ac0bba":"train_df = pd.read_csv(path\/'train.csv')\ntrain_df.head(2)","bacae419":"train_df.describe()","dec6a055":"cv = KFold(n_splits=5, shuffle=True, random_state=8)\nvalid_idxs = []\nfor _, valid_idx in cv.split(np.arange(len(train_df))):\n    valid_idxs += [valid_idx]","4cbd8938":"model_name = '..\/input\/roberta-transformers-pytorch\/distilroberta-base'","7ee3c92e":"dblock = DataBlock(blocks = [TransformersTextBlock(pretrained_model_name=model_name), RegressionBlock()],\n                   get_x=TextGetter('excerpt'),\n                   get_y=ItemGetter('target'),\n                   splitter=IndexSplitter(valid_idxs[0]))","4878c3f6":"dls = dblock.dataloaders(train_df, bs=16, val_bs=32, num_workers=2)","0ca1760d":"dls.show_batch(max_n=4)","26e50ab4":"model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=1)\nlearn = TransLearner(dls, model, metrics=rmse, path=output_path)","5e4038a0":"learn.lr_find()","0f96ae8d":"lr = 2e-5\nwd = 0.05","6141f7f2":"name = f\"{model_name}_lr{lr:.0e}-fold{0}\"\ngroup = f\"{model_name}_lr{lr:.0e}\"\nrun = wandb.init(name=name, group=group)","560455ab":"cbs=[WandbCallback(log_preds=False, log_model=False),\n     SaveModelCallback(monitor='_rmse', fname='model_0', comp=np.less)]\nlearn.fit_one_cycle(4, lr, wd=wd, cbs=cbs)","f37a79bb":"(output_path\/'models').ls()","79407206":"learn.validate()","f6e2ed17":"all_preds = []","74d70eab":"test_df = pd.read_csv(path\/'test.csv')\ntest_dl = dls.test_dl(test_df)\ntest_dl.show_batch()","10561938":"preds, _ = learn.get_preds(dl=test_dl)\nall_preds += [preds]","3de3ee26":"for i in range (1, len(valid_idxs)):\n    name = f\"{model_name}_lr{lr:.0e}-fold{i}\"\n    group = f\"{model_name}_lr{lr:.0e}\"\n    with wandb.init(name=name, group=group) as run:\n        dblock = DataBlock(blocks = [TransformersTextBlock(pretrained_model_name=model_name), RegressionBlock()],\n                       get_x=TextGetter('excerpt'),\n                       get_y=ItemGetter('target'),\n                       splitter=IndexSplitter(valid_idxs[i]))\n        dls = dblock.dataloaders(train_df, bs=16, val_bs=32, num_workers=2)\n        model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=1)\n        learn = TransLearner(dls, model, metrics=rmse)\n        cbs=[WandbCallback(log_preds=False, log_model=False),\n             SaveModelCallback(monitor='_rmse', fname=f'model_{i}', comp=np.less)]\n        learn.fit_one_cycle(4, 2e-5, wd=wd, cbs=cbs)\n        preds, _ = learn.get_preds(dl=test_dl)\n        all_preds += [preds]\n        del learn; gc.collect()\n        torch.cuda.empty_cache()","b52a9f79":"preds = torch.cat(all_preds, dim=1).mean(dim=-1)","4b3b0962":"submission = pd.read_csv(path\/'sample_submission.csv', index_col='id')\nsubmission['target'] = preds.numpy()\nsubmission.to_csv('submission.csv')","ff2aec5d":"## Submission\nFinally we can average the predictions from all models and submit:","260d7414":"## Data preprocessing","5f2fea0a":"Let's fit models on remaining folds and save all the prediction.","3d9d78bc":"## Cross validation","df1c9abb":"For demonstration purposes I'm using `distilroberta-base`. It is a lightweight distilled vertion of RoBERTa, which performes considerably worse. You can easily switch to other models by changing `model_name`. The `TransformersTextBlock` uses pretrained huggingface tokenizer internally and is set up by providing path to pretrained model. ","023dd8e0":"## Training on first fold","742ee545":"I'm going to log runs to Weights&Biases. This may be of greate use for further analysis of the results. As you shell see with fastai it doesn't require much extra work at all.","6c685622":"The best performing model is stored and loaded at the end of the training by `SaveModelCallback`:"}}