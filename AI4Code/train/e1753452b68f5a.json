{"cell_type":{"597d6691":"code","1838b2a3":"code","438a644b":"code","64a408b7":"code","9045fc6a":"code","05160c30":"code","7024fdc7":"code","91cfdf41":"code","87606a0e":"code","8802dea2":"code","97dc107f":"markdown","b1951acc":"markdown","28c8365f":"markdown","bbe4e84b":"markdown","e2952469":"markdown","cf7ccca6":"markdown"},"source":{"597d6691":"!pip install -q laserembeddings laserembeddings[zh] laserembeddings[ja]\n!pip install -q ftfy","1838b2a3":"# import fasttext\nimport ftfy\nimport html\nimport laserembeddings\nimport numpy as np\nimport pandas as pd\nimport re\nimport tensorflow as tf\nimport sys\n\nfrom fastcache import clru_cache\nfrom laserembeddings import Laser\nfrom typing import List, Union\nfrom urllib.parse import unquote\nfrom sklearn.model_selection import train_test_split","438a644b":"df_train = pd.read_csv('..\/input\/nlp-getting-started\/train.csv', index_col=0).fillna('')\ndf_test  = pd.read_csv('..\/input\/nlp-getting-started\/test.csv',  index_col=0).fillna('')\ndf_train","64a408b7":"def preprocess_text(df):\n    texts = df[['keyword', 'location', 'text']].agg(' '.join, axis=1)\n    texts = texts.apply(ftfy.fix_text)   # fix \\x89\n    texts = texts.apply(html.unescape)  \n    texts = texts.apply(unquote)         # remove %20\n    texts = texts.apply(lambda s: re.sub('@\\w+', ' ', s))            # remove @usernames\n    texts = texts.apply(lambda s: re.sub('#',    ' ', s))            # remove hashtag prefixes    \n    texts = texts.apply(lambda s: re.sub('\\n',   ' ', s))            # remove newlines\n    texts = texts.apply(lambda s: re.sub('\\w+:\/\/\\S+',  '<URL>', s))  # remove urls    \n    texts = texts.apply(lambda s: re.sub('\\s+',  ' ', s))            # remove multiple spaces    \n    return list(texts)\n    \npreprocess_text(df_train)[:10]\npreprocess_text(df_test)[:10]","9045fc6a":"%%bash\n# DOCS: https:\/\/github.com\/facebookresearch\/LASER\/blob\/master\/install_models.sh\n\nmkdir -p models\/laser\/\n# for FILE in bilstm.eparl21.2018-11-19.pt eparl21.fcodes eparl21.fvocab bilstm.93langs.2018-12-26.pt 93langs.fcodes 93langs.fvocab; do\nfor FILE in bilstm.93langs.2018-12-26.pt 93langs.fcodes 93langs.fvocab; do\n    wget -cq https:\/\/dl.fbaipublicfiles.com\/laser\/models\/$FILE -O models\/laser\/$FILE\ndone","05160c30":"# from config import config\n# from src.utils.fasttest_model import language_detect\n# from src.utils.punkt_tokenizer import punkt_tokenize_sentences\n\nconfig = {\n    \"laser\": {\n        \"base_dir\":  \".\/models\/laser\",\n        \"bpe_codes\": \".\/models\/laser\/93langs.fcodes\",\n        \"bpe_vocab\": \".\/models\/laser\/93langs.fvocab\",\n        \"encoder\":   \".\/models\/laser\/bilstm.93langs.2018-12-26.pt\",\n    }\n}\n\n# Instantiate encoder\n# BUG: CUDA GPU memory is exceeded if both laser and labse are loaded together \n# @clru_cache(None)\ndef get_laser_model():\n    laser_model = Laser(\n        bpe_codes = config['laser']['bpe_codes'],\n        bpe_vocab = config['laser']['bpe_vocab'],\n        encoder   = config['laser']['encoder'],\n        tokenizer_options = None,\n        embedding_options = None\n    )\n    return laser_model\n\n\ndef laser_encode(text: Union[str, List[str]], lang='en', normalize=True) -> np.ndarray:\n    \"\"\"\n    Encodes a corpus of text using LASER\n    :param text: Large block of text (will be tokenized), or list of pre-tokenized sentences\n    :param lang: 2 digit language code (optional autodetect)\n    :return:     embedding matrix\n    \"\"\"\n    laser_model = get_laser_model()\n    \n    # lang = lang or language_detect(text, threshold=0.0)\n    if isinstance(text, str):\n        # sentences = punkt_tokenize_sentences(text, lang=lang)\n        sentences = [ text ]\n    else:\n        sentences = list(text)\n\n    embedding = laser_model.embed_sentences(sentences, lang=lang)\n    \n    if normalize:\n        embedding = embedding \/ np.sqrt(np.sum(embedding**2, axis=1)).reshape(-1,1)\n        \n    return embedding","7024fdc7":"%%time \n\nX_train = laser_encode(preprocess_text(df_train))\nY_train = df_train['target']\n\nprint('X_train.shape', X_train.shape)\nprint('Y_train.shape', Y_train.shape)","91cfdf41":"# DOCS: https:\/\/keras.io\/examples\/keras_recipes\/antirectifier\/\n\n# Build the model\nmodel = tf.keras.Sequential([\n    tf.keras.Input(shape=(1024,)),\n    tf.keras.layers.Dense(512, activation=tf.keras.layers.LeakyReLU(alpha=0.1)),\n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.Dropout(0.25),\n    tf.keras.layers.Dense(128, activation=tf.keras.layers.LeakyReLU(alpha=0.1)),\n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.Dropout(0.25),\n    tf.keras.layers.Dense(32, activation=tf.keras.layers.LeakyReLU(alpha=0.1)),\n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.Dropout(0.25),\n    tf.keras.layers.Dense(8, activation=tf.keras.layers.LeakyReLU(alpha=0.1)),\n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.Dropout(0.25),\n    tf.keras.layers.Dense(1, activation=tf.keras.activations.sigmoid),\n])\n\n\ndef model_compile_fit(model, X, Y):\n    model.summary()\n    \n    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.1)\n        \n    # Compile the model\n    model.compile(\n        loss      = tf.keras.losses.BinaryCrossentropy(from_logits=True),\n        # optimizer = tf.keras.optimizers.RMSprop(learning_rate=0.001),\n        optimizer = tf.keras.optimizers.Adam(learning_rate=1e-5),\n        metrics   = [ tf.keras.metrics.BinaryAccuracy() ],\n    )\n    \n    # Train the model\n    model.fit(\n        X_train, Y_train, \n        batch_size = 32, \n        epochs     = 1000, \n        # validation_split = 0.2,\n        callbacks = [\n            # tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', verbose=0, patience=10),\n            # tf.keras.callbacks.ModelCheckpoint('model.h5',  monitor='binary_accuracy', mode='max', verbose=0, save_best_only=True)\n        ],\n        verbose=2\n    )\n    model.save('model.h5')\n    \n    print()\n    print('Train Accuracy')\n    model.evaluate(X_train, Y_train)\n\n    print('Test Accuracy')\n    model.evaluate(X_test, Y_test)\n\n    \nmodel_compile_fit(model, X_train, Y_train)","87606a0e":"%%time \n\nX_test = laser_encode(preprocess_text(df_test))\nY_test = tf.math.round( model.predict(X_test) ).numpy().astype(np.int32)","8802dea2":"df_submission = pd.read_csv('..\/input\/nlp-getting-started\/sample_submission.csv', index_col=0).fillna('')\ndf_submission['target'] = Y_test\ndf_submission.to_csv('submission.csv')\n!head submission.csv","97dc107f":"# Preprocess Text\n\nKeyword, location and text fields into a single string. \n\nSimple preprocessing is performed to remove HTML and encoded elements, @usernames, hashtag prefixes and urls.","b1951acc":"# LASER Embeddings\n\nThis encodes each of the strings as a LASER embedding (1024 dimentional vector)","28c8365f":"# NLP - LASER Embeddings + Keras\n\nThis approach encodes the tweets using [LASER](https:\/\/github.com\/yannvgn\/laserembeddings) multilingual sentence embeddings,\nfollowed by a [TF Keras](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras) dense neural network.","bbe4e84b":"# Neural Network - TF Keras\n\nDefine and train a dense neural network. \n\nThis inputs a 1024 LASER embedding and outputs a 1 bit classification prediction.\n\nA triangular shaped architecture is used, including Dropout and BatchNorm.","e2952469":"# Submission","cf7ccca6":"# Further Reading\n\nThis notebook is part of a series exploring Natural Language Processing\n- 0.74164 - [NLP Logistic Regression](https:\/\/www.kaggle.com\/jamesmcguigan\/disaster-tweets-logistic-regression)\n- 0.76677 - [NLP LASER Embeddings + Keras](https:\/\/www.kaggle.com\/jamesmcguigan\/nlp-laser-embeddings-keras)\n- 0.77536 - [NLP TF-IDF Classifier](https:\/\/www.kaggle.com\/jamesmcguigan\/disaster-tweets-tf-idf-classifier)\n- 0.79742 - [NLP Naive Bayes](https:\/\/www.kaggle.com\/jamesmcguigan\/nlp-naive-bayes)"}}