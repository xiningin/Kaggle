{"cell_type":{"67bd2465":"code","b4abd70f":"code","318ae813":"code","8965b535":"code","da9352a7":"code","2e1ea20a":"code","8cb4c2e1":"code","7cde51de":"code","03329651":"code","a9a76415":"code","d2860754":"code","8743da2b":"code","6c730d1f":"code","6902474d":"code","c9482224":"code","18bc8704":"code","2b92c709":"code","1fe5d333":"code","3589948c":"code","db3e7d89":"code","64eee38f":"code","f177fa8b":"code","cc924293":"code","33e24c0c":"code","590ca938":"code","4bfb21aa":"code","a0790f42":"code","e8b4d21f":"code","474dd2b4":"code","5dbb989b":"code","22bfbb92":"code","554003a6":"code","d50c644d":"code","50ea88ae":"code","06840330":"code","75a48b6f":"code","0adddd67":"code","6fbe310d":"code","f812a9ae":"code","d0cc116e":"code","ca4bfed9":"code","12a17722":"code","558e5e58":"code","161c63aa":"code","39112250":"code","e94267ef":"code","307f832d":"code","fbde268a":"code","b3a6bb8c":"code","32d5953f":"code","e104123a":"code","c802991b":"code","1715299d":"code","a2f4fb2c":"code","fe8f8c82":"code","3d2d514a":"code","99f59af6":"code","6718f9ea":"code","0fa0b073":"code","7ffebc41":"code","6e7f4e48":"code","706c7f8e":"code","3f2a42fd":"code","6561b93b":"code","e480b564":"code","0ae86979":"code","887a4ed2":"code","affc8fe0":"code","ff46052b":"code","4b8fc6ab":"code","f54fd81b":"code","e1421c58":"code","5845a65b":"code","495b09ac":"code","c937e035":"code","3dea5a62":"code","1d6105d1":"code","2f25ea02":"code","50391246":"code","8d3c8649":"code","151bdecd":"markdown","69492464":"markdown","a18efbab":"markdown","f74f1627":"markdown","67ed3a93":"markdown","21cd856b":"markdown","03d1322f":"markdown","c414ea8d":"markdown"},"source":{"67bd2465":"ls ..\/input\/competitive-data-science-predict-future-sales","b4abd70f":"import numpy as np\nimport pandas as pd\nimport sklearn\nimport gc\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n\nimport lightgbm as lgb\nfrom xgboost import XGBRegressor\nfrom sklearn.model_selection import cross_val_score\nfrom xgboost import plot_importance","318ae813":"%%time\nsales_train = pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/sales_train.csv', \n                          index_col='date', parse_dates=True)\nshops = pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/shops.csv')\nitem_categories = pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/item_categories.csv')\nitems = pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/items.csv')\nsample_sub = pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/sample_submission.csv')\ntest =  pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/test.csv')","8965b535":"print(len(set(sales_train['item_id'].values)))  # unique items\nsales_train.index","da9352a7":"sales_train.info()","2e1ea20a":"sales_train.loc[sales_train['item_id']==7409]['item_cnt_day'].plot(figsize=(12,5))","8cb4c2e1":"sales_train.loc[sales_train['item_id']==2552]['item_cnt_day'].plot(figsize=(12,5))","7cde51de":"sales_train.loc[sales_train['item_id']==7460]['item_cnt_day'].plot(figsize=(12,5))","03329651":"sales_train.loc[(sales_train['item_id']==2555) & (sales_train['shop_id']==25)]['item_cnt_day'].plot(figsize=(12,5))","a9a76415":"sales_train[['shop_id', 'item_id', 'item_cnt_day']]","d2860754":"sales_train.groupby('shop_id')['item_id'].count()","8743da2b":"df = sales_train[['item_cnt_day', 'shop_id', 'item_id']].groupby(['shop_id', 'item_id']).count()\ndf.reset_index()","6c730d1f":"df = sales_train[['item_cnt_day', 'shop_id', 'item_id']].groupby(['shop_id', 'item_id']).sum()\ndf.reset_index()","6902474d":"df.loc[df['item_cnt_day']==df['item_cnt_day'].max()]","c9482224":"sales_train[sales_train['shop_id']==28]","18bc8704":"df.plot(figsize=(20,5))","2b92c709":"sales_train.describe()","1fe5d333":"f1, axes = plt.subplots(1, 2, figsize=(12,5))\nf1.subplots_adjust(hspace=0.4, wspace=0.2)\nsns.boxplot(x=sales_train['item_price'], ax=axes[0])\nsns.boxplot(x=sales_train['item_cnt_day'], ax=axes[1])","3589948c":"## price outlier\nprint(sales_train.loc[sales_train['item_price']==sales_train['item_price'].max()])\nprint(sales_train.loc[sales_train['item_price']>20000])\nprint(sales_train.loc[sales_train['item_price']>30000])\nprint(sales_train.loc[sales_train['item_price']>50000])","db3e7d89":"sales_train = sales_train.drop(sales_train[sales_train['item_price']==307980].index)","64eee38f":"## item count outlier\nprint(sales_train.loc[sales_train['item_cnt_day']==sales_train['item_cnt_day'].max()])\nprint(sales_train.loc[sales_train['item_cnt_day']>2000])\nprint(sales_train.loc[sales_train['item_cnt_day']>1500])\nprint(sales_train.loc[sales_train['item_cnt_day']>800])","f177fa8b":"sales_train = sales_train.drop(sales_train[sales_train['item_cnt_day']>=1000].index)","cc924293":"f1, axes = plt.subplots(1, 2, figsize=(12,5))\nf1.subplots_adjust(hspace=0.4, wspace=0.2)\nsns.boxplot(x=sales_train['item_price'], ax=axes[0])\nsns.boxplot(x=sales_train['item_cnt_day'], ax=axes[1])","33e24c0c":"# \u042f\u043a\u0443\u0442\u0441\u043a \u041e\u0440\u0434\u0436\u043e\u043d\u0438\u043a\u0438\u0434\u0437\u0435, 56\nsales_train.loc[sales_train.shop_id == 57, 'shop_id'] = 0\ntest.loc[test.shop_id == 57, 'shop_id'] = 0\n# \u042f\u043a\u0443\u0442\u0441\u043a \u0422\u0426 \"\u0426\u0435\u043d\u0442\u0440\u0430\u043b\u044c\u043d\u044b\u0439\"\nsales_train.loc[sales_train.shop_id == 58, 'shop_id'] = 1\ntest.loc[test.shop_id == 58, 'shop_id'] = 1\n# \u0416\u0443\u043a\u043e\u0432\u0441\u043a\u0438\u0439 \u0443\u043b. \u0427\u043a\u0430\u043b\u043e\u0432\u0430 39\u043c\u00b2\nsales_train.loc[sales_train.shop_id == 11, 'shop_id'] = 10\ntest.loc[test.shop_id == 11, 'shop_id'] = 10","590ca938":"## extract city names from shop names\nfrom sklearn.preprocessing import LabelEncoder\nshops['city'] = shops['shop_name'].map(lambda row: row.split(' ')[0])\nshops.loc[shops.city == '!\u042f\u043a\u0443\u0442\u0441\u043a', 'city'] = '\u042f\u043a\u0443\u0442\u0441\u043a'\nencoder = LabelEncoder()\nshops['city_label'] = encoder.fit_transform(shops['city'])\n\n# removing city dummies, after looking at feature importance plot\n# shops = pd.concat((shops, pd.get_dummies(shops['city_label'],prefix='city')), axis=1)\nshops","4bfb21aa":"## add city labels in test dataset\ntest = test.join(shops, on='shop_id', rsuffix='_shop').drop(['shop_name', 'shop_id_shop', 'city'], axis=1)\ntest","a0790f42":"## Split categories\ncategories_split = item_categories['item_category_name'].str.split('-')\nitem_categories['main_category'] = categories_split.map(lambda row: row[0].strip())\nitem_categories['secondary_category'] = categories_split.map(lambda row: row[1].strip() if (len(row)>1) else 'N\/A')\n\n## Encode catgeroies\nitem_categories['main_cat_label'] = encoder.fit_transform(item_categories['main_category'])\nitem_categories['sec_cat_label'] = encoder.fit_transform(item_categories['secondary_category'])\n# item_categories = pd.concat((item_categories, pd.get_dummies(item_categories['main_cat_label'], prefix='main_cat')), axis=1) ## removing after looking at important features plot\n# item_categories = pd.concat((item_categories, pd.get_dummies(item_categories['sec_cat_label'], prefix='sec_cat')), axis=1)  ## removing after looking at important features plot\nitem_categories","e8b4d21f":"item_and_cat = items.join(item_categories, on='item_category_id', rsuffix='item_cat_').drop(['item_category_name', 'item_name', 'main_category', 'secondary_category', 'item_category_iditem_cat_'], axis=1)\n# 'item_category_id', 'main_cat_label', 'sec_cat_label'\nitem_and_cat","474dd2b4":"price_data = sales_train.groupby(['shop_id', 'item_id', 'date_block_num'])['item_price'].median().rename('item_price_median').reset_index()\nprice_data","5dbb989b":"# Create price categories (0-5, 5-10, 10,20, 20,30, 30-50, 50-100, >100)\ndef price_category(row):\n    if row.item_price_median<5.:\n        val = 1\n    elif row.item_price_median<10.:\n        val = 2\n    elif row.item_price_median<100.:\n        val = 3\n    elif row.item_price_median<200.:\n        val = 4\n    elif row.item_price_median<300.:\n        val = 5\n    elif row.item_price_median<500.:\n        val = 6\n    elif row.item_price_median<1000.:\n        val = 7\n    elif row.item_price_median>1000.:\n        val = 8\n    else:\n        val = 0\n    return val","22bfbb92":"%%time\nprice_data['price_cat'] = price_data.apply(price_category, axis=1)\nprice_data","554003a6":"monthly_sales = sales_train.groupby(['shop_id', 'item_id', 'date_block_num'])['item_cnt_day'].sum().rename('item_cnt_month').reset_index()\n\ntrain = pd.merge(monthly_sales, price_data, on=['shop_id', 'item_id', 'date_block_num'], how='left')\ntrain = pd.merge(train, item_and_cat, on=['item_id'], how='left')\ntrain = pd.merge(train, shops, on='shop_id', how='left')\ntrain = train.drop(['item_price_median'], axis=1)\ntrain","d50c644d":"pcat = price_data.drop(['price_cat', 'date_block_num'], axis=1).drop_duplicates().groupby(['shop_id', 'item_id'])['item_price_median'].median().rename('item_price_median').reset_index()\npcat['price_cat'] = pcat.apply(price_category, axis=1)\npcat","50ea88ae":"# pcat_uniq = price_data.drop(['item_price_median', 'date_block_num'], axis=1).drop_duplicates().reset_index().drop(['index'], axis=1)\n# pcat_uniq","06840330":"test = pd.merge(test, pcat, on=['shop_id', 'item_id'], how='left').reset_index().drop(['index'], axis=1)\ntest","75a48b6f":"test = test.drop(['ID', 'item_price_median'], axis=1)\ntest.columns","0adddd67":"df = pd.merge(items, sales_train[['item_id', 'item_price']], on='item_id', how='left')","6fbe310d":"df = df.groupby('item_category_id')['item_price'].median().rename('item_price_median').reset_index()","f812a9ae":"test = pd.merge(test, items[['item_id', 'item_category_id']], on='item_id', how='left')","d0cc116e":"test = pd.merge(test, df[['item_category_id', 'item_price_median']], on='item_category_id', how='left')\ntest","ca4bfed9":"%%time\ntest['price_cat_med'] = test.apply(price_category, axis=1)\ntest","12a17722":"test.columns","558e5e58":"test['price_cat'].fillna(test['price_cat_med'], inplace=True)\ntest.loc[test['price_cat'].isna()]","161c63aa":"test.drop(['price_cat_med', 'item_price_median'], axis=1, inplace=True)\ntest.columns","39112250":"test = pd.merge(test, item_categories.drop(['item_category_name', 'main_category', 'secondary_category'], axis=1), on='item_category_id', how='left')\n##, 'main_cat_label', 'sec_cat_label'\ntest.columns","e94267ef":"[item for item in train.columns if item not in test.columns]","307f832d":"train.drop(['shop_name', 'city'], axis=1, inplace=True)","fbde268a":"test['date_block_num']=34\ntest['item_cnt_month']=0\n[item for item in train.columns if item not in test.columns]","b3a6bb8c":"test['price_cat'] = test['price_cat'].astype(np.int8)\ntest['date_block_num'] = test['date_block_num'].astype(np.int8)\n\ntrain['price_cat'] = train['price_cat'].astype(np.int8)\ntrain['date_block_num'] = train['date_block_num'].astype(np.int8)\ntrain['item_cnt_month'] = train['item_cnt_month'].astype(np.int8)","32d5953f":"## prepare lag columns\ndef lag_feature(df, lags, col):\n    tmp = df[['date_block_num','shop_id','item_id',col]]\n    for i in lags:\n        shifted = tmp.copy()\n        shifted.columns = ['date_block_num','shop_id','item_id', col+'_lag_'+str(i)]\n        shifted['date_block_num'] += i\n        df = pd.merge(df, shifted, on=['date_block_num','shop_id','item_id'], how='left')\n    return df","e104123a":"## prepare lag columns with item_cnt aggregate\ndef prepare_lag_columns(df, lag, column_list, name):\n    tmp = df.groupby(column_list).agg({'item_cnt_month':['mean']})\n    tmp.columns = [name]\n    tmp.reset_index(inplace=True)\n    df = pd.merge(df, tmp, on=column_list, how='left')\n    df[name] = df[name].astype(np.float16)\n    df = lag_feature(df, lag, name)\n    df.drop([name], axis=1, inplace=True)\n    return df","c802991b":"## prepare lags with item_price aggregation\ndef prepare_lag_columns_price(df, column_list, name):\n    tmp = sales_train.groupby(column_list).agg({'item_price':['mean']})\n    tmp.columns = [name]\n    tmp.reset_index(inplace=True)\n    df = pd.merge(df, tmp, on=column_list, how='left')\n    df[name] = df[name].astype(np.float16)\n    return df","1715299d":"all_data = pd.concat([train, test], axis = 0, sort=False)\nall_data.fillna(0, inplace=True)\nall_data = all_data.reset_index()\nall_data","a2f4fb2c":"%%time\nall_data = lag_feature(all_data, [1,2,3,4,5,6,12], 'item_cnt_month')\nall_data","fe8f8c82":"%%time\nall_data = prepare_lag_columns(all_data, [1], ['date_block_num', 'item_id'], 'total_avg_month_cnt')\nall_data","3d2d514a":"%%time\nall_data = prepare_lag_columns(all_data, [1,2,3,4,5,6,12], ['date_block_num'], 'item_avg_month_cnt')\nall_data","99f59af6":"%%time\nall_data = prepare_lag_columns(all_data, [1,2,3,4,5,6,12], ['date_block_num', 'shop_id'], 'shop_avg_month_cnt')\nall_data","6718f9ea":"all_data.columns","0fa0b073":"%%time\nall_data = prepare_lag_columns(all_data, [1], ['date_block_num','city_label'], 'city_avg_month_cnt')\nall_data = prepare_lag_columns(all_data, [1], ['date_block_num','item_id','city_label'], 'item_city_avg_month_cnt')\nall_data = prepare_lag_columns(all_data, [1], ['date_block_num', 'item_category_id'], 'category_id_avg_month_cnt')\nall_data = prepare_lag_columns(all_data, [1], ['date_block_num', 'main_cat_label'], 'main_category_avg_month_cnt')\nall_data = prepare_lag_columns(all_data, [1], ['date_block_num', 'sec_cat_label'], 'secondary_category_avg_month_cnt')\nall_data = prepare_lag_columns(all_data, [1], ['date_block_num','shop_id','item_category_id'], 'shop_category_id_avg_month_cnt')\nall_data = prepare_lag_columns(all_data, [1], ['date_block_num','shop_id','main_cat_label'], 'shop_main_category_avg_month_cnt')\nall_data = prepare_lag_columns(all_data, [1], ['date_block_num','shop_id','sec_cat_label'], 'shop_secondary_category_avg_month_cnt')\nall_data","7ffebc41":"%%time\nall_data = prepare_lag_columns_price(all_data, ['item_id'], 'item_avg_price')\nall_data = prepare_lag_columns_price(all_data, ['date_block_num','item_id'], 'item_avg_price_month')\nall_data = lag_feature(all_data, [1,2,3,4,5,6], 'item_avg_price_month')\nall_data","6e7f4e48":"%%time\nfor lag in [1,2,3,4,5,6]:\n    all_data['trend_price_lag_'+str(lag)] = (all_data['item_avg_price_month_lag_'+str(lag)] - all_data['item_avg_price']) \/ all_data['item_avg_price']\n\nall_data    ","706c7f8e":"def clean_trend_price_lag(row):\n    for l in [1,2,3,4,5,6]:\n        if row['trend_price_lag_'+str(l)]:\n            return row['trend_price_lag_'+str(l)]\n    return 0","3f2a42fd":"%%time\ntmp_1, tmp_2, tmp_3, tmp_4 = [], [], [], []\ntmp_1 = pd.DataFrame(tmp_1)\ntmp_2 = pd.DataFrame(tmp_2)\ntmp_3 = pd.DataFrame(tmp_3)\ntmp_4 = pd.DataFrame(tmp_4)\ntmp_1 = all_data[:500000].apply(clean_trend_price_lag, axis=1)\ntmp_2 = all_data[500000:1000000].apply(clean_trend_price_lag, axis=1)\ntmp_3 = all_data[1000000:1500000].apply(clean_trend_price_lag, axis=1)\ntmp_4 = all_data[1500000:].apply(clean_trend_price_lag, axis=1)\nall_data['trend_price_lag'] = pd.concat([tmp_1, tmp_2, tmp_3, tmp_4])\nall_data['trend_price_lag'] = all_data['trend_price_lag'].astype(np.float16)\nall_data['trend_price_lag'].fillna(0, inplace=True)\nall_data","6561b93b":"for i in [1,2,3,4,5,6]:\n    all_data.drop(['item_avg_price_month_lag_'+str(i), 'trend_price_lag_'+str(i)], axis=1, inplace=True)","e480b564":"'''\n# Correlation matrix for monthly sales\nall_data_2 = all_data[all_data['date_block_num']<34]\n\n# Correlation matrix\nf = plt.figure(figsize=(9, 5))\nplt.matshow(all_data_2.corr(), fignum=f.number)\nplt.xticks(range(all_data_2.shape[1]),all_data_2.columns, fontsize=7, rotation=90)\nplt.yticks(range(all_data_2.shape[1]), all_data_2.columns, fontsize=7)\ncb = plt.colorbar()\ncb.ax.tick_params(labelsize=14)\n'''","0ae86979":"all_data.head()","887a4ed2":"all_data.drop(\n    ['price_cat', 'item_avg_price', 'item_avg_price_month', 'index'], \n    inplace=True, axis=1)\n\n#'sec_cat_label', main_cat_label, city_label","affc8fe0":"all_data.fillna(0, inplace=True)\nall_data.to_pickle('data.pkl')\ndata = pd.read_pickle('data.pkl')","ff46052b":"data = data[data.date_block_num > 11]","4b8fc6ab":"X_train = data[data.date_block_num < 33].drop(['item_cnt_month'], axis=1)\nY_train = data[data.date_block_num < 33]['item_cnt_month']\nX_valid = data[data.date_block_num == 33].drop(['item_cnt_month'], axis=1)\nY_valid = data[data.date_block_num == 33]['item_cnt_month']\nX_test = data[data.date_block_num == 34].drop(['item_cnt_month'], axis=1)\n\ngc.collect();","f54fd81b":"X_test","e1421c58":"%%time\nmodel=lgb.LGBMRegressor(\n        n_estimators=5000,\n        learning_rate=0.3,\n        min_child_weight=300,\n        #num_leaves=32,\n        colsample_bytree=0.8,\n        subsample=0.8,\n        max_depth=8,\n        #reg_alpha=0.04,\n        #reg_lambda=0.073,\n        #min_split_gain=0.0222415,\n        verbose=1,\n        seed=21)\n\nmodel.fit(X_train, Y_train,eval_metric=\"rmse\", eval_set=[(X_train, Y_train), (X_valid, Y_valid)], verbose=1, early_stopping_rounds = 10)","5845a65b":"print(*(all_data.columns), sep='\\n')","495b09ac":"[item for item in train.columns if item not in test.columns]","c937e035":"lgb.plot_importance(model, figsize=(10,14))","3dea5a62":"import pickle\nY_pred = model.predict(X_valid).clip(0, 20)\nY_test = model.predict(X_test).clip(0, 20)\n\nsubmission = pd.DataFrame({\n    \"ID\": test.index, \n    \"item_cnt_month\": Y_test\n})\nsubmission.to_csv('submission.csv', index=False)\n\n# save predictions for an ensemble\npickle.dump(Y_pred, open('xgb_train.pickle', 'wb'))\npickle.dump(Y_test, open('xgb_test.pickle', 'wb'))","1d6105d1":"def plot_features(booster, figsize):    \n    fig, ax = plt.subplots(1,1,figsize=figsize)\n    return plot_importance(booster=booster, ax=ax)","2f25ea02":"# model_xgb = XGBRegressor(\n#     max_depth=8,\n#     n_estimators=1000,\n#     min_child_weight=300, \n#     colsample_bytree=0.8, \n#     subsample=0.8, \n#     eta=0.3,    \n#     seed=21)\n\n# model_xgb.fit(\n#     X_train, \n#     Y_train, \n#     eval_metric=\"rmse\", \n#     eval_set=[(X_train, Y_train), (X_valid, Y_valid)], \n#     verbose=True, \n#     early_stopping_rounds = 10)","50391246":"# plot_features(model_xgb, (10,14))","8d3c8649":"# Y_pred_xg = model.predict(X_valid).clip(0, 20)\n# Y_test_xg = model.predict(X_test).clip(0, 20)\n\n# submission = pd.DataFrame({\n#     \"ID\": test.index, \n#     \"item_cnt_month\": Y_test_xg\n# })\n# submission.to_csv('xgb_submission.csv', index=False)\n\n# # save predictions for an ensemble\n# # pickle.dump(Y_pred, open('xgb_train.pickle', 'wb'))\n# # pickle.dump(Y_test, open('xgb_test.pickle', 'wb'))","151bdecd":"## Explore daily sale of items","69492464":"## Look for outliers and remove them","a18efbab":"## Extract main category and subcategory from category name","f74f1627":"## Load all files","67ed3a93":"## Impute missing item prices in test data with mean price of item category","21cd856b":"## Explore daily sale of items by shop","03d1322f":"## Merge all dataset to form training data","c414ea8d":"### Handle duplicate shops (see https:\/\/www.kaggle.com\/dlarionov\/feature-engineering-xgboost)"}}