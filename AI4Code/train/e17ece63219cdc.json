{"cell_type":{"55f359cc":"code","5236e68c":"code","03f211ec":"code","424e1dca":"code","f81216ee":"code","46680e38":"code","f047d430":"code","a46abf39":"code","020d4829":"code","43b21501":"code","baadb7a0":"code","6e47c7ee":"code","0e87bb42":"code","4aba89d9":"code","e016fedb":"code","f33ef977":"code","583a0f6d":"code","b387cc64":"code","b69a792d":"code","ee3ebe9d":"code","c4ff3c89":"code","9bf4cb97":"code","be5bfcbf":"code","4f81bedf":"markdown","f040de02":"markdown","f6ce2017":"markdown","364a0a94":"markdown","8398c6be":"markdown","f2cfa182":"markdown","701ffc14":"markdown","c14d553b":"markdown","8b53a3c5":"markdown","67cdbb6d":"markdown","4885c94d":"markdown","00aab4ed":"markdown","d03b8ec1":"markdown","de4788c0":"markdown"},"source":{"55f359cc":"import numpy as np \nimport pandas as pd \n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","5236e68c":"import matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nimport missingno as msno\nfrom scipy.stats import skew\nfrom sklearn.decomposition import PCA\nfrom sklearn.ensemble import RandomForestClassifier\nfrom lightgbm import LGBMClassifier\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import roc_auc_score,roc_curve\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom functools import partial\nimport optuna","03f211ec":"df=pd.read_csv('..\/input\/tabular-playground-series-mar-2021\/train.csv')\ndf.head()","424e1dca":"msno.matrix(df)","f81216ee":"cat_cols=df.select_dtypes('object')\n\nprint('Number of unique values for each categorical feature \\n')\nfor col in cat_cols:\n    print(f'{col} : {df[col].nunique()}')","46680e38":"explode=[0.1,0.1,0.1]\ncmap=plt.get_cmap('Paired')\ncolors=[cmap(i) for i in np.linspace(0,1,3)]\n\ncols=['cat5','cat10']\nfig=plt.figure(figsize=(15,15))\nfor i,col in enumerate(cols):\n    fig.add_subplot(1,2,i+1)\n    fig.set_size_inches(12,11)\n    pie=df[col].value_counts().head(3).plot.pie(shadow=True,\n                                           autopct='%1.1f%%',\n                                           explode=explode,\n                                           pctdistance=0.5,\n                                           colors=colors,\n                                           textprops={'fontsize':14})\n    plt.tight_layout()","f047d430":"num_cols=df.select_dtypes(['float64'])\nplt.style.use('seaborn-whitegrid')\n\nfor col in num_cols:\n    fig,ax=plt.subplots(2,1,sharex=True,\n                       gridspec_kw={'height_ratios':(0.25,0.75)})\n    fig.set_size_inches(7,6)\n    sns.boxplot(x=col,data=df,ax=ax[0])\n    sns.histplot(x=col,data=df,ax=ax[1])\n    ax[0].set_xlabel(col,fontsize=14)\n    ax[1].set_xlabel(col,fontsize=14)\n    ax[1].set_ylabel('Count',fontsize=14)\n    ax[0].set_yticks([])\n    sns.despine(ax=ax[1])\n    sns.despine(ax=ax[0],left=True)","a46abf39":"#dropping the id column since it won't be of much help in finding the correlation.\n\ncorr=df.drop('id',axis=1).corr(method='pearson')\nplt.figure(figsize=(10,8))\nplt.title('Correlation Analysis',fontsize=16)\nplt.xticks(rotation=90,fontsize=14)\nplt.yticks(fontsize=14)\n\nsns.heatmap(corr,annot=True,fmt='0.1f',\n            robust=True,cmap='coolwarm')","020d4829":"col1=['cont0','cont7','cont10','cont1']\ncol2=['cont10','cont0','cont7','cont2']\n\nfig=plt.figure()\nplt.style.use('seaborn-darkgrid')\nfor i in range(4):\n    fig.add_subplot(4,2,i+1)\n    fig.set_size_inches(10,12)\n    sns.scatterplot(x=col1[i],y=col2[i],data=df,\n                    alpha=0.1,edgecolor='none')\n    plt.tight_layout()","43b21501":"for col in num_cols:\n    print(col)\n    print('Skewness :',np.round(skew(df[col]),3))","baadb7a0":"cols=['cont8','cont9','cont10']\nfor col in cols:\n    df[col]=np.log(df[col])","6e47c7ee":"plt.figure(figsize=(6,6))\nsns.countplot(data=df,x='target')\nplt.xlabel('target',fontsize=14)\nplt.ylabel('Count',fontsize=14)","0e87bb42":"le=LabelEncoder()\nfor col in cat_cols:\n    df[col]=le.fit_transform(df[col])","4aba89d9":"y=df.target.values\nX=df.drop(['id','target'],axis=1).values","e016fedb":"def optimize(trial,x,y):\n    \n    num_iterations=trial.suggest_int('num_iterations',100,1500)\n    max_depth=trial.suggest_int('max_depth',3,15)\n    num_leaves=trial.suggest_int('num_leaves',10,100)\n    min_data_in_leaf=trial.suggest_int('min_data_in_leaf',1,100)\n    min_sum_hessian_in_leaf=trial.suggest_int('min_sum_hessian_in_leaf',1,200)\n    feature_fraction=trial.suggest_uniform('feature_fraction',1e-5,1.0)\n    bagging_fraction=trial.suggest_uniform('bagging_fraction',1e-5,1.0)\n    bagging_freq=trial.suggest_int('bagging_freq',1,10)\n    lambda_l1=trial.suggest_uniform('lambda_l1',1e-5,5.0)\n    lambda_l2=trial.suggest_uniform('lambda_l2',1e-5,10)\n   \n    model=LGBMClassifier(\n        num_iterations=num_iterations,\n        max_depth=max_depth,\n        num_leaves=num_leaves,\n        min_data_in_leaf=min_data_in_leaf,\n        min_sum_hessian_in_leaf= min_sum_hessian_in_leaf,\n        feature_fraction=feature_fraction,\n        bagging_fraction=bagging_fraction,\n        bagging_freq=bagging_freq,\n        lambda_l1=lambda_l1,\n        lambda_l2=lambda_l2\n    )\n    kf=StratifiedKFold(n_splits=5)\n    AUC=[]\n    for idx in kf.split(X=x,y=y):\n        train_idx,test_idx=idx[0],idx[1]\n        x_train,y_train=x[train_idx],y[train_idx]\n        x_test,y_test=x[test_idx],y[test_idx]\n       \n        model.fit(x_train,y_train)\n        preds=model.predict_proba(x_test)[:,1]\n        fold_auc=roc_auc_score(y_test,preds)\n        AUC.append(fold_auc)\n        \n    return -1*np.mean(AUC)","f33ef977":"#optimization_function=partial(optimize,x=X,y=y)\n#study=optuna.create_study(direction='minimize')\n\n#study.optimize(optimization_function,n_trials=10)","583a0f6d":"lgbm=LGBMClassifier(\n     num_iterations=1091,\n     max_depth=13,\n     num_leaves=22,\n     min_data_in_leaf=82,\n     min_sum_hessian_in_leaf=42,\n     feature_fraction=0.1631559284100434,\n     bagging_fraction=0.38583663547224584,\n     bagging_freq=7,\n     lambda_l1=0.054607760008535275,\n     lambda_l2=0.4441933265076425,\n     )\n\nX_train,X_val,y_train,y_val=train_test_split(X,y,test_size=0.3,stratify=y)\nlgbm.fit(X_train,y_train)\n","b387cc64":"lgbm_probs=lgbm.predict_proba(X_val)\n\n# Plotting the roc curve for probability prediction\nplt.style.use('seaborn-whitegrid')\nn_probs=[0 for _ in range(len(y_val))]\nlgbm_probs=lgbm_probs[:,1]\nns_auc=roc_auc_score(y_val,n_probs)\nlgbm_auc=roc_auc_score(y_val,lgbm_probs)\nprint('ROC AUC:%.3f' %(lgbm_auc))\nns_fpr,ns_tpr,_=roc_curve(y_val,n_probs)\nlgbm_fpr,lgbm_tpr,_=roc_curve(y_val,lgbm_probs)\nplt.figure(figsize=(9,7))\nplt.plot(ns_fpr,ns_tpr,linestyle='--',label='No skill')\nplt.plot(lgbm_fpr,lgbm_tpr,'g-',linewidth=2.3,label='positive outcome')\nplt.xlabel('False Positive Rate',fontsize=14)\nplt.ylabel('True positive rate',fontsize=14)\nplt.legend()","b69a792d":"df_=df.copy()\ndf_=df_.drop(['id','target'],axis=1)","ee3ebe9d":"x=pd.DataFrame(lgbm.feature_importances_)\nx.columns=['Feature Importance']\nx.index=df_.columns\nx=x.sort_values(by='Feature Importance',ascending=False)\n\nplt.style.use('default')\nplt.figure(figsize=(7,5))\nsns.barplot(x='Feature Importance',y=x.index,data=x)\nplt.xlabel('Feature Importance',fontsize=14)\nplt.ylabel('Feature',fontsize=14)\nplt.title('Feature Imporatnce Analysis',fontweight='bold',fontsize=10)\nyticks=plt.yticks(fontsize=8)\nxticks=plt.xticks(fontsize=8)","c4ff3c89":"X_test=pd.read_csv('..\/input\/tabular-playground-series-mar-2021\/test.csv')\nX_test.head()","9bf4cb97":"for col in cat_cols:\n    X_test[col],_=X_test[col].factorize()","be5bfcbf":"X_test['target']=lgbm.predict_proba(X_test.drop('id',axis=1))[:,1]\nsubmission=pd.DataFrame({'id':X_test['id'],'target':X_test['target']})\nsubmission.to_csv('my_submission.csv',index=False)","4f81bedf":"### Hyperparameter Optimization using Optuna\n* Below we fit and train a LGBMClassifier using StratifiedKFold. \n* Lastly we return the mean of the AUC score obtained as a result of 5 splits","f040de02":"### Checking for null values\n* missingno is a library which heps in visualizing missing values in our dataset","f6ce2017":"### Below heatmap illustrates correlation analysis of  a feature with the other features","364a0a94":"### Histogram and box plot below show distributions of all numerical columns.","8398c6be":"## **Exploring Numerical Columns**","f2cfa182":"### Cat10 has the maximum number of unique values followed by cat5. Let's plot a pie chart illustrating the top 3 maximum value counts in these categorical features","701ffc14":"### Checking skewness of numerical columns","c14d553b":"### cont7 and cont10 have a strong positive correlation while the same case is with cont7 and cont0. Lastly cont2 and cont1 have a correlation of 0.86. Below are the scatterplots for these features.","8b53a3c5":"### Checking for number of unique values per categorical column","67cdbb6d":"### Feature Importance Analysis","4885c94d":"## **Exploring Categorical Columns**","00aab4ed":"### Below we check the target feature and plot a count plot. We observe that class 0 has a greater count than that of class 1","d03b8ec1":"### From above we infer that no column has any missing vlaue","de4788c0":"### From above plots we infer that :\n*  For cat5 BI has the most value counts.\n* For cat10 however,all values have almost equal value counts."}}