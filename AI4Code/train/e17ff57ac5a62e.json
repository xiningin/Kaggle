{"cell_type":{"e5927287":"code","294d41b7":"code","51b19f3a":"code","2faa4976":"code","62ea2e7a":"code","74993345":"code","95981a6b":"code","5a2d301c":"code","1a6dd2d6":"code","eb52befc":"code","5d891895":"code","faecc6e3":"code","1edfbe73":"code","269ac3eb":"code","ac5a6675":"code","adc43f01":"code","576f95e2":"code","fab6bfa4":"code","201cf18a":"code","7e7d05fc":"code","d05355b4":"code","b879c27c":"code","34ee5822":"code","f98b205d":"code","f05edfac":"code","050386d9":"code","dc7c39ba":"code","644821a5":"code","443b6504":"code","0cd7e6d6":"code","2804d8e6":"code","52faa7d5":"code","42390d6a":"code","cf5afd29":"markdown","ea391c2e":"markdown","d5aa510d":"markdown","78891bc2":"markdown","ba01b117":"markdown","a0e024ac":"markdown","826e9fb7":"markdown","4dc8c2d6":"markdown","17521601":"markdown","01552a3a":"markdown","7f7a612f":"markdown","6ab38be7":"markdown","3b4c7fad":"markdown","955c0905":"markdown","e81b313a":"markdown","5c9cba90":"markdown","217b511b":"markdown","46d0dff9":"markdown","9fae1e3c":"markdown","08370701":"markdown"},"source":{"e5927287":"import numpy as np\nimport pylab as pl\nimport pandas as pd\nimport matplotlib.pyplot as plt \n%matplotlib inline\nimport seaborn as sns\nfrom sklearn.utils import shuffle\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import confusion_matrix,classification_report\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder, OrdinalEncoder\nfrom sklearn.model_selection import cross_val_score, GridSearchCV\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","294d41b7":"train= pd.read_csv('..\/input\/used-cars-price-prediction\/train-data.csv')\ntrain.head()","51b19f3a":"f, axes = plt.subplots(1,1, figsize = (16, 5))\ng1 = sns.distplot(train[\"Price\"], color=\"blue\",ax = axes)\nplt.title(\"Distributional of price\")","2faa4976":"import plotly.express as px\nfrom plotly.subplots import make_subplots\nimport plotly.graph_objs as go\n\nep = train['Owner_Type'].value_counts().reset_index()\nep.columns = [\n    'Owner_Type', \n    'percent'\n]\nep['percent'] \/= len(train)\n\nfig = px.pie(\n    ep, \n    names='Owner_Type', \n    values='percent', \n    title='Countplot of Owner_Type', \n    width=800,\n    height=500 \n)\n\nfig.show()","62ea2e7a":"cdi = train.sort_values(by='Price', ascending=False)[:100]\nfigure = plt.figure(figsize=(10,6))\nsns.barplot(y=cdi.Name, x=cdi.Price)\nplt.xticks()\nplt.xlabel('Price')\nplt.ylabel('Name')\nplt.title('Name (Cars) by Price')\nplt.show()","74993345":"\nplt.figure(figsize=[15,4])\nsns.countplot(x='Transmission', hue='Owner_Type',edgecolor=\"black\", alpha=0.7, data=train)\nsns.despine()\nplt.title(\"Countplot of transmission by Owner_Type \")\nplt.show()","95981a6b":"sns.barplot(train[\"Owner_Type\"],train[\"Kilometers_Driven\"],hue= train[\"Transmission\"],palette=\"spring\")\nplt.xticks(rotation=80)\nplt.title(\"Transmission: Kilometers_Driven comparsion\")","5a2d301c":"sns.barplot(train[\"Owner_Type\"],train[\"Kilometers_Driven\"],hue= train[\"Location\"],palette=\"spring\")\nplt.xticks(rotation=80)\nplt.title(\"Location: Kilometers_Driven comparsion\")","1a6dd2d6":"display(train[train[\"Owner_Type\"]==\"First\"][[\"Name\",\"Location\",\"Transmission\",\"Year\",\"Kilometers_Driven\",\"Fuel_Type\",\"Mileage\",\"Engine\",\"Power\",\n                                       \"Price\"]].sort_values(by=\"Price\", ascending= False).head(10).style.background_gradient(cmap=\"spring\"))","eb52befc":"display(train[train[\"Owner_Type\"]==\"Second\"][[\"Name\",\"Location\",\"Transmission\",\"Year\",\"Kilometers_Driven\",\"Fuel_Type\",\"Mileage\",\"Engine\",\"Power\",\n                                       \"Price\"]].sort_values(by=\"Price\", ascending= False).head(10).style.background_gradient(cmap=\"spring\"))","5d891895":"display(train[train[\"Owner_Type\"]==\"Third\"][[\"Name\",\"Location\",\"Transmission\",\"Year\",\"Kilometers_Driven\",\"Fuel_Type\",\"Mileage\",\"Engine\",\"Power\",\n                                       \"Price\"]].sort_values(by=\"Price\", ascending= False).head(10).style.background_gradient(cmap=\"spring\"))","faecc6e3":"display(train[train[\"Owner_Type\"]==\"Fourth & Above\"][[\"Name\",\"Location\",\"Transmission\",\"Year\",\"Kilometers_Driven\",\"Fuel_Type\",\"Mileage\",\"Engine\",\"Power\",\n                                       \"Price\"]].sort_values(by=\"Price\", ascending= False).head(10).style.background_gradient(cmap=\"spring\"))","1edfbe73":"display(train[train[\"Transmission\"]==\"Automatic\"][[\"Name\",\"Location\",\"Transmission\",\"Year\",\"Kilometers_Driven\",\"Fuel_Type\",\"Mileage\",\"Engine\",\"Power\",\n                                       \"Price\"]].sort_values(by=\"Price\", ascending= False).head(30).style.background_gradient(cmap=\"spring\"))","269ac3eb":"display(train[train[\"Transmission\"]==\"Manual\"][[\"Name\",\"Location\",\"Transmission\",\"Year\",\"Kilometers_Driven\",\"Fuel_Type\",\"Mileage\",\"Engine\",\"Power\",\n                                       \"Price\"]].sort_values(by=\"Price\", ascending= False).head(30).style.background_gradient(cmap=\"spring\"))","ac5a6675":"test= pd.read_csv('..\/input\/used-cars-price-prediction\/test-data.csv')\ntest.head()","adc43f01":"labels = {}\nfor col in train.select_dtypes(exclude = np.number).columns.tolist():\n    le = LabelEncoder().fit(pd.concat([train[col].astype(str),test[col].astype(str)]))   \n    train[col] = le.transform(train[col].astype(str))\n    test[col] = le.transform(test[col].astype(str))\n    labels [col] = le\nprint('Categorical columns:', list(labels.keys()))","576f95e2":"print(f'Percent of Nans in Train Data : {round(train.isna().sum().sum()\/len(train), 2)}')\nprint(f'Percent of Nans in Test  Data : {round(test.isna().sum().sum()\/len(test), 2)}')","fab6bfa4":"train = train.replace([np.inf, -np.inf], np.nan)\ntrain= train.fillna(train.mean())\ntrain","201cf18a":"test = test.replace([np.inf, -np.inf], np.nan)\ntest= test.fillna(test.mean())\ntest","7e7d05fc":"train = train.drop(columns=['New_Price'],\n                 axis=1)\ntrain = train.dropna(how='any')\nprint(train.shape)","d05355b4":"test= test.drop(columns=['New_Price'],\n\n                 axis=1)\ntest = test.dropna(how='any')\nprint(test.shape)","b879c27c":"#Select feature column names and target variable we are going to use for training\n\nfeatures =['Name','Location','Year','Kilometers_Driven','Fuel_Type','Transmission','Owner_Type','Mileage','Engine','Power','Seats']\ntarget = 'Price'","34ee5822":"#This is input which our classifier will use as an input.\ntrain[features].head(10)","f98b205d":"#This is input which our classifier will use as an input.\ntest[features].head(10)","f05edfac":"from sklearn.model_selection import train_test_split\nY = train['Price']\nX = train.drop(columns=['Price'])\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.15, random_state=9)\n\nprint('X train shape: ', X_train.shape)\nprint('Y train shape: ', Y_train.shape)\nprint('X test shape: ', X_test.shape)\nprint('Y test shape: ', Y_test.shape)","050386d9":"from sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import r2_score,mean_squared_error\nfrom sklearn.metrics import mean_absolute_error \n# We define the model\nestimator = RandomForestRegressor(random_state = 42,criterion='mse')\npara_grids = {\n            \"n_estimators\" : [10,50,100],\n            \"max_features\" : [\"auto\", \"log2\", \"sqrt\"],\n            'max_depth' : [4,5,6,7,8,9,15],\n            \"bootstrap\"    : [True, False]\n        }\n\n\nGrid = GridSearchCV(estimator, para_grids,cv= 5)\n# We train model\nGrid.fit(X_train, Y_train)\nbest_param = Grid.best_estimator_\nprint(best_param)\n","dc7c39ba":"# We predict target values (Split 15% from training data)\nY_predict = best_param.predict(X_test)\nY_predict","644821a5":"model = RandomForestRegressor(bootstrap=False, max_depth=15, max_features='log2',\n                      random_state=42)\n# We train model\nmodel.fit(train[features],train[target])","443b6504":"#Make predictions using the features from the test data set\npredictions = model.predict(test[features])\n","0cd7e6d6":"from sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom numpy import mean, std\n\ncv = KFold(n_splits=10, random_state=1, shuffle=True)\nscores = cross_val_score(model, train[features], train[target], cv=cv, n_jobs=-1)\nprint('Mean of Scores: %.3f' % (mean(scores)))","2804d8e6":"#Model 1\nfrom sklearn.metrics import mean_squared_error\nmean_squared_error(Y_test, Y_predict)\n","52faa7d5":"#Create a  DataFrame\nsubmission = pd.DataFrame({'Owner_Type':test['Owner_Type'],'Price':predictions})                        \n\n#Visualize the first 10 rows\nsubmission.head(10)","42390d6a":"#Convert DataFrame to a csv file that can be uploaded\n#This is saved in the same directory as your notebook\nfilename = 'submission.csv'\n\nsubmission.to_csv(filename,index=True)\n\nprint('Saved file: ' + filename)","cf5afd29":"# Model 1","ea391c2e":"# Transmission : Automatic\n","d5aa510d":"# Prepocessing","78891bc2":"# Model 3\n\nk-fold is a popular kind of cross-validation technique, in which, say k=10 for example, 9 folds for training and 1 fold for testing purpose and this repeats unless all folds get a chance to be the test set one by one. This way, it provides a good idea of the generalization ability of the model, especially when we have limited data and can't afford to split into test and training data.\n\nReference :https:\/\/www.researchgate.net\/post\/What-is-the-purpose-of-performing-cross-validation","ba01b117":"# Owner_Type = Third","a0e024ac":"## Data for training and validation (Measure MSE)\n\nTo select a set of training data that will be input in the Machine Learning algorithm, to ensure that the classification algorithm training can be generalized well to new data. For this study using a sample size of 15%, assumed it ideal ratio between training and validation","826e9fb7":"# Model","4dc8c2d6":"# Make Submission","17521601":"# Model 2","01552a3a":"# Measure mean squared error \n\nIn statistics, the mean squared error (MSE) or mean squared deviation (MSD) of an estimator (of a procedure for estimating an unobserved quantity) measures the average of the squares of the errors\u2014that is, the average squared difference between the estimated values and the actual value.\n\nReference : https:\/\/en.wikipedia.org\/wiki\/Mean_squared_error\n\n\n\n","7f7a612f":"Eliminate irrelevant variables in analysis such as New_Price.","6ab38be7":"# Check Missing Data","3b4c7fad":"# Visualization","955c0905":"# Owner_Type = First","e81b313a":"# Transmission : Manual","5c9cba90":"# Owner_Type =Fourth & Above","217b511b":"# GridSearchCV: RandomForestRegressor\n\n<img src='https:\/\/media.geeksforgeeks.org\/wp-content\/uploads\/20200516180708\/Capture482.png' width='400'>\n\n\n\n* GridSearchCV is a library function that is a member of sklearn's model_selection package. It helps to loop through predefined hyperparameters and fit your estimator (model) on your training set. So, in the end, you can select the best parameters from the listed hyperparameters. Reference : https:\/\/towardsdatascience.com\/grid-search-for-hyperparameter-tuning-9f63945e8fec#:~:text=What%20is%20GridSearchCV%3F,parameters%20from%20the%20listed%20hyperparameters.\n\n* A Random Forest is an ensemble technique capable of performing both regression and classification tasks with the use of multiple decision trees and a technique called Bootstrap and Aggregation, commonly known as bagging. The basic idea behind this is to combine multiple decision trees in determining the final output rather than relying on individual decision trees. Random Forest has multiple decision trees as base learning models. We randomly perform row sampling and feature sampling from the dataset forming sample datasets for every model. This part is called Bootstrap.\nReference : https:\/\/www.geeksforgeeks.org\/random-forest-regression-in-python\/\n","46d0dff9":"## prediction ","9fae1e3c":"# Owner_Type = Second","08370701":"## Prediction"}}