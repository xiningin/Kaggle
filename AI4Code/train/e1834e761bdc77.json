{"cell_type":{"6c15af74":"code","a342cd89":"code","4e8c091c":"code","d3a7b2b5":"code","bdae04eb":"code","0da66419":"code","3d2c2b51":"code","caeeaaeb":"code","0894589e":"code","714c193d":"code","7759310b":"code","b8608f7c":"code","2d1a19a2":"markdown","e4bfec2e":"markdown","63f5bf1a":"markdown","c95472bf":"markdown","5e3f430f":"markdown","f0ec810c":"markdown"},"source":{"6c15af74":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport keras\n\nimport os\nprint(os.listdir(\"..\/input\"))\n%matplotlib inline","a342cd89":"df_questions = pd.read_csv('..\/input\/Questions.csv', encoding='iso-8859-1')\ndf_tags = pd.read_csv('..\/input\/Tags.csv', encoding='iso-8859-1')\ndf_questions.head(n=2)","4e8c091c":"grouped_tags = df_tags.groupby(\"Tag\", sort='count').size().reset_index(name='count')\ngrouped_tags.Tag.describe()","d3a7b2b5":"num_classes = 100\ngrouped_tags = df_tags.groupby(\"Tag\").size().reset_index(name='count')\nmost_common_tags = grouped_tags.nlargest(num_classes, columns=\"count\")\ndf_tags.Tag = df_tags.Tag.apply(lambda tag : tag if tag in most_common_tags.Tag.values else None)\ndf_tags = df_tags.dropna()","bdae04eb":"import re \n\ndef strip_html_tags(body):\n    regex = re.compile('<.*?>')\n    return re.sub(regex, '', body)\n\ndf_questions['Body'] = df_questions['Body'].apply(strip_html_tags)\ndf_questions['Text'] = df_questions['Title'] + ' ' + df_questions['Body']","0da66419":"# denormalize tables\n\ndef tags_for_question(question_id):\n    return df_tags[df_tags['Id'] == question_id].Tag.values\n\ndef add_tags_column(row):\n    row['Tags'] = tags_for_question(row['Id'])\n    return row\n\ndf_questions = df_questions.apply(add_tags_column, axis=1)","3d2c2b51":"pd.set_option('display.max_colwidth', 400)\ndf_questions[['Id', 'Text', 'Tags']].head()","caeeaaeb":"from keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom sklearn.preprocessing import MultiLabelBinarizer\n\nmultilabel_binarizer = MultiLabelBinarizer()\nmultilabel_binarizer.fit(df_questions.Tags)\nlabels = multilabel_binarizer.classes_\n\nmaxlen = 180\nmax_words = 5000\ntokenizer = Tokenizer(num_words=max_words, lower=True)\ntokenizer.fit_on_texts(df_questions.Text)\n\ndef get_features(text_series):\n    \"\"\"\n    transforms text data to feature_vectors that can be used in the ml model.\n    tokenizer must be available.\n    \"\"\"\n    sequences = tokenizer.texts_to_sequences(text_series)\n    return pad_sequences(sequences, maxlen=maxlen)\n\n\ndef prediction_to_label(prediction):\n    tag_prob = [(labels[i], prob) for i, prob in enumerate(prediction.tolist())]\n    return dict(sorted(tag_prob, key=lambda kv: kv[1], reverse=True))","0894589e":"from sklearn.model_selection import train_test_split\n\nx = get_features(df_questions.Text)\ny = multilabel_binarizer.transform(df_questions.Tags)\nprint(x.shape)\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=9000)","714c193d":"most_common_tags['class_weight'] = len(df_tags) \/ most_common_tags['count']\nclass_weight = {}\nfor index, label in enumerate(labels):\n    class_weight[index] = most_common_tags[most_common_tags['Tag'] == label]['class_weight'].values[0]\n    \nmost_common_tags.head()","7759310b":"from keras.models import Sequential\nfrom keras.layers import Dense, Activation, Embedding, Flatten, GlobalMaxPool1D, Dropout, Conv1D\nfrom keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\nfrom keras.losses import binary_crossentropy\nfrom keras.optimizers import Adam\n\nfilter_length = 300\n\nmodel = Sequential()\nmodel.add(Embedding(max_words, 20, input_length=maxlen))\nmodel.add(Dropout(0.1))\nmodel.add(Conv1D(filter_length, 3, padding='valid', activation='relu', strides=1))\nmodel.add(GlobalMaxPool1D())\nmodel.add(Dense(num_classes))\nmodel.add(Activation('sigmoid'))\n\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['categorical_accuracy'])\nmodel.summary()\n\ncallbacks = [\n    ReduceLROnPlateau(), \n    EarlyStopping(patience=4), \n    ModelCheckpoint(filepath='model-conv1d.h5', save_best_only=True)\n]\n\nhistory = model.fit(x_train, y_train,\n                    class_weight=class_weight,\n                    epochs=20,\n                    batch_size=32,\n                    validation_split=0.1,\n                    callbacks=callbacks)","b8608f7c":"cnn_model = keras.models.load_model('model-conv1d.h5')\nmetrics = cnn_model.evaluate(x_test, y_test)\nprint(\"{}: {}\".format(model.metrics_names[0], metrics[0]))\nprint(\"{}: {}\".format(model.metrics_names[1], metrics[1]))","2d1a19a2":"## Preparing the contents of the dataframe\n\nThe question body contains html tags that we don't want to feed into our model. We will thus strip all tags and combine title and question body into a single field for simplicity.","e4bfec2e":"## Imbalanced Classes\nSome tags occur more often than others, thus the classes are not well balanced. The imbalanced class problem can be addressed by applying class weights, thus  weighting less frequent tags higher than very frequent tags.","63f5bf1a":"# Multi-label text classification with keras\n","c95472bf":"## Tokenizing the text\nThe text has to be vectorized so that we can feed it into our model. Keras comes with [several text preprocessing classes](https:\/\/keras.io\/preprocessing\/text\/) that we can use for that.\n\nThe labels need encoded as well, so that the 100 labels will be represented as 100 binary values in an array. This can be done with the [MultiLabelBinarizer](http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.preprocessing.MultiLabelBinarizer.html) from the sklearn library.","5e3f430f":"## Building a 1D Convolutional Neural Network","f0ec810c":"## Reducing the problem to the most common tags in the dataset\nWe only use the top 100 (arbitrarily picked number) tags because for rare tags there are simply not enough samples available to get reliable results."}}