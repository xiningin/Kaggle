{"cell_type":{"9577bbc5":"code","7bd92157":"code","3bb09b69":"code","0df7d622":"code","5b56455d":"code","25a549da":"code","32eabb5c":"code","cec0f662":"code","dfdfd285":"code","152792fe":"code","d6364404":"code","0e6c7fd3":"code","2967f7ad":"code","19d969ab":"code","cfca2f69":"code","57c2f11e":"code","e66c60b5":"code","5d451238":"code","7d0aeabc":"code","bf89efcf":"code","51901549":"code","56274c8b":"code","a1b16f37":"code","56f6b7f3":"code","511dd417":"code","5173b826":"markdown","b6886833":"markdown","65cc3867":"markdown","612f9924":"markdown","93a19c8b":"markdown","26cf3a54":"markdown","091535ef":"markdown","57fee873":"markdown","fa13d7a3":"markdown","682cf9cb":"markdown","cc75d44c":"markdown"},"source":{"9577bbc5":"# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","7bd92157":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport plotly.express as px\n%matplotlib inline\nimport sys,matplotlib,plotly\n\nprint(\"Python version: {}\".format(sys.version))\nprint(\"NumPy version: {}\".format(np.__version__))\nprint(\"pandas version: {}\".format(pd.__version__))\nprint(\"matplotlib version: {}\".format(matplotlib.__version__))\nprint(\"plotly version:{}\".format(plotly.__version__))","3bb09b69":"df=pd.read_csv('\/kaggle\/input\/heart-disease-uci\/heart.csv')","0df7d622":"df.head()","5b56455d":"print(df.info())\ndf.describe()","25a549da":"def summary(df, pred=None):\n    obs=df.shape[0]\n    types=df.dtypes\n    counts = df.apply(lambda x: x.count())\n    uniques = df.apply(lambda x: [x.unique()])\n    nulls=df.apply(lambda x: x.isnull().sum())\n    distincts=df.apply(lambda x: x.unique().shape[0])\n    missing_ratio=(df.isnull().sum()\/ obs)*100\n    print('Data Shape: ', df.shape)\n    if pred is None:\n        cols = ['types', 'counts', 'distincts', 'nulls', 'missing ratio', 'uniques']\n        str = pd.concat([types, counts, distincts, nulls, missing_ratio, uniques], axis = 1)\n    else:\n        corr = df.corr()[pred]\n        str = pd.concat([types, counts, distincts, nulls, missing_ratio, uniques, corr], axis = 1, sort=False)\n        corr_col = 'corr '+ pred\n        cols = ['types', 'counts', 'distincts', 'nulls', 'missing_ratio', 'uniques', corr_col ]\n    str.columns = cols\n    dtypes = str.types.value_counts()\n    print('___________________________\\nData types:\\n',str.types.value_counts())\n    print('___________________________')\n    return str","32eabb5c":"details=summary(df,'target')\ndetails.sort_values(by='missing_ratio', ascending=False)","cec0f662":"corr=df.corr(method='pearson')\nmask = np.triu(np.ones_like(corr, dtype=np.bool))\nf, ax = plt.subplots(figsize=(11, 9))\nsns.heatmap(corr, mask=mask, cmap='Spectral', vmax=.3, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5},annot=True)","dfdfd285":"from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.model_selection import GridSearchCV","152792fe":"df1=df.copy()\ndf1=df1.apply(LabelEncoder().fit_transform)\ndf1.head()","d6364404":"std_sclr=StandardScaler().fit(df1.drop('target',axis=1))","0e6c7fd3":"X=std_sclr.transform(df1.drop('target',axis=1))\ny=df['target']","2967f7ad":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)","19d969ab":"gbrt=GradientBoostingClassifier(max_depth=1,learning_rate=1,random_state=0)\ngbrt.fit(X_train,y_train)","cfca2f69":"print(\"Accuracy on training set: {:.3f}\".format(gbrt.score(X_train, y_train)))\nprint(\"Accuracy on test set: {:.3f}\".format(gbrt.score(X_test, y_test)))","57c2f11e":"param_grid = {'learning_rate':[0.15,0.1,0.05,0.01,0.005,0.001], 'n_estimators':[100,250,500,750,1000,1250,1500,1750]}","e66c60b5":"grid_search = GridSearchCV(\n    GradientBoostingClassifier(max_depth=4, min_samples_split=2, min_samples_leaf=1, subsample=1,max_features='sqrt', random_state=10), \n    param_grid, scoring='accuracy', cv=5)","5d451238":"grid_search.fit(X_train, y_train)\n#grid_search.grid_scores_, grid_search.best_params_, grid_search.best_score_","7d0aeabc":"print(\"Train set score: {:.2f}\".format(grid_search.score(X_train, y_train)))\nprint(\"Test set score: {:.2f}\".format(grid_search.score(X_test, y_test)))","bf89efcf":"print(\"Best parameters: {}\".format(grid_search.best_params_))\nprint(\"Best cross-validation score: {:.2f}\".format(grid_search.best_score_))","51901549":"print(\"Best estimator:\\n{}\".format(grid_search.best_estimator_))","56274c8b":"param_grid2 = {'max_depth': [2,3,4,5,6,7]}\ngrid_search2 = GridSearchCV(\n    GradientBoostingClassifier(max_depth=4, min_samples_split=2, min_samples_leaf=1, subsample=1,max_features='sqrt', random_state=10), \n    param_grid2, scoring='accuracy', cv=5)\n\ngrid_search2.fit(X_train, y_train)\n#grid_search.grid_scores_, grid_search.best_params_, grid_search.best_score_","a1b16f37":"print(\"Train set score: {:.2f}\".format(grid_search2.score(X_train, y_train)))\nprint(\"Test set score: {:.2f}\".format(grid_search2.score(X_test, y_test)))","56f6b7f3":"print(\"Best parameters: {}\".format(grid_search2.best_params_))\nprint(\"Best cross-validation score: {:.2f}\".format(grid_search2.best_score_))","511dd417":"print(\"Best estimator:\\n{}\".format(grid_search2.best_estimator_))","5173b826":"## Simple Gradient Boosting Classifier","b6886833":"## Scaling and Splitting Data","65cc3867":"Conclusion 2: Our accuracy had not changed significantly on a test set, but decreased train set accuracy.","612f9924":"Note: I don't want to focus on visualizing data.","93a19c8b":"# Gradient Boosting Machines and Grid Search","26cf3a54":"Conclusion 1: Our model will have best benefit when our learning_rate is equal to 0.01 and n_estimators to 250 having 0.84 accuracy comparing to previous example without tuning.","091535ef":"# Introduction\n\nThis Dataset is taken from UCI Database about heart diseases. Our goal is to predict the the presence of heart disease in patients ranging from 0 to 4. For this purpose, I will use GridSearchCV with Gradient Boosting Machines as a main algorithm comparing it with using Gradient Boosting Machine without cross validation.","57fee873":"## Gradient Boosting Classifier Using GridSearchCV\n\nIn order to perform grid search, I need to create a dictionary that GridSearchCV will use during tuning the parameters. Thus, (1) I will assign two paramters with 6 corresponding values (learning_rate and n_estimators) and (2) one paramter (max_depth) with 6 values.","fa13d7a3":"## A few words about attributes.\n\nThe dataset contains of 14 columns with 303 instances.\nFollowing is the outline of attributes:\n\n* age\n* sex\n* chest pain type (4 values)\n* resting blood pressure\n* serum cholestoral in mg\/dl\n* fasting blood sugar > 120 mg\/dl\n* resting electrocardiographic results (values 0,1,2)\n* maximum heart rate achieved\n* exercise induced angina\n* oldpeak = ST depression induced by exercise relative to rest\n* the slope of the peak exercise ST segment\n* number of major vessels (0-3) colored by flourosopy\n* thal: 3 = normal; 6 = fixed defect; 7 = reversable defect\n* target: 0-4\n\nAll data is integer, except oldpeak which is floating point number.","682cf9cb":"# Eploratory Data Analysis","cc75d44c":"# Basic Operations"}}