{"cell_type":{"5d717674":"code","818c4a03":"code","9486945f":"code","900af9a8":"code","3edccd44":"code","06ee473d":"code","69cbc797":"code","fd714c6b":"code","35253202":"code","584cc70d":"code","4befc32e":"code","1887c53c":"code","3cfcb428":"code","0a06a1fd":"code","52035dc5":"code","f88f1c7f":"code","0e011362":"code","b99a4c58":"code","d0a7d845":"code","e4a13f02":"code","432d97dc":"code","b631f486":"code","21a1ac4d":"code","14422262":"code","fc092872":"markdown","4c409e3d":"markdown","f6720736":"markdown","9c2b92f3":"markdown","621ddc42":"markdown","b8f5e039":"markdown","dd5d80e1":"markdown","36a24e97":"markdown","bab3eb32":"markdown","817b883f":"markdown","ee01eb00":"markdown","4ecf6dfc":"markdown","0a7a1c30":"markdown","7721dd44":"markdown","191ee118":"markdown"},"source":{"5d717674":"import numpy as np\nimport pandas as pd\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import LabelEncoder,OneHotEncoder\nfrom sklearn.model_selection import train_test_split","818c4a03":"data = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ndata = data[['Sex','Cabin','Embarked','Survived']]\ndata.head(3)","9486945f":"#checking missing values\ndata.isnull().sum()","900af9a8":"#non imputed data\ndf_nn = data.copy()\ndf = data.drop(columns=['Survived'])\ny = data['Survived']\n\n# We will fill the missing values\nimp = SimpleImputer(strategy=\"most_frequent\")\nimp.fit(df)\ndf[df.columns] = imp.transform(df)\n","3edccd44":"# train test split\n\nX_train,X_test,y_train,y_test = train_test_split(df,y)\nprint(X_train.shape)\nprint(X_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)","06ee473d":"dff = df.copy()\nfor feat in df.columns:\n    dff[feat] = dff[feat].astype('category')\n    dff[feat] = dff[feat].cat.codes\ndff.head()","69cbc797":"dff = df.copy()\nfor feat in dff.columns:\n    lb = LabelEncoder()\n    lb.fit(dff[feat])\n    dff[feat] = lb.transform(dff[feat])\n\nprint(dff.shape)\ndff.head()","fd714c6b":"df_train = X_train.copy()\ndf_test = X_test.copy()","35253202":"df_train = pd.get_dummies(df_train)\ndf_test = pd.get_dummies(df_test)\n\n# aliging the dataframes\ndf_train, df_test = df_train.align(df_test, join='left', axis=1) \n\n# checking df_test\ndf_test.head(3)","584cc70d":"df_train = df_train.fillna(0)\ndf_test = df_test.fillna(0)\n\nprint(df_train.shape)\nprint(df_test.shape)","4befc32e":"df_train.head(3)","1887c53c":"df_test.head(3)","3cfcb428":"# df_nn is the dataframe containing NaN values\ndff = df_nn.copy()\ndff = pd.get_dummies(dff,dummy_na=True)\n\nprint(dff.shape)\n","0a06a1fd":"dff = df.copy()\ndff = pd.get_dummies(dff,sparse=True)\nprint(dff.shape)","52035dc5":"df_train = X_train.copy()\ndf_test = X_test.copy()","f88f1c7f":"one = OneHotEncoder(handle_unknown ='ignore')\none.fit(df_train)\ntrain_vec = one.transform(df_train)\ntest_vec = one.transform(df_test)","0e011362":"feats = one.get_feature_names()\n# sparce to dense\ntrain_vec = train_vec.toarray()\ntest_vec = test_vec.toarray()\n\n\ntrain_df = pd.DataFrame(train_vec,columns=feats)\ntest_df = pd.DataFrame(test_vec,columns=feats)\n\nprint(train_df.shape)\nprint(test_df.shape)","b99a4c58":"train_df.head(3)","d0a7d845":"test_df.head(3)","e4a13f02":"dff = df.copy()\none = OneHotEncoder(sparse=False)\none.fit(dff)\ndff_ = one.transform(dff)\nfeats = one.get_feature_names()\ndff_ = pd.DataFrame(dff_,columns=feats)\nprint(dff_.shape)","432d97dc":"dff = df.copy()\none = OneHotEncoder(drop='first',sparse=False)\none.fit(dff)\ndff_ = one.transform(dff)\nfeats = one.get_feature_names()\ndff_ = pd.DataFrame(dff_,columns=feats)\nprint(dff_.shape)\n","b631f486":"df_train = X_train.copy()\ndf_test = X_test.copy()","21a1ac4d":"df['Cabin'].value_counts()","14422262":"\ndf_train['Cabin_encoded'] = df_train['Cabin'].apply(lambda x: 1 if x=='B96 B98' else 0)\ndf_test['Cabin_encoded'] = df_test['Cabin'].apply(lambda x: 1 if x=='B96 B98' else 0)\ndf_train['Cabin_encoded']","fc092872":"### (b) using Sklearn Label Encoder","4c409e3d":"\nMachine learning and deep learning models, require all input and output variables to be numeric. This means that if your data contains categorical data, you must encode it to numbers before you can fit and evaluate a model. There are several techniques for encoding categorical variables. Let us discuss about 3 main important types of encoding we use. These are the 3 main types of encoding that I usually use in my daytoday projects.\n\n1. Label Encoding\n2. Onehot Encoding\n3. Custom Encoding\n\n\n\n> Note: This is not a kernel regarding titanic survival prediction. This is just a tutorial to familarize different types of data encoding techniques.","f6720736":"You can see other various types of categorical encoding in internet. I just explained the most commmon ones that we use in our daily projects.","9c2b92f3":"# 3. Custom encoding\n\nHere we just encode the categories to numbers we want. I will explain this with one examples","621ddc42":"# 1. Label encoding\n\nIn Label encoding, each label is converted into an integer value. We will create a variable that contains the categories representing the education qualification of a person. \n\n### (a)  Using category codes approach\n\nThis approach requires the category column to be of \u2018category\u2019 datatype. By default, a non-numerical column is of \u2018object\u2019 type. So you might have to change type to \u2018category\u2019 before using this approach.","b8f5e039":"Using pandas get_dummies() in train and test data is little tricky. Its easier to apply get_dummies on entire dataframe. But a good approach is that the test data should contains only those features from train. So we will do the following approach.\n\n1. First apply get_dummies() on train data and test data seperatly.\n2. Now align the test data with features in train data.\n3. The second step can result in lot of NaN values in test data if there are new categories in features in test data.\n4. A better approach is fill those NaN values with zeros if the ML algorithm we use wont be able to handle NaN values","dd5d80e1":"I had seen people encoding categories in different manner. Sometimes the whole dataframe is enocded and then split in to train and test. In those cases the above encoding would work. Suppose we want to encode the variables after fitting on training data. In those cases better we depend on one hot encoding.","36a24e97":"\n> **Note 2:**  Sometimes we want to handle with high dimensional data for example 1000 +  dimensions. Also if the number of unique values is high, the encoded output will be too large. In those cases it is bettwe if we handle data sparce as it will speed up the computatiion. Let us see how we can produce sparce encoded vector.","bab3eb32":"## (b)Using sklearn Onehot Encoder\n\nWe had seen that its little tricky to use get_dummies(). Don't worry sklearn provides a more user friendly package to achieve the same..This is one of the most commonly used one hot encoders. I will also try to explain various use cases using this.\n","817b883f":"# 2 Onehot Encoding\n\n\n## (a) Using pandas get_dummies","ee01eb00":"In case of Cabin  we can see that there are 147 unique values. But out of this cabin 'B96 B98' is repeating 691 times and rest very few times. So Suppose we want to encode 'B96 B98' as 1 and rest as 0. Such type of encoding is called as custom encoding. Let us see how we can do this. ","4ecf6dfc":"**Notes:**\n\n1. handle_unknown = 'ignore\u2019 is important.If an unknown category is encountered during transform, the resulting one-hot encoded columns for this feature will be all zeros. ie, in earlier case we alligned and fill unknown category of test data with zeros. Here it will automatically handle these.\n\n2. Another important thing is that by default one hot encoder will return sparce vector. It will be useful for faster computations when handling high dimensional data. We can also turn it off by setiing sparse = False or lese we want to convert it to dense if we want to view the dataframe.","0a7a1c30":"> **Note 1:**\nIn normal case get_dummies() will ignore NaN values.Consider a case where we have NaN values in our dataframe.Suppose we want to handle it as a seperate column. Then we can use the following approach.\n","7721dd44":"Note: Example for getting non sparce vector directly.","191ee118":"Note: Droping first one hot feature of every column. This will be helpful mainly in case of linear models where multicollineariy can affects model."}}