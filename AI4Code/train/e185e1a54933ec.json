{"cell_type":{"05db4c73":"code","e6f7e308":"code","120b33d2":"code","0cef4ac7":"code","7dfd4534":"code","0949b3de":"code","ebc80546":"code","ac85a0e4":"code","2f87bc6b":"code","bde10f8d":"code","ba6941b9":"code","4099d35e":"code","dcd9b77b":"code","2ab925f1":"code","896a9151":"code","3272d347":"code","a52c8d64":"code","abac05f3":"code","9418705c":"code","e1885b9c":"code","fa249be1":"code","0fe8bbb7":"code","70b69ba9":"code","8a3583c8":"code","b0a783e5":"code","660c376b":"code","bbe53829":"code","9d5b4eb4":"code","1f6a2367":"code","a29d6e97":"code","d6e29d6d":"code","cf2a3b97":"code","8dce1d18":"code","7adda62e":"code","4645d507":"code","6370d84e":"code","dd1e8ea8":"code","9464f044":"code","0a10c441":"code","118815c5":"code","f3c2ed6c":"code","b2172b54":"code","369b1658":"code","a256d144":"code","18da20d1":"code","1d4c8f28":"code","48deb7f5":"code","be0b821d":"code","613acb17":"code","fcf209e2":"code","02fa03f5":"code","5fda9b35":"code","aa4aaf1e":"code","04b039e7":"code","63e5140e":"code","a044ed3b":"code","fc02a30a":"code","744e67c7":"code","fd077a71":"code","67cef405":"code","6559a880":"code","62a038d9":"code","1a78a0e5":"code","d6bf0fe4":"code","2e888e60":"code","1edf95a4":"code","559bd136":"code","229b3192":"code","afa0bbb2":"code","62db5b50":"code","5674aad4":"markdown","b49aea3b":"markdown","031b327c":"markdown","ef1ee2ff":"markdown","e6e32290":"markdown","adc90195":"markdown","8ef44157":"markdown","bc777294":"markdown","4d3d1110":"markdown","04f3a57a":"markdown","b3431f15":"markdown","c4531c18":"markdown","ca59899f":"markdown","b4c6f20c":"markdown","8465ea47":"markdown","8ab65460":"markdown","3a6a94e8":"markdown","07ac3819":"markdown","d14ff51d":"markdown","98590506":"markdown","2d85f035":"markdown","c3df7665":"markdown","1bf9fab7":"markdown","4e3d4ad2":"markdown","6ef9e9b9":"markdown","3787f819":"markdown","df7ee917":"markdown","98ef90f8":"markdown","07a45786":"markdown","845fe808":"markdown","8bfbedc0":"markdown","0e8cf971":"markdown","548eafd3":"markdown","72ff6105":"markdown","956a5c67":"markdown","f0876aa5":"markdown","b0c17e58":"markdown","461e8c12":"markdown","0d1affc9":"markdown","49fb7786":"markdown","e9510c6a":"markdown","ed8bcfd2":"markdown","9f26df26":"markdown","404a243e":"markdown","6bd14c01":"markdown","a6f772ca":"markdown","14ebaa6c":"markdown","9fc2e98b":"markdown"},"source":{"05db4c73":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","e6f7e308":"#for analysis of data, dataframe\nimport numpy as np\nimport pandas as pd\n\n#for plotting and stuffs\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline \n#the above line of code is known as a magic function, helps to display our plots just below our code in the notebook.\n\n#for model training & prediction\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression","120b33d2":"#read training data into 'train_df' dataframe\ntrain_df=pd.read_csv('..\/input\/train.csv')\n\n#read testing data into 'test_df' dataframe\ntest_df=pd.read_csv('..\/input\/test.csv')\n\n#combined dataset, will be handy in wrangling steps.\ncombined_df=[train_df,test_df]","0cef4ac7":"train_df.columns","7dfd4534":"test_df.columns","0949b3de":"#to know what type of data columns hold ; 'object' type means they hold string values\ntrain_df.dtypes","ebc80546":"test_df.dtypes","ac85a0e4":"train_df.info()","2f87bc6b":"test_df.info()","bde10f8d":"#train_df.info(verbose=False) will give a compact version of the above output, it set to True by default(in above case).\ntrain_df.info(verbose=False)","ba6941b9":"train_df.head() #by default it prints first 5 rows, any other integer can also be given inside parenthesis.","4099d35e":"test_df.head()","dcd9b77b":"train_df.describe()\n#this gives metric\/stats of various columns.","2ab925f1":"ax=train_df['Sex'].value_counts().plot.bar(title='Sex Distribution aboard Titanic',figsize=(8,4))\n\n#below loop is to print numeric value above the bars\nfor p in ax.patches:\n    ax.annotate(str(p.get_height()),(p.get_x(),p.get_height()*1.005))\n\nsns.despine()  #to remove borders (by default : from top & right side)","896a9151":"sns.set(style='whitegrid')\nax=sns.kdeplot(train_df['Age'])\nax.set_title('Age Distribution aboard the Titanic')\nax.set_xlabel('<---AGE--->')","3272d347":"print(train_df['Survived'].value_counts())\nl=['Not Survived','Survived']\nax=train_df['Survived'].value_counts().plot.pie(autopct='%.2f%%',figsize=(6,6),labels=l)\n#autopct='%.2f%%' is to show the percentage text on the plot\nax.set_ylabel('')","a52c8d64":"sns.countplot(train_df['Pclass'])\nsns.despine()","abac05f3":"sns.countplot(train_df['Embarked'])","9418705c":"train_df[['Sex','Survived']].groupby('Sex').mean()","e1885b9c":"train_df[['Pclass','Survived']].groupby('Pclass').mean()","fa249be1":"train_df.groupby(['Pclass','Survived'])['Pclass'].count()","0fe8bbb7":"sns.countplot(x='Pclass',hue='Survived',data=train_df)","70b69ba9":"train_df[['Embarked','Survived']].groupby('Embarked').mean()","8a3583c8":"train_df[['Parch','Survived']].groupby('Parch').mean()","b0a783e5":"train_df[['SibSp','Survived']].groupby('SibSp').mean()","660c376b":"ax=train_df[['Parch','Survived']].groupby('Parch').mean().plot.line(figsize=(8,4))\nax.set_ylabel('Survival')\nsns.despine()","bbe53829":"ax=train_df[['SibSp','Survived']].groupby('SibSp').mean().plot.line(figsize=(8,4))\nax.set_ylabel('Survival')\nsns.despine()","9d5b4eb4":"a=sns.FacetGrid(train_df,col='Survived')\na.map(sns.distplot, 'Age')","1f6a2367":"a=sns.FacetGrid(train_df,col='Pclass',row='Survived')\na.map(plt.hist,'Age')","a29d6e97":"train_df['Embarked'].value_counts()","d6e29d6d":"a=sns.FacetGrid(train_df,col='Embarked')\na.map(sns.distplot,'Survived')","cf2a3b97":"train_df.groupby(['Embarked','Survived'])['Embarked'].count()","8dce1d18":"a=sns.FacetGrid(train_df,col='Embarked')\na.map(sns.pointplot, 'Pclass','Survived','Sex') #colum order is x='Pclass', y='Survived', hue='Sex'\na.add_legend()","7adda62e":"train_df.groupby(['Embarked','Sex'])['Embarked'].count()","4645d507":"a=sns.FacetGrid(train_df,col='Survived')\na.map(sns.barplot,'Sex', 'Fare')","6370d84e":"combined_df[0].head(3) #[0] is train_df","dd1e8ea8":"combined_df[1].head(3)  #[1] is test_df","9464f044":"print('training data dimensions :',train_df.shape)\nprint('testing data dimensions :', test_df.shape)\nprint('combined data\\'s dimension are :\\n',combined_df[0].shape,'\\n',combined_df[1].shape)","0a10c441":"train_df[['PassengerId','Name','Ticket','Cabin']].head()","118815c5":"#removing mentioned columns from dataset\ntrain_df=train_df.drop(['Name','Ticket','Cabin','SibSp','Parch','PassengerId'],axis=1)\ntest_df=test_df.drop(['Name','Ticket','Cabin','SibSp','Parch'],axis=1)","f3c2ed6c":"# the combined data\ncombined_df=[train_df, test_df]","b2172b54":"#lets check the new dimensions\nprint('new training data dimensions :',train_df.shape)\nprint('new testing data dimensions :', test_df.shape)\nprint('new combined data\\'s dimension are :\\n',combined_df[0].shape,'\\n',combined_df[1].shape)","369b1658":"train_df.head(3)","a256d144":"#checking for any null values\ntrain_df.isnull().any() #True means null present","18da20d1":"test_df.isnull().any()","1d4c8f28":"# age columns\nprint('mean age in train data :',train_df['Age'].mean())\nprint('mean age in test data :',test_df['Age'].mean())","48deb7f5":"#replacing null values with 30 in age column\nfor df in combined_df:\n    df['Age']=df['Age'].replace(np.nan,30).astype(int)","be0b821d":"train_df['Embarked'].value_counts()","613acb17":"#most people embarked from 'S'. So, we'll replace the missing missing Embarked value by 'S'.\ntrain_df['Embarked']=train_df['Embarked'].replace(np.nan,'S')","fcf209e2":"#finding mean fare in test data\ntest_df['Fare'].mean()","02fa03f5":"#replace missing fare values in test data by mean\ntest_df['Fare']=test_df['Fare'].replace(np.nan,36).astype(int)","5fda9b35":"combined_df=[train_df,test_df]\nfor df in combined_df:\n    print(df.isnull().any()) #bool value = False means that there are no nulls in the column.","aa4aaf1e":"#will code female as 1 and male as 0\nfor df in combined_df:\n    df['Sex']=df['Sex'].map({'female':1,'male':0}).astype(int)","04b039e7":"train_df.head(3)","63e5140e":"#coding Embarked column as: S=2, C=1, Q=0\nfor df in combined_df:\n    df['Embarked']=df['Embarked'].map({'S':2,'C':1,'Q':0}).astype(int)","a044ed3b":"train_df.head(3)","fc02a30a":"#binning or making bands of age into intervals and then assigning labels to them(encoding the bands as 0,1,2,3,4)\nfor df in combined_df:\n    df['Age']=pd.cut(df['Age'],5,labels=[0,1,2,3,4]).astype(int) #pandas cut will help us divide age in bins","744e67c7":"train_df.head(3)","fd077a71":"#binning fares and assigning label 0,1,2,3 to their respective bins\nfor df in combined_df:\n    df['Fare']=pd.qcut(df['Fare'],4,labels=[0,1,2,3]).astype(int)","67cef405":"train_df.head(3)","6559a880":"test_df.head(3)","62a038d9":"X_train=train_df.drop('Survived',axis=1)\nY_train=train_df['Survived']\n\n#X_train is the entire training data except the Survived column, which is separately stored in Y_train. We will use these to train our MODEL !\n\nX_test=test_df.drop('PassengerId',axis=1).copy()\n#X_test is the test data, for on which we will apply model and predict the \"SURVIVED\" column for its entries.","1a78a0e5":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier","d6bf0fe4":"#first applying Logistic Regression\n\nlg = LogisticRegression()\nlg.fit(X_train, Y_train)\nY_pred1 = lg.predict(X_test)\naccu_lg = (lg.score(X_train, Y_train))\nround(accu_lg*100,2)","2e888e60":"#applying decision tree\n\ndtree = DecisionTreeClassifier()\ndtree.fit(X_train, Y_train)\nY_pred2 = dtree.predict(X_test)\naccu_dtree = (dtree.score(X_train, Y_train))\nround(accu_dtree*100,2)","1edf95a4":"#applying random forest\n\nrafo = RandomForestClassifier(n_estimators=100)\nrafo.fit(X_train, Y_train)\nY_pred3 = rafo.predict(X_test)\naccu_rafo = rafo.score(X_train, Y_train)\nround(accu_rafo*100,2)","559bd136":"#our goal was to predict survived column for test data, and were asked to submit a dataframe with 'PassengerId' and 'Survived' columns\n\nsubmission=pd.DataFrame({'PassengerId':test_df['PassengerId'],'Survived':Y_pred3})","229b3192":"submission.shape","afa0bbb2":"submission.head(10)","62db5b50":"submission.to_csv('submission.csv', index=False)","5674aad4":"As seen above- this is how data looks in the dataframe -train_df & test_df we created. Sweet. We can infer that:\n* There are few categorical variables.\n* Pclass has 3 categories namely 1,2 & 3\n* Sex - male, female category\n* Embarked has 3 catefories - S,Q,C\n* Survived has 2 - 1(survived) or 0(not survived)\n* Other columns such as Age, Ticket, Fare have continuous numerical values.","b49aea3b":"From above it is clear which columns hold 'int', 'float' and 'strings'. This is important as we will convert them accordingly ahead, to make it understandable for the machine(models) to do training, prediction etc.\n\nAnother method to have a detailed info about our dataset is to use \".info()\". As shown below.","031b327c":"From above it is clear that, \n* mean age of traveller onboard the Titanic is about 30.\n* The max age is 80 and min. is 0.42 (few months old baby !)\n* the avg. survival rate is 0.38, meaning the survival rate was less than 50% for passengers ! [Note: 1 means survived & 0 means not survived]\n* The mean fare is around 32 and the max fare is 512. Also looking at the std.deviation of Fare, it seems Fare varied greatly.\n\n\n","ef1ee2ff":"More than 600 passengers embarked from 'S', followed by 'C' and then 'Q'.","e6e32290":"Remember, we had combined both training and testing data earlier, below is a view of it.","adc90195":"Shown above - name of columns of both training & testing data, and it is clear from it that the test data is missing \"Survived\" column, and it is our **aim to predict that column**.","8ef44157":"The above SibSp vs Survival graph first increases (for SibSp 0 to 1) then comes down (for SibSp 1 to 5) eventually to become zero.","bc777294":"The above table makes it clear that, the survival of a female passenger was much higher than male passenger.","4d3d1110":"**Get to know the data we've loaded.** \n\n* This shall be the first step after we read data into dataframes.\n* We should know how many rows, columns the dataset has, their names, the values stored and their datatypes.\n* All this forms the first part of any analysis.","04f3a57a":"In training data, 'Embarked' also has missing values..","b3431f15":"The above plots and result analyse 'Embarked' and whether it impacts Survival. We find that :\n* In terms of numbers, max survival and also death, comes from passengers who embarked from 'S'.\n* Survival of members from 'S' is high compared to 'Q' and 'C'.\n* Passengers embarked from 'Q' had the least survival.","c4531c18":"It is clear from above bar graph that the male population on the titanic exceeded the female population. But what was their survival rate ? We'll find out.","ca59899f":"Looking at the peaks in above plots, we observe :\n* Many toddlers+children(upto 10 yrs age) survived\n* Many teenagers 20-25 years old, didn't survive\n* Many middle aged passengers, 30-45 survived.","b4c6f20c":"*Thank You*","8465ea47":"**LOADING LIBRARIES**\n\nThe first step is to load all the necessary packages and libraries that will be used in this kernel. This includes :\n* Numpy & Pandas for handling our Data & DataFrames.\n* Matplotlib, Seaborn for visualitsations.\n* Machine learning packages (from SKlearn) for training and prediction.\n","8ab65460":"**Data Wrangling**\n\n* Data Wrangling involves cleaning, organizing the data, making it more suitable for machine learning.\n* it involves steps, such as removing nulls, mapping, encoding etc.","3a6a94e8":"As seen above, 'Sex' column has been changed, males have been coded as 0 and females as 1.","07ac3819":"The above pointplot may seem complex, but is simple and insightful one.\n\n* Among passengers embarked from S, females had high survival compared to males. Also, as seen in first plot, the Survival decresed as we go from Pclass 1 to 3, for both sexes. (NOTE : Strong dip in Survival of females from Pclass 2 to 3)\n\n* Third plot, Embarked='Q', also has similar pattern, with females of high Pclass having high survival. BUT, males of Pclass 3 had slightly high survival chance compared to Pclass 2 & 3 (strange).\n\n* Now from the second plot, Embarked='C', this plot is unusual, as we can see** males survived more than female**. ","d14ff51d":"The above graph shows that,\n* most population was in the age group 20-40 years\n* toddlers+children exceeded old folks(>60 yrs) in number","98590506":"**Further Analysis**\n\nNow, we shall do detailed analysis of the data. Our goal is to predict survival for test data, and to do that we should find out what all factors(features) led to the survival(Survived=1) of a person.\n\nFor such detailed analysis, plots are the best !","2d85f035":"Null values interfere with our training and prediciton. So they have to be removed or be filled with relevant, suitable data.\n* Above results show, which columns have null values.. we'll correct them one by one.","c3df7665":"As seen above, we have modified our train and test datas, making it suitable for our models, to do training & prediciton properly.","1bf9fab7":"**LOAD THE DATA !**\n\nWe are given 3 files, one for training(train.csv) one for testing(test.csv) and a sample submission file(gender_submission.csv).\nWe shall read the first 2 files using pandas.","4e3d4ad2":"The above plot further strengthens our observations made earlier:\n* Among the the Classes, Class 3 had most deaths, that too in 20-30 age group.\n* Class 1 had the lowest passenger deaths.\n* Survival percentage of Class 1 passenger was highest, that too in age group 30-40.","6ef9e9b9":"The above plot confirms that :\n* Those who paid a higher ticket fare had more chances of Survivng.\n* Also,females paid higher ticket price compared to males.","3787f819":"From above it is clear that, our training data has 891 entries, with \"Age\", \"Cabin\" & \"Embarked\" columns having missing values.\nOur test data has 418 entries, and for these given 418 entries, we have to predict their survival. Even the test data is missing some values in \"Age\", \"Fare\", \"Cabin\" columns.","df7ee917":"Embarked vs Survived table shows that :\n* People embarked from 'C' had high Survival.\n* Followed by from 'Q' and 'S'","98ef90f8":"We note that :\n\n* Score of Logistic Regression is lowest\n\n* The score from Decision Tree and Random forest is similar.","07a45786":"Similarly, we found out earlier that Fare played imp. role in survival, so we bin fare in to groups, just as we did for age.","845fe808":"As seen above, 'Embarked' column has been changed, S, C and Q have been coded or changed to numeric values.","8bfbedc0":"As we know, some column have ***categorical values*** such as Sex, Pclass, Embarked.\n\nThe values in these columns can be \"categorised\" or can be put into certain categories. For eg: Sex in our dataset can be categorised as either male or female, similarly Embarked into S,Q,C.\n\nWe will now convert these into numeric data, or codify them. As done below. This is known as **ENCODING**\n","0e8cf971":"\n**Hello everyone**, \n* *This is my first kernel on the platform and also my first competition submission. *\n* *Being a beginner myself, I've tried to make this kernel simple and easy to understand for other fellow beginners who have just started.*\n* *Feel free to write down your opinions & comments.*","548eafd3":"We can see from above graph, majority(more than 400) travelled in Class3, followed by Class1(~200). We will soon find out chances of survival based on PClass !","72ff6105":"From above it is clear, the Submission file is as per the requirement. Now writing it to csv file format.","956a5c67":"**First look of our data !**","f0876aa5":"Now, we'll find how these columns relate to Survival (if at all).","b0c17e58":"We know from earlier analysis that age was a factor in the survival of a passenger.\n\nAlso the range of values age takes is very high(from 0.42 to 80).\n\n* So we shall divide age in to age groups\/bands for easier training and prediction. This is called **BINNING**.","461e8c12":"* We see from above, among 891 entries in training data, just 342 (38% as seen in pie chart) survived !\n* 549 passengers or ~61% did not make it !","0d1affc9":"We can see from above 2 tables that:\nSibSp i.e number of siblings ans spouse aboard the Titanic, Parch- number of parent\/children don't show any pattern\/trend with survival, meaning :\nLooking at the table of Parch & Survival, first Survival rate increases first, then decreases, then again increases.\nThe graph below makes it clear.","49fb7786":"We see above that, higher the Passenger class, higher the survival rate.","e9510c6a":"So, our final dataframe looks like the one shown above,\n* Pclass, Sex, Age(binned), Embarked and Fare(binned) will be our feature set, i.e these play a factor in the survival of passenger and will be used in PREDICTION of test data. ","ed8bcfd2":"The test data is missing values in Fare column. Lets deal with that now.","9f26df26":"Since mean age in both datasets is near 30, we'll replace null values with 30.","404a243e":"**MODELLING and PREDICTION**\n\nThe following models have been used:\n* Logistic Regression\n\n* Random Forest Classifier\n\n* Decision Tree Classifier","6bd14c01":"from the above table, we infer that, \n* Survival chances of passengers from Class 1 was the highest. Interms of numbers also, Class 1 passengers survived most.\n* Despite Class3 having the most passengers their **Survival rate** was the lowest, though the numbers were high compared to Class2 !\n* Above can be corroborated from the bar graph above.\n\n* **Hence we can say, the PClass in which passengers travelled had a role to play in their Survival**","a6f772ca":"We can see that, 'Name' 'Cabin' and 'Ticket' columns are random, and have no impact on Survival of passenger as other features had. Seriously - \"Whats in a name?!\"\n\n* Hence, we shall remove these columns (done below), as they don't contribute to our analysis.\n* Also, we saw earlier, SibSp, Parch didn't have any effect on Survival of a passenger, so remove those too.\n* Note : We will remove Passenger Id from the training data set also.","14ebaa6c":"**Now, lets map certain features on survival and see how they relate to it and understand their PLOTS**","9fc2e98b":"We have successfully dealt with NULL values. "}}