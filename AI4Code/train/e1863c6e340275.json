{"cell_type":{"edfb44d4":"code","59dfa3b1":"code","2315bdfe":"code","a02be1db":"code","09aee878":"markdown","02638d62":"markdown","335fdf50":"markdown","e77fe3b2":"markdown","d1535352":"markdown","b851efa3":"markdown","f7d3f970":"markdown","f553fc55":"markdown","af9f97f5":"markdown"},"source":{"edfb44d4":"!pip install openpyxl\n!pip install wordcloud\n!pip install calmap\n\n# Modules for data processing\nimport pandas as pd\nimport numpy as np\nimport os\nimport re\nimport sys\nfrom datetime import datetime\nimport calendar\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# REFERENCE: https:\/\/www.kaggle.com\/dmitryuarov\/eda-covid-19-impact-on-digital-learning\nSTATE_ABBR = {\n    'Alabama': 'AL', 'Alaska': 'AK', 'American Samoa': 'AS', 'Arizona': 'AZ', 'Arkansas': 'AR',\n    'California': 'CA', 'Colorado': 'CO', 'Connecticut': 'CT', 'Delaware': 'DE', 'District of Columbia': 'DC', 'District Of Columbia': 'DC',\n    'Florida': 'FL', 'Georgia': 'GA', 'Guam': 'GU', 'Hawaii': 'HI', 'Idaho': 'ID', 'Illinois': 'IL',\n    'Indiana': 'IN', 'Iowa': 'IA', 'Kansas': 'KS', 'Kentucky': 'KY', 'Louisiana': 'LA', 'Maine': 'ME',\n    'Maryland': 'MD', 'Massachusetts': 'MA', 'Michigan': 'MI', 'Minnesota': 'MN', 'Mississippi': 'MS',\n    'Missouri': 'MO', 'Montana': 'MT', 'Nebraska': 'NE', 'Nevada': 'NV', 'New Hampshire': 'NH',\n    'New Jersey': 'NJ', 'New Mexico': 'NM', 'New York': 'NY', 'North Carolina': 'NC', 'North Dakota': 'ND',\n    'Northern Mariana Islands':'MP', 'Ohio': 'OH', 'Oklahoma': 'OK', 'Oregon': 'OR', 'Pennsylvania': 'PA',\n    'Puerto Rico': 'PR', 'Rhode Island': 'RI', 'South Carolina': 'SC', 'South Dakota': 'SD', 'Tennessee': 'TN',\n    'Texas': 'TX', 'Utah': 'UT', 'Vermont': 'VT', 'Virgin Islands': 'VI', 'Virginia': 'VA', 'Washington': 'WA',\n    'West Virginia': 'WV', 'Wisconsin': 'WI', 'Wyoming': 'WY'\n}\nSTATE_NAME = dict([(y, x) for x, y in STATE_ABBR.items()])\n\n# ########################################################################\n# ### analyze_dataset\n# ########################################################################\n# #\n# # Simple function to help quickly analyze information & usability of\n# # a dataset. Provides information about shape, null values, unique\n# # values & basic statistical features.\n# #\n# # Inputs:\n# #   1.  df_path (string) -> Dataset path (if available)\n# #   2.  df (pandas dataframe) -> Dataset (if available)\n# #   3.  direct_df (boolean) -> Whether dataset path or dataset is\n# #       being provided\n# #   4.  processing_func (function) -> If dataset needs to be processed\n# #       before analyzing\n# #   5.  Other arguments for pd.read_csv(...) if dataset path is being\n# #       provided\n# #\n# # Return:   Either dataframe itself (if path provided) or head of\n# #           dataframe (if dataframe provided)\n# #\n# ########################################################################\n\n# def analyze_dataset(df_path = None, df = None, direct_df = False, processing_func = lambda x: x, **read_csv_args):\n    \n#     if(direct_df == False):\n#         df = pd.read_csv(df_path, **read_csv_args)\n#     df = processing_func(df)\n    \n#     num_rows, num_cols = df.shape\n#     dtypes = dict(df.dtypes.items())\n#     print(\"*****************\")\n#     print(\"Basic Info:\")\n#     print(\"*****************\\n\")\n#     print(f\"Shape of Dataset: {num_rows} rows, {num_cols} cols\")\n#     print(\"Columns:\")\n#     for col_idx, col in enumerate(df.columns):\n#         print(f\"\\t{col_idx+1}. {col}\\n\\t\\t\\t\\t\\t\\t\\t\\t{dtypes[col]}\")\n    \n#     print(\"\\n\\n\\n*****************\")\n#     print(\"Null Values:\")\n#     print(\"*****************\\n\")\n#     nulls = pd.isnull(df).sum()\n#     print(f\"Total Nulls: {nulls.sum()}\")\n#     nulls = nulls[nulls > 0]\n#     nulls = list(sorted(nulls.items(), key = lambda x: x[1], reverse = True))\n#     print(\"Columns with missing values:\")\n#     for col_idx, (col_name, col_missin_num) in enumerate(nulls):\n#         print(f\"\\t{col_idx + 1}. {col_name}\\n\\t\\t\\t\\t\\t\\t\\t\\t{col_missin_num} missing ({col_missin_num \/ num_rows * 100:.1f}%)\")\n    \n#     print(\"\\n\\n\\n*****************\")\n#     print(\"Column-specific:\")\n#     print(\"*****************\\n\")\n#     print(\"Unique values in columns:\")\n#     idx = 1\n#     for col in df.columns:\n#         nunique = df[col].nunique()\n#         if(nunique < 10):\n#             unique_vals = [\"'\" + str(x) + \"'\" for x in df[col].unique()]\n#             print(f\"{idx}. {col} has {nunique} unique values\")\n#             idx += 1\n#             print(f\"\\t[ {', '.join(unique_vals)} ]\")\n#     print(\"\\n\\nStatistical Features:\")\n#     print(df.describe())\n    \n#     print(\"\\n\\n\")\n#     if(direct_df == True):\n#         return df.head()\n#     else:\n#         return df\n\n########################################################################\n### load_main_dataset\n########################################################################\n#\n# Function to load the main processed & merged dataset for\n# engagement. Has options to merge selected datasets. The processing\n# has been done separately in another function `process_main_dataset`\n#\n# Inputs:\n#   1.  whether_merge_district (boolean) -> Whether to merge districts\n#       data\n#   2.  whether_merge_products (boolean) -> Whether to merge products\n#       data\n#   3.  whether_merge_dates (boolean) -> Whether to merge dates data\n#\n# Return:   Engagement data merged with other relevant datasets\n#\n########################################################################\n\ndef load_main_dataset(whether_merge_districts = True, whether_merge_products = True, whether_merge_dates = True):\n    def reduce_dtype_size(df):\n        numeric_cols = [x for x in df.columns if (df[x].dtype != object) & ('datetime' not in str(df[x].dtype))]\n        for numeric_col in numeric_cols:\n            if('float' in str(df[numeric_col].dtype)):\n                df[numeric_col] = pd.to_numeric(df[numeric_col], downcast = 'float')\n            elif(('uint' in str(df[numeric_col].dtype)) | ('bool' in str(df[numeric_col].dtype))):\n                df[numeric_col] = pd.to_numeric(df[numeric_col], downcast = 'unsigned')\n            else:\n                df[numeric_col] = pd.to_numeric(df[numeric_col], downcast = 'signed')\n        return df\n    \n    def merge_districts_data(engagement_data):\n        districts_data = pd.read_csv('\/kaggle\/input\/learnplatform-analysis-data\/districts_data.csv')\n        districts_data = reduce_dtype_size(districts_data)\n        merged_engagement_data = pd.merge(engagement_data, districts_data, how = 'left', on = 'district_id')\n        return merged_engagement_data\n\n    def merge_products_data(engagement_data):\n        products_data = pd.read_csv('\/kaggle\/input\/learnplatform-analysis-data\/products_data.csv')\n        products_data = reduce_dtype_size(products_data)\n        merged_engagement_data = pd.merge(engagement_data, products_data, how = 'left', left_on = 'lp_id', right_on = 'LP ID')\n        merged_engagement_data = merged_engagement_data.drop('LP ID', axis = 1)\n        return merged_engagement_data\n\n    def merge_dates_data(engagement_data):\n        dates_data = pd.read_csv('\/kaggle\/input\/learnplatform-analysis-data\/dates_data.csv', parse_dates = ['date'])\n        dates_data = reduce_dtype_size(dates_data)\n        merged_engagement_data = pd.merge(engagement_data, dates_data, how = 'left', left_on = 'time', right_on = 'date')\n        merged_engagement_data = merged_engagement_data.drop('date', axis = 1)\n        return merged_engagement_data\n\n    engagement_data = pd.read_csv('\/kaggle\/input\/learnplatform-analysis-data\/engagement_data.csv', parse_dates = ['time'])\n    engagement_data = reduce_dtype_size(engagement_data)\n    if(whether_merge_districts == True):\n        engagement_data = merge_districts_data(engagement_data)\n    if(whether_merge_products == True):\n        engagement_data = merge_products_data(engagement_data)\n    if(whether_merge_dates == True):\n        engagement_data = merge_dates_data(engagement_data)\n    \n    return engagement_data\n\n\n# ########################################################################\n# ### process_main_dataset\n# ########################################################################\n# #\n# # Function to process the main processed & save it for\n# # loading later from another function `load_main_dataset`.\n# #\n# # Inputs:\n# #   1.  whether_load_url_html_data (boolean) -> Whether to process\n# #       and save URL's HTML data\n# #\n# # Return:   None\n# #\n# ########################################################################\n\n# def process_main_dataset(whether_load_url_html_data = False):\n#     def get_all_na_idx(df):\n#         all_na_idx = df.isnull().all(axis=1)\n#         return all_na_idx[all_na_idx == True].keys()\n\n#     def add_dummys(df, dummy_cols, remove_orig_dummy_cols = False):\n#         dummy_df = df[dummy_cols]\n#         dummy_df = pd.get_dummies(dummy_df)\n        \n#         df = pd.concat([df, dummy_df], axis = 1)\n#         if(remove_orig_dummy_cols == True):\n#             df = df.drop(dummy_cols, axis = 1)\n        \n#         return df\n\n#     def reduce_dtype_size(df):\n#         numeric_cols = [x for x in df.columns if (df[x].dtype != object) & ('datetime' not in str(df[x].dtype))]\n#         for numeric_col in numeric_cols:\n#             if('float' in str(df[numeric_col].dtype)):\n#                 df[numeric_col] = pd.to_numeric(df[numeric_col], downcast = 'float')\n#             elif(('uint' in str(df[numeric_col].dtype)) | ('bool' in str(df[numeric_col].dtype))):\n#                 df[numeric_col] = pd.to_numeric(df[numeric_col], downcast = 'unsigned')\n#             else:\n#                 df[numeric_col] = pd.to_numeric(df[numeric_col], downcast = 'signed')\n#         return df\n    \n#     def load_districts_data():\n        \n#         def districts_data_preprocessing(districts_data):\n            \n#             def process_lower_upper_bounds(df_series):\n#                 processed_lower_series = []\n#                 processed_upper_series = []\n                \n#                 for row in df_series:                \n#                     if(pd.isnull(row) == True):\n#                         processed_lower_series.append(row)\n#                         processed_upper_series.append(row)\n#                     else:\n#                         assert(len(row[1:-1].split(', ')) == 2)\n#                         lower_val, upper_val = row[1:-1].split(', ')\n#                         lower_val = float(lower_val)\n#                         upper_val = float(upper_val)\n#                         processed_lower_series.append(lower_val)\n#                         processed_upper_series.append(upper_val)\n                \n#                 return processed_lower_series, processed_upper_series\n            \n#             for col in ['pct_black\/hispanic', 'pct_free\/reduced', 'county_connections_ratio', 'pp_total_raw']:\n#                 lower_series, upper_series = process_lower_upper_bounds(districts_data[col])\n#                 districts_data[col + '_lower_bound'] = pd.Series(lower_series, index = districts_data.index)\n#                 districts_data[col + '_upper_bound'] = pd.Series(upper_series, index = districts_data.index)\n#                 districts_data[col + '_bound_avg'] = pd.Series(np.add(lower_series, upper_series) \/ 2.0, index = districts_data.index)\n            \n#             districts_data = districts_data.drop(['pct_black\/hispanic', 'pct_free\/reduced', 'county_connections_ratio', 'pp_total_raw'], axis = 1)\n#             return districts_data\n        \n#         districts_data = pd.read_csv('\/kaggle\/input\/learnplatform-covid19-impact-on-digital-learning\/districts_info.csv')\n        \n#         districts_data = districts_data_preprocessing(districts_data)\n#         districts_data = districts_data.drop(get_all_na_idx(districts_data.drop('district_id', axis = 1))).reset_index(drop = True)\n#         districts_data = add_dummys(districts_data, ['locale'], remove_orig_dummy_cols = False)\n        \n#         #all_states = districts_data['state'].unique()\n#         #district_id_state_map = dict(districts_data[['district_id', 'state']].values)\n#         #state_district_id_map = dict([(x, [y for y in district_id_state_map if district_id_state_map[y] == x]) for x in all_states])\n#         #districts_data = districts_data.drop('state', axis = 1)\n        \n#         districts_data = reduce_dtype_size(districts_data)\n#         return districts_data\n\n#     districts_data = load_districts_data()\n    \n#     # URL Information Extraction\n#     #   For July 2021\n#     #   Using similarweb.com\n#     #   Avg Duration - in seconds\n#     #   Total Visits - in 1000s\n#     def load_url_html_data():\n#         url_html_dict = {}\n\n#         all_html_content = \"\"\n#         with open(f'.\/Data\/url_info\/combined_url_info_data.txt', 'r') as html_file:\n#             all_html_content = html_file.read()\n\n#         tot_num_files = len([x for x in all_html_content.split('---') if len(x.strip()) != 0])\n#         print(f\"Total No. of files: {tot_num_files}\\n\\n\")\n\n#         for html_content_idx, html_content in enumerate(all_html_content.split('---')):\n            \n#             html_content = html_content.strip()\n#             if(len(html_content) == 0):\n#                 print(\"ERROR: URL not found\")\n#                 sys.exit(\"\")\n            \n#             url_name = html_content.split('<')[0].strip()\n#             html_content = '<'.join(html_content.split('<')[1:])\n#             print(f\"{html_content_idx + 1}. File: {url_name}\\n\")\n#             if(url_name in url_html_dict):\n#                 print(\"ERROR: Name already exists\")\n#                 sys.exit(\"\")\n#             url_html_dict[url_name] = {}\n            \n#             # Global Rank\n#             global_rank = re.findall('\\\"GlobalRank\":\\[\\d+,\\d+,-?\\d+,\\d+\\]', html_content)\n#             if(len(global_rank) != 1):\n#                 print(\"ERROR: Global Rank\")\n#                 print(global_rank)\n#                 sys.exit(\"\")\n#             global_rank = int(global_rank[0].split('[')[1].split(',')[0])\n#             url_html_dict[url_name]['global_rank'] = global_rank\n\n#             # Country\n#             if('<img class=\"websiteRanks-titleIconImg\" src=\"\/images\/flags-svg\/flag-icon-us.svg\">' in html_content):\n#                 country = 'USA'\n#             else:\n#                 print(\"Country: Not USA!\")\n#                 country = 'Not_USA'\n#             url_html_dict[url_name]['country'] = country\n            \n#             # Country Rank\n#             country_rank = re.findall('\\\"CountryRanks\":\\{\"\\d+\":\\[\\d+,\\d+,-?\\d+,\\d+\\]\\}', html_content)\n#             if(len(country_rank) != 1):\n#                 print(\"ERROR: Country Rank\")\n#                 print(country_rank)\n#                 sys.exit(\"\")\n#             country_rank = int(country_rank[0].split('[')[1].split(',')[0])\n#             url_html_dict[url_name]['country_rank'] = country_rank\n            \n#             # Category\n#             category = re.findall('<a class=\"websiteRanks-nameText\" data-analytics-category=\"Internal Link\" data-analytics-label=\"Category Rank\/.+\" href=\"\/top-websites\/category\/.+\" itemprop=\"significantLink\">.+<\/a>', html_content)\n#             if(len(category) != 1):\n#                 print(\"ERROR: Category\")\n#                 print(category)\n#                 url_html_dict[url_name]['main_category'] = np.nan\n#                 url_html_dict[url_name]['sub_category'] = np.nan\n#             else:\n#                 category = category[0].split('Category Rank\/')[1].split('\"')[0]\n#                 main_category = category.split('\/')[0]\n#                 url_html_dict[url_name]['main_category'] = main_category\n#                 if(len(category.split('\/')) != 1):\n#                     sub_category = category.split('\/')[-1]\n#                 else:\n#                     sub_category = \"\"\n#                 url_html_dict[url_name]['sub_category'] = sub_category\n\n#             # Category Rank\n#             category_rank = re.findall('\\\"CategoryRank\\\":\\[\\d+,\\d+,-?\\d+,\\d+\\]', html_content)\n#             if(len(category_rank) != 1):\n#                 print(\"ERROR: Category Rank\")\n#                 print(category_rank)\n#                 sys.exit(\"\")\n#             category_rank = int(category_rank[0].split('[')[1].split(',')[0])\n#             url_html_dict[url_name]['category_rank'] = category_rank\n\n#             # Total Visits\n#             total_visits = re.findall('<span class=\"engagementInfo-valueNumber js-countValue\">.*\\d+[KMB]<\/span>', html_content)\n#             if(len(total_visits) != 1):\n#                 print(\"ERROR: Total Visits\")\n#                 print(total_visits)\n#                 url_html_dict[url_name]['total_visits'] = np.nan\n#             else:\n#                 total_visits = total_visits[0].split('>')[1].split('<')[0]\n#                 units = total_visits[-1]\n#                 total_visits = float(''.join([x for x in total_visits if x.isdigit()]))\n#                 if(units == 'K'):\n#                     total_visits = total_visits * 1\n#                 elif(units == 'M'):\n#                     total_visits = total_visits * 1000\n#                 elif(units == 'B'):\n#                     total_visits = total_visits * 1000000\n#                 url_html_dict[url_name]['total_visits'] = total_visits\n\n#             # Avg Duration\n#             avg_duration = re.findall('<span class=\"engagementInfo-valueNumber js-countValue\">\\d+:\\d+:\\d+<\/span>', html_content)\n#             if(len(avg_duration) != 1):\n#                 print(\"ERROR: Avg Duration\")\n#                 print(avg_duration)\n#                 url_html_dict[url_name]['avg_duration'] = np.nan\n#             else:\n#                 avg_duration = avg_duration[0].split('>')[1].split('<')[0]\n#                 hr_val, min_val, sec_val = avg_duration.split(':')\n#                 avg_duration = 3600 * int(hr_val) + 60 * int(min_val) + int(sec_val)\n#                 url_html_dict[url_name]['avg_duration'] = avg_duration\n\n#             # Page Visits\n#             page_visits = re.findall('<span class=\"engagementInfo-valueNumber js-countValue\">\\d+\\.\\d+<\/span>', html_content)\n#             if(len(page_visits) != 1):\n#                 print(\"ERROR: Page Visits\")\n#                 print(page_visits)\n#                 url_html_dict[url_name]['page_visits'] = np.nan\n#             else:\n#                 page_visits = float(page_visits[0].split('>')[1].split('<')[0])\n#                 url_html_dict[url_name]['page_visits'] = page_visits\n\n#             # Bounce Rate\n#             bounce_rate = re.findall('<span class=\"engagementInfo-valueNumber js-countValue\">\\d+.\\d+%<\/span>', html_content)\n#             if(len(bounce_rate) != 1):\n#                 print(\"ERROR: Bounce Rate\")\n#                 print(bounce_rate)\n#                 url_html_dict[url_name]['bounce_rate'] = np.nan\n#             else:\n#                 bounce_rate = float(bounce_rate[0].split('>')[1].split('%')[0])\n#                 url_html_dict[url_name]['bounce_rate'] = bounce_rate\n            \n#             # Description\n#             description = re.findall('<p itemprop=\"description\" class=\"websiteHeader-companyDescription js-companyDescription\">.+<\/p>', html_content)\n#             if(len(description) != 1):\n#                 url_html_dict[url_name]['description'] = np.nan\n#             else:\n#                 description = description[0].split('>')[1].split('<')[0]\n#                 url_html_dict[url_name]['description'] = description\n\n#         url_html_df = pd.DataFrame.from_dict(url_html_dict, orient = 'index').reset_index(drop = False)\n#         url_html_df.columns = ['URL'] + [*url_html_df.columns][1:]\n\n#         url_html_df['global_rank'] = url_html_df['global_rank'].replace({0: np.nan})\n#         url_html_df['country_rank'] = url_html_df['country_rank'].replace({0: np.nan})\n#         url_html_df['category_rank'] = url_html_df['category_rank'].replace({0: np.nan})\n\n#         additional_data = pd.read_csv('.\/Data\/url_info\/url_info_mobile.csv')\n#         url_html_df = pd.concat([url_html_df, additional_data], axis = 0)\n\n#         def find_subpage_level(url):\n#             level = url.split(':\/\/')[1]\n#             level = level.split('\/')\n#             level = [x for x in level if len(x.strip()) > 0]\n#             return len(level) - 1\n#         url_html_df['URL_subpage_level'] = url_html_df['URL'].apply(find_subpage_level)\n#         url_html_df['URL_subpage_visits'] = url_html_df.apply(lambda x: x['total_visits'] * (((100 - x['bounce_rate']) \/ 100) ** x['URL_subpage_level']), axis = 1)\n#         url_html_df['URL_page_duration'] = url_html_df['avg_duration'] \/ url_html_df['page_visits']\n#         url_html_df['URL_subpage_total_browsing_days'] = url_html_df['URL_subpage_visits'] * url_html_df['URL_page_duration'] \/ 60 \/ 60 \/ 24\n#         url_html_df['URL_subpage_avg_browsing_days'] = url_html_df['URL_subpage_total_browsing_days'] \/ 31\n#         url_html_df.to_csv('.\/Data\/url_info\/final_url_data.csv', index = False)\n    \n#     def load_products_data():\n    \n#         def products_data_preprocessing(products_data):\n\n#             products_data['Primary Category'] = products_data['Primary Essential Function'].apply(lambda x: x.split(' - ')[0] if pd.isna(x) == False else np.nan)\n#             products_data['Primary Category'] = products_data['Primary Category'].map({'LC': 'LC', 'CM': 'CM', 'SDO': 'SDO', 'LC\/CM\/SDO': 'Other'})\n#             products_data['Primary Essential Function'] = products_data['Primary Essential Function'].apply(lambda x: x.split(' - ')[1] if pd.isna(x) == False else np.nan)\n            \n#             def sector_map(sectors):\n                \n#                 sector_prek12 = []\n#                 sector_higher_ed = []\n#                 sector_corporate = []\n                \n#                 for sector in sectors:\n#                     if(pd.isna(sector) == True):\n#                         sector_prek12.append(0)\n#                         sector_higher_ed.append(0)\n#                         sector_corporate.append(0)\n#                     elif(sector == 'PreK-12'):\n#                         sector_prek12.append(1)\n#                         sector_higher_ed.append(0)\n#                         sector_corporate.append(0)\n#                     elif(sector == 'PreK-12; Higher Ed'):\n#                         sector_prek12.append(1)\n#                         sector_higher_ed.append(1)\n#                         sector_corporate.append(0)\n#                     elif(sector == 'PreK-12; Higher Ed; Corporate'):\n#                         sector_prek12.append(1)\n#                         sector_higher_ed.append(1)\n#                         sector_corporate.append(1)\n#                     elif(sector == 'Corporate'):\n#                         sector_prek12.append(0)\n#                         sector_higher_ed.append(0)\n#                         sector_corporate.append(1)\n#                     elif(sector == 'Higher Ed; Corporate'):\n#                         sector_prek12.append(0)\n#                         sector_higher_ed.append(1)\n#                         sector_corporate.append(1)\n#                     else:\n#                         print(f\"***\\nUnknown sector detected! {sector}\\n***\")\n                \n#                 return sector_prek12, sector_higher_ed, sector_corporate\n            \n#             sector_prek12, sector_higher_ed, sector_corporate = sector_map(products_data['Sector(s)'])\n#             products_data = products_data.assign(Sector_prek12 = sector_prek12)\n#             products_data = products_data.assign(Sector_higher_ed = sector_higher_ed)\n#             products_data = products_data.assign(Sector_corporate = sector_corporate)\n            \n#             products_data['Primary Essential Function'] = products_data['Primary Essential Function'].replace({\"Sites, Resources & References\": \"Sites, Resources & Reference\"})\n            \n#             # Correcting small mistakes\n#             products_data['URL'] = products_data['URL'].replace({'https:\/\/fligprid.com': 'https:\/\/flipgrid.com'})\n\n#             if(whether_load_url_html_data == True):\n#                 load_url_html_data()\n#             url_info_data = pd.read_csv('.\/Data\/url_info\/final_url_data.csv')\n#             url_info_data.columns = ['mainURL_' + x if x != 'URL' else x for x in url_info_data.columns]\n#             products_data = pd.merge(products_data, url_info_data, how = 'left', on = 'URL')\n            \n#             products_data['mainURL_country_rank'] = products_data.apply(lambda x: x if x['mainURL_country'] == 'USA' else np.nan, axis = 1)\n            \n#             return products_data\n        \n#         products_data = pd.read_csv('.\/Data\/learnplatform-covid19-impact-on-digital-learning\/products_info.csv')\n        \n#         products_data = products_data_preprocessing(products_data)\n#         products_data = add_dummys(products_data, ['Sector(s)', 'Primary Category', 'Primary Essential Function', 'mainURL_main_category'], remove_orig_dummy_cols = False)\n        \n#         products_data = reduce_dtype_size(products_data)\n#         return products_data\n\n#     products_data = load_products_data()\n\n#     # Not focusing on vacation dates since time-analysis is not priority\n\n#     def load_dates_data():\n#         days_2020 = pd.date_range(datetime(2020, 1, 1), datetime(2020, 12, 31))\n#         dates_data = pd.DataFrame.from_dict({'date': days_2020})\n        \n#         dates_data['month'] = dates_data['date'].apply(lambda x: x.month)\n#         dates_data['day'] = dates_data['date'].apply(lambda x: x.day)\n#         dates_data['day_of_week'] = dates_data['date'].apply(lambda x: calendar.day_name[x.weekday()])\n#         dates_data['is_weekend'] = dates_data['day_of_week'].apply(lambda x: 1 if (x == 'Saturday') | (x == 'Sunday') else 0)\n        \n#         us_holidays = pd.read_csv('.\/Data\/US Holiday Dates (2004-2021).csv', usecols = ['Date', 'Holiday'], parse_dates = ['Date'])\n#         dates_data = pd.merge(dates_data, us_holidays, how = 'left', left_on = 'date', right_on = 'Date')\n#         dates_data = dates_data.drop('Date', axis = 1)\n#         dates_data['is_holiday'] = dates_data['Holiday'].apply(lambda x: 0 if pd.isnull(x) == True else 1)\n        \n#         dates_data = add_dummys(dates_data, ['day_of_week', 'Holiday'], remove_orig_dummy_cols = False)\n#         dates_data = reduce_dtype_size(dates_data)\n#         return dates_data\n\n#     dates_data = load_dates_data()\n\n#     def load_engagement_data():\n\n#         engagement_data = pd.DataFrame()\n#         districts = []\n        \n#         for x in os.listdir('.\/Data\/learnplatform-covid19-impact-on-digital-learning\/engagement_data\/'):\n#             data_x = pd.read_csv(f'.\/Data\/learnplatform-covid19-impact-on-digital-learning\/engagement_data\/{x}', parse_dates = ['time'])\n#             engagement_data = pd.concat([engagement_data, data_x], axis = 0)\n#             districts.extend([int(x.split('.')[0])] * data_x.shape[0])\n        \n#         engagement_data['district_id'] = pd.Series(districts, index = engagement_data.index)\n        \n#         top_products_id = list(products_data['LP ID'].unique())\n#         districts_id = list(districts_data['district_id'].unique())\n#         engagement_data = engagement_data[engagement_data['lp_id'].isin(top_products_id)]\n#         engagement_data = engagement_data[engagement_data['district_id'].isin(districts_id)]\n        \n#         same_url_map = {\n#             33562: 75206,\n#             87841: 35971\n#         }\n#         engagement_data['lp_id'] = engagement_data['lp_id'].replace(same_url_map)\n#         engagement_data = engagement_data.groupby(['time', 'lp_id', 'district_id'])[['pct_access', 'engagement_index']].aggregate(np.nansum).reset_index()\n        \n#         engagement_data = reduce_dtype_size(engagement_data)\n#         return engagement_data\n\n#     engagement_data = load_engagement_data()\n\n#     # # Saving all datasets\n#     districts_data.to_csv('.\/Data\/Processed_Dataset\/districts_data.csv', index = False)\n#     products_data.to_csv('.\/Data\/Processed_Dataset\/products_data.csv', index = False)\n#     dates_data.to_csv('.\/Data\/Processed_Dataset\/dates_data.csv', index = False)\n#     engagement_data.to_csv('.\/Data\/Processed_Dataset\/engagement_data.csv', index = False)\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport plotly.express as px\n\nCOLOR_WHITE = '#F8F1FF'\nCOLOR_BLACK = '#231942'\nCOLOR_DARK_BLUE = '#156BB7'\nCOLOR_LIGHT_BLUE = '#63D1DF'\nCOLOR_GREEN = '#30DB8D'\nCOLOR_DARK_GREEN = '#0DAB6C'\nCOLOR_ORANGE = '#FBAB60'\nCOLOR_YELLOW = '#F8E16C'\nCOLOR_RED = '#DA4167'\n\nPLOT_THEME_LIGHT = {\n    'text': COLOR_BLACK,\n    'axis': COLOR_BLACK,\n    'subtitle': COLOR_DARK_BLUE,\n    'color+1': COLOR_DARK_GREEN,\n    'color+2': COLOR_YELLOW,\n    'color+3': COLOR_ORANGE,\n    'color+4': COLOR_DARK_BLUE,\n    'color-1': COLOR_RED,\n    'bg': COLOR_LIGHT_BLUE,\n    'inv': COLOR_WHITE,\n    'color+1_lower': '#064B30',\n    'color+1_higher': '#2EEFA2',\n    'gray': '#676076',\n}\nPLOT_THEME_LIGHT['groups'] = [PLOT_THEME_LIGHT[x] for x in ['color+1', 'color-1', 'color+3', 'color+4', 'color+2']]\n\nPLOT_THEME_DARK = {\n    'text': COLOR_WHITE,\n    'axis': COLOR_WHITE,\n    'subtitle': COLOR_LIGHT_BLUE,\n    'color+1': COLOR_GREEN,\n    'color+2': COLOR_YELLOW,\n    'color+3': COLOR_ORANGE,\n    'color+4': COLOR_LIGHT_BLUE,\n    'color-1': COLOR_RED,\n    'bg': COLOR_DARK_BLUE,\n    'inv': COLOR_BLACK,\n    'color+1_lower': '#188B57',\n    'color+1_higher': '#86EABD',\n    'gray': '#D6CFDB',\n}\nPLOT_THEME_DARK['groups'] = [PLOT_THEME_DARK[x] for x in ['color+1', 'color-1', 'color+3', 'color+4', 'color+2']]\n\ndef create_fig(nrows = 1, ncols = 1, width = 10, height = 5):\n    fig, ax = plt.subplots(nrows, ncols, figsize = (width, height))\n    return fig, ax\n\ndef remove_spines(ax, theme = {}):\n    ax.spines['top'].set_visible(False)\n    ax.spines['right'].set_visible(False)\n    ax.spines['bottom'].set_color(theme['axis'])\n    ax.spines['left'].set_color(theme['axis'])\n    return ax\n\ndef remove_all_spines(ax):\n    ax.spines['top'].set_visible(False)\n    ax.spines['right'].set_visible(False)\n    ax.spines['bottom'].set_visible(False)\n    ax.spines['left'].set_visible(False)\n    return ax\n\ndef set_titles_and_labels(fig, ax, fig_title = \"\", title = \"\", xlabel = \"\", ylabel = \"\", theme = {}):\n    fig.suptitle(fig_title, fontsize = 30, color = theme['text'])\n    ax.set_title(title, fontsize = 20, color = theme['subtitle'])\n    ax.set_xlabel(xlabel, fontsize = 15, color = theme['text'])\n    ax.set_ylabel(ylabel, fontsize = 15, color = theme['text'])\n    return fig, ax\n\ndef set_ticks(ax, theme):\n    ax.tick_params(axis = 'x', colors = theme['axis'])\n    ax.tick_params(axis = 'y', colors = theme['axis'])\n    return ax\n\ndef set_xticklabels(ax, labels, rotate_x = 0, theme = {}):\n    ax.set_xticks(np.arange(len(labels)))\n    ax.set_xticklabels(labels, color = theme['text'], rotation = rotate_x)\n    return ax\n\ndef set_yticklabels(ax, labels, rotate_y = 0, theme = {}):\n    ax.set_yticks(np.arange(len(labels)))\n    ax.set_yticklabels(labels, color = theme['text'], rotation = rotate_y)\n    return ax\n\ndef set_bg(fig, ax, theme):\n    fig.set_facecolor(theme['bg'])\n    ax.set_facecolor(theme['bg'])\n    return fig, ax\n\ndef select_theme(theme):\n    if(theme == 'DARK'):\n        return PLOT_THEME_DARK\n    else:\n        return PLOT_THEME_LIGHT\n\ndef set_legend(ax, theme):\n    ax.legend(loc = 'best')\n    return ax\n\ndef plot_decoration():\n    return \"\"\"\n    fig, ax = set_bg(fig, ax, theme); ax = set_ticks(ax, theme); ax = remove_spines(ax, theme); fig, ax = set_titles_and_labels(fig, ax, suptitle, title, xlabel, ylabel, theme);\n    \"\"\".strip()\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndef plot_lineplot(x_vals, y_vals, width = 15, height = 7, labels = [], suptitle = \"Lineplot\", title = \"Demo\", xlabel = \"\", ylabel = \"\", theme = 'DARK'):\n    theme = select_theme(theme)\n    fig, ax = create_fig(1, 1, width, height)\n    if(len(y_vals) > len(theme['groups'])):\n        group_colors = [(np.random.random(), np.random.random(), np.random.random()) for _ in range(len(y_vals))]\n    else:\n        group_colors = theme['groups'][:len(y_vals)]\n    for y_val_idx, y_val in enumerate(y_vals):\n        if(len(labels) == 0):\n            ax.plot(x_vals, y_val, lw = 3, color = group_colors[y_val_idx], label = f'line #{y_val_idx + 1}')\n        else:\n            ax.plot(x_vals, y_val, lw = 3, color = group_colors[y_val_idx], label = labels[y_val_idx])\n    ax = set_legend(ax, theme)\n    exec(plot_decoration())\n    plt.show()\n\ndef plot_barplot(x_names, y_vals, cats = [], width = 15, height = 7, suptitle = \"Barplot\", title = \"Demo\", xlabel = '', ylabel = '', theme = 'DARK', rotate_x = 0, rotate_y = 0):\n    theme = select_theme(theme)\n    fig, ax = create_fig(1, 1, width, height)\n    x_vals = np.arange(len(x_names))\n    if(len(cats) > 0):\n        uniq_cats = list(sorted(pd.Series(cats).unique()))\n        if(len(uniq_cats) > len(theme['groups'])):\n            group_colors = [(np.random.random(), np.random.random(), np.random.random()) for _ in range(len(uniq_cats))]\n        else:\n            group_colors = theme['groups'][:len(uniq_cats)]\n        group_colors = [group_colors[uniq_cats.index(x)] for x in cats]\n    else:\n        if(len(y_vals) > len(theme['groups'])):\n            group_colors = [(np.random.random(), np.random.random(), np.random.random()) for _ in range(len(y_vals))]\n        else:\n            group_colors = theme['groups'][:len(y_vals)]\n    ax.bar(x_vals, y_vals, color = group_colors)\n    ax = set_xticklabels(ax, x_names, rotate_x = rotate_x, theme = theme)\n    exec(plot_decoration())\n    plt.show()\n\ndef plot_scatterplot(x_vals, y_vals, cats = [1], width = 15, height = 7, suptitle = \"Scatterplot\", title = 'Demo', xlabel = '', ylabel = '', theme = 'DARK', annotate = False, annotate_texts = []):\n    theme = select_theme(theme)\n    fig, ax = create_fig(1, 1, width, height)\n    no_cats_passed = False\n    if(len(cats) == 1):\n        cats = np.ones(len(x_vals))\n        no_cats_passed = True\n    uniq_cats = pd.Series(cats).unique()\n    if(len(uniq_cats) > len(theme['groups'])):\n        group_colors = [(np.random.random(), np.random.random(), np.random.random()) for _ in range(len(uniq_cats))]\n    else:\n        group_colors = theme['groups'][:len(uniq_cats)]\n    for cat_idx, cat in enumerate(uniq_cats):\n        ax.scatter(x_vals[cats == cat], y_vals[cats == cat], color = group_colors[cat_idx], label = cat)\n    if(annotate == True):\n        for idx in range(len(x_vals)):\n            ax.annotate(annotate_texts[idx], (x_vals[idx], y_vals[idx]), color = theme['text'])\n    if(no_cats_passed == False):\n        ax = set_legend(ax, theme)\n    exec(plot_decoration())\n    plt.show()\n\ndef plot_us_map(state_vals_df, title, val_col, val_label, range_min_val = 0, range_max_val = 1, theme = 'DARK', state_col = 'STATE_ABBR'):\n    \n    theme = select_theme(theme)\n\n    layout = dict(\n        font_family = 'Source Sans Pro',\n        font_color = theme['text'],\n        title_text = title,\n        # To change\n        title_font = dict(\n            family = \"Source Sans Pro\",\n            size = 25,\n            color = theme['axis']\n        ),\n        geo_scope = 'usa',\n        paper_bgcolor = theme['bg'],\n        geo_bgcolor = theme['bg'],\n        geo = dict(\n            landcolor = theme['inv'],\n            subunitcolor = theme['gray'],\n            lakecolor = theme['bg'],\n        ),\n    )\n\n    fig = px.choropleth(\n        state_vals_df,\n        locations = state_col,\n        color = val_col,\n        color_continuous_scale = [theme['color+1_lower'], theme['color+1_higher']],\n        range_color = (range_min_val, range_max_val),\n        locationmode = \"USA-states\",\n        labels = {val_col : val_label, state_col: 'State'},\n    )\n\n    fig.update_layout(layout)\n    fig.update_layout(margin = {\"r\": 0, \"l\": 0, \"b\": 15})\n    fig.show()\n    \nDATA = load_main_dataset()","59dfa3b1":"# Video : 640 x 350\nfrom IPython.display import HTML\ndisplay(HTML('<div align=\"center\"><iframe width=\"640\" height=\"350\" src=\"https:\/\/www.youtube.com\/embed\/Fy19PNEXe1M\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; encrypted-media; gyroscope\"><\/iframe><\/div>'))","2315bdfe":"import numpy as np\nimport pandas as pd\nimport os\nimport re\nimport sys\nimport time\n\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nimport calmap\n\nengagement_data = DATA.groupby('time')[['pct_access', 'engagement_index']].aggregate(np.nanmean)\n\nfig, ax = plt.subplots(1, 1, figsize=(16,10), dpi= 80)\nax = calmap.yearplot(engagement_data['pct_access'], year = 2020, cmap = 'Blues', ax = ax)\nax.set_title(\"Calendar Map - pct_access\", fontsize = 20)\nax.set_ylabel(\"2020\", fontsize = 15)\nplt.show()","a02be1db":"engagement_data = DATA.groupby(['state', 'month'])[['pct_access', 'engagement_index']].aggregate(np.nanmean).reset_index(drop = False)\nengagement_data['state'] = engagement_data['state'].replace(STATE_ABBR)\n\nstate_vals_df = engagement_data.copy()\ntitle = \"Student Engagement - Statewise Timeline\"\nval_col = \"pct_access\"\nval_label = \"% Access\"\nrange_min_val = 0\nrange_max_val = 1\ntheme = 'DARK'\n\n# state_vals_df = covid19_severity_data\n# title = \"Covid Severity\"\n# val_col = \"ConfirmedCases\"\n# val_label = \"Confirmed Cases\"\n# range_min_val = 1500\n# range_max_val = 2500000\n# theme = 'DARK'\n\nstate_vals_df['month'] = state_vals_df['month'].apply(lambda x: ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec'][x - 1])\n\ntheme = select_theme(theme)\n\nlayout = dict(\n    font_family = 'Source Sans Pro',\n    font_color = theme['text'],\n    title_text = title,\n    # To change\n    title_font = dict(\n        family = \"Source Sans Pro\",\n        size = 25,\n        color = theme['axis']\n    ),\n    geo_scope = 'usa',\n    paper_bgcolor = theme['bg'],\n    geo_bgcolor = theme['bg'],\n    geo = dict(\n        landcolor = theme['inv'],\n        subunitcolor = theme['gray'],\n        lakecolor = theme['bg'],\n    ),\n)\n\nfig = px.choropleth(\n    state_vals_df,\n    locations = 'state',\n    color = val_col,\n    color_continuous_scale = [theme['color-1'], theme['color+1_higher']],\n    range_color = (range_min_val, range_max_val),\n    locationmode = \"USA-states\",\n    labels = {val_col : val_label, 'state': 'State'},\n    animation_frame = 'month'\n)\n\nfig.update_layout(layout)\nfig.update_layout(margin = {\"r\": 0, \"l\": 0, \"b\": 15})\nfig.show()","09aee878":"<div style=\"background-color: #F8F1FF; padding: 20px 50px;\">\n<span style=\"color:#156BB7;\">\n<font size='+2.5'>\n    <b>7:30 AM:<\/b>\n<\/font>\n<br>\n<br>\n<font size='+1.5'>\n    Oh no,\" Grace said coming out of the bathroom with a toothbrush in her mouth. Still in her Pokemon night gown, her eyes were barely open and her hair was frizzier than ever. \"<b>Maths<\/b> is the first class today,\" she sighed.\n    <br><br>\n    \"What's wrong with Maths?\" I asked.\n<br><br>\n    \"It's pointless!\" she replied with a visible irritation on her face. \"I want to be an actress,\" she continued. \"Or a photographer, or maybe even a blogger, but you don't need to learn Maths for that! Then why should I learn Maths?\"\n<\/font>\n<\/span>\n<\/div>","02638d62":"<div align='center'>\n    <font size='+2' color='#75D345'>\n        <a style=\"background-color:#63D1DF; padding: 15px;\" href='https:\/\/www.kaggle.com\/sakshatrao\/a-day-of-digital-learning-part-2-morning-mania' target=\"_blank\">Next: <b>Morning Mania<\/b><\/a>\n    <\/font>\n<\/div>","335fdf50":"<div>\n<font size='+1'>\n    The Student Engagement data is also provided for different counties. However, the county names have been de-identified, making it extremely difficult to work with them. Hence, much of my analysis will be done at the state-level. Maps like above help in understanding the distribution of the Student Engagement data over the different states of the USA. Adding a timeline slider helps in providing a bigger picture with respect to both state & time.\n    <br><br>\n    For more information about the provided data, visit <a href=https:\/\/www.kaggle.com\/c\/learnplatform-covid19-impact-on-digital-learning\/data>the official competition page<\/a>.\n<\/font>\n<\/div>","e77fe3b2":"<div align='center'>\n    <font size='+3' color='#75D345'>\n        <b>A Day of Digital Learning<\/b>\n    <\/font>\n    <br>\n    <font size='+2.5' color='#FBAB60'>\n        <b>Part 1: Wake Up!<\/b>\n    <\/font>\n    <br>\n    <br>\n    <a href=\"https:\/\/www.kaggle.com\/sakshatrao\/a-day-of-digital-learning-part-1-wake-up\">\n        <font size='+1'>\n            <b>&#9202; Wake Up!<\/b>\n        <\/font>\n    <\/a>\n    &emsp;|&emsp;\n    <a href=\"https:\/\/www.kaggle.com\/sakshatrao\/a-day-of-digital-learning-part-2-morning-mania\">\n        <font size='+1'>\n            &#127748; Morning Mania\n        <\/font>\n    <\/a>\n    &emsp;|&emsp;\n    <a href=\"https:\/\/www.kaggle.com\/sakshatrao\/a-day-of-digital-learning-part-3-sleepy-noons\">\n        <font size='+1'>\n            &#127774; Sleepy Noons\n        <\/font>\n    <\/a>\n    &emsp;|&emsp;\n    <a href=\"https:\/\/www.kaggle.com\/sakshatrao\/a-day-of-digital-learning-part-4-finally-done\">\n        <font size='+1'>\n            &#127769; Finally Done!\n        <\/font>\n    <\/a>\n<\/div>","d1535352":"<div>\n<font size='+1'>\n    The Calendar Map, in my opinion, is an outstanding tool to represent the student engagement data. Through this simple plot, several basic inferences can be easily made. Some of them include -\n    <ul>\n        <li>Student Engagement is much reduced during weekends<\/li>\n        <li>There is a drop in Student Engagement between June & August due to vacations<\/li>\n        <li>Student Engagement decreases from March onwards till the vacations end, after which it reaches very high values after September<\/li>\n        <li>It can also be seen that Student Engagement descreases on Holidays like Labour Day (Sept 7, Monday), Thanksgiving Eve\/Day (Nov 25-26, Wednesday-Thursday) & Martin Luther King, Jr. Day (Jan 20, Monday)<\/li>\n    <\/ul>\n    <b>So much information from just one graph!<\/b>\n<\/font>\n<\/div>","b851efa3":"<div style=\"background-color: #F8F1FF; padding: 20px 50px;\">\n<span style=\"color:#156BB7;\">\n<font size='+2.5'>\n    <b>7 AM:<\/b>\n<\/font>\n<br>\n<br>\n<font size='+1.5'>\n    As Grace sleeps in her bedroom, readying herself for another hectic day of online classes, let us also ready ourselves for today. Allow me to set the context.\n    <br><br>\n    I am a data analyst for a company involved in the education sector and Grace is my lovely 12-year old daughter. Over the past year, both of us have had to share my bedroom for our respective work - she attends her online class while I work on my project. The latest project I am working on involves analyzing the current state of digital education.\n    <br><br>\n    Spending most of my day with her has been a delightful experience. Her innocent curiosity have made our conversations very engaging and inspiring. Her simple questions & experiences have often helped me come up with innovative ideas for my work. Hopefully, today will be no different.\n<\/font>\n<\/span>\n<\/div>","f7d3f970":"<div>\n<font size='+1'>\n    Before moving any further, let me summarize the project that I'm currently working on. The essential idea is to understand the different trends & patterns in Digital Learning, which has become extremely important to understand due to the ongoing pandemic. The most important aspect of the data that I have been provided is Student Engagement which has been represented by two metrics - pct_access & engagement_index.\n    <br><br>\n    The data can be quickly represented by two interesting visualizations -\n<\/font>\n<\/div>","f553fc55":"<div>\n<font size='+1'>\n    As a data analyst, that seemed a little insulting to me. Sure, an actress or blogger might not need Maths directly; but my entire work is dependent on Maths. My company's services depend on Maths. To be fair, data analysis depends a lot on Maths.\n    <br><br>\n    But more importantly, Maths is not only useful for data analysts but also for each and every kind of business-owner. Critical decisions regarding expansion, marketing, recruitment, testing and many more are now being data-centered and involve several mathematical tools to recognize patterns, identify relationships and develop insights. Through Maths, one can be more assured about the benefits & risks of future decisions.\n    <br><br>\n    Take <font color='#FD7121'>LearnPlatform<\/font> for example. They strive towards developing an <b>EdTech Effectiveness System<\/b> where one of their main objectives is to build a safer, more equitable and effective EdTech ecosystem based on evidence. In simpler words, they help K-12 districts & Charter Networks in investing efficiently in EdTech products for their students & teachers. Through the use of extensive statistical analysis, they can help in understanding a district's EdTech habits, improve EdTech management and help in making data-informed budget decisions.\n    <br><br>\n    Not only this, but LearnPlatform provides much more for education agencies & education technology providers. Take a look at this 100-second video for an overview of their useful services -\n<\/font>\n<\/div>","af9f97f5":"<div>\n<font size='+1'>\n    That should be a sufficient introduction to the context of my analysis. Let us see how the rest of the day unfolds...\n<\/font>\n<\/div>"}}