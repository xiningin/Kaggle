{"cell_type":{"30396f7f":"code","e187f3b5":"code","cc381451":"code","d24e800f":"code","3da1df7a":"code","7f3a44c5":"code","402f2149":"code","88ee3d5e":"code","ce9895e0":"code","2f23069b":"code","4c278abb":"code","732758aa":"code","f023808f":"code","6e9d9e4c":"code","9cf70bae":"code","22787c9f":"code","02c29a59":"code","617ca7b3":"code","dde6d09a":"code","fb13fc4a":"code","ff266bdb":"code","701fa036":"code","26bc881a":"code","15ad29e5":"code","c1018e52":"code","b1ff2912":"code","f35f2968":"code","c8400b03":"code","05c1e7ad":"code","1547ad43":"code","ec6233c5":"code","9bcb570e":"code","1d8f3166":"code","91e68ac7":"code","5a127d9e":"code","4b396198":"code","3d067d49":"code","e6da2f9b":"code","6234b4fc":"code","b9e6fb1d":"code","f64579cc":"code","93550492":"code","7eeb40ff":"code","fdd0ab33":"code","8508d452":"code","3f2d79da":"code","6a73b926":"code","beb81c4b":"code","e73b2b89":"code","dc4b749c":"code","4b3b8f23":"code","96ae6255":"code","ed4cfb3d":"code","ed2af406":"code","b003871e":"code","45a69072":"code","cd81e9a0":"code","8135bd99":"code","c9aca335":"code","e427b6aa":"code","1967ea24":"code","6b2f8c00":"code","efd34263":"code","19b2f1a5":"code","9edf1aac":"code","c3570dd8":"code","4d0dc66d":"code","6affb8fd":"code","4d0badf3":"code","7565ac9e":"code","ba5742ba":"code","c8e4fe6f":"code","fa2e3bd6":"code","18c5aeab":"code","c633f704":"code","aea928e8":"code","e07e3270":"code","4cc2864c":"code","7ddff943":"code","a8a57263":"code","cbf9c5aa":"code","89ca172e":"code","55f7be9b":"code","9616d788":"code","3b6c6ec3":"code","abf631a6":"code","0e7349af":"code","6882f10b":"code","f70a7829":"code","c324400a":"code","d2598bb0":"code","3491c732":"code","514861e9":"code","68978a28":"code","37650d5e":"code","5a8c387a":"code","4fd1da86":"code","c1fa29a5":"code","4f4ae582":"code","cf636e70":"code","7abbd9bd":"code","5cb75e3d":"code","92f4e0d3":"code","051ec813":"code","ea3fbd33":"code","7a722d2c":"code","289d15cf":"code","9ad1e58a":"code","4c5de32f":"markdown","c8b0391e":"markdown","623f7fd9":"markdown","835a6e1a":"markdown","58b84523":"markdown","a46c939e":"markdown","9fe4cb04":"markdown","b4209113":"markdown","603e9318":"markdown","597080c4":"markdown","73b1e725":"markdown","9a64dd74":"markdown","8312ac5f":"markdown","267ced40":"markdown","1360d1ff":"markdown","c5d531e7":"markdown","12541951":"markdown","c38e7306":"markdown","ec49709d":"markdown","0f0c366e":"markdown","7fc32925":"markdown","215953d2":"markdown","7223340b":"markdown","b459554f":"markdown","5050f881":"markdown","64638378":"markdown","5815383e":"markdown","0f15c865":"markdown","6dbba728":"markdown","19c0ae03":"markdown","dec9bd19":"markdown","863f3f62":"markdown","7b4ea033":"markdown","28e78176":"markdown","5add1462":"markdown","c647dbbf":"markdown","db8cba31":"markdown","24edc8a8":"markdown","140b5631":"markdown","c974b1f6":"markdown"},"source":{"30396f7f":"# All Libraries\nfrom tensorflow.keras.layers import Dropout, Flatten, Dense, Activation, Reshape, LeakyReLU , PReLU, BatchNormalization\nfrom statsmodels.tsa.holtwinters import SimpleExpSmoothing, ExponentialSmoothing, Holt\nfrom statsmodels.tsa.arima_model import ARMA,ARIMA,ARMAResults, ARIMAResults\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf\nfrom keras.preprocessing.sequence import TimeseriesGenerator\nfrom pandas import DataFrame,Series,concat,read_csv,datetime\nfrom statsmodels.tsa.seasonal import seasonal_decompose\nfrom statsmodels.stats.stattools import durbin_watson\nfrom statsmodels.tsa.ar_model import AR, ARResults\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.preprocessing import MinMaxScaler\nfrom statsmodels.tsa.stattools import adfuller\nfrom keras.layers import Dense, LSTM\nfrom keras.models import Sequential\nimport statsmodels.tsa.api as smt\nimport matplotlib.pyplot as plt \nfrom matplotlib import pyplot\nimport statsmodels.api as sm\nimport scipy.stats as scs\nimport tensorflow as tf\nfrom numpy import array\nimport lightgbm as lgb\nimport seaborn as sns\nfrom math import sqrt\nimport pandas as pd\nimport numpy as np\nimport itertools","e187f3b5":"# xt = rho*x(t\u22121) + e(t)\n\ndef plotProcess(n_samples=1000, rho=0): # [3]\n    x = w = np.random.normal(size=n_samples)\n    for t in range(n_samples):\n        x[t] = rho * x[t-1] + w[t]\n\n    with plt.style.context('ggplot'):  \n        plt.figure(figsize=(10, 3))\n        plt.plot(x)\n        plt.title(\"Rho {}\\n Dickey-Fuller p-value: {}\".format(rho, round(sm.tsa.stattools.adfuller(x)[1], 3)))\n        \nfor rho in [0, 0.3, 0.6, 0.9, 1]:\n    plotProcess(rho=rho)\n    \n# With  rho=1, we have a random walk process. A non-stationary time series.\n# A time series is white noise if the variables are independent and identically distributed with a mean of zero.\n","cc381451":"df = pd.read_csv(\"\/kaggle\/input\/stock-time-series-20050101-to-20171231\/AAPL_2006-01-01_to_2018-01-01.csv\")[[\"Date\",\"Close\"]]\ndf[\"Date\"] = pd.to_datetime(df[\"Date\"], format = \"%Y-%m-%d\")\n\ndf_monthly = df.set_index(\"Date\").resample(\"M\")[\"Close\"].mean().reset_index()\ndf_monthly = df_monthly.set_index(\"Date\").asfreq('M').reset_index()\ndf_monthly = df_monthly.set_index(\"Date\")\ndf_monthly.index.freq = \"M\"","d24e800f":"df_monthly.head()","3da1df7a":"print(df_monthly.index.min().strftime(\"%Y-%m-%d\"))\nprint(df_monthly.index.max().strftime(\"%Y-%m-%d\"))","7f3a44c5":"df_monthly.Close.plot(figsize = (17,8), grid = True);","402f2149":"\ndef plotmovingaverage(series, window, plot_intervals=True, scale=1.96, plot_anomalies=True):\n    rolling_mean = series.rolling(window=window).mean()\n    plt.figure(figsize=(19,7))\n    plt.title(\"Moving Average - Trend \\n window size = {}\".format(window))\n    plt.plot(rolling_mean, \"red\", label=\"Trend\")\n    if plot_intervals:\n        mae = mean_absolute_error(series[window:], rolling_mean[window:])\n        deviation = np.std(series[window:] - rolling_mean[window:])\n        lower_bond = rolling_mean - (mae + scale * deviation)\n        upper_bond = rolling_mean + (mae + scale * deviation)\n        plt.plot(upper_bond, \"r--\", label=\"Upper \/ Lower\")\n        plt.plot(lower_bond, \"r--\")\n        if plot_anomalies:\n            anomalies = pd.DataFrame(index=series.index, columns=series.columns)\n            anomalies[series<lower_bond] = series[series<lower_bond]\n            anomalies[series>upper_bond] = series[series>upper_bond]\n            plt.plot(anomalies, \"ro\", markersize=15)\n        \n    plt.plot(series[window:], label=\"Read\")\n    plt.legend(loc= \"upper left\")\n    plt.grid(True)\n    \nplotmovingaverage(series=df_monthly, window=12)","88ee3d5e":"df_monthly_sma = df_monthly.copy()\ndf_monthly_sma.index = pd.to_datetime(df_monthly_sma.index, format = \"%Y-%m-%d\")","ce9895e0":"df_monthly_sma.index","2f23069b":"#\u00a0SMA - simple moving average\n\ndf_monthly_sma[\"3_month_SMA\"] = df_monthly_sma[\"Close\"].rolling(window = 3).mean()\ndf_monthly_sma[\"6_month_SMA\"] = df_monthly_sma[\"Close\"].rolling(window = 6).mean()\ndf_monthly_sma[\"12_month_SMA\"] = df_monthly_sma[\"Close\"].rolling(window = 12).mean()","4c278abb":"df_monthly_sma.plot(figsize = (16,6), grid = True);","732758aa":"span = 12\nalpha = 2\/(span+1)\n\ndf_monthly[\"ewma12\"] = df_monthly[\"Close\"].ewm(alpha = alpha, adjust = False).mean()","f023808f":"df_monthly","6e9d9e4c":"fitted_model = SimpleExpSmoothing(df_monthly[\"Close\"]).fit(smoothing_level = alpha, optimized = False)\ndf_monthly[\"ses12\"] = fitted_model.fittedvalues.shift(-1)","9cf70bae":"df_monthly.head()","22787c9f":"df_monthly.plot(figsize = (16,6), grid = True)","02c29a59":"df_monthly[\"DES_12_add\"] = ExponentialSmoothing(df_monthly[\"Close\"], trend = \"add\").fit().fittedvalues.shift(-1)","617ca7b3":"df_monthly.head()","dde6d09a":"df_monthly.iloc[:24].plot(figsize = (16,6))","fb13fc4a":"df_monthly[\"DES_12_mul\"] = ExponentialSmoothing(df_monthly[\"Close\"], trend = \"mul\").fit().fittedvalues.shift(-1)","ff266bdb":"df_monthly.iloc[:24].plot(figsize = (16,6));","701fa036":"df_monthly[\"seasonal_12_mul\"] = ExponentialSmoothing(df_monthly[\"Close\"], trend = \"mul\", seasonal = \"mul\", seasonal_periods = 12).fit().fittedvalues.shift(-1)","26bc881a":"df_monthly[[\"Close\", \"seasonal_12_mul\", \"ses12\"]].plot(figsize = (16,6))","15ad29e5":"df_monthly = df.set_index(\"Date\").resample(\"M\")[\"Close\"].mean().reset_index()\ndf_monthly = df_monthly.set_index(\"Date\").asfreq('M').reset_index()\ndf_monthly = df_monthly.set_index(\"Date\")\ndf_monthly.index.freq = \"M\"","c1018e52":"train = df_monthly[:-8]\ntest = df_monthly[-8:]\n\nstart = len(train)\nend = start + len(test) - 1","b1ff2912":"es_model = ExponentialSmoothing(train[\"Close\"], trend = \"mul\", seasonal = \"mul\", seasonal_periods = 12).fit()\nes_preds = es_model.forecast(len(test))","f35f2968":"test.plot(figsize = (16,7), legend = True, label = \"test\")\nes_preds.plot(grid =True, legend = True, label = \"pred_es\");","c8400b03":"mean_squared_error(test, es_preds)","05c1e7ad":"model = Holt(train, exponential = True)\n\nfit1 = model.fit()\npred1 = fit1.forecast(len(test))\n\ntest.plot()\npred1.plot(legend = True, label = \"PRED_1\");","1547ad43":"mean_squared_error(test, pred1)","ec6233c5":"\ndef adf_test(series,title=''):\n    \"\"\"\n    Pass in a time series and an optional title, returns an ADF report\n    \"\"\"\n    print(f'Augmented Dickey-Fuller Test: {title}')\n    result = adfuller(series.dropna(),autolag='AIC') \n    \n    labels = ['ADF test statistic','p-value','# lags used','# observations']\n    out = pd.Series(result[0:4],index=labels)\n\n    for key,val in result[4].items():\n        out[f'critical value ({key})']=val\n        \n    print(out.to_string())  \n    \n    if result[1] <= 0.05:\n        print(\"Strong evidence against the null hypothesis\")\n        print(\"Reject the null hypothesis\")\n        print(\"Data has no unit root and is stationary\")\n    else:\n        print(\"Weak evidence against the null hypothesis\")\n        print(\"Fail to reject the null hypothesis\")\n        print(\"Data has a unit root and is non-stationary\")","9bcb570e":"adf_test(df_monthly.Close)","1d8f3166":"df_monthly.Close.diff(1).plot(figsize = (19,8)); # first difference in order to stationary","91e68ac7":"adf_test(df_monthly.Close.diff(1)); # It works ! Time series is stationary","5a127d9e":"def tsplot(y, lags=None, figsize=(12, 7), style='bmh'): # [3]\n    if not isinstance(y, pd.Series):\n        y = pd.Series(y)\n        \n    with plt.style.context(style):    \n        fig = plt.figure(figsize=figsize)\n        layout = (2, 2)\n        ts_ax = plt.subplot2grid(layout, (0, 0), colspan=2)\n        acf_ax = plt.subplot2grid(layout, (1, 0))\n        pacf_ax = plt.subplot2grid(layout, (1, 1))\n        \n        y.plot(ax=ts_ax)\n        p_value = sm.tsa.stattools.adfuller(y)[1]\n        ts_ax.set_title('Time Series Analysis Plots\\n Dickey-Fuller: p={0:.5f}'.format(p_value))\n        smt.graphics.plot_acf(y, lags=lags, ax=acf_ax)\n        smt.graphics.plot_pacf(y, lags=lags, ax=pacf_ax)\n        plt.tight_layout()","4b396198":"tsplot(df_monthly.Close, lags=10)","3d067d49":"tsplot(df_monthly.Close.diff(1).dropna(), lags=10)","e6da2f9b":"result = seasonal_decompose(df_monthly, model=\"mul\", freq = 12)\nfig = result.plot()","6234b4fc":"result.seasonal.plot(figsize = (16,6));","b9e6fb1d":"model_ar = AR(train[\"Close\"])\nmodel_ar_fit1 = model_ar.fit(maxlag = 1)\nprediction_ar1 = model_ar_fit1.predict(start, end).rename(\"AR(1) Model\")\n\nmodel_ar = AR(train[\"Close\"])\nmodel_ar_fit2 = model_ar.fit(maxlag = 2)\nprediction_ar2 = model_ar_fit2.predict(start, end).rename(\"AR(2) Model\")\n\nmodel_ar = AR(train[\"Close\"])\nmodel_ar_fit3 = model_ar.fit(maxlag = 3)\nprediction_ar3 = model_ar_fit3.predict(start, end).rename(\"AR(3) Model\")\n\nmodel_ar = AR(train[\"Close\"])\nmodel_ar_fit4 = model_ar.fit(maxlag = 4)\nprediction_ar4 = model_ar_fit4.predict(start, end).rename(\"AR(4) Model\")","f64579cc":"labels = [\"AR-1\",\"AR-2\", \"AR-3\", \"AR-4\"]\npreds = [prediction_ar1,prediction_ar2,prediction_ar3,prediction_ar4]\n\nfor i in range(4):\n    error = mean_squared_error(test[\"Close\"], preds[i])\n    print(f'{labels[i]} MSE -> {error}')","93550492":"test.Close.plot(figsize = (15,8), legend = True)\nprediction_ar1.plot(legend = True)\nprediction_ar2.plot(legend = True)\nprediction_ar3.plot(legend = True)\nprediction_ar4.plot(legend = True)","7eeb40ff":"model_ar_fit1.summary()","fdd0ab33":"!pip install pmdarima\nfrom pmdarima import auto_arima","8508d452":"auto_arima(train.Close).summary()","3f2d79da":"model_arima = ARIMA(train.Close, order=(0,1,1)).fit()\nprediction_arima = model_arima.predict(start, end, typ = \"levels\").rename(\"ARIMA (0,1,1) Model\")","6a73b926":"test.Close.plot(figsize = (15,8),legend = True)\nprediction_arima.plot(legend = True)","beb81c4b":"stepwise_fit = auto_arima(train[\"Close\"],\n                          start_p=0, \n                          start_q=0,\n                          max_p=5, \n                          max_q=5, \n                          seasonal=True,\n                          m=12,\n                          trace=True)","e73b2b89":"prediction_stepwise = stepwise_fit.predict(start, end)\ntest.Close.plot(figsize = (15,8),legend = True)\nprediction_arima.plot(legend = True)","dc4b749c":"stepwise_fit.resid","4b3b8f23":"durbin_watson(df_monthly.Close) ","96ae6255":"df_monthly = df.set_index(\"Date\").resample(\"M\")[\"Close\"].mean().reset_index()\ndf_monthly = df_monthly.set_index(\"Date\").asfreq('M').reset_index()\ndf_monthly = df_monthly.set_index(\"Date\")\ndf_monthly.index.freq = \"M\"\n\ntrain = df_monthly[:-8]\ntest = df_monthly[-8:]","ed4cfb3d":"df_monthly[\"pred\"] = df_monthly.Close.shift(8)","ed2af406":"df_monthly.plot(figsize = (14,6), grid = True);","b003871e":"print(\"Test MSE:\", mean_squared_error(y_pred = df_monthly[-8:].pred, y_true = df_monthly[-8:].Close).round(1))","45a69072":"df_monthly = df.set_index(\"Date\").resample(\"M\")[\"Close\"].mean().reset_index()\ndf_monthly = df_monthly.set_index(\"Date\").asfreq('M').reset_index()\ndf_monthly = df_monthly.set_index(\"Date\")\ndf_monthly.index.freq = \"M\"\n\ndf_monthly_boosting = df_monthly.copy()\n\nresult = seasonal_decompose(df_monthly, freq = 12)\ndf_monthly_boosting[\"trend\"]=result.trend\ndf_monthly_boosting[\"seasonal\"]=result.seasonal\n\n\ndf_monthly_boosting.plot(figsize = (14,6), grid = True);","cd81e9a0":"df_monthly_boosting.head()","8135bd99":"# Should we use ????\n\n\ndf_monthly_boosting[\"8_month_SMA\"] = df_monthly_boosting[\"Close\"].rolling(window = 8).mean()\ndf_monthly_boosting[\"10_month_SMA\"] = df_monthly_boosting[\"Close\"].rolling(window = 10).mean()\ndf_monthly_boosting[\"12_month_SMA\"] = df_monthly_boosting[\"Close\"].rolling(window = 12).mean()\ndf_monthly_boosting[\"18_month_SMA\"] = df_monthly_boosting[\"Close\"].rolling(window = 18).mean()\ndf_monthly_boosting[\"24_month_SMA\"] = df_monthly_boosting[\"Close\"].rolling(window = 24).mean()","c9aca335":"df_monthly_boosting.plot(figsize = (16,6), grid = True);","e427b6aa":"# data leakage !! \ndf_monthly_boosting.drop([\"8_month_SMA\",\"10_month_SMA\",\"12_month_SMA\",\n                          \"18_month_SMA\",\"24_month_SMA\"], axis = 1, inplace = True)","1967ea24":"def rollingDataFrame(df, variable):\n    summary_mean = pd.DataFrame()\n    summary_median = pd.DataFrame()\n    summary_sum = pd.DataFrame()\n    summary_min = pd.DataFrame()\n    \n    window = [1,2]\n    for i in window:\n        summary_mean[\"mean_Roll\"+str(i)+\"_\"+str(variable)] = df[variable].transform(lambda x: x.shift(8).rolling(i).mean())\n        summary_median[\"median_Roll\"+str(i)+\"_\"+str(variable)] = df[variable].transform(lambda x: x.shift(8).rolling(i).median())\n        summary_min[\"min_Roll\"+str(i)+\"_\"+str(variable)] = df[variable].transform(lambda x: x.shift(8).rolling(i).min())\n        summary_sum[\"sum_Roll\"+str(i)+\"_\"+str(variable)] = df[variable].transform(lambda x: x.shift(8).rolling(i).sum())\n    \n    \n    df = pd.concat([df, \n                    summary_mean.reset_index(drop = True), \n                    summary_median.reset_index(drop = True),\n                    summary_sum.reset_index(drop = True) ,\n                    summary_min.reset_index(drop = True)], \n                    axis=1)\n    return df\n\ndef date_column(df, date_column = 'Date'):\n    df['quarter'] = df[date_column].dt.quarter\n    df['month'] = df[date_column].dt.month\n    df['year'] = df[date_column].dt.year\n    df['dayofyear'] = df[date_column].dt.dayofyear\n    df['weekofyear'] = df[date_column].dt.weekofyear\n    return df","6b2f8c00":"df_monthly_boosting = df_monthly_boosting.reset_index()","efd34263":"df_monthly_boosting = rollingDataFrame(df_monthly_boosting, variable='Close')\ndf_monthly_boosting = rollingDataFrame(df_monthly_boosting, variable='trend')\ndf_monthly_boosting = rollingDataFrame(df_monthly_boosting, variable='seasonal')\ndf_monthly_boosting = date_column(df_monthly_boosting)","19b2f1a5":"df_monthly_boosting.tail()","9edf1aac":"# Train-Valid-Test Split\n\ndf_monthly_boosting.drop([\"Date\",\"trend\",\"seasonal\"], axis = 1, inplace = True)\n\ntrain_boosting = df_monthly_boosting[:-16]\nx_train_boosting = train_boosting.drop(\"Close\", axis=1)\ny_train_boosting = train_boosting[\"Close\"]\n\nvalid_boosting = df_monthly_boosting[-16:-8]\nx_valid_boosting = valid_boosting.drop(\"Close\", axis=1)\ny_valid_boosting = valid_boosting[\"Close\"]\n\ntest_boosting = df_monthly_boosting[-8:]\nx_test_boosting = test_boosting.drop(\"Close\", axis=1)\ny_test_boosting = test_boosting[\"Close\"]","c3570dd8":"def plotImp(model, X , num = 10):\n    feature_imp = pd.DataFrame({'Value':model.feature_importance(),'Feature':X.columns})\n    plt.figure(figsize=(13, 7))\n    sns.set(font_scale = 1)\n    sns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp.sort_values(by=\"Value\", \n                                                        ascending=False)[0:num])\n    plt.title('LightGBM Features (avg over folds)')\n    plt.tight_layout()\n    plt.show()","4d0dc66d":"x_valid_boosting.columns","6affb8fd":"x_train_boosting.columns = [\"\".join (c if c.isalnum() else \"_\" for c in str(x)) for x in x_train_boosting.columns]\nx_valid_boosting.columns = [\"\".join (c if c.isalnum() else \"_\" for c in str(x)) for x in x_valid_boosting.columns]\n\n\ntrainLGB = lgb.Dataset(x_train_boosting, label = y_train_boosting,categorical_feature=['quarter', 'month', 'year', \n                                                                                       'dayofyear','weekofyear'])\nvalidLGB = lgb.Dataset(x_valid_boosting, label = y_valid_boosting,categorical_feature=['quarter', 'month', 'year', \n                                                                                       'dayofyear','weekofyear'])\n\nlgb_param ={   \n    'num_leaves': 2**3 - 1,\n    'max_depth': 20,\n    'learning_rate': 0.01,\n    'min_data_in_leaf': 10,\n    'metric': 'mse',\n           }\n\nevals_result = {}\n\nclf = lgb.train(lgb_param, trainLGB, num_boost_round = 10000, valid_sets=[trainLGB, validLGB], verbose_eval=100, \n                early_stopping_rounds = 2, evals_result=evals_result)\n","4d0badf3":"lgb.plot_metric(evals_result)\nprint(\"\/\/\/\/ Model Performance \/\/\/\/\")\n\npreds_test = clf.predict(x_test_boosting)\n\nprint(\"Test MSE:\", mean_squared_error(y_pred = preds_test,y_true = y_test_boosting).round(1))","7565ac9e":"preds = pd.DataFrame(preds_test, y_test_boosting).reset_index().rename(columns = {0:\"pred\"})\npreds.plot(figsize = (14,6), grid = True)","ba5742ba":"plotImp(clf, x_train_boosting)","c8e4fe6f":"!pip install ngboost","fa2e3bd6":"from ngboost import NGBRegressor\n","18c5aeab":"train_boosting = df_monthly_boosting[:-16]\nx_train_boosting = train_boosting.drop(\"Close\", axis=1)\ny_train_boosting = train_boosting[\"Close\"]\n\nvalid_boosting = df_monthly_boosting[-16:-8]\nx_valid_boosting = valid_boosting.drop(\"Close\", axis=1)\ny_valid_boosting = valid_boosting[\"Close\"]\n\ntest_boosting = df_monthly_boosting[-8:]\nx_test_boosting = test_boosting.drop(\"Close\", axis=1)\ny_test_boosting = test_boosting[\"Close\"]\n\nx_train_boosting = x_train_boosting.fillna(0)\nx_test_boosting = x_test_boosting.fillna(0)\nx_valid_boosting = x_valid_boosting.fillna(0)","c633f704":"ngb = NGBRegressor(\n    Base=DecisionTreeRegressor(criterion='friedman_mse',\n        max_depth=2),\n    learning_rate=0.1,\n    verbose=True, verbose_eval=100,)\n\nngb.fit(x_train_boosting, y_train_boosting, X_val = x_valid_boosting, Y_val = y_valid_boosting,\n        early_stopping_rounds = 1)\n\n","aea928e8":"tr_pred = ngb.predict(x_train_boosting)\nva_pred = ngb.predict(x_valid_boosting)\nts_pred = ngb.predict(x_test_boosting)\n\ntr_int = ngb.pred_dist(x_train_boosting).dist.interval(0.95)\nva_int = ngb.pred_dist(x_valid_boosting).dist.interval(0.95)\nts_int = ngb.pred_dist(x_test_boosting).dist.interval(0.95)\n\nprint(\"Test MSE:\", mean_squared_error(ts_pred, y_test_boosting))","e07e3270":"\ntr_res = x_train_boosting.copy()\ntr_res[\"SET\"] = \"TRAIN\"\ntr_res[\"PRED\"] = tr_pred\ntr_res[\"LOWER95\"] = tr_int[0]\ntr_res[\"LOWER95\"] = np.where(tr_res[\"LOWER95\"] < 0, 0, tr_res[\"LOWER95\"])\ntr_res[\"UPPER95\"] = tr_int[1]\n\n\nva_res = x_valid_boosting.copy()\nva_res[\"SET\"] = \"VALID\"\nva_res[\"PRED\"] = va_pred\nva_res[\"LOWER95\"] = va_int[0]\nva_res[\"LOWER95\"] = np.where(va_res[\"LOWER95\"] < 0, 0, va_res[\"LOWER95\"])\nva_res[\"UPPER95\"] = va_int[1]\n\nts_res = x_test_boosting.copy()\nts_res[\"SET\"] = \"TEST\"\nts_res[\"PRED\"] = ts_pred\nts_res[\"LOWER95\"] = ts_int[0]\nts_res[\"LOWER95\"] = np.where(ts_res[\"LOWER95\"] < 0, 0, ts_res[\"LOWER95\"])\nts_res[\"UPPER95\"] = ts_int[1]\n\nmodel_result = pd.concat([pd.concat([tr_res, va_res]), ts_res])\nmodel_result = model_result[[\"SET\",\"PRED\",\"LOWER95\",\"UPPER95\"]]","4cc2864c":"pd.merge(df_monthly.reset_index(),model_result,left_index=True, right_index=True).set_index(\"Date\").plot(figsize = (15,8), legend = True)\n","7ddff943":"pd.merge(df_monthly.reset_index(),model_result,left_index=True, right_index=True).set_index(\"Date\").iloc[-15:,].plot(figsize = (12,5), legend = True)\n","a8a57263":"from fbprophet import Prophet\n\ndf_monthly_prophet = df_monthly.copy()\ndf_monthly_prophet = df_monthly_prophet.reset_index()\n\ndf_monthly_prophet.columns=['ds','y']\n\ntrain = df_monthly_prophet[:-8]\ntest = df_monthly_prophet[-8:]","cbf9c5aa":"model = Prophet(yearly_seasonality=True)\nmodel.fit(train)","89ca172e":"future = model.make_future_dataframe(periods = 8, freq = 'M')  \nforecast = model.predict(future)\nforecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']].tail()","55f7be9b":"pd.merge(forecast,test,left_index=True, right_index=True)[['ds_x', 'yhat', 'yhat_lower', 'yhat_upper','y']].set_index(\"ds_x\").plot()","9616d788":"plot_forecast = model.plot(forecast)","3b6c6ec3":"print(tf.__version__)","abf631a6":"train = df_monthly[:-8]\ntest = df_monthly[-8:]\n\n\nsplit_time = len(train)\nseries = df_monthly.Close.values\ntime = np.arange(len(df_monthly), dtype=\"float32\")\n\n\ntime_train = time[:split_time]\nx_train = series[:split_time]\ntime_valid = time[split_time:]\nx_valid = series[split_time:]\n\n\nwindow_size = 8\nbatch_size = 1\nshuffle_buffer_size = split_time ","0e7349af":"def windowed_dataset(series, window_size, batch_size, shuffle_buffer):\n    dataset = tf.data.Dataset.from_tensor_slices(series)\n    dataset = dataset.window(window_size + 1, shift=1, drop_remainder=True)\n    dataset = dataset.flat_map(lambda window: window.batch(window_size + 1))\n    dataset = dataset.shuffle(shuffle_buffer).map(lambda window: (window[:-1], window[-1]))\n    dataset = dataset.batch(batch_size).prefetch(1)\n    return dataset\n\ndef plot_series(time, series, format=\"-\", start=0, end=None):\n    plt.plot(time[start:end], series[start:end], format)\n    plt.xlabel(\"Time\")\n    plt.ylabel(\"Value\")\n    plt.grid(True)","6882f10b":"tf.keras.backend.clear_session()\ntf.random.set_seed(51)\nnp.random.seed(51)\n\ndataset = windowed_dataset(x_train, window_size, batch_size=batch_size,shuffle_buffer=shuffle_buffer_size) \n\n\nmodel = tf.keras.models.Sequential([\n  tf.keras.layers.Lambda(lambda x: tf.expand_dims(x, axis=-1),\n                      input_shape=[window_size]),\n\n  tf.keras.layers.LSTM(64, return_sequences=True),\n  tf.keras.layers.LSTM(64, return_sequences=True),\n  tf.keras.layers.LSTM(16),\n  tf.keras.layers.Dense(1),\n  tf.keras.layers.Lambda(lambda x: x * 100.0)\n])\n\n#model = tf.keras.models.Sequential([\n#tf.keras.layers.Dense(100, input_shape=[window_size], activation=\"relu\"),\n#tf.keras.layers.Dense(50, activation=\"relu\"),\n#tf.keras.layers.Dense(40, activation=\"relu\"),\n#tf.keras.layers.Dense(30, activation=\"relu\"),\n#tf.keras.layers.Dense(20),\n#tf.keras.layers.Dense(10),\n#tf.keras.layers.Dense(5),\n#tf.keras.layers.Dense(2),\n#tf.keras.layers.Dense(1)\n#])\n\n\nmodel.compile(loss=\"mean_squared_error\",\n              optimizer=\"adam\",\n              metrics=[\"mape\"])\n\nhistory = model.fit(dataset,epochs=100)","f70a7829":"forecast=[]\nresults = []\n\nfor time in range(len(x_train)-window_size+1):\n    forecast.append(model.predict(x_train[time:time + window_size][np.newaxis]))\n    \nforecast = forecast[split_time - 2*window_size+1:]    \nresults = np.array(forecast)[:, 0, 0]\n\n\nplt.figure(figsize=(10, 6))\n\nplot_series(time_valid, x_valid)\nplot_series(time_valid, results)\n","c324400a":"# PREDCT FUTURE\nprediction=[]\nresults = []\n\nfor time in range(len(series)-window_size+1):\n    forecast.append(model.predict(series[time:time + window_size][np.newaxis]))\n    \nforecast = forecast[split_time - window_size+1:]    \nresults = np.array(forecast)[:, 0, 0]\npd.DataFrame(results, columns = [\"Prediction\"]).plot(figsize = (14,6), grid = True);","d2598bb0":"df_monthly = df.set_index(\"Date\").resample(\"M\")[\"Close\"].mean().reset_index()\ndf_monthly = df_monthly.set_index(\"Date\").asfreq('M').reset_index()\ndf_monthly = df_monthly.set_index(\"Date\")\ndf_monthly.index.freq = \"M\"\n\ntrain = df_monthly[:-8]\ntest = df_monthly[-8:]","3491c732":"scaler = MinMaxScaler()\nscaler.fit(train)\n\n\nscaled_train = scaler.transform(train)\nscaled_test = scaler.transform(test)\n\nn_inputs = 8\nn_features = 1\n\ngenerator = TimeseriesGenerator(scaled_train, scaled_train, length=n_inputs, batch_size = 1)","514861e9":"model = Sequential()\nmodel.add(LSTM(150, activation = \"relu\", input_shape = (n_inputs, 1)))\nmodel.add(Dense(1))\nmodel.compile(optimizer = \"adam\", loss = \"mse\")","68978a28":"model.summary()","37650d5e":"model.fit_generator(generator, epochs = 15)","5a8c387a":"test_prediction = []\nfirst_eval_batch = scaled_train[-n_inputs:]\ncurrent_batch = first_eval_batch.reshape((1,n_inputs,1))\n\nfor i in range(len(test)):\n    \n    current_pred= model.predict(current_batch)[0]\n    \n    test_prediction.append(current_pred)\n    current_batch = np.append(current_batch[:,1:,:], [[current_pred]], axis = 1)\n    \nprediction = scaler.inverse_transform(test_prediction)\ntest[\"pred\"] = prediction\n","4fd1da86":"test.plot()","c1fa29a5":"mean_squared_error(test[\"Close\"], test[\"pred\"])","4f4ae582":"train = df_monthly[:-8]\ntest = df_monthly[-8:]\n\n\n\ndef series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n    n_vars = 1 if type(data) is list else data.shape[1]\n    df = DataFrame(data)\n    cols, names = list(), list()\n    for i in range(n_in, 0, -1):\n        cols.append(df.shift(i))\n        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n    for i in range(0, n_out):\n        cols.append(df.shift(-i))\n        if i == 0:\n            names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n        else:\n            names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n    agg = concat(cols, axis=1)\n    agg.columns = names\n    if dropnan:\n        agg.dropna(inplace=True)\n    return agg\n \n\nraw_values = df_monthly.Close.values\n\n\nn_lag = 5\nn_seq = 8\nn_test = 8\nfeatures = 1\n\n\nraw_values = raw_values.reshape(len(raw_values),1) \n\n\n\nscaler = MinMaxScaler()\nscaler.fit(raw_values)\n\nscaled_values = scaler.transform(raw_values)\n\n\nsupervised = series_to_supervised(scaled_values, n_lag, n_seq)\nsupervised_values = supervised.values\n      \n\n\ntrain, test = supervised_values[0:-n_test], supervised_values[-n_test:]\n\n        \nbatch_size = 1\n\n\ntrain_X, train_y = train[:, 0:n_lag], train[:, n_lag:]\n\ntrain_X = train_X.reshape(train_X.shape[0], n_lag, features)\n\ntest_X , test_y = test[:, 0:n_lag], test[:, n_lag:]\ntest_X = test_X.reshape(test_X.shape[0], n_lag, features)\n","cf636e70":"\nmodel = Sequential()\nmodel.add(LSTM(256, batch_input_shape=(batch_size,train_X.shape[1], train_X.shape[2]),activation='relu',return_sequences=False))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(n_seq))\nmodel.add(Activation('relu'))\n\n\nmodel.compile(loss='mse', \n              optimizer=tf.keras.optimizers.SGD(lr=1e-3, momentum=0.9))\n\n\nes = EarlyStopping(monitor='val_loss', mode='auto', verbose=1, patience=3)\nreduce_lr_loss = ReduceLROnPlateau(monitor='val_loss', factor=0.015, patience=3, verbose=1, epsilon=1e-2, mode='min')\n\n\nhistory = model.fit(train_X, train_y, epochs=100, batch_size=batch_size, \n                    validation_data=(test_X, test_y), verbose=1, shuffle=False, callbacks=[es, reduce_lr_loss])\nmodel.reset_states()","7abbd9bd":"\nyhat = model.predict(test_X,batch_size)\n\n\nt1=list()\nt2=list()\ndic = {}\nfor g in range(0,n_seq,features):\n    for j in range(features-1,n_seq,features):\n        t1.append(g)\n        t1 = list(set(t1))\n        t2.append(j)\n        t2 = list(set(t2))\n        t1 = sorted(t1)\n        t2 = sorted(t2)\n        dictionary = dict(zip(t1, t2))        \n        for key,value in dictionary.items():\n            inv_yhat = np.array(pd.DataFrame(yhat).loc[:,key:value])\n            dic[key] = scaler.inverse_transform(inv_yhat)\n\n                      \nlisted = list()                 \nfor k in range(0,n_seq,features):\n                listed.append(dic[k][:,0])                    \n\npredicted = pd.DataFrame.from_records(listed)               \npredicted = predicted.transpose()\n\n\nm1=list()\nm2=list()\ndic = {}\nfor g in range(0,n_seq,features):\n    for j in range(features-1,n_seq,features):\n        m1.append(g)\n        m1 = list(set(m1))\n        m2.append(j)\n        m2 = list(set(m2))\n        m1 = sorted(m1)\n        m2 = sorted(m2)\n        dictionary = dict(zip(m1, m2))        \n        for key,value in dictionary.items():\n            inv_y = np.array(pd.DataFrame(test_y).loc[:,key:value])\n            dic[key] = scaler.inverse_transform(inv_y)\n           \n\nlisted = list()                 \nfor k in range(0,n_seq,features):\n                listed.append(dic[k][:,0])                    \n            \nreal = pd.DataFrame.from_records(listed)               \nreal = real.transpose()                       ","5cb75e3d":"\nrmse = mean_squared_error(real, predicted)\nprint('Test MSE: %.3f' % rmse)","92f4e0d3":"train_for_pred, test_for_pred = supervised_values[0:-n_test], supervised_values[-n_test:]\ntrain_X_pred = train_for_pred[:, n_seq:train_for_pred.shape[1]]\ntest_X_pred = test_for_pred[:, n_seq:test_for_pred.shape[1]]\ntrain_X_pred = train_X_pred.reshape((train_X_pred.shape[0], train_X_pred.shape[1] , features))\ntest_X_pred = test_X_pred.reshape((test_X_pred.shape[0], test_X_pred.shape[1], features))\n\nyhat_pred = model.predict(test_X_pred,batch_size)\n","051ec813":"\nt1=list()\nt2=list()\ndic_pred = {}\nfor g in range(0,n_seq,features):\n    for j in range(features-1,n_seq,features):\n        t1.append(g)\n        t1 = list(set(t1))\n        t2.append(j)\n        t2 = list(set(t2))\n        t1 = sorted(t1)\n        t2 = sorted(t2)\n        dictionary = dict(zip(t1, t2))        \n        for key,value in dictionary.items():\n            inv_yhat_pred = np.array(pd.DataFrame(yhat_pred).loc[:,key:value])\n            dic_pred[key] = scaler.inverse_transform(inv_yhat_pred)\n\n     \nlisted_pred = list()                 \nfor k in range(0,n_seq,features):\n                listed_pred.append(dic_pred[k][:,0])                    \n\n\npredicted_t_8 = pd.DataFrame.from_records(listed_pred)               \npredicted_t_8 = predicted_t_8.transpose()\n\np = list()\nfor i in range(0,8):\n    p.append(predicted_t_8.iloc[-1,i].astype('float32'))\n    \nf1 = pd.DataFrame(p)\nfinal_prediction_values = f1\n\ntest = df_monthly[-8:]\nfinal_prediction_values.index = test.index\nfinal_prediction_values.columns = [\"pred\"]","ea3fbd33":"pd.merge(test,final_prediction_values,left_index=True, right_index=True)","7a722d2c":"pd.merge(test,final_prediction_values,left_index=True, right_index=True).plot(figsize = (15,8), legend = True)","289d15cf":"\nrmse = mean_squared_error(test.Close, final_prediction_values.pred)\nprint('Test MSE: %.3f' % rmse)","9ad1e58a":"# [1] -> https:\/\/en.wikipedia.org\/wiki\/Time_series#:~:text=A%20time%20series%20is%20a,sequence%20of%20discrete%2Dtime%20data.\n# [2] -> https:\/\/cmapskm.ihmc.us\/rid=1052458821502_1749267941_6906\/components.pdf\n# [3] -> https:\/\/www.kaggle.com\/kashnitsky\n# [4] -> https:\/\/otexts.com\/fpp2\/stationarity.html#fn14\n# [5] -> https:\/\/www.statisticshowto.com\/adf-augmented-dickey-fuller-test\/\n# [6] -> https:\/\/people.duke.edu\/~rnau\/411diff.htm\n# [7] -> https:\/\/medium.com\/@josemarcialportilla\/using-python-and-auto-arima-to-forecast-seasonal-time-series-90877adff03c\n# https:\/\/github.com\/hincaltopcuoglu\/\n# https:\/\/www.udemy.com\/user\/joseportilla\/","4c5de32f":"We can fight non-stationarity using different approaches: various order differences, trend and seasonality removal, smoothing, and transformations like Box-Cox or logarithmic. [3]","c8b0391e":"![Stationary](http:\/\/miro.medium.com\/max\/1400\/1*xdblkZyg6YmmReAkZHUksw.png)","623f7fd9":"## What is the time series\nA time series is a series of data points indexed (or listed or graphed) in time order. Most commonly, a time series is a sequence taken at successive equally spaced points in time. Time series are used in statistics, signal processing, pattern recognition, econometrics, mathematical finance, weather forecasting, earthquake prediction, electroencephalography, control engineering, astronomy, communications engineering, and largely in any domain of applied science and engineering which involves temporal measurements. [1]","835a6e1a":"![](http:\/\/bit.ly\/2lDm5TJ)","58b84523":"![](http:\/\/2.bp.blogspot.com\/-BshCcFzpToI\/VQ4vmBJZEeI\/AAAAAAAAABg\/8XX8m2Mkyy8\/s1600\/3.jpg)","a46c939e":"##\u00a0ARIMA","9fe4cb04":"## Train Test Split","b4209113":"## Augmented Dickey Fuller Test (ADF)\n\nThe Augmented Dickey Fuller Test (ADF) is unit root test for stationarity. Unit roots can cause unpredictable results in your time series analysis. The Augmented Dickey-Fuller test can be used with serial correlation. The ADF test can handle more complex models than the Dickey-Fuller test, and it is also more powerful [5]. \n\nThe hypotheses for the test:\n- The null hypothesis for this test is that there is a unit root. (The null hypothesis is that the data are non-stationary.)\n- The alternate hypothesis differs slightly according to which equation you\u2019re using. The basic alternate is that the time series is stationary","603e9318":"## NGBOOST","597080c4":"## LSTM-2","73b1e725":"![](http:\/\/onlinelibrary.wiley.com\/cms\/asset\/0b016306-f288-40db-9958-207d49505da1\/atr1332-fig-0001-m.jpg)","9a64dd74":"# References","8312ac5f":"### Test for autocorrelation\n\nWe will use the Durbin-Watson test for autocorrelation.  The Durbin-Watson statistic ranges in value from 0 to 4. A value near 2 indicates non-autocorrelation; a value toward 0 indicates positive autocorrelation; a value toward 4 indicates negative autocorrelation.","267ced40":"## Prophet","1360d1ff":"# Deep Learning Models\n## LSTM","c5d531e7":"A stationary time series is one whose statistical properties such as mean, variance, autocorrelation, etc. are all constant over time. Most statistical forecasting methods are based on the assumption that the time series can be rendered approximately stationary (i.e., \"stationarized\") through the use of mathematical transformations. A stationarized series is relatively easy to predict: you simply predict that its statistical properties will be the same in the future as they have been in the past! [6]","12541951":"## ExponentialSmoothing","c38e7306":"# Box Jenkins (BJ) model approach","ec49709d":"<img src=\"https:\/\/miro.medium.com\/max\/1400\/0*bCS3EWiVfLIZqwIW.gif\">","0f0c366e":"## Naive Model","7fc32925":"# Time Series Analysis in Python (everyting you can do with Python and Time Series)","215953d2":"## Holt Winters","7223340b":"## What is Stationarity and Differencing?\n\nBefore applying any statistical model on a Time Series, series data has to be stationary. A stationary time series is one whose properties do not depend on the time at which the series is observed. If a process is stationary, that means it does not change its statistical properties over time, namely its mean and variance.[3] \n\nTransformations such as logarithms can help to stabilise the variance of a time series. Differencing can help stabilise the mean of a time series by removing changes in the level of a time series, and therefore eliminating (or reducing) trend and seasonality [4].\n\n- It should have constant mean.\n- It should have constant variance or standard deviation.\n- Auto-covariance should not depend on time.","b459554f":"## ARMA Models","5050f881":"##\u00a0Deep Neural Network","64638378":"White Test (Heteroskedasticity) \n- Ho: B1=B2=B3...=0 (constant variance)\n- H1: B1 != B2 != .. != 0\nOur daha has not constant variance (Prob(H) <0.05) Reject Ho.","5815383e":"# MODELS","0f15c865":"# Smooting","6dbba728":"\n<img src=\"https:\/\/media.giphy.com\/media\/ojmB7lOn3VUU8\/giphy.gif\">\n","19c0ae03":"lets check first difference","dec9bd19":"# THANK YOU <3","863f3f62":"## Decompose Analysis\n\nHere we can see there is an upward trend. We can use statsmodels to perform a decomposition of this time series. The decomposition of time series is a statistical task that deconstructs a time series into several components, each representing one of the underlying categories of patterns. With statsmodels we will be able to see the trend, seasonal, and residual components of our data. [7]","7b4ea033":"<img src=\"https:\/\/media.giphy.com\/media\/jsm7XMcyeTFJE4vHzO\/giphy.gif\">","28e78176":"## AR MODELS","5add1462":"# If you like, Pls Upvote notebook","c647dbbf":"* # If you like, Pls Upvote notebook","db8cba31":"# Boosting Methods\n## LGBM\n\n### Feature Engineering","24edc8a8":"## ACF & PACF Plotting","140b5631":"- Time series that show no autocorrelation are called white noise\n- Model Residual -> White noise time series is defined by a zero mean, constant variance, and zero correlation.","c974b1f6":"## The Components of a Time Series\n\nTime series has four components. Any time series can contain some or all of the following components: [2]\n- Trend (T)\n-  Seasonal (S)\n-  Cyclical (C)\n- Irregular - Random (I)\n\nThese components may be combined in different ways. It is usually assumed that they are multiplied or\nadded, i.e. \n\nyt = T \u00d7 C \u00d7 S \u00d7 I\nyt = T + C + S + I\n\n**Trend Component:**  The trend is the long term pattern of a time series. A trend can be positive or negative depending on whether\nthe time series exhibits an increasing long term pattern or a decreasing long term pattern. If a time series does not show an increasing or decreasing pattern then the series is stationary in the mean.\n\n**Cyclical component:** Any pattern showing an up and down movement around a given trend is identified as a cyclical pattern. The duration of a cycle depends on the type of business or industry being analyzed. \n\n**Seasonal component:** Seasonality occurs when the time series exhibits regular fluctuations during the same month (or months) every year, or during the same quarter every year. For instance, retail sales peak during the month of December. \n\n**Irregular component:**  This component is unpredictable. Every time series has some unpredictable component that makes it a random variable. In prediction, the objective is to \u201cmodel\u201d all the components to the point that the only component that remains unexplained is the random component.\n"}}