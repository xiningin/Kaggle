{"cell_type":{"bde9a7bf":"code","33ab7a0d":"code","a5ff959e":"code","43115612":"code","7bbcc61c":"code","a15b8133":"code","7afd2aff":"code","29240d00":"code","1dffbe40":"code","93e0f9e8":"code","7662b030":"code","b2582b61":"code","4593cdc6":"code","adb2585b":"code","36bb6fce":"code","31745e21":"code","83f724d9":"code","b58f6d16":"code","174c129e":"code","704314c9":"code","bdddec64":"code","317997ea":"code","75bf0c3f":"code","f4af34f8":"code","5ec3e7f9":"code","01db4794":"code","71df231f":"code","6db59829":"code","82c58546":"code","50359975":"code","aa842362":"code","59e753ad":"code","13bfe1f4":"code","1ceec8da":"code","eb76c68c":"code","4eb3748b":"code","4053ff4e":"code","37cf6341":"code","0a57736f":"code","75e53740":"code","6a558a44":"code","8e0a8cea":"code","6e0030a3":"code","028a4311":"code","2ee018b4":"code","d115d14a":"code","1bf8604e":"markdown","a7518e17":"markdown","fd067e56":"markdown","23c3ad2a":"markdown","c1fe13f0":"markdown","4bc67a71":"markdown","c209859d":"markdown","8c8d2844":"markdown","e7cedbb5":"markdown","44ef8eaa":"markdown","7ce2ef2a":"markdown","e547b891":"markdown","e0f0e13c":"markdown","e5d1e568":"markdown","d99c2a95":"markdown","e2c759ff":"markdown","a0da1d04":"markdown","8b44082c":"markdown","dc614add":"markdown","58dbccd8":"markdown","7aceb147":"markdown","c06081f6":"markdown","9eed780c":"markdown","0f45260d":"markdown","0d80cd68":"markdown","48c7f35d":"markdown","7d218819":"markdown","70a77dae":"markdown","204ccf56":"markdown","e68b522f":"markdown","c0af4e59":"markdown","72b4351b":"markdown","3d602eac":"markdown","93a8a6fc":"markdown","ce8301ab":"markdown","f170f905":"markdown","93fc65a3":"markdown","13a38f18":"markdown","d0262c84":"markdown","7bb008cc":"markdown","dca52165":"markdown","0fc1b691":"markdown","bffaa345":"markdown","27607e01":"markdown","93caa89c":"markdown","2e65950b":"markdown"},"source":{"bde9a7bf":"import pandas as pd                    # Pandas allows data manipulation\nimport numpy as np                     # NumPy allows specific numerical computations\nfrom matplotlib import pyplot as plt   # Matplotlib allows data visualization\nimport seaborn as sns\nimport time\nfrom random import randint\n\n# Scikit-Learn module allows to build several predictive models\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, roc_curve, auc, precision_recall_curve\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neural_network import MLPClassifier\n\n# Keras module allows to build Neural Network models with specific parameterizations\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense\n\nimport os\nos.environ[\"KMP_SETTINGS\"] = \"false\"","33ab7a0d":"# Function to plot counts for categorical variables\ndef Plot_Counts_For_Categorical_Variables(DF,Cols):\n    for p in Cols:\n        Counts = DF[p].value_counts()\n        Counts.sort_index(axis=0, ascending=True, inplace=True)\n        Labels = Counts.index\n        print(Counts)\n        #\n        Colors = sns.color_palette(\"Set2\")[0:len(Labels)]\n        plt.pie(Counts, labels=Counts.index, colors=Colors, autopct=\"%.1f%%\")\n        plt.show()\n    return\n\n# Function to compute model predictions\ndef Model_Predictions(Model, xTrain,yTrain, xTest,yTest, xValidat,yValidat, op=None):\n    # Number of observations\n    nTrain   = len(yTrain  )\n    nTest    = len(yTest   )\n    nValidat = len(yValidat)\n    \n    # Predictions\n    yTrain_p   = Model.predict(xTrain)\n    yTest_p    = Model.predict(xTest )\n    yValidat_p = Model.predict(xValidat)\n    \n    if (op==\"keras\"):\n        yTrain_p   =   yTrain_p.ravel()\n        yTest_p    =    yTest_p.ravel()\n        yValidat_p = yValidat_p.ravel()\n    \n    # Convert to Series\n    yTrain_p   = pd.Series(yTrain_p  , name=\"Churn_p\")\n    yTest_p    = pd.Series(yTest_p   , name=\"Churn_p\")\n    yValidat_p = pd.Series(yValidat_p, name=\"Churn_p\")\n    \n    return yTrain_p, yTest_p, yValidat_p\n\n#--------------------------------------------------------------------------------\n# Function to plot a Confusion Matrix\ndef Plot_Confusion_Matrix(CM,Title=\"Confusion Matrix\"):\n    GroupNames       = [\"True Neg\",\"False Pos\",\"False Neg\",\"True Pos\"]\n    GroupCounts      = [\"{0:0.0f}\".format(x) for x in CM.flatten()]\n    GroupPercentages = [\"{0:.2%}\".format(x) for x in CM.flatten()\/np.sum(CM)]\n    \n    Labels = [f\"{x1}\\n\\n{x2}\\n\\n{x3}\" for x1,x2,x3 in zip(GroupNames,GroupCounts,GroupPercentages)]\n    Labels = np.asarray(Labels).reshape(2,2)\n    \n    FontSize = 12\n    plt.figure(figsize=(6,5))\n    Ax = sns.heatmap(CM, annot=Labels, fmt=\"\", cmap=\"Blues\")\n    Ax.set_title(Title, fontsize=FontSize)\n    Ax.set_xlabel(\"Predicted Values\", fontsize=FontSize)\n    Ax.set_ylabel(\"Actual Values\", fontsize=FontSize)\n    Ax.xaxis.set_ticklabels([\"False\",\"True\"], fontsize=FontSize)\n    Ax.yaxis.set_ticklabels([\"False\",\"True\"], fontsize=FontSize)\n    plt.show()\n    return\n\n#--------------------------------------------------------------------------------\n# Function to plot the \"Receiver-Operating Curve\" used to validate Classification Models\ndef Plot_ROC_AUC(fpr,tpr,roc_auc):\n    plt.figure(figsize=(5,5))\n    plt.plot(fpr, tpr, \"-\", color=\"blue\", lw=1.5, label=\"ROC-AUC = {:0.3f}\".format(roc_auc))\n    plt.fill_between(fpr, 0.0, tpr, facecolor=\"blue\", alpha=0.10)\n    plt.plot([0, 1], [0, 1], color=\"gray\", lw=0.5)\n    plt.xlim([0,1])\n    plt.ylim([0,1])\n    plt.legend(loc=\"lower right\", fontsize=12, frameon=False, fancybox=True)\n    plt.xlabel(\"False Positive Rate (FPR)\", fontsize=14)\n    plt.ylabel( \"True Positive Rate (TPR)\", fontsize=14)\n    plt.title(\"ROC Curve\", fontsize=14)\n    plt.show()\n\n#--------------------------------------------------------------------------------\nSeed = 19781126   # Seed for use with random generators, to allow reproducible results\nplt.style.use(\"seaborn-bright\")\n%matplotlib inline","a5ff959e":"foo = \"\/kaggle\/input\/predicting-churn-for-bank-customers\/Churn_Modelling.csv\"\nBankChurn = pd.read_table(foo, header=0, skiprows=None, skipfooter=0, nrows=None, index_col=None,\n                          sep=\",\", decimal=\".\", na_values=None, low_memory=False)","43115612":"display(BankChurn)","7bbcc61c":"BankChurn.drop(columns=[\"RowNumber\",\"CustomerId\",\"Surname\"], axis=1, inplace=True)","a15b8133":"print(set(BankChurn[\"Gender\"]))","7afd2aff":"BankChurn.rename(columns={\"Gender\":\"IsMale\"}, inplace=True)\nBankChurn.replace(to_replace={\"IsMale\":{\"Male\":1,\"Female\":0}}, inplace=True)\n\nprint(set(BankChurn[\"IsMale\"]))","29240d00":"print(set(BankChurn[\"Geography\"]))","1dffbe40":"# Rename \"Geography\" as \"Country\"\nBankChurn.rename(columns={\"Geography\":\"Country\", \"Exited\":\"Churn\"}, inplace=True)\n\n# Separate numerical variables (to be normalized) and categorical variables (for one-hot-encoding)\nNumericalData   = BankChurn.select_dtypes(\"number\")\nCategoricalData = BankChurn.select_dtypes(\"object\")\n\nprint(CategoricalData.columns)","93e0f9e8":"# Create One-Hot-Enconded (OHE) variables for \"Country\"\nOHE = pd.get_dummies(CategoricalData)\nOHE = OHE.astype(\"int64\")\nOHE.head()","7662b030":"# Rebuild the 'BankChurn' dataset with the OHE variables\nBankChurn = pd.concat([CategoricalData,OHE,NumericalData], axis=1)\ndisplay(BankChurn)","b2582b61":"BankChurn.isna().sum()","4593cdc6":"CategoricalVars = [\"Country\"]\nLogicalVars     = [\"IsMale\", \"HasCrCard\", \"IsActiveMember\", \"Churn\"]\nNumericalVars   = [\"CreditScore\", \"Age\", \"Tenure\", \"Balance\", \"NumOfProducts\", \"EstimatedSalary\"]","adb2585b":"Plot_Counts_For_Categorical_Variables(BankChurn,CategoricalVars)","36bb6fce":"for p in BankChurn[\"Country\"].unique():\n    print(\"--------------------\")\n    print(\"Country =\",p)\n    print(\"--------------------\")\n    Plot_Counts_For_Categorical_Variables(BankChurn[BankChurn[\"Country\"]==p],[\"Churn\"])","31745e21":"Cols = [\"CreditScore\", \"Age\", \"Tenure\", \"Balance\", \"NumOfProducts\", \"EstimatedSalary\"]\nBins = [50, 25, 25, 25, 20, 50]\n\n# HISTOGRAMS of each variable\nfor c in [1,0]:\n    print(\"--------------------\")\n    print(\"Churn =\",c)\n    print(\"--------------------\")\n    Fig, Ax = plt.subplots(nrows=1, ncols=6, sharex=False, sharey=False, figsize=(22,3))\n    k = 0\n    if c: Color=\"red\"\n    else: Color=\"blue\"\n    for p in Cols:\n        Ax[k].hist(BankChurn.loc[BankChurn[\"Churn\"]==c,p], bins=Bins[k], color=Color, alpha=0.65)\n        Ax[k].set_title(p, fontsize=14, color=Color)\n        k += 1\n    plt.show()\n\n# BOXPLOTS of each variable\nfor c in [1,0]:\n    print(\"--------------------\")\n    print(\"Churn =\",c)\n    print(\"--------------------\")\n    Fig, Ax = plt.subplots(nrows=1, ncols=6, sharex=False, sharey=False, figsize=(22,3))\n    k = 0\n    if c: Color=\"red\"\n    else: Color=\"blue\"\n    for p in Cols:\n        Ax[k].boxplot(BankChurn.loc[BankChurn[\"Churn\"]==c,p], vert=False)\n        Ax[k].set_title(p, fontsize=14, color=Color)\n        k += 1\n    plt.show()","83f724d9":"# Compute the correlation matrix\nCorr = BankChurn.corr(method=\"pearson\")\n\n# Generate a mask for the upper triangle\nMask = np.triu(np.ones_like(Corr,dtype=bool))\n\n# Plot the correlations\nFig, Ax = plt.subplots(figsize=(12,10))\nsns.heatmap(Corr, mask=Mask, cmap=\"RdBu\", vmin=-0.60, vmax=0.60, fmt=\"0.3f\", annot=True,\n            center=0.0, square=True, linewidths=0.5, cbar_kws={\"shrink\":0.5})\nplt.show()","b58f6d16":"ToDrop = [\"Country\", \"CreditScore\", \"Tenure\", \"NumOfProducts\", \"HasCrCard\", \"EstimatedSalary\"]\nBankChurn.drop(columns=ToDrop, axis=1, inplace=True)\n\ndisplay(BankChurn)","174c129e":"#----------------------------------------\nValidatSize   = 0.1\nTrainTestSize = 1.0 - ValidatSize\n\nIdxTrainTest, IdxValidat = train_test_split(BankChurn.index, train_size=TrainTestSize, test_size=ValidatSize, random_state=Seed)\nXYTrainTest = BankChurn.iloc[IdxTrainTest]\nXYValidat   = BankChurn.iloc[IdxValidat]\n\nXYTrainTest.reset_index(drop=True, inplace=True)\nXYValidat.reset_index(drop=True, inplace=True)\n\n#----------------------------------------\nTrainSize = 0.90\nTestSize  = 1.0 - TrainSize\n\nIdxTrain, IdxTest = train_test_split(XYTrainTest.index, train_size=TrainSize, test_size=TestSize, random_state=Seed)\nXYTrain = XYTrainTest.iloc[IdxTrain]\nXYTest  = XYTrainTest.iloc[IdxTest]\n\nXYTrain.reset_index(drop=True, inplace=True)\nXYTest.reset_index(drop=True, inplace=True)\n\ndel IdxTrainTest, IdxValidat, XYTrainTest, IdxTrain, IdxTest","704314c9":"# Separate the outcome variable (\"Churn\") from the income variables\nYTrain   =   XYTrain[\"Churn\"]\nYTest    =    XYTest[\"Churn\"]\nYValidat = XYValidat[\"Churn\"]\n\nXTrain   =   XYTrain.drop([\"Churn\"], axis=1, inplace=False)\nXTest    =    XYTest.drop([\"Churn\"], axis=1, inplace=False)\nXValidat = XYValidat.drop([\"Churn\"], axis=1, inplace=False)","bdddec64":"display(XTrain)","317997ea":"XTrain.dtypes","75bf0c3f":"ToScale    = [\"Age\", \"Balance\"]\nNotToScale = [x for x in XTrain.columns if x not in ToScale]\n\nprint(\"Variables to scale     :\",ToScale)\nprint(\"Variables not to scale :\",NotToScale)","f4af34f8":"# Set the scaler\nScaler = StandardScaler()","5ec3e7f9":"XTrainScaled = XTrain.copy()\n\n# Fit the scaler using the Training dataset and transform (scale) its values\nXTrainScaled[ToScale] = Scaler.fit_transform(XTrainScaled[ToScale])\n\nXTrainScaled.head()","01db4794":"XTestScaled = XTest.copy()\n\n# Apply to the Testing dataset the scaling parameters computed from the Training dataset.\n# Use \".transform\" not \".fit_transform\" !\nXTestScaled[ToScale] = Scaler.transform(XTestScaled[ToScale])\n\nXTestScaled.head()","71df231f":"XValidatScaled = XValidat.copy()\n\n# Apply to the Validation dataset the scaling parameters computed from the Training dataset.\n# Use \".transform\" not \".fit_transform\" !\nXValidatScaled[ToScale] = Scaler.transform(XValidatScaled[ToScale])\n\nXValidatScaled.head()","6db59829":"# Use the scaled datasets for modeling\n\nXTrain   =   XTrainScaled.copy()\nXTest    =    XTestScaled.copy()\nXValidat = XValidatScaled.copy()","82c58546":"Ni = XTrain.shape[1]   # Number of input variables","50359975":"Model1 = LogisticRegression(penalty=\"l2\", tol=0.0001, C=1.0, random_state=Seed, solver=\"lbfgs\", max_iter=1000)\nModel1.fit(XTrain,YTrain)\n\n# Compute the predictions\nYTrain_p, YTest_p, YValidat_p = Model_Predictions(Model1, XTrain,YTrain, XTest,YTest, XValidat,YValidat, op=None)","aa842362":"print(\"Score for Train      : {:6.3f}\".format(Model1.score(XTrain  ,YTrain  )))\nprint(\"Score for Test       : {:6.3f}\".format(Model1.score(XTest   ,YTest   )))\nprint(\"Score for Validation : {:6.3f}\".format(Model1.score(XValidat,YValidat)))","59e753ad":"CM_Train   = confusion_matrix(YTrain  , YTrain_p  )\nCM_Test    = confusion_matrix(YTest   , YTest_p   )\nCM_Validat = confusion_matrix(YValidat, YValidat_p)\n\nPlot_Confusion_Matrix(CM_Validat,\"Confusion Matrix for Validation\")","13bfe1f4":"YTrainProba   = Model1.predict_proba(XTrain  )[:,1]\nYTestProba    = Model1.predict_proba(XTest   )[:,1]\nYValidatProba = Model1.predict_proba(XValidat)[:,1]\n\nFPR, TPR, Thresholds = roc_curve(YValidat,YValidatProba)\nROC_AUC = auc(FPR,TPR)\n\nPlot_ROC_AUC(FPR,TPR,ROC_AUC)","1ceec8da":"Model1 = DecisionTreeClassifier(criterion=\"gini\", splitter=\"best\", max_depth=8, min_samples_split=2, min_samples_leaf=1, random_state=Seed)\nModel1.fit(XTrain,YTrain)\n\n# Compute the predictions\nYTrain_p, YTest_p, YValidat_p = Model_Predictions(Model1, XTrain,YTrain, XTest,YTest, XValidat,YValidat, op=None)","eb76c68c":"print(\"Score for Train      : {:6.3f}\".format(Model1.score(XTrain  ,YTrain  )))\nprint(\"Score for Test       : {:6.3f}\".format(Model1.score(XTest   ,YTest   )))\nprint(\"Score for Validation : {:6.3f}\".format(Model1.score(XValidat,YValidat)))","4eb3748b":"CM_Train   = confusion_matrix(YTrain  , YTrain_p  )\nCM_Test    = confusion_matrix(YTest   , YTest_p   )\nCM_Validat = confusion_matrix(YValidat, YValidat_p)\n\nPlot_Confusion_Matrix(CM_Validat,\"Confusion Matrix for Validation\")","4053ff4e":"YTrainProba   = Model1.predict_proba(XTrain  )[:,1]\nYTestProba    = Model1.predict_proba(XTest   )[:,1]\nYValidatProba = Model1.predict_proba(XValidat)[:,1]\n\nFPR, TPR, Thresholds = roc_curve(YValidat,YValidatProba)\nROC_AUC = auc(FPR,TPR)\n\nPlot_ROC_AUC(FPR,TPR,ROC_AUC)","37cf6341":"Parameters = {\n  \"hidden_layer_sizes\" : (Ni, int(np.ceil(Ni\/2))),\n  \"activation\"         : \"relu\",\n  \"solver\"             : \"adam\",\n  \"alpha\"              : 0.01,\n  \"max_iter\"           : 5000,\n  \"random_state\"       : Seed,\n  \"tol\"                : 1E-6,\n  \"verbose\"            : False,\n  \"warm_start\"         : False,\n}\n\nModel2 = MLPClassifier(**Parameters)\nModel2.fit(XTrain,YTrain)\n\n# Compute the predictions\nYTrain_p, YTest_p, YValidat_p = Model_Predictions(Model2, XTrain,YTrain, XTest,YTest, XValidat,YValidat, op=None)","0a57736f":"print(\"Score for Train      : {:6.3f}\".format(Model2.score(XTrain  ,YTrain  )))\nprint(\"Score for Test       : {:6.3f}\".format(Model2.score(XTest   ,YTest   )))\nprint(\"Score for Validation : {:6.3f}\".format(Model2.score(XValidat,YValidat)))","75e53740":"CM_Train   = confusion_matrix(YTrain  , YTrain_p  )\nCM_Test    = confusion_matrix(YTest   , YTest_p   )\nCM_Validat = confusion_matrix(YValidat, YValidat_p)\n\nCM_TrainP   = (100*CM_Train  \/np.sum(CM_Train)  ).round(2)\nCM_TestP    = (100*CM_Test   \/np.sum(CM_Test)   ).round(2)\nCM_ValidatP = (100*CM_Validat\/np.sum(CM_Validat)).round(2)\n\nPlot_Confusion_Matrix(CM_Validat,\"Confusion Matrix for Validation\")","6a558a44":"YTrainProba   = Model2.predict_proba(XTrain  )[:,1]\nYTestProba    = Model2.predict_proba(XTest   )[:,1]\nYValidatProba = Model2.predict_proba(XValidat)[:,1]\n\nFPR, TPR, Thresholds = roc_curve(YValidat,YValidatProba)\nROC_AUC = auc(FPR,TPR)\n\nPlot_ROC_AUC(FPR,TPR,ROC_AUC)","8e0a8cea":"# Define the model structure\ndef NN_Classifier():\n    # Use the 'sigmoid' activation function in the output layer, so that the model returns the probability of an input to belong to a binary class.\n    # Use the 'softmax' activation function in the output layer, for multiclass classification problems.\n    model = Sequential()\n    AF = [\"linear\",\"relu\"][0]\n    model.add(Dense(name=\"IL_and_HL1\", activation=AF       , use_bias=True, units=Ni, input_shape=(Ni,)))\n    model.add(Dense(name=       \"HL2\", activation=AF       , use_bias=True, units=Ni))\n    model.add(Dense(name=       \"HL3\", activation=AF       , use_bias=True, units=int(np.ceil(Ni\/2))))\n    model.add(Dense(name=        \"OL\", activation=\"sigmoid\", use_bias=True, units=1))\n    \n    # Compile the model\n    Opt = [\"adam\",\"sgd\"][0]\n    model.compile(optimizer=Opt, loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n    return model","6e0030a3":"Model3 = NN_Classifier()\nModel3.fit(XTrain, YTrain, validation_data=(XTest,YTest), batch_size=10, epochs=20, verbose=0)   # verbose: 0, 1, or 2\n\nYTrainP_p, YTestP_p, YValidatP_p = Model_Predictions(Model3, XTrain,YTrain, XTest,YTest, XValidat,YValidat, op=\"keras\")\n\nYTrain_p   =   YTrainP_p.map(lambda x: 0 if x<0.5 else 1)\nYTest_p    =    YTestP_p.map(lambda x: 0 if x<0.5 else 1)\nYValidat_p = YValidatP_p.map(lambda x: 0 if x<0.5 else 1)","028a4311":"TrainScore   = Model3.evaluate(XTrain  , YTrain  , verbose=0)\nTestScore    = Model3.evaluate(XTest   , YTest   , verbose=0)\nValidatScore = Model3.evaluate(XValidat, YValidat, verbose=0)\n\nprint(\"Score for Train      : {:6.3f}\".format(  TrainScore[1]))\nprint(\"Score for Test       : {:6.3f}\".format(   TestScore[1]))\nprint(\"Score for Validation : {:6.3f}\".format(ValidatScore[1]))","2ee018b4":"CM_Train   = confusion_matrix(YTrain  , YTrain_p  )\nCM_Test    = confusion_matrix(YTest   , YTest_p   )\nCM_Validat = confusion_matrix(YValidat, YValidat_p)\n\nCM_TrainP   = (100*CM_Train  \/np.sum(CM_Train)  ).round(2)\nCM_TestP    = (100*CM_Test   \/np.sum(CM_Test)   ).round(2)\nCM_ValidatP = (100*CM_Validat\/np.sum(CM_Validat)).round(2)\n\nPlot_Confusion_Matrix(CM_Validat,\"Confusion Matrix for Validation\")","d115d14a":"FPR, TPR, Thresholds = roc_curve(YValidat,YValidatP_p)\nROC_AUC = auc(FPR,TPR)\n\nPlot_ROC_AUC(FPR,TPR,ROC_AUC)","1bf8604e":"At this point, we have all the necessary datasets (Training, Testing, and Validation) to build Machine Learning models for the classification problem of bank churn.\n\nSeveral methods to build classification models are available (eg: Logisti Regression, Decision Trees, Neural Networks, SVM) from specific Python modules (Scikit-Learn, KERAS, PyTorch). Some models are developed in the following.","a7518e17":"The model attained a classification accuracy of about 0.8 for the Validation dataset. This means that 80% of the predictions are correct - a good result.","fd067e56":"It's convenient to separate the variables according to their types, to better visualize them:","23c3ad2a":"This report presented how **data analysis** and **machine learning** can be applied to the problem of predicting **customer churns**. It was demonstrated that even using a few number of variables, it can be possible to develop predictive models with a very good performance (accuracy and ROC-AUC metrics). **All the models developed attained an accuracy of about 0.81, wich means 81% of success in predicting customer churn**.\n\nThe models developed are **initial baseline models** which can be further improved to reduce the False Negatives Rate. Suggestions for improvements include:\n * **Balance the outcome variable** \"Churn\" so that it has similar number of occurrences for each value (0 or 1).\n\n * Use **Grid Search** to explore several combinations of parameter values for each model, to identify the ones that lead to better performance metrics.\n\n * Use additional variables (if available) to enhance the informative content about customer churns, for example:\n   - The time when the customer signed in the bank, and the time when he\/she left the bank (in case of churn).\n   - The average number of financial operations (payments, transfers, cash withdrawal, etc) made by the customer.\n   - The number and value of credit card operations made by the customer per month.\n\n\n * Derive new variables using Clustering techniques on the existing ones.\n\n * Improve the model fitting to reduce the False Negative Rate, without impairing significantly the good overall accuracy obtained.","c1fe13f0":"Similarly to the previous model, this model has a significant False Negatives Rate, and may be further improved.","4bc67a71":"### 5.1. Training, Testing, and Validation datasets","c209859d":"The variables \"RowNumber\", \"CustomerId\", and \"Surname\" are meaningless for the problem under study and should be dropped-off from the dataset.","8c8d2844":"<hr>\n\n## 1. Introduction","e7cedbb5":"Looking at the bottom row of the correlation plot (the row of the outcome variable **\"Churn\"**), we can verify that variable **\"Age\"** has the strongest correlation with \"Churn\", followed by the variables **\"IsActiveMember\"**, **\"Balance\"**, and **\"IsMale\"**.\n\nThe variables \"NumOfProducts\" and \"Balance\" have a strong negative correlation between themselves, but this is not relavant to the outcome variable \"Churn\".\n\nHence, we can drop some variables that looks less correlated to the outcome variable \"Churn\". The variable \"Country\" will also be dropped, but its corresponding One-Hot-Encoded variables will be kept:","44ef8eaa":"Finally, let's check if the dataset has missing values:","7ce2ef2a":"The model attained a classification accuracy of about 0.8 for the Validation dataset. This means that 80% of the predictions are correct - a good result.\n\nNext, let's look at the Confusion Matrix for the Validation data.","e547b891":"The ROC-AUC value for the Neural Network model is a little better (greater) than the previous models, since a neural network has more flexibility to be parameterized for the problem.","e0f0e13c":"Since the dataset is complete e and has no missing values, we can proceed analyzing the data to get insights about the variables and their relationships.","e5d1e568":"The model attained a classification accuracy of about 0.8 for the Validation dataset. This means that 80% of the predictions are correct - a good result.","d99c2a95":"Similarly to the previous model, this model has a significant False Negatives Rate, and may be further improved.","e2c759ff":"<hr>\n\n## 3. Customer Churn Data","a0da1d04":"<hr>\n\n## <font color='blue'>Using Machine Learning to Predict Customer Churn in Financial Institutions<\/font>\n### Sidney A.A. Viana\n\n#### Version 0.1","8b44082c":"The model attained a classification accuracy of about 0.8 for the Validation dataset. This means that 80% of the predictions are correct - a good result.","dc614add":"Again, a good ROC-AUC value was obtained, and indicates that the model performs very good in predicting customer churns.","58dbccd8":"The variable \"Gender\" has only two values (\"Male\", \"Female\"). To allow the use of this variable by Machine Learning algorithms that can only process numerical values, we need to convert \"Gender\" to a numerical logical variable \"IsMale\":","7aceb147":"The dataset to be used to develop a customer churn model is available on Klagge: \"Churn_Modelling.csv\". It comprises the following variables:\n\n* ***RowNumber***: The row number of each observation, corresponding to a specific customer. (meaningless variable)\n* ***CustomerId***: The customer unique identifying number. (meaningless variable)\n* ***Surname***: The customer surname. (meaningless variable)\n* ***CreditScore***: The customer credit rank in the bank. (**numerical discrete variable**)\n* ***Geography***: The customer country. (**categorical nominal variable**)\n* ***Gender***: The customer gender. (**categorical nominal variable**)\n* ***Age***: The customer age, in integer years. (**numerical discrete variable**)\n* ***Tenure***: The number of customer possessions. (**numerical discrete variable**)\n* ***Balance***: The value of the customer account balance. (**numerical discrete variable**)\n* ***NumOfProducts***: The number of financial products used by the customer. (**numerical discrete variable**)\n* ***HasCrCard***: Indicate if the customer has a credit card or not. (**logical variable**)\n* ***IsActiveMember***: Indicate if the customer is an active member of the financial institution. (**logical variable**)\n* ***EstimatedSalary***: Estimated salary of the customer. (**numerical discrete variable**)\n* ***Exited***: Indicate if the customer left the financial institution (churn). This is the **outcome** variable. (**logical variable**)","c06081f6":"The ROC-AUC can reach a maximum value of 1.0 (for a perfect classifier). The ROC-AUC value obtained indicates that the model has a good performance in predicting customer churns. This is a very good ROC-AUC value, considering the few number of variables used to build the model.","9eed780c":"The Confusion Matrix shows that most of the misclassification errors comes from the False Negatives Rate of about 16%. False Negatives are predictes values \"no churn\" (Churn = 0) when the actual values are \"churn\" (Churn = 1), that is, when the model fails to predict a true churn. This means that, although the model attained a good overall accuracy, it may be further improved to have a better (lower) False Negative Rate.","0f45260d":"Before we start to analyzing data, we first configure the necessary computing resources (Python modules and functions).","0d80cd68":"<hr>\n\n## 4. Exploratory Data Analysis","48c7f35d":"### Scale XTrain","7d218819":"The above pie chart shows that most of the bank customers come from France. However, this information is not very useful if used alone. It's better to visualize how the countries are related to the outcome variable \"Churn\".","70a77dae":"### 10. Concluding Remarks","204ccf56":"<hr>\n\n### 7. DECISION TREE Classifier from Scikit-Learn","e68b522f":"<hr>\n\n### 6. LOGISTIC REGRESSION Classifier from Scikit-Learn","c0af4e59":"Financial coporations like banks and insurance companies have great concerns about **customer loyalty**, and need to prevent customer **churns**.\n\nSome of the motivations to promote customer loyalty and prevent churns are:\n\n * *It is more expensive to acquire new clients than to keep the existing ones.*\n * *There is a very little chance that a client who left the financial institution returns in the future.*\n * *A client that left the institution is less likely to reccommend it for other people.*\n\nIt is of great importance for financial companies to develop effective **churn prevention strategies**. This naturally requires the analysis of large amounts of data about the customers and their usage of financial services. In this context, **Data Analytics** and **Machine Learning** methods can be used with advantage to develop effective churn prevention strategies.\n\nThis report presents a practical example of how to analyze a given bank churn data and to develop some **Machine Learning models to predict customer churn**. The predictive models can be further used to support marketing campaigns to promote customer loyalty and prevent churns.","72b4351b":"### Scale XValidat","3d602eac":"### 8. NEURAL NETWORK Classifier from Scikit-Learn","93a8a6fc":"The three pie charts above indicate that German customers are more likely to leave the bank (churn) compared to French and Spanish customers. This suggests that the customer country \nmay be a good income variable to predict the likelihood of churn.\n\nNext, let's visualize the relationship of the numerical variables and the outcome variable \"Churn\".","ce8301ab":"Similarly for the previous model, a good ROC-AUC value was obtained, indicating that the model has a good performance in predicting customer churns.","f170f905":"From the above plots, some insights can be drawn:\n * The distributions of the variable \"Age\" have some differences for Churn=1 and Churn=0, so that \"Age\" may be a good predictor for \"Churn\". The same insight looks valid for the variable \"Balance\".\n * The variable \"CreditScore\" has similar distributions for Churn=1 and Churn=0, and may not be a good predictor for \"Churn\". The same insight looks valid for the variables \"Tenure\", \"NumOfProducts\", and \"EstimatedSalary\".\n\nAs an additional investigation, let's visualize the **correlation** between the variables.","93fc65a3":"The first step in a Data Analysis & Machine Learning process is to explore the data to get a deeper understanding about them, as well as to verify the existence of some relevant patterns and relationships between the variables.","13a38f18":"<hr>\n\n## 2. Computing Setup","d0262c84":"When preparing data to build predictive models, we need to be aware to the range of values (scale) of each variable, because the numerical optimization methods used by certain Machine Learning algorithms are sensitive to the scale of the variables and may not converge. Therefore, it's a good practice to perform **Feature Scaling** to normalize the range of values of the variables.\n\nFeature Scaling must be computed only on the **Training** dataset, and the resulting scale parameters are applied to the Testing and Validation datasets.","7bb008cc":"The variable \"Geography\" represents the country of the customer, and it has three values (\"France\", \"Germany\", \"Spain\").\n\nSimilarly to the former variable \"Gender\", it is convenient to transform \"Geography\" into a numerical variable. Two options can be used:\n * Assign three distinct integer numbers to each country, for example: France=1, Germany=2, and Spain=3; or France=1, Germany=0, and Spain=-1. The problem with this alternative is that the countries have NO meaning of order between themselves, but the assigned numbers will have this meaning.\n * Use **\"One-Hot-Encoding\" (OHE)** to assign a set of logical numbers (0 or 1) to each country. This alternative may be more convenient because it preserves the logical nature of the country values (*is it \"France\" or not?*)\n\nIn the folowing, we will rename the variable \"Geography\" as **\"Country\"** and then compute its One-Hot-Encoded values. For convenience, the outcome variable \"Exited\" will be renamed as **\"Churn\"**.","dca52165":"### 5.2. Feature Scaling","0fc1b691":"### 9. NEURAL NETWORK Classifier from KERAS","bffaa345":"Note that there are different types of variables. Depending on the kind of predictive model to be developed, it may be necessary to convert some variables to other convenient types to allow specific computations. For example, the categorical variable \"Gender\" has two values (\"Male\", \"Female\") and may be converted to a numerical variable with two values so that it can be used by some algorithms.","27607e01":"### Scale XTest","93caa89c":"When building Machine Learning models, it's necessary to split the data into some subsets:\n * A **Training** dataset to train (fit) the model parameters.\n * A **Testing** dataset to test (validate) the training process of the model.\n * A **Validation** dataset to validate the performance of the trained model as if it would be put into operation.\n\nThose datasets will be created by spliting the data.","2e65950b":"<hr>\n\n### 5. Data Pre-Processing"}}