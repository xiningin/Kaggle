{"cell_type":{"18e14f1d":"code","0efe52ec":"code","1a66f630":"code","9e373e23":"code","8fd1e0f2":"code","c2ee0c3e":"code","9e2b9c01":"code","3984134a":"code","8fd0dd7e":"code","11f43c5f":"code","433d9415":"code","657607e7":"code","73140f77":"code","3ac48efe":"code","47847659":"code","f2b03a8f":"code","db64a82d":"code","cfdbc718":"code","6e82f476":"code","dbd61277":"code","d54d8097":"code","8f8d3e6d":"code","b2c0eae3":"code","8cd196ff":"code","0d886b3e":"code","07b2fc29":"code","734f8252":"code","85cb45d9":"code","1635f6f4":"code","65459e8c":"code","3e6bbcc7":"code","4689f2de":"code","c9fc73e7":"code","d8850c34":"code","39fe3e76":"code","8ed53b10":"code","196d995a":"code","8e16a015":"code","bb8eb1aa":"markdown","f134f053":"markdown","a2d798fc":"markdown","2ce01d95":"markdown","7f4fcd5e":"markdown","61d29d56":"markdown","25c8ce12":"markdown","68dcb490":"markdown","54666f68":"markdown","fab0b6b8":"markdown","7fb49ed8":"markdown","8549b25e":"markdown","0aec1152":"markdown"},"source":{"18e14f1d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","0efe52ec":"df = pd.read_csv('..\/input\/quora-question-pairs\/train.csv.zip')","1a66f630":"df.head(10)","9e373e23":"df.info()","8fd1e0f2":"df.describe()","c2ee0c3e":"df.groupby('is_duplicate')['id'].count().plot.bar()","9e2b9c01":"#%config Completer.use_jedi = False","3984134a":"print(f'Total number of question pairs is : {len(df)}')","8fd0dd7e":"print('Total Percentage of questions that are not similar is: {}'.format(round(100-df['is_duplicate'].mean()*100,2)))\nprint('Total Percentage of questions that are similar is: {}'.format(round(df['is_duplicate'].mean()*100,2)))","11f43c5f":"df['is_duplicate'].mean()","433d9415":"quids = pd.Series(df['qid1'].tolist() + df['qid2'].tolist())\nquids\nunique_qs = np.unique(quids)\nlen_unique_qs = len(unique_qs)\nprint('Length of total questions are:',len(quids))\nprint('Toal number of unique questions are :',len_unique_qs)\nqs_morethan_one = np.sum(quids.value_counts()>1)\nprint('Question appearing more than once are:',(qs_morethan_one))\nprint('Percentage of Question appearing more than once are:',(qs_morethan_one\/len(unique_qs))*100)\nprint('Max number of times a single question is asked:',max(quids.value_counts()))","657607e7":"q_vals=quids.value_counts()\n\nq_vals=q_vals.values","73140f77":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\nx = [\"unique_questions\" , \"Repeated Questions\"]\ny =  [len_unique_qs , qs_morethan_one]\n\nplt.figure(figsize=(10,6))\nplt.title('Plot representing unique and repeated questions')\nsns.barplot(x,y)\nplt.show()","3ac48efe":"#checking whether there are any repeated pair of questions\n\npair_duplicates = df[['qid1','qid2','is_duplicate']].groupby(['qid1','qid2']).count().reset_index()\n\nprint (\"Number of duplicate questions\",(pair_duplicates).shape[0] - df.shape[0])","47847659":"plt.figure(figsize=(20,10))\nplt.hist(quids.value_counts(),bins=160)\nplt.yscale('log',nonposy='clip')\nplt.title('Log-Histogram of question apperance counts ')\nplt.xlabel('Number of occurance of questions')\nplt.ylabel('Number of questions')\nprint ('Maximum number of times a single question is repeated: {}\\n'.format(max(quids.value_counts()))) \nplt.show()","f2b03a8f":"df.isnull().sum()","db64a82d":"df=df.fillna('')\ndf.isnull().sum()","cfdbc718":"df[df['qid1']==3]","6e82f476":"# Freqeuncey of qid1 and qid2\n\ndf['qid1_freq'] = df.groupby('qid1')['qid1'].transform('count')\ndf['qid2_freq'] = df.groupby('qid2')['qid2'].transform('count')\n\n#Length of question1 and question2\n\ndf['q1_len'] = df['question1'].str.len()\ndf['q2_len'] = df['question2'].str.len()\n\n#Number of words in question1 and question2\n\ndf['q1_words'] = df['question1'].apply(lambda x:len(x.split(' ')))\ndf['q2_words'] = df['question2'].apply(lambda x:len(x.split(' ')))","dbd61277":"#common words in question1 and question2\ndef common_words(row):\n    q1_common = set(map(lambda x: x.lower().strip(),row['question1'].split(' ')))\n    q2_common = set(map(lambda x: x.lower().strip(),row['question2'].split(' ')))\n    return 1.0 * len(q1_common & q2_common)\n\ndf['len_common_words'] = df.apply(common_words,axis=1)","d54d8097":"#Total number of words in question1 and question2\n\ndef word_length(row):\n    w1 = set(map(lambda x: x ,row['question1'].split(' ')))\n    w2 = set(map(lambda x: x ,row['question2'].split(' ')))\n    \n    return 1.0 * (len(w1) + len(w2))\n\ndf['q1_q2_word_length'] = df.apply(word_length,axis=1)","8f8d3e6d":"#word share of question1 and question2\n\ndf['word_share'] = df['len_common_words'] \/ df['q1_q2_word_length']\n\n# qid1 and qid2 frequency sum\n\ndf['qid1_qid2_freq'] = df['qid1_freq'] + df['qid2_freq']\n\n# difference of qid1 and qid2 frequency\n\ndf['diff_qid1_qid2'] = df['qid1_freq'] - df['qid2_freq']","b2c0eae3":"#save the preprocessed file\ndf.to_csv('df_fe_without_preprocessing_train.csv',index=False)","8cd196ff":"df.head()","0d886b3e":"print (\"Minimum length of the questions in question1 : \" , min(df['q1_len']))\n\nprint (\"Minimum length of the questions in question2 : \" , min(df['q2_len']))\n\nprint (\"Number of Questions with minimum length [question1] :\", df[df['q1_len']== 1].shape[0])\n\nprint (\"Number of Questions with minimum length [question2] :\", df[df['q2_len']== 1].shape[0])","07b2fc29":"plt.figure(figsize=(12, 8))\n\nplt.subplot(1,2,1)\nsns.violinplot(x = 'is_duplicate', y = 'word_share', data = df[0:])\n\nplt.subplot(1,2,2)\nsns.distplot(df[df['is_duplicate'] == 1.0]['word_share'][0:],label = 1,color = 'red')\nsns.distplot(df[df['is_duplicate'] == 0.0]['word_share'][0:],label = 0,color = 'black')","734f8252":"plt.figure(figsize=(12, 8))\n\nplt.subplot(1,2,1)\nsns.violinplot(x = 'is_duplicate', y = 'len_common_words', data = df[0:])\n\nplt.subplot(1,2,2)\nsns.distplot(df[df['is_duplicate'] == 1.0]['len_common_words'][0:] , label = \"1\", color = 'red')\nsns.distplot(df[df['is_duplicate'] == 0.0]['len_common_words'][0:] , label = \"0\" , color = 'blue' )\nplt.show()","85cb45d9":"import tensorflow as tf\nimport tensorflow_hub as hub\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport os\nimport re\nimport keras.layers as layers\nfrom keras.models import Model\nfrom keras import backend as K\nnp.random.seed(10)\ntf.compat.v1.disable_v2_behavior() ","1635f6f4":"# enabling the pretrained model for trainig our custom model using tensorflow hub\nmodule_url = \"https:\/\/tfhub.dev\/google\/universal-sentence-encoder-large\/5\"\nembed = hub.load(module_url)\n\n# creating a method for embedding and will using method for every input layer \ndef UniversalEmbedding(x):\n    return embed(tf.squeeze(tf.cast(x,tf.string)))\n \n","65459e8c":"# Taking the question1 as input and ceating a embedding for each question before feed it to neural network\n\ndropout = 0.1\nq1 = layers.Input(shape=(1,),dtype=tf.string)\nembeding_q1 = layers.Lambda(UniversalEmbedding,output_shape=(512,))(q1)\n\n# Taking the question2 as input and ceating a embedding for each question before feed it to neural network\nq2 = layers.Input(shape=(1,),dtype=tf.string)\nembeding_q2 = layers.Lambda(UniversalEmbedding,output_shape=(512,))(q2)\n\n# Concatenating the both input layer\nmerged = layers.concatenate([embeding_q1,embeding_q2])\nmerged = layers.Dense(200,activation='relu')(merged)\nmerged = layers.Dropout(dropout)(merged)\n\n# Normalizing the input layer,applying dense and dropout  layer for fully connected model and to avoid overfitting \nmerged = layers.BatchNormalization()(merged)\nmerged = layers.Dense(200,activation='relu')(merged)\nmerged = layers.Dropout(dropout)(merged)\n\nmerged = layers.BatchNormalization()(merged)\nmerged = layers.Dense(200,activation='relu')(merged)\nmerged = layers.Dropout(dropout)(merged)\n\nmerged = layers.BatchNormalization()(merged)\nmerged = layers.Dense(200,activation='relu')(merged)\nmerged = layers.Dropout(dropout)(merged)\n\n# Using the Sigmoid as the activation function and binary crossentropy for binary classifcation as 0 or 1\nmerged = layers.BatchNormalization()(merged)\npred = layers.Dense(2,activation='sigmoid')(merged)\nmodel = Model(inputs=[q1,q2],outputs=pred)\nmodel.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\nmodel.summary()","3e6bbcc7":"from tensorflow.keras.utils import plot_model\nplot_model(model)","4689f2de":"# Pushing all the strings to a list and converting to ndarray\nq1= df.question1.tolist()\nq2= df.question2.tolist()\nlabels= df.is_duplicate.tolist()","c9fc73e7":"from sklearn.model_selection import train_test_split\nX1 = df['question1']\nX2 = df['question2']\ny = df['is_duplicate']\n# Using the sklearn to split data in question1 and question2 train and test in the ration 80-20 %\nX1_train, X1_test,X2_train, X2_test, y_train, y_test = train_test_split(X1, X2, y, test_size=0.2, random_state=42)","d8850c34":"train_q1 = X1_train.tolist()\ntrainq1 = np.array(train_q1,dtype=object)[:,np.newaxis]\ntrain_q2 = X2_train.tolist()\ntrainq2 = np.array(train_q2,dtype=object)[:,np.newaxis]\n\ntrain_labels = np.asarray(pd.get_dummies(y_train),dtype=np.int8)\n\ntest_q1 = X1_test.tolist()\ntest_q1 = np.array(test_q1, dtype=object)[:, np.newaxis]\ntest_q2 = X2_test.tolist()\ntest_q2 = np.array(test_q2, dtype=object)[:, np.newaxis]\n\ntest_labels = np.asarray(pd.get_dummies(y_test), dtype = np.int8)\n","39fe3e76":"from keras.callbacks import ModelCheckpoint\ntf.compat.v1.disable_v2_behavior() \n\nwith tf.compat.v1.Session() as session:\n    tf.compat.v1.keras.backend.set_session(session)\n    session.run(tf.compat.v1.global_variables_initializer())\n    session.run(tf.compat.v1.tables_initializer())\n    \n    filepath = 'model-{epoch:02d}-{val_acc:.2f}.hdf5'\n    checkpoint = ModelCheckpoint(filepath,monitor = 'val_loss',save_best_only=False,save_weights_only=True,mode='auto',period=1)\n    callback_list = [checkpoint]\n    \n    history = model.fit([trainq1,trainq2],train_labels,\n                        validation_data=([test_q1,test_q2],test_labels),\n                        epochs=10,\n                        batch_size=512,callbacks=callback_list)\n    model.save('final_model.h5')","8ed53b10":"import numpy as np\nimport tensorflow as tf\n\ntf.compat.v1.disable_v2_behavior()\nq1 = input(\"Type Question 1 here -->\")\nq2 = input(\"Type Question 2 here -->\") \nq1 = np.array([[q1],[q1]])\nq2 = np.array([[q2],[q2]])\n\n# Using the same tensorflow session for embedding the test string\nwith tf.compat.v1.Session() as session:\n    tf.compat.v1.keras.backend.set_session(session)\n    session.run(tf.compat.v1.global_variables_initializer())\n    session.run(tf.compat.v1.tables_initializer())\n    model.load_weights('.\/model-09-0.86.hdf5')\n    predicts = model.predict([q1,q2],verbose= 0)\n    predict_logits = predicts.argmax(axis=1)\n    print(\"----FINAL RESULT----\")\n    if(predict_logits[0] == 1):\n        print(\"****Questions are Similar****\")\n    else:\n        print(\"****Questions are not Similar****\")","196d995a":"import numpy as np\nimport tensorflow as tf\n\ntf.compat.v1.disable_v2_behavior()\nq1 = input(\"Type Question 1 here -->\")\nq2 = input(\"Type Question 2 here -->\") \nq1 = np.array([[q1],[q1]])\nq2 = np.array([[q2],[q2]])\n\n# Using the same tensorflow session for embedding the test string\nwith tf.compat.v1.Session() as session:\n    tf.compat.v1.keras.backend.set_session(session)\n    session.run(tf.compat.v1.global_variables_initializer())\n    session.run(tf.compat.v1.tables_initializer())\n    model.load_weights('.\/model-09-0.86.hdf5')\n    predicts = model.predict([q1,q2],verbose= 0)\n    predict_logits = predicts.argmax(axis=1)\n    print(\"----FINAL RESULT----\")\n    if(predict_logits[0] == 1):\n        print(\"****Questions are Similar****\")\n    else:\n        print(\"****Questions are not Similar****\")","8e16a015":"import numpy as np\nimport tensorflow as tf\n\ntf.compat.v1.disable_v2_behavior()\nq1 = input(\"Type Question 1 here -->\")\nq2 = input(\"Type Question 2 here -->\") \nq1 = np.array([[q1],[q1]])\nq2 = np.array([[q2],[q2]])\n\n# Using the same tensorflow session for embedding the test string\nwith tf.compat.v1.Session() as session:\n    tf.compat.v1.keras.backend.set_session(session)\n    session.run(tf.compat.v1.global_variables_initializer())\n    session.run(tf.compat.v1.tables_initializer())\n    model.load_weights('.\/model-09-0.86.hdf5')\n    predicts = model.predict([q1,q2],verbose= 0)\n    predict_logits = predicts.argmax(axis=1)\n    print(\"----FINAL RESULT----\")\n    if(predict_logits[0] == 1):\n        print(\"****Questions are Similar****\")\n    else:\n        print(\"****Questions are not Similar****\")","bb8eb1aa":"# Question duplicates\n\nThe task of identifying duplicated questions can be viewed as an instance of the paraphrase identification problem, which is a well-studied NLP task that uses natural language sentence matching\n(NLSM) to determine whether two sentences are paraphrase or not (2). This task has wide array of\nuseful NLP application. For example, in question-and-answer (QA) forums, there are vast numbers\nof duplicate questions. Identifying these duplicates and consolidating their answers increases the\nefficiency of such QA forums. Moreover, identifying questions with the same semantic content could\nhelp web-scale question answering systems that are increasingly concentrating on retrieving focused\nanswers to users\u2019 queries.\n\n\nIn this project, we focus on a dataset published by Quora.com containing over 400K annotated\nquestion pairs containing binary paraphrase labels.1\n. We believe that this dataset presents a great\nopportunity for the NLP practitioners tue to its scale and quality; it can result in systems that accurately\nidentify duplicate questions, thus increasing the quality of many QA forums.","f134f053":"### Data Overview\n\n- Data will be in a file Train.csv <br>\n- Train.csv contains 5 columns : qid1, qid2, question1, question2, is_duplicate \n- Number of rows in Train.csv = 404,290","a2d798fc":"<p>Quora is a place to gain and share knowledge\u2014about anything. It\u2019s a platform to ask questions and connect with people who contribute unique insights and quality answers. This empowers people to learn from each other and to better understand the world.<\/p>\n<p>\nOver 100 million people visit Quora every month, so it's no surprise that many people ask similarly worded questions. Multiple questions with the same intent can cause seekers to spend more time finding the best answer to their question, and make writers feel they need to answer multiple versions of the same question. Quora values canonical questions because they provide a better experience to active seekers and writers, and offer more value to both of these groups in the long term.\n<\/p>\n<br>\n> Credits: Kaggle \n","2ce01d95":"## Using Universal Sentence Encoder - For creating embedding of text at sentence level","7f4fcd5e":"# Training the Model for Detecting question duplicates","61d29d56":"# Training the Model","25c8ce12":"<h3>Checking for Duplicates <\/h3>","68dcb490":"__ Problem Statement __\n- Identify which questions asked on Quora are duplicates of questions that have already been asked. \n- This could be useful to instantly provide answers to questions that have already been answered. \n- We are tasked with predicting whether a pair of questions are duplicates or not. \"","54666f68":"We are given a minimal number of data fields here, consisting of:\n\n- id:  Looks like a simple rowID\n- qid{1, 2}:  The unique ID of each question in the pair\n- question{1, 2}:  The actual textual contents of the questions.\n- is_duplicate:  The label that we are trying to predict - whether the two questions are duplicates of each other.","fab0b6b8":"There are three null values we can fill with empty string","7fb49ed8":"# Basic Feature Extraction (before cleaning)\n\nLet us now construct a few features like:\n - ____freq_qid1____ = Frequency of qid1's\n - ____freq_qid2____ = Frequency of qid2's \n - ____q1len____ = Length of q1\n - ____q2len____ = Length of q2\n - ____q1_n_words____ = Number of words in Question 1\n - ____q2_n_words____ = Number of words in Question 2\n - ____word_Common____ = (Number of common unique words in Question 1 and Question 2)\n - ____word_Total____ =(Total num of words in Question 1 + Total num of words in Question 2)\n - ____word_share____ = (word_common)\/(word_Total)\n - ____freq_q1+freq_q2____ = sum total of frequency of qid1 and qid2 \n - ____freq_q1-freq_q2____ = absolute difference of frequency of qid1 and qid2 ","8549b25e":"# Analysis of some of the extracted features","0aec1152":"# Word Common"}}