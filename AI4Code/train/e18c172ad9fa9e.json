{"cell_type":{"ca592261":"code","06e44a26":"code","90d1f0cd":"code","fdb1f19f":"code","3d72707e":"code","07c7b1c8":"code","80cd0074":"code","d29fe39a":"code","1e68ab17":"code","32b85a07":"code","aa24ac44":"code","bd15494c":"code","af8bbc55":"code","b013b3d4":"code","0cc0ba1c":"markdown","f696f87b":"markdown","f96ed96c":"markdown","f32a22e4":"markdown","e6a2bf1d":"markdown","de3fdd3a":"markdown","6ef45101":"markdown","52cbfd18":"markdown","096792a5":"markdown","efa2e11f":"markdown","006416f8":"markdown","d3db0d16":"markdown","80d77237":"markdown","982663f4":"markdown","3d33a41d":"markdown","abf010ad":"markdown","fc2e3bcd":"markdown"},"source":{"ca592261":"import torch\n\ntorch.manual_seed(2022)  # set a fixed random seed, so our examples for 32-bit and 1-bit use the same values\n\nbatch_size = 2\nnum_inputs = 5\nnum_outputs = 4\n\nw = torch.randn(num_outputs, num_inputs, requires_grad=True)\ni = torch.randn(batch_size, num_inputs, requires_grad=True)\n\ndef toy_train_one_iteration(toy_w, toy_i, optimize_params):\n    optimizer = torch.optim.SGD(optimize_params, lr=0.1)\n    \n    def forward_and_print_loss(label):\n        output = torch.tensordot(toy_i, toy_w.T, dims=1)\n        loss = output.square().sum()  # use squared sum as a loss, which optimizes towards 0 for all outputs\n        print(f\"- Inputs ({label}):\\n\", toy_i.data)\n        print(f\"- Weights ({label}):\\n\", toy_w.data)\n        print(f\"- Output ({label}):\\n\", output.data)\n        print(f\"- Loss ({label}): {loss.item():.2f}\")\n        return loss\n    \n    loss = forward_and_print_loss(\"before\")\n    loss.backward()\n    optimizer.step()\n    print(\"\\nOptimize one step.\\n\")\n    forward_and_print_loss(\"after\")\n\ntoy_train_one_iteration(w, i, optimize_params=[w])","06e44a26":"torch.manual_seed(2022)  # set a fixed random seed, so our examples for 32-bit and 1-bit use the same values\n\nw = torch.randn(num_outputs, num_inputs, requires_grad=True)\ni = torch.randn(batch_size, num_inputs, requires_grad=True)\n\nprint(f\"- 32-bit Weights (before):\\n\", w.data)\n\nbinary_w = w.sign()\nbinary_i = i.sign()\n\ntoy_train_one_iteration(binary_w, binary_i, optimize_params=[w])\n\nprint(f\"- 32-bit Weights (after):\\n\", w.data)","90d1f0cd":"from typing import List, Callable\n        \nimport numpy as np\nimport matplotlib.pyplot as plt\nimport torch\n\n\ndef calculate_numeric_gradient(x_values: np.array, y_values: np.array, x_step_size: float):\n    # :param x_values: a list of equidistant x values\n    # :param y_values: the list of corresponding y values for an unknown function\n    # :param x_step_size: the fixed distance between two consecutive x values\n    # :return: a list of numeric gradients for each step in the function (with one less element than the given x_values)\n\n    # initialize a numpy array with the correct size\n    numeric_gradient = np.zeros(len(x_values) - 1)\n\n    # TODO: calculate the numeric gradient between each consecutive pair of x values based on the given y values\n    # TASK START - Start coding here:\n    raise NotImplementedError()\n    # TASK END\n    return numeric_gradient\n\n\ndef reconstruct_gradient_function(x_grad_values: np.array, x_step_size: float):\n    # :param x_grad_values: a list of calculated gradients (based on step size `x_step_size`)\n    # :param x_step_size: the distance between two consecutive x values that was used to calculate `x_grad_values`\n    # :return: a list of y values for a (representative) function that would create the given gradient values\n    #          (the y_values should be zero at the middle element)\n\n    # intialize an array with the correct size\n    y_values = np.zeros(len(x_grad_values))\n\n    # TODO: construct a list of y values for a (representative) function that would create the given gradient values\n    # HINT: start with any arbitrary value, create y_values step by step, finally subtract something from all values so the middle element becomes zero\n    # TASK START - Start coding here:\n    raise NotImplementedError()\n    # TASK END\n    return y_values\n\n\ndef plot_functions(functions_to_plot: List[Callable[[torch.tensor], torch.tensor]], xlim = (-1.5, 1.5), ylim = (-2.5, 2.5), steps = 100):\n    x_values = np.linspace(*xlim, steps)\n    x_step_size = (xlim[1] - xlim[0]) \/ steps\n    fig, axes = plt.subplots(len(functions_to_plot), 4, figsize=(16, len(functions_to_plot) * 3), subplot_kw={\"ylim\": ylim, \"xlim\": xlim})\n    if len(functions_to_plot) == 1:\n        axes = [axes]\n    # create the actual plot\n    for fn, axes_per_function in zip(functions_to_plot, axes):\n        x = torch.tensor(x_values, requires_grad=True)\n        y = fn(x)\n        y.sum().backward()\n\n        y_values = y.detach().numpy()\n        \n        numeric_gradient = calculate_numeric_gradient(x_values, y_values, x_step_size)\n        numeric_grad_function = reconstruct_gradient_function(x.grad, x_step_size)\n\n        axes_per_function[0].plot(x_values,      y_values,              label=f\"{fn.__name__} (function)\",           color=\"black\")\n        axes_per_function[1].plot(x_values[:-1], numeric_gradient,      label=f\"{fn.__name__} (numeric gradient)\",  color=\"darkred\")\n        axes_per_function[2].plot(x_values,      x.grad,                label=f\"{fn.__name__} (defined gradient)\",   color=\"black\")\n        axes_per_function[3].plot(x_values,      numeric_grad_function, label=f\"{fn.__name__} (gradient function)\", color=\"darkred\")\n        for axis in axes_per_function:\n            axis.axvline(x=0, c=\"lightgrey\", zorder=0)\n            axis.axhline(y=0, c=\"lightgrey\", zorder=0)\n            axis.legend()\n\n    plt.show()\n\n\ndef identity(x: torch.tensor):\n    return x\n\ndef square(x: torch.tensor):\n    return x * x\n\ndef cube(x: torch.tensor):\n    return x * x * x\n\ndef absolute(x: torch.tensor):\n    return x.abs()\n\n\nplot_functions([identity, square, cube, absolute])","fdb1f19f":"def sign(x: torch.tensor):\n    return x.sign()\n\nplot_functions([sign])","3d72707e":"from torch.nn import Module\nfrom torch.autograd.function import Function\n\nclass DifferentiableSignFunction(Function):\n    \n    @staticmethod\n    def forward(ctx, input_tensor, gradient_clipping_threshold):\n        ctx.save_for_backward(input_tensor, torch.tensor(gradient_clipping_threshold, device=input_tensor.device, dtype=input_tensor.dtype))\n\n        # Task 2a) - Forward\n        # TODO: implement the forward pass of the sign function (remember this function should not return 0, instead all 0s should become 1)\n        # TASK START - Start coding here:\n        raise NotImplementedError()\n        # TASK END\n        return input_tensor\n    \n    @staticmethod\n    def backward(ctx, output_gradient):\n        input_tensor, gradient_cancellation_threshold = ctx.saved_tensors\n\n        # Task 2a) - Backward\n        # TODO: implement the straight through estimator backward pass\n        # HINTs: \n        #     1) set the gradients (of output_gradient) to zero where the values of the input tensor are outside the range of [-theshold, threshold]\n        #     2) since two input values were passed to the forward function, the backward function expects two return values\n        #        but the second value would be the gradient for the cancellation threshold (which is a constant), you can return None\n        # TASK START - Start coding here:\n        raise NotImplementedError()\n        # TASK END\n        # HINT: the last line should look like this (of course you should use your own variable name):\n        # return gradients_cancelled, None\n\n\ndef differentiable_sign_function(input_tensor, gradient_cancellation_threshold):\n    # Task 2b)\n    # TODO: implement a sign function where you manipulate the computational graph (remember this function should not return 0, instead all 0s should become 1)\n    # HINTs: \n    #     1) to replace the backward pass of one part of the computational graph (attached to tensor a) with another one (attached to tensor b)\n    #        we can compute: (a - b).detach() + b\n    #        during the forward pass the result is a but during the backward pass (a - b) is removed from the graph, so the backward pass is only based on b\n    # TASK START - Start coding here:\n    raise NotImplementedError()\n    # TASK END\n    return None\n\n\nclass SignQuantization(Module):\n    def __init__(self, gradient_clipping_threshold=1.0):\n        super().__init__()\n        self.gradient_clipping_threshold = gradient_clipping_threshold\n    \n    def forward(self, input_tensor):\n        sign_solution_b = differentiable_sign_function(input_tensor, self.gradient_clipping_threshold)\n        if sign_solution_b is not None:\n            return sign_solution_b\n        return DifferentiableSignFunction.apply(input_tensor, self.gradient_clipping_threshold)","07c7b1c8":"def sign_ste(x: torch.tensor):\n    return SignQuantization()(x)\n\nplot_functions([sign_ste])","80cd0074":"sign_quantization = SignQuantization()\n\ntest_data = torch.tensor([-3, -2, -1, 0, 1, 2, 3], requires_grad=True, dtype=torch.float)\nexpected_result = torch.tensor([-1, -1, -1, 1, 1, 1, 1], dtype=torch.float)\nexpected_gradient = torch.tensor([0, 0, 1, 1, 1, 0, 0], dtype=torch.float)\n\nresult = sign_quantization(test_data)\nassert torch.allclose(expected_result, result.detach())\n\nloss = result.sum()\nloss.backward()\nassert torch.allclose(expected_gradient, test_data.grad)\nprint(\"Test passed.\")","d29fe39a":"import math\nimport pickle\nimport statistics\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision.transforms as tt\nimport imgaug\n\nfrom collections import defaultdict\nfrom typing import Type, List, Union\n\nfrom imgaug import augmenters as iaa\nfrom pathlib import Path\nfrom PIL import Image\nfrom tqdm.notebook import tqdm, trange\nfrom torch.optim import Optimizer\nfrom torch.optim.lr_scheduler import _LRScheduler, OneCycleLR\nfrom torch.utils.data import DataLoader, Dataset\n\n\nclass CIFAR100(Dataset):\n    \n    def __init__(self, dataset_path: Path, image_transforms: tt.Compose, image_augmentations: Union[None, Type[iaa.Augmenter]] = None):\n        super().__init__()\n        data = pickle.load(dataset_path.open(\"rb\"), encoding=\"bytes\")\n        self.images = data[b\"data\"]\n        self.labels = data[b\"fine_labels\"]\n        \n        self.image_transforms = image_transforms\n        self.image_augmentations = image_augmentations\n        \n        assert len(self.images) == len(self.labels), \"Number of images and labels is not equal!\"\n        \n    def __len__(self) -> int:\n        return len(self.images)\n    \n    def __getitem__(self, index: int) -> tuple:\n        image = self.images[index]\n        label = self.labels[index]\n        \n        image = np.reshape(image, (3, 32, 32))\n        image = np.transpose(image, (1, 2, 0))\n        \n        if self.image_augmentations is not None:\n            image = self.image_augmentations.augment_image(image)\n        image = self.image_transforms(Image.fromarray(image))\n        return image, label\n    \n\nimage_transformations = tt.Compose([\n    tt.ToTensor(),\n    tt.Normalize(\n        mean=(0.5074, 0.4867, 0.4411),\n        std=(0.2011, 0.1987, 0.2025)\n    )\n])\n\ntrain_augmentations = iaa.Sequential([\n    iaa.Fliplr(0.5),\n    iaa.CropAndPad(px=(-4, 4), pad_mode=\"reflect\")\n])\n\n    \ndef accuracy(predictions: torch.Tensor, labels: torch.Tensor, reduce_mean: bool = True) -> torch.Tensor:\n    predicted_classes = torch.argmax(F.softmax(predictions, dim=1), dim=1)\n    correct_predictions = torch.sum(predicted_classes == labels)\n    if reduce_mean:\n        return correct_predictions \/ len(labels)\n    return correct_predictions\n\n\ndef test_model(network: Type[nn.Module], data_loader: DataLoader) -> float:\n    num_correct_predictions = 0\n    device = get_device()\n    \n    for images, labels in data_loader:\n        images = to_device(images, device)\n        labels = to_device(labels, device)\n        predictions = network(images)\n        num_correct_predictions += float(accuracy(predictions, labels, reduce_mean=False).item())\n        \n    return num_correct_predictions \/ len(data_loader.dataset)\n\n\ndef get_device() -> torch.device:\n    if torch.cuda.is_available():\n        return torch.device(\"cuda\")\n    return torch.device(\"cpu\")\n\n\ndef to_device(data: torch.Tensor, device: torch.device) -> torch.Tensor:\n    if isinstance(data, (list, tuple)):\n        return [to_device(x, device) for x in data]\n    return data.to(device)\n\n\nBATCH_SIZE = 128\ntrain_dataset = CIFAR100(Path(\"\/kaggle\/input\/cifar100\/train\"), image_transformations, train_augmentations)\ntrain_data_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, pin_memory=True)\n\ntest_dataset = CIFAR100(Path(\"\/kaggle\/input\/cifar100\/test\"), image_transformations)\ntest_data_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, pin_memory=True)","1e68ab17":"!pip install bitorch torchinfo","32b85a07":"from cifar100_resnets import BasicBlock, ResNet\nfrom bitorch.layers import QConv2d\n\n\nclass BinaryBasicBlock(BasicBlock):\n    def __init__(self, input_planes, output_planes, stride):\n        super().__init__(input_planes, output_planes, stride)\n        # Task 3a)\n        # TODO: overwrite the two convolutional layers of BasicBlock with their binary versions (use \"sign\" to quantize inputs and weights and disable the bias)\n        #       (check https:\/\/www.kaggle.com\/bartzi\/cifar100-resnets to see how they defined in the original implementation)\n        # HINT: 1) this (only) concerns the properties `self.conv1` and `self.conv2` of this class\n        #       2) the binarized QConv2d layers should have the same number of input planes, output planes, stride and padding as their 32-bit version but no bias\n        #          and have input_quantization and weight_quantization set to \"sign\" to binarize both inputs and weights\n        # TASK START - Start coding here:\n        raise NotImplementedError()\n        # TASK END\n\n    def forward(self, x):\n        # Task 3b)\n        # TODO: copy the forward function from the original implementation and remove all calls to F.relu\n        #       (check https:\/\/www.kaggle.com\/bartzi\/cifar100-resnets to see the original implementation)\n        # TASK START - Start coding here:\n        raise NotImplementedError()\n        # TASK END\n        return out\n\n\nclass ResNet20(ResNet):\n    def __init__(self):\n        super().__init__(BasicBlock, [3, 3, 3], num_classes=100)\n\n\nclass BinaryResNet20(ResNet):\n    def __init__(self):\n        super().__init__(BinaryBasicBlock, [3, 3, 3], num_classes=100)","aa24ac44":"from torchinfo import summary as summary_32bit\n\nprint(summary_32bit(ResNet20(), input_size=(1, 3, 32, 32)))","bd15494c":"from bitorchinfo import summary as summary_1bit\nfrom bitorch.quantizations import Quantization\n\nprint(summary_1bit(BinaryResNet20(), input_size=(1, 3, 32, 32), quantization_base_class=Quantization))","af8bbc55":"loss_function = nn.CrossEntropyLoss()\n\ndef train_for_one_iteration(network: Type[nn.Module], batch: tuple, optimizer: Type[Optimizer]) -> float:\n    images, labels = batch\n    predictions = network(images)\n    loss = loss_function(predictions, labels)\n    \n    # Here come the real weight adjustments, first zero gradients, then calculate derivatives, followed by the actual update of the optimizer\n    optimizer.zero_grad()  # this sets gradients to zero (e.g. to clean up from any previous backward passes)\n    loss.backward()        # calculate gradients for our network\n    optimizer.step()       # update all weights in our network according to the computed gradients\n    \n    return float(loss.item())\n\ndef train(train_data: DataLoader, test_data: DataLoader, network: Type[nn.Module], optimizer: Type[Optimizer], \\\n          lr_scheduler: Type[_LRScheduler], num_epochs: int) -> dict:\n    device = get_device()\n    metrics = defaultdict(list)\n    for epoch in trange(num_epochs, desc=\"Epoch: \"):\n        losses = []\n        with tqdm(total=len(train_data), desc=\"Iteration: \") as progress_bar:\n            for iteration, batch in enumerate(train_data):\n                batch = to_device(batch, device)\n                loss = train_for_one_iteration(network, batch, optimizer)\n                losses.append(loss)\n\n                current_iteration = epoch * len(train_data) + iteration\n                metrics[\"loss\"].append({\"iteration\": current_iteration, \"value\": loss})\n                current_learning_rate = lr_scheduler.get_last_lr()[0]\n                metrics[\"learning_rate\"].append({\"iteration\": current_iteration, \"value\": current_learning_rate})\n\n                progress_bar.set_postfix({\"lr\": f\"{current_learning_rate:.6f}\"})\n                progress_bar.update()\n                lr_scheduler.step()\n\n            accuracy = test_model(network, test_data)\n            current_iteration = (epoch + 1) * (len(train_data))\n            metrics['mean_loss'].append({\"iteration\": current_iteration, \"value\": statistics.mean(losses)})\n            metrics['accuracy'].append({\"iteration\": current_iteration, \"value\": accuracy})\n            progress_bar.set_postfix_str(f\"Epoch {epoch}, Mean Loss: {statistics.mean(losses):.2f}, Test Accuracy: {accuracy:.2f}\")\n\n    return metrics\n\nlearning_rate = 0.01\nnum_epochs = 50\n\n# create our binary ResNet and move it to GPU\nnetwork = BinaryResNet20()\nnetwork = network.to(get_device())\n\n# create an optimizer for training, for BNNs it was shown, that Adam usually performs better than SGD\noptimizer = torch.optim.Adam(network.parameters(), lr=learning_rate)\nlr_scheduler = OneCycleLR(optimizer, learning_rate, epochs=num_epochs, steps_per_epoch=len(train_data_loader))\n\n# we are done with all setup and can start the training\nlogged_metrics = train(train_data_loader, test_data_loader, network, optimizer, lr_scheduler, num_epochs)","b013b3d4":"def plot_metrics(metrics: dict):\n    # we prepare the plotting by creating a set of axes for plotting, we want to put each metric in its own plot in a separate row\n    # furthermore, all plots should share the same x-axis values\n    fig, axes = plt.subplots(math.ceil(len(metrics) \/ 2), 2, sharex=True, figsize=(16, 10))\n\n    # we want to have a set of distinct colors for each logged metric\n    colors = iter(plt.cm.rainbow(np.linspace(0, 1, len(metrics))))\n    \n    # create the actual plot\n    for (metric_name, metric_values), axis in zip(metrics.items(), axes.flatten()):\n        iterations = []\n        values = []\n        for logged_value in metric_values:\n            iterations.append(logged_value[\"iteration\"])\n            values.append(logged_value[\"value\"])\n        axis.plot(iterations, values, label=metric_name, color=next(colors))\n        axis.legend()\n    plt.show()\n\n    \nplot_metrics(logged_metrics)","0cc0ba1c":"After training for 50 epochs, the model should achieve an accuracy of about 45%.\nIn general, converting a 32-bit model to a BNN model should allow us to achieve a similar accuracy with the binary model compared to the 32-bit model, but it usually is slightly lower (between 1 and 10%, depending on the task).\nOf course this can depend on the hyperparameters and the optimizer choice, but in general binary networks are less representative and thus slightly less accurate than 32-bit networks.\n\nAn exception is if the 32-bit model is highly overfitting on the training data (and both models achieve close to 100% training accuracy): in this case a binary model could achieve higher test accuracy, but this is a special case and not very common.\n\n## Plotting of Progress\n\nAs in the last exercise, we can now plot the train progress using the `plot_metrics` function:","f696f87b":"## Visualize the 1-bit model\n\nNow, we can also run the same code, but based on [bitorchinfo](https:\/\/github.com\/hpi-xnor\/bitorchinfo), which is an extended version of [torchinfo](https:\/\/github.com\/TylerYep\/torchinfo) to support the binary layers of [bitorch](https:\/\/github.com\/hpi-xnor\/bitorch), to visualize our binarized model.","f96ed96c":"# Training a Binary Neural Network on CIFAR-100 using BITorch and PyTorch\n\nThis is the fourth practical exercise of our course [Applied Edge AI](https:\/\/learn.ki-campus.org\/courses\/edgeai-hpi2022).\nIn the last exercise, we performed knowledge distillation to train a smaller student network based on a teacher network for CIFAR-100 using PyTorch.\nIn this exercise, we want to understand how Binary Neural Networks (BNNs) can be implemented, and then build and train a binary neural network for CIFAR-100 based on [BITorch](https:\/\/github.com\/hpi-xnor\/bitorch).\n\nSimilarly to the previous exercise, we provide you with a notebook with missing code sections.\nIn the graded quiz at the end of the week, we might ask some questions that deal with this exercise, so make sure to do the exercise (and have your output handy) **before** taking the quiz!","f32a22e4":"# What Now?\n\nSimilar to the previous weeks, you should keep in mind what you just did in this exercise, as we will ask about the implementation in the graded test.\n\nWe also want to offer further ideas you can explore:\n\nYou can check [the paper](https:\/\/arxiv.org\/abs\/1602.02830) of Hubara et al. and try to implement the stochastic option of the sign function.\n\nYou can try to train a quantized model that uses 4 bit instead of 1 bit.\n\nFor this you can use the functions `weightdorefa` and `inputdorefa` and set the number of bits in the `config` as shown below:\n```\nimport bitorch.quantization.config as QuantizationConfig\nQuantizationConfig.dorefa_bits = 4\n# now we can use \"weightdorefa\" and \"inputdorefa\" as arguments to QConv2d and they will use 4 bits\n```\n\nYou could also try to visualize these quantization functions for e.g., 2, 3, or 4 bits using the visualization you implemented in task 1 to understand how weights and activations are quantized to other bit-widths.\n```\nfrom bitorch.quantization import WeightDoReFa\nw_dorefa = WeightDoReFa(bits=4)\n# now we can pass some data through w_dorefa with forward\ny = w_dorefa.forward(x)\n```\n\nFinally, you can also try to understand how the `QConv2d` layer we used is actually implemented in [BITorch](https:\/\/github.com\/hpi-xnor\/bitorch\/blob\/main\/bitorch\/layers\/qconv.py).","e6a2bf1d":"If you are done, check your output with the provided example above.\n\nNext, we are going to visualize the `sign` function (which we used in the simple toy example) with our new code:","de3fdd3a":"## Binarizing Weights and Activations Directly\n\nThe idea of Binary Neural Networks (BNNs), was first proposed by Hubara et al. in their paper \"Binarized neural networks\" in 2016. A pre-print is available on [arxiv](https:\/\/arxiv.org\/abs\/1602.02830).\nThey suggest a deterministic option to binarize weights and feature maps to two values, +1 and -1: \n\n$$ \\mathrm{sign}(x) =\n    \\begin{cases}\n        +1  & \\text{if } x \\geq 0, \\\\\n        -1 & \\text{otherwise,}\n    \\end{cases} $$\n\nThis is very similar to most implementations of `sign` functions, except usual implementations return zero, if the input is zero.\nIn actual BNN implementations, weights and activations which are zero need to be treated properly (i.e. `sign` needs to return +1 for these), but for now we can ignore this special case to simplify things.\n\nFirst, we want to have a look at a small toy example again (similar to the toy example of our [week 1 exercise](https:\/\/www.kaggle.com\/jopyth\/edge-ai-w1-length)):","6ef45101":"As we can see, the gradient of the sign function is 0 everywhere.\nThis is not useful at all for the optimization and it explains why the weights were not updated in our toy example as well.\nAll gradients propagated through any `sign` function become zero!\nTherefore, we need to implement a version of the sign function that uses a so called Straight Through Estimator in the next step, fixing this problem.","52cbfd18":"## Comparing both models\n\nIt makes sense to make sure that the output shapes of both models are equal for a given layer (and all `BinaryBasicBlocks` use `QConv2d` instead of `Conv2d` layers).\nIf this is the case, it is a good sign, that our binary model is implemented correctly.\n\nWhen carefully checking the output above you should also be able to see, that two layers *should remain* in 32-bit: the very first `Conv2d` layer and the final `Linear` layer.\nThis is not a mistake: these layers are usually **not** quantized\/binarized, since they only account for a small number of parameters and operations and keeping them in 32-bit helps to keep the accuracy of the model high.\n\nThe parameters of the model should consist of 8,308 full precision (32-bit) parameters and 267,264 quantized parameters, and the model should have an estimated total size of 1.76 MB.\nThe computations should consist of 0.45 million full precision (32-bit) mult-adds and 40.11 million quantized mult-adds.\n\nAlso, double check whether you removed all calls to `F.relu` in the forward implementation, as these are not shown in the tabular overview. Otherwise the model performance is not going to be very good.\n\n# Train the Binary Model\n\nNow, we just need to perform the last setup steps and then start the training. \\O\/\n\nBefore starting the training below, you should enable the GPU acclerator in the sidebar on the right (you can open the sidebar by clicking on the |< Symbol in the top right, then select *Settings*, *Accelerator*, *GPU*).\n\nIf you have not done so at the beginning of working on this exercise (which is fine), this means the other cells need to be run again.\nTo do so, you can select *Run All* in the top toolbar.\nThe notebook should run most of the previous cells very quickly until the training below is executed.","096792a5":"## Installing BITorch\n\nTo further simplify the implementation, we want to use the [BITorch](https:\/\/github.com\/hpi-xnor\/bitorch) library, which provides binarized and quantized versions of most common layers used in Convolutional Neural Networks.\nOtherwise we would need to implement our own custom convolutions and linear (dense) layers.\n\nTo allow installation of packages, you need to have *your phone number verified* [in your Kaggle profile](https:\/\/www.kaggle.com\/me\/account). Then you can enable the Internet switch in the sidebar on the right (you can open the sidebar by clicking on the |< Symbol in the top right, then select Settings, Internet). The switch should show a checkmark afterwards.","efa2e11f":"Now you can visualize your implemented solution to check if the forward and backward passes are correct. Your implementation should be correct if your output looks like the picture below:\n\n![image.png](attachment:c0adb4ab-7f7c-49ce-9ec7-5ff38010d4f0.png)\n\nOnce your function visualization looks correct, you can now compare it to our other functions above (you can add them in the graph below if you want).\nThe numeric gradient still ist not useful for learning, but that gradient is not actually used by the network.\nInstead our defined gradient is used and now allows the network to learn.\nFinally, our (reconstructed) *gradient function* gives us an idea, of how the framework \"sees\" the function used in the **backward** pass.\nIt looks a little bit similar to the original *sign* function for x values smaller than -1 and larger than 1 and allows learning in between. ","006416f8":"In this case we can see that the inputs and weights were indeed binarized to -1 and +1, but the loss does not change?\nSomehow the model can not learn with the regular `sign` function, even the 32-bit weight values did not change at all.\n\n# Task 1: Visualizing Functions and Their Gradients\n\nTo understand this problem, we need to understand how the sign function is differentiated in pytorch.\nFor this we first are going to visualize how functions in general are differentiated. Your task is to implement two functions: \n\n- `calculate_numeric_gradients` where you get x and y values of a sampled function and have to calculate the gradient between these points\n- `reconstruct_gradient_function` where you get computed gradients of a function and have to return y values that represent a function that would create the given gradient values\n\nThese functions are then used to visualize the given functions.\n\nIf your results looks like this, you are done:\n\n![image.png](attachment:e228ba59-6ab2-492c-ba8d-bce177f939aa.png)","d3db0d16":"## Reusing Code\n\nNow that we have understood how we can implement a `sign` function that allows us to train a binary neural network, we can move to building a whole binary network.\nBut before we start, we can reuse parts of our previous code here for the data loading and define some imports needed later on (similar to our previous exercises).\nWe already added this code in the following cells, you do not need to change the code in the following cell.","80d77237":"\nAnother test you can do is to check whether the following numeric assertions hold true.\nIn this case we are also testing that gradients can be passed through our function (by asserting that `torch.allclose(expected_gradient, test_data.grad)` is true).\n\nIf you want you can also try to adapt our initial toy example and you are going to see that the 32-bit weights are now updated if we use our new `sign_quantization` function.","982663f4":"# Task 2: Implement the Sign Function with Straight Through Estimator\n\nNow we need to define the sign quantization function that actually allows our network to learn.\nTo do so, we are going to use a trick named Straight Through Estimator (STE).\nIn this case, in the **backward** pass we \"pretend\" that we only used a linear function during the **forward** pass.\nBut actually, we apply the sign function in the forward pass (but we set all zeros to one).\n\nHowever, we also do not want that our weights and features before the `sign` function become very large.\nSince the `sign` function only depends on the sign and not the absolute value, Hubara et al. suggested clipping the **gradients** (in the backward pass) if the corresponding **input values** are smaller than -1 or larger than +1.\nThey also suggest clipping the weights in the same way.\n\nLater research work has shown that it can be useful to vary this threshold, e.g. using +1.25 and -1.25 instead, so we are going to use a variable for this threshold.\n\nThere are (at least) two options to solve this task:\n\n- 2a) implement a custom operator (i.e. fill out `DifferentiableSignFunction`) where you can define the forward and backward pass explicitely\n- 2b) implement a sign function with STE by manipulating the computational graph with the `.detach()` function (you probably remember this function from the knowledge distillation lesson)\n\nYou can implement both versions, however only one is required.\nThus if you do not want to spend too much time, we suggest you  only implement one version and we think the first option might be a bit easier than the second.","3d33a41d":"After implementing this, we can perform a basic check of our implementation by visualizing both models and comparing them.\n\n## Visualize the 32-bit model\n\nNow, if we check the model summaries of the full precision and binary version of resnet, you should see a significant drop in model size, since most of the trainable parameters of the model are now quantized.\n\nTo do so, lets first print a summary of the 32-bit model:","abf010ad":"# Task 3: Defining the Binary Neural Network\n\nNow we are going to define a binary neural network for the Cifar100 dataset. For this we will be utilizing the same network architecture as in the past weeks, Resnet, but we will replace the basic building block, which is the key part of Resnets architecture, with a binarized version of it. The following picture visualizes the structure of the full precision (32 bit) version of the Resnet building Block (source of the illustration made by Arden Stiedemann: https:\/\/morioh.com\/p\/18a0e3c6e6c2):\n\n![image.png](attachment:88274d85-09fc-4e9b-973a-08380969659f.png)\n\nFor this task, it makes sense to first have a look at the 32-bit implementation of the `BasicBlock` of the [feature extractors](https:\/\/www.kaggle.com\/bartzi\/cifar100-resnets) which we used over the past weeks.\nYour task is to define a `BinaryBasicBlock` module based on the `BasicBlock` module with the following two changes:\n\n1. Replace the 32-bit Convolution layers with their 1-bit version\n2. Remove the ReLU layers during the forward implementation\n\n## Task 3a): Replace the 32-bit Convolution layers with their 1-bit version\n\nFirst, we need to adapt the two convolutional layers to use binarized convolutions instead.\nFor this task, we can use the `QConv` layers from [BITorch](https:\/\/github.com\/hpi-xnor\/bitorch).\nThese quantized convolutional layers take the same parameters as a conventional `Conv` Layer from pytorch, but add additional options to quantize the input values and the weights before doing the actual convolution operation.\nFurthermore, we typically use `bias=False` for binarized layers.\n\nSo if we want to convert, for example, the 32-bit convolution layer\n```\nConv2d(3, 16, kernel_size=3)\n```\nto a binary version using only 1 bit, we should use `QConv2d` instead, pass `\"sign\"` as the quantization methods, and add `bias=False`:\n```\nQConv2d(3, 16, kernel_size=3, bias=False, input_quantization=\"sign\", weight_quantization=\"sign\")\n```\n\nFor a full list of supported quantization functions, you can call `bitorch.quantizations.quantization_names()`.\nHowever for this task the \"sign\" quantization function should be used.\n\n## Task 3b): Remove the ReLU layers during the forward implementation\n\nSecondly, we need to remove the ReLU layers. Do you already know why?\n\nIf you remember the definition of a ReLU layer: it sets all negative values to zero.\nThis is not very useful for our BNN, since we want to use the *sign* quantization throughout the network to quantize the inputs.\nAnd the *sign* function for BNNs returns +1 for all values larger than or equal to zero.\nThus, a ReLU layer will remove a lot (if not all) information from the feature map for a binary convolution following the ReLU layer (the whole feature map will be only +1s).\n\nTherefore, we should remove it.\n(There are alternatives, such as moving the positioning of the ReLU layer to a position directly before a BatchNorm layer, but for our CIFAR100 model it is easier to simply remove it.)","fc2e3bcd":"This very simple example shows us nothing more and nothing less, than that regular deep learning with 32-bit values \"works\".\nIn the example we try to optimize the squared loss, so all weights should be changed in a way that the outputs get closer to zero.\nWe can see that the weights were updated and the loss was reduced from 13.71 to 3.33.\n\nNow let us try the same thing but adapt the toy example for binary networks:\nWe simply apply the sign function to weights and inputs and run the function again:"}}