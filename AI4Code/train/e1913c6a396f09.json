{"cell_type":{"c016d1ec":"code","a6f5d917":"code","fefb05a8":"code","88ea6336":"code","d28f9807":"code","6fc5f332":"code","93f86aa1":"code","c831f0b5":"code","04d1f13f":"code","11ce213d":"code","d5cd5144":"code","312b60ce":"code","eee0fce6":"code","e08330a6":"code","2fe9d4f4":"code","94a08fa9":"code","faf37d37":"code","822dcbdb":"code","6048fc51":"code","bccc1c7f":"code","ecef88ff":"code","85907859":"code","5fc71199":"markdown","d4b3d807":"markdown","099ff4d6":"markdown","166f76b0":"markdown","3872ce47":"markdown","184e4991":"markdown","a2ccb67a":"markdown","cfa4b721":"markdown","3252c121":"markdown","de7e7f42":"markdown","003d14b3":"markdown","d5103842":"markdown"},"source":{"c016d1ec":"import os\nfrom PIL import Image\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\n!pip install torchsummary\n# from torchsummary.summary import summary\nimport torchsummary as summary\nfrom tqdm.notebook import tqdm","a6f5d917":"device_num = str(torch.cuda.current_device())\ndevice = \"cuda:\"+device_num if torch.cuda.is_available() else \"cpu\"\ndevice = torch.device(device)\nprint('current_device : ',device)\nprint('[ memory stats ]\\n',torch.cuda.memory_stats())\nprint('[ memory summary]\\n',torch.cuda.memory_summary())","fefb05a8":"# define dataset path\nroot_dir = '..\/input\/cityscapes-image-pairs\/cityscapes_data'\ntrain_dir = os.path.join(root_dir,'train')\nval_dir = os.path.join(root_dir,'val')\n\n# make file name list & count\ntrain_fnms = os.listdir(train_dir)\nval_fnms = os.listdir(val_dir)\nprint('train set : %d EA \\nval set : %d EA' % (len(train_fnms),len(val_fnms)))","88ea6336":"# show sample image\nnum = 7\n# tmp_dir = os.path.join(train_dir,train_fnms[num])\ntmp_dir = os.path.join(train_dir,val_fnms[num])\ntmp_img = Image.open(tmp_dir).convert(\"RGB\")\nprint('[ Original Data ]')\nprint('image size : ',tmp_img.size)\nplt.imshow(tmp_img)\nplt.show()\n\n# seperate real & label image\ndef img_split(img):\n    img = np.array(img)\n    cityscape, label = img[:,:256,:],  img[:,256:,:]\n    return cityscape, label\ncityscape, label = img_split(tmp_img)\nc_shape = str(cityscape.shape)\nl_shape = str(label.shape)\nprint('[ Splited Data ]')\nfig,ax = plt.subplots(1,2,figsize=(10,5))\nax[0].imshow(cityscape)\nax[0].set_title('cityscape : %s' % (c_shape))\nax[1].imshow(label)\nax[1].set_title('lable : %s' % (l_shape))\nplt.show()","d28f9807":"\n# generate color comb\nnum_items= 5000\ncolor_arr = np.random.choice(range(256),3*num_items).reshape(-1,3)\nprint(color_arr.shape)\nprint(color_arr[:5,:])\n\n# random generated color clustering model\nnum_classes = 10\nlabel_model = KMeans(n_clusters=num_classes)\nlabel_model.fit(color_arr)\nlabel_model.predict(color_arr[:5,:])\n\ncityscape,label = img_split(tmp_img)\n# 3 color channel to clustered classes\nlabel_class = label_model.predict(label.reshape(-1,3)).reshape(256,256)\n\nfig, ax = plt.subplots(1,3,figsize=(15,5))\nax[0].imshow(cityscape)\nax[1].imshow(label)\nax[2].imshow(label_class)","6fc5f332":"class CityscapeDataset(Dataset):\n    def __init__(self,img_dir):\n        self.img_dir = img_dir\n        self.img_fnms = os.listdir(img_dir)\n        \n        def genLabelModel():\n            num_items=5000\n            num_classes = 10\n            color_arr = np.random.choice(range(256),3*num_items).reshape(-1,3)\n            label_model = KMeans(n_clusters=num_classes)\n            label_model.fit(color_arr)\n            return label_model\n        \n        self.label_model = genLabelModel()\n        \n        \n        \n    def __len__(self):\n        return len(self.img_fnms)\n    \n    # instance slicing\n    def __getitem__(self,index):\n        img_fnm = self.img_fnms[index]\n        img_fp = os.path.join(self.img_dir,img_fnm)\n        img = Image.open(img_fp).convert('RGB')\n        img = np.array(img)\n        cityscape,label = img_split(img)\n        label_class = self.label_model.predict(label.reshape(-1,3)).reshape(256,256)\n        cityscape = self.transform(cityscape)\n        label_class = torch.Tensor(label_class).long()\n        return cityscape,label_class\n    \n    def img_split(self,img):\n        img = np.array(img)\n        cityscape,label = img[:,:256,:], img[:,256:,:]\n        return cityscape,label\n    \n    def transform(self,img):\n        transforms_ops = transforms.Compose([\n            transforms.ToTensor(),\n            transforms.Normalize(mean=(0.485,0.456,0.406), std=(0.229,0.224,0.225))\n        ])\n        return transforms_ops(img)\n    \n        ","93f86aa1":"dataset = CityscapeDataset(train_dir)\nprint('Train set : %d EA' % (len(dataset)))\ncityscape, label_class = dataset[0]\nprint('Train data shape : %s \\nLabel_class shape : %s' %(cityscape.shape,label_class.shape))","c831f0b5":"class DoubleConv(nn.Module):\n    def __init__(self,c_in,c_out):\n        super().__init__()\n        self.double_conv = nn.Sequential(nn.Conv2d(c_in,c_out,3,padding=1,stride=1),\n                                         nn.BatchNorm2d(c_out),\n                                         nn.ReLU(inplace=True),\n                                         nn.Conv2d(c_out,c_out,3,padding=1,stride=1),\n                                         nn.BatchNorm2d(c_out),\n                                         nn.ReLU(inplace=True)\n                                        )\n    def forward(self,x):\n        return self.double_conv(x)\n    \nclass Down(nn.Module):\n    def __init__(self,c_in,c_out):\n        super().__init__()\n        self.down_conv = nn.Sequential(nn.MaxPool2d(2),\n                                       DoubleConv(c_in,c_out))\n    def forward(self,x):\n        return self.down_conv(x)\n    \nclass Up(nn.Module):\n    def __init__(self,c_in,c_out):\n        super().__init__()\n        self.up = nn.Upsample(scale_factor=2,mode='bilinear',align_corners=True)\n        self.double_conv = DoubleConv(c_in,c_out)\n        \n    def forward(self,x1,x2):\n        x1 = self.up(x1)\n        # padding\n        diffY = x2.size()[2] - x1.size()[2]\n        diffX = x2.size()[3] - x1.size()[3]\n        # for exact same size, pad each size\n        x1 = F.pad(x1,(diffX\/\/2,diffX-diffX\/\/2,diffY\/\/2,diffY-diffY\/\/2))\n        x = torch.cat([x2,x1],dim=1) # concat channel layer\n        x = self.double_conv(x)\n        return x\n    \nclass OutConv(nn.Module):\n    def __init__(self,c_in,c_out):\n        super().__init__()\n        self.conv = nn.Conv2d(c_in,c_out,kernel_size=1)\n    def forward(self,x):\n        return self.conv(x)\n    \nclass UNet(nn.Module):\n    def __init__(self,c_in,c_out):\n        super(UNet,self).__init__()\n        self.in_conv = DoubleConv(c_in,64)\n        self.down1 = Down(64,128)\n        self.down2 = Down(128,256)\n        self.down3 = Down(256,512)\n        self.down4 = Down(512,1024\/\/2)\n        \n        self.up1 = Up(1024,512\/\/2)\n        self.up2 = Up(512,256\/\/2)\n        self.up3 = Up(256,128\/\/2)\n        self.up4 = Up(128,64)\n        self.out_conv = OutConv(64,c_out)\n        \n    def forward(self,x):\n        # 3 256 256\n        x1 = self.in_conv(x)\n        # 64 256 256\n        x2 = self.down1(x1) # 64 128 128 -> 128,128,128 -> 128,128,128\n        x3 = self.down2(x2) \n        x4 = self.down3(x3)\n        x5 = self.down4(x4)\n        x = self.up1(x5,x4)\n        x = self.up2(x,x3)\n        x = self.up3(x,x2)\n        x = self.up4(x,x1)\n        x = self.out_conv(x)\n        return x","04d1f13f":"model = UNet(c_in=3,c_out=num_classes).to(device)\nsummary.summary(model,(3,256,256))#,device='cpu')","11ce213d":"# Practice torch.nn.functional padding \np1d = (20,50)\np2d = (1,1,2,2)\nout1 = F.pad(cityscape,p1d)\nout2 = F.pad(cityscape,p2d)\nplt.imshow(out1.numpy().transpose(1,2,0))","d5cd5144":"data_loader = DataLoader(dataset,batch_size=4)\nprint('Dataset : %d EA \\nDataLoader : %d SET' % (len(dataset),len(data_loader)))\n\nX,Y = iter(data_loader).next()\nX,Y = X.to(device),Y.to(device)\nprint('cityscape : %s | label : %s' %(X.shape,Y.shape))","312b60ce":"Y_pred = model(X)\nprint(Y_pred.shape)","eee0fce6":"batch_size = 16\nepochs =10\nlr = 0.01","e08330a6":"dataset = CityscapeDataset(train_dir)\ndata_loader = DataLoader(dataset, batch_size=batch_size)\nmodel = UNet(3,num_classes).to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=lr)","2fe9d4f4":"step_losses = []\nepoch_losses = []\nfor epoch in tqdm(range(epochs)):\n    epoch_loss = 0\n    for X, Y in tqdm(data_loader, total=len(data_loader), leave=False):\n        X, Y = X.to(device), Y.to(device)\n        optimizer.zero_grad()\n        Y_pred = model(X)\n        loss = criterion(Y_pred, Y)\n        loss.backward()\n        optimizer.step()\n        epoch_loss += loss.item()\n        step_losses.append(loss.item())\n    epoch_losses.append(epoch_loss\/len(data_loader))","94a08fa9":"model.cuda()","faf37d37":"fig,ax = plt.subplots(1,2,figsize=(10,5))\nax[0].plot(step_losses)\nax[1].plot(epoch_losses)","822dcbdb":"model_stat_dir = 'city_UNet_stat.pth'\nmodel_dir = 'city_UNet.pt'\nmodel_stat_dir_parallel = 'city_UNet_stat_parallel.pth'\ntorch.save(model.state_dict(),model_stat_dir)\ntorch.save(model,model_dir)\n\n# DataParallel model save\n# torch.save(model.module.state_dict(),model_stat_dir_parallel)\n\n# import torch.onnx as onnx\n# input_img = torch.zeros((1,3,256,256))\n# dummy_input = input_img.cuda()\n# onnx.export(model,dummy_input,'city_unet_model.onnx',opset_version=11)","6048fc51":"# select loading hardware : CPU or GPU\n# device = torch.device('cpu')\ndevice = torch.device('cuda')\n\n# define network + load state_dict\nmodel_1 = UNet(3,num_classes).to(device)\nmodel_1.load_state_dict(torch.load(model_stat_dir)) # map_location=device\nmodel_1.to(device)\n\n# load entire model\nmodel_2 = torch.load(model_dir)","bccc1c7f":"test_batch_size=8\nval_set = CityscapeDataset(val_dir)\nval_loader = DataLoader(val_set,batch_size=test_batch_size)\nX,Y = next(iter(val_loader))\nX,Y = X.to(device),Y.to(device)\nY_pred = model_1(X)\nprint(Y_pred.shape)\nY_pred = torch.argmax(Y_pred,dim=1)\nprint(Y_pred.shape)","ecef88ff":"inverse_transform = transforms.Compose([\n    transforms.Normalize((-0.485\/0.229, -0.456\/0.224, -0.406\/0.225), (1\/0.229, 1\/0.224, 1\/0.225))\n])","85907859":"fig, axes = plt.subplots(test_batch_size, 3, figsize=(3*5, test_batch_size*5))\n\nfor i in range(test_batch_size):\n    \n    landscape = inverse_transform(X[i]).permute(1, 2, 0).cpu().detach().numpy()\n    label_class = Y[i].cpu().detach().numpy()\n    label_class_predicted = Y_pred[i].cpu().detach().numpy()\n    \n    axes[i, 0].imshow(landscape)\n    axes[i, 0].set_title(\"Landscape\")\n    axes[i, 1].imshow(label_class)\n    axes[i, 1].set_title(\"Label Class\")\n    axes[i, 2].imshow(label_class_predicted)\n    axes[i, 2].set_title(\"Label Class - Predicted\")","5fc71199":"> ---\n> ### 2-1. define dataset path & make filename list & count","d4b3d807":"> ---\n> ### 2-3. Define class for 'Pre-processing' Datasets","099ff4d6":"- see sample image\n- seperate real & label image","166f76b0":"> ---\n> ### 2-2. Make Label_model for RGB to Classes","3872ce47":"# Contents\n  - [1. Import Packages](#1.-Import-Packages)\n  - [2. Load Dataset](#2.-Load-Dataset)\n  - [3, Define Model](#3.-Define-Model)\n  - [4. Train Model](#4.-Train-Model)\n  - [5. Save & Load Model](#5.-Save-&-Load-Model)\n  - [6. Validation](#6.-Validation)","184e4991":"---\n# 3. Define Model\n  - ref : https:\/\/github.com\/csm-kr\/dog_cat_colorization","a2ccb67a":"---\n# 6. Validation","cfa4b721":"---\n# 4. Train Model ","3252c121":"---\n# 1. Import Packages","de7e7f42":"---\n# 5. Save & Load Model\n\n### two kind of model save\n  - Network Structure + weights\n  - Entire Model\n  - DataParallel Model","003d14b3":"---\n# 2. Load Dataset","d5103842":"- torch.no_grad() : autograd engine \ub044\uae30 \/ \uba54\ubaa8\ub9ac\uc0ac\uc6a9\ub7c9\uc904\uc774\uace0,\uc5f0\uc0b0\uc18d\ub3c4\ub192\uc784\n- model.eval() : \ubaa8\ub378\ub9c1 \uc2dc training&inference \uc2dc \ub2e4\ub974\uac8c \ub3d9\uc791\ud558\ub294 layer\uc874\uc7ac\n    ex. Dropout, BatchNorm"}}