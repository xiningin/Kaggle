{"cell_type":{"7d22e6e4":"code","5b9453d8":"code","1291ac36":"code","d59e885f":"code","8e5b88f2":"code","e4ee7bd1":"code","67c042a5":"code","b11a9453":"code","0b795522":"code","9900e7d7":"code","4cb8940b":"code","c92342c9":"code","3acc8832":"code","4812c0a6":"code","a8276274":"code","2d430100":"code","59f7d631":"code","48f3e23d":"code","285fcafb":"code","fa15e7d7":"code","a6decf4b":"code","2b26b3a4":"code","bf3fff6c":"code","e0e37c61":"code","6a677fae":"code","ad822270":"code","1527afee":"code","e25d5f3d":"code","be7b2c28":"code","67811d8e":"code","82bc7df3":"code","84162a7e":"code","e2361bb2":"code","7f0879a3":"code","be25daaf":"code","00f5b362":"code","d47965de":"code","1b16e6fe":"code","e4b265a2":"code","a6911db3":"code","56d98244":"code","f4382677":"code","becc7dec":"code","b7815fb3":"markdown","4f7d0a07":"markdown","d0e76299":"markdown","44548d50":"markdown","687b6bd8":"markdown","86aae16c":"markdown","face264a":"markdown","8846e1db":"markdown","23f38f38":"markdown","0ec49031":"markdown","f2a6419b":"markdown","4788cef3":"markdown","26b93556":"markdown","272fa292":"markdown","ee3b2255":"markdown","cdac8645":"markdown","29cb4b2e":"markdown"},"source":{"7d22e6e4":"%%html\n<style>\n@import url('https:\/\/fonts.googleapis.com\/css?family=Ewert|Roboto&effect=3d|ice|');\nspan {font-family:'Roboto'; color:black; text-shadow: 5px 5px 5px #aaa;}  \ndiv.output_area pre{font-family:'Roboto'; font-size:110%; color: royalblue;}      \n<\/style>","5b9453d8":"import numpy,pandas,pylab,h5py,time,warnings,os\nfrom sklearn import datasets,preprocessing,impute,cluster,mixture,manifold,dummy,linear_model,svm\nfrom sklearn.feature_extraction import DictVectorizer \nfrom sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\nfrom sklearn.feature_selection import SelectKBest,chi2,RFE\nfrom sklearn.decomposition import PCA; from sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error,median_absolute_error,mean_absolute_error\nfrom sklearn.metrics import r2_score,explained_variance_score\nfrom sklearn.metrics import accuracy_score,hamming_loss,classification_report\nfrom sklearn.tree import DecisionTreeClassifier,ExtraTreeClassifier,DecisionTreeRegressor,ExtraTreeRegressor\nfrom sklearn.ensemble import BaggingClassifier,RandomForestClassifier,AdaBoostClassifier,GradientBoostingClassifier\nfrom sklearn.ensemble import BaggingRegressor,RandomForestRegressor,AdaBoostRegressor,GradientBoostingRegressor\nfrom sklearn.neighbors import KNeighborsRegressor,RadiusNeighborsRegressor\nfrom sklearn.neighbors import KNeighborsClassifier,RadiusNeighborsClassifier,NearestCentroid\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis,QuadraticDiscriminantAnalysis\nfrom sklearn.naive_bayes import GaussianNB,MultinomialNB,BernoulliNB\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.cross_decomposition import PLSRegression\nfrom sklearn.gaussian_process import GaussianProcessRegressor,GaussianProcessClassifier\nfrom sklearn.gaussian_process.kernels import WhiteKernel,RationalQuadratic,RBF\nfrom sklearn.semi_supervised import LabelPropagation,LabelSpreading\nfrom sklearn.isotonic import IsotonicRegression\nfrom sklearn.neural_network import MLPClassifier,BernoulliRBM,MLPRegressor","1291ac36":"[warnings.filterwarnings('ignore',category=el) for el in [UserWarning,FutureWarning,DeprecationWarning]]\nnumpy.set_printoptions(precision=4); pylab.style.use('seaborn-whitegrid')\nstyle_dict={'background-color':'gainsboro','color':'darkslategray','border-color':'white','font-family':'Roboto'}\ndef ohe(x): return preprocessing.OneHotEncoder(categories='auto').fit(x.reshape(-1,1))\\\n            .transform(x.reshape(-1,1)).toarray().astype('int16')\ndef sas(X,y): return train_test_split(X,y,test_size=0.2,random_state=1)\nprint(os.listdir(\"..\/input\"))","d59e885f":"def classifier_fit_score(classifier,classifier_name,clf_dataset,x_train,x_test,y_train,y_test):\n    classifier.fit(x_train,y_train); clf_datasets.append(clf_dataset)\n    classifier_list.append(str(classifier)); classifier_names.append(classifier_name) \n    classifier.fit(x_train,y_train)    \n    y_clf_train=classifier.predict(x_train); y_clf_test=classifier.predict(x_test)        \n    acc_clf_train=round(accuracy_score(y_train,y_clf_train),4)\n    acc_clf_test=round(accuracy_score(y_test,y_clf_test),4)\n    acc_train.append(acc_clf_train); acc_test.append(acc_clf_test)   \n    loss_clf_train=round(hamming_loss(y_train,y_clf_train),4)\n    loss_clf_test=round(hamming_loss(y_test,y_clf_test),4)\n    loss_train.append(loss_clf_train); loss_test.append(loss_clf_test)    \n    return [y_clf_train,y_clf_test,acc_clf_train,acc_clf_test,loss_clf_train,loss_clf_test]\ndef get_classifier_results():\n    return pandas.DataFrame({'classifier':classifier_list,'classifier_name':classifier_names,\n                             'clf_dataset':clf_datasets,'acc_train':acc_train,\n                             'acc_test':acc_test,'loss_train':loss_train,'loss_test':loss_test})       ","8e5b88f2":"def regressor_fit_score(regressor,regressor_name,dataset,x_train,x_test,y_train,y_test):\n    regressor_list.append(str(regressor)); regressor_names.append(regressor_name); reg_datasets.append(dataset)    \n    regressor.fit(x_train,y_train); y_reg_train=regressor.predict(x_train); y_reg_test=regressor.predict(x_test)    \n    r2_reg_train=round(r2_score(y_train,y_reg_train),4); r2_train.append(r2_reg_train)\n    r2_reg_test=round(r2_score(y_test,y_reg_test),4); r2_test.append(r2_reg_test)    \n    ev_reg_train=round(explained_variance_score(y_train,y_reg_train),4); ev_train.append(ev_reg_train)\n    ev_reg_test=round(explained_variance_score(y_test, y_reg_test),4); ev_test.append(ev_reg_test)    \n    mse_reg_train=round(mean_squared_error(y_train,y_reg_train),4); mse_train.append(mse_reg_train)\n    mse_reg_test=round(mean_squared_error(y_test,y_reg_test),4); mse_test.append(mse_reg_test)\n    mae_reg_train=round(mean_absolute_error(y_train,y_reg_train),4); mae_train.append(mae_reg_train)\n    mae_reg_test=round(mean_absolute_error(y_test,y_reg_test),4); mae_test.append(mae_reg_test)\n    mdae_reg_train=round(median_absolute_error(y_train,y_reg_train),4); mdae_train.append(mdae_reg_train)\n    mdae_reg_test=round(median_absolute_error(y_test,y_reg_test),4); mdae_test.append(mdae_reg_test)    \n    return [y_reg_train,y_reg_test,r2_reg_train,r2_reg_test,ev_reg_train,ev_reg_test,\n            mse_reg_train,mse_reg_test,mae_reg_train,mae_reg_test,mdae_reg_train,mdae_reg_test]\ndef get_regressor_results():\n    return pandas.DataFrame({'regressor':regressor_list,'regressor_name':regressor_names,\n                             'dataset':reg_datasets,'r2_train':r2_train,'r2_test':r2_test,\n                             'ev_train':ev_train,'ev_test':ev_test,\n                             'mse_train':mse_train,'mse_test':mse_test,\n                             'mae_train':mae_train,'mae_test':mae_test,\n                             'mdae_train':mdae_train,'mdae_test':mdae_test})","e4ee7bd1":"boston=datasets.load_boston(); housing=datasets.fetch_california_housing()\ndigits=datasets.load_digits(); wine=datasets.load_wine();\nfaces=datasets.fetch_olivetti_faces()\ntrain=datasets.fetch_20newsgroups(subset='train',shuffle=True,remove=('headers','footers','quotes'))\ntest=datasets.fetch_20newsgroups(subset='test',shuffle=True,remove=('headers','footers','quotes'))\n[X1,y1,X2,y2,X3,y3,X4,y4,X5,y5]=\\\n[boston.data,boston.target,housing.data,housing.target,digits.data,digits.target,\nwine.data,wine.target,faces.data,faces.target]","67c042a5":"# Image examples\nn=4; img=numpy.zeros((10*n,10*n))\nfor i in range(n): \n    for j in range(n): img[(10*i+1):(10*i+9),(10*j+1):(10*j+9)]=X3[i*n+j].reshape((8,8))\npylab.figure(figsize=(5,5)); pylab.imshow(img,cmap=pylab.cm.bone)\npylab.title('Examples of 64-dimensional digits')\npylab.xticks([]); pylab.yticks([]); pylab.show()","b11a9453":"# 5000x3 matrix, 3 features (2 responsible for targets), 1 target, 0.97 - the bias factor\n[X7,y7]=datasets.make_regression(5000,3,2,1,0.97)\nf,ax=pylab.subplots(ncols=3,figsize=(12,5)); k=[[0,1],[0,2],[1,2]]\n[ax[i].scatter(X7[:200,k[i][0]], X7[:200,k[i][1]],c=y7[:200],cmap=pylab.cm.tab10) for i in range(3)]\n[ax[i].set_xlabel('X7[%d]'%k[i][0]) for i in range(3)]; [ax[i].set_ylabel('X7[%d]'%k[i][1]) for i in range(3)]\npylab.show()\n# 5000x10 matrix, 10 features (8 - responsible for targets), 2 targets, 0.7 - the bias factor, 10.0 - the noise\n[X8,y8]=datasets.make_regression(5000,10,8,2,0.7,noise=10.0)","0b795522":"# Gaussian blobs for clustering, 1000 data points, 4 clusters\n[X9,y9]=datasets.make_blobs(n_samples=1000,centers=[[1,1],[-1,-1],[1,-1],[-1,1]],cluster_std=0.5)\npylab.figure(figsize=(12,5)); pylab.scatter(X9[:,0],X9[:,1],c=y9,cmap=pylab.cm.tab10)\npylab.scatter([1,-1,1,-1],[1,-1,-1,1],c='black',marker='*',s=150); pylab.show()","9900e7d7":"# 5000 data points, 2 features, 1 target with 3 labels\n[X10,y10]=datasets.make_multilabel_classification(n_classes=3,n_samples=5000,n_features=2)\nf,ax=pylab.subplots(1,figsize=(12,5)); m=['o','v','*']; a=[0.2,0.5,1]; s=[800,400,200]\n[ax.scatter(X10[:30,0],X10[:30,1],c=y10[:30,i],marker=m[i],alpha=a[i],cmap=pylab.cm.bwr,s=s[i]) for i in range(3)]\npylab.show()","4cb8940b":"# Classification. Handwritten letters\n# Read the h5 file\nf=h5py.File('..\/input\/classification-of-handwritten-letters\/LetterColorImages_123.h5','r')\nkeys=list(f.keys())\n# Create features and targets\nletters=u'\u0430\u0431\u0432\u0433\u0434\u0435\u0451\u0436\u0437\u0438\u0439\u043a\u043b\u043c\u043d\u043e\u043f\u0440\u0441\u0442\u0443\u0444\u0445\u0446\u0447\u0448\u0449\u044a\u044b\u044c\u044d\u044e\u044f'\nbackgrounds=numpy.array(f[keys[0]]); targets=numpy.array(f[keys[2]])\nletter_images=numpy.array(f[keys[1]])\/255\nprint(letters[targets[2000]-1])\npylab.figure(figsize=(5,5)); pylab.xticks([]); pylab.yticks([]); \npylab.imshow(letter_images[2000]); pylab.show()","c92342c9":"# Classification. Brand & Products\n# Read the h5 file\nf=h5py.File('..\/input\/style-color-images\/StyleColorImages.h5','r')\nkeys=list(f.keys())\n# Create features and targets\nbrands=numpy.array(f[keys[0]]); products=numpy.array(f[keys[2]])\nstyle_images=numpy.array(f[keys[1]])\/255\nprint(brands[1000],products[1000]) \npylab.figure(figsize=(5,5)); pylab.xticks([]); pylab.yticks([]); \npylab.imshow(style_images[1000]); pylab.show()","3acc8832":"# 28x28 grayscale images; 60,000 - the train set; 10,000 - the test set; labeled over 10 categories\nfrom keras.datasets import mnist\n(X_train11,y_train11),(X_test11,y_test11)=mnist.load_data()","4812c0a6":"# Basic examples\ntemperature=[{'city':'Hanoi','temperature':33.},{'city':'Frankfurt','temperature':16.},\n             {'city':'Houston','temperature':28.},\n             {'city':'Riyadh','temperature':38.},{'city':'Barcelona','temperature':17.},\n             {'city':'Ankara','temperature':27.}]\ncorpus=['Have you already set your goals for the New Year?',\n        'Do you want to lose ten kilos, run a marathon or speak fluent English?', \n        'Some experts believe that you need systems, not goals.','A system is something you do on a regular basis.',\n        'This means focusing on what you can control (your actions) rather than what you can\u2019t.',\n        'For example, do not focus on losing ten kilos.',\n        'Focus on shopping for healthy food and cooking something light every day.',\n        'Do not focus on the marathon.','Focus on the training schedule.',\n        'Invent a system to improve your English, one step at a time.','Good luck!']\ndv=DictVectorizer(); temperature_features=dv.fit_transform(temperature).toarray().astype('int16')\nprint(temperature_features); print(dv.get_feature_names())\ncv=CountVectorizer(min_df=1); corpus_features=cv.fit_transform(corpus)\ncorpus_array=corpus_features.toarray().astype('int16'); c_analyzer=cv.build_analyzer()\npylab.figure(figsize=(12,5))\nfor i in range(len(corpus_array)): pylab.scatter(range(len(corpus_array[i])),(corpus_array[i]*0.5+i),marker='s')\npylab.title(\"The Words' Occurrence in Sentences\",fontsize=15); pylab.show()\nprint(c_analyzer(corpus[0]))","a8276274":"vectorizer=TfidfVectorizer(sublinear_tf=True,max_df=0.5,stop_words='english')\nX_train6=vectorizer.fit_transform(train.data); X_test6=vectorizer.transform(test.data)\ny_train6,y_test6=train.target,test.target\ndel train,test","2d430100":"# Applying scalers\nSCX1=[X1,preprocessing.MinMaxScaler().fit_transform(X1),preprocessing.MaxAbsScaler().fit_transform(X1),\n      preprocessing.StandardScaler().fit_transform(X1),preprocessing.RobustScaler().fit_transform(X1)]\nn=50; pylab.figure(figsize=(12,5)); m=['*','v','^','<','>']\nlabels=['Real Data','MinMax Scaler','MaxAbs Scaler','Standard Scaler','Robust Scaler']\nfor i in range(5): pylab.scatter(range(n),SCX1[i][:n,4],marker=m[i],label=labels[i])\npylab.legend(loc=8); pylab.show()","59f7d631":"# Encoding categorical variables \ncat_brands,cat_products,cat_backgrounds,cat_targets=ohe(brands),ohe(products),ohe(backgrounds),ohe(targets)\nprint(targets[99:102]); print('=>'); print(cat_targets[99:102])","48f3e23d":"# Basic examples\nx1_train=[[0,17],[numpy.nan,4],[1,7],[3,numpy.nan],[5,12],[10,25]]\nx1_test=[[numpy.nan,5],[3,15],[8,numpy.nan],[4,11]]\n# mean & median imputers, custom transformer (log)\nmean_imp=impute.SimpleImputer(strategy='mean'); median_imp=impute.SimpleImputer(strategy='median')\nlog_trans=preprocessing.FunctionTransformer(numpy.log1p,validate=False)\nmean_imp.fit(x1_train); median_imp.fit(x1_train)\nx1_train_log=log_trans.transform(median_imp.transform(x1_train))\nx1_test_log=log_trans.transform(median_imp.transform(x1_test))\nfor el in [x1_test,mean_imp.transform(x1_test),median_imp.transform(x1_test),x1_test_log]:\n    print(el)","285fcafb":"classifiers=[AdaBoostClassifier,GradientBoostingClassifier,DecisionTreeClassifier,ExtraTreeClassifier]\nlabels=['AdaBoost','GradientBoosting','DecisionTree','ExtraTree']\npylab.figure(figsize=(12,5)); df_importance=pandas.DataFrame(columns=range(13))\nfor i in range(4):\n    df_importance.loc[i]=classifiers[i]().fit(X4,y4).feature_importances_\n    pylab.plot(df_importance.loc[i],'-v',label=labels[i],markersize=7,markerfacecolor=\"None\",markeredgewidth=2)\npylab.legend(loc=2,fontsize=10); pylab.title(\"Importance of Features; Wine Dataset\"); pylab.show()","fa15e7d7":"# Univariate Selection; Wine Dataset\nselect_kbest=SelectKBest(score_func=chi2,k=4); fit_select_kbest=select_kbest.fit(X4,y4)\nfeatures_select_kbest=fit_select_kbest.transform(X4)\nprint((\"Kbest scores: \\n%s\")%fit_select_kbest.scores_)\nprint((\"Selected features: \\n%s\")%features_select_kbest[0:5,:])\n# Recursive Feature Elimination; Wine Dataset\nrfe=RFE(linear_model.LogisticRegression(solver='liblinear',multi_class='ovr'),3); fit_rfe=rfe.fit(X4,y4)\nprint((\"RFE Selected Features: \\n%s\")%fit_rfe.support_)\nprint((\"Feature Ranking: \\n%s\")%fit_rfe.ranking_)\n# Principal Component Analysis; Boston Dataset\npca3=PCA(n_components=3); fit_pca=pca3.fit(X1)\nprint((\"Explained Variance: \\n%s\\n\")%fit_pca.explained_variance_ratio_)\nprint(fit_pca.components_[0],'\\n'); print(fit_pca.components_[1],'\\n')\nprint(fit_pca.components_[2])","a6decf4b":"t0=time.time(); X_emb=manifold.TSNE(n_components=2,learning_rate=700.0).fit_transform(X3)\nx_min,x_max=numpy.min(X_emb,0),numpy.max(X_emb,0); X_emb=(X_emb-x_min)\/(x_max-x_min)\nf,ax=pylab.subplots(1,figsize=(12,5)); pylab.axis(\"off\")\nfor i in range(X_emb.shape[0]):\n    pylab.text(X_emb[i,0],X_emb[i,1],str(y3[i]),color=pylab.cm.hsv(y3[i]\/10.))\npylab.title(\"t-SNE embedding %f s\"%(time.time()-t0)); pylab.show()","2b26b3a4":"Xy=[sas(X1,y1),sas(X2,y2),sas(X3,y3),sas(X4,y4),sas(X5,y5)]\n[[X_train1,X_test1,y_train1,y_test1],[X_train2,X_test2,y_train2,y_test2],\n [X_train3,X_test3,y_train3,y_test3],[X_train4,X_test4,y_train4,y_test4],\n [X_train5,X_test5,y_train5,y_test5]]=Xy\n[print([Xy[i][j].shape for j in range(4)]) for i in range(5)]\nprint('boston, housing, digits, wine, faces')\ndel X1,y1,X2,y2,X3,y3,X4,y4,X5,y5","bf3fff6c":"Xy2=[sas(X7,y7),sas(X8,y8),sas(X9,y9),sas(X10,y10)]\n[[X_train7,X_test7,y_train7,y_test7],[X_train8,X_test8,y_train8,y_test8],\n [X_train9,X_test9,y_train9,y_test9],[X_train10,X_test10,y_train10,y_test10]]=Xy2\n[print([Xy2[i][j].shape for j in range(4)]) for i in range(4)]\nprint('toy datasets: regression, regression 2, blobs, classification')\ndel X7,y7,X8,y8,X9,y9,X10,y10","e0e37c61":"Xy3=[sas(letter_images,targets),sas(style_images,brands),sas(style_images,products)]\n[[X_train12,X_test12,y_train12,y_test12],[X_train13,X_test13,y_train13,y_test13],\n [X_train14,X_test14,y_train14,y_test14]]=Xy3\n[print([Xy3[i][j].shape for j in range(4)]) for i in range(3)]\nprint('letters and labels, style images and brands, style images and products')\ndel letter_images,targets,backgrounds,style_images,brands,products","6a677fae":"classifier_list,classifier_names,clf_datasets=[],[],[]\nacc_train,acc_test,loss_train,loss_test=[],[],[],[]\ndf_list=['classifier_name','acc_train','acc_test','loss_train','loss_test']\nclf=[linear_model.LogisticRegression(solver='liblinear',multi_class='ovr'),\n     linear_model.LogisticRegressionCV(solver='liblinear',multi_class='ovr'),\n     linear_model.SGDClassifier(max_iter=1000,tol=0.00001),\n     linear_model.RidgeClassifier(),linear_model.RidgeClassifierCV(),\n     LinearDiscriminantAnalysis(),QuadraticDiscriminantAnalysis(),\n     svm.LinearSVC(),svm.SVC(gamma='scale',C=10.0,kernel='poly'),svm.NuSVC(gamma='scale',kernel='poly'),\n     KNeighborsClassifier(),RadiusNeighborsClassifier(radius=30),NearestCentroid(),\n     DecisionTreeClassifier(),ExtraTreeClassifier(),GaussianNB(),BernoulliNB(),MultinomialNB(),\n     BaggingClassifier(),RandomForestClassifier(n_estimators=64),\n     AdaBoostClassifier(),GradientBoostingClassifier(),\n     linear_model.Perceptron(max_iter=1000,tol=0.00001),\n     linear_model.PassiveAggressiveClassifier(max_iter=1000,tol=0.00001),\n     GaussianProcessClassifier(),LabelPropagation(),LabelSpreading()]","ad822270":"list3clf=['LogisticRegression','LogisticRegressionCV','SGDClassifier','RidgeClassifier', 'RidgeClassifierCV',\n          'LinearDiscriminantAnalysis','QuadraticDiscriminantAnalysis','LinearSVC', 'SVC','NuSVC',\n          'KNeighborsClassifier','RadiusNeighborsClassifier','NearestCentroid', \n          'DecisionTreeClassifier','ExtraTreeClassifier',\n          'GaussianNB','BernoulliNB','MultinomialNB','BaggingClassifier','RandomForestClassifier',\n          'AdaBoostClassifier','GradientBoostingClassifier','Perceptron','PassiveAggressiveClassifier']\ny3clf=[]\nfor i in range(len(list3clf)):\n    y3clf.append(classifier_fit_score(clf[i],list3clf[i],'Digits',X_train3,X_test3,y_train3,y_test3)[:2])\n[[y_train31,y_test31],[y_train32,y_test32],[y_train33,y_test33],[y_train34,y_test34],[y_train35,y_test35],\n[y_train36,y_test36],[y_train37,y_test37],[y_train38,y_test38],[y_train39,y_test39],[y_train310,y_test310],\n[y_train311,y_test311],[y_train312,y_test312],[y_train313,y_test313],[y_train314,y_test314],[y_train315,y_test315],\n[y_train316,y_test316],[y_train317,y_test317],[y_train318,y_test318],[y_train319,y_test319],[y_train320,y_test320],\n[y_train321,y_test321],[y_train322,y_test322],[y_train323,y_test323],[y_train324,y_test324]]=y3clf\ny_train925,y_test925=classifier_fit_score(clf[24],'GaussianProcessClassifier','Toy Blobs',\n                                          X_train9,X_test9,y_train9,y_test9)[:2]\ny_train926,y_test926=classifier_fit_score(clf[25],'LabelPropagation','Toy Blobs',\n                                          X_train9,X_test9,y_train9,y_test9)[:2]\ny_train927,y_test927=classifier_fit_score(clf[26],'LabelSpreading','Toy Blobs',\n                                          X_train9,X_test9,y_train9,y_test9)[:2]\n#y_train1211,y_test1211=classifier_fit_score(clf[10],'KNeighborsClassifier','Handwritten Letters',\n#                       X_train12.reshape(-1,32*32*3),X_test12.reshape(-1,32*32*3),y_train12,y_test12)[:2]\n#y_train1219,y_test1219=classifier_fit_score(clf[18],'BaggingClassifier','Handwritten Letters',\n#                       X_train12.reshape(-1,32*32*3),X_test12.reshape(-1,32*32*3),y_train12,y_test12)[:2]\n#y_train1220,y_test1220=classifier_fit_score(clf[19],'RandomForestClassifier','Handwritten Letters',\n#                       X_train12.reshape(-1,32*32*3),X_test12.reshape(-1,32*32*3),y_train12,y_test12)[:2]\n#y_train1222,y_test1222=classifier_fit_score(clf[21],'GradientBoostingClassifier','Handwritten Letters',\n#                       X_train12.reshape(-1,32*32*3),X_test12.reshape(-1,32*32*3),y_train12,y_test12)[:2]","1527afee":"df_classifier_results=get_classifier_results(); df_classifier_results.to_csv('classifier_results.csv')\ndf_classifier_results[df_list].sort_values('acc_test',ascending=False).style.set_properties(**style_dict)","e25d5f3d":"pylab.figure(figsize=(12,5)); n=50; x=range(n)\npylab.scatter(x,y_test3[:n],marker='*',s=600,color='royalblue',label='Real data')\npylab.scatter(x,y_test39[:n],marker='v',s=200,color='darkblue',label='SVC')\npylab.scatter(x,y_test32[:n],marker='s',s=100,color='darkgrey',label='Logistic RegressionCV')\npylab.scatter(x,y_test311[:n],marker='o',s=50,color='darkgreen',label='KNeighborsClassifier')\npylab.xlabel('Observations'); pylab.ylabel('Targets') \npylab.title('Classifiers. Test Results. Digits')\npylab.legend(loc=2,fontsize=10); pylab.show()","be7b2c28":"regressor_list,regressor_names,reg_datasets=[],[],[]\nr2_train,r2_test,ev_train, ev_test,mse_train,mse_test,mae_train,mae_test,mdae_train,mdae_test=\\\n[],[],[],[],[],[],[],[],[],[]\ndf_list2=['regressor_name','r2_train','r2_test','ev_train','ev_test',\n          'mse_train','mse_test','mae_train','mae_test','mdae_train','mdae_test']\nreg=[linear_model.LinearRegression(),linear_model.Ridge(),linear_model.RidgeCV(),\n     linear_model.Lasso(),linear_model.LassoLarsCV(),linear_model.RANSACRegressor(),\n     linear_model.BayesianRidge(),linear_model.ARDRegression(),\n     linear_model.HuberRegressor(),linear_model.TheilSenRegressor(),\n     PLSRegression(),DecisionTreeRegressor(),ExtraTreeRegressor(),\n     BaggingRegressor(),AdaBoostRegressor(),GradientBoostingRegressor(),RandomForestRegressor(),\n     linear_model.PassiveAggressiveRegressor(max_iter=1000,tol=0.001),linear_model.ElasticNet(),\n     linear_model.SGDRegressor(max_iter=1000,tol=0.001),svm.SVR(),KNeighborsRegressor(),\n     RadiusNeighborsRegressor(radius=1.5),GaussianProcessRegressor()]","67811d8e":"list1reg=['LinearRegression','Ridge','RidgeCV','Lasso','LassoLarsCV','RANSACRegressor',\n          'BayesianRidge','ARDRegression','HuberRegressor',\n          'TheilSenRegressor','PLSRegression','DecisionTreeRegressor',\n          'ExtraTreeRegressor','BaggingRegressor','AdaBoostRegressor',\n          'GradientBoostingRegressor','RandomForestRegressor']\ny1reg=[]; y7reg=[]\nfor i in range(len(list1reg)):\n    y1reg.append(regressor_fit_score(reg[i],list1reg[i],'Boston',X_train1,X_test1,y_train1,y_test1)[:2])\n[[y_train101,y_test101],[y_train102,y_test102],[y_train103,y_test103],\n [y_train104,y_test104],[y_train105,y_test105],[y_train106,y_test106],\n [y_train107,y_test107],[y_train108,y_test108],[y_train109,y_test109],\n [y_train110,y_test110],[y_train111,y_test111],[y_train112,y_test112],\n [y_train113,y_test113],[y_train114,y_test114],[y_train115,y_test115],\n [y_train116,y_test116],[y_train117,y_test117]]=y1reg\nlist7reg=['PassiveAggressiveRegressor','ElasticNet','SGDRegressor','SVR',\n          'KNeighborsRegressor','RadiusNeighborsRegressor','GaussianProcessRegressor']\nfor i in range(len(list7reg)):\n    y7reg.append(regressor_fit_score(reg[i],list7reg[i],'Toy Regression',X_train7,X_test7,y_train7,y_test7)[:2])\n[[y_train718,y_test718],[y_train719,y_test719],[y_train720,y_test720],[y_train721,y_test721],\n [y_train722,y_test722],[y_train723,y_test723],[y_train724,y_test724]]=y7reg","82bc7df3":"df_regressor_results=get_regressor_results(); df_regressor_results.to_csv('regressor_results.csv')\ndf_regressor_results[df_list2].sort_values('r2_test',ascending=False).style.set_properties(**style_dict)","84162a7e":"pylab.figure(figsize=(12,5)); n=30; x=range(n)\npylab.scatter(x,y_test1[:n],marker='*',s=100,color='black',label='Real data')\npylab.plot(x,y_test116[:n],lw=2,label='Gradient Boosting'); pylab.plot(x,y_test117[:n],lw=2,label='Random Forest')\npylab.plot(x,y_test114[:n],lw=2,label='Bagging'); pylab.plot(x,y_test115[:n],lw=2,label='Ada Boost')\npylab.plot(x,y_test113[:n],lw=2,label='ExtraTree')\npylab.xlabel('Observations'); pylab.ylabel('Targets'); pylab.title('Regressors. Test Results. Boston')\npylab.legend(loc=2,fontsize=10); pylab.show()","e2361bb2":"# Kernel Ridge; Toy regression 2\nreg25=KernelRidge(); reg25.fit(X_train8,y_train8); y_train825=reg25.predict(X_train8) \ny_test825=reg25.predict(X_test8); print(reg25.score(X_test8,y_test8))\npylab.figure(figsize=(12,5)); n=30; x=range(n)\npylab.scatter(x,y_test8[:n,0],marker='*',s=200,color='darkblue',label='Real data 1')\npylab.scatter(x,y_test8[:n,1],marker='*',s=200,color='darkgreen',label='Real data 2')\npylab.plot(x,y_test825[:n,0],lw=2,color='steelblue',label='Kernel Ridge 1')\npylab.plot(x,y_test825[:n,1],lw=2,color='seagreen',label='Kernel Ridge 2')\npylab.xlabel('Observations'); pylab.ylabel('Targets') \npylab.title('Kernel Ridge Regressor. Test Results. Toy Regression 2')\npylab.legend(loc=2,fontsize=10); pylab.show()","7f0879a3":"usl=[mixture.GaussianMixture(n_components=4,n_init=4),mixture.BayesianGaussianMixture(n_components=4,n_init=4),\n     manifold.Isomap(),manifold.LocallyLinearEmbedding(),manifold.SpectralEmbedding(),manifold.MDS(),manifold.TSNE()]\n# Gaussian Mixture; Toy blobs\nusl[0].fit(X_train9,y_train9); y_test91=usl[0].predict(X_test9)\nusl[1].fit(X_train9,y_train9); y_test92=usl[1].predict(X_test9)","be25daaf":"pylab.figure(figsize=(12,12)); pylab.scatter(X_test9[:,0],X_test9[:,1],c=y_test9,cmap=pylab.cm.tab10)\npylab.scatter(X_test9[:,0]+0.03,X_test9[:,1]+0.03,c=y_test91,alpha=0.4,cmap=pylab.cm.autumn)\npylab.scatter(X_test9[:,0]+0.06,X_test9[:,1]+0.06,c=y_test92,alpha=0.4,cmap=pylab.cm.winter)\npylab.scatter([1,-1,1,-1],[1,-1,-1,1],c='black',marker='*',s=150); pylab.show()","00f5b362":"# Multi-layer Perceptron; Classifier; Digits\nnn_clf1=MLPClassifier(hidden_layer_sizes=(512,),max_iter=70,solver='sgd',\n                      verbose=1,random_state=1,learning_rate_init=.01)\nnn_clf1.fit(X_train3,y_train3); nn_clf1.score(X_train3,y_train3),nn_clf1.score(X_test3,y_test3)","d47965de":"# Multi-layer Perceptron; Classifier; News\nnn_clf2=MLPClassifier(hidden_layer_sizes=(128,),max_iter=3,solver='adam',\n                      verbose=1,random_state=1,learning_rate_init=.01)\nnn_clf2.fit(X_train6,y_train6); print(nn_clf2.score(X_test6,y_test6))\ny_test6_predictions=nn_clf2.predict(X_test6)\npylab.figure(figsize=(12,5)); pylab.scatter(range(100),y_test6[:100],s=100)\npylab.scatter(range(100),y_test6_predictions[:100],s=25); pylab.show()","1b16e6fe":"# Multi-layer Perceptron; Classifier; Handwritten digits\nnn_clf3=MLPClassifier(hidden_layer_sizes=(256,),max_iter=5,solver='adam',\n                      verbose=1,random_state=1,learning_rate_init=.01)\nnn_clf3.fit(X_train11.reshape(-1,784),y_train11)\nprint(nn_clf3.score(X_test11.reshape(-1,784),y_test11))\ny_test11_predictions=nn_clf3.predict(X_test11.reshape(-1,784))\npylab.figure(figsize=(12,5)); pylab.scatter(range(100),y_test11[:100],s=100)\npylab.scatter(range(100),y_test11_predictions[:100],s=25); pylab.show()","e4b265a2":"# Multi-layer Perceptron; Classifier; Handwritten letters\nnn_clf4=MLPClassifier(hidden_layer_sizes=(1024,),max_iter=150,solver='sgd',\n                      verbose=0,random_state=1,learning_rate_init=.01)\nnn_clf4.fit(X_train12.reshape(-1,32*32*3),y_train12)\nnn_clf4.score(X_train12.reshape(-1,32*32*3),y_train12),\\\nnn_clf4.score(X_test12.reshape(-1,32*32*3),y_test12)","a6911db3":"# Multi-layer Perceptron; Regressor; Boston\nnn_reg1=MLPRegressor(hidden_layer_sizes=(104,),max_iter=1000,solver='lbfgs',alpha=0.015)\nnn_reg1.fit(X_train1,y_train1); y_test1_nn1=nn_reg1.predict(X_test1)\nnn_reg1.score(X_train1,y_train1),nn_reg1.score(X_test1,y_test1)","56d98244":"pylab.figure(figsize=(12,5)); n=50; x=range(n)\npylab.plot(x,y_test1[:n],'-o',color='black',label='Real data') \npylab.plot(x,y_test1_nn1[:n],'-o',label='MLP Predictions')\npylab.xlabel('Observations'); pylab.ylabel('Targets') \npylab.title('Multi-layer Perceptron. Test Results. Boston')\npylab.legend(loc=2,fontsize=10); pylab.show()","f4382677":"# Restricted Boltzmann machines; Digits\nX_train3_scaled=(X_train3-numpy.min(X_train3,0))\/(numpy.max(X_train3,0)+0.0001)\nX_test3_scaled=(X_test3-numpy.min(X_test3,0))\/(numpy.max(X_test3,0)+0.0001)\nlogistic=linear_model.LogisticRegression(solver='liblinear',multi_class='ovr',max_iter=50,tol=0.0001,C=5000.0)\nbrbm=BernoulliRBM(random_state=0,verbose=False); brbm.learning_rate,brbm.n_iter,brbm.n_components=0.05,50,64\nnn_clf2=Pipeline(steps=[('brbm',brbm),('logistic',logistic)])\nnn_clf2.fit(X_train3_scaled,y_train3)","becc7dec":"print(\"Logistic regression using BRBM features:\\n%s\\n\"%\\\n      (classification_report(y_test3,nn_clf2.predict(X_test3_scaled))))","b7815fb3":"## scaling","4f7d0a07":"<h1 style=\"color:royalblue; font-family:Ewert; font-size:200%;\" class=\"font-effect-3d\">Code Library, Style and Links<\/h1>\n\n[Version with SageMath Cells](https:\/\/olgabelitskaya.github.io\/sklearn_cookbook_sagecells.html)","d0e76299":"<h1 style=\"color:royalblue; font-family:Ewert; font-size:200%;\" class=\"font-effect-3d\">Extraction and Preprocessing<\/h1>\n## extraction","44548d50":"## features' importance","687b6bd8":"<h1 style=\"color:royalblue; font-family:Ewert; font-size:200%;\" class=\"font-effect-3d\">Supervised Learning. Classification<\/h1>","86aae16c":"## imputation of missing values","face264a":"## one-hot encoding","8846e1db":"### combining regression with kernels","23f38f38":"## external datasets","0ec49031":"<h1 style=\"color:royalblue; font-family:Ewert; font-size:200%;\" class=\"font-effect-3d\">Supervised Learning. Regression<\/h1>","f2a6419b":"## artificial datasets","4788cef3":"### unsupervised","26b93556":"## shuffling and splitting","272fa292":"<h1 style=\"color:royalblue; font-family:Ewert; font-size:200%;\" class=\"font-effect-3d\">Data<\/h1>\n## internal datasets\n[Dataset loading utilities](http:\/\/scikit-learn.org\/stable\/datasets\/index.html#datasets)","ee3b2255":"<h1 style=\"color:royalblue; font-family:Ewert; font-size:200%;\" class=\"font-effect-3d\">Neural Networks<\/h1>\n### supervised","cdac8645":"<h1 style=\"color:royalblue; font-family:Ewert; font-size:200%;\" class=\"font-effect-3d\">Unsupervised learning<\/h1>","29cb4b2e":"## dimensionality reduction\nUnivariate Selection, Recursive Feature Elimination, Principal Component Analysis, etc."}}