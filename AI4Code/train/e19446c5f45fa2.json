{"cell_type":{"0e885f32":"code","4e0d7110":"code","e71bcd1f":"code","b73cadc1":"code","20776201":"code","a1ea96d0":"code","166362db":"code","a9b57eca":"code","bf037485":"code","fbb8ce60":"code","3f987d96":"code","9b68cc25":"code","8c012468":"code","c9de274d":"code","ff36a3a1":"code","faf57da6":"code","46cca875":"code","f07ad061":"markdown","3a05967a":"markdown","6efa5bfa":"markdown","eab8f17a":"markdown","83ad79cf":"markdown","88e41328":"markdown","5de06f86":"markdown","3a3bd63d":"markdown","2b6bdb3b":"markdown","cc112e6f":"markdown","42d384a3":"markdown","84204cb3":"markdown","90521437":"markdown","2b88f26e":"markdown","4e4c59ef":"markdown","95c6e0da":"markdown","20d313b9":"markdown","a285e83f":"markdown","360a208d":"markdown","0194093b":"markdown","73f2d1e8":"markdown","15522d52":"markdown","35847bd7":"markdown"},"source":{"0e885f32":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns; sns.set(style=\"ticks\", color_codes=True)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","4e0d7110":"data = pd.read_csv('..\/input\/Seed_Data.csv')\ndata.sample(5)","e71bcd1f":"data.info()","b73cadc1":"data.describe()","20776201":"plt.figure(figsize=[8,8])\nsns.heatmap(data.corr(), annot=True, cmap=\"YlGnBu\")\nplt.title('Correlations of the Features')\nplt.show()","a1ea96d0":"sns.countplot(data['target'], palette='husl')\nplt.show()","166362db":"i = sns.pairplot(data, vars = ['A', 'P', 'C', 'LK', 'WK', 'A_Coef', 'LKG'] ,hue='target', palette='husl')\nplt.show()","a9b57eca":"a = sns.FacetGrid(data, col='target')\na.map(sns.boxplot, 'A', color='yellow', order=['0', '1', '2'])\n\np = sns.FacetGrid(data, col='target')\np.map(sns.boxplot, 'P', color='orange', order=['0', '1', '2'])\n\nc = sns.FacetGrid(data, col='target')\nc.map(sns.boxplot, 'C', color='red', order=['0', '1', '2'])\n\nlk = sns.FacetGrid(data, col='target')\nlk.map(sns.boxplot, 'LK', color='purple', order=['0', '1', '2'])\n\nwk = sns.FacetGrid(data, col='target')\nwk.map(sns.boxplot, 'WK', color='blue', order=['0', '1', '2'])\n\nacoef = sns.FacetGrid(data, col='target')\nacoef.map(sns.boxplot, 'A_Coef', color='cyan', order=['0', '1', '2'])\n\nlkg = sns.FacetGrid(data, col='target')\nlkg.map(sns.boxplot, 'LKG', color='green', order=['0', '1', '2'])","bf037485":"# Excluding target feature and create a new dataset:\ndf = data.iloc[:,0:7]\ndf.head(3)","fbb8ce60":"from sklearn.cluster import KMeans\n\nwcss = []\n\nfor k in range(1,10):\n    kmeans = KMeans(n_clusters=k)\n    kmeans.fit(df)\n    wcss.append(kmeans.inertia_)\n    \n# Visualization of k values:\n\nplt.plot(range(1,10), wcss, color='red')\nplt.title('Graph of k values and WCSS')\nplt.xlabel('k values')\nplt.ylabel('wcss values')\nplt.show()","3f987d96":"# Now we know our best k value is 3, I am creating a new kmeans model:\nkmeans2 = KMeans(n_clusters=3)\n\n# Training the model:\nclusters = kmeans2.fit_predict(df)\n\n# Adding a label feature with the predicted class values:\ndf_k = df.copy(deep=True)\ndf_k['label'] = clusters","9b68cc25":"fig, (ax1, ax2) = plt.subplots(1,2)\n\nax1 = plt.subplot(1,2,1)\nplt.title('Original Classes')\nsns.scatterplot(x='A', y='P', hue='target', style='target', data=data, ax=ax1)\n\nax2 = plt.subplot(1,2,2)\nplt.title('Predicted Classes')\nsns.scatterplot(x='A', y='P', hue='label', style='label', data=df_k, ax=ax2)\nplt.show()","8c012468":"print('Original Data Classes:')\nprint(data.target.value_counts())\nprint('-' * 30)\nprint('Predicted Data Classes:')\nprint(df_k.label.value_counts())","c9de274d":"from scipy.cluster.hierarchy import linkage, dendrogram\nplt.figure(figsize=[10,10])\nmerg = linkage(df, method='ward')\ndendrogram(merg, leaf_rotation=90)\nplt.title('Dendrogram')\nplt.xlabel('Data Points')\nplt.ylabel('Euclidean Distances')\nplt.show()","ff36a3a1":"from sklearn.cluster import AgglomerativeClustering\n\nhie_clus = AgglomerativeClustering(n_clusters=3, affinity='euclidean', linkage='ward')\ncluster2 = hie_clus.fit_predict(df)\n\ndf_h = df.copy(deep=True)\ndf_h['label'] = cluster2","faf57da6":"plt.title('Original Classes')\nsns.scatterplot(x='A', y='P', hue='target', style='target', data=data)\nplt.show()\nplt.title('K-Means Classes')\nsns.scatterplot(x='A', y='P', hue='label', style='label', data=df_k)\nplt.show()\nplt.title('Hierarchical Classes')\nsns.scatterplot(x='A', y='P', hue='label', style='label', data=df_h)\nplt.show()","46cca875":"print('Original Data Classes:')\nprint(data.target.value_counts())\nprint('-' * 30)\nprint('K-Means Predicted Data Classes:')\nprint(df_k.label.value_counts())\nprint('-' * 30)\nprint('Hierarchical Predicted Data Classes:')\nprint(df_h.label.value_counts())","f07ad061":"We can see our models' differences from the comparision of our algorithms' class counts.\n\nHope you enjoy my clustering example. If you have a better\/quicker way or a suggestion please write a comment below.\n\nAnd I would appreciate a lot if you up vote my kernel.","3a05967a":"From the dendrogram we can read there are 3 classes in our data set.","6efa5bfa":"Let's see how many target classes we have in our data:","eab8f17a":"Let's see our data's information:","83ad79cf":"Now I am going to visualize each feature. But in order to see the differences between the classes, I am going to show three boxplots for each feature. Each boxplot will show us the corresponding target class' values.","88e41328":"**HIERARCHICAL CLUSTERING ALGORITHM:**\n\n**Creating the Dendrogram:**\n\nWe use dendrogram to find how many classes we have in our data set.","5de06f86":"**EDA:**","3a3bd63d":"So we have equally separeted 70 x 3 classes of wheat kernels in our data.\n\nLet's visualize all the features using a pairplot:","2b6bdb3b":"And I want to see the correlations between the features:","cc112e6f":"Now we are ready for creating our machine learning clustering algorithms. \n\nIn this kernel I will use two methods; **k-means clustering** and **hierarchical clustering** algorithms.\n\nFor k-means clustering algorithm;\n* First of all I will find the best k value.\n* Than I will use this k value to create a k-means model.\n* And I will compare my original and k-means clustered datas.\n\nFor hierarchical clustering algorithm;\n* First I will apply a dendrogram in order to find how many classes do I have in my data.\n* Than I will use this class number to apply a hierarchical clustering algorithm.\n* Lastly I will compare my original, k-means and hierarchical clustered datas.\n","42d384a3":"**Hierarchical Clustering Algorithm:**","84204cb3":"We have 210 rows and 8 columns. There aren't any NaN value so we don't have to manipulate the missing values.\nThere is a column called target. That is our wheat's classification. Later I will exclude it from the data. Because my goal is to use sklearn clustering algorithms to classify different wheat types.","90521437":"Measurements of geometrical properties of kernels belonging to three different varieties of wheat.\n\nData has three different varieties of wheat: Kama, Rosa and Canadian.\n\nTo construct the data, seven geometric parameters (features) of wheat kernels were measured:\n\n* area A,\n* perimeter P,\n* compactness C = 4*pi*A\/P^2,\n* length of kernel,\n* width of kernel,\n* asymmetry coefficient\n* length of kernel groove.\n* target (Kama, Rosa or Canadian)","2b88f26e":"**Comparing Original, K-Means and Hierarchical Clustered Classes:**","4e4c59ef":"**K-MEANS AND HIERARCHICAL CLUSTERING ALGORITHMS THROUGH WHEAT GRAIN CLASSIFICATION**","95c6e0da":"**K-Means Clustering Algorithm:**","20d313b9":"We can easily see the three target classes from our pairplot above.","a285e83f":"**CREATING THE DATA SET:**","360a208d":"As an example; from the graph above, we have three boxplots showing LKG feature values according to three classes. \n* If our wheat grain has a LKG value around 4.9-5.2 it belongs to target 0, \n* If it's around 5.9-6.2 it belongs to target 1, \n* And finally if it's around 5.0-5.3 it belongs to target 2.","0194093b":"**Comparing Original Classes and K-Means Algorithm Classes:**\n\nFor visualization I will use only two features (A and P) for the original and predicted datasets. Different classes will have seperate color and styles.","73f2d1e8":"We already know that our data set has three classes. \n\nBu if we didn't know how many classes we have in our data set, we sould use this method. \n\nAnd according to the graph; it's elbow (where it bends) is the best k value for our K Means algorithm.","15522d52":"First of all I will have a look at my data features' statistics:","35847bd7":"**K-MEANS CLUSTERING ALGORITHM:**\n\n**Finding the best K value:**"}}