{"cell_type":{"aedc756a":"code","31f77d5c":"code","175dad9b":"code","ce359f6e":"code","0e62eb73":"code","268a8f45":"code","ac0243d6":"code","adeeead5":"code","b7d0cdaf":"markdown"},"source":{"aedc756a":"import numpy as np\nimport pandas as pd\nimport random\nimport os\nimport gc\n\nimport pickle\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.nn as nn\n\nfrom transformers import AdamW\nfrom sklearn.model_selection import KFold\n\nfrom tqdm.auto import tqdm\nfrom matplotlib import pyplot as plt\nfrom sklearn.metrics import mean_squared_error\nimport ubiquant\n\ndevice = torch.device(\"cuda\")","31f77d5c":"class GCF:\n    INPUT_ROOT = \"\/kaggle\/input\/ump-npy-dataset\/\"\n    MODEL_ROOT = \"\/kaggle\/input\/ump-train-lstm-pytorch\"\n    SCALER_PATH = \"\/kaggle\/input\/ump-npy-dataset\/std_scaler.pkl\"\n    LATEST_FEATURES = '\/kaggle\/input\/ump-features-history\/latest_features_128.npy'\n    \n    SEED = 0\n    MAX_LEN = 128\n    N_FOLDS = 5\n    \n    BS = 256\n    HIDDEN_SIZE = 128\n    FEAT_COLS = [f\"f_{i}\" for i in range(300)]","175dad9b":"class UMPLSTM(nn.Module):\n    def __init__(self):\n        super(UMPLSTM, self).__init__()\n        conv_out = 64\n        pool_k = 16\n\n        self.lstm = nn.LSTM(300, GCF.HIDDEN_SIZE, batch_first=True, num_layers=1, dropout=0.0)        \n        self.head = nn.Sequential(\n            nn.Linear(GCF.HIDDEN_SIZE, GCF.HIDDEN_SIZE),\n            nn.LayerNorm(GCF.HIDDEN_SIZE),\n            nn.ReLU(),\n            nn.Linear(GCF.HIDDEN_SIZE, 1),\n        )\n        \n        self.criterion = nn.MSELoss(reduction='none')\n        \n    def forward(self, _x, _y=None):\n        out, _ = self.lstm(_x)\n        regr = self.head(out)\n        regr = regr.squeeze(2)\n        \n        if _y is None:\n            return None, regr\n        \n        mask = (_y != 999).float()\n        loss = self.criterion(regr, _y)\n        loss = (loss * mask).mean()\n        \n        return loss, regr","ce359f6e":"load_path = f\"{GCF.MODEL_ROOT}\/ump_lstm_f0_best_rmse.pth\"\nmodel = UMPLSTM()\nmodel.load_state_dict(torch.load(load_path, map_location=torch.device('cpu')))\nmodel.to(device)\nmodel.eval()","0e62eb73":"scaler = pickle.load(open(GCF.SCALER_PATH, \"rb\"))\nscaler","268a8f45":"latest_features = np.load(GCF.LATEST_FEATURES)\nlatest_features.shape","ac0243d6":"env = ubiquant.make_env()   # initialize the environment\niter_test = env.iter_test()    # an iterator which loops over the test set and sample submission\nfor (test_df, sample_prediction_df) in iter_test:\n    _id = test_df['investment_id'].values\n    \n    # update new investment_id\n    if len(latest_features) <= _id.max():\n        add = _id.max() - len(latest_features) + 1\n        new_f = np.zeros((add, GCF.MAX_LEN, 300))\n        latest_features = np.vstack([latest_features, new_f])\n        \n    curr_x = scaler.transform(test_df[GCF.FEAT_COLS].values)\n    prev_x = torch.tensor(latest_features[_id, 1:])\n    x = torch.cat([prev_x, torch.tensor(curr_x).unsqueeze(1)], dim = 1)\n    \n    with torch.no_grad():\n        _, pred = model(x.float().to(device))\n\n    sample_prediction_df['target'] = pred[:, -1].cpu().tolist()  # make your predictions here\n    env.predict(sample_prediction_df)   # register your predictions\n    \n    # status update\n    latest_features[_id] = x.cpu().numpy()\n    \n    gc.collect()","adeeead5":"sample_prediction_df","b7d0cdaf":"# UMP Infer LSTM (pytorch)\n\nThis code is infer LSTM model.\n\nTraining code is here:  \nhttps:\/\/www.kaggle.com\/takamichitoda\/ump-train-lstm-pytorch\n\nLSTM model needs to input previous feature\u2019s history.  \nThat dataset is here:  \nhttps:\/\/www.kaggle.com\/takamichitoda\/ump-features-history\n\nIn this code, I infer by concatenating this historical data with the data from the API."}}