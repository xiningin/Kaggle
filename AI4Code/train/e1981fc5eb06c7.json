{"cell_type":{"63644971":"code","aa189011":"code","3f7e9691":"code","88f755bd":"code","1c0a4ac1":"code","d9a2a9b7":"code","1f65e9c4":"code","d1d91cb7":"code","db6d9171":"code","32177f99":"code","dc2fc9a0":"code","cd77b945":"code","eb2c0053":"code","e1203e4a":"code","8298407d":"code","3a528420":"code","7e81bba6":"code","cf96315c":"code","62e1f9f6":"code","6e4d5a5d":"code","0f6f762e":"code","01f5f95c":"code","009a5219":"code","3a1b72c2":"code","b3b3bfc2":"code","a92dc5e1":"code","2a20b164":"code","ee76968c":"code","d9e11d06":"code","c5033f4f":"code","a3620792":"code","43bdbb6e":"code","9a0493e0":"code","50deb224":"code","b6b80d2b":"code","27415205":"code","6b3064ce":"code","cf0607eb":"code","9ae8b9c1":"code","0ebd26ad":"code","2c89bb2f":"code","cb8c35fd":"code","efa45ec7":"code","00106040":"markdown","a3690308":"markdown","6f72ee5a":"markdown","5ec19817":"markdown","f577baf1":"markdown","7f6c1afa":"markdown","c7e61fc2":"markdown","a9221b0a":"markdown","0f7d5b36":"markdown","9541fc0b":"markdown","3748ae8c":"markdown","e302a9b4":"markdown","33a5dfbc":"markdown","fbb34a06":"markdown","c0a002e2":"markdown","3841ec04":"markdown","b2044894":"markdown","4bbd2158":"markdown","a5d00035":"markdown","d7a8c5ac":"markdown","b6f80e93":"markdown","3d0f1fef":"markdown"},"source":{"63644971":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd# data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","aa189011":"dataframe = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntest = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\n","3f7e9691":"\ndataframe.info()\nprint(dataframe.columns.values)","88f755bd":"sns.countplot(dataframe['Survived'], hue = dataframe['Sex'])\nDead, lives = dataframe.Survived.value_counts()\nmale, female = dataframe.Sex.value_counts()\nprint(\"Percentage of Male on ship:\", round(male\/(male+female)*100) )\nprint(\"Percentage of Female on ship:\", round(female\/(male+female)*100 ))","1c0a4ac1":"from matplotlib import pyplot as plt\nplt.figure(figsize=(40,8))\nsns.countplot(dataframe['Age'], hue = dataframe['Survived'])","d9a2a9b7":"#find out how many classes on ship\ndataframe.Pclass.unique()\n#so there are 3 classes on ship\ndataframe.Pclass.value_counts()\n#in which 1st, 2nd and 3rd class has 216, 184 and 491 respectively \n#The graph shows the most people died in 3rd class which is obvious from the\n#number of people who bought 3rd class tickets are high\n","1f65e9c4":"sns.countplot(dataframe['Pclass'], hue = dataframe['Survived'])\nt_p = dataframe.groupby('Pclass')['Survived']\nprint(t_p.sum())","d1d91cb7":"#The Embarked class does not give much info other than S class embarkment has ppl from all different classes\nsns.countplot(dataframe['Embarked'], hue = dataframe['Survived'])","db6d9171":"sns.distplot(dataframe['Fare'])\ndataframe['Fare'].describe()","32177f99":"#find the null values in different columns first \n#Find the null values if any in our DataFrame\ndataframe.isnull().values.any()\ndataframe.isnull().sum()\n\n","dc2fc9a0":"#for test dataset\ntest.isnull().sum()","cd77b945":"dataframe['Age'].fillna(round(dataframe['Age'].mean()), inplace = True)\ntest['Fare'].fillna(test['Fare'].mean(), inplace = True)\n#doing the same for Test\ntest['Age'].fillna(round(test['Age'].mean()), inplace = True)\n","eb2c0053":"dataframe.isnull().sum()\n","e1203e4a":"test.isnull().sum()","8298407d":"dataframe['Age'].head(10)","3a528420":"import seaborn as sns\ncorrelations = dataframe[dataframe.columns].corr(method='pearson')\nsns.heatmap(correlations, cmap=\"YlGnBu\", annot = True)","7e81bba6":"import heapq\n\nprint('Absolute overall correlations')\nprint('-' * 30)\ncorrelations_abs_sum = correlations[correlations.columns].abs().sum()\nprint(correlations_abs_sum, '\\n')\n\nprint('Weakest correlations')\nprint('-' * 30)\nprint(correlations_abs_sum.nsmallest(4))","cf96315c":"train_set = dataframe.drop( ['Name','Cabin', 'Ticket','PassengerId', ], axis = 1)\ntest_set = test.drop( ['Name','Cabin', 'Ticket', 'PassengerId', ], axis = 1)\n","62e1f9f6":"test_set.isnull().sum()","6e4d5a5d":"train_set['Embarked'].fillna(train_set['Embarked'].mode()[0], inplace = True)\n","0f6f762e":"#train_set = train_set.dropna()\n#test_set = test_set.dropna()","01f5f95c":"train_set.head()","009a5219":"test_set.head()","3a1b72c2":"y = train_set.iloc[:, 0].values\nX = train_set.iloc[:, train_set.columns != 'Survived'].values\nprint(X[0])","b3b3bfc2":"from sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder\nct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [6])], remainder='passthrough')\nX = np.array(ct.fit_transform(X))\nprint(X[2])","a92dc5e1":"#for test\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder\nct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [6])], remainder='passthrough')\ntest_set = np.array(ct.fit_transform(test_set))\nprint(test_set[1])\n","2a20b164":"from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nX[:, 4 ] = le.fit_transform(X[:,4])\n\nprint(X[1])\n","ee76968c":"#for test set\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\ntest_set[:, 4] = le.fit_transform(test_set[:,4])\nprint(test_set[2])","d9e11d06":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20 )","c5033f4f":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test =sc.transform(X_test)","a3620792":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV\n#from sklearn.linear_model import Ridge\n\nrfc=RandomForestClassifier()\nparameters= {'n_estimators':[ 100,200,300,400, 600],\n             'max_depth':[3,4,6,7],\n             'criterion':['entropy','gini']\n    }\n\nrfc=GridSearchCV(rfc, param_grid=parameters, cv = 5)\nrfc.fit(X_train,y_train)\nprint(\"The best value of leanring rate is: \",rfc.best_params_, )","43bdbb6e":"#RandomForest\n\nfrom sklearn.ensemble import RandomForestClassifier\nrf_model = RandomForestClassifier(criterion= 'gini', n_estimators = 100 ,max_depth = 6, random_state = 0)\nrf_model.fit(X_train, y_train)\ny_pred = rf_model.predict(X_test)\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nRandom_forest_acc= accuracy_score(y_test, y_pred)\nprint('acc = ', Random_forest_acc )\n","9a0493e0":"from sklearn.linear_model import LogisticRegression\nmodel = LogisticRegression()\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_test)\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nLogisticReg_acc= accuracy_score(y_test, y_pred)\nprint('acc = ', LogisticReg_acc )","50deb224":"from sklearn.svm import SVC\nmodel = SVC(kernel = 'rbf', random_state = 0)\nmodel.fit(X, y)\ny_pred = model.predict(X_test)\n\n\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nSVC_acc = accuracy_score(y_test, y_pred)\nprint('acc = ', SVC_acc )","b6b80d2b":"from sklearn.naive_bayes import GaussianNB\nmodel = GaussianNB()\nmodel.fit(X, y)\ny_pred = model.predict(X_test)\n\n\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nGaussian_acc = accuracy_score(y_test, y_pred)\nprint('acc = ', Gaussian_acc )","27415205":"from sklearn.tree import DecisionTreeClassifier\nmodel = DecisionTreeClassifier(criterion = 'entropy', random_state = 0)\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_test)\n\n\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nDT_acc = accuracy_score(y_test, y_pred)\nprint('acc = ', DT_acc )\n","6b3064ce":"from sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.model_selection import GridSearchCV\n#from sklearn.linear_model import Ridge\n\ngbc=GradientBoostingClassifier()\nparameters= {'n_estimators':[ 50,100,200,300, ],\n             'max_depth':[3,4,6,7]\n    }\n\ngbreg=GridSearchCV(gbc, param_grid=parameters, cv = 5 )\ngbreg.fit(X_train,y_train)\nprint(\"The best value of leanring rate is: \",gbreg.best_params_, )","cf0607eb":"from sklearn.ensemble import GradientBoostingClassifier\nmodel_gb = GradientBoostingClassifier(n_estimators = 100, max_depth =4, random_state = 42)\nmodel_gb.fit(X_train, y_train)\ny_pred = model_gb.predict(X_test)\n\n\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nGB_acc = accuracy_score(y_test, y_pred)\nprint('acc = ', GB_acc )","9ae8b9c1":"from xgboost import XGBClassifier\nclassifier = XGBClassifier()\nclassifier.fit(X_train, y_train)\n\n\ny_pred = classifier.predict(X_test)\nxgb_acc = accuracy_score(y_pred, y_test)\nprint('acc=',xgb_acc)","0ebd26ad":"print('RF_acc=', Random_forest_acc)\nprint('Logistic_acc=', LogisticReg_acc)\nprint('SVC_acc=', SVC_acc)\nprint('Gaussian_acc=', Gaussian_acc)\nprint('DecisionTree_acc=', DT_acc)\nprint('GradBoost_acc=', GB_acc)\nprint('XGBoost_acc=', xgb_acc)\n","2c89bb2f":"from sklearn.ensemble import RandomForestClassifier\nrf_model = RandomForestClassifier(criterion= 'gini', n_estimators = 100 ,max_depth = 6, random_state = 0)\nrf_model.fit(X, y)\n\n\nfinal_pred = rf_model.predict(test_set)\n\nfinal_pred","cb8c35fd":"survivors = pd.DataFrame(final_pred, columns = ['Survived'])\nlen(survivors)\nsurvivors.insert(0, 'PassengerId', test['PassengerId'], True)\nsurvivors","efa45ec7":"survivors.to_csv('Submission.csv', index = False)","00106040":"# To fill the missing values in different columns starting with age","a3690308":"Applying LabelEncoder to Sex Column","6f72ee5a":"**# survived based on Passenger Class**","5ec19817":"# Final Submission","f577baf1":"# ** Data Analysis**\n> Survival on this ship can be based on age, class and or the place they entered the ship \n\nSurvival based on Sex\n","7f6c1afa":"# Now implementing various classification models","c7e61fc2":"\n\n* Now test the gradient boost on Test dataset","a9221b0a":"Read all the datasets required","0f7d5b36":"Start the data analysis part where one has to understand what we are dealing with ","9541fc0b":"* although the class 1 and 2 has almost equal survival and death counts(class 1 has more survivals better than 2 as class 1 are \"rich\" people )\n* on the other hand class 3 has most death count\n\n![A_NIGHT_TO_REMEMBER_DISC01-07.jpg](attachment:A_NIGHT_TO_REMEMBER_DISC01-07.jpg)\n","3748ae8c":"**Please upvote if you like this notebook**","e302a9b4":"Check again for missing data after filling the age column","33a5dfbc":"*  The above graph of survival based on age is so obvious that the young and middle ages people from 16 to 40 has the highest death count as shown in blue bars on above graph \n* the people above 70 might have voluntarily chose to stay back on ship \n* The infants and kids are also saved as shown in above graph \n","fbb34a06":"We have to deal with missing values in Age, Cabin and Embarked\nThe Cabin data is not that helpful because when ship sinks everyone has to get out of their bunkers. phewwwwww \nThe Age can be replaced by simple mean of ages of all people","c0a002e2":"# Splitting the training set and test set as X_train and X_test","3841ec04":"1. We have seen Float, int and object(string or character) data types our raw dataset. \n2. The dataframe has columns with integers and Float values at PassengerID, Survived, Age, Sibsp, Parch, Fare, Pclass\nrest with object datatypes","b2044894":"* The columns Name and Cabin has unique values so dropping it might be a good idea. \n* Same with Test data","4bbd2158":"There are 3 passenger classes named as 1st, 2nd and 3rd class","a5d00035":"Also we had 2 missing values in Training set Embarked column\n","d7a8c5ac":"* Now we deal with categrical data in Train and Test set using LabelEncoder and OneHOtEncoder\n* Columns with Sex and Embarkment","b6f80e93":"1. Selecting the dependent and Independent variables \nSurvival being the dependent and rest of the data as independent variables","3d0f1fef":"* 65% of people on this sunked ship are males and rest are females\n* Also by looking at the graph of survival against the sex, there are more females survived than the male and the death count is also high for the males.\n* This shows the women are given the first seat at the rescue boats .."}}