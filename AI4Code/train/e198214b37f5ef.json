{"cell_type":{"32ea0e26":"code","ca3fc9b5":"code","21ca958d":"code","676a914c":"code","3a72fc06":"code","3032a4c1":"code","dc62770b":"code","56927a44":"code","00f07978":"code","022820f1":"code","4b81d877":"code","59586c48":"code","13797765":"code","428325bf":"code","6fc9330b":"code","7a26e985":"code","02af60f8":"code","32a2bb36":"code","0e2a0c43":"code","1f18c771":"code","80825b76":"code","92e2095d":"code","1ef91458":"code","9f21f7bf":"code","212598e6":"code","498d9b06":"code","883491ff":"code","853f7b9d":"code","a1f0a5fc":"code","8c776798":"code","cb89db23":"code","948110bb":"code","66257e79":"code","c72be28f":"code","695b694c":"code","1bfbb895":"code","4483dddb":"code","bc59999d":"code","b8158d19":"code","d67c23a4":"code","65198f9a":"markdown","672a0c50":"markdown","f7e74791":"markdown","81698f96":"markdown","34a31481":"markdown","495bb6dc":"markdown","b1c8af0c":"markdown","06c6b3e3":"markdown","2e168eb8":"markdown","defb78a9":"markdown","c6c9dadb":"markdown","85e39e3a":"markdown","f6f9b2bd":"markdown","b8cb4418":"markdown","9ec87ee4":"markdown","a35c4dac":"markdown"},"source":{"32ea0e26":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ca3fc9b5":"train_data = pd.read_csv('\/kaggle\/input\/games-ratings\/train.csv')\ntrain_data","21ca958d":"eval_data = pd.read_csv('\/kaggle\/input\/games-ratings\/eval.csv')\neval_data","676a914c":"train_data.isnull().sum()","3a72fc06":"eval_data.isnull().sum()","3032a4c1":"train_data.info()","dc62770b":"eval_data.info()","56927a44":"train_data.describe()","00f07978":"eval_data.describe()","022820f1":"train_data = train_data.drop(['id', 'title'], axis=1)\ntrain_data","4b81d877":"eval_data = eval_data.drop(['id'], axis=1)\neval_data","59586c48":"train_data.columns","13797765":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\nsns.countplot(data=train_data, x='console', hue='esrb_rating')","428325bf":"g = sns.PairGrid(data=train_data, x_vars=[\"esrb_rating\"], y_vars=['console', 'alcohol_reference', 'animated_blood', 'blood',\n       'blood_and_gore', 'cartoon_violence', 'crude_humor', 'drug_reference',\n       'fantasy_violence', 'intense_violence', 'language', 'lyrics',\n       'mature_humor', 'mild_blood', 'mild_cartoon_violence',\n       'mild_fantasy_violence', 'mild_language', 'mild_lyrics',\n       'mild_suggestive_themes', 'mild_violence', 'no_descriptors', 'nudity',\n       'partial_nudity', 'sexual_content', 'sexual_themes',\n       'simulated_gambling', 'strong_janguage', 'strong_sexual_content',\n       'suggestive_themes', 'use_of_alcohol', 'use_of_drugs_and_alcohol',\n       'violence'], aspect=3)\ng = g.map(sns.pointplot)","6fc9330b":"from sklearn.manifold import TSNE\nmodel = TSNE(learning_rate=200)\ntsne_features = model.fit_transform(train_data.drop('esrb_rating', axis=1))","7a26e985":"xs = tsne_features[:,0]\n\n# Select the 1th feature: ys\nys = tsne_features[:,1]\n\n# Scatter plot\nplt.scatter(xs, ys, alpha=0.5)","02af60f8":"train_data","32a2bb36":"train_data = train_data.drop(['console'],axis=1)\neval_data = eval_data.drop(['console'],axis=1)","0e2a0c43":"from sklearn.model_selection import train_test_split\n\nX = train_data.drop(['esrb_rating'], axis=1)\ny = np.ravel(train_data[['esrb_rating']])","1f18c771":"X_train, X_test, y_train, y_test = train_test_split(X, y)","80825b76":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import accuracy_score\nlr = LogisticRegression()\nlr.fit(X_train, y_train)\nlr.score(X_test, y_test)","92e2095d":"#weights = np.linspace(0.0,0.99,500)   ,\"class_weight\":[{0:x ,1:1.0 -x} for x in weights]\n\nparam = {'C':[0.1, 0.5, 1,10,15,20], 'penalty':['l1', 'l2'], 'solver':['lbfgs', 'liblinear', 'sag', 'saga']}\ngrid_lr = GridSearchCV(estimator=lr, param_grid=param, scoring='accuracy', cv=5, refit=True)\ngrid_lr.fit(X, y)","1ef91458":"grid_lr.best_score_","9f21f7bf":"lr_model = grid_lr.best_estimator_\nscores = []\n\nfor i in range(100):\n    X_train, X_test, y_train, y_test = train_test_split(X, y)\n    lr_model.fit(X_train, np.ravel(y_train))\n    scores.append(accuracy_score(y_test, lr_model.predict(X_test)))\n    \nscores_df = pd.DataFrame(scores)\nscores_df.describe()","212598e6":"train_data.columns","498d9b06":"X = train_data.drop(['esrb_rating'], axis=1)\n\nfrom sklearn.svm import SVC\nsvc = SVC()\nparam = {'kernel':['rbf', 'sigmoid', 'linear', 'poly'], 'C':[0.001, 0.01, 0.1, 1, 10], 'gamma':[0.001, 0.01, 0.1, 1, 'auto', 'scale']}\ngrid_svc = GridSearchCV(estimator=svc, param_grid=param, scoring='accuracy', cv=5, refit=True)\ngrid_svc.fit(X, y)\ngrid_svc.best_score_\n\nsvc_model = grid_svc.best_estimator_\nscores = []\n\nfor i in range(100):\n    X_train, X_test, y_train, y_test = train_test_split(X, y)\n    svc_model.fit(X_train, np.ravel(y_train))\n    scores.append(accuracy_score(y_test, svc_model.predict(X_test)))\n    \nscores_df = pd.DataFrame(scores)\nscores_df.describe()","883491ff":"from sklearn.tree import DecisionTreeClassifier\ndt = DecisionTreeClassifier()\nparam = {'criterion':['gini', 'entropy'], 'max_depth':[2,4,6,8,10,12], 'min_samples_leaf':[1, 2, 4, 6, 8, 10]}\ngrid_dt = GridSearchCV(estimator=dt, param_grid=param, scoring='accuracy', cv=5, refit=True)\ngrid_dt.fit(X, y)\ngrid_dt.best_score_","853f7b9d":"dt_model = grid_dt.best_estimator_\nscores = []\n\nfor i in range(100):\n    X_train, X_test, y_train, y_test = train_test_split(X, y)\n    dt_model.fit(X_train, np.ravel(y_train))\n    scores.append(accuracy_score(y_test, dt_model.predict(X_test)))\n    \nscores_df = pd.DataFrame(scores)\nscores_df.describe()","a1f0a5fc":"from sklearn.ensemble import RandomForestClassifier\nrfc = RandomForestClassifier()\nparam = {'n_estimators':[8, 16, 32, 100, 200], 'max_features':[2, 4, 6, 8, 10, 'auto', 'sqrt', 'log2']}\ngrid_rfc = GridSearchCV(estimator=rfc, param_grid=param, scoring='accuracy', cv=5, refit=True)\ngrid_rfc.fit(X, y)","8c776798":"grid_rfc.best_score_","cb89db23":"rfc_model = grid_rfc.best_estimator_\nscores = []\n\nfor i in range(100):\n    X_train, X_test, y_train, y_test = train_test_split(X, y)\n    rfc_model.fit(X_train, np.ravel(y_train))\n    scores.append(accuracy_score(y_test, rfc_model.predict(X_test)))\n    \nscores_df = pd.DataFrame(scores)\nscores_df.describe()","948110bb":"from sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier()\nparam = {'n_neighbors':[1, 2, 3, 5, 7, 9, 15, 30], 'leaf_size':[3, 5, 7, 9, 12, 15, 20, 30, 50]}\ngrid_knn = GridSearchCV(estimator=knn, param_grid=param, scoring='accuracy', cv=5, refit=True)\ngrid_knn.fit(X, y)","66257e79":"grid_knn.best_score_","c72be28f":"knn_model = grid_knn.best_estimator_\nscores = []\n\nfor i in range(100):\n    X_train, X_test, y_train, y_test = train_test_split(X, y)\n    knn_model.fit(X_train, np.ravel(y_train))\n    scores.append(accuracy_score(y_test, knn_model.predict(X_test)))\n    \nscores_df = pd.DataFrame(scores)\nscores_df.describe()","695b694c":"from xgboost import XGBClassifier\nxgbclassifier = XGBClassifier(eval_metric='mlogloss')\nparam = {'max_depth':[ 3, 5, 7, 9, 11], 'eta':[0.001, 0.01, 0.1, 0.2, 0.3], 'gamma': [0, 1, 2, 3, 4]}\ngrid_xbg = GridSearchCV(estimator=xgbclassifier, param_grid=param, scoring='accuracy', cv=5, refit=True)\ngrid_xbg.fit(X, y)","1bfbb895":"xgb_model = grid_xbg.best_estimator_\nscores = []\n\nfor i in range(100):\n    X_train, X_test, y_train, y_test = train_test_split(X, y)\n    xgb_model.fit(X_train, np.ravel(y_train))\n    scores.append(accuracy_score(y_test, xgb_model.predict(X_test)))\n    \nscores_df = pd.DataFrame(scores)\nscores_df.describe()","4483dddb":"eval_data = pd.read_csv('\/kaggle\/input\/games-ratings\/eval.csv')\neval_data","bc59999d":"# lr_model\n# svc_model\n# dt_model\n# rfc_model\n# knn_model\n# xbg_model\neval_data['esrb_rating'] = svc_model.predict(eval_data.drop(['id', 'console'], axis=1))\nsubmission = eval_data[['id', 'esrb_rating']]","b8158d19":"submission","d67c23a4":"submission.to_csv('submission_esrb_ratings_svc.csv', index=False)","65198f9a":"# K Nearest Neighbors model","672a0c50":"Drop 'id' and 'title' features as they should not have any relation witht the 'esrb_rating' of the game.","f7e74791":"# Check for Missing Values","81698f96":"# Load Data","34a31481":"The data is categorical data so there are no outliers in it.","495bb6dc":"# Check for Outliers","b1c8af0c":"Drop the console column since it should have little to no relation to the content\/rating of the game. The title and id columns were also dropped for the same reason.","06c6b3e3":"# Data Transformations and Feature Engineering","2e168eb8":"# XGBoost model","defb78a9":"# Exploratory Data Analysis","c6c9dadb":"# Decision Tree model","85e39e3a":"# Logistic Regression Model","f6f9b2bd":"# Random Forest model","b8cb4418":"# Best Model and Submission","9ec87ee4":"# Support Vector Machine","a35c4dac":"There does not appear to be any missing values in the dataset."}}