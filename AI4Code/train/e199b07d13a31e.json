{"cell_type":{"d4ab9631":"code","af4cd4ed":"code","655f0c30":"code","c89ad7fd":"code","87ca0112":"code","8f0cb836":"code","f7ec09bc":"code","3f82b53c":"code","494d337e":"code","458f0c4d":"code","1fabb68a":"code","1086fe54":"code","9b8af796":"code","cfb718c3":"code","3cbd1ae2":"code","99561008":"code","a0cc934f":"code","c63aee9c":"code","e529a2a8":"code","dbdd0ff2":"code","74c2343e":"code","5fd1a2aa":"code","4ae62355":"code","04c9920d":"markdown","f7fa848f":"markdown","1d4bb1e1":"markdown","c4ce3456":"markdown","7e3e4c8d":"markdown","2d86078b":"markdown","de85d5b8":"markdown","1ef1e0f0":"markdown","2a3233e4":"markdown","aed8f81b":"markdown","b2f4e458":"markdown","66e710d6":"markdown","6b491c5f":"markdown","25cb9e64":"markdown","f0d1a2b5":"markdown","da9eaeba":"markdown","d912097e":"markdown","648b62ee":"markdown","c45b9191":"markdown","b3c75e76":"markdown","b04dc2ca":"markdown","97f579b7":"markdown"},"source":{"d4ab9631":"import warnings\nimport numpy as np\nimport pandas as pd\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nfrom sklearn.feature_selection import chi2, VarianceThreshold\nimport sklearn.linear_model\n\n#supressing warnings for readability\nwarnings.filterwarnings(\"ignore\")\n\n# To plot pretty figures directly within Jupyter\n%matplotlib inline\n\n# choose your own style: https:\/\/matplotlib.org\/3.1.0\/gallery\/style_sheets\/style_sheets_reference.html\nplt.style.use('seaborn-whitegrid')\n\n# Go to town with https:\/\/matplotlib.org\/tutorials\/introductory\/customizing.html\n# plt.rcParams.keys()\nmpl.rc('axes', labelsize=14, titlesize=14)\nmpl.rc('figure', titlesize=20)\nmpl.rc('xtick', labelsize=12)\nmpl.rc('ytick', labelsize=12)\n\n# contants for figsize\nS = (8,8)\nM = (12,12)\nL = (14,14)\n\n# pandas options\npd.set_option(\"display.max.columns\", None)\npd.set_option(\"display.max.rows\", None)\npd.set_option(\"display.precision\", 2)\n\n# import data\ndf = pd.read_parquet('..\/input\/nhs-proms-case-study\/data\/interim\/knee-ccg.parquet')","af4cd4ed":"from sklearn.model_selection import StratifiedShuffleSplit\n\n\n# 999 is used as a sentinel value, replacing those with median\ndf[\"t1_eq_vas_impute\"] = df.t1_eq_vas.replace(\n    to_replace=999, value=np.median(df.t1_eq_vas)\n)\n\n# add t1_eq_vas categories\ndf['t1_eq_vas_cat'] = pd.cut(df.t1_eq_vas_impute, 10)\n\n# Only using 1 split for stratefied sampling, more folds are used later on in cross-validation\nsplit = StratifiedShuffleSplit(n_splits=1, test_size=0.3, random_state=42)\nfor train_index, test_index in split.split(df, df['t1_eq_vas_cat']):\n    df_train = df.loc[train_index]\n    df_test = df.loc[test_index]\n\n# remove columns so we continue working with original dataset\nfor set_ in (df_train, df_test):\n    set_.drop([\"t1_eq_vas_impute\", \"t1_eq_vas_cat\"], axis=1, inplace=True)","655f0c30":"from sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.pipeline import Pipeline, FeatureUnion\nfrom sklearn.preprocessing import OrdinalEncoder, OneHotEncoder, LabelEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.impute import SimpleImputer\n\n\n# group columns\nage_band = [\"age_band\"]\ngender = [\"gender\"]\nage_band_categories = sorted([x for x in df.age_band.unique() if isinstance(x, str)])\ncomorb = [\n    \"heart_disease\",\n    \"high_bp\",\n    \"stroke\",\n    \"circulation\",\n    \"lung_disease\",\n    \"diabetes\",\n    \"kidney_disease\",\n    \"nervous_system\",\n    \"liver_disease\",\n    \"cancer\",\n    \"depression\",\n    \"arthritis\",\n]\nboolean = [\"t0_assisted\", \"t0_previous_surgery\", \"t0_disability\"]\neq5d = [\"t0_mobility\", \"t0_self_care\", \"t0_activity\", \"t0_discomfort\", \"t0_anxiety\"]\neq_vas = [\"t0_eq_vas\"]\ncategorical = [\"t0_symptom_period\", \"t0_previous_surgery\", \"t0_living_arrangements\"]\noks_questions = [\n    col for col in df.columns if col.startswith(\"oks_t0\") and not col.endswith(\"_score\")\n]\noks_score = [\"oks_t0_score\"]\n\n# preprocessing pipelines for specific columns\nage_band_pipe = Pipeline(\n    steps=[\n        (\"impute\", SimpleImputer(missing_values=np.nan, strategy=\"most_frequent\")),\n        (\"ordinal\", OrdinalEncoder(categories=[age_band_categories])),\n    ]\n)\n\ngender_pipe = Pipeline(\n    steps=[\n        (\"impute\", SimpleImputer(missing_values=np.nan, strategy=\"most_frequent\")),\n                   ('onehot', OneHotEncoder()),\n    ]\n)\n\n\n# ColumnTransformer on all included columns.\n# Note columns that are not specified are dropped by default\ntransformers = {\n    \"age\": (\"age\", age_band_pipe, age_band),\n    \"gender\": (\"gender\", gender_pipe, gender),\n    \"comorb\": (\n        \"comorb\",\n        SimpleImputer(missing_values=9, strategy=\"constant\", fill_value=0),\n        comorb,\n    ),\n    \"categorical\": (\n        \"categorical\",\n        SimpleImputer(missing_values=9, strategy=\"most_frequent\"),\n        boolean + eq5d + categorical,\n    ),\n    \"oks\": (\n        \"oks\",\n        SimpleImputer(missing_values=9, strategy=\"most_frequent\"),\n        oks_questions,\n    ),\n    \"oks_score\": (\n        \"oks_score\",\n        SimpleImputer(missing_values=np.nan, strategy=\"most_frequent\"),\n        oks_score,\n    ),\n    \"eq_vas\": (\"eqvas\", SimpleImputer(missing_values=999, strategy=\"median\"), eq_vas),\n}\nprep = ColumnTransformer(transformers=[v for _, v in transformers.items()])\n\nX_train = prep.fit_transform(df_train)\nX_test = prep.fit_transform(df_test)","c89ad7fd":"# list of columns for convenience\n# https:\/\/stackoverflow.com\/questions\/54646709\/sklearn-pipeline-get-feature-name-after-onehotencode-in-columntransformer\nX_columns = pd.Series(\n    age_band\n    + prep.named_transformers_[\"gender\"][\"onehot\"].get_feature_names().tolist()\n    + comorb\n    + boolean\n    + eq5d\n    + categorical\n    + oks_questions\n    + oks_score\n    + eq_vas\n)","87ca0112":"# just as an example, not used in Pipeline\nclass ReplaceSentinels(BaseEstimator, TransformerMixin):\n    \"\"\"Replace sentinel values in dataframe.\n    \n    Attributes:\n        sentinel: sentinel value, default 9\n        replace_with: value to replace sentinel with, default np.nan\n    \"\"\"\n    def __init__(self, sentinel = 9, replace_with=np.nan):\n        self.sentinel = sentinel\n        self.replace_with = replace_with\n    def fit(self, X, y=None):\n        return self\n    def transform(self, X, ):\n        return X.replace(9, self.replace_with)\n","8f0cb836":"df_train.plot(kind='scatter', x='t0_eq_vas', y='t1_eq_vas', xlim=(0,100), ylim=(0,100), alpha=0.1, figsize=M);","f7ec09bc":"from sklearn.linear_model import LinearRegression\n\n\ndef fill_median(s):\n    return s.fillna(value=s.median()).to_frame() \n\neq = ['t0_eq_vas', 't1_eq_vas']\neq_prep = ColumnTransformer(transformers=\n                            [('eq',\n                              SimpleImputer(missing_values=999,\n                                            strategy='median'),\n                              eq),\n                            ])\neq_prep.fit(df_train)\n\n# note y = t1_eq\nt0_eq, t1_eq = eq_prep.transform(df_train)[:,0].reshape(-1,1), eq_prep.transform(df_train)[:,1]\n\n# prepare t1_eq_test for use in model assessment\nt1_eq_test = eq_prep.transform(df_test)[:,1]\n\n# simple linear regression\nlin_reg = LinearRegression()\nlin_reg.fit(t0_eq, t1_eq)\nlin_reg.intercept_, lin_reg.coef_, lin_reg.score(t0_eq, t1_eq)","3f82b53c":"from sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import cross_val_score\n\n\ndef display_scores(scores):\n    print(f\"Scores: {scores}\")\n    print(f\"Mean: {scores.mean():.4f}\")\n    print(f\"Standard deviation: {scores.std():.4f}\")\n\nscores = cross_val_score(lin_reg, t0_eq, t1_eq, scoring='neg_mean_squared_error', cv=5)\nlin_rmse_scores = np.sqrt(-scores)\ndisplay_scores(lin_rmse_scores)","494d337e":"from sklearn.feature_selection import SelectKBest, f_regression\n\nk10best = Pipeline(\n    steps=[\n        (\"prep\", ColumnTransformer(transformers=transformers.values())),\n        (\"kbest\", SelectKBest(f_regression, k=10)),\n#         (\"lin_reg\", LinearRegression())\n    ]\n)\n\nX_10best = k10best.fit(df_train, t1_eq).transform(df_train)\nlin_10best = LinearRegression()\nscores_10best = cross_val_score(lin_10best, X_10best, t1_eq, scoring='neg_mean_squared_error', cv=5)\nlin_10best_rmse_scores = np.sqrt(-scores_10best)\ndisplay_scores(lin_10best_rmse_scores)","458f0c4d":"# show features in descending order of importance\npd.concat(\n        {\"score\": pd.Series(k10best[\"kbest\"].scores_), \"feature\": X_columns}, axis=1\n    ).sort_values(\"score\", ascending=False)\n","1fabb68a":"from sklearn.linear_model import LassoCV\n\n# This is to avoid division by zero while doing np.log10\nEPSILON = 1e-4\n\nlasso = LassoCV(cv=5, random_state=42, n_jobs=-1).fit(X_train, t1_eq)","1086fe54":"plt.figure(figsize=S)\nplt.semilogx(lasso.alphas_ + EPSILON, np.sqrt(lasso.mse_path_), \":\")\nplt.plot(\n    lasso.alphas_ + EPSILON,\n    np.sqrt(lasso.mse_path_.mean(axis=-1)),\n    \"k\",\n    label=\"Average across the folds\",\n    linewidth=2,\n)\nplt.axvline(\n    lasso.alpha_ + EPSILON, linestyle=\"--\", color=\"k\", label=\"alpha: CV estimate\"\n)\nplt.legend()\nplt.xlabel(r\"$\\alpha$\")\nplt.ylabel(\"Root mean square error\")\nplt.title(\"Root mean square error on each fold: coordinate descent\")\nplt.axis(\"tight\");","9b8af796":"print(\"Best Lasso model:\\n\"\n      f\"  RMSE: {np.sqrt(mean_squared_error(lasso.predict(X_train), t1_eq)):.2f}\\n\"\n      f\"  intercept: {lasso.intercept_:.2f}\")","cfb718c3":"pd.concat({'coef_abs': pd.Series(abs(lasso.coef_)), 'coef': pd.Series(lasso.coef_), 'feature': X_columns}, axis=1).sort_values('coef_abs', ascending=False)","3cbd1ae2":"# final test\nprint(f\"MSE from training with CV: {np.sqrt(mean_squared_error(lasso.predict(X_train), t1_eq)):.2f}\\n\"\n      f\"MSE test: {np.sqrt(mean_squared_error(lasso.predict(X_test), t1_eq_test)):.2f}\")","99561008":"from sklearn.neighbors import KNeighborsRegressor\nfrom collections import defaultdict\n\nn_neighbors = [1, 2, 3, 5, 10, 20, 30, 50, 100, 200, 300]\nknn = defaultdict(dict) # use defaultdict for nested dicts\nfor n in n_neighbors:\n    knn[n][\"model\"] = KNeighborsRegressor(n_neighbors=n, n_jobs=-1).fit(X_train, t1_eq)\n    knn[n][\"cross validation scores\"] = cross_val_score(\n        knn[n][\"model\"], X_train, t1_eq, scoring=\"neg_mean_squared_error\", cv=5\n    )","a0cc934f":"# plot 1\/K vs. error rate just like in ISLR figure 3.19 (p. 108)\nknn_mse = [np.sqrt(np.mean(-knn[n]['cross validation scores'])) for n in n_neighbors]\nplt.figure(figsize=S)\nplt.semilogx([1\/n for n in n_neighbors], knn_mse, 'o-', markeredgecolor='w', markeredgewidth='2');","c63aee9c":"# minimum RMSE is 16.2 at 1\/K = 10-2, i.e. n=100\nknn_mse","e529a2a8":"# final test\nprint(f\"{np.sqrt(mean_squared_error(knn[100]['model'].predict(X_test), t1_eq_test)):.4f}\")    ","dbdd0ff2":"from sklearn.ensemble import RandomForestRegressor\n\nrf_reg = RandomForestRegressor(random_state=42)\nrf_reg.fit(X_train, t1_eq)\nrf_crosscv = cross_val_score(rf_reg, X_train, t1_eq, scoring=\"neg_mean_squared_error\", cv=5)","74c2343e":"print(f\"{np.sqrt(np.mean(-rf_crosscv)):.4f}\")","5fd1a2aa":"# final test\nprint(f\"{np.sqrt(mean_squared_error(rf_reg.predict(X_test), t1_eq_test)):.4f}\")    ","4ae62355":"pd.concat(\n    {\"feature\": X_columns, \"importance\": pd.Series(rf_reg.feature_importances_)}, axis=1\n).sort_values(\"importance\", ascending=False)","04c9920d":"### Summary\nWe have considered three basic machine learning algorithms:\n- (Simple) linear regression\n- LASSO\n- KNN\n\nA regression is often used as a baseline model, whereas LASSO and KNN are expected to have better performance. An obvious starting strategy, would be to: \n- Run a regression to get a baseline performance\n- Run LASSO and compare performance\n- With the selected variables in your LASSO, run KNN and compare performance\n\n","f7fa848f":"# Modeling: regression and linear modeling\n\n","1d4bb1e1":"### Python: Hands-on Machine Learning (2nd edition)\n\n- [End-to-end Machine Learning project (chapter 2)](https:\/\/github.com\/ageron\/handson-ml2\/blob\/master\/02_end_to_end_machine_learning_project.ipynb)\n- [Training linear models (chapter 4)](https:\/\/github.com\/ageron\/handson-ml2\/blob\/master\/04_training_linear_models.ipynb)\n\n\n### scikit-learn\n- [Tutorial cross validation score](https:\/\/scikit-learn.org\/stable\/auto_examples\/exercises\/plot_cv_diabetes.html?highlight=cross%20validation%20score)","c4ce3456":"### SelectKBest\n\nFor regression tasks, you often want to get a first idea which features contain the most information i.e. are the best predictors. There are various techniques to answer this question, such as stepwise selection. Scikit-learn has various [univariate feature selection](https:\/\/scikit-learn.org\/stable\/modules\/feature_selection.html) methods for this purpose. We will use [SelectKBest](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.feature_selection.SelectKBest.html#sklearn.feature_selection.SelectKBest).\n","7e3e4c8d":"The Lasso model performs best for lowest $\\alpha$","2d86078b":"### Writing custom transformers (advanced, see G\u00e9ron chapter 2)\nAlthough Scikit-Learn provides many useful transformers, you will need to write your own for tasks such as custom cleanup operations or combining specific attributes. You will want your transformer to work seamlessly with Scikit-Learn functionalities (such as pipelines), and since Scikit-Learn relies on duck typing (not inheritance), all you need to do is create a class and implement three methods: fit() (returning self), transform(), and fit_transform().\n\nWhen writing transformers for data preparation, you only need to define `transform()`. Basically, `ColumnTransformer` passes only the subset of columns from the original dataframe to the transformer. So when writing your own transformer you don't need to do any subsetting, but you can assume that the `transform()` method should be applied to the whole dataframe.\n\n","de85d5b8":"### Simple regression\nRegression of `t1_eq_vas` ~ `t0_eq_vas`. We don't get our hopes up, since the scatterplot is all over the place:","1ef1e0f0":"Using 10 KBest features, the model performs slightly better with RMSE of 16.1 +\/- 0.08","2a3233e4":"## Training and assessing linear models","aed8f81b":"## Learning objectives\n### Modeling: regression and linear modeling\nFor this lecture we are going to build various predictors for `t1_eq_vas`, i.e. the reported quality of life after operation. `t1_eq_vas` is measured on a scale from 0 to 100.\n\nWe are going to use stratefied sampling to ensure we don't introduce sampling bias, using `StratefiedShuffleSplit`. This is different from the simplest function `train_test_split` which is a random sampling method. This is generally fine if your dataset is large enough (especially relative to the number of attributes). But if it is not, you run the risk of introducing significant sampling bias.\n\nBy the end of this lecture you should know:\n- Know how to perform different regressions models in Python using scikit-learn\n- Know how to interpret and assess regression models, including the bias-variance trade-of\n\n","b2f4e458":"### RandomForest regression\n_consider this as a sneak preview for the next lecture, where we will use RandomForest for classification_","66e710d6":"### KNN","6b491c5f":"That looks promising: out-of-the-box RandomForest performs better than KNN (but slightly worse than Lasso). Like Lasso, we can inspect feature importance:","25cb9e64":"# Background to osteoarthritis case study\n\nThis is day 3 from the [5-day JADS NHS PROMs data science case study](https:\/\/github.com\/jads-nl\/execute-nhs-proms\/blob\/master\/README.md). To recap from the previous lectures, we have looked at defining the outcome of knee replacement.\n\n- Lecture 1:\n  - Good outcome for knee replacement Y is measured using difference in Oxford Knee Score (OKS)\n  - Research has shown that an improvement in OKS score of approx. 30% is relevant ([van der Wees 2017](https:\/\/github.com\/jads-nl\/execute-nhs-proms\/blob\/master\/references\/vanderwees2017patient-reported.pdf)). Hence an increase of +14 points is considered a 'good' outcome.\n  - To account for the ceiling effect, a high final `t1_oks_score` is also considered as a good outcome (even if `delta_oks_score` is smaller than 14)\n  \n- Lecture 2:\n  - We have constructed a combined outcome parameter using cut-off points for pain and physical functioning.\n","f0d1a2b5":"# Data preparation in a scikit-learn Pipeline\nPreviously we have already discussed the various steps in data preparation using [pandas](https:\/\/pandas.pydata.org\/). As explained in the [documentation of scikit-learn](https:\/\/scikit-learn.org\/stable\/modules\/compose.html#column-transformer), this may be problematic for one of the following reasons:\n\n* Incorporating statistics from test data into the preprocessors makes cross-validation scores unreliable (known as data leakage), for example in the case of scalers or imputing missing values.\n\n* You may want to include the parameters of the preprocessors in a [parameter search](https:\/\/scikit-learn.org\/stable\/modules\/grid_search.html#grid-search).\n\nTo this purpose, the [`ColumnTransformer` class](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.compose.ColumnTransformer.html?highlight=columntransformer#sklearn.compose.ColumnTransformer) has been recently added to scikit-learn. The documentation gives an example how to use this for [pre-processing mixed types](https:\/\/scikit-learn.org\/stable\/auto_examples\/compose\/plot_column_transformer_mixed_types.html#sphx-glr-auto-examples-compose-plot-column-transformer-mixed-types-py). Historically, `sklearn` transformers are designed to work with numpy arrays, not with pandas dataframes. You can use [`sklearn-pandas`](https:\/\/github.com\/scikit-learn-contrib\/sklearn-pandas) to bridge this gap or use `ColumnTransformer` directly on pandas DataFrames. We will use the latter.\n\n","da9eaeba":"### Regularized linear models: Lasso regression\n\nConstruct a Lasso regression with cross-validation, following the example from [scikit-learn documentation](https:\/\/scikit-learn.org\/stable\/auto_examples\/linear_model\/plot_lasso_model_selection.html#sphx-glr-auto-examples-linear-model-plot-lasso-model-selection-py). Recall that for regularized linear regression a cost function is added. In case of Lasso this is\n\n$$ J(\\Theta) = MSE (\\Theta) + \\alpha\\sum\\limits_{i=1}^n \\mid{\\Theta_{i}}\\mid$$\n\nThe larger $\\alpha$, the larger the penalty and hence more coefficients will be set to zero. By default `LassoCV` tries 100 different values for $\\alpha$ so let's plot MSE against $\\alpha$:","d912097e":"## Using ColumnsTransformers and Pipelines\n\nRecalling from the second lecture, we want to perform the following preprocessing per (group of) columns. In case feature requires more than one preprocessing step, the use of `Pipeline` is recommended.\n\n### Passing 1D or 2D arrays in your `Pipeline`\nIt is important to remember that `scikit-learn` can be quite fussy about the difference between passing 1D arrays\/series and 2D arrays\/dataframes.\n\nFor example, the following code will result in an error because `categories` needs to be a list of lists:\n```\nenc = OrdinalEncoder(categories=age_band_categories)\nenc.fit(df[age_band])\n```\n\nThe correct code is (brackets!):\n```\nenc = OrdinalEncoder(categories=[age_band_categories])\nenc.fit(df[age_band])\n```\n\n\n### Beware: difference between `OrdinalEncoder` and `OneHotEncoding`\nUsing `OrdinalEncoder` to generate an integer representation of a categorical variable can not be used directly with all scikit-learn estimators, as these expect continuous input, and would interpret the categories as being ordered, which is often not desired.\n\nAnother possibility to convert categorical features to features that can be used with scikit-learn estimators is to use a one-of-K, also known as one-hot or dummy encoding. This type of encoding can be obtained with the OneHotEncoder, which transforms each categorical feature with n_categories possible values into n_categories binary features, with one of them 1, and all others 0.","648b62ee":"## Discussion & summary","c45b9191":"### For discussion\n- Which model would you choose to predict `t1_eq_vas_` and why?\n- Reflect on the differences in feature importance between SelectKBest, Lasso coefficients and RandomForest\n  - See, for example, [this discussion](https:\/\/datascience.stackexchange.com\/questions\/12148\/feature-importance-via-random-forest-and-linear-regression-are-different)\n\n","b3c75e76":"Inspect the relative feature importance by looking at the (absolute) coefficients:","b04dc2ca":"This confirms a simple linear model has little flexibility (and high bias): the scores for the five CV-folds are very similar. Now that we have seen the simplest setup for a univariate lineair regression, let's try to find out which features are the best predictors.","97f579b7":"So this very first, basic model yields an $R^2$ of 0.12 which is not very exciting. Let's do a more robust cross validation using the Mean Square Error (MSE) as our metric"}}