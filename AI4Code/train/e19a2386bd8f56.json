{"cell_type":{"82483fb9":"code","581838e2":"code","2f09aa2c":"code","aef75552":"code","4b4c6188":"code","807857c1":"code","bc9545a1":"code","cef226a5":"code","1511cc01":"code","6291ff83":"code","4d6ac309":"code","d6ae3508":"code","caeb5220":"code","090daf19":"code","54b71143":"code","80a496e0":"code","acb749ff":"code","631e6c0a":"code","b8ef2c16":"code","4802c92a":"code","d1c628af":"code","6756a1cb":"code","9d8a27cb":"code","48c97a6d":"code","f28206cb":"code","66badc4d":"code","f3f79e8c":"code","eeb369ce":"code","010323c1":"code","9052b336":"code","f19be20e":"code","e3a0333b":"code","363e4909":"code","b51abf7d":"code","95bc0742":"code","fa86c196":"code","152e3edb":"code","40dbf3d2":"code","dae1283c":"code","81a73818":"code","ad09c9dc":"code","a5a4015a":"code","11897b30":"code","76829027":"code","13951489":"code","ce4e32a6":"code","517e3286":"code","9c02d1d7":"code","51aa2627":"code","e469b199":"code","77097f0b":"code","993747d8":"code","386b5d97":"code","80432b78":"code","f818ee8a":"code","b11b3c96":"code","8e9a6b1d":"code","2b31f086":"markdown","1f5fc88d":"markdown"},"source":{"82483fb9":"import pandas as pd","581838e2":"data=pd.read_csv('..\/input\/fake-news\/train.csv')\n","2f09aa2c":"data.head()\n\n#we will be using the title column for our prediction","aef75552":"#checking for null values in the dataset\n\ndata.isnull().sum()","4b4c6188":"data.shape","807857c1":"#we will use the title column so other columns will be of no use\n\ndata=data.drop(['text','author','id'],axis=1)","bc9545a1":"#there are some  null values in the title column also\n\ndata.isnull().sum()","cef226a5":"#as title is the only column is the what we are using if it contains NaN values we have to drop it.\n\ndata=data.dropna()","1511cc01":"data.isnull().sum()","6291ff83":"data.shape","4d6ac309":"data.head()","d6ae3508":"X=data['title']\ny=data['label']","caeb5220":"X.shape","090daf19":"#importing all necessary modules that we will be using to build our LSTM neural network\n\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Embedding\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.preprocessing.text import one_hot\nfrom tensorflow.keras.layers import LSTM\nfrom tensorflow.keras.layers import Dense","54b71143":"#we dropped some rows as there were nan values so reset index will make it uniform\n\nX=X.reset_index()","80a496e0":"X=X.drop(['index'],axis=1)","acb749ff":"X.tail()","631e6c0a":"#as we dropped some rows so to make the dataframe in order\ny=y.reset_index()","b8ef2c16":"y=y.drop(['index'],axis=1)","4802c92a":"y.tail()","d1c628af":"# importing nltk,stopwords and porterstemmer we are using stemming on the text we have and stopwords will help in removing the stopwords in the text\n\n#re is regular expressions used for identifying only words in the text and ignoring anything else\nimport nltk\nimport re\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer","6756a1cb":"\nps = PorterStemmer()\ncorpus = []\n#each row of the dataset is considered here.everything except the alphabets are removed ,stopwords are also being removed here .the text is converted in lowercase letters and stemming is performed\n#lemmatisation can also be used here at the end a corpus of sentences is created\nfor i in range(0, len(X)):\n    review = re.sub('[^a-zA-Z]', ' ',X['title'][i])\n    review = review.lower()\n    review = review.split()\n    \n    review = [ps.stem(word) for word in review if not word in stopwords.words('english')]\n    review = ' '.join(review)\n    corpus.append(review)","9d8a27cb":"corpus[30]","48c97a6d":"#vocabulary size\nvoc_size=5000","f28206cb":"#performing onr hot representation\n\nonehot_repr=[one_hot(words,voc_size)for words in corpus] ","66badc4d":"len(onehot_repr[0])","f3f79e8c":"len(onehot_repr[700])","eeb369ce":"#specifying a sentence length so that every sentence in the corpus will be of same length\n\nsent_length=25\n\n#using padding for creating equal length sentences\n\n\nembedded_docs=pad_sequences(onehot_repr,padding='pre',maxlen=sent_length)\nprint(embedded_docs)","010323c1":"#Creating model\n\nfrom tensorflow.keras.layers import Dropout\nembedding_vector_features=40\nmodel=Sequential()\nmodel.add(Embedding(voc_size,embedding_vector_features,input_length=sent_length))\nmodel.add(Dropout(0.3))\nmodel.add(LSTM(200))\nmodel.add(Dropout(0.3))\nmodel.add(Dense(1,activation='sigmoid'))\nmodel.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n","9052b336":"import numpy as np\nX_final=np.array(embedded_docs)\ny_final=np.array(y)","f19be20e":"X_final.shape,y_final.shape","e3a0333b":"#splitting the data for training and testing the model\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X_final, y_final, test_size=0.10, random_state=42)","363e4909":"model.fit(X_train,y_train,validation_data=(X_test,y_test),epochs=20,batch_size=64)","b51abf7d":"#loading test dataset for prediction\n\ntest=pd.read_csv('..\/input\/fake-news\/test.csv')","95bc0742":"test.head()","fa86c196":"#null values in the test dataset\n\ntest.isnull().sum()","152e3edb":"#using the title column only as we did in the train dataset\n\ntest=test.drop(['text','id','author'],axis=1)","40dbf3d2":"test.head()","dae1283c":"test.isnull().sum()","81a73818":"test.fillna('fake fake fake',inplace=True)\n\n#the solution file that can be submitted in kaggle expects it to have 5200 rows so we can't drop rows in the test dataset","ad09c9dc":"test.shape","a5a4015a":"#creating corpus for the test dataset exactly the same as we created for the training dataset\n\ncorpus_test = []\nfor i in range(0, len(test)):\n    review = re.sub('[^a-zA-Z]', ' ',test['title'][i])\n    review = review.lower()\n    review = review.split()\n    \n    review = [ps.stem(word) for word in review if not word in stopwords.words('english')]\n    review = ' '.join(review)\n    corpus_test.append(review)","11897b30":"#creating one hot representation for the test corpus\n\nonehot_repr_test=[one_hot(words,voc_size)for words in corpus_test] ","76829027":"#padding for the test dataset\nsent_length=25\n\nembedded_docs_test=pad_sequences(onehot_repr_test,padding='pre',maxlen=sent_length)\nprint(embedded_docs_test)","13951489":"X_test=np.array(embedded_docs_test)","ce4e32a6":"#making predictions for the test dataset\n\ncheck=model.predict_classes(X_test)","517e3286":"check","9c02d1d7":"check.shape","51aa2627":"test.shape","e469b199":"submit_sample=pd.read_csv('..\/input\/fake-news\/submit.csv')","77097f0b":"submit_sample.head()","993747d8":"type(check)","386b5d97":"check[0]","80432b78":"val=[]\nfor i in check:\n    val.append(i[0])","f818ee8a":"#inserting our predicted values in the submission file\n\nsubmit_sample['label']=val","b11b3c96":"submit_sample.head()","8e9a6b1d":"#saving the submission file\n\nsubmit_sample.to_csv('submission.csv',index=False)","2b31f086":"if this notebook was helpful please upvote.","1f5fc88d":"In this notebook we will be using a LSTM network to create a model to predict whether a news is fake or not.\n\nI have also solved the same problem using Tf-Idf you can refer to this link and have a look.\nhttps:\/\/github.com\/sid26ranjan\/fake-news-classifier\n\nMake sure you have enabled GPU in accelerator to speed up the training process and try various methods to achieve a different accuracy.\n\nI have tried to explain the things that i have used in the notebook.feedbacks and suggestions are most welcomed.\n\nPlease upvote if you find this notebook useful.\n\n"}}