{"cell_type":{"6ab959ea":"code","502a7595":"code","98c0e499":"code","cd7656da":"code","d8e51c7f":"code","1a3ccf9b":"code","42bae9c1":"code","f8294207":"code","e28b9fe6":"code","80c4aaa0":"code","ad1791d2":"code","2e1c406f":"code","6fbecd50":"code","c524f203":"code","7a1822d8":"code","1e03f3c8":"code","a9285259":"code","2384301e":"code","11b77c89":"code","8c79c7e4":"code","c992495f":"code","971a5a13":"code","dbd14fdb":"code","aadd8b55":"code","82a19f56":"code","515b0b48":"code","78165dee":"code","03c11a4f":"code","5150b6d4":"code","6930f086":"code","b28444c1":"code","4f340ebd":"code","e0e3d3d7":"code","8efad957":"code","21c6dc4d":"code","82ef35e2":"code","a99bc6f7":"code","9ff57bc0":"code","36ca8b2c":"code","5446082e":"code","d79f9c8d":"code","9c960d24":"code","ab85af5c":"code","0bb8f19c":"code","bd71aded":"code","54d6a4bc":"code","e2a62e41":"code","e227f964":"code","1d44ccd3":"code","479d755f":"code","11490038":"markdown","6f2507d1":"markdown","60f96e73":"markdown","6644a571":"markdown","ee671249":"markdown","0ca0dad4":"markdown","efcead29":"markdown","3d50fcc0":"markdown","8aa0dfdf":"markdown","dd5084df":"markdown","99a40222":"markdown","d070743a":"markdown","192a69b9":"markdown","b708ef84":"markdown","4666638c":"markdown","085cec9b":"markdown","2c858cd3":"markdown","decc9a8a":"markdown","b40cb3f4":"markdown","174e122d":"markdown","17006f7c":"markdown","dff30fde":"markdown","5d2b1ef6":"markdown","6ef584fe":"markdown","0f127f3b":"markdown","6c1e03c5":"markdown","0e10ccf1":"markdown","4b59c5de":"markdown","c98a7763":"markdown","20d8a4b2":"markdown","988acab1":"markdown"},"source":{"6ab959ea":"import pandas as pd\nimport numpy as np\nimport random\nfrom collections import Counter\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom timeit import default_timer as timer\nimport time\n\nfrom sklearn.model_selection import StratifiedKFold, learning_curve, GridSearchCV, cross_validate, train_test_split\nfrom sklearn.ensemble import VotingClassifier, RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, BaggingClassifier\nfrom sklearn.linear_model import LogisticRegressionCV\nfrom sklearn.naive_bayes import BernoulliNB, GaussianNB\nfrom sklearn.metrics import accuracy_score, roc_auc_score, precision_score, recall_score, f1_score, roc_curve\nfrom xgboost import XGBClassifier\nimport lightgbm as lgb","502a7595":"train = pd.read_csv(\"..\/input\/santander-customer-transaction-prediction\/train.csv\")\ntest = pd.read_csv(\"..\/input\/santander-customer-transaction-prediction\/test.csv\")","98c0e499":"## preview the data and check missing values\ntrain.info()\n# test.info()\n# train.isnull().sum()\nprint('\\nTraing dataset:')\nprint('isnull: ' + str(Counter(train.isnull().sum())))\nprint('target_labels: ' + str(Counter(train['target'])) + '\\n')\nprint('Testing dataset:')\nprint('isnull: ' + str(Counter(test.isnull().sum())) + '\\n')\n\n# train.sample(10)\ntrain.describe(include='all')","cd7656da":"## check the data outliers in each column\nfeatures = train.columns.values\noutlier_indices = []\n\nfor col in features[2:]:\n    # 1st quartile: 25%\n    Q1 = np.percentile(train[col],25)\n    # 3rd quartile: 75%\n    Q3 = np.percentile(train[col],75)\n    # interquartile range\n    IQR = Q3 - Q1\n    outlier_step = 1.5*IQR\n    # generate a list of indices of outliers for feature col\n    outlier_list_col = train[(train[col] < Q1-outlier_step) | (train[col] > Q3+outlier_step)].index \n    outlier_indices.extend(outlier_list_col)\n\n# a dictionary that count outlier features for each row index \noutlier_indices = Counter(outlier_indices) \nprint(\"Max number of outlier features in a row: \",max(outlier_indices.values()))\n","d8e51c7f":"train_ID = [i.split(\"_\")[1].strip() for i in train[\"ID_code\"]]\ntrain[\"ID\"] = pd.Series(train_ID)\np = sns.FacetGrid(train,col='target',aspect=2)\np.map(sns.distplot,'ID')","1a3ccf9b":"## firstly, visualize the label imbalance\nprint(train['target'].value_counts())\n\nf,axe=plt.subplots(1,2,figsize=(16,6))\ntrain['target'].value_counts().plot.pie(explode=[0,0.1],autopct='%1.1f%%',ax=axe[0],shadow=True)\n# axe[0].set_title('target')\naxe[0].set_ylabel('')\nsns.countplot('target',data=train,ax=axe[1])\n# axe[1].set_title('target')\nplt.show()","42bae9c1":"## plot all feature columns as well as their max\/mean\/min\ndescribe_df_train = train.describe(include='all').drop(labels=['ID_code', 'target','ID'], axis=1)\n# print(describe_df_train.columns.values)\nfeature_labels_train = np.array(describe_df_train.columns.str.lstrip('var_')).astype(np.int16)\nfeature_means_train = np.array(describe_df_train.loc['mean'])\nfeature_mins_train = np.array(describe_df_train.loc['min'])\nfeature_maxs_train = np.array(describe_df_train.loc['max'])\n\n## plot all feature columns as well as their max\/mean\/min\ndescribe_df_test = test.describe(include='all').drop(labels=['ID_code'], axis=1)\n# print(describe_df_test.columns.values)\nfeature_labels_test = np.array(describe_df_test.columns.str.lstrip('var_')).astype(np.int16)\nfeature_means_test = np.array(describe_df_test.loc['mean'])\nfeature_mins_test = np.array(describe_df_test.loc['min'])\nfeature_maxs_test = np.array(describe_df_test.loc['max'])\n\n## plot\nplt.figure(figsize=(12,16))\nplt.subplot(211)\nplt.fill_between(feature_labels_train,feature_mins_train,feature_maxs_train,alpha=0.2,color='g')\nplt.plot(feature_labels_train,feature_means_train,'o-',color='g',label='Train')\nplt.title(\"Train Dataset\");\nplt.xlabel(\"Feature Number #\")\nplt.ylabel(\"Feature Value\")\nplt.legend(loc=\"best\"); plt.grid()\nplt.subplot(212)\nplt.fill_between(feature_labels_test,feature_mins_test,feature_maxs_test,alpha=0.2,color='g')\nplt.plot(feature_labels_test,feature_means_test,'o-',color='g',label='Test')\nplt.title(\"Test Dataset\");\nplt.xlabel(\"Feature Number #\")\nplt.ylabel(\"Feature Value\")\nplt.legend(loc=\"best\"); plt.grid()","f8294207":"## randomly choose a list of features to look at\nfeature_plot_list = []\nfor i in range(12):\n    rand_feature = random.randrange(200)\n    feature_plot_list.append('var_%d'%rand_feature)\n\n## group by 'target' and show the mean\ntrain[['target']+feature_plot_list].groupby('target').mean()","e28b9fe6":"## plot the feature distributions by target\nfig, axs = plt.subplots(4,3,figsize=(20,16))\nfeature_idx = 0\nfor i in range(4):\n    for j in range(3): \n        g = sns.kdeplot(train[feature_plot_list[feature_idx]][train[\"target\"] == 0], color=\"Red\", ax =axs[i,j], shade = True)\n        g = sns.kdeplot(train[feature_plot_list[feature_idx]][train[\"target\"] == 1], color=\"blue\", ax =axs[i,j], shade = True)\n#         g = g.legend([\"Skewness: %.2f\"% (train[feature_plot_list[feature_idx]][train[\"target\"] == 0].skew()),\n#                       \"Skewness: %.2f\"% (train[feature_plot_list[feature_idx]][train[\"target\"] == 1].skew())])\n        g.legend([\"0\",\"1\"])\n        feature_idx += 1","80c4aaa0":"def reduce_memory_usage(df):\n    df_new = pd.DataFrame()\n    col2int_list = []\n    for col in df.columns:\n        if col in ['ID_code','target','ID']:\n            df_new[col] = df[col]\n        else:\n            isInt = False\n            max_val = df[col].max()\n            min_val = df[col].min()\n            ## check if column can be convert into integer \n            col_series_asint = df[col].astype(np.int64)\n            residule_series = df[col] - col_series_asint\n            total_residule = residule_series.sum()\n            if total_residule > -0.01 and total_residule < 0.01:\n                isInt = True\n                col2int_list.append(col)\n            if isInt:\n                df_new[col] = df[col].astype(np.int8)\n            else:\n                df_new[col] = df[col].astype(np.float32)\n    return df_new, col2int_list\n ","ad1791d2":"# start_mem_usg = train.memory_usage().sum() \/ 1024**2 \n# print(\"Memory usage of train dataframe is :\",start_mem_usg,\" MB\") \nprint (\"############## train dataset #############\")\nprint('Before reduce memory usuage:')\ntrain.info()\nprint('\\nAfter reduce memory usuage:')\ntrain_reduced,_ = reduce_memory_usage(train)\ntrain_reduced.info()\nprint (\"\\n############## test dataset #############\")\nprint('Before reduce memory usuage:')\ntest.info()\nprint('\\nAfter reduce memory usuage:')\ntest_reduced,_ = reduce_memory_usage(test)\ntest_reduced.info()","2e1c406f":"def feature_standardization(df_train,df_test):\n    df_train_stand = pd.DataFrame()\n    df_test_stand = pd.DataFrame()\n    for col in df_train.columns:\n        if col in ['ID_code','target','ID']:\n            df_train_stand[col] = df_train[col]\n            if col in df_test.columns:\n                df_test_stand[col] = df_test[col]\n        else:\n            mean = df_train[col].mean()\n            std  = df_train[col].std()\n            ## use the same parameters for both train and test set\n            df_train_stand[col] = (df_train[col] - mean)\/std\n            df_test_stand[col] = (df_test[col] - mean)\/std\n    return df_train_stand,df_test_stand","6fbecd50":"train_standard, test_standard = feature_standardization(train,test)\n\n# train_standard.dtypes\n\n## after the standarization, the datatype changed to float64, so we need to reduce the memory one more time here\ntrain_standard,_ = reduce_memory_usage(train_standard)\ntest_standard,_ = reduce_memory_usage(test_standard)\n\ntrain_standard.describe()","c524f203":"## plot the correlations between the first 20 features and the target column\n## results show very small corr values, so we will move on the the embedded method to rank the feature importance\nfig, axe = plt.subplots(figsize=(13,10))\ncolormap = sns.diverging_palette(220, 10, as_cmap = True)\nsns.heatmap(train_standard.iloc[:,1:22].corr(),annot=True, fmt = \".2f\", cmap = colormap, ax=axe)","7a1822d8":"## define the X, Y of train\/test set\ny = train_standard['target']\nX = train_standard.drop(labels=['ID_code','target','ID'],axis=1)\nX_test = test_standard.drop(labels=['ID_code'],axis=1)\n\n## train and cross-validation set split\nX_train, X_val, y_train, y_val = train_test_split(X,y,train_size=0.8,random_state=42,stratify=y)","1e03f3c8":"## k-fold train and cross-validation split\nkfold = StratifiedKFold(n_splits=5,shuffle=True,random_state=42)","a9285259":"## start with a lgb model and non-optimized parameters\nlgb_train_data = lgb.Dataset(X_train, label=y_train)\nlgb_val_data   = lgb.Dataset(X_val, label=y_val)\n\nparam = {\n    'metric':'auc',\n    'objective':'binary',\n    'verbosity':1,\n}\nnum_round = 2000\n\nbst = lgb.train(param,lgb_train_data,num_round,valid_sets=[lgb_val_data],verbose_eval=100,early_stopping_rounds=400)","2384301e":"## let's take a look at the ROC plot\ny_predict = bst.predict(X_val,num_iteration=bst.best_iteration)\nfpr, tpr, thresholds = roc_curve(y_val, y_predict)\nplt.figure(figsize=(6,5.5))\nplt.plot(fpr, tpr, label='LGBM')\nplt.xlabel('False positive rate')\nplt.ylabel('True positive rate')\nplt.title('ROC Curve')\nplt.legend(loc='best')\nplt.show()","11b77c89":"## plot the feature importance score of all features\nfeature_importances = pd.Series(bst.feature_importance(),index=X_train.columns)\nplt.figure(figsize=(14,5))\nfeature_importances.reset_index()[0].plot(kind='line',grid=True,label='LGBM')\nplt.xlabel('Feature Number')\nplt.ylabel('Feature Importance')\nplt.title('Feature Importance Plot')\nplt.legend(loc='best')\nplt.show()","8c79c7e4":"## lets have a comparison of \nfeature_scores = list(range(0,120,20))\ndisplay_df = pd.DataFrame(columns=['Feature Count',\"Accuracy\", \"ROC\", \"Precision\", \"Recall\", \"F1 Score\", \"Run Time\"])\n\nfor score in feature_scores:\n        \n    feature_selected = feature_importances[feature_importances>score].index.tolist()\n    feature_count = len(feature_selected)\n    \n    lgb_train_data_selected = lgb.Dataset(X_train[feature_selected],y_train)\n    lgb_val_data_selected   = lgb.Dataset(X_val[feature_selected], y_val)\n    \n    start = timer()\n    bst_selected = lgb.train(param,lgb_train_data_selected,num_round,valid_sets=[lgb_val_data_selected],verbose_eval=100,early_stopping_rounds=400)\n    run_time = timer()-start\n    y_predict = bst_selected.predict(X_val[feature_selected],num_iteration=bst_selected.best_iteration)\n    \n    # calculate threshod\n    fpr, tpr, thresholds = roc_curve(y_val, y_predict)\n    optimal_idx = np.argmax(tpr - fpr)\n    threshold = thresholds[optimal_idx]\n    y_predict_binary = np.where(y_predict > threshold, 1, 0)\n\n    # calculate evaluation metrics \n    roc = roc_auc_score(y_val,y_predict) \n    acc = accuracy_score(y_val,y_predict_binary)\n    prc = precision_score(y_val,y_predict_binary)\n    rcl = recall_score(y_val,y_predict_binary)\n    f1  = f1_score(y_val,y_predict_binary)\n    ## create a dataframe to summarize the scores\n    display_df.loc[score]=[feature_count,acc,roc,prc,rcl,f1,run_time]","c992495f":"## lets take a look at the summary table\ndisplay_df","971a5a13":"feature_selected = feature_importances[feature_importances>20].index.tolist()\nprint(feature_selected)","dbd14fdb":"## List of Models to check\nMLA = {\n    'LogisticRegression': LogisticRegressionCV(verbose=2),\n    'Naive Bayes': GaussianNB(),\n    'LightGBM': lgb.LGBMClassifier(verbose=1),\n#     'XGBoost': XGBClassifier(),\n}","aadd8b55":"## create a dataframe to report the training scores of each MLA\nMLA_columns = ['MLA_name','Parameters','Train_AUC','Validation_AUC','Validation_AUC_Sigma','Training_Time']\nMLA_compare = pd.DataFrame(columns=MLA_columns)\n\nfor _,(name,clf) in enumerate(MLA.items()):\n    print(\"Training:\",str(clf.__class__.__name__))\n    cv_results = cross_validate(clf,X,y,cv=kfold,scoring='roc_auc',return_train_score=True)\n    MLA_compare = MLA_compare.append({\n        'MLA_name': name,\n        'Parameters': str(clf.get_params()),\n        'Train_AUC': cv_results['train_score'].mean(),\n        'Validation_AUC': cv_results['test_score'].mean(),\n        'Validation_AUC_Sigma': cv_results['test_score'].std(),\n        'Training_Time': cv_results['fit_time'].mean() \n    },ignore_index = True)","82a19f56":"MLA_compare","515b0b48":"MLA_compare_w_featureSelection = MLA_compare.copy(deep=True)\n\nfor _,(name,clf) in enumerate(MLA.items()):\n    print(\"Training:\",str(clf.__class__.__name__))\n    cv_results = cross_validate(clf,X[feature_selected],y,cv=kfold,scoring='roc_auc',return_train_score=True)\n    MLA_compare_w_featureSelection = MLA_compare_w_featureSelection.append({\n        'MLA_name': name + ' w\/ FS',\n        'Parameters': str(clf.get_params()),\n        'Train_AUC': cv_results['train_score'].mean(),\n        'Validation_AUC': cv_results['test_score'].mean(),\n        'Validation_AUC_Sigma': cv_results['test_score'].std(),\n        'Training_Time': cv_results['fit_time'].mean() \n    },ignore_index = True)","78165dee":"MLA_compare_w_featureSelection","03c11a4f":"## optimize the parameter of Logistic Regression \nparam_grid_logReg = {\n    'scoring':['roc_auc'],\n    'multi_class': ['ovr'],\n#     'class_weight':[None, {0:1,1:9}],\n    'max_iter':[50,100],\n    'fit_intercept':[True],          # \"False\" is faster but worse fitting\n    'solver':['lbfgs'],  # 'sag' and 'saga' are slower and gave worse fitting,'newton-cg' is slightly better by much slower \n    'random_state':[42]\n}\nlogReg_clf = LogisticRegressionCV()\nlogReg_gs  = GridSearchCV(logReg_clf,param_grid=param_grid_logReg,cv=kfold,scoring='roc_auc',n_jobs=4,verbose=2)\nlogReg_gs.fit(X[feature_selected],y)","5150b6d4":"logReg_best = logReg_gs.best_estimator_\nprint(logReg_best.get_params())\nprint(logReg_gs.best_score_)","6930f086":"## optimize the parameter of Naive Bayes\nparam_grid_nb = {'priors': [None], 'var_smoothing': [1e-03,1e-06,1e-09]}\nnb_clf = GaussianNB()\nnb_gs  = GridSearchCV(nb_clf,param_grid=param_grid_nb,cv=kfold,scoring='roc_auc',verbose=2)\nnb_gs.fit(X[feature_selected],y)","b28444c1":"nb_best = nb_gs.best_estimator_\nprint(nb_best.get_params())\nprint(nb_gs.best_score_)","4f340ebd":"## optimize the parameter of Light GBM\nparam_grid_lgb = {\n    'boosting_type': ['gbdt'],\n#     'class_weight': [None, {0:1,1:7}],\n    'colsample_bytree': [0.05],  ## compared [0.05, 0.5, 1]\n    'learning_rate':[0.01],\n#     'max_depth':[5,20,-1],\n    'min_child_samples': [80],\n    'min_child_weight': [10],\n    'n_estimators':[10000],  ## compared [100,1000,10000,1000000]\n    'num_leaves':[13],         ## compared [13, 31]\n    'objective':['binary'],\n#     'random_state':[42],\n    'reg_alpha': [0],          ## compared [0,5,10]\n    'reg_lambda': [10],        ## compared [0,5,10]\n    'subsample': [0.4],        ## compared [0.4, 1]\n    'subsample_freq': [5],     ## compared [0, 5]\n#     'n_jobs':[4],\n}\n\nlgb_clf = lgb.LGBMClassifier()\nlgb_gs  = GridSearchCV(lgb_clf,param_grid=param_grid_lgb,cv=kfold,scoring='roc_auc',n_jobs=4,verbose=2)\nlgb_gs.fit(X[feature_selected],y)","e0e3d3d7":"lgb_best = lgb_gs.best_estimator_\nprint(lgb_best.get_params())\nprint(lgb_gs.best_score_)","8efad957":"# ## optimized hyperparameters\n# MLA_param = {\n#     'LogisticRegression': {'Cs': 10, 'class_weight': None, 'cv': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1.0, 'l1_ratios': None, 'max_iter': 50, 'multi_class': 'ovr', 'n_jobs': None, 'penalty': 'l2', 'random_state': 42, 'refit': True, 'scoring': 'roc_auc', 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0},\n#     'Naive Bayes': {'priors': None, 'var_smoothing': 1e-09},\n#     'LightGBM': {'boosting_type': 'gbdt', 'class_weight': None, 'colsample_bytree': 0.05, 'importance_type': 'split', 'learning_rate': 0.01, 'max_depth': -1, 'min_child_samples': 80, 'min_child_weight': 10, 'min_split_gain': 0.0, 'n_estimators': 10000, 'n_jobs': -1, 'num_leaves': 13, 'objective': 'binary', 'random_state': None, 'reg_alpha': 0, 'reg_lambda': 10, 'silent': True, 'subsample': 0.4, 'subsample_for_bin': 200000, 'subsample_freq': 5},\n# #     'XGBoost': {},\n# }\n\n## optimized estimators\nMLA_optm = {\n    'LogisticRegression': logReg_best,\n    'Naive Bayes': nb_best,\n    'LightGBM': lgb_best,\n#     'XGBoost': XGBClassifier(),\n}","21c6dc4d":"## plot learning curves\ndef plot_learning_curve(estimator, title, X, y, cv=None, n_jobs=-1, train_sizes=np.linspace(.1,1.0,5)):\n    plt.figure(figsize=(12,6))\n    plt.title(title)\n    plt.xlim((0,200000))\n    plt.xlabel(\"Training Sample Size (%)\")\n    plt.ylabel('AUC Score')\n    train_sizes_abs, train_scores, val_scores = learning_curve(estimator,X,y,scoring='roc_auc',cv=cv,n_jobs=n_jobs,train_sizes=train_sizes)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std  = np.std(train_scores, axis=1)\n    val_scores_mean   = np.mean(val_scores, axis=1)\n    val_scores_std    = np.std(val_scores, axis=1)\n    plt.grid()\n    plt.plot(train_sizes_abs,train_scores_mean,'o-',color='blue',label='Training AUC')\n    plt.plot(train_sizes_abs,val_scores_mean,'o-',color='r',label='Validating AUC')\n    plt.fill_between(train_sizes_abs,train_scores_mean-train_scores_std,train_scores_mean+train_scores_std,alpha=0.1,color='blue')\n    plt.fill_between(train_sizes_abs,val_scores_mean-val_scores_std,val_scores_mean+val_scores_std,alpha=0.1,color='r')\n    plt.legend(loc='best')\n    plt.show()\n    return plt  ","82ef35e2":"plot_learning_curve(logReg_best, 'Logistic Regression', X, y, cv=kfold)\nplot_learning_curve(nb_best, 'Naive Bayes', X, y, cv=kfold)\nplot_learning_curve(lgb_best, 'LightGBM', X, y, cv=kfold)","a99bc6f7":"lgb.plot_tree(lgb_best, tree_index=0, figsize = (20,14))","9ff57bc0":"MLA_prediction = pd.DataFrame(columns=list(MLA_optm.keys()))\n\nfor i,(column_name,optm_estimator) in enumerate (MLA_optm.items()):\n    MLA_prediction[column_name] = optm_estimator.predict(X_test[feature_selected])","36ca8b2c":"for column_name in MLA_prediction.columns:\n    print(MLA_prediction[column_name].value_counts())","5446082e":"## plot the correlations between the prediced\nfig, axe = plt.subplots(figsize=(8,6))\ncolormap = sns.diverging_palette(220, 10, as_cmap = True)\nsns.heatmap(MLA_prediction.corr(),annot=True, fmt = \".2f\", cmap = colormap, ax=axe)","d79f9c8d":"## ensembled estimator using voting classifier\nestimators = list(MLA_optm.items())\nvote_clf = VotingClassifier(estimators=estimators,voting='soft',n_jobs=4)\n## run cross validate \ncv_results = cross_validate(vote_clf,X[feature_selected],y,cv=kfold,scoring='roc_auc',return_train_score=True,return_estimator=True)","9c960d24":"print('Train_Score:',cv_results['train_score'])\nprint('mean:',cv_results['train_score'].mean())\nprint('Validate_Score:',cv_results['test_score'])\nprint('mean:',cv_results['test_score'].mean())\nprint('Run Time:',cv_results['fit_time'])\nprint('mean:',cv_results['fit_time'].mean())","ab85af5c":"## let's also compare if Light GBM outperform the ensemble method\nestimator = MLA_optm['LightGBM']\n## run cross validate \ncv_results_lgb = cross_validate(estimator,X[feature_selected],y,cv=kfold,scoring='roc_auc',return_train_score=True,return_estimator=True)","0bb8f19c":"print('Train_Score:',cv_results_lgb['train_score'])\nprint('mean:',cv_results_lgb['train_score'].mean())\nprint('Validate_Score:',cv_results_lgb['test_score'])\nprint('mean:',cv_results_lgb['test_score'].mean())\nprint('Run Time:',cv_results_lgb['fit_time'])\nprint('mean:',cv_results_lgb['fit_time'].mean())","bd71aded":"# vote_clf_optm = cv_results['estimator'][0]","54d6a4bc":"## final lightGBM model\nparams = {'objective' : \"binary\", \n               'boost':\"gbdt\",\n               'metric':\"auc\",\n               'boost_from_average':\"false\",\n               'num_threads':8,\n               'learning_rate' : 0.01,\n               'num_leaves' : 13,\n               'max_depth':-1,\n               'tree_learner' : \"serial\",\n               'feature_fraction' : 0.05,\n               'bagging_freq' : 5,\n               'bagging_fraction' : 0.4,\n               'min_data_in_leaf' : 80,\n               'min_sum_hessian_in_leaf' : 10.0,\n               'verbosity' : 1}\nnum_round = 1000000","e2a62e41":"%%time\nkFold = StratifiedKFold(n_splits=10, shuffle=False, random_state=42)\ny_pred_lgb = np.zeros(len(test_standard))\n\nfor fold_n, (train_index, valid_index) in enumerate(kFold.split(X,y)):\n    print('Fold', fold_n, 'started at', time.ctime())\n    X_train, X_valid = X.iloc[train_index], X.iloc[valid_index]\n    y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]\n    \n    train_data = lgb.Dataset(X_train, label=y_train)\n    valid_data = lgb.Dataset(X_valid, label=y_valid)\n        \n    lgb_model = lgb.train(params,train_data,num_round,\n                          valid_sets = [train_data, valid_data],verbose_eval=1000,early_stopping_rounds = 3500)##change 10 to 200\n            \n    y_pred_lgb += lgb_model.predict(X_test, num_iteration=lgb_model.best_iteration)\/5","e227f964":"## create the submit dataFrame\nsubmit = pd.DataFrame(columns=['ID_code','target'])\nsubmit['ID_code'] = test_standard['ID_code']\nsubmit['target']  = y_pred_lgb","1d44ccd3":"## finally! submit\nsubmit.to_csv(\"submit3.csv\", index=False)","479d755f":"print('Submit Data Distribution: \\n', submit['target'].value_counts())\nsubmit.sample(10)","11490038":"A simple training of these three models with default hyperparameters shows good AUC on validation set. In the next section, lets see if we can further improve the AUC score of logistic regression and lightGBM through hyperparameter tunning.","6f2507d1":"# **BinaryClassification: Customer_Transaction_Predict**\n\n1. **Define the Problem**\n2. **Load and Check the Data**\n    * 2.1 import data modeling libraries\n    * 2.2 preview data, check for outliers and missing values\n3. **Exploratory Data Analysis (EDA)**\n    * 3.1 data visualization and statistics\n    * 3.2 missing values imputation\n    * 3.3 skewed data preprocessing\n    * 3.4 reducing memory size\n    * 3.5 imbalanced output classes\n4. **Feature Engineering**\n    * 4.1 Feature Normalization\n    * 4.2 feature interactions\n    * 4.3 feature encodings\n    * 4.4 feature selection\n5. **Modeling & Hyperparameter Tunning**\n    * 5.1 split dataset\n    * 5.2 simple model and feature importance  \n    * 5.3 compare MLA models\n    * 5.4 hyperparameter tunning\n    * 5.5 visulization: learning curves and tree structure \n6. **Model Ensemble and Prediction**\n    * 6.1 prediction correlations\n    * 6.2 ensemble modeling\n    * 6.3 predict and submit results","60f96e73":"**3.2 missing values imputation**\n\nFrom the data preview in step 2.2, we saw no missing values in either the training dataset or the test dataset.\nSo here we can skip this step of filling the missing values.\n\n\n**3.3 skewed data preprocessing**\n\nIn the data visualization in step 3.1, we looked at randomly picked features and the skewness are very small.\nSo we can skip this step as well.","6644a571":"**5.2 simple model and feature importance**\n\nStart with a simple model can give us the baseline prediction of this problem, and also provide feature importance scores for guiding the feature selection.\n\nAs a starting model, it should be fast enough. Light GBM is clearly a good approach when dealing with such a large datasets.","ee671249":"**2.2 preview data, check for outliers and missing values**","0ca0dad4":"> As we see, the max number of outlier features that a row could have is only 4, and it is out of 200 features. So we don't have to drop any outliers.","efcead29":"Let's also take a look at the first tree structure in the light GBM estimator.","3d50fcc0":"# **2. Load and Check the Data**\n\n**2.1 import data modeling libraries**\n","8aa0dfdf":"**5.4 hyperparameter tunning**\n\nHere we will use GridSearchCV method to optimize the hyperparameters.","dd5084df":"Here, an inital modeling gives us 88.79% AUC score, which is not bad. We can now analyze the feature importance and evaluate if we can reduce the feature space with feature selection.","99a40222":"Here we can see that logistic regression is hard to see significant improvement, likely due to bias issue coming from the model simplicity.","d070743a":"**6.3 predict and submit results**\n\nSince lightGBM outperforms the ensembled model, we will use the lighGBM for submission.","192a69b9":"# **6. Model Ensemble and Prediction**\n\n**6.1 prediction correlations**\n\nCompare MLA predictions with each other, where 1 = exactly similar and 0 = exactly opposite. Based on the correlation, we can create a \"super algorithm\" (ensemble algorithm) by combining them.","b708ef84":"**5.3 compare MLA models**\n\nFor this binary classification problem with large feature space, lets consider the following models with relatively short run time:\n* Logistic regression\n* Naive Bayes\n* LightBGM\n* XGBoost\n* Ensemble\n","4666638c":"By setting the feature importance threshold to 20, we got 188 features. Even though we couldn't save much on run time or memory usage, we are able to increase the ROC by 0.1%.","085cec9b":"Here we can see that the memory usages of train and test datasets are reduced by 50%.","2c858cd3":"Here we can see that feature selection improves the ROC score for all three models.","decc9a8a":"Data shows almost no correlation between each feature, it indicate that all these anonymous features could have been well engineered. At this point, we will leave all the features for now, and re-evaluate the feature selection in the modeling step in section5.","b40cb3f4":"# **1. Define the Problem**\n \nThis kaggle project is to identify which customers will make a specific transaction in the future, irrespective of the amount of money transacted. The output Y is column \"target\" in the training dataset, which contains \"1\" for successful transaction and \"0\" for no transaction. This is a binary classification problem with supervised learning.\n\nSubmissions are evaluated on **area under the ROC curve** between the predicted probability and the observed target. So we need to use the **roc_auc_score** as our metrics.","174e122d":"Wow! By tunning hyperparameter, we get 0.9003 AUC now with LightGBM (We will see further improvement in we increase the 'n_estimators' to 1,000,000).","17006f7c":"# **4. Feature Engineering**\n\n**4.1 Feature Normalization**\n\nFrom the EDA in section3, we gain more insights of the dataset. We see that all features are numerical and are close to normal distributions. So we can perform standardization on each feature: \n\nX = (X - mean) \/ std\n\nThe feature normalization is essential for some ML models, e.g. SVM, and tree modes are typically not affected by any monotonic transformation.","dff30fde":"**3.5 imbalanced output classes**\n\nFrom section3.1 we can see that the output classed are highly imbalanced.\n\n0:179902\n1: 20098\n\nThere are different techniques to due with imbalanced classification problem, such as:\n* **Resampling**: undersampling of the majority class, or oversampling of the minority class\n* **Change performance metrics**: instead of using accuracy, use confusion matric, precision\/recall\/F1-score, or ROC-AUC\n* **Change machine learning algorithm**: use models that penalize mistakes on the minorty class, and tree models typically perform well on imbalanced data\n\nIn this study, we will focus on using performance metrics of ROC-AUC, and compare several different ML models.","5d2b1ef6":"For data visualization, we need to consider what kind of plots matches better with the data type.\nIn this dataset excluding the 'ID_code', we have two data types: (1) 'target': categorical; (2) 'var_#': numerical with continous values.\n\n* **For categorical x-axis and numerical y-axis, we can use:** boxplot, barplot, violinplot, hist, etc.\n* **For numerical x-axis and categorical legend, we can use:** displot, kdeplot, etc.\n\nHere we cannot plot all 200 feature columns, so we will randomly pick a few for this checkout","6ef584fe":"**4.4 Feature Selection**\n\nWe have 200 features here and 200k samples that can make training quite slow. So we want to reduce the feature space by evaluating feature importance and by removing the features that don't influence the outcome or have strong correlations with other features.\n\nHere we will try two different methods:\n* **Filtering method**: evaluate the pearson correlation between features, and between feature and target\n* **Embedded method**: rank the feature importance by built-in class in tree models (we will do this in **section 5**)","0f127f3b":"**4.2 Feature Interactions**\n\nIn this dataset, all features are senseless names, and we cannot do meaningful feature interaction to add new features. So we can skip this step.\n\n\n**4.3 Feature Encodings**\n\nIn this dataset, all features are numerical and senseless named, so we cannot do meaningful grouping or categorization. This step is also skipped. ","6c1e03c5":"For light GBM model, we have checked multiple hyperparameters. To save running time, we only listed the optimized values for some hyperparameters. The \"n_estimators\" of value 1,000,000 gives even better AUC score, however, we only listed 10,000 here to save run time in this step. ","0e10ccf1":"**6.2 ensemble modeling**","4b59c5de":"# **3. Exploratory Data Analysis (EDA)**\n**3.1 data visualization and statistics**\n\nThe column \"ID_code\" has 200K unique values and is not expected to be correlated to the prediction. Distribution plot of ID numbers with either target labels shows uniform distribution, and therefore we don't need to include the ID numbers in the modeling.","c98a7763":"**3.4 reducing memory size**\n\nFrom the plotted feature values in step 3.1, we can see that all features have mean value in the range of (0,25) and max range within (-100,100). So we can consider to reduce the memory size of train\/test dataframes in order to facilitate modeling step.","20d8a4b2":"**5.5 visulization: learning curves and tree structure**\n\nLearning curves are a good way to see the overfitting effect on the training set and the effect of the training size on the accuracy.\n","988acab1":"# **5. Modeling & Hyperparameter Tunning**\n\n**5.1 split dataset**\n\nHere we will split our dataset to 80% training set and 20% cross-validation set.\nWe will start with the \"train_test_split\" method for one pair of datasets to facilitate our simple model. Later, we will also use 10-fold split of train and cross-validation dataset for model comparisons."}}