{"cell_type":{"b7a32d19":"code","a230b493":"code","30c52069":"code","3033210d":"code","ddb80abd":"code","c5930a9a":"code","e95c406e":"code","1d36ffad":"code","192b88f4":"code","f6a95167":"code","6e8ab23f":"code","73ce584b":"code","ae6f5798":"code","645ac2ef":"code","3e797815":"code","11c3c94b":"code","b3aed676":"code","cd57c476":"code","3172cafc":"code","e3a9997d":"code","ad5900f6":"code","c2bbe557":"code","cc6854b8":"code","8bb8318a":"code","368e7f2c":"code","84e59548":"code","9098252f":"code","67bc1ffa":"code","d979e8c0":"code","05668dcf":"code","2d951231":"code","9b218820":"code","4166b39a":"code","5e5bf39f":"code","1e9dd728":"code","3a80556f":"code","e04628c9":"code","c5086af0":"code","29530ace":"code","2311b895":"code","e8cf567c":"code","777e9b1c":"code","50dea038":"code","ef96962c":"code","727afe94":"code","f25feec4":"code","de527168":"code","b18ed504":"code","884076e6":"code","56e4e241":"code","bae23f33":"code","92b48fa8":"code","b61a450d":"code","b11d7dfa":"code","1c088303":"code","9a0dd4ed":"code","ef556ac3":"code","06ae0c05":"code","3523c836":"code","0dc6714f":"code","ebd08f67":"code","f82109c6":"code","65446fdd":"code","174c6f5b":"code","71f4f5d4":"code","98f39a5e":"code","b3500cda":"code","e7998a4d":"code","b34a317a":"code","fc3b84ce":"code","7117c44b":"code","9c396721":"code","e24ed2ac":"code","d5c45d82":"markdown","64a8743a":"markdown","cc08324b":"markdown","1b36e6a1":"markdown","224ebe89":"markdown","ed1af543":"markdown","1568d2e2":"markdown","29b73ed9":"markdown","650c2b05":"markdown","5816338f":"markdown","cf38e7db":"markdown","7efa2fcf":"markdown","2cf96de4":"markdown","a60d0d2d":"markdown","ec26dc91":"markdown","35e3fcb7":"markdown","9b5b33c7":"markdown","2033184f":"markdown","9bdcb1f1":"markdown","41dd2158":"markdown","84e2cfc8":"markdown","d3e450f9":"markdown","b9b98714":"markdown","a8db81e8":"markdown","6148799f":"markdown","958bd817":"markdown","294f0588":"markdown","6e43b781":"markdown","4db2583c":"markdown","01abd00b":"markdown","a296f6e0":"markdown","e9724de9":"markdown","4b55574c":"markdown","c022bc99":"markdown","b00d1950":"markdown","f57c8e6b":"markdown","83c7a004":"markdown","0d9e1b5e":"markdown","2f4b9c77":"markdown","88f9f0d4":"markdown","2a6dd134":"markdown","39950259":"markdown","66113b6b":"markdown","9bd7b796":"markdown","893e40d5":"markdown","9442c422":"markdown","80b62cb8":"markdown","50a28ba4":"markdown","d64d05f0":"markdown","25d1aa9e":"markdown","7e4105ca":"markdown","e953f6f9":"markdown","2345db07":"markdown","c4c81806":"markdown","169081a5":"markdown","e66ae676":"markdown","21f6c7b2":"markdown","c0f8c45c":"markdown","a28de561":"markdown","a340815f":"markdown","5255d194":"markdown","206a4763":"markdown","09336302":"markdown","a611a6ab":"markdown","6cc00ef8":"markdown","c9394e58":"markdown","ed1f09a0":"markdown","98e683b7":"markdown","bb07ea1f":"markdown","a2949e96":"markdown","cdec2eb6":"markdown","565dde24":"markdown"},"source":{"b7a32d19":"#@title You can watch this notebook (in Turkish) on YouTube!\nfrom IPython.lib.display import YouTubeVideo\nYouTubeVideo('ioYLHUH_ZuI')","a230b493":"%matplotlib inline","30c52069":"import numpy as np\ny_true = np.array([[0, 1, 1, 1],[0,0,1,0],[1,1,0,0]])\ny_scores = np.array([[0.2, 0.6, 0.1, 0.8],[0.4,0.9,0.8,0.6],[0.8,0.4,0.5,0.7]])\n","3033210d":"threshold = 0.5\n","ddb80abd":"y_pred=[]\nfor sample in  y_scores:\n  y_pred.append([1 if i>=0.5 else 0 for i in sample ] )\ny_pred = np.array(y_pred)\ny_pred","c5930a9a":"from sklearn.metrics import accuracy_score\naccuracy_score(y_true, y_pred)","e95c406e":"from sklearn.metrics import confusion_matrix\n#confusion_matrix(y_true, y_pred)","1d36ffad":"print(\"Actual \\n\", y_true)\nprint(\"\\nPredicted \\n\",y_pred)\n","192b88f4":"from sklearn.metrics import multilabel_confusion_matrix\nmultilabel_confusion_matrix(y_true, y_pred)","f6a95167":"from sklearn.metrics import classification_report\n\nlabel_names = ['label A', 'label B', 'label C', 'label D']\n\nprint(classification_report(y_true, y_pred,target_names=label_names))","6e8ab23f":"print(\"Actual \\n\", y_true)\nprint(\"\\nPredicted \\n\",y_pred)","73ce584b":"from sklearn import metrics\nprint(\"None \", metrics.precision_score(y_true, y_pred, average=None))  \n","ae6f5798":"\nprint(\"micro: {:.2f}\".format(metrics.precision_score(y_true, y_pred, average='micro')))\nprint(\"macro: {:.2f} \".format( metrics.precision_score(y_true, y_pred, average='macro')))\nprint(\"weighted: {:.2f} \".format( metrics.precision_score(y_true, y_pred, average='weighted')))\nprint(\"samples: {:.2f} \".format( metrics.precision_score(y_true, y_pred, average='samples')))  \n\n\n","645ac2ef":"y_scores","3e797815":"print(\"micro: {:.2f}\".format(metrics.average_precision_score(y_true, y_scores, average='micro')))\nprint(\"macro: {:.2f} \".format( metrics.average_precision_score(y_true, y_scores, average='macro')))\nprint(\"weighted: {:.2f} \".format( metrics.average_precision_score(y_true, y_scores, average='weighted')))\nprint(\"samples: {:.2f} \".format( metrics.average_precision_score(y_true, y_scores, average='samples')))  ","11c3c94b":"np.concatenate((y_true.reshape((12,1), order='F'), y_pred.reshape((12,1), order='F')), axis=1)","b3aed676":"TP=((y_true * y_pred) == 1).sum()\nprint(\"TP: \", TP)","cd57c476":"converted_y_true= np.copy(y_true)\nconverted_y_true[converted_y_true==1] = 5\nconverted_y_true[converted_y_true==0] = 1\nconverted_y_true[converted_y_true==5] = 0\nFP= ((converted_y_true * y_pred)== 1).sum()\nprint(\"FP: \", FP)","3172cafc":"print(\" Micro Precision {:.2f}\".format(TP\/(TP+FP)))","e3a9997d":"print(\"Micro Precision: {:.2f}\".format(metrics.precision_score(y_true.ravel(), y_pred.ravel())))","ad5900f6":"print(classification_report(y_true, y_pred,target_names=label_names))","c2bbe557":"totalPrecision= 0\nfor i in range (len(label_names)):\n  p= metrics.precision_score(y_true[:,i], y_pred[:,i])\n  totalPrecision+= p\n  print(\"For {} precision: {:.2f}\".format(label_names[i], p))\nprint(\"Macro Precision: {:.2f}\".format(totalPrecision\/len(label_names)))","cc6854b8":"totalPrecision=0\ntotalSupport=0\nfor i in range (len(label_names)):\n  p= metrics.precision_score(y_true[:,i], y_pred[:,i])\n  support= (y_true[:,i]==1).sum()\n  totalSupport+=support\n  totalPrecision+= p*support\n  print(\"For {} precision: {:.2f} support: {}\".format(label_names[i], p, support ))\nprint(\"Weighted Precision: {:.2f}\".format(totalPrecision\/totalSupport))\n\n\n\n","8bb8318a":"print(classification_report(y_true, y_pred,target_names=label_names))","368e7f2c":"totalPrecision=0\n\nfor i in range (len(y_true)):\n  p= metrics.precision_score(y_true[i,:], y_pred[i,:])\n  totalPrecision+=p\n  print(\"For Sample {} precision: {:.2f} \".format(y_true[i,:], p ))\nprint(\"Sample Precision: {:.2f}\".format(totalPrecision\/len(y_true)))\n","84e59548":"print(\"Recall of each label: {}\".format(metrics.recall_score(y_true, y_pred, average=None)))","9098252f":"print(\"micro: {:.2f}\".format(metrics.recall_score(y_true, y_pred, average='micro')))\nprint(\"macro: {:.2f} \".format( metrics.recall_score(y_true, y_pred, average='macro')))\nprint(\"weighted: {:.2f} \".format( metrics.recall_score(y_true, y_pred, average='weighted')))\nprint(\"samples: {:.2f} \".format( metrics.recall_score(y_true, y_pred, average='samples')))  ","67bc1ffa":"print(\"F1 of each label: {}\".format(metrics.f1_score(y_true, y_pred, average=None)))","d979e8c0":"print(\"micro: {:.2f}\".format(metrics.f1_score(y_true, y_pred, average='micro')))\nprint(\"macro: {:.2f} \".format( metrics.f1_score(y_true, y_pred, average='macro')))\nprint(\"weighted: {:.2f} \".format( metrics.f1_score(y_true, y_pred, average='weighted')))\nprint(\"samples: {:.2f} \".format( metrics.f1_score(y_true, y_pred, average='samples')))  ","05668dcf":"print(\"y_true: \\n{}\".format(y_true))\nprint(\"y_scores: \\n{}\".format(y_scores))","2d951231":"def predictLabelForGivenThreshold(threshold):\n  y_pred=[]\n  for sample in  y_scores:\n    y_pred.append([1 if i>=threshold else 0 for i in sample ] )\n  return np.array(y_pred)\n  ","9b218820":"y_pred  = predictLabelForGivenThreshold(0.5)\nprint(classification_report(y_true, y_pred,target_names=label_names))","4166b39a":"y_pred  = predictLabelForGivenThreshold(0.2)\nprint(classification_report(y_true, y_pred,target_names=label_names))","5e5bf39f":"y_pred  = predictLabelForGivenThreshold(0.8)\nprint(classification_report(y_true, y_pred,target_names=label_names))","1e9dd728":"#@title You can watch this notebook (in Turkish) on YouTube!\nfrom IPython.lib.display import YouTubeVideo\nYouTubeVideo('KIQ_27Y0I1c')","3a80556f":"#@title Let's select first label as a binary classification:\nselectedLabel=2 #@param {type:\"integer\"}","e04628c9":"print(\"y_true\\n\", y_true)\ny_binary = y_true[:,selectedLabel]\nprint(\"y_binary\\n\", y_binary)","c5086af0":"y_binary_scores = y_scores[:,selectedLabel]\nprint(\"y_binary_scores\\n\", y_binary_scores)","29530ace":"from sklearn.metrics import roc_curve\n\nfpr, tpr, thresholds = roc_curve(y_binary, y_binary_scores)\nprint(\"selected label {}\\nfpr {} \\ntpr {} \\nthresholds {}\".format(label_names[selectedLabel], fpr, tpr, thresholds))","2311b895":"import matplotlib.pyplot as plt\nplt.figure()\nlw = 2\nplt.plot(fpr, tpr, color='darkorange',\n         lw=lw, label='ROC curve (area = ?)' )\nplt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic for {}'.format(label_names[selectedLabel]))\nplt.legend(loc=\"lower right\")\nplt.show()","e8cf567c":"from sklearn import datasets\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.datasets import make_multilabel_classification\nfrom sklearn.metrics import plot_roc_curve\n\nX, y = make_multilabel_classification(n_classes=5, n_labels=3, allow_unlabeled=False, random_state=1)\nX_train, X_test, y_train, y_test = train_test_split(X,y)\n\nprint(\"X_train {} y_train {} \".format(X_train.shape,y_train.shape))\n\n","777e9b1c":"clf = OneVsRestClassifier(LogisticRegression(random_state=0))\nclf.fit(X_train, y_train)\n  ","50dea038":"preds = clf.predict(X_test )\nscores = clf.predict_proba(X_test)","ef96962c":"sample=15\nprint(\"prediction for sample {} {}\".format(sample, preds[sample]))\nprint(\"probabilities for sample {} {}\".format(sample, scores[sample]))\n","727afe94":"#plot_roc_curve(clf, X_test, y_test)  \n#plt.show()","f25feec4":"clf = LogisticRegression(random_state=0)\nclf.fit(X_train, y_train[:,0])\nlabel_0= plot_roc_curve(clf, X_test, y_test[:,0], name= 'label 0',)  \nplt.show() ","de527168":"clf = LogisticRegression(random_state=0)\nclf.fit(X_train, y_train[:,1])\nax = plt.gca()\nlabel_1 = plot_roc_curve(clf, X_test, y_test[:,1], name= 'label 1', ax=ax, alpha=0.8)\nlabel_0.plot(ax=ax, alpha=0.8)\nplt.show()","b18ed504":"labelPlots ={}\nfor i in range (len(label_names)+1):\n  clf = LogisticRegression(random_state=0)\n  clf.fit(X_train, y_train[:,i])\n  ax = plt.gca()\n  labelPlots[i]= plot_roc_curve(clf, X_test, y_test[:,i], name= ('label_'+str(i)), ax=ax, alpha=0.8) \n  \nplt.show()\n\n","884076e6":"from sklearn.metrics import roc_auc_score\nprint(\"roc_auc_score for different averaging methods:\")\nprint(\"\\tmacro:{:.2} \".format(roc_auc_score(y_test, scores)))\nprint(\"\\tmicro: {:.2} \".format(roc_auc_score(y_test, scores, average='micro')))\nprint(\"\\tweighted: {:.2} \".format(roc_auc_score(y_test, scores, average='weighted')))\nprint(\"\\tNone: {} \".format(roc_auc_score(y_test, scores, average=None)))","56e4e241":"roc_auc_score(y_test, scores, average=None).mean()","bae23f33":"from sklearn import svm, datasets\nfrom sklearn.model_selection import train_test_split\nimport numpy as np\n\niris = datasets.load_iris()\nX = iris.data\ny = iris.target\n\n","92b48fa8":"X.shape\n\n","b61a450d":"# Add noisy features\nrandom_state = np.random.RandomState(0)\nn_samples, n_features = X.shape\n\nrandom_features= random_state.randn(n_samples, 200 * n_features)\nX = np.c_[X,random_features ]\n\n","b11d7dfa":"X.shape\n","1c088303":"# Limit to the two first classes, and split into training and test\nX_train, X_test, y_train, y_test = train_test_split(X[y < 2], y[y < 2],\n                                                    test_size=.5,\n                                                    random_state=random_state)\n\n# Create a simple classifier\nclassifier = svm.LinearSVC(random_state=random_state)\nclassifier.fit(X_train, y_train)\ny_score = classifier.decision_function(X_test)\ny_predict = classifier.predict(X_test)","9a0dd4ed":"print(y_score[:5])\nprint(y_predict[:5])\n","ef556ac3":"from sklearn.metrics import average_precision_score\naverage_precision = average_precision_score(y_test, y_score)\n\nprint('Average precision-recall score: {0:0.2f}'.format(\n      average_precision))","06ae0c05":"from sklearn.metrics import precision_recall_curve\nfrom sklearn.metrics import plot_precision_recall_curve\nimport matplotlib.pyplot as plt\n\ndisp = plot_precision_recall_curve(classifier, X_test, y_test)\ndisp.ax_.set_title('2-class Precision-Recall curve: '\n                   'AP={0:0.2f}'.format(average_precision))","3523c836":"from sklearn.preprocessing import label_binarize\n\n# Use label_binarize to be multi-label like settings\nY = label_binarize(y, classes=[0, 1, 2])\nn_classes = Y.shape[1]\n\n# Split into training and test\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=.5,\n                                                    random_state=random_state)\n\n# We use OneVsRestClassifier for multi-label prediction\nfrom sklearn.multiclass import OneVsRestClassifier\n\n# Run classifier\nclassifier = OneVsRestClassifier(svm.LinearSVC(random_state=random_state))\nclassifier.fit(X_train, Y_train)\ny_score = classifier.decision_function(X_test)\ny_predict =  classifier.predict(X_test)\n\n","0dc6714f":"print(y_score[:5])\nprint(y_predict[:5])","ebd08f67":"from sklearn.metrics import precision_recall_curve\nfrom sklearn.metrics import average_precision_score\n\n# For each class\nprecision = dict()\nrecall = dict()\naverage_precision = dict()\nfor i in range(n_classes):\n    precision[i], recall[i], _ = precision_recall_curve(Y_test[:, i],\n                                                        y_score[:, i])\n    average_precision[i] = average_precision_score(Y_test[:, i], y_score[:, i])\n\n# A \"micro-average\": quantifying score on all classes jointly\nprecision[\"micro\"], recall[\"micro\"], _ = precision_recall_curve(Y_test.ravel(),\n    y_score.ravel())\naverage_precision[\"micro\"] = average_precision_score(Y_test, y_score,\n                                                     average=\"micro\")\nprint('Average precision score, micro-averaged over all classes: {0:0.2f}'\n      .format(average_precision[\"micro\"]))","f82109c6":"y_true = np.array([0, 0, 1, 1])\ny_scores = np.array([0.1, 0.4, 0.35, 0.8])\nprecision, recall, thresholds = precision_recall_curve(\n    y_true, y_scores)\nprint(precision)\nprint(recall)\nprint(thresholds)\n","65446fdd":"plt.figure()\n\n#plt.step(np.append(0.0,thresholds), np.flip(precision), where='post')\nplt.step(np.append(0.0,thresholds), np.flip(recall), where='post')\nplt.xlabel('Treshold')\nplt.ylabel('Precision')\nplt.ylim([0.0, 1.05])\nplt.xlim([0.0, 1.0])\nplt.title(' precision  vs threshold ')\nplt.show()","174c6f5b":"# For each class\nprecision = dict()\nrecall = dict()\ntreshold = dict()\naverage_precision = dict()\n\n\nY_actual= np.array([[0,1,1,1], [0,0,1,0], [1,1,0,0]], dtype=np.int)\ny_prediction= np.array([[0.2,0.6,0.1,0.8],[0.4,0.9,0.8,0.6],[0.8,0.4,0.5,0.7]], dtype=np.float64)\n\nn_classes = Y_actual.shape[1]","71f4f5d4":"for i in range(n_classes):\n    precision[i], recall[i], treshold[i] = precision_recall_curve(Y_actual[:, i], y_prediction[:, i])\n    average_precision[i] =average_precision_score(Y_actual[:, i], y_prediction[:, i])\n\nprint (\"average_precision {} \".format(average_precision))","98f39a5e":"x=np.append(treshold[2],1)\n\nprint(x)\nprint((precision[2]))\n","b3500cda":"plt.figure()\n\nfor i in range(n_classes):\n  plt.step(np.append(treshold[i],1), precision[i], where='post')\n  plt.xlabel('Treshold')\n  plt.ylabel('Precision')\n  plt.ylim([0.0, 1.05])\n  plt.xlim([0.0, 1.0])\n  plt.title(\n    'Average precision score for class {}: AP={:.3f}'.format(i, average_precision[i]))\n  plt.show()","e7998a4d":"plt.figure()\n\nfor i in range(n_classes):\n  plt.step(recall[i], precision[i], where='post')\n  plt.xlabel('Recall')\n  plt.ylabel('Precision')\n  plt.ylim([0.0, 1.05])\n  plt.xlim([0.0, 1.0])\n  plt.title(\n    'Average precision score for class {}: AP={:.3f}'.format(i, average_precision[i]))\n  plt.show()","b34a317a":"\n\n\n# A \"micro-average\": quantifying score on all classes jointly\nprecision[\"micro\"], recall[\"micro\"], _ = precision_recall_curve(Y_actual.ravel(),\n    y_prediction.ravel())\naverage_precision[\"None\"] = average_precision_score(Y_actual, y_prediction, average=None)\naverage_precision[\"Void\"] = average_precision_score(Y_actual, y_prediction)\naverage_precision[\"micro\"] = average_precision_score(Y_actual, y_prediction, average=\"micro\")\naverage_precision[\"macro\"] = average_precision_score(Y_actual, y_prediction, average=\"macro\")\naverage_precision[\"samples\"] = average_precision_score(Y_actual, y_prediction, average=\"samples\")\naverage_precision[\"weighted\"] = average_precision_score(Y_actual, y_prediction, average=\"weighted\")\n\n#print('Average precision score, micro-averaged over all classes: {0:0.2f}'.format(average_precision[\"micro\"]))\n    \naverage_precision","fc3b84ce":"precision","7117c44b":"plt.figure()\nplt.step(recall['micro'], precision['micro'], where='post')\n\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.ylim([0.0, 1.05])\nplt.xlim([0.0, 1.0])\nplt.title(\n    'Average precision score, micro-averaged over all classes: AP={0:0.2f}'\n    .format(average_precision[\"micro\"]))","9c396721":"np.linspace(0.01, 1)","e24ed2ac":"from itertools import cycle\n# setup plot details\ncolors = cycle(['navy', 'turquoise', 'darkorange', 'cornflowerblue', 'teal'])\n\nplt.figure(figsize=(7, 8))\nf_scores = np.linspace(0.2, 0.8, num=4)\nlines = []\nlabels = []\nfor f_score in f_scores:\n    x = np.linspace(0.01, 1)\n    y = f_score * x \/ (2 * x - f_score)\n    l, = plt.plot(x[y >= 0], y[y >= 0], color='gray', alpha=0.2)\n    plt.annotate('f1={0:0.1f}'.format(f_score), xy=(0.9, y[45] + 0.02))\n\nlines.append(l)\nlabels.append('iso-f1 curves')\nl, = plt.plot(recall[\"micro\"], precision[\"micro\"], color='gold', lw=2)\nlines.append(l)\nlabels.append('micro-average Precision-recall (area = {0:0.2f})'\n              ''.format(average_precision[\"micro\"]))\n\nfor i, color in zip(range(n_classes), colors):\n    l, = plt.plot(recall[i], precision[i], color=color, lw=2)\n    lines.append(l)\n    labels.append('Precision-recall for class {0} (area = {1:0.2f})'\n                  ''.format(i, average_precision[i]))\n\nfig = plt.gcf()\nfig.subplots_adjust(bottom=0.25)\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.title('Extension of Precision-Recall curve to multi-class')\nplt.legend(lines, labels, loc=(0, -.38), prop=dict(size=14))\n\n\nplt.show()","d5c45d82":"### Classification Report when threshold = 0.8\n\n![alt text](https:\/\/github.com\/kmkarakaya\/ML_tutorials\/blob\/master\/images\/sample%20mutli%20label%20classification%20predictions%2008.png?raw=true)","64a8743a":"#Synthetic Data:\n[Calculate mean Average Precision (mAP) for multi-label classification](https:\/\/medium.com\/@hfdtsinghua\/calculate-mean-average-precision-map-for-multi-label-classification-b082679d31be)","cc08324b":"Plot Precision-Recall curve for each class and iso-f1 curves\n.............................................................\n\n\n","1b36e6a1":"In multi-label settings\n------------------------\n\nCreate multi-label data, fit, and predict\n...........................................\n\nWe create a multi-label dataset, to illustrate the precision-recall in\nmulti-label settings\n\n","224ebe89":"### In a multilabel classification, plot_roc_curve will **generate error!**","ed1af543":"**Notice**: Here, accuracy is based on if all the 4-class prediction is correct or not.\n\nIt is not label-based!\n\nTherefore accuracy is zero!\n\nWe have to use better metrics for multilabel classification","1568d2e2":"### Recall of each label\n\n![Predictions](https:\/\/github.com\/kmkarakaya\/ML_tutorials\/blob\/master\/images\/sample%20mutli%20label%20classification%20predictionss.png?raw=true)","29b73ed9":"Plot the micro-averaged Precision-Recall curve\n...............................................\n\n\n","650c2b05":"##Precision\n[precision_score](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.precision_score.html#sklearn-metrics-precision-score)\n\n\n* The precision is the ratio **tp \/ (tp + fp)** where tp is the number of true positives and fp the number of false positives. The precision is intuitively the ability of the classifier not to label as positive a sample that is negative.\n\n* The best value is 1 and the worst value is 0.\n\n![Sample Confusion Matrix](https:\/\/github.com\/kmkarakaya\/ML_tutorials\/blob\/master\/images\/confusion%20matrix%20sample.png?raw=true)\n\n***average*** parameter is required for multiclass\/multilabel targets. \n\n* **None**, the scores for each class are returned. Otherwise, this determines the type of averaging performed on the data:\n\n* '**micro**':\nCalculate metrics globally by counting the total true positives, false negatives and false positives.\n\n* '**macro**':\nCalculate metrics for each label, and find their unweighted mean. This does not take label imbalance into account.\n\n* '**weighted**':\nCalculate metrics for each label, and find their average weighted by support (the number of true instances for each label). This alters \u2018macro\u2019 to account for label imbalance; it can result in an F-score that is not between precision and recall.\n\n* '**samples**':\nCalculate metrics for each instance, and find their average (only meaningful for multilabel classification where this differs from accuracy_score).\n","5816338f":"### Which threshold?\n\n* Which threshold yileds better result?\n* Which metric do you care?\n* How can you compare different models' success\/performance?\n* Which precision\/recall\/f1 value to report?","cf38e7db":"---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n<ipython-input-107-d9b794d228ad> in <module>()\n----> 1 plot_roc_curve(clf, X_test, y_test)\n      2 plt.show()\n\n\/usr\/local\/lib\/python3.6\/dist-packages\/sklearn\/metrics\/_plot\/roc_curve.py in plot_roc_curve(estimator, X, y, sample_weight, drop_intermediate, response_method, name, ax, **kwargs)\n    178     if y_pred.ndim != 1:\n    179         if y_pred.shape[1] != 2:\n--> 180             raise ValueError(classification_error)\n    181         else:\n    182             y_pred = y_pred[:, 1]\n\n**ValueError: OneVsRestClassifier should be a binary classifier**","7efa2fcf":"###Average Precision\n[precision_score funtion has](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.precision_score.html#sklearn-metrics-precision-score)\n\n***average*** parameter is required for multiclass\/multilabel targets. \n\n* **None**, the scores for each class are returned. Otherwise, this determines the type of averaging performed on the data:\n\n* '**micro**':\nCalculate metrics globally by counting the total true positives, false negatives and false positives.\n\n* '**macro**':\nCalculate metrics for each label, and find their unweighted mean. This does not take label imbalance into account.\n\n* '**weighted**':\nCalculate metrics for each label, and find their average weighted by support (the number of true instances for each label). This alters \u2018macro\u2019 to account for label imbalance; it can result in an F-score that is not between precision and recall.\n\n* '**samples**':\nCalculate metrics for each instance, and find their average (only meaningful for multilabel classification where this differs from accuracy_score).","2cf96de4":"[SciKit Learn: \nModel evaluation: quantifying the quality of predictions](https:\/\/scikit-learn.org\/stable\/model_selection.html?highlight=multilabel%20metrics#model-selection-and-evaluation)\n\n","a60d0d2d":"# PART B: ROC & AUC","ec26dc91":"![Sample Data with scores](https:\/\/github.com\/kmkarakaya\/ML_tutorials\/blob\/master\/images\/sample%20mutli%20label%20classification%20scores.png?raw=true)\n","35e3fcb7":"First assume that in the data set we have only 3 samples as below. Each sample has 4 classes: A, B, C, and D. \nIn samples more than one classes exist! Therefore, the problem multi-label classification!\n\nAfter some training, some ML model produce the predicted scores.","9b5b33c7":"<img src=\"https:\/\/github.com\/kmkarakaya\/ML_tutorials\/blob\/master\/images\/sample%20mutli%20label%20classification%20scores.png?raw=true\" alt=\"sample\" width=\"700\"\/>","2033184f":"##Recall\n[recall_score](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.recall_score.html?highlight=recall_score#sklearn-metrics-recall-score)\n\n\n* Compute the recall: The recall is the ratio **tp \/ (tp + fn)** where tp is the number of true positives and fn the number of false negatives. The recall is intuitively the ability of the classifier to find all the positive samples.\n\n* The best value is 1 and the worst value is 0.\n\n![Sample Confusion Matrix](https:\/\/github.com\/kmkarakaya\/ML_tutorials\/blob\/master\/images\/confusion%20matrix%20sample.png?raw=true)\n\n\n***average*** parameter is required for multiclass\/multilabel targets. \n\n* **None**, the scores for each class are returned. Otherwise, this determines the type of averaging performed on the data:\n\n* '**micro**':\nCalculate metrics globally by counting the total true positives, false negatives and false positives.\n\n* '**macro**':\nCalculate metrics for each label, and find their unweighted mean. This does not take label imbalance into account.\n\n* '**weighted**':\nCalculate metrics for each label, and find their average weighted by support (the number of true instances for each label). This alters \u2018macro\u2019 to account for label imbalance; it can result in an F-score that is not between precision and recall.\n\n* '**samples**':\nCalculate metrics for each instance, and find their average (only meaningful for multilabel classification where this differs from accuracy_score).","9bdcb1f1":"<img src=\"https:\/\/github.com\/kmkarakaya\/ML_tutorials\/blob\/master\/images\/ROC.png?raw=true\" alt=\"drawing\" width=\"400\"\/>","41dd2158":"## [plot_roc_curve](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.plot_roc_curve.html?highlight=roc_curve#sklearn-metrics-plot-roc-curve)\n\nNote: this implementation is restricted to the** binary classification** task.","84e2cfc8":"IMPLICATIONS:\n* The precision-recall curve shows the **tradeoff** between precision and\nrecall for different threshold. \n\n* A **high area** under the curve represents\nboth **high** **recall** and high **precision**.\n\n* A **high precision** relates to a **low false positive rate**\n\n* A **high recall** relates to a **low false negative rate**. \n\n* **High scores for both** show that the classifier is returning **accurate results** (high precision), as well as returning a **majority of all positive results** (high recall).\n\n* A system with **high recall but low precision** returns **many results**, but **most** of its predicted labels are **incorrect** when compared to the training labels. \n\n* A system with **high precision but low recall** is just the opposite, returning **very few results**, but **most** of its predicted labels are **correct** when compared to the\ntraining labels. \n\n* An **ideal system** with **high precision** and **high recall** will return **many results**, with all results labeled **correctly**.\n\n","d3e450f9":"####  [Use average_precision_score function](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.average_precision_score.html#sklearn-metrics-average-precision-score)\n\n* average_precision_score(y_true, **y_score** , average='macro', sample_weight=None)\n\n*Compute average precision (AP) from prediction scores\n\n* average_precision_score summarizes a **precision-recall curve** as the weighted mean of precisions achieved **at each threshold**, with the increase in recall from the previous threshold used as the weight:\n\n>>$\\text{AP} = \\sum_n (R_n - R_{n-1}) P_n$\n \nwhere  and  are the precision and recall at the nth threshold [1]. \n* This implementation is not interpolated and is different from computing the area under the precision-recall curve with the trapezoidal rule, which uses linear interpolation and can be too optimistic.\n\n* Note: this implementation is restricted to the binary classification task or **multilabel** classification task.\n\n**IMPORTANT:** \n* Precision refers to precision at a **particular decision threshold**. For example, if you count any model output less than 0.5 as negative, and greater than 0.5 as positive. \n* But sometimes (especially if your classes are not balanced, or if you want to favor precision over recall or vice versa), you may want to **vary this threshold**. **average_precision_score function** gives you ***average precision at all such possible thresholds***, which is also similar to the ***area under the precision-recall curve***. \n* It is a useful metric to compare how well models are ordering the predictions, without considering any specific decision threshold.\n","b9b98714":"**Average precision** (AP) summarizes such a plot as the weighted mean of\nprecisions achieved at each threshold, with the increase in recall from the\nprevious threshold used as the weight:\n\n$\\text{AP} = \\sum_n (R_n - R_{n-1}) P_n$\n\nwhere $P_n$ and $R_n$ are the precision and recall at the\nnth threshold. A pair $(R_k, P_k)$ is referred to as an\n*operating point*.\n\nAP and the trapezoidal area under the operating points\n(:func:`sklearn.metrics.auc`) are common ways to summarize a precision-recall\ncurve that lead to different results. Read more in the\n`User Guide <precision_recall_f_measure_metrics>`.\n\nPrecision-recall curves are typically used in binary classification to study\nthe output of a classifier. In order to extend the precision-recall curve and\naverage precision to multi-class or multi-label classification, it is necessary\nto binarize the output. One curve can be drawn per label, but one can also draw\na precision-recall curve by considering each element of the label indicator\nmatrix as a binary prediction (micro-averaging).\n\n<div class=\"alert alert-info\"><h4>Note<\/h4><p>See also :func:`sklearn.metrics.average_precision_score`,\n             :func:`sklearn.metrics.recall_score`,\n             :func:`sklearn.metrics.precision_score`,\n             :func:`sklearn.metrics.f1_score`<\/p><\/div>\n","a8db81e8":"### In a loop","6148799f":"##F1 score\n[f1_score](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.f1_score.html?highlight=f1_score#sklearn-metrics-f1-score)\n\n\n* Compute the F1 score, also known as balanced F-score or F-measure\n\n* The F1 score can be interpreted as a ***weighted average of the precision and recall***, where an F1 score reaches its best value at 1 and worst score at 0. \n\n* The relative contribution of precision and recall to the F1 score are equal. The formula for the F1 score is:\n\n> >>`F1 = 2 * (precision * recall) \/ (precision + recall)`\n\n\nIn the multi-class and **multi-label** case, this is the** weighted average of the F1 score of each class**.\n\n\n***average*** parameter is required for multiclass\/multilabel targets. \n\n* **None**, the scores for each class are returned. Otherwise, this determines the type of averaging performed on the data:\n\n* '**micro**':\nCalculate metrics globally by counting the total true positives, false negatives and false positives.\n\n* '**macro**':\nCalculate metrics for each label, and find their unweighted mean. This does not take label imbalance into account.\n\n* '**weighted**':\nCalculate metrics for each label, and find their average weighted by support (the number of true instances for each label). This alters \u2018macro\u2019 to account for label imbalance; it can result in an F-score that is not between precision and recall.","958bd817":"#### Calculate macro Precision \n* Calculate metrics for each label, and find their **unweighted** mean. \n* This **does not** take label **imbalance** into account.\n\nThe precision is the ratio tp \/ (tp + fp) ","294f0588":"####Precision of each label","6e43b781":"#### Calculate weighted Precision \n*  Calculate metrics for each label, and find their average **weighted by support** (the number of true instances for each label). \n*  This alters \u2018macro\u2019 to account for label **imbalance**; it can result in an *F-score that is not between precision and recall*.\n\nThe precision is the ratio tp \/ (tp + fp) \n\n","4db2583c":"Plot the Precision-Recall curve\n................................\n\n","01abd00b":"#[Precision, recall and F-measures @ Multilabel classification ](https:\/\/scikit-learn.org\/stable\/modules\/model_evaluation.html#multiclass-and-multilabel-classification)\n\n\n* In multiclass and multilabel classification task, the notions of precision, \nrecall, and F-measures can be applied to each label independently. \n\n* There are a few ways to combine results across labels, specified by the ***average*** argument to the average_precision_score (multilabel only), f1_score, fbeta_score, precision_recall_fscore_support, precision_score and recall_score functions, as described above. \n\n* Note that for ***\u201cmicro\u201d***-averaging in a multiclass setting with all labels included will produce equal precision, recall and F, while ***\u201cweighted\u201d*** averaging may produce an F-score that is not between precision and recall.","a296f6e0":"### Synthetic Data\n","e9724de9":"In binary classification settings\n--------------------------------------------------------\n\nCreate simple data\n..................\n\nTry to differentiate the two first classes of the iris data\n\n","4b55574c":"#### Use precision_score function","c022bc99":"### Average F1","b00d1950":"##### Flatten & use as in binary classification","f57c8e6b":"# PART C: Precision-Recall Curve\n\n\nPrecision-Recall is a useful measure of success of prediction when the\nclasses are very **imbalanced**. In information retrieval, precision is a\nmeasure of result relevancy, while recall is a measure of how many truly\nrelevant results are returned.\n\n","83c7a004":"### Let's create a Syntectic multi Label dataset","0d9e1b5e":"The average precision score in multi-label settings\n....................................................\n\n","2f4b9c77":"### One label at a time\n\n* We can NOT directly use roc_curve funtion to **multilabel** classification.\n\n* However, we can select 1 label at a time and calculate its roc_curve values!\n\n* Remeber we are given that:\n\n![Sample Data with scores](https:\/\/github.com\/kmkarakaya\/ML_tutorials\/blob\/master\/images\/sample%20mutli%20label%20classification%20scores.png?raw=true)\n\n\n### Classification Report when threshold = 0.5\n\n![Predictions](https:\/\/github.com\/kmkarakaya\/ML_tutorials\/blob\/master\/images\/sample%20mutli%20label%20classification%20predictionss.png?raw=true)\n\n\n### Classification Report when threshold = 0.2\n\n![alt text](https:\/\/github.com\/kmkarakaya\/ML_tutorials\/blob\/master\/images\/sample%20mutli%20label%20classification%20predictions%2002.png?raw=true)\n\n### Classification Report when threshold = 0.8\n\n![alt text](https:\/\/github.com\/kmkarakaya\/ML_tutorials\/blob\/master\/images\/sample%20mutli%20label%20classification%20predictions%2008.png?raw=true)\n\n\n### For eachthreshold we will end up with a different Confusion Matrix. For example for threshold 0.8 for Label A we can fill the following Confusion Matrix as below:\n\n![Sample Confusion Matrix](https:\/\/github.com\/kmkarakaya\/ML_tutorials\/blob\/master\/images\/confusion%20matrix%20sample.png?raw=true)\n\n\n* Let's select first label as a binary classification\n","88f9f0d4":"### Get the predictions & scores from the classifier","2a6dd134":"<img src=\"https:\/\/github.com\/kmkarakaya\/ML_tutorials\/blob\/master\/images\/ROC.png?raw=true\" alt=\"drawing\" width=\"400\"\/>\n","39950259":"## Summary\n\n* **Remember**: We assumed that we are using **0.5 as the threshold** for prediction during all the computations above.\n\n* What hapens if we select **different** values as the threshold?\n\nLet's see!","66113b6b":"# [Confusion matrix](https:\/\/scikit-learn.org\/stable\/modules\/model_evaluation.html#confusion-matrix) \n\nThe confusion_matrix function evaluates classification accuracy by computing the confusion matrix.\n\nBy definition, entry i, j in a confusion matrix is the number of observations actually in group i, but predicted to be in group j. \n\n![Sample Confusion Matrix](https:\/\/github.com\/kmkarakaya\/ML_tutorials\/blob\/master\/images\/confusion%20matrix%20sample.png?raw=true)\n","9bd7b796":"## [roc_auc_score](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.roc_auc_score.html#sklearn-metrics-roc-auc-score)\n\nCompute Area Under the Receiver Operating Characteristic Curve (ROC AUC) from prediction scores.\n\nNote: **this implementation can be used** with binary, multiclass and **multilabel classification**, but some restrictions apply (see Parameters).\n\nroc_auc_score(y_true, y_score, average='macro', sample_weight=None, max_fpr=None, multi_class='raise', labels=None)\n\nParameters:\n\n* **y_true** array-like of shape (n_samples,) or (n_samples, n_classes)\nTrue labels or binary label indicators. The binary and multiclass cases expect labels with shape (n_samples,) while the **multilabel case expects binary label indicators with shape (n_samples, n_classes)**.\n\n* **y_score**: array-like of shape (n_samples,) or (n_samples, n_classes)\nTarget scores. In the binary and multilabel cases, these can be either probability estimates or non-thresholded decision values (as returned by decision_function on some classifiers). **In the multiclass** case, these must be **probability estimates which sum to 1**. The binary case expects a shape (n_samples,), and the scores must be the scores of the class with the greater label. The multiclass and **multilabel cases expect a shape (n_samples, n_classes). In the multiclass case, the order of the class scores must correspond to the order of labels, if provided, or else to the numerical or lexicographical order of the labels in y_true.**\n\n* **average**: {\u2018micro\u2019, \u2018macro\u2019, \u2018samples\u2019, \u2018weighted\u2019} or None, default=\u2019macro\u2019","893e40d5":"### F1 of each label","9442c422":"![Predictions](https:\/\/github.com\/kmkarakaya\/ML_tutorials\/blob\/master\/images\/sample%20mutli%20label%20classification%20predictionss.png?raw=true)","80b62cb8":"### Classification Report when threshold = 0.2\n\n![alt text](https:\/\/github.com\/kmkarakaya\/ML_tutorials\/blob\/master\/images\/sample%20mutli%20label%20classification%20predictions%2002.png?raw=true)","50a28ba4":"#### Finding TPR, FPR","d64d05f0":"# # Multi Label Model Evaluation","25d1aa9e":"#[Classification report](https:\/\/scikit-learn.org\/stable\/modules\/model_evaluation.html#classification-report)","7e4105ca":"### Classification Report when threshold = 0.5\n\n![Predictions](https:\/\/github.com\/kmkarakaya\/ML_tutorials\/blob\/master\/images\/sample%20mutli%20label%20classification%20predictionss.png?raw=true)","e953f6f9":"<a href=\"https:\/\/www.youtube.com\/watch?v=ioYLHUH_ZuI\" target=\"_parent\"><img src=\"https:\/\/github.com\/kmkarakaya\/ML_tutorials\/blob\/master\/images\/watch-on-youtube.png?raw=true\" alt=\"Watch on YouTube\"\/><\/a>\n\n\n\n\n([You can watch this notebook (in Turkish) on YouTube!](https:\/\/www.youtube.com\/watch?v=ioYLHUH_ZuI))\n\n","2345db07":"#Threshold: Let's assume we are using **0.5 as the threshold** for prediction\n","c4c81806":"# [Receiver operating characteristic (ROC) ](https:\/\/scikit-learn.org\/stable\/modules\/model_evaluation.html#receiver-operating-characteristic-roc)\n\nThe function **roc_curve** computes the **receiver operating characteristic** curve, or ROC curve. \n\nQuoting Wikipedia :\n* A receiver operating characteristic (ROC), or simply ROC curve, is a graphical plot which illustrates the ***performance of a binary classifier*** system as its discrimination **threshold is varied**. \n\n![Sample Confusion Matrix](https:\/\/github.com\/kmkarakaya\/ML_tutorials\/blob\/master\/images\/confusion%20matrix%20sample.png?raw=true)\n\n\n* It is created by plotting the fraction of ***true positives out of the positives*** (TPR = true positive rate) vs. the fraction of ***false positives out of the negatives*** (FPR = false positive rate), at **various threshold settings**. \n\n<img src=\"https:\/\/github.com\/kmkarakaya\/ML_tutorials\/blob\/master\/images\/ROC.png?raw=true\" alt=\"drawing\" width=\"500\"\/>\n\n\n* TPR is also known as **sensitivity**, and FPR is one minus the **specificity** or true negative rate.\u201d\n\n\n* The **roc_auc_score function** computes the ***area*** under the receiver operating characteristic (ROC) curve, which is also denoted by AUC or AUROC. By computing the area under the roc curve, **the curve information is summarized in one number**. \n\n* In **multi-label** classification, the roc_auc_score function is extended by ***averaging over the labels*** as above.\n\n\n","169081a5":"### One Label at a time: A simple work around","e66ae676":"INSIGHT:\n* Note that the **precision** **may not** **decrease** with **recall**. \n* The definition of **precision** ($\\frac{T_p}{T_p + F_p}$) shows that **lowering** the **threshold** of a classifier **may increase** the denominator, by increasing the\nnumber of results returned. \n\n* If the threshold was **previously set too high**, the\nnew results may all be true positives, which will **increase precision**. \n\n* If the previous threshold was **about right or too low**, further lowering the threshold will introduce false positives, **decreasing precision**.\n\n* Recall is defined as $\\frac{T_p}{T_p+F_n}$, where $T_p+F_n$ **does not depend** on the classifier **threshold**. \n\n* This means that **lowering** the classifier threshold **may increase recall**, by increasing the number of true positive\nresults. \n\n* It is also possible that **lowering** the threshold may leave recall **unchanged**, while the **precision fluctuates**.\n\n* The relationship between recall and precision can be observed in the **stairstep** area of the plot - **at the edges** of these steps a **small change** in the threshold considerably **reduces precision**, with only a **minor gain in recall**.\n\n","21f6c7b2":"#[Precision, recall and F-measures](https:\/\/scikit-learn.org\/stable\/modules\/model_evaluation.html#precision-recall-and-f-measures)\n\n\nIntuitively, precision is the ability of the classifier not to label as positive a sample that is negative, and recall is the ability of the classifier to find all the positive samples.\n\nThe F-measure can be interpreted as a weighted harmonic mean of the precision and recall:  reaches its best value at 1 and its worst score at 0.\n\nSeveral functions allow you to analyze the precision, recall and F-measures score.","c0f8c45c":"##### Count by yourself\n\n![Predictions](https:\/\/github.com\/kmkarakaya\/ML_tutorials\/blob\/master\/images\/sample%20mutli%20label%20classification%20predictionss.png?raw=true)","a28de561":"### Train a OneVsRestClassifier ","a340815f":"DEFINITIONS\n\n* **Precision** **($P$)** is defined as the number of true positives ($T_p$) over the number of true positives plus the number of false positives ($F_p$)\n$P = \\frac{T_p}{T_p+F_p}$\n\n* **Recall ($R$)** is defined as the number of true positives ($T_p$) over the number of true positives plus the number of false negatives ($F_n$).\n$R = \\frac{T_p}{T_p + F_n}$\n\n* **($F_1$) score** These quantities are also related to the **($F_1$) score**, which is defined as the harmonic mean of precision and recall.\n$F1 = 2\\frac{P \\times R}{P+R}$\n\n","5255d194":"#[Multilabel confusion matrix](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.multilabel_confusion_matrix.html?highlight=multilabel#sklearn-metrics-multilabel-confusion-matrix)\n\n\n* Compute a confusion matrix for each class or sample \n* **NOTICE:** The cells has different meaning from Binary Class Confusion Matrix! \n\n![MCM](https:\/\/github.com\/kmkarakaya\/ML_tutorials\/blob\/master\/images\/MCM%20confusion%20matrix%20sample.png?raw=true)","206a4763":"# **PART A: Basic Metrics**\n\n\n\n","09336302":"<a href=\"https:\/\/colab.research.google.com\/github\/kmkarakaya\/ML_tutorials\/blob\/master\/Multi_Label_Model_Evaulation.ipynb\" target=\"_parent\"><img src=\"https:\/\/colab.research.google.com\/assets\/colab-badge.svg\" alt=\"Open In Colab\"\/><\/a>","a611a6ab":"#### Calculate Samples Precision \n\n* Calculate metrics for each instance, and find their average (only meaningful for multilabel classification where this differs from accuracy_score).\n\n![Predictions](https:\/\/github.com\/kmkarakaya\/ML_tutorials\/blob\/master\/images\/sample%20mutli%20label%20classification%20predictionss.png?raw=true)\n","6cc00ef8":"### Average Recall","c9394e58":"#### Calculate micro Precision \n* Calculate metrics **globally** by counting the total true positives and false positives.\n\nThe precision is the ratio tp \/ (tp + fp) \n","ed1f09a0":"![Predictions](https:\/\/github.com\/kmkarakaya\/ML_tutorials\/blob\/master\/images\/sample%20mutli%20label%20classification%20predictionss.png?raw=true)","98e683b7":"[Definitions:](https:\/\/scikit-learn.org\/stable\/modules\/multiclass.html#multiclass-and-multilabel-algorithms)\n\n* **Multiclass classification:** classification task with more than two classes. ***Each sample can only be labelled as one class***.\n\n* For example, classification using features extracted from a set of images of fruit, ***where each image may either be of an orange, an apple, or a pear***. Each image is one sample and is labelled as ***one of the 3 possible classes***. Multiclass classification makes the assumption that each sample is assigned to **one and only one label** - one sample cannot, for example, be both a pear and an apple.\n\n\n* **Multilabel classification**: classification task labelling each sample with x labels from n_classes possible classes, where x can be **0 to n_classes** **inclusive**. This can be thought of as predicting properties of a sample that are ***not mutually exclusive***. Formally, a binary output is assigned to each class, for every sample. Positive classes are indicated with 1 and negative classes with 0 or -1. It is thus comparable to running n_classes binary classification tasks, for example with sklearn.multioutput.MultiOutputClassifier. This approach treats each label independently whereas multilabel classifiers may treat the multiple classes simultaneously, accounting for correlated behaviour amoung them.\n\n* For example, prediction of the topics relevant to a text document or video. The document or video **may be about one of** \u2018religion\u2019, \u2018politics\u2019, \u2018finance\u2019 or \u2018education\u2019, several of the topic classes or all of the topic classes.\n\n**Difference between multi-class classification & multi-label classification** is that in multi-class problems the classes are mutually exclusive, whereas for multi-label problems each label represents a different classification task, but the tasks are somehow related.\n","bb07ea1f":"Compute the average precision score\n...................................\n\n","a2949e96":"## [roc_curve](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.roc_curve.html?highlight=roc_curve#sklearn-metrics-roc-curve)\n\nCompute Receiver operating characteristic (ROC)\n\n* Note: this implementation is restricted to the **binary classification** task.\n\n\nReturns:\n\n>* **fpr**: Increasing false positive rates such that element i is the false positive rate of predictions with score >= thresholds[i].\n\n>* **tpr**: Increasing true positive rates such that element i is the true positive rate of predictions with score >= thresholds[i].\n\n>* **thresholds**: Decreasing thresholds on the decision function used to compute fpr and tpr. thresholds[0] represents no instances being predicted and is arbitrarily set to max(y_score) + 1.","cdec2eb6":"<a href=\"https:\/\/www.youtube.com\/watch?v=KIQ_27Y0I1c\" target=\"_parent\"><img src=\"https:\/\/github.com\/kmkarakaya\/ML_tutorials\/blob\/master\/images\/watch-on-youtube.png?raw=true\" alt=\"Watch on YouTube\"\/><\/a>\n\n\n\n\n([You can watch this notebook (in Turkish) on YouTube!](https:\/\/www.youtube.com\/watch?v=KIQ_27Y0I1c))","565dde24":"# [Accuracy score](https:\/\/scikit-learn.org\/stable\/modules\/model_evaluation.html#accuracy-score)\n\nThe accuracy_score function computes the accuracy, either the fraction (default) or the count (normalize=False) of correct predictions.\n\n**In multilabel classification**, the function returns the subset accuracy. If the entire set of predicted labels for a sample strictly match with the true set of labels, then the subset accuracy is 1.0; otherwise it is **0.0**.\n"}}