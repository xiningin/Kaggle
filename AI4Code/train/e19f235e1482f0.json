{"cell_type":{"84baf4a8":"code","0d4ccd68":"code","24f66d64":"code","04c08ae7":"code","9c95d88e":"code","69440613":"code","05752ea6":"code","2260624e":"code","6ee8621d":"code","9e61c826":"code","f4a86159":"code","de71fd53":"code","717db4a7":"code","f94e9fdf":"code","0db43796":"code","86336e79":"markdown","c36c278e":"markdown","885bf3a4":"markdown","2361c4f3":"markdown","2e3724ec":"markdown","f80b8cba":"markdown","b751e35d":"markdown","60b85ee2":"markdown","88d95b74":"markdown","0c876c4b":"markdown","feb17d61":"markdown","dbefe0ba":"markdown","b9e5cffc":"markdown","d5f07e75":"markdown"},"source":{"84baf4a8":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.layers import Dense, Activation,Dropout,Conv2D, MaxPooling2D,BatchNormalization\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras import regularizers\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nimport numpy as np\nimport pandas as pd\nimport time\nimport glob\nfrom skimage import io\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nfrom matplotlib.pyplot import imshow\nimport scipy\nfrom scipy.spatial import distance\nimport os\nimport cv2\nimport shutil\nfrom sklearn.metrics import confusion_matrix, classification_report","0d4ccd68":"mask_train_dir='..\/input\/face-mask-12k-images-dataset\/Face Mask Dataset\/Train\/WithMask'\nno_mask_train_dir='..\/input\/face-mask-12k-images-dataset\/Face Mask Dataset\/Train\/WithoutMask'\nmask_valid_dir='..\/input\/face-mask-12k-images-dataset\/Face Mask Dataset\/Validation\/WithMask'\nno_mask_valid_dir='..\/input\/face-mask-12k-images-dataset\/Face Mask Dataset\/Validation\/WithoutMask'\nmask_test_dir='..\/input\/face-mask-12k-images-dataset\/Face Mask Dataset\/Test\/WithMask'\nno_mask_test_dir='..\/input\/face-mask-12k-images-dataset\/Face Mask Dataset\/Test\/WithoutMask'\ndirlist=[mask_train_dir, no_mask_train_dir, mask_valid_dir, no_mask_valid_dir, mask_test_dir, no_mask_test_dir]\nclasses=['mask', 'no_mask', 'mask', 'no_mask', 'mask', 'no_mask']\nfilepaths=[]\nlabels=[]\nfor d,c in zip(dirlist, classes):\n    flist=os.listdir(d)\n    for f in flist:\n        fpath=os.path.join (d,f)\n        filepaths.append(fpath)\n        labels.append(c)\nprint ('filepaths: ', len(filepaths), '   labels: ', len(labels))","24f66d64":"Fseries=pd.Series(filepaths, name='file_paths')\nLseries=pd.Series(labels, name='labels')\ndf=pd.concat([Fseries,Lseries], axis=1)\ndf=pd.DataFrame(np.array(df).reshape(11792,2), columns = ['file_paths', 'labels'])\nprint(df['labels'].value_counts())","04c08ae7":"plt.figure(figsize=(14,10))\nfor i in range(15):\n    random = np.random.randint(1,len(df))\n    plt.subplot(3,5,i+1)\n    plt.imshow(cv2.imread(df.loc[random,\"file_paths\"]))\n    plt.title(df.loc[random, \"labels\"], size = 10, color = \"black\") \n    plt.xticks([])\n    plt.yticks([])\n    \nplt.show()","9c95d88e":"target_size=(299,299)\nbatch_size=64","69440613":"train_df, test_df=train_test_split(df, train_size=0.95, shuffle=True)\ntrain_df, valid_df=train_test_split(train_df, train_size=0.9, shuffle=True)","05752ea6":"print(train_df.labels.value_counts())\nprint(valid_df.labels.value_counts())\nprint(test_df.labels.value_counts())","2260624e":"train_datagen = ImageDataGenerator(preprocessing_function=tf.keras.applications.inception_resnet_v2.preprocess_input, zoom_range=0.2, rotation_range=40)\ntest_datagen = ImageDataGenerator(preprocessing_function=tf.keras.applications.inception_resnet_v2.preprocess_input)\ntrain_gen = train_datagen.flow_from_dataframe(train_df, x_col='file_paths', y_col='labels', target_size=target_size, batch_size=batch_size, color_mode='rgb', class_mode='categorical')\nvalid_gen = test_datagen.flow_from_dataframe(valid_df, x_col='file_paths', y_col='labels', target_size=target_size, batch_size=batch_size, color_mode='rgb', class_mode='categorical')\ntest_gen = test_datagen.flow_from_dataframe(test_df, x_col='file_paths', y_col='labels', target_size=target_size, batch_size=batch_size, color_mode='rgb', class_mode='categorical')","6ee8621d":"base_model = tf.keras.applications.InceptionResNetV2(include_top=False, input_shape=(299,299,3))\nbase_model.summary()","9e61c826":"model = tf.keras.Sequential([\n    base_model, tf.keras.layers.GlobalAveragePooling2D(), tf.keras.layers.Dense(128, activation='relu'), tf.keras.layers.BatchNormalization(), tf.keras.layers.Dropout(0.2), tf.keras.layers.Dense(2, activation='softmax')\n])\nlr=0.001\nmodel.compile(loss='categorical_crossentropy', optimizer=Adam(lr=lr), metrics=['accuracy'])","f4a86159":"patience = 1\nstop_patience = 3\nfactor = 0.5\n\ncallbacks = [\n    tf.keras.callbacks.ModelCheckpoint(\"classify_model.h5\", save_best_only=True, verbose = 0),\n    tf.keras.callbacks.EarlyStopping(patience=stop_patience, monitor='val_loss', verbose=1),\n    tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=factor, patience=patience, verbose=1)\n]","de71fd53":"epochs = 15\nhistory = model.fit(train_gen, validation_data=valid_gen, epochs=epochs, callbacks=callbacks, verbose=1)","717db4a7":"best_model = model","f94e9fdf":"best_model.load_weights('.\/classify_model.h5')","0db43796":"best_model.evaluate(test_gen)","86336e79":"# **Callbacks**","c36c278e":"# **Load Best Model**","885bf3a4":"**Create Image Generators**","2361c4f3":"# **Image Preprocessing**","2e3724ec":"# **Create Dataframe from Images**","f80b8cba":"# **Image Examples**","b751e35d":"**Dataset is mostly balanced**","60b85ee2":"**Set variables for Image Generation**","88d95b74":"**Split Dataframe into Train and Test Dataframes, then split the Train Dataframe into Train and Validation Dataframes**","0c876c4b":"# **Training Model**","feb17d61":"# **Import Libraries**","dbefe0ba":"# **Predictions on Test Set**","b9e5cffc":"**Looks like we can zoom a little and rotate images some.**","d5f07e75":"# **Model**"}}