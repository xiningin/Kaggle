{"cell_type":{"1302f550":"code","798b3a36":"code","3d6ff4c9":"code","f427e68a":"code","348e539f":"code","ccd5108a":"code","b59d6ca6":"code","7b84a4da":"code","81bad697":"code","517f1305":"code","be306e4b":"code","9cb88019":"code","dcf2cb0d":"code","e3d75108":"code","e7bc49f1":"code","672c7d00":"code","211c2b80":"code","35efef55":"code","84b68a8a":"code","33bb6bd0":"code","bc23fab7":"code","5a2fbb6e":"code","5d6764c7":"code","89a326e6":"code","7caaf626":"code","90b5ad33":"code","4bfd6f99":"code","066f4f50":"code","11222bfb":"code","8a9833ef":"code","2ca2e062":"code","75022492":"code","2d222cf9":"code","0feffcf3":"code","15ac871c":"code","b147744c":"code","77b421bd":"code","16246854":"code","c0193ed5":"code","c1f234ae":"code","f51d7954":"code","026dc59c":"code","f760dad8":"markdown","4543b951":"markdown","323c2804":"markdown","302ddc2f":"markdown","512739fa":"markdown","3896d99c":"markdown","928f4cd5":"markdown","29719f4d":"markdown","47183e06":"markdown","f58e1914":"markdown","6e069e26":"markdown","9caa887a":"markdown","2f710c86":"markdown","b92f9eb1":"markdown","cdcedbd3":"markdown","2edacce6":"markdown","008fd893":"markdown","1cdd712f":"markdown","9a678670":"markdown","1e282bea":"markdown"},"source":{"1302f550":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib import style\nfrom pandas.io.common import EmptyDataError\nplt.rcParams['figure.figsize'] = (10, 6)\nstyle.use('ggplot')\n\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.model_selection import cross_validate, cross_val_score, train_test_split, KFold, StratifiedKFold\nfrom scipy.stats import skew, kurtosis\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.base import clone, BaseEstimator, TransformerMixin\n\nimport os\nfrom functools import partial\nimport re\nfrom collections import Counter\n\nimport warnings\nwarnings.filterwarnings('ignore')","798b3a36":"import umap","3d6ff4c9":"user_root = \"..\/input\/Archived-users\/Archived users\/\"\nuser_fn_list = os.listdir(user_root)","f427e68a":"def read_one_file(fn, root):\n    out = dict()\n    with open(root + fn) as f:\n        for line in f.readlines():\n            k, v = line.split(\": \")\n            out[k] = v.strip()\n            out['ID'] = re.findall(r'_(\\w+)\\.', fn)[0]\n    return out","348e539f":"users_list = list(map(partial(read_one_file, root=user_root), user_fn_list))","ccd5108a":"users = pd.DataFrame(users_list)\nusers.replace('------', np.nan, inplace=True)\nusers.replace('', np.nan, inplace=True)\nusers['Levadopa'] = users['Levadopa'] == 'True'\nusers['MAOB'] = users['MAOB'] == 'True'\nusers['Parkinsons'] = users['Parkinsons'] == 'True'\nusers['Tremors'] = users['Tremors'] == 'True'\nusers['Other'] = users['Other'] == 'True'","b59d6ca6":"users.head()","7b84a4da":"keys_root = \"..\/input\/Archived-Data\/Tappy Data\/\"\nkeys_fn_list = os.listdir(keys_root)","81bad697":"sample = pd.read_csv(keys_root + keys_fn_list[0], delimiter='\\t', header=None, usecols=range(8))\nsample.columns = ['ID', 'Date', 'TS', 'Hand', 'HoldTime', 'Direction', 'LatencyTime', 'FlightTime']\nsample.head()","517f1305":"def read_one_key_file(fn, root):\n    try:\n        df = pd.read_csv(root + fn, delimiter='\\t', header=None, error_bad_lines=False,\n                         usecols=range(8), low_memory=False,\n                        dtype={0:'str', 1:'str', 2:'str', 3:'str', 4:'float', 5:'str', 6:'float', 7:'float'})\n        df.columns = ['ID', 'Date', 'TS', 'Hand', 'HoldTime', 'Direction', 'LatencyTime', 'FlightTime']\n    except ValueError:\n        # should try to remove the bad lines and return\n#         df = pd.DataFrame(columns = ['ID', 'Date', 'TS', 'Hand', 'HoldTime', 'Direction', 'LatencyTime', 'FlightTime'])\n        try:\n            df = pd.read_csv(root + fn, delimiter='\\t', header=None, error_bad_lines=False,\n                             usecols=range(8), low_memory=False)\n            df.columns = ['ID', 'Date', 'TS', 'Hand', 'HoldTime', 'Direction', 'LatencyTime', 'FlightTime']\n            df = df[df['ID'].apply(lambda x: len(str(x)) == 10)\n                   & df['Date'].apply(lambda x: len(str(x)) == 6)\n                   & df['TS'].apply(lambda x: len(str(x)) == 12)\n                   & np.in1d(df['Hand'], [\"L\", \"R\", \"S\"])\n                   & df['HoldTime'].apply(lambda x: re.search(r\"[^\\d.]\", str(x)) is None)\n                   & np.in1d(df['Direction'], ['LL', 'LR', 'RL', 'RR', 'LS', 'SL', 'RS', 'SR', 'RR'])\n                   & df['LatencyTime'].apply(lambda x: re.search(r\"[^\\d.]\", str(x)) is None)\n                   & df['FlightTime'].apply(lambda x: re.search(r\"[^\\d.]\", str(x)) is None)]\n            df['HoldTime'] = df['HoldTime'].astype(np.float)\n            df['LatencyTime'] = df['HoldTime'].astype(np.float)\n            df['FlightTime'] = df['HoldTime'].astype(np.float)\n        except EmptyDataError:\n            df =  pd.DataFrame(columns = ['ID', 'Date', 'TS', 'Hand', 'HoldTime', 'Direction', 'LatencyTime', 'FlightTime'])\n    except EmptyDataError:\n        df =  pd.DataFrame(columns = ['ID', 'Date', 'TS', 'Hand', 'HoldTime', 'Direction', 'LatencyTime', 'FlightTime'])\n    return df","be306e4b":"keys_list = list(map(partial(read_one_key_file, root=keys_root), keys_fn_list))","9cb88019":"keys = pd.concat(keys_list, ignore_index=True, axis=0)","dcf2cb0d":"keys.head()","e3d75108":"keys.shape","e7bc49f1":"key_id_set = set(keys['ID'].unique())\nprint(\"total user ids in key logs: {0}\".format(len(key_id_set)))\nuser_id_set = set(users['ID'].unique())\nprint(\"total user ids in user info: {0}\".format(len(user_id_set)))\noverlap_id_set = key_id_set.intersection(user_id_set)\nprint(\"overlapping ids: {0}\".format(len(overlap_id_set)))\ndiff_id_set = key_id_set.symmetric_difference(user_id_set)\nprint(\"non-matching ids: {0}\".format(len(diff_id_set)))","672c7d00":"sufficient_keys = keys.groupby('ID').size() >= 2000\nuser_w_sufficient_data = set(sufficient_keys.index[sufficient_keys])\nuser_eligible = set(users[((users['Parkinsons']) & (users['Impact'] == 'Mild') \n                       | (~users['Parkinsons']))\n                      & (~users['Levadopa'])]['ID'])\nuser_valid = user_w_sufficient_data.intersection(user_eligible)","211c2b80":"len(user_valid)","35efef55":"users.query('ID in @user_valid').groupby('Parkinsons').size()","84b68a8a":"# valid_keys = keys[(keys['HoldTime'] > 0)\n#                    & (keys['LatencyTime'] > 0)\n#                    & (keys['HoldTime'] < 2000)\n#                    & (keys['LatencyTime'] < 2000)\n#                    & np.isin(keys['ID'], list(user_valid))]\n\nvalid_keys = keys[np.isin(keys['ID'], list(user_valid))]\n\nvalid_keys.shape","33bb6bd0":"hold_by_user =  valid_keys[valid_keys['Hand'] != 'S'].groupby(['ID', 'Hand'])['HoldTime'].agg([np.mean, np.std, skew, kurtosis])","bc23fab7":"hold_by_user.head(10)","5a2fbb6e":"latency_by_user = valid_keys[np.isin(valid_keys['Direction'], ['LL', 'LR', 'RL', 'RR'])].groupby(['ID', 'Direction'])['LatencyTime'].agg([np.mean, np.std, skew, kurtosis])","5d6764c7":"latency_by_user.head(10)","89a326e6":"hold_by_user_flat = hold_by_user.unstack()\nhold_by_user_flat.columns = ['_'.join(col).strip() for col in hold_by_user_flat.columns.values]\nhold_by_user_flat['mean_hold_diff'] = hold_by_user_flat['mean_L'] - hold_by_user_flat['mean_R']\nhold_by_user_flat.head()","7caaf626":"latency_by_user_flat = latency_by_user.unstack()\nlatency_by_user_flat.columns = ['_'.join(col).strip() for col in latency_by_user_flat.columns.values]\nlatency_by_user_flat['mean_LR_RL_diff'] = latency_by_user_flat['mean_LR'] - latency_by_user_flat['mean_RL']\nlatency_by_user_flat['mean_LL_RR_diff'] = latency_by_user_flat['mean_LL'] - latency_by_user_flat['mean_RR']\nlatency_by_user_flat.head()","90b5ad33":"combined = pd.concat([hold_by_user_flat, latency_by_user_flat], axis=1)","4bfd6f99":"combined.shape","066f4f50":"combined.head()","11222bfb":"full_set = pd.merge(combined.reset_index(), users[['ID', 'Parkinsons']], on='ID')\nfull_set.set_index('ID', inplace=True)\n# full_set.dropna(inplace=True)  # should investigate why there are NAs despite choosing sequence length >= 2000\nfull_set.shape","8a9833ef":"full_set.head()","2ca2e062":"umapModel = umap.UMAP()\nembed = umapModel.fit_transform(full_set.iloc[:, :-1], full_set.iloc[:, -1])","75022492":"notPak = full_set.Parkinsons.map(lambda x: not x)\nplt.scatter(embed[full_set.Parkinsons, 0], embed[full_set.Parkinsons, 1], color=\"red\")\nplt.scatter(embed[notPak, 0], embed[notPak, 1], color=\"blue\")","2d222cf9":"import seaborn as sns\nplt.figure(figsize=(10, 18))\n\nfor i, c in enumerate(full_set.columns[:-1]):\n    plt.subplot(9, 3, i + 1)\n    sns.distplot(full_set[c][full_set.Parkinsons], color=\"red\", label=\"Patients\")\n    sns.distplot(full_set[c][notPak], color=\"blue\", label=\"not Patients\")","0feffcf3":"full_set.shape, embed.shape, train_X.shape","15ac871c":"from sklearn.svm import LinearSVC\nmodel = LinearSVC()\nrs = 42\n\ntrain_X = np.concatenate([full_set.iloc[:, :-1], pd.DataFrame(embed)], axis=1)\ntrain_y = full_set.iloc[:, -1]\n\nscoring = ['accuracy', 'f1', 'roc_auc']\nscores = cross_validate(\n    model,\n    train_X,\n    train_y,\n    cv=StratifiedKFold(n_splits=5, random_state=rs),\n    scoring=scoring,\n    return_train_score=True\n)","b147744c":"result = pd.DataFrame(scores)\nresult.index.name = \"fold\"\nresult","77b421bd":"# class SubsetTransformer(TransformerMixin):\n#     def __init__(self, start=0, end=None):\n#         self.start = start\n#         self.end = end\n        \n#     def fit(self, *_):\n#         return self\n    \n#     def transform(self, X, *_):\n#         if self.end is None:\n#             return X.iloc[:, self.start:]\n#         else:\n#             return X.iloc[:, self.start: self.end]","16246854":"# rs = 61","c0193ed5":"# select_left = SubsetTransformer(0, 9)\n# select_right = SubsetTransformer(9, -1)\n# scale = StandardScaler()\n# lda = LinearDiscriminantAnalysis()\n# gb = GradientBoostingClassifier(max_depth=7, n_estimators=127, random_state=rs)\n# # ensemble here\n# pl_left = Pipeline([('select_left', select_left), \n#                     ('normalise', scale), \n#                     ('LDA', lda), \n#                     ('classify', gb)])\n# pl_right = Pipeline([('select_left', select_right), \n#                     ('normalise', clone(scale)), \n#                     ('LDA', clone(lda)), \n#                     ('classify', clone(gb))])\n# vote = VotingClassifier([('left', pl_left), ('right', pl_right)], weights=[1, 1.2], voting=\"soft\")","c1f234ae":"# train_X, test_X, train_y, test_y = train_test_split(full_set.iloc[:, :-1], full_set.iloc[:, -1], test_size=0.35, stratify=full_set.iloc[:, -1], random_state=rs)\n\n# scoring = ['accuracy', 'precision', 'recall', 'f1']\n# scores = cross_validate(vote, train_X, train_y, cv=StratifiedKFold(n_splits=10, random_state=rs), scoring=scoring, return_train_score=True)","f51d7954":"# vote.fit(train_X, train_y)\n# (vote.predict(test_X) == test_y).sum(), test_y.shape[0]","026dc59c":"# pd.DataFrame(scores).mean()","f760dad8":"We now combine the hold time data and latency data together into the final dataset for machine learning.","4543b951":"We now try to produce the subset of user and keystroke data according to the orignal research's selection criteria.\n\n> \u2022 Those with at least 2000 keystrokes \n\n> \u2022 Of the ones with PD, just the ones with \u2018Mild\u2019 severity (since the study was into the detection\n> of PD at its early stage, not later stages)\n\n> \u2022 Those not taking levodopa (Sinemet1 and the like), in order to prevent any effect of that\n> medication on their keystroke characteristics.","323c2804":"There are a total of 9276350 valid logged keystrokes.","302ddc2f":"We now move on to key logging data:","512739fa":"## A simple model will work, e.g. SVM","3896d99c":"## Use UMAP to visualize the data","928f4cd5":"We have 55 non-PD and 32 PD participants here.","29719f4d":"<del>As we see here, we are not able to get close to the cross validation performance recorded in the original research.<\/del>","47183e06":"We remove keystrokes with negative hold\/latency times (error) and with very long hold\/latency times (more likely to be deliberate). This is not mentioned in the original paper but seems to improve later performance.","f58e1914":"<del>Now that we have the full dataset, we may start replicating the machine learning pipeline in the original paper. We start with a model with the same data processing pipeline but without the ensemble of multiple different models.<\/del>","6e069e26":"There are some broken lines in the CSVs here, therefore we have to define a custom read function that attempts to throw out ill-formed lines when necessary. We match the dataframe rows with the standard format for each column and remove the rows that fail to match in the final output.","9caa887a":"## Load Data","2f710c86":"## Load all packages","b92f9eb1":"<big>\n    **Conclusion: It seems that the dataset is completely separable!**\n<\/big>","cdcedbd3":"Let us see if the keylogs data and user data match:","2edacce6":"We define a function to read all user files:","008fd893":"In the original research, there were 53 participants. However, using the same criteria, we found 87 valid participants. It appears that the data we have do not match that used in the original research.","1cdd712f":"We then read all user files and convert them to appropriate formats:","9a678670":"The key logging data are in CSV format, as shown below. We may read all of them with Pandas and combine the dataframes.","1e282bea":"We now calculate the mean, standard deviation, skewness and kurtosis for the following fields:\n\n1. L\/R hand hold time\n2. LL\/LR\/RL\/RR transition latency\n\nWe also calculate the mean difference between L\/R hold time, LR\/RL latency and LL\/RR latency."}}