{"cell_type":{"cba73838":"code","b91bbfc8":"code","27e7670a":"code","f5dfe710":"code","7b414ccb":"code","09ac3f87":"code","c03775ff":"code","37dccb0c":"code","e2de12b6":"code","e2b47ff4":"code","8dba43f6":"code","ea193766":"code","f80b7081":"code","3005c9e4":"code","a99ed9f7":"code","debbdef4":"code","6e8187ae":"code","885c042e":"code","47670d9e":"code","992e79af":"code","71f199fd":"code","684fb1fe":"code","b8c51e1e":"code","f7651273":"code","7b04c2f0":"code","f0cdb542":"code","106ff3fe":"code","c07b50cd":"code","cd5616d6":"code","69cf4b56":"code","b67f27a3":"code","60e39bb2":"code","3413bbf4":"code","3125fa48":"code","b693c673":"code","4a2038d6":"code","e891779b":"code","deb2d071":"code","6028f69a":"code","a62a7608":"code","da8c3e2b":"code","a4082fb1":"code","f50f22c2":"code","4abaeeb4":"code","d415a497":"code","a18fb059":"code","bf501921":"code","9004331b":"code","15d06e2b":"code","e98dbb14":"code","e8b70950":"code","09cf91fa":"code","3f16053e":"code","f0199cdd":"code","85330f72":"code","4f9695ad":"code","ef441e4d":"code","9576a506":"code","d3f31628":"code","86ab6f3b":"code","fb3be817":"code","7212b350":"markdown","631c1ddc":"markdown","e95211e2":"markdown","84369be1":"markdown","635fcd3b":"markdown","a070c4c2":"markdown","71431fbd":"markdown","e3284223":"markdown","e706750f":"markdown","1ff40a31":"markdown"},"source":{"cba73838":"import numpy as np\nimport pandas as pd\nimport seaborn as sb\nimport matplotlib.pyplot as plt\nsb.set_style('darkgrid')\nimport warnings\nwarnings.filterwarnings('ignore')\n%matplotlib inline\n\nfrom sklearn.metrics import mean_squared_error","b91bbfc8":"train_data = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest_data = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')\nsample_submission = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv')","27e7670a":"train_data.head()","f5dfe710":"test_data.info(),train_data.info()","7b414ccb":"plt.figure(figsize=(18,8))\nsb.heatmap(train_data.isnull(),cmap=\"YlGnBu\",yticklabels=False,cbar=False)","09ac3f87":"def Find_missing_data (Data) :\n    \n    total = Data.isnull().sum().sort_values(ascending=False)\n    percent = (Data.isnull().sum()\/Data.isnull().count()).sort_values(ascending=False)\n    missing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n    \n    return missing_data","c03775ff":"missing_train = Find_missing_data(train_data)\nmissing_train.head(20)","37dccb0c":"missing_test = Find_missing_data(test_data)\nmissing_test.head(20)","e2de12b6":"def find_missed_data_to_limit (Data,drop_limit_percentage) :\n    \n    drop_limit = int((Data.count().max()) * drop_limit_percentage)\n    missed = Data.isnull().sum()\n    missed = missed[missed > 0]\n    missed_fill = missed[(missed <= drop_limit) & (missed > 0)]\n    missed_drop = missed[missed > drop_limit]\n    \n    return missed_fill,missed_drop","e2b47ff4":"# Find missed data in train & test\n\nmissed_fill_train , missed_drop_train = find_missed_data_to_limit( train_data ,0.25)\nmissed_fill_test , missed_drop_test = find_missed_data_to_limit( test_data ,0.25)","8dba43f6":"# Clean data from much nun values\n\ntrain_data_cleaned = train_data.drop(missed_drop_train.index , axis=1)\ntest_data_cleaned = test_data.drop(missed_drop_test.index , axis=1)","ea193766":"def fill_null_Data_with_mode (Data,filled_columns) :\n    \n    for col in filled_columns :\n        Data[col].fillna(Data[col].mode,inplace=True)\n        \n    return Data","f80b7081":"def fill_null_Data_with_mean (Data,filled_columns) :\n    \n    for col in filled_columns :\n        Data[col].fillna(Data[col].mean,inplace=True)\n        \n    return Data","3005c9e4":"# Fill little nun data (Optional fill)\n\n#train_data = fill_null_Data_with_mode( train_data_cleaned , missed_fill_train.index )\n#test_data = fill_null_Data_with_mode( test_data_cleaned , missed_fill_test.index )\n\ntrain_data = fill_null_Data_with_mean( train_data_cleaned , missed_fill_train.index )\ntest_data = fill_null_Data_with_mean( test_data_cleaned , missed_fill_test.index )","a99ed9f7":"# Check that Train data is clean\n\nfig = plt.figure(figsize=(18,8))\nsb.heatmap(train_data_cleaned.isnull(),cmap=\"YlGnBu\",yticklabels=False,cbar=False)","debbdef4":"## Check that Test data is clean\n\nfig = plt.figure(figsize=(18,8))\nsb.heatmap(test_data_cleaned.isnull(),cmap=\"YlGnBu\",yticklabels=False,cbar=False)","6e8187ae":"# Let us see correlation heatmap\n\nfig = plt.figure(figsize=(14,10))\nsb.heatmap(train_data_cleaned.corr())","885c042e":"# It seems like there are little corrlitions in our data","47670d9e":"# Dropping classified data\n\ndef drop_columns_of_type (Data,Type) :\n    \n    Data_types = Data.dtypes\n    Data = Data.drop(Data_types[Data_types==Type].index,axis=1)\n    \n    return Data","992e79af":"train_data_cleaned = drop_columns_of_type(train_data_cleaned,'object')\ntest_data_cleaned = drop_columns_of_type(test_data_cleaned,'object')","71f199fd":"# Get dummies\n\ndum_train = pd.get_dummies(train_data_cleaned.drop('Id',axis=1),drop_first=True)\ndum_test = pd.get_dummies(test_data_cleaned.drop('Id',axis=1),drop_first=True)","684fb1fe":"def find_corr_with (Data,column,num_of_corr_cols) :\n\n    corr = Data.corr()[column][dum_train.drop(column,axis=1).columns]\n    corr = corr.sort_values()\n    corr = corr[::-1]\n    \n    return corr.head(num_of_corr_cols)","b8c51e1e":"# Find most correlated data\n\ntrain_corr = find_corr_with(train_data_cleaned , 'SalePrice' , 10)\ntrain_corr","f7651273":"# Checking Train and Test data after cleaning\n\ntrain_data_cleaned.info(),test_data_cleaned.info()","7b04c2f0":"# Preparing our data to the model (notice that train cleaned cols != test cleaned cols)\n# So we will consider the intersection cols of Train and Test Data with correlated cols of train data with 'SalePrice'\n\nX_train = dum_train[test_data_cleaned.columns.intersection(train_corr.index)]\ny_train = dum_train['SalePrice']\n\nX_test = dum_test[test_data_cleaned.columns.intersection(train_corr.index)]\ny_test = sample_submission['SalePrice']\n\nX_train.info(),X_test.info()","f0cdb542":"def plot_data_with (Data,column,Hue=None) :\n    \n    Data[column.name]=column\n    plot = sb.PairGrid(Data,x_vars=Data.columns,y_vars=column.name,height=10,hue=Hue)\n    plot.map(sb.scatterplot)\n    Data.drop(column.name,axis=1,inplace=True)","106ff3fe":"# See plots of the training data\n\nplot_data_with(X_train,y_train)","c07b50cd":"from sklearn.linear_model import LinearRegression","cd5616d6":"lr = LinearRegression()","69cf4b56":"lr.fit(X_train,y_train)","b67f27a3":"pred_lr = lr.predict(X_test)","60e39bb2":"# See how much our model correctly predict the Prices\n\nprint('Training Score : ', lr.score(X_train,y_train))\nprint('Test Score : ', lr.score(X_test,y_test))\nprint('Error :', np.sqrt(mean_squared_error(pred_lr,y_test)))","3413bbf4":"from sklearn.ensemble import RandomForestRegressor","3125fa48":"rfr = RandomForestRegressor()","b693c673":"rfr.fit(X_train,y_train)","4a2038d6":"pred_rfr = rfr.predict(X_test)","e891779b":"print('Training Score : ', rfr.score(X_train,y_train))\nprint('Test Score : ', rfr.score(X_test,y_test))\nprint('Error :', np.sqrt(mean_squared_error(pred_rfr,y_test)))","deb2d071":"from sklearn.neighbors import KNeighborsRegressor","6028f69a":"knn = KNeighborsRegressor()","a62a7608":"knn.fit(X_train,y_train)","da8c3e2b":"pred_knn = knn.predict(X_test)","a4082fb1":"print('Training Score : ', knn.score(X_train,y_train))\nprint('Test Score : ', knn.score(X_test,y_test))\nprint('Error :', np.sqrt(mean_squared_error(pred_knn,y_test)))","f50f22c2":"from sklearn.neural_network import MLPRegressor","4abaeeb4":"mlpr = MLPRegressor()","d415a497":"mlpr.fit(X_train,y_train)","a18fb059":"pred_mlpr = mlpr.predict(X_test)","bf501921":"print('Training Score : ', mlpr.score(X_train,y_train))\nprint('Test Score : ', mlpr.score(X_test,y_test))\nprint('Error :', np.sqrt(mean_squared_error(pred_mlpr,y_test)))","9004331b":"from sklearn.ensemble import GradientBoostingRegressor","15d06e2b":"gbr = GradientBoostingRegressor()","e98dbb14":"gbr.fit(X_train,y_train)","e8b70950":"pred_gbr = gbr.predict(X_test)","09cf91fa":"print('Training Score : ', gbr.score(X_train,y_train))\nprint('Test Score : ', gbr.score(X_test,y_test))\nprint('Error :', np.sqrt(mean_squared_error(pred_gbr,y_test)))","3f16053e":"# Let's collect all results in a data frame\n\nresults_df = pd.DataFrame(data=np.array([pred_lr,pred_rfr,pred_knn,pred_mlpr,pred_gbr]).transpose(),\n                          index=y_test.index,\n                          columns=['Linear','Random Forest','K Neighbors',\n                                   'Multi-layer Perceptron','Gradient Boosting'])\nresults_df.head(5)","f0199cdd":"# Add to them the Actual Price of test\n\ny_test_df = pd.DataFrame(data=np.array([y_test]).transpose(),\n                          index=y_test.index,\n                          columns=['y test (Actual Values)'])\n\nresults_df_to_y_test = results_df.join(y_test_df)\nresults_df_to_y_test","85330f72":"# Now let's plot all results and actual values to compare them (just to look how our models had gone)\n\nplt.figure(figsize=(400,8))\nsb.lineplot(data=results_df_to_y_test)\n\n# It's some kind of big figure I think","4f9695ad":"# Now let's see our models errors and collect them into a data frame \n\nresults_errors_df = pd.DataFrame(data=np.array([ np.sqrt(mean_squared_error(pred_lr,y_test)),\n                                                 np.sqrt(mean_squared_error(pred_rfr,y_test)),\n                                                 np.sqrt(mean_squared_error(pred_knn,y_test)),\n                                                 np.sqrt(mean_squared_error(pred_mlpr,y_test)),\n                                                 np.sqrt(mean_squared_error(pred_gbr,y_test))    ]),\n                          index=['Linear','Random Forest','K Neighbors',\n                                   'Multi-layer Perceptron','Gradient Boosting'],\n                          columns=['Error'])\n\nresults_errors_df","ef441e4d":"# Plot errors to have a look at our prediciton errors\n\nplt.figure(figsize=(8,5))\nsb.lineplot(data=results_errors_df,marker='o')","9576a506":"# Which model has the least error ?\n\nresults_errors_df[ results_errors_df[\"Error\"] == results_errors_df[\"Error\" ].min()]","d3f31628":"my_submission = results_df['Multi-layer Perceptron']\nmy_submission","86ab6f3b":"my_submission_df = pd.DataFrame(data=np.array([my_submission]).transpose(),\n                          index=my_submission.index + 1461,\n                          columns=['SalePrice'])\nmy_submission_df.index.name = 'Id'\nmy_submission_df","fb3be817":"my_submission_df.to_csv('My Submission.csv')","7212b350":"## K Neighbors Regression Model :","631c1ddc":"## Linear Regression Model :","e95211e2":"## Multi-layer Perceptron Regression Model :","84369be1":"## Exploring Data :","635fcd3b":"## Random Forest Regression Model :","a070c4c2":"# House Prices Project\n\n## Importing Libraries and Data :","71431fbd":"# The results of Machine Learning Models :","e3284223":"## Gradient Boosting Regression Model :","e706750f":"## Data Cleaning (Preprocessing) :","1ff40a31":"# Machine Learning"}}