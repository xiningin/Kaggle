{"cell_type":{"50b498db":"code","d7380b69":"code","a659e541":"code","57170f4c":"code","ef972d3d":"code","132410ac":"code","f71c64f7":"code","e36f5ff3":"code","a0ae2606":"code","3afd4709":"code","4da86b01":"code","0ab01239":"code","799d5e1a":"code","8273f28a":"code","e5a79eb8":"code","5842f9f1":"code","26202122":"code","fa973202":"code","bffb2740":"code","4177f879":"code","43646bcd":"code","36baf6c4":"code","ae3012cb":"code","22b4616d":"code","35e0875a":"code","226d5c0f":"code","9ebdb0ea":"code","3ef2e3db":"code","ba3c4027":"code","a90d904e":"code","ffe31e15":"code","6dbae9b6":"code","9c75388f":"code","1b4aa9f4":"code","1fb98f11":"code","d1bfe87e":"markdown","2b5fbbe2":"markdown","05289aeb":"markdown","88af4c20":"markdown","0a68999d":"markdown","c37d2acb":"markdown","ed61942f":"markdown","8036c970":"markdown","2e8e07a9":"markdown","84128a90":"markdown","813f192c":"markdown","18e5fe62":"markdown","69d59750":"markdown","0938dfa0":"markdown","bab82eb0":"markdown","c5cdf5c4":"markdown","9bf1fdd4":"markdown","ba20f279":"markdown","41d84e88":"markdown","8c9eee35":"markdown","f6fd4f70":"markdown","a058e1a2":"markdown","751d5798":"markdown","facbcf2d":"markdown","de286441":"markdown","4be79447":"markdown"},"source":{"50b498db":"import pandas as pd\nimport numpy as np\nimport xgboost\nfrom collections import Counter\nimport gc\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as mpatches\nimport squarify \nfrom tqdm.notebook import tqdm\n\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.neighbors import KNeighborsRegressor\n\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import LinearRegression\nfrom xgboost import XGBRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import Lasso\nfrom lightgbm import LGBMRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom mlxtend.regressor import StackingCVRegressor\n\nimport optuna \nfrom optuna import Trial\nfrom optuna.samplers import TPESampler\n\npd.set_option('display.max_columns', 500)\npd.set_option('display.max_rows', 500)","d7380b69":"# load in csv files\ndf_train = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv')\ndf_test = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv')\n\n# merge \/ reset index\ndf_data = pd.concat([df_train, df_test])\ndf_data.reset_index(drop=True, inplace=True)\ndel df_train\ndel df_test\n\n# info\ndf_data.info(verbose=False, memory_usage='deep')","a659e541":"df_data.sample(5)","57170f4c":"# correlations \/ mask\ncorr = df_data.corr()\nmask = np.triu(np.ones_like(corr, dtype=bool))\n\n# plot \/ cmap\nfig, ax = plt.subplots(figsize=(11, 9))\ncmap = sns.diverging_palette(240, 10, as_cmap=True)\nsns.heatmap(corr, mask=mask, cmap=cmap, center=0, square=True, linewidths=.5, cbar_kws={\"shrink\": .6})\n\nplt.show()","ef972d3d":"# setup plot\nplot_cols = 3 # no of columns \/ change to preference\nlist_categorical_feats = [a for a in df_data.columns if (a not in list(corr.index))] # all categorical features\ntotal_rows = int(np.ceil(len(list_categorical_feats) \/ plot_cols))\n\nfig, ax = plt.subplots(nrows=total_rows, ncols=plot_cols, figsize=(20, 4 * total_rows)) # setup subplots\n\n# loop each feature \/ plot\nj = 0 # keeps track of rows\nfor i in range(0, len(list_categorical_feats)): # loop index of each feature\n    \n    # create plot\n    sns.boxplot(y=list_categorical_feats[i], x='SalePrice', data=df_data, palette='vlag', ax=ax[j,i % plot_cols], order=df_data.groupby(list_categorical_feats[i]).agg({'SalePrice':'mean'}).sort_values('SalePrice').index)\n    ax[j,i % plot_cols].set_title(list_categorical_feats[i])\n    ax[j,i % plot_cols].set_ylabel('')\n    ax[j,i % plot_cols].set_xlabel('')\n    \n    # increment row\n    if i % plot_cols == (plot_cols - 1): j += 1 # basically says at end of each column start a new row\n\nplt.show()","132410ac":"# missing by category \/ create df\nlist_missing_feats = [a for a in df_data.columns if (df_data[a].isnull().sum() > 0) & (not a == 'SalePrice')] # ex target\nlist_numerical_feats = [n for n in df_data.select_dtypes(include=['number']).columns] # all numeric datatypes\ndf_missing = pd.DataFrame(list_missing_feats, columns=['feature']) # cast to df (makes it easier)\n\n# build missing df\ndf_missing['feature_count'] = df_missing.apply(lambda x: df_data[x['feature']].isnull().sum(), axis=1) # count the no of missing features\ndf_missing['feature_type'] = np.where(df_missing['feature'].isin(list_numerical_feats), 'number', 'text')\ndf_missing = df_missing.sort_values('feature_count').reset_index(drop=True) # sort\ndf_missing['meh'] = 'x'\ndf_missing['feature_rank'] = df_missing.sort_values(['feature_count'], ascending=[True]).groupby(['meh']).cumcount() # great sql like function for ranking groups\n\n# create plot\nf, ax = plt.subplots(figsize = (20,8))\nsns.set_color_codes('pastel')\npalette = ['r' if (x == 'number') else 'b' for x in df_missing['feature_type']] # if number then red else blue\nz = sns.barplot(x='feature_count', y='feature', data=df_missing, label='Total', palette=palette, edgecolor='w')\n\n# enum each row \/ add label\nfor index, row in df_missing.iterrows():\n    text_col = 'b'\n    if row.feature_type == 'number': text_col = 'r'\n    z.text(row.feature_count + 15, row.feature_rank, str(row.feature_count), color=text_col, va=\"center\")\n\n# format plot\npatch_num = mpatches.Patch(color='r', label='Numerical')\npatch_text = mpatches.Patch(color='b', label='Text')\nax.legend(handles=[patch_num, patch_text])\nsns.despine(left = True, bottom = True)\nplt.show()","f71c64f7":"# impute known missing features\ndf_data['Functional'] = df_data['Functional'].fillna('Typ') # kaggle docs suggest this should be 'Typ' if null.\ndf_data['Utilities'] = df_data['Utilities'].fillna('AllPub') # only 1 case is not 'AllPub' - should drop this field\n\n# impute 'Na' or 0 missing features\nfor a in ['GarageYrBlt','GarageArea','GarageCars','TotalBsmtSF','MasVnrArea','BsmtFinSF1','BsmtFinSF2','BsmtFullBath','BsmtHalfBath']: df_data[a] = df_data[a].fillna(0)\nfor a in ['BsmtExposure','GarageType','GarageFinish','Alley','MiscFeature','BsmtFinType1','BsmtFinType2','BsmtQual','BsmtCond','GarageQual','GarageCond','FireplaceQu','Fence','PoolQC']: df_data[a] = df_data[a].fillna('Na')","e36f5ff3":"# obtain missing features\ndict_missing = {}\nfor b in [a for a in df_data.columns if (df_data[a].isnull().sum() > 0) & (not a == 'SalePrice')]: dict_missing[b] = df_data[b].isnull().sum()\ndict_missing = dict(sorted(dict_missing.items(), key=lambda item: item[1])) # sort by count\n\n# enum each missing feature\nfor f in dict_missing:\n    \n    # setup classifier\n    missing_estimator = KNeighborsClassifier(n_neighbors=10) # default as classifier\n    if (df_data[f].dtype == np.float64): missing_estimator = KNeighborsRegressor(n_neighbors=10) # if float use regressor\n    \n    # non null cols (will grow as we compute)\n    n_cols = [n for n in df_data.select_dtypes(include=['number']).columns if (df_data[n].isnull().sum()== 0) & (n not in ['Id','SalePrice'])]\n    \n    # fit\n    missing_estimator.fit(\n        df_data[n_cols][(df_data[f].notnull())], # all processed non null data \/ whereby 'f' is also non null\n        df_data[f][(df_data[f].notnull())] # non null 'f' values\n    )\n    \n    # make predictions \/ fill in missing\n    df_data[f] = df_data.apply(lambda x: missing_estimator.predict(np.array(x[n_cols]).reshape(1,-1))[0] if pd.isnull(x[f]) else x[f], axis=1) # predict missing","a0ae2606":"# apply log to sale price\ndf_data['y_SalePrice'] = np.log(df_data['SalePrice'])\ntarget = 'y_SalePrice'\n\n# plot\nfig, ax = plt.subplots(nrows=1, ncols=2, figsize=(16,5))\nsns.distplot(df_data['SalePrice'][(df_data['SalePrice'].notnull())], ax=ax[0])\nsns.distplot(df_data['y_SalePrice'][(df_data['y_SalePrice'].notnull())], ax=ax[1])\nax[0].set_xlabel('Pre Normalize\\nSkew: %f \\nKurt: %f' % (df_data['SalePrice'].skew(), df_data['SalePrice'].kurt()))\nax[1].set_xlabel('Post Normalize\\nSkew: %f \\nKurt: %f' % (df_data['y_SalePrice'].skew(), df_data['y_SalePrice'].kurt()))\nplt.show()","3afd4709":"# no transformation (basically place the prefix before the column name)\ndef _no_trans(df, list_cols, prefix='x_'):\n    \n    # enum cols \/ place prefix (could rename the columns to save space - but the df is small anyway)\n    for a in list_cols: df[prefix + a] = df[a]\n    return df\n\n# dummies\ndef _create_dummies(df, list_cols, prefix='x_'):\n    \n    # enum each passed column\n    for a in list_cols:\n        \n        # create dummies \/ append back to df \/ cleanup\n        df_dummies = pd.get_dummies(df[a].astype(str)) # cast to string\n        df[prefix + a + '_' + df_dummies.columns] = df_dummies # use prefix\n        del df_dummies\n        \n    return df\n\n# map values (since the maps are fairly consistant we can make 1 huge mapping)\ndef _map_val(df, list_cols, prefix='x_'):\n    \n    # enum cols \/ apply map\n    for a in list_cols: df[prefix + a] = df[a].fillna('Na').map({'Ex':5,'Gd':4,'TA':3,'Fa':2,'Po':1,'Na':0,'GLQ':6,'ALQ':5,'BLQ':4,'Rec':3,'LwQ':2,'Unf':1,'Y':1,'N':0,'GdPrv':4,'MnPrv':3,'GdWo':2,'MnWw':1})\n    return df\n\n# merge features\ndef _merge_features(df, list_pairs, prefix='x_'):\n\n    # enum pairs\n    for a in list_pairs:\n\n        # merge\n        list_options = list(set(df[a[0]].to_list() + df[a[1]].to_list())) # all options in columns (deduped)\n        df[[prefix + str(a[0]) + '_' + o for o in list_options]] = df.apply(lambda x: _merge_list(list_options, x[a[0]], x[a[1]]), axis=1, result_type='expand') # merge list\n    return df\n\n# merge list\ndef _merge_list(list_data, data_1, data_2):\n    \n    # generate return dummies\n    list_return = [0] * len(list_data)\n    list_return[list_data.index(data_1)] = 1\n    list_return[list_data.index(data_2)] = 1\n\n    return list_return\n\n# rank features\ndef _rank_features(df, list_cols, prefix='x_rank_'):\n    \n    # enum each column\n    for a in list_cols:\n\n        # rank data within feature by median \/ stash rankings in dict \/ map to new field\n        ranked_data = df[(df[target].notnull())].groupby(a).agg({target:'median'}).sort_values(target).index\n        dict_ranks = dict(zip(ranked_data, range(0, len(ranked_data))))\n        df[prefix + a] = df[a].map(dict_ranks)\n        \n    return df","4da86b01":"# segment into transform types\nlist_no_trans = ['LotArea','LotFrontage','MasVnrArea','BsmtFullBath','BsmtHalfBath','BsmtFinSF1','BsmtFinSF2','BsmtUnfSF','TotalBsmtSF','1stFlrSF','GrLivArea','OverallQual','OverallCond','YearBuilt','YearRemodAdd','2ndFlrSF','LowQualFinSF','FullBath','HalfBath','BedroomAbvGr','KitchenAbvGr','TotRmsAbvGrd','Fireplaces','WoodDeckSF','OpenPorchSF','EnclosedPorch','3SsnPorch','ScreenPorch','PoolArea','MiscVal','YrSold','GarageYrBlt','GarageArea','GarageCars']\nlist_to_dummy = ['MSSubClass','MSZoning','Electrical','MasVnrType','Street','Alley','LotShape','LandContour','LotConfig','LandSlope','Neighborhood','BldgType','HouseStyle','RoofStyle','RoofMatl','Foundation','BsmtExposure','Heating','GarageType','GarageFinish','PavedDrive','MiscFeature','SaleCondition','Functional','Utilities','SaleType']\nlist_to_map = ['ExterQual','ExterCond','KitchenQual','BsmtQual','BsmtCond','BsmtFinType1','BsmtFinType2','HeatingQC','CentralAir','FireplaceQu','GarageQual','GarageCond','PoolQC','Fence']\nlist_to_merge = [('Condition1','Condition2'),('Exterior1st','Exterior2nd')]\nlist_to_rank = ['Heating','HouseStyle','LotShape','MSSubClass','Neighborhood','SaleCondition','SaleType','MasVnrType']\n\n# pipe transformation\ndf_data = (df_data\n    .pipe(_no_trans, list_no_trans) # just prefix\n    .pipe(_create_dummies, list_to_dummy) # dummies\n    .pipe(_map_val, list_to_map) # mapping continuous values\n    .pipe(_merge_features, list_to_merge) # merge features\n    .pipe(_rank_features, list_to_rank) # rank categorical features\n)\n\n# fill in the MSSubClass that only appears in the train set\ndf_data['x_rank_MSSubClass'] = df_data['x_rank_MSSubClass'].fillna(8)\n\n# memory\ndf_data.info(verbose=False, memory_usage='deep')","0ab01239":"# transform to cyclical data\ndf_data['x_month_sin'] = np.sin(2 * np.pi * df_data['MoSold'] \/ 12.0)\ndf_data['x_month_cos'] = np.cos(2 * np.pi * df_data['MoSold'] \/ 12.0)\n\n# plot\nfig, ax = plt.subplots(nrows=1, ncols=3, figsize=(20,5))\nsns.lineplot(range(0,50), df_data['MoSold'][:50], ax=ax[0])\nsns.distplot(df_data['MoSold'], kde=False, bins=range(1,14), hist_kws={'rwidth':0.75}, ax=ax[1])\nsns.scatterplot(df_data['x_month_sin'], df_data['x_month_cos'], ax=ax[2])\nax[0].set_title('Month Continuous')\nax[1].set_title('Month Distribution')\nax[2].set_title('Cyclical Transform')\nplt.show()","799d5e1a":"random_state = 42\n\n# setup evaluation\ndef _evaluate(model, X_train, y_train):\n    \n    # cross validate\n    cv = KFold(n_splits=4, random_state=random_state, shuffle=True) # kfold settings\n    scores = cross_val_score(model, X_train.values, y_train.values, cv=cv, n_jobs=-1, error_score='neg_root_mean_squared_error')\n    return scores","8273f28a":"# lets keep tack of our progress in a dataframe\ndf_evaluation = pd.DataFrame(columns=['Description','Score','Std'])","e5a79eb8":"# get baseline score\nmask_base = (df_data[target].notnull()) # bool mask\nx_cols = [c for c in df_data.columns if c[:2] == 'x_'] # predictor cols\nxgb = XGBRegressor() # model\nscores = _evaluate(xgb, df_data[x_cols][mask_base], df_data[target][mask_base]) # evaluate\ndf_evaluation.loc[len(df_evaluation)] = 'baseline', np.mean(scores), np.std(scores) # log scores\nprint ('baseline: mean:', np.mean(scores), ', std:', np.std(scores))","5842f9f1":"f, ax = plt.subplots(figsize = (12,5))\nsns.distplot(df_data['MoSold'])\nsns.lineplot(x=[0, 14], y=[0.13, 0.13])\nplt.show()","26202122":"# you can see that March-August are the busy periods for selling houses - lets just create a simple bool flag\ndf_data['x_busy_period'] = np.where(df_data['MoSold'].isin([3, 4, 5, 6, 7, 8]), 1, 0)\n\n# evaluate\nmask_base = (df_data[target].notnull()) # bool mask\nx_cols = [c for c in df_data.columns if c[:2] == 'x_'] # predictor cols\nxgb = XGBRegressor() # model\nscores = _evaluate(xgb, df_data[x_cols][mask_base], df_data[target][mask_base]) # evaluate\ndf_evaluation.loc[len(df_evaluation)] = 'busy period', np.mean(scores), np.std(scores) # log scores\nprint ('busy period: mean:', np.mean(scores), ', std:', np.std(scores))","fa973202":"# totals of individual features\ndf_data['x_total_sq_ft'] = (df_data['BsmtFinSF1'] + df_data['BsmtFinSF2'] + df_data['1stFlrSF'] + df_data['2ndFlrSF'])\ndf_data['x_total_bath'] = (df_data['FullBath'] + (0.5 * df_data['HalfBath']) + df_data['BsmtFullBath'] + (0.5 * df_data['BsmtHalfBath']))\ndf_data['x_total_porch_sq_ft'] = (df_data['OpenPorchSF'] + df_data['3SsnPorch'] + df_data['EnclosedPorch'] + df_data['ScreenPorch'] + df_data['WoodDeckSF'])\ndf_data['x_total_qual'] = df_data['OverallQual'] + df_data['OverallCond']\ndf_data['x_total_qual_mean'] = (df_data['OverallQual'] + df_data['OverallCond']) \/ 2\n\n# evaluate\nmask_base = (df_data[target].notnull()) # bool mask\nx_cols = [c for c in df_data.columns if c[:2] == 'x_'] # predictor cols\nxgb = XGBRegressor() # model\nscores = _evaluate(xgb, df_data[x_cols][mask_base], df_data[target][mask_base]) # evaluate\ndf_evaluation.loc[len(df_evaluation)] = 'totals', np.mean(scores), np.std(scores) # log scores\nprint ('totals: mean:', np.mean(scores), ', std:', np.std(scores))","bffb2740":"# sqft per room\ndf_data['x_sqft_per_room'] = df_data['GrLivArea'] \/ (df_data['TotRmsAbvGrd'] + df_data['FullBath'] + df_data['HalfBath'] + df_data['KitchenAbvGr'])\n\n# evaluate\nmask_base = (df_data[target].notnull()) # bool mask\nx_cols = [c for c in df_data.columns if c[:2] == 'x_'] # predictor cols\nxgb = XGBRegressor() # model\nscores = _evaluate(xgb, df_data[x_cols][mask_base], df_data[target][mask_base]) # evaluate\ndf_evaluation.loc[len(df_evaluation)] = 'sqft room', np.mean(scores), np.std(scores) # log scores\nprint ('sqft room: mean:', np.mean(scores), ', std:', np.std(scores))","4177f879":"# property ages\ndf_data['x_age_built'] = df_data['YrSold'] - df_data['YearBuilt']\ndf_data['x_age_remod'] = df_data['YrSold'] - df_data['YearRemodAdd']\ndf_data['x_age_garage'] = df_data['YrSold'] - df_data['GarageYrBlt']\ndf_data['x_build_remod_sum'] = df_data['YearBuilt'] + df_data['YearRemodAdd'] # basically defines newness\ndf_data['x_build_remod_sub'] = df_data['YearRemodAdd'] - df_data['YearBuilt'] # time to remod\n\n# evaluate\nmask_base = (df_data[target].notnull()) # bool mask\nx_cols = [c for c in df_data.columns if c[:2] == 'x_'] # predictor cols\nxgb = XGBRegressor() # model\nscores = _evaluate(xgb, df_data[x_cols][mask_base], df_data[target][mask_base]) # evaluate\ndf_evaluation.loc[len(df_evaluation)] = 'property ages', np.mean(scores), np.std(scores) # log scores\nprint ('property ages: mean:', np.mean(scores), ', std:', np.std(scores))","43646bcd":"# under represented catagorys - tbc","36baf6c4":"# tbc","ae3012cb":"# calculate iqr\nq1 = df_data[target].quantile(0.25)\nq3 = df_data[target].quantile(0.75)\niqr = q3 - q1\nmin_quartile = q1 - 1.5 * iqr\nmax_quartile = q3 + 1.5 * iqr\n\n# iqr score\nmask_iqr = (df_data[target].notnull()) & (df_data[target] >= min_quartile) & (df_data[target] <= max_quartile) # bool mask\nx_cols = [c for c in df_data.columns if c[:2] == 'x_'] # predictor cols\nxgb = XGBRegressor() # model\nscores = _evaluate(xgb, df_data[x_cols][mask_iqr], df_data[target][mask_iqr]) # evaluate\ndf_evaluation.loc[len(df_evaluation)] = 'iqr', np.mean(scores), np.std(scores) # log scores\nprint ('iqr: mean:', np.mean(scores), ', std:', np.std(scores))","22b4616d":"# plot\nfig, ax = plt.subplots(nrows=1, ncols=2, figsize=(16,5))\nsns.lineplot(data=df_evaluation, x='Description', y='Score', sort=False, ax=ax[0])\nsns.lineplot(data=df_evaluation, x='Description', y='Std', sort=False, ax=ax[1])\nax[0].set_xlabel('Mean')\nax[1].set_xlabel('Std')\nplt.show()\nplt.show()","35e0875a":"# # xgb \/ objective function\n# def objective(trial):\n\n#     # hyperparameters\n#     param = {\n#         'objective':'reg:linear',\n#         'seed':random_state,\n#         'nthread':-1,\n#         'n_estimators':trial.suggest_int('n_estimators', 300, 3000),\n#         'max_depth':trial.suggest_int('max_depth', 2, 11),\n#         'learning_rate':trial.suggest_loguniform('learning_rate', 0.005, 0.8),\n#         'min_child_weight':trial.suggest_int('min_child_weight', 0, 8),\n#         'gamma':trial.suggest_int('gamma', 0, 6),\n#         'colsample_bytree':trial.suggest_discrete_uniform('colsample_bytree', 0.1, 1, 0.01),\n#         'subsample':trial.suggest_discrete_uniform('subsample', 0.1, 1, 0.01)\n#     }\n    \n#     # def model \/ evaluate\n#     mask_base = (df_data[target].notnull()) # bool mask\n#     x_cols = [c for c in df_data.columns if c[:2] == 'x_'] # predictor cols\n#     xgb = XGBRegressor(**param) # model\n#     scores = _evaluate(xgb, df_data[x_cols][mask_base], df_data[target][mask_base]) # evaluate\n    \n#     return np.mean(scores)\n\n# # run study\n# study = optuna.create_study(direction='maximize',sampler=TPESampler())\n# study.optimize(objective, n_trials=200)\n# gc.collect()\n\n# # output study\n# print (study.best_value)\n# print (study.best_params)\n\n# {'n_estimators': 2820, 'max_depth': 5, 'learning_rate': 0.014605889674267689, 'min_child_weight': 2, 'gamma': 0, 'colsample_bytree': 0.15000000000000002, 'subsample': 0.71}","226d5c0f":"# # svr \/ objective function\n# def objective(trial):\n    \n#     # hyperparameters\n#     param = {\n#         'cache_size':10000,\n#         'kernel':'poly',\n#         #'gamma':'auto',\n#         #'gamma':trial.suggest_float('gamma', 0.001, 0.9),\n#         'C':trial.suggest_float('C', 0.1, 0.9),\n#         'epsilon':trial.suggest_float('epsilon', 0.001, 0.1),\n#         'coef0':trial.suggest_float('coef0', 0.1, 0.9)\n#     }\n    \n#     # def model \/ evaluate\n#     mask_base = (df_data[target].notnull()) # bool mask\n#     x_cols = [c for c in df_data.columns if c[:2] == 'x_'] # predictor cols\n#     svr = SVR(**param) # model\n#     scores = _evaluate(svr, df_data[x_cols][mask_base], df_data[target][mask_base]) # evaluate\n    \n#     return np.mean(scores)\n\n# # run study\n# study = optuna.create_study(direction='maximize',sampler=TPESampler())\n# study.optimize(objective, n_trials=300)\n# gc.collect()\n\n# # output study\n# print (study.best_value)\n# print (study.best_params)\n# # {'C': 0.23086859625727427, 'epsilon': 0.006673842511350357, 'coef0': 0.8994782389230459}","9ebdb0ea":"# # ridge \/ objective function\n# def objective(trial):\n    \n#     # hyperparameters\n#     param = {\n#         'random_state':random_state,\n#         'alpha':trial.suggest_float('alpha', 0.5, 1),\n#         'tol':trial.suggest_float('tol', 0.4, 0.9),\n#     }\n    \n#     # def model \/ evaluate\n#     mask_base = (df_data[target].notnull()) # bool mask\n#     x_cols = [c for c in df_data.columns if c[:2] == 'x_'] # predictor cols\n#     ridge = Ridge(**param) # model\n#     scores = _evaluate(ridge, df_data[x_cols][mask_base], df_data[target][mask_base]) # evaluate\n    \n#     return np.mean(scores)\n\n# # run study\n# study = optuna.create_study(direction='maximize',sampler=TPESampler())\n# study.optimize(objective, n_trials=300)\n\n# # output study\n# print (study.best_value)\n# print (study.best_params)\n# # {'alpha': 0.9999894695819446, 'tol': 0.8042543139861018}","3ef2e3db":"# # lasso \/ objective function\n# def objective(trial):\n    \n#     # hyperparameters\n#     param = {\n#         'random_state':random_state,\n#         'tol':0.001, # had to increase because of converge warning, \n#         'alpha':trial.suggest_float('alpha', 0.0001, 1)\n#     }\n    \n#     # model \/ evaluate\n#     mask_base = (df_data[target].notnull()) # bool mask\n#     x_cols = [c for c in df_data.columns if c[:2] == 'x_'] # predictor cols\n#     lasso = Lasso(**param) # model\n#     scores = _evaluate(lasso, df_data[x_cols][mask_base], df_data[target][mask_base]) # evaluate\n    \n#     return np.mean(scores)\n\n# # run study\n# study = optuna.create_study(direction='maximize',sampler=TPESampler())\n# study.optimize(objective, n_trials=300)\n\n# # output study\n# print (study.best_value)\n# print (study.best_params)\n# # {'alpha': 0.0002789651831563553}","ba3c4027":"# # lgbm \/ objective function\n# def objective(trial):\n    \n#     # hyperparameters\n#     param = {\n#         'n_estimators':trial.suggest_int('n_estimators', 50, 300),\n#         'num_leaves':trial.suggest_int('num_leaves', 3, 15),\n#         'max_depth':trial.suggest_int('max_depth', 3, 12),\n#         'min_data_in_leaf':trial.suggest_int('min_data_in_leaf', 10, 100),\n#         'learning_rate':trial.suggest_float('learning_rate', 0.1, 0.5),\n#         'min_child_weight':trial.suggest_float('min_child_weight', 0.2, 0.6),\n        \n#     }\n    \n#     # model \/ evaluate\n#     mask_base = (df_data[target].notnull()) # bool mask\n#     x_cols = [c for c in df_data.columns if c[:2] == 'x_'] # predictor cols\n#     lgbm = LGBMRegressor(**param) # model\n#     scores = _evaluate(lgbm, df_data[x_cols][mask_base], df_data[target][mask_base]) # evaluate\n    \n#     return np.mean(scores)\n\n# # run study\n# study = optuna.create_study(direction='maximize',sampler=TPESampler())\n# study.optimize(objective, n_trials=300)\n\n# # output study\n# print (study.best_value)\n# print (study.best_params)\n# # {'n_estimators': 289, 'num_leaves': 6, 'max_depth': 8, 'min_data_in_leaf': 32, 'learning_rate': 0.10777894336456172, 'min_child_weight': 0.4011559466054581}","a90d904e":"# # rf \/ objective function\n# def objective(trial):\n    \n#     # hyperparameters\n#     param = {\n#         'random_state':random_state,\n#         'n_jobs':-1,\n#         'n_estimators':trial.suggest_int('n_estimators', 50, 300),\n#         'max_depth':trial.suggest_int('max_depth', 3, 15),\n#         'min_samples_split':trial.suggest_int('min_samples_split', 3, 12),\n#         'min_samples_leaf':trial.suggest_int('min_samples_leaf', 3, 10),\n#         'max_features':trial.suggest_int('max_features', 8, 60),\n#         'ccp_alpha':trial.suggest_float('ccp_alpha', 0.0, 0.0005),\n        \n#     }\n    \n#     # model \/ evaluate\n#     mask_base = (df_data[target].notnull()) # bool mask\n#     x_cols = [c for c in df_data.columns if c[:2] == 'x_'] # predictor cols\n#     rf = RandomForestRegressor(**param) # model\n#     scores = _evaluate(rf, df_data[x_cols][mask_base], df_data[target][mask_base]) # evaluate\n    \n#     return np.mean(scores)\n\n# # run study\n# study = optuna.create_study(direction='maximize',sampler=TPESampler())\n# study.optimize(objective, n_trials=300)\n\n# # output study\n# print (study.best_value)\n# print (study.best_params)\n# # {'n_estimators': 285, 'max_depth': 13, 'min_samples_split': 3, 'min_samples_leaf': 3, 'max_features': 55, 'ccp_alpha': 7.135644710065878e-07}","ffe31e15":"from sklearn.linear_model import LinearRegression","6dbae9b6":"# model params from optuna\nparam_xgb = {'n_estimators': 2820, 'max_depth': 5, 'learning_rate': 0.014605889674267689, 'min_child_weight': 2, 'gamma': 0, 'colsample_bytree': 0.15000000000000002, 'subsample': 0.71}\nparam_svr = {'C': 0.23086859625727427, 'epsilon': 0.006673842511350357, 'coef0': 0.8994782389230459}\nparam_ridge = {'alpha': 0.9999894695819446, 'tol': 0.8042543139861018}\nparam_lasso = {'alpha': 0.0002789651831563553}\nparam_lgbm = {'n_estimators': 289, 'num_leaves': 6, 'max_depth': 8, 'min_data_in_leaf': 32, 'learning_rate': 0.10777894336456172, 'min_child_weight': 0.4011559466054581}\nparam_rf = {'n_estimators': 285, 'max_depth': 13, 'min_samples_split': 3, 'min_samples_leaf': 3, 'max_features': 55, 'ccp_alpha': 7.135644710065878e-07}\n\n# models\nmodel_xgb = XGBRegressor(**param_xgb, random_state=random_state)\nmodel_svr = SVR(**param_svr)\nmodel_ridge = Ridge(**param_ridge, random_state=random_state)\nmodel_lasso = Lasso(**param_lasso, random_state=random_state, tol=0.001)\nmodel_lgbm = LGBMRegressor(**param_lgbm, random_state=random_state)\nmodel_rf = RandomForestRegressor(**param_rf, random_state=random_state)\nmodel_stack = StackingCVRegressor(\n    regressors=[model_xgb, model_svr, model_ridge, model_lasso, model_lgbm, model_rf],\n    meta_regressor=LinearRegression(n_jobs=-1), # seems to perform better than other regressors\n    cv=KFold(n_splits=4, random_state=random_state, shuffle=True), # kfold settings\n    n_jobs=-1,\n    random_state=random_state\n)\n\n# setup training data\nmask_base = (df_data[target].notnull()) # bool mask\nx_cols = [c for c in df_data.columns if c[:2] == 'x_'] # predictor cols\n\n# evaluate all\nscores_xgb = _evaluate(model_xgb, df_data[x_cols][mask_base], df_data[target][mask_base])\nscores_svr = _evaluate(model_svr, df_data[x_cols][mask_base], df_data[target][mask_base])\nscores_ridge = _evaluate(model_ridge, df_data[x_cols][mask_base], df_data[target][mask_base])\nscores_lasso = _evaluate(model_lasso, df_data[x_cols][mask_base], df_data[target][mask_base])\nscores_lgbm = _evaluate(model_lgbm, df_data[x_cols][mask_base], df_data[target][mask_base])\nscores_rf = _evaluate(model_rf, df_data[x_cols][mask_base], df_data[target][mask_base])\nscores_stack = _evaluate(model_stack, df_data[x_cols][mask_base], df_data[target][mask_base])\n\n# setup plot\nsns.set_style('whitegrid')\nfig, ax = plt.subplots(figsize=(11, 9))\nscores = [scores_xgb, scores_svr, scores_ridge, scores_lasso, scores_lgbm, scores_rf, scores_stack]\nmodels = ['xgb','svr','ridge','lasso','lgbm','rf','stack']\nsns.boxplot(y=models, x=scores)\n\n# decorate\nfor i in range(0, len(scores)):\n    ax.text(np.max([np.max(b) for b in [a for a in scores]]) * 1.02, i, str(np.mean(scores[i])), va='center') # mean score for each model\n\nplt.show()","9c75388f":"# setup training data\nmask_base = (df_data[target].notnull()) # bool mask\nx_cols = [c for c in df_data.columns if c[:2] == 'x_'] # predictor cols\n\n# fit all models\nmodel_xgb.fit(df_data[x_cols][mask_base].values, df_data[target][mask_base].values)\nmodel_svr.fit(df_data[x_cols][mask_base].values, df_data[target][mask_base].values)\nmodel_ridge.fit(df_data[x_cols][mask_base].values, df_data[target][mask_base].values)\nmodel_lasso.fit(df_data[x_cols][mask_base].values, df_data[target][mask_base].values)\nmodel_lgbm.fit(df_data[x_cols][mask_base].values, df_data[target][mask_base].values)\nmodel_rf.fit(df_data[x_cols][mask_base].values, df_data[target][mask_base].values)\nmodel_stack.fit(df_data[x_cols][mask_base].values, df_data[target][mask_base].values)\n\ngc.collect()","1b4aa9f4":"# blend function (note weights must add up to 1)\ndef _blend_predictions(X):\n    return ((0.2 * model_xgb.predict(X)) + \\\n        (0.1 * model_svr.predict(X)) + \\\n        (0.1 * model_ridge.predict(X)) + \\\n        (0.1 * model_lasso.predict(X)) + \\\n        (0.1 * model_lgbm.predict(X)) + \\\n        (0.1 * model_rf.predict(X)) + \\\n        (0.3 * model_stack.predict(np.array(X))))","1fb98f11":"# prep submission file\ndf_submission = df_data[['Id']][(df_data[target].isnull())].reset_index(drop=True).copy() # get id's \/ reset index\n\n# pick model for prediction\ndf_submission['Prediction'] = model_stack.predict(df_data[x_cols][(df_data[target].isnull())].values) # stack (lb: 0.11925)\n#df_submission['Prediction'] = model_xgb.predict(df_data[x_cols][(df_data[target].isnull())].values) # xgb (lb: 0.11951)\n#df_submission['Prediction'] = _blend_predictions(df_data[x_cols][(df_data[target].isnull())].values) # blended (lb: 0.12160)\n\n# calculate sales price (remember we applied log to SalePrice - this needs to be reverted)\ndf_submission['SalePrice'] = np.exp(df_submission['Prediction'])\n\n# export\ndf_submission[['Id','SalePrice']].to_csv('submission.csv', index=False)","d1bfe87e":"# Base Evaluation.","2b5fbbe2":"# Stacking.\nThis technique combines multiple models with the aim to provide more robust predictions with little variance, which we usually get when using a single regressor. <br>\nBelow we will feed our stacked model with our 6 optuna tuned models...","05289aeb":"Note: the above gives a warning, its to do with lasso and its tolorence.","88af4c20":"The above code gets every column prefixed with 'x_' and feeds it into an untuned model. It gives us our baseline score which we will try and improve upon...","0a68999d":"# Normalize.","c37d2acb":"Im not getting great results from SVR, could it be do with the hyperparameters? should i use rbf? do the features need scaling? - i need to look into this. <br>\nIts nice to see xgb regressor performing so well! And that all mt models (except SVR) have had at least 1 90% prediction, it shows promise that with a bit more work i could get all the averages within the 90%'s.","ed61942f":"# Blending.\nBlending is an interesting concept, i saw it a lot on the [jane-street-market-prediction](https:\/\/www.kaggle.com\/c\/jane-street-market-prediction) contest especially from the great work of [Yirun Zhang](https:\/\/www.kaggle.com\/gogo827jz). <br>\nI guess this method will reduce the chances of overfitting, and is more effective than taking an average of all the models. Deciding what the weights are for each model seems a bit trial and error, if someone knows any good notebooks i can read more on this please let me know! <br>\nSpoiler: I couldnt seem to beat my stack or xgb score with this method.","8036c970":"Initial observations are:\n* There are lots of categorical features - these will need to be transformed.\n* There are null values - which will need to be filled.\n* There are features that may need merging - Condition 1 \/ Condition 2 and Exterior1st \/ Exterior2nd.","2e8e07a9":"# Missing.","84128a90":"# Packages.","813f192c":"# Evaluation Function.\nThis is going to be our function for evaluating our progress of data engineering, its just a simple cross validation...","18e5fe62":"# Outliers.","69d59750":"# Transformation.\nTo makes things easier and add a degree of automation i will prefix all the columns to go into the model with 'x_' and apply the following transformations:\n* Map selected categorical features to continuous values.\n* Rank categorical features based on their grouped median SalePrice.\n* Create dummies for the rest of the categorical features.\n* Merge multi fields (Condition1, Condition2)\n* The rest have no transformation \/ just prefix the feature name.","0938dfa0":"# Extra Features.\nModels (especially Trees) need a bit of a helping hand with the features to pick out complex patterns. Lets use our intuition on the dataset to buld some features and hopefully improve our baseline score...","bab82eb0":"# Submission.","c5cdf5c4":"# Target.","9bf1fdd4":"# Explore.","ba20f279":"# Load.","41d84e88":"Good news is that there are lots of correlating features with SalePrice, as well as enough other correlating features that may help us fill in the missing values.","8c9eee35":"Models tend work better once the data is normalized, a good method if your data is right tailed (positive skew - graph on the left) is to apply a log transformation. If the data left tailed (negativley skewed) you could square its values.\n\nSkew and Kurtosis are metrics we can use to help identify if data follows a normal distrobution.\n\nSkew: Basically describes how symmetrical the dataset is, as a rule of thumb:\n* If abs(skew) < 0.5 - the data is symmetrical.\n* If abs(skew) between 0.5 and 1 - the data is moderatley skewed.\n* Otherwise the dataset is highley skewed.\n\nKurtosis: Basically measures the \"heaviness\" of the tails in a distribution, as a rule of thumb we need a value of abs(kurt) < 3, also known as mesokurtic.\n","f6fd4f70":"As you can see, we are adding value - but need to keep an eye on the standard deviation. <br>\nTime for hypertuning...","a058e1a2":"Initial Observations:\n* There are a few values we can binarize (street, alley, centralair).\n* We should drop utilities, there is only 1 sample in the other class.\n* There are features we can transform into continuous (po,fa, ex) typicaly these follow a sequential order.\n* There are a few outliers - these will need to be treated too.","751d5798":"For the rest we will try somthing more fancy...\n* Create a dictionary of missing columns (ordered by missing count).\n* Perform KNN to compute missing values.","facbcf2d":"# Optuna.\nGrid Search has always been my go-to for tuning models, until i discovered Optuna. The main difference between gridsearch and optuna is that with gridsearch you have to specify all the values you want to tune and it simply brute forces through all of the permutations and returns the best score. With optuna you just specify a range and optuna will work out the most optimal hyperparameter.<br>\n<br>\nOur plan is to hypertune several regression models with optuna (note this has been commented out) and use them for another concept called stacking and blending to further improve our predictions...","de286441":"Most of these features can be filled with 'Na' (meaning that there is no Pool, Alley, Fence etc). The numeric and rest of the categorical fields we could potentially use sklearns imputer or use KNN.","4be79447":"## Months.\nA problem with dealing with time \/ week \/ month data is that a model will struggle to understand that 00:00 comes after 11:59 or month 1 (Jan) comes after month 12 (Dec). A great way to deal with this is to use Sine and Cosine to transform into a cyclical feature, the following notebook is well worth a read if you want to find out more:\n\n[https:\/\/www.kaggle.com\/avanwyk\/encoding-cyclical-features-for-deep-learning](https:\/\/www.kaggle.com\/avanwyk\/encoding-cyclical-features-for-deep-learning)"}}