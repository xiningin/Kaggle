{"cell_type":{"2c12b45d":"code","509bb77b":"code","3aba01e3":"code","920863ef":"code","0cd2feb0":"code","2ab65f51":"code","2dfedddf":"code","a7e48f0b":"code","2e11b726":"code","a03bc2b3":"code","9d360e08":"code","39afce67":"code","6df7932e":"code","49d97a83":"code","56dbe13a":"code","5c9b62ea":"code","dce62865":"code","0bb0167c":"code","7888ed74":"code","6ccea9b2":"code","33ed9495":"code","d2191a5e":"code","6804a3d3":"code","e8cd86f9":"code","cc4ec62d":"code","68ba36bb":"code","63de6f89":"code","4a752a75":"code","e24688f6":"code","fcd06170":"code","73c47a3e":"markdown","7559f256":"markdown","ec4c2389":"markdown","061eb4fc":"markdown","3e5cefba":"markdown","aa695966":"markdown","f76ba56f":"markdown","6be96881":"markdown","70b77342":"markdown","4fd59a5c":"markdown","edc1f188":"markdown","6a8bc2ba":"markdown","89b40523":"markdown","ab956adc":"markdown","a42a8c72":"markdown"},"source":{"2c12b45d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport re\nimport sklearn\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","509bb77b":"train = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\n\ntrain.head(10)","3aba01e3":"print(train.isnull().sum()) # checking for missing values in the training dataset","920863ef":"print(test.isnull().sum()) # checking for missing values in the testing dataset","0cd2feb0":"train[\"Embarked\"].value_counts() # checking value counts for \"embarked\"","2ab65f51":"# Replacing the two missing values in \"embarked\" with a qualified guess of \"S\" (Southampton)\ntrain = train.fillna({\"Embarked\": \"S\"})","2dfedddf":"# Replacing the single missing value in \"fare\" with the median\ntest = test.fillna(test['Fare'].median())","a7e48f0b":"# Replacing missing values in \"age\" with the median\ntrain = train.fillna(train['Age'].median())\ntest = test.fillna(test['Age'].median())","2e11b726":"# Removing \"cabin\" and \"ticket\" entirely from both datasets\nremove_features = ['Cabin','Ticket']\ntrain = train.drop(remove_features, axis=1)\ntest = test.drop(remove_features, axis=1)","a03bc2b3":"print(train.isnull().sum()) # checking results","9d360e08":"print(test.isnull().sum()) # checking results","39afce67":"all_data = [train, test]\nfor dataset in all_data:\n    dataset['FamilySize'] = dataset ['SibSp'] + dataset['Parch'] + 1 # Adding the \"FamilySize\" feature (number of siblings\/spouses aboard + number of parents\/children aboard + 1) \n    dataset['IsAlone'] = 1 # Adding the \"IsAlone\" feature and initially set it to 1 (meaning yes, the person is alone)\n    dataset['IsAlone'].loc[dataset['FamilySize'] > 1] = 0 # and then update to 0 (meaning no, the person is not alone) if family size is greater than 1","6df7932e":"# Function that extracts titles from passenger names\ndef title(name):\n    find_title = re.search(' ([A-Za-z]+)\\.', name)\n    if find_title:\n        return find_title.group(1)\n    return \"\"","49d97a83":"# Creating a new feature \"Title\" that contains the titles of the passengers\ntrain['Title'] = train['Name'].apply(title)\ntest['Title'] = test['Name'].apply(title)","56dbe13a":"# Renaming and grouping all the titles. All uncommon titles such as \"Major\", \"Sir\" and \"Lady\" grouped as one (Prestige)\ntrain['Title'] = train['Title'].replace(['Lady', 'Countess','Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Prestige')\ntrain['Title'] = train['Title'].replace('Mlle', 'Miss')\ntrain['Title'] = train['Title'].replace('Ms', 'Miss')\ntrain['Title'] = train['Title'].replace('Mme', 'Mrs')\n    \ntest['Title'] = test['Title'].replace(['Lady', 'Countess','Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Prestige')\ntest['Title'] = test['Title'].replace('Mlle', 'Miss')\ntest['Title'] = test['Title'].replace('Ms', 'Miss')\ntest['Title'] = test['Title'].replace('Mme', 'Mrs')","5c9b62ea":"train.head(20)","dce62865":"test.head(20)","0bb0167c":"train.info() # Checking datatypes","7888ed74":"# Transforming the \"Sex\" data from (male\/female) to numeric values (0\/1)\ntrain['Sex'] = train['Sex'].map( {'female': 0, 'male': 1} ).astype(int)\ntest['Sex'] = test['Sex'].map( {'female': 0, 'male': 1} ).astype(int)\n\n# Transforming the \"Embarked\"(Port of Embarkation) data from (S\/C\/Q) to numeric values (0\/1\/2)\ntrain['Embarked'] = train['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2} ).astype(int)\ntest['Embarked'] = test['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2} ).astype(int)\n\n# Categorizing age intervals\ntrain.loc[ train['Age'] <= 16, 'Age'] = 0\ntrain.loc[(train['Age'] > 16) & (train['Age'] <= 32), 'Age'] = 1\ntrain.loc[(train['Age'] > 32) & (train['Age'] <= 48), 'Age'] = 2\ntrain.loc[(train['Age'] > 48) & (train['Age'] <= 64), 'Age'] = 3\ntrain.loc[ train['Age'] > 64, 'Age'] = 4 ;\ntrain['Age'] = train['Age'].astype(int)\n\ntest.loc[ test['Age'] <= 16, 'Age'] = 0\ntest.loc[(test['Age'] > 16) & (test['Age'] <= 32), 'Age'] = 1\ntest.loc[(test['Age'] > 32) & (test['Age'] <= 48), 'Age'] = 2\ntest.loc[(test['Age'] > 48) & (test['Age'] <= 64), 'Age'] = 3\ntest.loc[ test['Age'] > 64, 'Age'] = 4 ;\ntest['Age'] = test['Age'].astype(int)\n\n# Categorizing passenger fare price intervals\ntrain.loc[ train['Fare'] <= 7.91, 'Fare'] = 0\ntrain.loc[(train['Fare'] > 7.91) & (train['Fare'] <= 14.454), 'Fare'] = 1\ntrain.loc[(train['Fare'] > 14.454) & (train['Fare'] <= 31), 'Fare']   = 2\ntrain.loc[ train['Fare'] > 31, 'Fare'] = 3\ntrain['Fare'] = train['Fare'].astype(int)\n\ntest.loc[ test['Fare'] <= 7.91, 'Fare'] = 0\ntest.loc[(test['Fare'] > 7.91) & (test['Fare'] <= 14.454), 'Fare'] = 1\ntest.loc[(test['Fare'] > 14.454) & (test['Fare'] <= 31), 'Fare']   = 2\ntest.loc[ test['Fare'] > 31, 'Fare'] = 3\ntest['Fare'] = test['Fare'].astype(int)\n\n# Categorizing titles as numeric values\ntitles_as_numbers = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Prestige\": 5}\n\ntrain['Title'] = train['Title'].map(titles_as_numbers)\ntrain['Title'] = train['Title'].fillna(0)\n\ntest['Title'] = test['Title'].map(titles_as_numbers)\ntest['Title'] = test['Title'].fillna(0)","6ccea9b2":"train = train.drop([\"Name\"], axis=1) # no need for names anymore - we already got titles in a seperate column\ntest = test.drop([\"Name\"], axis=1)","33ed9495":"train.head(10)","d2191a5e":"test.head(10)","6804a3d3":"train.info()","e8cd86f9":"test.info()","cc4ec62d":"from sklearn.model_selection import train_test_split\n\nfeatures = train.drop(['Survived', 'PassengerId'], axis=1) # features (aka x) = all columns except for \"Survived\" and \"PassengerId\"\ntarget = train[\"Survived\"] # prediction target (aka y) = \"Survived\"\n\nx_train, x_test, y_train, y_test = train_test_split(features, target, test_size=0.25, random_state=0)","68ba36bb":"from sklearn.metrics import accuracy_score","63de6f89":"# Logistic Regression\nfrom sklearn.linear_model import LogisticRegression\n\nclf = LogisticRegression()\nclf.fit(x_train, y_train)\ny_pred = clf.predict(x_test)\nclf_acc = round(accuracy_score(y_pred, y_test) * 100, 2)\nprint(clf_acc)","4a752a75":"# Support Vector Machine\nfrom sklearn.svm import SVC\n\nsvc = SVC()\nsvc.fit(x_train, y_train)\ny_pred = svc.predict(x_test)\nsvc_acc = round(accuracy_score(y_pred, y_test) * 100, 2)\nprint(svc_acc)","e24688f6":"# Decision Tree\nfrom sklearn.tree import DecisionTreeClassifier\n\ndecisiontree = DecisionTreeClassifier()\ndecisiontree.fit(x_train, y_train)\ny_pred = decisiontree.predict(x_test)\ndecisiontree_acc = round(accuracy_score(y_pred, y_test) * 100, 2)\nprint(decisiontree_acc)","fcd06170":"ids = test['PassengerId']\npredictions = decisiontree.predict(test.drop('PassengerId', axis=1))\n\noutput = pd.DataFrame({ 'PassengerId' : ids, 'Survived': predictions })\noutput.to_csv('submission.csv', index=False)\n","73c47a3e":"# Train test split\nAlright, now that all the data looks good, it's time to make a train test split and feed some data to machine learning models to make some predictions. I will use part of the training data (25% in this case) to test the accuracy of some different models, and then make a submission to the kaggle competition based on which model does best.","7559f256":"# Introduction\nThe topic of this project is utilization of machine learning to solve a binary classification problem. The goal of binary classification is to categorise data points into one of two buckets: 0 or 1, true or false, or in this case survived or not survived. More specifically this project revolves around a dataset containing information about the passengers that were aboard the Titanic when it sank back in 1912. The objective of this project is to utilize machine learning on this data in order to try to predict who will survive and who will die. The following data of each passenger is provided in the dataset:\n* survived: survival (0 = no, 1 = yes)\n* pclass: ticket class (1 = 1st class, 2 = 2nd class, 3 = 3rd class)\n* sex: gender (male\/female)\n* age: age in years\n* sibsp: number of siblings \/ spouses aboard\n* parch: number of parents \/ children aboard\n* ticket: ticket number\n* fare: passenger fare\n* cabin: cabin number\n* embarked: port of embarkation (C = Cherbourg, Q = Queenstown, S = Southampton)","ec4c2389":"Another feature that seems irrelevant to me is \"name\". Converted into numbers this would basically just be another \"passengerId\" column. However, the titles people have could definitely be useful. There might be some correlation there between title and survival as some of them are of high social status and some might have been needed on the lifeboats such as doctors if people were injured.","061eb4fc":"And the datatypes in the datasets now look like this:","3e5cefba":"# Data cleaning\nThere are missing values in the datasets \"age\", \"cabin\", \"embarked\" and \"fare\" columns. Machine learning algorithms do not work if there are missing values in the data, so these will have to be dealt with. Since there is only a single missing value in \"fare\", I think it's fair to replace that one with the median. Let's see what we can do about \"embarked\" though.\n","aa695966":"The datasets now look like this:","f76ba56f":"Results mean that:\n\n644 people embarked the Titanic in Southampton\n\n168 people embarked the Titanic in Cherbourg\n\n77 people embarked the Titanic in Queenstown\n\nA qualified guess to fill out the two missing values in \"embarked\" would be \"S\"(Southampton)","6be96881":"# First look at the data","70b77342":"# Comparing accuracy of different models\nI'm mostly familiar with the logistic regression, support vector machine and decision tree classifiers, so these are the models I will test on. I'm importing and using sklearns \"accuracy_score\" to estimate the different models accuracy.","4fd59a5c":"\"Age\" has a decent amount of missing values, but I have chosen to keep it anyway and just replace the missing values with the median. Which is certainly not ideal, but what can you do. There might be a more elegant solution here that i'm unaware of. However, the \"cabin\" data I will remove entirely as there are way too many missing values. Aditionally I will remove \"ticket\".","edc1f188":"# Adding new features\n\nAs we were working with the Titanic dataset in class, we discussed that there might be some more relevant features than just the ones provided in the datasets. For example, by combining the features \"SibSp\" and \"Parch\" we get family size. Maybe people with family onboard survived more than people who were alone or the other way around - either way, this would be useful information for a machine learning algorithm. Therefore, I will add a \"FamilySize\" and \"IsAlone\" feature to the datasets.","6a8bc2ba":"# Submission\nLooks like the decision tree model is estimated to give the best survival predictions, so I will use that for my Kaggle competition submission below.","89b40523":"The datasets now look like this:","ab956adc":"# Transforming the data - making it readable for machine learning models\nMachine learning models can only recognise numerical data (int, float, etc.) and do not support these object data types such as names, sex(male\/female), title (mr\/mrs, etc.) and the point of embarkation (S\/C\/Q). Therefore, I will tranform this data to numeic values, e.g. \"0\/1\" instead of \"male\/female\", and \"0\/1\/2\" instead of \"S\/C\/Q\". Aditionally I will add some intervals to features such as \"age\" and \"fare\" which should make it easier for the machine learning algorithm to make generalizations and predictions since there won't be so many unique values.","a42a8c72":"At first glance we can already see that there are missing values in this dataset."}}