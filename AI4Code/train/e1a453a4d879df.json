{"cell_type":{"cb446846":"code","5348753d":"code","1e6b6e16":"code","3588f096":"code","c7da932a":"code","39b73612":"code","6135e16c":"code","8d5348e4":"code","3e41d48e":"code","10340a41":"code","5d23b8ee":"code","99af232c":"code","1a02c169":"code","6d873d1a":"code","a5a5c8e7":"code","872dc404":"code","535ad790":"code","a02fc6fe":"code","7d621f47":"code","b9202f30":"code","aac03274":"code","61f01fc7":"code","e54404cd":"code","a2be89c8":"code","00c18518":"code","d6c85155":"code","c90ebf5c":"code","338f0413":"code","0c62397c":"code","51f53ea9":"code","7d7692e6":"markdown","17ae95a3":"markdown","a19f2b34":"markdown","a71771d8":"markdown","5fa7530f":"markdown","60ac8c22":"markdown","b1eec6b7":"markdown","aefb3b35":"markdown","40aff44e":"markdown","12ebb7fa":"markdown","8bf4330c":"markdown","d9d467cc":"markdown","4804fdcb":"markdown","6d0abb97":"markdown","2c7aa7fe":"markdown","e1aec050":"markdown","2569fee6":"markdown","bbc71403":"markdown","6ee31f0f":"markdown","b2e5453e":"markdown","871a9e79":"markdown","c78ad767":"markdown","b27c0c91":"markdown","aeec2c8a":"markdown","29b72bb4":"markdown","5f990e2a":"markdown","22179888":"markdown","f964e3d6":"markdown","d9d9975b":"markdown","61bbdd99":"markdown","1df6a789":"markdown","de3b72b6":"markdown","9b2ff6cc":"markdown","e4036337":"markdown","edab1bc6":"markdown","d6345064":"markdown","191b4c4d":"markdown","a45393d4":"markdown"},"source":{"cb446846":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","5348753d":"# Preprocessing\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split\n\n# Evaluation metrics\nfrom sklearn.metrics import classification_report","1e6b6e16":"# Load dataframe\ntrain_df = pd.read_csv('..\/input\/mini-flight-delay-prediction\/flight_delays_train.csv')\n\n# Display dataframe's head\ntrain_df.head()","3588f096":"# Display dataframe info\ntrain_df.info()","c7da932a":"# Load test dataframe\ntest_df = pd.read_csv('..\/input\/mini-flight-delay-prediction\/flight_delays_test.csv')\n\n# Display test dataframe's head\ntest_df.head()","39b73612":"# Display test dataframe info\ntest_df.info()","6135e16c":"# UniqueCarrier\nuc_labels = train_df.UniqueCarrier.unique().tolist()\nlabel_dict_uc_train = {}\nfor index, possible_label in enumerate(uc_labels):\n    label_dict_uc_train[possible_label] = index\n\n# Origin\norigin_labels = train_df.Origin.unique().tolist()\nlabel_dict_origin_train = {}\nfor index, possible_label in enumerate(origin_labels):\n    label_dict_origin_train[possible_label] = index\n\n# Dest\ndest_labels = train_df.Dest.unique().tolist()\nlabel_dict_dest_train = {}\nfor index, possible_label in enumerate(dest_labels):\n    label_dict_dest_train[possible_label] = index\n\n# Mapping 'UniqueCarrier', 'Origin' and 'Dest'\ntrain_df['UniqueCarrier'] = train_df.UniqueCarrier.replace(label_dict_uc_train)\ntrain_df['Origin'] = train_df.Origin.replace(label_dict_origin_train)\ntrain_df['Dest'] = train_df.Dest.replace(label_dict_dest_train)\n\n# Map 'dep_delayed...' to 1\/0 and save it to 'delayed' variable\ndelayed = train_df['dep_delayed_15min'].map({'Y': 1, 'N': 0}).values\n\ntrain_df.head()","8d5348e4":"# UniqueCarrier\nuc_labels = test_df.UniqueCarrier.unique().tolist()\nlabel_dict_uc_test = {}\nfor index, possible_label in enumerate(uc_labels):\n    label_dict_uc_test[possible_label] = index\n\n# Origin\norigin_labels = test_df.Origin.unique().tolist()\nlabel_dict_origin_test = {}\nfor index, possible_label in enumerate(origin_labels):\n    label_dict_origin_test[possible_label] = index\n\n# Dest\ndest_labels = test_df.Dest.unique().tolist()\nlabel_dict_dest_test = {}\nfor index, possible_label in enumerate(dest_labels):\n    label_dict_dest_test[possible_label] = index\n\n# Map 'dep_delayed...' to 1\/0 and save it to 'delayed' variable\ntest_df['UniqueCarrier'] = test_df.UniqueCarrier.replace(label_dict_uc_test)\ntest_df['Origin'] = test_df.Origin.replace(label_dict_origin_test)\ntest_df['Dest'] = test_df.Dest.replace(label_dict_dest_test)\n\ntest_df.head()","3e41d48e":"# Removing the 'c-' from the data related to dates\nmonth = train_df['Month'].str.split('-')\ntrain_df['Mon']=month.apply(lambda x:int(x[1]))\n\nday = train_df['DayofMonth'].str.split('-')\ntrain_df['DOM']=day.apply(lambda x:int(x[1]))\n\ndow = train_df['DayOfWeek'].str.split('-')\ntrain_df['DOW']=dow.apply(lambda x:int(x[1]))\n\n# Drop redundant columns\ntrain_df = train_df.drop(['Month', 'DayofMonth', 'DayOfWeek'], axis=1)\n\n# Rename columns to 'Month', 'Day' and 'DayOfWeek'\ntrain_df.rename(columns={'Mon': 'Month',  'DOM': 'DayOfMonth',\n                         'DOW': 'DayOfWeek'}, inplace=True)\n\ntrain_df.head()","10340a41":"# Removing the 'c-' from the data related to dates\nmonth = test_df['Month'].str.split('-')\ntest_df['Mon']=month.apply(lambda x:int(x[1]))\n\nday = test_df['DayofMonth'].str.split('-')\ntest_df['DOM']=day.apply(lambda x:int(x[1]))\n\ndow = test_df['DayOfWeek'].str.split('-')\ntest_df['DOW']=dow.apply(lambda x:int(x[1]))\n\ntest_df.head()","5d23b8ee":"# Separate hours and minutes into their respective columns\ntrain_df['DepHour'] = train_df['DepTime']\/\/100\ntrain_df['DepHour'].replace(to_replace=[24,25], value=0, inplace=True)\n\ntrain_df['DepMinute'] = train_df['DepTime']%100\n\n# Save the time in minutes\ntrain_df['Minutes'] = train_df['DepMinute'] + train_df['DepHour']*60\n\n# Convert time to 'timedelta'\ntrain_df['Time'] = pd.to_timedelta(train_df['Minutes'], unit='m')\n\n# Drop irrelevant columns\ntrain_df = train_df.drop(['DepHour', 'DepMinute', 'Minutes', 'DepTime'], axis=1)\n\n# Cast 'datetime' to numeric\ntrain_df['Time'] = pd.to_numeric(train_df['Time'], downcast='float')\n\n# Rename column 'Time' to 'DepTime'\ntrain_df.rename(columns={'Time' : 'DepTime'}, inplace=True)\n\ntrain_df.head()","99af232c":"# Separate hours and minutes into their respective columns\ntest_df['DepHour'] = test_df['DepTime']\/\/100\ntest_df['DepHour'].replace(to_replace=[24,25], value=0, inplace=True)\n\ntest_df['DepMinute'] = test_df['DepTime']%100\n\n# Save the time in minutes\ntest_df['Minutes'] = test_df['DepMinute'] + test_df['DepHour']*60\n\n# Convert time to 'timedelta'\ntest_df['Time'] = pd.to_timedelta(test_df['Minutes'], unit='m')\n\n# Cast 'datetime' to numeric\ntest_df['Time'] = pd.to_numeric(test_df['Time'], downcast='float')\n\n# Drop redundant\/irrelevant columns\ntest_df = test_df.drop(['Month', 'DayofMonth', 'DayOfWeek', 'DepHour',\n                          'DepMinute', 'Minutes', 'DepTime'], axis=1)\n\n# Rename columns to 'Month', 'Day', 'DayOfWeek' and 'DepTime'\ntest_df.rename(columns={'Mon': 'Month',  'DOM': 'DayOfMonth',\n                         'DOW': 'DayOfWeek', 'Time': 'DepTime'}, inplace=True)\n\ntest_df.head()","1a02c169":"# Split data (the split is stratified by default)\ndata = train_df.sample(frac=0.75, random_state=31415)\n\ndata_unseen = train_df.drop(data.index)\ndata.reset_index(inplace=True, drop=True)\ndata_unseen.reset_index(inplace=True, drop=True)\n\nprint('Data for Modeling: ' + str(data.shape))\nprint('Unseen Data For Predictions: ' + str(data_unseen.shape))","6d873d1a":"!pip install pycaret\nfrom pycaret.classification import *","a5a5c8e7":"exp_cls101 = setup(data = data, target = 'dep_delayed_15min', session_id=27182,\n                   numeric_features = ['UniqueCarrier', 'Origin', 'Dest',\n                                       'Distance', 'Month', 'DayOfMonth',\n                                       'DayOfWeek', 'DepTime'],\n                   data_split_stratify=True, silent=True)","872dc404":"best_model = compare_models()","535ad790":"# Create model for evaluation\nlgbm = create_model('lightgbm')\n\n# Plot the confusion matrix\nplot_model(lgbm, plot='confusion_matrix')","a02fc6fe":"# Tune the model's hyperparameters\ntuned_lgbm = tune_model(lgbm)\n\n# Plot the tuned model's confusion matrix\nplot_model(tuned_lgbm, plot='confusion_matrix')","7d621f47":"# Create model for evaluation\nrfc = create_model('rf')\n\n# Plot the confusion matrix\nplot_model(rfc, plot='confusion_matrix')","b9202f30":"# Tune the model's hyperparameters\ntuned_rfc = tune_model(rfc)\n\n# Plot the tuned model's confusion matrix\nplot_model(tuned_rfc, plot='confusion_matrix')","aac03274":"# Create model for evaluation\ngbc = create_model('gbc')\n\n# Plot the confusion matrix\nplot_model(gbc, plot='confusion_matrix')","61f01fc7":"# Tune the model's hyperparameters\ntuned_gbc = tune_model(gbc)\n\n# Plot the tuned model's confusion matrix\nplot_model(tuned_gbc, plot='confusion_matrix')","e54404cd":"# Create model for evaluation\ndtc = create_model('dt')\n\n# Plot the confusion matrix\nplot_model(dtc, plot='confusion_matrix')","a2be89c8":"# Tune the model's hyperparameters\ntuned_dtc = tune_model(dtc)\n\n# Plot the confusion matrix\nplot_model(tuned_dtc, plot='confusion_matrix')","00c18518":"# Drop the column with truth values\ntrain_df = train_df.drop(['dep_delayed_15min'], axis=1)\n\n# Save columns names\natt = list(train_df.columns.values)\n\n# Normalize\ntrain_values = train_df.values\nscaler = preprocessing.MinMaxScaler()\nvalues_scaled = scaler.fit_transform(train_values)\n\n# Save to new dataframe\ntrain_scaled_df = pd.DataFrame(values_scaled,columns=att)\n\ntrain_scaled_df.head()","d6c85155":"# Save columns names\ntest_att = list(test_df.columns.values)\n\n# Normalize\ntest_values = test_df.values\ntest_scaler = preprocessing.MinMaxScaler()\ntest_values_scaled = test_scaler.fit_transform(test_values)\n\n# Save to new dataframe\ntest_scaled_df = pd.DataFrame(test_values_scaled,columns=test_att)\n\ntest_scaled_df.head()","c90ebf5c":"# Print tuned hyperparameters\nprint(tuned_gbc)","338f0413":"# Import gradient boost classifier\nfrom sklearn.ensemble import GradientBoostingClassifier\n\n# Save normalized train dataframe values to 'train_data' variable\ntrain_data = train_scaled_df.values\n\ngbc_model = GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse',\n                                       init=None, learning_rate=0.036,\n                                       loss='deviance', max_depth=7,\n                                       max_features=1.0, max_leaf_nodes=None,\n                                       min_impurity_decrease=0,\n                                       min_impurity_split=None,\n                                       min_samples_leaf=5, min_samples_split=9,\n                                       min_weight_fraction_leaf=0.0,\n                                       n_estimators=190, n_iter_no_change=None,\n                                       presort='deprecated', subsample=0.3,\n                                       tol=0.0001, validation_fraction=0.1,\n                                       verbose=0, warm_start=False)\n\n# Fit data from training dataset\ngbc_model.fit(train_data, delayed)","0c62397c":"# Get values from scaled test dataframe\ntest_sdf_values = test_scaled_df.values\n\n# Predict values using the trained GB classifier\npredicted = gbc_model.predict(test_sdf_values)","51f53ea9":"submission_df = pd.Series(predicted)\nsubmission_df = submission_df.map(lambda label: 'N' if label==0 else 'Y')\nsubmission_df.to_csv('submission.csv', index=False)","7d7692e6":"#### Test","17ae95a3":"#### Train","a19f2b34":"## Gradient Boost Classifier","a71771d8":"Creating the model with PyCaret is as simple as the following code suggests. Then, we can further evaluate the results and scores by plotting the confusion matrix.","5fa7530f":"#### Gradient Boosting","60ac8c22":"As we can see, all selected models suffer to predict delays with accuracy. I've tested both CatBoost and Extreme Gradient Boosting outside of this scope and they respond just the same (you can do the same if you want to check for yourself).\n\nIt is also notable, that both LGBM and Gradient Boosting respond somewhat well to hyperparameters tuning, while Random Forest just classifies everything as won't have any delays and Decision Tree start to generalize towards this path.\n\nIt should be noted, though, that Decision Tree (with default parameters) have the best results towards predicting delays, but then suffer slightly to predict non delayed flights.\n\nMy approach will be to try and predict more accurately non delayed flights, as, with previous tests, I couldn't find a model that predicted the delays well. I'm going to use Gradient Boosting Classifier, as a colleague used this same method with PyCaret, as well, and choose to use LGBM.","b1eec6b7":"### Split between modeling and validation","aefb3b35":"### Model compairson","40aff44e":"### Setup","12ebb7fa":"#### Test","8bf4330c":"#### Test","d9d467cc":"## Data handling: normalization","4804fdcb":"### Install and import PyCaret","6d0abb97":"# Mini flight delay prediction","2c7aa7fe":"### Print tuned hyperparameters","e1aec050":"#### Train","2569fee6":"#### Test dataframe","bbc71403":"### Imports for preprocessing and evaluation","6ee31f0f":"### Convert 'DepTime' to 'timedelta' and cast it to numeric","b2e5453e":"### Map carrier, origin\/destination airport codes and delayed to numeric attributes","871a9e79":"As we can see, the best classifiers (according to those metrics) are CatBoost, LGBM and Extreme Gradient Boosting, with LGBM being quicker than both, while obtaining similar scores.\n\nSince this submission is for a course I'm taking, I'll further evaluate LGBM, Random Forest, Gradient Boosting and Decision Tree, as we have studied the last three and LGBM has one of the best scores, while being quick to model.\n\nI won't be evaluating CatBoost or Extreme Gradient Boosting as these two take too long to run, as well as SVM and MLP, even though we have studied them, they too take a long time to run, but don't offer good results (I've run them beforehand).","c78ad767":"## Automated model test using PyCaret","b27c0c91":"#### Test DF","aeec2c8a":"### Train model using the whole training dataframe","29b72bb4":"#### Train DF","5f990e2a":"### Try and predict the results with the above model","22179888":"### Clean attributes related to date and cast them to int","f964e3d6":"### Submission","d9d9975b":"As we can see, both train and test dataframes have no missing values, so we can proceed to treat their data with that in mind.","61bbdd99":"The data is mostly treated. I'll be normalizing the values afterwards, as I am using PyCaret to compare the classifiers and it already does it in its runtime.","1df6a789":"#### Train","de3b72b6":"## Data handling","9b2ff6cc":"#### Train dataframe","e4036337":"#### Random forest","edab1bc6":"#### Decision tree","d6345064":"#### LGBM","191b4c4d":"### Kaggle imports and directory\/path configurations","a45393d4":"### Open and display dataframes"}}