{"cell_type":{"97d4132c":"code","2599d9af":"code","840ecebc":"code","523cde17":"code","6d54eb50":"code","e59cbc07":"code","217a78da":"code","71bdc256":"code","3a24f4ef":"code","b3cde86c":"code","61615d82":"code","a49c3693":"code","42f81391":"code","4d87d162":"code","e5018604":"code","7ce975c3":"code","4a1995d3":"code","4ea988f6":"code","2772262f":"markdown","f134b24a":"markdown","d90fdedc":"markdown","79f89416":"markdown","87eeaa4e":"markdown","d1b342f8":"markdown","608e2165":"markdown","e0ffdaa4":"markdown","ba7e8913":"markdown","f455cdb3":"markdown","2a16d74f":"markdown","2278ea61":"markdown","18d260a6":"markdown","0e6d10f8":"markdown","3c9cd366":"markdown","1ee38dd5":"markdown","dea95d51":"markdown","a96a08a9":"markdown","e6e948e5":"markdown","ecca2ca1":"markdown","6b359ace":"markdown","2681b3c9":"markdown","4ef66044":"markdown","bac0d238":"markdown","05edcef9":"markdown","92acc33e":"markdown","c7c75923":"markdown","0c70c0f1":"markdown"},"source":{"97d4132c":"!pip install transformers","2599d9af":"import os\nimport gc\nimport copy\nimport datetime\nimport time\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim import lr_scheduler\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.cuda import amp\nimport transformers\nfrom transformers import BertTokenizer,BertForSequenceClassification, BertModel, BertConfig\nfrom transformers import AdamW, get_linear_schedule_with_warmup\nfrom tqdm import tqdm\nfrom collections import defaultdict\nimport plotly.graph_objects as go\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import StratifiedKFold, KFold\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n","840ecebc":"class PROMPTEmbedding(nn.Module):\n    def __init__(self, \n                wte: nn.Embedding,\n                n_tokens: int = 10, \n                random_range: float = 0.5,\n                initialize_from_vocab: bool = True):\n        super(PROMPTEmbedding, self).__init__()\n        self.wte = wte\n        self.n_tokens = n_tokens\n        self.learned_embedding = nn.parameter.Parameter(self.initialize_embedding(wte,\n                                                                               n_tokens, \n                                                                               random_range, \n                                                                               initialize_from_vocab))\n            \n    def initialize_embedding(self, \n                             wte: nn.Embedding,\n                             n_tokens: int = 10, \n                             random_range: float = 0.5, \n                             initialize_from_vocab: bool = True):\n        if initialize_from_vocab:\n            return self.wte.weight[:n_tokens].clone().detach()\n        return torch.FloatTensor(wte.weight.size(1), n_tokens).uniform_(-random_range, random_range)\n            \n    def forward(self, tokens):\n        input_embedding = self.wte(tokens[:, self.n_tokens:])\n        learned_embedding = self.learned_embedding.repeat(input_embedding.size(0), 1, 1)\n        return torch.cat([learned_embedding, input_embedding], 1)","523cde17":"df = pd.read_csv(\"..\/input\/commonlitreadabilityprize\/train.csv\")\ntest_df = pd.read_csv(\"..\/input\/commonlitreadabilityprize\/test.csv\",usecols=[\"id\",\"excerpt\"])\nprint('Number of training sentences: {:,}\\n'.format(df.shape[0]))\ndf.sample(10)","6d54eb50":"def prep_text(text_df):\n    text_df = text_df.str.replace(\"\\n\",\"\",regex=False) \n    return text_df.str.replace(\"\\'s\",r\"s\",regex=True).values\ndf[\"excerpt\"] = prep_text(df[\"excerpt\"])\ntest_df[\"excerpt\"] = prep_text(test_df[\"excerpt\"])","e59cbc07":"def create_folds(data, num_splits):\n    # we create a new column called kfold and fill it with -1\n    data[\"kfold\"] = -1\n    \n    # the next step is to randomize the rows of the data\n    data = data.sample(frac=1).reset_index(drop=True)\n\n    # calculate number of bins by Sturge's rule\n    # I take the floor of the value, you can also\n    # just round it\n    num_bins = int(np.floor(1 + np.log2(len(data))))\n    \n    # bin targets\n    data.loc[:, \"bins\"] = pd.cut(\n        data[\"target\"], bins=num_bins, labels=False\n    )\n    \n    # initiate the kfold class from model_selection module\n    kf = StratifiedKFold(n_splits=num_splits)\n    \n    # fill the new kfold column\n    # note that, instead of targets, we use bins!\n    for f, (t_, v_) in enumerate(kf.split(X=data, y=data.bins.values)):\n        data.loc[v_, 'kfold'] = f\n    \n    # drop the bins column\n    data = data.drop(\"bins\", axis=1)\n\n    # return dataframe with folds\n    return data\n\n\n# create folds\ndf = create_folds(df, num_splits=5)","217a78da":"class CONFIG:\n    seed = 42\n    max_len = 331\n    train_batch = 16\n    valid_batch = 32\n    epochs = 10\n    n_tokens=20\n    learning_rate = 2e-5\n    splits = 5\n    scaler = amp.GradScaler()\n    model='bert-base-cased'\n    tokenizer = BertTokenizer.from_pretrained(model, do_lower_case=True)\n    tokenizer.save_pretrained('.\/tokenizer')\n    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')","71bdc256":"def set_seed(seed = CONFIG.seed):\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    os.environ['PYTHONHASHSEED'] = str(seed)\nset_seed(CONFIG.seed)","3a24f4ef":"class BERTDataset(Dataset):\n    def __init__(self,df):\n        self.text = df['excerpt'].values\n        self.target = df['target'].values\n        self.max_len = CONFIG.max_len\n        self.tokenizer = CONFIG.tokenizer\n        self.n_tokens=CONFIG.n_tokens\n        \n    def __len__(self):\n        return len(self.text)\n    \n    def __getitem__(self, index):\n        text = self.text[index]\n        text = ' '.join(text.split())\n        inputs = self.tokenizer.encode_plus(\n            text,\n            None,\n            truncation=True,\n            add_special_tokens=True,\n            max_length=self.max_len,\n            padding='max_length',\n            return_token_type_ids=True\n        )\n        inputs['input_ids']=torch.cat((torch.full((1,self.n_tokens), 500).resize(CONFIG.n_tokens),torch.tensor(inputs['input_ids'], dtype=torch.long)))\n        inputs['attention_mask'] = torch.cat((torch.full((1,self.n_tokens), 1).resize(CONFIG.n_tokens), torch.tensor(inputs['attention_mask'], dtype=torch.long)))\n\n        return {\n            'ids': inputs['input_ids'],\n            'mask': inputs['attention_mask'],\n    \n            'target': torch.tensor(self.target[index], dtype=torch.float)\n        }\n    ","b3cde86c":"model = BertForSequenceClassification.from_pretrained(\n    \"bert-base-uncased\",\n    num_labels = 1,\n    output_attentions = False,\n    output_hidden_states = False, \n)\nprompt_emb = PROMPTEmbedding(model.get_input_embeddings(), \n                      n_tokens=20, \n                      initialize_from_vocab=True)\nmodel.set_input_embeddings(prompt_emb)\nmodel.cuda()","61615d82":"def get_data(fold):\n    df_train = df[df.kfold != fold].reset_index(drop=True)\n    df_valid = df[df.kfold == fold].reset_index(drop=True)\n    \n    train_dataset = BERTDataset(df_train)\n    valid_dataset = BERTDataset(df_valid)\n\n    train_loader = DataLoader(train_dataset, batch_size=CONFIG.train_batch, \n                              num_workers=4, shuffle=True, pin_memory=True)\n    valid_loader = DataLoader(valid_dataset, batch_size=CONFIG.valid_batch, \n                              num_workers=4, shuffle=False, pin_memory=True)\n    \n    return train_loader, valid_loader","a49c3693":"train_dataloader,validation_dataloader=get_data(0)\nlen(train_dataloader)","42f81391":"param_optimizer = list(model.named_parameters())\nno_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\noptimizer_parameters = [\n    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], \n     'weight_decay': 0.0001},\n    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], \n     'weight_decay': 0.0}\n    ]  \n\noptimizer = AdamW(optimizer_parameters, lr=CONFIG.learning_rate)\n","4d87d162":"# Defining LR Scheduler\nscheduler = get_linear_schedule_with_warmup(\n    optimizer, \n    num_warmup_steps=0, \n    num_training_steps=len(train_dataloader)*CONFIG.epochs\n)\n\nlrs = []\nfor epoch in range(1, CONFIG.epochs + 1):\n    if scheduler is not None:\n        scheduler.step()\n    lrs.append(optimizer.param_groups[0][\"lr\"])\nlayout = go.Layout(template= \"plotly_dark\",title='Learning_rate')\nfig = go.Figure(layout=layout)\n\nfig.add_trace(go.Scatter(x=list(range(CONFIG.epochs)), y=lrs,\n                    mode='lines+markers',\n                    name='Learning_rate'))\nfig.show()","e5018604":"def loss_fn(output,target):\n     return torch.sqrt(nn.MSELoss()(output,target))\ndef format_time(elapsed):\n    elapsed_rounded = int(round((elapsed)))\n    return str(datetime.timedelta(seconds=elapsed_rounded))","7ce975c3":"def run(model,optimizer,scheduler):\n    set_seed(40)\n    scaler=CONFIG.scaler\n    training_stats = []\n    total_t0 = time.time()\n    best_rmse = np.inf\n    epochs=CONFIG.epochs\n    for epoch_i in range(0, epochs):\n        print(\"\")\n        print('======== Epoch {:} \/ {:} ========'.format(epoch_i + 1, epochs))\n        print('Training...')\n        t0 = time.time()\n        total_train_loss = 0\n        data_size=0\n        model.train()\n        for step, batch in enumerate(train_dataloader):\n            tr_loss=[]\n            b_input_ids = batch['ids'].to(CONFIG.device)\n            b_input_mask = batch['mask'].to(CONFIG.device)\n            b_labels = batch['target'].to(CONFIG.device)\n            batch_size = b_input_ids.size(0)\n            model.zero_grad() \n            with amp.autocast(enabled=True):\n                output= model(b_input_ids,attention_mask=b_input_mask)          \n                output=output[\"logits\"].squeeze(-1)\n                loss = loss_fn(output,b_labels)\n                tr_loss.append(loss.item()\/len(output))\n            scheduler.step()\n            scaler.scale(loss).backward()\n            scaler.step(optimizer)\n            scaler.update()\n        avg_train_loss = np.mean(tr_loss)    \n        training_time = format_time(time.time() - t0)\n        gc.collect()\n        print(\"\")\n        print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n        print(\"  Training epoch took: {:}\".format(training_time))\n        print(\"\")\n        print(\"Running Validation...\")\n\n        t0 = time.time()\n        model.eval()\n        val_loss = 0\n        allpreds = []\n        alltargets = []\n        for batch in validation_dataloader:\n            losses = []\n            with torch.no_grad():\n                device=CONFIG.device\n                ids = batch[\"ids\"].to(device)\n                mask = batch[\"mask\"].to(device)\n                output = model(ids,mask)\n                output = output[\"logits\"].squeeze(-1)\n                target = batch[\"target\"].to(device)\n                loss = loss_fn(output,target)\n                losses.append(loss.item()\/len(output))\n                allpreds.append(output.detach().cpu().numpy())\n                alltargets.append(target.detach().squeeze(-1).cpu().numpy())\n                \n        allpreds = np.concatenate(allpreds)\n        alltargets = np.concatenate(alltargets)\n        val_rmse=mean_squared_error(alltargets, allpreds, squared=False)\n        losses = np.mean(losses)\n        gc.collect() \n        validation_time = format_time(time.time() - t0)\n        print(\"  Validation Loss: {0:.2f}\".format(losses))\n        print(\"  Validation took: {:}\".format(validation_time))\n        \n        if val_rmse <= best_rmse:\n            print(f\"Validation RMSE Improved ({best_rmse} -> {val_rmse})\")\n            best_rmse = val_rmse\n            best_model_wts = copy.deepcopy(model.state_dict())\n            PATH = \"rmse{:.4f}_epoch{:.0f}.bin\".format(best_rmse, epoch_i)\n            torch.save(model.state_dict(), PATH)\n            print(\"Model Saved\")\n            \n        training_stats.append(\n        {\n            'epoch': epoch_i + 1,\n            'Training Loss': avg_train_loss,\n            'Valid. Loss': losses,\n            'Training Time': training_time,\n            'Validation Time': validation_time\n        }\n    ) \n    print(\"\")\n    print(\"Training complete!\")\n    return training_stats  ","4a1995d3":"def Visualizations(training_stats):\n    pd.set_option('precision', 2)\n    df_stats = pd.DataFrame(data=training_stats)\n    df_stats = df_stats.set_index('epoch')\n    layout = go.Layout(template= \"plotly_dark\")\n    fig = go.Figure(layout=layout)\n    fig.add_trace(go.Scatter(x=df_stats.index, y=df_stats['Training Loss'],\n                    mode='lines+markers',\n                    name='Training Loss'))\n    fig.add_trace(go.Scatter(x=df_stats.index, y=df_stats['Valid. Loss'],\n                    mode='lines+markers',\n                    name='Validation Loss'))\n    fig.show()","4ea988f6":"df=run(model,optimizer,scheduler)\nVisualizations(df)","2772262f":"# <p style=\"color:#159364; font-family:cursive;\">LOOK AT THE DATA<\/center><\/p>","f134b24a":"<p style=\"color:#159364; font-family:cursive;\">With prompt embeddings in the input,and all the layers have requires_grad True,you can try layer freezing as well\n<\/center><\/p>\n","d90fdedc":"# <p style=\"color:#159364; font-family:cursive;\">LEARNING RATE SCHEDULER<\/center><\/p>","79f89416":"# <p style=\"color:#159364; font-family:cursive;\">GET THE PREPARED DATA<\/center><\/p>","87eeaa4e":"# <p style=\"color:#159364; font-family:cursive;\">TRAINING CONFIGURATION<\/center><\/p>","d1b342f8":"# <p style=\"color:#159364; font-family:cursive;\">MODEL:BERT FOR SEQUENCE CLASSIFICATION from \ud83e\udd17 <\/center><\/p>","608e2165":" <h1 style=\"font-family:verdana;\"> <center>CommonLit Readability:Prompt Tuning BERT<\/center> <\/h1>","e0ffdaa4":"# <p style=\"color:#159364; font-family:cursive;\">REPRODUCIBILITY<\/center><\/p>","ba7e8913":"# <p style=\"color:#159364; font-family:cursive;\">DEFINE LOSS AND TIME FUNCTIONS<\/center><\/p>","f455cdb3":"\n![Upvote!](https:\/\/img.shields.io\/badge\/Upvote-If%20you%20like%20my%20work-07b3c8?style=for-the-badge&logo=kaggle)\n","2a16d74f":"Reference:https:\/\/github.com\/kipgparker\/","2278ea61":"\n<p style=\"color:#159364; font-family:cursive;\">INSTALL THE TRANSFORMERS PACKAGE FROM THE HUGGING FACE LIBRARY<\/center><\/p>\n","18d260a6":"# <p style=\"color:#159364; font-family:cursive;\">IMPORT THE LIBRARIES<\/center><\/p>","0e6d10f8":"# <p style=\"color:#159364; font-family:cursive;\">DEFINE PROMPT EMBEDDINGS CLASS<\/center><\/p>","3c9cd366":"# <p style=\"color:#159364; font-family:cursive;\">OPTIMIZER<\/center><\/p>","1ee38dd5":"<h4 style=\"font-family:verdana\">\n    What is Prompt Tuning?<br><br>\nPrompt-tuning is a simple yet effective mechanism for learning \u201csoft prompts\u201d to condition frozen language models to perform specific downstream tasks.Soft prompts are learned through backpropagation and can be tuned to incorporate\nsignal from any number of labeled examples. Finally, we show that conditioning,a frozen model with soft prompts confers benefits in robustness to domain transfer, as compared to full model tuning.<br>\nInstead of modeling classification as the probability of an output class given some input, p(y|X),where X is a series of tokens and y is a single class label, we now model it as conditional generation,where Y is a sequence of tokens that represent a class label.<br>\nPrompting is the approach of adding extra information for the model to condition on during its generation of Y . Normally, prompting is done by prepending a series of tokens, P, to the input X,such that the model maximizes the likelihood of the\ncorrect Y , p\u03b8(Y |[P; X]), while keeping the model parameters, \u03b8, fixed.<br>\nGiven a series of n tokens, {x0, x1, . . . , xn}, the first thing is embedding the tokens, forming a matrix Xe \u2208 Rn\u00d7e where e is the dimension ofthe embedding space. Our soft-prompts are represented as a parameter Pe \u2208 Rp\u00d7e\n, where p is the length of the prompt. Our prompt is then concatenated to the embedded input forming a single matrix [Pe; Xe] \u2208 R(p+n)\u00d7e\n    \n","dea95d51":"# <p style=\"color:#159364; font-family:cursive;\">A BIT OF PREPROCESSING<\/center><\/p>","a96a08a9":"# <p style=\"color:#159364; font-family:cursive;\">VISUALIZATION FUNCTION <\/center><\/p>","e6e948e5":"**NOTE:This notebook mainly illustrates the use of prompt embeddings in tuning your model.I didn't implement freezing because it wasn't giving good results in this case,just using the prompts embeddings worked good .Feel free to fork the notebook and experiment freezing or other things with it.**","ecca2ca1":"\n <p style=\"color:#159364; font-family:cursive;\">RUN THE MODEL WITH PROMPT EMBEDDINGS ON FOLD 0 <\/center><\/p>","6b359ace":"# <p style=\"color:#159364; font-family:cursive;\">DEFINE THE FUNCTION FOR TRAINING,VALIDATION AND RUNNING<\/center><\/p>","2681b3c9":"# <p style=\"color:#159364; font-family:cursive;\">FOLD:0<\/center><\/p>","4ef66044":"Code taken from:https:\/\/www.kaggle.com\/abhishek\/step-1-create-folds","bac0d238":"# <p style=\"color:#159364; font-family:cursive;\">DEFINE THE DATASET CLASS<\/center><\/p>","05edcef9":"# Let's start","92acc33e":"# <p style=\"color:#159364; font-family:cursive;\">CREATE FOLDS<\/center><\/p>","c7c75923":"\ud83d\udcccGPT-2 Fine Tuning:https:\/\/www.kaggle.com\/shreyasajal\/pytorch-openai-gpt2-commonlit-readability","0c70c0f1":"\ud83d\udccc[The Paper:The Power of Scale for Parameter-Efficient Prompt Tuning](https:\/\/arxiv.org\/pdf\/2104.08691v1.pdf)"}}