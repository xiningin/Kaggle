{"cell_type":{"a54a6f93":"code","704e0404":"code","84e6b6f6":"code","dbfb6d55":"code","66122216":"code","d8ed129a":"code","c743ac36":"code","59622990":"code","ff1d5c74":"code","a7eb2c3c":"code","8dcc7034":"code","ba278de4":"code","83652949":"code","fa7e802b":"code","5f6207c4":"markdown","0aed3449":"markdown","86712b11":"markdown","1d9930fb":"markdown","3c65bfd3":"markdown"},"source":{"a54a6f93":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n%matplotlib inline\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","704e0404":"fruits=pd.read_table('\/kaggle\/input\/fruits-with-colors-dataset\/fruit_data_with_colors.txt')\nfruits.head()","84e6b6f6":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\n#scaler = MinMaxScaler()\n#from sklearn.preprocessing import MinMaxScaler\n#from sklearn.preprocessing import minmax_scale\n#from sklearn.preprocessing import MaxAbsScaler\n#from sklearn.preprocessing import StandardScaler\n#from sklearn.preprocessing import RobustScaler\n#from sklearn.preprocessing import Normalizer\n#from sklearn.preprocessing import QuantileTransformer\n#from sklearn.preprocessing import PowerTransformer\n\nscaler.fit(fruits.drop(['fruit_name','fruit_label','fruit_subtype'],axis=1))\nscaled_features = scaler.transform(fruits.drop(['fruit_name','fruit_label','fruit_subtype'],axis=1))","dbfb6d55":"df_feat = pd.DataFrame(scaled_features,columns=['mass','width','height','color_score'])\ndf_feat.head()","66122216":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(scaled_features,fruits['fruit_label'],\n                                                    test_size=0.30,random_state=42)","d8ed129a":"from sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors=1)\nknn.fit(X_train,y_train)\npred = knn.predict(X_test)\nfrom sklearn.metrics import classification_report,confusion_matrix\nprint(confusion_matrix(y_test,pred))\nprint(classification_report(y_test,pred))","c743ac36":"print((pred==y_test).mean())","59622990":"import matplotlib.pyplot as plt\nerror_rate = []\n\n# Will take some time\nfor i in range(1,40):\n    \n    knn = KNeighborsClassifier(n_neighbors=i)\n    knn.fit(X_train,y_train)\n    pred_i = knn.predict(X_test)\n    error_rate.append(np.mean(pred_i != y_test))\n\n\n","ff1d5c74":"plt.figure(figsize=(10,6))\nplt.plot(range(1,40),error_rate,color='blue', linestyle='dashed', marker='o',\n         markerfacecolor='red', markersize=10)\nplt.title('Error Rate vs. K Value')\nplt.xlabel('K')\nplt.ylabel('Error Rate')\n","a7eb2c3c":"print('the best k='+str(error_rate.index(min(error_rate))+1))","8dcc7034":"ax = plt.axes(projection='3d')\n\n#mass\twidth\theight\tcolor_score scaled_features\n\n# Data for three-dimensional scattered points\nzdata = scaled_features[:,1]\nxdata = scaled_features[:,2]\nydata = scaled_features[:,3]\n#scaled_features[:,3]\n\nax.scatter3D(xdata, ydata, zdata, c=fruits['fruit_label'], cmap='viridis');","ba278de4":"#lets play with  plotly\nimport plotly.express as px\n\nfig = px.scatter_3d(fruits, x='mass', y='width', z='height',\n              color='color_score',symbol='fruit_name',opacity=0.7)\nfig.show()","83652949":"\n# Importamos desde scikit-learn nuestra clase de LDA\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n\n# Reduciremos el dataset de 4 a 2 dimensiones usando el par\u00e1metro n_components\n #fruit_label\tfruit_name\tfruit_subtype\tmass\twidth\theight\tcolor_score\nespecie=fruits['fruit_label'].unique()\nlda_model = LinearDiscriminantAnalysis(n_components=2).fit(df_feat , fruits['fruit_label'])\ndatos_tx = lda_model.transform(df_feat ).T\n\n# Y lo dibujamos, verde=setosa \/ rojo=virginica \/ negro=versicolor\nplt.scatter(datos_tx[0], datos_tx[1], c=['green' if x==1 else 'red' if x==2 else'blue' if x==3 else 'black' for x in fruits['fruit_label']])\nplt.show()","fa7e802b":"# Generamos un grid de valores para probar\nXX, YY = np.meshgrid(np.linspace(datos_tx[0].min(), datos_tx[0].max(), 50), np.linspace(datos_tx[1].min(), datos_tx[1].max(), 50))\npos = np.vstack([XX.ravel(), YY.ravel()])\n\nfrom sklearn.neighbors import KNeighborsClassifier\n\n# Creamos el objeto knn donde definimos el valor de k=5\nknn = KNeighborsClassifier(n_neighbors=1)\nknn.fit(datos_tx.T, fruits['fruit_label'])\npreds_knn = knn.predict(pos.T)\nplt.scatter(pos[0], pos[1],c=['green' if x==1 else 'red' if x==2 else'blue' if x==3 else 'black' for x in preds_knn])\nplt.show()\n\npreds_vx = knn.predict(datos_tx.T)\nprint('Precisi\u00f3n: ' + str(np.array(preds_vx == fruits['fruit_label']).mean()))","5f6207c4":"Choosing a K Value\nLet's go ahead and use the elbow method to pick a good K Value:","0aed3449":"An\u00e1lisis de discriminante lineal","86712b11":"does not improve if going beyowd the k=1","1d9930fb":"Hemos exitosamente llevado nuestro dataset de 4 a 2 dimensiones. Ahora podemos probar c\u00f3mo se desempe\u00f1a un clasificador k-NN","3c65bfd3":"**Predictions and Evaluations**"}}