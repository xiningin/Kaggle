{"cell_type":{"cc2bdaaf":"code","8fec250b":"code","889b3a2e":"code","2c71aecb":"code","595bda6f":"code","4cb8e66c":"code","411128b7":"code","b3477093":"code","f0e006aa":"code","55397a51":"code","19e5daa0":"code","a4c7d68c":"code","d56a8a07":"code","e0e9305d":"code","f510a1e7":"code","7ed546ac":"code","6db16583":"code","02cbe526":"code","9eca5779":"code","ae25f7ba":"code","2ef1e2a0":"code","e6937c87":"code","b321b0a9":"code","d9210e2a":"code","a4687587":"code","f540bffb":"code","facf2c1f":"code","0fe014f9":"code","92d277b7":"code","62f2c3d8":"code","c8914e60":"code","043eecf3":"code","0daf2384":"code","9372d383":"code","99108bac":"code","6720b0d6":"code","e5c8a8c2":"code","d65cdb3e":"code","a7e9ace0":"code","c78e9728":"markdown","c9d58578":"markdown","7bafc4f6":"markdown","2052e56a":"markdown","93e78f6b":"markdown","30626c0b":"markdown","7b5690ff":"markdown","8278b853":"markdown","9f03591e":"markdown","5ccfd1c9":"markdown","13a084be":"markdown","d1950a45":"markdown","9fefe5b7":"markdown","e6250f10":"markdown"},"source":{"cc2bdaaf":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom catboost import CatBoostClassifier\n\nfrom tqdm import tqdm_notebook\n\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)","8fec250b":"train_transaction = pd.read_csv('\/kaggle\/input\/ieee-fraud-detection\/train_transaction.csv')\ntrain_identity = pd.read_csv('\/kaggle\/input\/ieee-fraud-detection\/train_identity.csv')\ntest_transaction = pd.read_csv('\/kaggle\/input\/ieee-fraud-detection\/test_transaction.csv')\ntest_identity = pd.read_csv('\/kaggle\/input\/ieee-fraud-detection\/test_identity.csv')\n# train_transaction = pd.read_csv('train_transaction.csv')\n# train_identity = pd.read_csv('train_identity.csv')\n# test_transaction = pd.read_csv('test_transaction.csv')\n# test_identity = pd.read_csv('test_identity.csv')","889b3a2e":"train_transaction.head()","2c71aecb":"train_identity.head()","595bda6f":"train_df = train_transaction.merge(train_identity, how='left', left_on='TransactionID', right_on='TransactionID')","4cb8e66c":"train_df.shape","411128b7":"del train_transaction\ndel train_identity","b3477093":"test_df = test_transaction.merge(test_identity, how='left', left_on='TransactionID', right_on='TransactionID')","f0e006aa":"del test_transaction\ndel test_identity","55397a51":"one_value_cols = [col for col in train_df.columns if train_df[col].nunique() <= 1]\none_value_cols_test = [col for col in test_df.columns if test_df[col].nunique() <= 1]\n\nmany_null_cols = [col for col in train_df.columns if train_df[col].isnull().sum() \/ train_df.shape[0] > 0.9]\nmany_null_cols_test = [col for col in test_df.columns if test_df[col].isnull().sum() \/ test_df.shape[0] > 0.9]\n\nbig_top_value_cols = [col for col in train_df.columns if train_df[col].value_counts(dropna=False, normalize=True).values[0] > 0.9]\nbig_top_value_cols_test = [col for col in test_df.columns if test_df[col].value_counts(dropna=False, normalize=True).values[0] > 0.9]\n\ncols_to_drop = list(set(many_null_cols + many_null_cols_test + big_top_value_cols + big_top_value_cols_test + one_value_cols + one_value_cols_test))\ncols_to_drop.remove('isFraud')\nprint('{} features are going to be dropped for being useless'.format(len(cols_to_drop)))\n\ntrain_df = train_df.drop(cols_to_drop, axis=1)\ntest_df = test_df.drop(cols_to_drop, axis=1)","19e5daa0":"numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']","a4c7d68c":"cat_features = ['ProductCD', 'addr1', 'addr2', 'P_emaildomain', 'R_emaildomain', 'DeviceType', 'DeviceInfo'] \\\n                + [x for x in train_df.columns if 'card' in x] \\\n                + [x for x in train_df.columns if 'M' in x] \\\n                + [x for x in train_df.columns if ('id_' in x) and (int(x[-2:]) > 11)]","d56a8a07":"cat_features","e0e9305d":"for x in cat_features:\n    s = {y for y in set(train_df[x].unique()).symmetric_difference(set(test_df[x].unique())) if y==y}\n    l = train_df[x].nunique()\n    d = train_df[x].dtypes\n    if (len(s)\/l > 0.1) and (l > 10) and (d in numerics):\n        print('Feature %s; Type %s; Diff %d; Train len %d; Percent diff %.3f' % (x, d, len(s), l, 100*len(s)\/l))","f510a1e7":"cat_features= [x for x in cat_features if x not in ['addr1', 'addr2', 'card1', 'card3',\\\n                                                    'card5', 'id_13', 'id_14', 'id_17', 'id_19', 'id_20']]","7ed546ac":"corrs = train_df.drop(cat_features+['isFraud', 'TransactionID'], axis=1).corr()","6db16583":"tmp = corrs.abs().mask(np.eye(len(corrs), dtype=bool))\ntmp = tmp[tmp > 0.8]\ns = tmp.unstack().dropna()\n\ncorr_df = pd.DataFrame(s.index.tolist(), columns=['feature1', 'feature2'])","02cbe526":"corr_features = corr_df.feature1.unique().tolist()","9eca5779":"aggr_corr_df = corr_df.groupby('feature1').apply(lambda x:frozenset(list(x['feature1'])[:1]+list(x['feature2']))).reset_index()\naggr_corr_df.columns = ['feature1', 'features']\naggr_corr_df.drop_duplicates(subset='features', inplace=True)\naggr_corr_df['features'] = aggr_corr_df['features'].apply(list)\naggr_corr_df.shape","ae25f7ba":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import roc_auc_score","2ef1e2a0":"lr = LogisticRegression(solver='lbfgs')\nfeature_gini = {}\n\nfor col in tqdm_notebook(train_df[corr_features].columns):\n    tmp = pd.concat([train_df[[col]], train_df.isFraud], axis=1)\n    tmp.dropna(inplace=True)\n    lr.fit(tmp.drop('isFraud', axis=1), tmp.isFraud)\n    y_pred_col = lr.predict_proba(tmp.drop('isFraud', axis=1))[:,1]\n    roc_auc_col = roc_auc_score(tmp.isFraud, y_pred_col)\n    feature_gini[col] = np.abs((2*roc_auc_col - 1)*100)","e6937c87":"def max_in_list(list_):\n    return list_.index(max(list_))","b321b0a9":"aggr_corr_df['feature_gini'] = aggr_corr_df.features.apply(lambda x: x[max_in_list([feature_gini.get(i, 0) for i in x])])","d9210e2a":"good_features = aggr_corr_df.feature_gini.unique().tolist() + [x for x in train_df.columns if x not in corr_features]","a4687587":"for x in cat_features:\n    if train_df[x].dtypes in numerics:\n        train_df.loc[train_df[x].isnull(), x] = train_df[x].min() - 1000\n        test_df.loc[test_df[x].isnull(), x] = train_df[x].min() - 1000\n    else:\n        train_df.loc[train_df[x].isnull(), x] = 'undefined'\n        test_df.loc[test_df[x].isnull(), x] = 'undefined'","f540bffb":"for x in cat_features:\n    le = LabelEncoder()\n    le.fit(list(train_df[x].astype(str).values) + list(test_df[x].astype(str).values))\n    train_df[x] = le.transform(list(train_df[x].astype(str).values))\n    test_df[x] = le.transform(list(test_df[x].astype(str).values)) ","facf2c1f":"# data is unbalanced, hence we should initialize class weights\nclass_weights = [1, train_df.isFraud.value_counts()[0]\/train_df.isFraud.value_counts()[1]]","0fe014f9":"cb = CatBoostClassifier(eval_metric='AUC',\\\n                        class_weights=class_weights,\\\n                        n_estimators=1500,\\\n                        random_seed=42,\\\n                        one_hot_max_size=10,\\\n                        silent=True)","92d277b7":"from sklearn.model_selection import StratifiedKFold","62f2c3d8":"skf = StratifiedKFold(10)","c8914e60":"roc_auc_scores = []\ni = 0\nfor train_index, val_index in skf.split(train_df[good_features].drop('isFraud', axis=1), train_df.isFraud):\n    X_train, X_val = train_df[good_features].drop('isFraud', axis=1).iloc[train_index], train_df[good_features].drop('isFraud', axis=1).iloc[val_index]\n    y_train, y_val = train_df.isFraud.iloc[train_index], train_df.isFraud.iloc[val_index]\n    cb.fit(X_train, y_train, cat_features=cat_features)\n    y_pred = cb.predict_proba(X_val)[:,1]\n    roc_auc = roc_auc_score(y_val, y_pred)\n    roc_auc_scores.append(roc_auc)\n    print('AUC in Fold #' + str(i) + ': ' + str(roc_auc))\n    i+=1\nprint('Mean AUC: ' + str(np.mean(roc_auc_scores)))","043eecf3":"feature_dict = {'Features': cb.feature_names_, 'Importance': cb.feature_importances_}","0daf2384":"feature_imp = pd.DataFrame(feature_dict).sort_values(by=['Importance'], ascending=False)","9372d383":"plt.figure(figsize=(10,7))\ndf_imp = feature_imp.head(20)\nsns.barplot(y=df_imp['Features'], x=df_imp['Importance'], palette='coolwarm_r')","99108bac":"cb.fit(train_df.drop('isFraud', axis=1), train_df.isFraud, cat_features=cat_features)","6720b0d6":"y_pred = cb.predict_proba(test_df)","e5c8a8c2":"submission = pd.read_csv('\/kaggle\/input\/ieee-fraud-detection\/sample_submission.csv')","d65cdb3e":"submission['isFraud'] = y_pred","a7e9ace0":"submission.to_csv('submission.csv', index=False)","c78e9728":"To find best feature among correlated, we use one-factor logistic regression, and calculate Gini coefficient. The better it is - the better is feature.","c9d58578":"## Table of contents\n[Loading data](#p-load) <br>\n[Dealing with categorical features](#p-cat-features) <br>\n[Feature selection](#p-fselection) <br>\n[Parameters](#p-params) <br>\n[Training with cross-validation](#p-cv) <br>\n[Feature importances](#p-imp) <br>\n[Submission](#p-sub)","7bafc4f6":"There is too many unique values in some column, which are not of type object. Let's assume it is continuous variable.","2052e56a":"# CatBoost Baseline","93e78f6b":"# Submission <a name=\"p-sub\"><\/a>","30626c0b":"# Feature Importances <a name=\"p-imp\"><\/a>","7b5690ff":"# Loading data <a name=\"p-load\"><\/a>","8278b853":"We try to exclude correlated features, because they can worsen accuracy of boosting","9f03591e":"# Parameters <a name=\"p-params\"><\/a>","5ccfd1c9":"This is baseline notebook, which implements basic boosting classification with CatBoost.","13a084be":"Let's examine all categorical features which are given by default","d1950a45":"# Feature selection <a name=\"p-fselection\"><\/a>","9fefe5b7":"# Dealing with categorical features <a name=\"p-cat-features\"><\/a>","e6250f10":"# Training with cross-validation <a name=\"p-cv\"><\/a>"}}