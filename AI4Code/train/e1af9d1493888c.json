{"cell_type":{"70ba61c2":"code","ef0f6d2a":"code","1320c89a":"code","5cdd141f":"code","89a5105b":"code","a04b16f8":"code","c3e255b5":"code","654b88e6":"code","92031754":"code","aba40b66":"code","0dc32f8c":"code","06f28067":"code","64f857df":"code","9ffaff2c":"code","b666118e":"code","3a71f1a7":"code","094618b0":"code","34ef3d88":"code","1f753285":"code","305a867a":"markdown","3201957f":"markdown","6d29eefa":"markdown","c1de6b3f":"markdown","c2f0397e":"markdown","234ccb09":"markdown","f4279529":"markdown","e0c7f005":"markdown","4248a2f1":"markdown","73c1979a":"markdown","8bc42ba9":"markdown","3b8db0a2":"markdown","b6ccaacd":"markdown","30f16d1e":"markdown"},"source":{"70ba61c2":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ef0f6d2a":"# lets create a feature matrix\nfeature_names =    ['linear', 'nonlinear_square', 'nonlinear_sin',  'interaction_1', 'interaction_2',  'interaction_3',\n   'noise_1', 'noise_2', 'noise_3', 'noise_4', 'noise_5', 'noise_6','noise_7', 'noise_8', 'noise_9','noise_10']","1320c89a":"# define function for creating y\ndef yfromX(X):\n    y = X['linear'] + X['nonlinear_square']**2 + np.sin(3 * X['nonlinear_sin']) + (X['interaction_1'] * X['interaction_2'] * X['interaction_3'])\n    return y","5cdd141f":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\n\n# create x and y\nnp.random.seed(0)\n\nX = pd.DataFrame(np.random.normal(size = (20000, len(feature_names))), columns = feature_names)\ny = yfromX(X)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) ","89a5105b":"from sklearn.linear_model import LinearRegression\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\n\nfrom sklearn.metrics import mean_absolute_error\nfrom eli5.sklearn import PermutationImportance","a04b16f8":"# linear regression\nlr = LinearRegression()\nlr.fit(X_train, y_train)\n\nlr_train_preds = lr.predict(X_train)\nlr_test_preds = lr.predict(X_test)\n\nlr_train_mae = mean_absolute_error(y_train, lr_train_preds)\nlr_test_mae = mean_absolute_error(y_test, lr_test_preds)\n\nlr_fi = PermutationImportance(lr, cv = 'prefit', n_iter = 3).fit(X_train, y_train).feature_importances_","c3e255b5":"# KNN\nknn = KNeighborsRegressor(n_neighbors = int(np.sqrt(len(X_train))))\nknn.fit(X_train, y_train)\n\nknn_train_preds = knn.predict(X_train)\nknn_test_preds = knn.predict(X_test)\n\nknn_train_mae = mean_absolute_error(y_train, knn_train_preds)\nknn_test_mae = mean_absolute_error(y_test, knn_test_preds)\n\nknn_fi = PermutationImportance(knn).fit(X_train, y_train).feature_importances_","654b88e6":"# Support Vector Regression \nsvr = SVR(C = .1)\nsvr.fit(X_train, y_train)\n\nsvr_train_preds = svr.predict(X_train)\nsvr_test_preds = svr.predict(X_test)\n\nsvr_train_mae = mean_absolute_error(y_train, svr_train_preds)\nsvr_test_mae = mean_absolute_error(y_test, svr_test_preds)\n\nsvr_fi = PermutationImportance(svr).fit(X_train, y_train).feature_importances_","92031754":"# Random Forest \nrf = RandomForestRegressor(max_depth=5)\nrf.fit(X_train, y_train)\n\nrf_train_preds = rf.predict(X_train)\nrf_test_preds = rf.predict(X_test)\n\nrf_train_mae = mean_absolute_error(y_train, rf_train_preds)\nrf_test_mae = mean_absolute_error(y_test, rf_test_preds)\n\nrf_fi = rf.feature_importances_","aba40b66":"# XGBOOST \nxgb = XGBRegressor(max_depth=5)\nxgb.fit(X_train, y_train)\n\nxgb_train_preds = xgb.predict(X_train)\nxgb_test_preds = xgb.predict(X_test)\n\nxgb_train_mae = mean_absolute_error(y_train, xgb_train_preds)\nxgb_test_mae = mean_absolute_error(y_test, xgb_test_preds)\n\nxgb_fi = xgb.feature_importances_","0dc32f8c":"# Light GBM\nlgb = LGBMRegressor(max_depth=5)\nlgb.fit(X_train, y_train)\n\nlgb_train_preds = lgb.predict(X_train)\nlgb_test_preds = lgb.predict(X_test)\n\nlgb_train_mae = mean_absolute_error(y_train, lgb_train_preds)\nlgb_test_mae = mean_absolute_error(y_test, lgb_test_preds)\n\nlgb_fi = lgb.feature_importances_","06f28067":"# create a dataframe for feature importances and mean absolute error\nmae_df = pd.DataFrame(columns=['Train','Test'])\n\n# add mae's to the mae_df\nmae_df.loc['Linear Regression','Train'] =  lr_train_mae\nmae_df.loc['Linear Regression','Test'] =  lr_test_mae\n\nmae_df.loc['KNN','Train'] =  knn_train_mae\nmae_df.loc['KNN','Test'] =  knn_test_mae\n\nmae_df.loc['Support Vector Regression','Train'] =  svr_train_mae\nmae_df.loc['Support Vector Regression','Test'] =  svr_test_mae\n\nmae_df.loc['Random Forest','Train'] =  rf_train_mae\nmae_df.loc['Random Forest','Test'] =  rf_test_mae\n\nmae_df.loc['XGBoost','Train'] =  xgb_train_mae\nmae_df.loc['XGBoost','Test'] =  xgb_test_mae\n\nmae_df.loc['Light GBM','Train'] =  lgb_train_mae\nmae_df.loc['Light GBM','Test'] =  lgb_test_mae\n\nmae_df['Model'] = mae_df.index\nmae_df['Train'] = mae_df['Train'].astype(float)\nmae_df['Test'] = mae_df['Test'].astype(float)\n\nmae_df = pd.melt(mae_df, id_vars=['Model'], value_vars=['Train','Test'])","64f857df":"import matplotlib.pyplot as plt\nimport seaborn as sns\nfrom matplotlib.ticker import FuncFormatter \nimport matplotlib.ticker as mtick","9ffaff2c":"# plot mae \nfig,ax = plt.subplots(figsize=(10,4))\nax = sns.barplot(x='value',y='Model', hue='variable',data=mae_df.sort_values(by='value',ascending=False))\nplt.xlabel('Mean Absolute Error')\nplt.ylabel('')\nplt.title('Mean Absolute Error Comparison')\nplt.tight_layout()","b666118e":"# create faeature importance dataframe \nfi_df = pd.DataFrame(columns=['LR', 'KNN','SVR','RF','XGB','LGBM','Features'])\n\nfi_df['Features'] = feature_names\nfi_df['LR'] = lr_fi\nfi_df['KNN'] = knn_fi\nfi_df['SVR'] = svr_fi\nfi_df['RF'] = rf_fi\nfi_df['XGB'] = xgb_fi\nfi_df['LGBM'] = lgb_fi\/1000","3a71f1a7":"fig = plt.figure(figsize=(18,8))\n\nplt.subplot(2, 3, 1)\nax = sns.barplot(x='LR',y='Features',data=fi_df.sort_values(by='LR',ascending=False),color='b')\nax.xaxis.set_major_formatter(FuncFormatter(lambda y, _: '{:.0%}'.format(y)))\nplt.xlabel('Feature Importance')\nplt.ylabel('')\nplt.title('Linear Regression')\nplt.axvline(x=0.1, color='r', linestyle='dashed')\nplt.tight_layout()\n\nplt.subplot(2, 3, 2)\nax = sns.barplot(x='KNN',y='Features',data=fi_df.sort_values(by='KNN',ascending=False),color='b')\nax.xaxis.set_major_formatter(FuncFormatter(lambda y, _: '{:.0%}'.format(y)))\nplt.xlabel('Feature Importance')\nplt.ylabel('')\nplt.title('K-Nearest Neighbors')\nplt.axvline(x=0.1, color='r', linestyle='dashed')\nplt.tight_layout()\n\nplt.subplot(2, 3, 3)\nax = sns.barplot(x='SVR',y='Features',data=fi_df.sort_values(by='SVR',ascending=False),color='b')\nax.xaxis.set_major_formatter(FuncFormatter(lambda y, _: '{:.0%}'.format(y)))\nplt.xlabel('Feature Importance')\nplt.ylabel('')\nplt.title('Support Vector Regression')\nplt.axvline(x=0.1, color='r', linestyle='dashed')\nplt.tight_layout()\n\nplt.subplot(2, 3, 4)\nax = sns.barplot(x='RF',y='Features',data=fi_df.sort_values(by='RF',ascending=False),color='b')\nax.xaxis.set_major_formatter(FuncFormatter(lambda y, _: '{:.0%}'.format(y)))\nplt.xlabel('Feature Importance')\nplt.ylabel('')\nplt.title('Random Forest')\nplt.axvline(x=0.1, color='r', linestyle='dashed')\nplt.tight_layout()\n\nplt.subplot(2, 3, 5)\nax = sns.barplot(x='XGB',y='Features',data=fi_df.sort_values(by='XGB',ascending=False),color='b')\nax.xaxis.set_major_formatter(FuncFormatter(lambda y, _: '{:.0%}'.format(y)))\nplt.xlabel('Feature Importance')\nplt.ylabel('')\nplt.title('XGBoost')\nplt.axvline(x=0.1, color='r', linestyle='dashed')\nplt.tight_layout()\n\nplt.subplot(2, 3, 6)\nax = sns.barplot(x='LGBM',y='Features',data=fi_df.sort_values(by='LGBM',ascending=False),color='b')\nax.xaxis.set_major_formatter(FuncFormatter(lambda y, _: '{:.0%}'.format(y)))\nplt.xlabel('Feature Importance')\nplt.ylabel('')\nplt.title('Light GBM')\nplt.axvline(x=0.1, color='r', linestyle='dashed')\nplt.tight_layout()","094618b0":"from boruta import BorutaPy\n\nnew_rf = RandomForestRegressor(n_jobs = -1, max_depth = 5)\n\nboruta_selector = BorutaPy(new_rf, n_estimators = 'auto', random_state = 0)\nboruta_selector.fit(np.array(X_train), np.array(y_train))\n\nboruta_ranking = boruta_selector.ranking_\nselected_features = np.array(feature_names)[boruta_ranking <= 2]","34ef3d88":"boruta_ranking = pd.DataFrame(data=boruta_ranking, index=X_train.columns.values, columns=['values'])\nboruta_ranking['Variable'] = boruta_ranking.index\nboruta_ranking.sort_values(['values'], ascending=True, inplace=True)","1f753285":"fig,ax = plt.subplots(figsize=(8,4))\nax = sns.barplot(x='values',y='Variable',data=boruta_ranking, color='b')\nplt.title('Boruta Feature Ranking')\nplt.xlabel('')\nplt.ylabel('')\nplt.tight_layout()","305a867a":"## Boruta - A Brief Introduction\n\nSo what is Boruta and how does this work. Concretely, it is a feature selection algorithm that works as a wrapper around Random Forest. And also interestingly, the name 'Boruta' is derived from a demon that dwelled in Pine Forests in Slavic mythology. \n\nThe 4 below steps summarizes how the algorithm works:\n* It creates randomness by creating shuffled copies of the features (shadow features)\n* Applies the Random Forest model on this extended data and gets the feature importance\n* At every iteration it checks if the actual feature has a higher importance than the best of shadow features and removes features \n* It stops if all the features have been selected or rejected or if the number of specified iterations have been reached.\n\nIf you are interested in learning more you can read the original paper [here](https:\/\/www.jstatsoft.org\/article\/view\/v036i11) and the python implementation is [at this link.](https:\/\/github.com\/scikit-learn-contrib\/boruta_py)\n","3201957f":"To show that Boruta actually works I will be using a simulated dataset. I know, that most people will roll their eyes, but the best way to experiment is if we actually know the ground truth. \n\nThe idea is to create a feature matrix where some variables will bee related to y and all other variabes will just be noise. The feature importance should pick only the features that have a relationship with the target and leave the noise variables. ","6d29eefa":"Feature selection is one of the most important aspects in the machine learning. Domain knowledge plays an integral part for making intital educated guesses. But that may not always right. In my recent experiences working on a model for one of my clients, we realized that some of the baseline assumptions (key variables) the company had made and based their assumptions on really did not make a lot of impact on the model predictions.\n\nSo, it makes all the more important for us to choose the correct features for the final models before pushing it into production or claiming victory. ","c1de6b3f":"This kernel was inspired from the medium blog [here](https:\/\/towardsdatascience.com\/feature-selection-you-are-probably-doing-it-wrong-985679b41456). I attempted to verify some of the findings. The best way is to learn by doing!\n\nSo that's that for this kernel. Hope you enjoyed reading this as much as I enjoyed creating this :-) ","c2f0397e":"Feature importance is one of those tricky things that can easily make things go wrong. In a typical sense we can say that the feature has maximum effect on the dependendent variable when the importance value is 1 (100%) and has no effect when the importance is 0 (0%). For our case here we will use 10% as the threshold. So any feature with an importance value greater than 10% should be considered significant.","234ccb09":"As expected the Linear Regression, Support Vector Regression models perform poorly when picking features. The biggest surprise is the Random Forest model. It rates only couple of features above the 10% threshold. Also surprising is the KNN model that actually rates some of the actual features high.\n\nSo if we go with these results we will end up dropping the interaction features when we know y is defintely dependent on those. Same can be said for XGBoost though the interaction features have atleast a 1% effect on the target. The best performance is from Light GBM which picks all the correc features.","f4279529":"## Okay, enough talk! Let's get into action","e0c7f005":"Now we will compare this with Boruta. We will select the Random Forest Regressor since that had very poor performance.","4248a2f1":"The output from Boruta is a ranking number. So we can divide them into categories are need. All features with a rank 1 mean that these have a confirmed effect on the target variable. A rank 2 indicates that there is some influence. Anything above 3 can be rejected as it does not have any infuence.\n\n### **Wow!!! So using Boruta, we picked all the correct dependent variables and rejected the noise**. **Fanatastic!** \n\nThis shows that Boruta can be a very powerful tool to use and check feature importances from traditional methods.","73c1979a":"### Feature Selection - The Usual Way\n\nWe will first attempt to get the features the usual way by running a few popular and unpopular models.","8bc42ba9":"As expected the popular tree models such as XGboost and Light GBM perform the best. \n\nNow lets look at the feature importance. The models should not rate the noise highly and pick only the features that we know have an effect on the target y. ","3b8db0a2":"We will now create X and Y datasets.","b6ccaacd":"### Whats the big deal here? What are you going to do new?\n\nThe typical approach is to build a model, say a Random Forest or Light GBM (Kaggle Favorite!), quickly extract the feature importance and then make decisions based on that. The question then is whether this is really enough? What if the model selects features that are not really important? Would that not impact business decisions?\n\n**Enter Boruta!!!**\n\nI recently came across the algorithm called 'Boruta'. This was developed in 2010 for R and is available in python as well. And I wish somebody had told me this earlier. You will see why in the following sections. And I will also attempt to show how Boruta performs better than the popular algorithms.","30f16d1e":"So, only the first 6 features should be picked by the model and the rest should be discarded. Now, y should be a complex function of the dependent variables and should include linear and non_linear relationships along with interaction effects between the features.\n\nThe function below will be used to determine y. \n\ny = X_linear + (X_nonlinear_square)^2 + sin(3 * X_nonlinear_sin) + (X_interaction_1 + X_interaction_2 + X_interaction_3)"}}