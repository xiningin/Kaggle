{"cell_type":{"35fecc0a":"code","17727c43":"code","7198ea17":"code","8e4efb4d":"code","1e504174":"code","0155c3bc":"code","171b4492":"code","61aa0cdf":"code","890197fa":"code","e4e27779":"code","de40f4e1":"code","6ecebde6":"code","f20bcece":"code","5b1b80f1":"code","6621de28":"code","0b29b0d9":"code","9f734394":"code","58a51205":"code","a5e40db6":"code","1ef2a714":"code","aadd9584":"code","4412f344":"markdown","a499f113":"markdown","e1f39083":"markdown","2ce9c2a2":"markdown","5ddd41fb":"markdown","3b108e48":"markdown","fae8b540":"markdown","766f028a":"markdown","73e000ac":"markdown","1a983e4a":"markdown"},"source":{"35fecc0a":"import matplotlib.pyplot as plt\nimport seaborn as sns\ncolor = sns.color_palette()\n%matplotlib inline\n\nimport numpy as np\nimport pandas as pd\nimport json\n# import missingno as msno\n# import hvplot.pandas\n\nPATH = '..\/input\/'","17727c43":"json_cols = ['device', 'geoNetwork', 'totals', 'trafficSource']\n\nnan_list = [\"not available in demo dataset\",\n            \"unknown.unknown\",\n            \"(not provided)\",\n            \"(not set)\"\n#             ,\"Not Socially Engaged\" # this last one is borderline \n           ]\nnan_dict = {nl:np.nan for nl in nan_list}\n\n# columns to drop : https:\/\/www.kaggle.com\/c\/google-analytics-customer-revenue-prediction\/discussion\/65691#387112\nlist_single_value = ['trafficSource.campaignCode', 'socialEngagementType', 'totals.visits']\n\ndef df_prep(file):\n    df = pd.read_csv(file, dtype={'fullVisitorId': str, 'date': str}, \n            parse_dates=['date'],infer_datetime_format=True, nrows=None)\n    \n    for jc in json_cols:  # parse json  # Would probably be better with json_normalize from pandas\n        flat_df = pd.DataFrame(df.pop(jc).apply(pd.io.json.loads).values.tolist())\n        flat_df.columns = ['{}.{}'.format(jc, c) for c in flat_df.columns]\n        df = df.join(flat_df)\n    ad_df = df.pop('trafficSource.adwordsClickInfo').apply(pd.Series) # handle dict column\n    ad_df.columns = ['adwords.{}'.format(c) for c in ad_df.columns]\n    df = df.join(ad_df)\n    df.replace(nan_dict, inplace=True) # handle disguised NaNs\n    \n    # Remove all-missing columns\n    df.dropna(how=\"all\",axis=1,inplace=True)\n    \n    df.drop([c for c in list_single_value if c in df.columns], axis=1, inplace=True)\n    \n# ### From : https:\/\/www.kaggle.com\/mlisovyi\/flatten-json-fields-smart-dump-data\n    df['trafficSource.isTrueDirect'] = (df['trafficSource.isTrueDirect'].fillna(False)).astype(bool)\n    df['totals.bounces'] = df['totals.bounces'].fillna(0).astype(np.uint8)\n    df['totals.newVisits'] = df['totals.newVisits'].fillna(0).astype(np.uint8) # has NaNs ?\n    df['totals.pageviews'] = df['totals.pageviews'].fillna(0).astype(np.uint16)\n    \n    # rename lat Long\n    df.rename(columns={'geoNetwork.latitude':'Latitude', 'geoNetwork.longitude':\"Longitude\"},inplace=True)\n\n    #parse unix epoch timestamp\n    df.visitStartTime = pd.to_datetime(df.visitStartTime,unit='s',infer_datetime_format=True)\n    \n#     df.set_index(['fullVisitorId', 'sessionId'], inplace=True) # disabled for now\n\n    df.drop([\"sessionId\"],axis=1,inplace=True)\n    return df","7198ea17":"%%time\n\ntrain = df_prep(PATH+'train_v2.csv')\nprint(\"train Shape: \",train.shape)\ntest = df_prep(PATH+'test_v2.csv')\nprint(\"test Shape: \",test.shape)\ndisplay(train.head(7))","8e4efb4d":"train.columns","1e504174":"train[['channelGrouping', 'date', 'fullVisitorId', 'visitId',\n       'visitNumber', 'visitStartTime', 'device.browser',\n       'device.deviceCategory', 'device.isMobile', 'device.operatingSystem',\n       'geoNetwork.city', 'geoNetwork.continent', 'geoNetwork.country',\n       'geoNetwork.metro', 'geoNetwork.networkDomain', 'geoNetwork.region',\n       'geoNetwork.subContinent', 'totals.bounces', 'totals.hits',\n       'totals.newVisits', 'totals.pageviews', 'totals.transactionRevenue',\n        'trafficSource.adContent', 'trafficSource.campaign', 'trafficSource.isTrueDirect',\n       'trafficSource.keyword', 'trafficSource.medium',\n       'trafficSource.referralPath', 'trafficSource.source', 'adwords.page',\n       'adwords.slot', 'adwords.gclId', 'adwords.adNetworkType']].nunique()","0155c3bc":"### Many variables only contain a single variable, remove them:\n### change code version ; errors due to unhashable dicts\n# columns = [col for col in train.columns if train[col].nunique() > 1] # can also be done with \".any() command\"\n# print(len(columns))\n# train = train[columns]\n# test = test[columns]","171b4492":"train.visitStartTime.describe()","61aa0cdf":"#impute 0 for missing\/NaNs of target column\ntrain['totals.transactionRevenue'] = pd.to_numeric(train['totals.transactionRevenue'].fillna(0)) #.astype(\"float\")","890197fa":"train['totals.transactionRevenue'].dtype","e4e27779":"train.loc[train['totals.transactionRevenue']>0]['totals.transactionRevenue'].describe()","de40f4e1":"## https:\/\/www.kaggle.com\/ashishpatel26\/light-gbm-with-bayesian-style-parameter-tuning\n\ngdf = train.groupby(\"fullVisitorId\")[\"totals.transactionRevenue\"].sum().reset_index()\n\nplt.figure(figsize=(9,7))\nplt.scatter(range(gdf.shape[0]), np.sort(np.log1p(gdf[\"totals.transactionRevenue\"].values)))\nplt.xlabel('index', fontsize=12)\nplt.ylabel('TransactionRevenue', fontsize=12)\nplt.show()","6ecebde6":"print(\"Train set: shape {} with {} unique users\"\n      .format(train.shape,train['fullVisitorId'].nunique()))\nprint(\"Test set: shape {} with {} unique users\"\n      .format(test.shape,test['fullVisitorId'].nunique()))\nprint(\"Users in both train and test set:\",\n      len(set(train.fullVisitorId.unique()).intersection(set(test.fullVisitorId.unique()))))","f20bcece":"test.head()","5b1b80f1":"print(\"orig test shape:\",test.shape)\ntest_pred = test.set_index(\"visitStartTime\",drop=False).groupby(\"fullVisitorId\").last().reset_index().drop_duplicates(\"fullVisitorId\")\nprint(\"pred ready test shape:\",test_pred.shape)\ntest_pred.head()","6621de28":"# df2 = train.drop([#\"date\",\n#                   \"sessionId\"\n# #                   , \"visitId\" # ? \n#                  ],axis=1)\n\ndf2 = train.copy()\n\ndf2[\"sumLog_transactionRevenue\"] = df2[[\"fullVisitorId\",\"totals.transactionRevenue\"]].groupby(\"fullVisitorId\")[\"totals.transactionRevenue\"].transform(\"sum\")\n# log transform target (we don't do log1P on purpose)! \ndf2['sumLog_transactionRevenue'] = df2['sumLog_transactionRevenue'].apply(lambda x: np.log1p(x)) #.apply(lambda x: np.log(x) if x > 0 else x)\nprint(\"# unique visitor IDs : \", df2.fullVisitorId.nunique())\nprint(\"subset initial Data shape\", df2.shape)\ndf2 = df2.set_index(\"visitStartTime\",drop=False).groupby(\"fullVisitorId\").last().drop(\"totals.transactionRevenue\",axis=1).reset_index()\nprint(\"Data with target + only last entry in train per fullVisitorId:\", df2.shape)\ndf2.tail()","0b29b0d9":"df_context = pd.concat([train,test])\ndf_context.shape","9f734394":"# is enabling INDEXes, then keep index!\ndf2.to_csv(\"gstore_train_CLV_v1.csv.gz\",index=False,compression=\"gzip\")\n# train.to_csv(\"gstore_train_v1.csv.gz\",index=False,compression=\"gzip\")\n# test.to_csv(\"gstore_test_v1.csv.gz\",index=False,compression=\"gzip\")\n\ndf_context.to_csv(\"gstore_context_all_v1.csv.gz\",index=False,compression=\"gzip\")\ntest_pred.to_csv(\"gstore_test_Pred_v1.csv.gz\",index=False,compression=\"gzip\")","58a51205":"non_missing = len(train[~train['totals.transactionRevenue'].isnull()])\nnum_visitors = train[~train['totals.transactionRevenue'].isnull()]['fullVisitorId'].nunique()\nprint(\"totals.transactionRevenue has {} non-missing values or {:.3f}% (train set)\"\n      .format(non_missing, 100*non_missing\/len(train)))\nprint(\"Only {} unique users have transactions or {:.3f}% (train set)\"\n      .format(num_visitors, num_visitors\/train['fullVisitorId'].nunique()))\n# Logn Distplot\nrevenue = train['totals.transactionRevenue'].dropna().astype('float64')\nplt.figure(figsize=(10,4))\nplt.title(\"Natural log Distribution - Transactions revenue\")\nax1 = sns.distplot(np.log(revenue), color=\"#006633\", fit=norm)\n# Log10 Distplot\nplt.figure(figsize=(10,4))\nplt.title(\"Log10 Distribution - Transactions revenue\")\nax1 = sns.distplot(np.log10(revenue), color=\"#006633\", fit=norm)","a5e40db6":"target_df = pd.read_csv('..\/input\/train.csv', usecols=['totals'])\nflat_df = pd.io.json.json_normalize(target_df.totals.apply(json.loads))\nflat_df['transactionRevenue'] = flat_df.transactionRevenue.astype(np.float32)\nflat_df.transactionRevenue.isnull().sum()\/flat_df.shape[0]","1ef2a714":"flat_df.fillna(0, inplace=True)\nflat_dft.hist('transactionRevenue', bins=24) #.hvplo","aadd9584":"flat_df.replace(0, np.NaN, inplace=True)\nflat_df.hist('transactionRevenue', bins=25) #.hvplot","4412f344":"### Transaction Revenue\n\nOur target column, transactionRevenue, looks especially sparse. Let's look closer...","a499f113":"## Concat context\n* train + test historical data\n*Could drop last entries in test for space saving.. But might be wanted for country level feature, cooccurrence etc'?\n","e1f39083":"### finals target data\n* start with data per user at their final time stamp. This isn't what we'd use for a final model but can give us great insights , and iscompatible with feature engineering for the historical data!\n* Get last timestamp for each fullVisitorId,  sum historical totals.transactionRevenue transactions, then log that sum.\n    * There's no history for ~80% of users (i.e most appear only once). \n    * Also, user history - note train\/test disjoint and sparsity!\n    \n    \n    * featurize: https:\/\/stackoverflow.com\/questions\/45022226\/find-days-since-last-event-pandas-dataframe","2ce9c2a2":"*  we're predicting the natural log of the total revenue per unique user, which is, based on totals.transactionRevenue.  (Where a Nan is actually a 0).\n* We should log the sum total of *totals.transactionRevenue*\n     * https:\/\/www.kaggle.com\/c\/google-analytics-customer-revenue-prediction\/discussion\/65691#387112\n\n * https:\/\/www.kaggle.com\/mlisovyi\/flatten-json-fields-smart-dump-data\n * https:\/\/www.kaggle.com\/jpmiller\/showing-nan-in-its-various-forms","5ddd41fb":"## Save data\n* Could use Feather or binary format, but let's stay simple\n","3b108e48":"#### Test data\n* we predict once per customer. (in test) , using the last entry\n* multiple rows present = we have more history for them -> concat with train data for history.\n* Note the lack of overlap with train -> we may want to entirely exclude the y\/target from history, to avouid leaks! ","fae8b540":"Well, OK...there's quite a bit of 0s here.  Zooming in on the 1%  greater than 0 shows the difference between browsers and buyers.","766f028a":"### Data Prep\n\n* Additional ideas for missing values and unary columns top drop:\n https:\/\/www.kaggle.com\/mlisovyi\/flatten-json-fields-smart-dump-data\n * https:\/\/www.kaggle.com\/c\/google-analytics-customer-revenue-prediction\/discussion\/65691#387112","73e000ac":"## Target col: \n* We will want to sum then log at the end, (if we do it once per customer, VS predicting CLV at each point in time..?)\n* totals_transactionRevenue - nan is actually 0 \n* **Major, novel  feature: Transactions per session (mean and boolean)**\n    * Dan\n    \n* WE see most visitors never make any purchase","1a983e4a":"## NaN EDA cont\n*Source:  https:\/\/www.kaggle.com\/jsaguiar\/complete-exploratory-analysis\n        * code requires changing (fullVisitorId in index in my version)\n*  'totals_transactionRevenue' column=  the transaction value for each visit. \n*Train set has 98.72% of missing values which we can consider as zero revenue (no purchase).\n* The black lines are the closest normal distribution that we can fit to each distribution."}}