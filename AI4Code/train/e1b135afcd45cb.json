{"cell_type":{"20be939d":"code","dfd82ae1":"code","b3333ccb":"code","310de231":"code","c425cd8e":"code","6eb31da2":"code","f9a7429e":"code","f42e2e92":"code","c6834e69":"code","fe7a2cec":"code","2653b30a":"code","a4b48104":"code","ab20431c":"code","0595016f":"code","5609174d":"code","a587da32":"code","52d5cda8":"code","af9640f0":"code","7d38bb07":"markdown","1a2b6f68":"markdown","b45176f3":"markdown","dd89e2aa":"markdown","b13ddea4":"markdown","c4277094":"markdown","c49ec7d9":"markdown","9b654fb8":"markdown","cd81938d":"markdown"},"source":{"20be939d":"import os #functions for interacting with the operating system.\nfrom PIL import Image #adds support for opening, manipulating, and saving many different image file formats.\n'''image:- module provides a class with the same name which is used to represent a PIL image. The module also provides a number of factory functions, \nincluding functions to load images from files, and to create new images.'''\nimport matplotlib.pyplot as plt #for creating static, animated, and interactive visualizations\n\nimport torch #a framework that provides a wide range of algorithms for deep learning\nimport torchvision #consists of datasets, model architectures, and common image transformations for computer vision\nfrom torch.utils.data import DataLoader, Dataset, random_split\n'''data loading utility, represents a Python iterable over a dataset'''\n'''argument of DataLoader constructor is dataset, which indicates a dataset object to load data from. PyTorch'''\n'''Randomly split a dataset into non-overlapping new datasets of given lengths.'''\nimport torchvision.transforms as transforms\n#control over the transformations. This is useful if you have to build a more complex transformation pipeline (e.g. in the case of segmentation tasks).","dfd82ae1":"#For converting the dataset to torchvision dataset format\nclass VowelConsonantDataset(Dataset):\n    def __init__(self, file_path,train=True,transform=None):\n        self.transform = transform\n        self.file_path=file_path\n        self.train=train\n        self.file_names=[file for _,_,files in os.walk(self.file_path) for file in files]\n        self.len = len(self.file_names)\n        if self.train:\n            self.classes_mapping=self.get_classes()\n    def __len__(self):\n        return len(self.file_names)\n    \n    def __getitem__(self, index):\n        file_name=self.file_names[index]\n        image_data=self.pil_loader(self.file_path+\"\/\"+file_name)\n        if self.transform:\n            image_data = self.transform(image_data)\n        if self.train:\n            file_name_splitted=file_name.split(\"_\")\n            Y1 = self.classes_mapping[file_name_splitted[0]]\n            Y2 = self.classes_mapping[file_name_splitted[1]]\n            z1,z2=torch.zeros(10),torch.zeros(10)\n            z1[Y1-10],z2[Y2]=1,1\n            label=torch.stack([z1,z2])\n\n            return image_data, label\n\n        else:\n            return image_data, file_name\n          \n    def pil_loader(self,path):\n        with open(path, 'rb') as f:\n            img = Image.open(f)\n            return img.convert('RGB')\n\n      \n    def get_classes(self):\n        classes=[]\n        for name in self.file_names:\n            name_splitted=name.split(\"_\")\n            classes.extend([name_splitted[0],name_splitted[1]])\n        classes=list(set(classes))\n        classes_mapping={}\n        for i,cl in enumerate(sorted(classes)):\n            classes_mapping[cl]=i\n        return classes_mapping","b3333ccb":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n\nimport torchvision\nimport matplotlib.pyplot as plt\nfrom torchvision import datasets\n\nimport torchvision.transforms as transforms\n\nimport numpy as np\nimport pandas as pd\n\nimport copy\n\ntrain_on_gpu = torch.cuda.is_available()","310de231":"device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\ndevice","c425cd8e":"os.mkdir('..\/Inputs')\n\nimport zipfile\n\n# Will unzip the files so that you can see them..\nwith zipfile.ZipFile(\"..\/input\/padhai-tamil-vowel-consonant-classification\/train.zip\",\"r\") as z:\n    z.extractall(\"..\/Inputs\/\")\nwith zipfile.ZipFile(\"..\/input\/padhai-tamil-vowel-consonant-classification\/test.zip\",\"r\") as z:\n    z.extractall(\"..\/Inputs\/\")","6eb31da2":"transform = transforms.Compose([\n    transforms.ColorJitter(),\n    transforms.Resize(256),\n    transforms.CenterCrop(224),\n    transforms.ToTensor(), ])","f9a7429e":"batch_size = 60 #number of samples to work through before updating the internal model parameters. Size of Training Set\nfull_data=VowelConsonantDataset(\"..\/Inputs\/train\",train=True,transform=transform)\n# we are splitting total 10000 images into 9000 images for training and remaining 1000 for validation\ntrain_size = int(0.9 * len(full_data))\ntest_size = len(full_data) - train_size\n\ntrain_data, validation_data = random_split(full_data, [train_size, test_size])\n\ntrain_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\nvalidation_loader = torch.utils.data.DataLoader(validation_data, batch_size=batch_size, shuffle=True)\n\ntest_data=VowelConsonantDataset(\"..\/Inputs\/test\",train=False,transform=transform)\ntest_loader = torch.utils.data.DataLoader(test_data, batch_size=60,shuffle=False)","f42e2e92":"dataiter = iter(train_loader)\nimages, labels = dataiter.next()\nimg = images[1]\nnpimg = img.numpy()\nnpimg = np.transpose(npimg, (1, 2, 0))\nplt.figure(figsize = (1,1))\nplt.imshow(npimg)\nplt.show()","c6834e69":"def imshow(img):\n    npimg = img.numpy()\n    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n    plt.show()\n    \nimshow(torchvision.utils.make_grid(images))","fe7a2cec":"from torchvision import models","2653b30a":"class MyModel(nn.Module):\n    def __init__(self, num_classes1, num_classes2):\n        super(MyModel,self).__init__()\n        #model architectures for image classification:- mobilenet\n        self.model_snet = models.mobilenet_v2(pretrained=True)\n        self.model_snet.classifier = nn.Sequential(nn.Dropout(p=0.2))\n        #Dropout is a regularization technique for reducing overfitting in neural networks by preventing complex co-adaptations on training data.\n        \n        self.fc1 = nn.Linear(1280,num_classes1,bias=True) #input=1280 num_classes1(output)=10\n        #Applies a linear transformation to the incoming data, i.e. \/\/y= Ax+b\/\/.\n        #The input tensor given in forward(input) must be either a vector (1D tensor) or matrix (2D tensor).\n        '''linear1=torch.nn.Linear(N_FEATURES, hiddenLayerSize, bias=True) \n            torch.nn.init.xavier_uniform(linear1.weight)'''\n        torch.nn.init.xavier_uniform_(self.fc1.weight)\n        #weight initializer function\n        torch.nn.init.zeros_(self.fc1.bias)\n        #Returns a tensor filled with the scalar value 0, with the same size as input.\n        \n        self.fc2 = nn.Linear(1280,num_classes2,bias=True)\n        torch.nn.init.xavier_uniform_(self.fc2.weight)\n        torch.nn.init.zeros_(self.fc2.bias)\n        \n    def forward(self,x): # Forward pass: compute predicted y\n        x = self.model_snet(x)\n        out1 = self.fc1(x)\n        out2 = self.fc2(x)\n        return out1,out2","a4b48104":"net  = MyModel(10,10) # Dataset contains 10 classes of vowel & 10 consonent.","ab20431c":"net = net.to(device)","0595016f":"#to Compute accuracy\ndef evaluation(dataloader):\n    \n    total, correct = 0, 0\n    for data in dataloader:\n        #iterate over each data from dataloader \n        inputs, labels = data\n        inputs, labels = inputs.to(device), labels.to(device)\n        # Extracting Actual Labels\n        _, actual_v = torch.max(labels[:,0,:].data, 1) #actual vowel\n        _, actual_c = torch.max(labels[:,1,:].data, 1) #actual consonent\n        \n        outputs_v,outputs_c = net(inputs)\n        _, pred_v = torch.max(outputs_v.data, 1) #predicted vowel\n        _, pred_c = torch.max(outputs_c.data, 1) #predicted consonent\n        \n        total += labels.size(0)\n        correct_v = (pred_v == actual_v)*1 #correct vowel\n        correct_c = (pred_c == actual_c)*1 #correct consonent\n        correct_v[correct_v == 0] = 2 #if value = 0\n        correct_c[correct_c == 0] = 3\n        correct += ((correct_v==correct_c)).sum().item()\n    return 100 * correct \/ total # Prediction Percentage\n","5609174d":"import torch.optim as optim\n\nloss_fn = nn.CrossEntropyLoss()\n# Initialize the params, put together the arguments for the optimizer\n\nplist = [\n        {'params': net.fc1.parameters(), 'lr': 5e-3},\n        {'params': net.fc2.parameters(), 'lr': 5e-3}\n        ]\n        #parameters will use a learning rate of 5e-3\n    \nlr=0.01\n# an SGD optimizer\nopt = optim.SGD(net.parameters(),lr=0.01,momentum=0.9,nesterov=True)\n'''Once gradients have been computed using loss.backward(), calling optimizer.step() updates the parameters as defined by the optimization algorithm.'''\n# nestrov GD reduce the oscillation\n# opt = optim.Adam(my_model.parameters(),lr=0.01)","a587da32":"%%time\n#Epoch:- number times that the learning algorithm will work through the entire training dataset.\nloss_arr = []\nloss_epoch_arr = []\nmax_epochs = 10\nmin_loss = 1000\nbest_model = None\nfor epoch in range(max_epochs):\n\n    for i, data in enumerate(train_loader, 0):\n        # enumerate() returns a tuple containing a count (from start which defaults to 0) and the values obtained from iterating over iterable.\n        # get the inputs\n        inputs, labels = data\n        inputs, labels = inputs.to(device), labels.to(device)\n        labels_v = labels[:,0,:] #vowel\n        labels_c = labels[:,1,:] #consonent\n        _, actual_v = torch.max(labels_v.data, 1)\n        _, actual_c = torch.max(labels_c.data, 1)\n        opt.zero_grad() # Sets gradients of all model parameters to zero,so parameter update correctly\n        \n        outputs_v, outputs_c = net(inputs)\n        loss_v = loss_fn(outputs_v, actual_v) #loss in vowel\n        loss_c = loss_fn(outputs_c, actual_c) #loss in consonent\n        loss = torch.add(loss_v,loss_c)\n        loss.backward() #backward pass:- computes the derivative of the loss w.r.t. the parameters\n        opt.step() #causes the optimizer to take a step based on the gradients of the parameters.\n        \n        if min_loss > loss.item():\n            min_loss = loss.item()\n            best_model = copy.deepcopy(net.state_dict()) #create a fully independent clone of the original object and all of its children.\n        \n        loss_arr.append(loss.item())\n        \n        del inputs, labels, outputs_v, outputs_c\n        torch.cuda.empty_cache() ##to empty the unused memory after processing each batch\n        \n    loss_epoch_arr.append(loss.item())\n        \n    print('Epoch: %d\/%d, Test acc: %0.2f, Train acc: %0.2f' % (epoch, max_epochs, evaluation(validation_loader), evaluation(train_loader)))\n    \nnet.load_state_dict(best_model) #Selecting the best model\nplt.plot(loss_epoch_arr)\nplt.show()","52d5cda8":"plt.plot(loss_arr)\nplt.show()","af9640f0":"evaluation(validation_loader)","7d38bb07":"**Dataset to Torchvision Dataset**","1a2b6f68":"**Data Visualization**","b45176f3":"**Model**","dd89e2aa":"**import**","b13ddea4":"**Optimization**","c4277094":"**Data Loader**","c49ec7d9":"**Evaluation**","9b654fb8":"**Enabling GPU**","cd81938d":"**Training**"}}