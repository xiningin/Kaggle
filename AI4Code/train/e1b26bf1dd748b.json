{"cell_type":{"61ac0a39":"code","e74c0ad6":"code","f0c0dd61":"code","7f7dc3d7":"code","35b96fe0":"code","282f06e8":"code","77f6dc8c":"code","34180623":"code","8fd9e746":"code","78c7ba87":"code","bc34d719":"code","846e2a79":"code","b7ec4722":"code","72ca3ea7":"code","1c011800":"code","ed0f4ee0":"code","e6f01ac3":"code","79c3150f":"code","25c1f855":"code","b21fccc2":"code","59ae6bbb":"code","5a21d26f":"code","c8d9bf1b":"code","f3cc4c8c":"code","a603ccd1":"code","b9fa9946":"code","cadd3e2d":"code","951ea5c7":"code","4e94afc6":"code","892ee84d":"code","7a8ccd48":"code","f70b93ba":"code","33b7a597":"code","a5c2f77e":"code","5d894b91":"code","eb76b4e3":"code","b405e73f":"code","bea2124f":"code","d5fa682c":"code","b26e4e87":"code","cddbda5a":"code","333310c1":"code","24df4020":"code","4ef9ce83":"code","8cf1f230":"code","ac20b003":"code","5213b996":"code","5475c920":"code","f19eb06b":"code","5ee99127":"code","4f449234":"code","70597918":"code","ac56e897":"code","81c172de":"code","29c2250b":"code","c0dc0ab5":"code","e6de7fbb":"code","f23794d6":"code","be392397":"code","69d9f0af":"code","cbfa0b5d":"code","8dc5e0f8":"code","5b0ecf2b":"code","2ab57d02":"code","8bf2aeaf":"code","cb38a871":"code","9f8a0116":"code","2c3adae4":"code","e3d72849":"code","efc7d455":"code","ee2fea28":"code","e39c9879":"code","d3c490d1":"markdown","f45ecb99":"markdown","2fd3201a":"markdown","84a0fe72":"markdown","44a128f7":"markdown","baabfdce":"markdown","3ff716c3":"markdown","dbaf9a72":"markdown","bc974447":"markdown","90bdcc31":"markdown","4fa3ca90":"markdown","ef9055dd":"markdown","69d008a3":"markdown","7c7bf150":"markdown","65f77286":"markdown","3002b64f":"markdown","3272010f":"markdown","a71fdcf9":"markdown","fde2ce4b":"markdown","f7e40ecf":"markdown","69895835":"markdown"},"source":{"61ac0a39":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nfrom collections import Counter # counter\n\n# visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Machine Learning\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier, VotingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import plot_confusion_matrix\n\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","e74c0ad6":"train_data = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntest_data = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\ntest_PassengerId = test_data[\"PassengerId\"]","f0c0dd61":"train_data.head(25)","7f7dc3d7":"train_data.columns","35b96fe0":"train_data.info()","282f06e8":"train_data.isna().sum()","77f6dc8c":"train_data.describe()","34180623":"train_data.corr()","8fd9e746":"def func(pct, allvals):\n    absolute = int(pct\/100.*np.sum(allvals))\n    return \"{:.1f}%\\n({:d})\".format(pct, absolute)\n\nplt.figure(figsize=(8,8))\nwedges, texts, autotexts = plt.pie(x=train_data[\"Sex\"].value_counts(), labels=train_data[\"Sex\"].value_counts().index, autopct=lambda pct: func(pct, train_data[\"Sex\"].value_counts()))\nplt.setp(autotexts, size=13, weight=\"bold\")\nplt.rcParams.update({'font.size': 15})\nplt.title(\"Male\/Female Ratio\")\nplt.show()","78c7ba87":"categoric_list = [\"Survived\",\"Pclass\",\"SibSp\",\"Parch\",\"Embarked\"]\nfor c in categoric_list:\n    plt.figure(figsize=(8,6))\n    plt.bar(x=train_data[c].value_counts().index, height=train_data[c].value_counts())\n    plt.xticks(train_data[c].value_counts().index)\n    plt.ylabel(\"Number\")\n    plt.title(c)\n    plt.show()\n    print(train_data[c].value_counts())   ","bc34d719":"categoric_list.append(\"Sex\")\ncategoric_list.remove(\"Survived\")\n\nfor c in categoric_list:    \n    uniques_list = list(train_data[c].dropna().unique())\n    for u in uniques_list:\n        filtered = train_data.loc[train_data[c] == u]\n        plt.figure(figsize=(7,4))\n        plt.bar(x = filtered[\"Survived\"].value_counts().index, height = filtered[\"Survived\"].value_counts())\n        plt.xticks(filtered[\"Survived\"].value_counts().index)\n        plt.ylabel(\"Number\")\n        plt.xlabel(\"column name:\"+str(c)+\" \/ \"+\"unique value:\"+str(u))\n        plt.show()","846e2a79":"numeric_list = [\"Age\",\"Fare\"]\nfor numer in numeric_list:\n    plt.figure(figsize=(8,6))\n    plt.hist(train_data[numer],bins=50)\n    plt.xlabel(numer)\n    plt.title(\"{} distribution with histogram\".format(numer))\n    plt.show()    ","b7ec4722":"listem = [\"SibSp\", \"Parch\", \"Age\", \"Fare\",\"Pclass\",\"Survived\"]\nplt.figure(figsize=(8,6))\nsns.heatmap(train_data[listem].corr(), annot = True, fmt = \".2f\", linewidths=0.3)\nplt.show()","72ca3ea7":"g = sns.factorplot(x = \"SibSp\", y = \"Survived\", data = train_data, kind = \"bar\", size = 6)\ng.set_ylabels(\"Survival Probability\")\nplt.show()","1c011800":"g = sns.factorplot(x = \"Parch\", y = \"Survived\", data = train_data, kind = \"bar\", size = 6)\ng.set_ylabels(\"Survival Probability\")\nplt.show()","ed0f4ee0":"g = sns.factorplot(x = \"Pclass\", y = \"Survived\", data = train_data, kind = \"bar\", size = 6)\ng.set_ylabels(\"Survival Probability\")\nplt.show()","e6f01ac3":"g = sns.FacetGrid(train_data, col = \"Survived\", height=5)\ng.map(sns.distplot, \"Age\", bins = 20)\nplt.show()","79c3150f":"tick = list(range(int(train_data[\"Fare\"].min()),int(train_data[\"Fare\"].max()), 50))\ng = sns.FacetGrid(train_data, col = \"Survived\", height=7)\ng.map(sns.distplot, \"Fare\", bins = 20)\ng.set(xticks= tick)\nplt.show()","25c1f855":"g = sns.FacetGrid(train_data, row = \"Embarked\", col = \"Survived\", size = 3)\ng.map(sns.barplot, \"Sex\", \"Fare\")\ng.add_legend()\nplt.show()","b21fccc2":"def find_outlier(df, columns):\n    all_outliers_list = []\n    for c in columns:        \n        q25, q75 = np.percentile(df[c],25), np.percentile(df[c],75)\n        iqr = q75 - q25\n        cut_off = iqr * 1.5\n        lower, upper = q25 - cut_off, q75 + cut_off\n        outliers = train_data[(df[c] < lower) | (df[c] > upper)].index\n        all_outliers_list.extend(outliers)\n        \n    all_outliers_list = Counter(all_outliers_list)\n        \n    multiple_occurence_of_outliers = list(i for i, v in all_outliers_list.items() if v >= 2)\n    return multiple_occurence_of_outliers","59ae6bbb":"train_data.loc[find_outlier(train_data, [\"Fare\",\"SibSp\",\"Parch\"])]","5a21d26f":"my_train_data = train_data.copy()\nmy_train_data.drop(find_outlier(train_data, [\"Age\",\"Fare\",\"SibSp\",\"Parch\"]), axis = 0).reset_index(drop = True)","c8d9bf1b":"my_test_data = test_data.copy()\nlength_train = len(my_train_data)\ncombined = pd.concat([my_train_data,my_test_data], axis = 0).reset_index(drop = True)","f3cc4c8c":"combined.isna().sum()","a603ccd1":"combined[combined[\"Embarked\"].isnull()]","b9fa9946":"combined.boxplot(column = \"Fare\", by = \"Embarked\")\nplt.show()","cadd3e2d":"combined.boxplot(column = \"Pclass\", by = \"Embarked\")\nplt.show()","951ea5c7":"# Both of Box plot results show C is a good option for filling NaN value\ncombined[\"Embarked\"] = combined[\"Embarked\"].fillna(\"C\")","4e94afc6":"combined[combined[\"Fare\"].isnull()]","892ee84d":"mean_v = combined[combined[\"Pclass\"] == 3][\"Fare\"].mean()","7a8ccd48":"combined[\"Fare\"] = combined[\"Fare\"].fillna(mean_v)","f70b93ba":"combined.isna().sum()","33b7a597":"sns.factorplot(x = \"Sex\", y = \"Age\", data = combined, kind = \"box\") \nplt.show() \n# gender does not seem to correlated with age","a5c2f77e":"sns.factorplot(x = \"Pclass\", y = \"Age\", data = combined, kind = \"box\") \nplt.show() ","5d894b91":"sns.factorplot(x = \"SibSp\", y = \"Age\", data = combined, kind = \"box\") \nsns.factorplot(x = \"Parch\", y = \"Age\", data = combined, kind = \"box\") \nplt.show() ","eb76b4e3":"index_nan_age = list(combined[\"Age\"][combined[\"Age\"].isnull()].index)\nfor i in index_nan_age:\n    age_prediction = combined[\"Age\"][((combined[\"SibSp\"] == combined.iloc[i][\"SibSp\"]) &(combined[\"Parch\"] == combined.iloc[i][\"Parch\"])& (combined[\"Pclass\"] == combined.iloc[i][\"Pclass\"]))].median()\n    age_median = combined[\"Age\"].median()\n    if not np.isnan(age_prediction):\n        combined[\"Age\"].iloc[i] = age_prediction\n    else:\n        combined[\"Age\"].iloc[i] = age_median","b405e73f":"combined.isna().sum()","bea2124f":"combined[\"Name\"].head(10)","d5fa682c":"name = combined[\"Name\"]\ntitles = [i.split(\".\") for i in name]\ntitles = [i[0] for i in titles]\ntitles = [i.split(\",\") for i in titles]\ntitles = [i[1].replace(\" \",\"\") for i in titles]\ntitles","b26e4e87":"combined[\"Title\"] = titles\ncombined","cddbda5a":"plt.figure(figsize=(8,6))\nsns.countplot(x=\"Title\", data = combined)\nplt.xticks(rotation = 60)\nplt.show()","333310c1":"combined[\"Title\"] = combined[\"Title\"].replace([\"Lady\",\"the Countess\",\"Capt\",\"Col\",\"Don\",\"Dr\",\"Major\",\"Rev\",\"Sir\",\"Jonkheer\",\"Dona\"],\"other\")\ncombined[\"Title\"] = [0 if i == \"Master\" else 1 if i == \"Miss\" or i == \"Ms\" or i == \"Mlle\" or i == \"Mrs\" else 2 if i == \"Mr\" else 3 for i in combined[\"Title\"]]\ncombined.head(10)","24df4020":"g = sns.factorplot(x = \"Title\", y = \"Survived\", data = combined, kind = \"bar\")\ng.set_xticklabels([\"Master\",\"Mrs\",\"Mr\",\"Other\"])\ng.set_ylabels(\"Survival Probability\")\nplt.show()","4ef9ce83":"combined.drop(labels = [\"Name\"], axis = 1, inplace = True)","8cf1f230":"combined = pd.get_dummies(combined,columns=[\"Title\"])","ac20b003":"combined[\"Fsize\"] = combined[\"SibSp\"] + combined[\"Parch\"] + 1","5213b996":"combined[\"Fsize\"].value_counts()","5475c920":"g = sns.factorplot(x = \"Fsize\", y = \"Survived\", data = combined, kind = \"bar\")\ng.set_ylabels(\"Survival\")\nplt.show()","f19eb06b":"combined[\"family_size\"] = [1 if i < 5 else 0 for i in combined[\"Fsize\"]]","5ee99127":"combined = pd.get_dummies(combined,columns=[\"Embarked\",\"family_size\"])","4f449234":"combined.drop(labels = [\"Fsize\"], axis = 1, inplace = True)","70597918":"combined.head(10)","ac56e897":"combined[\"Pclass\"] = combined[\"Pclass\"].astype(\"category\")\ncombined[\"Sex\"] = combined[\"Sex\"].astype(\"category\")\ncombined = pd.get_dummies(combined, columns = [\"Pclass\",\"Sex\"])","81c172de":"combined.drop(labels = [\"PassengerId\", \"Cabin\",\"Ticket\"], axis = 1, inplace = True)","29c2250b":"combined.columns","c0dc0ab5":"combined.dtypes","e6de7fbb":"length_train # i will use this for splitting","f23794d6":"test = combined[length_train:]\ntest.drop(labels = [\"Survived\"],axis = 1, inplace = True)","be392397":"test","69d9f0af":"train = combined[:length_train]","cbfa0b5d":"train","8dc5e0f8":"y_train = train[\"Survived\"]\nx_train = train.drop(labels = [\"Survived\"],axis = 1)","5b0ecf2b":"X_train, X_test, Y_train, Y_test = train_test_split(x_train, y_train, test_size = 0.3, random_state=42)","2ab57d02":"print(\"X_train:\",len(X_train))\nprint(\"X_test:\",len(X_test))\nprint(\"y_train:\",len(Y_train))\nprint(\"y_test:\",len(Y_test))\nprint(\"test:\",len(test))","8bf2aeaf":"logistic_reg = LogisticRegression()\nlogistic_reg.fit(X_train,Y_train)\nscr_train = logistic_reg.score(X_train,Y_train)\nscr_test = logistic_reg.score(X_test,Y_test)\nprint(\"accuracy train: \",scr_train)\nprint(\"accuracy test: \",scr_test)","cb38a871":"logistic_reg.predict_proba(X_train) # prediction probability of each individual's survival","9f8a0116":"plot_confusion_matrix(estimator=logistic_reg, X=X_train, y_true=Y_train, cmap=plt.cm.Blues)\nplt.show()","2c3adae4":"random_state = 42\nclassifier = [DecisionTreeClassifier(random_state = random_state),\n             SVC(random_state = random_state),\n             RandomForestClassifier(random_state = random_state),\n             LogisticRegression(random_state = random_state),\n             KNeighborsClassifier()]\n\ndt_param_grid = {\"min_samples_split\" : range(10,500,20),\n                \"max_depth\": range(1,20,2)}\n\nsvc_param_grid = {\"kernel\" : [\"rbf\"],\n                 \"gamma\": [0.001, 0.01, 0.1, 1],\n                 \"C\": [1,10,50,100,200,300,1000]}\n\nrf_param_grid = {\"max_features\": [1,3,10],\n                \"min_samples_split\":[2,3,10],\n                \"min_samples_leaf\":[1,3,10],\n                \"bootstrap\":[False],\n                \"n_estimators\":[100,300],\n                \"criterion\":[\"gini\"]}\n\nlogreg_param_grid = {\"C\":np.logspace(-3,3,7),\n                    \"penalty\": [\"l1\",\"l2\"]}\n\nknn_param_grid = {\"n_neighbors\": np.linspace(1,19,10, dtype = int).tolist(),\n                 \"weights\": [\"uniform\",\"distance\"],\n                 \"metric\":[\"euclidean\",\"manhattan\"]}\nclassifier_param = [dt_param_grid,\n                   svc_param_grid,\n                   rf_param_grid,\n                   logreg_param_grid,\n                   knn_param_grid]","e3d72849":"models = [\"Decision Tree\", \"SVM\", \"Random Forest\", \"KNN\", \"Logistic Regression\"]\ncv_result = []\nbest_estimators = []\nfor i in range(len(classifier)):\n    clf = GridSearchCV(classifier[i], param_grid=classifier_param[i], cv = StratifiedKFold(n_splits = 10), scoring = \"accuracy\", n_jobs = -1,verbose = 1)\n    clf.fit(X_train,Y_train)\n    cv_result.append(clf.best_score_)\n    best_estimators.append(clf.best_estimator_)\n    print(models[i],\": \",cv_result[i])","efc7d455":"cv_results_df = pd.DataFrame({\"Cross Validation Means\":cv_result, \"Models\": models})\n\ng = sns.barplot(\"Cross Validation Means\", \"Models\", data = cv_results_df)\ng.set_xlabel(\"Mean Accuracy\")\ng.set_title(\"Scores\")\nplt.show()","ee2fea28":"votingC = VotingClassifier(estimators = [(\"dt\",best_estimators[0]), \n                                        (\"rfc\",best_estimators[2]),\n                                        (\"lr\",best_estimators[3])],\n                                        voting = \"hard\", n_jobs = -1)\nvotingC = votingC.fit(X_train, Y_train)\nprint(accuracy_score(votingC.predict(X_test),Y_test))","e39c9879":"test_survived_prediction = pd.Series(votingC.predict(test), name = \"Survived\").astype(int)\nresults = pd.concat([test_PassengerId, test_survived_prediction],axis = 1)\nresults.to_csv(\"titanic.csv\", index = False)","d3c490d1":"* Age is correlated with Pclass, SibSp and Parch","f45ecb99":"<a id = \"2\"><\/a><br>\n## Loading Data","2fd3201a":"<a id = \"4\"><\/a><br>\n## Data visualization","84a0fe72":"<a id = \"16\"><\/a><br>\n### Ensemble Learning","44a128f7":"<a id = \"14\"><\/a><br>\n### Train Test Split","baabfdce":"<a id = \"12\"><\/a><br>\n## Adding new features","3ff716c3":"<a id = \"8\"><\/a><br>\n### Plotting Correlation","dbaf9a72":"<a id = \"3\"><\/a><br>\n## Reviewing Data","bc974447":"<a id = \"10\"><\/a><br>\n## Outliers","90bdcc31":"### columns info:\n* pclass: Ticket class (1 = 1st, 2 = 2nd, 3 = 3rd)\n* sibsp : Passenger's Spouse and \/ or siblings that found in Titanic \n* parch : Passenger's Children and \/ or parents that found in Titanic\n* fare  : Ticket price\n* embarked: Passenger's boarding port (C = Cherbourg, Q = Queenstown, S = Southampton)[](http:\/\/)","4fa3ca90":"<a id = \"15\"><\/a><br>\n### Logistic Regression","ef9055dd":"<a id = \"6\"><\/a><br>\n### Plotting Survived,Pclass, SibSp, Parch, and Embarked Columns with Bar Plot","69d008a3":"<a id = \"17\"><\/a><br>\n## Final Submission","7c7bf150":"<a id = \"13\"><\/a><br>\n## Prediction Models","65f77286":"<a id = \"1\"><\/a><br>\n## Importing Data","3002b64f":"# Introduction\n\n<font color = 'blue'>\nContent: \n\n1. [Importing Data](#1)\n1. [Loading Data](#2)\n1. [Reviewing Data](#3)\n1. [Data Visualization](#4)    \n    * [Plotting Male and Female Ratio](#5)\n    * [Plotting Survived,Pclass, SibSp, Parch, and Embarked Columns with Bar Plot](#6)\n    * [Plotting Survived People Distribution](#7)\n    * [Plotting Correlation](#8)\n    * [Plotting Survival Probability](#9)\n1. [Outliers](#10)\n1. [Finding and Filling NaN Value](#11)\n1. [Adding New Features](#12)\n1. [Prediction Models](#13)    \n    * [Train Test Split](#14)\n    * [Logistic Regression](#15)\n    * [Ensemble Learning](#16)\n1. [Final Submission](#17)\n    ","3272010f":"<a id = \"11\"><\/a><br>\n## Finding and Filling NaN Value","a71fdcf9":"<a id = \"7\"><\/a><br>\n### Plotting Survived People Distribution ","fde2ce4b":"<a id = \"5\"><\/a><br>\n### Plotting Male and Female Ratio","f7e40ecf":"<a id = \"9\"><\/a><br>\n### Plotting Survival Probability","69895835":"I will use following machine learning methods:\n* Decision Tree\n* SVM\n* Random Forest\n* KNN\n* Logistic Regression"}}