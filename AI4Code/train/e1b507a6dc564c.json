{"cell_type":{"0abc4c3d":"code","6845bccf":"code","99baacf0":"code","63ea6060":"code","6c984e90":"code","e9b896ae":"code","308538da":"code","fddf4284":"code","bc233bdd":"code","7abe61c5":"code","3067111a":"code","3d4e617c":"code","fdd36d96":"code","a5f0ebbc":"markdown","a25d0965":"markdown","d3a040fc":"markdown"},"source":{"0abc4c3d":"%matplotlib inline\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom sklearn.model_selection import train_test_split\nfrom ipywidgets import  interact, interactive, fixed, interact_manual,FloatSlider","6845bccf":"train = pd.read_csv('..\/input\/train.csv')","99baacf0":"y_train = train.loc[:, train.columns == 'label'].values.flatten()\nx_train = train.loc[:, train.columns != 'label'].values\n    ","63ea6060":"# create a boolean matrix of the correct answers\ny_train_labels = [[(value == i) * 1 for i in range(0,10)] for value in y_train]","6c984e90":"BANDWIDTH = 0.2","e9b896ae":"def _pattern(input,name,feature_count,h):\n    with tf.variable_scope(name) as scope:\n        bias = tf.get_variable('bias',[feature_count, 1],initializer=tf.constant_initializer(0),dtype=tf.float32)\n        bandwidth = tf.constant(1.0\/(h * feature_count),dtype=tf.float32)\n        offset = tf.add(input, tf.transpose(bias))\n        mapping_layer = tf.map_fn(lambda x: (gaussian_tf(x)\/h),offset)\n        summing_layer = tf.reduce_sum(mapping_layer,axis=1)\n        return tf.multiply(summing_layer,bandwidth)","308538da":"\ndef model(features, labels, mode):\n\n    conv1 = tf.layers.conv2d(\n          inputs=tf.reshape(features[\"x\"], [-1, 28, 28, 1]),\n          filters=32,\n          kernel_size=[5, 5],\n          padding=\"same\",\n          activation=tf.nn.relu)\n\n    # Pooling Layer #1\n    pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=[2, 2], strides=2)\n\n    conv2 = tf.layers.conv2d(\n      inputs=pool1,\n      filters=64,\n      kernel_size=[5, 5],\n      padding=\"same\",\n      activation=tf.nn.relu)\n    pool2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=[2, 2], strides=2)\n\n    pool2_flat = tf.reshape(pool2, [-1, 7 * 7 * 64])\n\n\n    pool2_flat = tf.reshape(pool2, [-1, 7 * 7 * 64])\n    dense = tf.layers.dense(inputs=pool2_flat, units=1024, activation=tf.nn.relu)\n    dropout = tf.layers.dropout(\n        inputs=dense, rate=0.4, training=mode == tf.estimator.ModeKeys.TRAIN)\n\n    # Logits Layer\n    result = tf.layers.dense(inputs=dropout, units=10)\n\n    predictions = {\n      # Generate predictions (for PREDICT and EVAL mode)\n      \"classes\": tf.argmax(input=result, axis=1),\n      # Add `softmax_tensor` to the graph. It is used for PREDICT and by the\n      # `logging_hook`.\n      \"probabilities\": tf.nn.softmax(result, name=\"softmax_tensor\")\n    }\n    \n    if mode == tf.estimator.ModeKeys.PREDICT:\n        return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions)\n    \n    # Calculate Loss (for both TRAIN and EVAL modes)\n    loss =  tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=result, labels=labels))\n\n    # Configure the Training Op (for TRAIN mode)\n    if mode == tf.estimator.ModeKeys.TRAIN:\n        optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001)\n        train_op = optimizer.minimize(\n            loss=loss,\n            global_step=tf.train.get_global_step())\n        return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)\n\n    # Add evaluation metrics (for EVAL mode)\n    eval_metric_ops = {\n      \"accuracy\": tf.metrics.accuracy(\n          labels=tf.argmax(labels, 1), predictions=predictions[\"classes\"])}\n    return tf.estimator.EstimatorSpec(\n      mode=mode, loss=loss, eval_metric_ops=eval_metric_ops)\n","fddf4284":"classifier = tf.estimator.Estimator(\n    model_fn=model, model_dir=\".\/pnn_feedfoward_model\")","bc233bdd":"# Set up logging for predictions\ntensors_to_log = {\"probabilities\": \"softmax_tensor\"}\nlogging_hook = tf.train.LoggingTensorHook(\n    tensors=tensors_to_log, every_n_iter=50)","7abe61c5":"# Train the model\ntrain_input_fn = tf.estimator.inputs.numpy_input_fn(\n    x={\"x\": x_train.astype(np.float32)},\n    y=np.array(y_train_labels).astype(np.float32),\n    batch_size=2000,\n    num_epochs=None,\n    shuffle=True)\n\nclassifier.train(\n    input_fn=train_input_fn,\n    steps=100,\n    hooks=[logging_hook])\n","3067111a":"\n\n# Evaluate the model and print results\n# eval_input_fn = tf.estimator.inputs.numpy_input_fn(\n#     x={\"x\": x_test.astype(np.float32)},\n#     y=np.array(y_test_labels).astype(np.float32),\n#     num_epochs=1,\n#     shuffle=False)\n# eval_results = classifier.evaluate(input_fn=eval_input_fn)\n# print(eval_results)\n\n","3d4e617c":"test = pd.read_csv('..\/input\/test.csv').values","fdd36d96":"eval_input_fn = tf.estimator.inputs.numpy_input_fn(\n    x={\"x\": test.astype(np.float32)},\n    num_epochs=1,\n    shuffle=False)\nresult = pd.DataFrame([ {'ImageId':index + 1,'Label':i['classes']} for index,i in enumerate(classifier.predict(input_fn=eval_input_fn))])\n# result.to_csv('results.csv')\nresult.to_csv('submission.csv', index=False)","a5f0ebbc":"## Sources\n\nhttps:\/\/www.sciencedirect.com\/science\/article\/pii\/089360809090049Q\n\nhttps:\/\/stats.stackexchange.com\/questions\/244012\/can-you-explain-parzen-window-kernel-density-estimation-in-laymans-terms","a25d0965":"## Building Model","d3a040fc":"# Training"}}