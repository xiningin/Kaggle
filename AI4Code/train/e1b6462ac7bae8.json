{"cell_type":{"00ba5d6f":"code","2478d5cc":"code","818de3d4":"code","1df6612f":"code","2d5f554e":"code","58b744db":"code","6704eb32":"code","ddc96c42":"code","fe19e65b":"code","da841463":"code","efe5dc04":"code","b107e7b7":"code","81edb453":"code","89b60f59":"code","634e84d1":"code","6f16480c":"code","76be8a83":"code","5740a69f":"code","bd7a12fc":"code","aef9a55f":"code","57dac366":"code","47c8ab02":"code","e7612111":"code","8998f76f":"code","b1b621aa":"code","e9ea9e54":"markdown","27e9a5c0":"markdown","fde3a422":"markdown","b451c28c":"markdown","eda722e4":"markdown","281d41eb":"markdown","b4c7251a":"markdown","7d249ca7":"markdown","924e5bb6":"markdown","6769409b":"markdown","01b01203":"markdown","08fffb7e":"markdown","79ed04f1":"markdown","80db0803":"markdown","fae0311c":"markdown","260c8730":"markdown","6c88341f":"markdown","268a6727":"markdown"},"source":{"00ba5d6f":"\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.preprocessing import  LabelEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import RobustScaler\n\nfrom xgboost import XGBRegressor\nfrom sklearn.metrics import r2_score\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","2478d5cc":"# Read the training dataset\ntrain=pd.read_csv(\"\/kaggle\/input\/a-fine-windy-day-hackerearth-ml-challenge\/train_data.csv\")\ntrain.head()","818de3d4":"# Read the testing Dataset\ntest=pd.read_csv(\"\/kaggle\/input\/a-fine-windy-day-hackerearth-ml-challenge\/test_data.csv\")\ntest.head()","1df6612f":"# nan values in training dataset\ntrain.isnull().sum()","2d5f554e":"# Nan values in testing dataset\ntest.isnull().sum()","58b744db":"# Encoding training data\/\ns=[]\n# Encoding each column og object datatype\nfor f in train.columns: \n    if train[f].dtype=='object':\n        s.append(f)\n        lbl = LabelEncoder() \n        lbl.fit(list(train[f].values)) \n        train[f] = lbl.transform(list(train[f].values))\n        \ntrain.head()\n","6704eb32":"# Encoding testing dataset\ns_test=[]\n# Encoding each column og object datatype\nfor f in test.columns: \n    if test[f].dtype=='object':\n        s_test.append(f)\n        lbl = LabelEncoder() \n        lbl.fit(list(test[f].values)) \n        test[f] = lbl.transform(list(test[f].values))","ddc96c42":"# train dataset\nimputer = SimpleImputer()\nfor i in s:\n# filling the nan values\n    data = np.array(train[i], dtype=object)\n    train[i]=imputer.fit_transform(data.reshape(-1,1))","fe19e65b":"# test dataset\nimputer = SimpleImputer()\nfor i in s:\n# filling the nan values\n    data = np.array(test[i], dtype=object)\n    test[i]=imputer.fit_transform(data.reshape(-1,1))","da841463":"train.isnull().sum()","efe5dc04":"test.isnull().sum()","b107e7b7":"def get_redundant_pairs(train):\n    '''Get diagonal and lower triangular pairs of correlation matrix'''\n    pairs_to_drop = set()\n    cols = train.columns\n    for i in range(0, train.shape[1]):\n        for j in range(0, i+1):\n            pairs_to_drop.add((cols[i], cols[j]))\n    return pairs_to_drop\n\ndef get_top_abs_correlations(train, n=5):\n    au_corr = train.corr().abs().unstack()\n    labels_to_drop = get_redundant_pairs(train)\n    au_corr = au_corr.drop(labels=labels_to_drop).sort_values(ascending=False)\n    return au_corr[0:n]\n\nprint(\"Top Absolute Correlations\")\nprint(get_top_abs_correlations(train, 3))","81edb453":"df=train.drop(['tracking_id', 'datetime', \"motor_torque(N-m)\"],axis=1)\ndf.head()","89b60f59":"df_test=test.drop(['tracking_id', 'datetime', \"motor_torque(N-m)\"],axis=1)\ndf_test.head()","634e84d1":"def impute_nan(df,variable):\n    df[variable]=df[variable]\n    ##It will have the random sample to fill the na\n    random_sample=df[variable].dropna().sample(df[variable].isnull().sum(),random_state=0)\n    ##pandas need to have same index in order to merge the dataset\n    random_sample.index=df[df[variable].isnull()].index\n    df.loc[df[variable].isnull(),variable]=random_sample\n","6f16480c":"for i in df.columns.to_list():\n    if df[i].isnull:\n        impute_nan(df,i)\n    else:\n        pass","76be8a83":"for i in df_test.columns.to_list():\n    if df_test[i].isnull:\n        impute_nan(df_test,i)\n    else:\n        pass","5740a69f":"df_test.isnull().sum()","bd7a12fc":"df.isnull().sum()","aef9a55f":"# create x and y\nY = df['windmill_generated_power(kW\/h)']\nX = df.drop(['windmill_generated_power(kW\/h)'],axis=1)","57dac366":"scaler = RobustScaler()\nX = scaler.fit_transform(X)\ndf_test=scaler.fit_transform(df_test)","47c8ab02":"x_train,x_test,y_train,y_test = train_test_split(X,Y,train_size=0.86,random_state=42)\nprint(x_train.shape,y_train.shape)\nprint(x_test.shape,y_test.shape)","e7612111":"xgb = XGBRegressor(n_estimators=600,max_depth=6,booster='gbtree',n_jobs=10,\n                   learning_rate=0.1,reg_lambda=0.3,reg_alpha=0.4)\nxgb.fit(x_train,y_train)\ny_train_pred = xgb.predict(x_train)\ny_test_pred = xgb.predict(x_test)\nprint(r2_score(y_train,y_train_pred))\nprint(r2_score(y_test,y_test_pred))","8998f76f":"df_sub = test[['tracking_id','datetime']]\n\n# save the predictions on new dataset\nresults = xgb.predict(df_test)\ndf_sub['windmill_generated_power(kW\/h)'] = results","b1b621aa":"df_sub.head()","e9ea9e54":"#### We have Removed all the Null values in categorical variables","27e9a5c0":"Creating X and Y ","fde3a422":"## Building up the model","b451c28c":"## Read in the data","eda722e4":"## Finding correlation","281d41eb":"Building the model ","b4c7251a":"### Encoding each column with categorical values","7d249ca7":"## Importing required modules","924e5bb6":"# Getting the results on test data","6769409b":"## Handeling NaN values for continuous data using random Sampeling","01b01203":"## Handel NaN values for categorical data","08fffb7e":"#### All NaN values are filled","79ed04f1":"Splitting data into train and test set","80db0803":"# Feature Engineering","fae0311c":"# Thanks","260c8730":"### Removing Unnecessary Rows","6c88341f":"Scaling down the data","268a6727":"### Handeling Nan values"}}