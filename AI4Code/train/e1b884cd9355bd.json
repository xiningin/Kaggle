{"cell_type":{"c2153c22":"code","c257bff3":"code","9f34163d":"code","ea4ca109":"code","d88fb46d":"code","5a5800e5":"code","cb0dc8dc":"code","bc7a9dae":"code","2a270e3d":"code","a0007581":"code","70a9ac3a":"code","5a414134":"code","63a63a8a":"code","8489fc2f":"code","ce54d5da":"code","55291b4e":"code","f20d206c":"code","6ed463f5":"code","c76e5ae6":"code","a71ab2a9":"code","49285c96":"code","d90e1a4c":"code","4cba4c08":"code","0bf90f8e":"code","c1e7b855":"code","8d3f1aad":"code","a97a18ba":"code","ba9c588c":"code","31436eb0":"code","259f3e92":"code","84906836":"code","e350f4e7":"code","d04efa17":"code","0e901b95":"code","5046e35e":"code","2252636c":"code","429b3daf":"code","6ca95e2b":"code","f67139c3":"code","86b289e2":"code","3a5b0ea5":"code","5c14ff41":"code","37311e94":"code","2b0e35ff":"code","bd9241f1":"code","85a31117":"code","80ca497b":"code","74ac162d":"code","ae2f04d1":"code","12298ef9":"code","0210987b":"code","e5c5548d":"code","bb49113a":"code","34e66771":"code","42dd0f0d":"code","61ab025d":"code","6b706321":"code","0bcdbb43":"code","cd408e66":"code","dff9eac2":"code","0d438fee":"code","e384137e":"code","b42df65c":"code","476be948":"code","ee367ded":"code","d5b33219":"code","4a966c9b":"code","4fee6264":"code","23b48edb":"code","b9e9e574":"code","30798724":"code","cffd04b5":"code","35e545a5":"code","3c0e7808":"code","ea32e20b":"code","e61f6cdd":"code","8e3c170e":"code","bdb58e77":"code","513f3366":"code","ec1536c3":"code","94b676d2":"code","168a6e7b":"code","8e495240":"code","cef42de1":"code","ade4fcb0":"code","7a757819":"code","600b1266":"code","b72985de":"code","59a72c90":"code","943648f4":"markdown","022c7d2f":"markdown","dc7287fa":"markdown","c656919f":"markdown","8da4ec3b":"markdown","5a0bc37a":"markdown","bc648084":"markdown","53b0a5c3":"markdown","3731e7da":"markdown","5d959c73":"markdown","c9a259e5":"markdown"},"source":{"c2153c22":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c257bff3":"import pandas as pd\nimport numpy as np \nimport tensorflow as tf\nimport os\nimport re\nimport matplotlib.pyplot as plt\nimport seaborn as sns","9f34163d":"data_dir='..\/input\/200000-abstracts-for-seq-sentence-classification\/20k_abstracts_numbers_with_@\/'\n","ea4ca109":"filenames=[data_dir + filename for filename in os.listdir(data_dir)]\nfilenames","d88fb46d":"# Creating a function to read the txt Files\n# This function returns all the lines in the txt file as a list\ndef get_lines(filename):\n    with open(filename,\"r\") as f:\n        return f.readlines()","5a5800e5":"# Read the Lines in the Training Set\ntrain_data_lines=get_lines(data_dir+\"train.txt\")\n# train_data_lines[:20]","cb0dc8dc":"# Preprocessing Functions\n# Returns a list of dictionaries of abstract's lines\n# Dict Format --> {'TARGET':'Background\/Results\/Objetive\/Concludion','Text':'The actual statement'}\ndef preprocess_data(filename):\n    input_lines=get_lines(filename)\n    #This will be used to separte the abstracts from  one another using String mets\n    abstract_lines=\"\"\n    # Empty list of abstracts\n    abstract_samples=[]\n    for line in input_lines:\n        # Check for a new abstract\n        if line.startswith(\"###\"):\n            abstract_id=line\n            # And since we are in a new abstract we will Reset the abstract_lines\n            abstract_lines=\"\"\n        # Check for a new line \\n escape seq\n        elif line.isspace():\n            # Split the Lines of the abstract and will return a list of one abstract\n            abstract_line_split=abstract_lines.splitlines()\n            # Now we have to iterate through this singular abstract\n            for abstract_line_number, abstract_line in enumerate(abstract_line_split):\n                #  Enumerate() method adds a counter to an iterable and returns it in a form of enumerating object.\n                # Create a empty Dict per line\n                line_data={}\n                # Split on the tab \\t esc seq\n                target_text_split=abstract_line.split(\"\\t\")\n                # Get the Label of the sentence as the Label\n                line_data[\"target\"]=target_text_split[0]\n                # Get the Text of the Lien as the Text Key\n                line_data[\"text\"]=target_text_split[1].lower()\n                # Also adding the Line Nnumber as it will also aid the model\n                line_data[\"line_number\"]=abstract_line_number\n                # Number of Lines in that particular abstract\n                line_data[\"total_lines\"]=len(abstract_line_split)-1\n                # Now we have to append them to the absract_samples list\n                abstract_samples.append(line_data)\n        # So if both the cases are not there then the line is a labelled sentence\n        else:\n            abstract_lines+=line\n    return abstract_samples\n                \n            \n            ","bc7a9dae":"%%time\n# Get the data and preprocess it\ntrain_samples=preprocess_data(data_dir+\"train.txt\")\nval_samples=preprocess_data(data_dir+\"dev.txt\")\ntest_samples=preprocess_data(data_dir+\"test.txt\")","2a270e3d":"len(train_samples),len(val_samples),len(test_samples)","a0007581":"# Visualizing the Data\ntrain_samples[:20]","70a9ac3a":"# Now we have to turn this data into a df\ntrain_df=pd.DataFrame(train_samples)\ntest_df=pd.DataFrame(test_samples)\nval_df=pd.DataFrame(val_samples)","5a414134":"train_df.head(11)","63a63a8a":"# Checking the Spread of Data\ntrain_df.target.value_counts()","8489fc2f":"# Checking the Length of Lines\ntrain_df.total_lines.plot.hist()","ce54d5da":"# Get the list of just the text Columns\ntrain_sentences=train_df[\"text\"].tolist()\ntest_sentences=test_df[\"text\"].tolist()\nval_sentences=val_df[\"text\"].tolist()","55291b4e":"len(train_sentences),len(test_sentences),len(val_sentences)","f20d206c":"# Turning the target Labels into Numeric Data\n# We have 5 main labels -> Backgroun, Objective,Methods, Results, Conclusion\n# We'll encode them both 1HEC and Simple Nuemrical\nfrom sklearn.preprocessing import OneHotEncoder\n# Tensorflow is incompaible with sparse matrices\none_hot_encoder=OneHotEncoder(sparse=False)\n# You should reshape your X to be a 2D array not 1D array. Fitting a model requires requires a 2D array. i.e (n_samples, n_features)\ntrain_labels_one_hot=one_hot_encoder.fit_transform(train_df[\"target\"].to_numpy().reshape(-1,1))\nval_labels_one_hot=one_hot_encoder.fit_transform(val_df[\"target\"].to_numpy().reshape(-1,1))\ntest_labels_one_hot=one_hot_encoder.fit_transform(test_df[\"target\"].to_numpy().reshape(-1,1))\ntrain_labels_one_hot,val_labels_one_hot,test_labels_one_hot","6ed463f5":"# Label Encoding also for Baseline Model\nfrom sklearn.preprocessing import LabelEncoder\nle=LabelEncoder()\ntrain_labels_encoded=le.fit_transform(train_df[\"target\"])\ntest_labels_encoded=le.fit_transform(test_df[\"target\"])\nval_labels_encoded=le.fit_transform(val_df[\"target\"])\ntrain_labels_encoded, test_labels_encoded,val_labels_encoded","c76e5ae6":"# Retieving classes \nnum_classes=len(le.classes_)\nclass_names=le.classes_\nnum_classes,class_names","a71ab2a9":"# Turning them to tensors\n# Baseline Model\n# tfidf turns text into Numbers with the Formula\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.pipeline import Pipeline\n# Creating a Pipeline\n# A pipeline takes Multiple Tuples\nmodel_0=Pipeline([\n    (\"tf-idf\",TfidfVectorizer()),\n    (\"clf\",MultinomialNB())\n    \n])\nmodel_0.fit(train_sentences,train_labels_encoded)","49285c96":"# Evaluation and Prediction\n# In scikit learn it is score for eval\nmodel_0.score(val_sentences,val_labels_encoded)","d90e1a4c":"# Make Predictions for the Baseline Model\nbaseline_predictions=model_0.predict(val_sentences)\nbaseline_predictions","4cba4c08":"from sklearn.metrics import accuracy_score\nfrom sklearn.metrics import precision_recall_fscore_support\n# Models for Calculating different evaluation metrics\n# Returns a dict of different metrics\ndef calculate_results(y_true, y_pred):\n  # Calculate model accuracy\n  model_accuracy = accuracy_score(y_true, y_pred) * 100\n  # Calculate model precision, recall and f1 score using \"weighted average\n  model_precision, model_recall, model_f1, _ = precision_recall_fscore_support(y_true, y_pred, average=\"weighted\")\n  model_results = {\"accuracy\": model_accuracy,\n                  \"precision\": model_precision,\n                  \"recall\": model_recall,\n                  \"f1\": model_f1}\n  return model_results","0bf90f8e":"calculate_results(val_labels_encoded,baseline_predictions)\n","c1e7b855":"# Vectorize the text and then create Embeddings\nfrom tensorflow.keras import layers\n# How long is each sentence on average\nsent_lens=[len(sentence.split()) for sentence in train_sentences]\navg_sent_lens=np.mean(sent_lens)\navg_sent_lens\n# sent_lens\n#  So we will need Padding and Truncating as the input shapes must be maintained","8d3f1aad":"plt.hist(sent_lens,bins=9)","a97a18ba":"# Calculate the percentile of length of sentences\noutput_seq_len=int(np.percentile(sent_lens,95))\noutput_seq_len\n# So 95% sentences are in length of 55","ba9c588c":"# Creating a text Vectorization Layer\n# Mapping our text from words to Numbers\n# An embedding is a relatively low-dimensional space into which you can translate high-dimensional vectors. \n# Vocabulary size in the Research Paper is 68000\nmax_tokens=68000\nfrom tensorflow.keras.layers.experimental.preprocessing import TextVectorization\ntext_vectorizer=TextVectorization(max_tokens=max_tokens,output_sequence_length=output_seq_len)\n","31436eb0":"# Adapt the Text Vectorizer to the Training Data\n# We have to adapt it to only the training data so that val and test data are not seen\n# Later it can be fitted to the two latter\ntext_vectorizer.adapt(train_sentences)","259f3e92":"# Finding out how many words are there  in the training vocabulary and which are  most common\n# Also text vectorizer works pretty straightforwardly, 1 to most common word, 2 to 2nd most common word and so on\ntrain_vocab=text_vectorizer.get_vocabulary()\n# Size of Vocab\nprint(len(train_vocab))\n# 5 Most Common Words in the Vocab\nprint(train_vocab[:5])\n# Least common 5 words in the vocab\nprint(train_vocab[-5:])","84906836":"# Get the config of our Text Vectorizer\ntext_vectorizer.get_config()","e350f4e7":"# Create an Embedding Layer\n# More output dims , more emmbedding, more parameters to train\n# Masking the 0 considering themm as padding\ntoken_embed=layers.Embedding(input_dim=len(train_vocab),output_dim=128,mask_zero=True,name=\"token_embedding\")","d04efa17":"# Creating a Fast Loadinng Dataset with tf data API\n# https:\/\/www.tensorflow.org\/guide\/data_performance\n# https:\/\/www.tensorflow.org\/guide\/data\n# Turn our data into Tensorflow datasets\ntrain_dataset=tf.data.Dataset.from_tensor_slices((train_sentences,train_labels_one_hot))\nval_dataset=tf.data.Dataset.from_tensor_slices((val_sentences,val_labels_one_hot))\ntest_dataset=tf.data.Dataset.from_tensor_slices((test_sentences,test_labels_one_hot))\ntrain_dataset\n# <TensorSliceDataset shapes: ((), (5,)), types: (tf.string, tf.float64)>\n# Which indicates one Text Sample in first tuple, next tuple is (0,0,0,0,1) -> 1hc ","0e901b95":"# Pre fetching the data and making them into batches\n# Pre fetching reduces the Preparation time of Data taken by CPU\n# Pref-fetching in a Multi-threaded way Reduces time and Increases the amount of data as all cores can be utilized to Prepare the Data\n# The GPU will do the Computation\ntrain_dataset=train_dataset.batch(32).prefetch(tf.data.AUTOTUNE)\nval_dataset=val_dataset.batch(32).prefetch(tf.data.AUTOTUNE)\ntest_dataset=test_dataset.batch(32).prefetch(tf.data.AUTOTUNE)\ntrain_dataset\n# Run the Previous steps as well this otherwie the Shapes will not be fixed\n# PrefetchDataset shapes: ((None,), (None, 5)), types: (tf.string, tf.float64)","5046e35e":"# Creating the Model\ninputs=layers.Input(shape=(1,),dtype=tf.string)\ntext_vectors=text_vectorizer(inputs)\ntoken_embedding=token_embed(text_vectors)\nx=layers.Conv1D(64,kernel_size=5,padding=\"same\",activation=\"relu\")(token_embedding)\nx=layers.GlobalAveragePooling1D()(x)\noutputs=layers.Dense(num_classes,activation=\"softmax\")(x)\n# Indirect way of creating the Modelling the op ip\nmodel_1=tf.keras.Model(inputs,outputs)\n# Compiling the Model\nmodel_1.compile(loss=\"categorical_crossentropy\",optimizer=tf.keras.optimizers.Adam(),metrics=[\"accuracy\"])\n\n","2252636c":"model_1.summary()","429b3daf":"history_model_1=model_1.fit(train_dataset,epochs=5,validation_data=val_dataset)","6ca95e2b":"model_1.evaluate(val_dataset)","f67139c3":"# Making Predictions In terms of Probabilities\nmodel_1_prediction_probability=model_1.predict(val_dataset)\nmodel_1_prediction_probability\n# For all 30k statements our Model will output a 5 len list of Prediction Probability\n# And out of the 5 the index that is higher is the one in which our class thinks the \n# Sentence belongs","86b289e2":"# Now converting the Probabilities to classes\nmodel_1_prediction=tf.argmax(model_1_prediction_probability,axis=1)\nmodel_1_prediction","3a5b0ea5":"model_1_results=calculate_results(y_true=val_labels_encoded,y_pred=model_1_prediction)\nmodel_1_results","5c14ff41":"\n!pip install huggingface","37311e94":"# Model 2: Feature Extractor with pretrained token Embeddings\n# This is done to leverage the power of Transfer Learning\nimport tensorflow_hub as hub\ntf_hub_embedding_layers=hub.KerasLayer(\"https:\/\/tfhub.dev\/google\/universal-sentence-encoder\/4\",trainable=False,name=\"universal_sentence_encoder\")","2b0e35ff":"# Creating the Model\n# For the Tensorflow embedding we are using the input shape needs to be in the form of an empty list\ninputs = layers.Input(shape=[], dtype=tf.string)\npretrained_embedding = tf_hub_embedding_layers(inputs)\nx=layers.Dense(128,activation=\"relu\")(pretrained_embedding)\nx=layers.Dense(128,activation=\"relu\")(x)\n# Softmax because we are doing multiclass \noutputs=layers.Dense(5,activation=\"softmax\")(x)\nmodel_2=tf.keras.Model(inputs,outputs,name=\"model_2_transfer_learning\")\n# Compiling the Model\nmodel_2.compile(loss=\"categorical_crossentropy\",optimizer=tf.keras.optimizers.Adam(),metrics=[\"accuracy\"])\nmodel_2.summary()","bd9241f1":"# Fitting the Model\nhistory_model_2=model_2.fit(train_dataset,epochs=9,validation_data=val_dataset)","85a31117":"# Evaluating on the Validation Set\nmodel_2.evaluate(val_dataset)","80ca497b":"# Making Predictions In terms of Probabilities\nmodel_2_prediction_probability=model_2.predict(val_dataset)\nmodel_2_prediction_probability\n# For all 30k statements our Model will output a 5 len list of Prediction Probability\n# And out of the 5 the index that is higher is the one in which our class thinks the \n# Sentence belongs","74ac162d":"# Now converting the Probabilities to classes\nmodel_2_prediction=tf.argmax(model_2_prediction_probability,axis=1)\nmodel_2_prediction","ae2f04d1":"model_2_results=calculate_results(y_true=val_labels_encoded,y_pred=model_2_prediction)\nmodel_2_results","12298ef9":"# So we first need to build a Character Layer Tokenizer \ndef split_chars(text):\n    return \" \".join(list(text))\n# Text Splitting \ntrain_chars=[split_chars(sentence) for sentence in train_sentences]\ntrain_chars[:5]","0210987b":"val_chars=[split_chars(sentence) for sentence in val_sentences]\ntest_chars=[split_chars(sentence) for sentence in test_sentences]","e5c5548d":"# Finding the Average Character Length\nchar_lens=[len(sentence) for sentence in train_sentences]\nmean_char_len=np.mean(char_lens)\nmean_char_len","bb49113a":"# The meann is not the best way to gauge so we need a distribution and should take 95% CI\nplt.hist(char_lens,bins=6)\n\n# Finding the 95% CI Length\nci_char_len=int(np.percentile(char_lens,95))\nci_char_len\n# So 300 Characters must work","34e66771":"# Setting up a Text Vectorization Layer\n# Getting the vocab size\nimport string\nalphanumeric=string.ascii_lowercase+string.digits+string.punctuation\nalphanumeric","42dd0f0d":"# +2 for space and OOV UNK Token\nmax_char_vocab=len(alphanumeric)+2\nchar_vectorizer=TextVectorization(max_tokens=max_char_vocab,output_sequence_length=ci_char_len+10,name=\"char_vectorizer\")\n# Adapt it to the Training set of sequences\nchar_vectorizer.adapt(train_chars)","61ab025d":"char_vocab=char_vectorizer.get_vocabulary()\nprint(len(char_vocab))\nchar_vocab[:5],char_vocab[-5:]","6b706321":"# Make an Embedding Layer\n# As per the paper we have to make a 25 dim long Feature vector for Output dims\n# Each character gets embedded into a size 25 Feature Vector\n# Dont Mask Model \nchar_embed=layers.Embedding(input_dim=len(char_vocab),output_dim=25,mask_zero=False,name=\"character_embedding\")\n ","0bcdbb43":"# Creating character level Fast Loading Datasets\ntrain_char_dataset = tf.data.Dataset.from_tensor_slices((train_chars, train_labels_one_hot)).batch(32).prefetch(tf.data.AUTOTUNE)\nval_char_dataset = tf.data.Dataset.from_tensor_slices((val_chars, val_labels_one_hot)).batch(32).prefetch(tf.data.AUTOTUNE)\nval_char_dataset=tf.data.Dataset.from_tensor_slices((val_chars,val_labels_one_hot)).batch(32).prefetch(tf.data.AUTOTUNE)\n","cd408e66":"train_char_dataset","dff9eac2":"# Building a BI-LSTM Model with the Character Level Embeddings\n# Also experimenting with a Conv1D with Char level Embeddings\n# Make Conv1D on chars only\ninputs = layers.Input(shape=(1,), dtype=\"string\")\nchar_vectors = char_vectorizer(inputs)\nchar_embeddings = char_embed(char_vectors)\nx = layers.Conv1D(64, kernel_size=5, padding=\"same\", activation=\"relu\")(char_embeddings)\nx = layers.GlobalMaxPool1D()(x)\noutputs = layers.Dense(num_classes, activation=\"softmax\")(x)\nmodel_3 = tf.keras.Model(inputs=inputs,\n                         outputs=outputs,\n                         name=\"model_3_conv1D_char_embedding\")\n# Compile model\nmodel_3.compile(loss=\"categorical_crossentropy\",\n                optimizer=tf.keras.optimizers.Adam(),\n                metrics=[\"accuracy\"])","0d438fee":"model_3.summary()","e384137e":"model_3_history = model_3.fit(train_char_dataset,\n                               epochs=6,\n                               validation_data=val_char_dataset)","b42df65c":"model_3.evaluate(val_char_dataset)","476be948":"# Making Predictions In terms of Probabilities\nmodel_3_prediction_probability=model_3.predict(val_char_dataset)\nmodel_3_prediction_probability\n# For all 30k statements our Model will output a 5 len list of Prediction Probability\n# And out of the 5 the index that is higher is the one in which our class thinks the \n# Sentence belongs","ee367ded":"# Now converting the Probabilities to classes\nmodel_3_prediction=tf.argmax(model_3_prediction_probability,axis=1)\nmodel_3_prediction","d5b33219":"model_3_results=calculate_results(y_true=val_labels_encoded,y_pred=model_3_prediction)\nmodel_3_results","4a966c9b":"# Building a Tf Dataset to load it faster\n# Combine chars and tokens into a dataset\ntrain_char_token_data = tf.data.Dataset.from_tensor_slices((train_sentences, train_chars)) # make data\ntrain_char_token_labels = tf.data.Dataset.from_tensor_slices(train_labels_one_hot) # make labels\ntrain_char_token_dataset = tf.data.Dataset.zip((train_char_token_data, train_char_token_labels)) # combine data and labels\n\n# Prefetch and batch train data\ntrain_char_token_dataset = train_char_token_dataset.batch(32).prefetch(tf.data.AUTOTUNE) \n# Preparing the same for Validation set\nval_char_token_data = tf.data.Dataset.from_tensor_slices((val_sentences, val_chars))\nval_char_token_labels = tf.data.Dataset.from_tensor_slices(val_labels_one_hot)\nval_char_token_dataset = tf.data.Dataset.zip((val_char_token_data, val_char_token_labels))\nval_char_token_dataset = val_char_token_dataset.batch(32).prefetch(tf.data.AUTOTUNE)","4fee6264":"# Buidling a Multi Input Model that can accpet multiple data streams\n# So we will build two BI-LSTM Models one with char embeddings and token Embeddings\n# Also there is going to be a dropout introduced \n# One more difference is we will be using the functional API instead or Sequential API\n# What it does is that it will go through steps together and not sequentially\n# So first we will crate a Token Model\ntoken_inputs=layers.Input(shape=[],dtype=tf.string,name=\"token_input\")\ntoken_embeddings = tf_hub_embedding_layers(token_inputs)\ntoken_output = layers.Dense(128, activation=\"relu\")(token_embeddings)\ntoken_model = tf.keras.Model(token_inputs,token_output)","23b48edb":"# Then we will create a Character Model\nchar_inputs = layers.Input(shape=(1,), dtype=tf.string, name=\"char_input\")\nchar_vectors = char_vectorizer(char_inputs)\nchar_embeddings = char_embed(char_vectors)\n# The one with the char embeddings is passed through a BI-LSTM Model acc. to the Paper\nchar_bi_lstm = layers.Bidirectional(layers.LSTM(25))(char_embeddings) \nchar_model = tf.keras.Model(inputs=char_inputs, outputs=char_bi_lstm)","b9e9e574":"# Concatenating both the Models as mentioned\ntoken_char_concat = layers.Concatenate(name=\"token_char_hybrid\")([token_model.output,char_model.output])","30798724":"# Creating Combined Dropout and a dense Layer and then some dropout\ncombined_dropout = layers.Dropout(0.5)(token_char_concat)\ncombined_dense = layers.Dense(200, activation=\"relu\")(combined_dropout) \nfinal_dropout = layers.Dropout(0.5)(combined_dense)\n# Final Output, Label Pediction Layers\noutput_layer = layers.Dense(num_classes, activation=\"softmax\")(final_dropout)","cffd04b5":"# 5. Construct model with char and token inputs\n# 2 Inputs Token Model Input and Char Model Input \nmodel_4 = tf.keras.Model(inputs=[token_model.input, char_model.input],outputs=output_layer,name=\"model_4_token_and_char_embeddings\")","35e545a5":"model_4.summary()","3c0e7808":"# Plot hybrid token and character model and check if it has replicated the Paper\nfrom tensorflow.keras.utils import plot_model\nplot_model(model_4,show_shapes=True)","ea32e20b":"# Compiling the Model\n# None is the batch size in the shapes\nmodel_4.compile(loss=\"categorical_crossentropy\",optimizer=tf.keras.optimizers.SGD(),metrics=[\"accuracy\"])","e61f6cdd":"# Fit the model on tokens and chars\nmodel_4_history = model_4.fit(train_char_token_dataset,epochs=5,validation_data=val_char_token_dataset)","8e3c170e":"# Caryying out similar exercise for Model 4\n# model_4_prediction_probability=model_4.predict(val_char_dataset)\n# model_4_prediction_probability\n# model_4_prediction=tf.argmax(model_4_prediction_probability,axis=1)\n# model_4_prediction\n# model_4_results=calculate_results(y_true=val_labels_encoded,y_pred=model_4_prediction)\n# model_4_results","bdb58e77":"# Feature Engineering\n# Here we will take non obvious features from the Data and encoding them Numerically to aid the model\n# The order of Objectives, Methods, Results conclusion sequence is a non obvious feature\n# So we need to inject them into the Model and this is a part of the Feature Engineering\n# Encoding the Line Numbers\n# Engineering features incorporated need to be avaialable at test time\n# Positional Embeddings\ntrain_df[\"line_number\"].value_counts()\n# Almost 10K Abstracts have 10 + Lines","513f3366":"# Distribution\n# Using TF To create one hot encoded tensors of Line Numbers\n# They can be also used as it is the Lines they are but 1HC is better\ntrain_line_numbers_one_hot=tf.one_hot(train_df[\"line_number\"].to_numpy(),depth=15)\nval_line_numbers_one_hot=tf.one_hot(val_df[\"line_number\"].to_numpy(),depth=15)\ntest_line_numbers_one_hot=tf.one_hot(test_df[\"line_number\"].to_numpy(),depth=15)\ntrain_line_numbers_one_hot[:10],train_line_numbers_one_hot.shape","ec1536c3":"# We need to this similarly for Total Lines\ntrain_df[\"total_lines\"].value_counts()\n# We can take the Cutoff to e 20","94b676d2":"train_total_lines_one_hot=tf.one_hot(train_df[\"total_lines\"].to_numpy(),depth=20)\nval_total_lines_one_hot=tf.one_hot(val_df[\"total_lines\"].to_numpy(),depth=20)\ntest_total_lines_one_hot=tf.one_hot(test_df[\"total_lines\"].to_numpy(),depth=20)\ntrain_total_lines_one_hot,train_total_lines_one_hot.shape","168a6e7b":"# Buidling the Model\n# Token Level Model + Character Level Model + Positional Models (2) -> Line Numbers and Total Lines\n# Then we will Concatenate Model 1 and Model 2 \n# Then Combine 3,4 and Combination of 1&2 \n# Create an Output Layer\n# # # Model 1 # # #\ntoken_inputs=layers.Input(shape=[],dtype=tf.string,name=\"token_inputs\")\ntoken_embeddings=tf_hub_embedding_layers(token_inputs)\ntoken_outputs=layers.Dense(128,activation=\"relu\")(token_embeddings)\ntoken_model=tf.keras.Model(token_inputs,token_outputs)\n# Model 2 \nchar_inputs=layers.Input(shape=(1,),dtype=tf.string,name=\"char_inputs\")\nchar_vectors=char_vectorizer(char_inputs)\nchar_embeddings=char_embed(char_vectors)\n# 24 cos 25-26 is the vocab size and we need multiples of 8\nchar_bilstm=layers.Bidirectional(layers.LSTM(24))(char_embeddings)\nchar_model=tf.keras.Model(char_inputs,char_bilstm)\n\n#  Model 3 and Model 4\n# shape =(15,) as we have taken depth or size of line number till 15\nline_number_inputs=layers.Input(shape=(15,),dtype=tf.float32,name=\"line_number_inputs\")\nx=layers.Dense(32,activation=\"relu\")(line_number_inputs)\nline_number_model=tf.keras.Model(line_number_inputs,x)\n\ntotal_lines_inputs=layers.Input(shape=(20,),dtype=tf.float32,name=\"total_lines_inputs\")\ny=layers.Dense(32,activation=\"relu\")(total_lines_inputs)\ntotal_lines_model=tf.keras.Model(total_lines_inputs,y)\n\n# Concatenating the 1st and 2nd Model\ncombined_embeddings=layers.Concatenate(name=\"Hybrid_Merge_12\")([token_model.output,char_model.output])\n# Applying the Dropouts\nz=layers.Dense(256,activation=\"relu\")(combined_embeddings)\nz=layers.Dropout(0.5)(z)\n\n# Combining all of the above \nfinal_embeddings=layers.Concatenate(name=\"char_token_positional_embeddings\")([line_number_model.output,total_lines_model.output,z])\n\n# Creating the Output Layer for accepting the above layers and giving Probabilities\n\noutput_layer=layers.Dense(5,activation=\"softmax\",name=\"output_layer\")(final_embeddings)\n# Creating the Model\nmodel_5=tf.keras.Model([line_number_model.input,total_lines_model.input,token_model.input,char_model.input],output_layer,name=\"model_5_tribrid_embedding_model\")","8e495240":"# comma pachi je hoy e batch size maate hoy shapes ma\nmodel_5.summary()","cef42de1":"# Plot hybrid token and character model and check if it has replicated the Paper\nfrom tensorflow.keras.utils import plot_model\nplot_model(model_5,show_shapes=True)","ade4fcb0":"model_5.compile(loss=tf.keras.losses.CategoricalCrossentropy(label_smoothing=0.2),\n               optimizer=tf.keras.optimizers.SGD(),\n               metrics=[\"accuracy\"])\n# Label Smoothing is a regularization technique that introduces noise for the labels.","7a757819":"# Creating Fast Loading Datasets for Model 5 with tf.data API\n# ##### IMP IMP IMP IMP IMP IMP IMP IMP IMP IMP ###########\n# The orders of line number, total lines, tokens, chars has been and must be maintained throughout the Model Operations\ntrain_char_token_pos_data=tf.data.Dataset.from_tensor_slices((train_line_numbers_one_hot,train_total_lines_one_hot,train_sentences,train_chars))\ntrain_char_token_pos_labels=tf.data.Dataset.from_tensor_slices(train_labels_one_hot)\ntrain_char_token_pos_dataset=tf.data.Dataset.zip((train_char_token_pos_data,train_char_token_pos_labels))\ntrain_char_token_pos_dataset=train_char_token_pos_dataset.batch(32).prefetch(tf.data.AUTOTUNE)\n\nval_char_token_pos_data=tf.data.Dataset.from_tensor_slices((val_line_numbers_one_hot,val_total_lines_one_hot,val_sentences,val_chars))\nval_char_token_pos_labels=tf.data.Dataset.from_tensor_slices(val_labels_one_hot)\nval_char_token_pos_dataset=tf.data.Dataset.zip((val_char_token_pos_data,val_char_token_pos_labels))\nval_char_token_pos_dataset=val_char_token_pos_dataset.batch(32).prefetch(tf.data.AUTOTUNE)\n\ntest_char_token_pos_data=tf.data.Dataset.from_tensor_slices((test_line_numbers_one_hot,test_total_lines_one_hot,test_sentences,test_chars))\ntest_char_token_pos_labels=tf.data.Dataset.from_tensor_slices(test_labels_one_hot)\ntest_char_token_pos_dataset=tf.data.Dataset.zip((test_char_token_pos_data,test_char_token_pos_labels))\ntest_char_token_pos_dataset=test_char_token_pos_dataset.batch(32).prefetch(tf.data.AUTOTUNE)\n","600b1266":"train_char_token_pos_dataset","b72985de":"history_model_5 = model_5.fit(train_char_token_pos_dataset,\n                              epochs=9,\n                              validation_data=val_char_token_pos_dataset)\n","59a72c90":"# Caryying out similar exercise for Model 4\nmodel_5_prediction_probability=model_5.predict(val_char_dataset)\nmodel_5_prediction_probability\nmodel_5_prediction=tf.argmax(model_5_prediction_probability,axis=1)\nmodel_5_prediction\nmodel_5_results=calculate_results(y_true=val_labels_encoded,y_pred=model_5_prediction)\nmodel_5_results","943648f4":"<h3> Building a Model with Token + Character Embedding Model <\/h3>\n<h4> Concatenating Models ","022c7d2f":"During training, some number of layer outputs are randomly ignored or \u201cdropped out.\u201d This has the effect of making the layer look-like and be treated-like a layer with a different number of nodes and connectivity to the prior layer. In effect, each update to a layer during training is performed with a different \u201cview\u201d of the configured layer.","dc7287fa":"<h3>Pretained Token Embeddings + Character Emebeddings + Positional Embeddings <\/h3>","c656919f":"<h3> Building a Model with Character level Embeddings <\/h3>","8da4ec3b":"<h3>Preprocessing the Data","5a0bc37a":"This is the Project that I've been working on while taking this [Course](https:\/\/www.udemy.com\/course\/tensorflow-developer-certificate-machine-learning-zero-to-mastery\/learn\/)","bc648084":"In the paper they have further used this BI-LSTM and then concatenated and then again passed it through another BILSTM Model.","53b0a5c3":"<h3>  Model -> Naive Bayes Model <\/h3> <br>\n<h4> TF-IDF Multinomial Naive Bayes Classifier <\/h4>","3731e7da":"<h3> Conv 1D Model <\/h3>","5d959c73":"<h3> Tensorflow Hub Pre trained Embedding and Feature Extractor <\/h3>","c9a259e5":" <h4>\n Pre-processing the Data <br>\n List of dictionaries <br>\n Each line needs to be converted into a dictionary <br>\n Dictionary items are ordered, changeable, and does not allow duplicates.<br>\n Objectives, Methods, Results, Conclusins will all be Values to the 'TARGET' KEY<br>\n And the Text corresponding to them will be Values to the Key 'TEXT'<br>\n So each abstract would have about roughly 10-12 dicts for each statement<br>"}}