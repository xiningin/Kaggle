{"cell_type":{"f565ba7b":"code","4ceefc2a":"code","c0ed57b2":"code","d506e2c1":"code","95af653a":"code","c83e2a30":"code","c951316c":"code","1f35c165":"code","a797818c":"code","dc8118f2":"code","7e080e5a":"code","0efb59c9":"code","a8122e29":"code","5439e456":"code","c1f20930":"code","d96ed421":"code","e09b0990":"markdown","5d02775b":"markdown","1ba221c9":"markdown","87ed4f38":"markdown","2a232c95":"markdown","570e9238":"markdown","e18ef655":"markdown","7aee36da":"markdown","6c2d70c7":"markdown","f7a6c490":"markdown"},"source":{"f565ba7b":"# importing necessary libraries\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nimport os\n\nfrom tensorflow.keras.layers import Dense, Input, Conv2D, MaxPooling2D, UpSampling2D, Conv2DTranspose\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.preprocessing import image\n\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\n\n%matplotlib inline","4ceefc2a":"IMAGES_DIR = \"..\/input\/utkface-new\/UTKFace\"\nall_images = []\nfor i in os.listdir(IMAGES_DIR):\n    img = Image.open(os.path.join(IMAGES_DIR, i))\n    img = img.resize((64,64),Image.ANTIALIAS)\n    img = np.array(img) \/ 255.0\n    all_images.append(img)","c0ed57b2":"train_images = np.array(all_images[:20000])\ntest_images = np.array(all_images[20000:])\n\nprint(f\"Train Images: {train_images.shape}\")\nprint(f\"Test Images: {test_images.shape}\")","d506e2c1":"print(\"TRAIN DATA\".center(65))\nplt.figure(figsize=(8,8))\nfor i in range(9):\n    plt.subplot(3,3,i+1)\n    plt.imshow(train_images[i])\nplt.show()","95af653a":"print(\"TEST DATA\".center(65))\nplt.figure(figsize=(8,8))\nfor i in range(9):\n    plt.subplot(3,3,i+1)\n    plt.imshow(test_images[i])\nplt.show()","c83e2a30":"INPUT_LAYER = Input(shape = train_images.shape[1:], name=\"INPUT\")\n\nx = Conv2D(64,(3,3),activation='relu',padding=\"same\")(INPUT_LAYER)\nx = MaxPooling2D()(x)\nx = Conv2D(32,(3,3),activation='relu',padding=\"same\")(x)\nx = MaxPooling2D()(x)\n\nencoded = Conv2D(32,(3,3),activation='relu',padding=\"same\", name=\"CODE\")(x)\n\nencoder_model = Model(INPUT_LAYER, encoded)","c951316c":"encoder_model.summary()","1f35c165":"DECODER_INPUT = Input(shape = (16,16,32), name = \"DECODER_INPUT\")\n\nx = Conv2DTranspose(16,(3,3),activation='relu',padding='same')(DECODER_INPUT)\nx = UpSampling2D((2,2))(x)\nx = Conv2DTranspose(8,(3,3),activation='sigmoid',padding='same')(x)\nx = UpSampling2D((2,2))(x)\n\ndecoded = Conv2D(3, (3,3), padding='same', name=\"OUTPUT\")(x)\n\ndecoder_model = Model(DECODER_INPUT, decoded)","a797818c":"decoder_model.summary()","dc8118f2":"AE_MODEL_INPUT = Input(shape = train_images.shape[1:])\nencoder = encoder_model(AE_MODEL_INPUT)\ndecoder = decoder_model(encoder)\n\nautoencoder_model = Model(AE_MODEL_INPUT, decoder)\nautoencoder_model.compile(optimizer = \"adam\", loss=\"mse\")\nautoencoder_model.summary()","7e080e5a":"autoencoder_model.fit(\n    train_images,\n    train_images,\n    epochs = 100,\n    batch_size = 32,\n    validation_data = (test_images, test_images)\n)","0efb59c9":"plt.figure(figsize = (10,10))\nprint(\"ORIGINAL IMAGES\".center(65))\nfor i in range(9):\n    plt.subplot(3,3,i+1)\n    plt.imshow(test_images[i])\nplt.show()","a8122e29":"plt.figure(figsize = (10,10))\npred_images = autoencoder_model.predict(test_images[:9])\nprint(\"GENERATED IMAGES\".center(65))\nfor i in range(9):\n    plt.subplot(3,3,i+1)\n    plt.imshow(pred_images[i])\nplt.show()","5439e456":"generated_encodings = encoder_model.predict(test_images)\ngenerated_images = decoder_model.predict(generated_encodings)\nprint(f\"Generated Image Encodings: {generated_encodings.shape}\")\nprint(f\"Generated Images: {generated_images.shape}\")","c1f20930":"NUMBER_OF_TEST_IMAGES = len(test_images)\nmse = mean_squared_error(test_images.reshape(NUMBER_OF_TEST_IMAGES, 64*64*3), generated_images.reshape(NUMBER_OF_TEST_IMAGES, 64*64*3))\nmae = mean_absolute_error(test_images.reshape(NUMBER_OF_TEST_IMAGES, 64*64*3), generated_images.reshape(NUMBER_OF_TEST_IMAGES, 64*64*3))\nrmse = np.sqrt(mean_squared_error(test_images.reshape(NUMBER_OF_TEST_IMAGES, 64*64*3), generated_images.reshape(NUMBER_OF_TEST_IMAGES, 64*64*3)))\n\nprint(\"Mean Squared Error:\", mse)\nprint(\"Mean Absolute Error:\", mae)\nprint(\"Root Mean Squared Error:\", rmse)","d96ed421":"plt.figure(figsize = (11,7))\nx = [\"MSE\",\"MAE\",\"RMSE\"]\ny = [mse,mae,rmse]\nplt.barh(x, y)\n  \nfor index, value in enumerate(y):\n    plt.text(value, index,\n             str(value)[:7])\nplt.show()","e09b0990":"# A study on Autoencoders for Face Image Regeneration","5d02775b":"## What are Autoencoders?\nAutoencoders are special type of neural networks for whcih the input is same as output. These autoencoders try to compress the input into lower dimensional representation and try to reconstruct the image from the lowest representation of the images. This lower dimensional representation is also called as latent space representation. Autoencoders are very useful in dimensionality reduction, feature extraction, image copression etc. There are three major parts in an autoencoder:\n\n1. Encoder\n2. Code\n3. Decoder\n\nFirst the Input passes through the encoder and produces a compressed representation of the input image called Code. This lower dimensional representation or latent space representation is sent to the decoder model which then produces the similar image using only the Code (latent space representation of the image).\n\n<br\/>\n<div align=\"center\">\n\n<img src=\"https:\/\/miro.medium.com\/max\/875\/1*44eDEuZBEsmG_TCAKRI3Kw@2x.png\" alt=\"Autoencoder Architecture\"\/>\n<\/div>\n<br\/>\n\nAutoecoders are data specific i.e. autoencoders perform well only on the data on which they are trained on. This makes autoencoders impractical as a general technique.\n\n<br\/>\n    \nFor more information, please refer this article: <a href=\"https:\/\/towardsdatascience.com\/applied-deep-learning-part-3-autoencoders-1c083af4d798\">Autoencoders Article Link<\/a>","1ba221c9":"<b>Here we can see that how similar are the generated images with the original images. Training for more number of epochs and tweaking the parameters such as the learning rate, number of layers can improve the accuracy. We can even use transfer learning architectures for better results. <br\/>\nWe can also generate the encodings only for the input images which can be later used by the decoder model to generate the original images from the code representation of the images. Let us look into the code part for Code generation for the images.\n<\/b>","87ed4f38":"## Autoencoder implementation for face image regeneration","2a232c95":"### Creating the Autoencoder (Encoder + Decoder)","570e9238":"### Creating the Encoder Model","e18ef655":"### Testing the Autoencoder","7aee36da":"### Creating the Decoder Model","6c2d70c7":"### Evaluating the Autoencoder Model","f7a6c490":"### In this notebook I have explained:\n1. What are Autoencoders?\n2. Implementation of an Autoencoder for Face Image Regeneration.\n\n<b>Please do UPVOTE if you liked this notebook and gained some knowledge from it.<\/b>"}}