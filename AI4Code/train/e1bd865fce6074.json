{"cell_type":{"406351d1":"code","8db1ea19":"code","6eda1ce0":"code","cfe45930":"code","f0640833":"code","90165e74":"code","2fb25722":"code","3675832d":"code","a616cd7b":"code","63a27fb6":"code","f01b388b":"code","2ce3547c":"code","fd551105":"code","4a0caa78":"code","cfd62109":"code","f4a81689":"code","e3054dfc":"code","3f2b7a08":"code","6d9a3676":"code","aea1585e":"code","37fb51fd":"code","5575e4e5":"code","c5f1555c":"code","3129df86":"code","a75ba11c":"code","b9fb54a1":"code","776f027f":"code","cc936665":"code","dcc7e240":"code","2c646493":"code","74ef4854":"code","8cb7227b":"code","3c1eb0da":"code","c2fa146d":"code","b4153c1a":"code","58d4e8c0":"code","02bf25a6":"code","4a55d535":"code","d0bb2341":"code","dc16ce1a":"code","0ab5f762":"code","c4f934a7":"code","9a3d9a00":"code","363b33aa":"code","2da5ae88":"code","68cf573d":"code","3d218046":"code","71778cca":"code","6d2d8ad8":"code","a6673653":"code","c1181016":"code","7dc338f5":"markdown","985b257c":"markdown","56b0fc2a":"markdown","bd5ba47f":"markdown","34408c7a":"markdown","2cfdf5f6":"markdown","76c8a9b2":"markdown","ab1b5b0f":"markdown","7d3f6bc5":"markdown","3a8813de":"markdown","9a372834":"markdown","cf75173f":"markdown","88333eaf":"markdown","5b398205":"markdown","7f3705e1":"markdown","c6379ccc":"markdown","65123f75":"markdown","4c0b4cb7":"markdown","50fb1bcc":"markdown","4f87e781":"markdown","4df6c203":"markdown","97e29c1f":"markdown","a7eaa537":"markdown","c0c07c1e":"markdown","bd29b61d":"markdown","3c3e0bd7":"markdown"},"source":{"406351d1":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","8db1ea19":"import matplotlib.pyplot as plt \nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, cross_val_predict, StratifiedKFold, StratifiedShuffleSplit, RandomizedSearchCV\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler\nfrom sklearn.linear_model import SGDClassifier, LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, precision_recall_curve, roc_curve, roc_auc_score\n\nsns.set()\n\n%matplotlib inline","6eda1ce0":"original_data = pd.read_csv(\"..\/input\/red-wine-quality-cortez-et-al-2009\/winequality-red.csv\")\nmain_data = original_data.copy()\nmain_data.head(5)","cfe45930":"main_data.shape","f0640833":"main_data.describe()","90165e74":"main_data.info()","2fb25722":"main_data['quality'].value_counts().sort_index()","3675832d":"main_data.replace({'quality':{3:1, 4:1, 5:2, 6:2, 7:3, 8:3 }}, inplace = True)\nmain_data['quality'].value_counts().sort_index()","a616cd7b":"sns.countplot(x = 'quality', data = main_data)","63a27fb6":"main_data_grouped = main_data.groupby('quality').median()\nmain_data_grouped","f01b388b":"plt.figure(figsize=(16, 8))\nfor i, col in enumerate(main_data.columns[:-1]):\n    plt.subplot(3, 4, i + 1)\n    sns.barplot(x = main_data_grouped.index,\n            y = col, data = main_data_grouped, label=col)\n    plt.xlabel('')\n    plt.ylabel('')\n    plt.legend(loc='upper center')\n","2ce3547c":"plt.figure(figsize=(16, 8))\nfor i, col in enumerate(main_data.columns[:-1]):\n    plt.subplot(3, 4, i + 1)\n    sns.histplot(x = col, kde = True,\n            data = main_data, label = col)\n    plt.xlabel('')\n    plt.ylabel('')\n    plt.legend(loc = 'upper center')","fd551105":"features = main_data.drop('quality', axis = 1)\nlabels = main_data['quality']\n\ntrain_data, test_data, train_label, test_label = train_test_split(features, \n            labels ,test_size = 0.25, random_state = 0)\n\nprint(train_data.shape )\nprint(train_label.shape )\nprint(test_data.shape )\nprint(test_label.shape )\ntrain_data.head()","4a0caa78":"def Scaler(data):\n    #scale = MinMaxScaler()\n    scale = StandardScaler()\n    return pd.DataFrame(scale.fit_transform(data), \n                        columns = data.columns)\n       \ntrain_data, test_data = map(Scaler, [train_data, test_data])","cfd62109":"plt.figure(figsize=(16, 8))\nfor i, col in enumerate(train_data.columns):\n    plt.subplot(3, 4, i + 1)\n    sns.histplot(x = col, kde = True,\n            data = train_data, label = col)\n    plt.xlabel('')\n    plt.ylabel('')\n    plt.legend(loc = 'upper center')","f4a81689":"def Clf_score(clf, train_data, train_label):\n    print(\"{0:<10s}{1:*>20s}\".format('', ''))\n    print(\"Classifier is : \", str(clf)[:str(clf).find('(')])\n    \n    clf.fit(train_data, train_label)\n    print(\"\\nClassifier score : %.4f\" %(clf.score(train_data, train_label)))\n    cross_val = cross_val_score(clf, train_data, train_label, \n                                cv = 5, scoring = 'accuracy')\n    print(\"5 Fold Cross Validation accuracy is: %.2f\" %(cross_val.mean()))\n    print('\\n')\n    ","e3054dfc":"sgd_clf = SGDClassifier(random_state = 0, early_stopping=True)\nlog_clf = LogisticRegression(random_state = 0)\nsvm_clf = SVC(random_state = 0)\ntree_clf = DecisionTreeClassifier(random_state = 0)\nrdf_clf = RandomForestClassifier(bootstrap=False ,random_state = 0)\ngrd_clf = GradientBoostingClassifier(random_state = 0)\n\nclassifiers = [sgd_clf, log_clf, svm_clf, tree_clf, \n              rdf_clf, grd_clf]\n","3f2b7a08":"for clf in classifiers:\n    Clf_score(clf, train_data, train_label)","6d9a3676":"## Grid Search for SGD Classifier\n\n# param_grid = [{'loss': ['hinge', 'log', 'modified_huber',\n#     'squared_hinge'], 'alpha':[0.0001, 0.001, 0.01, 0.1]}]\n\n# grid_search = GridSearchCV(sgd_clf, param_grid=param_grid, \n#                            cv = 5)\n# grid_search.fit(train_data, train_label)\n# print(grid_search.best_params_)\n\n# {'alpha': 0.01, 'loss': 'log'}","aea1585e":"# SVM Grid Search\n\n# param_grid = [{'gamma': ['scale','auto'], \n#               'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n#               'C' : [10, 1, 0.1]\n#               }]\n\n\n# grid_search = GridSearchCV(svm_clf, param_grid=param_grid, \n#                            cv = 5)\n# grid_search.fit(train_data, train_label)\n# print(grid_search.best_params_)\n\n# {'C': 1, 'gamma': 'auto', 'kernel': 'rbf'}\n","37fb51fd":"# Random Forest Grid Search\n# param_grid = [{'n_estimators': [15, 25, 35], \n#               'min_samples_leaf' : [1, 2],\n#               'criterion':['gini', 'entropy']}\n#               ]\n\n# grid_search = GridSearchCV(rdf_clf, param_grid=param_grid, cv = 5)\n\n# grid_search.fit(train_data, train_label)\n# print(grid_search.best_params_)\n\n# {'criterion': 'entropy', 'min_samples_leaf': 2, 'n_estimators': 35}","5575e4e5":"sgd_clf = SGDClassifier(loss='log', alpha = 0.01, early_stopping=True, \n                    random_state = 0)\n\nsvm_clf = SVC(gamma = 'auto', kernel= 'rbf', C = 1, random_state = 0)\n\nrdf_clf = RandomForestClassifier(criterion='entropy' ,min_samples_leaf = 2,\n            n_estimators= 35, random_state = 0, bootstrap=False)\n\nclassifiers = [sgd_clf ,svm_clf, rdf_clf]\nfor clf in classifiers:\n    Clf_score(clf, train_data, train_label)","c5f1555c":"y_pred_svm = cross_val_predict(svm_clf, train_data, train_label, \n                           cv = 5)\ny_pred_rdf = cross_val_predict(rdf_clf, train_data, train_label, \n                           cv = 5)","3129df86":"conf_mat_svm = confusion_matrix(train_label, y_pred_svm)\nconf_mat_rdf = confusion_matrix(train_label, y_pred_rdf)\nprint(\"SVM Confusion Matrix: \\n\", conf_mat_svm)\nprint(\"SVM Confusion Matrix: \\n\", conf_mat_rdf)","a75ba11c":"sns.heatmap(conf_mat_rdf, cmap = 'gist_gray_r')","b9fb54a1":"y_pred = rdf_clf.predict(test_data)\ntest_accuracy_score = accuracy_score(test_label, y_pred)\nprint(\"Accuracy on test data is: \\n\", test_accuracy_score)","776f027f":"test_conf_mat = confusion_matrix(test_label, y_pred)\nprint(\"Test result Confusion Matrix: \\n\", test_conf_mat)","cc936665":"sns.heatmap(test_conf_mat, cmap = 'gist_gray_r')","dcc7e240":"new_data = original_data.replace({'quality':{3:0, 4:0, 5:0, 6:1, 7:1, 8:1}})\n\nprint(new_data['quality'].value_counts().sort_index())\nnew_label = new_data['quality']\nnew_feature = new_data.drop('quality', axis=1)","2c646493":"X_train, X_test, y_train, y_test = train_test_split(new_feature, \n            new_label ,test_size = 0.25, random_state = 0)\nprint(X_train.shape )\nprint(y_train.shape )\nprint(X_test.shape )\nprint(y_test.shape )\nX_train.head()","74ef4854":"sgd_clf = SGDClassifier(random_state = 0, early_stopping=True)\nsvm_clf = SVC(random_state = 0)\ntree_clf = DecisionTreeClassifier(random_state = 0)\nrdf_clf = RandomForestClassifier(bootstrap=False ,random_state = 0)\ngrd_clf = GradientBoostingClassifier(random_state = 0)\n\nclassifiers = [sgd_clf, svm_clf, tree_clf, \n              rdf_clf, grd_clf]\nfor clf in classifiers:\n    Clf_score(clf, X_train, y_train)","8cb7227b":"# # Random Forest Grid Search\n# param_grid = [{'n_estimators': [10, 15 , 25], \n#               'min_samples_leaf' : [1, 2, 3],\n#               'max_depth': [None, 8, 16],\n#               'min_samples_split': [2, 3],\n#               'max_features' : [\"auto\", \"sqrt\", \"log2\"],\n#               'criterion':['gini', 'entropy']}\n#               ]\n\n# grid_search = GridSearchCV(rdf_clf, param_grid=param_grid, cv = 5)\n\n# grid_search.fit(X_train, y_train)\n# print(grid_search.best_params_)\n\n# {'criterion': 'entropy', 'max_depth': None, 'max_features': 'auto', \n#  'min_samples_leaf': 2, 'min_samples_split': 2, 'n_estimators': 15}","3c1eb0da":"#SVM Grid Search\n# param_grid = [{'gamma': ['scale','auto'], \n#               'kernel': ['linear', 'rbf', 'sigmoid'],\n#               'C' : [50 , 10,1, 0.1, 0.05]   \n#               }]\n\n\n# grid_search = GridSearchCV(svm_clf, param_grid=param_grid, \n#                             cv = 5, verbose=3)\n\n# grid_search.fit(X_train, y_train)\n# print(grid_search.best_params_)\n\n# {'C': 10, 'gamma': 'scale', 'kernel': 'linear'}","c2fa146d":"svm_clf = SVC(gamma = 'scale', kernel= 'linear', C = 10, random_state = 0 \n              )\n\nrdf_clf = RandomForestClassifier(criterion='entropy' ,min_samples_leaf = 2,\n           max_depth=None, max_features='auto', min_samples_split = 2,\n           n_estimators= 15, random_state = 0, bootstrap=False\n           )\n\nClf_score(rdf_clf, X_train, y_train)\nClf_score(svm_clf, X_train, y_train)","b4153c1a":"y_pred_svm = cross_val_predict(svm_clf, X_train, y_train, \n                           cv = 5)\ncon_mat_svm = confusion_matrix(y_train, y_pred_svm)\nprint(con_mat_svm)","58d4e8c0":"sns.heatmap(con_mat_svm, cmap = 'gist_gray_r')","02bf25a6":"print(\"Precision Score is: %.4f\" %precision_score(y_train, y_pred_svm))\nprint(\"Recall Score is: %.4f\" %recall_score(y_train, y_pred_svm))","4a55d535":"def Plot_percision_recall(precision, recall, tresh):\n    plt.figure(figsize=(10, 6))\n    plt.plot(tresh, precision[:-1], 'k--', \n                 label = 'precision')\n    plt.plot(tresh, recall[:-1], 'b-', \n                 label = 'recall')\n    plt.xlabel('Treshold')\n    plt.legend()","d0bb2341":"y_probs = cross_val_predict(svm_clf, X_train, y_train, cv = 5, \n                          method = 'decision_function')\nprecision, recall, tresh = precision_recall_curve(y_train, y_probs)\n","dc16ce1a":"Plot_percision_recall(precision[:-20], recall[:-20], tresh[:-20])","0ab5f762":"def Plot_ROC(fpr, tpr, name):\n    plt.plot(fpr, tpr, label = name)\n    plt.plot([0, 1], [0, 1], 'k--', label = 'random classifier')\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.legend()","c4f934a7":"fpr, tpr, tresh = roc_curve(y_train, y_probs)\nplt.figure(figsize=(8, 6))\nPlot_ROC(fpr, tpr, 'Support Vector Machine')","9a3d9a00":"Clf_score(rdf_clf, X_train, y_train)\ny_pred_f = cross_val_predict(rdf_clf, X_train, y_train, \n                           cv = 5)","363b33aa":"con_mat_rdf = confusion_matrix(y_train, y_pred_f)\nprint(con_mat_rdf)","2da5ae88":"sns.heatmap(con_mat_rdf, cmap = 'gist_gray_r')","68cf573d":"print(\"Precision Score is: %.4f\" %precision_score(y_train, y_pred_f))\nprint(\"Recall Score is: %.4f\" %recall_score(y_train, y_pred_f))","3d218046":"y_probs_f = cross_val_predict(rdf_clf, X_train, y_train, cv = 5, \n                          method = 'predict_proba')\nprecision, recall, tresh = precision_recall_curve(y_train, y_probs_f[:, 1])","71778cca":"Plot_percision_recall(precision, recall, tresh)","6d2d8ad8":"fpr_f, tpr_f, tresh_f = roc_curve(y_train, y_probs_f[:, 1])","a6673653":"plt.figure(figsize=(8, 6))\nPlot_ROC(fpr, tpr, 'Support Vector Machine')\nPlot_ROC(fpr_f, tpr_f, 'Random Forest')\n\nprint(\"Area under %s ROC Curve is: %.2f\" %('SVM', roc_auc_score(y_train, y_pred_svm)))\nprint(\"Area under %s ROC Curve is: %.2f\" %('Random Forest', roc_auc_score(y_train, y_pred_f)))\n","c1181016":"y_pred_rdf = rdf_clf.predict(X_test)\ntest_accuracy_score = accuracy_score(y_test, y_pred_rdf)\nprint(\"Accuracy on test data is: \\n\", test_accuracy_score)","7dc338f5":"I first fit the following list of classifiers on the data with default hyperparameters to find out how accurate they are. Based on the results, I pick some of them and tweak their hyperparameters to improve their performance. ","985b257c":"This graph shows that some of the features, like volatile acidity or citric acid, might be more important than others, while some of the features are at the same level. However, median itself cannot be taken to account alone, and when we consider standard deviation of the distribution, the feature importance might change.The graph below shows the distribution of the data.","56b0fc2a":"The next cell shows that the accuracy improved slightly after tuning the hyperparameters.","bd5ba47f":"This graph shows that most of our data show some level of normal distribution, but some of them are highly skewed. At the same time, the scale of the features are quite different, and we must adjust the scaling.","34408c7a":"We can now see that all of the features are normalised.","2cfdf5f6":"Scaling the data using standard scale, one can use MinMax scaler aswell by uncommenting it, but, standard scaler gives better results.","76c8a9b2":"Now it's time to see how the classifier performs on the test data. We can see its performance is 87.5% and even better than the cross-validation score.","ab1b5b0f":"Fortunately, all the data are numeric, and there is no null or missing value.","7d3f6bc5":"We then create our train and test set; 75% train and development, 25% test.","3a8813de":"The result on the test data shows almost 82% accuracy, which is a good result based on the type of classification and the type of data we have.","9a372834":"We can see that the majority of our data is in group 2. So, we will probably encounter some problems in training other categories. First, we can group all the features to based on their quality to find more information about the data, and visualise them to have a better understanding.","cf75173f":"The following chart shows the same types of errors with the test set. ","88333eaf":"The following function takes classifier, data and labels and fits the classifier on the training data, then prints out the cross-validation result.","5b398205":"There are six wine quality classes. However, the majority of our data belong to classes 5, 6. So we clearly do not have enough data to train for all the classes. I think dividing the data into three classes might be a good idea. Therefore, I divide the qualities into 3 group: low:(3, 4) =1, medium(5, 6)=2 and high (7,8)=3.","7f3705e1":"Let's visualise the data.","c6379ccc":"I use SVM and Random forest to analyse the type of error they make.","65123f75":"**Thank you for taking the time, and wish you the best of luck!**","4c0b4cb7":"Random Forest's precision and recall scores:","50fb1bcc":"This chart compares the area under the ROC curve for both classifiers. It is clear Random Forest has a better performance.","4f87e781":"We first need to explore the data and get some insight.","4df6c203":"The following plot shows heatmap chart for Random Forest. The classifier is quite good at detecting class 2, but its performance significantly reduces detecting the other two groups. The reason is that we do not have enough data to train the classifier.","97e29c1f":"In this section, we calculate precision and recall scores for both SVM and Random Forest and plot their charts. Random Forest again outperforms SVM, and therefore we use it for the final prediction.","a7eaa537":"Most of our data are categorised in the quality groups of (5, 6). Therefore, the medium group of the previous section is more than 80% of the data alone. That's why we did not have enough data to train the classifier. Another way is to consider two groups of low and high quality. In this way, we have more or less equal groups and data.  \nThe Following part shows exactly the same process carried out before, just for two groups this time.\n","c0c07c1e":"The confusion matrix's results indicate that they are pretty much performing at the same level. However, Random Forest has slightly outperformed SVM and has better accuracy, so we use it for final prediction.","bd29b61d":"I selected Stochastic Gradient Decent, SVM and Random Forest among the classifiers and performed grid search on them. The following cells show the results. I have commented them out to speed up the running time, but I will use the results. Remember, we don't use the test data at all as we do not want our classifier to see the test data until the last step.","3c3e0bd7":"This chart shows ROC plot. Our aim is to maximise the area under ROC curve as it indicates better results. The area under ROC curve is higher for Random Forest classifier."}}