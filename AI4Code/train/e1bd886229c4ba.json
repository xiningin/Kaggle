{"cell_type":{"18ce4556":"code","65ba3568":"code","e43518ff":"code","9940d048":"code","f6b17e03":"code","6d062f31":"code","8444a34d":"code","7c894f4d":"code","63250e22":"code","8187e5d5":"code","c0bad8ce":"code","c724df07":"code","7a64e59f":"code","8e5d9a47":"code","b2ce1d9c":"code","2884272e":"code","d1ad0998":"code","cd5cc71c":"code","3c84019f":"code","9985f452":"code","b941a78a":"code","3cf1e121":"code","64c81b2a":"code","401b0c3d":"code","f91cf05f":"code","3cd31fc8":"code","79259f83":"code","4e6857ce":"code","44579ba8":"code","b57efddc":"code","3337f575":"code","f761afd4":"code","766946a8":"code","f5ed5e41":"code","500188cf":"code","6d1bba21":"code","b604b769":"code","9d3476f9":"code","c4b0b475":"code","e9bddce4":"code","f22670ac":"code","8d5f72ce":"code","2d7c9989":"code","72c4fde0":"code","48ab527e":"code","39d264f6":"code","62bd7544":"code","be491e9e":"code","5915b576":"markdown","f3e2137d":"markdown","f0f9ea7f":"markdown","4a34b383":"markdown","7896ea37":"markdown","54b034ad":"markdown","856c136c":"markdown","e0cb419c":"markdown","f394bf85":"markdown","d2948cc2":"markdown","b32072fc":"markdown","31642b88":"markdown","de7c7b07":"markdown","6619d233":"markdown","fed902d3":"markdown","f3be8217":"markdown","efe99515":"markdown","78bb50c7":"markdown","95005b74":"markdown"},"source":{"18ce4556":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\nimport matplotlib.pyplot as plt \nimport seaborn as sns\nimport spacy\nfrom spacy.matcher import Matcher","65ba3568":"train = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/test.csv\")\nsubmission =  pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/sample_submission.csv\")","e43518ff":"train.head()","9940d048":"train[~train.keyword.isnull()].tail()","f6b17e03":"sns.countplot(data = train, x = \"target\")","6d062f31":"train.keyword.nunique()","8444a34d":"text1 = dict(train[train.target==1].keyword.value_counts())\nwordcloud = WordCloud(width=800, height=400,background_color=\"white\").generate_from_frequencies(text1)\nplt.figure(figsize=[14,8])\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()","7c894f4d":"text2 = dict(train[train.target==0].keyword.value_counts())\nwordcloud = WordCloud(width=800, height=400,background_color=\"white\").generate_from_frequencies(text2)\nplt.figure(figsize=[14,8])\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()","63250e22":"test.head()","8187e5d5":"text3 = dict(test.keyword.value_counts())\nwordcloud = WordCloud(width=800, height=400,background_color=\"white\").generate_from_frequencies(text3)\nplt.figure(figsize=[14,8])\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()","c0bad8ce":" len(set(test.keyword.unique()).intersection(train.keyword.unique()))","c724df07":"\"outbreak\" in test.keyword.unique()","7a64e59f":"\"explode\" in test.keyword.unique()","8e5d9a47":"df_train = train","b2ce1d9c":"df_train[\"len\"] = df_train.text.str.len()","2884272e":"print(\"max tweet length of no disaster {0}\" .format(max(df_train[df_train.target == 0].len)))\nprint(\"max tweet length of disaster {0}\" .format(max(df_train[df_train.target == 1].len)))\n","d1ad0998":"print(\"min tweet length of no disaster {0}\" .format(min(df_train[df_train.target == 0].len)))\nprint(\"min tweet length of disaster {0}\" .format(min(df_train[df_train.target == 1].len)))\n","cd5cc71c":"print(df_train[df_train.target == 0][\"len\"].mean())\nprint(df_train[df_train.target == 1][\"len\"].mean())\n","3c84019f":"nlp = spacy.load('en')\nmatcher = Matcher(nlp.vocab)\nmatcher.add('HASHTAG', None, [{'ORTH': '#'}, {'IS_ASCII': True}])","9985f452":"def extract_hashtags(text):\n    doc = nlp(text)\n    matches = matcher(doc)\n    hashtags = []\n    for match_id, start, end in matches:\n        hashtags.append(doc[start+1:end].text.lower()) \n    return hashtags","b941a78a":"#hashtags = extract_hashtags(train.text[4])\n#hashtags","3cf1e121":"train['hashtags'] = train.text.apply(extract_hashtags)","64c81b2a":"hash_df_0 = dict(pd.Series(np.concatenate(train[train.target==0].hashtags.reset_index(drop = True))).value_counts())","401b0c3d":"\nwordcloud = WordCloud(width=800, height=400,background_color=\"white\").generate_from_frequencies(hash_df_0)\nplt.figure(figsize=[14,8])\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()","f91cf05f":"hash_df_1 = dict(pd.Series(np.concatenate(train[train.target==1].hashtags.reset_index(drop = True))).value_counts())","3cd31fc8":"\nwordcloud = WordCloud(width=800, height=400,background_color=\"white\").generate_from_frequencies(hash_df_1)\nplt.figure(figsize=[14,8])\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()","79259f83":"from sklearn.model_selection import train_test_split\nfrom sklearn import feature_extraction, linear_model, model_selection, preprocessing\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.naive_bayes import GaussianNB, MultinomialNB\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import accuracy_score, confusion_matrix, f1_score","4e6857ce":"from tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nimport tensorflow as tf","44579ba8":"# For CV \n\nX = train.text\n\ny = train.target\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=122)","b57efddc":"count_vectorizer = feature_extraction.text.CountVectorizer()","3337f575":"train_vectors = count_vectorizer.fit_transform(X_train)\ntest_vectors = count_vectorizer.transform(X_test)","f761afd4":"train_vectors.shape\ntest_vectors.shape","766946a8":"tfidf_transformer = TfidfTransformer()","f5ed5e41":"train_tfidf = tfidf_transformer.fit_transform(train_vectors)\ntest_tfidf = tfidf_transformer.fit_transform(test_vectors)","500188cf":"model =  MultinomialNB().fit(train_tfidf, y_train)","6d1bba21":"y_pred = model.predict(test_tfidf)","b604b769":"#model = RandomForestClassifier(n_estimators=1000, random_state=0).fit(train_tfidf, train.target)\n#predicted = model.predict(test_tfidf)","9d3476f9":"f1_score(y_test, y_pred, average='micro')","c4b0b475":"confusion_matrix(y_test, y_pred)","e9bddce4":"model_svm = SVC(gamma='scale').fit(train_tfidf, y_train)","f22670ac":"y_pred = model_svm.predict(test_tfidf)","8d5f72ce":"f1_score(y_test, y_pred, average='micro')","2d7c9989":"confusion_matrix(y_test, y_pred)","72c4fde0":"test_vectors_og = count_vectorizer.transform(test.text)\ntest_tfidf_og = tfidf_transformer.fit_transform(test_vectors_og)","48ab527e":"\npredicted = model.predict(test_tfidf_og)","39d264f6":"submission.target = predicted","62bd7544":"submission.head()","be491e9e":"submission.to_csv(\"submission.csv\", index=False)","5915b576":"# Sample Testing Data","f3e2137d":"### Keywords present in Train also in Test Data","f0f9ea7f":"The below function extracts hashtag - strips of `#` and converts the hashtag to lower case for normaliztion","4a34b383":"### TF_IDF","7896ea37":"### Number of unique Keywords in the dataset\n\nHoping for better predictability for these observations","54b034ad":"### Model Baseline ","856c136c":"### Loading Required Libs","e0cb419c":"# Word Cloud of Keywords in the test dataset (based on Frequency)\n","f394bf85":"### Tokenizer","d2948cc2":"### Reading Input data","b32072fc":"# Word Cloud of Keywords in the train dataset (based on Frequency)\n\n### Disaster Tweets (Target == 1)","31642b88":"### Naive Bayes","de7c7b07":"# Improvements","6619d233":"### This is just for tests ","fed902d3":"### SVM","f3be8217":"### Length","efe99515":"### Disaster Tweets (Target == 0)","78bb50c7":"# Sample Training Data","95005b74":"### Extract Hashtags"}}