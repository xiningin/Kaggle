{"cell_type":{"3e39733f":"code","ebc4a198":"code","9bc5a729":"code","b4bceb43":"code","80a38a60":"code","64a0574d":"code","edf509a6":"code","3b1bffab":"code","78caee50":"code","afd33a8d":"code","5c0acd2e":"code","eca4e3ab":"code","64a343ee":"code","7a80e660":"code","dd70e0f6":"code","2a02c891":"code","eb7d0415":"code","4c2c4d32":"code","3a300d4f":"code","fec97baf":"code","26144759":"code","0e3a5509":"code","25461ff0":"code","ebdea6bc":"code","8d809da5":"code","6e47f2c8":"code","92abae60":"code","7e3785f2":"code","6157989e":"markdown","e7b5a809":"markdown","a4e96d0f":"markdown","43451cb6":"markdown"},"source":{"3e39733f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \nimport math\nimport random\nimport pickle\nimport itertools\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix, label_ranking_average_precision_score, label_ranking_loss, coverage_error \n\nfrom sklearn.utils import shuffle\n\nfrom scipy.signal import resample\n\nimport matplotlib.pyplot as plt\n\nnp.random.seed(42)\n\nimport pickle\nfrom sklearn.preprocessing import OneHotEncoder\n\nfrom keras.models import Model\nfrom keras.layers import Input, Dense, Conv1D, MaxPooling1D, Softmax, Add, Flatten, Activation# , Dropout\nfrom keras import backend as K\nfrom keras.optimizers import Adam\nfrom keras.callbacks import LearningRateScheduler, ModelCheckpoint\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","ebc4a198":"df = pd.read_csv(\"..\/input\/mitbih_train.csv\", header=None)\ndf2 = pd.read_csv(\"..\/input\/mitbih_test.csv\", header=None)\ndf = pd.concat([df, df2], axis=0)","9bc5a729":"df.head()","b4bceb43":"df.info()","80a38a60":"df[187].value_counts()","64a0574d":"M = df.values\nX = M[:, :-1]\ny = M[:, -1].astype(int)","edf509a6":"del df\ndel df2\ndel M","3b1bffab":"C0 = np.argwhere(y == 0).flatten()\nC1 = np.argwhere(y == 1).flatten()\nC2 = np.argwhere(y == 2).flatten()\nC3 = np.argwhere(y == 3).flatten()\nC4 = np.argwhere(y == 4).flatten()","78caee50":"x = np.arange(0, 187)*8\/1000\n\nplt.figure(figsize=(20,12))\nplt.plot(x, X[C0, :][0], label=\"Cat. N\")\nplt.plot(x, X[C1, :][0], label=\"Cat. S\")\nplt.plot(x, X[C2, :][0], label=\"Cat. V\")\nplt.plot(x, X[C3, :][0], label=\"Cat. F\")\nplt.plot(x, X[C4, :][0], label=\"Cat. Q\")\nplt.legend()\nplt.title(\"1-beat ECG for every category\", fontsize=20)\nplt.ylabel(\"Amplitude\", fontsize=15)\nplt.xlabel(\"Time (ms)\", fontsize=15)\nplt.show()","afd33a8d":"def stretch(x):\n    l = int(187 * (1 + (random.random()-0.5)\/3))\n    y = resample(x, l)\n    if l < 187:\n        y_ = np.zeros(shape=(187, ))\n        y_[:l] = y\n    else:\n        y_ = y[:187]\n    return y_\n\ndef amplify(x):\n    alpha = (random.random()-0.5)\n    factor = -alpha*x + (1+alpha)\n    return x*factor\n\ndef augment(x):\n    result = np.zeros(shape= (4, 187))\n    for i in range(3):\n        if random.random() < 0.33:\n            new_y = stretch(x)\n        elif random.random() < 0.66:\n            new_y = amplify(x)\n        else:\n            new_y = stretch(x)\n            new_y = amplify(new_y)\n        result[i, :] = new_y\n    return result","5c0acd2e":"plt.plot(X[0, :])\nplt.plot(amplify(X[0, :]))\nplt.plot(stretch(X[0, :]))\nplt.show()","eca4e3ab":"result = np.apply_along_axis(augment, axis=1, arr=X[C3]).reshape(-1, 187)\nclasse = np.ones(shape=(result.shape[0],), dtype=int)*3\nX = np.vstack([X, result])\ny = np.hstack([y, classe])","64a343ee":"subC0 = np.random.choice(C0, 800)\nsubC1 = np.random.choice(C1, 800)\nsubC2 = np.random.choice(C2, 800)\nsubC3 = np.random.choice(C3, 800)\nsubC4 = np.random.choice(C4, 800)","7a80e660":"X_test = np.vstack([X[subC0], X[subC1], X[subC2], X[subC3], X[subC4]])\ny_test = np.hstack([y[subC0], y[subC1], y[subC2], y[subC3], y[subC4]])\n\nX_train = np.delete(X, [subC0, subC1, subC2, subC3, subC4], axis=0)\ny_train = np.delete(y, [subC0, subC1, subC2, subC3, subC4], axis=0)\n\nX_train, y_train = shuffle(X_train, y_train, random_state=0)\nX_test, y_test = shuffle(X_test, y_test, random_state=0)\n\ndel X\ndel y","dd70e0f6":"X_train = np.expand_dims(X_train, 2)\nX_test = np.expand_dims(X_test, 2)","2a02c891":"print(\"X_train\", X_train.shape)\nprint(\"y_train\", y_train.shape)\nprint(\"X_test\", X_test.shape)\nprint(\"y_test\", y_test.shape)","eb7d0415":"ohe = OneHotEncoder()\ny_train = ohe.fit_transform(y_train.reshape(-1,1))\ny_test = ohe.transform(y_test.reshape(-1,1))","4c2c4d32":"print(\"X_train\", X_train.shape)\nprint(\"y_train\", y_train.shape)\nprint(\"X_test\", X_test.shape)\nprint(\"y_test\", y_test.shape)","3a300d4f":"n_obs, feature, depth = X_train.shape\nbatch_size = 500","fec97baf":"K.clear_session()\n\ninp = Input(shape=(feature, depth))\nC = Conv1D(filters=32, kernel_size=5, strides=1)(inp)\n\nC11 = Conv1D(filters=32, kernel_size=5, strides=1, padding='same')(C)\nA11 = Activation(\"relu\")(C11)\nC12 = Conv1D(filters=32, kernel_size=5, strides=1, padding='same')(A11)\nS11 = Add()([C12, C])\nA12 = Activation(\"relu\")(S11)\nM11 = MaxPooling1D(pool_size=5, strides=2)(A12)\n\n\nC21 = Conv1D(filters=32, kernel_size=5, strides=1, padding='same')(M11)\nA21 = Activation(\"relu\")(C21)\nC22 = Conv1D(filters=32, kernel_size=5, strides=1, padding='same')(A21)\nS21 = Add()([C22, M11])\nA22 = Activation(\"relu\")(S11)\nM21 = MaxPooling1D(pool_size=5, strides=2)(A22)\n\n\nC31 = Conv1D(filters=32, kernel_size=5, strides=1, padding='same')(M21)\nA31 = Activation(\"relu\")(C31)\nC32 = Conv1D(filters=32, kernel_size=5, strides=1, padding='same')(A31)\nS31 = Add()([C32, M21])\nA32 = Activation(\"relu\")(S31)\nM31 = MaxPooling1D(pool_size=5, strides=2)(A32)\n\n\nC41 = Conv1D(filters=32, kernel_size=5, strides=1, padding='same')(M31)\nA41 = Activation(\"relu\")(C41)\nC42 = Conv1D(filters=32, kernel_size=5, strides=1, padding='same')(A41)\nS41 = Add()([C42, M31])\nA42 = Activation(\"relu\")(S41)\nM41 = MaxPooling1D(pool_size=5, strides=2)(A42)\n\n\nC51 = Conv1D(filters=32, kernel_size=5, strides=1, padding='same')(M41)\nA51 = Activation(\"relu\")(C51)\nC52 = Conv1D(filters=32, kernel_size=5, strides=1, padding='same')(A51)\nS51 = Add()([C52, M41])\nA52 = Activation(\"relu\")(S51)\nM51 = MaxPooling1D(pool_size=5, strides=2)(A52)\n\nF1 = Flatten()(M51)\n\nD1 = Dense(32)(F1)\nA6 = Activation(\"relu\")(D1)\nD2 = Dense(32)(A6)\nD3 = Dense(5)(D2)\nA7 = Softmax()(D3)\n\nmodel = Model(inputs=inp, outputs=A7)\n\nmodel.summary()","26144759":"def exp_decay(epoch):\n    initial_lrate = 0.001\n    k = 0.75\n    t = n_obs\/\/(10000 * batch_size)  # every epoch we do n_obs\/batch_size iteration\n    lrate = initial_lrate * math.exp(-k*t)\n    return lrate\n\nlrate = LearningRateScheduler(exp_decay)","0e3a5509":"adam = Adam(lr = 0.001, beta_1 = 0.9, beta_2 = 0.999)","25461ff0":"model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])","ebdea6bc":"history = model.fit(X_train, y_train, \n                    epochs=75, \n                    batch_size=batch_size, \n                    verbose=2, \n                    validation_data=(X_test, y_test), \n                    callbacks=[lrate])","8d809da5":"y_pred = model.predict(X_test, batch_size=1000)","6e47f2c8":"print(classification_report(y_test.argmax(axis=1), y_pred.argmax(axis=1)))","92abae60":"print(\"ranking-based average precision : {:.3f}\".format(label_ranking_average_precision_score(y_test.todense(), y_pred)))\nprint(\"Ranking loss : {:.3f}\".format(label_ranking_loss(y_test.todense(), y_pred)))\nprint(\"Coverage_error : {:.3f}\".format(coverage_error(y_test.todense(), y_pred)))","7e3785f2":"def plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n\n# Compute confusion matrix\ncnf_matrix = confusion_matrix(y_test.argmax(axis=1), y_pred.argmax(axis=1))\nnp.set_printoptions(precision=2)\n\n# Plot non-normalized confusion matrix\nplt.figure(figsize=(10, 10))\nplot_confusion_matrix(cnf_matrix, classes=['N', 'S', 'V', 'F', 'Q'],\n                      title='Confusion matrix, without normalization')\nplt.show()","6157989e":"# Data augmentation\n\nTo train properly the model, we sould have to augment all data to the same level. Nevertheless, for a first try, we will just augment the smallest class to the same level as class 1. With that we will be able to have a test set of around 5x800 observations.","e7b5a809":"# Split","a4e96d0f":"# Visual Input","43451cb6":"# Model\n\nNow let's re-create the model from the ArXiv Document"}}