{"cell_type":{"da00468b":"code","8929172d":"code","41dee705":"code","099c41c0":"code","72b4ac82":"code","071c0c4e":"code","e274a811":"code","06e86bce":"code","044a9b2e":"code","38720dc3":"code","f8b19698":"code","52cb9b12":"code","21d87c07":"code","962da216":"code","b8953c53":"code","659d2018":"code","08ae68e9":"code","ffb16f6e":"code","ddab6b9c":"code","bfc68bc2":"code","d6088805":"code","4d9007a8":"code","a525ca45":"code","a04106ed":"code","5e644cf4":"code","f2be448d":"code","92c1a6de":"code","4f74bcf9":"code","ba0b9374":"code","fe0f6d99":"code","db41a55d":"code","674c52ee":"code","b9a837ff":"code","e88301dc":"code","5c205d2e":"code","d9abecad":"code","de95272a":"code","4a10aa29":"code","9aa5ca57":"code","6000b0e1":"code","71860121":"code","08deeed3":"code","5ce1a2f1":"code","b9d62b50":"code","6511a8ca":"code","7a10b1ca":"code","bac7ac46":"code","4aa2376e":"code","41b5a735":"code","288a76d5":"code","4aef2fb3":"code","e50dbc24":"code","f9844ec2":"code","ecdb5153":"code","77ba6904":"code","31d25504":"code","c18b5906":"code","2b965917":"code","69f6a7a0":"code","b5b063b6":"code","a2021b65":"code","e580b97c":"code","7b364645":"code","b86604d3":"code","cad3e98a":"code","cfc6c253":"code","71b6e82b":"code","97e1cc2d":"code","7696ef93":"code","254c266c":"code","f2880770":"code","a3395908":"code","50102d3d":"code","8bc953e1":"code","fd2eb3bb":"code","c37132fa":"code","4f90613a":"code","c2b0e9fc":"code","7147a62e":"code","b488bc83":"code","ac065577":"code","80e1de65":"code","1aeae278":"code","8e455e50":"code","295639a6":"code","28e4bda8":"code","94e9b142":"code","0ed68af2":"code","0e8e7505":"code","d8f90a9d":"code","7aa87abb":"code","40b1673d":"code","664fbb06":"code","9cb33294":"code","69786de4":"code","749d64ee":"code","15202007":"code","b36069c6":"code","9485f68d":"code","d9e3c086":"code","23a4e4f2":"code","8edef6f4":"code","6676d4b0":"code","fd09ae48":"code","b11af73d":"code","e553ae4b":"code","d90bdead":"code","4633b29d":"code","45c3838d":"code","a88d6f35":"code","8773241d":"code","97e220b9":"code","3724a301":"code","79b14dc9":"code","72d8db49":"code","6a7d34fe":"code","731e603d":"code","b40ab0a4":"code","f2676e65":"code","1b3efb7b":"code","57e3a82f":"code","ae25dfa4":"code","b80e8f1a":"code","f02ba39c":"code","47d66cf8":"code","7c7c7062":"code","aa7db215":"code","47da6111":"code","15c81a0a":"code","f0b15475":"code","c9ab4176":"code","dcd52ecc":"code","b09923ad":"code","64029ac1":"code","d0201f51":"code","97b751ed":"code","adc9d6a4":"code","8dcecce9":"code","20ad54c2":"code","a1f36114":"code","98e44727":"code","6c9fd4d4":"code","07f2deb5":"code","33e19ae3":"code","51c35092":"code","90e47646":"code","97850dce":"code","40b8c2f0":"code","7ce78f0c":"code","94cd8df2":"code","9fa4dcda":"markdown","c68faa95":"markdown","5e0508a4":"markdown","5e726233":"markdown","db7caefd":"markdown","6074552c":"markdown","0b4a0a6d":"markdown","58a0e243":"markdown","0e7728e1":"markdown","118ad1df":"markdown","6401b809":"markdown","eff3e8fc":"markdown","970678b5":"markdown","ed3571d3":"markdown","b99ce181":"markdown","ac1cb7cb":"markdown","9fe1c528":"markdown","d7eda1e4":"markdown","c00d57cd":"markdown","b46fa493":"markdown","e90058e8":"markdown","1ef21ef7":"markdown","d2ad3609":"markdown","0f32fb82":"markdown","f3704ea8":"markdown","b433ed8d":"markdown","948d007a":"markdown","96e2d392":"markdown","c874cb6a":"markdown","cfbbf326":"markdown","33ad173d":"markdown","4f8c3b1f":"markdown","96074fff":"markdown","e8d9e414":"markdown","e1ad724d":"markdown","e8714b4f":"markdown","5daade66":"markdown","6ea910a7":"markdown","56aee4c7":"markdown","16890dbf":"markdown","96a39ca1":"markdown","939630bf":"markdown","08389b3c":"markdown","b830858f":"markdown","75250eaa":"markdown","4a46286c":"markdown","468bd900":"markdown","71396dee":"markdown","219d88fb":"markdown","cd15b90a":"markdown"},"source":{"da00468b":"# Start Python Imports\nimport math, time, random, datetime\n\n# Data Manipulation\nimport numpy as np\nimport pandas as pd\n\n# Visualization \nimport matplotlib.pyplot as plt\nimport missingno\nimport seaborn as sns\n\n# Preprocessing\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder, label_binarize\nfrom imblearn.over_sampling import SMOTE\n\n# Machine learning\nimport catboost\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import model_selection, tree, preprocessing, metrics, linear_model\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.svm import LinearSVC\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import LinearRegression, LogisticRegression, SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom catboost import CatBoostClassifier, Pool, cv\nfrom sklearn.metrics import confusion_matrix\n\n# Let's be rebels and ignore warnings for now\nimport warnings\nwarnings.filterwarnings('ignore')","8929172d":"claims = pd.read_csv(\"..\/input\/warranty-claims\/train.csv\")","41dee705":"claims.head()","099c41c0":"claims.shape","72b4ac82":"claims.loc[(claims.State == \"UP\"), \"State\"] = \"Uttar Pradesh\"  ## Replacing UP with Uttar Pradesh","071c0c4e":"claims.loc[(claims.Purpose == \"claim\"), \"Purpose\"] = \"Claim\"  ## Replacing claim with Claim","e274a811":"claims.loc[(claims.State == \"Telengana\"), \"City\"] = \"Hyderabad 1\"   ## Separating hyderbad among two states. like Andhra Pradesh = Hyderbad, Telengana = Hyderabad 1","06e86bce":"claims.info()","044a9b2e":"claims.Product_Age.plot.hist()","38720dc3":"claims.Claim_Value.plot.hist()","f8b19698":"claims.Call_details.plot.hist()","52cb9b12":"claims.describe()","21d87c07":"missingno.matrix(claims, figsize = (30,10))","962da216":"claims.isnull().sum()","b8953c53":"claims.shape","659d2018":"claims_bin = pd.DataFrame() # for discretised continuous variables\nclaims_con = pd.DataFrame() # for continuous variables","08ae68e9":"claims.dtypes","ffb16f6e":"# How many missing values does Region have?\nclaims.Region.isnull().sum() ","ddab6b9c":"# unique value counts\nclaims.Region.value_counts()","bfc68bc2":"#regions distribution \nfig = plt.figure(figsize=(20,7))\nsns.countplot(y='Region', data=claims);","d6088805":"# adding this to subset dataframes\nclaims_bin['Region'] = claims['Region']\nclaims_con['Region'] = claims['Region']","4d9007a8":"# How many missing values does State have?\nclaims.State.isnull().sum()","a525ca45":"#unique value count\nclaims.State.value_counts()","a04106ed":"#States distribution \nfig = plt.figure(figsize=(20,7))\nsns.countplot(y='State', data=claims);","5e644cf4":"# adding this to subset dataframes\nclaims_bin['State'] = claims['State']\nclaims_con['State'] = claims['State']","f2be448d":"# How many missing values does Area have?\nclaims.Area.isnull().sum()","92c1a6de":"#unique values count\nclaims.Area.value_counts()","4f74bcf9":"#States distribution \nfig = plt.figure(figsize=(15,2))\nsns.countplot(y='Area', data=claims);","ba0b9374":"# adding this to subset dataframes\nclaims_bin['Area'] = claims['Area']\nclaims_bin['Area'] = np.where(claims_bin['Area'] == 'Urban', 1, 0) # change Area to 1 for Urban and 0 for Rural\nclaims_con['Area'] = claims['Area']","fe0f6d99":"# How many missing values does State have?\nclaims.City.isnull().sum() ","db41a55d":"# unique value count\nclaims.City.value_counts()","674c52ee":"#City distribution \nfig = plt.figure(figsize=(20,7))\nsns.countplot(y='City', data=claims);","b9a837ff":"# adding this to subset dataframes\nclaims_bin['City'] = claims['City']\nclaims_con['City'] = claims['City']","e88301dc":"# How many missing values does Consumer_profile have?\nclaims.Consumer_profile.isnull().sum()","5c205d2e":"#unique value count\nclaims.Consumer_profile.value_counts()","d9abecad":"#Consumer_profile distribution \nfig = plt.figure(figsize=(10,4))\nsns.countplot(y='Consumer_profile', data=claims);","de95272a":"# adding this to subset dataframes\nclaims_bin['Consumer_profile'] = claims['Consumer_profile']\nclaims_bin['Consumer_profile'] = np.where(claims_bin['Consumer_profile'] == 'Business', 1, 0) # change Consumer profile to 1 for Business and 0 for Personal\nclaims_con['Consumer_profile'] = claims['Consumer_profile']","4a10aa29":"# How many missing values does Product_category have?\nclaims.Product_category.isnull().sum() ","9aa5ca57":"# unique value count\nclaims.Product_category.value_counts()","6000b0e1":"#Product_category distribution \nfig = plt.figure(figsize=(10,4))\nsns.countplot(y='Product_category', data=claims);","71860121":"# adding this to subset dataframes\nclaims_bin['Product_category'] = claims['Product_category']\nclaims_bin['Product_category'] = np.where(claims_bin['Product_category'] == 'Entertainment', 1, 0) # change Product_category to 1 for Entertainment and 0 for Household\nclaims_con['Product_category'] = claims['Product_category']","08deeed3":"# How many missing values does Product_type have?\nclaims.Product_type.isnull().sum()","5ce1a2f1":"#unique value count\nclaims.Product_type.value_counts()","b9d62b50":"#Product_category distribution \nfig = plt.figure(figsize=(10,4))\nsns.countplot(y='Product_type', data=claims);","6511a8ca":"# adding this to subset dataframes\nclaims_bin['Product_type'] = claims['Product_type']\nclaims_bin['Product_type'] = np.where(claims_bin['Product_type'] == 'TV', 1, 0) # change Product_type to 1 for TV and 0 for AC\nclaims_con['Product_type'] = claims['Product_type']","7a10b1ca":"# How many missing values does AC_1001_Issue have?\nclaims.AC_1001_Issue.isnull().sum()","bac7ac46":"claims.AC_1001_Issue.value_counts()","4aa2376e":"#AC_1001_Issue distribution \nfig = plt.figure(figsize=(10,2))\nsns.countplot(y='AC_1001_Issue', data=claims);","41b5a735":"# adding this to subset dataframes\nclaims_bin['AC_1001_Issue'] = claims['AC_1001_Issue']\nclaims_con['AC_1001_Issue'] = claims['AC_1001_Issue']","288a76d5":"# How many missing values does AC_1002_Issue have?\nclaims.AC_1002_Issue.isnull().sum()","4aef2fb3":"#unique value count\nclaims.AC_1002_Issue.value_counts()","e50dbc24":"#AC_1002_Issue distribution \nfig = plt.figure(figsize=(10,2))\nsns.countplot(y='AC_1002_Issue', data=claims);","f9844ec2":"# adding this to subset dataframes\nclaims_bin['AC_1002_Issue'] = claims['AC_1002_Issue']\nclaims_con['AC_1002_Issue'] = claims['AC_1002_Issue']","ecdb5153":"# How many missing values does AC_1003_Issue have?\nclaims.AC_1003_Issue.isnull().sum()","77ba6904":"#Unique value count\nclaims.AC_1003_Issue.value_counts()","31d25504":"#AC_1003_Issue distribution \nfig = plt.figure(figsize=(10,2))\nsns.countplot(y='AC_1003_Issue', data=claims);","c18b5906":"# adding this to subset dataframes\nclaims_bin['AC_1003_Issue'] = claims['AC_1003_Issue']\nclaims_con['AC_1003_Issue'] = claims['AC_1003_Issue']","2b965917":"# How many missing values does TV_2001_Issue have?\nclaims.TV_2001_Issue.isnull().sum()","69f6a7a0":"# unique value count\nclaims.TV_2001_Issue.value_counts()","b5b063b6":"#TV_2001_Issue distribution \nfig = plt.figure(figsize=(10,2))\nsns.countplot(y='TV_2001_Issue', data=claims);","a2021b65":"# adding this to subset dataframes\nclaims_bin['TV_2001_Issue'] = claims['TV_2001_Issue']\nclaims_con['TV_2001_Issue'] = claims['TV_2001_Issue']","e580b97c":"# How many missing values does TV_2002_Issue have?\nclaims.TV_2002_Issue.isnull().sum()","7b364645":"#unique value count\nclaims.TV_2002_Issue.value_counts()","b86604d3":"#TV_2002_Issue distribution \nfig = plt.figure(figsize=(10,2))\nsns.countplot(y='TV_2002_Issue', data=claims);","cad3e98a":"# adding this to subset dataframes\nclaims_bin['TV_2002_Issue'] = claims['TV_2002_Issue']\nclaims_con['TV_2002_Issue'] = claims['TV_2002_Issue']","cfc6c253":"# How many missing values does TV_2003_Issue have?\nclaims.TV_2003_Issue.isnull().sum()","71b6e82b":"#unique value count\nclaims.TV_2003_Issue.value_counts()","97e1cc2d":"#TV_2003_Issue distribution \nfig = plt.figure(figsize=(10,2))\nsns.countplot(y='TV_2003_Issue', data=claims);","7696ef93":"# adding this to subset dataframes\nclaims_bin['TV_2003_Issue'] = claims['TV_2003_Issue']\nclaims_con['TV_2003_Issue'] = claims['TV_2003_Issue']","254c266c":"# How many missing values does Claim_Value have?\nclaims.Claim_Value.isnull().sum()","f2880770":"claims[\"Claim_Value\"].mean() #14051.15 rs\nclaims[\"Claim_Value\"].median() #10000 rs","a3395908":"#imputed NA values or missing values with median of claim_value variable\nclaims[\"Claim_Value\"].fillna(10000,inplace=True)","50102d3d":"# How many different values of Claim_Value are there?\nfig = plt.figure(figsize=(30,20))\nsns.countplot(y=\"Claim_Value\", data=claims);","8bc953e1":"# How many unique kinds of Claim_Value are there?\nprint(\"There are {} unique values in Claim_Value.\".format(len(claims.Claim_Value.unique())))\n","fd2eb3bb":"# Add Claim Value to sub dataframes\nclaims_bin['Claim_Value'] = pd.cut(claims['Claim_Value'], bins=5) # discretised into 5 categories\nclaims_con['Claim_Value'] = claims['Claim_Value'] ","c37132fa":"def plot_count_dist(data, bin_df, label_column, target_column, figsize=(20, 5), use_bin_df=False):\n    \"\"\"\n    Function to plot counts and distributions of a label variable and \n    target variable side by side.\n    ::param_data:: = target dataframe\n    ::param_bin_df:: = binned dataframe for countplot\n    ::param_label_column:: = binary labelled column\n    ::param_target_column:: = column you want to view counts and distributions\n    ::param_figsize:: = size of figure (width, height)\n    ::param_use_bin_df:: = whether or not to use the bin_df, default False\n    \"\"\"\n    if use_bin_df: \n        fig = plt.figure(figsize=figsize)\n        plt.subplot(1, 2, 1)\n        sns.countplot(y=target_column, data=bin_df);\n        plt.subplot(1, 2, 2)\n        sns.distplot(data.loc[data[label_column] == 1][target_column], \n                     kde_kws={\"label\": \"Fraud\"});\n        sns.distplot(data.loc[data[label_column] == 0][target_column], \n                     kde_kws={\"label\": \"Genuine\"});\n    else:\n        fig = plt.figure(figsize=figsize)\n        plt.subplot(1, 2, 1)\n        sns.countplot(y=target_column, data=data);\n        plt.subplot(1, 2, 2)\n        sns.distplot(data.loc[data[label_column] == 1][target_column], \n                     kde_kws={\"label\": \"Fraud\"});\n        sns.distplot(data.loc[data[label_column] == 0][target_column], \n                     kde_kws={\"label\": \"Genuine\"});","4f90613a":"# What do our Claim Value bins look like?\nclaims_bin.Claim_Value.value_counts()","c2b0e9fc":"# Visualise the Claim Value bin counts as well as the Claim_Value distribution versus Fraud.\nplot_count_dist(data=claims,\n                bin_df=claims_bin,\n                label_column='Fraud', \n                target_column='Claim_Value', \n                figsize=(20,10), \n                use_bin_df=True)","7147a62e":"# How many missing values does Service_Centre have?\nclaims.Service_Centre.isnull().sum()","b488bc83":"#unique value count \nclaims.Service_Centre.value_counts()","ac065577":"#Service_centre distribution \nfig = plt.figure(figsize=(10,2))\nsns.countplot(y='Service_Centre', data=claims);","80e1de65":"# adding this to subset dataframes\nclaims_bin['Service_Centre'] = claims['Service_Centre']\nclaims_con['Service_Centre'] = claims['Service_Centre']","1aeae278":"# How many missing values does Product_Age have?\nclaims.Product_Age.isnull().sum()","8e455e50":"# How many different values of Product_Age are there?\nfig = plt.figure(figsize=(30,20))\nsns.countplot(y=\"Product_Age\", data=claims);","295639a6":"# How many unique kinds of Product_Age are there?\nprint(\"There are {} unique values in Product_Age.\".format(len(claims.Product_Age.unique())))","28e4bda8":"# Add Product_Age to sub dataframes\nclaims_bin['Product_Age'] = pd.cut(claims['Product_Age'], bins=5) # discretised\nclaims_con['Product_Age'] = claims['Product_Age'] ","94e9b142":"# What do our Product_Age bins look like?\nclaims_bin.Product_Age.value_counts()","0ed68af2":"# Visualise the Product_Age bin counts as well as the Product_Age distribution versus Fraud.\nplot_count_dist(data=claims,\n                bin_df=claims_bin,\n                label_column='Fraud', \n                target_column='Product_Age', \n                figsize=(20,10), \n                use_bin_df=True)","0e8e7505":"# How many missing values does Purchased_from have?\nclaims.Purchased_from.isnull().sum()","d8f90a9d":"#unique value count\nclaims.Purchased_from.value_counts()","7aa87abb":"#Purchased_from distribution \nfig = plt.figure(figsize=(20,7))\nsns.countplot(y='Purchased_from', data=claims);","40b1673d":"# adding this to subset dataframes\nclaims_bin['Purchased_from'] = claims['Purchased_from']\nclaims_con['Purchased_from'] = claims['Purchased_from']","664fbb06":"# How many missing values does Call_details have?\nclaims.Call_details.isnull().sum()","9cb33294":"# How many different values of Call_details are there?\nfig = plt.figure(figsize=(30,15))\nsns.countplot(y=\"Call_details\", data=claims);","69786de4":"# How many unique kinds of Call_details are there?\nprint(\"There are {} unique values in Call_details.\".format(len(claims.Call_details.unique())))\n","749d64ee":"# Add Call_details to sub dataframes\nclaims_bin['Call_details'] = pd.cut(claims['Call_details'], bins=5) # discretised\nclaims_con['Call_details'] = claims['Call_details'] ","15202007":"# What do our Call_details bins look like?\nclaims_bin.Call_details.value_counts()","b36069c6":"# Visualise the Call_details bin counts as well as the Call_details distribution versus Fraud.\nplot_count_dist(data=claims,\n                bin_df=claims_bin,\n                label_column='Fraud', \n                target_column='Call_details', \n                figsize=(20,10), \n                use_bin_df=True)","9485f68d":"# How many missing values does Purpose have?\nclaims.Purpose.isnull().sum()","d9e3c086":"#Unique value count\nclaims.Purpose.value_counts()","23a4e4f2":"#purpose distribution \nfig = plt.figure(figsize=(10,3))\nsns.countplot(y='Purpose', data=claims);","8edef6f4":"# adding this to subset dataframes\nclaims_bin['Purpose'] = claims['Purpose']\nclaims_con['Purpose'] = claims['Purpose']","6676d4b0":"# How many people fraudulent?\nfig = plt.figure(figsize=(10,3))\nsns.countplot(y='Fraud', data=claims);","fd09ae48":"# adding this to subset dataframes\nclaims_bin['Fraud'] = claims['Fraud']\nclaims_con['Fraud'] = claims['Fraud']","b11af73d":"claims_bin.head()","e553ae4b":"# One-hot encode binned variables\none_hot_cols = claims_bin.columns.tolist()\none_hot_cols.remove('Fraud')\nclaims_bin_enc = pd.get_dummies(claims_bin, columns=one_hot_cols)","d90bdead":"claims_bin_enc.head()","4633b29d":"claims_con.head()","45c3838d":"# One hot encode the categorical columns individually\nclaims_Region_one_hot = pd.get_dummies(claims_con['Region'],prefix='region')\nclaims_State_one_hot = pd.get_dummies(claims_con['State'],prefix='state')\nclaims_Area_one_hot = pd.get_dummies(claims_con['Area'],prefix='area')\nclaims_City_one_hot = pd.get_dummies(claims_con['City'],prefix='city')\nclaims_Conpro_one_hot = pd.get_dummies(claims_con['Consumer_profile'],prefix='consumer_profile')\nclaims_Procat_one_hot = pd.get_dummies(claims_con['Product_category'],prefix='product_category')\nclaims_Protyp_one_hot = pd.get_dummies(claims_con['Product_type'],prefix='product_type')\nclaims_Servc_one_hot = pd.get_dummies(claims_con['Service_Centre'],prefix='serrvice_centre')\nclaims_Purfrm_one_hot = pd.get_dummies(claims_con['Purchased_from'],prefix='purchased_from')\nclaims_Purpose_one_hot = pd.get_dummies(claims_con['Purpose'],prefix='purpose')","a88d6f35":"# Combine the one hot encoded columns with claims_con_enc\nclaims_con_enc = pd.concat([claims_con, \n                        claims_Region_one_hot, \n                        claims_State_one_hot, \n                        claims_Area_one_hot, \n                        claims_City_one_hot,\n                        claims_Conpro_one_hot, \n                        claims_Procat_one_hot,\n                        claims_Protyp_one_hot, \n                        claims_Servc_one_hot,\n                        claims_Purfrm_one_hot, \n                        claims_Purpose_one_hot,], axis=1)","8773241d":"# Drop the original categorical columns (because now they've been one hot encoded)\nclaims_con_enc = claims_con_enc.drop(['Region', 'State', 'Area','City',\n                                      'Consumer_profile','Product_category',\n                                      'Product_type','Service_Centre',\n                                      'Purchased_from','Purpose'], axis=1)","97e220b9":"# Let's look at claims_con_enc\nclaims_con_enc.head(20)","3724a301":"# Seclect the dataframe we want to use first for predictions\nselected_claims = claims_con_enc\nselected_claims.head()","79b14dc9":"x_sm = pd.DataFrame.copy(selected_claims)\ny_sm = x_sm.pop('Fraud')\nsm = SMOTE(random_state =101)\nx_train, y_train = sm.fit_sample(x_sm,y_sm)\nx_train.shape, y_train.shape","72d8db49":"x_train = pd.DataFrame(x_train)\ny_train = pd.DataFrame(y_train)","6a7d34fe":"vname = x_sm.columns\nx_train.columns = vname","731e603d":"test = pd.read_csv(\"..\/input\/warranty-claims\/test_1.csv\")","b40ab0a4":"missingno.matrix(test, figsize = (30,10))","f2676e65":"test.isnull().sum()","1b3efb7b":"test.head(20)","57e3a82f":"## Replacing UP with Uttar Pradesh \ntest.loc[(test.State == \"UP\"), \"State\"] = \"Uttar Pradesh\"","ae25dfa4":"## Replacing claim with Claim\ntest.loc[(test.Purpose == \"claim\"), \"Purpose\"] = \"Claim\"","b80e8f1a":"## Separating hyderbad among two states. like Andhra Pradesh = Hyderbad, Telengana = Hyderabad 1\ntest.loc[(test.State == \"Telengana\"), \"City\"] = \"Hyderabad 1\"","f02ba39c":"# Deleting first column\ntest.drop([\"Unnamed: 0\"],inplace=True,axis=1) ","47d66cf8":"test.head()","7c7c7062":"test.Claim_Value.isnull().sum()\ntest[\"Claim_Value\"].median() #median = 10000 Rs\ntest[\"Claim_Value\"].fillna(10000,inplace=True) # imputed with median ","aa7db215":"test.isnull().sum()","47da6111":"# One hot encode the categorical columns individually\ntest_Region_one_hot = pd.get_dummies(test['Region'],prefix='region')\ntest_State_one_hot = pd.get_dummies(test['State'],prefix='state')\ntest_Area_one_hot = pd.get_dummies(test['Area'],prefix='area')\ntest_City_one_hot = pd.get_dummies(test['City'],prefix='city')\ntest_Conpro_one_hot = pd.get_dummies(test['Consumer_profile'],prefix='consumer_profile')\ntest_Procat_one_hot = pd.get_dummies(test['Product_category'],prefix='product_category')\ntest_Protyp_one_hot = pd.get_dummies(test['Product_type'],prefix='product_type')\ntest_Servc_one_hot = pd.get_dummies(test['Service_Centre'],prefix='serrvice_centre')\ntest_Purfrm_one_hot = pd.get_dummies(test['Purchased_from'],prefix='purchased_from')\ntest_Purpose_one_hot = pd.get_dummies(test['Purpose'],prefix='purpose')","15c81a0a":"# Combine the one hot encoded columns with test\nx_test = pd.concat([test, \n                    test_Region_one_hot, \n                    test_State_one_hot, \n                    test_Area_one_hot, \n                    test_City_one_hot,\n                    test_Conpro_one_hot, \n                    test_Procat_one_hot,\n                    test_Protyp_one_hot, \n                    test_Servc_one_hot,\n                    test_Purfrm_one_hot, \n                    test_Purpose_one_hot,], axis=1)","f0b15475":"# Drop the original categorical columns (because now they've been one hot encoded)\nx_test = x_test.drop(['Region', 'State', 'Area','City',\n                      'Consumer_profile','Product_category',\n                      'Product_type','Service_Centre',\n                      'Purchased_from','Purpose'], axis=1)","c9ab4176":"x_test.head()","dcd52ecc":"x_test.isnull().sum()","b09923ad":"logreg = LogisticRegression()\nlogreg.fit(x_train, y_train)\ny_pred = logreg.predict(x_test)\nacc_log = round(logreg.score(x_train, y_train) * 100, 2)\nacc_log\n\n","64029ac1":"svc = SVC()\nsvc.fit(x_train, y_train)\ny_pred = svc.predict(x_test)\nacc_svc = round(svc.score(x_train, y_train) * 100, 2)\nacc_svc","d0201f51":"knn = KNeighborsClassifier(n_neighbors = 11)\nknn.fit(x_train, y_train)\ny_pred = knn.predict(x_test)\nacc_knn = round(knn.score(x_train, y_train) * 100, 2)\nacc_knn","97b751ed":"gaussian = GaussianNB()\ngaussian.fit(x_train, y_train)\ny_pred = gaussian.predict(x_test)\nacc_gaussian = round(gaussian.score(x_train, y_train) * 100, 2)\nacc_gaussian","adc9d6a4":"perceptron = Perceptron()\nperceptron.fit(x_train, y_train)\ny_pred = perceptron.predict(x_test)\nacc_perceptron = round(perceptron.score(x_train, y_train) * 100, 2)\nacc_perceptron","8dcecce9":"linear_svc = LinearSVC()\nlinear_svc.fit(x_train, y_train)\ny_pred = linear_svc.predict(x_test)\nacc_linear_svc = round(linear_svc.score(x_train, y_train) * 100, 2)\nacc_linear_svc","20ad54c2":"sgd = SGDClassifier()\nsgd.fit(x_train, y_train)\ny_pred = sgd.predict(x_test)\nacc_sgd = round(sgd.score(x_train, y_train) * 100, 2)\nacc_sgd","a1f36114":"decision_tree = DecisionTreeClassifier()\ndecision_tree.fit(x_train, y_train)\ny_pred = decision_tree.predict(x_test)\nacc_decision_tree = round(decision_tree.score(x_train, y_train) * 100, 2)\nacc_decision_tree","98e44727":"random_forest = RandomForestClassifier(n_estimators=100)\nrandom_forest.fit(x_train, y_train)\ny_pred = random_forest.predict(x_test)\nrandom_forest.score(x_train, y_train)\nacc_random_forest = round(random_forest.score(x_train, y_train) * 100, 2)\nacc_random_forest","6c9fd4d4":"models = pd.DataFrame({\n    'Model': ['Support Vector Machines', 'KNN', 'Logistic Regression', \n              'Random Forest', 'Naive Bayes', 'Perceptron', \n              'Stochastic Gradient Decent', 'Linear SVC', \n              'Decision Tree'],\n    'Score': [acc_svc, acc_knn, acc_log, \n              acc_random_forest, acc_gaussian, acc_perceptron, \n              acc_sgd, acc_linear_svc, acc_decision_tree]})\nmodels.sort_values(by='Score', ascending=False)","07f2deb5":"def feature_importance(model, data):\n    \"\"\"\n    Function to show which features are most important in the model.\n    ::param_model:: Which model to use?\n    ::param_data:: What data to use?\n    \"\"\"\n    fea_imp = pd.DataFrame({'imp': model.feature_importances_, 'col': data.columns})\n    fea_imp = fea_imp.sort_values(['imp', 'col'], ascending=[True, False]).iloc[-30:]\n    _ = fea_imp.plot(kind='barh', x='col', y='imp', figsize=(20, 10))\n    return fea_imp\n    #plt.savefig('catboost_feature_importance.png')","33e19ae3":"# Plot the feature importance scores\nfeature_importance(decision_tree, x_train)","51c35092":"# Create a list of columns to be used for the predictions\nwanted_test_columns = x_train.columns\nwanted_test_columns","90e47646":"# Make a prediction using the Decision Tree on the wanted columns\npredictions = decision_tree.predict(x_test[wanted_test_columns])","97850dce":"# Our predictions array is comprised of 0's and 1's (Fraud or Genuine)\npredictions[:20]","40b8c2f0":"final = pd.read_csv(\"..\/input\/warranty-claims\/test_1.csv\")\nfinal.head()","7ce78f0c":"# Create a submisison dataframe and append the relevant columns\nsubmission = pd.DataFrame()\nsubmission['Id'] = final['Unnamed: 0']\nsubmission['Prediction1'] = predictions # our model predictions on the test dataset\nsubmission.head()","94cd8df2":"submission.to_csv('submission.csv', index=False)","9fa4dcda":"# 8th column : AC_1001_Issue","c68faa95":"# 15th column : Service_Centre","5e0508a4":"# 18th column : Call_details","5e726233":"#To perform our data analysis, let's create two new dataframes\n","db7caefd":"# 19th column : Purpose","6074552c":"# 10th column : AC_1003_Issue","0b4a0a6d":"loading test data","58a0e243":"#Feature scaling","0e7728e1":"#checking mean median count of continous various variables in dataset ","118ad1df":"Feature Importance","6401b809":"# 7th column : Product_type","eff3e8fc":"# 4th column : City","970678b5":"7) Stochastic Gradient Descent","ed3571d3":"1) Logistoc Regression","b99ce181":"# 11th column : TV_2001_Issue","ac1cb7cb":"..# Different data types in the dataset","9fe1c528":"# 3rd column : Area","d7eda1e4":" 5) Perceptron","c00d57cd":"# 12th column : TV_2002_Issue","b46fa493":"Imputing NA values","e90058e8":"# 13th column : TV_2003_Issue","1ef21ef7":"11# Alternatively, you can see the number of missing values like this","d2ad3609":"3) KNN","0f32fb82":"# 16th column : Product_Age","f3704ea8":"# Submission","b433ed8d":"# 20th column : Fraud","948d007a":"6) Linear SVC","96e2d392":"# 5th column : Consumer_profile","c874cb6a":"# Model Evalution","cfbbf326":"# 9th column : AC_1002_Issue","33ad173d":"**Importing Libraries**","4f8c3b1f":"# 6th column : Product_category","96074fff":"Balancing data","e8d9e414":"2) SVM","e1ad724d":"8) Decision Tree","e8714b4f":"# 1st column : Region","5daade66":"# 14th column : Claim_Value","6ea910a7":"#Function to create count and distribution visualisations","56aee4c7":"converting array to DataFrame","16890dbf":"# 2nd column : State","96a39ca1":"9) Random Forest","939630bf":"# Building models","08389b3c":"EXPLORATORY DATA ANALYSIS","b830858f":"creating dummies","75250eaa":"# 17th column : Purchased_from","4a46286c":"#Histograms of continous variables","468bd900":"4) Gaussian Naive Bayes","71396dee":"Preprocessing for test data as train","219d88fb":"changing column names","cd15b90a":"#plot for missing values "}}