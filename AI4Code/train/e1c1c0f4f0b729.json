{"cell_type":{"8130d6ea":"code","2fec5295":"code","0b1c7dce":"code","d39b0642":"code","6126e6ae":"code","f7acfc56":"code","e83639bc":"code","4454299e":"code","91c3493d":"code","335ee684":"code","75ddffe0":"code","4f0a4aca":"code","6af82f97":"code","84bc64c2":"code","482805e2":"code","031d90ae":"code","73d5351d":"code","d1992006":"code","dbdca7ae":"code","7dfacdcd":"code","05bf25cc":"code","2aae4943":"code","f5e1e05f":"code","e70803a5":"code","cbc7c0d3":"code","0ba86c08":"code","2ba33d6b":"code","7f6d5909":"code","ce9129a7":"code","85c744e8":"code","1cf07a5b":"code","29cefc1f":"code","b7c52628":"code","83157a5c":"code","44af3adc":"code","2a0a14e2":"code","b3ce5d9a":"code","a349b673":"code","25ca6d8e":"code","6976a682":"code","36e11557":"code","5d59371a":"code","31309231":"code","7ee5d81b":"code","01a22bd0":"code","e2d41101":"code","e610c4ac":"code","c4b46f2c":"code","5c1852f6":"code","0fd6b669":"code","2c89ce06":"code","9634300c":"code","ae0fb346":"code","686d4abc":"code","2977eeb0":"code","6e3256f1":"code","bc69f3dc":"code","679b4784":"code","04963877":"code","094cc8a1":"code","f27fcdb3":"code","35ba44b3":"code","be30fee0":"code","70ceeca5":"code","7818d9c7":"code","4199ed05":"code","08f575ff":"markdown","e97bfd37":"markdown","589089bc":"markdown","a7635b5c":"markdown","1f7828ac":"markdown","2c7a555c":"markdown","0404cab7":"markdown","9c52ff6c":"markdown","6331dc9f":"markdown","c05d79ed":"markdown","8552a916":"markdown","5ae4c35a":"markdown","2e4c3539":"markdown","d06b2263":"markdown","2c2d0e48":"markdown","00e82936":"markdown","4b9f7f12":"markdown","7ce17d01":"markdown","d935aa72":"markdown","ddb12614":"markdown","ea3213f5":"markdown","da4fa05e":"markdown","31d312ad":"markdown","4dde3f2c":"markdown","6d07c2bc":"markdown","77b75174":"markdown","bc84341c":"markdown","4819e0da":"markdown"},"source":{"8130d6ea":"import numpy as np\nimport pandas as pd\nfrom ast import literal_eval\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom surprise import Reader, Dataset, SVD, KNNBaseline\nfrom surprise.model_selection import cross_validate\nimport seaborn as sn\nimport matplotlib.pyplot as plt\nfrom wordcloud import WordCloud, STOPWORDS\nimport plotly.graph_objs as go \nimport pylab as pl\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA\nfrom sklearn import preprocessing\nimport xgboost as xgb\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nimport datetime\nfrom fbprophet import Prophet\nimport matplotlib.pyplot as plt\nimport seaborn as sns","2fec5295":"credits = pd.read_csv('..\/input\/the-movies-dataset\/credits.csv')\nmoviesMetaData = pd.read_csv('..\/input\/the-movies-dataset\/movies_metadata.csv',low_memory=False)\nkeywords = pd.read_csv('..\/input\/the-movies-dataset\/keywords.csv')\nratings = pd.read_csv('..\/input\/the-movies-dataset\/ratings_small.csv')","0b1c7dce":"ratings.rename(columns={'movieId': 'id'}, inplace = True)","d39b0642":"moviesMetaData['id'] = moviesMetaData['id'].astype(str)\ncredits['id'] = credits['id'].astype(str)\nkeywords['id'] = keywords['id'].astype(str)\nratings['id'] = ratings['id'].astype(str)","6126e6ae":"ratings = ratings[['id']]\nratings = ratings.drop_duplicates()\n\nmoviesMetaData = pd.merge(moviesMetaData,ratings, on='id')","f7acfc56":"mainList= pd.merge(moviesMetaData, credits, on='id')\nmainList= pd.merge(mainList,keywords, on='id')\ncorrMatrix = mainList.corr()\nsn.heatmap(mainList.isnull(), yticklabels = False, cbar = False, cmap = 'viridis')","e83639bc":"def missingDF(data):\n    missing_df = data.isnull().sum(axis=0).reset_index()\n    missing_df.columns = ['column_name', 'missing_count']\n    missing_df['filling_factor'] = (mainList.shape[0] \n                                    - missing_df['missing_count']) \/ mainList.shape[0] * 100\n    missing_df.sort_values('filling_factor').reset_index(drop = True)\n    \n    missing_df = missing_df.sort_values('filling_factor').reset_index(drop = True)\n    y_axis = missing_df['filling_factor'] \n    x_label = missing_df['column_name']\n    x_axis = missing_df.index\n\n    fig = plt.figure(figsize=(11, 4))\n    plt.xticks(rotation=80, fontsize = 14)\n    plt.yticks(fontsize = 13)\n\n    plt.xticks(x_axis, x_label,family='fantasy', fontsize = 14 )\n    plt.ylabel('Filling factor (%)', family='fantasy', fontsize = 16)\n    plt.bar(x_axis, y_axis);\n    \n    return missing_df","4454299e":"table = missingDF(mainList)","91c3493d":"mainList['release_date'] =  pd.to_datetime(mainList['release_date']) \nmainList['years'] = mainList['release_date'].apply(lambda x: x.year)\n\nmainList[(mainList['years'] < 2019) & (mainList['years'] >= 1950)].groupby(by = 'years').mean()['vote_count'].plot()","335ee684":"mainList['budget'] = mainList['budget'].astype(float)\nmainList['popularity'] = mainList['popularity'].astype(float)\nsn.heatmap(mainList.corr(), annot=True)","75ddffe0":"mainList = mainList[['id', 'title', 'cast', 'crew', 'keywords', 'genres']]","4f0a4aca":"mainList.head(5)","6af82f97":"\nfeatures = ['cast', 'crew', 'keywords', 'genres']\nfor feature in features:\n    mainList[feature] = mainList[feature].apply(literal_eval)\n    \nmainList.head()","84bc64c2":"def getDirector(x):\n    for i in x:\n        if i['job'] == 'Director':\n            return i['name']\n    return np.nan","482805e2":"def getFirstThree(x):\n    if isinstance(x, list):\n        names = [i['name'] for i in x]\n        \n        if len(names) > 3:\n            names = names[:3]\n        return names\n\n    return []","031d90ae":"mainList['director'] = mainList['crew'].apply(getDirector)\n\nfeatures = ['cast', 'keywords', 'genres']\nfor feature in features:\n    mainList[feature] = mainList[feature].apply(getFirstThree)","73d5351d":"mainList[['title', 'cast', 'director', 'keywords', 'genres']].head(3)","d1992006":"def counting_values(df, column):\n    value_count = {}\n    for row in df[column].dropna():\n        if len(row) > 0:\n            for key in row:\n                if key in value_count:\n                    value_count[key] += 1\n                else:\n                    value_count[key] = 1\n        else:\n            pass\n    return value_count","dbdca7ae":"def count_director(df, column):\n    value_count = {}\n    for key in df[column].dropna():\n        if key in value_count:\n            value_count[key] += 1\n        else:\n            value_count[key] = 1\n        \n    return value_count\n    ","7dfacdcd":"sn.heatmap(mainList.isnull(), yticklabels = False, cbar = False, cmap = 'viridis')","05bf25cc":"def hist(data):\n    iData = dict(sorted(data.items(), key=lambda x: x[1],reverse=True)[:20])\n    pos = np.arange(len(iData.keys()))\n    width = 1.0\n    \n    ax = plt.axes()\n    ax.set_xticks(pos + (width \/ 2))\n    ax.set_xticklabels(iData.keys())\n    plt.yticks(fontsize = 15)\n    plt.xticks(rotation=85, fontsize = 15)\n    plt.grid()\n    plt.bar(iData.keys(), iData.values(), width, align = 'center', color='g')\n    plt.show()","2aae4943":"def createWordCloud(data):\n    wordcloud = WordCloud(max_font_size=100)\n\n    wordcloud.generate_from_frequencies(data)\n     \n    plt.figure(figsize=[10.1,10.1])\n    plt.imshow(wordcloud, interpolation=\"bilinear\")\n    plt.axis(\"off\")\n\n    plt.show()\n    genres_count = pd.Series(data)\n    genres_count.sort_values(ascending = False).head(20).plot(kind = 'bar', grid='True')","f5e1e05f":"createWordCloud(counting_values(mainList, 'genres'))","e70803a5":"createWordCloud(counting_values(mainList, 'cast'))","cbc7c0d3":"createWordCloud(counting_values(mainList, 'keywords'))","0ba86c08":"createWordCloud(count_director(mainList, 'director'))","2ba33d6b":"def deletingSpaces(x):\n    if isinstance(x, list):\n        return [str.lower(i.replace(\" \", \"\")) for i in x]\n    else:\n        \n        if isinstance(x, str):\n            return str.lower(x.replace(\" \", \"\"))\n        else:\n            return ''\n        \nfeatures = ['cast', 'keywords', 'director']\n\nfor feature in features:\n    mainList[feature] = mainList[feature].apply(deletingSpaces)\n    \nmainList.head(5)","7f6d5909":"def combineKeywords(x):\n    return ' '.join(x['keywords']) + ' ' + ' '.join(x['cast']) + ' ' + x['director'] + ' ' + ' '.join(x['genres'])\n\nmainList['myKeywords'] = mainList.apply(combineKeywords, axis=1)\n\nmainList['myKeywords'].head(5)","ce9129a7":"table = missingDF(mainList)","85c744e8":"count = CountVectorizer(stop_words='english')\ncount_matrix = count.fit_transform(mainList['myKeywords'])\ncosine_sim = cosine_similarity(count_matrix, count_matrix)","1cf07a5b":"mainList = mainList.reset_index()\nindices = pd.Series(mainList.index, index=mainList['title'])","29cefc1f":"def get_recommendations(title, cosine_sim=cosine_sim):\n    idx = indices[title]\n\n    sim_scores = list(enumerate(cosine_sim[idx]))\n\n    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n\n    sim_scores = sim_scores[1:11]\n\n    movie_indices = [i[0] for i in sim_scores]\n    \n    sim_value = [x[1] for x in sim_scores]\n    \n    result = indices.iloc[movie_indices]\n    \n    result[0:10] = sim_value\n\n    return(result)","b7c52628":"def show(title):\n    result = get_recommendations(title)\n\n    plt.figure(figsize=(10,5))\n    sn.barplot(x = result[0:10], y=result.index)\n    plt.title(\"Recommended Movies from \" + str.upper(title), fontdict= {'fontsize' :20})\n    plt.xlabel(\"Cosine Similarities\")\n    plt.show()\n","83157a5c":"show('Twelve Monkeys')","44af3adc":"show('Twelve Monkeys')","2a0a14e2":"get_recommendations('Twelve Monkeys')","b3ce5d9a":"reader = Reader()\nratings = pd.read_csv('..\/input\/the-movies-dataset\/ratings_small.csv')\ncorrMatrix = ratings.corr()\nratings.head()","a349b673":"corrMatrix = ratings.corr()\nsn.heatmap(corrMatrix, annot=True)","25ca6d8e":"data = Dataset.load_from_df(ratings[['userId', 'movieId', 'rating']], reader)","6976a682":"alg = KNNBaseline()\ncross_validate(alg, data, measures=['RMSE', 'MAE'])","36e11557":"trainset = data.build_full_trainset()\nalg.fit(trainset)","5d59371a":"alg.predict(1, 39)","31309231":"def getForecast(userId, title):\n    idx = indices[title]\n\n    sim_scores = list(enumerate(cosine_sim[idx]))\n\n    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n    \n    sim_scores = sim_scores[1:11]\n    \n    show(title)\n    \n    movie_indices = [i[0] for i in sim_scores]\n    \n    movies = mainList.iloc[movie_indices][['id', 'title']]\n    movies['id'] = movies['id'].astype(int)\n    \n    def getEst(item):\n        return alg.predict(userId, item['id']).est\n    \n    movies['est'] = movies.apply(getEst, axis=1)\n    \n    return movies.head(10)","7ee5d81b":"getForecast(5,'From Dusk Till Dawn')","01a22bd0":"getForecast(78,'From Dusk Till Dawn')","e2d41101":"getForecast(76,'Twelve Monkeys')","e610c4ac":"getForecast(32,'Twelve Monkeys')","c4b46f2c":"df = pd.read_csv('..\/input\/the-movies-dataset\/movies_metadata.csv', low_memory=False)","5c1852f6":"df.drop(df.index[19730],inplace=True)\ndf.drop(df.index[29502],inplace=True)\ndf.drop(df.index[35585],inplace=True)\n\ndf_numeric = df[['budget','popularity','revenue','runtime','vote_average','vote_count','title']]","0fd6b669":"df_numeric.head()","2c89ce06":"df_numeric.dropna(inplace=True)\n\ndf_numeric.isnull().sum()","9634300c":"df_numeric = df_numeric[df_numeric['vote_count']>30]","ae0fb346":"minmax_processed = preprocessing.MinMaxScaler().fit_transform(df_numeric.drop('title',axis=1))\ndf_numeric_scaled = pd.DataFrame(minmax_processed, index=df_numeric.index, columns=df_numeric.columns[:-1])","686d4abc":"Nc = range(1, 20)\nkmeans = [KMeans(n_clusters=i) for i in Nc]\n\nscore = [kmeans[i].fit(df_numeric_scaled).score(df_numeric_scaled) for i in range(len(kmeans))]\n\npl.plot(Nc,score)\npl.xlabel('Number of Clusters')\npl.ylabel('Score')\npl.title('Elbow Curve')\npl.show()","2977eeb0":"kmeans = KMeans(n_clusters=5)\nkmeans.fit(df_numeric_scaled)\ndf_numeric['cluster'] = kmeans.labels_\n\ndf_numeric.head(20)","6e3256f1":"plt.figure(figsize=(12,7))\naxis = sn.barplot(x=np.arange(0,5,1),y=df_numeric.groupby(['cluster']).count()['budget'].values)\nx=axis.set_xlabel(\"Cluster Number\")\nx=axis.set_ylabel(\"Number of movies\")","bc69f3dc":"df_numeric.groupby(['cluster']).mean()","679b4784":"df = pd.read_csv('..\/input\/the-movies-dataset\/movies_metadata.csv', low_memory=False)\n\ndf_numeric = df[['revenue','runtime','vote_average','vote_count']]\ndf_numeric.dropna(inplace=True)\nx_data = df_numeric[['revenue','runtime','vote_count']]\ny_data = df_numeric['vote_average']\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.2)\n\nD_train = xgb.DMatrix(X_train, label=y_train)\nD_test = xgb.DMatrix(X_test, label=y_test)\n\nparam = { \n    \"silent\":True,\"eta\":0.01,'subsample': 0.75,'colsample_bytree': 0.7,\"max_depth\":7, 'metric': 'rmse'} \n\nsteps = 20 \nmodel = xgb.train(param, D_train, steps)\n\npreds = model.predict(D_test)\n","04963877":"df = pd.read_csv('..\/input\/the-movies-dataset\/movies_metadata.csv', low_memory=False)\ndf = df.dropna()\ndf = df[['vote_average','revenue']]\ndf = df.loc[df['vote_average'] > 5]\ndf = df.loc[df['revenue'] < 1500000000]\ndf = df.loc[df['revenue'] > 1000000]\n\nsns.lmplot(x =\"vote_average\", y =\"revenue\", data = df, order = 2, ci = None) ","094cc8a1":"X = df.iloc[:, :-1].values\ny = df.iloc[:, -1].values\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 20, test_size = 0.5)\n\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\n\nmodel.score(X_test, y_test)\npred1 = model.predict(X_test) \nplt.scatter(X_test, y_test, color ='b') \nplt.plot(X_test, pred1, color ='k') \n  \nplt.show() ","f27fcdb3":"pred1 = model.predict(X_test) \nplt.scatter(X_test, y_test, color ='b') \nplt.plot(X_test, pred1, color ='k') \n  \nplt.show() ","35ba44b3":"df = pd.read_csv('..\/input\/the-movies-dataset\/movies_metadata.csv', low_memory=False)\ndf = df[['release_date', 'revenue']];\ndf = df.loc[df['revenue'] > 1000000]\ndf = df.loc[df['revenue'] < 1500000000]\ndf = df.loc[df['release_date'] > \"2012-01-21\"]\ndf = df.sort_values(by='release_date',ascending=True)\ndf.plot(figsize=(12,6), x='release_date',y='revenue')\ndf.reset_index(drop=True)\ndf.head(5)","be30fee0":"split_date = '2015-01-01'\ntrain_data = df[df['release_date'] <= split_date].copy()\ntest_data = df[df['release_date'] > split_date].copy()","70ceeca5":"model = Prophet()\nprophetData = train_data.reset_index().rename(columns={'release_date':'ds','revenue':'y'})\nmodel.fit(prophetData)\n\nprophetTestData = test_data.reset_index().rename(columns={'release_date':'ds','revenue':'y'})\nforecast = model.predict(df=prophetTestData)\nforecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']].head()","7818d9c7":"fig1 = model.plot(forecast)","4199ed05":"fig2 = model.plot_components(forecast)","08f575ff":"Take only the movies that have more than 30 votes.","e97bfd37":"Function for deleting spaces between name and surname, actors and director. This is to uniquely identify the actors and directors, rather than a separate name and surname.","589089bc":"We change the column type 'id' in all dateframes so that they can be combined","a7635b5c":"The function to find the first three elements, if the elements are less than 3, return all that is, if there are no elements - an empty list.","1f7828ac":"Connecting libraries","2c7a555c":"Compared several algorithms:\n\nSVD - {'test_rmse': array([0.89708382, 0.89360817, 0.90175499, 0.89434609, 0.89912869]),\n 'test_mae': array([0.69280077, 0.6873133 , 0.69288119, 0.68789517, 0.69535891])\n \nNMF - {'test_rmse': array([0.95434438, 0.95271784, 0.93976507, 0.94191008, 0.94411319]),\n 'test_mae': array([0.73496285, 0.73180698, 0.7217913 , 0.72560867, 0.72723372])\n \nKNNBasic - {'test_rmse': array([0.96725363, 0.97125242, 0.97014741, 0.96873569, 0.96112459]),\n 'test_mae': array([0.74064758, 0.74791949, 0.74719143, 0.74401927, 0.7398144 ])\n \nKNNWithMeans - {'test_rmse': array([0.91904553, 0.91733627, 0.92334314, 0.92268939, 0.91496662]),\n 'test_mae': array([0.70309962, 0.70265884, 0.70387752, 0.70920784, 0.70105248])\n \nKNNWithZScore - {'test_rmse': array([0.93017316, 0.92175447, 0.90740569, 0.91702126, 0.91801755]),\n 'test_mae': array([0.70783427, 0.70005986, 0.69185356, 0.69883193, 0.69449284])\n \nKNNBaseline - {'test_rmse': array([0.89283459, 0.89803848, 0.88978505, 0.89786181, 0.90196116]),\n 'test_mae': array([0.68536267, 0.68852963, 0.6818973 , 0.68884985, 0.68979706])\n \nand chose KNNBaseline (SVD algorithm shows similar results) as it has the best performance (RMSE - 0.89) and (MAE - 0.69)","0404cab7":"Find optimal k.\nWe are dealing with tradeoff between cluster size(hence the computation required) and the relative accuracy","9c52ff6c":"The recommendation feature, based on our selected data, accepts the movie name and returns 10 recommendations.","6331dc9f":"We merge the dateframes into one, and select only those columns to use for the recommendation.","c05d79ed":"Only keep the numeric columns for our analysis","8552a916":"We combine all the data we use for the recommendation into one row.","5ae4c35a":"Read data","2e4c3539":"# **Linear Regression**","d06b2263":"Drop all the rows with null values","2c2d0e48":"Movies Recommendation. The main idea behind this solution is to predict a movie's rating by a particular user, based on the ratings it has given to other films and also based on the ratings of other users of that movie.\nAlso, to make not random selection of films, I created a recommendation system that, based on movie data (genres, actors, directors, keywords), picks similar ones.","00e82936":"We create an array to find the movie index by its name. (For ease of use)","4b9f7f12":"Data filtering, we choose only the ones we need. The 'getFirstThree' function is applied to the following data: cast, keywords and genres.","7ce17d01":"Downloading data","d935aa72":"Function for finding the director in 'crew' data, if the director is not returned NaN","ddb12614":"The 'getForecast' function gives a predicted rating that will drive a movie to specific users based on the ratings it has given to other movies and based on the ratings that those films have similarly rated.","ea3213f5":"# **Time series**","da4fa05e":"Thanks to:\n* https:\/\/www.kaggle.com\/fabiendaniel\/film-recommendation-engine\n* https:\/\/www.kaggle.com\/ibtesama\/getting-started-with-a-movie-recommendation-system\n* https:\/\/www.kaggle.com\/sjj118\/movie-visualization-recommendation-prediction","31d312ad":"# **XGBoost**","4dde3f2c":"Normalize the data with MinMax scaling provided by sklearn","6d07c2bc":"Let's look at the cluster statistics.","77b75174":"With the help of \"CountVectorizer\" we build a matrix of word occurrence. With \"cosine_similarity\" we determine the similarity between films.","bc84341c":"# **K-Means Clustering**","4819e0da":"Let's see cluster sizes."}}