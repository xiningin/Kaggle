{"cell_type":{"2729af20":"code","7f8e4c54":"code","4b86f5a6":"code","93e22c13":"code","fdbe3d01":"code","1e37e777":"code","05d3c8a3":"code","74f8af7b":"code","67d2246c":"code","ddfa9446":"code","60f0c66e":"code","8229ed44":"code","183303cc":"code","89866fcd":"code","e2f21149":"code","d6cfdfec":"code","fd59714f":"code","d833ffd9":"code","1b31a3ea":"code","08253b30":"code","9994ebd8":"code","8bb9eef0":"code","6b98d9a1":"code","de0f2254":"code","7134a761":"code","8eddbe16":"code","52b06cfa":"code","588993e4":"code","ea9c9b25":"code","c57eeb45":"code","c50cfd19":"code","bbf28a19":"code","7c0e43c0":"code","aabbfd45":"code","7f7bac12":"code","fd8883a8":"markdown","af74e468":"markdown","77e02ee9":"markdown","9bbae6b2":"markdown","77d7f4b7":"markdown","cab27382":"markdown","d3cf6f1b":"markdown","32dadbb2":"markdown","47fc5293":"markdown","cde78d57":"markdown","565c52ae":"markdown","484c1181":"markdown"},"source":{"2729af20":"import os\nimport random\nimport json\nfrom gc import collect\nfrom functools import partial\nfrom collections import Counter, defaultdict\nfrom math import sqrt\nfrom operator import itemgetter\n\nfrom joblib import Parallel, delayed\n\nimport numpy as np\nimport scipy as sp\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport cv2\nimport dill\nfrom tqdm import tqdm\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import GroupKFold\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.decomposition import NMF, TruncatedSVD\nfrom sklearn.metrics import cohen_kappa_score, mean_squared_error\nfrom sklearn.metrics import confusion_matrix as sk_cmatrix\n\nimport lightgbm as lgb\n\nkernel = True\nnthread = 6","7f8e4c54":"# load data\ntrain_df = pd.read_csv(\"..\/input\/petfinder-adoption-prediction\/train\/train.csv\", encoding=\"utf-8\")\ntest_df = pd.read_csv(\"..\/input\/petfinder-adoption-prediction\/test\/test.csv\", encoding=\"utf-8\")\n\nbreed_labels_df = pd.read_csv(\"..\/input\/petfinder-adoption-prediction\/breed_labels.csv\")\nbreed_labels_df = pd.concat([pd.DataFrame([{\"BreedID\": 0, \"Type\": 0, \"BreedName\": \"None\"}]), breed_labels_df])\ncolor_labels_df = pd.read_csv(\"..\/input\/petfinder-adoption-prediction\/color_labels.csv\")\nstate_labels_df = pd.read_csv(\"..\/input\/petfinder-adoption-prediction\/state_labels.csv\")\n\nsmpsb_df = pd.read_csv(\"..\/input\/petfinder-adoption-prediction\/test\/sample_submission.csv\")","4b86f5a6":"# Since train and test are splited by RescuerID, I used GroupKFold for validation.\npetid_map = {v: i for i, v in enumerate(pd.concat([train_df[\"PetID\"], test_df[\"PetID\"]]))}\nrescuerid_encoder = LabelEncoder().fit(pd.concat([train_df[\"RescuerID\"], test_df[\"RescuerID\"]]))\n\nfor group, (_, group_idx) in enumerate(GroupKFold(n_splits=10).split(train_df,\n                                                                     train_df[\"AdoptionSpeed\"],\n                                                                     rescuerid_encoder.transform(train_df[\"RescuerID\"]))):\n    train_df.loc[group_idx, \"group\"] = group","93e22c13":"# metrix\n\ndef confusion_matrix(rater_a, rater_b, min_rating=None, max_rating=None):\n    \"\"\"\n    Returns the confusion matrix between rater's ratings\n    \"\"\"\n    assert(len(rater_a) == len(rater_b))\n    if min_rating is None:\n        min_rating = min(rater_a + rater_b)\n    if max_rating is None:\n        max_rating = max(rater_a + rater_b)\n    num_ratings = int(max_rating - min_rating + 1)\n    conf_mat = [[0 for i in range(num_ratings)]\n                for j in range(num_ratings)]\n    for a, b in zip(rater_a, rater_b):\n        conf_mat[a - min_rating][b - min_rating] += 1\n    return conf_mat\n\n\ndef histogram(ratings, min_rating=None, max_rating=None):\n    \"\"\"\n    Returns the counts of each type of rating that a rater made\n    \"\"\"\n    if min_rating is None:\n        min_rating = min(ratings)\n    if max_rating is None:\n        max_rating = max(ratings)\n    num_ratings = int(max_rating - min_rating + 1)\n    hist_ratings = [0 for x in range(num_ratings)]\n    for r in ratings:\n        hist_ratings[r - min_rating] += 1\n    return hist_ratings\n\n\ndef quadratic_weighted_kappa(y, y_pred):\n    \"\"\"\n    Calculates the quadratic weighted kappa\n    axquadratic_weighted_kappa calculates the quadratic weighted kappa\n    value, which is a measure of inter-rater agreement between two raters\n    that provide discrete numeric ratings.  Potential values range from -1\n    (representing complete disagreement) to 1 (representing complete\n    agreement).  A kappa value of 0 is expected if all agreement is due to\n    chance.\n    quadratic_weighted_kappa(rater_a, rater_b), where rater_a and rater_b\n    each correspond to a list of integer ratings.  These lists must have the\n    same length.\n    The ratings should be integers, and it is assumed that they contain\n    the complete range of possible ratings.\n    quadratic_weighted_kappa(X, min_rating, max_rating), where min_rating\n    is the minimum possible rating, and max_rating is the maximum possible\n    rating\n    \"\"\"\n    rater_a = y\n    rater_b = y_pred\n    min_rating=None\n    max_rating=None\n    rater_a = np.array(rater_a, dtype=int)\n    rater_b = np.array(rater_b, dtype=int)\n    assert(len(rater_a) == len(rater_b))\n    if min_rating is None:\n        min_rating = min(min(rater_a), min(rater_b))\n    if max_rating is None:\n        max_rating = max(max(rater_a), max(rater_b))\n    conf_mat = confusion_matrix(rater_a, rater_b,\n                                min_rating, max_rating)\n    num_ratings = len(conf_mat)\n    num_scored_items = float(len(rater_a))\n\n    hist_rater_a = histogram(rater_a, min_rating, max_rating)\n    hist_rater_b = histogram(rater_b, min_rating, max_rating)\n\n    numerator = 0.0\n    denominator = 0.0\n\n    for i in range(num_ratings):\n        for j in range(num_ratings):\n            expected_count = (hist_rater_a[i] * hist_rater_b[j]\n                              \/ num_scored_items)\n            d = pow(i - j, 2.0) \/ pow(num_ratings - 1, 2.0)\n            numerator += d * conf_mat[i][j] \/ num_scored_items\n            denominator += d * expected_count \/ num_scored_items\n\n    return (1.0 - numerator \/ denominator)\n\ndef rmse(actual, predicted):\n    return sqrt(mean_squared_error(actual, predicted))\n\nfrom copy import deepcopy\nparam = {'num_leaves': 31,\n         'min_data_in_leaf': 32, \n         'objective':'regression',\n         'max_depth': -1,\n         'learning_rate': 0.01,\n         \"min_child_samples\": 20,\n         \"boosting\": \"gbdt\",\n         \"feature_fraction\": 0.9,\n         \"bagging_freq\": 1,\n         \"bagging_fraction\": 0.9 ,\n         \"bagging_seed\": 11,\n         \"metric\": 'rmse',\n         \"lambda_l1\": 0.1,\n         \"nthread\":nthread,\n         \"verbosity\": -1}\n\ndef make_lgb_oofs(X_train, y_train, group, X_test, params, repeat=1, seedkeys=[\"bagging_seed\", \"seed\"]):\n    #folds = StratifiedKFold(n_splits=5, random_state=2434, shuffle=True)\n    params = deepcopy(params)\n    train_oof = np.zeros(len(X_train))\n    test_pred = np.zeros(len(X_test))\n\n    for j in range(repeat):\n        for key in seedkeys:\n            params[key] = 2434 + j\n        for i in range(5):\n            dev_idx = np.where((group\/\/2) != i)[0]\n            val_idx = np.where((group\/\/2) == i)[0]\n            dev_data = lgb.Dataset(X_train[dev_idx], label=y_train[dev_idx])\n            val_data = lgb.Dataset(X_train[val_idx], label=y_train[val_idx])\n\n            num_rounds = 10000\n            clf = lgb.train(params,\n                            dev_data,\n                            num_rounds,\n                            valid_sets=[dev_data, val_data],\n                            verbose_eval=100,\n                            early_stopping_rounds=200)\n            train_oof[val_idx] += clf.predict(X_train[val_idx]) \/ repeat\n            test_pred += clf.predict(X_test) \/ 5 \/ repeat\n        \n\n    return train_oof, test_pred","fdbe3d01":"class OptimizedRounder(object):\n    def __init__(self):\n        self.coef_ = 0\n    \n    def _kappa_loss(self, coef, X, y):\n        preds = pd.cut(X, [-np.inf] + list(np.sort(coef)) + [np.inf], labels = [0, 1, 2, 3, 4])\n        return -cohen_kappa_score(y, preds, weights = 'quadratic')\n    \n    def fit(self, X, y):\n        loss_partial = partial(self._kappa_loss, X = X, y = y)\n        initial_coef = np.percentile(X, [2.73, 23.3, 50.3, 72]) # <= keypoint\n        self.coef_ = sp.optimize.minimize(loss_partial, initial_coef, method = 'nelder-mead')\n    \n    def predict(self, X, coef):\n        preds = pd.cut(X, [-np.inf] + list(np.sort(coef)) + [np.inf], labels = [0, 1, 2, 3, 4])\n        return preds\n    \n    def coefficients(self):\n        return self.coef_['x']","1e37e777":"# [2.73, 23.3, 50.3, 72] is\n(np.add.accumulate(np.bincount(train_df.AdoptionSpeed)) \/ train_df.shape[0])[:4] * 100","05d3c8a3":"def load_metadata(path):\n    file = path.split(\"\/\")[-1]\n    pet_id = file[:-5].split(\"-\")[0]\n    file_id = file[:-5].split(\"-\")[1]\n    \n    with open(path, encoding=\"utf-8\") as f:\n        jfile = json.loads(f.read())\n    response = {\"labels\": [],\n                \"text\": {\"PetID\": pet_id,\n                         \"FileID\": file_id,\n                         \"description\": \"\"}}\n    \n    if \"labelAnnotations\" in jfile.keys():\n        for anot in jfile[\"labelAnnotations\"]:\n            response[\"labels\"].append({\"PetID\": pet_id,\n                                       \"FileID\": file_id,\n                                       \"description\": anot[\"description\"],\n                                       \"score\": anot[\"score\"]})\n\n    if \"imagePropertiesAnnotation\" in jfile.keys():\n        colors = np.zeros((10, 1, 3), dtype=np.uint8)\n        scores = np.zeros(10)\n        fractions = np.zeros(10)\n        getscore = itemgetter(\"score\")\n        for i, color in enumerate(sorted(jfile['imagePropertiesAnnotation'][\"dominantColors\"][\"colors\"],\n                                         key=getscore,\n                                         reverse=True)\n                                 ):\n\n            for j, c in enumerate([\"red\", \"green\", \"blue\"]):\n                if not color[\"color\"].get(c) is None:\n                    colors[i, 0, j] = color[\"color\"][c] \n                \n            scores[i] = color[\"score\"]\n            fractions[i] = color[\"pixelFraction\"]\n        hsv = cv2.cvtColor(colors, cv2.COLOR_RGB2HSV_FULL)\n        response[\"property\"] = {\"PetID\": pet_id,\n                                \"FileID\": file_id,\n                                \"top_red\": colors[0, 0, 0],\n                                \"top_green\": colors[0, 0, 1],\n                                \"top_blue\": colors[0, 0, 2],\n                                \"top_score\": scores[0],\n                                \"top_fraction\": fractions[0],\n                                \"top_hue\": hsv[0, 0, 0],\n                                \"top_saturation\": hsv[0, 0, 1],\n                                \"top_brightness\": hsv[0, 0, 2],\n                                \"top3_score\": scores[:3].sum(),\n                                \"top3_fraction\": fractions[:3].sum(),\n                                \"top3_area\": np.linalg.norm(np.cross((colors[1] - colors[0])[0], (colors[2] - colors[0])[0])),\n                                \"top10_fraction\": fractions.sum(),\n                                \"top10_score\": scores.sum()}\n\n    if 'cropHintsAnnotation' in jfile.keys():\n        tmp = jfile[\"cropHintsAnnotation\"][\"cropHints\"][0]\n        response[\"crop\"] = {\"PetID\": pet_id,\n                            \"FileID\": file_id,\n                            \"confidence\": tmp[\"confidence\"]}\n        if not tmp.get(\"importanceFraction\") is None:\n            response[\"crop\"][\"importanceFraction\"] = tmp[\"importanceFraction\"]\n    \n    if 'textAnnotations' in jfile.keys():\n        for anot in jfile[\"textAnnotations\"]:\n            response[\"text\"][\"description\"] += anot[\"description\"] + \" \"\n    \n    if \"faceAnnotations\" in jfile.keys():\n        faceanot = jfile[\"faceAnnotations\"][0]\n        response[\"face\"] = {\"PetID\": pet_id,\n                            \"FileID\": file_id,\n                            \"detectionConfidence\": faceanot['detectionConfidence'],\n                            'landmarkingConfidence': faceanot['landmarkingConfidence'],\n                            }\n    \n    return response","74f8af7b":"%%time\nmetadata_path = [dir_ + file for dir_ in [\"..\/input\/petfinder-adoption-prediction\/train_metadata\/\",\n                                          \"..\/input\/petfinder-adoption-prediction\/test_metadata\/\"]\n                                 for file in os.listdir(dir_)]\n\nresults = Parallel(n_jobs=-1, verbose=0)([delayed(load_metadata)(path) for path in metadata_path])\n\nlabels = []\nproperties = []\ncrops = []\nfaces = []\ntexts = []\nfor res in results:\n    if not res.get(\"labels\") is None:\n        labels.extend(res[\"labels\"])\n    if not res.get(\"property\") is None:\n        properties.append(res[\"property\"])\n    if not res.get(\"crop\") is None:\n        crops.append(res[\"crop\"])\n    if not res.get(\"face\") is None:\n        faces.append(res[\"face\"])\n    if not res.get(\"text\") is None:\n        texts.append(res[\"text\"])\n\nlabels_df = pd.DataFrame(labels)\nproperties_df = pd.DataFrame(properties)\ncrops_df = pd.DataFrame(crops)\nfaces_df = pd.DataFrame(faces)\ntexts_df = pd.DataFrame(texts)","67d2246c":"# sentiment ver.\ndef load_sentiments(path):\n    file = path.split(\"\/\")[-1]\n    pet_id = path.split(\"\/\")[-1][:-5]\n    \n    with open(path, encoding=\"utf-8\") as f:\n        jfile = json.loads(f.read())\n    \n    cnt = 0\n    score = []\n    magnitude = []\n    for sent in jfile.get(\"sentences\"):\n        cnt += 1\n        score.append(sent[\"sentiment\"][\"score\"])\n        magnitude.append(sent[\"sentiment\"][\"magnitude\"])\n\n    result = {\"PetID\": pet_id,\n              \"documentSentiment_score\": jfile['documentSentiment'][\"score\"],\n              \"documentSentiment_magnitude\": jfile['documentSentiment'][\"magnitude\"],\n              \"language\": jfile[\"language\"],\n              \"sentense_score_mean\": np.mean(score),\n              \"sentense_score_min\": np.min(score),\n              \"sentense_score_std\": np.std(score),\n              \"sentense_magnitude_mean\": np.mean(magnitude),\n              \"sentense_magnitude_min\": np.min(magnitude),\n              \"sentense_magnitude_std\": np.std(magnitude),\n             }\n    return result","ddfa9446":"%%time\nsentiment_path = [dir_ + file for dir_ in [\"..\/input\/petfinder-adoption-prediction\/train_sentiment\/\",\n                                           \"..\/input\/petfinder-adoption-prediction\/test_sentiment\/\"]\n                                  for file in os.listdir(dir_)]\nsentiment_df = pd.DataFrame(Parallel(n_jobs=-1, verbose=0)([delayed(load_sentiments)(path) for path in sentiment_path]))","60f0c66e":"train_newmeta_df = train_df[[\"PetID\"]]\ntest_newmeta_df = test_df[[\"PetID\"]]\n\n# labelAnnotations\nlabels_global_score = labels_df.groupby(\"PetID\")[\"score\"].agg([\"mean\", \"max\", \"min\", \"std\"])\nlabels_global_score.columns = [\"labels_global_score_\" + col for col in labels_global_score.columns]\n\ntrain_newmeta_df = train_newmeta_df.merge(labels_global_score.reset_index(),\n                          on=\"PetID\",\n                          how=\"left\")\ntest_newmeta_df = test_newmeta_df.merge(labels_global_score.reset_index(),\n                        on=\"PetID\",\n                        how=\"left\")\n\n\n# imagePropertiesAnnotation\nproperties_df.iloc[:, 2:] = (properties_df.iloc[:, 2:] - properties_df.iloc[:, 2:].mean())\/properties_df.iloc[:, 2:].std()\nprofile_properties_df = properties_df[properties_df[\"FileID\"] == \"1\"].drop(\"FileID\", axis=1)\nprofile_properties_df.columns = [\"profile_properties_\" + col if col != \"PetID\" else col for col in profile_properties_df.columns]\n\ntrain_newmeta_df = train_newmeta_df.merge(profile_properties_df,\n                                          on=\"PetID\",\n                                          how=\"left\")\ntest_newmeta_df = test_newmeta_df.merge(profile_properties_df,\n                                        on=\"PetID\",\n                                        how=\"left\")\n\n\nproperties_agg = properties_df.groupby(\"PetID\").agg({\"top_score\": [\"mean\", \"std\"],\n                                                     \"top10_score\": [\"mean\", \"std\"],\n                                                     \"top_fraction\": [\"mean\", \"std\"],\n                                                     \"top10_fraction\": [\"mean\", \"std\"]})\nproperties_agg.columns = [\"property_agg_\" + \"_\".join(col) for col in properties_agg.columns]\n\ntrain_newmeta_df = train_newmeta_df.merge(properties_agg,\n                          on=\"PetID\",\n                          how=\"left\")\ntest_newmeta_df = test_newmeta_df.merge(properties_agg,\n                        on=\"PetID\",\n                        how=\"left\")\n\n# cropHintsAnnotation\nprofile_crops_df = crops_df[crops_df[\"FileID\"] == \"1\"].drop(\"FileID\", axis=1)\ntrain_newmeta_df = train_newmeta_df.merge(profile_crops_df,\n                                          on=\"PetID\",\n                                          how=\"left\")\ntest_newmeta_df = test_newmeta_df.merge(profile_crops_df,\n                                        on=\"PetID\",\n                                        how=\"left\")\n\n# faceAnnotations\nfaces_df.columns = [\"FileID\", \"PetID\", \"face_crop_detectionConfidence\", \"face_crop_landmarkingConfidence\"]\nprofile_faces_df = faces_df[faces_df[\"FileID\"] == \"1\"].drop(\"FileID\", axis=1)\ntrain_newmeta_df = train_newmeta_df.merge(profile_faces_df,\n                                          on=\"PetID\",\n                                          how=\"left\")\ntest_newmeta_df = test_newmeta_df.merge(profile_faces_df,\n                                        on=\"PetID\",\n                                        how=\"left\")","8229ed44":"# agged_features\ntrain_newmeta_df.head()","183303cc":"texts_agg = texts_df.groupby(\"PetID\")[\"description\"].sum().reset_index()\ntexts_agg.columns = [\"PetID\", \"metadata_description\"]\n\ntrain_df = train_df.merge(texts_agg[[\"PetID\", \"metadata_description\"]],\n                          on=\"PetID\",\n                          how=\"left\")\ntest_df = test_df.merge(texts_agg[[\"PetID\", \"metadata_description\"]],\n                        on=\"PetID\",\n                        how=\"left\")","89866fcd":"train_df[\"Description\"] = train_df[\"Description\"].fillna(\"none\") + \" \" + train_df[\"metadata_description\"].fillna(\"none\")\ntest_df[\"Description\"] = test_df[\"Description\"].fillna(\"none\") + \" \" + test_df[\"metadata_description\"].fillna(\"none\")","e2f21149":"# data cleansing\nimport re\n\nall_text = pd.concat([train_df[\"Description\"], test_df[\"Description\"]]).fillna(\"none\").values\nshorted_forms = {\"i'm\":\"i am\",\"i'll\":\"i will\",\"i'd\":\"i had\",\"i've\":\"i have\",\"you're\":\"you are\",\"you'll\":\"you will\",\"you'd\":\"you had\",\"you've\":\"you have\",\"he's\":\"he has\",\"he'll\":\"he will\",\"he'd\":\"he had\",\"she's\":\"she has\",\"she'll\":\"she will\",\"she'd\":\"she had\",\"it's (or \u2018tis)\":\"it is\",\"it'll\":\"it will\",\"it'd\":\"it had\",\"it's\":\"it is\",\"we're\":\"we are\",\"we'll\":\"we will\",\"we'd\":\"we had\",\"we've\":\"we have\",\"they're\":\"they are\",\"they'll\":\"they will\",\"they'd\":\"they had\",\"they've\":\"they have\",\"that's\":\"that has\",\"that'll\":\"that will\",\"that'd\":\"that had\",\"who's\":\"who has\",\"who'll\":\"who will\",\"who'd\":\"who had\",\"what's\/what're\":\"what is\/what are\",\"what'll\":\"what will\",\"what'd\":\"what had\",\"what's\":\"what is\",\"where's\":\"where has\",\"where'll\":\"where will\",\"where'd\":\"where had\",\"when's\":\"when has\",\"when'll\":\"when will\",\"when'd\":\"when had\",\"why's\":\"why has\",\"why'll\":\"why will\",\"why'd\":\"why had\",\"how's\":\"how has\",\"how'll\":\"how will\",\"how'd\":\"how had\",\"what're\":\"what are\",\"isn't\":\"is not\",\"aren't\":\"are not\",\"wasn't\":\"was not\",\"weren't\":\"were not\",\"haven't\":\"have not\",\"hasn't\":\"has not\",\"hadn't\":\"had not\",\"won't\":\"will not\",\"wouldn't\":\"would not\",\"don't\":\"do not\",\"doesn't\":\"does not\",\"didn't\":\"did not\",\"can't\":\"cannot\",\"couldn't\":\"could not\",\"shouldn't\":\"should not\",\"mightn't\":\"might not\",\"mustn't\":\"must not\"}\nnoalphabet = set()\nfor text in tqdm(all_text):\n    noalphabet.update(list(re.sub(\"[0-9a-zA-Z\\s]\", \"\", text)))\n\ncleaned_texts = []\nnoalphabet_count = []\nrepwords = \"|\".join(map(re.escape, noalphabet))\nfor text in all_text:\n    text = text.lower()\n    for k, v in shorted_forms.items():\n        text = text.replace(k, v)\n    noalphabet_count.append(len(re.findall(repwords, text)))\n    text = re.sub(repwords, \" \", text)\n    cleaned_texts.append(re.sub(\"([0-9]+)\", \"\", text))","d6cfdfec":"# description\ntfv = TfidfVectorizer(min_df=3,  max_features=None,\n        strip_accents='unicode', analyzer='word', token_pattern=r'\\w{1,}',\n        ngram_range=(1, 3), use_idf=1, smooth_idf=1, sublinear_tf=1,\n        stop_words = 'english')\n\ntfv.fit(cleaned_texts)\nX_tfv = tfv.transform(cleaned_texts)\n\nsvd = TruncatedSVD(n_components=150, random_state=2434)\nsvd.fit(X_tfv)\nX_desc_tfv_svd = svd.transform(X_tfv)\n\nsvd = TruncatedSVD(n_components=16, random_state=2434)\nsvd.fit(X_tfv)\nX_desc_tfv_svd_mini = svd.transform(X_tfv)","fd59714f":"labels_agg = labels_df.groupby([\"PetID\", \"description\"])[\"score\"].max().reset_index()\n\nlabel_agg_text = labels_agg.groupby(\"PetID\")[\"description\"]\\\n                           .apply(lambda x: \" \".join(x))\\\n                           .reset_index()\\\n                           .rename(columns={\"description\": \"label_description_1\"})\n\ntrain_df = train_df.merge(label_agg_text,\n                          on=\"PetID\",\n                          how=\"left\")\ntest_df = test_df.merge(label_agg_text,\n                        on=\"PetID\",\n                        how=\"left\")\n\n# spsp as dammy\nlabel_agg_text = labels_agg.groupby(\"PetID\")[\"description\"]\\\n                           .apply(lambda x: \" spsp spsp \".join(x))\\\n                           .reset_index()\\\n                           .rename(columns={\"description\": \"label_description_2\"})\n\ntrain_df = train_df.merge(label_agg_text,\n                          on=\"PetID\",\n                          how=\"left\")\ntest_df = test_df.merge(label_agg_text,\n                        on=\"PetID\",\n                        how=\"left\")","d833ffd9":"# space version\nlebeldesc_texts = pd.concat([train_df[\"label_description_1\"], test_df[\"label_description_1\"]]).fillna(\"none\").values\n\ntfv = TfidfVectorizer(min_df=3,  max_features=None,\n        strip_accents='unicode', analyzer='word', token_pattern=r'\\w{1,}',\n        ngram_range=(1, 3), use_idf=1, smooth_idf=1, sublinear_tf=1,\n        stop_words = 'english')\n\ntfv.fit(lebeldesc_texts)\nX_tfv = tfv.transform(lebeldesc_texts)\nprint(\"shape is\", X_tfv.shape)\n\nsvd = TruncatedSVD(n_components=70, random_state=2434)#NMF(n_components=150, random_state=2434, shuffle=True, verbose=True)\nsvd.fit(X_tfv)\n\nX_labeldesc_tfv_svd = svd.transform(X_tfv)\n","1b31a3ea":"X_train = X_labeldesc_tfv_svd[:len(train_df)]\nX_test = X_labeldesc_tfv_svd[len(train_df):]\ny_train = train_df[\"AdoptionSpeed\"]\ngroup = train_df[\"group\"]\n\nparam = {'num_leaves': 31,\n         'min_data_in_leaf': 32, \n         'objective':'regression',\n         'max_depth': -1,\n         'learning_rate': 0.01,\n         \"min_child_samples\": 20,\n         \"boosting\": \"gbdt\",\n         \"feature_fraction\": 0.9,\n         \"bagging_freq\": 1,\n         \"bagging_fraction\": 0.9 ,\n         \"bagging_seed\": 11,\n         \"metric\": 'rmse',\n         \"lambda_l1\": 0.1,\n         \"nthread\":nthread,\n         \"verbosity\": -1}\n\ntrain_oof, test_oof = make_lgb_oofs(X_train, y_train, group, X_test, param)","08253b30":"print(rmse(y_train, train_oof))","9994ebd8":"# dammy version\nlebeldesc_texts = pd.concat([train_df[\"label_description_2\"], test_df[\"label_description_2\"]]).fillna(\"none\").values\n\ntfv = TfidfVectorizer(min_df=3,  max_features=None,\n        strip_accents='unicode', analyzer='word', token_pattern=r'\\w{1,}',\n        ngram_range=(1, 3), use_idf=1, smooth_idf=1, sublinear_tf=1,\n        stop_words = 'english')\n\ntfv.fit(lebeldesc_texts)\nX_tfv = tfv.transform(lebeldesc_texts)\nprint(\"shape is\", X_tfv.shape)\n\nsvd = TruncatedSVD(n_components=70, random_state=2434)#NMF(n_components=150, random_state=2434, shuffle=True, verbose=True)\nsvd.fit(X_tfv)\n\nX_labeldesc_tfv_svd = svd.transform(X_tfv)\n","8bb9eef0":"X_train = X_labeldesc_tfv_svd[:len(train_df)]\nX_test = X_labeldesc_tfv_svd[len(train_df):]\ny_train = train_df[\"AdoptionSpeed\"]\ngroup = train_df[\"group\"]\n\nparam = {'num_leaves': 31,\n         'min_data_in_leaf': 32, \n         'objective':'regression',\n         'max_depth': -1,\n         'learning_rate': 0.01,\n         \"min_child_samples\": 20,\n         \"boosting\": \"gbdt\",\n         \"feature_fraction\": 0.9,\n         \"bagging_freq\": 1,\n         \"bagging_fraction\": 0.9 ,\n         \"bagging_seed\": 11,\n         \"metric\": 'rmse',\n         \"lambda_l1\": 0.1,\n         \"nthread\":nthread,\n         \"verbosity\": -1}\n\ntrain_oof, test_oof = make_lgb_oofs(X_train, y_train, group, X_test, param)","6b98d9a1":"print(rmse(y_train, train_oof))","de0f2254":"import nltk\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom tqdm import tqdm\n\ndef load_fasttext_vectors(EMBEDDING_FILE):\n    vectors = dict()\n    with open(EMBEDDING_FILE, \"r\", encoding=\"utf-8\") as f:\n        for line in tqdm(f.readlines()[1:]):\n            key, *vec = line.rstrip().split()\n            vectors[key] = np.array(vec, dtype=np.float32)\n    return vectors\n\ndef load_glove_vectors(EMBEDDING_FILE):\n    vectors = dict()\n    with open(EMBEDDING_FILE, \"r\", encoding=\"utf-8\") as f:\n        for line in tqdm(f.readlines()):\n            key, *vec = line.rstrip().split()\n            vectors[key] = np.array(vec, dtype=np.float32)\n    return vectors\n\ndef get_swem_vectors(cleaned_texts, word_vectors, dim=300):\n    dim = word_vectors[\"word\"].shape[0]\n    swem_result = np.zeros((len(cleaned_texts), dim), dtype=np.float32) + 1e-6\n    exist_words = set(word_vectors.keys())\n\n    for i, text in tqdm(enumerate(cleaned_texts)):\n        vecs = []\n        for word in nltk.word_tokenize(text):\n            word = word.lower()\n            if word in exist_words:\n                vecs.append(word_vectors[word])\n        if len(vecs):\n            V = np.vstack(vecs)\n            swem_result[i] = V[np.argmax(np.abs(V), axis=0), np.arange(dim)]\n    return swem_result\n\ndef get_idf_weighted_vectors(cleaned_texts, word_vectors, idfs, dim=300):\n    dim = word_vectors[\"word\"].shape[0]\n    result = np.zeros((len(cleaned_texts), dim), dtype=np.float32) + 1e-6\n    exist_words = set(word_vectors.keys()) & set(idfs.keys())\n    \n    for i, text in tqdm(enumerate(cleaned_texts)):\n        idf_sum = 0\n        vec = np.zeros(dim, dtype=np.float32)\n        for word in nltk.word_tokenize(text):\n            word = word.lower()\n            if word in exist_words:\n                vec += word_vectors[word] * idfs[word]\n                idf_sum += idfs[word]\n        if idf_sum > 0:\n            vec \/= idf_sum\n        result[i] = vec\n    return result","7134a761":"tfidf = TfidfVectorizer()\ntfidf.fit(cleaned_texts)\nidfs = {k:tfidf.idf_[v] for k, v in tfidf.vocabulary_.items()}\n\n\nEMBEDDING_FILE = \"..\/input\/fatsttext-common-crawl\/crawl-300d-2M\/crawl-300d-2M.vec\"\ncommoncrawl_vectors = load_fasttext_vectors(EMBEDDING_FILE)\n\ncommoncrawl_swems = get_swem_vectors(cleaned_texts, commoncrawl_vectors) + 1e-8\ncommoncrawl_idfs = get_idf_weighted_vectors(cleaned_texts, commoncrawl_vectors, idfs)","8eddbe16":"del commoncrawl_vectors\ncollect()","52b06cfa":"rescuerid_tf = rescuerid_encoder.transform(pd.concat([train_df[\"RescuerID\"], test_df[\"RescuerID\"]]))\n\ncossim_res = []\nvecsize = np.sqrt(np.square(commoncrawl_swems).sum(axis=1))\nfor i in tqdm(range(commoncrawl_swems.shape[0])):\n    tmp = {}\n    cossim = ((commoncrawl_swems[i].reshape(1, -1) @ commoncrawl_swems.T) \/ (vecsize[i] * vecsize))[0]\n    cossim[i] = 0\n    same_rescuer = rescuerid_tf[i] == rescuerid_tf\n    different_cossim = cossim[np.where(same_rescuer^1)[0]]\n    tmp[\"defferent_rescuer_cossim_mean\"] = different_cossim.mean()\n    tmp[\"defferent_rescuer_cossim_std\"] = different_cossim.std()\n    tmp[\"defferent_rescuer_cossim_max\"] = different_cossim.max()\n    if same_rescuer.sum() > 1:\n        same_rescuer[i] = False\n        same_cossim = cossim[np.where(same_rescuer)[0]]\n        tmp[\"same_rescuer_cossim_mean\"] = same_cossim.mean()\n        tmp[\"same_rescuer_cossim_std\"] = same_cossim.std()\n        tmp[\"same_rescuer_cossim_max\"] = same_cossim.max()\n    cossim_res.append(tmp)\n\ncossim_df = pd.DataFrame(cossim_res)\ncossim_df.columns = [\"commoncrawl_\" + col for col in cossim_df.columns]","588993e4":"# image","ea9c9b25":"from keras.applications.densenet import DenseNet121, preprocess_input\nfrom keras.models import Model\nfrom keras.layers import Input, GlobalAveragePooling2D, Dense, Dropout, BatchNormalization\n\ndef make_model(BaseModel, base_shape, weights=\"imagenet\"):\n    inp = Input(base_shape)\n    base_model = BaseModel(input_tensor=inp, weights=weights, include_top=False)\n    x = base_model.output\n    out = GlobalAveragePooling2D()(x)\n    model = Model(inp, out)\n    return model\n\nweight_path = \"..\/input\/densenet-keras\/DenseNet-BC-121-32-no-top.h5\"\n\ndense121 = make_model(DenseNet121, (224, 224, 3), weights=weight_path)","c57eeb45":"import cv2\nfrom skimage import feature\nfrom imagehash import whash\nfrom PIL import Image\n\ndef resize_image(image, resized_shape):\n    h, w, c = image.shape\n    if h > w:\n        new_image = np.zeros((h, h, c), dtype=np.uint8)\n        left = (h-w)\/\/2\n        right = left + w\n        new_image[:, left:right, :] = image\n    else:\n        new_image = np.zeros((w, w, c), dtype=np.uint8)\n        top = (w-h)\/\/2\n        bottom = top + h\n        new_image[top:bottom, :, :] = image\n    resized_image = cv2.resize(new_image, resized_shape, cv2.INTER_LANCZOS4)\n    return resized_image\n\ndef shrink_image(image):\n    h, w, c = image.shape\n    if h > w:\n        new_h = 224\n        new_w = int((w * 224)\/\/h)\n    else:\n        new_w = 224\n        new_h = int((h * 224)\/\/w)\n    return cv2.resize(image, (new_w, new_h), cv2.INTER_LANCZOS4)\n\ndef padding_image(image):\n    h, w, c = image.shape\n    new_image = np.zeros((224, 224, 3), dtype=np.uint8)\n    if h == 224:\n        left = (h-w)\/\/2\n        right = left + w\n        new_image[:, left:right, :] = image\n    else:\n        top = (w-h)\/\/2\n        bottom = top + h\n        new_image[top:bottom, :, :] = image\n    return new_image","c50cfd19":"def image_analysis(path):\n    res = {}\n    \n    res[\"PetID\"], res[\"FileID\"] = path.split(\"\/\")[-1][:-4].split(\"-\")\n    image = cv2.imread(path)[:,:,[2, 1, 0]]\n    image_hight, image_width = image.shape[:2]\n    image_size = image_hight * image_width\n    image_aspect = image_width \/ image_hight\n    \n    grey_image = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n    canny_s3 = float(np.sum(feature.canny(grey_image, sigma=3))) \/ image_size\n    blurrness = cv2.Laplacian(grey_image, cv2.CV_64F).var()\n    \n    whash_res = whash(Image.fromarray(image))\n\n    dark_percent = np.all(image.reshape(-1, 3) <= 20, axis=1).mean()\n    light_percent = np.all(image.reshape(-1, 3) >= 240, axis=1).mean()\n\n    hsv_image = cv2.cvtColor(image, cv2.COLOR_RGB2HSV_FULL)\n    hue, saturation, brightness = hsv_image[:, :, 0], hsv_image[:, :, 1], hsv_image[:, :, 2]\n    hue_degree = hue \/ 255 * 2 * np.pi\n    hue_sin, hue_cos = np.sin(hue_degree), np.cos(hue_degree)\n\n    red, green, blue = image[:, :, 0], image[:, :, 1], image[:, :, 2]\n    rg, yb = red - green, (red+green)\/2 - blue\n    colorfulness = np.sqrt(rg.var() + yb.var()) + 0.3*np.sqrt(np.square(rg.mean()) + np.square(yb.mean()))\n\n    grayscale_simplicty = (np.cumsum(np.sort(np.histogram(grey_image.ravel(), 255, [0, 255])[0])) > image_size * .05).mean()\n    hue_simplicty = (np.cumsum(np.sort(np.histogram(hue.ravel(), 255, [0, 255])[0])) > image_size * .05).mean()\n\n    if res[\"FileID\"] == \"1\":\n        resized_image = resize_image(image, (224, 224))\n        #res_feature = extract_deep_feature(resized_image, Res_exter, resnet50)\n        #dense_feature = extract_deep_feature(resized_image, Dense_exter, densenet)\n        # deep_feature = np.hstack([vgg_feature, res_feature, dense_feature])[0]\n    else:\n        resized_image = None    \n    \n    res.update({\"image_hight\": image_hight,\n                \"image_width\": image_width,\n                \"image_size\": image_size,\n                \"image_aspect\": image_aspect,\n                \"dark_percent\": dark_percent,\n                \"light_percent\": light_percent,\n                \"canny_s3\": canny_s3,\n                \"blurrness\": blurrness,\n                \"hue_sin_mean\": hue_sin.mean(),\n                \"hue_cos_mean\": hue_cos.mean(),\n                \"red_mean\": red.mean(),\n                \"red_std\": red.std(),\n                \"green_mean\": green.mean(),\n                \"green_std\": green.std(),\n                \"blue_mean\": blue.mean(),\n                \"blue_srd\": blue.std(),\n                \"saturation_mean\": saturation.mean(),\n                \"saturarion_std\": saturation.std(),\n                \"brightness_mean\": brightness.mean(),\n                \"brightness_std\": brightness.std(),\n                \"colorfulness\": colorfulness,\n                \"greyscale_simplicity\": grayscale_simplicty,\n                \"hue_simplicty\": hue_simplicty,\n                \"whash\": whash_res,\n                \"image\": resized_image,\n                })\n    return res","bbf28a19":"def split_extracter(paths, exter, preprocess_func, n_splits=10):\n    splited_len = -(-len(paths)\/\/n_splits)\n    image_feat_df = pd.DataFrame()\n    exter_feature = []\n    all_keys = []\n    for j in tqdm(range(n_splits)):\n        r = Parallel(n_jobs=-1, verbose=0)([delayed(image_analysis)(image_path) for image_path in paths[splited_len*j:splited_len*(j+1)]])\n\n        keys = []\n        images = []\n\n        mini_images = np.zeros((len(r), 96, 96, 3))\n        for i in range(len(r)):\n            image = r[i].pop(\"image\")\n            if not image is None:\n                keys.append(r[i][\"PetID\"])\n                images.append(image)\n\n        tmp_image_feat = pd.DataFrame(r)\n        image_feat_df = pd.concat([image_feat_df, tmp_image_feat])        \n        image_array = np.zeros((len(keys), 224, 224, 3), dtype=np.float32)\n\n        for i in range(len(keys)):\n            image_array[i] = images[i]\n    \n        exter_feature.append(exter.predict(preprocess_func(image_array.astype(np.float32)),\n                                                   batch_size=32, verbose=1))\n        all_keys.extend(keys)\n\n    exter_feature = np.vstack(exter_feature)\n    exter_df = pd.DataFrame(all_keys)\n    exter_df.columns = [\"PetID\"]\n    exter_df = pd.concat([exter_df, pd.DataFrame(exter_feature)], axis=1)    \n         \n    return image_feat_df, exter_df","7c0e43c0":"image_path = [dir_ + file for dir_ in [\"..\/input\/petfinder-adoption-prediction\/train_images\/\",\n                                       \"..\/input\/petfinder-adoption-prediction\/test_images\/\"]\n                              for file in os.listdir(dir_)][:1000]","aabbfd45":"# if you use full dataset, it takes\n# densenet121: 4500 sec\n# gloval feature: 2700 sec\n# total: 7200 sec\n\nimage_feat_df, exter_df = split_extracter(image_path, dense121, preprocess_func=preprocess_input)","7f7bac12":"image_feat_df.head()","fd8883a8":"# Settings","af74e468":"# NLP","77e02ee9":"below is aggligateed features","9bbae6b2":"# image","77d7f4b7":"# metadata\/sentiment","cab27382":"## key point 1: Customized OptimizedRounder\ninitial_coef is initialized by AdoptionSpeed ratio","d3cf6f1b":"Great! It takes only 1 minutes.","32dadbb2":"## Key point 2: fast json loader\npd.io.json.json_normalize is easy to use but slow.\n1. \nSo I wrote metadata loader without pandas","47fc5293":"# Key point 5? : gloval image features\nI used this slide as a reference.\n\nhttps:\/\/www.slideshare.net\/JinZhan\/kaggle-avito-demand-prediction-challenge-9th-place-solution-124500050 # 21\n\nand I implemented the features described in this paper.\nCheng, H. et al. (2012). Multimedia Features for Click Prediction of New Ads in Display Advertising\n\n","cde78d57":"72nd place keypoints\n====================\n\nSince there are so many kernels, it is hard to follow other people's solutions. So I pick up codes from my solution.\n","565c52ae":"## Key point 3: concat labeldescriptions by dammy word","484c1181":"# Key point 4: text similarity\nDepending on RescuerID, you can see that the boilerplates are used. \n\nTherefore, I created text similarity as a feature using text embeddings."}}