{"cell_type":{"1c059b51":"code","ca2d9a4f":"code","abb7a88e":"code","794eaefa":"code","80abc740":"code","af4baf14":"code","754ad36c":"code","77498167":"code","3ec979af":"code","02565b4f":"code","78cf62ea":"code","6e1124b3":"code","9b797c07":"code","46ceba59":"code","b69c935a":"code","608b79b2":"code","48d834a9":"code","8e2eece3":"markdown","e8a71e42":"markdown","3abfffb7":"markdown","9aee7cf9":"markdown"},"source":{"1c059b51":"#Start notebook, import libraries and open file\n\nimport numpy as np\nimport pandas as pd\nfrom collections import Counter\nimport string\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\n#This part is useful for getting the filename and location the first time but after that you don't need it\n\"\"\"\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\"\"\"\n        \nfile = \"\/kaggle\/input\/scifi-stories-text-corpus\/internet_archive_scifi_v3.txt\"\n\n# should use context manager to open files but this doesn't work with Jupyter notebooks (which Kaggle is based on)\ntext = open(file,'r').read()\n\n# This is a small sample of the dataset that we can play with without long processing times\n# Hence the 'toy' name\ntoy_text = text[0:1000000]\n\nprint('done')","ca2d9a4f":"# split text into words and count most common\n\nlst = toy_text.split(\" \")\nprint('Number of characters = ', len(lst), '\\nNumber of words = ', len(lst), '\\n')\n\nsc = Counter(lst)\nprint(sc.most_common(20))","abb7a88e":"# Those words show repetition as uppercase does not equal lowercase\n\ntxt1 = 'hello'\ntxt2 = 'Hello'\n\nprint(txt1 == txt2)","794eaefa":"# Lets turn everything to lowercase and try again\n\nlower_lst = toy_text.lower().split(\" \")\nlower_sc = Counter(lower_lst)\nprint(lower_sc.most_common(20))","80abc740":"# Some of those results are just punctuation so lets remove them.\n\ntext_snip = toy_text[0:1000]\n\nprint('Original has punctuation: \\n', text_snip, '\\n')\n\ntext_snip2 = text_snip.translate(str.maketrans('', '', string.punctuation))\n\nprint('This one does not:\\n', text_snip2)","af4baf14":"# Lets put this all together and find out our sci-fi themes!\n\nfixed_text = text.lower().translate(str.maketrans('', '', string.punctuation)).split(\" \")\nfixed_sc = Counter(fixed_text)\n\nprint(fixed_sc.most_common(20))","754ad36c":"# You have to look at the top 200 results before you start getting sci-fi words!\n\nmc200 = fixed_sc.most_common(200)\n\nprint(mc200)","77498167":"# Zipf's Law\n\n# It has been observed that if you count the frequency of words in a text \n# then multiply that by the rank order of the words they tend to be equal\n\nimport matplotlib.pyplot as plt\n\ndf = pd.DataFrame.from_dict(fixed_sc, orient='index')\ndf.sort_values(0, ascending=False, inplace=True)\n\ndf = df.rename(columns={None :'event', 0:'count'})\ndf['rank'] = df['count'].rank(method='max', ascending=False)\ndf['Zipfs'] = df['count'] * df['rank']\n\ndf.iloc[0:30].plot(y='Zipfs')","3ec979af":"# Lets get rid of these 'stop' words\n\nimport nltk\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\n\nstop_words = stopwords.words('english')\n\n# We can extend this stopword list if we want\n\"\"\"\nstop_words.extend(['earth', 'alien', 'being'])\n\"\"\"\n\nremoved_stopwords = [word for word in fixed_text if word not in stop_words and word != '']\n\nprint(removed_stopwords[0:100])","02565b4f":"removed_sc = Counter(removed_stopwords)\n\nprint(removed_sc.most_common(100))","78cf62ea":"# Lets visualise this in a word cloud\n# NB. word clouds are not a scientific method as they have too many variables, but they do look pretty =)\n# Be careful running this function with NLTK as they are using similar key words (i.e. stopwords and STOPWORDS)\n\nimport matplotlib.pyplot as plt\nfrom wordcloud import WordCloud, STOPWORDS\n\ndef plot_cloud(wordcloud):\n    plt.figure(figsize=(40, 30))\n    plt.imshow(wordcloud) \n    plt.axis(\"off\");\n    \n# Notebooks allow you to run a function over multiple lines making it easier to read\n# Wordcloud also does the analysis for you in terms of counting words and removing stopwords\nwordcloud = WordCloud(width= 3000, height = 2000, random_state=1, background_color='black', \n                      colormap='Set2', collocations=False, stopwords = STOPWORDS).generate(text)\n\nplot_cloud(wordcloud)","6e1124b3":"# Using the previous text_snip2 (this is the first 1000 characters with punctuation removed) lets start creating vectors\n\n# This function has a lot of elements so lets unpack it\n# text_snip2.lower().split(\" \") means turn all words to lowercase and then split them on spaces\n# [word for word ... if word != ''] is a list comprehension that is removing blank entries\n# set(...) gets rid of all repeats\n# list(...) turns it back into a list so that we can use it\nall_words = list(set([word for word in text_snip2.lower().split(\" \") if word != '']))\n\n# now lets go back to the original text and find the sentences\nsentence_list = toy_text[0:1000].lower().split('. ')\n\nvector_list = []\n\nfor sentence in sentence_list:\n    # create an array that is the same length as the all_words array\n    sentence_vector = [0] * len(all_words)\n    sentence = sentence.translate(str.maketrans('', '', string.punctuation))\n    for word in sentence.split(\" \"):\n        if word == '':\n            continue\n        # This function is getting confused by abbreviations with full stops (i.e. U.S.A.)\n        # They aren't important for this demo so we will just skip them\n        elif word not in all_words:\n            continue\n        else:\n            # find the matching entry in the all_words list  so you know which point in the array to update\n            ind = all_words.index(word)\n            # increase the count for that entry by 1\n            sentence_vector[ind] = sentence_vector[ind] + 1\n    vector_list.append(sentence_vector)\n\n# This is just to make it easier to read\nfor vector in vector_list:\n    print(vector,'\\n')","9b797c07":"# This library gives us the frequency of a word in a story vs in the entire corpus\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n# this is to split the text into stories\ncorpus = text.split(\"All stories are fiction\")\n\ncorpus.pop(0)\ncorpus.pop(58)\n\n# ignore words that occur less than 10 times\ntfIdfVectorizer=TfidfVectorizer(use_idf=True, min_df= 10)\ntfIdf = tfIdfVectorizer.fit_transform(corpus)\n\n\nfeature_names = tfIdfVectorizer.get_feature_names()\ndense = tfIdf.todense()\ndenselist = dense.tolist()\ndf = pd.DataFrame(denselist, columns=feature_names)\n\ndf.head()","46ceba59":"from sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA\n\nkmeans = KMeans(n_clusters=4).fit(tfIdf)\nclusters =  kmeans.predict(tfIdf)\n\ndf['kmeans_cluster'] = clusters\n\ndf.head()","b69c935a":"# How can we know how many clusters to use? By trying them all!!\nnum_clusters = 50\nkmeans_tests = [KMeans(n_clusters=i, init='random', n_init=10) for i in range(1, num_clusters)]\nscore = [kmeans_tests[i].fit(tfIdf).score(tfIdf) for i in range(len(kmeans_tests))]\n\n# Plot the curve\nplt.plot(range(1, num_clusters),score)\nplt.xlabel('Number of Clusters')\nplt.ylabel('Score')\nplt.title('Elbow Curve')\nplt.show()","608b79b2":"from sklearn.feature_extraction.text import CountVectorizer\n\ncountVect = CountVectorizer(min_df= 10) #, binary=True, max_df= 50)\nbinaryVector = countVect.fit_transform(corpus)\n\nfeature_names = countVect.get_feature_names()\ndense = binaryVector.todense()\ndenselist = dense.tolist()\ndf = pd.DataFrame(denselist, columns=feature_names)\n\ndf.head()","48d834a9":"# How can we know how many clusters to use? By trying them all!!\nnum_clusters = 50\nkmeans_tests = [KMeans(n_clusters=i, init='random', n_init=10) for i in range(1, num_clusters)]\nscore = [kmeans_tests[i].fit(binaryVector).score(binaryVector) for i in range(len(kmeans_tests))]\n\n# Plot the curve\nplt.plot(range(1, num_clusters),score)\nplt.xlabel('Number of Clusters')\nplt.ylabel('Score')\nplt.title('Elbow Curve')\nplt.show()","8e2eece3":"### Lets put it all together with actual python libraries","e8a71e42":"### This curve isn't great \n\n#### This is probably due to the 'curse of dimensionality'\n\nWe have over 14,000 dimensions and only 58 stories\n\nPotentially we could do better if we look to see if a word occurs at all in any of the story","3abfffb7":"### The problem is that most of the words we use don't contribute much. Instead they provide context. \n\nIn the above sentence the only words we would want to look at are 'problem', 'contribute' and 'context'.","9aee7cf9":"### One common approach to looking at data is to create a vector of words\nThis is where you create an array with each place representing a word."}}