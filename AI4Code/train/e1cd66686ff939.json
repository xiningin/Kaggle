{"cell_type":{"47e75d5d":"code","a62c78c3":"code","e87ac6df":"code","83614596":"code","0d5fe0b4":"code","87ec6104":"code","12bba927":"code","23ed7ec3":"code","083bb184":"code","116a0dac":"code","8fd03d84":"code","d080b77b":"code","0bd30718":"code","4d532450":"code","f6fb8fc9":"code","3ed4b642":"code","9daa556c":"code","637e4746":"code","22f71c0e":"code","c0974c4e":"code","96efe15d":"code","0127aa3e":"code","95c56071":"code","77356f66":"code","421092f7":"code","bd470f3d":"code","09f31b0f":"code","371518b8":"code","ff699069":"code","5238bd03":"code","eac5382b":"code","a1c7968e":"code","7cf2dbb6":"code","fdc932da":"code","f8ccc869":"markdown","e0be5217":"markdown"},"source":{"47e75d5d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","a62c78c3":"data_train=pd.read_csv('\/kaggle\/input\/instagram-fake-spammer-genuine-accounts\/train.csv')\ndata_train","e87ac6df":"data_test=pd.read_csv('\/kaggle\/input\/instagram-fake-spammer-genuine-accounts\/test.csv')\ndata_test","83614596":"data = pd.concat([data_train, data_test], axis=0)\ndata","0d5fe0b4":"data.info()","87ec6104":"data.nunique()","12bba927":"data.describe()","23ed7ec3":"from scipy.stats import skew\ndata['fake'].skew()","083bb184":"corr_mat=data_train.corr().round(2)\n\nimport seaborn as sns\nsns.heatmap(corr_mat, annot=True)","116a0dac":"data_train.skew()","8fd03d84":"data_train['#followers'] = np.log1p(data_train['#followers'])\ndata_train['#posts'] = np.log1p(data_train['#posts'])","d080b77b":"X=data_train.drop(columns=['fake'])\nY=data_train['fake']","0bd30718":"Y.value_counts()","4d532450":"X.drop(columns=['nums\/length fullname','name==username', 'external URL'], inplace=True)","f6fb8fc9":"from sklearn.model_selection import train_test_split\nX_train,X_test,Y_train,Y_test=train_test_split(X,Y,test_size=0.1,random_state=7)\nprint(X_train.shape)\nprint(X_test.shape)\nprint(Y_train.shape)\nprint(Y_test.shape)","3ed4b642":"Y_train.value_counts()","9daa556c":"from imblearn.under_sampling import RandomUnderSampler\n\nrus = RandomUnderSampler(random_state=42, replacement=True)# fit predictor and target variable\nX_train, Y_train = rus.fit_resample(X_train, Y_train)\n\n","637e4746":"# from imblearn.over_sampling import SMOTE\n# smote = SMOTE(sampling_strategy='auto')\n# X_train, Y_train = smote.fit_sample(X_train, Y_train)\n# print(X_train.shape,Y_train.shape)","22f71c0e":"from sklearn.preprocessing import MinMaxScaler\nsc = MinMaxScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)\n","c0974c4e":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import GridSearchCV\n# from sklearn.linear_model import ElasticNet\n# from sklearn.linear_model import Ridge\nr_estimator=LogisticRegression(max_iter = 1000)\n\nparameters={'penalty' : ['l1', 'l2', 'elasticnet' ],\n               'class_weight' : ['balanced'],\n           'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']}\ngrid=GridSearchCV(estimator=r_estimator,param_grid=parameters,cv=7,verbose=True,n_jobs=-1)\ngrid.fit(X_train,Y_train)\ngrid.best_params_","96efe15d":"\nfrom  sklearn.linear_model import RidgeClassifier\n\nmodel =RidgeClassifier()\n\nmodel.fit(X_train, Y_train)","0127aa3e":"\nfrom sklearn.ensemble import RandomForestClassifier\n\nmodel = RandomForestClassifier(n_estimators = 500)\n\nmodel.fit(X_train, Y_train)","95c56071":"a = model.feature_importances_\nfor i in range(1, len(a)):\n    if a[i] < 0.01:\n        print('the columns ', X.columns[i], ' has feature value ', a[i])\n    \n","77356f66":"from sklearn.linear_model import LogisticRegression\nmodel = LogisticRegression(class_weight = 'balanced',\n                          penalty = 'l1',\n                          solver = 'liblinear',\n                          max_iter = 1000)\nmodel.fit(X_train, Y_train)","421092f7":"pred = model.predict(X_test)\npred","bd470f3d":"from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, classification_report, auc, roc_curve\n\nconfusion_matrix(pred, Y_test)\n","09f31b0f":"recall_score(Y_test, pred)","371518b8":"f1_score(pred,Y_test)","ff699069":"print(classification_report(pred, Y_test))","5238bd03":"import matplotlib.pyplot as plt\nfpr, tpr, thres = roc_curve(Y_test,  pred)\nplt.scatter(fpr, tpr)\nthres","eac5382b":"from sklearn.metrics import roc_auc_score\nroc_auc_score(pred, Y_test)","a1c7968e":"from sklearn.metrics import roc_auc_score\nimport matplotlib.pyplot as plt\ny_pred_proba = model.predict_proba(X_test)[::,1]\nfpr, tpr, _ = roc_curve(Y_test,  y_pred_proba)\nauc = roc_auc_score(Y_test, pred)\nplt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc))\nplt.legend(loc=4)\nplt.show()","7cf2dbb6":"plt.scatter(fpr, tpr)\nplt.show()","fdc932da":"# import plotly.graph_objects as go\n# import numpy as np\n\n\n# N = 70\n\n# fig = go.Figure(data=[go.Mesh3d(x=200*data['#follows'],\n#                    y=200*data['nums\/length username'],\n#                    z=200*data['#posts'],\n#                    opacity=0.5,\n#                    color='rgba(244,22,100,0.6)'\n#                   )])\n\n# fig.update_layout(\n#     scene = dict(\n#         xaxis = dict(nticks=4, range=[-100,100],),\n#                      yaxis = dict(nticks=4, range=[-50,100],),\n#                      zaxis = dict(nticks=4, range=[-100,100],),),\n#     width=700,\n#     margin=dict(r=20, l=10, b=10, t=10))\n\n# fig.show()","f8ccc869":"# converging two data set to single data set","e0be5217":"# logistic regresson"}}