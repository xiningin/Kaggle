{"cell_type":{"5a469404":"code","e0a0e3f8":"code","8cd59715":"code","1223ca81":"code","b1cf0b73":"code","d37b09ad":"code","07198fbc":"code","b75b9d9a":"code","3061dca9":"code","31797193":"code","aef23e48":"code","17dbec65":"code","0976ae3b":"code","90b82aa7":"code","3ce9837a":"code","b810b584":"code","3c0bbcdf":"code","a5344adb":"code","9ff690b2":"code","1019b38b":"code","8af3a3aa":"code","a0eecaac":"code","48aa7ec4":"code","12924e59":"code","74b14c19":"code","d15fdbd7":"code","ecadf2f7":"code","86d8d08d":"code","d45916c8":"code","68e17020":"code","6fc11ab9":"code","10469a93":"code","98632213":"code","67e04043":"code","bd26118f":"code","e26d467b":"code","f9077c96":"code","7180b1e7":"code","2809c1bf":"code","f5cc7911":"code","93322b8b":"code","44fd3d7a":"code","fff87c10":"code","4aa062b7":"code","e82f1a1a":"code","d16afc49":"code","288d6542":"code","b7eb21f2":"code","c05edc7b":"code","91996a73":"code","3c8e49d8":"code","58fff9eb":"code","82de21e4":"code","268a69ca":"code","2e48e1d4":"markdown","327ca543":"markdown","479838e5":"markdown","515ffc18":"markdown","e650ca39":"markdown","a0a6ae0b":"markdown","f9db3968":"markdown"},"source":{"5a469404":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","e0a0e3f8":"\nimport numpy as np\nimport pandas as pd\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","8cd59715":"df = pd.read_csv('..\/input\/creating-customer-segments\/customers.csv')","1223ca81":"df.head()","b1cf0b73":"df.columns","d37b09ad":"TrainX.Region.unique()","07198fbc":"x = df[['Region', 'Fresh', 'Milk', 'Grocery', 'Frozen',\n       'Detergents_Paper', 'Delicatessen']]\ny = df['Channel']","b75b9d9a":"from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(\n                     x, y, test_size=0.33, random_state=42)","3061dca9":"print(x_train.shape)\nprint(y_train.shape)","31797193":"from sklearn.tree import DecisionTreeClassifier\nimport matplotlib.pyplot as plt \n%matplotlib inline","aef23e48":"## from sklearn.tree import DecisionTreeClassifier\nclf_tree = DecisionTreeClassifier(criterion=\"entropy\",\n                                    max_depth=6,\n                                  random_state=17)\n\nclf_tree.fit(x_train,y_train)","17dbec65":"print(\"The arrays : \" +str(x_train.columns))\nprint(\"The weight of attribtes : \" + str(list(np.round(clf_tree.feature_importances_,4))))","0976ae3b":"import graphviz as wiz\nfrom sklearn.tree import export_graphviz\nfrom graphviz import render\nfrom IPython.display import Image , display\n\ndef tree_graph_to_png(tree,feature_names,image_name):\n    \n    tree_str = export_graphviz(\n    tree,feature_names= feature_names , filled =True ,out_file = None\n    )\n    \n    graph = wiz.Source(tree_str)\n    graph.render(image_name +'.dot',view = True)\n    os.system('dot -Tpng '+image_name+'.dot -o '+image_name+'.png')\n    display(Image(filename= image_name+'.png'))\n    ","90b82aa7":"tree_graph_to_png(\n    tree=clf_tree,\n    feature_names=x_train.columns,\n    image_name = 'decision_tree_basic.png'\n    )","3ce9837a":"y_pred = clf_tree.predict(x_test)","b810b584":"y_test.shape","3c0bbcdf":"from sklearn.metrics import accuracy_score","a5344adb":"accuracy_score(y_pred, y_test)\n\n","9ff690b2":"from sklearn.metrics import confusion_matrix\nconfusion_matrix(y_pred, y_test)","1019b38b":"from sklearn.metrics import precision_score, recall_score\nprecision_score(y_pred, y_test)\n","8af3a3aa":"recall_score(y_pred, y_test)","a0eecaac":"# To compute the F1 score, simply call the f1_score() function:\nfrom sklearn.metrics import f1_score\nf1_score(y_pred, y_test)","48aa7ec4":"def outlier_percent(data):\n    Q1 = data.quantile(0.25)\n    Q3 = data.quantile(0.75)\n    IQR = Q3 - Q1\n    minimum = Q1 - (1.5 * IQR)\n    maximum = Q3 + (1.5 * IQR)\n    num_outliers =  np.sum((data < minimum) |(data > maximum))\n    num_total = data.count()\n    return (num_outliers\/num_total)*100","12924e59":"from sklearn.cluster import KMeans\nfrom sklearn.impute import KNNImputer\n\n# elbow method\nfrom yellowbrick.cluster import KElbowVisualizer , SilhouetteVisualizer","74b14c19":"df.columns","d15fdbd7":"TrainX = df\n","ecadf2f7":"non_categorical_data = TrainX.drop(['Channel','Region'], axis=1)\nfor column in non_categorical_data.columns:\n    data = non_categorical_data[column]\n    percent = str(round(outlier_percent(data), 2))\n    print(f'Outliers in \"{column}\": {percent}%')\n","86d8d08d":"for column in non_categorical_data.columns:\n    data = non_categorical_data[column]\n    \n    Q1 = data.quantile(0.25)\n    Q3 = data.quantile(0.75)\n    IQR = Q3 - Q1\n    minimum = Q1 - (1.5 * IQR)\n    maximum = Q3 + (1.5 * IQR)\n \n    outliers = ((data < minimum) |(data > maximum))\n    non_categorical_data[column].loc[outliers] = np.nan\n    \nnon_categorical_data.isna().sum()","d45916c8":"imputer = KNNImputer()\nimp_data = pd.DataFrame(imputer.fit_transform(non_categorical_data), columns=non_categorical_data.columns)\nimp_data.isna().sum()","68e17020":"imp_data","6fc11ab9":"from sklearn.preprocessing import StandardScaler\nms=StandardScaler()\ntrainX=ms.fit_transform(imp_data)","10469a93":"from sklearn.decomposition import PCA\npca = PCA(n_components=0.9,random_state = 42)\npca.fit(trainX)\npc_names = ['PC_'+str(i) for i in range(1,len(pca.components_)+1)]\ntrainX = pd.DataFrame(pca.transform(trainX),columns = pc_names)\n","98632213":"print(trainX.shape) # pca conversion\nprint(trainX.columns) # pca conversion columns\n","67e04043":"model = KMeans(random_state=42)\ndistortion_visualizer = KElbowVisualizer(model, k=(2,10))\n\ndistortion_visualizer.fit(trainX)       \ndistortion_visualizer.show()       ","bd26118f":"kmeans=KMeans(n_clusters=5,random_state=0)\nkmeans.fit(trainX)","e26d467b":"prediction=kmeans.fit_predict(trainX)","f9077c96":"trainX['LABEL'] = prediction # pca data\nimp_data['LABEL'] = prediction # continuous data\nTrainX['LABEL'] = prediction # all data categorical and continuous","7180b1e7":"imp_data.columns","2809c1bf":"imp_data.LABEL.value_counts().plot.pie(autopct='%1.0f%%', pctdistance=0.7, labeldistance=1.1)","f5cc7911":"# cluster metrics\nfrom sklearn.metrics import davies_bouldin_score\nfrom sklearn.metrics import silhouette_score","93322b8b":"print('Davis Bouldin Index : {0}'.format(davies_bouldin_score(trainX.loc[:,trainX.columns!='LABEL'],trainX['LABEL'])))","44fd3d7a":"print('Silhouette Score : {0}'.format(silhouette_score(trainX.loc[:,trainX.columns!='LABEL'],trainX['LABEL'])))","fff87c10":"visual  =  SilhouetteVisualizer(kmeans,colors='yellowbrick')\nvisual.fit(trainX.drop(['LABEL'],axis=1))\nvisual.show()\n","4aa062b7":"#without PCA decompostion\nprint('Silhouette Score : {0}'.format(silhouette_score(imp_data.loc[:,imp_data.columns!='LABEL'],imp_data['LABEL'])))\nprint('Davis Bouldin Index : {0}'.format(davies_bouldin_score(imp_data.loc[:,imp_data.columns!='LABEL'],imp_data['LABEL'])))","e82f1a1a":"kmeans_wpca=KMeans(n_clusters=6,random_state=101)\nvisual=SilhouetteVisualizer(kmeans_wpca,colors='yellowbrick')\nvisual.fit(imp_data.drop(['LABEL'],axis=1))\nvisual.show()\n","d16afc49":"def spider_plot(data, title):\n    means = data.groupby(\"LABEL\").mean().to_numpy()\n    names = data.columns[0:-1]\n    label_loc = np.linspace(start=0, stop=2 * np.pi - 0.15 , num=len(names))\n    categories = np.arange(0, len(means))\n    plt.figure(figsize=(10,10))\n    plt.subplot(polar=True)\n    for i in range(len(means)):\n        plt.plot(label_loc, means[i], label=f'class {categories[i]}')\n    plt.title(f'Feature comparison ({title})', size=20)\n    lines, labels = plt.thetagrids(np.degrees(label_loc), labels=names)\n    plt.legend()\n    plt.show()","288d6542":"spider_plot(trainX , 'Customer Segments')","b7eb21f2":"spider_plot(imp_data , 'Customer Segments \/ Real data - Cleaned ')","c05edc7b":"spider_plot(TrainX , 'Customer Segments \/ Real data With Outliers')","91996a73":"import seaborn as sns","3c8e49d8":"def colorful_scatter(data):   \n    LABEL_COLOR_MAP = {0 : 'y',\n                       1 : 'g',\n                       2 : 'm',\n                       3 : 'k',\n                       4 : 'c',\n                       5: 'b'\n                       \n                       }\n    plt.figure(figsize=(20,10))\n    sns.violinplot(data=imp_data, x=\"Channel\", y=\"Region\", hue=\"LABEL\", palette=LABEL_COLOR_MAP)","58fff9eb":"imp_data['Channel'] = TrainX['Channel']\nimp_data['Region'] = TrainX['Region']","82de21e4":"colorful_scatter(TrainX)","268a69ca":" plt.figure(figsize=(20,10))\nsns.kdeplot(data=imp_data, x=\"Channel\", y=\"Region\", hue=\"LABEL\", palette={0 : 'y', 1 : 'g', 2 : 'c', 3 : 'k', 4 : 'm', 5: 'r'}, alpha=.7, height=20)","2e48e1d4":"KMeans Clustering","327ca543":"We cannot ","479838e5":"**We see that the attributes region and Delicatessen are not really used attributes. by s**","515ffc18":"**<h1>Clustering Algorithms**","e650ca39":"Well , we just made some valuable discoveries.","a0a6ae0b":"**<h1> Decision Tree Classifier**\n<H3> Trying to figure the establishment type based on the intake of produce and region data \n <h4> (the region trend may have a role to play) , we will be knowing soon enough\n     \n     ","f9db3968":"# Evaluate the Model\nLet's see how good\/bad is our model.\n\nWe start by caculating two metrics:\n\n* The Davis-Bouldin Index is the average similarity between each cluster and the closest cluster. Scores range from 0 and up. 0 indicates better clustering.\n\n* The Silhouette Coefficient is a value between -1 and 1. The higher the score, the better. 1 indicates tight clusters, and 0 means overlapping clusters."}}