{"cell_type":{"a7f96108":"code","0b7da1f3":"code","bec0e2e3":"code","51ef8ef9":"code","fedc6670":"code","65be4936":"code","f7ec9aec":"code","460f3900":"code","b06c7a57":"code","c51b575a":"code","e66a88b8":"code","8bd8a0c1":"code","781a8ecb":"code","ab3bb35d":"code","91da2112":"code","0d3c69d9":"code","47e28e7c":"code","b639ca5d":"code","e4922081":"markdown","bc423a07":"markdown","753d39fb":"markdown","250ec8c0":"markdown","ef68e137":"markdown","c85349f1":"markdown","8de1b394":"markdown","183ac5a4":"markdown","dfc0a8a4":"markdown","89bb7a11":"markdown","0fbd41c9":"markdown"},"source":{"a7f96108":"%matplotlib inline\nfrom __future__ import print_function\n\ntry:\n    xrange\nexcept NameError:\n    xrange = range\nimport pandas as pd\nimport numpy as np\nimport matplotlib\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans, MiniBatchKMeans\nfrom sklearn.metrics import silhouette_samples, silhouette_score\nfrom sklearn.preprocessing import StandardScaler, MaxAbsScaler","0b7da1f3":"def generate_2dim_normal(mean,variance,covariance,sample_size):\n    cov = [[variance,covariance],[covariance,variance]]\n    return np.random.multivariate_normal(mean,cov,sample_size)\ncluster1 = generate_2dim_normal(mean = [0,8],variance=1,covariance=0,sample_size=500)\ncluster2 = generate_2dim_normal(mean = [-1,0],variance=1,covariance=0,sample_size=500)\ncluster3 = generate_2dim_normal(mean = [10,10],variance=1,covariance=0,sample_size=300)\ncluster4 = generate_2dim_normal(mean = [5,5.5],variance=0.8,covariance=-0.1,sample_size=200)\ndata = np.vstack((cluster1,cluster2,cluster3,cluster4))","bec0e2e3":"fig = plt.figure()\nax = fig.add_subplot(1,1,1)\nax.scatter(data[:,0],data[:,1])\nax.set_title(\"scatter plot\")\nax.set_xlabel(\"X1\")\nax.set_ylabel(\"X2\")","51ef8ef9":"km = KMeans(n_clusters=4,init=\"k-means++\",n_init=10,max_iter=300)\n##Init is a sheet argument,max_iter is the max times to update the sheet\nkm.fit(data)\ncluster_labels = km.predict(data)","fedc6670":"fig = plt.figure()\nax = fig.add_subplot(1,1,1)\ncolorlist =[\"tomato\",\"antiquewhite\",\"blueviolet\",\"cornflowerblue\",\"darkgreen\",\"seashell\",\"skyblue\",\"mediumseagreen\"]\n\n#The unique number of cluster_ids is \u300c0,1,2,3\u300d\ncluster_ids = list(set(cluster_labels))\n\nfor k in range(len(cluster_ids)):\n    cluster_id = cluster_ids[k]\n    label_ = \"cluster = %d\" % cluster_id\n    data_by_cluster = data[cluster_labels == cluster_id]\n    ax.scatter(data_by_cluster[:,0],data_by_cluster[:,1],c=colorlist[k],label=label_)\n\nax.set_title(\"Clustering\")\nax.set_xlabel(\"X1\")\nax.set_ylabel(\"Y1\")\nax.legend(loc=\"lower right\")","65be4936":"#elbow method\nmax_cluster = 10\nclusters = range(1,max_cluster)\nintra_sum_of_square_list = []\nfor k in clusters:\n    km = KMeans(n_clusters=k,init=\"k-means++\",n_init=10,max_iter=300)\n    km.fit(data)\n    intra_sum_of_square_list.append(km.inertia_)\n#You can get wcss value. through [inertia_]","f7ec9aec":"fig = plt.figure()\nax = fig.add_subplot(1,1,1)\nax.set_title(\"Elbow Method\")\nax.set_xlabel(\"Number of Cluster\")\nax.set_ylabel(\"Intra sum of distances(WCSS)\")\nplt.plot(clusters,intra_sum_of_square_list)","460f3900":"#Silhouette blot (silhouette score - 1 to 1 points, close to 1, close to the center of the cluster, \n#conversely close to the center of the next cluster)\nn_clusters=4\nkm = KMeans(n_clusters=4,init=\"k-means++\",n_init=10,max_iter=300)\nkm.fit(data)\ncluster_labels = km.predict(data)\n\n#Calculate the average of silhouette scores\nsilhouette_avg = silhouette_score(data,cluster_labels)\n\n#Calculate the silhouette score for each data\neach_silhouette_score = silhouette_samples(data,cluster_labels,metric=\"euclidean\")","b06c7a57":"#Visualization\nfig =plt.figure()\nax = fig.add_subplot(1,1,1)\ny_lower =10\nfor i in range(n_clusters):\n    ith_cluster_silhouette_values = each_silhouette_score[cluster_labels == i]\n    ith_cluster_silhouette_values.sort()\n    size_cluster_i = ith_cluster_silhouette_values.shape[0]\n    y_upper = y_lower + size_cluster_i\n    \n    color = colorlist[i]\n    ax.fill_betweenx(np.arange(y_lower,y_upper),0,ith_cluster_silhouette_values,facecolor=color,edgecolor=color,alpha=0.3)\n    \n    #label the silhouse plots with their cluster numbers at the middle\n    ax.text(-0.05,y_lower + 0.5 * size_cluster_i,str(i))\n    \n    #compute the new y_lower for next plot\n    y_lower = y_upper +10 \n    \nax.set_title(\"Silhuoette plot\")\nax.set_xlabel(\"silhouette score\")\nax.set_ylabel(\"Cluster label\")\n    \n#the vertical line for average silhouette score of all the values\nax.axvline(x=silhouette_avg,color=\"red\",linestyle=\"--\")\n    \nax.set_yticks([])\nax.set_xticks([-0.2,0,0.2,0.4,0.6,0.8,1])\n    ","c51b575a":"df = pd.read_csv(\"..\/input\/Wholesale_customers_data.csv\")\n                 ","e66a88b8":"df.head()","8bd8a0c1":"df.describe()","781a8ecb":"#due to the category[channel,region]has the category values,not the continuity values,so, we should devide them first\ncols =[\"Fresh\",\"Milk\",\"Grocery\",\"Frozen\",\"Detergents_Paper\",\"Delicassen\"]\ndataframe = df[cols]\ndataframe.head()\n","ab3bb35d":"#standardScaler\nscaler = MaxAbsScaler()\n#min=0,MAX=1\n\n#scaler = StandardScaler()\n#mean=0,arufa=1\n\ndataset = scaler.fit_transform(dataframe)","91da2112":"#Elbow method\nmax_cluster = 10\nclusters = range(1,max_cluster)\nintra_sum_of_square_list = []\nfor k in clusters:\n    km = KMeans(n_clusters=k,init=\"k-means++\",n_init=10,max_iter=300)\n    km.fit(dataset)\n    intra_sum_of_square_list.append(km.inertia_)\nfig = plt.figure()\nax = fig.add_subplot(1,1,1)\nax.set_title(\"Elbow Method\")\nax.set_xlabel(\"Number of Cluster\")\nax.set_ylabel(\"Intra sum of distances(WCSS)\")\nplt.plot(clusters,intra_sum_of_square_list)\n    ","0d3c69d9":"#Silhouette Plot\nn_clusters=6\nkm = KMeans(n_clusters=6,init=\"k-means++\",n_init=10,max_iter=300)\nkm.fit(dataset)\ncluster_labels = km.predict(dataset)\n\n#Calculate the average of silhouette scores\nsilhouette_avg = silhouette_score(dataset,cluster_labels)\n\n#Calculate the silhouette score for each data\neach_silhouette_score = silhouette_samples(dataset,cluster_labels,metric=\"euclidean\")\nfig =plt.figure()\nax = fig.add_subplot(1,1,1)\ny_lower =10\nfor i in range(n_clusters):\n    ith_cluster_silhouette_values = each_silhouette_score[cluster_labels == i]\n    ith_cluster_silhouette_values.sort()\n    size_cluster_i = ith_cluster_silhouette_values.shape[0]\n    y_upper = y_lower + size_cluster_i\n    \n    color = colorlist[i]\n    ax.fill_betweenx(np.arange(y_lower,y_upper),0,ith_cluster_silhouette_values,facecolor=color,edgecolor=color,alpha=0.3)\n    \n    #label the silhouse plots with their cluster numbers at the middle\n    ax.text(-0.05,y_lower + 0.5 * size_cluster_i,str(i))\n    \n    #compute the new y_lower for next plot\n    y_lower = y_upper +10 \n    \nax.set_title(\"Silhuoette plot\")\nax.set_xlabel(\"Silhouette score\")\nax.set_ylabel(\"Cluster label\")\n    \n#the vertical line for average silhouette score of all the values\nax.axvline(x=silhouette_avg,color=\"red\",linestyle=\"--\")\n    \nax.set_yticks([])\nax.set_xticks([-0.2,0,0.2,0.4,0.6,0.8,1])","47e28e7c":"#we can see there are a lot of points at the cluster of number 0.\n#but it is hard to say the clustering has been devided perfectly.\n#Let us see the average of  each cluster at last.","b639ca5d":"km_centers =pd.DataFrame(km.cluster_centers_,columns=cols)\nkm_centers.plot.bar(ylim=[0,2],fontsize=10)\nkm_centers","e4922081":"<font size=\"6\">Preface<\/font>\n#This is the first time to learn clustering methods and want to share some Experience with clustering methods.","bc423a07":"**Dammi\u3000data**","753d39fb":"**<font size=\"6\">Let Us do some Exercises<\/font>**","250ec8c0":"**from the picture above ,we cannot confirm  the elbow of number clearly.\nSo Let us choose the number of six**\n","ef68e137":"**there are four main clustering methods as below**\n\n1. agglomerative nesting(AGNES)Hierarchical type\n>   It becomes the last one group gathering the group in Euclidean distance. If you decide n number of the organization, the cluster is decided.\n\n2.K-means(non-Hierarchical type)\n> First, you must decide k layer, select template in the randam, find the center of gravity (average value) with other points, and set the centroid to templatethe last most stable point. (the weakness is changed when the first template changes.)\n\n3.Speckle\n\n4.Self organizing map\uff08SOM\uff09\n","c85349f1":"**From this learning, we can know that the Evaluation of the clustering depend on the object of the analisis,\nso it would be better to choose the number of clustering,which easily for Interpretation**","8de1b394":"**<font size=\"6\">CLUSTERING<\/font>**","183ac5a4":"**Visualization**","dfc0a8a4":"**Using the K-mean to cluster**","89bb7a11":"**From the graph, we can get the information below**\n\n1. there is no difference between each category  in cluster 0\n\n2.in cluster 5,which need a quantity of  fresh food \n\n3. whether cluster 2 or cluster 4 has  some  features obviously\n such as group 2 has the need of [Milk,Grocery,Detergents_paper] ","0fbd41c9":"**<font size=\"6\">Find the best cluster count<\/font>**\n\n1.elbow method  --wcss(within-cluster sum of squares)\n\n2.Silhouette Plot\n\n3.Hierarchical cluster"}}