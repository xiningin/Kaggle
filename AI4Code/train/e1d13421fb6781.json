{"cell_type":{"0c28cf54":"code","c9fe8637":"code","8ba73855":"code","1b02a23a":"code","24dd0d05":"code","14357e0d":"code","6c3640fd":"code","ddbc1fea":"code","5ce05229":"code","b7d2c3e1":"code","5250c086":"code","ba995552":"code","339ced9f":"markdown","64c1d552":"markdown","55fc1023":"markdown","4fd30130":"markdown"},"source":{"0c28cf54":"from pathlib import Path\nfrom typing import Any, List, Tuple\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pytorch_lightning as pl\nfrom pytorch_lightning.loggers import WandbLogger\nfrom sklearn.model_selection import train_test_split\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import io, transforms, models\nimport torchvision.transforms.functional as TF\n\n# Wandb login:\nfrom kaggle_secrets import UserSecretsClient\nimport wandb\nuser_secrets = UserSecretsClient()\nsecret_value = user_secrets.get_secret(\"wandb_api_key\")\nwandb.login(key=secret_value)\n\n%matplotlib inline","c9fe8637":"FILES = \"\/kaggle\/input\/celeba-dataset\/img_align_celeba\/img_align_celeba\/\"\n# TRAIN_FILES = \"\/kaggle\/input\/coco-2017-dataset\/coco2017\/train2017\/\"\n# VALID_FILES = \"\/kaggle\/input\/coco-2017-dataset\/coco2017\/val2017\/\"\nIMAGE_SIZE = 64\nBATCH_SIZE = 64\nEPOCHS = 10\nLR = 1e-3\nCHANNELS = 3\nVALID_IMAGES = 5","8ba73855":"class ImageData(Dataset):\n    def __init__(self, files: List[str]):\n        self.files = files\n        self.resize = transforms.Resize((IMAGE_SIZE, IMAGE_SIZE))\n        \n    def __len__(self):\n        return len(self.files)\n    \n    def __getitem__(self, i):\n        img = io.read_image(self.files[i])\n        img = self.resize(img)\n        \n        if img.shape[0] == 1:\n            img = torch.cat([img]*3)\n\n        return img \/ 255.0 - 0.5\n\nfiles = [str(file) for file in Path(FILES).glob(\"*.jpg\")]\ntrain_files, valid_files = train_test_split(files, test_size=0.1)\n# train_files = [str(file) for file in Path(TRAIN_FILES).glob(\"*.jpg\")]\n# valid_files = [str(file) for file in Path(VALID_FILES).glob(\"*.jpg\")]\ntrain_ds = ImageData(train_files)\nvalid_ds = ImageData(valid_files)\ntrain_dl = DataLoader(\n    train_ds, \n    BATCH_SIZE, \n    shuffle=True, \n    drop_last=True, \n    num_workers=4,\n    pin_memory=True,\n)\nvalid_dl = DataLoader(\n    valid_ds, \n    BATCH_SIZE*2, \n    shuffle=False, \n    drop_last=False, \n    num_workers=4,\n    pin_memory=True,\n)","1b02a23a":"x = next(iter(train_dl))\nlen(train_ds), len(valid_ds), x.shape, x.mean(), x.std()","24dd0d05":"class DownSample(nn.Module):\n    def __init__(self, in_channels, out_channels, scale_factor=2):\n        super().__init__()\n        self.conv2d_1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n        self.conv2d_2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, stride=2)\n        self.batch_norm = nn.BatchNorm2d(out_channels)\n        \n    def forward(self, x):\n        x = F.gelu(self.conv2d_1(x))\n        x = F.gelu(self.conv2d_2(x))\n        \n        return self.batch_norm(x)\n\nclass Encoder(nn.Module):\n    def __init__(self, channels):\n        super().__init__()\n        self.downsampler = nn.ModuleList(\n            [DownSample(c_in, c_out) for c_in, c_out in zip(channels[:-1], channels[1:])]\n        )\n        self.squeeze_wh_1 = nn.Conv2d(channels[-1], channels[-1], kernel_size=2)\n        self.squeeze_wh_2 = nn.Conv2d(channels[-1], channels[-1], kernel_size=2)\n                \n    def forward(self, x):\n        for downsample in self.downsampler:\n            x = downsample(x)\n        \n        mu = self.squeeze_wh_1(x)\n        log_var = self.squeeze_wh_2(x)\n        return mu, log_var","14357e0d":"class UpSample(nn.Module):\n    def __init__(self, in_channels, out_channels, scale_factor=2):\n        super().__init__()\n        self.up_sample = nn.Upsample(scale_factor=scale_factor)\n        self.conv2d_1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n        self.conv2d_2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n        self.batch_norm = nn.BatchNorm2d(out_channels)\n        \n    def forward(self, x):\n        out = self.up_sample(x)\n        out = F.gelu(self.conv2d_1(out))\n        out = F.gelu(self.conv2d_2(out))\n        \n        return self.batch_norm(out)\n    \nclass Decoder(nn.Module):\n    def __init__(self, channels):\n        super().__init__()\n        self.upsampler = nn.ModuleList(\n            [UpSample(c_in, c_out) for c_in, c_out in zip(channels[:-1], channels[1:])]\n        )\n        self.conv_1 = nn.Conv2d(channels[-1], CHANNELS, kernel_size=1)\n        self.conv_2 = nn.Conv2d(CHANNELS, CHANNELS, kernel_size=1)\n        \n    def forward(self, z):\n        for upsample in self.upsampler:\n            z = upsample(z)\n            \n        z = F.leaky_relu(self.conv_1(z))\n        return torch.sigmoid(self.conv_2(z)) - 0.5","6c3640fd":"def kl_divergence(mu: torch.FloatTensor, log_var: torch.FloatTensor) -> torch.FloatTensor:\n    kl_divergence_per_instance = -0.5 * (1 + log_var - torch.square(mu) - torch.exp(log_var))\n    return kl_divergence_per_instance.mean()","ddbc1fea":"def inv_transform(image: torch.FloatTensor):\n    return (image + 0.5)\n\ndef get_wandb_images(image: torch.FloatTensor, reconstruction: torch.FloatTensor):\n    return [\n        wandb.Image(TF.to_pil_image(inv_transform(image))),\n        wandb.Image(TF.to_pil_image(inv_transform(reconstruction))),\n    ]","5ce05229":"class LightningModel(pl.LightningModule):\n    def __init__(\n        self,\n        encoder: nn.Module,\n        decoder: nn.Module,\n        learning_rate: float,\n    ):\n        super().__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n        self.learning_rate = learning_rate\n\n    def common_step(\n        self,\n        x: torch.FloatTensor,\n    ) -> torch.FloatTensor:\n        mu, log_var = self.encoder(x)\n        z = mu + torch.exp(0.5 * log_var) * torch.randn(mu.shape).to(mu.device)\n        out = self.decoder(z)\n        \n        kl_loss = kl_divergence(mu, log_var)\n        reconstruction_loss = F.mse_loss(x, out)\n        loss = kl_loss + reconstruction_loss\n        \n        return loss, reconstruction_loss, out\n\n    def training_step(\n        self, x: torch.FloatTensor, *args: List[Any]\n    ) -> torch.Tensor:\n        loss, reconstruction_loss, reconstruction = self.common_step(x)\n        self.log(name=\"Training loss\", value=loss, on_step=True, on_epoch=True)\n        self.log(name=\"Training reconstruction loss\", value=reconstruction_loss, on_step=True, on_epoch=True)\n        return loss\n    \n    def validation_step(\n        self, x: torch.FloatTensor, *args: List[Any]\n    ) -> None:\n        loss, reconstruction_loss, reconstruction = self.common_step(x)\n        self.log(name=\"Validation loss\", value=loss, on_step=True, on_epoch=True)\n        self.log(name=\"Validation reconstruction loss\", value=reconstruction_loss, on_step=True, on_epoch=True)\n        return x.cpu(), reconstruction.cpu()\n        \n    def validation_epoch_end(self, validation_step_outputs):\n        images, preds = zip(*validation_step_outputs)\n        images = torch.cat(images, dim=0)\n        preds = torch.cat(preds, dim=0)\n        columns = [\"image\", \"reconstruction\"]\n        indices = np.random.choice(len(images), VALID_IMAGES, replace=False)\n        rows = [get_wandb_images(images[i], preds[i]) for i in indices]\n        table = wandb.Table(data=rows, columns=columns)\n        self.logger.experiment.log({f\"epoch {self.current_epoch + 1} results\": table})\n        \n    def on_after_backward(self):\n        if self.trainer.global_step % 50 == 0:  # don't make the tf file huge\n            with torch.no_grad():\n                for name, param in self.named_parameters():\n                    if \"weight\" in name and not \"norm\" in name and param.requires_grad:\n                        self.logger.experiment.log({f\"{name}\": wandb.Histogram(param.cpu())})\n                        self.logger.experiment.log({f\"{name}_grad\": wandb.Histogram(param.grad.cpu())})\n                        \n    def configure_optimizers(self) -> torch.optim.Optimizer:\n        return torch.optim.AdamW(self.parameters(), lr=self.learning_rate)","b7d2c3e1":"!mkdir \/kaggle\/working\/logs\nencoder = Encoder([3, 4, 8, 16, 32, 64])\ndecoder = Decoder([64, 32, 16, 8, 4, 4, 4])\nlightning_model = LightningModel(encoder, decoder, LR)\nlogger = WandbLogger(\"VAE 2\", \"\/kaggle\/working\/logs\/\", project=\"VAE\")\ntrainer = pl.Trainer(\n    max_epochs=EPOCHS,\n    gpus=torch.cuda.device_count(),\n    gradient_clip_val=1.0,\n    logger=logger,\n    precision=16,\n#     auto_lr_find=True,\n#     limit_train_batches=10,\n#     limit_val_batches=10,\n)\ntrainer.fit(lightning_model, train_dl, valid_dl)","5250c086":"mu, log_var = encoder(x)\nz = mu + torch.exp(0.5*log_var) * torch.randn(mu.shape)\nout = decoder(z)\n\nout.shape, out.mean(), out.std(), out.min(), out.max(), x.min(), x.max()","ba995552":"i = 16\nprint(out[i].std(), x[i].std())\nimg1 = TF.to_pil_image(x[i] + 0.5)\nimg2 = TF.to_pil_image(out[i] + 0.5)\nplt.figure(figsize=(12, 5))\nplt.subplot(121)\nplt.imshow(img1)\nplt.subplot(122)\nplt.imshow(img2)\nplt.show()","339ced9f":"## Model","64c1d552":"### ","55fc1023":"## KL Divergence\nSee this [stack exchange](https:\/\/stats.stackexchange.com\/questions\/318184\/kl-loss-with-a-unit-gaussian) question for definition of KL divergence:","4fd30130":"## Lightning Trainer"}}