{"cell_type":{"6e9b1410":"code","883b4047":"code","54c019d2":"code","1d2c4fcd":"code","def3e460":"code","3311cc22":"code","1069a7fe":"code","784ef448":"code","c3920252":"code","dc36af98":"code","30f853a1":"code","3901c9c7":"code","88331e49":"code","5ea28d0b":"code","5a3073e4":"code","fcd2665e":"code","0cb9a269":"code","c7436d6b":"code","df07ac0e":"code","4aa67447":"code","6473b51c":"code","9981500b":"code","63de8ac6":"code","0c81a9d6":"code","b0314a82":"code","f38bd522":"code","bd17658f":"code","85193e07":"code","de923a2d":"code","29b2b55e":"code","7bd12ce3":"code","1f4f0777":"code","d48b3f43":"code","66ced176":"code","c5165150":"code","37c8c67c":"code","ddafbbe6":"code","3f316f0a":"markdown","a78fa5b7":"markdown"},"source":{"6e9b1410":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","883b4047":"\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nimport glob\nimport scipy\nimport cv2\n\nimport keras","54c019d2":"\nimport random","1d2c4fcd":"\ntrain_data = pd.read_csv('..\/input\/train.csv')","def3e460":"\ntrain_data.shape","3311cc22":"\ntrain_data.head()","1069a7fe":"\ntrain_data.has_cactus.unique()","784ef448":"\ntrain_data.has_cactus.hist()","c3920252":"\ntrain_data.has_cactus.value_counts()","dc36af98":"\ntrain_data.has_cactus.plot()","30f853a1":"\ndef image_generator2(batch_size = 16, all_data=True, shuffle=True, train=True, indexes=None):\n    while True:\n        if indexes is None:\n            if train:\n                if all_data:\n                    indexes = np.arange(train_data.shape[0])\n                else:\n                    indexes = np.arange(train_data[:15000].shape[0])\n                if shuffle:\n                    np.random.shuffle(indexes)\n            else:\n                indexes = np.arange(train_data[15000:].shape[0])\n            \n        N = int(len(indexes) \/ batch_size)\n       \n\n        # Read in each input, perform preprocessing and get labels\n        for i in range(N):\n            current_indexes = indexes[i*batch_size: (i+1)*batch_size]\n            batch_input = []\n            batch_output = [] \n            for index in current_indexes:\n                img = mpimg.imread('..\/input\/train\/train\/' + train_data.id[index])\n                batch_input += [img]\n                batch_input += [img[::-1, :, :]]\n                batch_input += [img[:, ::-1, :]]\n                batch_input += [np.rot90(img)]\n                \n                temp_img = np.zeros_like(img)\n                temp_img[:28, :, :] = img[4:, :, :]\n                batch_input += [temp_img]\n                \n                temp_img = np.zeros_like(img)\n                temp_img[:, :28, :] = img[:, 4:, :]\n                batch_input += [temp_img]\n                \n                temp_img = np.zeros_like(img)\n                temp_img[4:, :, :] = img[:28, :, :]\n                batch_input += [temp_img]\n                \n                temp_img = np.zeros_like(img)\n                temp_img[:, 4:, :] = img[:, :28, :]\n                batch_input += [temp_img]\n                \n                batch_input += [cv2.resize(img[2:30, 2:30, :], (32, 32))]\n                \n                batch_input += [scipy.ndimage.interpolation.rotate(img, 10, reshape=False)]\n                \n                batch_input += [scipy.ndimage.interpolation.rotate(img, 5, reshape=False)]\n                \n                \n                for _ in range(11):\n                    batch_output += [train_data.has_cactus[index]]\n                \n            batch_input = np.array( batch_input )\n            batch_output = np.array( batch_output )\n        \n            yield( batch_input, batch_output.reshape(-1, 1) )","3901c9c7":"positive_examples = train_data[train_data.has_cactus==1]\nnegative_examples = train_data[train_data.has_cactus==0]","88331e49":"def augment_img(img):\n    batch_input = []\n    batch_input += [img]\n    batch_input += [img[::-1, :, :]]\n    batch_input += [img[:, ::-1, :]]\n    batch_input += [np.rot90(img)]\n                \n    temp_img = np.zeros_like(img)\n    temp_img[:28, :, :] = img[4:, :, :]\n    batch_input += [temp_img]\n                \n    temp_img = np.zeros_like(img)\n    temp_img[:, :28, :] = img[:, 4:, :]\n    batch_input += [temp_img]\n                \n    temp_img = np.zeros_like(img)\n    temp_img[4:, :, :] = img[:28, :, :]\n    batch_input += [temp_img]\n                \n    temp_img = np.zeros_like(img)\n    temp_img[:, 4:, :] = img[:, :28, :]\n    batch_input += [temp_img]\n                \n    batch_input += [cv2.resize(img[2:30, 2:30, :], (32, 32))]\n                \n    batch_input += [scipy.ndimage.interpolation.rotate(img, 10, reshape=False)]\n                \n    batch_input += [scipy.ndimage.interpolation.rotate(img, 5, reshape=False)]\n    \n    return batch_input","5ea28d0b":"\ndef image_generator(batch_size = 8, all_data=True, shuffle=True, train=True, indexes=None):\n    while True:\n        if indexes is None:\n            if train:\n                indexes = positive_examples.index.tolist()\n                neg_indexes = negative_examples.index.tolist()\n                if shuffle:\n                    np.random.shuffle(indexes)\n                    np.random.shuffle(neg_indexes)\n            \n        N = int(len(indexes) \/ (batch_size\/2))\n        neg_N = int(len(neg_indexes) \/ (batch_size\/2))\n       \n        j = 0\n\n        # Read in each input, perform preprocessing and get labels\n        for i in range(N):\n            current_indexes = indexes[i*(batch_size\/\/2): (i+1)*(batch_size\/\/2)]\n            current_neg_indexes = neg_indexes[j*(batch_size\/\/2): (j+1)*(batch_size\/\/2)]\n            j = (j + 1) % neg_N\n            batch_input = []\n            batch_output = [] \n            for ind in range(len(current_indexes)):\n                index = current_indexes[ind]\n                neg_index = current_neg_indexes[ind]\n                \n                img = mpimg.imread('..\/input\/train\/train\/' + train_data.id[index])\n                batch_input.extend(augment_img(img))\n                for _ in range(11):\n                    batch_output += [train_data.has_cactus[index]]\n                \n                neg_img = mpimg.imread('..\/input\/train\/train\/' + train_data.id[neg_index])\n                batch_input.extend(augment_img(neg_img))\n                for _ in range(11):\n                    batch_output += [train_data.has_cactus[neg_index]]\n                \n#                 factor = 0.05\n#                 new_img = factor*neg_img + (1-factor)*img\n#                 batch_input.append(new_img)\n#                 batch_output += [factor*train_data.has_cactus[neg_index]+(1-factor)*train_data.has_cactus[index]]\n                \n#                 factor = 0.95\n#                 new_img = factor*neg_img + (1-factor)*img\n#                 batch_input.append(new_img)\n#                 batch_output += [factor*train_data.has_cactus[neg_index]+(1-factor)*train_data.has_cactus[index]]\n            \n                \n                \n            batch_input = np.array( batch_input )\n            batch_output = np.array( batch_output )\n        \n            yield( batch_input, batch_output.reshape(-1, 1) )","5a3073e4":"model = keras.models.Sequential()\nmodel.add(keras.layers.Conv2D(64, (5, 5), input_shape=(32, 32, 3)))\nmodel.add(keras.layers.BatchNormalization())\nmodel.add(keras.layers.LeakyReLU(alpha=0.3))\n\nmodel.add(keras.layers.Conv2D(64, (5, 5)))\nmodel.add(keras.layers.BatchNormalization())\nmodel.add(keras.layers.LeakyReLU(alpha=0.3))\n\nmodel.add(keras.layers.Conv2D(128, (5, 5)))\nmodel.add(keras.layers.BatchNormalization())\nmodel.add(keras.layers.LeakyReLU(alpha=0.3))\n\nmodel.add(keras.layers.Conv2D(128, (5, 5)))\nmodel.add(keras.layers.BatchNormalization())\nmodel.add(keras.layers.LeakyReLU(alpha=0.3))\n\nmodel.add(keras.layers.Conv2D(256, (3, 3)))\nmodel.add(keras.layers.BatchNormalization())\nmodel.add(keras.layers.LeakyReLU(alpha=0.3))\n\nmodel.add(keras.layers.Conv2D(256, (3, 3)))\nmodel.add(keras.layers.BatchNormalization())\nmodel.add(keras.layers.LeakyReLU(alpha=0.3))\n\nmodel.add(keras.layers.Conv2D(512, (3, 3)))\nmodel.add(keras.layers.BatchNormalization())\nmodel.add(keras.layers.LeakyReLU(alpha=0.3))\n\nmodel.add(keras.layers.Flatten())\n\n\nmodel.add(keras.layers.Dense(100))\nmodel.add(keras.layers.BatchNormalization())\nmodel.add(keras.layers.LeakyReLU(alpha=0.3))\n\nmodel.add(keras.layers.Dense(1, activation='sigmoid'))","fcd2665e":"\nmodel.summary()","0cb9a269":"\nopt = keras.optimizers.SGD(lr=0.0001, momentum=0.9, nesterov=True)\nmodel.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy'])","c7436d6b":"def step_decay_schedule(initial_lr=1e-3, decay_factor=0.75, step_size=10):\n    '''\n    Wrapper function to create a LearningRateScheduler with step decay schedule.\n    '''\n    def schedule(epoch):\n        return initial_lr * (decay_factor ** np.floor(epoch\/step_size))\n    \n    return keras.callbacks.LearningRateScheduler(schedule)\n\nlr_sched = step_decay_schedule(initial_lr=1e-3, decay_factor=0.75, step_size=2)\nearly_stop = keras.callbacks.EarlyStopping(monitor='loss', patience=3)\n\nmodel.fit_generator(image_generator(), steps_per_epoch= train_data.shape[0] \/ 8, epochs=10, callbacks=[lr_sched, early_stop])","df07ac0e":"# def step_decay_schedule(initial_lr=1e-3, decay_factor=0.75, step_size=10):\n#     '''\n#     Wrapper function to create a LearningRateScheduler with step decay schedule.\n#     '''\n#     def schedule(epoch):\n#         return initial_lr * (decay_factor ** np.floor(epoch\/step_size))\n    \n#     return keras.callbacks.LearningRateScheduler(schedule)\n\n# lr_sched = step_decay_schedule(initial_lr=1e-3, decay_factor=0.75, step_size=2)\n\n# model.fit_generator(image_generator(), steps_per_epoch= train_data.shape[0] \/ 8, epochs=20, callbacks=[lr_sched])","4aa67447":"model.evaluate_generator(image_generator2(), steps=train_data.shape[0]\/\/16)","6473b51c":"# model.evaluate_generator(image_generator(), steps=train_data.shape[0]\/\/8)","9981500b":"\n# keras.backend.eval(model.optimizer.lr.assign(0.00001))","63de8ac6":"\n# model.fit_generator(image_generator(), steps_per_epoch= train_data.shape[0] \/ 16, epochs=15)","0c81a9d6":"\nindexes = np.arange(train_data.shape[0])\nN = int(len(indexes) \/ 64)   \nbatch_size = 64\n\nwrong_ind = []\nfor i in range(N):\n            current_indexes = indexes[i*64: (i+1)*64]\n            batch_input = []\n            batch_output = [] \n            for index in current_indexes:\n                img = mpimg.imread('..\/input\/train\/train\/' + train_data.id[index])\n                batch_input += [img]\n                batch_output.append(train_data.has_cactus[index])\n            \n            batch_input = np.array( batch_input )\n#             batch_output = np.array( batch_output )\n\n            model_pred = model.predict_classes(batch_input)\n            for j in range(len(batch_output)):\n                if model_pred[j] != batch_output[j]:\n                    wrong_ind.append(i*batch_size+j)","b0314a82":"\nlen(wrong_ind)","f38bd522":"\nindexes = np.arange(train_data.shape[0])\nN = int(len(indexes) \/ 64)   \nbatch_size = 64\n\nwrong_ind = []\nfor i in range(N):\n            current_indexes = indexes[i*64: (i+1)*64]\n            batch_input = []\n            batch_output = [] \n            for index in current_indexes:\n                img = mpimg.imread('..\/input\/train\/train\/' + train_data.id[index])\n                batch_input += [img[::-1, :, :]]\n                batch_output.append(train_data.has_cactus[index])\n            \n            batch_input = np.array( batch_input )\n\n            model_pred = model.predict_classes(batch_input)\n            for j in range(len(batch_output)):\n                if model_pred[j] != batch_output[j]:\n                    wrong_ind.append(i*batch_size+j)","bd17658f":"\nlen(wrong_ind)","85193e07":"\nindexes = np.arange(train_data.shape[0])\nN = int(len(indexes) \/ 64)   \nbatch_size = 64\n\nwrong_ind = []\nfor i in range(N):\n            current_indexes = indexes[i*64: (i+1)*64]\n            batch_input = []\n            batch_output = [] \n            for index in current_indexes:\n                img = mpimg.imread('..\/input\/train\/train\/' + train_data.id[index])\n                batch_input += [img[:, ::-1, :]]\n                batch_output.append(train_data.has_cactus[index])\n            \n            batch_input = np.array( batch_input )\n\n            model_pred = model.predict_classes(batch_input)\n            for j in range(len(batch_output)):\n                if model_pred[j] != batch_output[j]:\n                    wrong_ind.append(i*batch_size+j)","de923a2d":"\nlen(wrong_ind)","29b2b55e":"\n!ls ..\/input\/test\/test\/* | wc -l","7bd12ce3":"\ntest_files = os.listdir('..\/input\/test\/test\/')","1f4f0777":"\nlen(test_files)","d48b3f43":"\nbatch = 40\nall_out = []\nfor i in range(int(4000\/batch)):\n    images = []\n    for j in range(batch):\n        img = mpimg.imread('..\/input\/test\/test\/'+test_files[i*batch + j])\n        images += [img]\n    out = model.predict(np.array(images))\n    all_out += [out]","66ced176":"\nall_out = np.array(all_out).reshape((-1, 1))\n","c5165150":"\nall_out.shape","37c8c67c":"\nsub_file = pd.DataFrame(data = {'id': test_files, 'has_cactus': all_out.reshape(-1).tolist()})","ddafbbe6":"\nsub_file.to_csv('sample_submission.csv', index=False)","3f316f0a":"**Model**","a78fa5b7":"**Exploration**\n"}}