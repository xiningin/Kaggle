{"cell_type":{"a8e829ba":"code","4d60a366":"code","98128713":"code","4e9e5b7d":"code","4bfffb86":"code","b526d0f0":"code","553c1bde":"code","4e368dee":"code","c5f70b98":"code","2b1c0eeb":"code","d9c694ad":"code","40cdf0b0":"code","12b28550":"code","9699b990":"code","65f3d23b":"code","b997990f":"code","98d384de":"code","bf984543":"code","2817fd11":"code","0ecf8add":"code","d7ae3199":"code","bc60b1ec":"code","5eb27741":"code","c6359aed":"code","27f963fe":"code","a83086f9":"code","e4ece971":"code","27f85cb8":"code","cca43a22":"markdown","a949ad0d":"markdown","bb24c7ed":"markdown","c8c43df0":"markdown","b63a98e7":"markdown","4cd1b036":"markdown","4ae44f7e":"markdown","18e9a605":"markdown"},"source":{"a8e829ba":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","4d60a366":"df = pd.read_csv(\"\/kaggle\/input\/water-potability\/water_potability.csv\")\nprint(df.shape)\ndf.head()\n","98128713":"df.info()","4e9e5b7d":"df.describe()","4bfffb86":"df.isna().sum()","b526d0f0":"df['ph'].plot(kind = 'hist')","553c1bde":"df[df['ph'].isna()]['Potability'].value_counts()","4e368dee":"#df['ph'].fillna(df['ph'].median() , inplace = True) \n#df['Sulfate'].fillna(df['Sulfate'].median() , inplace = True) \n#df['Trihalomethanes'].fillna(df['Trihalomethanes'].median() , inplace = True) ","c5f70b98":"df.dropna(inplace = True)","2b1c0eeb":"df.isna().sum()","d9c694ad":"df.shape","40cdf0b0":"df['ph'].plot(kind = 'hist')","12b28550":"## Potability :\nprint(df['Potability'].value_counts()\/len(df))\ndf['Potability'].value_counts().plot(kind = 'bar')","9699b990":"df.corr()","65f3d23b":"X = df.drop(['Potability'] ,axis =1 )\nY = df['Potability']\n\nfrom sklearn.model_selection import train_test_split\nx_train , x_test, y_train, y_test = train_test_split(X,Y, test_size = 0.25, random_state = 49) ","b997990f":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nx_train_std = sc.fit_transform(x_train)\nx_test_std = sc.transform(x_test)","98d384de":"from sklearn.metrics import confusion_matrix , classification_report\ndef modelResult(model , x_train ,y_train ,  x_test ,y_test ):\n    model.fit(x_train, y_train)\n    print(\"Training Accuracy \" , model.score(x_train, y_train))\n    print(\"Test Accuracy\" , model.score(x_test, y_test))\n    predict = model.predict(x_test)\n    print(confusion_matrix(predict, y_test))\n    print(classification_report(predict, y_test))","bf984543":"from sklearn.linear_model import LogisticRegression\nlr = LogisticRegression()\nmodelResult(lr , x_train ,y_train ,  x_test ,y_test )\nmodelResult(lr , x_train_std ,y_train ,  x_test_std ,y_test )","2817fd11":"from sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors=5)\nmodelResult(knn , x_train_std ,y_train ,  x_test_std ,y_test)","0ecf8add":"from sklearn.naive_bayes import GaussianNB\nnb =GaussianNB()\nmodelResult(nb , x_train ,y_train ,  x_test ,y_test)","d7ae3199":"from sklearn.svm import SVC\nsvm = SVC(C = 5 , gamma = 'auto')\nmodelResult(svm , x_train_std ,y_train ,  x_test_std ,y_test)","bc60b1ec":"from sklearn.tree import DecisionTreeClassifier\ndt = DecisionTreeClassifier(max_depth = 15 ,min_samples_split = 2 )\nmodelResult(dt , x_train ,y_train ,  x_test ,y_test)","5eb27741":"dt.feature_importances_","c6359aed":"feature_importance_df = pd.DataFrame()\n\nfeature_importance_df['Feature'] = X.columns\nfeature_importance_df['Feature_importance'] = dt.feature_importances_\nfeature_importance_df.sort_values(by = 'Feature_importance' , ascending= False)","27f963fe":"from sklearn.ensemble import RandomForestClassifier\nrfc = RandomForestClassifier(max_depth= 12 , max_features= 3)\nmodelResult(rfc , x_train ,y_train ,  x_test ,y_test)\n","a83086f9":"from sklearn.ensemble import BaggingClassifier\nbgc = BaggingClassifier(DecisionTreeClassifier(max_depth = 15 ,min_samples_split = 2 ) , )\nmodelResult(bgc , x_train ,y_train ,  x_test ,y_test)\n","e4ece971":"from sklearn.ensemble import AdaBoostClassifier , GradientBoostingClassifier \nadc = AdaBoostClassifier()\ngbc = GradientBoostingClassifier(n_estimators = 1000 )\n\nmodelResult(adc , x_train ,y_train ,  x_test ,y_test)","27f85cb8":"modelResult(gbc , x_train ,y_train ,  x_test ,y_test)","cca43a22":"## KNN","a949ad0d":"## EDA","bb24c7ed":"## Logisitic Regression ","c8c43df0":"## Ensemble : Bagging ","b63a98e7":"## Decision Tree","4cd1b036":"## Ensemble Boosting","4ae44f7e":"## Naive Bayes","18e9a605":"## SVM"}}