{"cell_type":{"4ff17d46":"code","c723a85a":"code","60ee6c85":"code","7c70103e":"code","07096a87":"code","30c4e21b":"code","98102119":"code","313ae5ce":"code","edd45aeb":"code","a8c6389f":"code","fc890031":"code","1d78a3d4":"code","b1fdfc2a":"code","363d5f5e":"code","65a6df44":"code","d79e16a4":"code","33712391":"code","9e7446ad":"code","8c5b560b":"code","8b3a3c92":"code","7d180a42":"code","14c315c4":"code","7288adc4":"code","1f0c0e51":"code","2d1368f8":"code","5e7da7ab":"markdown","d709473f":"markdown","d88642e3":"markdown","92d72975":"markdown","895d744c":"markdown","ad7d0870":"markdown","f6f06762":"markdown","5d6cd376":"markdown","cb64285a":"markdown","3f4ae063":"markdown","99d3c94a":"markdown","a8f6b0fb":"markdown"},"source":{"4ff17d46":"import os\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nimport pandas as pd\nimport pickle\nimport gc\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils import shuffle\n\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\n\nimport zipfile ","c723a85a":"gc.collect()","60ee6c85":"train = pd.read_csv(\"..\/input\/histopathologic-cancer-detection\/train_labels.csv\", dtype=str)\nprint(train.shape)","7c70103e":"train.head(10)","07096a87":"y_train = train.label\n\n(train.label.value_counts() \/ len(train)).to_frame().T","30c4e21b":"# Sample 16 images from the training set and display these along with their labels.\n\nplt.figure(figsize=(10,10)) # specifying the overall grid size\n\nfor i in range(16):\n    plt.subplot(4,4,i+1)    # the number of images in the grid is 6*6 (16)\n    img = mpimg.imread(f'..\/input\/histopathologic-cancer-detection\/train\/{train[\"id\"][i]}.tif')\n    plt.imshow(img)\n    plt.text(0, -5, f'Label {train[\"label\"][i]}')\n    plt.axis('off')\n    \nplt.tight_layout()\nplt.show()","98102119":"train_neg = train[train['label']=='0'].sample(10000,random_state=1)\ntrain_pos = train[train['label']=='1'].sample(10000,random_state=1)\n\ntrain_data = pd.concat([train_neg, train_pos], axis=0).reset_index(drop=True)\n\ntrain = shuffle(train_data)","313ae5ce":"train['label'].value_counts()","edd45aeb":"# function to apply the .tif extension\ndef append_ext(fn):\n    return fn+\".tif\"\n\n\ntrain['id'] = train['id'].apply(append_ext)\ntrain.head()","a8c6389f":"# Split the dataframe train into two DataFrames named train_df and valid_df. \n# Use 20% of the data for the validation set. \n# Use stratified sampling so that the label proportions are preserved.\n# Set a random seed for the split. \n\ntrain_df, valid_df = train_test_split(train, test_size=0.2, random_state=1, stratify=train.label)\n\nprint(train_df.shape)\nprint(valid_df.shape)","fc890031":"# Create image data generators for both the training set and the validation set. \n# Use the data generators to scale the pixel values by a factor of 1\/255. \ntrain_datagen = ImageDataGenerator(rescale=1\/255)\nvalid_datagen = ImageDataGenerator(rescale=1\/255)","1d78a3d4":"# Complete the code for the data loaders below. \n\nBATCH_SIZE = 64\n\ntrain_loader = train_datagen.flow_from_dataframe(\n    dataframe = train_df,\n    directory = '..\/input\/histopathologic-cancer-detection\/train\/',\n    x_col = 'id',\n    y_col = 'label',\n    batch_size = BATCH_SIZE,\n    seed = 1,\n    shuffle = True,\n    class_mode = 'categorical',\n    target_size = (32,32)\n)\n\nvalid_loader = train_datagen.flow_from_dataframe(\n    dataframe = valid_df,\n    directory = '..\/input\/histopathologic-cancer-detection\/train\/',\n    x_col = 'id',\n    y_col = 'label',\n    batch_size = BATCH_SIZE,\n    seed = 1,\n    shuffle = True,\n    class_mode = 'categorical',\n    target_size = (32,32)\n)","b1fdfc2a":"# Run this cell to determine the number of training and validation batches. \n\nTR_STEPS = len(train_loader)\nVA_STEPS = len(valid_loader)\n\nprint(TR_STEPS)\nprint(VA_STEPS)","363d5f5e":"# Use this cell to construct a convolutional neural network model. \n# Your model should make use of each of the following layer types:\n#    Conv2D, MaxPooling2D, Dropout, BatchNormalization, Flatten, Dense\n# You can start by mimicking the architecture used in the \n# Aerial Cactus competetition, but you should explore different architectures\n# by adding more layers and\/or adding more nodes in individual layers\n\nnp.random.seed(1)\ntf.random.set_seed(1)\n\ncnn1 = Sequential([\n    Conv2D(32, (3,3), activation = 'relu', padding = 'same', input_shape=(32,32,3)),\n    BatchNormalization(),\n    Conv2D(32, (3,3), activation = 'relu', padding = 'same'),\n    MaxPooling2D(2,2),\n    Dropout(0.2),\n    BatchNormalization(),\n\n    Conv2D(64, (3,3), activation = 'relu', padding = 'same'),\n    BatchNormalization(),\n    Conv2D(64, (3,3), activation = 'relu', padding = 'same'),\n    MaxPooling2D(2,2),\n    Dropout(0.4),\n    BatchNormalization(),\n    \n    Conv2D(128, (3,3), activation = 'relu', padding = 'same'),\n    BatchNormalization(),\n    Conv2D(128, (3,3), activation = 'relu', padding = 'same'),\n    MaxPooling2D(2,2),\n    Dropout(0.5),\n    BatchNormalization(),\n\n    Flatten(),\n    \n    Dense(128, activation='relu'),\n    Dropout(0.5),\n    Dense(16, activation='relu'),\n    Dropout(0.2),\n    BatchNormalization(),\n    # we have 2 here because we have 2 classes\n    Dense(2, activation='softmax')\n])\n\ncnn1.summary()","65a6df44":"opt = tf.keras.optimizers.Adam(0.001)\ncnn1.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy', tf.keras.metrics.AUC()])","d79e16a4":"%%time \n\nh1 = cnn1.fit(\n    x = train_loader, \n    steps_per_epoch = TR_STEPS, \n    epochs = 20,\n    validation_data = valid_loader, \n    validation_steps = VA_STEPS, \n    verbose = 1\n)","33712391":"history = h1.history\nprint(history.keys())","9e7446ad":"# Graph the result\n\nepoch_range = range(1, len(history['loss'])+1)\n\nplt.figure(figsize=[14,4])\n\nplt.subplot(1,3,1)\nplt.plot(epoch_range, history['loss'], label='Training')\nplt.plot(epoch_range, history['val_loss'], label='Validation')\nplt.xlabel('Epoch'); plt.ylabel('Loss'); plt.title('Loss')\nplt.legend()\n\nplt.subplot(1,3,2)\nplt.plot(epoch_range, history['accuracy'], label='Training')\nplt.plot(epoch_range, history['val_accuracy'], label='Validation')\nplt.xlabel('Epoch'); plt.ylabel('Accuracy'); plt.title('Accuracy')\nplt.legend()\n\nplt.subplot(1,3,3)\nplt.plot(epoch_range, history['auc'], label='Training')\nplt.plot(epoch_range, history['val_auc'], label='Validation')\nplt.xlabel('Epoch'); plt.ylabel('AUC'); plt.title('AUC')\nplt.legend()\n\nplt.tight_layout()\nplt.show()","8c5b560b":"tf.keras.backend.set_value(cnn1.optimizer.learning_rate, 0.0001)","8b3a3c92":"%%time \n\nh2 = cnn1.fit(\n    x = train_loader, \n    steps_per_epoch = TR_STEPS, \n    epochs = 20,\n    validation_data = valid_loader, \n    validation_steps = VA_STEPS, \n    verbose = 1\n)","7d180a42":"# Graph the result\n\nfor k in history.keys():\n    history[k] += h2.history[k]\n    \n\nepoch_range = range(1, len(history['loss'])+1)\n\nplt.figure(figsize=[14,4])\n\nplt.subplot(1,3,1)\nplt.plot(epoch_range, history['loss'], label='Training')\nplt.plot(epoch_range, history['val_loss'], label='Validation')\nplt.xlabel('Epoch'); plt.ylabel('Loss'); plt.title('Loss')\nplt.legend()\n\nplt.subplot(1,3,2)\nplt.plot(epoch_range, history['accuracy'], label='Training')\nplt.plot(epoch_range, history['val_accuracy'], label='Validation')\nplt.xlabel('Epoch'); plt.ylabel('Accuracy'); plt.title('Accuracy')\nplt.legend()\n\nplt.subplot(1,3,3)\nplt.plot(epoch_range, history['auc'], label='Training')\nplt.plot(epoch_range, history['val_auc'], label='Validation')\nplt.xlabel('Epoch'); plt.ylabel('AUC'); plt.title('AUC')\nplt.legend()\n\nplt.tight_layout()\nplt.show()","14c315c4":"tf.keras.backend.set_value(cnn1.optimizer.learning_rate, 0.0001)","7288adc4":"%%time \n\nh3 = cnn1.fit(\n    x = train_loader, \n    steps_per_epoch = TR_STEPS, \n    epochs = 20,\n    validation_data = valid_loader, \n    validation_steps = VA_STEPS, \n    verbose = 1\n)","1f0c0e51":"# Graph the result\nfor k in history.keys():\n    history[k] += h2.history[k]\n\nepoch_range = range(1, len(history['loss'])+1)\n\nplt.figure(figsize=[14,4])\n\nplt.subplot(1,3,1)\nplt.plot(epoch_range, history['loss'], label='Training')\nplt.plot(epoch_range, history['val_loss'], label='Validation')\nplt.xlabel('Epoch'); plt.ylabel('Loss'); plt.title('Loss')\nplt.legend()\n\nplt.subplot(1,3,2)\nplt.plot(epoch_range, history['accuracy'], label='Training')\nplt.plot(epoch_range, history['val_accuracy'], label='Validation')\nplt.xlabel('Epoch'); plt.ylabel('Accuracy'); plt.title('Accuracy')\nplt.legend()\n\nplt.subplot(1,3,3)\nplt.plot(epoch_range, history['auc'], label='Training')\nplt.plot(epoch_range, history['val_auc'], label='Validation')\nplt.xlabel('Epoch'); plt.ylabel('AUC'); plt.title('AUC')\nplt.legend()\n\nplt.tight_layout()\nplt.show()","2d1368f8":"cnn1.save('cancer_model15.h5')\npickle.dump(history, open(f'cancer_history15.pkl', 'wb'))","5e7da7ab":"## Building the CNN","d709473f":"## Saving the Model","d88642e3":"## Seeing the Distribution of Labels","92d72975":"## Creating Datagenerators","895d744c":"## Another Training Run to Smooth out Validation Graph","ad7d0870":"## Sampling a Few Images","f6f06762":"## One Last Training Run","5d6cd376":"## Splitting the Data","cb64285a":"## Fitting the CNN","3f4ae063":"## Graphing the Results","99d3c94a":"## Taking Even Amount of Neg and Pos Labels","a8f6b0fb":"## Importing Training Labels"}}