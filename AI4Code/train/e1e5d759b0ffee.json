{"cell_type":{"7dacaadf":"code","52fa6889":"code","3e880471":"code","c51298b9":"code","2d9dc336":"code","a2c0c07c":"code","5e47b03f":"code","0e877ae2":"code","fd07dfa0":"code","468ba8ad":"code","70ab280a":"code","5eb77ade":"code","f76291d8":"code","68ce6a91":"code","757a37fe":"code","90bc832a":"code","5f7a58b7":"code","f5d881d9":"code","1118cd5e":"code","950ec2e7":"code","282b37b2":"code","2e25f217":"code","03136cb7":"code","f708a1d4":"code","f69eee7f":"code","c5cde705":"code","e9ada023":"code","8464950e":"code","9e16df90":"code","b462a6ae":"code","7574cd99":"code","0779a3c8":"code","58dc2842":"code","897a9d44":"code","df7ea8cb":"code","4d2a1a08":"code","f6227093":"code","ff7fd013":"code","2e9b2d56":"code","cef6cb2d":"code","9a366d2c":"code","17d3fa0a":"code","53b78836":"code","72406c93":"code","4fd4764e":"code","037e316d":"code","bc3b6e2a":"code","88fa6a69":"code","ec8dbf18":"code","b274ab45":"code","9d93ab18":"code","ebcb92c8":"code","fe712944":"code","f56bfb57":"code","f44bfa53":"code","1b0f1416":"code","acd33f64":"code","3af88708":"code","73bcc6f6":"code","83409a81":"code","3078a512":"code","bc15bcd7":"code","774b725b":"code","346d628d":"code","e8c429ef":"code","47dab4e1":"code","b159d345":"code","1e98c6bd":"code","5b715871":"code","9ca9c147":"code","78dd6003":"code","98a7360a":"code","05e87153":"code","13f2dc79":"code","dc399a0f":"code","219c835b":"code","1b40af14":"code","114d7af0":"code","513ad503":"code","a9087584":"code","1d3b953b":"code","d8c69ff6":"code","33550987":"code","6f9d164b":"code","5884f13e":"code","ac878adc":"code","f034e1bb":"code","03b2edab":"code","7f3e011d":"code","ce56b238":"code","8726c7be":"code","e3be56aa":"code","8b5a1e9e":"markdown","395ce9c8":"markdown","8d35c5fb":"markdown","aa66bf27":"markdown","a6be2e63":"markdown","5ed71413":"markdown","e49ebe08":"markdown","9d548267":"markdown","ab1830f0":"markdown","9ed111d9":"markdown","d6e3131a":"markdown","324e800c":"markdown","6aabb8e7":"markdown","e161f2fb":"markdown","f571cad1":"markdown","2d19d3f3":"markdown","bd469dfb":"markdown","00c0c7a2":"markdown","34a3788c":"markdown","1fc83b8e":"markdown","e7069455":"markdown","af10df20":"markdown","4a943b59":"markdown","6b238f8c":"markdown","f11a2684":"markdown","1b8b242f":"markdown","87bd4c72":"markdown","b151844e":"markdown","2a4211ba":"markdown","bfc2c680":"markdown","e3649acb":"markdown","7e774dee":"markdown","b64454a4":"markdown","115a4474":"markdown","cc621276":"markdown","7ba971c5":"markdown","f52ab411":"markdown","e524b3d3":"markdown","8a791f3e":"markdown","ddc5fb22":"markdown","8d4420a7":"markdown"},"source":{"7dacaadf":"# Driver code to upload data files to Google Colabs\n\n\n# This section is for uploading files to Google Colabs from local drive\n\n#from google.colab import files\n#uploaded = files.upload()\n\n\n#This section is to upload files to Google Colabs from Google drive\n\n#!pip install -U -q PyDrive\n#from pydrive.auth import GoogleAuth\n#from pydrive.drive import GoogleDrive\n#from google.colab import auth\n#from oauth2client.client import GoogleCredentials\n# Authenticate and create the PyDrive client.\n#auth.authenticate_user()\n#gauth = GoogleAuth()\n#gauth.credentials = GoogleCredentials.get_application_default()\n#drive = GoogleDrive(gauth)\n\n# Another way to upload your dataset from Google Drive\n\n#from google.colab import drive\n#drive.mount('\/content\/drive')","52fa6889":"# Loading necessary libraries\n\nimport numpy as np\nimport pandas as pd\n\npd.set_option('display.max_colwidth', 200)\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n%matplotlib inline\n\nimport re\nimport string\n\nimport warnings\n\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning) \nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\nwarnings.filterwarnings(\"ignore\", category=UserWarning)","3e880471":"#train = pd.read_csv('\/content\/drive\/My Drive\/datasets\/twitter_data\/train.csv')\n#test = pd.read_csv('\/content\/drive\/My Drive\/datasets\/twitter_data\/test.csv')","c51298b9":"train = pd.read_csv('..\/input\/Twitter Sentiment Train.csv')\ntest = pd.read_csv('..\/input\/Twitter Sentiment Test.csv')","2d9dc336":"print('Training dataset has {} rows and {} columns'.format(train.shape[0], train.shape[1]))\nprint('Testing dataset has {} rows and {} columns'.format(test.shape[0], test.shape[1]))","a2c0c07c":"train.head(10)","5e47b03f":"test.head(10)","0e877ae2":"train.label.unique()","fd07dfa0":"train[train.label == 0].head(10)","468ba8ad":"train[train.label == 1].head(10)","70ab280a":"train.label.value_counts()","5eb77ade":"train_length = train.tweet.str.len()\ntest_length = test.tweet.str.len()","f76291d8":"plt.hist(train_length, bins=20, label='Tweets in training set')\nplt.hist(test_length, bins=20, label='Tweets in testing set')\nplt.legend()\nplt.show()","68ce6a91":"combined = train.append(test, ignore_index=True)\nprint('Combined data has {} rows and {} columns'.format(combined.shape[0], combined.shape[1]))","757a37fe":"combined.head(10)","90bc832a":"combined.tail(10)","5f7a58b7":"from nltk.stem.porter import PorterStemmer\nstemmer = PorterStemmer()\n\ndef clean_tweet(tweet):\n  \n  '''This function takes raw tweet as input and\n  returns cleaned tweet. It removes twitter handles, punctuation, \n  short words and stopwords which does not contribute much to our\n  analysis'''\n  clean_handle = re.sub(r'@[\\w]*', '', tweet)                                   # Removes twitter handles from tweets\n  clean_punc = re.sub(r'[^a-zA-Z#]', ' ', clean_handle)                          # Removes punctuation, special characters(except #tags) \n  clean_short_tokenized = [word for word in clean_punc.split() if len(word) > 3] # Remove short words and tokenize\n  clean_normalize = [stemmer.stem(word) for word in clean_short_tokenized]       # Stem tokenized words\n  return ' '.join(clean_normalize)","f5d881d9":"clean_tweet(combined.tweet.iloc[3])","1118cd5e":"combined.tweet = combined.tweet.apply(lambda x : clean_tweet(x))","950ec2e7":"from wordcloud import WordCloud, STOPWORDS\nwordcloud = WordCloud(width=800, height=500, random_state=42, max_font_size=110, stopwords=STOPWORDS)","282b37b2":"all_tweets = ' '.join(combined.tweet)\ngood_tweets = ' '.join(combined[combined.label == 0].tweet)\nbad_tweets = ' '.join(combined[combined.label == 1].tweet)","2e25f217":"bad_tweets","03136cb7":"plt.figure(figsize=(20,12))\nplt.subplot(1,3,1)\nplt.imshow(wordcloud.generate(all_tweets), interpolation='bilinear')\nplt.axis('off')\nplt.title('All tweets')\nplt.subplot(1,3,2)\nplt.imshow(wordcloud.generate(good_tweets), interpolation='bilinear')\nplt.axis('off')\nplt.title('Good tweets')\nplt.subplot(1,3,3)\nplt.imshow(wordcloud.generate(bad_tweets), interpolation='bilinear')\nplt.axis('off')\nplt.title('Bad tweets')\nplt.show()","f708a1d4":"def extract_hashtags(tweet):\n  hashtag = re.findall(r'#(\\w+)', tweet)\n  return hashtag","f69eee7f":"good_hashtags = extract_hashtags(good_tweets)\nbad_hashtags = extract_hashtags(bad_tweets)","c5cde705":"from nltk import FreqDist\ngood = FreqDist(good_hashtags)\nbad = FreqDist(bad_hashtags)\n\ngood_ht = pd.DataFrame({'Hashtag' : list(good.keys()) , 'Count' : list(good.values())}).sort_values('Count', ascending=False)\nbad_ht = pd.DataFrame({'Hashtag' : list(bad.keys()) , 'Count' : list(bad.values())}).sort_values('Count', ascending=False)","e9ada023":"plt.figure(figsize=(20,10))\nplt.subplot(2,1,1)\nsns.barplot(data=good_ht.iloc[:20], x='Hashtag', y='Count')\nplt.title('Hashtag frequency distribution - Good tweets')\nplt.subplot(2,1,2)\nsns.barplot(data=bad_ht.iloc[:20], x='Hashtag', y='Count')\nplt.title('Hashtag frequency distribution - Bad tweets')\nplt.show()","8464950e":" from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n import gensim","9e16df90":"bow_vectorizer = CountVectorizer(max_df=0.90, min_df=2, max_features=1000, stop_words='english')\nbow = bow_vectorizer.fit_transform(combined.tweet)\nbow.shape","b462a6ae":"tfidf_vectorizer = TfidfVectorizer(max_df=0.90, min_df=2, max_features=1000, stop_words='english')\ntfidf = tfidf_vectorizer.fit_transform(combined.tweet)\ntfidf.shape","7574cd99":"tokenized_tweets = combined.tweet.apply(lambda x : x.split())\nw2v_model = gensim.models.Word2Vec(\n            tokenized_tweets,\n            size=200, # desired no. of features\/independent variables\n            window=5, # context window size\n            min_count=2,\n            sg = 1, # 1 for skip-gram model\n            hs = 0,\n            negative = 10, # for negative sampling\n            workers= 2, # no.of cores\n            seed = 34) ","0779a3c8":"w2v_model.train(tokenized_tweets, total_examples=len(combined.tweet), epochs=20)","58dc2842":"w2v_model.wv.most_similar(positive='dinner')","897a9d44":"w2v_model.wv.most_similar(positive='trump')","df7ea8cb":"w2v_model['food']","4d2a1a08":"def word_vector(tweet):\n  \n  vec = np.zeros(200).reshape((1, 200))\n  count = 0\n  \n  for word in tweet:\n    try:\n      vec += w2v_model[word].reshape((1, 200))\n      count += 1\n    except KeyError:\n      continue\n      \n  if count!=0:\n    vec \/= count\n  return vec","f6227093":"wordvec_array = np.zeros((len(tokenized_tweets), 200))\nfor i in range(len(tokenized_tweets)):\n  wordvec_array[i,:] = word_vector(tokenized_tweets.iloc[i])","ff7fd013":"wordvec_df = pd.DataFrame(wordvec_array)","2e9b2d56":"wordvec_df.head()","cef6cb2d":"from tqdm import tqdm\ntqdm.pandas(desc='progress-bar')\nfrom gensim.models.doc2vec import LabeledSentence","9a366d2c":"def add_label(tweet):\n  output = []\n  \n  for i, s in zip(tweet.index, tweet):\n    output.append(LabeledSentence(s, ['tweet_'+str(i)]))\n  \n  return output","17d3fa0a":"labeled_tweets = add_label(tokenized_tweets)","53b78836":"len(labeled_tweets)","72406c93":"labeled_tweets[31961]","4fd4764e":"labeled_tweets[:10]","037e316d":"d2v_model = gensim.models.Doc2Vec(dm=1, # dm = 1 for \u2018distributed memory\u2019 model                                   \n                                  dm_mean=1, # dm = 1 for using mean of the context word vectors                                  \n                                  size=200, # no. of desired features\n                                  window=5, # width of the context window\n                                  negative=7, # if > 0 then negative sampling will be used                                 \n                                  min_count=5, # Ignores all words with total frequency lower than 2.\n                                  workers=3, # no. of cores\n                                  alpha=0.1, # learning rate\n                                  seed = 23) \nd2v_model.build_vocab([i for i in tqdm(labeled_tweets)])\nd2v_model.train(labeled_tweets, total_examples=len(combined.tweet), epochs=15)","bc3b6e2a":"doc2vec_array = np.zeros((len(tokenized_tweets), 200))\nfor i in range(len(combined.tweet)):\n  doc2vec_array[i, :] = d2v_model.docvecs[i].reshape((1, 200))\n  \ndoc2vec_df = pd.DataFrame(doc2vec_array)\ndoc2vec_df.shape","88fa6a69":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import f1_score, confusion_matrix, accuracy_score, roc_curve, auc","ec8dbf18":"train_bow = bow[:31962, :] # Training data had 31962 rows\ntest_bow = bow[31962:, :] # Testing data\nx_bow_train, x_bow_test, y_bow_train, y_bow_test = train_test_split(train_bow, train.label, test_size=0.3, random_state=42)","b274ab45":"lr = LogisticRegression()\nlr.fit(x_bow_train, y_bow_train)","9d93ab18":"bow_pred_prob = lr.predict_proba(x_bow_test)\nbow_pred_prob_fin = bow_pred_prob[:, 1] >= 0.3\nbow_pred_prob_fin = bow_pred_prob_fin.astype(np.int)\nprint('Accuracy score : ',accuracy_score(y_bow_test, bow_pred_prob_fin))\nprint('F1 Score : ',f1_score(y_bow_test, bow_pred_prob_fin))","ebcb92c8":"bow_pred = lr.predict(x_bow_test)\nprint('Accuracy score : ',accuracy_score(y_bow_test, bow_pred))\nprint('F1 Score : ',f1_score(y_bow_test, bow_pred))","fe712944":"bow_preds_lr = bow_pred_prob[:, 1]\nfpr, tpr, threshold = roc_curve(y_bow_test, bow_preds_lr)\nroc_auc = auc(fpr, tpr)","f56bfb57":"plt.title('Receiver Operating Characteristics')\nplt.plot(fpr, tpr, 'b', label='AUC = {:.2f}'.format(roc_auc))\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0, 1])\nplt.ylim([0, 1])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()","f44bfa53":"train_tfidf = tfidf[:31962, :]\ntest_tfidf = tfidf[31962:, :]\nx_tfidf_train, x_tfidf_test, y_tfidf_train, y_tfidf_test = train_test_split(train_tfidf, train.label, test_size=0.3, random_state=42)","1b0f1416":"lr.fit(x_tfidf_train, y_tfidf_train)\ntfidf_pred_prob = lr.predict_proba(x_tfidf_test)\ntfidf_pred_thresh = tfidf_pred_prob[:, 1] >= 0.3\ntfidf_pred = tfidf_pred_thresh.astype(np.int)\nprint('Accuracy score : ',accuracy_score(y_tfidf_test, tfidf_pred))\nprint('F1 Score : ',f1_score(y_tfidf_test, tfidf_pred))","acd33f64":"train_w2v = wordvec_df.iloc[:31962, :]\ntest_w2v = wordvec_df.iloc[31962:, :]\nx_w2v_train, x_w2v_test, y_w2v_train, y_w2v_test = train_test_split(train_w2v, train.label, test_size=0.3, random_state=42)","3af88708":"lr.fit(x_w2v_train, y_w2v_train)\nw2v_pred_prob = lr.predict_proba(x_w2v_test)\nw2v_pred_thresh = w2v_pred_prob[:, 1] >= 0.3\nw2v_pred = w2v_pred_thresh.astype(np.int)\nprint('Accuracy score : ',accuracy_score(y_w2v_test, w2v_pred))\nprint('F1 Score : ',f1_score(y_w2v_test, w2v_pred))","73bcc6f6":"train_d2v = doc2vec_df.iloc[:31962, :]\ntest_d2v = doc2vec_df.iloc[31962:, :]\nx_d2v_train, x_d2v_test, y_d2v_train, y_d2v_test = train_test_split(train_d2v, train.label, test_size=0.3, random_state=42)","83409a81":"lr.fit(x_d2v_train, y_d2v_train)\nd2v_pred_prob = lr.predict_proba(x_d2v_test)\nd2v_pred_thresh = d2v_pred_prob[:, 1] >= 0.3\nd2v_pred = d2v_pred_thresh.astype(np.int)\nprint('Accuracy score : ',accuracy_score(y_d2v_test, d2v_pred))\nprint('F1 Score : ',f1_score(y_d2v_test, d2v_pred))","3078a512":"from sklearn.svm import SVC\nsvc = SVC(kernel='linear', C=1, probability=True)","bc15bcd7":"svc.fit(x_bow_train, y_bow_train)\nbow_pred_prob = svc.predict_proba(x_bow_test)\nbow_pred_thresh = bow_pred_prob[:, 1] >= 0.3\nbow_pred = bow_pred_thresh.astype(np.int)\nprint('Accuracy score : ',accuracy_score(y_bow_test, bow_pred))\nprint('F1 Score : ',f1_score(y_bow_test, bow_pred))","774b725b":"svc.fit(x_tfidf_train, y_tfidf_train)\ntfidf_pred_prob = svc.predict_proba(x_tfidf_test)\ntfidf_pred_thresh = tfidf_pred_prob[:, 1] >= 0.3\ntfidf_pred = tfidf_pred_thresh.astype(np.int)\nprint('Accuracy score : ',accuracy_score(y_tfidf_test, tfidf_pred))\nprint('F1 Score : ',f1_score(y_tfidf_test, tfidf_pred))","346d628d":"svc.fit(x_w2v_train, y_w2v_train)\nw2v_pred_prob = svc.predict_proba(x_w2v_test)\nw2v_pred_thresh = w2v_pred_prob[:, 1] >= 0.3\nw2v_pred = w2v_pred_thresh.astype(np.int)\nprint('Accuracy score : ',accuracy_score(y_w2v_test, w2v_pred))\nprint('F1 Score : ',f1_score(y_w2v_test, w2v_pred))","e8c429ef":"svc.fit(x_d2v_train, y_d2v_train)\nd2v_pred_prob = svc.predict_proba(x_d2v_test)\nd2v_pred_thresh = d2v_pred_prob[:, 1] >= 0.3\nd2v_pred = d2v_pred_thresh.astype(np.int)\nprint('Accuracy score : ',accuracy_score(y_d2v_test, d2v_pred))\nprint('F1 Score : ',f1_score(y_d2v_test, d2v_pred))","47dab4e1":"from sklearn.ensemble import RandomForestClassifier\nrfc = RandomForestClassifier(n_estimators=400, random_state=11)","b159d345":"rfc.fit(x_bow_train, y_bow_train)\nbow_pred = rfc.predict(x_bow_test)\nprint('Accuracy score : ',accuracy_score(y_bow_test, bow_pred))\nprint('F1 Score : ',f1_score(y_bow_test, bow_pred))","1e98c6bd":"rfc.fit(x_tfidf_train, y_tfidf_train)\ntfidf_pred = rfc.predict(x_tfidf_test)\nprint('Accuracy score : ',accuracy_score(y_tfidf_test, tfidf_pred))\nprint('F1 Score : ',f1_score(y_tfidf_test, tfidf_pred))","5b715871":"rfc.fit(x_w2v_train, y_w2v_train)\nw2v_pred = rfc.predict(x_w2v_test)\nprint('Accuracy score : ',accuracy_score(y_w2v_test, w2v_pred))\nprint('F1 Score : ',f1_score(y_w2v_test, w2v_pred))","9ca9c147":"rfc.fit(x_d2v_train, y_d2v_train)\nd2v_pred = rfc.predict(x_d2v_test)\nprint('Accuracy score : ',accuracy_score(y_d2v_test, d2v_pred))\nprint('F1 Score : ',f1_score(y_d2v_test, d2v_pred))","78dd6003":"from xgboost import XGBClassifier\nxgb = XGBClassifier(max_depth=6, n_estimators=1000)","98a7360a":"xgb.fit(x_bow_train, y_bow_train)\nbow_pred = xgb.predict(x_bow_test)\nprint('Accuracy score : ',accuracy_score(y_bow_test, bow_pred))\nprint('F1 Score : ',f1_score(y_bow_test, bow_pred))","05e87153":"xgb.fit(x_tfidf_train, y_tfidf_train)\ntfidf_pred = xgb.predict(x_tfidf_test)\nprint('Accuracy score : ',accuracy_score(y_tfidf_test, tfidf_pred))\nprint('F1 Score : ',f1_score(y_tfidf_test, tfidf_pred))","13f2dc79":"xgb.fit(x_w2v_train, y_w2v_train)\nw2v_pred = xgb.predict(x_w2v_test)\nprint('Accuracy score : ',accuracy_score(y_w2v_test, w2v_pred))\nprint('F1 Score : ',f1_score(y_w2v_test, w2v_pred))","dc399a0f":"xgb.fit(x_d2v_train, y_d2v_train)\nd2v_pred = xgb.predict(x_d2v_test)\nprint('Accuracy score : ',accuracy_score(y_d2v_test, d2v_pred))\nprint('F1 Score : ',f1_score(y_d2v_test, d2v_pred))","219c835b":"from xgboost import DMatrix, cv, train","1b40af14":"dtrain = DMatrix(x_w2v_train, label=y_w2v_train)\ndtest = DMatrix(x_w2v_test, label=y_w2v_test)\ntest_set = DMatrix(test_w2v)","114d7af0":"params = {'objective':'binary:logistic',\n    'max_depth':6,\n    'min_child_weight':1,\n    'eta':.3,\n    'subsample':1,\n    'colsample_bytree':1,}","513ad503":"def custom_f1_eval(preds, dtrain):\n  labels = dtrain.get_label().astype(np.int)\n  preds = (preds >= 0.3).astype(np.int)\n  #print(preds)\n  \n  return [('f1_score', f1_score(labels, preds))]","a9087584":"gridsearch_params = [\n    (max_depth, min_child_weight)\n    for max_depth in range(6,10)\n    for min_child_weight in range(5,8)\n ]","1d3b953b":"max_f1 = 0.\nbest_params = None\n\nfor max_depth, min_child_weight in gridsearch_params:\n  print('CV with max_depth:{}, min_child_weight:{}'.format(max_depth, min_child_weight))\n  params['max_depth'] = max_depth\n  params['min_child_weight'] = min_child_weight\n  \n  #Cross Validation\n  cv_results = cv(params=params,\n                 dtrain=dtrain,\n                 feval = custom_f1_eval,\n                 num_boost_round=200,\n                 maximize=True,\n                 seed=16,\n                 nfold=5,\n                 early_stopping_rounds=10,)\n  #print(cv_results)\n  # Finding best F1 Score\n\n  mean_f1_score = cv_results['test-f1_score-mean'].max()\n  boost_rounds = cv_results['test-f1_score-mean'].argmax()\n  print('\\tF1 Score {} for {} rounds'.format(mean_f1_score, boost_rounds))\n\nif mean_f1_score > max_f1:\n  max_f1 = mean_f1_score\n  best_params = (max_depth, min_child_weight)\n\nprint(\"Best params: {}, {}, F1 Score: {}\".format(best_params[0], best_params[1], max_f1))  ","d8c69ff6":"params['max_depth'] = 6 #best_params[0]\nparams['min_child_weight'] = 7 #best_params[1]","33550987":"gridsearch_params = [(subsample, colsample)\n                    for subsample in [i\/10 for i in range(5, 10)]\n                    for colsample in [i\/10 for i in range(5,10)]]\n\nmax_f1 = 0\nbest_params = None\nfor subsample, colsample in gridsearch_params:\n    print(\"CV with subsample={}, colsample={}\".format(\n                             subsample,\n                             colsample))\n     # Update our parameters\n    params['colsample'] = colsample\n    params['subsample'] = subsample\n    cv_results = cv(\n        params,\n        dtrain,\n        feval= custom_f1_eval,\n        num_boost_round=200,\n        maximize=True,\n        seed=16,\n        nfold=5,\n        early_stopping_rounds=10\n    )\n     # Finding best F1 Score\n    mean_f1 = cv_results['test-f1_score-mean'].max()\n    boost_rounds = cv_results['test-f1_score-mean'].argmax()\n    print(\"\\tF1 Score {} for {} rounds\".format(mean_f1, boost_rounds))\n    if mean_f1 > max_f1:\n        max_f1 = mean_f1\n        best_params = (subsample, colsample) \n\nprint(\"Best params: {}, {}, F1 Score: {}\".format(best_params[0], best_params[1], max_f1))","6f9d164b":"params['subsample'] = best_params[0]\nparams['colsample_bytree'] = best_params[1]","5884f13e":"max_f1 = 0. \nbest_params = None \nfor eta in [.3, .2, .1, .05, .01, .005]:\n    print(\"CV with eta={}\".format(eta))\n     # Update ETA\n    params['eta'] = eta\n\n     # Run CV\n    cv_results = cv(\n        params,\n        dtrain,\n        feval= custom_f1_eval,\n        num_boost_round=1000,\n        maximize=True,\n        seed=16,\n        nfold=5,\n        early_stopping_rounds=20\n    )\n\n     # Finding best F1 Score\n    mean_f1 = cv_results['test-f1_score-mean'].max()\n    boost_rounds = cv_results['test-f1_score-mean'].argmax()\n    print(\"\\tF1 Score {} for {} rounds\".format(mean_f1, boost_rounds))\n    if mean_f1 > max_f1:\n        max_f1 = mean_f1\n        best_params = eta \nprint(\"Best params: {}, F1 Score: {}\".format(best_params, max_f1))","ac878adc":"params['eta'] = best_params\nparams['max_depth'] = 8\nparams['min_child_weight'] = 6\nparams['sub_sample'] = 0.9","f034e1bb":"params","03b2edab":"xgb_final = train(params,\n                  dtrain,\n                  feval=custom_f1_eval,\n                  num_boost_round=1000,\n                  maximize=True,\n                  evals=[(dtest, 'Validation')],\n                  early_stopping_rounds=10)","7f3e011d":"predictions = xgb_final.predict(test_set)","ce56b238":"test['label'] = (predictions >= 0.3).astype(np.int)","8726c7be":"test[test.label == 1].head()","e3be56aa":"submission = test[['id', 'label']]","8b5a1e9e":"**Doc2Vec Embedding**\n\nDoc2Vec model is an unsupervised algorithm to generate vectors for sentence\/paragraphs\/documents. This approach is an extension of the word2vec. The major difference between the two is that doc2vec provides an additional context which is unique for every document in the corpus. This additional context is nothing but another feature vector for the whole document. This document vector is trained along with the word vectors.","395ce9c8":"###  Model Building : Sentiment Analysis\n\nWe are now done with all the pre-modeling stages required to get the data in the proper form and shape. We will be building models on the datasets with different feature sets prepared in the earlier sections \u2014 Bag-of-Words, TF-IDF, word2vec vectors, and doc2vec vectors. We will use the following algorithms to build models:\n\n - Logistic Regression\n - Support Vector Machine\n - RandomForest\n - XGBoost\n - Evaluation Metric\n \n\nF1 score is being used as the evaluation metric. It is the weighted average of Precision and Recall. Therefore, this score takes both false positives and false negatives into account. It is suitable for uneven class distribution problems.\n\nThe important components of F1 score are:\n\n - True Positives (TP) - These are the correctly predicted positive values which means that the value of actual class is yes and the value of predicted class is also yes.\n - True Negatives (TN) - These are the correctly predicted negative values which means that the value of actual class is no and value of predicted class is also no.\n - False Positives (FP) \u2013 When actual class is no and predicted class is yes.\n - False Negatives (FN) \u2013 When actual class is yes but predicted class in no.\nPrecision = TP\/TP+FP\n\n - Recall = TP\/TP+FN\n\n - F1 Score = 2(Recall Precision) \/ (Recall + Precision)","8d35c5fb":"# Twitter Sentiment Analysis","aa66bf27":"Now let us train Doc2Vec model","a6be2e63":"Sentiment analysis (also known as opinion mining) is one of the many applications of Natural Language Processing. It is a set of methods and techniques used for extracting subjective information from text or speech, such as opinions or attitudes. In simple terms, it involves classifying a piece of text as positive, negative or neutral.","5ed71413":"In our data preprocessing we combined both training and testing data. Now for training the model we will retrieve the training data.","e49ebe08":"Word embeddings are the modern way of representing words as vectors. The objective of word embeddings is to redefine the high dimensional word features into low dimensional feature vectors by preserving the contextual similarity in the corpus. They are able to achieve tasks like King-man +woman = Queen, which is mind-blowing. A detailed study of Word2Vec feature will be presented in other kernel.\n\nThe advantages of using word embeddings over BOW or TF-IDF are:\n\n - Dimensionality reduction - significant reduction in the no. of features required to build a model.\n\n - It capture meanings of the words, semantic relationships and the different types of contexts they are used in.","9d548267":"### TFIDF Features","ab1830f0":"## Tweets Preprocessing and Cleaning\n","9ed111d9":"**Logistic Regression**\n\nLogistic Regression is a classification algorithm. It is used to predict a binary outcome (1 \/ 0, Yes \/ No, True \/ False) given a set of independent variables. You can also think of logistic regression as a special case of linear regression when the outcome variable is categorical, where we are using log of odds as the dependent variable. In simple words, it predicts the probability of occurrence of an event by fitting data to a logit function.","d6e3131a":"## Table of contents\n\n - Understand the Problem Statement\n - Tweets Preprocessing and Cleaning\n  - Data Inspection\n  - Data Cleaning\n - Story Generation and Visualization from Tweets\n - Extracting Features from Cleaned Tweets\n  - Bag-of-Words\n  - TF-IDF\n  - Word Embeddings\n - Model Building: Sentiment Analysis\n  - Logistic Regression\n  - Support Vector Machine\n  - RandomForest\n  - XGBoost\n - Model Fine-tuning\n - Summary","324e800c":"**Random Forest**\n\nRandom Forest is a versatile machine learning algorithm capable of performing both regression and classification tasks. It is a kind of ensemble learning method, where a few weak models combine to form a powerful model. In Random Forest, we grow multiple trees as opposed to a decision single tree. To classify a new object based on attributes, each tree gives a classification and we say the tree \u201cvotes\u201d for that class. The forest chooses the classification having the most votes (over all the trees in the forest).\n\nIt works in the following manner. Each tree is planted & grown as follows:\n\n - Assume number of cases in the training set is N. Then, sample of these N cases is taken at random but with replacement. This sample will be the training set for growing the tree.\n\n - If there are M input variables, a number m (m<M) is specified such that at each node, m variables are selected at random out of the M. The best split on these m variables is used to split the node. The value of m is held constant while we grow the forest.\n\n - Each tree is grown to the largest extent possible and there is no pruning.\n\n - Predict new data by aggregating the predictions of the ntree trees (i.e., majority votes for classification, average for regression).","6aabb8e7":"<a href=\"final_submission.csv\"> Download File <\/a>","e161f2fb":"From comparing both train and test dataset it is clear that we have to predict the label for corresponding tweets in test dataset. Let us check the nature of labels in train dataset.","f571cad1":"**Support Vector Machines**\n\nSupport Vector Machine (SVM) is a supervised machine learning algorithm which can be used for both classification or regression challenges. However, it is mostly used in classification problems. In this algorithm, we plot each data item as a point in n-dimensional space (where n is the number of features you have) with the value of each feature being the value of a particular coordinate. Then, we perform classification by finding the hyper-plane that differentiate the two classes","2d19d3f3":"## Extracting Features from Cleaned Tweets","bd469dfb":"## Understand the Problem Statement","00c0c7a2":"By observing the above few tweets corresponding to each labels, it is safe to conclude that label 0 is for non-offensive tweets and label 1 is for offensive tweets.","34a3788c":"### Word2Vec Features","1fc83b8e":"In any natural language processing task, cleaning raw text data is an important step. It helps in getting rid of the unwanted words and characters which helps in obtaining better features. If we skip this step then there is a higher chance that you are working with noisy and inconsistent data. The objective of this step is to clean noise those are less relevant to find the sentiment of tweets such as punctuation, special characters, numbers, and terms which don\u2019t carry much weightage in context to the text.\n\nBefore we begin cleaning, let\u2019s first combine train and test datasets. Combining the datasets will make it convenient for us to preprocess the data. Later we will split it back into train and test data.","e7069455":"**General Approach for Parameter Tuning**\n\nWe will follow the steps below to tune the parameters.\n\n - Choose a relatively high learning rate. Usually a learning rate of 0.3 is used at this stage.\n\n - Tune tree-specific parameters such as max_depth, min_child_weight, subsample, colsample_bytree keeping the learning rate fixed.\n\n - Tune the learning rate.\n\n - Finally tune gamma to avoid overfitting.","af10df20":"Let us check the label distribution in the train dataset","4a943b59":"In this section, we will explore the cleaned tweets. Exploring and visualizing data, no matter whether its text or any other data, is an essential step in gaining insights. Do not limit yourself to only these methods told in this course, feel free to explore the data as much as possible.\n\nBefore we begin exploration, we must think and ask questions related to the data in hand. A few probable questions are as follows:\n\n - What are the most common words in the entire dataset?\n - What are the most common words in the dataset for negative and positive tweets, respectively?\n - How many hashtags are there in a tweet?\n - Which trends are associated with my dataset?\n - Which trends are associated with either of the sentiments? Are they compatible with the sentiments?","6b238f8c":"  **XGBoost**\n  \n  Extreme Gradient Boosting (xgboost) is an advanced implementation of gradient boosting algorithm. It has both linear model solver and tree learning algorithms. Its ability to do parallel computation on a single machine makes it extremely fast. It also has additional features for doing cross validation and finding important variables. There are many parameters which need to be controlled to optimize the model.\n\nSome key benefits of XGBoost are:\n\n - Regularization - helps in reducing overfitting\n\n - Parallel Processing - XGBoost implements parallel processing and is blazingly faster as compared to GBM.\n\n - Handling Missing Values - It has an in-built routine to handle missing values.\n\n - Built-in Cross-Validation - allows user to run a cross-validation at each iteration of the boosting process","f11a2684":"Text is a highly unstructured form of data, various types of noise are present in it and the data is not readily analyzable without any pre-processing. The entire process of cleaning and standardization of text, making it noise-free and ready for analysis is known as text preprocessing. We will divide it into 2 parts:\n\n - Data Inspection\n - Data Cleaning","1b8b242f":"### Bag of Words","87bd4c72":"Now let us examine the distribution of tweet lengths in training and testing data set.","b151844e":"### Story Generation and Visualization from Tweets","2a4211ba":"So far our best performing model is XGBoost with Word2Vec feature. So we will try to perform some hyperparameter tuning on this model to see whether we can have a boost in model performance.","bfc2c680":"### Model Fine-tuning\n\nXGBoost has quite a many tuning parameters and sometimes it becomes tricky to properly tune them. This is what we are going to do in the following steps.","e3649acb":"From above plots it becomes clear that most frequent hashtags convey the tweet sentiment","7e774dee":"Let\u2019s train a Word2Vec model on our corpus.","b64454a4":"There are only two labels in the label column of train dataset. Let us check what kind of tweets does each of these labels correspond to.","115a4474":"It seems like we have more non-offensive tweets in our training set than offensive ones. In the train dataset, we have 2,242 (7%) tweets labeled as offensive, and 29,720 (93%) tweets labeled as non offensive. So, it is an imbalanced classification challenge.","cc621276":"**Word2Vec Embeddings**\n\nWord2Vec is not a single algorithm but a combination of two techniques \u2013 CBOW (Continuous bag of words) and Skip-gram model. Both of these are shallow neural networks which map word(s) to the target variable which is also a word(s). Both of these techniques learn weights which act as word vector representations.\n\nCBOW tends to predict the probability of a word given a context. A context may be a single adjacent word or a group of surrounding words. The Skip-gram model works in the reverse manner, it tries to predict the context for a given word.\n\n","7ba971c5":"How would we know how much the threshold should be?.\nFor that we could use ROC curve to find the threshold","f52ab411":"An imbalanced dataset is one in which labels are not in equal counts. Often small differences doesn't matter.  It often gives rise to *accuracy paradox*. \n\nConsider a dataset having test details of 100 cancer patients of which naturally 6 were found to have cancer. If we build a model which predicts whether the patient as cancer or not on the basis of random guessing it is going to be accurate 94% of the time. So does that mean we have a good machine learning model. No, rather it says we are using wrong metrics for evaluating model performance. So in the case of imbalanced datsets we usually resort to metrics such as confusion matrix, F1 score etc rather than accuracy score.","e524b3d3":"This notebook is made as a part of Twitter Sentiment Analysis course by Analytics Vidhya","8a791f3e":"## Data Cleaning","ddc5fb22":"### Data Inspection","8d4420a7":"Since our data contains tweets and not just words, we\u2019ll have to figure out a way to use the word vectors from word2vec model to create vector representation for an entire tweet. There is a simple solution to this problem, we can simply take mean of all the word vectors present in the tweet. The length of the resultant vector will be the same, i.e. 200. We will repeat the same process for all the tweets in our data and obtain their vectors. Now we have 200 word2vec features for our data.\n\nWe will use the below function to create a vector for each tweet by taking the average of the vectors of the words present in the tweet."}}