{"cell_type":{"478b39c9":"code","8516e3ef":"code","4d8e3a70":"code","44880a2a":"code","36efd88f":"code","b3160ada":"code","8a411a57":"code","40b79c6d":"code","808fdf78":"code","2609e61e":"code","057462d7":"code","4efaed0c":"code","ea6a130a":"code","b60fc6a8":"code","4254d253":"code","de31e99d":"code","d077540a":"code","988ba89f":"code","e5b68b45":"code","62f8cfa1":"code","d0da6911":"code","47bad36a":"code","2476c709":"code","2b192e32":"code","6d5c4094":"code","fe30e43c":"code","8c186571":"code","06fad7e4":"code","33a3c371":"code","b010eb69":"code","32adb441":"code","879e8412":"code","9e06be77":"code","fb73ebdd":"code","467ca475":"code","da570a96":"code","696ae7d0":"code","9e4e6eb8":"code","90d69744":"code","bc34b113":"code","b4981dc0":"code","404618fe":"code","6a47bd29":"code","4d43da46":"code","113ec73a":"code","f3ea293d":"code","693228a4":"code","bc438708":"code","fc50cc79":"markdown","e9c43798":"markdown","a1102f73":"markdown","d295752c":"markdown","8f453ba3":"markdown","1e6b9e16":"markdown","26c5f7ab":"markdown","20578bf6":"markdown","d99ff896":"markdown","9f2c0811":"markdown"},"source":{"478b39c9":"!pip install bert-for-tf2","8516e3ef":"import numpy as np\nimport math\nimport re\nimport pandas as pd\nfrom bs4 import BeautifulSoup\nimport random\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nimport tensorflow_hub as hub\nfrom tensorflow.keras import layers\nimport bert\nimport plotly.express as px\nfrom wordcloud import WordCloud","4d8e3a70":"!pip install sentencepiece","44880a2a":"data = pd.read_csv('..\/input\/stockmarket-sentiment-dataset\/stock_data.csv')","36efd88f":"data","b3160ada":"sns.heatmap(data.isnull());","8a411a57":"temp = data.describe()\ntemp.style.background_gradient(cmap='Oranges')","40b79c6d":"total = len(data)\nax1 = plt.figure(figsize=(12,5))\n\ng = sns.countplot(x='Sentiment', data=data)\ng.set_title(\"Evaluation\", fontsize=20)\ng.set_xlabel(\"Evaluation\", fontsize=17)\ng.set_ylabel(\"Values\", fontsize=17)\nsizes = []\nfor p in g.patches:\n    height = p.get_height()\n    sizes.append(height)\n    g.text(p.get_x()+p.get_width()\/2.,\n            height + 3,\n            '{:1.2f}%'.format(height\/total*100),\n            ha=\"center\", fontsize=10) \ng.set_ylim(0, max(sizes) * 1.1)","808fdf78":"fig2 = px.histogram(data,x='Sentiment',color='Sentiment',template='plotly_dark')\nfig2.show()","2609e61e":"positive = data[data['Sentiment'] == 1 ]\nnegative = data[data['Sentiment'] == -1]","057462d7":"data","4efaed0c":"plt.rcParams['figure.figsize'] = (10, 10)\nplt.style.use('fast')\nwc = WordCloud(background_color = 'orange', width = 1500, height = 1500).generate(str(positive['Text']))\nplt.title('Description Positive', fontsize = 15)\nplt.imshow(wc)\nplt.axis('off')\nplt.show()","ea6a130a":"plt.rcParams['figure.figsize'] = (10, 10)\nplt.style.use('fast')\nwc = WordCloud(background_color = 'orange', width = 1500, height = 1500).generate(str(negative['Text']))\nplt.title('Description Negative', fontsize = 15)\nplt.imshow(wc)\nplt.axis('off')\nplt.show()","b60fc6a8":"def clean_tweet(tweet):\n  tweet = BeautifulSoup(tweet, 'lxml').get_text()\n  tweet = re.sub(r\"@[A-Za-z0-9]+\", ' ', tweet)\n  tweet = re.sub(r\"https?:\/\/[A-Za-z0-9.\/]+\", ' ', tweet)\n  tweet = re.sub(r\"[^a-zA-Z.!?']\", ' ', tweet)\n  tweet = re.sub(r\" +\", ' ', tweet)\n  return tweet","4254d253":"test = '99 ' + data.Text[0]\ntest","de31e99d":"result = clean_tweet(test)\nresult","d077540a":"data_clean = [clean_tweet(tweet) for tweet in data.Text]","988ba89f":"data_clean[0:4]","e5b68b45":"data_labels = data.Sentiment.values\ndata_labels","62f8cfa1":"FullTokenizer = bert.bert_tokenization.FullTokenizer\nbert_layer = hub.KerasLayer('https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_L-24_H-1024_A-16\/1', trainable=False)  \nvocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\ndo_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\ntokenizer = FullTokenizer(vocab_file, do_lower_case)","d0da6911":"vocab_file","47bad36a":"len(tokenizer.vocab)","2476c709":"def encode_sentence(sent):\n  return tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sent))","2b192e32":"data_inputs = [encode_sentence(sentence) for sentence in data_clean]","6d5c4094":"data_with_len = [[sent, data_labels[i], len(sent)]\n                 for i, sent in enumerate(data_inputs)]","fe30e43c":"random.shuffle(data_with_len)\ndata_with_len.sort(key=lambda x: x[2])\nsorted_all = [(sent_lab[0], sent_lab[1])\n              for sent_lab in data_with_len if sent_lab[2] > 7]","8c186571":"all_dataset = tf.data.Dataset.from_generator(lambda: sorted_all,\n                                             output_types = (tf.int32, tf.int32))","06fad7e4":"next(iter(all_dataset))","33a3c371":"BATCH_SIZE = 32\nall_batched = all_dataset.padded_batch(BATCH_SIZE, padded_shapes=((None, ), ()))","b010eb69":"next(iter(all_batched))","32adb441":"len(sorted_all)","879e8412":"NB_BATCHES = len(sorted_all) \/\/ BATCH_SIZE\nNB_BATCHES","9e06be77":"NB_BATCHES_TEST = NB_BATCHES \/\/ 10\nNB_BATCHES_TEST","fb73ebdd":"all_batched.shuffle(NB_BATCHES)\ntest_dataset = all_batched.take(NB_BATCHES_TEST)\ntrain_dataset = all_batched.skip(NB_BATCHES_TEST)","467ca475":"next(iter(test_dataset))","da570a96":"next(iter(train_dataset))","696ae7d0":"class DCNN(tf.keras.Model):\n\n  def __init__(self,\n               vocab_size,\n               emb_dim=128,\n               nb_filters = 50,\n               FFN_units=512,\n               nb_classes=2,\n               dropout_rate=0.1,\n               training=False,\n               name=\"dcnn\"):\n    super(DCNN, self).__init__(name=name)\n\n    self.embedding = layers.Embedding(vocab_size, emb_dim)\n\n    self.bigram = layers.Conv1D(filters = nb_filters,\n                                kernel_size = 2,\n                                padding='valid',\n                                activation='relu')\n    self.trigram = layers.Conv1D(filters = nb_filters,\n                                kernel_size = 3,\n                                padding='valid',\n                                activation='relu')\n    self.fourgram = layers.Conv1D(filters = nb_filters,\n                                kernel_size = 4,\n                                padding='valid',\n                                activation='relu')\n    \n    self.pool = layers.GlobalMaxPool1D()\n\n    self.dense_1 = layers.Dense(units = FFN_units, activation='relu')\n    self.dropout = layers.Dropout(rate=dropout_rate)\n    if nb_classes == 2:\n      self.last_dense = layers.Dense(units=1, activation='sigmoid')\n    else:\n      self.last_dense = layers.Dense(units=nb_classes, activation='softmax')\n\n  def call(self, inputs, training):\n    x = self.embedding(inputs)\n    x_1 = self.bigram(x)\n    x_1 = self.pool(x_1)\n    x_2 = self.trigram(x)\n    x_2 = self.pool(x_2)\n    x_3 = self.fourgram(x)\n    x_3 = self.pool(x_3)\n\n    merged = tf.concat([x_1, x_2, x_3], axis = -1)\n    merged = self.dense_1(merged)\n    merged = self.dropout(merged, training)\n    output = self.last_dense(merged)\n\n    return output","9e4e6eb8":"VOCAB_SIZE = len(tokenizer.vocab)\nEMB_DIM = 200\nNB_FILTERS = 100\nFFN_UNITS = 256\nNB_CLASSES = 2\nDROPOUT_RATE = 0.2\nNB_EPOCHS = 4","90d69744":"Dcnn = DCNN(vocab_size=VOCAB_SIZE,\n            emb_dim=EMB_DIM,\n            nb_filters = NB_FILTERS,\n            FFN_units = FFN_UNITS,\n            nb_classes = NB_CLASSES,\n            dropout_rate = DROPOUT_RATE)","bc34b113":"if NB_CLASSES == 2:\n  Dcnn.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\nelse:\n  Dcnn.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['sparse_categorical_accuracy'])","b4981dc0":"history = Dcnn.fit(train_dataset,\n                   epochs=NB_EPOCHS)                ","404618fe":"history.history.keys()","6a47bd29":"plt.plot(history.history['loss'])\nplt.title('Loss progress');","4d43da46":"plt.plot(history.history['accuracy'])\nplt.title('Accuracy progress');","113ec73a":"results = Dcnn.evaluate(test_dataset)\nprint(results)","f3ea293d":"def get_prediction(sentence):\n  tokens = encode_sentence(sentence)\n  inputs = tf.expand_dims(tokens, 0) \n\n  output = Dcnn(inputs, training=False)\n\n  sentiment = math.floor(output*2)\n\n  if sentiment == 0:\n    print('negative')\n  elif sentiment == 1:\n    print('positive')","693228a4":"get_prediction(\" not do that again\")","bc438708":"get_prediction('This a big problem')","fc50cc79":"# Training","e9c43798":"# Evaluation","a1102f73":"# Database Creation","d295752c":"It is a well treated database, however small, unfortunately the algorithm did not adapt very well to this database.\n\nThanks!","8f453ba3":"# Model Building","1e6b9e16":"# Importing Libraries","26c5f7ab":"# If you find this notebook useful, support with an upvote \ud83d\udc4d","20578bf6":"# Tokenization","d99ff896":"# Cleaning the Texts","9f2c0811":"#  Analyzing the Data"}}