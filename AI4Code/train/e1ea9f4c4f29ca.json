{"cell_type":{"d7501bef":"code","8139354a":"code","4526784d":"code","c0283c0b":"code","afcd0cee":"code","c4b02ee5":"code","4bdbb272":"code","287fc581":"code","9c1d9fe8":"code","4a69ba96":"code","38d6858b":"code","7eddff0e":"code","5aa1ab70":"code","8e03b175":"code","bea99d3c":"code","2a2e5868":"code","c18c1605":"code","df09b017":"code","995b71b4":"code","3be47b97":"code","6246ada8":"code","36f7ddab":"code","dd7abe6a":"code","f77655f5":"code","6072d11a":"code","f00386d0":"code","744ce8c3":"code","e6875d94":"code","8f99e4c5":"code","8219fd3e":"markdown","973b497c":"markdown","81f020b2":"markdown","6555b858":"markdown","be5df4cd":"markdown","875be2e1":"markdown","c678edea":"markdown"},"source":{"d7501bef":"import pandas as pd\nimport numpy as np\n%matplotlib inline\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nimport re\nfrom collections import Counter\nfrom wordcloud import WordCloud, STOPWORDS\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import roc_curve, confusion_matrix, auc, accuracy_score\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.ensemble import RandomForestClassifier\nimport xgboost as xgb\nimport pickle\nimport cv2\nimport tensorflow as tf\nimport tensorflow_hub as hub\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.callbacks import *\nfrom tensorflow.keras import Model, Input, Sequential\nfrom datetime import datetime\nfrom tensorflow.keras import Input, Model\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras.utils import plot_model\nfrom tqdm import tqdm","8139354a":"tf.__version__, xgb.__version__, cv2.__version__, hub.__version__","4526784d":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","c0283c0b":"test = pd.read_json(\"\/kaggle\/input\/news-headlines-dataset-for-sarcasm-detection\/Sarcasm_Headlines_Dataset.json\", lines=True)\ntest.head()","afcd0cee":"test.info()","c4b02ee5":"train = pd.read_json(\"\/kaggle\/input\/news-headlines-dataset-for-sarcasm-detection\/Sarcasm_Headlines_Dataset_v2.json\", lines=True)\ntrain.head()","4bdbb272":"train.info()","287fc581":"plt.figure()\nsns.countplot(data = train, x = \"is_sarcastic\")\nplt.title(\"Class distribution\")\nplt.show()","9c1d9fe8":"def length(phrase):\n  return len(phrase.split())","4a69ba96":"train[\"length\"] = train[\"headline\"].apply(length)\ntrain.head()","38d6858b":"plt.figure()\nsns.displot(data = train, x = \"length\", kde = True)\nplt.title(\"distribution of number of words in headlines\")\nplt.show()","7eddff0e":"for i in [0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100]:\n  print(\"{0}th percentile is {1}\".format(i, np.percentile(train[\"length\"], i)))\nprint()\nfor i in [90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100]:\n  print(\"{0}th percentile is {1}\".format(i, np.percentile(train[\"length\"], i)))\nprint()\nfor i in [99, 99.10, 99.20, 99.30, 99.40, 99.50, 99.60, 99.70, 99.80, 99.90]:\n  print(\"{0}th percentile is {1}\".format(i, np.percentile(train[\"length\"], i)))\nprint()","5aa1ab70":"# Reference: https:\/\/stackoverflow.com\/a\/47091490\/6645883\n\ndef decontracted(phrase):\n    # specific\n    phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n\n    # general\n    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n    #phrase = re.sub(r\"[^A-Za-z0-9 ]+\", \"\", phrase)\n    return phrase.lower()","8e03b175":"train[\"headline\"] = train[\"headline\"].apply(decontracted)\ntest[\"headline\"] = test[\"headline\"].apply(decontracted)","bea99d3c":"# Reference: # https:\/\/www.geeksforgeeks.org\/generating-word-cloud-python\/\n\ndef wordcloud_plot(df):\n  comment_words = \"\"\n  stopwords = set(STOPWORDS)\n\n  # iterate through the csv file\n  for val in df.headline:\n    \n    # typecaste each val to string\n    val = str(val)\n\n    # split the value\n    tokens = val.split()\n    \n    # Converts each token into lowercase\n    for i in range(len(tokens)):\n      tokens[i] = tokens[i].lower()\n    \n    comment_words += \" \".join(tokens)+\" \"\n\n  wordcloud = WordCloud(width = 800, height = 800,\n          background_color = \"white\",\n          stopwords = stopwords,\n          min_font_size = 10).generate(comment_words)\n\n  # plot the WordCloud image\t\t\t\t\t\n  plt.figure(figsize = (8, 8), facecolor = None)\n  plt.imshow(wordcloud)\n  plt.axis(\"off\")\n  plt.tight_layout(pad = 0)\n  plt.show()","2a2e5868":"wordcloud_plot(train)","c18c1605":"wordcloud_plot(test)","df09b017":"y_train = train[\"is_sarcastic\"]\ny_test = test[\"is_sarcastic\"]","995b71b4":"!wget https:\/\/github.com\/nagi1995\/sarcastic-comment-detection\/raw\/main\/glove_vectors","3be47b97":"with open(\".\/glove_vectors\", \"rb\") as fi:\n  glove_model = pickle.load(fi)\n  glove_words = set(glove_model.keys())","6246ada8":"t = Tokenizer()\nt.fit_on_texts(train[\"headline\"])\n\nencoded_train = t.texts_to_sequences(train[\"headline\"])\nencoded_test = t.texts_to_sequences(test[\"headline\"])\n\nmax_length = 25\n\npadded_train = pad_sequences(encoded_train, \n                             maxlen = max_length, \n                             padding = \"post\", \n                             truncating = \"post\")\n\npadded_test = pad_sequences(encoded_test, \n                            maxlen = max_length, \n                            padding = \"post\", \n                            truncating = \"post\")\n\nprint(padded_train.shape, padded_test.shape, type(padded_train))\n\nvocab_size = len(t.word_index) + 1\nvocab_size","36f7ddab":"embedding_matrix = np.zeros((vocab_size, 300)) # vector len of each word is 300\n\nfor word, i in t.word_index.items():\n  if word in glove_words:\n    vec = glove_model[word]\n    embedding_matrix[i] = vec\n\nembedding_matrix.shape","dd7abe6a":"%load_ext tensorboard","f77655f5":"def checkpoint_path():\n  return \".\/model\/weights.{epoch:02d}-{val_accuracy:.4f}.hdf5\"\n\ndef log_dir():\n  return \".\/logs\/fit\/\" + datetime.now().strftime(\"%Y-%m-%d-%H:%M:%S\")\n\nearlystop = EarlyStopping(monitor = \"val_accuracy\", \n                          patience = 7, \n                          verbose = 1,  \n                          restore_best_weights = True, \n                          mode = 'max')\n\nreduce_lr = ReduceLROnPlateau(monitor = \"val_accuracy\", \n                              factor = .4642,\n                              patience = 3,\n                              verbose = 1, \n                              min_delta = 0.001,\n                              mode = 'max')\n","6072d11a":"tf.keras.backend.clear_session()\ninput = Input(shape = (max_length, ), name = \"input\")\n\nembedding = Embedding(input_dim = vocab_size, \n                      output_dim = 300, # glove vector size\n                      weights = [embedding_matrix], \n                      trainable = False)(input)\n\nlstm = LSTM(32)(embedding)\nflatten = Flatten()(lstm)\n\ndense = Dense(16, activation = None, \n              kernel_initializer = \"he_uniform\")(flatten)\n\ndropout = Dropout(.25)(dense)\nactivation = Activation(\"relu\")(dropout)\noutput = Dense(2, activation = \"softmax\", name = \"output\")(activation)\nmodel = Model(inputs = input, outputs = output)\n\nmodel.compile(optimizer = \"adam\", loss = \"sparse_categorical_crossentropy\", metrics = [\"accuracy\"])\n\nplot_model(model, to_file = \".\/model.png\", show_shapes = True)\n\nmodel.summary()","f00386d0":"plt.figure(figsize = (10, 20))\nimage = cv2.imread(\".\/model.png\")\nimage = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\nplt.imshow(image, cmap = \"gray\")\nplt.show()","744ce8c3":"tensorboard_callback = TensorBoard(log_dir = log_dir(), \n                                   histogram_freq = 1, \n                                   write_images = True)\n\ncheckpoint = ModelCheckpoint(filepath = checkpoint_path(), \n                             monitor='val_accuracy', \n                             verbose = 1, \n                             save_best_only = True, \n                             mode = \"max\")\n\ncallbacks_list = [checkpoint, earlystop, reduce_lr]\n\nhistory = model.fit(padded_train, y_train, \n                    validation_data = (padded_test, y_test), \n                    epochs = 30, \n                    batch_size = 32, \n                    callbacks = callbacks_list)","e6875d94":"plt.figure()\nL = len(history.history[\"loss\"]) + 1\nplt.plot(range(1, L), history.history[\"loss\"], \"bo-\", label = \"loss\")\nplt.plot(range(1, L), history.history[\"accuracy\"], \"g*-\", label = \"accuracy\")\nplt.plot(range(1, L), history.history[\"val_loss\"], \"y^-\", label = \"val_loss\")\nplt.plot(range(1, L), history.history[\"val_accuracy\"], \"ro-\", label = \"val_accuracy\")\nplt.legend()\nplt.xlabel(\"epoch\")\nplt.grid()\nplt.show()","8f99e4c5":"y_pred_softmax = model.predict(padded_test)\ny_pred = []\nfor i in range(len(y_pred_softmax)):\n  if  y_pred_softmax[i][0] >= 0.5:\n    y_pred.append(0)\n  else:\n    y_pred.append(1)\n\n\nprint(\"Accuracy:\", 100*accuracy_score(y_test, y_pred))\ncm = confusion_matrix(y_test, y_pred)\nsns.heatmap(cm, annot = True, fmt = \"d\")\nplt.xlabel(\"predicted label\")\nplt.ylabel(\"actual label\")\nplt.title(\"test confusion matrix\")\nplt.show()","8219fd3e":"# Load data","973b497c":"# Import Libraries","81f020b2":"# Deep learning","6555b858":"### model building","be5df4cd":"### training model","875be2e1":"### testing model","c678edea":"### callbacks"}}