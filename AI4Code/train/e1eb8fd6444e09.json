{"cell_type":{"37c7c652":"code","b0754979":"code","bdafbf92":"code","f16307eb":"code","87dfe299":"code","8e6a5dc6":"code","f2de7045":"code","7e0372d0":"code","ec51c183":"code","8c78e9f2":"code","41954706":"code","e7f5ccc0":"code","b378a854":"code","54f58c35":"code","47dc76de":"markdown","6718b843":"markdown","f2f0cafe":"markdown","92bdc2e5":"markdown","9de6ae18":"markdown","82738860":"markdown","9b9d5d4e":"markdown","39056394":"markdown","ac62c59e":"markdown","a60646a2":"markdown","e2ec9bbc":"markdown","c2e8b50a":"markdown","b30f8d5c":"markdown","2ff8c337":"markdown"},"source":{"37c7c652":"import numpy as np\nfrom tqdm import tqdm_notebook as tqdm\n\nimport numpy as np\n\n# Base code : https:\/\/yamalab.tistory.com\/92\nclass MatrixFactorization():\n    def __init__(self, R, k, learning_rate, reg_param, epochs, verbose=False):\n        \"\"\"\n        :param R: rating matrix\n        :param k: latent parameter\n        :param learning_rate: alpha on weight update\n        :param reg_param: beta on weight update\n        :param epochs: training epochs\n        :param verbose: print status\n        \"\"\"\n        self._R = R\n        self._num_users, self._num_items = R.shape\n        self._k = k\n        self._learning_rate = learning_rate\n        self._reg_param = reg_param\n        self._epochs = epochs\n        self._verbose = verbose\n\n\n    def fit(self):\n        \"\"\"\n        training Matrix Factorization : Update matrix latent weight and bias\n\n        \ucc38\uace0: self._b\uc5d0 \ub300\ud55c \uc124\uba85\n        - global bias: input R\uc5d0\uc11c \ud3c9\uac00\uac00 \ub9e4\uaca8\uc9c4 rating\uc758 \ud3c9\uade0\uac12\uc744 global bias\ub85c \uc0ac\uc6a9\n        - \uc815\uaddc\ud654 \uae30\ub2a5. \ucd5c\uc885 rating\uc5d0 \uc74c\uc218\uac00 \ub4e4\uc5b4\uac00\ub294 \uac83 \ub300\uc2e0 latent feature\uc5d0 \uc74c\uc218\uac00 \ud3ec\ud568\ub418\ub3c4\ub85d \ud574\uc90c.\n\n        :return: training_process\n        \"\"\"\n\n        # init latent features\n        self._P = np.random.normal(size=(self._num_users, self._k))\n        self._Q = np.random.normal(size=(self._num_items, self._k))\n\n        # init biases\n        self._b_P = np.zeros(self._num_users)\n        self._b_Q = np.zeros(self._num_items)\n        self._b = np.mean(self._R[np.where(self._R != 0)])\n\n        # train while epochs\n        self._training_process = []\n        for epoch in range(self._epochs):\n            # rating\uc774 \uc874\uc7ac\ud558\ub294 index\ub97c \uae30\uc900\uc73c\ub85c training\n            xi, yi = self._R.nonzero()\n            for i, j in zip(xi, yi):\n                self.gradient_descent(i, j, self._R[i, j])\n            cost = self.cost()\n            self._training_process.append((epoch, cost))\n\n            # print status\n            if self._verbose == True and ((epoch + 1) % 10 == 0):\n                print(\"Iteration: %d ; cost = %.4f\" % (epoch + 1, cost))\n\n\n    def cost(self):\n        \"\"\"\n        compute root mean square error\n        :return: rmse cost\n        \"\"\"\n\n        # xi, yi: R[xi, yi]\ub294 nonzero\uc778 value\ub97c \uc758\ubbf8\ud55c\ub2e4.\n        # \ucc38\uace0: http:\/\/codepractice.tistory.com\/90\n        xi, yi = self._R.nonzero()\n        # predicted = self.get_complete_matrix()\n        cost = 0\n        for x, y in zip(xi, yi):\n            cost += pow(self._R[x, y] - self.get_prediction(x, y), 2)\n        return np.sqrt(cost\/len(xi))\n\n\n    def gradient(self, error, i, j):\n        \"\"\"\n        gradient of latent feature for GD\n\n        :param error: rating - prediction error\n        :param i: user index\n        :param j: item index\n        :return: gradient of latent feature tuple\n        \"\"\"\n\n        dp = (error * self._Q[j, :]) - (self._reg_param * self._P[i, :])\n        dq = (error * self._P[i, :]) - (self._reg_param * self._Q[j, :])\n        return dp, dq\n\n\n    def gradient_descent(self, i, j, rating):\n        \"\"\"\n        graident descent function\n\n        :param i: user index of matrix\n        :param j: item index of matrix\n        :param rating: rating of (i,j)\n        \"\"\"\n\n        # get error\n        prediction = self.get_prediction(i, j)\n        error = rating - prediction\n\n        # update biases\n        self._b_P[i] += self._learning_rate * (error - self._reg_param * self._b_P[i])\n        self._b_Q[j] += self._learning_rate * (error - self._reg_param * self._b_Q[j])\n\n        # update latent feature\n        dp, dq = self.gradient(error, i, j)\n        self._P[i, :] += self._learning_rate * dp\n        self._Q[j, :] += self._learning_rate * dq\n\n\n    def get_prediction(self, i, j):\n        \"\"\"\n        get predicted rating: user_i, item_j\n        :return: prediction of r_ij\n        \"\"\"\n        return self._b + self._b_P[i] + self._b_Q[j] + self._P[i, :].dot(self._Q[j, :].T)\n\n\n    def get_complete_matrix(self):\n        \"\"\"\n        computer complete matrix PXQ + P.bias + Q.bias + global bias\n\n        - PXQ \ud589\ub82c\uc5d0 b_P[:, np.newaxis]\ub97c \ub354\ud558\ub294 \uac83\uc740 \uac01 \uc5f4\ub9c8\ub2e4 bias\ub97c \ub354\ud574\uc8fc\ub294 \uac83\n        - b_Q[np.newaxis:, ]\ub97c \ub354\ud558\ub294 \uac83\uc740 \uac01 \ud589\ub9c8\ub2e4 bias\ub97c \ub354\ud574\uc8fc\ub294 \uac83\n        - b\ub97c \ub354\ud558\ub294 \uac83\uc740 \uac01 element\ub9c8\ub2e4 bias\ub97c \ub354\ud574\uc8fc\ub294 \uac83\n\n        - newaxis: \ucc28\uc6d0\uc744 \ucd94\uac00\ud574\uc90c. 1\ucc28\uc6d0\uc778 Latent\ub4e4\ub85c 2\ucc28\uc6d0\uc758 R\uc5d0 \ud589\/\uc5f4 \ub2e8\uc704 \uc5f0\uc0b0\uc744 \ud574\uc8fc\uae30\uc704\ud574 \ucc28\uc6d0\uc744 \ucd94\uac00\ud558\ub294 \uac83.\n\n        :return: complete matrix R^\n        \"\"\"\n        return self._b + self._b_P[:, np.newaxis] + self._b_Q[np.newaxis:, ] + self._P.dot(self._Q.T)\n\n\n\n# run example\nif __name__ == \"__main__\":\n    # rating matrix - User X Item : (7 X 5)\n    R = np.array([\n        [1, 0, 0, 1, 3],\n        [2, 0, 3, 1, 1],\n        [1, 2, 0, 5, 0],\n        [1, 0, 0, 4, 4],\n        [2, 1, 5, 4, 0],\n        [5, 1, 5, 4, 0],\n        [0, 0, 0, 1, 0],\n    ])\n\n    # P, Q is (7 X k), (k X 5) matrix","b0754979":"%%time\nfactorizer = MatrixFactorization(R, k=3, learning_rate=0.01, reg_param=0.01, epochs=100, verbose=True)\nfactorizer.fit()","bdafbf92":"factorizer.get_complete_matrix()","f16307eb":"class MatrixFactorization():\n    def __init__(self, R, k, learning_rate, reg_param, epochs, verbose=False):\n        \"\"\"\n        :param R: rating matrix\n        :param k: latent parameter\n        :param learning_rate: alpha on weight update\n        :param reg_param: beta on weight update\n        :param epochs: training epochs\n        :param verbose: print status\n        \"\"\"\n        self._R = R\n        self._num_users, self._num_items = R.shape\n        self._k = k\n        self._learning_rate = learning_rate\n        self._reg_param = reg_param\n        self._epochs = epochs\n        self._verbose = verbose","87dfe299":"%%time\n\nR = np.array([\n    [1, 0, 0, 1, 3],\n    [2, 0, 3, 1, 1],\n    [1, 2, 0, 5, 0],\n    [1, 0, 0, 4, 4],\n    [2, 1, 5, 4, 0],\n    [5, 1, 5, 4, 0],\n    [0, 0, 0, 1, 0],\n])\n\nfactorizer = MatrixFactorization(R, k=3, learning_rate=0.01, reg_param=0.01, epochs=100, verbose=True)\nfactorizer.fit()","8e6a5dc6":"def fit(self):\n    \"\"\"\n    training Matrix Factorization : Update matrix latent weight and bias\n\n    \ucc38\uace0: self._b\uc5d0 \ub300\ud55c \uc124\uba85\n    - global bias: input R\uc5d0\uc11c \ud3c9\uac00\uac00 \ub9e4\uaca8\uc9c4 rating\uc758 \ud3c9\uade0\uac12\uc744 global bias\ub85c \uc0ac\uc6a9\n    - \uc815\uaddc\ud654 \uae30\ub2a5. \ucd5c\uc885 rating\uc5d0 \uc74c\uc218\uac00 \ub4e4\uc5b4\uac00\ub294 \uac83 \ub300\uc2e0 latent feature\uc5d0 \uc74c\uc218\uac00 \ud3ec\ud568\ub418\ub3c4\ub85d \ud574\uc90c.\n\n    :return: training_process\n    \"\"\"\n\n    # init latent features\n    self._P = np.random.normal(size=(self._num_users, self._k))\n    self._Q = np.random.normal(size=(self._num_items, self._k))\n\n    # init biases\n    self._b_P = np.zeros(self._num_users)\n    self._b_Q = np.zeros(self._num_items)\n    self._b = np.mean(self._R[np.where(self._R != 0)])\n\n    # train while epochs\n    self._training_process = []\n    for epoch in range(self._epochs):\n        # rating\uc774 \uc874\uc7ac\ud558\ub294 index\ub97c \uae30\uc900\uc73c\ub85c training\n        xi, yi = self._R.nonzero()\n        for i, j in zip(xi, yi):\n            self.gradient_descent(i, j, self._R[i, j])\n        cost = self.cost()\n        self._training_process.append((epoch, cost))\n\n        # print status\n        if self._verbose == True and ((epoch + 1) % 10 == 0):\n            print(\"Iteration: %d ; cost = %.4f\" % (epoch + 1, cost))","f2de7045":"# init latent features\nself._P = np.random.normal(size=(self._num_users, self._k))\nself._Q = np.random.normal(size=(self._num_items, self._k))\n\n# init biases\nself._b_P = np.zeros(self._num_users)\nself._b_Q = np.zeros(self._num_items)\nself._b = np.mean(self._R[np.where(self._R != 0)])","7e0372d0":"# train while epochs\nself._training_process = []\nfor epoch in range(self._epochs):\n    # rating\uc774 \uc874\uc7ac\ud558\ub294 index\ub97c \uae30\uc900\uc73c\ub85c training\n    xi, yi = self._R.nonzero()\n    for i, j in zip(xi, yi):\n        self.gradient_descent(i, j, self._R[i, j])\n    cost = self.cost()\n    self._training_process.append((epoch, cost))\n\n    # print status\n    if self._verbose == True and ((epoch + 1) % 10 == 0):\n        print(\"Iteration: %d ; cost = %.4f\" % (epoch + 1, cost))","ec51c183":"for i, j in zip(xi, yi):\n    self.gradient_descent(i, j, self._R[i, j])","8c78e9f2":"def gradient_descent(self, i, j, rating):\n    \"\"\"\n    graident descent function\n\n    :param i: user index of matrix\n    :param j: item index of matrix\n    :param rating: rating of (i,j)\n    \"\"\"\n\n    # get error\n    prediction = self.get_prediction(i, j)\n    error = rating - prediction\n\n    # update biases\n    self._b_P[i] += self._learning_rate * (error - self._reg_param * self._b_P[i])\n    self._b_Q[j] += self._learning_rate * (error - self._reg_param * self._b_Q[j])\n\n    # update latent feature\n    dp, dq = self.gradient(error, i, j)\n    self._P[i, :] += self._learning_rate * dp\n    self._Q[j, :] += self._learning_rate * dq","41954706":"def get_prediction(self, i, j):\n    \"\"\"\n    get predicted rating: user_i, item_j\n    :return: prediction of r_ij\n    \"\"\"\n    return self._b + self._b_P[i] + self._b_Q[j] + self._P[i, :].dot(self._Q[j, :].T)","e7f5ccc0":"# update biases\nself._b_P[i] += self._learning_rate * (error - self._reg_param * self._b_P[i])\nself._b_Q[j] += self._learning_rate * (error - self._reg_param * self._b_Q[j])\n\n# update latent feature\ndp, dq = self.gradient(error, i, j)\nself._P[i, :] += self._learning_rate * dp\nself._Q[j, :] += self._learning_rate * dq","b378a854":"def cost(self):\n    \"\"\"\n    compute root mean square error\n    :return: rmse cost\n    \"\"\"\n\n    # xi, yi: R[xi, yi]\ub294 nonzero\uc778 value\ub97c \uc758\ubbf8\ud55c\ub2e4.\n    # \ucc38\uace0: http:\/\/codepractice.tistory.com\/90\n    xi, yi = self._R.nonzero()\n    # predicted = self.get_complete_matrix()\n    cost = 0\n    for x, y in zip(xi, yi):\n        cost += pow(self._R[x, y] - self.get_prediction(x, y), 2)\n    return np.sqrt(cost\/len(xi))","54f58c35":"factorizer.get_complete_matrix()","47dc76de":"- self._training_process = [] \ub294 for\ubb38 \uc548\uc758 self._training_process.append((epoch, cost)) \uc5d0 \uc0ac\uc6a9\ub418\ub294 \ubd80\ubd84\uc73c\ub85c \ud559\uc2b5 \uc2dc\uc5d0 Epoch\uc640 Cost\ub97c \uc800\uc7a5\ud558\ub294 \ubd80\ubd84\uc785\ub2c8\ub2e4. for\ubb38\uc758 \uacbd\uc6b0 \ucc98\uc74c \ud30c\ub77c\ubbf8\ud130\ub85c \ubc1b\uc740 self._epochs \ub9cc\ud07c \ubc18\ubcf5\ud559\uc2b5\uc744 \uc9c4\ud589\ud558\uac8c \ub429\ub2c8\ub2e4.\n\n\n- \uba3c\uc800 \uc2dc\ud589\ub418\ub294 for\ubb38\uc758 \uccab \ubc88\uc9f8\ub85c \uc2dc\ud589\ub418\ub294 xi, yi = self._R.nonzero()\ub294 \ud3c9\uc810 \ud589\ub82c\uc5d0\uc11c 0\uc774 \uc544\ub2cc \ubd80\ubd84 \uc989, \uc0ac\uc6a9\uc790\uac00 \ud3c9\uc810\uc744 \ub9e4\uae34 \ubd80\ubd84\uc5d0 \ub300\ud574\uc11c\ub9cc \uac12\uc744 \ucd94\ucd9c\ud558\ub77c\ub294 \uc758\ubbf8\uc785\ub2c8\ub2e4. \uadf8 \uc774\uc720\ub294 \uc704\uc758 \uc774\ub860\uc5d0\uc11c \ub9d0\ud588\ub4ef\uc774 \uacb0\uce21\uce58\uac00 \uc544\ub2cc \ubd80\ubd84\uc744 \ud1b5\ud574\uc11c\ub9cc \ud559\uc2b5\uc744 \uc9c4\ud589\ud558\ub824\ub294 \uc758\ub3c4\uc785\ub2c8\ub2e4. \uc774\ud6c4\uc5d0 \ud574\ub2f9 \ubd80\ubd84\uc744 \ud1b5\ud574\uc11c \ubaa8\ub4e0 \ud3c9\uc810 \ubd80\ubd84\uc5d0 \ub300\ud574\uc11c gradient_descent\ub97c \uc2e4\ud589\ud574\uc90d\ub2c8\ub2e4.","6718b843":"\uae30\ubcf8\uc801\uc778 \uad6c\uc870\ub294 \uc704\uc640 \uac19\uc2b5\ub2c8\ub2e4. \ud55c\ubc88 __init__ \ubd80\ud130 \ucc28\uadfc\ucc28\uadfc \ucf54\ub4dc\uc758 \uc124\uba85\uc744 \uc2dc\uc791\ud558\uaca0\uc2b5\ub2c8\ub2e4.","f2f0cafe":"\ucc38\uace0\ub85c \uc774\ub97c \uc870\uae08 \uc218\uc815\ud574\uc11c PyTorch\ub97c \uc774\uc6a9\ud55c \ucf54\ub4dc\ub294 \ub2e4\uc74c\uc758 \ub9c1\ud06c\uc5d0\uc11c \ud655\uc778\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \ub2e4\uc74c \ud3ec\uc2a4\ud305\uc5d0\uc11c\ub294 ALS\uac00 \ubb34\uc5c7\uc778\uc9c0\uc5d0\uc11c\ubd80\ud130 \uc5b4\ub5bb\uac8c \uc0ac\uc6a9\ud558\ub294\uc9c0\uc5d0 \ub300\ud574 \uc54c\uc544\ubcf4\ub3c4\ub85d \ud558\uaca0\uc2b5\ub2c8\ub2e4.","92bdc2e5":"![](https:\/\/drive.google.com\/uc?export=view&id=1Nlhx0ilvzD4Vhxnj4WPrhnJNRFkRfBPf)","9de6ae18":"\uc774\ub54c, np.random.normal \ud568\uc218\ub97c \uc774\uc6a9\ud574\uc11c \ud589\ub82c\uc744 \ucd08\uae30\ud654\ud574\uc8fc\uace0 np.zeros \ud568\uc218\ub97c \uc774\uc6a9\ud574\uc11c bias \ubd80\ubd84\uc744 \ucd08\uae30\ud654\ud574\uc90d\ub2c8\ub2e4. \ud568\uc218\uc758 \uc758\ubbf8\ub294 \uc544\ub798\uc640 \uac19\uc2b5\ub2c8\ub2e4.\n\n- np.random.normal : (self._num_users, self._k)\uc758 \ud06c\uae30\ub85c \ud589\ub82c\uc744 \uc815\uaddc\ubd84\ud3ec \ud615\ud0dc\ub85c \ucd08\uae30\ud654\ud569\ub2c8\ub2e4. \uc704\uc758 \uc608\uc2dc\uc5d0\uc11c\ub294 User Latent Matrix\ub294 (7, 3)\uc758 \ud06c\uae30\ub97c Item Latent Matrix\ub294 (5, 3)\uc758 \ud06c\uae30\ub97c \uac00\uc9d0\n- np.zeros : self._num_users \ud639\uc740 self._num_items\uc758 \ud06c\uae30\ub9cc\ud07c\uc758 0 \uac12\uc744 \uac00\uc9c0\ub294 \ubca1\ud130\ub97c \uc0dd\uc131\ud569\ub2c8\ub2e4.\n- np.mean(self._R[np.where(self._R != 0)]) : \uc804\uccb4 \ud3c9\uc810\uc758 \ud3c9\uade0\uc744 \uacc4\uc0b0\n\n\uc774\ud6c4, \uc804\uccb4 \ud559\uc2b5 \uacfc\uc815\uc744 \uc9c4\ud589\ud569\ub2c8\ub2e4.","82738860":"fit \ud568\uc218\uc5d0\uc11c \uac00\uc7a5 \uba3c\uc800 \uc2e4\ud589\ub418\ub294 \ubd80\ubd84\uc740 \uc544\ub798\uc758 Latent Matrix\ub97c \ucd08\uae30\ud654\ud574\uc8fc\ub294 \ubd80\ubd84\uc785\ub2c8\ub2e4.","9b9d5d4e":"\uc704\uc758 \uc2e4\ud589\uc740 7\uac1c\uc758 User, 5\uac1c\uc758 Item\uc5d0 \ub300\ud55c \ud3c9\uc810 \ud589\ub82c(R)\uc5d0 \ub300\ud574\uc11c k=3, learning_rate=0.01, reg_param=0.01, epochs=100, verbose=True \uc640 \uac19\uc740 \ud30c\ub77c\ubbf8\ud130\ub97c \uac00\uc9c0\ub294 MatrixFactorization \uac1d\uccb4\ub97c \uc0dd\uc131\ud558\ub77c\ub294 \uc758\ubbf8\uc785\ub2c8\ub2e4. \uc0dd\uc131\ub41c \uac1d\uccb4\uc5d0\uc11c factorizer.fit()\uc744 \ud1b5\ud574\uc11c fit() \ud568\uc218\ub97c \uc2e4\ud589\ud558\uba74, \uc544\ub798\uc758 \ucf54\ub4dc\uac00 \uc2e4\ud589\ub418\uac8c \ub429\ub2c8\ub2e4.","39056394":"\uc774\ud6c4\uc5d0, error = rating - prediction \uc744 \ud1b5\ud574\uc11c \uc5bc\ub9c8\ub9cc\ud07c\uc758 \ucc28\uc774\uac00 \uc788\ub294\uc9c0 \uacc4\uc0b0\uc744 \ud569\ub2c8\ub2e4. \uadf8\ub9ac\uace0 \uc544\ub798\uc758 \uc218\uc2dd\uc5d0\uc11c \uc0ac\uc6a9\ud55c \uacf5\uc2dd\uc744 \uc774\uc6a9\ud574\uc11c self.gradient(error, i, j) \uc5d0\uc11c Gradient \uac12\uc744 \uacc4\uc0b0\ud558\uace0 Weight \ubc0f Bais\ub97c \uc5c5\ub370\uc774\ud2b8\ud569\ub2c8\ub2e4.","ac62c59e":"\uc77c\ub2e8, __init__ \ud568\uc218\ub294 Matrix Factorization\uc774\ub77c\ub294 \ud074\ub798\uc2a4\uac00 \ud638\ucd9c\ub420 \ub54c \uc790\ub3d9\uc73c\ub85c \uc2e4\ud589\ub418\ub294 \ubd80\ubd84\uc785\ub2c8\ub2e4. \ud30c\ub77c\ubbf8\ud130\ub85c\ub294 6\uac1c\uc758 \uc778\uc790\ub97c \ubc1b\ub294\ub370 \uac01\uac01 \uc544\ub798\uc758 \uc758\ubbf8\ub97c \uac00\uc9d1\ub2c8\ub2e4.\n\n- R: \ud3c9\uc810 \ud589\ub82c\n- k: User Latent\uc640 Item Latent\uc758 \ucc28\uc6d0\uc758 \uc218\n- learning_rate: \ud559\uc2b5\ub960\n- reg_param: Weight\uc758 Regularization \uac12\n- epochs: \uc804\uccb4 \ud559\uc2b5 \ud69f\uc218 (Total Epoch)\n- verbose: \ud559\uc2b5 \uacfc\uc815\uc744 \ucd9c\ub825\ud560\uc9c0 \uc5ec\ubd80 (True : 10\ubc88\ub9c8\ub2e4 cost \ucd9c\ub825, False : cost\ub97c \ucd9c\ub825\ud558\uc9c0 \uc54a\uc74c)","a60646a2":"- gradient_descent \ud568\uc218\uc758 \uacbd\uc6b0 \ud589\ub82c\uc758 \uc6d0\uc18c \uc704\uce58(i, j)\uc640 \ud3c9\uc810 \uac12(self._R[i, j])\uc744 \ubc1b\uc2b5\ub2c8\ub2e4. \ubc14\ub85c \uc2dc\uc791\ud558\ub294 prediction = self.get_prediction(i, j) \uc740 User Latent Matrix\uc640 Item Latent Matrix\uc758 \uacf1\uc744 \ud1b5\ud574\uc11c \ud3c9\uc810 \ud589\ub82c\uc758 \uac12\ub4e4\uc744 \uc0dd\uc131\ud558\ub294 \ubd80\ubd84\uc785\ub2c8\ub2e4.\n\n\n- self._P[i, :].dot(self._Q[j, :].T) \uc5d0\uc11c User Latent P\uc640 Item Latent Q\uac00 \uacf1\ud574\uc838\uc11c \ud3c9\uc810\uc744 \uacc4\uc0b0\ud558\uace0 Bias\ub97c \uc5c6\uc560\uae30 \uc704\ud574\uc11c \uc804\uccb4 \ud3c9\uade0(self._b), User\uc758 \ud3c9\uade0 \ud3c9\uc810(self._b_P[i]), Item\uc758 \ud3c9\uade0 \ud3c9\uc810(self._b_Q[j])\uc744 \ub354\ud574\uc90c\uc73c\ub85c\uc368 \uac12\uc744 \uc0dd\uc131\ud569\ub2c8\ub2e4.","e2ec9bbc":"## \ucf54\ub4dc\uc758 Step by Step \uc124\uba85","c2e8b50a":"\uc774\ud6c4\uc5d0, cost = self.cost() \uc5d0\uc11c \uc804\uccb4 Matrix\uc5d0 \ub300\ud574\uc11c \uc624\ucc28\ub97c \uacc4\uc0b0\ud558\uace0 \ucd9c\ub825\ud574\uc90d\ub2c8\ub2e4. cost += pow(self._R[x, y] - self.get_prediction(x, y), 2) \uc5d0\uc11c \uac01 \ud3c9\uc810 \ubcc4\ub85c \uc624\ucc28\ub97c \uacc4\uc0b0\ud558\uac8c \ub429\ub2c8\ub2e4.","b30f8d5c":"# Base code : https:\/\/yamalab.tistory.com\/92","2ff8c337":"\uc774\ud6c4, \ubaa8\ub4e0 Epoch\uc5d0 \ub300\ud574\uc11c \ud559\uc2b5\uc774 \uc644\ub8cc\ub418\uba74 factorizer.get_complete_matrix() \ub97c \ud1b5\ud574\uc11c \uc644\uc131\ub41c \ud3c9\uc810 \ud589\ub82c\uc744 \ucd94\ucd9c\ud558\uac8c \ub418\uba74 SGD\ub97c \uc774\uc6a9\ud55c \ud611\uc5c5 \ud544\ud130\ub9c1\uc774 \uc644\ub8cc\ub418\uac8c \ub429\ub2c8\ub2e4."}}