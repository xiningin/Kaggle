{"cell_type":{"6da8070c":"code","4129b203":"code","2ad3cc8c":"code","aeb0968d":"code","789ada94":"code","a55279fa":"code","c275033c":"code","c96554fc":"code","1607b337":"code","8232d205":"code","e6334adf":"code","14e160a4":"code","e6b78a54":"code","848a93f7":"code","591c99ab":"code","54e57f23":"code","9e4e885c":"code","39ee9733":"code","151b15c9":"code","1472591e":"code","b92133f3":"code","b456fe3d":"code","eaede740":"code","2178c3be":"code","97200498":"code","0f9c55b0":"code","c794f2d1":"code","ac6333c0":"code","691666ce":"code","9f3b5b03":"code","ef6aadbd":"markdown","3bffe011":"markdown","7f0db95b":"markdown","06f984a9":"markdown","0fa62ca7":"markdown","5d95f8cc":"markdown","9d21f10b":"markdown","1bc67985":"markdown","1077ccc3":"markdown","2cf03ca1":"markdown","3f1d8aa0":"markdown","25be7ca3":"markdown","b8383d45":"markdown","15ff20c4":"markdown","fd789187":"markdown","8ad7371d":"markdown","674441cb":"markdown"},"source":{"6da8070c":"!pip install seaborn --upgrade","4129b203":"import numpy as np\nimport pandas as pd\nimport seaborn as sb\nimport matplotlib.pyplot as plt","2ad3cc8c":"data = pd.read_csv('..\/input\/loan-prediction-problem-dataset\/train_u6lujuX_CVtuZ9i.csv')","aeb0968d":"data.head()","789ada94":"data.describe()","a55279fa":"data.info()","c275033c":"sb.countplot(data=data, x='Gender')","c96554fc":"sb.countplot(data=data, x='Married')","1607b337":"sb.countplot(data=data, x='Dependents')","8232d205":"sb.countplot(data=data, x='Self_Employed')","e6334adf":"sb.displot(data=data, x='ApplicantIncome', binwidth=2000)","14e160a4":"sb.displot(data=data, x='LoanAmount', binwidth=20)","e6b78a54":"sb.countplot(data=data, x='Loan_Amount_Term')","848a93f7":"sb.countplot(data=data, x='Credit_History')","591c99ab":"data.drop('Loan_ID', axis=1, inplace=True)","54e57f23":"data.isna().sum().sort_values(ascending=False)","9e4e885c":"data.fillna({\n    'Credit_History': data.Credit_History.mode()[0],\n    'Self_Employed': data.Dependents.mode()[0],\n    'LoanAmount': data.LoanAmount.median(),\n    'Dependents': data.Dependents.mode()[0],\n    'Loan_Amount_Term': data.Loan_Amount_Term.mode()[0],\n    'Gender': data.Gender.mode()[0],\n    'Married': data.Married.mode()[0]\n}, inplace=True)","39ee9733":"from sklearn.preprocessing import LabelEncoder\n\ngender_le = LabelEncoder()\ndata.Gender = gender_le.fit_transform(data.Gender)\nmarried_le = LabelEncoder()\ndata.Married = married_le.fit_transform(data.Married)\ndependents_le = LabelEncoder()\ndata.Dependents = dependents_le.fit_transform(data.Dependents)\neducation_le = LabelEncoder()\ndata.Education = education_le.fit_transform(data.Education)\nself_employed_le = LabelEncoder()\ndata.Self_Employed = self_employed_le.fit_transform(data.Self_Employed)\nproperty_area_le = LabelEncoder()\ndata.Property_Area = property_area_le.fit_transform(data.Property_Area)\nloan_status_le = LabelEncoder()\ndata.Loan_Status = loan_status_le.fit_transform(data.Loan_Status)","151b15c9":"from sklearn.model_selection import train_test_split\n\nX = data.drop('Loan_Status', axis=1)\ny = data.Loan_Status\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","1472591e":"from sklearn.tree import DecisionTreeClassifier\n\ndt_cls = DecisionTreeClassifier(criterion='entropy', random_state=42)\ndt_cls.fit(X_train, y_train)","b92133f3":"print(\"Decision tree depth :\", dt_cls.get_depth())\nprint(\"Decision tree number of leaves :\", dt_cls.get_n_leaves())","b456fe3d":"from sklearn.metrics import accuracy_score, f1_score\n\ny_train_pred = dt_cls.predict(X_train)\ny_test_pred = dt_cls.predict(X_test)\n\ndt_acc = accuracy_score(y_test, y_test_pred)\ndt_f1 = f1_score(y_test, y_test_pred)\nprint(\"Decision tree cls accuracy on test set : {} (on train set : {})\".format(dt_acc, accuracy_score(y_train, y_train_pred)))\nprint(\"Decision tree cls F1 score on test set : {} (on train set : {})\".format(dt_f1, f1_score(y_train, y_train_pred)))","eaede740":"from sklearn.model_selection import GridSearchCV\n\ndt_estimator = DecisionTreeClassifier(random_state=42)\ngrid_search_params = {\n    'criterion': ['gini', 'entropy'],\n    'splitter': ['best', 'random'],\n    'max_depth': range(1, 10)\n}\ncv_dt_model = GridSearchCV(estimator=dt_estimator, param_grid=grid_search_params)\ncv_dt_model.fit(X_train, y_train)","2178c3be":"print(\"Best depth :\", cv_dt_model.best_estimator_.get_depth())\nprint(\"Best number of leaves :\", cv_dt_model.best_estimator_.get_n_leaves())\nprint(\"Best params :\", cv_dt_model.best_estimator_.get_params())","97200498":"y_train_pred = cv_dt_model.predict(X_train)\ny_test_pred = cv_dt_model.predict(X_test)\n\ncv_dt_acc = accuracy_score(y_test, y_test_pred)\ncv_dt_f1 = f1_score(y_test, y_test_pred)\nprint(\"Optimized decision tree cls accuracy on test set : {} (on train set : {})\".format(cv_dt_acc, accuracy_score(y_train, y_train_pred)))\nprint(\"Optimized decision tree cls F1 score on test set : {} (on train set : {})\".format(cv_dt_f1, f1_score(y_train, y_train_pred)))","0f9c55b0":"from sklearn.ensemble import RandomForestClassifier\n\nrf_cls = RandomForestClassifier(criterion='entropy', random_state=42)\nrf_cls.fit(X_train, y_train)","c794f2d1":"y_train_pred = rf_cls.predict(X_train)\ny_test_pred = rf_cls.predict(X_test)\n\nrf_acc = accuracy_score(y_test, y_test_pred)\nrf_f1 = f1_score(y_test, y_test_pred)\nprint(\"Random Forest cls accuracy on test set : {} (on train set : {})\".format(rf_acc, accuracy_score(y_train, y_train_pred)))\nprint(\"Random Forest cls F1 score on test set : {} (on train set : {})\".format(rf_f1, f1_score(y_train, y_train_pred)))","ac6333c0":"feature_importances = pd.DataFrame({\n    'feature': X_train.columns,\n    'decision tree': dt_cls.feature_importances_,\n    'optimized decision tree': cv_dt_model.best_estimator_.feature_importances_,\n    'random forest': rf_cls.feature_importances_\n})\n\nfeature_importances_melted = feature_importances.melt(id_vars='feature', value_vars=feature_importances.columns.values[1:])\nfeature_importances_melted.sort_values(by='value', ascending=False, inplace=True)\n\nsb.barplot(data=feature_importances_melted, y='feature', x='value', hue='variable')","691666ce":"fig, ax = plt.subplots(figsize=(10,8))\nsb.heatmap(data.corr(), annot=True, fmt=\".2f\", linewidths=0.1, ax=ax)","9f3b5b03":"from tabulate import tabulate\n\nprint(tabulate([\n    ['Decision tree', round(dt_acc, 2), round(dt_f1, 2)],\n    ['Optimized DT', round(cv_dt_acc, 2), round(cv_dt_f1, 2)],\n    ['Random forest', round(rf_acc, 2), round(rf_f1, 2)]],\n    headers=['Model', 'Accuracy', 'F1 score']))","ef6aadbd":"## Handling missing values","3bffe011":"# EDA","7f0db95b":"### Correlation matrix\n\nCorrelation matrices allow us to check for correlation between our features and help us foresee such outcomes.  \nInspecting correlation matrices is usually part of EDA and it was my mistake not to do it in the first place.","06f984a9":"### Decision tree : hyperparameters tuning with CV","0fa62ca7":"#### Results\n\nSo our optimized decision tree does indeed perform better on the test set than our previous decision tree indicating we may have reduced overfitting.  \n**But we notice that cross-validation led to a weird discovery : our best tree has only a depth of 1 and 2 leaves !**","5d95f8cc":"### Random Forest","9d21f10b":"## Encoding","1bc67985":"Our optimized decision tree revolving solely around the credit history feature ends up achieving the best performance.","1077ccc3":"### Decision Tree","2cf03ca1":"As expected, our target variable Loan_Status has a high (>.5) positive Pearson correlation coefficient with Credit_History.","3f1d8aa0":"# Conclusion\n\n#### Final results (on test set)","25be7ca3":"#### Results\n\nWe notice a perfect fit on the training set with a pretty big drop in performance on the test set indicating we are probably overfitting the training set.  \nThat is a common issue with decision trees mainly solved by fine-tuning the hyperparameters of our decision tree or switching to random forest.","b8383d45":"### Explainability : feature importances\n\nLet's try to understand how our models made their predictions, especially our optimized decision tree with only 2 leaves.","15ff20c4":"As we could have guessed, *that* is the reason why our optimized decision tree has only 2 leaves : it makes all its predictions around **one single feature** !\\\nInterestingly enough, this feature (the credit history of the applicant) is also the most important one for our random forest but not for our simple decision tree.","fd789187":"#### Results\n\nWe're once again fitting the training set perfectly but our evaluation metrics improved compared to our simple decision tree.","8ad7371d":"# Modeling","674441cb":"# Preprocessing\n\n## Cleaning"}}