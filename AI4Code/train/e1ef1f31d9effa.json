{"cell_type":{"71145591":"code","43670386":"code","ea391eda":"code","5e57a2da":"code","75155d87":"code","86f01ff9":"code","28c9eed7":"code","24c7d903":"code","5efa5094":"code","0ce7e910":"code","3a9c39de":"code","f5df1d7d":"code","fbea20be":"code","6c3a9d46":"code","2e4de85d":"code","116cab3a":"code","975d405f":"code","454de735":"code","c16d40b5":"code","099814da":"code","40db97ad":"code","438e20bb":"code","38fa64c0":"markdown","6367ce37":"markdown","d9a23d40":"markdown","dc0e71ae":"markdown","f44db7a3":"markdown","662decd5":"markdown","28b2550e":"markdown","a30c9b65":"markdown","d2e87a19":"markdown","2c4c9c80":"markdown","4882db51":"markdown","2edf861b":"markdown","fbe6f757":"markdown","0f8273da":"markdown","4917369f":"markdown","e8f4dd3e":"markdown","ff7439df":"markdown","c1a6e4a0":"markdown","7cfd7a47":"markdown","3a1af5e8":"markdown","82ece384":"markdown","3e86d419":"markdown","f852f7fb":"markdown","a7a06c27":"markdown","06612051":"markdown","3235ca7c":"markdown","0c70c47d":"markdown","785ee887":"markdown","c228649a":"markdown","6dc8de43":"markdown","c59376eb":"markdown","d27a3c91":"markdown","dc3f0e63":"markdown","4e9a5cf9":"markdown"},"source":{"71145591":"import os\nimport sys\nimport time\nimport glob\nfrom pathlib import Path\n\nimport pandas as pd\nimport numpy as np\n\n# Parallel processing\nfrom joblib import Parallel\nfrom joblib import delayed\n\n# Preprocess\nfrom sklearn import preprocessing\nfrom sklearn import model_selection\n\n# Evaluation\nfrom sklearn.metrics import r2_score\n\n# Visullize\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Modeling\nimport lightgbm as lgb\n\n# Others\nimport warnings\nwarnings.simplefilter(\"ignore\")\n","43670386":"# Dataset path\ndata_path = Path('..\/input\/optiver-realized-volatility-prediction')\n\n# setting display option\npd.options.display.max_columns = 50","ea391eda":"# Objective variable\ntarget = 'target'\n\n# submission file setting\nsubmit_file = 'submission.csv'\nId_column = 'row_id'","5e57a2da":"#\u3000Log Return\ndef log_return(list_stock_prices):\n    return np.log(list_stock_prices).diff() \n\n# Realized Volatility\ndef realized_volatility(series_log_return):\n    return np.sqrt(np.sum(series_log_return**2))","75155d87":"# WAP calculation\ndef wap_calculation1(df):\n    return (df['bid_price1'] * df['ask_size1'] + df['ask_price1'] * df['bid_size1']) \/ (df['bid_size1'] + df['ask_size1'])\n\ndef wap_calculation2(df):\n    return (df['bid_price2'] * df['ask_size2'] + df['ask_price2'] * df['bid_size2']) \/ (df['bid_size2'] + df['ask_size2'])","86f01ff9":"# RMSPE\ndef rmspe(y_true, y_pred):\n    return  (np.sqrt(np.mean(np.square((y_true - y_pred) \/ y_true))))","28c9eed7":"def book_preprocessing(stock_id : int, data_type = 'train'):\n    # read data\n    df = pd.read_parquet(data_path \/ f'book_{data_type}.parquet\/stock_id={stock_id}\/')\n    \n    # set stock_id\n    df['stock_id'] = stock_id\n    \n    # WAP calculation\n    df['wap1'] = wap_calculation1(df)\n    df['wap2'] = wap_calculation2(df)\n    \n    # log return calculation\n    df['log_return1'] = df.groupby(['time_id'])['wap1'].apply(log_return).fillna(0)\n    df['log_return2'] = df.groupby(['time_id'])['wap2'].apply(log_return).fillna(0)    \n    # Log_return calculation each stock_id and time_id\n    df_realized_vol_per_stock = pd.DataFrame(df.groupby(['stock_id','time_id'])[['log_return1','log_return2']].agg(realized_volatility)).reset_index()\n    \n    return df_realized_vol_per_stock","24c7d903":"df_book = book_preprocessing(97, 'train')\ndf_book.head()","5efa5094":"def trade_preprocessing(stock_id : int, data_type = 'train'):\n    # read data\n    df = pd.read_parquet(data_path \/ f'trade_{data_type}.parquet\/stock_id={stock_id}\/')\n    \n    df = df.sort_values(by=['time_id', 'seconds_in_bucket']).reset_index(drop=True)\n    \n    # set stock_id\n    df['stock_id'] = stock_id\n    \n    # log return calculation\n    df['trade_log_return1'] = df.groupby(by = ['time_id'])['price'].apply(log_return).fillna(0)\n    \n    # Log_return calculation each stock_id and time_id\n    df = pd.DataFrame(df.groupby(['stock_id','time_id'])[['trade_log_return1']].agg(realized_volatility).reset_index())\n    \n    return df","0ce7e910":"df_trade = trade_preprocessing(0,'train')\ndf_trade.head()","3a9c39de":"def get_stock_stat(stock_id : int, data_type = 'train'):\n    \n    # parquet data processing\n    book_stat = book_preprocessing(stock_id, data_type)\n    trade_stat = trade_preprocessing(stock_id, data_type)\n    \n    #Merge book and trade features\n    stock_stat = book_stat.merge(trade_stat, on=['stock_id', 'time_id'], how='left').fillna(-999)\n    \n    return stock_stat","f5df1d7d":"def get_dataSet(stock_ids : list, data_type = 'train'):\n    # Parallel process of get_stock_stat \n    stock_stat = Parallel(n_jobs=-1)(\n        delayed(get_stock_stat)(stock_id, data_type) \n        for stock_id in stock_ids\n    )\n    # concat several stock_stats in vertical direction, axis=0(default)\n    stock_stat_df = pd.concat(stock_stat, ignore_index = True)\n\n    return stock_stat_df","fbea20be":"train=pd.read_csv(data_path \/ 'train.csv')\ntrain['row_id'] = train['stock_id'].astype(str) + '-' + train['time_id'].astype(str)\ndisplay(train.head())\nprint('train data shape:', train.shape)","6c3a9d46":"train_stock_stat_df = get_dataSet(stock_ids = train['stock_id'].unique(), data_type = 'train')\n\n# Merge train with train_stock_stat_df\ntrain = pd.merge(train, train_stock_stat_df, on = ['stock_id', 'time_id'], how = 'left')\nprint(f'Train shape: {train.shape}')\ndisplay(train.head(5))","2e4de85d":"test = pd.read_csv(data_path \/'test.csv')\ntest['row_id'] = test['stock_id'].astype(str) + '-' + test['time_id'].astype(str)\ndisplay(test.head())\nprint('test data shape:', test.shape)","116cab3a":"test_stock_stat_df = get_dataSet(stock_ids = test['stock_id'].unique(), data_type = 'test')\ntest = pd.merge(test, test_stock_stat_df, on = ['stock_id', 'time_id'], how = 'left').fillna(0)\nprint(f'Test shape: {test.shape}')\ndisplay(test.head(5))","975d405f":"# Parameters of Light GBM\nparams_lgbm = {\n        'task': 'train',\n        'boosting_type': 'gbdt',\n        'learning_rate': 0.01,\n        'objective': 'regression',\n        'metric': 'None',\n        'max_depth': -1,\n        'n_jobs': -1,\n        'feature_fraction': 0.7,\n        'bagging_fraction': 0.7,\n        'lambda_l2': 1,\n        'verbose': -1\n        #'bagging_freq': 5\n}","454de735":"# Define loss function for lightGBM training\ndef feval_RMSPE(preds, train_data):\n    labels = train_data.get_label()\n    return 'RMSPE', round(rmspe(y_true = labels, y_pred = preds),5), False","c16d40b5":"# training function\ndef light_gbm(X_train, y_train, X_val ,y_val, cats):\n    \n    # Create dataset\n    train_data = lgb.Dataset(X_train, label=y_train, categorical_feature=cats, weight=1\/np.power(y_train,2))\n    val_data = lgb.Dataset(X_val, label=y_val, categorical_feature=cats, weight=1\/np.power(y_val,2))\n    \n    # training\n    model = lgb.train(params_lgbm, \n                      train_data, \n                      n_rounds, \n                      valid_sets=val_data, \n                      feval=feval_RMSPE,\n                      verbose_eval= 250,\n                      early_stopping_rounds=500\n                     )\n    \n    # Prediction w\/ validation data\n    # preds_val = model.predict(train.loc[val_index, features_columns])\n    preds_val = model.predict(X_val)\n\n    # RMSPE calculation\n    score = round(rmspe(y_true = y_val, y_pred = preds_val),5)\n\n    # Prediction w\/ validation data\n    test_preds = model.predict(test[features_columns]).clip(0,1e10)\n    \n    # delete dataset\n    del train_data, val_data\n    \n    return score, model","099814da":"# Categorical data column list\ncats = ['stock_id']\n\nmodel_name = 'lgb1'\npred_name = f'pred_{model_name}'\n\nfeatures_columns = ['stock_id', 'log_return1', 'log_return2', 'trade_log_return1']\nprint(f'Train dataset columns : {len(features_columns)} features')\n\ntrain[pred_name] = 0\ntest[target] = 0\n\n# k-flods Ensemble Training\nn_folds = 4\nn_rounds = 10000\n\nkf = model_selection.KFold(n_splits=n_folds, shuffle=True, random_state=42)\n\n# Initialize scores dict\nscores_folds = {}\n# Initialize value in scores_folds(dict) to record each step in CV\nscores_folds[model_name] = []\n\n# Initial value\ncv_trial = 1\n\n# --- Cross Validation ---\nfor train_index, val_index in kf.split(range(len(train))):\n    \n    print(f'CV trial : {cv_trial} \/{n_folds}')\n    \n    # Divide dataset into train and validation data such as Cross Validation\n    X_train = train.loc[train_index, features_columns]\n    y_train = train.loc[train_index, target].values\n    X_val = train.loc[val_index, features_columns]\n    y_val = train.loc[val_index, target].values\n    \n    # train with Light GBM\n    rmspe_score, model = light_gbm(X_train, y_train, X_val ,y_val, cats)\n    \n    # record score data at each train in CV\n    scores_folds[model_name].append(rmspe_score)\n\n    # Each validation Summary \n    print(f'Fold-{cv_trial} Model-{model_name} RMSPE: {rmspe_score}')\n    print('-'*50)\n    \n    # Prediction w\/ validation data\n    test_preds = model.predict(test[features_columns]).clip(0,1e10)\n\n    test[target] += test_preds\n    cv_trial += 1","40db97ad":"# devide test target score into n_folds due to sum 4 preds value in CV process\ntest[target] = test[target]\/n_folds\n\n# score calculation\nscore = round(rmspe(y_true = train[target].values, y_pred = train[pred_name].values),5)\nprint(f'RMSPE {model_name}: {score} - Folds: {scores_folds[model_name]}')\n\ndisplay(test[[Id_column, target]].head(2))","438e20bb":"test[[Id_column, target]].to_csv(submit_file, index = False)","38fa64c0":"## 5-2. Cross Validation  \n[Link to Agenda](#Agenda)","6367ce37":"## 5-1. Training function1 - Light GBM  \n[Link to Agenda](#Agenda)","d9a23d40":"<a id='1'><\/a>","dc0e71ae":"# 4. Preprocessing dataset  \n[Link to Agenda](#Agenda)","f44db7a3":"<a id='4-4'><\/a>","662decd5":"<a id='5'><\/a>","28b2550e":"Check data content of one sample with book_preprocessing function  \ne.g. stock_id = 97","a30c9b65":"# 5.Training  \n[Link to Agenda](#Agenda)","d2e87a19":"# 1. Import modules  \n[Link to Agenda](#Agenda)","2c4c9c80":"<a id='7'><\/a>","4882db51":"Following function is training with Light GBM function.If you would like to try any other function, you could define another function and call it.","2edf861b":"# 2. Common Settings  \n[Link to Agenda](#Agenda)","fbe6f757":"<a id='4-5'><\/a>","0f8273da":"<a id='6'><\/a>","4917369f":"Check data content of one sample with trade_preprocessing function\ne.g. stock_id = 0","e8f4dd3e":"<a id='3'><\/a>","ff7439df":"<a id='4'><\/a>","c1a6e4a0":"## 4-1. Book parquet data processing  \n[Link to Agenda](#Agenda)","7cfd7a47":"<a id='4-3'><\/a>","3a1af5e8":"This notebook shows simple flow to deep dive into the competition.I appreciate community of kaggle.\nI refered to following notebooks.\n\n(Reference)  \n**Introduction to financial concepts and data**  \nhttps:\/\/www.kaggle.com\/jiashenliu\/introduction-to-financial-concepts-and-data  \n\n**LGB Starter**  \nhttps:\/\/www.kaggle.com\/manels\/lgb-starter\/notebook\n\n<a id='agenda'><\/a>","82ece384":"# 7. Submittion  \n[Link to Agenda](#Agenda)","3e86d419":"## 4-2. Trade parquet data processing  \n[Link to Agenda](#Agenda)","f852f7fb":"## 4-3. Merge book and trade data  \n[Link to Agenda](#Agenda)  \nMerge two data created by preprocessed with book_preprocessing and trade_preprocessing function","a7a06c27":"<a id='4-1'><\/a>","06612051":"# 6. Evaluation  \n[Link to Agenda](#Agenda)","3235ca7c":"# Agenda\n\n[1. Import modules  ](#1)   \n[2. Common settings](#2)  \n[3. Function Definition](#3)  \n[4. Preprocessing  ](#4)  \n [4-1. Book parquet data processing](#4-1)   \n [4-2. Trade parquet data processing](#4-2)    \n [ 4-3. Merge book and trade data ](#4-3)   \n  [4-4. Train data preprocessing ](#4-4)   \n  [4-5. Test data preprocessing](#4-5)  \n[5. Training](#5)   \n  [5-1. Training function1 - Light GBM](#5-1)  \n  [5-2. Cross Validation](#5-2)  \n[6. Evaluation](#6)  \n[7. Prediction](#7)    \n[8. Submission](#8)  ","0c70c47d":"## 4-5. Test data Preprocessing  \n[Link to Agenda](#Agenda)","785ee887":"To make sumbmission file as output","c228649a":"<a id='5-1'><\/a>","6dc8de43":"<a id='5-2'><\/a>","c59376eb":"# 3. Functions Definition  \n[Link to Agenda](#Agenda)","d27a3c91":"<a id='4-2'><\/a>","dc3f0e63":"## 4-4. Train data preprocessing  \n[Link to Agenda](#Agenda)  ","4e9a5cf9":"<a id='2'><\/a>"}}