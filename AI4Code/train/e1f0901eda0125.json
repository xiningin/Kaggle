{"cell_type":{"bc40fef5":"code","82f94f25":"code","df6a5ab7":"code","7e379dc9":"code","facde9d6":"code","e187f390":"code","01925aa7":"code","12ebc0a3":"code","026ed041":"code","baa76671":"code","5dbb6f38":"code","cf20a0c2":"code","d5385a28":"code","8f5781c1":"code","d5d8e80b":"code","9c5217a5":"code","e1bf30e3":"code","2db63c95":"code","d3e00b8a":"code","82bd8369":"code","d33fb257":"code","c84a6ac3":"code","ee546f91":"code","4d34d286":"code","afdfd60a":"code","37c14bf6":"code","5051d9c6":"code","008ba603":"code","a1c1f13e":"code","4bae0dd7":"code","5bf60dfd":"code","221aac79":"code","b89ec489":"code","18ef04a5":"code","db2cf00e":"code","f3f1df61":"code","a9aacb7f":"code","10e775a3":"code","36aa5f7c":"code","49d72e0e":"code","ab57d221":"code","db8ed58c":"code","fcddb0b5":"code","aff04743":"code","ddec0af8":"code","2ae36dd0":"code","4e26cd0f":"code","d3c42b91":"code","ba6d8e16":"code","56be27ae":"code","78b79339":"code","664891f9":"code","b9bb2af1":"code","1d12aff8":"code","b79f45f1":"code","e2e04a02":"code","6880a041":"code","bf9191cd":"code","2fb010c4":"code","a220eee5":"code","7508a25f":"code","86d1f16a":"code","718a6fb3":"code","0d6d1449":"code","cbf4153a":"code","23946e06":"code","e5ed8af9":"code","0b9c7547":"code","f5851862":"markdown"},"source":{"bc40fef5":"#######################################################################\n##### Trabajo - Examen Modulo Supervised Learning ############\n#######################################################################","82f94f25":"#Importar las librer\u00edas necesarias en Python.\nimport pandas as pd ## Manejo de dataframes o set de datos\n%matplotlib inline\nimport matplotlib.pyplot as plt2 ## Todo lo referente a gr\u00e1ficas y dem\u00e1s.\nimport matplotlib as plt\nimport numpy as np ## Todo lo referente a trabajar con vectores y matrices\nfrom scipy import stats ## Herramientas y algoritmos matem\u00e1ticos para python\nimport seaborn as sns # Se basa en Matplotlib y la complementa en el tema de graficos y dem\u00e1s.\n\n\nimport xgboost as xgb\nfrom numpy import array\nfrom sklearn.model_selection import train_test_split\n#from sklearn.metrics import confusion_matrix\n#from sklearn.metrics import accuracy_score\n#from sklearn.metrics import classification_report\nfrom urllib.request import urlretrieve","df6a5ab7":"#COMO ENCONTRAR SUS RUTAS\nimport os\nos.getcwd()","7e379dc9":"#Leer el dataset en un dataframe usando pandas\ndata = pd.read_csv(\"..\/input\/datacov\/Data_cov.csv\", sep=';' , encoding='latin-1') ","facde9d6":"data.head()","e187f390":"# Revisar los valores perdidos\n#Creamos una funci\u00f3n:\ndef num_missing(x):\n  return sum(x.isnull())\n\n#Aplicamos por columna:\nprint (\"Valores perdidos por columna\")\nprint (data.apply(num_missing, axis=0)) # Axis = 0 es vertical , axis =1 horizontal.","01925aa7":"#no existen missing values\n#por lo tanto no es necesario aplicar imputacion","12ebc0a3":"#######################################################################\n##### Analisis EDA ########################\n#######################################################################","026ed041":"#conociendo los tipos de datos\ndata.dtypes","baa76671":"np.unique(data['Sector'])","5dbb6f38":"np.unique(data['Dep'])","cf20a0c2":"np.unique(data['Sector de Impacto'])","d5385a28":"np.unique(data['Turismo'])","8f5781c1":"np.unique(data['Deuda rezagada vr'])","d5d8e80b":"def fdistribucion(base,decimales):\n    cont = 0\n    for i in lista_variables:\n        cont = cont + 1\n        print(\"------------------------------------------------------\")\n        print(str(cont),\". Var: \",i, sep = \"\")\n        print(pd.concat([pd.DataFrame(base[:][i].value_counts(dropna = False).index, columns = ['Atributo']),\n               pd.DataFrame(base[:][i].value_counts(dropna = False).values, columns = ['Cantidad']),\n               pd.DataFrame(np.round(100*base[:][i].value_counts(dropna = False).values\/len(base),decimales), columns = ['%Total'])], axis = 1))","9c5217a5":"lista_variables = ['Sector','Dep','Impactado','Turismo','Sector de Impacto']\nfdistribucion(data,1)","e1bf30e3":"cols= [\"Saldo\",\"Venta \",\"Saldo Promedio\",\"Deuda Sistea\",\"Pasivo\"]\ndata[cols].plot.box(figsize=(20,10))","2db63c95":"cols= [\"Pasivo\",\"Venta Formal\",\"Venta Formal\",\"Pasivo_desembolso\"]\ndata[cols].plot.box(figsize=(20,10))","d3e00b8a":"cols= [\"Deuda rezagada va\",\"pasivo rezagada va\",\"pasivo rezagada vr\",\"Pasivo_desembolso da\",\"Pasivo_desembolso dr\"]\ndata[cols].plot.box(figsize=(20,10))","82bd8369":"from sklearn.feature_selection import SelectKBest\n# FEATURE SELECTION\nX=data.drop(['Impactado'], axis=1)\nX=X.drop(['Sector'], axis=1)\nX=X.drop(['Dep'], axis=1)\ny=data['Impactado']","d33fb257":"best=SelectKBest(k=5)\nX_new = best.fit_transform(X, y)\nX_new.shape\nselected = best.get_support(indices=True)\nprint(X.columns[selected])","c84a6ac3":"from matplotlib import pyplot as plt\nimport seaborn as sb\nused_features =X.columns[selected]\n\ncolormap = plt.cm.viridis\nplt.figure(figsize=(7,7))\nplt.title('Pearson Correlation of Features', y=1.02, size=5)\nsb.heatmap(data[used_features].astype(float).corr(),linewidths=0.1,vmax=1.0, square=True, cmap=colormap, linecolor='white', annot=True)","ee546f91":"#############################################################\n#################### FEATURE ENGINEERING ####################\n#############################################################","4d34d286":"    plt.hist(data['Saldo'],bins=4)","afdfd60a":"    plt.hist(data['Saldo Promedio'],bins=4, rwidth =45)","37c14bf6":"data['rango_saldo']= data['Saldo'].apply(lambda x: 1 if x<1000000 \n                                         else (2 if x<2000000 \n                                               else (3 if x<3000000 \n                                                     else (4 if x<4000000 else 5))))","5051d9c6":"data['rango_saldo']= data['Saldo'].apply(lambda x: 1 if x<1000000 \n                                         else (2 if x<2000000 \n                                               else (3 if x<3000000 \n                                                     else (4 if x<4000000 else 5))))","008ba603":"print ( '\\n min ', data['Saldo Promedio'].min(),\n       '\\n max ',data['Saldo Promedio'].max()\n      )","a1c1f13e":"data['rango Saldo Promedio']= data['Saldo Promedio'].apply(lambda x: 1 if x<400000 \n                                         else (2 if x<800000 \n                                               else (3 if x<1200000 \n                                                     else (4 if x<1700000 else 5))))","4bae0dd7":"data['ratio saldo']=data['Saldo Promedio']\/ data['Saldo']\n#data['ratio deuda venta formal']=data['Deuda Sistea']\/ data['Venta Formal']\ndata['ratio deuda venta']=data['Deuda Sistea']\/ data['Venta ']\ndata['ratio deuda rezagada va ']=data['Deuda rezagada va']\/ data['Venta ']\ndata['ratio deuda rezagada vr ']=data['Deuda rezagada vr']\/ data['Venta ']\ndata['ratio pasivo  rezagada va ']=data['pasivo rezagada va']\/ data['Venta ']\ndata['ratio pasivo  rezagada vr ']=data['pasivo rezagada vr']\/ data['Venta ']","5bf60dfd":"#data=data.replace([np.inf, -np.inf], np.nan).dropna(subset=[\"ratio deuda venta formal\"], how=\"all\")","221aac79":"#Aplicamos por columna:\nprint (\"Valores perdidos por columna\")\nprint (data.apply(num_missing, axis=0)) # Axis = 0 es vertical , axis =1 horizontal.","b89ec489":"data.head()","18ef04a5":"\ndf_dum1 = pd.get_dummies(data[\"Turismo\"], prefix = 'Turismo')\ndf_dum2 = pd.get_dummies(data[\"Sector de Impacto\"], prefix = 'Sector_Imp')\ndf_dum3 = pd.get_dummies(data[\"Sector\"], prefix = 'Sector')\n#df_dum4 = pd.get_dummies(data[\"Dep\"], prefix = 'Dep')\n\ndata2=pd.concat([data,df_dum1,df_dum2,df_dum3],axis = 1)\ndel data2[\"Turismo\"]\ndel data2[\"Sector\"]\ndel data2[\"Sector de Impacto\"]\n","db2cf00e":"data2.head()","f3f1df61":"# Creamos un copia del dataset original para poder realizar las imputaciones\n# Imputacion del Data completo\ntrain_parametrica=data2.copy()\n# Le quitamos las variable Dep  para poder modelar la informaci\u00f3n \ntrain_parametrica = train_parametrica.drop('Dep', axis=1)\n#train_parametrica = train_parametrica.drop('Saldo', axis=1)\n#train_parametrica = train_parametrica.drop('Saldo Promedio', axis=1)\ntrain_parametrica.head(5)\n","a9aacb7f":"from sklearn.preprocessing import StandardScaler","10e775a3":"cols= train_parametrica.columns\nprint(cols)","36aa5f7c":"def fscaler(base):\n    cont = 0\n    for i in cols:\n        cont = cont + 1\n        print(str(cont),\". Var procesada: \",i, sep = \"\")\n        pd.DataFrame(StandardScaler().fit_transform(train_parametrica[[i]]))","49d72e0e":"fscaler(train_parametrica)","ab57d221":"# Creaci\u00f3n de la data de train y la data de test\n#from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(\n    train_parametrica.drop('Impactado', axis=1), train_parametrica['Impactado'], test_size=0.30)","db8ed58c":"#######################################################################\n##### Modelo de Regresi\u00f3n Log\u00edstica ###################################\n#######################################################################","fcddb0b5":"import statsmodels.api as sm\nlogit_model=sm.Logit(y_train,X_train)\n\n#Current function value: 0.641368","aff04743":"result=logit_model.fit()","ddec0af8":"result.summary2()","2ae36dd0":"## Paso N\u00b0 02 : Predecir sobre la data de test con el modelo entrenado\ny_pred=result.predict(X_test)\ny_pred2=np.where(y_pred<0.5,0,1)","4e26cd0f":"## Paso N\u00b0 03 : Evaluar indicadores\nfrom sklearn.metrics import (precision_score, recall_score,f1_score,accuracy_score,roc_auc_score,confusion_matrix)\n\nprint(\"\\tAccuracy: %1.3f\\n\" % accuracy_score(y_test, y_pred2))\nprint(\"\\tPrecision: %1.3f\" % precision_score(y_test, y_pred2))\nprint(\"\\tRecall: %1.3f\" % recall_score(y_test, y_pred2))\nprint(\"\\tROC AUC: %1.3f\" % roc_auc_score(y_test, y_pred2))\nprint(\"\\tF1: %1.3f\" % f1_score(y_test, y_pred2))\n\n#Accuracy: 0.583\n\n#\tPrecision: 0.571\n#\tRecall: 0.656\n#\tF1: 0.611","d3c42b91":"## Paso N\u00b0 03 : Evaluar indicadores\nfrom sklearn.metrics import classification_report\nprint(classification_report(y_test, y_pred2))","ba6d8e16":"#######################################################################\n##### Modelo de Regresi\u00f3n Log\u00edstica Penalizada ########################\n#######################################################################","56be27ae":"## Paso N\u00b0 01 : Entrenar un modelo\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import (precision_score, recall_score,f1_score,accuracy_score)\n\nlr = LogisticRegression(random_state=17, class_weight='balanced') \nlr.fit(X_train, y_train)","78b79339":"## Paso N\u00b0 02 : Predecir sobre la data de test con el modelo entrenado\ny_pred=lr.predict(X_test)","664891f9":"## Paso N\u00b0 03 : Evaluar indicadores\nprint(\"\\tAccuracy: %1.3f\\n\" % accuracy_score(y_test, y_pred))\nprint(\"\\tPrecision: %1.3f\" % precision_score(y_test, y_pred))\nprint(\"\\tRecall: %1.3f\" % recall_score(y_test, y_pred))\nprint(\"\\tROC AUC: %1.3f\" % roc_auc_score(y_test, y_pred))\nprint(\"\\tF1: %1.3f\" % f1_score(y_test, y_pred))\n\n#\tAccuracy: 0.610\n\n#\tPrecision: 0.596\n#\tRecall: 0.676\n#\tF1: 0.633","b9bb2af1":"## Paso N\u00b0 03 : Evaluar indicadores\n\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import roc_curve\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import (precision_score, recall_score,f1_score,accuracy_score)\n#from matplotlib import pyplot as plt\nlr = LogisticRegression(random_state=17, class_weight='balanced') \n#lr = LogisticRegression(penalty='l2')# Regresion Logistica Ridge\nlr.fit(X_train, y_train)\n\nlogit_roc_auc = roc_auc_score(y_test, lr.predict(X_test))\nfpr, tpr, thresholds = roc_curve(y_test, lr.predict_proba(X_test)[:,1])\nplt.figure()\nplt.plot(fpr, tpr, label='Logistic Regression (area = %0.2f)' % logit_roc_auc)\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic')\nplt.legend(loc=\"lower right\")\nplt.savefig('Log_ROC')\nplt.show()","1d12aff8":"## Paso N\u00b0 03 : Evaluar indicadores\nfrom sklearn.metrics import classification_report\nprint(classification_report(y_test, y_pred))","b79f45f1":"#################################################################################\n#################### 2DO MODELO #################################################\n#################################################################################","e2e04a02":"\ndata_train, data_test = train_test_split(data2.drop('Dep', axis=1), test_size=.3, random_state=1999)","6880a041":"#Veamos el tama\u00f1o de estos conjuntos de datos con el m\u00e9todo shape.\nprint(data_train.shape)\nprint(data_test.shape)\n","bf9191cd":"\ntrain_mat = xgb.DMatrix(data_train.drop(\"Impactado\", 1), label=data_train[\"Impactado\"])\ntest_mat = xgb.DMatrix(data_test.drop(\"Impactado\", 1), label=data_test[\"Impactado\"])","2fb010c4":"train_mat","a220eee5":"parametros = {\"booster\":\"gbtree\", \"max_depth\": 2, \"eta\": 0.3, \"objective\": \"binary:logistic\", \"nthread\":2}\nrondas = 10","7508a25f":"evaluacion = [(test_mat, \"eval\"), (train_mat, \"train\")]","86d1f16a":"modelo = xgb.train(parametros, test_mat, rondas, evaluacion)","718a6fb3":"modelo","0d6d1449":"prediccion = modelo.predict(test_mat)","cbf4153a":"prediccion","23946e06":"prediccion = [1 if i > .5 else 0 for i in prediccion]\nprediccion[:10]","e5ed8af9":"#precision_score, recall_score,f1_score,accuracy_score,roc_auc_score\ndef metricas(objetivo, prediccion):\n    matriz_conf = confusion_matrix(objetivo, prediccion)\n    score = accuracy_score(objetivo, prediccion)\n    reporte = classification_report(objetivo, prediccion)\n    auc = roc_auc_score(objetivo, prediccion)\n    metricas = [matriz_conf, score, reporte,auc]\n    return(metricas)","0b9c7547":"metricas = metricas(data_test[\"Impactado\"], prediccion)\n[print(i) for i in metricas]","f5851862":"1. La variable a predecir esta balanceda.\n1. 36% pertenece a Lima y un 64% pertenece a provincia"}}