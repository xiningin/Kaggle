{"cell_type":{"d6c74646":"code","8386feed":"code","56adb9ff":"code","d8bb74cc":"code","fcbee472":"code","c0677432":"code","e08db9b7":"code","54f0fba0":"code","bb83dbf2":"code","4a31d8fb":"code","0a1f8e38":"code","db361b7e":"code","23dc0115":"code","dc196a81":"code","e7ede7c2":"code","4e9225d2":"code","259aa390":"code","0ecb313a":"code","d9784014":"code","03d48726":"code","5cfba8c5":"code","e675a812":"code","9b5eee09":"code","e2fa0dc0":"code","17c678de":"code","0457f3f0":"code","0201f641":"code","0ad6fee7":"code","2e5f84a3":"code","33459d2f":"code","26d0d4b8":"code","4b726e8f":"code","1fb026e8":"markdown","f71eaaa1":"markdown","dde33e6d":"markdown","295612b6":"markdown","e37466ce":"markdown","1d456455":"markdown"},"source":{"d6c74646":"# Importing the libraries\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline","8386feed":"# Importing the dataset\ntrain = pd.read_csv('..\/input\/UCI_Credit_Card.csv')","56adb9ff":"train.head()","d8bb74cc":"train.columns","fcbee472":"train.isna().any()","c0677432":"## Histograms\nfig = plt.figure(figsize=(15, 12))\nplt.suptitle('Histograms of Numerical Columns', fontsize=20)\nfor i in range(train.shape[1]):\n    plt.subplot(9, 6, i + 1)\n    f = plt.gca()\n    f.set_title(train.columns.values[i])\n\n    vals = np.size(train.iloc[:, i].unique())\n    if vals >= 100:\n        vals = 100\n    \n    plt.hist(train.iloc[:, i], bins=vals, color='#3F5D7D')\nplt.tight_layout(rect=[0, 0.03, 1, 0.95])","e08db9b7":"train=train.rename(columns = {'default.payment.next.month':'payment_default'})\ntrain.columns","54f0fba0":"#Correlation with Quality with respect to attributes\ntrain.corrwith(train.payment_default).plot.bar(\n        figsize = (20, 10), title = \"Correlation with quality\", fontsize = 15,\n        rot = 45, grid = True)","bb83dbf2":"## Correlation Matrix\nsns.set(style=\"white\")\n\n# Compute the correlation matrix\ncorr = train.corr()","4a31d8fb":"corr.head()","0a1f8e38":"# Generate a mask for the upper triangle\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(18, 15))\n\n# Generate a custom diverging colormap\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\n\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})","db361b7e":"#Assigning and dividing the dataset\nX = train.drop('payment_default',axis=1)\ny=train['payment_default']","23dc0115":"X.head()","dc196a81":"y.head()","e7ede7c2":"train.columns[:-1]","4e9225d2":"features_label = train.columns[:-1]","259aa390":"#Fitting Random Forest Classification to the Training set\nfrom sklearn.ensemble import RandomForestClassifier\nclassifier = RandomForestClassifier(n_estimators = 200, criterion = 'entropy', random_state = 0)\nclassifier.fit(X, y)\nimportances = classifier.feature_importances_\nindices = np. argsort(importances)[::-1]\nfor i in range(X.shape[1]):\n    print (\"%2d) %-*s %f\" % (i + 1, 30, features_label[i],importances[indices[i]]))","0ecb313a":"plt.title('Feature Importances')\nplt.bar(range(X.shape[1]),importances[indices], color=\"green\", align=\"center\")\nplt.xticks(range(X.shape[1]),features_label, rotation=90)\nplt.xlim([-1, X.shape[1]])\nplt.show()","d9784014":"\n# Splitting the dataset into the Training set and Test set\nfrom sklearn.model_selection  import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 5)","03d48726":"# Feature Scaling\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train2 = pd.DataFrame(sc.fit_transform(X_train))\nX_test2 = pd.DataFrame(sc.transform(X_test))\nX_train2.columns = X_train.columns.values\nX_test2.columns = X_test.columns.values\nX_train2.index = X_train.index.values\nX_test2.index = X_test.index.values\nX_train = X_train2\nX_test = X_test2","5cfba8c5":"#Using Principal Dimensional Reduction\nfrom sklearn.decomposition import PCA\npca = PCA(n_components = 2)\nX_train = pca.fit_transform(X_train)\nX_test = pca.transform(X_test)\nexplained_variance = pca.explained_variance_ratio_\nprint(pd.DataFrame(explained_variance))","e675a812":"#### Model Building ####\n\n### Comparing Models\n\n## Logistic Regression\nfrom sklearn.linear_model import LogisticRegression\nclassifier = LogisticRegression(random_state = 0, penalty = 'l1')\nclassifier.fit(X_train, y_train)\n\n# Predicting Test Set\ny_pred = classifier.predict(X_test)\nfrom sklearn.metrics import confusion_matrix, accuracy_score, f1_score, precision_score, recall_score\nacc = accuracy_score(y_test, y_pred)\nprec = precision_score(y_test, y_pred)\nrec = recall_score(y_test, y_pred)\nf1 = f1_score(y_test, y_pred)\n\nresults = pd.DataFrame([['Logistic Regression', acc, prec, rec, f1]],\n               columns = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score'])\nprint(results)","9b5eee09":"## SVM (rbf)\nfrom sklearn.svm import SVC\nclassifier = SVC(random_state = 0, kernel = 'rbf')\nclassifier.fit(X_train, y_train)\n\n# Predicting Test Set\ny_pred = classifier.predict(X_test)\nacc = accuracy_score(y_test, y_pred)\nprec = precision_score(y_test, y_pred)\nrec = recall_score(y_test, y_pred)\nf1 = f1_score(y_test, y_pred)\n\nmodel_results = pd.DataFrame([['SVM (RBF)', acc, prec, rec, f1]],\n               columns = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score'])\n\nresults = results.append(model_results, ignore_index = True)\nprint(results)","e2fa0dc0":"## Randomforest\nfrom sklearn.ensemble import RandomForestClassifier\nclassifier = RandomForestClassifier(random_state = 0, n_estimators = 100,\n                                    criterion = 'entropy')\nclassifier.fit(X_train, y_train)\n\n# Predicting Test Set\ny_pred = classifier.predict(X_test)\nacc = accuracy_score(y_test, y_pred)\nprec = precision_score(y_test, y_pred)\nrec = recall_score(y_test, y_pred)\nf1 = f1_score(y_test, y_pred)\n\nmodel_results = pd.DataFrame([['Random Forest (n=100)', acc, prec, rec, f1]],\n               columns = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score'])\n\nresults = results.append(model_results, ignore_index = True)\nprint(results)","17c678de":"# Fitting GradientBoosting to the Training set\nfrom sklearn.ensemble import GradientBoostingClassifier\nclassifier = GradientBoostingClassifier(loss='deviance', learning_rate=0.1, n_estimators=200, subsample=1.0,\n                                   min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, \n                                   max_depth=3,\n                                   init=None, random_state=None, max_features=None, verbose=0)\nclassifier.fit(X_train, y_train)\n# Predicting Test Set\ny_pred = classifier.predict(X_test)\nacc = accuracy_score(y_test, y_pred)\nprec = precision_score(y_test, y_pred)\nrec = recall_score(y_test, y_pred)\nf1 = f1_score(y_test, y_pred)\n\nmodel_results = pd.DataFrame([['Gradient Boost (n=100)', acc, prec, rec, f1]],\n               columns = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score'])\n\nresults = results.append(model_results, ignore_index = True)\nprint(results)","0457f3f0":"results","0201f641":"## K-fold Cross Validation\nfrom sklearn.model_selection import cross_val_score\naccuracies = cross_val_score(estimator = classifier, X= X_train, y = y_train,\n                             cv = 10)\nprint(\"Random Forest Classifier Accuracy: %0.2f (+\/- %0.2f)\"  % (accuracies.mean(), accuracies.std() * 2))","0ad6fee7":"# Applying Grid Search\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom scipy.stats import randint\n\nparameters = {'loss' : ['deviance', 'exponential'],\n                 'n_estimators': randint(10, 500),\n                 'max_depth': randint(1,10)}\n\nfrom sklearn.model_selection import GridSearchCV\ngrid_search = RandomizedSearchCV(estimator=classifier, param_distributions=parameters, n_iter=10,\n                                   fit_params=None, cv=None, verbose=2)\n\nt0 = time.time()\ngrid_search = grid_search.fit(X_train, y_train)\nt1 = time.time()\nprint(\"Took %0.2f seconds\" % (t1 - t0))\n\nrf_best_accuracy = grid_search.best_score_\nrf_best_parameters = grid_search.best_params_\nrf_best_accuracy, rf_best_parameters","2e5f84a3":"# Predicting Test Set\ny_pred = grid_search.predict(X_test)\nacc = accuracy_score(y_test, y_pred)\nprec = precision_score(y_test, y_pred)\nrec = recall_score(y_test, y_pred)\nf1 = f1_score(y_test, y_pred)\n\nmodel_results = pd.DataFrame([['Random Forest (n=100, GSx2 + Gini)', acc, prec, rec, f1]],\n               columns = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score'])\n\nresults = results.append(model_results, ignore_index = True)\nresults","33459d2f":"import matplotlib.pyplot as plt\nimport itertools\n\nfrom sklearn import svm, datasets\nfrom sklearn.metrics import confusion_matrix\n\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    print(cm)\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.tight_layout()\n","26d0d4b8":"# Making the Confusion Matrix \nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test, y_pred)\nnp.set_printoptions(precision=2)\n\n# Plot normalized confusion matrix\nplot_confusion_matrix(cm,classes=[0,1])\nsns.set(rc={'figure.figsize':(6,6)})\nplt.show()\n","4b726e8f":"#Let's see how our model performed\nfrom sklearn.metrics import classification_report\nprint(classification_report(y_test, y_pred))","1fb026e8":"# Model Tuning and evaluation","f71eaaa1":"# Feature Extraction and selection","dde33e6d":"# Model Training","295612b6":"# Prediction on Test Results","e37466ce":"Dataset Information\nThis dataset contains information on default payments, demographic factors, credit data, history of payment, and bill statements of credit card clients in Taiwan from April 2005 to September 2005.\n\nContent\nThere are 25 variables:\n\nID: ID of each client\nLIMIT_BAL: Amount of given credit in NT dollars (includes individual and family\/supplementary credit\nSEX: Gender (1=male, 2=female)\nEDUCATION: (1=graduate school, 2=university, 3=high school, 4=others, 5=unknown, 6=unknown)\nMARRIAGE: Marital status (1=married, 2=single, 3=others)\nAGE: Age in years\nPAY_0: Repayment status in September, 2005 (-1=pay duly, 1=payment delay for one month, 2=payment delay for two months, ... 8=payment delay for eight months, 9=payment delay for nine months and above)\nPAY_2: Repayment status in August, 2005 (scale same as above)\nPAY_3: Repayment status in July, 2005 (scale same as above)\nPAY_4: Repayment status in June, 2005 (scale same as above)\nPAY_5: Repayment status in May, 2005 (scale same as above)\nPAY_6: Repayment status in April, 2005 (scale same as above)\nBILL_AMT1: Amount of bill statement in September, 2005 (NT dollar)\nBILL_AMT2: Amount of bill statement in August, 2005 (NT dollar)\nBILL_AMT3: Amount of bill statement in July, 2005 (NT dollar)\nBILL_AMT4: Amount of bill statement in June, 2005 (NT dollar)\nBILL_AMT5: Amount of bill statement in May, 2005 (NT dollar)\nBILL_AMT6: Amount of bill statement in April, 2005 (NT dollar)\nPAY_AMT1: Amount of previous payment in September, 2005 (NT dollar)\nPAY_AMT2: Amount of previous payment in August, 2005 (NT dollar)\nPAY_AMT3: Amount of previous payment in July, 2005 (NT dollar)\nPAY_AMT4: Amount of previous payment in June, 2005 (NT dollar)\nPAY_AMT5: Amount of previous payment in May, 2005 (NT dollar)\nPAY_AMT6: Amount of previous payment in April, 2005 (NT dollar)\ndefault.payment.next.month: Default payment (1=yes, 0=no)","1d456455":"# EDA and Visualization"}}