{"cell_type":{"7a135950":"code","8d9a3e88":"code","10c0948f":"code","c2970455":"code","7fb3cd1c":"code","93a50582":"code","04a793c9":"code","f3c7f8ab":"code","69650726":"code","c9fa6ee5":"code","cb7715ce":"code","a0bc655d":"code","49b34b07":"code","4e9eb32a":"code","e3dfc20a":"code","be420bb1":"code","19952e68":"code","32cc1ca3":"code","34bedfbe":"code","cec318c2":"code","7b5b17ca":"code","d801b87e":"code","54f36c08":"code","08a2a3f9":"code","2aa99b9a":"code","4e376623":"code","56a5e9ea":"code","538a3c5a":"code","06a7076e":"code","98e8d18b":"code","02db6264":"code","1374ed4b":"code","16a78ad3":"code","16b71b8f":"code","cff2e6fe":"code","5030641a":"code","3e2862d9":"code","99ec5187":"code","173c2a63":"code","2af2ce62":"code","a0ae667a":"code","5824ac04":"code","a0fa5dd8":"code","835d35bf":"code","deffd702":"code","75178923":"code","e77cd084":"code","ca934fdd":"code","a0cd61ac":"markdown","691736cf":"markdown","384b048a":"markdown","10af475d":"markdown","7df20253":"markdown","2f1904de":"markdown","36f4227a":"markdown","d8b42cf8":"markdown","b582341b":"markdown","d5bdbf76":"markdown","739e6438":"markdown","176ec313":"markdown","a7c122a6":"markdown","71b1d24f":"markdown","f53804bb":"markdown","099dc5a5":"markdown","35dc7ab9":"markdown","7e0f44ab":"markdown","b984308d":"markdown","dea7adf5":"markdown"},"source":{"7a135950":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\n\nimport plotly.express as px","8d9a3e88":"train = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')","10c0948f":"train","c2970455":"test","7fb3cd1c":"fig = px.histogram(train, x='SalePrice', color='OverallQual', marginal=\"rug\")\nfig.show()","93a50582":"fig = px.box(train, x='OverallQual', y='SalePrice', color='OverallQual')\nfig.show()","04a793c9":"print(train[train.columns[1:]].corr()['SalePrice'][:] > 0.60)","f3c7f8ab":"SaleCorr = train[['OverallQual', 'TotalBsmtSF', '1stFlrSF', 'GrLivArea', 'GarageCars', 'GarageArea', \n                    'SalePrice']]\nSaleCorr.corr()['SalePrice']","69650726":"#saleprice correlation matrix\ncorrmat = SaleCorr.corr()\nk = 7 #number of variables for heatmap\ncols = corrmat.nlargest(k, 'SalePrice')['SalePrice'].index\ncm = np.corrcoef(train[cols].values.T)\nsns.set(font_scale=1)\nhm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values)\nplt.show()","c9fa6ee5":"predictors_cols = ['OverallQual', 'TotalBsmtSF', '1stFlrSF', 'GrLivArea', 'GarageCars', 'GarageArea']","cb7715ce":"fig = px.scatter_matrix(SaleCorr,\n    dimensions=predictors_cols,\n    color=\"OverallQual\",\n    title=\"Pairplot high correlation values\",\n    labels={col:col.replace('_', ' ') for col in SaleCorr.columns})\nfig.update_layout(\n    autosize=False,\n    width=900,\n    height=800,\n    font=dict(size=8))\nfig.update_traces(\n    marker={'size':2},\n    diagonal_visible=True)\nfig.show()","a0bc655d":"#Training variables\ntrain_X = train[predictors_cols]\ntrain_y = train.SalePrice\n#Test\ntest_X = test[predictors_cols]","49b34b07":"from sklearn.ensemble import RandomForestRegressor","4e9eb32a":"rf = RandomForestRegressor(n_estimators = 1000, random_state = 42)\nrf.fit(train_X, train_y);","e3dfc20a":"test_X = test_X.fillna(method=\"ffill\")","be420bb1":"preds = rf.predict(test_X)\nprint(preds)","19952e68":"my_submission = pd.DataFrame({'Id': test.Id, 'SalePrice': preds})\n\n#my_submission.to_csv('submission3.csv', index=False)","32cc1ca3":"from sklearn.linear_model import LinearRegression","34bedfbe":"model = LinearRegression()\nmodel.fit(train_X, train_y)\nlinear_pred = model.predict(test_X)","cec318c2":"my_submission = pd.DataFrame({'Id': test.Id, 'SalePrice': linear_pred})\n# you could use any filename. We choose submission here\n#my_submission.to_csv('submission4.csv', index=False)","7b5b17ca":"print(train.isnull().sum().sort_values(ascending=False).head(), \"\\n\")\nprint(test.isnull().sum().sort_values(ascending=False).head())","d801b87e":"for item in train.columns:\n    if train[item].isnull().sum() >= 780:\n        train = train.drop([item], axis=1)\n        \nfor item in test.columns:\n    if test[item].isnull().sum() >= 780:\n        test = test.drop([item], axis=1)","54f36c08":"print(train.isnull().sum().sort_values(ascending=False).head(8), \"\\n\")\nprint(test.isnull().sum().sort_values(ascending=False).head(8))","08a2a3f9":"train[['GarageType','GarageCond','GarageQual','GarageFinish','GarageYrBlt']].head()","2aa99b9a":"# Making a method to fill-na values\ndef fill_na(df, column:list):\n    for item in column:\n        df[item]=df[item].fillna(df[item].mode()[0])\n    return df","4e376623":"Garage_features = ['BsmtCond','BsmtQual','FireplaceQu','GarageType','GarageCond','GarageQual','GarageFinish','GarageYrBlt']\ntrain = fill_na(train, Garage_features)\ntest = fill_na(test, Garage_features)","56a5e9ea":"# Garage year build is float variable but it needs to be a interger\ntrain['GarageYrBlt'] = train['GarageYrBlt'].astype(int)","538a3c5a":"# Checking if the shape is still correct\nprint(train.shape)\nprint(test.shape)","06a7076e":"categories = train.select_dtypes(include='O').keys()\ncategories","98e8d18b":"train_dum = pd.get_dummies(data= train, columns=categories, drop_first=True)\ntest_dum = pd.get_dummies(data= test, columns=categories, drop_first=True)\nprint(train_dum.shape)\nprint(test_dum.shape)","02db6264":"a = list(train_dum.columns)\nb = list(test_dum.columns) \n\nlist(set(a)- set(b))","1374ed4b":"train_dum = train_dum.drop(columns=['Exterior1st_Stone','Heating_GasA','Exterior1st_ImStucc','RoofMatl_Metal',\n                                    'Electrical_Mix','Condition2_RRAn', 'Utilities_NoSeWa','Condition2_RRNn',\n                                    'Heating_OthW','GarageQual_Fa','Exterior2nd_Other', 'RoofMatl_Roll', 'RoofMatl_Membran'\n                                    ,'Condition2_RRAe','HouseStyle_2.5Fin','RoofMatl_CompShg'], axis=1)","16a78ad3":"train_dum_X = train_dum.drop(columns='SalePrice', axis=1)\ntrain_y = train.SalePrice\n\n#Test variable is called: test_dum","16b71b8f":"import xgboost as xgb\nfrom xgboost import XGBClassifier","cff2e6fe":"xg_reg = xgb.XGBRegressor(objective ='reg:squarederror', colsample_bytree = 0.3, learning_rate = 0.1,\n                max_depth = 10, alpha = 10, n_estimators = 10000)","5030641a":"xg_reg.fit(train_dum_X, train_y)","3e2862d9":"preds = xg_reg.predict(test_dum)\nprint(preds)","99ec5187":"my_submission = pd.DataFrame({'Id': test.Id, 'SalePrice': preds})\n\n#my_submission.to_csv('submission5.csv', index=False)","173c2a63":"from sklearn.model_selection import RandomizedSearchCV\nclassifier = xgb.XGBRegressor()\nregressor = xgb.XGBRegressor()","2af2ce62":"booster=['gbtree','gblinear']\nbase_score=[0.25,0.5,0.75,1]","a0ae667a":"## Hyper Parameter Optimization\nn_estimators = [100, 500, 900, 1100, 1500]\nmax_depth = [2, 3, 5, 10, 15]\nlearning_rate=[0.05,0.1,0.15,0.20]\nmin_child_weight=[1,2,3,4]\n\n# Define the grid of hyperparameters to search\nhyperparameter_grid = {\n    'n_estimators': n_estimators,\n    'max_depth':max_depth,\n    'learning_rate':learning_rate,\n    'min_child_weight':min_child_weight,\n    'booster':booster\n    }","5824ac04":"# Set up the random search with 4-fold cross validation\nrandom_cv = RandomizedSearchCV(estimator=regressor,\n            param_distributions=hyperparameter_grid,\n            cv=5, n_iter=50,\n            scoring = 'neg_mean_absolute_error',n_jobs = 4,\n            verbose = 5, \n            return_train_score = True,\n            random_state=42)","a0fa5dd8":"#random_cv.fit(train_dum_X,train_y)","835d35bf":"#random_cv.best_params_\n\n# This returned:\n#{'n_estimators': 1100,\n# 'min_child_weight': 1,\n# 'max_depth': 3,\n#'learning_rate': 0.05,\n# 'booster': 'gbtree'}","deffd702":"xg_reg = xgb.XGBRegressor(objective ='reg:squarederror', colsample_bytree = 0.3, learning_rate = 0.05,\n                max_depth = 3, alpha = 10, n_estimators = 1100, min_child_weight = 1, booster= 'gbtree')","75178923":"xg_reg.fit(train_dum_X, train_y)","e77cd084":"predict_test = xg_reg.predict(test_dum)\nprint(predict_test)","ca934fdd":"my_submission = pd.DataFrame({'Id': test.Id, 'SalePrice': preds})\n\nmy_submission.to_csv('submission6.csv', index=False)","a0cd61ac":"These are the best parameters to use for this Extreme Gradient Boosting model","691736cf":"I noticed that the Garage features have all exactly 78 Null Values, so I wanted to see this dataframe form","384b048a":"I see that the shape isn't correct anymore so I want to find out which columns are the reason for this.\nThen I drop those columns in order to make it work for the Extreme Gradient Boosting algorithm.","10af475d":"# Linear regression\nThe resulting scores are placed at the end of the notebook.","7df20253":"# Conclusion\nThere are a lot of different ways to approach a challenge like this. The correlation analysis already gave some high scores but cleaning\/one-hot encoding went a step beyond. Data preperation is the key for a good working model. After that cross validation in combination with hyperparameterstuning can improve the accuracy even more. \n\n# Reflection\nThis was my first ever Kaggle competition, it took a lot of time for me until I saw some results. But I'm quite certain now that this is the way to improve as a starting Data Scientist. It's insane what I have learned from this competition alone. A big driver for me was the leaderboard, I found it really motivating to try and improve my previous score.","2f1904de":"# Random Forest\nThe resulting scores are placed at the end of the notebook.","36f4227a":"# Defining training & test variables\nHaving cleaned the Null-Values and did One-hot encoding, I want to see the results with the Extreme Gradient Boosting algorithm.","d8b42cf8":"# Scores\n*Try 1: position, 5088 - score: 2.234* <br>\nThis try was done in a different notebook in which I messed up the submission. I learned from this that it's really important for Kaggle competition to know what the submission needs to be. <br><br>\n*Try 2: position, 4875 - score: 0.41364 (XGB without cleaning)*<br>\nRegular XGB without cleaning the data first, overall a bad score.<br><br>\n*Try 3: position, 4054 - score: 0.174 (Random Forest)*<br>\nMy first good try was with the random forest model and using the high correlation features. This placed me somewhere at 50% of the leaderboard. <br><br>\n*Try 4: position, 5013 - score: 0.66817 (Linear Regression)*<br>\nSecond model I used was linear regression, this score was low so I decided not to use this model.<br><br>\n*Try 5: position, 1990 - score: 0.13715 (Null values & One-hot encoding)*<br>\nFifth try gave me a really high score, looking at the overall competition of 13.000 people being placed in the top 2000 is a real achievement for me. Handeling the clutter from the Null-Values really improved the models. One Hot encoding is a really usefull technique in combination with the Extreme Gradient Boosting model. <br><br>\n*Try 6: position, 1814 - score 0.13485 (Hyperparameters\/cross validation)* <br>\nMy final try to improve my score on the leaderboard, I hoped to get into the top 1000 with this but I managed to improve around 200 places. Using the optimal parameters by using cross validation to check a broad range made the model perform better. ","b582341b":"# High Correlation Analysis\nWhich house features have the highest correlation with the SalePrice? I'll do this by using the corr() command and taking the highest scoring features.\n","d5bdbf76":"I'll drop all the columns where the null-values consist of more then half the dataset, which is 780 rows.","739e6438":"# Kaggle Competition house prices\n## Goal:\nPredicting the Sale price for each house. For each test ID in the test set, a prediction must be made of the SalePrice value. Using only the features provided in the test dataframe.\n## Metric:\nSubmissions are evaluated on Root-Mean-Squared-Error (RMSE) between the logarithm of the predicted value and the logarithm of the observed sales price.\n\n## Personal goal:\nI want to use three different machine learning models and apply the fundamental elements in Data Science. The three models are; Linear Regression, Random Forest & Extreme Gradient Boosting. Fundamental techniques like; Exploratory Data Analysis, Correlation Analysis, Handeling Null values, One-hot encoding, Hypertuning & Cross Validation.\n","176ec313":"# Visualizing the six features\nFor this next visualization I use the pairplot technique, this shows each predictor feature in a scatterplot with the color being the overall quality of the house.\n\nI notice a lot of straight lines at 0 for Garage, First floor and Basement features. This means that there are a lot of Null-values in this dataset.","a7c122a6":"# Defining the train & test variables\nTaking the train dataset I only use the six high correlation features for the first predictions. Instead of using all the features, only using features that have proofed to correlate highly with SalePrice, will help improve the prediction score.  ","71b1d24f":"# XGB\nThese are the standard parameters for XGB. The resulting scores are placed at the end of the notebook.","f53804bb":"# One-hot encoding for multiple categorical columns\nThis dataset consists of many category columns, machine learning algorithms can't read the difference between different SaleConditions. One hot encoding transforms categories into values, making each category a new column and placing the count in rows. This will change the shape of the dataset drastically.","099dc5a5":"# Null values\nSo these scores are not high enough for my liking, this means the data needs to be cleaned more. First let's see what the amount of Null Values are.","35dc7ab9":"## Visualizing the correlation matrix\nNow that we have the correlation score of these six features for the SalePrice. I want to know if there is a correlation between these features. That's why I use a correlationmatrix to compare them with eachother.","7e0f44ab":"**Meaning of each feature:**\n- OverallQual: Overall material and finish quality\n- TotalBsmtSF: Total square feet of basement area\n- 1stFlrSF: First Floor square feet\n- GrLivArea: Above grade (ground) living area square feet\n- GarageCars: Size of garage in car capacity\n- GarageArea: Size of garage in square feet","b984308d":"# Hyperparameters tuning\nThe XGB gave the best score up untill now, so I want to evaluate the model. Hyperparameter tuning and cross validation will help me select the best parameters for this algorithm. This will take some time running but the results will improve the score.","dea7adf5":"# Exploratory Data Analysis\nFirst I want to plot the SalePrice in a distribution curve to see if the data is logical. The overall quality will be a higher score the higher the SalePrice. There are a lot of features, but I want to know which are the most important ones, then visualizing the result."}}