{"cell_type":{"68d0b360":"code","d7bb77f6":"code","4b9ffc48":"code","eeb07b9b":"code","efd091fc":"code","078b40a8":"code","1527a595":"code","8d1ee63d":"code","18236816":"code","10049786":"code","dab7f6a8":"code","cb182b71":"code","056709de":"code","bf821722":"code","c24c022c":"code","757ae341":"code","11ad3e8e":"code","d04d9157":"code","82422c96":"code","fdd03ccf":"code","0d6ad039":"code","d96f8f7a":"code","b7cd6b58":"code","33762ee8":"code","02733a5e":"code","06dd9bf5":"code","7d799bee":"code","79f8ffee":"code","55e1d06f":"code","22a05e7b":"code","cd1d8794":"code","250929fb":"code","6427dcee":"code","a940581d":"code","7172747a":"code","d70b1f3d":"code","eb650c39":"markdown","0bf46007":"markdown","e44719c1":"markdown","7271e820":"markdown","942d33b3":"markdown","e43c3109":"markdown","4b3917d6":"markdown","87735626":"markdown","c86112da":"markdown","6352b799":"markdown","a871ce23":"markdown","19613bc4":"markdown","6afcb207":"markdown","63ab81b3":"markdown","9125563f":"markdown","5c1c6cf5":"markdown","5ddbd880":"markdown","9c8c298b":"markdown","6e8950bd":"markdown","eb45b72c":"markdown","ab8d808a":"markdown","0fb5180c":"markdown","cde6b03a":"markdown","a9100b15":"markdown","53fa0f43":"markdown","a9f9a5fa":"markdown","6ba00f42":"markdown","bedab059":"markdown","847eb7ed":"markdown","05e4ced1":"markdown","7e214628":"markdown","f81aea5f":"markdown","c490a29e":"markdown","0a09832b":"markdown","d9b842be":"markdown","8d7dfa51":"markdown","aadcfd05":"markdown","c386883b":"markdown"},"source":{"68d0b360":"import matplotlib.pyplot as plt\nimport numpy as np\nimport os\nimport pandas as pd\nimport seaborn as sns\nimport xgboost as xgb\n\nfrom catboost import CatBoostRegressor, Pool\nfrom catboost import cv as catboost_cv\nfrom IPython.display import display\n\nfrom sklearn.cluster import MiniBatchKMeans\nfrom sklearn.model_selection import train_test_split, RandomizedSearchCV\nfrom sklearn.metrics import mean_squared_error","d7bb77f6":"%matplotlib inline","4b9ffc48":"DO_XGBOOST = True\nDO_CATBOOST = not DO_XGBOOST","eeb07b9b":"train_raw = pd.read_csv('..\/input\/train\/train.csv', index_col='id', \n                        parse_dates=['pickup_datetime', 'dropoff_datetime'])\ntest_raw = pd.read_csv('..\/input\/test\/test.csv', index_col='id', \n                       parse_dates=['pickup_datetime'])","efd091fc":"print('Train set:\\n')\ntrain_raw.info()","078b40a8":"print('Test set:\\n')\ntest_raw.info()","1527a595":"print('NaN or empty data (train): {}'.format(train_raw.isnull().values.any()))\nprint('NaN or empty data (test): {}'.format(test_raw.isnull().values.any()))","8d1ee63d":"def check_hidden(*dfs):\n    for df in dfs:\n        display(df.describe())\n\ncheck_hidden(train_raw, test_raw)","18236816":"max_dur_hrs = np.max(train_raw[\"trip_duration\"].values) \/ 3600\nprint(f'Max trip duration (hours): {max_dur_hrs:.3f}')\n\n_, (ax0, ax1) = plt.subplots(1, 2, figsize=(16, 8), sharex=False)\n\nax0.set_title('Trip duration distribution')\nax0.set_xlabel('Trip duration (s)')\n\nax1.set_title('LogP1-Trip duration distribution')\nax1.set_xlabel('log(Trip Duration + 1) ')\n\nax0.hist(train_raw['trip_duration'], bins=100);\nax1.hist(np.log1p(train_raw['trip_duration']), bins=100);","10049786":"train_set = train_raw.copy(deep=True).drop('dropoff_datetime', axis=1)\ntest_set = test_raw.copy(deep=True)","dab7f6a8":"checkpoints = {\n    'pre_engineering': {\n        'train': train_set.copy(deep=True),\n        'test': test_set.copy(deep=True)\n    }\n}","cb182b71":"def add_checkpoint(key):\n    d = { 'train': train_set.copy(deep=True), 'test': test_set.copy(deep=True) }\n    checkpoints.update({\n        key: d\n    })","056709de":"def restore_checkpoint(key):\n    return checkpoints[key]['train'].copy(deep=True), checkpoints[key]['test'].copy(deep=True)","bf821722":"def cat_encode(*dfs):\n    for df in dfs:\n        for col in df.select_dtypes('object').columns:\n            df[col] = df[col].astype('category').cat.codes","c24c022c":"cat_encode(train_set, test_set)","757ae341":"def extract_date_columns(*dfs):\n    for df in dfs:\n        df['pickup_datetime_month'] = df.pickup_datetime.dt.month\n        df['pickup_datetime_hour'] = df.pickup_datetime.dt.hour\n        df['pickup_datetime_minute'] = df.pickup_datetime.dt.minute\n        df['pickup_datetime_dow'] = df.pickup_datetime.dt.dayofweek","11ad3e8e":"extract_date_columns(train_set, test_set)","d04d9157":"def _haversine_distance(lat_a, long_a, lat_b, long_b):\n    '''Calculates the haversine distance between two geographic coordinates in meters.\n    \n    The haversine distance is defined as \"the distance a crow flies\" between two points.\n    \n    Parameters\n    ----------\n        lat_a  : latitude of point A (array-like)\n        long_a : longitude of point A (array-like)\n        lat_b  : latitude of point B  (array-like)\n        long_b : longitude of point B (array-like)\n    \n    Returns\n    -------\n        the haversine distance between the two points\n    '''\n    del_lat_rad = lat_b - lat_a\n    del_long_rad = long_b - long_a\n    \n    lat_a, long_a, lat_b, long_b, del_lat_rad, del_long_rad = \\\n        map(np.radians, [lat_a, long_a, lat_b, long_b, del_lat_rad, del_long_rad])\n    \n    def _a(del_lat, del_long, lat_a, lat_b):\n        return (\n            (np.sin(del_lat \/ 2) ** 2) +\n            (np.cos(lat_a) * np.cos(lat_b) *\n             np.sin(del_long \/ 2) ** 2)\n        )\n    \n    \n    def _c(a):\n        return 2 * np.arctan2(np.sqrt(a), np.sqrt(1 - a));\n    \n    # radius of the earth in meters\n    R = 6371e3\n    \n    a = _a(del_lat_rad, del_long_rad, lat_a, lat_b)\n    c = _c(a)\n    \n    return R * c\n\ndef add_manhattan_haversine_distance(*dfs):\n    '''Calculates the \"true\" Manhattan distance between two geographic coordinates.\n    \n    This is probably a bad estimation of a city-block distance, but is better than nothing.\n    \n    Parameters\n    ----------\n        dfs : dataframes in which to add the manhattan-haversine distance column\n    '''\n    for df in dfs:\n        inbounds = df.loc[(df['pickup_longitude'] <= -73.94) & \n                           df['dropoff_longitude'] <= -73.94]\n\n        inbounds_idx = inbounds.index\n\n        oob = df.loc[~df.index.isin(inbounds_idx), :]\n        oob_idx = oob.index\n\n        ib_lat_a = inbounds['pickup_latitude']\n        ib_lat_b = inbounds['dropoff_latitude']\n        ib_long_a = inbounds['pickup_longitude']\n        ib_long_b = inbounds['dropoff_longitude']\n\n        oob_lat_a = oob['pickup_latitude']\n        oob_lat_b = oob['dropoff_latitude']\n        oob_long_a = oob['pickup_longitude']\n        oob_long_b = oob['dropoff_longitude']\n\n        df['city_distance'] = pd.Series(np.zeros(shape=(len(df.index))), index=df.index)\n        df.loc[inbounds_idx, 'city_distance'] = _haversine_distance(ib_lat_a, ib_long_a, \n                                                                    ib_lat_a, ib_long_b) + \\\n                                                _haversine_distance(ib_lat_a, ib_long_a, \n                                                                    ib_lat_b, ib_long_a)\n        df.loc[oob_idx, 'city_distance'] = _haversine_distance(oob_lat_a, oob_long_a, \n                                                               oob_lat_b, oob_long_b)\n\n","82422c96":"add_manhattan_haversine_distance(train_set, test_set)","fdd03ccf":"add_checkpoint('with_manhattan')","0d6ad039":"_, (ax0, ax1) = plt.subplots(1, 2, figsize=(16, 8))\n\nax0.set_title('Trip distances')\n\nax1.set_title('log(Trip distances + 1)')\n\nax0.hist(train_set['city_distance'], color='blue', alpha=0.2, bins=100, label='train')\nax0.hist(test_set['city_distance'], color='red', bins=100, label='test')\nax1.hist(np.log1p(train_set['city_distance']), color='blue', \n         alpha=0.2, bins=100, label='train')\nax1.hist(np.log1p(test_set['city_distance']), \n         color='red',  bins=100, label='test')\n\nax0.legend()\nax1.legend();","d96f8f7a":"corr = train_set.corr()","b7cd6b58":"_, ax = plt.subplots(1, figsize=(12, 12))\n\nax.xaxis.set_ticklabels([c for c in corr.columns.values], rotation=45)\nax.yaxis.set_ticklabels([c for c in corr.columns.values], rotation=0)\n\nsns.heatmap(corr, annot=True, ax=ax, fmt='.3f');","33762ee8":"_, ax = plt.subplots(1, figsize=(8, 8))\nax.set_title('Trip duration vs Trip Distance')\nax.set_xlabel('City distance (m)')\nax.set_ylabel('Trip duration (s)')\nax.scatter(train_set['city_distance'][:500], train_set['trip_duration'][:500]);","02733a5e":"h = train_set.groupby('pickup_datetime_hour').mean()['trip_duration']\ndow = train_set.groupby('pickup_datetime_dow').mean()['trip_duration']\n_, (ax0, ax1) = plt.subplots(1, 2, figsize=(16, 8), sharey=True)\nax0.plot(h.index.values, h.values)\nax0.set_title('Mean trip duration vs Hour')\nax0.set_ylabel('Mean trip duration (s)');\nax0.set_xlabel('Hour of day')\n\nax1.plot(dow.index.values, dow.values)\nax1.set_title('Mean trip duration vs DOW')\nax1.set_xlabel('Day of week (Sunday-Saturday)');\n","06dd9bf5":"island_long_border = (-74.03, -73.75)\nisland_lat_border = (40.63, 40.85)\n\npickup_lat_train = train_set.pickup_latitude[:100000]\npickup_long_train = train_set.pickup_longitude[:100000]\n\ndropoff_lat_train = train_set.dropoff_latitude[:100000]\ndropoff_long_train = train_set.dropoff_longitude[:100000]\n\npickup_lat_test = test_set.pickup_latitude[:100000]\npickup_long_test = test_set.pickup_longitude[:100000]\n\ndropoff_lat_test = test_set.dropoff_latitude[:100000]\ndropoff_long_test = test_set.dropoff_longitude[:100000]\n\n_fig, (ax0, ax1) = plt.subplots(1, 2, figsize=(16, 8), sharex=True, sharey=True)\nax0.set_xlim(island_long_border)\nax0.set_ylim(island_lat_border);\nax0.set_title('Pickup locations (train)')\nax0.set_ylabel('Latitude')\nax0.set_xlabel('Longitude')\nax0.scatter(pickup_long_train, pickup_lat_train, c='blue', s=0.5)\nax1.scatter(dropoff_long_train, dropoff_lat_train, c='red', s=0.5)\nax1.set_title('Dropoff locations (train)')\n\n_fig, (ax0, ax1) = plt.subplots(1, 2, figsize=(16, 8), sharex=True, sharey=True)\nax0.set_xlim(island_long_border)\nax0.set_ylim(island_lat_border);\nax0.set_title('Pickup locations (test)')\nax0.set_ylabel('Latitude')\nax0.set_xlabel('Longitude')\nax0.scatter(pickup_long_test, pickup_lat_test, c='green', s=0.5)\nax1.scatter(dropoff_long_test, dropoff_lat_test, c='purple', s=0.5);\nax1.set_title('Dropoff locations (test)');","7d799bee":"%%time\ncluster_set = train_set.loc[:, ['dropoff_latitude', 'dropoff_longitude']]\n\nk = 50\nmbkmeans = MiniBatchKMeans(n_clusters=k, batch_size=32)\nmbkmeans.fit(cluster_set)\ncluster_set['cluster'] = pd.Series(mbkmeans.labels_, index=cluster_set.index)\n\n_, ax = plt.subplots(1, figsize=(12, 12))\n\nisland_long_border = (-74.03, -73.75)\nisland_lat_border = (40.63, 40.85)\n\nlat = cluster_set.iloc[:100000, :]['dropoff_latitude']\nlon = cluster_set.iloc[:100000, :]['dropoff_longitude']\nclusters = cluster_set.iloc[:100000, :]['cluster']\n\nax.scatter(lon.values, lat.values, c=clusters, cmap='tab20c', alpha=0.5, s=1)\nax.set_xlim(island_long_border)\nax.set_ylim(island_lat_border);\n\ntrain_set['dropoff_cluster'] = cluster_set['cluster']\ntest_set['dropoff_cluster'] = \\\n    pd.Series(mbkmeans.predict(test_set.loc[:, ['dropoff_latitude', 'dropoff_longitude']].values),\n              index=test_set.index)\n\ntrain_set['pickup_cluster'] = \\\n    pd.Series(mbkmeans.predict(train_set.loc[:, ['pickup_latitude', 'pickup_longitude']].values),\n              index=train_set.index)\ntest_set['pickup_cluster'] = \\\n    pd.Series(mbkmeans.predict(test_set.loc[:, ['pickup_latitude', 'pickup_longitude']].values),\n              index=test_set.index);","79f8ffee":"add_checkpoint('post_clustering')","55e1d06f":"train_set.drop('pickup_datetime', axis=1, inplace=True)\ntest_set.drop('pickup_datetime', axis=1, inplace=True)\nadd_checkpoint('pre_modeling')","22a05e7b":"X = train_set.drop('trip_duration', axis=1)\ny = np.log1p(train_set['trip_duration'])","cd1d8794":"X_train, X_validate, y_train, y_validate = train_test_split(X, y)","250929fb":"xgb_train = xgb.DMatrix(X_train, label=y_train)\nxgb_validate = xgb.DMatrix(X_validate, label=y_validate)\n\nevallist = [(xgb_train, 'training-set'), (xgb_validate, 'validation-set')]\n\nxgb_params = {\n    'n_estimators': 250,\n    'eta': 0.2,\n    'subsample': 0.6,\n    'max_depth': 10,\n    'colsample_bytree': 0.75,\n    'min_child_weight': 25,\n    'objective': 'reg:linear',\n    'booster': 'gbtree',\n    'nthread': -1,\n    'silent': 1,\n    'eval_metric': 'rmse'\n}","6427dcee":"if DO_XGBOOST:\n    xgb_regressor = xgb.train(xgb_params, xgb_train, 100, evallist,\n                              early_stopping_rounds=100, maximize=False,\n                              verbose_eval=10)","a940581d":"cat_features = [0, 6, 12, 13]\ntrain_pool = Pool(X_train, label=y_train, cat_features=cat_features)\nvalidation_pool = Pool(X_validate, label=y_validate, cat_features=cat_features)\ncat_regressor = CatBoostRegressor(\n    loss_function='RMSE',\n    eval_metric='RMSE',\n    iterations=250,\n    learning_rate=0.2,\n    depth=10\n)","7172747a":"if DO_CATBOOST:\n    cat_regressor.fit(train_pool, eval_set=validation_pool)","d70b1f3d":"def create_xgb_submission():\n    preds = xgb_regressor.predict(xgb.DMatrix(test_set))\n    preds_df = pd.DataFrame(np.expm1(preds), index=test_set.index)\n    preds_df.columns = ['trip_duration']\n    \n    preds_df.to_csv('submission.csv')\n    display(pd.read_csv('submission.csv').head())\n    \ndef create_catboost_submission():\n    pool = Pool(test_set, cat_features=cat_features)\n    preds = cat_regressor.predict(pool)\n    preds_df = pd.DataFrame(np.expm1(preds), index=test_set.index)\n    preds_df.columns = ['trip_duration']\n    \n    preds_df.to_csv('submission.csv')\n    display(pd.read_csv('submission.csv').head())\n\nif DO_XGBOOST:\n    create_xgb_submission()\nif DO_CATBOOST:\n    create_catboost_submission()","eb650c39":"<a id=\"catdata\"><\/a>\n### Categorical data\n\nAfter further investigation, it seems we have some categorical columns - `vendor_id` and `store_and_fwd_flag`. Because we are going to be using `XGBoost` as one of our models, we need to ensure that these data columns are represented numerically. This is a common step in most tree-based approaches (i.e. Random Forests, Decision Trees, etc.). Pandas has a nice feature which allows us to integer-encode categorical variables:\n\n    df := pd.DataFrame\n    df[column] = df[column].astype('category').cat.codes\n    \nIn our case, we technically don't even need to use this feature - the encoding is very simple. For example, for the `store_and_fwd_flag` column,\n\n    df['store_and_fwd_flag'] = (df['store_and_fwd_flag'] == 'Y').astype(np.int32)\n    \nEither way, we will use the Pandas approach.\n\n**NOTE**: It is very important that you apply feature engineering steps to both the train set and test set simultaneously. A good way to accomplish this is by using functions to perform dataframe manipulation, and passing the train and test set frames as arguments.","0bf46007":"#### Training\n\nNow that we've found the optimal model parameters, let's train the model!\n\nRemember, when using `xgboost`, you need to first wrap your training, validation, and test sets in a `xgb.DMatrix`.","e44719c1":"Our target variable is the `trip_duration` column in the **train** set. Keep in mind that the supplied **test** set does **not** have the `trip_duration` column (logical, as this column is what the models are scored on in the Kaggle competition). Likewise, the `dropoff_datetime` is only available to us in the train set.\n\nSo, it seems our common data between the train and test sets are:\n- `vendor_id`\n- `pickup_datetime`\n- `passenger_count`\n- `pickup_longitude`\n- `pickup_latitude`\n- `dropoff_longitude`\n- `dropoff_latitude`\n- `store_and_fwd_flag`","7271e820":"## Submission","942d33b3":"<a id=\"clustering\"><\/a>\n### Geospatial Clustering\n\nPlotting latitude vs longitude for both pickup and dropoff locations in the train and test sets yields some interesting (and frankly quite beautiful) information. We can see that the dropoff locations are more varied than the pickup locations, indicating that people typically take taxi rides out of Manhattan to differnt points of interest outside of the city limits. For example, the little concentrated cluster on the bottom right of the plot is JFK International Airport.\n\nFurthermore, a good thing to note is that the trian set and test sets seem to cover the same areas.","e43c3109":"Let's begin by taking a look at the data that is made available to us in both the train and test sets.\n\n*Yes, Kaggle tells us what columns are in the `.csv` files, but let's be explicit.*","4b3917d6":"When we take a look at the trip distances, it seems that we again have some outliers. Applying the same log transform (`np.log1p`) to the trip distances gives us a better look at their distributions.","87735626":"Very often latitude and longitude alone are not the strongest predictors (features), so doing some feature engineering to enrich their prediction strength is advised.\n\nWe can use `MiniBatchKMeans` to do some zone clustering, and add the clustering results as categorical features, i.e. assign the pickup and dropoff locations to cluster. As we have already determined, the dropoff locations represent a larger variety of locations, so we will train the clustering model using the train set dropoff latitude and longitude coordinates.","c86112da":"We've broken the .4 barrier on the training set, and have come pretty close on the validation set. Let's see if `CatBoost` can do any better.","6352b799":"Perhaps most obviously, it seems that `city_distance` is the feature which is most correlated with trip duration - seems logical. However, I'm quite surprised to see that the date information we extracted doesn't correlate very strongly.\n\nGranted, this does represent the Pearsman correlation, which assumes linear correlation. If we wanted to perhaps get a better picture, we could calculate the Spearman correlations. Let's do some more visualization of the date information versus trip duration.","a871ce23":"Unforunately the training process is quite verbose with `CatBoost`, so apologies for the scrolling.\n\nWe see that `CatBoost` peforms quite similarly to `XGBoost`, again coming quite close to breaking the .4 barrier on the validation set. The only thing left to do is to predict on the test set using our models and submit our work.","19613bc4":"<a id=\"eafe\"><\/a>","6afcb207":"<a id=\"distance\"><\/a>\n### Calculating trip distance\n\nAll traffic ideas aside, imagining that NYC was completely empty except for one taxi cab, the most obvious influencing factor on trip duration would be trip distance.\n\nWe're given the pickup and dropoff `(latitude, longitude)` information, which means that we can approximate the total distance of the trip. The most straightforward way to do so would be to calculate the [Haversine Distance](#https:\/\/www.movable-type.co.uk\/scripts\/latlong.html) between the pickup point and dropoff point.\n\nThe Haversine distance is defined as \"the distance a crow flies\" between two geographic coordinates. However, taxis don't fly - especially not in Manhattan. In Manhattan, they drive on a grid. For trips that leave or enter Manhattan, the Haversine distance is perfectly fine. However, for inter-Manhattan trips, another Kaggle user found an interesting way to approximate a more Manhattan-like distance using the Haversine distance formula,\n\n$D_{HM}(\\phi_A, \\theta_A, \\phi_B, \\theta_B) = D_H(\\phi_A, \\theta_A, \\phi_A, \\theta_B) + D_H(\\phi_A, \\theta_A, \\phi_B, \\theta_A)$\n\nwhere $\\phi_A$ and $\\phi_B$ are the latitudes of points A and B respectively, $\\theta_A$ and $\\theta_B$ are the longitudes of points A and B respectively, $D_H$ is the normal Haversine Distance, and $D_{HM}$ is the Manhattan-like distance.","63ab81b3":"## Exploratory Analysis, Feature Engineering","9125563f":"#### Cross-Validation\n\nAgain, in the interest of saving time, I have simply supplied the code I used to cross-validate the `CatBoost` model below. Note that `CatBoost` does not have a `scikit-learn` estimator-based interface, so you must rely on their cross-validation module (`catboost.cv`).","5c1c6cf5":"As expected, right around 3PM there's a spike in mean trip duration. This probably corresponds to the beginning of rush-hour traffic, which lasts until around 6 or 7PM.\n\nIn terms of the day of week, there seems to be slightly increased mean trip duration times - probably the lethargy of stressful NYC work-life kicking in right around hump day (Wednesday\/Thursday).\n\nThese two plots, despite the correlation heatmap, make me believe that the datetime information we extracted is going to be useful in the modeling phase.","5ddbd880":"I see some potential outliers in some of the columns, but in terms of missing data, everything seems to be in order.\n<a id=\"tvd\"><\/a>\n### Target variable distribution\n\nLet's look at the target variable in the training set.","9c8c298b":"<a id=\"missingdata\"><\/a>","6e8950bd":"```python\nxgb_param_grid = {\n    'n_estimators': [25, 100, 250],\n    'learning_rate': np.linspace(0.1, 0.3, num=5),\n    'subsample': np.linspace(0.3, 0.7, num=5),\n    'max_depth': [3, 6, 10],\n    'colsample_bytree': np.linspace(0.5, 0.75, num=3),\n    'min_child_weight': [1, 10, 25, 75]\n}\n\nX_train, X_validate, y_train, y_validate = train_test_split(X, y)\n\nxgb_regressor = xgb.XGBRegressor(objective='reg:linear', booster='gbtree')\nxgb_random_search = RandomizedSearchCV(xgb_regressor, \n                                       param_distributions=xgb_param_grid, \n                                       n_iter=25, scoring='neg_mean_squared_error',\n                                       error_score=0, verbose=3, n_jobs=-1, cv=5)\nxgb_random_search.fit(X_train, y_train)\nv_preds = xgb_random_search.predict(X_validate)\nprint(f'RMSE on validation set: {np.sqrt(mean_squared_error(y_validate, v_preds))}')\n```","eb45b72c":"<a id=\"dtdata\"><\/a>\n### `datetime` data ","ab8d808a":"<a id=\"featcor\"><\/a>\n### Feature correlation\n\nNow that we have done some feature engineering, let's see if we find any correlation between our selected features and the target variable.","0fb5180c":"#### *Small programming aside*\n\nHere I define some useful functions for keeping track of key points in the model-creation pipeline. There is probably a better way to do this, but this works fine for me.\n\nI keep a dictionary called `checkpoints` which contains `(str, dict)` key-value pairs. The dictionary values are copies of my train and test sets. I also create a method called `add_checkpoint(key)` which will store the current state of my train and test sets in the dictionary. This way, if I ever make a mistake and need to revert back to a particular preprocessing step, I can call `get_checkpoint(key)` without having to scroll up and down, re-executing cells.","cde6b03a":"#### Training","a9100b15":"Our train and test sets have a `pickup_datetime` column, which is of type `datetime64`. Pandas has some excellent functionality for extracting useful date-time information.\n\nIn the context of our problem, we should begin to think about what time-related features could affect the trip duration of a taxi ride. I think anyone that's been to NYC knows traffic is a nightmare at any time of the day, but it's especially true during rush-hour. I'd like to take a look at how the trip durations in the train set vary with different parts of the `pickup_datetime` - namely the month, the hour and minute of the day, and the day of the week.\n\nWe can extract that information using,\n\n    df[datetime_col].dt.month\n    df[datetime_col].dt.hour\n    df[datetime_col].dt.minute\n    df[datetime_col].dt.dayofweek\n    \nWe'll concatenate that information as new columns in the train and test frames.","53fa0f43":"## Discussion","a9f9a5fa":"## Modeling\n\nNow that our exploratory analysis and feature engineering is complete, it's time to build some models. We're going to train and cross-validate two models - `XGBoost` and `CatBoost`.\n\nThe first step is to drop the no longer needed datetime columns, and separate the features and labels.","6ba00f42":"```python\ncat_features = [0, 6, 12, 13]\ncat_cv_pool = Pool(X, label=y, cat_features=cat_features)\ncat_cv_params = {\n    'loss_function': 'RMSE',\n    'iterations': 250,\n    'learning_rate': 0.2,\n    'random_seed': 42,\n    'depth': 10\n}\ncv = catboost_cv(pool=cat_cv_pool, params=cat_cv_params, fold_count=5)\nprint(cv)\n```","bedab059":"The target variable seems to follow a log-normal distribution, and there are definitely some potential outliers - the max trip duration is almost 1000 hours. Therefore we apply `numpy`'s `log1p` function on the values to obtain a more normal-like distribution.\n\nThis is an important step to note! If, down the line, we perform training and validation on the log-transformed values for the `trip_duration`, our training metric can be `RMSE` (root mean squared error). Then for our submission, we can simply \"untransform\" the predictions by using `np.expm1(predictions)` in order to conform with the competition training metric, `RMSLE` (root mean squared logarithmic error).","847eb7ed":"We've tested two models, `XGBoost` (RMSE: 0.41134) and `CatBoost` (RMSE: 0.41398), and both performed well on the test set. However, we couldn't quite get below an RMSE of 0.4.\n\nIn terms of CV and training times, `CatBoost` is the winner by a landslide. Cross validation took 1 hour and 41 minutes on the supplied parameter grid, and the model trained in roughly 8 minutes. `XGBoost` with a randomized parameter grid search took 13 hours to cross-validate, and and trained in 12 minutes.\n\n`XGBoost` managed to obtain a better RMSE score, but not by much. I will of course be submitting the `XGBoost` as a final model - after all, it's a competition. However, in real world situations, we as datascientists are often faced with the dilemma of building a highly cross-validated and performant model versus taking a slight performance hit in the interest of streamlining productivity. If this were a client project, and my team and I were pressed for time, I'd probably go with `CatBoost` as it performs quite well, and in much less time.\n\nIn terms of the scores themselves, I'm sure that further feature engineering could help improve the models. There exists a variety of datasets containing valuable information which could enrich the feature space, and those who wish to expand upon the work we've done here are encouraged to investigate and contribute.\n\nI hope that this notebook was helpful and interesting.","05e4ced1":"It seems our train and test sets follow quite similar distributions, which is very nice.","7e214628":"#### Cross-Validation, Random Parameter Search\n\nFor `XGBoost` we can use `scikit-learn`'s random parameter searching functionality to perform cross validation, and to find optimal model parameters. `RandomizedSearchCV` is compatible with all `scikit-learn` estimators, *and* XGBoost. The `xgboost` library has a `sklearn` module (`xgboost.sklearn`) which contains a `scikit-learn` based implementation of the algorithm. We will use this implementation for cross-validation, and we will use the standard `xgboost` implementation for training and prediction.\n\n**NOTE**: In the interest of saving time in executing this notebook, I performed the parameter search and cross validation in a separate notebook, as it is quite a computationally expensive and time-consuming process. I'll supply a code example on how you can use `RandomizedSearchCV` with `XGBRegressor` below.","f81aea5f":"The clustering seems to be robust - and is very pretty to look at...","c490a29e":"# Predicting NYC Cab Trip Durations","0a09832b":"### CatBoost\n\n`CatBoost` is another tree-based boosting algorithm which works well with categorical variables. It is has a 4x time-performance when compared to `XGBoost` on large datasets, and a 2x time-performance on small-medium datasets. Because we have a few categorical variables in our data, it might be interesting to see if we can obtain any performance gains when compared to `XGBoost`.","d9b842be":"### Missing Data\n\nA good place to start is making sure that we don't have any missing data - either empty columns, NaN values, or \"hidden\" missing data (data which is present in the columns, but doesn't correspond to any tangible information)","8d7dfa51":"### XGBoost","aadcfd05":"The goal of this challenge is to predict taxi cab trip durations in NYC given some historical data. This notebook will outline a principled approach to building a machine learning model in the context of this challenge.\n\nWe will **not** be using external data sources such as the OSRM distance data, weather data, etc. All feature selection and feature engineering will be performed on the raw supplied data.\n\nI have decided to test two tree-based algorithms - the infamous `XGBoost`, and the slightly less talked about `CatBoost`. We're aiming to break the 0.4 barrier on the public Kaggle test set.\n\nThe notebook is organized into 4 sections:\n\n 1. [Exploratory Analysis and Feature Engineering](#eafe)  \n  1.1 [Missing Data](#missingdata)  \n  1.2 [Target Variable Distribution](#tvd)  \n  1.3 [Categorical Data](#catdata)  \n  1.4 [`datetime` Data](#dtdata)  \n  1.5 [Feature Correlation](#featcor)  \n  1.6 [Geospatial Clustering](#clustering)\n 2. [Modeling and Cross Validation](#Modeling)  \n  2.1 [XGBoost](#XGBoost)  \n  2.2 [CatBoost](#CatBoost)\n 3. [Submission](#Submission)\n 4. [Discussion](#Discussion)","c386883b":"The results are in! Unforunately we didn't break .4 on the test set, but we came quite close.\n\n`XGBoost`\n![XGBoost](.\/results\/xgboost_score.png)\n`CatBoost`\n![CatBoost](.\/results\/catboost_score.png)\n"}}