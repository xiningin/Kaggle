{"cell_type":{"36567b5c":"code","47435929":"code","a7643ce5":"code","79a5ba4a":"code","da328174":"code","80460b22":"code","87ae5964":"code","1673e23a":"code","84ad632b":"code","af456217":"code","8dd1cb52":"code","c6d77699":"code","33e2c3a9":"code","5b9eba09":"code","2c75a53d":"code","eb0330cd":"code","baae7930":"code","18dada38":"code","e9c269e3":"code","26a8ef3b":"markdown","9c801c02":"markdown","fb054e62":"markdown","59373bd2":"markdown","42b476be":"markdown","9e4c6e92":"markdown","23e330ef":"markdown","0ba24e19":"markdown","981e1b0f":"markdown","7d882f6f":"markdown","d1a27003":"markdown","84383a3f":"markdown"},"source":{"36567b5c":"import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom functools import partial\nimport tensorflow as tf\nimport keras","47435929":"import tensorflow_datasets as tfds\n\ndataset, info = tfds.load('tf_flowers', as_supervised=True, with_info=True)","a7643ce5":"info.splits","79a5ba4a":"class_names = info.features['label'].names\nprint(class_names)","da328174":"n_classes = info.features['label'].num_classes\nprint(n_classes)","80460b22":"dataset_size = info.splits['train'].num_examples\nprint(dataset_size)","87ae5964":"test_set_raw, valid_set_raw, train_set_raw = tfds.load(\n    'tf_flowers',\n    split=['train[:10%]', \"train[10%:25%]\", \"train[25%:]\"],\n    as_supervised=True\n)","1673e23a":"plt.figure(figsize=(12, 10))\nindex = 0\nfor img, label in train_set_raw.take(9):\n    index += 1\n    plt.subplot(3, 3, index)\n    plt.imshow(img)\n    plt.title(f\"Class: {class_names[label]}\")\n    plt.axis(\"off\")\nplt.show()","84ad632b":"def basic_preprocess(image, label):\n    resized_image = tf.image.resize(image, [224, 224])\n    final_image = keras.applications.xception.preprocess_input(resized_image)\n    return final_image, label","af456217":"def central_crop(image):\n    shape = tf.shape(image)\n    min_dim = tf.reduce_min([shape[0], shape[1]])\n    top_crop = (shape[0] - min_dim) \/\/ 4\n    bottom_crop = shape[0] - top_crop\n    left_crop = (shape[1] - min_dim) \/\/ 4\n    right_crop = shape[1] - left_crop\n    return image[top_crop:bottom_crop, left_crop:right_crop]\n\n\ndef random_crop(image):\n    shape = tf.shape(image)\n    min_dim = tf.reduce_min([shape[0], shape[1]]) * 90 \/\/ 100\n    return tf.image.random_crop(image, [min_dim, min_dim, 3])\n\n\ndef preprocess(image, label, randomize=False):\n    if randomize:\n        cropped_img = random_crop(image)\n        cropped_img = tf.image.random_flip_left_right(cropped_img)\n    else:\n        cropped_img = central_crop(image)\n    return basic_preprocess(cropped_img, label)","8dd1cb52":"batch_size = 32\ntrain_set = train_set_raw.shuffle(1000).repeat()\ntrain_set = train_set.map(partial(preprocess, randomize=True)).batch(batch_size).prefetch(1)\nvalid_set = valid_set_raw.map(preprocess).batch(batch_size).prefetch(1)\ntest_set = test_set_raw.map(preprocess).batch(batch_size).prefetch(1)","c6d77699":"plt.figure(figsize=(12, 12))\nfor x_batch, y_batch in train_set.take(1):\n    for index in range(9):\n        plt.subplot(3, 3, index+1)\n        plt.imshow(x_batch[index]\/2 + 0.5)\n        plt.title(f\"Class: {class_names[y_batch[index]]}\")\n        plt.axis('off')\nplt.show()","33e2c3a9":"plt.figure(figsize=(12, 12))\nfor x_batch, y_batch in test_set.take(1):\n    for index in range(9):\n        plt.subplot(3, 3, index+1)\n        plt.imshow(x_batch[index]\/2 + 0.5)\n        plt.title(f\"Class: {class_names[y_batch[index]]}\")\n        plt.axis('off')\nplt.show()","5b9eba09":"base_model = keras.applications.xception.Xception(weights='imagenet',\n                                                  include_top=False)\n\navg = keras.layers.GlobalAveragePooling2D()(base_model.output)\n\noutput = keras.layers.Dense(n_classes, activation='softmax')(avg)\n\nmodel = keras.models.Model(inputs=base_model.input, outputs=output)","2c75a53d":"for index, layer in enumerate(base_model.layers):\n    print(f\"{index} : {layer.name}\")","eb0330cd":"for layer in base_model.layers:\n    layer.trainable = False\n\noptimizer = keras.optimizers.SGD(lr=0.2, momentum=0.9, decay=0.01)\nmodel.compile(loss='sparse_categorical_crossentropy',\n              optimizer=optimizer,\n              metrics=['accuracy'])\n\nhistory = model.fit(train_set,\n                    steps_per_epoch=int(0.75 * dataset_size \/ batch_size),\n                    validation_data = valid_set,\n                    validation_steps = int(0.15 * dataset_size \/ batch_size),\n                    epochs=5)","baae7930":"for layer in base_model.layers:\n    layer.trainable=True\n\noptimizer = keras.optimizers.SGD(lr=0.01, momentum=0.9,\n                                 nesterov=True, decay=0.001)\n\nmodel.compile(loss='sparse_categorical_crossentropy',\n              optimizer=optimizer,\n              metrics=['accuracy'])\n\nhistory = model.fit(train_set,\n                    steps_per_epoch=int(0.75 * dataset_size \/ batch_size),\n                    validation_data = valid_set,\n                    validation_steps=int(0.15 * dataset_size \/ batch_size),\n                    epochs=40)\n","18dada38":"results = model.evaluate(test_set)","e9c269e3":"print(f\"Accuracy: {results[1]}\")","26a8ef3b":"### Loading the Xception network with imagenet weights","9c801c02":"### A look into the data","fb054e62":"### Loading the flowers dataset from tensorflow datasets","59373bd2":"### Additional preprocessing","42b476be":"### Model evaluation","9e4c6e92":"### Testing set","23e330ef":"### Basic Preprocessing","0ba24e19":"### Training set","981e1b0f":"### Complete training with slower learning rate","7d882f6f":"### Splitting into train, test and validation sets","d1a27003":"### Import necessary packages and libraries","84383a3f":"### Custom layer training"}}