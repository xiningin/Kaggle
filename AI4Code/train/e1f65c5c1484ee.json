{"cell_type":{"5e61b0f6":"code","f76d208e":"code","3c4ca1b9":"code","adab825a":"code","aa8e7db3":"code","36f364f4":"code","09a4181f":"code","fa6496b9":"code","6932b761":"code","71cc6edc":"code","04d29333":"code","7a4707d4":"code","c0eab6d6":"code","7be1b768":"code","4ed35ad5":"code","ec394a9d":"code","38f5077c":"code","8119fe39":"code","952c2c9d":"code","0b6a9d55":"code","fb1e0634":"code","8304f559":"code","e81b13ee":"code","67a965aa":"code","5e69481d":"code","8e7b6bc0":"code","79291cdd":"code","c0987e4d":"code","e2a4bbb6":"code","31e1ca25":"code","c038e5ce":"code","c1fa6695":"code","39ea22de":"code","21d6e918":"code","8665cfcf":"code","12e09082":"code","f371a9a8":"code","e867f766":"code","a6fd0319":"code","e630a1c7":"code","ecfed0e3":"code","2261319e":"code","fd53431c":"code","be68fecf":"code","265f54a8":"code","8aab48ba":"code","d6bfb36c":"markdown","2c9a5bc5":"markdown","f3928b7d":"markdown","6f6a7b81":"markdown","42bf6df7":"markdown","f1be68f8":"markdown","cc0a9053":"markdown","a0403835":"markdown","be79a292":"markdown","18ca01f7":"markdown","b0476f3c":"markdown","95080dc9":"markdown","2e442201":"markdown","04e8a9f6":"markdown","56943632":"markdown","dbc1abb6":"markdown","7fd636d7":"markdown","ccac8ded":"markdown","08241e0b":"markdown","66289a52":"markdown","16901e0c":"markdown","7842c55c":"markdown","c7559a25":"markdown","328bcf28":"markdown","a880eec9":"markdown","32edcb41":"markdown","8b07deeb":"markdown","61c5a2c4":"markdown","2e14d1d9":"markdown","9ae30dd7":"markdown","fc0540f9":"markdown","d56a8d95":"markdown","0ad46285":"markdown","c85f77a1":"markdown","063edb18":"markdown","3c9b7b36":"markdown"},"source":{"5e61b0f6":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","f76d208e":"import matplotlib.pyplot as plt\nimport seaborn as sns\nimport umap\n\nfrom sklearn.preprocessing import StandardScaler,RobustScaler,QuantileTransformer\nfrom sklearn.ensemble import RandomForestClassifier, VotingClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import LinearSVC\nfrom sklearn.model_selection import cross_val_score, StratifiedKFold,RandomizedSearchCV\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.pipeline import Pipeline\n\n\nfrom imblearn.over_sampling import SMOTENC\nfrom xgboost import XGBClassifier\nfrom catboost import CatBoostClassifier\n","3c4ca1b9":"df_train = pd.read_csv('..\/input\/tabular-playground-series-dec-2021\/train.csv')\ndf_test = pd.read_csv('..\/input\/tabular-playground-series-dec-2021\/test.csv')","adab825a":"def reduce_mem_usage(df, verbose=True):\n    \"\"\"\n    Reduce memory usage by downcasting features.\n    \n    Args:\n        df (pd.DataFrame): DataFrame with features.\n        verbose (bool): Determines verbosity of output.\n    Returns:\n        df (pd.DataFrame): DataFrame with reduces memory usage, due to smaller datatypes.\n    \"\"\"\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() \/ 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) \/ start_mem))\n    return df","aa8e7db3":"df_train = reduce_mem_usage(df_train)\ndf_test = reduce_mem_usage(df_test)","36f364f4":"def prep_test_data(df, engineer_features = True):\n    \"\"\"Transforms the test data to match model\/pipeline expectations\"\"\"\n    \n    df = df.drop(['Id','Soil_Type7', 'Soil_Type15'], axis = 1)\n    if engineer_features:\n        df[\"mean\"] = df.mean(axis=1)\n        df[\"std\"] = df.std(axis=1)\n        df[\"min\"] = df.min(axis=1)\n        df[\"max\"] = df.max(axis=1)\n    return df","09a4181f":"df_train.info()","fa6496b9":"df_train.isna().any()","6932b761":"df_train.duplicated().any()","71cc6edc":"df_train.describe().T","04d29333":"df_train = df_train.drop(['Id','Soil_Type7','Soil_Type15'], axis = 1)","7a4707d4":"fig, ax = plt.subplots(figsize = (15,12))\nsns.heatmap(df_train.corr(), ax = ax)","c0eab6d6":"features_train = df_train.drop('Cover_Type', axis = 1)\ntarget_train = df_train['Cover_Type']","7be1b768":"features_plot = features_train.sample(frac = 0.4) #using a subsample to speed up plotting\nprint(features_plot.shape)\nfig, axis = plt.subplots(nrows = 13,ncols = 4, figsize = (20,25))\naxis = axis.flatten()\n\nfor i,col in enumerate(features_plot):\n    sns.boxplot(x = features_plot[col], ax = axis[i])\n    axis[i].set_title(col)\nfig.tight_layout()","4ed35ad5":"print(df_train['Cover_Type'].value_counts())\nfig, ax = plt.subplots(figsize = (10,8))\nsns.countplot(x = df_train['Cover_Type'])","ec394a9d":"binary_cols = features_train.columns[10:]\nprint(len(binary_cols))","38f5077c":"fig, axis = plt.subplots(nrows = 7,ncols = 6, figsize = (20,25))\naxis = axis.flatten()\n\nfor i,col in enumerate(binary_cols):\n    sns.countplot(x = features_plot[col],hue = target_train, ax = axis[i])\n    axis[i].set_title(col)\n    sns.despine()\nfig.tight_layout()\n","8119fe39":"# features_plot = features_train.sample(frac = 0.2, random_state = 13) #utilizing a reduced set due to memory issues.\n# target_plot = target_train.iloc[features_plot.index]\n\n# standard_embedding = umap.UMAP(random_state = 13).fit_transform(features_plot)\n# plt.scatter(standard_embedding[:, 0], standard_embedding[:, 1], c=target_plot, s=0.1, cmap='Spectral')","952c2c9d":"dt_baseline = DecisionTreeClassifier(random_state = 13)","0b6a9d55":"cross_val_score(estimator=dt_baseline,\n                X=features_train,y=target_train, \n                scoring = 'accuracy', \n                n_jobs = 3, cv = 3, verbose = 1)\n","fb1e0634":"# dt_baseline.fit(features_train, target_train)\n# features_test = prep_test_data(df_test)\n# print(features_test.shape)\n# preds_baseline  = dt_baseline.predict(features_test)\n# df_submission_base = pd.DataFrame({'id': df_test.loc[:,'Id'], 'Cover_Type':preds_baseline})\n# print(df_submission_base.head())\n# df_submission_base.to_csv('submission.csv', index = None)","8304f559":"rf_baseline = RandomForestClassifier(n_estimators = 25, random_state = 13, n_jobs = -1)\ncross_val_score(estimator = rf_baseline,\n                X=features_train,y=target_train, \n                scoring = 'accuracy', \n                n_jobs = 3, cv = 3, verbose = 1)","e81b13ee":"# rf_baseline = RandomForestClassifier(n_estimators = 50, random_state = 13, n_jobs = -1)\n# rf_baseline.fit(features_train, target_train)\n# features_test = prep_test_data(df_test)\n# print(features_test.shape)\n# preds_baseline_rf  = rf_baseline.predict(features_test)\n# df_submission_base_rf = pd.DataFrame({'id': df_test.loc[:,'Id'], 'Cover_Type':preds_baseline_rf})\n# print(df_submission_base_rf.head())\n# df_submission_base_rf.to_csv('submission.csv', index = None) ","67a965aa":"features_train[\"mean\"] = features_train.mean(axis=1)\nfeatures_train[\"std\"] = features_train.std(axis=1)\nfeatures_train[\"min\"] = features_train.min(axis=1)\nfeatures_train[\"max\"] = features_train.max(axis=1)","5e69481d":"#features_train.drop(['mean', 'std', 'min', 'max'], axis = 1, inplace = True) #in case the summary stats don't work well","8e7b6bc0":"rf_baseline = RandomForestClassifier(n_estimators = 25, random_state = 13, n_jobs = -1)\ncross_val_score(estimator = rf_baseline,\n                X=features_train,y=target_train, \n                scoring = 'accuracy', \n                n_jobs = 3, cv = 3, verbose = 1)","79291cdd":"binary_cols_idx = [features_train.columns.get_loc(binary_col) for binary_col in binary_cols]\nminority_class_target = target_train[target_train == 5]\nminority_class_feat = features_train.loc[minority_class_target.index,:]","c0987e4d":"features_train = features_train.drop(minority_class_target.index, axis = 0)\ntarget_train = target_train.drop(minority_class_target.index, axis = 0)","e2a4bbb6":"sm = SMOTENC(random_state = 13, categorical_features = binary_cols_idx, sampling_strategy = 'minority')","31e1ca25":"features_train_resample = features_train.sample(frac = 0.05)\ntarget_train_resample = target_train[features_train_resample.index]","c038e5ce":"features_train_smote, target_train_smote = sm.fit_resample(features_train_resample, target_train_resample)","c1fa6695":"# drop the sampled data to re-add resampled\nfeatures_train = features_train.drop(features_train_resample.index, axis = 0)\ntarget_train = target_train.drop(target_train_resample.index, axis = 0)\nfeatures_train.append(features_train_smote, ignore_index = True)\ntarget_train.append(target_train_smote, ignore_index = True)","39ea22de":"features_train.shape, target_train.shape","21d6e918":"scaler = RobustScaler()\nfeatures_train_unscaled = features_train.copy()\nfeatures_train = scaler.fit_transform(features_train)","8665cfcf":"target_train = target_train.to_numpy()","12e09082":"model = CatBoostClassifier(objective = 'MultiClass', task_type = 'GPU')\n\ncatboost_scores = []\nkf = StratifiedKFold(n_splits = 3, shuffle = True, random_state = 13)\nfor fold, (train_idx, valid_idx) in enumerate(kf.split(X = features_train, y = target_train)):\n\n    print(f\"Fold {fold + 1}\")\n    features_train_fold = features_train[train_idx, :]\n    features_valid_fold = features_train[valid_idx, :]\n    target_train_fold = target_train[train_idx]\n    target_valid_fold = target_train[valid_idx]\n    \n    model.fit(features_train_fold, target_train_fold,\n          early_stopping_rounds = 200,\n          eval_set = [(features_valid_fold, target_valid_fold)],\n          verbose = 0)\n    \n    preds_valid = model.predict(features_valid_fold)\n    acc = accuracy_score(target_valid_fold,  preds_valid)\n    catboost_scores.append(acc)\n    print(f\"Fold {fold + 1}, acc: {acc:.5f}\")\n    \nprint(\"Mean Accuracy:\", np.mean(catboost_scores))","f371a9a8":"model = CatBoostClassifier(objective = 'MultiClass', task_type = 'GPU')\nmodel.fit(features_train, target_train,\n          early_stopping_rounds = 200,\n          verbose = 0)\n\n#prep test data\nfeatures_test = prep_test_data(df_test)\nfeatures_test = scaler.transform(features_test)\n\npreds_catboost  = model.predict(features_test)\n","e867f766":"df_submission_catboost = pd.DataFrame({'id': df_test.loc[:,'Id'], 'Cover_Type': preds_catboost.flatten()})\nprint(df_submission_catboost.head())\ndf_submission_catboost.to_csv('submission.csv', index = None) ","a6fd0319":"model = XGBClassifier(objective = 'multi:softmax', \n                      eval_metric = 'mlogloss', \n                      tree_method = 'gpu_hist', \n                      predictor = 'gpu_predictor')\n\nxgb_scores = []\nkf = StratifiedKFold(n_splits = 3, shuffle = True, random_state = 13)\nfor fold, (train_idx, valid_idx) in enumerate(kf.split(X = features_train, y = target_train)):\n\n    print(f\"Fold {fold + 1}\")\n    features_train_fold = features_train[train_idx, :]\n    features_valid_fold = features_train[valid_idx, :]\n    target_train_fold = target_train[train_idx]\n    target_valid_fold = target_train[valid_idx]\n    \n    model.fit(features_train_fold, target_train_fold,\n          early_stopping_rounds = 200,\n          eval_set = [(features_valid_fold, target_valid_fold)],\n          verbose = 0)\n    \n    preds_valid = model.predict(features_valid_fold)\n    acc = accuracy_score(target_valid_fold,  preds_valid)\n    xgb_scores.append(acc)\n    print(f\"Fold {fold + 1}, acc: {acc:.5f}\")\n    \nprint(\"Mean Accuracy:\", np.mean(xgb_scores))","e630a1c7":"model = XGBClassifier(objective = 'multi:softmax', \n                      eval_metric = 'mlogloss', \n                      tree_method = 'gpu_hist', \n                      predictor = 'gpu_predictor')\nmodel.fit(features_train, target_train,\n          verbose = 0)\n\n#prep test data\nfeatures_test = prep_test_data(df_test)\nfeatures_test = scaler.transform(features_test)\n\npreds_xgb  = model.predict(features_test)\ndf_submission_xgb = pd.DataFrame({'id': df_test.loc[:,'Id'], 'Cover_Type': preds_xgb.flatten()})\nprint(df_submission_xgb.head())\ndf_submission_xgb.to_csv('submission_xgb.csv', index = None) ","ecfed0e3":"print(preds_xgb)","2261319e":"# rf_clf = RandomForestClassifier(n_estimators = 50, random_state = 13, n_jobs = -1)\n# rf_searchspace = {'max_depth':[None, 5,  15],\n#                   'min_samples_split': [2,4,6,8,10],\n#                   'min_samples_leaf': [1,2],\n#                   'max_features': ['auto','log2'],\n#                   'bootstrap': [False, True],\n#                   'class_weight': ['balanced', None]}\n# search_cv = RandomizedSearchCV(estimator = rf_clf, \n#                                param_distributions = rf_searchspace, \n#                                cv = 3,\n#                                scoring = 'accuracy',\n#                                n_iter = 15,\n#                                verbose = 1,\n#                                n_jobs = 2\n#                               )\n# search_cv.fit(features_train_unscaled,target_train)\n# print(search_cv.best_score_)\n# print(search_cv.best_estimator_)","fd53431c":"class CatBoostClassifierInt(CatBoostClassifier):\n    def predict(self, data, prediction_type='Class', ntree_start=0, ntree_end=0, thread_count=1, verbose=None, task_type=\"CPU\"):\n        predictions = self._predict(data, prediction_type, ntree_start, ntree_end, thread_count, verbose, 'predict', task_type)\n\n        # This line is the only change\n        return np.asarray(predictions, dtype=np.int64).ravel()","be68fecf":"rf_clf = RandomForestClassifier(n_estimators = 50, random_state = 13, n_jobs = -1)\nxgb_clf = XGBClassifier(objective = 'multi:softmax', \n                      eval_metric = 'mlogloss', \n                      tree_method = 'gpu_hist', \n                      predictor = 'gpu_predictor')\ncatboost_clf = CatBoostClassifierInt(objective = 'MultiClass', \n                                   task_type = 'GPU')","265f54a8":"voting_clf = VotingClassifier(estimators = [('rf', rf_clf), ('xgb', xgb_clf), ('catboost', catboost_clf)])\ncross_val_score(estimator = voting_clf,\n                X = features_train, y = target_train, \n                scoring = 'accuracy', \n                n_jobs = 1, cv = 3, verbose = 0)","8aab48ba":"voting_clf.fit(features_train, target_train)\n#prep test data\nfeatures_test = prep_test_data(df_test)\nfeatures_test = scaler.transform(features_test)\n\npreds_voting = voting_clf.predict(features_test)\ndf_submission_voting = pd.DataFrame({'id': df_test.loc[:,'Id'], 'Cover_Type': preds_voting})\nprint(df_submission_voting.head())\ndf_submission_voting.to_csv('submission_voting.csv', index = None) ","d6bfb36c":"## Adressing class imbalances\n\nI am using SMOTENC here. However, due to the size of the dataset I am running into memory issues. I decided to only use SMOTENC on a fraction of the data,oversampling the class with only few samples. I am however dropping the class with only one sample.","2c9a5bc5":"First observations concerning the features:\n- Soil_Type7 and Soil_Type15 are always 0. They do not provide information -> drop\n- Hillshade should be a pixel value, but there are negatives which shouldn't be possbile. Probably due to the synthetic dataset.\n- There are some distance values, those technically shouldn't be < 0 (negative), but they are. Probably due to the synthetic dataset.\n- Basically all Soil_Types are mostly 0 with few 1. Meaning they are most not present. These could be strongly differenciating features between classes.","f3928b7d":"## EDA","6f6a7b81":"### Stats overview","42bf6df7":"The target is not only multiclass but also very imbalanced. Cover_Type 1&2 dominate, 3,6,7 are somewhat frequent, 4 and especially 5 are extremely rare.","f1be68f8":"Dropping ID, and Soil_Type7&15","cc0a9053":"## Target Counts for each binary Feature","a0403835":"## Scaling\n\nAs most models don't do well with unscaled data I will use a RobustScaler (due to some features showing outliers) to prepare the data. Only the Random Forest will be trained on the unscaled data.","be79a292":"It looks like there aren't clear clusters, at least not with this method. I attached the result so you don't have to run the cell.\n\n![image.png](attachment:d1f589f5-8cc0-4953-b601-709f667f6925.png)","18ca01f7":"The extremely simple baseline model already provides decent accuracy with around 0.94. Let's make it the first submission.","b0476f3c":"## Loading the Data","95080dc9":"General overview.","2e442201":"The Soil_Types are very unevenly distributed as noted before. Slope and Distances are left skewed and shades right skewed. Elevation and Aspect look to be close to Normal or T-Distributions.","04e8a9f6":"XGBoost also reaches very good performance and works much faster than CatBoost.\nI will also submit it's predictions and consider it for a voting classifier.","56943632":"## Tuned Random Forest","dbc1abb6":"Check for NAs and duplicates.","7fd636d7":"No duplicates or NAs. Very good.","ccac8ded":"Random Forest Baseline does a bit better than the Decision Tree which was expected. Let's make another submission.","08241e0b":"# EDA and Non-NN Models\n\n**This notebook is still a work-in-progress. I will add more documentation on the way. However feel free to check it out, comment etc.**\n\nThis months Playground problem is a multiclass-classification task, where models are evaluated on accuracy. In this notebook I am tackling the problem without Neural Networks. I will implement a seperate notebook for that later on. The data is a synthesized version of data about forest cover first mentioned in a paper by Blackard and Dean in 1999. I recommend checking out the paper to gain some insight into the original dataset and to get a nice view of how far ML has come. Their best model has an acc. of ~70% and they utilize wide neural nets, which was quite common in that time.","66289a52":"## UMAP\n\nI thought it might help to get a sense of how the different classes are clustered, whether they are all very close to each other or not. I decided to give umap a try.\n\n**WARNING: Cell takes long**","16901e0c":"## Baseline Models\n\nLet's start with simple baseline models before feature engineering to get a sense of the accuracy ranges we aim for.","7842c55c":"## CatBoost\n\n","c7559a25":"The results are not as good as with just the catboost. I will still submit them as they might generalize better.","328bcf28":"This submission score 0.9498. Not bad given that the model wasn't tuned nor any feature engineering done.","a880eec9":"Strong neg correlation between wilderness area 1 and 3 -> possible collinearity? Some small-medium sized correlations between soil types and elevation. Strong correlation between elevation and cover type (expected given the range of elevation and the biology of trees)","32edcb41":"## Basic Feature Engineering\n\nThe most basic feature engineering is to use summary statistics of the data as a feature. Let's try it and see if this changes baseline performance at all.","8b07deeb":"## Voting Classifier\n\nI intend to combine the Random Forest, CatBoost and maybe others to see if they work even better together.\n\nI already found out that CatBoost doesn't work well in the VotingClassifier from sklearn due to the shape of predictions. This can be solved by Subclassing the CatBoost model to change the outputs.","61c5a2c4":"This looks pretty promising. I am submitting the models predictions.","2e14d1d9":"Some binary features show different distributions for the classes, there are however no very distinct features that for example determine a class completely.","9ae30dd7":"### Split Features and Target","fc0540f9":"## XGBoost","d56a8d95":"## Imports","0ad46285":"## Reducing the Memory Usage\n\nThis code (except for the docstring) was provided by Luca Massaron in this discussion [thread](https:\/\/www.kaggle.com\/c\/tabular-playground-series-dec-2021\/discussion\/291844)","c85f77a1":"### Target distribution","063edb18":"### Correlation","3c9b7b36":"The additional features didn't move performance much in either direction."}}