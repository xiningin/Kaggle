{"cell_type":{"9fe0b316":"code","3e11635e":"code","fbcdd5cd":"code","93459218":"code","d805acea":"code","056ba4bb":"code","3850f21b":"code","b490dd50":"code","58d45188":"code","70cd329d":"code","8b8b09cf":"code","755e1a74":"code","df3879e9":"code","e38ad58f":"code","57bfc36e":"code","cf4c21bd":"code","c81df52a":"code","38816be1":"code","643f4f59":"code","c048af34":"code","5ab0164c":"code","e1ea3b3c":"code","e24fd746":"code","7fb226c8":"code","d678a8b9":"code","c2bbb6ad":"code","5b43b6cb":"code","6d4533b6":"code","118d7de5":"code","049e4ca9":"code","b67dc401":"code","2f90f2b4":"code","8af33fb3":"code","568baa8a":"code","0cf59967":"code","207d3ee1":"code","761f82aa":"code","ee40e7c5":"code","bd87d2a9":"code","a8055ad2":"markdown","b3f0d900":"markdown","114c47f4":"markdown","53474d13":"markdown","1f51526e":"markdown","90d5030c":"markdown","60678d2c":"markdown","c02b3251":"markdown","89a1e3d1":"markdown","22cd8a4c":"markdown","4b98103d":"markdown","49cb616e":"markdown","3451b411":"markdown","1e1d054d":"markdown","c9cea060":"markdown","db1456b9":"markdown","93de5495":"markdown","91b23f79":"markdown","03a5a5ff":"markdown","e934599b":"markdown","97e987be":"markdown","a77d46a8":"markdown","9e677a77":"markdown","82142c1e":"markdown","4bf06f54":"markdown","ca4b6827":"markdown","1cb084e0":"markdown","7f97ae9d":"markdown","9f2fec06":"markdown","68d23013":"markdown","bc4da0e8":"markdown"},"source":{"9fe0b316":"!pip install -q langdetect\n!pip install -q textstat","3e11635e":"import numpy as np\nimport pandas as pd\n\npd.set_option('display.max_rows', 100)\npd.set_option('display.max_columns', 100)\npd.set_option('display.width', 1000)\n\nfrom collections import defaultdict,Counter\nfrom multiprocessing import Pool\n\nimport textstat\nfrom statistics import *\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nimport plotly.figure_factory as ff\nimport plotly.express as px\nimport plotly.offline as py\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom nltk.stem import WordNetLemmatizer, PorterStemmer\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\n\nfrom langdetect import detect\n\nfrom wordcloud import WordCloud, STOPWORDS\n\nfrom scipy.stats import norm, kurtosis, skew\n\nfrom tqdm import tqdm\ntqdm.pandas() # To have a progress bar using progress_apply\n\nimport string, json, nltk, gc","fbcdd5cd":"nltk.download('stopwords')\nstop = set(stopwords.words('english'))\nplt.style.use('seaborn')","93459218":"TRAIN_UNINTENDED_BIAS = \"..\/input\/jigsaw-multilingual-toxic-comment-classification\/jigsaw-unintended-bias-train.csv\"\nTRAIN_TOXICITY = \"..\/input\/jigsaw-multilingual-toxic-comment-classification\/jigsaw-toxic-comment-train.csv\"\n\nVALIDATION = \"..\/input\/jigsaw-multilingual-toxic-comment-classification\/validation.csv\"\n\nTEST = \"..\/input\/jigsaw-multilingual-toxic-comment-classification\/test.csv\"","d805acea":"train_1_df = pd.read_csv(TRAIN_UNINTENDED_BIAS)\ntrain_2_df = pd.read_csv(TRAIN_TOXICITY)\n\nvalidation_df = pd.read_csv(VALIDATION)\n\ntest_df = pd.read_csv(TEST)","056ba4bb":"train_1_df.head()","3850f21b":"train_2_df.head()","b490dd50":"fig, ax = plt.subplots(1, 3, figsize=(20, 5))\n\nsns.countplot(train_1_df['toxic'].astype(int), ax=ax[0])\nax[0].set_title('Unintended bias dataset')\n\nsns.countplot(train_2_df['toxic'].astype(int), ax=ax[1])\nax[1].set_title('Toxicity dataset')\n\nsns.countplot(validation_df['toxic'].astype(int), ax=ax[2])\nax[2].set_title('Validation dataset')\n\nfig.suptitle('Toxicity distribution across datasets', fontweight='bold', fontsize=14)\n\nfig.show()","58d45188":"fig, ax = plt.subplots(1, 2, figsize=(15, 5))\n\nsns.countplot(validation_df['lang'], ax=ax[0])\nax[0].set_title('Validation')\n\nsns.countplot(test_df['lang'], ax=ax[1])\nax[1].set_title('Test')\n\nfig.suptitle('Language distribution across datasets', fontweight=\"bold\", fontsize=14)\nfig.show()","70cd329d":"fig, ax = plt.subplots(1,2, figsize=(15, 5))\n\nsns.distplot(train_1_df[train_1_df['toxic']==0]['comment_text'].str.len(), axlabel=\"Non toxic\", ax=ax[0])\nsns.distplot(train_1_df[train_1_df['toxic']==1]['comment_text'].str.len(), axlabel=\"Toxic\", ax=ax[1])\n\nfig.show()\n\nfig.suptitle(\"Distribution of number of No: Characters in Comments - Unintended bias dataset\", fontsize=14)","8b8b09cf":"fig, ax = plt.subplots(1,2, figsize=(15, 5))\n\nsns.distplot(train_2_df[train_2_df['toxic']==0]['comment_text'].str.len(), axlabel=\"Non toxic\", ax=ax[0])\nsns.distplot(train_2_df[train_2_df['toxic']==1]['comment_text'].str.len(), axlabel=\"Toxic\", ax=ax[1])\n\nfig.show()\n\nfig.suptitle(\"Distribution of number of No: Characters in Comments - Toxicity dataset\", fontsize=14)","755e1a74":"train_1_df[train_1_df['comment_text'].str.len() > 850][['comment_text', 'toxic']].sample(n=100).reset_index(drop=True)","df3879e9":"train_2_df[train_2_df['comment_text'].str.len() > 2000][[\"comment_text\", \"toxic\"]].sample(n=100).reset_index(drop=True)","e38ad58f":"fig, ax = plt.subplots(1,3, figsize=(15, 5))\n\nsns.distplot(validation_df[validation_df['toxic']==0]['comment_text'].str.len(), axlabel=\"Validation - Non toxic\", ax=ax[0])\nsns.distplot(validation_df[validation_df['toxic']==1]['comment_text'].str.len(), axlabel=\"Validation - Toxic\", ax=ax[1])\nsns.distplot(test_df['content'].str.len(), axlabel=\"Test\", ax=ax[2])\n\nfig.show()\n\nfig.suptitle(\"Distribution of number of No: Characters in Comments - Toxicity dataset\", fontsize=14)","57bfc36e":"validation_df[validation_df['comment_text'].str.len() > 1000][[\"comment_text\", 'lang', \"toxic\"]].sample(n=100).reset_index(drop=True)","cf4c21bd":"test_df[test_df['content'].str.len() > 1000][[\"content\", 'lang']].sample(n=100).reset_index(drop=True)","c81df52a":"fig, ax = plt.subplots(1, 2, figsize=(15, 5))\n\nvalidation_df[\"character_count\"] = validation_df['comment_text'].apply(lambda x: len(x))\ntest_df['character_count'] = test_df['content'].apply(lambda x: len(x))\n\ntest_df['character_count'] = test_df['character_count'].apply(lambda x: 1000 if x > 1000 else x) # Nicer formatting \n\nsns.boxplot('lang', 'character_count', data=validation_df, ax=ax[0])\nsns.boxplot('lang', 'character_count', data=test_df, ax=ax[1])\n\nfig.show()\n\nfig.suptitle('Distribution of # of characters for each language')","38816be1":"fig, ax = plt.subplots(1,2, figsize=(15, 5))\n\nsns.distplot(train_1_df[train_1_df['toxic']==0]['comment_text'].str.split().str.len(), axlabel=\"Non toxic\", ax=ax[0])\nsns.distplot(train_1_df[train_1_df['toxic']==1]['comment_text'].str.split().str.len(), axlabel=\"Toxic\", ax=ax[1])\n\nfig.show()\n\nfig.suptitle(\"Distribution of number of No: Words in Comments - Unintended bias\", fontsize=14)","643f4f59":"fig, ax = plt.subplots(1,2, figsize=(15, 5))\n\nsns.distplot(train_2_df[train_2_df['toxic']==0]['comment_text'].str.split().str.len(), axlabel=\"Non toxic\", ax=ax[0])\nsns.distplot(train_2_df[train_2_df['toxic']==1]['comment_text'].str.split().str.len(), axlabel=\"Toxic\", ax=ax[1])\n\nfig.show()\n\nfig.suptitle(\"Distribution of number of No: Words in Comments - Toxicity\", fontsize=14)","c048af34":"def whisker_plot_stats(train):\n    ## Number of words \n    train[\"num_words\"] = train[\"comment_text\"].progress_apply(lambda x: len(str(x).split()))\n\n    ## Number of unique words \n    train[\"num_unique_words\"] = train[\"comment_text\"].progress_apply(lambda x: len(set(str(x).split())))\n\n    ## Number of characters \n    train[\"num_chars\"] = train[\"comment_text\"].progress_apply(lambda x: len(str(x)))\n\n    ## Number of stopwords \n    train[\"num_stopwords\"] = train[\"comment_text\"].progress_apply(lambda x: len([w for w in str(x).lower().split() if w in STOPWORDS]))\n\n    ## Number of punctuations \n    train[\"num_punctuations\"] =train['comment_text'].progress_apply(lambda x: len([c for c in str(x) if c in string.punctuation]) )\n\n    ## Number of title case words\n    train[\"num_words_upper\"] = train[\"comment_text\"].progress_apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n\n    # Number of title case words\n    train[\"num_words_title\"] = train[\"comment_text\"].progress_apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\n\n    # Average length of the words\n    train[\"mean_word_len\"] = train[\"comment_text\"].progress_apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n    \n    return train","5ab0164c":"print('Train 1...')\ntrain_1_df = whisker_plot_stats(train_1_df)\nprint('Train 2...')\ntrain_2_df = whisker_plot_stats(train_2_df)","e1ea3b3c":"train_1_df['num_words'].loc[train_1_df['num_words']>100] = 100\ntrain_1_df['num_punctuations'].loc[train_1_df['num_punctuations']>10] = 10 \ntrain_1_df['num_chars'].loc[train_1_df['num_chars']>350] = 350 \ntrain_1_df['toxic'] = train_1_df['toxic'].apply(lambda x: 1 if x > 0.5 else 0)\n\ntrain_2_df['num_words'].loc[train_2_df['num_words']>100] = 100\ntrain_2_df['num_punctuations'].loc[train_2_df['num_punctuations']>10] = 10 \ntrain_2_df['num_chars'].loc[train_2_df['num_chars']>350] = 350 \n\n\n# figure related code\nfig, ax = plt.subplots(1, 2, figsize=(15, 5))\nfig.suptitle('Distribution of # words in toxicity dataset', fontsize=14, fontweight='bold')\n\nsns.boxplot(x='toxic', y='num_words', data=train_1_df, ax=ax[0])\nax[0].set_title('Unintended bias dataset')\n\nsns.boxplot(x='toxic', y='num_words', data=train_2_df, ax=ax[1])\nax[1].set_title('Toxicity dataset')\n\nfig.show()","e24fd746":"train_1_df[train_1_df['num_words'] >= 100]['comment_text'].sample(n=100).reset_index(drop=True)","7fb226c8":"train_2_df[train_2_df['num_words'] >= 100]['comment_text'].sample(n=100).reset_index(drop=True)","d678a8b9":"def preprocess_comments(df, stop=stop, n=1, col='comment_text'):\n    new_corpus=[]\n    \n    stem = PorterStemmer()\n    lem = WordNetLemmatizer()\n    \n    for text in tqdm(df[col], total=len(df)):\n        words = [w for w in word_tokenize(text) if (w not in stop)]\n       \n        words = [lem.lemmatize(w) for w in words if(len(w)>n)]\n     \n        new_corpus.append(words)\n        \n    new_corpus = [word for l in new_corpus for word in l]\n    \n    return new_corpus","c2bbb6ad":"fig,ax = plt.subplots(1, 2, figsize=(15,7))\n\nfor i in range(2):\n    new = train_1_df[train_1_df['toxic']== i]\n    corpus_train = preprocess_comments(new, {})\n    \n    dic = defaultdict(int)\n    for word in corpus_train:\n        if word in stop:\n            dic[word]+=1\n            \n    top = sorted(dic.items(), key=lambda x:x[1],reverse=True)[:10] \n    x, y = zip(*top)\n    ax[i].bar(x,y)\n    ax[i].set_title(str(i))\n\nfig.suptitle(\"Common stopwords in unintented bias dataset\")","5b43b6cb":"fig, ax = plt.subplots(1, 2, figsize=(15,7))\n\nfor i in range(2):\n    new = train_2_df[train_2_df['toxic']== i]\n    corpus_train_2 = preprocess_comments(new, {})\n    \n    dic = defaultdict(int)\n    for word in corpus_train:\n        if word in stop:\n            dic[word]+=1\n            \n    top = sorted(dic.items(), key=lambda x:x[1],reverse=True)[:10] \n    x, y = zip(*top)\n    ax[i].bar(x,y)\n    ax[i].set_title(str(i))\n\nfig.suptitle(\"Common stopwords in toxicity dataset\")","6d4533b6":"fig, ax = plt.subplots(1,2,figsize=(20,12))\n\nfor i in range(2):\n    new = train_2_df[train_2_df['toxic']==i]   \n    corpus = corpus_train\n    counter = Counter(corpus)\n    most = counter.most_common()\n    x = []\n    y = []\n    \n    for word,count in most[:20]:\n        if (word not in stop) :\n            x.append(word)\n            y.append(count)\n            \n    sns.barplot(x=y, y=x, ax=ax[i])\n    ax[i].set_title(str(i))\n    \nfig.suptitle(\"Common words in toxicity dataset\")","118d7de5":"fig, ax = plt.subplots(1,2,figsize=(20,12))\n\nfor i in range(2):\n    new = train_2_df[train_2_df['toxic']==i]   \n    corpus = corpus_train_2\n    counter = Counter(corpus)\n    most = counter.most_common()\n    x = []\n    y = []\n    \n    for word,count in most[:20]:\n        if (word not in stop) :\n            x.append(word)\n            y.append(count)\n            \n    sns.barplot(x=y, y=x, ax=ax[i])\n    ax[i].set_title(str(i))\n    \nfig.suptitle(\"Common words in unintended bias dataset\")","049e4ca9":"def get_top_ngram(corpus, n=None):\n    vec = CountVectorizer(ngram_range=(n, n),stop_words=stop).fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq = sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:20]","b67dc401":"fig, ax = plt.subplots(1, 2, figsize=(15, 10))\n\nfor i in range(2):\n    new = train_1_df[train_1_df['toxic'] == i]['comment_text']\n    top_n_bigrams = get_top_ngram(new, 2)[:20]\n    x, y = map(list, zip(*top_n_bigrams))\n    sns.barplot(x=y, y=x, ax=ax[i])\n    ax[i].set_title(str(i))\n    \nfig.suptitle('Common bigrams in unintended bias dataset')","2f90f2b4":"fig, ax = plt.subplots(1,2,figsize=(15,10))\n\nfor i in range(2):\n    new = train_2_df[train_2_df['toxic'] == i]['comment_text']\n    top_n_bigrams = get_top_ngram(new, 2)[:20]\n    x, y = map(list,zip(*top_n_bigrams))\n    sns.barplot(x=y,y=x,ax=ax[i])\n    ax[i].set_title(str(i))\n    \nfig.suptitle(\"Common bigrams in toxicity dataset\")","8af33fb3":"stopwords = set(STOPWORDS)\n\ndef show_wordcloud(data, title = None,ax=None):\n    wordcloud = WordCloud(\n        background_color='white',\n        stopwords=stopwords,\n        max_words=100,\n        max_font_size=30, \n        scale=3,\n        random_state=1 \n        )\n    \n    wordcloud = wordcloud.generate(str(data))\n    ax.imshow(wordcloud,interpolation='nearest')\n    ax.axis('off')","568baa8a":"fig, ax = plt.subplots(1,2,figsize=(20,12))\n\nfor i in range(2):\n    new = train_2_df[train_2_df['toxic'] == i]['comment_text']\n    show_wordcloud(new, ax=ax[i])\n    ax[i].set_title(str(i))\n    \nfig.suptitle('Wordcloud for toxicity dataset')","0cf59967":"fig, ax = plt.subplots(1,2,figsize=(20,12))\n\nfor i in range(2):\n    new = train_1_df[train_1_df['toxic'] == i]['comment_text']\n    show_wordcloud(new, ax=ax[i])\n    ax[i].set_title(str(i))\n    \nfig.suptitle('Wordcloud for unintended bias dataset')","207d3ee1":"def plot_readability(a, b, title, bins=0.4):\n    \n    # Setting limits\n    a = a[a >= 0]\n    a = a[a <= 100]\n    b = b[b >= 0]\n    b = b[b <= 100]\n    \n    trace1 = ff.create_distplot([a, b], ['non toxic', 'toxic'], bin_size=bins, show_rug=False)\n    trace1['layout'].update(title=title)\n    \n    py.iplot(trace1, filename='Distplot')\n    \n    table_data= [[\"Statistical Measures\",\"non toxic\",'toxic'],\n                 [\"Mean\",mean(a),mean(b)],\n                 [\"Standard Deviation\",pstdev(a),pstdev(b)],\n                 [\"Variance\",pvariance(a),pvariance(b)],\n                 [\"Median\",median(a),median(b)],\n                 [\"Maximum value\",max(a),max(b)],\n                 [\"Minimum value\",min(a),min(b)]]\n    \n    trace2 = ff.create_table(table_data)\n    py.iplot(trace2, filename='Table')","761f82aa":"fre_non_toxic = np.array(train_1_df[\"comment_text\"][train_1_df[\"toxic\"].astype(int) == 0].sample(n=150000).apply(textstat.flesch_reading_ease))\nfre_toxic = np.array(train_1_df[\"comment_text\"][train_1_df[\"toxic\"].astype(int) == 1].apply(textstat.flesch_reading_ease))\n\nplot_readability(fre_non_toxic, fre_toxic, \"Flesch Reading Ease - Unintended bias dataset\", 1) ","ee40e7c5":"fre_non_toxic = np.array(train_2_df['comment_text'][train_2_df['toxic'].astype(int) == 0].apply(textstat.flesch_reading_ease))\nfre_toxic = np.array(train_2_df['comment_text'][train_2_df['toxic'].astype(int) == 1].apply(textstat.flesch_reading_ease))\n\nplot_readability(fre_non_toxic, fre_toxic, \"Flesch Reading Ease - Toxicity dataset\", 1)","bd87d2a9":"fre_non_toxic = np.array(validation_df['comment_text'][validation_df['toxic'].astype(int) == 0].apply(textstat.flesch_reading_ease))\nfre_toxic = np.array(validation_df['comment_text'][validation_df['toxic'].astype(int) == 1].apply(textstat.flesch_reading_ease))\n\nplot_readability(fre_non_toxic, fre_toxic, \"Flesch Reading Ease - Validation set\", 1)","a8055ad2":"### Toxicity distribution","b3f0d900":"## Exploratory data analysis","114c47f4":"### Most common bi-grams","53474d13":"You can also see that there are 2 additional training files that contains preprocessed texts for BERT. \nHint: Since the sentence tokenization process varies from one transformer-based model to another, I recommend doing you own tokenization. For instance, XLM-Roberta and BERT Multilingual don't use the same preprocessing. \n\nIf you want to know more about XLM-Roberta tokenization and preprocessing, check Abhishek Thakur's latest video:\nhttps:\/\/www.youtube.com\/watch?v=U51ranzJBpY","1f51526e":"**Note: I will keep updating this notebook during the competition to give insights and share my latest findings and comments. If you like my work, don't hesitate to upvote ;)**","90d5030c":"Work in progress...\n\nYou can check:\n- https:\/\/www.kaggle.com\/rftexas\/gru-lstm-rnn-101","60678d2c":"## Introduction","c02b3251":"### Readability index","89a1e3d1":"**Sources and credits**: \n- https:\/\/www.kaggle.com\/shahules\/complete-eda-baseline-model-0-708-lb\n\n\n**Notebooks to get started**:\n- https:\/\/www.kaggle.com\/miklgr500\/jigsaw-tpu-bert-two-stage-training\n- https:\/\/www.kaggle.com\/xhlulu\/jigsaw-tpu-xlm-roberta\n- https:\/\/www.kaggle.com\/tanulsingh077\/deep-learning-for-nlp-zero-to-transformers-bert\n- https:\/\/www.kaggle.com\/abhishek\/inference-of-bert-tpu-model-ml-w-validation","22cd8a4c":"We need to be careful. The validation set doesn't have samples in all 6 languages on which we are tested in the test set. To build a relevant validation set, we need to add translated sentences in French, Portuguese and Russian to the validation set.","4b98103d":"As we can see, the text is quite polluted with punctuations and tags such as \"\\n\". We probably want to remove that in a pre-processing phase.\n\nWe also note that the unintended bias dataset has much cleaner text than the toxicity dataset... No punctuations or weird tags misplaced. \nWe can also note that some comments are just the repetition of some insults over and over again.\n\nLet's see how the text for validation and test is formatted...","49cb616e":"As expected, insults are the most common words in toxic comments...","3451b411":"Turkish comments have usually more characters than other languages, while spanish comments are the shortest character-wise. These are pieces of information that we can use to create meta-features in a model.","1e1d054d":"### Distribution of words","c9cea060":"### Most common stop words","db1456b9":"As expected, the training set is highly imbalanced. \n\nWe note that train unintended bias has continuous toxic values while toxicity training set has categorical values (0 or 1). To come up with such a plot, I assigned a threshold to unintended bias dataset: every toxicity value higher than 0.5 is toxic (1), else 0.","93de5495":"Full credits to shahules for this very interesting feature:\nhttps:\/\/www.kaggle.com\/shahules\/complete-eda-baseline-model-0-708-lb","91b23f79":"It only takes one toxic comment to sour an online discussion. The Conversation AI team, a research initiative founded by Jigsaw and Google, builds technology to protect voices in conversation. A main area of focus is machine learning models that can identify toxicity in online conversations, where toxicity is defined as anything rude, disrespectful or otherwise likely to make someone leave a discussion. If these toxic contributions can be identified, we could have a safer, more collaborative internet.\n\nIn the previous 2018 Toxic Comment Classification Challenge, Kagglers built multi-headed models to recognize toxicity and several subtypes of toxicity. In 2019, in the Unintended Bias in Toxicity Classification Challenge, you worked to build toxicity models that operate fairly across a diverse range of conversations. This year, we're taking advantage of Kaggle's new TPU support and challenging you to build multilingual models with English-only training data.\n\nJigsaw's API, Perspective, serves toxicity models and others in a growing set of languages (see our documentation for the full list). Over the past year, the field has seen impressive multilingual capabilities from the latest model innovations, including few- and zero-shot learning. We're excited to learn whether these results \"translate\" (pun intended!) to toxicity classification. Your training data will be the English data provided for our previous two competitions and your test data will be Wikipedia talk page comments in several different languages.\n\nAs our computing resources and modeling capabilities grow, so does our potential to support healthy conversations across the globe. Develop strategies to build effective multilingual models and you'll help Conversation AI and the entire industry realize that potential.\n\n![image.png](attachment:image.png)\n","03a5a5ff":"## Loading required packages","e934599b":"Again, we can see that the toxicity dataset have some extremely long comments. ","97e987be":"### Wordcloud","a77d46a8":"## Length distribution","9e677a77":"### Language distribution","82142c1e":"Toxic comments are more readable. Probably because they are less elaborate... An insult is pretty straight forward actually.","4bf06f54":"As in the Unintended bias competition, we notice that some ethnicities are targeted and more subject to racial hatred. It was the goal of the previous Jigsaw competition and we will need to make sure we don't create bias in our models.","ca4b6827":"### Distribution of characters","1cb084e0":"## Baseline BERT model","7f97ae9d":"For the toxicity dataset, we see that comments are much shorter. Let's visualize some tweets which are in the tails (that is above 1000 characters).","9f2fec06":"As we can see, validation and test sets are much more nicely formatted than toxicity dataset. We probably don't need to clean that much the unintended bias dataset, but we will need to take care of some punctuations and tags for the toxicity dataset, so that we have a distribution closer to the validation's and test's.\n\nLet's now explore the relationship between language and size of the comments...","68d23013":"### Most common words","bc4da0e8":"Quote from shahules' EDA, go check it out ;)\n\nReadability is the ease with which a reader can understand a written text. In natural language processing, the readability of text depends on its content. It focuses on the words we choose, and how we put them into sentences and paragraphs for the readers to comprehend. 9.1 The Flesch Reading Ease formula\n\n![image.png](attachment:image.png)\n\nIn the Flesch reading-ease test, higher scores indicate material that is easier to read; lower numbers mark passages that are more difficult to read. The formula for the Flesch reading-ease score (FRES) test is<br\/><br\/>\n90-100 - Very Easy<br\/>\n80-89 - Easy<br\/>\n70-79 - Fairly Easy<br\/>\n60-69 - Standard<br\/>\n50-59 - Fairly Difficult<br\/>\n30-49 - Difficult<br\/>\n0-29 - Very Confusing<br\/>"}}