{"cell_type":{"e4227f86":"code","701624ee":"code","981c0c50":"code","3d5180f6":"code","c9e936a7":"code","4ef54acd":"code","66661720":"code","54e8ccae":"code","e5876718":"code","eb13aeea":"code","96e832dc":"code","be892a68":"code","ad44cab5":"code","9ce3edd9":"code","57728392":"code","21920886":"code","2422e212":"code","aadb8c80":"code","04beb7f7":"code","19d77865":"code","e7fa871e":"code","a30cdd92":"code","40ff4ca1":"code","693b546e":"code","fe0d2940":"code","ab2691a2":"code","317b169e":"code","2120a56b":"code","afb0b34f":"code","8e4ebd00":"code","651bc4df":"code","8e7aa375":"code","ef49b72c":"markdown","939093c2":"markdown","f0c74be5":"markdown","64501c66":"markdown","b0c58fb1":"markdown","e7ef45a5":"markdown","da6379d8":"markdown","82de25ed":"markdown","c7110d87":"markdown","b58e56e7":"markdown","9a4fe17c":"markdown","fb2484f5":"markdown","59243257":"markdown","949b6c09":"markdown","6fc554d6":"markdown","a3ecfd5a":"markdown","61e54d1f":"markdown","cf1100fc":"markdown","17d6c56e":"markdown","6f6d647e":"markdown","1e90f8dc":"markdown","86630620":"markdown","826704da":"markdown","cbfc9f2e":"markdown","ad3c99d9":"markdown","8c161fe4":"markdown","b6cc3b1f":"markdown"},"source":{"e4227f86":"# True to spend extra time displaying graphs, False for speedy results\nshow_plots = True","701624ee":"import pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set(style=\"white\")\n\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\n\nfrom sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.decomposition import PCA\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n%matplotlib inline","981c0c50":"train = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')","3d5180f6":"train.info()","c9e936a7":"train.describe()","4ef54acd":"train.head()","66661720":"target_classes = range(1,8)\ntarget_class_names = ['Spruce\/Fir', 'Lodgepole Pine', 'Ponderosa Pine', \\\n                      'Cottonwood\/Willow', 'Aspen', 'Douglas-fir', 'Krummholz']\n\nnumerical_features = ['Elevation', 'Aspect', 'Slope', 'Horizontal_Distance_To_Hydrology', \\\n                    'Vertical_Distance_To_Hydrology', 'Horizontal_Distance_To_Roadways', \\\n                    'Hillshade_9am', 'Hillshade_Noon', 'Hillshade_3pm', 'Horizontal_Distance_To_Fire_Points']\n\ncategorical_features = [ 'Wilderness_Area', 'Soil_Type' ]","54e8ccae":"# extract target from data\ny = train['Cover_Type']\ntrain = train.drop('Cover_Type', axis=1)","e5876718":"# plot target var\nplt.hist(y, bins='auto')\nplt.title('Cover_Type')\nplt.xlabel('Class')\nplt.ylabel('# Instances')\nplt.show()","eb13aeea":"if show_plots:\n    for feature_name in numerical_features:\n        plt.figure()\n        sns.distplot(train[feature_name], label='train')\n        sns.distplot(test[feature_name], label='test')\n        plt.legend()\n        plt.show()","96e832dc":"if show_plots:\n    # categorical distributions btw train and test set\n    train_wilderness_categorical = train['Wilderness_Area1'].copy().rename('Wilderness_Area')\n    train_wilderness_categorical[train['Wilderness_Area2'] == 1] = 2\n    train_wilderness_categorical[train['Wilderness_Area3'] == 1] = 3\n    train_wilderness_categorical[train['Wilderness_Area4'] == 1] = 4\n\n    test_wilderness_categorical = test['Wilderness_Area1'].copy().rename('Wilderness_Area')\n    test_wilderness_categorical[test['Wilderness_Area2'] == 1] = 2\n    test_wilderness_categorical[test['Wilderness_Area3'] == 1] = 3\n    test_wilderness_categorical[test['Wilderness_Area4'] == 1] = 4\n\n    plt.figure()\n    sns.countplot(train_wilderness_categorical, label='train')\n    plt.title('Wilderness_Area in Train')\n\n    plt.figure()\n    sns.countplot(test_wilderness_categorical, label='test')\n    plt.title('Wilderness_Area in Test')\n\n    plt.show()","be892a68":"soil_classes = range(1,41)\n\ntrain_soiltype_categorical = train['Soil_Type1'].copy().rename('Soil_Type')\nfor cl in soil_classes:\n    train_soiltype_categorical[train['Soil_Type'+str(cl)] == 1] = cl\n\ntest_soiltype_categorical = test['Soil_Type1'].copy().rename('Soil_Type')\nfor cl in soil_classes:\n    test_soiltype_categorical[test['Soil_Type'+str(cl)] == 1] = cl\n\nplt.figure(figsize=(10, 5))\nsns.countplot(train_soiltype_categorical, label='train')\nplt.title('Soil_Type in Train')\n\nplt.figure(figsize=(10, 5))\nsns.countplot(test_soiltype_categorical, label='test')\nplt.title('Soil_Type in Test')\n\nplt.show()","ad44cab5":"pca = PCA(n_components=3)\ntrain_pca = pca.fit_transform(train)\nprint('Representation of dataset in 3 dimensions:\\n')\nprint(train_pca)","9ce3edd9":"if show_plots:\n    # graph pca in interactive 3d chart\n    # props to Roman Kovalenko's \"Data distribution & 3D Scatter Plots\" kernel for showing me where to find a good 3d graphing lib\n\n    colors = ['red', 'blue', 'green', 'black', 'purple', 'orange', 'gray']\n    # feel free to change the colors up - unfortunately there's usually a tradeoff between aesthetics and readability\n    # colors = ['#f45f42', '#f49241', '#db6a0d', '#dba00d', '#ead40e', '#ffb163', '#ea480e']\n\n    traces = []\n\n    # iterate over classes and add each set of points to traces list\n    for cl in target_classes:\n\n        # get all 3-d pca vectors that match the current class\n        class_pca = train_pca[y[y == cl].index.values]\n\n        class_pca_x = [ pt[0] for pt in class_pca]\n        class_pca_y = [ pt[1] for pt in class_pca]\n        class_pca_z = [ pt[2] for pt in class_pca]\n\n        trace = go.Scatter3d(\n            x=class_pca_x,\n            y=class_pca_y,\n            z=class_pca_z,\n            mode='markers',\n            marker=dict(\n                color=colors[cl-1],\n                size=3\n            ),\n            name=target_class_names[cl-1]\n        )\n\n        traces.append(trace)\n\n    layout = go.Layout(\n        margin=dict(\n            l=0,\n            r=0,\n            b=0,\n            t=0\n        )\n    )\n    fig = go.Figure(data=traces, layout=layout)\n    py.iplot(fig, filename='simple-3d-scatter')","57728392":"# drop uninformative features\ntrain = train.drop('Id', axis=1)","21920886":"# write a function to transform the train and test sets\n# we'll also append an underscore \"_\" to our engineered feature names to help differentiate them\ndef add_features(data):\n    data['Euclidean_Distance_To_Hydrology_'] = (data['Horizontal_Distance_To_Hydrology']**2 + data['Vertical_Distance_To_Hydrology']**2)**0.5\n    data['Mean_Distance_To_Amenities_'] = (data['Horizontal_Distance_To_Fire_Points'] + data['Horizontal_Distance_To_Hydrology'] + data['Horizontal_Distance_To_Roadways']) \/ 3.0\n    data['Elevation_Minus_Vertical_Distance_To_Hydrology_'] = data['Elevation'] - data['Vertical_Distance_To_Hydrology']\n    return data\n\ntrain = add_features(train)\ntest = add_features(test)","2422e212":"# # convert aspect angle in degrees to cos + sin\n# train['AspectCos'] = train['Aspect']\n# train['AspectSin'] = train['Aspect']\n\n# train['AspectCos'] = train['AspectCos'].apply(lambda x: np.cos(np.deg2rad(x)))\n# train['AspectSin'] = train['AspectSin'].apply(lambda x: np.sin(np.deg2rad(x)))\n\n# train = train.drop(['Aspect'], axis=1)","aadb8c80":"if show_plots:\n    # plot each feature (y axis) with target (x axis)\n    plt.figure(figsize=(30, 190))\n\n    # iterate through feature names and assign to pyplot subplot\n    for i,feature_name in enumerate(train.columns.values):\n        plt.subplot(19,3,i+1)\n        sns.violinplot(y, train[feature_name])\n        plt.title(feature_name, fontsize=30)\n\n    plt.show()","04beb7f7":"if show_plots:\n\n    # Compute the correlation matrix\n    corr = train.corr()\n\n    # Generate a mask for the upper triangle\n    mask = np.zeros_like(corr, dtype=np.bool)\n    mask[np.triu_indices_from(mask)] = True\n\n    # Set up the matplotlib figure\n    f, ax = plt.subplots(figsize=(110, 90))\n\n    # Generate a custom diverging colormap, use the line below to customize your color options\n    # sns.choose_diverging_palette()\n    cmap = sns.diverging_palette(8,132,99,50,50,9, as_cmap=True)\n\n    # Draw the heatmap with the mask and correct aspect ratio\n    sns_heatmap = sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0)\n\n    # since the heatmap is very large, use following line to save to png for close examination\n    # sns_heatmap.get_figure().savefig(\"corr_heatmap.png\")","19d77865":"# split data into train and test sets, using constant random state to better quantify our changes\nX_train, X_test, y_train, y_test = train_test_split(train, y, test_size=0.33, random_state=1)","e7fa871e":"# train model\nmodel = ExtraTreesClassifier(n_estimators=500)\nmodel.fit(X_train, y_train)","a30cdd92":"# plot feature importance\nplt.figure(figsize=(20,20))\nplt.barh(X_train.columns.values, model.feature_importances_)\nplt.title('Feature Importance')\nplt.ylabel('Feature Name')\nplt.xlabel('Gini Value')\nplt.show()","40ff4ca1":"# make predictions on the cross validation set\ny_pred = model.predict(X_test)\nn_correct = (y_pred == y_test).sum()\nn_total = (y_pred == y_test).count()\nprint('Accuracy:', n_correct\/n_total)","693b546e":"# table with data points, truth, and pred\nerrors = X_test.copy()\nerrors['truth'] = y_test\nerrors['pred'] = y_pred\nerrors = errors[errors['truth'] != errors['pred']]","fe0d2940":"print(errors.shape[0], 'errors over',y_pred.shape[0],'predictions')","ab2691a2":"errors.head()","317b169e":"errors.describe()","2120a56b":"errors.describe() - train.describe()","afb0b34f":"# x: classes y: # errors\nerror_truths = []\nfor cl in target_classes:\n    error_count = errors[errors['truth'] == cl]['truth'].count()\n    error_truths.append(error_count)\n    \nplt.bar(target_classes, error_truths)\nplt.title('Errors by truth class')\nplt.xlabel('True Class')\nplt.ylabel('# Errors')\nplt.show()","8e4ebd00":"# x: classes y: # errors\nerror_preds = []\nfor cl in target_classes:\n    error_count = errors[errors['pred'] == cl]['pred'].count()\n    error_preds.append(error_count)\n    \nplt.bar(target_classes, error_preds)\nplt.title('Errors by predicted class')\nplt.xlabel('Predicted Class')\nplt.ylabel('# Errors')\nplt.show()","651bc4df":"cf_matrix = confusion_matrix(errors['truth'], errors['pred'])\n\ncfm_df = pd.DataFrame(cf_matrix, index = [str(cl)+'t' for cl in target_classes],\n                  columns = [str(cl)+'p' for cl in target_classes])\n\nax = plt.axes()\nsns.heatmap(cfm_df, annot=True, fmt='g', ax=ax)\nax.set_title('Error Confusion Matrix')\nplt.xlabel('Predicted')\nplt.ylabel('Truth')\nplt.show()","8e7aa375":"prediction_classes = pd.Series(model.predict(test.drop('Id', axis=1))).rename('Cover_Type')\npredictions = pd.concat([test['Id'], prediction_classes], axis=1).reset_index().drop('index', axis=1)\npredictions.to_csv('submission.csv', index=False)\npredictions.head()","ef49b72c":"<a id='feature_overview'><\/a>\n#  Feature Overview\n__Elevation__ - Elevation in meters<br>\n__Aspect__ - Aspect in degrees azimuth<br>\n__Slope__ - Slope in degrees<br>\n__Horizontal_Distance_To_Hydrology__ - Horz Dist to nearest surface water features<br>\n__Vertical_Distance_To_Hydrology__ - Vert Dist to nearest surface water features<br>\n__Horizontal_Distance_To_Roadways__ - Horz Dist to nearest roadway<br>\n__Hillshade_9am (0 to 255 index)__ - Hillshade index at 9am, summer solstice<br>\n__Hillshade_Noon (0 to 255 index)__ - Hillshade index at noon, summer solstice<br>\n__Hillshade_3pm (0 to 255 index)__ - Hillshade index at 3pm, summer solstice<br>\n__Horizontal_Distance_To_Fire_Points__ - Horz Dist to nearest wildfire ignition points<br>\n__Wilderness_Area__ (4 binary columns, 0 = absence or 1 = presence) - Wilderness area designation<br>\n__Soil_Type__ (40 binary columns, 0 = absence or 1 = presence) - Soil Type designation<br>\n__Cover_Type__ (7 types, integers 1 to 7) - Forest Cover Type designation<br>","939093c2":"<a id='next_steps'><\/a>\n# Next Steps\n1. Try to minimize misclassifications between:\n    - class 1 and class 2\n    - class 3 and class 6\n2. Convert soil type descriptions to categorical features\n3. Downsample train set to closer resemble feature distribution of test set - especially for elevation, which seems to be one of the most important features, but has the highest mismatch","f0c74be5":"<a id='error_analysis'><\/a>\n# Error Analysis","64501c66":"<a id='train_model'><\/a>\n# Train Model","b0c58fb1":"**Separate features by type**","e7ef45a5":"For each numerical feature, plot the variable distributions in the train and test sets. We want to make sure the distributions are similar in order to perform well on the test set.","da6379d8":"Now we'll add the following features:\n* Euclidean distance to hydrology\n* Mean distance to amenities (fire points, hydrology, and roadways)\n* Elevation minus vertical distance to hydrology\n\nDisclaimer: these features were inspired by Lathwal's excellent public kernel. I have left out a few features so you can check his out for the full set!","82de25ed":"Additionally, I wrote code to convert the angle in degrees (circular, aka 0 = 360) to their cos and sin components - however, the code is commented because it actually lowered performance. Perhaps there was some bias in assigning certain angles which contains information that is lost when it is approximated as cos and sin.","c7110d87":"Let's try subtracting the descriptive statistics for all train data from those of the errors. This may help us spot trends (features that are underperforming).","b58e56e7":"# Table of Contents\n* [Config and Imports](#config_and_imports)<br>\n* [Feature Overview](#feature_overview)<br>\n* [Load and Explore Data](#load_and_explore_data)<br>\n* [Visualize and Transform Data](#visualize_and_transform_data)<br>\n* [Train Model](#train_model)<br>\n* [Error Analysis](#error_analysis)<br>\n* [Make Predictions on Test Set](#make_predictions)<br>\n* [Next Steps](#next_steps)<br>","9a4fe17c":"**Feature Correlations**","fb2484f5":"# <center>Forest Cover Classification<\/center>\n<center>If you liked this kernel and\/or found it helpful, please upvote it so others can see it too!<\/center>","59243257":"# The Forest Cover Type Challenge\n\nFrom the competition's <a href=\"https:\/\/www.kaggle.com\/c\/forest-cover-type-kernels-only\">overview page<\/a>:  \n\nIn this competition you are asked to predict the forest cover type (the predominant kind of tree cover) from cartographic variables. The actual forest cover type for a given 30 x 30 meter cell was determined from US Forest Service (USFS) Region 2 Resource Information System data. Independent variables were then derived from data obtained from the US Geological Survey and USFS. The data is in raw form and contains binary columns of data for qualitative independent variables such as wilderness areas and soil type.\n\nThis study area includes four wilderness areas located in the Roosevelt National Forest of northern Colorado. These areas represent forests with minimal human-caused disturbances, so that existing forest cover types are more a result of ecological processes rather than forest management practices.\n\n","949b6c09":"<a id='make_predictions'><\/a>\n# Make predictions on test set","6fc554d6":"Specifically interesting to note that the train and test distributions for Elevation differ significantly. This could be a problem if Elevation is an important feature. Now let's do the same for categorical features (Wilderness and Soil Type).","a3ecfd5a":"<img src=\"https:\/\/cdn-images-1.medium.com\/max\/500\/1*QNtpWNresupHbiV7Xe59gg.png\">","61e54d1f":"Nothing out of the ordinary here. Split into training and holdout sets, train a model, and plot feature importance. \n\nOriginally I used Random Forest, but I switched to ExtraTreesClassifier because it generalizes better. I use n_estimators=500 for decent time\/performance tradeoff.","cf1100fc":"**PCA**","17d6c56e":"Looks like most of the errors are because of:\n* Class 1 misclassified as Class 2, and vice versa\n* Class 3 missclassified as Class 6\n* Class 2 misclassified as Class 5","6f6d647e":"Looks like the classes are pretty intertwined, but there are definitely patterns emerging. Let's handle the data a bit more before training a model!","1e90f8dc":"**Plot distribution of target classes**","86630620":"<a id='load_and_explore_data'><\/a>\n# Load and Explore Data","826704da":"<a id='config_and_imports'><\/a>\n# Config and Imports","cbfc9f2e":"**Feature Engineering**","ad3c99d9":"Since we have so many features, it's really hard to read those features. There's a line of code in the previous cell to save this graphic to a CSV to take a closer look.\n\nFrom this graph we can draw a couple conclusions:\n* A couple of soil types show up as white, because they're actually entirely uniform, and thus useless.\n* Almost no correlation among soil types.\n* There is some correlation among the Hillshade times, as well as Slope and Elevation.","8c161fe4":"<a id='visualize_and_transform_data'><\/a>\n# Visualize and Transform Data","b6cc3b1f":"Let's make a confusion matrix to visualize where our errors are coming from."}}