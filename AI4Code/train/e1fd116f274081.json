{"cell_type":{"ec116aba":"code","c029e20f":"code","e9318f24":"code","28b1b947":"code","28979470":"code","9dee88de":"code","6232176a":"code","7db45b4c":"code","ae08d04e":"code","4e28b5a0":"code","b1945faa":"code","e56a06df":"code","b11831f9":"code","04abc100":"code","8bb17990":"code","14c45f38":"code","48df8e86":"code","d0c1688f":"code","901c5bc9":"code","26d9225d":"code","777ab42a":"code","1c47255f":"code","1bb3b97b":"code","c129187f":"code","053c540b":"code","c128e978":"code","002de7fb":"code","d79f5161":"code","b6e302f3":"code","658b77bd":"code","29afa0a3":"code","0f9fafee":"code","1a2c018a":"markdown","211adb74":"markdown","ff7a17ad":"markdown","013f5a23":"markdown","849a4a82":"markdown","f4002343":"markdown","3c9635b0":"markdown","34303a62":"markdown","89d88e17":"markdown","9a780ec0":"markdown","51a8bacf":"markdown"},"source":{"ec116aba":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c029e20f":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\n\nfrom transformers import BertPreTrainedModel, BertModel\nfrom transformers import AutoConfig, AutoTokenizer\n\nfrom sklearn import metrics\nfrom sklearn.model_selection import train_test_split\nfrom tqdm import tqdm, trange","e9318f24":"import matplotlib.pyplot as plt\nimport seaborn as sns","28b1b947":"# plt.style.use('dark_background')","28979470":"dftrain = pd.read_csv('..\/input\/commonlitreadabilityprize\/train.csv')\ndftest = pd.read_csv('..\/input\/commonlitreadabilityprize\/test.csv')\nsample_submission = pd.read_csv('..\/input\/commonlitreadabilityprize\/sample_submission.csv')","9dee88de":"train_data, validation = train_test_split(dftrain, test_size=0.25, random_state=21)","6232176a":"sample_submission.head()","7db45b4c":"dftrain.shape, dftest.shape","ae08d04e":"dftrain.head()","4e28b5a0":"word_count = dftrain['excerpt'].apply(lambda x: len(x.split()))","b1945faa":"fig = plt.figure(figsize=[10,7])\nsns.distplot(word_count, color=sns.xkcd_rgb['greenish teal'])\nplt.show()","e56a06df":"wc_test = dftest['excerpt'].apply(lambda x: len(x.split()))","b11831f9":"wc_test","04abc100":"word_count.max()","8bb17990":"MODEL_OUT_DIR = '\/kaggle\/working\/models\/bert_regressor'\n## Model Configurations\nMAX_LEN_TRAIN = 205\nMAX_LEN_VALID = 205\nMAX_LEN_TEST = 205\nBATCH_SIZE = 64\nLR = 1e-3\nNUM_EPOCHS = 10\nNUM_THREADS = 1  ## Number of threads for collecting dataset\nMODEL_NAME = 'bert-base-uncased'\n\nif not os.path.isdir(MODEL_OUT_DIR):\n    os.makedirs(MODEL_OUT_DIR)","14c45f38":"class Excerpt_Dataset(Dataset):\n\n    def __init__(self, data, maxlen, tokenizer): \n        #Store the contents of the file in a pandas dataframe\n        self.df = data.reset_index()\n        #Initialize the tokenizer for the desired transformer model\n        self.tokenizer = tokenizer\n        #Maximum length of the tokens list to keep all the sequences of fixed size\n        self.maxlen = maxlen\n\n    def __len__(self):\n        return self.df.shape[0]\n\n    def __getitem__(self, index):    \n        #Select the sentence and label at the specified index in the data frame\n        excerpt = self.df.loc[index, 'excerpt']\n        try:\n            target = self.df.loc[index, 'target']\n        except:\n            target = 0.0\n        identifier = self.df.loc[index, 'id']\n        #Preprocess the text to be suitable for the transformer\n        tokens = self.tokenizer.tokenize(excerpt) \n        tokens = ['[CLS]'] + tokens + ['[SEP]'] \n        if len(tokens) < self.maxlen:\n            tokens = tokens + ['[PAD]' for _ in range(self.maxlen - len(tokens))] \n        else:\n            tokens = tokens[:self.maxlen-1] + ['[SEP]'] \n        #Obtain the indices of the tokens in the BERT Vocabulary\n        input_ids = self.tokenizer.convert_tokens_to_ids(tokens) \n        input_ids = torch.tensor(input_ids) \n        #Obtain the attention mask i.e a tensor containing 1s for no padded tokens and 0s for padded ones\n        attention_mask = (input_ids != 0).long()\n        \n        target = torch.tensor(target, dtype=torch.float32)\n        \n        return input_ids, attention_mask, target","48df8e86":"class BertRegresser(BertPreTrainedModel):\n    def __init__(self, config):\n        super().__init__(config)\n        self.bert = BertModel(config)\n        #The output layer that takes the [CLS] representation and gives an output\n        self.cls_layer1 = nn.Linear(config.hidden_size,128)\n        self.relu1 = nn.ReLU()\n        self.ff1 = nn.Linear(128,128)\n        self.tanh1 = nn.Tanh()\n        self.ff2 = nn.Linear(128,1)\n\n    def forward(self, input_ids, attention_mask):\n        #Feed the input to Bert model to obtain contextualized representations\n        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n        #Obtain the representations of [CLS] heads\n        logits = outputs.last_hidden_state[:,0,:]\n        output = self.cls_layer1(logits)\n        output = self.relu1(output)\n        output = self.ff1(output)\n        output = self.tanh1(output)\n        output = self.ff2(output)\n        return output","d0c1688f":"def train(model, criterion, optimizer, train_loader, val_loader, epochs, device):\n    best_acc = 0\n    for epoch in trange(epochs, desc=\"Epoch\"):\n        model.train()\n        train_loss = 0\n        for i, (input_ids, attention_mask, target) in enumerate(iterable=train_loader):\n            optimizer.zero_grad()  \n            \n            input_ids, attention_mask, target = input_ids.to(device), attention_mask.to(device), target.to(device)\n            \n            output = model(input_ids=input_ids, attention_mask=attention_mask)\n            \n            loss = criterion(output, target.type_as(output))\n            loss.backward()\n            optimizer.step()\n            \n            train_loss += loss.item()\n        \n        print(f\"Training loss is {train_loss\/len(train_loader)}\")\n        val_loss = evaluate(model=model, criterion=criterion, dataloader=val_loader, device=device)\n        print(\"Epoch {} complete! Validation Loss : {}\".format(epoch, val_loss))","901c5bc9":"def evaluate(model, criterion, dataloader, device):\n    model.eval()\n    mean_acc, mean_loss, count = 0, 0, 0\n\n    with torch.no_grad():\n        for input_ids, attention_mask, target in (dataloader):\n            \n            input_ids, attention_mask, target = input_ids.to(device), attention_mask.to(device), target.to(device)\n            output = model(input_ids, attention_mask)\n            \n            mean_loss += criterion(output, target.type_as(output)).item()\n#             mean_err += get_rmse(output, target)\n            count += 1\n            \n    return mean_loss\/count","26d9225d":"def get_rmse(output, target):\n    err = torch.sqrt(metrics.mean_squared_error(target, output))\n    return err","777ab42a":"def predict(model, dataloader, device):\n    predicted_label = []\n    actual_label = []\n    with torch.no_grad():\n        for input_ids, attention_mask, target in (dataloader):\n            \n            input_ids, attention_mask, target = input_ids.to(device), attention_mask.to(device), target.to(device)\n            output = model(input_ids, attention_mask)\n                        \n            predicted_label += output\n            actual_label += target\n            \n    return predicted_label","1c47255f":"## Configuration loaded from AutoConfig \nconfig = AutoConfig.from_pretrained(MODEL_NAME)\n## Tokenizer loaded from AutoTokenizer\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n## Creating the model from the desired transformer model\nmodel = BertRegresser.from_pretrained(MODEL_NAME, config=config)\n## GPU or CPU\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n## Putting model to device\nmodel = model.to(device)\n## Takes as the input the logits of the positive class and computes the binary cross-entropy \n# criterion = nn.BCEWithLogitsLoss()\ncriterion = nn.MSELoss()\n## Optimizer\noptimizer = optim.Adam(params=model.parameters(), lr=LR)","1bb3b97b":"## Training Dataset\ntrain_set = Excerpt_Dataset(data=train_data, maxlen=MAX_LEN_TRAIN, tokenizer=tokenizer)\nvalid_set = Excerpt_Dataset(data=validation, maxlen=MAX_LEN_VALID, tokenizer=tokenizer)\ntest_set = Excerpt_Dataset(data=dftest, maxlen=MAX_LEN_TEST, tokenizer=tokenizer)\n\n\n## Data Loaders\ntrain_loader = DataLoader(dataset=train_set, batch_size=BATCH_SIZE, num_workers=NUM_THREADS)\nvalid_loader = DataLoader(dataset=valid_set, batch_size=BATCH_SIZE, num_workers=NUM_THREADS)\ntest_loader = DataLoader(dataset=test_set, batch_size=BATCH_SIZE, num_workers=NUM_THREADS)\n\n# print(len(train_loader))","c129187f":"train(model=model, \n      criterion=criterion,\n      optimizer=optimizer, \n      train_loader=train_loader,\n      val_loader=valid_loader,\n      epochs = 10,\n     device = device)","053c540b":"output = predict(model, test_loader, device)","c128e978":"output[0].shape","002de7fb":"output[0]","d79f5161":"out2 = []","b6e302f3":"for out in output:\n    out2.append(out.cpu().detach().numpy())","658b77bd":"out = np.array(out2).reshape(len(out2))","29afa0a3":"submission = pd.DataFrame({'id': dftest['id'], 'target':out})","0f9fafee":"submission.to_csv('submission.csv', index=False)","1a2c018a":"## Predict function","211adb74":"## Config","ff7a17ad":"## Train function","013f5a23":"## Prediction","849a4a82":"## Config","f4002343":"## Prep data","3c9635b0":"## Load Data","34303a62":"## Evaluation function","89d88e17":"## EDA","9a780ec0":"## Scripts","51a8bacf":"## Train"}}