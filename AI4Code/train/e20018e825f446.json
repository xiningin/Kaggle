{"cell_type":{"d5c96af5":"code","5e375b15":"code","f7da3422":"code","cd81fb78":"code","eff31ccb":"code","36a98c2d":"code","ea4889dd":"code","8382f3bf":"code","eded3434":"code","c8931d78":"code","dd74003e":"code","8fb3eca2":"code","c1e80e51":"code","6f283e3e":"code","026d6ebb":"code","30ec28ec":"code","196ace04":"code","d6dd0feb":"code","947e7ec2":"markdown","176e32fa":"markdown","3646d8b0":"markdown","b6f40511":"markdown","5cbba558":"markdown","74b9d192":"markdown","be7f6a1d":"markdown","445e8bef":"markdown","046548ed":"markdown","7ccb8400":"markdown","af39a4a8":"markdown","ea67dce2":"markdown"},"source":{"d5c96af5":"# Import original USP embeddings preprocessing from https:\/\/github.com\/nathanshartmann\/portuguese_word_embeddings\n!wget --quiet https:\/\/raw.githubusercontent.com\/nathanshartmann\/portuguese_word_embeddings\/master\/preprocessing.py","5e375b15":"import random\n\nimport pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils import class_weight\nfrom sklearn.metrics import classification_report\n\nfrom gensim.models import KeyedVectors\n\nimport tensorflow as tf\nimport tensorflow,keras as K\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\nfrom pprint import pprint\nfrom tqdm.notebook import tqdm\n\nimport preprocessing\n\nrandom.seed(0)\nnp.random.seed(0)\ntf.random.set_seed(0)","f7da3422":"# Load train data\nusecols = ['id', 'review_title', 'review_text']\nraw_train_data = pd.read_csv('..\/input\/i2a2-nlp-2021-sentiment-analysis\/train.csv', index_col='id', usecols=usecols + ['rating'])","cd81fb78":"# Concatenate text features\nraw_train_data['review'] = raw_train_data['review_title'] + ' ' + raw_train_data['review_text']\n\nwith pd.option_context('display.max_colwidth', None):\n    display(raw_train_data[['review', 'rating']].head())","eff31ccb":"%%time\n# Use preprocessing from https:\/\/github.com\/nathanshartmann\/portuguese_word_embeddings\nraw_train_data['review_clean'] = raw_train_data['review'].apply(preprocessing.clean_text)","36a98c2d":"with pd.option_context('display.max_colwidth', None):\n    display(raw_train_data.head())","ea4889dd":"X = raw_train_data['review_clean']\ny = raw_train_data['rating']\n\n# Split data into 80% for training and 20% for validation\nX_train, X_valid, y_train, y_valid = train_test_split(X, y,\n                                                      test_size=0.2,\n                                                      stratify=y,\n                                                      random_state=0)\n\n# Sanity chech\nX_train.shape, y_train.shape, X_valid.shape, y_valid.shape","8382f3bf":"word_count_list = [len(sentence.split()) for sentence in X_train]\n\nsns.set()\nsns.histplot(word_count_list, bins=[*set(word_count_list)])\nplt.xlabel('Word Count')\nplt.ylabel('Sentence')\nplt.xlim([-10, 128])\nplt.tight_layout();\n\nprint(f'Average word count: {np.mean(word_count_list):.2f}', end='\\n\\n')","eded3434":"tokenize_params = {\n    'tokenizer': {'oov_token': '<UNK>'},\n    'pad_sequences': {'maxlen': 64, 'padding': 'post', 'truncating': 'post'},\n}\n\ntokenize = Tokenizer(**tokenize_params['tokenizer'])\ntokenize.fit_on_texts(X_train)\n\nword_index = tokenize.word_index\n\n# Transform text to sequences\nX_train_seq = tokenize.texts_to_sequences(X_train)\nX_valid_seq = tokenize.texts_to_sequences(X_valid)\n\n# Pad sequences\nX_train_pad = pad_sequences(X_train_seq, **tokenize_params['pad_sequences'])\nX_valid_pad = pad_sequences(X_valid_seq, **tokenize_params['pad_sequences'])","c8931d78":"# Example\nprint('Sentence:', '-'*10, X_train[0],\n      '\\nSentece to ids:', '-'*15, X_train_pad[0], sep='\\n')","dd74003e":"le = LabelEncoder()\n\ny_train = le.fit_transform(y_train)\ny_valid = le.transform(y_valid)\n\ntarget_names = le.classes_","8fb3eca2":"%%time\n\n# Parse GloVe word-embeddings file\nglove_model = KeyedVectors.load_word2vec_format('..\/input\/glove-ptbr-nilc\/glove_s100.txt')\nprint(f'Found {len(glove_model.key_to_index):,} word vectors.')\nprint('---\\n')\n\n# Prepare GloVe word-embeddings matrix\nvocab_size = len(word_index) + 1 \nembedding_dim = 100\n\nembeddings_matrix = np.zeros((vocab_size, embedding_dim))\nfor word, i in tqdm(word_index.items(), leave=None):\n    if word in glove_model:\n        embeddings_matrix[i] = glove_model[word]\n\nprint(f'Embeddings matrix dimensions: {embeddings_matrix.shape[0]:,} rows and {embeddings_matrix.shape[1]} columns\\n')","c1e80e51":"K.backend.clear_session()\n\nclassifier = K.models.Sequential([\n    K.layers.Embedding(vocab_size,\n                       embedding_dim, \n                       weights=[embeddings_matrix],\n                       trainable=False,\n                       input_length=X_train_pad.shape[1]),\n    K.layers.SpatialDropout1D(0.3),\n    K.layers.Bidirectional(K.layers.LSTM(embedding_dim,\n                                         dropout=0.3,\n                                         return_sequences=True)),\n    K.layers.Bidirectional(K.layers.LSTM(embedding_dim,\n                                         dropout=0.3,\n                                         return_sequences=True)),\n    K.layers.GlobalMaxPool1D(),\n    K.layers.Dense(64, activation='relu'),\n    K.layers.Dropout(0.3),\n    K.layers.Dense(5, activation='softmax'),\n])\n\nlr = 1e-3\nclassifier.compile(optimizer=K.optimizers.Adam(lr=lr),\n                   loss='sparse_categorical_crossentropy',\n                   metrics=['accuracy'])\n\nclassifier.summary()","6f283e3e":"# Define callbacks\ncallbacks = [K.callbacks.ReduceLROnPlateau(factor=0.5, \n                                           patience=5, \n                                           verbose=1)]","026d6ebb":"%%time\nhistory = classifier.fit(X_train_pad,\n                         y_train,\n                         batch_size=32, \n                         epochs=50, \n                         callbacks=callbacks,\n                         validation_data=(X_valid_pad, y_valid), \n                         workers=4, \n                         use_multiprocessing=True);","30ec28ec":"hist = pd.DataFrame(history.history)\n\nfig, ax = plt.subplots(1, 2, figsize=(12, 4))\nax = ax.flat\n\nfor ii, metric in enumerate(['loss', 'accuracy']):\n    sns.lineplot(x=hist.index, y=f'{metric}', data=hist, label=f'Training {metric}', ax=ax[ii])\n    sns.lineplot(x=hist.index, y=f'val_{metric}', data=hist, label=f'Validation {metric}', ax=ax[ii])\n    ax[ii].legend(frameon=False)\n\nsns.despine()\nplt.tight_layout()","196ace04":"_, accuracy = classifier.evaluate(X_valid_pad, y_valid, verbose=0)\nprint(f'Accuracy: {accuracy:.2%}')","d6dd0feb":"# Predictions\npreds = np.argmax(classifier.predict(X_valid_pad), axis=1)\n\nprint(classification_report(y_valid, preds, target_names=target_names.astype(str)))","947e7ec2":"## Plotting results","176e32fa":"## Loading useful libraries and modules","3646d8b0":"## Evaluating the model","b6f40511":"## Preparing text data to feed the model","5cbba558":"## Transforming text into vectors","74b9d192":"## Estimating `maxlen` parameter","be7f6a1d":"## Preparing labels","445e8bef":"## Classification report","046548ed":"## Preprocessing GloVe embeddings","7ccb8400":"## Loading training and testing data","af39a4a8":"## Preprocessing raw text data","ea67dce2":"## Training and evaluating the model"}}