{"cell_type":{"d019b17b":"code","cf8cdcbd":"code","3740fcf2":"code","cd6dd338":"code","71cfafab":"code","2615f457":"code","cbb00198":"code","9e333d72":"code","40d64ab3":"code","b70b8366":"code","6a2b45b9":"code","e0f0e790":"code","4399cadc":"code","d3d18343":"code","13a80e85":"code","93f61ddc":"code","0ab8b5cd":"code","72c86fa8":"code","92a58ed1":"code","4d003f5f":"code","4a065de3":"code","60b69d2c":"markdown","9a3878ea":"markdown","aa703ed9":"markdown","cf849116":"markdown","4bd975db":"markdown","aaa1fedc":"markdown","d56f2328":"markdown","38035c98":"markdown"},"source":{"d019b17b":"!pip install rapidfuzz -qq","cf8cdcbd":"import pandas as pd\nsub = pd.read_csv('..\/input\/wikipedia-image-caption\/sample_submission.csv')\nsub.head(10)","3740fcf2":"captions = pd.read_csv('..\/input\/wikipedia-image-caption\/test_caption_list.csv')\nprint(len(captions))\ncaptions.head()","cd6dd338":"test = pd.read_csv('..\/input\/wikipedia-image-caption\/test.tsv', sep='\\t')\ntest.head()","71cfafab":"for i in range(5):\n    print(test.image_url.loc[i])","2615f457":"ls ..\/input\/wikipedia-image-caption\/image_data_test\/image_pixels","cbb00198":"tst0 = pd.read_csv('..\/input\/wikipedia-image-caption\/image_data_test\/image_pixels\/test_image_pixels_part-00000.csv', sep='\\t', header=None)\ntst0.head()","9e333d72":"import base64 \nfrom PIL import Image\nimport io\n\nimage_64_decode = base64.b64decode(tst0[1].loc[0])\nimg = Image.open(io.BytesIO(image_64_decode))\nimg","40d64ab3":"import datatable as dt\ntrain0 = dt.fread('..\/input\/wikipedia-image-caption\/train-00000-of-00005.tsv')\ntrain0.head()","b70b8366":"from urllib.parse import unquote\n\nt = test.image_url.loc[2]\n\ndef convert(t):\n    t = t.rsplit('\/',1)[1]\n    t = unquote(t)\n    t = t.replace('_', ' ')\n    t = t + ' [SEP]'\n    return(t)","6a2b45b9":"for i in range(5):\n    print(f'target: {train0[i,-1]}')\n    print(f'prediction: {convert(train0[i,1])}')\n    print()","e0f0e790":"test['prediction'] = test['image_url'].apply(convert)","4399cadc":"test.head()","d3d18343":"CAPTIONS = captions.caption_title_and_reference_description.values.tolist()\nlen(CAPTIONS)","13a80e85":"from rapidfuzz import process, fuzz","93f61ddc":"%%time\n\nfor i in range(5):\n    s = test.prediction.loc[i]\n    print(f'image_url: {s}')\n    res = process.extract(s, CAPTIONS, scorer=fuzz.ratio, processor=None, limit=5)\n    print(f'closest captions:')\n    for c in res:\n        print(c[0])\n    print('*'*60)\n    print()  ","0ab8b5cd":"def find_closest_match(s):\n    res = process.extract(s, CAPTIONS, scorer=fuzz.ratio, processor=None, limit=5)\n    res = [x[0] for x in res]\n    return res","72c86fa8":"from tqdm.auto import tqdm\ntqdm.pandas()","92a58ed1":"test['caption_title_and_reference_description'] = test['prediction'].progress_apply(find_closest_match)","4d003f5f":"sub = test[['id', 'caption_title_and_reference_description']]\nsub = sub.explode('caption_title_and_reference_description')\nsub.head()","4a065de3":"sub.to_csv('submission.csv', index=False)","60b69d2c":"## Test data\n\nLet's start with **sample submission**. We have an *id* column and *caption_title_and_reference_description* column, and for each id we predict 5 captions. We need to select them from a predefined set of captions we'll see next.","9a3878ea":"# Wikipedia - Image\/Caption Matching EDA\n\nThis looks like a very interesting competition (too bad it doesn't award points!!). Let's try to take a look at the data and try to submit a super simple baseline to check if our understanding is fine.\n\n**Note this is a new version, previously we've helped discover some issues with test data :)**","aa703ed9":"The test file contains a list of id's and image urls. Let's print a few of those urls. ","cf849116":"## Baseline\n\nLooking at the train data, it seems that there is a connect between page url and our target. We also have page url in our test data, so let's try to exploit that and make a caption prediction only based on page url, without looking at the image. We'll verify that heuristic on train data, then we'll try to fuzzy match that caption prediction with the list of test captions.","4bd975db":"## to be continued ...","aaa1fedc":"Let's take a look at some of the **test images**. It looks like we have the test image data in 5 csv files, and these include links to the images (upload and commons varieties) as well as the base64 string encoded version of the images. ","d56f2328":"## Train data\n\nThanks to [this kernel](https:\/\/www.kaggle.com\/udbhavpangotra\/reading-the-data-datatable-works-like-a-charm) for showing how to accelerate data reading with datatable, please give it an upvote! Here we have a bunch of columns including page and image urls, as well as what looks as our target: caption_title_and_reference_description. There is a [SEP] string at the end of each entry here, or in the middle if multiple entries are provided. Not fully sure where this is coming from. \n","38035c98":"We've now been able to score above 0.0000 on the leaderboard. To be honest, I'm not sure if using page_url is expected by the host so I asked that question on the forum. For sure, the more challenging and interesting aspect is matching the captions directly with images, and we'll try to tackle that next :) "}}