{"cell_type":{"68239fd0":"code","89d2a3c6":"code","37bc3733":"code","aae659eb":"code","54f39bfc":"code","c370045b":"code","dc85d0da":"code","d2c06ae3":"code","44d2f1db":"code","50fb8c99":"code","5cd98717":"code","85ca49b6":"code","170b375a":"code","cbb3c22b":"code","d8853e7d":"code","b28a098b":"code","b534b771":"code","9160ff8d":"code","d9580604":"code","29d2408f":"code","e731eb7e":"code","8abf76ee":"code","2a881b62":"code","d7170ee4":"code","dbded921":"code","645b9cef":"code","fe0fcfed":"code","f14f4e4a":"code","c1cead39":"code","4a0dce1d":"code","9b2b9298":"markdown","56c41376":"markdown","23ed0aa4":"markdown","20ff3601":"markdown","60838d10":"markdown","3ecace37":"markdown","3df39ba9":"markdown","0ded6b7b":"markdown","3ae92338":"markdown","db6e454c":"markdown","36401117":"markdown","d738f0ec":"markdown","148a3fb0":"markdown","700d918b":"markdown","dca6587a":"markdown","e5182bbb":"markdown","e3945c29":"markdown","ae5d99f5":"markdown","7c9d3884":"markdown"},"source":{"68239fd0":"from pandas.plotting import scatter_matrix\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom scipy import stats\n\nfrom xgboost import XGBClassifier\n\nfrom sklearn import model_selection\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.linear_model import LogisticRegression, SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder, LabelBinarizer, MinMaxScaler, StandardScaler, RobustScaler\nimport seaborn as sns\n","89d2a3c6":"#got on my nerves\nimport warnings\nwarnings.filterwarnings('ignore')","37bc3733":"#Get Data\n\n#HAD TO REWRITE A LOT of the CODE for this special .csv!\n#The adult.data.txt from the UCI website is a bit different\nurl = '..\/input\/adult.csv'\ndata = pd.read_csv(url)","aae659eb":"print(set(data.education), '\\n\\n',\n      set(data.occupation),'\\n\\n', \n      set(data.workclass))\ndata.head()","54f39bfc":"#Understand data with descriptive statistics\nprint(data.nunique(), data.education.value_counts(), data.dtypes, data.describe(), data.corr(), data.shape, data.isnull().values.any())","c370045b":"#correlation heatmap of dataset\n\n#took this code sample from another kernel on this site\ndef correlation_heatmap(df):\n    _ , ax = plt.subplots(figsize =(14, 12))\n    colormap = sns.diverging_palette(220, 10, as_cmap = True)\n    \n    _ = sns.heatmap(\n        df.corr(), \n        cmap = \"YlGn\",\n        square=True, \n        cbar_kws={'shrink':.9 }, \n        ax=ax,\n        annot=True, \n        linewidths=0.1,vmax=1.0, linecolor='white',\n        annot_kws={'fontsize':12 }\n    )\n    \n    plt.title('Pearson Correlation of Features', y=1.05, size=15)\n\ncorrelation_heatmap(data)\n","dc85d0da":"sns.set(style=\"ticks\")\nsns.pairplot(data, hue=\"income\")\nplt.show()","d2c06ae3":"#sns.distplot(data.education.value_counts())\n#sns.jointplot(x=\"education_num\", y=\"age\", data=data, kind=\"kde\");\nto_count = ['education','workclass', 'marital-status', 'occupation', 'relationship', 'race', 'gender']\n\nfig, axs = plt.subplots(nrows=3, ncols=3, figsize=(20,14))\n#fig.tight_layout()\n[sns.countplot(y=feature, hue='income', data=data, order=data[feature].value_counts().index , ax=axs.flatten()[idx]) for idx, feature in enumerate(to_count)]\n#[axs.flatten()[idx].set_title(feature) for idx, feature in enumerate(to_count)]\nplt.plot()","44d2f1db":"sns.distplot(data[data['income'] == '>50K']['age'], kde_kws={\"label\": \">$50K\"})\nsns.distplot(data[data['income'] == '<=50K']['age'], kde_kws={\"label\": \"<=$50K\"})\n#data['income'] == ' >50K'","50fb8c99":"#code sample borrowed from other kernel on this challenge\ng = sns.jointplot(x = 'age', \n              y = 'hours-per-week',\n              data = data, \n              kind = 'hex', \n              cmap= 'hot',\n              gridsize=50,\n              size=12)\n\n#http:\/\/stackoverflow.com\/questions\/33288830\/how-to-plot-regression-line-on-hexbins-with-seaborn\nsns.regplot(data.age, data['hours-per-week'], ax=g.ax_joint, scatter=False, color='grey')\nplt.show()","5cd98717":"#I've a feeling that education and education_num might be the same thing. Let's check\ndata[['education', 'educational-num']].groupby(['education'], as_index=False).mean().sort_values(by='educational-num', ascending=False)","85ca49b6":"df = data.copy()","170b375a":"#fixes a type bug and let's us use .strip\ndf['income'] = df['income'].astype('str')\n\ndf.income = df.income.apply(lambda el: el.strip() == \">50K\")\ny = df.income","cbb3c22b":"df.drop(columns=['income','fnlwgt', 'education'], inplace=True)\n\n#data without income\nX = df.copy()\nprint(X.shape, y.shape)\n\nvalidation_size = 0.20\nseed = 7\nX_train, X_test, Y_train, Y_test = model_selection.train_test_split(X, y, test_size=validation_size, random_state=seed)\n#X, y","d8853e7d":"X_train.head(), len(X_train), len(X_test)","b28a098b":"#Label Encode ==> eg Binary, but in same column\nLE_train = LabelEncoder()\nLE_train.fit(X_train['gender'])\n\nX_train.loc[:, 'gender'] = LE_train.transform(X_train.loc[:, 'gender'])\nX_test.loc[:, 'gender'] = LE_train.fit_transform(X_test.loc[:, 'gender'])\n\nprint(LE_train.classes_)\n\n##Short form without class inspection\n#df['sex'] = LabelEncoder().fit_transform(df['sex'])\n#This requires exact spelling + knowledge of \"Male\"\n## or data.sex = data.sex.apply(lambda el: 1 if (el.strip() == \"Male\") else 0)","b534b771":"#let's make it simple. I'd say having citizenship from the start is worth a lot\nX_train['native-country'] = X_train['native-country'].apply(lambda el: 1 if el.strip() == \"United-States\" else 0)\nX_test['native-country'] = X_test['native-country'].apply(lambda el: 1 if el.strip() == \"United-States\" else 0)\n","9160ff8d":"X_train.isnull().values.any(), X_test.isnull().values.any(), len(X_train), X_train['marital-status'].isnull().values.sum()","d9580604":"cols_to_binarize = ['marital-status', 'occupation', 'relationship', 'race', 'workclass']\n\nstored_binarizers = []\nfor col in cols_to_binarize:\n  lb = LabelBinarizer()\n  lb_fitted = lb.fit(X_train[col])\n  stored_binarizers.append(lb_fitted)\n\n#our binarizer class only knows what's in the training data\nfor b in stored_binarizers:\n  print(b.classes_)\n\nfor i, val in enumerate(cols_to_binarize):\n  print(X_train[val].head())","29d2408f":"print(len(X_train))\n\ndef replaceWithBinarized_legit(dataframe, column_names, stored_binarizers):\n  newDf = dataframe.copy()\n  for idx, column_name in enumerate(column_names):\n    if (not column_name in newDf):\n      return\n    \n    lb = stored_binarizers[idx]\n    lb_results = lb.transform(newDf[column_name])\n    binarized_cols = pd.DataFrame(lb_results, columns=lb.classes_)\n\n    newDf.drop(columns=column_name, inplace=True)\n    #THIS BS indexing fucked up merge cost 2 hours of my life. Thanks Obama\n    binarized_cols.index = newDf.index\n    #\n    newDf = pd.concat([newDf, binarized_cols], axis=1)\n  return newDf\n\nX_train = replaceWithBinarized_legit(X_train, cols_to_binarize, stored_binarizers)\nX_test = replaceWithBinarized_legit(X_test, cols_to_binarize, stored_binarizers)\n\nX_train.head()","e731eb7e":"#X_train = X_train.dropna()\nX_train.isnull().values.any(), X_test.isnull().values.any(), len(X_train), X_test.isnull().values.any()","8abf76ee":"#let scaler only know about training set\ncols_to_scale_standard = ['hours-per-week', 'age', 'educational-num']\nstandardSc = StandardScaler()\nstandardSc.fit(X_train[cols_to_scale_standard])\n\ncols_to_scale_robust = ['capital-gain', 'capital-loss']\nrobustSc = RobustScaler()\nrobustSc.fit(X_train[cols_to_scale_robust])","2a881b62":"\ndef scale_columns_legit(df, column_names, scaler):\n  d = df.copy()\n  for column_name in column_names:\n    if (not column_name in d.columns):\n      return\n  \n  scaled_array = scaler.transform(d[column_names])\n  scaled_df = pd.DataFrame(scaled_array, columns=column_names)\n  scaled_df.index = d.index\n  d.drop(columns=column_names, inplace=True)\n  return pd.concat([d, scaled_df], axis=1)\n\n\n#I think there are outliers in sight (capital is always inequal)\n#do log for extreme values\n\nX_train = scale_columns_legit(X_train, cols_to_scale_standard, standardSc)\nX_train = scale_columns_legit(X_train, cols_to_scale_robust, robustSc)\n\nX_test = scale_columns_legit(X_test, cols_to_scale_standard, standardSc)\nX_test = scale_columns_legit(X_test, cols_to_scale_robust, robustSc)\n\nX_train.head()","d7170ee4":"len(X_train.columns), len(X_train._get_numeric_data().columns), \"Are all columns numeric?\"\n","dbded921":"X_train.columns","645b9cef":"#this might be a candidate for imputation ?\nX_train = X_train.drop(columns=[\"?\"], axis=1)\nX_test = X_test.drop(columns=[\"?\"], axis=1)","fe0fcfed":"X_train.isnull().values.any(), X_test.isnull().values.any(), X_train.head()\nX_train.head()","f14f4e4a":"scoring = 'accuracy'\nmodels = []\n\nmodels.append(('LR', LogisticRegression()))\nmodels.append(('LDA', LinearDiscriminantAnalysis()))\nmodels.append(('KNN', KNeighborsClassifier()))\nmodels.append(('CART', DecisionTreeClassifier()))\nmodels.append(('NB', GaussianNB()))\nmodels.append(('XGB', XGBClassifier()))\nmodels.append(('GB', GradientBoostingClassifier()))\nmodels.append(('SGD', SGDClassifier()))\nmodels.append(('RF', RandomForestClassifier(n_jobs=-1)))\n\n\n#SVM CRASHES SYS\n#models.append(('SVM', SVC()))\n# evaluate each model in turn\nresults = []\nnames = []\nfor name, model in models:\n  kfold = model_selection.KFold(n_splits=10, random_state=seed)\n  cv_results = model_selection.cross_val_score(model, X_train, Y_train, cv=kfold, scoring=scoring)\n  results.append(cv_results)\n  names.append(name)\n  msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n  print(msg)","c1cead39":"print(X_train.shape, X_test.shape)","4a0dce1d":"gb = GradientBoostingClassifier()\ngb.fit(X_train, Y_train)\npredictions = gb.predict(X_test)\n#print(accuracy_score(Y_test, predictions))\nprint(confusion_matrix(Y_test, predictions))\nprint(classification_report(Y_test, predictions))","9b2b9298":"### Split our Data into Test and Training.\n**Do this before you start scaling, normalizing and hot encoding the data. It's cheating otherwise since the normalization and encodings would know about values and classes in the test dataset before validation**","56c41376":"##Encode: Rest of Categorical Columns.\n**At this point I'm losing patience and just going to use the label binarizer and see what happens**\nAll unique values in all the categories will become their own column and have either 0 or 1 as values","23ed0aa4":"Playing around with the data and trying different methods to clean, plot and shape it.\n\nGBoosting gets me an accuracy of ~ 0.868.\nThat's unexpectedly better than many submissions here, so if anybody sees a mistake with my model or process, please let me know.\n\n* There is better and faster ways to do this, but I tried to do a few things manually to get a feel for them.","20ff3601":"## RegressionPlot: How does Age affect working hours?\n**took this from some other kernel**\n\n\n\n**Pitfalls: Can mislead, eg here, after age 70 that seems to vary a lot. So the ~7% correlation might be overfitted**\n","60838d10":"There are some value judgements that don't add up. For example, people who  have a masters have proportionally more >50K earners than doctors.\nSo here is a continous value and the algorithms might think that more is automatically better.\nWe can leave it like this for now, and group it later (eg. High school dropout, prof.school, masters etc)","3ecace37":"## Encode: Native Countries. Born in US 1, Rest 0 (Simplified)\n**Although scikit's LabelBinarizer could map the 42 countries into 42 Columns with 0\/1 encoding, it's not worth the complexity (*although often algorithms work well regardless*). Also many countries are underrepresented and would skew our model**","3df39ba9":"# Setup and Load Data","0ded6b7b":"## Get a grasp on Dataset stats, types, shape and uniques","3ae92338":"# Data Wrangling: Filter, Map, Reduce\n**Now, we can touch (and mutate!)**","db6e454c":"## Scaling: Squish column values between -1 and 1\n**If normally distributed (age, work hours) standard is good. For extremes (capital_gain\/loss -- capitalism births extremes) robust scaling might be better**\n\n$$Standard Scaler = \\frac{X - \\overline{X} }{SD(X)}$$\n\n\n$$Robust Scaler = \\frac{X - Q_1{X} }{Q_3(X) - Q_1(X)}$$\n\nRobust Scaler handles outliers more gracefully","36401117":"# Inspect and Plot our Data.\n** We look, but do not yet Touch **","d738f0ec":"## Countplot of categories","148a3fb0":"Let's split off what we want to predict. y","700d918b":"#Data is Ready. Run it through a bunch of Algorithms and see what works best","dca6587a":"## Do any of the numerical values correlate?\n**Doesn't look promising. We'll revisit once we mapped categorical data to numerical**\n\nResults: \n*  Some Correlation of Age and Workhours and Education\n*  Fnlwgt seems useless feature.\n*  Capital gain\/loss don't anti-correlate -> Common sense: gotta have money to lose money","e5182bbb":"Now let's look closer at the best performer","e3945c29":"## Pairplotting Numerical Values","ae5d99f5":"## Encode: Gender. Male \/ Female into binary (1\/0)","7c9d3884":"##Plotting the Relationship of Age and Income"}}