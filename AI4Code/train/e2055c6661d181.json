{"cell_type":{"17f92a42":"code","31e66094":"code","6c44025b":"code","14be76f0":"code","0abe3bd8":"code","0d1ff2f4":"code","4de77752":"code","099cf2a7":"code","b3b21c7c":"code","ec646d77":"code","77f7f3c5":"code","5bce987c":"code","65b014c4":"code","ad9318df":"code","45a4368b":"code","c43883ed":"code","23e22161":"code","6ea52ce0":"code","2d46acc5":"code","35c6a2b2":"code","0aaedbdf":"code","ce7013cd":"code","fe4b6c84":"code","b14e1379":"code","ef3733bb":"code","1389609a":"code","12c1a1d7":"code","2840eec2":"code","b2b9b4c3":"code","6cef7d45":"code","6f6cf80b":"code","d342c184":"code","280a729b":"code","54784e7d":"code","4af508c4":"markdown","68c402e4":"markdown","d416ed88":"markdown","6452bf77":"markdown","eb8b5d07":"markdown","24819a5f":"markdown","e6c2474d":"markdown","150d3630":"markdown","4dec1b20":"markdown","9c818f1c":"markdown","d3b65a23":"markdown","a8ec7f99":"markdown","3e1afe16":"markdown","70649334":"markdown","e00a0c45":"markdown","22ca27c9":"markdown"},"source":{"17f92a42":"#import the necessary libraries\nimport numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import GradientBoostingClassifier\n\n\n\n\nimport warnings\nsns.set_style('whitegrid')\n\n%matplotlib inline\n\nwarnings.filterwarnings('ignore')","31e66094":"#read the train and test csv and transform them into DataFrames\ntrain = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')\n","6c44025b":"# get a look at the data\ntrain.head()","14be76f0":"#Look at the information about the variables\nprint('*'*10, ' ', 'Train data', ' ', '*'*10)\ntrain.info()","0abe3bd8":"#check if any variable in train contain NaN values\nprint(train.isna().sum())","0d1ff2f4":"#check if any variable in test contain NaN values\nprint(test.isna().sum())","4de77752":"#drop cabin column\ndel train['Cabin']\ndel test['Cabin']\n\n#drop ticket column\ndel train['Ticket']\ndel test['Ticket']\n\n#get the most used value in Embarked and fill NaN values with it\nmode = train['Embarked'].mode() # -> \"S\"\ntrain[\"Embarked\"] = train[\"Embarked\"].fillna(\"S\")\ntest[\"Embarked\"] = train[\"Embarked\"].fillna(\"S\")\n\n\n#get the mean and stdev of age in train \n\"\"\"\"age_mean = train[\"Age\"].mean()\nage_std = train[\"Age\"].std()\nnan_train = train[\"Age\"].isna().sum()\n\n#get the mean and stdev of age in test \nage_mean_test = test[\"Age\"].mean()\nage_std_test = test[\"Age\"].std()\nnan_test = test[\"Age\"].isna().sum()\n\n# generate random values for train using mean and stdev\nrand_train = np.random.randint(age_mean - age_std, age_mean + age_std, size=nan_train)\n\n# generate random values for test using mean and stdev\nrand_test = np.random.randint(age_mean_test - age_std_test, age_mean_test + age_std_test, size=nan_test)\"\"\"\n\n","099cf2a7":"#Fill the fare missing value with the mean value of the fare according to the Pclass\ntest['Fare'] = test['Fare'].fillna(test.groupby('Pclass').mean()['Fare'][3])\n","b3b21c7c":"#get the titles of the passangers\ncombine = [train, test]\n\nfor dataset in combine:\n    dataset['Title'] = dataset.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\n\npd.crosstab(train['Title'], train['Sex'])","ec646d77":"#replace the categories as shown above\nfor dataset in combine:\n    dataset['Title'] = dataset['Title'].replace(['Capt','Col', 'Don', 'Dr', 'Jonkheer', 'Major', 'Rev'],'Mr')\n    dataset['Title'] = dataset['Title'].replace(['Mme','Dona'], 'Mrs')\n    dataset['Title'] = dataset['Title'].replace(['Mlle','Ms'],'Miss')\n    dataset['Title'] = dataset['Title'].replace(['Countess','Lady','Sir'],'Royal')","77f7f3c5":"test.head()","5bce987c":"\n#Let's see the age distribution of the categories\ngb = train[['Title', 'Age']].groupby(['Title'],as_index=False)\nMasterl, Missl, Mrl, Mrsl, Royall= [], [], [], [], []\nages = {'Master': Masterl, 'Miss': Missl, 'Mr': Mrl, 'Mrs':Mrsl, 'Royal':Royall }\nfor key,value in gb:\n    ages[key] = value['Age'].values\nfor x in ages.keys():\n    ages[x] = [a for a in ages[x] if str(a) != 'nan']\n\nplt.figure(figsize=(15,10))\nfor i in range(5):\n    plt.subplot(2,3,i+1)\n    plt.hist(ages[list(ages.keys())[i]])\n    plt.xlabel(list(ages.keys())[i] + ' Age')\n    plt.ylabel('Frecuency')\n    plt.title(list(ages.keys())[i] + \"'s age distribution\")\n    \nplt.subplots_adjust(wspace=.5, hspace=.5)","65b014c4":"median_age = {}\nmedian_age['Master'], median_age['Miss'], median_age['Mr'], median_age['Mrs'], median_age['Royal'] =  train[['Title', 'Age']].groupby(['Title'],as_index=False).median()['Age'].values\nfor row in range(train.shape[0]):\n    if str(train['Age'][row]) == 'nan':\n        train['Age'][row] = median_age[train['Title'][row]]\n\nfor row in range(test.shape[0]):\n    if str(test['Age'][row]) == 'nan':\n        test['Age'][row] = median_age[test['Title'][row]]\n        \ndel train['Name']\ndel test['Name']\n","ad9318df":"train.head()","45a4368b":"print('Percentage survived = 1: ',((train['Survived'] == 1).sum()\/ train['Survived'].count()) * 100)\nprint('Percentage survived = 0: ',((train['Survived'] == 0).sum()\/ train['Survived'].count()) * 100)","c43883ed":"# plot Plcass, Sex, Sibspc, Parch, Embarked and Title since they are easier to visualize\nplt.figure(figsize=(15,20))\ncolumns = ['Pclass','Sex','SibSp','Parch','Embarked','Title']\ni=1\nfor column in columns:\n    plt.subplot(5,2,i)\n    sns.barplot(column, 'Survived', data=train)\n    i +=1\n\nplt.subplots_adjust(wspace=.5, hspace=.5)","23e22161":"# For visualizing fare we will make groups from 50 to 50 \n# For visualizing age we will make groups of 5 to 5\nfor frame in [train,test]:\n    frame['bin_age']=np.nan\n    frame['bin_age']=np.floor(frame['Age'])\/\/5\n    frame['bin_fare']=np.nan\n    frame['bin_fare']=np.floor(frame['Fare'])\/\/50","6ea52ce0":"x_label = [str((x-1)*5) + '-' + str(x*5) for x in range(1,17)]\naverage_age_bin = train[[\"bin_age\", \"Survived\"]].groupby(['bin_age'],as_index=False).mean()\n\nfacet = sns.FacetGrid(train, hue=\"Survived\",aspect=4)\nfacet.map(sns.kdeplot,'Age',shade= True)\nfacet.set(xlim=(0, train['Age'].max()))\nfacet.add_legend()\n\n\n# average survived passengers by age\nfig, axis1 = plt.subplots(1,1,figsize=(10,4))\nax = sns.barplot(x='bin_age', y='Survived', data=average_age_bin)\nax.set(xticklabels=x_label)\nax.set_xlabel('Age')\nplt.show()","2d46acc5":"x_label = [str((x-1)*50) + '-' + str(x*50) for x in range(1,8)]\naverage_fare = train[[\"bin_fare\", \"Survived\"]].groupby(['bin_fare'],as_index=False).mean()\nax = sns.barplot(x='bin_fare', y='Survived', data=average_fare)\nax.set(xticklabels=x_label)\nax.set_xlabel('Fare')\nplt.show()","35c6a2b2":"test.head()","0aaedbdf":"train_new = pd.get_dummies(train, columns=['Sex','Embarked','Title'], drop_first=True)\ntest_new = pd.get_dummies(test, columns=['Sex','Embarked','Title'], drop_first=True)","ce7013cd":"test_new['Title royal'] = np.zeros(test_new.shape[0], dtype=int)","fe4b6c84":"train_new.head()","b14e1379":"X_train= train_new.drop(['PassengerId','Survived'], axis=1)\nY_train = train_new['Survived']\nX_test = test_new.drop(['PassengerId'],axis=1)","ef3733bb":"X_train, X_val, y_train, y_val = train_test_split( X_train, Y_train, test_size=0.1, random_state=42)","1389609a":"# Gaussian Naive Bayes\ngaussian = GaussianNB()\ngaussian.fit(X_train, y_train)\ny_pred = gaussian.predict(X_val)\ngaussian_acc = round(accuracy_score(y_pred, y_val) * 100, 2)\ngaussian_acc\n","12c1a1d7":"#K-Nearest Neighbors\nknn = KNeighborsClassifier()\nknn.fit(X_train, y_train)\ny_pred = knn.predict(X_val)\nknn_acc = round(accuracy_score(y_pred, y_val) * 100, 2)\nknn_acc","2840eec2":"#Suport Vector Classifier\nsvc = SVC()\nsvc.fit(X_train, y_train)\ny_pred = svc.predict(X_val)\nsvc_acc = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(svc_acc)","b2b9b4c3":"#Decision Tree\ndecisiontree = DecisionTreeClassifier()\ndecisiontree.fit(X_train, y_train)\ny_pred = decisiontree.predict(X_val)\ndecision_tree_acc = round(accuracy_score(y_pred, y_val) * 100, 2)\ndecision_tree_acc","6cef7d45":"#Random Forest\nrandomforest = RandomForestClassifier(n_estimators=10000)\nrandomforest.fit(X_train, y_train)\ny_pred = randomforest.predict(X_val)\nrandom_forest_acc = round(accuracy_score(y_pred, y_val) * 100, 2)\nrandom_forest_acc","6f6cf80b":"#Gradient Boosting Classifier\ngbc = GradientBoostingClassifier()\ngbc.fit(X_train, y_train)\ny_pred = gbc.predict(X_val)\ngbc_acc = round(accuracy_score(y_pred, y_val) * 100, 2)\ngbc_acc","d342c184":"# Logistic Regression\nlogreg = LogisticRegression()\nlogreg.fit(X_train, y_train)\ny_pred = logreg.predict(X_val)\nlogreg_acc= round(accuracy_score(y_pred, y_val) * 100, 2)\nlogreg_acc","280a729b":"models = ['Naive Bayes', 'KNN', 'Support Vector Classifier', 'Decision Tree', 'Random Forest', 'Gradient Boosting Classifier', 'Logicstic Regresion' ]\nscores = [gaussian_acc, knn_acc, svc_acc, decision_tree_acc, random_forest_acc, gbc_acc, logreg_acc]\ndata = {'Models':models, 'Accuracy': scores}\nresults = pd.DataFrame(data)\nresults.sort_values(by='Accuracy', ascending=False)","54784e7d":"passengerIds = test_new['PassengerId']\npredictions = randomforest.predict(X_test)\nresult = pd.DataFrame({'PassengerId': passengerIds, 'Survived': predictions})\nresult.to_csv('predictions.csv', index=False)","4af508c4":"## Submission ","68c402e4":"Excluding the Survival variable which is the dependent variable we have to predict and the PassangerId which acts only as an index and can be ignore, we have:\n* **categorical features**: \n *  Sex: whether the passanger was male or female\n*  **ordinal features**:\n * Pclass: Socio-economic status of the passanger's ticket, 1(Upper), 2(Middle), 3(Lower)\n * Age: age of the passanger in years. Has NaN values\n * SibSp: number of sibling and spouse aboard the Titanic\n * Parch: number of parents and children aboard the Titanic\n * Fare: Passanger Fare, Has one NaN value\n * Embarked; port of embarkation (C = Cherbourg, Q = Queenstown, S = Southampton). Has NaN values\n* **alphanumerical** \n * Ticket: Ticket number, composed of letters and\/or numbers\n * Cabin: Cabin number, composed of letters and\/or numbers. Has NaN values","d416ed88":"## Modelling","6452bf77":"## Getting the data ready for modeling","eb8b5d07":"## Data Cleaning","24819a5f":"## Data Understanding","e6c2474d":"**observations**:\n* People with higher PClass have higher probabilities of surviving\n* People that have 0-2 SibSp or 1-3 Parch have higher probability of survival\n* People that belong to Royalty have the higher probability of survival\n* Babies (0-10) and older people (75-80) have higer probabilities of survival. People from 25 to 35 have the lowest probabilities of survival\n* People that paid the most expensive fares had higher probabilities of survivals, likewise, people with the cheapest cabins had lower probabilities of survival\n* Female have much higher probabilities of survival\n","150d3630":"First we split the training set into training and validation(10%) to test the accuracy of our models","4dec1b20":"We get the higher accuracy from Decision Tree and Random Forest, therefore, I will use Random Forest to predict X_test","9c818f1c":"## Load labraries and data \nFirst we beggin by loading the necessary libraries and data to solve the problem.\n![](http:\/\/)Among the libraries needed are [Numpy ](http:\/\/https:\/\/www.numpy.org\/)used commonly for matrix operations, [Pandas](http:\/\/https:\/\/pandas.pydata.org\/) for data analysis, [Matplotlib](http:\/\/https:\/\/matplotlib.org\/) for data visualization and [Sckit-learn](http:\/\/https:\/\/scikit-learn.org\/stable\/) for Machine Learning","d3b65a23":"We will be training different models in order to compare the performance. This models are:\n* Naive Bayes\n* KNN\n* SVC\n* Decision Tree\n* Random Forest\n* Gradient Boosting Classifier\n* Random Forest","a8ec7f99":"As wee see in the charts above the distribution of the ages is not normal, so using the mean is not the right way to fill in missing values, instead, we use the median","3e1afe16":"We have 17 different titles, some titles rarely appear and I replace them with more common titles that I believe fall into the same age category.\nI assume that:\n* Capt = Capitan, Col = Colonel, Don, Dr = Doctor, Jonkheer, Major, Reverend, Sir, all fall into the same age category as Mr\n* Coutness, Mme = Madame, Lady, Dona, fall into the same age category as Mrs\n* Mlle = mademoiselle, fall into the same age category as Ms\n* Coutness, Lady, Sir, fall into a new category of people that belong to Royalty","70649334":"We need to change the categorical string variables into numerical values the model understands, there are different ways of accomplishing this, for example, for the Embarked variable setting, 0=S, 1=C, 2=Q. However, this can imply some ordering that doesn't make sense, like S>C>Q or that 2C = Q. For that reason we will use dummy variables that removes such relationships","e00a0c45":"* We will remove the Ticket variable since it does not look to have any effect\n* We will fill the Fare value with the mean value of the fare corresponding to the Pclass of the register\n* Cabin has NaN in 76% of it's values. We will delete this column since the majority are NaN\n* Embarked has only .22% of it's values as NaN. We can fill the NaN values with the most occured value, however, it does not seem really important for survival prediction\n* Age has 19.865% of its values as NaN.There are multiple ways to address this problem, one can fill these values by generiting random variables using the mean and the standard deviation as shown in [this kernel](https:\/\/www.kaggle.com\/omarelgabry\/a-journey-through-titanic\/notebook). However I find it more reasonable to use the titles in the passanger's name to get an estimate of the passanger's age as shown [in this kernel]('https:\/\/www.kaggle.com\/nadintamer\/titanic-survival-predictions-beginner') \n* We will create another feature (Title) derived from name that indicates the age and socioeconomic state of the passanger (This is normally done in Feature Engineering but we need it to fill the missing age values)\n* We will remove the Name variable once we create the title variable since it isn't usefull anymore\n\nRemember that what you do to the training set you have to do the testing set as well","22ca27c9":"## Data Visualization\nI got the age visualization from [here](https:\/\/www.kaggle.com\/omarelgabry\/a-journey-through-titanic\/notebook)"}}