{"cell_type":{"b54eacdc":"code","c2611d68":"code","963b3436":"code","ea839075":"code","514cd5b6":"code","b2fb54fd":"markdown","83bcd2ed":"markdown","071bd0da":"markdown","8fe78345":"markdown","9467a105":"markdown"},"source":{"b54eacdc":"import pandas as pd\nimport numpy as np\nimport os\nimport lightgbm as lgb","c2611d68":"path = \"..\/input\/m5-forecasting-accuracy\"\n\ncalendar = pd.read_csv(os.path.join(path, \"calendar.csv\"))\nselling_prices = pd.read_csv(os.path.join(path, \"sell_prices.csv\"))\nsample_submission = pd.read_csv(os.path.join(path, \"sample_submission.csv\"))\nsales = pd.read_csv(os.path.join(path, \"sales_train_evaluation.csv\"))","963b3436":"from sklearn.preprocessing import OrdinalEncoder\n\ndef prep_calendar(df):\n    df = df.drop([\"date\", \"weekday\", \"event_type_1\", \"event_type_2\"], axis=1)\n    df = df.assign(d = df.d.str[2:].astype(int))\n    to_ordinal = [\"event_name_1\", \"event_name_2\"] \n    df[to_ordinal] = df[to_ordinal].fillna(\"1\")\n    df[to_ordinal] = OrdinalEncoder(dtype=\"int\").fit_transform(df[to_ordinal]) + 1\n    to_int8 = [\"wday\", \"month\", \"snap_CA\", \"snap_TX\", \"snap_WI\"] + to_ordinal\n    df[to_int8] = df[to_int8].astype(\"int8\")\n    \n    return df\n\ncalendar = prep_calendar(calendar)\ncalendar.head()","ea839075":"from sklearn.model_selection import train_test_split\n\nLAGS = [7, 28]\nWINDOWS = [7, 28]\nFIRST = 1942 # first to predict\nLENGTH = 28\n\ndef demand_features(df):\n    \"\"\" Derive features from sales data and remove rows with missing values \"\"\"\n    \n    for lag in LAGS:\n        df[f'lag_t{lag}'] = df.groupby('id')['demand'].transform(lambda x: x.shift(lag)).astype(\"float32\")\n        for w in WINDOWS:\n            df[f'rolling_mean_lag{lag}_w{w}'] = df.groupby('id')[f'lag_t{lag}'].transform(lambda x: x.rolling(w).mean()).astype(\"float32\")\n        \n    return df\n\ndef demand_features_eval(df):\n    \"\"\" Same as demand_features but for the step-by-step evaluation \"\"\"\n    out = df.groupby('id', sort=False).last()\n    for lag in LAGS:\n        out[f'lag_t{lag}'] = df.groupby('id', sort=False)['demand'].nth(-lag-1).astype(\"float32\")\n        for w in WINDOWS:\n            out[f'rolling_mean_lag{lag}_w{w}'] = df.groupby('id', sort=False)['demand'].nth(list(range(-lag-w, -lag))).groupby('id', sort=False).mean().astype(\"float32\")\n    \n    return out.reset_index()\n\ndef prep_data(df, drop_d=1000):\n    \"\"\" Prepare model data sets \"\"\"\n    \n    # Kick out old dates\n    df = df.drop([\"d_\" + str(i+1) for i in range(drop_d)], axis=1)\n\n    # Reshape to long\n    df = df.assign(id=df.id.str.replace(\"_evaluation\", \"\"))\n    df = df.reindex(columns=df.columns.tolist() + [\"d_\" + str(FIRST + i) for i in range(LENGTH)])\n    df = df.melt(id_vars=[\"id\", \"item_id\", \"store_id\", \"state_id\", \"dept_id\", \"cat_id\"], var_name='d', value_name='demand')\n    df = df.assign(d=df.d.str[2:].astype(\"int64\"),\n                   demand=df.demand.astype(\"float32\"))\n    \n    # Add demand features\n    df = demand_features(df)\n    \n    # Remove rows with NAs\n    df = df[df.d > (drop_d + max(LAGS) + max(WINDOWS))]\n \n    # Join calendar & prices\n    df = df.merge(calendar, how=\"left\", on=\"d\")\n    df = df.merge(selling_prices, how=\"left\", on=[\"store_id\", \"item_id\", \"wm_yr_wk\"])\n    df = df.drop([\"wm_yr_wk\"], axis=1)\n    \n    # Ordinal encoding of remaining categorical fields\n    for v in [\"item_id\", \"store_id\", \"state_id\", \"dept_id\", \"cat_id\"]:\n        df[v] = OrdinalEncoder(dtype=\"int\").fit_transform(df[[v]]).astype(\"int16\") + 1\n    \n    # Determine list of covariables\n    x = list(set(df.columns) - {'id', 'd', 'demand'})\n            \n    # Split into test, valid, train\n    test = df[df.d >= FIRST - max(LAGS) - max(WINDOWS) - 28]\n    df = df[df.d < FIRST]\n\n    xtrain, xvalid, ytrain, yvalid = train_test_split(df[x], df[\"demand\"], test_size=0.1, shuffle=True, random_state=54)\n    train = lgb.Dataset(xtrain, label = ytrain)\n    valid = lgb.Dataset(xvalid, label = yvalid)\n\n    return train, valid, test, x\n\ndef fit_model(train, valid):\n    \"\"\" Fit LightGBM model \"\"\"\n     \n    params = {\n        'metric': 'rmse',\n        'objective': 'poisson',\n        'seed': 200,\n        'force_row_wise' : True,\n        'learning_rate' : 0.08,\n        'lambda': 0.1,\n        'num_leaves': 63,\n        'sub_row' : 0.7,\n        'bagging_freq' : 1,\n        'colsample_bytree': 0.7\n    }\n\n    fit = lgb.train(params, \n                    train, \n                    num_boost_round = 2000, \n                    valid_sets = [valid], \n                    early_stopping_rounds = 200,\n                    verbose_eval = 100)\n    \n    lgb.plot_importance(fit, importance_type=\"gain\", precision=0, height=0.5, figsize=(6, 10));\n    \n    return fit\n\ndef pred_all(fit, test, x):\n    \"\"\" Calculate predictions \"\"\"\n    \n    # Recursive prediction\n    for i, day in enumerate(np.arange(FIRST, FIRST + LENGTH)):\n        test_day = demand_features_eval(test[(test.d <= day) & (test.d >= day - max(LAGS) - max(WINDOWS))])\n        test.loc[test.d == day, \"demand\"] = fit.predict(test_day[x])\n    \n    return test\n\ndef pred_to_csv(test, cols=sample_submission.columns, file=\"submission.csv\"):\n    \"\"\" Reshape predictions and save submission csv \"\"\"\n     \n    # Prepare for reshaping\n    test = test.assign(id=test.id + \"_\" + np.where(test.d < FIRST, \"validation\", \"evaluation\"),\n                       F=\"F\" + (test.d - FIRST + LENGTH + 1 - LENGTH * (test.d >= FIRST)).astype(\"str\"))\n    \n    # Reshape\n    submission = test.pivot(index=\"id\", columns=\"F\", values=\"demand\").reset_index()[cols].fillna(1)\n    \n    # Export\n    submission.to_csv(file, index=False)\n    \n    return True","514cd5b6":"train, valid, test, x = prep_data(sales, 1000 - 28)\nfit = fit_model(train, valid)\npred = pred_all(fit, test, x)\npred_to_csv(pred, cols=sample_submission.columns, file=\"submission.csv\")","b2fb54fd":"## Run the code","83bcd2ed":"## Prepare calendar data","071bd0da":"## Load data","8fe78345":"# M5 Forecast: Poisson loss (top 10%)\n\nThis very simple script was lucky enough to get top 10% in private lb. \n\nI moved the time horizon of the previous version of this notebook by 28 days and removed all \"magic\" stuff. The winning solutions did clearly better in terms of performance, but simplicity is worth a lot when it comes to maintainance etc.","9467a105":"## Helper functions\n\nWe need the following functions."}}