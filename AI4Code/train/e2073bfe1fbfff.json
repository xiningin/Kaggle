{"cell_type":{"93ee7bce":"code","7ac6f46f":"code","4bd64273":"code","75be8368":"code","1ad91aba":"code","438b15b3":"code","dd27ed12":"code","d4b384d1":"code","8842cead":"code","e3deffd4":"code","f0778f78":"code","43ffba19":"code","14fd3551":"code","e48ac86e":"code","d8b484f6":"code","2cb2754e":"code","ba487890":"code","68bd4c6a":"code","1f034712":"markdown","9e4d10c6":"markdown","73db1cab":"markdown","3c1bbfd2":"markdown","c6d68a67":"markdown","00233a3e":"markdown","305dad09":"markdown","8472de00":"markdown","01aee162":"markdown","db179c11":"markdown","dbd33e04":"markdown","2f90417f":"markdown","ddda4c84":"markdown","1bc4c3b1":"markdown"},"source":{"93ee7bce":"import os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport glob\n%matplotlib inline","7ac6f46f":"# set start time\nimport time\nkernel_start_time = time.time()","4bd64273":"!pip install detectron2 -f \\\n  https:\/\/dl.fbaipublicfiles.com\/detectron2\/wheels\/cu102\/torch1.7\/index.html","75be8368":"import detectron2\nimport cv2\n\n\nimport logging\nfrom collections import OrderedDict\nimport torch\nfrom torch.nn.parallel import DistributedDataParallel\n\nimport detectron2.utils.comm as comm\nfrom detectron2.checkpoint import DetectionCheckpointer, PeriodicCheckpointer\nfrom detectron2.config import get_cfg\nfrom detectron2.data import (\n    MetadataCatalog,\n    build_detection_test_loader,\n    build_detection_train_loader,\n)\nfrom detectron2.engine import default_argument_parser, default_setup, launch\nfrom detectron2.evaluation import (\n    COCOEvaluator,\n    inference_on_dataset,\n    print_csv_format,\n)\nfrom detectron2.modeling import build_model\nfrom detectron2.solver import build_lr_scheduler, build_optimizer\nfrom detectron2.utils.events import (\n    CommonMetricPrinter,\n    EventStorage,\n    JSONWriter,\n    TensorboardXWriter,\n)\n\nfrom detectron2.data import DatasetCatalog, MetadataCatalog\nfrom tqdm import tqdm\nimport numpy as np\nfrom detectron2 import model_zoo\nfrom detectron2.structures import BoxMode\nimport detectron2.utils.comm as comm\nfrom detectron2.engine import DefaultPredictor","1ad91aba":"# configs\nDEBUG=False\nTRAIN_CSV_PATH = \"..\/input\/vinbigdata-competition-jpg-data-2x-downsampled\/train_downsampled.csv\"\nTRAIN_FOLDER = \"..\/input\/vinbigdata-competition-jpg-data-2x-downsampled\/train\/train\"\nEPOCHS =30000000 # set to high value. Training stops after around 2.5 hours\nRUNTIME = 2.5*60*60 # SECONDS\n# RUNTIME = 1*60*60\nEVALUATE_EVERY_N_STEPS = 15\nOUTPUT_DIR=\"outputs\"","438b15b3":"traindf = pd.read_csv(TRAIN_CSV_PATH)\ntraindf.sample(2)","dd27ed12":"from sklearn import model_selection\ngkf = model_selection.GroupKFold(5)\nfor train_index, test_index in gkf.split(X=traindf,  groups=traindf[\"image_id\"]):\n    dftrain_val = traindf.loc[test_index,:]\n    dftrain_tr = traindf.loc[train_index,:]\n    break\n\ndftrain_tr.shape,dftrain_val.shape","d4b384d1":"def get_train_dicts(df):\n    dataset_dicts = []\n    for idx, img_id in enumerate(df[\"image_id\"].unique()):\n        record ={}\n        filename = os.path.join(TRAIN_FOLDER,img_id+\".jpg\")\n        record[\"file_name\"] = filename\n        # print(filename)\n        h,w = cv2.imread(filename).shape[:2]\n        h_ratio = 0.5\n        w_ratio = 0.5\n        record[\"image_id\"] = img_id\n        record[\"height\"] = h\n        record[\"width\"] = w\n        objects = []\n        try:\n            for _,row in df[df[\"image_id\"]==img_id].iterrows():\n                object_ = {\n                    \"bbox\" : [int(row[\"x_min\"])*w_ratio,int(row[\"y_min\"])*h_ratio,int(row[\"x_max\"])*w_ratio,int(row[\"y_max\"])*h_ratio],\n                    \"bbox_mode\" : BoxMode.XYXY_ABS,\n                    \"category_id\" : row[\"class_id\"]\n                }\n                objects.append(object_)\n        except:\n            objects = []\n            pass\n        record[\"annotations\"] = objects\n        dataset_dicts.append(record)\n    return dataset_dicts\n# get_train_dicts(dftrain_val_sampled)\nclasses = dftrain_tr[[\"class_id\",\"class_name\"]].drop_duplicates().sort_values(\"class_id\")\nprint(classes[classes[\"class_name\"]!=\"No finding\"][\"class_id\"].tolist())\nclasses = classes[classes[\"class_name\"]!=\"No finding\"][\"class_name\"].tolist()\nclasses","8842cead":"if DEBUG == True:\n    nsample = 200\n    train_sampled = []\n    for class_ in classes+[\"No finding\"]:\n        sampledimids = np.random.choice(dftrain_tr[dftrain_tr[\"class_name\"]==class_][\"image_id\"].unique(),nsample)\n        train_sampled.extend(sampledimids)\n    dftrain_tr_sampled = dftrain_tr[dftrain_tr[\"image_id\"].isin(train_sampled)]\n\n\n    nsample = 100\n    val_sampled = []\n    for class_ in classes+[\"No finding\"]:\n        sampledimids = np.random.choice(dftrain_val[dftrain_val[\"class_name\"]==class_][\"image_id\"].unique(),nsample)\n        val_sampled.extend(sampledimids)\n    dftrain_val_sampled = dftrain_val[dftrain_val[\"image_id\"].isin(val_sampled)]\n    print(dftrain_tr_sampled.shape,dftrain_val_sampled.shape)\n\n    d= \"train\"\n    DatasetCatalog.register(\"vin_\" + d, lambda d=d: get_train_dicts(dftrain_tr_sampled))\n    MetadataCatalog.get(\"vin_\" + d).set(thing_classes=classes)\n    d= \"val\"\n    DatasetCatalog.register(\"vin_\" + d, lambda d=d: get_train_dicts(dftrain_val_sampled))\n    MetadataCatalog.get(\"vin_\" + d).set(thing_classes=classes,evaluator_type=\"coco\")\n    \nelse:\n    d= \"train\"\n    DatasetCatalog.register(\"vin_\" + d, lambda d=d: get_train_dicts(dftrain_tr))\n    MetadataCatalog.get(\"vin_\" + d).set(thing_classes=classes)\n    d= \"val\"\n    DatasetCatalog.register(\"vin_\" + d, lambda d=d: get_train_dicts(dftrain_val))\n    MetadataCatalog.get(\"vin_\" + d).set(thing_classes=classes,evaluator_type=\"coco\")\n    ","e3deffd4":"os.makedirs(OUTPUT_DIR,exist_ok=True) \ncfg = get_cfg()\ncfg.merge_from_file(model_zoo.get_config_file(\"COCO-Detection\/faster_rcnn_R_50_FPN_3x.yaml\")) # Pretrained model\ncfg.DATASETS.TRAIN = (\"vin_train\",)\ncfg.DATASETS.TEST = (\"vin_val\",)\ncfg.DATALOADER.FILTER_EMPTY_ANNOTATIONS=False #negative samples also included\ncfg.DATALOADER.NUM_WORKERS = 4\ncfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-Detection\/faster_rcnn_R_50_FPN_3x.yaml\")  \ncfg.SOLVER.IMS_PER_BATCH = 8\ncfg.SOLVER.BASE_LR = 0.00025  \ncfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 512  \ncfg.MODEL.ROI_HEADS.NUM_CLASSES = 14\ncfg.OUTPUT_DIR=OUTPUT_DIR\nprint(cfg)","f0778f78":"# set up model,optimizers,scheduler\nmodel = build_model(cfg)\nmodel.train()\noptimizer = build_optimizer(cfg, model)\nscheduler = build_lr_scheduler(cfg, optimizer)\n# checkpointer helps in saving --> checkpointer.save('filename.pth')\ncheckpointer = DetectionCheckpointer(\n        model, cfg.OUTPUT_DIR, optimizer=optimizer, scheduler=scheduler\n    )\n# writer object saves loss metrics\nwriter = JSONWriter(os.path.join( \"test_metrics2.json\"))\ndata_loader = build_detection_train_loader(cfg)","43ffba19":"def do_evaluate(cfg, model):\n    results = OrderedDict()\n    for dataset_name in cfg.DATASETS.TEST:\n        print(dataset_name)\n        data_loader = build_detection_test_loader(cfg, dataset_name)\n        evaluator = COCOEvaluator(dataset_name, cfg, True, \"inference\")\n        results_i = inference_on_dataset(model, data_loader, evaluator)\n        results[dataset_name] = results_i\n        print(results)\n        if comm.is_main_process():\n            logger.info(\"Evaluation results for {} in csv format:\".format(dataset_name))\n            print_csv_format(results_i)\n    if len(results) == 1:\n        results = list(results.values())[0]\n    return results","14fd3551":"loss_list = []\neval_list = []\ntotal_loss_list = []\nlogger = logging.getLogger(\"detectron2\")\nos.makedirs(OUTPUT_DIR,exist_ok=True)\nwith EventStorage(0) as storage:\n    for data, iteration in zip(data_loader, range(0, EPOCHS)):\n        storage.step() #--.needed\n#         print(iteration)\n        \n        loss_dict = model(data)\n#         print(loss_dict)\n        losses = sum(loss for loss in loss_dict.values())\n#         print(losses)\n        loss_dict_reduced = {k: v.item() for k, v in comm.reduce_dict(loss_dict).items()}\n#         print(loss_dict_reduced)\n        losses_reduced = sum(loss for loss in loss_dict_reduced.values())\n#         print(losses_reduced)\n        loss_list.append(loss_dict_reduced)\n        total_loss_list.append(losses_reduced)\n        # break\n\n        optimizer.zero_grad()\n        losses.backward()\n        optimizer.step()\n        scheduler.step()\n        storage.put_scalars(total_loss=losses_reduced, **loss_dict_reduced)\n        writer.write()\n        if iteration % EVALUATE_EVERY_N_STEPS==0:\n            results = do_evaluate(cfg, model)\n    #         print(results)\n            eval_list.append(results[\"bbox\"])\n            checkpointer.save(\"model_final\") \n        if (time.time() - kernel_start_time) >RUNTIME:\n            print(\"time uppp\")\n            break","e48ac86e":"df_loss = pd.DataFrame(loss_list)\ndf_eval = pd.DataFrame(eval_list)\ndf_totloss = pd.DataFrame({\"tot_loss\":total_loss_list})\ndf_totloss.plot()","d8b484f6":"df_eval[\"AP50\"].plot()","2cb2754e":"\ncfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")  # path to the model we just trained\ncfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0\ncfg.MODEL.ROI_HEADS.NMS_THRESH_TEST = 0.4\npredictor = DefaultPredictor(cfg)\n","ba487890":"from detectron2.utils.visualizer import Visualizer\nfrom detectron2.utils.visualizer import ColorMode\nimport random\nmodel.eval()\nval_dict = DatasetCatalog.get(\"vin_val\")\nval_metadata = MetadataCatalog.get(\"vin_val\")\nfor d in random.sample(val_dict, 10):\n    img = cv2.imread(d[\"file_name\"])\n    visualizer = Visualizer(img[:, :, ::-1], metadata=val_metadata, scale=1)\n    out = visualizer.draw_dataset_dict(d)\n    plt.imshow(out.get_image())\n    plt.show()\n    outputs = predictor(img)  # format is documented at https:\/\/detectron2.readthedocs.io\/tutorials\/models.html#model-output-format\n    v = Visualizer(img[:, :, ::-1],\n                   metadata=val_metadata, \n                   scale=1, \n                   )\n    out = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n    plt.imshow(out.get_image())\n    plt.show()\n    print(\"-------------------------\")","68bd4c6a":"df_loss.to_csv(\"loss_list.csv\")\ndf_eval.to_csv(\"eval_list.csv\")\ndf_totloss.to_csv(\"total_loss_list.csv\")","1f034712":"### Format data\nDetectron2 data prep: create list of dicts  \n![image.png](attachment:image.png)","9e4d10c6":"### Configs","73db1cab":"### **** **Import everything**","3c1bbfd2":"### Evaluation function for calculating coco metrics: APmean, AP50...","c6d68a67":"## Evaluate","00233a3e":"#### Detectron2 library can be used to quickly train object detection models. \nIn this kernel, I will be refactoring code from detectron2's official github for this. Most of my code here is from: https:\/\/github.com\/facebookresearch\/detectron2\/blob\/master\/tools\/plain_train_net.py\n\nHere, I am trying to make custom training loop without using detectrons's built in trainer. This would give me more control over the training.\n\nAt around 15,000 epochs, this model can give 0.2 lb score","305dad09":"Please consider upvoting the dataset: https:\/\/www.kaggle.com\/raddar\/vinbigdata-competition-jpg-data-2x-downsampled","8472de00":"Evaluation config","01aee162":"#### Object detection models require very large training iterations. Kaggle kernels have limited time allowed for continuous running of gpus. So I will be stopping the training when execution time approaches 3 hours","db179c11":"### Prepare detectron2 config\nPotential for hyperparameter tuning","dbd33e04":"### Validation split","2f90417f":"### Register Dataset","ddda4c84":"### Build model, optimizer, scheduler, checkpointer(for saving model), writer(logging), data loader","1bc4c3b1":"### Training Loop\nPoints to note:\n* Training loop needs to be included inside an EventStorage block.\n* storage to be 'steped' after every epoch\n* storage.put_scalars() + writer.write() records loss in json file\n* for loss calculation :refer https:\/\/github.com\/facebookresearch\/detectron2\/blob\/master\/tools\/plain_train_net.py\n* checkpointer.save(filename) saves file to cfg.OUTPUT_DIR\n"}}