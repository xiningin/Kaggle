{"cell_type":{"dc372e01":"code","d5644e35":"code","88730880":"code","b239243c":"code","dc01a61d":"code","294f1b5e":"code","9420ddb5":"code","2c28e654":"code","98eb3afc":"code","4ab63c4e":"code","855776d8":"code","f84a6614":"markdown","86ee7bcd":"markdown","8bc89e1d":"markdown","450706a8":"markdown","06188343":"markdown","69381430":"markdown","1f17042f":"markdown","30af2024":"markdown"},"source":{"dc372e01":"import numpy as np\nimport os\nimport cv2\nfrom sklearn.model_selection import train_test_split\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras.layers import Dense, Activation,Dropout,Conv2D, MaxPooling2D,BatchNormalization, Flatten\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.metrics import categorical_crossentropy\nfrom tensorflow.keras import regularizers\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\nfrom tensorflow.keras.applications import imagenet_utils\nfrom tensorflow.keras.preprocessing import image\nfrom tensorflow.keras.models import Model, load_model, Sequential\nfrom tensorflow.keras.utils import to_categorical\nimport matplotlib.pyplot as plt\nimport time\nimport os","d5644e35":"# set ansi color values\nCblu ='\\33[34m'\nCend='\\33[0m'   # sets color back to default \nCred='\\033[91m'\nCblk='\\33[39m'\nCgreen='\\33[32m'\nCyellow='\\33[33m'","88730880":"d=r'\/kaggle\/input'\nd_list=os.listdir(d)\nd_path= os.path.join(d, d_list[0])\nd_list=os.listdir(d_path)\nnext_path=os.path.join(d_path, d_list[0])\nnext_list=os.listdir(next_path)\ndeeper_path=os.path.join(next_path, next_list[0])\ndeeper_list=os.listdir(deeper_path)\ninput_path=os.path.join(deeper_path, 'images')\ninput_list=os.listdir(input_path)\nsize=len(input_list)\nprint ('length of images is ', size)","b239243c":"data_list=[]\nlabels=[]\nfor i,f in enumerate(input_list):    \n    f_path=os.path.join(input_path, f)\n    data_list.append(cv2.imread (f_path))\n    labels.append (1 if f[0]=='P' else 0) \nlabels = tf.keras.utils.to_categorical(labels, num_classes = 2)\nY=np.array(labels)\nX=np.array(data_list)\nprint ('X shape is ', X.shape, ' Y shape is ', Y.shape)\n# just check to make sure labels match with input file names \nfor i in range (10):\n    f_path=os.path.join(input_path, input_list[i])\n    f=os.path.basename(f_path)\n    print('label = ',labels[i], '    file names = ', f)","dc01a61d":"xTrain, x, yTrain, y =train_test_split(X, Y, test_size=.2)\nxTest, xValid, yTest,yValid=train_test_split(x,y, test_size=.4)","294f1b5e":"\ntrain_gen=ImageDataGenerator(preprocessing_function=keras.applications.mobilenet.preprocess_input,\n                horizontal_flip=True,\n                samplewise_center=True,\n                width_shift_range=.2,\n                height_shift_range=.2,                \n                samplewise_std_normalization=True).flow(xTrain, yTrain, )\nval_gen=ImageDataGenerator(preprocessing_function=keras.applications.mobilenet.preprocess_input,\n                samplewise_center=True,                \n                samplewise_std_normalization=True).flow(xValid, yValid, shuffle=False)        \ntest_gen=ImageDataGenerator(preprocessing_function=keras.applications.mobilenet.preprocess_input,\n                samplewise_center=True,                \n                samplewise_std_normalization=True).flow(xTest, yTest, shuffle=False)\n\n        ","9420ddb5":"Top=False\nweights=None\nlayer_cut=-6\nlr_rate=.002\nrand_seed=128\nepochs=20\nmobile = tf.keras.applications.mobilenet.MobileNet( include_top=True,\n                                                           input_shape=(224,224,3),\n                                                           pooling='avg', weights='imagenet',\n                                                           alpha=1, depth_multiplier=1)  \n                 \nx=mobile.layers[layer_cut].output\nx=Dense(128, kernel_regularizer = regularizers.l2(l = 0.015), activation='relu')(x)\nx=Dropout(rate=.5, seed=rand_seed)(x)\npredictions=Dense (2, activation='softmax')(x)\nmodel = Model(inputs=mobile.input, outputs=predictions)\n        \nfor layer in model.layers:\n    layer.trainable=True\nmodel.compile(Adam(lr=lr_rate), loss='categorical_crossentropy', metrics=['accuracy'])","2c28e654":"class tr(tf.keras.callbacks.Callback):\n    best_weights=model.get_weights()\n    best_acc=0\n    patience=10\n    p_count=0\n    focus='acc'\n    def __init__(self):\n        super(tr, self).__init__()\n        self.best_acc = 0\n        self.patience=10\n        self.p_count=0\n    def on_batch_end(self, batch, logs=None): \n        epoch=logs.get('epoch')\n        acc=logs.get('accuracy')\n        if tr.best_acc>.9:\n            if tr.focus=='acc':\n                msg='{0}\\n with training accuracy= {1:7.4f} will now start adjusting learning rate based on validation loss\\n{2}'\n                print(msg.format(Cblu, tr.best_acc, Cend))\n                tr.focus='val'\n        else:\n            if tr.best_acc<acc:\n                #accuracy at batch end is better then highest accuracy thus far\n                #msg='\\non batch {0} accuracy improved from {1:7.4f}  to {2:7.4f} \\n'\n               # print(msg.format(batch + 1, tr.best_acc, acc ))\n                tr.best_acc=acc\n                tr.p_count=0\n                tr.best_weights=model.get_weights()\n           \n            else:\n                #accuracy on current batch was below highest accuracy thus far\n                tr.p_count=tr.p_count + 1\n                #msg='\\n for batch {0} current accuracy {1:7.4f}  was below highest accuracy of {2:7.4f} for {3} batches'\n                #print(msg.format(batch + 1, acc, tr.best_acc,tr.p_count))\n                if tr.p_count >= tr.patience:\n                    tr.p_count=0\n                    lr=float(tf.keras.backend.get_value(self.model.optimizer.lr))\n                    new_lr=lr*.95\n                    tf.keras.backend.set_value(self.model.optimizer.lr, new_lr) \n                    print('\\n adjusted learning rate for batch {0} to {1}\\n'.format(batch + 1, new_lr))\n                    \nclass val(tf.keras.callbacks.Callback):\n    best_loss=np.inf\n    best_weights=tr.best_weights\n    lr=float(tf.keras.backend.get_value(model.optimizer.lr))\n    def __init__(self):\n        super(val, self).__init__()\n        self.best_loss=np.inf\n        self.best_weights=tr.best_weights\n        self.lr=float(tf.keras.backend.get_value(model.optimizer.lr))\n    def on_epoch_end(self, epoch, logs=None):            \n        v_loss=logs.get('val_loss')\n        v_acc=logs.get('val_accuracy')\n        \n        if v_loss<val.best_loss:\n            msg='{0}\\nfor epoch {1} validation loss improved,saving weights with validation loss= {2:7.4f}\\n{3}'\n            print(msg.format(Cgreen,epoch + 1, v_loss, Cend))\n            val.best_loss=v_loss\n            val.best_weights=model.get_weights()\n        else:\n            if tr.focus=='val':\n                    #validation loss did not improve at end of current epoch\n                    lr=float(tf.keras.backend.get_value(self.model.optimizer.lr))\n                    new_lr=lr * .7\n                    tf.keras.backend.set_value(model.optimizer.lr, new_lr)\n                    msg='{0}\\n for epoch {1} current loss {2:7.4f} exceeds best boss of {3:7.4f} reducing lr to {4:11.9f}{5}'\n                    print(msg.format(Cyellow,epoch + 1, v_loss, val.best_loss, new_lr,Cend))\n        val.lr=float(tf.keras.backend.get_value(model.optimizer.lr))\ncallbacks=[tr(), val()]","98eb3afc":"start_epoch=0\nstart=time.time()\nresults = model.fit_generator(generator = train_gen, validation_data= val_gen, epochs=epochs, initial_epoch=start_epoch,\n                       callbacks = callbacks, verbose=1)\nstop=time.time()\nduration = stop-start\nhrs=int(duration\/3600)\nmins=int((duration-hrs*3600)\/60)\nsecs= duration-hrs*3600-mins*60\nmsg='{0}Training took\\n {1} hours {2} minutes and {3:6.2f} seconds {4}'\nprint(msg.format(Cblu,hrs, mins,secs,Cend))\ntacc=results.history['accuracy']\ntloss=results.history['loss']\nvacc=results.history['val_accuracy']\nvloss=results.history['val_loss']\n        ","4ab63c4e":"Epoch_count=len(tloss)\nEpochs=[]\nfor i in range (0,Epoch_count):\n    Epochs.append(i+1)\nindex_loss=np.argmin(vloss)#  this is the epoch with the lowest validation loss\nval_lowest=vloss[index_loss]\nindex_acc=np.argmax(vacc)\nval_highest=vacc[index_acc]\nplt.style.use('fivethirtyeight')\nsc_label='best epoch= '+ str(index_loss+1)\nvc_label='best epoch= '+ str(index_acc + 1)\nfig,axes=plt.subplots(nrows=1, ncols=2, figsize=(15,5))\naxes[0].plot(Epochs,tloss, 'r', label='Training loss')\naxes[0].plot(Epochs,vloss,'g',label='Validation loss' )\naxes[0].scatter(index_loss+1,val_lowest, s=150, c= 'blue', label=sc_label)\naxes[0].set_title('Training and Validation Loss')\naxes[0].set_xlabel('Epochs')\naxes[0].set_ylabel('Loss')\naxes[0].legend()\naxes[1].plot (Epochs,tacc,'r',label= 'Training Accuracy')\naxes[1].plot (Epochs,vacc,'g',label= 'Validation Accuracy')\naxes[1].scatter(index_acc+1,val_highest, s=150, c= 'blue', label=vc_label)\naxes[1].set_title('Training and Validation Accuracy')\naxes[1].set_xlabel('Epochs')\naxes[1].set_ylabel('Accuracy')\naxes[1].legend()\nplt.tight_layout\n#plt.style.use('fivethirtyeight')\nplt.show()\n","855776d8":"lr_rate=val.lr \nweights=val.best_weights #( weights with lowest validation loss)\nconfig = model.get_config()\npmodel = Model.from_config(config)  # copy of the model\npmodel.set_weights(weights) #load saved weights with lowest validation loss\npmodel.compile(Adam(lr=lr_rate), loss='categorical_crossentropy', metrics=['accuracy'])    \nprint('Training has completed. Now loading test set to see how accurate the model is')\nresults=pmodel.evaluate(test_gen, verbose=0)\nprint('{0}Model accuracy on Test Set is {1:7.2f} % {2}'.format(Cblu,results[1]* 100, Cend))\n    ","f84a6614":"Read in the image files. Note the first character of the file name is either P or N these service to indetify the files \nclass label- P for pollen or N for not pollen. Change these to integer markers. 1 for polen, 0 for no polen .\nConver the image list and label list to NP arrays","86ee7bcd":"the code below loads the best weights saved during training into the model and predicts on the test set","8bc89e1d":"The code below plots the  training and validation data","450706a8":"Struggle to find the input images-in kaggle what it shows as the input tree is usually wrong","06188343":"this code initiates training and stores the data at the end of each epoch","69381430":"Next make data generators for train, test and validate\n","1f17042f":"Split the data into three sets, Train, Test and Valid","30af2024":"Define subclasses of Keras callbacks. Thse subclasses are of twotypes. On batch end is used to adjust the learning rates\nat the end of a batch, The learning rate is reduced by a factor of .95 if the training accuracy has not improved for 10 consequtive\nbatces. It continues to monitor training accuracy until the accuracy reaches 90%. At that point the batch end no longer controls the\nlearning. Control transfer to class VAL. This monitors the validation loss. If the validation loss has not reduced at the end of an\nepoch, the learning rates is reduced by a factor of .5."}}