{"cell_type":{"ac601554":"code","09da11e7":"code","037a7f1a":"code","c8773905":"code","4898fbcc":"code","7b25c8f1":"code","b6e5bbad":"code","82299731":"code","d637a8aa":"code","562ca6ea":"code","22550179":"code","fb1afd6b":"code","1fd23d22":"code","162109b9":"code","9213d316":"code","dc5684c3":"code","c2ca2e4c":"code","7ba3c9c3":"code","0c641319":"markdown","ce68c508":"markdown","cf0f1427":"markdown","74105a35":"markdown","5122897d":"markdown","3d62377c":"markdown","3502d65a":"markdown","e55a60f1":"markdown"},"source":{"ac601554":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\ncompareScore = []\ndf = pd.read_csv('..\/input\/falldata\/falldeteciton.csv')\ndf.info()","09da11e7":"data = df.set_index(\"ACTIVITY\")\ndf_A1 = data.drop([0,2,3,4,5],axis=0)\ndf_A1.head()","037a7f1a":"# linear regression\nx = df_A1.BP.values.reshape(-1,1)\ny = df_A1.HR.values.reshape(-1,1)\nplt.scatter(x,y)\nplt.xlabel(\"BP\")\nplt.ylabel(\"HR\")\nplt.show()","c8773905":"from sklearn.linear_model import LinearRegression\nlr = LinearRegression()\nlr.fit(x,y)\n\n# y = m*x+ b\nb = lr.intercept_\nm = lr.coef_\nprint(\"y =\",m,\"*x + \",b)","4898fbcc":"# multiple linear regression\ndf = pd.read_csv('..\/input\/falldata\/falldeteciton.csv')\ndata = df.set_index('ACTIVITY')\ndf_A1 = data.drop([0,2,3,4,5],axis=0)\ndf_A1.head()","7b25c8f1":"x = df_A1.iloc[:,[4,5]].values\ny = df_A1.CIRCLUATION.values.reshape(-1,1)\n\nfrom sklearn.linear_model import LinearRegression\n\nlr = LinearRegression()\nlr.fit(x,y)\n\n# y = m*x + b\nb = lr.intercept_\nm = lr.coef_\n\nprint(\"y = \",m,\"*x + \",b)","b6e5bbad":"#polynomial linear regression\ndf = pd.read_csv('..\/input\/falldata\/falldeteciton.csv')\ndata = df.set_index('ACTIVITY')\nx = data.BP.values.reshape(-1,1)\ny = data.CIRCLUATION.values.reshape(-1,1)\n\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import LinearRegression\n\npolynomial_reg = PolynomialFeatures(degree=4)\nx_new = polynomial_reg.fit_transform(x)\n\nlinear_reg = LinearRegression()\nlinear_reg.fit(x_new,y)\n\ny_new = linear_reg.predict(x_new)\nplt.plot(x_new,y_new,color=\"red\")\n","82299731":"#random forest\ndf = pd.read_csv('..\/input\/falldata\/falldeteciton.csv')\ndata = df.set_index('ACTIVITY')\ndf_A1 = data.drop([0,2,3,4,5],axis=0)\nx = df_A1.BP.values.reshape(-1,1)\ny = df_A1.HR.values.reshape(-1,1)\n\nfrom sklearn.ensemble import RandomForestRegressor\n\nrf = RandomForestRegressor(n_estimators = 100, random_state = 42)\nrf.fit(x,y)\n\nx_ = np.arange(min(x),max(x),0.01).reshape(-1,1)\ny_new = rf.predict(x_).reshape(-1,1)\n\nplt.scatter(x,y,color=\"red\")\nplt.plot(x_,y_new,color=\"green\")\n","d637a8aa":"# Decision Tree\ndf = pd.read_csv('..\/input\/falldata\/falldeteciton.csv')\ndata = df.set_index('ACTIVITY')\ndf_A1 = data.drop([0,2,3,4,5],axis=0)\n\nx = df_A1.HR.values.reshape(-1,1)\ny = df_A1.CIRCLUATION.values.reshape(-1,1)\n\nfrom sklearn.tree import DecisionTreeRegressor\ntree_reg = DecisionTreeRegressor()\ntree_reg.fit(x,y)\n\nx_ = np.arange(min(x),max(x),0.01).reshape(-1,1)\ny_new = tree_reg.predict(x_).reshape(-1,1)\n\nplt.scatter(x,y,color=\"red\")\nplt.plot(x_,y_new,color=\"green\")\n\n","562ca6ea":"#logistic regression\ndf = pd.read_csv('..\/input\/falldata\/falldeteciton.csv')\ndata = df.set_index('ACTIVITY')\ndf_A1 = data.drop([0,2,3,4,5],axis = 0)\ny = df_A1.BP.values\nprint(\"Mean of SL: \",np.mean(y))\ndf_A1.BP = [1 if each > np.mean(y) else 0 for each in df_A1.BP]\ny = df_A1.BP.values\nx_data = df_A1.drop(\"SL\",axis=1)\nx = (x_data - np.min(x_data))\/(np.max(x_data) - np.min(x_data)).values\n\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x_data,y,test_size=0.3,random_state = 42)\n\nfrom sklearn.linear_model import LogisticRegression\n   \nlr = LogisticRegression()\nlr.fit(x_train,y_train)\n     \nprint(\"Test accuracy: \",format(lr.score(x_test,y_test)))\nlrScore = lr.score(x_test, y_test) * 100\ncompareScore.append(lrScore)\n\n# confusion matrix\ny_true = y_test\ny_pred = lr.predict(x_test)\n\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_true,y_pred)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nf,ax = plt.subplots(figsize=(5,5))\nsns.heatmap(cm,annot = True,linewidths=0.5,linecolor=\"red\",fmt=\".0f\",ax=ax)\nplt.xlabel(\"y_true\")\nplt.ylabel(\"y_pred\")\nplt.show()","22550179":"# k-NN(k Nearest Neighbor)\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndf = pd.read_csv('..\/input\/falldata\/falldeteciton.csv')\ndf.drop(['ACTIVITY','TIME','SL','EEG'],axis=1,inplace=True)\ny_data = df.HR.values\nmean = np.mean(y_data)\ny = [1 if each > mean else 0 for each in y_data] \nx_data = df.drop('HR',axis=1)\n\n\n#Normalization\nx = ((x_data - np.min(x_data)) \/ (np.max(x_data) - np.min(x_data))).values\n\nfrom sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split(x,y,test_size = 0.3,random_state = 42)\n\nfrom sklearn.neighbors import KNeighborsClassifier\n\nknn = KNeighborsClassifier(n_neighbors = 3)\nknn.fit(x_train,y_train)\nprint(\"{} nn score: {}\".format(3,knn.score(x_test,y_test)))\nknnScore = knn.score(x_test, y_test) * 100\ncompareScore.append(knnScore)\n# confusion matrix\ny_true = y_test\ny_pred = knn.predict(x_test)\n\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_true,y_pred)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nf,ax = plt.subplots(figsize=(5,5))\nsns.heatmap(cm,annot = True,linewidths=0.5,linecolor=\"red\",fmt=\".0f\",ax=ax)\nplt.xlabel(\"y_true\")\nplt.ylabel(\"y_pred\")\nplt.show()","fb1afd6b":"# SVM(Support Vector Machine)\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndf = pd.read_csv('..\/input\/falldata\/falldeteciton.csv')\na = df['CIRCLUATION']\ndf.CIRCLUATION = [1 if each > np.mean(a) else 0 for each in df.CIRCLUATION]\n\ncir_1 = df[df.CIRCLUATION == 1]\ncir_0 = df[df.CIRCLUATION == 0]\n\nsns.countplot(x='CIRCLUATION',data = df)\ndf.loc[:,'CIRCLUATION'].value_counts()","1fd23d22":"plt.scatter(cir_1.BP,cir_1.HR,label='High_Circluation',color=\"red\")\nplt.scatter(cir_0.BP,cir_0.HR,label='Low_Circluation',color=\"green\")\nplt.xlabel(\"BP\")\nplt.ylabel(\"HR\")\nplt.show()","162109b9":"y = df.CIRCLUATION.values.reshape(-1,1)\nx_data = df.drop('CIRCLUATION',axis=1)\n\nx = ((x_data - np.min(x_data)) \/ (np.max(x_data) - np.min(x_data))).values\n\nfrom sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.3,random_state=42)\n\nfrom sklearn.svm import SVC\nsvm = SVC(random_state=1)\nsvm.fit(x_train,y_train)\n\nprint(\"print accuracy SVM: \",svm.score(x_test,y_test))\nsvmScore = svm.score(x_test, y_test) * 100\ncompareScore.append(svmScore)\n# Confusion Matrix\n\ny_pred = svm.predict(x_test)\ny_true = y_test\n\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_true,y_pred)\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nf, ax = plt.subplots(figsize=(5,5))\nsns.heatmap(cm,annot = True, linewidths = 0.5, linecolor=\"red\", fmt=\".0f\",ax=ax)\nplt.xlabel(\"y_pred\")\nplt.ylabel(\"y_true\")\nplt.show()\n","9213d316":"# naive-bayes\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndf = pd.read_csv('..\/input\/falldata\/falldeteciton.csv')\ndf.TIME = [1 if each > np.mean(df.TIME) else 0 for each in df.TIME]\n\ntime_less = df[df.TIME == 0]\ntime_more = df[df.TIME == 1]\n\nsns.countplot(x = 'TIME', data = df)\ndf.loc[:,'TIME'].value_counts\n","dc5684c3":"plt.scatter(time_less.BP,time_less.HR,label = \"less_time\",color = \"red\")\nplt.scatter(time_more.BP,time_more.HR,label = \"more_time\",color = \"green\")\nplt.xlabel(\"BP\")\nplt.ylabel(\"HR\")\nplt.show()\n","c2ca2e4c":"y = df.TIME.values.reshape(-1,1)\nx_data = df.drop('TIME',axis = 1)\n\nx = ((x_data - np.min(x_data)) \/ (np.max(x_data) - np.min(x_data))).values\n\nfrom sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.3,random_state = 42)\n\nfrom sklearn.naive_bayes import GaussianNB\n\nnb = GaussianNB()\n\nnb.fit(x_train,y_train)\n\nprint(\"accuracy naive bayes: \",nb.score(x_test,y_test))\nnbScore = nb.score(x_test, y_test) * 100\ncompareScore.append(nbScore)\n# Confusion Matrix\n\ny_pred = nb.predict(x_test)\ny_true = y_test\n\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_true,y_pred)\n\n# %% cm visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nf, ax = plt.subplots(figsize = (5,5))\nsns.heatmap(cm,annot = True, linewidths = 0.5,linecolor=\"red\",fmt = \".0f\", ax=ax)\nplt.xlabel(\"y_predict\")\nplt.ylabel(\"y_true\")\nplt.show()\n","7ba3c9c3":"from plotly.offline import init_notebook_mode, iplot\ninit_notebook_mode(connected=True)\nimport plotly.graph_objs as go\n\nalgoList = [\"LogisticRegression\", \"KNN\", \"SVM\", \"NaiveBayes\"]\ncomparison = {\"Models\" : algoList, \"Accuracy\" : compareScore}\ndfComparison = pd.DataFrame(comparison)\n\nnewIndex = (dfComparison.Accuracy.sort_values(ascending = False)).index.values\nsorted_dfComparison = dfComparison.reindex(newIndex)\n\n\ndata = [go.Bar(\n               x = sorted_dfComparison.Models,\n               y = sorted_dfComparison.Accuracy,\n               name = \"Scores of Models\",\n               marker = dict(color = \"rgba(116,173,209,0.8)\",\n                             line=dict(color='rgb(0,0,0)',width=1.0)))]\n\nlayout = go.Layout(xaxis= dict(title= 'Models',ticklen= 5,zeroline= False))\n\nfig = go.Figure(data = data, layout = layout)\n\niplot(fig)\n","0c641319":"I want to see the relationship for BP and HP","ce68c508":"Using Linear Regression, I tried to linearize the values of HR and BP. ","cf0f1427":"As seen from the plot, we can conclude the relation between the values of HR and CIRCLUATION using Decision Tree Method.(category 1)","74105a35":"Apply Decision Tree method,using the values of HR and CIRCLUATION. ","5122897d":"Linearizing the values of BP and CIRCLUATION using polynomial linear regression","3d62377c":"I want to apply some of machine learning techniques for the dataset which includes only category 1(Walking)","3502d65a":"Apply Random Forest method,using the values of BP and HR. ","e55a60f1":"Using Multiple Linear Regrssion, I tooked the values BP and HR as a mulitple input for linearizing against CIRCLUATION.(Category 1)"}}