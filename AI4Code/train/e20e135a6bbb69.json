{"cell_type":{"6c2fd9e4":"code","0bfb449b":"code","0f2c5561":"code","ba73fb38":"code","4e1a1117":"code","e67aa557":"code","55928e25":"code","d85e45a0":"code","373dc3bf":"code","032d5d6c":"code","e05f44ff":"code","24897f42":"code","bf20c1e7":"code","a40b1837":"code","7d52d6fc":"code","dc1b28e9":"markdown","0af3a4ec":"markdown","76fb5d29":"markdown","69f908af":"markdown","83811d44":"markdown"},"source":{"6c2fd9e4":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session\n\npd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', None)","0bfb449b":"import sklearn\nsklearn.__version__","0f2c5561":"from sklearn.experimental import enable_iterative_imputer\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score\n\nfrom xgboost import XGBRegressor\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMRegressor\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostRegressor\nfrom catboost import CatBoostClassifier","ba73fb38":"train = pd.read_csv(\"..\/input\/tabular-playground-series-sep-2021\/train.csv\")\ntest = pd.read_csv(\"..\/input\/tabular-playground-series-sep-2021\/test.csv\")\nsample_solution = pd.read_csv(\"..\/input\/tabular-playground-series-sep-2021\/sample_solution.csv\")","4e1a1117":"xgb_params = {\n    'n_estimators' : 3600,\n    'reg_lambda' : 3,\n    'reg_alpha' : 26,\n    'subsample' : 0.6000000000000001,\n    'colsample_bytree' : 0.6000000000000001,\n    'max_depth' : 9,\n    'min_child_weight' : 5,\n    'gamma' : 13.054739572819486,\n    'learning_rate': 0.01,\n    'tree_method': 'gpu_hist',\n    'booster': 'gbtree'\n}\n\nlgbm_params = {\n    \"objective\": \"binary\",\n    \"learning_rate\": 0.008,\n    'device': 'gpu',\n    'n_estimators': 3205,\n    'num_leaves': 184,\n    'min_child_samples': 63,\n    'feature_fraction': 0.6864594334728974,\n    'bagging_fraction': 0.9497327922401265,\n    'bagging_freq': 1,\n    'reg_alpha': 19,\n    'reg_lambda': 19,\n    'gpu_platform_id': 0,\n    'gpu_device_id': 0,\n    'verbose' : -1\n}\n\ncatb_params = {\n    'iterations': 15585, \n    'objective': 'CrossEntropy', \n    'bootstrap_type': 'Bernoulli', \n    'od_wait': 1144, \n    'learning_rate': 0.023575206684596582, \n    'reg_lambda': 36.30433203563295, \n    'random_strength': 43.75597655616195, \n    'depth': 7, \n    'min_data_in_leaf': 11, \n    'leaf_estimation_iterations': 1, \n    'subsample': 0.8227911142845009,\n    'task_type' : 'GPU',\n    'devices' : '0',\n    'verbose' : 0\n}","e67aa557":"# quantile + median dataset\nquantile_train = train.copy()\n\nmedian_set = ['f9', 'f12', 'f26', 'f27', 'f28', 'f32', 'f33', 'f35', 'f62', 'f74', 'f82', 'f86', 'f98', 'f108', 'f116']\n\nfor f in range(1,119):\n    col_name = f'f{f}'\n    \n    if col_name in median_set:\n        quantile_train[col_name].fillna(train[col_name].median(), inplace=True)\n    else:\n        quantile_train[col_name].fillna(train[col_name].quantile(0.75), inplace=True)\n\ndrop_set = ['id', 'claim']\nX = quantile_train.drop(drop_set, axis = 1)\nY = quantile_train['claim']\n# ---------------------------------------------------------------------- data setting\nquantile_train = None","55928e25":"# xgboost\nmodel = XGBClassifier(**xgb_params)\ntrain_oof = np.zeros((957919,))\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\nfor fold, (train_idx, val_idx) in enumerate(skf.split(X, Y)):\n    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n    y_train, y_val = Y.iloc[train_idx], Y.iloc[val_idx]\n    \n    model = model.fit(X_train, y_train, verbose=0)\n    temp_oof = model.predict_proba(X_val)[:, 1]\n    train_oof[val_idx] = temp_oof\n    print(f'Fold {fold} AUC : ', roc_auc_score(y_val, temp_oof))\n    \nprint(f'XgBoost OOF AUC : ', roc_auc_score(Y, train_oof))\n\n# lgbm\nmodel = LGBMClassifier(**lgbm_params)\ntrain_oof = np.zeros((957919,))\nskf = StratifiedKFold(shuffle=True, random_state=42)\n\nfor fold, (train_idx, val_idx) in enumerate(skf.split(X, Y)):\n    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n    y_train, y_val = Y.iloc[train_idx], Y.iloc[val_idx]\n    \n    model = model.fit(X_train, y_train, verbose=0)\n    temp_oof = model.predict_proba(X_val)[:, 1]\n    train_oof[val_idx] = temp_oof\n    print(f'Fold {fold} AUC : ', roc_auc_score(y_val, temp_oof))\n    \nprint(f'LGBM OOF AUC : ', roc_auc_score(Y, train_oof))\n\n# catboost\nmodel = CatBoostClassifier(**catb_params)\ntrain_oof = np.zeros((957919,))\nskf = StratifiedKFold(shuffle=True, random_state=42)\n\nfor fold, (train_idx, val_idx) in enumerate(skf.split(X, Y)):\n    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n    y_train, y_val = Y.iloc[train_idx], Y.iloc[val_idx]\n    \n    model = model.fit(X_train, y_train, verbose=0)\n    temp_oof = model.predict_proba(X_val)[:, 1]\n    train_oof[val_idx] = temp_oof\n    print(f'Fold {fold} AUC : ', roc_auc_score(y_val, temp_oof))\n    \nprint(f'Catboost OOF AUC : ', roc_auc_score(Y, train_oof))","d85e45a0":"# new_feature : null has\n# \nquantile_train = train.copy()\nmedian_set = ['f9', 'f12', 'f26', 'f27', 'f28', 'f32', 'f33', 'f35', 'f62', 'f74', 'f82', 'f86', 'f98', 'f108', 'f116']\n\nquantile_train['n_missing'] = train.isnull().sum(axis = 1)  \n\n    \n# Fill Null by using quantile + median\nfor f in range(1,119):\n    col_name = f'f{f}'\n    \n    if col_name in median_set:\n        quantile_train[col_name].fillna(train[col_name].median(), inplace=True)\n    else:\n        quantile_train[col_name].fillna(train[col_name].quantile(0.75), inplace=True)\n\n\n    \ndrop_set = ['id', 'claim']\nX = quantile_train.drop(drop_set, axis = 1)\nY = quantile_train['claim']\n# ---------------------------------------------------------------------- data setting\nquantile_train = None","373dc3bf":"# xgboost\nmodel = XGBClassifier(**xgb_params)\ntrain_oof = np.zeros((957919,))\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\nfor fold, (train_idx, val_idx) in enumerate(skf.split(X, Y)):\n    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n    y_train, y_val = Y.iloc[train_idx], Y.iloc[val_idx]\n    \n    model = model.fit(X_train, y_train, verbose=0)\n    temp_oof = model.predict_proba(X_val)[:, 1]\n    train_oof[val_idx] = temp_oof\n    print(f'Fold {fold} AUC : ', roc_auc_score(y_val, temp_oof))\n    \nprint(f'XgBoost OOF AUC : ', roc_auc_score(Y, train_oof))\n\n# lgbm\nmodel = LGBMClassifier(**lgbm_params)\ntrain_oof = np.zeros((957919,))\nskf = StratifiedKFold(shuffle=True, random_state=42)\n\nfor fold, (train_idx, val_idx) in enumerate(skf.split(X, Y)):\n    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n    y_train, y_val = Y.iloc[train_idx], Y.iloc[val_idx]\n    \n    model = model.fit(X_train, y_train, verbose=0)\n    temp_oof = model.predict_proba(X_val)[:, 1]\n    train_oof[val_idx] = temp_oof\n    print(f'Fold {fold} AUC : ', roc_auc_score(y_val, temp_oof))\n    \nprint(f'LGBM OOF AUC : ', roc_auc_score(Y, train_oof))\n\n# catboost\nmodel = CatBoostClassifier(**catb_params)\ntrain_oof = np.zeros((957919,))\nskf = StratifiedKFold(shuffle=True, random_state=42)\n\nfor fold, (train_idx, val_idx) in enumerate(skf.split(X, Y)):\n    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n    y_train, y_val = Y.iloc[train_idx], Y.iloc[val_idx]\n    \n    model = model.fit(X_train, y_train, verbose=0)\n    temp_oof = model.predict_proba(X_val)[:, 1]\n    train_oof[val_idx] = temp_oof\n    print(f'Fold {fold} AUC : ', roc_auc_score(y_val, temp_oof))\n    \nprint(f'Catboost OOF AUC : ', roc_auc_score(Y, train_oof))","032d5d6c":"# new_feature : multiply all features\nfeatures = list(train.columns[1:119])\n\nquantile_train = train.copy()\nquantile_train['multiply'] = 1\n\nfor feature in features:\n    quantile_train['multiply'] = quantile_train[feature] * quantile_train['multiply']","e05f44ff":"# xgboost\nmodel = XGBClassifier(**xgb_params)\ntrain_oof = np.zeros((957919,))\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\nfor fold, (train_idx, val_idx) in enumerate(skf.split(X, Y)):\n    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n    y_train, y_val = Y.iloc[train_idx], Y.iloc[val_idx]\n    \n    model = model.fit(X_train, y_train, verbose=0)\n    temp_oof = model.predict_proba(X_val)[:, 1]\n    train_oof[val_idx] = temp_oof\n    print(f'Fold {fold} AUC : ', roc_auc_score(y_val, temp_oof))\n    \nprint(f'XgBoost OOF AUC : ', roc_auc_score(Y, train_oof))\n\n# lgbm\nmodel = LGBMClassifier(**lgbm_params)\ntrain_oof = np.zeros((957919,))\nskf = StratifiedKFold(shuffle=True, random_state=42)\n\nfor fold, (train_idx, val_idx) in enumerate(skf.split(X, Y)):\n    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n    y_train, y_val = Y.iloc[train_idx], Y.iloc[val_idx]\n    \n    model = model.fit(X_train, y_train, verbose=0)\n    temp_oof = model.predict_proba(X_val)[:, 1]\n    train_oof[val_idx] = temp_oof\n    print(f'Fold {fold} AUC : ', roc_auc_score(y_val, temp_oof))\n    \nprint(f'LGBM OOF AUC : ', roc_auc_score(Y, train_oof))\n\n# catboost\nmodel = CatBoostClassifier(**catb_params)\ntrain_oof = np.zeros((957919,))\nskf = StratifiedKFold(shuffle=True, random_state=42)\n\nfor fold, (train_idx, val_idx) in enumerate(skf.split(X, Y)):\n    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n    y_train, y_val = Y.iloc[train_idx], Y.iloc[val_idx]\n    \n    model = model.fit(X_train, y_train, verbose=0)\n    temp_oof = model.predict_proba(X_val)[:, 1]\n    train_oof[val_idx] = temp_oof\n    print(f'Fold {fold} AUC : ', roc_auc_score(y_val, temp_oof))\n    \nprint(f'Catboost OOF AUC : ', roc_auc_score(Y, train_oof))","24897f42":"# mean filled data\nfeatures = list(train.columns[1:119])\nmean_data = train.copy()\n\nmean_data['n_missing'] = mean_data[features].isna().sum(axis=1)\n\nmean_data = train.fillna(train.mean())\n\n# mean_data['min'] = mean_data[features].min(axis=1)\n# mean_data['max'] = mean_data[features].max(axis=1)\n# mean_data['mean'] = mean_data[features].mean(axis=1)\n# mean_data['std'] = mean_data[features].std(axis=1)\n\n\ndrop_set = ['id', 'claim']\nX = mean_data.drop(drop_set, axis = 1)\nY = mean_data['claim']\n# ----------------------------------------------------------------------------------------data seting\n\nmean_data = None","bf20c1e7":"# xgboost\nmodel = XGBClassifier(**xgb_params)\ntrain_oof = np.zeros((957919,))\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\nfor fold, (train_idx, val_idx) in enumerate(skf.split(X, Y)):\n    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n    y_train, y_val = Y.iloc[train_idx], Y.iloc[val_idx]\n    \n    model = model.fit(X_train, y_train, verbose=0)\n    temp_oof = model.predict_proba(X_val)[:, 1]\n    train_oof[val_idx] = temp_oof\n    print(f'Fold {fold} AUC : ', roc_auc_score(y_val, temp_oof))\n    \nprint(f'XgBoost OOF AUC : ', roc_auc_score(Y, train_oof))\n\n# lgbm\nmodel = LGBMClassifier(**lgbm_params)\ntrain_oof = np.zeros((957919,))\nskf = StratifiedKFold(shuffle=True, random_state=42)\n\nfor fold, (train_idx, val_idx) in enumerate(skf.split(X, Y)):\n    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n    y_train, y_val = Y.iloc[train_idx], Y.iloc[val_idx]\n    \n    model = model.fit(X_train, y_train, verbose=0)\n    temp_oof = model.predict_proba(X_val)[:, 1]\n    train_oof[val_idx] = temp_oof\n    print(f'Fold {fold} AUC : ', roc_auc_score(y_val, temp_oof))\n    \nprint(f'LGBM OOF AUC : ', roc_auc_score(Y, train_oof))\n\n# catboost\nmodel = CatBoostClassifier(**catb_params)\ntrain_oof = np.zeros((957919,))\nskf = StratifiedKFold(shuffle=True, random_state=42)\n\nfor fold, (train_idx, val_idx) in enumerate(skf.split(X, Y)):\n    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n    y_train, y_val = Y.iloc[train_idx], Y.iloc[val_idx]\n    \n    model = model.fit(X_train, y_train, verbose=0)\n    temp_oof = model.predict_proba(X_val)[:, 1]\n    train_oof[val_idx] = temp_oof\n    print(f'Fold {fold} AUC : ', roc_auc_score(y_val, temp_oof))\n    \nprint(f'Catboost OOF AUC : ', roc_auc_score(Y, train_oof))","a40b1837":"# mean filled data\nfeatures = list(train.columns[1:119])\nmean_data = train.copy()\n\nmean_data['n_missing'] = mean_data[features].isna().sum(axis=1)\n\nmean_data = train.fillna(train.mean())\n\nmean_data['min'] = mean_data[features].min(axis=1)\nmean_data['max'] = mean_data[features].max(axis=1)\nmean_data['mean'] = mean_data[features].mean(axis=1)\nmean_data['std'] = mean_data[features].std(axis=1)\n\n\ndrop_set = ['id', 'claim']\nX = mean_data.drop(drop_set, axis = 1)\nY = mean_data['claim']\n# ----------------------------------------------------------------------------------------data seting\n\nmean_data = None","7d52d6fc":"# xgboost\nmodel = XGBClassifier(**xgb_params)\ntrain_oof = np.zeros((957919,))\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\nfor fold, (train_idx, val_idx) in enumerate(skf.split(X, Y)):\n    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n    y_train, y_val = Y.iloc[train_idx], Y.iloc[val_idx]\n    \n    model = model.fit(X_train, y_train, verbose=0)\n    temp_oof = model.predict_proba(X_val)[:, 1]\n    train_oof[val_idx] = temp_oof\n    print(f'Fold {fold} AUC : ', roc_auc_score(y_val, temp_oof))\n    \nprint(f'XgBoost OOF AUC : ', roc_auc_score(Y, train_oof))\n\n# lgbm\nmodel = LGBMClassifier(**lgbm_params)\ntrain_oof = np.zeros((957919,))\nskf = StratifiedKFold(shuffle=True, random_state=42)\n\nfor fold, (train_idx, val_idx) in enumerate(skf.split(X, Y)):\n    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n    y_train, y_val = Y.iloc[train_idx], Y.iloc[val_idx]\n    \n    model = model.fit(X_train, y_train, verbose=0)\n    temp_oof = model.predict_proba(X_val)[:, 1]\n    train_oof[val_idx] = temp_oof\n    print(f'Fold {fold} AUC : ', roc_auc_score(y_val, temp_oof))\n    \nprint(f'LGBM OOF AUC : ', roc_auc_score(Y, train_oof))\n\n# catboost\nmodel = CatBoostClassifier(**catb_params)\ntrain_oof = np.zeros((957919,))\nskf = StratifiedKFold(shuffle=True, random_state=42)\n\nfor fold, (train_idx, val_idx) in enumerate(skf.split(X, Y)):\n    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n    y_train, y_val = Y.iloc[train_idx], Y.iloc[val_idx]\n    \n    model = model.fit(X_train, y_train, verbose=0)\n    temp_oof = model.predict_proba(X_val)[:, 1]\n    train_oof[val_idx] = temp_oof\n    print(f'Fold {fold} AUC : ', roc_auc_score(y_val, temp_oof))\n    \nprint(f'Catboost OOF AUC : ', roc_auc_score(Y, train_oof))","dc1b28e9":"# mean + all feature setting ------------------------------","0af3a4ec":"# quantile + multiply setting -----------------------------------------","76fb5d29":"# Quantile + missing setting--------------------------","69f908af":"# mean + missing ------------------------------------------------------------","83811d44":"# quantile_________________________________________________________________________"}}