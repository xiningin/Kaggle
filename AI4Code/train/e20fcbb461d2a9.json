{"cell_type":{"3cc4f691":"code","0d83daf6":"code","00a990cc":"code","33cb70e4":"code","90b4f331":"code","de66789c":"code","7e2fe1a6":"code","fab75f9d":"code","9d60e852":"code","1110004a":"code","97a40eb7":"code","5186cad2":"code","b9d24705":"code","2cf36b92":"code","3a15eb4b":"code","ceb23cdc":"code","414eea1f":"code","7077bece":"code","f7f3aba5":"code","0c17bd2c":"code","a5d6862a":"code","67452ffa":"code","318896b1":"code","7a1b7d87":"code","f65ebdd8":"code","ac31ee8b":"code","20e3d310":"code","83ad3077":"code","e337ee43":"code","fc405c9b":"code","81fb1559":"code","882b3276":"code","a07076e4":"code","0f422d87":"code","c04d73aa":"code","bda70435":"code","6012fa2f":"code","46ba6cdf":"code","77316751":"code","c6e210ee":"code","d8d2837f":"code","11b61532":"code","16d299c3":"code","c80025c2":"code","27884076":"code","1cdb481e":"code","4def850a":"code","bfa1263e":"code","67f881a8":"code","215dbbf4":"code","322f81ea":"code","46b3fb69":"code","08cc976e":"code","d851ab18":"code","c9c97cac":"code","d68d3d9f":"markdown","952784df":"markdown","ba509336":"markdown","67d061f0":"markdown"},"source":{"3cc4f691":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\n\nCredit = pd.read_csv(\"..\/input\/german-credit-data-with-risk\/german_credit_data.csv\")\ndf = Credit.copy()\ndf.drop([\"Unnamed: 0\"],axis=1,inplace=True)\ndf.head(3)","0d83daf6":"df.info()","00a990cc":"df.shape","33cb70e4":"df.corr()","90b4f331":"# Looking Corr table\n\nfrom matplotlib import pyplot as plt\n\ndf1 = df.copy()\ndf1 = df1.corr()\nplt.figure(figsize=(8.5,5.5))\ncorr = sns.heatmap(df1,xticklabels=df1.columns,yticklabels=df1.columns,annot=True)","de66789c":"df.describe().T","7e2fe1a6":"df.head(2)","fab75f9d":"from IPython.display import Image\nimport os\n!ls ..\/input\/\n\nImage(\"..\/input\/files-png\/Screenshot_1.png\")","9d60e852":"# EN\n# Creat a new variable beacuse scaling is good for forest methods\n\n\ndf.insert(1,\"Cat Age\",df[\"Age\"])\nfor i in df[\"Cat Age\"]:\n    if i<25:\n        df[\"Cat Age\"]=df[\"Cat Age\"].replace(i,\"0-25\")\n    elif (i>=25) and (i<40):\n        df[\"Cat Age\"]=df[\"Cat Age\"].replace(i,\"25-30\")\n    elif (i>=40) and (i<45):\n        df[\"Cat Age\"]=df[\"Cat Age\"].replace(i,\"30-35\")\n    elif (i>=45) and (i<40):\n        df[\"Cat Age\"]=df[\"Cat Age\"].replace(i,\"35-40\")\n    elif (i>=40) and (i<50):\n        df[\"Cat Age\"]=df[\"Cat Age\"].replace(i,\"40-50\")\n    elif (i>=50) and (i<76):\n        df[\"Cat Age\"]=df[\"Cat Age\"].replace(i,\"50-75\")","1110004a":"df.head(2)","97a40eb7":"Image(\"..\/input\/files-png\/Screenshot_2.png\")","5186cad2":"Image(\"..\/input\/files-png\/Screenshot_3.png\")","b9d24705":"from scipy import stats\n\ndf1 = df.copy()\n\ndf1 = df1[[\"Credit amount\",\"Purpose\"]]\n\ngroup = pd.unique(df1.Purpose.values)\n\nd_v1 = {grp:df1[\"Credit amount\"][df1.Purpose == grp] for grp in group}\n\nstats.levene(d_v1['radio\/TV'],d_v1['furniture\/equipment'],d_v1['car'],d_v1['business'],d_v1['domestic appliances'],d_v1['repairs'],\n                     d_v1['vacation\/others'],d_v1['education'])\n\n# EN\n# Is there a difference between the amount of credit the Purpose variable demands?\n\n\n# TR\n# Varyanslar homojen degil bu durumda KRUSKALL testi tercih edilebilir ama biz normal parametrik olani kullanacagiz","2cf36b92":"f, p = stats.f_oneway(d_v1['radio\/TV'],d_v1['furniture\/equipment'],d_v1['car'],d_v1['business'],d_v1['domestic appliances'],d_v1['repairs'],\n                     d_v1['vacation\/others'],d_v1['education'])\n\n(\"F istatistik: \"+str(f)+\" | P value degeri: \"+str(p))\n\n# EN\n# Apply Oneway anova\n\n# TR\n# Sonuca bakinca siniflar arasinda Cekilen kredi miktarinda anlamli bir farklilik gozukmekte","3a15eb4b":"(df.groupby(by=[\"Purpose\"])[[\"Credit amount\"]].agg(\"sum\") \/ df[\"Credit amount\"].sum())*100\n\n# EN\n# In the result, there is different between groups.\n# In this query we can see difference\n\n\n# TR\n# Bu esitsizliginde hangi gruplar tarafindan olusturuldugunu gorebilmekteyiz\n# Car & furniture\/equipment & radio\/TV en fazla kredi verilen siniflar olmus\n# Onceki islemlere donup baktigimiz zaman vacation\/others sinifi BAD degeri en yuksekti ve burada da verilen kredi dusuk gozukuyor","ceb23cdc":"table = pd.crosstab(df[\"Sex\"],df[\"Risk\"])\n\ntable\n\n# EN\n# if there are two categorical variable, use to Chi2 test.\n\n# TR\n# Bazi Iki kategorik grubu ve bu siniflardan birinde olma durumuna gore bad yada good olma arasinda bir iliski var mi karsilastiralim","414eea1f":"observed_values = table.values\nprint(\"Observe Values: -\\n\",observed_values)","7077bece":"from scipy.stats import chi2_contingency\n\nchi2, p, dof, ex = chi2_contingency(table)\nprint(\"Kikare degeri {} ve P value {}\".format(chi2,p))\n\n# EN\n# P value < 0.05 H0 rejected\n# There is a depend between Risk and Sex \n\n\n# TR\n# H0: Iki degisken arasinda bir iliski yoktur\n# H1: Iki degisken arasinda iliski vard\u0131r\n\n# Sonuca bakinca H0 red edilmistir. Yani iki degisken arasinda bir iliski vardir.\n# Kadin yada erkek olma durumuna gore Bad olma ve Good olma durumu degiskenlik gostermektedir","f7f3aba5":"# Bir diger merak ettigim durum ise Ev durumuna gore acaba risk gruplari arasinda anlamli bir farklilik var midir?\n\nhousing_risk = pd.crosstab(df[\"Housing\"],df[\"Risk\"])\n\nhousing_risk","0c17bd2c":"observed_values1 = housing_risk.values\nprint(\"Observe Values: -\\n\",observed_values1)","a5d6862a":"chi2, p, dof, ex = chi2_contingency(housing_risk)\nprint(\"Kikare degeri {} ve P value {}\".format(chi2,p))\n\n# EN\n# P value < 0.05 H0 rejected\n# There is a depend between Risk and Housing \n\n\n# H0: Iki degisken arasinda bir iliski yoktur\n# H1: Iki degisken arasinda iliski vard\u0131r\n\n# Gozle gorulebilir bir farklilik oldugu belliydi.\n# Lakin bu formel degildi ve test sonucu bunun formelligi de kesinlesmis bulunmakta.\n# Ev sahibi durumuna gore krediyi odeyememe(bad) veya krediyi odeme(good) durumu arasinda anlamli bir iliski vardir","67452ffa":"# EN \n# I am creating new variable\n# This variable is Monthly pay\n\n# TR\n# Simdi var olan data setinden bir degisken turetecegim\n# Bu degisken ise Cekilen kredi miktarinin odenecek aya bolumu\n# Yani kisinin aylik odemesi\n\ndf[\"monthly pay\"] = (df[\"Credit amount\"] \/ df[\"Duration\"]) ","318896b1":"df.head(3)","7a1b7d87":"df[df[\"Sex\"] == \"female\"][\"monthly pay\"].agg([\"min\",\"max\"])\n\n# EN\n# Min and Max difference is huge\n# If we will use the MEAN in this case do not use\n# instead we should use MEDIAN \n\n\n# TR\n# Aylik odeme miktarinda median kullanmak daha saglikli","f65ebdd8":"Image(\"..\/input\/another\/Screenshot_5.png\")","ac31ee8b":"# Coralation between variables\n\n# Simdi korelasyon degerlerine bakacagim \n\ndf.corr()","20e3d310":"# EN \n# I am creating new variable again but this variable job is only help raise prediction\n\n\n# TR\n# Ardindan toplam cekilen kredi miktarinin karesini olarak modele ekleyecegim\n# Bunu yapmamdaki sebep ise modelde ekstra bir degisken daha tutarak her hangi bir aciklayicilik artacak mi?\n# Ayni zamanda modele predict(tahmin) acisindan bir katkisi olacak mi gormek istiyorum\n\ndf[\"Credit amount**2\"] = df[\"Credit amount\"]**2\ndf.head(2)","83ad3077":"df.corr()","e337ee43":"df.head()","fc405c9b":"# EN\n# Scaling Duration \n\n\n# TR\n# Degisken donusumlerini hepsini gerceklestirip hangi durumda modeller nasil sonuclar veriyor onlari gormek icin son olarak,\n# Duration kismini da olceklendiriyorum sebebi ise agaclandirma yapilarinda kategorik olanlar daha iyi sonuc verebilmekte\n# Ayni zamanda degisken Vadeyi belirtiyor buda ayni zaman da kesikli oldugunu ve kategorik bir yapi sergiledigini hissetiriyor\n\ndf.insert(9,\"Cat Duration\",df[\"Duration\"])\nfor i in df[\"Cat Duration\"]:\n    if i<12:\n        df[\"Cat Duration\"]=df[\"Cat Duration\"].replace(i,\"0-12\")\n    elif (i>=12) and (i<24):\n        df[\"Cat Duration\"]=df[\"Cat Duration\"].replace(i,\"12-24\")\n    elif (i>=24) and (i<36):\n        df[\"Cat Duration\"]=df[\"Cat Duration\"].replace(i,\"24-36\")\n    elif (i>=36) and (i<48):\n        df[\"Cat Duration\"]=df[\"Cat Duration\"].replace(i,\"36-48\")\n    elif (i>=48) and (i<60):\n        df[\"Cat Duration\"]=df[\"Cat Duration\"].replace(i,\"48-60\")\n    elif (i>=60) and (i<=72):\n        df[\"Cat Duration\"]=df[\"Cat Duration\"].replace(i,\"60-72\")","81fb1559":"df.head(2)","882b3276":"# EN\n# I scaled Age and Duration now drop that variables it.\n\n\n# TR\n# Age(Yas) degiskenini ve Duration degiskenini cikariyorum veri setinden \n# Ayni zamanda kategorik olanlari Dummy hale getirecegim\n\ndf.drop(labels=[\"Age\",\"Duration\"],inplace=True,axis=1)\ndf.head(2)","a07076e4":"df.nunique()","0f422d87":"# EN\n# Preparation\n\n# TR\n# 2 den fazla olanlara Get dummy yaptim, cunku diger turlu bunlarin arasinda bir siralama olacagini dusunerek sayisi yuksek olana agirlik verecekti\n\ndf = pd.get_dummies(df,columns= [\"Cat Age\"], prefix= [\"Cat Age\"]) \ndf = pd.get_dummies(df,columns= [\"Job\"], prefix= [\"Job\"])\ndf = pd.get_dummies(df,columns= [\"Housing\"], prefix= [\"Housing\"])\ndf = pd.get_dummies(df,columns= [\"Saving accounts\"], prefix= [\"Saving accounts\"])\ndf = pd.get_dummies(df,columns= [\"Cat Duration\"], prefix= [\"Cat Duration\"])\ndf = pd.get_dummies(df,columns= [\"Checking account\"], prefix= [\"Checking account\"])\ndf = pd.get_dummies(df,columns= [\"Purpose\"], prefix= [\"Purpose\"])\ndf = pd.get_dummies(df,columns= [\"Sex\"], prefix= [\"Sex\"])","c04d73aa":"# Geriye kalan Sex ve Risk degiskenine LabelEncoder yapacagim 2 sinif olmasindan sebep\n\nfrom sklearn.preprocessing import LabelEncoder\n\nlbe = LabelEncoder()\n\n\ndf[\"Risk\"] = lbe.fit_transform(df[\"Risk\"])\n\ndf.head(2)","bda70435":"# Already we have Sex column, drop other\n\ndf.drop(labels=[\"Sex_female\"],axis=1,inplace=True)\n","6012fa2f":"\n\n\n\n# Simdi DEGISKENLERDEKI NaN degerleri dusurmustuk. Bu sekilde model kuracagim ve daha sonra impute islemi gerceklestirip sonuclari degerlendirecegim.\n\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.preprocessing import StandardScaler  \nfrom sklearn.neural_network import MLPClassifier\n","46ba6cdf":"from sklearn.model_selection import train_test_split\n\ny = df[\"Risk\"]\nX = df.drop(['Risk'], axis=1)\nX_train, X_test, y_train, y_test = train_test_split(X, y, \n                                                   test_size=0.30, \n                                                   random_state=42)","77316751":"int_float_train = list(X_train.select_dtypes(include=[\"float64\",\"int64\"]))\nint_float_train","c6e210ee":"int_float_test = list(X_test.select_dtypes(include=[\"float64\",\"int64\"]))\nint_float_test","d8d2837f":"scaler = StandardScaler()\n\nX_train_scaled = X_train.copy()\nX_test_scaled = X_test.copy()\n\nX_train_scaled.loc[:,int_float_train] = scaler.fit_transform(X_train_scaled.loc[:,int_float_train])\nX_test_scaled.loc[:,int_float_test] = scaler.fit_transform(X_test_scaled.loc[:,int_float_test])","11b61532":"X_train_scaled.head()","16d299c3":"tree_model=DecisionTreeClassifier().fit(X_train,y_train)\nrandomforest_model=RandomForestClassifier().fit(X_train,y_train)\nysa_model= MLPClassifier().fit(X_train_scaled, y_train)","c80025c2":"from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n\nprint(\"Karar agacinin Train seti Accuracy Scoru : \",accuracy_score(y_train,tree_model.predict(X_train)))\nprint(\"Karar agacinin Test seti Accuracy Scoru : \",accuracy_score(y_test,tree_model.predict(X_test)))\nprint(\"Test setine ait Recall, F1 Score gibi detaylar: \\n\",classification_report(y_test,tree_model.predict(X_test)))","27884076":"print(\"YSA  Train seti Accuracy Scoru : \",accuracy_score(y_train,ysa_model.predict(X_train_scaled)))\nprint(\"YSA Test seti Accuracy Scoru : \",accuracy_score(y_test,ysa_model.predict(X_test_scaled)))\nprint(\"Test setine ait Recall, F1 Score gibi detaylar: \\n\",classification_report(y_test,ysa_model.predict(X_test_scaled)))","1cdb481e":"print(\"Random Forest Train seti Accuracy Scoru : \",accuracy_score(y_train,randomforest_model.predict(X_train)))\nprint(\"Random Forest Test seti Accuracy Scoru : \",accuracy_score(y_test,randomforest_model.predict(X_test)))\nprint(\"Test setine ait Recall, F1 Score gibi detaylar: \\n\",classification_report(y_test,randomforest_model.predict(X_test)))","4def850a":"# EN\n# We can see Recall & F1 Score of predict 0 is low\n# the reason is Unbalanced data\n\n\n# TR\n# Dikkati ceken durum 0 tahmin durumunun dusuk olmasidir\n# Bunun sebebi ise dengesiz(unbalanced) bir veri seti olmasidir\n\ndf[\"Risk\"].value_counts()","bfa1263e":"\n\nfrom sklearn.model_selection import train_test_split, cross_val_score,GridSearchCV\nfrom sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n\ndef YsaClassifier(X_train,y_train,X_test,y_test):\n        scaler = StandardScaler()\n\n        X_train_scale = scaler.fit_transform(X_train)\n\n        X_test_scale =  scaler.fit_transform(X_test)\n  \n  \n        mlp_regres = MLPClassifier().fit(X_train_scale,y_train) \n        y_pred = mlp_regres.predict(X_test_scale)\n        Accuaracy = accuracy_score(y_test,y_pred)\n        matrix = classification_report(y_test,y_pred)\n  \n\n        params = {\"alpha\":[0.1,0.01,0.02,0.005], # alpha icin aranacak degerler\n              \"hidden_layer_sizes\":[(20,20),(100,50,150),(300,200,150)], # gizli katmanin dereceleri ve sayilari icin aranacak parametreler\n              \"activation\":[\"relu\",\"logistig\"],\n              'solver': ['adam', 'lbfgs']}# Son olarak birde iki tane fonksiyon var onlari denesin denedik \n\n        mlp_c = MLPClassifier()\n  \n        mlp_c = GridSearchCV(mlp_c,params,\n                       cv = 10,\n                       n_jobs = -1,\n                       verbose = 2)\n\n        mlp_c_tune = mlp_c.fit(X_train_scale,y_train)\n\n        bos = []\n        for i in mlp_c_tune.best_params_:\n            bos.append(mlp_c_tune.best_params_[i])\n\n        mlp_tuned = MLPClassifier(activation=bos[0],\n                         alpha=bos[1],hidden_layer_sizes=bos[2],\n                         solver=bos[3]).fit(X_train_scale,y_train)\n\n        y_pred1 = mlp_tuned.predict(X_test_scale)\n\n        Accuaracy1 = accuracy_score(y_test,y_pred1)\n        matrix1 = classification_report(y_test,y_pred1)\n\n\n        print(\"Tune Edilmemis Tahmin sonuclari Accuracy degeri: \",Accuaracy)\n        print(\"-------------------------------\")\n        print(\"Tune Edilmemis Recall,F1 etc  sonuc: \\n\",matrix)\n        print(\"****************************************************\")\n        print(\"Tune sonrasi Tahmin sonuclari Accuracy degeri:\",Accuaracy1)\n        print(\"-------------------------------\")\n        print(\"Tune sonrasi Recall,F1 etc sonuc: \\n\",matrix1)\n\n\nYsaClassifier(X_train,y_train,X_test,y_test)","67f881a8":"def DecisionClassifier(X_train,y_train,X_test,y_test):\n        cart = DecisionTreeClassifier()\n        cart_model = cart.fit(X_train, y_train)\n        y_pred = cart_model.predict(X_test)\n        Accuaracy = accuracy_score(y_test, y_pred)\n        matrix = classification_report(y_test,y_pred)\n\n        cart_grid = {\"max_depth\": range(1,10),\n            \"min_samples_split\" : list(range(2,50)),\n             \"criterion\":[\"gini\",\"entropy\"],\n             \"min_samples_leaf\": range(50,100)}\n\n        cart = DecisionTreeClassifier()\n        cart_cv = GridSearchCV(cart, cart_grid, cv = 10, n_jobs = -1, verbose = 2)\n        cart_cv_model = cart_cv.fit(X_train, y_train)\n\n        bos = []\n\n        for i in cart_cv_model.best_params_:\n              bos.append(cart_cv_model.best_params_[i])\n\n        cart = DecisionTreeClassifier(criterion = bos[0],max_depth = bos[1],min_samples_leaf=bos[2],min_samples_split = bos[3])\n        cart_tuned = cart.fit(X_train, y_train)\n\n        y_pred = cart_tuned.predict(X_test)\n        Accuaracy1 = accuracy_score(y_test, y_pred)\n        matrix1 = classification_report(y_test,y_pred)\n\n        print(\"En iyi parametre degerleri: \",cart_cv_model.best_params_)\n        print(\"Tune edilmemis modelin Accuaracy ve Detaylar: \")\n        print(Accuaracy)\n        print(matrix)\n        print(\"**********************************************\")\n        print(\"Tune sonrasi Accuaracy ve Detaylar: \")\n        print(Accuaracy1)\n        print(matrix1) \n\n\nDecisionClassifier(X_train,y_train,X_test,y_test)","215dbbf4":"def RandomForestsClass(X_train,y_train,X_test,y_test):\n        rf_model = RandomForestClassifier().fit(X_train, y_train)\n        y_pred = rf_model.predict(X_test)\n        Accuracy = accuracy_score(y_test, y_pred)  \n        Matrix = classification_report(y_test,y_pred)\n\n        params = {\"max_depth\": [2,5,8,10],\n            \"max_features\": [2,5,8],\n            \"n_estimators\": [10,500,1000],\n            \"min_samples_split\": [7,15,20]}\n\n\n        rf_model = RandomForestClassifier()\n\n        rf_cv_model = GridSearchCV(rf_model,\n                           params,\n                           cv = 10,\n                           n_jobs = -1,\n                           verbose = 2)\n\n        rf_cv_model.fit(X_train,y_train)  \n\n        bos = []\n\n        for i in rf_cv_model.best_params_:\n\n              bos.append(rf_cv_model.best_params_[i])\n\n        final_tune = RandomForestClassifier(max_depth=bos[0],max_features=bos[1],min_samples_split=bos[2],n_estimators=bos[3])\n\n        final_tune = final_tune.fit(X_train,y_train)\n\n        y_pred = final_tune.predict(X_test)\n\n        Accuracy1 = accuracy_score(y_test,y_pred)\n        Matrix1 = classification_report(y_test,y_pred)\n\n\n        Importance = pd.DataFrame({\"Importance\": final_tune.feature_importances_*100},\n                         index = X_train.columns)\n        importance = Importance.sort_values(by = \"Importance\", \n                       axis = 0, \n                       ascending = True).plot(kind =\"barh\", color = \"yellow\"),plt.xlabel(\"De\u011fi\u015fken \u00d6nem D\u00fczeyleri\")\n\n        print(\"En iyi parametre degerleri: \",rf_cv_model.best_params_)\n        print(\"                                               \")\n        print(\"Tune oncesi Accuracy ve Recall,F1 degerleri: \",Accuracy,\"\\n\",Matrix)\n        print(\"Tune sonrasi Accuracy ve Recall,F1  degerleri: \",Accuracy1,\"\\n\",Matrix1)\n        print(\"                                                \")\n        print(importance)\n\n\nRandomForestsClass(X_train,y_train,X_test,y_test)","322f81ea":"X_train.columns\n\n# EN\n# After at the Random forest, the best importance factors come to light\n# Their names are\n# 1-'monthly pay'\n# 2-'Credit amount**2'\n# 3-'Credit amount'\n# 4-'Checking account_little'\n# 5-'Cat Duration_48-60'\n# 6-'Checking account_moderate'\n\n\n# TR\n# Degisken onem duzeyine baktigimiz da cikan sonucta oncelikli olarak;\n# 1-'monthly pay'\n# 2-'Credit amount**2'\n# 3-'Credit amount'\n# 4-'Checking account_little'\n# 5-'Cat Duration_48-60'\n# 6-'Checking account_moderate'\n\n# durumlarina gore tahmin sonuclarinda en etkili olanlar olarak gorulmekte\n# Burada 2 adet degiskeni kendim olusturmustum. \"Monthly pay\" ve \"Credit amount**2\" ikincisini tamamiyle tahmin sonucunu artirmak adina kullandim.\n# Simdi bu degiskenlerden basit bir karar agaci gorseli gorelim","46b3fb69":"from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n\ndegisken = [\"monthly pay\",\"Credit amount\",\"Checking account_little\",\"Cat Duration_48-60\",\"Checking account_moderate\"]\n\ndata = df.loc[:,degisken]\n\ndata.head(2)\n\n\n","08cc976e":"X = data\ny = df[\"Risk\"]\n\nforest = RandomForestClassifier(max_depth = 3, n_estimators=5)\nforest.fit(X,y)\n\nestimator = forest.estimators_[4]\ntarget_names = [\"Result 0\",\"Result 1\"]\n\n\nfrom sklearn.tree import export_graphviz\n\nexport_graphviz(estimator,out_file=\"tree_limited.dot\",feature_names=X.columns,\n                class_names=target_names,rounded = True, proportion = False, precision = 2, filled = True)\n\n","d851ab18":"forest_1 = RandomForestClassifier(max_depth = None, n_estimators=5)\nforest_1 = forest_1.fit(X,y)\nestimator_non = forest_1.estimators_[4]\n\nexport_graphviz(estimator_non, out_file='tree_nonlimited.dot', feature_names = X.columns,\n                class_names = target_names,\n                rounded = True, proportion = False, precision = 2, filled = True)","c9c97cac":"!dot -Tpng tree_limited.dot -o tree_limited.png -Gdpi=600\n\nfrom IPython.display import Image\nImage(filename = 'tree_limited.png')","d68d3d9f":"# Now all models will be Tuned","952784df":"# Creating decision tree visual with 5 important variables","ba509336":"# EN\n# Preparation","67d061f0":"# Before bulding the model, I am scaling X_train X_test beacuse Neural Network It works that way"}}