{"cell_type":{"f86535f2":"code","0778b993":"code","9f80f35c":"code","505dab47":"code","e9ccbae3":"code","9fadf20d":"code","8b337861":"code","b3757ee4":"code","967923f0":"code","d9b7cd09":"code","7fb199f3":"code","91df69db":"code","b297c88b":"code","520d8a4a":"code","1dc8d19c":"code","9aa2fe0a":"code","26453203":"code","747eb1de":"code","6e8c2381":"code","c10cc56f":"code","8c423e44":"code","642a0820":"code","12ea23ec":"code","0d9a5da5":"code","0ea42919":"code","ba2677eb":"code","44a76bd1":"code","43241556":"code","638aac05":"code","c6b81569":"code","699ad6e5":"code","e780e795":"code","5924eed3":"code","7114fe1a":"code","44b5946f":"code","b8ff186b":"code","8889c8a6":"code","87abce47":"code","09c7356a":"code","ebf78c7d":"code","6acaaa31":"code","9572a90f":"code","d268a9e0":"code","59b62916":"code","91a929b7":"code","3ed8e51c":"code","b608e64c":"code","5f287e21":"code","0504e399":"code","e92bcbf3":"code","156d8334":"code","801133ae":"code","1f30b4ff":"code","88eaf133":"code","3d9ec7d4":"code","a829556b":"code","fdc9b1bc":"markdown","d5b3db98":"markdown","59715a0a":"markdown","76882751":"markdown","d39b9757":"markdown","68fc631a":"markdown","20639dc7":"markdown","02516a52":"markdown","4fc9e9bb":"markdown","d434438d":"markdown","1324239c":"markdown","0ca2f8ae":"markdown","d8ca7bc2":"markdown","b2e78bd5":"markdown","1060b04f":"markdown","624bb3c3":"markdown","f8a0dd95":"markdown","25b1af72":"markdown","cd9910b8":"markdown","e87aad58":"markdown","a931fd78":"markdown","f30ef2cf":"markdown","8b2c1983":"markdown","b52a1e5b":"markdown","1d9473c8":"markdown"},"source":{"f86535f2":"# Import libraries necessary for this project\nimport numpy as np\nimport pandas as pd\nfrom matplotlib import pyplot\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nfrom IPython.display import display # Allows the use of display() for DataFrames\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics.scorer import make_scorer, accuracy_score, recall_score, roc_auc_score, r2_score\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.svm import OneClassSVM\nfrom sklearn.ensemble import RandomForestClassifier\n\n\nimport xgboost as xgb\nfrom xgboost import plot_tree\nfrom xgboost import plot_importance\n\n# Pretty display for notebooks\n%matplotlib inline\n\nrandom_seed = 42","0778b993":"# Load the trianing\/Test dataset\ntry:\n    train = pd.read_csv('..\/input\/aps_failure_training_set_processed_8bit.csv')\n    test = pd.read_csv('..\/input\/aps_failure_test_set_processed_8bit.csv')\nexcept:\n    print(\"Dataset could not be loaded. Is the dataset missing?\")","9f80f35c":"# Display a description of the dataset\ndisplay(train.shape)\ndisplay(test.shape)","505dab47":"train.head()","e9ccbae3":"train['class'] = train['class'].apply(lambda x: 0 if x<=0 else 1)\ntest['class'] = test['class'].apply(lambda x: 0 if x<=0 else 1)","9fadf20d":"fig, ax = plt.subplots(1, 1, figsize=(20, 10))\ntrain.boxplot(ax=ax)\nplt.show()","8b337861":"train.describe()","b3757ee4":"X = train.drop('class', axis=1)\ny = train['class']\n\n# use the given test set, instead of creating from the training samples\nX_test_given = test.drop('class', axis=1)\ny_test_given = test['class']","967923f0":"y.value_counts()","d9b7cd09":"1000\/60000","7fb199f3":"y_test_given.value_counts()","91df69db":"fig, ax = plt.subplots(1, 1, figsize=(30, 30))\nsns.heatmap(X.corr(), vmax=1, vmin=-1, center=0, annot=True, ax=ax)","b297c88b":"scaler = MinMaxScaler().fit(X)\nX_scaled = scaler.transform(X)","520d8a4a":"fig, ax = plt.subplots(1, 1, figsize=(20, 10))\npd.DataFrame(X_scaled).boxplot(ax=ax)\nplt.show()","1dc8d19c":"pca = PCA(0.98)\npca.fit(X)\npca.n_components_\nX_reduced_data = pca.transform(X)\nX_reduced_data.shape","9aa2fe0a":"plt.xlabel(\"Number of components\")\nplt.ylabel(\"Cumulated Sum of Ration of variance explained\")\nplt.xticks(range(0,87))\nplt.grid(True)\nplt.plot(np.cumsum(pca.explained_variance_ratio_))\nplt.show()","26453203":"plt.xlabel(\"Number of components\")\nplt.ylabel(\"Ration of variance explained\")\nplt.xticks(range(0,87))\nplt.grid(True)\nplt.plot(pca.explained_variance_ratio_)\nplt.show()","747eb1de":"fig, ax = plt.subplots(1, 1, figsize=(30, 30))\nsns.heatmap(pd.DataFrame(X_reduced_data).corr(), vmax=1, vmin=-1, center=0, annot=True, ax=ax)","6e8c2381":"fig, ax = plt.subplots(1, 1, figsize=(20, 10))\npd.DataFrame(X_reduced_data).boxplot(ax=ax)\nplt.show()","c10cc56f":"%time\n# fit the model\nocsvm = OneClassSVM(nu=0.5, kernel='rbf', gamma='auto')\nocsvm.fit(X_reduced_data)\n\ny_pred = ocsvm.predict(X_reduced_data)","8c423e44":"OUTLIER_DATA = -1\npredicted_normal_index = np.where(y_pred != OUTLIER_DATA)\nX_normal = X_reduced_data[predicted_normal_index]\nfig, ax = plt.subplots(1, 1, figsize=(20, 10))\npd.DataFrame(X_normal).boxplot(ax=ax)\nplt.show()","642a0820":"predicted_normal_index = np.where(y_pred != OUTLIER_DATA)\ny_normal = y[predicted_normal_index[0]]\ny_normal.value_counts()","12ea23ec":"OUTLIER_DATA = -1\npredicted_anormaly_index = np.where(y_pred == OUTLIER_DATA)\nX_anomaly = X_reduced_data[predicted_anormaly_index]\nfig, ax = plt.subplots(1, 1, figsize=(20, 10))\npd.DataFrame(X_anomaly).boxplot(ax=ax)\nplt.show()","0d9a5da5":"predicted_anormaly_index = np.where(y_pred == OUTLIER_DATA)\ny_anomaly  = y[predicted_anormaly_index[0]]\ny_anomaly.value_counts()","0ea42919":"X_reduced_test_data = pca.transform(X_test_given)\n\nMIN = min(np.min(X_reduced_data), np.min(X_reduced_test_data)) - 0.1\nMIN","ba2677eb":"X_reduced_test_data_log = np.log(X_reduced_test_data - MIN)\n\nfig, ax = plt.subplots(1, 1, figsize=(20, 10))\npd.DataFrame(X_reduced_test_data_log).boxplot(ax=ax)\nplt.show()","44a76bd1":"X_train = X\nX_test = X_test_given\n\ny_train = y\ny_test = y_test_given","43241556":"rfPredictor = RandomForestClassifier(n_estimators=100, max_depth=5,random_state=42).fit(X,y)\npredictRF = rfPredictor.predict(X_test_given)\nscore = rfPredictor.predict_proba(X)\nprint('Best ROC-AUC: {:.4f}'.format(roc_auc_score(y, score[:, 1], average='macro')))\nprint(\"accuracy score : {}\".format(accuracy_score( y_test_given, predictRF)))\nprint(\"R-squared, coefficient of determination : {:.3f}\".format(r2_score(y_test, predictRF)))\nprint(classification_report( y_true = y_test_given, y_pred = predictRF))\nprint(confusion_matrix(y_true = y_test_given, y_pred = predictRF))","638aac05":"%%time\npredictor = xgb.XGBClassifier(seed=42)\n\npredictor.fit(X_train, y_train)","c6b81569":"score = predictor.predict_proba(X_train)\nprint('Best ROC-AUC: {:.4f}'.format(roc_auc_score(y_train, score[:, 1], average='macro')))\npredict = predictor.predict(X_test)\nprint(\"accuracy score : {}\".format(accuracy_score( y_test_given, predict)))\nprint(\"R-squared, coefficient of determination : {:.3f}\".format(r2_score(y_test, predict)))\nprint(classification_report( y_true = y_test, y_pred = predict ))\nconfusion_matrix(y_true = y_test, y_pred = predict )","699ad6e5":"%%time\npredictor = xgb.XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n       colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_delta_step=5,\n       max_depth=5, min_child_weight=1, missing=None, n_estimators=500,\n       n_jobs=1, nthread=4, objective='binary:logistic', random_state=0,\n       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=42, silent=True,\n       subsample=0.5, tree_method='gpu_exact', verbose=10)\n\npredictor.fit(X_train, y_train)","e780e795":"score = predictor.predict_proba(X_train)\nprint('Best ROC-AUC: {:.4f}'.format(roc_auc_score(y_train, score[:, 1], average='macro')))\npredict = predictor.predict(X_test)\nprint(\"accuracy score : {}\".format(accuracy_score( y_test_given, predict)))\nprint(\"R-squared, coefficient of determination : {:.3f}\".format(r2_score(y_test, predict)))\nprint(classification_report( y_true = y_test, y_pred = predict ))\nconfusion_matrix(y_true = y_test, y_pred = predict )","5924eed3":"fig, ax = plt.subplots(1, 1, figsize=(7, 25))\nplot_importance(predictor, max_num_features = pca.n_components_, ax=ax)\nplt.show()","7114fe1a":"plot_tree(predictor, rankdir='LR')\nfig = plt.gcf()\nfig.set_size_inches(150, 100)\nplt.show()","44b5946f":"X_train = X_reduced_data\nX_test = X_reduced_test_data\n\ny_train = y\ny_test = y_test_given","b8ff186b":"%%time\npredictor = xgb.XGBClassifier(seed=42)\n\npredictor.fit(X_train, y_train)","8889c8a6":"score = predictor.predict_proba(X_train)\nprint('Best ROC-AUC: {:.4f}'.format(roc_auc_score(y_train, score[:, 1], average='macro')))\npredict = predictor.predict(X_test)\nprint(\"accuracy score : {}\".format(accuracy_score( y_test_given, predict)))\nprint(\"R-squared, coefficient of determination : {:.3f}\".format(r2_score(y_test, predict)))\nprint(classification_report( y_true = y_test, y_pred = predict ))\nconfusion_matrix(y_true = y_test, y_pred = predict )","87abce47":"%%time\npredictor = xgb.XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n       colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_delta_step=5,\n       max_depth=5, min_child_weight=1, missing=None, n_estimators=500,\n       n_jobs=1, nthread=4, objective='binary:logistic', random_state=0,\n       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=42, silent=True,\n       subsample=0.5, tree_method='gpu_exact', verbose=10)\n\npredictor.fit(X_train, y_train)","09c7356a":"score = predictor.predict_proba(X_train)\nprint('Best ROC-AUC: {:.4f}'.format(roc_auc_score(y_train, score[:, 1], average='macro')))\npredict = predictor.predict(X_test)\nprint(\"accuracy score : {}\".format(accuracy_score( y_test_given, predict)))\nprint(\"R-squared, coefficient of determination : {:.3f}\".format(r2_score(y_test, predict)))\nprint(classification_report( y_true = y_test, y_pred = predict ))\nconfusion_matrix(y_true = y_test, y_pred = predict )","ebf78c7d":"fig, ax = plt.subplots(1, 1, figsize=(7, 25))\nplot_importance(predictor, max_num_features = pca.n_components_, ax=ax)\nplt.show()","6acaaa31":"plot_tree(predictor, rankdir='LR')\nfig = plt.gcf()\nfig.set_size_inches(150, 100)\nplt.show()","9572a90f":"MIN = min(np.min(X_reduced_data), np.min(X_reduced_test_data)) - 0.1\n\nX_train = np.log(X_reduced_data - MIN)\nX_test = np.log(X_reduced_test_data - MIN)\n\ny_train = y\ny_test = y_test_given","d268a9e0":"%%time\npredictor = xgb.XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n       colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_delta_step=5,\n       max_depth=5, min_child_weight=1, missing=None, n_estimators=500,\n       n_jobs=1, nthread=4, objective='binary:logistic', random_state=0,\n       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=42, silent=True,\n       subsample=0.5, tree_method='gpu_exact', verbose=10)\n\npredictor.fit(X_train, y_train)","59b62916":"score = predictor.predict_proba(X_train)\nprint('Best ROC-AUC: {:.4f}'.format(roc_auc_score(y_train, score[:, 1], average='macro')))\npredict = predictor.predict(X_test)\nprint(\"accuracy score : {}\".format(accuracy_score( y_test_given, predict)))\nprint(\"R-squared, coefficient of determination : {:.3f}\".format(r2_score(y_test, predict)))\nprint(classification_report( y_true = y_test, y_pred = predict ))\nconfusion_matrix(y_true = y_test, y_pred = predict )","91a929b7":"fig, ax = plt.subplots(1, 1, figsize=(7, 25))\nplot_importance(predictor, max_num_features = pca.n_components_, ax=ax)\nplt.show()","3ed8e51c":"plot_tree(predictor, rankdir='LR')\nfig = plt.gcf()\nfig.set_size_inches(150, 100)\nplt.show()","b608e64c":"df_eval = pd.DataFrame([{\"model\":\"RandomForest\", \"ROC-AUC\": 0.9885, \"accuracy score\" : 0.9858125, \"R-squared\" : 0.380},\n{\"model\":\"XGBoost\", \"ROC-AUC\": 0.9944, \"accuracy score\" : 0.98975, \"R-squared\" : 0.552},\n{\"model\":\"XGBoost  PCA\", \"ROC-AUC\": 1.0000, \"accuracy score\" : 0.9889375, \"R-squared\" : 0.517},\n{\"model\":\"XGBoost  Log Transform\", \"ROC-AUC\": 1.0000, \"accuracy score\" : 0.9884375, \"R-squared\" : 0.495}]).set_index('model', drop=True)\ndf_eval.plot(kind=\"bar\")","5f287e21":"X_train = X\nX_test = X_test_given\n\ny_train = y\ny_test = y_test_given","0504e399":"%%time\n\nparams = {\n'max_depth':[5,6,7],\n'learning_rate':[0.1],\n'gamma':[0.0],\n'min_child_weight':[1],\n'max_delta_step':[5],\n'colsample_bytree':[0.8],\n'n_estimators':[300, 500, 700],\n'subsample':[0.5],\n'objective':['binary:logistic'],\n'nthread':[4],\n'scale_pos_weight':[1],\n'seed':[random_seed],\n'verbose': [10],\n'tree_method':['gpu_exact']}\n\n\nmodel = xgb.XGBClassifier(tree_method='hist')\n#cv = GridSearchCV(model, params, cv=5, n_jobs=4, scoring='roc_auc')\ncv = GridSearchCV(model, params, cv=5, n_jobs=4, scoring='recall')\n\ncv.fit(X_train, y_train)\nprint(cv.best_estimator_)","e92bcbf3":"predictor = cv.best_estimator_\npredictor.save_model('.\/model\/xgb.model')","156d8334":"score = predictor.predict_proba(X_train)\nprint('Best ROC-AUC: {:.4f}'.format(roc_auc_score(y_train, score[:, 1], average='macro')))\npredict = predictor.predict(X_test)\nprint(\"accuracy score : {}\".format(accuracy_score( y_test, predict)))\nprint(\"R-squared, coefficient of determination : {:.3f}\".format(r2_score(y_test, predict)))\nprint(classification_report( y_true = y_test, y_pred = predict ))\nconfusion_matrix(y_true = y_test, y_pred = predict )","801133ae":"print(\"Best parameters: %s\" % cv.best_params_)\nprint(\"Best auroc score: %s\" % cv.best_score_)","1f30b4ff":"fig, ax = plt.subplots(1, 1, figsize=(7, 25))\nplot_importance(predictor, max_num_features = pca.n_components_, ax=ax)\nplt.show()","88eaf133":"plot_tree(predictor, rankdir='LR')\nfig = plt.gcf()\nfig.set_size_inches(300, 200)\nplt.show()","3d9ec7d4":"df_eval = pd.DataFrame([{\"model\":\"Tuned XGBoost\", \"ROC-AUC\": 1.0000, \"accuracy score\" : 0.99275, \"R-squared\" : 0.683},\n                        {\"model\":\"XGBoost(Default)\", \"ROC-AUC\": 0.9944, \"accuracy score\" : 0.98975, \"R-squared\" : 0.552}]).set_index('model', drop=True)\ndf_eval.plot(kind=\"bar\")","a829556b":"df_eval = pd.DataFrame([{\"model\":\"Tuned XGBoost\", \"ROC-AUC\": 1.0000, \"accuracy score\" : 0.99275, \"R-squared\" : 0.683},\n                        {\"model\":\"XGBoost(Default)\", \"ROC-AUC\": 0.9944, \"accuracy score\" : 0.98975, \"R-squared\" : 0.552},\n                        {\"model\":\"RandomForest\", \"ROC-AUC\": 0.9885, \"accuracy score\" : 0.9858125, \"R-squared\" : 0.380}]).set_index('model', drop=True)\ndf_eval.plot(kind=\"bar\")","fdc9b1bc":"Log transform","d5b3db98":"Looking at the actual proportion of class 1 showing abnormality of Air Pressure System, there are 60,000 training data in total, 1,000 of which are target positive class (17%).\nThis suggests that majority of the 171 sensors may indicate the same value as the steady state even in abnormal cases even when APS abnormality occurs.\nIt is unrealistic to perform a combination for discriminating abnormal conditions from among human's characteristics, and it is necessary to propose a method to automatically perform through a scientific approach.\n\n| Class | Training Dataset | Test Dataset |\n|-|-|-|\n|Negative Class | 59,000 | 15,625 |\n|Positive Class | 1,000 | 375 |\n|Percentage of Positive Class | 1.7% | 2.3% |\n\nIn considering the preprocessing, next, we investigate the correlation between the feature quantities of the data adopted this time.\n\nThe correlation coefficient between sensor data is shown as a heat map.\nThis heat map is displayed with red closer to white as the positive correlation is stronger, and blue closer to white as negatively strong.\nRegarding the relationship between the strength of the correlation and the color of the heat map, the scale on the right bar of the figure is shown.\nCombinations of feature quantities which are close to white in the heat map are totally seen.\nFrom this, it can be seen that there are many combinations of feature quantities with strong positive correlation.\n\nAs for the target data, since the name of the feature quantity is anonymized, it is unknown what the measurement object and method of the specific sensor are.\nTherefore, the measurement target between sensors is unknown here.\nIn general, there are cases where a plurality of sensors are installed for the same device to verify the difference in sensor information depending on the mounting position and the like.\nInferring from such a fact, it is conceivable that data of events occurring in the same equipment will be acquired and strong correlation will be seen.\n\nBy integrating feature values \u200b\u200bwith strong correlation and newly setting features with high independence, we will reduce the calculation cost when building the model and improve the prediction accuracy.","59715a0a":"The 75th percentile data is concentrated in the center.\nOn the other hand, it can be seen that the remaining data spread widely.\nConsider excluding data far from the center of the cluster as Outlier under the assumption that it is data after PCA application and is clustered\nIn this case, Outlier was judged by One Class SVM [3].\n\nIn converting to PCA, the feature amount was chosen with a contribution ratio of 98%. As a result, the feature quantity of 171 is reduced to 67. It can be seen that the feature quantities after reduction are uncorrelated and the cross correlation coefficient is almost 0.\n\nFor the feature amount after PCA conversion, drawing a box diagram shows that many data are aggregated around 0 and that the skirt of data not included in the 75 th percentile is long.\nFor a given training data, there are many uniform data, and it can be seen that the data deviating therefrom has diversity.","76882751":"\n| Parameter | Description | Search Rangge |\n|-|-|-|\n| max_depth (int) | Maximum tree depth for base learners. | [5,6,7] |\n| learning_rate (float) | Boosting learning rate (xgb\u2019s \u201ceta\u201d) | 0.1(Fixed) |\n| n_estimators (int) | Number of boosted trees to fit.| [300, 500, 700] |\n| objective (string or callable) | Specify the learning task and the corresponding learning objective or a custom objective function to be used. | ['binary:logistic'] |\n| booster (string) | Specify which booster to use | gbtree |\n| gamma (float) | Minimum loss reduction required to make a further partition on a leaf node of the tree. | [0.0] (Fixed) |\n|min_child_weight (int) |  Minimum sum of instance weight(hessian) needed in a child. | [1]|\n|max_delta_step (int) | Maximum delta step we allow each tree\u2019s weight estimation to be.| [5] |\n| subsample (float) | Subsample ratio of the training instance. | [0.5] (Fixed) |\n| colsample_bytree (float) | Subsample ratio of columns when constructing each tree. | [0.8] |","d39b9757":"### Provided Data\n\nIn this project, *the dataset consists of data collected from heavy Scania trucks in everyday usage. The system in focus is the Air Pressure system (APS) which generates pressurized air that is utilized in various functions in a truck, such as braking and gear changes.\n\nThe Air Pressure System is used to reduce load shocks and is an important system in logistics trucks.\nTraining data for model construction and test data for verification are given in advance in the Kaggle dataset.\nUsing the training data, we derive the prediction model of the occurrence of the failure (label) from the sensor data (features).\nThis prediction logic is applied to the test data and the ability of the prediction logic is evaluated.\nThe sensor data is anonymized for confidentiality reasons, and so we have to make the prediction model purely on mathematical and statistical approach.\n\nBelow, combinations of data sets and files given as training data and test data are shown.\n\n| dataset| file |\n|-|-|\n| Training Data to build a model | ..\/input\/aps_failure_training_set_processed_8bit.csv |\n| Test Data to evaluate the build model | ..\/input\/aps_failure_test_set_processed_8bit.csv |","68fc631a":"By seeing the decision tree, we can tell the importance of sensor by traversing the tree nodes from root to leaves.\n\nBy adjuesting searching the parameter, we can achieve the model improved classification capability.\nEspecially improved the recall.","20639dc7":"### Evaluation Metrics\n\nIn evaluating the prediction logic, it is a challenge to extract failure events as much as possible while maintaining the operation rate. For that reason, it is necessary to accurately derive the judgment of correctness. Here, to make the balance of Confusion Matrix, adopt a model with a high result of scoring by ROC-AUC.\nWe also use accuracy, precision, and recall score for flassifier capability comparison.\n\n### Benchmark Model\n\nAs benchmark, adopts 60% correct answer rate.\nThis is based on a hearing from an interview myself conducted that the failure rate prevented by regular maintenance is about 60% by experience.\nIt is worth considering the devided model in this project, if failure prediction is more than 60% Accuracy.\n\nHowever, it is desirable that the accuracy of the model be increased as much as possible.\nIt is evaluated as data preprocessing to be adopted and benchmark of hyper parameter of XGBoost model compared with Random Forest model.\n\nRandom Forest uses a set of decision trees, and each tree represents some decision path to the positive class, from features.\nSubset of features are used in each tree.\nThe features picked up are different amond trees. These features are used in the list of questions as a branch in the tree to reach from the ground to leaves(=income).\n\nUsually, a single tree is not strong enough to be used in practice. To overcome this, Random Forest uses a lot of decision trees which are slightly differentn with each other. When we get a new answer from those trees, we take the majority vote of among the trees to get a final result. Compared to employing a single tree, you can reduce the proportion of incorrect results. By default, a Random Forest will use the sqaure root of the number of features as the maximum features that it will look on any given branch. In our case we have total 171 features, so each decision will be the best of the 5(approximate) randomly selected features available.\n\nIt is difficult to measure the quality of a given model without quantifying its performance over training and testing. This is typically done using some type of performance metric, whether it is through calculating some type of error, the goodness of fit, or some other useful measurement. For this project, you will be calculating the coefficient of determination, R2, to quantify your model's performance. The coefficient of determination for a model is a useful statistic in regression analysis, as it often describes how \"good\" that model is at making predictions.\n\nThe values for R2 range from 0 to 1, which captures the percentage of squared correlation between the predicted and actual values of the target variable. A model with an R2 of 0 is no better than a model that always predicts the mean of the target variable, whereas a model with an R2 of 1 perfectly predicts the target variable. Any value between 0 and 1 indicates what percentage of the target variable, using this model, can be explained by the features. A model can be given a negative R2 as well, which indicates that the model is arbitrarily worse than one that always predicts the mean of the target variable.\n\nWe apply the R2 score to evaluate the modeling performance based on the equation below.\nTarald O. Kvalseth: \"Cautionary Note about R2\", The American Statistician Vol. 39, No. 4, Part 1 (Nov., 1985), pp. 279-285","02516a52":"#### XGBoost for reduced features set by PCA transformation\n","4fc9e9bb":"Precision,Recall\u306a\u3069\u307f\u3066\u3082\u30c7\u30fc\u30bf\u524d\u51e6\u7406\u3092\u884c\u308f\u306a\u3044XGBoost\u304c\u9ad8\u3044\u6027\u80fd\u3092\u767a\u63ee\u3057\u3066\u3044\u308b\u3053\u3068\u304c\u308f\u304b\u308b\u3002\n\n##### RandomForest\n              precision    recall  f1-score   support\n           0       0.99      1.00      0.99     15625\n           1       0.97      0.41      0.57       375\n\n \n##### XGBoost \n\n              precision    recall  f1-score   support\n           0       0.99      1.00      1.00     15625\n           1       0.92      0.75      0.83       375\n\n\n##### XGBoost  PCA\n\n              precision    recall  f1-score   support\n           0       0.99      1.00      0.99     15625\n           1       0.89      0.60      0.72       375\n\n\n##### XGBoost  Log Transform\n\n              precision    recall  f1-score   support\n           0       0.99      1.00      0.99     15625\n           1       0.89      0.58      0.70       375\n","d434438d":"R-squared, coefficient of determination\n$$ \n1 - \\frac{\\sum_{n-1}^{i=0} (y_i - \\hat{y})^2}{\\sum_{n-1}^{i=0} (y_i -\\overline{y})^2}\n$$ ","1324239c":"#### XGBoost after PCA and Log Transform","0ca2f8ae":"### Problem Statement\n\nHere with the dataset provided, build a classification model that appropriately classifies whether the data acquired from the sensor is a sensor data combination that leads to an Air Pressure System error.\nBased on this classification, failure occurrence is data is classified.\nIn general, when monitoring the operation status of assets using sensors, in most cases, it is necessary to acquire data of a time period during which no problem occurs while in a normal operation.\nTherefore, when constructing a model that detects an abnormal condition, it is necessary to confirm that the error state is appropriately included in the target data set.\n\nIn model construction, in order to improve classification accuracy, it is common to consider pre-processing of data according to the characteristics of the data set.\nIn this project, we first analyze the characteristics of the data set of the target model.\nIn addition, we will consider, implement and evaluate the application of the optimal model.\nIn constructing the classification model, we adopt XGBoost and tackle the problem with the following approaches.\n\n1. Load data to obtain training data set and test data set.\n2. Analyze data to understanda dataset and design necessary preprocessing of data for model construction.\n3. Apply some predicting model classifiers to benchmark the good classifiers in this use case.\n4. Tune the XGBoost predicting model classifiers predictor to improve the classification capability of the model.\n5. Evaluate with the model with given test set, with confusion matrix, and accuracy, precision, and f1 scores.\n6. Evaluate the contribution of feature, to discuss the possibility of reducing the sensors to predict failures.\n","d8ca7bc2":"#### XGBClassifier","b2e78bd5":"XGBoost improved the predictio model against Random Forest in this case.\nBut R2 score of 0.40 means that 40 percent of the variance in Y is predictable from X.\nWhich is lower than the regular maintenance measure.\n\nAlthough a better result is obtained than RainForest, it can be seen that the R2 score tends to be better without pre-processing the data.\nIt is also predicted that the R2 score is further improved by adjusting the hyper parameter.\nThe target data of this time has characteristics on the data distribution that the target class to be detected is included in the minority who deviates from the 75th percentile at the center of the distribution.\nIt is not perfectly random data but may be in accordance with the probability distribution peculiar to the measurement object.\nTherefore, it is inferred that preprocessing of data assuming a normal distribution does not lead to an improvement of the accuracy of the model, rather it is better to positively utilize data of minorities in distribution.\n\nFor these reasons, we will adjust the optimal hyperparameter with XGBoost without performing data preprocessing for the initial accuracy of more than 60%.\n\nIt is understood that XGBoost which does not perform data preprocessing also shows high performance even if Precision, Recall etc. are seen.","1060b04f":"With regard to training data and test data, preprocessing is not carried out based on the above, and given data is utilized.\nhttps:\/\/xgboost.readthedocs.io\/en\/latest\/python\/python_api.html#xgboost.XGBClassifier\n\nIn determining the hyperparameter, this project conducted the following search.\nWe adjusted several parameters beforehand and focused on * max_depth (int) *, * n_estimators (int) * which improved the effect.","624bb3c3":"#### Random Forest Classifier\n","f8a0dd95":"## Seek hyperparameters\n","25b1af72":"Specifically, when looking at the percentile value of some data, it can be seen that there are more sensor data values with the same value at each of the 25th, 50th and 75th\npercentiles. It is understood that the concentration is concentrated to some values extremely.\nThis is a situation that is sufficient if it is assumed that the target is a sensor, and most of them measure steady state other than the failure state. When performing machine\nabnormality diagnosis, it is thought that it is necessary to construct a model that derives less abnormal data from overwhelming majority of normal data.\nA part of each statistical data of the feature quantity in the training data set is displayed.","cd9910b8":"## Libraries Used (Imports)","e87aad58":"## Preprocessing\n\nCconsider the necessary preprocessing of the featues values from sensors below.\nIf the scales of the values for each feature are different, for example, the calculation error by the computer may affect the prediction preciseness and accuracy. Also, if there is extreme bias of the values, it may affect prediction accuracy.\nIn constructing the prediction model, it is customary to grasp the characteristics of a given training data, perform necessary pretreatment, and eliminate these influences as much as possible.\n\n### Necessity discussion on data scaling\n\nFirst of all, in seeing the bias of data, 171 feature quantities were plotted in the box diagram.\nAll data are within the range of -1 to 1.\nFor the 75th percentile, its width varies for each feature.\nAlthough the overall data has been preconditioned to fit within the range of -1 to 1, the data of the 75 th percentile is within the order of 0.01.\nAlso, it can be seen that data that does not fit in the 75th percentile (black point in the figure) called Outlier has spread in each feature amount.\n\nIn the following, we analyze the above-mentioned bias of distribution and features on Outlier and verify whether or not to process, and what kind of processing should be done if it should be processed.\n\nSpecifically, when looking at the percentile value of some data, it can be seen that there are more sensor data values with the same value at each of the 25th, 50th and 75th percentiles.\nIt is understood that the concentration is concentrated to some values extremely.\n\nThis is a situation that is sufficient if it is assumed that the target is a sensor, and most of them measure steady state other than the failure state.\nWhen performing machine abnormality diagnosis, it is thought that it is necessary to construct a model that derives less abnormal data from overwhelming majority of normal data.\n\nA part of each statistical data of the feature quantity in the training data set is displayed.\n","a931fd78":"In this data set, in order to discriminate whether it is failure data of Air Pressure System or other data,\ndata is classified into the following two classes.\n\n| Category | Description |\n|----------|-------------|\n| positive class | *consists of component failures for a specific component of the APS system.* |\n| negative class | *consists of trucks with failures for components not related to the APS.*  | \n\nIn the classifier introduced in this kernel, it is required to appropriately classify the data set that becomes a positive class.\n\nThus, in the following, we build a classification model that properly classifies the data set whose class is 1.\nIn the classifier introduced in this project, it is required to appropriately classify the data set that are classified to the positive class.\nIn the following, in order to make it easy to identify the two target classes by the machine learning model,\n0.992188 for positive class and -0.992188 for negative class, we re-labeled positive class as 1 and negative class as 0 with integer values.\n\nIn the following, in order to make it easy to identify the two target classes by the machine learning model,\n0.992188 for positive class and -0.992188 for negative class,\nWe re-labeled positive class as 1 and negative class as 0 with integer values.","f30ef2cf":"By using PCA (Primary Component Classifier) \u200b\u200bwe were able to reduce 171 feature quantities and convert them to 67 feature values \u200b\u200bwith low correlation with each other.\nHere, although it can be expected that the calculation cost and accuracy at the time of model construction can be improved, it is verified whether classification is performed properly.\nThe objective of this project is to increase the accuracy of the model that classifies the positive class (consists of component failures for a specific component of the APS system).\nWe confirmed how these data are classified after conversion by PCA.\n\nOne class SVM determined the percentage of Positive Class and Negative Class separating 50%.\nWhen dividing 60000 data into two, most Positive classes are classified into data classified as Out excluded by One Class SVM.\nIn classification by One Class SVM, the Positive class to be detected was almost excluded.\n\n|-| Positive | Negative |\n|-|-|-|\n|Out | 991 | 29009 |\n|In | 9 | 2991 |\n\nFor sensor data, most of the data is in normal state.\nAs for the data of this Negative class, it is data which was acquired at the time of abnormality to some kind of in-vehicle component.\nIn other words, it is data acquired in the situation where the abnormality of the target APS does not occur.\nTherefore, from the viewpoint of APS, it is considered that data biased to the normal state is acquired.","8b2c1983":"When the distribution of values is extremely different between feature quantities, accuracy may be affected by influence of computer rounding. Therefore, due to scale\nconversion, the influence due to the width of distribution may be avoided. The target data of this report was originally scaled in the range of -1 to 1 as shown in the box\ndiagram above. Therefore, as you can see in the box diagram below after the scale return, the state of the distribution did not change, and it was judged that the effect of\nscale conversion on this data is small. Scale conversion is not applied when building this model.","b52a1e5b":"#### XGBClassifier","1d9473c8":"## V. Conclusion\n\nBy adopting XGBoost, we could construct a classification prediction model with higher fitness than Random Forest used for benchmark.\nBy tuning the hyper parameter, it was possible to increase the fitness of the model."}}