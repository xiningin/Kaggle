{"cell_type":{"af205fb7":"code","b3a08de3":"code","665bea99":"code","3e1c342e":"code","6c81088f":"code","975ca64b":"code","f437f772":"code","cd08ff99":"code","1bca09e2":"code","f5f9447f":"code","b8461733":"code","3cc8aed5":"code","8c7c622c":"code","28a14c43":"code","4329357e":"code","a35d22f9":"code","56bede21":"code","47201bc4":"code","38d86bcb":"code","054d54c4":"code","64061598":"code","64389ea7":"code","6ab2a12e":"code","81a2cb10":"code","be752215":"code","84360df7":"code","b5accb3b":"code","40451e2a":"code","920ea2f8":"code","388fdd83":"code","cf11d1e0":"code","01e1ffd1":"code","aab3688b":"code","813eeb6e":"code","92e548ac":"code","34fae2a6":"code","40b6ac03":"code","28decafe":"code","ec5bbbc3":"code","18793aa3":"code","8d4b7527":"code","71a8946c":"markdown","19e3892f":"markdown","138bb367":"markdown","32ef6697":"markdown","5f05c85c":"markdown","f3bbf573":"markdown","2faccf6b":"markdown","d3360ce7":"markdown","0952490b":"markdown","c64bd937":"markdown","9d493556":"markdown","cc5ab89b":"markdown","044ce732":"markdown","00f764e6":"markdown","fa3e4d13":"markdown","38f8da08":"markdown","70f71ca1":"markdown","a2d874d2":"markdown","8e1c8e51":"markdown","8aa2a9d1":"markdown","29adbc83":"markdown","df93fb75":"markdown","b1e76dae":"markdown","99f2d3c8":"markdown","81686164":"markdown","01ea9ad1":"markdown","3ef7736e":"markdown","ed23a8be":"markdown","b59bfec1":"markdown","c4220099":"markdown","07488622":"markdown","67e04fee":"markdown","c5b57fae":"markdown","7c8ab215":"markdown"},"source":{"af205fb7":"# Imports\n\nimport numpy as np\nimport pandas as pd\nimport math\nimport tensorflow as tf\nimport matplotlib.pyplot as plt","b3a08de3":"# dataset path\nraw_dataset_url = '\/kaggle\/input\/australia_rain_tomorrow_raw.csv'","665bea99":"# Read column names from file\nfeature_list = list(pd.read_csv(raw_dataset_url, nrows =1))\n\nprint(feature_list)","3e1c342e":"# list of columns to remove\nlist_remove = ['RISK_MM']","6c81088f":"# define headers to load\nheader_list = [feature for feature in feature_list if feature not in list_remove]\n\n# open dataset csv file with desired columns\nraw_dataset = pd.read_csv(raw_dataset_url, sep = ',', usecols = header_list)","975ca64b":"# print random samples\nprint(raw_dataset.sample(5))","f437f772":"# get number of examples (rows)\nnb_row = raw_dataset.shape[0]\nprint('Number of examples:', nb_row)","cd08ff99":"# get No\/Yes ratio for 'RainTomorrow'\nraw_dataset['RainTomorrow'].value_counts()","1bca09e2":"# init list\nnan_report = [None] * len(header_list)\n\n# loop over feature list\nfor index, feature in enumerate(header_list):\n    nan_report[index] = raw_dataset[feature].isna().sum()\n    print(feature, ': ', nan_report[index], ' \/ ', round(nan_report[index] \/ nb_row * 100, 2), '%')\n","f5f9447f":"# list of features to be dropped\ndrop_list = ['Evaporation', 'Sunshine', 'Cloud9am', 'Cloud3pm']\n\n# drop those features from raw_dataset\nraw_dataset.drop(columns=drop_list, inplace=True)\n\n# update lists -- remove dropped features\nnan_report = [value for value, feature in zip(nan_report, header_list) if feature not in drop_list]\nheader_list = [feature for feature in header_list if feature not in drop_list]\n\n# check\nprint(raw_dataset.sample(5))","b8461733":"# init lists\nnumerical_feature = []\ncategorical_feature = []\n\n# loop over dataframe types\nfor feature, type in enumerate(raw_dataset.dtypes):\n    if type == 'float64': #assuming I checked raw_dataset.dtypes before\n        numerical_feature.append(header_list[feature])\n    else: #non 'float64' are objects\n        categorical_feature.append(header_list[feature])\n\nprint('Numerical features: ', numerical_feature)\nprint('\\nCategorical features: ', categorical_feature)","3cc8aed5":"eng_dataset = raw_dataset.copy()","8c7c622c":"# loop over categorical_feature\nfor feature in categorical_feature:\n    index = header_list.index(feature)\n    print(feature, ': ', nan_report[index], ' \/ ', round(nan_report[index] \/ nb_row * 100, 2), '%')","28a14c43":"# fill 'RainToday' NaN values with 'No'\neng_dataset['RainToday'].fillna('No', inplace=True)\n\n# fill 'WindGustDir', 'WindDir9am' and 'WindDir3pm' NaN values with 'UNK'\nfor feature in ['WindGustDir', 'WindDir9am', 'WindDir3pm']:\n    eng_dataset[feature].fillna('UNK', inplace=True)","4329357e":"# loop over numerical_feature\nfor feature in numerical_feature:\n    index = header_list.index(feature)\n    print(feature, ': ', nan_report[index], ' \/ ', round(nan_report[index] \/ nb_row * 100, 2), '%')","a35d22f9":"# init columns\ncolumns = numerical_feature.copy()\ncolumns.insert(0, 'Location')\n\n# create empty dataframes with shape\nmean_by_location_rain = {}\nmean_by_location_norain = {}\n\n# define utility to get means by location\ndef mean_by_location(input_df, rain):\n    \n    output_dict = {}\n    working_df = input_df.loc[input_df['RainTomorrow'] == rain].copy()\n    \n    # loop over input_df group by 'Location'\n    for location, databyloc_df in working_df[columns].groupby('Location'):\n    \n        loc_mean = {}\n    \n        # loop over columns of databyloc_df\n        for feature in list(databyloc_df[numerical_feature]):\n\n            loc_mean[feature] = databyloc_df[feature].mean()\n            \n        # append local dict to output_list\n        output_dict[location] = loc_mean\n\n    return(output_dict)\n\n\n# call utility\nmean_by_location_rain = mean_by_location(eng_dataset, rain='Yes')\nmean_by_location_norain = mean_by_location(eng_dataset, rain='No')\n","56bede21":"# init \nmean_by_rain = {}\nmean_by_norain = {}\n\n# loop over numerical_feature\nfor feature in numerical_feature:\n    mean_by_rain[feature] = eng_dataset.loc[eng_dataset['RainTomorrow'] == 'Yes', feature].mean()\n    mean_by_norain[feature] = eng_dataset.loc[eng_dataset['RainTomorrow'] == 'No', feature].mean()\n    \nprint('mean_by_rain: ', mean_by_rain)\nprint('\\nmean_by_norain: ', mean_by_norain)","47201bc4":"# fill local NaN values based on global means\nmean_by_location_rain = {location: {feature: mean_by_rain.get(feature) if math.isnan(value) else value for feature, value in loc_dict.items()}\n                         for location, loc_dict in mean_by_location_rain.items()}\n\nmean_by_location_norain = {location: {feature: mean_by_norain.get(feature) if math.isnan(value) else value for feature, value in loc_dict.items()}\n                         for location, loc_dict in mean_by_location_norain.items()}\n","38d86bcb":"print ('Nb of NaN:\\n', eng_dataset[numerical_feature].isna().sum())","054d54c4":"# loop over location_list\nfor (location, rain_dict), (_, norain_dict) in zip(mean_by_location_rain.items(), mean_by_location_norain.items()):\n    \n    mask = (eng_dataset['Location'] == location) & (eng_dataset['RainTomorrow'] == 'Yes')\n    eng_dataset.loc[mask] = eng_dataset.loc[mask].fillna(value=rain_dict)\n\n    mask2 = (eng_dataset['Location'] == location) & (eng_dataset['RainTomorrow'] == 'No')\n    eng_dataset.loc[mask2] = eng_dataset.loc[mask2].fillna(value=norain_dict)","64061598":"print ('\\nNb of NaN:\\n', eng_dataset[numerical_feature].isna().sum())","64389ea7":"print(eng_dataset.sample(5))","6ab2a12e":"# Split Date into Day, Month, Year and drop Date, Day, Year\neng_dataset[['Day','Month', 'Year']] = eng_dataset['Date'].str.split(\"\/\", expand=True)\neng_dataset.drop(['Date', 'Day', 'Year'], axis=1, inplace=True)\n\nprint(eng_dataset.sample(2))","81a2cb10":"# create location mapping dictionnary (climate types are already indexed)\nlocation_mapping = {'Adelaide': 0, 'Albany': 0, 'Albury': 0, 'AliceSprings': 1,\n                    'BadgerysCreek': 0, 'Ballarat': 0, 'Bendigo': 0, 'Brisbane': 2,\n                    'Cairns': 3, 'Canberra': 0, 'Cobar': 4, 'CoffsHarbour': 2,\n                    'Dartmoor': 0, 'Darwin': 3, 'GoldCoast': 2, 'Hobart': 0, 'Katherine': 3,\n                    'Launceston': 0, 'Melbourne': 0, 'MelbourneAirport': 0, 'Mildura': 4,\n                    'Moree': 2, 'MountGambier': 0, 'MountGinini': 0, 'Newcastle': 0, 'Nhil': 4,\n                    'NorahHead': 0, 'NorfolkIsland': 2, 'Nuriootpa': 0, 'PearceRAAF': 2,\n                    'Penrith': 0, 'Perth': 2, 'PerthAirport': 2, 'Portland': 0, 'Richmond': 0,\n                    'Sale': 0, 'SalmonGums': 4, 'Sydney': 0, 'SydneyAirport': 0, 'Townsville': 3,\n                    'Tuggeranong': 0, 'Uluru': 1, 'WaggaWagga': 0, 'Walpole': 0, 'Watsonia': 0,\n                    'Williamtown': 0, 'Witchcliffe': 0, 'Wollongong': 0, 'Woomera': 1}","be752215":"# replace location by climate type index\neng_dataset['Location'].replace(location_mapping, inplace=True)","84360df7":"print(eng_dataset.sample(5))","b5accb3b":"# extract unique values (they are same for WindGustDir, WindDir9am and WindDir3pm)\nwind_unique_values = eng_dataset['WindGustDir'].unique()\nwind_unique_values.sort()\n\n# create wind dir mapping\nwind_mapping = {key: value for value, key in enumerate(wind_unique_values)}\n\nprint('wind_mapping =', wind_mapping)\n\n# create yes\/no mapping\nbinary_mapping = {'No': 0, 'Yes':1}\n\nprint('binary_mapping =', binary_mapping)","40451e2a":"# replace \neng_dataset[['WindGustDir', 'WindDir9am', 'WindDir3pm']] = eng_dataset[['WindGustDir', 'WindDir9am', 'WindDir3pm']].replace(wind_mapping)\neng_dataset[['RainToday', 'RainTomorrow']] = eng_dataset[['RainToday', 'RainTomorrow']].replace(binary_mapping)\n\nprint(eng_dataset.sample(5))","920ea2f8":"# init\nmean_dict = {}\nrange_dict = {}\n\n# loop over numerical features\nfor feature in numerical_feature:\n    \n    # compute means\n    mean_dict[feature] = eng_dataset[feature].mean()\n    \n    # compute range\n    range_dict[feature] = eng_dataset[feature].max() - eng_dataset[feature].min()\n    \n    # nomalize feature\n    eng_dataset[feature] = (eng_dataset[feature] - mean_dict[feature]) \/ range_dict[feature]\n\n    \nprint('mean_dict:', mean_dict)\nprint('\\nrange_dict:', range_dict)","388fdd83":"print(eng_dataset.sample(5))","cf11d1e0":"# fixing month type..\neng_dataset['Month'] = eng_dataset['Month'].astype(float)","01e1ffd1":"# Label column\nlabel_col = ['RainTomorrow']\n\n# Dataset split\nsplit = [0.80, 0.10, 0.10]\nshuffle = True\n\n# model\nlearning_rate = 0.001\nbatch_size=32\nepochs=500 #I did 100 \/ 300 before, just trying to see where we go\nsteps_per_epoch=10","aab3688b":"# utility to shuffle and split dataset\ndef split_dataset(input_df, label_col, ratio_list, shuffle):\n    \n    # Parameters:\n    # -----------\n    #    - input_df: input dataframe\n    #    - label_col: name of the label column in input_df\n    #    - ratio_list: [] of ratios = 0.x\n    #    - shuffle: boolean if shuffle is requested\n    # Output:\n    # -------\n    #    - training_df, validation_df, test_df\n    \n    temp_df = input_df.copy()\n    \n    # Shuffle dataset\n    if (shuffle):\n        print('Shuffle dataset')\n        temp_df.sample(frac=1)\n\n    # Extract and drop labels from dataset\n    labels = temp_df[label_col].copy()\n    temp_df = temp_df.drop(label_col, axis=1)\n\n    # Compute split indexes\n    nb_row = temp_df.shape[0]\n    print('nb_row =', nb_row)\n    index_1 = int(nb_row * split[0])\n    index_2 = int(nb_row * (split[0] + split[1]))\n    print('index_1 =', index_1)\n    print('index_2 =', index_2)\n\n    # Split\n    training_set = temp_df[:index_1]\n    validation_set = temp_df[index_1:index_2]\n    test_set = temp_df[index_2:]\n    training_label = labels[:index_1]\n    validation_label = labels[index_1:index_2]\n    test_label = labels[index_2:]\n\n    # Check\n    print ('\\nTraining set :', training_set.shape)\n    print ('Training labels :', training_label.shape)\n    print ('Validation set :', validation_set.shape)\n    print ('Validation labels :', validation_label.shape)\n    print ('Test set :', test_set.shape)\n    print ('Test labels :', test_label.shape)\n    \n    # return\n    return training_set, training_label, validation_set, validation_label, test_set, test_label","813eeb6e":"# call split utility\ntraining_set, training_label, validation_set, validation_label, test_set, test_label = split_dataset(eng_dataset, label_col, split, shuffle)","92e548ac":"# clean session\ntf.keras.backend.clear_session()\n\n# define model\nmodel = tf.keras.models.Sequential([\n    tf.keras.layers.Dense(128, activation='relu', dtype='float32'),\n    tf.keras.layers.Dense(128, activation='relu', dtype='float32'),\n    tf.keras.layers.Dense(64, activation='relu', dtype='float32'),\n    tf.keras.layers.Dense(32, activation='relu', dtype='float32'),\n    tf.keras.layers.Dense(16, activation='relu', dtype='float32'),\n    tf.keras.layers.Dense(8, activation='relu', dtype='float32'),\n    tf.keras.layers.Dropout(0.5),\n    tf.keras.layers.Dense(1, activation=tf.nn.sigmoid)\n])\n\n# define optimizer\noptimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n\n# define compile options\nmodel.compile(optimizer=optimizer, loss='binary_crossentropy', metrics='binary_accuracy')","34fae2a6":"# train model\nhistory = model.fit(\n    x=training_set,\n    y=training_label,\n    batch_size=batch_size,\n    epochs=epochs,\n    verbose=1,\n    validation_data=(validation_set, validation_label),\n    steps_per_epoch=steps_per_epoch,\n)","40b6ac03":"plt.plot(history.history['binary_accuracy'])\nplt.plot(history.history['val_binary_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\n\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","28decafe":"# make prediction\nraw_predictions = model.predict(\n    test_set,\n    batch_size=None,\n    verbose=1,\n)","ec5bbbc3":"# utility to replace by binary value\ndef make_binary(input_data, threshold):\n    \n    output = input_data.copy()\n    output = np.nan_to_num(output)\n    output[output > threshold] = 1\n    output[output <= threshold] = 0\n    \n    return output","18793aa3":"binary_predictions = make_binary(raw_predictions, 0.43)\npredictions_df = pd.DataFrame(binary_predictions)\npredictions_df.rename({0: label_col[0]}, axis=1, inplace=True)\n\npredictions_df.sample(5)","8d4b7527":"# init\nnb_test_example = test_set.shape[0]\nindex = list(range(0, nb_test_example))\nmetrics = pd.DataFrame(np.nan, index=index, columns=['true_positive', 'false_positive', 'false_negative', 'true_negative'])\n\n# reindex labels\ntest_label.reset_index(drop=True, inplace=True)\n\n# compare predictions with labels \ncompare = list((predictions_df == test_label).any(1))\n\n# get count of prediction OK\nprediction_ok = compare.count(True)\nprediction_ko = compare.count(False)\n\n# compute accuracy\naccuracy = prediction_ok \/ test_set.shape[0]\n\n# compare predictions with labels\nmetrics['true_positive'] = np.where((predictions_df['RainTomorrow'] == True) & (test_label['RainTomorrow'] == True), True, False)\nmetrics['false_positive'] = np.where((predictions_df['RainTomorrow'] == True) & (test_label['RainTomorrow'] == False), True, False)\nmetrics['false_negative'] = np.where((predictions_df['RainTomorrow'] == False) & (test_label['RainTomorrow'] == True), True, False)\nmetrics['true_negative'] = np.where((predictions_df['RainTomorrow'] == False) & (test_label['RainTomorrow'] == False), True, False)\n\n# extract counts\ntrue_positive = np.count_nonzero(metrics['true_positive'])\nfalse_positive = np.count_nonzero(metrics['false_positive'])\nfalse_negative = np.count_nonzero(metrics['false_negative'])\ntrue_negative = np.count_nonzero(metrics['true_negative'])\n\n# Precition, recall\nprecision = true_positive \/ (true_positive + false_positive)\nrecall = true_positive \/ (true_positive + false_negative)\n\n#F1Scrore\nf1_score = (2 * precision * recall) \/ (precision + recall)\n\n# display\nprint('nb_row test set :', test_set.shape[0])\n\nprint('\\nPrediction OK :', prediction_ok)\nprint('Prediction KO :', prediction_ko)\nprint('Accuracy :', round(accuracy * 100, 2), '%')\n\nprint('\\nTrue_positive :', true_positive)\nprint('False_positive :', false_positive)\nprint('False_negative :', false_negative)\nprint('True_negative :', true_negative)\n\nprint('\\nPrecision :', round(precision, 2))\nprint('Recall :', round(recall, 2))\n\nprint('\\nF1 Score:', round(f1_score, 2))","71a8946c":"## 7. Conclusion\n\nActually the 88% accuracy is better than my draft version (was ~87%, I did a bit of engineering and fixed some issues).\nI would be interresting to try different options for NaN values, Location, Date, Wind directions - I didn't do so far - and check if it improves the predictions.\n\nMy feeling is that since the dataset contains 110316 examples without rain tomorrow and 31877 with rain (this is Australia ^^),\nthe model struggles a bit to correctly predict for rain:\n* False negative are quite high (an analysis of this could help improve the model)\n* Best result is obtained with a threshold at 0.43.. so it's like the model predictions need a little boost! (0.50 resulted in 86.xx% accuracy)\n","19e3892f":"I'm going to replace NaN by mean values, but in order to be more specific, I want to capture something like means by location by whether it will rain tomorrow or not.","138bb367":"## 4. Feature engineering","32ef6697":"## 6.2 Shuffle and split dataset","5f05c85c":"- 'Date', 'Location' and 'RainTomorrow' are all good, nothing to do :)\n- 'RainToday' is missing when previous day is not part of the dataset. Since that's 0.99% of the examples, I'm just going to put 'No' where it's missing - it would be interresting to see what happens if you delete those rows or set them to some 'unknown' value. (but not expecting a huge impact out of 0.99%)\n- 'WindGustDir', 'WindDir9am' and 'WindDir3pm' are wind directions (N, NNE, NE, ...), I'm adding 'UNK' for unknown ; you might want to try to fill NaN values with maybe the most common value per Location (I didn't try), or even maybe to just delete those rows as well.","f3bbf573":"# Australia, Rain Tomorrow.","2faccf6b":"### 3.3 Categorical features","d3360ce7":"# 6. Model definition and training\n\n## 6.1 Parameters","0952490b":"### 3.4 Numerical features","c64bd937":"*Time to fixe the Month type issue... (TensorFlow will throw an ambigous error at model.fit() time otherwise)*","9d493556":"Let's build dictionnaries to store global means (one if rain, one if no rain):","cc5ab89b":"## 2. Quick data observation","044ce732":"First, let's capture basic information about the dataset (available features, types, number of examples).","00f764e6":"### 6.4 Model training","fa3e4d13":"## 1. Introduction","38f8da08":"Now let's fill the numerical NaN values by:\n- first choice: mean by location by whether it will rain tomorrow or not,\n- if not available: mean by whether it will rain tomorrow or not.","70f71ca1":"### 3.1 Drop features\n\nFeatures with huge amount of NaN values will be dropped from dataset:\n- Evaporation :  42.79 %\n- Sunshine :  47.69 %\n- Cloud9am :  37.74 %\n- Cloud3pm :  40.15 %","a2d874d2":"### 6.6 Validate predictions","8e1c8e51":"## 3. NaN values analysis","8aa2a9d1":"*Note: I obviously missed to convert Month values to something acceptable by TensorFlow... You'll find a hack later on to fix that*","29adbc83":"### 4.2 Location\n\nThere are 49 locations in the dataset. That's a lot of different categories for a single feature.\nSome locations are very similar: Sydney and SydneyAirport or Melbourne and Richmond are very similar locations as compared to the size of Australia!\nActually I decided to group locations by climate type after a look at the climate zone:\n\n![](https:\/\/greenharvest.com.au\/Images\/Miscellaneous\/AustralianClimateZoneMap.png)\n\nSo Cairns is going to be 'Tropical', Sydney is 'Temperate', Perth is 'SubTropical' and Mildura is 'Grassland'...","df93fb75":"The 'RISK_MM' feature is a prediction of the amout of rain for the next day (in mm), so this has to be ignored.","b1e76dae":"### 6.5 Make predictions","99f2d3c8":"Let's print a sample of the dataset examples.","81686164":"### Other categorical features\n\nLet's index all other categorical features (WindGustDir, WindDir9am, WindDir3pm, RainToday and RainTomorrow).","01ea9ad1":"## 5. Feature normalization\n\nOkay now we're going to normalize all numerical features - not the categorical ones - with (x - mean \/ range).\nFeel free to try various normalization approaches and see how it impacts the accuracy.","3ef7736e":"Okay let's have a look at the NaN status for our categorical features:","ed23a8be":"That's going to be our reference: setting all predictions to 'No' gives 77.58% accuracy.","b59bfec1":"### 4.1 Date feature\n\nDate values are way too specific. I decided to just extract a new 'Month' feature from it, in order to introduce some seasonality.\nAt this point, I don't believe days are very useful, maybe week in month (1,2,3,4) or first\/second half of the month could help but I didn't try.\nI will skip year as well, feel free to try and see if it helps or not.","c4220099":"There are some location\/feature without any value:\nAlbany has no WindGustSpeed value at all for example.\nOne option could be to delete those rows again, but I will first try to fill those ones with global means over all locations depending on if it will rain tomorrow or not (that means WindGustSpeed for Albany will receive a different mean value for RainTomorrow=Yes and RainTomorrow=No)","07488622":"### 3.2 Extract list of numerical and categorical features","67e04fee":"### 6.3 Model definition","c5b57fae":"Dataset is provided by the Australian Government - Bureau of Meteorology (BOM): http:\/\/www.bom.gov.au\/climate\/data\/\n\nIt contains 140000+ examples, captured in different locations accross Australia, with daily values and label whether there was rain or not on the next day.\nGoal is to predict whether it will rain on the next day.","7c8ab215":"From here I will make a copy of raw_dataset as eng_dataset to keep the raw data safe if needed later."}}