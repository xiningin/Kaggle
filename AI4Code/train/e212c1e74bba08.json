{"cell_type":{"c2cc40d4":"code","ac47b4a8":"code","023482ff":"code","e2120b17":"code","124ae479":"code","66d54106":"code","fba35242":"code","6300740e":"code","1d0356d0":"code","a3498784":"code","f8d5781e":"code","5911ea80":"code","a6629197":"code","73ba351a":"code","297aa34d":"code","db97ff60":"code","4a0696e8":"code","bc342a71":"code","e8352b3f":"code","92aee4f6":"code","21bdd3dc":"code","6cd0d33e":"code","432687b8":"code","6dc4e857":"code","ec42f82f":"code","447e9f6d":"code","fbdfadc3":"code","55f2e155":"code","292c79df":"code","797038a8":"code","93ad9553":"code","cdf13300":"code","a4fd65ed":"code","52a5f818":"code","f0a5ec9a":"code","067e5003":"code","81216a6d":"code","80ec0bd5":"code","d50b587e":"markdown","115e26c3":"markdown","101c53e3":"markdown","fd810dd7":"markdown","bb694730":"markdown","c9223094":"markdown","c658c20d":"markdown","771c9c08":"markdown","8b504625":"markdown","22b064fd":"markdown","82bbff83":"markdown","27e02de2":"markdown","42bc5616":"markdown","42d51e47":"markdown","ca67ca73":"markdown","28d95067":"markdown","f8ac9866":"markdown","58ea444a":"markdown","eaa7cf37":"markdown","0927218c":"markdown","118c4899":"markdown","056bc495":"markdown","288d8261":"markdown","3f9bfb03":"markdown","cc73b16a":"markdown","e55420c8":"markdown"},"source":{"c2cc40d4":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\n\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n\n\nfrom sklearn import model_selection\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n\nfrom keras.wrappers.scikit_learn import KerasClassifier\n\nimport matplotlib.pyplot as plt","ac47b4a8":"import warnings\nwarnings.filterwarnings('ignore')","023482ff":"pd.set_option('display.max_columns', 50)\ndataset = pd.read_csv('\/kaggle\/input\/telecom-users-dataset\/telecom_users.csv', index_col = 0).reset_index(drop = True)","e2120b17":"dataset.head()","124ae479":"dataset.info()","66d54106":"dataset.convert_dtypes().dtypes","fba35242":"dataset['TotalCharges'] = dataset['TotalCharges'].replace(' ', np.nan).astype(float)","6300740e":"dataset.isna().sum()","1d0356d0":"dataset = dataset.convert_dtypes().dropna()","a3498784":"for col in dataset.columns:\n    print(col, 'Column index: %s' % dataset.columns.tolist().index(col), dataset[col].unique(), sep = '\\n')\n    print()","f8d5781e":"# define baseline model\ndef baseline_ann_model():\n    ann = tf.keras.models.Sequential()\n    ann.add(tf.keras.layers.Dense(units=64, activation='relu'))\n    ann.add(tf.keras.layers.Dense(units=64, activation='relu'))\n    ann.add(tf.keras.layers.Dense(units=2, activation='sigmoid'))\n    ann.compile(optimizer = 'adam', loss = 'categorical_crossentropy')\n    return ann","5911ea80":"lr = LogisticRegression()\nrfc = RandomForestClassifier()\nxgb = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,\n                                 max_depth=1, random_state=0)\n\nann_estimator = KerasClassifier(build_fn=baseline_ann_model, \n                                epochs=100, batch_size=32, verbose=0, validation_split = 0.2)","a6629197":"models = []\nmodels.append(('LR', lr))\nmodels.append(('RandomForest', rfc))\nmodels.append(('XGBoost', xgb))\nmodels.append(('ANN', ann_estimator))","73ba351a":"X = dataset.drop('customerID', axis = 1).iloc[:, :-1].values\ny = dataset.iloc[:, -1].values","297aa34d":"binary = [0, 2, 3, 5, 15] #\u00a0Columns index for binary data\ncategorical = [6, 7, 8, 9, 10, 11, 12, 13, 14, 16] #\u00a0Columns index for categorical data","db97ff60":"print(X)","4a0696e8":"le = LabelEncoder()\nfor bin_ in binary:\n    X[:, bin_] = le.fit_transform(X[:, bin_])","bc342a71":"print(X)","e8352b3f":"print(y)","92aee4f6":"y = le.fit_transform(y)","21bdd3dc":"print(y)","6cd0d33e":"X.shape","432687b8":"ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), categorical)], remainder='passthrough')\nX = np.array(ct.fit_transform(X))","6dc4e857":"X.shape","ec42f82f":"pd.isnull(X).sum()","447e9f6d":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nX = scaler.fit_transform(X)","fbdfadc3":"lr1 = LogisticRegression()\nann1 = ann_estimator\n\nprint('Fitting LR')\nlr1.fit(X.astype(float), y)\n\nprint('Fitting ANN')\nann1.fit(X.astype(float), y)","55f2e155":"lr_coefs = np.squeeze(lr1.coef_)\nfor_plot = range(1, len(lr_coefs) + 1)","292c79df":"weights, biases = ann_estimator.model.layers[0].get_weights()","797038a8":"plt.figure(figsize = (10,8))\nplt.plot(for_plot, lr_coefs, label = 'LR Coefficients')\nplt.plot(for_plot, [np.mean(x) for x in weights], c = 'g', label = 'First Layer Weight Vector Mean')\nplt.legend()\nplt.xlabel('variables')\nplt.ylabel('coefficients')\nplt.axhline(0, color='black')\nplt.title('ANN First Layer Weight Vectors vs. LR Coefs')\nplt.show()","93ad9553":"def KFold_ANN(X, y, n_splits, build_model_function, batch_size, epochs, metrics):\n    kf = KFold(n_splits=n_splits, shuffle=True)\n    cv_metric = dict()\n    for metric_name in metrics:\n        cv_metric[metric_name] = []\n\n    y_single = np.argmax(y, axis=1)\n\n    for train_index, val_index in kf.split(X):\n        # fit & predict\n        model = KerasClassifier(build_fn = build_model_function,\n                                batch_size = batch_size,\n                                epochs = epochs,\n                               verbose = 0)\n        \n        model.fit(X[train_index].astype(float), y[train_index])\n        pred = model.predict(X[val_index].astype(float)) \n\n        # get fold metric & append\n        for metric_name in metrics:\n            metric_score = metrics[metric_name]\n            fold_metric = metric_score(y_single[val_index], pred)\n            cv_metric[metric_name].append(fold_metric)\n    return cv_metric","cdf13300":"def onehot_transform(y): # onehot_transform function converts the target values 0 and 1 to one-hot vectors [1,0] and [0,1]\n\n    onehot_y = []\n\n    for numb in y:\n        onehot_arr = np.zeros(2)\n        onehot_arr[numb] = 1\n        onehot_y.append(np.array(onehot_arr))\n\n    return np.array(onehot_y)","a4fd65ed":"def Cross_Validate_Models(metrics, n_splits):# evaluate each model in turn\n    results = {}\n    names = []\n    for name, model in models:\n        names.append(name)\n        #print(name, model)\n        if name == 'ANN':\n            dummy_y = onehot_transform(y)\n            cv_results = KFold_ANN(X, dummy_y, n_splits, baseline_ann_model, 32, 100, metrics)\n        else:\n            kfold = model_selection.KFold(n_splits = n_splits, shuffle = True)\n            cv_results = model_selection.cross_validate(model, X, y, cv = kfold, scoring = [metric for metric in metrics])\n        results[name] = cv_results\n    return results, names","52a5f818":"metrics = {'accuracy': accuracy_score,\n        'recall': recall_score,\n        'precision': precision_score,\n        'f1': f1_score}","f0a5ec9a":"results, names = Cross_Validate_Models(metrics, n_splits = 10)","067e5003":"d_fix = {\n    'test_accuracy':'accuracy',\n    'test_precision':'precision',\n    'test_recall':'recall',\n    'test_f1':'f1',\n}\nnew_results = {}\nfor key in results:\n    if key != 'ANN':\n        rst = results[key]\n        new_results[key] = dict((d_fix[key], value) for (key, value) in rst.items() if 'test' in key)\n    else:\n        new_results[key] = results[key]","81216a6d":"def plot_model_comparison():\n    # boxplot algorithm comparison\n    metrics = ['accuracy','precision','recall','f1']\n    fig = plt.figure(figsize = (15,12))\n    for i in range(len(metrics)):\n        ax = plt.subplot(2, 2, i + 1)\n        metric = metrics[i]\n        res = []\n        for name in names:\n            res.append(new_results[name][metric])\n        fig.suptitle('Algorithm Comparison')\n        plt.boxplot(res)\n        plt.ylabel(metric + ' score')\n        plt.xlabel('models')\n        ax.set_xticklabels(names)\n    plt.show()","80ec0bd5":"plot_model_comparison()","d50b587e":"We will now perform cross-validation to compare several performance metrics between the models.","115e26c3":"## Variable description","101c53e3":"## Methodology","fd810dd7":"#### Categorical","bb694730":">Any business wants to maximize the number of customers. To achieve this goal, it is important not only to try to attract new ones, but also to retain existing ones. Retaining a client will cost the company less than attracting a new one. In addition, a new client may be weakly interested in business services and it will be difficult to work with him, while old clients already have the necessary data on interaction with the service.\n>\n>Accordingly, predicting the churn, we can react in time and try to keep the client who wants to leave. Based on the data about the services that the client uses, we can make him a special offer, trying to change his decision to leave the operator. This will make the task of retention easier to implement than the task of attracting new users, about which we do not know anything yet.\n>\n>You are provided with a dataset from a telecommunications company. The data contains information about almost six thousand users, their demographic characteristics, the services they use, the duration of using the operator's services, the method of payment, and the amount of payment.","c9223094":"The results show that the ANN underperforms compared to the other models.\n\n\nOne important advantage of ML models over ANNs is their ability to identify factor effects and their directions on the response variable, allowing us to easily verify which variables have the biggest impact, improving model interpretability. Although we can extract the weights from the first layer of the ANN related to the input variables, these are not as intuitive as the ML models feature importance: there's another layer with another set of weights also affecting the outcome. ","c658c20d":"### Data loading and variable inspection","771c9c08":"After a brief data overview, we will check how the different models interpret the many variables present in the dataset. We'd like to see how each variable impacts the final binary prediction in detail, and while this is possible for the ML algorithms, it's a bit more complicated with ANNs. Thus, we will retrieve the coefficients from the LR model and compare them to the weight vectors of the first layer of the neural network that has direct contact with the inputs.\nThen, we will cross-validate the models to obtain good performance metrics. This will help us find out which model is the best suited for this binary classification problem.\n\nNote: models will be cross-validated on data with minimum preprocessing and no feature engineering. Since the point here is to compare basic models, the ANN will also be a simple feed forward neural network with only two hidden layers (Input -> Dense(64) -> Dense(64) -> 2 output classes)","8b504625":"We first encode the binary columns and the target using LabelEncoder, then we one-hot categorical data using ColumnTransformer.","22b064fd":"## Modeling","82bbff83":"## Purpose","27e02de2":"While ANNs present a flexible structure and have shown to be powerful machines for prediction purposes, they can also be quite complex, having lots of parameters to choose (hidden layers, layer units, learning rate, etc.) which can be time-consuming.\nIn contrast, classic ML models are more friendly when coming up with a baseline model or setting up a benchmark to start a project.","42bc5616":"First, we define the models and check for differences in the way they consider training features.","42d51e47":"## Cross-validation for performance analysis","ca67ca73":"## Discussion","28d95067":"### Importing the libraries","f8ac9866":"## Churn Detection: model comparison using Telecom users dataset","58ea444a":"## Conclusion","eaa7cf37":"The purpose of this kernel is to compare different models on a binary classification churn detection problem: we will compare variable interpretability and performance metrics in order to gain insight on some of the algorithms pros and cons. The models chosen for this analysis are Logistic Regression (LR), Random Forest Classifier (RFC), Gradient Boosting Classifier (XGB) and a simple Artificial Neural Network (ANN). While Deep Learning models can be more powerful than traditional ML algorithms, they lack their easy and quick interpretation of variables. We will also be looking over the classification metrics regarding each model and we will see that the classic ML models (LR, RFC, XGB) outperform the vanilla network (ANN) in this binary classification problem.","0927218c":"We confirm there's no missing data.","118c4899":"The identification of key variables appears to be easier when using the logistic regression coefficients instead of the neural network weight vector means. Artificial neural networks are designed to approximate a hidden function by making complex connections, but analyzing it's structure will not give us any insight regarding that hidden function. On the other hand, the logistic regression gives a function approximation where the importance of each element is explicit.","056bc495":"TotalCharges is being detected as a string data column, due to some observations containing blanks (' '): we replace them with nan, convert the column to Float64 dtype and proceed to drop missing data.","288d8261":"### Preprocessing","3f9bfb03":"- customerID - customer id\n- gender - client gender (male \/ female)\n- SeniorCitizen - is the client retired (1, 0)\n- Partner - is the client married (Yes, No)\n- tenure - how many months a person has been a client of the company\n- PhoneService - is the telephone service connected (Yes, No)\n- MultipleLines - are multiple phone lines connected (Yes, No, No phone service)\n- InternetService - client's Internet service provider (DSL, Fiber optic, No)\n- OnlineSecurity - is the online security service connected (Yes, No, No internet service)\n- OnlineBackup - is the online backup service activated (Yes, No, No internet service)\n- DeviceProtection - does the client have equipment insurance (Yes, No, No internet service)\n- TechSupport - is the technical support service connected (Yes, No, No internet service)\n- StreamingTV - is the streaming TV service connected (Yes, No, No internet service)\n- StreamingMovies - is the streaming cinema service activated (Yes, No, No internet service)\n- Contract - type of customer contract (Month-to-month, One year, Two year)\n- PaperlessBilling - whether the client uses paperless billing (Yes, No)\n- PaymentMethod - payment method (Electronic check, Mailed check, Bank transfer (automatic), Credit card (automatic))\n- MonthlyCharges - current monthly payment\n- TotalCharges - the total amount that the client paid for the services for the entire time\n- Churn - whether there was a churn (Yes or No)","cc73b16a":"## Model explainability","e55420c8":"#### Binary"}}