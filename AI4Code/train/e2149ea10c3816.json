{"cell_type":{"c9482d9b":"code","ebb80ec3":"code","3100311c":"code","2bdd4b2f":"code","4f078b24":"code","d776ea83":"code","4ed2fdec":"code","309f3b2f":"code","99894d83":"code","4f6ed3b2":"code","81813dbc":"code","663bee99":"code","1a851c2d":"code","e63d77ba":"code","5edaabad":"code","2fa4b3d7":"code","eea71c0d":"code","a1b73ec0":"code","8ad21ade":"code","f1c71718":"code","f3a0003b":"code","b5de1356":"code","4b87864f":"code","23fc5b53":"code","1fc7d9f9":"code","20683301":"code","fe01ce25":"code","41249904":"code","153a1f88":"code","34f6c241":"code","16a97bc2":"code","9ed2670c":"code","7fddbe82":"code","38547516":"code","8a4f9bba":"code","f9791e66":"code","b39f7156":"code","34b958a9":"code","459cfc0d":"code","421e8cde":"code","363e6e72":"code","69b2037d":"code","00806c2f":"code","63c2894e":"code","5cc884e1":"code","a6d13238":"code","aa943260":"code","ceedba6b":"code","2fd668c4":"code","993b9f49":"code","954bb52b":"code","2845ef51":"code","cd4223e0":"code","5d3e6bbb":"code","3385c550":"code","243dcfb1":"code","b76ba800":"code","383504d2":"code","a066c5ef":"code","b753ca85":"code","47eb8288":"code","06c4fbd5":"code","86b01625":"code","ecdf764c":"code","8a968e5a":"code","0a9ed197":"code","2fdf33a3":"code","e6d84c21":"code","072599e0":"code","d49877c1":"code","abdfab02":"code","e914b80a":"code","0e4b70f2":"code","2b6ce528":"code","e3964789":"code","6e732af7":"code","4672e1d1":"code","a192ecec":"code","02ccbacb":"code","4c191529":"code","17010a5a":"code","86747e2a":"code","ad868d40":"markdown","d2c21ab7":"markdown","21e6b492":"markdown","03910e01":"markdown","37043bbf":"markdown","a017203e":"markdown","93268a57":"markdown","d95c5b7a":"markdown","1ff38de7":"markdown","87ecf0b2":"markdown","cb4f1a71":"markdown","992187b9":"markdown","7648306a":"markdown","7b03518a":"markdown","436f7083":"markdown","8aecc66a":"markdown","d999a977":"markdown","ee98fed9":"markdown","2088270c":"markdown","c129535c":"markdown","bd00678d":"markdown","6ec9a374":"markdown","6e4f823a":"markdown"},"source":{"c9482d9b":"# loading libraries\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n%matplotlib inline \n\nimport os\nprint(os.listdir(\"..\/input\"))\nimport warnings\nwarnings.filterwarnings(\"ignore\")","ebb80ec3":"df=pd.read_pickle(\"..\/input\/wm811k-wafer-map\/LSWMD.pkl\")\ndf.info()","3100311c":"df['failureNum']=df.failureType\nmapping_type={'Center':0,'Donut':1,'Edge-Loc':2,'Edge-Ring':3,'Loc':4,'Random':5,'Scratch':6,'Near-full':7,'none':8}\ndf=df.replace({'failureNum':mapping_type})","2bdd4b2f":"\ndf= df[(df['failureNum']>=0) & (df['failureNum']<=7)]\ndf = df.reset_index()\n","4f078b24":"import cv2","d776ea83":"column = ['waferMap', 'failureNum']\n\nfor col in df.columns:\n  if col not in column:\n    df = df.drop(col, axis = 1)\n\n# df_with_image_interpolation.head()\n\n# cnt  = 0\n# for x in np.unique(df_with_image_interpolation['waferMap'][0]):\n#   print(x)\n#   if(cnt > 100):\n#     break\n#   cnt += 1\ndef cubic_interpolation(img):\n  img = img.astype('float')\n  interpolated_image = cv2.resize(img, dsize = (45,45), interpolation=cv2.INTER_CUBIC)\n  return np.array(interpolated_image)\n","4ed2fdec":"feature_copy = df.copy()","309f3b2f":"def feature_func(dataframe):\n  feature = [np.array(x).astype(np.float32) for x in dataframe[:, 0]]\n  feature = np.array(feature).astype(np.float32)\n  feature = feature.reshape((25519, 45,45, 1))\n  return feature\n\ndef label(dataframe):\n  labels = np.array(dataframe[:, 1]).astype(np.float32)\n  return labels\n\ndef split():\n  return train_test_split( feature(), label() ,test_size = 0.25, random_state = 0, shuffle = True)","99894d83":"df['waferMap'] = df['waferMap'].apply(cubic_interpolation)\nfor i in df.columns:\n    if i != 'waferMap' and i != 'failureNum':\n        df.drop(i, axis = 1)","4f6ed3b2":"new_x = np.zeros((len(df['waferMap']), 45, 45, 3))","81813dbc":"#df.to_numpy()\nprint(df.to_numpy()[0][1])\ninput_x = feature_func(df.to_numpy())","663bee99":"label_x = label(df.to_numpy()) ","1a851c2d":"input_x.shape","e63d77ba":"def return_int(x):\n    if x < 0.5:\n        return 0\n    elif x < 1.5:\n        return 1\n    else:\n        return 2","5edaabad":"####very very time consuming code \n# \"\"\"\n# DONT RUN PLEASE\n# \"\"\"\n# for w in range(len(df['waferMap'])):\n#     for i in range(45):\n#         for j in range(45):\n#             new_x[w, i, j, return_int(input_x[w,i,j])] = 1","2fa4b3d7":"import gc\ngc.collect()","eea71c0d":"new_x.shape","a1b73ec0":"import tensorflow as tf\nfrom tensorflow import keras\nfrom sklearn.model_selection import train_test_split\n\nfrom tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Conv2D\nfrom tensorflow.keras.layers import Dropout, MaxPooling2D\n","8ad21ade":"def split():\n  return train_test_split( new_x , label_x ,test_size = 0.25, random_state = 0, shuffle = True)","f1c71718":"X_train, X_test, y_train, y_test = split()","f3a0003b":"# def minimum():\n#     min_arr = df['waferMap'].apply(np.min)\n#     return np.min(min_arr)\n# def maximum():\n#     max_arr = df['waferMap'].apply(np.max)\n#     return np.max(max_arr)\n\n\n# minimum_dataset = minimum()\n# maximum_dataset = maximum()\n\n\n# def min_max(x):\n#   x = np.subtract(x , minimum_dataset)\n#   y = np.subtract(maximum_dataset, minimum_dataset)\n#   return np.divide(x, y)\n\n# # minimium_array = df_with_image_interpolation['waferMap'].apply(np.min)\n# # minimum_ = np.min(minimium_array)\n# # maximum_array = df_with_image_interpolation['waferMap'].apply(np.max)\n# # maximum_ = np.max(maximum_array)\n# # minimum_, maximum_\n\n# df['waferMap'] = df['waferMap'].apply(min_max)\n\n# mapping_type = {'Center':0,'Donut':1,'Edge-Loc':2,'Edge-Ring':3,'Loc':4,'Random':5,'Scratch':6,'Near-full':7,'none':8, 'minimum':minimum_dataset, 'maximum': maximum_dataset }\n\n# type(df['waferMap'][0][0][0])\n","b5de1356":"X_train, X_test, y_train, y_test = tf.convert_to_tensor(X_train),  tf.convert_to_tensor(X_test), tf.convert_to_tensor(y_train), tf.convert_to_tensor(y_test)","4b87864f":"def create_model(input_shape_row, input_shape_col, channels=3):\n  base_model = keras.applications.VGG16(\n    weights='imagenet',  # Load weights pre-trained on ImageNet.\n    input_shape=(input_shape_row,input_shape_col, channels),\n    include_top=False)\n  base_model.trainable = False\n  model=keras.Sequential([\n  base_model,\n  GlobalAveragePooling2D(),\n  Dropout(0.33, seed = 0),\n  Dense(1024,activation='relu'),\n  Dropout(0.33, seed = 0),\n  Dense(512, activation='relu'),\n  Dropout(0.2, seed = 0),\n  Dense(512, activation='relu'),\n  Dense(8,activation='softmax')\n  ])\n  return model","23fc5b53":"def benchmark_model(input_shape_row, input_shape_col, channels=3):\n  base_model = keras.applications.VGG16(\n    weights='imagenet',  # Load weights pre-trained on ImageNet.\n    input_shape=(input_shape_row,input_shape_col, channels),\n    include_top=False)\n  base_model.trainable = False\n  model=keras.Sequential([\n  base_model,\n  Dense(8, activation = 'softmax')])\n  return model\n    \n    ","1fc7d9f9":"base_model = benchmark_model(45,45)","20683301":"optimizer = tf.keras.optimizers.Adam(\n#     learning_rate=0.0001,\n#     beta_1=0.9,\n#     beta_2=0.999,\n#     epsilon=1e-06,\n#     amsgrad=False,\n)","fe01ce25":"base_model.compile(optimizer=optimizer, loss = keras.losses.SparseCategoricalCrossentropy(), metrics=['accuracy',tf.keras.metrics.SparseTopKCategoricalAccuracy()])\n\n","41249904":"history = base_model.fit(X_train, y_train, validation_split=0.33, epochs=10)","153a1f88":"def plot_loss_acc(history, string1 = \"10,sgd,loss,base_model.png\"):\n    fig, ax = plt.subplots(2)\n    ax[0].plot(history.history['accuracy'])\n    ax[0].plot(history.history['val_accuracy'])\n    ax[0].set_title('model accuracy')\n    ax[0].set_ylabel('accuracy')\n    ax[0].set_xlabel('epoch')\n    ax[0].legend(['train', 'test'], loc='upper left')\n    #ax[0].savefig(\"10,sgd,accuracy,base_model.png\")\n    #ax[0].show()\n    # summarize history for loss\n\n\n    ax[1].plot(history.history['loss'])\n    ax[1].plot(history.history['val_loss'])\n    ax[1].set_title('model loss')\n    ax[1].set_ylabel('loss')\n    ax[1].set_xlabel('epoch')\n    ax[1].legend(['train', 'test'], loc='upper left')\n    plt.tight_layout()\n    plt.savefig(\"10,sgd,loss,base_model.png\")\n    plt.show()","34f6c241":"plot_loss_acc(history)","16a97bc2":"from tensorflow.keras import layers","9ed2670c":"def custom_bench_model(length, width, depth):\n    model = tf.keras.models.Sequential()\n    model.add(layers.Conv2D(32, (3, 3),padding='same', activation='relu', input_shape=(length, width, depth)))\n    model.add(layers.MaxPooling2D((2, 2)))\n    model.add(layers.Conv2D(64, (3, 3),padding='same',  activation='relu'))\n    model.add(layers.MaxPooling2D((2, 2)))\n    model.add(layers.Conv2D(64, (3, 3),padding='same',  activation='relu'))\n    model.add(layers.Flatten())\n    model.add(layers.Dense(64, activation='relu'))\n    model.add(layers.Dense(8, activation='softmax'))\n    return model","7fddbe82":"custom_bench_model = custom_bench_model(45,45,1)","38547516":"custom_bench_model.summary()","8a4f9bba":"custom_bench_model.compile(optimizer=tf.keras.optimizers.SGD(), loss = keras.losses.SparseCategoricalCrossentropy(), metrics=['accuracy'])\n","f9791e66":"input_x.shape","b39f7156":"label","34b958a9":"X, Y = tf.convert_to_tensor(input_x), tf.convert_to_tensor(label(df.to_numpy()))","459cfc0d":"#custom_bench_model.fit(X, Y, validation_split=0.33, epochs=10)","421e8cde":"plot_loss_acc()","363e6e72":"def custom_bench_l2_reg(length, width, depth):\n    model = tf.keras.models.Sequential([\n    Conv2D(32, (3, 3),padding='same', activation='relu', input_shape=(length, width, depth)),\n    MaxPooling2D((2, 2)),\n    Conv2D(64, (3, 3),padding='same',  activation='relu'),\n    MaxPooling2D((2, 2)),\n    Conv2D(64, (3, 3),padding='same',  activation='relu'),\n    tf.keras.layers.Flatten(),\n    Dense(128, activation='relu', kernel_regularizer = 'l2'),\n    Dense(64, activation='relu', kernel_regularizer = 'l2'),\n    Dense(8, activation='softmax')])\n    return model","69b2037d":"custom_bench_l2_model = custom_bench_l2_reg(45, 45, 1)","00806c2f":"# X_train = tf.keras.applications.vggi+16.preprocess_input(X_train)\n# X_test = tf.keras.applications.vggi+16.preprocess_input(X_test)\ndef plot_loss_accL2(history, i, string1):\n    fig, ax = plt.subplots(2, figsize=(10,15))\n    ax[i].plot(history.history['accuracy'])\n    ax[i].plot(history.history['val_accuracy'])\n    ax[i].set_title('model accuracy')\n    ax[i].set_ylabel('accuracy')\n    ax[i].set_xlabel('epoch')\n    ax[i].legend(['train', 'test'], loc='upper left')\n    #ax[i].savefig(\"i+1i,sgd,accuracy,base_model.png\")\n    #ax[i].show()\n    # summarize history for loss\n\n\n    ax[i+1].plot(history.history['loss'])\n    ax[i+1].plot(history.history['val_loss'])\n    ax[i+1].set_title('model loss')\n    ax[i+1].set_ylabel('loss')\n    ax[i+1].set_xlabel('epoch')\n    ax[i+1].legend(['train', 'test'], loc='upper left')\n    plt.tight_layout()\n    plt.savefig(string1, dpi = 300)\n    plt.show()","63c2894e":"def learning_rate_var(learning_rate):\n    custom_bench_l2_model.compile(optimizer=keras.optimizers.SGD(learning_rate), loss = keras.losses.SparseCategoricalCrossentropy(), metrics=['accuracy'])\n    history = custom_bench_l2_model.fit(tf.convert_to_tensor(input_x), tf.convert_to_tensor(label(df.to_numpy())),\n                                        validation_split=0.33, epochs=10)\n    file_name = \"custom_bench_l2_model\" + str(learning_rate) + \".png\"\n    plot_loss_accL2(history,  0, file_name)\n    \n    \n    \n    ","5cc884e1":"# learning_rate = 0.001\n# fig_custom_l2, ax_custom_l2 = plt.subplots(nrows=10, ncols = 2, figsize = (10, 15))\n# ax_custom_l2 = ax_custom_l2.ravel(order='C')\n# fig_custom_l2.tight_layout()","a6d13238":"learning_rate = 0.001\nfor i in range(0,10):\n    learning_rate_var(learning_rate)\n    learning_rate += 0.001\n    \n# fig_custom_l2.savefig(\"all_plots_10_epoch.png\", dpi = 500)\n    ","aa943260":"model_vgg_rms = create_model(45, 45)\n\n# X_train = tf.keras.applications.vgg16.preprocess_input(X_train)\n# X_test = tf.keras.applications.vgg16.preprocess_input(X_test)","ceedba6b":"optimizer = tf.keras.optimizers.Adam(\n    learning_rate=0.0001,\n    beta_1=0.9,\n    beta_2=0.999,\n    epsilon=1e-06,\n    amsgrad=False,\n)","2fd668c4":"model_vgg_rms.compile(optimizer=optimizer, loss = keras.losses.SparseCategoricalCrossentropy(), metrics=['accuracy',tf.keras.metrics.SparseTopKCategoricalAccuracy()])","993b9f49":"history = model_vgg_rms.fit(X_train, y_train, validation_split=0.33, epochs=10)","954bb52b":"output = model_vgg_rms.predict(X_test)\n\ndef return_label(row):\n  return np.argmax(row)\n\n\npredict_label = []\nfor row in output:\n  predict_label.append(return_label(row))\n\n\n\n\nx_label = ['Center','Donut','Edge-Loc','Edge-Ring','Loc','Random','Scratch','Near-full']\n","2845ef51":"from sklearn.metrics import confusion_matrix\nimport itertools\nimport matplotlib.pyplot as plt\nimport seaborn as sn\nimport matplotlib.cm as cm\n\nfrom sklearn.metrics import confusion_matrix\nx = confusion_matrix(y_test, predict_label)\ndf_cm = pd.DataFrame(x, index = x_label,\n                  columns = x_label)\n\n\ndf_cm","cd4223e0":"plt.figure(figsize = (10,7))\nsn.heatmap(df_cm, annot=True)\n\n\n\n","5d3e6bbb":"plt.figure(figsize = (10,7))\nsn.heatmap((df_cm - df_cm.mean()) \/ df_cm.std(), annot=True)","3385c550":"from sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\nplt.figure(figsize = (10,7))\nsn.heatmap(scaler.fit_transform(df_cm), annot=True)","243dcfb1":"model_vgg.save()","b76ba800":"# serialize model to JSON\nmodel_json = model_vgg.to_json()\nwith open(\"model_vgg_sgd.json\", \"w\") as json_file:\n    json_file.write(model_json)\n# serialize weights to HDF5\nmodel_vgg.save_weights(\"model_vgg_sgd.h5\")\nprint(\"Saved model to disk\")\n \n# later...\n \n","383504d2":"# load json and create model\njson_file = open('model.json', 'r')\nloaded_model_json = json_file.read()\njson_file.close()\nloaded_model = model_from_json(loaded_model_json)\n# load weights into new model\nloaded_model.load_weights(\"model.h5\")\nprint(\"Loaded model from disk\")","a066c5ef":"# cvv = type(df_with_image_interpolation['waferMap'][0])\n# df_with_image_interpolation[[type(x) == cvv for x in df_with_image_interpolation['waferMap']]]\ndf","b753ca85":"# np.save('saved_finally', saved)","47eb8288":"df.to_pickle('45,45 pickle')","06c4fbd5":"np.save('saved_finaly_yes_or_no', )","86b01625":"# loading libraries\nimport skimage\nfrom skimage import measure\nfrom skimage.transform import radon\nfrom skimage.transform import probabilistic_hough_line\nfrom skimage import measure\nfrom scipy import interpolate\nfrom scipy import stats","ecdf764c":"# illustration of 13 regions\nan = np.linspace(0, 2*np.pi, 100)\nplt.plot(2.5*np.cos(an), 2.5*np.sin(an))\nplt.axis('equal')\nplt.axis([-4, 4, -4, 4])\nplt.plot([-2.5, 2.5], [1.5, 1.5])\nplt.plot([-2.5, 2.5], [0.5, 0.5 ])\nplt.plot([-2.5, 2.5], [-0.5, -0.5 ])\nplt.plot([-2.5, 2.5], [-1.5,-1.5 ])\n\nplt.plot([0.5, 0.5], [-2.5, 2.5])\nplt.plot([1.5, 1.5], [-2.5, 2.5])\nplt.plot([-0.5, -0.5], [-2.5, 2.5])\nplt.plot([-1.5, -1.5], [-2.5, 2.5])\nplt.title(\" Devide wafer map to 13 regions\")\nplt.xticks([])\nplt.yticks([])\nplt.show()","8a968e5a":"def cal_den(x):\n    return 100*(np.sum(x==2)\/np.size(x))  \n\ndef find_regions(x):\n    rows=np.size(x,axis=0)\n    cols=np.size(x,axis=1)\n    ind1=np.arange(0,rows,rows\/\/5)\n    ind2=np.arange(0,cols,cols\/\/5)\n    \n    reg1=x[ind1[0]:ind1[1],:]\n    reg3=x[ind1[4]:,:]\n    reg4=x[:,ind2[0]:ind2[1]]\n    reg2=x[:,ind2[4]:]\n\n    reg5=x[ind1[1]:ind1[2],ind2[1]:ind2[2]]\n    reg6=x[ind1[1]:ind1[2],ind2[2]:ind2[3]]\n    reg7=x[ind1[1]:ind1[2],ind2[3]:ind2[4]]\n    reg8=x[ind1[2]:ind1[3],ind2[1]:ind2[2]]\n    reg9=x[ind1[2]:ind1[3],ind2[2]:ind2[3]]\n    reg10=x[ind1[2]:ind1[3],ind2[3]:ind2[4]]\n    reg11=x[ind1[3]:ind1[4],ind2[1]:ind2[2]]\n    reg12=x[ind1[3]:ind1[4],ind2[2]:ind2[3]]\n    reg13=x[ind1[3]:ind1[4],ind2[3]:ind2[4]]\n    \n    fea_reg_den = []\n    fea_reg_den = [cal_den(reg1),cal_den(reg2),cal_den(reg3),cal_den(reg4),cal_den(reg5),cal_den(reg6),cal_den(reg7),cal_den(reg8),cal_den(reg9),cal_den(reg10),cal_den(reg11),cal_den(reg12),cal_den(reg13)]\n    return fea_reg_den","0a9ed197":"df_withpattern['fea_reg']=df_withpattern.waferMap.apply(find_regions)","2fdf33a3":"x = [9,340, 3, 16, 0, 25, 84, 37]\nlabels2 = ['Center','Donut','Edge-Loc','Edge-Ring','Loc','Random','Scratch','Near-full']\n\nfig, ax = plt.subplots(nrows = 2, ncols = 4,figsize=(20, 10))\nax = ax.ravel(order='C')\nfor i in range(8):\n    ax[i].bar(np.linspace(1,13,13),df_withpattern.fea_reg[x[i]])\n    ax[i].set_title(df_withpattern.failureType[x[i]][0][0],fontsize=15)\n    ax[i].set_xticks([])\n    ax[i].set_yticks([])\n\nplt.tight_layout()\nplt.show() ","e6d84c21":"def change_val(img):\n    img[img==1] =0  \n    return img\n\ndf_withpattern_copy = df_withpattern.copy()\ndf_withpattern_copy['new_waferMap'] =df_withpattern_copy.waferMap.apply(change_val)","072599e0":"x = [9,340, 3, 16, 0, 25, 84, 37]\nlabels2 = ['Center','Donut','Edge-Loc','Edge-Ring','Loc','Random','Scratch','Near-full']\n\nfig, ax = plt.subplots(nrows = 2, ncols = 4, figsize=(20, 10))\nax = ax.ravel(order='C')\nfor i in range(8):\n    img = df_withpattern_copy.waferMap[x[i]]\n    theta = np.linspace(0., 180., max(img.shape), endpoint=False)\n    sinogram = radon(img, theta=theta)    \n      \n    ax[i].imshow(sinogram, cmap=plt.cm.Greys_r, extent=(0, 180, 0, sinogram.shape[0]), aspect='auto')\n    ax[i].set_title(df_withpattern_copy.failureType[x[i]][0][0],fontsize=15)\n    ax[i].set_xticks([])\nplt.tight_layout()\n\nplt.show() ","d49877c1":"def cubic_inter_mean(img):\n    theta = np.linspace(0., 180., max(img.shape), endpoint=False)\n    sinogram = radon(img, theta=theta)\n    xMean_Row = np.mean(sinogram, axis = 1)\n    x = np.linspace(1, xMean_Row.size, xMean_Row.size)\n    y = xMean_Row\n    f = interpolate.interp1d(x, y, kind = 'cubic')\n    xnew = np.linspace(1, xMean_Row.size, 20)\n    ynew = f(xnew)\/100   # use interpolation function returned by `interp1d`\n    return ynew\n\ndef cubic_inter_std(img):\n    theta = np.linspace(0., 180., max(img.shape), endpoint=False)\n    sinogram = radon(img, theta=theta)\n    xStd_Row = np.std(sinogram, axis=1)\n    x = np.linspace(1, xStd_Row.size, xStd_Row.size)\n    y = xStd_Row\n    f = interpolate.interp1d(x, y, kind = 'cubic')\n    xnew = np.linspace(1, xStd_Row.size, 20)\n    ynew = f(xnew)\/100   # use interpolation function returned by `interp1d`\n    return ynew  ","abdfab02":"df_withpattern_copy['fea_cub_mean'] =df_withpattern_copy.waferMap.apply(cubic_inter_mean)\ndf_withpattern_copy['fea_cub_std'] =df_withpattern_copy.waferMap.apply(cubic_inter_std)","e914b80a":"x = [9,340, 3, 16, 0, 25, 84, 37]\nlabels2 = ['Center','Donut','Edge-Loc','Edge-Ring','Loc','Random','Scratch','Near-full']\n\nfig, ax = plt.subplots(nrows = 2, ncols = 4,figsize=(20, 10))\nax = ax.ravel(order='C')\nfor i in range(8):\n    ax[i].bar(np.linspace(1,20,20),df_withpattern_copy.fea_cub_mean[x[i]])\n    ax[i].set_title(df_withpattern_copy.failureType[x[i]][0][0],fontsize=10)\n    ax[i].set_xticks([])\n    ax[i].set_xlim([0,21])   \n    ax[i].set_ylim([0,1])\nplt.tight_layout()\nplt.show() ","0e4b70f2":"fig, ax = plt.subplots(nrows = 2, ncols = 4,figsize=(20, 10))\nax = ax.ravel(order='C')\nfor i in range(8):\n    ax[i].bar(np.linspace(1,20,20),df_withpattern_copy.fea_cub_std[x[i]])\n    ax[i].set_title(df_withpattern_copy.failureType[x[i]][0][0],fontsize=10)\n    ax[i].set_xticks([])\n    ax[i].set_xlim([0,21])   \n    ax[i].set_ylim([0,0.3])\nplt.tight_layout()\nplt.show() ","2b6ce528":"x = [9,340, 3, 16, 0, 25, 84, 37]\nlabels2 = ['Center','Donut','Edge-Loc','Edge-Ring','Loc','Random','Scratch','Near-full']\n\nfig, ax = plt.subplots(nrows = 2, ncols = 4,figsize=(20, 10))\nax = ax.ravel(order='C')\nfor i in range(8):\n    img = df_withpattern_copy.waferMap[x[i]]\n    zero_img = np.zeros(img.shape)\n    img_labels = measure.label(img, neighbors=4, connectivity=1, background=0)\n    img_labels = img_labels-1\n    if img_labels.max()==0:\n        no_region = 0\n    else:\n        info_region = stats.mode(img_labels[img_labels>-1], axis = None)\n        no_region = info_region[0]\n    \n    zero_img[np.where(img_labels==no_region)] = 2 \n    ax[i].imshow(zero_img)\n    ax[i].set_title(df_withpattern_copy.failureType[x[i]][0][0],fontsize=10)\n    ax[i].set_xticks([])\nplt.tight_layout()\nplt.show() ","e3964789":"def cal_dist(img,x,y):\n    dim0=np.size(img,axis=0)    \n    dim1=np.size(img,axis=1)\n    dist = np.sqrt((x-dim0\/2)**2+(y-dim1\/2)**2)\n    return dist  \n\ndef fea_geom(img):\n    norm_area=img.shape[0]*img.shape[1]\n    norm_perimeter=np.sqrt((img.shape[0])**2+(img.shape[1])**2)\n    \n    img_labels = measure.label(img, neighbors=4, connectivity=1, background=0)\n\n    if img_labels.max()==0:\n        img_labels[img_labels==0]=1\n        no_region = 0\n    else:\n        info_region = stats.mode(img_labels[img_labels>0], axis = None)\n        no_region = info_region[0][0]-1       \n    \n    prop = measure.regionprops(img_labels)\n    prop_area = prop[no_region].area\/norm_area\n    prop_perimeter = prop[no_region].perimeter\/norm_perimeter \n    \n    prop_cent = prop[no_region].local_centroid \n    prop_cent = cal_dist(img,prop_cent[0],prop_cent[1])\n    \n    prop_majaxis = prop[no_region].major_axis_length\/norm_perimeter \n    prop_minaxis = prop[no_region].minor_axis_length\/norm_perimeter  \n    prop_ecc = prop[no_region].eccentricity  \n    prop_solidity = prop[no_region].solidity  \n    \n    return prop_area,prop_perimeter,prop_majaxis,prop_minaxis,prop_ecc,prop_solidity\n\ndf_withpattern_copy['fea_geom'] =df_withpattern_copy.waferMap.apply(fea_geom)","6e732af7":"df_withpattern_copy.fea_geom[340] #donut","4672e1d1":"df_all=df_withpattern_copy.copy()\na=[df_all.fea_reg[i] for i in range(df_all.shape[0])] #13\nb=[df_all.fea_cub_mean[i] for i in range(df_all.shape[0])] #20\nc=[df_all.fea_cub_std[i] for i in range(df_all.shape[0])] #20\nd=[df_all.fea_geom[i] for i in range(df_all.shape[0])] #6\nfea_all = np.concatenate((np.array(a),np.array(b),np.array(c),np.array(d)),axis=1) #59 in total","a192ecec":"label=[df_all.failureNum[i] for i in range(df_all.shape[0])]\nlabel=np.array(label)","02ccbacb":"import theano\nfrom theano import tensor as T\nfrom sklearn.cross_validation import train_test_split\nfrom keras.utils import np_utils\n\nX = fea_all\ny = label\n\nfrom collections import  Counter\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)                      \nprint('Training target statistics: {}'.format(Counter(y_train)))\nprint('Testing target statistics: {}'.format(Counter(y_test)))\n\nRANDOM_STATE =42\n","4c191529":"# ---multicalss classification ---# \n# One-Vs-One\nfrom sklearn.svm import LinearSVC\nfrom sklearn.multiclass import OneVsOneClassifier\nclf2 = OneVsOneClassifier(LinearSVC(random_state = RANDOM_STATE)).fit(X_train, y_train)\ny_train_pred = clf2.predict(X_train)\ny_test_pred = clf2.predict(X_test)\ntrain_acc2 = np.sum(y_train == y_train_pred, axis=0, dtype='float') \/ X_train.shape[0]\ntest_acc2 = np.sum(y_test == y_test_pred, axis=0, dtype='float') \/ X_test.shape[0]\nprint('One-Vs-One Training acc: {}'.format(train_acc2*100)) #One-Vs-One Training acc: 80.36\nprint('One-Vs-One Testing acc: {}'.format(test_acc2*100)) #One-Vs-One Testing acc: 79.04\nprint(\"y_train_pred[:100]: \", y_train_pred[:100])\nprint (\"y_train[:100]: \", y_train[:100])","17010a5a":"import itertools\nfrom sklearn.metrics import confusion_matrix\n\ndef plot_confusion_matrix(cm, normalize=False, title='Confusion matrix', cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    #print(cm)\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')    ","86747e2a":"# Compute confusion matrix\ncnf_matrix = confusion_matrix(y_test, y_test_pred)\nnp.set_printoptions(precision=2)\n\nfrom matplotlib import gridspec\nfig = plt.figure(figsize=(15, 8)) \ngs = gridspec.GridSpec(1, 2, width_ratios=[1, 1]) \n\n## Plot non-normalized confusion matrix\nplt.subplot(gs[0])\nplot_confusion_matrix(cnf_matrix, title='Confusion matrix')\n\n# Plot normalized confusion matrix\nplt.subplot(gs[1])\nplot_confusion_matrix(cnf_matrix, normalize=True, title='Normalized confusion matrix')\n\nplt.show()","ad868d40":"> **Radon-based feature (interpolate from row mean) **","d2c21ab7":"* If you have no idea which algorithm to choose, you may have a look on this Microsoft Azure Machine Learning Algorithm Cheat Sheet. Here is the link:[Machine Learning Algorithm Cheat Sheet](https:\/\/unsupervisedmethods.com\/cheat-sheet-of-machine-learning-and-python-and-math-cheat-sheets-a4afe4e791b6)","21e6b492":">** Radon-based feature (interpolate from row standard deviation)**","03910e01":">** Data summary**","37043bbf":"* The picture above shows density based features for 8 typical failure types. \n\n* It turns out that extracting density based feature is resonable and making the dataset more classifiable.","a017203e":"* pattern recognition confusion matrix","93268a57":"> **Step5: Improve results**","d95c5b7a":"* Divided wafer map into 13 parts and computed defects density accordingly. The 13 regions includes the inner 9 same regions and the top, bottom, left and right regions. \n\n* For each failure type, it has different density distribution patterns. For example, the center one of the inner 9 regions  will have high defects density for Center failure type.","1ff38de7":"**Step3: Choose algorithms**","87ecf0b2":"In summary, the process of improving results involves:\n\n* Algorithm Tuning: where discovering the best model is treated like a search problem through model parameter space.\n\n* Ensemble Methods: where the predictions made by multiple models are combined.\n\n* Extreme Feature Engineering: where the attribute decomposition and aggregation seen in data preparation is pushed to the limits.\n\nYou can discover more about in this blog: [Jason Brownlee's blog](https:\/\/machinelearningmastery.com\/process-for-working-through-machine-learning-problems\/)\n","cb4f1a71":"> **Randon-based Features (40)**","992187b9":"**Step4: Present results**","7648306a":"> **Combine all features together**\n\n* density-based features: 13\n\n* radon-based features: 40\n\n* geom-based features: 6\n\n* in total: 13+40+6=59   ","7b03518a":"> This module implements multiclass and multilabel learning algorithms: \n\nRefer to [scikits learning](http:\/\/ogrisel.github.io\/scikit-learn.org\/sklearn-tutorial\/modules\/multiclass.html)\n\n* **one-vs-the-rest :** one-vs-the-rest strategy consists in fitting one classifier per class. For each classifier, the class is fitted against all the other classes. \n\n* **one-vs-one:** one-vs-one classifier constructs one classifier per pair of classes.\n\n* error correcting output codes\n\nWe choose **One-VS-One multi-class SVMs** as our model based on literature review for this dataset.\n","436f7083":"> **Density-based Features (13)**","8aecc66a":">Target distribution","d999a977":"* The picture above shows radon tranform results for 8 typical failure types. \n\n* However, even we obtained radon transform values, we can not regard as features because the wafer vary in size. so in the next step, we using cubic interpolation to obtain fixed dimension feature values for row mean and row standard deviation from radon transform, for each one the dimension is fixed to 20.\n\n* In total, we extracted 40 dimensions for radon-based features.\n\n* For Cubic interpolation, please refer to scikit-image link about [Cubic interpolation](https:\/\/docs.scipy.org\/doc\/scipy\/reference\/tutorial\/interpolate.html).","ee98fed9":"* Randon-based features are based on the radon transform, which can generate a 2d representation of the wafer map according to a series of projections.\n\n* For Radon transform, please refer to scikit-image link about [Radon transform](http:\/\/scikit-image.org\/docs\/dev\/auto_examples\/transform\/plot_radon_transform.html).\n","2088270c":"# ","c129535c":"* The overall training accuracy is: **80.36%**\n\n* The overall testing accuracy is: **79.04%**","bd00678d":"> **Geometry-based Features (6)**","6ec9a374":"* Most **salient region identifying** can be regarded as noise filtering. In this work, we use region-labeling algorithm and choose the max area region as the most salient one.\n\n* Based on the salient region, we try to extract geometry features like area, perimeter, length of major axes,  length of minor axes, solidity and eccentricity.\n","6e4f823a":"> No Best Machine Learning Algorithm\n\nYou cannot know a priori which algorithm will be best suited for your problem.\n\nHere are some tips:\n\n* You can apply your favorite algorithm.\n* You can apply the algorithm recommended in a book or paper.\n* You can apply the algorithm that is winning the most Kaggle competitions right now.\n* You can apply the algorithm that works best with your test rig, infrastructure, database, or whatever.\n\nFor our multi class classification problem, we choose the most popular SVMs at the moment. "}}