{"cell_type":{"ec5f7a26":"code","cb2ac31c":"code","ae680966":"code","59e3ecb9":"code","f854b51e":"code","039d8402":"code","8b1bbe46":"code","e59dedf4":"code","adeb4c37":"code","f1e7d6d6":"code","428f468f":"code","de277a07":"code","dcbeb75f":"code","490d9b57":"code","779aba56":"markdown","a57d44a6":"markdown","170bf8c2":"markdown","1be06b59":"markdown","c94ff891":"markdown","2258b32e":"markdown","7a871165":"markdown","394e1114":"markdown","53916f1b":"markdown","08a171b9":"markdown","f6aa18d0":"markdown","5fd681b7":"markdown","adba9a2a":"markdown","1794b9d5":"markdown","b58296a0":"markdown"},"source":{"ec5f7a26":"# import pandas\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nimport tensorflow as tf\nimport matplotlib.pyplot as plt","cb2ac31c":"datasetnya = pd.read_csv('..\/input\/bbcnewsarchive\/bbc-news-data.csv', sep='\\t')\ndatasetnya.head()","ae680966":"new_dataset = datasetnya.drop(columns=['filename'])\nnew_dataset.head()","59e3ecb9":"kategori = pd.get_dummies(new_dataset.category)\nnewest_dataset = pd.concat([new_dataset, kategori], axis=1)\nnewest_dataset = newest_dataset.drop(columns='category')\nnewest_dataset","f854b51e":"\nbbc_news = newest_dataset['title'].values + '' + newest_dataset['content'].values\nbbc_berita = newest_dataset[['business', 'entertainment', 'politics', 'sport', 'tech']].values","039d8402":"bbc_news","8b1bbe46":"bbc_berita","e59dedf4":"bbc_news_train, bbc_news_test, bbc_berita_train, bbc_berita_test = train_test_split(bbc_news, bbc_berita, test_size=0.2)","adeb4c37":"tokenizer = Tokenizer(num_words=5000, oov_token='x')\ntokenizer.fit_on_texts(bbc_news_train) \ntokenizer.fit_on_texts(bbc_news_test)\n \nsekuens_train = tokenizer.texts_to_sequences(bbc_news_train)\nsekuens_test = tokenizer.texts_to_sequences(bbc_news_test)\n \npadded_train = pad_sequences(sekuens_train) \npadded_test = pad_sequences(sekuens_test)","f1e7d6d6":"model = tf.keras.Sequential([\n     tf.keras.layers.Embedding(input_dim=5000, output_dim=64),\n     tf.keras.layers.BatchNormalization(),\n     tf.keras.layers.LSTM(128),\n     tf.keras.layers.Dense(128, activation='relu'),\n     tf.keras.layers.Dropout(0.5),\n     tf.keras.layers.Dense(5, activation='softmax')\n     ])\nmodel.compile(loss='categorical_crossentropy',optimizer='rmsprop',metrics=['acc'])\nmodel.summary()","428f468f":"class myCallback(tf.keras.callbacks.Callback):\n  def on_epoch_end(self, epoch, logs={}):\n    if(logs.get('acc')>0.75 and logs.get('val_acc')>0.9):\n      self.model.stop_training = True\n      print(\"\\nAkurasinya > 75%!\")\n\nes_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, verbose=1)\ncallbacks = myCallback()","de277a07":"history = model.fit(padded_train, bbc_berita_train, epochs=50, \n                    validation_data=(padded_test, bbc_berita_test), verbose=2, callbacks=[callbacks], validation_steps=30)","dcbeb75f":"plt.plot(history.history['acc'])\nplt.plot(history.history['val_acc'])\nplt.title('Model Accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()","490d9b57":"plt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Model loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper right')\nplt.show()","779aba56":"Callback","a57d44a6":"Proses fit model","170bf8c2":"Proses tokenizer","1be06b59":"Split data menjadi data training dan data validation","c94ff891":"Kemudian import file yang akan kita gunakan, disini saya menggunakan dataset tentang berita dari BBC News.\n\nTampilkan 5 data teratas.","2258b32e":"Pertama kita import terlebih dahulu library yang akan kita gunakan.","7a871165":"Melihat label array","394e1114":"Model","53916f1b":"**Pendahuluan**\n\nHalo semuanya, setelah saya berusaha untuk menyelesaikan kelas belajar pengembangan machine learning dari dicoding indonesia sambil melakukan kesibukan lain seperti menyusun skripsi dan hal lainnya. Akhirnya saya berhasil menyelesaikan submission pertama yaitu membuat model NLP dengan TenserFlow. Akurasi yang harus didapatkan agar lulus submission pertama ini adalah 75%.","08a171b9":"**Model dan Plot**","f6aa18d0":"Ubah nilai pada dataframe menjadi sebuah numpy array","5fd681b7":"Model Loss","adba9a2a":"Kemudian drop data yang tidak akan kita gunakan.","1794b9d5":"Model Akurasi","b58296a0":"Melihat isi array"}}