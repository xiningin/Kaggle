{"cell_type":{"04cf3deb":"code","4d95eb3e":"code","0c8b0307":"code","29ca2b14":"code","5dc28994":"code","1c89c509":"code","43aaf625":"code","907697f0":"code","ab776150":"code","a21319c2":"code","a8d63dfa":"code","feaa24d8":"code","94031ade":"code","b01c471b":"code","1f19f41c":"code","bfee09a5":"code","f69fb7ce":"code","dcf7b360":"code","b1ab0802":"code","c47a3b75":"code","f77f3170":"code","d56858d4":"code","db60ffb5":"code","50f6e285":"code","193392c2":"code","711cd0c9":"code","8c9ffde3":"code","4e2c6ad3":"code","72caf232":"code","26557f64":"code","d62e3bdd":"code","f3169300":"code","1f88f592":"code","2d318a2b":"code","f21cde15":"code","6344c524":"code","ce4dca2c":"code","e877d77b":"code","a0790550":"code","e240fc4e":"code","57373bd6":"code","c3d3489a":"code","17475684":"code","a5be3942":"code","6247b8bd":"code","7b8438ec":"code","8ac76de0":"code","97cacc24":"code","d84fb4b2":"code","9df7db8b":"code","9810a02f":"code","4ac48786":"code","7376128c":"code","eda6fff2":"code","63adaa14":"code","32d4e650":"code","31df8a8c":"code","0e8c8d25":"code","876d56f8":"code","0c8f5e41":"code","1b743022":"code","3014b595":"code","e1360e6e":"code","4aca44db":"code","5cbab2ed":"code","0514ac5e":"code","146889b1":"code","5b5f7461":"code","ee0d49b7":"code","adbdd812":"code","d07961b7":"code","2ab5295c":"code","ad5d491c":"code","65505380":"code","3769fe50":"code","f34761ca":"code","0d82723f":"code","f99305b6":"code","1b206ba9":"code","094d0361":"code","9d7ba03d":"code","eacb04e7":"code","83a01e71":"code","2a04ea43":"code","33da27c0":"code","3e81d65c":"code","660ad988":"code","d2bfd618":"code","4ae85ef5":"code","b407daef":"code","e518980f":"code","1657a067":"code","9b374259":"code","dccd3355":"code","f8403847":"code","691d839f":"code","a06bf3fa":"code","8fc8167a":"code","4a71aff6":"code","b9f3e188":"code","1c4269c6":"code","ba1f8ee9":"code","9bc8e761":"code","4d367bdd":"code","82091ea0":"code","1b1beafc":"code","c1a65913":"code","19692fc1":"code","bcf2c9ff":"markdown","4276e192":"markdown","9e165287":"markdown","4f2e4bd6":"markdown","50b5f936":"markdown","a12cdbc4":"markdown","cfc2117c":"markdown","5e0e2884":"markdown","df93f3c7":"markdown","1b192917":"markdown","96575c50":"markdown","6b2e6c81":"markdown","9e7e555b":"markdown","b3a9fb9f":"markdown","c3923e92":"markdown","87734121":"markdown","35b4bca3":"markdown","5cb50b2d":"markdown","30a484f3":"markdown","3c525f23":"markdown","2d30ce00":"markdown","bfe140b7":"markdown","2b14f0d3":"markdown","2f8db7fe":"markdown","57ef7cf0":"markdown","d4a227bc":"markdown","36ff52af":"markdown","e0a33b54":"markdown","00d24673":"markdown","8b7181f4":"markdown","24ad3b1f":"markdown","8d637292":"markdown","3d62454f":"markdown","57f9c56c":"markdown","e7327448":"markdown","d7279d0b":"markdown","77f11b9e":"markdown"},"source":{"04cf3deb":"#Import necessary libraries\n%matplotlib inline\nimport sys\nimport numpy as np\nimport os\nimport sklearn\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl","4d95eb3e":"import pandas as pd\nhousing = pd.read_csv(\"..\/input\/california-housing-prices\/housing.csv\")","0c8b0307":"housing.head()","29ca2b14":"housing.info()","5dc28994":"housing[\"ocean_proximity\"].value_counts()","1c89c509":"housing.describe()","43aaf625":"%matplotlib inline\nimport matplotlib.pyplot as plt\nhousing.hist(bins=50, figsize=(20,15))\n#save_fig(\"attribute_histogram_plots\")\nplt.show()","907697f0":"# to make this notebook's output identical at every run\nnp.random.seed(42)","ab776150":"import numpy as np\n\n# For illustration only. Sklearn has train_test_split()\ndef split_train_test(data, test_ratio):\n    shuffled_indices = np.random.permutation(len(data))\n    test_set_size = int(len(data) * test_ratio)\n    test_indices = shuffled_indices[:test_set_size]\n    train_indices = shuffled_indices[test_set_size:]\n    return data.iloc[train_indices], data.iloc[test_indices]","a21319c2":"train_set, test_set = split_train_test(housing, 0.2)\nlen(train_set)","a8d63dfa":"len(test_set)","feaa24d8":"from zlib import crc32\n\ndef test_set_check(identifier, test_ratio):\n    return crc32(np.int64(identifier)) & 0xffffffff < test_ratio * 2**32\n\ndef split_train_test_by_id(data, test_ratio, id_column):\n    ids = data[id_column]\n    in_test_set = ids.apply(lambda id_: test_set_check(id_, test_ratio))\n    return data.loc[~in_test_set], data.loc[in_test_set]","94031ade":"import hashlib\n\ndef test_set_check(identifier, test_ratio, hash=hashlib.md5):\n    return hash(np.int64(identifier)).digest()[-1] < 256 * test_ratio","b01c471b":"def test_set_check(identifier, test_ratio, hash=hashlib.md5):\n    return bytearray(hash(np.int64(identifier)).digest())[-1] < 256 * test_ratio","1f19f41c":"housing_with_id = housing.reset_index()   # adds an `index` column\ntrain_set, test_set = split_train_test_by_id(housing_with_id, 0.2, \"index\")","bfee09a5":"housing_with_id[\"id\"] = housing[\"longitude\"] * 1000 + housing[\"latitude\"]\ntrain_set, test_set = split_train_test_by_id(housing_with_id, 0.2, \"id\")","f69fb7ce":"test_set.head()","dcf7b360":"from sklearn.model_selection import train_test_split\n\ntrain_set, test_set = train_test_split(housing, test_size=0.2, random_state=42)","b1ab0802":"test_set.head()","c47a3b75":"housing[\"median_income\"].hist()","f77f3170":"housing[\"income_cat\"] = pd.cut(housing[\"median_income\"],\n                               bins=[0., 1.5, 3.0, 4.5, 6., np.inf],\n                               labels=[1, 2, 3, 4, 5])","d56858d4":"housing[\"income_cat\"].value_counts()","db60ffb5":"housing[\"income_cat\"].hist()","50f6e285":"from sklearn.model_selection import StratifiedShuffleSplit\n\nsplit = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\nfor train_index, test_index in split.split(housing, housing[\"income_cat\"]):\n    strat_train_set = housing.loc[train_index]\n    strat_test_set = housing.loc[test_index]","193392c2":"strat_test_set[\"income_cat\"].value_counts() \/ len(strat_test_set)","711cd0c9":"housing[\"income_cat\"].value_counts() \/ len(housing)","8c9ffde3":"def income_cat_proportions(data):\n    return data[\"income_cat\"].value_counts() \/ len(data)\n\ntrain_set, test_set = train_test_split(housing, test_size=0.2, random_state=42)\n\ncompare_props = pd.DataFrame({\n    \"Overall\": income_cat_proportions(housing),\n    \"Stratified\": income_cat_proportions(strat_test_set),\n    \"Random\": income_cat_proportions(test_set),\n}).sort_index()\ncompare_props[\"Rand. %error\"] = 100 * compare_props[\"Random\"] \/ compare_props[\"Overall\"] - 100\ncompare_props[\"Strat. %error\"] = 100 * compare_props[\"Stratified\"] \/ compare_props[\"Overall\"] - 100","4e2c6ad3":"compare_props","72caf232":"for set_ in (strat_train_set, strat_test_set):\n    set_.drop(\"income_cat\", axis=1, inplace=True)","26557f64":"housing = strat_train_set.copy()","d62e3bdd":"housing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\")\n#save_fig(\"bad_visualization_plot\")","f3169300":"housing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", alpha=0.1)\n#save_fig(\"better_visualization_plot\")","1f88f592":"housing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", alpha=0.4,\n             s=housing[\"population\"]\/100, label=\"population\", figsize=(10,7),\n             c=\"median_house_value\", cmap=plt.get_cmap(\"jet\"), colorbar=True,\n             sharex=False)\nplt.legend()\n#save_fig(\"housing_prices_scatterplot\")","2d318a2b":"corr_matrix = housing.corr()","f21cde15":"corr_matrix[\"median_house_value\"].sort_values(ascending=False)","6344c524":"# from pandas.tools.plotting import scatter_matrix # For older versions of Pandas\nfrom pandas.plotting import scatter_matrix\n\nattributes = [\"median_house_value\", \"median_income\", \"total_rooms\",\n              \"housing_median_age\"]\nscatter_matrix(housing[attributes], figsize=(12, 8))\n#save_fig(\"scatter_matrix_plot\")","ce4dca2c":"housing.plot(kind=\"scatter\", x=\"median_income\", y=\"median_house_value\",\n             alpha=0.1)\nplt.axis([0, 16, 0, 550000])\n#save_fig(\"income_vs_house_value_scatterplot\")","e877d77b":"housing[\"rooms_per_household\"] = housing[\"total_rooms\"]\/housing[\"households\"]\nhousing[\"bedrooms_per_room\"] = housing[\"total_bedrooms\"]\/housing[\"total_rooms\"]\nhousing[\"population_per_household\"]=housing[\"population\"]\/housing[\"households\"]","a0790550":"corr_matrix = housing.corr()\ncorr_matrix[\"median_house_value\"].sort_values(ascending=False)","e240fc4e":"housing.plot(kind=\"scatter\", x=\"rooms_per_household\", y=\"median_house_value\",\n             alpha=0.2)\nplt.axis([0, 5, 0, 520000])\nplt.show()","57373bd6":"housing.describe()","c3d3489a":"housing = strat_train_set.drop(\"median_house_value\", axis=1) # drop labels for training set\nhousing_labels = strat_train_set[\"median_house_value\"].copy()","17475684":"sample_incomplete_rows = housing[housing.isnull().any(axis=1)].head()\nsample_incomplete_rows","a5be3942":"sample_incomplete_rows.dropna(subset=[\"total_bedrooms\"])    # option 1","6247b8bd":"sample_incomplete_rows.drop(\"total_bedrooms\", axis=1)       # option 2","7b8438ec":"median = housing[\"total_bedrooms\"].median()\nsample_incomplete_rows[\"total_bedrooms\"].fillna(median, inplace=True) # option 3","8ac76de0":"sample_incomplete_rows","97cacc24":"from sklearn.impute import SimpleImputer\nimputer = SimpleImputer(strategy=\"median\")","d84fb4b2":"housing_num = housing.drop(\"ocean_proximity\", axis=1)\n# alternatively: housing_num = housing.select_dtypes(include=[np.number])","9df7db8b":"imputer.fit(housing_num)","9810a02f":"imputer.statistics_","4ac48786":"housing_num.median().values","7376128c":"X = imputer.transform(housing_num)","eda6fff2":"housing_tr = pd.DataFrame(X, columns=housing_num.columns,\n                          index=housing.index)","63adaa14":"housing_tr.loc[sample_incomplete_rows.index.values]","32d4e650":"imputer.strategy","31df8a8c":"housing_tr = pd.DataFrame(X, columns=housing_num.columns,\n                          index=housing_num.index)","0e8c8d25":"housing_tr.head()","876d56f8":"housing_cat = housing[[\"ocean_proximity\"]]\nhousing_cat.head(10)","0c8f5e41":"from sklearn.preprocessing import OrdinalEncoder\n\nordinal_encoder = OrdinalEncoder()\nhousing_cat_encoded = ordinal_encoder.fit_transform(housing_cat)\nhousing_cat_encoded[:10]","1b743022":"ordinal_encoder.categories_","3014b595":"from sklearn.preprocessing import OneHotEncoder\n\ncat_encoder = OneHotEncoder()\nhousing_cat_1hot = cat_encoder.fit_transform(housing_cat)\nhousing_cat_1hot","e1360e6e":"housing_cat_1hot.toarray()","4aca44db":"cat_encoder = OneHotEncoder(sparse=False)\nhousing_cat_1hot = cat_encoder.fit_transform(housing_cat)\nhousing_cat_1hot","5cbab2ed":"cat_encoder.categories_","0514ac5e":"from sklearn.base import BaseEstimator, TransformerMixin\n\n# column index\nrooms_ix, bedrooms_ix, population_ix, households_ix = 3, 4, 5, 6\n\nclass CombinedAttributesAdder(BaseEstimator, TransformerMixin):\n    def __init__(self, add_bedrooms_per_room=True): # no *args or **kargs\n        self.add_bedrooms_per_room = add_bedrooms_per_room\n    def fit(self, X, y=None):\n        return self  # nothing else to do\n    def transform(self, X):\n        rooms_per_household = X[:, rooms_ix] \/ X[:, households_ix]\n        population_per_household = X[:, population_ix] \/ X[:, households_ix]\n        if self.add_bedrooms_per_room:\n            bedrooms_per_room = X[:, bedrooms_ix] \/ X[:, rooms_ix]\n            return np.c_[X, rooms_per_household, population_per_household,\n                         bedrooms_per_room]\n        else:\n            return np.c_[X, rooms_per_household, population_per_household]\n\nattr_adder = CombinedAttributesAdder(add_bedrooms_per_room=False)\nhousing_extra_attribs = attr_adder.transform(housing.values)","146889b1":"col_names = \"total_rooms\", \"total_bedrooms\", \"population\", \"households\"\nrooms_ix, bedrooms_ix, population_ix, households_ix = [\n    housing.columns.get_loc(c) for c in col_names] # get the column indices","5b5f7461":"housing_extra_attribs = pd.DataFrame(\n    housing_extra_attribs,\n    columns=list(housing.columns)+[\"rooms_per_household\", \"population_per_household\"],\n    index=housing.index)\nhousing_extra_attribs.head()","ee0d49b7":"from sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\n\nnum_pipeline = Pipeline([\n        ('imputer', SimpleImputer(strategy=\"median\")),\n        ('attribs_adder', CombinedAttributesAdder()),\n        ('std_scaler', StandardScaler()),\n    ])\n\nhousing_num_tr = num_pipeline.fit_transform(housing_num)","adbdd812":"housing_num_tr","d07961b7":"from sklearn.compose import ColumnTransformer\n\nnum_attribs = list(housing_num)\ncat_attribs = [\"ocean_proximity\"]\n\nfull_pipeline = ColumnTransformer([\n        (\"num\", num_pipeline, num_attribs),\n        (\"cat\", OneHotEncoder(), cat_attribs),\n    ])\n\nhousing_prepared = full_pipeline.fit_transform(housing)","2ab5295c":"housing_prepared","ad5d491c":"housing_prepared.shape","65505380":"from sklearn.base import BaseEstimator, TransformerMixin\n\n# Create a class to select numerical or categorical columns \nclass OldDataFrameSelector(BaseEstimator, TransformerMixin):\n    def __init__(self, attribute_names):\n        self.attribute_names = attribute_names\n    def fit(self, X, y=None):\n        return self\n    def transform(self, X):\n        return X[self.attribute_names].values","3769fe50":"num_attribs = list(housing_num)\ncat_attribs = [\"ocean_proximity\"]\n\nold_num_pipeline = Pipeline([\n        ('selector', OldDataFrameSelector(num_attribs)),\n        ('imputer', SimpleImputer(strategy=\"median\")),\n        ('attribs_adder', CombinedAttributesAdder()),\n        ('std_scaler', StandardScaler()),\n    ])\n\nold_cat_pipeline = Pipeline([\n        ('selector', OldDataFrameSelector(cat_attribs)),\n        ('cat_encoder', OneHotEncoder(sparse=False)),\n    ])","f34761ca":"from sklearn.pipeline import FeatureUnion\n\nold_full_pipeline = FeatureUnion(transformer_list=[\n        (\"num_pipeline\", old_num_pipeline),\n        (\"cat_pipeline\", old_cat_pipeline),\n    ])","0d82723f":"old_housing_prepared = old_full_pipeline.fit_transform(housing)\nold_housing_prepared","f99305b6":"np.allclose(housing_prepared, old_housing_prepared)","1b206ba9":"from sklearn.linear_model import LinearRegression\n\nlin_reg = LinearRegression()\nlin_reg.fit(housing_prepared, housing_labels)","094d0361":"# let's try the full preprocessing pipeline on a few training instances\nsome_data = housing.iloc[:5]\nsome_labels = housing_labels.iloc[:5]\nsome_data_prepared = full_pipeline.transform(some_data)\n\nprint(\"Predictions:\", lin_reg.predict(some_data_prepared))","9d7ba03d":"print(\"Labels:\", list(some_labels))","eacb04e7":"some_data_prepared","83a01e71":"from sklearn.metrics import mean_squared_error\n\nhousing_predictions = lin_reg.predict(housing_prepared)\nlin_mse = mean_squared_error(housing_labels, housing_predictions)\nlin_rmse = np.sqrt(lin_mse)\nlin_rmse","2a04ea43":"from sklearn.metrics import mean_absolute_error\n\nlin_mae = mean_absolute_error(housing_labels, housing_predictions)\nlin_mae","33da27c0":"from sklearn.tree import DecisionTreeRegressor\n\ntree_reg = DecisionTreeRegressor(random_state=42)\ntree_reg.fit(housing_prepared, housing_labels)","3e81d65c":"housing_predictions = tree_reg.predict(housing_prepared)\ntree_mse = mean_squared_error(housing_labels, housing_predictions)\ntree_rmse = np.sqrt(tree_mse)\ntree_rmse","660ad988":"from sklearn.model_selection import cross_val_score\n\nscores = cross_val_score(tree_reg, housing_prepared, housing_labels,\n                         scoring=\"neg_mean_squared_error\", cv=10)\ntree_rmse_scores = np.sqrt(-scores)","d2bfd618":"def display_scores(scores):\n    print(\"Scores:\", scores)\n    print(\"Mean:\", scores.mean())\n    print(\"Standard deviation:\", scores.std())\n\ndisplay_scores(tree_rmse_scores)","4ae85ef5":"lin_scores = cross_val_score(lin_reg, housing_prepared, housing_labels,\n                             scoring=\"neg_mean_squared_error\", cv=10)\nlin_rmse_scores = np.sqrt(-lin_scores)\ndisplay_scores(lin_rmse_scores)","b407daef":"from sklearn.ensemble import RandomForestRegressor\n\nforest_reg = RandomForestRegressor(n_estimators=100, random_state=42)\nforest_reg.fit(housing_prepared, housing_labels)","e518980f":"housing_predictions = forest_reg.predict(housing_prepared)\nforest_mse = mean_squared_error(housing_labels, housing_predictions)\nforest_rmse = np.sqrt(forest_mse)\nforest_rmse","1657a067":"from sklearn.model_selection import cross_val_score\n\nforest_scores = cross_val_score(forest_reg, housing_prepared, housing_labels,\n                                scoring=\"neg_mean_squared_error\", cv=10)\nforest_rmse_scores = np.sqrt(-forest_scores)\ndisplay_scores(forest_rmse_scores)","9b374259":"scores = cross_val_score(lin_reg, housing_prepared, housing_labels, scoring=\"neg_mean_squared_error\", cv=10)\npd.Series(np.sqrt(-scores)).describe()","dccd3355":"from sklearn.svm import SVR\n\nsvm_reg = SVR(kernel=\"linear\")\nsvm_reg.fit(housing_prepared, housing_labels)\nhousing_predictions = svm_reg.predict(housing_prepared)\nsvm_mse = mean_squared_error(housing_labels, housing_predictions)\nsvm_rmse = np.sqrt(svm_mse)\nsvm_rmse","f8403847":"from sklearn.model_selection import GridSearchCV\n\nparam_grid = [\n    # try 12 (3\u00d74) combinations of hyperparameters\n    {'n_estimators': [3, 10, 30], 'max_features': [2, 4, 6, 8]},\n    # then try 6 (2\u00d73) combinations with bootstrap set as False\n    {'bootstrap': [False], 'n_estimators': [3, 10], 'max_features': [2, 3, 4]},\n  ]\n\nforest_reg = RandomForestRegressor(random_state=42)\n# train across 5 folds, that's a total of (12+6)*5=90 rounds of training \ngrid_search = GridSearchCV(forest_reg, param_grid, cv=5,\n                           scoring='neg_mean_squared_error',\n                           return_train_score=True)\ngrid_search.fit(housing_prepared, housing_labels)","691d839f":"grid_search.best_params_","a06bf3fa":"grid_search.best_estimator_","8fc8167a":"cvres = grid_search.cv_results_\nfor mean_score, params in zip(cvres[\"mean_test_score\"], cvres[\"params\"]):\n    print(np.sqrt(-mean_score), params)","4a71aff6":"pd.DataFrame(grid_search.cv_results_)","b9f3e188":"from sklearn.model_selection import RandomizedSearchCV\nfrom scipy.stats import randint\n\nparam_distribs = {\n        'n_estimators': randint(low=1, high=200),\n        'max_features': randint(low=1, high=8),\n    }\n\nforest_reg = RandomForestRegressor(random_state=42)\nrnd_search = RandomizedSearchCV(forest_reg, param_distributions=param_distribs,\n                                n_iter=10, cv=5, scoring='neg_mean_squared_error', random_state=42)\nrnd_search.fit(housing_prepared, housing_labels)","1c4269c6":"cvres = rnd_search.cv_results_\nfor mean_score, params in zip(cvres[\"mean_test_score\"], cvres[\"params\"]):\n    print(np.sqrt(-mean_score), params)","ba1f8ee9":"feature_importances = grid_search.best_estimator_.feature_importances_\nfeature_importances","9bc8e761":"extra_attribs = [\"rooms_per_hhold\", \"pop_per_hhold\", \"bedrooms_per_room\"]\n#cat_encoder = cat_pipeline.named_steps[\"cat_encoder\"] # old solution\ncat_encoder = full_pipeline.named_transformers_[\"cat\"]\ncat_one_hot_attribs = list(cat_encoder.categories_[0])\nattributes = num_attribs + extra_attribs + cat_one_hot_attribs\nsorted(zip(feature_importances, attributes), reverse=True)","4d367bdd":"final_model = grid_search.best_estimator_\n\nX_test = strat_test_set.drop(\"median_house_value\", axis=1)\ny_test = strat_test_set[\"median_house_value\"].copy()\n\nX_test_prepared = full_pipeline.transform(X_test)\nfinal_predictions = final_model.predict(X_test_prepared)\n\nfinal_mse = mean_squared_error(y_test, final_predictions)\nfinal_rmse = np.sqrt(final_mse)","82091ea0":"final_rmse","1b1beafc":"from scipy import stats\n\nconfidence = 0.95\nsquared_errors = (final_predictions - y_test) ** 2\nnp.sqrt(stats.t.interval(confidence, len(squared_errors) - 1,\n                         loc=squared_errors.mean(),\n                         scale=stats.sem(squared_errors)))","c1a65913":"m = len(squared_errors)\nmean = squared_errors.mean()\ntscore = stats.t.ppf((1 + confidence) \/ 2, df=m - 1)\ntmargin = tscore * squared_errors.std(ddof=1) \/ np.sqrt(m)\nnp.sqrt(mean - tmargin), np.sqrt(mean + tmargin)","19692fc1":"zscore = stats.norm.ppf((1 + confidence) \/ 2)\nzmargin = zscore * squared_errors.std(ddof=1) \/ np.sqrt(m)\nnp.sqrt(mean - zmargin), np.sqrt(mean + zmargin)","bcf2c9ff":"Compare against the actual values:","4276e192":"Note that I hard coded the indices (3, 4, 5, 6) for concision and clarity in the book, but it would be much cleaner to get them dynamically, like this:","9e165287":"Alternatively, you can set `sparse=False` when creating the `OneHotEncoder`:","4f2e4bd6":"Now let's join all these components into a big pipeline that will preprocess both the numerical and the categorical features:","50b5f936":"# Step -3: Selecting a performance measure\n\nA typic measure for regression problem is root mean squared error (RMSE). It measures the standard deviation of the errors the system makes in its predictions. If there are many outliers we might also consider mean absolute error although RMSE is preferred for regression. We will be using RMSE here as there is not much outliers. \n\nMathematical formula for RMSE - \n\n![1_lqDsPkfXPGen32Uem1PTNg.png](attachment:13d4bce8-3959-4277-b61e-03cb9a309158.png)","a12cdbc4":"Also, `housing_extra_attribs` is a NumPy array, we've lost the column names (unfortunately, that's a problem with Scikit-Learn). To recover a `DataFrame`, you could run this:","cfc2117c":"Check that this is the same as manually computing the median of each attribute:","5e0e2884":"**Thanks a lot for joining and completing Day -1 of the workshop! If you have any questions you can ask me now or later in google classroom. To learn more about data science and machine learning, you can also subscribe to my Youtube channel, where I try to make tutorials related to ML, DL, DS, etc**\n\nLink to my youtube channel - \n\nhttps:\/\/www.youtube.com\/channel\/UCr3EvjAKPzq1CaQVTyJNtJg\n\n>Your subscription will encourage me to make more contents related to machine learning and data science!\nThanks a lot for your presence :)\n","df93f3c7":"# Step - 8: Select and train a model ","1b192917":"We could compute the interval manually like this:","96575c50":"By default, the `OneHotEncoder` class returns a sparse array, but we can convert it to a dense array if needed by calling the `toarray()` method:","6b2e6c81":"# Step - 4: Check your Assumptions\n\nIt is better to check all the assumptions considered so far again. Is it really a supervised learning task? Is it a regression task ? Is RMSE a perfect measure of performance? It is a good practice to ask this question about the problem formulation again. If everything is alright we can dive straight into the code! ","9e7e555b":"If you want an implementation that supports any hash function and is compatible with both Python 2 and Python 3, here is one:","b3a9fb9f":"The implementation of `test_set_check()` above works fine in both Python 2 and Python 3. In earlier releases, the following implementation was proposed, which supported any hash function, but was much slower and did not support Python 2:","c3923e92":"The argument `sharex=False` fixes a display bug (the x-axis values and legend were not displayed). This is a temporary fix (see: https:\/\/github.com\/pandas-dev\/pandas\/issues\/10611 ). Thanks to Wilmer Arellano for pointing it out.","87734121":"The result is the same as with the `ColumnTransformer`:","35b4bca3":"We can compute a 95% confidence interval for the test RMSE:","5cb50b2d":"# Step - 1: Gain knowledge about the data","30a484f3":"For reference, here is the old solution based on a `DataFrameSelector` transformer (to just select a subset of the Pandas `DataFrame` columns), and a `FeatureUnion`:","3c525f23":"Transform the training set:","2d30ce00":"Let's look at the score of each hyperparameter combination tested during the grid search:","bfe140b7":"Alternatively, we could use a z-scores rather than t-scores:","2b14f0d3":"# Step - 7: Prepare the data for Machine Learning algorithms","2f8db7fe":"Now let's build a pipeline for preprocessing the numerical attributes:","57ef7cf0":"*Let's start looking at the big picture*\n\n**Our task is to build a model of housing prices in California using the California census data from 1990. This data has metrics\/features such as the population, median income, median housing price, and so on for each block group in California. Block groups are the smallest geographical unit for which the US Census Bureau publishes sample data (a block group typically has a population of 600 to 3,000 people). We will just call them districts for short. Your model should learn from this data and be able to predict the median housing price in any district, given all the other metrics.**","d4a227bc":"Let's create a custom transformer to add extra attributes:","36ff52af":"The best hyperparameter combination found:","e0a33b54":"Now let's preprocess the categorical input feature, `ocean_proximity`:","00d24673":"![download.jpeg](attachment:4231f7a0-6de7-4b20-9fdc-ada4b7f747e1.jpeg)","8b7181f4":"# Assignment datasets link - \n\nhttps:\/\/www.kaggle.com\/adityadesai13\/used-car-dataset-ford-and-mercedes\n\nhttps:\/\/www.kaggle.com\/anmolkumar\/house-price-prediction-challenge\n\nhttps:\/\/www.kaggle.com\/isaienkov\/nba2k20-player-dataset\n\nhttps:\/\/www.kaggle.com\/neuromusic\/avocado-prices\n\nhttps:\/\/www.kaggle.com\/zohaib30\/streeteasy-dataset\n\n","24ad3b1f":"# Step - 6: Discover and visualize the data to gain insights","8d637292":"**Note**: since Scikit-Learn 0.22, you can get the RMSE directly by calling the `mean_squared_error()` function with `squared=False`.","3d62454f":"Remove the text attribute because median can only be calculated on numerical attributes:","57f9c56c":"# Step - 2: Frame the problem\n\n**Is it a supervised, unsupervised or RL problem? Is it a classification or regression or clustering task??**\n\nIt is obviously a supervised regression task as labels are given, also a continuous value will be predicted. It is also a multi-variate regression task since multiple features are used for prediction.","e7327448":"**Note**: we specify `n_estimators=100` to be future-proof since the default value is going to change to 100 in Scikit-Learn 0.22 (for simplicity, this is not shown in the book).","d7279d0b":"# Step - 9: Fine-tune your model","77f11b9e":"# Step - 5: Data Exploration\nTo get idea about the variables and how the data is structured and organized."}}