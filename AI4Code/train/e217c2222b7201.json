{"cell_type":{"a2ebb07b":"code","ebf3703b":"code","dccd136a":"code","ebddc2d0":"code","75cd6015":"code","3584f64e":"code","35277ef9":"code","5ef8ad18":"code","4c3ec62c":"code","84b705de":"code","78d48892":"code","aa75758b":"code","1c0a8606":"code","f22275c9":"code","98dd53d9":"code","88dbcca8":"code","1c2d1742":"code","8e5bc21d":"markdown","940768fb":"markdown","374ca322":"markdown","bdca0e07":"markdown","5fb04d0e":"markdown","ac6a9171":"markdown","590eaef5":"markdown","452dc095":"markdown","8d74af3d":"markdown","5a14cba6":"markdown","123153ef":"markdown","bf30c321":"markdown","18779dbd":"markdown","06988965":"markdown","8b7e8606":"markdown"},"source":{"a2ebb07b":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import mean_absolute_error as mae\nfrom sklearn.metrics import mean_squared_error as mse\nfrom sklearn.metrics import r2_score\n\nimport xgboost as xgb\nfrom xgboost import XGBRegressor\n\nimport pickle","ebf3703b":"for dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        file = os.path.join(dirname, filename)","dccd136a":"# reading the dataset using pandas and printing the head values\n\ndata = pd.read_csv(file)\ndata.head()","ebddc2d0":"data.info()","75cd6015":"data.describe()","3584f64e":"data.isna().sum() ## checking for null data","35277ef9":"plt.figure(figsize=(10,6))\n\nheatmap = sns.heatmap(data.corr(), annot=True, vmin=-1, vmax=1)\nheatmap.set_title('Correlation heatmap', pad=10, fontdict={'fontsize':12})\n\nplt.show()","5ef8ad18":"X = data[data.columns[data.columns!='Strength']].values \ny = data['Strength']","4c3ec62c":"# normalize the dataset\nprint(f'X mean: {X.mean()}')\nprint(f'X std: {X.std()}')\n\nX_normalized = (X - X.mean())\/X.std()\nX_normalized","84b705de":"## split the dataset\nX_train, X_test, y_train, y_test = train_test_split(X_normalized, y, \n                                                    test_size = 0.2, \n                                                    shuffle=True, \n                                                    random_state=128)","78d48892":"print(f'Train shape: X = {X_train.shape} ; y={y_train.shape}')\nprint(f'Test shape: X = {X_test.shape} ; y={y_test.shape}')","aa75758b":"#Prepare and train the model\n\nmodel = XGBRegressor(n_jobs=4)\n\n## grid search to find the best model parameter\n\nparam_grid = {\n        'n_estimators': [50, 100, 500],\n        'max_depth': [2, 4, 6, 8, 10],\n        'gamma': [0.001, 0.01],\n        'learning_rate': [0.01, 0.1, 0.3],\n        'booster': ['gbtree']\n    }\n\ngrid_search_model = GridSearchCV(model, param_grid=param_grid, cv=5, return_train_score=True)\n\ngrid_search_model.fit(X_train, y_train)\n\nprint(f'Best Score: {grid_search_model.best_score_}')\nprint(f'Best Param: {grid_search_model.best_params_}')","1c0a8606":"## fitting the best model\n\nbest_model = grid_search_model.best_estimator_\n\nbest_model.fit(X_train, y_train)\n\nprint(f'Train Score: {best_model.score(X_train, y_train)}')\nprint(f'Test Score: {best_model.score(X_test, y_test)}')","f22275c9":"prediction = best_model.predict(X_test)","98dd53d9":"print(f'Mean Absolute Error (MAE): {mae(y_test, prediction)}')\nprint(f'Mean Squared Error (MSE): {mse(y_test, prediction)}')\nprint(f'RMSE: {mse(y_test, prediction)**(1\/2)}')\nprint(f'R2 Score: {r2_score(y_test, prediction)}')","88dbcca8":"xgb.plot_importance(best_model)","1c2d1742":"plt.figure(figsize=(15,6))\n\nx_ax = range(len(y_test))\nplt.plot(x_ax, y_test, label=\"original\")\nplt.plot(x_ax, prediction, label=\"predicted\")\nplt.title(\"Concrete Strength prediction graph\")\nplt.legend()\nplt.show()","8e5bc21d":"6. Plotting the Concrete Strength Prediction Graph based on the original and predicted data","940768fb":"3. Describe and understand the data. The task her is to identify missing informations or any missing data.","374ca322":"The darker area of the correlation map shows higher corelation between the attributes.","bdca0e07":"The data seems to be pretty accurate in terms of the rows.","5fb04d0e":"In order to maintain the scaling between the attributes, normalizing the dataset is important","ac6a9171":"We will be using Grid Search to find the best model across multiple parameters selection.","590eaef5":"4. Splitting and Preparing the Dataset for training","452dc095":"Plotting the feature importance plot","8d74af3d":"# Concrete Compressive Strength Prediction Notebook\n\nThis notebook is for training and predicting Concrete Compressive Strength. The training was done on a 80-20 train-test split using XGBoost.","5a14cba6":"2. Read the dataset localed in the kaggle datastore","123153ef":"The data is pretty clean with no NULL records. So no need to do a lot of cleanup","bf30c321":"Now plotting the data and understanding the correlation between the attributes of the data.","18779dbd":"We need to predict \"Stength\" of the concrete. Hence the prediction value becomes \"Strength\" which we are assigned to variable \"y\". Rest of the feature columns are assigned to \"X\"","06988965":"1. Importing all the necessary libraries","8b7e8606":"5. Training the data using XGBoost"}}