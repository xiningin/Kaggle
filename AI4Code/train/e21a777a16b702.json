{"cell_type":{"6d899c67":"code","827200f7":"code","a3e4af84":"code","36045fad":"code","267f5321":"code","5d718212":"code","8970704e":"code","dda6e58b":"code","d3f2427c":"code","8ccdb247":"code","5e829993":"code","866ffd43":"code","65747419":"code","e53455f3":"code","e5d61195":"code","0b2908c0":"code","921a684f":"code","80fe1d7f":"code","8eb25672":"code","9abe3d09":"code","6faab18c":"code","1011979a":"code","982b873c":"code","8787f79b":"code","72384c4f":"code","10370384":"code","b85767ed":"code","e0e3b630":"code","86782241":"markdown"},"source":{"6d899c67":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt \nimport seaborn as sns \nfrom scipy.stats import skew, norm \nimport plotly.express as px\nfrom warnings import filterwarnings as filt\n\nfilt('ignore')\nplt.rcParams['figure.figsize'] = (12,6)\nplt.style.use('seaborn-darkgrid')\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","827200f7":"df = pd.read_csv('\/kaggle\/input\/breast-cancer-wisconsin-data\/data.csv')\ndf = df.drop(['id','Unnamed: 32'], axis = 1)\ndf.head()","a3e4af84":"df.shape","36045fad":"df.isnull().values.sum()","267f5321":"sns.countplot(df.diagnosis)","5d718212":"df.nunique()","8970704e":"df['diagnosis'] = df.diagnosis.apply(lambda x : 0 if x == 'B' else 1)","dda6e58b":"import eli5\nfrom eli5.sklearn import PermutationImportance\nfrom pdpbox import pdp\nimport shap\nfrom sklearn.ensemble import RandomForestClassifier as rfc\nfrom sklearn.feature_selection import mutual_info_classif","d3f2427c":"def permImp(x, y):\n    model = rfc().fit(x, y)\n    perm = PermutationImportance(model).fit(x, y)\n    return eli5.show_weights(perm , feature_names = x.columns.tolist())\n\ndef isolate(x, y, col):\n    model = rfc().fit(x, y)\n    pdp_dist = pdp.pdp_isolate(model, dataset = x, model_features = x.columns, feature = col)\n    return pdp.pdp_plot(pdp_dist, feature_name = col)\n\ndef forceplot(x, y, n_class = 0):\n    model = rfc().fit(x, y)\n    explainer = shap.TreeExplainer(model)\n    shap_value = explainer.shap_values[n_class]\n    expected_value = explainer.expected_value[n_class]\n    return shap.force_plot(expected_value, shap_value, feature_names = x.columns)\n\ndef plot_mi(score):\n    score = score.sort_values('mi_score', ascending = True)\n    return plt.barh(score.index, score.mi_score)\n\ndef mi_score(x, y):\n    score = pd.DataFrame(mutual_info_classif(x, y, discrete_features = False), index = x.columns, columns = ['mi_score'])\n    plot_mi(score)\n    return score.sort_values('mi_score', ascending = False)","8ccdb247":"from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, StratifiedKFold","5e829993":"x = df.drop(['diagnosis'], axis = 1)\ny = df.diagnosis\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 123, stratify = y)\nx_train.shape, x_test.shape, y_train.shape, y_test.shape","866ffd43":"permImp(x_train, y_train)","65747419":"sns.heatmap(df.corr())","e53455f3":"df.corrwith(df.diagnosis).sort_values(ascending = False).head(10)","e5d61195":"sns.scatterplot(data = df, x = 'concave points_worst', y = 'texture_worst', hue = 'diagnosis')","0b2908c0":"isolate(x_train, y_train, 'concave points_worst')","921a684f":"x_train.describe()","80fe1d7f":"mscore = mi_score(x_train, y_train)","8eb25672":"from sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom xgboost import XGBRFClassifier\nimport xgboost as xgb\nfrom lightgbm import LGBMClassifier\nfrom sklearn.linear_model import LogisticRegression\n\nfrom sklearn.model_selection import cross_validate, KFold\n\nfrom sklearn.metrics import classification_report, roc_auc_score, confusion_matrix\nfrom sklearn.pipeline import Pipeline\n\nfrom sklearn.preprocessing import StandardScaler , RobustScaler, MinMaxScaler","9abe3d09":"def best_model(x, y):\n    xgb.set_config(verbosity=0)\n    models = [LogisticRegression(), SVC(), KNeighborsClassifier(), GaussianNB(), rfc(), XGBRFClassifier(), LGBMClassifier()]\n    names = ['logistic regg','svm', 'knn', 'naive bayes', 'random forest', 'xgboost', 'lightgb']\n    scores = [[] for _ in range(4)]\n    for model in models:\n        for idx, scaler in enumerate([None, StandardScaler(), RobustScaler(), MinMaxScaler()]):\n            if scaler:\n                model = Pipeline(steps = [('scaler', scaler), ('model', model)])\n            #cv = StratifiedKFold(5, shuffle = True, random_state = 123)\n            cv = KFold(5, shuffle = True, random_state = 123)\n            score = cross_validate(model, X = x, y = y, cv = cv, scoring = 'f1')['test_score'].mean()\n            scores[idx].append(score)\n    return pd.DataFrame(scores, columns = names, index = ['None', 'std', 'robust', 'minmax']).T\n\ndef clf_report(yt, pred):\n    print()\n    print(classification_report(yt,  pred))\n    print()\n    \ndef get_score(xt, yt, xtest, ytest, model, scaler = None):\n    if scaler:\n        model = Pipeline(steps = [('scaler', scaler), ('model', model)])\n    model.fit(xt, yt)\n    pred = model.predict(xtest)\n    print(' Report '.center(60,'='))\n    print()\n    print(f\"training score  :===>  {model.score(xt, yt)}\")\n    print(f\"testing score   :===>  {model.score(xtest, ytest)}\")\n    clf_report(ytest, pred)\n    sns.heatmap(confusion_matrix(ytest, pred), fmt = '.1f', annot = True)\n    \n    \ndef gridcv(xt, yt, model, params, scaler = None):\n    if scaler:\n        model = Pipeline(steps = [('scaler', scaler), ('model', model)])\n    cv = KFold(5, shuffle = True, random_state = 123)\n    clf = GridSearchCV(model, param_grid = params, cv = cv, scoring = 'f1', return_train_score = True, verbose = 1)\n    clf.fit(xt, yt)\n    res = pd.DataFrame(clf.cv_results_).sort_values('mean_test_score', ascending = False)\n    return clf.best_estimator_, clf.best_params_, res[['mean_train_score','mean_test_score','params']]","6faab18c":"best_model(x_train, y_train)","1011979a":"params = {\n    'model__C' : [1, 10, 50, 100, 500, 1000],\n    'model__solver' : ['lbfgs', 'liblinear'],\n    'model__class_weight' : [None, 'balanced'],\n    'model__max_iter' : [100,1000]\n}\nclf, best_param, results = gridcv(x_train, y_train, LogisticRegression(), params, scaler = StandardScaler())","982b873c":"results.head()","8787f79b":"sns.lineplot(results.index, results.mean_train_score, color = 'blue')\nsns.lineplot(results.index, results.mean_test_score, color = 'red')\nplt.legend(['mean_train_score', 'mean_test_score'])","72384c4f":"best_param","10370384":"get_score(x_train, y_train, x_test, y_test, LogisticRegression(C = 0.1), scaler = StandardScaler())","b85767ed":"skewness = pd.DataFrame(np.abs(skew(df)), columns = ['skew_score'], index = df.columns).sort_values('skew_score', ascending = True)\nplt.barh(skewness.index, skewness.skew_score)\nplt.title('skewness score')","e0e3b630":"high_skewness = skewness[skewness.skew_score > 2].index\nfig, ax = plt.subplots(len(high_skewness), 2, figsize = (16, 10))\nfig.tight_layout()\nfor ind,col in enumerate(high_skewness):\n    sns.distplot(df[col], ax = ax[ind, 0])\n    sns.boxplot(df[col], ax = ax[ind, 1])","86782241":"* there are lot of features with extreme outliers, even the outliers are important for this dataset so we'll keep them\n* hence final F1-score is 98% "}}