{"cell_type":{"d33918b2":"code","e4be408b":"code","423ce110":"code","f7f5424d":"code","a3a3bda7":"code","c388ce8e":"code","e0d24b90":"code","e68b6381":"code","c536607e":"code","d68c4d4c":"code","310737a5":"code","1bacd064":"code","fc5752a6":"code","216d55b7":"code","1c11f001":"code","42e9ea6c":"code","2aa48200":"code","5e40af1c":"code","477607ab":"code","5a07ca73":"code","7cd7867b":"code","67aa6e25":"code","9ffa0e26":"code","34c8f23e":"code","a3d186b8":"code","187d0b4c":"code","a1c1b696":"code","de6ec7ab":"code","f1e92265":"code","bf7b794d":"code","ca20cbf9":"code","f562d4b7":"code","96a1ff67":"code","87bfc9f9":"code","6105e0ff":"code","b7981f18":"code","a59369ab":"code","924b5378":"code","cb1ad38f":"code","f520b88f":"code","23cc0b54":"code","42eac36b":"code","b569a116":"code","df4fb1f0":"code","fff68a7a":"markdown","211d5a46":"markdown","ab7f13d1":"markdown","01911229":"markdown","e34928c9":"markdown","4727543c":"markdown","43696924":"markdown","b492b8c6":"markdown","cfd66449":"markdown","052a5d2b":"markdown"},"source":{"d33918b2":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","e4be408b":"train=pd.read_csv('\/kaggle\/input\/tabular-playground-series-jan-2021\/train.csv')\ntest=pd.read_csv('\/kaggle\/input\/tabular-playground-series-jan-2021\/test.csv')","423ce110":"train.tail()","f7f5424d":"test.tail()","a3a3bda7":"print(f'Shape of the train dataframe is {train.shape}')\nprint(f'Shape of the test dataframe is {test.shape}')","c388ce8e":"train_nulls=pd.DataFrame(np.c_[train.isna().sum()],columns=['Num_of_Nulls'],index=train.isna().sum().index)\ntrain_nulls","e0d24b90":"test_nulls=pd.DataFrame(np.c_[test.isna().sum()],columns=['Num_of_Nulls'],index=test.isna().sum().index)\ntest_nulls","e68b6381":"train_dtypes=list(train.dtypes)\ntest_dtypes=list(test.dtypes)\nprint(f'Datatypes in train are {train_dtypes}')\nprint(f'Datatypes in test are {test_dtypes}')","c536607e":"train_cols=list(train.columns)\ntest_cols=list(test.columns)\nprint(f'Train columns are {train_cols}')\nprint(f'Test columns are {test_cols}')","d68c4d4c":"import matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')\n%matplotlib inline","310737a5":"train.describe()","1bacd064":"plt.figure(figsize=(10,10))\nimp_cols=train_cols[1:15]\ntarget_cols=train_cols[15]\ncorrl_matrix=train[imp_cols].corr()\nsns.heatmap(corrl_matrix,cbar=True);","fc5752a6":"corrl_features=[]\nfor i in range(len(corrl_matrix)):\n    for j in range(i):\n        if abs(corrl_matrix.iloc[i,j])>0.8:\n            col_name=corrl_matrix.columns[i]\n            corrl_features.append(col_name)\nprint(corrl_features)","216d55b7":"train=train.drop(columns=corrl_features,axis=1)\ntest=test.drop(columns=corrl_features,axis=1)\nprint(f'After dropping correlated features shape of train is {train.shape}')\nprint(f'After dropping correlated features shape of test is {test.shape}')","1c11f001":"print(f\"The number of duplicate rows in train dataset are {train.duplicated().sum()}\")\nprint(f\"The number of duplicate rows in test dataset are {test.duplicated().sum()}\")","42e9ea6c":"sns.distplot(train['target']);\nplt.xticks(range(0,10));","2aa48200":"imp_cols=[col for col in train.columns if col.startswith('cont')]\nprint(f\"Length of important columns in train dataset are : {len(imp_cols)}\")","5e40af1c":"plt.figure(figsize=(90,45))\nfor i in range(1,len(imp_cols)+1):\n    sns.distplot(train[imp_cols[i-1]],ax=plt.subplot(8,2,i))\n    plt.title(f'{imp_cols[i-1]}',fontsize=10)\nplt.savefig('.\/train.png')","477607ab":"import copy\ntrain_copy=copy.deepcopy(train)","5a07ca73":"train_copy_imp_cols=imp_cols+['target']\ntrain_copy_imp_cols","7cd7867b":"for i in train_copy_imp_cols:\n    train_copy[i]=train_copy[i].apply(lambda x:np.log1p(x))","67aa6e25":"train_copy.head()","9ffa0e26":"plt.figure(figsize=(90,45))\nfor i in range(1,len(train_copy_imp_cols)+1):\n    sns.distplot(train_copy[train_copy_imp_cols[i-1]],ax=plt.subplot(8,2,i))\n    plt.title(f'{train_copy_imp_cols[i-1]}',fontsize=10)\nplt.savefig('.\/train_copy.png')","34c8f23e":"columns=[col for col in train_copy.columns if col not in ['id','target']]\nX=train_copy[columns]\ny=train_copy['target']","a3d186b8":"print(X.shape)\nprint(y.shape)","187d0b4c":"from sklearn.linear_model import LinearRegression\nmodel=LinearRegression()\nmodel.fit(X,y)\nprint(f'Models score is {model.score(X,y)}')","a1c1b696":"test.head()","de6ec7ab":"for col in columns:\n    test[col]=test[col].apply(lambda x:np.log1p(x))","f1e92265":"X_test=test.iloc[:,1:]","bf7b794d":"predicted_values=model.predict(X_test)","ca20cbf9":"submission_df=pd.DataFrame(np.c_[test['id'],predicted_values],columns=['id','target'])\nsubmission_df['id']=submission_df['id'].astype('int')\nsubmission_df.shape","f562d4b7":"submission_df.dtypes","96a1ff67":"submission_df.to_csv('.\/submission.csv',index=False)","87bfc9f9":"from sklearn.preprocessing import Normalizer\nnorm=Normalizer()\nX=norm.fit_transform(X)","6105e0ff":"import tensorflow as tf\nfrom tensorflow.keras.models import Model,Sequential\nfrom tensorflow.keras.layers import Dense,Input\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras import backend as K\n","b7981f18":"input_layer = Input(shape=(13,),dtype=\"float64\",name=\"input_layer\")\ndense_layer1= Dense(units=128,activation='relu',kernel_initializer='he_normal',name='dense_layer_1')(input_layer)\ndense_layer2= Dense(units=64,activation='relu',kernel_initializer='he_normal',name='dense_layer_2')(dense_layer1)\ndense_layer3= Dense(units=32,activation='relu',kernel_initializer='he_normal',name='dense_layer_3')(dense_layer2)\ndense_layer4= Dense(units=16,activation='relu',kernel_initializer='he_normal',name='dense_layer_4')(dense_layer3)\ndense_layer5= Dense(units=8,activation='relu',kernel_initializer='he_normal',name='dense_layer_5')(dense_layer4)\noutput_layer= Dense(units=1,activation='linear',name='output_layer')(dense_layer5)\nmodel=Model(inputs=input_layer,outputs=output_layer)\nmodel.summary()","a59369ab":"def root_mean_squared_error(y_true, y_pred):\n        return K.sqrt(K.mean(K.square(y_pred - y_true))) \n","924b5378":"model.compile(optimizer='sgd',loss=root_mean_squared_error,metrics=[tf.keras.metrics.RootMeanSquaredError()])","cb1ad38f":"model.fit(X,y,epochs=30,verbose=1)","f520b88f":"X_test=norm.transform(X_test)","23cc0b54":"predicted_values=model.predict(X_test)","42eac36b":"submission_df=pd.DataFrame(np.c_[test['id'],predicted_values],columns=['id','target'])\nsubmission_df['id']=submission_df['id'].astype('int')\nsubmission_df.shape","b569a116":"submission_df['target']=submission_df['target'].apply(lambda x:np.exp(x)-1)","df4fb1f0":"submission_df.to_csv('submission.csv',index=False)","fff68a7a":"* Lets fit it on train copy dataset.","211d5a46":"# EDA on Train columns.","ab7f13d1":"## All  the datatypes are float64 only","01911229":"# Applying same log transformations on test dataset and lets predict the score on test datasets.","e34928c9":"## Since most of the fields are having long tails have applied log transformations on those fields.","4727543c":"# Dropping this correlated features from Train and Test datasets","43696924":"## No need to handle null values in both train and test datasets.","b492b8c6":"## Check for null values in the train dataframe.","cfd66449":"# Lets build the model using simple linear regression and check hows the performance would be without any hyper parameter tuning.","052a5d2b":"## Check for duplicated rows in Train and Test Datasets."}}