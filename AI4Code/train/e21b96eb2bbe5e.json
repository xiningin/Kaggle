{"cell_type":{"18a5f4ae":"code","3023f105":"code","affc0783":"code","f43b45f9":"code","e62fe9ee":"code","8153dc32":"code","3d3639a9":"code","aae18e25":"code","9ef55465":"code","d94d56c7":"code","da40ab7a":"code","74a987b8":"code","1e028846":"code","47b92230":"code","31554516":"code","a3e8565d":"code","b21c6afb":"markdown","33c23614":"markdown","30e0a6f2":"markdown","2c0ca600":"markdown","1cf0deb0":"markdown","26626fd2":"markdown","2ed51312":"markdown","40f2c54d":"markdown","dfb8b955":"markdown","2bfa8453":"markdown","264f6940":"markdown","f79343cd":"markdown","1fa602f0":"markdown"},"source":{"18a5f4ae":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","3023f105":"import matplotlib.pyplot as plt # this is used for the plot the graph \nimport seaborn as sns # used for plot interactive graph.\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import train_test_split \nfrom sklearn.naive_bayes import GaussianNB\nimport pandas_profiling as pp\nfrom sklearn.metrics import accuracy_score\n\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\n\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Activation, Dropout, Flatten, Dense\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nimport warnings","affc0783":"nRowsRead = 1000 # specify 'None' if want to read whole file\ndata = pd.read_csv('..\/input\/cusersmarildownloadsgermancsv\/german.csv', delimiter=';', encoding = \"ISO-8859-2\", nrows = nRowsRead)\ndata.dataframeName = 'german.csv'\nnRow, nCol = data.shape\nprint(f'There are {nRow} rows and {nCol} columns')\ndata.head()","f43b45f9":"y = data[\"Creditability\"]\nX = data.drop('Creditability',axis=1)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state = 3)","e62fe9ee":"tr = DecisionTreeClassifier(criterion = 'entropy',random_state=1,max_depth = 5)\ntr.fit(X_train, y_train)\ntr_predicted = tr.predict(X_test)\ntr_acc_score = accuracy_score(y_test, tr_predicted)\nprint(\"Accuracy of DecisionTreeClassifier:\",tr_acc_score*100,'\\n')","8153dc32":"rf = RandomForestClassifier(n_estimators=60, random_state=12,max_depth=5)\nrf.fit(X_train,y_train)\nrf_predicted = rf.predict(X_test)\nrf_acc_score = accuracy_score(y_test, rf_predicted)\nprint(\"Accuracy of Random Forest:\",rf_acc_score*100,'\\n')","3d3639a9":"KneiCl = KNeighborsClassifier(n_neighbors=7)\nKneiCl.fit(X_train, y_train)\nKneiCl_predicted = KneiCl.predict(X_test)\nKneiCl_acc_score = accuracy_score(y_test, KneiCl_predicted)\nprint(\"Accuracy of K-NeighborsClassifier:\",KneiCl_acc_score*100,'\\n')","aae18e25":"from xgboost import XGBClassifier\nxgb = XGBClassifier(learning_rate=0.01, n_estimators=30, max_depth=15,gamma=0.6, subsample=0.52,colsample_bytree=0.6,seed=27, \n                    reg_lambda=2, booster='dart', colsample_bylevel=0.6, colsample_bynode=0.5)\nxgb.fit(X_train, y_train)\nxgb_predicted = xgb.predict(X_test)\nxgb_acc_score = accuracy_score(y_test, xgb_predicted)\nprint(\"Accuracy of Extreme Gradient Boost:\",xgb_acc_score*100,'\\n')","9ef55465":"from catboost import CatBoostClassifier\nclf = CatBoostClassifier(\n    iterations=30, \n    learning_rate=0.3,depth = 3 )\nclf.fit(X_train, y_train,  plot=True)\npredicted = clf.predict(X_test)\npredicted_proba = clf.predict(X_test)\nprint(\"Accuracy is: \"+ str(clf.score(X_test,y_test)))","d94d56c7":"nb = GaussianNB()\nnb.fit(X_train,y_train)\nnbpred = nb.predict(X_test)\nnb_acc_score = accuracy_score(y_test, nbpred)\nprint(\"Accuracy of Naive Bayes model:\",nb_acc_score*100,'\\n')","da40ab7a":"ext = ExtraTreesClassifier()\next.fit(X_train , y_train)\nextpred = ext.predict(X_test)\next_acc_score = accuracy_score(y_test , extpred)\nprint(\"Accuracy of ExtraTreesClassifier model:\" , ext_acc_score * 100,'\\n')","74a987b8":"ada = AdaBoostClassifier()\nada.fit(X_train, y_train)\nadapred = ada.predict(X_test)\nada_acc_score = accuracy_score(y_test, adapred)\nprint(\"Accuracy of AdaBoostClassifier model : \", ada_acc_score * 100,'\\n')","1e028846":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)","47b92230":"model = Sequential()\n\nmodel.add(Dense(128,activation=\"relu\",input_dim=20))\nmodel.add(Dropout(0.4))\nmodel.add(Dense(128,activation=\"relu\"))\nmodel.add(Dropout(0.4))\nmodel.add(Dense(1,activation=\"sigmoid\"))\n\nmodel.compile(optimizer=\"adam\",loss=\"binary_crossentropy\",metrics=[\"accuracy\"])","31554516":"hist=model.fit(X_train,y_train,batch_size=60,epochs= 83,validation_data=(X_test,y_test))","a3e8565d":"#Code by Olga Belitskaya https:\/\/www.kaggle.com\/olgabelitskaya\/sequential-data\/comments\nfrom IPython.display import display,HTML\nc1,c2,f1,f2,fs1,fs2=\\\n'#eb3434','#eb3446','Akronim','Smokum',30,15\ndef dhtml(string,fontcolor=c1,font=f1,fontsize=fs1):\n    display(HTML(\"\"\"<style>\n    @import 'https:\/\/fonts.googleapis.com\/css?family=\"\"\"\\\n    +font+\"\"\"&effect=3d-float';<\/style>\n    <h1 class='font-effect-3d-float' style='font-family:\"\"\"+\\\n    font+\"\"\"; color:\"\"\"+fontcolor+\"\"\"; font-size:\"\"\"+\\\n    str(fontsize)+\"\"\"px;'>%s<\/h1>\"\"\"%string))\n    \n    \ndhtml('Thanks to princessa for all the code' )","b21c6afb":"#Artificial Neural Network","33c23614":"#According to that code ExtraTreesClassifier achieved the best \"Accuracy\":\n\nAccuracy of ExtraTreesClassifier model: 80.0 \n\nSince I'm not a DS I can't interpret what those models are showing. I've just run the cells.","30e0a6f2":"#KNeighborsClassifier","2c0ca600":"#Attention snippet below: input dim=20 number I wrote 1 less 21, which is the shape(21) of the data. \n\nOn the original input dim was 13 because data has 14 columns.","1cf0deb0":"#RandomForestClassifier","26626fd2":"#ExtraTreesClassifier","2ed51312":"#AdaBoostClassifier","40f2c54d":"![](https:\/\/www.ovh.com\/blog\/wp-content\/uploads\/2020\/05\/7A7FC4A4-9AEB-4151-90CD-C7B544E3F189-1024x537.png)ovh.com","dfb8b955":"#XGBClassifier","2bfa8453":"#CatBoostClassifier","264f6940":"#DecisionTreeClassifier","f79343cd":"#GaussianNB","1fa602f0":"#Code by princessa https:\/\/www.kaggle.com\/princessa\/heart-disease-prediction-9-models-ann-cat-xgb\/notebook"}}