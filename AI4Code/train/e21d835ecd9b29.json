{"cell_type":{"8bd54d81":"code","e68345dd":"code","cb570971":"code","2a0d05b1":"code","e5ec8808":"code","8a48c1e4":"code","5e56dbdf":"code","047b9648":"code","dddc5ee2":"code","d522281c":"code","b88ad419":"code","6d6f38dc":"code","57e433ac":"code","b0d7a2db":"code","7fc3cf24":"code","05a0a3a4":"code","387d7c43":"code","af8cfd14":"code","5471d11f":"code","f9a25f52":"code","6c5e7013":"code","f1ff3e0d":"code","5fa075a8":"code","26127e6a":"code","dacb5987":"code","5d9d8ab4":"code","7e6b4100":"code","d073054a":"code","21ea3768":"code","f941c9d6":"code","fd4e3595":"code","edf4be1d":"code","350489d2":"code","8570856b":"code","79095fe8":"code","bd206e11":"code","58ab81eb":"code","744bca5f":"code","96a7b4e4":"code","19b61ba6":"code","4adda192":"code","56a47637":"code","0dd6f2ac":"code","6cae70a4":"code","c7aa5b12":"code","cf908526":"code","c74f724b":"code","a90599ed":"code","7bd2bec7":"code","c6531df6":"code","52e5c53e":"code","c6bc8418":"code","f1f616fa":"code","3615478c":"markdown","463ea27f":"markdown","7713b573":"markdown","2d661f8a":"markdown","9fd26ddb":"markdown","fdbeb8f2":"markdown","756a7dc2":"markdown","9ec2d350":"markdown","3088611c":"markdown","1cc31f95":"markdown","068c22af":"markdown","d0c71583":"markdown","6505b83c":"markdown","fdad08f6":"markdown","9f1cfa4e":"markdown","7e223458":"markdown","e530a61b":"markdown","b488b5d0":"markdown","26600794":"markdown","2fc4b432":"markdown"},"source":{"8bd54d81":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport sklearn as skl\n\n%matplotlib inline","e68345dd":"data = pd.read_csv('\/kaggle\/input\/stroke-prediction-dataset\/healthcare-dataset-stroke-data.csv')\ndata.head()","cb570971":"data.info()","2a0d05b1":"data.describe()","e5ec8808":"def present_categorical(col):    \n    print(data[col].value_counts())\n    data[col].value_counts().plot.bar()","8a48c1e4":"present_categorical('gender')","5e56dbdf":"data = data[data['gender'] != 'Other']","047b9648":"present_categorical('ever_married')","dddc5ee2":"present_categorical('work_type')","d522281c":"present_categorical('Residence_type')","b88ad419":"present_categorical('smoking_status')","6d6f38dc":"def cat_plot(x):\n    sns.catplot(data=data, x=x, hue='stroke',kind='count')","57e433ac":"cat_plot('gender')","b0d7a2db":"cat_plot('ever_married')","7fc3cf24":"cat_plot('work_type')","05a0a3a4":"cat_plot('Residence_type')","387d7c43":"cat_plot('smoking_status')","af8cfd14":"from pandas.plotting import scatter_matrix\ncols = data.columns\ncols = cols.drop(['hypertension', 'heart_disease'])\n\n_ = scatter_matrix(data[cols], alpha=0.1, figsize=(14,14), hist_kwds={'bins': 30})","5471d11f":"def plot_face_grid(x):\n    g = sns.FacetGrid(data, col='stroke', height=6)\n    g.map(sns.kdeplot, x, shade=True).add_legend()","f9a25f52":"plot_face_grid('age')","6c5e7013":"plot_face_grid('bmi')","f1ff3e0d":"plot_face_grid('avg_glucose_level')","5fa075a8":"data.dropna(inplace=True)\ndata.drop(['id'], axis=1, inplace=True)","26127e6a":"fig, ax = plt.subplots(figsize=(10,5)) \nsns.heatmap(data.corr(), annot=True, ax=ax)","dacb5987":"from sklearn.model_selection import train_test_split\n\ntrain, test = train_test_split(data, train_size=0.8, test_size=0.2, random_state=123)\nval, test = train_test_split(test, train_size=0.5, test_size=0.5, random_state=123)\ntrain_y = train['stroke']\ntest_y = test['stroke']\nval_y = val['stroke']\n\ntrain.drop(['stroke'], axis=1, inplace=True)\ntest.drop(['stroke'], axis=1, inplace=True)\nval.drop(['stroke'], axis=1, inplace=True)","5d9d8ab4":"cat_cols = train.loc[:,data.dtypes == \"object\"].columns\nnum_cols = train.loc[:,data.dtypes != \"object\"].columns","7e6b4100":"from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\n\nnum_pipeline = Pipeline([\n        ('std_scaler', StandardScaler())\n    ])\n\ncat_pipeline = Pipeline([\n        ('one_hot', OneHotEncoder(handle_unknown='ignore'))\n    ])\n\nfull_pipeline = ColumnTransformer([\n        ('num', num_pipeline, num_cols),\n        ('cat', cat_pipeline, cat_cols)\n    ])\n    \n\ntrain = full_pipeline.fit_transform(train, train_y)\ntest = full_pipeline.fit_transform(test)\nval = full_pipeline.fit_transform(val)","d073054a":"train.shape","21ea3768":"from imblearn.over_sampling import SMOTE, BorderlineSMOTE, ADASYN, SVMSMOTE\nfrom imblearn.under_sampling import NearMiss, RandomUnderSampler, AllKNN, NeighbourhoodCleaningRule\n\nequalizers = [\n    SMOTE(),\n    BorderlineSMOTE(),\n    ADASYN(),\n    SVMSMOTE(),\n    NearMiss(),\n    RandomUnderSampler(),\n    AllKNN(),\n    NeighbourhoodCleaningRule()\n]","f941c9d6":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import classification_report, roc_auc_score, confusion_matrix\n\ndef train_and_evaluate(model, train, train_y, test, test_y, eq=None, train_model=True, threashold=0.5):\n    if train_model:\n        model.fit(train, train_y)\n    \n    results = model.predict_proba(test)\n    \n    proba = results[:,1]\n    results = (results[:,1] > threashold).astype(int)\n    \n    print('\/'*80)\n    print(model)\n    if eq != None:\n        print(eq)\n    print()\n    print('confusion_matrix')\n    print(confusion_matrix(test_y, results))\n    print('roc_auc')\n    print(roc_auc_score(test_y, proba))\n    print(classification_report(test_y, results))\n    \n    return proba","fd4e3595":"for eq in equalizers:\n    model = RandomForestClassifier(random_state=1234)\n    train_eq, train_y_eq = eq.fit_resample(train, train_y.ravel())\n    train_and_evaluate(model, train_eq, train_y_eq, test, test_y, eq)","edf4be1d":"eq = RandomUnderSampler()\ntrain, train_y = eq.fit_resample(train, train_y.ravel())\ntrain.shape","350489d2":"from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier, ExtraTreesClassifier\nfrom sklearn.linear_model import LogisticRegression, RidgeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import LinearSVC, SVC\nfrom xgboost.sklearn import XGBClassifier\nfrom lightgbm.sklearn import LGBMClassifier\n\n\n\nfrom scipy.stats import uniform\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV\nfrom sklearn.model_selection import cross_validate\n\nnp.random.seed(1234)","8570856b":"models = [\n    (AdaBoostClassifier(), 'AdaBoost'),\n    (RandomForestClassifier(), 'RandomForest'),\n    (ExtraTreesClassifier(), 'ExtraTreesClassifier'),\n    (LogisticRegression(), 'LogisticRegression'),\n    (KNeighborsClassifier(), 'KNeighbors'),\n    (SVC(probability=True), 'SVC'),\n    (XGBClassifier(use_label_encoder=False), 'XGB'),\n    (LGBMClassifier(), 'LGBM')\n]\n\ndef print_scores(scores, model_name):\n    print(model_name)\n    print()\n    print(scores)\n    print(\"mean: {}\".format(scores.mean()))\n    print(\"std: {}\".format(scores.std()))\n    print()\n    print()","79095fe8":"for model, name in models:\n    train_and_evaluate(model, train, train_y, test, test_y)","bd206e11":"scores = []\nscoring = ['roc_auc', 'balanced_accuracy']\nfor model, name in models:\n    score = cross_validate(model, train, train_y, cv=5, scoring=scoring)\n    scores.append((score['test_roc_auc'], name))    ","58ab81eb":"for score, name in scores:\n    print_scores(score, name)","744bca5f":"parameters = [\n    {\n    'C': [0.01, 0.5, 1, 2, 5, 10],\n    'kernel' : ['poly'],\n    'degree' : [2,3],\n    'gamma': ['scale', 'auto'],\n    'coef0': [0.5, 1, 2, 3],\n    'class_weight': ['balanced', None]    \n    },\n    {\n    'C': [0.01, 0.5, 1, 2, 5, 10],\n    'kernel' : ['rbf', 'sigmoid'],\n    'gamma': ['scale', 'auto'],\n    'class_weight': ['balanced', None]    \n    },\n    {\n    'C': [0.01, 0.5, 1, 2, 5, 10],\n    'kernel' : ['linear'],\n    'class_weight': ['balanced', None] \n    }\n]\n\nmodel = SVC(probability=True)\ngrid_search = GridSearchCV(model,\n                           param_grid=parameters,\n                           cv=5,\n                           scoring='roc_auc',\n                           refit='roc_auc',\n                           )\n\nr = grid_search.fit(train, train_y)\nscores = r.cv_results_\nsvc = r.best_estimator_","96a7b4e4":"max(scores['mean_test_score'])","19b61ba6":"for mean_score, params in sorted(list(zip(scores[\"mean_test_score\"], scores[\"params\"])),key = lambda x: x[0]):\n     print(mean_score, params)","4adda192":"parameters = [\n    {\n    'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],\n    'n_neighbors': [1, 3, 5, 7],\n    },\n]\n\nmodel = KNeighborsClassifier()\ngrid_search = GridSearchCV(model,\n                           param_grid=parameters,\n                           cv=5,\n                           scoring='roc_auc',\n                           refit='roc_auc',\n                           )\n\nr = grid_search.fit(train, train_y)\nscores = r.cv_results_\nknn = r.best_estimator_","56a47637":"max(scores['mean_test_score'])","0dd6f2ac":"for mean_score, params in sorted(list(zip(scores[\"mean_test_score\"], scores[\"params\"])),key = lambda x: x[0]):\n     print(mean_score, params)","6cae70a4":"parameters = [\n    {\n    'n_estimators': [10, 50, 100, 200],\n    'criterion': ['gini', 'entropy'],\n    'max_depth': [None, 2, 4, 8],\n    'min_samples_split': [2, 4, 8],\n    'min_samples_leaf': [1, 2, 4],\n    },\n]\n\nmodel = RandomForestClassifier()\ngrid_search = GridSearchCV(model,\n                           param_grid=parameters,\n                           cv=5,\n                           scoring='roc_auc',\n                           refit='roc_auc',\n                           )\n\nr = grid_search.fit(train, train_y)\nscores = r.cv_results_\nforest = r.best_estimator_","c7aa5b12":"max(scores['mean_test_score'])","cf908526":"for mean_score, params in sorted(list(zip(scores[\"mean_test_score\"], scores[\"params\"])),key = lambda x: x[0]):\n     print(mean_score, params)","c74f724b":"forest_proba = train_and_evaluate(forest, train, train_y, test, test_y, train_model=False)\nknn_proba = train_and_evaluate(knn, train, train_y, test, test_y, train_model=False)\nsvc_proba = train_and_evaluate(svc, train, train_y, test, test_y, train_model=False)","a90599ed":"logistic_reg = LogisticRegression()\nlogistic_reg.fit(train, train_y)\nlr_proba = train_and_evaluate(logistic_reg, train, train_y, test, test_y, train_model=False)","7bd2bec7":"from sklearn.metrics import f1_score\n\ndef test_threshold(probas, test_y):\n    results = []\n    for i in range(20, 70):\n        result = (probas > i \/ 100).astype(int)\n        results.append((f1_score(test_y, result), i \/ 100))\n    return sorted(results, key=(lambda x : x[0]), reverse=True)","c6531df6":"forest_best_f_score = test_threshold(forest_proba, test_y)[0]\nsvc_best_f_score = test_threshold(svc_proba, test_y)[0]\nknn_best_f_score = test_threshold(knn_proba, test_y)[0]\nlr_best_f_score = test_threshold(lr_proba, test_y)[0]","52e5c53e":"train_and_evaluate(forest, train, train_y, test, test_y, train_model=False, threashold=forest_best_f_score[1])\ntrain_and_evaluate(knn, train, train_y, test, test_y, train_model=False, threashold=knn_best_f_score[1])\ntrain_and_evaluate(svc, train, train_y, test, test_y, train_model=False, threashold=svc_best_f_score[1])\n_ = train_and_evaluate(logistic_reg, train, train_y, test, test_y, train_model=False, threashold=lr_best_f_score[1])\n","c6bc8418":"_ = train_and_evaluate(svc, train, train_y, val, val_y, train_model=False, threashold=svc_best_f_score[1])","f1f616fa":"_ = train_and_evaluate(logistic_reg, train, train_y, val, val_y, train_model=False, threashold=lr_best_f_score[1])","3615478c":"### We can see only 5% of data show patients who had a stroke. It is a clear inbalance which will not allow model to learn properly. To avoid that I will try a couple of methods(undersampling and oversampling) to eliminate the problem.\n### Let's check wich method works the best with RandomForestClassifier","463ea27f":"<img src=\"https:\/\/www.heart.org\/-\/media\/images\/news\/2019\/october-2019\/1017strokeptsd_sc.jpg\" alt=\"drawing\" height=\"600\" width=\"600\"\/>\n\n# **A stroke is a medical condition in which poor blood flow to the brain causes cell death.**\n### There are two main types of stroke: ischemic, due to lack of blood flow, and hemorrhagic, due to bleeding.Both cause parts of the brain to stop functioning properly. Signs and symptoms of a stroke may include an inability to move or feel on one side of the body, problems understanding or speaking, dizziness, or loss of vision to one side.Signs and symptoms often appear soon after the stroke has occurred. If symptoms last less than one or two hours, the stroke is a transient ischemic attack (TIA), also called a mini-stroke. A hemorrhagic stroke may also be associated with a severe headache. The symptoms of a stroke can be permanent. Long-term complications may include pneumonia and loss of bladder control.\n\n### **In this notebook I will analyze dataset to find what factors increase the probability of the storke and create model for automatic classification of this task.**\n\n\n## Please upvote my work if you find it helpful. Happy reading :)","7713b573":"### It seems like logistic regression obtains the best results, and that roc_auc is a good indicator to compare\n### Let's run the same but with cross validation to make sure which models are the best","2d661f8a":"### \"Other\" is present in single record, let's drop it because it seems irrelevant","9fd26ddb":"### avg_glucose_level and age seems to have the most significant impact","fdbeb8f2":"### Now, that we have our models lets find the best threashold for predictions. I will use fscore as a scoring metric","756a7dc2":"### Let's split data into train, test and val set","9ec2d350":"### Let's check the realtionship for numerical features either","3088611c":"### lets go through all categorical cols to see if everything looks correct","1cc31f95":"### Before creating the model lets drop all nan values, data set is small but there are not as many of them lets drop ids too, we won't need them for predictions","068c22af":"# Summary\n\n## The best model turned out to be SVC.\n## Final accuracy is around 85% and model recognize around 72% of strokes. \n\n### The biggest problem with the task is unbalanced and small dataset. It contains only around 200 positive examples. To improve the result the first step would be to collect more data. Assuming bigger dataset is available some data engineering seems to be a good idea. Trying kmeans before classification could work too. I could also try to impute values instead of dropping nan values.\n\n## Please let me know what are your thoughts, it is my fist public notebook. All mistakes found and hints provided are welcome. ","d0c71583":"### The only thing which seems to have a clear relationship with stoke is ever_married column\n### But generaly it is hard to spot anything because of the dataset imbalance\n","6505b83c":"### The proportion of records containing stroke seems to be very low. To train the model we will have to resolve this problem otherwise model will be skew towards non-stroke patients.","fdad08f6":"### Let's see the correlations between features","9f1cfa4e":"### Let's plot scatter matrix but before that to improve readibility we should remove binary columns","7e223458":"### Let's quickly go through couple models and pick 2~3 the best of them to try improve the results with various hyperparameters. We are going to try to maximize roc_auc score","e530a61b":"### As we can see, the best result was obtained by Logistic regression, second by SVC, and third by KNN.\n### Let's take these models and try to improve the score as much as we can. Instead of Logistic regression we will try to imporve 4th model RandomForest because it does not have any parameters to tune. ","b488b5d0":"### Now that we know how categorical data is distributed let's see what is the relationship with stroke","26600794":"## Now let's try 2 the best model on val data to evalute final accuracy and recall","2fc4b432":"### As we can see randomundersampler seems to be working the best(it maximize the recall for stoke) keeping "}}