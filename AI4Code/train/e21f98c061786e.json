{"cell_type":{"4db86980":"code","8575a9b6":"code","617cce6c":"code","177c07a4":"code","fda160e8":"code","655d2f8e":"code","8d155d68":"code","a58aab52":"code","7fc7a55b":"code","a93813b7":"code","78168db9":"code","0f0f5e4d":"code","aae50d83":"code","f45f5d3c":"code","beafe07b":"code","5ededddb":"code","bfc8b511":"code","9c21975b":"code","69a1bc2e":"code","ad0103bb":"code","c4e0f045":"code","319742a2":"code","f061f369":"code","8f92479e":"code","2996672b":"code","15182ebf":"code","38ec670a":"code","a7e225cb":"code","7b7858aa":"code","87b5f963":"code","ae18406f":"code","8b8dcfc9":"code","c5593b63":"code","df2ccb00":"code","63d57566":"code","a41354e9":"code","ca20aadb":"code","55b89550":"code","517d4d97":"code","16a3f542":"code","f73ebc2b":"code","79de82af":"code","23d9a8e0":"code","accbfab3":"code","53ec3518":"code","d46c8ae3":"code","29666ef2":"code","ee4889e7":"code","f15da4c5":"code","95d94560":"markdown","7310865e":"markdown","5379d879":"markdown","7b46c302":"markdown","158d19f9":"markdown","057a5c6e":"markdown","00a75408":"markdown","9b97331b":"markdown","92389444":"markdown","f8c8d0f4":"markdown","3dc89bee":"markdown","c1f18f15":"markdown","c3f9ae93":"markdown","eb0432d2":"markdown","6700f4d5":"markdown","854a97ba":"markdown","16126d89":"markdown","73e0f4a5":"markdown"},"source":{"4db86980":"# Import initial libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns","8575a9b6":"df = pd.read_csv(\"https:\/\/raw.githubusercontent.com\/anilak1978\/customer-churn\/master\/bigml_59c28831336c6604c800002a.csv\")","617cce6c":"df.head()","177c07a4":"# checking for missing values\ndf.isnull().sum().values.sum()","fda160e8":"# for loop to see unique values\nfor column in df.columns.values.tolist():\n    print(column)\n    print(df[column].unique())\n    print(\"\")","655d2f8e":"# check data types\ndf.dtypes","8d155d68":"# update churn data type and boolen values to 0 and 1\ndf[\"churn\"]=df[\"churn\"].astype(\"str\")","a58aab52":"df[\"churn\"]=df[\"churn\"].replace({\"False\":0, \"True\":1})","7fc7a55b":"# look at the brief overview of the data\ndf.info()","a93813b7":"# look at statistical information\ndf.describe()","78168db9":"# group the data to see churn rate by state\ndf_state = df.groupby(\"state\")[\"churn\"].mean().reset_index()","0f0f5e4d":"plt.figure(figsize=(20,5))\nsns.barplot(x=\"state\", y=\"churn\", data=df_state)","aae50d83":"# look at churn rate for all categorical variables\ncategorical_variables = [\"area code\", \"international plan\", \"voice mail plan\", \"state\"]\nfor i in categorical_variables:\n    data=df.groupby(i)[\"churn\"].mean().reset_index()\n    plt.figure(figsize=(20,5))\n    sns.barplot(x=data[i], y=\"churn\", data=data)","f45f5d3c":"# look at the distribution of categorical variables\nfor i in categorical_variables:\n    plt.figure(figsize=(20,5))\n    sns.countplot(x=df[i], data=df)","beafe07b":"plt.figure(figsize=(10,5))\nsns.countplot(x=df[\"churn\"], data=df)","5ededddb":"# Analysing numerical variables\nnumerical_variables=[\"account length\", \"number vmail messages\", \"total day minutes\", \"total day charge\", \"total day calls\", \n                     \"total day charge\", \"total eve minutes\", \"total eve charge\", \"total night minutes\",\n                    \"total intl minutes\", \"total intl calls\",\n                    \"total intl charge\", \"customer service calls\"]","bfc8b511":"# looking at relationship for each numerical variable and churn\nfor i in numerical_variables:\n    plt.figure(figsize=(20,5))\n    sns.regplot(x=df[i], y=\"churn\", data=df)","9c21975b":"# looking at correlation within numerical variables\ncorr=df.corr()\nplt.figure(figsize=(15,10))\nsns.heatmap(corr, annot=True)","69a1bc2e":"# feature selection\nX = df[[\"account length\", \"international plan\", \"total day charge\", \"total night charge\", \"total intl charge\", \"customer service calls\", \"state\"]]","ad0103bb":"# target selection\ny =df[\"churn\"]","c4e0f045":"# review feature set\nX[0:5]","319742a2":"# update state with one hot coding\nX=pd.get_dummies(X, columns=[\"state\"])","f061f369":"# make sure i am using feature set values \nX=X.values","8f92479e":"# preprocess to update str variables to numerical variables\nfrom sklearn import preprocessing\ninternational_plan=preprocessing.LabelEncoder()\ninternational_plan.fit([\"no\", \"yes\"])\nX[:,1] = international_plan.transform(X[:,1])","2996672b":"# create training and testing set\nfrom sklearn.model_selection import train_test_split\nX_trainset, X_testset, y_trainset, y_testset = train_test_split(X, y, test_size=0.2, random_state=3)","15182ebf":"#create model using random forest classifier and fit the training set\nfrom sklearn.ensemble import RandomForestClassifier\nrf_model = RandomForestClassifier(n_estimators=100)\nrf_model.fit(X_trainset, y_trainset)","38ec670a":"#create prediction using the model\nrf_pred = rf_model.predict(X_testset)\nrf_pred[0:5]","a7e225cb":"# Looking at the accuracy score (using two methods)\nfrom sklearn import metrics\nrf_model.score(X_testset, y_testset)\nmetrics.accuracy_score(y_testset, rf_pred)","7b7858aa":"# confusion matrics to find precision and recall\nfrom sklearn.metrics import confusion_matrix\nconfusion_matrix(y_testset, rf_pred)","87b5f963":"# Looking at the precision score\nfrom sklearn.metrics import precision_score\nprecision_score(y_testset, rf_pred)","ae18406f":"# Looking at the recall score\nfrom sklearn.metrics import recall_score\nrecall_score(y_testset, rf_pred)","8b8dcfc9":"# find probability for each prediction\nprob=rf_model.predict_proba(X_testset)[:,1]","c5593b63":"# look at ROC curve, which gives us the false and true positive predictions\nfrom sklearn.metrics import roc_curve\nfpr, tpr, thresholds=roc_curve(y_testset, prob)\nplt.plot(fpr, tpr)","df2ccb00":"# Looking at the area under the curve\nfrom sklearn.metrics import roc_auc_score\nauc=roc_auc_score(y_testset, prob)\nauc","63d57566":"#looking at the f1_score\nfrom sklearn.metrics import f1_score\nf1_score(y_testset, rf_pred)","a41354e9":"#Looking at the best possible estimator\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfrom sklearn.model_selection import GridSearchCV\nparam_grid={'n_estimators': np.arange(10,51)}\nrf_cv=GridSearchCV(RandomForestClassifier(), param_grid)\nrf_cv.fit(X,y)\nrf_cv.best_params_","ca20aadb":"# looking at the best feature score\nrf_cv.best_score_","55b89550":"# looking at the importance of each feature\nimportances=rf_model.feature_importances_","517d4d97":"# visualize to see the feature importance\nindices=np.argsort(importances)[::-1]\nplt.figure(figsize=(20,10))\nplt.bar(range(X.shape[1]), importances[indices])\nplt.show()","16a3f542":"# creating the svm model and fitting training set\n# make sure to update probability to True for proabbility evaluation\nfrom sklearn.svm import SVC\nsvc_model=SVC(probability=True)\nsvc_model.fit(X_trainset, y_trainset)","f73ebc2b":"# creating the svm prediction\nsvc_pred=svc_model.predict(X_testset)\nsvc_pred[0:5]","79de82af":"# look at the accuracy score\nsvc_model.score(X_testset, y_testset)","23d9a8e0":"# Look at the confusion matrix\nfrom sklearn.metrics import confusion_matrix\nconfusion_matrix(y_testset, svc_pred)","accbfab3":"#precision score for svm\nprecision_score(y_testset, svc_pred)","53ec3518":"# recall score for svm\nrecall_score(y_testset, svc_pred)","d46c8ae3":"# probability for each prediction\nprob_2=svc_model.predict_proba(X_testset)[:,1]","29666ef2":"# look at ROC curve\nfpr, tpr, thresholds=roc_curve(y_testset, prob_2)\nplt.plot(fpr, tpr)","ee4889e7":"# area under the curve\nauc=roc_auc_score(y_testset, prob)\nauc","f15da4c5":"# find ideal degree for SVM model\nparam_grid_2={'degree': np.arange(1,50)}\nsvc_cv=GridSearchCV(SVC(), param_grid_2)\nsvc_cv.fit(X,y)\nsvc_cv.best_params_","95d94560":"The model predicts 560 True Negatives, 13 False Positives, 54 False Negatives, 40 True Positives. ","7310865e":"## Model Evaluation Support Vector Machine","5379d879":"The churn rate for area code is around the same, while churn rate for internal plan is much higher and voice mail plan much lower.","7b46c302":"Couple things to note up to this point in the model evaluation phase; it is clear that I can certainly improve the precision , specially true positive count and n_estimator in the RandomForestClassifier parameter.","158d19f9":"Based on the bar chart, we can see that the first 5 features in the created X feature set is important and the rest are not. When we look at the X array, we will see that we can remove the variable state from the feature set.","057a5c6e":"The dataset doesnt require further cleaning. It doesnt have any missing values, data types are correct and it is further ready for exploration.","00a75408":"Both precision and recall score is much lower compare to RandomForestClassifier.","9b97331b":"SVM Accuracy score is lower than RandomForestClassifier accuracy score. ","92389444":"## Model Development - Random Forest\n\nAs it is obvious, we have labeled target (Churn) so I will use a classification to create a prediction model. I can create a model using Random Forest, Decision Tree, Support Vector Machine or Logistic Regression. I will use RandomForest and Support Vector Machine and compare the scores to see which model would be best. \n\nBased on our data exploration, I decided to select account length, international plan, total day charge, total night charge, total int charge, customer service calls and state as the feature set and of course the target (response) variable is Churn.","f8c8d0f4":"## Conclusion\n\nBased on my analysis, the top features that we can use in order to predict if the customer will leave the services of the company are; \"account length\", \"international plan\", \"total day charge\", \"total night charge\", \"total intl charge\", \"customer service calls\". Upon development and evaluation of the two models, modified version of the model that uses Random Forest Classifier can be used to predict the customer churn. When I state modified version, I mean removing the state variable as part of the feature set and updating the n_estimator to 49. The current rate of 89% , precision and racall rates can be approved by tuning the Random Forest Model.","3dc89bee":"## Introduction and Data\n\nCustomer Churn is one of the most important and common business problems. Even though it may not be fun to look at, it most definetly will provide insights on what improvements business can make in order to retain their customers. I have done couple of these analysis up to this point and my intend is to add couple more steps to the evaluation and model tunning process. The main business problem is the classic \"Can we predict which type of customers most likely to leave our services?\" The intend of this analysis is not to go into detail around broken down business objectives and problems but rather outline and practice the process of customer churn analysis and possible machine learning methodologies that I can use.\n\nData set is another common, publicly available and not real data set. The customer attributes are straight forward and self explanatory.","c1f18f15":"Looking at the categorical variables and churn, the data is imbalanced. ","c3f9ae93":"# Customer Churn Analysis and Prediction","eb0432d2":"## Data Collection and Cleaning","6700f4d5":"## Model Evaluation Random Forest\n\nIn order to evaluate the model, I will first look at the accuracy score, however keep in mind, in the data analysis phase we observed that the data is imbalanced so the score for the model that we will get may not be as accurate as we think. Hence, i will also look at the precision and recall values and f1_score. Finally i will evaluate the selected feature set to see if we can make any improvements to the model.","854a97ba":"## Model Development Support Vector Machine","16126d89":"## Data Exploration","73e0f4a5":"The model predicts 567 True Negatives, 6 False Positives, 83 False Negatives, 11 True Positives. Even though the False Positive count slighlty went down, the True Positives are significantly less compare to RandomForestClassifier."}}