{"cell_type":{"8df9c66e":"code","6b16ee70":"code","688a8832":"code","826d374b":"code","32d5c267":"code","e0fb3495":"markdown","c952dde1":"markdown","0c12c975":"markdown","08ec5000":"markdown","1eceabd3":"markdown","4a9a8d40":"markdown"},"source":{"8df9c66e":"import os \n\nimport numpy as np\n\nimport pandas as pd\n\nimport tensorflow as tf\n\nfrom tensorflow import keras\n\nfrom keras.preprocessing import text, sequence\n\nfrom keras.preprocessing.text import Tokenizer\n\nraw_train = pd.read_csv('..\/input\/jigsaw-unintended-bias-in-toxicity-classification\/train.csv')\n\nraw_test = pd.read_csv('..\/input\/jigsaw-unintended-bias-in-toxicity-classification\/test.csv')\n\nvocab_size = 100000\n\nmax_length = 220\n\ntext_column = 'comment_text'\n\ntarget_column = 'target'\n\nchar_filter = '!\"#$%&()*+,-.\/:;<=>?@[\\\\]^_`{|}~\\t\\n\u201c\u201d\u2019\\'\u221e\u03b8\u00f7\u03b1\u2022\u00e0\u2212\u03b2\u2205\u00b3\u03c0\u2018\u20b9\u00b4\u00b0\u00a3\u20ac\\\u00d7\u2122\u221a\u00b2\u2014'\n\nembedding_loc = '..\/input\/fasttext-crawl-300d-2m\/crawl-300d-2M.vec'","6b16ee70":"def prepare_data(train_data, test_data, text_column, target_column, char_filter, vocab_size, max_length):\n    \n    raw_x_test = test_data[text_column].astype(str)\n    \n    raw_x_train = train_data[text_column].astype(str)\n    \n    y_train = train_data[target_column].values\n    \n    tokenizer = Tokenizer(num_words=vocab_size, filters=char_filter)\n\n    tokenizer.fit_on_texts(list(raw_x_train) + list(raw_x_test))\n    \n    x_test = tokenizer.texts_to_sequences(raw_x_test)\n    \n    x_train = tokenizer.texts_to_sequences(raw_x_train)\n    \n    x_test = sequence.pad_sequences(x_test, maxlen = max_length)\n    \n    x_train = sequence.pad_sequences(x_train, maxlen = max_length)\n    \n    return x_train, y_train, x_test, tokenizer\n\ndef vec_parser(word, *coeffs):\n    \n    return word, np.asarray(coeffs, dtype='float32')\n\ndef build_embedding(embedding_loc, word_index, max_length, dimensionality):\n    \n    embedding_index = dict(vec_parser(*line.strip().split(\" \")) for line in open(embedding_loc, encoding='utf-8'))\n    \n    embedding_matrix = np.zeros((len(word_index) + 1, dimensionality))\n    \n    for word, i in word_index.items():\n        \n        try:\n            \n            embedding_matrix[i] = embedding_index[word]\n        \n        except:\n            \n            embedding_matrix[i] = embedding_index[\"unknown\"]\n            \n    embedding_layer = keras.layers.Embedding(len(word_index)+1, \n                                         dimensionality, \n                                         weights=[embedding_matrix], \n                                         input_length=max_length, \n                                         trainable=False)\n    \n    return embedding_index, embedding_matrix, embedding_layer\n\ndef joint_shuffle(x_data, y_data):\n    \n    if len(x_data) == len(y_data):\n    \n        p = np.random.permutation(len(x_data))\n    \n    return x_data[p], y_data[p]","688a8832":"x_train, y_train, x_test, tokenizer = prepare_data(raw_train, \n                                        raw_test, \n                                        text_column, \n                                        target_column, \n                                        char_filter, \n                                        vocab_size, \n                                        max_length)\n\nembedding_index, embedding_matrix, embedding_layer = build_embedding(embedding_loc, \n                                                                     tokenizer.word_index, \n                                                                     max_length, \n                                                                     300)\n","826d374b":"model = keras.Sequential()\n\nmodel.add(embedding_layer)\nmodel.add(keras.layers.GlobalAveragePooling1D())\nmodel.add(keras.layers.Dense(16, activation=tf.nn.relu))\nmodel.add(keras.layers.Dense(1, activation=tf.nn.sigmoid))\n\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n\nmodel.summary()\n\nx_train, y_train = joint_shuffle(x_train, y_train)\n\nx_val = x_train[:10000]\npartial_x_train = x_train[10000:]\n\ny_val = y_train[:10000]\npartial_y_train = y_train[10000:]\n\nsimple_callback = keras.callbacks.EarlyStopping(monitor='val_acc', \n                                                min_delta=0.005,\n                                                patience=5,\n                                                mode='max')\n\nhistory = model.fit(partial_x_train,\n                    partial_y_train,\n                    epochs=1,\n                    batch_size=512,\n                    validation_data=(x_val, y_val),\n                    verbose=1,\n                    callbacks=[simple_callback])","32d5c267":"raw_results = model.predict(x_test)\n\nresults = np.average(raw_results, axis=1)\n\nsubmission = pd.DataFrame.from_dict({\n    'id': raw_test.id,\n    'prediction': results})\n\nsubmission.to_csv('submission.csv', index=False)","e0fb3495":"#### Functions\n\nThe prepare_data function takes the pd.DataFrame training and test data objects and returns tokenized and padded x data, y training data and the fitted keras.Tokenizer object.  The build_embedding function creates the embedding index, embedding matrix and embedding layer from a word2vec source and a word index dictionary.  The build_embedding function was motivated by Dieter's excellent <a href='https:\/\/www.kaggle.com\/christofhenkel\/keras-baseline-lstm-attention-5-fold'> baseline lstm + attention 5-fold kernel <\/a>.    ","c952dde1":"### Jigsaw Unintended Bias in Toxicity Classification\n\n1. [Introduction](#Introduction) <br>\n2. [Functions](#Functions) <br>\n3. [Data](#Data) <br>\n4. [Embedding](#Embedding) <br>\n5. [Model](#Model) <br>\n6. [Submission](#Submission) <br>\n\n\n","0c12c975":"#### Introduction\n\nThis is a simple Neural Network (NN) Kernel using Keras preprocessing features and TensorFlow for backend processing.  This is a simple baseline kernel that does not perform as well as the more sophisticated kernels but does include several concise loading functions.  This kernel is intended to be used as a starting point for more elaborate kernels.    ","08ec5000":"#### Data\n\nThe data is loaded using the prepare_data function and the embedding is created using the build_embedding function.  ","1eceabd3":"#### Model\n\nThe model used is a simple feedforward neural network that is very is essentially the same as the one proposed in the <a href= 'https:\/\/www.tensorflow.org\/tutorials\/keras\/basic_text_classification'> Tensor Flow tutorial <\/a>.  ","4a9a8d40":"#### Submission\nThe predicted classification is placed into a pd.DataFrame object for submission and saved locally.  "}}