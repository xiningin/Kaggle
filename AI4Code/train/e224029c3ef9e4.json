{"cell_type":{"7db49fb5":"code","9512b069":"code","a0a2728f":"code","1eeeab70":"code","f959f84b":"code","6806af3f":"code","65f266fb":"code","fb1cd74d":"code","8594fc59":"code","34e5e13d":"code","3e68195f":"code","d19f4081":"code","2e7c701f":"code","dcb6f611":"code","9cf8d953":"code","fde19fbf":"code","b7c0ff4d":"code","4ff27ce9":"code","d45e3538":"markdown","2103f383":"markdown","ede7b631":"markdown","06a7f19f":"markdown","6ae4e333":"markdown","fd3badd3":"markdown","8cc5cafe":"markdown","3245daa6":"markdown","2fbd1c9b":"markdown","74a7644a":"markdown","e3f871e9":"markdown","35b91745":"markdown","81f79655":"markdown","bc1ac7a4":"markdown","3e547ad2":"markdown","236c7a89":"markdown","f9227b8e":"markdown","42522fff":"markdown","ddc98031":"markdown"},"source":{"7db49fb5":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport datetime\nimport tensorflow as tf\n\nimport warnings\nwarnings.filterwarnings('ignore')","9512b069":"#Import Data\ndf = pd.read_csv('..\/input\/sales-forecasting\/train.csv')\n","a0a2728f":"#Create a data frame containing daily sales\ndf['Order Date'] = pd.to_datetime(df['Order Date'])\ndf.sort_values(['Order Date'],inplace=True)\ndaily_sales = pd.DataFrame(df.groupby('Order Date',sort=False)['Sales'].sum())\ndaily_sales.reset_index(inplace=True)\ndaily_sales['Order Year'] = daily_sales['Order Date'].apply(lambda x:x.year)","1eeeab70":"#Split Data into train and test sets :\nX = np.array(daily_sales['Sales'])\nX_train = np.array(daily_sales[daily_sales['Order Year']!=2018]['Sales'])\nX_test = np.array(daily_sales[daily_sales['Order Year']==2018]['Sales'])\nprint('Train set size : ',len(X_train))\nprint('Test set size : ',len(X_test))","f959f84b":"def windowed_dataset(X, window_size, batch_size, shuffle_buffer):\n    #Expand dataset fir RNN input shape expectation\n    X = tf.expand_dims(X, axis=-1)\n    #Create a dataset \n    ds = tf.data.Dataset.from_tensor_slices(X)\n    #Windowing the data set, window_size lags (passed observations)\n    ds = ds.window(window_size + 1, shift=1, drop_remainder=True)\n    #Batching and shuffling observations\n    ds = ds.flat_map(lambda w: w.batch(window_size + 1))\n    ds = ds.shuffle(shuffle_buffer)\n    #Decompose into predictors and predicted components\n    ds = ds.map(lambda w: (w[:-1], w[-1]))\n    return ds.batch(batch_size).prefetch(1)","6806af3f":"window_size = 41\n#Split time defines the limit of training observations and start of validation observations.\nsplit_time = 908\n\n\nXp_train = windowed_dataset(X_train,window_size=window_size,batch_size=256,shuffle_buffer=len(X_train))\nXp_validation = windowed_dataset(X_test,window_size=window_size,batch_size=512,shuffle_buffer=len(X_test))","65f266fb":"model1 = tf.keras.models.Sequential([\n            \n            \n            #1D Convolutional layer (Helps in smoothing out some noise)\n            tf.keras.layers.Conv1D(filters=3, kernel_size=5,\n                      strides=1, padding=\"valid\",\n                      activation=\"relu\",\n                      input_shape=[None, 1]),\n   \n            \n        \n     \n            # 2 LSTM layers\n            tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(40, return_sequences=True)), \n    \n            tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(40, return_sequences=False)),  \n    \n            # 3 Dense layers comprising the output layer\n            tf.keras.layers.Dense(30),\n            tf.keras.layers.Activation('relu'),\n     \n    \n            tf.keras.layers.Dense(10),\n            tf.keras.layers.Activation('relu'),\n\n            \n            \n            tf.keras.layers.Dense(1),\n    \n          tf.keras.layers.Lambda(lambda x: x * 10000.0)\n        ])\n\n#Compile the model\n\nmodel1.compile(loss=tf.keras.losses.Huber(),optimizer='Adam',metrics=['mae'])","fb1cd74d":"history1 = model1.fit(Xp_train, epochs=500,validation_data=Xp_validation)","8594fc59":"mae=history1.history['mae']\nval_mae = history1.history['val_mae']\nepochs=range(len(mae)) \nfig = plt.figure(figsize=(12,6))\nax = fig.add_axes([0,0,1,1])\nax.plot(epochs, mae, 'b',label='Training mae')\nax.plot(epochs, val_mae, 'r',label='Validation mae')\nax.legend()\nplt.title('MAE')\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"MAE\")","34e5e13d":"def Model(Lambda,drop_rate) :\n    \n    model = tf.keras.models.Sequential([\n            \n            \n            \n            tf.keras.layers.Conv1D(filters=3, kernel_size=5,\n                      strides=1, padding=\"causal\",\n                      activation=\"relu\",\n                      input_shape=[None, 1],kernel_regularizer=tf.keras.regularizers.l2(Lambda)),\n   \n            tf.keras.layers.Dropout(drop_rate),\n        \n     \n            \n            tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(40, return_sequences=True,\n                                          kernel_regularizer=tf.keras.regularizers.l2(Lambda))), \n    \n            tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(40, return_sequences=False,\n                                          kernel_regularizer=tf.keras.regularizers.l2(Lambda))),  \n            \n            tf.keras.layers.Dropout(drop_rate),\n            \n  \n            tf.keras.layers.Dense(30,kernel_regularizer=tf.keras.regularizers.l2(Lambda)),\n            tf.keras.layers.Activation('relu'),\n     \n    \n            tf.keras.layers.Dense(10,kernel_regularizer=tf.keras.regularizers.l2(Lambda)),\n            tf.keras.layers.Activation('relu'),\n\n            \n            \n            tf.keras.layers.Dense(1,kernel_regularizer=tf.keras.regularizers.l2(Lambda)),\n    \n          tf.keras.layers.Lambda(lambda x: x * 10000.0)\n        ])\n\n    return model","3e68195f":"#Regularization factor space using random logarithmic selection :[1e-3,1] \nr = -3*np.random.rand(20)\nregu_factors = 10**r\n\n#Dropout rate space : [0,0.5]\ndrop_rates = np.linspace(0,0.4,5)\n\n#Hyperparameters space : Product of both previous spaces\nimport itertools\nhyper_space = list(itertools.product(regu_factors,drop_rates))\n\nprint('Subset of hyperparameters space :')\nprint(hyper_space[0:10])","d19f4081":"#Dictionaries to save results\nhistory_dict = dict()\nprediction_dict = dict()\ni=0\n\n#Loop over hyperparameter space components\nfor Lambda,rate in hyper_space :\n    print(f'Processing for ({Lambda},{rate})')\n    #Update parameters\n    model = Model(Lambda,rate)\n    #Compile model and train\n    model.compile(loss=tf.keras.losses.Huber(),optimizer='Adam',metrics=['mae'])\n    #Save training and validation history\n    history = model.fit(Xp_train, epochs=500,validation_data=Xp_validation,callbacks=[early_stop]) \n    history_dict[f'Training MAE {i}'] = history.history['mae']\n    history_dict[f'Validation MAE {i}'] = history.history['val_mae']\n    i+=1","2e7c701f":"model = Model(0.5,0.2)\nmodel.compile(loss=tf.keras.losses.Huber(),optimizer='Adam',metrics=['mae'])","dcb6f611":"from tensorflow.keras.callbacks import EarlyStopping\n#Recuparate best model\nearly_stop = EarlyStopping(monitor='val_mae',patience=500,restore_best_weights=True,mode='min')\nhistory = model.fit(Xp_train, epochs=500,validation_data=Xp_validation,callbacks=[early_stop]) ","9cf8d953":"mae=history.history['mae']\nval_mae = history.history['val_mae']\nprint('min validation mae : ',min(history.history['val_mae']))\n\nmin_val = min(history.history['val_mae'])\nindex = history.history['val_mae'].index(min_val)\nepochs=range(len(mae)) \nfig = plt.figure(figsize=(12,6))\nax = fig.add_axes([0,0,1,1])\nax.plot(epochs, mae, 'b',label='Training mae')\nax.plot(epochs, val_mae, 'r',label='Validation mae')\nax.plot(index,min_val,marker='*',ms=20,markerfacecolor='yellow',markeredgewidth=3, markeredgecolor='green')\nax.legend()\nplt.title('MAE')\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"MAE\")","fde19fbf":"#Given a series, this function predicts sales for each step.\ndef model_forecast(model, X, window_size):\n    #Creating a dataset\n    ds = tf.data.Dataset.from_tensor_slices(X)\n    #Windowing\n    ds = ds.window(window_size, shift=1, drop_remainder=True)\n    ds = ds.flat_map(lambda w: w.batch(window_size))\n    ds = ds.batch(908).prefetch(1)\n    #Predict \n    forecast = model.predict(ds)\n    return forecast","b7c0ff4d":"forecast = model_forecast(model,X[...,np.newaxis],window_size)\nforecast = forecast[:,-1].reshape((len(forecast),))","4ff27ce9":"train_predictions = forecast[:split_time-window_size]\ntest_predictions = forecast [split_time-window_size:-1]\nMSE_train = tf.keras.metrics.mean_squared_error(X_train[window_size:], train_predictions)\nMSE_test = tf.keras.metrics.mean_squared_error(X_test, test_predictions)\nMAE_train = tf.keras.metrics.mean_absolute_error(X_train[window_size:], train_predictions)\nMAE_test = tf.keras.metrics.mean_absolute_error(X_test, test_predictions)\nprint('Train RMSE = ',np.sqrt(MSE_train))\nprint('Test RMSE = ',np.sqrt(MSE_test))\nprint('Train MAE = ',MAE_train.numpy())\nprint('Test MAE = ',MAE_test.numpy())","d45e3538":"Create a data frame containing sales grouped by day. ","2103f383":"Apply the previous function on daily sales using a time window of 41 days.","ede7b631":"### Model","06a7f19f":"Clearly the model overfitts training set. ","6ae4e333":"Still a lot of work to do on regularization. Also note that daily sales contain a lot of noise and demonstrates no clear pattern...","fd3badd3":"# Regularized model","8cc5cafe":"Introduce L2 regularization and dropouts into the model","3245daa6":"It takes time to search for hyperparameters. Let's just proceed with some given model.","2fbd1c9b":"# This notebook contains some code inspired from Sequences, Time Series and Prediction deeplearning.ai course which is a great course to start learning Deep Learning on Tensorflow.","74a7644a":"# Regularization","e3f871e9":"Now, data must be transformed into windowed dataset, this is easily done using tensorflow, the next function is a simple way of doing it.","35b91745":"# Data Preprocessing","81f79655":"Split data into two sets, a Training Set and a Validation Set. The training set contains all observations except those occurring in year 2018 which are reserved for cross validation.","bc1ac7a4":"Create a hyperparameter space, in this case it is the space of pairs of the form (regularization factor,dropout rate). Hence, to create this space, simply create a space of regularization factor and one other for dropout rates and multiply them.","3e547ad2":"# Prediction","236c7a89":"# Model ","f9227b8e":"Let's apply this function to all Data and split results into training and testing results.","42522fff":"### Hyperparameters space","ddc98031":"### Explore the hyperparameter space "}}