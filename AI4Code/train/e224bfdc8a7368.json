{"cell_type":{"b919a416":"code","d08efff6":"code","eba1ce35":"code","be5fbea9":"code","0059bcfe":"code","6e5849e7":"markdown","6019ae03":"markdown","fcfeedb5":"markdown","6fa486cc":"markdown"},"source":{"b919a416":"import numpy as np # linear algebra\nfrom scipy.stats import pearsonr\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom pandas.api.types import union_categoricals\nfrom matplotlib import pyplot as plt \nimport seaborn as sns\nfrom pprint import pprint\nfrom time import time\n\nfrom os import listdir\nfrom os import path\n\n## Much code copied from the Faces example taken from scikit \n## (https:\/\/scikit-learn.org\/stable\/auto_examples\/applications\/plot_face_recognition.html)\nfrom time import time\nimport logging\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.neighbors import (KNeighborsClassifier,\n                               NeighborhoodComponentsAnalysis)\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\nprint(__doc__)\n## Display progress logs on stdout\nlogging.basicConfig(level = logging.INFO,\n                   format = '%(asctime)s %(message)s')\nrandom_state = 3412","d08efff6":"train_data = pd.read_csv(path.join(\"..\", \"input\", \"learn-together\", \"train.csv\"))\ntest_data = pd.read_csv(path.join(\"..\", \"input\", \"learn-together\", \"test.csv\"))","eba1ce35":"X = train_data.drop(['Cover_Type', 'Id'], axis = 1)\nX_test = test_data.drop(['Id'], axis = 1)\n\ny = train_data['Cover_Type']\ntest_Id = test_data.Id\n\ntarget_names = np.array(['Spruce\/Fir',\n                             'Lodgepole Pine',\n                             'Ponderosa Pine',\n                             'Cottonwood\/Willow',\n                             'Aspen',\n                             'Douglas-fir',\n                             'Krummholz'])","be5fbea9":"# Quantitative evaluation of the predictions using matplotlib\nd = [2,4]\nK = [1,5]\n\nknn_pipeline = Pipeline([\n    ('scale', StandardScaler(with_mean = True)),\n    ('nca', NeighborhoodComponentsAnalysis(max_iter = 50, random_state = random_state)),\n    ('clf', KNeighborsClassifier())\n])\n    \nknn_params = {\n    'scale__with_std': (True, False),\n    'nca__n_components': d,\n    'clf__n_neighbors': K\n}\n\nknn_grid = GridSearchCV(knn_pipeline, knn_params, cv=5,\n                               n_jobs=1, verbose=1, scoring = 'f1_weighted', return_train_score = True)\n\nprint(\"Performing grid search...\")\nprint(\"pipeline:\", [name for name, _ in knn_pipeline.steps])\nprint(\"parameters:\")\npprint(knn_params)\nt0 = time()\nknn_grid.fit(X, y)\nprint(\"done in %0.3fs\" % (time() - t0))\nprint()\n\nprint(\"Best score: %0.3f\" % knn_grid.best_score_)\nprint(\"Best parameters set:\")\nbest_parameters = knn_grid.best_estimator_.get_params()\nfor param_name in sorted(knn_params.keys()):\n    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))","0059bcfe":"logit_pipeline = Pipeline([\n    ('scale', StandardScaler()),\n    ('nca', NeighborhoodComponentsAnalysis(max_iter = 50, random_state = random_state)),\n    ('clf', LogisticRegression(multi_class = 'multinomial', solver = 'lbfgs' ))\n])\n    \nlogit_params = {\n    'scale__with_mean': (True, False),\n    'scale__with_std': (True, False),\n    'nca__n_components': d,\n    'clf__n_neighbors': K\n}\n\nlogit_grid = GridSearchCV(logit_pipeline, logit_params, cv=5,\n                               n_jobs=1, verbose=1, scoring = 'f1_weighted', return_train_score = True)\n\nprint(\"Performing grid search...\")\nprint(\"pipeline:\", [name for name, _ in logit_pipeline.steps])\nprint(\"parameters:\")\npprint(logit_params)\nt0 = time()\nlogit_grid.fit(X, y)\nprint(\"done in %0.3fs\" % (time() - t0))\nprint()\n\nprint(\"Best score: %0.3f\" % logit_grid.best_score_)\nprint(\"Best parameters set:\")\nbest_logit_parameters = logit_grid.best_estimator_.get_params()\nfor param_name in sorted(logit_params.keys()):\n    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))","6e5849e7":"# Design of the comparison\n1. Split training data into training\/validation sets\n2. Use NCA to reduce the input data to $d  = \\left\\{1,\\, 2,\\, 5\\right\\}$ dimensions. \n3. For each value of $d$: \n    - Fit predictors based on $K = \\left\\{1,\\, 3,\\, 5,\\, 9,\\, 17,\\,\\right\\}$-nearest neighbors.\n    - Fit a logistic regression classifier with multinomial loss function\n4. Compare these $3 + 3\\times 5 = 18$ predictive models on the validation set using weighted average of f1-score across cover types","6019ae03":"# Assumptions\n- NCA is a linear transformation of the data, so: \n    - The NCA process can be thought of as looking at each pair of features at a time, then rotating the plane and squishing\/stretching the axes for each NCA dimension\n","fcfeedb5":"# Hypotheses \n1. The value of a feature's coefficients across all NCA dimensions reflects this process\n    - if they are uniformly 0, then the feature does not influence the accuracy of a KNN-based classifier much\n    - if they are positive and much larger than the feature's standard deviation, then the more influential they are with respect the KNN predictor fit\n    - if they are negative and much larger than the feature's standard deviation, then patches of forest with similar values of the feature are _less_ likely to have the same cover and the association is likely spurious (this may be an outlier flag)\n2. If a feature is influential on the KNN fit due to its large scale, it will have large, positive NCA coefficients in a $d$-dimensional unscaled fit but a pattern of one or two positive NCA coefficients, but mostly coefficients of alternating signs in a scaled fit.\n    - NCA coefficients are informative about whether to scale the data before modeling.\n3. When the $K$ used in the nearest neighbor fit far exceeds that implied by the NCA transform (see paper linked in documentation) then there will be overfitting, but not likely severe overfitting. \n    - Using the smallest $d$ NCA model with adequate performance in the validation set will lead to the best model in the test set","6fa486cc":"# Intro\n- In my work on trying to classify forest cover so far, I've found that K-nearest neighbors (KNN) performs the best with little feature engineering: not even scaling. Why is it so good?\n- KNN is highly non-linear, even non-monotonic, and obviously exploits clustering in the data that is related to forest cover type. But it's basically impossible to interpret parsimoniously, especially in the case of a 1-nearest neighbor predictor. \n- This has me wondering if I can use [Neighborhood Components Analysis](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.neighbors.NeighborhoodComponentsAnalysis.html#sklearn.neighbors.NeighborhoodComponentsAnalysis) (NCA)  to help me \n    1. understand why KNN has performed so well \n    2. do some feature engineering so that I can get a better-performing interpretable model: e.g. better understand nonlinearity and scaling in the data\n    3. identify clusters and\/or outliers in the data that I can interpret\n- I think that this question is relevant because I don't think that there would be much use developing an uninterpretable model to predict forest cover type in a < 1 hectare patch\n    + Given that we require soil samples and survey measurements to make our predictions, the marginal cost of asking the person who is gathering those to classify the forest type seems minimal\n    + On the other hand, supposing this is some kind of remote-operated or autonomous vehicle that has a good soil lab built in, our efforts here might be valuable\n    + Even then, we might like to have some idea of what's going on so we could troubleshoot\n    + Some reasons we might want to predict forest cover type: allocation of conservation budget, number of hunting\/fishing licenses for a particular species, number of camping\/mushroom gathering\/mining\/construction permits, number of fire lookouts\/rangers, among others.\n- I also want to learn more about KNN and especially about NCA, which I've just found out about.\n- Sorry about the language in this kernel: I tend to be a little bit stilted in the way its used, which implies a lot more certainty than I would like. _Everything_ in here is questionable."}}