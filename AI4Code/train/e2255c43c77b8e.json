{"cell_type":{"a8fda013":"code","47b34f2f":"code","62148c79":"code","6d666fb4":"code","27679a12":"code","15171c90":"code","b7787c01":"code","5ce6ed82":"code","5ebe48ee":"code","5212bd71":"code","ff25256d":"code","a525983d":"code","f3d87afc":"code","1670795d":"code","46be2e87":"code","f2f2168a":"code","3bc7fafe":"code","08452e15":"code","d768a767":"code","7a1bd2a3":"code","6497e542":"code","dc21db57":"code","687b176a":"code","7bd4f709":"code","a50d5101":"code","39be1e3c":"code","baa7acd7":"code","1b84a980":"code","d1873eda":"code","03b0b984":"code","2f5c7349":"code","4a5a0043":"code","c53a4534":"code","da21c590":"code","890cb42e":"code","7fadd5b9":"code","2787ce0c":"code","7f516821":"code","683484b5":"code","1d53b268":"code","85b44aa4":"code","aa119d3f":"code","c3a9da38":"code","6cc48d04":"code","5e58431b":"code","05ec055d":"code","2a67a495":"code","db3c2879":"code","124604a5":"code","5a9356d4":"code","512d409b":"code","e8932137":"code","e3ee57e8":"code","4346bcaf":"code","c2e5edb4":"code","3122f9df":"code","94235b4b":"code","0d41101c":"code","203b484c":"code","857b0e08":"code","a5f4a3c7":"code","18fc8389":"code","d2fe9cdb":"code","fda2e7de":"code","19ef7934":"code","97e1d143":"code","adc2b8a9":"code","2129c2e8":"code","51912029":"code","d7c49234":"code","c3e12dde":"code","8ad1862a":"code","a64a8b79":"markdown","ef8ed559":"markdown","ae882501":"markdown","998ee31e":"markdown","58ea7438":"markdown","799c7d9e":"markdown","15af5b16":"markdown","4337fd4d":"markdown","75e356ca":"markdown","955060aa":"markdown","3de80879":"markdown","24e63cd8":"markdown","37a44190":"markdown","78b37210":"markdown","67320fc4":"markdown","f5856a8a":"markdown","0838d264":"markdown","0e2e5fb9":"markdown","cf319b96":"markdown","c6ac0efb":"markdown","950ada37":"markdown","19f49ec1":"markdown","ed037931":"markdown","6c4dd3bf":"markdown"},"source":{"a8fda013":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","47b34f2f":"# import all the libraries that are important\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA","62148c79":"# import all the necessary files for the analysis.\n\nbase_path = '\/kaggle\/input\/machine-learning-24-hrs-hackathon\/'\n\ntrain_df = pd.read_csv(base_path + 'train_SJC.csv', header=1)\nsumission_df = pd.read_csv(base_path + 'sample_submission.csv')","6d666fb4":"# to have a basic overview of the train dataset\nprint('shape: ',train_df.shape, end='\\n\\n')\ntrain_df.head()","27679a12":"# rename the columns as mentioned before.\n\ntrain_df = train_df.rename(columns={\"Unnamed: 2\":\"DateReported\", \n                                    \"Unnamed: 7\":\"DependentsOther\", \n                                    \"Unnamed: 11\": \"DaysWorkedPerWeek\"})\ntrain_df.columns","15171c90":"# check the data description\n\ntrain_df.info()","b7787c01":"# convert the date fields to datet time objects so that we can access them as dates and not objects.\n\ntrain_df['DateTimeOfAccident'] = pd.to_datetime(train_df['DateTimeOfAccident'])\ntrain_df['DateReported'] = pd.to_datetime(train_df['DateReported'])\ntrain_df[['DateTimeOfAccident','DateReported']].info()","5ce6ed82":"# to check if its follwing the correct format or not.\n\ntrain_df[['DateTimeOfAccident','DateReported']].head()","5ebe48ee":"# to find the difference in the 2 dates (in weeks)\ndef diff_weeks_col(df, col1, col2):\n    diff_weeks = df['DateReported'] - df['DateTimeOfAccident']\n    diff_weeks = diff_weeks\/np.timedelta64(1,'W')\n    return diff_weeks\n\ntrain_df['diff_weeks'] = diff_weeks_col(train_df, 'DateReported', 'DateTimeOfAccident')\ntrain_df['diff_weeks'].head()","5212bd71":"# check if there are any value under 0,and replace it with 0. The differnce is mostly because of the time not the date.\n\ntrain_df['diff_weeks'][train_df['diff_weeks']<0] = 0","ff25256d":"# cross check if there are still any values under 0.\n\n(train_df['diff_weeks']<0).value_counts()","a525983d":"# check for null values\n# you can also use .isna()\n\ntrain_df.isnull().sum()","f3d87afc":"# since the number of missing values are really low we can impute median and mode values in them.\n\ntrain_df['MaritalStatus'] = train_df['MaritalStatus'].fillna(train_df['MaritalStatus'].mode()[0])\ntrain_df['WeeklyWages']  = train_df['WeeklyWages'].fillna(train_df['WeeklyWages'].median())\ntrain_df['HoursWorkedPerWeek']  = train_df['HoursWorkedPerWeek'].fillna(train_df['HoursWorkedPerWeek'].median())","1670795d":"# to cross check if the values have been replaces for sure.\n\ntrain_df[['MaritalStatus','WeeklyWages', 'HoursWorkedPerWeek']].isna().sum()","46be2e87":"# To check if there are any duplicate values\n\ntrain_df.duplicated().value_counts()","f2f2168a":"# To check the unique values for all the columns\n\ntrain_df.nunique()","3bc7fafe":"# to check the number of duplicate values for the column ClaimNumber\ntrain_df.duplicated('ClaimNumber').value_counts()","08452e15":"# checking for multicollinearity \nsns.heatmap(train_df.corr()>0.3, annot = True, linewidths = 0.1)","d768a767":"df_heatmap = train_df[['DaysWorkedPerWeek','InitialIncurredCalimsCost','UltimateIncurredClaimCost', \"HoursWorkedPerWeek\"]].corr()\nsns.heatmap(df_heatmap, annot = True)","7a1bd2a3":"# split the columns into numeric and categorical\n\nnum_cols = train_df.select_dtypes(exclude=['object']).columns\ncat_cols = train_df.select_dtypes(include=['object']).columns","6497e542":"# to check the distributions of the other columns\n\ntrain_df.hist(figsize=(19,12))","dc21db57":"# to have an overview of the data distribution.\ntrain_df[num_cols].describe()","687b176a":"# to check the number of outliers per column:\ndef count_outliers(df):\n    Q1 = df[num_cols].drop(['DateTimeOfAccident', 'DateReported'], axis=1).quantile(0.25)\n    Q3 = df[num_cols].drop(['DateTimeOfAccident', 'DateReported'], axis=1).quantile(0.75)\n    IQR = Q3 - Q1\n    \n    print('Count of all outliers:\\n')\n    print(((df[num_cols].drop(['DateTimeOfAccident', 'DateReported'], axis=1) < (Q1 - 1.5 * IQR)) | \n     (df[num_cols].drop(['DateTimeOfAccident', 'DateReported'], axis=1) > (Q3 + 1.5 * IQR))).sum())\n    \ncount_outliers(train_df)","7bd4f709":"# Does the duration affect the final and the initial claim amount.\n\ntrain_df[['UltimateIncurredClaimCost','diff_weeks','InitialIncurredCalimsCost']].sort_values(by = 'diff_weeks').plot.line(x = 'diff_weeks', \n                                                                                                                          y = ['UltimateIncurredClaimCost', 'InitialIncurredCalimsCost'])","a50d5101":"# to check if the stratified sampling was successful\nfig, (ax1, ax2, ax3) = plt.subplots(ncols=3, sharey=True)\nsns.countplot(x = train_df[cat_cols[1]], ax=ax1)\nsns.countplot(x = train_df[cat_cols[2]], ax=ax2)\nsns.countplot(x = train_df[cat_cols[3]], ax=ax3)","39be1e3c":"# making a copy before outlier treatment.\nclean_train_df = train_df","baa7acd7":"clean_train_df.head()","1b84a980":"# check the number of outliers before treatment\n# outlier removal to be performed on HoursWorkedPerWeek, WeeklyWages, UltimateIncurredClaimCost\n\nprint('Shape: ',clean_train_df.shape, end='\\n\\n')\ncount_outliers(clean_train_df)","d1873eda":"num_cols","03b0b984":"# functions to remove the outliers\ndef outliers_limits(df, feature):\n    Q1= df[feature].quantile(0.25)\n    Q3 = df[feature].quantile(0.75)\n    IQR = Q3 - Q1\n    upper_limit = Q3 + 1.5 * IQR\n    lower_limit = Q1 - 1.5 * IQR\n    return upper_limit, lower_limit\n\ndef removal(df, feature, upper, lower):\n    new_df = df[(df[feature] > lower) & (df[feature] < upper)]\n    return new_df","2f5c7349":"# outlier treatment on outlier_cols1:    \noutlier_cols1 = ['HoursWorkedPerWeek', 'WeeklyWages', 'UltimateIncurredClaimCost']\noutlier_df1 = clean_train_df\n\nfor i in range(len(outlier_cols1)):\n    print('outlier treatment on :', outlier_cols1[i])\n    upper, lower = outliers_limits(clean_train_df, outlier_cols1[i])\n    \n    outlier_df1 = removal(outlier_df1, outlier_cols1[i], upper, lower)\n    print('Shape: ',outlier_df1.shape, end='\\n\\n')","4a5a0043":"# outlier treatment on outlier_cols2:\noutlier_cols2 = ['WeeklyWages','HoursWorkedPerWeek','InitialIncurredCalimsCost',\n                 'UltimateIncurredClaimCost' ,'diff_weeks']\noutlier_df2 = clean_train_df\n\nfor i in range(len(outlier_cols2)):\n    print('outlier treatment on :', outlier_cols2[i])\n    upper, lower = outliers_limits(clean_train_df, outlier_cols2[i])\n    \n    outlier_df2 = removal(outlier_df2, outlier_cols2[i], upper, lower)\n    print('Shape: ',outlier_df2.shape, end='\\n\\n')","c53a4534":"def bining(df, feature):\n    bins = [0, 20, 40, 60, 81]\n    labels = [20, 40, 60, 80]\n    df[feature] = pd.cut(df[feature] , bins=bins, labels=labels, include_lowest=True)\n\nbining(clean_train_df, 'Age')\nprint('After transformation:',clean_train_df['Age'].value_counts(), sep='\\n\\n')\nclean_train_df['Age'].hist(bins=4)","da21c590":"# Label encoding for columns Gender', 'MaritalStatus', 'PartTimeFullTime'\n\ngender_label = {'M':1, 'F':2, 'U': 3}\nmarital_label = {'M':1, 'U':2, 'S':3}\npartTime_label = {'F':1, 'P':2}\n\noutlier_df1['Gender'] = outlier_df1['Gender'].map(gender_label)\noutlier_df1['MaritalStatus']= outlier_df1['MaritalStatus'].map(marital_label)\noutlier_df1['PartTimeFullTime']= outlier_df1['PartTimeFullTime'].map(partTime_label)","890cb42e":"# for the second dataset\n\noutlier_df2['Gender'] = outlier_df2['Gender'].map(gender_label)\noutlier_df2['MaritalStatus']= outlier_df2['MaritalStatus'].map(marital_label)\noutlier_df2['PartTimeFullTime']= outlier_df2['PartTimeFullTime'].map(partTime_label)","7fadd5b9":"clean_train_df['Gender'] = clean_train_df['Gender'].map(gender_label)\nclean_train_df['MaritalStatus']= clean_train_df['MaritalStatus'].map(marital_label)\nclean_train_df['PartTimeFullTime']= clean_train_df['PartTimeFullTime'].map(partTime_label)","2787ce0c":"# to verify the changes\n\nclean_train_df[['PartTimeFullTime', 'MaritalStatus', 'Gender']].head()","7f516821":"# to convert the categorical type column to int64\n\nclean_train_df['Age'] = clean_train_df['Age'].astype('int64')","683484b5":"# to cross verify if there are any other columns left to be trasnsformed.\nclean_train_df.dtypes.value_counts()","1d53b268":"feature_imp = clean_train_df.copy()\nfeature_imp_cols = feature_imp.drop(['ClaimNumber', 'DateTimeOfAccident',\n                                     'DateReported','ClaimDescription'], axis=1).columns\nfeature_imp = feature_imp[feature_imp_cols]\nfeature_imp.info()","85b44aa4":"feature_imp.info()","aa119d3f":"X =  feature_imp[feature_imp_cols].drop('UltimateIncurredClaimCost', axis=1).values\nY = feature_imp[feature_imp_cols].values\n\npca_ = PCA()\npca = pca_.fit(X, Y)\nvar_expl = list(pca.explained_variance_ratio_)","c3a9da38":"var_df = pd.DataFrame(columns=['cols', 'var_expl'])\nX_cols = feature_imp[feature_imp_cols].drop('UltimateIncurredClaimCost', axis=1).columns\ncols = []\nfor i in range(len(X_cols)):\n    sum_col=''\n    for j in range(0,i+1):\n        sum_col = sum_col +', '+ X_cols[j]\n    cols.append(sum_col)\n    \nvar_df['cols']=cols\nvar_df['var_expl']=var_expl\nvar_df","6cc48d04":"1-var_df['var_expl'][1]","5e58431b":"plt.plot(var_df.index, var_df['var_expl'])","05ec055d":"test_df = pd.read_csv(base_path + 'Test_SJC.csv')\ntest_df.head()","2a67a495":"test_df.Age.max()","db3c2879":"# missing value:\ntest_df['MaritalStatus'] = test_df['MaritalStatus'].fillna(test_df['MaritalStatus'].mode()[0])\ntest_df.isna().sum()","124604a5":"# check duplicate\ntest_df.duplicated().value_counts()","5a9356d4":"test_df.info()","512d409b":"# date_time object conversion:\n\ntest_df['DateTimeOfAccident'] = pd.to_datetime(test_df['DateTimeOfAccident'])\ntest_df['DateReported'] = pd.to_datetime(test_df['DateReported'])\ntest_df[['DateTimeOfAccident','DateReported']].info()","e8932137":"test_df['diff_weeks'] = diff_weeks_col(test_df, 'DateReported', 'DateTimeOfAccident')\ntest_df['diff_weeks'].head()","e3ee57e8":"test_df['diff_weeks'][test_df['diff_weeks']<0] = 0\n(test_df['diff_weeks']<0).value_counts()","4346bcaf":"# binning age\nbining(test_df, 'Age')\nprint('After transformation:',test_df['Age'].value_counts(), sep='\\n\\n')\ntest_df['Age'].hist(bins=4)","c2e5edb4":"# convert categorical to int\n# test_df['Age'][test_df['Age'].isna()] = 80\ntest_df['Age'] = test_df['Age'].astype('int64')","3122f9df":"# label encoding\ngender_label = {'M':1, 'F':2, 'U': 3}\nmarital_label = {'M':1, 'U':2, 'S':3}\npartTime_label = {'F':1, 'P':2}\n\ntest_df['Gender'] = test_df['Gender'].map(gender_label)\ntest_df['MaritalStatus']= test_df['MaritalStatus'].map(marital_label)\ntest_df['PartTimeFullTime']= test_df['PartTimeFullTime'].map(partTime_label)","94235b4b":"test_df.dtypes.value_counts()","0d41101c":"from sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nimport sklearn.metrics as mt\nfrom math import sqrt\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score","203b484c":"col_in_use = ['Age', 'Gender','MaritalStatus', 'DependentChildren', 'DependentsOther', 'WeeklyWages',\n              'PartTimeFullTime', 'HoursWorkedPerWeek', 'DaysWorkedPerWeek', 'InitialIncurredCalimsCost',\n              'UltimateIncurredClaimCost', 'diff_weeks']","857b0e08":"col_test = ['Age', 'Gender','MaritalStatus', 'DependentChildren', 'DependentsOther', 'WeeklyWages',\n              'PartTimeFullTime', 'HoursWorkedPerWeek', 'DaysWorkedPerWeek', 'InitialIncurredCalimsCost', 'diff_weeks']\nx_test_final = test_df[col_test]","a5f4a3c7":"x_train, x_test, y_train, y_test = train_test_split(outlier_df1[col_in_use].drop('UltimateIncurredClaimCost', axis=1),\n                                                    outlier_df1['UltimateIncurredClaimCost'], test_size = 0.25, \n                                                    random_state=123)\nregr1 = LinearRegression(normalize=True)\n  \nregr1.fit(x_train, y_train)\nprint('Score: ',regr1.score(x_test, y_test))\nprint('RMSE: ',sqrt(mt.mean_squared_error(y_pred=regr1.predict(x_test),y_true=y_test)))","18fc8389":"# submission 1 :\ny_predict = regr1.predict(x_test_final)\nsumission_df['UltimateIncurredClaimCost'] = y_predict\nsumission_df.to_csv(\"try1.csv\",index=False)","d2fe9cdb":"x_train, x_test, y_train, y_test = train_test_split(outlier_df2[col_in_use].drop('UltimateIncurredClaimCost', axis=1),\n                                                    outlier_df2['UltimateIncurredClaimCost'], test_size = 0.25, \n                                                    random_state=123)\nregr2 = LinearRegression(normalize=True)\n  \nregr2.fit(x_train, y_train)\nprint('Score: ',regr2.score(x_test, y_test))\nprint('RMSE: ',sqrt(mt.mean_squared_error(y_pred=regr2.predict(x_test),y_true=y_test)))","fda2e7de":"# submission 2 :\ny_predict = regr2.predict(x_test_final)\nsumission_df['UltimateIncurredClaimCost'] = y_predict\nsumission_df.to_csv(\"try2.csv\",index=False)","19ef7934":"# cols in use : 'Age', 'Gender','MaritalStatus', 'WeeklyWages', 'HoursWorkedPerWeek', 'DaysWorkedPerWeek',\n# 'InitialIncurredCalimsCost'\n\n# cols dropped : 'DependentChildren', 'DependentsOther','PartTimeFullTime', 'diff_weeks'\n\nx_df = outlier_df1[col_in_use].drop(['UltimateIncurredClaimCost','DependentChildren', 'DependentsOther',\n                                    'PartTimeFullTime','diff_weeks'], axis=1)\nx_train, x_test, y_train, y_test = train_test_split(x_df,outlier_df1['UltimateIncurredClaimCost'], test_size = 0.25, \n                                                    random_state=123)\nregr3 = LinearRegression(normalize=True)\n  \nregr3.fit(x_train, y_train)\nprint('Score: ',regr3.score(x_test, y_test))\nprint('RMSE: ',sqrt(mt.mean_squared_error(y_pred=regr3.predict(x_test),y_true=y_test)))","97e1d143":"# submission 3 :\nx_d = x_test_final.drop(['DependentChildren', 'DependentsOther','PartTimeFullTime','diff_weeks'], axis=1)\ny_predict = regr3.predict(x_d)\nsumission_df['UltimateIncurredClaimCost'] = y_predict\nsumission_df.to_csv(\"try3.csv\",index=False)","adc2b8a9":"# cols in use : 'Age', 'Gender','MaritalStatus', 'WeeklyWages', 'HoursWorkedPerWeek', 'DaysWorkedPerWeek',\n# 'InitialIncurredCalimsCost'\n\n# cols dropped : 'DependentChildren', 'DependentsOther','PartTimeFullTime', 'diff_weeks'\n\nx_df = outlier_df2[col_in_use].drop(['UltimateIncurredClaimCost','DependentChildren', 'DependentsOther',\n                                    'PartTimeFullTime','diff_weeks'], axis=1)\nx_train, x_test, y_train, y_test = train_test_split(x_df,outlier_df2['UltimateIncurredClaimCost'], test_size = 0.25, \n                                                    random_state=123)\nregr4 = LinearRegression(normalize=True)\n  \nregr4.fit(x_train, y_train)\nprint('Score: ',regr4.score(x_test, y_test))\nprint('RMSE: ',sqrt(mt.mean_squared_error(y_pred=regr4.predict(x_test),y_true=y_test)))","2129c2e8":"# submission 4 :\nx_d = x_test_final.drop(['DependentChildren', 'DependentsOther','PartTimeFullTime','diff_weeks'], axis=1)\ny_predict = regr4.predict(x_d)\nsumission_df['UltimateIncurredClaimCost'] = y_predict\nsumission_df.to_csv(\"try4.csv\",index=False)","51912029":"X =  outlier_df1[col_in_use].drop('UltimateIncurredClaimCost', axis=1)\ny =  outlier_df1['UltimateIncurredClaimCost']\nregr5 = LinearRegression(normalize=True)\n  \nregr5.fit(X, y)\nprint('Score: ',regr1.score(X, y))\nprint('RMSE: ',sqrt(mt.mean_squared_error(y_pred=regr1.predict(X),y_true=y)))","d7c49234":"# submission 5 :\ny_predict = regr5.predict(x_test_final)\nsumission_df['UltimateIncurredClaimCost'] = y_predict\nsumission_df.to_csv(\"try5.csv\",index=False)","c3e12dde":"X =  outlier_df2[col_in_use].drop('UltimateIncurredClaimCost', axis=1)\ny =  outlier_df2['UltimateIncurredClaimCost']\nregr6 = LinearRegression(normalize=True)\n  \nregr6.fit(X, y)\nprint('Score: ',regr1.score(X, y))\nprint('RMSE: ',sqrt(mt.mean_squared_error(y_pred=regr1.predict(X),y_true=y)))","8ad1862a":"# submission 6 :\ny_predict = regr6.predict(x_test_final)\nsumission_df['UltimateIncurredClaimCost'] = y_predict\nsumission_df.to_csv(\"try6.csv\",index=False)","a64a8b79":"### 6.6 DS2 basic linear regression (full_data_set)","ef8ed559":"### 6.2 DS2 basic linear regression","ae882501":"### Insight:\n- The date columns have a uniform distribution. The rest of the columns seem to be skewed.","998ee31e":"### Insight:\n\n- <b>'Age'<\/b> : slightly positively skewed\n- <b>'DependentChildren'<\/b> : positively skewed\n- <b>'DependentsOther'<\/b> : positively skewed\n- <b>'WeeklyWages'<\/b> : highly positively skewed (outliers on both ends)\n- <b>'HoursWorkedPerWeek'<\/b> : highly positively skewed (outliers on both ends)\n- <b>'DaysWorkedPerWeek'<\/b> : negatively skewed (outliers on both ends)\n- <b>'InitialIncurredCalimsCost'<\/b> : highly positively skewed (outliers on both ends)\n- <b>'UltimateIncurredClaimCost'<\/b> : highly positively skewed (outliers on both ends)\n- <b>'diff_weeks'<\/b> : highly positively skewed","58ea7438":"### Insights:\n- The seems to be a high class imbalance between the levels in the columns <b>Gender<\/b>, <b>MaritalStatus<\/b> and <b>PartTimeFullTime<\/b>.","799c7d9e":"# 6. Data Modeling\n### Data Frame to be tested on:\n\n- outlier_df1 (DS1)\n- outlier_df2 (DS2)","15af5b16":"# 7. Conclusion:\n### Results:\n<table width=100%>\n    <th>Submission<\/th>\n    <th>Score<\/th>\n    <th>RMSE<\/th>\n    <th>Comment<\/th>\n    <tr align='center'>\n        <td>6.1<\/td>\n        <td>0.7229<\/td>\n        <td>2102.6666<\/td>\n        <td>DS1 basic linear regression<\/td>\n    <\/tr>\n    <tr align='center'>\n        <td>6.2<\/td>\n        <td>0.7205<\/td>\n        <td>2017.4054<\/td>\n        <td>DS2 basic linear regression<\/td>\n    <\/tr>\n    <tr align='center'>\n        <td>6.3<\/td>\n        <td>0.7231<\/td>\n        <td>2101.7731<\/td>\n        <td> DS1 basic linear regression (step wise regression)<\/td>\n    <\/tr>\n    <tr align='center'>\n        <td>6.4<\/td>\n        <td> 0.7206<\/td>\n        <td>2017.3301<\/td>\n        <td>DS2 basic linear regression (step wise regression)<\/td>\n    <\/tr>\n    <tr align='center'>\n        <td>6.5<\/td>\n        <td>0.7174<\/td>\n        <td>2118.4258<\/td>\n        <td>DS1 basic linear regression (full_data_set)<\/td>\n    <\/tr>\n    <tr align='center'>\n        <td>6.6<\/td>\n        <td>0.7027<\/td>\n        <td>2072.6855<\/td>\n        <td>DS2 basic linear regression (full_data_set)<\/td>\n    <\/tr>\n<\/table>\n\n### Interpretations:\n- All the models seem to perform more or less the same.\n\n- <b>Models 6.1 - 6.4 <\/b>seem to give us the least RMSE and the best accuracy amongst all.\n\n- <b>Models 6.5 & 6.6 <\/b>are just extentions of 6.1 and 6.2. The only difference is that they were trained and tested on entire train dataset.\n\n- We can say that general regression model with feature selectin step-wise might reduce the RMSE but difference is not very significant. <b>(for this dataset)<\/b>\n\n- <b>Model 6.4<\/b> would be an ideal choice as <b>step-wise regression reduced the number of columns<\/b>. Making the process faster, <b>yet keeping the accuracy and RMSE same (approx)<\/b>. DS2 is the outlier_df2 dataset which has <b>all the ouliers removed from it<\/b>. ","4337fd4d":"### 6.3 DS1 basic linear regression (step wise regression)","75e356ca":"### Columns with missing values:\n- MaritalStatus                22\n- WeeklyWages                  56\n- HoursWorkedPerWeek           49","955060aa":"### Assumption made:\n- There seems to be <b>29456 \/ 36176<\/b> unique values. There are repeated values in the test_df as well. <u> The value must have been recycled once the claims were resolved as the ClaimNumber has differnt values for all the other column.<\/u>","3de80879":"# 3. Data Trasformation","24e63cd8":"### Insights:\n- The claim amounts seems to be higher when the claim was made just after the accident. rather than when they were made later.","37a44190":"### Insights:\n- The first 2 columns itself help us retain 99.98% of the variance ","78b37210":"# 1. EDA","67320fc4":"### Note:\n- The 2 object type valiables are not to be included in the model so there is no need to transform them.\n\n- The 2 datetime obejcts have been converted to a 3rd column <b>diff_weeks<\/b>.","f5856a8a":"# 4. Feature Selection","0838d264":"### Insight:\n- <i>'DaysWorkedPerWeek'<\/i> and <i>'HoursWorkedPerWeek'<\/i> have a correlation of 0.4, which makes sense as they both refer to the time worked per week in differnt metrics. We will use <u>either one of the 2 in our final model<\/u> as they both provide the same information.\n\n- <i>'InitialIncurredCalimsCost'<\/i> and <i>'UltimateIncurredClaimCost'<\/i> have a correlation of 0.4, which is good as we are trying to predict the UltimateIncurredClaimCost and the <u>Ultimate claim cost will be dependent on the Intial claim cost.<\/u>","0e2e5fb9":"### Data fields\n- ClaimNumber: Unique policy identifier\n- DateTimeOfAccident: Date and time of accident\n- DateReported: Date that accident was reported\n- Age: Age of worker\n- Gender: Gender of worker\n- MaritalStatus: Martial status of worker. (M)arried, (S)ingle, (U)unknown.\n- DependentChildren: The number of dependent children\n- DependentsOther: The number of dependants excluding children\n- WeeklyWages: Total weekly wage\n- PartTimeFullTime: Binary (P) or (F)\n- HoursWorkedPerWeek: Total hours worked per week\n- DaysWorkedPerWeek: Number of days worked per week\n- ClaimDescription: Free text description of the claim\n- InitialIncurredClaimCost: Initial estimate by the insurer of the claim cost\n- UltimateIncurredClaimCost: Total claims payments by the insurance company. This is the field you are asked to predict in the test set.","cf319b96":"# 2. Data Visualisation","c6ac0efb":"### 6.5 DS1 basic linear regression (full_data_set)","950ada37":"### 6.4 DS2 basic linear regression (step wise regression)","19f49ec1":"### Insight:\n\n- <b>'Age'<\/b> : has low number of outliers can be ignored\n\n- <b>'DependentChildren'<\/b> and <b>'DependentsOther'<\/b>: the numbers or outliers are more but removing records on the basis of these columns may result in deletion of valuable records.\n\n- <b>'WeeklyWages'<\/b> : outlier treatment to be performed.\n\n- <b>'DaysWorkedPerWeek'<\/b> and <b>'HoursWorkedPerWeek'<\/b>: removing records from one of them should handle the other column as well because they are related.\n\n- <b>'InitialIncurredCalimsCost'<\/b> : outlier treatment to be performed.\n\n- <b>'UltimateIncurredClaimCost'<\/b> : outlier treatment to be performed.\n\n- <b>'diff_weeks'<\/b> : the numbers or outliers are more but removing records on the basis of these columns may result in deletion of valuable records.","ed037931":"# 5. Test Data Pre-processing","6c4dd3bf":"### 6.1 DS1 basic linear regression"}}