{"cell_type":{"12148e4a":"code","8e2122c2":"code","6095133c":"code","59a99929":"code","14e4964f":"code","2ba7744e":"code","8f849200":"code","48b23883":"code","bdeeea2a":"code","770020d7":"code","3cf7abf4":"code","7508a539":"code","a58a31aa":"markdown","60ddaecc":"markdown","ca73d204":"markdown"},"source":{"12148e4a":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nfrom xgboost import XGBRegressor\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","8e2122c2":"train = pd.read_csv(\"\/kaggle\/input\/30-days-of-ml\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/30-days-of-ml\/test.csv\")\nsub = pd.read_csv(\"\/kaggle\/input\/30-days-of-ml\/sample_submission.csv\")","6095133c":"train.head()","59a99929":"train.columns","14e4964f":"y = train.target\ntrain.drop(['target', 'id'], axis = 1, inplace = True)\ntest.drop(['id'], axis = 1, inplace = True)\ntrain.head()","2ba7744e":"test.head()","8f849200":"print(\"Train shape: \", train.shape, \"\\nTest shape: \", test.shape)","48b23883":"from sklearn.preprocessing import OrdinalEncoder\ncat_cols = [col for col in train.columns if 'cat' in col]\n\nX = train.copy()\nX_test = test.copy()\nenc = OrdinalEncoder()\nX[cat_cols] = enc.fit_transform(train[cat_cols])\nX_test[cat_cols] = enc.transform(test[cat_cols])\nX.head()","bdeeea2a":"from sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import KFold\n\n# Model hyperparameters\nxgb_params = {\n    'n_estimators': 5000,\n    'learning_rate': 0.1235,\n    'subsample': 0.95,\n    'colsample_bytree': 0.11,\n    'max_depth': 2,\n    'booster': 'gbtree', \n    'reg_lambda': 66.1,\n    'reg_alpha': 15.9,\n    'random_state':42\n}\n\"\"\"xgb_params = {'n_estimators': 10000,\n              'learning_rate': 0.35,\n              'subsample': 0.926,\n              'colsample_bytree': 0.84,\n              'max_depth': 2,\n              'booster': 'gbtree', \n              'reg_lambda': 35.1,\n              'reg_alpha': 34.9,\n              'random_state': 42,\n              'n_jobs': 4}\"\"\"","770020d7":"#Setting the kfold parameters\nkf = KFold(n_splits = 10, shuffle = True, random_state = 42)\n\noof_preds = np.zeros((X.shape[0],))\npreds = 0\nmodel_fi = 0\nmean_rmse = 0\n\nfor num, (train_id, valid_id) in enumerate(kf.split(X)):\n    X_train, X_valid = X.loc[train_id], X.loc[valid_id]\n    y_train, y_valid = y.loc[train_id], y.loc[valid_id]\n    \n    model = XGBRegressor(**xgb_params)\n    model.fit(X_train, y_train,\n             verbose = False,\n             eval_set = [(X_train, y_train), (X_valid, y_valid)],\n             eval_metric = \"rmse\",\n             early_stopping_rounds = 100)\n    \n    #Mean of the predictions\n    preds += model.predict(X_test) \/ 10 # Splits\n    \n    #Mean of feature importance\n    model_fi += model.feature_importances_ \/ 10 #splits\n    \n    #Out of Fold predictions\n    oof_preds[valid_id] = model.predict(X_valid)\n    fold_rmse = np.sqrt(mean_squared_error(y_valid, oof_preds[valid_id]))\n    print(f\"Fold {num} | RMSE: {fold_rmse}\")\n    \n    mean_rmse += fold_rmse \/ 10\n    \nprint(f\"\\nOverall RMSE: {mean_rmse}\")","3cf7abf4":"sub.target = preds\nsub.head()","7508a539":"sub.to_csv(\"submission_3.csv\", index = False)\nprint(\"Sent\")","a58a31aa":"In this notebook I'm only using a few things I learned on the lasts days of \"30 Days of ML\" so I'm keeping things as simple as I can.\n# Load and visualize dataset","60ddaecc":"# Modeling","ca73d204":"# Encoding categorical data"}}