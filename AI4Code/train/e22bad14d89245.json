{"cell_type":{"167642de":"code","d3fe8152":"code","2048619e":"code","5419315a":"code","27faa2a1":"code","5d54e4d8":"code","69d4fbeb":"code","8f7955bf":"code","dab85af9":"code","bf31a983":"code","724e079c":"code","750d2cac":"code","37ea1b99":"code","9206b15c":"code","21452811":"code","18178e6e":"code","61f0adda":"code","af4fac61":"code","acf058da":"markdown","dbbbd7be":"markdown","bfa4d2ec":"markdown","d660d31a":"markdown","95329857":"markdown","104df1b2":"markdown","5fba3306":"markdown","f1b68e86":"markdown","a94d79a9":"markdown","8623d591":"markdown","e7076a1b":"markdown","74b6c6c7":"markdown","b8f7a410":"markdown","ce808b38":"markdown","0dc9b31e":"markdown","8aafbac4":"markdown","534cf829":"markdown","23c76fed":"markdown","cf9ab2f9":"markdown","f391459e":"markdown","4410e1a4":"markdown","5a857264":"markdown","c3d83c9d":"markdown","04bce71e":"markdown","cb27380a":"markdown","953f9f79":"markdown","d3d8ed51":"markdown","93e6df2e":"markdown","635616c2":"markdown","afc91a6e":"markdown","2e7bd0c7":"markdown","358698eb":"markdown","502e8749":"markdown","59a85834":"markdown","f9881a74":"markdown","b5f225e6":"markdown","1c5eb9b3":"markdown","9e285575":"markdown","9b4621b4":"markdown","8053a9bd":"markdown","8df87532":"markdown","ef2ca2bd":"markdown","35e04435":"markdown","dad838ab":"markdown","4fc26d84":"markdown","cda8908e":"markdown","3df05eeb":"markdown","15992277":"markdown","5c9a5ffc":"markdown","d811d1aa":"markdown"},"source":{"167642de":"# Importing the necessary libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import cross_val_score\n\nimport statsmodels.formula.api as sm\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost import XGBRegressor\n\nimport warnings\nwarnings.filterwarnings(action='ignore', category=DeprecationWarning)","d3fe8152":"# Upload data\ndata = pd.read_csv('..\/input\/kc_house_data.csv')","2048619e":"# Check if there are missing observations\ndata.info()","5419315a":"# Check some of the observations\ndata.head()","27faa2a1":"# 1- Break down (date) into (year, month & day)\ndata['year'] = data['date'].apply(lambda x: x[:4]).astype(int)\ndata['month'] = data['date'].apply(lambda x: x[4:6]).astype(int)\ndata['day'] = data['date'].apply(lambda x: x[6:8]).astype(int)\n\n# Dropp (id and date)\ndata.drop(['id', 'date'], axis=1, inplace=True)\n\n\n# 2- Round the number of bathrooms\ndata['bathrooms'] = data['bathrooms'].apply(lambda x: round(x, 0))\ndata['bathrooms'] = data['bathrooms'].astype(int)\n\n# Round the number of floors\ndata['floors'] = data['floors'].apply(lambda x: round(x, 0))\ndata['floors'] = data['floors'].astype(int)\n\n\n# 3- Create X & y (independent and dependent variables) vectors to be used in charts and models\nX = data.drop(\"price\",axis=1).values\ny = data[\"price\"].values","5d54e4d8":"# Create a list with the algorithms that will be used to check features importance\ncombine = [RandomForestRegressor(random_state=5), XGBRegressor(random_state=5)]\n\n# Make a list of features' names to label the importance bars with it\ncolumns = data.drop(\"price\",axis=1).columns\n\n# Plot features importance charts when using Random Forest and XGBRegressor algorithms\nfor classifier in combine:\n    classifier.fit(X, y)\n    f, axes = plt.subplots(1, 1, figsize=(12, 4))\n    (pd.Series(classifier.feature_importances_, index=columns)\n       .nlargest(len(classifier.feature_importances_))\n       .plot(kind='barh'))\n    if classifier == combine[0]:\n        plt.title('Random Forest')\n    else:\n        plt.title('XGB Regressor')\n    plt.show()\n    accuracy = cross_val_score(estimator = classifier, X = X, y = y, cv = 10, n_jobs = -1)\n    print('Prices prediction accuracy using this model is: ', str(round((accuracy.mean() * 100), 2)))\n    print('The highest 5 features in terms of importance represent %.2f percent\\n\\n' % (pd.Series(classifier.feature_importances_, index=columns).sort_values(ascending=False)[0:5].sum() * 100))","69d4fbeb":"# Set chart size\nplt.figure(figsize = (17,8))\n\n# Create scatter plot to check the relationship between (lat & price)\nax1 = plt.subplot(221)\nax1 = sns.regplot('lat', 'price', data=data, fit_reg=False, ax=ax1)\n\n# Create scatter plot to check the relationship between (long & price)\nax2 = plt.subplot(222)\nax2 = sns.regplot('long', 'price', data=data, fit_reg=False, ax=ax2)\nplt.show()","8f7955bf":"# Set chart size\nplt.figure(figsize = (17,8))\n\n# Create scatter plot to check the relationship between (sqft_living & price)\nax1 = plt.subplot(221)\nax1 = sns.regplot('sqft_living', 'price', data=data, fit_reg=False, ax=ax1)\n\n# Create scatter plot to check the relationship between (sqft_living15 & price)\nax2 = plt.subplot(222)\nax2 = sns.regplot('sqft_living15', 'price', data=data, fit_reg=False, ax=ax2)\nplt.show()","dab85af9":"# Set chart size\nplt.figure(figsize = (25,8))\n\n# Create scatter plot to check the relationship between (grade & price)\nax1 = plt.subplot(221)\nax1 = sns.regplot('grade', 'price', data=data, fit_reg=False, ax=ax1)\nplt.show()","bf31a983":"# Create a new dataframe\ncopy_data = data.copy()\n\n# Group 'yr_built' values\nfor number in range(1890, 2021, 10):\n    copy_data.loc[(copy_data['yr_built'] > number) & (copy_data['yr_built'] <= (number + 10)), 'yr_built'] = number + 10\n\n# Group 'yr_renovated' values\nfor number in range(1890, 2021, 10):\n    copy_data.loc[(copy_data['yr_renovated'] > number) & (copy_data['yr_renovated'] <= (number + 10)), 'yr_renovated'] = number + 10\n\n# Group 'sqft_lot' values\nfor number in range(0, 1700001, 100000):\n    copy_data.loc[(copy_data['sqft_lot'] > number) & (copy_data['sqft_lot'] <= (number + 100000)), 'sqft_lot'] = number + 100000\n    \n# Group 'sqft_basement' values\nfor number in range(0, 5001, 500):\n    copy_data.loc[(copy_data['sqft_basement'] > number) & (copy_data['sqft_basement'] <= (number + 500)), 'sqft_basement'] = number + 500\n    \n# Group 'sqft_above' values\nfor number in range(0, 10001, 1000):\n    copy_data.loc[(copy_data['sqft_above'] > number) & (copy_data['sqft_above'] <= (number + 1000)), 'sqft_above'] = number + 1000\n\n# Create a list of all features that we will checked against grade\nparameters = ['yr_built', 'yr_renovated', 'sqft_lot', 'sqft_basement', 'sqft_above', 'floors', 'month', 'bedrooms', \n            'condition', 'waterfront', 'view', 'grade']\ncm = sns.light_palette(\"green\", as_cmap=True)\n\n# Display a table for each feature against grade\nfor number in range(0, len(parameters) - 1):\n    display(pd.crosstab(copy_data[parameters[number]], copy_data[parameters[len(parameters) - 1]]).style.background_gradient(cmap = cm))","724e079c":"plt.figure(figsize=(15,12))\nplt.title('Correlation of Features', fontsize=20)\nsns.heatmap(copy_data.corr().astype(float).corr(),vmax=1.0, annot=True)\nplt.show()","750d2cac":"# Create function to check the best hyperparameters for each model\ndef params_checker (algo, parameters, x, y):\n    grid_search = GridSearchCV(estimator = algo, param_grid = parameters, scoring = 'neg_mean_absolute_error', cv = 10, n_jobs = -1)\n    grid_search = grid_search.fit(x, y)\n    \n    # Print the mean absolute error for best parameters reached\n    print(\"- mean absolute error: %.2f\" % ((round(grid_search.best_score_, 2))))","37ea1b99":"# Create a dataframe that will hold each model's prediction accuracy calculated using cross validation.\naccuracy_dataframe = pd.DataFrame(columns=['Model', 'CV_Score'])","9206b15c":"# Aggregate all independent features and add a column of ones at the beginning (since statsmodels 'sm' library doesn't take in consideration the constant coefficient (b0) at the multiple linear regression equation)\nX_multi = np.append(arr = np.ones((len(data), 1)).astype(int), values = X, axis = 1)\n\n# Create an object with the significant features (after performing backward elimination, i removed feature number 20 \"month\" since it had a (P) value above 5% \"its (P) value was 12.8%\")\nX_opt = X_multi [:, [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 21]]\n\n# Fitting the data and checking the R-squared\nclassifier_OLS = sm.OLS(endog = y, exog = X_opt).fit()\nclassifier_OLS.summary()","21452811":"# Create parameters dictionary to use it with the 'params_checker' function\nparams = dict(normalize = [False])\n\n# Use grid search through 'params_checker' function and specify the algorithm, parameters, x & y to get the best parameters that generate the highest prediction accuracy\nparams_checker(LinearRegression(), params, X, y)\n\n# Create object without 'month' feature (since it's not a significant feature according to it's (P) value)\nX_opt = data.drop([data.columns[0], data.columns[len(data.columns)-2]], axis=1).values\n\n# Calculate the accuracy of the model using 10 fold cross validation\naccuracies = cross_val_score(estimator = LinearRegression(), X = X_opt, y = y, cv = 10, n_jobs = -1)\nprint('- Accuracy using 10 folds is:', round((accuracies.mean()) * 100, 3), '%')\n\n# Add the name of the model and the accuracy result to accuracy_dataframe\naccuracy_dataframe.loc[len(accuracy_dataframe)] = 'Multiple Linear', (str(round((accuracies.mean()) * 100, 3)) + ' %')","18178e6e":"# Create parameters dictionary to use it with the 'params_checker' function\nparams = dict(n_estimators=[165], min_samples_split=[3], random_state=[0])\n\n# Use grid search through 'params_checker' function and specify the algorithm, parameters, x & y to get the best parameters that generate the highest prediction accuracy\nparams_checker(RandomForestRegressor(), params, X, y)\n\n# Create object with the best parameters to use it in the cross validation\nalgo = RandomForestRegressor(n_estimators=165, min_samples_split=3, random_state=0)\n\n# Calculate the accuracy of the model using 10 fold cross validation\naccuracies = cross_val_score(estimator = algo, X = X, y = y, cv = 10, n_jobs = -1)\nprint('- Accuracy using 10 folds is:', round((accuracies.mean()) * 100, 3), '%')\n\n# Add the name of the model and the accuracy result to accuracy_dataframe\naccuracy_dataframe.loc[len(accuracy_dataframe)] = 'Random Forest', (str(round((accuracies.mean()) * 100, 3)) + ' %')","61f0adda":"# Create parameters dictionary to use it with the 'params_checker' function\nparams = dict(max_depth=[7], learning_rate=[0.1], n_estimators=[350], gamma=[0.00001], min_child_weight=[3], colsample_bytree=[0.7])\n\n# Use grid search through 'params_checker' function and specify the algorithm, parameters, x & y to get the best parameters that generate the highest prediction accuracy\nparams_checker(XGBRegressor(), params, X, y)\n\n# Create object with the best parameters to use it in the cross validation\nalgo = XGBRegressor(max_depth=7, learning_rate=0.1, n_estimators=350, gamma=0.00001, min_child_weight=3, colsample_bytree=0.7)\n\n# Calculate the accuracy of the model using 10 fold cross validation\naccuracies = cross_val_score(estimator = algo, X = X, y = y, cv = 10, n_jobs = -1)\nprint('- Accuracy using 10 folds is:', round((accuracies.mean()) * 100, 3), '%')\n\n# Add the name of the model and the accuracy result to accuracy_dataframe\naccuracy_dataframe.loc[len(accuracy_dataframe)] = 'XGBRegressor', (str(round((accuracies.mean()) * 100, 3)) + ' %')","af4fac61":"accuracy_dataframe = accuracy_dataframe.sort_values(['CV_Score'], ascending=False)\naccuracy_dataframe.reset_index(drop=True)","acf058da":"#### If we check the above charts, we will see that Random Forest Regressor expects that the highest 5 features in term of importance will contribute with around 85% in predicting prices. While according to XGBRegressor, their contribution will be around 58%.\n\n#### Based on the above assumption, we will check the top 5 most important features (lat,  long, sqft_living, sqft_living15, grade) and see their relation with prices.","dbbbd7be":"#### At this stage, it's time to see which algorithm will predict the prices with highest accuracy rate. Starting off by creating a function to check the best hyperparameters for each model:","bfa4d2ec":"## 2.1 Features Importance:","d660d31a":"### First, we upload the data and check if there are missing values:","95329857":"#### From the above cross tables, we can see how each grouped values related to each feature can affect the determination of grades. Another way to see these correlations is to check the corresponding correlation coefficient below:","104df1b2":"## 2.5 Grades against other features:","5fba3306":"#### We can see that the values of R-squared and accuracy obtained from performing cross validatoin are almost the same (70%) which indicates that, according to multiple linear regression, the independent features can explain almost 70% of the changes in prices.","f1b68e86":"#### According to the above plots, we can notice that the most expensive houses are in the north west (lat: 47.63, long: -122.2). Prices start to decrease by heading south and east.","a94d79a9":"#### We also can see that living area matters in determining the price.","8623d591":"## 3.3 XGBRegressor:","e7076a1b":"## 3.2 Random Forest:","74b6c6c7":"#### A positive correlation is illustrated indicating that prices increase with higher grades. What about the relashipship between grades and other features? In other words, to what extent other features determine the grade?","b8f7a410":"---","ce808b38":"#### Now, it's time to check how important each feature in the prediction process of the prices using two different algorithms (Random Forest and XGB Regressor):","0dc9b31e":"# Table of Contents:","8aafbac4":"#### Create a dataframe that holds the predictions' accuracies obtained from each model:","534cf829":"## 3.4 Models accuracies","23c76fed":"#### Check the best hyperparameters and its corresponding prediction accuracy:","cf9ab2f9":"## 2.2 Lat and long plotting against prices:","f391459e":"#### By having a look at the features, i think it would be better to:\n\n1- Break down (date) into 3 new columns (year, month & day) and then drop (date & id) columns.<br>\n2- Round the number of bathrooms and floors since representing their number as fraction wouldn't make sense.<br>\n3- Include all independent features in the analysis.","4410e1a4":"---","5a857264":"---","c3d83c9d":"#### According to the above results, XGBRegressor model has the best price prediction accuracy.","04bce71e":"## 2.4 Grades against prices:","cb27380a":"#### Checking the relationship between geographical locations and prices: ","953f9f79":"#### Check the best hyperparameters and its prediction accuracy:","d3d8ed51":"---","93e6df2e":"---","635616c2":"---","afc91a6e":"#### Checking the relationship between grades and prices: ","2e7bd0c7":"---","358698eb":"# 1. Data Exploration:","502e8749":"---","59a85834":"## Importing Libraries:","f9881a74":"---","b5f225e6":"# 2. Data Visualization:","1c5eb9b3":"* **1. Data Exploration:**\n   * 1.1 Data Completeness.\n   * 1.2 Features Engineering.\n* **2. Data Visualization:**\n  * 2.1 Features Importance.<br>\n  * 2.2 Lat and long plotting against prices.<br>\n  * 2.3 The areas of living against prices.<br>\n  * 2.4 Grades against prices.<br>\n  * 2.5 Grades against other features.<br>\n* **3. Modeling:**\n  * 3.1 Regression using Multiple Linear Regression.<br>\n  * 3.2 Regression using Random Forest.<br>\n  * 3.3 Regression using XGBRegressor.<br>\n  * 3.4 Models accuracies.","9e285575":"## 1.1 Data Completeness:","9b4621b4":"#### Checking the relationship between living areas and prices: ","8053a9bd":"## 2.3 The areas of living against prices:","8df87532":"---","ef2ca2bd":"---","35e04435":"## 1.2 Features Engineering:","dad838ab":"---","4fc26d84":"## 3.1 Multiple Linear Regression:","cda8908e":"#### Now, let's have a look on the scores for each model ranked from highest to lowest:","3df05eeb":"# 3. Modeling:","15992277":"#### Check the best hyperparameters and its corresponding prediction accuracy:","5c9a5ffc":"#### Check the significance of each feature \"(P) value\":","d811d1aa":"---"}}