{"cell_type":{"776e2554":"code","693e5405":"code","b15518ce":"code","e7022c2f":"markdown","8fa555c9":"markdown","3957d9c0":"markdown","bd1945cd":"markdown","cd01a977":"markdown","ff69d3e8":"markdown","e94e8a08":"markdown"},"source":{"776e2554":"from IPython.display import YouTubeVideo      \nYouTubeVideo('FURNYtoLKvQ')","693e5405":"import re\ndef ngram(text,n):\n    token = re.split(\"\\\\s+\", text)\n    ngrams=[]\n    for i in range(len(token)-n+1):\n        temp =[token[j]for j in range(i,i+n)]\n        ngrams.append(\" \".join(temp))\n    return ngrams","b15518ce":"text = \"Movie is fantastically bad, still had a good time with my friends\"\nngram(text,5)","e7022c2f":"## n-gram appication\n\n 1. Spelling Correction\n   \n  \"wow\" => \"wo\" \"ow\" => input = woo ==> wow,\n  \n 2. Next word prediction\n  \n  S1 = How r u \n  s2 = r u good \n  s3 = r u there \n  s4 = How r u doing  \n  \n  How r u - 2\n  r u good - 1 \n  r u there - 1\n  r u doing - 1\n  \n  whats our next word in below phrase.....u\/good\/there\/doing??????\n  \n  How r .....UUU\n  \n      \n 3. Word breaking\n  \n 4. text summarization\n  .......\n  \n  In our next tutorial, we will train the model by implemeting the logics ","8fa555c9":"## What is n-gram\n\n    used in text mining and NLP \n    continuos sequence of N tokens","3957d9c0":"Hello,\n\nYou can go also through the video explanation of this notebook below with technical rythm:)\n\nhttps:\/\/youtu.be\/FURNYtoLKvQ","bd1945cd":"# Steps towards word2vec\n  \n  Baics of DeepLearning","cd01a977":"## Why n-gram","ff69d3e8":"## How to use n-gram\n\nLets start with Character level\n\nSentence = ['I am good']\n\n1-gram\/unigram = [i, ,a,m, ,g,o,o,d]\n2-gram\/bigram  = ['i ',' a','am','m ',' g','go','oo','od']","e94e8a08":"# NLP  n-gram"}}