{"cell_type":{"254992a9":"code","eaca89ed":"code","debe5567":"code","2ec9f6a4":"code","7def9ce9":"code","ef8b4154":"code","c914bfe6":"code","398a5055":"code","b3232d81":"code","afbf8499":"code","783c007c":"code","f26c46c3":"code","7390c8e2":"code","fa4ce9ea":"code","5fa237c5":"code","de4c377f":"code","7f07fc80":"code","e5803fb5":"code","bbae14bc":"code","99987bac":"code","f6259c07":"code","1acf870a":"code","20fc38f5":"code","83905cbe":"code","ec2ac4fc":"code","851fbc7e":"code","5d3517c9":"markdown","9ba10e78":"markdown"},"source":{"254992a9":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd# data processing, CSV file I\/O (e.g. pd.read_csv)\nimport re\nimport nltk \nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom bs4 import BeautifulSoup\nfrom nltk.tokenize import word_tokenize\nimport spacy\nimport unidecode\nfrom word2number import w2n\n\n\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","eaca89ed":"#assign postive & negative score \ndf=pd.read_csv('\/kaggle\/input\/amazon-fine-food-reviews\/Reviews.csv')\ndf['Score']=df['Score'].apply(lambda x : 'positve' if x>3 else ('negative' if x<3 else 'neutral'))\n\n\n","debe5567":"#visualising the sentiment percentage in dataset before cleaning.\n\nplot_size = plt.rcParams[\"figure.figsize\"] \nprint(plot_size[0]) \nprint(plot_size[1])\n\nplot_size[0] = 10\nplot_size[1] = 8\nplt.rcParams[\"figure.figsize\"] = plot_size \n\ndf.Score.value_counts().plot(kind='pie', autopct='%1.0f%%')","2ec9f6a4":"# removing neutral score as of now\n#df.drop(df.index[df['Score']=='neutral'],inplace=True)\n#df.head()","7def9ce9":"#Sorting data for cleaning purpose\n\nsorted_data= df.sort_values('ProductId',axis=0,ascending =True)\nsorted_data.head()","ef8b4154":"sorted_data.shape","c914bfe6":"#removing duplicate entries for cleaning purpose\n\nfinal= sorted_data.drop_duplicates(subset={'UserId','ProfileName','Time','Text'},keep ='first',inplace=False)\nfinal.head()","398a5055":"final.shape","b3232d81":"#checking how much data is left after cleaning\n\nfinal['Id'].size\/df['Id'].size * 100","afbf8499":"#checking the obvious true condition helpfullnumer<=helpfulldenom \n\nfinal[final['HelpfulnessNumerator']>final['HelpfulnessDenominator']]","783c007c":"# cleaning according to above condition error entries\nfinal=final[final['HelpfulnessNumerator']<=final['HelpfulnessDenominator']]\nprint(final.shape)\n\n#how many postive & negative reviews are in our data set\n\nfinal['Score'].value_counts()","f26c46c3":"#checking if our feature i.e text is having any null values or not \n\nfinal['Text'].isnull().sum()","7390c8e2":"#visualising the sentiment percentage in dataset after cleaning.\n\nfinal.Score.value_counts().plot(kind='pie', autopct='%1.0f%%')","fa4ce9ea":"# taking only desired columns i.e [Text] that is our feature & [Score] which is label\n\nfinal= final[['Text','Score']]\nprint(final.shape)\nfinal.head()\n","5fa237c5":"from spacy import displacy\nfrom spacy.util import minibatch, compounding\n\nnlp = spacy.load('en_core_web_sm')","de4c377f":"# creating text preprocessing function\n\ndef strip_html_tags(text):\n    \"\"\"remove html tags from text\"\"\"\n    soup = BeautifulSoup(text, \"html.parser\")\n    stripped_text = soup.get_text(separator=\" \")\n    return stripped_text\n\n# exclude words from spacy stopwords list\ndeselect_stop_words = ['no', 'not']\nfor w in deselect_stop_words:\n    nlp.vocab[w].is_stop = False\n    \n\ndef remove_whitespace(text):\n    \"\"\"remove extra whitespaces from text\"\"\"\n    text = text.strip()\n    return \" \".join(text.split())\n\n\ndef remove_accented_chars(text):\n    \"\"\"remove accented characters from text, e.g. caf\u00e9\"\"\"\n    text = unidecode.unidecode(text)\n    return text    \ndef text_preprocessing(text, accented_chars=True, contractions=True, \n                       convert_num=True, extra_whitespace=True, \n                       lemmatization=True, lowercase=True, punctuations=True,\n                       remove_html=True, remove_num=True, special_chars=True, \n                       stop_words=True):\n    \"\"\"preprocess text with default option set to true for all steps\"\"\"\n    if remove_html == True: #remove html tags\n        text = strip_html_tags(text)\n    if extra_whitespace == True: #remove extra whitespaces\n        text = remove_whitespace(text)\n    if accented_chars == True: #remove accented characters\n        text = remove_accented_chars(text)\n    if lowercase == True: #convert all characters to lowercase\n        text = text.lower()\n        \n\n    doc = nlp(text) #tokenise text\n\n    clean_text = []\n    \n    for token in doc:\n        flag = True\n        edit = token.text\n        # remove stop words\n        if stop_words == True and token.is_stop and token.pos_ != 'NUM': \n            flag = False\n        # remove punctuations\n        if punctuations == True and token.pos_ == 'PUNCT' and flag == True: \n            flag = False\n        # remove special characters\n        if special_chars == True and token.pos_ == 'SYM' and flag == True: \n            flag = False\n        # remove numbers\n        if remove_num == True and (token.pos_ == 'NUM' or token.text.isnumeric()) \\\n        and flag == True:\n            flag = False\n        # convert number words to numeric numbers\n        if convert_num == True and token.pos_ == 'NUM' and flag == True:\n            edit = w2n.word_to_num(token.text)\n        # convert tokens to base form\n        elif lemmatization == True and token.lemma_ != \"-PRON-\" and flag == True:\n            edit = token.lemma_\n        # append tokens edited and not removed to list \n        if edit != \"\" and flag == True:\n            clean_text.append(edit)        \n    return clean_text        ","7f07fc80":"final['review']=final['Text'].apply(text_preprocessing)   ","e5803fb5":"final.head()","bbae14bc":"final['review']=final['review'].astype(str)\nfinal['review']=final['review'].replace('[','').replace(']','').replace(',',' ')\nfinal.head()","99987bac":"from sklearn.feature_extraction.text import TfidfVectorizer\n\nvectorizer = TfidfVectorizer (max_features=2500, min_df=7, max_df=0.8)\nfinal_counts = vectorizer.fit_transform(final['review'].values)","f6259c07":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(final_counts, final['Score'], test_size=0.2, random_state=0)","1acf870a":"# checking percentage of lables in training set\ny_train.value_counts().plot(kind='pie', autopct='%1.0f%%')","20fc38f5":"# checking percentage of lables in test set\ny_test.value_counts().plot(kind='pie', autopct='%1.0f%%')","83905cbe":"from sklearn.ensemble import RandomForestClassifier\n\ntext_classifier = RandomForestClassifier(n_estimators=200, random_state=0)\ntext_classifier.fit(X_train, y_train)","ec2ac4fc":"predictions = text_classifier.predict(X_test)","851fbc7e":"from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n\nprint(confusion_matrix(y_test,predictions))\nprint(classification_report(y_test,predictions))\nprint(accuracy_score(y_test, predictions))","5d3517c9":"so after cleaning the duplicate entries percentage of posiive ,negative and neutral reviews remains the same.","9ba10e78":"STEP-2\/\/\/ **TEXT PREPROCESSING **"}}