{"cell_type":{"7875bf67":"code","1e010ee9":"code","601517dc":"code","90ec15de":"code","9248f958":"code","043c4387":"code","5d5843ef":"code","46e1d66c":"code","160a65e5":"code","c8a29501":"code","e9b3ffe0":"code","bc2f6751":"code","54386302":"code","8efacf5b":"code","dde3fc02":"code","81f40969":"code","8a30716f":"code","1790dc47":"code","7b464d63":"code","50e5adb7":"code","c1113d4d":"code","6b3f2a7d":"code","cf0cd046":"code","c5e88707":"code","bc115e3d":"code","318335dd":"code","811b2711":"code","92ddc21c":"code","43ecd16f":"markdown","866f2f1c":"markdown","f0f4b9c0":"markdown","fed0e706":"markdown","86726623":"markdown","61107cd3":"markdown","b25a9ddf":"markdown","90dd96af":"markdown","7e585e36":"markdown","5962ccec":"markdown","c90c8c2a":"markdown","16ea8b74":"markdown","9d75f8db":"markdown"},"source":{"7875bf67":"import pandas as pd\nimport numpy as np\nfrom tqdm.auto import tqdm\nfrom bs4 import BeautifulSoup\nfrom collections import defaultdict\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n\nimport re \nimport scipy\nfrom scipy import sparse\n\nfrom IPython.display import display\nfrom pprint import pprint\nfrom matplotlib import pyplot as plt \n\nimport time\nimport scipy.optimize as optimize\nimport warnings\nwarnings.filterwarnings(\"ignore\")\npd.options.display.max_colwidth=300\npd.options.display.max_columns = 100\n\nfrom sklearn.model_selection import train_test_split\nfrom nltk.tokenize import word_tokenize\nfrom sklearn.linear_model import Ridge","1e010ee9":"df_train = pd.read_csv(\"..\/input\/jigsaw-toxic-comment-classification-challenge\/train.csv\")\n# df_test = pd.read_csv(\"..\/input\/jigsaw-unintended-bias-in-toxicity-classification\/train.csv\")\n# df_test_label = pd.read_csv(\"..\/input\/jigsaw-toxic-comment-classification-challenge\/test_labels.csv\").replace(-1,0)\ndf_sub = pd.read_csv(\"..\/input\/jigsaw-toxic-severity-rating\/comments_to_score.csv\")","601517dc":"df_train.head(2)","90ec15de":"# Create a score that measure how much toxic is a comment\ncat_mtpl = {'obscene': 0.16, 'toxic': 0.32, 'threat': 1.5, \n            'insult': 0.64, 'severe_toxic': 1.5, 'identity_hate': 1.5}\n\nfor category in cat_mtpl:\n    df_train[category] = df_train[category] * cat_mtpl[category]\n\ndf_train['score'] = df_train.loc[:, 'toxic':'identity_hate'].sum(axis=1)\n\ndf_train['y'] = df_train['score']\n\nmin_len = (df_train['y'] > 0).sum()  # len of toxic comments\ndf_y0_undersample = df_train[df_train['y'] == 0].sample(n=min_len, random_state=201)  # take non toxic comments\ndf_train_new = pd.concat([df_train[df_train['y'] > 0], df_y0_undersample])  # make new df\ndf_train_new.head(2)","9248f958":"df_train = df_train.rename(columns={'comment_text':'text'})","043c4387":"def text_cleaning(text):\n    '''\n    Cleans text into a basic form for NLP. Operations include the following:-\n    1. Remove special charecters like &, #, etc\n    2. Removes extra spaces\n    3. Removes embedded URL links\n    4. Removes HTML tags\n    5. Removes emojis\n    \n    text - Text piece to be cleaned.\n    '''\n    template = re.compile(r'https?:\/\/\\S+|www\\.\\S+') #Removes website links\n    text = template.sub(r'', text)\n    \n    soup = BeautifulSoup(text, 'lxml') #Removes HTML tags\n    only_text = soup.get_text()\n    text = only_text\n    \n    emoji_pattern = re.compile(\"[\"\n                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                               u\"\\U00002702-\\U000027B0\"\n                               u\"\\U000024C2-\\U0001F251\"\n                               \"]+\", flags=re.UNICODE)\n    text = emoji_pattern.sub(r'', text)\n    \n    text = re.sub(r\"[^a-zA-Z\\d]\", \" \", text) #Remove special Charecters\n    text = re.sub(' +', ' ', text) #Remove Extra Spaces\n    text = text.strip() # remove spaces at the beginning and at the end of string\n\n    return text","5d5843ef":"tqdm.pandas()\ndf_train['text'] = df_train['text'].progress_apply(text_cleaning)","46e1d66c":"df = df_train.copy()","160a65e5":"df['y'].value_counts()","c8a29501":"df['y'].value_counts(normalize=True)","e9b3ffe0":"min_len = (df['y'] >= 0.1).sum()\ndf_y0_undersample = df[df['y'] == 0].sample(n=min_len, random_state=201)\ndf = pd.concat([df[df['y'] >= 0.1], df_y0_undersample])\ndf['y'].value_counts()","bc2f6751":"# vec = TfidfVectorizer(min_df= 3, max_df=0.5, analyzer = 'char_wb', ngram_range = (3,5))\n# vec = TfidfVectorizer(min_df= 3, max_df=0.5, analyzer = 'char_wb', ngram_range = (3,6))\nvec = TfidfVectorizer(min_df= 3, max_df=0.5, analyzer = 'char_wb', ngram_range = (2,5),max_features=46000)\nX = vec.fit_transform(df['text'])\nX","54386302":"model = Ridge(alpha=0.5)\n# model = Ridge(alpha=0.485)\nmodel.fit(X, df['y'])","8efacf5b":"df_val = pd.read_csv(\"..\/input\/jigsaw-toxic-severity-rating\/validation_data.csv\")","dde3fc02":"df_val.head()","81f40969":"# tqdm.pandas()\n# df_val['less_toxic'] = df_val['less_toxic'].progress_apply(text_cleaning)\n# df_val['more_toxic'] = df_val['more_toxic'].progress_apply(text_cleaning)","8a30716f":"# X_less_toxic = vec.transform(df_val['less_toxic'])\n# X_more_toxic = vec.transform(df_val['more_toxic'])","1790dc47":"# p1 = model.predict(X_less_toxic)\n# p2 = model.predict(X_more_toxic)","7b464d63":"# # Validation Accuracy\n# (p1 < p2).mean()","50e5adb7":"df_sub = pd.read_csv(\"..\/input\/jigsaw-toxic-severity-rating\/comments_to_score.csv\")\n","c1113d4d":"tqdm.pandas()\ndf_sub['text'] = df_sub['text'].progress_apply(text_cleaning)","6b3f2a7d":"X_test = vec.transform(df_sub['text'])\np3 = model.predict(X_test)","cf0cd046":"df_sub['score'] = p3","c5e88707":"df_sub['score'].count()","bc115e3d":"df_sub['score'] = df_sub['score'] ","318335dd":"# 9 comments will fail if compared one with the other\ndf_sub['score'].nunique()","811b2711":"df_sub[['comment_id', 'score']].to_csv(\"submission.csv\", index=False)","92ddc21c":"df_sub","43ecd16f":"# Undersampling","866f2f1c":"<h2>Prepare submission file<\/h2>","f0f4b9c0":"# TF-IDF","fed0e706":"# Prepare train data","86726623":"<h2>Text cleaning<\/h2>","61107cd3":"# Import Library","b25a9ddf":"<h1>Fit Ridge<\/h1>","90dd96af":"<h2>Text cleaning<\/h2>","7e585e36":"# Prepare validation data","5962ccec":"<h2>Prediction<\/h2>","c90c8c2a":"<h3>Text Cleaning<\/h3>","16ea8b74":"# Prepare submission data ","9d75f8db":"<h2>\u30100.860\u3011TFIDF_Ridge_simple_baseline<\/h2>\n\nData from [Toxic Comment Classification Challenge](https:\/\/www.kaggle.com\/c\/jigsaw-toxic-comment-classification-challenge)\n"}}