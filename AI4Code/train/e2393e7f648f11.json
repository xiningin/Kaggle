{"cell_type":{"52181996":"code","0885c588":"code","94cbbf3f":"code","52fe63d1":"code","72b8ea5d":"code","333cd543":"code","54cd3728":"code","7e02b103":"code","e27aae7d":"code","e9899483":"code","42078e9e":"code","7f2dfa2b":"code","0fbc77d2":"code","9c46b701":"code","83ea3b5c":"code","44d6f48c":"code","a528dbfa":"code","1d47ec94":"code","b6537db6":"code","3dd3c525":"code","4c5a20da":"code","4df341e8":"code","b4d3fbb5":"code","f7654f16":"code","342c9c7a":"code","e2f42752":"code","d0db6c38":"code","deeb1919":"code","755bf4c6":"code","ea37dfad":"code","3e0b0306":"code","cadef1e0":"code","2b04a839":"code","5662f4f0":"code","77e7dd77":"code","410dfacb":"code","e570f54d":"code","07c7a315":"code","c2ce0564":"code","d20f1b9a":"code","58f2dadb":"code","04eb7533":"code","b018b3cf":"code","b39dceb5":"code","43b356cc":"code","b9f2bc2d":"code","b1b7a2e1":"code","5153ce90":"code","f6bed75d":"code","f68e8886":"code","5b819191":"markdown","56e0cfb6":"markdown","6f963e72":"markdown","9153981d":"markdown","578788ba":"markdown","f3a88ef7":"markdown","f863afd8":"markdown","9a00bc74":"markdown","3bb84d23":"markdown","7c62610b":"markdown","d910ab49":"markdown","6b67681b":"markdown","bb56c199":"markdown","5330d445":"markdown","3fd3cd1e":"markdown","6ab4ad31":"markdown","962c365b":"markdown","0db97a27":"markdown","09c3a306":"markdown","31d5a437":"markdown","039d5e74":"markdown","bf5ca421":"markdown","96e9e0fb":"markdown","33152d22":"markdown","23594967":"markdown","81599b57":"markdown","350e2071":"markdown","8b606dc5":"markdown","51067ce6":"markdown","dc2e4b1d":"markdown","990a20f7":"markdown","37299e49":"markdown","256a57b4":"markdown","cc5411c0":"markdown","4a1e483b":"markdown","feb5f18d":"markdown","c608da37":"markdown","c4d7c2e5":"markdown","b31016e4":"markdown","01bf0cb3":"markdown","7f3fb3cc":"markdown","4ae92921":"markdown","aa30bbd8":"markdown","b347fa0d":"markdown","f7b1b2f1":"markdown","2ca6d38c":"markdown","26c989da":"markdown","40ba537b":"markdown","cde236b0":"markdown","67558503":"markdown","cf5bd0ab":"markdown","2d1371c0":"markdown","610dc261":"markdown","457c24df":"markdown","320aa37e":"markdown","35da53f6":"markdown","36826c98":"markdown","08925ee8":"markdown","40a08d57":"markdown","99d66654":"markdown","7169a36d":"markdown","47c4f888":"markdown","92b7471a":"markdown"},"source":{"52181996":"# Import packages\nimport pandas as pd\nimport numpy as np\nfrom plotnine import * # using * saves me from writing plotnine before every ggplot use\n!pip install dfply # seems like dfply is not installed on kaggle\nfrom dfply import *\nimport folium\nimport statsmodels.api as sm\nimport itertools\nfrom sklearn.cross_decomposition import PLSRegression\nfrom sklearn.decomposition import PCA\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\nfrom sklearn.linear_model import LinearRegression, Ridge, RidgeCV, Lasso, LassoCV\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split, cross_val_score, KFold, ParameterGrid\nfrom sklearn.preprocessing import scale\nfrom sklearn.tree import DecisionTreeRegressor, plot_tree\nimport matplotlib.pyplot as plt\n\n# from mlxtend.feature_selection import ExhaustiveFeatureSelector as EFS\n\n# import data\nhouse_data = pd.read_csv(\"..\/input\/housesalesprediction\/kc_house_data.csv\")","0885c588":"# overview of data\nhouse_data.info()","94cbbf3f":"# are any data missing?\nprint('Number of missing data:', house_data.isnull().sum().sum()) ","52fe63d1":"# note that ggplot can be used in python, using the plotnine library\n\nprice_plot_theme = theme(axis_title = element_text(size = 12.5), \n                         axis_text = element_text(size = 9))\n\nprice_plot = (ggplot(data = house_data, mapping = aes(x = 'price')) + \n  # note that the whole object is in brackets - (ggplot...)\n  # also note that aes is preceded by mapping = and the column name is in quotes\n  geom_histogram(color = 'black', bins = 30) +\n  # makes a boxplot, bins = 30 was the value defaulted by R\n  geom_vline(xintercept = house_data['price'].mean(), linetype = 'dashed', \n             color = 'red', size = 2) +\n  # add line denoting mean house price - note how the mean value is called is\n  # different than R, you use the data first and select the column using [''],\n  # followed by .mean()\n  # size = 2 replaces lwd = 2 from R\n  theme_classic() + # white background, no gridlines\n  xlab('Price (US$)') + # change x axis label\n  ylab('Frequency') + # change y axis label\n  price_plot_theme + # change size of axis titles and text\n  scale_x_continuous(trans = 'log',\n                     breaks = np.array([100000, 250000, 500000, 1000000, \n                                        2500000, 5000000]),\n                     labels = np.array(['100,000', '250,000', '500,000', \n                                        '1,000,000', '2,500,000', \n                                        '5,000,000'])) +\n  scale_y_continuous(breaks = np.array(range(0, 3500, 500)),\n                     labels = np.array(range(0, 3500, 500)),\n                     limits = np.array([0, 3000]))\n # change x and y axis values\n # note that instead of using c() in R, numpy arrays are used\n # also note that in range(), the second number denotes when the range stops,\n # but that final number is not included - so if I want the numbers 0-3000\n # every 500, the stop number has to be 3500\n )\nprice_plot","72b8ea5d":"# dfply is a python equivalent to dplyr and has pipes that can be used to\n# chain multiple bits of code together\n\n# houses built over time\n# collate number of houses built per year\nyearBuilt = (house_data >> # note that >> replaces %>% as the pipe\n  group_by(X.yr_built) >>  # X refers to the data frame from the first line\n  # and has to be called explicitly unlike in R\n  summarize(rows = n(X.yr_built))\n  )\n\nyearBuilt_plot_theme = theme(axis_title = element_text(size = 12.5), \n                             axis_text = element_text(size = 10))\nyearBuilt_plot = (ggplot(data = yearBuilt, \n                         mapping = aes(x = 'yr_built', y = 'rows')) +\n  geom_bar(stat = 'identity', color = 'black') + # create bar chart\n  theme_classic() + # white background, no gridlines\n  xlab('Year') + # change x axis label\n  ylab('Houses Built') + # change y axis label\n  yearBuilt_plot_theme + # change the size of axis titles and axis text\n  scale_x_continuous(breaks = np.array(range(1900, 2020, 10)),\n                     labels = np.array(range(1900, 2020, 10)),\n                     limits = np.array([1899, 2016])) +\n  scale_y_continuous(breaks = np.array(range(0, 650, 50)),\n                     labels = np.array(range(0, 650, 50)),\n                     limits = np.array([0, 600]))\n  # change x and y axis values\n  )\nyearBuilt_plot","333cd543":"# mean price of house per year built\n# collate current mean price of house per year built\nyearBuilt_price = (house_data >> # using house_data\n  group_by(X.yr_built) >> # group all the data from the same year\n  summarize(mean_price = mean(X.price)) >>\n  # calculate current mean price for houses built in that year\n  arrange(X.yr_built)\n  )\n\nyearBuilt_price_plot_theme = theme(axis_title = element_text(size = 12.5), \n                                   axis_text = element_text(size = 10))\nyearBuilt_price_plot = (ggplot(data = yearBuilt_price, \n                               mapping = aes(x = 'yr_built', \n                                             y = 'mean_price')) +\n  geom_bar(stat = 'identity', color = 'black') + # create bar chart\n  theme_classic() + # white background, no gridlines\n  xlab('Year') + # change x axis label\n  ylab('Mean Price (US$)') + # change y axis label\n  yearBuilt_price_plot_theme + \n  # change the size of axis titles and axis text\n  geom_hline(yintercept = house_data['price'].mean(),\n             linetype = 'dashed', color = 'red', size = 2) +\n  # add line denoting mean house price\n  scale_x_continuous(breaks = np.array(range(1900, 2020, 10)),\n                     labels = np.array(range(1900, 2020, 10)),\n                     limits = np.array([1899, 2016])) +\n  scale_y_continuous(breaks = np.array(range(0, 900000, 100000)),\n                     labels = np.array(['0', '100,000', '200,000', '300,000',\n                                        '400,000', '500,000', '600,000',\n                                        '700,000', '800,000']))\n # change x and y axis values\n )\nyearBuilt_price_plot","54cd3728":"# Square footage of living area of surrounding 15 houses\nsqft_liv15_plot_theme = theme(axis_title = element_text(size = 12.5), \n                              axis_text = element_text(size = 10))\nsqft_liv15_plot = (ggplot(data = house_data, \n                          mapping = aes(x = 'sqft_living15', y = 'price')) +\n  geom_point(size = 1) +\n  # add data as points\n  theme_classic() + # white background, no gridlines\n  xlab('Square Footage of Living Area of Nearest 15 Houses') + \n  # change x axis label\n  ylab('Price (US$)') + # change y axis label\n  sqft_liv15_plot_theme + # change the size of axis titles and axis text\n  scale_x_continuous(trans = 'log',\n                     breaks = np.array([500, 1000, 2500, 5000]),\n                     labels = np.array(['500', '1,000', '2,500', '5,000'])) +\n  scale_y_continuous(trans = 'log',\n                     breaks = np.array([100000, 250000, 500000, 1000000, \n                                        2500000, 5000000]),\n                     labels = np.array(['100,000', '250,000', '500,000', \n                                        '1,000,000', '2,500,000', '5,000,000'])) \n # change x and y axis values\n )\nsqft_liv15_plot","7e02b103":"# Square footage of lot of surrounding 15 houses\nsqft_lot15_plot_theme = theme(axis_title = element_text(size = 12.5), \n                              axis_text = element_text(size = 10))\nsqft_lot15_plot = (ggplot(data = house_data, \n                          mapping = aes(x = 'sqft_lot15', y = 'price')) +\n  geom_point(size = 1) +\n  # add data as points\n  theme_classic() + # white background, no gridlines\n  xlab('Square Footage of Lot of Nearest 15 Houses') + \n  # change x axis label\n  ylab('Price (US$)') + # change y axis label\n  sqft_lot15_plot_theme + # change the size of axis titles and axis text\n  scale_x_continuous(trans = 'log',\n                     breaks = np.array([1000, 10000, 100000, 1000000]),\n                     labels = np.array(['1,000', '10,000', '100,000', \n                                        '1,000,000'])) +\n  scale_y_continuous(trans = 'log',\n                     breaks = np.array([100000, 250000, 500000, 1000000, \n                                        2500000, 5000000]),\n                     labels = np.array(['100,000', '250,000', '500,000', \n                                        '1,000,000', '2,500,000', '5,000,000']))\n # change x and y axis values\n )\nsqft_lot15_plot","e27aae7d":"# waterfront\nwft_plot_theme = theme(axis_title = element_text(size = 12.5),\n                       axis_text = element_text(size = 10))\n\nwft_plot = (ggplot(data = house_data, \n                   mapping = aes(x = house_data['waterfront'].astype('category'), \n                                 y = 'price')) + # note that instead of using\n            # factor() in R, in python you specify the column and use \n            # astype('category')\n  geom_boxplot() + # makes a boxplot\n  geom_hline(yintercept = house_data['price'].mean(),\n             linetype = 'dashed', color = 'red', size = 2) +\n  # add line denoting mean house price\n  theme_classic() + # white background, no gridlines\n  xlab('') +\n  ylab('Price (US$)') + # change y axis label\n  wft_plot_theme + # change axis title and text size\n  scale_x_discrete(labels = np.array(['Not on Waterfront', 'On Waterfront'])) +\n  scale_y_continuous(trans = 'log',\n                     breaks = np.array([100000, 250000, 500000, 1000000, \n                                        2500000, 5000000]),\n                     labels = np.array(['100,000', '250,000', '500,000', \n                                        '1,000,000', '2,500,000', '5,000,000']))\n  # change y-axis values\n  )\nwft_plot","e9899483":"# view\nview_plot_theme = theme(axis_title = element_text(size = 12.5),\n                        axis_text = element_text(size = 10))\nview_plot = (ggplot(data = house_data, \n                    mapping = aes(x = house_data['view'].astype('category'), \n                                  y = 'price')) + \n  geom_boxplot() + # makes a boxplot\n  geom_hline(yintercept = house_data['price'].mean(),\n             linetype = 'dashed', color = 'red', size = 2) +\n  # add line denoting mean house price\n  theme_classic() + # white background, no gridlines\n  xlab('View Rating') +\n  ylab('Price (US$)') + # change y axis label\n  view_plot_theme + # change axis title and text size\n  scale_y_continuous(trans = 'log',\n                     breaks = np.array([100000, 250000, 500000, 1000000, \n                                        2500000, 5000000]),\n                     labels = np.array(['100,000', '250,000', '500,000', \n                                        '1,000,000', '2,500,000', '5,000,000']))\n  # change y-axis values\n  )\nview_plot","42078e9e":"# grade\ngrd_plot_theme = theme(axis_title = element_text(size = 12.5),\n                       axis_text = element_text(size = 10))\ngrd_plot = (ggplot(data = house_data, \n                   mapping = aes(x = house_data['grade'].astype('category'), \n                                 y = 'price')) + \n  geom_boxplot() + # makes a boxplot\n  geom_hline(yintercept = house_data['price'].mean(),\n             linetype = 'dashed', color = 'red', size = 2) +\n  # add line denoting mean house price\n  theme_classic() + # white background, no gridlines\n  xlab('Grade') +\n  ylab('Price (US$)') + # change y axis label\n  grd_plot_theme + # change axis title and text size\n  scale_y_continuous(trans = 'log',\n                     breaks = np.array([100000, 250000, 500000, 1000000, \n                                        2500000, 5000000]),\n                     labels = np.array(['100,000', '250,000', '500,000', \n                                        '1,000,000', '2,500,000', '5,000,000']))\n  # change y-axis values\n  )\ngrd_plot","7f2dfa2b":"# condition\ncond_plot_theme = theme(axis_title = element_text(size = 12.5),\n                        axis_text = element_text(size = 10))\ncond_plot = (ggplot(data = house_data, \n                    mapping = aes(x = house_data['condition'].astype('category'), \n                                  y = 'price')) + \n  geom_boxplot() + # makes a boxplot\n  geom_hline(yintercept = house_data['price'].mean(),\n             linetype = 'dashed', color = 'red', size = 2) +\n  # add line denoting mean house price\n  theme_classic() + # white background, no gridlines\n  xlab('Condition') +\n  ylab('Price (US$)') + # change y axis label\n  cond_plot_theme + # change axis title and text size\n  scale_y_continuous(trans = 'log',\n                     breaks = np.array([100000, 250000, 500000, 1000000, \n                                        2500000, 5000000]),\n                     labels = np.array(['100,000', '250,000', '500,000', \n                                        '1,000,000', '2,500,000', '5,000,000']))\n  # change y-axis values\n  )\ncond_plot","0fbc77d2":"# Square footage of interior living space\nsqft_liv_plot_theme = theme(axis_title = element_text(size = 12.5), \n                            axis_text = element_text(size = 10))\nsqft_liv_plot = (ggplot(data = house_data, \n                        mapping = aes(x = 'sqft_living', y = 'price')) +\n  geom_point(size = 1) +\n  # add data as points\n  theme_classic() + # white background, no gridlines\n  xlab('Square Footage of Living Area') + # change x axis label\n  ylab('Price (US$)') + # change y axis label\n  sqft_liv_plot_theme + # change the size of axis titles and axis text\n  scale_x_continuous(trans = 'log',\n                     breaks = np.array([500, 1000, 2500, 5000, 10000]),\n                     labels = np.array(['500', '1,000', '2,500', '5,000',\n                                        '10,000'])) +\n  scale_y_continuous(trans = 'log',\n                     breaks = np.array([100000, 250000, 500000, 1000000, \n                                        2500000, 5000000]),\n                     labels = np.array(['100,000', '250,000', '500,000', \n                                        '1,000,000', '2,500,000', '5,000,000'])) \n  # change x and y axis values\n  )\nsqft_liv_plot","9c46b701":"# Square footage of above ground interior\nsqft_abv_plot_theme = theme(axis_title = element_text(size = 12.5), \n                            axis_text = element_text(size = 10))\nsqft_abv_plot = (ggplot(data = house_data, \n                        mapping = aes(x = 'sqft_above', y = 'price')) +\n  geom_point(size = 1) +\n  # add data as points\n  theme_classic() + # white background, no gridlines\n  xlab('Square Footage of Interior (Above Ground)') + # change x axis label\n  ylab('Price (US$)') + # change y axis label\n  sqft_abv_plot_theme + # change the size of axis titles and axis text\n  scale_x_continuous(trans = 'log',\n                     breaks = np.array([500, 1000, 2500, 5000, 7500]),\n                     labels = np.array(['500', '1,000', '2,500', '5,000', \n                                        '7,500'])) +\n  scale_y_continuous(trans = 'log',\n                     breaks = np.array([100000, 250000, 500000, 1000000, \n                                        2500000, 5000000]),\n                     labels = np.array(['100,000', '250,000', '500,000', \n                                        '1,000,000', '2,500,000', '5,000,000'])) \n  # change x and y axis values\n  )\nsqft_abv_plot","83ea3b5c":"# Square footage of land space\nsqft_lot_plot_theme = theme(axis_title = element_text(size = 12.5), \n                            axis_text = element_text(size = 10))\nsqft_lot_plot = (ggplot(data = house_data, \n                        mapping = aes(x = 'sqft_lot', y = 'price')) +\n  geom_point(size = 1) +\n  # add data as points\n  theme_classic() + # white background, no gridlines\n  xlab('Square Footage of Lot') + # change x axis label\n  ylab('Price (US$)') + # change y axis label\n  sqft_lot_plot_theme + # change the size of axis titles and axis text\n  scale_x_continuous(trans = 'log',\n                     breaks = np.array([1000, 10000, 100000, 1000000]),\n                     labels = np.array(['1,000', '10,000', '100,000', \n                                        '1,000,000'])) +\n  scale_y_continuous(trans = 'log',\n                     breaks = np.array([100000, 250000, 500000, 1000000, \n                                        2500000, 5000000]),\n                     labels = np.array(['100,000', '250,000', '500,000', \n                                        '1,000,000', '2,500,000', '5,000,000'])) \n  # change x and y axis values\n  )\nsqft_lot_plot","44d6f48c":"# Square footage of below ground interior\n# create a subset with houses that have a below ground area\nblw_data = house_data[house_data.sqft_basement > 0]\n\nsqft_blw_plot_theme = theme(axis_title = element_text(size = 12.5), \n                            axis_text = element_text(size = 10))\nsqft_blw_plot = (ggplot(data = blw_data, \n                        mapping = aes(x = 'sqft_basement', y = 'price')) +\n  geom_point(size = 1) +\n  # add data as points\n  theme_classic() + # white background, no gridlines\n  xlab('Square Footage of Interior (Below Ground)') + # change x axis label\n  ylab('Price (US$)') + # change y axis label\n  sqft_blw_plot_theme + # change the size of axis titles and axis text\n  scale_x_continuous(trans = 'log',\n                     breaks = np.array([10, 100, 1000, 5000]),\n                     labels = np.array(['10', '100', '1,000', '5,000'])) +\n  scale_y_continuous(trans = 'log',\n                     breaks = np.array([100000, 250000, 500000, 1000000, \n                                        2500000, 5000000]),\n                     labels = np.array(['100,000', '250,000', '500,000', \n                                        '1,000,000', '2,500,000', '5,000,000'])) \n  # change x and y axis values\n  )\nsqft_blw_plot","a528dbfa":"# floors\nfloor_plot_theme = theme(axis_title = element_text(size = 12.5),\n                         axis_text = element_text(size = 10))\nfloor_plot = (ggplot(data = house_data, \n                     mapping = aes(x = house_data['floors'].astype('category'), \n                                   y = 'price')) + \n  geom_boxplot() +\n  # makes a boxplot\n  geom_hline(yintercept = house_data['price'].mean(),\n             linetype = 'dashed', color = 'red', size = 2) +\n  # add line denoting mean house price\n  theme_classic() + # white background, no gridlines\n  xlab('Number of Floors') + # change x axis label\n  ylab('Price (US$)') + # change y axis label\n  floor_plot_theme + # change axis title and text size\n  scale_y_continuous(trans = 'log',\n                     breaks = np.array([100000, 250000, 500000, 1000000, \n                                        2500000, 5000000]),\n                     labels = np.array(['100,000', '250,000', '500,000', \n                                        '1,000,000', '2,500,000', '5,000,000']))\n # change y-axis values\n )\nfloor_plot","1d47ec94":"# bedrooms\nbed_plot_theme = theme(axis_title = element_text(size = 12.5),\n                       axis_text = element_text(size = 10))\nbed_plot = (ggplot(data = house_data, \n                   mapping = aes(x = house_data['bedrooms'].astype('category'), \n                                 y = 'price')) + \n  geom_boxplot() +\n  # makes a boxplot\n  geom_hline(yintercept = house_data['price'].mean(),\n             linetype = 'dashed', color = 'red', size = 2) +\n  # add line denoting mean house price\n  theme_classic() + # white background, no gridlines\n  xlab('Number of Bedrooms') + # change x axis label\n  ylab('Price (US$)') + # change y axis label\n  bed_plot_theme + # change axis title and text size\n  scale_y_continuous(trans = 'log',\n                     breaks = np.array([100000, 250000, 500000, 1000000, \n                                        2500000, 5000000]),\n                     labels = np.array(['100,000', '250,000', '500,000', \n                                        '1,000,000', '2,500,000', '5,000,000']))\n  # change y-axis values\n  )\nbed_plot","b6537db6":"# bathrooms\nbath_plot_theme = theme(axis_title = element_text(size = 12.5),\n                        axis_text = element_text(size = 10))\nbath_plot = (ggplot(data = house_data, \n                    mapping = aes(x = house_data['bathrooms'].astype('category'), \n                                  y = 'price')) + \n  geom_boxplot() + # makes a boxplot\n  geom_hline(yintercept = house_data['price'].mean(),\n             linetype = 'dashed', color = 'red', size = 2) +\n  # add line denoting mean house price\n  theme_classic() + # white background, no gridlines\n  xlab('Number of Bathrooms') + # change x axis label\n  ylab('Price (US$)') + # change y axis label\n  bed_plot_theme + # change axis title and text size\n  scale_x_discrete(breaks = np.array([0, 0.5, 1, 1.5, 2, 2.5, 3, 3.5, 4, 4.5, \n                                      5, 5.5, 6, 6.5, 7.5, 8]),\n                   labels = np.array(['0', 0.5, 1, 1.5, 2, 2.5, 3, 3.5, 4, 4.5, \n                                      5, 5.5, 6, 6.5, 7.5, 8])) +\n  scale_y_continuous(trans = 'log',\n                     breaks = np.array([100000, 250000, 500000, 1000000, \n                                2500000, 5000000]),\n                     labels = np.array(['100,000', '250,000', '500,000', \n                                        '1,000,000', '2,500,000', '5,000,000']))\n  # change x and y-axis values\n  )\nbath_plot","3dd3c525":"# given that this is housing, doesn't make sense to have 0 bedrooms,\n# so they are removed, as well as the 33 bedroom house\n# also remove any house with 0 bathrooms\nhouse_data = house_data[(house_data.bedrooms < 33) & (house_data.bedrooms > 0) \n                        & (house_data.bathrooms > 0)]","4c5a20da":"# create map\nhouse_map = folium.Map([47.4, -122.25], tiles = 'cartodbpositron',\n                       zoom_start = 9)\n\n# create subsets of the original dataset, for each of the (arbitrarily)\n# chosen price groups\nprice_group1_subset = house_data[house_data.price < 250000]\nprice_group2_subset = house_data[(house_data.price > 250000) \n                                 & (house_data.price < 500000)]\nprice_group3_subset = house_data[(house_data.price > 500000) \n                                 & (house_data.price < 750000)]\nprice_group4_subset = house_data[(house_data.price > 750000) \n                                 & (house_data.price < 1000000)]\nprice_group5_subset = house_data[house_data.price > 1000000]\n\n# template for text shown in LayerControl\nlegend_text = '<span style = \"color: {col};\"> {txt} <\/span>'\n\n# create price groups as FeatureGroups, so that each price groups is its \n# own layer on the map\nprice_group1 = folium.FeatureGroup(name = legend_text.format(\n    txt = 'Price: < $250,000', col = 'lawngreen'))\nprice_group2 = folium.FeatureGroup(name = legend_text.format(\n    txt = 'Price: $250,000 - $500,000', col = 'mediumseagreen'))\nprice_group3 = folium.FeatureGroup(name = legend_text.format(\n    txt = 'Price: $500,000 - $750,000', col = 'deepskyblue'))\nprice_group4 = folium.FeatureGroup(name = legend_text.format(\n    txt = 'Price: $750,000 - $1,000,000', col = 'blue'))\nprice_group5 = folium.FeatureGroup(name = legend_text.format(\n    txt = 'Price: > $1,000,000', col = 'navy'))\n\n# add price_group1 to map\nfor i in range(0, len(price_group1_subset)):\n    # for each house in this price group subset\n    \n    # template for text shown when the marker is clicked on \n    popup_text = '<b> Price: ${:,} <\/b> <br> House area (sqft): {:,} <br> Lot area (sqft): {:,}'\n    popup_text = popup_text.format(\n        price_group1_subset['price'].iloc[i], # find the price for this house\n        price_group1_subset['sqft_living'].iloc[i], # find the sqft_living \n        price_group1_subset['sqft_lot'].iloc[i] # find the sqft_lot\n        )\n    \n    # create popup with the text created previously and set the width & height\n    iframe = folium.IFrame(popup_text, width = 175, height = 100)\n    \n    # add marker of this particular house\n    folium.CircleMarker(\n        location = [price_group1_subset.iloc[i]['lat'], \n                    price_group1_subset.iloc[i]['long']],\n        popup = folium.Popup(iframe), # created previously\n        radius = 1,\n        opacity = 0.75,\n        color = 'lawngreen').add_to(price_group1) # add to FeatureGroup\nprice_group1.add_to(house_map) # add FeatureGroup to Map\n\n# repeat this for the other price groups\n\nfor i in range(0, len(price_group2_subset)):\n    \n    popup_text = '<b> Price: ${:,} <\/b> <br> House area (sqft): {:,} <br> Lot area (sqft): {:,}'\n    popup_text = popup_text.format(\n        price_group2_subset['price'].iloc[i],\n        price_group2_subset['sqft_living'].iloc[i],\n        price_group2_subset['sqft_lot'].iloc[i]\n        )\n    \n    iframe = folium.IFrame(popup_text, width = 175, height = 100)\n    \n    folium.CircleMarker(\n        location = [price_group2_subset.iloc[i]['lat'], \n                    price_group2_subset.iloc[i]['long']],\n        popup = folium.Popup(iframe),\n        radius = 1,\n        opacity = 0.75,\n        color = 'mediumseagreen').add_to(price_group2)\nprice_group2.add_to(house_map)\n\nfor i in range(0, len(price_group3_subset)):\n    \n    popup_text = '<b> Price: ${:,} <\/b> <br> House area (sqft): {:,} <br> Lot area (sqft): {:,}'\n    popup_text = popup_text.format(\n        price_group3_subset['price'].iloc[i],\n        price_group3_subset['sqft_living'].iloc[i],\n        price_group3_subset['sqft_lot'].iloc[i]\n        )\n    \n    iframe = folium.IFrame(popup_text, width = 175, height = 100)\n    \n    folium.CircleMarker(\n        location = [price_group3_subset.iloc[i]['lat'], \n                    price_group3_subset.iloc[i]['long']],\n        popup = folium.Popup(iframe),\n        radius = 1,\n        opacity = 0.75,\n        color = 'deepskyblue').add_to(price_group3)\nprice_group3.add_to(house_map)\n\nfor i in range(0, len(price_group4_subset)):\n    \n    popup_text = '<b> Price: ${:,} <\/b> <br> House area (sqft): {:,} <br> Lot area (sqft): {:,}'\n    popup_text = popup_text.format(\n        price_group4_subset['price'].iloc[i],\n        price_group4_subset['sqft_living'].iloc[i],\n        price_group4_subset['sqft_lot'].iloc[i]\n        )\n    \n    iframe = folium.IFrame(popup_text, width = 175, height = 100)\n    \n    folium.CircleMarker(\n        location = [price_group4_subset.iloc[i]['lat'], \n                    price_group4_subset.iloc[i]['long']],\n        popup = folium.Popup(iframe),\n        radius = 1,\n        opacity = 0.75,\n        color = 'blue').add_to(price_group4)\nprice_group4.add_to(house_map)\n\nfor i in range(0, len(price_group5_subset)):\n    \n    popup_text = '<b> Price: ${:,} <\/b> <br> House area (sqft): {:,} <br> Lot area (sqft): {:,}'\n    popup_text = popup_text.format(\n        price_group5_subset['price'].iloc[i],\n        price_group5_subset['sqft_living'].iloc[i],\n        price_group5_subset['sqft_lot'].iloc[i]\n        )\n    \n    iframe = folium.IFrame(popup_text, width = 175, height = 100)\n    \n    folium.CircleMarker(\n        location = [price_group5_subset.iloc[i]['lat'], \n                    price_group5_subset.iloc[i]['long']],\n        popup = folium.Popup(iframe),\n        radius = 1,\n        opacity = 0.75,\n        color = 'navy').add_to(price_group5)\nprice_group5.add_to(house_map)\n\n# add option to toggle the layers (different price groups) on the map,\n# the colour and price range of each price group is shown too\nfolium.LayerControl(position = 'bottomleft').add_to(house_map)\n\nhouse_map","4df341e8":"# create mock data\nexample_var_x = [7, 6, 9, 4, 12, 5, 10, 3, 13]\nexample_var_y = [9, 10, 7, 12, 4, 11, 6, 3, 13]\nexample_data = pd.DataFrame({'Variable 1': example_var_x, \n                             'Variable 2': example_var_y})\n\n# plot data, and highlight the residuals\nresidual_plot = (ggplot(mapping = aes(x = 'example_var_x', y = 'example_var_y'),\n                        data = example_data) +\n  geom_smooth(method = 'lm', se = False, colour = 'black') +\n  geom_segment(aes(xend = 'Variable 1', yend = 8.35),\n               colour = 'red') +\n  geom_point() +\n  theme_classic() + \n  theme(axis_title = element_text(size = 12.5),\n        axis_text = element_text(size = 10))\n  )\nresidual_plot","b4d3fbb5":"# first remove unnecessary variables ('id', 'date' and 'zipcode')\nmodel_data = house_data.drop(columns = ['id', 'date', 'zipcode'])\n\n# log transform some variables using NumPy\nmodel_data['price'] = np.log(model_data['price'])\nmodel_data['sqft_living'] = np.log(model_data['sqft_living'])\nmodel_data['sqft_lot'] = np.log(model_data['sqft_lot'])\nmodel_data['sqft_above'] = np.log(model_data['sqft_above'])\nmodel_data['sqft_basement'] = np.log(model_data['sqft_basement'] + 1)\nmodel_data['sqft_living15'] = np.log(model_data['sqft_living15'])\nmodel_data['sqft_lot15'] = np.log(model_data['sqft_lot15'])\n\n# create a way to track model stats, all values are initially nan but these \n# will be replaced when then test MSE's are calculated\n# panda's DataFrame replacing matrix() in R\nmodel_stats = pd.DataFrame(columns = ['Test MSE'],\n                           index = [\n                               'Best subset selection',\n                               'Forward stepwise selection', \n                               'Backward stepwise selection',\n                               'Ridge regression', \n                               'Lasso regression', 'PCR', 'PLS', \n                               'Single regression tree', 'Bagging', \n                               'Random forest', 'Boosting'\n                               ]\n                           )\n\n# denote the x (predictors) and y (response) variables, for train and test set\n# training set will be 75% of original dataset, test set is 25%\nx = model_data.drop('price', axis = 1)\ny = pd.DataFrame(model_data.price)\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.25,\n                                                    random_state = 35)\n","f7654f16":"# this takes about three hours to run on kaggle and it doesn't fully complete and nothing else works after it\n# error message:\n# Features: 130402\/131071IOPub message rate exceeded.\n# The notebook server will temporarily stop sending output\n# to the client in order to avoid crashing it.\n# To change this limit, set the config variable\n# `--NotebookApp.iopub_msg_rate_limit`.\n\n# Current values:\n# NotebookApp.iopub_msg_rate_limit=1000.0 (msgs\/sec)\n# NotebookApp.rate_limit_window=3.0 (secs)\n\n\n# So, I will just put the test MSE that I got from my own computer, but if you want to repeat this the code is below\n\n# EFS will consider every possible combination of features\n# the 'best' number of features is determined by 10-fold cross-validation,\n# the mean squared error on the held-out fold\n#lm = LinearRegression()\n#efs = EFS(lm, min_features = 1, max_features = 17, \n          #scoring = 'neg_mean_squared_error', cv = 10)\n\n# fit to training data\n#efs.fit(x_train, y_train)\n#efs.best_feature_names_\n# 16 features selected, only 'long' was not selected\n\n# use the selected features to predict values in test set\n#x_train_efs = efs.transform(x_train)\n#x_test_efs = efs.transform(x_test)\n#lm_house = lm.fit(x_train_efs, y_train)\n#y_pred = lm_house.predict(x_test_efs)\n\nlm_test_MSE = 0.06351712398582668\nbest_feature_names_ = ('bedrooms', 'bathrooms', 'sqft_living', \n                           'sqft_lot', 'floors', 'waterfront', 'view', \n                           'condition', 'grade', 'sqft_above', 'sqft_basement', \n                           'yr_built', 'yr_renovated', 'lat', 'sqft_living15', 'sqft_lot15')","342c9c7a":"print('Test set MSE: %.5f' % lm_test_MSE)\nprint('Number of features chosen:', len(best_feature_names_))\nprint('Features chosen:', best_feature_names_)","e2f42752":"# because EFS doesn't work, I've recreate the table produced below\n\n# extract stats of efs\n#efs_stats = pd.DataFrame.from_dict(efs.get_metric_dict()).T\n\n# dataframe to add the test mse and standard errors for the best model at\n# each size\n#efs_best_models = pd.DataFrame(columns = ['CV_score', 'std_error'])\n\n# for each sized model\n#for i in range(1, 17, 1):\n\n    #indexes = [] # list to track the indexes in efs_stats of the models with\n    # size i\n    \n    # iterate over all the models in efs_stats\n    #for j in range(0, len(efs_stats)):\n        \n        # if the length of the model is equal to i\n        #if len(efs_stats['feature_names'][j]) == i:\n            \n            # add the index (j) to indexes\n            #indexes.append(j)\n    \n    # extract the index of the model with the lowest cross val score\n    #min_avg_score_index = efs_stats[min(indexes):max(indexes)]['avg_score'].astype(float).idxmin(axis = 1)\n    \n    # use that index to extract the cv_score and std_error\n    #efs_best_models.at[i, 'CV_score'] = np.square(efs_stats['avg_score'][min_avg_score_index])\n    #efs_best_models.at[i, 'std_error'] = efs_stats['std_err'][min_avg_score_index]\n\n# change columns to floats\n#efs_best_models = efs_best_models.astype(float)\n\n\n# recreated table\nefs_best_models = efs_best_models1 = pd.DataFrame(data = {'CV_score': [0.07605759865727282,\n                                                                       0.07567839005496807,\n                                                                       0.07471145477652824,\n                                                                       0.0735012308766475,\n                                                                       0.07199211537679669,\n                                                                       0.06895525155189056,\n                                                                       0.06585606580426408,\n                                                                       0.05823823994049927,\n                                                                       0.050553675178276816,\n                                                                       0.042796856332271004,\n                                                                       0.0310474069290086,\n                                                                       0.023926368298656284,\n                                                                       0.016491804302500277,\n                                                                       0.014544475614421114,\n                                                                       0.012536455653603465,\n                                                                       0.00866700519371302],\n                                                          'std_error': [0.0035234919449906965,\n                                                                        0.003613041317735649,\n                                                                        0.0033761903770162245,\n                                                                        0.0034232814493334184,\n                                                                        0.003101720687690171,\n                                                                        0.002839548464126151,\n                                                                        0.003184049048283974,\n                                                                        0.0033458532098801174,\n                                                                        0.0033063287428112265,\n                                                                        0.0029888377848824275,\n                                                                        0.002766817317231102,\n                                                                        0.002141453537275933,\n                                                                        0.0013362830904681947,\n                                                                        0.0013186797000082193,\n                                                                        0.0014868116451182763,\n                                                                        0.0012730036196126162,]},\n                                                  index = list(range(1, 17)))\n\n\n# plot scores with std errors\nplot_bss_cv_theme = theme(axis_title = element_text(size = 12.5),\n                          axis_text = element_text(size = 10))\nplot_bss_cv = (ggplot(data = efs_best_models, \n                      mapping = aes(x = np.array(range(1, 17, 1)), \n                                    y = 'CV_score')) +\n  geom_point() + # plot points\n  geom_line() + # join points with line\n  geom_errorbar(mapping = aes(ymin = efs_best_models['CV_score'] - \n                              efs_best_models['std_error'],\n                              ymax = efs_best_models['CV_score'] + \n                              efs_best_models['std_error'])) +\n  \n  # add standard error bars\n  geom_hline(yintercept = 0.00994000519371302, linetype = 'dashed',\n             colour = 'red') + # add line denoting the standard error\n  # above the best model\n  theme_classic() + # white background, no gridlines\n  xlab('Model Size') + # change x axis label\n  ylab('Test MSE') + # change y axis label\n  plot_bss_cv_theme + # change the size of axis titles and axis text\n  scale_y_continuous(breaks = np.array(np.arange(0, 0.09, 0.01)),\n                     labels = np.array(np.arange(0, 0.09, 0.01)))\n  # change y-axis limits\n  )\nplot_bss_cv","d0db6c38":"# add test set MSE to matrix\nmodel_stats.loc['Best subset selection', 'Test MSE'] = lm_test_MSE","deeb1919":"## Forward stepwise selection\n# taken and adapted from:\n# http:\/\/www.science.smith.edu\/~jcrouser\/SDS293\/labs\/lab9-py.html\n\n# using 10-fold cross-validation, so set that up first\nk = 10\nnp.random.seed(seed = 35) # set seed to ensure reproducibility\nfolds = np.random.choice(k, size = len(y), replace = True)\n\n# dataframe to track cross-validation errors\ncrossVal_errors_fwd = pd.DataFrame(\n    columns = range(1, k + 1),\n    index = range(1, 18))\n\n# function to fit a linear model\ndef processSubset(features, X_train, Y_train, X_test, Y_test):\n    # fit to training set\n    model = sm.OLS(Y_train, X_train[list(features)])\n    model_fit = model.fit()\n    \n    # calculate test set MSE by using the fitted model to make predictions on \n    # the test set\n    predictions = model_fit.predict(X_test[list(features)])\n    test_MSE = np.mean((Y_test.subtract(predictions, axis = 0))**2)\n    test_MSE = test_MSE.tolist()\n\n    return {'Model': model_fit, 'Test MSE': test_MSE}\n\n# function to perform forward stepwise selection\ndef forward_stepwise_selection(predictors, X_train, Y_train, X_test, Y_test):\n    results = [] # empty for now\n    \n    # extract predictors that still need to be checked\n    # checks if each predictor in x_train is also not in predictors\n    remaining_predictors = [p for p in X_train.columns if p not in predictors]\n    \n    # for each of the remaining predictors\n    for p in remaining_predictors:\n        \n        # call processSubset to fit a linear model using predictors + each\n        # of the remaining predicotrs\n        results.append(processSubset(predictors + [p], \n                                     X_train, Y_train, X_test, Y_test))\n        \n    # turn results into a dataframe\n    models = pd.DataFrame(results)\n    \n    # choose the best model (with the lowest test set MSE)    \n    models.sort_values('Test MSE', inplace = True, axis = 0)\n    best_model = models[:1]\n\n    return best_model\n    \n# now write a for loop performing cross-validation\n# in the ith fold, the elements of folds that equal i are in the test set and \n# the remainder are in the training\n# dataframe to track the cross_validation errors for each model size\nmodels_crossVal_errors_fwd = pd.DataFrame(columns = ['Model', 'Test MSE'])\n\n# iterate over each fold\nfor i in range(1, k + 1):\n    \n    # reset predictors\n    predictors = []\n    \n    # iterate over each model size\n    for j in range(1, len(x.columns) + 1):\n        \n        # create train sets using all but fold i and test sets using the\n        # remaining fold i\n        x_train = x[folds != (i - 1)]\n        y_train = y[folds != (i - 1)]\n        x_test = x[folds == (i - 1)]\n        y_test = y[folds == (i - 1)]\n        \n        # call forward_stepwise_selection, training on every fold except i\n        # test on the ith fold\n        fss = forward_stepwise_selection(\n            predictors, x_train, y_train, x_test, y_test)\n        \n        # add model and test MSE for this fold i to models_crossVal_errors\n        models_crossVal_errors_fwd = models_crossVal_errors_fwd.append(fss)\n\n        # in order to add the test MSE for this model, the index of fss needs\n        # to be called but this changes over time, so the following code\n        # extracts the relevant index\n        # convert column to series\n        fss_test_MSE_column = pd.Series(fss['Test MSE'])\n        # extract index value as a list\n        fss_test_MSE_index = fss.index.values.tolist()\n        # convert element in fss_index to str\n        index_str = [str(i) for i in fss_test_MSE_index]\n        fss_index = str(''.join(index_str))                                    \n         \n        # add test MSE for this model size (j) and fold (i) to \n        # crossVal_errors_fwd - fss_index is finally converted to int\n        crossVal_errors_fwd.at[j, i] = pd.Series(fss['Test MSE'][int(fss_index)])\n\n        # extract predictors\n        predictors = list(fss['Model'])[0].model.exog_names\n        # this ensures that when the j loop runs again, the predictors start\n        # with the best selection at the previous model size\n       \n# this results in a matrix of test MSE, where the (i,j)th element is \n# equal to the test MSE for the ith cross-validation fold for the best\n# j-variable model\n# obtain a vector for which the jth element is the cross-validation\n# error for the j-variable model by averaging all the errors for that size\ncrossVal_errors_fwd_mean = crossVal_errors_fwd.apply(np.mean, axis = 1)\n\n# standard error of each model size test MSE (standard deviation of test set\n# MSE divided by the square root of the number of folds) before mean is found \ncrossVal_errors_fwd_se = pd.DataFrame(crossVal_errors_fwd_mean,\n                                      columns = ['Test MSE'])\ncrossVal_errors_fwd_se['SE'] = crossVal_errors_fwd.sem(axis = 1)\n\n# plot\nplot_fwd_cv_theme = theme(axis_title = element_text(size = 12.5),\n                          axis_text = element_text(size = 10))\nplot_fwd_cv = (ggplot(data = crossVal_errors_fwd_se, \n                      mapping = aes(x = np.array(range(1, 18, 1)), \n                                    y = 'Test MSE')) +\n  geom_point() + # plot points\n  geom_line() + # join points with line\n  geom_errorbar(mapping = aes(ymin = crossVal_errors_fwd_se['Test MSE'] - \n                              crossVal_errors_fwd_se['SE'],\n                              ymax = crossVal_errors_fwd_se['Test MSE'] + \n                              crossVal_errors_fwd_se['SE'])) +\n  \n  # add standard error bars\n  geom_hline(yintercept = 0.0659094255830648, linetype = 'dashed',\n             colour = 'red') + # add line denoting the standard error\n  # above the best model\n  theme_classic() + # white background, no gridlines\n  xlab('Model Size') + # change x axis label\n  ylab('Test MSE') + # change y axis label\n  plot_fwd_cv_theme + # change the size of axis titles and axis text\n  scale_y_continuous(limits = np.array([0.05, 0.25])) \n  # change y-axis limits\n  )\nplot_fwd_cv\n# the 12 variable model is the simplest model that has a test MSE within one\n# standard error of the best model (16 variables)","755bf4c6":"# therefore add the test set MSE of the 12 variable model to the matrix\nmodel_stats.loc['Forward stepwise selection', \n                'Test MSE'] = crossVal_errors_fwd_se['Test MSE'][12]","ea37dfad":"## Backwards stepwise selection\n# taken and adapted from:\n# http:\/\/www.science.smith.edu\/~jcrouser\/SDS293\/labs\/lab8-py.html\n\n# using 10-fold cross-validation, so set that up first\nk = 10\nnp.random.seed(seed = 35)\nfolds = np.random.choice(k, size = len(y), replace = True)\n\n# dataframe to track cross-validation errors\ncrossVal_errors_bwd = pd.DataFrame(\n    columns = range(1, k + 1),\n    index = range(1, 18)\n    )\n\n# function to perform backward stepwise selection\ndef backward_stepwise_selection(predictors, X_train, Y_train, X_test, Y_test):\n    results = [] # empty for now\n    \n    # for each combination of predictors (up to 16)\n    for combo in itertools.combinations(predictors, len(predictors) - 1):\n            # call processSubset on each combination and append to results\n            results.append(processSubset(combo, X_train, Y_train, X_test, Y_test))\n        \n    # turn results into a dataframe\n    models = pd.DataFrame(results)\n    \n    # choose the best model (with the lowest test set MSE)    \n    models.sort_values('Test MSE', inplace = True, axis = 0)\n    best_model = models[:1]\n\n    return best_model\n\nmodels_crossVal_errors_bwd = pd.DataFrame(columns = ['Model', 'Test MSE'])\n\n# iterate over each fold\nfor i in range(1, k + 1):\n    \n    predictors = x_train.columns\n    \n    # iterate over each model size\n    while(len(predictors) > 1):\n\n        # create train sets using all but fold i and test sets using the\n        # remaining fold i\n        x_train = x[folds != (i - 1)]\n        y_train = y[folds != (i - 1)]\n        x_test = x[folds == (i - 1)]\n        y_test = y[folds == (i - 1)]\n        \n        # call backward_stepwise_selection, training on every fold except i\n        # test on the ith fold\n        bss = backward_stepwise_selection(\n            predictors, x_train, y_train, x_test, y_test)\n        \n        # add model and test MSE for this fold i to models_crossVal_errors\n        models_crossVal_errors_bwd = models_crossVal_errors_bwd.append(bss)\n\n        # in order to add the test MSE for this model, the index of bss needs\n        # to be called but this changes over time, so the following code\n        # extracts the relevant index\n        # convert column to series\n        bss_test_MSE_column = pd.Series(bss['Test MSE'])\n        # extract index value as a list\n        bss_test_MSE_index = bss.index.values.tolist()\n        # convert element in fss_index to str\n        index_str = [str(i) for i in bss_test_MSE_index]\n        bss_index = str(''.join(index_str))                                    \n         \n        # add test MSE for this model size (j) and fold (i) to \n        # crossVal_errors_bwd - bss_index is finally converted to int\n        crossVal_errors_bwd.at[(len(predictors) - 1), i] = pd.Series(\n            bss['Test MSE'][int(bss_index)])\n\n        # extract predictors        \n        predictors = list(bss['Model'])[0].model.exog_names\n        \n# note that this means a 17 variable model is never looked at, but the point\n# of this is that it should be a subset of predictors anyway, so I'm content\n# with leaving the 17 variable model empty (best subset selection identified\n# that the 17 variable was not the best anyway)\n\n# this results in a matrix of test MSE, where the (i,j)th element is \n# equal to the test MSE for the ith cross-validation fold for the best\n# j-variable model\n# obtain a vector for which the jth element is the cross-validation\n# error for the j-variable model by averaging all the errors for that size\ncrossVal_errors_bwd_mean = crossVal_errors_bwd.apply(np.mean, axis = 1)\n\n# standard error of each model size test MSE (standard deviation of test set\n# MSE divided by the square root of the number of folds) before mean is found\ncrossVal_errors_bwd_se = pd.DataFrame(crossVal_errors_bwd_mean,\n                                      columns = ['Test MSE'])\ncrossVal_errors_bwd_se['SE'] = crossVal_errors_bwd.sem(axis = 1)\n\nplot_bwd_cv_theme = theme(axis_title = element_text(size = 22.5),\n                          axis_text = element_text(size = 20))\nplot_bwd_cv = (ggplot(data = crossVal_errors_bwd_se, \n                      mapping = aes(x = np.array(range(1, 18, 1)), \n                                    y = 'Test MSE')) +\n  geom_point() + # plot points\n  geom_line() + # join points with line\n  geom_errorbar(mapping = aes(ymin = crossVal_errors_bwd_se['Test MSE'] - \n                              crossVal_errors_bwd_se['SE'],\n                              ymax = crossVal_errors_bwd_se['Test MSE'] + \n                              crossVal_errors_bwd_se['SE'])) +\n  \n  # add standard error bars\n  geom_hline(yintercept = 0.06589432278232081, linetype = 'dashed',\n             colour = 'red') + # add line denoting the standard error\n  # above the best model\n  theme_classic() + # white background, no gridlines\n  xlab('Model Size') + # change x axis label\n  ylab('Test MSE') + # change y axis label\n  plot_bwd_cv_theme + # change the size of axis titles and axis text\n  scale_y_continuous(limits = np.array([0.05, 0.3])) \n  # change y-axis limits\n  )\nplot_bwd_cv\n# the 12 variable model is the simplest model that has a test MSE within one\n# standard error of the best model (16 variables)\n\n# therefore add the test set MSE of the 12 variable model to the matrix\nmodel_stats.loc['Backward stepwise selection', \n                'Test MSE'] = crossVal_errors_fwd_se['Test MSE'][12]\n","3e0b0306":"# set grid of alpha values\ngrid = 10**np.linspace(10, -2, 100)*0.5\n\n# perform ridge regression with 10-fold cross-validation to find the best alpha,\n# scoring is mean squared error (MSE)\nridge_model = RidgeCV(alphas = grid, scoring = 'neg_mean_squared_error', cv = 10)\n# fit to training data\nridge_model.fit(x_train, y_train)\n# extract best alpha\nridge_best_alpha = ridge_model.alpha_\n\n# new ridge model with best alpha\nridge2 = Ridge(alpha = ridge_best_alpha)\n# fit to training data\nridge2.fit(x_train, y_train)\n# predict on test data\nmean_squared_error(y_test, ridge2.predict(x_test))","cadef1e0":"# add test MSE to matrix\nmodel_stats.loc['Ridge regression', \n                'Test MSE'] = mean_squared_error(y_test, ridge2.predict(x_test))\n\n# show models so far\nmodel_stats","2b04a839":"# from: https:\/\/nbviewer.jupyter.org\/github\/JWarmenhoven\/ISL-python\/blob\/master\/Notebooks\/Chapter%206.ipynb\n\n# perform lasso regression with 10-fold cross-validation to find the best alpha,\n# 'random_state = 35' ensures reproducible results\nlasso_model = LassoCV(alphas = grid, cv = 10, random_state = 35)\n# fit to training data\nlasso_model.fit(x_train, y_train.values.ravel())\n# extract best alpha\nlasso_best_alpha = lasso_model.alpha_\n\n# new ridge model with best alpha\nlasso2 = Lasso(alpha = lasso_best_alpha)\n# fit to training data\nlasso2.fit(x_train, y_train)\n# predict on test data\nmean_squared_error(y_test, lasso2.predict(x_test))","5662f4f0":"# add test MSE to matrix\nmodel_stats.loc['Lasso regression', \n                'Test MSE'] = mean_squared_error(y_test, lasso2.predict(x_test))","77e7dd77":"# from: https:\/\/nbviewer.jupyter.org\/github\/JWarmenhoven\/ISL-python\/blob\/master\/Notebooks\/Chapter%206.ipynb\n\npca = PCA()\nlm = LinearRegression()\n# scale the predictors\nx_transformed = pca.fit_transform(scale(x_train))\nn = len(x_transformed)\n\n# using 10-fold cross-validation - different way of choosing folds so not\n# necessarily comparable with the other method, but it shouldn't make too\n# much difference\nfolds2 = KFold(n_splits = 10, shuffle = True, random_state = 35)\n\n# track MSE\npcr_mse = []\n\n# calculate MSE for the intercept\npcr_intercept = -1*cross_val_score(lm, np.ones((n, 1)), y_train, cv = folds2,\n                                   scoring = 'neg_mean_squared_error').mean()\npcr_mse.append(pcr_intercept)\n\n# now calculate MSE cross-validation for all 17 principal components\nfor i in range(1, 18, 1):\n    pcr_value = -1*cross_val_score(lm, x_transformed[:, :i], y_train, \n                                   cv = folds2,\n                                   scoring = 'neg_mean_squared_error').mean()\n    pcr_mse.append(pcr_value)\n\n\n# plot\npcr_mse = pd.DataFrame(pcr_mse)\nplot_pcr_cv_theme = theme(axis_title = element_text(size = 12.5),\n                          axis_text = element_text(size = 10))\nplot_pcr_cv = (ggplot(data = pcr_mse, \n                      mapping = aes(x = np.array(range(0, 18, 1)), \n                                    y = pcr_mse)) +\n  geom_point() + # plot points\n  geom_line() + # join points with line\n  geom_hline(yintercept = min(pcr_mse[0]), linetype = 'dashed',\n             colour = 'red') + # add line denoting the standard error\n  # above the best model\n  theme_classic() + # white background, no gridlines\n  xlab('Number of Principal Components') + # change x axis label\n  ylab('Test MSE') + # change y axis label\n  plot_pcr_cv_theme # change the size of axis titles and axis text\n  )\nplot_pcr_cv","410dfacb":"# transform test data\nx_transformed_test = pca.transform(scale(x_test))[:, :18]\n# train on training data\nlm = LinearRegression()\nlm.fit(x_transformed[:, :18], y_train)\n# test on test data\npcr_predictions = lm.predict(x_transformed_test)\nprint(mean_squared_error(y_test, pcr_predictions))\n\n# add test MSE to matrix\nmodel_stats.loc['PCR', 'Test MSE'] = mean_squared_error(y_test, pcr_predictions)","e570f54d":"# from: https:\/\/nbviewer.jupyter.org\/github\/JWarmenhoven\/ISL-python\/blob\/master\/Notebooks\/Chapter%206.ipynb\n\nn = len(x_train)\n\n# using 10-fold cross-validation \nfolds2 = KFold(n_splits = 10, shuffle = True, random_state = 35)\n\n# track MSE\npls_mse = []\n\n# now calculate MSE cross-validation for all 17 principal components\nfor i in range(1, 18, 1):\n    pls = PLSRegression(n_components = i)\n    pls_value = cross_val_score(pls, scale(x_train), y_train, cv = folds2,\n                                scoring = 'neg_mean_squared_error').mean()\n    pls_mse.append(pls_value)\n\n# plot\npls_mse = pd.DataFrame(pls_mse)\npls_mse[0] = pls_mse[0]**2\nplot_pls_cv_theme = theme(axis_title = element_text(size = 12.5),\n                          axis_text = element_text(size = 10))\nplot_pls_cv = (ggplot(data = pls_mse, \n                      mapping = aes(x = np.array(range(0, 17, 1)), \n                                    y = pls_mse)) +\n  geom_point() + # plot points\n  geom_line() + # join points with line\n  geom_hline(yintercept = min(pls_mse[0]), linetype = 'dashed',\n             colour = 'red') + # add line denoting the standard error\n  # above the best model\n  theme_classic() + # white background, no gridlines\n  xlab('Number of Principal Components') + # change x axis label\n  ylab('Test MSE') + # change y axis label\n  plot_pls_cv_theme # change the size of axis titles and axis text\n  )\nplot_pls_cv","07c7a315":"pls = PLSRegression(n_components = 4)\npls.fit(scale(x_train), y_train)\nprint(mean_squared_error(y_test, pls.predict(scale(x_test))))\n\n# add test MSE to matrix\nmodel_stats.loc['PLS', 'Test MSE'] = mean_squared_error(\n    y_test, pls.predict(scale(x_test))\n    )","c2ce0564":"model_stats","d20f1b9a":"# ccp_alpha is the complexity parameter used to determine pruning the tree\n# vector of alpha values will be created and used\nccp_alphas = np.array(list(np.arange(0, 0.01, 0.0005)), \n                      dtype = 'float64')\n\n# dataframe to track test MSE for the cross-validation using different alphas\nsingle_tree_stats = pd.DataFrame(columns = ccp_alphas, \n                                 index = list(range(1, 11, 1)))\n\n# iterate over each fold\nfor i in range(1, k + 1):\n    \n    # create train sets using all but fold i and test sets using the\n    # remaining fold i\n    x_train = x[folds != (i - 1)]\n    y_train = y[folds != (i - 1)]\n    x_test = x[folds == (i - 1)]\n    y_test = y[folds == (i - 1)]\n    \n    # using each alpha    \n    for ccp_alpha in ccp_alphas:\n        # create tree - i have (arbitrarily) chosen a max depth of 5\n        tree = DecisionTreeRegressor(random_state = 35, max_depth = 3,\n                                     ccp_alpha = ccp_alpha)\n        # fit to training values\n        tree.fit(x_train, y_train)\n        # create predictions using test set\n        tree_predict = tree.predict(x_test)\n        # calculate test set MSE\n        tree_test_mse = mean_squared_error(y_test, tree_predict)\n        # add to dataframe\n        single_tree_stats.loc[i, ccp_alpha] = tree_test_mse\n\n# find the mean test set MSE for each alpha\ntree_mean = single_tree_stats.apply(np.mean, axis = 0)\n\n# plot\nplot_tree_cv_theme = theme(axis_title = element_text(size = 12.5),\n                           axis_text = element_text(size = 10))\nplot_tree_cv = (ggplot(data = pd.DataFrame(tree_mean), \n                      mapping = aes(x = ccp_alphas, \n                                    y = tree_mean)) +\n  geom_point() + # plot points\n  geom_line() + # join points with line\n  geom_hline(yintercept = min(tree_mean[0:]), linetype = 'dashed',\n             colour = 'red') + # add line denoting the lowest test set MSE\n  theme_classic() + # white background, no gridlines\n  xlab('Alpha') + # change x axis label\n  ylab('Test MSE') + # change y axis label\n  plot_tree_cv_theme + # change the size of axis titles and axis text\n  scale_x_continuous(breaks = np.array(np.arange(0, 0.0125, 0.0025)),\n                     labels = np.array(np.arange(0, 0.0125, 0.0025)),\n                     limits = np.array([0, 0.01])) +\n  scale_y_continuous(breaks = np.array(np.linspace(0.05, 0.11, 5)),\n                     labels = np.array(np.linspace(0.05, 0.11, 5)),\n                     # using linspace because arange was outputting floats\n                     # with a large number of decimal places, linspace stops\n                     # this from happening\n                     limits = np.array([0.05, 0.11]))\n  # change x and y axis labels\n  )\nplot_tree_cv","58f2dadb":"# fit a tree to all training data - again going to use a max depth of 5\n# i have tried using everything default, but the tree is massive and illegible\nsingle_tree = DecisionTreeRegressor(random_state = 35, max_depth = 3,\n                                    ccp_alpha = 0)\n\n# fit to training values\nsingle_tree.fit(x_train, y_train)\n# create predictions using test set\nsingle_tree_predict = single_tree.predict(x_test)\n# calculate test set MSE\nsingle_tree_test_mse = mean_squared_error(y_test, single_tree_predict)\nfig, ax = plt.subplots(figsize = (17.5, 17.5))\nplot_tree(single_tree, fontsize = 10, ax = ax, feature_names = x_train.columns)\nplt.show()","04eb7533":"# add test MSE to matrix\nmodel_stats.loc['Single regression tree', 'Test MSE'] = single_tree_test_mse\nmodel_stats","b018b3cf":"# bagging is a special case of a random forest, where the subset of predictors\n# used is equal to the number of predictors - max_features = 17 denotes this\nbag_model = RandomForestRegressor(max_features = 17, random_state = 35)\n# fit to training data\nbag_model.fit(x_train, y_train.values.ravel())\n# make predictions using test data\nbag_predict = bag_model.predict(x_test)\n# calculate test set MSE\nbag_test_set_mse = mean_squared_error(y_test, bag_predict)\nprint(bag_test_set_mse)","b39dceb5":"# add test MSE to matrix\nmodel_stats.loc['Bagging', 'Test MSE'] = bag_test_set_mse","43b356cc":"# growing a random forest is similar, but a lower value of mtry is used\n# a value of 5 is used here, the same number I used in R\nrf_model = RandomForestRegressor(max_features = 5, random_state = 35)\n# fit to training data\nrf_model.fit(x_train, y_train.values.ravel())\n\n# extract importance of each variable\nrf_var_importance = pd.DataFrame({'Importance': rf_model.feature_importances_}, \n                                 index = x_train.columns)\n\n# plot\nplot_rf_var_importance_theme = theme(axis_title = element_text(size = 12.5),\n                                     axis_text = element_text(size = 10))\nplot_rf_var_importance = (ggplot(data = rf_var_importance,\n                                 mapping = aes(x = rf_var_importance.index,\n                                               y = rf_var_importance)) +\n  geom_col() + # plot bars\n  theme_classic() + # white background, no gridlines\n  xlab('Variable') + # change x axis label\n  ylab('Importance of Variable') + # change y axis label\n  plot_rf_var_importance_theme + # change the size of axis titles and axis text\n  coord_flip() # flips to put variables on y axis, improving readability\n  )\nplot_rf_var_importance","b9f2bc2d":"# make predictions using test data\nrf_predict = rf_model.predict(x_test)\n# calculate test set MSE\nrf_test_set_mse = mean_squared_error(y_test, rf_predict)\nprint(rf_test_set_mse)\n\n# add test MSE to matrix\nmodel_stats.loc['Random forest', 'Test MSE'] = rf_test_set_mse","b1b7a2e1":"# create a dictionary for the different values for the different parameters\nparameters = {'Learning rate': [0.001, 0.01, 0.1],\n              'Depth': [1, 2, 3, 4, 5],\n              'Min. Node Obs.': [5, 10, 15],\n              'Bag fraction': [0.65, 0.8, 1]}\n\n# ParameterGrid creates dictionaries for each of the different combinations\n# of parameters\nparameter_grid = ParameterGrid(parameters)\n\n# dataframe to track test MSE\nboost_stats = pd.DataFrame(columns = ['Test MSE']) ","5153ce90":"# iterate over each combination of parameters\nfor i in range(1, len(list(parameter_grid))):\n    \n    # create boosted model using parameters at i\n    boost_model = GradientBoostingRegressor(\n        learning_rate = list(parameter_grid)[i]['Learning rate'],\n        subsample = list(parameter_grid)[i]['Bag fraction'], # enables\n        # stochastic gradient boosting\n        min_samples_leaf = list(parameter_grid)[i]['Min. Node Obs.'],\n        max_depth = list(parameter_grid)[i]['Depth'],\n        random_state = 35\n        )\n    \n    # fit to training data\n    boost_model.fit(x_train, y_train.values.ravel())\n    # predict using training data\n    boost_predict = boost_model.predict(x_test)\n    # calculate test set MSE\n    boost_test_set_mse = mean_squared_error(y_test, boost_predict)\n    # add to dataframe\n    boost_stats.loc[i, 'Test MSE'] = boost_test_set_mse\n\nprint('Lowest test set MSE is', min(boost_stats['Test MSE']))\n# this was the last model used, which had the following parameters\nprint('The best model had these parameters:', list(parameter_grid)[134])\n# won't be repeating this by using different parameters, but shown it can be\n# in R and is possible if a bit trickier in python","f6bed75d":"# add test MSE to matrix\nmodel_stats.loc['Boosting', 'Test MSE'] = min(boost_stats['Test MSE'])","f68e8886":"model_stats","5b819191":"This graph is interesting in that it highlights there are some houses with **zero** bedrooms and one house with **33** bedrooms. I believe that it doesn\u2019t make sense for a house not to have at least one bedroom and the house with 33 is clearly an outlier, so these houses will be removed. As for the trend, there is generally an increase in median house price as the number of bedrooms increases.","56e0cfb6":"\nHere we see that the mean square footage of the *living area* of the closest 15 houses is correlated with the price of a house. The assumption here is that larger houses are more expensive (*this is looked at later in **section 2.5***) and if there are many large houses close to a house, it is likely that that house is large and therefore expensive.","6f963e72":"The worst models with the highest test MSEs were the single regression tree, followed by forward and backward stepwise selection. Lasso regression comes next, followed by best subset selection. PLS is the best of the linear model methods beating ridge regression which comes second. The best models were the remaining tree-based methods, boosting is the best model with a notable improvement over second-placed random forest and a substantial decrease in test MSE compared to the worst models.\n\nThe tree-based approaches also highlighted that latitude was the most significant predictor, followed by the square footage of the house and the grade of the house, and to a lesser extent longitude. This confirmed my early thoughts based on the exploratory data analysis in section 2.\n\nIf you\u2019ve made it this far, thank you for taking the time to read my analysis. I welcome any feedback in the comments. Consider checking out my previous [analysis of sex differences in suicide rates](https:\/\/www.kaggle.com\/thwaiteso\/analysis-of-sex-differences-in-suicide-rates-r) or a [binary classification of heart disease](https:\/\/www.kaggle.com\/thwaiteso\/binary-classification-of-heart-disease-r).\n\n","9153981d":"The median price for a house on the waterfront is **higher** than the average, but it isn\u2019t significantly different from the median price for a house not on the waterfront. I wouldn\u2019t expect \u2018waterfront\u2019 to be a particulary useful predictor","578788ba":"## 3.2 Shrinkage methods\n\n### 3.2.1 Ridge regression\n\nThe previous three approaches were methods for selecting subsets of predictors. Ridge regression is an approach that uses all predictors, but it shrinks the coefficent estimates (intercept and slope) towards zero which reduces their variance. Ridge regression does seek to minimise RSS (like least squares) but it introduces another term called a **shrinkage penalty**. The tuning paramter (*lambda*, \u03bb) controls the impact of both terms.\n\nWhen \u03bb = 0, the shrinkage penalty is not in effect, so ridge regression is just performing least squares. As \u03bb increases, the shrinkage penalty grows and the coefficients approach zero. When \u03bb is extremely large, the coefficients will be essentially zero (but never exactly) - leading to a null model with no predictors. How do you select a good value of \u03bb?\nCross-validation!\n\nYou can create a grid of \u03bb values, then use 10-fold cross-validation to train a ridge regression model for every value of \u03bb in the grid. The \u2018best\u2019 model with the \u2018best\u2019 \u03bb value is the model that has the lowest test MSE. Let\u2019s see how ridge regression performs:","f3a88ef7":"A test set MSE of 0.065 is worse than ridge regression and best subset selection but is better than forward and backward stepwise selection. ","f863afd8":"I'll then perform boosting, fitting a model to each combination of predictors:","9a00bc74":"# 2. Exploratory Data Analysis\n\nI am going to some initial data visualisation to understand what the trends of each variable are, before moving on to the predictive models.","3bb84d23":"The best test set MSE was found with an alpha of 0, 0.0005 or 0.001. I will select 0, this means the tree is not pruned at all. When fitting a tree with alpha = 0 to all the training data I will once again cap its depth at three.","7c62610b":"# 1. Introduction\nPredicting the price of a house is useful for both the seller and buyer. \nThe seller wants to ensure they are not under or overvaluing the house, as that could lead to missed profit or no interest from buyers. \nConversely, buyers will be interested in predicting a house price to see if they are getting a good deal on an undervalued house or if they are getting ripped off. \nThis dataset offers an opportunity to utilise multiple linear regression and tree-based methods to predict the prices of houses in King County, which includes Seattle, in the state of Washington. \nThese house prices are from May 2014 to May 2015. \n\nThis is the python equivalent to my previous analysis that I did in R. R is my preferred language but I wanted to learn some Python too and I thought a good way to learn it would be to take what I know in R and try to recreate it in Python. You can find the R project [here](https:\/\/www.kaggle.com\/thwaiteso\/advanced-regression-techniques-r).\n\nThe full code used in this analysis can be found on my [github](https:\/\/github.com\/thwaiteso\/Kaggle-Projects\/blob\/master\/Python\/Housing\/Housing.py).","d910ab49":"## 4.4 Boosting\n\nIn boosting, trees are grown slowly using information from previously grown trees, unlike bagging where trees are grown independently. Boosting does not utilse bootstrap sampling.\n\nThere are a number of parameters that can be changed to tune the performance:\n1. The shrinkage paramater (\u03bb) - this is the rate at which boosting learns\n2. The depth of the tree\n3. The minimum number of observations needed in a node\n4. The \u2018bag.fraction\u2019 paramater allowing stochastic gradient descent. In this approach, there is a global minimum of the loss function. If the loss function is U-shaped, it is not that difficult to find that minimum. However, if it shaped differently, the global minimum might be behind multiple local minimums which the algorithm would normally stop at. Stochastic gradient descent means a subset of the training data is used to grow a tree, a different subset for the bext tree, and so on. This increases the speed of running this algorithm, and while it doesn\u2019t guarantee the global minimum can be found it does make it more likely that local minimums and plateaus can be overcome.\n\nTherefore, I will use a number of different values for each parameter:","6b67681b":"Excellent, there are no missing data.","bb56c199":"## 2.7 Interactive map\n\nThis data set also provides latitude and longitude values for each house, meaning they can be plotted on a map. Each house is assigned to one of five price groups, with the colours denoted on the bottom left. Price groups can be selected or deselected using the layers tool on the bottom left. You can also click on individual houses and it will tell you the price, size of the house and the lot.","5330d445":"## 1.1 Import data\nThe dataset for this analysis can be found [here](https:\/\/www.kaggle.com\/harlfoxem\/housesalesprediction).\n","3fd3cd1e":"The number of floors appears to have little effect on house prices. If anything, the median house price for 3 and 3.5 floors is slightly less than or roughly equal to 2 and 2.5 floors. I would therefore not expect \u2018floors\u2019 to be a strong predictor of house price in the predictive models.","6ab4ad31":"## 2.4 House quality and its surrounding area \nThe next four variables I'll look at describe the quality of the house and its surrounding area. \n'waterfront' is a binary variable denoting if the house is on the waterfront ('1') or not ('0'). \n\n'view' is a variable describing the quality of the view from the house from 0-4, I couldn't find exactly what each value represents but a higher number is better.  \n\n'grade' denotes the quality of construction and design of the house, from 1-13 with [explanations](https:\/\/www.kingcounty.gov\/depts\/assessor\/Reports\/area-reports\/2017\/residential-westcentral\/~\/media\/depts\/assessor\/documents\/AreaReports\/2017\/Residential\/013.ashx) as follows:  \nGrades 1-3: Falls short of minimum building standards. Normally cabin or inferior structure.  \nGrade 4: Generally older low quality construction. Does not meet code.  \nGrade 5: Lower construction costs and workmanship. Small, simple design.  \nGrade 6: Lowest grade currently meeting building codes. Low quality materials, simple designs.  \nGrade 7: Average grade of construction and design. Commonly seen in plats and older subdivisions.  \nGrade 8: Just above average in construction and design. Usually better materials in both the exterior and interior finishes.  \nGrade 9: Better architectural design, with extra exterior and interior design and quality.  \nGrade 10: Homes of this quality generally have high quality features. Finish work is better, and more design quality is seen in the floor plans and larger square footage.   \nGrade 11: Custom design and higher quality finish work, with added amenities of solid woods, bathroom fixtures and more luxurious options.  \nGrade 12: Custom design and excellent builders. All materials are of the highest quality and all conveniences are present.  \nGrade 13: Generally custom designed and built. Approaching the Mansion level. Large amount of highest quality cabinet work, wood trim and marble; large entries.  \n\n'condition' describe the condition of the house in terms of the amount and urgency of repairs and maintenance needed, with [explanations](https:\/\/www.kingcounty.gov\/depts\/assessor\/Reports\/area-reports\/2017\/residential-westcentral\/~\/media\/depts\/assessor\/documents\/AreaReports\/2017\/Residential\/013.ashx) as follows:  \n1: Poor - Many repairs needed. Showing serious deterioration.  \n2: Fair - Some repairs needed immediately. Much deferred maintenance.  \n3: Average - Depending upon age of improvement; normal amount of upkeep for the age of the home.  \n4: Good - Condition above the norm for the age of the home. Indicates extra attention and care has been taken to maintain.  \n5: Very Good - Excellent maintenance and updating on home. Not a total renovation.  \n\nClearly we should be expecting house prices to increase if the house is on a waterfront and to increase as each of the other three variables increase (*note that price has been log transformed*):\n","962c365b":"## 2.3 Surrounding houses\n\nThere are two variable in this dataset, \u2018sqft_living15\u2019 and \u2018sqft_lot15\u2019, which describe the average square footage of the living area and average square footage of the lot of the closest 15 houses, respectively. The assumption here is that if a house is in a neighbourhood that has more expensive houses it is likely that that house is also expensive. Let\u2019s see how these variables affect house prices (*note that the two variables and price have been log transformed*):","0db97a27":"The lowest test MSE is with all 17 principal components, therefore it is essentially least squares. I'll now use all 17 components when training on all the training data:","09c3a306":"A test set MSE of 0.0623 is the second best so far.","31d5a437":"### 3.2.2 Lasso regression\n\nThis approach is also a shrinkage method, like ridge regression. In ridge regression, the fact that the coefficients are shrunk towards zero improves accuracy, but it can be difficult to interpret. Lasso regression overcomes this by shrinking coefficients towards zero and forces some to be exactly zero - it performs variable selection like the subset methods by removing variables.\n\nJust like ridge regression, lasso regression uses \u03bb. The same grid of \u03bb values will be used, then 10-fold cross-validation will be used to train a lasso regression model for every value of \u03bb in the grid. The \u2018best\u2019 model with the \u2018best\u2019 \u03bb value is the model that has the lowest test MSE. Let\u2019s see how lasso regression performs:","039d5e74":"## 3.3 Dimension reduction methods\n### 3.3.1 Principal components regression\nPrincipal components regression (PCR) is a dimension reduction technique.\nThere is one dimension for each predictor.\nThe first principal component is the direction through the data that captures the most variation in the data - a linear combination of all features.\nThe hope is that only a few components are responsible for most of the variation in the data and if all principal components are included in a model, this is just least squares.\nIt is similar to ridge regression, in that all the predictors are included - i.e. it does not perform variable selection.\n\nI'll refer you to some other sources for a better and more in-depth explanation: [Hands on Machine Learning with R, Chapter 17](https:\/\/bradleyboehmke.github.io\/HOML\/pca.html)\nand [An Introduction to Statistical Learning (page 231)](https:\/\/faculty.marshall.usc.edu\/gareth-james\/ISL\/ISLR%20Seventh%20Printing.pdf).\n\nYou decide on the number of components by performing cross-validation.\nPCR performs as follows:","bf5ca421":"Conversely, the mean square footage of the *lot* of the closest 15 houses has no obvious relationship with the price of a house. Most mean lot sizes are between 1,000 and 100,000 square feet, with little variation in house prices as this increases. I would expect sqft_living15 to be a better predictor of house prices that sqft_lot15 in the predictive models.","96e9e0fb":"A test set MSE of 0.062 is the best so far! Let's take a look at the models so far:","33152d22":"## 2.2 House age\n\nNext I want to see how many houses were built in a specific year and whether the age of the house has any effect on its current price.","23594967":"## 2.5 House dimensions\n\nAfter looking at the effects of the houses around a particular house and its external qualities, it makes sense to look at the dimensions of a house and its lot. Specifically, the square footage of the interior living space, lot, above ground area and below ground area, all of which are log transformed in addition to price:","81599b57":"Random forest is slightly worse than bagging but is still far superior to the linear model approaches.","350e2071":"As for the square footage of the lot, there isn\u2019t a noticeable increase in house price as square footage goes up, with most mean lot sizes being between 1,000 and 100,000 square feet as seen in the similar plot in **section 2.3**.","8b606dc5":"## 1.2 Tidy data\nLet's look at an overview of the data:","51067ce6":"## 2.1 Price distribution\n\nFirstly, I am going to distribution of the house prices (*note that price has been log transformed*):","dc2e4b1d":"### 3.1.3 Backwards stepwise selection\n\nThis approach is the opposite to forward stepwise - it starts with a model with all predictors. It then removes the least useful predictor at each model size. As with forward stepwise selection, this approach is faster than best subset selection but it doesn\u2019t necessarily find the true \u2018best\u2019 model. I will skip the analysis here, because it is very similar to the forward stepwise selection results.","990a20f7":"Finally, the results from \u2018condition\u2019 are curious. The median price for houses in top condition (\u20185\u2019) are not above the mean house price, though there is an increase in median house price as condition improves. As with \u2018waterfront\u2019 and \u2018view\u2019, I don\u2019t believe \u2018condition\u2019 will be a significant predictor.","37299e49":"You could keep on fine-tuning the parameters almost indefinitely, but I will stop for now. I will take the best model and add it to the matix.","256a57b4":"### 3.1.2 Forward stepwise selection\n\nThe next of the subset selection approaches is forward stepwise selection. In this approach, you start with a model with no predictors. Then, you add one variable to the model - the variable that results in the best reduction in RSS or increase in R<sup>2<\/sup>. This continues until you have all of the predictors in the model. Cross-validation is used here too - the process of adding variables takes place on the training folds and the model is tested on the held-out fold.\n\nHowever, forward stepwise selection may not necessarily select the most optimal combination of predictors at each level. Consider a dataset with predictors a, b, and c, where the best one variable model is with predictor a and the best two variable model is with predictors b and c. Best subset selection would find this combination, but forward stepwise selection wouldn\u2019t, because it has to choose predictor a for the first model and it builds on its previous selections.\n\nThe obvious question is then: why choose forward stepwise selection over best subset selection? The answer is that forward stepwise selection is faster than best subset selection. Best subset selection has to consider 2<sup>p<\/sup> models, where *p* equals the number of predictors, but forward stepwise selection considers 1+*p*(*p*+1)\/2 models.\n\nLet\u2019s see how forward stepwise selection performs:","cc5411c0":"The best model is with 16 variables, but a 12 variable model is the simplest model within one standard error of the best model. Therefore, the 12 variable model and its test set MSE will be added to the matrix.","4a1e483b":"## 3.1 Subset selection methods  \n### 3.1.1 Best subset selection\n\nBest subset selection takes the least squares approach, but as its name suggests it seeks to find the best **subset** of predictors. It does this by fitting a least squares regression for each possible combination of predictors - e.g. all of the models with only one of the predictors, all of the models with all of the combinations of two predictors, and so on. So, how do you determine what the \u2018best\u2019 model is? The most robust method is to validate each model under consideration and I will be using *k*-fold cross-validation.\n\nIn *k*-fold cross-validation the data you have is split into *k* folds (of approximately equal size) - essentially subsets of your data. For example, let\u2019s assume *k* = 10. Your model would be trained on nine of the 10 folds and tested on the remaining fold. The **mean squared error** (MSE) is calculated from the test on the held-out fold - this is a value for how close your models predictions are to the true values, so a *low* MSE indicates a more accurate model. Then, a different fold is chosen to be tested on, and your model is trained using the remaining nine folds. This continues until your model has been tested on each of the *k* folds, and the MSEs are averaged. This approach is repeated for each model under consideration.\n\nThe \u2018best\u2019 model is the one that has lowest MSE - that model has consistently been the most accurace across the validation process. I will be looking at many approaches to predicting the price of a house, so I have created a matrix to track the test MSE for each of these approaches.\n\nSo, let\u2019s perform best subset selection using *k*-fold cross-validation, where *k* = 10:","feb5f18d":"This single tree is the worst method so far. Fortunately, there are other methods that aggregate many trees to increase the predictive performance.","c608da37":"## 4.2 Bagging\n\n\u2018Bagging\u2019, aka bootstrap aggregation, is used to reduce the variance of a model. Bootstrapping is an approach where you take repeated samples from the same (training) data set. You train your model on each bootstrapped training set, and average their predictions.\n\nIn the context of trees, you can construct a tree for each bootstrapped training set and average their predictions. The trees are deep and unpruned - each tree therefore has high variance and low bias, but by averaging the trees you reduce the variance.","c4d7c2e5":"This test set MSE is a substantial improvement from a single tree, about twice as good and is actually the best performance so far. We're not done yet though.","b31016e4":"Here you can see the least squares estimate for the relationship between the data. The residuals are highlighted in red. The RSS is minimised such that any different intercept or slope would result in a higher RSS - i.e. the line you see cannot be changed or it will not fit the data as well. The residuals are added together, and that value is squared. This removes any negative sign, as we are only concerned with the total distance, not its direction.\n\nHowever, there are other approaches to improve the performance of the model other than least squares. These approaches seek to improve prediction accuracy and model interpretability.\n\nFor my predictive modelling, the variables \u2018id\u2019, \u2018date\u2019 and \u2018zipcode\u2019 will be removed. Furthermore, the variables \u2018price\u2019 ,\u2018sqft_living\u2019, \u2018sqft_lot\u2019, \u2018sqft_above\u2019, \u2018sqft_basement\u2019, \u2018sqft_living15\u2019 and \u2018sqft_lot15\u2019 will be log-transformed.","01bf0cb3":"So we can see that the ridge regression with the best \u03bb is the best model so far (with the lowest test MSE). Best subset selection is the best of the subset selection methods with forward and backward stepwise selection arriving at the same test MSE, slightly worse than best subset selection. ","7f3fb3cc":"You can see that generally there has been an **increase** in the number of houses built over time, but there are notable **declines** in the 1930s and 1970s. It is likely that the fall in the 30s is linked with the Great Depression which began with the Wall Street Crash in 1929. As for the 70s, the Cold War and Vietnam war were happening during this time and there was an oil crisis in 1973 and all may have contributed to the decline in houses built in that time.","4ae92921":"The rating of the view from a house has a marginal effect on median house price with a **slight increase** as view rating increases, but again it isn\u2019t really significant and I don\u2019t think it will be a useful predictor.","aa30bbd8":"# 3. Linear model approaches\n\nIn a linear model, a quantitative response can be predicted by one or more predictors, and you assume there is approximately a linear relationship between the response and the predictors. You typically fit a standard linear model using **least squares**.\n\nLeast squares is an approach to maximise the \u2018closeness\u2019 of the fit of the model to the data by minimising the **residual sum of squares** (RSS). The residual for an observation is the distance between the observed response and the predicted response from the model.\nConsider the graph below:","b347fa0d":"There is a similar story on this graph, highlighting that some houses have zero bathrooms. Again this doesn\u2019t seem realistic, so these houses will be removed too. As with the number of bedrooms, there is a general increase in median house price as the number of bathrooms increases.","f7b1b2f1":"It is immediately apparent that the more expensive houses are generally located near water and cheaper houses are generally more inland, confirming the observations in **section 2.4**. For example, there are no houses worth less than $500,000 on Mercer Island. The more expensive houses are also generally more northern than the cheapest houses, if you select only the cheapeast and most expensive price groups this is clear. Therefore, we might expect that latitude will be a significant predictor of the price of a house.","2ca6d38c":"# 4. Tree-based approaches\n\nTree-based methods involve segmenting the predictor space into separate regions. By following the \u2018rules\u2019 at each internal node, you will end up at a terminal node corresponding to a particular region. The prediction is the mean (or sometimes median) value for that region. See my [R version](https:\/\/www.kaggle.com\/thwaiteso\/advanced-regression-techniques-r) for a good illustration.\n\n## 4.1 Regression tree\n\nLet's see how a single regression tree performs. 'ccp_alpha' is the complexity parameter used to determine whether the tree needs pruning. I will use cross-validation to choose the best alpha from a range. I have capped the tree at a maximum depth of three because when left to be as complex as it likes, the resulting plot of the tree is illegible:","26c989da":"## 2.6 Floors, bedrooms and bathrooms\n\nHaving now analysed how the size of the house, the areas within it and the lot affects house prices, I\u2019ll now consider how the number of floors, bedrooms and bathrooms within a house affects its price, which is log transformed:","40ba537b":"\n# Advanced Regression Techniques - best subset selection, forward and backward stepwise selection, ridge and lasso regression, PCR, PLS, bagging, random forest and boosting\n## Ollie Thwaites\n## 04\/09\/20","cde236b0":"The plot shows that latitude is by far the most important variable with the grade of the house and the square footage of the living area rounding out the top tree.\n\nThat\u2019s all well and good, but is random forest an improvement over bagging?","67558503":"So, cross-validation has chosen a **16 variable** model. It is a subset, because the total number of predictors is 17, but really it\u2019s not much different than just including all predictors. Fortunately, we can select a model with less predictors, if the cross-validation error of a simpler model is within one standard error of the best model. See below:","cf5bd0ab":"For the below ground square footage, there isn\u2019t a lot of variation with most basements being around 1,000 square feet, with perhaps a slight trend upwards in price as size increases.","2d1371c0":"You can see a number of internal nodes used to split the data, starting with determining if the grade of the house is below or above 8.5. The number of (training) samples in each node is shown. The predictions ('value = ') are the house prices which were log-transformed - for reference 12.5 is 268,337.30, 13 is 442,413.40, 13.5 is 729,416.40 and 14 is 1,202,604.\n\nSo, how does the performance of this single tree stack up against the previous models?","610dc261":"The best model at size 16 is clearly the best model as it has the lowest test MSE. Furthermore, no other model is within one standard error of this model. Therefore this 16 variable model and its test set MSE will be added to the matrix.","457c24df":"However, the grade of the house clearly has a large effect on median house price. There is a **significant increase** in median house price as the grade of the house increases, with the lowest priced houses in the top grades (12 and 13) being higher than the highest priced houses in the bottom grades (1-4). I would expect \u2018grade\u2019 to be a significant predictor of house prices.","320aa37e":"21 variables explaining various features of the house itself and its surrounding environment, but I have an issue with some of them. The number of bathrooms is a decimal (float), ending in either .25, .5 or .75. The number of floors also has some values ending in .5. After some research, I believe the decimals refer to how \u2018full\u2019 the bathroom is - i.e. does it have a bath, shower, sink, toilet or a combination of all four. The problem with this system is a rating of \u20181.25\u2019 for the number of bathrooms implies one \u2018full\u2019 bathrooms and one \u2018.25\u2019 bathroom, but it could also be any combination of the decimal notations. I have decided to leave the variable as is, but it highlights that recording the data in a non-intuitive way can lead to problems with interpretation later on.\n\nFinally, I\u2019ll check if there is any missing data:","35da53f6":"Despite looking like I've repeated the same graph twice, these plots are different. Here you can see that the square footage of the total living area and the above ground appear to be correlated with price and you would expect these two variables to display similar information.","36826c98":"This test set MSE is almost identical to PCR, but is slightly better and therefore becomes the second best so far:","08925ee8":"## 4.3 Random forest\n\nA random forest is similar to bagging, in that a tree is fit to each bootstrapped training set. However, where bagging considers every variable when making a split, a random forest only considers a *random subset* of the predictors. This may sound counter-intuitive - why would you not want to consider every variable at each split?\n\nIf you have one very strong predictor, it is highly likely that in bagging this variable will be responsible for the first split in almost every tree - therefore the trees will look quite similar and the predictions from them will be highly correlated. Even by averaging these correlated predictions, the variance reduction is not as large as averaging uncorrelated predctions. This is where random forest steps in - by considering a (random) subset of predictors at each split, the trees will be *decorrelated* where all predictors are given more of a chance.\n\nThe number of predictors in each subset is therefore a factor to consider, but I will use the default value is this analysis, which is five in this case.\n\nBy using a subset of predictors at each split, you can determine which variables are more important in predicting the response:","40a08d57":"As for the mean house price for houses built in a particular year, houses built pre-1940 and post-1980 are generally **worth more** than average now whereas houses built from 1940-1980 are **worth less** on average. Perhaps there is a \u2018reverse sweet spot\u2019, where the houses built in 1940-1980 are not old enough to have the charm or appeal of the older houses and they are not new enough to be suitable for modern lifestyles.","99d66654":"The dashed red line denotes the mean house price: **$540,088**. You can see that not many houses are priced over 1 million dollars, the majority are below that and are roughly centered around the mean price.","7169a36d":"# 5. Conclusion\n\nI\u2019ll recap what I\u2019ve done in this analysis:  \n1) I used 11 different methods to predict house prices, seven based on the linear model and four tree-based methods  \n2) For each method, models were fit to training data, with some type of validation used to determine what the best model was  \n3) The best model for a given method was then tested on a test set, with the resulting MSE placed in a matrix  \n4) The matrix allows all of these models to be compared, with the results outlined below:","47c4f888":"The lowest test set MSE is again with all 17 components, but you can see that there is very little change after four components. Therefore, I am going to use four components when fitting to all the training data:","92b7471a":"### 3.3.2 Partial least squares\n\nPartial least squares (PLS) is also a dimension reduction techinque. PCR was an unsupervised approach - the response was not used to help determine the principal component directions. So, the directions might have explained the predictors but those directions are not necessarily the best directions for predicting the response.\n\nPLS overcomes this as a supervised approach by utilising the response to identify components that explain the predictors and the response. Once again, the number of components can be determined using cross-validation."}}