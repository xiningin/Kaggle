{"cell_type":{"4fbaf250":"code","d95ea7d9":"code","4655b0b8":"code","808b3de4":"code","307f4b30":"code","b4b32a27":"code","0b01a0de":"code","64fe70be":"code","d591355e":"code","a93daafe":"code","62b0f5c5":"code","878dad69":"code","ccd49367":"code","b9a3d9c0":"code","0f9a5152":"code","fac92275":"code","198ffe67":"code","3ee1e385":"code","3da222e0":"code","9353663c":"code","aba800fd":"code","94b8a56c":"code","055c94d6":"code","c75a156f":"code","1a7bde15":"code","05fac410":"code","2837c9ab":"code","f3521d34":"code","56d951c9":"code","492c57c3":"code","e4e40996":"code","09c054c2":"code","3b95e218":"code","0e6b5539":"code","0eb0f57d":"code","134a8a27":"code","09a8d3a7":"code","54b0ba9d":"code","ecdc1430":"code","00bd76ce":"code","0393f868":"code","f5bf06a8":"code","77b780c1":"code","c77fff38":"markdown","86271a2a":"markdown","eeeef9c7":"markdown","6dde7d5a":"markdown","c0483bc5":"markdown","957d6457":"markdown","58b54980":"markdown","3b3246d9":"markdown"},"source":{"4fbaf250":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d95ea7d9":"%ls '..\/input\/nlp-getting-started\/'","4655b0b8":"import warnings\nwarnings.filterwarnings(\"ignore\")\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport nltk\nimport re\nimport string\nimport emoji\nimport pickle\nfrom wordcloud import WordCloud, STOPWORDS\nimport matplotlib.pyplot as plt\nfrom sklearn.tree import DecisionTreeClassifier\nfrom tensorflow.keras.layers import Embedding, Dropout\nfrom tensorflow.keras.preprocessing.sequence  import pad_sequences\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.preprocessing.text import one_hot\nfrom tensorflow.keras.layers import LSTM\nfrom tensorflow.keras.layers import Dense\nnltk.download('stopwords')","808b3de4":"nltk.download('punkt')\nnltk.download('averaged_perceptron_tagger')\nnltk.download('maxent_ne_chunker')\nnltk.download('words')\nnltk.download('wordnet')\nfrom nltk import ne_chunk, pos_tag, word_tokenize\nfrom nltk.tree import Tree","307f4b30":"train = pd.read_csv(\"..\/input\/nlp-getting-started\/train.csv\")\ntest = pd.read_csv(\"..\/input\/nlp-getting-started\/test.csv\")","b4b32a27":"print(\"The number of rows and columns in train data is \", train.shape)\nprint(\"The number of rows and columns in test data is \", test.shape)","0b01a0de":"# Count of positive and negative target in train data\ntrain['target'].value_counts()","64fe70be":"sns.countplot(train['target'], data=train )","d591355e":"# Check for null values in train data\ntrain.isnull().sum()\/len(train)","a93daafe":"# Check for null values in test data\ntest.isnull().sum()\/len(train)","62b0f5c5":"train = train.drop(['location','keyword'], axis=1)\ntest  = test.drop(['location','keyword'], axis=1)","878dad69":"train.head()","ccd49367":"# Lets have a look at sample tweet\ntrain['text'].iloc[6]","b9a3d9c0":"def clean_text(text):\n  # Delete all the tags like \"< anyword >\"\n  text = re.sub(\"\\<([^)]+)\\>\", \"\", text)\n\n  # Delet data present in brackets\n  text = re.sub(\"\\(([^)]+)\\)\", \"\", text)\n  text = re.sub(\"\\[([^)]+)\\]\", \"\", text)\n\n  text = re.sub(r'@[A-Za-z0-9_]+','',text)\n  text = re.sub('#','', text)\n\n  # Remove all the newlines('\\n'), tabs('\\t'), \"-\", \"\\\".\n  text = re.sub(\"\\n\", \"\",text)\n  text = re.sub(\"\\t\", \"\",text)\n  text = re.sub(\"-\", \"\",text)\n\n  #Decontraction\n  # specific\n  text = re.sub(r\"won\\'t\", \"will not\", text)\n  text = re.sub(r\"can\\'t\", \"can not\", text)\n\n  # general\n  text = re.sub(r\"n\\'t\", \" not\", text)\n  text = re.sub(r\"\\'re\", \" are\", text)\n  text = re.sub(r\"\\'s\", \" is\", text)\n  text = re.sub(r\"\\'d\", \" would\", text)\n  text = re.sub(r\"\\'ll\", \" will\", text)\n  text = re.sub(r\"\\'t\", \" not\", text)\n  text = re.sub(r\"\\'ve\", \" have\", text)\n  text = re.sub(r\"\\'m\", \" am\", text)\n  \n\n  #Replace all the digits with space\n  text = re.sub(\"\\d\", \"\", text)\n  \n  # Remove emojis\n  text = re.sub(emoji.get_emoji_regexp(), r\"\", text)\n\n  #Remove URLSs\n  text = re.sub(r'https?:\\\/\\\/[A-Za-z0-9\\.\\\/]+','',text)\n  text = re.sub(r\"https?:\/\/\\S+|www\\.\\S+\",\"\",text)\n\n  # Convert all the words into lower case and lowe case and remove the words which are greater than or equal to 15 or less than or equal to 2.\n  text = \" \".join([e.lower() for e in text.split(\" \") if (len(e)>2 and len(e)<15)])\n  text = text.strip()\n\n  return text\n","0f9a5152":"#Function to remove punctuation\ndef remove_punc(text):\n  # define punctuation\n  punctuations = '''!()-[]{};:'\"\\,<>.\/?@#$%^&*_~'''\n  new_text = \"\"\n  for char in text:\n    if char not in punctuations:\n      new_text = new_text + char\n\n  return new_text","fac92275":"# Function to remove stopwords from text\nfrom nltk.corpus import stopwords\ndef remove_stopwords(text):\n  stop_words = set(stopwords.words('english'))\n  new_text = \"\"\n\n  for word in text.split():\n    if word not in stop_words:\n      new_text = new_text+\" \"+word\n  return new_text","198ffe67":"# Lemmatization\nfrom nltk.stem import WordNetLemmatizer \ndef lemmatize(text):\n  lemmatizer = WordNetLemmatizer()\n  sent = \" \".join([lemmatizer.lemmatize(word) for word in text.split()])\n  return sent","3ee1e385":"# Apply all preprocessing functions on train data\ntrain['text'] = train['text'].apply(remove_punc)\ntrain['text'] = train['text'].apply(remove_stopwords)\ntrain['text'] = train['text'].apply(clean_text)\ntrain['text'] = train['text'].apply(lemmatize)","3da222e0":"# Apply all preprocessing functions on test data\ntest['text'] = test['text'].apply(remove_punc)\ntest['text'] = test['text'].apply(remove_stopwords)\ntest['text'] = test['text'].apply(clean_text)\ntest['text'] = test['text'].apply(lemmatize)","9353663c":"# Get all the preprocessed tweets from training data into a list\ncorpus = list(train['text'])","aba800fd":"# Vocabulary size\nvoc_size = 10000","94b8a56c":"# Convert corpus to one hot representaion\none_hot_rep = [one_hot(word, voc_size) for word in corpus]","055c94d6":"# Get the average length of tweet\nmax_length = np.mean([len(doc) for doc in corpus])\nmax_length = int(max_length)","c75a156f":"# All the sentences\/docs in corpus should have same length.So we use padding.\nembedded_docs = pad_sequences(one_hot_rep, padding='pre',maxlen=max_length)\nprint(embedded_docs)","1a7bde15":"X = np.array(embedded_docs)\nY = np.array(train['target'])","05fac410":"print(X.shape)","2837c9ab":"print(Y.shape)","f3521d34":"# Split data for training and testing\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size=0.25, stratify = Y)","56d951c9":"print(\"Shape of train data\", X_train.shape)","492c57c3":"print(\"Shape of test data\", X_test.shape)","e4e40996":"# Creating LSTM model\nfrom tensorflow.keras.optimizers import Adam\ndim = 300\nmodel = Sequential()\nmodel.add(Embedding(voc_size, dim, input_length=max_length))\nmodel.add(Dropout(0.4))\nmodel.add(LSTM(32, recurrent_dropout=0.4))\nmodel.add(Dropout(0.4))\nmodel.add(Dense(1,activation='sigmoid'))\nmodel.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=0.0001), metrics=['accuracy'])\nprint(model.summary())","09c054c2":"model.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs=10, batch_size=64)","3b95e218":"y_pred = model.predict_classes(X_test)","0e6b5539":"# Lets have  a look at confusion matrix\nfrom sklearn.metrics import confusion_matrix, f1_score, accuracy_score\nconfusion_matrix(Y_test, y_pred) ","0eb0f57d":"print(\"Accuray for test data is\", accuracy_score(Y_test, y_pred))","134a8a27":"print(\"F1 score for test data is\", f1_score(Y_test, y_pred))","09a8d3a7":"test_corpus = list(test['text'])","54b0ba9d":"one_hot_rep = [one_hot(word, voc_size) for word in test_corpus]","ecdc1430":"embedded_docs_test = pad_sequences(one_hot_rep, padding='pre',maxlen=max_length)","00bd76ce":"test_labels = model.predict_classes(embedded_docs_test)","0393f868":"test_labels","f5bf06a8":"# Convert labels to list to add in submission file\ntest_labels_list = [i[0] for i in test_labels] ","77b780c1":"submission=pd.DataFrame({'id': test['id'], 'target':test_labels_list})\nprint(submission.head(10))\n\nfilename = 'submission.csv'\n\nsubmission.to_csv(filename,index=False)","c77fff38":"### Word Embedding","86271a2a":"### Load Libraries","eeeef9c7":"### Modelling","6dde7d5a":"### Convert test tweets to embeddings","c0483bc5":"1. Around 33% of location values are missing from train data. So lets drop Location feature.\n2. Keyword are words picked from the tweet text. So we can drop keyword also.","957d6457":"### Data Preprocessing","58b54980":"### Load Data","3b3246d9":"### Predict class labels for test tweet"}}