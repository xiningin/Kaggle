{"cell_type":{"daed155b":"code","d28541e8":"code","4f1c220e":"code","38dcafc2":"code","ce440a5a":"code","2a7ef798":"code","e79d38ac":"code","191786f8":"code","e20b894e":"code","9d2e8bc9":"code","d191cd81":"code","324047a2":"code","9018e149":"code","37486f69":"code","d83c7a4f":"code","b218f9bf":"code","c439c009":"code","ed0b8794":"code","88416ecc":"code","87536b15":"code","d74a5c9c":"code","23cc36cc":"code","ac9b60e0":"code","f2cb4f41":"code","938088d7":"code","eb2a8c2c":"code","eb3e18ef":"code","b59df99f":"code","bd6be882":"code","1313b292":"code","0289b54e":"code","0ae4b62a":"code","38b3f5a5":"code","ec8ce8e0":"code","5fddecb4":"code","d1df7f5f":"code","bad5926e":"code","171b911f":"code","85a660cb":"code","2a80d7a1":"code","8797e78c":"code","317649fb":"code","ccaad8a6":"code","297f155c":"code","1b9763fc":"code","60e0055b":"code","e36b1938":"code","e74fd70c":"code","73357bde":"code","bf17bd53":"code","df689014":"code","306cfe9d":"code","e8014039":"code","0c7f9b12":"code","6d44e95e":"code","15bf77af":"code","32a39ac5":"code","8ba5cf3a":"code","f6d2ae83":"code","a010985d":"code","95d3879b":"code","a987ae51":"code","df7892af":"code","5721ea51":"code","4321aaa7":"code","6574447f":"code","6e270e67":"code","c0b3c255":"code","31d4c2ac":"code","dc9abe39":"code","4c5ea3ff":"code","817b1d79":"code","3cae31b1":"code","a6e61749":"code","3f26c423":"code","5087523f":"code","92aec666":"code","606ddcd2":"code","80aa8fbf":"code","fefb2744":"code","fe14a427":"code","dcfbc69c":"code","1cb270e0":"code","6a7e88f2":"code","2c00f9f9":"code","db407fec":"code","b85edd9e":"markdown","0ab61be6":"markdown","21eb6571":"markdown","05f309c4":"markdown","7a3d6b46":"markdown","dae09c61":"markdown","98918ba4":"markdown","405c4001":"markdown","77cb32e8":"markdown","e3c7162d":"markdown","f057b174":"markdown","cb400d49":"markdown","252609c7":"markdown","665cd917":"markdown","6d69add0":"markdown","2ffb4885":"markdown","799c15fd":"markdown","fa2ea058":"markdown","e362bf62":"markdown","e4b49d7f":"markdown","35480fb9":"markdown","ba1ce2e1":"markdown","0928cbea":"markdown","fae9b245":"markdown","e196f668":"markdown","1113eac6":"markdown","27ea2f39":"markdown","6c78d650":"markdown","e3230588":"markdown","8f8177e7":"markdown","e1d52f32":"markdown","e9052bec":"markdown","5155b419":"markdown","a3f1ab6f":"markdown","5d441a85":"markdown","b3c5e0fe":"markdown","379b9925":"markdown","4dff85dc":"markdown","8daec024":"markdown","76449cb9":"markdown","c2def857":"markdown","1e78da89":"markdown","da7ce2ff":"markdown","4a40f2f2":"markdown","f5bc177f":"markdown","28be35d7":"markdown","ee9b2d60":"markdown","87a23f29":"markdown","faabd03d":"markdown","e56bef39":"markdown","8b9a828a":"markdown","f2b0c854":"markdown","4f64f0f6":"markdown","5136dc14":"markdown","d4344ac1":"markdown"},"source":{"daed155b":"import datetime\nimport numpy as np\nfrom scipy import stats, special\nimport statsmodels.formula.api as smf\nimport pandas as pd\nfrom pandas.api.types import CategoricalDtype\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder, Imputer, RobustScaler\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.linear_model import LinearRegression, ElasticNet\nfrom sklearn.pipeline import Pipeline\n\n# from skopt import BayesSearchCV\n# from skopt.space import Real, Categorical, Integer\n\nfrom xgboost.sklearn import XGBRegressor\n\n%config InlineBackend.figure_format = 'retina'\nplt.rcParams['figure.dpi']=200","d28541e8":"houses     = pd.read_csv('..\/input\/train.csv', index_col='Id')\nhousesTest = pd.read_csv('..\/input\/test.csv',  index_col='Id')\n\ntrainSet = []\ntrainSet.append(houses.shape[0])\ntrainSet.append(houses.shape[1])\ntestSet  = housesTest.shape\n\nhouses = pd.concat([houses,housesTest])","4f1c220e":"def renameColumns(df):\n    # Rename columns which names start with a number to workaround problems with statsmodels library\n\n    toRename={}\n    for c in df.columns:\n        if c[0].isdigit():\n            toRename[c]='n'+c\n\n    df.rename(columns=toRename, inplace=True)","38dcafc2":"renameColumns(houses)","ce440a5a":"# All _UPPERCASE categories will be used for imputed NaNs\n# _UNAVAILABLE: item is unavailable in the house, e.g. no pool, no garage in the house\n# _UNKNOWN: a serious data leak\n\ncategoricalFeatures=[\n     'BldgType',\n     'Condition1',\n     'Condition2',\n     'Electrical',\n     'Exterior1st',\n     'Exterior2nd',\n     'Foundation',\n     'Heating',\n     'MSZoning',\n     'MasVnrType',\n     'MiscFeature',\n     'Neighborhood',\n     'RoofMatl',\n     'RoofStyle',\n     'SaleCondition',\n     'SaleType'\n]\n\n# According to data_description.txt, most NaN in following features have a meaning\ncategoricalOrderedFeatures={\n    'Alley':        ['_UNAVAILABLE','Grvl', 'Pave'],\n    'BsmtCond':     ['_UNAVAILABLE','Po','Fa','TA','Gd','Ex'],\n    'BsmtExposure': ['_UNAVAILABLE','No', 'Mn', 'Av', 'Gd'],\n    'BsmtFinType1': ['_UNAVAILABLE','Unf','LwQ','Rec','BLQ','ALQ','GLQ'],\n    'BsmtFinType2': ['_UNAVAILABLE','Unf','LwQ','Rec','BLQ','ALQ','GLQ'],\n    'BsmtQual':     ['_UNAVAILABLE','Po','Fa','TA','Gd','Ex'],\n    'CentralAir':   ['N','Y'],\n    'ExterCond':    ['Po','Fa','TA','Gd','Ex'],\n    'ExterQual':    ['Po','Fa','TA','Gd','Ex'],\n    'Fence':        ['_UNAVAILABLE','MnWw','GdWo','MnPrv','GdPrv'],\n    'FireplaceQu':  ['_UNAVAILABLE','Po','Fa','TA','Gd','Ex'],\n    'Functional':   ['_UNKNOWN','Sal','Sev','Maj2','Maj1','Mod','Min2','Min1','Typ'],\n    'GarageCond':   ['_UNAVAILABLE','Po','Fa','TA','Gd','Ex'],\n    'GarageFinish': ['_UNAVAILABLE','Unf','RFn','Fin'],\n    'GarageQual':   ['_UNAVAILABLE','Po','Fa','TA','Gd','Ex'],\n    'GarageType':   ['_UNAVAILABLE','Detchd','CarPort','BuiltIn','Basment','Attchd','2Types'],\n    'HeatingQC':    ['Po','Fa','TA','Gd','Ex'],\n    'HouseStyle':   ['SLvl', 'SFoyer', '2.5Unf', '2.5Fin','2Story', '1.5Unf', '1.5Fin', '1Story'],\n    'KitchenQual':  ['_UNKNOWN','Po','Fa','TA','Gd','Ex'],\n    'LandContour':  ['Low','HLS','Bnk','Lvl'],\n    'LandSlope':    ['Sev','Mod','Gtl'],\n    'LotConfig':    ['Inside','Corner','CulDSac','FR2','FR3'],\n    'LotShape':     ['IR3','IR2','IR1','Reg'],\n    'PavedDrive':   ['N','P','Y'],\n    'PoolQC':       ['_UNAVAILABLE','Po','Fa','TA','Gd','Ex'],\n    'Street':       ['Grvl','Pave'],\n    'Utilities':    ['_UNKNOWN','NoSeWa','AllPub']\n}\n\n\n# Features which values might be unknown before house sale\nfutureFeatures = [\n    'MoSold',\n    'SaleType',\n    'SaleCondition',\n    'SalePrice',\n    'YrSold'\n]","2a7ef798":"def convertCategorical(df):\n    df['Functional'].fillna('Typ', inplace=True) # according to data_description.txt\n\n    # NaN in following features are trully _UNKNOWN\n    df['Utilities'].fillna('_UNKNOWN', inplace=True)\n    df['MSZoning'].fillna('_UNKNOWN', inplace=True)\n\n\n    df['Electrical'].fillna('_UNKNOWN', inplace=True)\n    df['Exterior1st'].fillna('_UNKNOWN', inplace=True)\n    df['Exterior2nd'].fillna('_UNKNOWN', inplace=True)\n    df['KitchenQual'].fillna('_UNKNOWN', inplace=True)\n    df['SaleType'].fillna('_UNKNOWN', inplace=True)\n    \n    # NaN in following features are trully _UNAVAILABLE\n    df['MiscFeature'].fillna('_UNAVAILABLE', inplace=True)\n    df['MasVnrType'].fillna('_UNAVAILABLE', inplace=True)\n\n\n    # Convert some columns to Pandas category data type, ordered and unordered\n    for col in categoricalFeatures:\n        df[col] = df[col].astype(CategoricalDtype())\n        df[col].cat.add_categories(['_AGGREGATED_MINORITIES'],inplace=True)\n\n    for col in categoricalOrderedFeatures.keys():\n        # All NaNs have a meaning here\n        # We can make this imputation after inspecting NaN a few cells below\n        df[col].fillna('_UNAVAILABLE', inplace=True)\n        df[col] = df[col].astype(CategoricalDtype(categories=categoricalOrderedFeatures[col],ordered=True))\n        df[col].cat.add_categories(['_AGGREGATED_MINORITIES'],inplace=True)","e79d38ac":"convertCategorical(houses)\nhouses.info()","191786f8":"# Inspect order of each category.\n# Find NaNs and figure out what to do with them...\nfor c in houses.dtypes[houses.dtypes=='category'].index.sort_values():\n    if np.nan in houses[c].unique().tolist():\n        # must print nothing because we handled NaNs a few cells above\n        print(c)\n        print(houses[c].unique())\n        print(houses[c].unique().tolist())\n        print()","e20b894e":"houses.plot.scatter(x='GrLivArea',y='SalePrice');","9d2e8bc9":"houses[(houses.GrLivArea>4000) & (houses.SalePrice<300000)][['GrLivArea','SalePrice']]","d191cd81":"def dropOutliers(df):\n    if 'SalePrice' in df.columns:\n        df.drop(df[(df.GrLivArea>4000) & (df.SalePrice<300000)].index,inplace=True)","324047a2":"dropOutliers(houses)\ntrainSet[0]=trainSet[0]-2","9018e149":"def skewAnalysis(df):\n    \"\"\"\n    Return a DataFrame with skeweness of each numeric feature, most skewed first.\n    High ranked skew variables are candidates to be log()ed or boxcox()ed before any regression.\n    \n    Written by Avi Alkalay\n    avi at unix dot sh\n    http:\/\/Avi.Alkalay.net\n    2018-09-27\n    \"\"\"\n    skewList=df.select_dtypes('number').apply(lambda x: stats.skew(x, nan_policy='omit')).sort_values(ascending=False)\n    nTypes=skewList.axes[0]\n    \n    sk=pd.DataFrame({'skew' : skewList})\n    sk['rank'] = range(1, len(sk) + 1)    \n    \n    for i in nTypes:\n        shift = 0.0\n        \n        if df[i].min() <= 0.0:\n            shift=-df[i].min()+1\n            \n        sk.loc[i,'BoxCox \u03bb'] = stats.boxcox_normmax(df[i]+shift, method='mle')\n        sk.loc[i,'BoxCox shift'] = shift\n        sk.loc[i,'Has NaN'] = df[i].isnull().values.any()\n        \n    return sk\n\n\n\n\ndef analyzeNormalization(df,variables,typ='log',bins=None):\n    \"\"\"\n    Plots graphics for each variable passed in variables array used to visualy spot if it\n    requires a log or boxcox transformation. A value distribution before and\n    after (typ=)log\/boxcox transformation against a perfect Gaussian curve and probability\n    plot before and after the (typ=)log\/boxcox transformation, summing 4 plots per variable.\n    \n    Written by Avi Alkalay\n    avi at unix dot sh\n    http:\/\/Avi.Alkalay.net\n    2018-09-27\n    \"\"\"\n    # typ=boxcox still unstable, waiting for scikit 1.1 for better boxcox implementation\n    \n    for v in variables:\n        shift=0.0\n        \n        if df[v].min() <= 0.:\n            shift=-df[v].min() + 1\n            \n        fig=plt.figure(num=f'Gaussian fit analisys for \u00ab{v}\u00bb')\n        fig.suptitle(f'Gaussian fit analisys for \u00ab{v}\u00bb',fontweight='bold')\n\n        axs=fig.subplots(nrows=2,ncols=2)\n\n        # Set common style for all 4 subplots...\n        for row in axs:\n            for axes in row:\n                # add grid:\n                axes.grid(linewidth=0.1)\n                \n                # thinner line width and smaller scatter size:\n                axes.set_prop_cycle(linewidth=[.5],markersize=[1])\n                \n                # remove ticks:\n                axes.tick_params(which='both',\n                                 bottom=False, top=False,\n                                 left=False, right=False,\n                                 labelbottom=False, labeltop=False,\n                                 labelleft=False, labelright=False)\n\n        \n        #(\u03bc,\u03c3) = stats.norm.fit(df[v])\n        sns.distplot(df[v],fit=stats.norm,ax=axs[0][0],bins=bins)\n        axs[0][0].set_title(f'Dist \u00d7 perfect Gauss curve', fontsize='small')\n        axs[0][0].set_ylabel('Frequency', fontsize='small')\n        axs[0][0].set_xlabel(f'{v}', fontsize='small')\n        #axs[0][0].legend(['Gauss curve (\u03bc={:.2f} \u2022 \u03c3={:.2f})'.format(\u03bc,\u03c3),f'\u00ab{v}\u00bb'], loc='best')\n        axs[0][0].legend(['Normal'], loc='best')\n\n\n        if typ == 'boxcox':\n            v_trans, \u03bb = stats.boxcox(df[v] + shift)\n        else:\n            v_trans = np.log(df[v] + shift)\n\n        #(\u03bc,\u03c3) = stats.norm.fit(v_log)\n        sns.distplot(v_trans,fit=stats.norm,ax=axs[1][0],bins=bins)\n        axs[1][0].set_ylabel('Frequency', fontsize='small')\n\n        if typ == 'boxcox':\n            axs[1][0].set_xlabel('BoxCox({}), \u03bb={:.3f}'.format(v,\u03bb), fontsize='small')\n        else:\n            axs[1][0].set_xlabel(f'log\u2091({v})', fontsize='small')\n\n        a=stats.probplot(df[v], fit=True, plot=axs[0][1])\n        a=stats.probplot(v_trans, fit=True, plot=axs[1][1])\n        axs[1][1].set_title('')\n        axs[0][1].set_xlabel('')\n        \n        # cleanup\n        del v_trans\n        del a\n        del axs\n        del axes\n        del fig\n\n","37486f69":"pd.set_option('display.max_rows', 100)\nskewAnalysis(houses)","d83c7a4f":"pd.reset_option('display.max_rows')","b218f9bf":"houses['KitchenQual'].value_counts().plot(kind='bar')","c439c009":"houses['KitchenQual'].value_counts()","ed0b8794":"houses[(houses.KitchenQual=='_UNKNOWN')][['KitchenQual']]","88416ecc":"houses['KitchenQual'].mode().head(1)","87536b15":"def modeToFeatureValue(df,feature,value):\n    df.loc[df[feature]==value, feature] = df[feature].mode().head(1).values[0]","d74a5c9c":"modeToFeatureValue(houses,'KitchenQual','_UNKNOWN')\nhouses['KitchenQual'].value_counts()","23cc36cc":"inspect=['Utilities','MSZoning','Electrical','Exterior1st','Exterior2nd','SaleType']\n\nfig=plt.figure()\n\ni=1\nfor f in inspect:\n    ax=fig.add_subplot((len(inspect)\/\/2)+(len(inspect)%2),2,i)\n    houses[f].value_counts().plot(kind='bar',title=f,ax=ax)\n    i=i+1\n\nplt.rcParams['figure.dpi']=200\nplt.show()","ac9b60e0":"for f in inspect:\n    print()\n    print(f'Distribution of \u00ab{f}\u00bb values:')\n    print(houses[f].value_counts(normalize=True))\n","f2cb4f41":"modeToFeatureValue(houses,'Utilities','_UNKNOWN')\nmodeToFeatureValue(houses,'MSZoning','_UNKNOWN')\nmodeToFeatureValue(houses,'Electrical','_UNKNOWN')\nmodeToFeatureValue(houses,'Exterior1st','_UNKNOWN')\nmodeToFeatureValue(houses,'Exterior2nd','_UNKNOWN')\nmodeToFeatureValue(houses,'SaleType','_UNKNOWN')","938088d7":"# Figure out what to do with NaNs on MasVnrArea\nhouses[houses.MasVnrType=='_UNAVAILABLE'][['MasVnrType','MasVnrArea']]","eb2a8c2c":"def zeroToFeature(df,feature):\n    df[feature].fillna(0, inplace=True)","eb3e18ef":"def meanToFeature(df,feature):\n    df[feature]=Imputer().fit_transform(df[[feature]])","b59df99f":"zeroToFeature(houses,'MasVnrArea')\nhouses[houses.MasVnrType=='_UNAVAILABLE'][['MasVnrType','MasVnrArea']]","bd6be882":"# Figure out what to do with NaNs on GarageYrBlt\n\n# Compare this NaNed feature with other Garage-related features\nhouses[(pd.isnull(houses.GarageYrBlt)) & (houses.GarageCars!=0.0 )][['GarageYrBlt','GarageCars','GarageArea']]","1313b292":"def copyToFeature(df,source,target):\n    \"\"\"Fill NaNs on target feature with what is on source feature\"\"\"\n    df.loc[pd.isnull(df[target]),target] = df[pd.isnull(df[target])][source]","0289b54e":"copyToFeature(houses,'YearBuilt','GarageYrBlt')","0ae4b62a":"houses[pd.isnull(houses['GarageYrBlt'])][['GarageYrBlt']]","38b3f5a5":"#meanToFeature(houses,'GarageYrBlt')\nhouses[houses.GarageCars==0][['GarageCars','GarageYrBlt','GarageArea']].head(80)","ec8ce8e0":"houses[pd.isnull(houses.GarageCars)][['GarageCars','GarageArea']]","5fddecb4":"zeroToFeature(houses,'GarageCars')\nzeroToFeature(houses,'GarageArea')\n\nhouses.loc[2577][['GarageCars']]","d1df7f5f":"houses[(pd.isnull(houses.BsmtFinSF1))|(pd.isnull(houses.BsmtFinSF2))][['BsmtFinSF1','BsmtFinType1','BsmtFinSF2','BsmtFinType2']]","bad5926e":"houses[(houses.BsmtFinType1=='_UNAVAILABLE') | (houses.BsmtFinType2=='_UNAVAILABLE')][['BsmtFinSF1','BsmtFinType1','BsmtFinSF2','BsmtFinType2']]","171b911f":"zeroToFeature(houses,'BsmtFinSF1')\nzeroToFeature(houses,'BsmtFinSF2')","85a660cb":"houses[(pd.isnull(houses.BsmtHalfBath)) | (pd.isnull(houses.BsmtFullBath))][['BsmtHalfBath','BsmtFullBath']]","2a80d7a1":"zeroToFeature(houses,'BsmtHalfBath')\nzeroToFeature(houses,'BsmtFullBath')","8797e78c":"houses[(pd.isnull(houses.TotalBsmtSF))][['TotalBsmtSF','BsmtQual']]","317649fb":"zeroToFeature(houses,'TotalBsmtSF')","ccaad8a6":"houses[(pd.isnull(houses.BsmtUnfSF))][['BsmtUnfSF','BsmtQual']]","297f155c":"zeroToFeature(houses,'BsmtUnfSF')","1b9763fc":"def aggregateMinorCategories(df,rate=0.05,targetCategory='_AGGREGATED_MINORITIES'):\n    for feature in list(categoricalOrderedFeatures.keys())+categoricalFeatures:\n        for c, r in df[feature].value_counts(normalize=True).items():\n            if r<rate:\n                df.loc[df[feature]==c, feature] = targetCategory","60e0055b":"# Figure out what to do with NaNs on LotFrontage\n\n# Lets see if we can estimate LotFrontage from LotArea...\nstudylotfrontage=houses[['LotArea','LotFrontage']].copy()\nstudylotfrontage['LogLotArea']=np.log(studylotfrontage['LotArea'])\nstudylotfrontage['LogLotFrontage']=np.log(studylotfrontage['LotFrontage'])\n\nplt.rcParams['figure.dpi']=200\npd.plotting.scatter_matrix(studylotfrontage);","e36b1938":"plt.scatter(x=studylotfrontage['LogLotArea'],y=studylotfrontage['LogLotFrontage'],s=1)\nplt.xlabel(\"LogLotArea\")\nplt.ylabel(\"LogLotFrontage\")\nplt.show()","e74fd70c":"# Use original data\nlotFrontageEstimator = LinearRegression().fit(\n    X=np.log(houses[pd.notnull(houses.LotFrontage)][['LotArea']]),\n    y=np.log(houses[pd.notnull(houses.LotFrontage)][['LotFrontage']])\n)\nprint('R\u00b2={}'.format(lotFrontageEstimator.score(\n    X=np.log(houses[pd.notnull(houses.LotFrontage)][['LotArea']]),\n    y=np.log(houses[pd.notnull(houses.LotFrontage)][['LotFrontage']])\n)))","73357bde":"predictedLotFrontage=lotFrontageEstimator.predict(np.log(houses[pd.isnull(houses.LotFrontage)][['LotArea']]))\n\nplt.scatter(x=studylotfrontage['LogLotArea'],y=studylotfrontage['LogLotFrontage'],s=1)\nplt.xlabel(\"LogLotArea\")\nplt.ylabel(\"LogLotFrontage\")\n\nplt.scatter(x=np.log(houses[pd.isnull(houses.LotFrontage)]['LotArea']),y=predictedLotFrontage,c='red', marker='^', s=3)\n\nplt.show()","bf17bd53":"def estimateLotFrontageByLotAreaRegression(df):\n#     lotFrontageEstimator = LinearRegression().fit(\n#         X=np.log(df[pd.notnull(df.LotFrontage)][['LotArea']]),\n#         y=np.log(df[pd.notnull(df.LotFrontage)][['LotFrontage']])\n#     )\n    \n    predictedLotFrontage=lotFrontageEstimator.predict(np.log(df[pd.isnull(df.LotFrontage)][['LotArea']]))\n\n    nanindex=df[pd.isnull(df.LotFrontage)].index\n\n    # Precision fill...\n    df.loc[pd.isnull(df.LotFrontage),'LotFrontage']=np.exp(predictedLotFrontage)","df689014":"print(\"{} predicted Lot Frontages for {} NaN Lot Frontages in dataset. Follows their IDs:\".format(predictedLotFrontage.shape[0],houses[pd.isnull(houses.LotFrontage)].shape[0]))\nnanindex=houses[pd.isnull(houses.LotFrontage)].index\nprint(nanindex)","306cfe9d":"def estimateLotFrontageByNeighborhoodMedian(df):\n    df['LotFrontage'] = df.groupby('Neighborhood')['LotFrontage'].transform(lambda x: x.fillna(x.median()))","e8014039":"#estimateLotFrontageByLotAreaRegression(houses)\nestimateLotFrontageByNeighborhoodMedian(houses)","0c7f9b12":"# Inspect\nhouses[['LotFrontage']]","6d44e95e":"def addTotalSF(df):\n    df['TotalSF'] = df['TotalBsmtSF'] + df['n1stFlrSF'] + df['n2ndFlrSF']","15bf77af":"addTotalSF(houses)","32a39ac5":"plt.rcParams['figure.dpi']=200\nanalyzeNormalization(houses,['LotArea','PoolArea','LowQualFinSF','LotFrontage','TotalBsmtSF','n1stFlrSF','GrLivArea','n2ndFlrSF','n3SsnPorch','TotalSF'],bins=200)","8ba5cf3a":"analyzeNormalization(houses.head(trainSet[0]),['SalePrice'],typ='boxcox',bins=200)","f6d2ae83":"#analyzeNormalization(houses,['LotArea','LotFrontage'],typ='boxcox', bins=200)","a010985d":"#analyzeNormalization(houses.head(trainSet[0]),['TotalSF','n1stFlrSF','LotArea','BsmtFinSF2','EnclosedPorch','ScreenPorch','KitchenAbvGr'],typ='boxcox',bins=200)\nanalyzeNormalization(houses,['TotalSF','n1stFlrSF','LotArea','LotFrontage','BsmtFinSF2','EnclosedPorch','ScreenPorch','KitchenAbvGr','GrLivArea','n1stFlrSF'],typ='boxcox',bins=200)","95d3879b":"def boxcoxToFeature(df,feature):\n    df[feature] = special.boxcox1p(df[feature],0.15)\n\ndef logToFeature(df,feature):\n    df[feature] = np.log1p(df[feature])","a987ae51":"logToFeature(houses,'TotalSF')\nlogToFeature(houses,'LotArea')\nlogToFeature(houses,'n1stFlrSF')\nlogToFeature(houses,'GrLivArea')","df7892af":"function = None\n\nfor feature in houses.columns.sort_values():\n    if feature in futureFeatures:\n        # skip features unknown in house pre-sales time, more likely to be Y and not X\n        continue\n        \n    col=feature.strip() # clean strange chars\n    if function:\n        function = '{} + {}'.format(function,col)\n    else:\n        function = col\n\nfunction=f'np.log(SalePrice) ~ {function}'\n\n# model = smf.ols(function, houses).fit()\n# print(model.summary())","5721ea51":"# unfinished...\ndef encodeOneHotFeatures(df):\n    for feature in list(categoricalOrderedFeatures.keys())+categoricalFeatures:\n        df2=pd.DataFrame(index=df.index)\n        df2[\"numerical_\" + feature] = LabelEncoder().fit_transform(df[feature])\n        x=OneHotEncoder().fit_transform(df2[[\"numerical_\" + feature]])\n        print(x)\n        df3=pd.DataFrame(data=x)\n        df.join(df3,how='left',rsuffix='_' + feature)","4321aaa7":"def encodeUnorderedFeatures(df):\n    # Incrementally add encoded unordered categorical features\n    for feature in categoricalFeatures:\n        df[\"numerical_\" + feature] = LabelEncoder().fit_transform(df[feature])","6574447f":"encodeUnorderedFeatures(houses)\n\nprint(houses.shape)\nhouses.head()","6e270e67":"def encodeOrderedFeatures(df):\n    # Incrementally add encoded ordered categorical features\n    for feature in categoricalOrderedFeatures.keys():\n        df[\"numerical_\" + feature]=-1 # initialize target feature\n        i=0\n        for category in df[feature].unique().categories:\n\n            # Get row indexes from this category from source DataFrame\n            indexes = df.index[df[feature] == category]\n\n            # Imputation\n            df.loc[indexes,\"numerical_\" + feature] = i\n\n            # move along\n            i=i+1","c0b3c255":"encodeOrderedFeatures(houses)\n\nprint(houses.shape)\nhouses.head()","31d4c2ac":"def dropNonNumeric(df):\n    numerical=df.select_dtypes(['number']).columns\n    al=df.columns\n    toDrop=[]\n    for c in al:\n        if c not in numerical:\n            toDrop.append(c)\n    \n    df.drop(columns=toDrop, inplace=True)","dc9abe39":"dropNonNumeric(houses)\nhouses.head()","4c5ea3ff":"def encodeAndDrop(df):\n    df=pd.get_dummies(df)\n    return df","817b1d79":"def featureEngineering(dataset):\n    renameColumns(dataset)\n    convertCategorical(dataset)\n    \n    dropOutliers(dataset)\n    \n    zeroToFeature(dataset,'MasVnrArea')\n    zeroToFeature(dataset,'BsmtFinSF1')\n    zeroToFeature(dataset,'BsmtFinSF2')\n    zeroToFeature(dataset,'BsmtHalfBath')\n    zeroToFeature(dataset,'BsmtFullBath')\n    zeroToFeature(dataset,'TotalBsmtSF')\n    zeroToFeature(dataset,'BsmtUnfSF')\n    zeroToFeature(dataset,'GarageCars')\n    zeroToFeature(dataset,'GarageArea')\n\n    copyToFeature(dataset,'YearBuilt','GarageYrBlt')\n#     meanToFeature(dataset,'GarageYrBlt')\n\n    modeToFeatureValue(dataset,'KitchenQual','_UNKNOWN')\n    modeToFeatureValue(dataset,'Utilities','_UNKNOWN')\n    modeToFeatureValue(dataset,'MSZoning','_UNKNOWN')\n    modeToFeatureValue(dataset,'Electrical','_UNKNOWN')\n    modeToFeatureValue(dataset,'Exterior1st','_UNKNOWN')\n    modeToFeatureValue(dataset,'Exterior2nd','_UNKNOWN')\n    modeToFeatureValue(dataset,'SaleType','_UNKNOWN')\n\n#     aggregateMinorCategories(dataset,0.001)\n    \n    addTotalSF(dataset)\n    \n#     boxcoxToFeature(dataset,'TotalSF')\n#     boxcoxToFeature(dataset,'LotArea')\n#     boxcoxToFeature(dataset,'n1stFlrSF')\n#     boxcoxToFeature(dataset,'GrLivArea')\n\n\n    logToFeature(dataset,'TotalSF')\n    logToFeature(dataset,'LotArea')\n    logToFeature(dataset,'n1stFlrSF')\n    logToFeature(dataset,'GrLivArea')\n\n\n    estimateLotFrontageByLotAreaRegression(dataset)\n    #estimateLotFrontageByNeighborhoodMedian(dataset)\n    \n    encodeUnorderedFeatures(dataset)\n    encodeOrderedFeatures(dataset)\n#     encodeOneHotFeatures(dataset)\n\n    dropNonNumeric(dataset)\n\n    #dataset=pd.get_dummies(dataset)","3cae31b1":"del houses\nhouses=pd.read_csv('..\/input\/train.csv', index_col='Id')","a6e61749":"featureEngineering(houses)","3f26c423":"def paramSeeker(estimator,param_grid,df):\n    #kf = StratifiedKFold(n_splits=5, shuffle=True)\n    hparamsearch = GridSearchCV(estimator = estimator, param_grid = param_grid,\n                                cv=5, scoring='neg_mean_squared_error')\n\n    # hparamsearch = BayesSearchCV(estimator = priceEstimator, search_spaces = searchSpace,\n    #                              cv=5, n_iter=200, n_points=5, scoring='neg_mean_squared_error')\n\n    hparamsearch.fit(df.drop(columns='SalePrice'),np.log(df['SalePrice']))\n\n    results = dict(\n        best_score = np.sqrt(-hparamsearch.best_score_),\n        best_params = hparamsearch.best_params_\n    )\n    \n    return results","5087523f":"xgbPriceEstimator=Pipeline([\n    ('scale',RobustScaler()),\n    ('xgb',XGBRegressor(n_jobs=4))\n])\n\nxgbParamGrid={\n    'xgb__n_estimators': [1800],\n    'xgb__max_depth': [3, 4],\n    'xgb__reg_alpha': [0.2, 0.25],\n    'xgb__reg_lambda': [1.1, 1.2],\n\n    'xgb__colsample_bytree': [0.4603],\n    'xgb__gamma': [0.0468],\n    'xgb__learning_rate': [0.05],\n    'xgb__min_child_weight': [1.7817],\n    'xgb__subsample': [0.5213],\n    'xgb__random_state': [42]\n}","92aec666":"enetPriceEstimator=Pipeline([\n    ('scale',RobustScaler()),\n    ('enet',ElasticNet())\n])\n\nenetParamGrid={\n    'enet__alpha': [0.0002, 0.0005, 0.001, 0.1],\n    'enet__l1_ratio': [0.7, 0.9, 1.1],\n    'enet__random_state': [42]\n}","606ddcd2":"xgbResults=paramSeeker(xgbPriceEstimator,xgbParamGrid,houses)\nenetResults=paramSeeker(enetPriceEstimator,enetParamGrid,houses)\n\nprint(xgbResults)\nprint()\nprint(enetResults)","80aa8fbf":"xgbPriceEstimator.set_params(**xgbResults['best_params'])\nenetPriceEstimator.set_params(**enetResults['best_params'])","fefb2744":"xgbPriceEstimator.fit(houses.drop(columns='SalePrice'),np.log(houses['SalePrice']))\nenetPriceEstimator.fit(houses.drop(columns='SalePrice'),np.log(houses['SalePrice']))","fe14a427":"housesTest=pd.read_csv('..\/input\/test.csv', index_col='Id')","dcfbc69c":"featureEngineering(housesTest)","1cb270e0":"results=pd.DataFrame(index=housesTest.index)\nresults['xgbSalePrice']=np.exp(xgbPriceEstimator.predict(housesTest))\nresults['enetSalePrice']=np.exp(enetPriceEstimator.predict(housesTest))","6a7e88f2":"housesTest['SalePrice']=(results['xgbSalePrice']+results['enetSalePrice'])\/2","2c00f9f9":"housesTest[['SalePrice']].to_csv(path_or_buf='submission.csv.gz',compression='gzip')","db407fec":"# RandomizedSearchCV 2018-09-21 16:51:16.586857\n# hparamsearch.best_params_ = {'xgb__max_depth': 4, 'xgb__n_estimators': 196, 'xgb__reg_alpha': 0.9, 'xgb__reg_lambda': 0.1}\n# hparamsearch.best_score_ = 0.8985498074355338\n\n# GridSearchCV 2018-09-22 09:40:45.967409\n# hparamsearch.best_params_ = {'xgb__max_depth': 4, 'xgb__n_estimators': 230, 'xgb__reg_alpha': 0.6, 'xgb__reg_lambda': 0.1}\n# hparamsearch.best_score_ = 0.9002134583272592\n\n# GridSearchCV 2018-09-22 10:00:53.267628\n# hparamsearch.best_params_ = {'xgb__max_depth': 4, 'xgb__n_estimators': 230, 'xgb__reg_alpha': 0.9, 'xgb__reg_lambda': 0.1}\n# hparamsearch.best_score_ = 0.898309723179664\n\n# GridSearchCV 2018-09-22 10:25:11.086841\n# hparamsearch.best_params_ = {'xgb__max_depth': 4, 'xgb__n_estimators': 230, 'xgb__reg_alpha': 0.95, 'xgb__reg_lambda': 0.125}\n# hparamsearch.best_score_ = 0.8985959477587327\n\n# GridSearchCV 2018-09-22 16:27:26.066470\n# hparamsearch.best_params_ = {'xgb__max_depth': 5, 'xgb__n_estimators': 230, 'xgb__reg_alpha': 0.95, 'xgb__reg_lambda': 0.18}\n# hparamsearch.best_score_ = 0.8988287397759668\n\n# GridSearchCV(RobustScaler) 2018-09-22 18:46:30.349091\n# hparamsearch.best_params_ = {'xgb__max_depth': 4, 'xgb__n_estimators': 230, 'xgb__reg_alpha': 0.96, 'xgb__reg_lambda': 0.2}\n# hparamsearch.best_score_ = 0.9003688412258621\n\n# GridSearchCV(RobustScaler+TotalSF) 2018-09-22 19:00:47.473969\n# hparamsearch.best_params_ = {'xgb__max_depth': 5, 'xgb__n_estimators': 230, 'xgb__reg_alpha': 0.98, 'xgb__reg_lambda': 0.2}\n# hparamsearch.best_score_ = 0.9014648252764706\n\n# GridSearchCV(RobustScaler+TotalSF+BoxCox) 2018-09-22 19:21:14.675157\n# hparamsearch.best_params_ = {'xgb__max_depth': 5, 'xgb__n_estimators': 230, 'xgb__reg_alpha': 0.98, 'xgb__reg_lambda': 0.2}\n# hparamsearch.best_score_ = 0.9012942441479855\n\n# GridSearchCV(RobustScaler+TotalSF+BoxCox) 2018-09-22 20:06:12.663002\n# hparamsearch.best_params_ = {'xgb__max_depth': 4, 'xgb__n_estimators': 235, 'xgb__reg_alpha': 0.95, 'xgb__reg_lambda': 0.24}\n# hparamsearch.best_score_ = 9.403486391711356e-05\n\n# GridSearchCV(RobustScaler+BoxCox+LotFrontageNeighborhood) 2018-09-22 20:36:12.203183\n# hparamsearch.best_params_ = {'xgb__max_depth': 4, 'xgb__n_estimators': 230, 'xgb__reg_alpha': 0.96, 'xgb__reg_lambda': 0.22}\n# hparamsearch.best_score_ = 9.368462858961698e-05\n# Kaggle score = 0.14483\n\n# GridSearchCV(RobustScaler+LotFrontageNeighborhood) 2018-09-22 20:45:17.838028\n# hparamsearch.best_params_ = {'xgb__max_depth': 4, 'xgb__n_estimators': 230, 'xgb__reg_alpha': 0.96, 'xgb__reg_lambda': 0.22}\n# hparamsearch.best_score_ = 9.368462858961698e-05\n# Kaggle score = 0.12689\n\n# GridSearchCV(RobustScaler+LotFrontageRegression) 2018-09-22 21:04:27.647685\n# hparamsearch.best_params_ = {'xgb__max_depth': 4, 'xgb__n_estimators': 235, 'xgb__reg_alpha': 0.95, 'xgb__reg_lambda': 0.24}\n# hparamsearch.best_score_ = 9.406959048473793e-05\n# Kaggle score = 0.12832\n\n# GridSearchCV(RobustScaler+LotFrontageRegression+NoLog) 2018-09-22 21:10:51.990014\n# hparamsearch.best_params_ = {'xgb__max_depth': 4, 'xgb__n_estimators': 230, 'xgb__reg_alpha': 0.95, 'xgb__reg_lambda': 0.24}\n# hparamsearch.best_score_ = 0.016329734882100273\n# Kaggle score = 0.14262\n\n# GridSearchCV(RobustScaler+LotFrontageGlobalRegression) 2018-09-22 21:19:57.323594\n# hparamsearch.best_params_ = {'xgb__max_depth': 4, 'xgb__n_estimators': 230, 'xgb__reg_alpha': 0.95, 'xgb__reg_lambda': 0.22}\n# hparamsearch.best_score_ = 9.432671389584923e-05\n# Kaggle score = 0.12680\n\n# GridSearchCV(RobustScaler+LotFrontageGlobalRegression) 2018-09-22 21:29:11.874732\n# hparamsearch.best_params_ = {'xgb__max_depth': 4, 'xgb__n_estimators': 250}\n# hparamsearch.best_score_ = 9.431363999930521e-05\n# Kaggle score = 0.12667\n\n# GridSearchCV(StandardScaler+LotFrontageGlobalRegression) 2018-09-22 21:34:12.214354\n# hparamsearch.best_params_ = {'xgb__max_depth': 4, 'xgb__n_estimators': 250}\n# hparamsearch.best_score_ = 0.00010021662013134085\n# Kaggle score = 0.13739\n\n# GridSearchCV(RobustScaler+LotFrontageGlobalRegression+ElasticNet) 2018-09-22 22:27:19.940156\n# hparamsearch.best_params_ = {'enet__alpha': 0.0005, 'enet__l1_ratio': 2.4}\n# hparamsearch.best_score_ = 0.00013411002114302706\n# Kaggle score = \n\n# GridSearchCV(RobustScaler+BoxCox+TotalSF+Mode) 2018-09-24 00:45:49.598859\n# hparamsearch.best_params_ = {'xgb__max_depth': 4, 'xgb__n_estimators': 240, 'xgb__reg_alpha': 0.98, 'xgb__reg_lambda': 0.2}\n# hparamsearch.best_score_ = 0.12542358749542298\n# Kaggle score = 0.39074\n\n# GridSearchCV(RobustScaler+Mode) 2018-09-24 00:45:49.598859\n# hparamsearch.best_params_ = {'xgb__max_depth': 6, 'xgb__n_estimators': 240, 'xgb__reg_alpha': 1, 'xgb__reg_lambda': 0.2}\n# hparamsearch.best_score_ = 0.12586607415571907\n# Kaggle score = 0.12977\n\n# GridSearchCV(RobustScaler+Mode+TotalSF+log1p+newhparam) 2018-09-24 09:24:15.272289\n# hparamsearch.best_params_ = {'xgb__colsample_bytree': 0.4603, 'xgb__gamma': 0.0468, 'xgb__learning_rate': 0.05, 'xgb__max_depth': 3, 'xgb__min_child_weight': 1.7817, 'xgb__n_estimators': 2200, 'xgb__reg_alpha': 0.464, 'xgb__reg_lambda': 0.8571, 'xgb__subsample': 0.5213}\n# hparamsearch.best_score_ = 0.12391159562822468\n# Kaggle score = 0.12539\n\n# GridSearchCV(RobustScaler+Mode+TotalSF+log1p+newhparam) 2018-09-24 09:35:05.235635\n# hparamsearch.best_params_ = {'xgb__colsample_bytree': 0.4603, 'xgb__gamma': 0.0468, 'xgb__learning_rate': 0.05, 'xgb__max_depth': 4, 'xgb__min_child_weight': 1.7817, 'xgb__n_estimators': 2200, 'xgb__random_state': 42, 'xgb__reg_alpha': 0.464, 'xgb__reg_lambda': 0.8571, 'xgb__subsample': 0.5213}\n# hparamsearch.best_score_ = 0.12263193522001599\n# Kaggle score = 0.12621\n\n# GridSearchCV(RobustScaler+Mode+TotalSF+log1p+newhparam) 2018-09-24 09:48:44.979292\n# hparamsearch.best_params_ = {'xgb__colsample_bytree': 0.4603, 'xgb__gamma': 0.0468, 'xgb__learning_rate': 0.05, 'xgb__max_depth': 3, 'xgb__min_child_weight': 1.7817, 'xgb__n_estimators': 2200, 'xgb__random_state': 42, 'xgb__reg_alpha': 0.464, 'xgb__reg_lambda': 0.8571, 'xgb__subsample': 0.5213}\n# hparamsearch.best_score_ = 0.12329124997588987\n# Kaggle score = 0.12632\n\n# GridSearchCV(RobustScaler+Mode+TotalSF+log1p+newhparam) 2018-09-24 10:03:43.146468\n# hparamsearch.best_params_ = {'xgb__colsample_bytree': 0.4603, 'xgb__gamma': 0.0468, 'xgb__learning_rate': 0.05, 'xgb__max_depth': 4, 'xgb__min_child_weight': 1.7817, 'xgb__n_estimators': 2200, 'xgb__random_state': 42, 'xgb__reg_alpha': 0.3, 'xgb__reg_lambda': 0.9, 'xgb__subsample': 0.5213}\n# hparamsearch.best_score_ = 0.12187840824676832\n# Kaggle score = 0.12618\n\n# GridSearchCV(RobustScaler+Mode+TotalSF+log1p+newhparam) 2018-09-24 10:29:37.716113\n# hparamsearch.best_params_ = {'xgb__colsample_bytree': 0.4603, 'xgb__gamma': 0.0468, 'xgb__learning_rate': 0.05, 'xgb__max_depth': 3, 'xgb__min_child_weight': 1.7817, 'xgb__n_estimators': 2200, 'xgb__random_state': 42, 'xgb__reg_alpha': 0.2, 'xgb__reg_lambda': 0.9, 'xgb__subsample': 0.5213}\n# hparamsearch.best_score_ = 0.12177648367932005\n# Kaggle score = 0.12911\n\n# GridSearchCV(RobustScaler+Mode+TotalSF+boxcox+newhparam) 2018-09-24 12:22:27.333097\n# hparamsearch.best_params_ = {'xgb__colsample_bytree': 0.4603, 'xgb__gamma': 0.0468, 'xgb__learning_rate': 0.05, 'xgb__max_depth': 3, 'xgb__min_child_weight': 1.7817, 'xgb__n_estimators': 2200, 'xgb__random_state': 42, 'xgb__reg_alpha': 0.2, 'xgb__reg_lambda': 0.9, 'xgb__subsample': 0.5213}\n# hparamsearch.best_params_ = 0.12177192528415169\n# Kaggle score = \n\n# GridSearchCV(RobustScaler+Mode+TotalSF+boxcox+newhparam+aggregatedMinorities5%) 2018-09-25 14:19:40.116817\n# hparamsearch.best_params_ = {'xgb__colsample_bytree': 0.4603, 'xgb__gamma': 0.0468, 'xgb__learning_rate': 0.05, 'xgb__max_depth': 4, 'xgb__min_child_weight': 1.7817, 'xgb__n_estimators': 2200, 'xgb__random_state': 42, 'xgb__reg_alpha': 0.2, 'xgb__reg_lambda': 1.2, 'xgb__subsample': 0.5213}\n# hparamsearch.best_params_ = 0.1241218566722285\n# Kaggle score = 0.12842\n\n    # GridSearchCV(RobustScaler+enet+xgb+outliers+bagging) 2018-09-25 14:19:40.116817\n    # hparamsearch.best_params_ = {'xgb__colsample_bytree': 0.4603, 'xgb__gamma': 0.0468, 'xgb__learning_rate': 0.05, 'xgb__max_depth': 4, 'xgb__min_child_weight': 1.7817, 'xgb__n_estimators': 2200, 'xgb__random_state': 42, 'xgb__reg_alpha': 0.2, 'xgb__reg_lambda': 1.2, 'xgb__subsample': 0.5213}\n    # hparamsearch.best_params_ = 0.114443634127553\n    # Kaggle score = 0.11924","b85edd9e":"## Categorical data handling\nMeat in this dataset is not numerical but categorical so I'll apply a special treatment for categories. I'm worried about the semantic of order. Since category labels will be turned into integers by `encodeOrderedFeatures()` later, we want integers to express this improvement in *External Quality*, for example, with smaller integer for *Poor* and bigger integer for *Excellent*. `'ExterQual': ['Po','Fa','TA','Gd','Ex']`.\n\nSo here is data sctructure and category schema for this dataset, as explained by [data_description.txt]. Note that `categoricalOrderedFeatures` already have labels for NaNs that will be imputed latter by `convertCategorical()`.","0ab61be6":"Hummm `LogLotArea`\u00d7`LogLotFrontage` seems nice. Lets zoom:","21eb6571":"`SalePrice`, `TotalSF`, `GrLivArea`, `n1stFlrSF` and `LotArea` are candidates to be normalized. Since BoxCox's \u03bb is very close to 0, we can use plain `log()` and not BoxCox normalization.","05f309c4":"# Make house price predictions with trained estimators","7a3d6b46":"Very few results and only for the UNAVAILABLE houses. It means NaN is no masonry veneer in the house.\n\nStrategy will be to fill `MasVnrArea` with zero.","dae09c61":"## Prepare estimators and find best hyper-parameters","98918ba4":"## Feature NaN, Skew and Gaussian fit Analysis\nHere is a set of helper functions to give some numerical and graphical insight about numerical features.\n\nThey'll show me who has NaN and rank the skewness of each. Higher skew rank, higher the chance we need to filter this feature through BoxCox or log().","405c4001":"Of course, there are simpler methods:","77cb32e8":"## Write CSV for submission","e3c7162d":"### Declare estimators and range of hyper-parameters to be tested: XGBoost","f057b174":"# Dataset analysis and Feature Engineering\nAll data transformations will be encapsulated and recorded in functions.\n\nThis is required for minimum code organization and reuse, because I'll have to repeat all these transformations again for the test set.\n\nWe'll use whole provided data (train + test) for best analisys and feature engineering.","cb400d49":"Bloody outliers... Lets see if they are in the training or testing set...","252609c7":"### NaN analysis for `GarageYrBlt`, `GarageCars` and `GarageArea`","665cd917":"Looks pretty linear to me when comparing `LogLotArea`\u00d7`LogLotFrontage`. Lets do a mini dirty regression...","6d69add0":"# [Ames House Prices Kaggle Competition](https:\/\/www.kaggle.com\/c\/house-prices-advanced-regression-techniques)\n\n*Notebook by [Avi Alkalay](https:\/\/Avi.Alkalay.net)*\n\nMy strategy is:\n- Remove all NaNs by filling with semantically valid values as mean() or even estimated values.\n- Special treatment for ordered categorical features with the hope this will increase my score. Use pandas `CategoricalDtype(ordered=True)` for this.\n- Analyze skewness of numerical features, including SalePrice, and decide for `np.log()` or `scipy.stats.boxcox()` on them.\n- I'll decide what to do with all NaNs\n- I'll write small functions to all dataset operations I need to do during analysis because I'll have to repeat them in sequence right before model training AND test dataset before predictions. So I'll call them all in sequence into my declarative `featureEngineering()` function that clearly prepares a dataset for training or prediction.\n- Then I use XGBoost and ElasticNet in bagging for predictions. Result is Kaggle score of 0.11924\n\n# Index\n\n1. [Dataset analysis and Feature Engineering](#Dataset-analysis-and-Feature-Engineering)\n    * [Categorical data handling](#Categorical-data-handling)\n    * [Outliers removal](#Outliers-removal)\n    * [Feature NaN, Skew and Gaussian fit Analysis](#Feature-NaN,-Skew-and-Gaussian-fit-Analysis)\n    * [NaN analysis for `LotFrontage` with regression](#NaN-analysis-for-LotFrontage)\n    * [Add feature: `TotalSF`](#Add-feature:-TotalSF)\n    * [Gaussian fit analysis of selected features: log or BoxCox ?](#Gaussian-fit-analysis-of-selected-features)\n2. [Preparation of Estimators](#Preparation-of-Estimators)\n    * [Batch dataset preparation with `featureEngineering` function](#Batch-dataset-preparation-with-featureEngineering-function)\n    * [Estimators Pipeline and hyper-parameters search](#Prepare-estimators-and-find-best-hyper-parameters)\n    * [XGBoost](#Declare-estimators-and-range-of-hyper-parameters-to-be-tested:-XGBoost)\n    * [ElasticNet](#Declare-estimators-and-range-of-hyper-parameters-to-be-tested:-ElasticNet)\n    * [Train estimators](#Train-final-estimators)\n3. [Make house price predictions with trained estimators](#Make-house-price-predictions-with-trained-estimators)\n    * [Load test data](#Load-test-data)\n    * [Batch feature engineering of test data](#Batch-feature-engineering-of-test-data)\n    * [Predict house prices from multiple estimators, bagging style](#Predict-house-prices-from-multiple-estimators,-bagging-style)\n    * [Write CSV for submission](#Write-CSV-for-submission)\n4. [Historical collected results](#Historical-collected-results)\n\nThis is my first Kaggle competition. I'm using it to train complex feature engineering and SciKit't [GridSearchCV](http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.GridSearchCV.html#sklearn.model_selection.GridSearchCV), [Pipeline](http:\/\/scikit-learn.org\/stable\/modules\/classes.html#module-sklearn.pipeline) and [PreProcessing](http:\/\/scikit-learn.org\/stable\/modules\/classes.html#module-sklearn.preprocessing) APIs.","2ffb4885":"Same thing but now numerical vision:","799c15fd":"# Preparation of Estimators\nAt this point we finished our dataset analysis and we know everything we need to do to our data in order to train our estimators correctly and then predict.","fa2ea058":"### NaN analysis for `TotalBsmtSF`\nThis is _total square feet of basement area_, so it has to relate with `BsmtQual`","e362bf62":"`paramSeeker` is a generic function that uses GridSearchCV to find best parameters for multi-estimators.","e4b49d7f":"### NaN analysis for  `BsmtFinSF1` and `BsmtFinSF2`\nThis is related to `BsmtFinType2`, so lets see them together:","35480fb9":"`SalePrice` is only available in the train set, so we'll have to filter.","ba1ce2e1":"Let's drop them...","0928cbea":"Lets move all UKNOWN (NaN) to the mode, as we did with `KitchenQual`:","fae9b245":"### NaN analysis for `BsmtUnfSF`\nThis is _unfinished square feet of basement area_, so it has to relate with `BsmtQual`","e196f668":"# Outliers removal\nWe've been told there are some outliers. Lets check...","1113eac6":"### Train final estimators\nUse best-found hyper-parameters and all observations to train usable estimators","27ea2f39":"### NaN analysis for `Electrical`, `Utilities`, `MSZoning`, `Exterior1st`, `Exterior2nd`, `SaleType`\nSimilar approach for these features. Use moda for UNKNOWN. Lets check.","6c78d650":"### Declare estimators and range of hyper-parameters to be tested: ElasticNet","e3230588":"### NaN analysis for `MasVnrArea`","8f8177e7":"NaNs in one single problematic observation.","e1d52f32":"### Find best hyper-parameters for all","e9052bec":"If output above is empty, we succesufly handled all NaN in categorical features.","5155b419":"Only 1 observation on the test set. I'll assume zero ft\u00b2","a3f1ab6f":"### NaN analysis for `LotFrontage`\nThis is the most complicated NaN filling in the dataset. I have a lot of missing data.\n\nMy strategy will be to see if we can estimate missing `LotFrontage` from `LotArea`.\n\nLets prepare some versions of these 2 features and plot them side by side:","5d441a85":"Good... Write a function that writes predictions back over to the NaN values of `houses[LotFrontage]`","b3c5e0fe":"### Aggregated Minorities","379b9925":"## Gaussian fit analysis of selected features","4dff85dc":"## Batch feature engineering of test data","8daec024":"One step dataset preparation with the function that encapsulates all feature engineering:","76449cb9":"## Some more preprocessing functions\nFinal pre-processing to convert everything to numbers:\n- all categories (unordered and ordered) to numbers\n- drop remaining non-numerical features","c2def857":"Its a simple matter of a missing zero","1e78da89":"If above was empty, means `GarageYrBlt=NaN` when `GarageCars==0`. We can't just imput 0 for NaNs because it messes with variance for estimators. So we'll use house `YearBuilt` for `GarageYrBlt`.","da7ce2ff":"Train models...","4a40f2f2":"So this is how it goes: I define a function that does some transformation in the data set and call it right after it passing the data set that will be modified by the function.","f5bc177f":"# Historical collected results","28be35d7":"Only 2 observations on the test set. I'll assume zero basement half bathrooms.","ee9b2d60":"## Add feature: `TotalSF`\nSince area related features are very important to determine house prices, we add one more feature which is the total area of basement, first and second floor areas of each house.","87a23f29":"Only 1 observation with NaN! Imput zero","faabd03d":"## Load test data","e56bef39":"### NaN analysis for `BsmtHalfBath` and `BsmtFullBath`","8b9a828a":"## Batch dataset preparation with `featureEngineering` function \nAll dataset operations we concluded we need to do above were encapsulated in small semi-generic functions that can be applied either to the training and to the testing dataset. And we'll aggregate them all into our `featureEngineering()` function that transforms inplace the dataset that is received as a parameter.\n\nAll dataset manipulations made until now were just to *test* the procedures we've been writing. **Real feature engineering will happen from now on.** For this, we'll delete all datasets in memory and reread them from storage. ","f2b0c854":"### NaN analysis for `KitchenQual`\nNaN became UNKNOWN. Lets check its hits.","4f64f0f6":"## NaN analysis","5136dc14":"## Statistical analysis of SalePrice","d4344ac1":"## Predict house prices from multiple estimators, bagging style"}}