{"cell_type":{"0b9e9f4f":"code","4ff21ac9":"code","8cce215e":"code","7279a920":"code","2b4ae3a5":"code","d2b27b1d":"code","c3d08b03":"code","83d03336":"code","4e9380fd":"code","f3360a76":"code","374de3da":"code","704b7035":"code","b328cf24":"code","7709307f":"code","f874e1bc":"code","0c99f9d4":"code","b33a9f2b":"code","d86dbb9d":"code","fc0ee67b":"code","76a5b377":"code","c8f6270e":"code","c30273d5":"code","005fd725":"code","0eb92134":"code","95d875a9":"code","b359a9a1":"code","ab97fe0f":"code","413447a8":"code","a38d401b":"code","492f03e8":"code","55cf3f05":"code","2aeb00e0":"code","72cb391b":"code","f21b87c9":"code","4de0f8d9":"code","8e291c0a":"code","db34b73d":"code","6492f997":"code","e332c6f4":"code","08d090c1":"code","291dee3a":"markdown","850d19e3":"markdown","ba7fbd0f":"markdown","8c58672d":"markdown","6b89235a":"markdown","2f21dd11":"markdown","0f268de9":"markdown","518011ac":"markdown","a5dfd3ea":"markdown","75386c55":"markdown","41f906d3":"markdown"},"source":{"0b9e9f4f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n# References : https:\/\/www.kaggle.com\/manojvijayan\/feature-engineering-and-xgboost\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport seaborn as sns\nplt.style.use('fivethirtyeight')\n\nimport geopy.distance\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\nimport gc\n\n\n#=====================================================\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import train_test_split\nfrom lightgbm import LGBMRegressor\nfrom xgboost import XGBRegressor\nfrom sklearn import svm\nfrom sklearn.linear_model import SGDRegressor\nfrom sklearn import tree\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\n# Any results you write to the current directory are saved as output.","4ff21ac9":"def load_Data():\n    train = pd.read_csv(\"..\/input\/train.csv\", nrows=20_00_000, low_memory=True)\n    test = pd.read_csv(\"..\/input\/test.csv\", nrows=20_00_000, low_memory=True)\n    return train,test","8cce215e":"train, test = load_Data()","7279a920":"train.head(5)","2b4ae3a5":"train.describe()","d2b27b1d":"train.info()","c3d08b03":"train.isnull().sum()","83d03336":"train = train.fillna(0)","4e9380fd":"train['key2'] = pd.to_datetime(train['key'], errors='coerce')\ntrain['key2'].head()\ntrain.info()","f3360a76":"test['key2'] = pd.to_datetime(test['key'], errors='coerce')","374de3da":"train['fare_amount'].plot(kind='box')","704b7035":"gc.collect()\ntrain.describe()","b328cf24":"print(\"% of fares above 25$ - {:0.2f}\".format(train[train['fare_amount'] > 25]['key'].count()*100\/train['key'].count()))\nprint(\"% of fares above 50$ - {:0.2f}\".format(train[train['fare_amount'] > 50]['key'].count()*100\/train['key'].count()))\nprint(\"% of fares above 100$ - {:0.2f}\".format(train[train['fare_amount'] > 100]['key'].count()*100\/train['key'].count()))\nprint(\"% of fares below 0$ - {:0.2f}\".format(train[train['fare_amount'] < 0]['key'].count()*100\/train['key'].count()))","7709307f":"# fig, axxx = plt.subplot(2,2, figsize=(20,10))\n# train[~(train['fare_amount'] > 25)]['fare_amount'].plot(kind='box', ax = axxx[0][0])\n# train[~(train['fare_amount'] > 50)]['fare_amount'].plot(kind='box', ax = axxx[0][1])\n# train[~(train['fare_amount'] > 100)]['fare_amount'].plot(kind='box', ax = axxx[1][0])\n# train[~(train['fare_amount'] < 0 )]['fare_amount'].plot(kind='box', ax = axxx[1][1])\n\nfig, axarr = plt.subplots(2, 2, figsize=(20, 10))\n\ntrain[~(train['fare_amount'] > 25)]['fare_amount'].plot(kind=\"box\",ax=axarr[0][0])\ntrain[~(train['fare_amount'] > 50)]['fare_amount'].plot(kind=\"box\",ax=axarr[0][1])\ntrain[~(train['fare_amount'] > 100)]['fare_amount'].plot(kind=\"box\",ax=axarr[1][0])\ntrain[~(train['fare_amount'] < 0 )]['fare_amount'].plot(kind='box',ax = axarr[1][1])","f874e1bc":"train['passenger_count'].plot(kind='box')","0c99f9d4":"print(\"Count of invalid pickup latitude\", train[(train['pickup_latitude'] > 90) | (train['pickup_latitude'] < -90) ]['pickup_latitude'].count())\nprint(\"Count of invalid dropoff latitude\", train[(train['dropoff_latitude'] > 90) | (train['dropoff_latitude'] < -90) ]['dropoff_latitude'].count())\nprint(\"Count of invalid pickup longitude\", train[(train['pickup_longitude'] > 180) | (train['pickup_longitude'] < -180) ]['pickup_longitude'].count())\nprint(\"Count of invalid dropoff longitude\", train[(train['dropoff_longitude'] > 180) | (train['dropoff_longitude'] < -180) ]['dropoff_longitude'].count())","b33a9f2b":"print(\"Count of invalid pickup latitude\", test[(test['pickup_latitude'] > 90) | (test['pickup_latitude'] < -90) ]['pickup_latitude'].count())\nprint(\"Count of invalid dropoff latitude\", test[(test['dropoff_latitude'] > 90) | (test['dropoff_latitude'] < -90) ]['dropoff_latitude'].count())\nprint(\"Count of invalid pickup longitude\", test[(test['pickup_longitude'] > 180) | (test['pickup_longitude'] < -180) ]['pickup_longitude'].count())\nprint(\"Count of invalid dropoff longitude\", test[(test['dropoff_longitude'] > 180) | (test['dropoff_longitude'] < -180) ]['dropoff_longitude'].count())","d86dbb9d":"train = train[~((train['pickup_latitude'] > 90) | (train['pickup_latitude'] < -90))]\ntrain = train[~((train['dropoff_latitude'] > 90) | (train['dropoff_latitude'] < -90))]\ntrain = train[~((train['pickup_longitude'] > 180) | (train['pickup_longitude'] < -180))]\ntrain = train[~((train['dropoff_longitude'] > 180) | (train['dropoff_longitude'] < -180))]","fc0ee67b":"train['distance']=train[['pickup_longitude', 'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude']].apply(lambda x: \n                                                                                                geopy.distance.VincentyDistance((x['pickup_latitude'],x['pickup_longitude']),\n                                                                                                                               (x['dropoff_latitude'],x['dropoff_longitude'])).km,axis=1)","76a5b377":"train.drop(['pickup_longitude', 'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude'],axis=1, inplace=True)","c8f6270e":"test['distance']=test[['pickup_longitude', 'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude']].apply(lambda x: \n                                                                                                geopy.distance.VincentyDistance((x['pickup_latitude'],x['pickup_longitude']),\n                                                                                                                               (x['dropoff_latitude'],x['dropoff_longitude'])).km,axis=1)","c30273d5":"test.drop(['pickup_longitude', 'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude'],axis=1, inplace=True)","005fd725":"print(\"% of trips above 25 KM - {:0.2f}\".format(train[train['distance'] > 25]['key'].count()*100\/train.count()['key']))","0eb92134":"train['year'] = train['key2'].apply(lambda x : x.year)\ntrain['month'] = train['key2'].apply(lambda x : x.month)\ntrain['day'] = train['key2'].apply(lambda x : x.day)\ntrain['day of week'] = train['key2'].apply(lambda x : x.weekday())\ntrain['hour'] = train['key2'].apply(lambda x : x.hour)\ntrain[\"week\"] = train['key2'].apply(lambda x: x.week)\ntrain['day_of_year'] = train['key2'].apply(lambda x:x.dayofyear)\ntrain['week_of_year'] = train['key2'].apply(lambda x:x.weekofyear)\ntrain['quarter'] = train['key2'].apply(lambda x:x.quarter)","95d875a9":"train.columns","b359a9a1":"test['year'] = test['key2'].apply(lambda x : x.year)\ntest['month'] = test['key2'].apply(lambda x : x.month)\ntest['day'] = test['key2'].apply(lambda x : x.day)\ntest['day of week'] = test['key2'].apply(lambda x : x.weekday())\ntest['hour'] = test['key2'].apply(lambda x : x.hour)\ntest[\"week\"] = test['key2'].apply(lambda x: x.week)\ntest['day_of_year'] = test['key2'].apply(lambda x:x.dayofyear)\ntest['week_of_year'] = test['key2'].apply(lambda x:x.weekofyear)\ntest['quarter'] = test['key2'].apply(lambda x:x.quarter)","ab97fe0f":"column_list = ['passenger_count', 'distance', 'year', 'month', 'day', 'day of week', 'hour','week','day_of_year', 'week_of_year', 'quarter']\ny_train_ = train['fare_amount']\nX_train_ = train.drop(['fare_amount'], axis=1)","413447a8":"X_train_ = train[column_list] \nX_test = test[column_list]","a38d401b":"X_train_.shape,y_train_.shape,X_test.shape","492f03e8":"from sklearn.model_selection import train_test_split\nX_train, X_val, y_train, y_val = train_test_split(X_train_, y_train_, test_size=0.1)\n\nX_train.shape,X_val.shape,y_train.shape,y_val.shape","55cf3f05":"import xgboost as xgb\nfrom bayes_opt import BayesianOptimization","2aeb00e0":"dtrain = xgb.DMatrix(X_train, label=y_train)\ndtest = xgb.DMatrix(X_val)","72cb391b":"# def xgb_evaluate(max_depth, gamma, min_child_weight, max_delta_step, subsample,colsample_bytree, eta):\n#     params = {'eval_metric': 'rmse',\n#               'max_depth': int(max_depth),\n#               'subsample': 0.8,\n#               'eta': eta,\n#               'gamma': gamma,\n#               'colsample_bytree': colsample_bytree,\n#               'min_child_weight': min_child_weight,\n#               'max_delta_step':max_delta_step\n#              }\n#     # Used around 1000 boosting rounds in the full model\n#     cv_result = xgb.cv(params, dtrain, num_boost_round=100, nfold=3)    \n    \n#     # Bayesian optimization only knows how to maximize, not minimize, so return the negative RMSE\n#     return -1.0 * cv_result['test-rmse-mean'].iloc[-1]","f21b87c9":"# xgb_bo = BayesianOptimization(xgb_evaluate, {'max_depth': (1, 15), \n#                                              'max_depth': (2, 12),\n#                                              'gamma': (0.001, 10.0),\n#                                              'min_child_weight': (0, 20),\n#                                              'max_delta_step': (0, 10),\n#                                              'subsample': (0.4, 1.0),\n#                                              'colsample_bytree' :(0.4, 1.0),\n#                                              'eta': (0.01,0.1)\n#                                             })\n# # Use the expected improvement acquisition function to handle negative numbers\n# # Optimally needs quite a few more initiation points and number of iterations\n# xgb_bo.maximize(init_points=3, n_iter=5, acq='ei')","4de0f8d9":"# params = xgb_bo.res['max']['max_params']\n# params['max_depth'] = int(params['max_depth'])","8e291c0a":"params = {'colsample_bytree': 1.0,\n 'eta': 0.1,\n 'gamma': 0.001,\n 'max_delta_step': 10.0,\n 'max_depth': 12,\n 'min_child_weight': 20.0,\n 'subsample': 1.0}","db34b73d":"from sklearn.metrics import mean_squared_error\n# Train a new model with the best parameters from the search\nmodel2 = xgb.train(params, dtrain, num_boost_round=250)\n\n# Predict on testing and training set\ny_pred = model2.predict(dtest)\ny_train_pred = model2.predict(dtrain)\n\n# Report testing and training RMSE\nprint(np.sqrt(mean_squared_error(y_val, y_pred)))\nprint(np.sqrt(mean_squared_error(y_train, y_train_pred)))","6492f997":"fscores = pd.DataFrame({'X': list(model2.get_fscore().keys()), 'Y': list(model2.get_fscore().values())})\nfscores.sort_values(by='Y').plot.bar(x='X')","e332c6f4":"dtest = xgb.DMatrix(X_test)\ny_pred_test = model2.predict(dtest)","08d090c1":"sub = pd.read_csv(\"..\/input\/sample_submission.csv\")\nsub[\"Target\"] = y_pred_test\nsub.to_csv('submission.csv', index=False)","291dee3a":"# 5. Inspect Responce Variable \"Fare Amount\"","850d19e3":"# 2.Check the top rows","ba7fbd0f":"### We can see that fare_amount\n\n- **Fare_Amount > 25 Dollar = 7.10 Percent**\n- **Fare_Amount > 50 Dollar = 1.28 Percent**\n- **Fare_Amount > 100 Dollar = 0.004 Percent**\n- **Fare_Amount < 0 Dollar = 38 Count We get**","8c58672d":"### Check Prices is above 25 dollar and below 0 dollar","6b89235a":"# 7.Generate Time Related feature","2f21dd11":"# 8.Assign Value to Responce Variable","0f268de9":"## NYC TaXi Fare Prediction","518011ac":"# 1. Read Dataset","a5dfd3ea":"# 6.Calculate the new column Distance by Lambda Function","75386c55":"# 4.Convert key to Datetime format","41f906d3":"# 3.Check Missing Value and fill it by 0."}}