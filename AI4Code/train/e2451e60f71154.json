{"cell_type":{"fff1cd0d":"code","a74430d3":"code","5521168d":"code","9890dc96":"code","abd23a45":"code","bed3bf29":"code","3fb016fd":"code","15e596f0":"code","cbabbd0b":"code","4808e1b9":"code","b6a11a33":"code","ebfb3c75":"code","bac9a63d":"code","e94788ce":"code","b7f1f832":"code","8a1a55da":"code","bf56866e":"code","03f3a498":"code","65928d97":"code","e7a371ff":"code","8901ac93":"code","f6eda44c":"code","f5177a9b":"code","34febbe8":"code","3c34ba39":"code","d282cc51":"code","97527c71":"code","e22cc7b1":"code","11900be1":"code","1f43ce03":"code","613be49e":"code","88a181ef":"code","264058cc":"code","d554bac2":"markdown","892e0960":"markdown","1adeec73":"markdown","fedc5cf7":"markdown","91aafc13":"markdown","07e6c89a":"markdown","cdd79258":"markdown","583e2eeb":"markdown","a5dd0fec":"markdown","14f42287":"markdown","9c59361d":"markdown","42b33b73":"markdown","b694b159":"markdown","34aa7c29":"markdown","9bd5f28d":"markdown","c06e4fab":"markdown","85cac4fa":"markdown","202e87e3":"markdown","1f4a18bc":"markdown","49338cf1":"markdown","961458fb":"markdown","5800e729":"markdown","835cfef9":"markdown","ab2b0b3a":"markdown","e9eca116":"markdown","c84f7f76":"markdown","2b6e2add":"markdown","dd72fd38":"markdown","97bdab4f":"markdown","5e63dd19":"markdown","dc030d4c":"markdown","e3139727":"markdown","aab21654":"markdown","1a59052b":"markdown","3748d7d8":"markdown","9e88b2da":"markdown","c5e74185":"markdown","2b294f7b":"markdown","1d23512f":"markdown","b44255b0":"markdown","7efc0280":"markdown","6d05728e":"markdown","7d337a3e":"markdown","f968b1fb":"markdown","9cc6be66":"markdown","88bcffce":"markdown","9563d28e":"markdown","52823295":"markdown","3de5fb13":"markdown","90ccb9a9":"markdown"},"source":{"fff1cd0d":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt # this is used for the plot the graph \nimport seaborn as sns # used for plot interactive graph.\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import average_precision_score\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import precision_recall_curve\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import auc\nfrom sklearn.svm import SVC\n%matplotlib inline","a74430d3":"df=pd.read_csv('..\/input\/heart.csv')\ndf.head(5)","5521168d":"df.describe()","9890dc96":"df.shape","abd23a45":"import pandas_profiling\nx=pandas_profiling.ProfileReport(df)\nx","bed3bf29":"df.isnull().sum()","3fb016fd":"%matplotlib inline\nplt.figure(figsize=(10,10))\nsns.heatmap(df.corr(),annot=True,fmt='.1f')\nplt.show()","15e596f0":"df_corr=df.corr()['target'][:-1]\nfeature_list=df_corr[abs(df_corr)>0.1].sort_values(ascending=False)\nfeature_list","cbabbd0b":"sns.distplot(df['target'],rug=True)\nplt.show()","4808e1b9":"import plotly\nimport plotly.graph_objs as go\nfrom plotly.offline import init_notebook_mode, iplot\ninit_notebook_mode(connected=True)\ncol = \"target\"\ngrouped = df[col].value_counts().reset_index()\ngrouped = grouped.rename(columns = {col : \"count\", \"index\" : col})\n\n## plot\ntrace = go.Pie(labels=grouped[col], values=grouped['count'], pull=[0.05, 0])\nlayout = {'title': 'Target(0 = No, 1 = Yes)'}\nfig = go.Figure(data = [trace], layout = layout)\niplot(fig)","b6a11a33":"sns.distplot(df['sex'],rug=True)\nplt.show()","ebfb3c75":"from plotly.offline import init_notebook_mode, iplot\ninit_notebook_mode(connected=True)\ncol = \"sex\"\ngrouped = df[col].value_counts().reset_index()\ngrouped = grouped.rename(columns = {col : \"count\", \"index\" : col})\n\n## plot\ntrace = go.Pie(labels=grouped[col], values=grouped['count'], pull=[0.05, 0])\nlayout = {'title': 'Male(1), Female(0)'}\nfig = go.Figure(data = [trace], layout = layout)\niplot(fig)","bac9a63d":"col='sex'\nd1=df[df['target']==0]\nd2=df[df['target']==1]\nv1=d1[col].value_counts().reset_index()\nv1=v1.rename(columns={col:'count','index':col})\nv1['percent']=v1['count'].apply(lambda x : 100*x\/sum(v1['count']))\nv1=v1.sort_values(col)\nv2=d2[col].value_counts().reset_index()\nv2=v2.rename(columns={col:'count','index':col})\nv2['percent']=v2['count'].apply(lambda x : 100*x\/sum(v2['count']))\nv2=v2.sort_values(col)\ntrace1 = go.Bar(x=v1[col], y=v1[\"count\"], name=0, marker=dict(color=\"#a678de\"))\ntrace2 = go.Bar(x=v2[col], y=v2[\"count\"], name=1, marker=dict(color=\"#6ad49b\"))\ndata = [trace1, trace2]\nlayout={'title':\"target over the sex(male or female)\",'xaxis':{'title':\"target\"}}\n#layout = go.Layout(title=\"Content added over the years\", legend=dict(x=0.1, y=1.1, orientation=\"h\"))\nfig = go.Figure(data, layout=layout)\niplot(fig)\n","e94788ce":"sns.distplot(df['cp'],rug=True)\nplt.show()","b7f1f832":"col='cp'\nd1=df[df['target']==0]\nd2=df[df['target']==1]\nv1=d1[col].value_counts().reset_index()\nv1=v1.rename(columns={col:'count','index':col})\nv1['percent']=v1['count'].apply(lambda x : 100*x\/sum(v1['count']))\nv1=v1.sort_values(col)\nv2=d2[col].value_counts().reset_index()\nv2=v2.rename(columns={col:'count','index':col})\nv2['percent']=v2['count'].apply(lambda x : 100*x\/sum(v2['count']))\nv2=v2.sort_values(col)\ntrace1 = go.Bar(x=v1[col], y=v1[\"count\"], name=0)\ntrace2 = go.Bar(x=v2[col], y=v2[\"count\"], name=1)\ndata = [trace1, trace2]\nlayout={'title':\"target over the chaist pain\",'xaxis':{'title':\"Chaist pain type\"}}\n#layout = go.Layout(title=\"Content added over the years\", legend=dict(x=0.1, y=1.1, orientation=\"h\"))\nfig = go.Figure(data, layout=layout)\niplot(fig)\n","8a1a55da":"sns.distplot(df['thalach'],rug=True)\nplt.show()","bf56866e":"col='thalach'\nd1=df[df['target']==0]\nd2=df[df['target']==1]\nv1=d1[col].value_counts().reset_index()\nv1=v1.rename(columns={col:'count','index':col})\nv1['percent']=v1['count'].apply(lambda x : 100*x\/sum(v1['count']))\nv1=v1.sort_values(col)\nv2=d2[col].value_counts().reset_index()\nv2=v2.rename(columns={col:'count','index':col})\nv2['percent']=v2['count'].apply(lambda x : 100*x\/sum(v2['count']))\nv2=v2.sort_values(col)\ntrace1 = go.Scatter(x=v1[col], y=v1[\"count\"], name=0, marker=dict(color=\"#a678de\"))\ntrace2 = go.Scatter(x=v2[col], y=v2[\"count\"], name=1, marker=dict(color=\"#6ad49b\"))\ndata = [trace1, trace2]\nlayout={'title':\"target over the person's maximum heart rate achieved\"}\nfig = go.Figure(data, layout=layout)\niplot(fig)","03f3a498":"sns.distplot(df['fbs'],rug=True)\nplt.show()","65928d97":"col='fbs'\nd1=df[df['target']==0]\nd2=df[df['target']==1]\nv1=d1[col].value_counts().reset_index()\nv1=v1.rename(columns={col:'count','index':col})\nv1['percent']=v1['count'].apply(lambda x : 100*x\/sum(v1['count']))\nv1=v1.sort_values(col)\nv2=d2[col].value_counts().reset_index()\nv2=v2.rename(columns={col:'count','index':col})\nv2['percent']=v2['count'].apply(lambda x : 100*x\/sum(v2['count']))\nv2=v2.sort_values(col)\ntrace1 = go.Bar(x=v1[col], y=v1[\"count\"], name=0, marker=dict(color=\"#a678de\"))\ntrace2 = go.Bar(x=v2[col], y=v2[\"count\"], name=1, marker=dict(color=\"#6ad49b\"))\ndata = [trace1, trace2]\nlayout={'title':\"target over the person's fasting blood sugar \",'xaxis':{'title':\"fbs(> 120 mg\/dl, 1 = true; 0 = false)\"}}\n#layout = go.Layout(title=\"Content added over the years\", legend=dict(x=0.1, y=1.1, orientation=\"h\"))\nfig = go.Figure(data, layout=layout)\niplot(fig)","e7a371ff":"sns.distplot(df['age'],rug=True)\nplt.show()","8901ac93":"col='age'\nd1=df[df['target']==0]\nd2=df[df['target']==1]\nv1=d1[col].value_counts().reset_index()\nv1=v1.rename(columns={col:'count','index':col})\nv1['percent']=v1['count'].apply(lambda x : 100*x\/sum(v1['count']))\nv1=v1.sort_values(col)\nv2=d2[col].value_counts().reset_index()\nv2=v2.rename(columns={col:'count','index':col})\nv2['percent']=v2['count'].apply(lambda x : 100*x\/sum(v2['count']))\nv2=v2.sort_values(col)\ntrace1 = go.Scatter(x=v1[col], y=v1[\"count\"], name=0, marker=dict(color=\"#a678de\"))\ntrace2 = go.Scatter(x=v2[col], y=v2[\"count\"], name=1, marker=dict(color=\"#6ad49b\"))\ndata = [trace1, trace2]\nlayout={'title':\"target over the age\"}\nfig = go.Figure(data, layout=layout)\niplot(fig)","f6eda44c":"sns.lmplot(x=\"trestbps\", y=\"chol\",data=df,hue=\"cp\")\nplt.show()","f5177a9b":"chest_pain=pd.get_dummies(df['cp'],prefix='cp',drop_first=True)\ndf=pd.concat([df,chest_pain],axis=1)\ndf.drop(['cp'],axis=1,inplace=True)\nsp=pd.get_dummies(df['slope'],prefix='slope')\nth=pd.get_dummies(df['thal'],prefix='thal')\nframes=[df,sp,th]\ndf=pd.concat(frames,axis=1)\ndf.drop(['slope','thal'],axis=1,inplace=True)","34febbe8":"df.head(5)","3c34ba39":"X = df.drop(['target'], axis = 1)\ny = df.target.values","d282cc51":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=0)","97527c71":"from sklearn.preprocessing import StandardScaler\nsc_X=StandardScaler()\nX_train=sc_X.fit_transform(X_train)\nX_test=sc_X.transform(X_test)","e22cc7b1":"#LogisticRegression\nlr_c=LogisticRegression(random_state=0)\nlr_c.fit(X_train,y_train)\nlr_pred=lr_c.predict(X_test)\nlr_cm=confusion_matrix(y_test,lr_pred)\nlr_ac=accuracy_score(y_test, lr_pred)\n\n#SVM classifier\nsvc_c=SVC(kernel='linear',random_state=0)\nsvc_c.fit(X_train,y_train)\nsvc_pred=svc_c.predict(X_test)\nsv_cm=confusion_matrix(y_test,svc_pred)\nsv_ac=accuracy_score(y_test, svc_pred)\n\n#Bayes\ngaussian=GaussianNB()\ngaussian.fit(X_train,y_train)\nbayes_pred=gaussian.predict(X_test)\nbayes_cm=confusion_matrix(y_test,bayes_pred)\nbayes_ac=accuracy_score(bayes_pred,y_test)\n\n#SVM regressor\nsvc_r=SVC(kernel='rbf')\nsvc_r.fit(X_train,y_train)\nsvr_pred=svc_r.predict(X_test)\nsvr_cm=confusion_matrix(y_test,svr_pred)\nsvr_ac=accuracy_score(y_test, svr_pred)\n\n#RandomForest\nrdf_c=RandomForestClassifier(n_estimators=10,criterion='entropy',random_state=0)\nrdf_c.fit(X_train,y_train)\nrdf_pred=rdf_c.predict(X_test)\nrdf_cm=confusion_matrix(y_test,rdf_pred)\nrdf_ac=accuracy_score(rdf_pred,y_test)\n\n# DecisionTree Classifier\ndtree_c=DecisionTreeClassifier(criterion='entropy',random_state=0)\ndtree_c.fit(X_train,y_train)\ndtree_pred=dtree_c.predict(X_test)\ndtree_cm=confusion_matrix(y_test,dtree_pred)\ndtree_ac=accuracy_score(dtree_pred,y_test)\n\n#KNN\nknn=KNeighborsClassifier(n_neighbors=2)\nknn.fit(X_train,y_train)\nknn_pred=knn.predict(X_test)\nknn_cm=confusion_matrix(y_test,knn_pred)\nknn_ac=accuracy_score(knn_pred,y_test)","11900be1":"plt.figure(figsize=(20,10))\nplt.subplot(2,4,1)\nplt.title(\"LogisticRegression_cm\")\nsns.heatmap(lr_cm,annot=True,cmap=\"Blues\",fmt=\"d\",cbar=False)\nplt.subplot(2,4,2)\nplt.title(\"SVM_regressor_cm\")\nsns.heatmap(sv_cm,annot=True,cmap=\"Blues\",fmt=\"d\",cbar=False)\nplt.subplot(2,4,3)\nplt.title(\"bayes_cm\")\nsns.heatmap(bayes_cm,annot=True,cmap=\"Oranges\",fmt=\"d\",cbar=False)\nplt.subplot(2,4,4)\nplt.title(\"RandomForest\")\nsns.heatmap(rdf_cm,annot=True,cmap=\"Blues\",fmt=\"d\",cbar=False)\nplt.subplot(2,4,5)\nplt.title(\"SVM_classifier_cm\")\nsns.heatmap(svr_cm,annot=True,cmap=\"Reds\",fmt=\"d\",cbar=False)\nplt.subplot(2,4,6)\nplt.title(\"DecisionTree_cm\")\nsns.heatmap(dtree_cm,annot=True,cmap=\"Blues\",fmt=\"d\",cbar=False)\nplt.subplot(2,4,7)\nplt.title(\"kNN_cm\")\nsns.heatmap(knn_cm,annot=True,cmap=\"Blues\",fmt=\"d\",cbar=False)\nplt.show()","1f43ce03":"print('LogisticRegression_accuracy:\\t',lr_ac)\nprint('SVM_regressor_accuracy:\\t\\t',svr_ac)\nprint('RandomForest_accuracy:\\t\\t',rdf_ac)\nprint('DecisionTree_accuracy:\\t\\t',dtree_ac)\nprint('KNN_accuracy:\\t\\t\\t',knn_ac)\nprint('SVM_classifier_accuracy:\\t',sv_ac)\nprint('Bayes_accuracy:\\t\\t\\t',bayes_ac)","613be49e":"def plotting(true,pred):\n    fig,ax=plt.subplots(1,2,figsize=(10,5))\n    precision,recall,threshold = precision_recall_curve(true,pred[:,1])\n    ax[0].plot(recall,precision,'g--')\n    ax[0].set_xlabel('Recall')\n    ax[0].set_ylabel('Precision')\n    ax[0].set_title(\"Average Precision Score : {}\".format(average_precision_score(true,pred[:,1])))\n    fpr,tpr,threshold = roc_curve(true,pred[:,1])\n    ax[1].plot(fpr,tpr)\n    ax[1].set_title(\"AUC Score is: {}\".format(auc(fpr,tpr)))\n    ax[1].plot([0,1],[0,1],'k--')\n    ax[1].set_xlabel('False Positive Rate')\n    ax[1].set_ylabel('True Positive Rate')","88a181ef":"plt.figure()\nplotting(y_test,gaussian.predict_proba(X_test))","264058cc":"model_accuracy = pd.Series(data=[lr_ac,sv_ac,bayes_ac,svr_ac,rdf_ac,dtree_ac,knn_ac], \n                index=['LogisticRegression','SVM_classifier','Bayes','SVM_regressor',\n                                      'RandomForest','DecisionTree_Classifier','KNN'])\nfig= plt.figure(figsize=(10,7))\nmodel_accuracy.sort_values().plot.barh()\nplt.title('Model Accracy')","d554bac2":"Here I use Pandas profiling, it reduce lots of our work.\nPandas profiling provides analysis like type, unique values, missing values, quantile statistics, mean, mode, median, standard deviation, sum, skewness, frequent values, histograms, correlation between variables, count, heatmap visualization, etc.","892e0960":"The 51.2% person having the fasting blood sugar rate over 120 mg\/dl and 55% person having the fasting blood sugar rate below 120 mg\/dl is affected by heart disease.","1adeec73":"# With a Healthy Heart the Beat Goes On  \n","fedc5cf7":"* A receiver operating characteristic curve, or ROC curve, is a graphical plot that illustrates the diagnostic ability of a binary classifier system as its discrimination threshold is varied.\n* A precision-recall curve is a plot of the precision (y-axis) and the recall (x-axis) for different thresholds, much like the ROC curve. A no-skill classifier is one that cannot discriminate between the classes and would predict a random class or a constant class in all cases\n","91aafc13":"* Cardiovascular disease is the leading cause of death in women in Australia with 90% of women having one risk factor. \n* The causes including high blood pressure, high cholesterol, smoking, diabetes, weight and family history are discussed.\n* A woman's risk also goes up if she's had a miscarriage or had her ovaries or uterus removed.\n*  Women's hearts are affected by stress and depression more than men's. Depression makes it difficult to maintain a healthy lifestyle.","07e6c89a":"#### Spliting the 80% of the dataset into train_data and 20% of the dataset into test_data","cdd79258":"### THALACH","583e2eeb":"This dataset gives a number of variables along with a target condition of having or not having heart disease. It contains 76 attributes, but all published experiments refer to using a subset of 14 of them. In particular, the Cleveland database is the only one that has been used by ML researchers to this date. The \"goal\" field refers to the presence of heart disease in the patient.","a5dd0fec":"We have used a number of machine learning algorithms,these are \n* 1.logistic regression\n> The meaning of the term regression is very simple: any process that attempts to find relationships between variables is called regression. Logistic regression is regression because it finds relationships between variables. It is logistic because it uses logistic function as a link function.\n* 2.support vector machine (SVM)\n> A support vector machine (SVM) is a supervised machine learning model that uses classification algorithms for two-group classification problems. So you're working on a text classification problem.\n* 3.k nearest neighborhood (kNN)\n> K nearest neighbors is a simple algorithm that stores all available cases and classifies new cases based on a similarity measure (e.g., distance functions). KNN has been used in statistical estimation and pattern recognition\n* 4.GradientBoostingClassifier\n> Gradient boosting is a machine learning technique for regression and classification problems, which produces a prediction model in the form of an ensemble of weak prediction models\n* 5.RandomForestClassifier\n> Random forests or random decision forests are an ensemble learning method for classification, regression and other tasks that operates by constructing a multitude of decision trees at training time and outputting the class that is the mode of the classes or mean prediction of the individual trees\n* 6.GaussianNB\n> Other functions can be used to estimate the distribution of the data, but the Gaussian (or Normal distribution) is the easiest to work with because you only need to estimate the mean and the standard deviation from your training data.\n* 7.DecisionTreeClassifier algorithms. \n> A decision tree classifier is a tree in which internal nodes are labeled by features. ... The classifier categorizes an object xi by recursively testing for the weights that the features labeling the internal nodes have in vector xi, until a leaf node is reached. The label of this node is then assigned to xi","14f42287":"### FASTING BLOOD SUGAR","9c59361d":"#### Training & Testing of Model","42b33b73":"#### Loading the Data","b694b159":"from theabove corelation plot we see that cp(chest pain),thalch and slope are highly corelated with the target.","34aa7c29":"## Data Visulization","9bd5f28d":"### Feature selection\n\n#### **>>** Xstant=X-mean(X)\/st.dev(X)\n\n#### **>>** Xnorm=X-min(X)\/max(X)-min(X)","c06e4fab":"### Women are 4 times more likely to die from heart disease than breast cancer","85cac4fa":"Now,I will check null on all data and If data has null, I will sum of null data's. In this way, how many missing data is in the data.\n","202e87e3":"### SEX","1f4a18bc":"### CHAIST PAIN","49338cf1":"The person having the heart rate over 140 is more likely to have heart disease, therefor we conclude that we have to check our heart rate monthly if its over the thalach 140 then we have to concult the doctor and much concious to the health.","961458fb":"<img src=\"https:\/\/www.narayanahealth.org\/sites\/default\/files\/nh-campaigns\/heart-rhythm\/images\/banner2.jpg\" style=\"width: 950px;\"\/>","5800e729":"from the above plot we see that 45.5% people does not have disease and 54.5% have heart disease.","835cfef9":"It's a clean, easy to understand set of data. However, the meaning of some of the column headers are not obvious. Here's what they mean,\n\n1.age: The person's age in years\n\n2.sex: The person's sex (1 = male, 0 = female)\n\n3.cp: The chest pain experienced (Value 1: typical angina, Value 2: atypical angina, Value 3: non-anginal pain, Value 4: asymptomatic)\n\n4.trestbps: The person's resting blood pressure (mm Hg on admission to the hospital)\n\n5.chol: The person's cholesterol measurement in mg\/dl\n\n6.fbs: The person's fasting blood sugar (> 120 mg\/dl, 1 = true; 0 = false)\n\n7.restecg: Resting electrocardiographic measurement (0 = normal, 1 = having ST-T wave abnormality, 2 = showing probable or definite left ventricular hypertrophy by Estes' criteria)\n\n8.thalach: The person's maximum heart rate achieved\n\n9.exang: Exercise induced angina (1 = yes; 0 = no)\n\n10.oldpeak: ST depression induced by exercise relative to rest ('ST' relates to positions on the ECG plot. See more here)\n\n11.slope: the slope of the peak exercise ST segment (Value 1: upsloping, Value 2: flat, Value 3: downsloping)\n\n12.ca: The number of major vessels (0-3)\n\n13.thal: A blood disorder called thalassemia (3 = normal; 6 = fixed defect; 7 = reversable defect)\n\n14.target: Heart disease (0 = no, 1 = yes)\n","ab2b0b3a":"## Cause of Heart Disease","e9eca116":"In the above plot we see that 27.2% persons having chaist pain type 0, 82% having chaist pain type 1, 79.3% having chaist pain type 2 and 69.5% having chaist pain type 3. These person have heart disease, from this data we observe that those who have chaist pain type 1 and chaist pain type 2 is more likely to affected by heart disease.","c84f7f76":"#### ROC and Precision Recall Curve for the model which gives the heighest accuracy","2b6e2add":"### TARGET","dd72fd38":"there are no missing data in this dataset","97bdab4f":"Looking at information of heart disease risk factors led me to the following: high cholesterol, high blood pressure, diabetes, weight, family history and smoking 3. According to another source 4, the major factors that can't be changed are: increasing age, male gender and heredity. Note that thalassemia, one of the variables in this dataset, is heredity. Major factors that can be modified are: Smoking, high cholesterol, high blood pressure, physical inactivity, and being overweight and having diabetes. Other factors include stress, alcohol and poor diet\/nutrition.","5e63dd19":"#### AGE","dc030d4c":"1.  Excess weight, especially around the stomach area, increases a woman's risk of developing cardiovascular disease and lack of physical activity makes it worse.\n2. Diabetes causes damage to blood vessels so diabetes is a major factor in developing cardiovascular disease.\n3. Unhealthy foods, lack of exercise, lead to heart disease. So can high blood pressure, infections, and birth defects.\n4. Smoking is one of the biggest causes of cardiovascular disease.\n5. Just a few cigarettes a day can damage the blood vessels and reduce the amount of oxygen available in our blood.\n*  But other things might surprise you.","e3139727":"### Summary\nWe started with the data exploration where we got a feeling for the dataset, checked about missing data and learned which features are important. During this process we used Plotly, seaborn and matplotlib to do the visualizations. During the data preprocessing part, we converted features into numeric ones, grouped values into categories and created a few new features. Afterwards we started training machine learning models, and applied cross validation on it. Of course there is still room for improvement, like doing a more extensive feature engineering, by comparing and plotting the features against each other and identifying and removing the noisy features. You could also do some ensemble learning.Lastly, we looked at it\u2019s confusion matrix and computed the models precision.","aab21654":"### Accuracy of the models","1a59052b":"## Preventions","3748d7d8":"## Creating Different Machine Learning Model ","9e88b2da":"* Quit smoking.\n* Control other health conditions, such as high blood pressure, high cholesterol and diabetes.\n* Exercise at least 30 minutes a day on most days of the week.\n* Eat a diet that's low in salt and saturated fat.\n* Maintain a healthy weight.\n* Reduce and manage stress.\n* Practice good hygiene.","c5e74185":"In the heart disease UCI dataset only 31.7% are female and rest are male. we have find that either male or female which have likely to heart disease.","2b294f7b":"From this plot we identify that person having age between 40 to 65 year is more likely to affected by heart disease, therefor person having the age in this range have to more concisous about there health. ","1d23512f":"## Loading appropriate libraries","b44255b0":"## Creating Dummy Variables\nfrom the dataset'cp', 'thal' and 'slope' are categorical variables we'll turn them into dummy variables.","7efc0280":"StandardScaler will transform your data such that its distribution will have a mean value 0 and standard deviation of 1. Given the distribution of the data, each value in the dataset will have the sample mean value subtracted, and then divided by the standard deviation of the whole dataset.","6d05728e":"Describe function is a function that allows analysis between the numerical values contained in the data set. Using this function count, mean, std, min, max, 25%, 50%, 75%.\nAs seen in this section, most values are generally categorized. This means that we need to integrate other values into this situation. These; age, trestbps, chol, thalach","7d337a3e":"#### Plotting the Accuracy of the models","f968b1fb":"## Your feedback is very important to me,I hope this kernel is helpfull for you -->> upvote will appreciate me for further work.","9cc6be66":"##### from the above plot we see that the rate of heart disease in females have more in comprission of male. ","88bcffce":"In addition, we will analyze for this dataset. We will use a wide range of tools for this part. If there's value in there, we'il do it there. Finally, machine learning algorithms are estimated.","9563d28e":"## Symptoms","52823295":"# Introduction","3de5fb13":"* Chest pain, chest tightness, chest pressure and chest discomfort (angina)\n* Shortness of breath\n* Pain, numbness, weakness or coldness in your legs or arms if the blood vessels in those parts of your body are narrowed\n* Pain in the neck, jaw, throat, upper abdomen or back.\n* Heart failure is also an outcome of heart disease, and breathlessness can occur when the heart becomes too weak to circulate blood.\n* Some heart conditions occur with no symptoms at all, especially in older adults and individuals with\u00a0diabetes.","90ccb9a9":"Here we plot the performance or the accuracy of the different machine learning model, in this plot we observe that the different models have diffrent performence."}}