{"cell_type":{"8b3e14a0":"code","4f43eedf":"code","40824b60":"code","7d9a8fc4":"code","efd0a54b":"code","5009507f":"code","71cabe5f":"code","11560981":"code","447e6cde":"code","4856bd97":"code","452d91cd":"code","3df82dae":"code","2291e379":"code","63ba6a6a":"code","733a150d":"code","96f9e378":"code","73ed795d":"code","61815d15":"code","2b183a66":"code","69e5029d":"code","04047084":"code","14a70a64":"code","bc09d496":"code","f2e50ef4":"code","5b6e29ae":"code","b873a1a7":"code","71798666":"code","9bbefd9a":"code","3708bd69":"code","732381de":"code","e5fe9d58":"code","86ac4945":"code","3b72f627":"code","6bb724eb":"code","9f8d8201":"code","04d11dc3":"code","4b779f33":"code","3eb31a75":"code","ec399e88":"code","88af3a5d":"code","b9759cbf":"code","69c4a4bc":"code","daac8145":"code","64ef44b7":"code","0e0e78ab":"code","e3869c38":"code","594b7bac":"markdown","835c1404":"markdown","66ff8c3c":"markdown","8562e726":"markdown","2f4c0ceb":"markdown","3757a7e1":"markdown","33158f02":"markdown","184be2c4":"markdown","6fbfebc7":"markdown","fc2a83f4":"markdown","d4282223":"markdown","3d6215a7":"markdown","a4433a00":"markdown","d5ae09d6":"markdown","65783666":"markdown"},"source":{"8b3e14a0":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport plotly.graph_objects as go\nimport plotly.express as px\n\n#importing libraries to use the library functions","4f43eedf":"train=pd.read_csv('..\/input\/house-price-prediction-with-boston-housing-dataset\/train1.csv')\n#train contains the file information  which is in csv format","40824b60":"train","7d9a8fc4":"train.columns\n#listing the column names of the dataset\/dataframe","efd0a54b":"train.dtypes\n#checking the datatypes of different columns of the dataframe","5009507f":"train.shape\n#shape of the dataframe ie no. of rows and columns","71cabe5f":"train.duplicated().sum()\n#checking for any duplicates in the data","11560981":"train.isnull().sum()\n#checking for any null values in the data","447e6cde":"train.dropna(inplace=True,axis=0)\n#removing the null values in the data","4856bd97":"train.info()\n#getting the information of dataframe such as no. of entries,data columns,non-null count,data types,etc","452d91cd":"train.describe()\n#checking for statistical summary such as count,mean,etc. of numeric columns","3df82dae":"sns.boxplot(data=train,orient='h',palette='Set2')\n#checking for any outliers in the data","2291e379":"sns.boxplot(x=train['AGE'])","63ba6a6a":"sns.boxplot(x=train['TAX'])","733a150d":"sns.boxplot(x=train['Id'])","96f9e378":"sns.boxplot(x=train['MEDV'])","73ed795d":"Q1 = train.quantile(0.25)\nQ3 = train.quantile(0.75)\nIQR = Q3 - Q1\nprint(IQR)","61815d15":"train.corr()\n#finding the correlation between different variables\/features","2b183a66":"train_corr=train.corr()\nf,ax=plt.subplots(figsize=(12,7))\nsns.heatmap(train_corr,cmap='viridis',annot=True)\nplt.title(\"Correlation between features\",weight='bold',fontsize=18)\nplt.show()\n\n#plotting the heatmap for different features","69e5029d":"X = train.drop('MEDV', axis = 1)\ny = train['MEDV']","04047084":"from sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\n\ny = np.round(train['MEDV'])\n\n#Apply SelectKBest class to extract top 5 best features\nbestfeatures = SelectKBest(score_func=chi2, k=5)\nfit = bestfeatures.fit(X,y)\ndfscores = pd.DataFrame(fit.scores_)\ndfcolumns = pd.DataFrame(X.columns)\n\n# Concat two dataframes for better visualization\nfeatureScores = pd.concat([dfcolumns,dfscores],axis=1)\nfeatureScores.columns = ['Specs','Score'] #naming the dataframe columns\nfeatureScores","14a70a64":"print(featureScores.nlargest(5,'Score')) #print 5 best features","bc09d496":"from sklearn.ensemble import ExtraTreesClassifier\nimport matplotlib.pyplot as plt\nmodel = ExtraTreesClassifier()\nmodel.fit(X,y)","f2e50ef4":"print(model.feature_importances_) #use inbuilt class feature_importances of tree based classifiers","5b6e29ae":"# splitting the dataset into training and test sets\n\nfrom sklearn.model_selection import train_test_split\n\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size = 0.3, random_state = 42)\n\nprint(X_train.shape)\nprint(y_train.shape)\nprint(X_test.shape)\nprint(y_test.shape)","b873a1a7":"from sklearn.linear_model import LinearRegression\n\nmodel = LinearRegression()\nmodel.fit(X_train,y_train)\ny_predict = model.predict(X_train)\n\n# calculating the accuracies\nprint(\"Training Accuracy :\",model.score(X_train,y_train)*100)\nprint(\"Testing Accuracy :\",model.score(X_test,y_test)*100)","71798666":"from sklearn.metrics import mean_squared_error,r2_score\nprint(\"Model Accuracy\",r2_score(y,model.predict(X))*100)","9bbefd9a":"test=pd.read_csv('..\/input\/house-price-prediction-with-boston-housing-dataset\/test1.csv')\n#test contains the file information  which is in csv format","3708bd69":"test","732381de":"test.shape","e5fe9d58":"# Checking for missing values\ntest.isna().sum()","86ac4945":"test['CRIM'] = test['CRIM'].fillna(test['CRIM'].mean())\ntest['ZN'] = test['ZN'].fillna(test['ZN'].mean())\ntest['INDUS'] = test['INDUS'].fillna(test['INDUS'].mean())\ntest['CHAS'] = test['CHAS'].fillna(test['CHAS'].mean())\ntest['AGE'] = test['AGE'].fillna(test['AGE'].mean())\ntest['LSTAT'] = test['LSTAT'].fillna(test['LSTAT'].mean())","3b72f627":"y_pred = model.predict(test)","6bb724eb":"test['MEDV'] = y_pred","9f8d8201":"test","04d11dc3":"submission = test.drop(['CRIM','ZN','INDUS','CHAS','NOX','RM','AGE','DIS','RAD','TAX','PTRATIO','B','LSTAT'],axis=1)\nsubmission","4b779f33":"submission.to_csv('submit.csv',index=False)","3eb31a75":"X = train.drop('MEDV', axis = 1)\ny = train['MEDV']","ec399e88":"# splitting the dataset into training and test sets\n\nfrom sklearn.model_selection import train_test_split\n\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size = 0.2, random_state = 0)\n\nprint(X_train.shape)\nprint(y_train.shape)\nprint(X_test.shape)\nprint(y_test.shape)","88af3a5d":"from sklearn.ensemble import RandomForestRegressor\nrfr = RandomForestRegressor()\nrfr.fit(X_train,y_train)\ny_predict = rfr.predict(X_train)\n\n# calculating the accuracies\nprint(\"Training Accuracy :\",rfr.score(X_train,y_train)*100)\nprint(\"Testing Accuracy :\",rfr.score(X_test,y_test)*100)","b9759cbf":"from sklearn.metrics import mean_squared_error,r2_score\nprint(\"Model Accuracy\",r2_score(y,rfr.predict(X))*100)","69c4a4bc":"test = test.drop('MEDV', axis = 1)\ny_pred = rfr.predict(test)","daac8145":"test['MEDV'] = y_pred","64ef44b7":"test","0e0e78ab":"submission = test.drop(['CRIM','ZN','INDUS','CHAS','NOX','RM','AGE','DIS','RAD','TAX','PTRATIO','B','LSTAT'],axis=1)\nsubmission","e3869c38":"submission.to_csv('submit1.csv',index=False)","594b7bac":"## Conclusion\n### Therefore, it can be seen from the evaluation of three models that Random Forest Regressor performed better than Linear Regression.","835c1404":"### Correlation matrix\nCorrelation coefficients quantify the association between variables or features of a dataset. These statistics are of high importance for science and technology, and Python has great tools that you can use to calculate them. SciPy, NumPy, and Pandas correlation methods are fast, comprehensive, and well-documented.\n\nThe correlation matrix can be used to estimate the linear historical relationship between the returns of multiple assets. You can use the built-in . corr() method on a pandas DataFrame to easily calculate the correlation matrix. Correlation ranges from -1 to 1.","66ff8c3c":"### From the above heatmap ,we can conclude that :\n#### >> Price(MEDV) greatly depends upon features RM (positively correlated) and LSTAT (negatively correlated).\n#### >> Also, the features AGE and DIS are negatively correlated with each other.\ni.e if a house is older then Weighted distances to five Boston employment centers decreases.\nSimilarly, other such pairs are \"DIS - NOX\", \"DIS - INDUS\" and \"LSTAT - RM\".\n#### >> And features TAX and RAD are positively correlated with each other.\ni.e if Tax increases, accessibility to radial highways also increases.","8562e726":"## Feature Importance\nFeature importance refers to techniques that assign a score to input features based on how useful they are at predicting a target variable.The role of feature importance in a predictive modeling problem.","2f4c0ceb":"### Heatmap\nA heatmap is a graphical representation of data in which data values are represented as colors. That is, it uses color in order to communicate a value to the reader. This is a great tool to assist the audience towards the areas that matter the most when you have a large volume of data.","3757a7e1":"## Feature Selection\nFeature Selection is the process where you automatically or manually select those features which contribute most to your prediction variable or output in which you are interested in. Having irrelevant features in your data can decrease the accuracy of the models and make your model learn based on irrelevant features.","33158f02":"### Linear Regression\n\nLinear regression is a basic and commonly used type of predictive analysis.The overall idea of regression is to examine two things: \n                            \n(1) does a set of predictor variables do a good job in predicting an outcome (dependent) variable? \n\n(2) Which variables in particular are significant predictors of the outcome variable, and in what way do they\u2013indicated by the magnitude and sign of the beta estimates\u2013impact the outcome variable?  \n\n  These regression estimates are used to explain the relationship between one dependent variable and one or more independent variables.  ","184be2c4":"# Boston House Price Prediction\n\nThis competition is all about predicting the prices of houses in Boston Suburb based on different parameters using Linear Regression. For this, we will be using the Boston Housing Dataset from Kaggle.","6fbfebc7":"## Data Cleaning\nData cleansing or data cleaning is the process of detecting and correcting (or removing) corrupt or inaccurate records from a record set, table, or database and refers to identifying incomplete, incorrect, inaccurate or irrelevant parts of the data and then replacing, modifying, or deleting the dirty or coarse data.","fc2a83f4":"### Box plot for outliers\nIn descriptive statistics, a box plot is a method for graphically depicting groups of numerical data through their quartiles. Box plots may also have lines extending vertically from the boxes (whiskers) indicating variability outside the upper and lower quartiles, hence the terms box-and-whisker plot and box-and-whisker diagram. Outliers may be plotted as individual points.","d4282223":"#### Predictive Modelling\n\nPredictive modeling is a powerful way to add intelligence to your application. It enables applications to predict outcomes against new data. The act of incorporating predictive analytics into your applications involves two major phases: model training and model deployment.","3d6215a7":"### Splitting the dataset\n\nTypically,we separate a data set into a training set and testing set, most of the data is used for training, and a smaller portion of the data is used for testing. Analysis Services randomly samples the data to help ensure that the testing and training sets are similar.","a4433a00":"## EDA with Data Visualization\n\nExploratory Data Analysis refers to the critical process of performing initial investigations on data so as to discover patterns,to spot anomalies,to test hypothesis and to check assumptions with the help of summary statistics and graphical representations.\n\nData visualization is the graphical representation of data in order to interactively and efficiently convey insights to clients, customers, and stakeholders in general.","d5ae09d6":"## Modelling and Prediction","65783666":"### Random Forest\n\nRandom forest is like bootstrapping algorithm with Decision tree (CART) model. Say, we have 1000 observation in the complete population with 10 variables. Random forest tries to build multiple CART models with different samples and different initial variables. For instance, it will take a random sample of 100 observation and 5 randomly chosen initial variables to build a CART model. It will repeat the process (say) 10 times and then make a final prediction on each observation. Final prediction is a function of each prediction. This final prediction can simply be the mean of each prediction."}}