{"cell_type":{"1f881680":"code","8ac8ac9c":"code","c84f5d7b":"code","cf120a79":"code","48d1c311":"code","588b2bca":"code","e9ec79cd":"code","f2561996":"code","0ff5bbfc":"code","f7badc46":"code","aa3b3b99":"code","8efbe959":"code","40322816":"code","52b822ce":"code","0de2d416":"code","48411e26":"code","3bf6f21a":"code","c4792204":"code","794683af":"code","af20f131":"code","4c99dfb7":"code","06bb8122":"code","ea92409b":"code","36d7504b":"code","746a8de3":"code","92928e26":"code","c7a9c950":"code","2a8f2a0f":"code","48b00e1c":"code","06aab477":"code","102257c5":"code","0b9c06d4":"markdown","3c24406c":"markdown","025e1d0b":"markdown","4f82e8c0":"markdown","91f347f4":"markdown","ae2d3c48":"markdown","dc99c39f":"markdown","c5363ede":"markdown","61b50df8":"markdown","b8de9c43":"markdown","5a325b5d":"markdown","79c1aa0b":"markdown"},"source":{"1f881680":"# Importing the libraries\n\nimport numpy as np # for working with arrays\nnp.set_printoptions(suppress=True) # not a must but nice to avoid scientific notation\n\n\nimport pandas as pd # as usual for handling dataframes\npd.options.display.float_format = '{:.4f}'.format #same for pandas to turn off scientific notation","8ac8ac9c":"# Importing the dataset\ndataset = pd.read_csv('..\/input\/wine.csv')","c84f5d7b":"# Quick check of the dataframe proportions\ndataset.shape","cf120a79":"# Checking the first 5 rows to get familiar with the data\ndataset.head()","48d1c311":"# Getting basic descriptives for all nummerical variables\ndataset.describe()","588b2bca":"# Selecting the relevant data\n# using the iloc selector allows to grab a range 2-15 of columns\n# withouth having to call their names. That's practical\n# Also, we ask for values only, as we are going to pass the data into\n# the ML algorithms in the form of arrays rather than pandas DFs\n\nX = dataset.iloc[:, 2:15].values\ny = dataset.iloc[:, 1].values","e9ec79cd":"# Encoding categorical data\nfrom sklearn.preprocessing import LabelEncoder","f2561996":"# From labels to numbers\nlabelencoder_y = LabelEncoder()\ny = labelencoder_y.fit_transform(y)","0ff5bbfc":"# Feature scaling\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\n\nX = scaler.fit_transform(X)","f7badc46":"# We can check our transform data using pandas describe\npd.DataFrame(X).describe()","aa3b3b99":"# Splitting the dataset into the Training set and Test set\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 21)","8efbe959":"# We first import and train a Logistic Regression\n\nfrom sklearn.linear_model import LogisticRegression\n\nclassifier = LogisticRegression(random_state = 22)\n\nclassifier.fit(X_train, y_train)\n\n\n# After training the model we should jump further down (over the next 2 models)\n# To evaluate the results","40322816":"# Fitting Random Forest Classification to the Training set\nfrom sklearn.ensemble import RandomForestClassifier\n\nclassifier = RandomForestClassifier(n_estimators = 50, criterion = 'entropy', random_state = 22)\n\nclassifier.fit(X_train, y_train)","52b822ce":"# Finally we train a Support Vector Classifier\nfrom sklearn.svm import SVC\n\nclassifier = SVC(kernel = 'linear', random_state = 21)\n\nclassifier.fit(X_train, y_train)","0de2d416":"# Applying k-Fold Cross Validation\nfrom sklearn.model_selection import cross_val_score\n\naccuracies = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 5)\n\nprint(accuracies.mean())\nprint(accuracies.std())","48411e26":"# Predicting the Test set results\ny_pred = classifier.predict(X_test)","3bf6f21a":"# Making a classification report\nfrom sklearn.metrics import classification_report\n\ncm = classification_report(y_test, y_pred)","c4792204":"print(cm)","794683af":"# Transforming nummerical labels to wine types\n\ntrue_wines = labelencoder_y.inverse_transform(y_test)\n\npredicted_wines = labelencoder_y.inverse_transform(y_pred)","af20f131":"# Creating a pandas DataFrame and cross-tabulation\n\ndf = pd.DataFrame({'true_wines': true_wines, 'predicted_wines': predicted_wines}) \n\npd.crosstab(df.true_wines, df.predicted_wines)","4c99dfb7":"# We import KMeans and creade a model object (we know that there are 3 wines...kind of cheating)\nfrom sklearn.cluster import KMeans\n\nmodel = KMeans(n_clusters = 3)","06bb8122":"# Fitting the model is super easy, jsut one line\nmodel.fit(X_train)","ea92409b":"# Prediction is easy, too\n\npredicted_wine_clusters = model.predict(X_train)\n\npredicted_new_wine_clusters = model.predict(X_test)","36d7504b":"# Quick print out of the labels\n\npredicted_wine_clusters","746a8de3":"# Transforming nummerical labels to wine types\n\ntrue_wines = labelencoder_y.inverse_transform(y_train)\n\ndf = pd.DataFrame({'true_wines': true_wines, 'predicted_wines': predicted_wine_clusters}) \npd.crosstab(df.true_wines, df.predicted_wines)","92928e26":"from sklearn.manifold import TSNE\n\ntsne = TSNE()\n\nviz = tsne.fit_transform(X)","c7a9c950":"model_all = KMeans(n_clusters = 3)\nmodel_all.fit(X)","2a8f2a0f":"predicted_wine_clusters_all = model_all.predict(X)","48b00e1c":"true_wines = labelencoder_y.inverse_transform(y)\n\ndf = pd.DataFrame({'true_wines': true_wines, 'predicted_wines': predicted_wine_clusters_all}) \npd.crosstab(df.true_wines, df.predicted_wines)","06aab477":"import matplotlib.pyplot as plt\nimport seaborn as sns","102257c5":"plt.figure(figsize=(10,10))\nsns.scatterplot(viz[:,0],viz[:,1], hue=)","0b9c06d4":"Yes, there is a ```class_lable``` in the dataset but for the sake of learning and because it is very simple, we are going to construct our class_lables on our own. For this we will use the ```LabelEncoder``` from Scikit-Learn. Note that in contrast to Pandas, the Scikit-Learn is more of a (HUGE!!!) Library where you have to import different functionalities separately. You can find an index of all classes [here](http:\/\/scikit-learn.org\/stable\/modules\/classes.html).","3c24406c":"Now that we fitted or trained a model we need to figure out how well it performes. This approach to evaluation is very different from what many of you are used to from econometrics. \n\nHere we are not interested in a model summary table, rather we will be exploring predictive performance.\nIn the next cell we ask the classifier object (our trained model) to gives us predictions for data it never has seen before.\n\nThen we will compare the predictions made against the real-world values that we actually know.","025e1d0b":"There is also a slightly more intuitive way to evaluate our predictions in the case of a multiclass-classification where we cannot just create a confusion-matrix. What we can do is using pandas to crosstabulate our real against our predicted wines.\n\nTo get the wine names, we will use the ```inverse_transform``` function of our ```labelencoder```","4f82e8c0":"As you have seen from the descriptives above our variables lie on very different scales. Therefore, we will standardize them before going further. The procedure using the ```StandardScaler```is exactly the same as before with the label encoder.\n\nThis scaling will for each value substract the mean (of the column) and devide it by the standard deviation, thus bringing them all on the same scale with a mean of 0 and a standard deviation of 1.","91f347f4":"![alt text](https:\/\/uproxx.files.wordpress.com\/2015\/12\/bender-pointless-day.jpg?quality=95)\n\nNow it's time for the model to meet the wine data.\n\nWe will be using 3 different models. The reason why we use 3 models is because, it is nice to see how easy it is to switch them aroun to experiment what works best. Since we can calculate an (kind of) objective quality measure, it is easy to compare and evaluate them agains each other. \n\n*   Logistic Regression\n*   Suport Vector Classifier\n* Random Forest Classifier\n\nRemember that this is a classification problem rather than a regression. The models will be estimating probabilities for some class vs. other classes.","ae2d3c48":"**But is that not the same as PCA or soe other kind of clustering?**\n\nWell, let's try to use unsupervised learning on the same data-set. We will be using KMeans (because it is simple and nice for illustration)\n\nJust as before, we import a model class, define a model object and fit it. Same 3 steps as before.","dc99c39f":"Note that the clustering model never met any y-values - only X values","c5363ede":"In the next step we split the data into a training and a test-set. Very often you will see a split of 80\/20 %\n\n\n![alt text](https:\/\/cdn-images-1.medium.com\/max\/1000\/1*4G__SV580CxFj78o9yUXuQ.png)\n\n80% of the data will be used to fit a model, while we will keep 20% of the data for testing the models performance.\n\nThe train_test_split class takes 4 parameters: (X, y, test_size = 0.2, random_state = 21)\n\n\n1.   Input matrix: X\n2.   Output matrix: y\n3. The test size: We take 20%\n4. A random state (optional): Some number for the random generator that will shuffle the values*\n\n*The whole random state thing is mostly for easier reproducibility and can also be let our. \n\n\n\n","61b50df8":"We can see here that means and spread (standard deviation) of the features is very different and thus we will need to standardize the dataset. \n\n\n> \"As a rule of thumb I\u2019d say: When in doubt, just standardize the data, it shouldn\u2019t hurt.\"\" [Sebastian Raschka](https:\/\/sebastianraschka.com\/Articles\/2014_about_feature_scaling.html)","b8de9c43":"# Classification of Italian Wines\n![alt text](https:\/\/viaverdimiami.com\/wp-content\/uploads\/2017\/07\/Italian-Wine.jpg)\n\nIn this notebook we will be using supervised learning to classify Italian wines. \nThe question is: Can we teach a machine to figure out which type of wine an obseration belongs to?\n\nWe will work with a famous but small dataset that can be found [here](https:\/\/archive.ics.uci.edu\/ml\/datasets\/wine) (along more informaion).\nThe data is clean, contains only numerical and no missing values. We will not do any EDA but only focus on prediction. The only preprocessing step will be standardization of the physiochemical variables.\n\nWe will be using Pandas and Scikit-Learn which are both parts of the Anaconda distribution.","5a325b5d":"Classes such as the ```LabelEncoder``` or any modely type that you import have several parameters that can (but don't have to be) specified. Also, you are usually fitting them to some data first before performind transformations. Thus, they are *cutom-made* for each use case and therefore you will need to define an encoder object from the imported class. This is a general philosophy behind all Scikit-Learn classes. The good news: The syntax is the same across all classes.\n\nBelow we first define a ```labelencoder_y``` and then use the ```fit_transform``` method (we could also first use ```fit``` and then ```transform```) to turn our wine-type names into numbers.","79c1aa0b":"Perhaps this time the algorithm was just lucky because of a random allocation of the data in the train-test split. To make sure which model is the most accurate, we can run a k-Fold Cross Validation deviding x_train into (here) 10 parts, training on 9 and testing on 1. This will be done 10 times, every time measuring the accuracy and finally returning the average accuracy.\n\n![alt text](https:\/\/www.researchgate.net\/profile\/Kiret_Dhindsa\/publication\/323969239\/figure\/fig10\/AS:607404244873216@1521827865007\/The-K-fold-cross-validation-scheme-133-Each-of-the-K-partitions-is-used-as-a-test.png)"}}