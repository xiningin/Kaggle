{"cell_type":{"3cf7d281":"code","06b7edd7":"code","2d8137a9":"code","971ad36b":"code","4d2ad20e":"code","e1ae6b4e":"code","3864ceab":"code","c344363f":"code","22d1bdee":"code","16aa5549":"code","ec8c6629":"code","f1b76a8c":"code","7fcf9784":"code","bc2b9e00":"code","4595d415":"code","6545910b":"code","c69c43cf":"code","59300491":"code","01f32330":"code","be93c24d":"code","ae90d5f6":"code","0571fe67":"code","e0a30c64":"code","eb804355":"code","3c304184":"code","74f3eb7e":"code","cf0a41f6":"code","42606c3a":"code","7e412577":"code","51a9171f":"code","72db25d6":"code","bc117e63":"code","cc4b0d0b":"code","4e32f628":"code","0f4efbc8":"code","0a988fda":"markdown","067179dd":"markdown","d08575af":"markdown","f95488aa":"markdown","db0cc573":"markdown","b16f4371":"markdown","cd6be9dc":"markdown","236de2c1":"markdown","2ab84db5":"markdown","9be85421":"markdown","ee548d9b":"markdown","01f4de45":"markdown","8906c196":"markdown"},"source":{"3cf7d281":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","06b7edd7":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\nfrom wordcloud import WordCloud\n\nimport nltk\nfrom nltk.corpus import stopwords\nimport re, string\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional, Flatten\nfrom tensorflow.keras.layers import Dropout\nfrom sklearn.metrics import confusion_matrix, classification_report, accuracy_score","2d8137a9":"train = pd.read_csv('..\/input\/nlp-with-disaster-tweets-cleaning-data\/train_data_cleaning.csv')","971ad36b":"train.head(3)","4d2ad20e":"train.shape","e1ae6b4e":"train.info()","3864ceab":"# Checking if any duplicate records are present\n\nduplicate = train[train.duplicated()] \nduplicate","c344363f":"train.describe(include='object')","22d1bdee":"# Checking for null values\n\ntrain.isnull().sum()","16aa5549":"# Dropping 'id' and 'location'\n\ntrain.drop(['id', 'location'], axis=1, inplace=True)","ec8c6629":"# Dropping null values in 'keyword'\n\ntrain.dropna(inplace=True)","f1b76a8c":"# Checking for null values again\n\ntrain.isnull().sum()","7fcf9784":"# Visualizing the disribution of 'target'\n\nplt.style.use('seaborn')\nsns.countplot(train['target'])\nplt.title('Distribution of target', fontsize=20)","bc2b9e00":"# Visualizing count of each keywod\n\npx.histogram(train, x=train['keyword'], title='Count of Keyword')","4595d415":"# Group by target and print count of keyword\n\na = train.groupby('target').agg({'keyword':'count'})\na.plot(kind='bar', color='green')\nplt.legend()","6545910b":"# Expanding contractions\n\n# Dictionary of English Contractions\ncontractions_dict = { \"ain't\": \"are not\",\"'s\":\" is\",\"aren't\": \"are not\",\n                     \"can't\": \"cannot\",\"can't've\": \"cannot have\",\n                     \"'cause\": \"because\",\"could've\": \"could have\",\"couldn't\": \"could not\",\n                     \"couldn't've\": \"could not have\", \"didn't\": \"did not\",\"doesn't\": \"does not\",\n                     \"don't\": \"do not\",\"hadn't\": \"had not\",\"hadn't've\": \"had not have\",\n                     \"hasn't\": \"has not\",\"haven't\": \"have not\",\"he'd\": \"he would\",\n                     \"he'd've\": \"he would have\",\"he'll\": \"he will\", \"he'll've\": \"he will have\",\n                     \"how'd\": \"how did\",\"how'd'y\": \"how do you\",\"how'll\": \"how will\",\n                     \"I'd\": \"I would\", \"I'd've\": \"I would have\",\"I'll\": \"I will\",\n                     \"I'll've\": \"I will have\",\"I'm\": \"I am\",\"I've\": \"I have\", \"isn't\": \"is not\",\n                     \"it'd\": \"it would\",\"it'd've\": \"it would have\",\"it'll\": \"it will\",\n                     \"it'll've\": \"it will have\", \"let's\": \"let us\",\"ma'am\": \"madam\",\n                     \"mayn't\": \"may not\",\"might've\": \"might have\",\"mightn't\": \"might not\", \n                     \"mightn't've\": \"might not have\",\"must've\": \"must have\",\"mustn't\": \"must not\",\n                     \"mustn't've\": \"must not have\", \"needn't\": \"need not\",\n                     \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\"oughtn't\": \"ought not\",\n                     \"oughtn't've\": \"ought not have\",\"shan't\": \"shall not\",\"sha'n't\": \"shall not\",\n                     \"shan't've\": \"shall not have\",\"she'd\": \"she would\",\"she'd've\": \"she would have\",\n                     \"she'll\": \"she will\", \"she'll've\": \"she will have\",\"should've\": \"should have\",\n                     \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\",\"so've\": \"so have\",\n                     \"that'd\": \"that would\",\"that'd've\": \"that would have\", \"there'd\": \"there would\",\n                     \"there'd've\": \"there would have\", \"they'd\": \"they would\",\n                     \"they'd've\": \"they would have\",\"they'll\": \"they will\",\n                     \"they'll've\": \"they will have\", \"they're\": \"they are\",\"they've\": \"they have\",\n                     \"to've\": \"to have\",\"wasn't\": \"was not\",\"we'd\": \"we would\",\n                     \"we'd've\": \"we would have\",\"we'll\": \"we will\",\"we'll've\": \"we will have\",\n                     \"we're\": \"we are\",\"we've\": \"we have\", \"weren't\": \"were not\",\"what'll\": \"what will\",\n                     \"what'll've\": \"what will have\",\"what're\": \"what are\", \"what've\": \"what have\",\n                     \"when've\": \"when have\",\"where'd\": \"where did\", \"where've\": \"where have\",\n                     \"who'll\": \"who will\",\"who'll've\": \"who will have\",\"who've\": \"who have\",\n                     \"why've\": \"why have\",\"will've\": \"will have\",\"won't\": \"will not\",\n                     \"won't've\": \"will not have\", \"would've\": \"would have\",\"wouldn't\": \"would not\",\n                     \"wouldn't've\": \"would not have\",\"y'all\": \"you all\", \"y'all'd\": \"you all would\",\n                     \"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\n                     \"y'all've\": \"you all have\", \"you'd\": \"you would\",\"you'd've\": \"you would have\",\n                     \"you'll\": \"you will\",\"you'll've\": \"you will have\", \"you're\": \"you are\",\n                     \"you've\": \"you have\"}\n\n# Regular expression for finding contractions\ncontractions_re=re.compile('(%s)' % '|'.join(contractions_dict.keys()))\n\n# Function for expanding contractions\ndef expand_contractions(text,contractions_dict=contractions_dict):\n    def replace(match):\n        return contractions_dict[match.group(0)]\n    return contractions_re.sub(replace, text)\n\n# Expanding Contractions in train, test\ntrain['text'] = train['text'].apply(lambda x:expand_contractions(x))","c69c43cf":"# Cleaning text\n\ndef clean_text(text):\n    text = text.lower()                                  # lower-case all characters\n    text =  re.sub(r'@\\S+', '',text)                     # remove twitter handles\n    text =  re.sub(r'http\\S+', '',text)                  # remove urls\n    text =  re.sub(r'pic.\\S+', '',text) \n    text =  re.sub(r\"[^a-zA-Z+']\", ' ',text)             # only keeps characters\n    text = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', text+' ')      # keep words with length>1 only\n    text = \"\".join([i for i in text if i not in string.punctuation])\n    words = nltk.tokenize.word_tokenize(text)\n    stopwords = nltk.corpus.stopwords.words('english')   # remove stopwords\n    text = \" \".join([i for i in words if i not in stopwords and len(i)>2])\n    text= re.sub(\"\\s[\\s]+\", \" \",text).strip()            # remove repeated\/leading\/trailing spaces\n    return text","59300491":"train['text'] = train['text'].apply(clean_text)","01f32330":"# Word cloud of text in disaster and non-disaster tweets\n\n# Cleaned dataframe of disaster\ndf_true = train[train.target == 1]\n\ntext_true = \" \".join(txt for txt in df_true['text'])\n\ntext_cloud = WordCloud(collocations=False, background_color='black').generate(text_true)\nplt.axis(\"off\")\nplt.imshow(text_cloud, interpolation='bilinear')","be93c24d":"# Cleaned dataframe of non-disaster\ndf_fake = train[train.target == 0]\n\ntext_fake = \" \".join(txt for txt in df_fake['text'])\n\ntext_cloud = WordCloud(collocations=False, background_color='black').generate(text_fake)\nplt.axis(\"off\")\nplt.imshow(text_cloud, interpolation='bilinear')","ae90d5f6":"# Splitting independent and dependent variables\n\nX = train['text']\ny = train['target']","0571fe67":"# Splitting the dataset into train and test set\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","e0a30c64":"# Convert sentences to sequences\n\nvocab_size = 1500\ntokenizer = Tokenizer(num_words=vocab_size)\ntokenizer.fit_on_texts(X_train)","eb804355":"X_train = tokenizer.texts_to_sequences(X_train)\nX_test = tokenizer.texts_to_sequences(X_test)","3c304184":"max_length = 120\n\nX_train_padded = pad_sequences(X_train, maxlen=max_length, padding='pre')\nX_test_padded = pad_sequences(X_test, maxlen=max_length, padding='pre')","74f3eb7e":"y_train = np.array(y_train)\ny_test = np.array(y_test)","cf0a41f6":"embedding_vector_features = 40\n\nmodel = Sequential()\nmodel.add(Embedding(vocab_size,embedding_vector_features,input_length=120))\nmodel.add(Dropout(0.3))\nmodel.add(LSTM(100))\nmodel.add(Dropout(0.3))\nmodel.add(Dense(1,activation='sigmoid'))\nmodel.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])","42606c3a":"h = model.fit(X_train_padded, y_train, validation_data=(X_test_padded,y_test), epochs=30, batch_size=64)","7e412577":"model.evaluate(X_test_padded, y_test)","51a9171f":"# Predicting on test data\n\ny_pred = model.predict_classes(X_test_padded)","72db25d6":"# Calculating accuracy\n\naccuracy_score(y_test,y_pred)","bc117e63":"# Classification report\n\nprint(classification_report(y_test, y_pred))","cc4b0d0b":"# Confusion matrix\n\ncm = confusion_matrix(y_test, y_pred)\n\ngroup_names = ['True Neg', 'False Pos', 'False Neg', 'True Pos']\ngroup_counts = [\"{0:0.0f}\".format(value) for value in\n                cm.flatten()]\ngroup_percentages = [\"{0:.2%}\".format(value) for value in\n                     cm.flatten()\/np.sum(cm)]\nlabels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in\n          zip(group_names,group_counts,group_percentages)]\nlabels = np.asarray(labels).reshape(2,2)\nsns.heatmap(cm, annot=labels, fmt='', cmap='PuRd')","4e32f628":"# Plotting loss and val_loss\n\nplt.plot(h.history['loss'], label='loss')\nplt.plot(h.history['val_loss'], label='val_loss')\nplt.legend()","0f4efbc8":"# Plotting accuracy and val_accuracy\n\nplt.plot(h.history['accuracy'], label='acc')\nplt.plot(h.history['val_accuracy'], label='val_acc')\nplt.legend()","0a988fda":"<b> Words in non-disaster tweets:- <\/b> new, love, one, time, body, day, video, people, life, youtube, lol, etc.","067179dd":"<b> Words in disaster tweets:- <\/b> disaster, new, fire, via, year, suicide, police, home, dead, bombing, storm, etc.","d08575af":"<b> Hence, all null values are removed from the dataset. <\/b>","f95488aa":"## Text Preprocessing","db0cc573":"## Importing libraries","b16f4371":"<b> More than 3000 keywords are for disaster tweets and more than 4000 are for non-disaster tweets. <\/b>","cd6be9dc":"## Loading the dataset","236de2c1":"<b> Maximum words have a count greater than 30. <\/b>","2ab84db5":"## Model","9be85421":"<b> The dataset has 2 integer and 3 object columns. <\/b>","ee548d9b":"## Data Visualization","01f4de45":"<b> No duplicate records are present in the dataset. <\/b>","8906c196":"<b> The dataset has 7613 rows and 5 columns. <\/b>"}}