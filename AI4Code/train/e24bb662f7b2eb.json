{"cell_type":{"972322d0":"code","4de66ee1":"code","73d24510":"code","faeedf29":"code","79d3e147":"code","1943a642":"code","f9191cd7":"code","d8889dfc":"code","b313086f":"code","614762e5":"code","d80dbe98":"code","d5734a17":"code","f7ad0f32":"code","f1af4824":"code","b3a51ec4":"code","2f0c7318":"code","3940ad3e":"code","8158766c":"code","2eae0dde":"code","52690e70":"markdown","32e7d36d":"markdown","5c9ce6b0":"markdown","0c40ae65":"markdown","aa9b2387":"markdown","72fad170":"markdown"},"source":{"972322d0":"import pandas as pd\nimport numpy as np\nimport random\n#data imported from https:\/\/www.kaggle.com\/jatindersehdev\/poetry-analysis-data\ndata=pd.read_csv('..\/input\/all.csv')","4de66ee1":"data.head()","73d24510":"data['type'].value_counts()","faeedf29":"data['age'].value_counts()","79d3e147":"data['content'].str.split().str.len().sum()","1943a642":"data['content'].str.split().str.len().value_counts().head()","f9191cd7":"# Taking only small poems\nnew_data=data[data['content'].str.split().str.len()<245]\nnew_data['content'].str.split().str.len().sum()","d8889dfc":"new_data['type'].value_counts()\n#My dataset is similar even after removing long poems","b313086f":"new_data['age'].value_counts()","614762e5":"poems=new_data['content']","d80dbe98":"poems.str.split().head()","d5734a17":"#applying PorterStemming\nfrom nltk.stem import PorterStemmer\nps = PorterStemmer()\ndef stem_sentences(sentence):\n    tokens = sentence.split()\n    stemmed_tokens=[]\n    for token in tokens:\n        #Tokenize and also get rid of any punctuation\n        token.replace(' @$\/#.-:&*+=[]?!(){},''\">_<;%','')\n        #Remove any non alphanumeric characters\n        token.replace('[^a-zA-Z0-2019]', '')\n        #applying porter stemming to my content\n        if len(token)>1:\n            #porter stemming does not work sometimes\n            try:\n                temp=ps.stem(token)\n                stemmed_tokens.append(temp)\n            except temp == '':\n                continue\n    #stemmed_tokens = [ps.stem(token) for token in tokens]\n    return ' '.join(stemmed_tokens)\npoems_stem= poems.apply(stem_sentences)\npoems_stem=poems_stem.str.replace('\\n', \" \")\npoems_stem=poems_stem.str.replace(\"\\t\", \" \")\npoems_stem=poems_stem.str.replace(\"\\r\", \" \")\npoems_stem=poems_stem.str.replace(\",\",\" \").replace(\".\",\" \")\npoems_stem=poems_stem.str.replace(\"\\W\",\" \")\npoems_stem.head()","f7ad0f32":"#No. of words in total\npoems_stem.str.split().str.len().sum()","f1af4824":"all_poems=[]\nfor poem in poems_stem:\n    all_poems.append(poem)","b3a51ec4":"# using Hashing Vectorizer\nfrom sklearn.feature_extraction.text import HashingVectorizer\n# create the transform\nvectorizer = HashingVectorizer(n_features=20)\n# encode document\nvector = vectorizer.transform(all_poems)\n# summarize encoded vector\nprint(vector.shape)\nX=vector.toarray()\nX","2f0c7318":"m,n=vector.shape\n#K-means Clustering\nK=4\n\ndef findClosestCentroids(X, centroids):\n    idx = np.zeros(m,int)\n    for i in range(m):\n        Mindistance=sum(np.square(X[i]-centroids[0]))\n        idx[i]=0\n        for j in range(1,K):\n            distance=sum(np.square(X[i]-centroids[j]))\n            if (distance<Mindistance):\n                Mindistance=distance\n                idx[i]=j\n    return idx           \n                \ndef computeCentroids(X, idx, K):\n    sum=np.zeros(n)\n    count=0\n    for i in range(K):\n        sum=np.zeros(n)\n        count=0\n        for j in range(m):\n            if(i==idx[j]):\n                count=count+1\n                sum=sum+X[j]\n        if(count==0):\n            centroids[i]=X[random.randint(1,m)]\n        else:\n            centroids[i]=sum\/count\n    return centroids\n\n#from np.random import permutation\nrandidx= np.random.permutation(m)\ncentroids=X[randidx[1:K+1]]\nmax_iter=100\nidx = np.zeros(m,int)\nnew_idx= np.zeros(m,int)\nfor i in range(max_iter):\n    # For each example in X, assign it to the closest centroid\n    if i==0:\n        idx = findClosestCentroids(X, centroids)\n    else:\n        new_idx=findClosestCentroids(X, centroids)\n    if np.array_equal(idx,new_idx) and i!=0:\n        temp=i\n        break\n    else:\n        idx=new_idx\n    # Given the memberships, compute new centroids\n    centroids = computeCentroids(X, idx, K)\ncentroids\nidx_kmeans=idx\ny = np.bincount(idx)\nii = np.nonzero(y)[0]\nnp.vstack((ii,y[ii])).T\ny[ii]","3940ad3e":"from sklearn.cluster import SpectralClustering\nclustering=SpectralClustering(n_clusters=4,assign_labels=\"discretize\",random_state=0).fit(X)\nidx=clustering.labels_\nidx_spectral=idx\ny = np.bincount(idx)\nii = np.nonzero(y)[0]\nnp.vstack((ii,y[ii])).T\ny[ii]","8158766c":"from sklearn.decomposition import PCA\npca = PCA(n_components=2)\nprincipalComponents = pca.fit_transform(X)\nprincipalDf = pd.DataFrame(data = principalComponents\n             , columns = ['principal component 1', 'principal component 2'])\nprincipalDf.head()","2eae0dde":"import matplotlib.pyplot as plt\nfig = plt.figure(figsize = (8,8))\nax = fig.add_subplot(1,1,1) \nax.set_xlabel('Principal Component 1', fontsize = 15)\nax.set_ylabel('Principal Component 2', fontsize = 15)\nax.set_title('2 component PCA', fontsize = 20)\nlegend = ['Cluster-1', 'Cluster-2', 'Cluster-3','Cluster-4']\ntargets=[0,1,2,3]\ncolors = ['r', 'g', 'b','y']\nfor target, color in zip(targets,colors):\n    indicesToKeep= idx_spectral==target\n    ax.scatter(principalDf.loc[indicesToKeep, 'principal component 1']\n               , principalDf.loc[indicesToKeep, 'principal component 2']\n               , c = color\n               , s = 50)\nax.legend(legend)\nax.grid()","52690e70":"## PCA and Data Visualisation","32e7d36d":"## Spectral Clustering","5c9ce6b0":"## Porter Stemming","0c40ae65":"Poem Analysis\n=========\n##### Data Analysis and pre-processing\nThe Data contained 573 different poems having a 99353 words.Long poems which had length more than 245 were ignored.\nPorter Stemming was applied on 500 poems with all the punctuation marks removed.\nData was reduced to 500 poems having 49577 words in total.\nHashing vectorisation was used to extract feature vector from the stemmed poems.\nEach poem was reduced to a &#8477;<sup>20<\/sup> vector.\n\n##### Unsupervised clustering\n\nTwo main methods were used to cluster the unlabeled data.\n\n  - K-means Clustering\n  - Spectral Clustering\n  \nThe data was labeled as 0,1,2,3 as 4 new labels.\n\n##### Visualisation\n\nUsing PCA(Principal Component Analysis) , the 500x20 feature matrix was reduced to 500x2 feature matix and labelled data has been plotted for spectral clustering.\n\n##### Softwares\/packages required\n\n  - Pandas : for loading dataset\n  - numpy : for basic vector arithmetics\n  - random : for random initialisation of centriods in K-means Clustering\n  - NLTK(Natural Language toolkit) : Porter Stemming algorithm\n  - scikit-learn: Hashing Vectorizer,Spectral Clustering and PCA\n  - matplotlib: For plotting","aa9b2387":"## Hashing Vectorizer","72fad170":"## K-means Clustering"}}