{"cell_type":{"6504d449":"code","4c3ba7da":"code","d5b3c72f":"code","5ca05e34":"code","5c6495eb":"code","ec1bf627":"code","ff354f1a":"code","0cc52521":"code","7fff540e":"code","5cfe9d86":"code","e6b77b7b":"code","84c61bd1":"code","4eacc675":"code","1a8b171a":"code","348d7b9b":"code","71566d60":"code","d5dd5bd9":"code","b3225db5":"code","60977809":"code","84f9ee0c":"code","f908a6e8":"code","36e94927":"code","ff474cad":"code","52949943":"code","8ef7c535":"code","3da5dc1b":"code","fd1f7b06":"code","ae0af358":"code","7fd60c82":"code","68a9e061":"code","a4a80007":"code","80de09da":"code","9a908d1b":"code","6bbdcc52":"markdown","3f58096a":"markdown","994935ab":"markdown","b75854a3":"markdown","44d75558":"markdown","6f2008b5":"markdown","0034cfdb":"markdown","d1b2f419":"markdown","a598672f":"markdown","aab4143a":"markdown","b89a56d8":"markdown","a01bf9a0":"markdown","ff70ff0e":"markdown","e9c524d8":"markdown"},"source":{"6504d449":"import os \nimport sys\nimport random\nimport math\nimport numpy as np\nimport cv2\nimport matplotlib.pyplot as plt\nimport json\nimport pydicom\nfrom imgaug import augmenters as iaa\nfrom tqdm import tqdm\nimport pandas as pd \nimport glob ","4c3ba7da":"DATA_DIR = '\/kaggle\/input'\n\n# Directory to save logs and trained model\nROOT_DIR = '\/kaggle\/working'","d5b3c72f":"!git clone https:\/\/www.github.com\/matterport\/Mask_RCNN.git\nos.chdir('Mask_RCNN')\n#!python setup.py -q install","5ca05e34":"# Import Mask RCNN\nsys.path.append(os.path.join(ROOT_DIR, 'Mask_RCNN'))  # To find local version of the library\nfrom mrcnn.config import Config\nfrom mrcnn import utils\nimport mrcnn.model as modellib\nfrom mrcnn import visualize\nfrom mrcnn.model import log","5c6495eb":"train_dicom_dir = os.path.join(DATA_DIR, 'stage_1_train_images')\ntest_dicom_dir = os.path.join(DATA_DIR, 'stage_1_test_images')","ec1bf627":"def get_dicom_fps(dicom_dir):\n    dicom_fps = glob.glob(dicom_dir+'\/'+'*.dcm')\n    return list(set(dicom_fps))\n\ndef parse_dataset(dicom_dir, anns): \n    image_fps = get_dicom_fps(dicom_dir)\n    image_annotations = {fp: [] for fp in image_fps}\n    for index, row in anns.iterrows(): \n        fp = os.path.join(dicom_dir, row['patientId']+'.dcm')\n        image_annotations[fp].append(row)\n    return image_fps, image_annotations ","ff354f1a":"# The following parameters have been selected to reduce running time for demonstration purposes \n# These are not optimal \n\nclass DetectorConfig(Config):\n    \"\"\"Configuration for training pneumonia detection on the RSNA pneumonia dataset.\n    Overrides values in the base Config class.\n    \"\"\"\n    \n    # Give the configuration a recognizable name  \n    NAME = 'pneumonia'\n    \n    # Train on 1 GPU and 8 images per GPU. We can put multiple images on each\n    # GPU because the images are small. Batch size is 8 (GPUs * images\/GPU).\n    GPU_COUNT = 1\n    IMAGES_PER_GPU = 8\n    \n    BACKBONE = 'resnet50'\n    \n    NUM_CLASSES = 2  # background + 1 pneumonia classes\n    \n    IMAGE_MIN_DIM = 256\n    IMAGE_MAX_DIM = 256\n    RPN_ANCHOR_SCALES = (32, 64, 128, 256)\n    TRAIN_ROIS_PER_IMAGE = 32\n    MAX_GT_INSTANCES = 3\n    DETECTION_MAX_INSTANCES = 3\n    DETECTION_MIN_CONFIDENCE = 0.7\n    DETECTION_NMS_THRESHOLD = 0.1\n\n    STEPS_PER_EPOCH = 200\n    \nconfig = DetectorConfig()\nconfig.display()","0cc52521":"class DetectorDataset(utils.Dataset):\n    \"\"\"Dataset class for training pneumonia detection on the RSNA pneumonia dataset.\n    \"\"\"\n\n    def __init__(self, image_fps, image_annotations, orig_height, orig_width):\n        super().__init__(self)\n        \n        # Add classes\n        self.add_class('pneumonia', 1, 'Lung Opacity')\n        \n        # add images \n        for i, fp in enumerate(image_fps):\n            annotations = image_annotations[fp]\n            self.add_image('pneumonia', image_id=i, path=fp, \n                           annotations=annotations, orig_height=orig_height, orig_width=orig_width)\n            \n    def image_reference(self, image_id):\n        info = self.image_info[image_id]\n        return info['path']\n\n    def load_image(self, image_id):\n        info = self.image_info[image_id]\n        fp = info['path']\n        ds = pydicom.read_file(fp)\n        image = ds.pixel_array\n        # If grayscale. Convert to RGB for consistency.\n        if len(image.shape) != 3 or image.shape[2] != 3:\n            image = np.stack((image,) * 3, -1)\n        return image\n\n    def load_mask(self, image_id):\n        info = self.image_info[image_id]\n        annotations = info['annotations']\n        count = len(annotations)\n        if count == 0:\n            mask = np.zeros((info['orig_height'], info['orig_width'], 1), dtype=np.uint8)\n            class_ids = np.zeros((1,), dtype=np.int32)\n        else:\n            mask = np.zeros((info['orig_height'], info['orig_width'], count), dtype=np.uint8)\n            class_ids = np.zeros((count,), dtype=np.int32)\n            for i, a in enumerate(annotations):\n                if a['Target'] == 1:\n                    x = int(a['x'])\n                    y = int(a['y'])\n                    w = int(a['width'])\n                    h = int(a['height'])\n                    mask_instance = mask[:, :, i].copy()\n                    cv2.rectangle(mask_instance, (x, y), (x+w, y+h), 255, -1)\n                    mask[:, :, i] = mask_instance\n                    class_ids[i] = 1\n        return mask.astype(np.bool), class_ids.astype(np.int32)","7fff540e":"# training dataset\nanns = pd.read_csv(os.path.join(DATA_DIR, 'stage_1_train_labels.csv'))\nanns.head()","5cfe9d86":"image_fps, image_annotations = parse_dataset(train_dicom_dir, anns=anns)","e6b77b7b":"ds = pydicom.read_file(image_fps[0]) # read dicom image from filepath \nimage = ds.pixel_array # get image array","84c61bd1":"# show dicom fields \nds","4eacc675":"# Original DICOM image size: 1024 x 1024\nORIG_SIZE = 1024","1a8b171a":"# split dataset into training vs. validation dataset \nimage_fps_list = list(image_fps)\nrandom.seed(42)\nrandom.shuffle(image_fps_list)\n\nval_size = 1500\nimage_fps_val = image_fps_list[:val_size]\nimage_fps_train = image_fps_list[val_size:]\n\nprint(len(image_fps_train), len(image_fps_val))","348d7b9b":"# prepare the training dataset\ndataset_train = DetectorDataset(image_fps_train, image_annotations, ORIG_SIZE, ORIG_SIZE)\ndataset_train.prepare()","71566d60":"# Show annotation(s) for a DICOM image \ntest_fp = random.choice(image_fps_train)\nimage_annotations[test_fp]","d5dd5bd9":"# prepare the validation dataset\ndataset_val = DetectorDataset(image_fps_val, image_annotations, ORIG_SIZE, ORIG_SIZE)\ndataset_val.prepare()","b3225db5":"# Load and display random sample and their bounding boxes\n\nclass_ids = [0]\nwhile class_ids[0] == 0:  ## look for a mask\n    image_id = random.choice(dataset_train.image_ids)\n    image_fp = dataset_train.image_reference(image_id)\n    image = dataset_train.load_image(image_id)\n    mask, class_ids = dataset_train.load_mask(image_id)\n\nprint(image.shape)\n\nplt.figure(figsize=(10, 10))\nplt.subplot(1, 2, 1)\nplt.imshow(image)\nplt.axis('off')\n\nplt.subplot(1, 2, 2)\nmasked = np.zeros(image.shape[:2])\nfor i in range(mask.shape[2]):\n    masked += image[:, :, 0] * mask[:, :, i]\nplt.imshow(masked, cmap='gray')\nplt.axis('off')\n\nprint(image_fp)\nprint(class_ids)","60977809":"# Image augmentation (light but constant)\naugmentation = iaa.Sequential([\n    iaa.OneOf([ ## geometric transform\n        iaa.Affine(\n            scale={\"x\": (0.98, 1.04), \"y\": (0.98, 1.04)},\n            translate_percent={\"x\": (-0.03, 0.03), \"y\": (-0.05, 0.05)},\n            rotate=(-5, 5),\n            shear=(-3, 3),\n        ),\n        iaa.PiecewiseAffine(scale=(0.002, 0.03)),\n    ]),\n    iaa.OneOf([ ## brightness or contrast\n        iaa.Multiply((0.85, 1.15)),\n        iaa.ContrastNormalization((0.85, 1.15)),\n    ]),\n    iaa.OneOf([ ## blur or sharpen\n        iaa.GaussianBlur(sigma=(0.0, 0.12)),\n        iaa.Sharpen(alpha=(0.0, 0.12)),\n    ]),\n])\n\n# test on the same image as above\nimggrid = augmentation.draw_grid(image[:, :, 0], cols=5, rows=2)\nplt.figure(figsize=(30, 12))\n_ = plt.imshow(imggrid[:, :, 0], cmap='gray')","84f9ee0c":"model = modellib.MaskRCNN(mode='training', config=config, model_dir=ROOT_DIR)","f908a6e8":"NUM_EPOCHS = 16\nLEARNING_RATE = 0.006\n\n# Train Mask-RCNN Model \nimport warnings \nwarnings.filterwarnings(\"ignore\")","36e94927":"%%time\n## first epochs with higher lr to speedup the learning\nmodel.train(dataset_train, dataset_val,\n            learning_rate=LEARNING_RATE*2,\n            epochs=2,\n            layers='all',\n            augmentation=None)  ## no need to augment yet","ff474cad":"%%time\nmodel.train(dataset_train, dataset_val,\n            learning_rate=LEARNING_RATE,\n            epochs=NUM_EPOCHS,\n            layers='all',\n            augmentation=augmentation)","52949943":"# select trained model \ndir_names = next(os.walk(model.model_dir))[1]\nkey = config.NAME.lower()\ndir_names = filter(lambda f: f.startswith(key), dir_names)\ndir_names = sorted(dir_names)\n\nif not dir_names:\n    import errno\n    raise FileNotFoundError(\n        errno.ENOENT,\n        \"Could not find model directory under {}\".format(self.model_dir))\n    \nfps = []\n# Pick last directory\nfor d in dir_names: \n    dir_name = os.path.join(model.model_dir, d)\n    # Find the last checkpoint\n    checkpoints = next(os.walk(dir_name))[2]\n    checkpoints = filter(lambda f: f.startswith(\"mask_rcnn\"), checkpoints)\n    checkpoints = sorted(checkpoints)\n    if not checkpoints:\n        print('No weight files in {}'.format(dir_name))\n    else:\n        checkpoint = os.path.join(dir_name, checkpoints[-1])\n        fps.append(checkpoint)\n\nmodel_path = sorted(fps)[-1]\nprint('Found model {}'.format(model_path))","8ef7c535":"class InferenceConfig(DetectorConfig):\n    GPU_COUNT = 1\n    IMAGES_PER_GPU = 1\n\ninference_config = InferenceConfig()\n\n# Recreate the model in inference mode\nmodel = modellib.MaskRCNN(mode='inference', \n                          config=inference_config,\n                          model_dir=ROOT_DIR)\n\n# Load trained weights (fill in path to trained weights here)\nassert model_path != \"\", \"Provide path to trained weights\"\nprint(\"Loading weights from \", model_path)\nmodel.load_weights(model_path, by_name=True)","3da5dc1b":"# set color for class\ndef get_colors_for_class_ids(class_ids):\n    colors = []\n    for class_id in class_ids:\n        if class_id == 1:\n            colors.append((.941, .204, .204))\n    return colors","fd1f7b06":"# Show few example of ground truth vs. predictions on the validation dataset \ndataset = dataset_val\nfig = plt.figure(figsize=(10, 30))\n\nfor i in range(6):\n    image_id = random.choice(dataset.image_ids)\n    \n    original_image, image_meta, gt_class_id, gt_bbox, gt_mask = \\\n        modellib.load_image_gt(dataset_val, inference_config, \n                               image_id, use_mini_mask=False)\n    \n    print(original_image.shape)\n    plt.subplot(6, 2, 2*i + 1)\n    visualize.display_instances(original_image, gt_bbox, gt_mask, gt_class_id, \n                                dataset.class_names,\n                                colors=get_colors_for_class_ids(gt_class_id), ax=fig.axes[-1])\n    \n    plt.subplot(6, 2, 2*i + 2)\n    results = model.detect([original_image]) #, verbose=1)\n    r = results[0]\n    visualize.display_instances(original_image, r['rois'], r['masks'], r['class_ids'], \n                                dataset.class_names, r['scores'], \n                                colors=get_colors_for_class_ids(r['class_ids']), ax=fig.axes[-1])\n    ","ae0af358":"# Get filenames of test dataset DICOM images\ntest_image_fps = get_dicom_fps(test_dicom_dir)","7fd60c82":"# Make predictions on test images, write out sample submission\ndef predict(image_fps, filepath='submission.csv', min_conf=0.98):\n    # assume square image\n    resize_factor = ORIG_SIZE \/ config.IMAGE_SHAPE[0]\n    #resize_factor = ORIG_SIZE\n    with open(filepath, 'w') as file:\n        file.write(\"patientId,PredictionString\\n\")\n\n        for image_id in tqdm(image_fps):\n            ds = pydicom.read_file(image_id)\n            image = ds.pixel_array\n            # If grayscale. Convert to RGB for consistency.\n            if len(image.shape) != 3 or image.shape[2] != 3:\n                image = np.stack((image,) * 3, -1)\n            image, window, scale, padding, crop = utils.resize_image(\n                image,\n                min_dim=config.IMAGE_MIN_DIM,\n                min_scale=config.IMAGE_MIN_SCALE,\n                max_dim=config.IMAGE_MAX_DIM,\n                mode=config.IMAGE_RESIZE_MODE)\n\n            patient_id = os.path.splitext(os.path.basename(image_id))[0]\n\n            results = model.detect([image])\n            r = results[0]\n\n            out_str = \"\"\n            out_str += patient_id\n            out_str += \",\"\n            assert( len(r['rois']) == len(r['class_ids']) == len(r['scores']) )\n            if len(r['rois']) == 0:\n                pass\n            else:\n                num_instances = len(r['rois'])\n\n                for i in range(num_instances):\n                    if r['scores'][i] > min_conf:\n                        out_str += ' '\n                        out_str += str(round(r['scores'][i], 2))\n                        out_str += ' '\n\n                        # x1, y1, width, height\n                        x1 = r['rois'][i][1]\n                        y1 = r['rois'][i][0]\n                        width = r['rois'][i][3] - x1\n                        height = r['rois'][i][2] - y1\n                        bboxes_str = \"{} {} {} {}\".format(x1*resize_factor, y1*resize_factor, \\\n                                                           width*resize_factor, height*resize_factor)\n                        out_str += bboxes_str\n\n            file.write(out_str+\"\\n\")","68a9e061":"submission_fp = os.path.join(ROOT_DIR, 'submission.csv')\npredict(test_image_fps, filepath=submission_fp)\nprint(submission_fp)","a4a80007":"output = pd.read_csv(submission_fp, names=['patientId', 'PredictionString'])\noutput.head(60)","80de09da":"# show a few test image detection example\ndef visualize(): \n    image_id = random.choice(test_image_fps)\n    ds = pydicom.read_file(image_id)\n    \n    # original image \n    image = ds.pixel_array\n    \n    # assume square image \n    resize_factor = ORIG_SIZE \/ config.IMAGE_SHAPE[0]\n    \n    # If grayscale. Convert to RGB for consistency.\n    if len(image.shape) != 3 or image.shape[2] != 3:\n        image = np.stack((image,) * 3, -1) \n    resized_image, window, scale, padding, crop = utils.resize_image(\n        image,\n        min_dim=config.IMAGE_MIN_DIM,\n        min_scale=config.IMAGE_MIN_SCALE,\n        max_dim=config.IMAGE_MAX_DIM,\n        mode=config.IMAGE_RESIZE_MODE)\n\n    patient_id = os.path.splitext(os.path.basename(image_id))[0]\n    print(patient_id)\n\n    results = model.detect([resized_image])\n    r = results[0]\n    for bbox in r['rois']: \n        print(bbox)\n        x1 = int(bbox[1] * resize_factor)\n        y1 = int(bbox[0] * resize_factor)\n        x2 = int(bbox[3] * resize_factor)\n        y2 = int(bbox[2]  * resize_factor)\n        cv2.rectangle(image, (x1,y1), (x2,y2), (77, 255, 9), 3, 1)\n        width = x2 - x1 \n        height = y2 - y1 \n        print(\"x {} y {} h {} w {}\".format(x1, y1, width, height))\n    plt.figure() \n    plt.imshow(image, cmap=plt.cm.gist_gray)\n\nvisualize()\nvisualize()\nvisualize()\nvisualize()","9a908d1b":"# remove files to allow committing (hit files limit otherwise)\n!rm -rf \/kaggle\/working\/Mask_RCNN","6bbdcc52":"### Examine the annotation data, parse the dataset, and view dicom fields","3f58096a":"**Mask-RCNN Sample Starter Model for the RSNA Pneumonia Detection Challenge**\n\n[MD.ai](https:\/\/www.md.ai). The dataset for this challenge, created on the MD.ai platform in collaboration with the Radiological Society of North America (RSNA), the Society of Thoracic Radiology (STR), the US National Institutes of Health (NIH), and Kaggle.\nThis notebook covers the basics of parsing the competition dataset, training using a detector basd on the [Mask-RCNN algorithm](https:\/\/arxiv.org\/abs\/1703.06870) for object detection and instance segmentation.    \n**Note that the Mask-RCNN detector configuration parameters have been selected to reduce training time for demonstration purposes, they are not optimal.\n**\n\nThis is based on our deep learning for medical imaging lessons: \n\n- Lesson 1. Classification of chest vs. adominal X-rays using TensorFlow\/Keras [Github](https:\/\/github.com\/mdai\/ml-lessons\/blob\/master\/lesson1-xray-images-classification.ipynb) [Annotator](https:\/\/public.md.ai\/annotator\/project\/PVq9raBJ)\n- Lesson 2. Lung X-Rays Semantic Segmentation using UNets. [Github](https:\/\/github.com\/mdai\/ml-lessons\/blob\/master\/lesson2-lung-xrays-segmentation.ipynb)\n[Annotator](https:\/\/public.md.ai\/annotator\/project\/aGq4k6NW\/workspace) \n- Lesson 3. RSNA Pneumonia detection using Kaggle data format [Github](https:\/\/github.com\/mdai\/ml-lessons\/blob\/master\/lesson3-rsna-pneumonia-detection-kaggle.ipynb) [Annotator](https:\/\/public.md.ai\/annotator\/project\/LxR6zdR2\/workspace) \n- Lesson 3. RSNA Pneumonia detection using MD.ai python client library [Github](https:\/\/github.com\/mdai\/ml-lessons\/blob\/master\/lesson3-rsna-pneumonia-detection-mdai-client-lib.ipynb) [Annotator](https:\/\/public.md.ai\/annotator\/project\/LxR6zdR2\/workspace) \n\n*Copyright 2018 MD.ai, Inc.   \nLicensed under the Apache License, Version 2.0*","994935ab":"### Now it's time to train the model. Note that training even a basic model can take a few hours. \n\nNote: the following model is for demonstration purpose only. We have limited the training to one epoch, and have set nominal values for the Detector Configuration to reduce run-time. \n\n- dataset_train and dataset_val are derived from DetectorDataset \n- DetectorDataset loads images from image filenames and  masks from the annotation data\n- model is Mask-RCNN","b75854a3":"### Display a random image with bounding boxes","44d75558":"### Final steps - Create the submission file","6f2008b5":"### First: Install Kaggle API for download competition data.","0034cfdb":"### Install Matterport's Mask-RCNN model from github.\nSee the [Matterport's implementation of Mask-RCNN](https:\/\/github.com\/matterport\/Mask_RCNN).","d1b2f419":"### Let's look at a sample annotation. We see a bounding box with (x, y) of the the top left corner as well as the width and height.","a598672f":"### Some setup functions and classes for Mask-RCNN\n\n- dicom_fps is a list of the dicom image path and filenames \n- image_annotions is a dictionary of the annotations keyed by the filenames\n- parsing the dataset returns a list of the image filenames and the annotations dictionary","aab4143a":"### Split the data into training and validation datasets","b89a56d8":"### How does the predicted box compared to the expected value? Let's use the validation dataset to check. \n\nNote that we trained only one epoch for **demonstration purposes ONLY**. You might be able to improve performance running more epochs. ","a01bf9a0":"### Create and prepare the training dataset using the DetectorDataset class.","ff70ff0e":"### Image Augmentation. Try finetuning some variables to custom values","e9c524d8":"###  MD.ai Annotator \n\nAdditionally, If you are interested in augmenting the existing annotations, you can use the MD.ai annotator to view DICOM images, and create annotatios to be exported.  \nMD.ai annotator project URL for the Kaggle dataset: https:\/\/public.md.ai\/annotator\/project\/LxR6zdR2\/workspace\n\n**Annotator features**\n- The annotator can be used to view DICOM images and create image and exam level annotations.\n- You can apply the annotator to filter by label, adjudicate annotations, and assign annotation tasks to your team.\n- Notebooks can be built directly within the annotator for rapid model development.\n- The data wrangling is abstracted away by the interface and by our MD.ai library.\n- Simplifies image annotation in order to widen the participation in the futrue of medical image deep learning.\n\nThe annotator allows you to create initial annotations, build and run models, modify\/finetune the annotations based on predicted values, and repeat.  \nThe MD.ai python client library implements functions to easily download images and annotations and to prepare the datasets used to train the model for classification. See the following example notebook for parsing annotations and training using MD.ai annotator: \nhttps:\/\/github.com\/mdai\/ml-lessons\/blob\/master\/lesson3-rsna-pneumonia-detection-mdai-client-lib.ipynb  \n- MD.ai URL: https:\/\/www.md.ai  \n- MD.ai documentation URL: https:\/\/docs.md.ai\/"}}