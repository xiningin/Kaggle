{"cell_type":{"6ddc3991":"code","cad623dc":"code","619bc228":"code","01f428a5":"code","f7cdcf28":"code","c3ee42ba":"code","bea31855":"code","8b19b554":"code","22db9f06":"markdown","151b3aaa":"markdown","d3d58e92":"markdown","48b29f87":"markdown","fd1dd4df":"markdown"},"source":{"6ddc3991":"##################################################\n# Imports\n##################################################\n\nimport numpy as np\nimport cv2\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport IPython.display as ipd\n\n\n##################################################\n# Params\n##################################################\n\nDATA_BASE_FOLDER = '\/kaggle\/input\/ml-project-speech-recognition-challenge'\nSAMPLE_RATE = 16000\nHOP_LEN = 512","cad623dc":"##################################################\n# Load dataset\n##################################################\n\n# Load annotations\ndf_train = pd.read_csv(os.path.join(DATA_BASE_FOLDER, 'train.csv'))\ndf_validation = pd.read_csv(os.path.join(DATA_BASE_FOLDER, 'validation.csv'))\nlabels = sorted(list(set(df_train['word'].values)))\ny_train = df_train['word'].map(lambda w: labels.index(w)).values\ny_validation = df_validation['word'].map(lambda w: labels.index(w)).values\n\n# Load audio\naudio_train = np.load(os.path.join(DATA_BASE_FOLDER, 'train_audio.npy'))","619bc228":"# Load Features\nx_train_raw = np.load(os.path.join(DATA_BASE_FOLDER, 'train_feat.npy'))\nx_validation_raw = np.load(os.path.join(DATA_BASE_FOLDER, 'validation_feat.npy'))\n\n# Plot audio feature\nidx = 1205\ntime = np.arange(1, SAMPLE_RATE + 1, HOP_LEN) \/ SAMPLE_RATE\nplt.figure(figsize=(10, 5))\nplt.title(f'Mel-Spectrogram of audio: {df_train[\"word\"][idx]}', fontweight='bold')\nplt.imshow(x_train_raw[idx], aspect='auto', origin='low', cmap='inferno')\nxticks = plt.xticks()[0].astype(np.int32)\nplt.xticks(xticks[1:-1], [f'{1000 * t:.0f}' for t in time[xticks[1:-1]]])\nplt.xlabel('Time [ms]', fontweight='bold')\nplt.ylabel('Log Mel-Spectogram', fontweight='bold')\nplt.grid(lw=0.4, c='w', alpha=0.4)\nplt.show()\n\n# Play audio\nipd.Audio(audio_train[idx], rate=SAMPLE_RATE)","01f428a5":"# Resize the features\nx_train = []\nfor x_i in x_train_raw:\n    x_train += [cv2.resize(x_i, (32, 32))]\nx_train = np.array(x_train)\nx_validation = []\nfor x_i in x_validation_raw:\n    x_validation += [cv2.resize(x_i, (32, 32))]\nx_validation = np.array(x_validation)\n\n# Plot audio feature\nidx = 1205\nplt.figure(figsize=(5, 3))\nplt.title(f'Mel-Spectrogram of audio: {df_train[\"word\"][idx]}', fontweight='bold')\nplt.imshow(x_train[idx], aspect='auto', origin='low', cmap='inferno')\nplt.grid(lw=0.4, c='w', alpha=0.4)\nplt.show()\n\n# Play audio\nipd.Audio(audio_train[idx], rate=SAMPLE_RATE)","f7cdcf28":"# Flatten the features\nx_train = x_train.reshape(x_train.shape[0], -1)\nx_validation = x_validation.reshape(x_validation.shape[0], -1)\n\nprint(f'Features dimension size: {x_train.shape[-1]}')","c3ee42ba":"##################################################\n# Implement you model here\n##################################################\n\n\n\n\n\n\n\n\n\n","bea31855":"##################################################\n# Evaluate the model here\n##################################################\n\n# Use this function to evaluate your model\ndef accuracy(y_pred, y_true):\n    '''\n    input y_pred: ndarray of shape (N,)\n    input y_true: ndarray of shape (N,)\n    '''\n    return (1.0 * (y_pred == y_true)).mean()\n\n# Report the accuracy in the train and validation sets.\n\n\n\n\n\n\n\n","8b19b554":"##################################################\n# Save your test prediction in y_test_pred\n##################################################\n\ny_test_pred = None\n\n# Create submission\nsubmission = pd.read_csv(os.path.join(DATA_BASE_FOLDER, 'sample_submission.csv'))\nif y_test_pred is not None:\n    submission['class'] = [labels[int(y_i)] for y_i in y_test_pred]\nsubmission.to_csv('my_submission.csv', index=False)","22db9f06":"# Evaluation","151b3aaa":"# Send the submission for the challenge","d3d58e92":"# Model\n\nHere you have to implement a model (or more models, for finding the most accurate) for classification.\n\nYou can use the [`sklearn`](https:\/\/scikit-learn.org\/stable\/) (or optionally other more advanced frameworks such as [`pytorch`](https:\/\/pytorch.org\/) or [`tensorflow`](https:\/\/www.tensorflow.org\/)) package that contains a pool of models already implemented that perform classification. (SVMs, NNs, LR, kNN, ...)","48b29f87":"# Dataset\n\nThe dataset is a reduced version of the [`TensorFlow Speech Commands Dataset`](https:\/\/www.tensorflow.org\/datasets\/catalog\/speech_commands) and contains audio waveforms of the words:\n- `down`, \n- `go`, \n- `left`, \n- `off`, \n- `on`, \n- `right`, \n- `stop`, \n- `up`.\n\n\nTrain \/ Validation Split\n- 1600 train samples, 200 for each class\n- 109 validation samples","fd1dd4df":"# Feature Extraction\n\nThe speech is a time series signal and a well known strategy for extracting a good representation of the raw audio is to mimic the processing of the auditory system of the humans. A well established feature representation for speech is the so called \"log mel-spectrum\". This feature in fact, takes into account how humans perceive both the frequencies and the amplitude of the sound logarithmically. If you want to dig more into this topic [here](https:\/\/medium.com\/@jonathan_hui\/speech-recognition-feature-extraction-mfcc-plp-5455f5a69dd9) you can find some details. \n\n![auditory-system](https:\/\/www.researchgate.net\/profile\/Morteza_Khaleghi_Meybodi\/publication\/322343133\/figure\/fig1\/AS:581011472093184@1515535337239\/Figure-31-Schematic-of-the-auditory-system-with-its-primary-components-including.png)\n\n\nFor this project these features are precomputed: for each audio waveform of 1 sec duration, the log mel-spectrum is a bi-dimensional representation (frequency vs time) of shape [128, 32]. Here, we first resize the \"image\" into a [32, 32] matrix and then we flatten the representation into a 32x32 = 1024 vector."}}