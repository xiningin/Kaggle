{"cell_type":{"01a51b07":"code","3e507791":"code","98a2c74c":"code","1cb558e7":"code","e0f27a6d":"code","9ec6810f":"code","93646024":"code","5b80c156":"code","78e01e68":"code","8d958ef1":"code","05c9c079":"code","3059af29":"code","621accfa":"code","70300505":"code","75db2cb9":"code","c79b3d73":"code","b095e571":"code","23092f12":"code","c872b1b5":"code","5c18f1e6":"code","7efc2fb6":"code","46c749a3":"code","ca39f356":"code","a8af675b":"code","a26aa91e":"markdown","c0ef530e":"markdown","953164b9":"markdown","2396aacf":"markdown","95281951":"markdown","f6229a60":"markdown","eb78542e":"markdown","128e398d":"markdown","049098ca":"markdown","838bb41c":"markdown"},"source":{"01a51b07":"%matplotlib inline\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader\nfrom torchvision import transforms, datasets\nimport torch.optim as optim\n\nfrom PIL import Image\nimport matplotlib.pyplot as plt\n\nimport numpy as np\n\nuse_cuda = torch.cuda.is_available()","3e507791":"im_normal = Image.open('..\/input\/chest-xray-pneumonia\/chest_xray\/train\/NORMAL\/IM-0651-0001.jpeg')\nim_p1 = Image.open('..\/input\/chest-xray-pneumonia\/chest_xray\/train\/PNEUMONIA\/person1139_bacteria_3082.jpeg')\nim_p2 = Image.open('..\/input\/chest-xray-pneumonia\/chest_xray\/train\/PNEUMONIA\/person1139_virus_1882.jpeg')\n\nplt.subplot(1, 3, 1)\nplt.imshow(im_normal)\nplt.title('Normal chest xray')","98a2c74c":"train_transform = transforms.Compose([transforms.Resize((224, 224)),\n                                      transforms.RandomRotation(20),\n                                      #transforms.RandomHorizontalFlip(),\n                                      transforms.CenterCrop(224),\n                                      transforms.ToTensor(),\n                                      transforms.Normalize((.5,.5,.5),\n                                                           (.5,.5,.5))])\n\ntest_transform = transforms.Compose([transforms.Resize((224, 224)),\n                                    transforms.CenterCrop(224),\n                                    transforms.ToTensor(),\n                                    transforms.Normalize((.5,.5,.5),\n                                                         (.5,.5,.5))])","1cb558e7":"# datasets\ndata_dir = '..\/input\/chest-xray-pneumonia\/chest_xray\/'\n\n# train_data = datasets.ImageFolder(data_dir + 'train', train_transform)\ntest_data = datasets.ImageFolder(data_dir + 'test', test_transform)\nvalid_data = datasets.ImageFolder(data_dir + 'val', test_transform)","e0f27a6d":"dataset_train = datasets.ImageFolder(data_dir + 'train', train_transform)                                                                         ","9ec6810f":"#Create a list of image's labels\nlabels = [label[1] for label in dataset_train.imgs]","93646024":"sum(labels)","5b80c156":"import pandas as pd\n\n#Count number of items in labels\nfrom collections import Counter\nimport seaborn as sns\ncount=Counter(labels)\n#Array that has the length of  the 2 labes (0 , 1)\nclass_count=np.array([count[0],count[1]])\n#The summation of items in each class\ntotal = sum(class_count)\n#Which class has less items \nmin_ = min(count[0], count[1])\n#which class has more items\nmax_ = max(count[0], count[1])\n#Allocating weight for each class\nif count[0] < count[1]:\n    weight = [((total\/2)\/min_)\/total, (max_\/(total\/2))\/total]\n    print(weight)\nelse:\n    weight = [(max_\/(total\/2))\/total, ((total\/2)\/min_)\/total]\n    print(weight)\n# if elements in each class is [40,60]\n# The weight would be calculated as the following\n# 40*(((100\/2)\/40)\/100) + 60*(((100\/2)\/60)\/100)\n# where each side before and after 40 should be equal 50%\n# to summarize it, the class which has less elements would get higher probability to be choosen in the batch\n\n#Plotting each class to show imbalance\nsns.countplot(class_count)","78e01e68":"#List that has all the weights for each item in both classes\nsamples_weight = np.array([weight[t] for t in labels])\nsamples_weight=torch.from_numpy(samples_weight)\nlen(samples_weight)","8d958ef1":"#Choosing images from the  dataset based on their weights\nsampler = torch.utils.data.WeightedRandomSampler(samples_weight, len(samples_weight))\nsampler","05c9c079":"#DataLoader with batch size 10\ntrain_loader = torch.utils.data.DataLoader(dataset_train, batch_size=10, sampler = sampler, num_workers=2, pin_memory=True)  \n# train_loader = DataLoader(train_data, batch_size=64, num_workers=2, shuffle=True, pin_memory=True)\ntest_loader = DataLoader(test_data, batch_size=64, num_workers=2, shuffle=True, pin_memory=True)\nvalid_loader = DataLoader(valid_data, batch_size=64, num_workers=2, shuffle=False, pin_memory=True)","3059af29":"class XRayNet(nn.Module):\n    def __init__(self):\n        super(XRayNet, self).__init__()\n        \n        # input image size 3, 224, 224\n        self.conv1 = nn.Conv2d(3, 32, 3, stride = 1, padding = 1)\n        self.batch1 = nn.BatchNorm2d(32, affine=True, track_running_stats=True)\n        \n        # image size 8, 112, 112\n        self.conv2 = nn.Conv2d(32, 56, 3, stride = 1, padding = 1)\n        self.batch2 = nn.BatchNorm2d(56, affine=True, track_running_stats=True)\n        \n        # image size 16, 56, 56\n        self.conv3 = nn.Conv2d(56, 64, 3, stride = 1, padding = 1)\n        self.batch3 = nn.BatchNorm2d(64, affine=True, track_running_stats=True)\n        \n        self.conv4 = nn.Conv2d(64, 64, 3, stride = 1, padding = 1)\n        self.batch4 = nn.BatchNorm2d(64, affine=True, track_running_stats=True)\n        \n        self.pool = nn.MaxPool2d(2, 2)\n        \n        # input size 32, 28, 28\n        self.fc1 = nn.Linear(64 * 14 * 14, 4096)\n        self.fc2 = nn.Linear(4096, 512)\n        self.fc3 = nn.Linear(512, 64)\n        self.fc4 = nn.Linear(64, 2)\n        \n        self.batch5 = nn.BatchNorm1d(512)\n        self.batch6 = nn.BatchNorm1d(64)\n        \n        self.drop = nn.Dropout(p=.3)\n        \n    def forward(self, x):\n        \n        x = F.relu(self.conv1(x), inplace=True)\n        x = self.pool(self.batch1(x))\n        \n        x = F.relu(self.conv2(x), inplace=True)\n        x = self.pool(self.batch2(x))\n        x = self.drop(x)\n        \n        x = F.relu(self.conv3(x), inplace=True)\n        x = self.pool(self.batch3(x))\n        x = self.drop(x)\n        \n        x = F.relu(self.conv4(x), inplace=True)\n        x = self.pool(self.batch4(x))\n        \n        x = x.view(-1, 64 * 14 * 14)\n        \n        x = F.relu(self.fc1(x))\n        x = self.drop(x)\n        x = F.relu(self.fc2(x))\n        x = self.batch5(x)\n        x = self.drop(x)\n        x = F.relu(self.fc3(x))\n        x = self.batch6(x)\n        x = self.drop(x)\n        x = self.fc4(x)\n        \n        return x","621accfa":"XRayModel = XRayNet()\n\nif use_cuda:\n    XRayModel.cuda()\n\nXRayModel","70300505":"criterion = nn.CrossEntropyLoss()\n\noptimizer = optim.Adam(XRayModel.parameters(), lr=0.001)","75db2cb9":"n_epoch = 30\nmin_val_loss = np.Inf\n\ntrain_losses = []\nval_losses = []\n\nfor e in range(n_epoch):\n    \n    running_loss = 0\n    val_loss = 0\n    \n    # train mode\n    for images, labels in train_loader:\n        if use_cuda:\n            images, labels = images.cuda(), labels.cuda()\n            \n        # zero grad\n        optimizer.zero_grad()\n        \n        output = XRayModel(images)\n        \n        loss = criterion(output, labels)\n        \n        running_loss += loss.item() * images.size(0)\n        \n        loss.backward()\n        \n        optimizer.step()\n        \n    # valid mode\n    for images, labels in valid_loader:\n        if use_cuda:\n            images, labels = images.cuda(), labels.cuda()\n            \n        XRayModel.eval()\n        \n        output = XRayModel(images)\n        \n        loss = criterion(output, labels)\n        \n        val_loss += loss.item() * images.size(0)\n        \n    XRayModel.train()\n    \n    epoch_train_loss = running_loss \/ len(train_loader.dataset)\n    epoch_val_loss = val_loss \/ len(valid_loader.dataset)\n    print('Epoch {}, train loss : {}, validation loss :{}'.format(e, epoch_train_loss, epoch_val_loss))\n    \n    train_losses.append(epoch_train_loss)\n    val_losses.append(epoch_val_loss)\n    \n    if epoch_val_loss <= min_val_loss:\n        print('Validation loss decreased {} -> {}. Saving model...'.format(min_val_loss, epoch_val_loss))\n        min_val_loss = epoch_val_loss\n        torch.save(XRayModel.state_dict(), 'ashish_best.pth')","c79b3d73":"XRayModel1 = XRayNet()\nXRayModel1.load_state_dict(torch.load('ashish_best.pth'))","b095e571":"if use_cuda:\n    XRayModel1.cuda()","23092f12":"def imshow(img):\n    img = img \/ 2 + 0.5     # unnormalize\n    npimg = img.numpy()\n    plt.figure(figsize=(20, 15))\n    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n    plt.show()","c872b1b5":"import torchvision\ndataiter = iter(test_loader)\nimages, labels = dataiter.next()\n\n# print images\nimshow(torchvision.utils.make_grid(images[:24]))\nprint('GroundTruth: ', ' '.join('%5s' % labels[j] for j in range(24)))","5c18f1e6":"from ignite.metrics import Accuracy, Precision, Recall\n\ndef thresholded_output_transform(output):\n    y_pred, y = output\n    y_pred = torch.round(y_pred)\n    return y_pred, y\n\nbinary_accuracy = Accuracy(thresholded_output_transform)\nprecision = Precision(thresholded_output_transform)\nrecall = Recall(thresholded_output_transform)","7efc2fb6":"def CalculateMetrics(XRayModel1):\n    y_true, y_pred = [], []\n    with torch.no_grad():\n        for images, labels in test_loader:\n            if use_cuda:\n                images, labels = images.cuda(), labels.cuda()\n            outputs = XRayModel1(images)\n            _, predicted = torch.max(outputs.data, 1)\n            y_true.extend(labels.detach().cpu().numpy())\n            y_pred.extend(predicted.detach().cpu().numpy())\n            binary_accuracy.update((predicted, labels))\n            precision.update((predicted, labels))\n            recall.update((predicted, labels))\n            \n        print('Model accuracy : ', binary_accuracy.compute())\n        print('Model Precision : ', precision.compute().item())\n        print('Model Recall : ', recall.compute().item())\n        return y_true, y_pred","46c749a3":"y_true, y_pred = CalculateMetrics(XRayModel1)","ca39f356":"from sklearn.metrics import confusion_matrix\n\ncm= confusion_matrix(y_true, y_pred )","a8af675b":"from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=np.array([0, 1]),)\nfig, ax = plt.subplots(figsize=(10,10))\ndisp.plot(ax=ax)\n","a26aa91e":"#### Loss and optimizer","c0ef530e":"**Confusion Matrix**","953164b9":"**Evaluation**","2396aacf":"#### Data loaders","95281951":"#### datasets","f6229a60":"**Dealing with imbalance**","eb78542e":"## Model","128e398d":"Testing the model","049098ca":"#### Train model","838bb41c":"**As we can see number of images in one class is higher than the other that is why we used random sampler to deal with this imbalace**"}}