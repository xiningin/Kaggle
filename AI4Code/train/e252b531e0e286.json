{"cell_type":{"5c602385":"code","024e74b3":"code","8dec7916":"code","5f363ede":"code","15f78f38":"code","eb22fc75":"code","a5b9e4b1":"code","9bc226b3":"code","748f6891":"markdown"},"source":{"5c602385":"%matplotlib inline\nimport pandas as pd\nimport numpy as np\nfrom dateutil.relativedelta import relativedelta\n\nimport torch\nfrom torch import nn, optim\nfrom torch.utils.data import TensorDataset, DataLoader\n\nfrom livelossplot import PlotLosses","024e74b3":"def add_datetime_features(df, drop_first=True,\n                          one_hot_features=[\"Month\", \"Dayofweek\", \"Hour\"],\n                          circular_encoding=[]):\n    features = [\"Year\", \"Week\", \"Day\", \"Dayofyear\", \"Month\", \"Dayofweek\",\n                # \"Is_year_end\", \"Is_year_start\", \"Is_month_end\", \"Is_month_start\",\n                \"Hour\", \"Minute\",]\n\n    datetime = pd.to_datetime(df.Date * (10 ** 9))\n    df['Datetime'] = datetime \n    \n    for feature in features:\n        new_column = getattr(datetime.dt, feature.lower())\n        if feature in one_hot_features:\n            df = pd.concat([df, pd.get_dummies(new_column, prefix=feature, drop_first=drop_first)], axis=1)\n        elif feature in circular_encoding:\n            diff = len(new_column.value_counts())\n            df[feature + \"_sin\"] = np.sin(2 * np.pi * new_column \/ diff)\n            df[feature + \"_cos\"] = np.cos(2 * np.pi * new_column \/ diff)\n        else:\n            df[feature] = new_column\n    return df","8dec7916":"df = pd.read_csv(\"..\/input\/train_electricity.csv\")\ndf = add_datetime_features(df,\n                           one_hot_features=[],\n                           circular_encoding=[\"Month\", \"Dayofweek\", \"Hour\"],\n                           drop_first=False)\n\nprint(df.columns)\n\neval_from = df['Datetime'].max() + relativedelta(months=-6)\ntrain_df = df[df['Datetime'] < eval_from]\nvalid_df = df[df['Datetime'] >= eval_from]\n\n\nlabel_col = \"Consumption_MW\"  # The target values are in this column\nto_drop = [label_col, \"Date\", \"Datetime\"] \n\ntrain_df_X = train_df.drop(to_drop, axis=1)\nvalid_df_X = valid_df.drop(to_drop, axis=1)\n\nmeans = train_df_X.mean()\nstds = train_df_X.std()\n\ndef normalize(dfx, means=means, stds=stds):\n    return (dfx - means) \/ stds\n\ntrain_df_X = normalize(train_df_X)\nvalid_df_X = normalize(valid_df_X)\n\ninput_channels = len(train_df_X.columns)\ninput_channels","5f363ede":"# let's use differences as the baseline\n(train_df.Consumption_MW - train_df.Production_MW).describe()","15f78f38":"batch_size = 1024\n\ntrain_X = torch.from_numpy(train_df_X.values.astype(np.float32))\ntrain_Y = torch.from_numpy((train_df.Consumption_MW - train_df.Production_MW).values.astype(np.float32)).unsqueeze(1)\n\nvalid_X = torch.from_numpy(valid_df_X.values.astype(np.float32))\nvalid_Y = torch.from_numpy((valid_df.Consumption_MW - valid_df.Production_MW).values.astype(np.float32)).unsqueeze(1)\n\ndataloaders = {\n    'train': DataLoader(\n        TensorDataset(train_X, train_Y),\n        batch_size=batch_size, shuffle=True, num_workers=4),\n    'validation': DataLoader(\n        TensorDataset(valid_X, valid_Y),\n        batch_size=batch_size, shuffle=False, num_workers=4)\n}","eb22fc75":"device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n\ndef train_model(model, optimizer, num_epochs=10):\n    liveloss = PlotLosses()\n    model = model.to(device)\n    criterion = nn.MSELoss()\n    \n    for epoch in range(num_epochs):\n        logs = {}\n        for phase in ['train', 'validation']:\n            if phase == 'train':\n                model.train()\n            else:\n                model.eval()\n\n            running_loss = 0.0\n\n            for inputs, labels in dataloaders[phase]:\n                inputs = inputs.to(device)\n                labels = labels.to(device)\n\n                outputs = model(inputs)\n                loss = criterion(outputs, labels)\n\n                if phase == 'train':\n                    optimizer.zero_grad()\n                    loss.backward()\n                    optimizer.step()\n\n                running_loss += loss.item() * inputs.size(0)\n\n            epoch_loss = running_loss \/ len(dataloaders[phase].dataset)\n            \n            prefix = ''\n            if phase == 'validation':\n                prefix = 'val_'\n\n            logs[prefix + 'rmse'] = np.sqrt(epoch_loss)\n        \n        liveloss.update(logs)\n        liveloss.draw()","a5b9e4b1":"class Nonlinear2(nn.Module):\n    def __init__(self, hidden_size):\n        super().__init__()\n        \n        self.fc = nn.Sequential(\n            nn.Linear(input_channels, hidden_size),\n            nn.ReLU(),\n            nn.Linear(hidden_size, hidden_size),\n            nn.ReLU(),\n            nn.Linear(hidden_size, 1)\n        )\n        \n    def forward(self, x):\n        return -430.489065 + 505.123157 * self.fc(x)","9bc226b3":"model = Nonlinear2(8)\noptimizer = optim.Adam(model.parameters(), lr=1e-4)\ntrain_model(model, optimizer, num_epochs=100)","748f6891":"A simple example of **livelossplot**: Live training loss plot in Jupyter Notebook for Keras, PyTorch and others.\n\n* Repo: https:\/\/github.com\/stared\/livelossplot\n* Install: `!pip install livelossplot` (or in a Kaggle kernel, Settings > Packages)"}}