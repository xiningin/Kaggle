{"cell_type":{"28b92c68":"code","81317ed5":"code","616058b4":"code","cf386cf9":"code","649eed80":"code","287acdcd":"code","d3f9c269":"code","d459b8f5":"markdown"},"source":{"28b92c68":"# import necessary libraries\nimport numpy as np\nimport gym\nfrom gym import spaces","81317ed5":"# custom 2d grid world enviroment which extends gym.Env\nclass TwoDGridWorld(gym.Env):\n    \"\"\"\n        - a size x size grid world which agent can ba at any cell other than terminal cell\n        - terminal cell is set to be the last cell or bottom right cell in the grid world\n        - 5x5 grid world example where X is the agent location and O is the tremial cell\n          .....\n          .....\n          ..X..\n          .....\n          ....O -> this is the terminal cell where this is agent headed to  \n        - Reference : https:\/\/github.com\/openai\/gym\/blob\/master\/gym\/core.py\n    \"\"\"\n    metadata = {'render.modes': ['console']}\n    \n    # actions available \n    UP   = 0\n    LEFT = 1\n    DOWN = 2\n    RIGHT= 3\n    \n    def __init__(self, size):\n        super(TwoDGridWorld, self).__init__()\n        \n        self.size      = size # size of the grid world\n        self.end_state = (size*size) - 1 # bottom right or last cell\n        \n        # randomly assign the inital location of agent\n        self.agent_position = np.random.randint( (self.size*self.size) - 1 )\n        \n        # respective actions of agents : up, down, left and right\n        self.action_space = spaces.Discrete(4)\n        \n        # set the observation space to (1,) to represent agent position in the grid world \n        # staring from [0,size*size)\n        self.observation_space = spaces.Box(low=0, high=size*size, shape=(1,), dtype=np.uint8)\n\n    def step(self,action):\n        info = {} # additional information\n        \n        reward = 0;\n        \n        row  = self.agent_position \/\/ self.size\n        col  = self.agent_position % self.size\n        if action == self.UP:\n            if row != 0:\n                self.agent_position -= self.size\n            else:\n                reward = 0\n        elif action == self.LEFT:\n            if col != 0:\n                self.agent_position -= 1\n            else:\n                reward = 0\n        elif action == self.DOWN:\n            if row != self.size - 1:\n                self.agent_position += self.size\n            else:\n                reward = 0\n        elif action == self.RIGHT:\n            if col != self.size - 1:\n                self.agent_position += 1\n            else:\n                reward = 0\n        else:\n            raise ValueError(\"Received invalid action={} which is not part of the action space\".format(action))\n        \n        \n        done   = bool(self.agent_position == self.end_state)\n        \n        # reward agent when it is in the terminal cell, else reward = 0\n        reward = 1 if done else reward \n        \n        return np.array([self.agent_position]).astype(np.uint8), reward, done, info\n    \n    def render(self, mode='console'):\n        '''\n            render the state\n        '''\n        if mode != 'console':\n          raise NotImplementedError()\n        \n        row  = self.agent_position \/\/ self.size\n        col  = self.agent_position % self.size\n        \n        for r in range(self.size):\n            for c in range(self.size):\n                if r == row and c == col:\n                    print(\"X\",end='')\n                else:\n                    print('.',end='')\n            print('')\n\n    def reset(self):\n        # -1 to ensure agent inital position will not be at the end state\n        self.agent_position = np.random.randint( (self.size*self.size) - 1 )\n        \n        return np.array([self.agent_position]).astype(np.uint8)\n    \n    def close(self):\n        pass","616058b4":"# This is to test if custom enviroment created properly\n# If the environment don't follow the gym interface, an error will be thrown\n\n!pip install 'tensorflow==1.15.0'\n\nfrom stable_baselines.common.env_checker import check_env\n\nenv = TwoDGridWorld(5)\ncheck_env(env, warn=True)\n\n# try draw the grid world\nobs = env.reset()\nenv.render()","cf386cf9":"# install stable-baselines\n!apt-get update\n!apt-get install -y cmake libopenmpi-dev python3-dev zlib1g-dev\n!pip install \"stable-baselines[mpi]==2.9.0\"","649eed80":"# import various RL algorithms\nfrom stable_baselines import DQN, PPO2, A2C, ACKTR \n\n# Instantiate the env\nGRID_SIZE  = 5\nenv   = TwoDGridWorld(GRID_SIZE)\n\n# Train the agent\nmodel = ACKTR('MlpPolicy', env, verbose=1).learn(100000)","287acdcd":"# running the simulation with trained model to verify result\n\nobs = env.reset()\nn_steps = 50\nfor step in range(n_steps):\n  action, _ = model.predict(obs, deterministic=True)\n  print(\"Step {}\".format(step + 1))\n  print(\"Action: \", action)\n  obs, reward, done, info = env.step(action)\n  print('obs=', obs, 'reward=', reward, 'done=', done)\n  env.render(mode='console')\n  if done:\n    # Note that the VecEnv resets automatically\n    # when a done signal is encountered\n    print(\"Goal reached!\", \"reward=\", reward)\n    break","d3f9c269":"# print the trained policy map\n# recall UP = 0, LEFT =1 , DOWN = 2, RIGHT = 3\nfor i in range(GRID_SIZE**2):\n    obs = [i]\n    action, _ = model.predict(obs, deterministic=True)\n    \n    if i % GRID_SIZE == 0 and i != 0:\n        print('') # newline \n    print(action,end=\"\")","d459b8f5":"In this notebook, I tried to create a simple 2D grid world with Openai Gym, and train an agent using stable-baselines to reach the goal state wherever he is at the 2D grid world. This should share some insight how you can create your custom enviroment and train your agent with many advanced RL algorithms (Thanks to stable-baselines). \n\n> Gym is a toolkit for developing and comparing reinforcement learning algorithms.\n"}}