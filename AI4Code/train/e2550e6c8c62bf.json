{"cell_type":{"b691d2fa":"code","2fa7f58c":"code","c0a6ce3d":"code","25b9c0b0":"code","baf8c981":"code","95da139c":"code","68922642":"code","6ab166f3":"code","673f5c0c":"code","28a345d8":"code","d48c3732":"code","c0d4cbea":"code","b083bc42":"code","cefe0494":"code","5bf3d325":"code","fbff807f":"code","111ee8b3":"code","fd8c24d4":"code","dbe9726e":"code","4aab948d":"code","54b3bda6":"code","482eb6f9":"code","0aed16ae":"code","216a6775":"code","6833b68b":"code","f99b63c7":"code","9212e379":"code","778ba4b9":"code","a9e0d46c":"markdown","2461a5c8":"markdown","848c18a2":"markdown","24599f48":"markdown","5cacc49e":"markdown","197d69e3":"markdown","914ad3d3":"markdown","170c6d2f":"markdown","f799b361":"markdown","5610d9fa":"markdown","1652aefc":"markdown","bde3f0cf":"markdown","39f33fa9":"markdown","5c2cf77b":"markdown","ddceac32":"markdown","26df03b4":"markdown","97496931":"markdown","2e2e0888":"markdown","110f99e4":"markdown","b3e7f74f":"markdown","9b7bbc54":"markdown","d92f3539":"markdown"},"source":{"b691d2fa":"import numpy as np\nimport pylab as pl\nimport pandas as pd\nimport matplotlib.pyplot as plt \n%matplotlib inline\nimport seaborn as sns\nfrom sklearn.utils import shuffle\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import confusion_matrix,classification_report\nfrom sklearn.model_selection import cross_val_score, GridSearchCV\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","2fa7f58c":"Data = pd.read_csv(\"..\/input\/udemy-courses\/udemy_courses.csv\")\n\n","c0a6ce3d":"Data.info()\nData[0:10]","25b9c0b0":"cnt_pro = Data['is_paid'].value_counts()\nplt.figure(figsize=(6,4))\nsns.barplot(cnt_pro.index, cnt_pro.values, alpha=0.8)\nplt.ylabel('Number of Data', fontsize=12)\nplt.xlabel('is_paid', fontsize=12)\nplt.xticks(rotation=80)\nplt.show();","baf8c981":"cnt_pro = Data['subject'].value_counts()\nplt.figure(figsize=(6,4))\nsns.barplot(cnt_pro.index, cnt_pro.values, alpha=0.8)\nplt.ylabel('Number of subject', fontsize=12)\nplt.xlabel('Subject', fontsize=12)\nplt.xticks(rotation=80)\nplt.show();","95da139c":"num_reviews= Data[Data['level']=='All Levels'].groupby(['is_paid']).agg({'num_reviews':['sum']})\nnum_lectures = Data[Data['level']=='All Levels'].groupby(['is_paid']).agg({'num_lectures':['sum']})\ntotal= num_reviews.join(num_lectures)\n\n\nplt.figure(figsize=(15,10))\nplt.subplot(2, 2, 1)\ntotal.plot(ax=plt.gca(), title='All Levels')","68922642":"num_reviews= Data[Data['level']=='Beginner Level'].groupby(['is_paid']).agg({'num_reviews':['sum']})\nnum_lectures = Data[Data['level']=='Beginner Level'].groupby(['is_paid']).agg({'num_lectures':['sum']})\ntotal= num_reviews.join(num_lectures)\n\n\nplt.figure(figsize=(15,10))\nplt.subplot(2, 2, 1)\ntotal.plot(ax=plt.gca(), title='Beginner Level')","6ab166f3":"num_reviews= Data[Data['level']=='Intermediate Level'].groupby(['is_paid']).agg({'num_reviews':['sum']})\nnum_lectures = Data[Data['level']=='Intermediate Level'].groupby(['is_paid']).agg({'num_lectures':['sum']})\ntotal= num_reviews.join(num_lectures)\n\n\nplt.figure(figsize=(15,10))\nplt.subplot(2, 2, 1)\ntotal.plot(ax=plt.gca(), title='Intermediate Level')","673f5c0c":"num_reviews= Data[Data['level']=='Expert Level'].groupby(['is_paid']).agg({'num_reviews':['sum']})\nnum_lectures = Data[Data['level']=='Expert Level'].groupby(['is_paid']).agg({'num_lectures':['sum']})\ntotal= num_reviews.join(num_lectures)\n\n\nplt.figure(figsize=(15,10))\nplt.subplot(2, 2, 1)\ntotal.plot(ax=plt.gca(), title='Expert Level')","28a345d8":"#Top 30 Most Popular Courses by num_subscribers\ntop_course = Data.sort_values(by='num_subscribers', ascending=False)[:30]\nfigure = plt.figure(figsize=(10,6))\nsns.barplot(y=top_course.course_title, x=top_course.num_subscribers)\nplt.xticks()\nplt.xlabel('num_subscribers')\nplt.ylabel('Course_title')\nplt.title('The Most Popular Courses')\nplt.show()","d48c3732":"#Top 30 Most Popular Courses Reviews by num_reviews\ntop_course = Data.sort_values(by='num_reviews', ascending=False)[:30]\nfigure = plt.figure(figsize=(10,6))\nsns.barplot(y=top_course.course_title, x=top_course.num_reviews)\nplt.xticks()\nplt.xlabel('num_reviews')\nplt.ylabel('Course_title')\nplt.title('The Most Popular Courses Reviews')\nplt.show()","c0d4cbea":"#Top 30 Num_lectures by num_lectures\ntop_course = Data.sort_values(by='num_lectures', ascending=False)[:30]\nfigure = plt.figure(figsize=(10,6))\nsns.barplot(y=top_course.course_title, x=top_course.num_lectures)\nplt.xticks()\nplt.xlabel('num_lectures')\nplt.ylabel('Course_title')\nplt.title('Num_lectures Courses')\nplt.show()","b083bc42":"Data1= Data[['is_paid','price','num_subscribers','num_reviews','num_lectures','content_duration','subject']] #Subsetting the data\ncor = Data.corr() #Calculate the correlation of the above variables\nsns.heatmap(cor, square = True) #Plot the correlation as heat map","cefe0494":"Data1.head()","5bf3d325":"#Frequency distribution of classes\"\ntrain_outcome = pd.crosstab(index=Data[\"is_paid\"],  # Make a crosstab\n                              columns=\"count\")      # Name the count column\n\ntrain_outcome","fbff807f":"#Select feature column names and target variable we are going to use for training\nsubject = {'Web Development': 1 ,'Business Finance': 2, 'Musical Instruments': 3, 'Graphic Design': 4} \nData1.subject = [subject[item] for item in Data1.subject] \nprint(Data1)","111ee8b3":"print(\"Any missing sample in test set:\",Data1.isnull().values.any(), \"\\n\")","fd8c24d4":"from sklearn.model_selection import train_test_split\nY = Data1['is_paid']\nX = Data1.drop(columns=['is_paid'])\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=9)","dbe9726e":"print('X train shape: ', X_train.shape)\nprint('Y train shape: ', Y_train.shape)\nprint('X test shape: ', X_test.shape)\nprint('Y test shape: ', Y_test.shape)","4aab948d":"X_test","54b3bda6":"Y_test","482eb6f9":"# We define the number of trees in the forest in 100. \n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import confusion_matrix\n\n# We define the model\nrfcla = RandomForestClassifier(n_estimators=100,random_state=49,n_jobs=-1)\n\n# We train model\nrfcla.fit(X_train, Y_train)\n\n# We predict target values\nY_predict5 = rfcla.predict(X_test)","0aed16ae":"Y_predict5","216a6775":"test_acc_rfcla = round(rfcla.fit(X_train,Y_train).score(X_test, Y_test)* 100, 2)\ntrain_acc_rfcla = round(rfcla.fit(X_train, Y_train).score(X_train, Y_train)* 100, 2)","6833b68b":"# The confusion matrix\nrfcla_cm = confusion_matrix(Y_test, Y_predict5)\nf, ax = plt.subplots(figsize=(5,5))\nsns.heatmap(rfcla_cm, annot=True, linewidth=0.7, linecolor='black', fmt='g', ax=ax, cmap=\"Greens\")\nplt.title('Random Forest Classification Confusion Matrix')\nplt.xlabel('Y predict')\nplt.ylabel('Y test')\nplt.show()","f99b63c7":"model1 = pd.DataFrame({\n    'Model': ['Random Forest'],\n    'Train Score': [train_acc_rfcla],\n    'Test Score': [test_acc_rfcla]\n})\nmodel1.sort_values(by='Test Score', ascending=False)","9212e379":"from sklearn.metrics import average_precision_score\naverage_precision = average_precision_score(Y_test, Y_predict5)\n\nprint('Average precision-recall score: {0:0.2f}'.format(\n      average_precision))","778ba4b9":"from sklearn.metrics import precision_recall_curve\nfrom sklearn.metrics import plot_precision_recall_curve\nimport matplotlib.pyplot as plt\n\ndisp = plot_precision_recall_curve(rfcla,X_train, Y_train)\ndisp.ax_.set_title('2-class Precision-Recall curve: '\n                   'AP={0:0.2f}'.format(average_precision))","a9e0d46c":"# Distribution of is_paid : True or False","2461a5c8":"# Intermediate Level","848c18a2":"This work try to predict free courses or paid courses using Random Forest.\n\nVariable prediction:\n1. is_paid\tprice\n1. num_subscribers\t\n1. num_reviews\t\n1. num_lectures\t\n1. content_duration\t\n1. subject\n\nTarget \n* is_paid : True , False\n\n","24599f48":"# Accuracy\nis closeness of the measurements to a specific value. Accuracy has two definitions:\n1. More commonly, it is a description of systematic errors, a measure of statistical bias; low accuracy causes a difference between a result and a \"true\" value. ISO calls this trueness.\n1. Alternatively, ISO defines[[1](https:\/\/en.wikipedia.org\/wiki\/Accuracy_and_precision)] accuracy as describing a combination of both types of observational error above (random and systematic), so high accuracy requires both high precision and high trueness.","5cacc49e":"# The 30  Most Popular Courses Reviews","197d69e3":"# Beginner Level","914ad3d3":"# Subject","170c6d2f":"# Top 30 Most Popular Courses","f799b361":"# Number of reviews","5610d9fa":"# Precision and Recall\n\nPrecision is a description of random errors, a measure of statistical variability.\nIn simpler terms, given a set of data points from repeated measurements of the same quantity, the set can be said to be accurate if their average is close to the true value of the quantity being measured, while the set can be said to be precise if the values are close to each other. While Recall is defined as the fraction of relevant documents retrieved compared to the total number of relevant documents (true positives divided by true positives+false negatives).","1652aefc":"As you can see above, we obtain the heatmap of correlation among the variables. The color palette in the side represents the amount of correlation among the variables. The lighter shade represents a high correlation.","bde3f0cf":"# Result","39f33fa9":"# Random forest\nTo understand the random forest model, we must first learn about the decision tree, the basic building block of a random forest. We need to talk about trees before we can get into forests.   A decision tree is a flowchart-like tree structure where an internal node represents feature, the branch represents a decision rule, and each leaf node represents the outcome. The decision tree analyzes a set of data to construct a set of rules or questions, which are used to predict a class, i.e., the goal of decision tree is to create a model that predicts the value of a target variable by learning simple decision rules inferred from the data features. In this sense the decision tree selects the best attribute using to divide the records, converting that attribute into a decision node and dividing the data set into smaller subsets, to finally start the construction of the tree repeating this process recursively.\n\nExample\n![https:\/\/cdn-images-1.medium.com\/max\/824\/0*J2l5dvJ2jqRwGDfG.png](https:\/\/cdn-images-1.medium.com\/max\/824\/0*J2l5dvJ2jqRwGDfG.png)\n\n\nPrinciple of the decision tree\u2019s algorithm\n\n* Root Node: This attribute is used for dividing the data into two or more sets. The feature attribute in this node is selected based on Attribute Selection Techniques.\n* Branch or Sub-Tree: A part of the entire decision tree is called branch or sub-tree.\n* Splitting: Dividing a node into two or more sub-nodes based on if-else conditions.\n* Decision Node: After splitting the sub-nodes into further sub-nodes, then it is called as the decision node.\n* Leaf or Terminal Node: This is the end of the decision tree where it cannot be split into further sub-nodes.\n* Pruning: Removing a sub-node from the tree is called pruning.\n\n\n![https:\/\/cdn-images-1.medium.com\/max\/688\/0*pb-1ufHK-OmR8k7r.png](https:\/\/cdn-images-1.medium.com\/max\/688\/0*pb-1ufHK-OmR8k7r.png)\n\n\n\nSource: https:\/\/www.kdnuggets.com\/","5c2cf77b":"# Measurement\nWhen the classification process was already done. This work evaluated the results using the Confusion Matrix.\n\n# Confusion matrix\n\nConfusion Matrix is commonly used for a summarization of prediction results\non a classification problem.The number of correct and incorrect predictions \nis summarized with counting values and each value broken down for each class. \nEach of them is the key to the confusion matrix. It shows the classification \nmodel is confused when it makes predictions, at this point in here it gives us \ninsight not only into the errors being made by a classifier but also show the \ntypes of errors that are being made [[3](https:\/\/www.geeksforgeeks.org\/confusion-matrix-machine-learning\/)]. ","ddceac32":"# Plotting Heatmap\nHeatmap can be defined as a method of graphically representing numerical data where individual data points contained in the matrix are represented using different colors. The colors in the heatmap can denote the frequency of an event, the performance of various metrics in the data set, and so on. Different color schemes are selected by varying businesses to present the data they want to be plotted on a heatmap [2].","26df03b4":"# SPLITING DATA\nData for training and testing To select a set of training data that will be input in the Machine Learning algorithm, to ensure that the classification algorithm training can be generalized well to new data. For this study using a sample size of 20% ( aims to reduce the overfitting effect).","97496931":"# Prediction: Udemy Free Courses or Paid Courses","2e2e0888":"# Working of Random forest classification\nRandom forest is a supervised learning algorithm that creates a forest randomly. This forest, is a set of decision trees, most of the times trained with the bagging method. The essential idea of bagging is to average many noisy but approximately impartial models, and therefore reduce the variation. Each tree is constructed using the following algorithm:\n\n* Let $N$ be the number of test cases, $M$ is the number of variables in the classifier.\n* Let $m$ be the number of input variables to be used to determine the decision in a given node; $m<M$.\n* Choose a training set for this tree and use the rest of the test cases to estimate the error.\n* For each node of the tree, randomly choose $m$ variables on which to base the decision. Calculate the best partition of the training set from the $m$ variables.\n\n\n**Advantages Random Forest:**\n* runtimes are quite fast\n* Are able to deal with unbalanced and missing data\n\n\n\n**Random Forest is a prediction ensemble, in which:\n* The training dataset :  divided into sub datasets.\n* Each sub dataset is processed to generate a prediction model.\n* Each prediction model is applied to each test data\n* A prediction is by using the \u201cmaximum voting\u201d method.\n\n**To illustrate the concept, you can see below:\n**\n\n\n![https:\/\/miro.medium.com\/max\/592\/1*i0o8mjFfCn-uD79-F1Cqkw.png](https:\/\/miro.medium.com\/max\/592\/1*i0o8mjFfCn-uD79-F1Cqkw.png)","110f99e4":"# Top 30  Num_lectures","b3e7f74f":"# All Levels","9b7bbc54":"Transfer sting to numeric","d92f3539":"# Expert Level"}}