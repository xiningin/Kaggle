{"cell_type":{"a33a629a":"code","67f64dc8":"code","b2a31119":"code","42e5b20d":"code","7ad6ebe2":"code","837caf9e":"code","ddfe4e80":"code","582cc519":"code","15dc7c9f":"code","e8e139c2":"code","b5b130e3":"code","b4ea0f84":"code","f894ffa8":"code","db15400e":"code","158dec3e":"code","7096c5d8":"markdown","a6812f56":"markdown","0ae0eb46":"markdown","ef9dbdfc":"markdown","6f57cf9e":"markdown","d33db7e1":"markdown"},"source":{"a33a629a":"# Imports\n\nimport numpy as np\nimport pandas as pd\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch import optim\n\nimport torchvision.transforms as transforms\nfrom torchvision import datasets\n\nfrom sklearn.model_selection import train_test_split\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","67f64dc8":"# Importaci\u00f3n de datos\n# Data import\nd_train = pd.read_csv(\"..\/input\/digit-recognizer\/train.csv\")\nd_test = pd.read_csv(\"..\/input\/digit-recognizer\/test.csv\")","b2a31119":"# Formato actual de d_train\n# d_train current format\nd_train.head()","42e5b20d":"# Formato actual de d_test\n# d_test current format\nd_test.head()\n\n#Nota: estas im\u00e1genes no tinen 'label' porque son las que queremos predecir con el modelo!\n#Note: these images don't have 'label' because they're the ones we want to predict with the model!","7ad6ebe2":"# Transformaci\u00f3n de datos a tensor\n# Data transformation to tensor\n\n# 1. Conversi\u00f3n de los DataFrame a NumPy arrays utilizando pandas.DataFrame.to_numpy. Las im\u00e1genes y los labels se separan\n# 1. Convert DataFrame to NumPy arrays using pandas.DataFrame.to_numpy. Images and labels are separated\n\n# Train dataset\nx_train = d_train.iloc[:, 1:].to_numpy(dtype = np.float)\nx_train = np.reshape(x_train, newshape = (len(x_train), 1, 28, 28))\ny_train = d_train['label'].to_numpy()\n\nprint(x_train.shape)\nprint(y_train.shape)\n\n# Test dataset\nx_test = d_test.to_numpy(dtype = np.float)\nx_test = np.reshape(x_test, newshape = (len(x_test), 1, 28, 28))\n\nprint(x_test.shape)","837caf9e":"# 2. Normalizaci\u00f3n de los datos. Se dividir\u00e1n los valores entre 255 ya que se encuentran entre 0 y 255,\n# haciendo que los nuevos valores se encuentre entre 0 y 1.\n# 2. Data normalization. Values will be divided by 255 because they're set between 0 and 255,\n# making it so the new values are between 0 and 1.\n\nx_train = x_train \/ 255\nx_test = x_test \/ 255\n\n# Imprimamos algunos valores\n# Let's print some values\n\nprint(x_train[0])\nprint(x_test[0])","ddfe4e80":"# 3. Conversi\u00f3n de los arrays a tensor\n# 3. Convert array to tensor\n\n# Train data\nX_tr = torch.from_numpy(x_train).type(torch.FloatTensor)\nY_tr = torch.from_numpy(y_train)\n\n# Test data\nX_te = torch.from_numpy(x_test).type(torch.FloatTensor)\nY_te = torch.from_numpy(np.zeros(x_test.shape))\n\n# Validation\nX_train, X_va, Y_train, Y_va = train_test_split(X_tr, Y_tr, test_size = 0.2)\n\n# Sets\ntrain_set = torch.utils.data.TensorDataset(X_train, Y_train)\nval_set = torch.utils.data.TensorDataset(X_va, Y_va)\ntest_set = torch.utils.data.TensorDataset(X_te, Y_te)\n\ntrainloader = torch.utils.data.DataLoader(train_set, batch_size = 32, shuffle = False)\nvalloader = torch.utils.data.DataLoader(val_set, batch_size = 32, shuffle = False)\ntestloader = torch.utils.data.DataLoader(test_set, batch_size = 32, shuffle = False)","582cc519":"# Ver un ejemplo del dataset MNIST\n# Seeing an example from the MNIST dataset\ndataiter = iter(trainloader)\nimages, labels = dataiter.next()\nplt.imshow(images[1].numpy().squeeze(), cmap='Greys_r');","15dc7c9f":"# Ahora se plantear\u00e1 la arquitectira del modelo CNN que se encargar\u00e1 de clasificar las im\u00e1genes\n# Now I'll build the architecture for the CNN model that will clasify the images\n\n# La arquitectura ser\u00e1: 2 capas convolucionales, y se seguir\u00e1 con 2 capas lineales. (inicialmente intent\u00e9 con 3 \n# capas convolucionales pero no funcionaba el modelo). \n# Se utilizar\u00e1 Batch Normalization, Weight Initialization, Pooling y Dropout.\n\nclass NMIST_CNN(nn.Module):\n    \n    def __init__(self):\n        \n        super(NMIST_CNN, self).__init__()\n        \n        # Capa 1: capa convolucional 1, 1 --> 16\n        self.cn1 = nn.Conv2d(1, 16, kernel_size = 4, padding = 2)\n        nn.init.xavier_normal_(self.cn1.weight)\n        nn.init.zeros_(self.cn1.bias)\n        self.bn1 = nn.BatchNorm2d(num_features = 16)\n        \n        # Capa 2: capa convolucional 2, 16 --> 32\n        self.cn2 = nn.Conv2d(16, 32, kernel_size = 4)\n        nn.init.xavier_normal_(self.cn2.weight)\n        nn.init.zeros_(self.cn2.bias)\n        self.bn2 = nn.BatchNorm2d(num_features = 32)\n        \n        # Capa 3: capa lineal 1, 800 --> 200\n        self.fc1 = nn.Linear(800, 200)\n        nn.init.xavier_normal_(self.fc1.weight)\n        nn.init.zeros_(self.fc1.bias)\n        self.bn3 = nn.BatchNorm1d(num_features = 200)\n        \n        # Capa 4: capa lineal 2 (output), 200 --> 10\n        self.fc2 = nn.Linear(200, 10)\n        self.bn4 = nn.BatchNorm1d(num_features = 10)\n        \n        # Dropout, utilizar\u00e9 0.2 como siempre se hizo en las tareas\n        self.dropout = nn.Dropout(p = 0.2)\n        \n        #Pooling\n        self.pooling = nn.MaxPool2d((2,2))\n        \n        # (parece que me gustan los numeros 2^n)\n        \n    def forward(self, x):\n        \n        # Capa 1\n        x = F.relu(self.bn1(self.cn1(x)))\n        x = self.pooling(x)\n        x = self.dropout(x)\n        \n         # Capa 2\n        x = F.relu(self.bn2(self.cn2(x)))\n        x = self.pooling(x)\n        x = self.dropout(x)\n        \n        # Flatten tensor\n        x = x.view(-1, 800)\n        \n        # Capa 3\n        x = F.relu(self.bn3(self.fc1(x)))\n        x = self.dropout(x)\n        \n        # Capa 4\n        x = F.log_softmax(self.bn4(self.fc2(x)), dim = 1)\n        \n        return x","e8e139c2":"model = NMIST_CNN()\nprint(model)","b5b130e3":"criterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.003)\n\nepochs = 30\n\ntrain_losses, test_losses, train_accuracies, test_accuracies = [], [], [], []\n\nfor e in range(epochs):\n    \n    tr_loss = 0\n    tr_accuracy = 0\n    \n    model.train()\n    \n    for images, labels in trainloader:\n        \n        optimizer.zero_grad()\n        \n        results = model(images)\n        \n        loss = criterion(results, labels)\n        \n        loss.backward()\n        \n        optimizer.step()\n        \n        tr_loss += loss.item()\n        \n        top_p, top_class = results.topk(1, dim=1)\n        equals = top_class == labels.view(*top_class.shape)\n        tr_accuracy += torch.mean(equals.type(torch.FloatTensor))\n        \n    else:\n        \n        te_loss = 0\n        te_accuracy = 0\n        \n        with torch.no_grad():\n            \n            model.eval()\n            \n            for images, labels in valloader:\n                \n                test_results = model(images)\n                \n                loss2 = criterion(test_results, labels)\n                \n                te_loss += loss2.item()\n                \n                top_p, top_class = test_results.topk(1, dim=1)\n                equals = top_class == labels.view(*top_class.shape)\n                te_accuracy += torch.mean(equals.type(torch.FloatTensor))\n        \n        train_losses.append(tr_loss\/len(trainloader))\n        train_accuracies.append(tr_accuracy.item()\/len(trainloader))\n        \n        test_losses.append(te_loss\/len(valloader))\n        test_accuracies.append(te_accuracy.item()\/len(valloader))\n                \n        print(\"Epoch: \" + str(e+1))\n        print(\"Train loss: \" + str(tr_loss\/len(trainloader)))\n        print(f'Train accuracy: {tr_accuracy.item()*100\/len(trainloader)}%')\n        print(\"Validation loss: \" + str(te_loss\/len(valloader)))\n        print(f'Validation accuracy: {te_accuracy.item()*100\/len(valloader)}%')\n        print('')","b4ea0f84":"# Graficar Loss\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.plot(train_losses, label='Train Loss')\nplt.plot(test_losses, label='Test Loss')\nplt.legend()\nplt.show()","f894ffa8":"# Graficar Accuracy\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.plot(train_accuracies, label='Train Accuracy')\nplt.plot(test_accuracies, label='Test Accuracy')\nplt.legend()\nplt.show()","db15400e":"# Correr el modeo para obtener outputs\nprediction = []\n\nwith torch.no_grad():\n\n    model.eval()\n    \n    for images, x in testloader:\n\n        predictions = model(images)\n        top_p, top_class = predictions.topk(1, dim=1)\n\n        for n in range(len(predictions)):\n            prediction.append(top_class[n].item())\n        \n\nsubmit = pd.DataFrame(data={'Label':prediction})\nsubmit.insert(0, 'ImageId', range(1, 1 + len(submit)), True)\nsubmit.head()","158dec3e":"# Subir output a Kaggle\nsubmit.to_csv('submission.csv', index = False)\nsubmit.head()","7096c5d8":"# Resultados\n# Results","a6812f56":"# Arquitectura del Modelo\n# Model architecture","0ae0eb46":"# Entrenamiento del Modelo\n# Model training","ef9dbdfc":"# Submit Test","6f57cf9e":"# Datos: importaci\u00f3n y preprocesamiento\n# Data: importing and preprocessing","d33db7e1":"# Visualizaci\u00f3n de Datos\n# Data visualization"}}