{"cell_type":{"3d33faf3":"code","358ca14c":"code","2ae6d3b8":"code","3ebfed3d":"code","858687e3":"code","113b9644":"code","28cc9a5c":"code","6c063645":"code","170aaea7":"code","2c3972b7":"code","08406196":"code","7cb3c7d4":"code","5f104579":"code","4f76437c":"code","976c124c":"code","862f9331":"code","8323c892":"code","ee1029b4":"code","0cb0bb42":"code","1ca00285":"code","e3371e18":"code","ac33af2b":"code","dc075e50":"code","31ff7baa":"code","318f501c":"code","4ffe9b1f":"code","0e171f75":"code","f61a6653":"code","1685e074":"code","6696bf59":"code","8c9f31f6":"code","e841a6c9":"code","7be6b2a5":"code","02212fc1":"code","421d1bc7":"code","f486152d":"code","886524fe":"code","b99e02c5":"code","f6f7b21e":"code","ab9e214a":"code","816d8e4e":"code","a6f883d2":"code","4159b571":"code","5b142829":"code","6d1849fb":"code","faa68ab7":"code","d9d1b8b6":"code","f2c08d98":"code","0153fb25":"code","1c43032f":"code","04a65ee6":"code","0d7fd024":"code","1ce38b8b":"code","357797d3":"code","ffd0ac54":"code","8fdc9eae":"code","431ba87a":"code","30625de3":"code","66bd20e6":"code","9fd37f76":"code","38b88274":"code","c8fb6b6c":"code","9193ce8a":"code","92aeed4b":"code","10879ddd":"code","32639bee":"code","54c62b04":"code","c2b782aa":"code","9f89e4db":"code","dfcec082":"code","ab9cba76":"code","1ee42572":"code","e7b8b53b":"code","ed2cc3e9":"code","bf4e4ca5":"code","d990ebe9":"code","84a5ae19":"code","3784a5cf":"code","5350ebf3":"code","7813b8ed":"code","66ef82b7":"code","6a502a11":"code","a8663d4c":"code","9d48b711":"code","2f8f9229":"code","8e7983b8":"code","16e4bdf5":"code","3a89c199":"code","c0dcb05d":"code","d02be40e":"code","e2b3e6e4":"code","a1a0dfe8":"code","713e7970":"code","e71867ea":"code","fa59ca07":"code","5c306fe4":"code","c05d53e8":"code","42525972":"code","4a90d30d":"code","18546338":"code","f81c552a":"code","ba072584":"code","9108a21d":"code","87b29dbb":"code","e9ae9bce":"code","370078f9":"code","ace563c7":"code","ffc16a65":"code","aa115048":"code","402a19e4":"code","268016d9":"code","29beacba":"code","3d42a1a1":"code","d32a049c":"code","78efb42c":"code","c340430d":"code","51bc3906":"code","b573d084":"code","7bc8b07f":"code","e68e91a3":"code","84ee1ead":"code","0239ab3b":"code","4477a7a0":"code","6b9177b1":"code","42d0a4d7":"markdown","62c8c4a7":"markdown","3933a225":"markdown","ccc298f6":"markdown","9bce3b0d":"markdown","8f984443":"markdown","63083baa":"markdown","d5053d77":"markdown","6608b350":"markdown","e763e97b":"markdown","74e7b8c4":"markdown","4ea0a921":"markdown","f72ddc5a":"markdown","035991dd":"markdown","482166e9":"markdown","e879aab6":"markdown","b899a777":"markdown","34b2de0b":"markdown","e4a00b92":"markdown","f4ae6a02":"markdown","c8420be9":"markdown","b86ae46d":"markdown","2d4ff117":"markdown","c23de048":"markdown","7ffd2b9f":"markdown"},"source":{"3d33faf3":"import pandas as pd\nimport numpy as np\nimport re\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom gensim.parsing.preprocessing import STOPWORDS, strip_tags, strip_numeric, strip_punctuation, strip_multiple_whitespaces, remove_stopwords, strip_short, stem_text\nfrom nltk.corpus import stopwords\nimport pickle\nimport en_core_web_sm\nimport csv\nimport json\nimport nltk\nimport langid\nfrom gensim.test.utils import common_texts, get_tmpfile\nfrom gensim.models import Word2Vec\nfrom gensim.models.doc2vec import Doc2Vec, TaggedDocument\nfrom scipy.spatial.distance import cosine, cdist\nimport matplotlib.pyplot as plt","358ca14c":"path = '..\/input\/CORD-19-research-challenge\/'\npath2 = '..\/input\/CORD-19-research-challenge\/Kaggle\/target_tables\/8_risk_factors\/'","2ae6d3b8":"#loading creating the dataframes\nmeta_df = pd.read_csv(path + 'metadata.csv') \nmeta_df = meta_df[['publish_time','title','abstract','cord_uid', 'doi', 'journal', 'url','pdf_json_files']]\n\ntarget_smoking_df = pd.read_csv(path2 +'Smoking Status.csv', index_col= 'Unnamed: 0')\ntarget_smoking_df = target_smoking_df[['Date', 'Study', 'Study Link', 'Journal', 'Study Type','Added on']]\ntarget_diabetes_df = pd.read_csv(path2 +'Diabetes.csv', index_col= 'Unnamed: 0')\ntarget_diabetes_df = target_diabetes_df[['Date', 'Study', 'Study Link', 'Journal', 'Study Type','Added on']]\ntarget_hypertension_df = pd.read_csv(path2+'Hypertension.csv', index_col= 'Unnamed: 0')\ntarget_hypertension_df = target_hypertension_df[['Date', 'Study', 'Study Link', 'Journal', 'Study Type','Added on']]\nprint('meta length:',len(meta_df))\nprint('smoking target table length:',len(target_smoking_df))\nprint('diabetes target table length:',len(target_diabetes_df))\nprint('hypertension target table length:',len(target_hypertension_df))","3ebfed3d":"#Merging the abstracts in the target tables\ntarget_smoking_df = target_smoking_df.merge(meta_df,how='inner', left_on='Study', right_on='title')\n#target_smoking_df = target_smoking_df[['Date','title','abstract']]\ntarget_diabetes_df = target_diabetes_df.merge(meta_df,how='inner', left_on='Study', right_on='title')\n#target_diabetes_df = target_diabetes_df[['Date','title','abstract']] >meta_df[['title', 'abstract']]\ntarget_hypertension_df = target_hypertension_df.merge(meta_df,how='inner', left_on='Study', right_on='title')\n#target_hypertension_df = target_hypertension_df[['Date','title','abstract']]","858687e3":"plt.figure(figsize=(20,10))\nmeta_df.isna().sum().plot(kind='bar', stacked=True)","113b9644":"#Getting rid of duplicates and Null Values in meta dataframe\nprint(\"Initial length:\",len(meta_df))\nmeta_df.drop_duplicates(subset='title', keep=\"first\", inplace=True)\nmeta_df.drop_duplicates(subset='abstract', keep=\"first\", inplace=True)\nprint(\"After dropping duplicates:\",len(meta_df))\nmeta_df.dropna(axis=0, inplace=True, subset=['publish_time','title','abstract'])\nprint(\"After dropping N\/A:\",len(meta_df))\nmeta_df.reset_index(inplace=True)","28cc9a5c":"#Getting rid of duplicates and Null Values in target dataframes\nprint(\"Initial length-somking:\",len(target_smoking_df))\ntarget_smoking_df.drop_duplicates(subset='title', keep=\"first\", inplace=True)\ntarget_smoking_df.drop_duplicates(subset='abstract', keep=\"first\", inplace=True)\nprint(\"After dropping duplicates-somking:\",len(target_smoking_df))\ntarget_smoking_df.dropna(axis=0, inplace=True, subset = ['title','abstract'])\nprint(\"After dropping N\/A-somking:\",len(target_smoking_df))\ntarget_smoking_df.reset_index(inplace=True)\nprint(\"Initial length-diabetes:\",len(target_diabetes_df))\ntarget_diabetes_df.drop_duplicates(subset='title', keep=\"first\", inplace=True)\ntarget_diabetes_df.drop_duplicates(subset='abstract', keep=\"first\", inplace=True)\nprint(\"After dropping duplicates-diabetes:\",len(target_diabetes_df))\ntarget_diabetes_df.dropna(axis=0, inplace=True, subset = ['title','abstract'])\nprint(\"After dropping N\/A-diabetes:\",len(target_diabetes_df))\ntarget_diabetes_df.reset_index(inplace=True)\nprint(\"Initial length-hypertension:\",len(target_hypertension_df))\ntarget_hypertension_df.drop_duplicates(subset='title', keep=\"first\", inplace=True)\ntarget_hypertension_df.drop_duplicates(subset='abstract', keep=\"first\", inplace=True)\nprint(\"After dropping duplicates-hypertension:\",len(target_hypertension_df))\ntarget_hypertension_df.dropna(axis=0, inplace=True, subset = ['title','abstract'])\nprint(\"After dropping N\/A-hypertension:\",len(target_hypertension_df))\ntarget_hypertension_df.reset_index(inplace=True)","6c063645":"target_smoking_df.head(6)","170aaea7":"target_diabetes_df.head(6)","2c3972b7":"target_hypertension_df.head(6)","08406196":"#Creating the year column\nmeta_df['date'] = pd.to_datetime(meta_df['publish_time'], format ='%Y-%m-%d',errors='coerce')\nmeta_df['year'] = pd.DatetimeIndex(meta_df['date']).year.fillna(0).astype(int)\nmeta_df.head()","7cb3c7d4":"#Filtering our metadata based on publish year\nmeta_df = meta_df[meta_df['year']>2019]\nmeta_df.reset_index(inplace=True, drop=True)\nprint('After publish year filter',len(meta_df))\nmeta_df.head()","5f104579":"for i in range(len(meta_df.abstract)):\n        meta_df.abstract[i] =  strip_numeric(meta_df.abstract[i]) #Remove digits\n        meta_df.abstract[i] =  strip_punctuation(str(meta_df.abstract[i]))  #Remove punctuation\n        meta_df.abstract[i] =  strip_multiple_whitespaces(str(meta_df.abstract[i])) #Remove multiple whitespaces","4f76437c":"for i in range(len(target_smoking_df.abstract)):\n        target_smoking_df.abstract[i] =  strip_numeric(target_smoking_df.abstract[i]) #Remove digits\n        target_smoking_df.abstract[i] =  strip_punctuation(str(target_smoking_df.abstract[i]))  #Remove punctuation\n        target_smoking_df.abstract[i] =  strip_multiple_whitespaces(str(target_smoking_df.abstract[i])) #Remove multiple whitespaces","976c124c":"for i in range(len(target_diabetes_df.abstract)):\n        target_diabetes_df.abstract[i] =  strip_numeric(target_diabetes_df.abstract[i]) #Remove digits\n        target_diabetes_df.abstract[i] =  strip_punctuation(str(target_diabetes_df.abstract[i]))  #Remove punctuation\n        target_diabetes_df.abstract[i] =  strip_multiple_whitespaces(str(target_diabetes_df.abstract[i])) #Remove multiple whitespaces","862f9331":"for i in range(len(target_hypertension_df.abstract)):\n        target_hypertension_df.abstract[i] =  strip_numeric(target_hypertension_df.abstract[i]) #Remove digits\n        target_hypertension_df.abstract[i] =  strip_punctuation(str(target_hypertension_df.abstract[i]))  #Remove punctuation\n        target_hypertension_df.abstract[i] =  strip_multiple_whitespaces(str(target_hypertension_df.abstract[i])) #Remove multiple whitespaces","8323c892":"#Turning everything lowecase\nfor i in range(len(meta_df.abstract)):\n    meta_df.abstract[i] = meta_df.abstract[i].lower()\nfor i in range(len(target_smoking_df.abstract)):\n    target_smoking_df.abstract[i] = target_smoking_df.abstract[i].lower()\nfor i in range(len(target_diabetes_df.abstract)):\n    target_diabetes_df.abstract[i] = target_diabetes_df.abstract[i].lower()\nfor i in range(len(target_hypertension_df.abstract)):\n    target_hypertension_df.abstract[i] = target_hypertension_df.abstract[i].lower()","ee1029b4":"#making a back-up just in case :)\nbackup_meta=meta_df\nbackup_target_smoking=target_smoking_df\nbackup_target_diabetes=target_diabetes_df\nbackup_meta_target_hypertension=target_hypertension_df","0cb0bb42":"# filtering for covid Literature (meta df)\nsearchfor = ['covid','corona','ncov']\nmeta_df = meta_df[meta_df['abstract'].str.contains('|'.join(searchfor), na = False, case=False)] #doesn't consider NA and is case insensitive\nmeta_df.reset_index(inplace=True, drop=True)\nprint('After applying covid lit.', len(meta_df))\nmeta_df","1ca00285":"#Assigning languages to each article and filtering on English (meta df)\nmeta_df['language']='unknown'\nfor i in range(len(meta_df['abstract'])):   \n    meta_df['language'][i]=langid.classify(meta_df['abstract'][i])[0]\nmeta_df=meta_df[meta_df.language.isin(['en'])]\nmeta_df.reset_index(inplace=True, drop=True)\nprint('After applying languege filter', len(meta_df))\nmeta_df.head()","e3371e18":"#removing the stopwords and short words (less than 3)\nfor i in range(len(meta_df.abstract)):\n    meta_df.abstract[i] = remove_stopwords(meta_df.abstract[i])\n    meta_df.abstract[i] = strip_short(meta_df.abstract[i])\n    \nfor i in range(len(target_smoking_df.abstract)):\n    target_smoking_df.abstract[i] = remove_stopwords(target_smoking_df.abstract[i])\n    target_smoking_df.abstract[i] = strip_short(target_smoking_df.abstract[i])\n    \nfor i in range(len(target_diabetes_df.abstract)):\n    target_diabetes_df.abstract[i] = remove_stopwords(target_diabetes_df.abstract[i])\n    target_diabetes_df.abstract[i] = strip_short(target_diabetes_df.abstract[i])\n    \nfor i in range(len(target_hypertension_df.abstract)):\n    target_hypertension_df.abstract[i] = remove_stopwords(target_hypertension_df.abstract[i])\n    target_hypertension_df.abstract[i] = strip_short(target_hypertension_df.abstract[i])","ac33af2b":"#making copies for stemming \nstemmed_meta = meta_df.copy()\n\nstemmed_smoking = target_smoking_df.copy()\n#stemmed_smoking = stemmed_smoking.drop(columns=['index'])\n\nstemmed_hypertension = target_hypertension_df.copy()\n#stemmed_hypertension = stemmed_hypertension.drop(columns=['index'])\n\nstemmed_diabetes = target_diabetes_df.copy()\n#stemmed_diabetes = stemmed_diabetes.drop(columns=['index'])","dc075e50":"#Stemming\nfor i in range(len(stemmed_meta.abstract)):\n    stemmed_meta.abstract[i] = stem_text(stemmed_meta.abstract[i])\n\nfor i in range(len(stemmed_smoking.abstract)):\n    stemmed_smoking.abstract[i] = stem_text(stemmed_smoking.abstract[i])\n\nfor i in range(len(stemmed_diabetes.abstract)):\n    stemmed_diabetes.abstract[i] = stem_text(stemmed_diabetes.abstract[i])\n\nfor i in range(len(stemmed_hypertension.abstract)):\n    stemmed_hypertension.abstract[i] = stem_text(stemmed_hypertension.abstract[i])","31ff7baa":"print(stemmed_meta.abstract[0])\nprint('')\nprint(stemmed_smoking.abstract[0])\nprint('')\nprint(stemmed_diabetes.abstract[0])\nprint('')\nprint(stemmed_hypertension.abstract[0])","318f501c":"#Making a Matrix representing the count of each word in meta\nvectorizer_meta = CountVectorizer(max_features = 100)\nX_meta = vectorizer_meta.fit_transform(stemmed_meta.abstract)\n\n#Creating count of BOW for all articles in meta\ncorpus_meta = vectorizer_meta.get_feature_names()\ncorpus_meta = np.asarray(corpus_meta) #converting corpus to np.array\ncount_meta = pd.DataFrame(data=corpus_meta) #creating the data frame\ncount_meta.rename(columns={0 :'Key'}, inplace=True)\ncount_meta.set_index('Key', inplace=True) #setting index as key\n#adding the rest of the data\nfor i in range(len(stemmed_meta.abstract)):\n    count_meta[str(i)] = X_meta[i].toarray()[0]","4ffe9b1f":"vectorizer_smoking = CountVectorizer(min_df=0.05)\nX_smoking = vectorizer_smoking.fit_transform(stemmed_smoking.abstract)\n\n#Creating count of BOW for all articles in target\ncorpus_smoking = vectorizer_smoking.get_feature_names()\ncorpus_smoking = np.asarray(corpus_smoking) #converting corpus to np.array\ncount_smoking = pd.DataFrame(data=corpus_smoking) #creating the data frame\ncount_smoking.rename(columns={0 :'Key'}, inplace=True)\ncount_smoking.set_index('Key', inplace=True) #setting index as key\n#adding the rest of the data\nfor i in range(len(stemmed_smoking.abstract)):\n    count_smoking[str(i)] = X_smoking[i].toarray()[0]","0e171f75":"vectorizer_diabetes = CountVectorizer(min_df=0.05)\nX_diabetes = vectorizer_diabetes.fit_transform(stemmed_diabetes.abstract)\n    \ncorpus_diabetes = vectorizer_diabetes.get_feature_names()\ncorpus_diabetes = np.asarray(corpus_diabetes) #converting corpus to np.array\ncount_diabetes = pd.DataFrame(data=corpus_diabetes) #creating the data frame\ncount_diabetes.rename(columns={0 :'Key'}, inplace=True)\ncount_diabetes.set_index('Key', inplace=True) #setting index as key\n#adding the rest of the data\nfor i in range(len(stemmed_diabetes.abstract)):\n    count_diabetes[str(i)] = X_diabetes[i].toarray()[0]","f61a6653":"vectorizer_hypertension = CountVectorizer(min_df=0.05)\nX_hypertension = vectorizer_hypertension.fit_transform(stemmed_hypertension.abstract)\n\ncorpus_hypertension = vectorizer_hypertension.get_feature_names()\ncorpus_hypertension = np.asarray(corpus_hypertension) #converting corpus to np.array\ncount_hypertension = pd.DataFrame(data=corpus_hypertension) #creating the data frame\ncount_hypertension.rename(columns={0 :'Key'}, inplace=True)\ncount_hypertension.set_index('Key', inplace=True) #setting index as key\n#adding the rest of the data\nfor i in range(len(stemmed_hypertension.abstract)):\n    count_hypertension[str(i)] = X_hypertension[i].toarray()[0]","1685e074":"#Adding a total column\ncount_meta.loc[:,'Total'] = count_meta.sum(numeric_only=True, axis=1)\ncount_smoking.loc[:,'Total'] = count_smoking.sum(numeric_only=True, axis=1)\ncount_diabetes.loc[:,'Total'] = count_diabetes.sum(numeric_only=True, axis=1)\ncount_hypertension.loc[:,'Total'] = count_hypertension.sum(numeric_only=True, axis=1)","6696bf59":"#sorting based on total counts\ncount_meta = count_meta.sort_values('Total', ascending = False)\ncount_smoking = count_smoking.sort_values('Total', ascending = False)\ncount_diabetes = count_diabetes.sort_values('Total', ascending = False)\ncount_hypertension = count_hypertension.sort_values('Total', ascending = False)","8c9f31f6":"count_meta","e841a6c9":"count_smoking","7be6b2a5":"count_diabetes","02212fc1":"count_hypertension","421d1bc7":"#making smaller dfs\ncount_meta = count_meta['Total']\ncount_smoking = count_smoking['Total']\ncount_diabetes = count_diabetes['Total']\ncount_hypertension = count_hypertension['Total']   ","f486152d":"count_meta.head()","886524fe":"count_smoking.head()","b99e02c5":"count_diabetes.head()","f6f7b21e":"count_hypertension.head()","ab9e214a":"#word cloud to see what our meta data is mostly about\n%matplotlib inline\nfrom wordcloud import WordCloud, STOPWORDS\ntext = meta_df.abstract\nwordcloud = WordCloud(\n    width = 2000,\n    height = 1000,\n    background_color = 'black',\n    stopwords = STOPWORDS).generate(str(text))\nfig = plt.figure(\n    figsize = (40, 30),\n    facecolor = 'k',\n    edgecolor = 'k')\nplt.imshow(wordcloud, interpolation = 'bilinear')\nplt.axis('off')\nplt.tight_layout(pad=0)\nplt.show()","816d8e4e":"print(\"Smoking word cloud:\")\ntext = target_smoking_df.abstract\nwordcloud = WordCloud(\n    width = 2000,\n    height = 1000,\n    background_color = 'black',\n    stopwords = STOPWORDS).generate(str(text))\nfig = plt.figure(\n    figsize = (40, 30),\n    facecolor = 'k',\n    edgecolor = 'k')\nplt.imshow(wordcloud, interpolation = 'bilinear')\nplt.axis('off')\nplt.tight_layout(pad=0)\nplt.show()","a6f883d2":"print (\"Diabetes word cloud:\")\ntext = target_diabetes_df.abstract\nwordcloud = WordCloud(\n    width = 2000,\n    height = 1000,\n    background_color = 'black',\n    stopwords = STOPWORDS).generate(str(text))\nfig = plt.figure(\n    figsize = (40, 30),\n    facecolor = 'k',\n    edgecolor = 'k')\nplt.imshow(wordcloud, interpolation = 'bilinear')\nplt.axis('off')\nplt.tight_layout(pad=0)\nplt.show()","4159b571":"print (\"Hypertension word cloud:\")\ntext = target_hypertension_df.abstract\nwordcloud = WordCloud(\n    width = 2000,\n    height = 1000,\n    background_color = 'black',\n    stopwords = STOPWORDS).generate(str(text))\nfig = plt.figure(\n    figsize = (40, 30),\n    facecolor = 'k',\n    edgecolor = 'k')\nplt.imshow(wordcloud, interpolation = 'bilinear')\nplt.axis('off')\nplt.tight_layout(pad=0)\nplt.show()","5b142829":"#top 100 words in meta we will choose the noise from them\nwith pd.option_context('display.max_rows', None, 'display.max_columns', None):\n    display(count_meta.index.tolist())","6d1849fb":"noise = ['re', 'charactist', 'dim','chronic' , 'obsv', 'intub','mortal', 'sev', 'charactist', 'obsv','conduct','admiss', 'factor', 'death', 'sex', 'male', 'old', 'march', 'blood', 'great', 'low', 'discharg', 'adult', 'bmi', 'logist', 'diff', 'respect', 'icu', 'fatal', 'main', 'numb', 'point', 'state', 'er', 'ratio','independ','characterist','critic','predict','progress','meta','odd','multivari','ill','non','laboratori','nlr','cohort','count', 'record', 'find', 'regress','median', 'score','observ', 'retrospect','survivor', 'assess','significantli', 'adjt', 'total','earli','index','help', 'analys', 'unit','admit', 'preval', 'peopl','analyz','common', 'scienc','februari','follow', 'signific','calcul', 'collect','like', 'evalu', 'examin', 'search','random','end','januari','suggest','background', 'articl', 'copyright', 'rightreserv', 'object', 'import', 'aim', 'covid','patient','cov','infect','diseas','sar','case','coronaviru','pandem','studi','health','sever','result','clinic','risk','data','respiratori','test','hospit','care','report','includ','time','effect','model','treatment','method','viru','outbreak','number','dai','increas','provid','spread','base','associ','acut','measur','develop','control','symptom','china','caus','rate','differ','countri','present','epidem','high','conclus','emerg','current','group','us','public','syndrom','posit','viral','need','level','confirm','transmiss','medic','novel','respons','popul','new','potenti','ag','prevent','identifi','analysi','compar','cell','manag','human','global','review','social','outcom','relat','world','year','drug','detect','import','impact','inform','protect','specif','activ','perform','higher','gener','wuhan','avail','show']\nprint(len(noise))","faa68ab7":"#removing noise words from meta\n#noise = ['re', 'charactist', 'dim','chronic' , 'obsv', 'intub','mortal', 'sev', 'charactist', 'obsv','conduct','admiss', 'factor', 'death', 'sex', 'male', 'old', 'march', 'blood', 'great', 'low', 'discharg', 'adult', 'bmi', 'logist', 'diff', 'respect', 'icu', 'fatal', 'main', 'numb', 'point', 'state', 'er', 'ratio','independ','characterist','critic','predict','progress','meta','odd','multivari','ill','non','laboratori','nlr','cohort','count', 'record', 'find', 'regress','median', 'score','observ', 'retrospect','survivor', 'assess','significantli', 'adjt', 'total','earli','index','help', 'analys', 'unit','admit', 'preval', 'peopl','analyz','common', 'scienc','februari','follow', 'signific','calcul', 'collect','like', 'evalu', 'examin', 'search','random','end','januari','suggest','background', 'articl', 'copyright', 'rightreserv', 'object', 'import', 'aim', 'covid','patient','cov','infect','diseas','sar','case','coronaviru','pandem','studi','health','sever','result','clinic','risk','data','respiratori','test','hospit','care','report','includ','time','effect','model','treatment','method','viru','outbreak','number','dai','increas','provid','spread','base','associ','acut','measur','develop','control','symptom','china','caus','rate','differ','countri','present','epidem','high','conclus','emerg','current','group','us','public','syndrom','posit','viral','need','level','confirm','transmiss','medic','novel','respons','popul','new','potenti','ag','prevent','identifi','analysi','compar','cell','manag','human','global','review','social','outcom','relat','world','year','drug','detect','import','impact','inform','protect','specif','activ','perform','higher','gener','wuhan','avail','show']\nfor i in range(len(stemmed_meta['abstract'])):\n    for j in range(len(noise)):\n        stemmed_meta['abstract'][i] = re.sub(noise[j], r'', stemmed_meta['abstract'][i]) ","d9d1b8b6":"#removing noise words from target tables\nfor i in range(len(stemmed_smoking['abstract'])):\n    for j in range(len(noise)):\n        stemmed_smoking['abstract'][i] = re.sub(noise[j], r'', stemmed_smoking['abstract'][i]) \n\nfor i in range(len(stemmed_diabetes['abstract'])):\n    for j in range(len(noise)):\n        stemmed_diabetes['abstract'][i] = re.sub(noise[j], r'', stemmed_diabetes['abstract'][i]) \n\nfor i in range(len(stemmed_hypertension['abstract'])):\n    for j in range(len(noise)):\n        stemmed_hypertension['abstract'][i] = re.sub(noise[j], r'', stemmed_hypertension['abstract'][i]) ","f2c08d98":"#Applying the strip short and multiple white spaces on all documents again\nfor i in range(len(stemmed_meta.abstract)):\n    stemmed_meta.abstract[i] = strip_short(stemmed_meta.abstract[i], minsize = 2)\n    stemmed_meta.abstract[i] = strip_multiple_whitespaces(str(stemmed_meta.abstract[i]))\n\nfor i in range(len(stemmed_smoking.abstract)):\n    stemmed_smoking.abstract[i] = strip_short(stemmed_smoking.abstract[i], minsize = 2)\n    stemmed_smoking.abstract[i] = strip_multiple_whitespaces(str(stemmed_smoking.abstract[i]))\n\nfor i in range(len(stemmed_diabetes.abstract)):\n    stemmed_diabetes.abstract[i] = strip_short(stemmed_diabetes.abstract[i], minsize = 2)\n    stemmed_diabetes.abstract[i] = strip_multiple_whitespaces(str(stemmed_diabetes.abstract[i]))\n\nfor i in range(len(stemmed_hypertension.abstract)):\n    stemmed_hypertension.abstract[i] = strip_short(stemmed_hypertension.abstract[i], minsize = 2)\n    stemmed_hypertension.abstract[i] = strip_multiple_whitespaces(str(stemmed_hypertension.abstract[i]))\n\nprint(stemmed_meta['abstract'][0])\nprint('')\nprint(stemmed_smoking['abstract'][0])\nprint('')\nprint(stemmed_diabetes['abstract'][0])\nprint('')\nprint(stemmed_hypertension['abstract'][0])  ","0153fb25":"#We recreate the count dataframe for the target table to see the effects ==> smoking \nvectorizer_smoking = CountVectorizer(min_df=0.05)\nX_smoking = vectorizer_smoking.fit_transform(stemmed_smoking.abstract)\n#Creating count of BOW for all articles in target\ncorpus_smoking = vectorizer_smoking.get_feature_names()\ncorpus_smoking = np.asarray(corpus_smoking) #converting corpus to np.array\ncount_smoking = pd.DataFrame(data=corpus_smoking) #creating the data frame\ncount_smoking.rename(columns={0 :'Key'}, inplace=True)\ncount_smoking.set_index('Key', inplace=True) #setting index as key\n#adding the rest of the data\nfor i in range(len(stemmed_smoking.abstract)):\n    count_smoking[str(i)] = X_smoking[i].toarray()[0]\ncount_smoking.loc[:,'Total'] = count_smoking.sum(numeric_only=True, axis=1)\ncount_smoking = count_smoking.sort_values('Total', ascending = False)\ncount_smoking = count_smoking['Total']\nprint (\"New smoking word cloud:\")\nd = {}\nd = count_smoking.to_dict()\nwordcloud = WordCloud(\n    width = 2000,\n    height = 1000,\n    background_color = 'black',\n    stopwords = STOPWORDS)\nwordcloud.generate_from_frequencies(d)\nfig = plt.figure(\n    figsize = (40, 30),\n    facecolor = 'k',\n    edgecolor = 'k')\nplt.imshow(wordcloud, interpolation = 'bilinear')\nplt.axis('off')\nplt.tight_layout(pad=0)\nplt.show()","1c43032f":"#We recreate the count dataframe for the target table to see the effects ==> diabetes\nvectorizer_diabetes = CountVectorizer(min_df=0.05)\nX_diabetes = vectorizer_diabetes.fit_transform(stemmed_diabetes.abstract)\n#Creating count of BOW for all articles in target\ncorpus_diabetes = vectorizer_diabetes.get_feature_names()\ncorpus_diabetes = np.asarray(corpus_diabetes) #converting corpus to np.array\ncount_diabetes = pd.DataFrame(data=corpus_diabetes) #creating the data frame\ncount_diabetes.rename(columns={0 :'Key'}, inplace=True)\ncount_diabetes.set_index('Key', inplace=True) #setting index as key\n#adding the rest of the data\nfor i in range(len(stemmed_diabetes.abstract)):\n    count_diabetes[str(i)] = X_diabetes[i].toarray()[0]\ncount_diabetes.loc[:,'Total'] = count_diabetes.sum(numeric_only=True, axis=1)\ncount_diabetes = count_diabetes.sort_values('Total', ascending = False)\ncount_diabetes = count_diabetes['Total']\nprint (\"New diabetes word cloud:\")\nd = {}\nd = count_diabetes.to_dict()\nwordcloud = WordCloud(\n    width = 2000,\n    height = 1000,\n    background_color = 'black',\n    stopwords = STOPWORDS)\nwordcloud.generate_from_frequencies(d)\nfig = plt.figure(\n    figsize = (40, 30),\n    facecolor = 'k',\n    edgecolor = 'k')\nplt.imshow(wordcloud, interpolation = 'bilinear')\nplt.axis('off')\nplt.tight_layout(pad=0)\nplt.show()","04a65ee6":"#We recreate the count dataframe for the target table to see the effects ==> hypertension \nvectorizer_hypertension = CountVectorizer(min_df=0.05)\nX_hypertension = vectorizer_hypertension.fit_transform(stemmed_hypertension.abstract)\n#Creating count of BOW for all articles in target\ncorpus_hypertension = vectorizer_hypertension.get_feature_names()\ncorpus_hypertension = np.asarray(corpus_hypertension) #converting corpus to np.array\ncount_hypertension = pd.DataFrame(data=corpus_hypertension) #creating the data frame\ncount_hypertension.rename(columns={0 :'Key'}, inplace=True)\ncount_hypertension.set_index('Key', inplace=True) #setting index as key\n#adding the rest of the data\nfor i in range(len(stemmed_hypertension.abstract)):\n    count_hypertension[str(i)] = X_hypertension[i].toarray()[0]\ncount_hypertension.loc[:,'Total'] = count_hypertension.sum(numeric_only=True, axis=1)\ncount_hypertension = count_hypertension.sort_values('Total', ascending = False)\ncount_hypertension = count_hypertension['Total']\nprint (\"New hypertension word cloud:\")\nd = {}\nd = count_hypertension.to_dict()\nwordcloud = WordCloud(\n    width = 2000,\n    height = 1000,\n    background_color = 'black',\n    stopwords = STOPWORDS)\nwordcloud.generate_from_frequencies(d)\nfig = plt.figure(\n    figsize = (40, 30),\n    facecolor = 'k',\n    edgecolor = 'k')\nplt.imshow(wordcloud, interpolation = 'bilinear')\nplt.axis('off')\nplt.tight_layout(pad=0)\nplt.show()","0d7fd024":"#dropping duplicates again\n#stemmed_meta=stemmed_meta.drop(index=20963)\n#stemmed_meta.sort_values('abstract')\n#stemmed_meta2 = stemmed_meta.copy()\n#for i in range(len(stemmed_meta.abstract)):\n#    stemmed_meta2.title[i] = stem_text(stemmed_meta2.title[i])\nstemmed_meta.drop_duplicates(subset='title', keep=\"first\", inplace=True)\nstemmed_meta.drop_duplicates(subset='abstract', keep=\"first\", inplace=True)\nstemmed_meta.reset_index(inplace=True, drop=True)\nstemmed_smoking.drop_duplicates(subset='title', keep=\"first\", inplace=True)\nstemmed_smoking.drop_duplicates(subset='abstract', keep=\"first\", inplace=True)\nstemmed_smoking.reset_index(inplace=True, drop=True)\nstemmed_diabetes.drop_duplicates(subset='title', keep=\"first\", inplace=True)\nstemmed_diabetes.drop_duplicates(subset='abstract', keep=\"first\", inplace=True)\nstemmed_diabetes.reset_index(inplace=True, drop=True)\nstemmed_hypertension.drop_duplicates(subset='title', keep=\"first\", inplace=True)\nstemmed_hypertension.drop_duplicates(subset='abstract', keep=\"first\", inplace=True)\nstemmed_hypertension.reset_index(inplace=True, drop=True)\n#print(stemmed_meta.loc[12374,'abstract'])\n#print(stemmed_meta.loc[1402, 'abstract'])","1ce38b8b":"filename = 'meta_stemmed'\noutfile = open(filename,'wb')\npickle.dump(stemmed_meta,outfile)\noutfile.close()","357797d3":"filename = 'smoking_stemmed'\noutfile = open(filename,'wb')\npickle.dump(stemmed_smoking,outfile)\noutfile.close()","ffd0ac54":"filename = 'diabetes_stemmed'\noutfile = open(filename,'wb')\npickle.dump(stemmed_diabetes,outfile)\noutfile.close()","8fdc9eae":"filename = 'hypertension_stemmed'\noutfile = open(filename,'wb')\npickle.dump(stemmed_hypertension,outfile)\noutfile.close()","431ba87a":"#loading the stemmed data\n#path3 = '..\/input\/output\/'\npath3 = '..\/input\/stemmed-data\/'\n#infile = open(path3+'meta_stemmed','rb')\ninfile = open(path3 +'meta_stemmed','rb')\nstemmed_meta = pickle.load(infile)\ninfile.close()\n\n","30625de3":"#loading the stemmed data\ninfile = open(path3+'smoking_stemmed','rb')\nstemmed_smoking = pickle.load(infile)\ninfile.close()","66bd20e6":"#loading the stemmed data\ninfile = open(path3+'hypertension_stemmed','rb')\nstemmed_hypertension = pickle.load(infile)\ninfile.close()","9fd37f76":"#loading the stemmed data\ninfile = open(path3+'diabetes_stemmed','rb')\nstemmed_diabetes = pickle.load(infile)\ninfile.close()","38b88274":"# Making the stemmed abstracts into a list\nmeta_list = list(stemmed_meta.abstract)\nsmoking_list = list(stemmed_smoking.abstract)\ndiabetes_list = list(stemmed_diabetes.abstract)\nhypertension_list = list(stemmed_hypertension.abstract)","c8fb6b6c":"# splitting corpus\nmeta_corpus = [doc.split() for doc in meta_list]\nprint(meta_corpus[0])\nprint('')\nsmoking_corpus = [doc.split() for doc in smoking_list]\nprint(smoking_corpus[0])\nprint('')\ndiabetes_corpus = [doc.split() for doc in diabetes_list]\nprint(diabetes_corpus[0])\nprint('')\nhypertension_corpus = [doc.split() for doc in hypertension_list]\nprint(hypertension_corpus[0])","9193ce8a":"# initiaize the model building vocabulary with the abstracts from meta\nmeta_sentences = [TaggedDocument(doc, [i]) for i, doc in enumerate(meta_corpus)]\nd2v_model = Doc2Vec(vector_size=20, min_count=5, workers=11, alpha=0.025, epochs=200)\nd2v_model.build_vocab(meta_sentences)\nd2v_model.train(meta_sentences, total_examples=d2v_model.corpus_count, epochs=d2v_model.epochs)\nd2v_model.random.seed(0)","92aeed4b":"# Saving the model trainee with the orginal dataset\nfrom gensim.test.utils import get_tmpfile\nfname = get_tmpfile(\"my_doc2vec_model\")\nd2v_model.save(fname)\nmodel = Doc2Vec.load(fname)","10879ddd":"# saving the model in a pickle file, because it takes at list half an hour to run...\npickle.dump(model, open(\"d2v_meta_saved.pkl\", \"wb\"))","32639bee":"# Applying the model to the target smoking\nd2v_smoking = []\nfor i in range(len(smoking_corpus)):\n    model.random.seed(0)\n    d2v_smoking.append(model.infer_vector(smoking_corpus[i], epochs=200))\n\nd2v_smoking[1]","54c62b04":"# Applying the model to the target diabetes\nd2v_diabetes = []\nfor i in range(len(diabetes_corpus)):\n    model.random.seed(0)\n    d2v_diabetes.append(model.infer_vector(diabetes_corpus[i], epochs=200))\n\nd2v_diabetes[1]","c2b782aa":"# Applying the model to the target hypertension\nd2v_hypertension = []\nfor i in range(len(hypertension_corpus)):\n    model.random.seed(0)\n    d2v_hypertension.append(model.infer_vector(hypertension_corpus[i], epochs=200))\n\nd2v_hypertension[1]","9f89e4db":"# Taking target 1 as an example in the filtered dataset\ncheck = stemmed_meta.title.str.contains(stemmed_smoking.title[1])\nprint(stemmed_smoking.title[1])\nprint('check:')\nprint(stemmed_meta[check])","dfcec082":"stemmed_meta[check]","ab9cba76":"#Cheking the embedding for the model equivalent to the smoking 1 \nmodel[16819]","1ee42572":"d2v_smoking[1]","e7b8b53b":"# Checking that the similarity identifies the document on the original dataset\nsimilars = model.docvecs.most_similar(positive=[d2v_smoking[1]], topn=3)\nprint(similars)","ed2cc3e9":"# confirming that the stemmed abstracts are identical #####CHECK HERE#####\nprint(stemmed_meta['title'][16819])\nprint(meta_list[16819])\nprint(smoking_list[1])","bf4e4ca5":"# Making a function for getting the top similar to add to the target table\n\ndef get_similar_docs(target_df, meta_df, d2v_model, d2v_target):\n    \"\"\"\n      This function takes:\n      [1] a target table dataframe\n      [2] the metadata table dataframe\n      [3] doc2vec model based on the metadata abstracts\n      [4] doc2vec model of the target table obtained with the metadata doc2vec model\n\n      Both the target and the metadata tables should contain columns: title, abstract and pdf_json_files.\n\n      For this function to run successfully, \n      the following packages need to be installed:\n       from gensim.models.doc2vec import Doc2Vec\n       import pandas as pd\n\n      At the end it prints the value count of the final dataframes that contains the following columns:\n      ('index', 'original_db', 'similarity_percentage', 'title', 'abstract', 'pdf_json_files');\n\n      It mades 3 dataframes:\n      * not_target: it contains all the new docs found\n      * similar_to_target_df: it contains the original results from the similarity function (target articles + 1st, 2nd and 3rd most similar)\n      * new_docs_target_df: it contains the target articles + similar docs that are not in the target table\n      \n      At the end it returns the new_docs_target_df and the not_target.\n      \n    \"\"\"\n    # Run the similarity test assuming all titles are in the filtered dataset:\n    similar_to_target = []\n    for i in range(len(target_df.title)):\n        sim_test = d2v_model.docvecs.most_similar(positive=[d2v_target[i]], topn=3)\n        #this way the list could be used to create a dataframe\n        similar_to_target.append([target_df['index'][i], 'target', 1, \n                                  target_df.title[i], \n                                  target_df.abstract[i], \n                                  target_df.pdf_json_files[i]])\n        \n        similar_to_target.append([meta_df['index'][sim_test[0][0]], 'most similar', sim_test[0][1], \n                                  meta_df.title[sim_test[0][0]], \n                                  meta_df.abstract[sim_test[0][0]], \n                                  meta_df.pdf_json_files[sim_test[0][0]]])\n\n        #checking if the second and third most similar docs are in target table, if not then append them:\n        if meta_df.title[sim_test[1][0]] not in list(target_df.title):\n            similar_to_target.append([meta_df['index'][sim_test[1][0]], 'second most similar', sim_test[1][1], \n                                      meta_df.title[sim_test[1][0]], meta_df.abstract[sim_test[1][0]], meta_df.pdf_json_files[sim_test[1][0]]])\n        \n        elif meta_df.title[sim_test[2][0]] not in list(target_df.title):\n            similar_to_target.append([meta_df['index'][sim_test[2][0]], 'third most similar', sim_test[2][1], \n                                      meta_df.title[sim_test[2][0]], meta_df.abstract[sim_test[2][0]], meta_df.pdf_json_files[sim_test[2][0]]])\n\n# creating a dataframe with the top 3 most similar docs of the target ones!\n    df_colum = ['original_index', 'original_db', 'similarity_percentage', 'title', 'abstract', 'pdf_json_files']\n    similar_to_target_df = pd.DataFrame(similar_to_target, index=range(len(similar_to_target)), columns=df_colum)\n    # removing the duplicates\n    new_docs_target_df = similar_to_target_df.drop_duplicates(subset='title', keep=\"first\", inplace=False)\n    new_docs_target_df.reset_index(drop=True, inplace=True)\n    # filtering the target docs, and staying only with the new docs\n    not_target = new_docs_target_df[new_docs_target_df['original_db'] !='target']\n    not_target.reset_index(drop=True, inplace=True)\n    \n    print('From the orginal similarity test, we get a total of ' + str(len(similar_to_target_df)) +' articles, counting the target ones and their most similars from metadata.')\n    print('After filtering the duplicates from that dataframe, we get a total of ' + str(len(new_docs_target_df)) +' articles.')\n    print('Finally, after filtering the target ones, we end with a total of ' + str(len(not_target)) +' new possible articles for the target table.')\n    \n    #return not_target, similar_to_target_df, new_docs_target_df\n    return new_docs_target_df, not_target","d990ebe9":"def get_relevant_docs(dataframe, target):\n    \"\"\"\n    This function will get the relevant docs from a dataframe depending on a target subject.\n    \n    The dataframe needs to have a column 'pdf_json_files' on it, containing json files names.\n    The target is in str format.\n    \n    At the end it will print the number of relevant docs and \n    return a list of lists for each relevant article with its:\n        [0] = original index,\n        [1] = original body text \n        [2] = if target or not   \n    \n    \"\"\"\n    # parsing through all the docs in the target table\n    related_docs = []\n    \n    for i in range(len(dataframe)):\n        ori_ind = dataframe.original_index[i]\n        ori_tab = dataframe.original_db[i]\n        try:\n            # open json file\n            with open(path + dataframe.pdf_json_files[i], 'r') as myfile:\n                data=myfile.read()\n            # parse file\n            obj = json.loads(data)\n            body = obj['body_text']\n            # having a list of parts of the text for better parsing\n            just_text = [body[d]['text'] for d in range(len(body))]\n            clean_body = [text.lower() for text in just_text]\n            clean_body = [strip_numeric(text) for text in clean_body] # Remove numbers\n            clean_body = [strip_punctuation(text) for text in clean_body] # Remove punctuation\n            clean_body = [strip_multiple_whitespaces(text) for text in clean_body] # Remove multiple spaces\n            clean_body = [remove_stopwords(text) for text in clean_body] #removing the stopwords\n            clean_body = [strip_short(text) for text in clean_body]\n            stem_body = [stem_text(text) for text in clean_body]\n            relevant_parts = []\n            # check if the doc is related with the target\n            for t in range(len(stem_body)):\n                if target in stem_body[t]:\n                    # save the index of relevant parts\n                    relevant_parts.append(t)\n            # save the docs in a list that has: target_index, clean_json_body, original_json_body\n            if len(relevant_parts) != 0:\n                # convert json body_text into a text to have the original text \n                original_text=''\n                for d in range(len(body)):\n                    original_text = original_text+body[d]['text']\n                related_docs.append([ori_ind, original_text, ori_tab])\n                #related_docs.append([ori_ind, original_text, relevant_parts, clean_body])  \n        except:\n            TypeError\n        \n    print('You have ' + str(len(related_docs)) + ' relevant docs')\n    return related_docs","84a5ae19":"def build_relevant_docs_df(meta_df, target_df, relevant_docs_list):\n    \"\"\"\n    This function needs the metadata dataframe, the target dataframe and a list of relevant documents.\n    \n    The list of relevant documents must contain one list for each relevant doc, that \n    has 3 values: [0] = original index,\n                  [1] = original body text \n                  [2] = if target or not  \n                  \n    Finally this function returns a dataframe with all the relevant documents body text obtained from the json file\n    and its corresponding columns from the metadata table.\n    \n    \"\"\"\n    relevant_for_target = []\n    for i in range(len(relevant_docs_list)): \n    #this way the list could be used to create a dataframe\n        index_rev = relevant_docs_list[i][0]\n        #print('from relevant list')\n        #print(index_rev)\n        \n        if relevant_docs_list[i][2] != 'target':\n            df_index = list(meta_df[meta_df['index']== index_rev].index)[0]\n            relevant_for_target.append([index_rev, \n                                        meta_df.publish_time[df_index],  \n                                        meta_df.title[df_index], \n                                        meta_df.abstract[df_index], \n                                        meta_df.cord_uid[df_index], \n                                        meta_df.doi[df_index],\n                                        meta_df.journal[df_index],\n                                        meta_df.url[df_index],\n                                        meta_df.pdf_json_files[df_index],\n                                        relevant_docs_list[i][1],\n                                        relevant_docs_list[i][2]\n                                       ])\n        elif relevant_docs_list[i][2] == 'target':\n            df_index = list(target_df[target_df['index']== index_rev].index)[0]\n            #print('in target table')\n            #print(df_index)\n            \n            relevant_for_target.append([index_rev, \n                                        target_df.Date[df_index],  \n                                        target_df.Study[df_index], \n                                        target_df.abstract[df_index], \n                                        target_df.cord_uid[df_index], \n                                        target_df.doi[df_index],\n                                        target_df.journal[df_index],\n                                        target_df.url[df_index],\n                                        target_df.pdf_json_files[df_index],\n                                        relevant_docs_list[i][1],\n                                        relevant_docs_list[i][2]\n                                       ])\n        \n    # creating a dataframe with the relevant docs including all needed columns\n    df_colum = ['original_index', 'publish_time', 'title', 'abstract', 'cord_uid', 'doi',\n                    'journal', 'url', 'pdf_json_files', 'body_text', 'target_or_not']\n    relevant_df = pd.DataFrame(relevant_for_target, index=range(len(relevant_for_target)), columns=df_colum)\n    return relevant_df","3784a5cf":"# For smoking target\nsmoking_new_docs, smoking_not_target = get_similar_docs(stemmed_smoking, stemmed_meta, model, d2v_smoking)\nsmoking_new_docs.head(3)","5350ebf3":"# For diabetes\ndiabetes_new_docs, diabetes_not_target = get_similar_docs(stemmed_diabetes, stemmed_meta, model, d2v_diabetes)\ndiabetes_new_docs.head()\n","7813b8ed":"# For hypertension\nhyper_new_docs, hyper_not_target = get_similar_docs(stemmed_hypertension, stemmed_meta, model, d2v_hypertension)\nhyper_new_docs.head(3)","66ef82b7":"### Modifying function so that it works with target table dataframe\ndef get_relevant_docs_mod(dataframe, target):\n    \"\"\"\n    This function will get the relevant docs from a dataframe depending on a target subject.\n    \n    The dataframe needs to have a column 'pdf_json_files' on it, containing json files names.\n    The target is in str format.\n    \n    At the end it will print the number of relevant docs and \n    return a list of lists for each relevant article with its:\n        [0] = parts of the body where the target appears\n        [1] = clean body   \n    \n    IT IS MODIFIED SO IT WORKS WITH TARGET TABLE!!\n    \"\"\"\n    # parsing through all the docs in the target table\n    related_docs = []\n    \n    for i in range(len(dataframe)):\n        try:\n            # open json file\n            with open(path + dataframe.pdf_json_files[i], 'r') as myfile:\n                data=myfile.read()\n            # parse file\n            obj = json.loads(data)\n            body = obj['body_text']\n            # having a list of parts of the text for better parsing\n            just_text = [body[d]['text'] for d in range(len(body))]\n            clean_body = [text.lower() for text in just_text]\n            clean_body = [strip_numeric(text) for text in clean_body] # Remove numbers\n            clean_body = [strip_punctuation(text) for text in clean_body] # Remove punctuation\n            clean_body = [strip_multiple_whitespaces(text) for text in clean_body] # Remove multiple spaces\n            clean_body = [remove_stopwords(text) for text in clean_body] #removing the stopwords\n            clean_body = [strip_short(text) for text in clean_body]\n            stem_body = [stem_text(text) for text in clean_body]\n            relevant_parts = []\n            # check if the doc is related with the target\n            for t in range(len(stem_body)):\n                if target in stem_body[t]:\n                    # save the index of relevant parts\n                    relevant_parts.append(t)\n            # save the docs in a list \n            if len(relevant_parts) != 0:\n                related_docs.append([relevant_parts, clean_body])   \n        except:\n            TypeError\n            #print('error')\n            \n    print('You have ' + str(len(related_docs)) + ' relevant docs')\n    return related_docs","6a502a11":"stem_text('smoking')","a8663d4c":"# Checking how many json files we have available\nprint('pdf json files we have available in original target table')\nprint(stemmed_smoking.pdf_json_files.notna().sum())\nprint('pdf json files we have available in new target table')\nprint(smoking_new_docs.pdf_json_files.notna().sum())","9d48b711":"# For smoking\nprint('Original relevant target articles: ')\nsmoking_target_check = get_relevant_docs_mod(stemmed_smoking, 'smoke')\nprint('New articles added: ')\nsmoking_new_relevant = get_relevant_docs(smoking_not_target, 'smoke')\nprint('Total relevant target articles: ')\nsmoking_relevant_docs = get_relevant_docs(smoking_new_docs, 'smoke')","2f8f9229":"stem_text('diabetes')","8e7983b8":"# Checking how many json files we have available\nprint('pdf json files we have available in original target table')\nprint(stemmed_diabetes.pdf_json_files.notna().sum())\nprint('pdf json files we have available in new target table')\nprint(diabetes_new_docs.pdf_json_files.notna().sum())","16e4bdf5":"# For diabetes\nprint('Original relevant target articles: ')\ndiabetes_target_check = get_relevant_docs_mod(stemmed_diabetes, 'diabet')\nprint('New articles added: ')\ndiabetes_new_relevant = get_relevant_docs(diabetes_not_target, 'diabet')\nprint('Total relevant target articles: ')\ndiabetes_relevant_docs = get_relevant_docs(diabetes_new_docs, 'diabet')","3a89c199":"stem_text('hypertension')","c0dcb05d":"# Checking how many json files we have available\nprint('pdf json files we have available in original target table')\nprint(stemmed_hypertension.pdf_json_files.notna().sum())\nprint('pdf json files we have available in new target table')\nprint(hyper_new_docs.pdf_json_files.notna().sum())","d02be40e":"# For hypertension\nprint('Original relevant target articles: ')\nhyper_target_check = get_relevant_docs_mod(stemmed_hypertension, 'hypertens')\nprint('New articles added: ')\nhyper_new_relevant = get_relevant_docs(hyper_not_target, 'hypertens')\nprint('Total relevant target articles: ')\nhyper_relevant_docs = get_relevant_docs(hyper_new_docs, 'hypertens')","e2b3e6e4":"# For smoking\nsmoking_relevant_df = build_relevant_docs_df(stemmed_meta, stemmed_smoking, smoking_relevant_docs)\nsmoking_relevant_df.head()\n","a1a0dfe8":"smoking_relevant_df.target_or_not.value_counts()","713e7970":"#pickle.dump(smoking_relevant_df, open(\"smoking_new_target.pkl\", \"wb\"))","e71867ea":"# For diabetes\ndiabetes_relevant_df = build_relevant_docs_df(stemmed_meta, stemmed_diabetes,diabetes_relevant_docs)\ndiabetes_relevant_df.head()\n","fa59ca07":"diabetes_relevant_df.target_or_not.value_counts()","5c306fe4":"#pickle.dump(diabetes_relevant_df, open(\"diabetes_new_target.pkl\", \"wb\"))","c05d53e8":"# For hypertension\nhyper_relevant_df = build_relevant_docs_df(stemmed_meta, stemmed_hypertension,hyper_relevant_docs)\nhyper_relevant_df","42525972":"hyper_relevant_df.target_or_not.value_counts()","4a90d30d":"#pickle.dump(hyper_relevant_df, open(\"hypertension_new_target.pkl\", \"wb\"))","18546338":"#smoke_df = pickle.load(open('smoking_new_target.pkl', 'rb'))\nsmoke_df = smoking_relevant_df","f81c552a":"#diabetes_df = pickle.load(open('diabetes_new_target.pkl', 'rb'))\ndiabetes_df = diabetes_relevant_df","ba072584":"#hypertension_df = pickle.load(open('hypertension_new_target.pkl', 'rb'))\nhypertension_df = hyper_relevant_df","9108a21d":"smoke_df['sent']=''     # this column will store tokenized version of the body text\nsmoke_df['summary']=''  # this column will store the summaries","87b29dbb":"diabetes_df['sent']=''     # this column will store tokenized version of the body text\ndiabetes_df['summary']=''  # this column will store the summaries","e9ae9bce":"hypertension_df['sent']=''     # this column will store tokenized version of the body text\nhypertension_df['summary']=''  # this column will store the summaries","370078f9":"from nltk.tokenize import sent_tokenize # PASS THIS T THE BEGINNING\nfor i in range(len(smoke_df.body_text)):\n    smoke_df.sent[i]=sent_tokenize(smoke_df.body_text[i])","ace563c7":"for i in range(len(diabetes_df.body_text)):\n    diabetes_df.sent[i]=sent_tokenize(diabetes_df.body_text[i])","ffc16a65":"for i in range(len(hypertension_df.body_text)):\n    hypertension_df.sent[i]=sent_tokenize(hypertension_df.body_text[i])","aa115048":"def countOccurences(sentence, word):     \n    # split the string by spaces in a \n    a = sentence.split()  \n    # search for pattern in a \n    count = 0\n    for i in range(0, len(a)):           \n        # if match found increase count  \n        if (word == a[i]): \n            count = count + 1             \n    return count ","402a19e4":"# identify different stemmed version of smok*\nprint(stem_text(\"smoker\"))\nprint(stem_text(\"smoking\"))\nprint(stem_text(\"smokers\"))\nprint(stem_text(\"smoke\"))","268016d9":"print(stem_text(\"diabetes\"))","29beacba":"print(stem_text(\"hypertension\"))\nprint(stem_text(\"hypertensive\"))","3d42a1a1":"for j in range(len(smoke_df.body_text)):\n    # d is an internal dataframe for preprocessing on sentence level of the documents and count occurence of relveant words\n    d=pd.DataFrame(index=np.arange(len(smoke_df.sent[j])))\n    d['content']= 'none'\n    d['original']='none'\n    d['index']=d.index\n    d['count']= 0\n    for k in range(len(smoke_df.sent[j])): #iterating through sentences of a document\n        d.content[k]=smoke_df.sent[j][k]\n        d.original[k]=smoke_df.sent[j][k]\n        d.content[k] =  strip_numeric(d.content[k]) #Remove digits\n        d.content[k] =  strip_punctuation(d.content[k])  #Remove punctuation\n        d.content[k] =  strip_multiple_whitespaces(d.content[k]) #Remove multiple whitespaces   \n        d.content[k] = d.content[k].lower() # lower characters\n        d.content[k] = remove_stopwords(d.content[k]) #remove stopwords\n        d.content[k] = strip_short(d.content[k]) # remove short words\n        d.content[k] = stem_text(d.content[k]) #stem the words \n        sentence = d.content[k] # \n        word =stem_text(\"smoke\") # get first relevant word\n        word2 =stem_text(\"smoker\") # get second relevant word\n        d['count'][k]=float((countOccurences(sentence, word))) +float((countOccurences(sentence, word2))) #storing the amount of relevant words\n    x=d.loc[d['count'].idxmax()] # getting the dataframe d with the maximal amount of relevant words   \n    smoke_df.summary[j]=x['original'] # storing the sentence with the maximal amount of relevant words in the summary","d32a049c":"for j in range(len(diabetes_df.body_text)):\n    # d is an internal dataframe for preprocessing on sentence level of the documents and count occurence of relveant words\n    d=pd.DataFrame(index=np.arange(len(diabetes_df.sent[j])))\n    d['content']= 'none'\n    d['original']='none'\n    d['index']=d.index\n    d['count']= 0\n    for k in range(len(diabetes_df.sent[j])): #iterating through sentences of a document\n        d.content[k]= diabetes_df.sent[j][k]\n        d.original[k]= diabetes_df.sent[j][k]\n        d.content[k] =  strip_numeric(d.content[k]) #Remove digits\n        d.content[k] =  strip_punctuation(d.content[k])  #Remove punctuation\n        d.content[k] =  strip_multiple_whitespaces(d.content[k]) #Remove multiple whitespaces   \n        d.content[k] = d.content[k].lower() # lower characters\n        d.content[k] = remove_stopwords(d.content[k]) #remove stopwords\n        d.content[k] = strip_short(d.content[k]) # remove short words\n        d.content[k] = stem_text(d.content[k]) #stem the words \n        sentence = d.content[k] # \n        word =stem_text(\"diabetes\") # get first relevant word     \n        d['count'][k]=float((countOccurences(sentence, word)))  #storing the amount of relevant words\n    x=d.loc[d['count'].idxmax()] # getting the dataframe d with the maximal amount of relevant words   \n    diabetes_df.summary[j]=x['original'] # storing the sentence with the maximal amount of relevant words in the summary\n    ","78efb42c":"for j in range(len(hypertension_df.body_text)):\n    # d is an internal dataframe for preprocessing on sentence level of the documents and count occurence of relveant words\n    d=pd.DataFrame(index=np.arange(len(hypertension_df.sent[j])))\n    d['content']= 'none'\n    d['original']='none'\n    d['index']=d.index\n    d['count']= 0\n    for k in range(len(hypertension_df.sent[j])): #iterating through sentences of a document\n        d.content[k]= hypertension_df.sent[j][k]\n        d.original[k]= hypertension_df.sent[j][k]\n        d.content[k] =  strip_numeric(d.content[k]) #Remove digits\n        d.content[k] =  strip_punctuation(d.content[k])  #Remove punctuation\n        d.content[k] =  strip_multiple_whitespaces(d.content[k]) #Remove multiple whitespaces   \n        d.content[k] = d.content[k].lower() # lower characters\n        d.content[k] = remove_stopwords(d.content[k]) #remove stopwords\n        d.content[k] = strip_short(d.content[k]) # remove short words\n        d.content[k] = stem_text(d.content[k]) #stem the words \n        sentence = d.content[k] # \n        word =stem_text(\"hypertension\") # get first relevant word     \n        d['count'][k]=float((countOccurences(sentence, word)))  #storing the amount of relevant words\n    x=d.loc[d['count'].idxmax()] # getting the dataframe d with the maximal amount of relevant words   \n    hypertension_df.summary[j]=x['original'] # storing the sentence with the maximal amount of relevant words in the summary\n","c340430d":"smoke_df = smoke_df.drop(columns=[\"sent\",\"body_text\"]) \n# body_text and sent are not necessary anymore","51bc3906":"diabetes_df = diabetes_df.drop(columns=[\"sent\",\"body_text\"]) \n# body_text and sent are not necessary anymore","b573d084":"hypertension_df = hypertension_df.drop(columns=[\"sent\",\"body_text\"]) \n# body_text and sent are not necessary anymore","7bc8b07f":"smoke_df.head()","e68e91a3":"diabetes_df.head()","84ee1ead":"hypertension_df.head()","0239ab3b":"for i in range(len(smoke_df)):\n    print('')\n    print(\"Title:\",smoke_df.title[i],\",\" ,smoke_df.publish_time[i], \"(\",smoke_df.target_or_not[i], \")\")\n    print('')\n    print(\"Summary:\",smoke_df.summary[i])\n    print('')","4477a7a0":"for i in range(len(diabetes_df)):\n    print('')\n    print(\"Title:\",diabetes_df.title[i],\",\" ,diabetes_df.publish_time[i], \"(\",diabetes_df.target_or_not[i], \")\")\n    print('')\n    print(\"Summary:\",diabetes_df.summary[i])\n    print('')","6b9177b1":"for i in range(len(hypertension_df)):\n    print('')\n    print(\"Title:\",hypertension_df.title[i],\",\" ,hypertension_df.publish_time[i], \"(\",hypertension_df.target_or_not[i], \")\")\n    print('')\n    print(\"Summary:\",hypertension_df.summary[i])\n    print('')","42d0a4d7":"##### For diabetes:","62c8c4a7":"##### identify the sentences which mention the risk factor the most","3933a225":"### As we could have guessed our meta table is mostly about Covid and our target table is mostly about risk factors. \n### However, we should remove the noises that are not relevant to our topics and mess with our modeling","ccc298f6":"##### name of smoke_df could be changed to the dataframe name which is currently worked on for the relevant risk factor","9bce3b0d":"##### For smoking:","8f984443":"Running the function to get similar docs and obtaining **new_docs_target_df, not_in_target_df**","63083baa":"**To get relevant documents checking json files:**","d5053d77":"Getting the **relevant documents based on their body text**","6608b350":"## Preparing for doc2vec model","e763e97b":"##### display final dataframe","74e7b8c4":"##### getting rid of columns which we don't need anymore","4ea0a921":"**To build a final dataframe of the new docs for the target table:**","f72ddc5a":"Creating the relevant docs dataframes","035991dd":"## Creating summaries for new target tables","482166e9":"## Loading stemmed datasets","e879aab6":"This next step takes a lot of Time!!!","b899a777":"##### For hypertension:","34b2de0b":"Explain here about comorbid...","e4a00b92":"## Preparing for the word cloud visualization","f4ae6a02":"**To get similar docs to the target:**","c8420be9":"##### tokenizing the full body text in sentences","b86ae46d":"##### how the summaries looks like","2d4ff117":"### Now that we have our bag of words and their counts let's make some visualizations","c23de048":"##### defining a count function specialized on a given word","7ffd2b9f":"##### creating two new columns in the dataframe"}}