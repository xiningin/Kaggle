{"cell_type":{"f21313b9":"code","dc3b7e1a":"code","ab0788af":"code","29c272c0":"code","bf5a6119":"code","2ac69139":"code","ce91309e":"code","2bf6f6ef":"code","35ab5366":"code","4a2e0b75":"code","34f53913":"code","c589f48b":"code","65c0a5aa":"code","b79afb65":"code","3ad2efbc":"code","e69ae725":"code","bc90deef":"code","0adf9526":"code","5ce61cac":"code","7867f9fe":"code","f2d3cf54":"code","4e75f9f1":"code","0ba249d3":"code","a67ed21e":"code","4d77ad4a":"code","57510cb4":"code","8d136c96":"code","3608df1a":"code","5fe72b6e":"code","d008dea4":"code","ed6051dd":"code","b4c2dc5c":"code","6b05c12b":"code","5444750e":"code","26895133":"code","fc837fe1":"code","0ee404fa":"code","eab15f0f":"code","48e6a8d5":"code","870212d4":"code","ac18f4fb":"code","e610fd24":"code","96c0b1cf":"code","6a22935a":"code","061ea92f":"code","5abadc95":"markdown","b484fb7c":"markdown","dbc81b4e":"markdown","3a0fa381":"markdown","ab5917d4":"markdown","1b88d198":"markdown","c1d059a8":"markdown","84c24715":"markdown","628e85a6":"markdown","d73e8f91":"markdown","bc576755":"markdown","a3329592":"markdown","dda524bb":"markdown","df420296":"markdown","476abb6b":"markdown","2ba1f8a4":"markdown","0e77d4e5":"markdown","a421e691":"markdown","718cab95":"markdown","3da22952":"markdown","e8f223d1":"markdown","b6d2f714":"markdown","d073e563":"markdown","d8db71bb":"markdown","732d4b5c":"markdown","b7a47d1a":"markdown"},"source":{"f21313b9":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","dc3b7e1a":"for dirname, _, filenames in os.walk('\/kaggle'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","ab0788af":"import matplotlib.pyplot as plt\nimport seaborn as sns\ncolor = sns.color_palette()\nsns.set_style('darkgrid')\n\nimport plotly_express as px\n\nfrom scipy import stats\nfrom scipy.stats import norm, skew \n\nimport pandas_profiling\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n\n\nfrom lightgbm import LGBMRegressor","29c272c0":"print(\"List of files:\", os.listdir('\/kaggle\/input\/house-prices-advanced-regression-techniques'))\n\n# Train data\ndf_train = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv')\nprint(\"\\nTrain data length:\",df_train.shape)\nprint(\"\\nTrain data columns:\",df_train.columns)\nprint(\"\\nTrain data columns:\",df_train.info())\nprint(\"\\nTrain data:\\n\\n\",df_train.head())","bf5a6119":"# Test data\ndf_test = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv')\nprint(\"\\nTest data length:\",df_test.shape)","2ac69139":"# Correlation\ndf_train_corr = df_train.corr()\ndf_train_corr","ce91309e":"df_train_corr.style.background_gradient(cmap='coolwarm', axis=None)","2bf6f6ef":"# SalePrice has highest corr with OverallQual\ndf_train_corr[['SalePrice','OverallQual']].style.background_gradient(cmap='coolwarm', axis=None)","35ab5366":"# Use panda profile report\n# df_train.profile_report()","4a2e0b75":"df_train['SalePrice'].describe()","34f53913":"ax=df_train['SalePrice'].plot.hist(bins=100, alpha=0.6)","c589f48b":"# Use matplotlib\n\n# plt.style.use('ggplot')\nplt.hist(df_train['SalePrice'], bins = 100)\n\n# Add title and axis names\nplt.title('Sales Price')\nplt.xlabel('Price')\nplt.ylabel('Frequency') \n\nplt.show()","65c0a5aa":"# Scatter Plot\nfig, ax = plt.subplots()\nax.scatter(df_train['GrLivArea'], df_train['SalePrice'])\nplt.xlabel('GrLivArea', fontsize=12)\nplt.ylabel('SalePrice', fontsize=12)\nplt.title('Sale Price', fontsize=16)\nplt.show()","b79afb65":"# QQ-plot\nfig = plt.figure()\nax = fig.add_subplot()\nres = stats.probplot(df_train['SalePrice'], plot=plt)\nplt.show()","3ad2efbc":"# Scatter Plot with color from 2nd variable\npx.scatter(df_train, x='GrLivArea', y='SalePrice', color='OverallQual')","e69ae725":"# Scatter Plot with color from 2nd variable\npx.scatter(df_train, x='TotalBsmtSF', y='SalePrice', color='OverallQual')","bc90deef":"# Box Plot\npx.box(df_train[['OverallQual', 'SalePrice']].sort_values(by='OverallQual')\n       , x='OverallQual'\n       , y='SalePrice'\n       , color='OverallQual')","0adf9526":"# Box Plot\npx.box(df_train[['SaleCondition', 'SalePrice']].sort_values(by='SaleCondition')\n       , x='SaleCondition'\n       , y='SalePrice'\n       , color='SaleCondition')","5ce61cac":"# Box Plot\npx.box(df_train[['ExterQual', 'SalePrice']].sort_values(by='ExterQual')\n       , x='ExterQual'\n       , y='SalePrice'\n       , color='ExterQual')","7867f9fe":"sns.distplot(df_train['SalePrice'] , fit=norm);\n\n# Get the fitted parameters used by the function\n(mu, sigma) = norm.fit(df_train['SalePrice'])\nprint( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n\n# Plot the distribution\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='upper right')\n\nax = plt.axes()\nplt.ylabel('Frequency')\nplt.title('SalePrice distribution')\n","f2d3cf54":"sns.heatmap(df_train_corr, \n            xticklabels=df_train_corr.columns.values,\n            yticklabels=df_train_corr.columns.values)","4e75f9f1":"df_train.drop(['SalePrice'], axis = 1).describe().T","0ba249d3":"# Clean outliers\nprint(\"Length of data before dropping outliers:\", len(df_train))\ndf_train = df_train.drop(df_train[(df_train['GrLivArea']>4000) \n                                & (df_train['SalePrice']<300000)].index)\nprint(\"Length of data after dropping outliers:\", len(df_train))\ndf_train = df_train.drop(df_train[(df_train['GrLivArea']>5000) \n                                | (df_train['SalePrice']>500000)].index)\nprint(\"Length of data after dropping outliers:\", len(df_train))","a67ed21e":"# Quantitative Variables\nquan_var = [q for q in df_train.columns if df_train.dtypes[q] != 'object']\nquan_var.remove('SalePrice') \nquan_var.remove('Id')\nprint(\"Quantitative Variables:\\n\", quan_var)\n\n# Qualitative Variables\nqual_var = [q for q in df_train.columns if df_train.dtypes[q] == 'object']\nprint(\"\\nQualitative Variables:\\n\", qual_var)","4d77ad4a":"# Combine all data\nntrain = df_train.shape[0]\nntest = df_test.shape[0]\ny_train = df_train.SalePrice.values\ndf_all_data = pd.concat((df_train, df_test)).reset_index(drop=True)\ndf_all_data.drop(['Id','SalePrice'], axis=1, inplace=True)\nprint(\"all_data size is : {}\".format(df_all_data.shape))\n\n# Calculate missing data ratio\ndf_all_data_na = (df_all_data.isnull().sum() \/ len(df_all_data)) * 100\ndf_all_data_na = df_all_data_na.drop(df_all_data_na[df_all_data_na == 0].index).sort_values(ascending=False)[:50]\nmissing_data = pd.DataFrame({'Missing Ratio' :df_all_data_na})\nprint('Missing data percentage:\\n',missing_data.head(50))\n\n# Plot\nf, ax = plt.subplots(figsize=(15, 12))\nplt.xticks(rotation='90')\nax.set_facecolor(\"white\")\nsns.barplot(x=df_all_data_na.index, y=df_all_data_na)\nsns.color_palette('pastel')\nplt.xlabel('Features', fontsize=12)\nplt.ylabel('Percent of missing values', fontsize=12)\nplt.title('Percent missing data by feature', fontsize=15)","57510cb4":"df_all_data","8d136c96":"df_result = pd.DataFrame(columns=['Model','RMSE','MSE','Summary'])\nprint(df_result)","3608df1a":"# Run Linear Regression on a single variable that has the highest corr with dependent variable\nX = df_train[['OverallQual']]\ny = df_train['SalePrice']\n\n# Train Test split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n\n# Linear Regression Model\nlr = LinearRegression()\nlr.fit(X_train, y_train)\ny_pred = lr.predict(X_test)\n\n# RMSE\nmse = mean_squared_error(y_test, y_pred)\nrmse = np.sqrt(mean_squared_error(y_test, y_pred))\nprint(\"Root Mean Squared Error: {:.2f}\".format(rmse))\ndf_result = df_result.append(pd.DataFrame([['Linear Regression'\n                                            , rmse\n                                            , mse\n                                            ,'Baseline model'                               \n                                           ]], columns=df_result.columns))\nprint(df_result)","5fe72b6e":"# RandomForestRegressor\nrf = RandomForestRegressor(random_state=10)\nrf.fit(X_train,y_train)\ny_pred_rf = rf.predict(X_test)\nmse = mean_squared_error(y_test, y_pred_rf)\nrmse = np.sqrt(mean_squared_error(y_test, y_pred_rf))\nprint(\"Root Mean Squared Error: {:.2f}\".format(rmse))\ndf_result = df_result.append(pd.DataFrame([['RandomForestRegressor'\n                                            , rmse\n                                            , mse\n                                            ,'Baseline model'                               \n                                           ]], columns=df_result.columns))\nprint(df_result)","d008dea4":"# Get the list of variable based on missing data ratio\nfeatures_for_reg = missing_data[missing_data['Missing Ratio']<70].index.values.tolist()\n\n\n# Get Dummies\nX_all = pd.get_dummies(df_all_data[features_for_reg])\nX_all.fillna(0, inplace=True)\n\nX = X_all[0:len(df_train)]\ny = df_train['SalePrice']\n\n# Initiate train test split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n\nrf = RandomForestRegressor(random_state=3)\nrf.fit(X_train,y_train)\ny_pred_rf = rf.predict(X_test)\nmse = mean_squared_error(y_test, y_pred_rf)\nrmse = np.sqrt(mean_squared_error(y_test, y_pred_rf))\nprint(\"Root Mean Squared Error: {:.2f}\".format(rmse))\ndf_result = df_result.append(pd.DataFrame([['RandomForestRegressor'\n                                            , rmse\n                                            , mse\n                                            ,'Features with less than 70% missing data'                               \n                                           ]], columns=df_result.columns))\n\n\n\n# Calculate feature importances\nimportances = rf.feature_importances_\n# Sort feature importances in descending order\nindices = np.argsort(importances)[::-1]\n# Rearrange feature names so they match the sorted feature importances\nnames = [X_train.columns[i] for i in indices]\n\nprint(\"Most important:\\n\", names[:10])\nprint(\"Least important:\\n\", names[(-10):])\n","ed6051dd":"# Get the list of variable based on rf feature importance\nn_features = 45\nfeatures_for_reg = names[:n_features]\n\n\n# Run Linear Regression\nX_all = X_all[features_for_reg]\nX_all.fillna(0, inplace=True)\n\nX = X_all[0:len(df_train)]\ny = df_train['SalePrice']\n\n# Initiate train test split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n\nrf = RandomForestRegressor(random_state=3)\nrf.fit(X_train,y_train)\ny_pred_rf = rf.predict(X_test)\nmse = mean_squared_error(y_test, y_pred_rf)\nrmse = np.sqrt(mean_squared_error(y_test, y_pred_rf))\nprint(\"Root Mean Squared Error: {:.2f}\".format(rmse))\ndf_result = df_result.append(pd.DataFrame([['RandomForestRegressor'\n                                            , rmse\n                                            , mse\n                                            ,'Important features based on RF'                               \n                                           ]], columns=df_result.columns))\n\n\n\n# Calculate feature importances\nimportances = rf.feature_importances_\n# Sort feature importances in descending order\nindices = np.argsort(importances)[::-1]\n# Rearrange feature names so they match the sorted feature importances\nnames = [X_train.columns[i] for i in indices]\n\nprint(\"Most important:\\n\", names[:10])\nprint(\"Least important:\\n\", names[(-10):])","b4c2dc5c":"# New feature\ndf_all_data[\"OverallQual_Garage_GrLivArea\"] = df_all_data[\"OverallQual\"] * df_all_data[\"GarageArea\"] * df_all_data[\"GrLivArea\"]\n\n# Get Dummies\nX_all = pd.get_dummies(df_all_data)\nX_all.fillna(0, inplace=True)\n\nX = X_all[0:len(df_train)]\ny = df_train['SalePrice']\n\n# Initiate train test split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n\nrf = RandomForestRegressor(random_state=3)\nrf.fit(X_train,y_train)\ny_pred_rf = rf.predict(X_test)\nmse = mean_squared_error(y_test, y_pred_rf)\nrmse = np.sqrt(mean_squared_error(y_test, y_pred_rf))\nprint(\"Root Mean Squared Error: {:.2f}\".format(rmse))\ndf_result = df_result.append(pd.DataFrame([['RandomForestRegressor'\n                                            , rmse\n                                            , mse\n                                            ,'Features engineering'                               \n                                           ]], columns=df_result.columns))\n\n\n\n# Calculate feature importances\nimportances = rf.feature_importances_\n# Sort feature importances in descending order\nindices = np.argsort(importances)[::-1]\n# Rearrange feature names so they match the sorted feature importances\nnames = [X_train.columns[i] for i in indices]\n\nprint(\"Most important:\\n\", names[:10])\nprint(\"Least important:\\n\", names[(-10):])\n","6b05c12b":"# Get the list of variable based on missing data ratio\nfeatures_to_drop = missing_data[missing_data['Missing Ratio']>=70].index.values.tolist()\n\n\n# Get Dummies\nX_all = pd.get_dummies(df_all_data[df_all_data.columns.difference(features_to_drop)])\nX_all.fillna(0, inplace=True)\n\nX = X_all[0:len(df_train)]\ny = df_train['SalePrice']\n\n# Initiate train test split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n\nrf = RandomForestRegressor(random_state=3)\nrf.fit(X_train,y_train)\ny_pred_rf = rf.predict(X_test)\nmse = mean_squared_error(y_test, y_pred_rf)\nrmse = np.sqrt(mean_squared_error(y_test, y_pred_rf))\nprint(\"Root Mean Squared Error: {:.2f}\".format(rmse))\ndf_result = df_result.append(pd.DataFrame([['RandomForestRegressor'\n                                            , rmse\n                                            , mse\n                                            ,'Features Engineering & < 70% missing data'                               \n                                           ]], columns=df_result.columns))\n\n\n\n# Calculate feature importances\nimportances = rf.feature_importances_\n# Sort feature importances in descending order\nindices = np.argsort(importances)[::-1]\n# Rearrange feature names so they match the sorted feature importances\nnames = [X_train.columns[i] for i in indices]\n\nprint(\"Most important:\\n\", names[:10])\nprint(\"Least important:\\n\", names[(-10):])\n\n","5444750e":"lgb_model = LGBMRegressor().fit(X_train, y_train)\ny_pred_lgb = lgb_model.predict(X_test)\n\nmse = mean_squared_error(y_test, y_pred_lgb)\nrmse = np.sqrt(mean_squared_error(y_test, y_pred_lgb))\nprint(\"Root Mean Squared Error: {:.2f}\".format(rmse))\ndf_result = df_result.append(pd.DataFrame([['LGBMRegressor'\n                                            , rmse\n                                            , mse\n                                            ,'Features Engineering & < 70% missing data'                               \n                                           ]], columns=df_result.columns))","26895133":"# Grid search (narrow down to this grid after several iterations)\nlgb_params = {\"learning_rate\": [0.005, 0.01],\n               \"n_estimators\": [5000],\n               \"max_depth\": [4, 5],\n               \"feature_fraction\": [0.1, 0.2, 0.3],\n               \"colsample_bytree\": [0.8],\n               'num_leaves': [4, 5]}\n                              \nlgb_cv_model = GridSearchCV(lgb_model,\n                             lgb_params,\n                             cv=10,\n                             n_jobs=-1,\n                             verbose=2).fit(X_train, y_train)\n\nprint(lgb_cv_model.best_params_)\n\n# use best params\nlgb_tuned = LGBMRegressor(**lgb_cv_model.best_params_).fit(X_train, y_train)\ny_pred_lgb = lgb_tuned.predict(X_test)\n\nmse = mean_squared_error(y_test, y_pred_lgb)\nrmse = np.sqrt(mean_squared_error(y_test, y_pred_lgb))\nprint(\"Root Mean Squared Error: {:.2f}\".format(rmse))\n\ndf_result = df_result.append(pd.DataFrame([['LGBMRegressor'\n                                            , rmse\n                                            , mse\n                                            ,'Tuned model with Features Engineering & < 70% missing data'                               \n                                           ]], columns=df_result.columns))","fc837fe1":"gb_model = GradientBoostingRegressor()\ngb_model.fit(X_train, y_train)\ny_pred_gb = gb_model.predict(X_test)\n\nmse = mean_squared_error(y_test, y_pred_gb)\nrmse = np.sqrt(mean_squared_error(y_test, y_pred_gb))\nprint(\"Root Mean Squared Error: {:.2f}\".format(rmse))\n\ndf_result = df_result.append(pd.DataFrame([['GradientBoostingRegressor'\n                                            , rmse\n                                            , mse\n                                            ,'Features Engineering & < 70% missing data'                               \n                                           ]], columns=df_result.columns))","0ee404fa":"# Grid search (narrow down to this grid after several iterations)\ngb_params = {'n_estimators': [1000,5000],\n             'max_depth': [4,5],\n             'min_samples_split': [3,5],\n             'learning_rate': [0.005, 0.01],\n             'loss': ['ls']}\n                              \ngb_cv_model = GridSearchCV(gb_model,\n                           gb_params,\n                           cv=10,\n                           n_jobs=-1,\n                           verbose=2).fit(X_train, y_train)\n\nprint(gb_cv_model.best_params_)\n\n# use best params\ngb_tuned = GradientBoostingRegressor(**gb_cv_model.best_params_).fit(X_train, y_train)\ny_pred_gb = gb_tuned.predict(X_test)\n\nmse = mean_squared_error(y_test, y_pred_gb)\nrmse = np.sqrt(mean_squared_error(y_test, y_pred_gb))\nprint(\"Root Mean Squared Error: {:.2f}\".format(rmse))\n\ndf_result = df_result.append(pd.DataFrame([['GradientBoostingRegressor'\n                                            , rmse\n                                            , mse\n                                            ,'Tuned & Features Engineering & < 70% missing data'                               \n                                           ]], columns=df_result.columns))","eab15f0f":"df_all_data.describe().T","48e6a8d5":"df_all_data_scaled =  pd.DataFrame()\n\nscaler = StandardScaler()\n\nfor col in quan_var:    \n    scaler.fit((np.array(df_all_data[col])).reshape(-1, 1))\n    scaled_list = scaler.transform((np.array(df_all_data[col])).reshape(-1, 1))\n    \n    # Convert list of list to flat list before putting back to the df\n    df_all_data_scaled[col] = [item for elem in scaled_list for item in elem]\n    \ndf_all_data_scaled.describe().T\n","870212d4":"# New feature\ndf_all_data_scaled[\"OverallQual_Garage_GrLivArea\"] = df_all_data_scaled[\"OverallQual\"] * \\\n                                                     df_all_data_scaled[\"GarageArea\"] * \\\n                                                     df_all_data_scaled[\"GrLivArea\"]\n\n# Get Dummies\nX_all = pd.get_dummies(df_all_data_scaled)\nX_all.fillna(0, inplace=True)\n\nX = X_all[0:len(df_train)]\ny = df_train['SalePrice']\n\n# Initiate train test split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n\nrf_scaled = RandomForestRegressor(random_state=3)\nrf_scaled.fit(X_train,y_train)\ny_pred_rf = rf_scaled.predict(X_test)\nmse = mean_squared_error(y_test, y_pred_rf)\nrmse = np.sqrt(mean_squared_error(y_test, y_pred_rf))\nprint(\"Root Mean Squared Error: {:.2f}\".format(rmse))\ndf_result = df_result.append(pd.DataFrame([['RandomForestRegressor'\n                                            , rmse\n                                            , mse\n                                            ,'Features engineering & Scaled'                               \n                                           ]], columns=df_result.columns))\n\n\n\n# Calculate feature importances\nimportances = rf_scaled.feature_importances_\n# Sort feature importances in descending order\nindices = np.argsort(importances)[::-1]\n# Rearrange feature names so they match the sorted feature importances\nnames = [X_train.columns[i] for i in indices]\n\nprint(\"Most important:\\n\", names[:10])\nprint(\"Least important:\\n\", names[(-10):])\n","ac18f4fb":"# Get the list of variable based on missing data ratio\nfeatures_to_drop = missing_data[missing_data['Missing Ratio']>=70].index.values.tolist()\n\n\n# Get Dummies\nX_all = pd.get_dummies(df_all_data_scaled[df_all_data_scaled.columns.difference(features_to_drop)])\nX_all.fillna(0, inplace=True)\n\nlgb_scaled = LGBMRegressor().fit(X_train, y_train)\ny_pred_lgb = lgb_scaled.predict(X_test)\n\nmse = mean_squared_error(y_test, y_pred_lgb)\nrmse = np.sqrt(mean_squared_error(y_test, y_pred_lgb))\nprint(\"Root Mean Squared Error: {:.2f}\".format(rmse))\ndf_result = df_result.append(pd.DataFrame([['LGBMRegressor'\n                                            , rmse\n                                            , mse\n                                            ,'Features Engineering & Scaled & < 70% missing data'                               \n                                           ]], columns=df_result.columns))","e610fd24":"# Grid search (narrow down to this grid after several iterations)\nlgb_params = {\"learning_rate\": [0.005, 0.01],\n               \"n_estimators\": [5000],\n               \"max_depth\": [4, 5],\n               \"feature_fraction\": [0.1, 0.2, 0.3],\n               \"colsample_bytree\": [0.8],\n               'num_leaves': [4, 5]}\n                              \nlgb_cv_model = GridSearchCV(lgb_scaled,\n                             lgb_params,\n                             cv=10,\n                             n_jobs=-1,\n                             verbose=2).fit(X_train, y_train)\n\nprint(lgb_cv_model.best_params_)\n\n# use best params\nlgb_scaled_tuned = LGBMRegressor(**lgb_cv_model.best_params_).fit(X_train, y_train)\ny_pred_lgb_scaled_tuned = lgb_scaled_tuned.predict(X_test)\n\nmse = mean_squared_error(y_test, y_pred_lgb_scaled_tuned)\nrmse = np.sqrt(mean_squared_error(y_test, y_pred_lgb_scaled_tuned))\nprint(\"Root Mean Squared Error: {:.2f}\".format(rmse))\n\ndf_result = df_result.append(pd.DataFrame([['LGBMRegressor'\n                                            , rmse\n                                            , mse\n                                            ,'Tuned model with Features Engineering & Scaled & < 70% missing data'                               \n                                           ]], columns=df_result.columns))","96c0b1cf":"df_result['RMSE'] = df_result['RMSE'].astype(int)\ndf_result['MSE'] = df_result['MSE'].astype(int)\ndf_result","6a22935a":"# # Predict using rf\n# X_test = X_all.iloc[len(df_train):len(X_all)]\n# y_pred_rf = rf.predict(X_test)\n\n# # Predict using lgb\n# X_test = X_all.iloc[len(df_train):len(X_all)]\n# y_pred_lgb = lgb_model.predict(X_test)\n\n# # Predict using lgb_tuned\n# X_test = X_all.iloc[len(df_train):len(X_all)]\n# y_pred_lgb_tuned = lgb_tuned.predict(X_test)\n\n# Predict using rf_scaled\nX_test = X_all.iloc[len(df_train):len(X_all)]\ny_pred_rf_scaled = rf_scaled.predict(X_test)\n\n# Predict using lgb_scaled\nX_test = X_all.iloc[len(df_train):len(X_all)]\ny_pred_lgb_tuned = lgb_scaled_tuned.predict(X_test)","061ea92f":"# Submission\nsub = pd.DataFrame()\nsub['Id'] = df_test['Id']\nsub['SalePrice'] = y_pred_lgb_tuned\nsub.to_csv('submission.csv',index=False)","5abadc95":"### Features - Importance","b484fb7c":"### Prediction Submission","dbc81b4e":"### Feature Engineering and Missing Value","3a0fa381":"## 4.3. Feature Scaling <a id='4.3'><\/a>","ab5917d4":"## 3.2. Independent Variables<a id='3.2'><\/a>","1b88d198":"### 3.2.2. Quantitative - Qualitative Features<a id='3.2.2'><\/a>","c1d059a8":"# 04. Prediction <a id='04'><\/a>","84c24715":"# 01. Import Library<a id='01'><\/a>","628e85a6":"### Random Forest","d73e8f91":"# 05. Results<a id='05'><\/a>","bc576755":"### 3.1.3. Use seaborn<a id='3.1.3'><\/a>","a3329592":"### 3.1.1. Use matplotlib<a id='3.1.1'><\/a>","dda524bb":"## 4.1. Baseline Model <a id='4.1'><\/a>","df420296":"### Gradient Boosting Regressor","476abb6b":"# 02. Load Data <a id='02'><\/a>","2ba1f8a4":"### Linear Regression","0e77d4e5":"### 3.1.2. Use plotly_express<a id='3.1.2'><\/a>","a421e691":"### Features - Missing Ratio","718cab95":"### Light GBM best params","3da22952":"## 4.2. Feature Engineering <a id='4.2'><\/a>","e8f223d1":"### GB with Best Params","b6d2f714":"# Table of Contents\n\n[01. Import Library](#01)<br>\n\n[02. Load Data](#02)<br>\n\n[03. Exploratory Data Analysis (EDA)](#03)<br>\n\n&nbsp;&nbsp;&nbsp;[3.1. Dependent Variable](#3.1)<br>\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[3.1.1. Use matplotlib](#3.1.1)<br>\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[3.1.2. Use plotly_express](#3.1.2)<br>\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[3.1.3. Use seaborn](#3.1.3)<br>\n\n&nbsp;&nbsp;&nbsp;[3.2. Independent Variables](#3.2)<br>\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[3.2.1. Outliers](#3.2.1)<br>\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[3.2.2. Quantitative - Qualitative Features](#3.2.2)<br>\n\n[04. Prediction](#04)<br>\n\n&nbsp;&nbsp;&nbsp;[4.1. Baseline Model](#4.1)<br>\n&nbsp;&nbsp;&nbsp;[4.2. Feature Engineering](#4.2)<br>\n&nbsp;&nbsp;&nbsp;[4.3. Feature Scaling](#4.3)<br>\n\n\n[05. Results](#05)<br>\n\n","d073e563":"# 03. Exploratory Data Analysis (EDA)<a id='03'><\/a>","d8db71bb":"### Light GBM","732d4b5c":"## 3.1. Dependent Variable<a id='3.1'><\/a>","b7a47d1a":"### 3.2.1. Outliers<a id='3.2.1'><\/a>"}}