{"cell_type":{"0f9e962f":"code","27c065a4":"code","86e5f75b":"code","14f41d9a":"code","705bbdbc":"code","2c4c9c90":"code","0af2db17":"code","eaa88da0":"code","07b18015":"code","b306b035":"code","e28d3adb":"code","965ba59d":"code","1659f704":"code","f23518a0":"code","1bfa7b65":"code","62ed5dba":"code","3668140f":"code","1e71b5bd":"code","901b4589":"code","e35eb0ce":"code","3f4b2cbb":"code","0b0c8903":"code","4a25b8dc":"code","8f86f911":"code","4978dac0":"code","369c1fcc":"code","affaa3f5":"code","b23fb4c9":"code","603c0a03":"code","dda2f15b":"code","e6c3bd86":"code","5a56c637":"code","9d4c4093":"markdown","d927a3d7":"markdown","95511fe9":"markdown","22265dcc":"markdown","3ec881be":"markdown","68fe7e92":"markdown","256c1cc3":"markdown","f91ef02d":"markdown"},"source":{"0f9e962f":"import numpy as np \nimport pandas as pd \nfrom nltk.corpus import stopwords\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.preprocessing import LabelBinarizer\nfrom scipy.sparse import hstack, csr_matrix\nfrom catboost import CatBoostRegressor\nfrom sklearn.metrics import mean_squared_log_error\nimport re\nimport gc\nimport matplotlib.pyplot as plt\n\n\n# Pandas setting to display more dataset rows and columns\npd.set_option('display.max_rows', 100)\npd.set_option('display.max_columns', 500)\npd.set_option('display.max_colwidth', None)\npd.set_option('display.float_format', lambda x: '%.5f' % x)\n\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","27c065a4":"!apt-get install p7zip\n!p7zip -d -f -k \/kaggle\/input\/mercari-price-suggestion-challenge\/train.tsv.7z\n!unzip -o \/kaggle\/input\/mercari-price-suggestion-challenge\/sample_submission_stg2.csv.zip\n!unzip -o \/kaggle\/input\/mercari-price-suggestion-challenge\/test_stg2.tsv.zip","86e5f75b":"train = pd.read_csv(\"train.tsv\", low_memory=False, sep='\\t')#, nrows=100000)\ntest = pd.read_csv(\"test_stg2.tsv\", low_memory=False, sep='\\t')#, nrows=100000)\ntrain.info(memory_usage=\"deep\")","14f41d9a":"test.info(memory_usage=\"deep\")","705bbdbc":"train.head()","2c4c9c90":"# Amount of missing values in both train and test datasets\ndf = pd.concat([train.drop([\"train_id\", \"price\"], axis=1).isna().sum(), test.drop([\"test_id\"], axis=1).isna().sum()], axis=1)\ndf.columns = [\"Train\", \"Test\"]\n\nfig, axs = plt.subplots(nrows=2, ncols=6, figsize=(16,6))\n\ni=0\nfig.set_facecolor('white')\nfor r in np.arange(1):\n    for c in np.arange(6):\n        explode = (0, 0.1)\n        axs[r, c].pie([len(train)-df.iloc[i][\"Train\"], df.iloc[i][\"Train\"]], \n                      labels=[\"Filled\\nvalues\", \"Missing\\nvalues\"],\n                      explode=explode, autopct=\"%1.1f%%\", labeldistance=1.2,\n                      colors=[\"teal\", \"salmon\"], startangle=55)\n        axs[r+1, c].pie([len(test)-df.iloc[i][\"Test\"], df.iloc[i][\"Test\"]], \n                      labels=[\"Filled\\nvalues\", \"Missing\\nvalues\"],\n                      explode=explode, autopct=\"%1.1f%%\", labeldistance=1.2,\n                      colors=[\"teal\", \"salmon\"], startangle=55)\n        axs[r, c].set_title(\"Train: \" + df.index[i], pad=10)\n        axs[r+1, c].set_title(\"Test: \" + df.index[i], pad=10)\n        i += 1\nfig.suptitle(\"Amount of missing values in train and test datasets\")\nplt.show();","0af2db17":"# Amount of unique values in both train and test datasets\ndf = pd.concat([train.drop([\"train_id\", \"price\"], axis=1).nunique(), test.drop([\"test_id\"], axis=1).nunique()], axis=1)\ndf.columns = [\"Train\", \"Test\"]\ndf.T","eaa88da0":"# Top 5 names in each dataset\ntrain[\"name\"].value_counts()[:5], test[\"name\"].value_counts()[:5]","07b18015":"# Top 5 brand names in each dataset\ntrain[\"brand_name\"].value_counts()[:5], test[\"brand_name\"].value_counts()[:5]","b306b035":"# Top 5 category names\ntrain[\"category_name\"].value_counts()[:5], test[\"category_name\"].value_counts()[:5]","e28d3adb":"# Top 5 item descriptions in each dataset\ntrain[\"item_description\"].value_counts()[:5], test[\"item_description\"].value_counts()[:5]","965ba59d":"# Item condition distribution\n\nfig, ax = plt.subplots(figsize=(12, 7))\nx = np.arange(1, 6, 1)\ny1 = train[\"item_condition_id\"].value_counts().values\/len(train[\"item_condition_id\"])*100\ny2 = test[\"item_condition_id\"].value_counts().values\/len(test[\"item_condition_id\"])*100\nbars1 = ax.bar(x-0.2,\n               y1,\n               width=0.4, color=\"cornflowerblue\", label=\"Train dataset\", edgecolor=\"black\")\nbars2 = ax.bar(x+0.2,\n               y2,\n               width=0.4, color=\"palevioletred\", label=\"Test dataset\", edgecolor=\"black\")\nax.set_title(\"Item condition values distribution in the both datasets\", fontsize=20, pad=15)\nax.set_ylabel(\"Fraction of all values\", fontsize=15, labelpad=10)\nax.set_xlabel(\"Value\", fontsize=15, labelpad=10)\nax.set_yticks(ax.get_yticks())\nax.set_yticklabels([str(int(x))+\"%\" for x in ax.get_yticks()])\nax.tick_params(axis=\"x\", labelsize=12)\nax.tick_params(axis=\"y\", labelsize=12)\nax.bar_label(bars1, [str(round(x, 2))+\"%\" for x in y1], padding=3, fontsize=12)\nax.bar_label(bars2, [str(round(x, 2))+\"%\" for x in y2], padding=3, fontsize=12)\nax.grid(axis=\"y\")\nax.margins(0.05, 0.1)\nax.legend(fontsize=12)\nplt.show();","1659f704":"# Shipment distribution\n\nfig, ax = plt.subplots(figsize=(7, 7))\nx = np.arange(0, 2, 1)\ny1 = train[\"shipping\"].value_counts().values\/len(train[\"shipping\"])*100\ny2 = test[\"shipping\"].value_counts().values\/len(test[\"shipping\"])*100\nbars1 = ax.bar(x-0.15,\n               y1,\n               width=0.3, color=\"cornflowerblue\", label=\"Train dataset\", edgecolor=\"black\")\nbars2 = ax.bar(x+0.15,\n               y2,\n               width=0.3, color=\"palevioletred\", label=\"Test dataset\", edgecolor=\"black\")\nax.set_title(\"Item condition values distribution in the both datasets\", fontsize=20, pad=15)\nax.set_ylabel(\"Fraction of all values\", fontsize=15, labelpad=15)\nax.set_xlabel(\"Value\", fontsize=15, labelpad=15)\nax.set_yticks(ax.get_yticks())\nax.set_yticklabels([str(int(x))+\"%\" for x in ax.get_yticks()])\nax.tick_params(axis=\"x\", labelsize=12)\nax.tick_params(axis=\"y\", labelsize=12)\nax.set_xticks([0, 1])\nax.set_xticklabels([\"Shipment is included\", \"Shipment is not included\"])\nax.bar_label(bars1, [str(round(x, 2))+\"%\" for x in y1], padding=3, fontsize=13)\nax.bar_label(bars2, [str(round(x, 2))+\"%\" for x in y2], padding=3, fontsize=13)\nax.grid(axis=\"y\")\nax.margins(0.05, 0.1)\nax.legend(fontsize=12)\nplt.show();","f23518a0":"# Price distribution\nfig, axs = plt.subplots(ncols=1, nrows=2, figsize=(14, 12))\n\nplt.subplots_adjust(hspace = 0.4)\n\naxs[0].hist(train[\"price\"], bins=100, color=\"sandybrown\", edgecolor=\"black\")\naxs[0].set_title(\"Price distribution\", fontsize=20, pad=5)\naxs[0].set_xlabel(\"Price\", fontsize=15, labelpad=5)\naxs[0].set_ylabel(\"Amount of items\", fontsize=15, labelpad=5)\naxs[0].set_yticks(np.arange(0, 1000000, 100000))\naxs[0].tick_params(axis=\"y\", labelsize=13)\naxs[0].tick_params(axis=\"x\", labelsize=13)\naxs[0].grid(axis=\"y\")\naxs[0].margins(0.05, 0.05)\n\naxs[1].hist(train[train[\"price\"] < 150][\"price\"], bins=np.arange(0, 150, 2.5), color=\"rosybrown\", edgecolor=\"black\")\naxs[1].set_title(\"Price distribution from 0 to 150\", fontsize=20, pad=5)\naxs[1].set_xlabel(\"Price\", fontsize=15, labelpad=5)\naxs[1].set_ylabel(\"Amount of items\", fontsize=15, labelpad=5)\naxs[1].set_xticks(np.arange(0, 180, 10))\naxs[1].set_yticks(np.arange(0, 270000, 20000))\naxs[1].tick_params(axis=\"y\", labelsize=13)\naxs[1].tick_params(axis=\"x\", labelsize=13)\naxs[1].grid(axis=\"y\")\naxs[1].margins(0.05, 0.05)","1bfa7b65":"# Price dependance on item condition\n\nfig, ax = plt.subplots(figsize=(14, 7))\nx = np.arange(1, 6, 1)\ny1 = train.groupby(\"item_condition_id\")[\"price\"].mean()\ny2 = train.groupby(\"item_condition_id\")[\"price\"].median()\n\nbars1 = ax.bar(x-0.2,\n               y1,\n               width=0.4, color=\"mediumorchid\", label=\"Mean price\", edgecolor=\"black\")\nbars2 = ax.bar(x+0.2,\n               y2,\n               width=0.4, color=\"lightseagreen\", label=\"Median price\", edgecolor=\"black\")\n\nax.set_title(\"Mean and median price for different item condition\", fontsize=20, pad=15)\nax.set_ylabel(\"Price\", fontsize=15, labelpad=15)\nax.set_xlabel(\"Item condition id\", fontsize=15, labelpad=10)\n# ax.set_yticklabels([str(int(x))+\"%\" for x in ax.get_yticks()])\nax.tick_params(axis=\"x\", labelsize=13)\nax.tick_params(axis=\"y\", labelsize=13)\nax.bar_label(bars1, [str(round(x, 2)) for x in y1], padding=3, fontsize=13)\nax.bar_label(bars2, [str(round(x, 2)) for x in y2], padding=3, fontsize=13)\nax.grid(axis=\"y\")\nax.margins(0.05, 0.1)\nax.legend(fontsize=13)\nplt.show();","62ed5dba":"# Top 20 brand prices\n\nfig, ax = plt.subplots(figsize=(14, 7))\n\nbrands = train[\"brand_name\"].value_counts().index[:20]\nmean_prices = train.groupby(\"brand_name\")[\"price\"].mean()\nmedian_prices = train.groupby(\"brand_name\")[\"price\"].median()\nx = np.arange(0, len(brands), 1)\n\nbars1 = ax.bar(x-0.2,\n               mean_prices.loc[brands].values,\n               width=0.4, color=\"cornflowerblue\", label=\"Mean price\", edgecolor=\"black\")\nbars2 = ax.bar(x+0.2,\n               median_prices.loc[brands].values,\n               width=0.4, color=\"darkorange\", label=\"Median price\", edgecolor=\"black\")\n\nax.set_title(\"Mean and median price for items of the top 20 brands\", fontsize=25, pad=15)\nax.set_ylabel(\"Price\", fontsize=15, labelpad=15)\nax.set_xlabel(\"Brand name\", fontsize=15, labelpad=10)\nax.set_xticks(x)\nax.set_xticklabels(brands, rotation = 60, ha=\"right\", rotation_mode='anchor')\nax.tick_params(axis=\"x\", labelsize=15)\nax.tick_params(axis=\"y\", labelsize=15)\nax.grid(axis=\"y\")\nax.margins(0.025, 0.05)\nax.legend(fontsize=15)\nplt.show();","3668140f":"df = pd.DataFrame()\ndf[\"descr_len\"] = train[\"item_description\"].str.len()\nprint(f\"Train dataset desription length info:\\n{df['descr_len'].describe(percentiles=[.1, .25, .5, .75, .90, .99])}\")\n\ndf = pd.DataFrame()\ndf[\"descr_len\"] = test[\"item_description\"].str.len()\nprint(f\"\\nTest dataset desription length info:\\n{df['descr_len'].describe(percentiles=[.1, .25, .5, .75, .90, .99])}\")","1e71b5bd":"df = pd.DataFrame()\ndf[\"descr_len\"] = train[\"item_description\"].str.len()\ndf[\"price\"] = train[\"price\"]\n\nfig, ax = plt.subplots(figsize=(14, 7))\n\nplot1 = ax.plot(df.groupby(\"descr_len\")[\"price\"].mean().index,\n                df.groupby(\"descr_len\")[\"price\"].mean().values,\n                color=\"mediumvioletred\", label=\"Mean price\")\n\n\nplot1 = ax.plot(df.groupby(\"descr_len\")[\"price\"].median().index,\n                df.groupby(\"descr_len\")[\"price\"].median().values,\n                color=\"dodgerblue\", label=\"Mean price\")\n\nax.set_title(\"Price distribution\", fontsize=20, pad=5)\nax.set_xlabel(\"Item description length\", fontsize=15, labelpad=5)\nax.set_ylabel(\"Price\", fontsize=15, labelpad=5)\nax.tick_params(axis=\"y\", labelsize=13)\nax.tick_params(axis=\"x\", labelsize=13)\nax.grid(axis=\"y\")\nax.legend(fontsize=15)\nax.margins(0.05, 0.05)","901b4589":"# Lets make the same plot but with 5 chars wide bins \ndf = pd.DataFrame()\ndf[\"descr_len\"] = train[\"item_description\"].str.len()\ndf[\"price\"] = train[\"price\"]\ndf[\"descr_len\"] = pd.cut(df[\"descr_len\"], np.arange(0, 1055, 5), right=False)\ndf = df.groupby(\"descr_len\")[\"price\"].mean()\ndf.index = np.arange(5, 1055, 5)\n\nfig, ax = plt.subplots(figsize=(14, 7))\n\nplot1 = ax.plot(df.index, df.values, color=\"mediumvioletred\", label=\"Mean price\")\n\ndf = pd.DataFrame()\ndf[\"descr_len\"] = train[\"item_description\"].str.len()\ndf[\"price\"] = train[\"price\"]\ndf[\"descr_len\"] = pd.cut(df[\"descr_len\"], np.arange(0, 1055, 5), right=False)\ndf = df.groupby(\"descr_len\")[\"price\"].median()\ndf.index = np.arange(5, 1055, 5)\n\nplot2 = ax.plot(df.index, df.values, color=\"dodgerblue\", label=\"Median price\")\n\nax.set_title(\"Price distribution\", fontsize=20, pad=5)\nax.set_xlabel(\"Item description length\", fontsize=15, labelpad=5)\nax.set_ylabel(\"Price\", fontsize=15, labelpad=5)\nax.tick_params(axis=\"y\", labelsize=13)\nax.tick_params(axis=\"x\", labelsize=13)\nax.grid(axis=\"y\")\nax.legend(fontsize=15)\nax.margins(0.05, 0.05)","e35eb0ce":"# Getting rid of rows with zero or negative price and resetting DataFrame index\ntrain.drop(train[train[\"price\"] <= 0].index, inplace=True)\ntrain.reset_index(inplace=True, drop=True)","3f4b2cbb":"def process_data(df):\n    # Category_name column splitting into three columns\n    df[[\"First_category\", \"Second_category\", \"Third_category\"]] = df[\"category_name\"].str.split('\/', 2, expand=True)\n\n    # Deleting unneeded column\n    df.drop(\"category_name\", axis=1, inplace=True)\n\n    # Adding new features indicating missing data\n    df[\"item_description\"] = df[\"item_description\"].replace({\"No description yet\": np.nan})\n    df[\"Category_was_missing\"] = df[\"First_category\"].isna()\n    df[\"Brand_was_missing\"] = df[\"brand_name\"].isna()\n    df[\"Description_was_missing\"] = df[\"item_description\"].isna()\n\n    # Replacing NaN values with \"missing\" string\n    for i in [\"brand_name\", \"First_category\", \"Second_category\", \"Third_category\", \"item_description\"]:\n        df[i] = df[i].fillna(\"missing\")\n\n    # Adding features indicating that there was a price tag in item description or name\n    df[\"Price_was_in_description\"] = df[\"item_description\"].str.contains(\"\\[rm\\]\")\n    df[\"Price_was_in_name\"] = df[\"name\"].str.contains(\"\\[rm\\]\")\n    \n    # Adding a new column with description text length devided into 5 character intervals\n    df[\"descr_len\"] = df[\"item_description\"].str.len()\n    df[\"descr_len\"] = pd.cut(df[\"descr_len\"], np.arange(0, 1055, 5), right=False)\n    df[\"descr_len\"] = df[\"descr_len\"].astype(\"string\")\n\n    # Stopwords import from nltk \n    stop_words = set(stopwords.words(\"english\"))\n\n    # Processing of name and description columns\n    for column in [\"item_description\", \"brand_name\", \"name\"]:\n        processed_column = []\n        for text_row in df[column]:\n            text_row = text_row.replace('[rm]', '')\n            text_row = re.sub('[^A-Za-z0-9]+', ' ', text_row)\n            if column != \"brand_name\":\n                text_row = ' '.join(word for word in text_row.lower().split() if word not in stop_words)\n            processed_column.append(text_row.strip())\n        df[column] = processed_column\n\n    # Processing of category columns\n    for column in [\"First_category\", \"Second_category\", \"Third_category\"]:    \n        processed_column = []\n        for text_row in df[column]:\n            text_row = text_row.replace(' ','')\n            text_row = text_row.replace('&','_')\n            text_row = re.sub('[^A-Za-z0-9_]+', ' ', text_row)\n            processed_column.append(text_row.lower().strip())\n        df[column] = processed_column\n\n    return df","0b0c8903":"train = process_data(train)","4a25b8dc":"def make_tokens_count_plot(count_vercorizer, matrix, column_name, set_name=\"train\"):\n    # Using one color for train set plots and the other for test\/valid set\n    if set_name == \"train\":\n        color = \"lightcoral\"\n    else:\n        color = \"steelblue\"\n        \n    df = pd.DataFrame()\n    df[\"tokens\"] = count_vercorizer.get_feature_names()\n    df[\"counts\"] = np.asarray(matrix.sum(axis=0))[0]\n    x = df.sort_values(\"counts\", axis=0, ascending=False).head(50)[\"tokens\"]\n    y = df.sort_values(\"counts\", axis=0, ascending=False).head(50)[\"counts\"]\n\n    fig, ax = plt.subplots(figsize=(15, 6))\n\n    ax.bar(x, y, color=color, edgecolor=\"black\")\n    ax.set_title(f\"Most popular words in the {column_name} column of the {set_name} dataset\", fontsize=20, pad=15)\n    ax.set_ylabel(\"Count\", fontsize=14, labelpad=15)\n    ax.set_xlabel(\"Word\", fontsize=14, labelpad=10)\n    ax.set_xticks(x)\n    ax.set_xticklabels(x, rotation = 60, ha=\"right\", rotation_mode='anchor')\n    ax.tick_params(axis=\"x\", labelsize=14)\n    ax.tick_params(axis=\"y\", labelsize=14)\n    ax.grid(axis=\"y\")\n    ax.margins(0.025, 0.05)\n    plt.show();","8f86f911":"def get_transformed_train_valid_data(df, y, cat_features):\n    \"\"\"\n    Vectorizes and return train and valid data. For valid data transformation get_transformed_test_data() is called.\n    \"\"\"    \n    X_train, X_valid, y_train, y_valid = train_test_split(df, y, test_size=0.2, random_state=42)\n    \n    label_binarizers = []\n    binarized_columns = []\n    count_vercorizers = []\n    vectorized_columns = []\n    \n    for column in cat_features:\n        binarizer = LabelBinarizer(sparse_output=True)\n        binarized_column = binarizer.fit_transform(X_train[column])\n        label_binarizers.append(binarizer)\n        binarized_columns.append(binarized_column)\n        \n  \n    vectorizer = CountVectorizer(min_df=7, max_features=20000)\n    vectorized_column = vectorizer.fit_transform(X_train[\"name\"])\n    count_vercorizers.append(vectorizer)\n    vectorized_columns.append(vectorized_column)\n    make_tokens_count_plot(vectorizer, vectorized_column, \"name\", \"train\")\n    \n    \n    vectorizer = CountVectorizer(min_df=15, ngram_range=(1, 2), max_features=40000)\n    vectorized_column = vectorizer.fit_transform(X_train[\"item_description\"])\n    count_vercorizers.append(vectorizer)\n    vectorized_columns.append(vectorized_column)\n    make_tokens_count_plot(vectorizer, vectorized_column, \"item_description\", \"train\")\n    \n    vectorizer = CountVectorizer(min_df=30, ngram_range=(3, 3), max_features=5000)\n    vectorized_column = vectorizer.fit_transform(X_train[\"item_description\"])\n    count_vercorizers.append(vectorizer)\n    vectorized_columns.append(vectorized_column)\n    make_tokens_count_plot(vectorizer, vectorized_column, \"item_description\", \"train\")\n    print(f\"Name columns vectorized shape is {vectorized_columns[0].shape}\")\n    print(f\"Item_description columns vectorized shape is {vectorized_columns[1].shape}\")\n    print(f\"Item_description columns vectorized (1,3) shape is {vectorized_columns[2].shape}\")\n    \n\n    X_train_stack = hstack((binarized_columns[0], binarized_columns[1], binarized_columns[2],\n                            binarized_columns[3], binarized_columns[4], binarized_columns[5],\n                            binarized_columns[6], binarized_columns[7], binarized_columns[8],\n                            binarized_columns[9], binarized_columns[10], binarized_columns[11],\n#                             vectorized_columns[0])).tocsr()                            \n                            vectorized_columns[0], vectorized_columns[1], vectorized_columns[2])).tocsr()\n    \n    X_valid_stack = get_transformed_test_data(X_valid, cat_features, label_binarizers, count_vercorizers)\n\n   \n    return X_train_stack, X_valid_stack, y_train, y_valid, label_binarizers, count_vercorizers\n\ndef get_transformed_test_data(df, cat_features, label_binarizers, count_vercorizers):\n    \"\"\"\n    Vectorizes and return test data. Can be used to vectorize valid data as well.\n    \"\"\"    \n    binarized_columns = []\n    vectorized_columns = []\n    \n    for num, column in enumerate(cat_features):\n        binarizer = label_binarizers[num]\n        binarized_column = binarizer.transform(df[column])\n        binarized_columns.append(binarized_column)\n        \n    vectorizer = count_vercorizers[0]\n    vectorized_column = vectorizer.transform(df[\"name\"])\n    vectorized_columns.append(vectorized_column)\n    make_tokens_count_plot(vectorizer, vectorized_column, \"name\", \"valid\/test\")\n    \n    vectorizer = count_vercorizers[1]\n    vectorized_column = vectorizer.transform(df[\"item_description\"])\n    vectorized_columns.append(vectorized_column)\n    make_tokens_count_plot(vectorizer, vectorized_column, \"item_description\", \"valid\/test\")\n    \n    vectorizer = count_vercorizers[2]\n    vectorized_column = vectorizer.transform(df[\"item_description\"])\n    vectorized_columns.append(vectorized_column)\n    make_tokens_count_plot(vectorizer, vectorized_column, \"item_description\", \"valid\/test\")    \n    print(f\"Name columns vectorized shape is {vectorized_columns[0].shape}\")\n    print(f\"Item_description columns vectorized shape is {vectorized_columns[1].shape}\")\n    print(f\"Item_description columns vectorized (1,3) shape is {vectorized_columns[2].shape}\")\n    \n\n    X_test_stack = hstack((binarized_columns[0], binarized_columns[1], binarized_columns[2],\n                           binarized_columns[3], binarized_columns[4], binarized_columns[5],\n                           binarized_columns[6], binarized_columns[7], binarized_columns[8],\n                           binarized_columns[9], binarized_columns[10], binarized_columns[11],\n#                            vectorized_columns[0])).tocsr()\n                           vectorized_columns[0], vectorized_columns[1], vectorized_columns[2])).tocsr()\n\n   \n    return X_test_stack\n\ndef get_transformed_train_data(df, cat_features):\n    \"\"\"\n    Vectorizes and return train data only\n    \"\"\"\n    \n    label_binarizers = []\n    binarized_columns = []\n    count_vercorizers = []\n    vectorized_columns = []\n    \n    for column in cat_features:\n        binarizer = LabelBinarizer(sparse_output=True)\n        binarized_column = binarizer.fit_transform(df[column])\n        label_binarizers.append(binarizer)\n        binarized_columns.append(binarized_column)\n        \n    vectorizer = CountVectorizer(min_df=7, max_features=20000)\n    vectorized_column = vectorizer.fit_transform(df[\"name\"])\n    count_vercorizers.append(vectorizer)\n    vectorized_columns.append(vectorized_column)\n    make_tokens_count_plot(vectorizer, vectorized_column, \"name\", \"train\")\n    \n    \n    vectorizer = CountVectorizer(min_df=15, ngram_range=(1, 2), max_features=40000)\n    vectorized_column = vectorizer.fit_transform(df[\"item_description\"])\n    count_vercorizers.append(vectorizer)\n    vectorized_columns.append(vectorized_column)\n    make_tokens_count_plot(vectorizer, vectorized_column, \"item_description\", \"train\")\n    \n    vectorizer = CountVectorizer(min_df=30, ngram_range=(3, 3), max_features=5000)\n    vectorized_column = vectorizer.fit_transform(df[\"item_description\"])\n    count_vercorizers.append(vectorizer)\n    vectorized_columns.append(vectorized_column)\n    make_tokens_count_plot(vectorizer, vectorized_column, \"item_description\", \"train\")\n    print(f\"Name columns vectorized shape is {vectorized_columns[0].shape}\")\n    print(f\"Item_description columns vectorized shape is {vectorized_columns[1].shape}\")\n    print(f\"Item_description columns vectorized (1,3) shape is {vectorized_columns[2].shape}\")\n    \n\n    X_train_stack = hstack((binarized_columns[0], binarized_columns[1], binarized_columns[2],\n                            binarized_columns[3], binarized_columns[4], binarized_columns[5],\n                            binarized_columns[6], binarized_columns[7], binarized_columns[8],\n                            binarized_columns[9], binarized_columns[10], binarized_columns[11],\n#                             vectorized_columns[0])).tocsr()                            \n                            vectorized_columns[0], vectorized_columns[1], vectorized_columns[2])).tocsr()\n\n   \n    return X_train_stack, label_binarizers, count_vercorizers","4978dac0":"cat_features = [\"item_condition_id\", \"First_category\", \"Second_category\", \"Third_category\",\n                \"shipping\", \"brand_name\", \"Description_was_missing\", \"Price_was_in_name\",\n                \"Price_was_in_description\", \"Brand_was_missing\", \"Category_was_missing\",\n                \"descr_len\"]\n\n# # Get train and valid data to measure models performance\n# X_train, X_valid, y_train, y_valid, label_binarizers, count_vercorizers = \\\n# get_transformed_train_valid_data(train.drop([\"train_id\", \"price\"], axis=1), \n#                                  np.log1p(train[\"price\"]), cat_features)\n\nX_train, label_binarizers, count_vercorizers = get_transformed_train_data(train.drop([\"train_id\", \"price\"], axis=1), cat_features)\ny_train = np.log1p(train[\"price\"])","369c1fcc":"# Deleting train dataset to free memory\nfor column in train.columns:\n    train.drop(column, axis=1, inplace=True)\n    \ndel train\ngc.collect()","affaa3f5":"def rmsle(y_true, y_preds):\n    return np.sqrt(mean_squared_log_error(y_true, y_preds))\n\n\ndef get_scores(model, X_train, X_valid, y_train, y_valid):\n    train_preds = model.predict(X_train)\n    val_preds = model.predict(X_valid)\n    scores = {\"Training MAE\": mean_absolute_error(y_train, train_preds),\n              \"Validation MAE\": mean_absolute_error(y_valid, val_preds),\n              \"Training RMSLE\": rmsle(y_train, train_preds),\n              \"Validation RMSLE\": rmsle(y_valid, val_preds),\n              \"Training R^2\": model.score(X_train, y_train),\n              \"Validation R^2\": model.score(X_valid, y_valid)}\n    return scores","b23fb4c9":"%%time\n\nmodel = CatBoostRegressor(random_state=42, thread_count=4, verbose=100,\n                          iterations=7000, loss_function='RMSE', learning_rate=0.45)\n\nmodel.fit(X_train, y_train)","603c0a03":"# Deleting train dataset to free memory\ndel X_train, y_train\ngc.collect()","dda2f15b":"# Predictions dataframe initialization\npreds = pd.DataFrame(columns = [\"test_id\", \"price\"])\npreds[\"test_id\"] = test[\"test_id\"]","e6c3bd86":"test = process_data(test.drop(\"test_id\", axis=1))\n\nX_test = get_transformed_test_data(test, cat_features, label_binarizers, count_vercorizers)","5a56c637":"preds[\"price\"] = np.expm1(model.predict(X_test))\npreds.to_csv('submission.csv', index=False)\npreds.head()","9d4c4093":"# **Metrics**","d927a3d7":"# **Data management**","95511fe9":"As per two plots above we can see that the datasets are pretty balanced by categorical features.","22265dcc":"Lets check the item description length in the both datasets.","3ec881be":"# **Model training**","68fe7e92":"* train_id or test_id - the id of the listing\n* name - the title of the listing. Note that we have cleaned the data to remove text that look like prices (e.g. 20 USD) to avoid leakage. These removed prices are represented as [rm]\n* item_condition_id - the condition of the items provided by the seller\n* category_name - category of the listing\n* brand_name\n* price - the price that the item was sold for. This is the target variable that you will predict. The unit is USD. This column doesn't exist in test.tsv since that is what you will predict.\n* shipping - 1 if shipping fee is paid by seller and 0 by buyer\n* item_description - the full description of the item. Note that we have cleaned the data to remove text that look like prices (e.g. 20 USD) to avoid leakage. These removed prices are represented as [rm]","256c1cc3":"# **Train data predictions and submission**","f91ef02d":"# **EDA**"}}