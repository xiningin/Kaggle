{"cell_type":{"fefe3b39":"code","d961528c":"code","a6523301":"code","09ebe59a":"code","2247d17c":"code","05293a38":"code","bf3d7afc":"code","867bc882":"code","677e8f59":"code","14c8f6a8":"code","84f02a56":"code","42d679aa":"code","f56c88ea":"code","3ccdad88":"code","683804cb":"code","6233c0b1":"code","cba6ff53":"code","3a4a7530":"code","5a2ad11b":"code","bfb3e79a":"code","56593ef2":"code","4c791b7c":"code","1cd37cd2":"code","b177b2cd":"code","76ea8513":"code","0f2c3523":"code","d7ab71a5":"code","bfb54085":"code","eabf39f6":"code","d366473c":"code","ce563a4d":"code","944ad189":"code","48d48e6e":"code","f47b6458":"code","3ed5cb92":"code","53c0ad93":"code","181207f6":"code","de986f5e":"code","8ada43ee":"code","1cce331a":"code","4922f25a":"code","dd242878":"markdown","e12fa765":"markdown","6b46ef7f":"markdown","98962ede":"markdown","6eeb5e67":"markdown","f0ede2a2":"markdown","f9607718":"markdown","94fcd136":"markdown","18406dba":"markdown","9dca3b3c":"markdown","17845d51":"markdown","2f61cc5c":"markdown","942270c6":"markdown","fbb890bf":"markdown","0d6dfcc9":"markdown","a8ca0fe3":"markdown","64cb9c27":"markdown","0e80caba":"markdown","a25cad16":"markdown","77dd87a5":"markdown","e5b2cfd7":"markdown","f006396e":"markdown","b1de8f4f":"markdown","8a3b6d93":"markdown","0f4a4621":"markdown","0f58d2ea":"markdown","35d3b2a5":"markdown","847e3d00":"markdown","77192a49":"markdown","0e821a2f":"markdown"},"source":{"fefe3b39":"import numpy as np\nimport cv2, zlib, base64, io\nfrom PIL import Image\nimport json\nimport matplotlib.pyplot as plt\nimport os\n\nimport warnings  \nwarnings.filterwarnings('ignore')","d961528c":"# functions for data conversion. Source: https:\/\/docs.supervise.ly\/data-organization\/00_ann_format_navi\/04_supervisely_format_objects#bitmap\n\ndef base64_2_mask(s):\n    z = zlib.decompress(base64.b64decode(s))\n    n = np.fromstring(z, np.uint8)\n    mask = cv2.imdecode(n, cv2.IMREAD_UNCHANGED)[:, :, 3].astype(bool)\n    return mask\n\ndef mask_2_base64(mask):\n    img_pil = Image.fromarray(np.array(mask, dtype=np.uint8))\n    img_pil.putpalette([0,0,0,255,255,255])\n    bytes_io = io.BytesIO()\n    img_pil.save(bytes_io, format='PNG', transparency=0, optimize=0)\n    bytes = bytes_io.getvalue()\n    return base64.b64encode(zlib.compress(bytes)).decode('utf-8')","a6523301":"# get all image names\nimg_path = \"..\/input\/football-advertising-banners-detection\/football\/images\"\nimage_names = os.listdir(img_path)\nprint(f\"Total images: {len(image_names)}\")","09ebe59a":"# total number of json files\njson_path = \"..\/input\/football-advertising-banners-detection\/football\/annotations\"\njson_names = os.listdir(json_path)\nprint(f\"Total jsons: {len(json_names)}\")","2247d17c":"print(f\"Number unique json_names {len(set(json_names))}\")","05293a38":"img_name_as_json = [img_name + \".json\" for img_name in image_names]\njson_wo_img = list(set(json_names) - set(img_name_as_json))\nprint(len(json_wo_img))","bf3d7afc":"for elem in json_wo_img:\n    print(elem)","867bc882":"# open and load meta.json\nwith open(\"..\/input\/football-advertising-banners-detection\/football\/meta.json\") as meta_file:\n    meta_json = json.load(meta_file)\n","677e8f59":"# All classes in meta.json file\n\nall_classes = [item[\"title\"] for item in meta_json[\"classes\"]]\nprint(all_classes)","14c8f6a8":"classes_dict = dict(zip(all_classes, [0]*len(all_classes)))\nclasses_dict[\"without\"] = 0\nprint(classes_dict)","84f02a56":"def open_json(json_name):\n    #open .json file and return dictionary \n    with open(os.path.join(json_path, json_name)) as json_file:\n        label = json.load(json_file)\n    return label","42d679aa":"for item in img_name_as_json:\n    json_file = open_json(item)\n    if len(json_file[\"objects\"]) == 0:\n        classes_dict[\"without\"] += 1\n    else:\n        for i in json_file[\"objects\"]:\n            classes_dict[i[\"classTitle\"]] += 1\nprint(classes_dict)\n        ","f56c88ea":"classes_remove = []\nfor i in classes_dict.keys():\n    if classes_dict[i] == 0:\n        classes_remove.append(i)\nfor i in classes_remove:\n    all_classes.remove(i)\n\n    classes_dict.pop(i)\nprint(all_classes)\nprint(classes_dict)","3ccdad88":"all_classes.insert(0, \"background\")\nclasses_values = dict(zip(all_classes, range(len(all_classes)))) \nprint(classes_values)","683804cb":"import plotly.graph_objects as go\nprint(\"Number of labels on all images\")\n\ngo.Figure(data=[go.Pie(labels=list(classes_dict.keys()), values=list(classes_dict.values()))])\n","6233c0b1":"def full_mask(json_file):\n    # return full mask with corresponding labels\n    \n    # get dictionary from json file\n    json_file = open_json(json_file)\n    \n   \n    #create mask with zeros\n    height = json_file[\"size\"][\"height\"]\n    width = json_file[\"size\"][\"width\"]\n    mask = np.zeros((height, width, len(all_classes)))\n    \n    # iterate throug all masks(can be several with different brand and with one)\n    for i in range(len(json_file['objects'])):\n        # get name of banner\n        label = json_file['objects'][i][\"classTitle\"]\n        \n        #convert name to number\n        label_number = classes_values[label]\n        \n        # get bitmap string\n        mask_str = json_file['objects'][i]['bitmap']['data']\n        \n        # convert from bitmap to image(true\/false)\n        mask_small = base64_2_mask(mask_str)\n        \n        #get startpoint of mask in image\n        start_point = json_file['objects'][i]['bitmap']['origin']\n        \n#         # paint mask as corresponding number\n#         mask[start_point[1]:start_point[1] + mask_small.shape[0],start_point[0]:start_point[0] + mask_small.shape[1]] = (label_number) * mask_small\n        \n        # mask 0 - not banner 1 - is banner\n        mask[start_point[1]:start_point[1] + mask_small.shape[0],\n             start_point[0]:start_point[0] + mask_small.shape[1],\n             label_number] = mask_small\n    mask[:, :, 0] = 1 - np.sum(mask, axis=-1)\n    return mask","cba6ff53":"def show_img_mask(number_img):\n    plt.figure(figsize=(10, 5))\n    plt.title(\"image\")\n    img = cv2.imread(os.path.join(img_path, image_names[number_img]))\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    plt.imshow(img)\n    plt.show()\n    plt.figure(figsize=(15, 7))\n    all_masks = full_mask(img_name_as_json[number_img])\n    for i in range(len(all_classes)):\n        plt.subplot(2, 4,i+1)\n        plt.title(all_classes[i])\n        plt.imshow(all_masks[:, :, i])\n    plt.show()","3a4a7530":"show_img_mask(667)","5a2ad11b":"import random\n\ndef generator_img(image_names, batch_size):\n    \n    \n    images = []\n    masks = []\n    \n    while True:\n        random.seed(np.random.randint(999))\n        image_names_shuffled = image_names[:]\n        random.shuffle(image_names_shuffled)\n        for image_name in image_names_shuffled:\n            #get image\n            img = np.array(cv2.imread(os.path.join(img_path, image_name)))\n            # resize it to (512, 256, 3), becouse some img has 1284, 720 dim\n            img = cv2.resize(img, dsize=(512, 256), interpolation=cv2.INTER_CUBIC)\n            # get correspond json\n            mask_json = image_name + \".json\"\n            # get full mask\n            mask = np.array(full_mask(mask_json))\n            # resize mask also and add last dim + type as uint8\n            mask = cv2.resize(mask, dsize=(512, 256), interpolation=cv2.INTER_CUBIC).astype('uint8')\n            #some mask have negative value, replace it with 0\n            mask[mask<0.5] = 0\n            mask[mask>0.5] = 1\n            # append img and mask with corresponding shape and value\n            images.append(img)\n            masks.append(mask)\n            \n            if len(images) >= batch_size:\n                yield np.stack(images, 0)\/255., np.stack(masks, 0).astype(\"float\")\n                \n                images = []\n                masks = []\n    ","bfb3e79a":"#create test generator\ngen = generator_img(image_names, 20) ","56593ef2":"# data from generator\ntrain_x, train_y = next(gen)\nprint(train_x.shape, train_y.shape)","4c791b7c":"def plot_banners(img, masks):\n    plt.figure(figsize=(20, 10))\n    plt.subplot(1, 2, 1)\n    plt.title(\"image\")\n    plt.imshow(img)\n    plt.show()\n    \n    plt.figure(figsize=(15, 8))\n    plt.subplot(1, 2, 2)\n    for i in range(len(all_classes)):\n        plt.subplot(2, 4,i+1)\n        plt.title(f\"class: {all_classes[i]}\")\n        plt.imshow(masks[:, :, i])\n    plt.show()","1cd37cd2":"index = 0\nplot_banners(train_x[index], train_y[index])","b177b2cd":"from sklearn.model_selection import train_test_split\ntrain_images, test_images = train_test_split(image_names, test_size=0.1)\nprint(len(train_images), len(test_images))","76ea8513":"train_generator = generator_img(train_images, 10)\nvalid_generator = generator_img(test_images, 10)","0f2c3523":"t_x, t_y = next(gen)\nprint('x', t_x.shape, t_x.dtype, t_x.min(), t_x.max())\nprint('y', t_y.shape, t_y.dtype, t_y.min(), t_y.max())","d7ab71a5":"from tensorflow.keras.preprocessing.image import ImageDataGenerator\n\n# arguments for changing image for training\naug_args = dict(featurewise_center = False, \n                  samplewise_center = False,\n                  rotation_range = 45, \n                  width_shift_range = 0.1, \n                  height_shift_range = 0.1,\n                  zoom_range = [0.9, 1.25],\n                  data_format = 'channels_last')\n\nimage_gen = ImageDataGenerator(**aug_args)\n\ndef create_aug_gen(in_gen):\n    \n    for in_x, in_y in in_gen:\n        seed = np.random.randint(999)\n        \n        gen_x = image_gen.flow(in_x*255,\n                              batch_size=in_x.shape[0],\n                              seed=seed,\n                              shuffle=True)\n        gen_y = image_gen.flow(in_y,\n                              batch_size=in_y.shape[0],\n                              seed=seed,\n                              shuffle=True)\n        \n        yield next(gen_x)\/255., next(gen_y)","bfb54085":"aug_gen = create_aug_gen(gen)\nt_x, t_y = next(aug_gen)\nprint('x', t_x.shape, t_x.dtype, t_x.min(), t_x.max())\nprint('y', t_y.shape, t_y.dtype, t_y.min(), t_y.max())\n","eabf39f6":"from tensorflow.keras.layers import Input\nfrom tensorflow.keras.layers import BatchNormalization\nfrom tensorflow.keras.layers import Conv2D\nfrom tensorflow.keras.layers import Activation\nfrom tensorflow.keras.layers import MaxPooling2D\nfrom tensorflow.keras.layers import Dropout \nfrom tensorflow.keras.layers import Conv2DTranspose\nfrom tensorflow.keras.layers import concatenate\nfrom tensorflow.keras.models import Model","d366473c":"def conv_block(tensor, nfilters, size=3, padding='same', initializer=\"he_normal\"):\n    x = Conv2D(filters=nfilters, kernel_size=(size, size), padding=padding, kernel_initializer=initializer)(tensor)\n    x = BatchNormalization()(x)\n    x = Activation(\"relu\")(x)\n    x = Conv2D(filters=nfilters, kernel_size=(size, size), padding=padding, kernel_initializer=initializer)(x)\n    x = BatchNormalization()(x)\n    x = Activation(\"relu\")(x)\n    return x\n\n\ndef deconv_block(tensor, residual, nfilters, size=3, padding='same', strides=(2, 2)):\n    y = Conv2DTranspose(nfilters, kernel_size=(size, size), strides=strides, padding=padding)(tensor)\n    y = concatenate([y, residual], axis=3)\n    y = conv_block(y, nfilters)\n    return y\n\n\ndef Unet(img_height, img_width, nclasses=6, filters=64):\n# down\n    input_layer = Input(shape=(img_height, img_width, 3), name='image_input')\n    conv1 = conv_block(input_layer, nfilters=filters)\n    conv1_out = MaxPooling2D(pool_size=(2, 2))(conv1)\n    conv2 = conv_block(conv1_out, nfilters=filters*2)\n    conv2_out = MaxPooling2D(pool_size=(2, 2))(conv2)\n    conv3 = conv_block(conv2_out, nfilters=filters*4)\n    conv3_out = MaxPooling2D(pool_size=(2, 2))(conv3)\n    conv4 = conv_block(conv3_out, nfilters=filters*8)\n    conv4_out = MaxPooling2D(pool_size=(2, 2))(conv4)\n    conv4_out = Dropout(0.5)(conv4_out)\n    conv5 = conv_block(conv4_out, nfilters=filters*16)\n    conv5 = Dropout(0.5)(conv5)\n# up\n    deconv6 = deconv_block(conv5, residual=conv4, nfilters=filters*8)\n    deconv6 = Dropout(0.5)(deconv6)\n    deconv7 = deconv_block(deconv6, residual=conv3, nfilters=filters*4)\n    deconv7 = Dropout(0.5)(deconv7) \n    deconv8 = deconv_block(deconv7, residual=conv2, nfilters=filters*2)\n    deconv9 = deconv_block(deconv8, residual=conv1, nfilters=filters)\n# output\n    output_layer = Conv2D(filters=nclasses, kernel_size=(1, 1))(deconv9)\n    output_layer = BatchNormalization()(output_layer)\n    output_layer = Activation('softmax')(output_layer)\n\n    model = Model(inputs=input_layer, outputs=output_layer, name='Unet')\n    return model","ce563a4d":"import tensorflow.keras.backend as K\n\n\ndef dice_coef(y_true, y_pred, smooth=0.01):\n    \n    intersection = K.sum(y_true * y_pred, axis=[1,2,3])\n    union = K.sum(y_true, axis=[1,2,3]) + K.sum(y_pred, axis=[1,2,3])\n    return K.mean( (2. * intersection + smooth) \/ (union + smooth), axis=0)\n\ndef dice_lose(in_gt, in_pred):\n    return 1 - dice_coef(in_gt, in_pred)\n","944ad189":"from tensorflow.keras.optimizers import Adam\n\nunet = Unet(img_height = 256, img_width = 512, nclasses=7, filters=16)\n\nunet.compile(optimizer=Adam(1e-4, decay=1e-6), loss=dice_lose, metrics=[dice_coef, 'accuracy'])\n\ntrain_aug_gen = create_aug_gen(train_generator)\nvalid_aug_gen = create_aug_gen(valid_generator)\n\nunet.summary()","48d48e6e":"from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n\nmodel_callbacks = [ModelCheckpoint(\n                                filepath=\"best_model.h5\",\n                                save_weights_only=True,\n                                monitor='val_dice_coef',\n                                mode='max',\n                                save_best_only=True,\n                                save_freq=\"epoch\"),# for save on every epoch\n                     EarlyStopping(\n                                 monitor='val_dice_coef',\n                                 mode='max',\n                                 patience=3,\n                                 verbose=1)]","f47b6458":"from tensorflow.keras.models import load_model\nimport os\n\n# parameters for traning\nnumber_train_steps = 500\nnumber_valid_steps = 50\n\n# change to true to train model\nto_train = False\n\nunet.load_weights(\"..\/input\/segment-weights\/best_model.h5\")\nprint(\"Model are loaded\")\n\nif to_train:\n    unet.fit(train_aug_gen,\n                   steps_per_epoch=number_train_steps,\n                   epochs=50, \n                   validation_data=valid_aug_gen,\n                   validation_steps=number_valid_steps, \n                   callbacks=[model_callbacks])  \n\n","3ed5cb92":"# unet.save_weights(\"best_model.h5\")","53c0ad93":"valid_generator = generator_img(test_images, 10)","181207f6":"# predict\nval_x, val_y = next(valid_generator)\npredicted_x = unet.predict(val_x)","de986f5e":"\n\ndef threshold(prediction, threshold=0.5):\n    result = np.copy(prediction)\n    result[result >= threshold] = 1\n    result[result < threshold] = 0\n    \n    return result","8ada43ee":"def plot_pred_true(img, masks, true):\n    plt.figure(figsize=(10, 5))\n    plt.title(\"image\")\n    plt.imshow(img)\n    plt.show()\n    \n    plt.figure(figsize=(15, 5))\n    for i in range(7):\n        plt.subplot(2, 7, i+1)\n        plt.title(f\"pred: {all_classes[i]}\")\n        plt.imshow(masks[:, :, i])\n    \n    for i in range(7):\n        plt.subplot(2, 7, i+8)\n        plt.title(f\"true: {all_classes[i]}\")\n        plt.imshow(true[:, :, i])\n\n    plt.show()","1cce331a":"from tensorflow.keras.utils import to_categorical\n\nindex = 4\n\n# with postprocesing using threshold and categorical\nplot_pred_true(val_x[index], to_categorical(np.argmax(threshold(predicted_x[index], 0.5), axis=-1),num_classes=7), val_y[index])","4922f25a":"# calculate dice coef for 500 validation imgs\n\n\naug_generator_dice = create_aug_gen(generator_img(test_images, 10))\n\nnumber_generet = 50\ndice_coef_v = 0\n\nfor i in range(number_generet):\n    val_x_c, val_y_c = next(aug_generator_dice)\n    predict_y = unet.predict(val_x_c)\n    predict_y_threshold = to_categorical(np.argmax(threshold(predict_y, 0.5), axis=-1),num_classes=7)\n    dice_coef_v += dice_coef(y_true=val_y_c, y_pred=predict_y_threshold)\n    \n\nprint(f\"Dice coef for model: {dice_coef_v\/number_generet}\")","dd242878":"<h3> Create threshold function for better model result","e12fa765":"<h3> We have low persentage of banners on images, we will use dice coef for metrics, losses","6b46ef7f":"<h3>Compile model, use Adam optimizer, and dice for lose and metrics","98962ede":"<h3> import all important layers for U-Net model","6eeb5e67":"<h3> U-Net model","f0ede2a2":"<h3>Split dataset on train and test - 0.9 to 0.1","f9607718":"# Create generator for data augmentation and future training","94fcd136":"<h2> First let's import main libraries and remove warnings","18406dba":"# Show image and mask","9dca3b3c":"<h3>We have 85 json_files that doesn't have there images. We will ignore them","17845d51":"<h2>Function show_img_mask will show image and mask by there index in image_names[index]","2f61cc5c":"# Create model","942270c6":"<h3> Function for showing img, prediction and true masks","fbb890bf":"<h3> Function to calculate dice coef (accuracy of our model)","0d6dfcc9":"<h2>Showing results","a8ca0fe3":"<h3> Create checkpoints and Early stoping for control overfiting","64cb9c27":"<h2> Get validation data and explore results","0e80caba":"<h3> For showing img and there banner","a25cad16":"<h2>We Have 0.98 dice coef for our model","77dd87a5":"<h1> Explore generator and there work","e5b2cfd7":"<h3> Will return generator that create:\n<h4>    image_size - (batch_size, width, height, 3) \n<h4>    mask_size  - (batch_size, width, height, classes) ","f006396e":"<h3>We will get background as final 7 class. All other image pixels","b1de8f4f":"<h3> This functions will decode base64 to img mask as bitmap","8a3b6d93":"<h3> Can make sum up that banners with adidas and unicredit are 0 and a little images without banners(or banners that not have masks)","0f4a4621":"<h3> Function for read json files","0f58d2ea":"<h3> Create simpe generators for training and validation data","35d3b2a5":"<h2> Get pretrained weights from and show parameters for training model","847e3d00":"<h4>Saving model","77192a49":"# Create augment generator function","0e821a2f":"<h2> Function full_mask read and then convert json file to mask"}}