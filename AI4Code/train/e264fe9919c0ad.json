{"cell_type":{"4a986df7":"code","80ffcbef":"code","0d47ce87":"code","be0b6241":"code","5e64f188":"code","62311824":"code","aae23579":"code","51af06ba":"code","aeb5cf6a":"code","cbb9bd1e":"code","2c5fe4d0":"code","7927710d":"code","530eb241":"code","504539e1":"code","e48ed890":"code","10c46262":"code","c98b9c0b":"code","5bf2a0a3":"code","c8f86602":"code","3212a02d":"code","1805a6fb":"markdown","08e95daf":"markdown"},"source":{"4a986df7":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n\nimg_paths = []\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input\/'):\n    for filename in filenames:\n        #print(os.path.join(dirname, filename))\n        if(filename[-3:] == 'jpg'):\n            img_paths.append(os.path.join(dirname, filename))\n        \n\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","80ffcbef":"import numpy as np\nimport pandas as pd \nimport keras\nimport glob\nimport imageio\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport os\nimport PIL\nfrom tensorflow.keras import layers\nimport time\nimport tensorflow as tf\n\nfrom IPython import display","0d47ce87":"IMG_WIDTH = 112\nIMG_HEIGHT = 112\nBATCH_SIZE = 32\n\n# UTILITIES\ndef load(image_file):\n  # height, width, channels\n  image = tf.io.read_file(image_file)\n  image = tf.image.decode_jpeg(image)\n\n  input_image = tf.cast(image, tf.float32)\n\n  return input_image\n\ndef resize(input_image, height, width):\n  input_image = tf.image.resize(input_image, [height, width], \n                                method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n  return input_image\n\ndef normalize(input_image):\n  input_image = (input_image \/ 127.5) - 1\n  return input_image\n\ndef load_image_train(image_file):\n\n  input_image = load(image_file)\n  input_image = resize(input_image, IMG_WIDTH, IMG_HEIGHT)\n  input_image = normalize(input_image)\n\n  return input_image","be0b6241":"import random\n# Load in batch_size images, selected at random, and also get their labels\n# Convert to tensor and return (batch_size, width, height, channels), (batch_size, # of outputs)\ndef get_batch(batch_size=32):\n    img_1 = random.randint(0, len(img_paths))\n    images = []\n    \n    for i in range(batch_size):\n        while True:\n            try:\n                img_path = random.randint(0, len(img_paths))\n                # Make sure dimensionality of data is correct\n                image = load_image_train(img_paths[img_path])\n                if(image.shape[2] == 3): \n                    images.append(image)\n                    break\n            except:\n                continue\n        \n        \n    \n    # IMAGES: (BATCH_SIZE, width, height, channels)\n    return tf.stack(images, axis=0) ","5e64f188":"get_batch(32).shape","62311824":"def make_generator_model():\n    model = tf.keras.Sequential()\n    model.add(layers.Dense(7*7*256, use_bias=False, input_shape=(100,)))\n    model.add(layers.BatchNormalization())\n    model.add(layers.LeakyReLU())\n\n    model.add(layers.Reshape((7, 7, 256)))\n    #assert model.output_shape == (None, 7, 7, 256) # Note: None is the batch size\n\n    model.add(layers.Conv2DTranspose(128, (5, 5), strides=(1, 1), padding='same', use_bias=False))\n    #assert model.output_shape == (None, 7, 7, 128)\n    model.add(layers.BatchNormalization())\n    model.add(layers.LeakyReLU())\n\n    model.add(layers.Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same', use_bias=False))\n    #assert model.output_shape == (None, 14, 14, 64)\n    model.add(layers.BatchNormalization())\n    model.add(layers.LeakyReLU())\n\n    model.add(layers.Conv2DTranspose(32, (5, 5), strides=(2, 2), padding='same', use_bias=False, activation='tanh'))\n    #assert model.output_shape == (None, 28, 28, 3)\n    \n    model.add(layers.Conv2DTranspose(16, (5, 5), strides=(2, 2), padding='same', use_bias=False, activation='tanh'))\n    #assert model.output_shape == (None, 56, 56, 3)\n    \n    model.add(layers.Conv2DTranspose(3, (5, 5), strides=(2, 2), padding='same', use_bias=False, activation='tanh'))\n    #assert model.output_shape == (None, 128, 128, 3)\n    print(model.output_shape)\n\n    return model","aae23579":"generator = make_generator_model()\n\nnoise = tf.random.normal([1, 100])\ngenerated_image = generator(noise, training=False)\n\nplt.imshow(generated_image[0, :, :, :], cmap='gray')","51af06ba":"def make_discriminator_model():\n    model = tf.keras.Sequential()\n    model.add(layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same',\n                                     input_shape=[112, 112, 3]))\n    model.add(layers.LeakyReLU())\n    model.add(layers.Dropout(0.3))\n\n    model.add(layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same'))\n    model.add(layers.LeakyReLU())\n    model.add(layers.Dropout(0.3))\n\n    model.add(layers.Flatten())\n    model.add(layers.Dense(1))\n\n    return model","aeb5cf6a":"discriminator = make_discriminator_model()\ndecision = discriminator(generated_image)\nprint (decision)","cbb9bd1e":"generator.summary()","2c5fe4d0":"discriminator.summary()","7927710d":"# This method returns a helper function to compute cross entropy loss\ncross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)","530eb241":"def discriminator_loss(real_output, fake_output):\n    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n    total_loss = real_loss + fake_loss\n    return total_loss","504539e1":"def generator_loss(fake_output):\n    return cross_entropy(tf.ones_like(fake_output), fake_output)","e48ed890":"generator_optimizer = tf.keras.optimizers.Adam(1e-4)\ndiscriminator_optimizer = tf.keras.optimizers.Adam(1e-4)","10c46262":"EPOCHS = 50\nnoise_dim = 100\nnum_examples_to_generate = 16\n\nseed = tf.random.normal([num_examples_to_generate, noise_dim])","c98b9c0b":"import matplotlib.image as mpimg\n\ndef generate_and_save_images(model, epoch, test_input):\n  # Notice `training` is set to False.\n  # This is so all layers run in inference mode (batchnorm).\n  predictions = model(test_input, training=False)\n\n  fig = plt.figure(figsize=(4,4))\n\n  plt.imshow(predictions[0])\n  plt.axis('off')\n  plt.show()\n","5bf2a0a3":"@tf.function\ndef train_step(images):\n    noise = tf.random.normal([BATCH_SIZE, noise_dim])\n\n    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n      generated_images = generator(noise, training=True)\n\n      real_output = discriminator(images, training=True)\n      fake_output = discriminator(generated_images, training=True)\n\n      gen_loss = generator_loss(fake_output)\n      disc_loss = discriminator_loss(real_output, fake_output)\n\n    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n\n    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))","c8f86602":"def train(steps):\n  print('entered')\n  for step in range(steps):\n    start = time.time()\n\n    print(get_batch(32).shape)\n    train_step(get_batch(32))\n\n    # Produce images for the GIF as we go\n    display.clear_output(wait=True)\n    generate_and_save_images(generator,\n                             step + 1,\n                             seed)\n\n    print ('Time for step {} is {} sec'.format(step + 1, time.time()-start))\n\n  # Generate after the final epoch\n  display.clear_output(wait=True)\n  generate_and_save_images(generator,\n                           steps,\n                           seed)","3212a02d":"train(200)","1805a6fb":"Above is the final output after a few epochs.","08e95daf":"This notebook basically applies the code from the following tensorflow tutorial (implementing Pix2Pix) but changes a bit of the code so that it is compatible with the artwork dataset: \nhttps:\/\/www.tensorflow.org\/tutorials\/generative\/pix2pix"}}