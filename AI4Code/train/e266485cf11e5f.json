{"cell_type":{"3c7773f2":"code","c505df12":"code","b3b54167":"code","5c3cc02e":"code","545aafbd":"code","24020706":"code","093e6707":"code","9cb6a8a5":"code","0eb81cb0":"code","a0725aaf":"code","7bb9d462":"code","72118c0c":"code","89b76307":"code","a14c7f81":"code","35baffb2":"code","c0d4fb25":"code","cc90e6cb":"code","7f784eee":"code","a56c1757":"code","5e84526e":"code","abce51e4":"code","516bd2cb":"code","9ff02bf0":"code","4c537259":"code","aca13e0f":"code","27992810":"code","b2764578":"code","8d045087":"code","8082839c":"code","ec69b202":"code","83891e54":"code","11e6ada6":"code","71b10a82":"code","77776587":"code","4ed9655f":"markdown","c32bff8c":"markdown","38f9fd2d":"markdown","b2f40527":"markdown","3cbd5fa4":"markdown","ce6d0e12":"markdown","89f76249":"markdown","54302bce":"markdown","64972c65":"markdown","e955e692":"markdown","5089a7d2":"markdown","af6809da":"markdown","adc4b416":"markdown","814c7c61":"markdown","95dd2c12":"markdown","48870a4d":"markdown","3c867edc":"markdown","7d229680":"markdown","ca2b4f85":"markdown","d2b193a4":"markdown","7734484b":"markdown","20ed4749":"markdown","c86c03e2":"markdown","c2860c22":"markdown"},"source":{"3c7773f2":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c505df12":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import Sequential\nfrom tensorflow.keras.layers import Flatten, Dense, Dropout, BatchNormalization\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, accuracy_score, classification_report\nfrom sklearn import preprocessing\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.linear_model import LogisticRegression\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom sklearn.metrics import accuracy_score","b3b54167":"data=pd.read_csv(\"\/kaggle\/input\/creditcardfraud\/creditcard.csv\")","5c3cc02e":"data.shape","545aafbd":"data.head()","24020706":"data.isnull().sum()","093e6707":"data.info()","9cb6a8a5":"var = data.columns.values\n\ni = 0\nt0 = data.loc[data['Class'] == 0]\nt1 = data.loc[data['Class'] == 1]\n\nsns.set_style('whitegrid')\nplt.figure()\nfig, ax = plt.subplots(8,4,figsize=(16,28))\n\nfor feature in var:\n    i += 1\n    plt.subplot(8,4,i)\n    sns.kdeplot(t0[feature], bw=0.5,label=\"Class = 0\")\n    sns.kdeplot(t1[feature], bw=0.5,label=\"Class = 1\")\n    plt.xlabel(feature, fontsize=12)\n    locs, labels = plt.xticks()\n    plt.tick_params(axis='both', which='major', labelsize=12)\nplt.show()","0eb81cb0":"plt.figure(figsize = (16,10))\nplt.title('Credit Card Transactions features correlation plot', size = 20)\ncorr = data.corr()\nsns.heatmap(corr,xticklabels=corr.columns,yticklabels=corr.columns,linewidths=.1,cmap=\"Greens\",fmt='.1f',annot=True)\nplt.show()","a0725aaf":"data[\"Class\"].value_counts().plot(kind=\"bar\",color=\"red\")\nplt.title(\"Frequency of the target classes\", size=20)\nplt.xlabel(\"Target Labels\", size = 18)","7bb9d462":"target = pd.DataFrame(data[\"Class\"].value_counts())\ntarget.style.background_gradient(cmap=\"Reds\")","72118c0c":"X=data.drop(columns=[\"Class\"])\ny=data[\"Class\"]","89b76307":"names=X.columns\nscaled_df = preprocessing.scale(X)\nscaled_df = pd.DataFrame(scaled_df,columns=names)","a14c7f81":"scaled_df.head()","35baffb2":"scaled_df[[\"Amount\",\"Time\"]].describe()","c0d4fb25":"X_train, X_test, y_train, y_test = train_test_split(scaled_df, y, test_size = 0.30, random_state = 0, shuffle = True, stratify = y)","cc90e6cb":"X_train.shape, X_test.shape","7f784eee":"y_train.value_counts()","a56c1757":"y_test.value_counts()","5e84526e":"sm = SMOTE(random_state = 33)\nX_train_new, y_train_new = sm.fit_sample(X_train, y_train.ravel())","abce51e4":"pd.Series(y_train_new).value_counts().plot(kind=\"bar\")","516bd2cb":"clf = LogisticRegression(solver = 'lbfgs')\nclf.fit(X_train_new, y_train_new)\ntrain_pred = clf.predict(X_train_new)\ntest_pred = clf.predict(X_test)","9ff02bf0":"print('Accuracy score for Training Dataset = ', accuracy_score(train_pred, y_train_new))\nprint('Accuracy score for Testing Dataset = ', accuracy_score(test_pred, y_test))","4c537259":"cm=confusion_matrix(y_test, test_pred)\ncm","aca13e0f":"plt.figure(figsize=(8,6))\nsns.set(font_scale=1.2)\nsns.heatmap(cm, annot=True, fmt = 'g', cmap=\"Reds\", cbar = False)\nplt.xlabel(\"Predicted Label\", size = 18)\nplt.ylabel(\"True Label\", size = 18)\nplt.title(\"Confusion Matrix Plotting for Logistic Regression model\", size = 20)","27992810":"print(\"Percentage for 'no fraud' cases wrong classification using Logistic Regression is:\", (2018\/85295)*100)\nprint(\"Percentage for 'Fraud' cases wrong prediction Logistic Regression is:\", (13\/148)*100)","b2764578":"model = Sequential()\nmodel.add(Dense(X_train_new.shape[1], activation = 'relu', input_dim = X_train_new.shape[1]))\nmodel.add(BatchNormalization())\n\nmodel.add(Dense(64, activation = 'relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.5))\n\nmodel.add(Dense(64, activation = 'relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.5))\n\nmodel.add(Dense(1, activation = 'sigmoid'))","8d045087":"optimizer = keras.optimizers.Adam(lr=0.0001)\nmodel.compile(optimizer = optimizer, loss = 'binary_crossentropy')","8082839c":"early_stop = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience = 10)","ec69b202":"history = model.fit(x=X_train_new, y=y_train_new, batch_size = 256, epochs=150,\n          validation_data=(X_test, y_test), verbose=1,\n          callbacks=[early_stop])","83891e54":"evaluation_metrics=pd.DataFrame(model.history.history)\nevaluation_metrics.plot(figsize=(10,5))\nplt.title(\"Loss for both Training and Validation\", size = 20)","11e6ada6":"y_pred = model.predict_classes(X_test)","71b10a82":"cm_nn=confusion_matrix(y_test, y_pred)\ncm_nn","77776587":"plt.figure(figsize=(8,6))\nsns.set(font_scale=1.2)\nsns.heatmap(cm_nn, annot=True, fmt = 'g', cmap=\"winter\", cbar = False)\nplt.xlabel(\"Predicted Label\", size = 18)\nplt.ylabel(\"True Label\", size = 18)\nplt.title(\"Confusion Matrix Plotting for Neural Network model\", size = 20)","4ed9655f":"# Model Architecture","c32bff8c":"Our data is pretty imbalanced as can be seen just below.","38f9fd2d":"To protect our model from overfitting, we will use early stop feature of tensorflow, which will once identify that the evaluation metric that we mentioned, if it stopped improving further, it will stop the number of epochs. Also we have used learning rate of 0.0001.","b2f40527":"Let see how features are distributed w.r.t., our target variable, which is \"Class\".","3cbd5fa4":"# Using Neural Network","ce6d0e12":"Also let's check a few thing about the splitted data before we proceed.","89f76249":"So by seeing the time and amount features, we can say that the features has been scaled.","54302bce":"# Splitting the Data\n\n\nNow we will split the standardized dataset into train and test, and then do over-sampling on the training dataset, and then we will do the classification based on the training.","64972c65":"As here we are dealing with the problem of imbalanced dataset, so we will try to balance it using a technique called \"**SMOTE**\" which is the short form of **Synthetic Minority Over-sampling Technique**, this is another method of simple over-sampling technique, but here instead of just duplicating the minority the class, synthetic data are produced, so according to me it is much better compared to simple over-sampling, which just randomly duplicates the minority class to balance it. There is another method, by which we can solve this problem of unbalaced data, which is \"Down-Sampling\", but I am not a big fan of that technique cause there is a lot of data loss which happens while trying to achieve that.\n","e955e692":"So if we compare this with the Logistic Regression model, the little problem here is that, it is doing very good prediction for the majority class, which is 0 or \"**No Fraud**\" cases, but for minority class, which is 1 or \"**Fraud**\" cases, it is performing a little less better than the Logistic Regression. But I guess with a little more hyperparamters tuning, the model will be able to perform better than the Logistic Regression even for **minority class**.","5089a7d2":"To work with this dataset, we need to bring all the variables in the same scale. But before that we will do a little more exploration.","af6809da":"So it is pretty much balanced now, and we can build our predictive model with it now.","adc4b416":"Now let's compile the model.","814c7c61":"# SMOTE\n\n\nNow we will start the process.","95dd2c12":"The hyperparameters that we have used here are **Batch Normalization**, and **Dropout**. And the activation function we have used here for hidden layers are \"**relu**\", and for output, it is \"**Sigmoid**\" function. Also we have used 2 hidden layers with 64 units for each layer.","48870a4d":"# Confusion Matrix for Logistic Regression Model","3c867edc":"Now we will try to standardize all our input features and for that we will seperate the input from the output feature, so that it will be easy for us.","7d229680":"So from the above confusion matrix, we can see that the nummber of wrong classifications done for 0, which is \"no fraud\" is 2018 out of 85295, and number of wrong classification done for 1, which is \"Fraud happened\" is 13 out of 148, or in terms of percentages, let's see below.","ca2b4f85":"# Confusion Matrix for Neural Network","d2b193a4":"# Using Logistic Regression","7734484b":"![creditcardfraud1280.jpg](attachment:creditcardfraud1280.jpg)","20ed4749":"# Credit Card Fraud Detection","c86c03e2":"Below is the exact frequency values for both the target labels.","c2860c22":"Now we will see whether it has been balanced or not."}}