{"cell_type":{"175fba82":"code","b1be1b07":"code","0deb0cef":"code","53b041a9":"code","c2bbf36d":"code","6027aa01":"code","12dc410f":"code","f15c1462":"code","63d4dc7a":"code","683ada2b":"code","7a77194a":"code","47f946ae":"code","67c227e6":"code","47fac2d2":"code","6218bbe9":"code","b1deefe5":"code","a607b2e9":"code","e9fb2a43":"code","0a84aa18":"markdown","e1db1e8f":"markdown","e9ed56d1":"markdown","83dc1332":"markdown","11057cb1":"markdown","d0fd5404":"markdown","8e016bd1":"markdown","9f37eb33":"markdown"},"source":{"175fba82":"import numpy as np\nimport pandas as pd \nfrom tensorflow import keras\nfrom sklearn.model_selection import train_test_split\nimport keras_tuner as kt\nimport tensorflow as tf\nimport gc","b1be1b07":"class Config:\n    validation_split = 0.15\n    dataset_name = \"tabular-playground-series-nov-2021\"\n    train_path = \"\/kaggle\/input\/%s\/train.csv\"%(dataset_name)\n    test_path = \"\/kaggle\/input\/%s\/test.csv\"%(dataset_name)\n    sample_submission_path = \"\/kaggle\/input\/%s\/sample_submission.csv\"%(dataset_name)\n    id_field = \"id\"\n    label_field = \"target\"\n    hyperparameter_tuning_trial = 50\n    epochs = 50\n    train_with_fulldataset = True\n    sample_rate = 0.05\n    model_path = \"model.h5\"\n    submission_path = \"submission.csv\"\n    batch_size = 1024\nconfig = Config()","0deb0cef":"train = pd.read_csv(config.train_path)","53b041a9":"train.head()","c2bbf36d":"train.pop(config.id_field)\ntarget = train.pop(config.label_field)","6027aa01":"train_mean = train.mean()\ntrain_std = train.std()","12dc410f":"train = (train - train_mean) \/ train_std","f15c1462":"X_train, X_val, y_train, y_val = train_test_split(train, target, test_size=config.validation_split, random_state=42)","63d4dc7a":"X_train.shape, X_val.shape, y_train.shape, y_val.shape","683ada2b":"del train\ndel target\ngc.collect()","7a77194a":"train_indices = np.random.choice(X_train.shape[0], int(X_train.shape[0] * config.sample_rate))\nX_train_subset = X_train.iloc[train_indices]\ny_train_subset = y_train.iloc[train_indices]\nval_indices = np.random.choice(X_val.shape[0], int(X_val.shape[0] * config.sample_rate))\nX_val_subset = X_val.iloc[val_indices]\ny_val_subset = y_val.iloc[val_indices]","47f946ae":"def build_model(hp):\n    inputs = tf.keras.layers.Input((X_train.shape[1]))\n    width = hp.Choice('width', [16, 32, 64])\n    depth = hp.Choice('depth', [3, 6, 9])\n    x = keras.layers.Dense(\n            width, \n            activation='swish'\n        )(inputs)\n    for i in range(depth):\n        if i == 0:\n            x = inputs\n        x = keras.layers.Dense(\n            width, \n            activation=\"swish\"\n        )(x)\n        x = keras.layers.Dropout(\n            hp.Choice('dropout', [0.1, 0.2])\n        )(x)\n        if (i + 1) % 3 == 0:\n            x = keras.layers.BatchNormalization()(x)\n            x = keras.layers.Concatenate()([x, inputs])\n    output = keras.layers.Dense(1, activation=\"sigmoid\")(x)\n    model = keras.Model(inputs=inputs, outputs=output)\n    adam = keras.optimizers.Adam(learning_rate=hp.Float(\"learing_rate\", 1e-5, 5e-3))\n    model.compile(loss='binary_crossentropy', optimizer=adam, metrics=[\"accuracy\", keras.metrics.AUC()])\n    return model","67c227e6":"tuner = kt.RandomSearch(\n    build_model,\n    objective=kt.Objective(\"val_auc\", direction=\"max\"),\n    max_trials=config.hyperparameter_tuning_trial)\ntuner.search(x=X_train_subset, y=y_train_subset, epochs=5, batch_size=config.batch_size, validation_data=(X_val_subset, y_val_subset))\nbest_model = tuner.get_best_models()[0]\nkeras.utils.plot_model(best_model, show_shapes=True)","47fac2d2":"best_hp = tuner.get_best_hyperparameters()[0]\nbest_hp.get_config()[\"values\"]","6218bbe9":"if not config.train_with_fulldataset:\n    model = best_model\nelse:\n    keras.backend.clear_session()\n    model = tuner.hypermodel.build(best_hp)\n    early_stopping = keras.callbacks.EarlyStopping(patience=10)\n    model_checkpoint = keras.callbacks.ModelCheckpoint(config.model_path, save_best_only=True)\n    reduce_lr =  keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n                              patience=5, min_lr=1e-7)\n    history = model.fit(x=X_train, y=y_train, epochs=config.epochs, batch_size=config.batch_size, validation_data=(X_val, y_val), callbacks=[early_stopping, model_checkpoint, reduce_lr])\n    model.load_weights(config.model_path)\n    pd.DataFrame(history.history).plot()","b1deefe5":"del X_train\ndel y_train\ndel X_val\ndel y_val\ngc.collect()","a607b2e9":"test = pd.read_csv(config.test_path)\n_ = test.pop(config.id_field)\ntest = (test - train_mean) \/ train_std","e9fb2a43":"sample_submission = pd.read_csv(config.sample_submission_path)\ny_pred = model.predict(test).reshape(-1)\nprint(y_pred.shape)\nsample_submission[config.label_field] = y_pred\nsample_submission.to_csv(config.submission_path, index=False)","0a84aa18":"## Import and preprocess datasets","e1db1e8f":"## Configuration","e9ed56d1":"## TPS-11-21: DNN","83dc1332":"### Choose a small sample for hyperparameter tuning","11057cb1":"## Model Development","d0fd5404":"Here is best parameters:","8e016bd1":"## Setup","9f37eb33":"## Submission"}}