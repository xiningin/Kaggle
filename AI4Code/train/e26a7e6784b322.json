{"cell_type":{"b33ad963":"code","e1ba7cac":"code","0b9ed4df":"code","e7fc6205":"code","b771f33d":"code","eb29ee4a":"code","204d202a":"code","d081eb08":"code","8805ee9a":"code","84be224b":"code","cf310854":"code","9708c400":"code","e849bcbb":"code","3509949b":"code","27e08aa1":"code","bea4b55c":"code","fe4f4c1c":"code","2917b88b":"code","29e19cf7":"code","1e95714d":"code","c77bd866":"code","4936da5a":"code","15bf6beb":"code","490e005c":"code","cea6ae23":"code","6932083b":"code","21767796":"code","bc63a28d":"code","e84a28c5":"code","4b62008a":"code","9f9eb48b":"code","7e1f076f":"code","9396773a":"code","77cd1ef0":"code","26a36e46":"code","a8e5ecbc":"code","eba71a1c":"code","81cc37ee":"code","52472911":"code","d07d4f56":"code","2875acc8":"code","fd954137":"code","f0fc42d7":"code","2110ab12":"code","89292342":"code","1c13b510":"code","53bd414c":"code","e3b04943":"code","d0265ff9":"code","c5b94162":"code","5efe5edc":"code","dcf97112":"code","c5ed52c1":"code","fd69a7a5":"code","dcda6d01":"code","95612eec":"code","1eff2845":"code","d21ea7ce":"code","4d16c7a4":"code","fd8a652e":"code","53f679e7":"code","6ac2c927":"code","b751b15f":"code","9d48b845":"code","572cf9f5":"code","5ccea94b":"code","2ac47e82":"code","67b47d32":"code","fca15307":"code","77cf34be":"code","3f0da440":"code","83425e50":"code","a2512b4f":"code","3d21e8c5":"code","a36b868a":"code","dc6bcd8c":"code","e7e69c87":"code","a7dca627":"code","27799b65":"code","2f539a0c":"markdown","3bb9c3a1":"markdown","29efdc3b":"markdown","b374eb4f":"markdown","745418a6":"markdown","9da08581":"markdown","6e8e21e9":"markdown","248721de":"markdown","72cdef8a":"markdown","4c7501d3":"markdown","db8fbb38":"markdown","72d846f9":"markdown","a5a71210":"markdown","7f3266e2":"markdown","794a8b96":"markdown","2850e9b8":"markdown","97924b00":"markdown","49e1a0d1":"markdown","bc46192b":"markdown","6e531104":"markdown"},"source":{"b33ad963":"from keras.models import Sequential\nfrom keras.layers import Input, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Convolution2D,Conv2D\nfrom keras.layers import AveragePooling2D, MaxPooling2D, Dropout, GlobalMaxPooling2D, GlobalAveragePooling2D\nfrom tensorflow.keras.optimizers import SGD\nfrom keras.callbacks import TensorBoard\nfrom keras import applications\nfrom tensorflow import keras\nfrom keras.layers import Dense,Conv2D,MaxPool2D,BatchNormalization,Flatten\nfrom keras.models import Sequential \nfrom keras.preprocessing import image\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom matplotlib import pyplot as plt \nimport cv2\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport gc\nimport json\nimport math\nimport cv2\nimport PIL \nimport random\nimport os \nfrom PIL import Image\nimport seaborn as sns\nsns.set(style='darkgrid')\nfrom sklearn.preprocessing import LabelEncoder\nfrom tensorflow.keras.optimizers import Adam\nimport matplotlib.pyplot as plt\nimport pandas as pd \nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import cohen_kappa_score, accuracy_score\nimport scipy\nfrom tqdm import tqdm\nimport tensorflow as tf\nfrom keras import backend as K\nimport gc   \nfrom functools import partial\nfrom sklearn import metrics\nfrom collections import Counter\nimport json\nimport itertools\nfrom IPython.display import display, Image\n\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import KFold\nfrom sklearn.preprocessing import OneHotEncoder\nfrom tqdm import tqdm\nfrom sklearn.decomposition import PCA\nimport glob\nimport pandas as pd","e1ba7cac":"#mega folder path\npath = '..\/input\/fat-content-data\/FAT CONTENT'\n\n#input image size\nIMG_WIDTH=150\nIMG_HEIGHT=150","0b9ed4df":"#converting all images names to lower case\n# for file in os.listdir(greasy):\n#   os.rename(greasy + file, greasy + file.lower())","e7fc6205":"#converting extensions (jpeg to jpg)\nfor folder in glob.glob(os.path.join(path,'*')):\n  for filename in glob.glob(os.path.join(folder,'*')):\n    os.rename(os.path.join(path, filename), os.path.join(path, filename.replace('.jpeg','.jpg')))","b771f33d":"#handling diff file extensions\nimg_ext= dict()\nfor folder in glob.glob(os.path.join(path,'*')):\n  for files in glob.glob(os.path.join(folder,'*')):\n    extension = os.path.basename(files).split('.')[1]\n    img_ext[extension] = img_ext.get(extension,0) +1\nprint('No. of images belonging to diff. file extensions:')\nimg_ext    ","eb29ee4a":"#image folder renaming \ndef image_renaming(path, name):\n  files = os.listdir(path)\n  for index, file in enumerate(files): \n      os.rename(os.path.join(path, file), os.path.join(path, name.join([str(index), '.jpg'])))","204d202a":"image_renaming(oily, '_fat_content_oily') ","d081eb08":"#label encoding image labels\nlabels = os.listdir(path)\n# class_names_label = {class_name:i for i, class_name in enumerate(labels)}\nnb_classes = len(labels)\nprint('No. of classes: {}, \\nTexture Label names:{}'.format(nb_classes, labels))","8805ee9a":"def unsharp_mask(img, kernel_size=(5, 5), sigma=1.0, amount=1.0, threshold=0):\n    \"\"\"Return a sharpened version of the image, using an unsharp mask.\"\"\"\n    blurred = cv2.GaussianBlur(img, kernel_size, sigma)\n    sharpened = float(amount + 1) * img - float(amount) * blurred\n    sharpened = np.maximum(sharpened, np.zeros(sharpened.shape))\n    sharpened = np.minimum(sharpened, 255 * np.ones(sharpened.shape))\n    sharpened = sharpened.round().astype(np.uint8)\n    if threshold > 0:\n        low_contrast_mask = np.absolute(img - blurred) < threshold\n        np.copyto(sharpened, img, where=low_contrast_mask)\n    return sharpened\n\ndef image_enhance(img):\n    #upscaling the image\n    scale_percent = 220 # percent of original size\n    width = int(img.shape[1] * scale_percent \/ 100)\n    height = int(img.shape[0] * scale_percent \/ 100)\n    dim = (width, height)\n  \n    # resize image\n    resized_img = cv2.resize(img, dim, interpolation = cv2.INTER_AREA)\n    \n    #sharepening the image\n    sharpened_image = unsharp_mask(resized_img)\n\n    return sharpened_image","84be224b":"from PIL import Image\ndef create_dataset(img_folder):\n   \n    img_data_array=[]\n    class_name=[]\n   \n    for dir1 in os.listdir(img_folder):\n        for file in os.listdir(os.path.join(img_folder, dir1)):\n       \n            image_path= os.path.join(img_folder, dir1,  file)\n        \n            image= cv2.imread( image_path, cv2.COLOR_BGR2RGB)\n            image=cv2.resize(image, (IMG_HEIGHT, IMG_WIDTH),interpolation = cv2.INTER_AREA)\n            image = np.array(image)\n            image = unsharp_mask(image)\n            image = image.astype('float32')\n            image \/= 255 \n            img_data_array.append(image)\n            class_name.append(dir1)\n    return img_data_array, class_name","cf310854":"img_data, class_name = create_dataset(path) ","9708c400":"target_dict={k: v for v, k in enumerate(np.unique(class_name))}\ntarget_dict  ","e849bcbb":"target_val=  [target_dict[class_name[i]] for i in range(len(class_name))] ","3509949b":"len(target_val)","27e08aa1":"plt.imshow(img_data[1000])\ntarget_val[1000] ","bea4b55c":"#no.of images in every class\nfrom IPython.display import Image, display\n\nnum = []\nfor label in labels:\n    path = '..\/input\/fat-content-data\/FAT CONTENT\/{0}\/'.format(label)\n    folder_data = os.listdir(path)\n    k = 0\n    print('\\n \\033[1m', label.upper())\n    for image_path in folder_data:\n        if k < 3:\n            display(Image(path+image_path))\n        k = k+1\n    num.append(k)\n    print('\\033[1m There are ', k,' images in ', label, 'class')","fe4f4c1c":"def original_data_distribution(labels, num):\n  plt.figure(figsize = (8,8))\n  plt.bar(labels, num)\n  plt.title('NUMBER OF IMAGES CONTAINED IN EACH CLASS')\n  plt.xlabel('Texture classes')\n  plt.ylabel('count')\n  plt.show()\n\noriginal_data_distribution(labels, num)","2917b88b":"# DATA AUGMENTATION\ndef augmentation(x):\n  augs = ImageDataGenerator(shear_range = 0.2,\n                            zoom_range = 0.2,\n                            horizontal_flip = True,\n                            width_shift_range=0.2,\n                            height_shift_range=0.2,\n                            rotation_range=15,\n                            vertical_flip=True,\n                            fill_mode='reflect',\n                            data_format='channels_last',\n                            brightness_range=[0.5, 1.5],\n                            featurewise_center=True,\n                            featurewise_std_normalization=True)\n  return augs.fit(x)","29e19cf7":"# def single_img_data(label_path):\n#   test_images = []\n#   for files in tqdm(os.listdir(label_path)):\n#       img_path = os.path.join(label_path, files)\n#       image = cv2.imread(img_path)\n#       image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n#       image = cv2.resize(image, IMG_SIZE) \n#       # Append the image and its corresponding label to the output\n#       test_images.append(image)\n#   test_images = np.array(test_images, dtype = 'float32')\/255\n#   return test_images\n\n# #performing image data augmentation on less data classes\n# greasy_data = single_img_data(greasy)\n# oily_data = single_img_data(oily)\n\n# augmentation(greasy_data)\n# augmentation(oily_data)","1e95714d":"# #TRAIN TEST SPLIT\n# from sklearn.model_selection import train_test_split\n# x_train, x_val, y_train, y_val = train_test_split(img_data, target_val, test_size=0.2, random_state=42, shuffle=True)","c77bd866":"# x_train = np.array(x_train)\n# y_train =  np.array(y_train)\n# x_val = np.array(x_val)\n# y_val = np.array(y_val)","4936da5a":"# augmentation(x_train)\n# augmentation(x_val)","15bf6beb":"# print('X_train shape : {} \\nX_val shape: {}'.format(x_train.shape, x_val.shape))","490e005c":"!pip install smote-variants","cea6ae23":"X = np.asarray(img_data)\ny = np.asarray(target_val)","6932083b":"import smote_variants as sv\noversampler= sv.MulticlassOversampling(sv.LLE_SMOTE())\nX_samp, y_samp= oversampler.sample(X.reshape(X.shape[0], -1), y.ravel())","21767796":"#converting back to the original shape\nX_samp = X_samp.reshape(X_samp.shape[0], 150, 150, 3)","bc63a28d":"print('X orig shape : {} \\nAfter sampling shape: {}'.format(X.shape, X_samp.shape))","e84a28c5":"print(np.unique(y, return_counts=True))\nprint(np.unique(y_samp, return_counts=True))","4b62008a":"#TRAIN TEST SPLIT\nfrom sklearn.model_selection import train_test_split\nx_train, x_val, y_train, y_val = train_test_split(X_samp, y_samp, test_size=0.2, random_state=42, shuffle=True)","9f9eb48b":"x_train = np.array(x_train)\ny_train =  np.array(y_train)\nx_val = np.array(x_val)\ny_val = np.array(y_val)","7e1f076f":"augmentation(x_train)\naugmentation(x_val)","9396773a":"print('X_train shape : {} \\nX_val shape: {}'.format(x_train.shape, x_val.shape))","77cd1ef0":"original_data_distribution(labels, num)","26a36e46":"from matplotlib.pyplot import figure\nfigure(num=None, figsize=(5, 5), dpi=80, facecolor='w', edgecolor='k')\nunique, counts = np.unique(y_train, return_counts=True)\nplt.bar(unique, counts)\nunique, counts = np.unique(y_val, return_counts=True)\nplt.bar(unique, counts)\n\nplt.title('After train-test split distribution (SMOTE-LLE)')\nplt.xlabel('Class')\nplt.xticks(unique)\nplt.ylabel('Frequency')\nplt.ylim([0,1000])\nplt.yticks(np.arange(0,1500,250))\nplt.legend('tv',loc='best')\n\nplt.show()","a8e5ecbc":"from sklearn.utils import class_weight\nclass_weights = dict(zip(np.unique(y_train), class_weight.compute_class_weight('balanced',\n                                                 np.unique(y_train),\n                                                 y_train)))","eba71a1c":"class_weights","81cc37ee":"datagen = ImageDataGenerator(shear_range = 0.2,\n                          zoom_range = 0.2,\n                          horizontal_flip = True,\n                          width_shift_range=0.2,\n                          height_shift_range=0.2,\n                          rotation_range=15,\n                          vertical_flip=True,\n                          fill_mode='reflect',\n                          data_format='channels_last',\n                          brightness_range=[0.5, 1.5],\n                          featurewise_center=True,\n                          featurewise_std_normalization=True)","52472911":"from keras.utils.data_utils import Sequence\nfrom imblearn.over_sampling import RandomOverSampler\nfrom imblearn.tensorflow import balanced_batch_generator\n  \nclass BalancedDataGenerator(Sequence):\n    \"\"\"ImageDataGenerator + RandomOversampling\"\"\"\n    def __init__(self, x, y, datagen, batch_size=32):\n        self.datagen = datagen\n        self.batch_size = min(batch_size, x.shape[0])\n        datagen.fit(x)\n        self.gen, self.steps_per_epoch = balanced_batch_generator(x.reshape(x.shape[0], -1), y, sampler=RandomOverSampler(), batch_size=self.batch_size, keep_sparse=True)\n        self._shape = (self.steps_per_epoch * batch_size, *x.shape[1:])\n        \n    def __len__(self):\n        return self.steps_per_epoch\n\n    def __getitem__(self, idx):\n        x_batch, y_batch = self.gen.__next__()\n        x_batch = x_batch.reshape(-1, *self._shape[1:])\n        return self.datagen.flow(x_batch, y_batch, batch_size=self.batch_size).next()","d07d4f56":"balanced_gen = BalancedDataGenerator(x_train, y_train, datagen, batch_size=64)\nbalanced_gen_val = BalancedDataGenerator(x_val, y_val, datagen, batch_size=64)\nsteps_per_epoch = balanced_gen.steps_per_epoch ","2875acc8":"steps_per_epoch","fd954137":"class LossLearningRateScheduler(tf.keras.callbacks.History):\n    \"\"\"\n    A learning rate scheduler that relies on changes in loss function\n    value to dictate whether learning rate is decayed or not.\n    LossLearningRateScheduler has the following properties:\n    base_lr: the starting learning rate\n    lookback_epochs: the number of epochs in the past to compare with the loss function at the current epoch to determine if progress is being made.\n    decay_threshold \/ decay_multiple: if loss function has not improved by a factor of decay_threshold * lookback_epochs, then decay_multiple will be applied to the learning rate.\n    spike_epochs: list of the epoch numbers where you want to spike the learning rate.\n    spike_multiple: the multiple applied to the current learning rate for a spike.\n    \"\"\"\n\n    def __init__(self, base_lr, lookback_epochs, spike_epochs = None, spike_multiple = 10, decay_threshold = 0.002, decay_multiple = 0.7, loss_type = 'val_loss'):\n\n        super(LossLearningRateScheduler, self).__init__()\n\n        self.base_lr = base_lr\n        self.lookback_epochs = lookback_epochs\n        self.spike_epochs = spike_epochs\n        self.spike_multiple = spike_multiple\n        self.decay_threshold = decay_threshold\n        self.decay_multiple = decay_multiple\n        self.loss_type = loss_type\n\n\n    def on_epoch_begin(self, epoch, logs=None):\n\n        if len(self.epoch) > self.lookback_epochs:\n\n            current_lr = tf.keras.backend.get_value(self.model.optimizer.lr)\n\n            target_loss = self.history[self.loss_type] \n\n            loss_diff =  target_loss[-int(self.lookback_epochs)] - target_loss[-1]\n\n            if loss_diff <= np.abs(target_loss[-1]) * (self.decay_threshold * self.lookback_epochs):\n\n                print(' '.join(('Changing learning rate from', str(current_lr), 'to', str(current_lr * self.decay_multiple))))\n                tf.keras.backend.set_value(self.model.optimizer.lr, current_lr * self.decay_multiple)\n                current_lr = current_lr * self.decay_multiple\n\n            else:\n\n                print(' '.join(('Learning rate:', str(current_lr))))\n\n            if self.spike_epochs is not None and len(self.epoch) in self.spike_epochs:\n                print(' '.join(('Spiking learning rate from', str(current_lr), 'to', str(current_lr * self.spike_multiple))))\n                tf.keras.backend.set_value(self.model.optimizer.lr, current_lr * self.spike_multiple)\n\n        else:\n\n            print(' '.join(('Setting learning rate to', str(self.base_lr))))\n            tf.keras.backend.set_value(self.model.optimizer.lr, self.base_lr)\n\n\n        return tf.keras.backend.get_value(self.model.optimizer.lr)\n","f0fc42d7":"callback_lr = LossLearningRateScheduler(base_lr=0.001, lookback_epochs=3)","2110ab12":"nb_classes ","89292342":"def plot_accuracy(history,title):\n    plt.title(title)\n    plt.plot(history.history['accuracy'])\n    plt.plot(history.history['val_accuracy'])\n    plt.ylabel('accuracy')\n    plt.xlabel('epoch')\n    plt.legend(['train_accuracy', 'validation_accuracy'], loc='best')\n    plt.show()\n\ndef plot_loss(history,title):\n    plt.title(title)\n    plt.plot(history.history['loss'])\n    plt.plot(history.history['val_loss'])\n    plt.ylabel('loss')\n    plt.xlabel('epoch')\n    plt.legend(['train_loss', 'validation_loss'], loc='best')\n    plt.show()","1c13b510":"##MODEL 1 - DENSENET 201 \nfrom tensorflow.keras.applications import DenseNet201\nimport tensorflow.keras.layers as L\nstrategy = tf.distribute.get_strategy()\n\nwith strategy.scope():\n    dnet201 = DenseNet201(\n        input_shape=(IMG_HEIGHT, IMG_WIDTH, 3) ,\n        weights='imagenet',\n        include_top=False\n    )\n    dnet201.trainable = True\n\n    model = tf.keras.Sequential([\n        dnet201,\n        L.GlobalAveragePooling2D(),\n        L.Dense(nb_classes, activation='softmax')\n    ])\n\n    optimizer = Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n    model.compile(\n        optimizer= optimizer,\n        loss = 'sparse_categorical_crossentropy',\n        metrics=['accuracy']\n    )\n\nmodel.summary()","53bd414c":"# history = model.fit(x_train, y_train, class_weight=class_weights,validation_data=(x_val, y_val),epochs=30,shuffle=True,batch_size=64)\nfilepath = 'best_model_densenet.hdf5'\nfrom tensorflow.keras.callbacks import Callback\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n\ncheckpoint = ModelCheckpoint(filepath=filepath,\n                             monitor='val_accuracy',\n                             verbose=1,\n                             save_best_only=True)\n\nstop = EarlyStopping(monitor='val_accuracy', patience =5,\n                      verbose=0, mode='auto', baseline=None, \n                      restore_best_weights=False)\n\ncallbacks = [checkpoint,stop,callback_lr]\n\nhistory_dense = model.fit(x=tf.cast(np.array(x_train), tf.float64),\n                         y=tf.cast(list(map(int,y_train)),tf.int32), \n                         epochs=50,\n                         batch_size=32,\n                         validation_data=(x_val,y_val),\n                         shuffle=True,\n                         callbacks = callbacks) ","e3b04943":"# #Saving the model\n# with open('\/content\/drive\/MyDrive\/Image Data\/deep learning textures models\/model_v2_fat_content_history2.json', 'w') as f:\n#     json.dump(history2.history, f)","d0265ff9":"plot_accuracy(history_dense, 'DENSNET')\nplot_loss(history_dense, 'DENSNET')","c5b94162":"from keras.applications.vgg16 import VGG16\n\ninput_shape=(IMG_WIDTH,IMG_HEIGHT,3)\n\ntmodel_base = VGG16(weights='imagenet', include_top=False, input_shape=input_shape)\n    \nmodel2 = Sequential()\nmodel2.add(tmodel_base)\nmodel2.add(BatchNormalization())\nmodel2.add(Dropout(0.50))\nmodel2.add(Flatten())\nmodel2.add(Dense(512, activation='relu'))\nmodel2.add(BatchNormalization())\nmodel2.add(Dropout(0.25))\nmodel2.add(Dense(nb_classes, activation='softmax', name='output_layer'))\nmodel2.summary()\n\nmodel2.compile(loss='sparse_categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n\nmodel2.summary()","5efe5edc":"filepath = 'best_model_vgg.hdf5'\nfrom tensorflow.keras.callbacks import Callback\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n\ncheckpoint = ModelCheckpoint(filepath=filepath,\n                             monitor='val_accuracy',\n                             verbose=1,\n                             save_best_only=True)\n\nstop = EarlyStopping(monitor='val_accuracy', patience =5,\n                      verbose=0, mode='auto', baseline=None, \n                      restore_best_weights=False)\n\ncallbacks = [checkpoint,stop]\n\nhistory_vgg = model2.fit(x=tf.cast(np.array(x_train), tf.float64),\n                         y=tf.cast(list(map(int,y_train)),tf.int32), \n                         epochs=50,\n                         batch_size=64,\n                         validation_data=(x_val,y_val),\n                         shuffle=True,\n                         callbacks = callbacks) ","dcf97112":"plot_accuracy(history_vgg,'FOOD101-VGG16')\nplot_loss(history_vgg,'FOOD101-VGG16')","c5ed52c1":"model3 = Sequential()\n\n# Step 1 - Convolution\nmodel3.add(Convolution2D(filters = 56,kernel_size = (3,3), activation = 'relu', input_shape = (IMG_HEIGHT,IMG_WIDTH,3)))\n\n# Step 2 - Pooling\nmodel3.add(MaxPooling2D(pool_size = (2,2)))\n\n# Adding a second convolutional layer\nmodel3.add(Convolution2D(32,(3,3),activation = 'relu'))\nmodel3.add(MaxPooling2D(pool_size = (2,2)))\n\n# Step 3 - Flattening\nmodel3.add(Flatten())\n\n# Step 4 - Full connection\nmodel3.add(Dense(units = 64, activation = 'relu'))\nmodel3.add(Dense(units = nb_classes , activation = 'softmax'))\n\n# Compiling the CNN\nmodel3.compile(optimizer = Adam(lr=0.0001), loss = 'sparse_categorical_crossentropy', metrics = ['accuracy'])\nmodel3.summary()","fd69a7a5":"# history_three = model3.fit(x=tf.cast(np.array(x_train), tf.float64),\n#                          y=tf.cast(list(map(int,y_train)),tf.int32), \n#                          epochs=30,\n#                          batch_size=32,\n#                          validation_data=(x_val,y_val)) \n\nfilepath = 'best_model_cnn.hdf5'\nfrom tensorflow.keras.callbacks import Callback\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n\ncheckpoint = ModelCheckpoint(filepath=filepath,\n                             monitor='val_accuracy',\n                             verbose=1,\n                             save_best_only=True)\n\nstop = EarlyStopping(monitor='val_accuracy', patience =5,\n                      verbose=0, mode='auto', baseline=None, \n                      restore_best_weights=False)\n\ncallbacks = [checkpoint,stop,callback_lr]\n\nhistory_cnn = model3.fit(x=tf.cast(np.array(x_train), tf.float64),\n                         y=tf.cast(list(map(int,y_train)),tf.int32), \n                         epochs=50,\n                         batch_size=32,\n                         validation_data=(x_val,y_val),\n                         shuffle=True) ","dcda6d01":"plot_accuracy(history_cnn,'FOOD101-CNN')\nplot_loss(history_cnn,'FOOD101-CNN')","95612eec":"# # k - fold validation\n# from keras.wrappers.scikit_learn import KerasClassifier\n# from keras.utils import np_utils\n# from sklearn.model_selection import cross_val_score\n# from sklearn.model_selection import KFold\n# from sklearn.preprocessing import LabelEncoder\n# from sklearn.pipeline import Pipeline\n\n# def baseline_model():\n#         # create model\n#         model3 = Sequential()\n\n#         # Step 1 - Convolution\n#         model3.add(Convolution2D(filters = 56,kernel_size = (3,3), activation = 'relu', input_shape = (IMG_HEIGHT,IMG_WIDTH,3)))\n\n#         # Step 2 - Pooling\n#         model3.add(MaxPooling2D(pool_size = (2,2)))\n\n#         # Adding a second convolutional layer\n#         model3.add(Convolution2D(32,(3,3),activation = 'relu'))\n#         model3.add(MaxPooling2D(pool_size = (2,2)))\n\n#         # Step 3 - Flattening\n#         model3.add(Flatten())\n\n#         # Step 4 - Full connection\n#         model3.add(Dense(units = 64, activation = 'relu'))\n#         model3.add(Dense(units = nb_classes , activation = 'softmax'))\n\n#         # Compile model\n#         model3.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n# \t      return model3\n\n# estimator = KerasClassifier(build_fn=baseline_model, epochs=50, batch_size=5, verbose=0)\n\n# kfold = KFold(n_splits=10, shuffle=True)\n\n# results = cross_val_score(estimator, x_train,y_train,  cv=kfold)\n# print(\"Baseline: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))","1eff2845":"#MODEL -EFFICIENTNETB3\nfrom tensorflow.keras.applications.efficientnet import EfficientNetB3\n\ndef create_model(image_input_shape):\n\n    decay = 0.000001\n    input_image = tf.keras.layers.Input(shape=image_input_shape)\n    x = EfficientNetB3( weights='imagenet', include_top=False)(input_image)\n    \n    \n    x = BatchNormalization()(x)\n    x = Dropout(0.2)(x)\n    \n    x = Flatten()(x)\n    x = Dense(256, activation='relu')(x)\n    x = Dropout(0.1)(x)\n    x = Dense(nb_classes, activation ='softmax')(x)\n    \n    return tf.keras.models.Model(inputs=input_image, outputs=x)\n\ninput_image_shape = (x_train.shape[1],x_train.shape[2],3)\ninput_image_shape ","d21ea7ce":"import tensorflow_addons as tfa\nmodel_4 = create_model(input_image_shape)\nmodel_4.compile(loss='sparse_categorical_crossentropy' ,optimizer=tfa.optimizers.LazyAdam(0.001), metrics=['accuracy'])\nmodel_4.summary()","4d16c7a4":"filepath = 'best_model.hdf5'\nfrom tensorflow.keras.callbacks import Callback\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n\ncheckpoint = ModelCheckpoint(filepath=filepath,\n                             monitor='val_accuracy',\n                             verbose=1,\n                             save_best_only=True)\n\nstop = EarlyStopping(monitor='val_accuracy', patience =5,\n                      verbose=0, mode='auto', baseline=None, \n                      restore_best_weights=False)\n\ncallbacks = [checkpoint,stop,callback_lr]\n\n# history = model.fit(x=x_train, y=y_train,validation_data = (x_val, y_val),epochs=30,shuffle=True)\n\n# history = model_4.fit(x_train, y_train, \n#                     class_weight=class_weights,\n#                     validation_data=(x_val, y_val),\n#                     epochs=30,\n#                     shuffle=True,\n#                     callbacks=callbacks)\n\nhistory_four = model_4.fit(x=tf.cast(np.array(x_train), tf.float64),\n                         y=tf.cast(list(map(int,y_train)),tf.int32), \n                         epochs=50,\n                         batch_size=32,\n                         validation_data=(x_val,y_val),\n                         shuffle=True) ","fd8a652e":"plot_accuracy(history_four,'FOOD101-EFF')\nplot_loss(history_four,'FOOD101-EFF')","53f679e7":"from keras.models import load_model\nmodel = load_model(\"best_model_densenet.hdf5\")\nloss, accuracy = model.evaluate(x_val, y_val) ","6ac2c927":"y_pred_class = np.argmax(model.predict(x_val), axis=-1)\n\nfrom sklearn.metrics import confusion_matrix\ny_pred = model.predict(x_val) \ncm = confusion_matrix(y_val, y_pred_class) \nsns.heatmap(cm, annot=True)\n\nscore = model.evaluate(x_val, y_val, verbose=0) \nprint(\"%s: %.2f%%\" % (model.metrics_names[0], score[0]*100)) \nprint(\"%s: %.2f%%\" % (model.metrics_names[1], score[1]*100))","b751b15f":"##Evaluation for model 1 [DenseNet]\nfrom sklearn.metrics import classification_report\nprint(classification_report(y_val, y_pred_class))","9d48b845":"test_images = \"..\/input\/dish-images\/ReadyToUse - 195 Dishes\"\ndish_files = os.listdir(test_images)\n\n#displaying a sample of images [original]\nimport random\nplt.figure(figsize=(20,20)) \nfor i in range(5):\n    file = random.choice(os.listdir(test_images))\n    image_path= os.path.join(test_images, file)\n    img= plt.imread(image_path)\n    x = np.array(img)\n    ax=plt.subplot(1,5,i+1)\n    ax.title.set_text(file)\n    plt.imshow(img) ","572cf9f5":"file_names = glob.glob('..\/input\/dish-images\/ReadyToUse - 195 Dishes\/*.jpg') \nimages = [i.split(\"\/\")[-1]for i in file_names]\n\ntest_data = pd.DataFrame(images,columns=['dish_id'])\ntest_data.head()","5ccea94b":"#resizing and enhancing emages\ndef unsharp_mask(img, kernel_size=(5, 5), sigma=1.0, amount=1.0, threshold=0):\n    \"\"\"Return a sharpened version of the image, using an unsharp mask.\"\"\"\n    blurred = cv2.GaussianBlur(img, kernel_size, sigma)\n    sharpened = float(amount + 1) * img - float(amount) * blurred\n    sharpened = np.maximum(sharpened, np.zeros(sharpened.shape))\n    sharpened = np.minimum(sharpened, 255 * np.ones(sharpened.shape))\n    sharpened = sharpened.round().astype(np.uint8)\n    if threshold > 0:\n        low_contrast_mask = np.absolute(img - blurred) < threshold\n        np.copyto(sharpened, img, where=low_contrast_mask)\n    return sharpened","2ac47e82":"IMAGE_SIZE = (150,150)\ndataset = \"..\/input\/dish-images\/ReadyToUse - 195 Dishes\"\noutput = [] \ntest_images = []\nfor files in tqdm(os.listdir(dataset)):\n    img_path = os.path.join(dataset, files) \n    image = cv2.imread(img_path) \n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    image = cv2.resize(image, IMAGE_SIZE, interpolation = cv2.INTER_AREA) \n    image = unsharp_mask(image)\n    # Append the image and its corresponding label to the output\n    test_images.append(image)\ntest_images = np.array(test_images, dtype = 'float32')\/255","67b47d32":"plt.imshow(test_images[150])","fca15307":"y_test_dense = np.argmax(model.predict(test_images), axis=-1)\ny_test_dense ","77cf34be":"target_dict","3f0da440":"df_dense = test_data.assign(Portion_size = y_test_dense)","83425e50":"df_dense.head()","a2512b4f":"test_labels = np.array(target_dict)[df_dense['Portion_size']]\ntest_labels","3d21e8c5":"df_predict = test_data.assign(Portion_size = test_labels)\ndf_predict['dish_id'] = df_predict['dish_id'].str.replace('.jpg','')\ndf_predict.head()","a36b868a":"recipes_df = pd.read_csv('\/content\/drive\/MyDrive\/Analysis data\/PowerBIData-195 dishes\/Viz recipes4.0.csv')\nrecipes_df = recipes_df[['dish_id','dish']]\nrecipes_df.dropna(inplace=True)\nrecipes_df.drop_duplicates(inplace=True) \n \nfinal_DF = pd.merge(df_predict, recipes_df, on='dish_id', how='left')","dc6bcd8c":"final_DF.head()","e7e69c87":"final_DF.to_csv('\/content\/drive\/MyDrive\/Analysis data\/Image analysis results\/Portion size analysis 195.csv', index=False)","a7dca627":"final_DF.loc[final_DF['Portion_size'] == 'small'].head(10)","27799b65":"img = plt.imread('\/content\/drive\/MyDrive\/Image Data\/ReadyToUse - 195 dishes\/FD_46.jpg')\nplt.imshow(img)\n# final_DF.loc[final_DF['dish_id'] == 'FD_118']","2f539a0c":"### **Train-Test spiliting**\n\n---\n\n","3bb9c3a1":"##### MODEL-4 [HANDLING WITH UNBALANCED DATA] ","29efdc3b":"##### MODEL-3 : SIMPLER CNN MODEL APPROACH","b374eb4f":"### **Evaluation**\n---","745418a6":"### **Image and data Processing**\n---\n\n\n\n\n\n\n\n","9da08581":"##### MODEL-2 : VGG16","6e8e21e9":"##### Oversampling Image Data using LLE based SMOTE","248721de":"#### Handling Unbalanced Data\n---\n---","72cdef8a":"##### Data Augmentation & Random Over Sampling\n","4c7501d3":"#####**Image Data Loading and Preprocessing**","db8fbb38":"##### Learning Rate Scheduler","72d846f9":"###**Importing Libraries**\n\n","a5a71210":"##### MODEL-1 : DENSENET","7f3266e2":"###**Model Training**\n---","794a8b96":"######Re-weighting\n","2850e9b8":"### **Prediction on Test Data**\n\n---\n\n","97924b00":"Attaching predicted labels with test data","49e1a0d1":"Loading and displaying test images data","bc46192b":"#####**Image naming and extensions handling**","6e531104":"##### **Visualising Image Data**"}}