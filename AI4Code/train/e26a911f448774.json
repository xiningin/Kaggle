{"cell_type":{"997c3fc1":"code","caa3fa25":"code","959d5068":"code","7cb9bb31":"code","75d1ec5e":"code","57793ab6":"code","fe0eb7d0":"code","550b9465":"code","bbc48070":"code","edd35a22":"code","34689102":"code","a5cdf2ad":"code","084dba74":"code","f4de4430":"code","eb7b5d32":"code","8278e30b":"code","e2240519":"code","fcda6d46":"code","6e40e841":"code","6e8600a5":"code","e2d93a5b":"code","1654cd8c":"code","88b5a272":"code","44b8752d":"code","a64b39eb":"code","dea53021":"code","b9f83502":"code","7cd553a4":"code","8d00f0c5":"code","aec664e5":"code","84b7486d":"code","02f85056":"code","453cede6":"code","fa7d55b3":"code","b0c1fb89":"code","f2973b0b":"code","57c49aa7":"code","e6d4c3b4":"code","6cf036ee":"code","97e1eb7f":"markdown","1c171cca":"markdown","809f4894":"markdown","e0a820f1":"markdown","0fdec0ad":"markdown","e2a36091":"markdown","4443a290":"markdown","e793f6f8":"markdown","8096a904":"markdown","d958636a":"markdown","93547fe8":"markdown","1fe3433c":"markdown","06a2c5ed":"markdown","fbee6ac6":"markdown","7e1f527d":"markdown","db41d651":"markdown","6bde159e":"markdown"},"source":{"997c3fc1":"import os\ngpu = !nvidia-smi --query-gpu=gpu_name --format=csv\ntry:\n    print(f\"[INFO] Environment GPU -> {gpu[1]}\")\nexcept:\n    print(\"[INFO] No GPU found\")\nprint(f\"[INFO] No. of CPU cores -> {os.cpu_count()}\")","caa3fa25":"import os\nimport gc\nimport joblib\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nfrom typing import Union\n\nfrom sklearn import metrics\nfrom sklearn.impute import KNNImputer\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.tree import ExtraTreeRegressor\n\nimport optuna\nimport lightgbm as lgb\nimport optuna.integration.lightgbm as lightgbm_tuner\nfrom xgboost import XGBRegressor\nfrom catboost import CatBoostRegressor\nfrom optuna.samplers import TPESampler\nfrom optuna.integration.lightgbm import LightGBMTunerCV\n\ntqdm.pandas()\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","959d5068":"INPUT_DIR = \"..\/input\/datathon-21\"\ntrain_path = os.path.join(INPUT_DIR, \"train.csv\")\ntest_path = os.path.join(INPUT_DIR, \"test.csv\")\n\ntrain_df_raw = pd.read_csv(train_path)\ntest_df_raw = pd.read_csv(test_path)","7cb9bb31":"train_df_raw.head()","75d1ec5e":"test_df_raw.head()","57793ab6":"print(train_df_raw.columns)\n\nprint(test_df_raw.columns)\n\nprint(train_df_raw.shape, test_df_raw.shape)","fe0eb7d0":"train_df_raw.isnull().sum()","550b9465":"test_df_raw.isnull().sum()","bbc48070":"train_df_raw.dtypes","edd35a22":"# dropping location column\ntrain_df_raw = train_df_raw.drop(['Location'], axis=1)\ntest_df_raw = test_df_raw.drop(['Location'], axis=1)","34689102":"def preprocess_data(train_df : pd.DataFrame, test_df : pd.DataFrame) -> Union[pd.DataFrame, pd.DataFrame]:\n    \n    # converting object dtype cols to float dtype\n    train_df = train_df.progress_apply(pd.to_numeric, errors='coerce', axis=1)\n    test_df = test_df.progress_apply(pd.to_numeric, errors='coerce', axis=1)\n    \n    # ------------------------\n    # filling missing values\n    # ------------------------\n    \n    # filling target nans with mean\n    target = train_df['Target_Death_Rate'].fillna(train_df['Target_Death_Rate'].mean())\n\n    train_feats = train_df.drop([\"ID\", \"Target_Death_Rate\"], axis=1) \n    test_feats = test_df.drop([\"ID\"], axis=1)\n    \n    # combining train + test for imputing and scaling\n    combined = pd.concat([train_feats, test_feats]).reset_index(drop=True)\n\n    knn_imputer = KNNImputer(n_neighbors = 3)\n    combined = pd.DataFrame(knn_imputer.fit_transform(combined), columns = combined.columns)\n    \n    # scale data (in future)\n    # doesn't work for this dataset\n    \n    # splitting back to train and test\n    train_df_processed = combined.iloc[:len(target), :]\n    train_df_processed.insert(0, 'ID', train_df['ID'].astype(int))\n\n#     train_df_processed['ID'] = train_df['ID'].astype(int)\n    train_df_processed['Target_Death_Rate'] = target\n    \n    test_df_processed = combined.iloc[len(target):, :].reset_index(drop=True)\n    test_df_processed.insert(0, 'ID', test_df['ID'].astype(int))\n    \n    print(len(train_df_processed), len(test_df_processed))\n    \n    return train_df_processed, test_df_processed","a5cdf2ad":"train_df, test_df = preprocess_data(train_df_raw, test_df_raw)","084dba74":"def add_features(train_df, test_df):\n    \n    for df in [train_df, test_df]:\n        df[\"Prv_Public_HealthCov_ratio_RateOfIncidence\"] = df[\"Prv_Public_HealthCov\"] \/ df[\"RateOfIncidence\"]\n        df[\"RateOfIncidence_ratio_Prv_Public_HealthCov\"] = df[\"RateOfIncidence\"] \/ df[\"Prv_Public_HealthCov\"]\n        df[\"RateOfIncidence_ratio_Income\"] = df[\"RateOfIncidence\"] \/ df[\"Income\"]\n        df[\"Edu_Bachdegree25_diff_Black_Residents\"] = df[\"Edu_Bachdegree25\"] - df[\"Black_Residents\"]\n        df[\"Income_ratio_RateOfIncidence\"] = df[\"Income\"] \/ df[\"RateOfIncidence\"]\n        df[\"Prv_HealthCov_ratio_RateOfIncidence\"] = df[\"Prv_HealthCov\"] \/ df[\"RateOfIncidence\"]\n        df[\"Edu_Bachdegree25_ratio_16+_Unemployed\"] = df[\"Edu_Bachdegree25\"] \/ df[\"16+_Unemployed\"]\n        df[\"Married_sum_Edu_Bachdegree25\"] = df[\"Married\"] + df[\"Edu_Bachdegree25\"]\n        df[\"RateOfIncidence_ratio_Prv_HealthCov\"] = df[\"RateOfIncidence\"] \/ df[\"Prv_HealthCov\"]\n        df[\"Poverty%_multiply_RateOfIncidence\"] = df[\"Poverty%\"] * df[\"RateOfIncidence\"]\n        \n        # drop unecessary columns\n        df = df.drop([\"Married%\", \"Edu_less_HS\", \"Edu_Clg\"], axis=1)\n    \n    return train_df, test_df","f4de4430":"train_df, test_df = add_features(train_df, test_df)\n\nprint(len(train_df.columns), len(test_df.columns))","eb7b5d32":"def create_folds(data):\n    data[\"fold\"] = -1\n\n    data = data.sample(frac=1).reset_index(drop=True)\n\n    # calculate the number of bins by Sturge's rule\n    num_bins = int(np.floor(1 + np.log2(len(data))))\n\n    # bin targets\n    data.loc[:, \"bins\"] = pd.cut(data[\"Target_Death_Rate\"], bins=num_bins, labels=False)\n\n    kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\n    # fill the new kfold column\n    # note, instead of targets use bins\n\n    for f, (t_, v_) in enumerate(kf.split(X=data, y=data.bins.values)):\n        data.loc[v_, 'fold'] = f\n\n    data = data.drop(\"bins\", axis=1)\n    data['fold'] = data['fold'].astype(int)\n    return data","8278e30b":"train_folds = create_folds(train_df).reset_index(drop=True)\n\nprint(train_folds['fold'].value_counts())","e2240519":"# target distribution of complete train data\ntrain_folds['Target_Death_Rate'].plot.kde()\n# target distribution in each fold\nfor fold in range(5):\n    target_curr_fold = train_folds[train_folds[\"fold\"] == fold]['Target_Death_Rate']\n    target_curr_fold.plot.kde()","fcda6d46":"train_folds.head()","6e40e841":"# final sanity check to remove any nan or inf values\ndef clean_dataset(df):\n    assert isinstance(df, pd.DataFrame), \"df needs to be a pd.DataFrame\"\n    df.dropna(inplace=True)\n    indices_to_keep = ~df.isin([np.nan, np.inf, -np.inf]).any(1)\n    return df[indices_to_keep].astype(np.float64)\n\ntrain_folds = clean_dataset(train_folds)\nprint(train_folds.shape)","6e8600a5":"def train_model(train_folds : pd.DataFrame, model_dispatcher : dict, folds : list):\n    \n    for name, model in model_dispatcher.items():\n        print(\"\\n\\n\" + \"-\" * 10 + f\" Training {name} \" + \"-\"*10)\n        \n        cvs = []        \n        for fold in folds:\n            train = train_folds[train_folds['fold'] != fold].reset_index(drop=True)\n            val = train_folds[train_folds['fold'] == fold].reset_index(drop=True)\n\n            x_train = train.drop([\"ID\", \"fold\", \"Target_Death_Rate\"], axis=1)\n            y_train = train[\"Target_Death_Rate\"]\n            \n            \n            x_val = val.drop([\"ID\", \"fold\", \"Target_Death_Rate\"], axis=1)\n            x_val = x_val[x_train.columns]\n            y_val = val[\"Target_Death_Rate\"]\n\n            model.fit(x_train, y_train)\n\n            val_preds = model.predict(x_val)\n            r2_score = metrics.r2_score(y_val, val_preds)\n            print(f\"{name}  |  fold: {fold}   |   r2_score: {r2_score:.5f}\")\n            \n            # save model\n            model_dir = f\"\/kaggle\/working\/{name}_models\"\n            os.makedirs(model_dir, exist_ok=True)\n            joblib.dump(model, os.path.join(model_dir, f\"{name}_fold_{fold}.pkl\"))\n            \n            cvs.append(r2_score)\n        print(f\"\\tMean CV --> {np.array(cvs).mean():.5f}\")","e2d93a5b":"model_dispatcher = {\n    \"randomforest\" : RandomForestRegressor(n_jobs=-1),\n    \"xgboost\" : XGBRegressor(n_jobs=-1),\n    \"catboost\" : CatBoostRegressor(thread_count=-1,silent=True),\n    \"lightbgm\" : lgb.LGBMRegressor(),\n    \"knn\" : KNeighborsRegressor(),\n    \"svr\" : SVR(),\n    \"extratrees\" : ExtraTreeRegressor(),\n    \"GBDT\" : GradientBoostingRegressor()}\n\n# train 5-fold CV models for model_dispatcher\ntrain_model(train_folds, model_dispatcher, folds = [0, 1, 2, 3, 4])\n\ngc.collect()","1654cd8c":"def inference(model_name):\n    \n    test_feats = test_df.drop([\"ID\"], axis=1)\n    \n    fin_preds = np.zeros(953)\n    cv = 0\n    for fold in range(5):\n        \n        t_model = joblib.load(f\"\/kaggle\/working\/{model_name}_models\/{model_name}_fold_{fold}.pkl\")\n        \n        preds = t_model.predict(test_feats)\n        fin_preds += preds\n        \n    fin_preds \/= 5\n    \n    return fin_preds","88b5a272":"fin_fin_preds = np.zeros(953)\nnames = [\"catboost\", \"GBDT\", \"lightbgm\", \"randomforest\"]\nfor name in names:\n    fin_preds = np.round(inference(model_name=name), 1)\n    print(fin_preds[:5])\n    fin_fin_preds += fin_preds\n\nfin_fin_preds \/= 4\nprint(f\"Final preds -> {fin_fin_preds[:5]}\")","44b8752d":"# submission\nsub = pd.DataFrame()\nsub['ID'] = test_df['ID'].astype(int)\nsub['Target_Death_Rate'] = fin_fin_preds\nsub.to_csv('vanilla_avg_blend.csv', index=False)\n\nsubmit_path = '.\/vanilla_avg_blend.csv'\nsubmit = pd.read_csv(submit_path)\ndisplay(submit.head())\nsubmit.shape\nsubmit.dtypes","a64b39eb":"train_feats = train_folds.drop([\"ID\", \"Target_Death_Rate\", \"fold\"], axis=1)\ntarget = train_folds['Target_Death_Rate']\ntest_feats = test_df.drop([\"ID\"], axis=1)","dea53021":"def objective(trial):\n\n    param_grid = {\n        \"loss_function\": trial.suggest_categorical(\"loss_function\", [\"RMSE\", \"MAE\"]),\n        \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-5, 1e0),\n        \"l2_leaf_reg\": trial.suggest_loguniform(\"l2_leaf_reg\", 1e-2, 1e0),\n        \"colsample_bylevel\": trial.suggest_float(\"colsample_bylevel\", 0.01, 0.1),\n        \"depth\": trial.suggest_int(\"depth\", 1, 10),\n        \"boosting_type\": trial.suggest_categorical(\"boosting_type\", [\"Ordered\", \"Plain\"]),\n        \"bootstrap_type\": trial.suggest_categorical(\"bootstrap_type\", [\"Bayesian\", \"Bernoulli\", \"MVS\"]),\n        \"min_data_in_leaf\": trial.suggest_int(\"min_data_in_leaf\", 2, 20),\n        \"one_hot_max_size\": trial.suggest_int(\"one_hot_max_size\", 2, 20),  \n    }\n    \n    # Conditional Hyper-Parameters\n    if param_grid[\"bootstrap_type\"] == \"Bayesian\":\n        param_grid[\"bagging_temperature\"] = trial.suggest_float(\"bagging_temperature\", 0, 10)\n    elif param_grid[\"bootstrap_type\"] == \"Bernoulli\":\n        param_grid[\"subsample\"] = trial.suggest_float(\"subsample\", 0.1, 1)\n\n    reg = CatBoostRegressor(**param_grid, thread_count=-1)\n    \n    scores = 0\n    for fold in range(5):\n        train = train_folds[train_folds['fold'] != fold].reset_index(drop=True)\n        val = train_folds[train_folds['fold'] == fold].reset_index(drop=True)\n\n        x_train = train.drop([\"ID\", \"fold\", \"Target_Death_Rate\"], axis=1)\n        y_train = train[\"Target_Death_Rate\"]\n            \n            \n        x_val = val.drop([\"ID\", \"fold\", \"Target_Death_Rate\"], axis=1)\n        x_val = x_val[x_train.columns]\n        y_val = val[\"Target_Death_Rate\"]\n\n        reg.fit(x_train, y_train, eval_set=[(x_val, y_val)], verbose=0, early_stopping_rounds=100)\n\n        val_preds = reg.predict(x_val)\n        r2_score = metrics.r2_score(y_val, val_preds)\n        \n        scores += r2_score\n    \n    score = scores \/ 5\n    return score\n\n# run hyperparameter search\nstudy_cat = optuna.create_study(sampler=TPESampler(), direction=\"maximize\")\nstudy_cat.optimize(objective, n_trials=100)","b9f83502":"print(\"Number of completed trials: {}\".format(len(study_cat.trials)))\nprint(\"Best trial:\")\ntrial_cat = study_cat.best_trial\nprint(\"\\tBest Score Catboost: {}\".format(trial_cat.value))\nprint(\"\\tBest Params Catboost: \")\nfor key, value in trial_cat.params.items():\n    print(\"    {}: {}\".format(key, value))\n\n# inference with tuned model\ntuned_cat = CatBoostRegressor(**trial_cat.params, silent=True)\n\ntuned_cat.fit(train_feats, target)\ncat_tuned_preds = tuned_cat.predict(test_feats)","7cd553a4":"# def objective(trial):\n    \n#     param_grid = {\n#             'criterion' : trial.suggest_categorical('criterion', ['mse', 'mae']),\n#             'bootstrap' : trial.suggest_categorical('bootstrap',['True','False']),\n#             'max_depth' : trial.suggest_int('max_depth', 1, 10000),\n#             'max_features' : trial.suggest_categorical('max_features', ['auto', 'sqrt','log2']),\n#             'min_samples_split': trial.suggest_int('min_samples_split', 1, 150),\n#             'min_samples_leaf': trial.suggest_int('min_samples_leaf', 2, 60),\n# #             'max_leaf_nodes' : trial.suggest_int('max_leaf_nodes', 1, 10000),\n#             'n_estimators' : trial.suggest_int('n_estimators', 30, 1500),\n        \n#     }\n\n    \n#     reg = RandomForestRegressor(**param_grid, n_jobs=-1)\n    \n    \n\n#     scores = 0\n#     for fold in range(5):\n#         train = train_folds[train_folds['fold'] != fold].reset_index(drop=True)\n#         val = train_folds[train_folds['fold'] == fold].reset_index(drop=True)\n\n#         x_train = train.drop([\"ID\", \"fold\", \"Target_Death_Rate\"], axis=1)\n#         y_train = train[\"Target_Death_Rate\"]\n            \n            \n#         x_val = val.drop([\"ID\", \"fold\", \"Target_Death_Rate\"], axis=1)\n#         x_val = x_val[x_train.columns]\n#         y_val = val[\"Target_Death_Rate\"]\n\n#         reg.fit(x_train, y_train)\n\n#         val_preds = reg.predict(x_val)\n#         r2_score = metrics.r2_score(y_val, val_preds)\n        \n#         scores += r2_score\n    \n#     score = scores \/ 5\n#     return score\n\n# # ------------ skipping because of high execution time ------------\n# # run hyperparameter search\n# study_rf = optuna.create_study(direction='maximize')\n# study_rf.optimize(objective, n_trials=20)","8d00f0c5":"# print(\"Number of completed trials: {}\".format(len(study_rf.trials)))\n# print(\"Best trial:\")\n# trial_rf = study_rf.best_trial\n# print(\"\\tBest Score RandomForest: {}\".format(trial_rf.value))\n# print(\"\\tBest Params RandomForest: \")\n# for key, value in trial_rf.params.items():\n#     print(\"    {}: {}\".format(key, value))\n\n# # inference with tuned model\n# tuned_rf = RandomForestRegressor(**trial_rf.params)\n# tuned_rf.fit(train_feats, target)\n\n# rf_tuned_preds = tuned_rf.predict(test_feats)","aec664e5":"def objective(trial):\n    \n    param = {\n#         'tree_method':'gpu_hist',  # this parameter means using the GPU when training our model to speedup the training process\n        'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n        'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n        'colsample_bytree': trial.suggest_categorical('colsample_bytree', [0.3,0.4,0.5,0.6,0.7,0.8,0.9, 1.0]),\n        'subsample': trial.suggest_categorical('subsample', [0.4,0.5,0.6,0.7,0.8,1.0]),\n        'learning_rate': trial.suggest_categorical('learning_rate', [0.008,0.009,0.01,0.012,0.014,0.016,0.018, 0.02]),\n        'n_estimators': 4000,\n        'max_depth': trial.suggest_categorical('max_depth', [5,7,9,11,13,15,17,20]),\n        'random_state': trial.suggest_categorical('random_state', [24, 48,2020]),\n        'min_child_weight': trial.suggest_int('min_child_weight', 1, 300),\n    }\n    \n#     if gpu:\n#         param['gpu_id'] = 0\n#         param['tree_method'] = 'gpu_hist'\n    \n    reg = XGBRegressor(**param, n_jobs=-1)\n    scores = 0\n    for fold in range(5):\n        train = train_folds[train_folds['fold'] != fold].reset_index(drop=True)\n        val = train_folds[train_folds['fold'] == fold].reset_index(drop=True)\n\n        x_train = train.drop([\"ID\", \"fold\", \"Target_Death_Rate\"], axis=1)\n        y_train = train[\"Target_Death_Rate\"]\n            \n            \n        x_val = val.drop([\"ID\", \"fold\", \"Target_Death_Rate\"], axis=1)\n        x_val = x_val[x_train.columns]\n        y_val = val[\"Target_Death_Rate\"]\n\n        reg.fit(x_train, y_train, eval_set=[(x_val, y_val)], verbose=0, early_stopping_rounds=100)\n\n        val_preds = reg.predict(x_val)\n        r2_score = metrics.r2_score(y_val, val_preds)\n        \n        scores += r2_score\n    \n    score = scores \/ 5\n    return score\n\n# run hyperparameter-search\nstudy_xgb = optuna.create_study(direction=\"maximize\")\nstudy_xgb.optimize(objective, n_trials=100)","84b7486d":"print(\"Number of completed trials: {}\".format(len(study_xgb.trials)))\nprint(\"Best trial:\")\ntrial_xgb = study_xgb.best_trial\nprint(\"\\tBest Score XGBoost: {}\".format(trial_xgb.value))\nprint(\"\\tBest Params XGBoost: \")\nfor key, value in trial_xgb.params.items():\n    print(\"    {}: {}\".format(key, value))\n\n# inference with tuned model\n\ntuned_xgb = XGBRegressor(**trial_xgb.params)\ntuned_xgb.fit(train_feats, target)\n\nxgb_tuned_preds = tuned_xgb.predict(test_feats)","02f85056":"def tune_lgbm():\n        \n    params = {\n        'objective': 'regression',\n        'metric': 'rmse'\n    }\n\n    score = 0\n    tuned_folds_lgbm = []\n    for fold in range(5):\n        train = train_folds[train_folds['fold'] != fold].reset_index(drop=True)\n        val = train_folds[train_folds['fold'] == fold].reset_index(drop=True)\n\n        x_train = train.drop([\"ID\", \"fold\", \"Target_Death_Rate\"], axis=1)\n        y_train = train[\"Target_Death_Rate\"]\n            \n            \n        x_val = val.drop([\"ID\", \"fold\", \"Target_Death_Rate\"], axis=1)\n        x_val = x_val[x_train.columns]\n        y_val = val[\"Target_Death_Rate\"]\n        \n        lgb_train = lgb.Dataset(x_train, y_train)\n        lgb_valid = lgb.Dataset(x_val, y_val, reference=lgb_train)\n        tuned_lgbm = lightgbm_tuner.train(params, lgb_train, \n                                      valid_sets=lgb_valid,\n                                      num_boost_round=1000,\n                                      early_stopping_rounds=100,\n                                      verbose_eval=10)\n        \n        val_preds = tuned_lgbm.predict(x_val)\n        r2_score = metrics.r2_score(y_val, val_preds)\n        \n        score += r2_score\n        tuned_folds_lgbm.append(tuned_lgbm)\n        \n    score \/= 5\n    return tuned_folds_lgbm, score\n\n# # run hyperparameter-search\n# study_lgbm = optuna.create_study(direction=\"maximize\")\n# study_lgbm.optimize(objective, n_trials=100)\n\ntuned_lgbm, score = tune_lgbm()","453cede6":"print(f\"Score of tuned LightGBM --> {score}\")\ntuned_lgbm_preds = np.zeros(953)\nfor m in tuned_lgbm:\n    preds = m.predict(test_feats)\n    tuned_lgbm_preds += preds\ntuned_lgbm_preds \/= 5\ntuned_lgbm_preds[:5]","fa7d55b3":"# tuned_cat_preds, tuned_xgb_preds, tuned_lgbm_preds --> preds on test set\n# tuned_cat, tuned_xgb, tuned_lgbm --> models\n\nfinal_model_dispatcher = {\n    # vanilla models\n    \"randomforest\": None,\n    \"catboost\": None,\n    \"lightbgm\": None,\n    \"xgboost\": None,\n    \"GBDT\" : None,\n    # tuned model \n    \"tuned_catboost\": CatBoostRegressor(**trial_cat.params, silent=True),\n    \"tuned_xgboost\": XGBRegressor(**trial_xgb.params),\n    \n    \n    \n}\n\npreds_df = pd.DataFrame()\nval_pred_dfs = []\n\nfor name, model in final_model_dispatcher.items():\n\n    cv_score = 0 \n    for fold in range(5):\n        train = train_folds[train_folds['fold'] != fold].reset_index(drop=True)\n        val = train_folds[train_folds['fold'] == fold].reset_index(drop=True)\n\n        x_train = train.drop([\"ID\", \"fold\", \"Target_Death_Rate\"], axis=1)\n        y_train = train['Target_Death_Rate']\n        x_val = val.drop([\"ID\", \"fold\", \"Target_Death_Rate\"], axis=1)\n        x_val = x_val[x_train.columns]\n        y_val = val[\"Target_Death_Rate\"]\n\n        if model is None:\n            model = joblib.load(f\"\/kaggle\/working\/{name}_models\/{name}_fold_{fold}.pkl\")\n            val_preds = model.predict(x_val)\n        else:\n            model.fit(x_train, y_train)\n            val_preds = model.predict(x_val)\n        r2_score = metrics.r2_score(y_val, val_preds)\n\n        cv_score += r2_score    \n        val.loc[:, f\"{name}_pred\"] = val_preds\n        val_pred_dfs.append(val[['ID', 'fold', 'Target_Death_Rate', f\"{name}_pred\"]])\n    cv_score \/= 5\n    print(f\"Model {name} cv score --> {cv_score:.5f}\")\n    \noof_df = pd.concat(val_pred_dfs)\n\n# getting lightgbm predictions for each fold\ncv_lgb = 0\nval_lgb_pred = []\nfor m in tuned_lgbm:\n    train = train_folds[train_folds['fold'] != fold].reset_index(drop=True)\n    val = train_folds[train_folds['fold'] == fold].reset_index(drop=True)\n\n    x_train = train.drop([\"ID\", \"fold\", \"Target_Death_Rate\"], axis=1)\n    x_val = val.drop([\"ID\", \"fold\", \"Target_Death_Rate\"], axis=1)\n    x_val = x_val[x_train.columns]\n    y_val = val[\"Target_Death_Rate\"]\n    \n    val_preds = m.predict(x_val)\n    val.loc[:, \"tuned_lgbm_preds\"] = val_preds\n    val_lgb_pred.append(val[['ID', 'fold', 'Target_Death_Rate', \"tuned_lgbm_preds\"]])\n\nval_lgb_pred = pd.concat(val_lgb_pred)\n\n\n# xgboost has some bug for some reason, maybe implementation wasn't correct","b0c1fb89":"from functools import partial\nfrom scipy.optimize import fmin\nclass OptimizeR2Score:\n    def __init__(self):\n        self.coef_ = 0\n    \n    def _r2_score(self, coef, X, y):\n        x_coef = X * coef\n        predictions = np.sum(x_coef, axis=1)\n        r2_score = metrics.r2_score(y, predictions)\n        return r2_score\n    \n    def fit(self, X, y):\n        partial_loss = partial(self._r2_score, X=X, y=y)\n        init_coef = np.random.dirichlet(np.ones(X.shape[1]))\n        self.coef_ = fmin(partial_loss, init_coef, disp=True)\n    \n    def predict(self, X):\n        x_coef = X * self.coef_\n        predictions = np.sum(x_coef, axis=1)\n        return predictions","f2973b0b":"# def find_optimal_blend_weights(fold):\n#     train = train_folds[train_folds['fold'] != fold].reset_index(drop=True)\n#     val = train_folds[train_folds['fold'] == fold].reset_index(drop=True)\n\n#     x_train = train.drop([\"ID\", \"fold\", \"Target_Death_Rate\"], axis=1)\n#     x_val = val.drop([\"ID\", \"fold\", \"Target_Death_Rate\"], axis=1)\n#     x_val = x_val[x_train.columns]\n#     y_val = val[\"Target_Death_Rate\"]\n\n#     opt_blend = OptimizeR2Score()\n#     opt_blend.fit(x_train, y_train)\n#     preds = opt.predict(x_val)\n#     r2_score = metrics.r2_score(x_val, preds)\n#     print(f\"fold: {fold}, r2_score: {r2_score}\")\n    \n#     return opt_blen.coef_\n\n# # run\n# targets = train_folds['Target_Death_Rate']","57c49aa7":"vanilla_avg_blend = pd.read_csv(\".\/vanilla_avg_blend.csv\")\nvanilla_avg_blend_preds = vanilla_avg_blend[\"Target_Death_Rate\"]\nvanilla_avg_blend_preds[:5]","e6d4c3b4":"final_sub_preds = 0.6 * cat_tuned_preds + 0.1 * tuned_lgbm_preds + 0.3 * vanilla_avg_blend_preds\nfinal_sub_preds[:10]","6cf036ee":"# submission\nsub_file_name = \"tuned_cat_lgbm_vanilla_avg_blend.csv\"\nsub = pd.DataFrame()\nsub['ID'] = test_df['ID'].astype(int)\nsub['Target_Death_Rate'] = final_sub_preds\nsub.to_csv(sub_file_name, index=False)\n\nsubmit_path = sub_file_name\nsubmit = pd.read_csv(submit_path)\ndisplay(submit.head())\nsubmit.shape\nsubmit.dtypes","97e1eb7f":"# Train vanilla models","1c171cca":"# EDA","809f4894":"# Blending","e0a820f1":"# LightGBM + Optuna","0fdec0ad":"# Xgboost + Optuna","e2a36091":"#### All 5 folds have same distribution of target variable.Now, data is ready for modelling","4443a290":"# 5-fold Cross-validation setup","e793f6f8":"# Inference","8096a904":"# Hyperparameter Tuning","d958636a":"# Catboost + Optuna","93547fe8":"# Submission","1fe3433c":"## Cleaning","06a2c5ed":"# Feature Engineering","fbee6ac6":"# Read data","7e1f527d":"# Imports","db41d651":"# Finding optimal weights for blending","6bde159e":"# RandomForest + Optuna (skipped because of high execution time)"}}