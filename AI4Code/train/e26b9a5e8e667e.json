{"cell_type":{"63055544":"code","15f8085a":"code","c52bcca0":"code","bbedcec5":"code","a01da479":"code","6bba3b12":"code","443f059e":"code","d9cdf7ce":"code","588b7408":"code","4c3aea81":"code","e7c2365b":"code","206df4b5":"code","f60f9271":"code","dfff2cf1":"code","b3076ed2":"markdown","1503e812":"markdown","7264ded3":"markdown","e4f6db4b":"markdown","07098f2a":"markdown"},"source":{"63055544":"# Install Dependencies\n\nimport numpy as np \nimport pandas as pd\nimport time\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\nimport matplotlib.pyplot as plt","15f8085a":"# Import Data\n\nfrom subprocess import check_output\nfrom datetime import time\nprint(check_output([\"ls\", \"..\/input\"]).decode(\"utf8\"))\n\ndf = pd.read_csv(\"..\/input\/dataset.csv\")\nprint(df.shape)\ndf.head()","c52bcca0":"# Data Preprocess, remove un-required features\n\ndf = df.drop([\"date\", \"time\", \"username\"], axis=1)\nprint(df.describe())\ndata = df.values\nX = data[:, 1:]  # all rows, no label\ny = data[:, 0]  # all rows, label only","bbedcec5":"# Model Definition\ndef svmClassifier(scaledData_X, scaledData_Y, firstRun=0, model=None):\n    '''\n    \"It accepts the scaled features (training data) and trains a SVM Regressor\"\n    Args:\n        Accepts scaled training dataset and 'firstRun' flag to distinguish between first time and online training.\n    Returns:\n        Loaded Model    \n    '''\n    \n    try:\n        # for reproducible result\n        np.random.seed(3)\n        #HyperParameter and model element asigning\n        xTrain = scaledData_X\n        yTrain = scaledData_Y\n\n        if yTrain.shape[0] != 1:\n            yTrain = yTrain.reshape(-1,1)\n\n        xShape = xTrain.shape\n        # print(xShape)\n        yShape = yTrain.shape\n        # print(\"input: \", xShape[1], \" output: \", yShape)\n\n        # Make sure model is only created once and trained numerously\n        if(firstRun==1):\n            model = SGDClassifier(loss=\"hinge\", penalty=\"l2\", alpha=0.0001, max_iter=3000, tol=None, shuffle=True, verbose=0, learning_rate='adaptive', eta0=0.01, early_stopping=False)\n            # Suggested number of passes for convergence\n            model.n_iter = np.ceil(10**6 \/ len(scaledData_Y))\n            # Train model using fit\n            model.fit(xTrain, yTrain)\n        else:\n            # Use the model passed to the function\n            # As partial fit only runs for 1 epoch, we repeat for all samples.\n            for _ in range(3):\n                for i in range(xShape[0]):\n                    #print(f'for i={i}, \\t actual {xTrain[i].shape}')\n                    x = xTrain[i].reshape(1, -1)\n                    #print(f'Now, {x.shape}')\n                    model.partial_fit(x, yTrain[i])\n            \n        \n        print('Done Training')\n        message = \"Successfully trained SVMClassifier\"\n\n    except Exception as e:\n        message = e\n        model = {}\n\n    return message, model\n","a01da479":"# Data Split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)","6bba3b12":"# Scale the data to be between -1 and 1\nscaler = StandardScaler()\nscaler.fit(X_train)\nX_train = scaler.transform(X_train)\nX_test = scaler.transform(X_test)","443f059e":"# Train\nmessage, model = svmClassifier(X_train, y_train, 1); print(message)\nmodel.score(X_test, y_test)","d9cdf7ce":"# Things done only once...\nX_split = np.array_split(X, 4)\ny_split = np.array_split(y, 4)\nscaler = StandardScaler()","588b7408":"# Part 1: fit for first time\nX_train, X_test, y_train, y_test = train_test_split(X_split[0],y_split[0],random_state=999)\n\n# Scale the data to be between -1 and 1\nscaler.fit(X_train)\nX_train = scaler.transform(X_train)\nX_test = scaler.transform(X_test)\n\nmessage, model = svmClassifier(X_train, y_train, 1); print(message)\nmodel.score(X_test, y_test)\n","4c3aea81":"#Save model\nimport pickle\nmodelSaveFile = 'model_data_id.sav'\npickle.dump(model, open(modelSaveFile, 'wb'))","e7c2365b":"# Part 2: partial_fit\nX_train, X_test, y_train, y_test = train_test_split(X_split[1],y_split[1],random_state=999)\n\n# Load the model\nmodel = pickle.load(open(modelSaveFile, 'rb'))\n# Scale the data to be between -1 and 1\nscaler.fit(X_train)\nX_train = scaler.transform(X_train)\nX_test = scaler.transform(X_test)\n\nmessage, model = svmClassifier(X_train, y_train, 0, model); print(message)\nprint(model.score(X_test, y_test))\n\n# Save model\nmodelSaveFile = 'model_data_id2.sav'\npickle.dump(model, open(modelSaveFile, 'wb'))","206df4b5":"# Part 3: partial_fit\nX_train, X_test, y_train, y_test = train_test_split(X_split[2],y_split[2],random_state=999)\n\n# Load the model\nmodel = pickle.load(open(modelSaveFile, 'rb'))\n# Scale the data to be between -1 and 1\nscaler.fit(X_train)\nX_train = scaler.transform(X_train)\nX_test = scaler.transform(X_test)\n\nmessage, model = svmClassifier(X_train, y_train, 0, model); print(message)\nprint(model.score(X_test, y_test))\n\n# Save model\nmodelSaveFile = 'model_data_id3.sav'\npickle.dump(model, open(modelSaveFile, 'wb'))","f60f9271":"# Part 4: partial_fit\nX_train, X_test, y_train, y_test = train_test_split(X_split[3],y_split[3],random_state=999)\n\n# Load the model\nmodel = pickle.load(open(modelSaveFile, 'rb'))\n# Scale the data to be between -1 and 1\nscaler.fit(X_train)\nX_train = scaler.transform(X_train)\nX_test = scaler.transform(X_test)\n\nmessage, model = svmClassifier(X_train, y_train, 0, model); print(message)\nprint(model.score(X_test, y_test))\n\n# Save model\nmodelSaveFile = 'model_data_id4.sav'\npickle.dump(model, open(modelSaveFile, 'wb'))","dfff2cf1":"from sklearn.metrics import accuracy_score\nfrom sklearn.metrics import f1_score\n\ndef metrics(yActual, yPredicted):\n    print(f'accuracy_score: {accuracy_score(yActual, yPredicted)}')\n    print(f\"f1 score: {f1_score(yActual, yPredicted, average='weighted')}\")\n\nmetrics(y_test, model.predict(X_test))","b3076ed2":"# 2. For online training","1503e812":"# 1. For Entire Data","7264ded3":"## Observation\n\nUsually, `partial_fit` has seen to be prone to [reduction](https:\/\/stackoverflow.com\/questions\/47665417\/mlp-with-partial-fit-performing-worse-than-with-fit-in-a-supervised-classifi) or [fluctuation](https:\/\/stackoverflow.com\/questions\/44004926\/partial-fit-with-sgdclassifier-gives-fluctuating-accuracy) in accuracy. To some extent, this can be slightly mitigated by shuffling and provinding __only__ small fractions of the entire dataset. But, for larger data, online training only seems to give reducing accuracies, with SGDClassifier\/SVM Classifier. \n\nI tried to experiment with it and discovered that using a low learning rate can help us sometimes. The rough analogy is, on training the same model on large data repeateadly, leads to the model forgetting what it learnt from the previous data. So, using a __tiny__ learning rate slows down the rate of learning as well as forgetting! \n\nRather than manually providing a rate, we can use `adaptive` learning rate functionality provided by `sklearn`. Notice the model initialisation part, \n\n```python3\nmodel = SGDClassifier(loss=\"hinge\", penalty=\"l2\", alpha=0.0001, max_iter=3000, tol=None, shuffle=True, verbose=0, learning_rate='adaptive', eta0=0.01, early_stopping=False)\n```\n\nThis is described in the [scikit docs] as:\n> \u2018adaptive\u2019: eta = eta0, as long as the training keeps decreasing. Each time n_iter_no_change consecutive epochs fail to decrease the training loss by tol or fail to increase validation score by tol if early_stopping is True, the current learning rate is divided by 5.\n\nInitially, with each round of new data, we would get the drop in accuracies as:\n\n7.9 -> 98.89 -> 47.7 -> 29.4\n\nNow, we get better results... although, overfitting lurks around... with 100% accuracy.","e4f6db4b":"## Metrics and Scoring\n\nUse the function to evaluate model performance.","07098f2a":"# Online SVM Classifier\n\nThis notebook aims to demonstrate how to train an online classifier based on SGD Classifier. It is based on the [SGDClassifer demonstrated](https:\/\/www.kaggle.com\/nsrose7224\/sgdclassifier) by [Nick Rose](https:\/\/www.kaggle.com\/nsrose7224) and uses the dataset provided by [Viktor Malyi](https:\/\/www.kaggle.com\/vmalyi)\n\nI will try to demonstrate how good performance can be obtained in either of the cases:\n\n1. Providing the entire dataset together.\n2. Online Training: Retraining the model repeatedly, for all fractions of dataset"}}