{"cell_type":{"70538455":"code","c4dcd14a":"code","d7f82688":"code","9a4e820c":"code","8ec20cd9":"code","c904b134":"code","cc6052e2":"code","4e6037d1":"code","ee8f8d03":"code","26e687c3":"code","65caa74b":"code","089ce5e7":"code","e28d766e":"code","618c66f3":"code","e54d535f":"code","3dd9f1ac":"code","828b0e6f":"code","33cd1e0b":"code","d13a8fe3":"code","7f35ffa2":"code","65cdf03b":"code","843d7a15":"code","616aeb46":"code","8763ae24":"code","0fd7b259":"code","19e38942":"code","695da0a7":"code","4fe8f9c5":"code","3dcd4ce3":"code","680e60c0":"code","2edbdcb7":"code","69436304":"code","8f9efff2":"code","001b10fb":"code","e017a275":"code","ae4b023f":"code","5113649c":"code","c96550c9":"code","7683d0e7":"code","498b1730":"code","3edc946a":"code","bab08fc8":"markdown","d8ed0a87":"markdown","39efe8f4":"markdown","df8f4312":"markdown","b9b236ea":"markdown","e77deec1":"markdown","afefd597":"markdown","b50582f6":"markdown","e55ff08d":"markdown","200a2f99":"markdown","b3680096":"markdown","7a65af49":"markdown","24cc4302":"markdown","f4259281":"markdown","cc67bb77":"markdown","3acd5e45":"markdown","172b7137":"markdown","e13f04bd":"markdown"},"source":{"70538455":"# import required libraries\nimport numpy as np\nimport pandas as pd\n\n# visualizations \nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn import metrics\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score, roc_auc_score, plot_roc_curve","c4dcd14a":"# to display all columns\npd.options.display.max_columns = 999","d7f82688":"# igonre warnings\nimport warnings\nwarnings.filterwarnings('ignore')","9a4e820c":"train_data = pd.read_csv('..\/input\/tabular-playground-series-nov-2021\/train.csv')\ntest_data = pd.read_csv('..\/input\/tabular-playground-series-nov-2021\/test.csv')","8ec20cd9":"train_data.head()","c904b134":"test_data.head()","cc6052e2":"print(train_data.shape)\nprint(test_data.shape)\nprint(train_data.info())\nprint(test_data.info())","4e6037d1":"def reduce_memory_usage(df, verbose=True):\n    numerics = [\"int8\", \"int16\", \"int32\", \"int64\", \"float16\", \"float32\", \"float64\"]\n    start_mem = df.memory_usage().sum() \/ 1024 ** 2\n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == \"int\":\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n    end_mem = df.memory_usage().sum() \/ 1024 ** 2\n    if verbose:\n        print(\n            \"Mem. usage decreased to {:.2f} Mb ({:.1f}% reduction)\".format(\n                end_mem, 100 * (start_mem - end_mem) \/ start_mem\n            )\n        )\n    return df","ee8f8d03":"train_data = reduce_memory_usage(train_data)\ntest_data = reduce_memory_usage(test_data)","26e687c3":"# lets use 1Lakh of data for our models building\ntrain_data['target'].value_counts(normalize=True)*100","65caa74b":"train_data.isna().sum().sort_values(ascending=False)","089ce5e7":"train_data.describe(include='all')","e28d766e":"from sklearn.model_selection import train_test_split","618c66f3":"X = train_data.drop(['id','target'], axis=1)\ny = train_data['target']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)","e54d535f":"print(X_train.shape, y_train.shape)\nprint(X_test.shape, y_test.shape)","3dd9f1ac":"from sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\n\nnum_cols = X_train.columns\n\nX_train[num_cols] = scaler.fit_transform(X_train[num_cols])\nX_test[num_cols] = scaler.transform(X_test[num_cols])","828b0e6f":"X_train.describe()","33cd1e0b":"from sklearn.linear_model import LogisticRegression\nlogreg = LogisticRegression(random_state=42, n_jobs=-1, solver='liblinear')\n\nlogreg.fit(X_train, y_train)\nlogreg.score","d13a8fe3":"# evaluating function\n\ndef evaluation_final(mod, x_test, y_test, y_test_pred):\n    \n    print('Evaluation Report on Test set:')\n    print(confusion_matrix(y_test, y_test_pred))\n    print(classification_report(y_test, y_test_pred))\n    print('Accuracy of Test data:',accuracy_score(y_test, y_test_pred))\n    \n    # plot roc_curve for test\n    plot_roc_curve(mod, x_test, y_test)\n    plt.show()\n    # roc_auc_score\n    print('roc_auc_score:', roc_auc_score(y_test, mod.predict_proba(x_test)[:,1]))\n","7f35ffa2":"y_test_pred_log = logreg.predict(X_test)\nevaluation_final(logreg, X_test, y_test, y_test_pred_log)","65cdf03b":"# from sklearn.decomposition import PCA\n# pca = PCA()\n\n# # fit X_train\n# pca.fit(X_train)","843d7a15":"# # plot variance explained ratio\n# plt.figure(figsize=[8,6])\n# plt.bar(range(1,len(pca.explained_variance_ratio_)+1), pca.explained_variance_ratio_)\n# plt.show()","616aeb46":"# var_cumu = np.cumsum(pca.explained_variance_ratio_)\n\n# # Making a scree plot\n# fig = plt.figure(figsize=[12,7])\n# plt.plot(var_cumu)\n# plt.xlabel('no of principal components')\n# plt.ylabel('explained variance - cumulative')\n# plt.show()","8763ae24":"# np.cumsum(np.round((pca.explained_variance_ratio_*100),2))","0fd7b259":"# pca_60 = PCA(n_components=60)\n\n# # fit_transform\n# X_train_60 = pca_60.fit_transform(X_train)\n# print(\"Shape of X_train_60:\", X_train_60.shape)\n\n# X_test_60 = pca_60.fit_transform(X_test)\n# print(\"Shape of X_test_60:\", X_test_60.shape)","19e38942":"from xgboost import XGBClassifier\nfrom sklearn.model_selection import GridSearchCV","695da0a7":"# # function for getting optimum hyperparameter tuning \n# def tune_hyperparameter(parameters,X_train,y_train,n_folds = 5):\n    \n#     xgb_model = XGBClassifier(random_state=42, n_jobs=-1, tree_method='gpu_hist')\n    \n#     xgb_model_cv = GridSearchCV(estimator=xgb_model,\n#                                        param_grid=parameters,\n#                                        n_jobs=-1,\n#                                        cv=n_folds,\n#                                        scoring='roc_auc',\n#                                        verbose=1,\n#                                        refit=True)\n\n\n#     xgb_model_cv.fit(X_train, y_train)\n#     scores = xgb_model_cv.cv_results_\n\n#     for key in parameters.keys():\n#         hyperparameters = key\n#         break\n\n#     # plotting accuracies for parameters\n#     plt.figure(figsize=(16,5))\n#     # plt.plot(scores[\"param_\"+hyperparameters], scores[\"mean_train_score\"], label=\"training accuracy\")\n#     plt.plot(scores[\"param_\"+hyperparameters], scores[\"mean_test_score\"], label=\"test accuracy\")\n#     plt.xlabel(hyperparameters)\n#     plt.ylabel(\"ROC_AUC_SCORE\")\n#     plt.legend()\n#     plt.show()","4fe8f9c5":"# # learning_rate\n# params = {'learning_rate': [0.05, 0.1, 0.2, 0.3, 0.5]}\n# tune_hyperparameter(parameters=params, X_train=X_train_60, y_train=y_train)","3dcd4ce3":"# learning_rate=0.2","680e60c0":"# # fit the model with n_estimators parameters\n# params = {'n_estimators': [300, 500, 800, 1000, 1300]}\n# tune_hyperparameter(parameters=params, X_train=X_train_60, y_train=y_train)","2edbdcb7":"# # fit the model with n_estimators parameters again\n# params = {'n_estimators': [50, 100, 150, 200]}\n# tune_hyperparameter(parameters=params, X_train=X_train_60, y_train=y_train)","69436304":"xgb_model = XGBClassifier(random_state=42, n_jobs=-1, tree_method='gpu_hist')\n    \nparameters = {'learning_rate': [0.025, 0.05, 0.1, 1.5, 0.2, 0.25],\n             'n_estimators': [100, 200, 400, 500],\n             'min_child_weight': [50, 100, 200, 300]}\n#              'min_sample_leaf': [10, 30, 50]}\n#              'max_depth':[20, 40, 50, 60]}\n\nxgb_model_cv = GridSearchCV(estimator=xgb_model,\n                                       param_grid=parameters,\n                                       n_jobs=-1,\n                                       cv=4,\n                                       scoring='roc_auc',\n                                       verbose=1,\n                                       refit=True)","8f9efff2":"%%time\nxgb_model_cv.fit(X_train, y_train)","001b10fb":"# plot roc_auc scores\nplt.figure(figsize=[10,6])\nplt.plot(range(1, len(xgb_model_cv.cv_results_['mean_test_score'])+1), xgb_model_cv.cv_results_['mean_test_score'])\nplt.xlabel('Fits', fontsize=12)\nplt.ylabel('ROC_AUC Score', fontsize=12)\nplt.show()","e017a275":"final_best_xgb_model = xgb_model_cv.best_estimator_\nfinal_best_xgb_model","ae4b023f":"y_actual_pred_xgb = final_best_xgb_model.predict(X_test)\nevaluation_final(final_best_xgb_model, X_test, y_test, y_actual_pred_xgb)","5113649c":"# submitting the actual test data prediction\nsub_df = pd.DataFrame({'id': test_data['id']})\nsub_df.head()","c96550c9":"# perform scaling\nnew_test_data = test_data.drop('id', axis=1)\nnew_test_num_cols = new_test_data.columns\n\nnew_test_data[new_test_num_cols] = scaler.transform(new_test_data[new_test_num_cols])\nnew_test_data.describe()","7683d0e7":"# # fit_transform PCA\n# new_test_data_60 = pca_60.fit_transform(new_test_data)\n# print(new_test_data_60.shape)","498b1730":"# predicting the probabilities adding to the sub_df dataframe\ny_actual_test_pred_final_proba = final_best_xgb_model.predict_proba(new_test_data)[:,1]\nsub_df['target'] = y_actual_test_pred_final_proba\nsub_df.head()","3edc946a":"# submission\nsub_df.to_csv('submission.csv', index=False)\nprint('Output file generated!!')","bab08fc8":"### Let's check train data","d8ed0a87":"## Import both train and test data","39efe8f4":"More roc_auc at low value of n_estimators.","df8f4312":"# Tabular Playground Series - Nov 2021","b9b236ea":"**Lets take n_estimators=100 for the final model**","e77deec1":"## Train test Split","afefd597":"**Building an Optimized model with new Hyperparameters**","b50582f6":"## PCA","e55ff08d":"**The dataset is balanced.**","200a2f99":"### Along with Hyperparameter Tuning","b3680096":"**98% of variance is explained by 60 components**","7a65af49":"- No Null values","24cc4302":"## Feature Scaling","f4259281":"### PCA on Actual Test Data","cc67bb77":"## Prediction on Actual Test Data","3acd5e45":"## Logistic Regression","172b7137":"## XGBoost","e13f04bd":"The ROC_AUC Curve is max at learning_rate=0.2"}}