{"cell_type":{"10a2ecaf":"code","86211197":"code","2f5dba8a":"code","6b59dcd1":"code","b749fe82":"code","ac7ea798":"code","eb4d3a49":"code","b292cc44":"code","f56089ee":"code","093ce89b":"code","fbfd2dc4":"code","31e26a10":"code","b48a1b11":"markdown","73dcf3cc":"markdown","80b0f02f":"markdown","13c2399e":"markdown","b3e421ea":"markdown","dfebc278":"markdown","4e6e90a7":"markdown","22a7cea1":"markdown","1bcc5e15":"markdown","2eba37f0":"markdown","d05a51ae":"markdown","bdc7e590":"markdown","ef97a4c7":"markdown","da4655e9":"markdown","b0bbcf29":"markdown"},"source":{"10a2ecaf":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","86211197":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.linear_model import LinearRegression\ndf = pd.read_csv(\"..\/input\/biomechanical-features-of-orthopedic-patients\/column_2C_weka.csv\",sep = \",\")","2f5dba8a":"df.head()","6b59dcd1":"plt.scatter(df.pelvic_incidence,df.sacral_slope) \nplt.xlabel(\"pelvic_incidence\") \nplt.ylabel(\"sacral_slope\") \nplt.show()","b749fe82":"lr = LinearRegression()\nx = df.pelvic_incidence.values.reshape(-1,1)\ny = df.sacral_slope.values.reshape(-1,1)\nlr.fit(x,y)\nplt.scatter(x,y)\narray = np.array([10,20,30,40,50,60,70,80,90,100,110,120]).reshape(-1,1)\ny_head = lr.predict(array)\nplt.plot(array,y_head)\n\n\n","ac7ea798":"y_head = lr.predict([[40]])\nprint(y_head)","eb4d3a49":"x = df.iloc[:,[0,1,2,3,4]].values\ny = df.degree_spondylolisthesis.values.reshape(-1,1)\nmlr = LinearRegression()\nmlr.fit(x,y)\nprediction = mlr.predict(np.array([[60,20,40,40,100]]))\nprint(\"prediction: \",prediction)","b292cc44":"lr = LinearRegression()\nx = df.lumbar_lordosis_angle.values.reshape(-1,1)\ny = df.sacral_slope.values.reshape(-1,1)\nlr.fit(x,y)\nplt.scatter(x,y)\ny_head = lr.predict(x)\nplt.plot(x,y_head,color = \"red\",label = \"linear\")\nplt.show()\n","f56089ee":"from sklearn.preprocessing import PolynomialFeatures","093ce89b":"plr = PolynomialFeatures(degree = 2)\nx_p = plr.fit_transform(x)\nprint(x_p)\n\n","fbfd2dc4":"lr2 = LinearRegression()\nlr2.fit(x_p,y)","31e26a10":"lr = LinearRegression()\nx = df.lumbar_lordosis_angle.values.reshape(-1,1)\ny = df.sacral_slope.values.reshape(-1,1)\nlr.fit(x,y)\nplt.scatter(x,y)\ny_head = lr.predict(x)\nplt.plot(x,y_head,color = \"red\",label = \"linear\")\ny_head2 = lr2.predict(x_p)\nplt.plot(x,y_head2,color =\"green\",label = \"poly\")\nplt.legend()\nplt.show()\n","b48a1b11":"# Content\n# 1.  Linear Regression\n# 2.  Multiple Linear Regression\n# 3.  Polynomial Linear Regression\n","73dcf3cc":"* Features of Data","80b0f02f":"* Prediction of random value (40)","13c2399e":"* Import library for polynomial linear regression","b3e421ea":"* A second degrees polynomial was defined and x^2 feature was obtained (last column)[](http:\/\/).","dfebc278":"# 3-Polynomial Linear Regression\n* Formulas: y = b0 + (b1 * x1) + (b2 * x2^2) ..... (bn * xn^n)\n* b0 represent bias value and other bn parameters pepresent coefficient of features.\n* x represent first 5 features\n* y represent degree_spondylolisthesis\n\nWe will choose the degree of the equation to be 2.","4e6e90a7":"# 2-Multiple Linear Regression\n* Formulas: y = b0 + (b1 * x1) + (b2 * x2) ..... (bn * xn)\n* b0 represent bias value and other bn parameters pepresent coefficient of features.\n* x represent first 5 features\n* y represent degree_spondylolisthesis\n\n\n\n","22a7cea1":"* Visualization","1bcc5e15":"* The value of the degree_spondylolisthesis feature was predicted using the first 5 features.","2eba37f0":"* Visualize line and fitting","d05a51ae":"* Linear regression analysis was performed for selected features and plotted.","bdc7e590":"* Plot Data","ef97a4c7":"* Fiting was done.","da4655e9":"* Import Libraries and Data\n","b0bbcf29":"# 1-LINEAR REGRESSION\n**Formulas: y = b0 + (b1 * x)**\n* b0 represent bias value and b1 parameters represent coefficient of  x feature.\n* x represent pelvic_incidence\n* y represent lumbar_lordosis_angle\n\n**Our main aim is acquire minimun MSE (Mean Square Error). MSE is defined as MSE = sum(residual^2)\/n\nResidual = y - y_head It means residual defined as difference between real y values and predicted y values in our data set.\nThan we sum all residual^2 values and divide by n that is represent summing number. \"lr.fit()\" function gives the lowest MSE for us.**"}}