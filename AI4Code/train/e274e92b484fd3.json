{"cell_type":{"90b7ed30":"code","d2af8772":"code","59705788":"code","5f378ca2":"code","b9656032":"code","738dde0c":"code","067096ce":"code","14d5e36e":"code","05383a5e":"code","ce392766":"code","039a1cac":"code","6c0a1f22":"code","33821e6a":"code","739507f9":"code","be885203":"code","66794141":"code","4bcc5a83":"code","84c68af5":"code","474521aa":"code","dde6af5f":"code","c425c305":"code","a02edfba":"code","a352d5dc":"code","fecacfdf":"code","d0723e2a":"code","41a5ea40":"markdown","1e69a499":"markdown","073f732c":"markdown","a8668229":"markdown","007bb48e":"markdown","0c3b7f32":"markdown","39d6aa60":"markdown","77834e56":"markdown","ea3ebe85":"markdown","daeb8474":"markdown","9a1f5077":"markdown","a9c3fbe2":"markdown","d5bb77a0":"markdown","c094ccaf":"markdown","599710cd":"markdown","0f3fbd34":"markdown","abbe18f3":"markdown","14eb8640":"markdown","abdcce0e":"markdown"},"source":{"90b7ed30":"import numpy as np \nimport pandas as pd \nimport nltk\nimport string as s\nfrom nltk.corpus import stopwords\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nimport re\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n","d2af8772":"train_data=pd.read_csv(\"\/kaggle\/input\/ag-news-classification-dataset\/train.csv\",header=0,names=['classid','title','desc'])\ntest_data=pd.read_csv(\"\/kaggle\/input\/ag-news-classification-dataset\/test.csv\",header=0,names=['classid','title','desc'])","59705788":"train_data.head()","5f378ca2":"test_data.head()","b9656032":"train_data.shape","738dde0c":"test_data.shape","067096ce":"sns.countplot(train_data.classid);","14d5e36e":"sns.countplot(test_data.classid);","05383a5e":"train_x=train_data.desc\ntest_x=test_data.desc\ntrain_y=train_data.classid\ntest_y=test_data.classid","ce392766":"def remove_html(text):\n    html_pattern = re.compile('<.*?>')\n    return html_pattern.sub(r'', text)\n\ntrain_x=train_x.apply(remove_html)\ntest_x=test_x.apply(remove_html)","039a1cac":"def remove_urls(text):\n    url_pattern = re.compile(r'https?:\/\/\\S+|www\\.\\S+')\n    return url_pattern.sub(r'', text)\n\ntrain_x=train_x.apply(remove_urls)\ntest_x=test_x.apply(remove_urls)","6c0a1f22":"def word_tokenize(txt):\n    tokens = re.findall(\"[\\w']+\", txt)\n    return tokens\ntrain_x=train_x.apply(word_tokenize)\ntest_x=test_x.apply(word_tokenize)","33821e6a":"def remove_stopwords(lst):\n    stop=stopwords.words('english')\n    new_lst=[]\n    for i in lst:\n        if i.lower() not in stop:\n            new_lst.append(i)\n    return new_lst\n\ntrain_x=train_x.apply(remove_stopwords)\ntest_x=test_x.apply(remove_stopwords) ","739507f9":"def remove_punctuations(lst):\n    new_lst=[]\n    for i in lst:\n        for  j in  s.punctuation:\n            i=i.replace(j,'')\n        new_lst.append(i)\n    return new_lst\ntrain_x=train_x.apply(remove_punctuations) \ntest_x=test_x.apply(remove_punctuations)","be885203":"def remove_numbers(lst):\n    nodig_lst=[]\n    new_lst=[]\n\n    for i in  lst:\n        for j in  s.digits:\n            i=i.replace(j,'')\n        nodig_lst.append(i)\n    for i in  nodig_lst:\n        if  i!='':\n            new_lst.append(i)\n    return new_lst\ntrain_x=train_x.apply(remove_numbers)\ntest_x=test_x.apply(remove_numbers)","66794141":"import nltk\n\ndef stemming(text):\n    porter_stemmer = nltk.PorterStemmer()\n    roots = [porter_stemmer.stem(each) for each in text]\n    return (roots)\n\ntrain_x=train_x.apply(stemming)\ntest_x=test_x.apply(stemming)","4bcc5a83":"lemmatizer=nltk.stem.WordNetLemmatizer()\ndef lemmatzation(lst):\n    new_lst=[]\n    for i in lst:\n        i=lemmatizer.lemmatize(i)\n        new_lst.append(i)\n    return new_lst\ntrain_x=train_x.apply(lemmatzation)\ntest_x=test_x.apply(lemmatzation)","84c68af5":"def remove_extrawords(lst):\n    stop=['href','lt','gt','ii','iii','ie','quot','com']\n    new_lst=[]\n    for i in lst:\n        if i not in stop:\n            new_lst.append(i)\n    return new_lst\n\ntrain_x=train_x.apply(remove_extrawords)\ntest_x=test_x.apply(remove_extrawords) ","474521aa":"train_x=train_x.apply(lambda x: ''.join(i+' ' for i in x))\ntest_x=test_x.apply(lambda x: ''.join(i+' '  for i in x))","dde6af5f":"from sklearn.feature_extraction.text  import TfidfVectorizer\ntfidf=TfidfVectorizer(min_df=8,ngram_range=(1,3))\ntrain_1=tfidf.fit_transform(train_x)\ntest_1=tfidf.transform(test_x)\nprint(\"No. of features extracted\")\nprint(len(tfidf.get_feature_names()))\nprint(tfidf.get_feature_names()[:100])\n\ntrain_arr=train_1.toarray()\ntest_arr=test_1.toarray()","c425c305":"pd.DataFrame(train_arr[:100], columns=tfidf.get_feature_names())","a02edfba":"%%time\nfrom sklearn.naive_bayes  import MultinomialNB \nNB_MN=MultinomialNB(alpha=0.52)\nNB_MN.fit(train_arr,train_y)\npred=NB_MN.predict(test_arr)\n\n","a352d5dc":"print(\"first 20 actual labels\")\nprint(test_y.tolist()[:20])\nprint(\"first 20 predicted labels\")\nprint(pred.tolist()[:20])","fecacfdf":"from sklearn.metrics  import f1_score,accuracy_score\nprint(\"F1 score of the model\")\nprint(f1_score(test_y,pred,average='micro'))\nprint(\"Accuracy of the model\")\nprint(accuracy_score(test_y,pred))\nprint(\"Accuracy of the model in percentage\")\nprint(round(accuracy_score(test_y,pred)*100,3),\"%\")","d0723e2a":"from sklearn.metrics import  confusion_matrix\nsns.set(font_scale=1.5)\ncof=confusion_matrix(test_y, pred)\ncof=pd.DataFrame(cof, index=[i for i in range(1,5)], columns=[i for i in range(1,5)])\nplt.figure(figsize=(8,8))\n\nsns.heatmap(cof, cmap=\"PuRd\",linewidths=1, annot=True,square=True,cbar=False,fmt='d',xticklabels=['World','Sports','Business','Science'],yticklabels=['World','Sports','Business','Science'])\nplt.xlabel(\"Predicted Class\");\nplt.ylabel(\"Actual Class\");\n\nplt.title(\"Confusion Matrix for News Article Classification\");","41a5ea40":"## Evaluation of Results","1e69a499":"## Analyzing Data","073f732c":"## Removal of Punctuation Symbols","a8668229":"## Lemmatization of Data","007bb48e":"## Stemming of Dataset","0c3b7f32":"**Countplot of Testdata**","39d6aa60":"## Removal of Numbers(digits)","77834e56":"### Importing libraries","ea3ebe85":"# Preprocessing of Data\n\nThe data is preprocessed, in NLP it is also known as text normalization. Some of the most common methods of text normalization are \n* Tokenization\n* Lemmatization\n* Stemming\n","daeb8474":"## Removal of URLs","9a1f5077":"## Feature Extraction\n \n Features are extracted from the dataset and TF-IDF(Term Frequency - Inverse Document Frequency) is used for this purpose.","a9c3fbe2":"## Removal of Stopwords","d5bb77a0":"## Removal of HTML tags","c094ccaf":"## Splitting Data into Input and Label ","599710cd":"# Training of Model\n\n### Model 1- Multinomial Naive Bayes","0f3fbd34":"## Tokenization of Data","abbe18f3":"**Countplot of Train data**","14eb8640":"# Classification of News Articles \n\nIt is a notebook for multiclass classification of News articles which are having classes numbered 1 to 4, where 1 is \"World News\", 2 is \"Sports News\", 3 is \"Business News\" and 4 is \"Science-Technology News\".","abdcce0e":"Seeing the countplot of the training data and testing data we can say that the datasets are balanced"}}