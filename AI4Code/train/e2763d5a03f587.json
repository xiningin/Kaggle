{"cell_type":{"f8e3b6f0":"code","bc73a3a2":"code","23436155":"code","486fad22":"code","76bf5990":"code","b6921a5e":"code","e925f59c":"code","c0000acf":"code","27a3c78a":"code","da9d4b40":"code","bedf4852":"code","df1cfff5":"code","66aa66cd":"code","28bd7500":"code","4a947faf":"code","a8e08b9b":"code","f07de238":"markdown","97b307ff":"markdown","7a628d6c":"markdown","bc0aa39e":"markdown","f93ba702":"markdown","06289a11":"markdown","603b8c5d":"markdown","0cd516f7":"markdown","7b5f3ad7":"markdown","2e5d19ae":"markdown","e9c5dcf3":"markdown","fad83504":"markdown"},"source":{"f8e3b6f0":"PREPROCESS_AND_TRAIN = True","bc73a3a2":"import numpy as np \nimport pandas as pd \nimport os\nfrom glob import glob\nfrom tqdm.auto import tqdm\nimport json\nimport re\nfrom unidecode import unidecode\nfrom collections import Counter\nimport pickle\nimport gensim \nimport logging\nimport sys\n\n# We create a logger handler so that gensim outputs its logging to stdout, which will then appear in the notebook cells.\n# This is nice to have to be able to see the progress of the training process\nlogger = logging.getLogger()\n\nif logger.hasHandlers():\n        logger.handlers.clear()\nconsole_handler = logging.StreamHandler(sys.stdout)\nconsole_handler.setLevel(logging.INFO)\nconsole_handler.setFormatter(logging.Formatter('%(asctime)s : %(levelname)s : %(message)s'))\nlogger.addHandler(console_handler)\nlogger.info('hi')","23436155":"# Regex used for cleaning and tokenisation\nspace = re.compile('\\s+')\nreference = re.compile(r'[\\(\\[]\\d+(, ?\\d+)?[\\)\\]]')\nlinks = re.compile(r'https?:\/\/\\S+')\nsentence  = re.compile(r'(\\S{3,})[.!?]\\s')\nhyphen_1 = re.compile(r'([A-Z0-9])\\-(.)')\nhyphen_2 = re.compile(r'(.)\\-([A-Z0-9])')\n\nlicense_phrases = [r'\\(which was not peer-reviewed\\)',\n                    'The copyright holder for this preprint',\n                    'It is made available under a is the author\/funder',\n                    'who has granted medRxiv a license to display the preprint in perpetuity',\n                    'medRxiv preprint',\n                    r'(CC(?:-BY)?(?:-NC)?(?:-ND)? \\d\\.\\d (?:International license)?)',\n                    'Submitted manuscript.', \n                   'Not peer reviewed.']\n\nlicense_phrases = re.compile('|'.join(license_phrases), re.I)\nPAT_ALPHABETIC = re.compile(r'(((?![\\d])\\w)+)', re.UNICODE) # from gensim - removes digits - keeps only other alpha numeric and tokenises on everything else\nPAT_ALL = re.compile(r'((\\d+(,\\d+)*(\\.\\d+)?)+|([\\w_])+)', re.UNICODE) # Includes digits - tokenises on space and non alphanumeric characters (except _)\n\ndef clean_text(text):\n    text = text.replace('\\t', ' ').replace('\\n', ' ')\n    text = sentence.sub(r'\\1 _SENT_ ', text)\n    text = text.replace('doi:', ' http:\/\/a.website.com\/')\n    text = unidecode(text) # converts any non-unicode characters to a unicode equivalent\n    text = hyphen_1.sub(r'\\1\\2', text)\n    text = hyphen_2.sub(r'\\1\\2', text)\n    text = links.sub(' ', text)\n    text = license_phrases.sub(' ', text)\n    text = reference.sub(' ', text)\n    text = space.sub(' ', text)\n\n    return text.strip()\n\ndef fetch_tokens(text, reg_pattern):\n    for match in reg_pattern.finditer(text):\n        yield match.group()\n\ndef tokenise(text, remove_stop=False, lowercase=False, include_digits=True):\n    text = clean_text(text)\n    \n    if lowercase:\n        text = text.lower()\n    \n    if include_digits:\n        tokens = list(fetch_tokens(text, reg_pattern=PAT_ALL))\n    else:\n        tokens = list(fetch_tokens(text, reg_pattern=PAT_ALPHABETIC))\n            \n    if remove_stop:\n        return ' '.join([x for x in tokens if x.lower() not in stopWords])\n    else:\n        return ' '.join(tokens)","486fad22":"test_text = \"\"\"A few test sentences for the tokeniser and cleaner.\nWith a web link http:\/\/something.co.uk\/ and including a large number 100,000.50 and small ones 5.5.\nBut not including footnote references (10, 34) and [23].\nRemove hyphens from Covid-19 and MERS-SARS to keep these as one token;\nBut add spaces for hyphenated-lowercase-words to keep the individual words as tokens.\"\"\"","76bf5990":"print(tokenise(test_text))","b6921a5e":"LICENSE_TYPES = ['comm_use_subset',\n                 'noncomm_use_subset',\n                 'pmc_custom_license',\n                 'biorxiv_medrxiv']\n\n# DATA_PATH = '\/kaggle\/input\/CORD-19-research-challenge\/2020-03-13'\nDATA_PATH = '\/kaggle\/input\/CORD-19-research-challenge'\nPRE_PROCESSED_PATH = '\/kaggle\/input\/paragraph-search-using-word2vec'","e925f59c":"def iter_info():\n    \"\"\"\n    Custom iterator to extract paragraphs from body text and reference text, alongside some of the document's meta data \n    \"\"\"\n    for licence in LICENSE_TYPES:\n        for root, dirs, files in os.walk(os.path.join(DATA_PATH, licence)):\n            for name in files: \n                fname = os.path.join(root, name)\n                document = json.load(open(fname))\n            \n                refs = {}\n                for para_num, para in enumerate(document['body_text']):\n                    paragraph = {'title': document['metadata']['title'],\n                                'id': document['paper_id'],\n                                'contains_refs': False,\n                                'section': para['section'],\n                                'para_num': para_num,\n                                'para_text': para['text'],\n                                'licence': licence}\n\n                    # check for references within this paragraph, but only add text once per paragraph\n                    for ref in para['ref_spans']:\n                        if ref['ref_id'] in document['ref_entries'].keys() and ref['ref_id'] not in refs.keys():\n                            refs[ref['ref_id']] = True\n                            paragraph['contains_refs'] = True \n                            paragraph['para_text'] += ' _STARTREF_ ' + document['ref_entries'][ref['ref_id']]['text'] + ' _ENDREF_ '\n                    yield paragraph\n\n                # Add any references text that has been missed\n                for ref_id, ref_element in document['ref_entries'].items():\n                    if ref_id not in refs.keys():\n                        paragraph = {'title': document['metadata']['title'],\n                                    'id': document['paper_id'],\n                                    'contains_refs': False,\n                                    'para_num': ref_id,\n                                    'para_text': ref_element['text'],\n                                    'section': 'References',\n                                    'licence': licence}\n                        yield paragraph","c0000acf":"if PREPROCESS_AND_TRAIN:\n    all_data = []\n    texts = iter_info()\n    for cc, t in tqdm(enumerate(texts)):\n        t['tokenised'] = tokenise(t['para_text'])\n        all_data.append(t)\n#         if cc == 50000:\n#             break\n    # Convert to dataframe and save\n    all_data = pd.DataFrame(all_data)\n    all_data.to_pickle('CORD_19_all_papers.pkl')\nelse:\n    all_data = pd.read_pickle(os.path.join(PRE_PROCESSED_PATH, 'CORD_19_all_papers.pkl'))","27a3c78a":"all_data.head()","da9d4b40":"class Paragraphs(object):\n    def __init__(self, corpus_df, min_words_in_sentence=10):\n        self.corpus_df = corpus_df\n        self.min_words = min_words_in_sentence\n\n    def __iter__(self): \n        for tot_paras, para in enumerate(self.corpus_df.tokenised, 1):\n            if (tot_paras % 100000 == 0):\n                logger.info(\"Read {0} paras\".format(tot_paras))\n            word_list = para.replace('\\n', ' ').split(' ')\n            if len(word_list) >= self.min_words:\n                yield word_list","bedf4852":"if PREPROCESS_AND_TRAIN:\n    documents = Paragraphs(all_data, min_words_in_sentence=10)\n\n    vocab = Counter()\n    for line in tqdm(documents):\n        vocab.update(line)\n\n    # Save these word frequencies\n    vocab = dict(vocab)\n    pickle.dump(vocab, open('covid_vocab_frequencies.pkl', 'wb'))\nelse:\n    vocab = pickle.load(open(os.path.join(PRE_PROCESSED_PATH, 'covid_vocab_frequencies.pkl'), 'rb'))","df1cfff5":"vocab['COVID19']","66aa66cd":"print(f'{len(vocab)} unique tokens in total.')\nprint(f'Most common words are: {Counter(vocab).most_common(10)}')\nprint(f'COVID19 mentioned {vocab[\"COVID19\"]} times.')","28bd7500":"EMBEDDING_DIMS = 100","4a947faf":"if PREPROCESS_AND_TRAIN:\n    # build vocabulary within Gensim\n    model = gensim.models.Word2Vec(\n        size=EMBEDDING_DIMS,\n        window=6,\n        min_count=5)\n\n    model.build_vocab(documents)\n    print(f'Number of paragraphs in corpus: {model.corpus_count}')","a8e08b9b":"if PREPROCESS_AND_TRAIN:\n    model.train(sentences=documents, \n            epochs=5, \n            total_examples=model.corpus_count, \n            total_words=model.corpus_total_words)\n    model.save('covid_w2v')    \nelse:\n    model = gensim.models.Word2Vec.load(os.path.join(PRE_PROCESSED_PATH, 'covid_w2v'))","f07de238":"# Building a paragraph search tool using word2vec\nThis notebook builds a simple, general purpose, search tool based on word embeddings. We start by training word embeddings using word2vec, then calculate weighted average vectors for each paragraph in each of the papers, and finally use nearest neighbours to find related content. \n\nTraining our own embeddings has a significant advantage in that the corpus we are dealing with has highly specific vocabulary containing many words that are unlikely to appear in standard pretrained embeddings. The disadvantage is that we don't have very much text with which to train the embeddings, and the vocabulary is large.\n\nA few points about the general approach and assumptions I'm making:\n- I do some light cleaning of the text, e.g. removing url and doi links, and references numbers; but including other numbers. Additional cleaning rules might be justified, but I'm not familiar enough with the raw data to know what these should be.\n- I don't bother removing stop words. This has a knock-on effect on word2vec training - it might be worth having a slightly larger 'window' to get a more meaningful context. \n- I'm maintaining casing of the text. My justification is that these are scientific documents where case may well be important. (I'm not so sure about this assumption...).\n- The vector for a chunk of text (sentence, paragraph, document, etc) is calculated in a manner analogous to TF-IDF - we take the average of the vectors for each word in the chunk, but weight each vector by the inverse of the log of the frequency of that word in the corpus (Note that this effectively removes stopwords from the weighted average). ","97b307ff":"These are some helper functions for cleaning and tokenising the texts. Our aim is to clean up the text a bit, but without removing what could be important details in scientific papers - e.g. numbers. However, the text does include lots of footnote reference numbers, so we do try to remove those. ","7a628d6c":"First we create an iterator which will yield lists of tokens to pass to the word2vec training.","bc0aa39e":"Let's look at the most common words in the corpus, and the size of the vocabulary.","f93ba702":"Running the entire preprocessing and word2vec training takes about an hour. If you just want to test the interactive search tool, set `PREPROCESS_AND_TRAIN = False` below. The script will then load the saved versions of the embeddings and paragraph vectors.","06289a11":"### Create and train a gensim word2vec model","603b8c5d":"Pretty much what we expected given that we haven't removed stopwords.","0cd516f7":"The next bit of code trains a word2vec model. There are lots of hyperparamter options. The key ones are:\n- _size_: the number of dimensions that the embedding will have;\n- _window_: the model looks this many tokens either side of a central token. These window tokens provide the context with which to predict the central word (or vica versa under the skipgram approach);\n- *min_count*: the minimum number of times a token must appear in the corpus in order to have an embedding;\n- _epoch_: the number of times the model passes over the corpus when training.","7b5f3ad7":"Before training we will count how many times each word occurs in the corpus. We need these corpus frequencies later when calculating weighted average document, paragraph or sentence vectors.","2e5d19ae":"## Train word embeddings ","e9c5dcf3":"These look OK, so now we will process all the JSON files to extract the body text of the papers and tokenise it. We will save everything into a pandas DataFrame for convenience. \n\nIn addition to the body text, we will extract reference text and add it to the relevant paragraph. Where it doesn't appear to be linked to any paragraph (presumably a mistake in the extraction from the original document), we simple add the reference text as an extra paragraph. ","fad83504":"Let's test the cleaning and tokenisation on a sample bit of text. \n\nNote that we try to add a special token '\\_SENT\\_' between sentences to give us the option to split the paragraphs into sentences later on."}}