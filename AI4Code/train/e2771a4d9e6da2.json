{"cell_type":{"5cc13a4b":"code","1a220f18":"code","d0d13f6d":"code","811e9c32":"code","b9ea5644":"code","ab255467":"code","3b73bbe4":"code","bc6de05b":"code","1e546771":"code","f725c87c":"code","b28bf879":"markdown","7753c09b":"markdown","3b710400":"markdown","9a8fffa6":"markdown","414f6150":"markdown","e3600251":"markdown","bb7f8cdf":"markdown","d42cbbd4":"markdown","fa4da2f6":"markdown","12a0c787":"markdown"},"source":{"5cc13a4b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\n# Installing dependencies\n#data preprocessing\nimport pandas as pd\n#math operations\nimport numpy as np\n#data visualization\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","1a220f18":"# Extract training data into a dataframe for further manipulation\ntrain = pd.read_csv('..\/input\/train.csv', nrows=6000000, dtype={'acoustic_data': np.int16, 'time_to_failure': np.float64})\n\n# Print first 10 entries\ntrain.head(10)","d0d13f6d":"#visualize 1% of samples data, first 100 datapoints\ntrain_ad_sample_df = train['acoustic_data'].values[::100]\ntrain_ttf_sample_df = train['time_to_failure'].values[::100]\n\n#function for plotting based on both features\ndef plot_acc_ttf_data(train_ad_sample_df, train_ttf_sample_df, title=\"Acoustic data and time to failure: 1% sampled data\"):\n    fig, ax1 = plt.subplots(figsize=(12, 8))\n    plt.title(title)\n    plt.plot(train_ad_sample_df, color='r')\n    ax1.set_ylabel('acoustic data', color='r')\n    plt.legend(['acoustic data'], loc=(0.01, 0.95))\n    ax2 = ax1.twinx()\n    plt.plot(train_ttf_sample_df, color='b')\n    ax2.set_ylabel('time to failure', color='b')\n    plt.legend(['time to failure'], loc=(0.01, 0.9))\n    plt.grid(True)\n\nplot_acc_ttf_data(train_ad_sample_df, train_ttf_sample_df)\ndel train_ad_sample_df\ndel train_ttf_sample_df","811e9c32":"# let's create a function to generate some statistical features based on the training data\ndef gen_features(X):\n    strain = []\n    strain.append(X.mean())\n    strain.append(X.std())\n    strain.append(X.min())\n    strain.append(X.max())\n    strain.append(X.kurtosis())\n    strain.append(X.skew())\n    strain.append(np.quantile(X,0.01))\n    strain.append(np.quantile(X,0.05))\n    strain.append(np.quantile(X,0.95))\n    strain.append(np.quantile(X,0.99))\n    strain.append(np.abs(X).max())\n    strain.append(np.abs(X).mean())\n    strain.append(np.abs(X).std())\n    return pd.Series(strain)","b9ea5644":"train = pd.read_csv('..\/input\/train.csv', iterator=True, chunksize=150_000, dtype={'acoustic_data': np.int16, 'time_to_failure': np.float64})\n\nX_train = pd.DataFrame()\ny_train = pd.Series()\nfor df in train:\n    ch = gen_features(df['acoustic_data'])\n    X_train = X_train.append(ch, ignore_index=True)\n    y_train = y_train.append(pd.Series(df['time_to_failure'].values[-1]))\n    \n\n# Let's see what we have done\nX_train.describe()","ab255467":"# LSTM Model\nfrom sklearn.preprocessing import StandardScaler\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import LSTM\n\nscaler = StandardScaler()\nscaler.fit(X_train)\nX_train_scaled = scaler.transform(X_train)\n\n# Reshape to correct dimensions\nX_train_scaled = X_train_scaled.reshape(X_train_scaled.shape[0],X_train_scaled.shape[1],1)\n\n# Model\nmodel = Sequential()\nmodel.add(LSTM(50,  input_shape=(X_train_scaled.shape[1], X_train_scaled.shape[2])))\nmodel.add(Dense(1))\nmodel.compile(loss='mae', optimizer='adam')\n\n# Fit network\nhistory = model.fit(X_train_scaled, \n                    y_train, \n                    epochs=15,\n                    batch_size=64,\n                    verbose=0)\n\nmodel.summary()\n","3b73bbe4":"# Evaluate model\nfrom sklearn.metrics import mean_absolute_error\n    \ny_pred = model.predict(X_train_scaled)\nmae = mean_absolute_error(y_train, y_pred)\nprint('%.5f' % mae)\n\n","bc6de05b":"# submission format\nsubmission = pd.read_csv('..\/input\/sample_submission.csv', index_col='seg_id')\nX_test = pd.DataFrame()\n\n# prepare test data\nfor seg_id in submission.index:\n    seg = pd.read_csv('..\/input\/test\/' + seg_id + '.csv')\n    ch = gen_features(seg['acoustic_data'])\n    X_test = X_test.append(ch, ignore_index=True)\n\nX_test = scaler.transform(X_test)","1e546771":"# model of choice here\nX_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)\ny_hat = model.predict(X_test)","f725c87c":"# write submission file\nsubmission['time_to_failure'] = y_hat\nsubmission.to_csv('submission.csv')","b28bf879":"## Finalise Submission\nSiraj doesn't provide the code for this. I had to glean this from other kernels. So far, we have only seen training data. We haven't dealt with test data, so that needs to be done as well.","7753c09b":"## Import the Dataset\nSince we aren't using the Google Colab framework, we can import the dataset directly. ","3b710400":"## Future Directions\n1. Recurrent Networks. See my kernel, [RNN Starter Kernel](https:\/\/www.kaggle.com\/devilears\/rnn-starter-kernel-with-notebook), for starter code.\n1. Genetic Algorithms.\n1. [Ordinary Differential Equation Network](https:\/\/towardsdatascience.com\/paper-summary-neural-ordinary-differential-equations-37c4e52df128). See also [Neural Ordinary Differential Equations](https:\/\/paperswithcode.com\/paper\/neural-ordinary-differential-equations).","9a8fffa6":"## Evaluate Model\nDo the hyper parameter tuning or other means of evaluating the model here.","414f6150":"For this, note how the chunks are used to iterate through this fairly large dataset. Each chunk is processed, and the results are appended to the *X_train* and *y_train* dataframes. This is somewhat different from some of the other kernels that I used, where all the chunks weren't iterated through and the training data was just limited.","e3600251":"### Generate predictions for the submission.\n\nThe prediction here is from the LSTM model. You would have to reinitialise the SVR with the hyper parameter tuning values and get your prediction from that if you want to submit SVR model data.","bb7f8cdf":"## Feature Engineering\nAdding statistical features. Siraj adds a few of the most common ones, plus he has a handy description of most of these in his [notebook on github](https:\/\/github.com\/llSourcell\/Kaggle_Earthquake_challenge\/blob\/master\/Earthquake_Challenge.ipynb). \n\nThis kernel, called [Earthquakes FE More Features and Samples](https:\/\/www.kaggle.com\/artgor\/earthquakes-fe-more-features-and-samples), contains more features. There are also links to more ideas if you want to engineer additional features. For this kernel, I'm sticking to Siraj's basics.","d42cbbd4":"## Implement Model\nIn this case, I am implementing an [LSTM Model](https:\/\/machinelearningmastery.com\/multivariate-time-series-forecasting-lstms-keras\/). \n\nSiraj implemented an SVR and a CatBoost model. Since I want to use this as a template, I didn't replicate the SVR or CatBoost model here, but decided to implement a different one. If you are interested in the others, have a look at Siraj's work. The SVR seems to get a better score than the CatBoost model.\n\nI ran into some trouble with the shapes of the training and testing data. The LSTM model itself is not that important to me here, since I only want to use this kernel as a template for other models.\n\nI've removed the parts where I split into training and test sets, since this is time series data and it's not really necessary. \n\n","fa4da2f6":"# A kernel modeled after Siraj's recent video\nSiraj probably needs no introduction. He recently did a [video on this earthquake competition](https:\/\/www.youtube.com\/watch?v=TffGdSsWKlA), which is informative to data science noobs like me. I like the step by step approach. Watch that video if you want to see how to construct a kernel from scratch, but do note that it runs on Google's Colab framework.\n\nFollowing Siraj's step by step approach, I'm following my own steps in constructing this kernel in a Kaggle environment. I added the submission steps, since I want to use this kernel as a template with other models instead of the LSTM. \n\n## My Steps\n1. Install the Dependencies\n1. Import the Dataset\n1. Exploratory Data Analysis\n1. Feature Engineering\n1. Implement Model\n1. Evaluate Model\n1. Finalise Submission\n1. Future Directions\n\n## Install the Dependencies\nFortunately, the kaggle dependencies are already installed. I opted to install model specific dependencies in that code block, since the code is easier to modify in terms of swopping out models. It's also easier to see what you have imported and what is still missing in this way.","12a0c787":"## Exploratory Data Analysis\nFor this, we just visualise the data and see what we can deduce from the graphs."}}