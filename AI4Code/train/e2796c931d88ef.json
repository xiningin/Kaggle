{"cell_type":{"c642f8c4":"code","1c8d7693":"code","eda11c4a":"code","b14e860f":"code","416d1b41":"code","019eb8b7":"code","a5a67b49":"code","9a3b50ef":"code","ca0b7dd7":"code","753a3e0c":"code","d7c7f1d8":"code","ccbdc80f":"code","538f7b4c":"code","c8e0a5af":"code","1466f858":"code","ae247b49":"code","d4f03dd9":"code","a721400a":"code","eef343a7":"code","1107e26a":"code","7258f7a2":"code","444a1b60":"code","11b936db":"code","f7f1f035":"code","36f41f54":"code","efbc689f":"code","29fbb20b":"code","2d04736c":"code","5a91ee1c":"code","30ca4cc3":"code","28821733":"code","0101dba1":"code","7c9e64ab":"code","d00edbcc":"code","19352850":"code","a5ed565b":"code","57e8fb4e":"code","cda3d7df":"code","7c1b9e0f":"code","fece093c":"code","847edb54":"markdown","486ed82a":"markdown","b49a42ec":"markdown","a4fbe817":"markdown","623aa0fc":"markdown","110caecf":"markdown","8b44e906":"markdown","82038f4a":"markdown","3428881f":"markdown","f237a99d":"markdown","22c88297":"markdown","2afa112b":"markdown","60b2d229":"markdown","7525caa6":"markdown","8d6442cd":"markdown","4e605de7":"markdown","309feb55":"markdown","fca2a376":"markdown","f91d4071":"markdown","bcfffdde":"markdown","b57f9b97":"markdown","771994fb":"markdown","2aea1703":"markdown","af98eab0":"markdown","a7e0bd46":"markdown","eb4a69b6":"markdown","a1450e9f":"markdown"},"source":{"c642f8c4":"# Work with Data - the main Python libraries\nimport numpy as np\nimport pandas as pd\nimport pandas_profiling as pp\n\n# Visualization\nimport matplotlib.pyplot as plt\n\n# Preprocessing\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.model_selection import train_test_split, KFold, ShuffleSplit, GridSearchCV\n\n# Modeling\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nimport xgboost as xgb\nfrom xgboost.sklearn import XGBRegressor\n\n# Metrics\nfrom sklearn.metrics import r2_score\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","1c8d7693":"# Download training data\ntrain = pd.read_csv('\/kaggle\/input\/ammonium-prediction-in-river-water\/train.csv')","eda11c4a":"# Display the first 5 rows of the training dataframe.\ntrain.head()","b14e860f":"# Information for training data\ntrain.info()","416d1b41":"# Download test data\ntest = pd.read_csv('..\/input\/ammonium-prediction-in-river-water\/test.csv')","019eb8b7":"# Display the 7 last rows of the training dataframe\ntest.tail()","a5a67b49":"test.info()","9a3b50ef":"# Select the stations with the most data in training dataset\ntrain = train.drop(['Id','3','4','5','6','7'], axis = 1)\ntrain = train.dropna().reset_index(drop=True)\ntrain.info()","ca0b7dd7":"# Display the statistics for training data\ntrain.describe()","753a3e0c":"# EDA with Pandas Profiling\npp.ProfileReport(train)","d7c7f1d8":"# Selecting a target featute and removing it from training dataset\ntarget = train.pop('target')","ccbdc80f":"# Select the stations with the most data in test dataset\ntest = test.drop(['Id','3','4','5','6','7'], axis = 1)\ntest = test.dropna().reset_index(drop=True)","538f7b4c":"# EDA with Pandas Profiling\npp.ProfileReport(test)","c8e0a5af":"# Display basic information about the test data\ntest.info()","1466f858":"# Standartization data\nscaler = StandardScaler()\ntrain = pd.DataFrame(scaler.fit_transform(train), columns = train.columns)\n\n# Display training data\ntrain","ae247b49":"# Display the statistics for training data\ntrain.describe()","d4f03dd9":"# Standartization data\ntest = pd.DataFrame(scaler.transform(test), columns = test.columns)\n# Display test\ndisplay(test)","a721400a":"# Display the statistics for training data\ntest.describe()","eef343a7":"trainAdd = pd.read_csv('..\/input\/ammonium-prediction-in-river-water\/test.csv')","1107e26a":"# Standartization data\nrScaler = RobustScaler()\n\ntrainAdd = pd.DataFrame(rScaler.fit_transform(trainAdd), columns = trainAdd.columns)\n# Display training data\ntrainAdd","7258f7a2":"# Training data splitting to new training (part of the all training) and validation data\ntrain_all = train.copy()\ntarget_all = target.copy()\ntrain, valid, target_train, target_valid = train_test_split(train_all, target_all, test_size=0.2, random_state=0)","444a1b60":"# Display information about new training data\ntrain.info()","11b936db":"# Display information about validation data\nvalid.info()","f7f1f035":"# Cross-validation of training data with shuffle\ncv_train =  KFold(n_splits=5, shuffle=False, random_state=0)","36f41f54":"# Creation the dataframe with the resulting score of all models\nresult = pd.DataFrame({'model' : ['Decision Tree Regressor', 'Random Forest Regressor', 'XGBoost Regressor'], \n                       'train_score': 0, 'valid_score': 0, 'y_train':  [[], [], []], 'y_val':  [[], [], []], 'y_test':  [[], [], []]})\nresult","efbc689f":"#\u0414\u043b\u044f \u043f\u043e\u0434\u0430\u043b\u044c\u0448\u043e\u0433\u043e \u0432\u0438\u043a\u043e\u0440\u0438\u0441\u0442\u0430\u043d\u043d\u044f \u0442\u0430 \u043e\u043f\u0442\u0438\u043c\u0456\u0437\u0430\u0446\u0456\u0457 \u043a\u043e\u0434\u0443 \u0441\u0442\u0432\u043e\u0440\u0435\u043d\u043e \u0444\u0443\u043d\u043a\u0446\u0456\u044e, \u0449\u043e \u0443\u043d\u0456\u0432\u0435\u0440\u0441\u0430\u043b\u0456\u0437\u0443\u0454 \u0437\u0430\u043f\u0443\u0441\u043a \u043c\u043e\u0434\u0435\u043b\u0435\u0439\n#\u0424\u0443\u043d\u043a\u0446\u0456\u044f \u043d\u0430 \u0432\u0438\u0445\u043e\u0434\u0456 \u0434\u0430\u0454 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u0438 \u0440\u043e\u0437\u0440\u0430\u0445\u0443\u043d\u043a\u0443 \u0437\u0430 \u043f\u0435\u0432\u043d\u043e\u044e \u043c\u043e\u0434\u0435\u043b\u043b\u044e \u0434\u043b\u044f \u0442\u0440\u0435\u0439\u043d\u043e\u0432\u043e\u0457, \u0432\u0430\u043b\u0456\u0434\u0430\u0446\u0456\u0439\u043d\u043e\u0457 \u0442\u0430 \u0442\u0435\u0441\u0442\u043e\u0432\u043e\u0457 \u0432\u0438\u0431\u0456\u0440\u043e\u043a. \n\ndef get_model(train, valid, target_train, target_valid, model_name, name, param_grid, cv_train, result):\n    model = model_name\n    grid = GridSearchCV(model,\n                        param_grid,\n                        cv = cv_train,\n                        verbose=False)\n    grid.fit(train, target_train)\n    \n    # Prediction for training data\n    y_train = grid.predict(train)\n    print(grid.best_params_)\n    \n    # Accuracy of model\n    r2_score_acc = round(r2_score(target_train, y_train)*100,1)\n    print(f'Accuracy of {name} model training is {r2_score_acc}')\n    \n    result.loc[result['model'] == name, 'train_score'] = r2_score_acc\n    result.at[result.loc[result['model'] == name].index[0], 'y_train'] = y_train\n    \n    # Print rounded r2_score_acc to 2 decimal values after the text\n    y_val = grid.predict(valid)\n    r2_score_acc_valid = round(r2_score(target_valid, y_val)*100,1)\n    result.loc[result['model'] == name, 'valid_score'] = r2_score_acc_valid\n    result.at[result.loc[result['model'] == name].index[0], 'y_val'] = y_val\n    \n    print(f'Accuracy of {name} model prediction for valid dataset is {r2_score_acc_valid}')\n    \n    result.at[result.loc[result['model'] == name].index[0], 'y_test'] = grid.predict(test)\n    return result","29fbb20b":"decision_tree = DecisionTreeRegressor()\nparam_grid = {'min_samples_leaf': [i for i in range(5,10)], 'max_depth': [i for i in range(3,12)]}\nresult = get_model(train, valid, target_train, target_valid, decision_tree ,'Decision Tree Regressor', param_grid, cv_train, result)","2d04736c":"rf = RandomForestRegressor()\nparam_grid = {'n_estimators': [10, 100, 500], 'min_samples_leaf': [i for i in range(5,10)], \n              'max_features': ['auto'], 'max_depth': [i for i in range(4,6)], \n              'criterion': ['mse'], 'bootstrap': [False]}\nresult = get_model(train, valid, target_train, target_valid, rf ,'Random Forest Regressor', param_grid, cv_train, result)","5a91ee1c":"# XGBoost Regressor\nxgb = XGBRegressor(verbosity=0)\nparameters = {'nthread':[4], #when use hyperthread, xgboost may become slower\n              'objective':['reg:linear'],\n              'learning_rate': [.03, 0.05, .07], #so called `eta` value\n              'max_depth': [5, 6, 7],\n              'min_child_weight': [4],\n              'silent': [1],\n              'subsample': [0.7],\n              'colsample_bytree': [0.7],\n              'n_estimators': [500]}\nresult = get_model(train, valid, target_train, target_valid, xgb ,'XGBoost Regressor', param_grid, cv_train, result)","30ca4cc3":"# Building plot for prediction for the valid data \n","28821733":"# Building plot for prediction for the test data \n","0101dba1":"def plot_prediction(result, type_plot, target_train=[]):\n    if (type_plot == 'training'):\n        result_type = 'y_train'\n        result_title = 'Prediction for the training data'\n    if (type_plot == 'validation'):\n        result_type = 'y_val'\n        result_title = 'Prediction for the validation data'\n    if (type_plot == 'testing'):\n        result_type = 'y_test'\n        result_title = 'Prediction for the testing data'\n    x = np.arange(len(result.at[0, result_type]))\n    plt.figure(figsize=(16,10))\n    if (type_plot != 'testing'):\n        plt.scatter(x, target_train, label = \"Target data\", color = 'g')\n    plt.scatter(x, result.at[0, result_type], label = \"Decision Tree prediction\", color = 'b')\n    plt.scatter(x, result.at[1, result_type], label = \"Random Forest prediction\", color = 'y')\n    plt.scatter(x, result.at[2, result_type], label = \"XGB prediction\", color = '#17becf')\n    plt.plot(x, np.full(len(result.at[0, result_type]), 0.5), label = \"Maximum allowable value\", color = 'r')\n    plt.title(result_title)\n    plt.legend(loc='best')\n    plt.grid(True)\n    ","7c9e64ab":"plot_prediction(result, 'training', target_train)","d00edbcc":"plot_prediction(result, 'validation', target_valid)","19352850":"plot_prediction(result, 'testing')","a5ed565b":"# Display results of modeling\nresult.sort_values(by=['valid_score', 'train_score'], ascending=False)","57e8fb4e":"# Select models with minimal overfitting\nresult_best = result[(result['train_score'] - result['valid_score']).abs() < 5]\nresult_best.sort_values(by=['valid_score', 'train_score'], ascending=False)","cda3d7df":"# Select the best model\nresult_best.nlargest(1, 'valid_score')","7c1b9e0f":"# Find a name of the best model (with maximal valid score)\nbest_model_name = result_best.loc[result_best['valid_score'].idxmax(result_best['valid_score'].max()), 'model']","fece093c":"print(f'The best model is \"{best_model_name}\"')","847edb54":"### 3.2. Data standartization<a class=\"anchor\" id=\"3.2\"><\/a>\n\n[Back to Table of Contents](#0.1)","486ed82a":"## 4. Modeling<a class=\"anchor\" id=\"4\"><\/a>\n\n[Back to Table of Contents](#0.1)","b49a42ec":"**ADDITIONAL TASKS:** \n1. Add to dataframe result also calculated array: y_test.\n2. Add the line with XGBRegressor model prediction (train, valid, test take from the dataframe result).\n3. Creation the function with all commands and output information for all models (for type_plot = 'training', 'valid' or 'test'):\n\n        plot_prediction(result, type_plot='training')","a4fbe817":"### 4.1. Decision Tree Regressor<a class=\"anchor\" id=\"4.1\"><\/a>\n\n[Back to Table of Contents](#0.1)","623aa0fc":"**It is important to make sure** that all features in the training and test datasets:\n* do not have missing values (number of non-null values = number of entries of index) \n* all features have a numeric data type (int8, int16, int32, int64 or float16, float32, float64).","110caecf":"**TASK:** Building plot for prediction for the test data.","8b44e906":"**TASK:** Display the statistics for test data","82038f4a":"**TASK:** Standardize the test dataset with the same scaler and display it","3428881f":"### 3.4. Cross-validation of training data<a class=\"anchor\" id=\"3.4\"><\/a>\n\n[Back to Table of Contents](#0.1)","f237a99d":"**ADDITIONAL TASKS:** \n1. Set number of splitting = 5, 7, 10 and to compare of results.\n2. Try use another method for cross-validation of training data (without shuffle):\n\n        KFold(n_splits=5, shuffle=False, random_state=0)","22c88297":"## 2. Download data<a class=\"anchor\" id=\"2\"><\/a>\n\n[Back to Table of Contents](#0.1)","2afa112b":"## 5. Visualization<a class=\"anchor\" id=\"6\"><\/a>\n\n[Back to Table of Contents](#0.1)","60b2d229":"**ADDITIONAL TASK:** Try use RobustScaler or MinMaxScaler instead of StandardScaler and to analyze what is the difference for accuracy of models will be below.","7525caa6":"### 4.2. Random Forest Regressor<a class=\"anchor\" id=\"4.2\"><\/a>\n\n[Back to Table of Contents](#0.1)","8d6442cd":"## 3. EDA & FE & Preprocessing data<a class=\"anchor\" id=\"3\"><\/a>\n\n[Back to Table of Contents](#0.1)","4e605de7":"**ADDITIONAL TASK:** Try use other values in the parameter test_size above: 0.1, 0.15, 0.3, 0.5 and to analyze what is the difference for accuracy of models will be below.","309feb55":"### 4.3. XGBoost Regressor<a class=\"anchor\" id=\"4.3\"><\/a>\n\n[Back to Table of Contents](#0.1)","fca2a376":"### 3.1. Statistics & FE<a class=\"anchor\" id=\"3.1\"><\/a>\n\n[Back to Table of Contents](#0.1)","f91d4071":"**TASK:** Building plot for prediction for the valid data.","bcfffdde":"### 3.3. Training data splitting<a class=\"anchor\" id=\"3.3\"><\/a>\n\n[Back to Table of Contents](#0.1)","b57f9b97":"**ADDITIONAL TASK:** Add the XGBRegressor model (the same commands as in 4.1 and 4.2 adapted to the library xgb). Please see example in the notebooks: \n* [BOD prediction in river - 15 regression models](https:\/\/www.kaggle.com\/vbmokin\/bod-prediction-in-river-15-regression-models)\n* [XGBRegressor with GridSearchCV](https:\/\/www.kaggle.com\/jayatou\/xgbregressor-with-gridsearchcv)","771994fb":"The analysis showed that many values are only available in stations 1 and 2, while others have much less data. I propose select only these two stations.","2aea1703":"**TASK:** Make EDA for the test dataset by Pandas Profiling","af98eab0":"**ADDITIONAL TASKS:** \n1. Add to dataframe result also calculated array: y_train, y_val.\n2. Creation the function with all commands and output information (in each section of this chapter 4) for all models:\n\n        result = get_model(train, valid, target_train, target_valid, model_name, param_grid, cv_train, result)","a7e0bd46":"## 1. Import libraries<a class=\"anchor\" id=\"1\"><\/a>\n\n[Back to Table of Contents](#0.1)","eb4a69b6":"**TASK:** Display information about validation data","a1450e9f":"## 6. Select the best model <a class=\"anchor\" id=\"7\"><\/a>\n\n[Back to Table of Contents](#0.1)"}}