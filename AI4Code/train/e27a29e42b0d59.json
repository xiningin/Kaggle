{"cell_type":{"73356daa":"code","f0a5dec6":"code","cabfbe61":"code","d547fa08":"code","cac2964e":"code","6815f6c0":"code","7d614caa":"code","04cf07f0":"code","40084b9b":"code","7bf53e24":"code","f3948571":"code","2829a52a":"code","7a7ba734":"code","07a9abd5":"code","c730141c":"code","be9b3d3f":"code","39f58c16":"code","baf3ff32":"code","dc8cadc1":"code","4a4dfcab":"code","d7f2a5de":"code","096a59de":"code","627e0be9":"code","988d4a87":"code","2633d5bc":"code","997992fe":"code","828c6297":"code","5aefe7a1":"code","59013d9e":"markdown","27c31854":"markdown","d6f71d66":"markdown","c14e5178":"markdown","3eb1d65f":"markdown","385ac46d":"markdown","99aac9b1":"markdown","9c3bf5f8":"markdown","49c23d0a":"markdown","5fae3daa":"markdown","d204d333":"markdown","e1cf72ea":"markdown","899fae50":"markdown","5d4780ed":"markdown","07a51793":"markdown","44dd98f5":"markdown","d4dc46fa":"markdown","d63f58a9":"markdown","b6f87dea":"markdown","b9b0669b":"markdown","62bbaeb7":"markdown","0f977b29":"markdown","28982167":"markdown","b8dede59":"markdown","3bbba4b1":"markdown","7cebe326":"markdown","0791714e":"markdown","a275130e":"markdown","e44bdc45":"markdown","89abead6":"markdown","f42d0f55":"markdown","5977c437":"markdown","ccc3fc5c":"markdown","16e8e1ad":"markdown","4009ec0e":"markdown","674dc39e":"markdown","33d572df":"markdown","4b1b25a7":"markdown","ea8ca400":"markdown"},"source":{"73356daa":"#Importing libraries\nimport pandas as pd\nimport numpy as np\nimport os\nimport plotly.express as px\nfrom sklearn.cluster import KMeans\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, mean_absolute_percentage_error\nfrom sklearn.preprocessing import StandardScaler\nimport xgboost\nimport warnings\nwarnings.filterwarnings('ignore')","f0a5dec6":"#Importing User Defined modules\nos.chdir(r'\/kaggle\/input\/solar-eda')\nimport ads_creation as ads_crtn\nimport eda_analysis as eda\nimport feature_engg as feat_engg\nimport winsorize as wz\nimport regression as regr\nimport clustering as clst\nfrom constants import ATTR, OUTLIER_METHOD, TIME_INVARIANT_ATTR, TIME_VARIANT_ATTR, SPLIT_PCT, IDENTIFIERS","cabfbe61":"#Funciton to create list of timestamps from first to last\ndef create_ts_list(df,date_col):\n    '''\n    This function creates the list of timestamps from first ts to last ts\n\n    Input:\n    1. df: pandas dataframe, this is the ads subsetted for one panel\n    2. date_col: str, this is the name of the column containing the dates\n\n    Return\n    1. dates: pandas dataframe, containing all the timestamps from start to end\n    '''\n    dates = pd.date_range(start=df[date_col].min(), end = df[date_col].max(),freq = '15min')\n    dates = dates.to_frame(index=False)\n    dates = dates.rename(columns = {0:'DATE'})\n    return dates\n","d547fa08":"#Function to merge in the missing dates in the ADS\ndef complt_ts(df,date_col):\n    '''\n    This function introduces the missing time stamps in ads\n\n    Input:\n    1. df: pandas dataframe, this is the ads subsetted for one panel\n    2. date_col: str, this is the name of the column containing the dates\n\n    Return\n    1. df: pandas dataframe, containing all the timestamps from start to end\n    '''\n    date_list = create_ts_list(df,date_col)\n    df = pd.merge(left=date_list,right=df,how='left', on='DATE')\n    return df\n","cac2964e":"#Performing data completions at a panel level\ndef complt_ts_panel_lvl(df,panel_id_col,date_col):\n    '''\n    This function completes the time stamps for all panels and applies MVT as well\n\n    Input:\n    1. df: pandas dataframe, this is the ads for which timestamps need to be completed\n    2. panel_id_col: str, the name of the column containing the panel IDs\n    3. date_col, str, the name of the column containing the dates\n\n    Returns\n    1. df: pandas dataframe, containing all the timestamps from start to end\n    '''\n    df = df.groupby(panel_id_col).apply(lambda x: complt_ts(x, date_col))\n    df = df.reset_index(drop=True)\n    df['TIME_OF_DAY'] = df['DATE'].apply(lambda x: 1 if x.hour in range(6,18) else 0)\n    #if we dont do this ffill here then the all the obs for inverter ids won't reach\n    df[panel_id_col] = df[panel_id_col].ffill()\n    df = df.groupby(panel_id_col).apply(lambda x: ads_crtn.missing_value_treatment(x))\n    return df\n","6815f6c0":"#Function to try different lags, starting from 2 days out till 2 hours and check the accuracy\ndef get_optimal_lag(train_df,init_lag,pass_threshold,mode,panel_id_col,model_obj,feature_selection=False):\n    '''\n    This function gets the lag that is most optimal in predicting the target variable\n    Either in isolated mode or cumulative mode\n\n    Input:\n    1. train_df: pandas dataframe, this is the ads\n    2. init_lag: int, the largest lag to begin with\n    3. pass_threshold: float, the minimum r-squared values required\n    4. mode: str, isolated or cumulative mode\n    - Isolated mode: just creates one set of lags and check in silo\n    - Cumulative mode: creates a group of lags like 2 day and 1 day lagged variables and checks accuracy together\n\n    Return:\n    1. model_dict: python dictionary, this dict contains model objects for each panel\n    2. metric_dict: python dictionary, this dict contains error metrics for each panel\n    '''\n\n    #Creating a validation df because here we will be choosing the optimal lag\n    #this is a form of tuning and test dataset should not be used\n    train_df,validation_df = eda.train_test_split(train_df,'INVERTER_ID',0.9)\n    r2 = 0\n    lag = init_lag\n    lag_list = [init_lag]\n\n    #while loop runs till the time the condition is true\n    #here we want the loop to stop as soon as one condition is met and the condition to become false\n    #if we do or between the two conditions, while loop will still keep running because 0 + 1 = 1 and it will keep running\n    #but if we put 'and' then 0 * 1 = 0 and we get the desired result\n    while (r2 < pass_threshold) and (lag > 0):\n        print('Doing for lag: ',lag)\n        train_df_lagged = feat_engg.create_lagged_features(train_df,TIME_VARIANT_ATTR,lag_list,'INVERTER_ID','DATE')\n        validation_df_lagged = feat_engg.create_lagged_features(validation_df,TIME_VARIANT_ATTR,lag_list,'INVERTER_ID','DATE')\n\n        features = [feature for feature in train_df_lagged.columns if '_lag_' in feature]\n        features = features + TIME_INVARIANT_ATTR\n        \n        if feature_selection == True:\n            features = feat_engg.rfe_feature_selection(train_df_lagged,\n                                                RandomForestRegressor(random_state=42),\n                                                features,'PER_TS_YIELD')\n\n        models,metrics = regr.panel_wise_model(train_df_lagged,validation_df_lagged,panel_id_col,'PER_TS_YIELD',\n                                              model_obj,features=features)\n        r2 = metrics[train_df_lagged[panel_id_col].unique()[0]][3]\n        print(r2)\n\n        if mode == 'isolated':\n            lag -= 8\n            lag_list = [lag]\n        elif mode == 'cumulative':\n            lag -= 8\n            lag_list.append(lag)\n    return models,metrics\n","7d614caa":"#Perseverence model\ndef fwd_step_rf_model(train_df,test_df,model_dict,panel_name):\n    '''\n    This function implements a perseverence model i.e. predicts 15 min in advance then uses that prediction \n    to churn out one more prediction\n\n    Input\n    1. train_df: pandas dataframe, training dataset\n    2. test_df: pandas dataframe, testing dataset\n    3. model_dict: dictionary, dict containing the prediciton models\n    4. panel_name: str, string containing panel name for which the model will be run\n\n    Returns:\n    1. predicitions: list, which contains the predicitons made for 2 days\n    '''\n    predictions = []\n    scaler = StandardScaler()\n    scaler.fit(train_df.values.reshape(-1,1))\n    train_df_scaled = scaler.transform(train_df.values.reshape(-1,1))\n    pred_obs = train_df_scaled[-1]\n    \n    for iter in range(len(test_df)):\n        if iter == 0:\n            pred_obs = model_dict[panel_name].predict(pred_obs.reshape(-1,1))\n            predictions.append(pred_obs)\n        else:\n            pred_obs = model_dict[panel_name].predict(scaler.transform(pred_obs.reshape(-1,1)))\n            predictions.append(pred_obs)\n\n    return predictions\n","04cf07f0":"#Acutals vs Forecast\ndef act_vs_fcst(y_true,y_pred,date_series):\n    '''\n    This function plots actuals vs forecasts\n\n    Input\n    1. y_true: pandas series, ground truth\n    2. y_pred: pandas series, predictions\n    3. date_series: pandas series, series with the dates\n    '''\n    actual_df = pd.DataFrame({'DATE':date_series, 'Panel':'Actual',\n                        'Actual':y_true})\n    pred_df = pd.DataFrame({'DATE':date_series, 'Panel':'Predicted',\n                        'Predicted':y_pred})\n\n    plot_df = pd.concat([actual_df,pred_df])\n    fig = px.line(plot_df,'DATE',['Actual','Predicted'])\n    fig.show()\n\n    return\n","40084b9b":"#Function for cleaning data\ndef remove_anomaly(df):\n    '''\n    This function removes instances wherein we have irradiation but AC power is 0\n\n    Input:\n    1. df: pandas dataframe, dataset containing noise\n\n    Returns:\n    df: pandas dataframe, dataset that is clean\n    '''\n    anam_idx = df.index[(df['IRRADIATION'] != 0) & (df['AC_POWER'] == 0)]\n    df = df.drop(anam_idx)\n    return df\n","7bf53e24":"#ADS creation, train-test split, outlier treatment\nads = ads_crtn.create_ads()\ntrain_ads, test_ads = eda.train_test_split(ads,'INVERTER_ID',SPLIT_PCT)\n\n#leaving out total yield since it value since it value increase exponentially and rightly so\noutlier_feature = [feature for feature in ATTR if feature != 'TOTAL_YIELD'] \nclip_model = wz.winsorize(OUTLIER_METHOD)\nclip_model.fit(train_ads[outlier_feature],'INVERTER_ID')\ntrain_ads[outlier_feature] = clip_model.transform(train_ads[outlier_feature])\ntest_ads[outlier_feature] = clip_model.transform(test_ads[outlier_feature])\n","f3948571":"#Completing timestamps, so that each panel has all the timestamps from start to finish\ntrain_ads_exp = complt_ts_panel_lvl(train_ads,'INVERTER_ID','DATE')\ntest_ads_exp = complt_ts_panel_lvl(test_ads,'INVERTER_ID','DATE')\n\n#making ADS stationary\nnon_stnry_invtr_list = eda.return_non_stnry_invtr_list(train_ads_exp,'INVERTER_ID')\ntrain_ads = eda.make_ads_stnry(train_ads_exp,non_stnry_invtr_list,'INVERTER_ID','DATE')\n\nnon_stnry_invtr_list = eda.return_non_stnry_invtr_list(test_ads_exp,'INVERTER_ID')\ntest_ads = eda.make_ads_stnry(test_ads_exp,non_stnry_invtr_list,'INVERTER_ID','DATE')\n\n#Creating lagged features which are lagged by exactly 2 days so that they can then be used for prediction\n\ntrain_ads_exp = feat_engg.create_lagged_features(train_ads_exp,TIME_VARIANT_ATTR,[192],'INVERTER_ID','DATE')\ntest_ads_exp = feat_engg.create_lagged_features(test_ads_exp, TIME_VARIANT_ATTR, [192], 'INVERTER_ID','DATE')\n\nfeatures = [feature + '_lag_192' for feature in TIME_VARIANT_ATTR]\nfeatures = features + TIME_INVARIANT_ATTR\n\nmodels,metrics = regr.panel_wise_model(train_ads_exp,test_ads_exp,\n                                        'INVERTER_ID','PER_TS_YIELD',\n                                        RandomForestRegressor(random_state=42),\n                                        features=features)\n\n\nlow_acc_invtr = [(key,metrics[key][3]) for key in metrics.keys() if metrics[key][3] < 0.8]\ninvtr_list = [val_pair[0] for val_pair in low_acc_invtr]\nprint(low_acc_invtr)\n","2829a52a":"metrics_rfe = {}\nfor invertor in invtr_list[0:2]:\n    feat_set = feat_engg.rfe_feature_selection(train_ads_exp.loc[train_ads_exp['INVERTER_ID']==invertor,],\n                                                RandomForestRegressor(random_state=42),\n                                                features,'PER_TS_YIELD')\n    \n    models,metrics = regr.panel_wise_model(train_ads_exp.loc[train_ads_exp['INVERTER_ID']==invertor,],\n                                            test_ads_exp.loc[test_ads_exp['INVERTER_ID']==invertor,],\n                                            'INVERTER_ID',\n                                            'PER_TS_YIELD',\n                                            RandomForestRegressor(random_state=42),\n                                            features=feat_set)\n    print('Added metric for :',invertor)\n    metrics_rfe[invertor] = metrics[invertor]","7a7ba734":"print(metrics_rfe)","07a9abd5":"#Finding the lag which gives the best accuracy, starting from 2 days out and moving closer to t-1\n#All the different lags are looked at in isolation or cumulatively\n#conducting this experiment with only one panel, if successful only then will scale up to other panels\n\ntrial_df = train_ads.loc[train_ads['INVERTER_ID']==invtr_list[1],]\n\n#running it in isolated mode first\ntry_model,try_metric = get_optimal_lag(trial_df,192,0.8,'isolated',\n                                        'INVERTER_ID',\n                                        RandomForestRegressor(random_state=42),False)\n\n#running the pipeline in cumulative mode\n#better performance as compared to before but still did not cross 80% on validation dataset\n\n#Commenting the below piece of code, since it takes a lot of time on local system\n#try_model,try_metric = get_optimal_lag(trial_df,192,0.8,'cumulative')\n","c730141c":"#Creating lagged version of target variable \ntrain_ads_exp = feat_engg.create_lagged_features(train_ads,['PER_TS_YIELD'],[1],'INVERTER_ID','DATE')\ntest_ads_exp = feat_engg.create_lagged_features(test_ads, ['PER_TS_YIELD'], [1], 'INVERTER_ID','DATE')\n\nfeatures = [feature for feature in train_ads_exp.columns if '_lag_' in feature]\n\nmodels,metrics = regr.panel_wise_model(train_ads_exp,test_ads_exp,\n                                        'INVERTER_ID','PER_TS_YIELD',\n                                        RandomForestRegressor(random_state=42),features=features)\n\nlow_acc_invtr = [(key,metrics[key][3]) for key in metrics.keys() if metrics[key][3] < 0.8]\ninvtr_list = [val_pair[0] for val_pair in low_acc_invtr]\n\n#Persevernce Model\nfwd_models = {}\nfor invtr in invtr_list:\n    print('Modelling for panel: ',invtr)\n    preds = fwd_step_rf_model(train_ads_exp.loc[train_ads_exp['INVERTER_ID'] == invtr,features],\n                            test_ads_exp.loc[test_ads_exp['INVERTER_ID'] == invtr,features],\n                            models,invtr)\n\n\n    y_pred = [prediction for sublist in preds for prediction in sublist]\n\n    r2 = r2_score(test_ads_exp.loc[test_ads_exp['INVERTER_ID'] == invtr,'PER_TS_YIELD'],y_pred)\n    fwd_models[invtr] = r2\n","be9b3d3f":"print(fwd_models['4UPUqMRk7TRMgml'])","39f58c16":"model = xgboost.XGBRegressor(random_state=42)\n\ntrain_ads_exp_xgb = train_ads_exp.loc[train_ads_exp['INVERTER_ID'].isin(invtr_list),]\ntest_ads_exp_xgb = test_ads_exp.loc[test_ads_exp['INVERTER_ID'].isin(invtr_list),]\nmodels_xgb,metrics_xgb = regr.panel_wise_model(train_ads_exp_xgb,test_ads_exp_xgb,'INVERTER_ID','PER_TS_YIELD',model,features=features)\n\nlow_acc_invtr = [(key,metrics_xgb[key][3]) for key in metrics_xgb.keys() if metrics_xgb[key][3] < 0.8]\ninvtr_list = [val_pair[0] for val_pair in low_acc_invtr]","baf3ff32":"low_acc_invtr","dc8cadc1":"#Completing timestamps, so that each panel has all the timestamps from start to finish\ntrain_ads_exp = complt_ts_panel_lvl(train_ads,'INVERTER_ID','DATE')\ntest_ads_exp = complt_ts_panel_lvl(test_ads,'INVERTER_ID','DATE')\n\ntrain_ads_exp = feat_engg.create_lagged_features(train_ads_exp,TIME_VARIANT_ATTR,[192],'INVERTER_ID','DATE')\ntest_ads_exp = feat_engg.create_lagged_features(test_ads_exp, TIME_VARIANT_ATTR, [192], 'INVERTER_ID','DATE')\n\nfeatures = [feature + '_lag_192' for feature in TIME_VARIANT_ATTR]\nfeatures = features + TIME_INVARIANT_ATTR\n\nmodels,metrics = regr.panel_wise_model(train_ads_exp,test_ads_exp,\n                                        'PLANT_ID','PER_TS_YIELD',\n                                        RandomForestRegressor(random_state=42),\n                                        features=features)","4a4dfcab":"print(metrics)","d7f2a5de":"train_ads_clst = train_ads.loc[train_ads['PLANT_ID']==4136001.0].reset_index(drop=True)\ntest_ads_clst = test_ads.loc[test_ads['PLANT_ID']==4136001.0].reset_index(drop=True)\n\n#running the below loop to give us the number of clusters that are optimal for this clustering exercise\n#10 clusters are coming out to be most optimal, so lets go ahead witthat\n\nn_clust = clst.get_n_clusters(train_ads_clst.loc[:,~train_ads_clst.columns.isin(IDENTIFIERS)],2,11)\n\n#kmeans clustering with 10 clusters\nclusterer = KMeans(n_clusters=n_clust, random_state=42)\ncluster_labels = clusterer.fit_predict(train_ads_clst.loc[:,~train_ads_clst.columns.isin(IDENTIFIERS)])\n\ntrain_ads_clst = pd.concat([pd.DataFrame({'CLUSTER_ID':list(cluster_labels)}),train_ads_clst],axis=1)\n\n#putting the cluster IDs in the test ads as well\ninvtr_clst_map = train_ads_clst[['INVERTER_ID','CLUSTER_ID']].drop_duplicates().reset_index(drop=True)\ntest_ads_clst = pd.merge(test_ads_clst,invtr_clst_map,on='INVERTER_ID',how='inner')\n\n","096a59de":"#Making model for each cluster\ntrain_ads_clst = feat_engg.create_lagged_features(train_ads_clst,TIME_VARIANT_ATTR,[192],'INVERTER_ID','DATE')\ntest_ads_clst = feat_engg.create_lagged_features(test_ads_clst, TIME_VARIANT_ATTR, [192], 'INVERTER_ID','DATE')\n\nfeatures = [feature + '_lag_192' for feature in TIME_VARIANT_ATTR]\nfeatures = features + TIME_INVARIANT_ATTR\n\nmodels,metrics = regr.panel_wise_model(train_ads_clst,test_ads_clst,\n                                        'CLUSTER_ID','PER_TS_YIELD',\n                                        RandomForestRegressor(random_state=42),\n                                        features=features)","627e0be9":"print(metrics)","988d4a87":"train_ads_exp = train_ads.loc[train_ads['PLANT_ID']==4136001.0].reset_index(drop=True)\ntest_ads_exp = test_ads.loc[test_ads['PLANT_ID']==4136001.0].reset_index(drop=True)\n\n_,metrics = get_optimal_lag(train_ads_exp,192,0.8,'isolated','PLANT_ID',RandomForestRegressor(random_state=42))\n","2633d5bc":"    train_ads_exp = remove_anomaly(train_ads)\n    test_ads_exp = remove_anomaly(test_ads)\n\n    train_ads_exp = complt_ts_panel_lvl(train_ads_exp,'INVERTER_ID','DATE')\n    test_ads_exp = complt_ts_panel_lvl(test_ads_exp,'INVERTER_ID','DATE')\n\n    train_ads_exp = feat_engg.create_lagged_features(train_ads_exp,TIME_VARIANT_ATTR,[192],'INVERTER_ID','DATE')\n    test_ads_exp = feat_engg.create_lagged_features(test_ads_exp, TIME_VARIANT_ATTR, [192], 'INVERTER_ID','DATE')\n\n    features = [feature + '_lag_192' for feature in TIME_VARIANT_ATTR]\n    features = features + TIME_INVARIANT_ATTR + ['TIME_OF_DAY']\n    \n    models,metrics = regr.panel_wise_model(train_ads_exp,test_ads_exp,\n                                        'PLANT_ID','PER_TS_YIELD',\n                                        RandomForestRegressor(random_state=42),\n                                        features=features)","997992fe":"metrics","828c6297":"scaler = StandardScaler()\nscaler.fit(train_ads_exp.loc[train_ads_exp['PLANT_ID'] == 4135001,features])\n\ntrain_df_scaled = scaler.transform(train_ads_exp.loc[train_ads_exp['PLANT_ID'] == 4135001,features])\ntest_df_scaled = scaler.transform(test_ads_exp.loc[test_ads_exp['PLANT_ID'] == 4135001, features])\n\npanel_idx = test_ads_exp.index[test_ads_exp['INVERTER_ID'] == '1BY6WEcLGh8j5v7']\n\ny_pred = models[4135001].predict(test_df_scaled[panel_idx])\n\nact_vs_fcst(test_ads_exp.loc[panel_idx,'PER_TS_YIELD'], y_pred, test_ads_exp.loc[panel_idx,'DATE'])","5aefe7a1":"scaler = StandardScaler()\nscaler.fit(train_ads_exp.loc[train_ads_exp['PLANT_ID'] == 4136001,features])\n\ntrain_df_scaled = scaler.transform(train_ads_exp.loc[train_ads_exp['PLANT_ID'] == 4136001,features])\ntest_df_scaled = scaler.transform(test_ads_exp.loc[test_ads_exp['PLANT_ID'] == 4136001, features])\n\npanel_idx = test_ads_exp.index[test_ads_exp['INVERTER_ID'] == 'Qf4GUc1pJu5T6c6']\n\ny_pred = models[4136001].predict(test_df_scaled[panel_idx])\n\nact_vs_fcst(test_ads_exp.loc[panel_idx,'PER_TS_YIELD'], y_pred, test_ads_exp.loc[panel_idx,'DATE'])","59013d9e":"<h3> From the above experiment we saw that we are having a lot of trouble predicting for the plant 4136001 <\/h3>","27c31854":"<h1> Experiment 2 <\/h1>","d6f71d66":"<h2> Performed this experiment on only 2 panels, for faster execution. Got very poor accuracy with rfecv as well <\/h2>","c14e5178":"<h1> Experiment 4 <\/h1>","3eb1d65f":"<h2> XGBoost perfomed well, but still not 80% + accuracy<\/h2>","385ac46d":"<h1> Common steps for all Experiments <\/h1>","99aac9b1":"<h2> Making a model at a plant level, moving away from panel level models <\/h2>","9c3bf5f8":"<h2> In this experiment, We will create lagged versions of all our features, wherein all the features would be lagged be 2 days. This way we can use the observations from today and predict 2 days in advance <\/h2>","49c23d0a":"<h1> Experiment 7 <\/h1>","5fae3daa":"<h2> Plotting Actuals vs Predicted for a panel from plant 4136001 <\/h2>","d204d333":"<h2> Getting good accuracy for plant 1, but very low for plant 2 <\/h2>","e1cf72ea":"Format for reading the results: {Panel_name: (mape,mae,mse,r-squared)","899fae50":"<h1> Experiment 10 <\/h1>","5d4780ed":"<h2> Trying to find optimal lag while making a model at a plant level <\/h2>","07a51793":"<h2> Making a XGBoost model for all the panels that did not perform well in Experiment 1<\/h2>","44dd98f5":"<h2> Did not achieve good results with this as well, no lag was able to give more than 80% accuracy <\/h2>","d4dc46fa":"<h1> Experiment 1 <\/h1>","d63f58a9":"<h1> Experiment 6<\/h1>","b6f87dea":"<h2> Printed the accuracy for an example panel, like this panel most panels have negative r-squared. So this experiment also failed miserably <\/h2>","b9b0669b":"<h2> Here as well our model does well, but not quite as good as plant 1 model. We can try and squeze out more performance, but we will stop here for now <\/h2>","62bbaeb7":"Format for reading the results: (Panel_Name:R squared value)","0f977b29":"<h2> Removing the noisy observations from the data, in which there is no AC power even when the Irradiation is non zero <\/h2>","28982167":"Format for reading the results: {Plant name: (mape,mae,mse,r-squared)}","b8dede59":"<h1> Experiment 5 <\/h1>","3bbba4b1":"<h2> Performing rfecv based on RF on the panels that gave poor accuracy in the previous step. So, that the significant variables may increase the accuracy <\/h2>","7cebe326":"<h1> Experiment 3 <\/h1>","0791714e":"<h2> As we can see the model is able to very well capture the variation present in the data <\/h2>","a275130e":"<h1> Helper Functions <\/h1>","e44bdc45":"<h2> Plotting Actuals vs Predicted for a panel from plant 4135001 <\/h2>","89abead6":"<h2>Perseverence model<\/h2>\n<h2>Forecast only 15 min in advance and use that forecast as input and forecast for another 15 min<\/h2>","f42d0f55":"Format for reading the result: (Panel_Name:R-squared)","5977c437":"<h2>Clustering within plant 4136001 and making a model for each cluster<\/h2>","ccc3fc5c":"<h1> Experiment 9 <\/h1>","16e8e1ad":"<h2> With this denoised dataset, our random forest model finally comes good and we get 80% + accuracy on both the data sets <\/h2>","4009ec0e":"<h2> Even one model for each cluster is not giving good accuracy, will have to move onto something else <\/h2>","674dc39e":"<h2> Finding the most optimal lag starting from 2 days to 15 min. To further use in perseverence model <\/h2>","33d572df":"<h2> Tried to get optimal lag with feature selection in cumulative mode & ran the code on google colab. It took a lot of time and did not get favorable results <\/h2>","4b1b25a7":"<h2> No luck with different lags when modelling for the entire panel as well <\/h2>","ea8ca400":"<h1> Experiment 8 <\/h1>"}}