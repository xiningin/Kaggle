{"cell_type":{"0a257281":"code","ceb84f03":"code","f920a8fd":"code","e8b01f57":"code","71ede0bf":"code","a3b735c3":"code","ad6fb444":"code","d00d4883":"code","5671a2f6":"code","4e6806ea":"markdown","9993dac1":"markdown","68105e27":"markdown","07126a6c":"markdown","cecad54b":"markdown","7ce82fdd":"markdown"},"source":{"0a257281":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","ceb84f03":"train = pd.read_csv('..\/input\/drugsComTrain_raw.csv')\ntrain.head()","f920a8fd":"train.shape","e8b01f57":"rev = np.array(train['review'])\nrev[:10]","71ede0bf":"rev[-10:]","a3b735c3":"drugs = train['drugName'].value_counts()\ndrugs","ad6fb444":"# number of drugs with only 1 review\none_rev = len(np.where(drugs==1)[0])\nprint('Drugs with one review: ', one_rev)\nprint('Drugs with one review: ', round((one_rev\/drugs.shape[0])*100), '%')","d00d4883":"cond = train['condition'].value_counts()\ncond","5671a2f6":"# number of conditions with only 1 review\none_rev = len(np.where(cond==1)[0])\nprint('Conditions with one review: ', one_rev)\nprint('Conditions with one review: ', round((one_rev\/drugs.shape[0])*100), '%')","4e6806ea":"Note it seems like the characters `&#039;` are used to represent apostrophes.","9993dac1":"# Topic Proposal\nOur team will look at the usefulness of reviews, and explore methods of dissecting reviews with natural language processing to prepare text and extract features that are predictive of review quality. \n\nWe plan to use named entity extraction to prepare text with tags on drug names. From this prepared text we will create features which we hypothesize are predictive, e.g. whether they mention drug interactions, \u201cside effects\u201d, or dosage. One subtask for creating features will be to perform opinion extraction, and create features related to the opinions, e.g. sentiment and ratio of opinion words to non-opinion words. Another subtask of feature creation will be determining whether the opinions are centered on the target drug or another related drug. We hypothesize this may be determined by assuming a relationship between the drug rating and the sentiment around the target drug.\n\nAfter data preparation and feature creation, the new feature set will be used to train a model to predict the usefulness score. The main goal is to create an accurate and comprehendible model so that we can better understand what information people want to get out of drug reviews.","68105e27":"## Explore drugs","07126a6c":"## Explore conditions","cecad54b":"Note above that a data quality issue appears in the conditions; condition is substituted with how many users found the comment helpful. These rows should be isolated and dealt with specially.\n\nAlso note that because there are some drugs\/conditions that are rare, so the usefulness score will not be standard (e.g. rare conditions only get a few ratings of 'useful'). We should standardize based on rarity of 'condition', not drug.","7ce82fdd":"## Scan through sample reviews"}}