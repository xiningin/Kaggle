{"cell_type":{"36f9c1f8":"code","f6c3d2cf":"code","903032e6":"code","d340b0c8":"code","4e1139d2":"code","67eeca33":"code","009174f6":"code","3eb32411":"code","988403a2":"code","f653137d":"code","99adf4d0":"code","3e00d104":"code","84c5588f":"code","f03ef26a":"code","87dcf107":"code","8ceb7486":"code","4c37e2dc":"code","f491f78d":"markdown","3aace4dc":"markdown","7173a212":"markdown","1d7b9194":"markdown","9ba8e93e":"markdown","4f3897e3":"markdown","dcd5c0cd":"markdown","7d17caa8":"markdown","17f09f6d":"markdown"},"source":{"36f9c1f8":"!npm install -g @lux-ai\/2021-challenge@latest &> \/dev\/null\n!pip install kaggle-environments -U &> \/dev\/null","f6c3d2cf":"%%bash\n\nAGENT_ST=\"..\/input\/lux-ai-published-agents\/stonet2000\/lux-ai-season-1-jupyter-notebook-tutorial\/v18\"\nAGENT_GP=\"..\/input\/lux-ai-published-agents\/glmcdona\/reinforcement-learning-openai-ppo-with-python-game\/v47\"\nAGENT_IA=\"..\/input\/lux-ai-published-agents\/ilialar\/lux-ai-risk-averse-baseline\/v9\"\nAGENT_H2=\"..\/input\/lux-ai-published-agents\/huikang\/lux-ai-working-title-bot\/v2\"\nAGENT_H3=\"..\/input\/lux-ai-published-agents\/huikang\/lux-ai-working-title-bot\/v3\"\nAGENT_SA=\"..\/input\/lux-ai-published-agents\/shoheiazuma\/lux-ai-with-imitation-learning\/v3\"\nAGENT_SR=\"..\/input\/lux-ai-published-agents\/stefanschulmeister87\/pure-rule-based-agent\/v21\"\nAGENT_RD=\"..\/input\/lux-ai-published-agents\/realneuralnetwork\/lux-ai-with-il-decreasing-learning-rate\/v3\"\nAGENT_AB=\"..\/input\/lux-ai-published-agents\/andrej0marinchenko\/lux-ai-big-bd-model-train\/v3\"\nAGENT_AD=\"..\/input\/lux-ai-published-agents\/adityasharma01\/lux-ai-with-il-decreasing-learning-rate\/v7\"\nAGENT_H7=\"..\/input\/lux-ai-published-agents\/huikang\/lux-ai-working-title-bot-private-version\/v76\"\nAGENT_SU=\"..\/input\/lux-ai-published-agents\/shoheiazuma\/lux-ai-submit-unet-il\/v6\"\nAGENT_H9=\"..\/input\/lux-ai-published-agents\/huikang\/lux-ai-working-title-bot-private-version\/v129\"\nAGENT_IN=\"..\/input\/lux-ai-published-agents\/ironbar\/luxai\/best_local_agent_nairu\"\nAGENT_TB=\"..\/input\/lux-ai-published-agents\/isaiahPressman\/Kaggle_Lux_AI_2021\/best\"\n\n\nexport AGENT_A_DIR=$AGENT_TB\nexport AGENT_B_DIR=$AGENT_H9\nrm -rf agent-a\/\nrm -rf agent-b\/\nmkdir -p agent-a\/\nmkdir -p agent-b\/\ncp -r $AGENT_A_DIR\/* agent-a\/\ncp -r $AGENT_B_DIR\/* agent-b\/\n\necho $AGENT_A_DIR > .AGENT_A_DIR\necho $AGENT_B_DIR > .AGENT_B_DIR","903032e6":"!cd agent-a && tar -xvzf *.tar.gz &> \/dev\/null\n!cd agent-b && tar -xvzf *.tar.gz &> \/dev\/null","d340b0c8":"%%writefile agent-b\/main.py\nfrom typing import Dict\nimport sys\nfrom agent import agent\n\ntry: # for Toad Brigade\n    from lux_ai.rl_agent.rl_agent import agent\nexcept:\n    pass\n\nif __name__ == \"__main__\":\n\n    def read_input():\n        \"\"\"\n        Reads input from stdin\n        \"\"\"\n        try:\n            return input()\n        except EOFError as eof:\n            raise SystemExit(eof)\n    step = 0\n    class Observation(Dict[str, any]):\n        def __init__(self, player=0) -> None:\n            self.player = player\n            # self.updates = []\n            # self.step = 0\n    observation = Observation()\n    observation[\"updates\"] = []\n    observation[\"step\"] = 0\n    observation[\"remainingOverageTime\"] = 60.\n    player_id = 0\n    while True:\n        inputs = read_input()\n        observation[\"updates\"].append(inputs)\n\n        if inputs == \"D_DONE\":\n            if step == 0:  # the codefix\n                player_id = int(observation[\"updates\"][0])\n                observation.player = player_id\n                observation[\"player\"] = player_id\n                observation[\"width\"], observation[\"height\"] = map(int, observation[\"updates\"][1].split())\n            actions = agent(observation, None)\n            observation[\"updates\"] = []\n            step += 1\n            observation[\"step\"] = step\n            print(\",\".join(actions))\n            print(\"D_FINISH\")","4e1139d2":"!cp agent-b\/main.py agent-a\/main.py","67eeca33":"!ls agent-a","009174f6":"!ls agent-b","3eb32411":"%%bash\nGFOOTBALL_DATA_DIR=C lux-ai-2021 \\\n    --loglevel 0 --maxtime 30000  --output . \\\n    --width 12 --height 12 .\/agent-a\/main.py .\/agent-b\/main.py","988403a2":"!ls replays\/\n!cp replays\/* .","f653137d":"%%writefile evaluate_for_map_size.sh\n\nMAP_SIZE=$1\nfor run in {1..1000}; \n    do GFOOTBALL_DATA_DIR=C lux-ai-2021 --seed $run --loglevel 1 --maxtime 10000 \\\n    --height $MAP_SIZE --width $MAP_SIZE --storeReplay=false --storeLogs=false \\\n    .\/agent-a\/main.py .\/agent-b\/main.py >> logs-$MAP_SIZE.txt;\ndone","99adf4d0":"!chmod +x .\/evaluate_for_map_size.sh","3e00d104":"!timeout 1.2h bash .\/evaluate_for_map_size.sh 12","84c5588f":"!timeout 1.6h bash .\/evaluate_for_map_size.sh 16","f03ef26a":"!timeout 2.4h bash .\/evaluate_for_map_size.sh 24","87dcf107":"!timeout 3.2h bash .\/evaluate_for_map_size.sh 32","8ceb7486":"wins_template = \"\"\"\n    { rank: 1, agentID: 0, name: '.\/agent-a\/main.py' },\n    { rank: 2, agentID: 1, name: '.\/agent-b\/main.py' }\n\"\"\"\n\ndraw_template = \"\"\"\n    { rank: 1, agentID: 0, name: '.\/agent-a\/main.py' },\n    { rank: 1, agentID: 1, name: '.\/agent-b\/main.py' }\n\"\"\"\n\nlose_template = \"\"\"\n    { rank: 1, agentID: 1, name: '.\/agent-b\/main.py' },\n    { rank: 2, agentID: 0, name: '.\/agent-a\/main.py' }\n\"\"\"\n\nmap_sizes = [12,16,24,32]\ntotal_score = 0\nfor map_size in map_sizes:\n    logfile_name = f\"logs-{map_size}.txt\"\n    with open(logfile_name) as f:\n        data_string = f.read()\n        wins = data_string.count(wins_template)\n        draw = data_string.count(draw_template)\n        lose = data_string.count(lose_template)\n        score = (wins + draw \/ 2)\/(wins + lose + draw)*100\n        total_score += score\/len(map_sizes)\n        print(f\"Map size: {map_size}, Score: {score:.3f}, Stats: {wins}\/{draw}\/{lose}\")\nprint(f\"Total score: {total_score:.0f}\")","4c37e2dc":"!cat .AGENT_A_DIR\n!cat .AGENT_B_DIR","f491f78d":"This is some code fix I need to make so that the imitation agent can run with `main.py`.","3aace4dc":"# Batch Evaluation\n\nIf you want measure the winrate between two agents, you need to play many matches.\n\nFor each map size, we play a number of matches until timeout. For larger maps, we play for a longer time in total.\n\nTo reduce uncertainty in relative performance, the seed of the matches will have to be consistent over different plays.","7173a212":"You can upload the replay file onto https:\/\/2021vis.lux-ai.org\/ to see the battle.\n\n(If you figure out a way to view the replay on Kaggle notebooks given the replay file, please share with us the method)","1d7b9194":"Use your statistics knowledge to calculate the confidence interval of the winrate. There are [tools](https:\/\/epitools.ausvet.com.au\/ciproportion) online that you can use to estimate the confidence interval.","9ba8e93e":"# Evaluation Summary\n\nWe estimate of the winrate with the weighted average of wins. A draw is considered half a win.","4f3897e3":"# Installation\n\nInstall the command line interface (CLI) published by the contest makers.\n\nYou will also need to update the `kaggle-environments` Python package.","dcd5c0cd":"# Load Agents\n\nCopy the published agents from the notebook output into local directory, and unzip the submission files.\n\nYou may seek the source notebook for the submission files. However, the top solution of the notebook may not be the latest version. Exporting to a Kaggle Dataset is a workaround. To standardise the procedures, all agents will be loaded from the dataset.","7d17caa8":"# Single Evaluation\n\nWe play a single match on a 12x12 board.","17f09f6d":"# Lux AI Agent Evaluation\n\nThis notebook shows how you can compare performance of two agents on Kaggle with the `lux-ai-2021` CLI.\n\nIf you want to play many matches to determine the winrate between two agents, you can run the tournament with Kaggle notebook \"Save and Run\".\n\nThere are a few benefits to this arrangement\n- This is one way to document your winrate.\n- You do not need to use your computing resources.\n- You can work on other things while this runs.\n- This arrangement is on Kaggle, where you might be working on.\n\nHowever, there are a few issues\n- You are only limited to 4 CPUs on Kaggle.\n- A notebook run is at most 9 hours at once.\n- You are not able to see intermediate results.\n- You will need to upload your agent to Kaggle.\n- To view the replays, you will need to upload your replays to https:\/\/2021vis.lux-ai.org\/\n- I needed to add a few lines to `main.py` so that it can run some agents with the CLI.\n\nRegardless, I hope this is a demo on how you can use the CLI - what needs to be installed and what input does it expects."}}