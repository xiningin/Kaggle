{"cell_type":{"950f786b":"code","4801a6c3":"code","8ca9b635":"code","cc0673da":"code","a8d21db1":"code","158584e7":"code","02d7375c":"code","ca67ff5a":"code","5226482c":"code","0f1090d0":"code","af174138":"code","b3a24787":"code","845763ca":"code","fb24df4d":"code","c01310b9":"code","f4ef419c":"code","d3fdb738":"code","3d6d04be":"code","7ee6a4fe":"code","fe0b388d":"code","720bc4a9":"code","1025c289":"code","f3243f73":"code","3e584114":"code","66ec6a10":"code","d0e10c2a":"code","7217ae97":"code","01891ed8":"code","1871d74c":"code","f72d97e4":"code","e194340f":"code","0f67f1d0":"code","83bdcf87":"code","0ea47708":"code","468c6a4d":"code","006b5de5":"code","1fa7a4ef":"code","116c4974":"code","836771f4":"code","883441b1":"code","27b473e5":"code","d8a5bcda":"code","179dd020":"code","2da6902d":"code","dcea2565":"code","bee91994":"code","c4e9c265":"code","3fac170e":"code","5aa2727a":"code","59d19dd6":"code","432b16b4":"code","ef5401a3":"code","bd34eb2a":"code","821312ab":"code","76d03bb8":"code","77cf2b7f":"code","5376e758":"code","7b6313ba":"code","22c4dd71":"code","20e2eee4":"code","3c43a1ff":"code","b3c617a0":"code","0150f23f":"code","1338c522":"code","6771cee5":"code","c0d62a51":"code","f083daf7":"code","852af88f":"code","de294656":"code","3611c328":"code","433018d7":"code","d7f7f986":"code","dfc2983f":"code","4ce089a4":"code","50566897":"code","b919781b":"code","f2edbe09":"code","0dca9574":"code","23466655":"code","c70062b5":"code","57275862":"code","a44e64cd":"code","a4f3fba8":"code","264ff263":"code","50969eaf":"code","d93405e5":"code","cc6e443a":"code","2375ed71":"code","7886fa07":"code","2c8ef9a3":"code","52ecd648":"code","661f476b":"code","ba8f4a2c":"code","53e6db8e":"code","4fdd1524":"code","3699d446":"code","ab94267c":"code","e59c7b72":"code","7f1a6725":"code","757b1d06":"code","be5d8071":"code","9fabaa2d":"code","933c7bc1":"code","bee9799d":"code","e52070d8":"code","4ac664fd":"code","42296e02":"code","86117b7a":"code","8af58aee":"code","dad5b10d":"code","118f9d73":"code","000bed2e":"code","70c1739c":"code","86a18f07":"code","92723c67":"code","94fae377":"code","8f25eab6":"code","74a5abbd":"code","346cc266":"code","1132476e":"code","d95b7059":"code","42763e4e":"code","c783aa76":"code","e14861e2":"code","67630b6a":"code","7448a987":"code","be935861":"code","446f37b9":"code","d9e08ed4":"code","eaffcfef":"code","89ddc73f":"markdown","c3a2495c":"markdown","d5a21ffc":"markdown","60de8c05":"markdown","4927b729":"markdown","184a33b0":"markdown","4039e8d5":"markdown","5d080328":"markdown","685acf06":"markdown","23dc08a0":"markdown","5ff04643":"markdown","32125d9a":"markdown","3f5f9925":"markdown","46c5c07c":"markdown","c3c73831":"markdown","7567c73f":"markdown","87b342e6":"markdown","3fb32b4b":"markdown","cd592493":"markdown","9ddc827a":"markdown","59e7020c":"markdown","8a0657ef":"markdown","8c2d5bd9":"markdown","8cf80876":"markdown","d24e9dd0":"markdown","1157a38a":"markdown","e8f44897":"markdown","a377bdf8":"markdown","d1c833d9":"markdown","28ea3d2e":"markdown","f5e57b75":"markdown","f4896e1d":"markdown","f559d635":"markdown","a672f070":"markdown","2d44a936":"markdown","78f28135":"markdown","2b87c340":"markdown","d1cbd91b":"markdown","71170e3f":"markdown","eb1cd7af":"markdown","6d70d67e":"markdown","b72cd4b0":"markdown","7a67baf1":"markdown","eb150749":"markdown","6dc23e20":"markdown","3a345957":"markdown","580e6901":"markdown","4d48abb4":"markdown","8a946584":"markdown","15cdf3ad":"markdown","50dac56a":"markdown","f8f3fa15":"markdown","0e561af5":"markdown","4b787757":"markdown","f024eac5":"markdown","9ea89dbf":"markdown","df65d484":"markdown","4cfd97db":"markdown","49c8f64f":"markdown","f0a89953":"markdown","1d59d90a":"markdown","08db02c8":"markdown","22685c4b":"markdown","c858c18e":"markdown","9856a970":"markdown","552f6098":"markdown","ee46c6ab":"markdown","db647765":"markdown","e32e046b":"markdown","4b269ea2":"markdown","e9543c56":"markdown","93224bf7":"markdown","94a0d0ea":"markdown","66890d4f":"markdown","5574a34e":"markdown","26f4dfed":"markdown","6e215084":"markdown","9a333196":"markdown","c54cab89":"markdown","c4a5cc8b":"markdown","a03a3def":"markdown","6ab4fd65":"markdown","7ff19f25":"markdown","53d26924":"markdown","a5e37090":"markdown","d04fc48f":"markdown","4bec75ae":"markdown","377efa1e":"markdown","5e0c868d":"markdown","9bfbbfc6":"markdown","c017c1bb":"markdown","3339ff4a":"markdown","cba2f9a0":"markdown","3a5544d8":"markdown","b5bf9ed0":"markdown","01207a6c":"markdown","f4de19fd":"markdown","21bc2352":"markdown","fa14f916":"markdown","81b2733d":"markdown","8670858f":"markdown","51af89b1":"markdown","596ad423":"markdown","67e555ad":"markdown","b410e7d8":"markdown","5b1ad390":"markdown","4fad2cbb":"markdown"},"source":{"950f786b":"import tensorflow as tf\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\nimport os\nimport string\nimport itertools\nimport re\n\nfrom wordcloud import WordCloud\n\nimport nltk \nnltk.download('stopwords')\nfrom nltk.stem import PorterStemmer\nfrom nltk.corpus import stopwords \n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\n\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras.layers import Embedding\nfrom tensorflow.keras.layers import SpatialDropout1D\nfrom tensorflow.keras.layers import RNN\nfrom tensorflow.keras.layers import LSTM\nfrom tensorflow.keras.layers import Bidirectional\nfrom tensorflow.keras.layers import GRU\nfrom tensorflow.keras.layers import Dropout\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.regularizers import l2\nfrom tensorflow.keras.layers import Activation\nfrom tensorflow.keras.losses import Hinge\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.metrics import Accuracy\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n\n%matplotlib inline","4801a6c3":"for dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","8ca9b635":"gpus = tf.config.experimental.list_physical_devices('GPU')\nprint(\"Num GPUs Available: \", len(gpus))","cc0673da":"tf.config.experimental.set_memory_growth(gpus[0], enable=True)\nprint(\"Tensorflow Version\",tf.__version__)","a8d21db1":"data = pd.read_csv('..\/input\/sentiment140\/training.1600000.processed.noemoticon.csv',encoding='latin', header=None)\ndata.head(5)","158584e7":"data.columns = ['sentiment', 'id', 'date', 'query', 'user_id', 'text']\ndata.head(5)","02d7375c":"list(data.columns.values)","ca67ff5a":"data.shape","5226482c":"data.count()","0f1090d0":"data.dtypes","af174138":"sentiment_counts = data.sentiment.value_counts()\nprint(sentiment_counts)","b3a24787":"text_counts = data.text.value_counts()\nprint(text_counts)","845763ca":"data.describe()","fb24df4d":"data['sentiment'].value_counts().plot(kind='bar')\n\n#plt.savefig('img\/data_set_1_sentiment_bar_chart.png', format='png')","c01310b9":"data['sentiment'].value_counts().plot(kind='pie')\n\n#plt.savefig('img\/data_set_1_sentiment_pie_chart.png', format='png')","f4ef419c":"data = data.drop(['id', 'date', 'query', 'user_id'], axis=1)\n\nencoded_data = data","d3fdb738":"labels_to_sentiment = {0:\"Negative\", 4:\"Positive\"}\ndef label_decoder(label):\n    return labels_to_sentiment[label]\n    encoded_data.sentiment = encoded_data.sentiment.apply(lambda x: label_decoder(x))\nencoded_data.head(5)","3d6d04be":"val_count = encoded_data.sentiment.value_counts()\n\nplt.figure(figsize=(8,4))\nplt.bar(val_count.index, val_count.values)\nplt.title(\"1.6 Million Twitter Tweets Sentiment Data Distribution\")\n\n#plt.savefig('img\/data_set_1_distribution_bar_chart.png', format='png')","7ee6a4fe":"reduced_data = encoded_data\nreduced_data.head(5)","fe0b388d":"data_positive = reduced_data[reduced_data['sentiment'] == 4]\ndata_negative = reduced_data[reduced_data['sentiment'] == 0]\nprint(len(data_positive), len(data_negative))","720bc4a9":"data_positive = data_positive.iloc[:int(len(data_positive)\/80)]\ndata_negative = data_negative.iloc[:int(len(data_negative)\/80)]\nprint(len(data_positive), len(data_negative))","1025c289":"reduced_data = pd.concat([data_positive, data_negative])\nlen(reduced_data)","f3243f73":"cleaned_data = reduced_data\ncleaned_data.head(5)","3e584114":"plt.figure(figsize = (20,20)) \nPositive_WC = WordCloud(max_words = 1000 , width = 1000 , height = 500).generate(\" \".join(cleaned_data[cleaned_data.sentiment == 4].text))\nplt.imshow(Positive_WC , interpolation = 'bilinear')\n\n#plt.savefig('img\/data_set_1_positive_words.png', format='png')","66ec6a10":"plt.figure(figsize = (20,20)) \nNegative_WC = WordCloud(max_words = 1000 , width = 1000 , height = 500).generate(\" \".join(cleaned_data[cleaned_data.sentiment == 0].text))\nplt.imshow(Negative_WC , interpolation = 'bilinear')\n\n#plt.savefig('img\/data_set_1_negative_words.png', format='png')","d0e10c2a":"def remove_URL(text):\n    url = re.compile(r\"https?:\/\/\\S+|www\\.\\S+\")\n    return url.sub(r\"\", text)\n\ndef remove_html(text):\n    html = re.compile(r\"<.*?>\")\n    return html.sub(r\"\", text)","7217ae97":"text_cleaning_re = \"@\\S+|https?:\\S+|http?:\\S|[^A-Za-z0-9]+\"    \n\ndef text_cleaning(text):\n    text = re.sub(text_cleaning_re, ' ', str(text).lower()).strip()\n    return text","01891ed8":"def number_cleaning(text):\n    text = ''.join(c for c in text if not c.isdigit())\n    return text","1871d74c":"def remove_emoji(string):\n    emoji_pattern = re.compile(\n    \"[\"\n        u\"\\U0001F600-\\U0001F64F\" #emoticons\n        u\"\\U0001F300-\\U0001F5FF\" #symbols & pictographs\n        u\"\\U0001F680-\\U0001F6FF\" #transport & map symbols\n        u\"\\U0001F1E0-\\U0001F1FF\" #FLAGS on (iOS)\n        u\"\\U00002702-\\U000027B0\"\n        u\"\\U000024C2-\\U0001F251\"\n        \"]+\",\n        flags=re.UNICODE,\n    )\n    return emoji_pattern.sub(r\"\", string)","f72d97e4":"def remove_punctuation(text):\n    table = str.maketrans(\"\", \"\", string.punctuation)\n    return text.translate(table)","e194340f":"def stemming_words(text):\n    ps=nltk.porter.PorterStemmer()\n    text= ' '.join([ps.stem(word) for word in text.split()])\n    return text","0f67f1d0":"stop = set(stopwords.words(\"english\"))\n\ndef remove_stopwords(text):\n    text = [word.lower() for word in text.split() if word.lower() not in stop]\n    \n    return \" \".join(text)","83bdcf87":"cleaned_data[\"text\"] = cleaned_data.text.apply(lambda x: remove_URL(x))\ncleaned_data[\"text\"] = cleaned_data.text.apply(lambda x: remove_html(x))\ncleaned_data[\"text\"] = cleaned_data.text.apply(lambda x: text_cleaning(x))\ncleaned_data[\"text\"] = cleaned_data.text.apply(lambda x: number_cleaning(x))\ncleaned_data[\"text\"] = cleaned_data.text.apply(lambda x: remove_emoji(x))\ncleaned_data[\"text\"] = cleaned_data.text.apply(lambda x: remove_punctuation(x))\ncleaned_data[\"text\"] = cleaned_data.text.apply(lambda x: stemming_words(x))\ncleaned_data[\"text\"] = cleaned_data[\"text\"].apply(remove_stopwords)","0ea47708":"cleaned_data.head(5)","468c6a4d":"cleaned_data.text","006b5de5":"train_size = 0.8","1fa7a4ef":"train_data, test_data = train_test_split(cleaned_data, test_size=1-train_size, random_state=7) \n\nprint(\"Train data size:\", len(train_data))\nprint(\"Test data size\", len(test_data))","116c4974":"train_data.head(10)","836771f4":"vocabulary_size = 100000\nmax_word_length = 280\noov_tk = \"<OOV>\"\ntrunc_type='post'\npadding_type='post'","883441b1":"tokenizer = Tokenizer(num_words = vocabulary_size, oov_token = oov_tk)\ntokenizer.fit_on_texts(train_data.text)\n\nword_index = tokenizer.word_index\nvocab_size = len(tokenizer.word_index) + 1\n\nprint(\"Vocabulary size :\", vocab_size)","27b473e5":"train_sequences = tokenizer.texts_to_sequences(train_data.text) \ntest_sequences = tokenizer.texts_to_sequences(test_data.text)","d8a5bcda":"x_train = pad_sequences(train_sequences, maxlen=max_word_length, padding=padding_type, truncating=trunc_type)\nx_test = pad_sequences(test_sequences, maxlen=max_word_length, padding=padding_type, truncating=trunc_type)\n\nprint(\"Training X Shape:\",x_train.shape)\nprint(\"Testing X Shape:\",x_test.shape)","179dd020":"labels = train_data.sentiment.unique().tolist()","2da6902d":"encoder = LabelEncoder()\nencoder.fit(train_data.sentiment.to_list())\n\ny_train = encoder.transform(train_data.sentiment.to_list())\ny_test = encoder.transform(test_data.sentiment.to_list())\n\ny_train = y_train.reshape(-1,1)\ny_test = y_test.reshape(-1,1)\n\nprint(\"y_train shape:\", y_train.shape)\nprint(\"y_test shape:\", y_test.shape)","dcea2565":"vocabulary_size =len(word_index)\nprint('The size of the sentiment dataset vocabulary is: ', vocabulary_size)","bee91994":"training_sequence = x_train.shape[1]\nprint('The length of training sequence is: ', training_sequence)","c4e9c265":"print('The length of testing sequence is: ', x_test.shape[1])","3fac170e":"embeddings_index = {}\n\nwith open('..\/input\/glove6b\/glove.6B.300d.txt', encoding='utf-8') as f:\n    for line in f:\n        values = line.split()\n        word = values[0]\n        coefs = np.asarray(values[1:], dtype='float32')\n        embeddings_index[word] = coefs\n    f.close()\n\nprint('Found %s word vectors.' % len(embeddings_index))","5aa2727a":"word_index = tokenizer.word_index\nprint('Found %s unique tokens.' % len(word_index))","59d19dd6":"embeddings_index","432b16b4":"num_words = len(word_index) + 1\nembedding_dimension = 300\n\nembedding_matrix = np.zeros((vocab_size, embedding_dimension))\n\nfor word, i in word_index.items():\n    if i < num_words:\n        embedding_vector = embeddings_index.get(word)\n        if embedding_vector is not None:\n            embedding_matrix[i] = embedding_vector","ef5401a3":"embedding_matrix","bd34eb2a":"word_index[\"good\"]","821312ab":"embeddings_index.get(\"good\")","76d03bb8":"(embedding_matrix[7] == embeddings_index.get(\"good\")).all()","77cf2b7f":"print(x_train.shape)\nprint(y_train.shape)","5376e758":"print(x_test.shape)\nprint(y_test.shape)","7b6313ba":"print(\"Training on GPU...\") if tf.test.is_gpu_available() else print(\"Training on CPU...\")","22c4dd71":"LSTM_SVM_model = tf.keras.Sequential([\n\n    tf.keras.layers.Embedding(vocab_size, embedding_dimension, \n                            weights=[embedding_matrix], \n                            input_length=max_word_length, \n                            trainable=False),\n    \n    tf.keras.layers.SpatialDropout1D(0.40),\n    tf.keras.layers.LSTM((128), return_sequences=True),\n    tf.keras.layers.Dropout(0.20),\n    tf.keras.layers.LSTM((128)),\n    tf.keras.layers.Dropout(0.30),\n    tf.keras.layers.Dense(200, activation='relu'),\n    tf.keras.layers.Dense(1, kernel_regularizer=l2(0.01)),\n    tf.keras.layers.Activation('linear')\n])","20e2eee4":"LSTM_SVM_model.compile(loss=tf.keras.losses.Hinge(),\n              optimizer=tf.keras.optimizers.Adam(1e-4),\n              metrics=['accuracy'])","3c43a1ff":"print(LSTM_SVM_model.summary())","b3c617a0":"number_of_epochs = 10\nbatch_size = 256\n\nLSTM_SVM_history = LSTM_SVM_model.fit(x_train, y_train, \n                                    epochs = number_of_epochs,\n                                    batch_size = batch_size,\n                   validation_data=(x_test, y_test), \n                                    verbose=1)","0150f23f":"plt.plot(LSTM_SVM_history.history['accuracy'])\nplt.plot(LSTM_SVM_history.history['val_accuracy'])\nplt.title('LSTM-SVM Model Accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Validation'], loc='upper left')\n#plt.savefig('img\/data_set_1_LSTM_SVM_history_model_accuracy.png', format='png')\nplt.show()\n\n\nplt.plot(LSTM_SVM_history.history['loss'])\nplt.plot(LSTM_SVM_history.history['val_loss'])\nplt.title('LSTM-SVM Model Loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Validation'], loc='upper left')\n#plt.savefig('img\/data_set_1_LSTM_SVM_model_loss.png', format='png')\nplt.show()","1338c522":"#LSTM_SVM_model.save('saved_model\/LSTM_SVM_Model')","6771cee5":"#LSTM_SVM_saved_model = tf.keras.models.load_model('saved_model\/LSTM_SVM_Model')","c0d62a51":"BILSTM_SVM_model = tf.keras.Sequential([\n\n    tf.keras.layers.Embedding(vocab_size, embedding_dimension, \n                            weights=[embedding_matrix], \n                            input_length=max_word_length, \n                            trainable=False),\n    \n    tf.keras.layers.SpatialDropout1D(0.40),\n    tf.keras.layers.Bidirectional(tf.keras.layers.RNN(tf.keras.layers.LSTMCell(128), return_sequences=True)),\n    tf.keras.layers.Dropout(0.20),\n    tf.keras.layers.Bidirectional(tf.keras.layers.RNN(tf.keras.layers.LSTMCell(128))),\n    tf.keras.layers.Dropout(0.30),\n    tf.keras.layers.Dense(200, activation='relu'),\n    tf.keras.layers.Dense(1, kernel_regularizer=l2(0.01)),\n    tf.keras.layers.Activation('linear')\n])","f083daf7":"BILSTM_SVM_model.compile(loss=tf.keras.losses.Hinge(),\n              optimizer=tf.keras.optimizers.Adam(1e-4),\n              metrics=['accuracy'])","852af88f":"print(BILSTM_SVM_model.summary())","de294656":"number_of_epochs = 10\nbatch_size = 256\n\nBILSTM_SVM_history = BILSTM_SVM_model.fit(x_train, y_train, \n                                    epochs = number_of_epochs,\n                                    batch_size = batch_size,\n                   validation_data=(x_test, y_test), \n                                    verbose=1)","3611c328":"plt.plot(BILSTM_SVM_history.history['accuracy'])\nplt.plot(BILSTM_SVM_history.history['val_accuracy'])\nplt.title('BILSTM-SVM Model Accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Validation'], loc='upper left')\n#plt.savefig('img\/data_set_1_BILSTM_SVM_model_accuracy.png', format='png')\nplt.show()\n\n\nplt.plot(BILSTM_SVM_history.history['loss'])\nplt.plot(BILSTM_SVM_history.history['val_loss'])\nplt.title('BILSTM-SVM Model Loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Validation'], loc='upper left')\n#plt.savefig('img\/data_set_1_BILSTM_SVM_model_loss.png', format='png')\nplt.show()","433018d7":"#BILSTM_SVM_model.save('saved_model\/BILSTM_SVM_Model')","d7f7f986":"#BILSTM_SVM_saved_model = tf.keras.models.load_model('saved_model\/BILSTM_SVM_Model')","dfc2983f":"GRU_SVM_model = tf.keras.Sequential([\n\n    tf.keras.layers.Embedding(vocab_size, embedding_dimension, \n                            weights=[embedding_matrix], \n                            input_length=max_word_length, \n                            trainable=False),\n    \n    tf.keras.layers.SpatialDropout1D(0.40),\n    tf.keras.layers.GRU((128), return_sequences=True),\n    tf.keras.layers.Dropout(0.20),\n    tf.keras.layers.GRU((128)),\n    tf.keras.layers.Dropout(0.30),\n    tf.keras.layers.Dense(200, activation='relu'),\n    tf.keras.layers.Dense(1, kernel_regularizer=l2(0.01)),\n    tf.keras.layers.Activation('linear')\n])","4ce089a4":"GRU_SVM_model.compile(loss=tf.keras.losses.Hinge(),\n              optimizer=tf.keras.optimizers.Adam(1e-4),\n              metrics=['accuracy'])","50566897":"print(GRU_SVM_model.summary())","b919781b":"number_of_epochs = 10\nbatch_size = 256\n\nGRU_SVM_history = GRU_SVM_model.fit(x_train, y_train, \n                                    epochs = number_of_epochs,\n                                    batch_size = batch_size,\n                   validation_data=(x_test, y_test), \n                                    verbose=1)","f2edbe09":"plt.plot(GRU_SVM_history.history['accuracy'])\nplt.plot(GRU_SVM_history.history['val_accuracy'])\nplt.title('GRU-SVM Model Accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Validation'], loc='upper left')\n#plt.savefig('img\/data_set_1_GRU_SVM_model_accuracy.png', format='png')\nplt.show()\n\nplt.plot(GRU_SVM_history.history['loss'])\nplt.plot(GRU_SVM_history.history['val_loss'])\nplt.title('GRU-SVM Model Loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Validation'], loc='upper left')\n#plt.savefig('img\/data_set_1_GRU_SVM_model_loss.png', format='png')\nplt.show()","0dca9574":"#GRU_SVM_model.save('saved_model\/GRU_SVM_Model')","23466655":"#GRU_SVM_saved_model = tf.keras.models.load_model('saved_model\/GRU_SVM_Model')","c70062b5":"BIGRU_SVM_model = tf.keras.Sequential([\n\n    tf.keras.layers.Embedding(vocab_size, embedding_dimension, \n                            weights=[embedding_matrix], \n                            input_length=max_word_length, \n                            trainable=False),\n    \n    tf.keras.layers.SpatialDropout1D(0.40),\n    tf.keras.layers.Bidirectional(tf.keras.layers.RNN(tf.keras.layers.GRUCell(128), return_sequences=True)),\n    tf.keras.layers.Dropout(0.20),\n    tf.keras.layers.Bidirectional(tf.keras.layers.RNN(tf.keras.layers.GRUCell(128))),\n    tf.keras.layers.Dropout(0.30),\n    tf.keras.layers.Dense(200, activation='relu'),\n    tf.keras.layers.Dense(1, kernel_regularizer=l2(0.01)),\n    tf.keras.layers.Activation('linear')\n])","57275862":"BIGRU_SVM_model.compile(loss=tf.keras.losses.Hinge(),\n              optimizer=tf.keras.optimizers.Adam(1e-4),\n              metrics=['accuracy'])","a44e64cd":"print(BIGRU_SVM_model.summary())","a4f3fba8":"number_of_epochs = 10\nbatch_size = 256\n\nBIGRU_SVM_history = BIGRU_SVM_model.fit(x_train, y_train, \n                                    epochs = number_of_epochs,\n                                    batch_size = batch_size,\n                   validation_data=(x_test, y_test), \n                                    verbose=1)","264ff263":"plt.plot(BIGRU_SVM_history.history['accuracy'])\nplt.plot(BIGRU_SVM_history.history['val_accuracy'])\nplt.title('BIGRU-SVM Model Accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Validation'], loc='upper left')\n#plt.savefig('img\/data_set_1_BIGRU_SVM_model_accuracy.png', format='png')\nplt.show()\n\n\nplt.plot(BIGRU_SVM_history.history['loss'])\nplt.plot(BIGRU_SVM_history.history['val_loss'])\nplt.title('BIGRU-SVM Model Loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Validation'], loc='upper left')\n#plt.savefig('img\/data_set_1_BIGRU_SVM_model_loss.png', format='png')\nplt.show()","50969eaf":"#BIGRU_SVM_model.save('saved_model\/BIGRU_SVM_Model')","d93405e5":"#BIGRU_SVM_saved_model = tf.keras.models.load_model('saved_model\/BIGRU_SVM_Model')","cc6e443a":"test_loss, test_acc = LSTM_SVM_model.evaluate(x_test, y_test)\n\nprint('Test Loss: {}'.format(test_loss))\nprint('Test Accuracy: {}'.format(test_acc))","2375ed71":"LSTM_SVM_scores = LSTM_SVM_model.predict(x_test, verbose=1, batch_size=256)","7886fa07":"LSTM_SVM_scores","2c8ef9a3":"LSTM_SVM_y_pred=np.where(LSTM_SVM_scores>0.5,1,0)","52ecd648":"LSTM_SVM_y_pred","661f476b":"print(accuracy_score(y_test, LSTM_SVM_y_pred))","ba8f4a2c":"print(classification_report(y_test, LSTM_SVM_y_pred))","53e6db8e":"LSTM_SVM_CM=confusion_matrix(y_test, LSTM_SVM_y_pred)\nprint(LSTM_SVM_CM)","4fdd1524":"def plot_confusion_matrix(cm, classes,\n                        normalize=False,\n                        title='Confusion matrix',\n                        cmap=plt.cm.Blues):\n    \n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalised confusion matrix\")\n    else:\n        print('Confusion matrix, without normalisation')\n\n    print(cm)\n\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, cm[i, j],\n            horizontalalignment=\"center\",\n            color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')","3699d446":"categories = ['Negative','Positive']\nLSTM_SVM_cnf_matrix = confusion_matrix(y_test, LSTM_SVM_y_pred)\nplot_confusion_matrix(cm=LSTM_SVM_cnf_matrix, classes=categories, title='LSTM-SVM Confusion Matrix')\n#plt.savefig('img\/data_set_1_LSTM_SVM_CM.png', format='png')\nplt.show()","ab94267c":"test_loss, test_acc = BILSTM_SVM_model.evaluate(x_test, y_test)\n\nprint('Test Loss: {}'.format(test_loss))\nprint('Test Accuracy: {}'.format(test_acc))","e59c7b72":"BILSTM_SVM_scores = BILSTM_SVM_model.predict(x_test, verbose=1, batch_size=256)","7f1a6725":"BILSTM_SVM_scores","757b1d06":"BILSTM_SVM_y_pred=np.where(BILSTM_SVM_scores>0.5,1,0)","be5d8071":"BILSTM_SVM_y_pred","9fabaa2d":"print(accuracy_score(y_test, BILSTM_SVM_y_pred))","933c7bc1":"print(classification_report(y_test, BILSTM_SVM_y_pred))","bee9799d":"BILSTM_SVM_CM=confusion_matrix(y_test, BILSTM_SVM_y_pred)\nprint(BILSTM_SVM_CM)","e52070d8":"def plot_confusion_matrix(cm, classes,\n                        normalize=False,\n                        title='Confusion matrix',\n                        cmap=plt.cm.Blues):\n    \n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalised confusion matrix\")\n    else:\n        print('Confusion matrix, without normalisation')\n\n    print(cm)\n\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, cm[i, j],\n            horizontalalignment=\"center\",\n            color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')","4ac664fd":"categories = ['Negative','Positive']\nBILSTM_SVM_cnf_matrix = confusion_matrix(y_test, BILSTM_SVM_y_pred)\nplot_confusion_matrix(cm=BILSTM_SVM_cnf_matrix, classes=categories, title='BILSTM-SVM Confusion Matrix')\n#plt.savefig('img\/data_set_1_BILSTM_SVM_CM.png', format='png')\nplt.show()","42296e02":"test_loss, test_acc = GRU_SVM_model.evaluate(x_test, y_test)\n\nprint('Test Loss: {}'.format(test_loss))\nprint('Test Accuracy: {}'.format(test_acc))","86117b7a":"GRU_SVM_scores = GRU_SVM_model.predict(x_test, verbose=1, batch_size=256)","8af58aee":"GRU_SVM_scores","dad5b10d":"GRU_SVM_y_pred=np.where(GRU_SVM_scores>0.5,1,0)","118f9d73":"GRU_SVM_y_pred","000bed2e":"print(accuracy_score(y_test, GRU_SVM_y_pred))","70c1739c":"print(classification_report(y_test, GRU_SVM_y_pred))","86a18f07":"GRU_SVM_CM=confusion_matrix(y_test, GRU_SVM_y_pred)\nprint(GRU_SVM_CM)","92723c67":"def plot_confusion_matrix(cm, classes,\n                        normalize=False,\n                        title='Confusion matrix',\n                        cmap=plt.cm.Blues):\n    \n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalised confusion matrix\")\n    else:\n        print('Confusion matrix, without normalisation')\n\n    print(cm)\n\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, cm[i, j],\n            horizontalalignment=\"center\",\n            color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')","94fae377":"categories = ['Negative','Positive']\nGRU_SVM_cnf_matrix = confusion_matrix(y_test, GRU_SVM_y_pred)\nplot_confusion_matrix(cm=GRU_SVM_cnf_matrix, classes=categories, title='GRU-SVM Confusion Matrix')\n#plt.savefig('img\/data_set_1_GRU_SVM_CM.png', format='png')\nplt.show()","8f25eab6":"test_loss, test_acc = BIGRU_SVM_model.evaluate(x_test, y_test)\n\nprint('Test Loss: {}'.format(test_loss))\nprint('Test Accuracy: {}'.format(test_acc))","74a5abbd":"BIGRU_SVM_scores = BIGRU_SVM_model.predict(x_test, verbose=1, batch_size=256)","346cc266":"BIGRU_SVM_scores","1132476e":"BIGRU_SVM_y_pred=np.where(BIGRU_SVM_scores>0.5,1,0)","d95b7059":"BIGRU_SVM_y_pred","42763e4e":"print(accuracy_score(y_test, BIGRU_SVM_y_pred))","c783aa76":"print(classification_report(y_test, BIGRU_SVM_y_pred))","e14861e2":"BIGRU_SVM_CM=confusion_matrix(y_test, BIGRU_SVM_y_pred)\nprint(BIGRU_SVM_CM)","67630b6a":"def plot_confusion_matrix(cm, classes,\n                        normalize=False,\n                        title='Confusion matrix',\n                        cmap=plt.cm.Blues):\n    \n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalised confusion matrix\")\n    else:\n        print('Confusion matrix, without normalisation')\n\n    print(cm)\n\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, cm[i, j],\n            horizontalalignment=\"center\",\n            color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')","7448a987":"categories = ['Negative','Positive']\nBIGRU_SVM_cnf_matrix = confusion_matrix(y_test, BIGRU_SVM_y_pred)\nplot_confusion_matrix(cm=BIGRU_SVM_cnf_matrix, classes=categories, title='BIGRU-SVM Confusion Matrix')\n#plt.savefig('img\/data_set_1_BIGRU_SVM_CM.png', format='png')\nplt.show()","be935861":"test_sentence1 = [ \"The flight was amazing #POG. I will fly again because it was exquisite and the bathtub was ok :)\", \n                  \"This REEESUBHUMAN political party are absolutely horrible, racist, dumb and rude >:(\", \n                  \"Hope they get nuked. such an uncvis.ised uvu twats booooooo you stink lol XD\", \n                  \"This game called Destiny it's alright, it provides decent loot and ok rewards. I will play it again with my friends\", \n                  \"Bruh, Imma SSJ3 kamehameha those dementors from Harry Potter because they are scary. \"]\n\n\n#Converting the sentences to sequences using a tokenizer\ntest_sequences1 = tokenizer.texts_to_sequences(test_sentence1)\n\n#Padding the new sequences to make them have the same dimension\ntest_padded1 = pad_sequences(test_sequences1, maxlen = max_word_length, \n                             padding = padding_type, truncating = trunc_type)\n\n#Testing out the new padded data on a trained LSTM-SVM model\ntest_padded1 = np.array(test_padded1)\nprint(LSTM_SVM_model.predict(test_padded1))","446f37b9":"test_sentence2 = [ \"The flight was amazing #POG. I will fly again because it was exquisite and the bathtub was ok :)\", \n                  \"This REEESUBHUMAN political party are absolutely horrible, racist, dumb and rude >:(\", \n                  \"Hope they get nuked. such an uncvis.ised uvu twats booooooo you stink lol XD\", \n                  \"This game called Destiny it's alright, it provides decent loot and ok rewards. I will play it again with my friends\", \n                  \"Bruh, Imma SSJ3 kamehameha those dementors from Harry Potter because they are scary. \"]\n\n\n#Converting the sentences to sequences using tokenizer\ntest_sequences2 = tokenizer.texts_to_sequences(test_sentence2)\n\n#Padding the new sequences to make them have same dimensions\ntest_padded2 = pad_sequences(test_sequences2, maxlen = max_word_length, \n                             padding = padding_type, truncating = trunc_type)\n\n#Testing out the new padded data on a trained Bidirectional LSTM-SVM model\ntest_padded2 = np.array(test_padded2)\nprint(BILSTM_SVM_model.predict(test_padded2))","d9e08ed4":"test_sentence3 = [ \"The flight was amazing #POG. I will fly again because it was exquisite and the bathtub was ok :)\", \n                  \"This REEESUBHUMAN political party are absolutely horrible, racist, dumb and rude >:(\", \n                  \"Hope they get nuked. such an uncvis.ised uvu twats booooooo you stink lol XD\", \n                  \"This game called Destiny it's alright, it provides decent loot and ok rewards. I will play it again with my friends\", \n                  \"Bruh, Imma SSJ3 kamehameha those dementors from Harry Potter because they are scary.\"]\n\n\n# Converting the sentences to sequences using tokenizer\ntest_sequences3 = tokenizer.texts_to_sequences(test_sentence3)\n\n# padding the new sequences to make them have same dimensions\ntest_padded3 = pad_sequences(test_sequences3, maxlen = max_word_length, \n                             padding = padding_type, truncating = trunc_type)\n\n#Testing out the new padded data on a trained GRU-SVM model\ntest_padded3 = np.array(test_padded3)\nprint(GRU_SVM_model.predict(test_padded3))","eaffcfef":"test_sentence4 = [ \"The flight was amazing #POG. I will fly again because it was exquisite and the bathtub was ok :)\", \n                  \"This REEESUBHUMAN political party are absolutely horrible, racist, dumb and rude >:(\", \n                  \"Hope they get nuked. such an uncvis.ised uvu twats booooooo you stink lol XD\", \n                  \"This game called Destiny it's alright, it provides decent loot and ok rewards. I will play it again with my friends\", \n                  \"Bruh, Imma SSJ3 kamehameha those dementors from Harry Potter because they are scary.\"]\n\n\n# Converting the sentences to sequences using tokenizer\ntest_sequences4 = tokenizer.texts_to_sequences(test_sentence4)\n\n# padding the new sequences to make them have same dimensions\ntest_padded4 = pad_sequences(test_sequences4, maxlen = max_word_length, \n                             padding = padding_type, truncating = trunc_type)\n\n#Testing out the new padded data on a trained BIGRU-SVM model\ntest_padded4 = np.array(test_padded4)\nprint(BIGRU_SVM_model.predict(test_padded4))","89ddc73f":"**Test accuracy and loss**","c3a2495c":"**Saving the GRU-SVM model**","d5a21ffc":"# Bidirectional LSTM-SVM Testing Class","60de8c05":"# Training the Bidirectional GRU-SVM Model","4927b729":"**Importing the necessary libraries**","184a33b0":"**Reducing the data set**\n\n* Reasons: Hardware GPU power limitation and creating an equal data set to compare it against another product prototype.","4039e8d5":"Important Note: The Losses **Hinge** function converts the encoded labels from 0 to 1 which are negative and positive to -1 and 1.","5d080328":"**Testing the hybrid model against the testing data set**","685acf06":"**Unique Values**","23dc08a0":"**Hyper parameters**\n\nDescription:\n* 100,000 will be the vocabulary range.\n* 280 words was chosen as the max length because this is currently the maximum number of characters you can input on Twitter as of May 2021.","5ff04643":"**Tokenising the training data set**\n\nDescription:\n* OOV means out of vocabulary. The tokenizer will create a token for each word and replace each word it does not recognise with the 'OOV' token instead.\n* num_words stores the maximum number of vocabulary words.\n* word_index is a dictionary containing a key for a word and a value which is a token linked specifically to a word.","32125d9a":"**Training the GRU-SVM model**","3f5f9925":"Separating the positive and negative rows","46c5c07c":"**Saving the Bidirectional GRU-SVM model**","c3c73831":"# Stage 4: Tokenisation","7567c73f":"**Plotting the Bidirectional GRU-SVM train graph**","87b342e6":"# Stage 1: Load the Data Set","3fb32b4b":"**Classification report (Precision, Recall, F1-Score and Accuracy)**","cd592493":"**Creating the Bidirectional LSTM-SVM Model**","9ddc827a":"Removing emojis","59e7020c":"**Testing the LSTM-SVM model by using a basic test function**","8a0657ef":"# Training the LSTM-SVM Model","8c2d5bd9":"**Training the Bidirectional LSTM-SVM model**","8cf80876":"**Test accuracy and loss**","d24e9dd0":"**Assigning label numbers with positive and negative traits**","1157a38a":"**Using the TensorFlow-GPU version instead of the CPU version**","e8f44897":"Positive words","a377bdf8":"**Confusion Matrix**","d1c833d9":"Checking the distribution of the data before the reduction phase","28ea3d2e":"# LSTM-SVM Accuracy Analysis","f5e57b75":"**Viewing the cleaned data**","f4896e1d":"**Classification report (Precision, Recall, F1-Score and Accuracy)**","f559d635":"**Confusion Matrix**","a672f070":"**Testing the hybrid model against the testing data set**","2d44a936":"# GRU-SVM Accuracy Analysis","78f28135":"Joining both the data sets back together","2b87c340":"**Training the LSTM-SVM model**","d1cbd91b":"Negative words","71170e3f":"# GRU-SVM Testing Class","eb1cd7af":"**Confusion Matrix**","6d70d67e":"**Checking to see where the training of the model will be done (GPU or CPU)**","b72cd4b0":"**Test accuracy and loss**","7a67baf1":"Visual representation of the data","eb150749":"**Creating the Bidirectional GRU-SVM Model**","6dc23e20":"**Sequences**\n\nDescription:\n* Placing the train and test data sets into sequences. This creates sequences of tokens representing each sentence.","3a345957":"# Stage 6: Pretrained GloVe word embedding","580e6901":"# Bidirectional LSTM-SVM Accuracy Analysis","4d48abb4":"# **Using four recurrent neural network architecture with support vector machine to detect and understand human emotions in text-based data set.**\n\n**RNN(LSTM, GRU, Bidirectional LSTM and Bidirectional GRU)-SVM models**\n\nThis product has 9 stages to detect and analysis positive and negative Twitter tweets. These nine stages are Load the Data Set, Text Preprocessing, Splitting the Data Set, Tokenisation, Label Encoding, Pretrained GloVe word embedding, Training the hybrid models (LSTM-SVM, GRU-SVM, Bi_LSTM-SVM, Bi_GRU-SVM), Accuracy Analysis and Testing Class.","8a946584":"**Saving the LSTM-SVM model**","15cdf3ad":"**Testing out the embedding matrix**","50dac56a":"**Loading the GloVe word embedding 6-billion and 300-dimensional text**","f8f3fa15":"**Analysing the training and testing data sets shapes**","0e561af5":"**Creating the embedding matrix**","4b787757":"# Training the GRU-SVM Model","f024eac5":"The Bidirectional GRU-SVM model is the best model to use to detect human emotion in Twitter text tweets. This model can understand positive, negative, neutral, and sarcastic sentences but fails to understand a scary joke sentence compared to the Bidirectional LSTM-SVM model which is able to detect only neutral and the negative sentences. This is possible because the Bidirectional GRU-SVM model GRU model has 2 gates which are the update and reset gate compared to the Bidirectional LSTM-SVM model LSTM model which has 3 gates and they are the input, forget and output gates. This simplicity is the GRU main benefit combined with the SVM optimal hyperplanes which is designed for the separation of training data without errors and soft margins which allows for an analytic treatment of learning with errors on the training set compared to the LSTM model which are complex with 3 gates.","9ea89dbf":"Removing any leftovers of unnecessary texts","df65d484":"**Testing the Bidirectional LSTM-SVM model by using a basic test function**","4cfd97db":"# Training the Bidirectional LSTM-SVM Model","49c8f64f":"**Testing the hybrid model against the testing data set**","f0a89953":"**Plotting the LSTM-SVM train graph**","1d59d90a":"**Accuracy**","08db02c8":"**Encoding the sentiment labels**","22685c4b":"Splitting the data set into 80% training and 20% testing set then randomise it for balance purpose","c858c18e":"# Stage 9: Testing Class","9856a970":"**Classification report (Precision, Recall, F1-Score and Accuracy)**","552f6098":"# Conclusion","ee46c6ab":"**Plotting the GRU-SVM train graph**","db647765":"# Bidirectional GRU-SVM Testing Class","e32e046b":"Dividing the data set into an equal amount","4b269ea2":"# Stage 3: Splitting the Data Set","e9543c56":"# Stage 2: Text Preprocessing","93224bf7":"**Testing the hybrid model against the testing data set**","94a0d0ea":"**Accuracy**","66890d4f":"**Loading the Bidirectional LSTM-SVM model**","5574a34e":"**Accuracy**","26f4dfed":"**Saving the Bidirectional LSTM-SVM model**","6e215084":"**Data set Examination**","9a333196":"**Applying text preprocessing**","c54cab89":"Removing integers","c4a5cc8b":"**Loading the GRU-SVM model**","a03a3def":"# Stage 7: Training the four RNN-SVM Models","6ab4fd65":"Removing stopwords","7ff19f25":"# Stage 8: Accuracy Analysis","53d26924":"**Testing the GRU-SVM model by using a basic test function**","a5e37090":"**Plotting the Bidirectional LSTM-SVM train graph**","d04fc48f":"**Loading the LSTM-SVM model**","4bec75ae":"**Displaying the most used words in the positive and negative sentiment labels**","377efa1e":"# Stage 5: Label Encoding","5e0c868d":"**Activating the Kaggle OS path for data set retrieval**","9bfbbfc6":"**Testing the Bidirectional GRU-SVM model by using a basic test function**","c017c1bb":"# LSTM-SVM Testing Class","3339ff4a":"**Creating the GRU-SVM Model**","cba2f9a0":"**Text and sentiment columns are kept while the rest are dropped**","3a5544d8":"**Test accuracy and loss**","b5bf9ed0":"Removing punctuation","01207a6c":"**Classification report (Precision, Recall, F1-Score and Accuracy)**","f4de19fd":"Stemming the text using the Porter version","21bc2352":"Removing URL and HTML","fa14f916":"# Bidirectional GRU-SVM Accuracy Analysis","81b2733d":"Five user created English sentences will be used to evaluate if the hybrid models is able to distinguish between positive and negative text.\n\n* 1st sentence is a positive tweet\n* 2nd sentence is a negative tweet\n* 3rd sentence is a sarcastic tweet\n* 4th sentence is a neutral tweet\n* 5th sentence is a scary joke tweet","8670858f":"**Training the Bidirectional GRU-SVM model**","51af89b1":"**Loading the Bidirectional GRU-SVM model**","596ad423":"**Padding**\n\nDescription:\n*  Padding is used to handle sentences with different lengths.","67e555ad":"**Accuracy**","b410e7d8":"**Viewing important parameters data**","5b1ad390":"**Confusion Matrix**","4fad2cbb":"**Creating the LSTM-SVM Model**"}}