{"cell_type":{"780ff370":"code","dec83220":"code","d2a9a1ae":"code","56d20141":"code","dfa2b389":"code","f9ca9327":"code","f6ff34f5":"code","29564d6e":"code","70be377f":"code","2473163e":"code","3e1fa212":"code","1278b69c":"code","3ad88b4b":"code","1d27016a":"code","b1402518":"code","63b10211":"code","f1e4bf84":"code","1f04c750":"code","0dbe5c28":"code","932cf8aa":"code","4977d90b":"markdown","d7d7f023":"markdown","5a148482":"markdown"},"source":{"780ff370":"# \u0417\u0430\u0433\u0440\u0443\u0437\u0438\u043c \u0434\u0430\u043d\u043d\u044b\u0435 (+\u043f\u0440\u043e\u0432\u0435\u0440\u043a\u0430 \u043d\u0430\u043b\u0438\u0447\u0438\u044f \u0444\u0430\u0439\u043b\u043e\u0432)\nimport os\nimport pandas as pd\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n for filename in filenames: print(os.path.join(dirname, filename))\ndf = pd.read_csv('\/kaggle\/input\/heart-attack-analysis-prediction-dataset\/heart.csv')\ndf.head(3)","dec83220":"# Imports & some toolkit\nimport builtins, time\n# time.strf\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nfrom IPython.display import display\ndef d(*a):\n    for i in a: display(i.head(3) if isinstance(i, (type(df), type(df.age))) else i)\ndef print(*a, **kw): builtins.print(*a, \" \".join([f\"{k}={v}\" for k, v in kw.items()]) if kw else '')","d2a9a1ae":"# \u0427\u0442\u043e \u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u044f\u044e\u0442 \u0434\u0430\u043d\u043d\u044b\u0435?\ndic = {}\nfor i in list(df.columns): dic[i] = df[i].value_counts().shape[0]\ndf.info()\nd(pd.DataFrame(dic, index=[\"unique count\"]))\ndf.describe()","56d20141":"# \u0414\u0443\u0431\u043b\u0438\u043a\u0430\u0442\u044b?\nduplicates = df.duplicated().sum()\nprint(duplicates=duplicates)\nif duplicates: df = df.drop_duplicates(keep=\"first\")","dfa2b389":"# \u041a\u0430\u043a \u0440\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u044b \u0434\u0430\u043d\u043d\u044b\u0435 \u0432 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0430\u0445?\ndf.hist(figsize=(16,8))","f9ca9327":"# import seaborn as sns\n# from matplotlib import pyplot as plt\n# plt.subplots(figsize=(10,10))\n# encoded_data, encoders = number_encode_features(df)\n# sns.heatmap(encoded_data.corr(), square=True, annot=True, fmt='.2f')\n# plt.show()","f6ff34f5":"# Corellation matrix\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nplt.subplots(figsize=(12, 10))\nsns.heatmap(df.corr(), square=True, annot=True)\nplt.show()","29564d6e":"#? \u0435\u0441\u0442\u044c \u043b\u0438 \u0432\u044b\u0431\u0440\u043e\u0441\u044b?..\ndf.plot(subplots=True, figsize = (10, 20))\nplt.show()","70be377f":"# \u041f\u0440\u043e\u0432\u0435\u0440\u0438\u043c \u0438 \u0443\u0434\u0430\u043b\u0438\u043c \u043f\u0440\u043e\u043f\u0443\u0441\u043a\u0438\ndf_c = df\ndf_c = df_c.dropna()\ndf.count(), df_c.count()","2473163e":"# \u0420\u0430\u0437\u043e\u0431\u044c\u0451\u043c \u043d\u0430 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0438 \u0438 \u0442\u0430\u0440\u0433\u0435\u0442\ny = df['output']\nX = df.copy()\ndel X['output']\nd(df, X.head(3), y.head(3))","3e1fa212":"from sklearn.model_selection import train_test_split\nfrom sklearn import metrics\n\nfrom sklearn import dummy\nfrom sklearn.neighbors import KNeighborsClassifier\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n# d(X_train, X_test, y_train, y_test)\n# d([len(i) for i in (X_train, X_test, y_train, y_test)])\n\nknn = KNeighborsClassifier(n_neighbors=59) \nclf_mp = dummy.DummyClassifier(\"most_frequent\").fit(X_train, y_train)\nclf_knn = knn.fit(X_train, y_train)\ny_mp = clf_mp.predict(X_test)\ny_knn = clf_knn.predict(X_test)\n\n\ndef classification_report(alg_pred_dict, y_test=y_test, **kw):\n    for a, pred in alg_pred_dict.items(): print(\n        f\"{'-'*88}\\n{a}\\t{kw}:\\n\",\n        metrics.classification_report(y_test, pred),\n    )\nclassification_report(dict(\n        MP=y_mp,\n        KNN=y_knn,\n    ))","1278b69c":"# target = {0 : 'less chance of heart attack',  1 : 'more chance of heart attack'}\ntarget = ( 'less chance of heart attack',  'more chance of heart attack')\n# Confusion matrix\n#~^ encoders = df\nimport matplotlib\n\n# if 1:\ndef conf_matr(test, pred, target=(0, 1)):\n    fig = plt.figure(figsize=(8,8))\n    nn_mtx = metrics.confusion_matrix(test, pred)\n    print(nn_mtx)\n    font = {'weight' : 'bold', 'size'   :22}\n\n    matplotlib.rc('xtick', labelsize=20) \n    matplotlib.rc('ytick', labelsize=20) \n#     df = pd.DataFrame(test)\n    sns.heatmap(nn_mtx, annot=True, fmt=\"d\", \n                xticklabels=target,\n                yticklabels=target,\n               )\n    plt.ylabel(\"Real value\")\n    plt.xlabel(\"Predicted value\")\n    \n##conf_matr(y_test, y_knn, df=df)\nconf_matr(y_test, y_mp)\nconf_matr(y_test, y_knn)","3ad88b4b":"# \u0424\u0443\u043d\u043a\u0446\u0438\u044f \u043e\u0442\u0440\u0438\u0441\u043e\u0432\u043a\u0438 \u0433\u0440\u0430\u0444\u0438\u043a\u043e\u0432\ndef grid_plot(x, y, x_label, title, y_label='cross_val'):\n    plt.figure(figsize=(12, 6))\n    plt.grid(True)\n    plt.plot(x, y, 'go-')\n    plt.xlabel(x_label)\n    plt.ylabel(y_label)\n    plt.title(title)","1d27016a":"# \u0441 \u043a\u0440\u043e\u0441\u0441-\u0432\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u0435\u0439\nfrom sklearn.model_selection import GridSearchCV\n\ndef GridSearchCV_knn(*linspace, X, cls):\n    # \u0417\u0430\u0434\u0430\u0434\u0438\u043c \u0441\u0435\u0442\u043a\u0443 - \u0441\u0440\u0435\u0434\u0438 \u043a\u0430\u043a\u0438\u0445 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0439 \u0432\u044b\u0431\u0438\u0440\u0430\u0442\u044c \u043d\u0430\u0438\u043b\u0443\u0447\u0448\u0438\u0439 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440.\n    knn_grid = {'n_neighbors': np.array(np.linspace(*linspace), dtype='int')} # \u043f\u0435\u0440\u0435\u0431\u0438\u0440\u0430\u0435\u043c \u043f\u043e \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u0443 <<n_neighbors>>, \u043f\u043e \u0441\u0435\u0442\u043a\u0435 \u0437\u0430\u0434\u0430\u043d\u043d\u043e\u0439 np.linspace(2, 100, 10)\n    gs = GridSearchCV(cls, knn_grid, cv=5, n_jobs = -1)\n    gs.fit(X, y)\n    print(gs.best_params_, gs.best_score_)\n    # \u0421\u0442\u0440\u043e\u0438\u043c \u0433\u0440\u0430\u0444\u0438\u043a \u0437\u0430\u0432\u0438\u0441\u0438\u043c\u043e\u0441\u0442\u0438 \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u0430 \u043e\u0442 \u0447\u0438\u0441\u043b\u0430 \u0441\u043e\u0441\u0435\u0434\u0435\u0439\n    # \u0437\u0430\u043c\u0435\u0447\u0430\u043d\u0438\u0435: \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u044b \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f \u0445\u0440\u0430\u043d\u044f\u0442\u0441\u044f \u0432 \u0430\u0442\u0440\u0438\u0431\u0443\u0442\u0435 cv_results_ \u043e\u0431\u044a\u0435\u043a\u0442\u0430 gs\n    return grid_plot(knn_grid['n_neighbors'], gs.cv_results_['mean_test_score'], 'n_neighbors', 'KNeighborsClassifier')\n\nGridSearchCV_knn(2, 100, 10, X=X, cls=knn), \\\nGridSearchCV_knn(15, 35, 11, X=X, cls=knn),","b1402518":"# \u0441 \u043c\u0430\u0441\u0448\u0442\u0430\u0431\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u0435\u043c \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432\nfrom sklearn.preprocessing import scale\nX_scaled = scale(np.array(X, dtype='float'), with_std=True, with_mean=True)\n\nGridSearchCV_knn(15, 35, 11, X=X_scaled, cls=knn),\\\nGridSearchCV_knn(2, 100, 10, X=X_scaled, cls=knn)","63b10211":"def knn(**kw):\n    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.33, random_state=42)\n    knn = KNeighborsClassifier(**kw)\n    clf_knn = knn.fit(X_train, y_train)\n    y_knn = clf_knn.predict(X_test)\n    classification_report(dict(\n            KNN=y_knn,\n        ), **kw)\n    conf_matr(y_test, y_knn)\n    return y_knn\n\ny_knn = knn(n_neighbors=59)\ny_knn = knn(n_neighbors=12)\ny_knn = knn(n_neighbors=27)","f1e4bf84":"X0 = X.copy(); y0 = y.copy()\ndf0 = df.copy()\ndatasets = [df]\ndatasets","1f04c750":"X = X0.copy(); y = y0.copy()\ndf = df0.copy()\n# datasets = [df]\n# datasets","0dbe5c28":"# conf_matr??\ndef conf_matr(test, pred, target=(0, 1)):\n    fig = plt.figure(figsize=(4,4))\n    nn_mtx = metrics.confusion_matrix(test, pred)\n    print(nn_mtx)\n    font = {'weight' : 'bold', 'size'   :22}\n    matplotlib.rc('xtick', labelsize=20) \n    matplotlib.rc('ytick', labelsize=20) \n    sns.heatmap(nn_mtx, annot=True, fmt=\"d\", \n                xticklabels=target, yticklabels=target,\n               )\n    plt.ylabel(\"Real value\")\n    plt.xlabel(\"Predicted value\")\n    ","932cf8aa":"# from: https:\/\/scikit-learn.org\/stable\/auto_examples\/classification\/plot_classifier_comparison.html\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import ListedColormap\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.datasets import make_moons, make_circles, make_classification\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.gaussian_process import GaussianProcessClassifier\nfrom sklearn.gaussian_process.kernels import RBF\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n\nh = .02  # step size in the mesh\n\nnames = [\"Nearest Neighbors\", \"Linear SVM\", \"RBF SVM\", \"Gaussian Process\",\n         \"Decision Tree\", \"Random Forest\", \"Neural Net\", \"AdaBoost\",\n         \"Naive Bayes\", \"QDA\"]\n\nclassifiers = [\n    KNeighborsClassifier(3),\n    SVC(kernel=\"linear\", C=0.025),\n    SVC(gamma=2, C=1),\n    GaussianProcessClassifier(1.0 * RBF(1.0)),\n    DecisionTreeClassifier(max_depth=5),\n    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),\n    MLPClassifier(alpha=1, max_iter=1000),\n    AdaBoostClassifier(),\n    GaussianNB(),\n    QuadraticDiscriminantAnalysis()\n]\n\nX, y = make_classification(n_features=2, n_redundant=0, n_informative=2, random_state=1, n_clusters_per_class=1)\nrng = np.random.RandomState(2)\nX += 2 * rng.uniform(size=X.shape)\nlinearly_separable = (X, y)\n\n# datasets = [make_moons(noise=0.3, random_state=0),\n#             make_circles(noise=0.2, factor=0.5, random_state=1),\n#             linearly_separable\n#             ]\n\n##figure = plt.figure(figsize=(27, 9))\nfigure = plt.figure(figsize=(27, 27))\ni = 1\n# iterate over datasets\n# for ds_cnt, ds in enumerate(datasets):\nds_cnt, ds = 0, df\nif 1:\n    # preprocess dataset, split into training and test part\n#     X, y = ds\n    X = StandardScaler().fit_transform(X)\n    X_train, X_test, y_train, y_test = \\\n        train_test_split(X, y, test_size=.4, random_state=42)\n\n    x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                         np.arange(y_min, y_max, h))\n\n    # just plot the dataset first\n    cm = plt.cm.RdBu\n    cm_bright = ListedColormap(['#FF0000', '#0000FF'])\n    ax = plt.subplot(len(datasets), len(classifiers) + 1, i)\n    if ds_cnt == 0:\n        ax.set_title(\"Input data\")\n    # Plot the training points\n    ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright,\n               edgecolors='k')\n    # Plot the testing points\n    ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright, alpha=0.6,\n               edgecolors='k')\n    ax.set_xlim(xx.min(), xx.max())\n    ax.set_ylim(yy.min(), yy.max())\n    ax.set_xticks(())\n    ax.set_yticks(())\n    i += 1\n\n    # iterate over classifiers\n    for name, clf in zip(names, classifiers):\n        ax = plt.subplot(len(datasets), len(classifiers) + 1, i)\n        classfr = clf.fit(X_train, y_train)\n        score = clf.score(X_test, y_test)\n        \n#         def knn(**kw):\n#             X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.33, random_state=42)\n#             knn = KNeighborsClassifier(**kw)\n#             clf_knn = knn.fit(X_train, y_train)\n#             y_knn = clf_knn.predict(X_test)\n#             classification_report(dict(\n#                     KNN=y_knn,\n#                 ), **kw)\n#             conf_matr(y_test, y_knn)\n#             return y_knn\n\n#         clf_knn = knn.fit(X_train, y_train)\n#         y_knn = clf_knn.predict(X_test)\n\n        predict = classfr.predict(X_test)\n        kw={}\n        try:\n            classification_report({name : predict}, y_test=y_test, **kw)\n        except Exception as e: print(name, 'classification_report ERR:', e)\n        try:\n            conf_matr(y_test, predicts)\n        except Exception as e: print(name, 'conf_matr ERR:', e)\n\n        # Plot the decision boundary. For that, we will assign a color to each\n        # point in the mesh [x_min, x_max]x[y_min, y_max].\n        if hasattr(clf, \"decision_function\"):\n            Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\n        else:\n            Z = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]\n\n        # Put the result into a color plot\n        Z = Z.reshape(xx.shape)\n        ax.contourf(xx, yy, Z, cmap=cm, alpha=.8)\n\n        # Plot the training points\n        ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright,\n                   edgecolors='k')\n        # Plot the testing points\n        ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright,\n                   edgecolors='k', alpha=0.6)\n\n        ax.set_xlim(xx.min(), xx.max())\n        ax.set_ylim(yy.min(), yy.max())\n        ax.set_xticks(())\n        ax.set_yticks(())\n        if ds_cnt == 0:\n            ax.set_title(name)\n        ax.text(xx.max() - .3, yy.min() + .3, ('%.2f' % score).lstrip('0'),\n                size=15, horizontalalignment='right')\n        i += 1\n\nplt.tight_layout()\nplt.show()","4977d90b":"##### *_\u0417\u0430\u0434\u0430\u043d\u0438\u0435 \u043d\u0430 \u043e\u0442\u043b\u0438\u0447\u043d\u043e_*: \u0440\u0430\u0441\u0441\u043c\u043e\u0442\u0440\u0435\u0442\u044c \u043d\u0435 \u0442\u043e\u043b\u044c\u043a\u043e knn, \u043d\u043e \u0438 \u0434\u0440\u0443\u0433\u0438\u0435 (https:\/\/scikit-learn.org\/stable\/auto_examples\/classification\/plot_classifier_comparison.html) \u043a\u043b\u0430\u0441\u0441\u0438\u0444\u0438\u043a\u0430\u0442\u043e\u0440\u044b, \u043f\u043e\u043f\u0440\u043e\u0431\u043e\u0432\u0430\u0442\u044c \u043e\u0431\u0443\u0447\u0438\u0442\u044c \u043d\u0435\u0441\u043a\u043e\u043b\u044c\u043a\u043e \u0438\u0437 \u043d\u0438\u0445 \u0438 \u0441\u0440\u0430\u0432\u043d\u0438\u0442\u044c \u043f\u043e\u043b\u0443\u0447\u0435\u043d\u043d\u044b\u0439 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442 \u043f\u043e \u043c\u0435\u0442\u0440\u0438\u043a\u0430\u043c \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u0430 \u043d\u0430 \u0442\u0435\u0441\u0442\u0435.\n\n### \u041e\u043f\u0446\u0438\u043e\u043d\u0430\u043b\u044c\u043d\u044b\u0435 \u043a\u0440\u0438\u0442\u0435\u0440\u0438\u0438 \u043d\u0430 \u043e\u0442\u043b\u0438\u0447\u043d\u043e (8-10 \/10):\n* \u0415\u0441\u0442\u044c \u043f\u0435\u0440\u0435\u0431\u043e\u0440 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u043e\u0432 \u043e\u0442\u043b\u0438\u0447\u043d\u044b\u0445 (https:\/\/scikit-learn.org\/stable\/auto_examples\/classification\/plot_classifier_comparison.html) \u043e\u0442 knn \u0430\u043b\u0433\u043e\u0440\u0438\u0442\u043c\u043e\u0432?\n* \u0415\u0441\u0442\u044c \u043b\u0438 \u0430\u0443\u0433\u043c\u0435\u043d\u0442\u0430\u0446\u0438\u044f \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432 (https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.preprocessing.PolynomialFeatures.html)?\n* \u0415\u0441\u0442\u044c \u043b\u0438 \u0430\u0443\u0433\u043c\u0435\u043d\u0442\u0430\u0446\u0438\u044f \u0434\u0430\u043d\u043d\u044b\u0445 (https:\/\/habr.com\/ru\/post\/461285\/) \u043d\u0430 train \u0447\u0430\u0441\u0442\u0438?\n* \u0415\u0441\u0442\u044c \u043b\u0438 \u0441\u0440\u0430\u0432\u043d\u0435\u043d\u0438\u0435 \u0440\u0430\u0437\u043d\u044b\u0445 \u0432\u0430\u0440\u0438\u0430\u043d\u0442\u043e\u0432 \u043a\u043e\u0434\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u044f \u0434\u0430\u043d\u043d\u044b\u0445?\n* \u0415\u0441\u0442\u044c \u043b\u0438 \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u0435 \u043e\u0442\u043b\u0438\u0447\u043d\u044b\u0445 (https:\/\/medium.com\/pytorch\/carefree-learn-tabular-datasets-%EF%B8%8F-pytorch-e329b2f008f2) (\u0442\u044b\u043a (https:\/\/towardsdatascience.com\/deep-learning-using-pytorch-for-tabular-data-c68017d8b480)) \u043e\u0442 sklearn \u0430\u043b\u0433\u043e\u0440\u0438\u0442\u043c\u043e\u0432? (\u0432\u0432\u0435\u0434\u0435\u043d\u0438\u0435 \u0432 pytorch \u0445\u043e\u0440\u043e\u0448\u0438\u0439 \u043e\u0444\u0438\u0446\u0438\u0430\u043b\u044c\u043d\u044b\u0439 \u0442\u0443\u0442\u043e\u0440\u0438\u0430\u043b (https:\/\/pytorch.org\/tutorials\/beginner\/basics\/intro.html) \u0438 \u0442\u0443\u0442\u043e\u0440\u0438\u0430\u043b\u044b \u043d\u0430 \u0440\u0443\u0441\u0441\u043a\u043e\u043c 1 (https:\/\/www.youtube.com\/watch?v=6dfvtp5qLds) \u0438 2 (https:\/\/youtu.be\/aW9BgoKalY0))\n\u0412\u044b\u043f\u043e\u043b\u043d\u0435\u043d\u0438\u0435 \u043d\u0430 10 \u043d\u0435 \u0442\u0440\u0435\u0431\u0443\u0435\u0442 \u0432\u0441\u0435\u0445 \u043f\u0443\u043d\u043a\u0442\u043e\u0432 \u0432\u044b\u0448\u0435, \u043d\u043e \u0447\u0435\u043c \u0431\u043e\u043b\u044c\u0448\u0435 - \u0442\u0435\u043c \u0432\u0435\u0440\u043e\u044f\u0442\u043d\u0435\u0435.","d7d7f023":"# \u0414\u043e\u043c\u0430\u0448\u043d\u0435\u0435 \u0437\u0430\u0434\u0430\u043d\u0438\u0435 \u21162:\n\u041d\u0435\u043e\u0431\u0445\u043e\u0434\u0438\u043c\u043e \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u0442\u044c \u0440\u0438\u0441\u043a \u043f\u0440\u043e\u0431\u043b\u0435\u043c \u0441 \u0441\u0435\u0440\u0434\u0446\u0435\u043c (\u043a\u043e\u043b\u043e\u043d\u043a\u0430 output) \u043d\u0430 \u043e\u0441\u043d\u043e\u0432\u0435 \u0434\u0430\u043d\u043d\u044b\u0445 (https:\/\/www.kaggle.com\/rashikrahmanpritom\/heart-attack-analysis-prediction-dataset) (\u043d\u0435\u043e\u0431\u0445\u043e\u0434\u0438\u043c\u0430 \u0440\u0435\u0433\u0438\u0441\u0442\u0440\u0430\u0446\u0438\u044f \u043d\u0430 kaggle).  1\n\n### \u041a\u0440\u0438\u0442\u0435\u0440\u0438\u0438, \u043f\u043e \u043a\u043e\u0442\u043e\u0440\u044b\u043c \u043f\u0440\u043e\u0432\u0435\u0440\u044f\u0435\u0442\u0441\u044f \u0440\u0430\u0431\u043e\u0442\u0430:\n* \u0412\u044b\u043f\u043e\u043b\u043d\u0435\u043d\u0430 \u043b\u0438 \u0437\u0430\u0433\u0440\u0443\u0437\u043a\u0430 \u0434\u0430\u043d\u043d\u044b\u0445? (\u043a\u043e\u0440\u0440\u0435\u043a\u0442\u043d\u043e \u043b\u0438 \u0437\u0430\u0433\u0440\u0443\u0436\u0435\u043d \u0444\u0430\u0439\u043b, \u0432\u0441\u0435 \u043b\u0438 \u0437\u0430\u0433\u0440\u0443\u0437\u0438\u043b\u043e\u0441\u044c \u0438 \u0432 \u043f\u0440\u0430\u0432\u0438\u043b\u044c\u043d\u043e\u043c \u043b\u0438 \u0444\u043e\u0440\u043c\u0430\u0442\u0435)\n* \u0415\u0441\u0442\u044c \u043b\u0438 \u043f\u0435\u0440\u0432\u0438\u0447\u043d\u043e\u0435 \u0438\u0441\u0441\u043b\u0435\u0434\u043e\u0432\u0430\u043d\u0438\u0435? (\u0430\u043d\u043e\u043c\u0430\u043b\u0438\u0438, \u043f\u0440\u043e\u043f\u0443\u0441\u043a\u0438, \u043a\u043e\u0440\u0440\u0435\u043a\u0442\u043d\u043e\u0441\u0442\u044c \u0437\u0430\u043f\u043e\u043b\u043d\u0435\u043d\u0438\u044f)\n* \u0415\u0441\u0442\u044c \u043b\u0438 \u043e\u0441\u043d\u043e\u0432\u043d\u043e\u0435 \u0438\u0441\u0441\u043b\u0435\u0434\u043e\u0432\u0430\u043d\u0438\u0435 (EDA)? (\u0431\u0430\u043b\u0430\u043d\u0441 \u043a\u043b\u0430\u0441\u0441\u043e\u0432, \u043b\u0438\u043d\u0435\u0439\u043d\u044b\u0435 \u043a\u043e\u0440\u0440\u0435\u043b\u044f\u0446\u0438\u0438, \u0433\u0440\u0443\u043f\u043f\u0438\u0440\u043e\u0432\u043a\u0438 \u0441 \u0446\u0435\u043b\u0435\u0432\u044b\u043c, \u0442\u0435\u0437\u0438\u0441\u043d\u044b\u0435 \u0432\u044b\u0432\u043e\u0434\u044b \u0441\u043b\u043e\u0432\u0430\u043c\u0438)\n* \u0415\u0441\u0442\u044c \u043b\u0438 \u043f\u0440\u0435\u0434\u043e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0430? (\u043a\u043e\u0434\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u0435 \u0434\u0430\u043d\u043d\u044b\u0445 - one-hot\/label encoder, \u043d\u043e\u0440\u043c\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u044f, train\/val\/test \u0438\u043b\u0438 train\/test split, \u0441\u0442\u0440\u0430\u0442\u0438\u0444\u0438\u043a\u0430\u0446\u0438\u044f \u043f\u043e \u0431\u0430\u043b\u0430\u043d\u0441\u0438\u0440\u043e\u0432\u043a\u0435)\n* \u0415\u0441\u0442\u044c \u043b\u0438 \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u0435 \u0430\u043b\u0433\u043e\u0440\u0438\u0442\u043c\u0430 \u0438 \u043f\u0435\u0440\u0435\u0431\u043e\u0440 \u0433\u0438\u043f\u0435\u0440\u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u043e\u0432? (grid search \u0438 \u043d\u0435 \u0442\u043e\u043b\u044c\u043a\u043e, \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u0435 \u0430\u043b\u0433\u043e\u0440\u0438\u0442\u043c\u0430 c \u043b\u0443\u0447\u0448\u0438\u043c\u0438 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u0430\u043c\u0438)\n* \u0415\u0441\u0442\u044c \u043b\u0438 \u043e\u0446\u0435\u043d\u043a\u0430 \u0430\u043b\u0433\u043e\u0440\u0438\u0442\u043c\u0430? (\u0420\u0430\u0437\u043d\u044b\u0435 \u043c\u0435\u0442\u0440\u0438\u043a\u0438 \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u0430? \u0418\u043d\u0442\u0435\u0440\u043f\u0440\u0435\u0442\u0430\u0446\u0438\u044f \u043e\u0448\u0438\u0431\u043e\u043a)","5a148482":"## About this dataset\nAge : Age of the patient\n\nSex : Sex of the patient\n\nexang: exercise induced angina (1 = yes; 0 = no)\n\nca: number of major vessels (0-3)\n\ncp : Chest Pain type chest pain type\n\nValue 1: typical angina\nValue 2: atypical angina\nValue 3: non-anginal pain\nValue 4: asymptomatic\ntrtbps : resting blood pressure (in mm Hg)\n\nchol : cholestoral in mg\/dl fetched via BMI sensor\n\nfbs : (fasting blood sugar > 120 mg\/dl) (1 = true; 0 = false)\n\nrest_ecg : resting electrocardiographic results\n\nValue 0: normal\nValue 1: having ST-T wave abnormality (T wave inversions and\/or ST elevation or depression of > 0.05 mV)\nValue 2: showing probable or definite left ventricular hypertrophy by Estes' criteria\nthalach : maximum heart rate achieved\n\ntarget : 0= less chance of heart attack 1= more chance of heart attack"}}