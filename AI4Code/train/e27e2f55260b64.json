{"cell_type":{"9ab215ec":"code","e8486c75":"code","8fbbb307":"code","7722ba14":"code","f5dd3464":"code","1a9a751a":"code","ccfa2df8":"code","6654ad29":"code","50ab4089":"code","8728693b":"code","2c780cf7":"code","883c58cd":"code","cd9d087a":"code","f2ce4d05":"code","7a309a02":"code","9b4297ce":"code","fc9696e7":"code","62f38784":"code","c02a3bd9":"code","e17a7154":"code","5ca2924c":"code","588f1158":"code","57f2b40b":"code","84fc06fe":"code","02e7be0c":"code","8deac09e":"code","b8ad9fbd":"code","24b5951a":"code","99de3629":"code","ecbc4ed7":"code","ac2c321a":"code","e714f9c3":"code","fd2c365f":"code","689c697b":"code","b7e6d03a":"code","f7febf54":"code","6a88a3fc":"code","f244ea58":"code","dc9b4ba4":"code","d30cf260":"code","bc3bb3f9":"code","fb9a7083":"markdown","f4757460":"markdown","3460a659":"markdown","4dc793b3":"markdown","d0b10a03":"markdown","4e32f56c":"markdown","8b1186f7":"markdown","8ee0412f":"markdown","ea8221d2":"markdown","940fb9af":"markdown","5cc72506":"markdown","c492cf64":"markdown","a26f4bbf":"markdown"},"source":{"9ab215ec":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","e8486c75":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\n\ndf=pd.read_csv(\"\/kaggle\/input\/heart-disease-uci\/heart.csv\")\n\n\n## Datasetimiz \u00fcst\u00fcnde \u00e7e\u015fitli man\u00fcplasyonlar yapaca\u011f\u0131z datam\u0131z\u0131 anlamaya \u00e7al\u0131\u015faca\u011f\u0131z.\n## \u00c7e\u015fitli g\u00f6rselle\u015ftirmeler yapaca\u011f\u0131z\n## Makine \u00f6\u011frenmesi algoritmalar\u0131n\u0131 tek tek deneyerece\u011fiz\n## Datasetimiz \u00fczerinde en ba\u015far\u0131l\u0131 olan algoritmay\u0131 g\u00f6rm\u00fc\u015f olaca\u011f\u0131z","8fbbb307":"df.info","7722ba14":"df.head()","f5dd3464":"df.columns \n\n\n##age:age in years\n##sex:(1 = male; 0 = female)\n##cp:chest pain type\n##trestbps:resting blood pressure (in mm Hg on admission to the hospital)\n##chol:serum cholestoral in mg\/dl\n##fbs:(fasting blood sugar > 120 mg\/dl) (1 = true; 0 = false)\n##restecg:resting electrocardiographic results\n##thalach:maximum heart rate achieved\n##exang:exercise induced angina (1 = yes; 0 = no)\n##oldpeak:ST depression induced by exercise relative to rest\n##slope:the slope of the peak exercise ST segment\n##ca:number of major vessels (0-3) colored by flourosopy\n##thal:3 = normal; 6 = fixed defect; 7 = reversable defect\n##target:1 or 0\n\n\n\n","1a9a751a":"df.rename(columns={\"cp\":\"chest_pain\",\"trestbps\":\"blood_pressure\",\"chol\":\"serum_cholestoral\",\"fbs\":\"blood_sugar\",\"restecg\":\"electrocardiographic_results\",\"thalach\":\"heart_rate\",\"exang\":\"exercise\"},inplace=True)\n\n## Daha anla\u015f\u0131l\u0131r olmas\u0131 a\u00e7\u0131s\u0131ndan column isimlerimizi de\u011fi\u015ftirdik.","ccfa2df8":"df.head()","6654ad29":"df.isnull().sum()\n\n##Kay\u0131p verimiz bulunmuyor.E\u011fer kay\u0131p verimiz bulunsayd\u0131 bu durumu \u00e7e\u015fitli teknikler kullanarak man\u00fcple etmemiz gerekirdi.","50ab4089":"df.shape #14 adet \u00f6zelli\u011fimiz var","8728693b":"df.describe() ","2c780cf7":"#Columnlar\u0131n birbiriyle korelasyonu\nplt.figure(figsize=(15,9))\nsns.heatmap(df.corr(),cmap='Reds',annot=True,fmt='.2f') \nplt.show()","883c58cd":"# targetlar\u0131n ka\u00e7ar hastada oldu\u011funu bulabiliriz. Target columnu bizim labellar\u0131m\u0131z.\nplt.figure(figsize = (10,6))\ntarget = df['target']\ntarget_values = target.value_counts()\nplt.bar(target_values.index, target_values)\nplt.xlabel(\"Target Types\")\nplt.ylabel(\"Count\")\nplt.title(\"Counts Of Target Types\")\nplt.show()\n\n#Dengeli bir da\u011f\u0131l\u0131m var. E\u011fer datasetimizde labellar\u0131m\u0131z dengesiz bir da\u011f\u0131l\u0131m g\u00f6sterseydi. \u00d6\u011frenme do\u011fru ger\u00e7ekle\u015femezdi.","cd9d087a":"def plot_hist(variable):\n    \n    \n    var = df[variable]\n    \n    #B\u00fct\u00fcn featurelar\u0131m\u0131z i\u00e7in histogram grafikleri\n    \n    plt.figure(figsize = (10,3))\n    plt.hist(var,bins=75)\n    plt.xlabel(variable)\n    plt.ylabel(\"Count\")\n    plt.title(\"{} Distrubiton with Histogram\".format(variable))\n    plt.show()","f2ce4d05":"features=['age', 'sex', 'chest_pain', 'blood_pressure', 'serum_cholestoral',\n       'blood_sugar', 'electrocardiographic_results', 'heart_rate', 'exercise',\n       'oldpeak', 'slope', 'ca', 'thal', 'target']\nfor i in features:\n    plot_hist(i)","7a309a02":"## Some Relations ","9b4297ce":"df[[\"age\",\"target\"]].groupby([\"target\"]).mean().sort_values(by=\"age\",ascending=False)","fc9696e7":"df[[\"sex\",\"target\"]].groupby([\"target\"]).mean().sort_values(by = 'sex', ascending = False)","62f38784":"df[[\"chest_pain\",\"target\"]].groupby([\"target\"]).mean().sort_values(by = 'chest_pain', ascending = False)","c02a3bd9":"df[[\"blood_pressure\",\"target\"]].groupby([\"target\"]).mean().sort_values(by = 'blood_pressure', ascending = False)","e17a7154":"df[[\"serum_cholestoral\",\"target\"]].groupby([\"target\"]).mean().sort_values(by = 'serum_cholestoral', ascending = False)","5ca2924c":"df[[\"blood_sugar\",\"target\"]].groupby([\"target\"]).mean().sort_values(by = 'blood_sugar', ascending = False)","588f1158":"y=df[\"target\"].values\nx=df.drop(columns=[\"target\"],axis=1)","57f2b40b":"x=(x-np.min(x))\/(np.max(x)-np.min(x)).values","84fc06fe":"x.head()","02e7be0c":"y","8deac09e":"from sklearn.model_selection import train_test_split\n\nx_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.15,random_state=101)","b8ad9fbd":"x_train","24b5951a":"from sklearn.linear_model import LogisticRegression\n\nlog_reg=LogisticRegression()\nlog_reg.fit(x_train,y_train)\n\npredict=log_reg.predict(x_test)\n","99de3629":"log_reg.score(x_test,y_test)\n\n","ecbc4ed7":"from sklearn.neighbors import KNeighborsClassifier\nknn=KNeighborsClassifier(n_neighbors=3)\nknn.fit(x_train,y_train)\npredict_2=knn.predict(x_test)","ac2c321a":"knn.score(x_test,y_test)","e714f9c3":"aral\u0131k=np.arange(1,25)\ntrain_do\u011fruluk=[]\ntest_do\u011fruluk=[]\n\nfor i,k in enumerate(aral\u0131k):\n    knn=KNeighborsClassifier(n_neighbors=k)\n    knn.fit(x_train,y_train)\n    predict_2=knn.predict(x_test)\n    train_do\u011fruluk.append(knn.score(x_train,y_train))\n    test_do\u011fruluk.append(knn.score(x_test,y_test))\n    \n# \u015eimdi do\u011fruluk grafi\u011fini \u00e7izdirece\u011fiz\n\nplt.figure(figsize=[13,8])\nplt.plot(aral\u0131k, test_do\u011fruluk, label = 'Test Do\u011frulu\u011fu')\nplt.plot(aral\u0131k, train_do\u011fruluk, label = 'Training Do\u011frulu\u011fu')\nplt.legend()\nplt.title('-value VS Do\u011fruluk')\nplt.xlabel('Number of Neighbors')\nplt.ylabel('Accuracy')\nplt.xticks(aral\u0131k)\nplt.savefig('graph.png')\nplt.show()\nprint(\"En iyi do\u011fruluk {} with K = {}\".format(np.max(test_do\u011fruluk),1+test_do\u011fruluk.index(np.max(test_do\u011fruluk))))\n","fd2c365f":"from sklearn.svm import SVC\n\nsvc=SVC(random_state=101)\nsvc.fit(x_train,y_train)\n\npredict_3=svc.predict(x_test)\n","689c697b":"svc.score(x_test,y_test)","b7e6d03a":"from sklearn.ensemble import RandomForestClassifier\n\nrandom_forest=RandomForestClassifier(n_estimators=10)\nrandom_forest.fit(x_train,y_train)\npredict_4=random_forest.predict(x_test)\n","f7febf54":"random_forest.score(x_test,y_test)","6a88a3fc":"ara=np.arange(1,30)\ntrain_do\u011fruluk_2=[]\ntest_do\u011fruluk_2=[]\n\nfor i,k in enumerate(ara):\n    random_forest=RandomForestClassifier(n_estimators=k)\n    random_forest.fit(x_train,y_train)\n    predict=random_forest.predict(x_test)\n    \n    test_do\u011fruluk_2.append(random_forest.score(x_test,y_test))\n    train_do\u011fruluk_2.append(random_forest.score(x_train,y_train))\n    \n# \u015eimdi do\u011fruluk grafi\u011fini \u00e7izdirece\u011fiz\n\nplt.figure(figsize=[13,8])\nplt.plot(ara, test_do\u011fruluk_2, label = 'Test Do\u011frulu\u011fu')\nplt.plot(ara, train_do\u011fruluk_2, label = 'Training Do\u011frulu\u011fu')\nplt.legend()\nplt.title('-value VS Do\u011fruluk')\nplt.xlabel('Number of Neighbors')\nplt.ylabel('Accuracy')\nplt.xticks(ara)\nplt.show()","f244ea58":"test_do\u011fruluk_2","dc9b4ba4":"max(test_do\u011fruluk_2)\n\n##random forest algoritmas\u0131 i\u00e7in n_estimators=19 oldu\u011funda ba\u015far\u0131m\u0131z %91 civar\u0131 oldu","d30cf260":"from sklearn.tree import DecisionTreeClassifier\n\ndecision_tree=DecisionTreeClassifier()\ndecision_tree.fit(x_train,y_train)\n\npredict_5=decision_tree.predict(x_test)","bc3bb3f9":"decision_tree.score(x_test,y_test)","fb9a7083":"# Supervised Learning\n\nSupervised learning algoritmalar\u0131n\u0131 kullanaca\u011f\u0131z. Temel olarak elimizdeki etiketli veriyi train ve test verilerine ay\u0131raca\u011f\u0131z. Train verilerinde ger\u00e7ekle\u015ftirdi\u011fimiz e\u011fitimi Test verileri \u00fczerinde deneyerek ba\u015far\u0131lar\u0131 test edece\u011fiz. \n****","f4757460":"**Normalizasyon i\u015flemi say\u0131sal de\u011fer olarak b\u00fcy\u00fck featurelar\u0131n di\u011fer featurelar\u0131 ezmemesi i\u00e7in yap\u0131lmal\u0131d\u0131r. Aksi halde say\u0131sal de\u011feri y\u00fcksek olan \u00f6zellikler modelde di\u011ferleri \u00fczerinde \u00fcst\u00fcnl\u00fck kurabilir.**","3460a659":"# Machine Learning Classification Algoritmalar\u0131\n\n\n\n**S\u0131n\u0131fland\u0131rma kavram\u0131, basit\u00e7e bir veri k\u00fcmesi (data set) \u00fczerinde tan\u0131ml\u0131 olan \u00e7e\u015fitli s\u0131n\u0131flar aras\u0131nda veriyi da\u011f\u0131tmakt\u0131r. S\u0131n\u0131fland\u0131rma algoritmalar\u0131, verilen e\u011fitim k\u00fcmesinden bu da\u011f\u0131l\u0131m \u015feklini \u00f6\u011frenirler ve daha sonra s\u0131n\u0131f\u0131n\u0131n belirli olmad\u0131\u011f\u0131 test verileri geldi\u011finde do\u011fru \u015fekilde s\u0131n\u0131fland\u0131rmaya \u00e7al\u0131\u015f\u0131rlar.\n\nVeri k\u00fcmesi \u00fczerinde verilen bu s\u0131n\u0131flar\u0131 belirten de\u011ferlere etiket (label) ismi verilir ve gerek e\u011fitim gerekse test s\u0131ras\u0131nda verinin s\u0131n\u0131f\u0131n\u0131n belirlenmesi i\u00e7in kullan\u0131l\u0131rlar.\n\n**Biz bu notebook \u00fczerinde etiketli veriler ile \u00e7al\u0131\u015faca\u011f\u0131z.\n****","4dc793b3":"# 1)Logistic Regression Classification\n\n\n![log%20reg.JPG](attachment:log%20reg.JPG)\n\n\n\n","d0b10a03":"# K-Nearest Neighbour(KNN) Classification\n\n**1)K de\u011ferini se\u00e7\n  2)K en yak\u0131n data noktalar\u0131n\u0131 bul\n  3)K en yak\u0131n kom\u015fu aras\u0131nda hangi labeldan ka\u00e7 tane var hesapla\n  4)Test etti\u011fimiz point ya da data hangi de\u011fere ait tespit et\n  **\n  \n****\n","4e32f56c":"**Support Vector Machine algoritmas\u0131n\u0131 kullanarak e\u011fitti\u011fimiz modelimizin ba\u015far\u0131s\u0131 yakla\u015f\u0131k olarak %82 oldu.**","8b1186f7":"**Logistic Regression Classification, deep learning algoritmalar\u0131n\u0131n da temelidir diyebiliriz. Biz Logistic regression classification \u0131 elimizde iki farkl\u0131 s\u0131n\u0131f\u0131 olan datalar \u00fczerinde kullanabiliriz. Etiket olarak 1,0 veya kedi,k\u00f6pek olan datalar \u00f6rnek g\u00f6sterilebilir. Yukar\u0131daki compute graph\u0131n da g\u00f6sterdi\u011fi gibi logistic regressionun arkas\u0131nda temel bir matematik yatmaktad\u0131r.\n\n**Temel ama\u00e7 weight ve bias de\u011ferlerini forward ve backward propagationlar ile s\u00fcrekli g\u00fcncelleyerek e\u011fitmektir**\n\n\n**Forward Propagation:\n**\n**** Z=(w.T).x_train+b****\n**** y_head=sigmoid(z)****\n**Loss function ve cost function hesaplan\u0131r**\n\n**Backward Propagation:**\n\n**![image.png](attachment:image.png)**\n\n\n\n","8ee0412f":"# Random Forest Classification\n\n**Train ve Test datalar\u0131 olarak datasetimizi zaten b\u00f6lm\u00fc\u015ft\u00fck. Test datam\u0131z\u0131 bir kenara koyal\u0131m ve train datam\u0131z \u00fczerinden konu\u015fal\u0131m. \u00d6ncelikle train datam\u0131z\u0131n \u00fcst\u00fcnden N tane sample se\u00e7elim ve bunlara subsample ismini verelim. Sonras\u0131nda se\u00e7ti\u011fimiz bu subsample'\u0131 decision tree ler ile train edelim ve tekrar ba\u015fa d\u00f6nelim,bir d\u00f6ng\u00fc halinde e\u011fitimimizi ger\u00e7ekle\u015ftirelim. \u0130\u015fte sezgisel olarak bu modele random forest diyece\u011fiz.**\n\n**A\u015fa\u011f\u0131da decision treelerin basit\u00e7e g\u00f6rselle\u015ftirilmi\u015f hali bulunmakta**\n\n**![image.png](attachment:image.png)**\n\n\n**A\u015fa\u011f\u0131da Random Forest Algoritmas\u0131n\u0131n basit\u00e7e g\u00f6rselle\u015ftirilmi\u015f hali bulunmakta**\n\n\n**![0_a8KgF1IINziv7KIQ.png](attachment:0_a8KgF1IINziv7KIQ.png)**","ea8221d2":"# Support Vector Machines(SVM) Classification\n\n**S\u0131n\u0131fland\u0131rma (Classification) konusunda kullan\u0131lan olduk\u00e7a etkili ve basit y\u00f6ntemlerden birisidir. S\u0131n\u0131fland\u0131rma i\u00e7in bir d\u00fczlemde bulunan iki grup aras\u0131nda bir s\u0131n\u0131r \u00e7izilerek iki grubu ay\u0131rmak m\u00fcmk\u00fcnd\u00fcr. Bu s\u0131n\u0131r\u0131n \u00e7izilece\u011fi yer ise iki grubun da \u00fcyelerine en uzak olan yer olmal\u0131d\u0131r. \u0130\u015fte SVM bu s\u0131n\u0131r\u0131n nas\u0131l \u00e7izilece\u011fini belirler.**\n\n\n**![image.png](attachment:image.png)**","940fb9af":"**Classification modellerinin kar\u015f\u0131la\u015ft\u0131r\u0131lmas\u0131\n\n****KNN modeli : 0.869566****\n****SVM modeli : 0.826086****\n****Decision Tree modeli : 0.84782****\n****Random Forest modeli : 0.91304****\n****Logistic Regression modeli : 0.86956****\n****Datasetimiz i\u00e7in en uygun model Random Forest oldu.\n\n ****Te\u015fekk\u00fcrler ****","5cc72506":"**n_neighbors=3 de\u011feri i\u00e7in yakla\u015f\u0131k olarak y\u00fczde 82 lik bir ba\u015far\u0131 yakalad\u0131k. Peki farkl\u0131 n de\u011ferleri i\u00e7in daha y\u00fcksek ba\u015far\u0131 alabilir miyiz? **","c492cf64":"**Logistic Regression Classification algoritmas\u0131yla yakla\u015f\u0131k olarak %86l\u0131k bir ba\u015far\u0131 elde edebildik.**","a26f4bbf":"# Decision Tree Classification\n\n**Decision Tree'nin amac\u0131 veri \u00f6zelliklerinden basit kurallar \u00e7\u0131kar\u0131p bu kurallar\u0131 \u00f6\u011frenerek bir de\u011fi\u015fkenin de\u011ferini tahmin eden modeli olu\u015fturmakt\u0131r.**"}}