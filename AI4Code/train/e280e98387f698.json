{"cell_type":{"b4d25c24":"code","4f74b3b3":"code","6e51331c":"code","c802d7ad":"code","53edc007":"code","23ed0e25":"code","83a80591":"code","f564a207":"code","e0bdc500":"code","95376859":"code","e885698b":"code","e3d0498c":"code","8d362028":"code","7cbff714":"code","95c52341":"code","3b2d74c7":"code","3f7c00a7":"code","0125d65f":"code","abb4e463":"code","e11471e9":"code","3aa55f33":"code","875743c3":"code","b8006050":"code","4ae76086":"code","a86440f3":"markdown","5f02628f":"markdown","cde5cf9b":"markdown","f8ec5a83":"markdown","5e46c96b":"markdown"},"source":{"b4d25c24":"nb_name = 'mnist-lenet-spatial-domain'","4f74b3b3":"#################### Output ####################\nlog_path_dev = f'log-{nb_name}-dev.csv'\nlog_path_train = f'log-{nb_name}-train.csv'\nparams_path = f'params-{nb_name}.csv'\nsubmission_path = f'submisison.csv'","6e51331c":"import os\nimport pickle as pkl\nimport time\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf","c802d7ad":"def load_dataset(val_split = 0.25):\n    np.random.seed(0)\n    train_data = pd.read_csv('..\/input\/train.csv')\n    test_data = pd.read_csv(\"..\/input\/test.csv\")\n    train_y = train_data['label'].values\n    train_x = train_data.drop(columns=['label']).values\n    train_x = train_x.reshape(-1,28,28)\n    test_x = test_data.values\n    test_x = test_x.reshape(-1,28,28)\n\n    # Data Normalization\n    mean, std = train_x.mean(), train_x.std()\n    train_x = (train_x - mean)\/std\n    test_x = (test_x - mean)\/std\n    \n    train_x = train_x.reshape((-1, 1, 28, 28))\n    test_x = test_x.reshape((-1, 1, 28, 28))\n    \n    # Validation Set\n    indices = np.arange(len(train_x))\n    np.random.shuffle(indices)\n    pivot = int(len(train_x) * (1 - val_split))\n    train_x, val_x = train_x[indices[:pivot]], train_x[indices[pivot:]]\n    train_y, val_y = train_y[indices[:pivot]], train_y[indices[pivot:]]\n    \n    return train_x, train_y, val_x, val_y, test_x","53edc007":"train_x, train_y, val_x, val_y, test_x = load_dataset(1 - 28\/42)\ntrain_x.shape, val_x.shape, test_x.shape","23ed0e25":"perm=[0, 3, 2, 1] # Back and forth between NHWC and NCHW\ntrain_x, val_x, test_x = train_x.transpose(perm), val_x.transpose(perm), test_x.transpose(perm)\ntrain_x.shape, val_x.shape, test_x.shape","83a80591":"def conv_layer(name, x, k, f1, f2, s=1, padding='SAME'):\n    with tf.variable_scope(name):\n        value = tf.truncated_normal([k, k, f1, f2], stddev=1e-1)\n        w = tf.get_variable('w', initializer=value)\n        conv = tf.nn.conv2d(x, w, [1, 1, s, s], padding)\n\n        value = tf.constant(1e-1, tf.float32, [f2])\n        bias = tf.get_variable('bias', initializer=value)\n        out = tf.nn.relu(tf.nn.bias_add(conv, bias))\n        \n        tf.summary.histogram('weights', w)\n        tf.summary.histogram('bias', bias)\n        tf.summary.histogram('activations', out)\n        \n        return out\n\ndef pool_layer(name, x, stride=2, padding='SAME'):\n    with tf.variable_scope(name):\n        param = [1, stride, stride, 1]\n        x = tf.nn.max_pool(x, param, param, padding)\n        return x\n\ndef conv_net(x, out=10, is_training=True):\n    k, f1, f2, h1, h2 = 5, 6, 16, 120, 84 # LeNet\n    \n    # Define a scope for reusing the variables\n    with tf.variable_scope('ConvNet', reuse=tf.AUTO_REUSE):\n        # Input Channels     \n        c = x.shape.as_list()[-1] # For NHWC\n\n        x = conv_layer('Conv1', x, k, c, f1)\n        x = pool_layer('Pool1', x)\n        x = conv_layer('Conv2', x, k, f1, f2)\n        x = pool_layer('Pool2', x)\n\n        x = tf.contrib.layers.flatten(x, scope='Flatten')\n        x = tf.contrib.layers.fully_connected(x, h1, tf.nn.relu, scope='Dense1')\n        x = tf.contrib.layers.fully_connected(x, h2, tf.nn.relu, scope='Dense2')\n        y = tf.contrib.layers.fully_connected(x, out, None, scope='Logits')\n        return y","f564a207":"#################### Dataset Iteration ####################\ndef batch(x, y, batch_size=256):\n    num_steps = len(x) \/\/ batch_size\n    remainder = len(x) % batch_size\n    samples = np.arange(len(x))\n    np.random.shuffle(samples)\n    \n    for step in range(num_steps):\n        a, b = step * batch_size, (step + 1) * batch_size\n        yield x[samples[a:b]], y[samples[a:b]]\n\n    '''\n    a, b = num_steps * batch_size, num_steps * batch_size + remainder\n    yield x[samples[a:b]], y[samples[a:b]]\n    '''","e0bdc500":"def get_trainable_params(params_path = None):\n    params = list()\n    columns = ['name', 'shape', 'params']\n    for var in tf.trainable_variables():\n        params.append([\n            var.name,\n            var.shape,\n            np.prod(var.shape.as_list())\n        ])\n    return pd.DataFrame(params, columns=columns)","95376859":"num_epochs = 100\nval_step = int(num_epochs\/10)\nlearning_rate = 1e-3","e885698b":"tf.reset_default_graph()\n\nimage_shape = train_x.shape[1:]\n    \nx = tf.placeholder(tf.float32, shape=(None, *image_shape))\ny = tf.placeholder(tf.int64, shape=(None,))\n\n# Building Model\nlogits = conv_net(x)\n\nwith tf.name_scope('loss'):\n    loss = tf.losses.sparse_softmax_cross_entropy(y, logits)\n    tf.summary.scalar('loss', loss)\n\nwith tf.name_scope('train'):\n    optimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n\nwith tf.name_scope('accuracy'):\n    prediction = tf.argmax(logits, 1)\n    true_pos = tf.equal(y, prediction)\n    accuracy = tf.reduce_mean(tf.cast(true_pos, tf.float32))\n    tf.summary.scalar('accuracy', accuracy)","e3d0498c":"sess = tf.Session()","8d362028":"%%time\n\n# Initializing the variables\nsess.run(tf.global_variables_initializer())\n\nmerged_summary = tf.summary.merge_all()\nwriter = tf.summary.FileWriter('.\/summary')\nwriter.add_graph(sess.graph)\n\nlog = list()\nprint('Training Model')\nfor epoch in range(num_epochs):\n    # Model Training\n    tic = time.perf_counter()\n\n    batches = list(batch(train_x, train_y))\n    for i, (batch_x, batch_y) in enumerate(batches):\n        if i % int(num_epochs * 1.5) == 0:\n            s = sess.run(merged_summary, feed_dict={x: batch_x, y: batch_y})\n            writer.add_summary(s, epoch*len(batches) + i)\n        sess.run(optimizer, feed_dict={x: batch_x, y: batch_y})\n\n    tac = time.perf_counter()\n    tictac = tac-tic\n\n    # Model Validation\n    if (epoch+1)%val_step == 0:\n        train_loss, train_acc, val_loss, val_acc = [], [], [], []\n\n        # Evaluation on Training Set\n        for batch_x, batch_y in batch(train_x, train_y):\n            loss_, acc_ = sess.run((loss, accuracy), feed_dict={x: batch_x, y: batch_y})\n            train_loss.append(loss_)\n            train_acc.append(acc_)\n\n        # Evaluation on Validation Set\n        for batch_x, batch_y in batch(val_x, val_y):\n            loss_, acc_ = sess.run((loss, accuracy), feed_dict={x: batch_x, y: batch_y})\n            val_loss.append(loss_)\n            val_acc.append(acc_)\n\n        train_loss = np.array(train_loss).sum()\n        train_acc = np.array(train_acc).mean() * 100\n        \n        val_loss = np.array(val_loss).sum()\n        val_acc = np.array(val_acc).mean() * 100\n\n        params = [epoch+1, tictac, train_loss, train_acc, val_loss, val_acc]\n        msg = 'epoch: {}'\n        msg += ' time: {:.3f}s train_loss: {:.3f} train_acc: {:.3f}'\n        msg += ' val_loss: {:.3f} val_acc: {:.3f}'\n        print(msg.format(*params))\n        log.append(params)","7cbff714":"df_params = get_trainable_params()\ndf_params.to_csv(params_path)\ndf_params","95c52341":"df_params['params'].sum()","3b2d74c7":"cols = ['epoch', 'time', 'train_loss', 'train_acc', 'val_loss', 'val_acc']\ndf_log_dev = pd.DataFrame(log, columns=cols)\ndf_log_dev = df_log_dev.set_index('epoch')\ndf_log_dev.to_csv(log_path_dev)\ndf_log_dev","3f7c00a7":"df_log_dev.mean()","0125d65f":"fig, ax_ = plt.subplots(2,1, figsize=(9,7))\n\ndf_error_rate = df_log_dev[['train_loss', 'val_loss']]\nax = df_error_rate.plot(title='Overfit Analysis', marker='.', ax=ax_[0])\nax.set_xlabel('Epoch')\nax.set_ylabel('Loss');\n\ndf_error_rate = df_log_dev[['train_acc', 'val_acc']]\nax = df_error_rate.plot(marker='.', ax=ax_[1])\nax.set_xlabel('Epoch')\nax.set_ylabel('Accuracy');","abb4e463":"data_x = np.concatenate((train_x, val_x))\ndata_y = np.concatenate((train_y, val_y))","e11471e9":"%%time\n\nlog = list()\nprint('Training Model')\nfor epoch in range(num_epochs):\n    # Model Training\n    tic = time.perf_counter()\n\n    batches = list(batch(data_x, data_y))\n    for i, (batch_x, batch_y) in enumerate(batches):\n        sess.run(optimizer, feed_dict={x: batch_x, y: batch_y})\n\n    tac = time.perf_counter()\n    tictac = tac-tic\n\n    # Model Validation\n    if (epoch+1)%val_step == 0:\n        train_loss, train_acc = list(), list()\n\n        # Evaluation on Training Set\n        for batch_x, batch_y in batch(train_x, train_y):\n            loss_, acc_ = sess.run((loss, accuracy), feed_dict={x: batch_x, y: batch_y})\n            train_loss.append(loss_)\n            train_acc.append(acc_)\n\n        train_loss = np.array(train_loss).sum()\n        train_acc = np.array(train_acc).mean() * 100\n\n        params = [epoch+1, tictac, train_loss, train_acc]\n        msg = 'epoch: {}'\n        msg += ' time: {:.3f}s train_loss: {:.3f} train_acc: {:.3f}'\n        print(msg.format(*params))\n        log.append(params)","3aa55f33":"cols = ['epoch', 'time', 'train_loss', 'train_acc']\ndf_log_train = pd.DataFrame(log, columns=cols)\ndf_log_train = df_log_train.set_index('epoch')\ndf_log_train.to_csv(log_path_train)\ndf_log_train","875743c3":"fig, ax_ = plt.subplots(2,1, figsize=(9,7))\n\ndf_error_rate = df_log_train[['train_loss']]\nax = df_error_rate.plot(title='Overfit Analysis', marker='.', ax=ax_[0])\nax.set_xlabel('Epoch')\nax.set_ylabel('Loss');\n\ndf_error_rate = df_log_train[['train_acc']]\nax = df_error_rate.plot(marker='.', ax=ax_[1])\nax.set_xlabel('Epoch')\nax.set_ylabel('Accuracy');","b8006050":"test_y = sess.run(prediction, feed_dict={x: test_x})","4ae76086":"output = pd.DataFrame(test_y, columns=['Label'])\noutput.index = np.arange(1, len(output) + 1)\noutput.index.names = ['ImageId']\noutput.to_csv(submission_path)","a86440f3":"## Setting up Training Environment","5f02628f":"# Development Phase\n## Training Model\n### Tweaks allowed based on validation feedback","cde5cf9b":"## Loading MNIST Dataset","f8ec5a83":"# Retraining Model","5e46c96b":"## Model Architecture"}}