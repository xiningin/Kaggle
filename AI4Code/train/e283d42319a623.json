{"cell_type":{"47590edb":"code","4abf7f89":"code","f04582d5":"code","e35a6cd7":"code","45ce740d":"code","af89f590":"code","b1cf3dfa":"code","d27a26db":"code","39501a72":"code","8471b395":"code","cf326fd7":"code","c877a797":"code","c1ef431e":"code","dad81c05":"code","da162110":"code","3d3f26f2":"code","2690681d":"code","d803b90e":"code","a4872dfe":"code","dde13207":"code","2c3e966d":"code","77392173":"code","3fe0116b":"code","4c00bc7a":"code","09c33874":"code","b2587ccc":"code","a2649ec8":"code","7c4565a5":"code","6da517f3":"code","d5b8e8d2":"code","f70c61fd":"code","95bc2f3d":"code","dddf0abf":"code","2eb7b45e":"code","1253950a":"code","5b7e3213":"code","46828017":"code","decb8124":"code","08d9d814":"code","c335babf":"code","a6314546":"code","5984b1de":"code","33b375d4":"code","0bd82047":"code","c3e404c2":"markdown","03953796":"markdown","d59415da":"markdown","8347d2e5":"markdown","6410ccb6":"markdown","b9070247":"markdown","b3da9753":"markdown","d9666644":"markdown","38032a36":"markdown","8d6d5622":"markdown","4373e9b1":"markdown","018d15b3":"markdown","e55e1eb3":"markdown","ebb79f3b":"markdown","074d6edc":"markdown","a293b4e2":"markdown","688e1b3d":"markdown","0d366849":"markdown","317315d6":"markdown","72ab8e7e":"markdown","4b3d15cd":"markdown"},"source":{"47590edb":"# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","4abf7f89":"#Import Libraries\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nfrom scipy.stats import norm\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy import stats\nimport descartes\nimport geopandas as gpd\nfrom shapely.geometry import Point, Polygon\nimport category_encoders as ce\nfrom sklearn.preprocessing import OneHotEncoder\nimport warnings\nwarnings.filterwarnings('ignore')\n%matplotlib inline","f04582d5":"#Get the data\ndf_train = pd.read_csv('..\/input\/torontohousingprices\/dataset_final.csv')\ndf_train.columns","e35a6cd7":"pd.set_option('display.float_format', lambda x: '%.f' % x)\n\n#Descriptive statistics summary\ndf_train.describe()\n","45ce740d":"#Sold Price histogram\nsns.distplot(df_train['Sold_price'])","af89f590":"#Scatter plot 'Squarefootage'\/'Sold_price'\nvar = 'Squarefootage'\ndata = pd.concat([df_train['Sold_price'], df_train[var]], axis=1)\ndata.plot.scatter(x=var, y='Sold_price')","b1cf3dfa":"print('There are', df_train['Community'].nunique(), 'communities in',\n       df_train['Municipality District'].nunique(), 'districts.')","d27a26db":"# Get average 'Sold_price' across communities (neighbourhoods)\ncommunities = df_train[['Sold_price','Community']].groupby(['Community'])\n\ncommunity_prices = pd.concat([communities.mean(), communities.count()], axis=1)\ncommunity_prices.columns = ['Avg. Price', 'Count']\ncommunity_prices_sorted = community_prices.sort_values(by=['Avg. Price'], ascending=False)\ncommunity_prices_sorted","39501a72":"community_prices_sorted.plot.bar(figsize=(24,6))\ncommunity_prices_sorted['Count'].plot(kind='bar', color='Orange', secondary_y=True)","8471b395":"# Get average 'Sold_price' across districts\ndistricts = df_train[['Sold_price','Municipality District']].groupby(['Municipality District'])\n\ndistricts_prices = pd.concat([districts.mean(), districts.count()], axis=1)\ndistricts_prices.columns = ['Avg. Price', 'Count']\ndistricts_prices_sorted = districts_prices.sort_values(by=['Avg. Price'], ascending=False)\ndistricts_prices_sorted","cf326fd7":"districts_prices_sorted.plot.bar(figsize=(24,6))\ndistricts_prices_sorted['Count'].plot(kind='bar', color='Orange', secondary_y=True)","c877a797":"# Communities with <5 data points\ncommunity_prices_sorted[community_prices_sorted['Count'] <= 15].sort_values(by=['Count'], ascending=True).to_csv(\".\/output\")","c1ef431e":"#get map\nneighbourhood_map = gpd.read_file('..\/input\/folder\/forAnalysis\/Neighbourhoods\/Neighbourhoods.shp')\nneighbourhood_map['neighbourhood'] = neighbourhood_map['FIELD_7'].str.replace(' \\(.+\\)', '').str.lower()\n\n#merge data\ncommunity_prices['Neighbourhood'] = community_prices.index.str.lower()\ncommunity_prices.reset_index(drop=True, inplace=True)\n\nmerged = neighbourhood_map.set_index('neighbourhood').join(community_prices.set_index('Neighbourhood'))\nmerged = merged.reset_index()\nmerged = merged.fillna(0)\n\n#create heat map\nfig, ax = plt.subplots(1, figsize=(40, 20))\nax.axis('off')\nax.set_title('Heat Map of House Prices by Neighbourhood in Toronto', fontdict={'fontsize': '40', 'fontweight' : '3'})\n\n#create colorbar as a legend\ncolor = 'Oranges'\nvmin, vmax = 0, merged['Avg. Price'].max()\nsm = plt.cm.ScalarMappable(cmap=color, norm=plt.Normalize(vmin=vmin, vmax=vmax))\nsm._A = []\ncbar = fig.colorbar(sm)\ncbar.ax.tick_params(labelsize=15)\n\n# plot map - annotated neighbourhoods with >5 count\nmerged.plot('Avg. Price', cmap=color, linewidth=0.8, ax=ax, edgecolor='0.8', figsize=(40, 20))\nfor idx, row in merged.iterrows():\n    if(row['Count'] <= 5):\n        plt.annotate(s=int(row['Count']), xy=(row['FIELD_11'], row['FIELD_12']),\n                 horizontalalignment='center', fontsize='large', color='black', wrap=True)\nplt.show()","dad81c05":"# List of 0 count neighbourhoods\nmerged[merged['Count']<16][['neighbourhood','Avg. Price', 'Count']].sort_values(by=['Count'], ascending=True).to_csv('.\/output.csv')","da162110":"#correlation matrix\ncorrmat = df_train.corr()\nf, ax = plt.subplots(figsize=(12, 9))\nsns.heatmap(corrmat, vmax=.8, square=True);","3d3f26f2":"# Get average 'Sold_price' across communities (neighbourhoods)\ntypes = df_train[['Sold_price','Type']].groupby(['Type'])\n\ntype_prices = pd.concat([types.mean(), types.count()], axis=1)\ntype_prices.columns = ['Avg. Price', 'Count']\ntype_prices.sort_values(by=['Avg. Price'], ascending=False)","2690681d":"oneHot = OneHotEncoder(handle_unknown='ignore')\nce_ohe = ce.OneHotEncoder(cols = ['Type'], use_cat_names=True)\ndf_train= ce_ohe.fit_transform(df_train)\nprint(ce_ohe)\ndf_train.head()","d803b90e":"# Get average 'Sold_price' across communities (neighbourhoods)\nstyles = df_train[['Sold_price','Style']].groupby(['Style'])\n\nstyle_prices = pd.concat([styles.mean(), styles.count()], axis=1)\nstyle_prices.columns = ['Avg. Price', 'Count']\nstyle_prices.sort_values(by=['Avg. Price'], ascending=False)","a4872dfe":"oneHot = OneHotEncoder(handle_unknown='ignore')\nce_ohe = ce.OneHotEncoder(cols = ['Style'], use_cat_names=True)\ndf_train= ce_ohe.fit_transform(df_train)\nprint(ce_ohe)\ndf_train.head()","dde13207":"# Bin districts\ndf_train['district_bin'] = df_train['Municipality District']\ndf_train['district_bin'][df_train['Municipality District'] == 'C12'] = 'district_bin_0'\ndf_train['district_bin'][df_train['Municipality District'] == 'C04'] = 'district_bin_1'\ndf_train['district_bin'][df_train['Municipality District'] == 'C09'] = 'district_bin_2'\ndf_train['district_bin'][df_train['Municipality District'] == 'C03'] = 'district_bin_3'\ndf_train['district_bin'][df_train['Municipality District'] == 'E02'] = 'district_bin_4'\ndf_train['district_bin'][df_train['Municipality District'] == 'C02'] = 'district_bin_4'\ndf_train['district_bin'][df_train['Municipality District'] == 'W07'] = 'district_bin_5'\ndf_train['district_bin'][df_train['Municipality District'] == 'W02'] = 'district_bin_6'\ndf_train['district_bin'][df_train['Municipality District'] == 'E01'] = 'district_bin_6'\ndf_train['district_bin'][df_train['Municipality District'] == 'W01'] = 'district_bin_6'\ndf_train['district_bin'][df_train['Municipality District'] == 'C13'] = 'district_bin_7'\ndf_train['district_bin'][df_train['Municipality District'] == 'E03'] = 'district_bin_7'\ndf_train['district_bin'][df_train['Municipality District'] == 'C06'] = 'district_bin_7'\ndf_train['district_bin'][df_train['Municipality District'] == 'C07'] = 'district_bin_8'\ndf_train['district_bin'][df_train['Municipality District'] == 'E06'] = 'district_bin_8'\ndf_train['district_bin'][df_train['Municipality District'] == 'W03'] = 'district_bin_8'\ndf_train['district_bin'][df_train['Municipality District'] == 'C14'] = 'district_bin_8'\ndf_train['district_bin'][df_train['Municipality District'] == 'W08'] = 'district_bin_8'\ndf_train['district_bin'][df_train['Municipality District'] == 'C11'] = 'district_bin_9'\ndf_train['district_bin'][df_train['Municipality District'] == 'E10'] = 'district_bin_9'\ndf_train['district_bin'][df_train['Municipality District'] == 'C10'] = 'district_bin_9'\ndf_train['district_bin'][df_train['Municipality District'] == 'E08'] = 'district_bin_10'\ndf_train['district_bin'][df_train['Municipality District'] == 'W06'] = 'district_bin_10'\ndf_train['district_bin'][df_train['Municipality District'] == 'C01'] = 'district_bin_10'\ndf_train['district_bin'][df_train['Municipality District'] == 'C08'] = 'district_bin_11'\ndf_train['district_bin'][df_train['Municipality District'] == 'W09'] = 'district_bin_11'\ndf_train['district_bin'][df_train['Municipality District'] == 'W04'] = 'district_bin_11'\ndf_train['district_bin'][df_train['Municipality District'] == 'E07'] = 'district_bin_12'\ndf_train['district_bin'][df_train['Municipality District'] == 'W05'] = 'district_bin_12'\ndf_train['district_bin'][df_train['Municipality District'] == 'E04'] = 'district_bin_12'\ndf_train['district_bin'][df_train['Municipality District'] == 'C15'] = 'district_bin_12'\ndf_train['district_bin'][df_train['Municipality District'] == 'E05'] = 'district_bin_12'\ndf_train['district_bin'][df_train['Municipality District'] == 'E11'] = 'district_bin_13'\ndf_train['district_bin'][df_train['Municipality District'] == 'E09'] = 'district_bin_13'\ndf_train['district_bin'][df_train['Municipality District'] == 'W10'] = 'district_bin_14'\n\n# One-hot encode districts\noneHot = OneHotEncoder(handle_unknown='ignore')\nce_ohe = ce.OneHotEncoder(cols = ['district_bin'], use_cat_names=True)\ndf_train= ce_ohe.fit_transform(df_train)\ndf_train.head()","2c3e966d":"def bin_range(start_of_range,start_of_next_range):\n    return range(list(community_prices_sorted.index.values).index(start_of_range),list(community_prices_sorted.index.values).index(start_of_next_range))\n\n# Set Community Bins\nbins = [bin_range('Bridle Path-Sunnybrook-York Mills','Lawrence Park South')],\\\n        [bin_range('Lawrence Park South','Lawrence Park North')],\\\n        [bin_range('Lawrence Park North','Princess-Rosethorn')],\\\n        [bin_range('Princess-Rosethorn','Bedford Park-Nortown')],\\\n        [bin_range('Bedford Park-Nortown','St. Andrew-Windfields')],\\\n        [bin_range('St. Andrew-Windfields','Lambton Baby Point')],\\\n        [bin_range('Lambton Baby Point','Danforth')],\\\n        [bin_range('Danforth','Highland Creek')],\\\n        [bin_range('Highland Creek','Danforth Village-East York')],\\\n        [bin_range('Danforth Village-East York','Edenbridge-Humber Valley')],\\\n        [bin_range('Edenbridge-Humber Valley','Woodbine Corridor')],\\\n        [bin_range('Woodbine Corridor','Cabbagetown-South St. James Town')],\\\n        [bin_range('Cabbagetown-South St. James Town','University')],\\\n        [bin_range('University','Dovercourt-Wallace Emerson-Junction')],\\\n        [bin_range('Dovercourt-Wallace Emerson-Junction','Rouge E10')],\\\n        [bin_range('Rouge E10','Waterfront Communities C8')],\\\n        [bin_range('Waterfront Communities C8','Rockcliffe-Smythe')],\\\n        [bin_range('Rockcliffe-Smythe','Briar Hill-Belgravia')],\\\n        [bin_range('Briar Hill-Belgravia','Bayview Village')],\\\n        [bin_range('Bayview Village','Dorset Park')],\\\n        [bin_range('Dorset Park','Eglinton East')],\\\n        [bin_range('Eglinton East','Mount Olive-Silverstone-Jamestown')],\\\n        [bin_range('Mount Olive-Silverstone-Jamestown','Elms-Old Rexdale')],\\\n        [bin_range('Elms-Old Rexdale','Black Creek')]\n\ndf_train['community_bin'] = np.nan\n\nfor idx,item in enumerate(bins):\n    bin_name = 'community_bin_' + str(idx)\n    df_train['community_bin'][df_train['Community'].isin(community_prices_sorted.index.values[item])] = bin_name\n    last_bin_name = bin_name\n\n# Set end of last range \ndf_train['community_bin'][df_train['Community'] == 'Black Creek'] = last_bin_name\n    \n# One-hot encode Community Bins\noneHot = OneHotEncoder(handle_unknown='ignore')\nce_ohe = ce.OneHotEncoder(cols = ['community_bin'], use_cat_names=True)\ndf_train= ce_ohe.fit_transform(df_train)\ndf_train.head()","77392173":"beds = df_train[['Bedrooms','Dens']]\nmultiplier = 0.48\n\nbeds_prices = pd.concat([beds.Bedrooms, beds.Dens, beds.Bedrooms.astype(float) + multiplier * beds.Dens.astype(float)], axis=1)\npd.options.display.float_format = '{:,.2f}'.format\nbeds_prices.columns = ['Bedrooms', 'Dens', 'Combined']\n\nbeds_prices","3fe0116b":"#Combined vs. Bedrooms vs. Dens \nbedrooms = pd.concat([df_train['Sold_price'], beds_prices['Bedrooms']], axis=1)\nbedrooms.plot.scatter(x='Bedrooms', y='Sold_price')\n\ndens = pd.concat([df_train['Sold_price'], beds_prices['Dens']], axis=1)\ndens.plot.scatter(x='Dens', y='Sold_price')\n\ncombined = pd.concat([df_train['Sold_price'], beds_prices['Combined']], axis=1)\ncombined.plot.scatter(x='Combined', y='Sold_price')\n\nprint('Correlation:\\nBedrooms - ', df_train['Sold_price'].corr(beds_prices['Bedrooms']),\n                  '\\nDens     - ', df_train['Sold_price'].corr(beds_prices['Dens']),\n                  '\\nCombined - ', df_train['Sold_price'].corr(beds_prices['Combined']))\n","4c00bc7a":"# Correlation score of 'Combined' as a function of the multiplier\ndef combined_correlation(multiplier):\n    return df_train['Sold_price'].corr(beds.Bedrooms + beds.Dens * multiplier)\n\n# Create table with possible multiplier values on each row\noptimize = pd.DataFrame(columns=['multiplier','combined_correlation'])\noptimize['multiplier'] = np.linspace(0,1,1000) # 1000 values b\/w 0 and 1\n\n# Calculate Correlation score for each row\nfor idx, row in optimize.iterrows():   \n    optimize['combined_correlation'][idx] = combined_correlation(optimize['multiplier'][idx])\n\n# Get multiplier for maximum Correlation score\nprint('multiplier: ', optimize.loc[optimize['combined_correlation']==optimize['combined_correlation'].max(), 'multiplier'].iloc[0])\n","09c33874":"bathrooms = pd.concat([df_train['Sold_price'], df_train['Bathrooms']], axis=1)\nbathrooms.plot.scatter(x='Bathrooms', y='Sold_price')\n\nkitchen = pd.concat([df_train['Sold_price'], df_train['Kitchens']], axis=1)\nkitchen.plot.scatter(x='Kitchens', y='Sold_price')\n\nrooms = pd.concat([df_train['Sold_price'], df_train['Rooms']], axis=1)\nrooms.plot.scatter(x='Rooms', y='Sold_price')\n\nparking = pd.concat([df_train['Sold_price'], df_train['Parking Total']], axis=1)\nparking.plot.scatter(x='Parking Total', y='Sold_price')\n\nprint('Correlation:\\nBathrooms - ', df_train['Sold_price'].corr(df_train['Bathrooms']),\n                  '\\nKitchens  - ', df_train['Sold_price'].corr(df_train['Kitchens']),\n                  '\\nRooms     - ', df_train['Sold_price'].corr(df_train['Dens']),\n                  '\\nParking   - ', df_train['Sold_price'].corr(df_train['Parking Total']))","b2587ccc":"# histogram and normal probability plot\nsns.distplot(df_train['Squarefootage'], fit=norm)\nfig = plt.figure()\nres = stats.probplot(df_train['Squarefootage'], plot=plt)","a2649ec8":"# log squarefootage\ndf_train['log(Squarefootage)'] = np.log(df_train['Squarefootage'])\n\nsns.distplot(df_train['log(Squarefootage)'], fit=norm)\nfig = plt.figure()\nres = stats.probplot(df_train['log(Squarefootage)'], plot=plt)","7c4565a5":"df_train = df_train.drop(['longitude', 'latitude', 'Rooms', 'Kitchens', 'Squarefootage', 'Community', 'Municipality District'], axis=1)\ndf_train.head()","6da517f3":"# Drop 7 dens\ndf_train = df_train.drop(df_train[df_train['Dens']==7].index.values)\n\n# Drop Bathrooms 9+\ndf_train = df_train.drop(df_train[df_train['Bathrooms']>=9].index.values)\ndf_train = df_train.drop(df_train[df_train['Bathrooms']==0].index.values)\n\n# Drop Parking 9+\ndf_train = df_train.drop(df_train[df_train['Parking Total']>=9].index.values)\n\n# Round down .5 Parking Total\ndf_train[\"Parking Total\"].replace({\"1.5\": \"1\", \"3.5\": \"3\", \"5.5\": \"5\"}, inplace=True)","d5b8e8d2":"df_train[:1].values","f70c61fd":"# Scale Data\nfrom sklearn.preprocessing import MinMaxScaler\n\nscaler = MinMaxScaler(feature_range=(0, 1))\nscaled_train = scaler.fit_transform(df_train)\n\nscaled_train = pd.DataFrame(data=scaled_train, columns=df_train.columns)","95bc2f3d":"scaled_train[:1].values","dddf0abf":"print(\"Columns\")\nprint(scaled_train.columns)\n# Get scaled values\n# Print out the adjustment that the scaler applied to the total_earnings column of data\nprint(\"Note: sold_price values were scaled by multiplying by {:.10f} and adding {:.6f}\".format(scaler.scale_[0], scaler.min_[0]))\nmultiplied_by = scaler.scale_[0]\nadded = scaler.min_[0]\nprint(\"Multiplied by\")\nprint(scaler.scale_)\nprint(\"Added\")\nprint(scaler.min_)","2eb7b45e":"# Split Data\ntrain, test = np.split(scaled_train.sample(frac=1, random_state=42),\\\n                        [int(.9*len(scaled_train))])\n\nsns.distplot(train['Sold_price'])\nsns.distplot(test['Sold_price'])\n\nprint(len(train))\nprint(len(test))\nprint(len(test)+len(train))","1253950a":"y_train_data = train.values[:,0]\ny_test_data = test.values[:,0]\nx_train_data = train.values[:,1:]\nx_test_data = test.values[:,1:]","5b7e3213":"# Get input shape\nlen(x_train_data[0])","46828017":"# Imports\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom keras import Sequential\nfrom keras.layers import Dense, Dropout","decb8124":"# Build model\nmodel = Sequential()\n\nmodel.add(Dense(50, activation='relu'))\nmodel.add(Dense(100, activation='relu'))\nmodel.add(Dense(50, activation='relu'))\nmodel.add(Dense(50, activation='relu'))\nmodel.add(Dropout(0.3))\nmodel.add(Dense(50, activation='relu'))\nmodel.add(Dense(1))\n\nmodel.compile(loss='mean_squared_error', optimizer='adam')","08d9d814":"# Train the model\nhist = model.fit(\n    x_train_data,\n    y_train_data,\n    epochs=50,\n    shuffle=True,\n    verbose=2,\n    validation_data=(x_test_data, y_test_data)\n)","c335babf":"import matplotlib.pyplot as plt\n\nplt.plot(hist.history['loss'])\nplt.plot(hist.history['val_loss'])\nplt.title('Model loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Val'], loc='upper right')\nplt.show()","a6314546":"model.save('model')","5984b1de":"loaded_model = keras.models.load_model('model')","33b375d4":"x_test_data[:1]","0bd82047":"# Inference\nprediction = loaded_model.predict(x_test_data[:1])\ny_0 = prediction[0][0]\nprint('Prediction with scaling - {}',format(y_0))\ny_0 -= added\ny_0 \/= multiplied_by\nprint(\"Housing Price Prediction  - ${}\".format(y_0))\nactual = y_test_data[:1]\nprint('Actual with scaling - {}',format(actual))\nactual -= added\nactual \/= multiplied_by\nprint(\"Housing Price Prediction  - ${}\".format(actual))","c3e404c2":"#### Use one hot encoding for Type data","03953796":"#### Compare Combined Correlation","d59415da":"### Transform Skewed Data","8347d2e5":"### 'Sold_price' Relationship with Location Variables","6410ccb6":"### Bathrooms, Kitchens, Rooms, Parking Total Correlations","b9070247":"### Correlation matrix","b3da9753":"### 'Sold_price' Relationship with Numerical Variables","d9666644":"### Analyze 'Sold_price' ","38032a36":"## Begin Data Engineering","8d6d5622":"### Drop\/Transform Outliers","4373e9b1":"#### Optimize Dens Multiplier","018d15b3":"### Type","e55e1eb3":"### Drop latitude, longitude, kitchens, squarefootage, rooms rows","ebb79f3b":"### Combine Bedrooms and Den Columns","074d6edc":"### Normalize values to help NN train","a293b4e2":"### Separate Dataset into Train (70%), Validation (20%),Test (10%)","688e1b3d":"## Finished Data Engineering\n## Begin Neural Network","0d366849":"### Visualize Results","317315d6":"### Style","72ab8e7e":"### Setup input, label data","4b3d15cd":"### District and Community Encoding"}}