{"cell_type":{"d6b35d5b":"code","a767eef1":"code","8dc9a6d1":"code","f4e5277b":"code","02c2f814":"code","dde33e41":"code","73fb6c11":"code","0f308635":"code","5703b008":"code","0686bfa1":"code","1caed895":"code","0f1ebf18":"code","50b778ed":"code","c7c603b1":"code","940ffb12":"code","6fffe184":"code","6c103a0e":"code","33c24d66":"code","0f382dc8":"code","86cca6e2":"code","359b179f":"code","0edd6f80":"code","117b39e6":"code","de8064d4":"code","56cd302a":"code","9c9a397e":"code","d3f4e886":"code","1eecb0a1":"code","fd382fed":"code","dc910f0a":"code","c26353ce":"code","3acc8eda":"code","b93ea275":"code","d2d159df":"code","660110c1":"code","a08ca109":"code","0100dae4":"code","eafbe6f0":"code","854a4c81":"code","8b28f0e5":"code","369c3eb7":"code","c8b71330":"code","8c5e75b0":"code","8da23234":"code","46a35977":"code","40f507ee":"code","44b43a2c":"code","32fc86f8":"code","1418cb68":"code","47665678":"code","87eaa69a":"code","70a4853d":"code","a710e0b8":"code","af7c5a09":"code","5c87ecfa":"code","d2ff6099":"code","925601e1":"code","9163ba57":"code","18777e33":"code","50931507":"code","79ec785d":"code","abf90a36":"code","3ebfef1f":"code","e0168529":"code","4ada9fea":"code","2e98b359":"code","2e776f61":"code","a370540b":"code","e28540f0":"code","bb3a02ce":"markdown","46507bb1":"markdown","bcd91e7e":"markdown","4cf15193":"markdown","fe0c4a9a":"markdown","b580ee2b":"markdown","53666209":"markdown","282a50d4":"markdown","4ef4bf6d":"markdown","8b528f95":"markdown","0ea6f2dc":"markdown","07fc4da0":"markdown","58d11086":"markdown","36e6b01a":"markdown","d5cb8c9c":"markdown","3922a7c6":"markdown","59b4a6cc":"markdown","41844f88":"markdown","3db3fb98":"markdown","637e43eb":"markdown","f86a36ab":"markdown","50ce35af":"markdown","aa008318":"markdown","bde5ea76":"markdown","d9ae729a":"markdown","72674072":"markdown","6b258634":"markdown","ba5c2a94":"markdown","973fd599":"markdown","d735c8b6":"markdown","a5a07a03":"markdown","c90ad86b":"markdown","cc98159c":"markdown","678c1a7a":"markdown","6320f277":"markdown","ba97004f":"markdown","92f045e9":"markdown","f89badec":"markdown","83f842a6":"markdown","b7ce3635":"markdown","c8d018a8":"markdown","409fa858":"markdown","ca92aa1f":"markdown","424d9f06":"markdown","6cf6629f":"markdown","a2c41b65":"markdown","d950976c":"markdown","4bae1541":"markdown","1073f3f3":"markdown","089c4739":"markdown","379947c9":"markdown","64697d22":"markdown","025b9074":"markdown","06bb3611":"markdown","2c6ec5f5":"markdown","5291bb7d":"markdown","1b1fa990":"markdown"},"source":{"d6b35d5b":"# Importing the relevent packages\n\n# Math\/Linear algebra\nimport numpy      as np\nimport pandas     as pd\nimport statistics as sts\nimport math\nimport scipy      as sp\n\n# Data visulization\nimport matplotlib.pyplot as plt\nimport seaborn    as sns\nsns.set(style=\"ticks\", color_codes=True)\n\n# Machine Learning Model\nfrom sklearn import linear_model\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier","a767eef1":"# Import data\ntrainData = pd.read_csv('..\/input\/titanic\/train.csv')\ntestData  = pd.read_csv('..\/input\/titanic\/test.csv')\nprint(trainData.shape)\nprint(testData.shape)","8dc9a6d1":"# An overview of our columns\/data types.\ntrainData.head(5)","f4e5277b":"# View of the data type.\ntrainData.dtypes","02c2f814":"# Convert Pclass values from Integers to Strings.\ntrainData['Pclass'] = trainData['Pclass'].astype('str')\ntestData['Pclass']  = testData['Pclass'].astype('str')","dde33e41":"# A basic view of mean, max, etc. of numeric columns\ntrainData.describe()","73fb6c11":"# Display Number of Null and Unique Values by Columns\n\ntrainNull = pd.DataFrame(trainData.isnull().sum())\ntrainUnique = pd.DataFrame(trainData.nunique())\ntrainNull.columns = ['Training Null Value Count']\ntrainUnique.columns = ['Training Unique Value Count']\n\ntestNull = pd.DataFrame(testData.isnull().sum())\ntestUnique = pd.DataFrame(testData.nunique())\ntestNull.columns = ['Testing Null Value Count']\ntestUnique.columns = ['Testing Unique Value Count']\n\ntable1 = trainNull.join(testNull)\ntable2 = trainUnique.join(testUnique)\n\ntable1.join(table2)","0f308635":"# Passenger Survival Probability Mass Fuction (Pmf).\n# Recall: 0 = 'Died', 1 = 'Survived'\nsns.countplot(trainData['Survived'])\nsurvivalRatio = round(len(trainData[trainData['Survived']==1])\/\n                     (len(trainData[trainData['Survived']==1])+len(trainData[trainData['Survived']==0])),2)\n\nprint(\"Survival Rate: {}\".format(survivalRatio))\n\npd.DataFrame({'Count':[trainData['Survived'].value_counts()[0],trainData['Survived'].value_counts()[1]]}, \n             index = ['Died \"0\"','Lived \"1\"'])","5703b008":"# Passenger Sex Pmf.\nx = trainData.groupby('Sex')['PassengerId'].count()\ny = testData.groupby('Sex')['PassengerId'].count()\ngenderCount = pd.DataFrame({'dataset':['Train','Train','Test','Test'],\n                            'sex':['Male','Female','Male','Female'],\n                            'count':[x[1],x[0],y[1],y[0]]})\n\nsns.barplot(x=\"dataset\", y=\"count\", hue=\"sex\", data=genderCount) \n\n\ntrainSexRatio = round(len(trainData[trainData['Sex']=='male'])\/\n                      (len(trainData[trainData['Sex']=='male'])+len(trainData[trainData['Sex']=='female'])),2)\ntestSexRatio = round(len(testData[testData['Sex']=='male'])\/\n                     (len(testData[testData['Sex']=='male'])+len(testData[testData['Sex']=='female'])),2)\n\n\nd = {'Train': [trainSexRatio], 'Test': [testSexRatio]}\npd.DataFrame(d, index = ['Percentage Male'])","0686bfa1":"# Passenger Pclass Pmf.\nx = trainData.groupby('Pclass')['PassengerId'].count()\ny = testData.groupby('Pclass')['PassengerId'].count()\n\npClassTable = pd.DataFrame({'dataset':['Train','Train','Train','Test','Test','Test'],\n                           'Pclass': [1,2,3,1,2,3],\n                           'Count':[x.iloc[0],x.iloc[1],x.iloc[2],y.iloc[0],y.iloc[1],y.iloc[2]]})\n\nsns.barplot(x='dataset', y='Count', hue='Pclass',data=pClassTable)\n\nd = pd.DataFrame({'Train':[round(x.iloc[0]\/x.sum(),2), round(x.iloc[1]\/x.sum(),2), round(x.iloc[2]\/x.sum(),2)],\n                   'Test':[round(y.iloc[0]\/y.sum(),2), round(y.iloc[1]\/y.sum(),2), round(y.iloc[2]\/y.sum(),2)]},\n                index=[1,2,3])\nd","1caed895":"# Passenger Age Pmf.\nfig_dims = (16, 7)\nfig, ax = plt.subplots(figsize=fig_dims)\n\nplt.subplot(2,2,1)\nsns.distplot(trainData['Age'],color='blue')\nplt.subplot(2,2,2)\nsns.distplot(testData['Age'],color='orange')\nplt.subplot(2,2,3)\nsns.distplot(trainData['Age'],color='blue')\nsns.distplot(testData['Age'],color='orange')\n\nd = pd.DataFrame({'Median':[trainData['Age'].median(),testData['Age'].median()]},index=['Train','Test'])\nd\n\n# Train data displayed on top Left, Test data displayed on the top Right","0f1ebf18":"# Fare Distribution Check\n\nfig_dims = (16, 7)\nfig, ax = plt.subplots(figsize=fig_dims)\n\nplt.subplot(2,2,1)\nsns.distplot(trainData['Fare'],color='blue')\nplt.subplot(2,2,2)\nsns.distplot(testData['Fare'],color='orange')\nplt.subplot(2,2,3)\nsns.distplot(trainData['Fare'],color='blue')\nsns.distplot(testData['Fare'],color='orange')\n\nd = pd.DataFrame({'Median':[trainData['Fare'].median(),testData['Fare'].median()]},index=['Train','Test'])\nd\n\n# Train data displayed on top Left, Test data displayed on the top Right","50b778ed":"# Total Revenue Generated by the Titanic.\nround(trainData['Fare'].sum() + testData['Fare'].sum(), 2)","c7c603b1":"#Distribution of Sibilings\/Spouse Count\nsns.countplot(trainData['SibSp'])\npd.DataFrame(trainData['SibSp'].value_counts())","940ffb12":"#Distribution of Parent\/Children Count\nsns.countplot(trainData['Parch'])\npd.DataFrame(trainData['Parch'].value_counts())","6fffe184":"# Probably have to move this down too the 3rd section.\nsns.barplot(trainData['Parch'],trainData['SibSp'])","6c103a0e":"#Location Embarked Distribution Check.\n\nx = trainData.groupby('Embarked')['PassengerId'].count()\ny = testData.groupby('Embarked')['PassengerId'].count()\n\nembarkedTable = pd.DataFrame({'dataset':['Train','Train','Train','Test','Test','Test'],\n                           'Embarked': ['C','Q','S','C','Q','S'],\n                           'Count':[x.iloc[0],x.iloc[1],x.iloc[2],y.iloc[0],y.iloc[1],y.iloc[2]]})\n\nsns.barplot(x='dataset', y='Count', hue='Embarked',data=embarkedTable)\n\n# Probability Mass Function Table\nd = pd.DataFrame({'Train':[round(x.iloc[0]\/x.sum(),2), round(x.iloc[1]\/x.sum(),2), round(x.iloc[2]\/x.sum(),2)],\n                   'Test':[round(y.iloc[0]\/y.sum(),2), round(y.iloc[1]\/y.sum(),2), round(y.iloc[2]\/y.sum(),2)]},\n                index=['C','Q','S'])\nd","33c24d66":"# Extracting passengers 'Title' from 'Name'\ntrainData['Title'] = trainData.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\n\nx = trainData.groupby('Title')['Survived'].sum()\/trainData.groupby('Title')['PassengerId'].count()\n\nsurvByTitle = pd.DataFrame([trainData.groupby('Title')['Survived'].sum(),x]).T\nsurvByTitle.columns = ['Survived','Survival Ratio']\n\npd.crosstab(trainData['Title'], trainData['Sex']).join(survByTitle, on='Title').round(2)","0f382dc8":"trainData[(trainData['Title']=='Dr') & (trainData['Sex']=='female')]","86cca6e2":"trainData['Title'] = trainData['Title'].replace(['Lady', 'Countess','Capt', 'Col',\\\n \t'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n\ntrainData['Title'] = trainData['Title'].replace('Mlle', 'Miss')\ntrainData['Title'] = trainData['Title'].replace('Ms', 'Miss')\ntrainData['Title'] = trainData['Title'].replace('Mme', 'Mrs')\ntrainData = trainData.drop(columns=['Name'])\ntrainData.head(5)","359b179f":"# Extract Title from names for the test data.\ntestData['Title'] = testData.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\n\ntestData['Title'] = testData['Title'].replace(['Lady', 'Countess','Capt', 'Col',\\\n \t'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\ntestData['Title'] = testData['Title'].replace('Mlle', 'Miss')\ntestData['Title'] = testData['Title'].replace('Ms', 'Miss')\ntestData['Title'] = testData['Title'].replace('Mme', 'Mrs')\ntestData = testData.drop(columns=['Name'])\ntestData.head(5)","0edd6f80":"# Title Distribution Check\nx = pd.DataFrame(trainData['Title'].value_counts())\ny = pd.DataFrame(testData['Title'].value_counts())\n\nsns.barplot(x=x.index,y=x['Title'])\n\n# Probability mass functions\nx.columns=['Train']\ny.columns=['Test']\n\nround(x.div(891),2).join(round(y.div(418),2))","117b39e6":"#isCabin distribution check.\n#Note: 0 -> Passenger has a cabin, 1 -> Passenger doesn't have a cabin.\n\nisCabinTrain = trainData.isnull()[['Cabin']].astype(int)\nisCabinTest  = testData.isnull()[['Cabin']].astype(int)\n\nsns.countplot(isCabinTrain['Cabin'])\n\n# Ratio Table\nx = round(isCabinTrain['Cabin'].sum()\/891,2)\ny = round(isCabinTest['Cabin'].sum()\/418,2)\npd.DataFrame({'Train':x,'Test':y},index=['Percentage of Non-Cabin Passengers'])","de8064d4":"# Replacing the old Cabin column with our new isCabin Information.\ntrainData = trainData.drop(columns=['Cabin']).join(isCabinTrain)\ntestData = testData.drop(columns=['Cabin']).join(isCabinTest)","56cd302a":"# Mean age values age based on title and Pclass.\nmeanTable = pd.DataFrame(trainData.groupby(['Title','Pclass'])['Age'].mean())\nmeanTable","9c9a397e":"# Here we impute Age values based on Mean given title.\n\nfor i in range(len(trainData)):\n    if math.isnan(trainData.loc[i,'Age']) == True:\n        x = trainData.loc[(i,'Pclass')]\n        y = trainData.loc[(i,'Title')]\n        trainData.loc[i,'Age'] = meanTable.loc[(y,x),'Age']\n\ntrainData.isna().sum()","d3f4e886":"# Same thing for Test Copy.\n\nfor i in range(len(testData)):\n    if math.isnan(testData.loc[i,'Age']) == True:\n        x = testData.loc[(i,'Pclass')]\n        y = testData.loc[(i,'Title')]\n        testData.loc[i,'Age'] = meanTable.loc[(y,x),'Age']\n\ntestData.isna().sum()","1eecb0a1":"#Fill in null fare value.\nfor i in range(len(testData)):\n    if math.isnan(testData.loc[(i,'Fare')]) == True:\n        testData.loc[(i, 'Fare')] = testData.describe()['Fare']['mean']\ntestData.isna().sum()","fd382fed":"trainDataClean = trainData.drop(columns = ['Ticket'])\ntestDataClean = testData.drop(columns = ['Ticket'])\ntrainDataClean.head()","dc910f0a":"# Creating Age Bins\nbins = [0,10,20,30,40,50,80]\nx = pd.cut(trainDataClean['Age'],bins)\ntrainDataClean['AgeBins'] = x","c26353ce":"trainDataDummy = pd.get_dummies(trainDataClean.drop(columns=['AgeBins']))\ntrainDataDummy.head()","3acc8eda":"testDataDummy = pd.get_dummies(testDataClean)\ntestDataDummy.head()","b93ea275":"# Correlation Matrix\nfig_dims = (16, 7)\nfig, ax = plt.subplots(figsize=fig_dims)\n\nsns.heatmap(round(trainDataDummy.drop(columns=['PassengerId']).corr(),2), annot = True, cmap='Blues')","d2d159df":"sns.barplot(trainDataClean['Sex'],trainDataClean['Survived'])\n\nx = round(trainDataClean[trainDataClean['Sex']=='male']['Survived'].sum()\/\n          trainDataClean[trainDataClean['Sex']=='male'].count()['PassengerId'],2)\ny = round(trainDataClean[trainDataClean['Sex']=='female']['Survived'].sum()\/\n          trainDataClean[trainDataClean['Sex']=='female'].count()['PassengerId'],2)\n\nw = pd.DataFrame({'Survival Rate':[x,y]},index = ['Male', 'Female'])\nw","660110c1":"#table not displaying values\n#look at traindataclea.head()\nsns.barplot(trainDataClean['Pclass'],trainDataClean['Survived'])\n\nx = round(trainDataClean[trainDataClean['Pclass']==1]['Survived'].sum()\/\n          trainDataClean[trainDataClean['Pclass']==1].count()['PassengerId'],2)\ny = round(trainDataClean[trainDataClean['Pclass']==2]['Survived'].sum()\/\n          trainDataClean[trainDataClean['Pclass']==2].count()['PassengerId'],2)\nz = round(trainDataClean[trainDataClean['Pclass']==3]['Survived'].sum()\/\n          trainDataClean[trainDataClean['Pclass']==3].count()['PassengerId'],2)\n\nw = pd.DataFrame({'Survival Rate':[x,y,z]},index = ['Pclass 1', 'Pclass 2', 'Pclass 3'])\nw","a08ca109":"# Survival and Age\nfig_dims = (13, 5)\nfig, ax = plt.subplots(figsize=fig_dims)\n\ny = pd.DataFrame(trainDataClean['AgeBins'].value_counts()).sort_index()\n\nplt.subplot(1,2,1)\nsns.barplot(trainDataClean['AgeBins'],trainDataClean['Survived'])\nplt.subplot(1,2,2)\nsns.barplot(y.index,y['AgeBins'])","0100dae4":"# I wonder if it makes sence to classify pclass as an interger and not a dummy.\n# See the right hand graph to get the idea.\n\n#Age vs sex. Age vs. Pclass\nfig_dims = (13, 5)\nfig, ax = plt.subplots(figsize=fig_dims)\n\n# Creating Age Bins\nz = trainData.copy()\nbins = [0,10,20,30,40,50,80]\nx = pd.cut(z['Age'],bins)\nz['AgeBins'] = x\n\nplt.subplot(1,2,1)\nsns.countplot(x = 'AgeBins',hue = 'Sex',data = z)\nplt.subplot(1,2,2)\nsns.countplot(x = 'AgeBins',hue = 'Pclass',data = z)","eafbe6f0":"# Combining two columns together\ntrainDataClean['FamilySize']=trainDataClean['Parch']+trainDataClean['SibSp']","854a4c81":"fig_dims = (16, 4)\nfig, ax = plt.subplots(figsize=fig_dims)\n\nplt.subplot(131)\nsns.barplot(trainDataClean['SibSp'],trainDataClean['Survived'])\nplt.subplot(132)\nsns.barplot(trainDataClean['Parch'],trainDataClean['Survived'])\nplt.subplot(133)\nsns.barplot(trainDataClean['FamilySize'],trainDataClean['Survived'])","8b28f0e5":"trainDataClean[['Pclass','Fare']].groupby('Pclass').mean()\n\nsns.barplot(trainDataClean['Pclass'],trainDataClean['Fare'])","369c3eb7":"#watn to review the above text too see if it still makes sence in context.\ng = sns.FacetGrid(trainDataClean, col=\"Survived\")\ng = g.map(plt.hist, \"Fare\")\n\nx = trainDataClean[['Fare','Survived']][trainDataClean['Survived']==0].mean()['Fare']\ny = trainDataClean[['Fare','Survived']][trainDataClean['Survived']==1].mean()['Fare']\npd.DataFrame({'Average Fare':[x,y]},index=['Died','Survived'])","c8b71330":"# Verifying isCabin is useful\n\n# Table of isCabin Ratios by pClass\n# Hypothesis: Richer Passenger Classes will have more people in Cabins.\nx = trainDataClean[['Pclass','PassengerId']].groupby('Pclass').count()\ny = trainDataClean[['Cabin','Pclass']].groupby('Pclass').sum()\nz = x.join(y)\nw = pd.DataFrame({'Ratio Of Cabins':[round(((216-40)\/216),2),round(((216-168)\/184),2),round(((491-479)\/491),2)]},index = [1,2,3])\n\n#Also didnt mention how this was first class graph, even then its significance is not too impressive.\n# This section seems unessasary, should be a simpler test.\nsns.barplot(trainDataClean['Cabin'],trainData['Survived'])\nplt.xlabel(('Cabin','NoCabin'))\nw\n\n# Sum => number of isNull cabins?? ie 216-40 = #of Cabins","8c5e75b0":"# possible to add:\n# isCabin vs. Age, other metrics, would be interesting\n# more of the inbetweener visualizations: Fare pclass relations, etc.","8da23234":"sns.barplot(trainDataClean['Embarked'],trainDataClean['Survived'], order=['C','Q','S'])","46a35977":"#small detail, but would like to see the table transposed.\n\n# Graph of Destination Counts split by Gender\nx = pd.DataFrame(trainDataClean.groupby(['Embarked','Sex'])['PassengerId'].count())\nx = x.reset_index()\ng = sns.barplot(x='Embarked',y='PassengerId',hue='Sex',data=x, order=['C','Q','S'])\ng.set(ylabel='Count')\n\n# Table of Female Density\ny = pd.DataFrame({\n    'C':[round(x.loc[(0,'PassengerId')]\/(x.loc[(0,'PassengerId')]+x.loc[(1,'PassengerId')]),2)],\n    'Q':[round(x.loc[(2,'PassengerId')]\/(x.loc[(2,'PassengerId')]+x.loc[(3,'PassengerId')]),2)],\n    'S':[round(x.loc[(4,'PassengerId')]\/(x.loc[(4,'PassengerId')]+x.loc[(5,'PassengerId')]),2)]\n},index=['Percentage Female'])\ny","40f507ee":"# Logistic Regression\n# Random Forest\n# Support Vector Machine\n# naive Bayes Classifier\n# Descision Tree (??)","44b43a2c":"# Visual refresher of the data we are analyzing.\ntrainDataDummy.head()","32fc86f8":"# Now to drop superfluous columns to avoid singularities\ntrainRegression = trainDataDummy.drop(columns=['Pclass_3','Sex_male','Title_Rare'])\ntestRegression  = testDataDummy.drop(columns=['Pclass_3','Sex_male','Title_Rare'])\ntrainRegression.head()","1418cb68":"#delete cell after, downloading data for R\n\n#trainDataDummy.to_csv('datata.csv',index=False)","47665678":"# Converting our Dataframe to Array for Regression functions. \nxTrain = np.array(trainRegression.drop(columns=['Survived','PassengerId']))\nyTrain = np.array(trainRegression['Survived'])\n\nxTest = np.array(testRegression.drop(columns=['PassengerId']))","87eaa69a":"#Model 1: statsmodel\nimport statsmodels.api as sm\nxTrainC = sm.add_constant(xTrain)\n\nlogit_model=sm.Logit(yTrain,xTrainC)\nresult=logit_model.fit()\nresult.summary2()","70a4853d":"# Redo\n\n# Model 2: R General Linear Model.\n# Note that I inserted the x_i naming convention from the above model for comparison clarity.\n# (need to add x_i labels)\n\n#Coefficients:\n#               Estimate Std. Error t value Pr(>|t|)    \n#(Intercept)   0.3788422  0.2850327   1.329 0.184155    \n#Age          -0.0041073  0.0012278  -3.345 0.000857 ***\n#SibSp        -0.0674527  0.0130218  -5.180 2.76e-07 ***\n#Parch        -0.0516446  0.0180051  -2.868 0.004226 ** \n#Fare          0.0004775  0.0003284   1.454 0.146358    \n#Cabin        -0.1243457  0.0482163  -2.579 0.010073 *  \n#Pclass_1      0.2194919  0.0579775   3.786 0.000164 ***\n#Pclass_2      0.1595071  0.0336178   4.745 2.44e-06 ***\n#Sex_female    0.6193557  0.2266592   2.733 0.006411 **  \n#Embarked_C    0.0018730  0.2607483   0.007 0.994270    \n#Embarked_Q   -0.0215083  0.2634672  -0.082 0.934955    \n#Embarked_S   -0.0630647  0.2603695  -0.242 0.808672    \n#Title_Master  0.5258732  0.1123357   4.681 3.30e-06 ***\n#Title_Miss   -0.1097969  0.2143779  -0.512 0.608665    \n#Title_Mr      0.0033758  0.0858123   0.039 0.968629    \n#Title_Mrs     0.0087526  0.2143398   0.041 0.967437    ","a710e0b8":"pd.DataFrame({'Column':['Intercept','Pclass','Age','SibSp','Parch',\n                        'Fare','Cabin','Sex_male','Embarked_C','Embarked_Q',\n                        'Embarked_S','Title_Master','Title_Miss','Title_Mr','Title_Mrs'],\n             'Model 1 P-Value':[np.nan,0,0.0021,0,0.0046,\n                                0.2289,0.0312,0.3384,0.0013,0.0042,\n                                0.0044,0,0.0802,0.8708,0.0102],\n             'Model 2 P-Value':[0,0,0.0004,0,0.0050,\n                                0.2374,0.0275,0.0071,0.9869,0.9472,\n                                0.8388,0,0.6366,0.9778,0.9148],\n             'Model 1: Significant':[np.nan,'Yes','Yes','Yes','Yes',\n                                     'No','Yes','No','Yes','Yes',\n                                     'Yes','Yes','No','No','Yes'],\n             'Model 2: Significant':['Yes','Yes','Yes','Yes','Yes',\n                                     'No','Yes','Yes','No','No',\n                                     'No','Yes','No','No','No']})","af7c5a09":"logReg = linear_model.LogisticRegression(max_iter=500)\nlogReg.fit(xTrain,yTrain)","5c87ecfa":"logReg.score(xTrain,yTrain)","d2ff6099":"print(logReg.intercept_)\nprint(logReg.coef_)","925601e1":"yPred = logReg.predict(xTest)","9163ba57":"submission = pd.DataFrame({\n        \"PassengerId\": testData[\"PassengerId\"],\n        \"Survived\": yPred\n    })\nsubmission.to_csv('submission.csv', index=False)","18777e33":"trainRegression.head()","50931507":"# Random Forest (w\/Continuous dataset1)\n\nrandom_forest = RandomForestClassifier()\nrandom_forest.fit(xTrain,yTrain)\nY_predRF1 = random_forest.predict(xTest)\nrandom_forest.score(xTrain,yTrain)","79ec785d":"submissionRFMCont = pd.DataFrame({\n                \"PassengerId\": testData[\"PassengerId\"],\n                \"Survived\": Y_predRF1\n                })\nsubmissionRFMCont.to_csv('submissionRFM1.csv', index=False)","abf90a36":"# Creating Age Bins\ntrainRFM = trainRegression.copy()\ntestRFM = testRegression.copy()\n\nbins1 = [0,10,20,30,40,50,80]\nx0 = pd.cut(trainRFM['Age'],bins1)\nx1 = pd.cut(testRFM['Age'],bins1)\ntrainRFM['AgeBins'] = x0\ntestRFM['AgeBins'] = x1\n\n\nbins2 = [0,20,40,60,80,100,150,200,600]\ny0 = pd.cut(trainRFM['Fare'],bins2)\ny1 = pd.cut(testRFM['Fare'],bins2)\ntrainRFM['FareBins'] = y0\ntestRFM['FareBins'] = y1\n\n\ntrainRFM1 = trainRFM.drop(columns=['Age','Fare'])\ntestRFM1  = testRFM.drop(columns =['Age','Fare'])\n\ntrainRFM2 = pd.get_dummies(trainRFM1)\ntestRFM2  = pd.get_dummies(testRFM1)\ntrainRFM2.head()","3ebfef1f":"# Random Forest (w\/Categorical dataset2)\n\n# Converting our Dataframe to Array for Regression functions. \nxTrain1 = np.array(trainRFM2.drop(columns=['Survived','PassengerId']))\nyTrain1 = np.array(trainRFM2['Survived'])\n\nxTest1 = np.array(testRFM2.drop(columns=['PassengerId']))\n\nrandom_forest = RandomForestClassifier()\nrandom_forest.fit(xTrain1,yTrain1)\nY_predRFM2 = random_forest.predict(xTest1)\nrandom_forest.score(xTrain1,yTrain1)","e0168529":"submissionRFM2 = pd.DataFrame({\n        \"PassengerId\": testData[\"PassengerId\"],\n        \"Survived\": Y_predRFM2\n    })\nsubmissionRFM2.to_csv('submissionRFM2.csv', index=False)","4ada9fea":"# Decision Tree\n\ndecision_tree = DecisionTreeClassifier()\ndecision_tree.fit(xTrain1, yTrain1)\nY_pred = decision_tree.predict(xTest1)\ndecision_tree.score(xTrain1, yTrain1)","2e98b359":"# Stocastic Gradient Descent\nsgd = SGDClassifier()\nsgd.fit(xTrain, yTrain)\nY_pred = sgd.predict(xTest)\nsgd.score(xTrain, yTrain)","2e776f61":"# Linear SVC\n\nlinear_svc = LinearSVC()\nlinear_svc.fit(xTrain, yTrain)\nY_pred = linear_svc.predict(xTest)\nacc_linear_svc = round(linear_svc.score(xTrain, yTrain) * 100, 2)\nacc_linear_svc","a370540b":"# Gaussian Naive Bayes\n\ngaussian = GaussianNB()\ngaussian.fit(xTrain, yTrain)\nY_pred = gaussian.predict(xTest)\nacc_gaussian = round(gaussian.score(xTrain, yTrain) * 100, 2)\nacc_gaussian","e28540f0":"# K-Nearest Neighbours\n\nknn = KNeighborsClassifier(n_neighbors = 3)\nknn.fit(xTrain, yTrain)\nY_pred = knn.predict(xTest)\nacc_knn = round(knn.score(xTrain, yTrain) * 100, 2)\nacc_knn","bb3a02ce":"### Data Dictionary\n* PassengerId: Unique integer value for each passenger.\n* Survived: Values = (0,1) -> 0 = Passenger Died, 1 = Passenger Survived\n* Pclass: Values = (1,2,3) -> 1st class ticket, 2nd class ticket, 3rd class ticket.\n* Name: Passenger Name, Format = (LastName, Title. FirstName MiddleName (NickName)).\n* Sex: male, female.\n* Age: Float value from 0 to 80.\n* SibSp: (Sibling\/Spouse): Count of Siblings and Spouses of the given passenger abord the Titanic.\n* Parch: (Parent\/Children): Count of Parents and Children of the given passenger abord the Titanic.\n* Ticket: Ticket Number\n* Fare: Price of the ticket.\n* Cabin: Name of the Cabin the passenger stayed in.\n* Embarked: Values = (C,Q,S) -> Location of Port Passenger was en route to: C = Cherbourg, Q = Queenstown, S = Southampton.","46507bb1":"## Section 4: Model Selection","bcd91e7e":"Morbib fact: we can estimate total revenue the titanic generated from passengers during its life span to be **$43,586.11**.  \n  \nThe cost of building of the titanic was roughly 7.5 million dollars (priced in 1912, roughly 400 million in todays dollars), which would imply a massive finnacial loss. Interestingly, years later the titanic has proven to have been a profitable enterprise: https:\/\/www.investopedia.com\/financial-edge\/0412\/the-titanic-proves-its-worth.aspx","4cf15193":"The following notebook is based on the introductory kaggle challenge \"Titanic: Machine Learning from Disaster\", which can be found here:  \n(https:\/\/www.kaggle.com\/c\/titanic)","fe0c4a9a":"The data above has been cleaned to a satisfactory degree, and is what we will use for the final two stages of our analysis.  \n  \nWe will first visualize the relationships that exists between our variables to extract insights and test our modeling assumptions (Section 3), and we will follow that by running and scoring our machine learning models (Section 4).","b580ee2b":"# Machine Learning from Disaster: an EDA  \n\n![](https:\/\/static3.thetravelimages.com\/wordpress\/wp-content\/uploads\/2018\/11\/titanic1-e1542497861799.jpg?q=50&fit=crop&w=960&h=500&dpr=1.5)","53666209":"The final step of cleaning is to convert our categorical data into '[Dummy Variables](https:\/\/en.wikipedia.org\/wiki\/Dummy_variable_(statistics))'.  \nThis step is merly to convert the forum of our data into one that is usable by the program.","282a50d4":"Roughly 77% of our passengers are without a cabin.","4ef4bf6d":"Combaring our table of Percentage male to the Survival ratios in the previous cell, we don't notice a similar downward trend. However, the values do not different too wildly from the expected downward trend.","8b528f95":"This graph is one that I found very confusing at first sight. Why would the destination of the passenger effect there survival rates?  \nThere are two good explinations for this:  \n1) Location maybe correlated with other factors, ie. assume more women go to Charlote. This theory maybe able to be supported by exploring the correlates deeper.  \n2) The passengers may have been divided into sections abord the titanic based off of there destination. Ie. maybe the passengers headed to destination S were in the lower rungs and had more difficulty making it to the life boats when the iceberg was struck.","0ea6f2dc":"### References: \n\nA good overview of many basic ML techniques applied to the Titanic Dataset (u\/Manav Sehgal):  \nhttps:\/\/www.kaggle.com\/startupsci\/titanic-data-science-solutions\n  \nA great notebook with very good looking plots (u\/Kristof T):  \nhttps:\/\/www.kaggle.com\/kristoft\/sthetics-are-a-moral-imperative  \n\nA useful discussion on improving the imputing technique for null age values (u\/Allohvk):  \nhttps:\/\/www.kaggle.com\/c\/titanic\/discussion\/157929","07fc4da0":"As seen in Sehgal's work, we will combine the low frequency titles into a new category (rare).\n  \nAs well as combine common titles together to simplify our Title column to six unique titles.","58d11086":"## Table of Contents\n### Section 1: Data Overview and Basic Visualizations\n1.1) Introduction and Importing Data.  \n1.2) Data Dictionary and Overview.  \n1.3) Visualizing Distributions.  \n \n### Section 2: Data Cleaning\n2.1) Converting Name to Title.  \n2.2) Converting Cabin to its Indicator Function.  \n2.3) Imputing Age, and Other Missing Values.  \n2.4) Converting Categorical Variables to Dummy Variables.  \n\n### Section 3: Visualizing the Relationship Between Variables\n\n### Section 4: Model Selection","36e6b01a":"The first thing we are looking for in the table above is the number of null ('missing') values from our training dataset and our testing dataset.  \n  \nNote that:  \n1) 177 out of 891 Age values are missing in our training data set.  \n2) 86 out of 418 Age values are missing from our test data set.  \n3) 687 out of 891 Cabin Values are missing in our training data set.  \n4) 327 out of 418 Cabin values are missing in our testing data set.  \n  \n**Dealing with 1) and 2):**  \nAs we will see in our analysis in Section 3, Age is a strong predictor of survival. So one of our major tasks of our data cleaning is to fill in the missing ('null') age values in both training and testing data sets. This task will be completed in Section 2.3.  \n  \n**Dealing with 3) and 4):**  \nThere are two things to notice with the Cabin columns: most of the values are null, and the information that is populated is not easily useable. One approach that I have seen is too use the first letters used in the Cabin strings to split them into classess (A,B,C), however, each of these sections would contain very few passengers and would add too much complexity to our regression model, causeing gross overfitting.  \n  \nSo, the approach we will take here is too transform the Cabin column into a simple indicator of whether or not the Cabin information was orginally null or populated. This is not a technique I have seen in other Kaggle Notebooks, so we are required to do a simple check to see if our transformation of the Cabin column is a meaningful predictor variable of survival.  \n  \nOur transformation will be done in Section 2.2: Cabin Transformation, and our analysis of the columns usfulness will be covered in Section 3 where we will check its correlation with Survival and the other columns.","d5cb8c9c":"With a careful eye, we notice that Pclass is labeled as an interger. In reality, the passenger classes could be labeled with a categorical variable like (A class, B class, C class) instead of (1,2,3). So it makes more sence to convert these values to strings rather than keep them as intergers.  \n  \nWe will complete this step below.","3922a7c6":"Next we will tackle the missing age values.  \n\nHere we borrow a technique I saw from **u\/Allohvk**, which is apperently a common strategy for imporving the regression score to be >80%.  \n \nWe can use the title of our passenger to get a better estimate of their age (ie. the title \"Master\" was typically used for young children, where as \"Miss\" is for adult aged women).  \nIn addition to estimating age from title, we can group the population by **Passenger Class** as well to get more accurate age estimation.","59b4a6cc":"## 1.3) Visualizing Distributions","41844f88":"((Need to redo this blurb after redoing the model section)).\n\nIn this section we will take a statistical approach to selecting the best Logistic Regression Model.  \nWe will compare the output of three methods of calculating the regression coeffcients.  \n1) statsmodels Python Package.  \n2) glm function in R.  \n3) linear_model.LogisticRegression() Python Package.  \n\n(Interestingly enough, all three packages give different coeffcients based off the same data.)","3db3fb98":"In this section, our goal is to plot the distributions of our columns for two purposes:  \n1) To view the probability mass function (Pmf) of each of our columns to get basic insight on the distributions.\n2) To compare the distributions of the Test and Training data sets.  \n  \nThe reason for checking 2) is to verify a basic assumption nessassary for making predictions. That is both our training and testing data are roughly similarly distributed across our different dimensions. This is essentially the opposite too the problem of over fitting and is known as [Generalization Error](https:\/\/en.wikipedia.org\/wiki\/Generalization_error).","637e43eb":"Here is the outline of our columns and a sneak peak at some of our data. Note by above that we have 12 columns and 891 rows in the training dataset.","f86a36ab":"The above results are very wacky, not usuable in my opinion. And I can't seem to determine what went wrong with the regression model.\n  \nBelow are the results from the same dataset run through R's glm function, and it gave much more reasonable results.\n  \nIt should be noted that the third model I ran (linearModel.LogisticRegression) also appears to have given resonable results, and is the model we are submitting to the competition (with a score of 77%).","50ce35af":"Here we have 'buketed' the contiuous age values to get a better visulization of survival rates.  \nThe left graph represents the survival rate of each age range, where as the graph on the right is the count of the passengers in each age range.  \n  \nWhat we see is a bit of an odd pattern. Recall that the correlate between age and survival was -0.08, which implies that as age increases, survival rates decline slightly, however, in this graph we see a slightly more complicated pattern emerge.  \n  \nWe know from history that women and children had priority when boarding the life boats, which gives a reasonable explination as to why we see high survival rates for the young passengers. But the downward trend seen in the first three age buckets mysteriously jumps up at the 30-40 bucket before slowly declining again.   \n  \nThe first place too look to gain insight onto this mystery is to see the correlation between age and sex to see if there are a greater preportion of older women who can account for the jump up in survival rates. ","aa008318":"A cool thing to notice here is that we had only one female doctor on board.","bde5ea76":"From the above figures, we take away that the **Overall Survival Rate** is **38%**.","d9ae729a":"### Section 2.1: Converting Name to Title","72674072":"The next step of cleanig our data is to **remove** the Ticket Column. There does not appear to be an easy way too modify it to give us usful information for our Machine Learning model.  \nHowever, if I come across such a method I will update this.","6b258634":"To begin this section, we will start with viewing the correlation matrix and begin exploring how the dimensions of our data influence each other.","ba5c2a94":"Here we see some very interesting results:  \n  \n1) In the 20-80 age range, there are more men than women. With a very significant difference from 20-30.  \n2) There is a much higher proportion of 3rd class passenger in the 0-30 age range, which dramatically drops in the 40-80 age range.","973fd599":"### Section 2.3: Imputing Age, and Other Values","d735c8b6":"Our first step is to Extra the Titles of our passengers from their names.  \nThis code is esscetially copied and pasted from u\/Manav Sehgal, see his work in the reference section.","a5a07a03":"* Explination Needed","c90ad86b":"# Section 2: Data Cleaning","cc98159c":"An **Equal Ratio** of gender distribution between the **Training and Testing Datsets** is a good sign.  \nAs we will see later, female passengers were more likely to survive than male passengers, so it is interesting to note that ~65% of passengers were male on board the titanic.","678c1a7a":"### Section 4.2: Random Forest Model","6320f277":"The above set of graphs does paint an interesting picture. Recall that the third graph 'Family Size' is construceted from the addtion of the 'SibSp' and 'Parch' columns. What we see here is patterns that we can not make accurate statistical calls on in the first two figures, but they do appear to follow some shape. Then in the third figure we get a interesting picture where as family size increases survival chances increase until a family size of 4 is reached where it drops dramatically, but with inaccurate statistical predictors.  \n  \nThere is more room to be expolored here with correlates of familySize, prehaps they have more insight as to the nature of the increasing survival rates with family size.","ba97004f":"With wide success, we have executed this beautiful technique. An otherwise useless set of data, names, has been transformed to an incredibly useful categorical variable that contains rich, predictive information.  \n  \nAll of the above titles, with the exception of Rare, are used in english to denote gender, and age.  \n  \nTo take full advantage of our efforts, titles will be used as our primary tool when we fill in the missing age values in **Section 2.3: Imputing Age Values and Others**.","92f045e9":"## 1.2) Data Dictionary and Overview","f89badec":"This is an interesting finding, passengers of greater wealth (more expensive tickets) were more likley to survive.  \nWas there a bias in the distribution of life boats towards the wealthy? Or were there other factors like passengers of lower class being further away from the surface of the ship, and thus tended to be last in line for the life boats?  \n  \nPersonaly I think this finding is a bit hard to find a clear explination for, why would wealth\/price of the ticket be an indicator for survival?","83f842a6":"Here is the new variable that I added \"isCabin\", the indicator fuction that displays whether or not the initial cabin value was null or not. What we have done above is demonstrate that it is a useful predictor of survival, and thus will help us when constructing both our regression model and our random forest model.","b7ce3635":"The above graph is a great visulization to show the effect that the passengers gender has on the survivability rate.  \n  \nGiven that our survival rate for the population was 38%, we can use our findings above to estimate the gender balance in our original sample. This is a very similar method to the recommended first task of this competition, using only the gender information to predict survival in the testing data.  \n  \nTo run this fun side quest, we compute the simple equation seen below.\n  \nx\\*0.19+(891-x)\\*0.74=0.38   *Where 0<=x<=891*  \nx\\*0.19+891\\*0.74-x\\*0.74=0.38  \nx(-0.55)+659.34 = 0.38  \nx = (0.38-659.34)\/(-0.55)  \nx ~ 1198  \n  \nSo here what the gender survival rate is telling us is that all people abord the titanic were men in order to get the results we see. Obviously, this estimation is very crude, but it helps to illistrate the usefulness of the logistic regression model. ie., that given more passengers in our test data set, we can do much better than just assume that all the people who died were male with the regression model.","c8d018a8":"We will use the information in the above plot by two means:  \n  \n1) Explore the correlates with survival, the variable we are hoping to predict.  \n2) Explore other significant correlations shown to us by the above table.","409fa858":"Here we see that interestingly enough, there are more 1st class passengers than 2nd class passengers.  \nWe also note that ~50% of passengers are 3rd class.","ca92aa1f":"The above graph shows that passengers who payed more for their ticket, (ie. the passengers tend to be in first class, while the poorer passengers are in third class) tended to survive more often then their poorer counter parts.  \n\nThis is actually a neat way of visualizing Fare paid to survivability. Notice that the above method relies on the correlation between Fare and Pclass that we observed from our Correlation heatmap at the top of this section.    \nSee the graphs below to see how much better the above method is than the more direct approach of comparing Fare directly too Survived.","424d9f06":"### Section 2.2: Converting Cabin to its Indicator Function","6cf6629f":"In the blocks below, we aim to assess whether if the passenger has a cabin is relevent to the survivability of the passenger.","a2c41b65":"Ignore the next few cells for the moment, My models went all wacky on me and I cant seem to fix them.\n\n(The Purpose of Viewing theses two output of analitics is too assess if the logistic model is over fitting the data.  \n  \nWhat we are considering here is the p-vaules of the coefficients and comparing the p-values Variable wise between models.)","d950976c":"## 1.1) Introduction and Importing Data","4bae1541":"Note that in the graphs above, the test and train distributions of passengers age overlap pretty closely... a good sign for our predictive models.","1073f3f3":"### Section 2.4: Converting Categorical Variables to Dummy Variables","089c4739":"# Section 1: Data Overview and Basic Visulizations","379947c9":"(Please Note that this is still a work in progress, and things will continue to be cleaned and updated).\n  \nThis EDA is aimed at the curious viewer. If you have minimal experience with python or statistics, some concepts and methods in this notebook may seem difficult to understand. I have not made this as an introductory guide to the subject, but all the concepts and techniques should be relativly accessible to those who don't mind spending sometime googling when stuck and scratching their heads.  \n  \nFor those who are total beginners, or get stuck I recommend checking out [u\/Manav Sehgal](https:\/\/www.kaggle.com\/startupsci\/titanic-data-science-solutions) article on the Titanic ML Challenge.","64697d22":"In the above we see that ~75% of passengers were on route to Southampton. We will have to wait and see if destination is at all a usful predictor of survival.\n* Note: It does appear to be a useful predictor, which is a very strange result.","025b9074":"## Section 3: Visualizing The Relationship Between Variables","06bb3611":"This section focuses on altering the data to make it usable with our machine learning algorithms, as well as extracting information from the Name and Cabin columns which in their current form are useless, but do contain usefull information.","2c6ec5f5":"As noted above, the Cabin column has many missing values and the values that are present are practically unusable. The solution to this is to replace the Cabin column with a simple indicator function that denotes whether the Cabin data was originally null or not.  \n  \nIn **Section 3**, we will test to see if our transformation of the Cabin column holds any predictive power.","5291bb7d":"As you can see above, the mean age varies notibly from the different subsets of the population.","1b1fa990":"![](https:\/\/images.findagrave.com\/photos250\/photos\/2020\/187\/11126145_82d8295f-c5bd-4f72-82a1-0f7cc1315189.webp)\n![](https:\/\/images.findagrave.com\/photos250\/photos\/2019\/284\/11126145_e16ac9dd-d803-43d9-a32a-b91de039cc51.jpeg)\nhttps:\/\/www.encyclopedia-titanica.org\/titanic-survivor\/alice-leader.html"}}