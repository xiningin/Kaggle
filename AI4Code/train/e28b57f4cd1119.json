{"cell_type":{"9f425457":"code","948b6548":"code","d268c172":"code","374c2c7c":"code","a46a5025":"code","06d955bd":"code","f0086df5":"code","b0c2bbe3":"code","5551084d":"code","0775059e":"code","3c63110c":"code","d5231a2f":"code","a81f28cd":"code","51b15126":"code","f703ed9d":"code","f7e981c9":"code","8ec8a909":"code","ebe9e2c4":"code","f1524648":"code","50921504":"code","50a75644":"code","01e8772d":"code","2c17fb38":"markdown","11657496":"markdown","5a7700b8":"markdown","b724f5f8":"markdown","ac8173e9":"markdown","d59ec8f0":"markdown"},"source":{"9f425457":"import pandas as pd       \nimport matplotlib as mat\nimport matplotlib.pyplot as plt    \nimport numpy as np\nimport seaborn as sns\n%matplotlib inline\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import StandardScaler\n\nfrom lightgbm import LGBMClassifier\n\nfrom hyperopt.pyll.base import scope\nfrom hyperopt import hp, fmin, tpe, Trials\nfrom hyperopt import space_eval\n\n\nimport warnings\nwarnings.filterwarnings('ignore')","948b6548":"df_train = pd.read_csv('..\/input\/tabular-playground-series-may-2021\/train.csv', index_col = 'id')\nY_train = df_train['target'].copy()\nX_train = df_train.copy().drop('target', axis = 1)\n\nX_test = pd.read_csv('..\/input\/tabular-playground-series-may-2021\/test.csv', index_col = 'id')","d268c172":"df_train","374c2c7c":"df_train.info()","a46a5025":"df_train.describe().T","06d955bd":"df_train.nunique().sort_values()","f0086df5":"plt.figure(figsize=(6,4))\n\nclass_order = ['Class_1', 'Class_2', 'Class_3', 'Class_4']\nax = sns.countplot(x=\"target\", data=df_train, palette=\"BuPu\", order = class_order)\n\nplt.xlabel(\"Class\", fontsize= 12)\nplt.ylabel(\"N_Samples\", fontsize= 12)\nplt.title(\"Number of Samples per Class\", fontsize= 13)\nplt.ylim(0,100000)\n\nfor p in ax.patches:\n    ax.annotate((p.get_height()), (p.get_x()+0.22, p.get_height()+3000))\n\nplt.show()","b0c2bbe3":"features = X_train.columns\n\nplt.figure(figsize=(15,60))\n\nfor i,col in enumerate(features):    \n    plt.subplot(25,2,i + 1)\n    sns.distplot(df_train.loc[:,col])\n    plt.ylabel('')\n    plt.tight_layout()\n\nplt.show()","5551084d":"def cv_function (X_train, Y_train, model, splits = 10):\n    \n    kfold = StratifiedKFold(n_splits = splits)\n    logloss = []\n   \n    cv_pred = np.zeros((100000,4))\n    \n    for idx in kfold.split(X=X_train, y=Y_train):\n        train_idx, test_idx = idx[0], idx[1]\n        xtrain = X_train.iloc[train_idx]\n        ytrain = Y_train.iloc[train_idx]\n        xtest = X_train.iloc[test_idx]\n        ytest = Y_train.iloc[test_idx]\n        \n        # fit model for current fold\n        model.fit(xtrain, ytrain, \n            early_stopping_rounds = 100, eval_set = [(xtest,ytest)], verbose = False)\n\n        #create predictions\n        preds = model.predict_proba(xtest)\n        cv_pred[test_idx] = preds\n                              \n        # calculate and append accuracy\n        fold_logloss = metrics.log_loss(ytest,preds)\n        print(\"LogLoss: {0:0.5f}\". format(fold_logloss))\n        logloss.append(fold_logloss)\n        \n    print (np.mean(logloss))\n    #return np.mean(accuracies)\n    return cv_pred","0775059e":"lgbm_model = LGBMClassifier(n_estimators = 2000, learning_rate = 0.02, random_state = 42, num_class = 4, metric = 'multi_logloss')","3c63110c":"#lgbm_cvpred = cv_function(X_train, Y_train, lgbm_model)\n#1.094724908866833","d5231a2f":"def objective(params):\n\n    \n    clf_search = LGBMClassifier(n_estimators = 2000, learning_rate = 0.02, random_state = 42, num_class = 4, metric = 'multi_logloss', verbosity = -1)\n    clf_search.set_params(**params)\n   \n    search_cvpred = cv_function(X_train, Y_train, clf_search, splits = 5)  \n    score =metrics.log_loss(Y_train, search_cvpred)\n    print(\"Logloss: {0:0.6f}\".format(score)) \n    \n    return score\n    \nparams_lgbm = {\n    \"max_depth\": scope.int(hp.quniform(\"max_depth\", 3, 25, 1)),\n    \"subsample\": hp.uniform(\"subsample\",0.4,1),\n    \"colsample_bytree\": hp.uniform(\"colsample_bytree\",0.4,1),\n    \"min_child_weight\": scope.int(hp.quniform(\"min_child_weight\", 0.1, 1.0, 0.1)),    \n    \"min_child_samples\": scope.int(hp.quniform(\"min_child_samples\", 20, 100, 5)),\n    \"num_leaves\": scope.int(hp.quniform(\"num_leaves\", 7, 256, 1)),\n    \"reg_alpha\": hp.uniform('reg_alpha', 0.0, 1),\n    \"reg_lambda\": hp.uniform('reg_lambda', 0.0, 1),\n    \n}\n\n#Uncomment to run hyperopt\n\n#trials = Trials()\n\n#best = fmin(\n#    fn=objective,\n#    space = params_lgbm, \n#    algo=tpe.suggest, \n#    max_evals=50, \n#    trials=trials\n#)\n\n#print(\"Best: {}\".format(best))\n#trials.results","a81f28cd":"#best_params_lgbm = space_eval(params_lgbm, best)\n#print(best_params_lgbm)\n\n#Best parameters\nbest_params_lgbm = {'colsample_bytree': 0.4083405369693822, 'max_depth': 17, 'min_child_samples': 95, \n                    'min_child_weight': 0, 'num_leaves': 10, 'reg_alpha': 0.6966573230086442, \n                    'reg_lambda': 0.5138577842412738, 'subsample': 0.9800623921808034}","51b15126":"lgbm_tuned = lgbm_model\nlgbm_tuned = lgbm_tuned.set_params(**best_params_lgbm)\nlgbm_tuned","f703ed9d":"#lgbm_tuned_cvpred = cv_function(X_train, Y_train, lgbm_tuned)\n#1.0918248375370458","f7e981c9":"def prediction (X_train, Y_train, model, X_test):\n    \n    kfold = StratifiedKFold(n_splits = 10)\n\n    y_pred = np.zeros((50000,4))\n    train_oof = np.zeros((100000,4))\n    \n    for idx in kfold.split(X=X_train, y=Y_train):\n        train_idx, val_idx = idx[0], idx[1]\n        xtrain = X_train.iloc[train_idx]\n        ytrain = Y_train.iloc[train_idx]\n        xval = X_train.iloc[val_idx]\n        yval = Y_train.iloc[val_idx]\n        \n        # fit model for current fold\n        model.fit(xtrain, ytrain, \n            early_stopping_rounds = 100, eval_set = [(xval,yval)], verbose = False)\n\n        #create predictions\n        y_pred += model.predict_proba(X_test)\/kfold.n_splits\n        print(y_pred)\n               \n        val_pred = model.predict_proba(xval)\n        # getting out-of-fold predictions on training set\n        train_oof[val_idx] = val_pred\n        \n        # calculate and append logloss\n        fold_logloss = metrics.log_loss(yval,val_pred)\n        print(\"Logloss: {0:0.5f}\". format(fold_logloss))\n  \n    return y_pred, train_oof","8ec8a909":"lgbm_pred, train_oof  = prediction (X_train, Y_train, lgbm_tuned, X_test)","ebe9e2c4":"print(\"Logloss: {0:0.6f}\".format(metrics.log_loss(Y_train,train_oof))) #Logloss: 1.091825","f1524648":"train_oof = pd.DataFrame(train_oof, columns = ['Class_1', 'Class_2', 'Class_3', 'Class_4'])\ntrain_oof","50921504":"pred_test = pd.DataFrame(lgbm_pred, columns = ['Class_1', 'Class_2', 'Class_3', 'Class_4'])\npred_test","50a75644":"train_oof.to_csv('lgbm_train_oof.csv', index=False)\ntrain_oof","01e8772d":"output = pred_test\noutput['id'] = X_test.index\noutput.to_csv('submission.csv', index=False)\n\noutput","2c17fb38":"# <center>Tabular Playground Series - May\/2021<center>\n## <center> LightGBM Tuned with Hyperopt<center>\n---\n    \nI didn't perform an extensive EDA, since there were already great notebooks on it. My Suggestion:\n- [[TPS-May] Categorical EDA](https:\/\/www.kaggle.com\/subinium\/tps-may-categorical-eda) by [@subinium](https:\/\/www.kaggle.com\/subinium)\n    \nHyperparameter tuning with Hyperopt:\n- [Approaching (Almost) Any Machine Learning Problem](https:\/\/github.com\/abhi1thakur\/approachingalmost) by [@abhishek](https:\/\/www.kaggle.com\/abhishek)\n    \nMy other notebooks in this competition:\n- [Tabular Playground Series - May\/2021: Neural Network with Keras](https:\/\/www.kaggle.com\/jonaspalucibarbosa\/tps05-21-nn-with-keras-first-nn)\n- [Tabular Playground Series - May\/2021: Model Stacking using Logistic Regression as Meta-Learner](https:\/\/www.kaggle.com\/jonaspalucibarbosa\/tps05-21-model-stacking-meta-learner-lr)","11657496":"## Importing Libraries and Datasets","5a7700b8":"## Base LightGBM","b724f5f8":"## Exploring the Data","ac8173e9":"## Making Predictions","d59ec8f0":"## Hyperparameter Tuning with Hyperopt"}}