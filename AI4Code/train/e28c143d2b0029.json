{"cell_type":{"0a9f0bb1":"code","5a9e8a33":"code","a16d3684":"code","2bddf17d":"code","a3ece98c":"code","4d15596e":"code","6549ff3c":"code","ee1c642b":"code","edd5d196":"code","7e79b0ed":"code","2650687b":"code","a02b8451":"code","b5258f14":"code","160df2dc":"code","68646dc3":"code","f837a6d1":"code","1b5d3238":"code","7ff0f1e0":"markdown","65f799b1":"markdown","4ab7c84f":"markdown","98cbfbe7":"markdown","9d1d7cdf":"markdown","5809c2fa":"markdown","868289d9":"markdown","1069375f":"markdown","cbd222a1":"markdown","5aa3de4b":"markdown","7fb832ae":"markdown","e33ebe58":"markdown","eb485582":"markdown"},"source":{"0a9f0bb1":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \nfrom sklearn.metrics import accuracy_score\nfrom sklearn.ensemble import RandomForestClassifier\n\n#importing the relevant libraries necessary","5a9e8a33":"#Basic settings\n\ndatatrain = pd.read_csv(\"..\/input\/modeltrap\/train.csv\", low_memory = False, skipinitialspace=True)\ndatatest = pd.read_csv(\"..\/input\/modeltrap\/test.csv\", low_memory = False, skipinitialspace=True)\npd.options.display.float_format = '{:,.2f}'.format #to standardize the formatting\n\ndatatrain.head()\n#returns the first five rows of the training dataset - to check the data, features etc\n\ndatatest.head()\n#returns the first five rows of the test dataset - to check the data, features etc","a16d3684":"default_count = datatrain.default.sum()\n#calculates the total number of defaults in the training set\n\ndefault_percentage = default_count\/len(datatrain)*100\n#calculates the overall percentage of defaults within the entire training set \n\nprint(f'The share of defaults in the training set is {default_percentage:.2f}%')\n#prints the percentage value calculated previously, to 2 decimal places","2bddf17d":"max_default_zip = datatrain.groupby(by=\"ZIP\").default.mean().idxmax()\n\nprint(f'The postcode that has the highest default rate is {max_default_zip}')\n#Finds the postcode that has the highest number of defaults","a3ece98c":"default_train = datatrain.default[datatrain.year== 0].mean()\n\nprint(f'The Year 1 training set default rate is {default_train*100:.2f}%')\n#Calculates the mean default rate within the first year of the training set","4d15596e":"corr_age_income = datatrain.age.corr(datatrain.income)\n\nprint(f'The correlation between age and income in the training set is {corr_age_income *100:.2f}%')\n#Calculates the correlation between age and income in the training dataset to two decimal places","6549ff3c":"predictors = ['ZIP', 'rent', 'education',  'income', 'loan_size', 'payment_timing', 'job_stability','occupation']\ntarget = ['default']\n#specifying which features to use within our predictive model to call upon in future use to simplify the code\n\nX_train = pd.get_dummies(datatrain[predictors])\nY_train = datatrain[target]\n#creating the dependent and independent variables, all of which come from the training dataset \n\nclf = RandomForestClassifier(n_jobs=-1,n_estimators=100,oob_score=True,random_state=42,max_depth=4)\n#RandomForestClassifier is pulled from the sklearn library\n\nclf.fit(X_train,Y_train)\n#Estimating the coefficients of the Random Forest ie fitting the model\n\nin_sample_score = accuracy_score(y_pred = clf.predict(X_train),y_true=Y_train)\n#calculate the in sample accuracy score\n\nprint(f'The in sample accuracy is {in_sample_score*100:.2f}%')","ee1c642b":"oob_score = (clf.oob_score_)*100\n\nprint(f'The out of bag score of the model is {oob_score:.4f}%')\n#calculats the OOB score to four decimal places ","edd5d196":"X_test = pd.get_dummies(datatest[predictors])\nY_test = datatest[target].values\n\noos_score = clf.score(X_test, Y_test)*100\n#Creating new variables for the test model \n\nprint(f'The out of sample accuracy score is {oos_score:.2f}%')\n\n#OOS score is much less that OOB and in-sample score","7e79b0ed":"maj_dist = clf.predict(X_test[datatest.minority == 0])*100\n#using the RandomForestClassifier within the clf variable to predict average default probability for non-minorities ie when the minority feature equals 0.\n\nprint(f'The predicted average default probability for non-minorities is {maj_dist.mean():.2f}%')","2650687b":"min_dist = clf.predict(X_test[datatest.minority == 1])\n#performing a similar action as Q8 but for minorities ie when minority is equal to 1 \n\nprint(f'The predicted average default probability for minorities is {min_dist.mean()*100:.2f}%')","a02b8451":"pred = clf.predict(X_test)\n\nprint(f'Share of successful applicants amongst: minority members: {(len(datatest[(~pred) & (datatest.minority==1)]) \/ len(datatest[datatest.minority==1]))*100:.2f}%, non-minority members: {(len(datatest[(~pred) & (datatest.minority==0)]) \/ len(datatest[datatest.minority==0]))*100:.2f}%')\n\nprint(f'Share of successful applicants amongst: females: {(len(datatest[(~pred) & (datatest.sex==1)]) \/ len(datatest[datatest.sex==1]))*100:.2f}%, males: {(len(datatest[(~pred) & (datatest.sex==0)]) \/ len(datatest[datatest.sex==0]))*100:.2f}%')","b5258f14":"#Rejected applicants by sex: Male - 18.5%, Female - 18.9%. \n#There is therefore gender parity as the same proportions of M & F applicants received loans.\n#However, this was not the case with minorities and non-minorities; minorities received a 6% approval rate, while non-minorities received a 31% approval rate. ","160df2dc":"#Generating the array of predicted default stats\nY_oos_pred = clf.predict(X_test)\nout = pd.Series(data=Y_oos_pred)\nout = out.rename('defaults')\n\n#Collecting the predictions & regressors & sex w\/ minority in 1 dataset\ndata_q10 = pd.concat([out, X_test, datatest.sex, datatest.minority],axis=1)\n\n#Looking for any interaction between minority status and gender:\ndata_q10.groupby(by=['sex','minority']).mean().defaults\n\n#We can see that the model does not predict any additional increase in default rates as a result of the interaction between \"minority\" and \"sex\" i.e. no further discrimination","68646dc3":"pred = pd.DataFrame(data = [clf.predict(X_test)]).T\n#generating a dataframe for the prediction variable\n\ny_test_df = pd.DataFrame(data = [datatest.default.tolist()]).T\n#generate a dataframe for the observed defaults\n\nx_test_df = pd.DataFrame(data = [datatest.minority.tolist(),datatest.sex.tolist()]).T\n#generate a dataframe for the minority and gender status\n\naa = pd.concat([pred, y_test_df, x_test_df], ignore_index=True, axis=1)\n#concatenating all three matrices together\n\naa.columns = ['pred','test','minority','sex']\n#specify the column names\n\nprint(f'Share of granted loans amongst those who would repay: minority members: {(1 - aa[\"pred\"][(aa[\"minority\"] == 1) & (aa[\"test\"]==0)].mean())*100:.2f}% \\\\\\\nnon-minority: {(1 - aa[\"pred\"][(aa[\"minority\"] == 0) & (aa[\"test\"]==0)].mean())*100:.2f}%')\n\nprint(f'Share of granted loans amongst those who would repay: female: {(1 - aa[\"pred\"][(aa[\"sex\"] == 1) & (aa[\"test\"]==0)].mean())*100:.2f}% \\\nmale: {(1 - aa[\"pred\"][(aa[\"sex\"] == 0) & (aa[\"test\"]==0)].mean())*100:.2f}%')","f837a6d1":"#What do these shares indicate about the likelihood of default in different population groups that secured loans?\n\ndata_q12 = pd.concat([out, datatest.default, datatest.sex, datatest.minority],axis=1)\n\n#We then create a sub-sample \"paid\" - ie the applicants that repaid their loan. \n#We'll then check whether the model correctly predicted that they would pay.\n\npaid = data_q12[(data_q12.default==0)]\n\n#Looking at predicted default rates among those applicants who did not default\npaid.groupby(by='minority').mean().defaults\n\n#The model has incorrectly rejected 12% of loan-worthy within the non-minority demographic and 30.5% loan-worthy applicants from a minority background.\n#This violates equal opportunity.\n\n#Looking at the same, but split by gender\npaid.groupby(by='sex').mean().defaults\n\n#Same effect present but of smaller magnitude - 20% of loan-worthy males were rejected vs 23% of loan-worthy females\n\napproved = data_q12[(data_q12.defaults==0)]\napproved.groupby(by='minority').mean().default","1b5d3238":"#using matplotlib to create a chart showing the varying importance of the features within the model\n\nfeatures = X_test.columns.values\nimportances = clf.feature_importances_\nindices = np.argsort(importances)\n\nplt.title('Feature Importances')\nplt.barh(range(len(indices)), importances[indices], color='c', align='center')\nplt.yticks(range(len(indices)), [features[i] for i in indices])\nplt.xlabel('Relative Importance')\nplt.ylabel('Features')\nplt.show()","7ff0f1e0":"# Question 4 What is the correlation between age and income in the training dataset? ","65f799b1":"# Question 11 Has the loan granting scheme achieved demographic parity? ","4ab7c84f":"# Question 7 What is the out of sample accuracy? ","98cbfbe7":"# Question 12 - Is the loan granting scheme equal opportunity? ","9d1d7cdf":"# Question 3 What is the default rate in the training set for the first year for which you have data?","5809c2fa":"# Question 9 What is the predicted average default probability for all minority members in the test set?","868289d9":"# Question 6 What is the out of bag score for the model? ","1069375f":"# Question 2 Which ZIP code has the highest default rate in the training dataset?","cbd222a1":"# Question 1 What percentage of your training set loans are in default?","5aa3de4b":"# Question 10 Is the loan granting scheme (the cutoff, not the model) group unaware? \n### Yes the group is unaware. The cutoff point (50%) is the same for every category\/group.","7fb832ae":"# Question 8 What is the predicted average default probability for all non-minority members in the test set?","e33ebe58":"## Additional Info","eb485582":"# Question 5 What is the in-sample model accuracy? "}}