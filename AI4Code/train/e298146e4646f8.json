{"cell_type":{"cebae3f4":"code","5e23a150":"code","57660c3a":"code","a55a0aaa":"code","6bfbd9b3":"code","3fb0878e":"code","9a12386c":"code","5c116bdc":"code","687e5427":"code","d4c36736":"code","d29fa54c":"code","4aa02d59":"code","5c4d88fc":"code","9a1b919e":"code","ee9a27f0":"code","76cc7ae3":"code","0020c4fc":"code","9f926766":"code","5d185584":"code","1b1fe764":"code","acc88156":"code","360eccaf":"code","ecbfd16d":"code","29d6adee":"code","0b436b98":"code","be160501":"markdown","a6ae97e2":"markdown","5ede2cb6":"markdown","068148d1":"markdown"},"source":{"cebae3f4":"import numpy as np\nimport pandas as pd\nfrom sklearn.metrics import log_loss\nfrom sklearn.metrics import f1_score\nfrom collections import Counter\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.model_selection import KFold, StratifiedKFold\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn import preprocessing \nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom sklearn.decomposition import PCA\nfrom tqdm.notebook import tqdm\nimport gensim\nimport os\nos.listdir(\"..\/input\/ykc-cup-2nd\/\")","5e23a150":"train = pd.read_csv(\"..\/input\/ykc-cup-2nd\/train.csv\")\ntest = pd.read_csv(\"..\/input\/ykc-cup-2nd\/test.csv\")\nsub = pd.read_csv(\"..\/input\/ykc-cup-2nd\/sample_submission.csv\")\ntrain.shape, test.shape, sub.shape","57660c3a":"train.head()","a55a0aaa":"test.head()","6bfbd9b3":"## train\u3068test\u3092\u304f\u3063\u3064\u3051\u3066\u4e00\u62ec\u3067\u7279\u5fb4\u91cf\u4f5c\u6210\u3092\u3059\u308b\ndf = pd.concat([train, test])\ndf = df.reset_index(drop=True)\ndf.shape","3fb0878e":"target = \"department_id\" ## \u4e88\u6e2c\u5bfe\u8c61\n\nn_split = 5 ## cross validation\u306efold\u6570\nkfold = StratifiedKFold(n_splits=n_split, shuffle = True, random_state=42) # feature engineering\u3067\u4f7f\u3046\u304b\u3082\u3057\u308c\u306a\u3044\u306e\u3067\u5148\u306b\u7528\u610f","9a12386c":"df[\"product_name\"] = df[\"product_name\"].apply(lambda words : words.lower().replace(\",\", \"\").replace(\"&\", \"\").split(\" \"))\ndf.head()","5c116bdc":"model_names = [\"crawl-300d-2M\", \"crawl-300d-2M-subword\", \"wiki-news-300d-1M\", \"wiki-news-300d-1M-subword\"]","687e5427":"## \u8a13\u7df4\u6e08\u307f\u306e\u5358\u8a9e\u30d9\u30af\u30c8\u30eb\u3092\u8aad\u307f\u8fbc\u3093\u3067\uff0cproduct_name\u306b\u542b\u307e\u308c\u308b\u5358\u8a9e\u3092\u30d9\u30af\u30c8\u30eb\u306b\u5909\u63db\u3057\u3066\u5e73\u5747\u3092\u53d6\u308b\u3053\u3068\u3067\uff0c\u5404product_id\u306b\u5bfe\u3057\u3066\u7279\u5fb4\u91cf\u30d9\u30af\u30c8\u30eb\u3092\u4f5c\u6210\u3059\u308b\n\nfasttext_pretrain_cols = []\n\nfrom collections import defaultdict\nunused_words = defaultdict(int)\n\nfor model_name in model_names:\n    model = pd.read_pickle(f\"..\/input\/fasttext\/{model_name}.pkl\") \n\n    def to_mean_vec(x, model):\n        v = np.zeros(model.vector_size)\n        for w in x:\n            try:\n                v += model[w] ## \u5358\u8a9e\u304c\u8a13\u7df4\u6e08\u307f\u30e2\u30c7\u30eb\u306evocab\u306b\u3042\u3063\u305f\u3089\n            except:\n                unused_words[w] += 1 ## \u30d9\u30af\u30c8\u30eb\u304c\u5b58\u5728\u3057\u306a\u304b\u3063\u305f\u5358\u8a9e\u3092\u30e1\u30e2\n        v = v \/ (np.sqrt(np.sum(v ** 2)) + 1e-16) ## \u9577\u3055\u30921\u306b\u6b63\u898f\u5316\n        return v   \n    \n    def to_max_vec(x, model):\n        v = np.zeros(model.vector_size)\n        for w in x:\n            try:\n                v = np.maximum(v, model[w])\n            except:\n                pass\n            \n        return v\n        \n    mean_vecs = df[\"product_name\"].apply(lambda x : to_mean_vec(x, model))\n    mean_vecs = np.vstack(mean_vecs)\n    cols = [f\"fasttext_pretrain_{model_name}_mean_vec{k}\" for k in range(mean_vecs.shape[1])]\n    fasttext_pretrain_cols += cols\n    mean_vec_df = pd.DataFrame(mean_vecs, columns=cols)\n    df = pd.concat([df, mean_vec_df], axis = 1)\n    \n    max_vecs = df[\"product_name\"].apply(lambda x : to_max_vec(x, model))\n    max_vecs = np.vstack(max_vecs)\n    cols = [f\"fasttext_pretrain_{model_name}_max_vec{k}\" for k in range(max_vecs.shape[1])]\n    fasttext_pretrain_cols += cols\n    max_vec_df = pd.DataFrame(max_vecs, columns=cols)\n    df = pd.concat([df, max_vec_df], axis = 1)\ndf.head()","d4c36736":"## train\u3068test\u3092\u5206\u96e2\ntrain = df[~df[target].isna()]\ntest = df[df[target].isna()]","d29fa54c":"def to_weighted_count_vec(x, word_sets):\n    v = np.zeros(21)\n    \n    for w in x:\n        hits = []\n        for i in range(21):\n            if w in word_sets[i]:\n                hits.append(i)\n                \n        for i in hits:\n            v[i] += 1.0 \/ len(hits)\n            \n    return v\n\nweighted_count_cols = [f\"weighted_count_vec{k}\" for k in range(21)]","4aa02d59":"# train\n\ntrain_weighted_count = pd.DataFrame(index=train.index, columns=weighted_count_cols, dtype=np.float32)\n\nfor i_fold, (train_idx, valid_idx) in enumerate(kfold.split(train, train[target])):    \n    ## train data\n    tr = train.loc[train_idx]\n\n    ## valid data\n    va = train.loc[valid_idx]\n    \n    word_sets = [set(sum(tr[tr[\"department_id\"] == i][\"product_name\"], [])) for i in range(21)]\n    vecs = va[\"product_name\"].apply(lambda x : to_weighted_count_vec(x, word_sets))\n    vecs = np.vstack(vecs)\n    \n    vec_df = pd.DataFrame(vecs, index=va.index, columns=weighted_count_cols)\n    train_weighted_count.loc[valid_idx, :] = vec_df\n\ntrain = pd.concat([train, train_weighted_count], axis=1)","5c4d88fc":"train.head()","9a1b919e":"# test\n\ntest_weighted_count = pd.DataFrame(index=test.index, columns=weighted_count_cols, dtype=np.float32)\n\nword_sets = [set(sum(train[train[\"department_id\"] == i][\"product_name\"], [])) for i in range(21)]\nvecs = test[\"product_name\"].apply(lambda x : to_weighted_count_vec(x, word_sets))\nvecs = np.vstack(vecs)\ntest_weighted_count.loc[:, :] = pd.DataFrame(vecs, index=test.index, columns=weighted_count_cols)\ntest = pd.concat([test, test_weighted_count], axis=1)","ee9a27f0":"test.head()","76cc7ae3":"features = fasttext_pretrain_cols + weighted_count_cols + [\"order_rate\", \"order_dow_mode\", \"order_hour_of_day_mode\"]## \u4e88\u6e2c\u306b\u4f7f\u7528\u3059\u308b\u7279\u5fb4\u91cf\u306e\u540d\u524d","0020c4fc":"# normalization\nscaler = preprocessing.StandardScaler()\ntrain[features] = scaler.fit_transform(train[features])\ntest[features] = scaler.transform(test[features])","9f926766":"import math\nimport random\nfrom typing import List, NoReturn, Union, Tuple, Optional, Text, Generic, Callable, Dict\n\n# keras\nimport tensorflow as tf\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras import callbacks\nfrom tensorflow.keras import optimizers\nfrom tensorflow.keras import models\nfrom tensorflow.keras import losses\nfrom tensorflow.keras import utils\nfrom tensorflow.keras import layers\nimport tensorflow_addons as tfa\n\ndef seed_everything(seed : int) -> NoReturn :    \n    random.seed(seed)\n    np.random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    tf.random.set_seed(seed)\n\nseed_everything(1220)    \n\n# adapted from https:\/\/github.com\/ghmagazine\/kagglebook\/blob\/master\/ch06\/ch06-03-hopt_nn.py\nparams = {\n    'hidden_layers': 1,\n    'hidden_units': 128,\n    'hidden_activation': 'relu', \n    'lr': 1e-4,\n    'batch_size': 128,\n    'epochs': 100\n}\n    \ndef nn_model(L : int):\n    \"\"\"\n    NN hyperparameters and models\n    \n    :INPUT: \n    \n    :L: the number of features (int)\n    \"\"\"\n\n    # NN model architecture\n    n_neuron = params['hidden_units']\n\n    inputs = layers.Input(shape=(L, ), dtype='float32')\n    x = layers.Dense(n_neuron, activation=params['hidden_activation'])(inputs)\n    #x = layers.BatchNormalization()(x)\n\n    # stack more layers\n    for i in np.arange(params['hidden_layers'] - 1):\n        x = layers.Dense(n_neuron \/\/ (2 * (i+1)), activation=params['hidden_activation'])(x)\n        #x = layers.BatchNormalization()(x)\n    \n    # output\n    out = layers.Dense(21, activation='softmax', name = 'out')(x)\n    model = models.Model(inputs=inputs, outputs=out)\n\n    # compile\n    # loss = tfa.losses.SigmoidFocalCrossEntropy() # focal loss for imbalanced data\n    model.compile(loss=\"categorical_crossentropy\", optimizer=optimizers.Adam(lr=params['lr']))\n    \n    return model\n\n    # callbacks\n    history = model.fit(train_set['X'], train_set['y'], callbacks=[er, ReduceLR],\n                        epochs=params['epochs'], batch_size=params['batch_size'],\n                        validation_data=(val_set['X'], val_set['y']))        \n\n    return model, fi\n","5d185584":"## cross validation\npreds_test = []\nscores = []\noof = np.zeros((len(train), 21))\nfor i_fold, (train_idx, valid_idx) in enumerate(kfold.split(train, train[target])):\n    print(f\"--------fold {i_fold}-------\")\n    \n    ## train data\n    x_tr = train.loc[train_idx, features]\n    y_tr = train.loc[train_idx, target]\n\n    ## valid data\n    x_va = train.loc[valid_idx, features]\n    y_va = train.loc[valid_idx, target]\n\n    # train NN model\n    ## one-hot encoding y\n    ohe = preprocessing.OneHotEncoder(sparse=False, categories='auto')\n    y_tr_ohe = ohe.fit_transform(y_tr.values.reshape(-1, 1))\n    y_va_ohe = ohe.transform(y_va.values.reshape(-1, 1))\n\n    ## define NN model\n    nn = nn_model(len(features))\n    \n    ### callbacks\n    er = callbacks.EarlyStopping(patience=8, min_delta=params['lr'], restore_best_weights=True, monitor='val_loss')\n    ReduceLR = callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, verbose=1, epsilon=params['lr'], mode='min')\n    checkpoint_filepath = ''\n    model_checkpoint_callback = callbacks.ModelCheckpoint(filepath=f'mybestweight_fold{i_fold}.hdf5', save_weights_only=True, monitor='val_loss', save_best_only=True)\n    \n    nn.fit(x_tr.values, y_tr_ohe, callbacks=[er, ReduceLR, model_checkpoint_callback], epochs=params['epochs'], batch_size=params['batch_size'],\n                        validation_data=(x_va.values, y_va_ohe))  \n    \n    def predict_proba(x):\n        return nn.predict(x)\n    \n    ## predict on valid\n    pred_val = predict_proba(x_va)\n    oof[valid_idx] += pred_val\n    \n    ## evaluate\n    score = {\n        \"logloss\"  : log_loss(y_va, pred_val),\n        \"f1_micro\" : f1_score(y_va, np.argmax(pred_val, axis = 1), average = \"micro\")}\n    print(score)\n    scores.append(score)\n    \n    ## predict on test\n    pred_test = predict_proba(test[features])\n    preds_test.append(pred_test)","1b1fe764":"score_df = pd.DataFrame(scores)\nscore_df","acc88156":"score_df.mean()","360eccaf":"oof_df = pd.DataFrame(oof)\noof_df.to_csv(\"oof_nn.csv\", index = False)","ecbfd16d":"for i in range(len(preds_test)):\n    pred_df = pd.DataFrame(preds_test[i])\n    pred_df.to_csv(f\"pred_{i}_nn.csv\", index = False)","29d6adee":"## cv\u306e\u5404fold\u3067\u8a08\u7b97\u3057\u305f\u4e88\u6e2c\u5024\u306e\u5e73\u5747\u3092\u6700\u7d42\u7684\u306a\u4e88\u6e2c\u5024\u306b\npred_test_final = np.array(preds_test).mean(axis = 0)\npred_test_final = np.argmax(pred_test_final, axis = 1)","0b436b98":"sub[\"department_id\"] = pred_test_final\nsub.to_csv(\"submission_nn.csv\", index = False)\nsub.head()","be160501":"## train","a6ae97e2":"## submission","5ede2cb6":"## feature engineering","068148d1":"## read data "}}