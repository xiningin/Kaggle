{"cell_type":{"d936f137":"code","baa28e07":"code","0ae78ea4":"code","badc291a":"code","5e5c7a43":"code","7a9df05a":"code","9933c375":"code","9bd4c380":"code","a88f4f95":"code","cd26cff1":"code","e4cf6d35":"code","67d8f6d1":"code","f9ba281f":"code","11ba9748":"code","8203504f":"code","cb4ddf43":"code","8fd9d927":"code","529b4780":"code","cd2360a8":"code","69842409":"code","58061792":"code","5d56deba":"code","deee688a":"code","da47fc3d":"code","58068597":"markdown","c93cc797":"markdown","d04e0d47":"markdown","857237f6":"markdown","347111e7":"markdown","39d7b9e3":"markdown","e9e07238":"markdown","5dee7e50":"markdown"},"source":{"d936f137":"import warnings\nwarnings.filterwarnings('ignore')\nimport pandas as pd\nimport numpy as np\nimport random\nimport matplotlib.pyplot as plt\nimport gc\nimport os\nfrom tqdm.notebook import tqdm\nfrom sklearn.preprocessing import OrdinalEncoder\n\nimport torch\nimport pandas as pd\nimport numpy as np\nimport gc\nimport math\nimport datetime\nfrom sklearn import preprocessing\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom scipy.sparse import csr_matrix\nfrom sklearn.metrics import mean_squared_error\n\nimport os\nimport time\nimport tqdm\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import TensorDataset, DataLoader,Dataset\nfrom tqdm import tqdm_notebook as tqdm\n\ndevice = torch.device('cuda')\n# device = torch.device('cpu')\n\nNUM_ITEMS = 30490\nDAYS_PRED = 28\n\n\ndef seed_everything(seed=42):\n    random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n\nseed_everything()    ","baa28e07":"path = \"..\/input\/m5-forecasting-accuracy\"\ncalendar = pd.read_csv(os.path.join(path, \"calendar.csv\"))\nselling_prices = pd.read_csv(os.path.join(path, \"sell_prices.csv\"))\nsample_submission = pd.read_csv(os.path.join(path, \"sample_submission.csv\"))\nsales = pd.read_csv(os.path.join(path, \"sales_train_validation.csv\"))","0ae78ea4":"def reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() \/ 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) \/ start_mem))\n    return df","badc291a":"def prep_calendar(df):\n    df = df.drop([\"date\", \"weekday\"], axis=1)\n    df = df.assign(d = df.d.str[2:].astype(int))\n    df = df.fillna(\"missing\")\n    cols = list(set(df.columns) - {\"wm_yr_wk\", \"d\"})\n    df[cols] = OrdinalEncoder(dtype=\"int\").fit_transform(df[cols])\n    df = reduce_mem_usage(df)\n    return df\n\ndef prep_selling_prices(df):\n    gr = df.groupby([\"store_id\", \"item_id\"])[\"sell_price\"]\n    df[\"sell_price_rel_diff\"] = gr.pct_change()\n    df[\"sell_price_roll_sd7\"] = gr.transform(lambda x: x.rolling(7).std())\n    df[\"sell_price_cumrel\"] = (gr.shift(0) - gr.cummin()) \/ (1 + gr.cummax() - gr.cummin())\n    df[\"price_unique\"] = gr.transform('nunique')\n    df = reduce_mem_usage(df)\n    return df\n\ndef reshape_sales(df, drop_d = None):\n    if drop_d is not None:\n        df = df.drop([\"d_\" + str(i + 1) for i in range(drop_d)], axis=1)\n    df = df.assign(id=df.id.str.replace(\"_validation\", \"\"))\n    df = df.reindex(columns=df.columns.tolist() + [\"d_\" + str(1913 + i + 1) for i in range(2 * 28)])\n    df = df.melt(id_vars=[\"id\", \"item_id\", \"dept_id\", \"cat_id\", \"store_id\", \"state_id\"],\n                 var_name='d', value_name='demand')\n    df = df.assign(d=df.d.str[2:].astype(\"int16\"))\n    return df\n\ndef prep_sales(df):\n    df['lag_t28'] = df.groupby(['id'])['demand'].transform(lambda x: x.shift(28))\n    df['rolling_mean_t7'] = df.groupby(['id'])['demand'].transform(lambda x: x.shift(28).rolling(7).mean())\n    df['rolling_mean_t28'] = df.groupby(['id'])['demand'].transform(lambda x: x.shift(28).rolling(28).mean())\n    df['rolling_mean_t56'] = df.groupby(['id'])['demand'].transform(lambda x: x.shift(28).rolling(56).mean())\n    df['rolling_mean_t84'] = df.groupby(['id'])['demand'].transform(lambda x: x.shift(28).rolling(84).mean())\n    df['rolling_mean_t168'] = df.groupby(['id'])['demand'].transform(lambda x: x.shift(28).rolling(168).mean())\n    df['rolling_std_t7'] = df.groupby(['id'])['demand'].transform(lambda x: x.shift(28).rolling(7).std())\n    df['rolling_std_t28'] = df.groupby(['id'])['demand'].transform(lambda x: x.shift(28).rolling(28).std())\n\n    # Remove rows with NAs except for submission rows. rolling_mean_t168 was selected as it produces most missings\n    df = df[(df.d >= 1914) | (pd.notna(df.rolling_mean_t168))]\n    df = reduce_mem_usage(df)\n\n    return df\n","5e5c7a43":"calendar = prep_calendar(calendar)\nselling_prices = prep_selling_prices(selling_prices)\nsales = reshape_sales(sales, 1000)\nsales = prep_sales(sales)\n\nsales = sales.merge(calendar, how=\"left\", on=\"d\")\ngc.collect()\n\nsales = sales.merge(selling_prices, how=\"left\", on=[\"wm_yr_wk\", \"store_id\", \"item_id\"])\nsales.drop([\"wm_yr_wk\"], axis=1, inplace=True)\ngc.collect()\nsales.head()\n\ndel selling_prices","7a9df05a":"sales.info()","9933c375":"cat_id_cols = [\"item_id\", \"dept_id\", \"store_id\", \"cat_id\", \"state_id\"]\ncat_cols = cat_id_cols + [\"wday\", \"month\", \"year\", \"event_name_1\", \n                          \"event_type_1\", \"event_name_2\", \"event_type_2\"]\n\nfor i, v in enumerate(tqdm(cat_id_cols)):\n    sales[v] = OrdinalEncoder(dtype=\"int\").fit_transform(sales[[v]])\n\nsales = reduce_mem_usage(sales)\nsales.head()\ngc.collect()","9bd4c380":"num_cols = [\"sell_price\", \"sell_price_rel_diff\", \"sell_price_roll_sd7\", \"sell_price_cumrel\",\n            \"lag_t28\", \"rolling_mean_t7\", \"rolling_mean_t28\", \"rolling_mean_t56\", \n            \"rolling_mean_t84\", \"rolling_mean_t168\", \"rolling_std_t7\", \"rolling_std_t28\"]\nbool_cols = [\"snap_CA\", \"snap_TX\", \"snap_WI\"]\n\ndense_cols = num_cols + bool_cols\n\n# Need to do column by column due to memory constraints\nfor i, v in enumerate(tqdm(num_cols)):\n    sales[v] = sales[v].fillna(sales[v].median())\n    \nsales.head()","a88f4f95":"test = sales[sales.d >= 1914]\ntest = test.assign(id=test.id + \"_\" + np.where(test.d <= 1941, \"validation\", \"evaluation\"),\n                   F=\"F\" + (test.d - 1913 - 28 * (test.d > 1941)).astype(\"str\"))\ntest.head()\ngc.collect()","cd26cff1":"def make_X(df):\n    X = {\"dense1\": df[dense_cols].to_numpy()}\n    for i, v in enumerate(cat_cols):\n        X[v] = df[[v]].to_numpy()\n    return X\n\n# Submission data\nX_test = make_X(test)\n\n\ntotal_train = sales[sales.d < 1914]\ntrain_idx, val_idx = train_test_split(total_train.index, test_size=0.05, \n                                      random_state=42, shuffle=True)\n\n\n\nvalid = (make_X(total_train.iloc[val_idx]),\n         total_train[\"demand\"].iloc[val_idx])\n\nX_train = make_X(total_train.iloc[train_idx])\ny_train = total_train[\"demand\"].iloc[train_idx]\n                             \ndel sales, total_train\ngc.collect()","e4cf6d35":"class M5Loader:\n    def __init__(self, X, y, shuffle=True, batch_size=10000, cat_cols=[]):\n        self.X_cont = X[\"dense1\"]\n        self.X_cat = np.concatenate([X[k] for k in cat_cols], axis=1)\n        self.y = y\n\n        self.shuffle = shuffle\n        self.batch_size =batch_size\n        self.n_conts = self.X_cont.shape[1]\n        self.len = self.X_cont.shape[0]\n        n_batches, remainder = divmod(self.len, self.batch_size)\n        \n        if remainder > 0:\n            n_batches += 1\n        self.n_batches = n_batches\n        self.remainder = remainder #for debugging\n        \n        self.idxes = np.array([i for i in range(self.len)])\n\n    def __iter__(self):\n        self.i = 0\n        if self.shuffle:\n            ridxes = self.idxes\n            np.random.shuffle(ridxes)\n            self.X_cat = self.X_cat[[ridxes]]\n            self.X_cont = self.X_cont[[ridxes]]\n            if self.y is not None:\n                self.y = self.y[[ridxes]]\n                \n        return self\n\n    def __next__(self):\n        if self.i  >= self.len:\n            raise StopIteration\n            \n        if self.y is not None:\n            y = torch.FloatTensor(self.y[self.i:self.i+self.batch_size].astype(np.float32))\n        else:\n            y = None\n            \n        xcont = torch.FloatTensor(self.X_cont[self.i:self.i+self.batch_size])\n        xcat = torch.LongTensor(self.X_cat[self.i:self.i+self.batch_size])\n        \n        batch = (xcont, xcat, y)\n        self.i += self.batch_size\n        return batch\n\n    def __len__(self):\n        return self.n_batches           ","67d8f6d1":"bs = 10000\nshuffle = True\ntrain_loader = M5Loader(X_train, y_train.values, cat_cols=cat_cols, batch_size=bs, shuffle=shuffle)\nval_loader = M5Loader(valid[0], valid[1].values, cat_cols=cat_cols, batch_size=bs, shuffle=shuffle)","f9ba281f":"# Sorry for harcoding these but this saves RAM\nuniques = [3049, 7, 10, 3, 3, 7, 12, 6, 31, 5, 5, 5]\ndims = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\nemb_dims = [(x, y) for x, y in zip(uniques, dims)]\nprint(emb_dims)\nn_cont = train_loader.n_conts","11ba9748":"class ZeroBalance_RMSE(nn.Module):\n    def __init__(self, penalty=1.12):\n        super().__init__()\n        self.penalty = penalty\n        \n    def forward(self, y_pred, y_true):\n        y_pred = y_pred.squeeze()\n        y_true = torch.FloatTensor(y).to(device)\n        sq_error = torch.where(y_true==0, (y_true-y_pred)**2, self.penalty*(y_true-y_pred)**2)\n        return torch.sqrt(torch.mean(sq_error))\n\nclass RMSE(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.mse = nn.MSELoss()\n    \n    def forward(self, y_pred, y_true):\n        y_pred = y_pred.squeeze()\n        y_true = torch.FloatTensor(y).to(device)\n        return torch.sqrt(self.mse(y_pred, y_true))    \n    \nclass MSE(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.mse = nn.MSELoss()\n    \n    def forward(self, y_pred, y_true):\n        y_pred = y_pred.squeeze()\n        y_true = torch.FloatTensor(y).to(device)\n        return self.mse(y_pred, y_true)  \n\ndef rmse_metric(y_pred, y_true):\n    y_pred = np.array(y_pred)\n    y_true = np.array(y_true)\n    return np.sqrt(np.mean((y_pred-y_true)**2))     ","8203504f":"def init_weights(m):\n    if type(m) == nn.Linear:\n        torch.nn.init.xavier_uniform(m.weight)\n        m.bias.data.fill_(0.01)\n        \nclass LinearBlock(nn.Module):\n    def __init__(self, inp, out):\n        super().__init__()\n        self.block = nn.Sequential(\n                            nn.Linear(inp, out),\n#                             nn.LeakyReLU(),\n                            nn.ReLU(),\n#                             nn.Tanh(),\n        )\n    \n    def forward(self, x):\n        return self.block(x)\n\nclass M5Net(nn.Module):\n    def __init__(self, emb_dims, n_cont, device=device):\n        super().__init__()\n        self.device = device\n\n        # Embedding layers\n        self.emb_layers = nn.ModuleList([nn.Embedding(x, y) for x, y in emb_dims])\n        n_embs = sum([y for x, y in emb_dims])\n        \n        self.n_embs = n_embs\n        self.n_cont = n_cont\n        inp_dim = n_embs + n_cont\n        self.inp_dim = inp_dim\n        \n        hidden_dim = 200\n\n        self.fn = nn.Sequential(\n                 LinearBlock(inp_dim, hidden_dim),\n                 LinearBlock(hidden_dim, hidden_dim\/\/2),\n                 LinearBlock(hidden_dim\/\/2, hidden_dim\/\/4),\n                 LinearBlock(hidden_dim\/\/4, hidden_dim\/\/8),\n        )          \n        \n        self.out = nn.Linear(hidden_dim\/\/8, 1)\n\n        self.fn.apply(init_weights)\n        self.out.apply(init_weights)\n        \n\n    def encode_and_combine_data(self, cont_data, cat_data):\n        xcat = [el(cat_data[:, k]) for k, el in enumerate(self.emb_layers)]\n        xcat = torch.cat(xcat, 1)\n        x = torch.cat([xcat, cont_data], 1)\n        return x   \n    \n    def forward(self, cont_data, cat_data):\n        cont_data = cont_data.to(self.device)\n        cat_data = cat_data.to(self.device)\n        inputs = self.encode_and_combine_data(cont_data, cat_data)\n        x = self.fn(inputs)\n        x = self.out(x)\n        return x","cb4ddf43":"model = M5Net(emb_dims, n_cont).to(device)","8fd9d927":"model","529b4780":"epochs = 45\nlr = 0.0002\ncriterion = ZeroBalance_RMSE()\n\ntorch.manual_seed(42)\nmodel = M5Net(emb_dims, n_cont).to(device)\noptimizer = torch.optim.RMSprop(model.parameters(), lr=lr)\n\nscheduler = torch.optim.lr_scheduler.MultiStepLR(\n                    optimizer, [20, 35, 40], gamma=0.5, \n                    last_epoch=-1)","cd2360a8":"train_losses = []\nval_losses = []\n\nfor epoch in tqdm(range(epochs)):\n    train_loss, val_loss = 0, 0\n\n    #Training phase\n    model.train()\n    bar = tqdm(train_loader)\n    \n    for i, (X_cont, X_cat, y) in enumerate(bar):\n        optimizer.zero_grad()\n        out = model(X_cont, X_cat)\n        loss = criterion(out, y)   \n        loss.backward()\n        optimizer.step()\n        \n        with torch.no_grad():\n            train_loss += loss.item()\/len(train_loader)\n            bar.set_description(f\"{loss.item():.3f}\")\n    \n    print(f\"Running Train loss: {train_loss}\")\n    \n    #Validation phase      \n    with torch.no_grad():\n        model.eval()\n        for phase in [\"train\", \"valid\"]:\n            rloss = 0\n            if phase == \"train\":\n                loader = train_loader\n            else:\n                loader = val_loader\n            \n            y_true = []\n            y_pred = []\n            \n            for i, (X_cont, X_cat, y) in enumerate(loader):\n                out = model(X_cont, X_cat)\n                loss = criterion(out, y)\n                rloss += loss.item()\/len(loader)\n                y_pred += list(out.detach().cpu().numpy().flatten())\n                y_true += list(y.cpu().numpy())\n                \n            rrmse = rmse_metric(y_pred, y_true)    \n            print(f\"[{phase}] Epoch: {epoch} | Loss: {rloss:.4f} | RMSE: {rrmse:.4f}\")\n      \n    train_losses.append(train_loss)    \n    val_losses.append(rloss)        \n    scheduler.step()","69842409":"import pickle\ndef save(x, fname):\n    with open(fname, \"wb\") as handle:\n        pickle.dump(x, handle)\n        \nsave(model, \"model.pth\")","58061792":"plt.plot(train_losses)\nplt.plot(val_losses)\nplt.show()","5d56deba":"test_loader = M5Loader(X_test, y=None, cat_cols=cat_cols, batch_size=bs, shuffle=False)\npred = []\nwith torch.no_grad():\n        model.eval()\n        for i, (X_cont, X_cat, y) in enumerate(tqdm(test_loader)):\n            out = model(X_cont, X_cat)\n            pred += list(out.cpu().numpy().flatten())    \npred = np.array(pred)            ","deee688a":"test[\"demand\"] = pred.clip(0)\nsubmission = test.pivot(index=\"id\", columns=\"F\", values=\"demand\").reset_index()[sample_submission.columns]\nsubmission = sample_submission[[\"id\"]].merge(submission, how=\"left\", on=\"id\")\nsubmission.head()","da47fc3d":"submission.to_csv(\"submission.csv\", index=False)","58068597":"## Architecture","c93cc797":"# Model\n## Loss functions","d04e0d47":"# Make Predictions","857237f6":"# Fast DataLoader\nI am not using the pytorch dataset and dataloader classes because they are very slow for large batch sizes of tabular data. Therefore, I have created a custom dataloader of my own that creates and return batches of shuffled data. ","347111e7":"# Train","39d7b9e3":"# Pytorch DNN\n- A simple, timeseries agnostic deep neural network using pytorch framework. \n- This kernel can be considered a pytorch port of this amazing [kernel](https:\/\/www.kaggle.com\/mayer79\/m5-forecast-keras-with-categorical-embeddings-v2) by [MichaelMayer](https:\/\/www.kaggle.com\/mayer79) with minor changes.\n- Embedding categorical features using `nn.Embedding`.\n- The Neural net here has no time series based component which is a very bad idea, this is just started code.","e9e07238":"# Make data","5dee7e50":"# Imports"}}