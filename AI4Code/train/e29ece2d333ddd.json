{"cell_type":{"922db879":"code","fb58e858":"code","2288ca67":"code","5121c189":"code","5ec63208":"code","4f36ba33":"code","f2fc4c75":"code","7e9db32e":"code","2d601d53":"code","7ec33ff1":"code","85d9ae04":"code","9c344482":"code","812744fd":"code","a3233160":"code","5b678495":"code","86e7bd99":"markdown","d8674844":"markdown","52fc660c":"markdown","a5845c8e":"markdown","c20a4103":"markdown","5efba42f":"markdown","d29e0ce8":"markdown","689484b5":"markdown","66f4cf3a":"markdown"},"source":{"922db879":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nsns.set_style('darkgrid')\n\nfrom sklearn.datasets import fetch_20newsgroups\nfrom wordcloud import WordCloud, STOPWORDS\nfrom collections import Counter\nfrom tqdm.auto import tqdm","fb58e858":"dataset = fetch_20newsgroups(subset='all')\n\nX = pd.Series(dataset['data'])\ny = pd.Series(dataset['target'])","2288ca67":"fig, ax = plt.subplots(figsize=(20, 6))\nfig.suptitle('Target Class Distribution', fontsize=24)\ny.apply(lambda i: dataset['target_names'][i]).value_counts().plot.pie()","5121c189":"fig, ax = plt.subplots(figsize=(20, 6))\nwordcloud = WordCloud(height=600, width=2000, stopwords=STOPWORDS).generate(' '.join((X.values)))\nax.imshow(wordcloud)\nax.set_title('Word Cloud', fontsize=24)\nax.xaxis.set_visible(False)\nax.yaxis.set_visible(False)","5ec63208":"import re\nX = X.apply(lambda text: re.sub(r\"\\s+\", \" \", re.sub(r\"[^A-Za-z]\", \" \", text)).lower())\nX = X.apply(lambda text: ' '.join(x for x in text.split() if len(x) > 2) )","4f36ba33":"fig, ax = plt.subplots(5, 4, figsize=(20, 14))\nfig.suptitle('Class wise multiset of words', fontsize=24)\nfor i in range(20):\n    u = Counter((' '.join((X[y != i].values))).split())\n    a = Counter((' '.join((X[y == i].values))).split())\n    wordcloud = WordCloud(height=600, width=1000, stopwords=STOPWORDS).generate_from_frequencies((a-u))\n    ax[i\/\/4][i%4].imshow(wordcloud)\n    ax[i\/\/4][i%4].set_title(f'{dataset[\"target_names\"][i]}', fontsize=18)\n    ax[i\/\/4][i%4].xaxis.set_visible(False)\n    ax[i\/\/4][i%4].yaxis.set_visible(False) ","f2fc4c75":"def plot_model_learning(history, title):\n    fig, ax = plt.subplots(2, 1, figsize=(20, 8))\n    df = pd.DataFrame(history.history)\n    df[['accuracy', 'val_accuracy']].plot(ax=ax[0])\n    df[['loss', 'val_loss']].plot(ax=ax[1])\n    ax[0].set_title('Model Accuracy', fontsize=12)\n    ax[1].set_title('Model Loss', fontsize=12)\n    fig.suptitle(f'{title}: Model Metrics', fontsize=18)","7e9db32e":"seq_length = int(round(X.apply(lambda x: len(str(x).split())).quantile(0.75)))\nprint('Sequence Length', seq_length)\nvocab_size = len(set(' '.join(X.values).split()))\nprint('Vocab size', vocab_size)\nembedding_vecor_length = 200\nMAX_EPOCHS = 100","2d601d53":"from keras.models import Sequential\nfrom keras.layers import Embedding, SpatialDropout1D, SimpleRNN, LSTM, GRU, Bidirectional, Dense \nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing import sequence\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\nfrom keras.utils import plot_model\nfrom keras.initializers import Constant","7ec33ff1":"tokenizer = Tokenizer(num_words=vocab_size, oov_token='<unw>')\ntokenizer.fit_on_texts(X)\nX = tokenizer.texts_to_sequences(X)\nX = sequence.pad_sequences(X, maxlen=seq_length)\ny = pd.get_dummies(y)","85d9ae04":"es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=7)\nfilepath = \"model.h5\"\nckpt = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\nrlp = ReduceLROnPlateau(monitor='loss', patience=3, verbose=1)\n\nmodel = Sequential()\nmodel.add(Embedding(vocab_size, embedding_vecor_length, input_length=seq_length))\nmodel.add(SpatialDropout1D(0.2))\nmodel.add(Bidirectional(GRU(128)))\nmodel.add(Dense(20, activation='sigmoid'))\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n\nmodel.summary()\nplot_model(model, show_shapes=True)\n\nhistory = model.fit(x=X, y=y, validation_split=0.1,\n    callbacks=[es, ckpt, rlp], epochs=MAX_EPOCHS\n)\nplot_model_learning(history, 'Bidirectional GRU')","9c344482":"from gensim.models.keyedvectors import KeyedVectors\n\ndef prepare_embedding_index(embedding_name):\n    embeddings_index = {}\n    if embedding_name == 'Word2Vec':\n        embeddings_index = KeyedVectors.load_word2vec_format(\n            '..\/input\/googlenewsvectorsnegative300\/GoogleNews-vectors-negative300.bin', binary=True\n        )\n        embedding_dim = 300\n\n    else:\n        if embedding_name == 'Glove':\n            f = open('..\/input\/glove-global-vectors-for-word-representation\/glove.6B.200d.txt')\n            embedding_dim = 200\n        elif embedding_name == 'fastText':\n            f = open('..\/input\/fasttext-wikinews\/wiki-news-300d-1M.vec')\n            embedding_dim = 300\n            \n        for line in tqdm(f):\n            values = line.split()\n            word = values[0]\n            coefs = np.asarray(values[1:], dtype='float32')\n            embeddings_index[word] = coefs\n    \n    return embeddings_index, embedding_dim\n    \ndef prepare_embedding_matrix(embeddings_index, embedding_dim, vocab_size, tokenizer):\n    count_known = 0\n    count_unknown = 0\n        \n    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n\n    for word, i in tqdm(tokenizer.word_index.items()):\n        if i >= vocab_size:\n            continue\n        embedding_vector = None\n        try:\n            embedding_vector = embeddings_index[word]\n        except:\n            pass\n        if embedding_vector is not None:\n            embedding_matrix[i] = embedding_vector\n            count_known += 1\n        else:\n            embedding_matrix[i] = np.random.randn(embedding_dim)\n            count_unknown += 1\n\n    print(f'{count_known} known vectors\\n{count_unknown} random vectors') \n    return embedding_matrix","812744fd":"es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=7)\nfilepath = \"model_glove.h5\"\nckpt = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\nrlp = ReduceLROnPlateau(monitor='loss', patience=3, verbose=1)\n\nembedding_matrix = prepare_embedding_matrix(\n    *prepare_embedding_index('Glove'), vocab_size=vocab_size, tokenizer=tokenizer\n)\n\nmodel = Sequential()\nmodel.add(Embedding(\n    vocab_size, embedding_matrix.shape[1], embeddings_initializer=Constant(embedding_matrix), \n    input_length=seq_length, trainable=True\n))\nmodel.add(Bidirectional(GRU(128)))\nmodel.add(Dense(20, activation='sigmoid'))\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n\nmodel.summary()\nplot_model(model, show_shapes=True)\n\nhistory = model.fit(x=X, y=y, validation_split=0.1,\n    callbacks=[es, ckpt, rlp], epochs=MAX_EPOCHS \n)\nplot_model_learning(history, 'GloVe')","a3233160":"es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=7)\nfilepath = \"model_fastText.h5\"\nckpt = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\nrlp = ReduceLROnPlateau(monitor='loss', patience=3, verbose=1)\n\nembedding_matrix = prepare_embedding_matrix(\n    *prepare_embedding_index('fastText'), vocab_size=vocab_size, tokenizer=tokenizer\n)\n\nmodel = Sequential()\nmodel.add(Embedding(\n    vocab_size, embedding_matrix.shape[1], embeddings_initializer=Constant(embedding_matrix), \n    input_length=seq_length, trainable=True\n))\nmodel.add(Bidirectional(GRU(128)))\nmodel.add(Dense(20, activation='sigmoid'))\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n\nmodel.summary()\nplot_model(model, show_shapes=True)\n\nhistory = model.fit(x=X, y=y, validation_split=0.1,\n    callbacks=[es, ckpt, rlp], epochs=MAX_EPOCHS \n)\nplot_model_learning(history, 'fastText')","5b678495":"es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=7)\nfilepath = \"model_word2vec.h5\"\nckpt = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\nrlp = ReduceLROnPlateau(monitor='loss', patience=3, verbose=1)\n\nembedding_matrix = prepare_embedding_matrix(\n    *prepare_embedding_index('Word2Vec'), vocab_size=vocab_size, tokenizer=tokenizer\n)\n\nmodel = Sequential()\nmodel.add(Embedding(\n    vocab_size, embedding_matrix.shape[1], embeddings_initializer=Constant(embedding_matrix), \n    input_length=seq_length, trainable=True\n))\nmodel.add(Bidirectional(GRU(128)))\nmodel.add(Dense(20, activation='sigmoid'))\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n\nmodel.summary()\nplot_model(model, show_shapes=True)\n\nhistory = model.fit(x=X, y=y, validation_split=0.1,\n    callbacks=[es, ckpt, rlp], epochs=MAX_EPOCHS \n)\nplot_model_learning(history, 'Word2Vec')","86e7bd99":"# Recurrent Neural Network\n\nA recurrent neural network (RNN) is a class of artificial neural networks where connections between nodes form a directed graph along a temporal sequence. This allows it to exhibit temporal dynamic behavior. Derived from feedforward neural networks, RNNs can use their internal state (memory) to process variable length sequences of inputs.This makes them applicable to tasks such as Time Series Modellig, Audio\/Speech Processing, Natural Language Processing\n\n![img](https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/b\/b5\/Recurrent_neural_network_unfold.svg\/1920px-Recurrent_neural_network_unfold.svg.png)\n<center>Unfolded basic recurrent neural network<\/center>\n<br>\n\nTaking the simplest form of a recurrent neural network, the activation function is tanh, the weight at the recurrent neuron is Whh and the weight at the input neuron is Wxh, we can write the equation for the state at time t as:\n\n![img](https:\/\/cdn.analyticsvidhya.com\/wp-content\/uploads\/2017\/12\/06005300\/eq2.png)\n\nThe Recurrent neuron in this case is just taking the immediate previous state into consideration. For longer sequences the equation can involve multiple such states. Once the final state is calculated we can go on to produce the output\n\nNow, once the current state is calculated we can calculate the output state as:\n\n![img](https:\/\/cdn.analyticsvidhya.com\/wp-content\/uploads\/2017\/12\/06005750\/outeq.png)\n\nIn conclusion:\n\n* A single time step of the input is supplied to the network i.e. xt is supplied to the network\n* We then calculate its current state using a combination of the current input and the previous state i.e. we calculate ht\n* The current ht becomes ht-1 for the next time step\n* We can go as many time steps as the problem demands and combine the information from all the previous states\n* Once all the time steps are completed the final current state is used to calculate the output yt\n* The output is then compared to the actual output and the error is generated\n* The error is then backpropagated to the network to update the weights(we shall go into the details of backpropagation in further sections) and the network is trained\n\n\n## Embedding\n\nMachine learning models take vectors (arrays of numbers) as input. When working with text, the first thing you must do is come up with a strategy to convert strings to numbers (or to \"vectorize\" the text) before feeding it to the model.\n\n**One-hot encodings**\n\nAs a first idea, you might \"one-hot\" encode each word in your vocabulary. Consider the sentence \"The cat sat on the mat\". The vocabulary (or unique words) in this sentence is (cat, mat, on, sat, the). To represent each word, you will create a zero vector with length equal to the vocabulary, then place a one in the index that corresponds to the word. This approach is shown in the following diagram.\n\n![img](https:\/\/github.com\/tensorflow\/docs\/blob\/master\/site\/en\/tutorials\/text\/images\/one-hot.png?raw=1)\n\nThis approach is inefficient. A one-hot encoded vector is sparse (meaning, most indices are zero). Imagine you have 10,000 words in the vocabulary. To one-hot encode each word, you would create a vector where 99.99% of the elements are zero.\n\n**Encode each word with a unique number**\n\nA second approach you might try is to encode each word using a unique number. Continuing the example above, you could assign 1 to \"cat\", 2 to \"mat\", and so on. You could then encode the sentence \"The cat sat on the mat\" as a dense vector like [5, 1, 4, 3, 5, 2]. This appoach is efficient. Instead of a sparse vector, you now have a dense one (where all elements are full).\n\nThere are two downsides to this approach, however:\n* The integer-encoding is arbitrary (it does not capture any relationship between words).\n* An integer-encoding can be challenging for a model to interpret. A linear classifier, for example, learns a single weight for each feature. Because there is no relationship between the similarity of any two words and the similarity of their encodings, this feature-weight combination is not meaningful.\n\n**Word embeddings**\n\nWord embeddings give us a way to use an efficient, dense representation in which similar words have a similar encoding. Importantly, you do not have to specify this encoding by hand. An embedding is a dense vector of floating point values (the length of the vector is a parameter you specify). Instead of specifying the values for the embedding manually, they are trainable parameters (weights learned by the model during training, in the same way a model learns weights for a dense layer). It is common to see word embeddings that are 8-dimensional (for small datasets), up to 1024-dimensions when working with large datasets. A higher dimensional embedding can capture fine-grained relationships between words, but takes more data to learn.\n\n![img](https:\/\/github.com\/tensorflow\/docs\/blob\/master\/site\/en\/tutorials\/text\/images\/embedding2.png?raw=1)\n\n## Bidirectional RNNs\nFor sequences other than time series (e.g. text), it is often the case that a RNN model can perform better if it not only processes sequence from start to end, but also backwards. For example, to predict the next word in a sentence, it is often useful to have the context around the word, not only just the words that come before it.\n\nReference: \n* [Introduction to Recurrent Neural Networks](https:\/\/www.analyticsvidhya.com\/blog\/2017\/12\/introduction-to-recurrent-neural-networks\/)\n* [Word Embeddins](https:\/\/www.tensorflow.org\/tutorials\/text\/word_embeddings)\n* [Recurrent Neural Networks (RNN) with Keras](https:\/\/www.tensorflow.org\/guide\/keras\/rnn)\n\n","d8674844":"# Text Cleaning\n* remove all punctuation marks and numbers from the documents and lower case the text\n* remove words with less than 3 characters","52fc660c":"## Gated Recurrent Unit\n\nThe GRU is the newer generation of Recurrent Neural networks and is pretty similar to an LSTM. GRU\u2019s got rid of the cell state and used the hidden state to transfer information. It also only has two gates, a reset gate and update gate which adjust the incorporation of new input with the previous memory and an update gate that controls the preservation of the precious memory are introduced. These adaptively control how much each hidden unit remembers or forgets while reading\/generating a sequence.\n\nThe update gate acts similar to the forget and input gate of an LSTM. It decides what information to throw away and what new information to add.\n\nThe reset gate is another gate is used to decide how much past information to forget.\n\n![img](https:\/\/image.slidesharecdn.com\/nlpdl06forslideshareenghelvetica-160706022723\/95\/recent-progress-in-rnn-and-nlp-5-638.jpg?cb=1467843604)\n\nReference:\n[Long Short Term Memory(LSTM) and Gated Recurrent Units(GRU)](https:\/\/prvnk10.medium.com\/long-short-term-memory-lstm-and-gated-recurrent-units-gru-240d8a62db9)","a5845c8e":"## Long Short Term Memory\n\nIn theory, RNNs are absolutely capable of handling long-term dependencies. A human could carefully pick parameters for them to solve toy problems of this form. Sadly, in practice, RNNs don\u2019t seem to be able to learn them. \n\n![img](https:\/\/colah.github.io\/posts\/2015-08-Understanding-LSTMs\/img\/LSTM3-SimpleRNN.png)\n<center>The repeating module in a standard RNN contains a single layer.<\/center>\n\nLSTMs also have this chain like structure, but the repeating module has a different structure. Instead of having a single neural network layer, there are four, interacting in a very special way.\n\n![img](https:\/\/colah.github.io\/posts\/2015-08-Understanding-LSTMs\/img\/LSTM3-chain.png)\n<center>The repeating module in an LSTM contains four interacting layers.<\/center>\n<br><br>\n\nThe key to LSTMs is the cell state, the horizontal line running through the top of the diagram. The cell state is kind of like a conveyor belt. It runs straight down the entire chain, with only some minor linear interactions. It\u2019s very easy for information to just flow along it unchanged.\n\nThe LSTM does have the ability to remove or add information to the cell state, carefully regulated by structures called gates. Gates are a way to optionally let information through. They are composed out of a sigmoid neural net layer and a pointwise multiplication operation.\n\nThe sigmoid layer outputs numbers between zero and one, describing how much of each component should be let through. A value of zero means \u201clet nothing through,\u201d while a value of one means \u201clet everything through!\u201d. An LSTM has three of these gates, to protect and control the cell state.\n\nThe first step in our LSTM is to decide what information we\u2019re going to throw away from the cell state. This decision is made by a sigmoid layer called the \u201cforget gate layer.\u201d\n\n![img](https:\/\/colah.github.io\/posts\/2015-08-Understanding-LSTMs\/img\/LSTM3-focus-f.png)\n\nThe next step is to decide what new information we\u2019re going to store in the cell state. This has two parts. First, a sigmoid layer called the \u201cinput gate layer\u201d decides which values we\u2019ll update. Next, a tanh layer creates a vector of new candidate values, that could be added to the state. In the next step, we\u2019ll combine these two to create an update to the state.\n\n![img](https:\/\/colah.github.io\/posts\/2015-08-Understanding-LSTMs\/img\/LSTM3-focus-i.png)\n\nIt\u2019s now time to update the old cell state, Ct\u22121, into the new cell state Ct. The previous steps already decided what to do, we just need to actually do it. We multiply the old state by ft, forgetting the things we decided to forget earlier. Then we add it\u2217C~t. This is the new candidate values, scaled by how much we decided to update each state value.\n\n![img](https:\/\/colah.github.io\/posts\/2015-08-Understanding-LSTMs\/img\/LSTM3-focus-C.png)\n\nFinally, we need to decide what we\u2019re going to output. This output will be based on our cell state, but will be a filtered version. First, we run a sigmoid layer which decides what parts of the cell state we\u2019re going to output. Then, we put the cell state through tanh (to push the values to be between \u22121 and 1) and multiply it by the output of the sigmoid gate, so that we only output the parts we decided to.\n\n![img](https:\/\/colah.github.io\/posts\/2015-08-Understanding-LSTMs\/img\/LSTM3-focus-o.png)\n\nSource: \n[Understanding LSTM Networks](https:\/\/colah.github.io\/posts\/2015-08-Understanding-LSTMs\/)","c20a4103":"# Pre Trained Embeddings","5efba42f":"## GloVe: Global Vectors for Word Representation\n\nGloVe is an unsupervised learning algorithm for obtaining vector representations for words. Training is performed on aggregated global word-word co-occurrence statistics from a corpus, and the resulting representations showcase interesting linear substructures of the word vector space.\n\nReference: [GloVe](https:\/\/nlp.stanford.edu\/projects\/glove\/)","d29e0ce8":"# Dataset Exploration\nThe 20 Newsgroups data set is a collection of approximately 20,000 newsgroup documents, partitioned (nearly) evenly across 20 different newsgroups.\n\nThe data is organized into 20 different newsgroups, each corresponding to a different topic. Some of the newsgroups are very closely related to each other (e.g. comp.sys.ibm.pc.hardware \/ comp.sys.mac.hardware), while others are highly unrelated (e.g misc.forsale \/ soc.religion.christian).\n\n> The sklearn.datasets.fetch_20newsgroups function is a data fetching \/ caching functions that downloads the data archive from the original 20 newsgroups website, extracts the archive contents in the ~\/scikit_learn_data\/20news_home folder and calls the sklearn.datasets.load_files on either the training or testing set folder, or both of them","689484b5":"## Word2Vec\n\nWord2Vec is not a singular algorithm, rather, it is a family of model architectures and optimizations that can be used to learn word embeddings from large datasets. Embeddings learned through Word2Vec have proven to be successful on a variety of downstream natural language processing tasks.\n\nThe papers proposed two methods for learning representations of words:\n\n* **Continuous Bag-of-Words Model** which predicts the middle word based on surrounding context words. The context consists of a few words before and after the current (middle) word. This architecture is called a bag-of-words model as the order of words in the context is not important.\n* **Continuous Skip-gram Model** which predict words within a certain range before and after the current word in the same sentence. A worked example of this is given below.\n\nWe'll use the skip-gram approach in this task\n\nSource: [Word2Vec](https:\/\/www.tensorflow.org\/tutorials\/text\/word2vec)","66f4cf3a":"## fastText\nFastText is an open-source, free, lightweight library that allows users to learn text representations and text classifiers. It works on standard, generic hardware. Models can later be reduced in size to even fit on mobile devices.\n\nSource: [fastText](https:\/\/fasttext.cc\/)"}}