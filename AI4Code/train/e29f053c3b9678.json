{"cell_type":{"e917f17c":"code","631e5fd6":"code","f3565b2c":"code","f01f7da1":"code","ddd62b29":"code","d33bc503":"code","bad55c0e":"code","b769f31d":"code","772af2ce":"code","64b4dc0c":"code","ed96d2d5":"code","609ffcd1":"code","3ed02702":"code","6fd7e1cd":"code","22e0e23d":"code","c078e2f0":"code","f58ba2f1":"code","62e8e5b1":"code","7c07ae4c":"code","96bd4101":"code","4a97dcf1":"code","11e3fe78":"code","89f98ca3":"code","a831fe72":"code","8f8aa3a7":"code","189ee2f3":"code","2ef63b6b":"code","3107d9bb":"code","a7a2e20a":"code","c86c735f":"code","f1313d0a":"markdown","5f26bc09":"markdown","7e6355a8":"markdown","17739015":"markdown","320cbfdb":"markdown","b04c7b10":"markdown"},"source":{"e917f17c":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport cv2\nimport tensorflow as tf\nfrom tqdm import tqdm\n\nfrom sklearn.model_selection import train_test_split\n\n%matplotlib inline","631e5fd6":"train_df = pd.read_csv('..\/input\/aptos2019-blindness-detection\/train.csv')\ntest_df = pd.read_csv('..\/input\/aptos2019-blindness-detection\/test.csv')","f3565b2c":"IMG_SIZE = 224","f01f7da1":"def load_img(path):\n    image = cv2.imread(path)\n    return image","ddd62b29":"fig, axes = plt.subplots(3,3,figsize=(10,10))\nselection = np.random.choice(train_df.index, size=9, replace=False)\nimages = '..\/input\/aptos2019-blindness-detection\/train_images\/'+train_df.loc[selection]['id_code']+'.png'\nfor image, axis in zip(images, axes.ravel()):\n    img = load_img(image)\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    axis.imshow(img)","d33bc503":"def remove_unwanted_space(image, threshold=7):\n    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n    mask = gray_image > threshold\n    return image[np.ix_(mask.any(1), mask.any(0))]","bad55c0e":"def preprocess_img(path):\n    image = load_img(path)\n    image = remove_unwanted_space(image, 5)\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    image = cv2.resize(image, (IMG_SIZE, IMG_SIZE))\n    image = cv2.addWeighted(image,4, cv2.GaussianBlur(image, (0,0), 30), -4, 128)\n    return image","b769f31d":"fig, axes = plt.subplots(3,3,figsize=(10,10))\nselection = np.random.choice(train_df.index, size=9, replace=False)\nimages = '..\/input\/aptos2019-blindness-detection\/train_images\/'+train_df.loc[selection]['id_code']+'.png'\nfor image, axis in zip(images, axes.ravel()):\n    img = preprocess_img(image)\n    axis.imshow(img)","772af2ce":"def circle_crop(img):\n    circle_img = np.zeros((IMG_SIZE, IMG_SIZE), np.uint8)\n    cv2.circle(circle_img, ((int)(IMG_SIZE\/2),(int)(IMG_SIZE\/2)), int(IMG_SIZE\/2), 1, thickness=-1)\n    img = cv2.bitwise_and(img, img, mask=circle_img)\n    return img","64b4dc0c":"fig, axes = plt.subplots(3,3,figsize=(10,10))\nselection = np.random.choice(train_df.index, size=9, replace=False)\nimages = '..\/input\/aptos2019-blindness-detection\/train_images\/'+train_df.loc[selection]['id_code']+'.png'\nfor image, axis in zip(images, axes.ravel()):\n    img = circle_crop(preprocess_img(image))\n    axis.imshow(img)","ed96d2d5":"N = train_df.shape[0]\ntrain = np.empty((N, IMG_SIZE, IMG_SIZE, 3), dtype=np.uint8)\nfor i, image_id in enumerate(tqdm(train_df['id_code'])):\n    train[i,:,:,:] = circle_crop(preprocess_img('..\/input\/aptos2019-blindness-detection\/train_images\/'+image_id+'.png'))","609ffcd1":"N = test_df.shape[0]\ntest = np.empty((N, IMG_SIZE, IMG_SIZE, 3), dtype=np.uint8)\nfor i, image_id in enumerate(tqdm(test_df['id_code'])):\n    test[i,:,:,:] = circle_crop(preprocess_img('..\/input\/aptos2019-blindness-detection\/test_images\/'+image_id+'.png'))","3ed02702":"X_train, X_val, y_train, y_val = train_test_split(train, train_df['diagnosis'], test_size=0.15, random_state=42)","6fd7e1cd":"BATCH_SIZE = 32","22e0e23d":"train_data_generator = tf.keras.preprocessing.image.ImageDataGenerator(\n        zoom_range=[0.9, 1.0],\n        fill_mode='constant',\n        cval=0.,  # value used for fill_mode = \"constant\"\n        horizontal_flip=True,  # randomly flip images\n        vertical_flip=True,  # randomly flip images\n        rotation_range=120\n    )","c078e2f0":"val_data_generator = tf.keras.preprocessing.image.ImageDataGenerator()","f58ba2f1":"train_gen = train_data_generator.flow(X_train, y_train, batch_size=BATCH_SIZE)","62e8e5b1":"val_gen = val_data_generator.flow(X_val, y_val, batch_size=BATCH_SIZE)","7c07ae4c":"resnet = tf.keras.applications.ResNet50(include_top=False, weights='..\/input\/resnet50\/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5', input_shape=(IMG_SIZE, IMG_SIZE, 3))","96bd4101":"model = tf.keras.Sequential([\n    resnet,\n    tf.keras.layers.GlobalAveragePooling2D(),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.Dropout(0.5),\n    tf.keras.layers.Dense(256, activation='relu'),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.Dropout(0.5),\n    tf.keras.layers.Dense(1, activation='relu')\n])","4a97dcf1":"checkpoint = tf.keras.callbacks.ModelCheckpoint('model_weights.hdf5', monitor='val_loss', verbose=1, \n                             save_best_only=True, mode='min')","11e3fe78":"optimizer = tf.keras.optimizers.Adam(lr=0.00005)","89f98ca3":"model.compile(optimizer=optimizer, loss='mse')","a831fe72":"steps_per_epoch = int(np.ceil(X_train.shape[0]\/BATCH_SIZE))\nval_steps_per_epoch = int(np.ceil(X_val.shape[0]\/BATCH_SIZE))","8f8aa3a7":"history = model.fit(train_gen, validation_data=val_gen, steps_per_epoch=steps_per_epoch, \n                              validation_steps=val_steps_per_epoch, callbacks=[checkpoint], epochs=25)","189ee2f3":"model.load_weights('model_weights.hdf5')","2ef63b6b":"prediction = model.predict(test)","3107d9bb":"for i, pred in enumerate(prediction):\n    if pred < 0.5:\n        prediction[i] = 0\n    elif pred < 1.5:\n        prediction[i] = 1\n    elif pred < 2.5:\n        prediction[i] = 2\n    elif pred < 3.5:\n        prediction[i] = 3\n    else:\n        prediction[i] = 4","a7a2e20a":"prediction = np.squeeze(prediction.astype(np.int8))","c86c735f":"sample = pd.read_csv(\"..\/input\/aptos2019-blindness-detection\/sample_submission.csv\")\nsample.diagnosis = prediction\nsample.to_csv(\"submission.csv\", index=False)","f1313d0a":"There seems a correlation between the way images are cropped in the training data and the target variables. In order to avoid the model predicting targets based on the way images are cropped, we can circle crop the eyes ourself in preprocessing. Refer to this kernel for more info: https:\/\/www.kaggle.com\/taindow\/be-careful-what-you-train-on ","5f26bc09":"Let us take a look at some of the images.","7e6355a8":"Let's crop images so that the extra spaces surrounding eyes are removed.","17739015":"We can consider this problem as a regression problem instead of classification. Here targets 0,1,2,3,4 are different stages of diabetic retinopathy and are not just independent classes. Their magnitude is representative of the severity of the disease. So this is a regression problem.","320cbfdb":"Ben Graham's preprocessing method (last competition) is used for fixing lighting conditions. Please refer to this kernel for more: https:\/\/www.kaggle.com\/ratthachat\/aptos-updatedv14-preprocessing-ben-s-cropping","b04c7b10":"As we can see, there are extra black areas surrounding the eyes which can be removed. The lighting conditions also differ significantly from image to image."}}