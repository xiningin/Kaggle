{"cell_type":{"78aa001b":"code","390ebe6b":"code","d9355e16":"code","92f66eae":"code","dbb09c6b":"code","a036ad03":"code","75cbdffc":"code","e501b356":"code","745f9730":"code","9a42d18b":"code","89fcc2c5":"markdown","490af8c9":"markdown","9a8bb3c8":"markdown","8cf9f5c7":"markdown"},"source":{"78aa001b":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport keras\nfrom keras import Input\nfrom keras.metrics import Precision, Recall\nfrom keras.models import Sequential, Model\nfrom keras.layers import Dense, Conv2D , MaxPool2D , Flatten , Dropout , BatchNormalization, SeparableConv2D\nfrom keras.optimizers import Adam\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report,confusion_matrix\nfrom keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\nimport cv2\nimport os","390ebe6b":"path = '..\/input\/chest-xray-pneumonia\/chest_xray\/chest_xray\/'\n\nfor comb in ['train', 'val', 'test']:\n    normal = len(os.listdir(path + comb + '\/NORMAL'))\n    pneumonia = len(os.listdir(path + comb + '\/PNEUMONIA'))\n    print('Set: {}, normal images: {}, pneumonia images: {}'.format(comb, normal, pneumonia))\n    if comb == 'train':\n        bar_count = []\n        bar_count.extend([\"Pneumonia\" for i in range(pneumonia)])\n        bar_count.extend([\"Normal\" for i in range(normal)])\n        sns.set_style('darkgrid')\n        sns.countplot(bar_count)","d9355e16":"def process_data(batch_size):\n    # We normalize the data (cnn converges faster on [0..1] data than [0..255])\n    # And in order to avoid overfitting we artifcially expand our data set using parameters such \n    #as zoom_range, vertical_flip etc.\n    train_datagen = ImageDataGenerator(rescale = 1.\/255, zoom_range = 0.3, vertical_flip = True) \n    test_val_datagen = ImageDataGenerator(rescale=1.\/255)\n    \n    # Obtains image with the definded bach size which will be useful later when we fit the model\n    train_gen = train_datagen.flow_from_directory(\n                 directory = path + 'train', \n                 target_size = (150, 150), \n                 batch_size = batch_size, \n                 class_mode = 'binary', \n                 shuffle = True)\n\n    test_gen = test_val_datagen.flow_from_directory(\n                directory = path + 'test', \n                target_size = (150, 150), \n                batch_size = batch_size, \n                class_mode = 'binary', \n                shuffle = True)\n    \n    # The predictions will be based on one batch size\n    test_data = []\n    test_labels = []\n\n    for cond in ['\/NORMAL\/', '\/PNEUMONIA\/']:\n        for img in (os.listdir(path + 'test' + cond)):\n            img = plt.imread(path+'test'+cond+img)\n            img = cv2.resize(img, (150, 150))\n            # Such that the shape of the images is (150, 150, 3)\n            img = np.dstack([img, img, img])\n            # We normalize the data again ([0..255 --> 0..1])\n            img = img.astype('float32') \/ 255\n            if cond=='\/NORMAL\/':\n                label = 0\n            elif cond=='\/PNEUMONIA\/':\n                label = 1\n            test_data.append(img)\n            test_labels.append(label)\n        \n    test_data = np.array(test_data)\n    test_labels = np.array(test_labels)\n    \n    return train_gen, test_gen, test_data, test_labels","92f66eae":"epochs = 10\nbatch_size = 32\n\ntrain_gen, test_gen, test_data, test_labels = process_data(batch_size)","dbb09c6b":"def conv_block(filters):\n    block = Sequential([\n        SeparableConv2D(filters, (3,3), activation='relu', padding='same'),\n        SeparableConv2D(filters, (3,3), activation='relu', padding='same'),\n        BatchNormalization(),\n        MaxPool2D((2,2))\n    ])\n    \n    return block","a036ad03":"def dense_block(units, dropout_rate):\n    block = Sequential([\n        Dense(units, activation='relu'),\n        Dropout(dropout_rate)\n    ])\n    \n    return block","75cbdffc":"\nmodel = Sequential([\n    # First (input) convutional block\n    Input(shape=(150,150, 3)),\n    Conv2D(16, (3,3), activation='relu', padding='same'),\n    Conv2D(16, (3,3), activation='relu', padding='same'),\n    MaxPool2D((2,2), padding = 'same'),\n    \n    # Second convutional block\n    conv_block(32),\n\n    # Third convutional block\n    conv_block(64),\n    \n    # Fourth convutional block\n    conv_block(128),\n    Dropout(0.2),\n    \n    # Fifth convutional block\n    conv_block(256),\n    Dropout(0.2),\n    \n    Flatten(),\n    dense_block(512, 0.7),\n    dense_block(128, 0.5),\n    dense_block(64, 0.3),\n    Dense(1, activation = 'sigmoid')\n])\n\nmodel.summary()\nmodel.compile(loss='binary_crossentropy', optimizer= 'adam', metrics=['accuracy'])\n\n# Callbacks\n\ncheckpoint = ModelCheckpoint(filepath='best_weights.h5', save_best_only=True)\nlr_reduce = ReduceLROnPlateau(monitor='val_loss', factor=0.3, patience=2, verbose=2, mode='max')\nearly_stop = EarlyStopping(patience=10, restore_best_weights=True)\n","e501b356":"hist = model.fit(\n           train_gen, steps_per_epoch = train_gen.samples \/\/ batch_size, \n           epochs = epochs, \n           validation_data = test_gen, \n           validation_steps = test_gen.samples \/\/ batch_size, \n           callbacks=[checkpoint, early_stop, lr_reduce])","745f9730":"fig, ax = plt.subplots(1, 2, figsize=(10, 3))\nax = ax.ravel()\n\nfor i, met in enumerate(['accuracy', 'loss']):\n    ax[i].plot(hist.history[met])\n    ax[i].plot(hist.history['val_' + met])\n    ax[i].set_title('Model {}'.format(met))\n    ax[i].set_xlabel('epochs')\n    ax[i].set_ylabel(met)\n    ax[i].legend(['train', 'val'])","9a42d18b":"from sklearn.metrics import accuracy_score, confusion_matrix\n\npreds = model.predict(test_data)\n\nacc = accuracy_score(test_labels, np.round(preds))*100\ncm = confusion_matrix(test_labels, np.round(preds))\ntn, fp, fn, tp = cm.ravel()\n\nprint('CONFUSION MATRIX ------------------')\nprint(cm)\n\nprint('\\nTEST METRICS ----------------------')\nprecision = tp\/(tp+fp)*100\nrecall = tp\/(tp+fn)*100\nprint('Accuracy: {}%'.format(acc))\nprint('Precision: {}%'.format(precision))\nprint('Recall: {}%'.format(recall))\nprint('F1-score: {}'.format(2*precision*recall\/(precision+recall)))\n\nprint('\\nTRAIN METRIC ----------------------')\nprint('Train acc: {}'.format(np.round((hist.history['accuracy'][-1])*100, 2)))","89fcc2c5":"# **Preprocessing data**","490af8c9":"# **Training the Model:**\n- **Conv2D** -> Creates a convolution kernel that is wind with layers input which helps produce a tensor of outputs.\n- **BatchNormalization** -> Method used to make artificial neural networks faster and more stable through normalization of the input layer. \n- **MaxPool2D** -> It states the maximum output within a rectangular neighbourhood.\n- **Dropout** -> Is regularization technique to avoid overfitting (increase ther validation accuracy) thus increasing the generalizing power.","9a8bb3c8":"# **Imports**","8cf9f5c7":"# **Loading the dataset and visualizing it**"}}