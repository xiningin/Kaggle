{"cell_type":{"e9d72d51":"code","3defa7e5":"code","120547d4":"code","069907bb":"code","7a5aee39":"code","50069fda":"code","ab4d3fd0":"code","b410998f":"code","61db93f9":"code","cf06f9f9":"code","18429f69":"code","6591fd24":"code","e6d06c08":"code","c05be3a0":"code","bb91ee16":"code","259e08fd":"code","b49f9254":"code","c6be0aa6":"code","3c0acfd9":"code","272332b8":"code","9ea63206":"code","ce964e7a":"code","b80ca649":"code","fc3586be":"code","6b4edac1":"code","4000fbd4":"code","8c48b420":"code","c249309c":"code","27852048":"code","1eab04ac":"code","c13784d1":"code","7ca3d7c3":"code","0b072124":"code","93a81b52":"code","cf7e02a9":"code","2dabee06":"code","4e4ee7f8":"code","0fa88235":"code","70c44289":"code","c02dc9f0":"code","78263807":"code","7b9935ec":"code","c40a0f5c":"markdown","fb368df5":"markdown","0b74fb7f":"markdown","c2b124cb":"markdown","1efa1b7a":"markdown","3b3ad0de":"markdown","d7fe7c68":"markdown","b5fa754e":"markdown","2c3eda4e":"markdown","95867222":"markdown","9b79b35e":"markdown","d298b740":"markdown","3ad5762c":"markdown","5b6c0649":"markdown","2f1d6abc":"markdown","0a81cfe2":"markdown","7da35c8e":"markdown"},"source":{"e9d72d51":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as pyplot\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","3defa7e5":"df = pd.read_excel('\/kaggle\/input\/william\/William.xlsx')\ndf.head(10)","120547d4":"reviews_df = df[['sentence','Overall Sentiment\\n(Positive\/Negative\/Neutral)', 'Services & Staff\\n(0 = not mentioned, \\n1 = mentioned)', 'Services & Staff Sentiment', 'Amenities \\n(0 = not mentioned, \\n1 =  mentioned)', 'Amenities Sentiment', 'Hotel Condition \\n(0 = not mentioned, \\n1 =  mentioned)', 'Hotel Condition Sentiment', 'Cleanliness\\n(0 = not mentioned, \\n1 =  mentioned)', 'Cleanliness Sentiment']]\nreviews_df.head(10)","069907bb":"reviews_df['Overall Sentiment\\n(Positive\/Negative\/Neutral)'].value_counts()","7a5aee39":"from matplotlib import pylab\nfrom pylab import *\nfig = plt.figure()\nax = fig.add_axes([0,0,1,1])\nax.axis('equal')\nlabel = ['Pos', 'Neg', 'Neutral']\nax.pie(reviews_df['Overall Sentiment\\n(Positive\/Negative\/Neutral)'].value_counts(), labels = label,autopct='%1.2f%%')\nplt.title('Sentiment Distr')\nplt.show","50069fda":"aspect_df = reviews_df[['Services & Staff\\n(0 = not mentioned, \\n1 = mentioned)',\n 'Amenities \\n(0 = not mentioned, \\n1 =  mentioned)',\n 'Hotel Condition \\n(0 = not mentioned, \\n1 =  mentioned)',\n 'Cleanliness\\n(0 = not mentioned, \\n1 =  mentioned)']]\n\naspect_dist = reviews_df.apply(pd.Series.value_counts)\naspect_dist","ab4d3fd0":"columns = ['Services & Staff\\n(0 = not mentioned, \\n1 = mentioned)',\n 'Amenities \\n(0 = not mentioned, \\n1 =  mentioned)',\n 'Hotel Condition \\n(0 = not mentioned, \\n1 =  mentioned)',\n 'Cleanliness\\n(0 = not mentioned, \\n1 =  mentioned)']\n\nmentioned_list = []\nfor name in columns:\n    mentioned_list.append(aspect_df.apply(pd.Series.value_counts)[name].tolist()[1])\n\nprint(\"Service & Staff: \",mentioned_list[0],\"\\nAmenities: \",mentioned_list[1],\"\\nHotel Condition: \",\n      mentioned_list[2],\"\\nCleanliness: \",mentioned_list[3])","b410998f":"fig = plt.figure()\nax = fig.add_axes([0,0,1,1])\nax.axis('equal')\nlabel = columns\nax.pie(mentioned_list, labels = label,autopct='%1.2f%%')\nplt.title('Aspect Distribution')\n","61db93f9":"fig, (ax1,ax2, ax3, ax4) = plt.subplots(1,4,figsize=(20,30))\nlabel = ['Pos', 'Neg', 'Neutral']\nax1.pie(reviews_df['Services & Staff Sentiment'].value_counts(), labels = label,autopct='%1.2f%%')\nax1.set_title('Services & Staff Sentiment')\n\n\nax2.pie(reviews_df['Amenities Sentiment'].value_counts(), labels = label,autopct='%1.2f%%')\nax2.set_title('Amenities Sentiment')\n\n\nax3.pie(reviews_df['Hotel Condition Sentiment'].value_counts(), labels = label,autopct='%1.2f%%')\nax3.set_title('Hotel Condition Sentiment')\n\n\nax4.pie(reviews_df['Cleanliness Sentiment'].value_counts(), labels = label,autopct='%1.2f%%')\nax4.set_title('Cleanliness Sentiment')\n\nplt.show","cf06f9f9":"import re\nreviews_df['sentence'] = reviews_df['sentence'].str.replace('[^\\w\\s]','')\nreviews_df['sentence'] = reviews_df['sentence'].str.replace('\\d+', '')\nreviews_df['sentence'] = reviews_df['sentence'].str.lower()","18429f69":"drop = reviews_df[pd.isnull(reviews_df['Overall Sentiment\\n(Positive\/Negative\/Neutral)'])].index\nreviews_df.drop(drop , inplace=True)\nreviews_df = reviews_df.reset_index(drop = True) \nreviews_df.head(10)","6591fd24":"reviews_df['sentence'].replace('', np.nan, inplace=True)\ndrop = reviews_df[pd.isnull(reviews_df['sentence'])].index\nreviews_df.drop(drop , inplace=True)\nreviews_df = reviews_df.reset_index(drop = True) \nreviews_df.head(10)","e6d06c08":"import nltk\nfrom nltk.corpus import stopwords\nstop_words = set(stopwords.words('english'))","c05be3a0":"reviews_df['no_sw'] = reviews_df['sentence'][:].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))","bb91ee16":"reviews_df","259e08fd":"import nltk\nfrom nltk import pos_tag, word_tokenize","b49f9254":"# reviews_df['cleaned'] = [' '.join([Speller(i) for i in x.split()]) for x in reviews_df['no_sw']]","c6be0aa6":"# wordfreq = {}\n# for sentence in reviews_df['no_sw']:\n#     tokens = word_tokenize(sentence)\n#     sent_vec = []\n#     for token in tokens:\n#         if token not in wordfreq.keys():\n#             wordfreq[token] = 1\n#         else:\n#             wordfreq[token] += 1","3c0acfd9":"# import heapq\n# num_features = 2500\n# most_freq = heapq.nlargest(num_features, wordfreq, key=wordfreq.get)","272332b8":"# sentence_vectors = []\n# for sentence in reviews_df['no_sw']:\n#     sentence_tokens = nltk.word_tokenize(sentence)\n#     sent_vec = []\n#     for token in most_freq:\n#         if token in sentence_tokens:\n#             sent_vec.append(1)\n#         else:\n#             sent_vec.append(0)\n#     sentence_vectors.append(sent_vec)","9ea63206":"# sentence_vectors = np.asarray(sentence_vectors)\n# sentence_vectors[0]","ce964e7a":"labels = []\nfor i in reviews_df['Overall Sentiment\\n(Positive\/Negative\/Neutral)']:\n    if i == \"Positive\":\n        labels.append(0)\n    elif i == \"Neutral\":\n        labels.append(1)\n    else:\n        labels.append(2)","b80ca649":"from sklearn.model_selection import train_test_split\n\nsentences = reviews_df['no_sw'].values\n\nsent_train, sent_test, y_train, y_test = train_test_split(sentences, labels, stratify=labels, test_size=0.2, random_state=42)","fc3586be":"from sklearn.feature_extraction.text import CountVectorizer\n\nvectorizer = CountVectorizer()\nvectorizer.fit(sent_train)\n\nX_train = vectorizer.transform(sent_train)\nX_test  = vectorizer.transform(sent_test)\nX_train","6b4edac1":"from sklearn.ensemble import RandomForestClassifier\n\ntext_classifier = RandomForestClassifier(n_estimators=200, random_state=0)\ntext_classifier.fit(X_train, y_train)","4000fbd4":"predictions = text_classifier.predict(X_test)","8c48b420":"from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n\nprint(confusion_matrix(y_test,predictions))\nprint(classification_report(y_test,predictions))\nprint(accuracy_score(y_test, predictions))","c249309c":"import keras\nfrom keras.models import Sequential\nfrom keras import layers\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom keras.layers import Dense\nfrom keras.layers import LSTM\nfrom keras.layers import Dropout\nfrom keras.layers.convolutional import Conv1D\nfrom keras.layers.convolutional import MaxPooling1D\nfrom keras.layers.embeddings import Embedding\nfrom keras.preprocessing import sequence\nfrom keras.callbacks import EarlyStopping","27852048":"y_train = keras.utils.to_categorical(y_train, 3)\ny_test = keras.utils.to_categorical(y_test, 3)","1eab04ac":"input_dim = X_train.shape[1] # Number of features\n\nmodel = Sequential()\nmodel.add(layers.Dense(10, input_dim=input_dim, activation='relu'))\nmodel.add(layers.Dense(3, activation='softmax'))","c13784d1":"model.compile(loss='categorical_crossentropy', \n              optimizer='adam', \n              metrics=['accuracy'])\nmodel.summary()","7ca3d7c3":"history = model.fit(X_train, y_train,\n                    epochs=10,\n                    verbose=True,\n                    validation_data=(X_test, y_test),\n                    batch_size=5)","0b072124":"loss, accuracy = model.evaluate(X_train, y_train, verbose=False)\nprint(\"Training Accuracy: {:.4f}\".format(accuracy))\nloss, accuracy = model.evaluate(X_test, y_test, verbose=False)\nprint(\"Testing Accuracy:  {:.4f}\".format(accuracy))","93a81b52":"y_pred = model.predict(X_test, batch_size=5, verbose=1)\ny_pred_bool = np.argmax(y_pred, axis=1)\ny_labels = np.argmax(y_test, axis=1)\n\nprint(classification_report(y_labels, y_pred_bool))","cf7e02a9":"from keras.preprocessing.text import Tokenizer\n\ntokenizer = Tokenizer(num_words=5000)\ntokenizer.fit_on_texts(sent_train)\n\nX_train = tokenizer.texts_to_sequences(sent_train)\nX_test = tokenizer.texts_to_sequences(sent_test)\n\nvocab_size = len(tokenizer.word_index) + 1  ","2dabee06":"from keras.preprocessing.sequence import pad_sequences\n\nmaxlen = 100\n\nX_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\nX_test = pad_sequences(X_test, padding='post', maxlen=maxlen)\n\nprint(X_train[0, :])","4e4ee7f8":"embedding_dim = 50\n\nmodel = Sequential()\nmodel.add(layers.Embedding(vocab_size, embedding_dim, input_length=maxlen))\nmodel.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\nmodel.add(Dense(10, activation='relu'))\nmodel.add(Dense(3, activation='sigmoid'))\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel.summary()","0fa88235":"history = model.fit(X_train, y_train,\n                    epochs=5,\n                    verbose=True,\n                    validation_data=(X_test, y_test),\n                    batch_size=32,\n                    workers=4,\n                    use_multiprocessing=True,\n                    max_queue_size=100)","70c44289":"loss, accuracy = model.evaluate(X_train, y_train, verbose=False)\nprint(\"Training Accuracy: {:.4f}\".format(accuracy))\nloss, accuracy = model.evaluate(X_test, y_test, verbose=False)\nprint(\"Testing Accuracy:  {:.4f}\".format(accuracy))\n\ny_pred = model.predict(X_test, batch_size=5, verbose=1)\ny_pred_bool = np.argmax(y_pred, axis=1)\ny_labels = np.argmax(y_test, axis=1)\n\nprint(classification_report(y_labels, y_pred_bool))","c02dc9f0":"embedding_dim = 128\n\nmodel = Sequential()\nmodel.add(layers.Embedding(vocab_size, embedding_dim, input_length=maxlen))\nmodel.add(Conv1D(filters=64, kernel_size=5, padding='same', activation='relu'))\nmodel.add(MaxPooling1D(pool_size=4))\nmodel.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\nmodel.add(Dense(3, activation='sigmoid'))\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel.summary()","78263807":"history = model.fit(X_train, y_train,\n                    epochs=20,\n                    verbose=True,\n                    validation_data=(X_test, y_test),\n                    batch_size=32,\n                    workers=4,\n                    use_multiprocessing=True,\n                    max_queue_size=100)","7b9935ec":"loss, accuracy = model.evaluate(X_train, y_train, verbose=False)\nprint(\"Training Accuracy: {:.4f}\".format(accuracy))\nloss, accuracy = model.evaluate(X_test, y_test, verbose=False)\nprint(\"Testing Accuracy:  {:.4f}\".format(accuracy))\n\ny_pred = model.predict(X_test, batch_size=5, verbose=1)\ny_pred_bool = np.argmax(y_pred, axis=1)\ny_labels = np.argmax(y_test, axis=1)\n\nprint(classification_report(y_labels, y_pred_bool))","c40a0f5c":"## Pre-processing","fb368df5":"## Overall Sentiment Distribution ","0b74fb7f":"### Vectorize","c2b124cb":"### Evaluations","1efa1b7a":"### Read data","3b3ad0de":"### Select number of most frequent words to use as features","d7fe7c68":"## CountVectorizer","b5fa754e":"### Sample with RFC","2c3eda4e":"## LSTM & CNN","95867222":"## Aspect Distribution","9b79b35e":"### Remove stop words","d298b740":"## LSTM","3ad5762c":"## Tokenize","5b6c0649":"## Bag of words","2f1d6abc":"### Filter data","0a81cfe2":"### Drop empty cells","7da35c8e":"## Deep Learning"}}