{"cell_type":{"c2c04306":"code","b47ea568":"code","209723c3":"code","aac190e2":"code","4f1a07f3":"code","c793f32d":"code","78967909":"code","ea5a68cf":"code","a640fb11":"code","adbaca42":"code","cc8aac25":"code","c587301f":"code","a0b850dd":"code","56fb57b0":"code","f103b69e":"code","31323cd7":"code","ca4a8e74":"code","cd274c4b":"code","c4f05acf":"code","6f10e4a8":"code","ee27b93e":"markdown","55ecaa7b":"markdown","bb92a71b":"markdown","45693dfc":"markdown","af23d58c":"markdown","86db8813":"markdown"},"source":{"c2c04306":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","b47ea568":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\n\nimport tensorflow as tf\nimport tensorflow.keras.losses as losses\nimport tensorflow.keras.metrics as metrics\n\nfrom tensorflow.keras.layers import Conv2D, Dense, Dropout, InputLayer, Softmax, Flatten, MaxPool2D, BatchNormalization\nfrom tensorflow.keras.layers.experimental.preprocessing import RandomZoom\nfrom tensorflow.keras.optimizers import Adam, SGD\nfrom tensorflow.keras.metrics import Accuracy\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\nfrom tensorflow.keras.datasets import mnist\nfrom keras.utils.vis_utils import plot_model\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.utils.np_utils import to_categorical","209723c3":"df_train = pd.read_csv(\"..\/input\/digit-recognizer\/train.csv\")\ndf_test = pd.read_csv(\"..\/input\/digit-recognizer\/test.csv\")\n\n# Removing and storing the class labels in another variable\nY = df_train['label']\ndf_train.drop([\"label\"], axis=1, inplace=True)\n\nprint(df_train.shape, df_test.shape, Y.shape)","aac190e2":"df_train.head()","4f1a07f3":"(x_train, y_train), (x_test, y_test) = mnist.load_data()\nx_train = x_train.reshape(60000, -1)\nx_test = x_test.reshape(10000, -1)\ndf_train = np.concatenate([df_train, x_train, x_test], axis = 0)\nY = np.concatenate([Y, y_train, y_test], axis = 0)\nprint(df_train.shape, Y.shape)","c793f32d":"# Scaling the pixel values to be between 0 and 1\ndf_train = df_train.astype('float32')\ndf_test = df_test.astype('float32')\ndf_train = df_train \/ 255\ndf_test = df_test \/ 255","78967909":"# The image is having dimensions 28*28\nim_dim = 28\n\n# Reshaping the dataset, so that we can display the individual images, and model them\ndf_train = tf.reshape(df_train, (-1, im_dim, im_dim, 1))\ndf_test = tf.reshape(df_test, (-1, im_dim, im_dim, 1))\nprint(df_train.shape, df_test.shape)","ea5a68cf":"fig,axes = plt.subplots(5, 5, figsize = (6,6))\naxes = axes.ravel()\n\nfor i in np.arange(0,25):\n    axes[i].imshow(df_train[i])\n    axes[i].axis(\"off\")","a640fb11":"# Defining some of the key parameters\nnum_classes = 10\nbatch_size = 32\nepochs = 100","adbaca42":"model = tf.keras.Sequential(layers = [\n    Conv2D(filters=32, kernel_size=3, activation=\"relu\", input_shape=(28, 28, 1)),\n    BatchNormalization(),\n    Conv2D(filters=32, kernel_size=3, activation=\"relu\"),\n    BatchNormalization(),\n    MaxPool2D(pool_size=(2, 2)),\n    BatchNormalization(),\n    Dropout(rate=0.2),\n    Flatten(),\n    Dense(128, activation=\"relu\"),\n    BatchNormalization(),\n    Dense(10, activation=\"softmax\")\n])","cc8aac25":"model.summary()","c587301f":"plot_model(model, show_shapes=True, show_layer_names=True)","a0b850dd":"# Defining the Adam Optimizer\nsgd = SGD(lr=2e-2, decay=1e-6, momentum=0.9)\n\n# Defining the callbacks\nreduce_lr = ReduceLROnPlateau(\n    monitor = 'val_loss', factor = 0.01, patience = 10, \n    min_delta = 1e-4, verbose = 1\n)\n# early_st = EarlyStopping(\n#     monitor='val_loss', min_delta=1e-3,\n#     patience=5, verbose=1, restore_best_weights=True, mode = 'min'\n# )\n\n# Compiling the model\nmodel.compile(loss=\"categorical_crossentropy\", metrics=[\"accuracy\"], optimizer=sgd)","56fb57b0":"# Converting the class labels into one-hot form\nY_oh = to_categorical(Y, num_classes=num_classes)","f103b69e":"# # Using real-time Data Augmentation\n# datagen = ImageDataGenerator(\n#     featurewise_center = True, featurewise_std_normalization = True,\n#     rotation_range = 10, fill_mode = 'nearest', validation_split = 0.2\n# )\n# datagen.fit(df_train)\n\n# train_generator = datagen.flow(df_train, Y_oh, batch_size = batch_size, subset = 'training')\n# val_generator = datagen.flow(df_train, Y_oh, batch_size = batch_size, subset = 'validation')\n\n# # Training the model using generators\n# model.fit(\n#     train_generator, batch_size = batch_size,\n#     epochs = epochs, verbose = 1, validation_data = val_generator,\n#     use_multiprocessing = True, workers = -1\n# )","31323cd7":"# Without using any augmentation\nmodel.fit(\n    df_train, Y_oh, batch_size = batch_size,\n    epochs = epochs, verbose = 1, validation_split = 0.2,\n    use_multiprocessing = True, callbacks = [reduce_lr]\n)","ca4a8e74":"y_pred = model.predict(df_train)\ny_pred = tf.math.argmax(y_pred, axis = 1)\nacc = metrics.Accuracy()\nprint(acc(Y, y_pred))","cd274c4b":"y_sub = model.predict(df_test)\ny_sub = tf.math.argmax(y_sub, axis = 1)\ny_sub = pd.Series(y_sub)\nprint(y_sub.shape, type(y_sub))","c4f05acf":"df_sub = pd.read_csv(\"..\/input\/digit-recognizer\/sample_submission.csv\")\ndf_sub.loc[ : , 'Label'] = y_sub","6f10e4a8":"df_sub.to_csv(\"submission.csv\", index = False)","ee27b93e":"# Digit Recognizer (Notebook 1)\n- Hola amigos, this notebook covers my code for the **Digit Recognizer Competition**, which can be found [here](https:\/\/www.kaggle.com\/c\/digit-recognizer).\n- I also have created a fork of this notebook, in order to test out various different architectures, which can be found [here](https:\/\/www.kaggle.com\/elemento\/digitrecognizer-2).\n- The key features of these 2 notebooks are **Scaling the pixel values**, and **Data Augmentation** (used in the fork).\n- As for the architectures, I tried out various different ones, using Conv2D, Dense, Dropout, MaxPool2D, BatchNormalization, etc.\n- I experimented with various optimizers such as RMSProp, Adam, SGD, etc, and different callbacks like Reduce Learning Rate on Plateau and Early Stop.","55ecaa7b":"# Training the Model","bb92a71b":"# Visualizing & Processing the Data","45693dfc":"# Making the submission","af23d58c":"# Installing & Importing Packages","86db8813":"# Importing the data"}}