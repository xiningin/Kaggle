{"cell_type":{"38a6851a":"code","f751bc00":"code","7c26a308":"code","7415e648":"code","119873af":"code","f3dd8782":"code","36411c1e":"code","9ace3557":"code","af614731":"code","bb30916d":"code","797ebc7a":"code","7f49ff1f":"code","601847e4":"code","d774721a":"code","f68f4a10":"code","18ee83c8":"code","8cf96e0b":"code","250d6a45":"code","d35c7391":"code","7202db28":"code","b4440db9":"code","e70b71c9":"code","f53d074c":"code","cbc29fe4":"code","25710cee":"code","3480f967":"code","e1c5b41a":"code","c9a1ab4d":"code","ae33ff42":"code","1acfd91b":"code","f80f5cee":"code","ba80bd93":"code","8cc82474":"markdown","a917b195":"markdown","031207a1":"markdown","8fea5d08":"markdown","4c89f25d":"markdown","db882d02":"markdown","30df395c":"markdown","7db1f160":"markdown","5915f38f":"markdown","916e226f":"markdown","b1cfb9cc":"markdown","0f4ebed2":"markdown","f1898a6f":"markdown","e13fcaeb":"markdown","5bc6247e":"markdown","0867b9de":"markdown","bf46d392":"markdown","212a520e":"markdown"},"source":{"38a6851a":"!pip install -U https:\/\/github.com\/sberbank-ai-lab\/LightAutoML\/raw\/fix\/logging\/LightAutoML-0.2.16.2-py3-none-any.whl\n!pip install openpyxl","f751bc00":"# Standard python libraries\nimport os\nimport time\n\n# Essential DS libraries\nimport numpy as np\nimport pandas as pd\nfrom sklearn.metrics import mean_squared_log_error\nimport matplotlib.pyplot as plt\nimport torch\n\n# LightAutoML presets, task and report generation\nfrom lightautoml.automl.presets.tabular_presets import TabularAutoML, TabularUtilizedAutoML\nfrom lightautoml.tasks import Task\nfrom lightautoml.dataset.roles import DatetimeRole\nfrom lightautoml.report.report_deco import ReportDeco","7c26a308":"N_THREADS = 4\nN_FOLDS = 5\nRANDOM_STATE = 42\nTIMEOUT = 2 * 3600\nTARGET_NAME = 'target'","7415e648":"np.random.seed(RANDOM_STATE)\ntorch.set_num_threads(N_THREADS)","119873af":"%%time\n\ntrain_data = pd.read_csv('..\/input\/tabular-playground-series-jul-2021\/train.csv')\ntrain_data.head()","f3dd8782":"train_data.shape","36411c1e":"%%time\n\ntest_data = pd.read_csv('..\/input\/tabular-playground-series-jul-2021\/test.csv')\ntest_data.head()","9ace3557":"test_data.shape","af614731":"%%time\n\nsample_sub = pd.read_csv('..\/input\/tabular-playground-series-jul-2021\/sample_submission.csv')\nsample_sub.head()","bb30916d":"sample_sub.shape","797ebc7a":"# Pseudolabels from true dataset \npseudolabels_true = pd.read_excel('\/kaggle\/input\/air-quality-time-series-data-uci\/AirQualityUCI.xlsx')\npseudolabels_true = pseudolabels_true.iloc[7110:].reset_index(drop = True)\npseudolabels_true.rename({'CO(GT)': 'target_carbon_monoxide', 'C6H6(GT)': 'target_benzene', 'NOx(GT)': 'target_nitrogen_oxides'}, axis = 1, inplace = True)\npseudolabels_true","7f49ff1f":"pseudolabels_preds = pd.read_csv('..\/input\/tps-lightautoml-baseline-with-pseudolabels\/lightautoml_with_pseudolabelling_kernel_version_15.csv')\npseudolabels_preds","601847e4":"test_data['target_carbon_monoxide'] = np.where(pseudolabels_true['target_carbon_monoxide'].values >= 0, \n                                               pseudolabels_true['target_carbon_monoxide'].values, \n                                               pseudolabels_preds['target_carbon_monoxide'].values)\ntest_data['target_benzene'] = np.where(pseudolabels_true['target_benzene'].values >= 0, \n                                       pseudolabels_true['target_benzene'].values, \n                                       pseudolabels_preds['target_benzene'].values)\ntest_data['target_nitrogen_oxides'] = np.where(pseudolabels_true['target_nitrogen_oxides'].values >= 0, \n                                       pseudolabels_true['target_nitrogen_oxides'].values, \n                                       pseudolabels_preds['target_nitrogen_oxides'].values)\n    \ntest_data","d774721a":"test_data['target_carbon_monoxide'].value_counts()","f68f4a10":"test_data['target_benzene'].value_counts()","18ee83c8":"test_data['target_nitrogen_oxides'].value_counts()","8cf96e0b":"ALL_DF = pd.concat([train_data, test_data]).reset_index(drop = True)\nprint(ALL_DF.shape)","250d6a45":"# Feature engineering func from Remek Kinas kernel with MLJAR (https:\/\/www.kaggle.com\/remekkinas\/mljar-code-minimal) - do not forget to upvote his kernel\n    \nimport math\n\ndef pb_add(X):\n    X['day'] = X.date_time.dt.weekday\n    is_odd = (X['sensor_4'] < 646) & (X['absolute_humidity'] < 0.238)\n    X['is_odd'] = is_odd\n    diff = X['date_time'] - min(X['date_time'])\n    trend = diff.dt.days\n    X['f1s'] = np.sin(trend * 2 * math.pi \/ (365 * 1)) \n    X['f1c'] = np.cos(trend * 2 * math.pi \/ (365 * 1))\n    X['f2s'] = np.sin(2 * math.pi * trend \/ (365 * 2)) \n    X['f2c'] = np.cos(2 * math.pi * trend \/ (365 * 2)) \n    X['f3s'] = np.sin(2 * math.pi * trend \/ (365 * 3)) \n    X['f3c'] = np.cos(2 * math.pi * trend \/ (365 * 3)) \n    X['f4s'] = np.sin(2 * math.pi * trend \/ (365 * 4)) \n    X['f4c'] = np.cos(2 * math.pi * trend \/ (365 * 4)) \n    X['fh1s'] = np.sin(diff.dt.seconds * 2 * math.pi \/ ( 3600 * 24 * 1))\n    X['fh1c'] = np.cos(diff.dt.seconds * 2 * math.pi \/ ( 3600 * 24 * 1))\n    X['fh2s'] = np.sin(diff.dt.seconds * 2 * math.pi \/ ( 3600 * 24 * 2))\n    X['fh2c'] = np.cos(diff.dt.seconds * 2 * math.pi \/ ( 3600 * 24 * 2))\n    X['fh3s'] = np.sin(diff.dt.seconds * 2 * math.pi \/ ( 3600 * 24 * 3))\n    X['fh3c'] = np.cos(diff.dt.seconds * 2 * math.pi \/ ( 3600 * 24 * 3))\n    \n    sensor_features = [\n        'deg_C', \n        'relative_humidity', 'absolute_humidity', \n        'sensor_1', 'sensor_2', 'sensor_3', 'sensor_4', 'sensor_5' ]\n    \n    lags = [-1, -4, -24, -7 * 24]  \n    for sensor_feature in sensor_features:\n        this = X[sensor_feature]\n\n        for lag in lags:\n            feature = f'{sensor_feature}_{abs(lag)}b'\n            this_f = X[sensor_feature].shift(lag)\n            X[feature] = (this_f - this).fillna(0)\n        # look forwards\n        for lag in lags:\n            feature = f'{sensor_feature}_{abs(-lag)}f'\n            this_f = X[sensor_feature].shift(-lag)\n            X[feature] = (this_f - this).fillna(0)\n            \n    return X\n\nALL_DF['date_time'] = pd.to_datetime(ALL_DF['date_time'])\nALL_DF[\"hour\"] = ALL_DF[\"date_time\"].dt.hour\nALL_DF[\"working_hours\"] =  ALL_DF[\"hour\"].isin(np.arange(8, 21, 1)).astype(\"int\")\nALL_DF[\"is_weekend\"] = (ALL_DF[\"date_time\"].dt.dayofweek >= 5).astype(\"int\")\nALL_DF['hr'] = ALL_DF.date_time.dt.hour * 60 + ALL_DF.date_time.dt.minute\nALL_DF['satday'] = (ALL_DF.date_time.dt.weekday==5).astype(\"int\")\nALL_DF[\"SMC\"] = (ALL_DF[\"absolute_humidity\"] * 100) \/ ALL_DF[\"relative_humidity\"]\nALL_DF.drop(columns = 'hour', inplace = True)\n\npb_add(ALL_DF)\n\nALL_DF['date_time'] = ALL_DF['date_time'].astype(str)","d35c7391":"def create_target_feats(df):\n    for lag in [1, 4, 24, 7 * 24]:\n        for t in ['target_carbon_monoxide', 'target_benzene', 'target_nitrogen_oxides']:\n            df['{}_lag_{}'.format(t, lag)] = df[t].shift(lag)\n            df['{}_lag_m{}'.format(t, lag)] = df[t].shift(-lag)\n            df['diff_{}_{}'.format(t, lag)] = df['{}_lag_m{}'.format(t, lag)] - df['{}_lag_{}'.format(t, lag)]\n            df['div_{}_{}'.format(t, lag)] = df['{}_lag_m{}'.format(t, lag)] \/ df['{}_lag_{}'.format(t, lag)]\ncreate_target_feats(ALL_DF)","7202db28":"ALL_DF","b4440db9":"train_data, test_data = ALL_DF.iloc[:(len(ALL_DF) - len(test_data)), :], ALL_DF.iloc[(len(ALL_DF) - len(test_data)):, :]\nprint(train_data.shape, test_data.shape)","e70b71c9":"train_data.head()","f53d074c":"test_data.head()","cbc29fe4":"%%time\n\ndef rmsle_metric(y_true, y_pred, sample_weight, **kwargs):\n    mask = (sample_weight > 1)\n    return mean_squared_log_error(y_true[mask], np.clip(y_pred[mask], 0, None), **kwargs) ** 0.5\n\ntask = Task('reg', loss = 'rmsle', metric = rmsle_metric, greater_is_better=False)","25710cee":"?DatetimeRole","3480f967":"%%time\n\ntargets_and_drop = {\n    'target_carbon_monoxide': [],\n    'target_benzene': [],\n    'target_nitrogen_oxides': []\n}\n\nroles = {\n    # delete day of month from features\n    DatetimeRole(base_date=False, base_feats=True, seasonality=('d', 'wd', 'hour')): 'date_time'\n}","e1c5b41a":"%%time \nimportances = {}\ndt = pd.to_datetime(ALL_DF['date_time'])\nfor targ in targets_and_drop:\n    print('='*50, '='*50, sep = '\\n')\n    automl = TabularAutoML(task = task, \n                           timeout = TIMEOUT,\n                           cpu_limit = N_THREADS,\n                           reader_params = {'n_jobs': N_THREADS, 'cv': N_FOLDS, 'random_state': RANDOM_STATE},\n                           general_params = {'use_algos': [['lgb', 'lgb_tuned', 'cb', 'cb_tuned']]},\n                           verbose = 3 # 0 for no output, 1 - only main steps, 2 - more detailed, 3 - show everything including model scores, optuna iterations etc.\n                          )\n    \n    ALL_DF['weight'] = [1.001] * len(train_data) + list(np.where(pseudolabels_true[targ].values >= 0, 1.001, 0.999))\n    roles['weights'] = 'weight'\n\n    roles['target'] = targ\n    roles['drop'] = targets_and_drop[targ]\n\n    if targ == 'target_nitrogen_oxides':\n        oof_pred = automl.fit_predict(ALL_DF[dt >= np.datetime64('2010-09-01')], roles = roles)\n    else:\n        oof_pred = automl.fit_predict(ALL_DF, roles = roles)\n    print('oof_pred:\\n{}\\nShape = {}'.format(oof_pred, oof_pred.shape))\n    \n    # MODEL STRUCTURE - NEW FEATURE\n    print('\\nFitted model structure:\\n{}\\n'.format(automl.create_model_str_desc()))\n    \n    # Fast feature importances calculation\n    fast_fi = automl.get_feature_scores('fast')\n    importances[targ] = fast_fi\n    \n    test_pred = automl.predict(test_data)\n    print('Prediction for te_data:\\n{}\\nShape = {}'.format(test_pred, test_pred.shape))\n    \n    sample_sub[targ] = np.clip(test_pred.data[:, 0], 0, None)","c9a1ab4d":"for targ in targets_and_drop:\n    plt.figure(figsize = (30, 10))\n    importances[targ].set_index('Feature')['Importance'].plot.bar()\n    plt.title('Feature importances for {} model'.format(targ))\n    plt.grid(True)\n    plt.show()","ae33ff42":"sample_sub","1acfd91b":"pseudolabels_true[['target_carbon_monoxide','target_benzene','target_nitrogen_oxides']]","f80f5cee":"for targ in targets_and_drop:\n    preds = sample_sub[targ].values\n    real_values = pseudolabels_true[targ].values\n    final_preds = np.where(real_values >= 0, real_values, preds)\n    print(final_preds)\n    sample_sub[targ] = final_preds","ba80bd93":"sample_sub.to_csv('lightautoml_with_pseudolabelling_kernel_version_16.csv', index = False)","8cc82474":"## Do not forget to upvote if you like the kernel \ud83d\udc4d","a917b195":"- [Official LightAutoML github repo](https:\/\/github.com\/sberbank-ai-lab\/LightAutoML)\n- [LightAutoML documentation](https:\/\/lightautoml.readthedocs.io\/en\/latest)\n- [Pseudolabelling technique description post](https:\/\/www.kaggle.com\/c\/tabular-playground-series-apr-2021\/discussion\/231738#1268903)\n- [Baseline LightAutoML kernel without pseudolabelling](https:\/\/www.kaggle.com\/alexryzhkov\/tps-july-21-lightautoml-baseline)","031207a1":"# Step 3. LightAutoML model creation - TabularAutoML preset","8fea5d08":"# =============== LightAutoML model building ===============\n\n\n# Step 1. Task setup\n\nOn the cell below we create Task object - the class to setup what task LightAutoML model should solve with specific loss and metric if necessary (more info can be found [here](https:\/\/lightautoml.readthedocs.io\/en\/latest\/generated\/lightautoml.tasks.base.Task.html#lightautoml.tasks.base.Task) in our documentation):","4c89f25d":"### Checking BIZEN idea from comments - no drop for any target, another targets using as features","db882d02":"In next the cell we are going to create LightAutoML model with `TabularAutoML` class - preset with default model structure like in the image below:\n\n<img src=\"https:\/\/github.com\/sberbank-ai-lab\/lightautoml-datafest-workshop\/raw\/master\/imgs\/tutorial_blackbox_pipeline.png\" alt=\"TabularAutoML preset pipeline\" style=\"width:70%;\"\/>\n\nin just several lines. Let's discuss the params we can setup:\n- `task` - the type of the ML task (the only **must have** parameter)\n- `timeout` - time limit in seconds for model to train\n- `cpu_limit` - vCPU count for model to use\n- `reader_params` - parameter change for Reader object inside preset, which works on the first step of data preparation: automatic feature typization, preliminary almost-constant features, correct CV setup etc. For example, we setup `n_jobs` threads for typization algo, `cv` folds and `random_state` as inside CV seed.\n- `general_params` - we use `use_algos` key to setup the model structure to work with (two LGBM models and two CatBoost models on the first level and their weighted composition creation on the second). This setup is only to speedup the kernel, you can remove this `general_params` setup if you want the whole LightAutoML model to run.\n\n**Important note**: `reader_params` key is one of the YAML config keys, which is used inside `TabularAutoML` preset. [More details](https:\/\/github.com\/sberbank-ai-lab\/LightAutoML\/blob\/master\/lightautoml\/automl\/presets\/tabular_config.yml) on its structure with explanation comments can be found on the link attached. Each key from this config can be modified with user settings during preset object initialization. To get more info about different parameters setting (for example, ML algos which can be used in `general_params->use_algos`) please take a look at our [article on TowardsDataScience](https:\/\/towardsdatascience.com\/lightautoml-preset-usage-tutorial-2cce7da6f936). ","30df395c":"## Important - cell below works only because of the data leak. In real life you can't create features using the future data. Be careful!","7db1f160":"# Step 0.3. Imported models setup\n\nFor better reproducibility we fix numpy random seed with max number of threads for Torch (which usually try to use all the threads on server):","5915f38f":"## Official LightAutoML github repository is [here](https:\/\/github.com\/sberbank-ai-lab\/LightAutoML)\n\n## Upvote is the best motivator \ud83d\udc4d\n\n## UPD: Many thanks for all the fellow Kagglers who have mentioned this notebook in their work - this is yours win as well!\n\n# Step 0.0. LightAutoML installation","916e226f":"# Step 4. Create submission file","b1cfb9cc":"To solve the task, we need to setup columns roles. The **only role you must setup is target role**, everything else (drop, numeric, categorical, group, weights etc.) is up to user - LightAutoML models have automatic columns typization inside:","0f4ebed2":"## Don't know what to do with -200? Use pseudolabelling \ud83e\uddd0","f1898a6f":"# Step 0.1. Import libraries\n\nHere we will import the libraries we use in this kernel:\n- Standard python libraries for timing, working with OS etc.\n- Essential python DS libraries like numpy, pandas, scikit-learn and torch (the last we will use in the next cell)\n- LightAutoML modules: presets for AutoML, task and report generation module","e13fcaeb":"# Additional materials","5bc6247e":"# Step 0.4. Data loading\nLet's check the data we have:","0867b9de":"# Step 0.2. Constants\n\nHere we setup the constants to use in the kernel:\n- `N_THREADS` - number of vCPUs for LightAutoML model creation\n- `N_FOLDS` - number of folds in LightAutoML inner CV\n- `RANDOM_STATE` - random seed for better reproducibility\n- `TIMEOUT` - limit in seconds for model to train\n- `TARGET_NAME` - target column name in dataset","bf46d392":"This step can be used if you are working inside Google Colab\/Kaggle kernels or want to install LightAutoML on your machine:\n## Note: here we use our developer version with new logging and model structure description","212a520e":"# Step 2. Feature roles setup"}}