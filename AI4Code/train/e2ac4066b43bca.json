{"cell_type":{"de858e52":"code","f0d9414d":"code","013edb37":"code","035666fc":"code","2b2f1856":"code","8fb50d5a":"code","0aa0a62f":"code","0a6aecd7":"code","71487fde":"code","3c108fae":"code","4dd55858":"code","0a2b7c11":"code","b67326bc":"code","8f41b376":"code","1e939613":"code","18f9d875":"code","c08d4169":"code","669ef4a6":"code","8f48a228":"code","3575eaa5":"code","86972ea2":"code","ec0dc9a4":"code","17e62666":"code","8e387b90":"code","8c55d823":"code","317f43e0":"code","bf34780b":"code","020226f8":"markdown","7e35446a":"markdown","629b83e0":"markdown","9d60cb5c":"markdown","a3fe6ef6":"markdown","2bf62632":"markdown","19d68e00":"markdown","dd607e0e":"markdown"},"source":{"de858e52":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import confusion_matrix, accuracy_score, mean_squared_error, r2_score, roc_auc_score, roc_curve, classification_report\nfrom sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\nfrom matplotlib.pyplot import plot\nfrom sklearn.model_selection import cross_val_predict","f0d9414d":"data_set = pd.read_csv(\"..\/input\/credit\/train1.csv\")\npd.pandas.set_option('display.max_columns', None)\npd.pandas.set_option('display.max_rows', None)\ndata_set.head()","013edb37":"data_set.columns= ['Id','status', 'duration', 'credit_history', 'purpose', 'amount', 'savings', 'employment_duration', 'installment_rate', 'personal_status_sex', 'other_debtors', 'present_residence', 'property', 'age', 'other_installment_plans', 'housing', 'number_credits', 'job', 'people_liable', 'telephone', 'foreign_worker', 'credit_risk']","035666fc":"def check_df(dataframe, head=5):\n    print(\"##################### Shape #####################\")\n    print(dataframe.shape)\n    print(\"##################### Types #####################\")\n    print(dataframe.dtypes)\n    print(\"##################### Head #####################\")\n    print(dataframe.head(head))\n    print(\"##################### Tail #####################\")\n    print(dataframe.tail(head))\n    print(\"##################### NA #####################\")\n    print(dataframe.isnull().sum())\n    print(\"##################### Quantiles #####################\")\n    print(dataframe.quantile([0, 0.05, 0.50, 0.95, 0.99, 1]).T)","2b2f1856":"check_df(data_set,5)","8fb50d5a":"import seaborn as sns\nimport matplotlib.pyplot as plt","0aa0a62f":"plt.hist(data_set['amount'],bins=30)\nplt.show()\ndata_set.head()\n","0a6aecd7":"y = data_set['credit_risk']\nX =data_set.drop(['credit_risk'],axis=1)","71487fde":"# split the data 80% train 20% test\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","3c108fae":"cart_model = DecisionTreeClassifier().fit(X_train, y_train)\ny_pred = cart_model.predict(X_test)\naccuracy_score(y_test, y_pred)","4dd55858":"cart = DecisionTreeClassifier()\ncart_params = {\"max_depth\": [1,3,5,8,10],\"min_samples_split\":[2,3,5,10,20,50]}\ncart_cv_model =GridSearchCV(cart, cart_params, cv= 10, n_jobs = -1, verbose=2).fit(X_train,y_train)","0a2b7c11":"cart_cv_model.best_params_","b67326bc":"cart_tuned= DecisionTreeClassifier(max_depth=1, min_samples_split=2).fit(X_train,y_train)\ny_pred = cart_tuned.predict(X_test)\naccuracy_score(y_test,y_pred)","8f41b376":"y_train_pred = cross_val_predict(cart_tuned, X_train, y_train, cv=5)\ncon=confusion_matrix(y_train, y_train_pred)","1e939613":"f, ax = plt.subplots(figsize=(5,5))\nsns.heatmap(con,fmt=\".0f\", annot=True,linewidths=0.2, ax=ax)\nplt.xlabel(\"Predicted Decision Tree\")\nplt.ylabel(\"Actually Decision Tree\")\nplt.show()\nprint(classification_report(y_test,y_pred))","18f9d875":"# logistic regression model\nloj_model = LogisticRegression(solver= \"liblinear\").fit(X_train,y_train)\ny_pred = loj_model.predict(X_test)\nprint(accuracy_score(y_test,y_pred))","c08d4169":"y_train_pred = cross_val_predict(loj_model, X_train, y_train, cv=5)\ncon=confusion_matrix(y_train, y_train_pred)","669ef4a6":"f, ax = plt.subplots(figsize=(5,5))\nsns.heatmap(con,fmt=\".0f\", annot=True,linewidths=0.2, ax=ax)\nplt.xlabel(\"Predicted Logistic Regression\")\nplt.ylabel(\"Actually Logistic Regression\")\nplt.show()\nprint(classification_report(y_test,y_pred))","8f48a228":"# naive bayes model\nnv_model = GaussianNB().fit(X_train,y_train)\ny_pred = nv_model.predict(X_test)\naccuracy_score(y_test, y_pred)","3575eaa5":"param_grid_nb = {\n    'var_smoothing': np.logspace(0,-9, num=100)\n}\nnbModel_grid = GridSearchCV(estimator=GaussianNB(), param_grid=param_grid_nb, verbose=1, cv=10, n_jobs=-1)\nnbModel_grid.fit(X_train, y_train)\nprint(nbModel_grid.best_estimator_)","86972ea2":"nb_tuned = GaussianNB(var_smoothing=0.0023101297000831605).fit(X_train,y_train)\ny_pred = nb_tuned.predict(X_test)\naccuracy_score(y_test,y_pred)","ec0dc9a4":"y_train_pred = cross_val_predict(nb_tuned, X_train, y_train, cv=5)\ncon=confusion_matrix(y_train, y_train_pred)","17e62666":"f, ax = plt.subplots(figsize=(5,5))\nsns.heatmap(con,fmt=\".0f\", annot=True,linewidths=0.2, ax=ax)\nplt.xlabel(\"Predicted Naive Bayes\")\nplt.ylabel(\"Actually Naive Bayes\")\nplt.show()\nprint(classification_report(y_test,y_pred))","8e387b90":"import scikitplot.metrics as splt\nimport sklearn.metrics as mt\n\npred_prob1 = cart_tuned.predict_proba(X_test)\npred_prob2 = loj_model.predict_proba(X_test)\npred_prob3 = nb_tuned.predict_proba(X_test)\n\nfpr1, tpr1, thresh1 = roc_curve(y_test, pred_prob1[:,1], pos_label=1)\nfpr2, tpr2, thresh2 = roc_curve(y_test, pred_prob2[:,1], pos_label=1)\nfpr3, tpr3, thresh3 = roc_curve(y_test,pred_prob3[:,1], pos_label=1)\n\n# roc curve for tpr = fpr\nrandom_probs = [0 for i in range(len(y_test))]\np_fpr, p_tpr, _ = roc_curve(y_test, random_probs, pos_label=1)","8c55d823":"from sklearn.metrics import roc_auc_score\n\n# auc scores\nauc_score1 = roc_auc_score(y_test, pred_prob1[:,1])\nauc_score2 = roc_auc_score(y_test, pred_prob2[:,1])\nauc_score3 = roc_auc_score(y_test, pred_prob3[:,1])\n\nprint(auc_score1, auc_score2, auc_score3)","317f43e0":"import matplotlib.pyplot as plt\nplt.style.use('seaborn')\n\n# plot roc curves\nplt.plot(fpr1, tpr1, linestyle='--',color='orange', label='CART')\nplt.plot(fpr2, tpr2, linestyle='--',color='green', label='Log Reg')\nplt.plot(fpr3, tpr3, linestyle='--',color='gray', label='NB')\nplt.plot(p_fpr, p_tpr, linestyle='--', color='blue')\n# title\nplt.title('ROC curve')\n# x label\nplt.xlabel('False Positive Rate')\n# y label\nplt.ylabel('True Positive rate')\n\nplt.legend(loc='best')\nplt.savefig('ROC',dpi=300)\nplt.show();","bf34780b":"import seaborn as sns\n\n# compared the models\nmodels= [cart_tuned, loj_model, nb_tuned]\n\nresult = []\nresults = pd.DataFrame(columns= [\"Models\", \"Accuracy\"])\n\nfor model in models:\n    isimler = model.__class__.__name__\n    y_pred = model.predict(X_test)\n    dogruluk = accuracy_score(y_test, y_pred)\n    result = pd.DataFrame([[isimler, dogruluk*100]], columns= [\"Models\",\"Accuracy\"])\n    results = results.append(result)\n\nsns.barplot(x = 'Accuracy', y= 'Models', data=results, color=\"r\")\nplt.xlabel('Accuracy %')\nplt.title('Accuracy Rate of Models');\nplt.show()","020226f8":"**CREDICT RISK PREDICTON**\n\n**The main aim of the project is to create multiple models which assess the application and decide applicant has good or bad credit risks and found the best model.**","7e35446a":"Create the Logistic Regression Model\n\nIt is a based on the concept of probability and is a predictive analysis algorithm. \n\nUsed for hyperparameters solver \"liblinear\". Because its useful for small dataset","629b83e0":"tuned model and calculate new accuracy score","9d60cb5c":"Calculate the new accuracy score","a3fe6ef6":"Create the Decision Tree Classifier Model","2bf62632":"We can see the ROC curve above, we can anayze that the best algorithm is CART model","19d68e00":"Create the Naive Bayesian model\n\nNa\u00efve Bayes classifier based on the Bayes theorem. Calculates the probability of each state for a data and classifies it according to the highest probability value.\n","dd607e0e":"Hyperparameters"}}