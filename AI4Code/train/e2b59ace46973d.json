{"cell_type":{"1a745f2c":"code","491e9731":"code","3681a28a":"code","cb5eedc0":"code","abae892c":"code","1e6df698":"code","9f134932":"code","fc2bbced":"markdown","0ece8b66":"markdown","28166e46":"markdown","d566d6c1":"markdown","53a4575a":"markdown","cd77dbb7":"markdown","9f1a6862":"markdown","3f2f5ad9":"markdown","6b094f9f":"markdown"},"source":{"1a745f2c":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom tensorflow import keras\nfrom tensorflow.keras.callbacks import EarlyStopping,ReduceLROnPlateau\n\nfrom sklearn.model_selection import StratifiedKFold \n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","491e9731":"data_train = pd.read_csv(\"..\/input\/digit-recognizer\/train.csv\")\nX_test = pd.read_csv(\"..\/input\/digit-recognizer\/test.csv\")","3681a28a":"X, y = data_train.drop(labels = [\"label\"],axis = 1)\/255.,data_train[\"label\"]\nX_test = X_test\/255.","cb5eedc0":"X = X.values.reshape(-1,28,28,1)\nX_test = X_test.values.reshape(-1,28,28,1)","abae892c":"learning_rate_reduction = ReduceLROnPlateau(monitor='val_acc', \n                                            patience=3,# How many epochs to wait before reduct lerning rate\n                                            verbose=1, \n                                            factor=0.3, \n                                            min_lr=0.00001)\nearly_stopping = EarlyStopping(\n    min_delta=0.000001, # minimium amount of change to count as an improvement\n    patience=20, # how many epochs to wait before stopping\n    restore_best_weights=True,\n)","1e6df698":"skf = StratifiedKFold(n_splits=3,random_state=42,shuffle=True)\nsub = pd.DataFrame(data=None, index=(range(1,28001)), columns=None, dtype=None, copy=False)\nfor train_index, val_index in skf.split(X, y):\n    model = keras.Sequential([\n    keras.layers.Conv2D(filters = 32, kernel_size = (3,3),padding = 'Same', \n                 activation ='relu', input_shape = (28,28,1)),\n    keras.layers.MaxPool2D(pool_size=(2,2)),\n    keras.layers.BatchNormalization(),\n    keras.layers.Dropout(0.4),\n    keras.layers.Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', \n                 activation ='relu'),\n    keras.layers.MaxPool2D(pool_size=(2,2)),\n    keras.layers.BatchNormalization(),\n    keras.layers.Dropout(0.4),\n    keras.layers.Conv2D(filters = 128, kernel_size = (3,3),padding = 'Same', \n                 activation ='relu'),\n    keras.layers.MaxPool2D(pool_size=(2,2)),\n    keras.layers.BatchNormalization(),\n    keras.layers.Dropout(0.4),\n    keras.layers.Flatten(),\n    keras.layers.Dense(28*28, activation='relu'),\n    keras.layers.BatchNormalization(),\n    keras.layers.Dropout(0.4),\n    keras.layers.Dense(28*28, activation='relu'),\n    keras.layers.BatchNormalization(),\n    keras.layers.Dropout(0.4),\n    keras.layers.Dense(28*28, activation='relu'),\n    keras.layers.Dense(10, activation='softmax')\n    ])\n    X_train, X_val = X[train_index], X[val_index]\n    y_train, y_val = y[train_index], y[val_index]\n    model.compile(optimizer='adam',\n              loss='sparse_categorical_crossentropy',\n              metrics=['accuracy'])\n    history = model.fit(\n    x=X_train, y=y_train, batch_size=100, epochs=250, verbose=1, callbacks=[early_stopping,learning_rate_reduction],\n    validation_data=(X_val,y_val), shuffle=True)\n    # predict results\n    results = model.predict(X_test)\n    # select the index with the maximum probability\n    results = np.argmax(results,axis = 1)\n    results = pd.Series(results,name=\"Label\")\n    sub = pd.concat([sub, results],axis=1)\n","9f134932":"sub[\"result\"] = sub.mode(dropna=True,axis=1)[0]\nresult = pd.Series(sub[\"result\"],name=\"Label\")\nsubmission = pd.concat([pd.Series(range(1,28001),name = \"ImageId\"),result],axis = 1)\nsubmission = submission.dropna().astype('int32')\nsubmission.to_csv(\"mnist_ansamble_of_cnn.csv\",index=False)","fc2bbced":"# Import necessary libraries","0ece8b66":"# Making a submission","28166e46":"# We split the dataset on 7 folds.The folds are made by preserving the percentage of samples for each class.","d566d6c1":"# **In this notebook i will show how easy to take 0.99475 in Public Score.**\n","53a4575a":"# Load data from csv files","cd77dbb7":"# Reshape images to (batch_size x height x width x channels)","9f1a6862":"You can change here:\n* n_splits(number to splits,and see how to change the result)\n* batch_size(for instance take batch_size  200 or 50)\n* or change  probability of an element to be zeroed in Dropout layers\n* change patience in early_stopping and learning_rate_reduction","3f2f5ad9":"# Split data and labels and normalize images","6b094f9f":"# Define a training strategy we reduct learning after 3 epochs without improvements,and stop the train after 30 epochs without improvements"}}