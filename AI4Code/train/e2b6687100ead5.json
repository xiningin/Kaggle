{"cell_type":{"3577b11e":"code","687f7ab5":"code","7782d903":"code","d4a37794":"code","49706684":"code","bfb661ad":"code","53dcac11":"code","a220a746":"code","28157a09":"code","8e91e24f":"code","2280be21":"code","98e35f4b":"code","5f8cd76e":"code","bee060e4":"code","b7cfe92b":"code","a7abc0d2":"code","01b17319":"code","51a44323":"code","b9f8f131":"code","40d34fc9":"code","807e4522":"code","9b7202e0":"code","e02ac97d":"code","16c14fa2":"code","95a00c1f":"code","1a15e75b":"code","8501bb57":"code","e356f163":"markdown","24e0ead6":"markdown","c94dc11e":"markdown","99c67a76":"markdown","49aaa8e1":"markdown","532ec0bf":"markdown","6b2d0452":"markdown","beacbb99":"markdown","2b066956":"markdown","7b170665":"markdown","bb306a21":"markdown","0e235f3f":"markdown","56589d50":"markdown","76bb7269":"markdown","6065cc98":"markdown"},"source":{"3577b11e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","687f7ab5":"import pandas as pd\npd.plotting.register_matplotlib_converters()\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nprint(\"Setup Complete\")","7782d903":"# Fill in the line below: Specify the path of the CSV file to read for train and \nmy_filepath = \"..\/input\/titanic\/train.csv\"\nmy_filepath_test = \"..\/input\/titanic\/test.csv\"\n#We load the training csv file in X, the panda DataFrame \nX_train_full = pd.read_csv(my_filepath)\n#We load the test file in X_test taht we can change and test_data that we won't change \ntest_data = pd.read_csv(my_filepath_test)\nX_test = pd.read_csv(my_filepath_test)\n#We immediatly delete any rows where we don't have target values : \nX_train_full = X_train_full.dropna(subset = ['Survived'] )","d4a37794":"X_train_full.head()","49706684":"X_train_full.info()","bfb661ad":"X_train_full.describe()","53dcac11":"X_train_full.corr()","a220a746":"sns.barplot(x='Pclass',y='Survived',data=X_train_full)","28157a09":"sns.barplot(x='Parch',y='Survived',data=X_train_full)","8e91e24f":"sns.barplot(x='Embarked',y='Survived',data=X_train_full)","2280be21":"sns.barplot(x='Sex',y='Survived',data=X_train_full)","98e35f4b":"sns.lmplot(x=\"Fare\", y=\"PassengerId\", hue=\"Survived\", data=X_train_full,height=5, aspect=3)","5f8cd76e":"# Drop outliers spotted on the graph \nprint(X_train_full.shape) \n\nX_train_full = X_train_full.drop(X_train_full[X_train_full['Fare'] > 500].index)\n\nprint(X_train_full.shape)","bee060e4":"def missing_percentage(df):\n    \"\"\"This function takes a DataFrame(df) as input and returns two columns, total missing values and total missing values percentage\"\"\"\n    total = df.isnull().sum().sort_values(ascending = False)\n    percent = round(df.isnull().sum().sort_values(ascending = False)\/len(df)*100,2)\n    return pd.concat([total, percent], axis=1, keys=['Total','Percent'])\n\n\n\n#missing values in train data\nmissing_percentage(X_train_full)","b7cfe92b":"#missing values in test data\nmissing_percentage(X_test)","a7abc0d2":"print(len(X_train_full.Cabin.unique()))","01b17319":"X_train_full = X_train_full.drop(['Cabin'], axis=1)\nX_test = X_test.drop(['Cabin'], axis=1)","51a44323":"X_train_full['Title'] = X_train_full.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\nX_test['Title']  = X_test.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\npd.crosstab(X_train_full['Title'], X_train_full['Sex'])\nX_test.head()","b9f8f131":"X_train_full.Title.value_counts()","40d34fc9":"X_test.Title.value_counts()","807e4522":"for i in [X_train_full,X_test]:\n  i['Title'] =i['Title'].replace(['Lady', 'Countess','Capt', 'Col',\n  'Don', 'Major', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n  i['Title'] = i['Title'].replace('Mlle', 'Miss')\n  i['Title'] = i['Title'].replace('Ms', 'Miss')\n  i['Title'] = i['Title'].replace('Mme', 'Mrs')\n  i['Title'] = i['Title'].replace('Dr', 'Mrs')\n  i['Title'] = i['Title'].replace('Rev', 'Mrs')\n\n\n\nX_train_full[['Title', 'Survived']].groupby(['Title'], as_index=False).mean()\nX_train_full = X_train_full.drop(['Name'], axis=1)\nX_test = X_test.drop(['Name'], axis=1)","9b7202e0":"# Separate target from predictors\ny = X_train_full.Survived\nX = X_train_full.drop(['Survived'], axis=1)\n\n# We don't use train test split we use cross validation here. \n\n# \"Cardinality\" means the number of unique values in a column\n# Select categorical columns with relatively low cardinality (convenient but arbitrary)\ncategorical_cols = [cname for cname in X.columns if X[cname].nunique() < 10 and \n                        X[cname].dtype == \"object\"]\n\n# Select numerical columns\nnumerical_cols = [cname for cname in X.columns if X[cname].dtype in ['int64', 'float64']]\n\n#Drop passenger ID it doesn't help building the model we will use it as index :\n\nnumerical_cols.remove('PassengerId')\n\n\n# Keep selected columns only\nmy_cols = categorical_cols + numerical_cols\nX_train = X[my_cols].copy()\nX_test = X_test[my_cols].copy()","e02ac97d":"print(categorical_cols)","16c14fa2":"from sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder\n\n# Preprocessing for numerical data\nnumerical_transformer = SimpleImputer(strategy='median')\n\n# Preprocessing for categorical data\n# We drop first column to avoid dummy trap\ncategorical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='most_frequent')),\n    ('onehot', OneHotEncoder(drop = 'first'))\n])\n\n# Bundle preprocessing for numerical and categorical data\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numerical_transformer, numerical_cols),\n        ('cat', categorical_transformer, categorical_cols)\n    ])","95a00c1f":"from sklearn.ensemble import RandomForestClassifier\n\nmodel = RandomForestClassifier(n_estimators=100, max_depth=7, random_state=0)\n","1a15e75b":"from sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import mean_absolute_error\n\n# Bundle preprocessing and modeling code in a pipeline\nmy_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n                              ('model', model)\n                             ])\n\n# Multiply by -1 since sklearn calculates *negative* MAE\nscores = -1 * cross_val_score(my_pipeline, X_train, y,\n                              cv=5,\n                              scoring='neg_mean_absolute_error')\n\nprint(\"MAE scores:\\n\", scores)\n\nprint(\"Average MAE score (across experiments):\")\nprint(scores.mean())","8501bb57":"my_pipeline.fit(X_train,y)\npredictions = my_pipeline.predict(X_test)\n\noutput = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions})\noutput.to_csv('my_submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")","e356f163":"Almost all the values that are not null are unique wich mean that we can even impute this column, let's drop it. ","24e0ead6":"# Step 2 - Analyze the data","c94dc11e":"# Step - 1 - Load the data\n\nI also delete right away any rows with NaN values in the target feature \"Survived\" ","99c67a76":"# Let's see if we can use the column name to build our model ","49aaa8e1":"It looks from the correlation table that there is two interesting numerical data regarding survived : \n\n\n*  P class has a small negative correlation wich mean that the smaller the class the higher probability to survive\n*  Fare has the small positive correlation wich means that the higher the fare the higher probability to survive\n\nWe want to confirm this hypothesis :","532ec0bf":"# Step 3 - Checking NaN and empty values","6b2d0452":"<a id=\"6\"><\/a> <br>\n# 6) Create the submission file ","beacbb99":"**Content:**\n1. [Analysis of the Dataset](#1) \n1. [Creation of training DataSet](#2)\n1. [Creating preprocessing pipeline](#3)\n1. [Define the models](#4)\n1. [Fit and evluate our models](#5)\n1. [Submission](#6)","2b066956":"<a id=\"2\"><\/a> <br>\n# 2) Creation of training Dataset  : \n\nHere we want to prepare our variables and have clear notations easy to work with. \n\nIt will be important to have those clear notations when we will use pipelines ","7b170665":"Here we can see outliers : \n* An outlier is an observation that lies an abnormal distance from other values in a random sample from a population\n\nThose 3 values will modify our predictions but are not \"Normal values\" we need to get rid of them :","bb306a21":"<a id=\"4\"><\/a> <br>\n# 4) Define the models you want to use and compare : \n\nHere we decide the models we want to choose, we will compare those models later on with cross validation. \n\nModel performances are also influenced by feature engineering, for example gradient descent (XGBoost) need scaling. ","0e235f3f":"<a id=\"5\"><\/a> <br>\n# 5) Fit the model pipeline and evaluate by cross validation : ","56589d50":"<a id=\"3\"><\/a> <br>\n# 3) Creating the preprocessing Pipeline : \n\nHere we use a Pipeline to preprocess out data, it's our feature engineering pipeline. ","76bb7269":"We can clearly see that age and cabin are the two main issues with missing values. \n\nLet's see if cabin is worth saving or should we drop it : ","6065cc98":"<a id=\"1\"><\/a> <br>\n# 1. Analysis of the Dataset\n\nHere we are going to use Seaborn, Matplotlib and other librairies to analyze the Dataset. \n\nHaving a good understanding of features dynamic is very important. "}}