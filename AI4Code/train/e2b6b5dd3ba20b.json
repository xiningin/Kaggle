{"cell_type":{"660f3305":"code","8b89e6ce":"code","75d1cd4c":"code","a4da93e3":"code","b0fcfbea":"code","6eca7304":"code","4deb33f5":"code","43b86ff8":"code","3dd12f3c":"code","bf6994f8":"code","adac338d":"code","b0044521":"code","449ed46d":"code","d7a9ba08":"code","f8473dc6":"code","d142675f":"code","c432d05f":"code","6ece113c":"code","594201bb":"code","4f7bb6fd":"code","227a691a":"code","fc23e113":"code","84899fa3":"code","ec628aa3":"code","403e3bea":"code","a537079a":"code","ae4da21f":"code","a67b7356":"code","a8a2e535":"markdown","7a347cb5":"markdown","19950b44":"markdown","e9b520e6":"markdown","5d2b2b6f":"markdown"},"source":{"660f3305":"from keras.datasets import imdb","8b89e6ce":"((XT,YT),(Xt,Yt)) = imdb.load_data(num_words=10000)   #XT training # Xt testing","75d1cd4c":"len(Xt),len(XT)","a4da93e3":"print(XT[0])","b0fcfbea":"word_idx = imdb.get_word_index()","6eca7304":"# print(word_idx.items())    # you run this cell to see the output","4deb33f5":"idx_word = dict([value,key] for (key,value) in word_idx.items())","43b86ff8":"# print(idx_word.items())    # you can also run this cell to see the output","3dd12f3c":"actual_review = ' '.join([idx_word.get(idx-3,'#') for idx in XT[0]])","bf6994f8":"print(actual_review)","adac338d":"import pandas as pd\nimport numpy as np \nimport matplotlib.pyplot as plt","b0044521":"##next step ----> Vectorize the data\n## Vocab size --> 10,000 we will make sure every sentence is represented by a vector of len 10,000 [0000010001001011...]\n\n\ndef  vectorize_sentences(sentences,dim = 10000):\n  outputs = np.zeros((len(sentences),dim))\n\n\n  for i,idx in enumerate(sentences):\n    outputs[i,idx] = 1\n\n  return outputs","449ed46d":"X_train  = vectorize_sentences(XT)\nX_test = vectorize_sentences(Xt)","d7a9ba08":"print(X_train.shape)\nprint(X_test.shape)","f8473dc6":"print(X_train[0])","d142675f":"Y_train  = np.asarray(YT).astype('float32')\nY_test = np.asarray(Yt).astype('float32')","c432d05f":"from keras import models\nfrom keras.layers import Dense","6ece113c":"# define the model\nmodel  = models.Sequential()\nmodel.add(Dense(16,activation = 'relu' , input_shape = (10000,)))\nmodel.add(Dense(16,activation = 'relu'))\nmodel.add(Dense(1,activation = 'sigmoid'))","594201bb":"# here we are compiling\nmodel.compile(optimizer='rmsprop',loss='binary_crossentropy',metrics=['accuracy']) # you can use adam insted of rmsprop","4f7bb6fd":"model.summary()","227a691a":"x_val = X_train[:5000]\nx_train_new = X_train[5000:]\n\ny_val = Y_train[:5000]\ny_train_new = Y_train[5000:]","fc23e113":"hist = model.fit(x_train_new,y_train_new,epochs = 4,batch_size=512,validation_data =(x_val,y_val))","84899fa3":"h = hist.history","ec628aa3":"plt.plot(h['val_loss'],label = 'validation loss')\nplt.plot(h['loss'],label = 'training loss')\nplt.xlabel('epochs')\nplt.ylabel('loss')\nplt.legend()\nplt.show()\nplt.style.use('seaborn')","403e3bea":"plt.plot(h['val_accuracy'],label = 'validation Acc')\nplt.plot(h['accuracy'],label = 'training Acc')\nplt.xlabel('epochs')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.show()\nplt.style.use('seaborn')","a537079a":"h = hist.history","ae4da21f":"# let's calculate accuracy\nmodel.evaluate(X_test,Y_test)[1]","a67b7356":"model.evaluate(X_train,Y_train)[1]","a8a2e535":"## Training and validation","7a347cb5":"***Data Preparation***","19950b44":"## Sentiment Analysis on IMDB Reviews\nThis is a simple project created with the intention of putting into practice knowledge learned about Machine Learning, Natural Language Processing and Sentiment Anaylisis.\n\n## Dataset\nThis is a dataset for binary sentiment classification containing substantially more data than previous benchmark datasets. We provide a set of 25,000 highly polar movie reviews for training, and 25,000 for testing.\n\nwe do not need to download datasets locally as necessary functions have been included to download dataset from jupyter notebook","e9b520e6":"### Build a network\n## Define our model architecture\n1 use fully connected\/dense layers with RELU activation\n\n2 two hidden layers with 16 unit each\n\n3 one output layer with 1 unit(sigmoid activation funct)\n","5d2b2b6f":"## Visualize"}}