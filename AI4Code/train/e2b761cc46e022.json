{"cell_type":{"ae8a7de0":"code","aad7aed5":"code","2c3b64c3":"code","3bc2b67e":"code","8c555cb2":"code","ec0292f3":"code","6aab1367":"code","ec1743ba":"code","308fb96b":"code","84b97726":"code","a5488509":"code","f4ed0782":"code","742bb32a":"code","b8a6484f":"code","0c7d1264":"code","d8b5d76c":"code","aa5653a3":"code","b8f9d8a2":"code","283e3706":"code","1810280d":"code","23c649fe":"code","90536b10":"code","934c402d":"code","99fdb909":"code","af5e126a":"code","8930b207":"markdown","d3eb9268":"markdown","799fef74":"markdown","cb9dc9f7":"markdown","fffd2b88":"markdown"},"source":{"ae8a7de0":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","aad7aed5":"import shutil\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom PIL import Image\n\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Input, Add, Dense, Activation, BatchNormalization, Flatten, Conv2D, AveragePooling2D, MaxPooling2D, GlobalMaxPooling2D, Dropout\nfrom tensorflow.keras import Model\nfrom tensorflow.keras.applications import ResNet50, MobileNetV2, ResNet101\nfrom tensorflow.keras.optimizers import Adam, SGD\nfrom tensorflow.keras.preprocessing import image\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator, save_img\n\nimport cv2","2c3b64c3":"! unzip -o \/kaggle\/input\/platesv2\/plates.zip","3bc2b67e":"# \u6383\u9664\nif (os.path.exists('data_augment')):\n    shutil.rmtree('data_augment')\nif (os.path.exists('data_test')):\n    shutil.rmtree('data_test')\n\n# \u753b\u50cf\u751f\u6210\u7528 \u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u4f5c\u6210\nos.makedirs('data_augment\/plates\/train\/cleaned')\nos.makedirs('data_augment\/plates\/train\/dirty')\n\n# \u30c6\u30b9\u30c8\u753b\u50cf\u52a0\u5de5\u7528 \u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u4f5c\u6210\nos.makedirs('data_test\/plates\/test')","8c555cb2":"# \u753b\u50cf\u52a0\u5de5\u30fb\u751f\u6210\u7cfb\u95a2\u6570\n\n\n## \u753b\u50cf\u6a19\u6e96\u5316\ndef image_standardization(img):\n    return tf.image.per_image_standardization(\n        img\n    )\n\n\n## \u30b0\u30ec\u30fc\u30b9\u30b1\u30fc\u30eb\ndef image_grayscale(img):\n    \n    # PIL\u578b -> OpenCV\u578b\n    img = np.array(img, dtype=np.uint8)\n    img = img[:, :, ::-1]\n    \n    # \u30b0\u30ec\u30fc\u30b9\u30b1\u30fc\u30eb\n    #img_gray, _ = cv2.decolor(img)\n    img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n    \n    # PIL\u578b \u3092\u8fd4\u5374\n    return cv2.cvtColor(img_gray, cv2.COLOR_BGR2RGB)\n\n## \u80cc\u666f\u9664\u53bb\ndef grabCutFirst(img):\n\n    # PIL\u578b -> OpenCV\u578b\n    img = np.array(img, dtype=np.uint8)\n    img = img[:, :, ::-1]\n    \n    height, width = img.shape[:2]\n    rect = (15, 15, width-30, height-30)\n    \n    mask = np.zeros(img.shape[:2],np.uint8)\n    bgdModel = np.zeros((1,65),np.float64)\n    fgdModel = np.zeros((1,65),np.float64)\n    cv2.grabCut(img,mask,rect,bgdModel,fgdModel,5,cv2.GC_INIT_WITH_RECT)\n    mask2 = np.where((mask==2)|(mask==0),0,1).astype('uint8')\n    output_img = img*mask2[:,:,np.newaxis]   # \u80cc\u666f\u304c[0,0,0]\uff08\u9ed2\uff09\u3068\u306a\u308b\n\n    # \u80cc\u666f\u9818\u57df\u306e\u53d6\u5f97\n    background = img - output_img\n\n    # \u9ed2\u304b\u3089\u767d\u306b\u5909\u63db\n    background[np.where((background > [0, 0, 0]).all(axis = 2))] = [255, 255, 255]\n\n    # \u5408\u6210\u5024 + PIL\u578b \u3092\u8fd4\u5374\n    return cv2.cvtColor(background + output_img, cv2.COLOR_BGR2RGB)\n\n\n## \u30af\u30ed\u30c3\u30d7\u95a2\u6570\ndef crop(img, l):\n\n    img = Image.fromarray(img.astype(np.uint8))\n    \n    # \u30af\u30ed\u30c3\u30d7\n    l2 = l \/\/ 2     # \u30af\u30ed\u30c3\u30d7\u3057\u305f\u3044\u5927\u304d\u3055\u306e\u534a\u5206\n    w, h = img.size # \u753b\u50cf\u306e\u6a2a\u5e45\u3068\u9ad8\u3055\n    w2 = w \/\/ 2     # \u6a2a\u5e45\u306e\u534a\u5206\n    h2 = h \/\/ 2     # \u9ad8\u3055\u306e\u534a\u5206\n    img = img.crop((w2 - l2, h2 - l2, w2 + l2, h2 + l2))\n\n    # \u30ea\u30b5\u30a4\u30ba\uff5c\u5165\u529b\u30b5\u30a4\u30ba\u306b\u623b\u3059\n    img = img.resize((w, h))\n    \n    return img\n\n\n## \u5b66\u7fd2\u753b\u50cf\u52a0\u5de5\u30fb\u751f\u6210\u95a2\u6570\ndef image_transform_for_training(org_image_dir_path, crop_size_list, rotation_range):\n    \n    # ImageDataGenerator \u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\u751f\u6210\n    datagen = ImageDataGenerator(\n           rotation_range=rotation_range,\n           width_shift_range=0,\n           height_shift_range=0,\n           shear_range=0,\n           zoom_range=0,\n           horizontal_flip=False,\n           vertical_flip=False)#,\n           #preprocessing_function=image_standardization)\n    \n    for org_image_file_name in os.listdir(org_image_dir_path):\n        \n        root, ext = os.path.splitext(org_image_file_name)\n        if (ext != '.jpg'):\n            continue\n\n        #print('image transform for training : ' + org_image_file_name)\n            \n        # \u753b\u50cf\u30d5\u30a1\u30a4\u30eb\u3092PIL\u5f62\u5f0f\u3067\u30aa\u30fc\u30d7\u30f3\n        img = image.load_img(org_image_dir_path + '\/' + org_image_file_name)\n        # PIL\u5f62\u5f0f\u3092numpy\u306endarray\u5f62\u5f0f\u306b\u5909\u63db\n        img = image.img_to_array(img)\n        # \u80cc\u666f\u9664\u53bb\n        x = grabCutFirst(img)\n        # (height, width, 3) -> (1, height, width, 3)\n        x = x.reshape((1,) + x.shape)\n        \n        # \u753b\u50cf\u751f\u6210\n        i = 0\n        for d in datagen.flow(x, batch_size=1):\n            \n            grab_cut_img = grabCutFirst(d[0])\n            \n            for l in crop_size_list:\n                crop_img = image.img_to_array(crop(grab_cut_img, l))\n                #std_img = np.array(image_standardization(crop_img))\n                std_img = crop_img\n                std_img = image_grayscale(std_img)\n                save_img('data_augment\/' + org_image_dir_path + '\/' + root + '_' + str(l) + '_' + str(i * rotation_range) + ext, Image.fromarray(std_img.astype(np.uint8)))\n            \n            i = i + 1\n            if ((360\/rotation_range) <= i):\n                break\n\n                \n## \u30c6\u30b9\u30c8\u753b\u50cf\u52a0\u5de5\u95a2\u6570\ndef image_transfrom_for_test(org_image_dir_path, crop_size_list):\n    \n    for org_image_file_name in os.listdir(org_image_dir_path):\n\n        root, ext = os.path.splitext(org_image_file_name)\n        if (ext != '.jpg'):\n            continue\n    \n        #print('image transform for test : ' + org_image_file_name)\n    \n        img = image.load_img(org_image_dir_path + '\/' + org_image_file_name)\n        img = image.img_to_array(img)\n        img = grabCutFirst(img) # \u80cc\u666f\u9664\u53bb\n        \n        for l in crop_size_list:\n\n            if (os.path.exists('data_test\/' + org_image_dir_path + '\/' + str(l)) == False):\n                os.makedirs('data_test\/' + org_image_dir_path + '\/' + str(l))\n\n            crop_img =  image.img_to_array(crop(img, l))\n            #std_img = np.array(image_standardization(crop_img))\n            std_img = crop_img\n            std_img = image_grayscale(std_img)\n            save_img('data_test\/' + org_image_dir_path + '\/' + str(l) + '\/' + org_image_file_name,Image.fromarray(std_img.astype(np.uint8)))   ","ec0292f3":"# \u52a0\u5de5\u7528\u306e\u30af\u30ed\u30c3\u30d7\u30b5\u30a4\u30ba\u7a2e\u985e\ncrop_size_training_list = [91, 171, 251]\ncrop_size_test_list = [251]\n\n# \u751f\u6210\u7528\u306e\u56de\u8ee2\u5358\u4f4d\u89d2\u5ea6\nrotation_range = 90\n\n# \u52a0\u5de5\u30fb\u751f\u6210\n## \u5b66\u7fd2\u753b\u50cf\u52a0\u5de5\u30fb\u751f\u6210\nimage_transform_for_training('plates\/train\/cleaned', crop_size_training_list, rotation_range)\nimage_transform_for_training('plates\/train\/dirty', crop_size_training_list, rotation_range)\n## \u30c6\u30b9\u30c8\u753b\u50cf\u52a0\u5de5\nimage_transfrom_for_test('plates\/test', crop_size_test_list)","6aab1367":"# \u5b66\u7fd2\u56de\u6570\u7b49\nimage_size = (224, 224)\nbatch_size = 4\nepochs = 30","ec1743ba":"# \u5b66\u7fd2\u30c7\u30fc\u30bf\u30fb\u8a13\u7df4\ntrain_ds = tf.keras.preprocessing.image_dataset_from_directory(\n    #plates\/train\",\n    \"data_augment\/plates\/train\",\n    #labels='inferred', \n    #label_mode='categorical',\n    validation_split=0.3,\n    subset=\"training\",   # training => \u5168\u4f53\u30b5\u30f3\u30d7\u30eb x 0.7\n    seed=1307,\n    image_size=image_size,\n    batch_size=batch_size,\n)\n\n# \u5b66\u7fd2\u30c7\u30fc\u30bf\u30fb\u30d0\u30ea\u30c7\u30fc\u30b7\u30e7\u30f3\nval_ds = tf.keras.preprocessing.image_dataset_from_directory(\n    #plates\/train\",\n    \"data_augment\/plates\/train\",\n    #labels='inferred', \n    #label_mode='categorical',\n    validation_split=0.3,\n    subset=\"validation\",   # validation => \u5168\u4f53\u30b5\u30f3\u30d7\u30eb x 0.3\n    seed=1307,\n    image_size=image_size,\n    batch_size=batch_size,\n)","308fb96b":"# \u5b66\u7fd2\u30c7\u30fc\u30bf\u30fb\u30d0\u30ea\u30c7\u30fc\u30b7\u30e7\u30f3\uff5c\u76ee\u8996\u78ba\u8a8d\nplt.figure(figsize=(20, 20))\nfor images, labels in val_ds.take(1):\n    for i in range(batch_size):\n        ax = plt.subplot(7, 5, i + 1)\n        plt.imshow(images[i].numpy().astype(\"uint8\"))\n        plt.title(int(labels[i]))\n        plt.axis(\"off\")","84b97726":"# \u30c7\u30a3\u30fc\u30d7\u30e9\u30fc\u30cb\u30f3\u30b0\u30e2\u30c7\u30eb\ndef get_model():\n    # \u8ee2\u79fb\u30e2\u30c7\u30eb\n    ## https:\/\/keras.io\/ja\/applications\/#resnet50\n    input_shape = image_size + (3,)\n    model_res = ResNet101(include_top=False, input_shape=input_shape, weights='imagenet')\n    \n    # \u8ffd\u52a0\u30ec\u30a4\u30e4\u30fc\n    x = model_res.output\n    \n    x = Flatten()(x)\n\n    x = Dense(256)(x)\n    x = Activation('relu')(x)\n    x = Dropout(.5)(x)\n    \n    x = Dense(256)(x)\n    x = Activation('relu')(x)\n    x = Dropout(.5)(x)\n    \n    x = Dense(128)(x)\n    x = Activation('relu')(x)\n    x = Dropout(.5)(x)\n    \n    x = Dense(1)(x)\n    \n    outputs = Activation('sigmoid')(x)\n\n    # \u8ee2\u79fb\u30e2\u30c7\u30eb\u306e\u5b66\u7fd2\u306f\u3057\u306a\u3044\n    # \u8ffd\u52a0\u3057\u305f\u5c64\u4ee5\u5916\u306f\u30d5\u30ea\u30fc\u30ba\u3059\u308b\u3002(\u30d1\u30e9\u30e1\u30fc\u30bf\u66f4\u65b0\u3057\u306a\u3044)\n    for l in model_res.layers[1:]:\n        l.trainable = False\n    \n    # \u5408\u6210\uff5c\u8ee2\u79fb\u30e2\u30c7\u30eb + \u8ffd\u52a0\u30ec\u30a4\u30e4\u30fc\n    model = Model(model_res.input, outputs)\n    \n    return model","a5488509":"# \u30e2\u30c7\u30eb\u30a4\u30f3\u30b9\u30bf\u30f3\u30b9 | \u540c\u3058\u30e2\u30c7\u30eb\u5185\u5bb9\u3067\u30a2\u30f3\u30b5\u30f3\u30d6\u30eb\u5b66\u7fd2\u5b9f\u65bd\nmodels = {}\nmodels[0] = get_model()\nmodels[1] = get_model()\nmodels[2] = get_model()\n#models[3] = get_model()\n#models[4] = get_model()\n#models[0].summary()","f4ed0782":"# \u5b66\u7fd2\ndef learning(key, model):\n\n    ## \u30b3\u30f3\u30d1\u30a4\u30eb\n    #model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n    #model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n    #model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['binary_accuracy'])\n    model.compile(optimizer=Adam(decay=0.01), loss='binary_crossentropy', metrics=['binary_accuracy'])\n\n    ## \u5b66\u7fd2\u5b9f\u65bd\n    ### \u30a2\u30fc\u30ea\u30fc\u30b9\u30c8\u30c3\u30d4\u30f3\u30b0\u8a2d\u5b9a\n    callback = tf.keras.callbacks.EarlyStopping(\n        monitor='val_loss', \n        patience=15\n    )\n    ### \u30e2\u30c7\u30eb\u4fdd\u5b58\u8a2d\u5b9a\n    checkpoint = tf.keras.callbacks.ModelCheckpoint(\n        '\/tmp\/checkpoint_' + str(key), \n        monitor='val_binary_accuracy', \n        save_best_only=True   # val_binary_accuracy \u304c\u6700\u826f\u306e\u3082\u306e\u3092\u4fdd\u5b58\u3059\u308b\u8a2d\u5b9a\n    )\n    ### \u30d5\u30a3\u30c3\u30c6\u30a3\u30f3\u30b0\n    return model.fit(\n        train_ds,\n        validation_data=val_ds, \n        epochs=epochs, \n        callbacks=[callback, checkpoint]\n    )","742bb32a":"# \u5b66\u7fd2\u5b9f\u65bd\nresults = {}\nfor key, model in models.items():\n    print('=== model-' + str(key) + ' fiting ===')\n    results[key] = learning(key, model)","b8a6484f":"# \u5b66\u7fd2\u6982\u8981\u898b\u3048\u308b\u5316\nresult = results[0]\nhis_range = len(result.history['loss'])\n\nplt.figure(figsize=(15, 5))\n\nplt.subplot(1, 2, 1)\nplt.plot(range(1, his_range+1), result.history['binary_accuracy'], label=\"training\")\nplt.plot(range(1, his_range+1), result.history['val_binary_accuracy'], label=\"validation\")\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\n\nplt.subplot(1, 2, 2)\nplt.plot(range(1, his_range+1), result.history['loss'], label=\"training\")\nplt.plot(range(1, his_range+1), result.history['val_loss'], label=\"validation\")\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\n\nplt.show()","0c7d1264":"! ls plates\/test\/ | head ","d8b5d76c":"# \u30c6\u30b9\u30c8\u30b8\u30a7\u30cd\u30ec\u30a4\u30bf\u30fc\ndef create_test_generator(l):\n    test_datagen = ImageDataGenerator()\n    return test_datagen.flow_from_directory(  \n        'data_test\/plates\/test',\n        classes=[str(l)],\n        target_size = image_size,\n        batch_size = 100,\n        shuffle = False,        \n        class_mode = None)  ","aa5653a3":"# \u30c6\u30b9\u30c8\u30b8\u30a7\u30cd\u30ec\u30a4\u30bf\u30fc\u751f\u6210\ntest_generators = {}\nfor l in crop_size_test_list:\n    test_generators[str(l)] = create_test_generator(l)","b8f9d8a2":"# \u30ca\u30a4\u30fc\u30d6\u30c6\u30b9\u30c8_1\n\n## \u753b\u50cf\u306e\u8aad\u307f\u8fbc\u307f\n#img = Image.open('data_test\/plates\/test\/0028.jpg')\n#img = img.resize(image_size)\n#plt.imshow(img)\n\n## \u4e88\u6e2c\n#img = np.array(img)\n#model.predict([img[None,...]])","283e3706":"# \u30ca\u30a4\u30fc\u30d6\u30c6\u30b9\u30c8_2\nmodel = models[0]\n\n## \u9806\u756a\u30ea\u30bb\u30c3\u30c8\ntest_generator = test_generators[str(crop_size_test_list[0])]\ntest_generator.reset()\n\n## \u7cbe\u5ea6\u611f\u76ee\u8996\u30c1\u30a7\u30c3\u30af  \nfor d in test_generator:\n    for i in range(30):\n        print(model.predict([d[i][None,...]]))\n        plt.imshow(d[i].astype(np.uint8))\n        plt.show()\n    break","1810280d":"# \u4e88\u6e2c\npredicts = {}\nfor key, model in models.items():\n    for key_gen, test_generator in test_generators.items():\n        # \u9806\u756a\u30ea\u30bb\u30c3\u30c8\n        test_generator.reset()\n        # \u4e88\u6e2c\n        predicts['model:' + str(key) + ' - inputsize:' + str(key_gen)] = pd.Series(\n            np.ravel( # \u4e00\u6b21\u5143\u5316\n                model.predict_generator(\n                    test_generator, \n                    steps = len(test_generator.filenames)\n                )\n            )\n        )","23c649fe":"predicts_df = pd.DataFrame(predicts)\npredicts_df.head(30)","90536b10":"sub_df = pd.read_csv('..\/input\/platesv2\/sample_submission.csv')","934c402d":"f = lambda x: 'dirty' if x > 0.5 else 'cleaned'\nsub_df['label'] = pd.DataFrame(\n    np.mean(\n        predicts_df, \n        axis=1\n    )\n)\nsub_df['label'] = sub_df['label'].apply(f)\nsub_df.head(30)","99fdb909":"sub_df['label'].value_counts()","af5e126a":"sub_df.to_csv('submission.csv', index=False)","8930b207":"# MAKE submission.csv","d3eb9268":"# INTRODUCTION","799fef74":"# TRANSFORM IMAGES","cb9dc9f7":"# LEARNING MODEL","fffd2b88":"# PREPARE FOR LEARNING MODEL"}}