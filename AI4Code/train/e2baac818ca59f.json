{"cell_type":{"a1ced74a":"code","9b47738c":"code","b1553972":"code","037c32de":"code","1cd3b0ac":"code","e0cc35f4":"code","4987c02e":"code","a5793b27":"code","e8e01e28":"code","a9d53a44":"code","0bfb1178":"code","d3307207":"code","e4cd1f0e":"code","1d7288bc":"code","9713336e":"code","a5021518":"code","bc411029":"code","c05cac45":"code","6b937d9d":"code","d8e3f870":"code","1fbe53ac":"code","c54a3850":"code","81456655":"code","306ea9ae":"markdown","74c78733":"markdown","33fbc895":"markdown","0c25d6ac":"markdown","8f6cfc26":"markdown","8d20dff7":"markdown","5270fbb3":"markdown","b559130f":"markdown"},"source":{"a1ced74a":"import pandas as pd # for data preprocessing in the workspace\nimport numpy as np #calculus and linear algebra\n# plotters\nimport matplotlib.pyplot as plt \nimport seaborn as sns\n\nimport plotly.plotly as py  # plotly\nfrom plotly.offline import init_notebook_mode, iplot\ninit_notebook_mode(connected=True)\nimport plotly.graph_objs as go","9b47738c":"Data = pd.read_csv('..\/input\/vgsales.csv')\nData.head(20)# first 20 records ","b1553972":"Data.columns","037c32de":"plt.figure(figsize=(15,8))\nsns.set()\nplt.grid(True)\nsort_plat = Data['Platform'].value_counts().sort_values(ascending=False)\nsort_plat.head()\nsns.barplot(y=sort_plat.index,x=sort_plat.values,orient='h')\nplt.xlabel('Values count')\nplt.ylabel('Games Platform')\nplt.title('Grouped Platforms count')","1cd3b0ac":"\ndf_gl=Data.loc[:99,:] # data.iloc[:100,:] -- data.head(100)\n\nimport plotly.graph_objs as go\n\ntrace1=go.Scatter(\n                x=df_gl.Rank,\n                y=df_gl.NA_Sales,\n                mode=\"lines+markers\",\n                name=\"North America Sales\",\n                text=df_gl.Name)\ntrace2=go.Scatter(\n                x=df_gl.Rank,\n                y=df_gl.EU_Sales,\n                mode=\"lines\",\n                name=\"Europe Sales\",\n                text=df_gl.Name)\n\nedit_df=[trace1,trace2]\nlayout=dict(title=\"World rank of the top 100 video games, EU Sales and NA Sales .\",\n            xaxis=dict(title=\"World Rank\",tickwidth=5,ticklen=8,zeroline=False))\nfig=dict(data=edit_df,layout=layout)\niplot(fig)","e0cc35f4":"Data.describe()","4987c02e":"\nsns.heatmap(Data.corr(),cbar=True,annot=True)","a5793b27":"max_Sales = Data[Data['Global_Sales']==max(Data['Global_Sales'])]\nmax_Sales","e8e01e28":"sns.set()\nsns.regplot(Data['Global_Sales'],Data['NA_Sales'])\nplt.xlabel('Global Sales')\nplt.ylabel('North America Sales')\nplt.title('Global Sales - NA Sales ')","a9d53a44":"sns.regplot(Data['Global_Sales'],Data['EU_Sales'])\nplt.xlabel('Global Sales')\nplt.ylabel('Europe Sales')\nplt.title('Global Sales - EU Sales ')","0bfb1178":"sns.regplot(Data['Global_Sales'],Data['JP_Sales'])\nplt.xlabel('Global Sales')\nplt.ylabel('Japan Sales')\nplt.title('Global Sales - JP Sales ')","d3307207":"sns.regplot(Data['Global_Sales'],Data['Other_Sales'])\nplt.xlabel('Other countries Sales')\nplt.ylabel('North America Sales')\nplt.title('Global Sales - Others Sales ')","e4cd1f0e":"plt.figure(figsize=(15,8))\ncop = Data.copy()\ncop.sort_values('Global_Sales',ascending=False)\nprint(cop.shape)\ncop1 = cop.head(1000).copy()\nsns.barplot(y=cop1['Publisher'],x=cop1['Global_Sales'],orient='h')\n","1d7288bc":"#Some label encoding since we have some categorical DATA\nobj_cols = [col for col in cop.columns if cop[col].dtype=='object']\nprint('Columns that will be encoded are ='+str(obj_cols))","9713336e":"#Quick peak into NA columns\n\nfig = plt.figure(figsize=(15, 8))\ncop.isna().sum().sort_values(ascending=True).plot(kind='barh', fontsize=20)\n","a5021518":"cop.drop('Year',axis=1)","bc411029":"print(cop.shape)","c05cac45":"# Get number of unique entries in each column with categorical data\nobject_nunique = list(map(lambda col: cop[col].nunique(), obj_cols))\nd = dict(zip(obj_cols, object_nunique))\n\n# Print number of unique entries by column, in ascending order\nsorted(d.items(), key=lambda x: x[1])","6b937d9d":"from sklearn.model_selection import train_test_split #Best approach to test the model\nfrom sklearn.metrics import mean_absolute_error # mean absolute error , error = predictions - validation_y then abs for pos value\nfrom sklearn.tree import DecisionTreeRegressor #model\nfeatures = ['NA_Sales','EU_Sales','JP_Sales','Other_Sales']#our features\nX = cop[features]\ny = cop.Global_Sales #target\ntrain_X , val_X , train_y , val_y = train_test_split(X,y,test_size=0.25,random_state=1)\nmodel = DecisionTreeRegressor(random_state=1)\nmodel.fit(train_X,train_y)","d8e3f870":"predictions =model.predict(val_X)\nmae = mean_absolute_error(predictions, val_y)\nprint('Mean absolute error '+str(mae))","1fbe53ac":"\ndf = pd.DataFrame({'Actual': val_y, 'Predicted': predictions})\ndf\n","c54a3850":"\ndf1 = df.head(80)\ndf1.plot(kind='bar',figsize=(15,8))\nplt.show()","81456655":"val_X['Global_Sales']=predictions\nprint(len(df.index))\nval_X['Rank'] = df.index\nval_X[['Rank','Global_Sales']].to_csv('sub_for_nothing.csv',index=False)\ndf.to_csv('predvsval_y.csv',index=False)","306ea9ae":"# In our Case we won't talk about houses but about VIDEO GAMES HAHAH\n**Don't expect a lot of EDAs ,as the titles says its an intro to machine learning decisions tree model **","74c78733":"**Since we have 4 categorical data with far more than 10 entries , wont be good to OH( One hot encoding) them , label coding will be fair enough but will overfit the model , we will just skip the label encoding **","33fbc895":"As we can clearly see , NA_Sales is the highest correlation but we cant deny EU_Sales and the Other_Sales that are stongly correlated too.","0c25d6ac":"# SIMPLE EDA to show the correlation and explains the model choosen","8f6cfc26":"### We reached together the end of this Kernel , if you found it useful an upvote would be very appreciated , thanks see you in the next kernel !","8d20dff7":"## Let's have a look on the game platform which contains the most games.","5270fbb3":"## World rank of the top 100 video games, north america sales and europe sales.","b559130f":"# HI , this kernel shows machine learning basics for the very first beginners \n We'll start with a model called the Decision Tree. There are fancier models that give more accurate predictions. But decision trees are easy to understand, and they are the basic building block for some of the best models in data science.\n\nFor simplicity, we'll start with the simplest possible decision tree. \n\n![First Decision Trees](http:\/\/i.imgur.com\/7tsb5b1.png)\n\nIt divides houses into only two categories. The predicted price for any house under consideration is the historical average price of houses in the same category.\n\nWe use data to decide how to break the houses into two groups,  and then again to determine the predicted price in each group.  This step of capturing patterns from data is called **fitting** or **training** the model. The data used to **fit** the model is called the **training data**.  \n\nThe details of how the model is fit (e.g. how to split up the data) is complex enough that we will save it for later. After the model has been fit, you can apply it to new data to **predict** prices of additional homes."}}