{"cell_type":{"26547956":"code","52b1c782":"code","32eeefcf":"code","b912a873":"code","e33803b9":"code","e1dbd8ab":"code","c7f53216":"code","127cb615":"code","d50a4429":"code","cf00b1ba":"code","1157b21f":"code","45868390":"code","c9da74e2":"code","126438ac":"code","debe24e6":"code","15bfc4e8":"code","008a6088":"code","042c74be":"code","ffdfa66f":"code","de4b1b04":"code","eda51a6a":"code","9d49bf4a":"code","b414a2f2":"code","87d15664":"code","bb192fa8":"code","1765c30a":"code","dc404d22":"code","130da2d6":"code","6c5b9cc0":"markdown","f9766d22":"markdown","f469a1ac":"markdown","bd5e5eff":"markdown","9987d711":"markdown","affea615":"markdown","a8786ca3":"markdown","9597e617":"markdown","cd8e6bdd":"markdown","9e5c89ac":"markdown","50fc815a":"markdown","eec9c961":"markdown","4de32c5c":"markdown","a67d01c2":"markdown","88b54c32":"markdown","68e8c3a6":"markdown"},"source":{"26547956":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","52b1c782":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom scipy import stats\nfrom sklearn.tree import DecisionTreeRegressor","32eeefcf":"train = pd.read_csv('..\/input\/train.csv',index_col='Id')\ntest = pd.read_csv('..\/input\/test.csv',index_col='Id')","b912a873":"test.head()","e33803b9":"nan_cols=train.columns[train.isna().any()]\ntrain[nan_cols].isna().sum().sort_values(ascending=False) \/ train.shape[0] * 100","e1dbd8ab":"sns.set(style=\"white\", palette=\"muted\", color_codes=True)\n\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(7, 5))\nsns.despine(left=True)\n\nsns.distplot(train.SalePrice, kde=False, color=\"b\", ax=ax)\nplt.title('Distribution of Home Prices',fontdict=dict(size=18,weight='bold'))\nplt.setp(ax, yticks=[])\nplt.tight_layout();","c7f53216":"# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(7, 5))\nsns.despine(left=True)\n\nsns.distplot(np.log(train.SalePrice), kde=False, color=\"b\", ax=ax)\nplt.title('Log-Norm Distribution of Home Prices',fontdict=dict(size=18,weight='bold'))\nplt.setp(ax, yticks=[])\nplt.tight_layout();","127cb615":"# built-in formula:\nprint('Skew: '+str(stats.skew(train.SalePrice)))\nprint('Log-Norm Skew: '+str(stats.skew(np.log(train.SalePrice))))","d50a4429":"# built-in kurtosis formula:\nprint('Kurtosis: '+str(stats.kurtosis(train.SalePrice)))\nprint('Log-Norm Kurtosis: '+str(stats.kurtosis(np.log(train.SalePrice))))","cf00b1ba":"q1,q3 = train.SalePrice.describe()[['25%','75%']]\niqr = q3 - q1\nmax_right = q3 + 1.5*iqr\nmin_left = q1 - 1.5*iqr","1157b21f":"print('Lower Bound: '+str(min_left))\nprint('Upper Bound: '+str(max_right))","45868390":"outliers = train.loc[(train.SalePrice < min_left) | (train.SalePrice > max_right)] \noutliers.shape[0] \/ train.shape[0]","c9da74e2":"left_outliers = train.loc[(train.SalePrice < min_left)]\nleft_outliers.shape[0]","126438ac":"numeric_features = train[[train.columns[i] for i in range(len(train.columns)) if train.dtypes[i] in [int,float]]]\nnumeric_features.head()","debe24e6":"sns.set(style=\"white\")\n\n# Compute the correlation matrix\ncorr = numeric_features.corr()\n\n# Generate a mask for the upper triangle\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(15, 11))\n\n# Generate a custom diverging colormap\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\n\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})","15bfc4e8":"corr.loc['MiscVal'].loc['SalePrice']","008a6088":"# with hot-coding\ntrain_cleaned = pd.get_dummies(train[[c for c in train if c != 'MiscVal']],dummy_na=False,drop_first=True)\ntrain_cleaned_cols = train_cleaned.columns","042c74be":"# subset categorical dummy vars for analysis\ndummy_features = train_cleaned[[c for c in train_cleaned.columns if '_' in c or c == 'SalePrice']]\ndummy_features.head()","ffdfa66f":"# Compute the correlation matrix\ncorr = dummy_features.corr()\n\nsale_correlations = pd.DataFrame(corr.loc['SalePrice'][abs(corr.loc['SalePrice']) >.1])\nsale_correlations['R_2'] = sale_correlations ** 2\nsale_correlations.sort_values('R_2',ascending=False).head(10)","de4b1b04":"from sklearn.impute import SimpleImputer\nmy_imputer = SimpleImputer()\nnew_data = pd.DataFrame(my_imputer.fit_transform(train_cleaned))\nnew_data.columns = train_cleaned.columns","eda51a6a":"# This script comes from sklearn @ \n# https:\/\/towardsdatascience.com\/an-approach-to-choosing-the-number-of-components-in-a-principal-component-analysis-pca-3b9f3d6e73fe\nimport numpy as np\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import MinMaxScaler","9d49bf4a":"scaler = MinMaxScaler(feature_range=[0, 1])\n\n# scaling dummy variables won't change them so we don't have to worry about them.\nnew_data = scaler.fit_transform(new_data[[c for c in new_data.columns if c != 'SalePrice']])","b414a2f2":"new_data = pd.DataFrame(new_data,columns=[c for c in train_cleaned_cols if c != 'SalePrice'])\nnew_data.head()","87d15664":"#Fitting the PCA algorithm with our Data\npca = PCA().fit(new_data)\n#Plotting the Cumulative Summation of the Explained Variance\nplt.figure(figsize=(10,7))\nplt.plot(np.cumsum(pca.explained_variance_ratio_))\nplt.xlabel('Number of Components')\nplt.ylabel('Variance (%)') #for each component\nplt.title('Home Prices Dataset Explained Variance',fontsize=18)\nplt.show();","bb192fa8":"explained_var = pd.DataFrame({'n_components':list(np.arange(new_data.shape[1])),\n                              'cumsum':list(np.cumsum(pca.explained_variance_ratio_))})\nexplained_var.loc[explained_var['cumsum'] > .99].iloc[:5,:]","1765c30a":"pca = PCA(n_components=156)\ntransformed_features = pca.fit_transform(new_data)\ntransformed_features[:5,:5]","dc404d22":"transformed_data = pd.DataFrame(transformed_features)\ntransformed_data['SalePrice'] = list(np.log(train.SalePrice))\ntransformed_data.head()","130da2d6":"transformed_data.to_csv('transformed_pca.csv',index=False)","6c5b9cc0":"I don't want to remove these outliers before passing them into a model, because this is not a small number of records relative to the overall size (~4%).","f9766d22":"All of the outliers are on the right tail, which makes sense because this distribution has major right skew.","f469a1ac":"We can use this initial cleaned dataset for experimentation, although it's highly likely that this is not perfect as is. Transforming and cleaning the data is a process of trial and error.","bd5e5eff":"Since MiscVal has such a small correlation with SalePrice and it's over 99% null, this might be worth removing, unless it only matters for that <1% data.","9987d711":"Since this is much larger than 0 (kurtosis for perfectly normal distribution), we can infer that the peak is higher and narrower than for a normal distribution in this data.\n\nIt is also worth noting that, to me, it looks like there is potentially a second minor peak, a relative maximum, if you will, around the 400k mark, but I won't spend too much time on this.\n\nFortunately, the log-normalized SalePrice exhibits symmetry and very low kurtosis. Therefore, we will use the log-normalized form of SalePrice in our predictions.","affea615":"## Analyzing the Data: Test for Skew and Kurtosis of Home Prices\n\nThe output variable looks like it has a right skewed distribution; let's test this.\n\nFor reference:\n\n* If skewness is less than -1 or greater than 1, the distribution is highly skewed.\n* If skewness is between -1 and -0.5 or between 0.5 and 1, the distribution is moderately skewed.\n* If skewness is between -0.5 and 0.5, the distribution is approximately symmetric.","a8786ca3":"## Analyzing the Data: Outlier Analysis","9597e617":"## Looking at the Response Variable","cd8e6bdd":"It looks like we have a lot of missing values, but this could also be another category\/label rather than a missing item; for example the \"Alley\" feature could just mean it has no alley when the value is None.\n\nLet's have a look at the features with null values.","9e5c89ac":"## Dimensionality Reduction Using Factor Analysis\/PCA\n\nIn many machine learning models, it is common to use Principle Components Analysis (PCA) to reduce the data dimensionality and eliminate collinearities. It might be beneficial to compare outcome of PCA and EFA (exploratory factor analysis).\n\nBefore we can do this, we have to account for null values. To optimally do this, we should research why each of these null values exist first, but in the meantime, we will perform a simple imputation by just filling in null values with the mean value of that variable. Note that this is only on numeric variables, as the dummy variables account for null values for the categorical variables.\n\nSecondly, we will scale all of our numeric features to the [0, 1] range as a normalization step.","50fc815a":"Almost all of the records have a null value for PoolQC. This makes sense, as the vast majority of properties do not have a swimming pool.I am somewhat surprised by the alley and the fence, however. This could indicate some bias in how this data was collected.","eec9c961":"We can see that a majority of the numeric features are positively correlated with SalePrice, but we also see that there are many collinearities. We should work to remove some of these. There also appear to be some features that are not very correlated with SalePrice at all, including MiscVal and YrSold. ","4de32c5c":"## Checking Correlation with Numeric Variables","a67d01c2":"## Transforming and Analyzing Categorical Variables","88b54c32":"# Investigate the Data","68e8c3a6":"We can see in the above visualization that over 99% of the variance is explained by 156 components, meaning we can fit our data to just 156 components and still be confident that almost all of the variance is accounted for. We have managed to reduce the number of features by about 40% using this method."}}