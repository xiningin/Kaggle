{"cell_type":{"ed0a9220":"code","6abebfb7":"code","8c983ac4":"code","7ffddbcd":"code","6789ab03":"code","ece7e13b":"code","72d67399":"code","4c458562":"code","944cd351":"code","4ff714d1":"code","34511dd9":"code","b454cc02":"code","6055d17e":"code","0050ee4e":"code","a1e1f6ae":"code","e37e0c72":"code","19defdac":"code","bf934e1e":"code","da4ba2ac":"code","a61eecca":"code","78a715ca":"code","436cd035":"markdown","e1e1fb62":"markdown","f773a917":"markdown","ac2dfb03":"markdown","75e12569":"markdown","e705ecb2":"markdown","845475e3":"markdown","3f0bc9b0":"markdown","cbfd9396":"markdown"},"source":{"ed0a9220":"import pandas as pd\nimport numpy as np\n# import pingouin as pg , kaggle does not support pingouin\nfrom sklearn.preprocessing import PowerTransformer\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.feature_selection import SelectKBest, f_classif\nfrom imblearn.combine import SMOTETomek\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, f1_score, roc_auc_score","6abebfb7":"spam = pd.read_csv('..\/input\/spambase\/spambase_csv.csv')","8c983ac4":"spam.head()","7ffddbcd":"x = spam[spam.drop('class', axis = 1).columns]\ny = spam['class']\nxtrain, xtest, ytrain, ytest = train_test_split(x,y, train_size = 0.7, random_state = 42)","6789ab03":"def skewness_check(data, skew_cols = False, non_skew = False):\n    skew_feats = data.skew().sort_values(ascending = False)\n    skewness = pd.DataFrame(skew_feats, columns = ['Skew'])\n    skew_dict = {'High':0, 'Moderate':0, 'None':0}\n    \n    if skew_cols == True:\n        df = skewness[((skewness['Skew'] <= -1) | (skewness['Skew'] >= 1)) | ((skewness['Skew'] > -1) & (skewness['Skew'] <= -0.5)) | ((skewness['Skew'] >= 0.5) & (skewness['Skew'] < 1))]\n        return df\n    \n    elif skew_cols == False and non_skew == False:\n        for row in skewness['Skew']:\n            if row <= -1 or row >= 1:\n                skew_dict['High'] += 1\n            elif (row > -1 and row <= -0.5) or (row >= 0.5 and row < 1):\n                skew_dict['Moderate'] += 1\n            else:\n                skew_dict[\"None\"] += 1\n        return pd.DataFrame.from_dict(skew_dict, orient = 'index', columns = ['Skew'])\n    \n    elif non_skew == True:\n        df_non_skew = skewness[((skewness['Skew']>= 0) & (skewness['Skew']< 0.5)) |((skewness['Skew']> -0.5) & (skewness['Skew']<= 0))]\n        return df_non_skew\n   ","ece7e13b":"skewness_check(xtrain)","72d67399":"def correlation(data, threshold = 0.75):\n    col_corr = set()\n    corr_matrix = data.corr(method = 'spearman')\n    for i in range(len(corr_matrix.columns)):\n        for j in range(i):\n            if abs(corr_matrix.iloc[i,j]) >= threshold:\n                colname = corr_matrix.columns[i]\n                col_corr.add(colname)\n    return list(col_corr)\n    ","4c458562":"high_corr_columns = correlation(xtrain)\nhigh_corr_columns","944cd351":"xtrain.drop(high_corr_columns, axis = 1, inplace = True)\nxtest.drop(high_corr_columns, axis = 1, inplace = True)","4ff714d1":"# Run on your own Jupyter notebook with pingouin installed.\n# pg.homoscedasticity(xtrain, method=\"levene\", alpha=.05)","34511dd9":"def feature_selection(x,y):\n    skb = SelectKBest(score_func = f_classif, k = 'all')\n    skb.fit_transform(x,y)\n    col_names = x.columns.values[skb.get_support()]\n    scores = skb.scores_[skb.get_support()]\n    col_scores = list(zip(col_names, scores))\n    df = pd.DataFrame(col_scores, columns = ['Feature','Score'])\n    mean_score = df['Score'].mean()\n    max_score = df['Score'].max()\n    filtered_df = df[(df['Score'] >= mean_score) & (df['Score'] <= max_score)]\n\n    return filtered_df.sort_values('Score', ascending = False)","b454cc02":"df = feature_selection(xtrain, ytrain)\ndf","6055d17e":"FS_xtrain = xtrain[list(df['Feature'])]\nFS_xtest = xtest[list(df['Feature'])]","0050ee4e":"# Transforming the highly skewed features to reduce skewness to approximate normally distributed data.\ndef data_transform_PT(data_train, data_test):\n    \n    pt = PowerTransformer(method = 'yeo-johnson',\n                         standardize = False) # Using yeo-johnson because data contains values of zero.\n    data_train_transformed = pd.DataFrame(pt.fit_transform(data_train),\n                                         columns = data_train.columns)\n    data_test_transformed = pd.DataFrame(pt.transform(data_test),\n                                        columns = data_test.columns)\n    return data_train_transformed, data_test_transformed","a1e1f6ae":"xtrain_PT, xtest_PT = data_transform_PT(FS_xtrain,FS_xtest)","e37e0c72":"skewness_check(xtrain_PT)","19defdac":"def skew_comparison(x_1, x_2):\n    skew_feats = x_1.skew().sort_values(ascending = False)\n    skewness = pd.DataFrame(skew_feats, columns = ['Skew Before'])\n    \n    skew_feats_2 = x_2.skew().sort_values(ascending = False)\n    skewness_2 = pd.DataFrame(skew_feats_2, columns = ['Skew After'])\n    \n    df = skewness.merge(skewness_2, right_index = True, left_index = True)\n    df['Skew Reduction'] = -abs(df['Skew Before'] - df['Skew After'])\n    \n    return df","bf934e1e":"skew_comparison(xtrain, xtrain_PT)","da4ba2ac":"print(ytrain.value_counts())\nprint(ytest.value_counts())","a61eecca":"smt = SMOTETomek(random_state = 42)\nxtrain_res, ytrain_res = smt.fit_resample(xtrain_PT, ytrain)\nxtest_res, ytest_res = smt.fit_resample(xtest_PT, ytest)\n\nprint(ytrain_res.value_counts())\nprint(ytest_res.value_counts())","78a715ca":"gnb = GaussianNB()\ngnb.fit(xtrain_res,ytrain_res)\npredictions = gnb.predict(xtest_res)\naccuracy = accuracy_score(ytest_res,predictions)\nf1 = f1_score(ytest_res,predictions)\nauc = roc_auc_score(ytest_res,predictions)\n\nprint('accuracy: ', accuracy)\nprint('f1: ', f1)\nprint('AUC: ', auc)","436cd035":"All features are highly skewed. We will be dropping features based on high correlation first to prevent redundancy. This will also satisfy the assumption of independence for Gaussian Naive Bayes.","e1e1fb62":"- H0: Variances are equal.\n- H1: Variances are not equal.\n\nP-value is greater than 0.05, which means the test failed to reject the null hypothesis. There is homogeneity of variance among all features. We will be able to use the ANOVA F-test for feature selection.","f773a917":"- If skewness is less than -1 or greater than 1, the distribution is highly skewed.\n- If skewness is between -1 and -0.5 or between 0.5 and 1, the distribution is moderately skewed.\n- If skewness is between -0.5 and 0.5, the distribution is approximately symmetric\n\n","ac2dfb03":"Although there is still a significant amount of highly skewed features, skewness as been highly reduced. To check this, a function called `skew_comparison` will be created to compared the change in skewness.","75e12569":"## Data Transformation","e705ecb2":"# Spam Classifer With Gaussian Naive Bayes","845475e3":"## Correcting Imbalance","3f0bc9b0":"## Feature Selection","cbfd9396":"## Machine Learning with GaussianNB"}}