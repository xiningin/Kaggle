{"cell_type":{"94db58c2":"code","8a7320e4":"code","fa233237":"code","88c58f6e":"code","9e1c1a05":"code","082f935c":"code","734654b1":"code","cb6563b9":"code","16ef3980":"code","ad0ac65b":"code","0bf93a77":"code","b9a16853":"code","3a4c174b":"code","2ef17d03":"code","6a4406e7":"code","91dc3423":"code","6826979a":"code","d082f62e":"code","64801b1f":"code","02c6fae1":"code","c80d8bce":"code","77972e17":"code","19311f36":"code","6249fbef":"code","70b742a8":"code","b7617dfc":"code","82671ba1":"code","df7a76d6":"code","fc3b9032":"code","15c34ad8":"code","0ea58854":"code","ce0a7e69":"code","25581521":"code","f1bea917":"code","8935c48b":"code","d7da7228":"code","2a305b58":"code","c30233cf":"code","f0708961":"code","0a246cfb":"code","ea59e76d":"code","20d0f716":"code","ad7bbaaf":"code","b8b77f4c":"code","d7e66963":"code","12cf34b6":"code","f615c7b5":"code","9dd62504":"code","44cb9a5c":"code","de2444d5":"code","7fe43c82":"code","f22def32":"code","aac19dcc":"code","3bc11008":"code","bf231eda":"code","9c8a9947":"code","8bb0bffa":"code","094407c7":"code","5d69dd83":"code","66387942":"code","d642e662":"code","b8885bd3":"code","006987b1":"code","82acfa5a":"code","69ad9a77":"code","525beb03":"code","d24367de":"code","fa4716a6":"code","87e293e4":"code","63288ef9":"code","780b9c75":"code","47c98f65":"code","e78dfc69":"code","f5514194":"code","78bd5a75":"code","ed7f82e9":"code","2cda35e7":"code","3508849b":"code","2b577a7d":"code","c97d8b54":"code","2597c9ac":"code","ce9a8a0d":"code","5cedb21a":"code","ae60166a":"code","39905bc7":"code","ee2ffc63":"code","833f826b":"code","0ce209fe":"markdown","e32253c7":"markdown","df599161":"markdown"},"source":{"94db58c2":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","8a7320e4":"## Importing all the necessary libraries.\nimport pandas as pd\nimport numpy as np\npd.options.display.max_rows = 300\npd.options.display.max_columns = 300\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn import linear_model\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import Lasso\nfrom sklearn.model_selection import GridSearchCV\n\nimport os\n\n# hide warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n","fa233237":"## making two dataframes by importing the test and train data from the csv files.\npd.options.display.max_columns = 300\ntrain_df=pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntest_df=pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")","88c58f6e":"##Viewing the contents of train.csv\ntrain_df.head()","9e1c1a05":"## Viewing the number of rows and columns in train.csv\ntrain_df.shape","082f935c":"##Viewing the contents of the columns in train.csv\n\ntrain_df.describe()","734654b1":"## Tpe of data in the columns of the dataframe\ntrain_df.info()","cb6563b9":"## Viewing the type of data in test.csv\ntest_df.info()","16ef3980":"#Checking the percentage missing values in each column for both train and test dataset\nround(100*(train_df.isnull().sum()\/len(train_df.index)), 2)","ad0ac65b":"round(100*(test_df.isnull().sum()\/len(test_df.index)), 2)","0bf93a77":"## Here we can see both test and train dataset have more than %)% missing values in the cabin column \n## hence deleting the Cabin column from both test and train dataset\ntrain_df=train_df.drop('Cabin',axis=1)\ntest_df=test_df.drop('Cabin',axis=1)\n","b9a16853":"train_df.info()","3a4c174b":"##Data Preparation\n##\n#df3['GarageYrBlt']=df3['GarageYrBlt'].convert_objects(convert_numeric=True)\ndf1=train_df\ndf1_test=test_df\n## Deleting the columns which do not look relevant for our analysis from both train and test dataset\n#df1=df1.drop(['Ticket','Name','PassengerId'],axis=1)\n#df1_test=df1_test.drop(['Ticket','Name','PassengerId'],axis=1)\n","2ef17d03":"##Vewing the shape of our dataframes after deleting the columns\ndf1.shape\nprint(list(df1.columns))","6a4406e7":"df1_test.shape\nprint(list(df1_test.columns))","91dc3423":"##Creating dummy variables\nembark = pd.get_dummies(df1['Embarked'],prefix='Embarked',drop_first=True)\n#Adding the results to the new dataframe\ndf1 = pd.concat([df1,embark],axis=1)\n\npclass = pd.get_dummies(df1['Pclass'],prefix='Pclass',drop_first=True)\n#Adding the results to the new dataframe\ndf1 = pd.concat([df1,pclass],axis=1)\n\ndf1['Sex'] = df1['Sex'].map({'male': 1, 'female': 0})\n\nparch = pd.get_dummies(df1['Parch'],prefix='Parch',drop_first=True)\n#Adding the results to the new dataframe\ndf1 = pd.concat([df1,parch],axis=1)\n\nsibsp = pd.get_dummies(df1['SibSp'],prefix='SibSp',drop_first=True)\n#Adding the results to the new dataframe\ndf1 = pd.concat([df1,sibsp],axis=1)\n\n### Repeating the same for the test set\n\nembark = pd.get_dummies(df1_test['Embarked'],prefix='Embarked',drop_first=True)\n#Adding the results to the new dataframe\ndf1_test = pd.concat([df1_test,embark],axis=1)\n\npclass = pd.get_dummies(df1_test['Pclass'],prefix='Pclass',drop_first=True)\n#Adding the results to the new dataframe\ndf1_test = pd.concat([df1_test,pclass],axis=1)\n\ndf1_test['Sex'] = df1_test['Sex'].map({'male': 1, 'female': 0})\n\nparch = pd.get_dummies(df1_test['Parch'],prefix='Parch',drop_first=True)\n#Adding the results to the new dataframe\ndf1_test = pd.concat([df1_test,parch],axis=1)\n\nsibsp = pd.get_dummies(df1_test['SibSp'],prefix='SibSp',drop_first=True)\n#Adding the results to the new dataframe\ndf1_test = pd.concat([df1_test,sibsp],axis=1)\n","6826979a":"df2=df1\ndf2=df2.drop(['Embarked','Pclass','Parch','SibSp'],axis=1)\n\ndf2_test=df1_test\ndf2_test=df2_test.drop(['Embarked','Pclass','Parch','SibSp'],axis=1)","d082f62e":"df2.shape","64801b1f":"#Checking the datatype of all the columns of test and train dataset\ndf2.info()\n","02c6fae1":"#Viewing if there are any object datatype in our test dataframe\ndf2_test.info()","c80d8bce":"#Checking for the outlier in the continuous variables in test and train dataset\nnum_df= df2[['Age','Fare']] \n","77972e17":"num_df.describe(percentiles=[.25,.5,.75,.90,.95,.99])","19311f36":"num_df.boxplot('Age',figsize=(10,8))","6249fbef":"num_df.boxplot('Fare',figsize=(10,8))","70b742a8":"# Viewing outlier in Age column\ndf2[df2['Age']>75]","b7617dfc":"##Removing the outlier in Age column\ndf3=(df2[df2['Age']<75]) ","82671ba1":"# Viewing outlier in Fare column\ndf3[df3['Fare']>500]","df7a76d6":"##Removing the outlier in Fare column\ndf4=(df3[df3['Fare']<500]) ","fc3b9032":"##Checking for missing values in the daraset\nround((df4.isnull().sum()\/len(df4.index)), 2)","15c34ad8":"## Feature Standardisation\n## We will perform feature standardisation for numberic\/countinuous columns.\n# Features Scaling and checking dataframe\ndf4.info()","0ea58854":"\nfinal_df = df4[['Age','Fare']]","ce0a7e69":"normalized_df=(final_df-final_df.mean())\/final_df.std()\ndf4 = df4.drop(['Age','Fare'], 1)\ndf4 = pd.concat([df4,normalized_df],axis=1)\ndf4.head()","25581521":"## Checking survived rate\nsurvived = (sum(df4['Survived'])\/len(df4['Survived'].index))*100\nprint(survived)\n","f1bea917":"#### Model Building\ndf5=df4\nX = df5.drop(['Ticket','Name','PassengerId','Survived'],axis=1)\n#X1=df2_test.drop(['Ticket','Name','PassengerId'],axis=1)\n# Putting response variable to y\ny = df5['Survived']\n\ny.head()\n","8935c48b":"X_train=X\ny_train=y\n#X_test=X1\n#, X_test, y_train, y_test = train_test_split(X,y, train_size=0.7,test_size=0.3,random_state=100)\n#print(list(df2_test.columns))","d7da7228":"## using RFE for feature selection\nfrom sklearn.linear_model import LogisticRegression\nlogreg = LogisticRegression()\nfrom sklearn.feature_selection import RFE\nrfe = RFE(logreg, 8)             # running RFE with 8 variables as output\nrfe = rfe.fit(X,y)\nprint(rfe.support_)           # Printing the boolean results\nprint(rfe.ranking_)           # Printing the ranking","2a305b58":"# Variables selected by RFE \nlist(zip(X_train.columns, rfe.support_, rfe.ranking_))","c30233cf":"col = X_train.columns[rfe.support_]","f0708961":"## Selecting the columns as chosen after performing RFE\nX_train.columns[~rfe.support_]","0a246cfb":"## Assessing model with statsmodel\nimport statsmodels.api as sm \nX_train_sm = sm.add_constant(X_train[col])\nlogm2 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\nres = logm2.fit()\nres.summary()","ea59e76d":"## AS we can see the P-value for Parch_4 is very high hence removing Parch_6\ncol=col.drop('Parch_4',1)","20d0f716":"## Assessing model with statsmodel\nX_train_sm = sm.add_constant(X_train[col])\nlogm3 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\nres = logm3.fit()\nres.summary()","ad7bbaaf":"## AS we can see that the p-value for SibSp_5 is very high hence dropping that column\ncol=col.drop('SibSp_5',1)","b8b77f4c":"# Let's re-run the model using the selected variables\nX_train_sm = sm.add_constant(X_train[col])\nlogm4 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\nres = logm4.fit()\nres.summary()","d7e66963":"##Now thw p-value for all columns is below 5% checking the VIF values for all columns\n### LEts check VIF\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nvif = pd.DataFrame()\nvif['Features'] = X_train[col].columns\nvif['VIF'] = [variance_inflation_factor(X_train[col].values, i) for i in range(X_train[col].shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","12cf34b6":"## THe VIF value for all the columns is less than 5 which is acceptable hence all columns are significant\n## making prediction on the train dataset\ny_train_pred = res.predict(X_train_sm).values.reshape(-1)","f615c7b5":"y_train_pred[:10]","9dd62504":"### Creating a dataframe with actual survived flag  and the predicted probabilities\n\ny_train_pred_final = pd.DataFrame({'Survived':y_train.values, 'Survived_Prob':y_train_pred})\n## the index number from original dataframe is stored as customer id\ny_train_pred_final['PassengerId'] = y_train.index\ny_train_pred_final.head()","44cb9a5c":"\n##Also adding a Survived_pred column with a cutoff\n## of 0.5 or 50% cutoff  i.e. all value above 50% probabilities are 1\n\ny_train_pred_final['Survived_pred'] = y_train_pred_final.Survived_Prob.map(lambda x: 1 if x > 0.5 else 0)\n\n# Let's see the head\ny_train_pred_final.head()","de2444d5":"from sklearn import metrics\n# Confusion matrix \nconfusion = metrics.confusion_matrix(y_train_pred_final.Survived, y_train_pred_final.Survived_pred )\nprint(confusion)","7fe43c82":"# Let's check the overall accuracy.\nprint(metrics.accuracy_score(y_train_pred_final.Survived, y_train_pred_final.Survived_pred))","f22def32":"## Plotting ROC curve to get a more clear threshold value\ndef draw_roc( actual, probs ):\n    fpr, tpr, thresholds = metrics.roc_curve( actual, probs,\n                                              drop_intermediate = False )\n    auc_score = metrics.roc_auc_score( actual, probs )\n    plt.figure(figsize=(5, 5))\n    plt.plot( fpr, tpr, label='ROC curve (area = %0.2f)' % auc_score )\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate or [1 - True Negative Rate]')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver operating characteristic example')\n    plt.legend(loc=\"lower right\")\n    plt.show()\n\n    return None","aac19dcc":"fpr, tpr, thresholds = metrics.roc_curve( y_train_pred_final.Survived, y_train_pred_final.Survived_Prob, drop_intermediate = False )","3bc11008":"draw_roc(y_train_pred_final.Survived, y_train_pred_final.Survived_Prob)","bf231eda":"# finding optimal cutoff\n### IT is that probability where we get balanced sensitivity and specificity\n","9c8a9947":"# Let's create columns with different probability cutoffs \nnumbers = [float(x)\/10 for x in range(10)]\nfor i in numbers:\n    y_train_pred_final[i]= y_train_pred_final.Survived_Prob.map(lambda x: 1 if x > i else 0)\ny_train_pred_final.head()","8bb0bffa":"# Now let's calculate accuracy sensitivity and specificity for various probability cutoffs.\ncutoff_df = pd.DataFrame( columns = ['prob','accuracy','sensi','speci'])\nfrom sklearn.metrics import confusion_matrix\n\n# TP = confusion[1,1] # true positive \n# TN = confusion[0,0] # true negatives\n# FP = confusion[0,1] # false positives\n# FN = confusion[1,0] # false negatives\n\nnum = [0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\nfor i in num:\n    cm1 = metrics.confusion_matrix(y_train_pred_final.Survived, y_train_pred_final[i] )\n    total1=sum(sum(cm1))\n    accuracy = (cm1[0,0]+cm1[1,1])\/total1\n    \n    speci = cm1[0,0]\/(cm1[0,0]+cm1[0,1])\n    sensi = cm1[1,1]\/(cm1[1,0]+cm1[1,1])\n    cutoff_df.loc[i] =[ i ,accuracy,sensi,speci]\nprint(cutoff_df)","094407c7":"# Let's plot accuracy sensitivity and specificity for various probabilities.\ncutoff_df.plot.line(x='prob', y=['accuracy','sensi','speci'])\nplt.show()","5d69dd83":"### from above plot and the above table 0.4 is the cutoff as the three values i.e accuracy,sensitivity and specificity\n### are nearly the same or coincide","66387942":"y_train_pred_final['final_survived'] = y_train_pred_final.Survived_Prob.map( lambda x: 1 if x > 0.4 else 0)\n\ny_train_pred_final.head()","d642e662":"# Let's check the overall accuracy.\nmetrics.accuracy_score(y_train_pred_final.Survived, y_train_pred_final.final_survived)","b8885bd3":"confusion2 = metrics.confusion_matrix(y_train_pred_final.Survived, y_train_pred_final.final_survived )\nconfusion2","006987b1":"TP = confusion2[1,1] # true positive \nTN = confusion2[0,0] # true negatives\nFP = confusion2[0,1] # false positives\nFN = confusion2[1,0] # false negatives","82acfa5a":"# Let's see the sensitivity of our logistic regression model\nTP \/ float(TP+FN)","69ad9a77":"# Let us calculate specificity\nTN \/ float(TN+FP)","525beb03":"# Calculate false postive rate - predicting converted when customer have not converted\nprint(FP\/ float(TN+FP))","d24367de":"# Positive predictive value \nprint (TP \/ float(TP+FP))","fa4716a6":"# Negative predictive value\nprint (TN \/ float(TN+ FN))","87e293e4":"### Making predictions on Test-set\nX_test=df2_test\nX_test = X_test[col]\nX_test.head()","63288ef9":"## Feature standardisation on test set for age column_\n\nfinal_df_test = X_test[['Age']]\nnormalized_df=(final_df_test-final_df_test.mean())\/final_df_test.std()\nX_test = X_test.drop(['Age'], 1)\nX_test = pd.concat([X_test,normalized_df],axis=1)\nX_test.head()\n\n","780b9c75":"X_test_sm = sm.add_constant(X_test)","47c98f65":"y_test_pred = res.predict(X_test_sm)","e78dfc69":"y_test_pred[:10]","f5514194":"# Converting y_pred to a dataframe which is an array\ny_pred_1 = pd.DataFrame(y_test_pred)","78bd5a75":"# Let's see the head\ny_pred_1.head()","ed7f82e9":"## Having the final test-dataframe with passenger id and survived column\n#y_test_df = pd.DataFrame(y_test)\ny_pred_1['PassengerId'] =df2_test['PassengerId']","2cda35e7":"y_pred_1.head()","3508849b":"y_pred_1= y_pred_1.rename(columns={ 0 : 'Survived_Prob'})\ny_pred_1['Survived'] = y_pred_1.Survived_Prob.map(lambda x: 1 if x > 0.4 else 0)","2b577a7d":"y_pred_1.head()","c97d8b54":"y_pred_Final=y_pred_1","2597c9ac":"## getting the final dataframe with ust two column survived and passenger id\ny_pred_Final=y_pred_Final.drop(\"Survived_Prob\",axis=1)","ce9a8a0d":"## Viewing the contents of the final dataframe\ny_pred_Final.head()","5cedb21a":"## Viewing the shape of the final dataframe\ny_pred_Final.shape","ae60166a":"## Checking survived rate in the test dataframe\nsurvived = (sum(y_pred_Final['Survived'])\/len(y_pred_Final['Survived'].index))*100\nprint(survived)","39905bc7":"## Sorting the dataframe with respect to passengerid in ascending order\ny_pred_Final=y_pred_Final.sort_values('PassengerId',ascending=True)","ee2ffc63":"y_pred_Final.head()","833f826b":"y_pred_Final.to_csv('submission_15.csv', index=False)","0ce209fe":"## Hence as per requirement we have 416 rows and two columns namely PassengerID and Survived","e32253c7":"As we can see from above there are no missing values in the dataframe","df599161":"##### Hence we have a survived rate of 40%"}}