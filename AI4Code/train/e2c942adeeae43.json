{"cell_type":{"5fa1f432":"code","c5aea1eb":"code","43d1b496":"code","205f6688":"code","7b34b2a8":"code","8366cb86":"code","69f0df76":"code","7f2b6b87":"code","0765a396":"code","a9223007":"code","a846981d":"code","7d6fa49f":"code","3c1a6e2b":"code","9af935e1":"code","45d3366f":"code","4846e95f":"code","867e1465":"code","24e789ce":"code","24822025":"code","e515d5ba":"code","5becbb03":"code","84a6831d":"code","18c49651":"code","ac70edd3":"code","4ca52da9":"code","0fc4bd8d":"code","04b4ed05":"code","83ba19a2":"code","8739ca03":"code","a89f8780":"code","37a0a9b6":"code","da07d008":"code","08679add":"code","351814ee":"code","0268f390":"code","417626a4":"code","fb67c964":"markdown","99ef9cb5":"markdown","0e610b07":"markdown","62e5013a":"markdown","f42170bc":"markdown","31b75d25":"markdown","d927768d":"markdown","740de030":"markdown","251a664a":"markdown","4321af20":"markdown","ed8e975f":"markdown","149303d4":"markdown","2c5d3e85":"markdown","78415a21":"markdown","236fb347":"markdown","e80ac816":"markdown","aa586553":"markdown","f1c4881c":"markdown","bbcb6052":"markdown","4c143352":"markdown","a645a4cc":"markdown","4b5d08f9":"markdown","377285a9":"markdown","6bd76bd3":"markdown","45d578fb":"markdown","d82d37a3":"markdown","9db1cdcc":"markdown","ff422b87":"markdown","351ab1c8":"markdown","7d0944d9":"markdown","f7511b55":"markdown","61c02c27":"markdown","954301a8":"markdown","8bbac5b0":"markdown","310075e1":"markdown","ffef86d8":"markdown","9947999c":"markdown","2da611be":"markdown","fcceae2d":"markdown"},"source":{"5fa1f432":"# imports\nimport pandas as pd \nimport numpy as np\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nimport seaborn as sns\nimport sys\nimport warnings\n\nif not sys.warnoptions:\n    warnings.simplefilter(\"ignore\")","c5aea1eb":"data_path = '..\/input\/customer-segmentation-tutorial-in-python\/Mall_Customers.csv'\ndf = pd.read_csv(data_path)","43d1b496":"df.head()","205f6688":"df.drop('CustomerID', axis=1, inplace = True)\ndf.head()","7b34b2a8":"df.shape","8366cb86":"df.info()","69f0df76":"df.isnull().sum()","7f2b6b87":"df.describe()","0765a396":"cor = df.corr()\nsns.set(font_scale=1.4)\nplt.figure(figsize=(9,8))\nsns.heatmap(cor, annot=True, cmap='plasma')\nplt.tight_layout()\nplt.show()","a9223007":"# -Distribution Plots-\n\nplt.figure(figsize=(16,12),facecolor='#9DF08E')\n\n# Spending Score\nplt.subplot(3,3,1)\nplt.title('Spending Score\\n', color='#FF000B')\nsns.distplot(df['Spending Score (1-100)'], color='orange')\n\n# Age\nplt.subplot(3,3,2)\nplt.title('Age\\n', color='#FF000B')\nsns.distplot(df['Age'], color='#577AFF')\n\n# Annual Income \nplt.subplot(3,3,3)\nplt.title('Annual Income\\n', color='#FF000B')\nsns.distplot(df['Annual Income (k$)'], color='black')\n\nplt.suptitle(' Distribution Plots\\n', color='#0000C1', size = 30)\nplt.tight_layout()","a846981d":"# Before-After Label Encoder\n\nfrom sklearn.preprocessing import LabelEncoder\n\nprint('\\033[0;32m' + 'Before Label Encoder\\n' + '\\033[0m' + '\\033[0;32m', df['Gender'])\n\nle = LabelEncoder()\ndf['Gender'] = le.fit_transform(df.iloc[:,0])\n\nprint('\\033[0;31m' + '\\n\\nAfter Label Encoder\\n' + '\\033[0m' + '\\033[0;31m', df['Gender'])","7d6fa49f":"# Let's look at the current state of our Data Frame.\ndf.head()","3c1a6e2b":"# Let's calculate how much to shop for which gender\n\nspending_score_male = 0\nspending_score_female = 0\n\nfor i in range(len(df)):\n    if df['Gender'][i] == 1:\n        spending_score_male = spending_score_male + df['Spending Score (1-100)'][i]\n    if df['Gender'][i] == 0:\n        spending_score_female = spending_score_female + df['Spending Score (1-100)'][i]\n\n\nprint('\\033[1m' + '\\033[93m' + f'Males Spending Score  : {spending_score_male}')\nprint('\\033[1m' + '\\033[93m' + f'Females Spending Score: {spending_score_female}')","9af935e1":"# Let's try to understand the relationship between gender and spending score.\n \n# Number of genders\n    \nplt.figure(figsize=(16,16),facecolor='#54C6C0')\nplt.subplot(3,3,1)\nplots = sns.barplot(x=['Female','Male'], y=df['Gender'].value_counts(), data=df)  \n\nfor bar in plots.patches:\n    plots.annotate(format(bar.get_height(), '.0f'), \n                   (bar.get_x() + bar.get_width() \/ 2, \n                    bar.get_height()), ha='center', va='center',\n                   size=13, xytext=(0, 8),\n                   textcoords='offset points',color='red')\n    \nplt.xlabel(\"Gender\", size=14)\nplt.ylabel(\"Number\", size=14)\nplt.yticks(np.arange(0,116,10),size='14')\nplt.grid(False)\nplt.title(\"Number of Genders\\n\", color=\"red\", size='22')\n\n\n\n# Gender & Total Spending Score\n\nlist_genders_spending_score = [int(spending_score_female),int(spending_score_male)]\nseries_genders_spending_score = pd.Series(data = list_genders_spending_score)\n\n\nplt.subplot(3,3,2)\nplots = sns.barplot(x=['Female','Male'], y=series_genders_spending_score, palette=['yellow','purple'])  \n\nfor bar in plots.patches:\n    plots.annotate(format(bar.get_height(), '.0f'), \n                   (bar.get_x() + bar.get_width() \/ 2, \n                    bar.get_height()), ha='center', va='center',\n                   size=13, xytext=(0, 8),\n                   textcoords='offset points',color='red')\n    \nplt.xlabel(\"Gender\", size=14)\nplt.ylabel(\"Total Spending Score\", size=14)\nplt.yticks(np.arange(0,6001,1000),size='14')\nplt.grid(False)\nplt.title(\"Gender & Total Spending Score\\n\", color=\"red\", size='22')\n\n\n\n# Gender & Mean Spending Score \n\nlist_genders_spending_score_mean = [int(spending_score_female\/df['Gender'].value_counts()[0]),int(spending_score_male\/df['Gender'].value_counts()[1])]\nseries_genders_spending_score_mean = pd.Series(data = list_genders_spending_score_mean)\n\nplt.subplot(3,3,3)\nplots = sns.barplot(x=['Female','Male'], y=series_genders_spending_score_mean, palette='hsv')  \n\nfor bar in plots.patches:\n    plots.annotate(format(bar.get_height(), '.0f'), \n                   (bar.get_x() + bar.get_width() \/ 2, \n                    bar.get_height()), ha='center', va='center',\n                   size=13, xytext=(0, 8),\n                   textcoords='offset points',color='red')\n    \nplt.xlabel(\"Gender\", size=14)\nplt.ylabel(\"Mean Spending Score\", size=14)\nplt.yticks(np.arange(0,71,10),size='14')\nplt.grid(False)\nplt.title(\"Gender & Mean Spending Score\\n\", color=\"red\", size='22')\nplt.tight_layout()\nplt.show()","45d3366f":"# Let's look at the relationship between Age and Spending score\n\nplt.figure(figsize=(12,8))\nsns.scatterplot(x = df['Age'], y = df['Spending Score (1-100)'])\nplt.title('Age - Spending Score', size = 23, color='red')","4846e95f":"# Let's look at the relationship between Annual Income and Spending Score\n\nplt.figure(figsize=(12,8))\nsns.scatterplot(x = df['Annual Income (k$)'], y = df['Spending Score (1-100)'], palette = \"red\")\nplt.title('Annual Income - Spending Score', size = 23, color='red')","867e1465":"# x assignment\nx = df.iloc[:,0:].values \nprint(\"\\033[1;31m\"  + f'X data before PCA:\\n {x[0:5]}')\n\n\n# standardization before PCA\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX = sc.fit_transform(x)\n\n\n# PCA\nfrom sklearn.decomposition import PCA\npca = PCA(n_components = 2) \nX_2D = pca.fit_transform(X)\nprint(\"\\033[0;32m\" + f'\\nX data after PCA:\\n {X_2D[0:5,:]}')","24e789ce":"# finding optimum number of clusters\nfrom sklearn.cluster import KMeans\nwcss_list = []\n\nfor i in range(1,11):\n    kmeans_test = KMeans(n_clusters = i, init ='k-means++', random_state=88)\n    kmeans_test.fit(X_2D)\n    wcss_list.append(kmeans_test.inertia_)\n\nplt.figure(figsize=(9,6))\nplt.plot(range(1, 11), wcss_list)\nplt.title('The Elbow Method', color='red',fontsize='23')\nplt.xlabel('Number of clusters')\nplt.xticks(np.arange(1,11))\nplt.ylabel('WCSS')\nplt.show()","24822025":"# KMeans\nkmeans = KMeans(n_clusters = 4, init ='k-means++', random_state=88)\ny_kmeans = kmeans.fit_predict(X_2D)","e515d5ba":"# clusters visualization\nplt.figure(1 , figsize = (16 ,9))\nplt.scatter(X_2D[y_kmeans == 0, 0], X_2D[y_kmeans == 0, 1], s = 80, c = 'orange', label = 'Cluster-1')\nplt.scatter(X_2D[y_kmeans == 1, 0], X_2D[y_kmeans == 1, 1], s = 80, c = 'red', label = 'Cluster-2')\nplt.scatter(X_2D[y_kmeans == 2, 0], X_2D[y_kmeans == 2, 1], s = 80, c = 'green', label = 'Cluster-3')\nplt.scatter(X_2D[y_kmeans == 3, 0], X_2D[y_kmeans == 3, 1], s = 80, c = 'purple', label = 'Cluster-4')\nplt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s = 375, c = 'brown', label = 'Centroids')\nplt.title(\"Customers' Clusters\")\nplt.xlabel('PCA Variable-1', color='red')\nplt.ylabel('PCA Variable-2', color='red')\nplt.legend()\nplt.show()","5becbb03":"# x assignment\nx = df[['Age','Annual Income (k$)','Spending Score (1-100)']].values\nx_df = df[['Age','Annual Income (k$)','Spending Score (1-100)']] # this line for 3d scatter plot","84a6831d":"# finding optimum number of clusters\nwcss_list = []\n\nfor i in range(1,11):\n    kmeans_test = KMeans(n_clusters = i, init ='k-means++', random_state=88)\n    kmeans_test.fit(x)\n    wcss_list.append(kmeans_test.inertia_)\n\nplt.figure(figsize=(9,6))\nplt.plot(range(1, 11), wcss_list)\nplt.title('The Elbow Method', color='red',fontsize='23')\nplt.xlabel('Number of clusters')\nplt.xticks(np.arange(1,11))\nplt.ylabel('WCSS')\nplt.show()","18c49651":"# KMeans\nkmeans = KMeans(n_clusters = 6, init ='k-means++', random_state=88)\nclusters = kmeans.fit_predict(x_df)\nx_df['label'] = clusters","ac70edd3":"# # clusters visualization\nfig = px.scatter_3d(data_frame=x_df, x='Age', y='Annual Income (k$)', z='Spending Score (1-100)',color = 'label', size = 'label')\nfig.show()","4ca52da9":"# x assignment\nx = df[['Age','Annual Income (k$)']].values","0fc4bd8d":"# finding optimum number of clusters\nwcss_list = []\n\nfor i in range(1,11):\n    kmeans_test = KMeans(n_clusters = i, init ='k-means++', random_state=88)\n    kmeans_test.fit(x)\n    wcss_list.append(kmeans_test.inertia_)\n\nplt.figure(figsize=(9,6))\nplt.plot(range(1, 11), wcss_list)\nplt.title('The Elbow Method', color='red',fontsize='23')\nplt.xlabel('Number of clusters')\nplt.xticks(np.arange(1,11))\nplt.ylabel('WCSS')\nplt.show()","04b4ed05":"# KMeans\nkmeans = KMeans(n_clusters = 2, init ='k-means++', random_state=88)\ny_kmeans = kmeans.fit_predict(x)","83ba19a2":"# clusters visualization\nplt.figure(1 , figsize = (16 ,9))\nplt.scatter(x[y_kmeans == 0, 0], x[y_kmeans == 0, 1], s = 80, c = '#13DB8C', label = 'Cluster-1')\nplt.scatter(x[y_kmeans == 1, 0], x[y_kmeans == 1, 1], s = 80, c = '#72BAFF', label = 'Cluster-2')\nplt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s = 350, c = 'brown', label = 'Centroids')\nplt.title(\"Customers' Clusters\")\nplt.xlabel('Age', color='red')\nplt.ylabel('Annual Income (k$)', color='red')\nplt.legend()\nplt.show()","8739ca03":"# x assignment\nx = df[['Annual Income (k$)','Spending Score (1-100)']].values","a89f8780":"# finding optimum number of clusters\nwcss_list = []\n\nfor i in range(1,11):\n    kmeans_test = KMeans(n_clusters = i, init ='k-means++', random_state=88)\n    kmeans_test.fit(x)\n    wcss_list.append(kmeans_test.inertia_)\n\nplt.figure(figsize=(9,6))\nplt.plot(range(1, 11), wcss_list)\nplt.title('The Elbow Method', color='red',fontsize='23')\nplt.xlabel('Number of clusters')\nplt.xticks(np.arange(1,11))\nplt.ylabel('WCSS')\nplt.show()","37a0a9b6":"kmeans = KMeans(n_clusters = 5, init ='k-means++', random_state=88)\ny_kmeans = kmeans.fit_predict(x)","da07d008":"# clusters visualization\nplt.figure(1 , figsize = (16 ,9))\nplt.scatter(x[y_kmeans == 0, 0], x[y_kmeans == 0, 1], s = 80, c = 'orange', label = 'Cluster-1')\nplt.scatter(x[y_kmeans == 1, 0], x[y_kmeans == 1, 1], s = 80, c = 'red', label = 'Cluster-2')\nplt.scatter(x[y_kmeans == 2, 0], x[y_kmeans == 2, 1], s = 80, c = 'purple', label = 'Cluster-3')\nplt.scatter(x[y_kmeans == 3, 0], x[y_kmeans == 3, 1], s = 80, c = 'lime', label = 'Cluster-4')\nplt.scatter(x[y_kmeans == 4, 0], x[y_kmeans == 4, 1], s = 80, c = 'blue', label = 'Cluster-5')\nplt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s = 375, c = 'brown', label = 'Centroids')\nplt.title(\"Customers' Clusters\")\nplt.xlabel('Annual Income (k$)', color='red')\nplt.ylabel('Spending Score', color='red')\nplt.legend()\nplt.show()","08679add":"# x assignment\nx = df[['Age','Spending Score (1-100)']].values","351814ee":"# finding optimum number of clusters\nwcss_list = []\n\nfor i in range(1,11):\n    kmeans_test = KMeans(n_clusters = i, init ='k-means++', random_state=88)\n    kmeans_test.fit(x)\n    wcss_list.append(kmeans_test.inertia_)\n\nplt.figure(figsize=(9,6))\nplt.plot(range(1, 11), wcss_list)\nplt.title('The Elbow Method', color='red',fontsize='23')\nplt.xlabel('Number of clusters')\nplt.xticks(np.arange(1,11))\nplt.ylabel('WCSS')\nplt.show()","0268f390":"# KMeans\nkmeans = KMeans(n_clusters = 4, init ='k-means++', random_state=88)\ny_kmeans = kmeans.fit_predict(x)","417626a4":"# clusters visualization\nplt.figure(1 , figsize = (16 ,9))\nplt.scatter(x[y_kmeans == 0, 0], x[y_kmeans == 0, 1], s = 80, c = '#00FF00', label = 'Cluster-1')\nplt.scatter(x[y_kmeans == 1, 0], x[y_kmeans == 1, 1], s = 80, c = '#00FFFF', label = 'Cluster-2')\nplt.scatter(x[y_kmeans == 2, 0], x[y_kmeans == 2, 1], s = 80, c = '#FF00FF', label = 'Cluster-3')\nplt.scatter(x[y_kmeans == 3, 0], x[y_kmeans == 3, 1], s = 80, c = '#FF4500', label = 'Cluster-4')\nplt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s = 375, c = 'brown', label = 'Centroids')\nplt.title(\"Customers' Clusters\")\nplt.xlabel('Age', color='red')\nplt.ylabel('Spending Score', color='red')\nplt.legend()\nplt.show()","fb67c964":"<a id = \"9\"><\/a><br> <p style = \"font-size : 30px; color :#0009B7 ; font-family : 'Comic Sans MS'; text-align : center; background-color : #F7CF2D; border-radius: 5px 5px;\">Clustering (Age & Spending Score)<\/p> ","99ef9cb5":"**'5' is optimum number of clusters.**","0e610b07":"![Customer_Segmentation_SonerKar_First.png](attachment:aee91c6b-c912-4f37-b5b8-6036f97864f2.png)","62e5013a":"**Let's look at the correlations.**","f42170bc":"**Also in this way we can see that there is no null data.**","31b75d25":"<a id = \"4\"><\/a><br> <p style = \"font-size : 30px; color :#0009B7 ; font-family : 'Comic Sans MS'; text-align : center; background-color : #F7CF2D; border-radius: 5px 5px;\">Clustering (4 Variables)<\/p> ","d927768d":"**'4' is optimum number of clusters. Because the most break in the chart is at that point. This is how we will select the next optimal n_clusters.**","740de030":"<a id = \"10\"><\/a><br> <p style = \"font-size : 30px; color :#0009B7 ; font-family : 'Comic Sans MS'; text-align : center; background-color : #F7CF2D; border-radius: 5px 5px;\">Conclusion<\/p> ","251a664a":"**The distributions are generally similar to the normal distribution, with only a few standard deviations. The 'more normal' distribution among the distributions is the 'Spending Score'. That's good because it's our target column.**","4321af20":"**I see an ID column here, I'll drop it right away because the ID is just a unique number that identifies each row, not a feature therefore ID column is meaningless in this analysis.**","ed8e975f":"**Let's see how data looks like.**","149303d4":"* It seems very clear that there is no big difference between male and female customers, so a gender-based audience should not be chosen.\n* In addition, it seems that the audience between the ages of 20-40 spend more in this store compared to people in other age groups, making special campaigns for the audience between the ages of 20-40 can increase the profit of the supermarket.\n*  This is not the optimal strategy, but it could be an alternative. Since the average spending scores of middle-income (40k-70k dollars) customers in this store are also at a medium level, it is difficult to increase their spending to higher levels because their income is not conducive to this, but by making campaigns to increase the number of these customers, the store can increase its profit by acquiring more middle-income customers.\n* I think the best strategy would be to target high-income customers. The reason is that some of the high-income customers spend high, while a significant portion of these customers spend low, there may be some things that low-spenders are not satisfied. Improvements to be made in service and quality can increase the spending of high-income customers who come to the store, but do not.\n* The distribution of the data was generally good, but the standard deviations were a little high\n* There was no significant positive correlation between the data, only a negative correlation between age and spending score that could be important, showing us that older people who choose this supermarket spend less money than people in other age groups.\n","2c5d3e85":"**Our dataset consists of 200 rows and 4 columns.**","78415a21":"<font color = '#E22812'>\nContent:\n\n1. [Tutorial](#0)\n1. [Dataset Information](#1)\n1. [Exploratory Data Analysis (EDA)](#2)  \n1. [Clusterings](#3)      \n    *           [Clustering (4 Variables)](#4)\n        *          [PCA](#5)\n    *           [Clustering (Age & Annual Income & Spending Score)](#6)\n    *           [Clustering (Age & Annual Income)](#7)\n    *           [Clustering (Annual Income & Spending Score)](#8)\n    *           [Clustering (Age & Spending Score)](#9)\n1. [Conclusion](#10)","236fb347":"# PCA\n**Why PCA?**\n<a id = \"5\"><\/a><br> \n**Since we will be doing clustering with 4 variables, I will reduce the size, because after the clustering, I may have trouble in 4D visualization while visualizing. I will do the dimension reduction with PCA, PCA works like this: for example we have a dataset with 4 columns, we want to make it as many columns as we want, I will reduce it to 2 columns.**","e80ac816":"![supermarket mall my picture.PNG](attachment:0685e60d-32f6-4a66-9608-5319e6b5fac7.PNG)\n\n**People between the ages of 20-40 have made more purchases, considering the inference we just made about women, we can make our target audience more specific.**","aa586553":"<a id = \"2\"><\/a><br> <p style = \"font-size : 45px; color :#190033 ; font-family : 'Comic Sans MS'; text-align : center; background-color : #66FFB2; border-radius: 5px 5px;\"><strong>Exploratory Data Analysis (EDA)<\/strong><\/p>","f1c4881c":"<a id = \"0\"><\/a><br> \n<p style = \"font-size : 22px; color : black ; font-family : 'newtimeroman'; \"><strong>What is Clustering?<\/strong><\/p> \n\n\nIn short, it is the grouping of data with similar characteristics in a data set.\n\n<p style = \"font-size : 22px; color : black ; font-family : 'newtimeroman'; \"><strong>How to make a good clustering?<\/strong><\/p> \n\n\nThe data in the same cluster should be as similar as possible, while the clusters should be as different from each other as possible.\n\nLet's take an example, let's say there are two clusters named A and B. The data in set A should be very similar to each other, and the data in set B should be very similar to each other. But sets A and B must be very different from each other.\n\n<p style = \"font-size : 22px; color : black ; font-family : 'newtimeroman'; \"><strong>What is Unsupervised Learning?<\/strong><\/p> \n\nIt is a type of algorithm that learns patterns from unlabeled data.\n\n<p style = \"font-size : 17px; color : black ; font-family : 'newtimeroman'; \"><strong>Relationship Between Clustering & Unsupervised Learning<\/strong><\/p> \n\nClustering is an unsupervised machine learning task that automatically divides the data into clusters, or groups of similar items. It does this without having been told how the groups should look ahead of time. As we may not even know what we\u2019re looking for, clustering is used for knowledge discovery rather than prediction. It provides an insight into the natural groupings found within data.\n\n\n<p style = \"font-size : 22px; color : black ; font-family : 'newtimeroman'; \"><strong>To Better Understand Clustering: Classification vs Clustering<\/strong><\/p> \n\n\n![clustering_soner_kar.png](attachment:2052f200-135d-43ad-a2d6-6319d5c5eed8.png)\n\nAs seen in figure b, we are trying to divide the same data type into different segments, that is, according to the closeness of the data.\n\nIn figure a, remember the classification problem, there were different types, such as male-female, smoker-non-smoker, this is also expressed by triangle-square in the figure. So there is a huge difference between the two main problems, namely;\nIn classification there is unsupervised learning, in figure a we have the data, we create the classes ourselves. on the left there are already different classes!\n\nThat's why there is a pre-class definition on the left, that is, there is pre-supervision, we teach our surveillance to the machine, that is, the machine tries to see through the eyes we see, but in 'Clustering', the machine is completely free, it perceives itself, it creates the classes itself.\n\n\n<p style = \"font-size : 22px; color : black ; font-family : 'newtimeroman'; \"><strong>Clustering Usage Areas<\/strong><\/p> \n\n* Customer Segmentation \n* Market Segmentation\n* Health and Image Processing\n* Computer Vision\n\n![clustering_usageareas_sonerkar.jpg](attachment:12d88396-4029-46f6-907c-285a193f453b.jpg)\n\n<p style = \"font-size : 22px; color : black ; font-family : 'newtimeroman'; \"><strong>Market Segmentation<\/strong><\/p> \n\nThere are 4 types of market segmentation:\n\n> <p style = \"font-size : 16px; color : black ; font-family : 'newtimeroman'; \"><strong>1. Demographic Segmentation<\/strong><\/p> \n\n> \n> Often the most common and easiest to create, demographic segmentation covers the standard, factual or statistical type information about people. Think, all the different \u201cdemographics\u201d you might find in your Google Analytics review.\n> \n> Some examples of demographic segmentation include:\n> \n> * Age\n> * Gender\n> * Income\n> * Occupation\n> * Family size\n> * Race\n> * Religion\n> * Marital Status\n> * Education\n> * Ethnicity\n\n> <p style = \"font-size : 16px; color : black ; font-family : 'newtimeroman'; \"><strong>2. Psychographic Segmentation<\/strong><\/p> \n> \n> Psychographic segmentation focuses on grouping your target audience by their personalities or inner traits. These aren\u2019t quite as obvious as demographics because they don\u2019t show up on the surface.\n> \n> These are some examples of psychographic segmentation:\n> \n> * Values\n> * Goals\n> * Needs\n> * Pain points\n> * Hobbies\n> * Personality traits\n> * Interests\n> * Political party affiliation\n> * Sexual orientation\n\n> <p style = \"font-size : 16px; color : black ; font-family : 'newtimeroman'; \"><strong>3. Geographic Segmentation<\/strong><\/p>\n> \n> Geographic segmentation is, you guessed it, market segmentation based on the customer\u2019s geographic location. This type of segmentation is especially useful if you have multiple brick-and-mortar locations or offices.\n> \n> These are a few ways you might think about creating a geographic segment:\n> \n> * Zip code\/post code\n> * City\n> * Country\n> * Population density\n> * Distance from a certain location (like your office or store)\n> * Climate\n> * Time zone\n> * Dominate language\n\n> <p style = \"font-size : 16px; color : black ; font-family : 'newtimeroman'; \"><strong>4. Behavioral Segmentation<\/strong><\/p>\n> \n> Behavioral segmentation focuses on the actions your target audience takes. Similar to psychographic segmentation, behavioral segmentation can often take a bit more research and data than geographic or demographic because it goes deeper than surface-level info.\n> \n> However, unlike psychographic, behavioral segmentation tends to focus more on purchases and interactions rather than opinions or thoughts.\n> \n> Here are some examples of behavioral market segmentation you may want to consider:\n> \n> * Purchasing habits\n> * Brand interactions (for example, following or interacting on social media vs calling customer service)\n> * Spending habits\n> * Customer loyalty\n> * Actions taken on a website (such as reading a blog or signing up for your newsletter)\n>\n> <p> For more you can click <a href=\"https:\/\/www.referralcandy.com\/blog\/what-is-market-segmentation\/\">here<\/a><\/p>\n\n\n<p style = \"font-size : 22px; color : black ; font-family : 'newtimeroman'; \"><strong>Customer Segmentation<\/strong><\/p> \n\n\nCustomer segmentation is the practice of dividing a company's customers into groups that reflect similarity among customers in each group.\n\nWhy should this segmentation be done correctly? Grouping people correctly increases the probability of the customer to buy the product, and putting it wrong decreases it.\nthink of customer behavior like playing tennis, taking the subway, etc. For example, if we show special ads to those who take the subway, it will work for our benefit.\n\nI would like to talk about 4 concepts related to customer segmentation.\n\n<p style = \"font-size : 17px; color : black ; font-family : 'newtimeroman'; \"><strong>1. Recommender systems & Collaboration Filtering & Content-Based Filtering.<\/strong><\/p> \n\nRecommender systems are techniques that allow companies to develop sales and marketing and as a result, attract more customers. A recommendation system tries to predict the evaluation of a product made by the user or which product the user will prefer. Collaborative filtering is based on the assumption that people who agreed in the past will agree in the future, and that they will like similar kinds of items as they liked in the past. Recommender systems work very simply like this: For example, if there are two people named Robert and William, let's assume that after Robert likes a product, William likes that product too. Later, when Robert likes another product, this time the recommender system presents the second product Robert liked in front of William.  \n\nClustering is used to build recommender systems, this is a detailed topic, but in short: using customers' weighted RFM(Recency - Frequency - Monetary) and expectation maximization clustering algorithms and their combination with the closest K-neighbors, recommendations for each cluster is independently extracted.\n\nIn Content-Based Filtering, on the other hand, two things are handled, the first is the user's profile, that is, what he likes or dislikes, and the second is the product profile, that is, the product's features. If the user profile and the product profile match, the product is recommended to the user.\n\n<p style = \"font-size : 17px; color : black ; font-family : 'newtimeroman'; \"><strong>2. Special Offers<\/strong><\/p> \n\nAs a result of the evaluations made through customer segmentation, the profit volume of the companies can be increased by making special campaigns for the target audience.\n\n\n<p style = \"font-size : 17px; color : black ; font-family : 'newtimeroman'; \"><strong>3. Threat and Fraud Detection<\/strong><\/p> \n\nOutlier data, that is, outlier for clustering, does not belong to any group, since this outlier data is a threat and fraud, we teach them to the machine like this.\n\n<p style = \"font-size : 17px; color : black ; font-family : 'newtimeroman'; \"><strong>4. Filling Missing Data<\/strong><\/p> \n\nFor successful customer segmentation, there must be no missing data. Clustering provides an important benefit in filling the missing data, for example, if the customer's salary information is missing, instead of filling it with the average of all customers' salaries, it is better to fill it with the average salary of the cluster that the customer is in. Thus, the empty parts in the data set are filled with more accurate data, which positively affects the performance of the models.\n\n\n\n\n<p style = \"font-size : 23px; color : black ; font-family : 'newtimeroman'; \"><strong>Commonly Used Clustering Algorithms<\/strong><\/p> \n\n* KMeans Algorithm\n* Hierarchical Clustering\n \n \n<p style = \"font-size : 22px; color : black ; font-family : 'newtimeroman'; \"><strong>KMeans Algorithm<\/strong><\/p> \n\n\n***Step 1:*** How many clusters will be selected as a parameter from the user.   \n(There is also an algorithm that finds the optimum number of clusters, called XMeans)       \n\n***Step 2:*** k center points are randomly selected (k: number of clusters)\n\n***Step 3:*** Each data point is assigned to the related cluster according to its nearest center point.\n\n***Step 4:*** The center points are shifted by calculating new center points for each cluster.\n\n***Step 5:*** After the new center points are determined, Step 3 and Step 4 are repeated until the system stabilizes.\n\nI made a gif to visualize the steps above, let's watch it.\n\n## ![](https:\/\/media.giphy.com\/media\/otWmVwzlPqpWLTdN4w\/giphy.gif)\n                       \n                       \n\n<p style = \"font-size : 22px; color : black ; font-family : 'newtimeroman'; \"><strong>WCSS and Elbow Method<\/strong><\/p> \n\n* WCSS (within cluster sum of squares) is the sum of squares of the distances of each data point in all clusters to their respective centroids.   \n  Below you can see the calculation of the WCSS value in a clustering example with 3 clusters.        \n\n![WCSS_Equation.jpg](attachment:572e2eea-7664-48bc-aa39-2cc0ea8892ed.jpg)\n  \n\n* The Elbow Method is used to determine the optimal k value (number of clusters).\n\n![Elbow_Method_Figure.jpg](attachment:338f2878-895b-449f-bf1f-0d53b539016a.jpg)\n\nThe WCSS value is calculated for every case where the number of clusters is from 1 to 10, and the optimal number of clusters was determined as '3' for the above example (also, this point called the elbow point). The thing taken into account when determining the number of clusters is the part of the graph where the most breakage occurs. The name of this method is the 'Elbow' Method because the part with the most breakage looks like a real elbow.\n\n\n<p style = \"font-size : 18px; color : black ; font-family : 'newtimeroman'; \"><strong>References:<\/strong><\/p> \n\n* https:\/\/en.wikipedia.org\/wiki\/Unsupervised_learning\n* https:\/\/www.optimove.com\/resources\/learning-center\/customer-segmentation\n* https:\/\/en.wikipedia.org\/wiki\/Recommender_system\n* https:\/\/www.referralcandy.com\/blog\/what-is-market-segmentation\/\n* https:\/\/www.btkakademi.gov.tr\/portal\/course\/python-ile-makine-ogrenmesi-11800\n* https:\/\/www.emerald.com\/insight\/content\/doi\/10.1108\/K-07-2014-0130\/full\/html\n* https:\/\/www.linkedin.com\/pulse\/finding-optimal-number-clusters-k-means-through-elbow-asanka-perera\/\n\n***Written by Soner Kar                                                                                                                               \n28\/07\/2021***  ","bbcb6052":"<a id = \"3\"><\/a><br> <p style = \"font-size : 45px; color :#190033 ; font-family : 'Comic Sans MS'; text-align : center; background-color : #66FFB2; border-radius: 5px 5px;\"><strong>CLUSTERINGS<\/strong><\/p> ","4c143352":" <h1 style=\"background-color:#10DEFF\n;font-family:Comic Sans MS;font-size:250%;text-align:center;border-radius: 15px 50px;\"> Thank you for reading\ud83d\ude03 If you found this notebook helpful, please upvote\ud83d\udc4d and write down positive or negative opinions\ud83d\udcac<\/h1><a id=note><\/a>","a645a4cc":"<p style = \"font-size : 45px; color :#190033 ; font-family : 'Comic Sans MS'; text-align : center; background-color : #66FFB2; border-radius: 5px 5px;\"><strong>TUTORIAL ABOUT CLUSTERING, KMEANS and CUSTOMER SEGMENTATION<\/strong><\/p> ","4b5d08f9":"<a id = \"6\"><\/a><br> <p style = \"font-size : 30px; color :#0009B7 ; font-family : 'Comic Sans MS'; text-align : center; background-color : #F7CF2D; border-radius: 5px 5px;\">Clustering (Age & Annual Income & Spending Score)","377285a9":"**3D visualization was used as there were 3 variables.**","6bd76bd3":"**What do we understand from these 3 graphs?**\n\n**There is no significant difference in the mean spending scores of males and females. Since the mean spending scores are very close to each other, the difference between the total spending scores is the difference between the number of male and female customers, but this difference is not serious. Considering all this, it would be meaningless to choose a gender-based target audience. \u2713**","45d578fb":"<a id = \"8\"><\/a><br> <p style = \"font-size : 30px; color :#0009B7 ; font-family : 'Comic Sans MS'; text-align : center; background-color : #F7CF2D; border-radius: 5px 5px;\">Clustering (Annual Income & Spending Score)<\/p> ","d82d37a3":"![Clustering_LastMessage.jpg](attachment:bbb4da8b-3211-4077-9552-c73323888a7c.jpg)","9db1cdcc":"<a id = \"7\"><\/a><br> <p style = \"font-size : 30px; color :#0009B7 ; font-family : 'Comic Sans MS'; text-align : center; background-color : #F7CF2D; border-radius: 5px 5px;\">Clustering (Age & Annual Income)<\/p> ","ff422b87":"**It is clear from this correlation table that older customers have less income and therefore spend less money.**","351ab1c8":"**As you can see, X data, which we defined as 4 dimensional (red part), has now been reduced to 2 dimensions (green part) thanks to PCA.**","7d0944d9":"**'6' is optimum number of clusters**","f7511b55":"**We saw multiple values of data with the .describe( ) method. What caught my attention here was the height of the standard deviations. Because in the column whose average is 50, about half of it, that is 25 standard deviations, there is also the same situation in the annual income. The Age column also has a standard deviation of about 33% compared to the mean.**","61c02c27":"**'4' is optimum number of clusters.**","954301a8":"**All the columns have become numeric, good.**","8bbac5b0":"![supermarker mall my  figure2.PNG](attachment:2f02dba8-788c-4540-a217-f616c745e98e.PNG)\n\n**One of the two regions shown can be selected as the target audience. Even though the number of people whose annual income is between (40-60)k$ is higher (we understand this from the number of data points), the number of that audience is higher but the spending score is low, so if we make shopping attractive for them by choosing the target audience from the two regions above, we will see more profit can be made.**\n","310075e1":"**'2' is optimum number of clusters.**","ffef86d8":"**As you can see, we converted the 'Gender' column to numeric using the 'Label Encoder'.**\n\n**Male --> 1 , Female -->0** ","9947999c":"**Context**\n \n This data set is created only for the learning purpose of the customer segmentation concepts , also known as market basket analysis . I will demonstrate this by using unsupervised ML technique (KMeans Clustering Algorithm) in the simplest form.\n \n **Content**\n \n You are owing a supermarket mall and through membership cards , you have some basic data about your customers like Customer ID, age, gender, annual income and spending score.\n Spending Score is something you assign to the customer based on your defined parameters like customer behavior and purchasing data.\n \n **Problem Statement**\n \n You own the mall and want to understand the customers like who can be easily converge (Target Customers) so that the sense can be given to marketing team and plan the strategy accordingly.\n \n **Acknowledgements**\n \n From Udemy's Machine Learning A-Z course.\n \n **Inspiration**\n By the end of this case study , you would be able to answer below questions.\n 1- How to achieve customer segmentation using machine learning algorithm (KMeans Clustering) in Python in simplest way.\n 2- Who are your target customers with whom you can start marketing strategy (easy to converse)\n 3- How the marketing strategy works in real world","2da611be":"<a id = \"1\"><\/a><br> <p style = \"font-size : 45px; color :#190033 ; font-family : 'Comic Sans MS'; text-align : center; background-color : #66FFB2; border-radius: 5px 5px;\"><strong>Dataset Information<\/strong><\/p> ","fcceae2d":"**There is no null data, it's good \u2713** \n\n**Dtype of Gender --> Object, therefore I am going to convert it to numeric.**"}}