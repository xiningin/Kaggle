{"cell_type":{"b66b27e3":"code","b8b746c8":"code","bfa3cfd9":"code","f2f24538":"code","5b13ea40":"code","ae3bf838":"code","9fefaad1":"code","f8a2606d":"code","24598b4e":"code","0e126014":"code","a74b3460":"code","1519c100":"code","bb63fe05":"code","4660a5a2":"code","1bf34c55":"code","5b154e7d":"code","df37c390":"code","46223b1a":"code","09ecb3aa":"code","e48a1146":"code","f9028912":"code","bd1db06e":"code","cd86b620":"code","c7cf6dfa":"code","f6892eb0":"code","f55c8ef7":"code","3a9267a9":"code","3a321efa":"code","0210d1ce":"code","6fb4bcc1":"code","520642f5":"code","2ac4aecc":"code","5095409a":"code","862f2f18":"code","26dd55a2":"code","c1f3fce1":"code","b504a4bd":"code","d9276d10":"code","377a80e4":"code","f8adc9ed":"code","113a679b":"code","877e0ab0":"code","bafc46ed":"code","f0288643":"code","58ab736e":"code","f9d3e043":"markdown","c0e3baab":"markdown","06a38161":"markdown","cdcc824d":"markdown","dcf00b45":"markdown","c9de9a1c":"markdown","aafa4cc3":"markdown","daa0e597":"markdown","5e7a69bb":"markdown","c595a06c":"markdown","58ecd099":"markdown","7e82a09f":"markdown","d71cc8df":"markdown","56240ac1":"markdown","8607852c":"markdown","4997befe":"markdown"},"source":{"b66b27e3":"#Import required libraries\nimport numpy as np \nimport pandas as pd \nimport warnings\nwarnings.filterwarnings('ignore')\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","b8b746c8":"df=pd.read_csv('..\/input\/talkingdata-adtracking-fraud-detection\/train_sample.csv')\ndf.head()","bfa3cfd9":"#convert timestamp to datatime\ndef todatetime(df):\n    df['click_time']=pd.to_datetime(df['click_time'])\n    df['click_hour']=df['click_time'].dt.hour\n    df['click_day']=df['click_time'].dt.day\n    df['click_weekday']=df['click_time'].dt.weekday\n    df['click_month']=df['click_time'].dt.month\n    df['click_year']=df['click_time'].dt.year\n    return df","f2f24538":"df=todatetime(df)","5b13ea40":"df=df.drop(['click_time'],axis=1)","ae3bf838":"df.head()","9fefaad1":"df.isnull().sum()","f8a2606d":"df=df.drop('attributed_time',axis=1)","24598b4e":"#Shuffling observations\ndf=df.sample(frac=1)\ndf","0e126014":"import seaborn as sn\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(10,8))\nsn.countplot(x='is_attributed',data=df)\nplt.ylabel('Number of clicks')\nplt.show()","a74b3460":"df['is_attributed'].value_counts()","1519c100":"target_label=df['is_attributed']\ntarget_label.shape","bb63fe05":"ones=df[df['is_attributed']==1]\nzeros=df[df['is_attributed']==0]","4660a5a2":"df=df.drop(['is_attributed','ip'],axis=1)","1bf34c55":"from sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test=train_test_split(df,target_label,test_size=0.2,random_state=42)\nprint(x_train.shape,y_train.shape)\nprint(x_test.shape,y_test.shape)","5b154e7d":"x_train,x_val,y_train,y_val=train_test_split(x_train,y_train,test_size=0.1,random_state=42)\nprint(x_train.shape,y_train.shape)\nprint(x_val.shape,y_val.shape)","df37c390":"import lightgbm as lgb\n#load datasets in lgb formate\ntrain_data=lgb.Dataset(x_train,label=y_train,free_raw_data=False)\nvalidation_data=lgb.Dataset(x_val,label=y_val,free_raw_data=False)","46223b1a":"#set parameters for training\nparams={ 'num_leaves':160,\n        'object':'binary',\n        'metric':['auc','binary_logloss']\n       }","09ecb3aa":"\n#Original LGB model before sampling\nnum_round=100\ndef lgb_basemodel(x_train,y_train):\n    lgb_model=lgb.train(params,train_data,num_round,valid_sets=validation_data,early_stopping_rounds=20)\n    return lgb_model","e48a1146":"#LGBM model after resampling the data using Under sampling techniques\nfrom imblearn.under_sampling import RandomUnderSampler \ndef lgb_downsampling(x_train,y_train):\n    lgb_enn=RandomUnderSampler(random_state=42)\n    x_resample,y_resample=lgb_enn.fit_resample(x_train,y_train)\n    train_data=lgb.Dataset(x_resample,label=y_resample,free_raw_data=False)\n    lgb_model=lgb.train(params,train_data,num_round,valid_sets=validation_data,early_stopping_rounds=20)\n    return lgb_model,x_resample,y_resample;","f9028912":"#LGBM model after resampling the data using Up sampling techniques\nfrom imblearn.over_sampling import SMOTE  #Balances the classes by performing upsampling on minority class\ndef lgb_upsampling(x_train,y_train):\n    lgb_smote= SMOTE(random_state=42)\n    x_resample,y_resample=lgb_smote.fit_resample(x_train,y_train)\n    train_data=lgb.Dataset(x_resample,label=y_resample)\n    lgb_model=lgb.train(params,train_data,num_round,valid_sets=validation_data,early_stopping_rounds=20)\n    return lgb_model,x_resample,y_resample;","bd1db06e":"weight_factor=zeros.shape[0]\/ones.shape[0]  # Ratio of number of samples in majority class to number of samples in minority class\nprint('Weight factor is %0.2f'%(weight_factor))","cd86b620":"#set parameters for training\nparams1={ 'num_leaves':160,\n        'object':'binary',\n        'metric':['auc','binary_logloss'],\n        'scale_pos_weight':397.41                 #Weight of minority class\n       }","c7cf6dfa":"#LGBM model using Upweighting technique for handling the imbalanced data\ndef lgb_Upweighting(x_train,y_train):\n    lgb_model=lgb.train(params1,train_data,num_round,valid_sets=validation_data,early_stopping_rounds=20)\n    return lgb_model;","f6892eb0":"#Basemodel\nlgb_basemodel=lgb_basemodel(x_train,y_train)","f55c8ef7":"#Downsampling model\nlgb_downsampling,x_down,y_down=lgb_downsampling(x_train,y_train)","3a9267a9":"y_down_df=pd.DataFrame(y_down)\ny_down.shape","3a321efa":"plt.hist(y_down);","0210d1ce":"#Upsampling model\nlgb_upsampling,x_up,y_up=lgb_upsampling(x_train,y_train)","6fb4bcc1":"y_up_df=pd.DataFrame(y_up)\ny_up.shape","520642f5":"plt.hist(y_up);","2ac4aecc":"#Upweighting model \nlgb_upweighting=lgb_Upweighting(x_train,y_train);","5095409a":"#Basemodel\ny_base=lgb_basemodel.predict(x_test)","862f2f18":"#Upsampling\ny_upsampling=lgb_upsampling.predict(x_test)","26dd55a2":"#Downsampling\ny_downsampling=lgb_downsampling.predict(x_test)","c1f3fce1":"#Upweighting\ny_upweighting=lgb_upweighting.predict(x_test)","b504a4bd":"from sklearn.metrics import confusion_matrix, classification_report, roc_curve, roc_auc_score\nimport scikitplot as skplt","d9276d10":"#Basemodel\nskplt.metrics.plot_confusion_matrix(y_test,y_base>0.5,normalize=False,figsize=(12,8),title='Confusion matrix for base model')  \nplt.show()","377a80e4":"#Upsampling\nskplt.metrics.plot_confusion_matrix(y_test,y_upsampling>0.5,normalize=False,figsize=(12,8),title='Confusion matrix for upsampling model')  #0.5 is threshold value\nplt.show()","f8adc9ed":"#downsampling\nskplt.metrics.plot_confusion_matrix(y_test,y_downsampling>0.5,normalize=False,figsize=(12,8),title='Confusion matrix for downsampling model')  #0.5 is threshold value\nplt.show()","113a679b":"#Upweighting\nskplt.metrics.plot_confusion_matrix(y_test,y_upweighting>0.5,normalize=False,figsize=(12,8),title='Confusion matrix for upweighting model')  #0.5 is threshold value\nplt.show()","877e0ab0":"#Base model\ncm_base=classification_report(y_test,y_base>0.5)\nprint(cm_base)","bafc46ed":"#Upsampling model\ncm_up=classification_report(y_test,y_upsampling>0.5)\nprint(cm_up)","f0288643":"#Downsampling model\ncm_up=classification_report(y_test,y_downsampling>0.5)\nprint(cm_up)","58ab736e":"#Upweighting model\ncm_upweight=classification_report(y_test,y_upweighting>0.5)  # 0.5 is threshold value\nprint(cm_upweight)","f9d3e043":"**Building LGBM model**","c0e3baab":"**Create validation dataset from train dataset**","06a38161":"**Classification report for all Logistic models**","cdcc824d":"**Testing models on unseen dataset**","dcf00b45":"**Handling Imbalanced datasets**    \nThere are 4 commonly used methods for handling imbalanced datasets   \n**Sampling methods for handling imbalance datasets**\n1. Downsampling (Under sampling)\n2. Upsampling (Over sampling)\n3. Upweighting\n4. Combination of over- and under-sampling\n\n1.**Downsampling(Under sampling):**  \n In this the classes are balanced by performing resampling on majority class. It reduces the size of majority class in order to match with the size of the minority class.\n There are different methods to perform this sampling. one of those method is Random Under sampler, which selectes a subset of data randomly to balance the data in target classes.\n\n2.**Upsampling (Over sampling):**\n  This method is used when there is an insufficient data. In this the resampling performed on minority class. It increases the size of the minority class in order to match with the size   of the majority class.One best example of this sampling is SMOTE (Synthetic minority over sampling Technique).It works by creating synthetic samples from the minor class to balance the   data in target classes.\n  \n3.**Upweighting:**\n  Here the classes are balanced by scaling the weight of minority class. This weight is scaled by taking the ratio of number of samples in majority class to the number of samples in       minority class. A minority class weight of 30 (say) means the model treats the minority class as 30 times as important as it would majority class of weight 1. \n  \n Generally these 3 methods are most widely used to handle imbalanced data in building ML models. For DL models there is a separate batch generator to handle highly imbalanced data.\n \n For more details about sampling methods, please check below link,  \n https:\/\/imbalanced-learn.readthedocs.io\/en\/stable\/api.html#module-imblearn.over_sampling\n ","c9de9a1c":"**Distribution of classes in train dataset before sampling**","aafa4cc3":"The target label data is highly imbalanced (99.75:0.25)%. This needs to be balanced to make the model to be generalize well.","daa0e597":"**Classes:**  \n0- User will not download an app after clicking a mobile app advertisement    \n1- User will download an app after clicking a mobile app advertisement","5e7a69bb":"**Split the dataset into train & test sets**","c595a06c":"**Conclusions:-**\n\n1. Both LGB base & upweighting models are performing same. These models are not at all learning positive labels, so always predicting negative class.\n2. LGB Upsampling (SMOTE) permormed well among all models. This model performance can be improved further by hypermeter tuning\n3. LGB downsampling model performing reasonally but not good.\n\nAll above models are data dependent, so they may perform well on some datasets but not all. It is better to build all models & choose best among for prediction.\nFor large datasets, Ensemble methods can be employed, but these are computationally expensive. Also it is good to check all other sampling methods along with SMOTE & RandomUnderSampler for better understanding of models to handle unbalanced datasets.","58ecd099":"**Train the LGBM models**","7e82a09f":"**LGBM model with Upsampling**","d71cc8df":"**LGBM model with Upweighting**","56240ac1":"**Plot confusion matrix for all Logistic models**","8607852c":"**LGBM basemodel**","4997befe":"**LGBM model with Downsampling**"}}