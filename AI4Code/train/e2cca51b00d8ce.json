{"cell_type":{"9636782c":"code","3fc8b21a":"code","0ea861df":"code","e310035a":"code","657d5707":"code","4a8712fa":"code","41c3598f":"code","6670857e":"code","20a4803b":"code","89e0e65d":"code","d4f96312":"code","0d4ea358":"code","de088fc3":"code","83a51660":"code","f31274a2":"code","907a0e20":"code","8c30f9d2":"code","8d036f03":"code","636a098b":"markdown","4c8a2323":"markdown","805a7e4e":"markdown","be29aa51":"markdown","720cf2cb":"markdown","df020d9b":"markdown","8e9f2759":"markdown","b9776a38":"markdown","7a47822d":"markdown"},"source":{"9636782c":"import pandas as pd\nimport numpy as np\nimport sys\nsys.path.append('..\/input\/timm-pytorch-image-models\/pytorch-image-models-master')\nimport os\nimport sys\nimport time\nimport cv2\nimport PIL.Image\nimport random\nfrom sklearn.metrics import accuracy_score\nfrom tqdm.notebook import tqdm\nimport torch\nfrom torch.utils.data import DataLoader, Dataset\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision\nimport torchvision.transforms as transforms\nimport torch.optim as optim\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\nimport albumentations\nfrom tqdm.notebook import tqdm\nimport matplotlib.pyplot as plt\nimport gc\nfrom sklearn.metrics import roc_auc_score\n%matplotlib inline\nimport seaborn as sns\nfrom pylab import rcParams\nimport timm\nfrom warnings import filterwarnings\nfrom sklearn.preprocessing import LabelEncoder\nimport math\nimport glob\nfrom sklearn.model_selection import StratifiedKFold, GroupKFold\nfilterwarnings(\"ignore\")\n\ndevice = torch.device('cuda')","3fc8b21a":"def seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False # set True to be faster\n    print(f'Setting all seeds to be {seed} to reproduce...')\nseed_everything(42)","0ea861df":"image_size = 512\nbatch_size = 16\nn_worker = 4\ninit_lr = 3e-4\nn_epochs = 6 # from my experiments, use > 25 when margin = 0.5\nfold_id = 0\nholdout_id = 0\nvalid_every = 5\nsave_after = 10\nmargin = 0.5 # 0 for faster convergence, larger may be beneficial\nsearch_space = np.arange(40, 100, 10) # in my experiments, thresholds should be between 40 - 90 (\/100) for cosine similarity\nuse_amp = False # todo: figure how to work with pytorch native amp\ndebug = True # set this to False to train in full\nkernel_type = 'baseline'\nmodel_dir = '.\/weights\/'\ndata_dir = '..\/input\/shopee-product-matching\/train_images'\n! mkdir $model_dir","e310035a":"df_train = pd.read_csv('..\/input\/shopee-product-matching\/train.csv')\ndf_train['file_path'] = df_train.image.apply(lambda x: os.path.join(data_dir, x))\ndf_train.head(5)","657d5707":"gkf = GroupKFold(n_splits=5)\ndf_train['fold'] = -1\nfor fold, (train_idx, valid_idx) in enumerate(gkf.split(df_train, None, df_train.label_group)):\n    df_train.loc[valid_idx, 'fold'] = fold","4a8712fa":"le = LabelEncoder()\ndf_train.label_group = le.fit_transform(df_train.label_group)","41c3598f":"transforms_train = albumentations.Compose([\n    albumentations.Resize(image_size, image_size),\n    albumentations.HorizontalFlip(p=0.5),\n    albumentations.RandomBrightnessContrast(p=0.5, brightness_limit=(-0.2, 0.2), contrast_limit=(-0.2, 0.2)),\n    albumentations.HueSaturationValue(p=0.5, hue_shift_limit=0.2, sat_shift_limit=0.2, val_shift_limit=0.2),\n    albumentations.ShiftScaleRotate(p=0.5, shift_limit=0.0625, scale_limit=0.2, rotate_limit=20),\n    albumentations.CoarseDropout(p=0.5),\n    albumentations.Normalize()\n])\n\ntransforms_valid = albumentations.Compose([\n    albumentations.Resize(image_size, image_size),\n    albumentations.Normalize()\n])","6670857e":"class SHOPEEDataset(Dataset):\n    def __init__(self, df, mode, transform=None):\n        \n        self.df = df.reset_index(drop=True)\n        self.mode = mode\n        self.transform = transform\n        \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, index):\n        row = self.df.loc[index]\n        img = cv2.imread(row.file_path)\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        \n        if self.transform is not None:\n            res = self.transform(image=img)\n            img = res['image']\n                \n        img = img.astype(np.float32)\n        img = img.transpose(2,0,1)\n        \n        if self.mode == 'test':\n            return torch.tensor(img).float()\n        else:\n            return torch.tensor(img).float(), torch.tensor(row.label_group).float()","20a4803b":"dataset = SHOPEEDataset(df_train, 'train', transform = transforms_train)\nrcParams['figure.figsize'] = 15,5\nfor i in range(2):\n    f, axarr = plt.subplots(1,5)\n    for p in range(5):\n        idx = i*5 + p\n        img, label = dataset[idx]\n        axarr[p].imshow(img.transpose(0,1).transpose(1,2).squeeze())\n        axarr[p].set_title(label.item())","89e0e65d":"class ArcModule(nn.Module):\n    def __init__(self, in_features, out_features, s = 10, m = margin):\n        super().__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.s = s\n        self.m = m\n        self.weight = nn.Parameter(torch.FloatTensor(out_features, in_features))\n        nn.init.xavier_normal_(self.weight)\n\n        self.cos_m = math.cos(m)\n        self.sin_m = math.sin(m)\n        self.th = torch.tensor(math.cos(math.pi - m))\n        self.mm = torch.tensor(math.sin(math.pi - m) * m)\n\n    def forward(self, inputs, labels):\n        cos_th = F.linear(inputs, F.normalize(self.weight))\n        cos_th = cos_th.clamp(-1, 1)\n        sin_th = torch.sqrt(1.0 - torch.pow(cos_th, 2))\n        cos_th_m = cos_th * self.cos_m - sin_th * self.sin_m\n        # print(type(cos_th), type(self.th), type(cos_th_m), type(self.mm))\n        cos_th_m = torch.where(cos_th > self.th, cos_th_m, cos_th - self.mm)\n\n        cond_v = cos_th - self.th\n        cond = cond_v <= 0\n        cos_th_m[cond] = (cos_th - self.mm)[cond]\n\n        if labels.dim() == 1:\n            labels = labels.unsqueeze(-1)\n        onehot = torch.zeros(cos_th.size()).cuda()\n        labels = labels.type(torch.LongTensor).cuda()\n        onehot.scatter_(1, labels, 1.0)\n        outputs = onehot * cos_th_m + (1.0 - onehot) * cos_th\n        outputs = outputs * self.s\n        return outputs","d4f96312":"class SHOPEEDenseNet(nn.Module):\n\n    def __init__(self, channel_size, out_feature, dropout=0.5, backbone='densenet121', pretrained=True):\n        super(SHOPEEDenseNet, self).__init__()\n        self.backbone = timm.create_model(backbone, pretrained=pretrained)\n        self.channel_size = channel_size\n        self.out_feature = out_feature\n        self.in_features = self.backbone.classifier.in_features\n        self.margin = ArcModule(in_features=self.channel_size, out_features = self.out_feature)\n        self.bn1 = nn.BatchNorm2d(self.in_features)\n        self.dropout = nn.Dropout2d(dropout, inplace=True)\n        self.fc1 = nn.Linear(self.in_features * 16 * 16 , self.channel_size)\n        self.bn2 = nn.BatchNorm1d(self.channel_size)\n        \n    def forward(self, x, labels=None):\n        features = self.backbone.features(x)\n        features = self.bn1(features)\n        features = self.dropout(features)\n        features = features.view(features.size(0), -1)\n        features = self.fc1(features)\n        features = self.bn2(features)\n        features = F.normalize(features)\n        if labels is not None:\n            return self.margin(features, labels)\n        return features","0d4ea358":"model = SHOPEEDenseNet(512, df_train.label_group.nunique())\nmodel.to(device);","de088fc3":"def train_func(train_loader):\n    model.train()\n    bar = tqdm(train_loader)\n    if use_amp:\n        scaler = torch.cuda.amp.GradScaler()\n    losses = []\n    for batch_idx, (images, targets) in enumerate(bar):\n\n        images, targets = images.to(device), targets.to(device).long()\n        \n        if debug and batch_idx == 100:\n            print('Debug Mode. Only train on first 100 batches.')\n            break\n            \n        if use_amp:\n            with torch.cuda.amp.autocast():\n                logits = model(images, targets)\n                loss = criterion(logits, targets)\n            scaler.scale(loss).backward()\n            if ((batch_idx + 1) %  accumulation_step == 0) or ((batch_idx + 1) == len(train_loader)):\n                scaler.step(optimizer)\n                scaler.update()\n                optimizer.zero_grad()\n        else:\n            logits = model(images, targets)\n            loss = criterion(logits, targets)\n            loss.backward()\n            optimizer.step()\n            optimizer.zero_grad()\n\n        losses.append(loss.item())\n        smooth_loss = np.mean(losses[-30:])\n\n        bar.set_description(f'loss: {loss.item():.5f}, smth: {smooth_loss:.5f}')\n\n    loss_train = np.mean(losses)\n    return loss_train\n\n\ndef valid_func(valid_loader):\n    model.eval()\n    bar = tqdm(valid_loader)\n\n    PROB = []\n    TARGETS = []\n    losses = []\n    PREDS = []\n\n    with torch.no_grad():\n        for batch_idx, (images, targets) in enumerate(bar):\n\n            images, targets = images.to(device), targets.to(device).long()\n\n            logits = model(images, targets)\n\n            PREDS += [torch.argmax(logits, 1).detach().cpu()]\n            TARGETS += [targets.detach().cpu()]\n\n            loss = criterion(logits, targets)\n            losses.append(loss.item())\n           \n            bar.set_description(f'loss: {loss.item():.5f}')\n\n    PREDS = torch.cat(PREDS).cpu().numpy()\n    TARGETS = torch.cat(TARGETS).cpu().numpy()\n    accuracy = (PREDS==TARGETS).mean()\n   \n    loss_valid = np.mean(losses)\n    return loss_valid, accuracy\n\ndef generate_test_features(test_loader):\n    model.eval()\n    bar = tqdm(test_loader)\n    \n    FEAS = []\n    TARGETS = []\n\n    with torch.no_grad():\n        for batch_idx, (images) in enumerate(bar):\n\n            images = images.to(device)\n\n            features = model(images)\n\n            FEAS += [features.detach().cpu()]\n\n    FEAS = torch.cat(FEAS).cpu().numpy()\n    \n    return FEAS","83a51660":"def row_wise_f1_score(labels, preds):\n    scores = []\n    for label, pred in zip(labels, preds):\n        n = len(np.intersect1d(label, pred))\n        score = 2 * n \/ (len(label)+len(pred))\n        scores.append(score)\n    return scores, np.mean(scores)","f31274a2":"def find_threshold(df, lower_count_thresh, upper_count_thresh, search_space):\n    '''\n    Compute the optimal threshold for the given count threshold.\n    '''\n    score_by_threshold = []\n    best_score = 0\n    best_threshold = -1\n    for i in tqdm(search_space):\n        sim_thresh = i\/100\n        selection = ((FEAS@FEAS.T) > sim_thresh).cpu().numpy()\n        matches = []\n        oof = []\n        for row in selection:\n            oof.append(df.iloc[row].posting_id.tolist())\n            matches.append(' '.join(df.iloc[row].posting_id.tolist()))\n        tmp = df.groupby('label_group').posting_id.agg('unique').to_dict()\n        df['target'] = df.label_group.map(tmp)\n        scores, score = row_wise_f1_score(df.target, oof)\n        df['score'] = scores\n        df['oof'] = oof\n        \n        selected_score = df.query(f'count > {lower_count_thresh} and count < {upper_count_thresh}').score.mean()\n        score_by_threshold.append(selected_score)\n        if selected_score > best_score:\n            best_score = selected_score\n            best_threshold = i\n            \n    plt.title(f'Threshold Finder for count in [{lower_count_thresh},{upper_count_thresh}].')\n    plt.plot(score_by_threshold)\n    plt.axis('off')\n    plt.show()\n    print(f'Best score is {best_score} and best threshold is {best_threshold\/100}')","907a0e20":"criterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr = init_lr)\nscheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, n_epochs)","8c30f9d2":"df_train_this = df_train[df_train['fold'] != fold_id]\ndf_valid_this = df_train[df_train['fold'] == fold_id]\n\ndf_valid_this['count'] = df_valid_this.label_group.map(df_valid_this.label_group.value_counts().to_dict())\n\ndataset_train = SHOPEEDataset(df_train_this, 'train', transform = transforms_train)\ndataset_valid = SHOPEEDataset(df_valid_this, 'test', transform = transforms_valid)\n\ntrain_loader = torch.utils.data.DataLoader(dataset_train, batch_size=batch_size, shuffle=True, num_workers = n_worker)\nvalid_loader = torch.utils.data.DataLoader(dataset_valid, batch_size=batch_size, shuffle=False, num_workers = n_worker)","8d036f03":"for epoch in range(n_epochs):\n    scheduler.step()\n    loss_train = train_func(train_loader)\n    if epoch % valid_every == 0: \n        print('Now generating features for the validation set to simulate the submission.')\n        FEAS = generate_test_features(valid_loader)\n        FEAS = torch.tensor(FEAS).cuda()\n        print('Finding Best Threshold in the given search space.')\n        find_threshold(df = df_valid_this, \n               lower_count_thresh = 0, \n               upper_count_thresh = 999,\n               search_space = search_space)\n        if epoch >= save_after:\n            torch.save(model.state_dict(), f'{model_dir}{kernel_type}_fold{fold_id}_densenet_{image_size}_epoch{epoch}.pth')","636a098b":"## Summary\nThis is the training pipeline for a simple arcface densenet model. This model was also proposed in our [RANZCR 2nd place solution](https:\/\/www.kaggle.com\/c\/ranzcr-clip-catheter-line-classification\/discussion\/227407) for image retrieval; it did extremely well on matching. The model arch is implemented by my ex teammate [sheep](https:\/\/www.kaggle.com\/steamedsheep). I want to try this in shopee competition as well. With no parameter tuning, I was able to get 0.653 on the public leaderboard with a CV around 0.732 with a proper validation scheme as I proposed in this notebook. Don't hesitate if you have any questions; answering your questions can help me learn as well. \n\n<img src=\"https:\/\/i.ibb.co\/xSbhXwd\/pipeline-shopee.png\" alt=\"pipeline-shopee\" border=\"0\">\n\nInference notebook: https:\/\/www.kaggle.com\/underwearfitting\/pytorch-densenet-arcface-validation-inference","4c8a2323":"## Train","805a7e4e":"## Utils","be29aa51":"## Model","720cf2cb":"## Imports","df020d9b":"## Transforms","8e9f2759":"## Make Folds","b9776a38":"## Configuration","7a47822d":"## Dataset"}}