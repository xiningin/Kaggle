{"cell_type":{"3b6ea313":"code","8800b359":"code","978e2dbc":"code","8ff8f870":"code","5334ff7f":"code","e9a18658":"code","4c7d1b5f":"code","e54b7a05":"code","192624a9":"code","0cb8605a":"code","712b73e0":"code","8b9e4b5b":"code","896c1515":"code","6a7254b9":"code","db2e0e62":"code","81544e22":"code","56c18e19":"code","79dfee2b":"code","5fc7753a":"code","91be6588":"code","44097df2":"code","d94bdb2d":"code","d9d94dd9":"code","78c089b6":"code","6178e795":"code","d48ced6c":"markdown","70a4a6f5":"markdown","a9b1160f":"markdown","a779785a":"markdown","aeb61dac":"markdown","57f9229a":"markdown","82cb1ee8":"markdown","bb0ef149":"markdown","b26f58a2":"markdown","c5c302a8":"markdown","f623f604":"markdown","519411e9":"markdown","ab7d101d":"markdown"},"source":{"3b6ea313":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","8800b359":"train = pd.read_csv(\"..\/input\/jigsaw-unintended-bias-in-toxicity-classification\/train.csv\")\ntest = pd.read_csv(\"..\/input\/jigsaw-unintended-bias-in-toxicity-classification\/test.csv\")","978e2dbc":"train.head(5)","8ff8f870":"test.head(5)","5334ff7f":"train.info()","e9a18658":"test.info()","4c7d1b5f":"import matplotlib.pyplot as plt\nplt.hist(train['target'])","e54b7a05":"!pip install BeautifulSoup4","192624a9":"train_sample = train.sample(frac = 0.1, random_state = 42, axis = 'index')\ntrain_sample.info()","0cb8605a":"from bs4 import BeautifulSoup # Text Cleaning\nimport re, string # Regular Expressions, String\nfrom nltk.corpus import stopwords # stopwords\nfrom nltk.stem.porter import PorterStemmer # for word stemming\nfrom nltk.stem import WordNetLemmatizer # for word lemmatization\nimport unicodedata\nimport html\n\n# set of stopwords to be removed from text\nstop = set(stopwords.words('english'))\n\n# update stopwords to have punctuation too\nstop.update(list(string.punctuation))\n\ndef clean_text(text):\n    \n    # Remove unwanted html characters\n    re1 = re.compile(r'  +')\n    x1 = text.lower().replace('#39;', \"'\").replace('amp;', '&').replace('#146;', \"'\").replace(\n    'nbsp;', ' ').replace('#36;', '$').replace('\\\\n', \"\\n\").replace('quot;', \"'\").replace(\n    '<br \/>', \"\\n\").replace('\\\\\"', '\"').replace('<unk>', 'u_n').replace(' @.@ ', '.').replace(\n    ' @-@ ', '-').replace('\\\\', ' \\\\ ')\n    text = re1.sub(' ', html.unescape(x1))\n    \n    # remove non-ascii characters\n    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n    \n#     # strip html\n#     soup = BeautifulSoup(text, 'html.parser')\n#     text = soup.get_text()\n    \n    # remove between square brackets\n    text = re.sub('\\[[^]]*\\]', '', text)\n    \n    # remove URLs\n    text = re.sub(r'http\\S+', '', text)\n    \n    # remove twitter tags\n    text = text.replace(\"@\", \"\")\n    \n    # remove hashtags\n    text = text.replace(\"#\", \"\")\n    \n    # remove all non-alphabetic characters\n    text = re.sub(r'[^a-zA-Z ]', '', text)\n    \n    # remove stopwords from text\n    final_text = []\n    for word in text.split():\n        if word.strip().lower() not in stop:\n            final_text.append(word.strip().lower())\n    \n    text = \" \".join(final_text)\n    \n    # lemmatize words\n    lemmatizer = WordNetLemmatizer()    \n    text = \" \".join([lemmatizer.lemmatize(word) for word in text.split()])\n    text = \" \".join([lemmatizer.lemmatize(word, pos = 'v') for word in text.split()])\n    \n    # replace all numbers with \"num\"\n    text = re.sub(\"\\d\", \"num\", text)\n    \n    return text.lower()","712b73e0":"train_sample['clean_comment_text'] = train_sample['comment_text'].apply(clean_text)\ntest['clean_comment_text'] = test['comment_text'].apply(clean_text)","8b9e4b5b":"train_sample['clean_comment_text'].head(5)","896c1515":"plt.hist(list(train_sample['clean_comment_text'].str.split().map(lambda x: len(x))))","6a7254b9":"np.median(np.array(train_sample['clean_comment_text'].str.split().map(lambda x: len(x))))","db2e0e62":"embedding_dict = pd.read_pickle('..\/input\/pickled-glove840b300d-for-10sec-loading\/glove.840B.300d.pkl')","81544e22":"# Sequences creation, truncation and padding\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\n\n# Setting up the tokenizer\nvocab_size = 10000\ntokenizer = Tokenizer(num_words = vocab_size, oov_token = 'UNK')\ntokenizer.fit_on_texts(list(train_sample['clean_comment_text']) + list(test['comment_text']))\n\nmax_len = 18\nX_train_seq = tokenizer.texts_to_sequences(train_sample['clean_comment_text'])\nX_test_seq = tokenizer.texts_to_sequences(test['comment_text'])\n\nX_train_seq = pad_sequences(X_train_seq, maxlen = max_len, truncating = 'post', padding = 'post')\nX_test_seq = pad_sequences(X_test_seq, maxlen = max_len, truncating = 'post', padding = 'post')\ny_train = np.array(train_sample['target']).astype(int)\n\nprint(f\"X_train shape: {X_train_seq.shape}\")\nprint(f\"X_test shape: {X_test_seq.shape}\")\nprint(f\"y_train shape: {y_train.shape}\")","56c18e19":"from sklearn.model_selection import train_test_split\n\nX_train_seq, X_val_seq, y_train, y_val = train_test_split(X_train_seq, y_train, test_size = 0.2, random_state = 42)\n\nprint(f\"X_train shape: {X_train_seq.shape}\")\nprint(f\"X_val shape: {X_val_seq.shape}\")\nprint(f\"y_train shape: {y_train.shape}\")\nprint(f\"y_val shape: {y_val.shape}\")","79dfee2b":"num_words = len(tokenizer.word_index)\nprint(f\"Number of unique words: {num_words}\")","5fc7753a":"# Applying GloVE representations on our corpus\n\nembedding_matrix=np.zeros((num_words,300))\n\nfor word,i in tokenizer.word_index.items():\n    if i < num_words:\n        emb_vec = embedding_dict.get(word)\n        if emb_vec is not None:\n            embedding_matrix[i] = emb_vec    \n            \nembedding_matrix.shape","91be6588":"# Setting up the model\n\nfrom keras import layers\nfrom keras.models import Sequential\n\ndef setup_lstm_model(max_len, n_latent_factors):\n    \n    model = Sequential()\n    model.add(layers.Embedding(num_words, n_latent_factors, weights = [embedding_matrix], \n                               input_length = max_len, trainable = False))\n    model.add(layers.LSTM(units = max_len, return_sequences = True))\n    model.add(layers.GlobalAveragePooling1D())\n    model.add(layers.Dense(units = 1, activation = 'sigmoid'))\n    \n    return model","44097df2":"lstm_model = setup_lstm_model(max_len = max_len, n_latent_factors = 300)\nlstm_model.summary()","d94bdb2d":"# Final hyperparameter configurations\nlstm_model.compile(optimizer = 'adam', loss = 'binary_crossentropy')\n\nbatch_size = 512\nepochs = 5\n\nlstm_model.fit(X_train_seq, y_train, epochs=epochs, batch_size = batch_size, validation_data = (X_val_seq, y_val))","d9d94dd9":"def setup_gru_model(max_len, n_latent_factors):\n    \n    model = Sequential()\n    model.add(layers.Embedding(num_words, n_latent_factors, weights = [embedding_matrix], \n                               input_length = max_len, trainable = False))\n    model.add(layers.GRU(units = max_len, return_sequences = True))\n    model.add(layers.GlobalAveragePooling1D())\n    model.add(layers.Dense(units = 1, activation = 'sigmoid'))\n    \n    return model","78c089b6":"gru_model = setup_gru_model(max_len = max_len, n_latent_factors = 300)\ngru_model.summary()","6178e795":"# Final hyperparameter configurations and fitting\ngru_model.compile(optimizer = 'adam', loss = 'binary_crossentropy')\n\nbatch_size = 512\nepochs = 5\n\ngru_model.fit(X_train_seq, y_train, epochs=epochs, batch_size = batch_size, validation_data = (X_val_seq, y_val))","d48ced6c":"## Sampling from the data","70a4a6f5":"## Sequence Length Analysis","a9b1160f":"## Text Cleaning","a779785a":"# Text Representation","aeb61dac":"## Text Tokenization as Sequences","57f9229a":"## Loading the embedding matrix","82cb1ee8":"## LSTM","bb0ef149":"# Modeling","b26f58a2":"# Data Exploration","c5c302a8":"## Target Variable Exploration","f623f604":"## Text Cleaning and Preprocessing","519411e9":"## GRU","ab7d101d":"## Train Validation Split"}}