{"cell_type":{"e784500a":"code","62fb8e63":"code","7eb0be6a":"code","f803045f":"code","0d6aa0dc":"code","ff3ed8d0":"code","a3b60a3e":"code","79022886":"code","cb134160":"code","46359c8a":"code","b3141a16":"code","4bf4e091":"code","251b002d":"code","a48507f0":"code","6b54015a":"code","c9710037":"code","0486b87f":"code","7b164572":"code","85984d9f":"code","86028c3c":"code","3b174f39":"code","2dc93bb6":"code","cda3964c":"code","2644b84f":"markdown","c897f6f5":"markdown","7958717b":"markdown","e3915066":"markdown","d44cfe1b":"markdown","d567b5a2":"markdown","c6dade40":"markdown","ca638713":"markdown","f5215686":"markdown","a9624d8c":"markdown","32bfb5f1":"markdown","66f014c9":"markdown","55754534":"markdown","a21f2c0f":"markdown","795984a8":"markdown","776c91ae":"markdown"},"source":{"e784500a":"#Loading the necessary libraries\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn import preprocessing\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nimport statsmodels.api as sm","62fb8e63":"back_data = pd.read_csv(\"..\/input\/Dataset_spine.csv\")","7eb0be6a":"del back_data['Unnamed: 13']\nback_data.columns = ['pelvic_incidence','pelvic tilt','lumbar_lordosis_angle','sacral_slope','pelvic_radius','degree_spondylolisthesis','pelvic_slope','Direct_tilt','thoracic_slope','cervical_tilt','sacrum_angle','scoliosis_slope','Status']","f803045f":"## Understanding the structure of the data variables\nback_data.info()\n\n##Checking for missing values. There are no missing values\nprint(back_data.isnull().sum())\n\n## split of the Status column between the two levels Abnormal and Normal\nprint(back_data.Status.describe())","0d6aa0dc":"corr_back = back_data.corr()\n# Generate a mask for the upper right triangle of the square - one half is enough to convey the correlation \n## between the predictors\nmask = np.zeros_like(corr_back, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n\n# Generate the correlation plot \nsns.heatmap(corr_back, mask=mask, center=0, square=True, linewidths=.5)\n\nplt.show()","ff3ed8d0":"# Seeing the correlation values\ncorr_back","a3b60a3e":"back_data.groupby('Status').mean()","79022886":"back_data.groupby('Status').median()","cb134160":"## Generating 3*4 matrix of box plots\nfig, axes = plt.subplots(3, 4, figsize = (15,15))\naxes = axes.flatten()\n\nfor i in range(0,len(back_data.columns)-1):\n    sns.boxplot(x=\"Status\", y=back_data.iloc[:,i], data=back_data, orient='v', ax=axes[i])\n\nplt.tight_layout()\nplt.show()","46359c8a":"back_data.loc[back_data.Status=='Abnormal','Status'] = 1\nback_data.loc[back_data.Status=='Normal','Status'] = 0","b3141a16":"X = back_data.loc[:, back_data.columns != \"Status\"]\ny = back_data.loc[:, back_data.columns == \"Status\"]","4bf4e091":"def data_preprocess(X,y):\n    X_train, X_test, y_train, y_test = train_test_split(X, y.values.ravel(), test_size=0.3, random_state=0)\n    from sklearn.preprocessing import StandardScaler\n    scaler = StandardScaler(copy=True, with_mean=True, with_std=True)\n\n    scaler.fit(X_train)\n\n    # Now apply the transformations to the data:\n    train_scaled = scaler.transform(X_train)\n    test_scaled = scaler.transform(X_test)\n    return(train_scaled, test_scaled, y_train, y_test)","251b002d":"def logistic_regression(x,y):\n    logreg = LogisticRegression().fit(x, y)\n    return(logreg)","a48507f0":"X_train_scaled, X_test_scaled, y_train, y_test = data_preprocess(X,y)\n\nlogreg_result = logistic_regression(X_train_scaled, y_train)\n\nprint(\"Training set score: {:.3f}\".format(logreg_result.score(X_train_scaled,y_train)))\nprint(\"Test set score: {:.3f}\".format(logreg_result.score(X_test_scaled,y_test)))","6b54015a":"logit_model = sm.Logit(y_train, X_train_scaled)\nresult = logit_model.fit()\nprint(result.summary2())","c9710037":"#Removing the highly correlated variables which also had high standard error\ncols_to_include = [cols for cols in X.columns if cols not in ['pelvic_incidence', 'pelvic tilt','sacral_slope']]\nX = back_data[cols_to_include]","0486b87f":"X_train_scaled, X_test_scaled, y_train, y_test = data_preprocess(X,y)\n\nlogreg_result = logistic_regression(X_train_scaled, y_train)\n\nprint(\"Training set score: {:.3f}\".format(logreg_result.score(X_train_scaled,y_train)))\nprint(\"Test set score: {:.3f}\".format(logreg_result.score(X_test_scaled,y_test)))","7b164572":"# to get the statistical summary of the new model\nlogit_model=sm.Logit(y_train,X_train_scaled)\nresult=logit_model.fit()\nprint(result.summary2())","85984d9f":"# considering only the variables which have p-value less than 0.05\nX_trim_1 = X.loc[:,['lumbar_lordosis_angle','pelvic_radius','degree_spondylolisthesis']]","86028c3c":"X_train_scaled, X_test_scaled, y_train, y_test = data_preprocess(X_trim_1,y)\n\nlogreg_result = logistic_regression(X_train_scaled, y_train)\n\nprint(\"Training set score: {:.3f}\".format(logreg_result.score(X_train_scaled,y_train)))\nprint(\"Test set score: {:.3f}\".format(logreg_result.score(X_test_scaled,y_test)))","3b174f39":"logit_model=sm.Logit(y_train,X_train_scaled)\nresult=logit_model.fit()\nprint(result.summary2())","2dc93bb6":"# assigning the model predicted values to y_pred\ny_pred = logreg_result.predict(X_test_scaled)\n\n# assigning the string Normal and Abnormal to the 0 and 1 values respectively. This is useful in plotting \n# the confusion matrix\ny_pred_string = y_pred.astype(str)\ny_pred_string[np.where(y_pred_string == '0')] = 'Normal'\ny_pred_string[np.where(y_pred_string == '1')] = 'Abnormal'\n\ny_test_string = y_test.astype(str)\ny_test_string[np.where(y_test_string == '0')] = 'Normal'\ny_test_string[np.where(y_test_string == '1')] = 'Abnormal'","cda3964c":"from sklearn.metrics import confusion_matrix\nax= plt.subplot()\nlabels = ['Abnormal','Normal']\ncm = confusion_matrix(y_test_string, y_pred_string, labels)\nsns.heatmap(cm, annot=True, ax = ax); #annot=True to annotate cells\n\n# labels, title and ticks\nax.set_xlabel('Predicted labels');ax.set_ylabel('True labels'); \nax.set_title('Confusion Matrix'); \nax.xaxis.set_ticklabels(['Abnormal', 'Normal']); ax.yaxis.set_ticklabels(['Abnormal', 'Normal']);\nplt.show()","2644b84f":"## Initial Data Exploration\n - Checking if there are any missing values in the dataset. Turns out there aren't any.\n - Checking the frequency of the different values of the Status column. Abnormal is 210 of the 310 observations. This means that it is not really an unbalanced dataset.","c897f6f5":"## To explain or to predict\n\n - While the previous model implemented was good in predicting the Target variable on a test set, we did not interpret anything about the individual features - which variable(s) influence the Target variable more. \n - Since this is a medical dataset, there could be a need for explaining the effect of individual variables on the response variable. Hence it would be a good idea to explore the model co-efficients of the predictor variables to see how well each of them influence the response. \n - Let us attempt this in the next section.","7958717b":"## Understanding the model result summary\n\n - The above model did not converge because some variables were highly correlated with each other and this would have led to the correlation\/ covariance matrix to be singular. \n - A matrix can become singular if any rows(columns) can be expressed as a linear combination of any other rows (columns). \n - In fact, it was very intersting to note that in our data, the Pelvic Incidence column values are an exact sum of Pelvic Tilt and Sacral Slope. So that explains.\n - Also in our statistical test results, the Standard error values are very high and p-value is 1 for these three variables. Hence we will remove them and re run the model. ","e3915066":"## Multi Collinearity check\n - Checking if the individual columns are correlated with each other. In which case, they might end up having the same predictive power or explaining the same variation in the dependent variable. \n - The correlation matrix\/ plot is a good way to establish multi collinearity between the dependent variables. Anything closer to +- 1 indicates high correlation between those two predictor variables.\n - We can observe from the plot that pelvic_incidence is highly correlated with pelvic tilt, sacral slope, degree spondylolisthesis and lumbar lordosis angle.","d44cfe1b":"## Interpreting the causative power of the predictors\n\n- Now that we have decided on our final model, let us interpret the co-efficient estimates of the three predictor variables from the above statistical summary. \n    - Both lumbar_lordosis_angle and pelvic_radius have a negative co-efficient indicating that for every unit increase in the values of these variables, the log of odds of the Status being Abnormal decreases. \n    - degree_spondylolisthesis has a positive relationship with Abnormal status i.e for every unit increase in the value of degree_spondylolisthesis the log of odds of the Status being Abnormal increases.","d567b5a2":"## Bi-variate analysis - Relation of each predictor with Target variable\n\nThe status column has two values - 'Abnormal' and 'Normal'. We would like to explore how each of the 12 predictor variables vary with respect to the Status value. We would be more interested in those predictor variables which have a noticeable difference in their values corresponding to 'Normal' and 'Abnormal'. \n\nWe calculate the mean\/ median of values corresponding to Normal and Abnormal for each of the predictor variables. Some observations:\n\n - The first six variables have noticeable difference in the means corresponding to Normal and Abnormal.\n - The mean corresponding to Abnormal for degree_spondylolisthesis is quite higher than the corresponding median. Could be due to outliers.\n - pelvic_radius has lower values for Abnormal as compared to Normal.","c6dade40":"### Box Plots\nTo visualise the above data exploration, we make use of Box Plots as we are comparing Categorical and continuous variables.\n\nA couple of observations which jump out:\n - For the variable degree_spondilolisthesis, 'Normal' status clearly has a much lower range of values as compared to 'Abnormal'. Also shows the presence of a distant outlier. Not removing the outlier - as without domain knowledge, it would be hard to interpret whether it is an incorrect or a rare value.\n - For the variable 'Pelvic Radius' while 'Abnormal' has a much higher range of values, the median value of 'Abnormal' is lower than the median values of 'Normal'.","ca638713":"## Final Model Selection\n\n- The test result below indicate that there is a marginal increase in predictive power - the accuracy on the test set has increased from 74% to 77%. \n- This can be our final chosen model (even though Variable 1 - lumbar_lordosis_angle has a p-value marginally greater than 0.05). \n","f5215686":"## Model convergence\n- The model has now converged after removing the highly correlated variables.\n- There are a few predictors with p-values less than 0.05 (assuming a 95% confidence level). Let us consider only those predictors and re run the model","a9624d8c":"## Unnamed column names\n\n - There is an additional column with notes about the names of each of the individual attributes. This is removed.\n - The columns are not named in the dataset. Adding the column names to each of the corresponding columns. ","32bfb5f1":"# Prediction of back pain using Logistic Regression, Python\n\nThe data comprises of 13 columns and 310 observations. 12 columns are numerical attributes of the spine\/ back. The last column is the Status of the patient - Abnormal indicates presence of Back pain and Normal indicates no back pain. The intent is to predict the Status based on the 12 variables. \n\nIn this kernel, I explore the different variables and use Logistic Regression to predict and understand the causal effects of the different predictors.","66f014c9":"## Modelling and Feature Interpretation\n\n - Implementing a logistic regression classifier with a train test split in a 70:30 ratio. \n - The fitted model when applied on the test data returns an accuracy of 81.7%. ","55754534":"## Sensitivity and Specificity\n\nFrom the confusion matrix, we can calculate the Sensitivity True Positive\/ (True positive + False Negative) and Specificity - True Negative \/ (True Negative + False Positive). \nSensitivity = 74.6% and Specificity = 76.6%\n\nI am not quite sure in this specific problem context, which one should be given more importance (or should have a higher value). ","a21f2c0f":"The test results indicate that the predictive power has gone down after removing the highly correlated variables. Let us look at the statistical summary below.","795984a8":"## Final processing\n\n - For modelling purpose, we map all the predictor variables to a array X and the target variable to an array Y. \n - The class labels 'Abnormal' and 'Normal' are numerically encoded to 1 and 0. While this is not necessary as the sklearn module can handle it internally, it is convenient for graphing the Receiver Operating curve (if required).\n - The variables are subjected to Standardization (mean zero and unit variance) before being fed to the model.","776c91ae":"### Thanks for coming along all the way. Would love to hear inputs for improvement. Thanks."}}