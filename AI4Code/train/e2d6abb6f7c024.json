{"cell_type":{"bbd1d97b":"code","655757e1":"code","110e358e":"code","ebe4ba7b":"code","083d649e":"code","9c8bb011":"code","f698ffa1":"code","a01c8d8a":"code","0e7ceaa1":"code","a074d12d":"code","525f27a7":"code","4f4c5bf5":"markdown","892fc6e2":"markdown","d3fe2913":"markdown"},"source":{"bbd1d97b":"import os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator","655757e1":"# setting the path and the labels list for classification of targets on the basis in human understandable form\n\ntrain_dir = os.path.join('\/kaggle\/input\/garbage-classification\/Garbage classification\/Garbage classification')\nlabels = ['cardboard', 'glass', 'metal', 'paper', 'plastic', 'trash']","110e358e":"# checking the size of data available to us for training out model\n\nfor label in labels:\n    directory = os.path.join(train_dir, label)\n    print(\"Images of label \\\"\" + label + \"\\\":\\t\", len(os.listdir(directory)))","ebe4ba7b":"# plotting images of different review for understanding the dataset\n\nplt.figure(figsize=(30,14))\n\nfor i in range(6):\n    directory = os.path.join(train_dir, labels[i])\n    for j in range(10):\n        path = os.path.join(directory, os.listdir(directory)[j])\n        img = mpimg.imread(path)\n        \n        plt.subplot(6, 10, i*10 + j + 1)\n        plt.imshow(img)\n        \n        if j == 0:\n            plt.ylabel(labels[i], fontsize=20)\n        \nplt.setp(plt.gcf().get_axes(), xticks=[], yticks=[]);\nplt.tight_layout()\nplt.show()","083d649e":"# checking size of individual image\n\ndirectory = os.path.join(train_dir, 'cardboard')\npath = os.path.join(directory, os.listdir(directory)[0])\nimage = mpimg.imread(path)\nimage.shape","9c8bb011":"# creating the model\n\nmodel = tf.keras.Sequential([\n  tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(384, 512, 3)),\n  tf.keras.layers.BatchNormalization(),\n  tf.keras.layers.Conv2D(32, (3, 3), activation='relu'),\n  tf.keras.layers.BatchNormalization(),\n  tf.keras.layers.MaxPooling2D(2, 2),\n  tf.keras.layers.Dropout(0.2),\n    \n  tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n  tf.keras.layers.BatchNormalization(),\n  tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n  tf.keras.layers.BatchNormalization(),\n  tf.keras.layers.MaxPooling2D(2, 2),\n  tf.keras.layers.Dropout(0.2),\n    \n  tf.keras.layers.Conv2D(128, (3, 3), activation='relu'),\n  tf.keras.layers.BatchNormalization(),\n  tf.keras.layers.Conv2D(128, (3, 3), activation='relu'),\n  tf.keras.layers.BatchNormalization(),\n  tf.keras.layers.MaxPooling2D(2, 2),\n  tf.keras.layers.Dropout(0.2),\n\n  tf.keras.layers.Conv2D(256, (3, 3), activation='relu'),\n  tf.keras.layers.BatchNormalization(),\n  tf.keras.layers.Conv2D(256, (3, 3), activation='relu'),\n  tf.keras.layers.BatchNormalization(),\n  tf.keras.layers.MaxPooling2D(2, 2),\n  tf.keras.layers.Dropout(0.2),\n    \n  tf.keras.layers.Flatten(),\n  tf.keras.layers.Dense(512, activation='relu'),\n  tf.keras.layers.BatchNormalization(),\n  tf.keras.layers.Dropout(0.4),\n  tf.keras.layers.Dense(6, activation='softmax')\n])\n\nmodel.summary()","f698ffa1":"model.compile(loss='sparse_categorical_crossentropy', optimizer=keras.optimizers.Adam(lr = 0.0001), metrics=['accuracy'])","a01c8d8a":"# creating generators for combining data and increasing the gainable insights by slightly modifying the images in the dataset\n\ntrain_datagen = ImageDataGenerator(horizontal_flip=True,vertical_flip=True,\n                                   rotation_range=15,zoom_range=0.1,\n                                   width_shift_range=0.15,height_shift_range=0.15,\n                                   shear_range=0.1,\n                                   fill_mode=\"nearest\",\n                                   rescale=1.\/255., \n                                   validation_split=0.2)\n\ntrain_generator = train_datagen.flow_from_directory(train_dir, target_size=(384, 512), batch_size=32, class_mode='binary', subset='training')\nvalidation_generator = train_datagen.flow_from_directory(train_dir, target_size=(384, 512), batch_size=32, class_mode='binary', subset='validation')","0e7ceaa1":"# creating the callback function so that it can be used to end the training in case reached a good accuracy rate (above 90%)\n\nclass myCallback(tf.keras.callbacks.Callback):\n    def on_epoch_end(self, epoch, logs={}):\n        if(logs.get('accuracy')>0.90):\n            print(\"\\nReached 90% accuracy so cancelling training!\")\n            self.model.stop_training = True\n\ncallbacks = myCallback()","a074d12d":"history = model.fit(train_generator, epochs=50, verbose=1, validation_data=validation_generator, callbacks=[callbacks])","525f27a7":"cat = int(input('Enter any category by index: '))\nind = int(input('Enter any index to test: '))\n\ndirectory = os.path.join(train_dir, labels[cat % 6])\ntry:\n    path = os.path.join(directory, os.listdir(directory)[ind])\n    img = mpimg.imread(path)\n    x = keras.preprocessing.image.img_to_array(img)\n    x = np.expand_dims(x, axis=0)\n\n    images = np.vstack([x])\n    classes = model.predict(images)\n    pred = labels[np.argmax(classes)]\n    \n    plt.imshow(img)\n    plt.xticks([])\n    plt.yticks([])\n    plt.title('Actual: {}      Pred: {}'.format(labels[cat], pred))\n    \nexcept:\n    print('Invalid Value')","4f4c5bf5":"# Deep Learning","892fc6e2":"# Importing Libraries","d3fe2913":"# Setting up File locations and Checking sample images"}}