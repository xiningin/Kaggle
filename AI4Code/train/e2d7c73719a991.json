{"cell_type":{"6a66ed00":"code","47c20060":"code","72b3a457":"code","bbbddda9":"code","e2a2a860":"code","d656e6e1":"code","edf014d4":"code","384c41ba":"code","88a32628":"code","0eae4e52":"code","785bc411":"code","75815322":"code","122f0e21":"code","00ccd443":"code","2c7a31bf":"code","8044ffe8":"code","38b57150":"code","5ba675fe":"code","c6dabec2":"code","49ad6858":"code","bd22838f":"code","f040e61a":"code","2b960134":"code","1f04505f":"code","28769644":"code","0dd48c6c":"code","a5f78fdd":"code","545f235f":"code","292f9316":"code","6117b360":"code","e3b5ac0a":"code","50e06439":"code","7d1dbee9":"code","bba71426":"code","4bc3850b":"code","e52dc1fe":"code","93420153":"code","7b514429":"code","e16cfe0c":"code","559d115b":"code","4d550c23":"code","e5fa22b5":"code","c662b2ac":"code","5a2f8748":"markdown","0a8a61d7":"markdown","8243f336":"markdown","6bef5975":"markdown","06d5c128":"markdown","35affb86":"markdown","cd4faa96":"markdown","46a0a15e":"markdown"},"source":{"6a66ed00":"import tensorflow as tf\nfrom tensorflow import keras\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport glob\nimport random\nfrom matplotlib.patches import Rectangle\nfrom lxml import etree\n%matplotlib inline","47c20060":"import os\nos.listdir('..\/input\/images\/images')","72b3a457":"os.listdir('..\/input\/label\/label')","bbbddda9":"image_path = glob.glob('..\/input\/images\/images\/*\/*.jpg')\nlen(image_path)","e2a2a860":"image_path[:3]","d656e6e1":"xmls_path = glob.glob('..\/input\/label\/label\/*.xml')\nlen(xmls_path)","edf014d4":"xmls_path[:3]","384c41ba":"#xml_name extraction\nxmls_train = [p.split('\/')[-1].split('.')[0] for p in xmls_path]\nxmls_train[:3]","88a32628":"#img_name extraction\nimgs_train = [img for img in image_path if (img.split('\/')[-1].split)('.jpg')[0] in xmls_train]\nimgs_train[:3]","0eae4e52":"len(imgs_train),len(xmls_path)","785bc411":"#check the image to label sorts\nxmls_path.sort(key=lambda x:x.split('\/')[-1].split('.xml')[0])\nimgs_train.sort(key=lambda x:x.split('\/')[-1].split('.jpg')[0])\nxmls_path[:3],imgs_train[:3]","75815322":"#labels names\nnames = [x.split(\"\/\")[-2] for x in imgs_train]\nnames[:3]","122f0e21":"names = pd.DataFrame(names,columns=['Types'])\nnames","00ccd443":"#onehot for mutiple classes\nfrom sklearn.preprocessing import LabelBinarizer\n\nClass = names['Types'].unique()\nClass_dict = dict(zip(Class, range(1,len(Class)+1)))\nnames['str'] = names['Types'].apply(lambda x: Class_dict[x])\nlb = LabelBinarizer()\nlb.fit(list(Class_dict.values()))\ntransformed_labels = lb.transform(names['str'])\ny_bin_labels = []  \n\nfor i in range(transformed_labels.shape[1]):\n    y_bin_labels.append('str' + str(i))\n    names['str' + str(i)] = transformed_labels[:, i]","2c7a31bf":"Class_dict","8044ffe8":"names.drop('str',axis=1,inplace=True)\nnames.drop('Types',axis=1,inplace=True)\nnames.head()","38b57150":"#analysis rectangular box value in xmls\ndef to_labels(path):\n    xml = open('{}'.format(path)).read()                         #read xml in path \n    sel = etree.HTML(xml)                     \n    width = int(sel.xpath('\/\/size\/width\/text()')[0])     #extract the width\/height\n    height = int(sel.xpath('\/\/size\/height\/text()')[0])    #extract the x,y value\n    xmin = int(sel.xpath('\/\/bndbox\/xmin\/text()')[0])\n    xmax = int(sel.xpath('\/\/bndbox\/xmax\/text()')[0])\n    ymin = int(sel.xpath('\/\/bndbox\/ymin\/text()')[0])\n    ymax = int(sel.xpath('\/\/bndbox\/ymax\/text()')[0])\n    return [xmin\/width, ymin\/height, xmax\/width, ymax\/height]   #return the four relative points ","5ba675fe":"#set value to labels\nlabels = [to_labels(path) for path in xmls_path]\nlabels[:3]","c6dabec2":"#set four labels as outputs\nout1,out2,out3,out4 = list(zip(*labels))        \n#convert to np.array\nout1 = np.array(out1)\nout2 = np.array(out2)\nout3 = np.array(out3)\nout4 = np.array(out4)\nlabel = np.array(names.values)","49ad6858":"#label to tf.data\nlabel_datasets = tf.data.Dataset.from_tensor_slices((out1,out2,out3,out4,label))\nlabel_datasets","bd22838f":"#def load_image function\ndef load_image(path):\n    image = tf.io.read_file(path)                           \n    image = tf.image.decode_jpeg(image,3)               \n    image = tf.image.resize(image,[224,224])               \n    image = tf.cast(image\/127.5-1,tf.float32)                 \n    return image      ","f040e61a":"#build dataset\ndataset = tf.data.Dataset.from_tensor_slices(imgs_train)\ndataset = dataset.map(load_image)","2b960134":"dataset_label = tf.data.Dataset.zip((dataset,label_datasets))","1f04505f":"#batch constant\nBATCH_SIZE = 16\nAUTO = tf.data.experimental.AUTOTUNE","28769644":"#batch extraction and shuffle\ndataset_label = dataset_label.repeat().shuffle(500).batch(BATCH_SIZE)\ndataset_label = dataset_label.prefetch(AUTO)","0dd48c6c":"#Split dataset\ntest_count = int(len(imgs_train)*0.2)\ntrain_count = len(imgs_train) - test_count\ntest_count,train_count","a5f78fdd":"train_dataset = dataset_label.skip(test_count)\ntest_dataset = dataset_label.take(test_count)","545f235f":"train_dataset","292f9316":"species_dict = {v:k for k,v in Class_dict.items()}","6117b360":"#check from train_data\nfor img, label in train_dataset.take(1):\n    plt.imshow(keras.preprocessing.image.array_to_img(img[0]))     \n    out1,out2,out3,out4,out5 = label                            \n    xmin,ymin,xmax,ymax = out1[0].numpy()*224,out2[0].numpy()*224,out3[0].numpy()*224,out4[0].numpy()*224\n    rect = Rectangle((xmin,ymin),(xmax-xmin),(ymax-ymin),fill=False,color='r')  \n    ax = plt.gca()                      \n    ax.axes.add_patch(rect)   \n    pred_imglist = []\n    pred_imglist.append(species_dict[np.argmax(out5[0])+1])\n    plt.title(pred_imglist)\n    plt.show()","e3b5ac0a":"#Convolution based\nconv = keras.applications.xception.Xception(weights='imagenet',\n                                            include_top=False,\n                                            input_shape=(224,224,3),\n                                            pooling='avg')","50e06439":"#open trainable\nconv.trainable = True","7d1dbee9":"#define Conv + FC structure\ninputs = keras.Input(shape=(224,224,3))\nx = conv(inputs)\nx1 = keras.layers.Dense(1024,activation='relu')(x)\nx1 = keras.layers.Dense(512,activation='relu')(x1)\n\n\nout1 = keras.layers.Dense(1,name='out1')(x1)\nout2 = keras.layers.Dense(1,name='out2')(x1)\nout3 = keras.layers.Dense(1,name='out3')(x1)\nout4 = keras.layers.Dense(1,name='out4')(x1)\n\nx2 = keras.layers.Dense(1024,activation='relu')(x)\nx2 = keras.layers.Dropout(0.5)(x2)\nx2 = keras.layers.Dense(512,activation='relu')(x2)\nout_class = keras.layers.Dense(10,activation='softmax',name='out_item')(x2)\n\nout = [out1,out2,out3,out4,out_class]\n\nmodel = keras.models.Model(inputs=inputs,outputs=out)\nmodel.summary()","bba71426":"#model compille\nmodel.compile(keras.optimizers.Adam(0.0003),\n              loss={'out1':'mse',\n                    'out2':'mse',\n                    'out3':'mse',\n                    'out4':'mse',\n                    'out_item':'categorical_crossentropy'},\n              metrics=['mae','acc'])","4bc3850b":"#learning_rate reduce module\nlr_reduce = keras.callbacks.ReduceLROnPlateau('val_loss', patience=6, factor=0.5, min_lr=1e-6)","e52dc1fe":"history = model.fit(train_dataset,\n                   steps_per_epoch=train_count\/\/BATCH_SIZE,\n                   epochs=200,\n                   validation_data=test_dataset,\n                   validation_steps=test_count\/\/BATCH_SIZE)","93420153":"#training visualization\ndef plot_history(history):                \n    hist = pd.DataFrame(history.history)           \n    hist['epoch']=history.epoch\n    \n    plt.figure()                                     \n    plt.xlabel('Epoch')\n    plt.ylabel('MSE')               \n    plt.plot(hist['epoch'],hist['loss'],\n            label='Train Loss')\n    plt.plot(hist['epoch'],hist['val_loss'],\n            label='Val Loss')                           \n    plt.legend()\n    \n    plt.figure()                                      \n    plt.xlabel('Epoch')\n    plt.ylabel('Val_MAE')               \n    plt.plot(hist['epoch'],hist['val_out1_mae'],\n            label='Out1_mae')\n    plt.plot(hist['epoch'],hist['val_out2_mae'],\n            label='Out2_mae')\n    plt.plot(hist['epoch'],hist['val_out3_mae'],\n            label='Out3_mae')\n    plt.plot(hist['epoch'],hist['val_out4_mae'],\n            label='Out4_mae')\n    plt.legend()      \n    \n    plt.figure()                                      \n    plt.xlabel('Epoch')\n    plt.ylabel('Val_Item_Acc')               \n    plt.plot(hist['epoch'],hist['val_out_item_acc'],\n            label='Out5_acc')\n    \n    plt.show()\n    \nplot_history(history)      ","7b514429":"mae = model.evaluate(test_dataset)","e16cfe0c":"print('out1_mae in test:{}'.format(mae[6]))\nprint('out2_mae in test:{}'.format(mae[8]))\nprint('out3_mae in test:{}'.format(mae[10]))\nprint('out4_mae in test:{}'.format(mae[12]))\nprint('class_label in test:{}'.format(mae[15]))","559d115b":"model.save(\"class_location.h5\")","4d550c23":"species_dict = {v:k for k,v in Class_dict.items()}","e5fa22b5":"species_dict","c662b2ac":"plt.figure(figsize=(10,24))\nfor img,_ in train_dataset.take(1):\n    out1,out2,out3,out4,label = model.predict(img)\n    for i in range(3):\n        plt.subplot(3,1,i+1)            \n        plt.imshow(keras.preprocessing.image.array_to_img(img[i]))    \n        pred_imglist = []\n        pred_imglist.append(species_dict[np.argmax(out5[i])+1])\n        plt.title(pred_imglist)\n        xmin,ymin,xmax,ymax = out1[i]*224,out2[i]*224,out3[i]*224,out4[i]*224\n        rect = Rectangle((xmin,ymin),(xmax-xmin),(ymax-ymin),fill=False,color='r') \n        ax = plt.gca()                   \n        ax.axes.add_patch(rect)        ","5a2f8748":"### 2. Model Building","0a8a61d7":"#### 1.1 Image_Label_Preview","8243f336":"#### 2.1 The VGG16 model ","6bef5975":"#### 2.2 Compile & Fitting","06d5c128":"#### 1.2 Load Data","35affb86":"#### 2.3 Model Evaluation","cd4faa96":"### 1. Data Preprocessing","46a0a15e":"#### 1.3 Extraction & Input pipe "}}