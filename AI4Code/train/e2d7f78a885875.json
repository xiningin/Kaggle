{"cell_type":{"46171c35":"code","9d924bf8":"code","48499bce":"code","053076d5":"code","2385d7b4":"code","2b81be46":"code","07bb3cba":"code","32f4a0f7":"code","feeee7f2":"code","36275048":"code","4845be88":"code","356af56f":"code","075a6cb8":"code","11a50771":"code","1fab147d":"code","623fcefd":"code","3e1714a1":"code","7b7f55cc":"code","20405193":"code","50630716":"code","8e83cf1f":"code","fd821ccf":"code","f4d2ad30":"code","a4449313":"code","f42e543e":"markdown","cefd54c3":"markdown","9612be1d":"markdown","9d74be6a":"markdown","5cecf986":"markdown","9098a52b":"markdown","9dd1e3cb":"markdown","4727dcd1":"markdown","69c72655":"markdown","2d2a6001":"markdown","b69c3d12":"markdown","d399dd83":"markdown","282224d2":"markdown","9dabdf32":"markdown","080ab994":"markdown","ebeb4306":"markdown","37b69431":"markdown","31819bb5":"markdown","0af51d5f":"markdown","2c3e72aa":"markdown","4579557d":"markdown","f7e3613b":"markdown","00e6ac3c":"markdown","c8fff30a":"markdown","f3cb18d2":"markdown","be8df9d9":"markdown","ed2210c1":"markdown","c03210f1":"markdown","6775df1c":"markdown","a87f1520":"markdown","2e4704a3":"markdown","ef8f6896":"markdown"},"source":{"46171c35":"import numpy as np\nimport pandas as pd","9d924bf8":"# Importing the dataset\ndataset = pd.read_csv('..\/input\/toapakarall\/toasemuapakar.csv')","48499bce":"# untuk melakukan check pada jumlah baris dan kolom pada data\ndataset.shape","053076d5":"dataset.head()","2385d7b4":"dataset.describe()","2b81be46":"# melihat jumlah per masing\" data target\ndataset.groupby('Pakar').size()","07bb3cba":"feature_columns = ['Breakdown Voltage','Water Content','Dissolved Gass Analysis']\nX = dataset[feature_columns].values\ny = dataset['Pakar'].values\nprint (\"Feature :\")\ndisplay(X)\nprint (\"Target :\")\ndisplay(y)","32f4a0f7":"from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\ndisplay(y)\ny = le.fit_transform(y)\nprint (y)\nprint (le)","feeee7f2":"df_norm = dataset[['Breakdown Voltage','Water Content','Dissolved Gass Analysis']].apply(lambda X: (X - X.min()) \/ (X.max() - X.min()))\ndf_norm.sample(n=4)\ndf_norm.describe()","36275048":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)","4845be88":"import matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","356af56f":"plt.figure()\nsns.pairplot(dataset.drop([\"NO\",\"Serial Number\"], axis=1), hue = \"Pakar\", height=3, markers=[\"o\", \"s\", \"D\"])\nplt.show()","075a6cb8":"# Fitting clasifier to the Training set\n# Loading libraries\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nfrom sklearn.model_selection import cross_val_score\n\n# Instantiate learning model (k = 3)\nclassifier = KNeighborsClassifier(n_neighbors=3)\n\n# Fitting the model\nclassifier.fit(X_train, y_train)\n\n# Predicting the Test set results\ny_pred = classifier.predict(X_test)\n","11a50771":"from sklearn.neighbors import KNeighborsClassifier\n\ntrain_scores = []\ntest_scores = []\n\nfor i in range(1,15):\n\n    knn = KNeighborsClassifier(i)\n    knn.fit(X_train,y_train)\n    \n    train_scores.append(knn.score(X_train,y_train))\n    test_scores.append(knn.score(X_test,y_test))\n\nmax_train_score = max (train_scores)\ntrain_scores_ind = [i for i, v in enumerate(train_scores) if v == max_train_score]\nprint('Max train score {} % and k = {}'.format(max_train_score*100,list(map(lambda x: x+1, train_scores_ind))))","1fab147d":"#membuat confusion matrix\ncm = confusion_matrix(y_test, y_pred)\ncm","623fcefd":"accuracy = accuracy_score(y_test, y_pred)*100\nprint('Accuracy of our model is equal ' + str(round(accuracy, 2)) + ' %.')","3e1714a1":"# creating list of K for KNN\nk_list = list(range(1,50,2))\n# creating list of cv scores\ncv_scores = []\n\n# perform 10-fold cross validation\nfor k in k_list:\n    knn = KNeighborsClassifier(n_neighbors=k)\n    scores = cross_val_score(knn, X_train, y_train, cv=10, scoring='accuracy')\n    cv_scores.append(scores.mean())","7b7f55cc":"# changing to misclassification error\nMSE = [1 - x for x in cv_scores]\n#menampilkan grafik Hubungan nilai K yang digunakan dengan misclasification error\nplt.figure()\nplt.figure(figsize=(15,10))\nplt.title('The optimal number of neighbors', fontsize=20, fontweight='bold')\nplt.xlabel('Number of Neighbors K', fontsize=15)\nplt.ylabel('Misclassification Error', fontsize=15)\nsns.set_style(\"whitegrid\")\nplt.plot(k_list, MSE)\n\nplt.show()","20405193":"# finding best k\nbest_k = k_list[MSE.index(min(MSE))]\nprint(\"The optimal number of neighbors is %d.\" % best_k)","50630716":"import numpy as np\nimport pandas as pd\nimport scipy as sp\n\nclass MyKNeighborsClassifier():\n    \"\"\"\n    My implementation of KNN algorithm.\n    \"\"\"\n    \n    def __init__(self, n_neighbors=11):\n        self.n_neighbors=n_neighbors\n        \n    def fit(self, X, y):\n        \"\"\"\n        Fit the model using X as array of features and y as array of labels.\n        \"\"\"\n        n_samples = X.shape[0]\n        # number of neighbors can't be larger then number of samples\n        if self.n_neighbors > n_samples:\n            raise ValueError(\"Number of neighbors can't be larger then number of samples in training set.\")\n        \n        # X and y need to have the same number of samples\n        if X.shape[0] != y.shape[0]:\n            raise ValueError(\"Number of samples in X and y need to be equal.\")\n        \n        # finding and saving all possible class labels\n        self.classes_ = np.unique(y)\n        \n        self.X = X\n        self.y = y\n        \n    def predict(self, X_test):\n        \n        # number of predictions to make and number of features inside single sample\n        n_predictions, n_features = X_test.shape\n        \n        # allocationg space for array of predictions\n        predictions = np.empty(n_predictions, dtype=int)\n        \n        # loop over all observations\n        for i in range(n_predictions):\n            # calculation of single prediction\n            predictions[i] = single_prediction(self.X, self.y, X_test[i, :], self.n_neighbors)\n\n        return(predictions)","8e83cf1f":"def single_prediction(X, y, x_train, k):\n    \n    # number of samples inside training set\n    n_samples = X.shape[0]\n    \n    # create array for distances and targets\n    distances = np.empty(n_samples, dtype=np.float64)\n\n    # distance calculation\n    for i in range(n_samples):\n        distances[i] = (x_train - X[i]).dot(x_train - X[i])\n    \n    # combining arrays as columns\n    distances = sp.c_[distances, y]\n    # sorting array by value of first column\n    sorted_distances = distances[distances[:,0].argsort()]\n    # celecting labels associeted with k smallest distances\n    targets = sorted_distances[0:k,1]\n\n    unique, counts = np.unique(targets, return_counts=True)\n    return(unique[np.argmax(counts)])","fd821ccf":"# Instantiate learning model (k = 11)\nmy_classifier = MyKNeighborsClassifier(n_neighbors=11)\n\n# Fitting the model\nmy_classifier.fit(X_train, y_train)\n\n# Predicting the Test set results\nmy_y_pred = my_classifier.predict(X_test)","f4d2ad30":"accuracy = accuracy_score(y_test, my_y_pred)*100\nprint('Accuracy of our model is equal ' + str(round(accuracy, 2)) + ' %.')","a4449313":"prediksi = [] \nprint(\"Program ini akan menentukan tindakan purifikasi minyak transformer berdasarkan parameter pengukuran. silahkan input hasil pengujian minyak transformer :\")\nBDV = input(\"Masukan hasil test Breakdown Voltage : \")\nWC = input(\"Masukan hasil test Water Content : \")\nDGA = input(\"Masukan hasil test Dissolved Gass Analysis : \")\nnew_prediction = classifier.predict(np.array([[BDV,WC,DGA]]))\nlabel_idx = np.argmax(new_prediction) \nprint (\"Keputusan purifikasi adalah :\",new_prediction) \nprint (\"0 = ganti\")\nprint (\"1 = purifikasi\")\nprint (\"2 = tunda\")","f42e543e":"# K Nearest Neighbor ","cefd54c3":"### 1. KNN Theory","9612be1d":"Building confusion matrix:","9d74be6a":"#### 2.2 Load dataset","5cecf986":"prediksi = []\nprint(\"Program ini akan menentukan tindakan purifikasi minyak transformer berdasarkan parameter pengukuran. silahkan input hasil pengujian minyak transformer :\")\nBDV = input(\"Masukan hasil test Breakdown Voltage : \")\nWC = input(\"Masukan hasil test Water Content : \")\nDGA = input(\"Masukan hasil test Dissolved Gass Analysis : \")\nnew_prediction = classifier.predict(np.array([[BDV,WC,DGA]]))\nlabel_idx = np.argmax(new_prediction)\nprint (\"Keputusan purifikasi adalah :\",new_prediction)\nprint (\"0 = ganti\"\"1 = purifikasi\"\"2 = tunda\")","9098a52b":"#### 4.3. Using cross-validation for parameter tuning:","9dd1e3cb":"#### 2.5 Label encoding","4727dcd1":"KNN can be used for both classification and regression predictive problems. KNN falls in the supervised learning family of algorithms. Informally, this means that we are given a labelled dataset consiting of training observations $(x,y)$ and would like to capture the relationship between $x$ and $y$. More formally, our goal is to learn a function $h: X\\rightarrow Y$ so that given an unseen observation $x$, $h(x)$ can confidently predict the corresponding output $y$.\n","69c72655":"NOTE: As we can see dataset contain six columns: Id, SepalLengthCm, SepalWidthCm, PetalLengthCm, PetalWidthCm and Species. The actual features are described by columns 1-4. Last column contains labels of samples. Firstly we need to split data into two arrays: X (features) and y (labels).","2d2a6001":"#### 4.1. Making predictions","b69c3d12":"Let's split dataset into training set and test set, to check later on whether or not our classifier works correctly.","d399dd83":"#### 2.6 Spliting dataset into training set and test set","282224d2":"#### 2.1 Import libraries","9dabdf32":"#### 2.4 Dividing data into features and labels","080ab994":"NOTE: As we can see labels are categorical. KNeighborsClassifier does not accept string labels. We need to use LabelEncoder to transform them into numbers. ganti correspond to 0, purifikasi correspond to 1 and tunda correspond to 2.","ebeb4306":"#### 4.2. Evaluating predictions","37b69431":"STEP 1: Cgoose the number K of neighbors\n\nSTEP 2: Take the K nearest neighbors of the new data point, according to your distance metric\n\nSTEP 3: Among these K neighbors, count the number of data points to each category\n\nSTEP 4: Assign the new data point to the category where you counted the most neighbors","31819bb5":"Calculating model accuracy:","0af51d5f":"#### 2.3 Summarize the Dataset","2c3e72aa":"### 4. Using KNN for classification","4579557d":"### 5. My own KNN implementation","f7e3613b":"#### 1.1 Type of algorithm","00e6ac3c":"Lastly, because features values are in the same order of magnitude, there is no need for feature scaling. Nevertheless in other sercostamses it is extremly important to apply feature scaling before running classification algorythms.","c8fff30a":"NOTE: Iris dataset includes three iris species with 50 samples each as well as some properties about each flower. One flower species is linearly separable from the other two, but the other two are not linearly separable from each other.","f3cb18d2":"#### 1.3 Algorithm steps","be8df9d9":"#### 3.3. Pairplot","ed2210c1":"In the classification setting, the K-nearest neighbor algorithm essentially boils down to forming a majority vote between the K most similar instances to a given \u201cunseen\u201d observation. Similarity is defined according to a distance metric between two data points. The k-nearest-neighbor classifier is commonly based on the Euclidean distance between a test sample and the specified training samples. Let $x_{i}$ be an input sample with $p$ features $(x_{i1}, x_{i2},..., x_{ip})$, $n$ be the total number of input samples $(i=1,2,...,n)$.  The Euclidean distance between sample $x_{i}$ and $x_{l}$ is is defined as: \n\n\n$$d(x_{i}, x_{l}) = \\sqrt{(x_{i1} - x_{l1})^2 + (x_{i2} - x_{l2})^2 + ... + (x_{ip} - x_{lp})^2}$$\n\nSometimes other measures can be more suitable for a given setting and include the Manhattan, Chebyshev and Hamming distance.","c03210f1":"### 2. Importing and preperation of data","6775df1c":"#### 1.2 Distance measure","a87f1520":"### 3. Data Visualization","2e4704a3":"### 6. Bibliography\n\n1. MIT Lecture: https:\/\/www.youtube.com\/watch?v=09mb78oiPkA\n2. Iris dataset: https:\/\/www.kaggle.com\/uciml\/iris\n3. Theory: http:\/\/www.scholarpedia.org\/article\/K-nearest_neighbor\n\n4. https:\/\/machinelearningmastery.com\/tutorial-to-implement-k-nearest-neighbors-in-python-from-scratch\/\n5. https:\/\/kevinzakka.github.io\/2016\/07\/13\/k-nearest-neighbor\/\n6. https:\/\/www.analyticsvidhya.com\/blog\/2014\/10\/introduction-k-neighbours-algorithm-clustering\/","ef8f6896":"Pairwise is useful when you want to visualize the distribution of a variable or the relationship between multiple variables separately within subsets of your dataset."}}