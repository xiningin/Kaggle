{"cell_type":{"c4931c2a":"code","19ed2093":"code","5c17d39f":"code","5fec0f55":"code","ef45156e":"code","8b2fadfa":"code","fba5d092":"code","04ecd8e8":"code","40c57a6d":"code","27cdf589":"code","dcbcceed":"code","4eb0738c":"code","7fc5a6a0":"code","4abd7cdf":"code","f21acffb":"code","f526a13c":"code","5f5b7b22":"code","b7df58df":"code","6882cc7a":"code","37350f1c":"code","f833e8d0":"code","675e3a28":"code","f693c55f":"code","63735e1d":"code","3f8c3b18":"code","b7064659":"code","04509e32":"code","3829f8dd":"code","8c874d6f":"code","8c7fd42d":"code","96b02de6":"code","9e4d2ee6":"code","f97c7a88":"code","88cd2606":"code","7107d3f3":"code","e40d6b32":"code","2acf6a65":"code","44dfcd0c":"code","6600ee02":"code","1d5eb297":"code","a129298f":"code","97d74abe":"code","7234ed74":"code","8fc76841":"code","2cdab7b5":"code","c9dcb70c":"code","a667608b":"code","ed7fc50a":"code","522fddf8":"code","5b0754ac":"code","a32a0035":"code","d84ebdaf":"code","a02486db":"code","a1b1b2ed":"code","6292b3db":"code","d6596e84":"code","44fc02f1":"code","567285d3":"code","0dc2be14":"code","52148f9f":"code","13b3ca88":"code","1bc60511":"code","fae44894":"code","35433aab":"code","ae2a194a":"code","6e5e2e5b":"code","5160e43f":"code","cb169709":"code","72b10c4d":"code","50d08506":"code","086889a2":"code","df53124f":"code","23b8125a":"code","22984ee5":"code","bee8440b":"code","41db8c34":"code","f0e2c654":"code","f8f0459d":"code","e160380c":"code","4ecbfc86":"code","6b1dd370":"code","cf5eb6ec":"code","98d11406":"code","e255024f":"code","f5ead2bc":"code","f27097c7":"markdown","d8284859":"markdown","0d2ef436":"markdown","4e612c8c":"markdown","54f54690":"markdown","daa588e8":"markdown","8f761394":"markdown","544a5c0e":"markdown","af7f5a59":"markdown","67339468":"markdown","060fbba4":"markdown","54d1d8a2":"markdown","f346579a":"markdown","286f0c2b":"markdown","0396a59e":"markdown","8ec2d813":"markdown","3dc44ebf":"markdown","4746b54b":"markdown","d082d986":"markdown","a714445b":"markdown","054b1bb8":"markdown","9feb50a3":"markdown","cae66b73":"markdown","3cf2e221":"markdown","a74b99b4":"markdown","4db95186":"markdown","a6c1ce99":"markdown","abd2c80b":"markdown","95830ddc":"markdown","789f2429":"markdown","c7c85d6b":"markdown","63d60f0d":"markdown","ed49ffd2":"markdown","280e20c7":"markdown","8ce38cb2":"markdown","be40b367":"markdown","56d46846":"markdown","abda85e8":"markdown","53d1fdc0":"markdown","dd0407fc":"markdown","a53b1e07":"markdown","a269b169":"markdown","cb98c032":"markdown","e2962978":"markdown","132064df":"markdown","e984fb57":"markdown","77cf4e0b":"markdown","a82821e1":"markdown","b69667f5":"markdown","4fd6af59":"markdown","72423f0b":"markdown","8ef02fdf":"markdown","51e0fb3b":"markdown","8f1eeaed":"markdown","80fcd603":"markdown","764b2b69":"markdown","6cee0f8a":"markdown","22d9a3f6":"markdown","b2acc890":"markdown","5fbee3d6":"markdown","1d81af53":"markdown","67b4a90c":"markdown","6f021a64":"markdown","a296e0be":"markdown"},"source":{"c4931c2a":"# Data manipulation\nimport pandas as pd\nimport numpy as np\n\n# Visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport missingno\n\n# Preprocessing\nfrom sklearn.preprocessing import OneHotEncoder\n\n# ML algorithms\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import LinearSVC\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\n\n# Validation\nfrom sklearn.model_selection import cross_val_score\n\n#Metrics\nfrom sklearn.metrics import confusion_matrix, roc_auc_score, f1_score\n\nimport warnings\nwarnings.filterwarnings('ignore')","19ed2093":"train = pd.read_csv(\"..\/input\/titanic\/train.csv\")\ntest = pd.read_csv(\"..\/input\/titanic\/test.csv\")\ngender_submission = pd.read_csv(\"..\/input\/titanic\/gender_submission.csv\")","5c17d39f":"train.head()","5fec0f55":"test.head()","ef45156e":"gender_submission.head()","8b2fadfa":"train.describe()","fba5d092":"missingno.matrix(train, figsize = (30,10)) #really like this way of visualization of missing values","04ecd8e8":"train.isnull().sum()","40c57a6d":"print(\"% of missing values in Age column: \", round(train[\"Age\"].isnull().sum()\/len(train)*100, 2))\nprint(\"% of missing values in Cabin column: \", round(train[\"Cabin\"].isnull().sum()\/len(train)*100, 2))\nprint(\"% of missing values in Embarked column: \", round(train[\"Embarked\"].isnull().sum()\/len(train)*100, 2))","27cdf589":"df_train = train.drop([\"Age\", \"Cabin\", \"Embarked\"], axis=1)\ndf_train.head()","dcbcceed":"fig = plt.figure(figsize=(20,1))\nsns.countplot(y='Survived', data=df_train);","4eb0738c":"sns.countplot(x='Pclass', orient='v', data=df_train);","7fc5a6a0":"sns.countplot(x='Pclass', orient='v', hue=\"Survived\", data=df_train);","4abd7cdf":"print(\"% survived in first class:\", round(df_train.Survived[(df_train.Pclass == 1) & (df_train.Survived == 1)].count()\/df_train.Pclass[df_train.Pclass == 1].count()*100, 2))\nprint(\"% survived in second class:\", round(df_train.Survived[(df_train.Pclass == 2) & (df_train.Survived == 1)].count()\/df_train.Pclass[df_train.Pclass == 2].count()*100, 2))\nprint(\"% survived in first class:\", round(df_train.Survived[(df_train.Pclass == 3) & (df_train.Survived == 1)].count()\/df_train.Pclass[df_train.Pclass == 3].count()*100, 2))","f21acffb":"df_train[\"Name\"].head(20)","f526a13c":"df_train[\"Name\"].value_counts()","5f5b7b22":"df_train = df_train.drop(\"Name\", axis=1)\ndf_train","b7df58df":"sns.countplot(x=\"Sex\", hue=\"Survived\", data=df_train);","6882cc7a":"print(\"% survived in males:\", round(df_train.Survived[(df_train.Sex == \"male\") & (df_train.Survived == 1)].count()\/df_train.Sex[df_train.Sex == \"male\"].count()*100, 2))\nprint(\"% survived in females:\", round(df_train.Survived[(df_train.Sex == \"female\") & (df_train.Survived == 1)].count()\/df_train.Sex[df_train.Sex == \"female\"].count()*100, 2))","37350f1c":"df_train[\"Sex\"] = np.where(df_train[\"Sex\"] == \"female\", 1, 0)\ndf_train","f833e8d0":"df_train[\"SibSp\"].value_counts()","675e3a28":"df_train[\"Parch\"].value_counts()","f693c55f":"sns.countplot(x=\"SibSp\", hue=\"Survived\", data=df_train)","63735e1d":"sns.countplot(x=\"Parch\", hue=\"Survived\", data=df_train)","3f8c3b18":"df_train[\"Ticket\"].head(20)","b7064659":"df_train = df_train.drop(\"Ticket\", 1)\ndf_train.head()","04509e32":"df_train.Fare.describe()","3829f8dd":"first_class_mean = df_train.Fare[df_train.Pclass == 1].mean()\nsecond_class_mean = df_train.Fare[df_train.Pclass == 2].mean()\nthird_class_mean = df_train.Fare[df_train.Pclass == 3].mean()\nprint(\"Mean fare for first class ticket:\", round(first_class_mean, 2))\nprint(\"Mean fare for second class ticket:\", round(second_class_mean, 2))\nprint(\"Mean fare for third class ticket:\", round(third_class_mean, 2))","8c874d6f":"sns.catplot(x=\"Survived\", y=\"Fare\", data=df_train);","8c7fd42d":"df_train","96b02de6":"df_train = df_train.drop(\"PassengerId\", 1)\ndf_train","9e4d2ee6":"one_hot_cols = [\"Pclass\", \"SibSp\", \"Parch\"]\nenc = OneHotEncoder(handle_unknown='ignore', sparse=False)\nOH_cols = pd.DataFrame(enc.fit_transform(df_train[one_hot_cols]))\nOH_cols.columns = enc.get_feature_names(one_hot_cols)\n\ndf_train_enc = df_train.drop(one_hot_cols, 1)\ndf_train_enc = pd.concat([df_train_enc, OH_cols], 1)\ndf_train_enc","f97c7a88":"X_train = df_train_enc.drop(\"Survived\", 1)\ny_train = df_train_enc[\"Survived\"]","88cd2606":"print(X_train.shape)\nprint(y_train.shape)","7107d3f3":"def test_algorithm(model):\n    scores = cross_val_score(model, X_train, y_train, cv=5)\n    print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n    return scores.mean()\n\nacc_scores = {}","e40d6b32":"logreg = LogisticRegression()\nlogreg_acc = test_algorithm(logreg)\nacc_scores[\"Logistic Regression\"] = logreg_acc","2acf6a65":"knn = KNeighborsClassifier()\nknn_acc = test_algorithm(knn)\nacc_scores[\"KNN\"] = knn_acc","44dfcd0c":"gnb = GaussianNB()\ngnb_acc = test_algorithm(gnb)\nacc_scores[\"Gaussian Naive Bayes\"] = gnb_acc","6600ee02":"svc = LinearSVC()\nsvc_acc = test_algorithm(svc)\nacc_scores[\"SVC\"] = svc_acc","1d5eb297":"sgd = SGDClassifier()\nsgd_acc = test_algorithm(sgd)\nacc_scores[\"Stochastic Gradient Descent\"] = sgd_acc","a129298f":"decision_tree = DecisionTreeClassifier()\ndecision_tree_acc = test_algorithm(decision_tree)\nacc_scores[\"Decision Tree\"] = decision_tree_acc","97d74abe":"gbc = GradientBoostingClassifier()\ngbc_acc = test_algorithm(gbc)\nacc_scores[\"Gradient Boosting\"] = gbc_acc","7234ed74":"acc_scores_df = pd.DataFrame(acc_scores.values(), index=acc_scores.keys())\nacc_scores_df.columns = [\"Score\"]\nacc_scores_df.sort_values(by=\"Score\", ascending=False)","8fc76841":"df_test = test\ndf_test = df_test.drop([\"Name\", \"Age\", \"Ticket\", \"Cabin\", \"Embarked\"], 1)\ndf_test = df_test.fillna(df_test[\"Fare\"].mean()) #we have one missing value in Fare columns, let's fill it with mean from all values in column\n\ndf_test[\"Sex\"] = np.where(df_test[\"Sex\"] == \"female\", 1, 0)\n\nOH_cols = pd.DataFrame(enc.transform(df_test[one_hot_cols]))\nOH_cols.columns = enc.get_feature_names(one_hot_cols)\n\ndf_test_enc = df_test.drop(one_hot_cols, 1)\ndf_test_enc = pd.concat([df_test_enc, OH_cols], 1)\ndf_test_enc","2cdab7b5":"X_test = df_test_enc.drop(\"PassengerId\", 1)\ngbc.fit(X_train, y_train)\ny_test = gbc.predict(X_test)\ny_test","c9dcb70c":"df_submission = pd.DataFrame()\ndf_submission[\"PassengerId\"] = df_test[\"PassengerId\"]\ndf_submission[\"Survived\"] = y_test\ndf_submission.head()","a667608b":"df_submission.to_csv(\".\/submission.csv\", index=False)","ed7fc50a":"train[train[\"Age\"].isnull()]","522fddf8":"train.Age.describe()","5b0754ac":"train.Age.dropna().sort_values().head(20)","a32a0035":"bins = [0, 10, 20, 30, 40, 50, 60, 70, 80]\nbuckets = pd.cut(train['Age'].dropna(), bins=bins)\nbuckets","d84ebdaf":"sns.countplot(x=pd.cut(train['Age'].dropna(), bins=bins), orient='v', hue=\"Survived\", data=train.drop([\"Embarked\",\"Cabin\"], 1).dropna());","a02486db":"nan_age = train[train[\"Age\"].isna()]\nsns.countplot(y=\"Survived\", data=nan_age);","a1b1b2ed":"new_train = pd.DataFrame()\nnew_train[\"Survived\"] = train[\"Survived\"]\nbins = [0, 10, 20, 30, 40, 50, 60, 70, 80]\nage_bins = pd.cut(train['Age'].fillna(train['Age'].mean()), bins=bins)\nnew_train[\"Age\"] = age_bins\nnew_train","6292b3db":"train[train[\"Embarked\"].isnull()]","d6596e84":"train[\"Embarked\"].value_counts()","44fc02f1":"new_train[\"Embarked\"] = train[\"Embarked\"].fillna(\"S\")\nnew_train","567285d3":"print(\"Number of missing values:\", train[\"Cabin\"].isnull().sum())\ntrain[\"Cabin\"].value_counts().head(50)","0dc2be14":"print(train[~train.Cabin.isnull()].Pclass.value_counts())\nprint(train.Pclass.value_counts())","52148f9f":"cabin_df = pd.DataFrame()\ncabin_df = train[[\"Survived\", \"Cabin\"]].dropna()\ncabin_df[\"Cabin_letter\"] = cabin_df[\"Cabin\"].str[0]\ncabin_df","13b3ca88":"sns.countplot(x=\"Cabin_letter\", hue=\"Survived\", data=cabin_df)","1bc60511":"new_train[\"Cabin_letter\"] = train[\"Cabin\"].fillna(\"None\").str[0]\nnew_train","fae44894":"train[\"Name\"].head(50)","35433aab":"split_names = train[\"Name\"].str.split('.')\ntitles = [x[0].split(',')[1].strip() for x in split_names]\ntitles_df = pd.DataFrame()\ntitles_df[\"Title\"] = titles\ntitles_df[\"Survived\"] = train[\"Survived\"]\nsns.countplot(x=\"Title\", hue=\"Survived\", data=titles_df)","ae2a194a":"titles_df[\"Title\"].value_counts()","6e5e2e5b":"train[\"Ticket\"]","5160e43f":"train[\"Ticket\"].value_counts().head(50)","cb169709":"split_tickets = train[\"Ticket\"].str.split(' ')\nsplit_tickets = [x[1] if len(x) > 1 else x[0] for x in split_tickets]\nsplit_tickets","72b10c4d":"tickets_df = pd.DataFrame()\ntickets_df[\"Survived\"] = train[\"Survived\"]\ntickets_df[\"Ticket\"] = split_tickets\ntickets_df","50d08506":"tickets_df_filtered = tickets_df.drop(tickets_df[(tickets_df[\"Ticket\"] == \"2.\") | (tickets_df[\"Ticket\"] == 'LINE')].index)","086889a2":"tickets_df_filtered[\"Ticket\"] = pd.to_numeric(tickets_df_filtered[\"Ticket\"], errors='coerce')","df53124f":"bins = [0, 5000, 10000, 50000, 100000, 200000]\ntickets_df_filtered[\"Ticket\"] = tickets_df_filtered[\"Ticket\"].dropna().astype('float')\n#tickets_bins = pd.cut(tickets_df_filtered[\"Ticket\"], bins=bins)\ntickets_bins = pd.qcut(tickets_df_filtered[\"Ticket\"], q=5) #qcut divide data into equal bins\ntickets_bins ","23b8125a":"sns.countplot(x=tickets_bins, hue=\"Survived\", data=tickets_df_filtered)","22984ee5":"new_train[\"Ticket_number\"] = tickets_df[\"Ticket\"]\nnew_train.loc[(new_train[\"Ticket_number\"] == \"2.\") | (new_train[\"Ticket_number\"] == 'LINE'), \"Ticket_number\"] = np.NaN\nnew_train[\"Ticket_number\"]  = pd.to_numeric(new_train[\"Ticket_number\"], errors='coerce')\nnew_train[\"Ticket_number\"] = new_train[\"Ticket_number\"].fillna(method=\"backfill\")\ntickets_df_filtered[\"Ticket\"] = tickets_df_filtered[\"Ticket\"].dropna().astype('float')\nnew_train[\"Ticket_number\"] = pd.qcut(new_train[\"Ticket_number\"], q=5)\n\n#new_train[\"Ticket_number\"] = new_train[\"Ticket_number\"].astype(\"string\")\n#new_train[\"Ticket_number\"].fillna(\"(28573.6, 237647.4]\")\n#new_train[new_train[\"Ticket_number\"].isnull()]\n#new_train[\"Ticket_number\"] = new_train[\"Ticket_number\"].astype(\"category\")\n","bee8440b":"new_train","41db8c34":"new_train[[\"Pclass\", \"Sex\", \"SibSp\", \"Parch\", \"Fare\"]] = train[[\"Pclass\", \"Sex\", \"SibSp\", \"Parch\", \"Fare\"]]\nnew_train","f0e2c654":"new_train[\"SibSpParch\"] = np.where(new_train[\"SibSp\"] + new_train[\"Parch\"] > 0, 1, 0)\nnew_train","f8f0459d":"new_train = new_train.drop([\"SibSp\", \"Parch\"], 1)\nnew_train","e160380c":"one_hot_cols = [\"Pclass\", \"Age\", \"Embarked\", \"Cabin_letter\", \"Ticket_number\", \"Sex\"]\nenc = OneHotEncoder(handle_unknown='ignore', sparse=False)\nOH_cols = pd.DataFrame(enc.fit_transform(new_train[one_hot_cols]))\nOH_cols.columns = enc.get_feature_names(one_hot_cols)\n\nnew_train_enc = new_train.drop(one_hot_cols, 1)\nnew_train_enc = pd.concat([new_train_enc, OH_cols], 1)\nnew_train_enc","4ecbfc86":"def preprocessing(df, ticket_bins=[], enc=None):\n    new_df = pd.DataFrame()\n    age_bins = [0, 10, 20, 30, 40, 50, 60, 70, 80]\n    new_df[\"Age\"] = pd.cut(df['Age'].fillna(df['Age'].mean()), bins=age_bins)\n    new_df[\"Embarked\"] = df[\"Embarked\"].fillna(\"S\")\n    new_df[\"Cabin_letter\"] = df[\"Cabin\"].fillna(\"None\").str[0]\n    \n    #tickets\n    split_tickets = df[\"Ticket\"].str.split(' ')\n    split_tickets = [x[1] if len(x) > 1 else x[0] for x in split_tickets]\n    new_df[\"Ticket_number\"] = split_tickets\n    new_df.loc[(new_df[\"Ticket_number\"] == '2') | (new_df[\"Ticket_number\"] == \"2.\") | (new_df[\"Ticket_number\"] == 'LINE'), \"Ticket_number\"] = np.NaN\n\n    new_df[\"Ticket_number\"]  = pd.to_numeric(new_df[\"Ticket_number\"], errors='coerce')\n    \n    new_df[\"Ticket_number\"] = new_df[\"Ticket_number\"].fillna(method=\"bfill\")\n    \n    if len(ticket_bins) < 1:\n        new_df[\"Ticket_number\"], ticket_bins = pd.qcut(new_df[\"Ticket_number\"], q=5, retbins=True)\n    else:\n        new_df[\"Ticket_number\"] = pd.cut(new_df[\"Ticket_number\"], bins = ticket_bins)\n    \n    \n    new_df[[\"Pclass\", \"Sex\", \"SibSp\", \"Parch\", \"Fare\"]] = df[[\"Pclass\", \"Sex\", \"SibSp\", \"Parch\", \"Fare\"]]\n    new_df[\"SibSpParch\"] = np.where(df[\"SibSp\"] + df[\"Parch\"] > 0, 1, 0)\n    new_df = new_df.drop([\"SibSp\", \"Parch\"], 1)\n    new_df[\"Fare\"] = new_df[\"Fare\"].fillna(method=\"bfill\")\n    \n    one_hot_cols = [\"Pclass\", \"Age\", \"Embarked\", \"Cabin_letter\", \"Ticket_number\", \"Sex\"]\n    if enc == None:\n        enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n        OH_cols = pd.DataFrame(enc.fit_transform(new_df[one_hot_cols]))\n        \n    else:\n        OH_cols = pd.DataFrame(enc.transform(new_df[one_hot_cols]))\n    OH_cols.columns = enc.get_feature_names(one_hot_cols)\n\n    new_df_enc = new_df.drop(one_hot_cols, 1)\n    new_df_enc = pd.concat([new_df_enc, OH_cols], 1)\n    \n    return new_df_enc, ticket_bins, enc","6b1dd370":"train_by_function, ticket_bins, enc = preprocessing(train)\nif train_by_function.shape == new_train_enc.drop(\"Survived\", 1).shape:\n    print(\"Both dataframes have same shape\")\nelse:\n    print(\"Something wrong with preprocessing function\")","cf5eb6ec":"models = logreg, knn, gnb, svc, sgd, decision_tree, gbc\n\ndef new_test_algorithm(model, X_train, y_train):\n    accuracies = {}\n    trained_models = {}\n    for model in models:\n        scores = cross_val_score(model, X_train, y_train, cv=5)\n        print(model, \"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n        accuracies[model] = scores.mean()\n    return accuracies\n\n\naccs = new_test_algorithm(gbc, train_by_function, train[\"Survived\"])","98d11406":"gbc.fit(train_by_function, train[\"Survived\"])\nX_test, _, _ = preprocessing(test, ticket_bins, enc)\nX_test.isnull().sum()\ny_test = gbc.predict(X_test)\ndf_submission = pd.DataFrame()\ndf_submission[\"PassengerId\"] = test[\"PassengerId\"]\ndf_submission[\"Survived\"] = y_test\ndf_submission.to_csv(\".\/submission_2.csv\", index=False)","e255024f":"matrix = confusion_matrix(gender_submission.Survived, y_test)\nprint(matrix)","f5ead2bc":"print(\"People who didn't survived and predicted didn't survived:\", matrix[0][0])\nprint(\"People who survived and predicted didn't survived:\", matrix[0][1])\nprint(\"People who didn't survived and predicted survived:\", matrix[1][0])\nprint(\"People who survived and predicted survived:\", matrix[1][1])","f27097c7":"We didn't return our model earlier, so now we have train it again. We'll fix this neglect later.","d8284859":"Ok, I see now that we don't need PassengerId. Just drop it.","0d2ef436":"# Sumbission","4e612c8c":"Now we only need to make our submission.\n\nThis time we reproduce our code that edit our test dataframe, but in next steps we'll create pipelines, which allows us to avoid re-typing.","54f54690":"Gaussian Naive Bayes","daa588e8":"Try our algorithms.","8f761394":"We have now 5 features columns and our label column Survived. All values are numerical, which is good. But we should check that numbers are nominal or ordinal.\n\nGenerally, difference is that nominal values we can't compare. E.g. if we have category of cars, were 0 is red, and 1 is black, we can't say, that black is more than red. But we can say, that 2 liters engine capacity is more than 1.5 - that's ordinal value.\n\nIn our case all values expected Fare are nominal.","544a5c0e":"Quite good distribution for us (not for passengers, unfortunately). Nothing to do here for now, we can focus on features. First Pclass, which is ticket class.","af7f5a59":"I already did some more advanced work in data science, but what a data scientist would I be if I hadn't make submission in Titanic Challange? So let's get started!","67339468":"## Encoding","060fbba4":"We have to put much effort to make something useful from it. We'll do it later, for now just drop this column, because we just want create baseline predict model.","54d1d8a2":"# Bulding models","f346579a":"Great! Now save our submission dataframe.","286f0c2b":"# Train model","0396a59e":"We are done!\n\nAlmost, to be precise. Still are many things we could do better. But our first submission is ready. Below is more advanced stuff, which I think may improve our score. \n\nFirst look again on columns we drop because of missing values.","8ec2d813":"Linear Support Vector Machines","3dc44ebf":"So what else we can do? I think since people who have sibling\/spouse or\/and parents\/children on board, generally have more chances to survive, we'll produce new column based on SibSp and Parch, where we define if passenger had somebody related with him on ship.","4746b54b":"We have three files. *gender_sumbission* is only for check our prediction model results on *test* data. We don't need to bother about it for now. Let's check *train* data instead.","d082d986":"Ok, we have our predictions. Let's create submission dataframe. As we saw earlier, we need columns PassengerId and Survived.","a714445b":"As we could suppose, first class tickets were more expensive than others. On plot we can see that we have probably two extreme values, and generally higher fare = higher chances to survive. Maybe we could make more conlusions with that column, but leave it, at least for now.","054b1bb8":"So far so good.\n\nCheck if we could do something with Name column.","9feb50a3":"Everything looks fine, so we can drop columns SibSp and Parch.","cae66b73":"We encode this later, with other values and create pipeline.\n\nLet's see Embarked column now. This column defines port of embarkation and C = Cherbourg, Q = Queenstown, S = Southampton.","3cf2e221":"Most passengers have traveled alone, but who has been with family or spouse (especially one or two) had much better chances to survive. We could have assumptions about it (parents rescue their childrens, men rescue their wifes and fiancees), but leave this column at it is for now.\n\nCheck Ticket column.","a74b99b4":"Not everyone, and unexpectedly some passengers in second and third class have cabins also. So let's check how many pasengers with cabins survived, depend on cabin letter.","4db95186":"As we see, we have some missing values (quite a lot, if you look at Cabin column). We'll face them later, for now and creating first and baseline model just remove them from dataset. I remove all columns, but probably for Embarked it would be enough to drop only two rows that have missing values.","a6c1ce99":"We knowe already, that film \"Titanic\" has right in Rose-Jack situation about they tickets. And how about their gender?","abd2c80b":"Decision Tree Classifier","95830ddc":"Since we have to predict if passenger survived or not, let's look closely on Survived column.","789f2429":"As we may expect (since we know that many of first class passengers survived and most of them have own cabin) more passengers with cabins survived than not. But we have some disproportion between A, F, G and E, D, B cabins. It could help us. Let's add this column with fill our NaNs as \"N\" cabin.","c7c85d6b":"# Submission","63d60f0d":"# EDA (Early Data Analysis)","ed49ffd2":"Now we need encode our caterogical values to numbers.","280e20c7":"So what the heck is that? Easy - we got 245 people, who we predict that didn't survive and actually they didn't. But 21 that we predict dead already survived. 27 who w predict as \"survived\" didn't, and 152 we corectly predicted live after disaster.","8ce38cb2":"We return ticket_bins (with retbins=True in pd.qcut function) and encoder, because we will need it in test data preprocessing.\n\nCheck if our function work properly.","be40b367":"We saved all scores to acc_scores dictiorany. Check which model have best accuracy. ","56d46846":"Not the best situation for us, guys. Again - we want to keep this column for sure. But for our machine learning models we need to change categories to numerical.","abda85e8":"We should encode our ordinal to 0 and 1. Let's use OneHotEncoder.","53d1fdc0":"Great, we've looked again at all columns we droped earlier and we handle with all of them except Name.\n\nLet's add to our dataframe columns, which we've already analyzed.","dd0407fc":"# More advanced work with columns","a53b1e07":"Once more, we need dive deeply in, and we don't want now. Drop it and check last column.","a269b169":"Gradient Boost Trees","cb98c032":"Probably nothing we can do here. Mr and Miss\/Mrs titles will give us same results as Sex column. Master title have quite good survive %, but we don't add this column to our train.\n\nWe also drop Ticket, chceck this column.","e2962978":"# Look cosely on each column","132064df":"Now time for column, which have the most missing values and probably be the toughest one. Cabin means just cabin number.","e984fb57":"As we suspected - near 2\/3 first class passengers survived, while only 1\/4 third class passengers. Definitely we should leave this columns in our dataframe.\n\nCheck Name column now.","77cf4e0b":"So we have only two missing values. I think we could just remove this rows and it wouldn't influence on our prediction, but better just fill it with \"S\", because in Southampton port most passengers start their journey.","a82821e1":"I supossed, that maybe children's age wasn't write, but I was wrong. \n\nWe could fill missing values with mean, but let's first check % of survives in each age category.","b69667f5":"So after some imports take a look what we have in our data.","4fd6af59":"K-Nearest Neighbours","72423f0b":"I was afraid that fill Age column with mean could affect on our predictions, but it shouldn't since Survived\/Not-survived ratio is similar for passengers with missing age nad passengers in age between 20 and 30 (mean is 29.39)\n\nHere is tough decision (at least for me) to make. We have to decide, what we do with our Age column. Options for me:\n\n1. We fill empty values with mean and treat Age as numerical value.\n2. We put Age into bins (as you see above), mark missing values as (20,30] and treat Age as caterogical value.\n\nFor now, let's try second option.","8ef02fdf":"# Imports","51e0fb3b":"Again Gradient Boosting Classifier have best accuracy score. Make submission with this model and then check if it's *really* the best.","8f1eeaed":"To prevent re-typing very similar code let's create function for testing algorithms. It's also help if we would like to expand our job with algorithms.","80fcd603":"## Find missing values","764b2b69":"That's something! We get only numbers from Ticket column (drop some strange values that was produced during spliting), then divide them up to equal bins. And now we can say, that if you have ticket number below 11483 or higher than 237647 risk than you didn't survive Titanic disaster increases significantly.\n\nI wanted just add our strange values (\"2.\", \"LINE\", and some strings) to middle bin (as you can see in commented code), instead of I change these values to NaN and then fill them with backfill method. It means that we use next valid information to fill gap.","6cee0f8a":"We already checked our models with accuracy score, which is quite good if you are sure that your model work properly. But imagine some situation, that you have in your test data 990 who survived and only 10 who didn't survive. If your model didn't do well, it could give in predictions all 1000 passengers as \"not survived\" and still have 100% of accuracy! That's ridiculous. To avoid that, we can use different evaluation metrics. Most common in classification problem are Confusion Matrix, ROC AUC and F1 Score. Let's check Confusion Matrix.","22d9a3f6":"Stochastic Gradient Descent","b2acc890":"Now we're ready to start building our machine learning models! Number of samples is low, so we can check many models with different algorithms.\n\nBut first we have to separate our label and features. ","5fbee3d6":"It's interesting, that there was more first class tickets, than second class.\nBut as we know for Titanic film, you had much higher probability to survive, when you had first class ticket (poor Jack). Is our data confirm that?","1d81af53":"Logistic Regression","67b4a90c":"We could train our model now, but first let's think about our test data. If we would like to check our score on test data we need to make same operation as on train data, so rewrite all the code, that transform data. How avoid that? We can make function to preprocessing or make pipeline. For now try function.","6f021a64":"687 missing values means a lot. Most passengers have own cabins, but there may be some pattern in letters in cabins numbers. First check if everyone in first class have cabin.","a296e0be":"Two next columns looks quite similiar. SibSp is number of siblings \/ spouses aboard the Titanic and Parch is number. of parents \/ children aboard the Titanic. Let's look closer at it."}}