{"cell_type":{"1589f64e":"code","36827e5b":"code","41696f32":"code","f25740f5":"code","3dc9eab6":"code","68798c29":"code","3843beff":"code","a9647135":"code","2ae99852":"code","1d1b4954":"code","98a4f767":"code","9543c543":"code","a8349faf":"code","a0a516a7":"code","69a3fd9d":"code","9306eaec":"code","23d72296":"code","03ba28a3":"code","bcd2baa2":"code","7eb71844":"code","d38e7afb":"markdown","e72721e5":"markdown","9bd553e1":"markdown","c3b0afee":"markdown","2373f2fb":"markdown","1ced3844":"markdown","5854fceb":"markdown","cceb64ae":"markdown","256863cc":"markdown"},"source":{"1589f64e":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","36827e5b":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn import ensemble, tree, linear_model\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.metrics import r2_score, mean_squared_error\nfrom sklearn.utils import shuffle\n\n%matplotlib inline\nimport warnings\nwarnings.filterwarnings('ignore')","41696f32":"dir = \"\/kaggle\/input\/house-prices-advanced-regression-techniques\/\"\ntrain = pd.read_csv(dir+'train.csv')\ntest = pd.read_csv(dir+'test.csv')\ny = np.log1p(train[\"SalePrice\"])\ntest_ID = test['Id']","f25740f5":"all_data = pd.concat((train, test)).reset_index(drop=True)\nall_data.drop(['Id'], axis=1, inplace=True)\nall_data.drop(['SalePrice'], axis=1, inplace=True)\n\nfor col in [\"PoolQC\", \"MiscFeature\", \"Alley\", \"Fence\", \"FireplaceQu\",\n            'GarageType', 'GarageFinish', 'GarageQual', 'GarageCond',\n           'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2',\n           \"MasVnrType\", \"MSSubClass\"]:\n    all_data[col] = all_data[col].fillna('None')\n\nall_data[\"Functional\"] = all_data[\"Functional\"].fillna(\"Typ\")\nfor col in ['GarageYrBlt', 'GarageArea', 'GarageCars',\n            'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath',\n           \"MasVnrArea\"]:\n    all_data[col] = all_data[col].fillna(0)\n    \nall_data[\"LotFrontage\"] = all_data.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(\n    lambda x: x.fillna(x.median()))\n\nfor col in [\"MSZoning\", \"Electrical\", \"KitchenQual\", \"Exterior1st\", \"Exterior2nd\", \"SaleType\"]:\n    all_data[col] = all_data[col].fillna(all_data[col].mode()[0])\n\nall_data = all_data.drop(['Utilities'], axis=1)\n\nntrain = train.shape[0]\nntest = test.shape[0]","3dc9eab6":"#Check remaining missing values if any \nall_data_na = (all_data.isnull().sum() \/ len(all_data)) * 100\nall_data_na = all_data_na.drop(all_data_na[all_data_na == 0].index).sort_values(ascending=False)\nprint(\"NA column:\", len(all_data_na))","68798c29":"all_data['MSSubClass'] = all_data['MSSubClass'].apply(str)\n# all_data['OverallCond'] = all_data['OverallCond'].astype(str)\n\nall_data['YrSold'] = all_data['YrSold'].astype(str)\nall_data['MoSold'] = all_data['MoSold'].astype(str)","3843beff":"categorical_features = all_data.select_dtypes(include = [\"object\"]).columns\nnumerical_features = all_data.select_dtypes(exclude = [\"object\"]).columns\ntrain_num = all_data[numerical_features]\ntrain_cat = all_data[categorical_features]\n\nprint(\"Numerical features : \" + str(len(numerical_features)))\nprint(\"Categorical features : \" + str(len(categorical_features)))","a9647135":"# for c in categorical_features:\n#     print(c, all_data[c].unique())\nfrom sklearn import preprocessing\nordinals = [\"MoSold\", \"YrSold\"]\nfor c in ordinals:\n    labels = all_data[c].unique()\n    labels.sort()\n    \n    le = preprocessing.LabelEncoder()\n    le.fit(labels)\n    all_data[c] = le.transform(all_data[c]) \n    \nqc = {\n    \"ExterQual\": [\"Ex\", \"Gd\", \"TA\", \"Fa\", \"Po\"], \n    \"ExterCond\": [\"Ex\", \"Gd\", \"TA\", \"Fa\", \"Po\"], \n    \"BsmtQual\": [\"Ex\", \"Gd\", \"TA\", \"Fa\", \"Po\", \"None\"], \n    \"BsmtCond\": [\"Ex\", \"Gd\", \"TA\", \"Fa\", \"Po\", \"None\"],\n    \"BsmtExposure\" : [\"Gd\", \"Av\", \"Mn\", \"No\", \"None\"],\n    \"BsmtFinType1\": [\"GLQ\", \"ALQ\", \"BLQ\", \"Rec\", \"LwQ\", \"Unf\", \"None\"],\n    \"BsmtFinType2\": [\"GLQ\", \"ALQ\", \"BLQ\", \"Rec\", \"LwQ\", \"Unf\", \"None\"],\n    \"HeatingQC\": [\"Ex\", \"Gd\", \"TA\", \"Fa\", \"Po\"], \n    \"KitchenQual\": [\"Ex\", \"Gd\", \"TA\", \"Fa\", \"Po\"], \n    \"FireplaceQu\": [\"Ex\", \"Gd\", \"TA\", \"Fa\", \"Po\", \"None\"],\n    \"GarageQual\": [\"Ex\", \"Gd\", \"TA\", \"Fa\", \"Po\", \"None\"],\n    \"GarageCond\": [\"Ex\", \"Gd\", \"TA\", \"Fa\", \"Po\", \"None\"],\n    \"PoolQC\": [\"Ex\", \"Gd\", \"TA\", \"Fa\", \"None\"], \n}\n\nfor c in qc:\n    labels = qc[c]\n    \n    le = preprocessing.LabelEncoder()\n    le.fit(labels)\n    all_data[c] = le.transform(all_data[c]) \n    \nall_data.shape","2ae99852":"# One-hot encoding for others\nall_data = pd.get_dummies(all_data)\nall_data.shape","1d1b4954":"train = all_data[:ntrain]\ntest = all_data[ntrain:]","98a4f767":"# train = pd.concat([train_cat,train_num],axis=1)\n# X_train,X_test,y_train,y_test = train_test_split(train,y,test_size = 0.3,random_state= 0)\n\n# train.shape, X_train.shape, X_test.shape","9543c543":"from sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC\nfrom sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split\nfrom sklearn.metrics import mean_squared_error\nimport xgboost as xgb\nimport lightgbm as lgb","a8349faf":"n_folds = 5\ndef rmsle_cv(model, X, y):\n    kf = KFold(n_folds, shuffle=True, random_state=0).get_n_splits(train.values)\n    rmse= np.sqrt(-cross_val_score(model, X, y, scoring=\"neg_mean_squared_error\", cv = kf))\n    return(rmse)","a0a516a7":"lasso = make_pipeline(RobustScaler(), Lasso(alpha =0.0005, random_state=1))\nENet = make_pipeline(RobustScaler(), ElasticNet(alpha=0.0005, l1_ratio=.9, random_state=3))\nKRR = KernelRidge(alpha=0.6, kernel='polynomial', degree=2, coef0=2.5)\nGBoost = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05,\n                                   max_depth=4, max_features='sqrt',\n                                   min_samples_leaf=15, min_samples_split=10, \n                                   loss='huber', random_state =5)\nXgb = xgb.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, \n                             learning_rate=0.05, max_depth=3, \n                             min_child_weight=1.7817, n_estimators=2200,\n                             reg_alpha=0.4640, reg_lambda=0.8571,\n                             subsample=0.5213, silent=1,\n                             random_state =7, nthread = -1)\nLgb = lgb.LGBMRegressor(objective='regression',num_leaves=5,\n                              learning_rate=0.05, n_estimators=720,\n                              max_bin = 55, bagging_fraction = 0.8,\n                              bagging_freq = 5, feature_fraction = 0.2319,\n                              feature_fraction_seed=9, bagging_seed=9,\n                              min_data_in_leaf =6, min_sum_hessian_in_leaf = 11)","69a3fd9d":"modelDesc = {\n    \"lasso\": {\"name\": \"Lasso\", \"obj\": lasso},\n    \"ENet\": {\"name\": \"ENet\", \"obj\": ENet},\n    \"KRR\": {\"name\": \"KRR\", \"obj\": KRR},\n    \"GBoost\": {\"name\": \"GBoost\", \"obj\": GBoost},\n    \"Xgb\": {\"name\": \"Xgb\", \"obj\": Xgb},\n    \"Lgb\": {\"name\": \"Lgb\", \"obj\": Lgb},\n}","9306eaec":"# Weighted Average model\nWAmodel = {}\nfor m in modelDesc:\n    obj = modelDesc[m][\"obj\"]\n    obj.fit(train.values, y)\n    \n    scores = rmsle_cv(obj, train.values, y)\n    print(\"Averaged score: {} {:.4f} ({:.4f})\\n\".format(modelDesc[m][\"name\"], scores.mean(), scores.std()))\n    \n    WAmodel[m] = (obj, scores.mean())","23d72296":"from sklearn.metrics import mean_squared_error\n\ndef weightAveragePredict(models, X):\n    weights = [1.0\/WAmodel[m][1] for m in WAmodel]\n    weights = np.array(weights)\n    weights = weights\/weights.sum()\n    \n    yhat = None\n    ind = 0\n    for m in WAmodel:\n        obj = WAmodel[m][0]\n        p = obj.predict(X)\n        p = weights[ind]*p\n        ind += 1\n        \n        if yhat is None:\n            yhat = p\n        else:\n            yhat = yhat + p\n        \n    return yhat\n    \ny_pred = weightAveragePredict(WAmodel, train.values)\nscore = np.sqrt(mean_squared_error(y, y_pred))\nprint(score)\n\n","03ba28a3":"from sklearn.linear_model import LinearRegression\n\ndef predictBaseModels(models, X):\n    yhat = []\n    ind = 0\n    for m in WAmodel:\n        obj = WAmodel[m][0]\n        p = obj.predict(X)\n        yhat.append(p)\n    \n    yhat = np.array(yhat)\n    yhat = np.transpose(yhat)\n    return yhat\n\ndef trainMetaModel(yhat, y):\n    reg = LinearRegression().fit(yhat, y)\n    return reg\n\nyhat = predictBaseModels(WAmodel, train.values)\nmeta = trainMetaModel(yhat, y)\n\ny_pred = meta.predict(yhat)\nscore = np.sqrt(mean_squared_error(y, y_pred))\nprint(score)","bcd2baa2":"# y_pred = weightAveragePredict(WAmodel, test.values)\n\n_y_pred = predictBaseModels(WAmodel, test.values)\ny_pred = meta.predict(_y_pred)\n\nexp_y_pred = np.expm1(y_pred)","7eb71844":"sub = pd.DataFrame()\nsub['Id'] = test_ID\nsub['SalePrice'] = exp_y_pred\nsub.to_csv('submission.csv',index=False)","d38e7afb":"## Categorical Encoding","e72721e5":"# Stacked Models","9bd553e1":"# Predicting","c3b0afee":"# Modelling","2373f2fb":"## Casting type","1ced3844":"# Base Models","5854fceb":"# Feature Engineering","cceb64ae":"## Filling NA","256863cc":"Code [from this notebook](https:\/\/www.kaggle.com\/serigne\/stacked-regressions-top-4-on-leaderboard)"}}