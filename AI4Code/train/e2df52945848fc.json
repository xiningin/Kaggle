{"cell_type":{"7827ea3c":"code","9e66219b":"code","490e5a1e":"code","6abbf99f":"code","79912d8e":"code","dcb8b6fa":"code","aa667175":"code","69a45c1b":"code","a41fc212":"code","9c254a36":"code","5af666c9":"code","a629e11a":"code","28b7557f":"code","d655a5d7":"code","2c6c5b81":"code","f8441864":"code","7aefa957":"code","251480a5":"code","527cde61":"code","c3741756":"markdown","e2a9303f":"markdown","5d315402":"markdown","a5e71ac9":"markdown","75ce38d8":"markdown","d7777c9b":"markdown","4f588bf8":"markdown","ac1a3f83":"markdown","897dfad3":"markdown","6ed143e1":"markdown","a14823a2":"markdown","484858a2":"markdown","a3e9084a":"markdown","f1d8b503":"markdown","e6e4dce9":"markdown","0a735673":"markdown"},"source":{"7827ea3c":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\ndat=[]\nimport os\n# load emotion text\nfor page in range(3):\n    dat.append(pd.read_excel('..\/input\/emotion-text\/emotion_text.xlsx',sheet_name = page))\n               \nDat = pd.concat(dat)\n\nDat # to show","9e66219b":"tag,tags = [],[]\nfor txt in Dat[['\u6807\u7b7e']].values.tolist():\n    tags.extend(txt[0].replace('\u7684','').split('\uff0c'))\n    tag.append(txt[0].replace('\u7684','').split('\uff0c'))\n    \ntag_dict = {t:i for i,t in enumerate(list(set(tags)))}\n\n# to get tag vectors for each sentence, thus , it's a matrix\n\ntag_mat = np.zeros((len(Dat),len(tag_dict)),dtype='int8')# init tag_vec to save 0\/1 matrix\nfor i,t in enumerate(tag):\n    for ti in t:\n        tag_mat[i,tag_dict[ti]]=1\ntag_mat=np.array(tag_mat)","490e5a1e":"# these tags only appear ones !! no more information\n\n[list(tag_dict.keys())[i] for i,k in enumerate(np.array(tag_mat).sum(0)) if k<=1]","6abbf99f":"# glance the corelation among labels\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nplt.figure(figsize=(6,5))\ncmap = sns.cubehelix_palette(start = 1.5, rot = 3, gamma=0.8, as_cmap = True)\npt = pd.DataFrame(np.array(tag_mat)).corr()   \nsns.heatmap(pt, linewidths = 0.05, vmax=1, vmin=-1,cmap=\"RdBu\")\nplt.show()","79912d8e":"# tag is uncool for classification, thus, we need enbalance it quantity later\n{list(tag_dict.keys())[i]:k for i,k in enumerate(np.array(tag_mat).sum(0)) if k>0}","dcb8b6fa":"# load stopword\nwith open('..\/input\/stopwordch\/stopwordCh.txt','r',encoding='utf8') as f:\n    stopword = [w.strip() for w in f.readlines()]\n    stopword.append(' ')","aa667175":"import jieba\njieba.load_userdict(['..\/input\/emotion-word\/anger.txt','..\/input\/emotion-word\/neg.txt','..\/input\/emotion-word\/pos.txt'])\ns_lst = Dat[['\u5185\u5bb9']].values.tolist()# load sentence list\n\ns_cut=[]# save sentence word cut\nfor s in s_lst:\n    lst = jieba.cut(s[0])\n    s_cut.append([w for w in list(lst) if w not in stopword+['\\n','\\xa0']])","69a45c1b":"# for strengthen the sentence\n!pip install synonyms","a41fc212":"import synonyms,random\n# shuffle sample \n\nsample = list(zip(s_cut,tag_mat))\nrandom.shuffle(sample)\ns_cut,tag_mat = tuple(zip(*sample))\n\n# divide sample into training set and test set at rate 0.8:0.2\n\ns_cut_train = s_cut[:int(len(s_cut)*0.8)]\ns_cut_test = s_cut[int(len(s_cut)*0.8):]\ntag_mat_train = tag_mat[:int(len(tag_mat)*0.8)]\ntag_mat_test = tag_mat[int(len(tag_mat)*0.8):]\n\n# enhance the training set \ns_cut_train_extend = []\nalpha = 0.9\nbeta = 0.9\ngamma = 0.9\ntag_mat_train_new = []\nind_cnt={list(tag_dict.values())[i]:k for i,k in enumerate(np.array(tag_mat).sum(0)) if k>0}\n\nfor j,s in enumerate(s_cut_train):\n    # to add how many word seq for each one and make all kinds of quantity balance\n    for k in range(int(sum(ind_cnt.values())\/ind_cnt[tag_mat_train[j].argmax()])): \n        # form the taget vector at the same time\n        tag_mat_train_new.append(tag_mat_train[j])\n        \n        s_new = []\n        for i,w in enumerate(s): #choose one opption with rate\n            \n            # rexchange position with previous word at rate 1-beta\n            if random.choices((0,1),(beta,1-beta))[0]&len(s_new):# opption at p = beta and isnot the first word\n                nextword = s_new.pop()\n                # invert\n                s_new.append(w)\n                s_new.append(nextword)\n            # delet word with probability at 1-alpha\n            elif random.choices((0,1),(alpha,1-alpha))[0]:# opption at p = alpha\n                pass   \n            # replace with synonyms at rate 1-gamma\n            elif random.choices((0,1),(alpha,1-alpha))[0]:# opption at p = alpha\n                if synonyms.nearby(w,2)[0]:\n                    s_new.append(synonyms.nearby(w,2)[0][1])\n            else:\n                s_new.append(w)\n            \n            \n\n        s_cut_train_extend.append(s_new)# add nearby sentences for ten times\n\n# shuffle training set then       \nsample = list(zip(s_cut_train_extend,tag_mat_train_new))\nrandom.shuffle(sample)\ns_cut_train_extend,tag_train = tuple(zip(*sample))\n\ntag_train = np.array(tag_train)# tag vec that extend","9c254a36":"np.array(tag_mat_train).sum(0)","5af666c9":"np.array(tag_mat_train_new).sum(0)","a629e11a":"word_lst=[]\nfor s in s_cut_train_extend+s_cut_test:\n    word_lst.extend(s)\nword_lst = list(set(word_lst))\nd_w2i = {w:i for i,w in enumerate(word_lst)}\n# save the dictionary to storage\nwith open('.\/d_w2i.txt','w') as f:\n    f.write(str(d_w2i))","28b7557f":"'''get digit data and organize'''\nfrom keras.preprocessing.sequence import pad_sequences\n\nLEN = 2000# the longest sequence\ndef tokenizer(d_w2i,txt_c,len=LEN):\n    '''given dictionary word to digt , txt cut and pad LEN. return digt tok '''\n    tok = []\n    for i,s in enumerate(txt_c):\n        tok.append([])\n        for w in s:\n            try:\n                if d_w2i[w] != None:\n                    tok[i].append(d_w2i[w])\n                else:\n                    tok[i].append(0)#if len of seq not approch to LEN add zeros before the indexs\n            except:\n                tok[i].append(0)\n    tok = pad_sequences(tok,LEN)\n    return tok\n\ntok_train = tokenizer(d_w2i,s_cut_train_extend)\ntok_test = tokenizer(d_w2i,s_cut_test)","d655a5d7":"'''load word embeding model'''\nfrom keras.preprocessing.sequence import pad_sequences\nimport codecs\ndef Load_model_emb(data_dict,embMod_path):\n    '''\n    Input: word-index dictionary, Mode path;\n    Return: word Embedding Matrix which can be process by keras.layers.Embedding\n    '''\n    with codecs.open(embMod_path,'r','utf-8') as f:\n        emb_mat = np.zeros((len(data_dict) + 1, 300))\n        size =None\n        num = 0 #num of hits\n        for line in f:\n            line = line.strip().split(' ')\n            if not size:\n                size=line\n                continue\n            if line[0] in data_dict.keys():\n                num+=1\n                emb_mat[data_dict[line[0]]] = np.asarray(line[1:], dtype='float32')\n    return emb_mat,len(data_dict)-num\nEmb_mod,oov = Load_model_emb(d_w2i,'..\/input\/w2v-cn\/sgns.sogounews.bigram-char')\noov_rate = oov\/len(d_w2i) # print num of word that out of vocabulary\noov_rate","2c6c5b81":"'''build model'''\nfrom keras.models import Sequential,save_model,load_model\nfrom keras.layers import *\ndef build_mod(tok,tag,emb_mat,drop_rate=0.3):\n    model = Sequential()\n    # embedding with unknown method\n\n    #Embedding(if cant work, we may input the vectors into the network directly)\n    model.add(Embedding(emb_mat.shape[0],emb_mat.shape[1],weights=[emb_mat],input_length=tok.shape[1],trainable = False,))\n    # drop some data randomly, thus, we can prevent overfitting\n    model.add(Dropout(drop_rate))\n\n    # GRU\n    model.add(GRU(2**4))\n    model.add(Dropout(drop_rate))\n    \n    # fully connection\n    model.add(Dense(2**6,activation='relu'))\n    model.add(Dropout(drop_rate))\n    \n    #the last layer, we use softmax for classification\n    model.add(Dense(tag.shape[1],activation = 'softmax'))\n\n    # set loss as crossentropy,and use method as Adam, and use accuracy as metric to analyse the result\n    model.compile(loss = 'categorical_crossentropy',optimizer = 'adam',metrics = ['accuracy'])\n    return model\nmod = build_mod(np.array(tok_train),np.array(tag_train),Emb_mod)\nmod.summary()","f8441864":"mod.fit(x=tok_train,y=np.array(tag_train), validation_split=0.2,batch_size=2**4,epochs=6,workers=4)","7aefa957":"mod.evaluate(tok_test,np.array(tag_mat_test))","251480a5":"show_data = (np.array(tag_mat_train).sum(0),np.array(tag_mat_train_new).sum(0),np.array(tag_mat_test).sum(0))\npd.DataFrame(data=show_data,columns=tag_dict.keys(),index=['\u8bad\u7ec3\u96c6\u6807\u7b7e\u6570','\u589e\u5f3a\u7684\u8bad\u7ec3\u96c6\u6807\u7b7e\u6570\u76ee','\u6d4b\u8bd5\u96c6\u6807\u7b7e\u6570\u76ee'])","527cde61":"import jieba\n# add emotional words to vocabulary\njieba.load_userdict(['..\/input\/emotion-word\/anger.txt','..\/input\/emotion-word\/neg.txt','..\/input\/emotion-word\/pos.txt'])\n\n# load stopword\nwith open('..\/input\/stopwordch\/stopwordCh.txt','r',encoding='utf8') as f:\n    stopword = [w.strip() for w in f.readlines()]\n    stopword.append(' ')\n    \ns_lst = input('please input sentence')\nif type(s_lst) is str: # format Str to List\n    s_lst = [s_lst]\n\n# save sentence word cut\ns_cut=[]\nfor s in s_lst:\n    lst = jieba.cut(s)\n    s_cut.append([w for w in list(lst) if w not in stopword+['\\n','\\xa0']])\n    \n# load dictionary \nwith open('.\/d_w2i.txt','r') as f:\n    d_w2i = eval(f.read())\n\n# get digit label matrix\n'''get digit data and organize'''\nfrom keras.preprocessing.sequence import pad_sequences\n\nLEN = 2000# the longest sequence\ndef tokenizer(d_w2i,txt_c,len=LEN):\n    '''given dictionary word to digt , txt cut and pad LEN. return digt tok '''\n    tok = []\n    for i,s in enumerate(txt_c):\n        tok.append([])\n        for w in s:\n            try:\n                if d_w2i[w] != None:\n                    tok[i].append(d_w2i[w])\n                else:\n                    tok[i].append(0)#if len of seq not approch to LEN add zeros before the indexs\n            except:\n                tok[i].append(0)\n    tok = pad_sequences(tok,LEN)\n    return tok\n\ntok = tokenizer(d_w2i,s_cut)\n\n'''predict with model'''\nfrom keras.models import load_model\nres = mod.predict(tok)\n\n'''afterward'''\npd.DataFrame(res\/res.max(),columns=['\u6109\u5feb', '\u9893\u5e9f', '\u6fc0\u6d3b', '\u538c\u70e6', '\u6ee1\u8db3', '\u5b81\u9759', '\u75b2\u4e4f', '\u6124\u6012', '\u82e6\u607c', '\u6cae\u4e27', '\u7d27\u5f20', '\u60b2\u89c2'])","c3741756":"## Testing\nTest the generalization ability of the model, not really well.","e2a9303f":"## Load Text\nLoad samples(texts) to Memory and get participle with `jieba`. At the same time, we can remove stopwords from the words sequences. The result of the process is a value `s_cut` participled","5d315402":"## Load Table & Show","a5e71ac9":"## RNN Model\nWe build a recurrent neural network(RNN) as a classifier, in which we set some dropout layers to prevent overfitting and GRU layer to learn the pattern.","75ce38d8":"# Enrich Data\nIn order to improve the size of data, we need to use data enhancement technology, and then generate samples for training. The main methods are: randomly delete words in the original word sequences, replace synonyms or replace positions. Thus we need a new package `synonyms`(the word-hub must be download from network)","d7777c9b":"## Apply to Real Scenes\n\nIn CMD we can input a Chinese sentence(Further, we could input a list in parameter`s_lst`).","4f588bf8":"## Summary\nIn order to classify emotions, we construct a RNN based on GRU layer. The main contradiction is that the sample size is too small and the prediction index is too large. The prominent problem is that the number of texts with different emotions varies greatly. We use text enhancement technology to improve the problem of small sample size, but it needs professional knowledge to simplify the classification index. \n\nIn order to solve the problem of imbalance in the number of texts, this preprocessing set the enhancement multiple as the total number divided by the number of the first kind of sentence to round. However, since there may be multiple categories of sentences, this method is not stable for the balance of the number, and needs to be improved in the algorithm.","ac1a3f83":"## Check the data\nTo find shortcoming of the data, such as shortage of specific kind of sample and corelation between tags(whether we could slash the type of emotion or not). Last but not the least the balance of quantity of emotion tag.","897dfad3":"## Model Fitting\nPut training data into RNN to evaluate a set of parameters","6ed143e1":"## Processing Label Vectors\nThis cell transfer label to label vectors in which one represent exist particular attribution, zero means not exists it. \n","a14823a2":"quantity after enbalancing(much better):","484858a2":"## Load Stopword\nMost stop words interfere with classification, so they are removed directly after we load it.","a3e9084a":"## Load Word2vector Embedding Model\nWe can get the Pre training model [here](https:\/\/github.com\/Embedding\/Chinese-Word-Vectors). By this way, we can calculate similarity of each pair.\n\nYet the hind remain in the huge rate of out of vocabulary (oov) with the Pre-training model mentioned above.","f1d8b503":"quantity before enbalencing:","e6e4dce9":"## Build a Dictionary for Words Considered\nIn order to fit the word2vector embedding model that we got later, we may creat a dictionary that contain all the vital word for classification. We save the word to index dictionary to the value `d_w2i`","0a735673":"## Tokenize the Matrix\nWe can transfer meaningful words sequences to index matrix, thanks for which machine could recognize it. We call it Tokenization. As a result every word is replaced by a index which we can match from w2i dictionary we create above. So that, we solve it as a numerical calculation problem.\n\nWhat's more, In order to make each index vector equal in length, we need to add zero before it, which we called padding."}}