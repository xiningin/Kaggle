{"cell_type":{"5422b4e6":"code","1ec7da45":"code","151a048e":"code","1aa6e57a":"code","65e15f76":"code","a0873c0e":"code","e521425e":"code","5776efb2":"code","8a68445f":"code","a1fca91b":"code","0b060902":"code","ddad0398":"code","4735e3dc":"code","98be808d":"code","36b5e52a":"code","faf176eb":"code","a1d5da76":"code","2db7c79f":"code","5023916e":"code","83062e42":"code","a8ba7204":"code","d4640a03":"code","9fb0de1d":"code","01154ec0":"code","13b8155f":"code","e63de07d":"code","c85662f5":"code","19c89372":"code","25541d4c":"code","c791aaba":"code","58f01901":"code","c31dca12":"code","08ef8252":"code","ba6527c7":"code","5775988e":"code","5214cb5f":"code","2da19d84":"code","c4acfe1b":"code","ebbb95ff":"code","7c847862":"code","97ac5540":"code","3e524301":"code","16d5778f":"code","edf92a09":"code","32631072":"code","6593af47":"code","d8ab6d7a":"code","fd497ba4":"markdown","2f2b5c92":"markdown","2de688ce":"markdown","810231d2":"markdown","8a6ef0eb":"markdown","ba70d503":"markdown","ff1a749e":"markdown","296f74c8":"markdown","b02013e5":"markdown","c65f6f16":"markdown","f4c4283c":"markdown","01733c24":"markdown","ec857943":"markdown","a17bd4ab":"markdown","468abdc4":"markdown","825d2577":"markdown","d39bba47":"markdown","df169011":"markdown","110eacca":"markdown","a4602ab8":"markdown","0928a600":"markdown","adf4e5a1":"markdown","ac1d8e6f":"markdown","c78226c9":"markdown","ae2eb551":"markdown","589eb3e2":"markdown","09a3a427":"markdown","121db7f5":"markdown","69ffd3af":"markdown","d21b1d42":"markdown","80ec73dd":"markdown","5becac40":"markdown","37777526":"markdown","e0c9a8da":"markdown"},"source":{"5422b4e6":"import pandas as pd \nimport numpy as np \nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","1ec7da45":"df= pd.read_csv('..\/input\/pima-indians-diabetes-database\/diabetes.csv')\ndf.head()\n","151a048e":"df.columns","1aa6e57a":"df.shape","65e15f76":"df.info()","a0873c0e":"df.describe()","e521425e":"features_with_na=[features for features in df.columns if df[features].isnull().sum()>1]\n## 2- step print the feature name and the percentage of missing values\nfor feature in features_with_na:\n    print(feature, np.round(df[feature].isnull().mean(), 4),  ' % missing values')\nfeatures_with_na","5776efb2":"numerical_features = [feature for feature in df.columns if df[feature].dtypes != 'O']\nlen(numerical_features)\n","8a68445f":"numerical_features","a1fca91b":"discrete_feature=[feature for feature in numerical_features if len(df[feature].unique())<25]\nprint(\"Discrete Variables Count: {}\".format(len(discrete_feature)))","0b060902":"discrete_feature","ddad0398":"continuous_feature=[feature for feature in numerical_features if feature not in discrete_feature]\nprint(\"Continuous feature Count {}\".format(len(continuous_feature)))","4735e3dc":"for feature in continuous_feature:\n    data=df.copy()\n    data[feature].hist(bins=25)\n    plt.xlabel(feature)\n    plt.ylabel(\"Count\")\n    plt.title(feature)\n    plt.show()","98be808d":"fig, ax = plt.subplots(figsize=(10,8))\nsns.heatmap(df.corr(),annot=True,ax=ax)","36b5e52a":"sns.displot(x='Age', hue='Outcome', data=df, alpha=0.6)\nplt.show()","faf176eb":"diabetes = df[df['Outcome']==1]\nsns.displot(diabetes.Age, kind='kde')\nplt.show()","a1d5da76":"sns.displot(diabetes.Age, kind='ecdf')\nplt.grid(True)\nplt.show()","2db7c79f":"ranges = [0, 30, 40, 50, 60, 70, np.inf]\nlabels = ['0-30', '30-40', '40-50', '50-60', '60-70', '70+']\n\ndiabetes['Age'] = pd.cut(diabetes['Age'], bins=ranges, labels=labels)\ndiabetes['Age'].head()","5023916e":"sns.countplot(diabetes.Age)","83062e42":"df.head()","a8ba7204":"sns.displot(diabetes.BMI,kind='kde'),sns.displot(df.BMI,kind='kde')\n","d4640a03":"df.head()","9fb0de1d":"categorical_vars = ['Pregnancies']\ncontinuous_vars= ['Glucose','BloodPressure','SkinThickness','Insulin','BMI','DiabetesPedigreeFunction','Age']","01154ec0":"for feature in continuous_vars:\n    data=df.copy()\n    if 0 in data[feature].unique():\n        pass\n    else:\n        data[feature]=np.log(data[feature])\n        data.boxplot(column=feature)\n        plt.ylabel(feature)\n        plt.title(feature)\n        plt.show()","13b8155f":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import  BaggingClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.ensemble import AdaBoostClassifier","e63de07d":"#Creating a copy\ndata= df","c85662f5":"data.head()","19c89372":"\nscaler = StandardScaler()\n\n# define the columns to be encoded and scaled\n\n\n# encoding the categorical columns\ndata = pd.get_dummies(data, columns = categorical_vars, drop_first = True)\n\nX = data.drop(['Outcome'],axis=1)\ny = data[['Outcome']]\n\ndata[continuous_vars] = scaler.fit_transform(X[continuous_vars])\n\n# defining the features and target\nX = data.drop(['Outcome'],axis=1)\ny = data[['Outcome']]\n\n","25541d4c":"X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.1)","c791aaba":"lr = LogisticRegression(random_state=42)\n\nknn = KNeighborsClassifier()\npara_knn = {'n_neighbors':np.arange(1, 50)}\n\ngrid_knn = GridSearchCV(knn, param_grid=para_knn, cv=5)\n\ndt = DecisionTreeClassifier()\npara_dt = {'criterion':['gini','entropy'],'max_depth':np.arange(1, 100), 'min_samples_leaf':[1,2,4,5,10,20,30,40,80,100]}\ngrid_dt = GridSearchCV(dt, param_grid=para_dt, cv=5)\n\nrf = RandomForestClassifier()\n\n# Define the dictionary 'params_rf'\nparams_rf = {\n    'n_estimators':[100, 350, 500],\n    'min_samples_leaf':[2, 10, 30]\n}\ngrid_rf = GridSearchCV(rf, param_grid=params_rf, cv=5)","58f01901":"dt = DecisionTreeClassifier(criterion='gini', max_depth=20, min_samples_leaf=5, random_state=42)\nknn = KNeighborsClassifier(n_neighbors=3)\nrf = RandomForestClassifier(n_estimators=500, min_samples_leaf=2, random_state=42)","c31dca12":"# Define the list classifiers\nclassifiers = [('Logistic Regression', lr), ('K Nearest Neighbours', knn), ('Classification Tree', dt), ('Random Forest', rf)]\n","08ef8252":"for clf_name, clf in classifiers:    \n \n    # Fit clf to the training set\n    clf.fit(X_train, y_train)    \n   \n    # Predict y_pred\n    y_pred = clf.predict(X_test)\n    \n    # Calculate accuracy\n    accuracy = accuracy_score(y_pred, y_test) \n   \n    # Evaluate clf's accuracy on the test set\n    print('{:s} : {:.3f}'.format(clf_name, accuracy))","ba6527c7":"from sklearn.ensemble import AdaBoostClassifier\n\nada = AdaBoostClassifier(base_estimator=rf, n_estimators=100, random_state=1)\n\nada.fit(X_train, y_train)\n\ny_pred = ada.predict(X_test)\n\naccuracy_score(y_pred, y_test)","5775988e":"importances = pd.Series(data=rf.feature_importances_,\n                        index= X_train.columns)\n\n# Sort importances\nimportances_sorted = importances.sort_values()\n\n# Draw a horizontal barplot of importances_sorted\nplt.figure(figsize=(10, 10))\nimportances_sorted.plot(kind='bar',color='orange')\nplt.title('Features Importances')\nplt.show()","5214cb5f":"from sklearn.model_selection import KFold, StratifiedKFold\nfrom sklearn.metrics import accuracy_score\n\ndef cross_val(X, y, model, params, folds=5):\n\n    skf = StratifiedKFold(n_splits=folds, shuffle=True, random_state=21)\n    for fold, (train_idx, test_idx) in enumerate(skf.split(X, y)):\n        print(f\"Fold: {fold}\")\n        x_train, y_train = X.iloc[train_idx], y.iloc[train_idx]\n        x_test, y_test = X.iloc[test_idx], y.iloc[test_idx]\n\n        alg = model(**params)\n        alg.fit(x_train, y_train,\n                eval_set=[(x_test, y_test)],\n                early_stopping_rounds=100,\n                verbose=400)\n\n        pred = alg.predict(x_test)\n        accuracy = accuracy_score(y_test, pred)\n#         log_loss_score = log_loss(y_test,pred)\n        print(f\" accuracy : {accuracy}\")\n        print(\"-\"*50)\n    return alg\n        \n\n","2da19d84":"lgb_params= {'learning_rate': 0.0001, \n             'n_estimators': 20000, \n             'max_bin': 94,\n             'num_leaves': 5, \n             'max_depth': 30, \n             'reg_alpha': 8.457, \n             'reg_lambda': 6.853, \n             'subsample': 0.749}","c4acfe1b":"from lightgbm import LGBMClassifier\nlgb_model = cross_val(X, y, LGBMClassifier, lgb_params)","ebbb95ff":"from xgboost import XGBClassifier\nclassifier = XGBClassifier(n_estimators = 10000,predictor = 'gpu_predictor',tree_method = 'gpu_hist',learning_rate = 0.01,max_depth=29,max_leaves = 31,eval_metric = 'mlogloss',verbosity = 3)\nclassifier.fit(X,y)","7c847862":"y_pred=classifier.predict(X_test)\ny_test=np.array(y_test)\nprint(\"accuracy_score_XGBOOST: \",accuracy_score(y_pred,y_test))","97ac5540":"from tensorflow.keras.layers import Dense,Dropout,Flatten\nfrom tensorflow.keras.layers import MaxPooling2D,GlobalAveragePooling2D,BatchNormalization,Activation\nfrom tensorflow import keras\nimport tensorflow as tf","3e524301":"X_train.shape","16d5778f":"\nmodel = tf.keras.Sequential()\nmodel.add(Dense(1024, input_dim=23, activation= \"relu\"))\nmodel.add(Dropout(0.3))\nmodel.add(Dense(512, activation= \"relu\"))\nmodel.add(Dropout(0.4))\nmodel.add(Dense(128, activation= \"relu\"))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(32, activation= \"relu\"))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(1))\nmodel.summary() #Print model Summary","edf92a09":"model.compile(loss= \"binary_crossentropy\" , optimizer=\"adam\", metrics=[\"accuracy\"])","32631072":"Performance = model.fit(X_train, y_train, validation_split =0.1,epochs=5)","6593af47":"model.evaluate(X_test,y_test)","d8ab6d7a":"my_dpi = 50 # dots per inch .. (resolution)\nplt.figure(figsize=(400\/my_dpi, 400\/my_dpi), dpi = my_dpi)\nplt.plot(Performance.history['accuracy'], label='train accuracy')\nplt.plot(Performance.history['val_accuracy'], label='val accuracy')\nplt.legend()\nplt.show()\nplt.savefig('AccVal_acc')","fd497ba4":"# Number of Numerical Variables","2f2b5c92":"**NOTE THAT THERE AREN'T OUTLIERS**","2de688ce":"**The accuracy of the following models are** \n1. **Logistic Regression : 0.805**\n2. **K Nearest Neighbours : 0.727**\n3. **Classification Tree : 0.740**\n4. **Random Forest : 0.779**\n5. **Adaboost Classiefier : 0.779**\n6. **ANN : 0.770**\n**Note that the Neural Network model overfits thus it isn't advisable to use Neural Network models as there are not complex patterns we need to know , neither we need to figure out any high degree of non-linearity.**","810231d2":"# NEURAL NETWORK APPROACH","8a6ef0eb":"# Overview","ba70d503":"**Let Us Know if We Have any missing values**","ff1a749e":"**DESCRIPTION OF THE DATASET**","296f74c8":"**WE SEE THAT AGES BETWEEN 50-60 ARE THE MOST PRONE TO HEART ATTACKS**","b02013e5":"**WE SEE THAT LOGISTIC REGRESSION PERFORMS THE BEST**","c65f6f16":"Diabetes is a disease that occurs when your blood glucose, also called blood sugar, is too high. Blood glucose is your main source of energy and comes from the food you eat. Insulin, a hormone made by the pancreas, helps glucose from food get into your cells to be used for energy. Sometimes your body doesn\u2019t make enough\u2014or any\u2014insulin or doesn\u2019t use insulin well. Glucose then stays in your blood and doesn\u2019t reach your cells.\n\nOver time, having too much glucose in your blood can cause health problems. Although diabetes has no cure, you can take steps to manage your diabetes and stay healthy.\n\nAccording to Wikipedia the number of people with diabetes in India has increased from 26 million in 1990 to 65 million in 2016. According to the 2019 National Diabetes and Diabetic Retinopathy Survey report released by the Ministry of Health and Family Welfare, the prevalence was found to be 11.8% in people over the age of 50. The prevalence of diabetes is 6.5% and prediabetes 5.7% among the adults below the age of 50 years, according to the DHS survey.The prevalence was similar in both male (12%) and female (11.7%) populations. It was higher in urban areas. When surveyed for diabetic retinopathy, which threatens eyesight, 16.9% of the diabetic population aged up to 50 years were found to be affected. Per the report, diabetic retinopathy in the 60-69-years age group was 18.6%, in the 70-79-years age group it was 18.3%, and in those over 80 years of age it was 18.4%. A lower prevalence of 14.3% was observed in the 50-59-years age group. High prevalence of diabetes is reported in economically and epidemiologically advanced states such as Tamil Nadu and Kerala, where many research institutes which conduct prevalence studies are also present.","f4c4283c":"Wow!! We got to know all of the features are numerical variables ! ","01733c24":"# Inference","ec857943":"# LIGHT GBM","a17bd4ab":"**PREPARING THE DATASET FOR MODEL**","468abdc4":"# THANK YOU , IF YOU LIKE THE NOTEBOOK PLEASE DO UP VOTE","825d2577":"# How will we proceed ?","d39bba47":"# **EDA**","df169011":"# **UNDERSTANDING THE DATA**","110eacca":"# Including Required Packages ","a4602ab8":"**We need to know the number of discrete variables, Let us find it out !**","0928a600":"# What are the features?\n","adf4e5a1":"So we know that there are 9 features that has been included in the dataset needed to determine Heart Attack","ac1d8e6f":"**Now let's deal with the Continuous Variables**","c78226c9":"![](https:\/\/images.everydayhealth.com\/images\/diabetes-awareness-month-1440x810.jpg)","ae2eb551":"1. **Understanding the Data**\n\n2. **EDA**\n\n3. **Model Building**\n\n4. **Model Performance**\n\n5. **Inference**\n","589eb3e2":"# **MODEL BUILDING**","09a3a427":"**GREAT! We don't have any null values in the dataset! That would make the work a lot easier**","121db7f5":"# XGBoost","69ffd3af":"**IMPORTING THE NECESSARY LIBRARIES**","d21b1d42":"**READING THE DATA**","80ec73dd":"**So we see that the most important factor which leads to Diabetes is age and blood glucose level, so it is advisable to the general people to take proper care of the aged people as much as they can and following are the few guidelines that help them.\nDoctors generally advise a person to get his\/her blood sugar tested when:**\n\n**Urinate (pee) a lot, often at night**\n\n**Are very thirsty**\n\n**Lose weight without trying**\n\n**Are very hungry**\n\n**Have blurry vision**\n\n**Have numb or tingling hands or feet**\n\n**Feel very tired**\n\n**Have very dry skin**\n\n**Have sores that heal slowly**\n\n**Have more infections than usual**\n\n\n","5becac40":"**Results against the Age**","37777526":"# **MODEL PERFORMANCES**","e0c9a8da":"# **DIABETES DATA EDA MODEL BUILDING**"}}