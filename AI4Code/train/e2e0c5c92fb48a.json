{"cell_type":{"82d78eba":"code","6797dcf5":"code","3ddbba9f":"code","c472ee16":"code","dc2f1610":"code","f787cae5":"code","7aba4d41":"code","53f4e9d2":"code","de423b26":"code","468572a0":"code","dfc2fcf4":"code","e2bc2193":"code","d7ec3cda":"markdown","7ffdbf65":"markdown","804a2282":"markdown","ca28072d":"markdown","9abf7898":"markdown","6aaab4b9":"markdown","54d3d504":"markdown","10c8488b":"markdown","f52bac44":"markdown","d6312bf1":"markdown","a906cee9":"markdown","b19e0b09":"markdown","013eff58":"markdown"},"source":{"82d78eba":"import warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense, BatchNormalization, Reshape\nfrom keras.callbacks import LearningRateScheduler\nfrom keras.optimizers import Adam\nfrom keras.layers import Conv2D, Dropout, Conv2DTranspose, UpSampling2D\n\nimport numpy as np\nimport os\nimport cv2\nimport matplotlib.pyplot as plt\nfrom PIL import Image","6797dcf5":"def list_images(basePath, contains=None):\n    # return the set of files that are valid\n    return list_files(basePath, validExts=(\".jpg\", \".jpeg\", \".png\", \".bmp\"), contains=contains)\n\ndef list_files(basePath, validExts=(\".jpg\", \".jpeg\", \".png\", \".bmp\"), contains=None):\n    # loop over the directory structure\n    for (rootDir, dirNames, filenames) in os.walk(basePath):\n        # loop over the filenames in the current directory\n        for filename in filenames:\n            # if the contains string is not none and the filename does not contain\n            # the supplied string, then ignore the file\n            if contains is not None and filename.find(contains) == -1:\n                continue\n\n            # determine the file extension of the current file\n            ext = filename[filename.rfind(\".\"):].lower()\n\n            # check to see if the file is an image and should be processed\n            if ext.endswith(validExts):\n                # construct the path to the image and yield it\n                imagePath = os.path.join(rootDir, filename).replace(\" \", \"\\\\ \")\n                yield imagePath\n                \ndef load_images(directory='', size=(64,64)):\n    images = []\n    labels = []  # Integers corresponding to the categories in alphabetical order\n    label = 0\n    \n    imagePaths = list(list_images(directory))\n    \n    for path in imagePaths:\n        \n        if not('OSX' in path):\n        \n            path = path.replace('\\\\','\/')\n\n            image = cv2.imread(path) #Reading the image with OpenCV\n            image = cv2.resize(image,size) #Resizing the image, in case some are not of the same size\n\n            images.append(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n    \n    return images","3ddbba9f":"images=np.array(load_images('..\/input'))","c472ee16":"_,ax = plt.subplots(5,5, figsize = (8,8)) \nfor i in range(5):\n    for j in range(5):\n        ax[i,j].imshow(images[5*i+j])\n        ax[i,j].axis('off')","dc2f1610":"noise_size = 10000\nepsilon = 0.00001 # Small float added to variance to avoid dividing by zero in the BatchNorm layers.\nimg_shape = (64, 64, 3)","f787cae5":"# Create generative model\n\nmodel = Sequential()\n\nmodel.add(Dense(1024, activation='elu', input_shape=(noise_size,))) # (noise_size) -> (1024)\nmodel.add(BatchNormalization())\nmodel.add(Reshape((8,8,16))) # (8, 8, 16)\nmodel.add(Dropout(0.1))\n\nmodel.add(Conv2D(64, (3, 3), activation='elu', padding='same')) # (8, 8, 128)\nmodel.add(BatchNormalization())\nmodel.add(UpSampling2D((2, 2))) # (16, 16, 128)\nmodel.add(Dropout(0.1))\n\nmodel.add(Conv2D(64, (3, 3), activation='elu', padding='same')) # (16, 16, 64)\nmodel.add(BatchNormalization())\nmodel.add(UpSampling2D((2, 2))) # (32, 32, 64)\nmodel.add(Dropout(0.1))\n\nmodel.add(Conv2D(32, (3, 3), activation='elu', padding='same')) # (32, 32, 32)\nmodel.add(BatchNormalization())\nmodel.add(UpSampling2D((2, 2))) # (64, 64, 32)\nmodel.add(Dropout(0.1))\n\nmodel.add(Conv2D(3, (3, 3), activation='sigmoid', padding='same')) # (64, 64, 3)\n\nmodel.compile(optimizer=Adam(lr=0.005), loss='binary_crossentropy')\n\nmodel.summary()","7aba4d41":"# Fix random seed for reproductibility\nnp.random.seed(0)\n\n# Selecting a random sample of noise_size images\nidx = np.random.randint(0,len(images),noise_size)\n\n# Scaling the data\ntrain_y = images[idx,:,:,:]\/255.\n\n# Creating the identity matrix of size noise_size\ntrain_X = np.zeros((noise_size,noise_size))\nfor i in range(noise_size):\n    train_X[i,i] = 1","53f4e9d2":"epochs = 300","de423b26":"def schedule(step):\n    \n    if step < 100:\n        return 0.001\n    \n    elif step < 250:\n        return 0.0001\n    \n    else:\n        return 0.00003","468572a0":"scheduler = LearningRateScheduler(schedule, verbose=0)\n\nmodel.fit(train_X, train_y, epochs=epochs, batch_size=128, callbacks=[scheduler], verbose=0)","dfc2fcf4":"# Recalling from memorized faces\n\nfor k in range(3):\n    \n    plt.figure(figsize=(15,1.5))\n    \n    for j in range(10):\n    \n        one_hot_input = np.zeros((noise_size))\n        one_hot_input[np.random.randint(noise_size)] = 1\n        \n        img = model.predict(one_hot_input.reshape((-1, noise_size)))\n        img = Image.fromarray((255*img).astype('uint8').reshape((64, 64, 3)))\n        \n        plt.subplot(1,10,j+1)\n        \n        plt.axis('off')\n        plt.imshow(img)\n        \n    plt.show()","e2bc2193":"for k in range(10):\n    a = np.random.randint(noise_size)\n    b = np.random.randint(noise_size)\n    print(\"Simple pixel average\")\n    plt.figure(figsize=(10,3))\n    \n    \n    for j in range(10):\n        input_vector = np.zeros((noise_size))\n        \n        proportion = j\/9\n\n        img = proportion*train_y[a] + (1-proportion)*train_y[b]\n        img = Image.fromarray((255*img).astype('uint8').reshape((64,64,3)))\n        \n        plt.subplot(2,5,j+1)\n        plt.axis('off')\n        plt.imshow(img)\n        \n    plt.show()\n    \n    print(\"Merge with generative network\")\n    plt.figure(figsize=(10,3))\n    \n    for j in range(10):\n        input_vector = np.zeros((noise_size))\n        \n        proportion = j\/9\n        # Percentage of the input coming from the first one-hot vector\n        input_vector[a] = proportion\n        # Percentage of the input coming from the second one-hot vector\n        input_vector[b] = 1-proportion\n        \n        input_vector = input_vector\/(np.sqrt(input_vector.dot(input_vector.T)))\n\n        img = model.predict(input_vector.reshape((-1, noise_size)))\n        img = Image.fromarray((255*img).astype('uint8').reshape((64,64,3)))\n        \n        plt.subplot(2,5,j+1)\n        plt.axis('off')\n        plt.imshow(img)\n        \n    plt.show()","d7ec3cda":"Most of the images are cropped the same way, with the top of the head missing and a small face inclination. However, a few images do not contain faces and may hinder the model.","7ffdbf65":"## Generate images","804a2282":"## Loading data","ca28072d":"I was inspired by this notebook : https:\/\/www.kaggle.com\/cdeotte\/supervised-generative-dog-net from Chris Deotte","9abf7898":"We clearly recognize faces similar to those in the dataset, every one of these images is the reproduction of an image from the dataset.\n\nNow let's try to **create original images** by merging faces !\n\nWe will compare the results with a basic pixel average of the two pictures.","6aaab4b9":"## Creating Model","54d3d504":"# Merging anime faces with a generative network","10c8488b":"The goal of this approach is to train a network to reproduce anime style faces, we want the model to create new faces in the style of the input data.\n\nTo do this, we will apply the following protocol :\n* Select a sample of N images in the dataset\n* Create N one-hot vectors of size (N), **each vector corresponding to an image**\n* Train the network to reproduce the images from the one-hot vectors\n\nOnce the network is trained, we will display merges of different faces by feeding a linear combination of one-hot vectors to the network.\n\n**We want to avoid creating a simple linear combination of output pixels**, our goal is to output a realistic anime face for each input combination. This is why we have to create small layers in the network, the model must lack the capacity to simply memorize every image. It must only memorize basic features of the image and learn to create a face from those basic features.","f52bac44":"## Prepare data","d6312bf1":"## Idea of this notebook","a906cee9":"## Importing modules","b19e0b09":"## Training phase","013eff58":"We can see the difference between the two approaches, **the pixel average creates a superposition of faces while the generative method shows a progressive modification of shapes** to transition between the two faces. However, the generated images are blurry since the generative network is not large enough to create precise images.\n\nSome transitions between images are successful and create convincing images.\n\nHowever, when the inclination or the eye artstyle from the two images is too different, the intermediate images present some strange patterns."}}