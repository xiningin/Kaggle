{"cell_type":{"c57d650f":"code","8e527de7":"code","698fd7f7":"code","6be9d0fd":"code","f549c7e2":"code","436d6b88":"code","294995ed":"code","a5dff6f4":"code","fd862694":"code","e1b5339a":"code","bbccd699":"code","0361630c":"code","24f29be9":"code","23831516":"code","068a52a3":"code","b74eb7c0":"code","f2daa149":"code","f69defa0":"markdown","2e8f5d8e":"markdown","9aa2d594":"markdown","06f4faab":"markdown","d82b34e4":"markdown","c67acfb1":"markdown","a57e449f":"markdown","cb80411b":"markdown","76b909e9":"markdown","2a543ba2":"markdown","b879dfe7":"markdown","c19a9494":"markdown"},"source":{"c57d650f":"from pathlib import Path\nimport pandas as pd","8e527de7":"data_path = Path(\"..\/input\/google-smartphone-decimeter-challenge\")\ndf_test = pd.read_csv(\n    data_path \/ 'baseline_locations_test.csv')\ndf_sub    = pd.read_csv(\n    data_path \/ 'sample_submission.csv')","698fd7f7":"import numpy as np\nfrom tqdm.notebook import tqdm","6be9d0fd":"truths = (data_path \/ 'train').rglob('ground_truth.csv')\n    # returns a generator\n\ndf_list = []\ncols = ['collectionName', 'phoneName', 'millisSinceGpsEpoch', 'latDeg',\n       'lngDeg']\n\nfor t in tqdm(truths, total=73):\n    df_phone = pd.read_csv(t, usecols=cols)  \n    df_list.append(df_phone)\ndf_truth = pd.concat(df_list, ignore_index=True)\n\ndf_basepreds = pd.read_csv(data_path \/ 'baseline_locations_train.csv', usecols=cols)\ndf_all = df_truth.merge(df_basepreds, how='inner', on=cols[:3], suffixes=('_truth', '_basepred'))","f549c7e2":"def calc_haversine(lat1, lon1, lat2, lon2):\n    \"\"\"Calculates the great circle distance between two points\n    on the earth. Inputs are array-like and specified in decimal degrees.\n    \"\"\"\n    RADIUS = 6_367_000\n    lat1, lon1, lat2, lon2 = map(np.radians, [lat1, lon1, lat2, lon2])\n    dlat = lat2 - lat1\n    dlon = lon2 - lon1\n    a = np.sin(dlat\/2)**2 + \\\n        np.cos(lat1) * np.cos(lat2) * np.sin(dlon\/2)**2\n    dist = 2 * RADIUS * np.arcsin(a**0.5)\n    return dist","436d6b88":"df_all['dist'] = calc_haversine(df_all.latDeg_truth, df_all.lngDeg_truth, \n    df_all.latDeg_basepred, df_all.lngDeg_basepred)","294995ed":"df_all.dist.describe()","a5dff6f4":"df_all.sort_values(by = 'dist',ascending = False)[['collectionName','dist']].head(10)","fd862694":"df_test['dist_pre'] = 0\ndf_test['dist_pro'] = 0","e1b5339a":"df_test['latDeg_pre'] = df_test['latDeg'].shift(periods=1,fill_value=0)\ndf_test['lngDeg_pre'] = df_test['lngDeg'].shift(periods=1,fill_value=0)\ndf_test['latDeg_pro'] = df_test['latDeg'].shift(periods=-1,fill_value=0)\ndf_test['lngDeg_pro'] = df_test['lngDeg'].shift(periods=-1,fill_value=0)\ndf_test['dist_pre'] = calc_haversine(df_test.latDeg_pre, df_test.lngDeg_pre, df_test.latDeg, df_test.lngDeg)\ndf_test['dist_pro'] = calc_haversine(df_test.latDeg, df_test.lngDeg, df_test.latDeg_pro, df_test.lngDeg_pro)\n\nlist_phone = df_test['phone'].unique()\nfor phone in list_phone:\n    ind_s = df_test[df_test['phone'] == phone].index[0]\n    ind_e = df_test[df_test['phone'] == phone].index[-1]\n    df_test.loc[ind_s,'dist_pre'] = 0\n    df_test.loc[ind_e,'dist_pro'] = 0","bbccd699":"df_test.dist_pre.describe()","0361630c":"pro_95 = df_test['dist_pro'].mean() + (df_test['dist_pro'].std() * 2)\npre_95 = df_test['dist_pre'].mean() + (df_test['dist_pre'].std() * 2)\nind = df_test[(df_test['dist_pro'] > pro_95)&(df_test['dist_pre'] > pre_95)][['dist_pre','dist_pro']].index\n\nfor i in ind:\n    df_test.loc[i,'latDeg'] = (df_test.loc[i-1,'latDeg'] + df_test.loc[i+1,'latDeg'])\/2\n    df_test.loc[i,'lngDeg'] = (df_test.loc[i-1,'lngDeg'] + df_test.loc[i+1,'lngDeg'])\/2","24f29be9":"!pip install simdkalman","23831516":"from pathlib import Path\nimport numpy as np\nimport pandas as pd\nimport simdkalman\nfrom tqdm.notebook import tqdm","068a52a3":"T = 1.0\nstate_transition = np.array([[1, 0, T, 0, 0.5 * T ** 2, 0], [0, 1, 0, T, 0, 0.5 * T ** 2], [0, 0, 1, 0, T, 0],\n                             [0, 0, 0, 1, 0, T], [0, 0, 0, 0, 1, 0], [0, 0, 0, 0, 0, 1]])\nprocess_noise = np.diag([1e-5, 1e-5, 5e-6, 5e-6, 1e-6, 1e-6]) + np.ones((6, 6)) * 1e-9\nobservation_model = np.array([[1, 0, 0, 0, 0, 0], [0, 1, 0, 0, 0, 0]])\nobservation_noise = np.diag([5e-5, 5e-5]) + np.ones((2, 2)) * 1e-9\n\nkf = simdkalman.KalmanFilter(\n        state_transition = state_transition,\n        process_noise = process_noise,\n        observation_model = observation_model,\n        observation_noise = observation_noise)","b74eb7c0":"def apply_kf_smoothing(df, kf_=kf):\n    unique_paths = df[['collectionName', 'phoneName']].drop_duplicates().to_numpy()\n    for collection, phone in tqdm(unique_paths):\n        cond = np.logical_and(df['collectionName'] == collection, df['phoneName'] == phone)\n        data = df[cond][['latDeg', 'lngDeg']].to_numpy()\n        data = data.reshape(1, len(data), 2)\n        smoothed = kf_.smooth(data)\n        df.loc[cond, 'latDeg'] = smoothed.states.mean[0, :, 0]\n        df.loc[cond, 'lngDeg'] = smoothed.states.mean[0, :, 1]\n    return df","f2daa149":"kf_smoothed_baseline = apply_kf_smoothing(df_test)\ndf_sub = df_sub.assign(\n    latDeg = kf_smoothed_baseline.latDeg,\n    lngDeg = kf_smoothed_baseline.lngDeg\n)\ndf_sub.to_csv('submission.csv', index=False)","f69defa0":"### Step.3 Kalman filter -----------------------------------------------------------------","2e8f5d8e":"### \u25a0 Update (2021-06-02)\nI got some advice from @Kyosuke0924 .I changed some code to speed up the process.","9aa2d594":"#### Thank you for reading to the end.I look forward to your comments.","06f4faab":"Finally, apply the Kalman filter.","d82b34e4":"### Step.2 Correct outliers in the test data.----------------------------------------------","c67acfb1":"If the distance between the previous and next pass is large, it is either an outlier or a case of speed (straight line movement), so fix the value to the middle of the previous and next pass.","a57e449f":"### Step.1 Check for outliers in training data --------------------------------------------","cb80411b":"There seems to be an outlier, although it is smaller than the training data.","76b909e9":"### \u25a0 Overview\n\nI thought a simple and easy post-processing based on my experience of using google maps on a daily basis.  \nHave you ever had an experience where the google map location information was way off?  I thought that the data set of this competition might contain such outliers.\n\n\nIn this notebook, Correct the outliers in the baseline and apply the Kalman filter afterwards.\n\nScore(Public Leaderboard)  \nOriginal : 7.190  \noutlier correction : 7.180  \nKalman filter : 6.164  ","2a543ba2":"### \u25a0 Reference\nI used these great notebooks as a guide in creating this notebook. Thank you for publishing the notebook.\n\n\n(Baseline :by JohnM)\nhttps:\/\/www.kaggle.com\/jpmiller\/baseline-from-host-data\n\n(Kalman filter : by Marcin Bodych)\nhttps:\/\/www.kaggle.com\/emaerthin\/demonstration-of-the-kalman-filter","b879dfe7":"There are some points where the difference between baseline and ground truth is more than 1,000 meters.It is surprising.  \nDealing with these outliers is likely to improve the score.","c19a9494":"Since there is no 'dist' in the test data, identify outliers based on their distance from the previous and next pass."}}