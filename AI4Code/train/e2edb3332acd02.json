{"cell_type":{"3b380043":"code","f7f5891b":"code","deae053d":"code","d5e59f69":"code","90ab9f81":"code","a43b375a":"code","77fae32e":"code","359ba10c":"code","1e843eb8":"code","68463fd4":"code","70003f09":"code","beff399d":"code","68da7454":"code","cdafd42b":"code","b785b70e":"code","c5fd5a8a":"code","d9e910a8":"code","30c4e500":"code","a179c922":"code","336fdf0f":"code","00745364":"code","a633ae44":"code","355aa4fd":"code","0861c042":"code","0bc73baf":"code","de3769e5":"code","4aad50f1":"code","9a0e48b7":"code","be27b7de":"code","9917ce14":"markdown","9431b5c0":"markdown","44f04575":"markdown","28ef2441":"markdown","bfd91bac":"markdown","dd60fbb1":"markdown","fb09c3ed":"markdown","a0811188":"markdown","dbbcaaff":"markdown","e254629a":"markdown","16391c9c":"markdown","80eb2211":"markdown","337f1bec":"markdown","b422a23c":"markdown","18f76405":"markdown","79e43cf5":"markdown","f901f85b":"markdown","8333c1ca":"markdown","6552937c":"markdown","3e6d79dd":"markdown","1cb0dc2c":"markdown","ad5e1777":"markdown","189e7b3e":"markdown","52dc9e2d":"markdown"},"source":{"3b380043":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","f7f5891b":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\npd.options.display.max_rows = 4000\npd.options.display.max_columns = 100\nsns.set(style = \"whitegrid\")\nfrom collections import Counter\n\n\n","deae053d":"data = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')\nprint ('Shape of train and test : ', data.shape,test.shape)\ndata.head(10)","d5e59f69":"#Check the columns which have majority of % Nulls\ndata.isnull().sum()\/len(data)","90ab9f81":"\ndf = data.nunique().sort_values().reset_index()\ndf.columns = ['Features','UniqueCount'] \ncols = df[df['UniqueCount']<=10]['Features'].values\nprint('Set of columns having 10 or less distinct classes :\\n\\n' ,cols)\n\nprint('\\nPercentage of each class  :\\n')\n\nfor features in cols:\n    c,c_percentage = dict(data[features].value_counts()),dict(data[features].value_counts(normalize = True))\n    print(features,c,c_percentage)","a43b375a":"def fn_feature_engg(df):\n#     Alley : More than 90 % of the data is null , so remove this column\n#     PoolQC : 99% of the data is null , so remove this column\n#     Fence : 80% of the data is null , so remove this column\n#     MiscFeature : 96 % of the data is null , so remove this column\n\n#     Remove columns where the variability id very low( if a single class appears for more than 90% of rows)\n#     Utilities\n#     CentralAir\n#     Street\n#     BsmtHalfBath\n#     LandSlope\n#     PavedDrive\n#     BsmtCond\n#     KitchenAbvGr \n#     Electrical \n#     GarageQual\n#     GarageCond\n#     Heating\n#     RoofMatl\n#     Condition2\n#     PoolArea \n    \n    \n    cols_to_del = ['Id','Alley','PoolQC','Fence','MiscFeature',\n                   'Utilities','CentralAir','Street','BsmtHalfBath','LandSlope','PavedDrive','BsmtCond','KitchenAbvGr','Electrical','GarageQual','GarageCond','Heating','RoofMatl','Condition2','PoolArea']\n    df = df.drop(cols_to_del,axis = 1)\n    \n    \n    # Impute the categorical values with constants\n    fill_dict = {('MasVnrType',): 'None',('MasVnrArea','GarageYrBlt','LotFrontage') : 0,('BsmtQual','BsmtExposure','BsmtFinType1','BsmtFinType2','FireplaceQu','GarageType','GarageFinish') : \"NA\"}\n   \n\n    for keys,values in fill_dict.items():\n        for cols in keys:\n            #print(cols,values)\n            df[cols].fillna(values,inplace = True)\n            \n#      Imputation of the null values for the test set\n\n\n    cols_to_impute_median = ['BsmtFinSF1','BsmtFinSF2','BsmtUnfSF','TotalBsmtSF','GarageArea']\n    cols_to_impute_mode = ['Exterior1st','Exterior2nd','BsmtFullBath','SaleType','GarageCars','MSZoning','KitchenQual', 'Functional']\n    \n\n    for cols in cols_to_impute_median :\n        df[cols] = df[cols].fillna(df[cols].median())\n        \n    for cols in cols_to_impute_mode :\n        df[cols] = df[cols].fillna(df[cols].mode()[0])\n        \n        \n        \n#      Make categorical columns  \n\n    cat_cols_1 = ['ExterQual', 'ExterCond', 'BsmtQual', 'HeatingQC', 'KitchenQual','FireplaceQu']\n    cat_1 = ['Ex','Gd','TA','Fa','Po','NA']\n    cat_1_mapping = {'Ex':6,'Gd':5,'TA':4,'Fa':3,'Po':2,'NA':1}\n\n    cat_cols_2 = ['BsmtFinType1', 'BsmtFinType2']\n    cat_2 = ['GLQ','ALQ','BLQ','Rec','LwQ','Unf','NA']\n    cat_2_mapping = {'GLQ':7,'ALQ':6,'BLQ':5,'Rec':4,'LwQ':3,'Unf':2,'NA':1}\n\n\n    cat_cols_3 = ['BsmtExposure']\n    cat_3 = ['Gd','Av','Mn','No','NA']\n    cat_3_mapping = {'Gd':5,'Av':4,'Mn':3,'No':2,'NA':1}\n    \n    cat_cols_4 = ['LotShape']\n    cat_4 = ['Reg','IR1','IR2','IR3']\n    cat_4_mapping = {'Reg':4,'IR1':3,'IR2':2,'IR3':1}\n    \n    cat_cols_5 = ['GarageFinish']\n    cat_5 = ['Fin','RFn','Unf','NA']\n    cat_5_mapping = {'Fin':4,'RFn':3,'Unf':2,'NA':1}\n\n\n\n    for cols in cat_cols_1 :\n        df[cols] = df[cols].map(cat_1_mapping)\n    for cols in cat_cols_2 :\n        df[cols] = df[cols].map(cat_2_mapping)\n    for cols in cat_cols_3 :\n        df[cols] = df[cols].map(cat_3_mapping)\n    for cols in cat_cols_4 :\n        df[cols] = df[cols].map(cat_4_mapping)        \n    for cols in cat_cols_5 :\n        df[cols] = df[cols].map(cat_5_mapping)  \n            \n    ## Createnew features and delete old features\n    \n\n#     # YrSold  - YearBuilt =   YearsOld (New feature Drop YrSold , YearBuilt)\n#     # YrSold  - YearRemodAdd = YearsRemod(New feature Drop YrSold , YearRemodAdd)\n#     # if YearBuilt  < YearRemodAdd = IsRemod(New feature)\n#     # YrSold-GarageYrBlt  YearsOldGarage(New Feature Drop GarageYrBlt )\n\n    df['YearsOld'] = df['YrSold'] - df['YearBuilt']\n    df['IsRemod'] = (df['YearBuilt'] < df['YearRemodAdd']).astype(int)\n    df['YearsRemod'] = df['YrSold'] - df['YearRemodAdd']\n    df['YearsOldGarage'] = df['YrSold']-df['GarageYrBlt']\n\n\n    df[df['YearsOld'] < 0]['YearsOld'] = 0\n    df[df['YearsRemod'] < 0]['YearsRemod'] = 0\n    df[df['YearsOldGarage'] < 0]['YearsOldGarage'] = 0\n \n    \n    \n\n    cols_to_del = ['YrSold','YearBuilt','MoSold','YearRemodAdd','GarageYrBlt']\n    df = df.drop(cols_to_del,axis = 1)\n    \n    print('Shape of the return dataframe : ',df.shape )\n    return df","77fae32e":"train = fn_feature_engg(data)\ntrain.head()","359ba10c":"train.isnull().sum()\/len(train)","1e843eb8":"target = 'SalePrice'","68463fd4":"from scipy import stats\ny= train[target]\nfig, ax = plt.subplots(1, 2)\nfig.set_size_inches(10, 5)\nplt.subplot(1, 2, 1)\nsns.distplot(y)\nplt.subplot(1, 2, 2)\nstats.probplot(y,plot=plt)\nplt.tight_layout()","70003f09":"y= np.log1p(train[target])\nfig, ax = plt.subplots(1, 2)\nfig.set_size_inches(10, 5)\nplt.subplot(1, 2, 1)\nsns.distplot(y)\nplt.subplot(1, 2, 2)\nstats.probplot(y,plot=plt)\nplt.tight_layout()","beff399d":"train[target]=np.log1p(train[target])","68da7454":"train.info()","cdafd42b":"categorical_columns = list(train.select_dtypes(include = ['object']).columns)\ncategorical_columns = categorical_columns + ['MSSubClass','OverallQual','OverallCond','ExterQual','ExterCond','BsmtQual','BsmtExposure','BsmtFinType1','BsmtFinType2','HeatingQC','KitchenQual','FireplaceQu','IsRemod']\n\nCategorical_ordinal_columns = ['LotShape','OverallQual','OverallCond','ExterQual','ExterCond','BsmtQual','BsmtExposure','BsmtFinType1','BsmtFinType2','HeatingQC','KitchenQual','FireplaceQu','GarageFinish']\nCategorical_nominal_columns = [i for i in categorical_columns if i not in Categorical_ordinal_columns]\n\nnumerical_cols = [i for i in train.columns if i not in categorical_columns]\n\n\n\nprint('Number of categorical columns :', len(categorical_columns))\nprint(categorical_columns)\n\n\nprint('\\nNumber of categorical ordinal columns :', len(Categorical_ordinal_columns))\nprint(Categorical_ordinal_columns)\n\nprint('\\nNumber of categorical nominal columns :', len(Categorical_nominal_columns))\nprint(Categorical_nominal_columns)\n\nprint('\\nNumber of numerical columns :', len(numerical_cols))\nprint(numerical_cols)\n","b785b70e":"train[numerical_cols].corr()[target].abs().sort_values(ascending = False)","c5fd5a8a":"# Consider the first few columns where the correlation is high above the threshold\n\nthreshold = 0.2\n\ndf = train[numerical_cols].corr()['SalePrice'].abs().sort_values(ascending = False).reset_index()\ndf.columns = ['Features','Correlation'] \nnumerical_cols = df[df['Correlation']>=threshold]['Features'].to_list()\n\n\n\nprint(numerical_cols,target)\n\n","d9e910a8":"# # find the categorical \n# categorical_columns = train.select_dtypes(exclude = ['number']).columns\n# print('Categorical Columns:\\n\\n',categorical_columns)\n\n# #separate nominal and ordinal\n# Categorical_ordinal_columns = ['ExterQual', 'ExterCond', 'BsmtQual', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'HeatingQC','KitchenQual','FireplaceQu']\n# Categorical_nominal_columns = [i for i in categorical_columns if i not in Categorical_ordinal_columns]\n# print ('\\nCategorical Nominal Columns:\\n\\n',Categorical_nominal_columns)\n# print ('\\nCategorical Ordinal Columns:\\n\\n',Categorical_ordinal_columns)","30c4e500":"#plot the boxplot for each category\nfig, ax = plt.subplots(len(Categorical_ordinal_columns)\/\/5 + 1, 5)\nfig.set_size_inches(20, 10)\nfor idx,col in enumerate(Categorical_ordinal_columns):\n    sns.boxplot(x=col,y='SalePrice',data = train,ax=ax[idx\/\/5, idx%5])\nplt.tight_layout()","a179c922":"#plot the boxplot for each category\nfig, ax = plt.subplots(len(Categorical_nominal_columns)\/\/5 + 1, 5)\nfig.set_size_inches(30, 20)\nfor idx,col in enumerate(Categorical_nominal_columns):\n    sns.boxplot(x=col,y='SalePrice',data = train,ax=ax[idx\/\/5, idx%5])\nplt.tight_layout()    ","336fdf0f":"selected_columns = numerical_cols + categorical_columns\nprint('Number of columns selected {0} \\n\\n{1}'.format(len(selected_columns),selected_columns))\nprint('\\nTarget column : ',target)\n\nfeature_train = train[selected_columns]\n\n","00745364":"fig, ax = plt.subplots(figsize=(30,10))\ncmap = sns.cubehelix_palette(light=1, as_cmap=True)\nsns.heatmap(feature_train.corr().abs(),annot = True,fmt = '.2f',cmap = cmap,ax=ax)\n","a633ae44":"cols_to_remove = [target]  #+ ['GarageArea','TotRmsAbvGrd','1stFlrSF','FirePlaces']\nselected_columns = [i for i in selected_columns if i not in cols_to_remove]\nprint('Number of columns selected {0} \\n\\n{1}'.format(len(selected_columns),selected_columns))\n\ny_train = feature_train['SalePrice']\nx_train = feature_train[selected_columns]\n\nprint('Shape of x and y train : ',x_train.shape,y_train.shape)\n","355aa4fd":"from sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler ,OrdinalEncoder,OneHotEncoder,RobustScaler,PowerTransformer\nfrom sklearn.compose import ColumnTransformer\n\n\n#Ordinal Encoder\nordinal_encoder = OrdinalEncoder()\n    \n# Categorical Ordinal Columns\ncat_ordinal_encoder = OrdinalEncoder()\n\n\n# Categorical Nominal Columns\ncat_nominal_encoder = OneHotEncoder(handle_unknown = 'ignore')\n\n# scale the data \nscaler = StandardScaler(with_mean=False)\nrobust_scaler = RobustScaler(with_centering=False)\npower_transform = PowerTransformer()\n\n\n\n\ntransformer = ColumnTransformer([('cat_nominal_encoder', cat_nominal_encoder,Categorical_nominal_columns)\n                                ], remainder ='passthrough'    \n                                )\n\n","0861c042":"from sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import GridSearchCV,cross_val_score\nfrom sklearn.ensemble import RandomForestRegressor,GradientBoostingRegressor\nfrom xgboost import XGBRegressor\n\nrandom_forest_regressor = RandomForestRegressor(random_state=0)\ngradient_boost_regressor = GradientBoostingRegressor(random_state = 0)\nxgboost_regressor = XGBRegressor(random_state = 0)\n\npipeline = Pipeline([\n                    ('column_transformer',transformer),\n#                     ('standard_scaler',scaler),\n                    ('robust_scaler',robust_scaler),\n#                     ('power_transform',power_transform),\n                    ('classifier',random_forest_regressor)\n])\n\nparams_classifier = [\n                    {\n                       'classifier' : [random_forest_regressor],\n                       'classifier__n_estimators' : [100,125],\n                       'classifier__max_depth' : [1,3,5,7]\n                     },\n                     {\n                       'classifier' : [gradient_boost_regressor],\n                       'classifier__loss':  [ 'huber','ls','lad','quantile'],\n                       'classifier__learning_rate' : [0.08,0.07,0.05],\n                       'classifier__n_estimators' : [125],\n                       'classifier__subsample' : [0.6,0.7,0.8],\n                       'classifier__max_depth' : [3,5,7]\n                     },\n                    {'classifier' : [xgboost_regressor]}\n                   ]\n\ngriseachcv = GridSearchCV(estimator = pipeline,param_grid = params_classifier,verbose = 1,cv = 10,scoring = 'neg_mean_squared_log_error',n_jobs = -1 )\ngriseachcv.fit(x_train,y_train)\n\n","0bc73baf":"gridsearchresult = pd.DataFrame(griseachcv.cv_results_)\ngridsearchresult.sort_values(by=['rank_test_score'])\nprint('Best Estimator :', griseachcv.best_estimator_)\nprint('Best score :',griseachcv.best_score_)\nprint('Best param :',griseachcv.best_params_)","de3769e5":"x_test = fn_feature_engg(test)[selected_columns]\n## Check if there are nulls\nx_test.isnull().sum()\n","4aad50f1":"id = test['Id']\npredicted_price = np.expm1(griseachcv.predict(x_test))\n\npredictions = pd.DataFrame(zip(id,predicted_price),columns = ['Id','SalePrice'])","9a0e48b7":"predictions.head()","be27b7de":"import csv\npredictions.to_csv('output_final.csv',index= False)\n","9917ce14":"Categorical and numerical columns","9431b5c0":"#### Find the numerical columns","44f04575":"# Modelling","28ef2441":"Transform the target to make it more normalised","bfd91bac":"### FInd the pairwise correlation","dd60fbb1":"We see that there is very  high correlation btween\n* GarageArea-GarageCars\n* 1stFlrSF-TotalBsmtSF\n* TotRmsAbvGrd - GrLivArea","fb09c3ed":"#### Find the categorical columns","a0811188":"### Target column\nCheck the skewness of the target if any though distribution and QQ plot","dbbcaaff":"We keep all the categorical columns as it is seen from the graphs that saleprice is correlated wiht them.Though for nominal it is difficult to visualise , but for ordinal it is evident","e254629a":"### Create a function to enginneer hte features -  drop map create etc. This same function will be used for the test data","16391c9c":"### Prepare the test data","80eb2211":"1. Delete columns\n    * Alley : More than 90 % of the data is null , so remove this column\n    * PoolQC : 99% of the data is null , so remove this column\n    * Fence  : 80% of the data is null , so remove this column\n    * MiscFeature : 96 % 0of the data is null , so remove this column \n    * Utilities   : 100 % of the data belongs to one class, so remove this column \n    \n    \n2. Impute columns\n    * LotFrontage  : 259 null... Impute wiht 0\n    * (Test) Exterior1st  : 1 Null.....Impute with mode\n    * (Test) Exterior2nd  : 1 Null.....Impute with mode\n    * MasVnrType : 8  null... Impute to None\n    * MasVnrArea : 8  null, same as MasVnrType... Impute to 0  as MasVnrArea for None MasVnrType is all 0\n    * BsmtQual   : 37  null ... Impute to string NA\n    * BsmtCond   : 37  null ... Impute to string NA\n    * BsmtExposure : 38  null...Impute to string NA\n    * BsmtFinType1 : 37  null...Impute to string NA\n    * BsmtFinType2 : 38  null...Impute to string NA \n    * (Test)BsmtFinSF1   : 1 Null.....Impute with median\n    * (Test)BsmtFinSF2   : 1 Null.....Impute with median\n    * (Test)BsmtUnfSF    : 1 Null.....Impute with median\n    * (Test)TotalBsmtSF    : 1 Null.....Impute with median\n    * (Test)BsmtHalfBath   : 2 Null.....Impute with mode\n    * (Test)BsmtFullBath  : 2 Null.....Impute with mode\n    * Electrical   : 1  null... Impute with the mode of electrical as mode value describes most of the observations.\n    * (Test)Functional   : 2 Null.....Impute with mode\n    * FireplaceQu  : 690 null...Impute to string NA  \n    * GarageType   : 81 null..Impute to string NA\n    * GarageYrBlt  : 81 null..Impute to 0\n    * GarageFinish : 81 null..Impute to string NA  \n    * GarageQual : 81 null..Impute to string NA\n    * GarageCond : 81 null..Impute to string NA\n    * (Test)SaleType : 1 Null.....Impute with mode","337f1bec":"### Check the percentage of nulls","b422a23c":"### Remove the highly correlated columns","18f76405":"## There are numerous features which might be correlated between them or not helpful in prediction. Lets drop them to reduce the feature set.","79e43cf5":"Gridsearch might take longer to complete.","f901f85b":"### Check How many are nulls","8333c1ca":"# Some Observations","6552937c":"### Predictions","3e6d79dd":"# Import the data","1cb0dc2c":"when the correlated columns are removed the leaderboard score reduced a bit. So kept those columns as it is.","ad5e1777":"### Check the columns which have majority of values skewed to 1 class","189e7b3e":"# EDA","52dc9e2d":"### Correlation of the variables with target"}}