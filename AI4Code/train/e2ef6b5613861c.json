{"cell_type":{"bfe233be":"code","70a20a31":"code","b7879a93":"code","6b60f153":"code","31c8ff8f":"code","c477ce65":"code","3340d9c1":"code","4f5d8eb8":"code","c325ec04":"code","df5a813b":"code","515d2245":"code","33817463":"code","023969f6":"code","f7ac02a5":"code","d693f0e6":"code","111c0233":"code","5244a82e":"code","dbaacb50":"code","c271b664":"code","932f528f":"code","0f15e80e":"code","d5417e48":"code","56e609c4":"code","2996cd2a":"code","539b17d3":"code","4b72bd65":"code","d708179e":"code","401a6cf1":"code","6f8ba220":"code","a53a286f":"code","2eb4e695":"code","ec3a9405":"code","8e8893aa":"code","bb9ec299":"code","7e3f256e":"code","0714735b":"code","81676abc":"code","ac4c1b9b":"code","9561b073":"code","03e84eb2":"markdown","fe258de7":"markdown","82c9b2df":"markdown","dac8bb99":"markdown","767bd2ee":"markdown","a01a477d":"markdown","5bd7b169":"markdown","118c00be":"markdown","2c0fa897":"markdown","fafb508d":"markdown","366ac332":"markdown","3379a5fd":"markdown","edd2bc8e":"markdown","1559fa06":"markdown","734a7c67":"markdown","61af1bf1":"markdown","2332bb5f":"markdown","ae658bbf":"markdown","ab605cab":"markdown","3f112795":"markdown","f4e425f6":"markdown","61d63470":"markdown","ff7e4053":"markdown","9277ba08":"markdown","79007be6":"markdown","020e0f34":"markdown","278ec5d8":"markdown","312875ee":"markdown","61a455e6":"markdown","d67d8bb9":"markdown","bd4159e1":"markdown","f2928dbf":"markdown","7b6cd2f0":"markdown","7f136b36":"markdown","298c7cc4":"markdown","3d238e7d":"markdown","60929996":"markdown","9dae8c6b":"markdown","3e993f2e":"markdown","e9100731":"markdown","fe5a397e":"markdown","452e6c4a":"markdown","0ccf7be1":"markdown","f3f99467":"markdown","b28ee984":"markdown","a5d2c216":"markdown"},"source":{"bfe233be":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set(context=\"notebook\", palette=\"Spectral\", style = 'darkgrid' ,font_scale = 1.5, color_codes=True)\nimport warnings\nwarnings.filterwarnings('ignore')\n%matplotlib inline\n#plt.style.use('ggplot')\n#ggplot is R based visualisation package that provides better graphics with higher level of abstraction\nimport os","70a20a31":"diamond_data = pd.read_csv(\"..\/input\/diamonds.csv\")","b7879a93":"diamond_data.info()","6b60f153":"diamond_data.head()","31c8ff8f":"diamond_data = diamond_data.drop([\"Unnamed: 0\"],axis=1)\ndiamond_data.head()","c477ce65":"plt.figure(figsize=(20,20))  # on this line I just set the size of figure to 12 by 10.\np=sns.heatmap(diamond_data.corr(), annot=True,cmap='RdYlGn',square=True)  # seaborn has very simple solution for heatmap","3340d9c1":"p=sns.pairplot(diamond_data)","4f5d8eb8":"diamond_data.describe()","c325ec04":"print(\"Number of rows with x == 0: {} \".format((diamond_data.x==0).sum()))\nprint(\"Number of rows with y == 0: {} \".format((diamond_data.y==0).sum()))\nprint(\"Number of rows with z == 0: {} \".format((diamond_data.z==0).sum()))\nprint(\"Number of rows with depth == 0: {} \".format((diamond_data.depth==0).sum()))","df5a813b":"diamond_data[['x','y','z']] = diamond_data[['x','y','z']].replace(0,np.NaN)","515d2245":"diamond_data.isnull().sum()","33817463":"diamond_data.dropna(inplace=True)","023969f6":"diamond_data.shape","f7ac02a5":"diamond_data.isnull().sum()","d693f0e6":"p = diamond_data.hist(figsize = (20,20),bins=150)","111c0233":"p = sns.factorplot(x='cut', data=diamond_data , kind='count',aspect=2.5 )","5244a82e":"p = sns.factorplot(x='cut', y='price', data=diamond_data, kind='box' ,aspect=2.5 )","dbaacb50":"p = diamond_data.hist(figsize = (20,20), by=diamond_data.cut,grid=True)","c271b664":"p = sns.factorplot(x='color', data=diamond_data , kind='count',aspect=2.5 )","932f528f":"p = sns.factorplot(x='color', y='price', data=diamond_data, kind='box' ,aspect=2.5 )","0f15e80e":"p = sns.factorplot(x='clarity', data=diamond_data , kind='count',aspect=2.5 )","d5417e48":"p = sns.factorplot(x='clarity', y='price', data=diamond_data, kind='box' ,aspect=2.5)","56e609c4":"one_hot_encoders_diamond_data =  pd.get_dummies(diamond_data)\none_hot_encoders_diamond_data.head()","2996cd2a":"# a structured approach\ncols = one_hot_encoders_diamond_data.columns\ndiamond_clean_data = pd.DataFrame(one_hot_encoders_diamond_data,columns= cols)\ndiamond_clean_data.head()","539b17d3":"from sklearn.preprocessing import StandardScaler\nsc_X = StandardScaler()\nnumericals =  pd.DataFrame(sc_X.fit_transform(diamond_clean_data[['carat','depth','x','y','z','table']]),columns=['carat','depth','x','y','z','table'],index=diamond_clean_data.index)","4b72bd65":"numericals.head()","d708179e":"diamond_clean_data_standard = diamond_clean_data.copy(deep=True)\ndiamond_clean_data_standard[['carat','depth','x','y','z','table']] = numericals[['carat','depth','x','y','z','table']]","401a6cf1":"diamond_clean_data_standard.head()","6f8ba220":"plt.figure(figsize=(20,20))  # on this line I just set the size of figure to 12 by 10.\np=sns.heatmap(diamond_clean_data.corr(), annot=True,cmap='RdYlGn')  # seaborn has very simple solution for heatmap","a53a286f":"plt.figure(figsize=(20,20))  # on this line I just set the size of figure to 12 by 10.\np=sns.heatmap(diamond_clean_data_standard.corr(), annot=True,cmap='RdYlGn')  # seaborn has very simple solution for heatmap","2eb4e695":"x = diamond_clean_data_standard.drop([\"price\"],axis=1)\ny = diamond_clean_data_standard.price","ec3a9405":"from sklearn.model_selection import train_test_split\ntrain_x, test_x, train_y, test_y = train_test_split(x, y,random_state = 2,test_size=0.3)","8e8893aa":"from sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import r2_score\nfrom sklearn import linear_model\n\nregr = linear_model.LinearRegression()\nregr.fit(train_x,train_y)\ny_pred = regr.predict(test_x)\nprint(\"accuracy: \"+ str(regr.score(test_x,test_y)*100) + \"%\")\nprint(\"Mean absolute error: {}\".format(mean_absolute_error(test_y,y_pred)))\nprint(\"Mean squared error: {}\".format(mean_squared_error(test_y,y_pred)))\nR2 = r2_score(test_y,y_pred)\nprint('R Squared: {}'.format(R2))\nn=test_x.shape[0]\np=test_x.shape[1] - 1\n\nadj_rsquared = 1 - (1 - R2) * ((n - 1)\/(n-p-1))\nprint('Adjusted R Squared: {}'.format(adj_rsquared))","bb9ec299":"las_reg = linear_model.Lasso()\nlas_reg.fit(train_x,train_y)\ny_pred = las_reg.predict(test_x)\nprint(\"accuracy: \"+ str(las_reg.score(test_x,test_y)*100) + \"%\")\nprint(\"Mean absolute error: {}\".format(mean_absolute_error(test_y,y_pred)))\nprint(\"Mean squared error: {}\".format(mean_squared_error(test_y,y_pred)))\nR2 = r2_score(test_y,y_pred)\nprint('R Squared: {}'.format(R2))\nn=test_x.shape[0]\np=test_x.shape[1] - 1\n\nadj_rsquared = 1 - (1 - R2) * ((n - 1)\/(n-p-1))\nprint('Adjusted R Squared: {}'.format(adj_rsquared))","7e3f256e":"rig_reg = linear_model.Ridge()\nrig_reg.fit(train_x,train_y)\ny_pred = rig_reg.predict(test_x)\nprint(\"accuracy: \"+ str(rig_reg.score(test_x,test_y)*100) + \"%\")\nprint(\"Mean absolute error: {}\".format(mean_absolute_error(test_y,y_pred)))\nprint(\"Mean squared error: {}\".format(mean_squared_error(test_y,y_pred)))\nR2 = r2_score(test_y,y_pred)\nprint('R Squared: {}'.format(R2))\nn=test_x.shape[0]\np=test_x.shape[1] - 1\n\nadj_rsquared = 1 - (1 - R2) * ((n - 1)\/(n-p-1))\nprint('Adjusted R Squared: {}'.format(adj_rsquared))","0714735b":"l = list(range(0,len(diamond_clean_data_standard.columns)))","81676abc":"import statsmodels.formula.api as smf\nX = np.append(arr = np.ones((diamond_clean_data_standard.shape[0], 1)).astype(int), values = diamond_clean_data_standard.drop(['price'],axis=1).values, axis = 1)\nX_opt = X[:, l]\nregressor_ols = smf.OLS(endog = diamond_clean_data_standard.price, exog = X_opt).fit()\nregressor_ols.summary()","ac4c1b9b":"l.pop(5)\nX = np.append(arr = np.ones((diamond_clean_data_standard.shape[0], 1)).astype(int), values = diamond_clean_data_standard.drop(['price'],axis=1).values, axis = 1)\nX_opt = X[:, l]\nregressor_ols = smf.OLS(endog = diamond_clean_data_standard.price, exog = X_opt).fit()\nregressor_ols.summary()","9561b073":"l.pop(5)\nX = np.append(arr = np.ones((diamond_clean_data_standard.shape[0], 1)).astype(int), values = diamond_clean_data_standard.drop(['price'],axis=1).values, axis = 1)\nX_opt = X[:, l]\nregressor_ols = smf.OLS(endog = diamond_clean_data_standard.price, exog = X_opt).fit()\nregressor_ols.summary()","03e84eb2":"### What is R Squared?\n\nR2 measures the proportion of the total deviation of Y from its mean which is explained by the regression model. The closer the R2 is to unity, the greater the explanatory power of the regression equation. An R2 close to 0 indicates that the regression equation will have very little explanatory power.\n\n### R2 = RSS\/TSS= 1 - ESS\/TSS\n","fe258de7":"### Mostly the median prices are very low as compared to the the highest price values for all the categories in colors, cut and clarity. The upper quartile is bigger. It shows that whichever category it may be there's a variety of diamonds that are still very expensive. \n### For example in the cut category, be it ideal, fair or any other type, there are diamonds in those categories with high prices. ","82c9b2df":"###  Types of Regressions:\n* Linear Regression\n* Polynomial Regression\n* Logistic Regression\n* Quantile Regression\n* Ridge Regression\n* Lasso Regression\n* ElasticNet Regression\n* Principal Component Regression\n* Partial Least Square Regression\n* Support Vector Regression\n* Ordinal Regression\n* Poisson Regression\n* Negative Binomial Regression\n* Quasi-Poisson Regression\n* Cox Regression","dac8bb99":"#### Let's check if the data has any null values","767bd2ee":"# 3. Stats Model Interpretation and Backward Elimination Technique\n<a id=\"model_stats3\" >","a01a477d":"## 3.1. Selection Techniques in Multiple Regression\n<a id=\"model_stats3.1\" ><\/a>\nThe main approaches are:\n\n1. **Forward selection**, which involves starting with no variables in the model, testing the addition of each variable using a chosen model fit criterion, adding the variable (if any) whose inclusion gives the most statistically significant improvement of the fit, and repeating this process until none improves the model to a statistically significant extent.\n2. **Backward elimination**, which involves starting with all candidate variables, testing the deletion of each variable using a chosen model fit criterion, deleting the variable (if any) whose loss gives the most statistically insignificant deterioration of the model fit, and repeating this process until no further variables can be deleted without a statistically significant loss of fit.\n3. **Bidirectional elimination**, a combination of the above, testing at each step for variables to be included or excluded.","5bd7b169":"## 2.3.2. Ridge Regression Implementation\n<a id=\"model_training2.3.2\" >","118c00be":"## 1.4.3. One hot encoding implementation\n<a id=\"data_handling1.4.3\" >","2c0fa897":"1. Run Linear Regression using all independent variables.\n2. Check out the p-values for all the independent variables.\n3. If you choose the confidence level to be 95% then remove the independent variable having maximum p-value and greater than 0.05.\n4. Rerun Linear Regression on the new data.\n5. Repeat step 2 to 4 till no p-value is greater than 0.05","fafb508d":"## What is Categorical Data?\nCategorical data are variables that contain label values rather than numeric values.\n\nThe number of possible values is often limited to a fixed set.\n\nCategorical variables are often called nominal.\n\nSome examples include:\n\nA \u201cpet\u201d variable with the values: \u201cdog\u201d and \u201ccat\u201c.\nA \u201ccolor\u201d variable with the values: \u201cred\u201c, \u201cgreen\u201d and \u201cblue\u201c.\nA \u201cplace\u201d variable with the values: \u201cfirst\u201d, \u201csecond\u201d and \u201cthird\u201c.\nEach value represents a different category.\n\nSome categories may have a natural relationship to each other, such as a natural ordering.\n\nThe \u201cplace\u201d variable above does have a natural ordering of values. This type of categorical variable is called an ordinal variable.\n\n\n\n### What is the Problem with Categorical Data?\nSome algorithms can work with categorical data directly.\n\nFor example, a decision tree can be learned directly from categorical data with no data transform required (this depends on the specific implementation).\n\nMany machine learning algorithms cannot operate on label data directly. They require all input variables and output variables to be numeric.\n\nIn general, this is mostly a constraint of the efficient implementation of machine learning algorithms rather than hard limitations on the algorithms themselves.\n\nThis means that categorical data must be converted to a numerical form. If the categorical variable is an output variable, you may also want to convert predictions by the model back into a categorical form in order to present them or use them in some application.\n\n\n","366ac332":"### What to do with the zero's in x,y and z?","3379a5fd":"# 1. Data Handling\n<a id=\"data_handling\" >","edd2bc8e":"## 1.2. Univariate and Bivariate Data Exploration\n<a id=\"data_handling1.2\" >","1559fa06":"## Yet to be updated","734a7c67":"# 2. Model Training\n<a id=\"model_training2\" >","61af1bf1":"#### *It somehow makes me feel that these categories have less of an influence on the price. Please let me know in the comment section.*","2332bb5f":"## 2.3. Ridge and Lasso Regression\n<a id=\"model_training2.3\" >","ae658bbf":"#### So far so good but what about the categorical data in the columns cut,clarity and color?? The model doesn't take strings!!","ab605cab":"![](https:\/\/www.tutorialspoint.com\/managerial_economics\/images\/regression_equation.jpg)","3f112795":"![](https:\/\/files.realpython.com\/media\/Introduction-to-Python_Watermarked.48eeee4e1109.jpg)","f4e425f6":"## 1.5. Data Scaling\n<a id=\"data_handling1.5\" >","61d63470":"## 2.2. Linear Regression\n<a id=\"model_training2.2\" >","ff7e4053":"Since the dataset is big enough dropping 20 rows shouldn't cost us much hence the nans have been dropped.","9277ba08":"## 1.4.2. Treatment Techniques for Categorical Data\n<a id=\"data_handling1.4.2\" >","79007be6":"### **Here we can justify the table column because is shows some correlation with the one hot encoded variables so let's not drop it**","020e0f34":"## 1.4. Categorical Data Essense\n<a id=\"data_handling1.4\" >","278ec5d8":"Table of Content:\n\n<a href=\"#data_handling\">1. Data Handling<\/a>\n<br><a href=\"#data_handling1.1\">1.1. Getting sense of the data<\/a>\n<br><a href=\"#data_handling1.2\">1.2. Univariate and Bivariate Data Exploration<\/a>\n<br><a href=\"#data_handling1.3\">1.3. Treatment of data for missing values<\/a>\n<br><a href=\"#data_handling1.4\">1.4. Categorical Data Essense<\/a>\n<br><a href=\"#data_handling1.4.1\">1.4.1. About Categorical Data<\/a>\n<br><a href=\"#data_handling1.4.2\">1.4.2. Treatment Techniques for Categorical Data<\/a>\n<br><a href=\"#data_handling1.4.3\">1.4.3. One hot encoding implementation<\/a>\n<br><a href=\"#data_handling1.5\">1.5. Data Scaling<\/a>\n<br><br><a href=\"#model_training2\">2. Model Training<\/a>\n<br><a href=\"#model_training2.1\">2.1. What is Regression?<\/a>\n<br><a href=\"#model_training2.2\">2.2. Linear Regression<\/a>\n<br><a href=\"#model_training2.3\">2.3. Ridge and Lasso Regression<\/a>\n<br><a href=\"#model_training2.3.1\">2.3.1. Lasso Regression Implementation<\/a>\n<br><a href=\"#model_training2.3.2\">2.3.2. Ridge Regression Implementation<\/a>\n<br><br><a href=\"#model_stats3\">3. Stats Model Interpretation and Backward Elimination Technique<\/a>\n<br><a href=\"#model_stats3.1\">3.1. Selection Techniques in Multiple Regression<\/a>\n<br><a href=\"#model_stats3.2\">3.2. How to perform backward elimination?<\/a>\n<br><a href=\"#model_stats3.3\">3.3. Why Stats Model Library<\/a>\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n","312875ee":"## 1.3. Treatment of data for missing values\n<a id=\"data_handling1.3\" >","61a455e6":"### Let's take a look at the correlation heatmap \n#### One cool thing is that when we plot before and after standardisation the stats are still that same which is just yet another sign of how useful standardisation is ","d67d8bb9":"## Also notice in the code below how adjusted r squared penalises for irrelevant or useless independent variables","bd4159e1":"### The minimum values for x,y and z here are 0 but it is not possible because according to the data description they are the length, width and depth","f2928dbf":"## 2.3.1. Lasso Regression Implementation\n<a id=\"model_training2.3.1\" >","7b6cd2f0":"## 1.4.1. About Categorical Data\n<a id=\"data_handling1.4.1\" >","7f136b36":"Thanks to the resources:\n1. https:\/\/www.tutorialspoint.com\/managerial_economics\/regression_technique.htm\n2. https:\/\/udemy.com\n3. https:\/\/www.r-bloggers.com\/15-types-of-regression-you-should-know\/\n4. https:\/\/en.wikipedia.org\/wiki\/Stepwise_regression","298c7cc4":"### A zero value in these rows means missing data so we can replace the zeros with nan. Another thing to notice is that the depth column doesn't have a single zero value. We know that depth is calculated using the three parameters only.","3d238e7d":"### Few observations from the above plot\n1. x,y, and z have a very strong relation with price but surprisingly depth (which comes from x,y, and z) doesn't has a significant relation with price.\n2. Carat has a strong relation with price\n3. Table doesn't have a significant relation with price or any other variable as well ( We can try dropping that when making the model) ","60929996":"## 3.3. Why Stats Model Library\n<a id=\"model_stats3.3\" ><\/a>\n### StatsModel offers statistics and econometric tools that are top of the line and validated against other statistics software like Stata and R. When you need a variety of linear regression models, mixed linear models, regression with discrete dependent variables, and more \u2013 StatsModels has options. Since we need p-values for backward elimination which is provided in this great report format by StatsModel library it has been used to apply backward elimination technique.","9dae8c6b":"## 3.2. How to perform backward elimination?\n<a id=\"model_stats3.2\" ><\/a>","3e993f2e":"## 2.1. What is Regression?\n<a id=\"model_training2.1\" >","e9100731":"### One hot encoding to the rescue!","fe5a397e":"## Ridge and Lasso regression are powerful techniques generally used for creating parsimonious models in presence of a \u2018large\u2019 number of features. \n\n#### Here \u2018large\u2019 can typically mean either of two things:\n\n1. Large enough to enhance the tendency of a model to overfit (as low as 10 variables might cause overfitting)\n2. Large enough to cause computational challenges. With modern systems, this situation might arise in case of millions or billions of features\nThough Ridge and Lasso might appear to work towards a common goal, the inherent properties and practical use cases differ substantially. If you\u2019ve heard of them before, you must know that they work by penalizing the magnitude of coefficients of features along with minimizing the error between predicted and actual observations. These are called \u2018regularization\u2019 techniques. The key difference is in how they assign penalty to the coefficients:\n\n### Ridge Regression:\nPerforms L2 regularization, i.e. adds penalty equivalent to square of the magnitude of coefficients\nMinimization objective = LS Obj + \u03b1 * (sum of square of coefficients)\n### Lasso Regression:\nPerforms L1 regularization, i.e. adds penalty equivalent to absolute value of the magnitude of coefficients\nMinimization objective = LS Obj + \u03b1 * (sum of absolute value of coefficients)\nNote that here \u2018LS Obj\u2019 refers to \u2018least squares objective\u2019, i.e. the linear regression objective without regularization.\n\n#### For Further Reference: https:\/\/www.analyticsvidhya.com\/blog\/2016\/01\/complete-tutorial-ridge-lasso-regression-python\/","452e6c4a":"## 1.1. Getting sense of the data\n<a id=\"data_handling1.1\" >","0ccf7be1":"### Regression is a statistical technique that helps in qualifying the relationship between the interrelated economic variables. The first step involves estimating the coefficient of the independent variable and then measuring the reliability of the estimated coefficient. This requires formulating a hypothesis, and based on the hypothesis, we can create a function.\n\nIf a manager wants to determine the relationship between the firm\u2019s advertisement expenditures and its sales revenue, he will undergo the test of hypothesis. Assuming that higher advertising expenditures lead to higher sale for a firm. The manager collects data on advertising expenditure and on sales revenue in a specific period of time. This hypothesis can be translated into the mathematical function, where it leads to \u2212\n\n### Y = A + Bx\n\nWhere Y is sales, x is the advertisement expenditure, A and B are constant.\n\nAfter translating the hypothesis into the function, the basis for this is to find the relationship between the dependent and independent variables. The value of dependent variable is of most importance to researchers and depends on the value of other variables. Independent variable is used to explain the variation in the dependent variable. It can be classified into two types \u2212\n\n1. Simple regression \u2212 One independent variable\n\n2. Multiple regression \u2212 Several independent variables\n\n1. Simple Regression\nFollowing are the steps to build up regression analysis \u2212\n\nSpecify the regression model\nObtain data on variables\nEstimate the quantitative relationships\nTest the statistical significance of the results\nUsage of results in decision-making\nFormula for simple regression is \u2212\n\n### Y = a + bX + u\n\nY= dependent variable\n\nX= independent variable\n\na= intercept\n\nb= slope\n\nu= random factor\n\nCross sectional data provides information on a group of entities at a given time, whereas time series data provides information on one entity over time. When we estimate regression equation it involves the process of finding out the best linear relationship between the dependent and the independent variables.\n\n## Method of Ordinary Least Squares (OLS)\nOrdinary least square method is designed to fit a line through a scatter of points is such a way that the sum of the squared deviations of the points from the line is minimized. It is a statistical method. Usually Software packages perform OLS estimation.\n\n### Y = a + bX\n\n## Co-efficient of Determination (R2)\nCo-efficient of determination is a measure which indicates the percentage of the variation in the dependent variable is due to the variations in the independent variables. R2 is a measure of the goodness of fit model. Following are the methods \u2212\n\n## Total Sum of Squares (TSS)\nSum of the squared deviations of the sample values of Y from the mean of Y.\n\n### TSS = SUM ( Yi \u2212 Y)2\n\nYi = dependent variables\n\nY = mean of dependent variables\n\ni = number of observations\n\n## Regression Sum of Squares (RSS)\nSum of the squared deviations of the estimated values of Y from the mean of Y.\n\n### RSS = SUM ( \u1ef6i \u2212 uY)2\n\n\u1ef6i = estimated value of Y\n\nY = mean of dependent variables\n\ni = number of variations\n\n## Error Sum of Squares (ESS)\nSum of the squared deviations of the sample values of Y from the estimated values of Y.\n\n### ESS = SUM ( Yi \u2212 \u1ef6i)2\n\n\u1ef6i = estimated value of Y\n\nYi = dependent variables\n\ni = number of observations\n\n\n","f3f99467":"#### There's an unecessary column which needs to be dropped","b28ee984":"![](https:\/\/www.tutorialspoint.com\/managerial_economics\/images\/error_sum_of_squares.jpg)","a5d2c216":"### How to Convert Categorical Data to Numerical Data?\nThis involves two steps:\n\n### 1. Integer Encoding\nAs a first step, each unique category value is assigned an integer value.\n\nFor example, \u201cred\u201d is 1, \u201cgreen\u201d is 2, and \u201cblue\u201d is 3.\n\nThis is called a label encoding or an integer encoding and is easily reversible.\n\nFor some variables, this may be enough.\n\nThe integer values have a natural ordered relationship between each other and machine learning algorithms may be able to understand and harness this relationship.\n\nFor example, ordinal variables like the \u201cplace\u201d example above would be a good example where a label encoding would be sufficient.\n\n### 2. One-Hot Encoding\nFor categorical variables where no such ordinal relationship exists, the integer encoding is not enough.\n\nIn fact, using this encoding and allowing the model to assume a natural ordering between categories may result in poor performance or unexpected results (predictions halfway between categories).\n\nIn this case, a one-hot encoding can be applied to the integer representation. This is where the integer encoded variable is removed and a new binary variable is added for each unique integer value.\n\nIn the \u201ccolor\u201d variable example, there are 3 categories and therefore 3 binary variables are needed. A \u201c1\u201d value is placed in the binary variable for the color and \u201c0\u201d values for the other colors.\n\n\nFor further reference : https:\/\/machinelearningmastery.com\/why-one-hot-encode-data-in-machine-learning\/"}}