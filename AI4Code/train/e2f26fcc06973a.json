{"cell_type":{"787d828e":"code","6e9e8c73":"code","4da7dbee":"code","1a7017e8":"code","5a07eeda":"code","e65772f7":"code","92ed4675":"code","fb9a8af7":"code","a848cf0c":"code","a679d1ae":"code","32ff8ad2":"code","ce8fc2bc":"markdown","cbbcf280":"markdown","b5d11924":"markdown","52185f20":"markdown","f0726843":"markdown","954d2d0a":"markdown","af5baa64":"markdown","6a6a9645":"markdown","181a482c":"markdown","b8c99712":"markdown","e45c22b5":"markdown","4c9be34c":"markdown"},"source":{"787d828e":"# Necessary packages\nimport pandas as pd # Data manipulation\nimport matplotlib.pyplot as plt # Plotting\nimport re, io, json, requests # Essentialls\nimport tweepy as tp # API to interact with twitter\nimport nltk # API for feature engineering (NLP tools)\n# Note: if this is your first time using nltk,consider running:\n#nltk.download('stopwords') ","6e9e8c73":"#!\/usr\/bin\/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\nCreated on Sat Jul 14 17:03:17 2018\n@author: andrewcaide\n\"\"\"\n\n'''\n# Original code, stored in 'twitter_credentials.py':\nglobal CONSUMER_KEY\nglobal CONSUMER_SECRET\nglobal ACCESS_TOKEN\nglobal ACCESS_TOKEN_SECRET\n\nCONSUMER_KEY = \"cxxxxxxxxxxxxxf\"\nCONSUMER_SECRET = \"xxxxxxxxk\"\nACCESS_TOKEN = \"41xxxxxxxxxxxxxxxxxxxxxxxM\"\nACCESS_TOKEN_SECRET = \"1xxxxxxxxx2\"\n''' #---------------------- END ORIGINAL FILE -----------------------\n\n# Work-around:\nclass twitter_creds:\n    def __init__(self, credential_list):\n        self.CONSUMER_KEY = credential_list[0]#CONSUMER_KEY\n        self.CONSUMER_SECRET = credential_list[1]#CONSUMER_SECRET\n        self.ACCESS_TOKEN = credential_list[2]#ACCESS_TOKEN\n        self.ACCESS_TOKEN_SECRET = credential_list[3]#ACCESS_TOKEN_SECRET\n\n        \n# ***********************************************************************************\n# ***********************************************************************************\n\n#                         !! IMPORTANT !!\n#                    NEEDS TO BE EDITED BY **YOU**\n# Delete the string and replace it with a string containing your specific key\n\nCONSUMER_KEY = \"your_consumer_key [EDIT THIS]\"\nCONSUMER_SECRET = \"your_consumer_secret_key [EDIT THIS]\"\nACCESS_TOKEN = \"your_access_token [EDIT THIS]\"\nACCESS_TOKEN_SECRET = \"your_access_token_secret_key [EDIT THIS]\"\n\n# ***********************************************************************************\n# ***********************************************************************************\n\n# Keep the rest untouched.\ncredentials = [CONSUMER_KEY,CONSUMER_SECRET,ACCESS_TOKEN, ACCESS_TOKEN_SECRET]\n    \ntwitter_credentials = twitter_creds(credentials)\nprint(\"Consumer key: {}\".format(twitter_credentials.CONSUMER_KEY))\nprint(\"Access token: {}\".format(twitter_credentials.ACCESS_TOKEN))","4da7dbee":"# Helper functions\ndef authenticate_twitter_app():\n    \n    # Authentication\n    consumer_key = twitter_credentials.CONSUMER_KEY \n    consumer_secret = twitter_credentials.CONSUMER_SECRET \n    auth = tp.OAuthHandler(consumer_key, consumer_secret)\n\n    # token stuff\n    access_token = twitter_credentials.ACCESS_TOKEN \n    access_token_secret = twitter_credentials.ACCESS_TOKEN_SECRET \n    auth.set_access_token(access_token, access_token_secret)\n    return(auth)\n\ndef get_user_timeline_tweets(twitter_client, user_list, num_tweets):\n\n    tweets = []\n    for user in user_list:\n        # This returns tweets & re-tweets, 10 at a time\n        # We need to research more about getting larger volume back in time with max_id, since_id, etc.\n        print(f'Getting {num_tweets} tweets for {user}. ', end = '')\n        try:\n            for tweet in tp.Cursor(twitter_client.user_timeline, id=user).items(num_tweets):\n                tweets.append(tweet)    \n        except tp.RateLimitError:\n            print(f'SLEEPING DUE TO RATE LIMIT ERROR!!!!')\n            time.sleep(15 * 60)\n        except Exception as e:\n            print(f'SOME ERROR OCCURRED...PASSING!!!')\n            print(e.__doc__)\n            pass\n    return(tweets)\n         \ndef produce_status_LoDs(statuses):\n    \n    tweet_LoD = []\n    user_LoD = []\n    for status in statuses:\n        \n        tweet_dict = {}\n        user_dict = {}\n        \n        tweet_dict['user_id'] = status.author.id\n        tweet_dict['user_screen_name'] = status.author.screen_name\n        #tweet_dict['created_at'] = status.created_at.isoformat()\n        tweet_dict['created_at'] = str(status.created_at)\n        tweet_dict['id'] = status.id\n        tweet_dict['id_str'] = status.id_str\n        tweet_dict['text'] = status.text\n        tweet_dict['source'] = status.source\n        tweet_dict['truncated'] = status.truncated\n        tweet_dict['retweet_count'] = status.retweet_count\n        tweet_dict['favorite_count'] = status.favorite_count\n        tweet_dict['lang'] = status.lang\n        tweet_dict['is_tweet'] = ((re.search('RT', status.text) == None))\n        ###Preimium API only?:  tweet_dict['retweeted_status'] = status.retweeted_status\n        ###Preimium API only?:  tweet_dict['reply_count'] = status.reply_count\n        ###Premium API only?:  tweet_dict['possibly_sensitive'] = status.possibly_sensitive\n        \n        tweet_LoD.append(tweet_dict)\n                \n        # user data\n        user_dict['id'] = status.author.id\n        #user_dict['id_str'] = status.author.id_str\n        user_dict['name'] = status.author.name\n        user_dict['screen_name'] = status.author.screen_name\n        user_dict['location'] = status.author.location\n        #user_dict['url'] = status.author.url\n        user_dict['description'] = status.author.description\n        user_dict['verified'] = status.author.verified\n        user_dict['followers_count'] = status.author.followers_count\n        user_dict['listed_count'] = status.author.listed_count\n        user_dict['favourites_count'] = status.author.favourites_count\n        user_dict['statuses_count'] = status.author.statuses_count\n        #user_dict['created_at'] = status.author.created_at.isoformat()\n        user_dict['created_at'] = str(status.author.created_at)\n        #user_dict['utc_offset'] = status.author.utc_offset\n        user_dict['time_zone'] = status.author.time_zone\n        user_dict['lang'] = status.author.lang\n        \n        # Non-twitter, enriched field that dev team is manually managing\n        if status.author.verified:\n            user_dict['known_bot'] = False\n        else:\n            user_dict['known_bot'] = False # We will change to true when importing known bots\n        user_LoD.append(user_dict)\n        \n    return(tweet_LoD, user_LoD)","1a7017e8":"auth = authenticate_twitter_app()\ntwitter_client = tp.API(auth)","5a07eeda":"#!\/usr\/bin\/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\nCreated on Sat Jul 14 22:45:11 2018\n@authors: andrewcaide and eumarassis\nbot list pulled from:\n    https:\/\/www.nbcnews.com\/tech\/social-media\/now-available-more-200-000-deleted-russian-troll-tweets-n844731\n\"\"\"\n\ntweets_url = \"http:\/\/nodeassets.nbcnews.com\/russian-twitter-trolls\/tweets.csv\"\ntweets_content = requests.get(tweets_url).content\n\nbots_url = \"http:\/\/nodeassets.nbcnews.com\/russian-twitter-trolls\/users.csv\"\nbots_content = requests.get(bots_url).content\n\nbots = pd.read_csv(io.StringIO(bots_content.decode('utf-8')))\ntweets = pd.read_csv(io.StringIO(tweets_content.decode('utf-8')))\n\n#print(\"Cursory examination of the tweets dataframe:\")\n#print(tweets.dtypes)\n\ndef produce_bot_LoDs(bots, tweets):\n    '''\n    Read in dataframe of bots and their tweets. Organize them according to Mark's format. \n    \n    Args: \n        dataframes - Bots, Tweets\n    \n    Returns: \n        Cleaned dataframe, with an extra column: 'Bot_Status' = True\n    '''\n    # Fix tweets:\n    tweet_colnames = ['created_at','favorite_count','id','id_str','is_tweet','lang','retweet_count',\n                 'source','text','truncated','user_screen_name']\n    \n    # Ask mark to see if he can pull out hashtags from his tweets!!\n    tweets['truncated'] = False\n    tweets['is_tweet'] = tweets['text'].apply(lambda x: False if str(x).find('RT') == -1 else True)\n    tweets = tweets.rename(columns={'tweet_id': 'id'}) \n    tweets['id_str'] = str(tweets['id'])\n    tweets['user_screen_name'] = tweets['user_key']\n    tweets['lang'] = 'en'\n    #tweets['created_at'] = tweets['created_str']\n    \n    tweet_output = tweets[tweet_colnames]\n    # Fix users:\n    user_colnames = ['created_at','description','favourites_count','followers_count','id',\n                      'lang','listed_count','location','name','screen_name','statuses_count',\n                      'time_zone','verified']\n    \n    bots_output = bots[user_colnames]\n    bots_output['known_bot'] = True\n    return(bots_output, tweet_output)\n\nbots_Clean, tweets_Clean = produce_bot_LoDs(bots, tweets)\nprint(bots_Clean.shape)","e65772f7":"# This might not run; it has to be executed locally.\nuser_json  = bots_Clean.to_json(orient='records')\ntweet_json = tweets_Clean.to_json(orient='records')\n\n'''\nwith open('data\/b_tweet_table_out.json', 'w') as outfile:  \n    json.dump(tweet_json, outfile)\n    \nwith open('data\/b_user_table_out.json', 'w') as outfile:  \n    json.dump(user_json, outfile)\n'''\n# Uncomment above if you'd like to save the data.","92ed4675":"# Define list of non-verified users. This will be our verification data.\nnv_user_list=['@oovoo_javer_ceo','@slactochile','@jaymijams','@ChefDoubleG',\n'@mrpotato','@Rcontreras777','@MissMaseline','@mike434prof','@NonativeEuan',\n'@mbspyder','@vaggar99','@AfifaAssel','@esruben','@Victorhuvaldez','@JesiaQuispe',\n'@TurnbowRosemary','@todaav','@Pasho53013866','@tonyaba18632641','@ghostsignal1',\n'@chubbyleena','@genre_addis','@DarrrellWalraven','@onegearrico','@abadreen',\n'@somerice','@unsaltCarthage','@Cmiln01','@Kitter_44','@ashish3vedi',\n'@HugoMunissa','@TODthegiant','@LissyBee4','@anna_adamcova','@jerwinbroas2',\n'@Queenprominent','@IndianhawkFB','@7998472','@rjerome217','@CharlesNcharg14',\n'@Truthseeker1237','@guywpt','@bernoroel','@DavidOrr4','@backworldsman1',\n'@jimmythecoat','@wrwveit','@TriggaGhj','@duckmesick','@tyjopow','@mskoch',\n'@jaspect_wan','@WiseSparticus','@Mr_AdiSingh','@Live9Fortknox','@mrfridberg',\n'@vibolnet','@paulanderson801','@Supanovawhatevs','@politicalpatfan','@DAvidofny1',\n'@Tvat_64','@S_Nenov','@HglundNiklas','@LBoertjes','@anBiabhail','@iantuck99',\n'@JumahSaisi','@QteleOluwatobi1','@woodgrovect','@LeeThecritch','@mkinisa1',\n'@Anfieldvianne','@DonUbani','@JardyRaines','@BagbyCarole','@JopiHuangShen',\n'@scottwms84','@gander99','@biller_jon','@aeal_ve','@DesjardinsKarla','@LBonxe',\n'@joey_gomez','@anthoamick844','@Brettwadeart','@zac_slocumb','@NatoNogo','@Twu76',\n'@Monoclops37','@dwhite612','@_bwright','@InsaneGamer1983','@AsaWatts6','@Niallpolke',\n'@84newsnerny','@BrownWilliamF','@MariusD53205774']\n\n# Define list of verified users. Will use these accounts as confirmed 'non-bot's.\nv_user_list=['@BarackObama','@rihanna','@realDonaldTrump','@secupp','@ChairmanKimNK',\n'@taylorswift13','@ladygaga','@TheEllenShow','@Cristiano','@YouTube','@katyperry',\n'@jtimberlake','@KimKardashian','@ArianaGrande','@britneyspears','@cnnbrk','@BillGates',\n'@narendramodi','@Oprah','@SecPompeo','@nikkihaley','@SamSifton','@FrankBruni',\n'@The_Hank_Poggi','@krassenstein','@TheJordanRachel','@MrsScottBaio',\n'@ClaireBerlinski','@java','@JakeSherman','@jaketapper','@jakeowen','@AndrewCMcCarthy',\n'@tictoc','@thedailybeast','@mitchellvii','@GadSaad','@Joy_Villa','@RashanAGary',\n'@DallasFed','@Gab.ai','@bigleaguepolitics','@Circa','@EmilyMiller','@francesmartel',\n'@andersoncooper','@nico_mueller','@NancyGrace','@washingtonpost','@ThePSF', '@pnut',\n'@EYJr','@MCRofficial','@RM_Foundation','@tomwaits','@burbunny','@justinbieber',\n'@TherealTaraji','@duttypaul','@AvanJogia','@AlecJRoss','@s_vakarchuk','@elongmusk',\n'@StephenColletti','@jem','@tonyparker','@vitorbelfort','@jeff_green22',\n'@TomJackson57','@robbiewilliams','@AshleyMGreene','@edhornick','@mattdusk',\n'@ReggieEvans30','@RachelNichols1','@AndersFoghR','@PalmerReport',\n'@KAKA','@Robbie_OC','@josiahandthe','@OKKenna','@CP3','@crystaltamar',\n'@MichelleDBeadle','@Jonnyboy77','@kramergirl','@johnwoodRTR','@StevePeers',\n'@AdamSchefter','@georgelopez','@CharlieDavies9','@Nicole_Murphy',\n'@vkhosla','@NathanPacheco','@SomethingToBurn','@jensstoltenberg','@Devonte_Riley',\n'@FreddtAdu','@Erik_Seidel','@Pamela_Brunson','@MMRAW','@russwest44','@shawnieora',\n'@wingoz','@ToddBrunson','@NathanFillion','@LaurenLondon','@francescadani',\n'@howardhlederer','@MrBlackFrancis','@GordonKljestan','@thehitwoman','@KeriHilson',\n'@druidDUDE','@jimjonescapo','@myfamolouslife','@PAULVANDYK','@SteveAustria',\n'@bandofhoreses','@jaysean','@justdemi','@MaryBonoUSA','@PaulBrounMD','@jrich23','@Eve6',\n'@st_vincent','@Padmasree','@jamiecullum','@GuyKawasaki','@PythonJones','@sffed',\n'@howardlindzon','@xonecole','@AlisonSudol','@SuzyWelch','@topchefkevin','@MarcusCooks',\n'@Rick_Bayless','@ShaniDavis','@scottylago','@danielralston','@crystalshawanda',\n'@TheRealSimonCho','@ItsStephRice','@IvanBabikov','@DennyMdotcom','@TFletchernordic',\n'@RockneBru86','@JuliaMancuso','@RyanOBedford','@speedchick428','@JennHeil',\n'@katadamek','@kathryn_kang','@alejandrina_gr','@RaymondArroyo','@JonHaidt',\n'@DKShrewsbury','@faisalislam','@miqdaad','@michikokakutani','@mehdirhasan','@AbiWilks',\n'@hugorifkind','@kylegriffin1','@timothy_stanley','@NAXWELL','@PT_Dawson','@MaiaDunphy',\n'@zachheltzel','@KatyWellhousen','@NicholasHoult','@ryanbroems','@LlamaGod','@boozan',\n'@DarrenMattocks','@BraulioAmado','@bernierobichaud','@ThisisSIBA','@Jill_Perkins3',\n'@D_Breitenstein','@George_McD','@RedAlurk','@NickRobertson10','@kevinvu','@Henry_Kaye',\n'@Chris_Biele','@tom_watson','@MikeSegalov','@edballs','@TalbertSwan','@eugenegu',\n'@Weinsteinlaw','@BrittMcHenry','@ava','@McFaul','@DaShanneStokes','@funder',\n'@BrunoAmato_1','@DirkBlocker','@TrevDon','@DavidYankovich','@KirkDBorne','@JohnLegere',\n'@JustinPollard','@MattDudek','@CoachWash56','@RexxLifeRaj','@SageRosenfels18']\n\n\nprint(\"Number of non-verified users: {}\".format(len(nv_user_list)))\nprint(\"Number of verified users: {}\".format(len(v_user_list)))","fb9a8af7":"# To accurately classify our users, let's pull at least 100 tweets from each account.\nnum_tweets = 100\n\n# Helper function to get fixed number of tweets and put in results\ndef get_tweets(twitter_client,v_user_list, num_tweets):\n    statuses = get_user_timeline_tweets(twitter_client,v_user_list, num_tweets)\n    \n    # Create list to write to json file\n    tweet_LoD, user_LoD = produce_status_LoDs(statuses)\n    \n    # Put in DF in case you skip the out\/in via json below\n    tweet_df = pd.DataFrame(tweet_LoD)\n    user_df = pd.DataFrame(user_LoD)\n    return(tweet_df, user_df)\n","a848cf0c":"# Get verified users, write them to HD\nv_tweet_df, v_user_df  = get_tweets(twitter_client,v_user_list, num_tweets)\n\nuser_json = v_user_df.to_json(orient='records')\ntweet_json = v_tweet_df.to_json(orient='records')\n'''\nwith open('v_tweet_table_out.json', 'w') as outfile:  \n    json.dump(tweet_json, outfile)\nwith open('v_user_table_out.json', 'w') as outfile:  \n    json.dump(user_json, outfile)\n'''\n\n# Get unverified users, write them to HD\nnv_tweet_df, nv_user_df  = get_tweets(twitter_client,nv_user_list, num_tweets)\n\nuser_json = nv_user_df.to_json(orient='records')\ntweet_json = nv_tweet_df.to_json(orient='records')\n'''\nwith open('data\/v_tweet_table_out.json', 'w') as outfile:  \n    json.dump(tweet_json, outfile)\nwith open('data\/v_user_table_out.json', 'w') as outfile:  \n    json.dump(user_json, outfile)\n'''\n# Uncomment above if you'd like to save the data.","a679d1ae":"user_df = nv_user_df.append(v_user_df) #, ignore_index=True)\ntweet_df = nv_tweet_df.append(v_tweet_df) #, ignore_index=True)\n\nb_tweet_df = pd.read_json(tweet_json)\nb_user_df = pd.read_json(user_json)","32ff8ad2":"'''\nwith open('data\/b_tweet_table_out.json') as json_file:  \n    tweet_json = json.load(json_file)\n\nwith open('data\/b_user_table_out.json') as json_file:  \n    user_json = json.load(json_file)\n    \ntweets_Clean = pd.read_json(tweet_json)\nbots_Clean = pd.read_json(user_json)\n'''\n\nfinal_user_dffinal_us  = user_df.append(bots_Clean)\nfinal_tweet_df = tweet_df.append(tweets_Clean)\n\n'''\nwith open('data\/final_tweet_master.json', 'w') as outfile:  \n    json.dump(tweet_json, outfile)\n    \nwith open('data\/final_user_master.json', 'w') as outfile:  \n    json.dump(user_json, outfile)\n'''\n# Uncomment above if you'd like to save the data.","ce8fc2bc":"# Twitter Bot Classification Project\n\n\n## Foreword\n\nThis project was completed in fulfillment of Harvard's CS109 Data Science taught in the summer semester of 2018. The inspiration for this assignment comes from a personal fascination with what can be done with Natural Language Processing (NLP), specifically how NLP can be used to gain large-scale insight (such as discussion topics, feelings) without the need to interact with the population though surveys or questionaires. While the latter methods of data collection provide irreplacable value, sampling directly from the population has the potential to provide faster, unfiltered, and honest insights at the expense of some accuracy.\n\nThe aim of this project was to gain proficiency with the tools used in large scale data science-based NLP projects: performing essential data gathering and mining, storing\/accessing data in\/from cloud-based databases, using NLP APIs from local and cloud-based APIs, and use statistical techniques (bagging, boosting, random forests) to classify (predict) the likelihood of a twitter account being a bot. \n\n\n## Introduction\n\n\n### Project Background and Purpose\n\nMost people who use twitter are aware of the possibility that tweets received are tweets generated by computer algorithms or \u2018bots\u2019. What most people may struggle with are 'innocuous' bot behaviors, whos activities range from 'liking' posts and 'commenting' on them, which may contribute to increasing the original post's legitimacy. In this context, the bots aim to resonate the message or sentiment of the post, and if there are 10k+ likes and reshares, it's tempting to think the post may be legitimate. The primary concern is that bots can cause societal damage by propagating \u2018fake news\u2019 (and associated harmful sentiments), thereby planting harmful and influencing ideology. The most prominent potential impact of \u2018fake news\u2019 is on how recipients of fake news vote in elections, both in the US and abroad.\n\nBots are dangerous. To mitigate risks associated with bot activity, Twitter takes many steps which, allegidly, include the use of machine learning algorithms to detect and terminate bot accounts.  This project aims to tackle their problem by exploring a variety of modeling tools learned throughout the semester.\n\n\n\n### Project Overview\n\nThis project spans three notebooks. Outlined below are the steps (and their notebooks):\n\n**1. Notebook 1: Preparation** [*This Notebook*]\n1.  Data Collection\n2. Storage\n3. EDA\n\n**2. Notebook 2: Feature Engineering** [Found Here](https:\/\/www.kaggle.com\/housemusic\/twitter-bot-detection-feature-engineering?scriptVersionId=5625424) \n4. NLP with Microsoft Azure\n5. NLP with NLTK\n6. EDA\n\n**3. Notebook 3: Modeling**\n7. Modeling\n8. Results\/Conclusion\n\n\n### Notes\n\n***1***.  This notebook captures our data collection process, however I **recommend** running these steps locally if you are inclined. The reasons for this is that you need to modify the notebook to include your private Twitter API credentials to recreate the data collection steps in project. \n\n***2***.  This issue resonates into the second notebook. Microsoft Azure is used to collect high-level NLP features (sentiment scores and key phrases). To overcome the previously outlined issue, data is supplied in the third notebook. \n\n***3***. This notebook  only contains pre-modeling items: Data Collection and Storage. ","cbbcf280":"### Merge the user tweets with the bot tweets.\n\nLastly, we'll combine the tweets and store them locally. If you are running this notebook locally, the bot tweets are re-loaded below. ","b5d11924":"### Import the remaining helper functions and authenticate your credentials:","52185f20":"### Take a look at the results printed above. \n\nThere should be few errors - few of the supplied accounts are inactive. If all of the results produce errors, make sure your credentials are correct! The last few steps involve merging all three collections (bots, verified, and unverified), tweets and user information. \n","f0726843":"### Store the tweets\n\nFor the purpose of an online notebook, storing the files does not have any practical sense. Feel free to uncomment and run the save\/open commands on a local copy of this notebook. Be sure to have a folder named 'data' in the same directory where this journal is stored.","954d2d0a":"### Reading in **user** tweets\n\nThis section reads Tweets available on Twitter. Beneath are two lists: unverified users (nv_user) and verified users (v_user). Both lists of verified and unverified users were (ineffeciently) collected by browsing through twitter.  These lists may be edited to your discretion. ","af5baa64":"### Helper function for reading in tweets.","6a6a9645":"## This concludes the data collection\/cleaning segment of this project. Thank you for following along!\n\nIn the coming days I shall upload the next two journals in this project. \n\n\nCheers\n\n-Andrew","181a482c":"## Data Collection \n\n### Run locally or skip! (see Notes above)\n\nThis process is outlined below.\n\n1. Load essential libraries\n2. Read and clean tweets\n3. Store tweets\n\n### Loading Libraries","b8c99712":"## Helper functions for cleaning and pulling data\n\n#### The first block here needs to be edited with your credentials. \n\n**IMPORTANT:** Ideally this is stored locally in a file.  The file I used, twitter_credentials.py, would be loaded in with 'import twitter_credentials' and the variables would be accessed with 'twitter_credentials.CONSUMER_KEY'.\n\nTo overcome this problem on kaggle (and avoid problems with previously written code), we're making a class object called twitter_credentials. ** Please be sure to edit the personal keys\/token section if you want to execute the code!**","e45c22b5":"### Reading in **bot** tweets\n\nThis section reads Tweets from known bots that have been publishded to nbcnews.com at the following address:\n\nhttps:\/\/www.nbcnews.com\/tech\/social-media\/now-available-more-200-000-deleted-russian-troll-tweets-n844731\n\n> **Run this only once - takes a while to run. **","4c9be34c":"### Use the helper functions to obtain the tweets and save them as json files.\n\nAgain, the steps requiring saving locally (\"with open...) are commented out."}}