{"cell_type":{"9e458e3a":"code","382a4c09":"code","85df2af4":"code","64219260":"code","91cd5c76":"code","c1356ae2":"code","e2eac2a3":"markdown","2eb271e6":"markdown","e906b4e8":"markdown","674ad486":"markdown","84d1e336":"markdown","79136949":"markdown","0a81fd84":"markdown"},"source":{"9e458e3a":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport os, sys\nimport time\n\nimport torch\nfrom torch.autograd import Variable\nfrom tqdm.notebook import tqdm\nimport sklearn.preprocessing as pre\n\nplt.rcParams['figure.figsize'] = [10,10]\n\n# Import data\ndf = pd.DataFrame()\ndf['plastic'] = pd.Series([100, 50, 30])\ndf['paper'] = pd.Series([0, 50, 0])\ndf['glass'] = pd.Series([0, 50, 90])\ndf['student'] = pd.Series([1, 0, 0])\ndf['worker'] = pd.Series([0, 0, 1])\ndf['elder'] = pd.Series([0, 1, 0])\n\ndata_x = np.array(df[[\"plastic\",\"paper\",\"glass\"]], dtype=np.float32)\ndata_y = np.array(df[[\"student\",\"worker\",\"elder\"]], dtype=np.float32)\n\nx_train = torch.from_numpy(data_x)\ny_train = torch.from_numpy(data_y)","382a4c09":"def train(model, optimizer, losslogger, start_epoch, num_epoch, run_id, X, y, checkpoint_name, verbose=False):\n\n    global_iteration = 0\n    loss_function = torch.nn.MSELoss()\n\n    t = tqdm(range(start_epoch,start_epoch+num_epoch))\n\n    for epoch in t:\n        input = Variable(X)\n        target = Variable(y)\n\n        # forward\n        out = model(input)\n        loss = loss_function(out, target)\n\n        # backward\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        # show\n        t.set_description('Epoch[{}\/{}], loss: {:.6f}'\n            .format(epoch + 1, num_epoch, loss.data.item()))\n        \n        # set in logs\n        df = pd.DataFrame()\n        df['chackpoint_name'] = pd.Series(checkpoint_name)\n        df['epoch'] = pd.Series(epoch)\n        df['Loss'] = pd.Series(loss.data.item())\n        df['run'] = run_id\n        losslogger = losslogger.append(df)\n\n\n    if verbose==True:\n        # Print model's state_dict\n        print(\"Model's state_dict:\")\n        for param_tensor in model.state_dict():\n            print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())\n\n        # Print optimizer's state_dict\n        print(\"Optimizer's state_dict:\")\n        for var_name in optimizer.state_dict():\n            print(var_name, \"\\t\", optimizer.state_dict()[var_name])\n\n        # predicting\n        print(model(torch.tensor([[500, 500, 500]], dtype=torch.float32)))\n\n    state = {'epoch': epoch + 1, 'state_dict': model.state_dict(),\n             'optimizer': optimizer.state_dict(), 'losslogger': losslogger, }\n    torch.save(state, f'{checkpoint_name}')\n\ndef load_checkpoint(model, optimizer, losslogger, filename):\n    # Note: Input model & optimizer should be pre-defined.  This routine only updates their states.\n    start_epoch = 0\n    if os.path.isfile(filename):\n        print(\"=> loading checkpoint '{}'\".format(filename))\n        checkpoint = torch.load(filename)\n        start_epoch = checkpoint['epoch']\n        model.load_state_dict(checkpoint['state_dict'])\n        optimizer.load_state_dict(checkpoint['optimizer'])\n        losslogger = checkpoint['losslogger']\n        print(\"=> loaded checkpoint '{}' (epoch {})\"\n                  .format(filename, checkpoint['epoch']))\n    else:\n        print(\"=> no checkpoint found at '{}'\".format(filename))\n\n    return model, optimizer, start_epoch, losslogger\n","85df2af4":"# Start\nstart_epoch = 0\n\n# number of epochs\nnum_epoch = 100\n\n# Logger\nlosslogger = pd.DataFrame()\n\n# Model\nmodel = torch.nn.Sequential(\n    torch.nn.Linear(3,3, bias=True),\n    torch.nn.ReLU(),\n    torch.nn.Linear(3,3, bias=True),\n    torch.nn.ReLU(),\n    torch.nn.Linear(3,3, bias=True),\n    torch.nn.ReLU(),\n    torch.nn.Softmax(dim=1)\n)\n\n# optimizer\noptimizer = torch.optim.SGD(model.parameters(), lr=1e-2, momentum=0.9)\n\n\n# Checkpoint name\ncheckpoint_name = 'checkpoint.pth.tar'\n\ntrain(model, optimizer, losslogger, start_epoch, num_epoch, 0, x_train, y_train, checkpoint_name)\ntime.sleep(8)","64219260":"# Load and continute train \/\/ run 1\nmodel, optimizer, start_epoch, losslogger = load_checkpoint(model, optimizer, losslogger, filename=checkpoint_name)\n!rm 'checkpoint.pth.tar'\ntrain(model, optimizer, losslogger, start_epoch, num_epoch, 1, x_train, y_train, checkpoint_name)\ntime.sleep(8)\n\n# Load and continute train \/\/ run 2\nmodel, optimizer, start_epoch, losslogger = load_checkpoint(model, optimizer, losslogger, filename=checkpoint_name)\n!rm 'checkpoint.pth.tar'\ntrain(model, optimizer, losslogger, start_epoch, num_epoch, 2, x_train, y_train, checkpoint_name)\ntime.sleep(8)\n\n# Load and continute train \/\/ run 3\nmodel, optimizer, start_epoch, losslogger = load_checkpoint(model, optimizer, losslogger, filename=checkpoint_name)\n!rm 'checkpoint.pth.tar'\ntrain(model, optimizer, losslogger, start_epoch, num_epoch, 3, x_train, y_train, checkpoint_name)\ntime.sleep(8)\n\n# Load and continute train \/\/ run 4\nmodel, optimizer, start_epoch, losslogger = load_checkpoint(model, optimizer, losslogger, filename=checkpoint_name)\n!rm 'checkpoint.pth.tar'\ntrain(model, optimizer, losslogger, start_epoch, num_epoch, 4, x_train, y_train, checkpoint_name)\ntime.sleep(8)\n","91cd5c76":"losslogger","c1356ae2":"count = 0\nfor run_id in tqdm(range(len(losslogger.run.unique()))):\n    df = losslogger[losslogger.run==run_id]\n    n = len(df)\n    plt.plot(np.arange(count,count+n),losslogger[losslogger.run==run_id].Loss.values);\n    count += n\nplt.show()","e2eac2a3":"## Verify Loss is Continuous Across Loading Cycles","2eb271e6":"## Load and Continue Training","e906b4e8":"## First Training Loop","674ad486":"# References:\n- https:\/\/discuss.pytorch.org\/t\/loading-a-saved-model-for-continue-training\/","84d1e336":"## Import and Data I\/O","79136949":"# <center> Demo for Continuing Training with Checkpoints","0a81fd84":"## Utility Functions"}}