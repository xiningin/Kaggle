{"cell_type":{"187afdb4":"code","c7c4623b":"code","94149863":"code","d7582cb0":"code","e0b1503e":"code","283d7ae8":"code","8f1e513c":"code","efe5d13e":"code","6bffa0c2":"code","8ef18521":"code","7da5cc8e":"code","9e97b0bc":"code","6ce46ae4":"code","254e86cb":"code","02efdc20":"code","f5297557":"code","f6c5ed3f":"code","d6eabbfb":"code","b18ad02b":"code","e84a8419":"code","f27b3c90":"code","cebb193e":"code","b0ed66d8":"code","17d225a7":"code","8e773404":"code","9a13b559":"code","ffa2ce57":"markdown","e5a09995":"markdown","8777b41d":"markdown","226f99a9":"markdown","0721141f":"markdown","04d4faed":"markdown","2c639616":"markdown","492ef4ae":"markdown","45963ba8":"markdown","377f91be":"markdown"},"source":{"187afdb4":"# Update to transformers 2.8.0\n!pip install -q transformers --upgrade\n!pip show transformers","c7c4623b":"import os\nimport pickle\n\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport tensorflow.keras.layers as L\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, average_precision_score, roc_auc_score\nimport matplotlib.pyplot as plt\nimport transformers\nfrom transformers import AutoTokenizer, TFAutoModel, TFElectraModel, ElectraTokenizer\nfrom tqdm.notebook import tqdm\nfrom tokenizers import BertWordPieceTokenizer","94149863":"tqdm.pandas()","d7582cb0":"def fast_encode(texts, tokenizer, chunk_size=256, maxlen=512, enable_padding=False):\n    \"\"\"\n    https:\/\/www.kaggle.com\/xhlulu\/jigsaw-tpu-distilbert-with-huggingface-and-keras\n    \n    ---\n    \n    Inputs:\n        tokenizer: the `fast_tokenizer` that we imported from the tokenizers library\n    \"\"\"\n    tokenizer.enable_truncation(max_length=maxlen)\n    if enable_padding:\n        tokenizer.enable_padding(max_length=maxlen)\n    \n    all_ids = []\n    \n    for i in tqdm(range(0, len(texts), chunk_size)):\n        text_chunk = texts[i:i+chunk_size].tolist()\n        encs = tokenizer.encode_batch(text_chunk)\n        all_ids.extend([enc.ids for enc in encs])\n    \n    return np.array(all_ids)","e0b1503e":"def combine_qa_ids(q_ids, a_ids, tokenizer, maxlen=512):\n    \"\"\"\n    Given two arrays of IDs (questions and answers) created by\n    `fast_encode`, we combine and pad them.\n    Inputs:\n        tokenizer: The original tokenizer (not the fast_tokenizer)\n    \"\"\"\n    combined_ids = []\n\n    for i in tqdm(range(q_ids.shape[0])):\n        ids = []\n        ids.append(tokenizer.cls_token_id)\n        ids.extend(q_ids[i])\n        ids.append(tokenizer.sep_token_id)\n        ids.extend(a_ids[i])\n        ids.append(tokenizer.sep_token_id)\n        ids.extend([tokenizer.pad_token_id] * (maxlen - len(ids)))\n\n        combined_ids.append(ids)\n    \n    return np.array(combined_ids)","283d7ae8":"def encode_qa(questions, answers, tokenizer, chunk_size=256, maxlen=512):\n    \"\"\"\n    https:\/\/www.kaggle.com\/xhlulu\/jigsaw-tpu-distilbert-with-huggingface-and-keras\n    \"\"\"\n    tokenizer.enable_truncation(max_length=maxlen)\n    tokenizer.enable_padding(max_length=maxlen)\n    all_ids = []\n    \n    for i in tqdm(range(0, len(questions), chunk_size)):\n        q_chunk = questions[i:i+chunk_size].tolist()\n        a_chunk = answers[i:i+chunk_size].tolist()\n        text_chunk = list(zip(q_chunk, a_chunk))\n        \n        encs = tokenizer.encode_batch(text_chunk)\n        all_ids.extend([enc.ids for enc in encs])\n    \n    return np.array(all_ids)","8f1e513c":"def truncate_text(text, tokenizer, chunk_size=256, maxlen=256):\n    \"\"\"\n    Ensure that the text does not have more than maxlen tokens\n    \"\"\"\n    tokenizer.enable_truncation(max_length=maxlen)\n    all_norm_str = []\n    \n    for i in tqdm(range(0, len(text), chunk_size)):\n        chunk = text[i:i+chunk_size].tolist()\n        encs = tokenizer.encode_batch(chunk)\n        all_norm_str.extend([str(enc.normalized_str) for enc in encs])\n    \n    return all_norm_str","efe5d13e":"def build_model(transformer, max_len=None):\n    \"\"\"\n    https:\/\/www.kaggle.com\/xhlulu\/jigsaw-tpu-distilbert-with-huggingface-and-keras\n    \"\"\"\n    input_ids = L.Input(shape=(max_len, ), dtype=tf.int32)\n    \n    x = transformer(input_ids)[0]\n    x = x[:, 0, :]\n    x = L.Dense(1, activation='sigmoid', name='sigmoid')(x)\n    \n    # BUILD AND COMPILE MODEL\n    model = Model(inputs=input_ids, outputs=x)\n    model.compile(\n        loss='binary_crossentropy', \n        metrics=['accuracy'], \n        optimizer=Adam(lr=1e-5)\n    )\n    \n    return model","6bffa0c2":"def save_model(model, sigmoid_dir='transformer', transformer_dir='transformer'):\n    \"\"\"\n    Special function to load a keras model that uses a transformer layer\n    \"\"\"\n    os.makedirs(transformer_dir, exist_ok=True)\n    os.makedirs(sigmoid_dir, exist_ok=True)\n    \n    transformer = model.layers[1]\n    transformer.save_pretrained(transformer_dir)\n    \n    sigmoid_path = os.path.join(sigmoid_dir,'sigmoid.pickle')\n    sigmoid = model.get_layer('sigmoid').get_weights()\n    pickle.dump(sigmoid, open(sigmoid_path, 'wb'))\n\n    \ndef load_model(sigmoid_dir='transformer', transformer_dir='transformer', \n               architecture=\"electra\", max_len=None):\n    \"\"\"\n    Special function to load a keras model that uses a transformer layer\n    \"\"\"\n    sigmoid_path = os.path.join(sigmoid_dir,'sigmoid.pickle')\n    \n    if architecture == 'electra':\n        transformer = TFElectraModel.from_pretrained(transformer_dir)\n    else:\n        transformer = TFAutoModel.from_pretrained(transformer_dir)\n    model = build_model(transformer, max_len=max_len)\n    \n    sigmoid = pickle.load(open(sigmoid_path, 'rb'))\n    model.get_layer('sigmoid').set_weights(sigmoid)\n    \n    return model","8ef18521":"# Detect hardware, return appropriate distribution strategy\ntry:\n    # TPU detection. No parameters necessary if TPU_NAME environment variable is\n    # set: this is always the case on Kaggle.\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    # Default distribution strategy in Tensorflow. Works on CPU and single GPU.\n    strategy = tf.distribute.get_strategy()\n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)","7da5cc8e":"AUTO = tf.data.experimental.AUTOTUNE\n\n# Configuration\nEPOCHS = 5\nBATCH_SIZE = 16 * strategy.num_replicas_in_sync\nMAX_LEN = 512\nMODEL = 'google\/electra-base-discriminator'","9e97b0bc":"df = pd.concat([\n    pd.read_csv(f'\/kaggle\/input\/stackexchange-qa-pairs\/pre_covid\/{group}.csv')\n    for group in ['expert', 'biomedical', 'general']\n]).reset_index(drop=True)\n\ndf.head()","6ce46ae4":"questions = df.title + \"[SEP]\" + df.question","254e86cb":"# First load the real tokenizer\ntokenizer = ElectraTokenizer.from_pretrained(MODEL)\n# Save the loaded tokenizer locally\ntokenizer.save_pretrained('.')\n# Reload it with the huggingface tokenizers library\nfast_tokenizer = BertWordPieceTokenizer('vocab.txt', lowercase=True, add_special_tokens=False)\nfast_tokenizer","02efdc20":"# q_ids = fast_encode(questions.values, fast_tokenizer, maxlen=MAX_LEN\/\/2 - 2)\n# a_ids = fast_encode(df.answer.values, fast_tokenizer, maxlen=MAX_LEN\/\/2 - 2)\n# wa_ids = fast_encode(df.wrong_answer.values, fast_tokenizer, maxlen=MAX_LEN\/\/2 - 2)\n\n# correct_ids = combine_qa_ids(q_ids, a_ids, tokenizer, maxlen=MAX_LEN)\n# wrong_ids = combine_qa_ids(q_ids, wa_ids, tokenizer, maxlen=MAX_LEN)","f5297557":"correct_ids = np.load('\/kaggle\/input\/stackexchange-encode-for-electra\/correct_ids.npy')\nwrong_ids = np.load('\/kaggle\/input\/stackexchange-encode-for-electra\/wrong_ids.npy')","f6c5ed3f":"input_ids = np.concatenate([correct_ids, wrong_ids])\n\nlabels = np.concatenate([\n    np.ones(correct_ids.shape[0]),\n    np.zeros(wrong_ids.shape[0])\n]).astype(np.int32)","d6eabbfb":"train_idx, test_idx = train_test_split(\n    np.arange(input_ids.shape[0]), \n    test_size=0.3, \n    random_state=0\n)\n\nvalid_idx, test_idx = train_test_split(\n    test_idx, \n    test_size=0.5, \n    random_state=1\n)","b18ad02b":"train_ids = input_ids[train_idx]\nvalid_ids = input_ids[valid_idx]\ntest_ids = input_ids[test_idx]\n\ntrain_labels = labels[train_idx]\nvalid_labels = labels[valid_idx]\ntest_labels = labels[test_idx]","e84a8419":"train_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices((train_ids, train_labels))\n    .repeat()\n    .shuffle(2048)\n    .batch(BATCH_SIZE)\n    .prefetch(AUTO)\n)\n\nvalid_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices((valid_ids, valid_labels))\n    .batch(BATCH_SIZE)\n    .cache()\n    .prefetch(AUTO)\n)\n\ntest_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices(test_ids)\n    .batch(BATCH_SIZE)\n)","f27b3c90":"%%time\nwith strategy.scope():\n    transformer_layer = TFElectraModel.from_pretrained(MODEL)\n    model = build_model(transformer_layer, max_len=MAX_LEN)\nmodel.summary()","cebb193e":"n_steps = train_labels.shape[0] \/\/ BATCH_SIZE\n\ntrain_history = model.fit(\n    train_dataset,\n    steps_per_epoch=n_steps,\n    validation_data=valid_dataset,\n    epochs=EPOCHS\n)","b0ed66d8":"save_model(model)","17d225a7":"hist_df = pd.DataFrame(train_history.history)\nhist_df.to_csv('train_history.csv')\nhist_df","8e773404":"with strategy.scope():\n    model = load_model(max_len=MAX_LEN)","9a13b559":"y_score = model.predict(test_dataset, verbose=1).squeeze()\ny_pred = y_score.round().astype(int)\nprint(\"AP:\", average_precision_score(test_labels, y_score))\nprint(\"ROC AUC:\", roc_auc_score(test_labels, y_score))\nprint(classification_report(test_labels, y_pred))","ffa2ce57":"## Modeling","e5a09995":"## Build datasets objects","8777b41d":"## TPU Configs","226f99a9":"### Train model","0721141f":"## Train test split","04d4faed":"## Helper functions","2c639616":"## Bert tokenizer","492ef4ae":"## Load data","45963ba8":"## Eval","377f91be":"## Convert text to matrices\n\nCaveat: Since a lot of the questions on stackexchange goes over 256, characters, we end up truncating a large part (if not all) of the answers. Thus, we need to \"pre\" truncate them by separately encode the questions and answers, and use a functions to combine them again.\n\nNote: Here we are not actually encoding it, instead we load the encoded q&a pairs from another notebook, in order to limit memory consumption."}}