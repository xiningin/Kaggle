{"cell_type":{"7f0b12bb":"code","25ee9788":"code","27e3c23b":"code","b4e179ee":"code","b421e645":"code","881f6f47":"code","cecee490":"code","221303a3":"code","912f754a":"code","c42ce86e":"code","cc3bfea0":"code","f6ba345d":"code","2d35bf10":"code","58e6fc7d":"code","cd18665a":"code","d3fbc399":"code","f1e62c38":"code","edb98e3e":"code","99c242b9":"code","4a138d25":"code","92a572d2":"code","1efdc284":"code","e6ea45e6":"code","e2a3e1fc":"code","14337b43":"code","e5784cca":"code","e4cd6b89":"code","d5c4bcf2":"markdown","ed15cc59":"markdown","c74f6366":"markdown","f74f45a9":"markdown","7d054d88":"markdown","7226bb81":"markdown","882b4f88":"markdown","edd52a62":"markdown","b23d6047":"markdown","7be4a2bc":"markdown","eefa7706":"markdown","096e7469":"markdown","c5443796":"markdown","afd2096f":"markdown","ae1cb2d2":"markdown","c5c10dc4":"markdown","47cb4f5d":"markdown","cd7b9f9b":"markdown","ab7bd81c":"markdown","be584eb5":"markdown","0bc2eb5d":"markdown","1a6103dc":"markdown","c3b4158b":"markdown","2841473e":"markdown","44a4fdfb":"markdown"},"source":{"7f0b12bb":"# libs\nimport random\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.mixture import GaussianMixture\nfrom sklearn.metrics import confusion_matrix\n\nfrom lightgbm import LGBMRegressor\nfrom catboost import CatBoostRegressor\nfrom lightgbm import LGBMClassifier\nfrom xgboost import XGBRegressor\n\nimport optuna \nfrom optuna import Trial\nfrom optuna.samplers import TPESampler","25ee9788":"# load\ndf_train = pd.read_csv('\/kaggle\/input\/tabular-playground-series-jan-2021\/train.csv')\ndf_test = pd.read_csv('\/kaggle\/input\/tabular-playground-series-jan-2021\/test.csv')\ndf_train.info(verbose=False, memory_usage='deep')","27e3c23b":"# config\ntrain_mode = False\ntarget = 'target'\nrandom_state = 42\nx_cols = [c for c in df_train.columns if 'cont' in c] # all training columns","b4e179ee":"# evaluation function\ndef _evaluate(model, x_cols, df, target=target, n_folds=5):\n    \n    oof = np.zeros(len(df[target])) # means 'out of fold' - basically where we are going to store our test fold predictions\n    preds_test = np.zeros(len(df_test)) # test set predictions\n\n    # enum folds\n    kf = KFold(n_splits=n_folds, random_state=random_state, shuffle=True)\n    for idx_train, idx_test in kf.split(df):\n\n        # setup test \/ train data\n        x_train = df.loc[idx_train, x_cols].values\n        y_train = df.loc[idx_train, target].values\n        x_test = df.loc[idx_test, x_cols].values\n        y_test = df.loc[idx_test, target].values\n    \n        # fit \/ predict\n        model.fit(x_train, y_train)#, eval_set = [(x_test, y_test)], early_stopping_rounds=100, verbose=False)\n        preds_train = model.predict(x_test) # train set predictions (used for hypertuning)\n        preds_test += model.predict(df_test[x_cols].values) \/ n_folds\n        \n        # append train predictions\n        oof[idx_test] = preds_train\n    \n    return oof, preds_test","b421e645":"# get baseline score\nlgbm = LGBMRegressor(seed=random_state)\noof, preds_test = _evaluate(lgbm, x_cols, df_train)\n\n# plot\nfig, ax = plt.subplots(figsize=(12, 4))\nsns.kdeplot(df_train[target], color='b', label='actual')\nsns.kdeplot(oof, color='r', label='prediction')\nax.set_title('mse: ' + str(round(mean_squared_error(df_train[target], oof, squared=False), 5)))\nplt.show()","881f6f47":"# correlations \/ mask\ncorr = df_train.corr()\nmask = np.triu(np.ones_like(corr, dtype=bool))\n\n# plot\nfig, ax = plt.subplots(figsize=(11, 9))\ncmap = sns.diverging_palette(230, 20, as_cmap=True)\nsns.heatmap(corr, mask=mask, cmap=cmap, center=0, square=True, linewidths=.5, cbar_kws={\"shrink\": .6})\n\nplt.show()","cecee490":"# setup plot\nplot_cols = 5 # no of columns \/ change to preference\nlist_feats = [a for a in df_train.columns if 'cont' in a] # all features\ntotal_rows = int(np.ceil(len(list_feats) \/ plot_cols))\nfig, ax = plt.subplots(nrows=total_rows, ncols=plot_cols, figsize=(20, 4 * total_rows)) # setup subplots\n\n# loop each feature \/ plot\nj = 0 # keeps track of rows\nfor i in range(0, len(list_feats)): # loop index of each feature\n    \n    # create plot\n    sns.boxplot(data=[df_train[list_feats[i]], df_test[list_feats[i]]], palette='vlag', ax=ax[j,i % plot_cols])\n    ax[j,i % plot_cols].set_title(list_feats[i])\n    ax[j,i % plot_cols].set_ylabel('')\n    ax[j,i % plot_cols].set_xlabel('')\n    \n    # increment row\n    if i % plot_cols == (plot_cols - 1): j += 1 # basically says at end of each column start a new row\n\nplt.show()","221303a3":"# setup plot\nplot_cols = 5 # no of columns \/ change to preference\nlist_feats = [a for a in df_train.columns if 'cont' in a] # all features\ntotal_rows = int(np.ceil(len(list_feats) \/ plot_cols))\nfig, ax = plt.subplots(nrows=total_rows, ncols=plot_cols, figsize=(20, 4 * total_rows)) # setup subplots\n\n# loop each feature \/ plot\nj = 0 # keeps track of rows\nfor i in range(0, len(list_feats)): # loop index of each feature\n    \n    # create plot\n    sns.distplot(df_train[list_feats[i]], ax=ax[j,i % plot_cols])\n    ax[j,i % plot_cols].set_title(list_feats[i])\n    ax[j,i % plot_cols].set_ylabel('')\n    ax[j,i % plot_cols].set_xlabel('')\n    \n    # increment row\n    if i % plot_cols == (plot_cols - 1): j += 1 # basically says at end of each column start a new row\n\nplt.show()","912f754a":"# mask of rows outside iqr\ndef _iqr_mask(feature):\n    \n    # compute quantiles\n    q1 = df_train[feature].quantile(0.25)\n    q3 = df_train[feature].quantile(0.75)\n    iqr = q3 - q1\n    min_quartile = q1 - 1.5 * iqr\n    max_quartile = q3 + 1.5 * iqr\n\n    # create bool mask\n    mask_iqr = (df_train[feature] >= min_quartile) & (df_train[feature] <= max_quartile)\n    \n    return mask_iqr\n\n# get iqr mask for target\nmask_iqr_target = _iqr_mask(target)\n\n# plot\nfig, ax = plt.subplots(nrows=2, ncols=2, figsize=(20,5))\nsns.boxplot(data=df_train[target], ax=ax[0,0])\nsns.kdeplot(df_train[target], ax=ax[1,0])\nsns.boxplot(data=df_train.loc[mask_iqr_target, target], ax=ax[0,1])\nsns.kdeplot(df_train.loc[mask_iqr_target, target], ax=ax[1,1])\nax[0,0].set_title('pre iqr')\nax[0,1].set_title('post iqr')\nplt.show()","c42ce86e":"# # remove outliers \/ reset index\n# df_train = df_train[df_train['cont7'] > -0.03]\n# df_train = df_train.loc[_iqr_mask('cont10')]\n# df_train.reset_index(drop=True, inplace=True)\n\n# plot\nfig, ax = plt.subplots(nrows=1, ncols=2, figsize=(20,5))\nsns.boxplot(df_train['cont7'], ax=ax[0])\nsns.boxplot(df_train['cont10'], ax=ax[1])\nax[0].set_title('cont7')\nax[1].set_title('cont10')\nplt.show()","cc3bfea0":"# lgbm \/ objective function\ndef objective(trial):\n    \n    # hyperparameters\n    param = {\n        'boosting_type':'gbdt',\n        'num_leaves':trial.suggest_int('num_leaves', 3, 150),\n        'max_depth':trial.suggest_int('max_depth', -1, 20),\n        'learning_rate':trial.suggest_float('learning_rate', 0.001, 0.6),\n        'n_estimators':trial.suggest_int('n_estimators', 50, 500),\n        'min_child_weight':trial.suggest_float('min_child_weight', 0.2, 0.6),\n        'min_child_samples':trial.suggest_int('min_child_samples', 15, 30),\n        'subsample':trial.suggest_float('subsample', 0.5, 1.0),\n        'subsample_freq':trial.suggest_int('subsample_freq', 3, 150),\n        'random_state':random_state,\n        'lambda_l1':trial.suggest_float('lambda_l1', 0.0, 5.0)\n    }\n    \n    # model \/ evaluate\n    lgbm = LGBMRegressor(**param) # model\n    x_cols = [c for c in df_train.columns if 'cont' in c] # all columns\n    oof, preds_test = _evaluate(lgbm, x_cols, df_train.reset_index(drop=True)) # evaluate\n    \n    return mean_squared_error(df_train[target].reset_index(drop=True), oof, squared=False)\n\nif train_mode:\n\n    # run study\n    study = optuna.create_study(direction='minimize', sampler=TPESampler())\n    study.optimize(objective, n_trials=200)\n\n    # output study\n    print (study.best_value)\n    print (study.best_params)","f6ba345d":"# cat \/ objective function\ndef objective(trial):\n    \n    # hyperparameters\n    param = {\n        'iterations':1000,\n        'verbose':False,\n        'random_state':random_state,\n        'loss_function':'RMSE',\n        'bootstrap_type':'Bernoulli',\n        'learning_rate':trial.suggest_float('learning_rate', 0.0001, 0.31),\n        'max_depth':trial.suggest_int('max_depth', 3, 10),\n        'colsample_bylevel':trial.suggest_float('colsample_bylevel', 0.3, 0.8),\n    }\n    \n    # model \/ evaluate\n    cat = CatBoostRegressor(**param) # model\n    x_cols = [c for c in df_train.columns if 'cont' in c] # all columns\n    oof, preds_test = _evaluate(cat, x_cols, df_train, target=target) # evaluate\n    \n    return mean_squared_error(df_train[target], oof, squared=False)\n\nif train_mode:\n    \n    # run study\n    study = optuna.create_study(direction='minimize', sampler=TPESampler())\n    study.optimize(objective, n_trials=100)\n\n    # output study\n    print (study.best_value)\n    print (study.best_params)","2d35bf10":"# xgb \/ objective function\ndef objective(trial):\n    \n    # hyperparameters\n    param = {\n        'random_state':random_state,\n        'objective':'reg:squarederror',\n        'booster':'gbtree',\n        'learning_rate':trial.suggest_float('learning_rate', 0.001, 0.1),\n        'alpha':trial.suggest_float('alpha', 0.001, 0.1),\n        'colsample_bylevel':trial.suggest_float('colsample_bylevel', 0.05, 0.1),\n        'colsample_bytree':trial.suggest_float('colsample_bytree', 0.1, 0.9),\n        'gamma':trial.suggest_float('gamma', 0, 0.5),\n        'max_depth':trial.suggest_int('max_depth', 3, 18),\n        'min_child_weight': trial.suggest_float ('min_child_weight', 1, 20),\n        #'reg_lambda':trial.suggest_int('reg_lambda', 0, 10),\n        #'reg_alpha':trial.suggest_int('reg_alpha', 10, 50),\n        'subsample':trial.suggest_float('subsample', 0.3, 0.7),\n    }\n    \n    # model \/ evaluate\n    xgb = XGBRegressor(**param) # model\n    x_cols = [c for c in df_train.columns if 'cont' in c] # all columns\n    oof, preds_test = _evaluate(xgb, x_cols, df_train, target=target) # evaluate\n    \n    return mean_squared_error(df_train[target], oof, squared=False)\n\nif train_mode:\n    \n    # run study\n    study = optuna.create_study(direction='minimize', sampler=TPESampler())\n    study.optimize(objective, n_trials=100)\n\n    # output study\n    print (study.best_value)\n    print (study.best_params)","58e6fc7d":"# hyper params per each model\nparam_lgbm = {'num_leaves': 148, 'max_depth': 19, 'learning_rate': 0.04168752594808129, 'n_estimators': 468, 'min_child_weight': 0.35392113973764505, 'min_child_samples': 29, 'subsample': 0.9348697769228501, 'subsample_freq': 6, 'lambda_l1': 4.639129744838143}\nparam_cat = {'learning_rate': 0.07468089271003528, 'max_depth': 8, 'colsample_bylevel': 0.7338241468797853}\nparam_xgb = param_xgb = {'learning_rate': 0.0652222304334701, 'alpha': 0.0036866921576056855, 'colsample_bylevel': 0.09959606060270643, 'colsample_bytree': 0.863554381598069, 'gamma': 0.3959383978062547, 'max_depth': 15, 'min_child_weight': 19.357558021086128, 'subsample': 0.6991638855748524}\n\n# lgbm hyperopt score\nlgbm = LGBMRegressor(seed=random_state, **param_lgbm)\noof_lgbm, preds_lgbm = _evaluate(lgbm, x_cols, df_train, n_folds=5)\nprint ('hyper lgbm:', mean_squared_error(df_train[target], oof_lgbm, squared=False))\n\n# catboost hyperopt score\ncat = CatBoostRegressor(iterations=1000, verbose=False, random_state=random_state, loss_function='RMSE', bootstrap_type='Bernoulli', **param_cat)\noof_cat, preds_cat = _evaluate(cat, x_cols, df_train, n_folds=5)\nprint ('hyper cat:', mean_squared_error(df_train[target], oof_cat, squared=False))\n\n# xgb score\nxgb = XGBRegressor(random_state=random_state, **param_xgb)\noof_xgb, preds_xgb = _evaluate(xgb, x_cols, df_train, n_folds=2) # scaled back so it runs quicker for public.\nprint ('hyper xgb:', mean_squared_error(df_train[target], oof, squared=False))\n\n# blend score\nprint ('hyper blend:', mean_squared_error(df_train[target], ((oof_lgbm * 0.4) + (oof_cat * 0.4) + (oof_xgb * 0.2)), squared=False))","cd18665a":"# merge model outputs to test\ndf_test['hyper_lgbm'] = preds_lgbm\ndf_test['hyper_cat'] = preds_cat\ndf_test['hyper_xgb'] = preds_xgb\ndf_test['hyper_blend'] = (preds_lgbm * 0.4) + (preds_cat * 0.4) + (preds_xgb * 0.2)\n\n# assign target\ndf_test['target'] = df_test['hyper_blend']\ndf_test[['id','target']].to_csv('submission_hyper_blend.csv', index=False)","d3fbc399":"n_folds = 3 # controls no of times model is run and no of folds within evaluation\n\n# arrays to store oof and predictions\noof_lgbm_seeds = np.zeros(len(df_train)) \noof_car_seeds = np.zeros(len(df_train))\npreds_lgbm_seeds = np.zeros(len(df_test)) \npreds_car_seeds = np.zeros(len(df_test)) \n\n# enum each fold \/ generate random num (used for seed)\nfor g in range(n_folds):\n    rand_num = random.randint(1, 5000)\n    \n    # lgbm predictions\n    lgbm = LGBMRegressor(seed=rand_num, **param_lgbm)\n    oof, preds_lgbm = _evaluate(lgbm, x_cols, df_train, n_folds=n_folds)\n    oof_lgbm_seeds += (oof \/ n_folds) # merge with seed results\n    preds_lgbm_seeds += (preds_lgbm \/ n_folds) # merge with seed results\n    \n    # cat boost predictions\n    cat = CatBoostRegressor(iterations=1000, verbose=False, random_state=rand_num, loss_function='RMSE', bootstrap_type='Bernoulli', **param_cat)\n    oof, preds_cat = _evaluate(cat, x_cols, df_train, n_folds=n_folds)\n    oof_car_seeds += (oof \/ n_folds) # merge with seed results\n    preds_car_seeds += (preds_cat \/ n_folds) # merge with seed results","f1e62c38":"print ('hyper lgbm:', mean_squared_error(df_train[target], oof_lgbm_seeds, squared=False))\nprint ('hyper cat:', mean_squared_error(df_train[target], oof_car_seeds, squared=False))\n\n# hyper lgbm: 0.6959915123976514\n# hyper cat: 0.697341630050542","edb98e3e":"# merge model outputs to test\ndf_test['seed_lgbm'] = preds_lgbm_seeds\ndf_test['seed_cat'] = preds_car_seeds\ndf_test['seed_blend'] = (preds_lgbm_seeds * 0.4) + (preds_car_seeds * 0.6)\n\n# assign target\ndf_test['target'] = df_test['seed_blend']\ndf_test[['id','target']].to_csv('submission_seed_blend.csv', index=False)","99c242b9":"# apply gaussian mix \/ assign bins back to training\ngmm = GaussianMixture(n_components=2, random_state=42)\ngmm.fit(df_train[target].values.reshape(-1, 1))\ndf_train['target_gmm'] = gmm.predict(df_train[target].values.reshape(-1, 1))\n\n# masks for each gmm bin\nmask_gmm_0 = df_train['target_gmm'] == 0\nmask_gmm_1 = df_train['target_gmm'] == 1\n\n# plot\nfig, ax = plt.subplots(figsize=(12, 4))\nsns.kdeplot(data=df_train.loc[mask_gmm_0, target], label='gmm bin 0')\nsns.kdeplot(data=df_train.loc[mask_gmm_1, target], label='gmm bin 1')\nplt.show()","4a138d25":"# oof masks for dataframe\noof_gmm_0, preds = _evaluate(lgbm, x_cols, df_train.loc[mask_gmm_0].reset_index(drop=True))\noof_gmm_1, preds = _evaluate(lgbm, x_cols, df_train.loc[mask_gmm_1].reset_index(drop=True))\n\n# plot\nfig, ax = plt.subplots(nrows=1, ncols=2, figsize=(18, 4))\nsns.kdeplot(data=df_train.loc[mask_gmm_0, target], ax=ax[0], label='actual')\nsns.kdeplot(data=oof_gmm_0, ax=ax[0], label='prediction')\nsns.kdeplot(data=df_train.loc[mask_gmm_1, target], ax=ax[1], label='actual')\nsns.kdeplot(data=oof_gmm_1, ax=ax[1], label='prediction')\n\n# decorate\nax[0].set_title('gmm_0: ' + str(round(mean_squared_error(df_train.loc[mask_gmm_0, target], oof_gmm_0, squared=False), 5)))\nax[1].set_title('gmm_1: ' + str(round(mean_squared_error(df_train.loc[mask_gmm_1, target], oof_gmm_1, squared=False), 5)))\nplt.show()","92a572d2":"# split training data \/ reset indexes\ndf_gmm_test = df_train.sample(100000).copy()\ndf_gmm_train = df_train[(~df_train['id'].isin(df_gmm_test['id'].to_list()))].copy()\ndf_gmm_test.reset_index(drop=True, inplace=True)\ndf_gmm_train.reset_index(drop=True, inplace=True)\nprint (df_gmm_train.shape, df_gmm_test.shape)","1efdc284":"# baseline score for gmm classifier\nlgbm_gmm_class = LGBMClassifier(random_state=random_state)\noof, preds = _evaluate(lgbm_gmm_class, x_cols, df_gmm_train, target='target_gmm')\nprint(accuracy_score(df_gmm_train['target_gmm'], oof))\nprint(confusion_matrix(df_gmm_train['target_gmm'], oof))","e6ea45e6":"# # lgbm \/ objective function\n# def objective(trial):\n    \n#     # hyperparameters\n#     param = {\n#         'num_leaves':trial.suggest_int('num_leaves', 3, 150),\n#         'max_depth':trial.suggest_int('max_depth', -1, 20),\n#         'learning_rate':trial.suggest_float('learning_rate', 0.001, 0.6),\n#         'n_estimators':trial.suggest_int('n_estimators', 50, 500),\n#         'min_child_weight':trial.suggest_float('min_child_weight', 0.2, 0.6),\n#         'min_child_samples':trial.suggest_int('min_child_samples', 15, 30),\n#         'subsample':trial.suggest_float('subsample', 0.5, 1.0),\n#         'subsample_freq':trial.suggest_int('subsample_freq', 3, 150),\n#         'random_state':random_state,\n#         'lambda_l1':trial.suggest_float('lambda_l1', 0.0, 5.0)\n#     }\n    \n#     # model \/ evaluate\n#     lgbm_gmm_class = LGBMClassifier(**param) # model\n#     oof, preds = _evaluate(lgbm_gmm_class, x_cols, df_gmm_train, target='target_gmm') # evaluate\n    \n#     return accuracy_score(df_gmm_train['target_gmm'], oof)\n\n# # run study\n# study = optuna.create_study(direction='maximize',sampler=TPESampler())\n# study.optimize(objective, n_trials=30) # scaled down for public (i maxed out at 0.6)\n\n# # output study\n# print (study.best_value)\n# print (study.best_params)","e2a3e1fc":"# optuna hyper params\nparam_lgbm_gmm_class = {'num_leaves': 144, 'max_depth': 10, 'learning_rate': 0.0934759313797003, 'n_estimators': 107, 'min_child_weight': 0.30363944085393396, 'min_child_samples': 17, 'subsample': 0.9463879414095, 'subsample_freq': 80, 'lambda_l1': 4.67131782429971}\n\n# init classifiers \/ regressors (re-fit other ones on new partition)\nlgbm_gmm_class = LGBMClassifier(random_state=random_state, **param_lgbm_gmm_class)\nlgbm_gmm_reg_0 = LGBMRegressor(seed=random_state)\nlgbm_gmm_reg_1 = LGBMRegressor(seed=random_state)\nlgbm_gmm_reg = LGBMRegressor(seed=random_state, **param_lgbm)\nlgbm_cat_reg = CatBoostRegressor(iterations=1000, verbose=False, random_state=random_state, loss_function='RMSE', bootstrap_type='Bernoulli', **param_cat)\n\n# masks for each gmm bin\nmask_gmm_0 = df_gmm_train['target_gmm'] == 0\nmask_gmm_1 = df_gmm_train['target_gmm'] == 1\n\n# fit\nlgbm_gmm_class.fit(df_gmm_train[x_cols], df_gmm_train['target_gmm'])\nlgbm_gmm_reg_0.fit(df_gmm_train.loc[mask_gmm_0, x_cols].reset_index(drop=True), df_gmm_train.loc[mask_gmm_0, target].reset_index(drop=True))\nlgbm_gmm_reg_1.fit(df_gmm_train.loc[mask_gmm_1, x_cols].reset_index(drop=True), df_gmm_train.loc[mask_gmm_1, target].reset_index(drop=True))\nlgbm_gmm_reg.fit(df_gmm_train[x_cols], df_gmm_train[target])\nlgbm_cat_reg.fit(df_gmm_train[x_cols], df_gmm_train[target])\n\n# class predictions (obtain probabilities rather than class)\ndf_gmm_test[['gmm_class_' + str(a) + '_pred' for a in lgbm_gmm_class.classes_]] = lgbm_gmm_class.predict_proba(df_gmm_test[x_cols])\n\n# regression predictions (on all data)\ndf_gmm_test['lgbm'] = lgbm_gmm_reg.predict(df_gmm_test[x_cols])\ndf_gmm_test['cat'] = lgbm_cat_reg.predict(df_gmm_test[x_cols])\ndf_gmm_test['hyper_blend'] = (df_gmm_test['lgbm'] * 0.5) + (df_gmm_test['cat'] * 0.5)\ndf_gmm_test['gmm_reg_0'] = lgbm_gmm_reg_0.predict(df_gmm_test[x_cols])\ndf_gmm_test['gmm_reg_1'] = lgbm_gmm_reg_1.predict(df_gmm_test[x_cols])","14337b43":"# plot\nfig, ax = plt.subplots(nrows=1, ncols=2, figsize=(18, 4))\nsns.kdeplot(df_gmm_test['lgbm'], ax=ax[0], label='lgbm')\nsns.kdeplot(df_gmm_test['cat'], ax=ax[0], label='cat boost')\nsns.kdeplot(df_gmm_test['hyper_blend'], ax=ax[0], label='blend')\nsns.kdeplot(df_gmm_test['target'], ax=ax[0], label='target')\nsns.kdeplot(df_gmm_test['gmm_reg_0'], ax=ax[1], label='regression 0')\nsns.kdeplot(df_gmm_test['gmm_reg_1'], ax=ax[1], label='regression 1')\nsns.kdeplot(df_gmm_test['target'], ax=ax[1], label='target')\n\n# decorate\nax[0].set_title('full regressors')\nax[1].set_title('gmm regressors')\nplt.show()","e5784cca":"# create super target\ndef _super_target(x):\n    \n    # certain class\n    if x['gmm_class_0_pred'] >= 0.7: return x['gmm_reg_0']\n    if x['gmm_class_1_pred'] >= 0.7: return x['gmm_reg_1']\n    \n    return x['hyper_blend']\n\n# apply super target\ndf_gmm_test['super_target'] = df_gmm_test.apply(lambda x: _super_target(x), axis=1)\n\n# plot\nfig, ax = plt.subplots(figsize=(18, 4))\nsns.kdeplot(df_gmm_test[target])\nsns.kdeplot(df_gmm_test['super_target'])\nplt.show()","e4cd6b89":"print ('blend:', mean_squared_error(df_gmm_test[target], df_gmm_test['hyper_blend'], squared=False))\nprint ('super:', mean_squared_error(df_gmm_test[target], df_gmm_test['super_target'], squared=False))","d5c4bcf2":"# Optuna.","ed15cc59":"# Baseline.\nLets get a baseline score to try and beat...","c74f6366":"Straight away you can see cont2 has a different distribution and cont7 \/ cont10 have extra outliers - we can trial removing them.\n## Distributions.","f74f45a9":"To summarise the below:\n* Fit a classification tree that can predict which part of the Gmm it belongs to.\n* Fit 2 regression trees on each Gmm. (roughly about 0.38 mse).\n* Re fit the Lgbm and Catboost models on the new training set.\n* Perform predictions on the test set.","7d054d88":"This produces a LB score of: <br>\n0.69851 - no records removed. <br>\n0.69858 - cont7 \/ cont 10 outliers removed. <br>\n0.69762 - xgb blend.","7226bb81":"It seems too convenient that the target doesnt correlate with anything. Initial observations are that fields 1 and 6-13 correlate well and 2-5 not so well. <br>\nIt does lead you to think that this is 2 datasets merged together and the challenge is to unpick it.\n## Boxplots (vs test data).","882b4f88":"## Iqr.\nUsing the Interquartile Range (Iqr) to remove outliers is fairly common practice, I wonder why sklearn doesnt have its own Iqr function?","edd52a62":"Lb: 0.69847 - barely any difference.","b23d6047":"Unfortunatley not that accurately, can we boost a better value...","7be4a2bc":"Visually this looks promising, but the LB tells me otherwise...","eefa7706":"# Submission (pt2).","096e7469":"# Optuna.\nAlways been a fan of GridSearch, until i found Optuna (its much more flexible, uses ranges rather that brute forcing lists and works better on larger datasets)...","c5443796":"The competition is about fine margins, which i have had reasonable success by blending models and averaging many outputs. I havent had much success by removing outliers, transforming the data and attempting to normalize the target... all which seems counter intuitive.<br>\n\nSo is there another method we could use to gain a competative advantage, since a fractional improvement could mean a decent place on the leaderboard?\n\nThis kernel is inspired by the notebook: [handling-multimodal-distributions-fe-techniques](https:\/\/www.kaggle.com\/iamleonie\/handling-multimodal-distributions-fe-techniques\/comments). <br>\nThe below shows there is potential to get a great score if we can split target into 2 distributions.<br>","afd2096f":"# Submission (pt1).\nLets blend our model prediction to get a more robust average...","ae1cb2d2":"# Gaussian Mixture (Gmm).\nThis was inspired by [https:\/\/www.kaggle.com\/iamleonie\/handling-multimodal-distributions-fe-techniques\/comments](https:\/\/www.kaggle.com\/iamleonie\/handling-multimodal-distributions-fe-techniques\/comments).<br>\nThe basics of the following process are:\n* Split the data into 2 based on the Gmm distribution output.\n* Train a classification model to predict which distribution it belongs to.\n* Train 2 regression models on each of the distributions.","c5c10dc4":"Mse of 0.702, you can also see above that the model hasnt quite handled the biomodal distribution. <br>\n# Eda.\n## Correlations.","47cb4f5d":"Visually the distribution looks much better and yeilds a much better training score, however the same cannot be said about the LB score.","cd7b9f9b":"Making a prediction using the classifier and the 2 Gmm regressors doesnt actually yeild a good prediction (its easy to see why). <br>\nSo instead, can we gain a competative advantage using our blended model then use our Gmm classifier (whereby the prediction is 90%+) to use the 2 Gmm regressors, lets call this 'super target'...","ab7bd81c":"Even though we have only gone from 0.702 to 0.696 - its a huge improvement!","be584eb5":"Interesting results, we have gone from 0.71 to 0.38\/0.36. If we could only split the test set accurately we could be onto a winning prediction...\n# Classification.\nTo make this easier to test we will reserve a random 3rd of the training data for a final test.","0bc2eb5d":"# Seeds.\nIf at first you dont succeed, pick another seed and try again (or blend a few)... <br>\nThe following is commented out because of the time it takes to run.","1a6103dc":"# Evaluation.\nSimple kfold prediction, returns oof and an average prediction on the test set (which i disregard when tuning).","c3b4158b":"# Cont 7 \/ Cont 10\nAgain i get a better oof score when the following are removed, this isnt reflected in the LB score.","2841473e":"Run each of our hyper optimized models, but this time keep hold of the test predictions...","44a4fdfb":"This dataset is riddled with multimodal distributions, we may get a better result from binning... something to test."}}