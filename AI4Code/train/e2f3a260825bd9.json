{"cell_type":{"5dcef8af":"code","bf028c2b":"code","4b4b85c8":"code","8bccd9db":"code","ecea880f":"code","d0e69ba1":"code","54c4d407":"code","a1c7bc9a":"code","7aaadb81":"code","9005abff":"code","c5b7a580":"code","7d1a03a2":"code","fc7ff925":"code","8f7e1bc7":"code","6d27066b":"code","0cfe8cb1":"code","769c4633":"code","a08ecc7f":"code","be7d88ab":"code","8938f4c0":"code","cc22efe4":"code","be3d6710":"markdown","ffe05871":"markdown","aa156aa7":"markdown","576ab143":"markdown","42c84d7a":"markdown","02c53a4f":"markdown","a27a0a59":"markdown","8257c8c6":"markdown"},"source":{"5dcef8af":"# for working with files \nimport glob\nimport os\nimport shutil\nimport itertools  \nfrom tqdm import tqdm\n\n# for working with images\nfrom PIL import Image\nimport numpy as np\nimport pandas as pd\nfrom skimage import transform\nimport matplotlib.pyplot as plt\nimport cv2 as cv\nimport scipy.io\nimport random\n\n# tensorflow stuff\nimport tensorflow as tf\nprint(tf.__version__)\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Input, Dense, Conv2D, MaxPooling2D, Flatten, Activation, Dropout, BatchNormalization, GlobalAveragePooling2D, Add\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras import regularizers, optimizers, Model\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau\nfrom tensorflow.keras.activations import relu, softmax\n\n\n\n# for evaluation\nfrom sklearn.metrics import classification_report, confusion_matrix","bf028c2b":"!mkdir car_data_cropped\/\n!mkdir car_data_cropped\/train\n!mkdir car_data_cropped\/test","4b4b85c8":"cars_annos = pd.read_csv('..\/input\/anno_train.csv',header=None)\nfnames = []\nclass_ids = []\nbboxes = []\nlabels = []\n\n\nfor i in range(len(cars_annos)):\n    annotation = cars_annos.iloc[i]\n    bbox_x1 = annotation[1]\n    bbox_y1 = annotation[2]\n    bbox_x2 = annotation[3]\n    bbox_y2 = annotation[4]\n    class_id = annotation[5]\n    labels.append('%04d' % (class_id,))\n    fname = annotation[0]\n    bboxes.append((bbox_x1, bbox_y1, bbox_x2, bbox_y2))\n    class_ids.append(class_id)\n    fnames.append(fname)\n\n\nl = glob.glob('..\/input\/car_data\/car_data\/train\/*\/*')\n\nfor j in tqdm(range(len(l))):\n    i = fnames.index(l[j].split('\/')[-1])\n    labels[i]\n    (x1, y1, x2, y2) = bboxes[i]\n    fname=l[j].split('\/')[-1]\n    class_name = l[j].split('\/')[-2]\n    src_path = os.path.join('..\/input\/car_data\/car_data\/train\/'+class_name+'\/', fname)\n    src_image = cv.imread(src_path)\n\n    height, width = src_image.shape[:2]\n\n    # margins of 16 pixels\n    margin = 16\n    x1 = max(0, x1 - margin)\n    y1 = max(0, y1 - margin)\n    x2 = min(x2 + margin, width)\n    y2 = min(y2 + margin, height)\n    # print(\"{} -> {}\".format(fname, label))\n\n\n    dst_path = os.path.join('car_data_cropped\/train\/', class_name)\n    if not os.path.exists(dst_path):\n        os.makedirs(dst_path)\n\n\n    dst_path = os.path.join(dst_path, fname)\n\n\n    crop_image = src_image[y1:y2, x1:x2]\n    #dst_img = cv.resize(src=crop_image, dsize=(img_height, img_width))\n    cv.imwrite(dst_path, crop_image)","8bccd9db":"cars_annos = pd.read_csv('..\/input\/anno_test.csv',header=None)\nfnames = []\nclass_ids = []\nbboxes = []\nlabels = []\n\n\nl = glob.glob('..\/input\/car_data\/car_data\/test\/*\/*')\n\n\nfor i in range(len(cars_annos)):\n    annotation = cars_annos.iloc[i]\n    bbox_x1 = annotation[1]\n    bbox_y1 = annotation[2]\n    bbox_x2 = annotation[3]\n    bbox_y2 = annotation[4]\n    class_id = annotation[5]\n    labels.append('%04d' % (class_id,))\n    fname = annotation[0]\n    bboxes.append((bbox_x1, bbox_y1, bbox_x2, bbox_y2))\n    class_ids.append(class_id)\n    fnames.append(fname)\n","ecea880f":"for j in tqdm(range(len(l))):\n    i = fnames.index(l[j].split('\/')[-1])\n\n    (x1, y1, x2, y2) = bboxes[i]\n    fname=l[j].split('\/')[-1]\n\n    class_name = l[j].split('\/')[-2]\n    src_path = os.path.join('..\/input\/car_data\/car_data\/test\/'+class_name+'\/', fname)\n    src_image = cv.imread(src_path)\n\n    height, width = src_image.shape[:2]\n\n    # margins of 16 pixels\n    margin = 16\n    x1 = max(0, x1 - margin)\n    y1 = max(0, y1 - margin)\n    x2 = min(x2 + margin, width)\n    y2 = min(y2 + margin, height)\n    # print(\"{} -> {}\".format(fname, label))\n\n\n    dst_path = os.path.join('car_data_cropped\/test\/', class_name)\n    if not os.path.exists(dst_path):\n        os.makedirs(dst_path)\n\n    dst_path = os.path.join(dst_path, fname)\n\n\n    crop_image = src_image[y1:y2, x1:x2]\n    #dst_img = cv.resize(src=crop_image, dsize=(img_height, img_width))\n\n    cv.imwrite(dst_path, crop_image)","d0e69ba1":"attempt = 1\nif attempt == 0:\n    train_datagen=ImageDataGenerator(rotation_range=20,\n                                     width_shift_range=0.1,\n                                     height_shift_range=0.1,\n                                     zoom_range=0.2,\n                                     horizontal_flip=True)\n\n\n    valid_datagen=ImageDataGenerator(rotation_range=20,\n                                    zoom_range=0.15,\n                                    horizontal_flip=True)\n\n\n    train_generator=train_datagen.flow_from_directory(\n        directory=\"car_data_cropped\/train\/\",\n        batch_size=64,\n        seed=42,\n        target_size=(224,224))\n\n\n    valid_generator=valid_datagen.flow_from_directory(\n        directory=\"car_data_cropped\/test\/\",\n        batch_size=64,\n        seed=42,\n        target_size=(224,224))\n\n#--------------------------------------------------------------------------\nif attempt == 1:\n    from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n\n    train_datagen=ImageDataGenerator(rotation_range=15,\n                                     width_shift_range=0.1,\n                                     height_shift_range=0.1,\n                                     zoom_range=0.2,\n                                     horizontal_flip=True,\n                                     preprocessing_function=preprocess_input)\n\n    valid_datagen=ImageDataGenerator(horizontal_flip=True, \n                                     preprocessing_function=preprocess_input)\n\n\n    train_generator=train_datagen.flow_from_directory(\n        directory=\"car_data_cropped\/train\/\",\n        batch_size=64,\n        seed=42,\n        target_size=(224,224))\n\n\n    valid_generator=valid_datagen.flow_from_directory(\n        directory=\"car_data_cropped\/test\/\",\n        batch_size=300,\n        seed=42,\n        target_size=(224,224))\n","54c4d407":"def block(n_output, upscale=False):\n    # n_output: number of feature maps in the block\n    # upscale: should we use the 1x1 conv2d mapping for shortcut or not\n    \n    # keras functional api: return the function of type\n    # Tensor -> Tensor\n    \n    def f(x):\n        \n        # H_l(x):\n        # first pre-activation\n        h = BatchNormalization()(x)\n        h = Activation(relu)(h)\n        # first convolution\n        h = Conv2D(kernel_size=3, filters=n_output, strides=1, padding='same', kernel_regularizer=regularizers.l2(0.01))(h)\n        \n        # second pre-activation\n        h = BatchNormalization()(x)\n        h = Activation(relu)(h)\n        # second convolution\n        h = Conv2D(kernel_size=3, filters=n_output, strides=1, padding='same', kernel_regularizer=regularizers.l2(0.01))(h)\n        \n        # f(x):\n        if upscale:\n            # 1x1 conv2d\n            f = Conv2D(kernel_size=1, filters=n_output, strides=1, padding='same')(x)\n        else:\n            # identity\n            f = x\n        \n        # F_l(x) = f(x) + H_l(x):\n        return Add()([f, h])\n    \n    return f","a1c7bc9a":"# input tensor is the 28x28 grayscale image\ninput_tensor = Input((224, 224, 3))\n\n# first conv2d with post-activation to transform the input data to some reasonable form\nx = Conv2D(kernel_size=3, filters=16, strides=1, padding='same', kernel_regularizer=regularizers.l2(0.01))(input_tensor)\nx = BatchNormalization()(x)\nx = Activation(relu)(x)\n\n# F_1\nx = block(16)(x)\n# F_2\nx = block(16)(x)\n\n# F_3\n# H_3 is the function from the tensor of size 28x28x16 to the the tensor of size 28x28x32\n# and we can't add together tensors of inconsistent sizes, so we use upscale=True\nx = block(32, upscale=True)(x)       # !!! <------- Uncomment for local evaluation\n# F_4\nx = block(32)(x)                     # !!! <------- Uncomment for local evaluation\n# F_5\nx = block(32)(x)                     # !!! <------- Uncomment for local evaluation\n\n# F_6\nx = block(48, upscale=True)(x)       # !!! <------- Uncomment for local evaluation\n# F_7\nx = block(48)(x)                     # !!! <------- Uncomment for local evaluation\n\n# last activation of the entire network's output\nx = BatchNormalization()(x)\nx = Activation(relu)(x)\n\n# average pooling across the channels\n# 28x28x48 -> 1x48\nx = GlobalAveragePooling2D()(x)\n\n# dropout for more robust learning\nx = Dropout(0.2)(x)\n\n# last softmax layer\nx = Dense(units=196, kernel_regularizer=regularizers.l2(0.01))(x)\nx = Activation(softmax)(x)\n\nmodel = Model(inputs=input_tensor, outputs=x)\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])","7aaadb81":"len(model.layers)","9005abff":"model.summary()","c5b7a580":"for x,y in valid_generator:\n    x_val = x\n    y_val = y\n    break;","7d1a03a2":"IMAGE_SIZE = 224\n# Base model with MobileNetV2\nIMG_SHAPE = (IMAGE_SIZE, IMAGE_SIZE, 3)\n\nbase_model = tf.keras.applications.MobileNetV2(input_shape=IMG_SHAPE,alpha = .5,\n                                               include_top=False, \n                                               weights='imagenet')\n\nx = base_model.output\nx = tf.keras.layers.GlobalAveragePooling2D()(x)\nx = Dropout(.6)(x)\nprediction_layer = tf.keras.layers.Dense(196, activation='softmax')(x)\n\nlearning_rate = 0.0001\n\nmodel=Model(inputs=base_model.input,outputs=prediction_layer)\n\nfor layer in model.layers[:80]:\n    layer.trainable=False\nfor layer in model.layers[80:]:\n    layer.trainable=True\n# \n\noptimizer=tf.keras.optimizers.Adam(lr=learning_rate,clipnorm=0.001)\ndef get_lr_metric(optimizer):\n    def lr(y_true, y_pred):\n        return optimizer.lr\n    return lr\n\nlr_metric = get_lr_metric(optimizer)\n\nmodel.compile(optimizer=optimizer,\n             loss='categorical_crossentropy',\n             metrics=['accuracy',lr_metric])\n","fc7ff925":"\nreduce_lr = ReduceLROnPlateau('val_acc', factor=0.1, patience=1, verbose=1)\n\nmodel.fit(train_generator,\n          steps_per_epoch=100,\n          validation_data=(x_val,y_val),\n          epochs=40,verbose=1)\n\n","8f7e1bc7":"scoreSeg = model.evaluate_generator(valid_generator)\nprint(\"Accuracy = \",scoreSeg[1])\n\nfor i,j in valid_generator:\n    print(i.shape, j.shape)\n    p = model.predict(i)\n    p = p.argmax(-1)\n    t = j.argmax(-1)\n    print(classification_report(t,p))\n    print(confusion_matrix(t,p))\n    break;\n","6d27066b":"p","0cfe8cb1":"{i:j for j,i in valid_generator.class_indices.items()}","769c4633":"t","a08ecc7f":"tf.keras.models.save_model(\n    model,\n    \"cars1_half.h5\"\n)\n","be7d88ab":"1 \/196","8938f4c0":"ls -la","cc22efe4":"from IPython.display import FileLink\nFileLink('cars1_half2.h5')","be3d6710":"# Step 4 -> Evaluate the model","ffe05871":"## 2. Crop the images in the training folder\nThanks to Stanford, they provide crop dimensions for the car in each photo ","aa156aa7":"# Step 3 -> prepare the image generators","576ab143":"# Step 2 -> Image preprocessing\n## 1. New directories\nMake new directories to store the preprocessed images","42c84d7a":"# <font color='red'> Attempt 0: Build a tiny resnet from scratch <\/font>","02c53a4f":"# <font color='red'> Attempt 2: use mobilenet<\/font>","a27a0a59":"# Why a tiny model ?\nFor several real time applications that require the model to run local and every second, The accurate resnet architectures fail ! because they are very large models \n\n\n# Step 1 -> Load needed libraries","8257c8c6":"## 3. Crop the images in the test directory\nHere I will use them as validation data "}}