{"cell_type":{"8464fac0":"code","e5c79299":"code","3873f31a":"code","6dc37fb2":"code","e11e1488":"code","94c90288":"code","cae7d28a":"code","dffb61c4":"code","e16c5410":"code","8937ff53":"code","def5cedb":"code","bfd6b6db":"code","e6200950":"code","4328a469":"code","1f5bc61f":"code","2ccd9a65":"code","8f2eced3":"code","9741cd69":"code","2577df38":"code","e613acc0":"code","78441c0a":"code","25e518c6":"code","a23db584":"code","3657a493":"code","447ac70b":"code","18f0f27c":"code","17ff64b0":"code","ca841107":"code","b00c2862":"code","695cd057":"code","84b15970":"code","409f3ff7":"code","19f510c1":"code","9ba19387":"code","0c651925":"code","749ba66c":"code","0857b14b":"markdown","2aeec21b":"markdown","d988f9ae":"markdown","b233486d":"markdown","e8de6d43":"markdown","625c9f89":"markdown","3417e4b0":"markdown","1b44aa7b":"markdown","67bba418":"markdown","4f8a1645":"markdown","440f184e":"markdown","cc707797":"markdown","d9f1ae57":"markdown","6ee802df":"markdown","49d095ea":"markdown","0a7e1811":"markdown","d21643a5":"markdown","bac135ce":"markdown","0468b635":"markdown","07dee427":"markdown","76570b14":"markdown","a3e24ce3":"markdown","dbd14fa7":"markdown","1a3a7289":"markdown"},"source":{"8464fac0":"import numpy as np\nimport pandas as pd \nimport matplotlib as mpl\nimport matplotlib.pyplot as plt \nimport seaborn as sns \nimport warnings \nwarnings.filterwarnings('ignore')","e5c79299":"data = pd.read_csv('..\/input\/heart-attack-analysis-prediction-dataset\/heart.csv')\ndata.head()\n# output = target ","3873f31a":"data.describe(include='all')","6dc37fb2":"data.isnull().sum()","e11e1488":"# dtypes \ndata.dtypes","94c90288":"# Also check out the unique valeus in each columns\n\ndict = {}\nfor i in list(data.columns):\n    dict[i] = data[i].value_counts().shape[0]\n\npd.DataFrame(dict, index=['unique count']).transpose()","cae7d28a":"fig = plt.figure(figsize=(18,15))\ngs = fig.add_gridspec(3,3)\n\n\n\nax0 = fig.add_subplot(gs[0,0])\nax1 = fig.add_subplot(gs[0,1])\nax2 = fig.add_subplot(gs[0,2])\nax3 = fig.add_subplot(gs[1,0])\nax4 = fig.add_subplot(gs[1,1])\nax5 = fig.add_subplot(gs[1,2])\nax6 = fig.add_subplot(gs[2,0])\nax7 = fig.add_subplot(gs[2,1])\nax8 = fig.add_subplot(gs[2,2])\n\nax_sex = ax0\nsns.countplot(x='sex', data=data, ax=ax_sex, palette='YlGnBu')\nsns.despine()\n\nax_exng = ax1\nsns.countplot(x='exng',data=data, ax=ax_exng, palette='YlGnBu')\nsns.despine()\n\nax_caa = ax2\nsns.countplot(x='caa', data=data, ax=ax_caa, palette='YlGnBu')\nsns.despine()\n\nax_cp = ax3\nsns.countplot(x='cp', data=data, ax=ax_cp, palette='YlGnBu')\nsns.despine()\n\nax_fbs = ax4\nsns.countplot(x='fbs',data=data, ax=ax_fbs, palette='YlGnBu')\nsns.despine()\n\nax_restecg = ax5\nsns.countplot(x='restecg',  data=data, ax=ax_restecg, palette='YlGnBu')\nsns.despine()\n\nax_slp = ax6\nsns.countplot(x='slp',  data=data, ax=ax_slp, palette='YlGnBu')\nsns.despine()\n\nax_thall = ax7\nsns.countplot(x='thall',  data=data, ax=ax_thall, palette='YlGnBu')\nsns.despine()\n\nax8.spines[\"bottom\"].set_visible(False)\nax8.spines[\"left\"].set_visible(False)\nax8.spines[\"top\"].set_visible(False)\nax8.spines[\"right\"].set_visible(False)\nax8.tick_params(left=False, bottom=False)\nax8.set_xticklabels([])\nax8.set_yticklabels([])\n\n\nplt.show();","dffb61c4":"fig = plt.figure(figsize=(18,15))\ngs = fig.add_gridspec(3,3)\n\"\"\"sns.set_style(\"white\")\nsns.set_context(\"poster\", font_scale = 0.5)\"\"\"\n\nax0 = fig.add_subplot(gs[0,0])\nax1 = fig.add_subplot(gs[0,1])\nax2 = fig.add_subplot(gs[0,2])\nax3 = fig.add_subplot(gs[1,0])\nax4 = fig.add_subplot(gs[1,1])\nax5 = fig.add_subplot(gs[1,2])\nax6 = fig.add_subplot(gs[2,0])\nax7 = fig.add_subplot(gs[2,1])\nax8 = fig.add_subplot(gs[2,2])\n\nax_sex = ax0\nsns.countplot(x='sex', hue='output', data=data, ax=ax_sex, palette='YlGnBu')\nsns.despine()\n\nax_exng = ax1\nsns.countplot(x='exng', hue='output', data=data, ax=ax_exng, palette='YlGnBu')\nsns.despine()\n\nax_caa = ax2\nsns.countplot(x='caa', hue='output', data=data, ax=ax_caa, palette='YlGnBu')\nsns.despine()\n\nax_cp = ax3\nsns.countplot(x='cp', hue='output', data=data, ax=ax_cp, palette='YlGnBu')\nsns.despine()\n\nax_fbs = ax4\nsns.countplot(x='fbs', hue='output', data=data, ax=ax_fbs, palette='YlGnBu')\nsns.despine()\n\nax_restecg = ax5\nsns.countplot(x='restecg', hue='output', data=data, ax=ax_restecg, palette='YlGnBu')\nsns.despine()\n\nax_slp = ax6\nsns.countplot(x='slp', hue='output', data=data, ax=ax_slp, palette='YlGnBu')\nsns.despine()\n\nax_thall = ax7\nsns.countplot(x='thall', hue='output', data=data, ax=ax_thall, palette='YlGnBu')\nsns.despine()\n\nax8.spines[\"bottom\"].set_visible(False)\nax8.spines[\"left\"].set_visible(False)\nax8.spines[\"top\"].set_visible(False)\nax8.spines[\"right\"].set_visible(False)\nax8.tick_params(left=False, bottom=False)\nax8.set_xticklabels([])\nax8.set_yticklabels([])\n\nplt.show()","e16c5410":"print('1)', data.groupby(['sex', 'output'])['output'].count())\nprint('')\nprint('')\nprint('2)', data.groupby(['exng', 'output'])['output'].count())\nprint('')\nprint('')\nprint('3)', data.groupby(['caa', 'output'])['output'].count())\nprint('')\nprint('')\nprint('4)', data.groupby(['cp', 'output'])['output'].count())\nprint('')\nprint('')\nprint('5)', data.groupby(['fbs', 'output'])['output'].count())\nprint('')\nprint('')\nprint('6)', data.groupby(['restecg', 'output'])['output'].count())\nprint('')\nprint('')\nprint('7)', data.groupby(['slp', 'output'])['output'].count())\nprint('')\nprint('')\nprint('8)', data.groupby(['thall', 'output'])['output'].count())\n","8937ff53":"fig = plt.figure(figsize=(18,15))\ngs = fig.add_gridspec(2,3)\n\nax0 = fig.add_subplot(gs[0,0])\nax1 = fig.add_subplot(gs[0,1])\nax2 = fig.add_subplot(gs[0,2])\nax3 = fig.add_subplot(gs[1,0])\nax4 = fig.add_subplot(gs[1,1])\n\n\nax_age = ax0\nsns.kdeplot(x='age', hue='output', data=data, fill=True, alpha=.5, linewidth=0, ax=ax_age, palette='YlGnBu', shade=True)\nsns.despine()\n\nax_trtbps = ax1\nsns.kdeplot(x='trtbps', hue='output', data=data, fill=True, alpha=.5, linewidth=0, ax=ax_trtbps, palette='YlGnBu', shade=True)\nsns.despine()\n\nax_chol = ax2\nsns.kdeplot(x='chol', hue='output', data=data, fill=True, alpha=.5, linewidth=0, ax=ax_chol, palette='YlGnBu', shade=True)\nsns.despine()\n\nax_thalachh = ax3\nsns.kdeplot(x='thalachh', hue='output', data=data, fill=True, alpha=.5, linewidth=0, ax=ax_thalachh, palette='YlGnBu', shade=True)\nsns.despine()\n\nax_oldpeak = ax4\nsns.kdeplot(x='oldpeak', hue='output', data=data, fill=True, alpha=.5, linewidth=0, ax=ax_oldpeak, palette='YlGnBu', shade=True)\nsns.despine()\n\n","def5cedb":"fig = plt.figure(figsize=(18,15))\ngs = fig.add_gridspec(2,3)\n\nax0 = fig.add_subplot(gs[0,0])\nax1 = fig.add_subplot(gs[0,1])\nax2 = fig.add_subplot(gs[0,2])\nax3 = fig.add_subplot(gs[1,0])\nax4 = fig.add_subplot(gs[1,1])\n\n\nax_age = ax0\nsns.boxplot(x='age',data=data, ax=ax_age, palette='YlGnBu')\nsns.despine()\n\nax_trtbps = ax1\nsns.boxplot(x='trtbps',data=data, ax=ax_trtbps, palette='YlGnBu')\nsns.despine()\n\nax_chol = ax2\nsns.boxplot(x='chol',data=data, ax=ax_chol, palette='YlGnBu')\nsns.despine()\n\nax_thalachh = ax3\nsns.boxplot(x='thalachh',data=data, ax=ax_thalachh, palette='YlGnBu')\nsns.despine()\n\nax_oldpeak = ax4\nsns.boxplot(x='oldpeak',data=data, ax=ax_oldpeak, palette='YlGnBu')\nsns.despine()\n","bfd6b6db":"fig = plt.figure(figsize=(14,8))\ngs = fig.add_gridspec(3,4)\nsns.set_style(\"white\")\nsns.set_context(\"poster\", font_scale = 0.5)\n\nax_target = fig.add_subplot(gs[:2,:2])\nsns.countplot(x='output', data=data, ax=ax_target, palette='YlGnBu')\nsns.despine()\n\n","e6200950":"f,ax = plt.subplots(1,2, figsize=(15,8))\n\n\ndata.loc[data['output'] == 0].plot.hist(ax=ax[0], bins=20, edgecolor='black', color='lightgray')\nax[0].set_title('Age & target = 0')\n\ndata.loc[data['output'] == 1].plot.hist(ax=ax[1], bins=20, edgecolor='black', color='red')\nax[1].set_title('Age & target=1')\n\nplt.show();","4328a469":"data.corr()","1f5bc61f":"fig = plt.figure(figsize=(14,8))\nsns.heatmap(data.corr(), annot=True, cmap='YlGnBu')\nplt.show()","2ccd9a65":"cat_columns = ['sex','exng','caa','cp','fbs','restecg','slp','thall']\ncon_columns = [\"age\",\"trtbps\",\"chol\",\"thalachh\",\"oldpeak\"]\ntarget_column = [\"output\"]\n","8f2eced3":"# Categorical Features \ndata = pd.get_dummies(data=data, columns=cat_columns)\ndata.columns","9741cd69":"\"\"\"# delect Outlier \ndef outliers_iqr(data):\n    q1, q3 = np.percentile(data, [25, 75])\n    iqr = q3 - q1\n    lower_bound = q1 - (iqr * 1.5)\n    upper_bound = q3 + (iqr * 1.5)\n    \n    return np.where((data > upper_bound)|(data < lower_bound))\n\n# delete 'trtbps', 'chol', 'thalachh', 'oldpeak'\ntrtbps_outlier = outliers_iqr(data['trtbps'])[0]\nchol_outlier = outliers_iqr(data['chol'])[0]\nthalachh_outlier = outliers_iqr(data['thalachh'])[0]\noldpeak_outlier = outliers_iqr(data['oldpeak'])[0]\"\"\"","2577df38":"\"\"\"data.loc[trtbps_outlier, 'trtbps']\"\"\"","e613acc0":"\"\"\"data.loc[chol_outlier, 'chol']\"\"\"","78441c0a":"\"\"\"data.loc[thalachh_outlier, 'thalachh']\"\"\"","25e518c6":"\"\"\"data.loc[oldpeak_outlier, 'oldpeak']\"\"\"","a23db584":"\"\"\"# Concatenate all the array \nlead_outlier_index = np.concatenate((trtbps_outlier,\n                                     chol_outlier,\n                                     thalachh_outlier,\n                                     oldpeak_outlier), axis=None)\n\nprint(len(lead_outlier_index))\nlead_outlier_index\"\"\"","3657a493":"\"\"\"# Put into 'lead_not_outlier_index' which is not the outliers data \nlead_not_outlier_index = []\n\nfor i in data.index:\n    if i not in lead_outlier_index:\n        lead_not_outlier_index.append(i)\n\"\"\"","447ac70b":"\"\"\"data = data.loc[lead_not_outlier_index]\ndata = data.reset_index(drop=True)\ndata.columns\"\"\"","18f0f27c":"from sklearn.preprocessing import RobustScaler\n\nscaler = RobustScaler()\ndata[con_columns] = scaler.fit_transform(data[con_columns])\n\ndata[con_columns]","17ff64b0":"data.head()","ca841107":"data['output'].value_counts","b00c2862":"x = data.drop('output', axis=1)\ny = data['output'].values","695cd057":"from sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split(x,y,\n                                                    test_size=0.1,\n                                                    random_state=52,\n                                                    shuffle=True)","84b15970":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom lightgbm import LGBMClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom xgboost import XGBClassifier\n\nfrom sklearn.metrics import f1_score","409f3ff7":"# 1. LogisticRegression\n\nlr = LogisticRegression()\n\nlr.fit(x_train, y_train)\n\ny_pred = lr.predict(x_test)\n\nprint(f\"Logistic Regression F1 Score: {f1_score(y_test, y_pred, average='micro')}\")","19f510c1":"# 2. Support Vector Machine\n\nsvc = SVC(probability=True)\n\nsvc.fit(x_train, y_train)\n\ny_pred = svc.predict(x_test)\n\nprint(f\"Support Vector Machine F1 Score: {f1_score(y_test, y_pred, average='micro')}\")","9ba19387":"# 3. Rnadom Forest\n\nrf = RandomForestClassifier()\n\nrf.fit(x_train, y_train)\n\ny_pred = rf.predict(x_test)\n\nprint(f\"RandomForest F1 Score: {f1_score(y_test, y_pred, average='micro')}\")","0c651925":"# 4. LightGBM\n\nlgb = LGBMClassifier()\n\nlgb.fit(x_train, y_train)\n\ny_pred = lgb.predict(x_test)\n\nprint(f\"LightGBM F1 Score: {f1_score(y_test, y_pred, average='micro')}\")","749ba66c":"# 5. KNeighborsClassifier \n\nknn = KNeighborsClassifier()\n\nknn.fit(x_train, y_train)\n\ny_pred = knn.predict(x_test)\n\nprint(f\"KNeighborsClassifier F1 Score: {f1_score(y_test, y_pred, average='micro')}\")","0857b14b":"Now!! We delected all the outliers in the continous columns. \n\nLet's use the StandardScaler to do the feature engineering. ","2aeec21b":"# Part3: Predictive Modeling\n\n## 1) Running Basic Algorithms\n\nWe have gained some insights from the EDA part. But with that, we cannot accurately predict or tell whether a heart attack will occur or not.. So now we will predict by using some great Classification Algorithms. Following are the algorithms I will use to make the model:\n\n1) Logistic Regression\n\n2) Support Vector Machines(Linear and radial)\n\n3) Random Forest\n\n4) LightGBM\n\n5) KNeighborClassifier\n\n6) XGBoost","d988f9ae":"Now we can seperate our columns into categorical features and continouse features. \n\n**Categorical Features**:   sex, exng, caa, cp, fbs, restecg, slp, thall\n\n**Continous Features**:   age, trbps, chol, thalachh, oldpeak\n\n**Target Feature**:   output","b233486d":"# Part1: Exploratory Data Analysis(EDA):","e8de6d43":"# Part2: Feature Engineering and Data Cleaning:\nFirst devide the columns into Categorical feature and Continuous Features.\n\nI am going to use dummies values to categorical features and use StandardScaler to numerical features","625c9f89":"**First of all, we will going to check out all the features of our data**","3417e4b0":"## Every Categorical Features Distribution\n\n**Categorical Features**:   sex, exng, caa, cp, fbs, restecg, slp, thall\n","1b44aa7b":"## Target Feature","67bba418":"## Correlation Between Features ","4f8a1645":"## Categorical Features & Target Feature","440f184e":"\n\n# Contents of the Notebook :\n \n# Part1: Exploratory Data Analysis(EDA):\n#### 1)Analysis of the features.\n\n#### 2)Finding any relations or trends considering multiple features.\n\n# Part2: Feature Engineering and Data Cleaning:\n\n#### 1)Converting features into suitable form for modeling.\n\n# Part3: Predictive Modeling\n#### 1)Running Basic Algorithms.\n\n","cc707797":"## Check Out the Outlier in Continous Features","d9f1ae57":"## Data Check ","6ee802df":"**Continous Features**:   age, trbps, chol, thalachh, oldpeak","49d095ea":"* **Age** : Age of the patient\n* **Sex** : Sex of the patient\n* **exang**: exercise induced angina (1 = yes; 0 = no)\n* **cp** : Chest Pain type chest pain type\n\n     -Value 1: typical angina\n\n     -Value 2: atypical angina\n\n     -Value 3: non-anginal pain\n\n     -Value 4: asymptomatic\n* **ca**: number of major vessels (0-3)\n* **trtbps** : resting blood pressure (in mm Hg)\n* **chol** : cholestoral in mg\/dl fetched via BMI sensor\n* **fbs** : (fasting blood sugar > 120 mg\/dl) (1 = true; 0 = false)\n* **rest_ecg** : resting electrocardiographic results\n\n     -Value 0: normal\n\n     -Value 1: having ST-T wave abnormality (T wave inversions and\/or ST elevation or depression of > 0.05 mV)\n\n     -Value 2: showing probable or definite left ventricular hypertrophy by Estes' criteria\n\n* **thalach** : maximum heart rate achieved\n* **target** : 0= less chance of heart attack 1= more chance of heart attack","0a7e1811":"Categorical Features","d21643a5":"Great! There is no missing valeus in our data.\n\nNow let's check out all the data **dtypes** and **unique values** before EDA. We are going to conducte separately according to dtypes.","bac135ce":"Continuous Features\n\nWe found out some outlier in the continous features. So first before we start feature engineering, let's del all the outliers in the continous features. \n\n### But !!! Since the data is so small!! The result was bette when I didn't delect the outliers. ","0468b635":"NOW! The feature engineering is clear! Next we are going to split the train-test set and go modeling ~!","07dee427":"## Continous Features & Target","76570b14":"The target value is column 'output' !!! ","a3e24ce3":"## Train - Valid - Test split","dbd14fa7":"We can check out that there is some outlier in continous features. ","1a3a7289":"### If you liked the notebook, consider giving an upvote. \n### Feel free to give me any comments !!!"}}