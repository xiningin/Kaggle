{"cell_type":{"2f28a245":"code","e12a777b":"code","5cc3f56a":"code","5b5b9499":"code","e541d557":"code","a2cabf11":"code","6ee5a632":"code","b4fec86e":"code","25d2110a":"code","fc957c90":"code","35aed355":"code","9b4f9d6e":"code","2da34427":"code","06f52ade":"code","bd825672":"code","ea46d1cc":"code","e7a70537":"code","4ee3b5d4":"code","6234a46f":"code","6fe9531c":"code","3e930ce7":"code","fad39559":"code","ad2be411":"code","6fd0537e":"code","315f92c9":"code","acbb75af":"code","47c18a77":"code","b51ee15f":"code","9627ab61":"code","11ec68b6":"code","20b646f3":"code","77e2c631":"code","a82a3578":"code","d5135a62":"code","228432e2":"code","f4a52b1d":"code","5b996063":"code","aae2ae31":"code","b2de36f9":"code","acf57fee":"code","8a3e8263":"code","79a766db":"code","b5765c7b":"code","046e45af":"markdown","f7f7407e":"markdown","a294665b":"markdown","3bd9998d":"markdown","9aa80bdb":"markdown","b64bc397":"markdown"},"source":{"2f28a245":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","e12a777b":"import pandas as pd\nimport numpy as np\n%matplotlib Inline\ndf = pd.read_csv(r'\/kaggle\/input\/diabetes-data-set\/diabetes.csv')","5cc3f56a":"from sklearn.model_selection import KFold, StratifiedKFold, train_test_split,cross_val_score, cross_val_predict\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier","5b5b9499":"df","e541d557":"x = df.drop(['Outcome'],axis=1)","a2cabf11":" y = df['Outcome'] ","6ee5a632":"df.sort_index()","b4fec86e":"df.isnull().sum()","25d2110a":"df.info()","fc957c90":"df.dtypes","35aed355":"Tx, Tex, Ty, Tey = train_test_split(x,y, test_size=0.3, random_state=12)","9b4f9d6e":"modelLR = LogisticRegression(max_iter=200)","2da34427":"modelLR.fit(Tx,Ty)","06f52ade":"predict_LOg_Reg = modelLR.predict(Tex)","bd825672":"print(classification_report(predict_LOg_Reg,Tey))","ea46d1cc":"kfold = KFold(n_splits=15, random_state=12, shuffle=True)","e7a70537":"model_kfold = LogisticRegression(max_iter = 200)\n#results_kfold = cross_val_score(model_kfold, x, y, cv=kfold)\nkfold_predict = cross_val_predict(model_kfold, x, y, cv=kfold)","4ee3b5d4":"from statistics import mean, stdev","6234a46f":"# Stratified k_fold\nmodel_s_kfold = LogisticRegression(max_iter = 200)\n#results_kfold = cross_val_score(model_kfold, x, y, cv=kfold)\ncv1 = KFold(n_splits=10, random_state = 12, shuffle=True)  \ns_kfold_predict = cross_val_predict(model_kfold, x, y, cv=kfold)\nscores = cross_val_score(model_s_kfold, x, y, scoring='accuracy', cv=cv1, n_jobs=-1)\nprint('Accuracy: %.3f (%.3f)' % (mean(scores), stdev(scores)))","6fe9531c":"kfold_predict","3e930ce7":"len(kfold_predict)","fad39559":"# Cross Validation - Holdout Validation","ad2be411":"# K-Fold\ncv1 = KFold(n_splits=10, random_state = 12, shuffle=True)      \nscores = cross_val_score(modelLR, x, y, scoring='accuracy', cv=cv1, n_jobs=-1)\nscores\nprint('Accuracy: %.3f (%.3f)' % (mean(scores), stdev(scores)))","6fd0537e":"# Holdout\nresult = modelLR.score(Tx,Ty)\nprint(\"Accuracy: %.2f%%\" %(result*100.0))","315f92c9":"# Cross Validation: Stratified KFold - Validation","acbb75af":"from sklearn.model_selection import StratifiedKFold\nskfold = StratifiedKFold(n_splits=3, random_state=100, shuffle=True)\nmodel_skfold = LogisticRegression(solver ='liblinear')\nresults_skfold = cross_val_score(model_skfold, x, y, cv=skfold)\nprint(\"Accuracy: %.2f%%\" %(results_skfold.mean()*100.0))","47c18a77":"model_DTC = DecisionTreeClassifier()","b51ee15f":"model_DTC.fit(Tx,Ty)","9627ab61":"predict_LOg_Reg = modelLR.predict(Tex)","11ec68b6":"print(classification_report(predict_LOg_Reg,Tey))","20b646f3":"# Cross Validation - Kfold Validation\ncv1 = KFold(n_splits=10, random_state = 12, shuffle=True)      \nscores = cross_val_score(model_DTC, x, y, scoring='accuracy', cv=cv1, n_jobs=-1)\nscores\nprint('Accuracy: %.3f (%.3f)' % (mean(scores), stdev(scores)))","77e2c631":"# sTRATIFIED K-fOLD\nfrom sklearn.model_selection import StratifiedKFold\nskfold = StratifiedKFold(n_splits=3, random_state=100, shuffle=True)\nmodel_skfold = DecisionTreeClassifier()\nresults_skfold = cross_val_score(model_skfold, x, y, cv=skfold)\nprint(\"Accuracy: %.2f%%\" %(results_skfold.mean()*100.0))","a82a3578":"# holdout validation\nresult = model_DTC.score(Tx,Ty)\nprint(\"Accuracy: %.2f%%\" %(result*100.0))","d5135a62":"model_rfc = RandomForestClassifier()","228432e2":"model_rfc.fit(Tx,Ty)","f4a52b1d":"predict_rfc = modelLR.predict(Tex)","5b996063":"print(classification_report(predict_rfc,Tey))","aae2ae31":"# Cross Validation - Kfold Validation\ncv1 = KFold(n_splits=10, random_state = 12, shuffle=True)      \nscores = cross_val_score(model_rfc, x, y, scoring='accuracy', cv=cv1, n_jobs=-1)\nscores\nprint('Accuracy: %.3f (%.3f)' % (mean(scores), stdev(scores)))\n\n# sTRATIFIED K-fOLD\nfrom sklearn.model_selection import StratifiedKFold\nskfold = StratifiedKFold(n_splits=3, random_state=100, shuffle=True)\nmodel_skfold = DecisionTreeClassifier()\nresults_skfold = cross_val_score(model_skfold, x, y, cv=skfold)\nprint(\"Accuracy: %.2f%%\" %(results_skfold.mean()*100.0))\n\n# holdout validation\nresult = model_rfc.score(Tx,Ty)\nprint(\"Accuracy: %.2f%%\" %(result*100.0))","b2de36f9":"# Randomized Search CV\nfrom sklearn.model_selection import RandomizedSearchCV\nrandom_search = {'criterion': ['entropy', 'gini'],\n 'max_depth': list(np.linspace(5, 1200, 10, dtype = int)) + [None],\n 'max_features': ['auto', 'sqrt','log2', None],\n 'min_samples_leaf': [4, 6, 8, 12],\n 'min_samples_split': [3, 7, 10, 14],\n 'n_estimators': list(np.linspace(5, 1200, 3, dtype = int))}\nclf = RandomForestClassifier()\nmodel = RandomizedSearchCV(estimator = clf, param_distributions = random_search, \n cv = 4, verbose= 5, random_state= 101, n_jobs = -1)\nmodel.fit(Tx,Ty)\nmodel.best_params_","acf57fee":"#Grid Search\nfrom sklearn.model_selection import GridSearchCV\ngrid_search = {\n 'criterion': [model.best_params_['criterion']],\n 'max_depth': [model.best_params_['max_depth']],\n 'max_features': [model.best_params_['max_features']],\n 'min_samples_leaf': [model.best_params_['min_samples_leaf'] - 2, \n                      model.best_params_['min_samples_leaf'], \n                      model.best_params_['min_samples_leaf'] + 2],\n 'min_samples_split': [model.best_params_['min_samples_split'] - 3, \n                       model.best_params_['min_samples_split'], \n                       model.best_params_['min_samples_split'] + 3],\n 'n_estimators': [model.best_params_['n_estimators'] - 150, model.best_params_['n_estimators'] - 100,\n                  model.best_params_['n_estimators'], \n                  model.best_params_['n_estimators'] + 100, model.best_params_['n_estimators'] + 150]\n}\nprint(grid_search)","8a3e8263":"model = GridSearchCV(estimator = clf, param_grid = grid_search, cv = 4, verbose=5, n_jobs=-1)\nmodel.fit(Tx,Ty)","79a766db":"z = model.predict(Tex)","b5765c7b":"from matplotlib import pyplot as plt\nplt.hist(z)","046e45af":"# Logisitic Regression","f7f7407e":"# Cross Validation - K-Fold","a294665b":"# Decision Trees Classifier","3bd9998d":"# RandomForestClassifier","9aa80bdb":"# Write code for fine-tuning of hyperparameter of random forest and decision tree using random search and grid search. \n","b64bc397":"# Use the provided dataset title \u201cdiabetes.csv\u201d. Calculate the efficiency of models using Logistic regression, Decision tree, and Random forest \n#  Apply the logistic regression, random forest, and decision tree along with the holdout method, k-fold method, stratified k-fold"}}