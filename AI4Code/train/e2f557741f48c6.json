{"cell_type":{"5dcf6f70":"code","d3fc8650":"code","07ef7fa4":"code","2eebbee3":"code","e1452604":"code","9d0888c7":"code","6d21c916":"code","5b4ac302":"code","3e104c7a":"markdown"},"source":{"5dcf6f70":"import warnings\nwarnings.filterwarnings('ignore')\nimport pandas as pd\nimport numpy as np\npd.set_option('display.max_columns', 500)\npd.set_option('display.max_rows', 500)\nfrom typing import Union\nfrom tqdm.auto import tqdm as tqdm\nfrom sklearn import preprocessing\nimport gc\nimport lightgbm as lgb\nimport random\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","d3fc8650":"# consistent results\ndef seed_everything(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n\n# function to read our preprocessed data with some basic features for the experiment\ndef read_data():\n    # read main data\n    data = pd.read_pickle('\/kaggle\/input\/m5-small-fe\/data_small_fe.pkl')\n    # read calendar data\n    calendar = pd.read_csv('\/kaggle\/input\/m5-forecasting-accuracy\/calendar.csv')\n    # read price data\n    sell_prices = pd.read_csv('\/kaggle\/input\/m5-forecasting-accuracy\/sell_prices.csv')\n    # read submission\n    submission = pd.read_csv('\/kaggle\/input\/m5-forecasting-accuracy\/sample_submission.csv')\n    # get data to evaluate our model\n    sales_train_validation = pd.read_csv('\/kaggle\/input\/m5-forecasting-accuracy\/sales_train_validation.csv')\n    train_fold_df = sales_train_validation.iloc[:,:-28]\n    valid_fold_df = sales_train_validation.iloc[:,-28:]\n    del sales_train_validation\n    \n    return data, calendar, sell_prices, submission, train_fold_df, valid_fold_df","07ef7fa4":"# define lgbm simple model using custom loss and eval metric for early stopping\ndef run_lgb(data, features, custom_asymmetric_train, custom_asymmetric_valid):\n    \n    # going to evaluate with the last 28 days\n    x_train = data[data['date'] <= '2016-03-27']\n    y_train = x_train['demand']\n    x_val = data[(data['date'] > '2016-03-27') & (data['date'] <= '2016-04-24')]\n    y_val = x_val['demand']\n    test = data[(data['date'] > '2016-04-24')]\n    del data\n    gc.collect()\n\n    # define random hyperparammeters\n    params = {\n        'boosting_type': 'gbdt',\n        'objective': 'regression',\n        'n_jobs': -1,\n        'seed': 42,\n        'learning_rate': 0.1,\n        'bagging_fraction': 0.75,\n        'bagging_freq': 10, \n        'colsample_bytree': 0.75}\n\n    train_set = lgb.Dataset(x_train[features], y_train)\n    val_set = lgb.Dataset(x_val[features], y_val)\n    \n    del x_train, y_train\n\n    model = lgb.train(params, train_set, num_boost_round = 2500, early_stopping_rounds = 50, \n                      valid_sets = [train_set, val_set], verbose_eval = 100, fobj = custom_asymmetric_train, \n                      feval = custom_asymmetric_valid)\n    \n    val_pred = model.predict(x_val[features])\n    y_pred = model.predict(test[features])\n    x_val['demand'] = val_pred\n    test['demand'] = y_pred\n    x_val = x_val[['id', 'date', 'demand']]\n    test = test[['id', 'date', 'demand']]\n\n    return x_val, test","2eebbee3":"# class to evaluate our model\nclass WRMSSEEvaluator(object):\n    \n    group_ids = ( 'all_id', 'state_id', 'store_id', 'cat_id', 'dept_id', 'item_id',\n        ['state_id', 'cat_id'],  ['state_id', 'dept_id'], ['store_id', 'cat_id'],\n        ['store_id', 'dept_id'], ['item_id', 'state_id'], ['item_id', 'store_id'])\n\n    def __init__(self, \n                 train_df: pd.DataFrame, \n                 valid_df: pd.DataFrame, \n                 calendar: pd.DataFrame, \n                 prices: pd.DataFrame):\n        '''\n        intialize and calculate weights\n        '''\n        self.calendar = calendar.copy()\n        self.prices = prices.copy()\n        self.train_df = train_df.copy()\n        self.valid_df = valid_df.copy()\n        self.train_target_columns = [i for i in self.train_df.columns if i.startswith('d_')]\n        self.weight_columns = self.train_df.iloc[:, -28:].columns.tolist()\n\n        self.train_df['all_id'] = \"all\"\n\n        self.id_columns = [i for i in self.train_df.columns if not i.startswith('d_')]\n        self.valid_target_columns = [i for i in self.valid_df.columns if i.startswith('d_')]\n\n        if not all([c in self.valid_df.columns for c in self.id_columns]):\n            self.valid_df = pd.concat([self.train_df[self.id_columns], self.valid_df],\n                                      axis=1, \n                                      sort=False)\n        self.train_series = self.trans_30490_to_42840(self.train_df, \n                                                      self.train_target_columns, \n                                                      self.group_ids)\n        self.valid_series = self.trans_30490_to_42840(self.valid_df, \n                                                      self.valid_target_columns, \n                                                      self.group_ids)\n        self.weights = self.get_weight_df()\n        self.scale = self.get_scale()\n        self.train_series = None\n        self.train_df = None\n        self.prices = None\n        self.calendar = None\n\n    def get_scale(self):\n        '''\n        scaling factor for each series ignoring starting zeros\n        '''\n        scales = []\n        for i in tqdm(range(len(self.train_series))):\n            series = self.train_series.iloc[i].values\n            series = series[np.argmax(series!=0):]\n            scale = ((series[1:] - series[:-1]) ** 2).mean()\n            scales.append(scale)\n        return np.array(scales)\n    \n    def get_name(self, i):\n        '''\n        convert a str or list of strings to unique string \n        used for naming each of 42840 series\n        '''\n        if type(i) == str or type(i) == int:\n            return str(i)\n        else:\n            return \"--\".join(i)\n    \n    def get_weight_df(self) -> pd.DataFrame:\n        \"\"\"\n        returns weights for each of 42840 series in a dataFrame\n        \"\"\"\n        day_to_week = self.calendar.set_index(\"d\")[\"wm_yr_wk\"].to_dict()\n        weight_df = self.train_df[[\"item_id\", \"store_id\"] + self.weight_columns].set_index(\n            [\"item_id\", \"store_id\"]\n        )\n        weight_df = (\n            weight_df.stack().reset_index().rename(columns={\"level_2\": \"d\", 0: \"value\"})\n        )\n        weight_df[\"wm_yr_wk\"] = weight_df[\"d\"].map(day_to_week)\n        weight_df = weight_df.merge(\n            self.prices, how=\"left\", on=[\"item_id\", \"store_id\", \"wm_yr_wk\"]\n        )\n        weight_df[\"value\"] = weight_df[\"value\"] * weight_df[\"sell_price\"]\n        weight_df = weight_df.set_index([\"item_id\", \"store_id\", \"d\"]).unstack(level=2)[\n            \"value\"\n        ]\n        weight_df = weight_df.loc[\n            zip(self.train_df.item_id, self.train_df.store_id), :\n        ].reset_index(drop=True)\n        weight_df = pd.concat(\n            [self.train_df[self.id_columns], weight_df], axis=1, sort=False\n        )\n        weights_map = {}\n        for i, group_id in enumerate(tqdm(self.group_ids, leave=False)):\n            lv_weight = weight_df.groupby(group_id)[self.weight_columns].sum().sum(axis=1)\n            lv_weight = lv_weight \/ lv_weight.sum()\n            for i in range(len(lv_weight)):\n                weights_map[self.get_name(lv_weight.index[i])] = np.array(\n                    [lv_weight.iloc[i]]\n                )\n        weights = pd.DataFrame(weights_map).T \/ len(self.group_ids)\n\n        return weights\n\n    def trans_30490_to_42840(self, df, cols, group_ids, dis=False):\n        '''\n        transform 30490 sries to all 42840 series\n        '''\n        series_map = {}\n        for i, group_id in enumerate(tqdm(self.group_ids, leave=False, disable=dis)):\n            tr = df.groupby(group_id)[cols].sum()\n            for i in range(len(tr)):\n                series_map[self.get_name(tr.index[i])] = tr.iloc[i].values\n        return pd.DataFrame(series_map).T\n    \n    def get_rmsse(self, valid_preds) -> pd.Series:\n        '''\n        returns rmsse scores for all 42840 series\n        '''\n        score = ((self.valid_series - valid_preds) ** 2).mean(axis=1)\n        rmsse = (score \/ self.scale).map(np.sqrt)\n        return rmsse\n\n    def score(self, valid_preds: Union[pd.DataFrame, np.ndarray]) -> float:\n        assert self.valid_df[self.valid_target_columns].shape == valid_preds.shape\n\n        if isinstance(valid_preds, np.ndarray):\n            valid_preds = pd.DataFrame(valid_preds, columns=self.valid_target_columns)\n\n        valid_preds = pd.concat([self.valid_df[self.id_columns], valid_preds],\n                                axis=1, \n                                sort=False)\n        valid_preds = self.trans_30490_to_42840(valid_preds, \n                                                self.valid_target_columns, \n                                                self.group_ids, \n                                                True)\n        self.rmsse = self.get_rmsse(valid_preds)\n        self.contributors = pd.concat([self.weights, self.rmsse], \n                                      axis=1, \n                                      sort=False).prod(axis=1)\n        return np.sum(self.contributors)","e1452604":"# function to evaluate our model results (get WRMSSE)\ndef evaluate(x_val, train_fold_df, valid_fold_df, calendar, sell_prices):\n    x_val = pd.pivot(x_val, index = 'id', columns = 'date', values = 'demand').reset_index()\n    x_val.columns = ['id'] + ['d_' + str(i) for i in range(1886, 1914)]\n    x_val = train_fold_df[['id']].merge(x_val, on = 'id')\n    x_val.drop('id', axis = 1, inplace = True)\n    evaluator = WRMSSEEvaluator(train_fold_df, valid_fold_df, calendar, sell_prices)\n    score = evaluator.score(x_val)\n    return score\n\ndef predict(test, submission):\n    predictions = test[['id', 'date', 'demand']]\n    predictions = pd.pivot(predictions, index = 'id', columns = 'date', values = 'demand').reset_index()\n    predictions.columns = ['id'] + ['F' + str(i + 1) for i in range(28)]\n\n    evaluation_rows = [row for row in submission['id'] if 'evaluation' in row] \n    evaluation = submission[submission['id'].isin(evaluation_rows)]\n\n    validation = submission[['id']].merge(predictions, on = 'id')\n    final = pd.concat([validation, evaluation])\n    final.to_csv('submission.csv', index = False)","9d0888c7":"# define cost and eval functions\ndef custom_asymmetric_train(y_pred, y_true):\n    y_true = y_true.get_label()\n    residual = (y_true - y_pred).astype(\"float\")\n    grad = np.where(residual < 0, -2 * residual, -2 * residual * 1.15)\n    hess = np.where(residual < 0, 2, 2 * 1.15)\n    return grad, hess\n\ndef custom_asymmetric_valid(y_pred, y_true):\n    y_true = y_true.get_label()\n    residual = (y_true - y_pred).astype(\"float\")\n    loss = np.where(residual < 0, (residual ** 2) , (residual ** 2) * 1.15) \n    return \"custom_asymmetric_eval\", np.mean(loss), False","6d21c916":"# seed everything\nseed_everything(42)\n# reading data\ndata, calendar, sell_prices, submission, train_fold_df, valid_fold_df = read_data()\n# get feature columns, also ignoring some features because we dont have enough ram and they overffit\nfeatures = [col for col in data.columns if col not in ['id', 'demand', 'part', 'date', 'wm_yr_wk', 'mean_demand_month', 'std_demand_month', 'max_demand_month', 'mean_demand_week', 'std_demand_week', \n                                                       'max_demand_week']]\n\nprint(f'We are training with {len(features)} fetures')\ndata.tail()","5b4ac302":"# run model with asymmetric loss function\nx_val, test = run_lgb(data, features, custom_asymmetric_train, custom_asymmetric_valid)\nscore = evaluate(x_val, train_fold_df, valid_fold_df, calendar, sell_prices)\nprint(f'Our wrmsse is {score}')\n# predict test\npredict(test, submission)","3e104c7a":"# Objective\n\n* Try different asymmetric functions and check leaderboard\n\n* Going to try 3 different versions, no weight, 1.10 weight, 1.15 weight.\n\n# Comments\n\n* Good results are being obtained by using rmsse as the cost function and weights in the lgb.Dataset creation.\n* Also train 28 models, where each model is used to predict 1 day. This way we can use recent information (demand lags).\n\nIn the future im going to try this aproaches\n\n* Another important point is the validation strategy. I dont believe that validation of the last 28 days is a good strategy. Maybee, time series split validation (evaluate last 3 months) or evaluate the same period of the previous year is a good strat (just some ideas, need to check all of them).\n\n# Results\n\n* Experiment 1: \n\ncv -> 0.58130\n\nlb -> 0.66572\n\n* Experiment 2: \n\ncv -> 0.61086\n\nlb -> 0.59939\n\nLook at that using asymmetric cost function increase our lb but decrease our cv, wired.....something is wrong???\n\n* Experiment 3:\n\ncv -> Check the end of the notebook\n\nlb -> Check the score of the notebook\n\nI hope this results help you!."}}