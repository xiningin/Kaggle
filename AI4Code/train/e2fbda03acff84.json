{"cell_type":{"940ce4e4":"code","9fd6c063":"code","3eb32e5f":"code","33e4bc46":"code","4aae26e1":"code","d9acc3c9":"markdown","488a10e4":"markdown","878068c0":"markdown","13bde6df":"markdown"},"source":{"940ce4e4":"import numpy as np\nimport torch ","9fd6c063":"#Dataset\nx = np.array([1,2,3,4], dtype=np.float32)\ny = np.array([2,4,6,8], dtype=np.float32) #y = x^2\n\n#forward pass\ndef forward_pass(x):\n    return x*w\n\n#mse loss\ndef loss_compute(y,yhat):\n    return np.mean((yhat-y)**2)\n\n#gradient computation\n# mse = ((yhat-y)**2)\/N\n# dJ\/dy = 2(yhat-y)\/N\n# dy\/dw = x\n# So, dJ\/dw = 2x(yhat-y)\/N\n#but yhat = x * w\n#So, dJ\/dw = 2x(w*x - y)\/N\n\ndef gradient(y,yhat,x):\n    return np.mean(np.dot(2*x, (yhat - y)))\n\nlearning_rate = 0.01\nepochs = 100\nw = 0.0\nfor ep in range(epochs):\n    ypred = forward_pass(x)\n    loss = loss_compute(y,ypred)\n    \n    if (ep%10==0):\n        print(\"Loss in epoch {} is {}\".format(ep+1,loss))\n\n    dj_dw = gradient(y,ypred,x)\n    w -= learning_rate*dj_dw","3eb32e5f":"x = torch.tensor([1,2,3,4], dtype = torch.float32)\ny = torch.tensor([2,4,6,8], dtype = torch.float32)\n\nweights = torch.tensor(0.0, dtype = torch.float32, requires_grad=True)\n\ndef forward_pass(x):\n    return x * weights\n\ndef loss_compute(y,yhat):\n    return ((yhat-y)**2).mean()\n\nlearning_rate = 0.1\nepochs = 100\nfor ep in range(epochs):\n    ypred = forward_pass(x)\n    loss = loss_compute(y,ypred)\n    loss.backward()\n    \n    if (ep%10==0):\n        print(\"Loss in epoch {} is {}\".format(ep+1,loss))\n    \n    with torch.no_grad():\n        weights -= learning_rate * weights.grad\n    weights.grad.zero_()\n    \nprint(\"Predicted value of 7.5 is {}\".format(forward_pass(7.5)))","33e4bc46":"import torch.nn as nn","4aae26e1":"x = torch.tensor([[1],[2],[3],[4]], dtype=torch.float32)\ny = torch.tensor([[2],[4],[6],[8]], dtype=torch.float32)\n\nn_samples, n_features = x.shape\n\ninput_size = n_features\noutput_size = n_features\n\nmodel = nn.Linear(input_size, output_size)\n\nloss = nn.MSELoss()\noptimizer = torch.optim.Adam(model.parameters(), lr = 0.01) #Adam or RMSprop\nepochs = 1000\n\nfor ep in range(epochs):\n    ypred = model(x)\n    \n    l = loss(y,ypred)    \n    l.backward()\n    \n    if (ep%100==0):\n        [w,b] = model.parameters()\n        print(\"Loss in epoch {} is {}\".format(ep+1,l))\n    \n    optimizer.step()       #gradient update\n    optimizer.zero_grad()  #zero gradients\n    \ntest_sample = torch.tensor([7.5], dtype=torch.float32)\n    \nprint(model(test_sample).item())\nprint(\"Final parameters: {},{}\".format(w,b))","d9acc3c9":"Loss is decreasing with more epochs!!","488a10e4":"# Part 1: Using only Numpy","878068c0":"# Pytorch Training Pipeline: Model, Loss, optimizer","13bde6df":"# Part 2: Using Only Pytorch"}}