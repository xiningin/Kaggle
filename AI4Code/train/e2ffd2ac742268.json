{"cell_type":{"8fac594a":"code","7ba9f56b":"code","f729cf68":"code","85b3fe53":"code","36d8e56f":"code","3c1097d8":"code","7fabdddb":"code","a56d2216":"code","6d45cc55":"code","06213e34":"code","ab876ff0":"code","10c5e95a":"code","dcb9eb76":"code","48435e76":"code","73162789":"code","b4c15326":"code","7067366c":"code","37de57ed":"code","4b867f2c":"code","8fbf3c4a":"code","6eeafe7f":"code","758f5ba8":"code","25c0013b":"code","9273e855":"code","f50289eb":"markdown","1aa70eff":"markdown","f4dc92b3":"markdown","817c1642":"markdown","73f0ad8e":"markdown","88b3f9fb":"markdown","ba008aab":"markdown","269f3f36":"markdown","7b8cd7e4":"markdown","f2712b87":"markdown","e650f82b":"markdown","73f76cbf":"markdown","801a2f2b":"markdown","bdd61702":"markdown","9fc041a4":"markdown","8378dd63":"markdown","07276a48":"markdown","bacca665":"markdown","4783ed7a":"markdown","73640728":"markdown","04887cb3":"markdown","f861030a":"markdown","ab8a3515":"markdown","5172e225":"markdown"},"source":{"8fac594a":"import pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nfrom sklearn.naive_bayes import MultinomialNB\n\nimport nltk\n#nltk.download()\nfrom nltk.tokenize import word_tokenize\n\nimport warnings\nwarnings.filterwarnings('ignore')","7ba9f56b":"import os \nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames: \n        print(os.path.join(dirname, filename))","f729cf68":"data = pd.read_csv(os.path.join(dirname, filename))\ndata.head()","85b3fe53":"data.shape","36d8e56f":"data = data.drop('Unnamed: 0', axis=1)\ndata.head()","3c1097d8":"data.info()","7fabdddb":"# website_url\n\ndata.website_url.nunique()","a56d2216":"# Dropping duplicate entries\n\ndata = data.drop_duplicates(subset='website_url').reset_index(drop=True)\ndata.shape","6d45cc55":"# Category\n\ndata.Category.value_counts(dropna=False)","06213e34":"# cleaned_website_text\n\ndata.cleaned_website_text","ab876ff0":"# filtering noun words from corpus\n\nfiltered_words = []\nfor i in range(data.shape[0]):\n    tagged = nltk.pos_tag(word_tokenize(data.cleaned_website_text[i]), tagset='universal')\n    filtered_words.append(' '.join([word_tag[0] for word_tag in tagged if word_tag[1]=='NOUN']))","10c5e95a":"# adding filtered_words to our dataframe\n\ndata['filtered_words'] = filtered_words\ndata.head()","dcb9eb76":"# Vectorized form of tfidf \n\ntfidf = TfidfVectorizer(sublinear_tf=True,\n                        min_df=5,\n                        stop_words = 'english',\n                        max_features = 1500)\nfeat = tfidf.fit_transform(data.filtered_words)","48435e76":"# feature\/term names\n\ntfidf.get_feature_names()","73162789":"# Converting tfidfvector to dataframe\n\nX = pd.DataFrame(feat.toarray(), columns = tfidf.get_feature_names())\nX.head()","b4c15326":"# checking range for values in X\n\nX.describe()","7067366c":"# encoding labes to derive y\n\nle = preprocessing.LabelEncoder()\ny = le.fit_transform(data.Category)\ny","37de57ed":"df_f = pd.concat([X, data.Category], axis=1)\ndf_f.head()","4b867f2c":"plt.subplots(figsize=(10,15))\nfor i, col in enumerate(df_f.columns[0:5]):\n    plt.subplot(int(df_f.columns[0:5].shape[0]\/2)+1, 2, i+1)\n    df_f.groupby('Category').mean()[col].plot(kind='barh', title=col)","8fbf3c4a":"# train-test split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, \n                                     train_size = 0.7, \n                                     test_size = 0.3, \n                                     random_state = 100,\n                                     stratify = y)","6eeafe7f":"# Model development \n\nmodel = MultinomialNB()\nmodel.fit(X_train, y_train)","758f5ba8":"# Making prediction and evaluation\n\ny_train_pred = model.predict(X_train)\ny_test_pred = model.predict(X_test)\n\n# Accuracy check\n\nprint('Train accuracy:', round(accuracy_score(y_train, y_train_pred),2))\nprint('Test accuracy:', round(accuracy_score(y_test, y_test_pred),2))","25c0013b":"plt.subplots(figsize=(12,8))\nsns.heatmap(confusion_matrix(y_train, y_train_pred),\n           annot=True,\n           cmap='YlGnBu')","9273e855":"plt.subplots(figsize=(12,8))\nsns.heatmap(confusion_matrix(y_test, y_test_pred),\n           annot=True,\n           cmap='YlGnBu')","f50289eb":"### Reading Dataset","1aa70eff":"Before developing TF-IDF based array we need to filter out words with `Parts Of Speech (POS)` other than `NOUN`. The logic behind it is that `NOUN` gives better representation of website content as compared to other `POS`.\n\nHere we would be using `NLTK` library to tokenize the corpus from each website and assign `POS` tag to each word. Then we would filter in only `NOUN` tagged words and reverse convert them into corpus. The filtered words corpus would be meaningless for us but can enhance the performance of our model.","f4dc92b3":"## 4. Data Visualization","817c1642":"## 3. Data Preparation","73f0ad8e":"#### Confusion Matrix","88b3f9fb":"Let's develop TF-IDF based array using `filtered_words` column from out dataset. Here we have selected some hyperparameters as per following.\n\n- min_df: When building the vocabulary ignore terms that have a document frequency strictly lower than the given threshold. This value is also called cut-off in the literature. We would use 5 as min frequency.\n\n- stop_words: We would remove English stopwords if at all they are present in the filtered words. Stopwords adds no meaning to the model.\n\n- max_features: Maximum features to be included the the data. We are going to use 1500 features. \n\nNote: Naive Bayes algorithm can easily handle larger number of features as compared to data size.","ba008aab":"Data is having 1408 entries.\nHere we have first column which indicates serial hence we would be dropping it.","269f3f36":"## 7. Conclusion\/Recommendation","7b8cd7e4":"## 2. Data Cleaning","f2712b87":"### Importing Dependencies","e650f82b":"-----------------","73f76cbf":"## 6. Model Development-validation and evaluation","801a2f2b":"It can be observed that `website_url` column is having 1384 unique entries. That means we have some duplicate entries in our dataset. We would be dropping those entries. ","bdd61702":"#### Feature matrix development method:\n\nWe would extract words from `cleaned_website_text` columns and create an array of those words. Out of that array we would filter out english language stop_words which generally doesn't give any meaningful insight about the document. Out of the remaining words we would chose top `n` most recurring words and consider them as feature. \n\nNext part is to assign feature value to each of the data entry. For assigning feature value, we would use `TF-IFD` method.\n\n`TF-IDF:` TF-IDF is a method of comparing how relevant a word is to any particular document (here website) from a given bunch of documents. \n\n$TF-IDF = TF(t,d)*IDF(t,D)$\n\nwhere, \n\nTF(t,d) = (count of t in d)\/(number of words in d)\n\n$IDF(t,D) = log(D\/(df + 1))$\n\n$t$--> term of interest\n\n$d$ --> document of interest\n\n$D$ --> Set of documents\n\n$df$ --> occurrence of t in documents\n\nTo summarize, TF-IDF score of any feature for any particular document would be high if that feature term occurs rarely in other documents and\/or percentage occurrence for that term in the given document is higher.\n\n\nAdditional Reading Source: \nhttps:\/\/towardsdatascience.com\/tf-idf-for-document-ranking-from-scratch-in-python-on-real-world-dataset-796d339a4089","9fc041a4":"Dataset is not having any null entry hence we do not need to treat it for null values.","8378dd63":"Confusion matrix indicates that,\n\n- Model is not able to estimate category 0 and 6 correctly both while training and testing. The reason can be their lesser frequency.\n- Model is clearly not overfitting which is a good sign.","07276a48":"`Category` column is having 16 unique entries, which means this is a multi-class classification problem. Hence, we would be using `MultinomialNB` algorithm to solve it. MultinomialNB is efficient algorithm and can handle multi-class problems easily.","bacca665":"## 1. Data exploring and understanding","4783ed7a":"## 5. Train-test split","73640728":"Observations,\n\n- Term `accessories` is more related to `E-Commerce`. \n- Term `accessibility` is more related to `Law and Government`. ","04887cb3":"- NLP can be used for text processing and feature development to be used in ML model building.\n- Multinomial Naive Bayes algorithm can be used to perform multi-class classification with large number of classes.\n- Naive Bayes in general can handle large number of features as compared to data size quite easily.\n- For future work, we can increase number of data entries for classes with less frequency and hence improve the model further.","f861030a":"Dataset is having 4 columns including target variable `Category`.","ab8a3515":"`cleaned_website_text` column contains the content text from the website. We would be using this column to develop features for out model.","5172e225":"# Problem Statement\n\nAim of this exercise is to classify the websites based on their content. Here, we have got a dataset which contains list of websites, their kind and text extract from them.\n\nHere, we would be using `Natural Language Processing(NLP)` techniques to develop out dataset and then create a Naive Bayes model based on it to classify websites.\n\n\n## Methodology\n\n1. Data exploring and understanding: This step involves preliminary level data understanding and exploring.\n\n2. Data Cleaning: This step includes cleaning the existing data. We check the data for any missing values and treat them as per the requirements. We also need to look for constant value columns as that is not going to add any additional value to out analysis. Sometimes columns with very high proportion of any particular value also doesn't add any values. Hence, getting rid of them helps with further analysis.\n\n3. Data Preparation: This step is mainly useful for feeding in the data into the model. It involves steps like creating dummy variables, scaling etc. depending upon the data type.  Here, we would be using `NLP` techniques to develop our dataset which can be fed to model.\n\n4. Data Visualization: This step involves visualizing our dataset and check for relationship amongst independent variables. We can also reduce some feature columns here but it should not be aggressive.\n\n5. Train-test split: This step involves splitting the dataset into train and test parts.\n\n6. Model Development-validation and evaluation: This steps involves training the model and validate it. It involves evaluating the model using relevant metrics.\n\n7. Conclusion\/Recommendation: It involves drawing conclusions and recommendations to business."}}