{"cell_type":{"9a354b0e":"code","63bac708":"code","8e0699d2":"code","99db2c08":"code","5459eea8":"code","88a718e5":"code","f9efc81f":"code","7f5e81da":"code","8ada444a":"code","50f4b657":"code","5f55949e":"code","29b08d9c":"code","917bb6f8":"code","c18d40a8":"code","58f93253":"code","d9c21127":"code","6cec8067":"code","037436b6":"code","724f3b61":"code","fdb4bc97":"code","5f3228a7":"markdown","ff006f0f":"markdown","842cdad2":"markdown","5b0394bb":"markdown","8492d886":"markdown","c51456d8":"markdown","5d2652ba":"markdown","67c0a7cf":"markdown","f434fb11":"markdown","fd9131ca":"markdown","9c621657":"markdown","91dc57a1":"markdown","6637a733":"markdown","315be7a9":"markdown","6f450cd8":"markdown","38a6e75f":"markdown","f60e7e34":"markdown","499aa7c5":"markdown","21f8231e":"markdown","e053f1fd":"markdown","e2dd278d":"markdown"},"source":{"9a354b0e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","63bac708":"import warnings\nwarnings.filterwarnings(\"ignore\") #To ignore warnings to get clean output\nimport pandas as pd\nimport matplotlib.pyplot as plt","8e0699d2":"data=pd.read_csv(\"\/kaggle\/input\/twitter-airline-sentiment\/Tweets.csv\")\nprint(len(data))#total number of entries\ndata.head()\n","99db2c08":"data.isnull().sum()","5459eea8":"data.drop(columns=['negativereason','negativereason_confidence',\n                  'airline_sentiment_gold','tweet_coord','negativereason_gold','tweet_location','user_timezone'\n                  ,'tweet_id','retweet_count','name','tweet_created','airline_sentiment_confidence'],inplace=True)","88a718e5":"data['token_length'] = [len(x.split(\" \")) for x in data.text]\nmax(data.token_length)","f9efc81f":"data.airline_sentiment.value_counts()","7f5e81da":"data['airline'].unique()","8ada444a":"count_neg={}\ncount_pos={}\ncount_neu={}\nfor i in data['airline'].unique():\n    x=len(data.loc[(data['airline']==i) & (data['airline_sentiment']=='negative')])\n    count_neg.update({i:x})\nfor i in data['airline'].unique():\n    x=len(data.loc[(data['airline']==i) & (data['airline_sentiment']=='positive')])\n    count_pos.update({i:x})\nfor i in data['airline'].unique():\n    x=len(data.loc[(data['airline']==i) & (data['airline_sentiment']=='neutral')])\n    count_neu.update({i:x})","50f4b657":"print(count_neg)\nprint(count_pos)\nprint(count_neu)","5f55949e":"import numpy as np\n# set width of bar\nbarWidth = 0.25\nplt.figure(figsize=(20,10))\n \n# set height of bar\nbars1 = count_neg.values()\nbars2 = count_pos.values()\nbars3 = count_neu.values()\n \n# Set position of bar on X axis\nr1 = np.arange(len(bars1))\nr2 = [x + barWidth for x in r1]\nr3 = [x + barWidth for x in r2]\n \n# Make the plot\nplt.bar(r1, bars1, color='#7f6d5f', width=barWidth, edgecolor='white', label='neg')\nplt.bar(r2, bars2, color='#557f2d', width=barWidth, edgecolor='white', label='pos')\nplt.bar(r3, bars3, color='#2d7f5e', width=barWidth, edgecolor='white', label='neu')\n \n# Add xticks on the middle of the group bars\nplt.xlabel('Airlines', fontweight='bold',fontsize=18)\nplt.xticks([r + barWidth for r in range(len(bars1))], [i for i in data['airline'].unique()],fontsize=16)\n                                                       \n \n# Create legend & Show graphic\nplt.legend(fontsize=\"x-large\")\nplt.show()\n\n","29b08d9c":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\ntvec = TfidfVectorizer(stop_words=None, max_features=100000, ngram_range=(1, 3))\nlr = LogisticRegression()\n\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import precision_score, recall_score, f1_score\n\ndef lr_cv(splits, X, Y, pipeline, average_method):\n    \n    kfold = StratifiedKFold(n_splits=splits, shuffle=True, random_state=777)\n    accuracy = []\n    precision = []\n    recall = []\n    f1 = []\n    for train, test in kfold.split(X, Y):\n        lr_fit = pipeline.fit(X[train], Y[train])\n        prediction = lr_fit.predict(X[test])\n        scores = lr_fit.score(X[test],Y[test])\n        \n        accuracy.append(scores * 100)\n        precision.append(precision_score(Y[test], prediction, average=average_method)*100)\n        print('              negative    neutral     positive')\n        print('precision:',precision_score(Y[test], prediction, average=None))\n        recall.append(recall_score(Y[test], prediction, average=average_method)*100)\n        print('recall:   ',recall_score(Y[test], prediction, average=None))\n        f1.append(f1_score(Y[test], prediction, average=average_method)*100)\n        print('f1 score: ',f1_score(Y[test], prediction, average=None))\n        print('-'*50)\n\n    print(\"accuracy: %.2f%% (+\/- %.2f%%)\" % (np.mean(accuracy), np.std(accuracy)))\n    print(\"precision: %.2f%% (+\/- %.2f%%)\" % (np.mean(precision), np.std(precision)))\n    print(\"recall: %.2f%% (+\/- %.2f%%)\" % (np.mean(recall), np.std(recall)))\n    print(\"f1 score: %.2f%% (+\/- %.2f%%)\" % (np.mean(f1), np.std(f1)))","917bb6f8":"from nltk.tokenize import TweetTokenizer\nimport re\nfrom nltk.corpus import stopwords\nfrom nltk.stem import LancasterStemmer\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import wordnet\nimport nltk\n\ndef clean_tweet(tweet):\n    return ''.join(re.sub(r\"(@[A-Za-z0-9]+)|(http\\S+)|(#[A-Za-z0-9]+)|(\\$[A-Za-z0-9]+)|(RT)|([0-9]+)\",\"\",tweet))\ndef remove_special_chars(tweets):  # it unrolls the hashtags to normal words\n    for remove in map(lambda r: re.compile(re.escape(r)), [\",\", \":\", \"\\\"\", \"=\", \"&\", \";\", \"%\", \"$\",\n                                                                     \"@\", \"%\", \"^\", \"*\", \"(\", \")\", \"{\", \"}\",\n                                                                     \"[\", \"]\", \"|\", \"\/\", \"\\\\\", \">\", \"<\", \"-\",\n                                                                     \"!\", \"?\", \".\", \"'\",\n                                                                     \"--\", \"---\", \"#\"]):\n        tweets.replace(remove, \"\", inplace=True)\n    return tweets\nlem=WordNetLemmatizer()\ntkn=TweetTokenizer()\nps=LancasterStemmer()\npd.options.display.max_colwidth=1000\n\ndef filter_tweet(tweet):\n    filtered=[]\n    for w in tweet:\n        if w.lower() not in stopwords.words('english'):\n            filtered.append(w)\n    return filtered\ndef get_pos(word):\n    tag=nltk.pos_tag([word])[0][1][0]\n    if tag =='J':\n        return wordnet.ADJ\n    elif tag =='V':\n        return wordnet.VERB\n    elif tag =='N':\n        return wordnet.NOUN\n    elif tag =='R':\n        return wordnet.ADV\n    else:\n        return wordnet.NOUN","c18d40a8":"data['cleantweet']=data['text'].apply(lambda row: clean_tweet(row))\nremove_special_chars(data.cleantweet)\ndata.head()","58f93253":"data['tokenized_text'] = data.apply(lambda row : tkn.tokenize(row['cleantweet']), axis=1)\n\n\ndata['filteredsent'] = data['tokenized_text']#.apply(lambda row : filter_tweet(row))\n\n\ndata['Lemmatized']=data.apply(lambda row :[lem.lemmatize(i,pos=get_pos(i)) for i in row['filteredsent']],axis=1)\n\n\ndata['stemwords'] = data.apply(lambda row : [ps.stem(i) for i in row['filteredsent']],axis=1)\n\n\n","d9c21127":"#The final sentence is made from lemetized words.It can be changed to stemmed words.\n#Totally upto user.This sentence will be input to sklearn's feature extractor.\ndata['prtext']=data['Lemmatized'] \n\n\ndata['prtext']=data['prtext'].apply(lambda row : ' '.join(row))\n","6cec8067":"data.tail()","037436b6":"lr_cv(5, data.prtext, data.airline_sentiment, original_pipeline, 'macro')","724f3b61":"from imblearn.pipeline import make_pipeline\nfrom imblearn.over_sampling import SMOTE\nSMOTE_pipeline = make_pipeline(tvec, SMOTE(random_state=777),lr)","fdb4bc97":"lr_cv(5, data.text, data.airline_sentiment, SMOTE_pipeline, 'macro')\n","5f3228a7":"From the above plot, we see United, US airways and american have more negative reviews as compared to less.Also how can we use this information to improve our model?\nIf you have seen the tweets, you might have noticed that in each review these airlines are mentioned, which means in negative reviews, the names of these airlines are going to pop up more.So our model may bias on this basis and chances are having airline names in the text are going to effect the model result.We want our model to extract the sentiment from the text which contain some meaning , instead of airline names, so we are going to remove them in pre-processing stage.","ff006f0f":"Let us see what are the airlines for which we have the reviews.We are going to use this information ahead","842cdad2":"Next we print the few last tweets to see how preprocessing chhanged the tweets","5b0394bb":"The problem with tweets is that they are not written formally and therefore before using them in our model we need to actually do a lot of pre-processing to get a meaningful chunk of words","8492d886":"We are going to use the imblearn library to implement SMOTE","c51456d8":"According to the original research paper \u201cSMOTE: Synthetic Minority Over-sampling Technique\u201d (Chawla et al., 2002), \u201csynthetic samples are generated in the following way:\n1. Take the difference between the feature vector (sample) under consideration and its nearest neighbour.\n2. Multiply this difference by a random number between 0 and 1, and add it to the feature vector under consideration. \nThis causes the selection of a random point along the line segment between two specific features. This approach effectively forces the decision region of the minority class to become more general.","5d2652ba":"Here we are used our function to train the model the display the results, such as recall precision, accuracy and f1 score.If you are not familiar with them checkout [this article](https:\/\/www.mygreatlearning.com\/blog\/confusion-matrix-an-overview-with-python-and-r).","67c0a7cf":"# Twitter Sentiment analysis | SMOTE for oversampling","f434fb11":"To check the maximum length of our tweets, we write these lines of code.This is not important for sentimental anlysis, so you can skip this","fd9131ca":"Next we check our csv file for Null values.Here we find quite columns which have alot of null values, so it is better to drop them. Also we drop columns which have no significance to sentiment such as tweet id.So for simplicily we only keep the sentiment and tweet columns which are needed primarily for our notebook.","9c621657":"Now let us check  each airline for their positive ,negative and neutral reviews and plot a bar graph to give us some insights.","91dc57a1":"From the above results,we see that we get a better accuracy and f1 score from using SMOTE.This  means that the model is not biased towarda any particular class (neg,pos or neu).Thus we conclude that using SMOTE did actually increase the model and this is an effective way to deal with imbalanced datasets.","6637a733":"Here I am going to use a dataset that contains reviews about airline services. We are brifly going to analyse the dataset to get some insights. So let us start.","315be7a9":"I have used NLTK to preprocess the tweets.The special characters,RTs, mentions(@airline) and hashtags(#airline) are removed from the text.Then I have used various preprocessings steps such as tokenization, stemming and lemmatization to bring the all the words to their base form.If you are not familiar with them, go through [this article](https:\/\/www.mygreatlearning.com\/blog\/nltk-tutorial-with-python\/)","6f450cd8":"Now we use [logistic regression](https:\/\/www.mygreatlearning.com\/blog\/logistic-regression-with-examples-in-python-and-r) along with [tf-idf vectorizer](https:\/\/www.mygreatlearning.com\/blog\/bag-of-words\/) to extract sentiment from text.Tf-Idf is used to get features out from the text while as logistic regression is a simple classifier.\nWe are going to train the model on imbalanced dataset as well as banalnced dataset and see how it ll change the result.\nHere we are going to use k-fold to train our model and then use the macro average to get the final result","38a6e75f":"First let us import some libraries that we need beforehand.We are going to import more libraries ahead depending on our requirements.","f60e7e34":"Here I am going with the base form produced by lemmatization and ten join the tokens to form a sentence.You can use the stemmed version of words too.","499aa7c5":"First we will import the dataset which is in a csv format, so we use read_csv method of [pandas](http:\/\/https:\/\/www.mygreatlearning.com\/blog\/python-pandas-tutorial\/) to store this data in a dataframe.Next we print first 5 rows using the head() method ","21f8231e":"SMOTE:Since our dataset is not balanced,we are going to use SMOTE to oversample our data.Here I am going to briefly explain how SMOTE works.\nSMOTE(Synthetic Minority Over-Sampling Technique) is an over-sampling approach in which the minority class is over-sampled by creating \u201csynthetic\u201d examples rather than by over-sampling with replacement(dulicating the data).\n","e053f1fd":"Next, we check the number of data points in each class. Clearly this a imbalanced dataset, which is why we ll be using SMOTE later to see how that effects the results","e2dd278d":"Twitter is an American microblogging and social networking service on which users post and interact with messages known as \"tweets\". People from all over the world express their emotions via their tweets and in this notebook we will see how can we extract these emotions from the text or tweets."}}