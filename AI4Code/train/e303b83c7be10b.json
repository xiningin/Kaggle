{"cell_type":{"5067e54a":"code","51f45a27":"code","c1669cb5":"code","b6e1bec8":"code","a1bcbef0":"code","df480750":"code","71bfa5fa":"code","e2b422be":"code","22965234":"code","2e829f46":"code","5033608d":"code","6d829016":"code","ca47882e":"code","6722f655":"code","3249f7da":"code","8050999c":"code","44aa7512":"code","38d44083":"code","fc0bae86":"code","c41d397c":"code","81335827":"code","e85a4594":"code","d7354f26":"code","ec36a341":"code","28ae61bd":"code","6c672f40":"code","4eb317de":"code","ea08ddda":"code","18b433c8":"code","eccbf193":"markdown","5e98197b":"markdown","5c1b8d3d":"markdown","d838e5b7":"markdown","6f7e04b0":"markdown","2e494fa7":"markdown","c7f013b4":"markdown","400a6ed8":"markdown","ac71fa46":"markdown","a4c3d740":"markdown","f50dddf8":"markdown","a9be1528":"markdown","76a05e7b":"markdown","468295d9":"markdown","ab4103c1":"markdown","01586d9d":"markdown","62bcbb58":"markdown","ef54b07a":"markdown","e937e34a":"markdown","eab814af":"markdown"},"source":{"5067e54a":"# Install Yellowbrick Library\n!pip install --upgrade pip -q\n!pip install yellowbrick -q","51f45a27":"# Generic\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os, gc, warnings\nwarnings.filterwarnings(\"ignore\")\n\n# SKlearn\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import f1_score\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import KFold,StratifiedKFold\n\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.linear_model import LogisticRegressionCV, LogisticRegression, SGDClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import GaussianNB\n\n'''Yellowbrick Libraries'''\n\n# Model Selection\nfrom yellowbrick.classifier import ClassificationReport\nfrom yellowbrick.model_selection import ValidationCurve,LearningCurve, CVScores, FeatureImportances, RFECV\n\n# Feature Analysis Visualizers\nfrom yellowbrick.features import JointPlotVisualizer, jointplot\nfrom yellowbrick.features import rank1d, rank2d\n\n# Target Visualizers Imports\nfrom yellowbrick.target import BalancedBinningReference\nfrom yellowbrick.target import ClassBalance\nfrom yellowbrick.target import FeatureCorrelation, feature_correlation\n\n# Confusion Matrix \nfrom yellowbrick.classifier import ConfusionMatrix\n\n# ROC AUC\nfrom yellowbrick.classifier import ROCAUC\n\n# Precision - Recall Curves\nfrom yellowbrick.classifier import PrecisionRecallCurve, prcurve\n\n# Discrimination Threshold\nfrom yellowbrick.classifier import DiscriminationThreshold\n\n# Tabular Data\nfrom tabulate import tabulate\n\n# Matplotlib\nimport matplotlib.pyplot as plt","c1669cb5":"# Abalone Data Load\nurl = '..\/input\/all-datasets-for-practicing-ml\/Class\/Class_Abalone.csv'\ndf = pd.read_csv(url, header='infer')\n\n# Selecting only Male & Female\ndf = df[df['Sex'].isin(['M','F'])]\n\n# Total Records\nprint(\"Total Records: \", df.shape[0])\n\n# Records per Sex\nprint(\"Records per Sex:\\n\",df.Sex.value_counts())\n\n# Feature Extraction - Label Encoding\nencoder = LabelEncoder()\ncolumns = df.columns\ndf['Sex']= encoder.fit_transform(df['Sex']) \n\n# Feature & Target Selection\ntarget = ['Sex']   \nfeatures = columns [1:]\n\nX = df[features]\ny = df[target]\n\n# Dataset Split\n''' Training = 90% & Validation = 10%  '''\ntest_size = 0.1\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=test_size, random_state=0, shuffle=True) ","b6e1bec8":"# Define SKLearn Models\n\nmodels = [\n           DecisionTreeClassifier(random_state=42),\n           KNeighborsClassifier(),\n           LinearDiscriminantAnalysis(),\n           GaussianNB(),\n           SVC(gamma='auto',verbose=0),\n           RandomForestClassifier(n_estimators=300, verbose=0)\n         ]","a1bcbef0":"# Function to Calculate Scores\n\ndef calc_scores(X, y, estimator):\n    \n    kfold = KFold(n_splits=10, random_state=None, shuffle=False)\n    cross_val_results = cross_val_score(estimator, X, y, cv=kfold, scoring='f1', verbose=0)\n    \n    f1_mean = cross_val_results.mean()\n    model_name = estimator.__class__.__name__\n    print(model_name, \":--\", '{:.3}'.format(f1_mean))\n    \n\n# Calculate Scores\nprint(\"*** Mean F1 Scores ***\")\nfor model in models:\n    calc_scores(X_train, y_train, model)","df480750":"tab_data = []\ndef calc_scores_viz(X, y, estimator):\n    \n    kfold = KFold(n_splits=10, random_state=None, shuffle=False)\n    cross_val_results = cross_val_score(estimator, X, y, cv=kfold, scoring='f1', verbose=0)\n    \n    f1_mean = cross_val_results.mean()\n    model_name = estimator.__class__.__name__\n    #print(model_name, \":--\", '{:.3}'.format(f1_mean))\n    tab_data.append([model_name, '{:.3}'.format(f1_mean)])\n    \n    # Instantiate the classification model and visualizer\n    visualizer = ClassificationReport(estimator, classes=['Female', 'Male'],\n        cmap=\"PuBu\", size=(600, 360))\n    visualizer.fit(X, y)\n    visualizer.score(X, y)\n    visualizer.show()\n    ","71bfa5fa":"for model in models:\n    calc_scores_viz(X_train, y_train, model)\n\nprint(tabulate(tab_data, headers=['Classifiers','Mean F1 Score'], tablefmt='pretty'))","e2b422be":"# Garbage Collect\ngc.collect()","22965234":"_, axes = plt.subplots(ncols=2, figsize=(18,10))\n\nrank1d(X, ax=axes[0], show=False, orient='h', color='g')\nrank2d(X, ax=axes[1], show=False, colormap='BuPu')\nplt.show() ","2e829f46":"# Garbage Collect\ngc.collect()","5033608d":"# Length vs Whole_Weight\nvis1 = JointPlotVisualizer(columns=[\"Length\", \"Whole_Weight\"])\n\nvis1.fit_transform(X, y)        # Fit and transform the data\nvis1.show()    ","6d829016":"# Diameter vs Rings\nvis2 = JointPlotVisualizer(columns=[\"Diameter\", \"Rings\"])\n\nvis2.fit_transform(X, y)        # Fit and transform the data\nvis2.show()    ","ca47882e":"# Garbage Collect\ngc.collect()","6722f655":"# Balanced Binning Reference\nvisualizer = BalancedBinningReference()\n\nvisualizer.fit(y.T.squeeze())        # Fit the data to the visualizer\nvisualizer.show()        # Finalize and render the figure","3249f7da":"# Class Balance\nvisualizer = ClassBalance(labels=[\"Male\", \"Female\"])\n\nvisualizer.fit(y.T.squeeze())        # Fit the data to the visualizer\nvisualizer.show() ","8050999c":"'''Feature Correlation'''\n\nvisualizer = FeatureCorrelation(method='mutual_info-classification', feature_names=list(features), sort=True)\n\nvisualizer.fit(X, np.array(y.T.squeeze()))        # Fit the data to the visualizer\nvisualizer.show()              # Finalize and render the figure","44aa7512":"# Garbage Collect\ngc.collect()","38d44083":"# Classification Report - SVC\n\nmodel = SVC(gamma='auto',verbose=0)\nvisualizer = ClassificationReport(model, classes=['Male','Female'], support=True, cmap='PuBu')\n\nvisualizer.fit(X_train, y_train)        # Fit the visualizer and the model\nvisualizer.score(X_val, y_val)        # Evaluate the model on the test data\nvisualizer.show() ","fc0bae86":"# Confusion Matrix - Abalone with Model = SVC\n\nab_cm = ConfusionMatrix(model, classes=['Male','Female'],label_encoder={1: 'Male', 2: 'Female'}, cmap='PuBu')\n\nab_cm.fit(X_train, y_train)\nab_cm.score(X_val, y_val)\n\nab_cm.show()","c41d397c":"# ROC - AUC Curve for Abalone Dataset with model = GaussianNB\n\nvisualizer = ROCAUC(GaussianNB(), classes=['Male','Female'])\n\nvisualizer.fit(X_train, y_train)        # Fit the training data to the visualizer\nvisualizer.score(X_val, y_val)        # Evaluate the model on the test data\nvisualizer.show()  ","81335827":"# Precision - Recall Curves for Abalone Dataset with model = SVC\n\nviz = PrecisionRecallCurve(model)\nviz.fit(X_train, y_train)\nviz.score(X_val, y_val)\nviz.show()","e85a4594":"# Discrimination Threshold for Abalone Dataset with model = SVC\n\nvisualizer = DiscriminationThreshold(model)\n\nvisualizer.fit(X, y)        # Fit the data to the visualizer\nvisualizer.show()           # Finalize and render the figure","d7354f26":"# Garbage Collection\ngc.collect()","ec36a341":"# Validation Curve for Abalone Dataset with Model = SVC\ncv = StratifiedKFold(12)\nparam_range = np.logspace(-6, -1, 12)\n\nviz = ValidationCurve(SVC(), param_name=\"gamma\", param_range=param_range,logx=True, cv=cv, scoring=\"f1_weighted\", n_jobs=8,)\n\nviz.fit(X, y)\nviz.show()","28ae61bd":"# Learning Curve for Abalone Dataset with GaussianNB\n\ncv = StratifiedKFold(n_splits=12)\nsizes = np.linspace(0.3, 1.0, 10)\n\n# Instantiate the classification model and visualizer\nmodel = GaussianNB()\nvisualizer = LearningCurve(model, cv=cv, scoring='f1_weighted', train_sizes=sizes, n_jobs=4)\n\nvisualizer.fit(X, y)        # Fit the data to the visualizer\nvisualizer.show()  ","6c672f40":"# Cross Validation Scores for Abalone Dataset with Model = SVC\ncv = StratifiedKFold(n_splits=12, random_state=42)\n\n# Instantiate the classification model and visualizer\nmodel = SVC(gamma='auto',verbose=0)\nvisualizer = CVScores(model, cv=cv, scoring='f1_weighted', color='g')\n\nvisualizer.fit(X, y)        # Fit the data to the visualizer\nvisualizer.show()           # Finalize and render the figure","4eb317de":"# Feature Importance for Abalone Dataset with Model = RandomForest\n\nviz = FeatureImportances(RandomForestClassifier(n_estimators=300, verbose=0), labels=list(features), relative=False,colors='g')\n\n# Fit and show the feature importances\nviz.fit(X, y)\nviz.show()","ea08ddda":"# Recursive Feature Elimination for Abalone Dataset with Model = SVC\n\nvisualizer = RFECV(SVC(kernel='linear', C=1))\n\nvisualizer.fit(X, y)        # Fit the data to the visualizer\nvisualizer.show()    ","18b433c8":"# Garbage Collect\ngc.collect()","eccbf193":"# Data Prep\n\n**Note: For simplicity sake, I will convert the Abalone dataset to Binary with only Male & Female labels.**","5e98197b":"# Model Selection Visualizers\n\nYellowbrick visualizers are intended to steer the model selection process. Generally, model selection is a search problem defined as follows: given N instances described by numeric properties and (optionally) a target for estimation, find a model described by a triple composed of features, an algorithm and hyperparameters that best fits the data. For most purposes the \u201cbest\u201d triple refers to the triple that receives the best cross-validated score for the model type.\n\nThe **yellowbrick.model_selection** package provides visualizers for inspecting the performance of cross validation and hyper parameter tuning. Many visualizers wrap functionality found in sklearn.model_selection and others build upon it for performing multi-model comparisons.\n\nThe currently implemented model selection visualizers are as follows:\n\n* Validation Curve: visualizes how the adjustment of a hyperparameter influences training and test scores to tune the bias\/variance trade-off.\n\n* Learning Curve: shows how the size of training data influences the model to diagnose if a model suffers more from variance error vs. bias error.\n\n* Cross Validation Scores: displays cross-validated scores as a bar chart with average as a horizontal line.\n\n* Feature Importances: rank features by relative importance in a model\n\n* Recursive Feature Elimination: select a subset of features by importance\n\nModel selection makes heavy use of cross validation to measure the performance of an estimator. Cross validation splits a dataset into a training data set and a test data set; the model is fit on the training data and evaluated on the test data. This helps avoid a common pitfall, overfitting, where the model simply memorizes the training data and does not generalize well to new or unknown input.\n\n\n\n## Validation Curve\n\nModel validation is used to determine how effective an estimator is on data that it has been trained on as well as how generalizable it is to new input. To measure a model\u2019s performance we first split the dataset into training and test splits, fitting the model on the training data and scoring it on the reserved test data. In order to maximize the score, the hyperparameters of the model must be selected which best allow the model to operate in the specified feature space. Most models have multiple hyperparameters and the best way to choose a combination of those parameters is with a grid search. However, it is sometimes useful to plot the influence of a single hyperparameter on the training and test data to determine if the estimator is underfitting or overfitting for some hyperparameter values.","5c1b8d3d":"## ROCAUC\n\nA ROCAUC (Receiver Operating Characteristic\/Area Under the Curve) plot allows the user to visualize the tradeoff between the classifier\u2019s sensitivity and specificity. The Receiver Operating Characteristic (ROC) is a measure of a classifier\u2019s predictive quality that compares and visualizes the tradeoff between the model\u2019s sensitivity and specificity. When plotted, a ROC curve displays the true positive rate on the Y axis and the false positive rate on the X axis on both a global average and per-class basis. The ideal point is therefore the top-left corner of the plot: false positives are zero and true positives are one. This leads to another metric, area under the curve (AUC), which is a computation of the relationship between false positives and true positives. The higher the AUC, the better the model generally is. However, it is also important to inspect the \u201csteepness\u201d of the curve, as this describes the maximization of the true positive rate while minimizing the false positive rate.\n\n#### Note: We'll use the Gaussian Naive Bayes Model for this instead of SVC.","d838e5b7":"## Discrimination Threshold\n\nA visualization of precision, recall, f1 score, and queue rate with respect to the discrimination threshold of a binary classifier. The discrimination threshold is the probability or score at which the positive class is chosen over the negative class. Generally, this is set to 50% but the threshold can be adjusted to increase or decrease the sensitivity to false positives or to other application factors.\n\n#### Note: This visualizer only works for binary classification.","6f7e04b0":"## Cross Validation Scores\n\nGenerally we determine whether a given model is optimal by looking at it\u2019s F1, precision, recall, and accuracy (for classification), or it\u2019s coefficient of determination (R2) and error (for regression). However, real world data is often distributed somewhat unevenly, meaning that the fitted model is likely to perform better on some sections of the data than on others. Yellowbrick\u2019s CVScores visualizer enables us to visually explore these variations in performance using different cross validation strategies.","2e494fa7":"## Feature Correlation\n\nThis visualizer calculates Pearson correlation coefficients and mutual information between features and the dependent variable. This visualization can be used in feature selection to identify features with high correlation or large mutual information with the dependent variable.","c7f013b4":"## Confusion Matrix\n\nThe ConfusionMatrix visualizer is a ScoreVisualizer that takes a fitted scikit-learn classifier and a set of test X and y values and returns a report showing how each of the test values predicted classes compare to their actual classes. Data scientists use confusion matrices to understand which classes are most easily confused. These provide similar information as what is available in a ClassificationReport, but rather than top-level scores, they provide deeper insight into the classification of individual data points.\n\n#### Note: We'll use the same model (SVC) as we used above in the Classification Report\n","400a6ed8":"## Direct Data Visualization\n\nSometimes for feature analysis you simply need a scatter plot to determine the distribution of data. Machine learning operates on high dimensional data, so the number of dimensions has to be filtered. As a result these visualizations are typically used as the base for larger visualizers; however you can also use them to quickly plot data during ML analysis\n\nHere we'll try to visualise the relation\/distribution of \n\n* Length & Whole_Weight\n* Diameter & Rings\n","ac71fa46":"# Feature Analysis Visualizers\n\n\n## Rank Features\n\nRank1D and Rank2D evaluate single features or pairs of features using a variety of metrics that score the features on the scale [-1, 1] or [0, 1] allowing them to be ranked. The scores are visualized on a lower-left triangle heatmap so that patterns between pairs of features can be easily discerned for downstream analysis.\n\n### Rank 1D\nA one-dimensional ranking of features utilizes a ranking algorithm that takes into account only a single feature at a time (e.g. histogram analysis). By default we utilize the Shapiro-Wilk algorithm to assess the normality of the distribution of instances with respect to the feature. A barplot is then drawn showing the relative ranks of each feature.\n\n### Rank 2D\n\nA two-dimensional ranking of features utilizes a ranking algorithm that takes into account pairs of features at a time (e.g. joint plot analysis). The pairs of features are then ranked by score and visualized using the lower left triangle of a feature co-occurence matrix.\n\nBy default, the Rank2D visualizer utilizes the Pearson correlation score to detect colinear relationships.","a4c3d740":"# Yellowbrick: Machine Learning Visualization\n\nYellowbrick is a library that extends the Scikit-Learn API to make model selection and hyperparameter tuning easier. Under the hood, it\u2019s using Matplotlib. In this tutorial we'll look at the Model Selection & couple of visualisation API provided by Yellowbrick.\n\nAs always, I have kept the notebook well commented & organised for easy understanding.\n\n\n### Please do consider to UPVOTE if you find this notebook helpful.","f50dddf8":"## Feature Importances\n\nThe feature engineering process involves selecting the minimum required features to produce a valid model because the more features a model contains, the more complex it is (and the more sparse the data), therefore the more sensitive the model is to errors due to variance. A common approach to eliminating features is to describe their relative importance to a model, then eliminate weak features or combinations of features and re-evalute to see if the model fairs better during cross-validation. Many model forms describe the underlying impact of features relative to each other. In scikit-learn, Decision Tree models and ensembles of trees such as Random Forest, Gradient Boosting, and Ada Boost provide a feature_importances_ attribute when fitted. The Yellowbrick FeatureImportances visualizer utilizes this attribute to rank and plot relative importances.","a9be1528":"# Target Visualizers\n\nTarget visualizers specialize in visually describing the dependent variable for supervised modeling, often referred to as y or the target.\n\nThe following visualizations are currently implemented:\n\n* **Balanced Binning Reference**: Generate histogram with vertical lines showing the recommended value point to bin data into evenly distributed bins.\n\n* **Class Balance**: Visual inspection of the target to show the support of each class to the final estimator.\n\n* **Feature Correlation**: Plot correlation between features and dependent variables.\n\n\n## Balanced Binning Reference\n\nFrequently, machine learning problems in the real world suffer from the curse of dimensionality; you have fewer training instances than you\u2019d like and the predictive signal is distributed (often unpredictably!) across many different features. Sometimes when the your target variable is continuously-valued, there simply aren\u2019t enough instances to predict these values to the precision of regression. In this case, we can sometimes transform the regression problem into a classification problem by binning the continuous values into makeshift classes.\n\nTo help the user select the optimal number of bins, the BalancedBinningReference visualizer takes the target variable y as input and generates a histogram with vertical lines indicating the recommended value points to ensure that the data is evenly distributed into each bin.","76a05e7b":"# Model Selection\n\n**Yellowbrick** offers a visual diagnostic tools for model SKLearn model comparison inorder to select the best model for the dataset.\n\nThis tutorial notebook uses the Abalone dataset. The objective is to predict if an abalone is male, female or other based on its characteristics.\n\n\n## Modeling & Evaluation\n\n**Precision** is the number of correct positive results divided by the number of all positive results (e.g. How many of the abalone's sex we predicted would be male\/female?).\n\n**Recall** is the number of correct positive results divided by the number of positive results that should have been returned (e.g. How many of the abalone's sex that were male did we accurately predict were male?).\n\nThe **F1 score** is a measure of a test\u2019s accuracy. It considers both the precision and the recall of the test to compute the score. The F1 score can be interpreted as a weighted average of the precision and recall, where an F1 score reaches its best value at 1 and worst at 0.","468295d9":"## Learning Curve \n\nA learning curve shows the relationship of the training score versus the cross validated test score for an estimator with a varying number of training samples. This visualization is typically used to show two things:\n\n* How much the estimator benefits from more data (e.g. do we have \u201cenough data\u201d or will the estimator get better if used in an online fashion).\n\n* If the estimator is more sensitive to error due to variance vs. error due to bias.\n\n#### Note: From above cross validation, we know that the GaussianNB model has the least F1-Score. We'll use this model to check the learning curve. ","ab4103c1":"# Libraries","01586d9d":"## Recursive Feature Elimination\n\nRecursive feature elimination (RFE) is a feature selection method that fits a model and removes the weakest feature (or features) until the specified number of features is reached. Features are ranked by the model\u2019s coef_ or feature_importances_ attributes, and by recursively eliminating a small number of features per loop, RFE attempts to eliminate dependencies and collinearity that may exist in the model. RFE requires a specified number of features to keep, however it is often not known in advance how many features are valid. To find the optimal number of features cross-validation is used with RFE to score different feature subsets and select the best scoring collection of features. The RFECV visualizer plots the number of features in the model along with their cross-validated test score and variability and visualizes the selected number of features.","62bcbb58":"## Precision-Recall Curves\n\nPrecision-Recall curves are a metric used to evaluate a classifier\u2019s quality, particularly when classes are very imbalanced. The precision-recall curve shows the tradeoff between precision, a measure of result relevancy, and recall, a measure of how many relevant results are returned. A large area under the curve represents both high recall and precision, the best case scenario for a classifier, showing a model that returns accurate results for the majority of classes it selects.\n\nSince our dataset is Binary Class, we'll do these curves for Binary Classification with the same model","ef54b07a":"## Class Balance\n\nOne of the biggest challenges for classification models is an imbalance of classes in the training data. Severe class imbalances may be masked by relatively good F1 and accuracy scores \u2013 the classifier is simply guessing the majority class and not making any evaluation on the underrepresented class. There are several techniques for dealing with class imbalance such as stratified sampling, down sampling the majority class, weighting, etc. But before these actions can be taken, it is important to understand what the class balance is in the training data. The ClassBalance visualizer supports this by creating a bar chart of the support for each class, that is the frequency of the classes\u2019 representation in the dataset.\n\nFor this dataset, we'll check the class balance for \"Male\" & \"Female\"\n\n\n","e937e34a":"# Classification Visualizers \n\nClassification models attempt to predict a target in a discrete space, that is assign an instance of dependent variables one or more categories. Classification score visualizers display the differences between classes as well as a number of classifier-specific visual evaluations. We currently have implemented the following classifier evaluations:\n\n* **Classification Report**: A visual classification report that displays precision, recall, and F1 per-class as a heatmap.\n\n* **Confusion Matrix**: A heatmap view of the confusion matrix of pairs of classes in multi-class classification.\n\n* **ROCAUC**: Graphs the receiver operating characteristics and area under the curve.\n\n* **Precision-Recall Curves**: Plots the precision and recall for different probability thresholds.\n\n* **Class Balance**: Visual inspection of the target to show the support of each class to the final estimator.\n\n* **Class Prediction Error**: An alternative to the confusion matrix that shows both support and the difference between actual and predicted classes.\n\n* **Discrimination Threshold**: Shows precision, recall, f1, and queue rate over all thresholds for binary classifiers that use a discrimination probability or score.\n\n\n## Classification Report\n\nThe classification report visualizer displays the precision, recall, F1, and support scores for the model. In order to support easier interpretation and problem detection, the report integrates numerical scores with a color-coded heatmap. All heatmaps are in the range (0.0, 1.0) to facilitate easy comparison of classification models across different classification reports.","eab814af":"Based on the **F1 Score** above, we can see that SVC is performing slightly better than others. \n\nWe will now modify the above function to use **Yellowbrick\u2019s ClassificationReport** class. **ClassificationReport** is a model visualizer that displays the **precision**, **recall**, and **F1 scores**. This visual model analysis tool integrates numerical scores as well as color-coded heatmaps in order to support easy interpretation and detection.\n\n**Type I error** (or a \u201cfalse positive\u201d) is detecting an effect that is not present (e.g. determining an abalone is male when it is in fact female).\n\n**Type II error** (or a \u201cfalse negative\u201d) is failing to detect an effect that is present (e.g. believing an abalone is female when it is in fact male)."}}