{"cell_type":{"8d31a7f4":"code","2af86395":"code","e5bfa50d":"code","ab861063":"code","77179c0e":"code","8460b0a5":"code","ec328644":"code","072618fc":"code","3b9dcb89":"code","b661a823":"code","244ac00f":"code","1d89a47f":"code","1c3a7f28":"code","62057b31":"code","de4a8ff5":"code","3f7010f7":"code","9bc69d7e":"code","ce1ec363":"code","86eec5e1":"code","47221b4a":"code","e5b9ac84":"code","2b30aa34":"code","d411c28a":"code","6fb8cd40":"code","12c9c385":"code","4bf1a252":"code","833cb375":"markdown","39c4b69f":"markdown","67416788":"markdown","ee71da27":"markdown","dbcc9b09":"markdown","7077b850":"markdown","169edd43":"markdown","f031c72b":"markdown","e3e59db8":"markdown","d8ac7bdc":"markdown","206dc1cf":"markdown","19ef3405":"markdown","477afc77":"markdown","527f3a45":"markdown"},"source":{"8d31a7f4":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","2af86395":"import matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\nimport shap\nimport pandas_profiling as pp\n%matplotlib inline\n","e5bfa50d":"data = pd.read_csv('..\/input\/hr-analytics-job-change-of-data-scientists\/aug_train.csv')\ndtest = pd.read_csv('..\/input\/hr-analytics-job-change-of-data-scientists\/aug_test.csv')\ndata.head()","ab861063":"data.head()","77179c0e":"num_null = {}\npercent_null = {}\nnul_list = []\nnum = data.shape[0]\nfor col in data.columns.tolist():\n    col_null = data[col].isnull().sum()\n    if col_null > 0:\n        num_null[col] = col_null\n        percent_null[col] = round(col_null\/num,2)\n        nul_list.append(col)\nnum_ = pd.Series(num_null)\nper_ = pd.Series(percent_null)\ncon = pd.concat([num_,per_],axis=1).rename(columns={0:\"null_numm\",1:\"null_percent\"})\n\ncm = sns.light_palette(\"#348498\", as_cmap=True)\nprint(\"{} columns have missing values.\".format(data.isnull().any().sum()))\nprint(\"--------------------------------------\")\ncon.style.background_gradient(cmap=cm,subset=con.columns.tolist())","8460b0a5":"#test data  \nnu_null = {}\npercen_null = {}\nnu = dtest.shape[0]\ntest_nulist = []\nfor col in dtest.columns.tolist():\n    col_nul = dtest[col].isnull().sum()\n    if col_nul > 0:\n        nu_null[col] = col_nul\n        percen_null[col] = round(col_nul\/nu,2)\n        test_nulist.append(col)\n\nnu_ = pd.Series(nu_null)\npe_ = pd.Series(percen_null)\nco = pd.concat([nu_,pe_],axis=1).rename(columns={0:\"nul_numm\",1:\"nul_percent\"})\n\nc = sns.light_palette(\"#348498\", as_cmap=True)\nprint(\"{} columns have missing values.\".format(dtest.isnull().any().sum()))\nprint(\"--------------------------------------\")\nco.style.background_gradient(cmap=cm,subset=co.columns.tolist())","ec328644":"#data_train     null columns ->unique\nfor i in nul_list:\n    print(i+'\\t-->unique:',data[i].unique())","072618fc":"#data_test null columns ->unique\nfor i in test_nulist:\n    print(i+'\\t-->unique:',dtest[i].unique())","3b9dcb89":"cat_list = data.select_dtypes(include='object').columns.tolist()[1:]\nfig,ax = plt.subplots(3,3,figsize=(24,24))\nwith sns.axes_style({'axes.edgecolor': '#6b778d','ytick.color': \"#6b778d\",'text.color': '#6b778d','axes.labelcolor': '#6b778d','xtick.color': '#6b778d'}):\n    with sns.plotting_context({'ytick.labelsize':10,'legend.fontsize': 10,'axes.labelsize': 12}):\n        for var,ax in zip(cat_list,ax.flat):\n            sns.countplot(data=data,x=var,hue=\"target\",palette=sns.color_palette(\"pastel\"),edgecolor=sns.color_palette(\"dark\", 3),ax=ax)\n            ax.set_title(var+\"_target\",color=\"#17223b\")\n            sns.despine(bottom=True)\n","b661a823":"with sns.axes_style({'axes.edgecolor': '#6b778d','ytick.color': \"#6b778d\",'text.color': '#6b778d','axes.labelcolor': '#6b778d','xtick.color': '#6b778d'}):\n    with sns.plotting_context({'ytick.labelsize':10,'legend.fontsize': 10,'axes.labelsize': 12}):\n        plt.figure(figsize=(10,6))\n        sns.histplot(data=data,x=\"training_hours\",hue=\"target\",kde=True,color=\"#30A2DA\")\n        plt.title(\"train_hours\",color=\"#17223b\")\n        sns.despine(bottom=True)","244ac00f":"plt.figure(figsize=(10,6))\nfig = sns.countplot(data=data,x=\"target\",palette=sns.color_palette(\"pastel\"),edgecolor=sns.color_palette(\"dark\", 3))\nsns.despine(bottom=True)","1d89a47f":"# missing columns  --['gender','enrolled_university','education_level','major_discipline','experience','company_size','company_type','last_new_job']\n#drop gender(other)  fill null \n# data[\"gender\"] = data[\"gender\"].fillna(\"Male\")\n# dtest[\"gender\"] = dtest[\"gender\"].fillna(\"Male\")\n#drop enrollde_university\n# data = data.drop(data.loc[data[\"enrolled_university\"] == \"NaN\"].index)\n# dtest = dtest.drop(dtest.loc[dtest[\"enrolled_university\"] == \"NaN\"].index)\n#fill education_level\n#education_level\t-->unique: ['Graduate' 'High School' 'Masters' nan 'Phd' 'Primary School']\n# data[\"education_level\"] = data[\"education_level\"].fillna(\"Graduate\")\n# dtest[\"education_level\"] = dtest[\"education_level\"].fillna(\"Graduate\")\n#major_discipline\t-->unique: ['STEM' nan 'Other' 'Business Degree' 'Arts' 'Humanities' 'No Major']\n# data[\"major_discipline\"] = data[\"major_discipline\"].fillna(\"Other\")\n# dtest[\"major_discipline\"] = dtest[\"major_discipline\"].fillna(\"Other\")\n#experience\t-->unique: ['9' '5' '<1' '11' '>20' '10' '14' '3' '20' '8' '4' '13' '2' '6' '7' '1''19' '15' '16' nan '17' '18' '12'\n# data[\"experience\"] = data[\"experience\"].fillna(0)\n# dtest[\"experience\"] = dtest[\"experience\"].fillna(0)\n#company_size\t-->unique: ['<10' nan '10\/49' '10000+' '100-500' '50-99' '1000-4999' '500-999''5000-9999']\n#company_type\t-->unique: [nan 'Pvt Ltd' 'Funded Startup' 'Other' 'Public Sector''Early Stage Startup' 'NGO']\n#last_new_job\t-->unique: ['1' 'never' '>4' '2' '4' '3' nan]\ndata = data.drop(data.loc[data[\"gender\"] == \"Other\"].index)\ndtest = dtest.drop(dtest.loc[dtest[\"gender\"] == \"Other\"].index)\ndata = data.dropna()\ndtest = dtest.dropna()","1c3a7f28":"data = data.drop(columns=[\"city\",\"enrollee_id\"])\ndtest = dtest.drop(columns=[\"city\"])","62057b31":"def cate_one(df,*strname):\n    strname = list(strname)\n    dataset = df.copy()\n    getdum = pd.get_dummies(dataset[strname])\n    dataset = dataset.drop(strname,axis=1)\n    dataset = pd.concat([dataset,getdum],axis=1)\n    \n    return dataset\n\ndata = cate_one(data,\"gender\")\ndtest = cate_one(dtest,'gender')\n\ndata.head()","de4a8ff5":"relev = {\"No relevent experience\":0,\"Has relevent experience\":1}\nenrol = {\"no_enrollment\":0,\"Part time course\":1,\"Full time course\":2}\n\n#education_level\t-->unique: ['Graduate' 'Masters' 'High School' nan 'Phd' 'Primary School']\nedu = {\"Primary School\":0,\"High School\":1,\"Graduate\":2,\"Masters\":3,\"Phd\":4}\n\n#experience\t-->unique: ['>20' '15' '5' '<1' '11' '13' '7' '17' '2' '16' '1' '4' '10' '14' '18''19' '12' '3' '6' '9' '8' '20' ]\nexpre = {\">20\":21,\"<1\":0}\n\n#company_size\t-->unique: ['<10' nan '10\/49' '10000+' '100-500' '50-99' '1000-4999' '500-999''5000-9999']\ncomsize = {\"<10\":0,\"10\/49\":1,\"50-99\":2,\"100-500\":3,\"500-999\":4,\"1000-4999\":5,\"5000-9999\":6,\"10000+\":7}\n\n#last_new_job\t-->unique: ['1' '>4' 'never' '4' '3' '2' nan]\nlas = {\"never\":0,\"1\":1,\"2\":2,\"3\":3,\"4\":4,\">4\":5}\n\ndata[\"relevent_experience\"] = data[\"relevent_experience\"].map(relev)\ndtest[\"relevent_experience\"] = dtest[\"relevent_experience\"].map(relev)\n\ndata[\"enrolled_university\"] = data[\"enrolled_university\"].map(enrol)\ndtest[\"enrolled_university\"] = dtest[\"enrolled_university\"].map(enrol)\n\ndata[\"education_level\"] = data[\"education_level\"].map(edu)\ndtest[\"education_level\"] = dtest[\"education_level\"].map(edu)\n\ndata[\"experience\"] = data[\"experience\"].replace(expre)\ndtest[\"experience\"] = dtest[\"experience\"].replace(expre)\ndata[\"experience\"] = data[\"experience\"].map(lambda x:float(x))\ndtest[\"experience\"] = dtest[\"experience\"].map(lambda x:float(x))\n\ndata[\"company_size\"] = data[\"company_size\"].map(comsize)\ndtest[\"company_size\"] = dtest[\"company_size\"].map(comsize)\n\ndata[\"last_new_job\"] = data[\"last_new_job\"].map(las)\ndtest[\"last_new_job\"] = dtest[\"last_new_job\"].map(las)\n\n#major_discipline\t-->unique: ['STEM' nan 'Other' 'Business Degree' 'Arts' 'Humanities' 'No Major']\n#company_type\t-->unique: [nan 'Pvt Ltd' 'Funded Startup' 'Other' 'Public Sector','Early Stage Startup' 'NGO']\ndata[\"major_discipline\"] = data[\"major_discipline\"].map(lambda x:len(x))\ndata[\"company_type\"] = data[\"company_type\"].map(lambda x:len(x))\n\ndtest[\"major_discipline\"] = dtest[\"major_discipline\"].map(lambda x:len(x))\ndtest[\"company_type\"] = dtest[\"company_type\"].map(lambda x:len(x))\n\ndata.head()","3f7010f7":"plt.figure(figsize=(10,8))\nsns.heatmap(data.corr(),annot=True,color=\"b\")","9bc69d7e":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier,AdaBoostClassifier,RandomForestClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.model_selection import train_test_split,GridSearchCV\nfrom sklearn.metrics import recall_score,precision_score,ConfusionMatrixDisplay,roc_auc_score,confusion_matrix","ce1ec363":"X = data.drop(columns=[\"target\"])\nY = data[\"target\"]\nxtrain,xtest,ytrain,ytest = train_test_split(X,Y,test_size=0.25,random_state=123)","86eec5e1":"#step 1\nxtrain[\"experience\"] = (xtrain[\"experience\"] - xtrain[\"experience\"].mean())\/xtrain[\"experience\"].std()\nxtest[\"experience\"] = (xtest[\"experience\"] - xtest[\"experience\"].mean())\/xtest[\"experience\"].std()\ndtest[\"experience\"] = (dtest[\"experience\"] - dtest[\"experience\"].mean())\/dtest[\"experience\"].std()\n#step 2\nxtrain[\"training_hours\"] = (xtrain[\"training_hours\"] - xtrain[\"training_hours\"].mean())\/xtrain[\"training_hours\"].std()\nxtest[\"training_hours\"] = (xtest[\"training_hours\"] - xtest[\"training_hours\"].mean())\/xtest[\"training_hours\"].std()\ndtest[\"training_hours\"] = (dtest[\"training_hours\"] - dtest[\"training_hours\"].mean())\/dtest[\"training_hours\"].std()\n\nxtrain","47221b4a":"de = DecisionTreeClassifier(random_state=123)\nde.fit(xtrain,ytrain)\ndepre = de.predict(xtest)\ndecm = confusion_matrix(ytest,depre)\nprint('recall_score:',recall_score(ytest,depre))\nprint('precision_score:',precision_score(ytest,depre))\nprint('roc_auc_score:',roc_auc_score(ytest,depre))\ndis = ConfusionMatrixDisplay(decm)\ndis.plot()\nplt.show()\nde.feature_importances_","e5b9ac84":"rf = RandomForestClassifier(random_state=123)\nrf.fit(xtrain,ytrain)\nrfpre = rf.predict(xtest)\nrfcm = confusion_matrix(ytest,rfpre)\nprint('recall_score:',recall_score(ytest,rfpre))\nprint('precision_score:',precision_score(ytest,rfpre))\nprint('roc_auc_score:',roc_auc_score(ytest,rfpre))\nrfdis = ConfusionMatrixDisplay(rfcm)\nrfdis.plot()\nplt.show()\nrf.feature_importances_","2b30aa34":"gb = GradientBoostingClassifier(random_state=123)\ngb.fit(xtrain,ytrain)\ngbpre = gb.predict(xtest)\ngpcm = confusion_matrix(ytest,gbpre)\nprint('recall_score:',recall_score(ytest,gbpre))\nprint('precision_score:',precision_score(ytest,gbpre))\nprint('roc_auc_score:',roc_auc_score(ytest,gbpre))\ngpdis = ConfusionMatrixDisplay(gpcm)\ngpdis.plot()\nplt.show()\ngb.feature_importances_","d411c28a":"ada = AdaBoostClassifier(random_state=123)\nada.fit(xtrain,ytrain)\nadpre = ada.predict(xtest)\nadcm = confusion_matrix(ytest,adpre)\nprint('recall_score:',recall_score(ytest,adpre))\nprint('precision_score:',precision_score(ytest,adpre))\nprint('roc_auc_score:',roc_auc_score(ytest,adpre))\naddis = ConfusionMatrixDisplay(adcm)\naddis.plot()\nplt.show()\nada.feature_importances_","6fb8cd40":"ml = MLPClassifier(hidden_layer_sizes=(100,50,20),random_state=123)\nml.fit(xtrain,ytrain)\nmlpre = ada.predict(xtest)\nmlcm = confusion_matrix(ytest,mlpre)\nprint('recall_score:',recall_score(ytest,mlpre))\nprint('precision_score:',precision_score(ytest,mlpre))\nprint('roc_auc_score:',roc_auc_score(ytest,mlpre))\nmldis = ConfusionMatrixDisplay(mlcm)\nmldis.plot()\nplt.show()","12c9c385":"xg = XGBClassifier(random_state=123)\nxg.fit(xtrain,ytrain)\nxgpre = ada.predict(xtest)\nxgcm = confusion_matrix(ytest,xgpre)\nprint('recall_score:',recall_score(ytest,xgpre))\nprint('precision_score:',precision_score(ytest,xgpre))\nprint('roc_auc_score:',roc_auc_score(ytest,xgpre))\nxgdis = ConfusionMatrixDisplay(xgcm)\nxgdis.plot()\nplt.show()","4bf1a252":"testpre = xg.predict(dtest.drop(columns=[\"enrollee_id\"]))\nsub = pd.DataFrame()\nsub[\"enrollee_id\"] = dtest[\"enrollee_id\"]\nsub[\"target\"] = testpre\nsub.head()\n","833cb375":"#### 1\u3001DecisionTreeClassifier","39c4b69f":"#### 1. According to the first picture, we can see that the proportion of men who are not looking for a job change far exceeds the proportion of men who are looking for a job change.\n#### 2. In the second picture, we can see that most candidates with relevant experience do not look for job changes in a large proportion.\n#### 3. in the types of registered courses, most people are not registered for courses and are not willing to look for job changes.\n#### 4. Most of these groups have a high degree of education.\n#### 5. The candidate's major is basically STEM. This shows that many people are not changing industries.\n#### 6. In the group that does not change their jobs, many people have more than 20 years of work experience. This can actually explain in disguise that the longer you work, the more you hope you can stabilize. In contrast, those with less work experience will have a significantly higher rate of changing jobs.\n#### 7. Among the groups that do not plan to change jobs, the number of their employer companies is basically between 50-500.\n#### 8. Among the groups that do not change their jobs, the type of employer is basically pv ltd\n#### 9. The proportion of unchanged jobs exceeds the proportion of changed jobs, and employees who have just joined the company for about a year are less willing to change jobs.","67416788":"####   1. The company type has the largest missing value, with a missing percentage of 32%.\n####   2. The company size has the largest missing value, with a missing ratio of 31%.","ee71da27":"# <br>Data preprocessing\n 1. Fill or delete rows with missing values\n 2. Categorical feature and numerical feature processing\n 3. Oversample the data\n 4. Considering the problem of data leakage, the data set should be divided first, and then the numerical features should be normalized. ->I don't know if this is correct\u3002","dbcc9b09":"### 1. Fill or delete rows with missing values -- train and test data","7077b850":"#### Obviously, the training time variable is not normally distributed. It has a right skew. ","169edd43":"#### Finally, let us look at the data situation of the target variable","f031c72b":"#### 2.RandomForestClassifier","e3e59db8":"#### We can see that the number of people who do not plan to change jobs is the largest, and the data is indeed unbalanced.","d8ac7bdc":"#### 3.GradientBoostingClassifier","206dc1cf":"#### 5.MLClassifier","19ef3405":"### 2.Categorical feature and numerical feature processing","477afc77":"#### 6.XGBClassifier","527f3a45":"#### 4.AdaBoostClassifier"}}