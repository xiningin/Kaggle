{"cell_type":{"9f384fd8":"code","68b5b19d":"code","76da630c":"code","ef546828":"code","d5ae89b2":"code","49223a75":"code","59705210":"code","3f95a584":"code","38bfb460":"markdown","bf016150":"markdown","80640aa8":"markdown","7558b548":"markdown","d373e275":"markdown","8e94112d":"markdown","404510bb":"markdown","572d2a68":"markdown","834441e4":"markdown"},"source":{"9f384fd8":"import pandas\nimport numpy\n\ntrain = pandas.read_csv(\"..\/input\/ventilator-pressure-prediction\/train.csv\")\ntest = pandas.read_csv(\"..\/input\/ventilator-pressure-prediction\/test.csv\")\nsubmission = pandas.read_csv(\"..\/input\/ventilator-pressure-prediction\/sample_submission.csv\")\ntrain","68b5b19d":"cont_features = [\"time_step\", \"u_in\"]\ncat_features = [\"R\", \"C\"]\n\ntarget = train[\"pressure\"]","76da630c":"# Cast categorical columns to integer types\ntrain[\"R\"] = train[\"R\"].astype(numpy.int8)\ntrain[\"C\"] = train[\"C\"].astype(numpy.int8)\ntrain[\"u_out\"] = train[\"u_out\"].astype(numpy.int8)\n\ntest[\"R\"] = test[\"R\"].astype(numpy.int8)\ntest[\"C\"] = test[\"C\"].astype(numpy.int8)\ntest[\"u_out\"] = test[\"u_out\"].astype(numpy.int8)\n\n# Look two steps into the past for u_in (u_out doesn't matter)\ntrain[\"u_in_last_n1\"] = train.groupby(\"breath_id\")[\"u_in\"].shift(1)\ntrain[\"u_in_last_n2\"] = train.groupby(\"breath_id\")[\"u_in\"].shift(2)\ntest[\"u_in_last_n1\"] = test.groupby(\"breath_id\")[\"u_in\"].shift(1)\ntest[\"u_in_last_n2\"] = test.groupby(\"breath_id\")[\"u_in\"].shift(2)\n\n# Look two steps into the future\ntrain[\"u_in_future_n1\"] = train.groupby(\"breath_id\")[\"u_in\"].shift(-1)\ntrain[\"u_in_future_n2\"] = train.groupby(\"breath_id\")[\"u_in\"].shift(-2)\ntrain[\"u_out_future_n1\"] = train.groupby(\"breath_id\")[\"u_out\"].shift(-1)\ntrain[\"u_out_future_n2\"] = train.groupby(\"breath_id\")[\"u_out\"].shift(-2)\ntest[\"u_in_future_n1\"] = test.groupby(\"breath_id\")[\"u_in\"].shift(-1)\ntest[\"u_in_future_n2\"] = test.groupby(\"breath_id\")[\"u_in\"].shift(-2)\ntest[\"u_out_future_n1\"] = test.groupby(\"breath_id\")[\"u_out\"].shift(-1)\ntest[\"u_out_future_n2\"] = test.groupby(\"breath_id\")[\"u_out\"].shift(-2)\n\n# Fill missing values with zeros\ntrain.fillna(0, inplace=True)\ntest.fillna(0, inplace=True)\n\n# u_out values should just be integers for categoricals\ntrain[\"u_out_future_n1\"] = train[\"u_out_future_n1\"].astype(numpy.int8)\ntrain[\"u_out_future_n2\"] = train[\"u_out_future_n2\"].astype(numpy.int8)\ntest[\"u_out_future_n1\"] = test[\"u_out_future_n1\"].astype(numpy.int8)\ntest[\"u_out_future_n2\"] = test[\"u_out_future_n2\"].astype(numpy.int8)\n\n# Grab the max and min values of u_in\ntrain[\"u_in_max\"] = train.groupby(\"breath_id\")[\"u_in\"].max()\ntest[\"u_in_max\"] = test.groupby(\"breath_id\")[\"u_in\"].max()\n\ntrain[\"u_in_min\"] = train.groupby(\"breath_id\")[\"u_in\"].min()\ntest[\"u_in_min\"] = test.groupby(\"breath_id\")[\"u_in\"].min()\n\n# Differences between current, max, min, and mean values for u_in\ntrain[\"u_in_last_n1_diff\"] = train[\"u_in\"] - train[\"u_in_last_n1\"]\ntrain[\"u_in_last_n2_diff\"] = train[\"u_in\"] - train[\"u_in_last_n2\"]\ntrain[\"u_in_future_n1_diff\"] = train[\"u_in\"] - train[\"u_in_future_n1\"]\ntrain[\"u_in_future_n2_diff\"] = train[\"u_in\"] - train[\"u_in_future_n2\"]\n\ntrain[\"u_in_max_diff\"] = train.groupby(\"breath_id\")[\"u_in\"].max() - train[\"u_in\"]\ntrain[\"u_in_min_diff\"] = train.groupby(\"breath_id\")[\"u_in\"].min() - train[\"u_in\"]\ntrain[\"u_in_mean_diff\"] = train.groupby(\"breath_id\")[\"u_in\"].mean() - train[\"u_in\"]\n\ntest[\"u_in_max_diff\"] = test.groupby(\"breath_id\")[\"u_in\"].max() - test[\"u_in\"]\ntest[\"u_in_min_diff\"] = test.groupby(\"breath_id\")[\"u_in\"].min() - test[\"u_in\"]\ntest[\"u_in_mean_diff\"] = test.groupby(\"breath_id\")[\"u_in\"].mean() - test[\"u_in\"]\n\ntest[\"u_in_last_n1_diff\"] = test[\"u_in\"] - test[\"u_in_last_n1\"]\ntest[\"u_in_last_n2_diff\"] = test[\"u_in\"] - test[\"u_in_last_n2\"]\ntest[\"u_in_future_n1_diff\"] = test[\"u_in\"] - test[\"u_in_future_n1\"]\ntest[\"u_in_future_n2_diff\"] = test[\"u_in\"] - test[\"u_in_future_n2\"]\n\n# Grab cumulative sum of u_in\ntrain[\"u_in_cumulative\"] = train.groupby(\"breath_id\")[\"u_in\"].cumsum()\ntest[\"u_in_cumulative\"] = test.groupby(\"breath_id\")[\"u_in\"].cumsum()\n\n# Make a note of the previous time for each row\ntrain[\"last_time_step\"] = train.groupby(\"breath_id\")[\"time_step\"].shift(1)\ntest[\"last_time_step\"] = test.groupby(\"breath_id\")[\"time_step\"].shift(1)\n\n# Calculate the size of the step for each row from the previous observation\ntrain[\"step_size\"] = train[\"time_step\"] - train[\"last_time_step\"]\ntest[\"step_size\"] = test[\"time_step\"] - test[\"last_time_step\"]\n\n# Add new features to the appropriate feature lists\ncont_features.append(\"u_in_last_n1\")\ncont_features.append(\"u_in_last_n2\")\ncont_features.append(\"u_in_future_n1\")\ncont_features.append(\"u_in_future_n2\")\ncont_features.append(\"step_size\")\ncont_features.append(\"u_in_max_diff\")\ncont_features.append(\"u_in_min_diff\")\ncont_features.append(\"u_in_mean_diff\")\ncont_features.append(\"u_in_cumulative\")\ncat_features.append(\"u_out_future_n1\")\ncat_features.append(\"u_out_future_n2\")\ncont_features.append(\"u_in_last_n1_diff\")\ncont_features.append(\"u_in_last_n2_diff\")\ncont_features.append(\"u_in_future_n1_diff\")\ncont_features.append(\"u_in_future_n2_diff\")","ef546828":"from category_encoders import LeaveOneOutEncoder\nfrom sklearn.preprocessing import LabelEncoder\n\nxgb_cat_features = []\nlgb_cat_features = []\ncb_cat_features = []\nhgbc_cat_features = []\n\nloo_features = []\nle_features = []\n\ndef label_encode(train_df, test_df, column):\n    le = LabelEncoder()\n    new_feature = \"{}_le\".format(column)\n    le.fit(train_df[column].unique().tolist() + test_df[column].unique().tolist())\n    train_df[new_feature] = le.transform(train_df[column])\n    test_df[new_feature] = le.transform(test_df[column])\n    return new_feature\n\ndef loo_encode(train_df, test_df, column):\n    loo = LeaveOneOutEncoder()\n    new_feature = \"{}_loo\".format(column)\n    loo.fit(train_df[column], train_df[\"pressure\"])\n    train_df[new_feature] = loo.transform(train_df[column])\n    test_df[new_feature] = loo.transform(test_df[column])\n    return new_feature\n\nfor feature in cat_features:\n    loo_features.append(loo_encode(train, test, feature))\n    le_features.append(label_encode(train, test, feature))\n    \nxgb_cat_features.extend(loo_features)\nlgb_cat_features.extend(le_features)\ncb_cat_features.extend(cat_features)\nhgbc_cat_features.extend(loo_features)","d5ae89b2":"new_train = train[(train[\"u_out\"] == 0)]\ntarget = new_train[\"pressure\"]","49223a75":"import gc\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nfrom lightgbm import LGBMRegressor\nfrom xgboost import XGBRegressor\nfrom catboost import CatBoostRegressor\nfrom sklearn.linear_model import Ridge\n\nfrom sklearn.experimental import enable_hist_gradient_boosting\nfrom sklearn.ensemble import HistGradientBoostingRegressor\n\nfrom sklearn.model_selection import GroupKFold, KFold\nfrom sklearn.metrics import mean_absolute_error\n\nrandom_state = 2021\nn_folds = 5\nk_fold = GroupKFold(n_splits=n_folds)\n\nxgb_train_preds = numpy.zeros(len(new_train.index), )\nxgb_test_preds = numpy.zeros(len(test.index), )\n\nlgb_train_preds = numpy.zeros(len(new_train.index), )\nlgb_test_preds = numpy.zeros(len(test.index), )\n\nhgbc_train_preds = numpy.zeros(len(new_train.index), )\nhgbc_test_preds = numpy.zeros(len(test.index), )\n\nfor fold, (train_index, test_index) in enumerate(k_fold.split(new_train, target, new_train[\"breath_id\"])):\n    print(\"--> Fold {}\".format(fold + 1))\n    y_train = target.iloc[train_index]\n    y_valid = target.iloc[test_index]\n\n    xgb_features = xgb_cat_features + cont_features\n    xgb_x_train = pandas.DataFrame(new_train[xgb_features].iloc[train_index])\n    xgb_x_valid = pandas.DataFrame(new_train[xgb_features].iloc[test_index])\n\n    xgb_model = XGBRegressor(\n        seed=random_state,\n        n_estimators=10000,\n        verbosity=1,\n        eval_metric=\"mae\",\n        tree_method=\"gpu_hist\",\n        gpu_id=0,\n        alpha=0.17318869995918917,\n        colsample_bytree=0.867867790089098,\n        gamma=0.38457003274587587,\n        reg_lambda=9.642931533488165,\n        learning_rate=0.09834191296472465,\n        max_bin=696,\n        max_depth=13,\n        min_child_weight=1.8360422783281707,\n        subsample=0.7191624126962364,\n    )\n    xgb_model.fit(\n        xgb_x_train,\n        y_train,\n        eval_set=[(xgb_x_valid, y_valid)], \n        verbose=0,\n        early_stopping_rounds=200,\n    )\n\n    train_oof_preds = xgb_model.predict(xgb_x_valid)\n    test_oof_preds = xgb_model.predict(test[xgb_features])\n    xgb_train_preds[test_index] = train_oof_preds\n    xgb_test_preds += test_oof_preds \/ n_folds\n    print(\": XGB - MAE Score = {}\".format(mean_absolute_error(y_valid, train_oof_preds)))\n   \n    del(xgb_x_train)\n    del(xgb_x_valid)\n    del(xgb_model)\n    _ = gc.collect()\n    \n    \n    lgb_features = lgb_cat_features + cont_features\n    lgb_x_train = pandas.DataFrame(new_train[lgb_features].iloc[train_index])\n    lgb_x_valid = pandas.DataFrame(new_train[lgb_features].iloc[test_index])\n\n    lgb_model = LGBMRegressor(\n        cat_feature=[x for x in range(len(lgb_cat_features))],\n        random_state=random_state,\n        early_stopping_round=200,\n        metric=\"mae\",\n        n_estimators=100000,\n        n_jobs=-1,\n        verbose=-1,\n        cat_l2=39.95489636774699,\n        cat_smooth=35.550900375908554,\n        colsample_bytree=0.8046962761788545,\n        learning_rate=0.6023144602962871,\n        max_bin=395,\n        max_depth=16,\n        min_child_samples=359,\n        min_data_per_group=209,\n        num_leaves=163,\n        reg_alpha=5.832855074792787,\n        reg_lambda=6.454871156617244,\n        subsample=0.7264718486219024,\n        subsample_freq=1,\n    )\n    lgb_model.fit(\n        lgb_x_train,\n        y_train,\n        eval_set=[(lgb_x_valid, y_valid)], \n        verbose=0,\n    )\n\n    train_oof_preds = lgb_model.predict(lgb_x_valid)\n    test_oof_preds = lgb_model.predict(test[lgb_features])\n    lgb_train_preds[test_index] = train_oof_preds\n    lgb_test_preds += test_oof_preds \/ n_folds\n    print(\": LGB - MAE Score = {}\".format(mean_absolute_error(y_valid, train_oof_preds)))\n\n    del(lgb_x_train)\n    del(lgb_x_valid)\n    del(lgb_model)\n    _ = gc.collect()\n\n    \n    hgbc_features = hgbc_cat_features + cont_features\n    hgbc_x_train = pandas.DataFrame(new_train[hgbc_features].iloc[train_index])\n    hgbc_x_valid = pandas.DataFrame(new_train[hgbc_features].iloc[test_index])\n\n    hgbc_model = HistGradientBoostingRegressor(\n        random_state=2021,\n        l2_regularization=0.226902431039134,\n        learning_rate=0.4389644036579206,\n        max_bins=127,\n        max_depth=63,\n        max_leaf_nodes=364,\n    )\n    hgbc_model.fit(\n        hgbc_x_train,\n        y_train,\n    )\n\n    train_oof_preds = hgbc_model.predict(hgbc_x_valid)\n    test_oof_preds = hgbc_model.predict(test[hgbc_features])\n    hgbc_train_preds[test_index] = train_oof_preds\n    hgbc_test_preds += test_oof_preds \/ n_folds\n    print(\": HGBC - MAE Score = {}\".format(mean_absolute_error(y_valid, train_oof_preds)))\n\n    del(hgbc_x_train)\n    del(hgbc_x_valid)\n    del(hgbc_model)\n    _ = gc.collect()\n\n    print(\"\")\n    \nprint(\"--> Overall metrics\")\nprint(\": XGB - MAE Score = {}\".format(mean_absolute_error(target, xgb_train_preds)))\nprint(\": LGB - MAE Score = {}\".format(mean_absolute_error(target, lgb_train_preds)))\nprint(\": HGBC - MAE Score = {}\".format(mean_absolute_error(target, hgbc_train_preds)))","59705210":"from scipy.special import expit\n\nrandom_state = 2021\nn_folds = 5\nk_fold = KFold(n_splits=n_folds, random_state=random_state, shuffle=True)\n\nl1_train = pandas.DataFrame(data={\n    \"xgb\": xgb_train_preds.tolist(),\n    \"lgb\": lgb_train_preds.tolist(),\n    \"hgbc\": hgbc_train_preds.tolist(),\n})\nl1_test = pandas.DataFrame(data={\n    \"xgb\": xgb_test_preds.tolist(),\n    \"lgb\": lgb_test_preds.tolist(),\n    \"hgbc\": hgbc_test_preds.tolist(),\n})\n\ntrain_preds = numpy.zeros(len(l1_train.index), )\ntest_preds = numpy.zeros(len(l1_test.index), )\nfeatures = [\"xgb\", \"lgb\", \"hgbc\"]\n\nfor fold, (train_index, test_index) in enumerate(k_fold.split(l1_train, target)):\n    print(\"--> Fold {}\".format(fold + 1))\n    y_train = target.iloc[train_index]\n    y_valid = target.iloc[test_index]\n\n    x_train = pandas.DataFrame(l1_train[features].iloc[train_index])\n    x_valid = pandas.DataFrame(l1_train[features].iloc[test_index])\n    \n    model = Ridge(random_state=random_state)\n    model.fit(\n        x_train,\n        y_train,\n    )\n\n    train_oof_preds = model.predict(x_valid)\n    test_oof_preds = model.predict(l1_test[features])\n    train_preds[test_index] = train_oof_preds\n    test_preds += test_oof_preds \/ n_folds\n    print(\": MAE Score = {}\".format(mean_absolute_error(y_valid, train_oof_preds)))\n    print(\"\")\n    \nprint(\"--> Overall metrics\")\nprint(\": MAE Score = {}\".format(mean_absolute_error(target, train_preds)))","3f95a584":"submission[\"pressure\"] = test_preds.tolist()\nsubmission.to_csv(\"submission.csv\", index=False)","38bfb460":"Encode features depending on model type. Some of the categorical features must be encoded differently depending on the model.","bf016150":"# Save Predictions","80640aa8":"# Load Data","7558b548":"# Introduction\n\nThis model demonstrates simple kernel stacking using XGBoost, LightGBM, Hist Gradient Boosting Regression, and Ridge Regression. It uses 5-fold cross validation to build each model, and makes both test and training predictions out-of-fold. Those results are then fed into the level 2 Ridge model, where 5-fold cross validation is used again to make out-of-fold predictions for the submission result. Basic feature engineering is employed. If you like the model, please consider upvoting!\n\n# Credits\n\nThe following discussions and other kernels have all contributed to the additional feature engineering seen within this kernel. If you find this kernel useful, please visit these discussions and upvote information contained therein.\n\n* Discussion - [A simple feature that improved my score](https:\/\/www.kaggle.com\/c\/ventilator-pressure-prediction\/discussion\/273974) by [\nCarl McBride Ellis](https:\/\/www.kaggle.com\/carlmcbrideellis)\n* Kernel - [LGBM LOVER'S](https:\/\/www.kaggle.com\/shivansh002\/lgbm-lover-s) by [OnePunchMan](https:\/\/www.kaggle.com\/shivansh002\/lgbm-lover-s)","d373e275":"# Build Level 2 Model","8e94112d":"# Generate Level 1 Models","404510bb":"Finally, we'll define the dataset to train on. As specified by the competition evaluation page:\n\n* The expiratory phase is not scored\n\nThis means we don't need to bother learning about pressures when `u_out` is `1`. ","572d2a68":"Given that this is time series information, we can use the previous 2 steps worth of information for our machine learning model. Although not strictly a good idea, we can also give it 2 steps of information about the future, since we have the entire breathing sequence available to us.  In addition, we'll calculate the step size instead of using the raw `time_step` value. Note that we do all of this based on `breath_id`.","834441e4":"# Define Features"}}