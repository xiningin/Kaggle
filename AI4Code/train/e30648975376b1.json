{"cell_type":{"4c7d6ae3":"code","4bc69bc2":"code","1ba6efe0":"code","d3558701":"code","cfcd7b72":"code","77382d44":"code","b56ce897":"code","a1242fa5":"code","fcc95dbf":"code","8a0255e2":"code","9301d209":"code","fb083594":"code","32d07f68":"code","ea3a8ef5":"code","6bf1c76b":"code","0714ebed":"code","ded0e4ef":"code","70c837c9":"code","a43762d0":"code","379b33e9":"code","45342fe7":"code","d6fca622":"markdown","c9366b6d":"markdown","6387516d":"markdown","0aa1da5d":"markdown","b24c4fb0":"markdown","2402e466":"markdown","97e0f81e":"markdown","bfbab0c4":"markdown","08e1bd93":"markdown","1210979b":"markdown","1d99f90a":"markdown","12e9925f":"markdown"},"source":{"4c7d6ae3":"import nltk\nimport pandas as pd\nimport numpy\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom nltk.tokenize import RegexpTokenizer\nfrom  collections import Counter\nimport re\nimport gc","4bc69bc2":"file_0=open(\"..\/input\/xaa\",mode='r',encoding='latin-1')","1ba6efe0":"file_content=file_0.read()\nfile_0.close()","d3558701":"print(\"file size is\",len(file_content)\/(1024*1024))\nprint(\"no of line in file is\",len(file_content.split('\\n')))\nprint(\"no of word in file is\",len(file_content.replace('\\n',' ').split(' ')))","cfcd7b72":"temp = re.sub(r'[^a-zA-Z0-9\\-\\s]*', r'', file_content)\ntemp = re.sub(r'(\\-|\\s+)', ' ', temp)\ndel(file_content)\ngc.collect()","77382d44":"token_nltk=nltk.word_tokenize(temp)\ndel(temp)\ngc.collect()","b56ce897":"tri_gram=nltk.ngrams(token_nltk,3)","a1242fa5":"gm=Counter(tri_gram)\ntoken=gm.keys()\nfrequency=gm.values()\ndel(gm)\ngc.collect()","fcc95dbf":"length_trigram=sum(frequency)\nlength_dist_trigram=len(token)","8a0255e2":"print(\"total number of  tri-gram \",length_trigram)\nprint(\"total number of distinct tri-gram \",length_dist_trigram)","9301d209":"tri_gram=nltk.ngrams(token_nltk,3)\nfdist_trigram = nltk.FreqDist(tri_gram)","fb083594":"fdist_trigram.plot(10)","32d07f68":"l=fdist_trigram.most_common(10)\nx=[]\ny=[]\nfor v1,v2 in l:\n    x.append(str(v1))\n    y.append(v2)\n    \n# x-coordinates of left sides of bars \nleft =[1, 2, 3, 4, 5,6,7,8,9,10]\n\n# heights of bars \nheight = y \n\n# labels for bars \ntick_label =x\n\n# plotting a bar chart \nplt.bar(left, height, tick_label = tick_label, color = ['red', 'green']) \nplt.xticks(left,tick_label,rotation=90)\n# naming the x-axis \nplt.xlabel('tri-gram') \n# naming the y-axis \nplt.ylabel('frequency') \n# plot title \nplt.title('tri-gram frequency distribution') \n\n# function to show the plot \nplt.show() \n","ea3a8ef5":"l_100=fdist_trigram.most_common(1000)\nx_new=[]\ny_new=[]\nc=len(token_nltk)\ni=1\nfor v1,v2 in l_100:\n    x_new.append(i)\n    y_new.append(v2*i\/c)\n    i=i+1\n    \nplt.plot(x_new,y_new)\n\nplt.xlabel('rank') \n# naming the y-axis \nplt.ylabel('value') \n# plot title \nplt.title('tri-gram rank vs rank*frequency') \n\n# function to show the plot \nplt.show() ","6bf1c76b":"i=1\ns=0\ngram_70=int(length_trigram*0.7)+1\nprint(\"70% of the selected corpus is \",gram_70)\nval=int(length_dist_trigram*.7)\nfor v1,v2 in fdist_trigram.most_common(val):\n    s=s+v2\n    if s>gram_70:\n        break\n    i=i+1 \n    \nprint(\"no of token require to cover 70% of corpus is \",i)\nprint(\"and total tri-gram of {} token is {}\".format(i,s))\nprint(\"{} %  tokens cover the 70% corpus\".format(i\/length_dist_trigram*100))","0714ebed":"df=pd.DataFrame(columns=[\"tri-Gram\",\"frequency\"])","ded0e4ef":"del(l)\ndel(l_100)\ndel(tri_gram)\ndel(fdist_trigram)\ngc.collect()","70c837c9":"df['tri-Gram']=token","a43762d0":"\ndf['frequency']=frequency","379b33e9":"df.head()","45342fe7":"df.to_csv(\"tri-gram.csv\")","d6fca622":"tokenization of the corpus using nltk tools","c9366b6d":"frequency distribution of bi-gram","6387516d":"removing punctuation and other symbol","0aa1da5d":"checking zipfs law for corpous using plot","b24c4fb0":"plotting histogram for most comman top 10 token with highest frequency","2402e466":"creating tri-grams sequence","97e0f81e":"number of bi-grams  required for 70% coverage of the corpus","bfbab0c4":"Reading the file ","08e1bd93":"**read the content of file in a string file_content**","1210979b":"file description","1d99f90a":"plotting top 10 token which have highest frequency","12e9925f":"\nafter tokenization corpus and token size"}}