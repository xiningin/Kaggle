{"cell_type":{"e22ee148":"code","1a949fc6":"code","aa38cd62":"code","b18db7f7":"code","c8773484":"code","02a762db":"code","fa5febf5":"code","9ace64fe":"code","482d63fc":"code","bd36b2ee":"code","c9d899c2":"code","7c65ca50":"code","b39fae08":"code","1a970e9d":"code","d5cc07be":"code","b7ee7ae3":"code","d2089990":"code","c9f0f82e":"markdown","e98e98e9":"markdown","5cd43310":"markdown","78410f6d":"markdown","1bbd2b7c":"markdown","140e3756":"markdown"},"source":{"e22ee148":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","1a949fc6":"customers = pd.read_csv(\"..\/input\/default-of-credit-card-clients-dataset\/UCI_Credit_Card.csv\")\ncustomers.head()","aa38cd62":"import missingno as msno \nmsno.matrix(customers)","b18db7f7":"#check for null values\nprint('Data columns with null values:',customers.isnull().sum(), sep = '\\n')","c8773484":"feature_cols = ['LIMIT_BAL', 'PAY_0', 'PAY_2', 'PAY_3', 'PAY_4', 'PAY_5', 'PAY_6', 'BILL_AMT1', 'BILL_AMT2', 'BILL_AMT3', 'BILL_AMT4', 'BILL_AMT5', 'BILL_AMT6', 'PAY_AMT1', 'PAY_AMT2', 'PAY_AMT3', 'PAY_AMT4', 'PAY_AMT5', 'PAY_AMT6', 'default.payment.next.month']\nfeature_cols","02a762db":"#make Dataframe\ncusData = pd.DataFrame(customers,columns = feature_cols)\ncusData","fa5febf5":"#Renaming last column\ncusData.iloc[:,-1:]\ncusData.columns = [*cusData.columns[:-1], 'Regular']","9ace64fe":"#settting \nX=cusData.drop(['Regular'],axis=1)\ny=cusData.Regular\n\n#print(X)\n#print(y)","482d63fc":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y,train_size=0.8,random_state=1)\nprint(\"Input Training:\",X_train.shape)\nprint(\"Input Test:\",X_test.shape)\nprint(\"Output Training:\",y_train.shape)\nprint(\"Output Test:\",y_test.shape)","bd36b2ee":"from sklearn.linear_model import LogisticRegression\nfrom sklearn import metrics\nlogreg = LogisticRegression()\nlogreg.fit(X_train, y_train)\ny_pred = logreg.predict(X_test)\nprint('Accuracy of logistic regression classifier on test set: {:.2f}'.format(logreg.score(X_test, y_test)))","c9d899c2":"from sklearn.tree import DecisionTreeClassifier\ndt = DecisionTreeClassifier(criterion=\"gini\", max_depth=3)\ndt = dt.fit(X_train,y_train)\ny_pred = dt.predict(X_test)\nprint(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))","7c65ca50":"from sklearn.ensemble import RandomForestClassifier\nrt=RandomForestClassifier(n_estimators=100)\nrt.fit(X_train,y_train)\ny_pred=rt.predict(X_test)\nfrom sklearn import metrics\nprint(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))","b39fae08":"from sklearn.neighbors import KNeighborsClassifier\nerror_rate = []\n\nfor i in range(1,40):\n    \n    knn = KNeighborsClassifier(n_neighbors=i)\n    knn.fit(X_train,y_train)\n    pred_i = knn.predict(X_test)\n    error_rate.append(np.mean(pred_i != y_test))","1a970e9d":"import matplotlib.pyplot as plt\nplt.figure(figsize=(10,10))\nplt.plot(range(1,40),error_rate,color='blue', linestyle='dashed', marker='o',\n         markerfacecolor='red', markersize=10)\nplt.title('Error Rate vs. K Value')\nplt.xlabel('K')\nplt.ylabel('Error Rate')","d5cc07be":"from sklearn.metrics import confusion_matrix\nknn = KNeighborsClassifier(n_neighbors= 10, metric='euclidean')\nknn.fit(X_train, y_train)\ny_pred = knn.predict(X_test)\nconfusion_matrix = confusion_matrix(y_test, y_pred)\nprint(\"Confusion Matrix:\\n\",confusion_matrix)\nprint(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))","b7ee7ae3":"from sklearn.naive_bayes import BernoulliNB \nfrom sklearn.model_selection import cross_val_score\ngnb = BernoulliNB() \ngnb.fit(X_train, y_train) \n  \n#Applying and predicting \ny_pred = gnb.predict(X_test) \ncv_scores = cross_val_score(gnb, X, y, \n                            cv=10,\n                            scoring='precision')\nprint(\"Cross-validation precision: %f\" % cv_scores.mean())","d2089990":"from sklearn.ensemble import GradientBoostingClassifier\n\nlr_list = [0.05, 0.075, 0.1, 0.25, 0.5, 0.75, 1]\n\nfor learning_rate in lr_list:\n    gb_clf = GradientBoostingClassifier(n_estimators=20, learning_rate=learning_rate, max_features=2, max_depth=2, random_state=0)\n    gb_clf.fit(X_train, y_train)\n    \n    print(\"Learning rate: \", learning_rate)\n    print(\"Accuracy score (training): {0:.3f}\".format(gb_clf.score(X_train, y_train)))\n    print(\"Accuracy score (validation): {0:.3f}\".format(gb_clf.score(X_test, y_test)))","c9f0f82e":"# **Naive Bayes Classifier with Cross Validation**","e98e98e9":"# **Random Forest Classifier**","5cd43310":"# **K Nearest Neighbours**","78410f6d":"# **Decision Tree**","1bbd2b7c":"# **Logistic Regression**","140e3756":"# Graident Bossting Algorithm"}}