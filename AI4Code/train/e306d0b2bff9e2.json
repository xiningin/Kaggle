{"cell_type":{"02fcbb5e":"code","32b5ce46":"code","bf9b0543":"code","a7091f39":"code","0e6795fe":"code","972aadaa":"code","187eca46":"code","c2fc4fd8":"code","aa4b08b3":"code","88c6b93f":"code","60612e65":"code","2434acab":"code","2b3f0072":"code","115858ae":"code","9fd10e30":"code","793dd201":"code","c201ebfd":"code","8c3c9785":"code","6644f3d9":"code","536ccecd":"code","a77e27da":"code","0c3538e5":"code","a88e1ac4":"code","dce26bc1":"code","cf0f3e36":"code","e86f73a3":"code","89e2a8cb":"code","aecda242":"code","eff24b7d":"code","8a3cd44b":"code","659dbff5":"code","c43fb2db":"code","dce6585b":"code","bf8582e0":"code","78c105c9":"code","6e26d5ab":"code","a1e730f6":"code","6761823e":"code","cb8ab21d":"code","67202306":"code","1c3c0712":"code","d182bc41":"code","91198560":"code","53050434":"code","a2f9a2c9":"code","a39f7116":"code","3108c7a0":"code","69b4293b":"code","948960ff":"code","443c518b":"code","2fe61622":"code","265e69c0":"code","1ce5d09c":"code","63b9d9c6":"code","88dff44e":"code","5b6710d5":"code","134bcfe7":"code","203cd9a5":"code","7f7d9827":"code","a20ac8e0":"code","e5771ec1":"code","44903feb":"code","cccc74e0":"code","471eee1e":"code","d3e8d162":"code","90c9f3ad":"code","50c89ece":"code","01705908":"code","bdcaa6a2":"code","835b31b3":"code","a91e5e80":"code","9f20d0bc":"code","955aa768":"code","7f84a662":"code","d0a8cbca":"code","cc6e7606":"code","855ef304":"code","acf239ce":"code","d5355da9":"code","8cd73774":"code","054e4a40":"markdown","d4c6a26d":"markdown","61d05c16":"markdown","c73ea2e0":"markdown","63b05721":"markdown","1333d558":"markdown","d5eb50ce":"markdown","da82e9e2":"markdown","298f9232":"markdown","acb5c692":"markdown","d850b43c":"markdown","81e0ad88":"markdown","571810b4":"markdown","891c85d5":"markdown","236d1692":"markdown","30217b5c":"markdown","370a5db4":"markdown","475fa281":"markdown","4d8c76ca":"markdown","e38cc043":"markdown","d4cb0c6e":"markdown","50d31eef":"markdown","0efd2768":"markdown","51b36d00":"markdown","7c3c127d":"markdown","f6c46773":"markdown","b1ff3017":"markdown","51fe8c7f":"markdown","7dd157bf":"markdown","5f2c73fc":"markdown","e5650213":"markdown","681d9438":"markdown","d7a40a32":"markdown","fa23dc5e":"markdown","c2353017":"markdown","de865bdf":"markdown","bc57f87a":"markdown","6416936e":"markdown","d32d627b":"markdown","32c0890b":"markdown","bd6d173c":"markdown","e28a0339":"markdown","91d113a0":"markdown","b0d52727":"markdown","7870e4b3":"markdown","77cfb100":"markdown","aea4ad51":"markdown","a1c9fc3f":"markdown","fd57f909":"markdown","7a1f737c":"markdown","adb6ed01":"markdown","c9212fd9":"markdown","126a6867":"markdown","feffb78b":"markdown","a1a47a12":"markdown","0d98c8c0":"markdown","811601a0":"markdown","d46a5dd4":"markdown","a5ad57a8":"markdown","ef2be3ea":"markdown","e8a28961":"markdown","bd91e617":"markdown","6ccfdf8f":"markdown","7aaf8bde":"markdown","a110bc58":"markdown","6364f86f":"markdown","40d31ab8":"markdown","e1ace9e9":"markdown","2cc42744":"markdown","0e2dca65":"markdown","ee5272b9":"markdown","d59f13cb":"markdown","d1958132":"markdown","af6abddb":"markdown","a152ecaf":"markdown","527d9909":"markdown","cf916149":"markdown","e79fe53b":"markdown","a2ab100f":"markdown","e6839dd3":"markdown"},"source":{"02fcbb5e":"import numpy as np\nimport pandas as pd\nimport scipy \nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline","32b5ce46":"data=pd.read_csv('..\/input\/life-expectancy-who\/Life Expectancy Data.csv')\n\n# Let us name our dataset as 'data'. And load it to see what it has init.","bf9b0543":"data.head(n=5)\n\n# Let us load first five observations of our dataset.","a7091f39":"data.info()","0e6795fe":"sns.heatmap(pd.isnull(data));\n\n# This plot highlights the null values.","972aadaa":"a=list(data.columns)\nb=[]\nfor i in a:\n    c=data[i].isnull().sum()\n    b.append(c)\nnull_df=pd.DataFrame({'Feature name':a,'no. of Nan':b})\nnull_df","187eca46":"# This piece of code will fill the null values of the selected feature with its mean.\n# This one is for filling Nans in 'Life expectancy' column.\n\ndata['Life expectancy ']=data['Life expectancy '].fillna(value=data['Life expectancy '].mean())\n\n# Lets also do the same for Adult Mortality.\n\ndata['Adult Mortality']=data['Adult Mortality'].fillna(value=data['Adult Mortality'].mean())\n","c2fc4fd8":"corr_data=data.corr()\ncorr_data","aa4b08b3":"sns.heatmap(corr_data)","88c6b93f":"sns.scatterplot(x=data['Schooling'],y=data['Alcohol']);\n\n#The semicolon atlast in the code is to hide the address of the plot which is not that required but I personally like doing that","60612e65":"# These values are mean values of the selected interval of other feature.\n\ndef impute_Alcohol(cols):\n    al=cols[0]\n    sc=cols[1]\n    if pd.isnull(al):\n        if sc<=2.5:\n            return 4.0\n        elif 2.5<sc<=5.0:\n            return 1.5\n        elif 5.0<sc<=7.5:\n            return 2.5\n        elif 7.5<sc<=10.0:\n            return 3.0\n        elif 10.0<sc<=15:\n            return 4.0\n        elif sc>15:\n            return 10.0\n    else:\n        return al\n    \ndata['Alcohol']=data[['Alcohol','Schooling']].apply(impute_Alcohol,axis=1)","2434acab":"sns.heatmap(pd.isnull(data))","2b3f0072":"data['Alcohol']=data['Alcohol'].fillna(value=data['Alcohol'].mean())","115858ae":"# Rechecking the Heatmap.\nsns.heatmap(pd.isnull(data))","9fd10e30":"sns.distplot(data['Alcohol']);","793dd201":"scipy.stats.skew(data['Alcohol'],axis=0)\n\n# This shows that this column has positive skew.","c201ebfd":"sns.scatterplot(x=data['Life expectancy '],y=data['Polio']);\n\n# Scattterplot between them.","8c3c9785":"def impute_polio(c):\n    p=c[0]\n    l=c[1]\n    if pd.isnull(p):\n        if l<=45:\n            return 80.0\n        elif 45<l<=50:\n            return 67.0\n        elif 50<l<=60:\n            return 87.44\n        elif 60<l<=70:\n            return 91\n        elif 70<l<=80:\n            return 94.3\n        elif l>80:\n            return 95\n    else:\n        return p\n    \ndata['Polio']=data[['Polio','Life expectancy ']].apply(impute_polio,axis=1)","6644f3d9":"a=list(data.columns)\nb=[]\nfor i in a:\n    c=data[i].isnull().sum()\n    b.append(c)\nnull_df=pd.DataFrame({'Feature name':a,'no. of Nan':b})\nnull_df","536ccecd":"# Scatter plot between these features.\n\nsns.scatterplot(x=data['Polio'],y=data['Diphtheria ']);","a77e27da":"def impute_Diptheria(c):\n    d=c[0]\n    p=c[1]\n    if pd.isnull(d):\n        if p<=10:\n            return 75.0\n        elif 10<p<=40:\n            return 37.0\n        elif 40<p<=45:\n            return 40.0\n        elif 45<p<=50:\n            return 50.0\n        elif 50<p<=60:\n            return 55.0\n        elif 60<p<=80:\n            return 65.0\n        elif p>80:\n            return 90.0\n    else:\n        return d\ndata['Diphtheria ']=data[['Diphtheria ','Polio']].apply(impute_Diptheria,axis=1)","0c3538e5":"a=list(data.columns)\nb=[]\nfor i in a:\n    c=data[i].isnull().sum()\n    b.append(c)\nnull_df=pd.DataFrame({'Feature name':a,'no. of Nan':b})\nnull_df\n\n# A look at the null values again.","a88e1ac4":"sns.scatterplot(x=data['Diphtheria '],y=data['Hepatitis B']);\n\n# Scatterplot between them.","dce26bc1":"def impute_HepatatisB(cols):\n    hep=cols[0]\n    dip=cols[1]\n    if pd.isnull(hep):\n        if dip<=15:\n            return 75.0\n        elif 15<dip<=30:\n            return 20.0\n        elif 30<dip<=45:\n            return 38.0\n        elif 45<dip<=60:\n            return 43.0\n        elif 60<dip<=80:\n            return 63.0\n        elif dip>80:\n            return 88.4\n    else:\n        return hep\n    \ndata['Hepatitis B']=data[['Hepatitis B','Diphtheria ']].apply(impute_HepatatisB,axis=1)","cf0f3e36":"data[data['Diphtheria ']>80.0]['Hepatitis B'].mean()\n\n# Mean for imputing Diptheria Nans in 80-100 interval.","e86f73a3":"a=list(data.columns)\nb=[]\nfor i in a:\n    c=data[i].isnull().sum()\n    b.append(c)\nnull_df=pd.DataFrame({'Feature name':a,'no. of Nan':b})\nnull_df","89e2a8cb":"sns.scatterplot(x=data['Life expectancy '],y=data[' BMI ']);","aecda242":"def impute_BMI(c):\n    b=c[0]\n    l=c[1]\n    if pd.isnull(b):\n        if l<=50:\n            return 25.0\n        elif 50<l<=60:\n            return 25.0\n        elif 60<l<=70:\n            return 32.0\n        elif 70<l<=80:\n            return 46.8\n        elif 80<l<=100:\n            return 60.0\n    else:\n        return b\n    \ndata[' BMI ']=data[[' BMI ','Life expectancy ']].apply(impute_BMI,axis=1)","eff24b7d":"a=list(data.columns)\nb=[]\nfor i in a:\n    c=data[i].isnull().sum()\n    b.append(c)\nnull_df=pd.DataFrame({'Feature name':a,'no. of Nan':b})\nnull_df","8a3cd44b":"sns.scatterplot(y=data['Total expenditure'],x=data['Alcohol']);","659dbff5":"def impute_Total_exp(c):\n    t=c[0]\n    a=c[1]\n    if pd.isnull(t):\n        if a<=2.5:\n            return 5.08\n        elif 2.5<a<=5.0:\n            return 6.0\n        elif 5.0<a<=10.0:\n            return 6.71\n        elif 10.0<a<=12.5:\n            return 6.9\n        elif a>12.5:\n            return 6.68\n    else:\n        return t\n    \ndata['Total expenditure']=data[['Total expenditure','Alcohol']].apply(impute_Total_exp,axis=1)        ","c43fb2db":"sns.scatterplot(x=data['percentage expenditure'],y=data['GDP']);","dce6585b":"def impute_GDP(c):\n    g=c[0]\n    p=c[1]\n    if pd.isnull(g):\n        if p<=1250:\n            return 1100.0\n        elif 1250<p<=2500:\n            return 1800.0\n        elif 2500<p<=3750:\n            return 2900.0\n        elif 3750<p<=7500:\n            return 3500.0\n        elif 7500<p<=8750:\n            return 4500.0\n        elif 8750<p<=10000:\n            return 5000.0\n        elif 10000<p<=11250:\n            return 5700.0\n        elif 11250<p<=12500:\n            return 7000.0\n        elif 12500<p<=15000:\n            return 8000.0\n        elif 15000<p<=17500:\n            return 9000.0\n        elif p>17500:\n            return 8500.0\n    else:\n        return g\n    \ndata['GDP']=data[['GDP','percentage expenditure']].apply(impute_GDP,axis=1)","bf8582e0":"sns.scatterplot(x=data['infant deaths'],y=data['Population']);","78c105c9":"def impute_population(c):\n    p=c[0]\n    i=c[1]\n    if pd.isnull(p):\n        if i<=100:\n            return 0.19*((10)**9)\n        elif 100<i<=250:\n            return 0.18*((10)**9)\n        elif 250<i<=350:\n            return 0.02*((10)**9)\n        elif 350<i<=900:\n            return 0.1*((10)**9)\n        elif 900<i<=1100:\n            return 0.18*((10)**9)\n        elif 1100<i<=1250:\n            return 0.05*((10)**9)\n        elif 1250<i<=1500:\n            return 0.19*((10)**9)\n        elif 1500<i<=1750:\n            return 0.05*((10)**9)\n        elif i>1750:\n            return 0.1*((10)**9)\n    else:\n        return p\n    \ndata['Population']=data[['Population','infant deaths']].apply(impute_population,axis=1)","6e26d5ab":"sns.scatterplot(x=data[' BMI '],y=data[' thinness  1-19 years']);","a1e730f6":"def impute_Thin_1(c):\n    t=c[0]\n    b=c[1]\n    if pd.isnull(t):\n        if b<=10:\n            return 5.0\n        elif 10<b<=20:\n            return 10.0\n        elif 20<b<=30:\n            return 8.0\n        elif 30<b<=40:\n            return 6.0\n        elif 40<b<=50:\n            return 3.0\n        elif 50<b<=70:\n            return 4.0\n        elif b>70:\n            return 1.0\n    else:\n        return t\n    \ndata[' thinness  1-19 years']=data[[' thinness  1-19 years',' BMI ']].apply(impute_Thin_1,axis=1)","6761823e":"sns.scatterplot(x=data[' BMI '],y=data[' thinness 5-9 years']);","cb8ab21d":"def impute_Thin_1(c):\n    t=c[0]\n    b=c[1]\n    if pd.isnull(t):\n        if b<=10:\n            return 5.0\n        elif 10<b<=20:\n            return 10.0\n        elif 20<b<=30:\n            return 8.0\n        elif 30<b<=40:\n            return 6.0\n        elif 40<b<=50:\n            return 3.0\n        elif 50<b<=70:\n            return 4.0\n        elif b>70:\n            return 1.0\n    else:\n        return t\n    \ndata[' thinness 5-9 years']=data[[' thinness 5-9 years',' BMI ']].apply(impute_Thin_1,axis=1)","67202306":"sns.scatterplot(x=data['Life expectancy '],y=data['Income composition of resources']);","1c3c0712":"def impute_Income(c):\n    i=c[0]\n    l=c[1]\n    if pd.isnull(i):\n        if l<=40:\n            return 0.4\n        elif 40<l<=50:\n            return 0.42\n        elif 50<l<=60:\n            return 0.402\n        elif 60<l<=70:\n            return 0.54\n        elif 70<l<=80:\n            return 0.71\n        elif l>80:\n            return 0.88\n    else:\n        return i\n        \ndata['Income composition of resources']=data[['Income composition of resources','Life expectancy ']].apply(impute_Income,axis=1)      ","d182bc41":"sns.scatterplot(x=data['Life expectancy '],y=data['Schooling']);","91198560":"def impute_schooling(c):\n    s=c[0]\n    l=c[1]\n    if pd.isnull(s):\n        if l<= 40:\n            return 8.0\n        elif 40<l<=44:\n            return 7.5\n        elif 44<l<50:\n            return 8.1\n        elif 50<l<=60:\n            return 8.2\n        elif 60<l<=70:\n            return 10.5\n        elif 70<l<=80:\n            return 13.4\n        elif l>80:\n            return 16.5\n    else:\n        return s\n    \ndata['Schooling']=data[['Schooling','Life expectancy ']].apply(impute_schooling,axis=1)","53050434":"data[(data['Life expectancy ']>80) & (data['Life expectancy ']<=90)]['Schooling'].mean()\n\n# Example of how iam deciding values for filling Nans above\n# You can see above in range above 80 we got avg. as 16.5 so we have imputed it that way.","a2f9a2c9":"a=list(data.columns)\nb=[]\nfor i in a:\n    c=data[i].isnull().sum()\n    b.append(c)\nnull_df=pd.DataFrame({'Feature name':a,'no. of Nan':b})\nnull_df","a39f7116":"y=data['Life expectancy ']","3108c7a0":"sns.distplot(y);","69b4293b":"X=data.drop('Life expectancy ',axis=1)","948960ff":"X.info()","443c518b":"X['Country']","2fe61622":"# Lets see unique values of this feature.\n\nX['Country'].unique()","265e69c0":"# Lets see number of unique values.\nX['Country'].nunique()","1ce5d09c":"# Lets have look at the other object type feature.\nX['Status'].unique()","63b9d9c6":"Country_dummy=pd.get_dummies(X['Country'])\n# Dummy variables for Country feature.","88dff44e":"status_dummy=pd.get_dummies(X['Status'])\n# Dummy variables for status feature.","5b6710d5":"X.drop(['Country','Status'],inplace=True,axis=1)","134bcfe7":"X=pd.concat([X,Country_dummy,status_dummy],axis=1)","203cd9a5":"X.info()","7f7d9827":"X.head()","a20ac8e0":"from sklearn.model_selection import train_test_split","e5771ec1":"# Lets set 30% for testing and 70% for training the model.\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=101)","44903feb":"from sklearn.linear_model import LinearRegression","cccc74e0":"Linear_model= LinearRegression()","471eee1e":"Linear_model.fit(X_train,y_train)","d3e8d162":"predictions1=Linear_model.predict(X_test)\n\n# Naming it predictions1 because we are going to use some more models now.","90c9f3ad":"predictions1[0:10]\n\n# First 10 predictions.","50c89ece":"from sklearn.metrics import mean_squared_error","01705908":"print(mean_squared_error(y_test,predictions1)**(0.5))","bdcaa6a2":"from sklearn.metrics import r2_score","835b31b3":"r2_score(y_test,predictions1)","a91e5e80":"from sklearn.linear_model import Ridge","9f20d0bc":"ridge_model=Ridge()","955aa768":"ridge_model.fit(X_train,y_train)","7f84a662":"predictions2=ridge_model.predict(X_test)","d0a8cbca":"print(mean_squared_error(y_test,predictions2)**(0.5))","cc6e7606":"from sklearn.linear_model import Lasso","855ef304":"lasso_model=Lasso(alpha=0.00000001)\n\n# Alpha value here was selected after choosing 8 different combinations like 0.1,0.001,0.0001...etc.","acf239ce":"lasso_model.fit(X_train,y_train)","d5355da9":"predictions3=lasso_model.predict(X_test)","8cd73774":"print(mean_squared_error(y_test,predictions3)**(0.5))","054e4a40":"This was all about the project. ","d4c6a26d":"The DARKEST and the LIGHTEST blocks represets that there is a strong relationship between those attributes.","61d05c16":"### Here is the list of features that we are going to impute with other features:","c73ea2e0":"If we could observe from the heatmap the zone in which null values were failed to be filled is the same zone in which the other feature had its Null values. So for now lets fill those remaining Nans with the Mean Value.","63b05721":" But Lasso is also very powerful when we have alot of attributes in our data when p>n.","1333d558":"As you can see there are some features that are quite well correlated with each other. So, we would impute these nans using the other features which are nicely correlated with eachother.","d5eb50ce":"Now its time for imputing 'Hepatitis B' using 'Diptheria' feature.","da82e9e2":"Lets look at the distribution of the alcohol column.","298f9232":"In genral we impute null values using the MEAN, MEDIAN Or MODE of that specific feature. Lets see how many null values does each of the features has.","acb5c692":"It is always good to fill Nans using bunch of different values as we know that a feature is less likely to have a continous value about 200 or more times and Iam pretty sure that by doing this we would definitely come up with a good result.","d850b43c":"- Even though we had good score by using Lasso we choose Linear regression over it.","81e0ad88":"- As we choose more generalised method the flexibility in model decreases which indirectly results in the interpretability  of the model.","571810b4":"** But luckily we found an option of imputing 'Polio' feature with the 'Life expactancy' which nicely correlates with it. So for now lets impute 'Polio' feature firstly with 'Life expactancy' and then would impute others using this","891c85d5":"Lets solve this problem by using Linear Regression and see what it gives us.","236d1692":"## How do we impute NULL VALUES ?","30217b5c":"It has 22 columns and nearly 3000 entries and with the inequality in no. of observations from each feature it is clear that it has got some null values. Lets try to fix them up and try to solve the problem.","370a5db4":"We have fit our model, Now lets go for testing !","475fa281":"Now we dont have any null values in 'alcohol' column.","4d8c76ca":"Now none of them above features have Nans in them, Let us split the dataset and look for fitting a model.","e38cc043":"Let us now Visualise these correlation values using a heatmap again.","d4cb0c6e":"- Impute 'Thin 5-9' feature with 'BMI' feature.","50d31eef":"lets check our X or Predictors dataset.","0efd2768":"Now this looks good we have about 214 columns.","51b36d00":"Looks like we have got our predictions, lets have a look at our predictions.","7c3c127d":"Looks like huge dataset with over 20 features. Its almost not possible to get insight from this dataset by just loading its head. Lets read the info and check for its total number of observations and examine it whether it has null values or not.","f6c46773":"Let us check our Target Variable, also its distribution.","b1ff3017":"Now lets split the model","51fe8c7f":"It has 20 Quantitative and 2 Qualitative features.","7dd157bf":"Lets repeat this for all the features.","5f2c73fc":"Lets have a look at the Null values again","e5650213":"Now lets concatenate these 'Dummies' with our X dataset.","681d9438":"Where, 'p' stands for no. of features and 'n' stands for no. of observations.","d7a40a32":"##### Looks like we are still left with some Nans. But why did this exactly happened ?","fa23dc5e":"Now as we have finished filling Nans lets have a look at that null_df which shows no. of Nans.","c2353017":"- We can use interactions when the data has alot of correlation in between the features.","de865bdf":"### Other methods to improve our model:","bc57f87a":"Lets begin the process of imputing. ","6416936e":"- Impute 'BMI' feature with 'Life expactancy' feature.","d32d627b":"It almost have a normal distribution with negative skew.","32c0890b":"Let us write a simple algorithm which prints a dataframe which shows no. of nans from each column.","bd6d173c":"- Impute 'Thin 1-19' feature with 'BMI' feature.","e28a0339":"As we are predicting Life Expectancy our 'Target'(y) variable will be 'Life expectancy'. And remaining attributes would be considered as X or Predictors.","91d113a0":"We can create dummy variables for this objects to fit well in the model. Lets build the dummy variables.","b0d52727":"We can see in columns like 'Life Expectancy' and 'Adult Mortality' there were few Nans values which wasn't visible in our heatmap. So lets fill them up with their average value as they are Continous features.","7870e4b3":"Let us read the Dataset using Pandas 'read_csv' command.","77cfb100":"# THANK YOU !","aea4ad51":"- Now our next in list is 'Hepatatis B' which highly correlates with 'Diptheria', But both of them have null values in the same zone.\n","a1c9fc3f":"- Interpretability in the model is much useful when we are solving problems, where our main goal would be knowing the relations in between the features.","fd57f909":"- If we want to impute 'Diptheria' first with highly correlated feature 'Polio' then that again would be of no use because it has again the same problem of null values(Nans) in same Zone.","7a1f737c":"Imputing the selected values for each interval. It is the same way like we did for the 'Alcohol' feature.","adb6ed01":"We would be using better feature engineering for keeping things simple. This has a capability to get a score which matches other models which are more flexible models modelled using Hyper parameter tuning.","c9212fd9":"Oops! There are 'object' type of features in our Predictor(X) dataset. Lets explore them and try to convert them into numericals.","126a6867":"From the above correlation matrix 'Alcohol' feature nicely correlates with the 'Schooling' feature. Now lets plot a Scatterplot between them and observe the trend.","feffb78b":"Now lets cross check whether values were filled or not by using the same heatmap.","a1a47a12":"- Some times it is better to have more Adjusted R2, AIC(Akaike information Criterion), BIC(Bayesian information criterion) for judging our model","0d98c8c0":"Please give this is an upvote if you really found this helpful.","811601a0":"Looks like pretty good score ! Now lets see how it works if we use Ridge and Lasso Regression models.","d46a5dd4":"Now lets have a looks at its R Square Value.","a5ad57a8":" Lets repeat this for imputing  Nans for all other features as well.","ef2be3ea":"firstly lets drop those two object features and then concatenate it.","e8a28961":"# Solving this using Linear regression ","bd91e617":"- Impute 'Population' feature with 'Infant death' feature.","6ccfdf8f":"Now again cross checking the imputed values.","7aaf8bde":"Let us explore and find where the null values are actually present in the Dataset. It is kind of easy to visualize it rather than having it in any other format. Luckily we have heatmaps which uses isnull command to highlight the null values.","a110bc58":"- Impute 'GDP' feature with 'percentage expenditure\t' feature.","6364f86f":"Looks good but we do need some metrics to evaluate our model. In this Regression tasks nothing better than using RMSE. Lets Examine our model using RMSE.","40d31ab8":"But wait ! We are filling about 200 Nans, Is it ok fill Nans with MEAN this time ? the answer is absolutely Yes. But I thought of doing it in a different way. As it has Nans 20 times larger than the previous one, I thought to fill it using other column which represents best this column. ","e1ace9e9":"- I have selected these values for imputing the Nans by observing a the trends in between selected interval.","2cc42744":"Now we dont have any nulls in polio section. Lets impute 'Diphtheria' using 'Polio' feature.","0e2dca65":"- Impute 'Total expenditure' with 'Alcohol' feature.","ee5272b9":"lets see info of our X data.","d59f13cb":"### When to choose Linear Regression over Lasso ?","d1958132":"There is a huge positive skew for this distribution. Lets look at its Kurtosis value.","af6abddb":"- Impute 'Schooling' feature and 'Income Composition of resources' feature with 'Life expactancy' feature.","a152ecaf":"We can see Lasso almost all reached Linear regression model by using alpha=0.00000001 that means using Linear Regression is almost all ok.","527d9909":"Let's import the important libraries to operate the data.","cf916149":"Now Lets move on to filling the other columns as well. We will move one by one by filling the Nans of each feature. Our next in target is \"ALCOHOL\" feature which has almost about 194 Nans.","e79fe53b":"#### Important observations:","a2ab100f":"Ex: The mean value of Alcohol which is in between 5-10 of Schooling column is 4.0","e6839dd3":"To do this let us first see the correlation matrix of these features using '.corr' method by pandas."}}