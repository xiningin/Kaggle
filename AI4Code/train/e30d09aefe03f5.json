{"cell_type":{"9b8e8c31":"code","e96cfa19":"code","32925d80":"code","a4e1754e":"code","332da855":"code","03f37c2e":"code","589447bc":"code","91c7eb61":"code","d04a0a01":"code","dba76494":"code","ead59774":"code","d4e7bb9a":"code","570061f9":"code","ebb6b8f7":"code","29383e81":"code","bd8ffde8":"code","4540bb4f":"code","cd9e9029":"code","5bb5556c":"code","13aebaa2":"code","b9aab5cf":"code","2fa7f207":"code","e03efa6c":"code","b5c449d7":"code","5a2f7c53":"code","e83fdf66":"code","9de291b9":"code","d606f760":"markdown","538c3d21":"markdown","494230b4":"markdown","86e0440e":"markdown","8a72ce6b":"markdown","71e5498a":"markdown","2f2f422c":"markdown","33b5687b":"markdown","740b3b33":"markdown","0e93f062":"markdown","acad3304":"markdown","113004ac":"markdown","78cdafc4":"markdown","ec1538ed":"markdown","baa34a37":"markdown","6a6417c0":"markdown","bf6809ad":"markdown","da0592d0":"markdown","91a41bf5":"markdown","670e32d5":"markdown"},"source":{"9b8e8c31":"from datetime import datetime\nimport numpy as np             #for numerical computations like log,exp,sqrt etc\nimport pandas as pd            #for reading & storing data, pre-processing\nimport matplotlib.pylab as plt #for visualization\n#for making sure matplotlib plots are generated in Jupyter notebook itself\n%matplotlib inline             \nfrom statsmodels.tsa.stattools import adfuller\nfrom statsmodels.tsa.stattools import acf, pacf\nfrom statsmodels.tsa.seasonal import seasonal_decompose\nfrom statsmodels.tsa.arima_model import ARIMA\nfrom matplotlib.pylab import rcParams\nrcParams['figure.figsize'] = 10, 6","e96cfa19":"dataset=pd.read_excel('\/media\/gargi\/Data\/DataSets\/Airlines+Data.xlsx')\ndataset['Month'] = pd.to_datetime(dataset['Month'],infer_datetime_format=True) #convert from string to datetime\nindexedDataset = dataset.set_index(['Month'])\nindexedDataset.head(5)","32925d80":"plt.xlabel('Date')\nplt.ylabel('Number of air passengers')\nplt.plot(indexedDataset)","a4e1754e":"#Determine rolling statistics\nrolmean = indexedDataset.rolling(window=12).mean() #window size 12 denotes 12 months, giving rolling mean at yearly level\nrolstd = indexedDataset.rolling(window=12).std()\nprint(rolmean,rolstd)","332da855":"#Plot rolling statistics\norig = plt.plot(indexedDataset, color='blue', label='Original')\nmean = plt.plot(rolmean, color='red', label='Rolling Mean')\nstd = plt.plot(rolstd, color='black', label='Rolling Std')\nplt.legend(loc='best')\nplt.title('Rolling Mean & Standard Deviation')\nplt.show(block=False)","03f37c2e":"#Perform Augmented Dickey\u2013Fuller test:\nprint('Results of Dickey Fuller Test:')\ndftest = adfuller(indexedDataset['Passengers'], autolag='AIC')\n\ndfoutput = pd.Series(dftest[0:4], index=['Test Statistic','p-value','#Lags Used','Number of Observations Used'])\nfor key,value in dftest[4].items():\n    dfoutput['Critical Value (%s)'%key] = value\n    \nprint(dfoutput)","589447bc":"#Estimating trend\nindexedDataset_logScale = np.log(indexedDataset)\nplt.plot(indexedDataset_logScale)","91c7eb61":"#The below transformation is required to make series stationary\nmovingAverage = indexedDataset_logScale.rolling(window=12).mean()\nmovingSTD = indexedDataset_logScale.rolling(window=12).std()\nplt.plot(indexedDataset_logScale)\nplt.plot(movingAverage, color='red')","d04a0a01":"datasetLogScaleMinusMovingAverage = indexedDataset_logScale - movingAverage\ndatasetLogScaleMinusMovingAverage.head(12)\n\n#Remove NAN values\ndatasetLogScaleMinusMovingAverage.dropna(inplace=True)\ndatasetLogScaleMinusMovingAverage.head(10)","dba76494":"def test_stationarity(timeseries):\n    \n    #Determine rolling statistics\n    movingAverage = timeseries.rolling(window=12).mean()\n    movingSTD = timeseries.rolling(window=12).std()\n    \n    #Plot rolling statistics\n    orig = plt.plot(timeseries, color='blue', label='Original')\n    mean = plt.plot(movingAverage, color='red', label='Rolling Mean')\n    std = plt.plot(movingSTD, color='black', label='Rolling Std')\n    plt.legend(loc='best')\n    plt.title('Rolling Mean & Standard Deviation')\n    plt.show(block=False)\n    \n    #Perform Dickey\u2013Fuller test:\n    print('Results of Dickey Fuller Test:')\n    dftest = adfuller(dataset['Passengers'], autolag='AIC')\n    dfoutput = pd.Series(dftest[0:4], index=['Test Statistic','p-value','#Lags Used','Number of Observations Used'])\n    for key,value in dftest[4].items():\n        dfoutput['Critical Value (%s)'%key] = value\n    print(dfoutput)\n    ","ead59774":"test_stationarity(datasetLogScaleMinusMovingAverage)","d4e7bb9a":"exponentialDecayWeightedAverage = indexedDataset_logScale.ewm(halflife=12, min_periods=0, adjust=True).mean()\nplt.plot(indexedDataset_logScale)\nplt.plot(exponentialDecayWeightedAverage, color='red')","570061f9":"datasetLogScaleMinusExponentialMovingAverage = indexedDataset_logScale - exponentialDecayWeightedAverage\ntest_stationarity(datasetLogScaleMinusExponentialMovingAverage)","ebb6b8f7":"datasetLogDiffShifting = indexedDataset_logScale - indexedDataset_logScale.shift()\nplt.plot(datasetLogDiffShifting)","29383e81":"datasetLogDiffShifting.dropna(inplace=True)\ntest_stationarity(datasetLogDiffShifting)","bd8ffde8":"decomposition = seasonal_decompose(indexedDataset_logScale) \n\ntrend = decomposition.trend\nseasonal = decomposition.seasonal\nresidual = decomposition.resid\n\nplt.subplot(411)\nplt.plot(indexedDataset_logScale, label='Original')\nplt.legend(loc='best')\n\nplt.subplot(412)\nplt.plot(trend, label='Trend')\nplt.legend(loc='best')\n\nplt.subplot(411)\nplt.plot(seasonal, label='Seasonality')\nplt.legend(loc='best')\n\nplt.subplot(411)\nplt.plot(residual, label='Residuals')\nplt.legend(loc='best')\n\nplt.tight_layout()\n\n#there can be cases where an observation simply consisted of trend & seasonality. In that case, there won't be \n#any residual component & that would be a null or NaN. Hence, we also remove such cases.\ndecomposedLogData = residual\ndecomposedLogData.dropna(inplace=True)\ntest_stationarity(decomposedLogData)","4540bb4f":"decomposedLogData = residual\ndecomposedLogData.dropna(inplace=True)\ntest_stationarity(decomposedLogData)","cd9e9029":"#ACF & PACF plots\n\nlag_acf = acf(datasetLogDiffShifting, nlags=20)\nlag_pacf = pacf(datasetLogDiffShifting, nlags=20, method='ols')\n\n#Plot ACF:\nplt.subplot(121)\nplt.plot(lag_acf)\nplt.axhline(y=0, linestyle='--', color='gray')\nplt.axhline(y=-1.96\/np.sqrt(len(datasetLogDiffShifting)), linestyle='--', color='gray')\nplt.axhline(y=1.96\/np.sqrt(len(datasetLogDiffShifting)), linestyle='--', color='gray')\nplt.title('Autocorrelation Function')            \n\n#Plot PACF\nplt.subplot(122)\nplt.plot(lag_pacf)\nplt.axhline(y=0, linestyle='--', color='gray')\nplt.axhline(y=-1.96\/np.sqrt(len(datasetLogDiffShifting)), linestyle='--', color='gray')\nplt.axhline(y=1.96\/np.sqrt(len(datasetLogDiffShifting)), linestyle='--', color='gray')\nplt.title('Partial Autocorrelation Function')\n            \nplt.tight_layout()            ","5bb5556c":"#AR Model\n#making order=(2,1,0) gives RSS=1.5023\nmodel = ARIMA(indexedDataset_logScale, order=(2,1,0))\nresults_AR = model.fit(disp=-1)\nplt.plot(datasetLogDiffShifting)\nplt.plot(results_AR.fittedvalues, color='red')\nplt.title('RSS: %.4f'%sum((results_AR.fittedvalues - datasetLogDiffShifting['Passengers'])**2))\nprint('Plotting AR model')","13aebaa2":"#MA Model\nmodel = ARIMA(indexedDataset_logScale, order=(0,1,2))\nresults_MA = model.fit(disp=-1)\nplt.plot(datasetLogDiffShifting)\nplt.plot(results_MA.fittedvalues, color='red')\nplt.title('RSS: %.4f'%sum((results_MA.fittedvalues - datasetLogDiffShifting['Passengers'])**2))\nprint('Plotting MA model')","b9aab5cf":"# AR+I+MA = ARIMA model\nmodel = ARIMA(indexedDataset_logScale, order=(2,1,2))\nresults_ARIMA = model.fit(disp=-1)\nplt.plot(datasetLogDiffShifting)\nplt.plot(results_ARIMA.fittedvalues, color='red')\nplt.title('RSS: %.4f'%sum((results_ARIMA.fittedvalues - datasetLogDiffShifting['Passengers'])**2))\nprint('Plotting ARIMA model')","2fa7f207":"predictions_ARIMA_diff = pd.Series(results_ARIMA.fittedvalues, copy=True)\nprint(predictions_ARIMA_diff.head())","e03efa6c":"#Convert to cumulative sum\npredictions_ARIMA_diff_cumsum = predictions_ARIMA_diff.cumsum()\nprint(predictions_ARIMA_diff_cumsum)","b5c449d7":"predictions_ARIMA_log = pd.Series(indexedDataset_logScale['Passengers'].iloc[0], index=indexedDataset_logScale.index)\npredictions_ARIMA_log = predictions_ARIMA_log.add(predictions_ARIMA_diff_cumsum, fill_value=0)\npredictions_ARIMA_log.head()","5a2f7c53":"# Inverse of log is exp.\npredictions_ARIMA = np.exp(predictions_ARIMA_log)\nplt.plot(indexedDataset)\nplt.plot(predictions_ARIMA)","e83fdf66":"indexedDataset_logScale","9de291b9":"results_ARIMA.plot_predict(1,264) ","d606f760":"# Read Data \n\nTime series deals with 2 columns, one is temporal ie: month in this case & another is the value to be forecasted ie: airplane passengers. To make plotting graphs easier, we set the index of pandas dataframe to the Month. During plots, the index will act by default as the x-axis & since it has only 1 more column, that will be automatically taken as the y-axis","538c3d21":"# Exponential Decay Transformation ","494230b4":"# Building Models ","86e0440e":"From the ACF graph, we see that curve touches y=0.0 line at x=2. Thus, from theory, Q = 2 From the PACF graph, we see that curve touches y=0.0 line at x=2. Thus, from theory, P = 2\n\nARIMA is AR + I + MA. Before, we see an ARIMA model, let us check the results of the individual AR & MA model. Note that, these models will give a value of RSS. Lower RSS values indicate a better model.","8a72ce6b":"# Time Shift Transformation","71e5498a":"We observe that the Time Series is stationary & also the series for moving avg & std. dev. is almost parallel to x-axis thus they also have no trend.\nAlso,\n\np-value has decreased from 0.022 to 0.005.\nTest Statistic value is very much closer to the Critical values.\nBoth the points say that our current transformation is better than the previous logarithmic transformation. Even though, we couldn't observe any differences by visually looking at the graphs, the tests confirmed decay to be much better.\nBut lets try one more time & find if an even better solution exists. We will try out the simple time shift technique, which is simply:\n\nGiven a set of observation on the time series:\n\ud835\udc650,\ud835\udc651,\ud835\udc652,\ud835\udc653,....\ud835\udc65\ud835\udc5b \n\nThe shifted values will be:\n\ud835\udc5b\ud835\udc62\ud835\udc59\ud835\udc59,\ud835\udc650,\ud835\udc651,\ud835\udc652,....\ud835\udc65\ud835\udc5b  <---- basically all xi's shifted by 1 pos to right\n\nThus, the time series with time shifted values are:\n\ud835\udc5b\ud835\udc62\ud835\udc59\ud835\udc59,(\ud835\udc651\u2212\ud835\udc650),(\ud835\udc652\u2212\ud835\udc651),(\ud835\udc653\u2212\ud835\udc652),(\ud835\udc654\u2212\ud835\udc653),....(\ud835\udc65\ud835\udc5b\u2212\ud835\udc65\ud835\udc5b\u22121)","2f2f422c":"# Data Transformation to achieve Stationarity","33b5687b":"For a Time series to be stationary, its ADCF test should have:\n\n1.p-value to be low (according to the null hypothesis)\n2.The critical values at 1%,5%,10% confidence intervals should be as close as possible to the Test Statistics\nFrom the above ADCF test result, we see that p-value(at max can be 1.0) is very large. Also critical values are no where close to the Test Statistics. Hence, we can safely say that our Time Series at the moment is not stationary","740b3b33":"# Import Libraries","0e93f062":"We see that our predicted forecasts are very close to the real time series values indicating a fairly accurate model.","acad3304":"There are a couple of ways to achieve stationarity through data transformation like taking  \ud835\udc59\ud835\udc5c\ud835\udc5410 , \ud835\udc59\ud835\udc5c\ud835\udc54\ud835\udc52 , square, square root, cube, cube root, exponential decay, time shift and so on ...\n\nIn our notebook, lets start of with log transformations. Our objective is to remove the trend component. Hence, flatter curves( ie: paralle to x-axis) for time series and rolling mean after taking log would say that our data transformation did a good job.","113004ac":"# Log Scale Transformation","78cdafc4":"From above graph, we observe that our intuition that *\"subtracting two related series having similar trend components will make the result stationary\"* is true. We find that:\n\n1.p-value has reduced from 0.99 to 0.022.\n2.The critical values at 1%,5%,10% confidence intervals are pretty close to the Test Statistic. Thus, from above 2 points, we can say that our given series is stationary.\nBut, in the spirit of getting higher accuracy, let us explore & try to find a better scale than our current log.\n\nLet us try out Exponential decay.\n","ec1538ed":"# Plotting ACF & PACF","baa34a37":"# Prediction & Reverse transformations","6a6417c0":"By combining AR & MA into ARIMA, we see that RSS value has decreased from either case , indicating ARIMA to be better than its individual component models.\n\nWith the ARIMA model built, we will now generate predictions. But, before we do any plots for predictions ,we need to reconvert the predictions back to original form. This is because, our model was built on log transformed data.","bf6809ad":"From the above graph, we see that rolling mean itself has a trend component even though rolling standard deviation is fairly constant with time. For our time series to be stationary, we need to ensure that both the rolling statistics ie: mean & std. dev. remain time invariant or constant with time. Thus the curves for both of them have to be parallel to the x-axis, which in our case is not so.\n\nTo further augment our hypothesis that the time series is not stationary, let us perform the ADCF test.","da0592d0":"From the plot below, we can see that there is a Trend compoenent in th series. Hence, we now check for stationarity of the data","91a41bf5":"From above graph, we see that even though rolling mean is not stationary, it is still better than the previous case, where no transfromation were applied to series. So we can atleast say that we are heading in the right direction.\n\nWe know from above graph that both the Time series with log scale as well as its moving average have a trend component. Thus we can apply a elementary intuition: subtraction one from the other should remove the trend component of both. Its like:\n\n\ud835\udc59\ud835\udc5c\ud835\udc54\ud835\udc60\ud835\udc50\ud835\udc4e\ud835\udc59\ud835\udc52\ud835\udc3f=\ud835\udc60\ud835\udc61\ud835\udc4e\ud835\udc61\ud835\udc56\ud835\udc5c\ud835\udc5b\ud835\udc4e\ud835\udc5f\ud835\udc66\ud835\udc5d\ud835\udc4e\ud835\udc5f\ud835\udc61(\ud835\udc3f1)+\ud835\udc61\ud835\udc5f\ud835\udc52\ud835\udc5b\ud835\udc51(\ud835\udc3f\ud835\udc47) \n\ud835\udc5a\ud835\udc5c\ud835\udc63\ud835\udc56\ud835\udc5b\ud835\udc54\ud835\udc4e\ud835\udc63\ud835\udc54\ud835\udc5c\ud835\udc53\ud835\udc59\ud835\udc5c\ud835\udc54\ud835\udc60\ud835\udc50\ud835\udc4e\ud835\udc59\ud835\udc52\ud835\udc34=\ud835\udc60\ud835\udc61\ud835\udc4e\ud835\udc61\ud835\udc56\ud835\udc5c\ud835\udc5b\ud835\udc4e\ud835\udc5f\ud835\udc66\ud835\udc5d\ud835\udc4e\ud835\udc5f\ud835\udc61(\ud835\udc341)+\ud835\udc61\ud835\udc5f\ud835\udc52\ud835\udc5b\ud835\udc51(\ud835\udc34\ud835\udc47) \n\ud835\udc5f\ud835\udc52\ud835\udc60\ud835\udc62\ud835\udc59\ud835\udc61\ud835\udc60\ud835\udc52\ud835\udc5f\ud835\udc56\ud835\udc52\ud835\udc60\ud835\udc45=\ud835\udc3f\u2212\ud835\udc34=(\ud835\udc3f1+\ud835\udc3f\ud835\udc47)\u2212(\ud835\udc341+\ud835\udc34\ud835\udc47)=(\ud835\udc3f1\u2212\ud835\udc341)+(\ud835\udc3f\ud835\udc47\u2212\ud835\udc34\ud835\udc47) \nSince, L & A are series & it moving avg, their trend will be more or less same, Hence\nLT-AT nearly equals to 0\n\nThus trend component will be almost removed. And we have,\n\n\ud835\udc45=\ud835\udc3f1\u2212\ud835\udc341 , our final non-trend curve","670e32d5":"\n\n\nFrom above 2 graphs, we can see that, visually this is the best result as our series along with rolling statistic values of moving avg & moving std. dev. is very much flat & stationary. But, the ADCF test shows us that:\n\np-value of 0.07 is not as good as 0.005 of exponential decay.\nTest Statistic value not as close to the critical values as that for exponential decay.\nWe have thus tried out 3 different transformation: log, exp decay & time shift. For simplicity, we will go with the log scale. The reason for doing this is that we can revert back to the original scale during forecasting.\n\nLet us now break down the 3 components of the log scale series using a system libary function. Once, we separate our the components, we can simply ignore trend & seasonality and check on the nature of the residual part."}}