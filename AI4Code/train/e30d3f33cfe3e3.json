{"cell_type":{"dfb335ac":"code","0a06f2b1":"code","c6eee365":"code","19f065ed":"code","2952aed4":"code","0d55f45b":"code","4b9e9f16":"code","4847ab1c":"code","34f75f86":"code","688ede12":"code","cc46236b":"code","aeae746b":"code","4d884a41":"code","5aab8b1f":"code","43cbbb4c":"code","a32d4efe":"code","1392bf82":"code","c473cd2a":"code","cb5b2ef6":"code","c8917c07":"code","45b77106":"code","20ee898b":"code","d72811b7":"code","35820620":"code","0c02ecdc":"code","8fece2b7":"code","02f3c46e":"code","a4803ab7":"code","d7629b14":"code","49a63cb2":"code","cfb713d7":"code","7085b6ca":"code","c6dff5dc":"code","f5a61a5c":"code","d2c785de":"code","53c849af":"code","0680a2fd":"code","b1e0cb93":"code","75468985":"code","82264f4b":"code","c4e9d305":"code","d734c071":"code","1ff3c633":"code","a0072485":"code","84b96e28":"code","33594f15":"code","afe44d17":"code","b92c4315":"code","79ecd053":"code","e86cd782":"code","35a61f66":"code","1febf902":"code","82674784":"code","46f3f855":"code","050c622d":"markdown","874f779d":"markdown","71a8e54e":"markdown","02e591fc":"markdown"},"source":{"dfb335ac":"import os\nimport numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report","0a06f2b1":"data = pd.read_csv('..\/input\/lung-cancer-dataset-by-staceyinrobert\/survey lung cancer.csv')\ndata.head()","c6eee365":"data.shape ","19f065ed":"data.columns","2952aed4":"data.describe()","0d55f45b":"data.info() ","4b9e9f16":"data.isna().sum()","4847ab1c":"all_cols = data.columns\nnumerical_cols = data._get_numeric_data().columns.to_list() #list of columns containing numeric data\ncategorical_cols = list(set(all_cols)- set(numerical_cols))","34f75f86":"numerical_cols, categorical_cols","688ede12":"categorical_cols","cc46236b":"#[data[x].unique() for x in categorical_cols[0:] ]","aeae746b":"data.drop(['GENDER'], inplace=True, axis=1)\n#data.drop(['Surname'], inplace=True, axis=1)","4d884a41":"data.head()","5aab8b1f":"data.shape","43cbbb4c":"from sklearn.preprocessing import LabelEncoder\n#convert y axis data to int value\nlb = LabelEncoder() \ndata['LUNG_CANCER'] = lb.fit_transform(data['LUNG_CANCER'])","a32d4efe":"data","1392bf82":"#Pairwise correlation of all columns in the dataframe.\ndata.corr() #convert all the values so that it will represent b\/w -1 and +1","c473cd2a":"data","cb5b2ef6":"y = data.pop('LUNG_CANCER') #will pop the sel price collumn and drop it in y\nX = data #remaining data","c8917c07":"X.head()","45b77106":"y.head()","20ee898b":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=3)","d72811b7":"X_train.shape, X_test.shape","35820620":"labels = ['True Neg','False Pos','False Neg','True Pos']\ncategories = ['Lung Cancer Not Detected', 'Lung Cancer Detected']","0c02ecdc":"from sklearn.metrics import make_scorer\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.model_selection import cross_validate\nscoring = {'accuracy':make_scorer(accuracy_score), \n           'precision':make_scorer(precision_score),\n           'recall':make_scorer(recall_score), \n           'f1_score':make_scorer(f1_score)}","8fece2b7":"from sklearn.neighbors import KNeighborsClassifier\nneigh = KNeighborsClassifier() #neigh => model\nneigh.fit(X_train, y_train) #training the data\npredictions = neigh.predict(X_test)   #X-test ---> Y_test\n","02f3c46e":"print(classification_report(y_test, predictions))","a4803ab7":"# Confusion Matrix\nconfusion_matrix(y_test,predictions)","d7629b14":"\ncf_matrix =confusion_matrix(y_test,predictions)\nsns.heatmap(cf_matrix, annot=True)","49a63cb2":"make_confusion_matrix(cf_matrix, group_names=labels, categories=categories)","cfb713d7":"kn = cross_validate(neigh, X_test, y_test, cv=5, scoring=scoring)\nprint(kn)","7085b6ca":"from sklearn.ensemble import GradientBoostingClassifier\nclf = GradientBoostingClassifier(random_state=0)\nclf.fit(X_train, y_train)\npredictions = clf.predict(X_test)","c6dff5dc":"print(classification_report(y_test, predictions))","f5a61a5c":"# Confusion Matrix\n#y_pred=clf.predict(X_test)\nconfusion_matrix(y_test,predictions)","d2c785de":"cf_matrix =confusion_matrix(y_test,predictions)\nsns.heatmap(cf_matrix, annot=True)","53c849af":"make_confusion_matrix(cf_matrix, group_names=labels, categories=categories)","0680a2fd":"gb = cross_validate(clf, X_test, y_test, cv=5, scoring=scoring)\nprint(gb)","b1e0cb93":"from sklearn.tree import DecisionTreeClassifier\ndtr = DecisionTreeClassifier(random_state=0)\ndtr.fit(X_train, y_train)\npredictions = dtr.predict(X_test)","75468985":"from sklearn.metrics import classification_report\nprint(classification_report(y_test, predictions))","82264f4b":"# Confusion Matrix\n#y_pred=dtr.predict(X_test)\nconfusion_matrix(y_test,predictions)","c4e9d305":"cf_matrix =confusion_matrix(y_test,predictions)\nsns.heatmap(cf_matrix, annot=True)","d734c071":"make_confusion_matrix(cf_matrix, group_names=labels, categories=categories)","1ff3c633":"#to read\n#from sklearn.model_selection import cross_val_score\n#cross_val_score(dtr, X_train, y_train, cv=10)\ndt = cross_validate(dtr, X_test, y_test, cv=5, scoring=scoring)\nprint(dt)","a0072485":"from sklearn.ensemble import RandomForestClassifier\nrf = RandomForestClassifier(random_state=0)\nrf.fit(X_train, y_train)\npredictions = rf.predict(X_test)","84b96e28":"from sklearn.metrics import classification_report\nprint(classification_report(y_test, predictions))","33594f15":"# Confusion Matrix\n#y_pred=rf.predict(X_test)\nconfusion_matrix(y_test,predictions)","afe44d17":"cf_matrix =confusion_matrix(y_test,predictions)\nsns.heatmap(cf_matrix, annot=True)","b92c4315":"make_confusion_matrix(cf_matrix, group_names=labels, categories=categories)","79ecd053":"rfc = cross_validate(rf, X_test, y_test, cv=5, scoring=scoring)\nprint(rfc)","e86cd782":"    models_scores_table = pd.DataFrame({'K Neighbors Classifier':[kn['test_accuracy'].mean(),\n                                                               kn['test_precision'].mean(),\n                                                               kn['test_recall'].mean(),\n                                                               kn['test_f1_score'].mean()],\n                                       \n                                      'Gradient boosting Classifier':[gb['test_accuracy'].mean(),\n                                                                   gb['test_precision'].mean(),\n                                                                   gb['test_recall'].mean(),\n                                                                   gb['test_f1_score'].mean()],\n                                       \n                                      'Decision Tree Classifier':[dt['test_accuracy'].mean(),\n                                                       dt['test_precision'].mean(),\n                                                       dt['test_recall'].mean(),\n                                                       dt['test_f1_score'].mean()],\n                                       \n                                      'Random Forrest Classifier':[rfc['test_accuracy'].mean(),\n                                                       rfc['test_precision'].mean(),\n                                                       rfc['test_recall'].mean(),\n                                                       rfc['test_f1_score'].mean()]},\n                                      \n                                      index=['Accuracy', 'Precision', 'Recall', 'F1 Score'])","35a61f66":"models_scores_table","1febf902":"models_scores_table.loc['Accuracy']","82674784":"fig = plt.figure()\nax = fig.add_axes([0,0,0.75,0.75])\nlangs = ['K Neighbors', 'Gradient boosting', 'Decision Tree', 'Random Forrest']\nstudents = np.array(models_scores_table.loc['Accuracy'])\nax.bar(langs,students)\nplt.show()","46f3f855":"def make_confusion_matrix(cf,\n                          group_names=None,\n                          categories='auto',\n                          count=True,\n                          percent=True,\n                          cbar=True,\n                          xyticks=True,\n                          xyplotlabels=True,\n                          sum_stats=True,\n                          figsize=None,\n                          cmap='Blues',\n                          title=None):\n    '''\n    This function will make a pretty plot of an sklearn Confusion Matrix cm using a Seaborn heatmap visualization.\n    Arguments\n    ---------\n    cf:            confusion matrix to be passed in\n    group_names:   List of strings that represent the labels row by row to be shown in each square.\n    categories:    List of strings containing the categories to be displayed on the x,y axis. Default is 'auto'\n    count:         If True, show the raw number in the confusion matrix. Default is True.\n    normalize:     If True, show the proportions for each category. Default is True.\n    cbar:          If True, show the color bar. The cbar values are based off the values in the confusion matrix.\n                   Default is True.\n    xyticks:       If True, show x and y ticks. Default is True.\n    xyplotlabels:  If True, show 'True Label' and 'Predicted Label' on the figure. Default is True.\n    sum_stats:     If True, display summary statistics below the figure. Default is True.\n    figsize:       Tuple representing the figure size. Default will be the matplotlib rcParams value.\n    cmap:          Colormap of the values displayed from matplotlib.pyplot.cm. Default is 'Blues'\n                   See http:\/\/matplotlib.org\/examples\/color\/colormaps_reference.html\n                   \n    title:         Title for the heatmap. Default is None.\n    '''\n\n\n    # CODE TO GENERATE TEXT INSIDE EACH SQUARE\n    blanks = ['' for i in range(cf.size)]\n\n    if group_names and len(group_names)==cf.size:\n        group_labels = [\"{}\\n\".format(value) for value in group_names]\n    else:\n        group_labels = blanks\n\n    if count:\n        group_counts = [\"{0:0.0f}\\n\".format(value) for value in cf.flatten()]\n    else:\n        group_counts = blanks\n\n    if percent:\n        group_percentages = [\"{0:.2%}\".format(value) for value in cf.flatten()\/np.sum(cf)]\n    else:\n        group_percentages = blanks\n\n    box_labels = [f\"{v1}{v2}{v3}\".strip() for v1, v2, v3 in zip(group_labels,group_counts,group_percentages)]\n    box_labels = np.asarray(box_labels).reshape(cf.shape[0],cf.shape[1])\n\n\n    # CODE TO GENERATE SUMMARY STATISTICS & TEXT FOR SUMMARY STATS\n    if sum_stats:\n        #Accuracy is sum of diagonal divided by total observations\n        accuracy  = np.trace(cf) \/ float(np.sum(cf))\n\n        #if it is a binary confusion matrix, show some more stats\n        if len(cf)==2:\n            #Metrics for Binary Confusion Matrices\n            precision = cf[1,1] \/ sum(cf[:,1])\n            recall    = cf[1,1] \/ sum(cf[1,:])\n            f1_score  = 2*precision*recall \/ (precision + recall)\n            stats_text = \"\\n\\nAccuracy={:0.3f}\\nPrecision={:0.3f}\\nRecall={:0.3f}\\nF1 Score={:0.3f}\".format(\n                accuracy,precision,recall,f1_score)\n        else:\n            stats_text = \"\\n\\nAccuracy={:0.3f}\".format(accuracy)\n    else:\n        stats_text = \"\"\n\n\n    # SET FIGURE PARAMETERS ACCORDING TO OTHER ARGUMENTS\n    if figsize==None:\n        #Get default figure size if not set\n        figsize = plt.rcParams.get('figure.figsize')\n\n    if xyticks==False:\n        #Do not show categories if xyticks is False\n        categories=False\n\n\n    # MAKE THE HEATMAP VISUALIZATION\n    plt.figure(figsize=figsize)\n    sns.heatmap(cf,annot=box_labels,fmt=\"\",cmap=cmap,cbar=cbar,xticklabels=categories,yticklabels=categories)\n\n    if xyplotlabels:\n        plt.ylabel('True label')\n        plt.xlabel('Predicted label' + stats_text)\n    else:\n        plt.xlabel(stats_text)\n    \n    if title:\n        plt.title(title)","050c622d":"#4. Random Forrest Classifier","874f779d":"# 3. Decision Tree Classifier","71a8e54e":"#2. Gradient boosting Classifier","02e591fc":"#1. K Neighbors Classifier\n"}}