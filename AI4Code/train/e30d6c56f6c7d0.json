{"cell_type":{"9be430ae":"code","4d3184ea":"code","31d37d34":"code","3e3f8859":"code","acd2d194":"code","d24383b6":"code","1450bebe":"code","12fb6bf9":"code","91f93aab":"code","798c84b3":"code","5981c5aa":"code","a0529789":"code","efd50a1b":"code","a19fb044":"code","f7390792":"code","271fd384":"code","19efcaa5":"code","e185a2cb":"code","2edfbb71":"code","d6868537":"code","9453f5a2":"code","761dc86e":"code","c8406d4d":"code","f60c8b7d":"code","8fb4e446":"code","54669835":"code","b12350bf":"code","073113cb":"code","83fb54f3":"code","2129170b":"code","b8189f24":"code","f63f95d3":"code","ff47ded2":"code","997817cc":"code","3537794b":"code","219fcc3f":"code","efcbc749":"code","4c803c00":"code","a98250fc":"code","d2d3d77a":"code","22c7c90f":"code","16cd7549":"code","9bfd889f":"code","c07ff98a":"code","1dc2debc":"code","a2d07a07":"code","cb4e0a82":"code","ea425ef4":"code","ca8cd75d":"code","385d179e":"code","d7db68be":"code","dcb5fe4f":"code","fdc960b9":"code","8a6d34f5":"code","b339b36c":"code","7e7974df":"code","d056b376":"code","df07e71d":"code","22b7a2ca":"code","9573b1c3":"code","198cef7f":"code","2a18b467":"code","f99173f2":"code","7be9fef3":"code","0e70e860":"code","fb44c83e":"code","b2c6bcec":"code","6ea4bbee":"code","65328cea":"code","e378516a":"code","03f2928a":"code","f64c3ca0":"code","988e435a":"code","fd608523":"code","8ba2e632":"markdown","7c6766f0":"markdown","1a9463ee":"markdown","474d72d2":"markdown","a1b78319":"markdown","e6e22b8d":"markdown","0c825d04":"markdown","1ccacb64":"markdown","66ed8bd4":"markdown","d5d04f0a":"markdown","d6f6d1a0":"markdown","d5a1856e":"markdown","db4e48b4":"markdown","c3eda6b7":"markdown","1bdc8b52":"markdown","38f92d67":"markdown","f75fcf55":"markdown","07e7dc4b":"markdown","d1f8226e":"markdown","b1c977f1":"markdown","bdfb7074":"markdown","e63a9652":"markdown","07def4b7":"markdown","9ea174d4":"markdown","60eae96a":"markdown","5f5e6253":"markdown","f6f432c7":"markdown","1c36e7e5":"markdown","a8f93ad3":"markdown","515575bf":"markdown","32454069":"markdown","a4775c18":"markdown","36a5c6be":"markdown","4814bb6f":"markdown","750327de":"markdown","f9eba0c3":"markdown","51e40c3c":"markdown","d44eed5e":"markdown","aa69d641":"markdown","0570d405":"markdown","4873b130":"markdown","44c92e8b":"markdown","c86e0781":"markdown","d42bf7dd":"markdown","b17993f8":"markdown","6718babf":"markdown","c2ecd991":"markdown","2637c56f":"markdown","e28649f9":"markdown","89a36c8f":"markdown","8721ba51":"markdown","6fbd5d42":"markdown","e529a6e6":"markdown"},"source":{"9be430ae":"# Import libraries to store data\nimport pandas as pd\nimport numpy as np\n# Import libraries to visualize data\nimport matplotlib.pyplot as plt\nfrom matplotlib.ticker import MultipleLocator\nimport seaborn as sns\nfrom ipywidgets import interact, interact_manual\n# Import libraries to process data\nimport tsfresh\nfrom scipy.signal import welch\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\n# Import libraries to classify data and score results\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import classification_report, f1_score\nimport xgboost as xgb\n# Import libraries used in functions and for feedback\nimport time\nimport random\nimport warnings\nwarnings.filterwarnings(\"ignore\") # Setting values on a Pandas DataFrame \n# sometimes throws errors, so I'll silence the warnings.\n\n# The below has to be set because matplotlib and XGB crash \n# by causing a duplicate copy of a library to be run. It's a \n# known issue, unfortunately.\nimport os\nos.environ['KMP_DUPLICATE_LIB_OK']='True'","4d3184ea":"f_train = pd.read_csv('..\/input\/X_train.csv')\nt_train = pd.read_csv('..\/input\/y_train.csv')\ntest = pd.read_csv('..\/input\/X_test.csv')\ntrain = pd.merge(t_train, f_train, how = 'outer', left_on = 'series_id', right_on = 'series_id')","31d37d34":"train.head()","3e3f8859":"train.groupby('series_id').mean().describe()","acd2d194":"test.groupby('series_id').mean().describe()","d24383b6":"surfaces = list(train.surface.unique())\nmeta = list(train.columns[0:5])\nposition = list(train.columns[5:9])\nmotion = list(train.columns[9:])\nfeatures = position+motion","1450bebe":"@interact\ndef plot_series(surface = surfaces, series_number = list(range(1,10))):\n    subset = train[train.surface == surface]\n    g = subset.groupby('series_id')\n    series_id = list(g.groups.keys())[series_number]\n    plt.figure(figsize = (10,24))\n    signal = g.get_group(series_id)\n    for i,feature in enumerate(features):\n        ax = plt.subplot(5,2,i+1)\n        ax.plot(signal[feature])\n        ax.set_title(feature)\n        ax.set_xlabel('Measurement Number')\n        ax.set_ylabel('Magnitude')","12fb6bf9":"@interact\ndef plot_overlapping_motion_graphs(feature = motion, number = (10,100,10)):\n    colors = ['grey','red','orange','yellow','green','blue','indigo','pink','brown']\n    plt.figure(figsize = (12,8))\n    plt.xlabel('Measurement Number')\n    plt.ylabel('Magnitude')\n    plt.title('Overlapping Motion Signals Sorted by Color')\n    for surface, color in zip(surfaces,colors):\n        series_ids = train.loc[train.surface==surface].series_id.unique()\n        for i, series in enumerate(series_ids[0:number]):\n            if i == 0:\n                plt.plot(train[train.series_id==series][feature].values, \n                     alpha = 0.05, \n                     color = color,\n                     label = surface)\n            else:\n                plt.plot(train[train.series_id==series][feature].values, \n                     alpha = 0.05, \n                     color = color)\n    leg = plt.legend()\n    for lh in leg.legendHandles: \n        lh.set_alpha(1)","91f93aab":"def RMS(signal):\n    signal = np.array(signal)\n    return np.sqrt((signal**2).mean())","798c84b3":"@interact\ndef plot_RMS_histograms(feature = motion):\n    plt.figure(figsize = (12,8))\n    df = pd.DataFrame()\n    g = train.groupby('series_id')\n    df['Surface'] = g.max()['surface']\n    df['RMS'] = g[feature].apply(RMS)\n    for surface in surfaces:\n        sns.distplot(df.loc[df.Surface==surface]['RMS'], \n                     hist = False, \n                     label = surface.title(),)\n    plt.legend()\n    plt.title('RMS Histograms for One Feature by Surface')","5981c5aa":"@interact\ndef plot_psd_overview(feature = motion, segment_size = (4,6)):\n    plt.figure(figsize = (12,8))\n    nperseg = 2**segment_size\n    for surface in surfaces:\n        freq,psd = welch(train[train.surface==surface][feature], \n                         nperseg = nperseg, \n                         return_onesided = True,\n                         scaling = 'spectrum')\n        plt.plot(psd,label = surface)\n    plt.legend()\n    plt.title('PSD Estimate by Surface (grouped data)')\n    plt.xlabel('Frequency (1\/sample)')\n    plt.ylabel('Magnitude of Signal')","a0529789":"def psd(signal):\n    _,PSD = welch(signal, nperseg = 128, scaling = 'spectrum')\n    return PSD","efd50a1b":"@interact\ndef plot_all_psds(feature = motion, surface = surfaces):\n    plt.figure(figsize = (12,8))\n    max_value = []\n    # Plot PSDs of every other surface in grey first\n    g = train[train.surface != surface].groupby('series_id')\n    for PSD in g[feature].apply(psd):\n        max_value.append(max(PSD))\n        plt.plot(PSD, color = 'gray', alpha = 0.05)\n    # Plot PSDs of surface in red\n    g = train[train.surface == surface].groupby('series_id')\n    for PSD in g[feature].apply(psd):\n        max_value.append(max(PSD))\n        plt.plot(PSD, color = 'red', alpha = 0.075)\n    \n    ylim = np.quantile(max_value, 0.99)\n    plt.ylim(0,ylim)\n    plt.title('Overlapped PSDs for one motion; \\none surface (red) compared to all surfaces (grey)')\n    plt.xlabel('Frequency')\n    plt.ylabel('Magnitude at Frequency')","a19fb044":"g = train.groupby(by = 'group_id')\ng67 = g.get_group(67)\ng67_g = g67.groupby('series_id')\nf, axes = plt.subplots(2, 2, figsize = (12,8))\naxs = [axes[0,0], axes[0,1], axes[1,0], axes[1,1]]\norientations = ['orientation_X','orientation_Y','orientation_Z','orientation_W']\nfor name, group in g67_g:\n    for ax,orientation in zip(axs,orientations):\n        group[orientation].plot(label = name, ax=ax, title = orientation)","f7390792":"def find_matches(current_signal, df, in_front = True, threshold = 0.01):\n    ## Returns a dataframe of the series ids, sums, and position (front\/end) of the likely matches for the given front and end values.\n    orientations = ['orientation_X','orientation_Y','orientation_Z','orientation_W']\n    selection = ['series_id'] + orientations\n    if in_front:\n        end = current_signal.iloc[127][selection]\n        front = df.loc[df.measurement_number ==0][selection]\n    else:\n        front = current_signal.iloc[0][selection]\n        end = df.loc[df.measurement_number ==127][selection]\n    compare = (front-end).abs()\n    if in_front:\n        compare.series_id = front.series_id\n    else:\n        compare.series_id = end.series_id\n    compare['sums'] = compare[orientations].sum(axis=1)\n    compare['in_front'] = in_front\n    compare.drop(labels = orientations, axis = 1, inplace = True)\n    compare.sort_values(by = 'sums', inplace = True)\n    if any(compare.sums < threshold):\n        return compare.loc[compare.sums < threshold], True\n    else:\n        return 0,False","271fd384":"def continuity(current_signal, compare_signal, in_front = True, merge_threshold = 4, slope_threshold = 6):\n    orientations = ['orientation_X','orientation_Y','orientation_Z','orientation_W']\n    select = ['series_id'] + orientations\n    if in_front:\n        end = current_signal[select]\n        front = compare_signal[select]\n    else:\n        front = current_signal[select]\n        end = compare_signal[select]\n    stitched = pd.concat([end,front])\n    diff = stitched.diff()\n    \n    # Check for continuity based on difference at merge point\n    mean = diff[orientations].iloc[120:127].mean()\n    std = diff[orientations].iloc[120:127].std()\n    test = (diff[orientations].iloc[128]-mean)\/std\n    if any(test.abs() > merge_threshold):\n        return False\n    \n    # Check for continuity based on slopes (t-test of means)\n    slope_front = diff[orientations].iloc[120:127].mean()\n    std_front = diff[orientations].iloc[120:127].std()\n    var_front = std_front.apply(np.square)\n    slope_end = diff[orientations].iloc[129:136].mean()\n    std_end = diff[orientations].iloc[129:136].std()\n    var_end = std_end.apply(np.square)\n    t_value = (slope_front-slope_end)\/(var_front\/8+var_end\/8).apply(np.sqrt)\n    if any(t_value.abs() > slope_threshold):\n        return False\n    \n    return True","19efcaa5":"def group_series(df, match_threshold = 0.01, continuity_thresholds = (4,6)):\n    df_c = df.copy()\n    # Create variables\n    lookup_dictionary = {}\n    new_group_id = -1\n    l_init = len(df_c)\n    t_start = time.time()\n    # Cycle through df\n    while len(df_c):\n        # Give feedback with regard to end time\n        t_elapsed = time.time()\n        t_diff = t_elapsed-t_start\n        l_now = len(df_c)\n        frac_done = 1-(l_now\/l_init)\n        if frac_done > 0:\n            ETA = t_diff\/frac_done-t_diff\n        else:\n            ETA = 0\n        message = 'Grouping Progress:{0:1.2f}%\\tETA:{1:1.2f}min'.format(frac_done*100,ETA\/60 )\n        print(message, end='\\r')\n        # Add new group to lookup dictionary\n        new_group_id +=1\n        lookup_dictionary[new_group_id] = []\n        # Start by selecting first available signal\n        start_series = df_c.series_id.unique()[0]\n        start_signal = df_c.loc[df_c.series_id==start_series]\n        current_signal = start_signal\n        # Add series to lookup dictionary\n        lookup_dictionary[new_group_id].append(start_series)\n        # Delete entry from df\n        indices =  start_signal.index\n        df_c.drop(indices,inplace = True)\n        # Prepare to expand forwards and backwards\n        expand_forward = True\n        expand_backward = True\n        # Expand signal forward first\n        while (expand_forward) & (len(df_c)>0):\n            candidates, match = find_matches(current_signal, \n                                             df_c, \n                                             in_front = True,\n                                             threshold = match_threshold)\n            continuity_tracker = []\n            if match:\n                for _,candidate in candidates.iterrows():\n                    compare_signal = df_c.loc[df_c.series_id == candidate.series_id]\n                    if continuity(current_signal, \n                                  compare_signal, \n                                  in_front = candidate.in_front,\n                                  merge_threshold = continuity_thresholds[0],\n                                  slope_threshold = continuity_thresholds[1]):\n                        current_series = candidate.series_id\n                        lookup_dictionary[new_group_id].append(current_series)\n                        current_signal = df_c.loc[df_c.series_id==current_series]\n                        df_c.drop(current_signal.index, inplace = True)\n                        continuity_tracker.append(True)\n                        break\n                    else:\n                        continuity_tracker.append(False)\n                if any(continuity_tracker):\n                    continue\n                else: # If nothing passed the continuity check then stop expanding forward\n                    expand_forward = False\n            else: # If no candidates, then stop expanding forward\n                expand_forward = False\n            # Clean variable space\n            del candidates,match\n        # Expand signal backwards\n        current_signal = start_signal\n        while (expand_backward) & (len(df_c)>0):\n            candidates, match = find_matches(current_signal, \n                                             df_c, \n                                             in_front = False,\n                                             threshold = match_threshold)\n            continuity_tracker = []\n            if match:\n                for _,candidate in candidates.iterrows():\n                    compare_signal = df_c.loc[df_c.series_id == candidate.series_id]\n                    if continuity(current_signal, \n                                  compare_signal, \n                                  in_front = candidate.in_front,\n                                  merge_threshold = continuity_thresholds[0],\n                                  slope_threshold = continuity_thresholds[1]):\n                        current_series = candidate.series_id\n                        lookup_dictionary[new_group_id].insert(0,current_series)\n                        current_signal = df_c.loc[df_c.series_id==current_series]\n                        df_c.drop(current_signal.index, inplace = True)\n                        continuity_tracker.append(True)\n                        break\n                    else:\n                        continuity_tracker.append(False)\n                if any(continuity_tracker):\n                    continue\n                else: # If nothing passed the continuity check then stop expanding forward\n                    expand_backward = False\n            else: # If no candidates, then stop expanding forward\n                expand_backward = False\n            # Clean variable space\n            del candidates, match\n    print(\"Grouping Done                             \")\n    print(\"Total Groups: {}\".format(new_group_id))\n    return lookup_dictionary","e185a2cb":"def stitch_series(df,ordered_series_array):\n    osa = ordered_series_array\n    g = df.groupby('series_id')\n    sub_dfs = []\n    for series in osa:\n        temp_df = g.get_group(series)\n        sub_dfs.append(temp_df)\n    df_out = pd.concat(sub_dfs)\n    df_out.measurement_number = list(range(0,len(df_out)))\n    return df_out","2edfbb71":"def recast_df(df, match_threshold = 0.01, continuity_thresholds = (4,6)):\n    lookup_dict = group_series(df, \n                               match_threshold=match_threshold, \n                               continuity_thresholds=continuity_thresholds)\n    sub_dfs = []\n    for key,group in lookup_dict.items():\n        temp_df = stitch_series(df,group)\n        temp_df['new_group_id'] = key\n        sub_dfs.append(temp_df)\n    stitched = pd.concat(sub_dfs)\n    return stitched","d6868537":"stitched_train = recast_df(train, match_threshold=0.05, continuity_thresholds=(6,9))","9453f5a2":"stitched_train.tail()","761dc86e":"@interact\ndef plot_stitched_signals(surface = surfaces, group = (1,10)):\n    sub_stitched = stitched_train.loc[stitched_train.surface==surface]\n    g = sub_stitched.groupby('new_group_id')\n    group_id = list(g.groups.keys())[group]\n    plt.figure(figsize = (12,18))\n    signal = g.get_group(group_id)\n    for i,feature in enumerate(features):\n        ax = plt.subplot(5,2,i+1)\n        ax.plot(signal['measurement_number'], signal[feature])\n        ax.set_title(feature)\n        ax.set_xlabel('Measurement Number')\n        ax.set_ylabel('Magnitude')","c8406d4d":"g = stitched_train.groupby('new_group_id')\nsurface_count = []\nsample_count = []\nfor name, group in g:\n    surfaces = group.surface.unique()\n    surface_count.append(len(surfaces))\n    sample_count.append(len(group))\nprint('Max Surface Count:{0}'.format(max(surface_count)))\nprint('Min Sample Count:{0}'.format(min(sample_count)))\nprint('Max Sample Count:{0}'.format(max(sample_count)))","f60c8b7d":"g = stitched_train.groupby('group_id')\nsample_count = []\nfor name, group in g:\n    surfaces = group.surface.unique()\n    surface_count.append(len(surfaces))\n    sample_count.append(len(group))\nprint('Max sample count in original groups:{0}'.format(max(sample_count)))","8fb4e446":"# Drop unnecessary columns from train, and add new columns to keep track of train\/test\ntrain.drop(['row_id','group_id'], axis = 1, inplace = True)\ntrain['orig_id'] = train.series_id\ntrain['test'] = 0\n# Drop unnecessary columns from test, and add new columns as above\ntest.drop(['row_id'], axis = 1, inplace = True)\ntest['surface'] = 'no_surface'\ntest['orig_id'] = test.series_id\ntest['test'] = 1\ntest.series_id += train.series_id.max()+1","54669835":"all_data = pd.concat([train,test], sort=False, ignore_index=True)","b12350bf":"all_stitched = recast_df(all_data, match_threshold=0.05, continuity_thresholds=(9,9))","073113cb":"g = all_stitched.groupby('new_group_id')\nsurface_count = []\nsample_count = []\nfor name, group in g:\n    surfaces = group.surface.unique()\n    surface_count.append(len(surfaces))\n    sample_count.append(len(group))\nprint('Max Surface Count:{0}'.format(max(surface_count)))\nprint('Min Sample Count:{0}'.format(min(sample_count)))\nprint('Max Sample Count:{0}'.format(max(sample_count)))","83fb54f3":"g = all_stitched.groupby('new_group_id')\nnew_dfs = []\nerror_surfaces = []\nerror_group_ids = []\nmatched = 0\nunmatched = 0\nunmatched_ids = []\nfor name, group in g:\n    surfaces = list(group.surface.unique())\n    # If a group contains one known surface and a total of two surfaces, then reclassify the unknown data as appropriate\n    if (len(surfaces)==2) & ('no_surface' in surfaces):\n        surfaces.remove('no_surface')\n        group['surface'] = surfaces[0]\n        new_dfs.append(group)\n        matched +=1\n    # If a group contains two known surfaces then store it as a group \"in error\"    \n    elif (len(surfaces)==2) & ('no_surface' not in surfaces):\n        error_surfaces.append(surfaces)\n        new_dfs.append(group)\n    # If a group has three surfaces or more than store it as a group \"in error\"    \n    elif (len(surfaces)>2):\n        error_surfaces.append(surfaces)\n        error_group_ids.append(name)\n        new_dfs.append(group)\n    # Else, increase the unmatched counter by one if there's only one surface and it's unknown\n    else:\n        new_dfs.append(group)\n        if surfaces[0] == 'no_surface':\n            unmatched += 1\n            unmatched_ids.append(name)\n# Make a new dataframe from reclassified data\nsorted_data = pd.concat(new_dfs)\nsorted_data.reset_index(inplace=True, drop = True)","2129170b":"print('Number of unmatched groups:{0}'.format(unmatched))","b8189f24":"error_surfaces","f63f95d3":"error_group_ids","ff47ded2":"# Some groups take >2 mins to plot because of the number of available samples\n@interact\ndef plot_grouped_data(group_id = (1,90)):\n    g = all_stitched.groupby('new_group_id')\n    plt.figure(figsize = (12,30))\n    signal = g.get_group(group_id)\n    for i,feature in enumerate(features):\n        ax = plt.subplot(5,2,i+1)\n        sns.lineplot(x = 'measurement_number',\n                     y = feature,\n                     data = signal, \n                     hue='surface', \n                     ax=ax);\n        ax.set_title(feature)\n        ax.set_xlabel('Measurement Number')\n        ax.set_ylabel('Magnitude')","997817cc":"def reassign_surfaces(new_surface_list, group_id, df):\n    # Pull group from dataframe first\n    error_group = df[sorted_data.new_group_id == group_id]\n    reassign = {}\n    start = 1\n    end = 1\n    segment = 0\n    escape = 0 \n    while escape == 0:\n        # Find segments that are represented by one surface (do this recursively)\n        temp_dict = {}\n        surface = error_group.surface.iloc[0]\n        start = error_group.first_valid_index()\n        if len(error_group.surface.unique()) > 1:\n            end = error_group.loc[error_group.surface != surface].first_valid_index()\n        else:\n            end = error_group.last_valid_index()\n            escape = 1\n        error_group.drop(range(start,end),inplace = True)\n        \n        #Store Data\n        if surface == 'no_surface':\n            temp_dict['surface'] = new_surface_list[segment]\n            temp_dict['start'] = start\n            temp_dict['end'] = end\n            reassign[segment] = temp_dict\n            segment += 1\n    # Use data to reassign surfaces\n    for key,value in reassign.items():\n        df.iloc[value['start']:value['end'],1] = value['surface']\n    \n    fixed_group = df[sorted_data.new_group_id == group_id]\n    plt.figure(figsize = (12,4))\n    sns.lineplot(x = 'measurement_number',\n                     y = 'orientation_X',\n                     data = fixed_group, \n                     hue='surface');","3537794b":"surface_2 = ['concrete','wood','wood']\nreassign_surfaces(surface_2,2,sorted_data)","219fcc3f":"surface_9 = ['concrete', 'concrete']\nreassign_surfaces(surface_9, 9, sorted_data)","efcbc749":"output = sorted_data[sorted_data.test == 1][['orig_id','surface']].groupby('orig_id').max().reset_index()\noutput.to_csv(path_or_buf = 'preClassification.csv', \n              header = ['series_id','surface'], \n              index = False)\nestimated_score = sum(output.surface!='no_surface')\/len(output)","4c803c00":"real_score = 0.55*0.6878 + 0.45*0.8175\nprint('Estimated Score: {0:1.3f}'.format(estimated_score))\nprint('Real Score: {0:1.3f}'.format(real_score))\nprint('Accuracy: {0:1.3f}'.format(real_score\/estimated_score))","a98250fc":"extract_from = sorted_data.drop(['surface','test','new_group_id', 'orig_id']+orientations, axis = 1)\nextract_from.head()","d2d3d77a":"params = {'abs_energy':None,\n          'absolute_sum_of_changes':None,\n          'agg_autocorrelation':[{'f_agg':'var','maxlag':32}],\n          'change_quantiles':[{'ql':0.25,'qh':0.75,'isabs':True, 'f_agg':'mean'},\n                             {'ql':0.25,'qh':0.75,'isabs':True, 'f_agg':'std'}],\n          'cid_ce':[{'normalize':True},{'normalize':False}],\n          'fft_aggregated':[{'aggtype': 'centroid'},\n                            {'aggtype': 'variance'},\n                            {'aggtype': 'skew'},\n                            {'aggtype': 'kurtosis'}],\n          'c3': [{'lag': 1}, {'lag': 2}, {'lag': 3}],\n          'standard_deviation': None,\n          'variance': None,\n          'skewness': None,\n          'kurtosis': None,\n          'maximum': None,\n          'minimum': None,\n          'sample_entropy':None,\n          'mean_abs_change':None,\n          'sum_values':None,\n          'quantile': [{'q': 0.1},\n                       {'q': 0.2},\n                       {'q': 0.3},\n                       {'q': 0.4},\n                       {'q': 0.6},\n                       {'q': 0.7},\n                       {'q': 0.8},\n                       {'q': 0.9}],\n          'large_standard_deviation': [{'r': 0.25},{'r':0.35}],\n          'fft_coefficient': [{'coeff': 0, 'attr': 'real'},\n                              {'coeff': 1, 'attr': 'real'},\n                              {'coeff': 2, 'attr': 'real'},\n                              {'coeff': 3, 'attr': 'real'},\n                              {'coeff': 4, 'attr': 'real'},\n                              {'coeff': 5, 'attr': 'real'},\n                              {'coeff': 6, 'attr': 'real'},\n                              {'coeff': 7, 'attr': 'real'},\n                              {'coeff': 8, 'attr': 'real'},\n                              {'coeff': 9, 'attr': 'real'},\n                              {'coeff': 10, 'attr': 'real'},\n                              {'coeff': 11, 'attr': 'real'},\n                              {'coeff': 12, 'attr': 'real'},\n                              {'coeff': 13, 'attr': 'real'},\n                              {'coeff': 14, 'attr': 'real'},\n                              {'coeff': 15, 'attr': 'real'},\n                              {'coeff': 16, 'attr': 'real'},\n                              {'coeff': 17, 'attr': 'real'},\n                              {'coeff': 18, 'attr': 'real'},\n                              {'coeff': 19, 'attr': 'real'},\n                              {'coeff': 20, 'attr': 'real'},\n                              {'coeff': 21, 'attr': 'real'},\n                              {'coeff': 22, 'attr': 'real'},\n                              {'coeff': 23, 'attr': 'real'},\n                              {'coeff': 24, 'attr': 'real'},\n                              {'coeff': 25, 'attr': 'real'},\n                              {'coeff': 26, 'attr': 'real'},\n                              {'coeff': 27, 'attr': 'real'},\n                              {'coeff': 28, 'attr': 'real'},\n                              {'coeff': 29, 'attr': 'real'},\n                              {'coeff': 30, 'attr': 'real'},\n                              {'coeff': 31, 'attr': 'real'},\n                              {'coeff': 32, 'attr': 'real'},\n                              {'coeff': 33, 'attr': 'real'},\n                              {'coeff': 34, 'attr': 'real'},\n                              {'coeff': 35, 'attr': 'real'},\n                              {'coeff': 36, 'attr': 'real'},\n                              {'coeff': 37, 'attr': 'real'},\n                              {'coeff': 38, 'attr': 'real'},\n                              {'coeff': 39, 'attr': 'real'},\n                              {'coeff': 40, 'attr': 'real'},\n                              {'coeff': 41, 'attr': 'real'},\n                              {'coeff': 42, 'attr': 'real'},\n                              {'coeff': 43, 'attr': 'real'},\n                              {'coeff': 44, 'attr': 'real'},\n                              {'coeff': 45, 'attr': 'real'},\n                              {'coeff': 46, 'attr': 'real'},\n                              {'coeff': 47, 'attr': 'real'},\n                              {'coeff': 48, 'attr': 'real'},\n                              {'coeff': 49, 'attr': 'real'},\n                              {'coeff': 50, 'attr': 'real'},\n                              {'coeff': 51, 'attr': 'real'},\n                              {'coeff': 52, 'attr': 'real'},\n                              {'coeff': 53, 'attr': 'real'},\n                              {'coeff': 54, 'attr': 'real'},\n                              {'coeff': 55, 'attr': 'real'},\n                              {'coeff': 56, 'attr': 'real'},\n                              {'coeff': 57, 'attr': 'real'},\n                              {'coeff': 58, 'attr': 'real'},\n                              {'coeff': 59, 'attr': 'real'},\n                              {'coeff': 60, 'attr': 'real'},\n                              {'coeff': 61, 'attr': 'real'},\n                              {'coeff': 62, 'attr': 'real'},\n                              {'coeff': 63, 'attr': 'real'},\n                              {'coeff': 64, 'attr': 'real'}],\n          \n         }","22c7c90f":"extracted_features = tsfresh.extract_features(extract_from,\n                                              column_id='series_id',\n                                              column_sort='measurement_number',\n                                              n_jobs = 2, \n                                              default_fc_parameters = params)","16cd7549":"efc = extracted_features.copy()\ntsfresh.utilities.dataframe_functions.impute(efc)\ny = all_stitched.groupby('series_id').max()['surface']\nmatched_y = y[y!='no_surface']\nmatched_ef = efc.iloc[matched_y.index]","9bfd889f":"features_filtered = tsfresh.select_features(matched_ef, matched_y)","c07ff98a":"feature_columns = features_filtered.columns","1dc2debc":"metadata = all_stitched.groupby('series_id').max()[['surface','test','new_group_id', 'orig_id']]","a2d07a07":"filtered_extracted = metadata.join(extracted_features[feature_columns])","cb4e0a82":"train_filtered = filtered_extracted[filtered_extracted.test == 0]\ntest_filtered = filtered_extracted[filtered_extracted.test == 1]","ea425ef4":"X_filtered = train_filtered.drop(labels = ['surface','test','new_group_id', 'orig_id'], \n                                 axis = 1)\ny_filtered = train_filtered.surface\n\nX_f_train, X_f_test, y_f_train, y_f_test = train_test_split(X_filtered, y_filtered, test_size=0.25, random_state=42, stratify = y_filtered)","ca8cd75d":"scaler_f = StandardScaler()\nX_f_train_scaled = scaler_f.fit_transform(X_f_train)\nX_f_test_scaled = scaler_f.transform(X_f_test)","385d179e":"RFC = RandomForestClassifier(n_estimators=50, random_state = 42, n_jobs=-1)\nRFC.fit(X_f_train, y_f_train)\ny_f_test_RFC = RFC.predict(X_f_test)\nprint(classification_report(y_f_test, y_f_test_RFC))","d7db68be":"NB = GaussianNB()\nNB.fit(X_f_train_scaled, y_f_train)\ny_test_NB = NB.predict(X_f_test_scaled)\nprint(classification_report(y_f_test, y_test_NB))","dcb5fe4f":"C_range = np.logspace(-4, 0, 4)\nparam_grid = dict(C=C_range)\nSVM = GridSearchCV(SVC(kernel = 'linear'), \n                    param_grid=param_grid, \n                    n_jobs=-1, \n                    pre_dispatch=2, \n                    cv = 3)\nSVM.fit(X_f_train_scaled, y_f_train)\ny_test_svc = SVM.predict(X_f_test_scaled)\nprint(classification_report(y_f_test, y_test_svc))","fdc960b9":"KNN = KNeighborsClassifier(10)\nKNN.fit(X_f_train_scaled,y_f_train)\ny_test_knn = KNN.predict(X_f_test_scaled)\nprint(classification_report(y_f_test, y_test_knn))","8a6d34f5":"DT = DecisionTreeClassifier()\nDT.fit(X_f_train_scaled,y_f_train)\ny_test_dt = DT.predict(X_f_test_scaled)\nprint(classification_report(y_f_test, y_test_dt))","b339b36c":"param_grid = {\n    'max_depth': [3,4,5,6],  # the maximum depth of each tree\n    'min_child_weight':np.linspace(0.8,1.2,4),\n    'gamma': np.linspace(0,0.2,4),\n}","7e7974df":"# The below parameters were found using GridSearchCV, using the parameter search basis above.\nXGB = xgb.sklearn.XGBClassifier(learning_rate = 0.025,\n                                objective = 'multi:softmax',\n                                n_estimators = 150,\n                                max_depth = 5,\n                                min_child_weight = 1.2,\n                                subsample=0.8,\n                                colsample_bytree = 0.8,\n                                gamma = 0.066,\n                                n_jobs = 4,\n                                nthreads = 1,\n                                silent = True,\n                                seed = 42)\nXGB.fit(X_f_train_scaled,y_f_train)\ny_test_XGB = XGB.predict(X_f_test_scaled)\nprint(classification_report(y_f_test, y_test_svc))","d056b376":"def get_predictions(filtered_data, classifiers, scaler):\n    warnings.filterwarnings(\"ignore\")\n    \n    df = filtered_data[['surface','test','new_group_id', 'orig_id']]\n    X = filtered_data.drop(labels = ['surface','test','new_group_id','orig_id'], \n                                 axis = 1).values\n    X_scaled = scaler.transform(X)\n    for name, classifier_dict in classifiers.items():\n        clf = classifier_dict['classifier']\n        if classifier_dict['scale']:\n            prediction = clf.predict(X_scaled)\n        else:\n            prediction = clf.predict(X)\n        df[name] = prediction\n    warnings.resetwarnings()\n    return df","df07e71d":"classifiers = {'RFC':{'classifier':RFC,'scale': False},\n               'NB':{'classifier':NB,'scale':True},\n               'SVM':{'classifier':SVM,'scale':True},\n               'KNN':{'classifier':KNN,'scale':True},\n               'DT':{'classifier':DT,'scale':True},\n               'XGB':{'classifier':XGB,'scale':True}}","22b7a2ca":"multi_predictions = get_predictions(filtered_data=train_filtered,\n                              classifiers=classifiers, \n                              scaler=scaler_f)","9573b1c3":"def classifier_ensemble(predictions, classifier_weights, groupby = 'new_group_id'):\n    warnings.filterwarnings(\"ignore\")\n    df = predictions[['test','new_group_id','orig_id']]\n    df['surface'] = 'None'\n    g = predictions.groupby(groupby)\n    for name, group in g:\n        index = group.index\n        weighted_value_counts = []\n        for classifier,weight in classifier_weights.items():\n            value_counts = group[classifier].value_counts()\n            weighted_value_counts.append(value_counts*weight)\n        all_counts = pd.DataFrame(weighted_value_counts).fillna(value=0)\n        surface = all_counts.sum().sort_values(ascending=False).index[0]\n        df.loc[index,'surface'] = surface\n    warnings.resetwarnings()\n    return df","198cef7f":"classifier_weights = {classifier:1 for classifier in classifiers.keys()}\ndel classifier_weights['NB'] # Naive Bayes classification wasn't great, so I won't include it\nensemble_prediction = classifier_ensemble(predictions=multi_predictions,\n                                 classifier_weights=classifier_weights)","2a18b467":"print(classification_report(train_filtered.surface, ensemble_prediction.surface))","f99173f2":"def CEParamSearch(y_true, multi_predictions, classifier_names, \n                  groupby = 'new_group_id', generations = 20, \n                  seed = 1, mutation_rate = 0.35):  \n    # Generate random weights first\n    np.random.seed(seed)\n    weights = np.random.rand(len(classifiers))\n    classifier_weights = {classifier:weight for classifier,weight in zip(classifier_names,weights)}\n    \n    # Classify the data and calculate a score with the weights\n    ensemble_prediction = classifier_ensemble(multi_predictions,classifier_weights,groupby=groupby)\n    y_pred = ensemble_prediction.surface\n    score = sum(y_pred == y_true)\/len(y_pred)\n    \n    # Keep track of score and time\n    scores = [score]\n    start = time.time()\n    \n    \n    # Hunt for a better solution\n    gene = random.choice(classifier_names)\n    orig_mut_rate = mutation_rate\n    for generation in range(1,generations+1):\n        # Make new weights using small mutation\n        new_weights = classifier_weights\n        mutation = (np.random.rand(1)-0.5)*mutation_rate\n        \n        new_weights[gene] += mutation\n        #Constrain weights to [0,1]\n        if new_weights[gene] < 0:\n            new_weights[gene] = 0\n        if new_weights[gene] > 1:\n            new_weights[gene] = 1\n            \n        # Classify data and obtain new score\n        ensemble_prediction = classifier_ensemble(multi_predictions,new_weights,groupby=groupby)\n        y_pred = ensemble_prediction.surface\n        new_score = sum(y_pred == y_true)\/len(y_pred)\n        change = new_score - score\n        # Select fittest weights\n        if change < 0: # If new score is worse change the gene, reestablish mutation rate\n            scores.append(score)\n            gene = random.choice(classifier_names)\n            mutation_rate = orig_mut_rate\n        elif change == 0: # Mutate, but don't change gene or the mutation rate\n            scores.append(score)\n            classifier_weights = new_weights\n        elif change > 0: # Mutate and change mutation rate, but don't change the gene\n            mutation_rate \/= 0.75\n            score = new_score\n            scores.append(score)\n            classifier_weights = new_weights\n        \n        # Update User\n        progress = generation\/generations\n        time_elapsed = (time.time()-start)\n        ETA = (time_elapsed\/progress-time_elapsed)\/60\n        print('Score:{0:1.5f}\\t|\\tChange:{1:1.5f}\\t|\\tTime Remaining: {2:1.2f} min'.format(score,change,ETA), end = '\\r')\n    \n    plt.plot(scores)\n    return classifier_weights","7be9fef3":"classifier_names = list(classifier_weights.keys())","0e70e860":"classifier_weights = CEParamSearch(train_filtered.surface,\n                                   multi_predictions,\n                                   classifier_names, \n                                   generations = 25,\n                                   mutation_rate = 0.5,\n                                   seed = 40)","fb44c83e":"classifier_weights = {'RFC': 1,\n 'SVM': 1,\n 'KNN': 0,\n 'DT': 0.22006725,\n 'XGB': 0.38377276}","b2c6bcec":"multi_predictions_test = get_predictions(test_filtered,classifiers,scaler_f)","6ea4bbee":"ensemble_prediction_test = classifier_ensemble(predictions=multi_predictions_test,\n                                 classifier_weights=classifier_weights)","65328cea":"output = ensemble_prediction_test[['orig_id','surface']].reset_index().drop('series_id', axis =1)\noutput.to_csv(path_or_buf = 'Ensemble_Classified.csv', \n              header = ['series_id','surface'], \n              index = False)","e378516a":"metadata = sorted_data.groupby('series_id').max()[['surface','test','new_group_id', 'orig_id']]\nfiltered_extracted_matched = metadata.join(extracted_features[feature_columns])\nunmatched_filtered = filtered_extracted_matched[filtered_extracted_matched.surface == 'no_surface']","03f2928a":"multi_predictions_unmatched = get_predictions(unmatched_filtered, classifiers, scaler_f)\nensemble_prediction_unmatched = classifier_ensemble(multi_predictions_unmatched, classifier_weights)\nunmatched_output = ensemble_prediction_unmatched[['orig_id','surface']].reset_index().drop('series_id', axis =1)","f64c3ca0":"matched_output = metadata[(metadata.test == 1)&(metadata.surface != 'no_surface')][['orig_id','surface']].reset_index().drop('series_id', axis =1)","988e435a":"os.chdir(\"\/kaggle\/working\/\")","fd608523":"output = pd.concat([matched_output,unmatched_output])\noutput.to_csv('Hybrid_Classified.csv', \n              header = ['series_id','surface'], \n              index = False)","8ba2e632":"Now I'll load the data and merge it where necessary in order to make it easier to manipulate later. I'll also preview the data in order to get a better sense of what I'm dealing with.","7c6766f0":"I have reason to believe that the stitching put together groups that didn't exist before. The longest group that was stitched is 17920 samples long (140 series). I'll check to see if the original dataset has a group with an equal number of samples","1a9463ee":"It would be useful to export the current matches to make sure that I'm on track. I'll select the data and then come up with an estimate score to compare against.","474d72d2":"Now I'll check to make sure that each new group in the stitched train dataframe is homogenous with regards to a surface, since they're homogenous with regards to a surface in the original train dataframe.","a1b78319":"The PSD of all of the data for a specific surface might hide the disparity found between samples. I'll graph the series specific PSDs for a given surface and feature next to get a sense of the variability.","e6e22b8d":"This is fewer total groups than stitching the training data! I did change the thresholds slightly, but even if the thresholds were kept the same the number of groups doesn't scale with the size of the dataframe. This indicates to me that some of the test data is part of runs that are present in the train data. I'll check the maximum and minimum sample counts, as well as the maximum surface count in a group.","0c825d04":"How interesting. This one does better at predicting hard_tiles at the expense of predicting carpet. The scores aren't awful, but they're certainly not great.","1ccacb64":"One thing that I haven't done is to explore the position data. Since the data was recorded in groups, it is possible to stitch it back together. I believe that this will help me to better differentiate the underlying signals, and if necessary upsample to increase the amount of training data. Upsampling can be done by slicing the data for 128 samples at every interval, since the original slicing was arbitrary to begin with. Stitching the data will also help to understand the holistic motion of the robot.\n\nThe graph below shows how the data can stitched using location cues. Series are stitched together so that the last entry of a series in an orientation is continuous with the first entry of the same orientation in a different series. This is equivalent to moving the lines below so that they they form one continuous line. This means that I'll have to forget the index and rewrite the measurement_numbers.","66ed8bd4":"### Table of Contents\n1. [Introduction](#section1)\n2. [Importing and Previewing Data](#section2)\n3. [Exploratory Data Analysis](#section3)\n4. [Stitching Time Series](#section4)\n5. [Feature Extraction](#section5)\n6. [Classifier Exploration](#section6)\n7. [Meta Ensemble Classifier](#section7)\n8. [Hybrid Classification](#section8)","d5d04f0a":"<br>\n<p style=\"font-size:48px;text-align:center;line-height:100%\">\n    A Hybrid Classifier to Predict Surface Type from IMU Data\n<\/p>\n<br>\n<p style=\"font-size:24px;text-align:center;line-height:100%\">\n    Pedro Jofre Lora\n<\/p>\n<br>","d6f6d1a0":"Check a regular decision tree **using the filtered, scaled data**","d5a1856e":"My suspicion is confirmed. Long runs were themselves cut up into groups, which were further cut up into series. It is possible (and likely given the public kernels that leverage position as a predictor) that the test data was cut up from a long run that also includes train data, so I'll toss all of the data into one large dataframe and stitch it together. At worst, I'll have all of the data stitched. I can get rid of group_id and row_id since I won't be using those. I'll then make sure that series ids are unique between the groups, and merge the groups using \"no_surface\" as the surface for the unknown groups.","db4e48b4":"It's helpful to keep some of the column names as specific variables since I plan to reuse them throughout my analysis. I've done this below.","c3eda6b7":"<a id='section1'><\/a>\n# Introduction","1bdc8b52":"The train and test datasets contain 3810 and 3816 sets of 128-sample-long signals, respectively. Each individual signal contains metadata, position data, and motion data. The metadata includes the series_id, group_id (training set only), surface (training set only), row_id, and measurement number. \nThe metadata is not a useful feature in predicting the surface of the robot since they were assigned either randomly or by convention. The measurement number helps to organize the time series, and in a sense is a stand-in for the time component of the time series.\nThe motion data describes the position of the robot as a quaternion. This data is most likely integrated (numerical integration) from the motion data. It is unlikely that the position data itself is helpful in determining the surface on which the robot is moving, since the robot's orientation is not dependent on its surface.\nThe motion data is comprised of  linear acceleration in the X, Y, and Z planes, and angular velocity in the same planes. This data is most likely to help in determining the surface on on which the robot is moving. I will explore some of this data in the next section in order to better understand the robot's motion and determine which features, if any, show characteristic signatures for a given surface.","38f92d67":"While TSFRESH has a set of automated parameters that I could have used, they're not necessarilly tuned for this specific problem AND they return an absolutely massive dataframe that my Macbook Air can't handle. I chose these specific set of features based on my exploration of the data and my intuition on the motion of the robot.\n\nTSFRESH is capable of selecting features for me by applying a series of hypothesis tests using the target (most of these tests are ANOVA, Fisher's test, or variants). I will allow it to tell me which features are most useful, though I'll likely use a combination of the reduced and full dataframes when I begin to create classifiers.","f75fcf55":"The results of CEParamSearch are random, though they do tend towards optimization as you can see in the graph above. The classifier weights below were used for a late submission whose public and private score I reveal below.","07e7dc4b":"From these graphs it appears that some surfaces have signals with greater variability in their amplitude than others. One way to estimate this is to look at histograms of the root-mean-square (RMS) of the signal. I'll write a simple RMS function, apply it to columns, and plot histograms of the RMS of each signal grouped by surface type for a specific feature.","d1f8226e":"Stiching time series is easy to do using the group_ids by simply matching the ends together. In the absence of a group_id, however, the problem becomes more interesting. It's not helpful to stitch the training series together unless I can do the same for the test set, so it's imperative to come up with a mechanism that can do it \"blind\", so to speak.\n\nThe first logical step is to find the possible matches that a series might have. This is easily done by comparing the ends of each series with those of every other series in a dataframe, and returning only those other series who are close (in all positions) to the chosen series. The \"closeness\" is a threshold that can be set by the user to select return more or fewer matches. The threshold is compared to the sum of the absolute differences between each position feature on the ends of both series.\n\nHaving a list of possible matches is helpful, but stitching requires that only one or zero matches be allowed. Given that this robot is a real device, we know that its motion will be contiguous and differentiable at all spots. A second function will check the contiguity and differentiability of the signals at the join. These will both be done through thresholds, since the data is discrete.\n\nFinally, a subroutine wraps the positive and negative select functions and iterates through the entire dataframe in order to return correctly ordered lists of series to be stitched together. See the collapsed functions below for more information.","b1c977f1":"The following notebook contains my annotated work to solve the Kaggle 2019 CareerCon Competition, in which I placed 11th overall. This competition challenged competitors to solve a classification problem in order to help a robot safely navigate its environment by predicting the surface on which the robot was moving using data from the Inertial Measurement Units. 128 samples (likely 0.25 seconds) were available from each of 10 channels (4 position and 6 motion) to make predictions.\n\nMy approach can be broken down into four distinct steps. The first step exploits the position data in order to regroup series in the train and test datasets combined. Then, I extracted time and freqency dependent features from each series and used the hypothesis testing tools in TSFRESH to select for the most relevant features. In the third step, I trained a slew of standard classifiers and evaluated their performance. Finally, I built a meta ensemble classifier that leveraged individual classifiers to produce a more robust prediction.\n\nYou may be wondering why I consider my solution a hybrid classifier. First, let me distinguish between a hybrid and an ensemble classifier. An ensemble classifier lets individual classifiers cast votes on a classification and then chooses the class with the most votes (sometimes in a weighted fashion). Each classifier can (and does) run in parallel, and each casts a vote only at the end. In contrast, a hybrid classifier lets individual classifiers classify subsets of the data. These subset classifications can then be used directly or can be fed into another classifier in order to classify again. For this reason, a hybrid classifier must be run in sequence. \n\nThis analysis must be run in sequence, since the results of the first classification (regrouping) is necessary for the meta ensemble classifier to make predictions. The results of regrouping are *also used directly*, though only to classify a subset of the data. Furthermore, the features that are calculated from the data are themselves classified (selected) by running hypothesis tests using the target information. This solution is a hybrid classifier for all of these reasons.\n\nFinally, though this work is my own, it is certainly the case that reading through comments and looking at publicly available kernels on Kaggle influenced my work! In particular, I wouldn't have known about TSFRESH had I not seen [Anuran Chakraborty's kernel](https:\/\/www.kaggle.com\/chanuran\/tsfresh-feature). The notebook from which I submitted my final entry did not use TSFRESH, and instead, I calculated a smaller set of features by hand. This was more work than using TSFRESH, which is a rich package. \nI also acknowledge that many of us had the same intuition that the position data bled across the train and test groups since it appeared that position was an important predictor in early public kernels. At the onset, I wanted to build a solution that attended to the physics of the problem, so I actually tossed out the position data. As many of us found, using the motion data alone was not enough to place in the top 100. Given that I'm actively seeking a position in a data science role, I relaxed my constraints and decided to look for ways to use the position data in a meaningful way. On a flight back from Shenzhen, it dawned on me that the group_ids in the training set meant that data had been split from longer runs, so I started working on an algorithm to restitch the data in order to upsample. When I landed, I found that there had already been some preliminary work done in that realm, but I chose to stick by the solution that I developed in-flight. Later, I realized that creating groups was useful precisely because I could leverage it in a hybridized approach. \n\nI hope that my thoughts come out in the remainder of the notebook. Please feel free to contact me via comments here on Kaggle, by email at [jofrepp@gmail.com](mailto:jofrepp@gmail.com), or through [LinkedIn](https:\/\/www.linkedin.com\/in\/pedro-jofre-lora-b7167095\/) if you have specific questions or comments about this kernel. Enjoy!","bdfb7074":"# Abstract","e63a9652":"The PSDs are helpful, but there is significant variability between PSDs of the same surface. Again, this won't be enough to categorize the data. There are hundreds of features ([link](https:\/\/www.sciencedirect.com\/science\/article\/pii\/S0925231218304843?via%3Dihub)) that can be extracted from signal data, so to perform the exhaustive search would be mind numbing and not entirely helpful. Instead, I'll use the TSFRESH package in order to extract the salient features for this dataset.","07def4b7":"<a id='section4'><\/a>\n# Stitching Time Series","9ea174d4":"Classifying the data by stitching groups together is 94.7% accurate. If I choose to accept this stitched and classified data, then I can do no better than ~96% accuracy on the final dataset because I have already misclassified 4% of the data. This is okay for now, though it's something I can improve on later.","60eae96a":"I will intentionally use the unclassified stitched data below to not introduce any extraneous noise into the classifiers. Once the classifiers are trained I can toss them the data that I have yet to classify.","5f5e6253":"Let's see what one of those stitched groups looks like, organized by surface like I did above.","f6f432c7":"I'll also scale the data since this is helpful for some classifiers. I will have to keep track of whether or not the data that I'm using is scaled.","1c36e7e5":"A hybrid classifier system was developed to predict the surface on which a robot was traversing using the signals output from the onboard IMU sensors. The hybrid classifier leveraged four steps of classification (denoted by (1), (2), (3), and (4)), performed in series in order to enhance the accuracy of the predictions. (1) First, the position data was exploited to predict the groups from which the data was originally split. These predicted groups generated sample sizes that were orders of magnitude greater than the original samples and predicted nearly 75% of the data. (2) Using TSFRESH, motion signals were then analyzed at the original sample size to generate a dataframe of time and frequency dependent features. These features were themselves classified based on hypothesis tests run in TSFRESH to reduce the total number of features used for standard classification techniques. (3) Six standard classifiers were tuned and trained on the dataframe developed in (3), which yielded a series of unique predictions at the smallest sample level. (4) A meta ensemble classifier was developed that leveraged the unique predictions from (3) alongside the groups that were predicted in (1) to produce a final prediction. This meta ensemble classifier was tuned by weighting the results of the standard classifiers, and a simple evolutionary algorithm was used to optimize the weights for each classifier. The hybrid classifier as a whole achieved 93.3% accuracy on the provided test set, which was significantly better than ensemble classifiers alone (66.5% accuracy) or position based grouping alone (75.9% accuracy). This finding corroborates other findings([1](https:\/\/pdfs.semanticscholar.org\/17d6\/d64202aeeeab07a09c3ff06cbee5f0632ce0.pdf), [2](https:\/\/www.hindawi.com\/journals\/cin\/2017\/1930702\/), [3](https:\/\/link.springer.com\/article\/10.1007\/s10489-009-0194-7)) that purpose-built hybrid classifiers outperform base and ensemble classifiers alone.","a8f93ad3":"Now I'll check Naive Bayes. **I'll use the filtered and scaled data.**","515575bf":"There is at least one run that has more than two surfaces, which isn't great. I'll rerun the analysis, but this time I'll store the groups that are in error. I'll also reclassify data if there are two surfaces and one of the surfaces is \"no_surface\". This is the first classification that I'll perform in this hybrid classifier. Not all of the data will be classified now, but I suspect that a majority will be. I am assuming that an individual group represents only one surface almost all of the time.","32454069":"The hybrid classifier that I developed was clearly better than using classical or ensemble classification schemes by themselves. Even a meta-ensemble of the standard classification schemes significantly outperforms a single classification scheme, which is helpful knowledge for future classification problems. Building this meta-classifier is simple enough, and tuning its performance even for a large number of parameters can be done using an evolutionary approach.","a4775c18":"<a id='section8'><\/a>\n# Hybrid Classification\nIt's unfair to say that the previous ensemble classification was not a hybrid classification already. Hybrid classifiers are those that classify over multiple iterations, rather than classifying by comparing the output of different processes. The above ensemble classification was built on the existing classification that was done to select features, and the classification to find groups. This following hybrid classification leverages all of that information to its extreme. I will accept the classification that was applied after finding groups, use the classified features to obtain outputs from individual classifiers, classify the remaining unclassified data using the ensemble classifier I generated, and finally stitch the position matched classification with the ensemble matched classification.","36a5c6be":"<a id='section3'><\/a>\n# Exploratory Data Analysis","4814bb6f":"A private score of 0.8280 and a public score of 0.4656 is good given that I'm not leveraging all of the position data. The public score mostly benefits from the position matched data.","750327de":"<a id='section9'><\/a>\n# Conclusion","f9eba0c3":"I'll plot the groups by group_id and color by surface to better understand what's happening. The group ids above are the groups that are \"in error\", so it is worthwhile to examine those closely.","51e40c3c":"Now perform linear SVC using a grid search. This grid search is small, so it runs fairly quickly. **I will use the filtered and scaled dataset for this.**","d44eed5e":"The classifier-specific predictions can be used in an ensemble classifier function that is written below. The function accepts the predictions and a dictionary that includes the classifiers to use and their weights.","aa69d641":"Now perform KNN **using the filtered and scaled data**.","0570d405":"I'll have to perform my own parameter search on the classifier ensemble in order to find the weights that produce the best results. Using a grid approach might not be the best course of action since there are 5 parameters to set. Instead, I'll use an evolutionary algorithm with a maximum number of generations.","4873b130":"As expected, the position data is most likely dependent on the particular series. The motion data, on the other hand, appears like it has unique signals in each direction. It's difficult to tell what the differences are between surfaces because of the auto-scale that is applied to the graphs. I'll look at all data plotted on a single graph for a given motion using low alpha values and different colors for each surface.","44c92e8b":"The first classifier I'll test is the Random Forest Classifier. **I'll use the filtered extracted dataset without scaling.**","c86e0781":"<a id='section6'><\/a>\n# Classifier Exploration\nThe purpose of this section is to develop a set of classifiers that I can leverage for a hybrid classifier. It's only worthwhile to keep classifiers that are relatively accurate, but I'll have to explore a series of them in order to choose the best ones. I'll make a train\/test (validate) set first in order to assess the classifiers. Then I'll dive headlong into the different kinds of classifiers that are available.","d42bf7dd":"It looks like I can safely manually assign surfaces to groups 2 and 9 based on the graphs above using the position data and a visual estimate of the RMS of the motion data. Talk about supervised learning... The function below helps me reassign surfaces.","b17993f8":"I'll look at the position and motion channels of individual series first to look at the ranges of the features. I'll organize the graphs by surface in order to have a clearer picture of the differences that might exist between them. I'll use ipywidgets to make this exploration interactive, which lets me scan back and forth between sets. You will have to fork this code in order to use the interactive feature, but you can see one instance of the graphs displayed.","6718babf":"The results of the Kaggle submission are wonderful! **A private score of 0.8777 and a public score of 0.9876 yields a combined accuracy of 0.9272 on the whole dataset.** We already know that ~4% of the data was misclassified after the groups were stitched, so only about ~3% of the remaining unclassified data was misclassified by the meta ensemble classifier. Overall, I'm thrilled with this score! To improve these scores I could tune all of the individual classifiers, search for more features to extract, and improve and the grouping mechanism.","c2ecd991":"<a id='section5'><\/a>\n# Feature Extraction\nThe next step is to extract features from the signals in order to develop a series of classifiers to classify the remaining data. We've looked at some features above (RMS, PSD), but there are a wealth of other features that I can pull out that are likely meaningful. The package TSFRESH was developed to automate feature extraction and selection from time series data. Their work is published in [this paper linked here](https:\/\/www.sciencedirect.com\/science\/article\/pii\/S0925231218304843?via%3Dihub), and the packgage documentation is publicly available [here](https:\/\/tsfresh.readthedocs.io\/en\/latest\/).\n\nI will only be extracting features from the motion data, since its the only data that should have any predictive power. After all, the position of the robot shouldn't predict what surface it's on (it's not the case that robots only walk east on concrete, though that would make for an interesting story). The position data was exploited above since I knew that the training data was split from a series of longer runs (original groups) and suspected that the same was true for the test data. If I have time, I will analyze how well the model performs in the absence of knowledge from stitching the data. It makes sense to keep the data stitched, however, since it is reasonable to assume that the robot walks on the same surface for at least an extended period of time.\n\nIn any case, I'll extract the relevant features below. Open up the collapsed code to see which features I chose to extract.","2637c56f":"<a id='section2'><\/a>\n# Importing and Previewing Data\n\nFirst, I'll load the libraries needed for this particular analysis. I've put the entire list of libraries below so that someone else running this kernel doesn't get halfway through before they realize that they're missing a library.","e28649f9":"The good news is that there are only 23 unmatched groups. This significantly reduces the complexity of the problem, since I will assume that each group only represents one surface. The unmatched ids are 68:90. \nThe bad news is that there were three groups that raised an error. All of the groups had more than two surfaces. My gut feeling is that they shouldn't have been stitched, though perhaps it's the case that the robot did meander from one surface to another in a meta-run. Those groups and their surfaces are shown below.","89a36c8f":"And finally, perform XGBoost **on the filtered and scaled data.**","8721ba51":"Now that I have an optimized set of classifier weights I will run the unclassified dataset through the ensemble classifier to check its performance against Kaggle.","6fbd5d42":"<a id='section7'><\/a>\n# Meta Ensemble Classifier\nThe real utility in stitching groups together is the ability to classify the entire group as one surface based on the results of classifying its constituent series. The easiest way to think of this is that the series vote on the classification of the group. The surface with the most \"votes\" is used to classify the entire group, similar to how RFC methods work. Furthermore, the results of each classifier above can be used in order to determine the votes for a group! Since the voting occurs as a sum of the number of instances that each surface appears in a group, each classifier can be assigned a relative weight in the voting schema. I'll employ this method to classify the remaining data.\n\nFirst I'll make a function that generates and joins predictions from the classifier instances.","e529a6e6":"As expected, the RMS of each signal does appear to be different between groups as a whole, though not different enough that classification could be done purely on the RMS of the signals. I'll look at the power spectral density of the signals next."}}