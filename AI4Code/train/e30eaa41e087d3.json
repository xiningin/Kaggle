{"cell_type":{"3d5a14b6":"code","e3c74412":"code","ff0c579a":"code","e606eecb":"code","cacfc23f":"code","87d0d181":"code","6edb3236":"code","c857425e":"code","6607d3aa":"code","ef719643":"code","f48b4711":"code","37b7bf14":"code","30d13a57":"code","7d6e54df":"code","1f470b88":"code","1bfbf507":"code","94883406":"code","3ead1953":"code","ebfe30b6":"code","2e7a4c6e":"code","163a8da2":"code","91caef1e":"code","01f50d6a":"code","fb8193b2":"markdown","87121441":"markdown","fd4edbb6":"markdown"},"source":{"3d5a14b6":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","e3c74412":"df=pd.read_csv('\/kaggle\/input\/fashionmnist\/fashion-mnist_train.csv')","ff0c579a":"df.shape","e606eecb":"df.head()","cacfc23f":"X=df.iloc[:,1:].values","87d0d181":"y=df.iloc[:,0].values","6edb3236":"import matplotlib.pyplot as plt","c857425e":"a=X[0].reshape(28,28)","6607d3aa":"plt.imshow(a)","ef719643":"from sklearn.model_selection import train_test_split","f48b4711":"X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2)","37b7bf14":"from sklearn.neighbors import KNeighborsClassifier","30d13a57":"clf=KNeighborsClassifier(n_neighbors=5)","7d6e54df":"clf.fit(X_train,y_train)","1f470b88":"y_pred=clf.predict(X_test)","1bfbf507":"from sklearn.metrics import accuracy_score\naccuracy_score(y_test,y_pred)","94883406":"#Feature Scaling\n\nfrom sklearn.preprocessing import StandardScaler\nsc=StandardScaler()\nX_train=sc.fit_transform(X_train)\nX_test = sc.transform(X_test)","3ead1953":"#Apply PCA\n\nfrom sklearn.decomposition import PCA\npca=PCA(n_components=200)\nX_train_new = pca.fit_transform(X_train)\nX_test_new=pca.transform(X_test)","ebfe30b6":"print(X_train.shape)\nprint(X_train_new.shape)","2e7a4c6e":"pca.explained_variance_ratio_","163a8da2":"clf.fit(X_train_new,y_train)","91caef1e":"y_pred_new = clf.predict(X_test_new)","01f50d6a":"accuracy_score(y_test,y_pred_new)","fb8193b2":"**We get almost the same kind of accuracy. And hence the beauty of PCA. But taking all the features it gave 85% accuracy with a lot of run time. And with top 100 features that determines the label, it gave 86.2% accuracy**","87121441":"**We can get the optimum feature of the model by running through a loop from 1-785.**\n\n*Note: I wont do do this step in this notebook, it will take a lot of time*","fd4edbb6":"**So by taking all the features, It took alot of time to run and gave 85% accuracy**\n\n**Now, we will do the same thing but will take top 200 features on which the y label exactly depends** "}}