{"cell_type":{"9a5e59f0":"code","d9efb7e8":"code","1c670e7b":"code","16350bfb":"code","a0b23bb7":"code","4595ac71":"code","334c804d":"code","4b2a7175":"code","bd6ff673":"code","c7ba125c":"code","66bedff5":"code","f07c10cb":"code","151989ca":"markdown","289cb031":"markdown","7d29634e":"markdown"},"source":{"9a5e59f0":"import pandas as pd \nimport numpy as np \nimport matplotlib.pyplot as plt \nimport seaborn as sns \n\n%matplotlib inline \n\nimport nltk\nimport re\nimport warnings \nwarnings.filterwarnings('ignore')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn import linear_model,metrics\n\nimport xgboost as xgb\nimport joblib\nimport pickle","d9efb7e8":"df_train=pd.read_csv('..\/input\/commonlitreadabilityprize\/train.csv')","1c670e7b":"df_train.head()","16350bfb":"df_train.shape","a0b23bb7":"df_train['target'].hist();\n\n## Target is normally distributed and also kind of scaled .","4595ac71":"### Just keep the regular columns \ndf_train=df_train[['excerpt','target']]","334c804d":"###  . Lets fit a simple Liner regression model to this dataset and see the score . I am going to do some basic text cleaning\nimport spacy \nnlp=spacy.load('en_core_web_sm')\nstopwords=nlp.Defaults.stop_words\n\nimport nltk \nfrom nltk.tokenize import word_tokenize\n\n\ndef preprocess(df):\n    df.loc[:,'cleaned']=df['excerpt'].apply(lambda x: str(x))      #convert to string \n    df.loc[:,'cleaned']=df['cleaned'].apply(lambda x: x.lower())   #lowercase the words \n    df.loc[:,'cleaned']=df['cleaned'].apply(lambda x: re.sub(r'[^\\w\\s]','',x)) #removes punctuation and spaces ,newline,tab\n    df.loc[:,'cleaned']=df['cleaned'].apply(lambda x: word_tokenize(x)) # split words \n    df.loc[:,'cleaned']=df['cleaned'].apply(lambda x: [word for word in x  if word not in stopwords]) #remove stopwords \n    df.loc[:,'cleaned']=df['cleaned'].apply(lambda x: \" \".join(x)) # join back ","4b2a7175":"preprocess(df=df_train)","bd6ff673":"df_train.head()","c7ba125c":"### Create folds \n\nfrom sklearn import model_selection\ndf_train['fold']=-1\n\nkf=model_selection.KFold(n_splits=3,shuffle=True, random_state=42)\n\nfor fold,(train_index,valid_index) in enumerate(kf.split(df_train['cleaned'])):\n    df_train.loc[valid_index,'fold']=fold","66bedff5":"##3 Let see if TF-IDF vectorizer with Ridge regrssion works well \n## Building the custom cross validation loop \n\ntrain_RMSE=[]\nval_RMSE=[]\ni=1\nfor fold in np.arange(df_train['fold'].nunique()):\n    \n    X_train,y_train= df_train[df_train['fold']!=fold]['cleaned'],df_train[df_train['fold']!=fold]['target']\n    X_val,y_val    = df_train[df_train['fold']==fold]['cleaned'],df_train[df_train['fold']==fold]['target']\n    \n    tfidf=TfidfVectorizer(lowercase=False,tokenizer=word_tokenize,ngram_range=(1,4),max_features=1000)\n    \n    tfidf.fit(X_train)\n    train_transform=tfidf.transform(X_train)\n    val_transform=tfidf.transform(X_val)\n    \n    lr=linear_model.Ridge()\n    lr.fit(train_transform,y_train)\n    \n    train_preds=lr.predict(train_transform)\n    \n    val_preds=lr.predict(val_transform)\n    \n    train_score =metrics.mean_squared_error(y_train,train_preds)\n    val_score   =metrics.mean_squared_error(y_val,val_preds)\n    \n    train_RMSE.append(train_score)\n    val_RMSE.append(val_score)\n    \n    \n    print(f\"*** FINISHED FOLD {i} Train_RMSE={train_score} and Valid_RMSE={val_score} ***\")\n    i=i+1","f07c10cb":"## Train a model for prediction \n\n\ntfidf=TfidfVectorizer(lowercase=False,tokenizer=word_tokenize,ngram_range=(1,4),max_features=1000)\n\ntransformed_text=tfidf.fit_transform(df_train['cleaned'])\n\nmodel=linear_model.Ridge()\nmodel.fit(transformed_text,df_train['target'])\n\n# save the model to disk\nfilename = 'Ridge_Regression_1000features.sav'\njoblib.dump(model, filename)\n\npickle.dump(tfidf, open(\"tfidf.pickle\", \"wb\"))","151989ca":"## Dump model for prediction","289cb031":"## CommonLit Readability Prize\n\n- *Motivation*\nIn this competition, you\u2019ll build algorithms to rate the complexity of reading passages for grade 3-12 classroom use. To accomplish this, you'll pair your machine learning skills with a dataset that includes readers from a wide variety of age groups and a large collection of texts taken from various domains. Winning models will be sure to incorporate text cohesion and semantics.\n\n- *Problem* \nSo this competition has given mainly text and we need to predict a difficulty score for the model . So this is a supervised regression problem. \n\n- *My Approach* \nAlways start simple algorithim first so I am going to implement a simple TF-IDF vectorizer for extracting features from the model and then fit it with a simple Ridge Regression . With this approach I was able to get 0.7 on the private leader board. \n\nAlso I have not optimized the model yet. \n\nBelow notbook documents my approach\n","7d29634e":"### Import the data"}}