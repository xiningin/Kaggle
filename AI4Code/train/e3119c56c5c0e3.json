{"cell_type":{"a9f768fa":"code","f87e8b95":"code","a51f2b4e":"code","be12bd4a":"code","ef96a16e":"code","e477e33a":"code","81ad2989":"code","e09f3ae0":"code","f5041b42":"code","6afbfa54":"code","627974e6":"code","da734d0d":"code","775f2123":"code","040ee116":"code","e08b0d00":"code","3799efad":"code","d6c4ae7f":"code","5267f214":"code","88c32afd":"code","e4bf4fe1":"code","21688b62":"code","14c642f8":"code","0d1a69bc":"code","6bc5afb4":"code","0972fc13":"code","99e12c34":"code","2ba6e9d9":"code","71620a9f":"code","c35617a5":"code","765def55":"code","db4d180d":"code","3d77d82a":"code","2bfa3d00":"markdown","5dec4d83":"markdown","57568a76":"markdown","52c040ba":"markdown","0e914036":"markdown","dfae8a97":"markdown","bd3c74f3":"markdown","1c597b8a":"markdown","226b9a27":"markdown","8c7b6db2":"markdown","6aa9b1b6":"markdown","17a7c666":"markdown","f2b61134":"markdown","530b1179":"markdown","276accb8":"markdown","7c9e4bf7":"markdown","6d915f1b":"markdown","7a586f77":"markdown","4a9c6485":"markdown","451e890e":"markdown","a169adf0":"markdown","97f55e63":"markdown","1da85e0c":"markdown","9c7dcae0":"markdown","8c6cf9fc":"markdown"},"source":{"a9f768fa":"import json\nimport requests\nimport time\nimport os\nimport pickle\nimport csv\nimport numpy as np","f87e8b95":"def read_prepared_specter_embeddings(csv_path):\n    with open(csv_path, newline='') as csvfile:\n        l = list(csv.reader(csvfile, delimiter=','))\n    # The entries have as their first value a unique id (hash),\n    # followed by the actual vector. Extract this into a dict\n    # for better access.\n    embeddingsDict = {}\n    uidList = []\n    for paperE in l:\n        cp = list(paperE)\n        uid = cp.pop(0)\n        uidList.append(uid)\n        newList = np.array([float(emb) for emb in cp])\n        embeddingsDict[uid] = newList\n    return uidList, embeddingsDict","a51f2b4e":"uidList, cord19_dict = read_prepared_specter_embeddings(\n    '\/kaggle\/input\/CORD-19-research-challenge\/cord19_specter_embeddings_2020-04-10\/cord19_specter_embeddings_2020-04-10.csv')","be12bd4a":"myGuardianAPIkey = None\nGuardianArticlesStoragePath = '\/kaggle\/input\/guardian-covid19\/Guardian_Covid-19' # TODO: absolute kaggle path","ef96a16e":"def grabArticles(nArticles, sectionRestr = None):\n    for page in range(nArticles\/\/10):\n        searchTerm = 'Covid-19'\n        requestParams = { 'q': searchTerm \n                        , 'show-fields': 'bodyText'\n                        , 'page': page+1\n                        , 'api-key': myGuardianAPIkey\n                        }\n        if sectionRestr is not None:\n            requestParams['section'] = sectionRestr\n        relevantArticlesResp = requests.get(\n                    'https:\/\/content.guardianapis.com\/search'\n                  , requestParams ).json()\n            \n        for entry in relevantArticlesResp['response']['results']:\n            fname = '_'.join(entry['webUrl'].split('\/')[3:]) + '.json'\n            textContent = entry['fields']['bodyText']\n            # Pack the content into a dictionary structure compatible\n            # for the SemanticScholar encoder. We use the body text as\n            # both the article text and the abstract: arguably, a news\n            # article is already more like the abstract of a scientific\n            # article, since it is written in a higher-level style\n            # suitable for non-experts.\n            jsonFormatted = {\n                     'url': entry['webUrl']\n                   , 'title': entry['webTitle']\n                   , 'abstract': textContent\n                   , 'body_text': textContent\n                   }\n            with open(GuardianArticlesStoragePath+'\/'+fname, 'w') as fh:\n                json.dump(jsonFormatted, fh)\n        time.sleep(10) # avoid too fast requests to API","e477e33a":"# grabArticles(150)            # Uncomment to retrive articles anew\n# grabArticles(50, 'science')","81ad2989":"URL = \"https:\/\/model-apis.semanticscholar.org\/specter\/v1\/invoke\"\nMAX_BATCH_SIZE = 16","e09f3ae0":"# Source: https:\/\/github.com\/allenai\/paper-embedding-public-apis\ndef chunks(lst, chunk_size=MAX_BATCH_SIZE):\n    \"\"\"Splits a longer list to respect batch size\"\"\"\n    for i in range(0, len(lst), chunk_size):\n        yield lst[i : i + chunk_size]\n        \ndef embed(papers):\n    embeddings_by_paper_id: Dict[str, List[float]] = {}\n\n    for chunk in chunks(papers):\n        # Allow Python requests to convert the data above to JSON\n        response = requests.post(specterURL, json=chunk)\n\n        if response.status_code != 200:\n            raise RuntimeError(\"Sorry, something went wrong, please try later!\")\n\n        for paper in response.json()[\"preds\"]:\n            embeddings_by_paper_id[paper[\"paper_id\"]] = paper[\"embedding\"]\n\n    return embeddings_by_paper_id\n\ndef loadJsonPaper(fp):\n    paperId = os.path.basename(fp).split('.')[0]\n    with open(fp, 'r') as fh:\n        content = json.load(fh)\n    content['paper_id'] = paperId\n    return content","f5041b42":"relevantNewsArticleIds = ['world_2020_mar_16_health-experts-criticise-nhs-advice-to-take-ibuprofen-for-covid-19',\n                          'science_2020_mar_25_can-chloroquine-really-help-treat-coronavirus-patients',\n                          'science_2020_feb_20_doctors-hiv-ebola-drugs-coronavirus-cure-covid-19',\n                          'sport_2020_mar_26_ecb-steve-elworthy-cricket-coronavirus',\n                          'world_2020_mar_30_fall-in-covid-19-tests-putting-lives-at-risk-critics-claim',\n                          'world_2020_mar_29_ventilator-challenge-uk-to-start-production-in-covid-19-fight']","6afbfa54":"# We have already saved the embeddings previously. Set this to False\n# to request them anew.\nloadPrecomputedNewsEmbeddings = True\n\ndef getRelevantNewsEmbeddings():\n    embeddingDir = GuardianArticlesStoragePath+'_specter'\n    if not loadPrecomputedNewsEmbeddings:\n        os.makedirs(embeddingDir, exist_ok=True)\n\n    if loadPrecomputedNewsEmbeddings:\n        articleDict = {}\n        for articleId in relevantNewsArticleIds:\n            articleDict[articleId] = pickle.load(\n              open(embeddingDir+'\/'+articleId, 'rb'))\n        return articleDict\n    else:\n        relevantNewsContent = [\n         loadJsonPaper(\"Guardian_Covid-19\/\"+articleId+\".json\")\n            for articleId in relevantNewsArticleIds ]\n        embedded = embed(relevantNewsContent)\n        for articleId, embd in embedded.items():\n            fp = embeddingDir+'\/'+articleId\n            pickle.dump(embd, open(fp, 'wb'))\n        return embedded\n\ndef getRelevantNewsEmbedIDarray():\n    embeddingDir = GuardianArticlesStoragePath+'_specter'\n    if not loadPrecomputedNewsEmbeddings:\n        os.makedirs(embeddingDir, exist_ok=True)\n    if loadPrecomputedNewsEmbeddings:\n        articleArray = np.zeros((len(relevantNewsArticleIds),768))\n        IdList = []\n        for i, articleId in enumerate(relevantNewsArticleIds):\n            articleArray[i] = pickle.load(\n              open(embeddingDir+'\/'+articleId, 'rb'))\n            IdList.append(articleId)\n        return IdList, articleArray","627974e6":"from sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE\nfrom matplotlib import pyplot as plt","da734d0d":"def pcadimred(data):\n    pca = PCA(n_components=0.8)\n    return pca.fit_transform(data)","775f2123":"cord19_full = np.zeros((len(uidList),len(cord19_dict[uidList[0]])))\nfor i,uid in enumerate(uidList):\n    cord19_full[i,:] = cord19_dict[uid]","040ee116":"newsidList, news_embeddings = getRelevantNewsEmbedIDarray()","e08b0d00":"cord19_news_full = np.vstack((cord19_full,news_embeddings))\nprint('Original dimensions: ', np.shape(cord19_news_full))\ncord19_news_red = pcadimred(cord19_news_full)\nprint('Reduced dimensions using PCA: ', np.shape(cord19_news_red))","3799efad":"def tsnedimred(data):\n    tsne = TSNE(verbose=1)\n    return tsne.fit_transform(data)","d6c4ae7f":"# Uncomment to calculate again (takes a few minutes)\n#cord19_news_2d = tsnedimred(cord19_news_red)","5267f214":"# Saving and loading cord19_news_2d to disk\ncord19_news_2d = np.load('\/kaggle\/input\/covidtemp\/cord19_news_2d.npy')\n#cord19_news_2d = np.load('\/kaggle\/working\/cord19_news_2d.npy')\nnp.save('\/kaggle\/working\/cord19_news_2d', cord19_news_2d)","88c32afd":"cord19_2d = (cord19_news_2d[:-len(newsidList),0], cord19_news_2d[:-len(newsidList),1])\nnews_2d = (cord19_news_2d[len(uidList):,0], cord19_news_2d[len(uidList):,1])\ndata = (cord19_2d, news_2d)\ncolors = (\"blue\", \"red\")\ngroups = (\"CORD-19\", \"News articles\")\nsize = (1,30)\n\n# Create plot\nfig = plt.figure(figsize=(15, 15))\nax = fig.add_subplot(1, 1, 1, facecolor=\"1.0\")\n\nfor data, color, group, size in zip(data, colors, groups, size):\n    x, y = data\n    ax.scatter(x, y, alpha=0.8, c=color, edgecolors='none', s=size, label=group)\n\nplt.title('t-SNE of CORD-19 and 6 news articles')\nplt.legend(loc=2)\nplt.savefig(\"\/kaggle\/working\/t-sne_cord19_news.png\")\nplt.show()","e4bf4fe1":"cord19_2d = (cord19_news_2d[:-len(newsidList),0], cord19_news_2d[:-len(newsidList),1])\nnews_2d = np.asarray((cord19_news_2d[len(uidList):,0], cord19_news_2d[len(uidList):,1]))\n\ndata = (cord19_2d, news_2d[:,0], news_2d[:,1], news_2d[:,2], news_2d[:,3], news_2d[:,4], news_2d[:,5])\ncolors = (\"blue\", \"red\", \"green\", \"black\", \"yellow\", \"cyan\", \"orange\")\ngroups = (\"CORD-19\", newsidList[0], newsidList[1], newsidList[2], newsidList[3], newsidList[4], newsidList[5])\nsize = (1,30,30,30,30,30,30)\n\n# Create plot\nfig = plt.figure(figsize=(15, 15))\nax = fig.add_subplot(1, 1, 1, facecolor=\"1.0\")\n\nfor data, color, group, size in zip(data, colors, groups, size):\n    x, y = data\n    ax.scatter(x, y, alpha=0.8, c=color, edgecolors='none', s=size, label=group)\n\nplt.title('t-SNE of CORD-19 and 6 news articles')\nplt.legend(loc='lower right')\nplt.savefig(\"\/kaggle\/working\/t-sne_cord19_news_withlabels.png\")\nplt.show()","21688b62":"from sklearn.metrics.pairwise import cosine_similarity\nimport pandas as pd\nimport pprint","14c642f8":"# Hack to force abstract column to be wider\nabstract_long_name = '_________________abstract_of_research_paper_________________'\n\ndef read_metadata(metadata_path):\n    meta_df = pd.read_csv(metadata_path, dtype={\n        'pubmed_id': str,\n        'Microsoft Academic Paper ID': str, \n        'doi': str\n    })\n    # Hack to force abstract column to be wider\n    meta_df.rename(\n        columns = {'abstract': abstract_long_name}, \n        inplace = True)\n    return meta_df\nmetadata = read_metadata('\/kaggle\/input\/CORD-19-research-challenge\/metadata.csv')","0d1a69bc":"def get_cosine_similarity(feature_vec_1, feature_vec_2):\n    \"\"\"https:\/\/medium.com\/@Intellica.AI\/comparison-of-different-word-embeddings-on-text-similarity-a-use-case-in-nlp-e83e08469c1c\"\"\"\n    return cosine_similarity(feature_vec_1.reshape(1, -1), feature_vec_2.reshape(1, -1))[0][0]\n\ndef get_cosine_similarities(cord_papers_embeddings, news_embedding):\n    d = {}\n    for uid, paper_embedding in cord_papers_embeddings.items():\n        d[uid] = get_cosine_similarity(news_embedding, paper_embedding)\n    return d\n\ndef add_cosine_sim_to_metadata_df(metadata_df, similarities_dict):\n    metadata_df_c = metadata_df.copy() # Deep copy\n    metadata_df_c['cosine_similarity'] = metadata_df_c['cord_uid'].map(similarities_dict)\n    return metadata_df_c.sort_values(by='cosine_similarity', ascending=False)","6bc5afb4":"import textwrap\n\ndef make_clickable(val):\n    # target _blank to open new window\n    return '<a target=\"_blank\" href=\"{}\">URL<\/a>'.format(val)\n\ndef change_abstract_name(val):\n    cell_length = 800\n    val = str(val)\n    if len(val) > cell_length:\n        val = textwrap.wrap(val, cell_length)[0] + '...'\n    return val\n\ndef url_style(metadata_df):\n    return metadata_df.style.format({'url': make_clickable, \n                                     abstract_long_name: change_abstract_name})\n\ncols_to_display = ['cosine_similarity', 'publish_time', 'url', 'title', \n                   abstract_long_name, 'authors', 'journal', 'cord_uid']\n\ndef show_k_most_similar_papers(metadata_df, k):\n    return url_style(metadata_df.head(k).loc[:, cols_to_display])\n\ndef show_k_least_similar_papers(metadata_df, k):\n    return url_style(metadata_df.tail(k).loc[:, cols_to_display])","0972fc13":"news_embeddings = getRelevantNewsEmbeddings()\npp = pprint.PrettyPrinter()\npp.pprint(list(news_embeddings.keys()))","99e12c34":"ibuprofen_embedding = np.array(news_embeddings['world_2020_mar_16_health-experts-criticise-nhs-advice-to-take-ibuprofen-for-covid-19'])\nibuprofen_similarity = add_cosine_sim_to_metadata_df(metadata, get_cosine_similarities(cord19_dict, ibuprofen_embedding))","2ba6e9d9":"show_k_most_similar_papers(ibuprofen_similarity, 10)","71620a9f":"show_k_least_similar_papers(ibuprofen_similarity, 5)","c35617a5":"cricket_embedding = np.array(news_embeddings['sport_2020_mar_26_ecb-steve-elworthy-cricket-coronavirus'])\ncricket_similarity = add_cosine_sim_to_metadata_df(metadata, get_cosine_similarities(cord19_dict, cricket_embedding))","765def55":"show_k_most_similar_papers(cricket_similarity, 10)","db4d180d":"ebola_embedding = np.array(news_embeddings['science_2020_feb_20_doctors-hiv-ebola-drugs-coronavirus-cure-covid-19'])\nebola_similarity = add_cosine_sim_to_metadata_df(metadata, get_cosine_similarities(cord19_dict, ebola_embedding))","3d77d82a":"show_k_most_similar_papers(ebola_similarity, 10)","2bfa3d00":"## Cosine similarity functions\n\nCosine similarity is used as a measure of distance between two vector embeddings.\n\n\\begin{equation}\n\\text{similarity} = \\cos(\\theta) = {\\mathbf{A} \\cdot \\mathbf{B} \\over \\|\\mathbf{A}\\| \\|\\mathbf{B}\\|} = \\frac{ \\sum\\limits_{i=1}^{n}{A_i  B_i} }{ \\sqrt{\\sum\\limits_{i=1}^{n}{A_i^2}}  \\sqrt{\\sum\\limits_{i=1}^{n}{B_i^2}} }\n\\end{equation}\n\nhttps:\/\/en.wikipedia.org\/wiki\/Cosine_similarity","5dec4d83":"### The Ibuprofen article - 10 most similar research papers\n\nThe list below is ranked by the cosine similarity between the Ibuprofen news article and the research papers in question.\n\nWhat do we see? The Guardian story is about concern in the UK that the NHS was advicing to take ibuprofen. Our most similar research paper seems to nail this topic pretty well. It is an article about NSAIDs and its safety in the treatment of COVID-19 patients. Further down the list results are not that clear. Some research papers are about UK health service, but does not seem to mention ibuprofen or NSAIDs in particular.","57568a76":" We retrieved a collection of articles as top-hits for the `Covid-19` search term. Most of the articles are in the section `world`, but of particular interest are those from the `science` section so we also request those separately.","52c040ba":"## Imports","0e914036":"# COVID-19 News Matcher\n\n## *Matching news articles with COVID-19 research papers*\n\nThe topic of this submission is to demonstrate the matching of newspaper articles with research papers of the COVID-19 Open Research Dataset Challenge (CORD-19). This would allow to search for research papers that are related to a given newspaper article. In this way, we want to contribute to knowledge transfer of science and facilitate to gather relevant research insights that are connected to topics discussed in the media and facilitate the access of this expert knowledge to the public and decision makers.\n\nOur work investigates how far such a matching can be accomplished using only a common, standardised vector embedding \u2013 specifically, using an embedding that is optimised for scientific articles also for newspaper articles. One purpose of this approach is to see whether the scientific classification generalises to news, which would indicate if it is also generalisable to other non-science domains.\n\nWe use the SPECTER embeddings of the abstracts of COVID-19 related research papers that are available from CORD-19. Then we use the public SPECTER api to get embeddings for newspaper articles of \"The Guardian\". We assume that the writing style of the abstracts is somewhat similar to that of a newspaper article. Based on the embeddings, we can compute cosine similarities and find the most similar research papers based on their abstracts for a given newspaper article.\n\nReaders are encouraged to test the notebook with more news articles, or other texts from different domains.\n\nCredits: \n* https:\/\/github.com\/allenai\/paper-embedding-public-apis (https:\/\/www.semanticscholar.org\/)\n* https:\/\/medium.com\/@Intellica.AI\/comparison-of-different-word-embeddings-on-text-similarity-a-use-case-in-nlp-e83e08469c1c\n* https:\/\/www.kaggle.com\/maksimeren\/covid-19-literature-clustering \n* https:\/\/www.theguardian.com\/","dfae8a97":"# 3. Dimensionality reduction and visual interpretation\n\nNow that we have the embedding vectors of the research papers and news articles, we are interested in how they relate to each other. One way to investigate this is to reduce the dimensionality so far that we can actually create 2d plots. First, we use principal component analysis (PCA) to reduce dimensions from 768 to 87, and then use t-SNE to reduce dimension from 87 to 2. The reason for the more primitive PCA step is that doing t-SNE (T-distributed Stochastic Neighbor Embedding) from 768 to 2 directly is computational slow. Finally we plot the embeddings.\n\nCredits: https:\/\/www.kaggle.com\/maksimeren\/covid-19-literature-clustering ","bd3c74f3":"We see that the news articles all are located in the same region of the scatterplot. This suggests that they have some similarity between each other and don't cover the broadness in terminology and topics of the research article abstracts. The individual closeness of news article embeddings in the 2D space is hard to interpret. We conclude that the analysis of the relation between research abstracts and news articles based on embeddings is worth being studied in more detail.","1c597b8a":"### Visualisation using t-SNE","226b9a27":"### The cricket article - 10 most similar research papers\n\nThe list below is ranked by the cosine similarity between the cricket news article and the research papers in question.\n\nWhat about these matches? The Guardian article is about cancellation of cricket games and how\/if future games can be arranged during the pandemic. Our best match is about events being cancelled. Not exactly sports as this seem to be related to *chemical events*. Further down the list the results are somewhat mixed. Not all the results seem to be related to events.","8c7b6db2":"## Anecdotal testing with a few select news articles\n\n* Ebola article: https:\/\/www.theguardian.com\/science\/2020\/feb\/20\/doctors-hiv-ebola-drugs-coronavirus-cure-covid-19\n* Cricket article: https:\/\/www.theguardian.com\/sport\/2020\/mar\/26\/ecb-steve-elworthy-cricket-coronavirus\n* Ibuprofen article: https:\/\/www.theguardian.com\/world\/2020\/mar\/16\/health-experts-criticise-nhs-advice-to-take-ibuprofen-for-covid-19","6aa9b1b6":"### Dimensionality reduction using PCA","17a7c666":"## Adding similarity scores to the metadata dataframe","f2b61134":"## The Ebola article \n\nEbola article: https:\/\/www.theguardian.com\/science\/2020\/feb\/20\/doctors-hiv-ebola-drugs-coronavirus-cure-covid-19","530b1179":"# 2. SPECTER embedding of news articles\nFor comparison, we use articles from the Guardian newspaper and encode them into the vector embedding using SemanticScholar's API service. First the articles themselves need to be fetched.","276accb8":"## The Ibuprofen article\nIbuprofen article: https:\/\/www.theguardian.com\/world\/2020\/mar\/16\/health-experts-criticise-nhs-advice-to-take-ibuprofen-for-covid-19","7c9e4bf7":"# 1. SPECTER embeddings of research papers\n\nOur analysis is based not on the article plaintext, but on vector embeddings of entire article abstracts. While this loses a lot of information, it also allows us to tap into a simplification that has been extensively studied and optimised for other work.\n\nSPECTER is one such embedding format. In the CORD19 datasets, this embedding is already available in a single csv file, so for the scientific papers we can simply load it. (We also use this format for newspaper articles, see further below.)","6d915f1b":"## Reading metadata.csv","7a586f77":"### The Ibuprofen article - 5 least similar\n\nAs as sanity check we also show the least similar articles. After some inspection we noticed that the least similar were usually research papers with no abstracts. That would make sense since its embedding could be considered corrupt or non-existent. In a future revision of the code a `dropna()` function should be implemented to drop papers without abstracts from matching.\n\nIn the next article examples we will not show the least similar matches.","4a9c6485":"# 5. Further work\n\nIn this report we have shown promising results, however limited and anecdotal, of using SPECTER embeddings for news articles and finding similarities between news and research papers. Limitations: English language only, only one news source (The Guardian), only a few articles and no real quantitative measurement of the results.\n\nWe want to further optimize the embeddings and maybe account try to adjust for the different domains (news vs. research). It would also be interesting to investigate whether cosine similarity is the best measure of similarity for this problem. \n\nFinally we are also considering creating an online tool to help internet researchers fact check news articles with published research. Input news article URL, output list of research papers most similar to the news article. Any feedback on the need for, as well as the features of, such a tool is much appreciated.","451e890e":"### Vector embedding\nTo get the news articles into a compatible vector format to the scientific ones, we use a SemanticScholar service, which takes the JSON encoding we created in `grabArticles` and returns the desired vector embedding.","a169adf0":"## The cricket article \n\nCricket article: https:\/\/www.theguardian.com\/sport\/2020\/mar\/26\/ecb-steve-elworthy-cricket-coronavirus","97f55e63":"### The Ebola article - 10 most similar research papers\n\nThe list below is ranked by the cosine similarity between the Ebola news article and the research papers in question.\n\nThe Guardian article is about how medical professionals are investigating whether medications for other diseases, like Ebola and HIV, are effective against the current pandemic. In general the research papers seem to match very well to this topic.","1da85e0c":"### Embedding of 6 articles\nWe use for comparison a manual selection of 6 articles from the retrieved Guardian corpus. The articles have been chosen both from science-near and more trivial contexts.\n\n* Ebola article: https:\/\/www.theguardian.com\/science\/2020\/feb\/20\/doctors-hiv-ebola-drugs-coronavirus-cure-covid-19\n* Chloroquine article: https:\/\/www.theguardian.com\/science\/2020\/mar\/25\/can-chloroquine-really-help-treat-coronavirus-patients\n* Cricket article: https:\/\/www.theguardian.com\/sport\/2020\/mar\/26\/ecb-steve-elworthy-cricket-coronavirus\n* Ventilator article: https:\/\/www.theguardian.com\/world\/2020\/mar\/29\/ventilator-challenge-uk-to-start-production-in-covid-19-fight\n* Ibuprofen article: https:\/\/www.theguardian.com\/world\/2020\/mar\/16\/health-experts-criticise-nhs-advice-to-take-ibuprofen-for-covid-19\n* Testing article: https:\/\/www.theguardian.com\/world\/2020\/mar\/30\/fall-in-covid-19-tests-putting-lives-at-risk-critics-claim","9c7dcae0":"# 4 News article matching\n\nWe are approaching the central question behind this notebook: *Can we use the vector embedding of a news article to find similar research papers in the CORD-19 database?*\n\nThrough the t-SNE analysis in the previous section we saw that the news articles embeddings \"reside amongst the research papers\" and are not clustered too strongly in their own news cluster. While this is a good sign it will be interesting to see the, for instance, 10 most similar research papers to a given news article. We will do this here through a completely anecdotal study of a couple of the Guardian articles.","8c6cf9fc":"### Guardian API\nRequesting from the Guardian API requires registering as a developer. After registration, one can use the key for retrieving the articles."}}