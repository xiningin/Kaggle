{"cell_type":{"4ccfdd95":"code","46a00fb9":"code","8a4522b3":"code","67183a52":"code","9bf3ed61":"code","977bc702":"code","5159da1b":"code","cd6a83ae":"code","8d5feeb3":"code","03ba9862":"code","837624f3":"code","6d3042be":"code","7040fede":"code","e2c8e993":"code","cd789e28":"code","ddd78acb":"code","8fe7a78e":"code","541368cc":"code","ffae3bb0":"code","0019bb7c":"code","d5a5d9ae":"code","b8b80cc4":"code","4bbce583":"code","1a5a3e8a":"code","f1a8f9ca":"code","1b1e3d5b":"code","e8b06d41":"code","76aa816f":"code","50403a52":"code","1ba0fe5d":"code","0370e299":"code","707662ad":"code","d3b6d465":"code","31302d10":"code","a4041ae7":"code","3c7ab6cf":"code","531c02a0":"code","47e6c837":"code","fdba8dfc":"code","05da6847":"code","a62bad94":"markdown","7473a50e":"markdown","b565f3c8":"markdown","6e11a541":"markdown","334fc2ac":"markdown","2cad617e":"markdown","7c534927":"markdown","985cd7aa":"markdown","67252787":"markdown","7289d2e0":"markdown","3e8659a2":"markdown","b3417e1b":"markdown","bd642830":"markdown","080d72ea":"markdown","7ac75704":"markdown","5290df33":"markdown","f859a690":"markdown","fc8a98bf":"markdown","77fcbaaf":"markdown","f3040493":"markdown","f721c2a1":"markdown","306f58f8":"markdown","041ffd85":"markdown","862166cc":"markdown","83a42764":"markdown","2bde57f9":"markdown","a48531cd":"markdown","cdd8962d":"markdown","003f0965":"markdown","7ed248a7":"markdown","5ef10a1f":"markdown","896246af":"markdown","8dd457ad":"markdown","2dfa2d7d":"markdown","69be7ea1":"markdown","ad869b4b":"markdown","502322d8":"markdown","c50b92f3":"markdown","9df9c66d":"markdown","e55b849d":"markdown"},"source":{"4ccfdd95":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport statsmodels.api as sm\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import r2_score\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nfrom statsmodels.stats.stattools import durbin_watson\nfrom statsmodels.graphics.tsaplots import plot_acf , plot_pacf\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor","46a00fb9":"df = pd.read_csv('..\/input\/insurance\/insurance.csv')\ndf.head()","8a4522b3":"df.describe()","67183a52":"sns.distplot(df['charges']);","9bf3ed61":"print(\"Skewness: %f\" % df['charges'].skew())\nprint(\"Kurtosis: %f\" % df['charges'].kurt())","977bc702":"sns.distplot(df['bmi'])","5159da1b":"sns.distplot(df['age'])","cd6a83ae":"# Smoker\nplt.figure(figsize = (8,8))\nplt.subplot(2,2,1)\nlabels = df.smoker.value_counts().index\ncolors = ['green','red']\nexplode = [0,.1]\nsizes = df.smoker.value_counts().values\nplt.pie(sizes, explode=explode, shadow=False,labels=labels, colors=colors, autopct='%1.1f%%')\nplt.title('Smoker',fontsize = 15)\nplt.legend(labels,bbox_to_anchor=(0.8, -0.04, 0.4, 1))\n\n# Sex\nplt.subplot(2,2,2)\nlabel = df.sex.value_counts().index\nsize = df.sex.value_counts().values\ncolor = ['teal','pink']\nplt.pie(size, explode=explode, shadow=False,labels=label, colors=color, autopct='%1.1f%%')\nplt.title('Sex',fontsize = 15)\nplt.legend(label,bbox_to_anchor=(0.8, -0.04, 0.4, 1))\nplt.show()","8d5feeb3":"sns.swarmplot(x=\"smoker\", y=\"charges\",hue=\"sex\", data=df)\nplt.show()","03ba9862":"sns.countplot(x = 'sex', hue = 'smoker', data = df)","837624f3":"sns.countplot(x = 'region', hue = 'smoker', data = df)","6d3042be":"sns.countplot(x = 'children', hue = 'smoker', data = df)","7040fede":"features = ['sex', 'smoker', 'children','region']\nfor var in features:\n    sns.countplot(df[var])\n    plt.show()","e2c8e993":"f, ax = plt.subplots(figsize=(16, 8))\nfig = sns.boxplot(x='age', y=\"charges\", hue='smoker', data=df)\nplt.xticks(rotation=90);","cd789e28":"# age < 20\nsns.boxplot(x = 'smoker', y = 'charges', data = df[(df.age < 20)])","ddd78acb":"# bmi > 30\nsns.boxplot(x = 'sex', y = 'charges', data = df[(df.bmi > 30)])","8fe7a78e":"# bmi <= 30\nsns.boxplot(x = 'sex', y = 'charges', data = df[(df.bmi <= 30)])","541368cc":"df.sort_values(by=\"charges\", ascending= False).head(10).style.background_gradient(cmap=\"spring\")","ffae3bb0":"plt.figure(figsize = (10,10))\nsns.heatmap(df.corr(), annot=True)","0019bb7c":"from sklearn.preprocessing import LabelEncoder\nfeatures = ['sex','smoker','region']\nfor feature in features:    \n    lab = LabelEncoder()\n    lab.fit(df[feature]) \n    df[feature] = lab.transform(df[feature])","d5a5d9ae":"X = df.iloc[:,:-1]\ny = df.iloc[:,-1]","b8b80cc4":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 42)","4bbce583":"from sklearn.ensemble import RandomForestRegressor\nreg = LinearRegression()\nreg.fit(X_train, y_train)","1a5a3e8a":"y_pred = reg.predict(X_test)","f1a8f9ca":"sns.pairplot(df, x_vars=['age','bmi'], y_vars='charges', size=5, aspect=0.7)","1b1e3d5b":"residuals = y_test - y_pred\nmean_residuals = np.mean(residuals)\nprint(\"Mean of Residuals {}\".format(mean_residuals))","e8b06d41":"# seaborn residual plot\nsns.residplot(y_pred, residuals, lowess=True,scatter_kws={'alpha': 0.5},line_kws={'color': 'red', 'lw': 1, 'alpha': 0.8})\nplt.title('Residual plot')\nplt.xlabel('Predicted values')\nplt.ylabel('Residuals');","76aa816f":"import statsmodels.stats.api as sms\nfrom statsmodels.compat import lzip\nnames = ['Lagrange multiplier statistic', 'p-value',\n        'f-value', 'f p-value']\ntest = sms.het_breuschpagan(residuals, X_test)\nlzip(names, test)","50403a52":"#### Usind distplot\np = sns.distplot(residuals, kde=True)\np = plt.title('Normality of error terms')","1ba0fe5d":"sm.qqplot(residuals, line ='r')\nplt.show()","0370e299":"# Plot the autocorrelation with 0.05 significance level\nplot_acf(residuals, alpha = 0.05, lags=40)\nplt.show() ","707662ad":"# Plot the partial autocorrelation \n# 0.05 significance level\nplot_pacf(residuals, alpha = 0.05, lags=40)\nplt.show()","d3b6d465":"# Using statsmodels.durbin_watson() method\ndw = durbin_watson(residuals)\ndw ","31302d10":"# Calculating VIF\nvif = pd.DataFrame()\nvif[\"variables\"] = X.columns\nvif[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif","a4041ae7":"# R Square\nr2_score(y_pred, y_test)","3c7ab6cf":"# Mean Absolute Error\nmean_absolute_error(y_pred, y_test)","531c02a0":"# Mean Squared Error\nmean_squared_error(y_pred, y_test)","47e6c837":"x = sm.add_constant(X_train)\nmodel = sm.OLS(y_train,x)","fdba8dfc":"res = model.fit()\nres.params","05da6847":"res.summary()","a62bad94":"#### Breusch-Pagan\n\nThis test is used to determine presence of heteroskedasticity. If you find p < 0.05, you reject the null hypothesis and infer that heteroskedasticity is present.","7473a50e":"####  Top High Medical charges (all are smokers)","b565f3c8":"##  <a id='assumption'>Assumptions for Linear Regression<\/a>","6e11a541":"#### Linear Model","334fc2ac":"### Checking for Autocorrelation","2cad617e":"#### Pie Chart","7c534927":"#### skewness and kurtosis","985cd7aa":"## <a id='evaluation'>Model Evaluation<\/a> ","67252787":"#### Distplot","7289d2e0":"### Normal Distribution of error terms","3e8659a2":"## <a id='visualize'>Data Visualization<\/a>","b3417e1b":"#### Linearity","bd642830":"### Checking for Multicollinearity\n\n#### Variance Inflation Factor (VIF) \n\nThis metric is used to check multicollinearity. VIF <=4 implies no multicollinearity but VIF >=10 suggests high multicollinearity. Alternatively, you can also look at the tolerance (1\/VIF) value to determine correlation in IVs. In addition, you can also create a correlation matrix to determine collinear variables.","080d72ea":"#### Countplot","7ac75704":"#### Statistical Description","5290df33":"#### Import Librabries","f859a690":"#### Swarmplot","fc8a98bf":"### Checking for Homoscedasticity\n\nResidual vs. Fitted Values Plot\n\nIdeally, this plot shouldn't show any pattern. But if you see any shape (curve, U shape), it suggests non-linearity in the data set. In addition, if you see a funnel shape pattern, it suggests your data is suffering from heteroskedasticity, i.e. the error terms have non-constant variance.","77fcbaaf":"#### Table of Content\n\n##### <a href='#visualize'>Data Visualization <\/a> \n\n##### <a href='#preprocess'>Data Preprocessing <\/a> \n\n##### <a href='#creation'>Model Creation <\/a> \n\n##### <a href='#assumption'>Assumption of Linear Regression <\/a> \n\n##### <a href='#evaluation'>Model Evaluation <\/a> \n\n##### <a href='#ols'>OLS Method in Detail  <\/a> ","f3040493":"#### Here the overall p value of the model is significant that is its less than the alpha value 0.05 and F-statistics have higher value. ","f721c2a1":"## <a id='ols'>OLS Method<\/a> \n\nOLS technique tries to reduce the sum of squared errors \u2211[Actual(y) - Predicted(y')]\u00b2 by finding the best possible value of regression coefficients (\u03b20, \u03b21, etc).","306f58f8":"### Linear Regression with Assumptions and OLS Method","041ffd85":"#### Distribution of bmi","862166cc":"This notebook explain about the assumptions of linear regression and ols method in detail. One of the most essential steps to take before applying linear regression.","83a42764":"#### Durbin Watson Statistic (DW) Test \n\nThis test is used to check autocorrelation. Its value lies between 0 and 4. A DW=2 value shows no autocorrelation. However, a value between 0 to 2 implies positive autocorrelation, while 2 to 4 implies negative autocorrelation.\n","2bde57f9":"#### Boxplot","a48531cd":"#### Read data","cdd8962d":"## <a id='creation'>Model Creation<\/a>","003f0965":"#### Mean of Residual","7ed248a7":"#### Independent and Dependent Variable","5ef10a1f":"#### Heatmap","896246af":"## <a id='preprocess'>Data Preprocessing<\/a>","8dd457ad":"#### Distribution of age","2dfa2d7d":"![image.png](attachment:image.png)","69be7ea1":"#### Normality Q-Q Plot\n\nAs the name suggests, this plot is used to determine the normal distribution of errors. It uses standardized values of residuals. Ideally, this plot should show a straight line. If you find a curved, distorted line, then your residuals have a non-normal distribution (problematic situation).","ad869b4b":"<b>F Statistics<\/b> - It evaluates the overall significance of the model. It compares the full model with an intercept only (no predictors) model. Its value can range between zero and any arbitrary large number. Naturally, higher the F statistics, better the model.\n\n<b>Intercept<\/b> - This is the \u03b2o value. It's the prediction made by model when all the independent variables are set to zero.\n\n<b>coef<\/b> - This represents regression coefficients for respective variables. It's the value of slope. Let's interpret it for age. We can say, when age is increased by 1 unit, holding other variables constant, charges increase by a value of 261.9106.\n\n<b>Std. Error<\/b> - This determines the level of variability associated with the estimates. Smaller the standard error of an estimate is, more accurate will be the predictions.\n\n<b>t value<\/b> - t statistic is generally used to determine variable significance, i.e. if a variable is significantly adding information to the model. t value > 2 suggests the variable is significant. I used it as an optional value as the same information can be extracted from the p value.\n\n<b>p value<\/b> - It's the probability value of respective variables determining their significance in the model. p value < 0.05 is always desirable.\n\n<b>Durbin Watson Test<\/b> - This test is used to check autocorrelation. Its value lies between 0 and 4. A DW = 2 value shows no autocorrelation. However, a value between 0 to 2 implies positive autocorrelation, while 2 to 4 implies negative autocorrelation.\n\n<b>Jarque-Bera Test<\/b> - The Jarque-Bera Test, a type of Lagrange multiplier test, is a test for normality. Normality is one of the assumptions for many statistical tests, like the t test or F test; the Jarque-Bera test is usually run before one of these tests to confirm normality.\nIn general, a large J-B value indicates that errors are not normally distributed.\n\n<b>Skew<\/b> - A normal distribution has a skew of zero (i.e. it\u2019s perfectly symmetrical around the mean)\n\n<b>Kurtosis<\/b> -  A normal distribution has kurtosis of three, kurtosis tells you how much data is in the tails and gives you an idea about how \u201cpeaked\u201d the distribution is.\n\n<b>Akaike Information Criteria (AIC)<\/b> - You can look at AIC as counterpart of adjusted r square in multiple regression. It's an important indicator of model fit. It follows the rule: Smaller the better. AIC penalizes increasing number of coefficients in the model. In other words, adding more variables to the model wouldn't let AIC increase. It helps to avoid overfitting.\n\nLooking at the AIC metric of one model wouldn't really help. It is more useful in comparing models (model selection). So, build 2 or 3 models and compare their AIC. The model with the lowest AIC will be relatively better.","502322d8":"**Thank you very much for your attention to my work. if you liked my work please upvote :)**","c50b92f3":"#### Target Variable","9df9c66d":"signs of autocorelation since there are spikes outside the blue confidence interval region.","e55b849d":"#### Split into train and test"}}