{"cell_type":{"26c6f4eb":"code","7955261a":"code","b5641f02":"code","f164d252":"code","4e7caf85":"code","457e9e77":"code","dc73eae4":"code","9c06ff35":"code","5e7433ac":"code","ed3265ab":"code","6af2bfd8":"code","a8120394":"code","f146fb7a":"code","6012b052":"code","7e540e9a":"code","693cce9a":"code","5cde0dac":"code","0560812d":"code","3fc4fed5":"code","0f1318e1":"code","8cfb0b2f":"code","a5264c7c":"code","048f7a5b":"code","248c358a":"code","7ec8e16f":"code","c57f3911":"code","9503e692":"code","68b93537":"code","8150e89d":"code","de8175ad":"code","283d118a":"code","25912f89":"code","f053b47e":"code","bde21199":"code","8e6e7e0e":"code","1989557a":"code","3c83d4f3":"code","cadd7a08":"code","14eeac7a":"code","e830c935":"code","1913f623":"code","6611180b":"code","406ed7ca":"code","d464728b":"code","514fc2a8":"code","d31f3a43":"code","0eab867f":"code","62066197":"code","7efd952d":"code","1ddb54ea":"code","f034c2fb":"code","f71f4d64":"code","2c3bfcf9":"markdown","eb10d6e7":"markdown","627e4600":"markdown","097e2330":"markdown","c036e537":"markdown","4e61c7b6":"markdown","6500e8ea":"markdown","4bcd1b89":"markdown","d3b173fd":"markdown","0e67a73e":"markdown","f98d8487":"markdown","719461df":"markdown","80b29075":"markdown","4ddb46b5":"markdown","18800f4a":"markdown","a371c534":"markdown","8c7d7a6a":"markdown","500cb849":"markdown","f63557ad":"markdown","fa072eb3":"markdown","1a56fd3d":"markdown","623844ef":"markdown","dfe55e91":"markdown","8e774142":"markdown","d299edd8":"markdown","b8d48ad2":"markdown"},"source":{"26c6f4eb":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n%matplotlib inline\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","7955261a":"sell_prices = pd.read_csv('\/kaggle\/input\/m5-forecasting-accuracy\/sell_prices.csv')\nsell_prices.head()","b5641f02":"sell_prices['id'] = sell_prices['item_id'] + '_' + sell_prices['store_id']\nsell_prices['sell_price'].describe()","f164d252":"sales = pd.read_csv('\/kaggle\/input\/m5-forecasting-accuracy\/sales_train_evaluation.csv')\nsales.head()","4e7caf85":"sales.info()","457e9e77":"calendar = pd.read_csv('\/kaggle\/input\/m5-forecasting-accuracy\/calendar.csv')\ncalendar.head()","dc73eae4":"calendar.info()","9c06ff35":"sales_training = sales.iloc[:,6:]\nsales_training.head()","5e7433ac":"sample_submission = pd.read_csv('\/kaggle\/input\/m5-forecasting-accuracy\/sample_submission.csv')\nsample_submission","ed3265ab":"rows = [0, 42, 1024, 10024]\nfig, axes = plt.subplots(nrows=len(rows), ncols=1, figsize=(10,15))\nfor ax, row in zip(axes, rows):\n    sales_training.iloc[row].plot(ax=ax)","6af2bfd8":"rows = [0, 42, 1024, 10024]\nfig, axes = plt.subplots(nrows=len(rows), ncols=1, figsize=(10,15))\nfor ax, row in zip(axes, rows):\n    sales_training.iloc[row].rolling(30).mean().plot(ax=ax)","a8120394":"days_to_forecast = ['F'+str(x) for x in range(1, 29)]\n\nvalidation_pred = pd.DataFrame(sales['id'])\nfor day_to_forecast in days_to_forecast:\n    validation_pred[day_to_forecast] = sales_training.mean(axis=1)\nevaluation_pred = validation_pred.copy()\nevaluation_pred['id'] = sales['item_id'] + '_' + sales['store_id'] + '_evaluation'\npredictions = pd.concat([validation_pred, evaluation_pred])\npredictions","f146fb7a":"# predictions.to_csv('.\/baseline_predictions.csv', index=False)","6012b052":"from statsmodels.tsa.stattools import adfuller","7e540e9a":"D = []\n\ntry:\n    stationarity_differences = pd.read_csv('\/kaggle\/input\/m5-precomputed1\/stationarity_differences1.csv').iloc[:,0]\nexcept FileNotFoundError:\n    for index, row in sales_training.iterrows():\n        d = 0\n        p_val = adfuller(row, autolag='AIC')[1]\n        while p_val > 0.05:\n            d += 1\n            row = row.diff()[1:]\n            p_val = adfuller(row, autolag='AIC')[1]\n        D.append(d)\n    pd.Series(D).to_csv('.\/stationarity_differences.csv', index=False)\n    stationarity_differences = pd.read_csv('\/kaggle\/input\/m5-precomputed\/stationarity_differences.csv').iloc[:,0]","693cce9a":"stationarity_differences.value_counts()","5cde0dac":"sales_training.iloc[12791].plot()","0560812d":"stationary_train_sales = np.diff(sales_training.values, axis=1)","3fc4fed5":"from sklearn.preprocessing import StandardScaler","0f1318e1":"scaler = StandardScaler(with_mean=False)\nscaler.fit(stationary_train_sales.T)\nX_train = scaler.transform(stationary_train_sales.T).T\nscales = scaler.scale_","8cfb0b2f":"calendar","a5264c7c":"sales_normalized = calendar[['wm_yr_wk','d']].iloc[:1941]\nsales_normalized = pd.DataFrame(X_train, columns=sales_normalized['d'][1:])\nsales_normalized.insert(0, 'id', sales['item_id'] + '_' + sales['store_id'])\nsales_normalized","048f7a5b":"rows = [0, 42, 1024, 10024]\nfig, axes = plt.subplots(nrows=len(rows), ncols=1, figsize=(10,15))\nfor ax, row in zip(axes, rows):\n    sales_normalized.iloc[row, 1:].plot(ax=ax)","248c358a":"sales_normalized","7ec8e16f":"rows = [42, 10024]\nfig, axes = plt.subplots(nrows=len(rows), ncols=1, figsize=(10,15))\nfor ax, row in zip(axes, rows):\n    integrated_series = np.cumsum(sales_normalized.iloc[row, 1:]*scales[row])\n    c = sales_training.iloc[row, 0]\n    integrated_series = pd.Series(integrated_series + c).shift(1)\n    integrated_series[:100].plot(ax=ax, style='r--', legend=True, label='re-integrated')\n    sales_training.iloc[row][:100].plot(ax=ax, legend=True, label='original')\n    total_numerical_error = np.abs(np.array(pd.Series(integrated_series)[1:].to_numpy() - sales_training.iloc[row,1:-1].to_numpy())).sum()\n    ax.set_title('Total numerical error: {:.2f}'.format(total_numerical_error))","c57f3911":"sns.distplot(sell_prices['id'].value_counts(), kde=False, axlabel='number of weeks the product was priced on')","9503e692":"sales_two_week_sum = sales_training.rolling(14, axis=1).sum()","68b93537":"for col in range(13):\n    sales_two_week_sum.iloc[:, col] = sales_two_week_sum.iloc[:, 13]\n    \nis_off_the_shelf = sales_two_week_sum == 0\n#to the days when the products were off for 14 last days we add those 14 days\nis_off_the_shelf = is_off_the_shelf | is_off_the_shelf.shift(-13, axis=1)\nis_on_the_shelf = is_off_the_shelf == False\n# True\/False to 1\/0\n# is_on_the_shelf = is_on_the_shelf.astype('int')","8150e89d":"is_on_the_shelf","de8175ad":"rows = [0, 42, 1024, 10024]\nfig, axes = plt.subplots(nrows=len(rows), ncols=1, figsize=(10,15))\nfor ax, row in zip(axes, rows):\n    shelf = pd.DataFrame(is_on_the_shelf.iloc[row])\n    shelf.columns = ['is_on_the_shelf']\n    shelf['sold'] = sales_training.iloc[row]\n    shelf = shelf.reset_index()\n    shelf.drop('index', inplace=True, axis=1)\n    shelf[shelf['is_on_the_shelf'] == True]['sold'].plot(legend=True, label='on shelf', ax=ax)\n    shelf[shelf['is_on_the_shelf'] == False]['sold'].plot(style='o', legend=True, label='not on shelf', ax=ax)","283d118a":"sales['dept_id'].value_counts()","25912f89":"sales['cat_id'].value_counts()","f053b47e":"sales['state_id'].value_counts()","bde21199":"from sklearn.preprocessing import OneHotEncoder","8e6e7e0e":"encoder = OneHotEncoder()\ndept_encoded = encoder.fit_transform(sales['dept_id'].values.reshape(-1,1))\ncat_encoded = encoder.fit_transform(sales['cat_id'].values.reshape(-1,1))\nstate_encoded = encoder.fit_transform(sales['state_id'].values.reshape(-1,1))","1989557a":"train_prices = pd.read_csv('\/kaggle\/input\/train-price-parser\/train_prices.csv')\n#sort IDs to match our order of IDs\ntrain_prices['id'] = train_prices['id'].astype(\"category\")\ncorrect_order = sales['item_id'] + '_' + sales['store_id']\ntrain_prices['id'].cat.set_categories(correct_order, inplace=True)\ntrain_prices = train_prices.sort_values([\"id\"]).reset_index().drop(columns=['index'])","3c83d4f3":"print('train_prices - observed {:.2f}% missing values'.format(train_prices.isnull().sum(axis=1).mean()\/1970 * 100))","cadd7a08":"from sklearn.impute import SimpleImputer","14eeac7a":"imputer = SimpleImputer(strategy='constant',fill_value='no_event')\nimputed_calendar_primary = imputer.fit_transform(calendar['event_name_1'].to_numpy().reshape(-1,1))\nimputed_calendar_secondary = imputer.fit_transform(calendar['event_name_2'].to_numpy().reshape(-1,1))","e830c935":"imputed_calendar = np.hstack((imputed_calendar_primary,imputed_calendar_secondary))","1913f623":"# a quick note - this has for some reason already beed 'differenciated', by which a mean the holidays lasting for few days\n# are denoted as beggining and end of the holliday\nencoder = OneHotEncoder()\ncalendar_encoded = encoder.fit_transform(imputed_calendar)\n\n# the line meaning that no event happens dubled so we throw one out\ncalendar_encoded = calendar_encoded[:,:-1]","6611180b":"# never forget to equally differentiate every time series!\nis_on_the_shelf_diff = is_on_the_shelf.diff(axis=1).iloc[:,1:]\nis_on_the_shelf_diff = is_on_the_shelf_diff.astype('int')","406ed7ca":"class M5_SeriesGenerator:\n    def __init__(self, day_zero = 1941, max_rows = 30490):\n        self.day_zero = day_zero\n        self.max_rows = max_rows\n        self.rows_remaining = np.arange(self.max_rows)\n        \n    def reset(self):\n        self.rows_remaining = np.arange(self.max_rows)\n        \n    def next_batch(self, in_points=30, out_points=3, batch_size=10):\n        X_batch = []\n        X_past_batch = []\n        scale_batch = []\n        c_batch = []\n        facts_batch = []\n        y_batch = []\n        \n        for _ in range(batch_size):\n            if self.rows_remaining.shape[0] == 0:\n                return False, (None, None)\n        \n            row = np.random.randint(self.rows_remaining.shape[0])\n            self.rows_remaining = np.delete(self.rows_remaining, row)\n            X_train_start = self.day_zero-1-in_points\n            X_prev_year_start = self.day_zero-365\n\n            while is_on_the_shelf.iloc[row, X_train_start+in_points] == False:\n                if self.rows_remaining.shape[0] == 0:\n                    return False, (None, None)\n                row = np.random.randint(self.rows_remaining.shape[0])\n                self.rows_remaining = np.delete(self.rows_remaining, row)\n\n            Xsales_train = sales_normalized.iloc[row, X_train_start+1:X_train_start+in_points+1].values.astype(np.float32)\n            Xsales_prev_year = sales_normalized.iloc[row, X_prev_year_start:X_prev_year_start+out_points].values.astype(np.float32)\n\n            Y_train_start = X_train_start+in_points\n            Yprices_train = train_prices.iloc[row, Y_train_start+2:Y_train_start+out_points+2].values.astype(np.float32)\n            Yevents_train = calendar_encoded[Y_train_start+1:Y_train_start+out_points+1, :].toarray().astype(int)\n            Ydept_train = np.tile(dept_encoded[row].toarray().astype(int),(out_points,1))\n            Ycat_train = np.tile(cat_encoded[row].toarray().astype(int),(out_points,1))\n            Ystate_train = np.tile(state_encoded[row].toarray().astype(int),(out_points,1))\n            Ysales_train = sales_training.iloc[row, Y_train_start+1:Y_train_start+out_points+1].values.astype(int).flatten()\n            \n            Yfacts = np.hstack((Yprices_train.reshape(-1, 1), Yevents_train, Ydept_train, Ycat_train, Ystate_train))\n            integral_constant = sales_training.iloc[row, X_train_start+in_points]\n            scale = scales[row]\n            \n            X_batch.append(Xsales_train.reshape(-1, 1))\n            X_past_batch.append(Xsales_prev_year.reshape(-1, 1))\n            scale_batch.append(scale)\n            c_batch.append(integral_constant)\n            facts_batch.append(Yfacts)\n            y_batch.append(Ysales_train)\n        return True, ((np.asarray(X_batch), np.concatenate((np.asarray(X_past_batch), np.asarray(facts_batch)), axis=2), np.asarray(scale_batch), np.asarray(c_batch)), np.asarray(y_batch))","d464728b":"def eval_series_data_gen(in_points = 120, out_points=28, day_zero=1913, max_row=30490, day_step=1):    \n    row = 0\n    while row < max_row:\n        X_batch = []\n        X_past_batch = []\n        scale_batch = []\n        c_batch = []\n        facts_batch = []\n        y_batch = []\n        X_start = day_zero-1-in_points\n        X_prev_year_start = day_zero-365\n\n        Y_start = X_start+in_points\n        Ysales = sales_training.iloc[row, Y_start+1:Y_start+out_points+1].values.astype(int).flatten()\n        y_batch.append(Ysales)\n        \n        if is_on_the_shelf.iloc[row, X_start+in_points] == False:\n            row += day_step\n            yield False, (None, np.asarray(y_batch))\n        else:\n            Xsales = sales_normalized.iloc[row, X_start+1:X_start+in_points+1].values.astype(np.float32)\n            Xsales_prev_year = sales_normalized.iloc[row, X_prev_year_start:X_prev_year_start+out_points].values.astype(np.float32)\n\n\n            Yprices = train_prices.iloc[row, Y_start+2:Y_start+out_points+2].values.astype(np.float32)\n            Yevents = calendar_encoded[Y_start+1:Y_start+out_points+1, :].toarray().astype(int)\n            Ydept = np.tile(dept_encoded[row].toarray().astype(int),(out_points,1))\n            Ycat = np.tile(cat_encoded[row].toarray().astype(int),(out_points,1))\n            Ystate = np.tile(state_encoded[row].toarray().astype(int),(out_points,1))\n            \n\n            Yfacts = np.hstack((Yprices.reshape(-1, 1), Yevents, Ydept, Ycat, Ystate))\n            integral_constant = sales_training.iloc[row, X_start+in_points]\n            scale = scales[row]\n\n            X_batch.append(Xsales.reshape(-1, 1))\n            X_past_batch.append(Xsales_prev_year.reshape(-1, 1))\n            scale_batch.append(scale)\n            c_batch.append(integral_constant)\n            facts_batch.append(Yfacts)\n            \n            row += day_step\n            yield True, ((np.asarray(X_batch), np.concatenate((np.asarray(X_past_batch), np.asarray(facts_batch)), axis=2), np.asarray(scale_batch), np.asarray(c_batch)), np.asarray(y_batch))","514fc2a8":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.layers import BatchNormalization","d31f3a43":"class M5_Net(keras.Model):\n    def __init__(self, input_timesteps, output_timesteps, batch_size=1):\n        super(M5_Net, self).__init__()\n        self.input_timesteps = input_timesteps\n        self.output_timesteps = output_timesteps\n        self.batch_size = batch_size\n\n        self.gru1 = tf.keras.layers.GRU(32, return_sequences=True)\n        self.gru1a = tf.keras.layers.GRU(64, return_sequences=True)\n        self.gru2 = tf.keras.layers.GRU(64, return_sequences=True)\n        self.gru2a = tf.keras.layers.GRU(32, return_sequences=True)\n        self.gru_out = tf.keras.layers.GRU(1, return_sequences=True)\n        self.dense1 = keras.layers.Dense(self.output_timesteps, activation=\"selu\", kernel_initializer=\"lecun_normal\")\n        \n    def call(self, input_data):\n        series_data, historical_data, scale, integral_constant = input_data\n        \n        x = BatchNormalization()(self.gru1(series_data))\n        x = BatchNormalization()(self.gru1a(x))\n        x = tf.reshape(x, [self.batch_size, -1])\n        x = BatchNormalization()(self.dense1(x))\n        x = tf.reshape(x, [self.batch_size, -1, 1])\n        x = tf.concat([x,\n                       historical_data,\n                       np.expand_dims(np.tile(integral_constant, (self.output_timesteps,1)).T, axis=2),\n                       np.expand_dims(np.tile(scale, (self.output_timesteps,1)).T, axis=2)\n                      ], axis=2)\n        x = BatchNormalization()(self.gru2(x))\n        x = BatchNormalization()(self.gru2a(x))\n        x = BatchNormalization()(self.gru_out(x))\n        x = tf.reshape(x, [self.batch_size, -1])\n        \n        @tf.function\n        def inverse_normalize(x):\n            sales_pred = tf.transpose(tf.math.multiply(tf.transpose(x), y=scale))\n            sales_pred = tf.math.cumsum(sales_pred, axis=1)\n            sales_pred += np.tile(integral_constant, (self.output_timesteps,1)).T\n            return sales_pred\n        \n        sales_pred = inverse_normalize(x)\n        return sales_pred","0eab867f":"from math import sqrt\n\nIN_POINTS = 120\nOUT_POINTS = 28\nBATCH_SIZE = 16\nmodel = M5_Net(input_timesteps=IN_POINTS, output_timesteps=OUT_POINTS, batch_size=BATCH_SIZE)\n\nloss_object = tf.keras.losses.MeanSquaredError()\n\ndef loss(model, x, y, training):\n    y_ = model(x, training=training)\n\n    return loss_object(y_true=y, y_pred=y_)\n\ndef grad(model, inputs, targets):\n    with tf.GradientTape() as tape:\n        loss_value = loss(model, inputs, targets, training=True)\n    return loss_value, tape.gradient(loss_value, model.trainable_variables)\n","62066197":"model.load_weights('\/kaggle\/input\/checkpoint6\/croc_model6.ckpt')","7efd952d":"\noptimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)\nM5_series_gen = M5_SeriesGenerator(day_zero = 1913-365)\nbatch_sequence = [16, 16, 16, 16,  16, 16, 16, 32, 32, 32, 32, 32, 32, 32]\ntraining = []\n\nVAL_SIZE = 400\nvalidation = []\nfor epoch in range(len(batch_sequence)):\n    BATCH_SIZE = batch_sequence[epoch]\n    model.batch_size = BATCH_SIZE\n    epoch_loss = []\n    more_data_available, (X_train, y_train) = M5_series_gen.next_batch(in_points=IN_POINTS, out_points=OUT_POINTS, batch_size=BATCH_SIZE)\n    while True:\n        more_data_available, (X_train, y_train) = M5_series_gen.next_batch(in_points=IN_POINTS, out_points=OUT_POINTS, batch_size=BATCH_SIZE)\n        if more_data_available == False:\n            break;\n            \n        loss_value, grads = grad(model, X_train, y_train)\n        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n        clipped_loss = loss_object(y_true=y_train, y_pred=tf.clip_by_value(model(X_train, training=True), clip_value_min=0, clip_value_max=np.inf))\n        epoch_loss.append(sqrt(clipped_loss))\n    training.append(np.array(epoch_loss).mean())\n    epoch_val_loss = []\n    model.batch_size = 1\n    for on, (X_val, y_true) in eval_series_data_gen(in_points=IN_POINTS, out_points=OUT_POINTS, day_zero=1913, day_step=30490\/\/VAL_SIZE):\n        if on:\n            y_val = tf.clip_by_value(model(X_val, training=True), clip_value_min=0, clip_value_max=np.inf)\n        else:\n            y_val = np.zeros((1,OUT_POINTS))\n        val_loss = loss_object(y_true=y_true, y_pred=y_val)\n        epoch_val_loss.append(val_loss)\n    model.batch_size = BATCH_SIZE\n    validation.append(np.array(epoch_val_loss).mean())\n    print(f'Epoch {epoch} training loss: {training[-1]}, Epoch {epoch} validation loss: {validation[-1]}')\n    model.save_weights('.\/croc_model{}.ckpt'.format(epoch))\n    M5_series_gen.reset()","1ddb54ea":"pd.Series(training).plot(legend=True, label='training')\npd.Series(validation).plot(legend=True, label='validation')","f034c2fb":"N = 24\nmodel.batch_size = N\n_, (X_train, y_train) = M5_series_gen.next_batch(in_points=IN_POINTS, out_points=OUT_POINTS, batch_size=N)\nrows = np.arange(N)\nfig, axes = plt.subplots(nrows=len(rows), ncols=1, figsize=(10,N*3))\ny_ = tf.clip_by_value(model(X_train, training=True), clip_value_min=0, clip_value_max=np.inf)\nfor ax, row in zip(axes, rows):\n    pd.Series(y_train[row]).plot(legend=True, label='ground truth',ax=ax)\n    pd.Series(y_[row]).plot(legend=True, label='forecast',ax=ax)\n","f71f4d64":"IN_POINTS = 120\nOUT_POINTS = 28\nmodel.batch_size = 1\n\nvalidation = []\nevaluation = []\niteration = 0\nfor  on, (X, y) in eval_series_data_gen(in_points=IN_POINTS, out_points=OUT_POINTS, day_zero=1913):\n    if on:\n        val = tf.clip_by_value(model(X, training=True), clip_value_min=0, clip_value_max=np.inf).numpy().squeeze()\n        validation.append(val)\n    else:\n        validation.append(np.zeros(OUT_POINTS))\n        \nfor  on, (X, y) in eval_series_data_gen(in_points=IN_POINTS, out_points=OUT_POINTS, day_zero=1941):\n    if on:\n        ev = tf.clip_by_value(model(X, training=True), clip_value_min=0, clip_value_max=np.inf).numpy().squeeze()\n        evaluation.append(ev)\n    else:\n        evaluation.append(np.zeros(OUT_POINTS))\n\nsample_submission.iloc[:30490, 1:] = validation\nsample_submission.iloc[30490:, 1:] = evaluation\n        \nsample_submission.to_csv('.\/final_prediction.csv', index=False)\n    ","2c3bfcf9":"## Train\/test generator","eb10d6e7":"## Outcome\n\nWe cannot show it yet, as the validation data are not yet public, but the RMSE for this solution was 1.69158\n","627e4600":"# Introduction\n\nIn this competition, the fifth iteration, we will use hierarchical sales data from Walmart, the world\u2019s largest company by revenue, to forecast daily sales for the next 28 days.\n    The data, covers stores in three US States :\n    \n    -California,\n    -Texas,\n    -Wisconsin\n    \n   and includes item level:\n   \n    -department,\n    -product categories,\n    -store details. \n    \n   In addition, it has explanatory variables such as:\n   \n    -price,\n    -promotions,\n    -day of the week,\n    -special events. \n    \n   Together, this robust dataset can be used to improve forecasting accuracy.\n   \n   ## Datasets\n   \n   \n\n**calendar.csv** - Contains information about the dates on which the products are sold.\n\n**sales_train_validation.csv** - Contains the historical daily unit sales data per product and store [d_1 - d_1913]\n\n**sell_prices.csv** - Contains information about the price of the products sold per store and date.\n","097e2330":"Well.. we definitely should address that intermittency at some point.. But other that that, it looks pretty good.\nLet's differenciate our data a single time (we will ignore this single anomaly, it's non-statonarity will not do us nearly any charm), and then normalize their scale.","c036e537":"# Final results\n\nWe can now try to predict our validation and evaluation data, throw it at the website to rank us and visualize the outcome. First, let's see how we did - the result will be seen as the score for this notebook.","4e61c7b6":"# Visualizing the outcome\n\nFinally, we can visualize our outcome by drawing it along with ground truth","6500e8ea":"\n# Baseline model\n\nIn order to evaluate our progress we have to set up a baseline first. To do that, we'll simply take the overall average for every product-store relation and set it as our forecast value for each of 56 days.\n","4bcd1b89":"As we can see, the data seems to be changing on a rapid daily basis. \nInterestingly enough, as promissed in the competition description, there seems to be an occasional **intermittency** - that is - the data sometimes stops changing and just stays at zero.\n\nWe will look into this later, for now, let's take a one more look, this time at the average monthly sales.","d3b173fd":"*sale_prices* table was quite in the wrong format, so we had to do some merging and pivoting to match the format of the rest of our data. All that needed a lot of RAM and time, so it was done on a separate kernel, that can be found [here](https:\/\/www.kaggle.com\/patrykradon\/train-price-parser\/notebook). \n\nAnother problem was that there was about 20% missing data there. Thats not ideal, especially that as we have mentioned - usually products were actally sold, despite their missing price.\nBut since it is that way and we have already taken care of products being off the shelf, let's just make another assumption:\n\n**If someone did not care to note the price down, it probably did not change**, so all that we had to do was to impute the null values with those from before they went missing. If the beggining was missing we would fill it with one from after it first appeared. \n\nSince this other notebook was for parsing anyway, we did Pandas ffil() followed by bfill() before saving it. Of course we had to differentiate it once, because we have to be consistent about differentiation when it comes to time series.\n\nNow all we have to do is fetch ready parsed data from there.","0e67a73e":"# Preprocessing and data engineering","f98d8487":"# Model selection\n\nNow that we have our inputs and desired outputs, it is time to build a model that will learn to connect these two.\n\nFor this project we will use  **RNN layers with GRU memory units** for the time series inputs, along with auxhilary inputs for our facts. All that will go through one **final dense layer** hopefully giving us complex enough model to succesfully forecast the 56 following days.","719461df":"In total, with the information we have gathered, we can plan out what our algorithm will try to do:\n\n* t - any point in time where our observations take place\n* p - amount of days that we will try to predict\n* k - amount of days before the time window we want to forecast used by our algorithm \n\n## Intermittency\n\nNow, we do not have a reliable data about products being on the shelf or not, so we will have to use our own estimate made by us in a previous section. Normally, it would be up for client to establish if the productss will be on the shelves or not during the forecasted time, but since it is a competiton we will make another asumption:\n\n**if at the end of known period the produt was off the shelf, we predict that it will be off the shelf for the next p days**\n\n## Algorithm\n\n(on purple, we have coloured the <span style=\"color:purple\">facts<\/span>, so the things we know about the values we try to predict.) \n\n**algorithm** <- sales_normalized(t : t+k), sales_normalized(t-365+k : t-365+k+p), <span style=\"color:purple\">is_on_the_shelf(t+k : t+k+p), calendar_encoded(t+k : t+k+p), dept_encoded(t+k : t+k+p), cat_encoded(t+k : t+k+p), state_encoded(t+k : t+k+p)<\/span> \n\n\n**algorithm** -> sales_training(t+k : t+k+p) * <span style=\"color:purple\">is_on_the_shelf(t+k : t+k+p)<\/span>\n\n\nKeeping all that in mind, let's build a generator that will produce the input data.","80b29075":"## Loading and defining our data","4ddb46b5":"**And there we have it!**\nEvery row is now stationery and scaled to a common point of reference! \n\nBut before we go any further, let's check what happens if we try to bring it back.","18800f4a":"Great! Let's see how our data works when we add this on-the-shelf information","a371c534":"So far, no strong trends differ those series. But to make sure that we can efficiently train on all time series simultaneously, we can 'integrate' them by making them all stationery, and then normalize them.\n\nWe will do that by **discrete-differentiating** each of them until the **Augmented Dickey-Fueller test** will give us an 95% chance that no series has a trend left!\n\nTo keep those series still representing out data, we will only put all those levels of differentiation and scales aside for the sake of forecasting, and then integrate and scale them back up for a final result.","8c7d7a6a":"# Exploration and visualization\nFor that, we will first start with a simple exploration.\nSo let's plot some of our time series to see what are we going to be dealing with.","500cb849":"Interesting, it seems like item-store relations differ in amount of days they were priced on. That might be responsible for our intermittency.\n\n**Unfortunately** after some exploration and research into sell_price data, we had conclude that it was not very helpfull for figuring out intermittency. Apparently, sometimes the price for a product was just missing in the database, sometimes it was there even though not a single unit was sold in weeks. We had to figure out a different way to recognize it.\n\nLet's try making subtitute assumption: \n\n**if the amount of items sold stayed as 0 for at least 2 weeks, we treat this part as intermittent**. That way, we will be able to very quickly evaluate which days not to consider for training. Even if that means ignoring some products that were being on the shelves, but rarely bought.. well we can't really do everythng here given the time we have, and that would be more in 'rare event analysis' category so the best model for that would probably be to just use a Poisson distribution anyway.","f63557ad":"As expected, we will have to prepare a 56 days forecast. First 28 will serve for a validation and the following 28 will remain unknown and serve to evaluate our final score in the overall ranking","fa072eb3":"Great! It can be brought back to original function without nearly any numerical loss.\n\n\nNow we just have to addres the intermittency and we are good to go! Let's start with analysing item-store relations in our sell_price data, maybe when the products were out of the store, their price was 0 or missing.","1a56fd3d":"## Imports","623844ef":"# Summary\n\nThe project turned out to be very edecational in many different ways. We know that training it specificaly for validation days was a little bit of a cheat and we wish we had enough time to make it an general tool for such tasks but for what we had, it turned out very satisfying. ","dfe55e91":"### Now let's take a  look at our target","8e774142":"# Feature engineering\n\nNow that we have or data cleaned up, we can finally go to the next stage of our project - feature engineering!\n\nSo, the plan is - since data looks pretty tidy already we will simply encode all the categorical features, match them to the data and prepare a sort of a pipeline to get the series to generate 'train_x' and 'train_y'.\n\nLet's pick features one by one, examine them and choose the suitable encoding.","d299edd8":"### So to sum up, it seems like it sometimes recognizes some specific behafior it tries to mimic. On the earlier epochs it was trying to mimic seasonalities, now it has mostly settled on predicting linear and not linear trends with the few exceptions.\n\nThe reasson for that may simply be that we did not had enough time or data to run tens of epochs on time seires from diffrenet periods. Many epochs on low batch size could give us the kick needed to get out of trend-based local minimas. Another reason for that might have been a not complex enough network, but for the limited computing time we have had, we could not afford anything to expensive. ","b8d48ad2":"# Training and tuning the model\n\nIt's time to make a training loop, put on some metrics, initialize our parameters and run our model through a couple of epochs."}}