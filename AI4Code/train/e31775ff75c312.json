{"cell_type":{"123f73b2":"code","0f6a52cb":"code","1aca612a":"code","2b8fd0e1":"code","dea91f02":"code","da4a490d":"code","d51ffbc8":"code","bd2b749b":"code","f5d489a8":"code","d7768d1f":"code","f2bdbeb6":"code","a62ec93c":"code","5a2f8cb5":"code","c2513d94":"code","fa35d056":"code","0517b871":"code","cccc3eae":"code","f782521b":"code","a4ce670a":"code","1d77a074":"code","84bf17c2":"code","a48710ec":"code","b48b91db":"code","635beacd":"code","fccf4205":"code","aac3aea6":"code","dd9919d4":"code","7e950869":"code","80970c03":"code","55151f99":"code","cb60918c":"code","1b4127dc":"code","d338776b":"code","2e76edde":"code","482fd0eb":"code","430f0ab4":"code","508d9d6b":"code","de58efa9":"code","034505f5":"markdown","49180b34":"markdown","6ea03bd7":"markdown","2ed659cd":"markdown","c534f03d":"markdown","49aedcae":"markdown","ffc032f2":"markdown","abed7589":"markdown","d239218b":"markdown"},"source":{"123f73b2":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","0f6a52cb":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nplt.style.use('fivethirtyeight')","1aca612a":"df=pd.read_csv('..\/input\/iris-flower-dataset\/IRIS.csv', sep=',')\ndf.head()","2b8fd0e1":"df['species'].value_counts()\n","dea91f02":"df.columns\n","da4a490d":"X=df.iloc[:,0:4].values\ny=df.iloc[:,4].values","d51ffbc8":"label_dict={1: 'Iris-setosa',\n           2: 'Iris-virginica',\n           3: 'Iris-versicolor'}\nfeature_dict={0: 'sepal_length',1: 'sepal_width',2: 'petal_length',3: 'petal_width'}\n\nwith plt.style.context('seaborn-whitegrid'):\n    plt.figure(figsize=(8,6))\n    for i in range(4):\n        plt.subplot(2,2,i+1)\n        for lab in ('Iris-setosa','Iris-virginica','Iris-versicolor'):\n            plt.hist(X[y==lab, i],\n                    label=lab,\n                    bins=10,\n                    alpha=0.3)\n        plt.xlabel(feature_dict[i])\n    plt.legend(loc='upper right', fancybox=True,fontsize=8)\n    \n    plt.tight_layout()\n    plt.show()","bd2b749b":"from sklearn.preprocessing import StandardScaler\nX_std=StandardScaler().fit_transform(X)","f5d489a8":"mean_vec=np.mean(X_std,axis=0)\ncov_mat=(X_std-mean_vec).T.dot((X_std-mean_vec))\/(X_std.shape[0]-1)\nprint(\"Covariance Matrix \\n%s\" %cov_mat)","d7768d1f":"print(\"Numpy Covariance matrix \\n%s\" %np.cov(X_std.T))\n","f2bdbeb6":"cov_mat=np.cov(X_std.T)\n\neig_vals, eig_vecs=np.linalg.eig(cov_mat)\n\nprint(\"Eigenvectors \\n%s\" %eig_vecs)\nprint(\"Eigenvelues \\n%s\" %eig_vals)","a62ec93c":"corr_mat1=np.corrcoef(X_std.T)\n\neig_vals, eig_vecs=np.linalg.eig(corr_mat1)\n\nprint(\"Eigenvectors \\n%s\" %eig_vecs)\nprint(\"Eigenvelues \\n%s\" %eig_vals)","5a2f8cb5":"cor_mat2=np.corrcoef(X.T)\neig_vals, eig_vecs=np.linalg.eig(cor_mat2)\n\nprint(\"Eigenvectors \\n%s\" %eig_vecs)\nprint(\"Eigenvelues \\n%s\" %eig_vals)","c2513d94":"u, s, v=np.linalg.svd(X_std.T)\nu","fa35d056":"for ev in eig_vecs.T:\n    np.testing.assert_array_almost_equal(1.0, np.linalg.norm(ev))\nprint(\"Everithing is ok\")","0517b871":"# Make a list of (eigenvalue, eigenvector) tuples\neig_pairs = [(np.abs(eig_vals[i]), eig_vecs[:,i]) for i in range(len(eig_vals))]\n\n# Sort the (eigenvalue, eigenvector) tuples from high to low\neig_pairs.sort(key=lambda x: x[0], reverse=True)\n\n# Visually confirm that the list is correctly sorted by decreasing eigenvalues\nprint('Eigenvalues in descending order:')\nfor i in eig_pairs:\n    print(i[0])","cccc3eae":"tot = sum(eig_vals)\nvar_exp = [(i \/ tot)*100 for i in sorted(eig_vals, reverse=True)]\ncum_var_exp = np.cumsum(var_exp)","f782521b":"with plt.style.context('seaborn-whitegrid'):\n    plt.figure(figsize=(6, 4))\n\n    plt.bar(range(4), var_exp, alpha=0.5, align='center',\n            label='individual explained variance')\n    plt.step(range(4), cum_var_exp, where='mid',\n             label='cumulative explained variance')\n    plt.ylabel('Explained variance ratio')\n    plt.xlabel('Principal components')\n    plt.legend(loc='best')\n    plt.tight_layout()\n","a4ce670a":"matrix_w = np.hstack((eig_pairs[0][1].reshape(4,1),\n                      eig_pairs[1][1].reshape(4,1)))\n\nprint('Matrix W:\\n', matrix_w)","1d77a074":"Y = X_std.dot(matrix_w)","84bf17c2":"with plt.style.context('seaborn-whitegrid'):\n    plt.figure(figsize=(6, 4))\n    for lab, col in zip(('Iris-setosa', 'Iris-versicolor', 'Iris-virginica'),\n                        ('blue', 'red', 'green')):\n        plt.scatter(Y[y==lab, 0],\n                    Y[y==lab, 1],\n                    label=lab,\n                    c=col)\n    plt.xlabel('Principal Component 1')\n    plt.ylabel('Principal Component 2')\n    plt.legend(loc='lower center')\n    plt.tight_layout()\n    plt.show()","a48710ec":"from sklearn.decomposition import PCA as sklearnPCA\nsklearn_pca = sklearnPCA(n_components=2)\nY_sklearn = sklearn_pca.fit_transform(X_std)","b48b91db":"with plt.style.context('seaborn-whitegrid'):\n    plt.figure(figsize=(6, 4))\n    for lab, col in zip(('Iris-setosa', 'Iris-versicolor', 'Iris-virginica'),\n                        ('blue', 'red', 'green')):\n        plt.scatter(Y_sklearn[y==lab, 0],\n                    Y_sklearn[y==lab, 1],\n                    label=lab,\n                    c=col)\n    plt.xlabel('Principal Component 1')\n    plt.ylabel('Principal Component 2')\n    plt.legend(loc='upper center')\n    plt.tight_layout()\n    plt.show()","635beacd":"X=df.iloc[:,0:2].values\ny=df.iloc[:,4].values","fccf4205":"label_dict={1: 'Iris-setosa',\n           2: 'Iris-virginica',\n           3: 'Iris-versicolor'}\nfeature_dict={0: 'sepal_length',1: 'sepal_width',2: 'petal_length',3: 'petal_width'}\n\n","aac3aea6":"from sklearn.preprocessing import StandardScaler\nX_std=StandardScaler().fit_transform(X)","dd9919d4":"mean_vec=np.mean(X_std,axis=0)\ncov_mat=(X_std-mean_vec).T.dot((X_std-mean_vec))\/(X_std.shape[0]-1)\nprint(\"Covariance Matrix \\n%s\" %cov_mat)","7e950869":"print(\"Numpy Covariance matrix \\n%s\" %np.cov(X_std.T))","80970c03":"cov_mat=np.cov(X_std.T)\n\neig_vals, eig_vecs=np.linalg.eig(cov_mat)\n\nprint(\"Eigenvectors \\n%s\" %eig_vecs)\nprint(\"Eigenvelues \\n%s\" %eig_vals)","55151f99":"corr_mat1=np.corrcoef(X_std.T)\n\neig_vals, eig_vecs=np.linalg.eig(corr_mat1)\n\nprint(\"Eigenvectors \\n%s\" %eig_vecs)\nprint(\"Eigenvelues \\n%s\" %eig_vals)","cb60918c":"cor_mat2=np.corrcoef(X.T)\neig_vals, eig_vecs=np.linalg.eig(cor_mat2)\n\nprint(\"Eigenvectors \\n%s\" %eig_vecs)\nprint(\"Eigenvelues \\n%s\" %eig_vals)","1b4127dc":"X=df.iloc[:,0:3].values\ny=df.iloc[:,4].values","d338776b":"label_dict={1: 'Iris-setosa',\n           2: 'Iris-virginica',\n           3: 'Iris-versicolor'}\nfeature_dict={0: 'sepal_length',1: 'sepal_width',2: 'petal_length',3: 'petal_width'}","2e76edde":"from sklearn.preprocessing import StandardScaler\nX_std=StandardScaler().fit_transform(X)","482fd0eb":"mean_vec=np.mean(X_std,axis=0)\ncov_mat=(X_std-mean_vec).T.dot((X_std-mean_vec))\/(X_std.shape[0]-1)\nprint(\"Covariance Matrix \\n%s\" %cov_mat)","430f0ab4":"print(\"Numpy Covariance matrix \\n%s\" %np.cov(X_std.T))","508d9d6b":"cov_mat=np.cov(X_std.T)\n\neig_vals, eig_vecs=np.linalg.eig(cov_mat)\n\nprint(\"Eigenvectors \\n%s\" %eig_vecs)\nprint(\"Eigenvelues \\n%s\" %eig_vals)","de58efa9":"corr_mat1=np.corrcoef(X_std.T)\n\neig_vals, eig_vecs=np.linalg.eig(corr_mat1)\n\nprint(\"Eigenvectors \\n%s\" %eig_vecs)\nprint(\"Eigenvelues \\n%s\" %eig_vals)","034505f5":"# Obs\n* As we can see that PCA is good for higher dimension \n* As dimension reduces we can see that Correlation Matrix and COvariance Matrix are producing equal eigen values\n* https:\/\/www.kaggle.com\/jurk06\/pca-iris-dataset","49180b34":"# Projection Matrix","6ea03bd7":"# Eigendecomposition of the raw data based on the correlation matrix","2ed659cd":"# Workig with 3 features only ","c534f03d":"# Sorting Eigenpairs","49aedcae":"# Shortcut - PCA in scikit-learn","ffc032f2":"# Singular Value Decomposition","abed7589":"# Working with Two features only","d239218b":"# Correlation Matrix"}}