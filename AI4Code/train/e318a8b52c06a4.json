{"cell_type":{"cc5d924f":"code","17c20583":"code","122b1f54":"code","1f63c702":"code","a0fc45ab":"code","633921de":"code","943e7b28":"code","453b43c9":"code","cbe8d33b":"code","b5cd5534":"code","b9fd8c5a":"code","0ff79f04":"code","30729a75":"code","95cf1759":"code","aca793f2":"code","b22b81e5":"code","7f9b170d":"code","341007f9":"code","04a2ff92":"code","3a40f096":"code","cd391530":"code","fda51d88":"code","e19066b7":"code","fd262d05":"code","ab5d7acd":"code","32aacc65":"code","2a40e722":"code","9c1acc34":"markdown","59ae04b0":"markdown","5a49658c":"markdown","46e9a69b":"markdown","9a8f8215":"markdown","0331f8a7":"markdown","c95a3f85":"markdown","0fd74c50":"markdown","f655f240":"markdown","89a36858":"markdown","ebc8e524":"markdown","4b03c811":"markdown"},"source":{"cc5d924f":"from collections import Counter\nfrom collections import defaultdict\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer \nfrom wordcloud import WordCloud\nimport re\nimport pickle\nimport nltk\n# nltk.download('wordnet')\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt","17c20583":"!pip install codeprofile","122b1f54":"!pip install memory_utils","1f63c702":"from codeprofile import profiler\nfrom memory_utils import print_memory","a0fc45ab":"# Load small version of df with papers\npickle_in = open('\/kaggle\/input\/darvirian\/df.pkl', 'rb')\ndf = pickle.load(pickle_in)","633921de":"print_memory()","943e7b28":"## Load pickle file worddic (numeric version)\npickle_in = open('\/kaggle\/input\/darvirian\/worddic_all_200426-2.pkl', 'rb')\nworddic = pickle.load(pickle_in)","453b43c9":"print_memory()","cbe8d33b":"## Load pickle file word2idx\npickle_in = open('\/kaggle\/input\/darvirian\/word2idx_200426-2.pkl', 'rb')\nword2idx = pickle.load(pickle_in)","b5cd5534":"## Load pickle file idx2word\npickle_in = open('\/kaggle\/input\/darvirian\/idx2word_200426-2.pkl', 'rb')\nidx2word = pickle.load(pickle_in)","b9fd8c5a":"## Load pickle file sentences\npickle_in = open('\/kaggle\/input\/darvirian\/sentences_200426-2.pkl', 'rb')\nsentences = pickle.load(pickle_in)","0ff79f04":"# Create word search which takes multiple words (sentence) and finds documents that contain these words along with metrics for ranking:\n\n# Output: searchsentence, words, fullcount_order, combocount_order, fullidf_order, fdic_order\n# (1) searchsentence: original sentence to be searched\n# (2) words: words of the search sentence that are found in the dictionary (worddic)\n# (3) fullcount_order: number of occurences of search words\n# (4) combocount_order: percentage of search terms\n# (5) fullidf_order: sum of TD-IDF scores for search words (in ascending order)\n# (6)) fdic_order: exact match bonus: word ordering score\n\n# >>> example on limited dataset (first three docs of biorxiv))\n# search('Full-genome phylogenetic analysis')\n# (1) ('full-genome phylogenetic analysis',  # searchsentence: original search sentence\n# (2) ['phylogenetic', 'analysis'], # words: two of the search words are in the dictionary worddic\n# (3) [(1, 7), (0, 1)], # fullcount_order: the search words (as found in dict) occur in total 7 times in doc 1 and 1 time in doc 0\n# (4) [(1, 1.0), (0, 0.5)], # combocount_order: max value is 1, in doc 1 all searchwords (as in dict) are present (1), in doc 0 only 1 of the 2 search words are present (0.5)\n# (5) [(1, 0.0025220519886750533), (0, 0.0005167452472220973)], # fullidf_order: doc 1 has a total (sum) tf-idf of 0.0025220519886750533, doc 0 a total tf-idf of 0.0005167452472220973\n# (6) [(1, 1)]) # fdic_order: doc 1 has once two search words next to each other\n# <<<\n\ndef search(searchsentence, must_have_words=None):\n    # split sentence into individual words\n    searchsentence = searchsentence.lower()\n    # split sentence in words\n    words = word_tokenize(searchsentence)\n    # remove duplicates in search words\n     \n    # add must_have_words to search words\n    if must_have_words != None:\n        words = must_have_words + words\n\n    # lowercase all words\n    words = [word.lower() for word in words]\n                               \n    # keep characters as in worddic\n    words = [re.sub(r'[^a-zA-Z0-9]', '', str(w)) for w in words]\n    # words = [re.sub('[^a-zA-Z0-9]+', ' ', str(w)) for w in words]\n   \n    # Remove single characters\n    # words = [re.sub(r'\\b[a-zA-Z0-9]\\b', '', str(x)) for w in words]\n  \n    # remove empty characters\n    words = [word for word in words if word]\n\n    # remove stopwords\n    stop_words = set(stopwords.words('english'))\n    words = [word for word in words if word not in stop_words]\n    \n    # lemmatize search words\n    lemmatizer = WordNetLemmatizer()\n    words = [lemmatizer.lemmatize(word) for word in words]\n \n    # remove duplicates\n    words = list(set(words))\n    \n    # keep only the words that are in the dictionary; remove words if not in word2idx \n    [print('Word not in dictionary:', word) for word in words if word not in word2idx]\n    words = [word for word in words if word in word2idx]\n        \n    # number of words    \n    numwords = len(words)\n    \n    # word2idx all words\n    words = [word2idx[word] for word in words]\n\n    # Subset of worddic with only search words\n    worddic_sub = {key: worddic[key] for key in words}\n    \n    # temp dictionaries\n    enddic = {}\n    idfdic = {}\n    closedic = {}\n\n\n    ## metrics fullcount_order and fullidf_order: \n    # sum of number of occurences of all words in each doc (fullcount_order) \n    # and sum of TF-IDF score (fullidf_order)\n    for word in words:\n        # print(word)\n        for indpos in worddic[word]:\n            # print(indpos)\n            index = indpos[0]\n            amount = len(indpos[1])\n            idfscore = indpos[2]\n            # check if the index is already in the dictionary: add values to the keys\n            if index in enddic.keys():\n                enddic[index] += amount\n                idfdic[index] += idfscore\n            # if not, just make a two new keys and store the values\n            else:\n                enddic[index] = amount\n                idfdic[index] = idfscore\n    fullcount_order = sorted(enddic.items(), key=lambda x: x[1], reverse=True)\n    fullidf_order = sorted(idfdic.items(), key=lambda x: x[1], reverse=True)\n\n\n    ## metric combocount_order: \n    # percentage of search words (as in dict) that appear in each doc\n    words_docs = defaultdict(list)\n    # get for each word the docs which it is in\n    for k in worddic_sub.keys():\n        for i in range(len(worddic_sub[k])):\n            words_docs[k].append(worddic_sub[k][i][0])\n    # keep onlt the unique docs per word      \n    for k in words_docs:\n        words_docs[k] = set(words_docs[k])\n    # combination of all docs\n    comboindex = []\n    for k in words_docs:\n        comboindex += words_docs[k]\n    # count the number of each doc (from 0 to max number of search words)\n    combocount = Counter(comboindex) \n    # divide by number of search words (to get in range from [0,1])\n    for key in combocount:\n        combocount[key] = combocount[key] \/ numwords\n    # sort from highest to lowest\n    combocount_order = sorted(combocount.items(), key=lambda x: x[1], reverse=True)\n\n\n    ## metric closedic: \n    # check on words appearing in same order as in search\n    fdic_order = 0 # initialization (in case of a single search word)\n    if numwords > 1:\n        # list with docs with a search word        \n        x = [index[0] for record in [worddic[z] for z in words] for index in record]\n        # list with docs with more than one search word\n        # y = sorted(list(set([i for i in x if x.count(i) > 1])))\n        counts = np.bincount(x)\n        y = list(np.where([counts>1])[1])\n\n        # dictionary of documents and all positions \n        # (for docs with more than one search word in it)\n        closedic = {}\n        y = set(y) # speed up processing\n        for wordbig in [worddic[x] for x in words]:\n            for record in wordbig:\n                if record[0] in y:\n                    index = record[0]\n                    positions = record[1]\n                    try:\n                        closedic[index].append(positions)\n                    except:\n                        closedic[index] = []\n                        closedic[index].append(positions)\n    \n        ## metric fdic: \n        # number of times search words appear in a doc in descending order\n        fdic = {}\n        # fdic_order = []\n        for index in y: # list with docs with more than one search word\n            x = 0 \n            csum = []            \n            for seqlist in closedic[index]:\n                while x > 0:\n                    secondlist = seqlist # second word positions\n                    x = 0\n                    # first and second word next to each other (in same order)\n                    sol = [1 for i in firstlist if i + 1 in secondlist]\n                    csum.append(sol)\n                    fsum = [item for sublist in csum for item in sublist] \n                    fsum = sum(fsum) \n                    fdic[index] = fsum\n\n                while x == 0:\n                    firstlist = seqlist # first word positions \n                    x += 1 \n        fdic_order = sorted(fdic.items(), key=lambda x: x[1], reverse=True)\n    \n    ## keep only docs that contains all must_have_words\n    if must_have_words != None and numwords > 1:\n        \n        # lowercase all words\n        must_have_words = [word.lower() for word in must_have_words]                             \n        # keep characters as in worddic (without space)\n        must_have_words = [re.sub(r'[^a-zA-Z0-9]', '', str(w)) for w in must_have_words]   \n        # lemmatize words\n        lemmatizer = WordNetLemmatizer()\n        must_have_words = [lemmatizer.lemmatize(word) for word in must_have_words]\n        # remove duplicates\n        must_have_words = list(set(must_have_words))\n        # keep only the words that are in the dictionary word2idx \n        must_have_words = [word2idx[word] for word in must_have_words if word in word2idx]\n\n        # option: get list of all (unique) docs which have one of the must_have_words\n        # must_have_!docs = set([doc[0] for word in must_have_words for doc in worddic_sub[word]])\n\n        # option: get list of all (unique) docs which have all of the must_have_words\n        # create must_have_docs from all docs with a search w\n        must_have_docs = [doc[0] for word in must_have_words for doc in worddic_sub[word]]\n        must_have_docs_unique = Counter(must_have_docs).most_common()\n        # check if overlapping documents exist (if note give a message) \n        num_musthavewords = must_have_docs_unique[0][1] # first doc has highest occurences\n        # check if no verlapping at all\n        if num_musthavewords == 0: \n            print('Must have words do not have overlapping documents: search breaks')\n            return\n        # check if partly overlapping\n        if num_musthavewords != len(must_have_words): \n            print('Must have words not 100% overlap: {} out of {}'.format(num_musthavewords, len(must_have_words)))\n\n        else:\n            # create doc list with all docs which have all must_have_words \n            must_have_docs_unique = [doc[0] for doc in must_have_docs_unique if doc[1] == num_musthavewords]      \n  \n            # update the score metrics containing only docs with the must have words\n            fullcount_order = [list_of_list for list_of_list in fullcount_order\\\n                               if list_of_list[0] in must_have_docs_unique]\n            combocount_order = [list_of_list for list_of_list in combocount_order\\\n                                if list_of_list[0] in must_have_docs_unique]    \n            fullidf_order = [list_of_list for list_of_list in fullidf_order\\\n                             if list_of_list[0] in must_have_docs_unique]\n            fdic_order = [list_of_list for list_of_list in fdic_order\\\n                          if list_of_list[0] in must_have_docs_unique]\n    \n    \n    ## idx2word all words (transform words again in characters instead of numbers)\n    words = [idx2word[word] for word in words]\n\n    return (searchsentence, words, fullcount_order, combocount_order, fullidf_order, fdic_order)","30729a75":"# Create a rule based rank and return function\n\n# term = \"what is correlation between SARS-CoV-2 and Alzheimer's?\"\n# must_have_words = ['SARS-CoV-2', 'Alzheimer', 'correlation']\n\ndef rank(term, must_have_words=None):\n\n    # get results from search\n    results = search(term, must_have_words)\n    # get metrics\n    # search words found in dictionary:\n    search_words = results[1] \n    # number of search words found in dictionary:\n    num_search_words = len(results[1]) \n    # number of search words (as in dict) in each doc (in descending order):\n    num_score = results[2] \n    # percentage of search words (as in dict) in each doc (in descending order):\n    per_score = results[3]\n    # sum of tfidf of search words in each doc (in ascending order):\n    tfscore = results[4] \n    # fidc order:\n    order_score = results[5] \n\n    # list of documents in order of relevance\n    final_candidates = []\n\n    ## no search term(s) not found\n    if num_search_words == 0:\n        print('Search term(s) not found')\n\n\n    ## single term searched (as in dict): return the following 5 scores\n    if num_search_words == 1:\n        # document numbers:\n        num_score_list = [l[0] for l in num_score] \n        # take max 3 documents from num_score:\n        num_score_list = num_score_list[:3] \n        # add the best percentage score:\n        num_score_list.append(per_score[0][0]) \n        # add the best tfidf score\n        num_score_list.append(tfscore[0][0]) \n        # remove duplicate document numbers\n        final_candidates = list(set(num_score_list)) \n\n    ## more than one search word (and found in dictionary)\n    # ranking is based on an intelligent commbination of the scores\n    if num_search_words > 1:\n\n        ## set up a dataframe with scores of size all documents (initalized with 0)\n        total_number_of_docs = len(df)\n        doc_score_columns = ['num_score', 'per_score', 'tf_score', 'order_score']\n        doc_score = pd.DataFrame(0, index=np.arange(total_number_of_docs), columns=doc_score_columns)\n        \n        # plot a score in a daraframe (index is doc number)\n        def doc_plot(type_score):\n            score_doc = [0]*total_number_of_docs\n            for i in range(len(type_score)):\n                x = type_score[i][0] # document number\n                score_doc[x] = float(type_score[i][1]) # score\n            return score_doc\n        \n        # Fill-in for each doc the score\n        doc_score.num_score = doc_plot(num_score)\n        doc_score.per_score = doc_plot(per_score)\n        doc_score.tf_score = doc_plot(tfscore)\n        doc_score.order_score = doc_plot(order_score)\n        \n        # Normalize (to the sum or max)\n        # TODO CHANGE from sum to max\n        normalization = sum(doc_score.num_score)\n        normalization = max(doc_score.num_score)\n        doc_score.num_score = [float(i)\/normalization for i in doc_score.num_score]\n        \n        # keep per_score (percentage of search words in document) as it is (between 0 and 1)\n        \n        normalization = max(doc_score.tf_score)        \n        doc_score.tf_score = [float(i)\/normalization for i in doc_score.tf_score]\n        \n        normalization = max(doc_score.order_score)\n        if normalization != 0:\n            doc_score.order_score = [float(i)\/normalization for i in doc_score.order_score]\n        \n        # sum all scores to get a Grand Score\n        doc_score['sum'] = (doc_score.num_score +\\\n                            doc_score.per_score +\\\n                            doc_score.tf_score +\\\n                            doc_score.order_score)\n        # keep only the values with a sum > 0\n        doc_score = doc_score[doc_score['sum'] > 0]\n        \n        # get the docs (i.e index) sorted from high to low ranking\n        final_candidates = list(doc_score.sort_values('sum', ascending=False).index)\n    \n        # keep top 10 candidates\n        final_candidates = final_candidates[:10]        \n\n    # print final candidates\n    print('\\nFound search words:', results[1])\n\n    # top results: sentences with search words, paper ID (and document number), authors and abstract\n    df_results = pd.DataFrame(columns=\\\n              ['Title', 'Paper_id', 'Document_no', 'Authors', 'Abstract', 'Sentences', 'Search_words'])\n    for index, results in enumerate(final_candidates):\n        df_results.loc[index, 'Title'] = df.title[results]\n        df_results.loc[index, 'Paper_id'] = df.paper_id[results]\n        df_results.loc[index, 'Document_no'] = results\n        df_results.loc[index, 'Authors'] = df.authors[results]\n        df_results.loc[index, 'Abstract'] = df.abstract[results]\n\n        # get sentences with search words and all search words in the specific document\n        sentence_index, search_words_found = search_sentence(results, search_words)\n        # all sentences with search words\n        df_results.loc[index, 'Sentences'] = sentence_index\n        # all search words (also multiple instances) \n        df_results.loc[index, 'Search_words'] = search_words_found\n          \n    return final_candidates, df_results","95cf1759":"def search_sentence(doc_number, search_words):\n    sentence_index = [] # all sentences with search words \n    search_words_found = [] # all found search words\n    lemmatizer = WordNetLemmatizer()\n    \n    # temporarily version of document to clean\n    doc = sentences[doc_number]\n    # clean text\n    doc = [re.sub(r'[^a-zA-Z0-9]', '', str(x)) for x in doc]\n    # remove single characters\n    doc = [re.sub(r'\\b[a-zA-Z0-9]\\b', '', str(x)) for x in doc]\n    # lower case words\n    doc = [word.lower() for word in doc]\n    # lemmatize\n    doc = [lemmatizer.lemmatize(word) for word in doc]   \n    \n    for i, sentence in enumerate(doc):\n       \n        # all search words (also multiple instances)\n        for search_word in search_words:\n            if search_word in sentence:\n                # take original sentence\n                sentence_index.append(sentences[doc_number][i])\n                break\n        # all search words (also multiple instances)\n        for search_word in search_words:\n            if search_word in sentence:\n                search_words_found.append(search_word)\n        # [search_words_found.append(search_word) for search_word in search_words if search_word in sentence_temp]\n            \n    return sentence_index, search_words_found ","aca793f2":"## Highlight in a text specific words in a specific color\n# return this text with the highlighted words\n\ndef highlight_words(text, words, color):\n    \n    # color set\n    color_set = {'red': '\\033[31m', 'green': '\\033[32m','blue': '\\033[34m','reset': '\\033[39m'}\n\n    lemmatizer = WordNetLemmatizer() \n    \n    # wrap words in color\n    for word in words:\n        # text_lower = text.lower() # lowercase words\n        text_temp = [re.sub(r'[^a-zA-Z]', '', word) for word in text]\n        # lemmatize\n        text_temp = [lemmatizer.lemmatize(word) for word in text_temp] \n        # idxs = [i for i, x in enumerate(text) if x.lower() == word]\n        idxs = [i for i, x in enumerate(text_temp) if x.lower() == word]\n        for i in idxs:\n            text[i] = color_set[color] + text[i] + color_set['reset']\n            \n    # join the list back into a string and print\n    text_highlighted = ' '.join(text)\n    return(text_highlighted)\n         \n\n## Main function of printing the papers in ranked order\n# Select per document: \n# - top_n: number of top n papers to be displayed\n# - show_sentences: display the sentences which contains search words\n# - show_wordcloud: dsiplay a cloud word of these sentences\ndef print_ranked_papers(ranked_result, top_n=3, show_abstract=True, show_sentences=True):\n\n    # print top n result (with max number of documents from ranked_result)\n    for index in range(min(top_n, len(ranked_result))):    \n\n        ## preparation        \n        # list of search words\n        search_words = list(set(ranked_result.Search_words[index]))\n\n        # preparation to display wordcloud      \n        text_sentences_wc = ranked_result.Sentences[index]\n        text_sentences_wc = [re.sub(r'[^a-zA-Z0-9]', ' ', str(x)) for x in text_sentences_wc]\n        # remove single characters\n        text_sentences_wc = [re.sub(r'\\b[a-zA-Z0-9]\\b', '', str(x)) for x in text_sentences_wc]\n        # lower case words\n        text_sentences_wc = [word.lower() for word in text_sentences_wc]\n        # lemmatize\n        lemmatizer = WordNetLemmatizer() \n        text_sentences_wc = [lemmatizer.lemmatize(word) for word in text_sentences_wc] \n        # join all sentences and seperate by a return\n        text_sentences_wc = '\\n'.join(text_sentences_wc)\n        # spit in seperate words\n        text_sentences_split_wc = word_tokenize(text_sentences_wc)\n\n        # preparation to print out sentences \n        # join all sentences and seperate by a return\n        text_sentences = '\\n'.join(ranked_result.Sentences[index])\n        # Spit in seperate words \n        text_sentences_split = text_sentences.split()\n\n\n        ## print most important items per document (paper)\n        # and in case of 'nan' write 'not available'\n       \n        # ranking number and title\n        if pd.isnull(ranked_result.Title[index]):\n            print('\\n\\nRESULT {}:'. format(index+1), 'Title not available')\n        else: \n                # Print Result from 1 and not 0\n                print('\\n\\nRESULT {}:'. format(index+1), ranked_result.Title[index]) \n    \n        # generate cloud word\n        wordcloud = WordCloud()\n        img = wordcloud.generate_from_text(' '.join(text_sentences_split_wc))\n        plt.imshow(img)\n        plt.axis('off')\n        plt.show()   \n       \n        # count all search word in document and present them from highest to lowest  \n        dict_search_words =\\\n            dict(Counter(ranked_result.Search_words.iloc[index]).most_common())\n        print('\\nI Number of search words in paper:')\n        for k,v in dict_search_words.items():\n            print('- {}:'.format(k), v)\n            \n        # paper id and document number\n        print('\\nII Paper ID:', ranked_result.Paper_id[index], \n              '(Document no.: {})'. format(ranked_result.Document_no[index]))\n        \n        # authors\n        if pd.isnull(ranked_result.Abstract[index]):\n            print('\\nIII Authors:', 'Authors not available')\n        else:\n            print('\\nIII Authors:', ranked_result.Authors[index])\n        print('\\n')\n        \n        # abstract\n        if show_abstract == True:\n            if pd.isnull(ranked_result.Abstract[index]):\n                print('Abstract not available')\n            else: \n                # split abstract in seperate words\n                abstract_sentences_split = ranked_result.Abstract[index].split()\n                # highlight the search words in red\n                print(highlight_words(abstract_sentences_split, search_words, 'red'))\n              \n        ## show sentences with search words in green\n        if show_sentences == True:\n            print('\\nIV Sentences in paper containing search words:\\n')\n            print(highlight_words(text_sentences_split, search_words,'green'))","b22b81e5":"# Fill in your search sentence; e.g.: 'Fullgenome phylogenetic analysis'\nmust_have_word = ['genome']\nsearch_example = 'Fullgenome phylogenetic analysis'\npapers, rank_result = rank(search_example, must_have_word)\n# papers, rank_result = rank(search_example)\n\n# Print final candidates\nprint('Top 10 papers (document numbers):', papers)\n\n# Print results\nprint_ranked_papers(rank_result, top_n=2, show_abstract=True, show_sentences=True)","7f9b170d":"# =============================================================================\n# CASE EUvsVirus: Health & Life, Research\n# =============================================================================\n# mapping of covid literature with perspectives of tests\/medication\/vaccination development\nmust_have_words = ['covid', 'tests', 'medication', 'vaccination', 'development']\n\npapers, rank_result = rank('tests medication vaccination development', must_have_words)\n\n# Print final candidates\nprint('Ranked papers (document numbers):', papers)\n\n# Print results\nprint_ranked_papers(rank_result, top_n=3, show_abstract=True, show_sentences=True)","341007f9":"# =============================================================================\n# CASE 1: Real-time tracking of whole genomes\n# =============================================================================\npapers, rank_result = rank('Real-time tracking of whole genomes and a mechanism for coordinating the rapid dissemination of that information to inform the development of diagnostics and therapeutics and to track variations of the virus over time.')\n\n# Print final candidates\nprint('Top 10 papers (document numbers):', papers)\n\n# Print results\nprint_ranked_papers(rank_result, top_n=3, show_abstract=True, show_sentences=False)","04a2ff92":"# =============================================================================\n# CASE 2: Access to geographic and temporal diverse sample sets\n# =============================================================================\nmust_have_word = ['Nagoya']\npapers, rank_result = rank('Access to geographic and temporal diverse sample sets to understand geographic distribution and genomic differences, and determine whether there is more than one strain in circulation. Multi-lateral agreements such as the Nagoya Protocol could be leveraged.', must_have_word)\n\n# Print final candidates\nprint('Top 10 papers (document numbers):', papers)\n\n# Print results\nprint_ranked_papers(rank_result, top_n=1, show_abstract=False, show_sentences=True)","3a40f096":"# =============================================================================\n# CASE 3a (first sentence): Evidence that livestock could be infected\n# =============================================================================\npapers, rank_result = rank('Evidence that livestock could be infected (e.g., field surveillance, genetic sequencing, receptor binding) and serve as a reservoir after the epidemic appears to be over.')\n\n# Print final candidates\nprint('Top 10 papers (document numbers):', papers)\n\n# Print results\nprint_ranked_papers(rank_result, top_n=4, show_abstract=True, show_sentences=False)","cd391530":"# =============================================================================\n# CASE 3b (full text): Evidence that livestock could be infected\n# =============================================================================\npapers, rank_result = rank('Evidence that livestock could be infected (e.g., field surveillance, genetic sequencing, receptor binding)\\\n                            and serve as a reservoir after the epidemic appears to be over.\\\n                            Evidence of whether farmers are infected, and whether farmers could have played a role in the origin.\\\n                            Surveillance of mixed wildlife- livestock farms for SARS-CoV-2 and other coronaviruses in Southeast Asia.\\\n                            Experimental infections to test host range for this pathogen.')\n\n# Print final candidates\nprint('Top 10 papers (document numbers):', papers)\n\n# Print results\nprint_ranked_papers(rank_result, top_n=4, show_abstract=True, show_sentences=False)","fda51d88":"# =============================================================================\n# CASE 3c (first subquestion): Evidence of whether farmers are infected, and whether farmers could have played a role in the origin.\n# =============================================================================\nmust_have_word = ['farmer']\npapers, rank_result = rank('Evidence of whether farmers are infected, and whether farmers could have played a role in the origin.', must_have_word)\n\n# Print final candidates\nprint('Top 10 papers (document numbers):', papers)\n\n# Print results\nprint_ranked_papers(rank_result, top_n=2, show_abstract=True, show_sentences=False)","e19066b7":"# =============================================================================\n# CASE 3d (second subquestion): Surveillance of mixed wildlife- livestock farms for SARS-CoV-2 and other coronaviruses in Southeast Asia.\n# =============================================================================\nmust_have_word = ['Surveillance', 'covid']\npapers, rank_result = rank('Surveillance of mixed wildlife- livestock farms for SARS-CoV-2 and other coronaviruses in Southeast Asia.', must_have_word)\n\n# Print final candidates\nprint('Top 10 papers (document numbers):', papers)\n\n# Print results\nprint_ranked_papers(rank_result, top_n=2, show_abstract=True, show_sentences=True)","fd262d05":"# =============================================================================\n# CASE 3e (third subquestion): Experimental infections to test host range for this pathogen\n# =============================================================================\npapers, rank_result = rank('Experimental infections to test host range for this pathogen.')\n\n# Print final candidates\nprint('Top 10 papers (document numbers):', papers)\n\n# Print results\nprint_ranked_papers(rank_result, top_n=1, show_abstract=True, show_sentences=False)","ab5d7acd":"# =============================================================================\n# CASE 4: Animal host(s) and any evidence of continued spill-over to humans\n# =============================================================================\nmust_have_word = ['animal']\npapers, rank_result = rank('Animal host(s) and any evidence of continued spill-over to humans', must_have_word)\n\n# Print final candidates\nprint('Top 10 papers (document numbers):', papers)\n\n# Print results\nprint_ranked_papers(rank_result, top_n=1, show_abstract=True, show_sentences=True)","32aacc65":"# =============================================================================\n# CASE 5: Socioeconomic and behavioral risk factors for this spill-over\n# =============================================================================\npapers, rank_result = rank('Socioeconomic and behavioral risk factors for this spill-over')\n\n# Print final candidates\nprint('Top 10 papers (document numbers):', papers)\n\n# Print results\nprint_ranked_papers(rank_result, top_n=5, show_abstract=False, show_sentences=False)","2a40e722":"# =============================================================================\n# CASE 6: Sustainable risk reduction strategies\n# =============================================================================\npapers, rank_result = rank('Sustainable risk reduction strategies')\n\n# Print final candidates\nprint('Top 10 papers (document numbers):', papers)\n\n# Print results\nprint_ranked_papers(rank_result, top_n=10, show_abstract=False, show_sentences=False)","9c1acc34":"# Track memory usage","59ae04b0":"# Task\n\nWhat do we know about virus genetics, origin, and evolution?\n\n# Goal\n\nCreate a search engine that provides for a researcher the most relevant papers from a large database of documents (about 30000) in a quick overview and really fast.\n\n\n# Team Darvirian\n-\tHenry Bol (notebook)\n-\tJoseph Ambrose Paragan\n-\tYao Yao\n\n\n# Overview and structure\n\nMajor focus has been given to make it for a **researcher** as *easy and relevant* as possible:\n1. The researcher can write a search sentence as a normal sentence (the engine filters this to relevant search words) \n1. The researcher has the option to give a 'must-have-word': a dominant word in the search query \n1. A word cloud of the sentences containing search words gives a validation of the value 'in the blink of an eye'\n1. The number of search words in the paper gives another impression of the focus of the document\n1. The abstract is provided (if available) with the search words in red\n1. The sentences of the paper which contain search words are shown with the search word in green\nThe solution is generic and any question considering the set of Covid-9 paper can be asked\n\nSpecial attention has been given to the speed of providing the results:\n- the loading (once) of the preprocessed dictionary takes about a minute \n- afterwards respons for a normal search sentence is immediate\n- even case 3 with the full text (68 words) gives a respons within a few seconds\n\n\n# Example case 3c (first subquestion)\nResearch question: 'Evidence of whether farmers are infected, and whether farmers could have played a role in the origin.'\n\nAs a must-have-word is chosen: 'farmer'.\n\nOverall result:\n* Found search words: ['evidence', 'whether', 'farmers', 'whether', 'farmers', 'could', 'played', 'role']\n* Ranked papers (document numbers): [11340, 11264, 14524, 22844, 2644, 27620, 1532]\n\nRESULT 2: Human-livestock contacts and their relationship to transmission of zoonotic pathogens, a systematic review of literature-NC-ND license (http:\/\/creativecommons.org\/licenses\/by-nc-nd\/4.0\/)\n\nWordcloud from sentences in paper containing search words giving an immediate impression of the search value:\n![Unknown-3.png](attachment:Unknown-3.png)\n\nSearch words (#), paper ID and authors: \n![Screenshot%202020-04-21%20at%2008.22.22.png](attachment:Screenshot%202020-04-21%20at%2008.22.22.png)\n\nPart of abstract with search words in red:\n![Screenshot%202020-04-21%20at%2008.12.06.png](attachment:Screenshot%202020-04-21%20at%2008.12.06.png)\n\nPart of sentences with search words in green:\n![Screenshot%202020-04-21%20at%2008.16.58.png](attachment:Screenshot%202020-04-21%20at%2008.16.58.png)\n\n# 3 Implementation\n\n### Parameters and relevance\n\nSeveral parameters can be set to select the output from a question (\u2018search sentence\u2019):\n- The top n most relevant papers \n- Show the sentences per paper in which search words occur\n- Show the abstract or not (if the abstarct is available) \n\nThe **most relevant** is rule-based and uses an *intelligent combination* of several metrics: \n* Do the search words in the question appear in the same order and are these located next to each other?\n* How many of the words in the question are found in a paper (100% is the best score)?\n* How many times do the words occur in a document?\n* What is the TF-IDF score for each word in each document?\n\nThese metrics are weighted (normalized) and summed to a Grand Score. The top 10 ranking documents are presented.\n\n\n### Preprocessing\nThe data is preprocessed: tokenize, lemmatize, remove stopwords, vectorize (embedding), calculate TF-IDF, positions of words in documents, word2idx and idx2word. To reduce the size of the number of words single characters and words that occur only once in all documents are removed.\nMain output is the **dictionary worddic** with:\n* KEY: word\n* VALUES: list of doc indexes where the word occurs plus per doc: word position(s) and tfidf-score\nThis makes it possible for the search and rank engine to determine the ranking scores.\n\nSee: https:\/\/github.com\/HenryBol\/darvirian\/blob\/master\/Darvirian_Preprocessing_Engine.py\n\nThe output of the prepocessing step is loaded via pickle files in this engine.\n\n### Further improvements\n* Tuning of the combination of ranking rules to futher increase relevance\n* Take care of other languages than English\n* Make production version\n* The preprocessed pickle files are using a lot of RAM in the Kaggle notebook\n\n# References\n\n### Github\n\nhttps:\/\/github.com\/HenryBol\/darvirian\n\n### Credits\n* Inspiration: https:\/\/www.kaggle.com\/amitkumarjaiswal\/nlp-search-engine\n* CSV files from: https:\/\/www.kaggle.com\/xhlulu\/cord-19-eda-parse-json-and-generate-clean-csv\n\n\n\n# Contents\n* PART I: Load the data\n* PART II: The Search Engine: function 'search'\n* PART III: The Rank Engine: function 'rank'\n* PART IV: Function: 'search_sentence'\n* PART V: Function: 'print_ranked_papers'\n* PART VI: Examples\n* PART VII: Case EUvsVirus\n* PART VIII: Cases Kaggle CORD-19\n\n# Cases Kaggle CORD-19\n* CASE 1: Real-time tracking of whole genomes\n* CASE 2: Access to geographic and temporal diverse sample sets\n* CASE 3a (first sentence): Evidence that livestock could be infected\n* CASE 3b (full text): Evidence that livestock could be infected\n* CASE 3c (first subquestion): Evidence of whether farmers are infected, and whether farmers could have played a role in the origin\n* CASE 3d (second subquestion): Surveillance of mixed wildlife- livestock farms for SARS-CoV-2 and other coronaviruses in Southeast Asia\n* CASE 3e (third subquestion): Experimental infections to test host range for this pathogen\n* CASE 4: Animal host(s) and any evidence of continued spill-over to humans\n* CASE 5: Socioeconomic and behavioral risk factors for this spill-over\n* CASE 6: Sustainable risk reduction strategies","5a49658c":"![Human_Vaccin.jpeg](attachment:Human_Vaccin.jpeg)","46e9a69b":"# PART VI: Example","9a8f8215":"# PART VII: Case EUvsVirus","0331f8a7":"# PART VIII: Cases Kaggle CORD-19","c95a3f85":"# PART IV: Function 'print_ranked_papers'","0fd74c50":"# Import the libraries","f655f240":"# PART I: Load data","89a36858":"# PART II: The Search Engine","ebc8e524":"# PART V: Function 'search_sentence'","4b03c811":"# PART III: The Rank Engine"}}