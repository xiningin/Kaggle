{"cell_type":{"6add14e1":"code","079b1af7":"code","edcd3e09":"code","04e83ddd":"code","afc3081a":"code","51218121":"code","1b5b432d":"code","bf042617":"code","15715c89":"code","cbc9db45":"markdown","42e772f8":"markdown","6ab2acb1":"markdown","cd1d45d9":"markdown","778eab2a":"markdown","473b6293":"markdown","9f844b49":"markdown","555bfabd":"markdown","770ec01c":"markdown","cdd71045":"markdown","9283bafb":"markdown","bdfada79":"markdown","7e9c9031":"markdown"},"source":{"6add14e1":"import numpy as np\nimport pandas as pd\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.utils.validation import check_X_y, check_array","079b1af7":"path = \"\/kaggle\/input\/nlp-getting-started\/\"\ntrain_df = pd.read_csv(path + \"train.csv\")\nsubmit_df = pd.read_csv(path + \"test.csv\")\n\ny_train = train_df['target']\ny_train.unique()\n\ntrain_df.head()","edcd3e09":"vocabulary = []\n_ = [vocabulary.extend(x.split()) for i,x in enumerate(train_df['text'])]","04e83ddd":"vocabulary = np.array(vocabulary)\nvocab = np.unique(vocabulary)\n\nprint(\"Vocab:\", len(vocab))","afc3081a":"vectorizer = CountVectorizer(vocabulary=vocab)\nword_counts = vectorizer.fit_transform(train_df.text.to_numpy()).toarray()\n\nX_train = pd.DataFrame(word_counts, columns=vocab).to_numpy()","51218121":"class NaiveBayes():\n    def __init__(self, alpha=1.0):\n        self.prior = None\n        self.word_counts = None\n        self.lk_word = None\n        self.alpha = alpha\n        \n    def fit(self, x, y):\n        '''\n        Fit the features and the labels\n        Calculate prior, word_counts and lk_word\n        '''\n        x, y = check_X_y(x, y)\n        n = x.shape[0]\n        \n        # calculate the prior - number of text belonging to a particular class (real or fake)\n        x_per_class = np.array([x[y == c] for c in np.unique(y)])\n        self.prior = np.array([len(x_class) \/ n for x_class in x_per_class])\n        \n        # calculate the likelihood for each word 'lk_word'\n        self.word_counts = np.array([sub_arr.sum(axis=0) for sub_arr in x_per_class]) + self.alpha\n        self.lk_word = self.word_counts \/ self.word_counts.sum(axis=1).reshape(-1, 1)\n        \n        return self\n    \n    def _get_class_numerators(self, x):\n        '''\n        Calculate for each class, the likelihood that an entire message conditional\n        on the message belonging to a particular class (real or fake)\n        '''\n        n, m = x.shape[0], self.prior.shape[0]\n        \n        class_numerators = np.zeros(shape=(n, m))\n        for i, word in enumerate(x):\n            word_exists = word.astype(bool)\n            lk_words_present = self.lk_word[:, word_exists] ** word[word_exists]\n            lk_message = (lk_words_present).prod(axis=1)\n            class_numerators[i] = lk_message * self.prior\n        \n        return class_numerators\n    \n    def _normalized_conditional_probs(self, class_numerators):\n        '''\n        Conditional probabilities = class_numerators \/ normalize_term\n        '''\n        # normalize term is the likelihood of an entire message (addition of all words in a row)\n        normalize_term = class_numerators.sum(axis=1).reshape(-1,1)\n        conditional_probs = class_numerators \/ normalize_term\n        assert(conditional_probs.sum(axis=1) - 1 < 0.001).all(), 'rows should sum to 1'\n        \n        return conditional_probs\n    \n    def predict_proba(self, x):\n        '''\n        Return the probabilities for each class (fake or real)\n        '''\n        class_numerators = self._get_class_numerators(x)\n        conditional_probs = self._normalized_conditional_probs(class_numerators)\n        \n        return conditional_probs\n    \n\n    def predict(self, x):\n        '''\n        Return the answer with the highest probability (argmax)\n        '''\n        return self.predict_proba(x).argmax(axis=1)","1b5b432d":"NaiveBayes().fit(X_train, y_train).predict(X_train)","bf042617":"submit_df.head()","15715c89":"# word counts for submit using vectorizer from sklearn\nword_counts_submit = vectorizer.fit_transform(submit_df.text.to_numpy()).toarray()\n\nsubmit_x = pd.DataFrame(word_counts_submit, columns=vocab).to_numpy()\n\n# use naive bayes to predict the submission\nresult = NaiveBayes().fit(X_train, y_train).predict(submit_x)\n\nfinal_result = list(map(list, zip(submit_df.id.to_numpy(), result)))\n\nfinal_df = pd.DataFrame(final_result, columns=['id', 'target'])\nfinal_df.to_csv(\"submission.csv\",index=False)","cbc9db45":"## Create the vocabulary, the word count and prepare the train data","42e772f8":"# Definition","6ab2acb1":"Thank you and don't forget to up-vote in order to support the community ;)","cd1d45d9":"# Load the data","778eab2a":"Naive Bayes is a classification model based on Bayes Theorem.\nBayes Theorem universal example is to classify email messages between spam and ham.\n\nThe equations of Naive Bayes for this 'Real or Not?' is:\n\nP(real | text) = (P(text | real) * P(real)) \/ P(text)<br>\nP(fake | text) = (P(text | fake) * P(fake)) \/ P(text)","473b6293":"The goal of this notebook is to learn what is the Naive Bayes algorithm by implementing from the scratch the sklearn MultinomialNB class.","9f844b49":"|  Variable      | Math     |  Description                                                      |\n|----------------|----------|-------------------------------------------------------------------|\n| prior          | P(y)     |  Probability of any random selected message belonging to a class  |\n| ik_word        | P(Xi&#124;y)  |  Likelihood of each word, conditional on message class            |\n| lk_message     | P(x&#124;y)   |  Likelihood of an entire message belonging to a particular class  |\n| normalize_term | P(x)     |  Likelihood of an entire message across all possible classes      |","555bfabd":"<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style=\"color:#02288b; background:#c50244; border:1px dashed;\" role=\"tab\" aria-controls=\"home\"><center>Submission<\/center><\/h3>","770ec01c":"<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style=\"color:#02288b; background:#c50244; border:1px dashed;\" role=\"tab\" aria-controls=\"home\"><center>Implementation<\/center><\/h3>","cdd71045":"<div style=\"height:200px;\">\n    <img src=\"https:\/\/storage.googleapis.com\/kaggle-media\/competitions\/nlp1-cover.jpg\"\/>\n<\/div>","9283bafb":"<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style=\"color:#02288b; background:#c50244; border:1px dashed;\" role=\"tab\" aria-controls=\"home\"><center>Intro<\/center><\/h3>","bdfada79":"# Goal","7e9c9031":"<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style=\"color:#02288b; background:#c50244; border:1px dashed;\" role=\"tab\" aria-controls=\"home\"><center>Data<\/center><\/h3>"}}