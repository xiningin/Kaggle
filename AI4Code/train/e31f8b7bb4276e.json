{"cell_type":{"4127617d":"code","6db5e43b":"code","d852b63a":"code","3d5ca48d":"code","fc93ec10":"code","94119298":"code","e7050642":"code","69e09fe9":"code","afb4071f":"code","b4612df3":"code","b77c83e0":"code","157e7d16":"code","0d1ea742":"code","093ce274":"code","ed0fc5e8":"code","986d42a4":"code","67ab7b7e":"code","b955b924":"code","f49e8f08":"code","53811638":"code","242a5ca2":"code","056ec038":"code","7afd6f4f":"code","6618b59f":"code","0ec794be":"code","8e492b9a":"code","f380dbff":"code","d1604772":"code","957263a4":"code","d170dd06":"code","af8819c0":"code","8896959e":"code","212e8793":"code","9d11376e":"code","ae9e068a":"code","22cffa2e":"markdown","a56c1fb7":"markdown","be8a449f":"markdown","249a4090":"markdown","540b054c":"markdown","a1efe308":"markdown","f98ca016":"markdown","641db5ec":"markdown","7602d911":"markdown","01e483e2":"markdown","4c4da080":"markdown","2c8f1eea":"markdown","cbe1226e":"markdown","973060be":"markdown","922efe6a":"markdown","8515acd9":"markdown"},"source":{"4127617d":"import os\nimport json\nimport re\nimport nltk\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom wordcloud import WordCloud, STOPWORDS\nfrom spacy.lang.en import English\nfrom collections import defaultdict","6db5e43b":"pd.options.display.max_rows=100","d852b63a":"nlp=English()\nwordcloud=WordCloud(stopwords=STOPWORDS, \n                    width=600,\n                    height=300,\n                    background_color='white',\n                    #max_font_size=50,\n                    #max_words=100\n                   )","3d5ca48d":"%%time\npub_df=pd.read_csv('..\/input\/publication-datasets\/publication_dataset.csv')\npub_df.head()","fc93ec10":"pub_df.num_positive_samples.describe()","94119298":"print(\"90% of the publications had {} sentences which mentions the datalabels\".format(pub_df.num_positive_samples.quantile(q=0.9)))\n","e7050642":"plt.figure(figsize=(12, 5))\nplt.xticks(rotation=45)\nsns.countplot(data=pub_df, x='num_positive_samples')\nplt.show()","69e09fe9":"pub_df['pos_neg_ratio']=pub_df.num_positive_samples.div(pub_df.num_negative_samples)\npub_df.head()","afb4071f":"pub_df[pub_df.num_negative_samples==0]","b4612df3":"pub_df[pub_df.num_negative_samples>0].pos_neg_ratio.describe()","b77c83e0":"pub_df.head()","157e7d16":"pos_sentences=[]\nfor sentences in pub_df.sentences.values:\n    sentences=eval(sentences)\n    pos_sents=sentences['pos_sents']\n    pos_sentences+=pos_sents\n    \nprint(len(pos_sentences))","0d1ea742":"%%time\nall_sentence_text=''\nfor sent in pos_sentences:\n    all_sentence_text+=sent.lower()+\" \"","093ce274":"wordcloud_image=wordcloud.generate(all_sentence_text)\nplt.imshow(wordcloud_image)\nplt.show()","ed0fc5e8":"def get_sent_tokens(sent):\n    doc=nlp(sent)\n    tokens=[]\n    for token in doc:\n        if token.is_stop or token.is_punct or token.is_digit:\n            continue\n        token=token.lower_.strip()\n        if len(token)<=2:\n            continue\n        tokens.append(token)\n    return tokens","986d42a4":"pos_sent_df=pd.DataFrame.from_dict({'sentence': pos_sentences})\npos_sent_df['tokens']=pos_sent_df.sentence.apply(get_sent_tokens)\n\npos_sent_df.head()","67ab7b7e":"unigrams=defaultdict(int)\nfor tokens in pos_sent_df.tokens:\n    for token in tokens:\n        unigrams[token]+=1\nunigram_df=pd.DataFrame.from_dict({\n    'word': list(unigrams.keys()),\n    'freq': list(unigrams.values())\n})\n\nunigram_df=unigram_df.sort_values('freq', ascending=False)\nunigram_df.head()","b955b924":"print(\"Number Of Unigrams:\", len(unigrams))\nprint(\"Number Of Unigrams >2 freq:\", unigram_df[unigram_df.freq>2].shape[0])\nprint(\"Number Of Unigrams >10 freq:\", unigram_df[unigram_df.freq>10].shape[0])","f49e8f08":"unigram_df[unigram_df.freq>5].freq.describe()","53811638":"plt.hist(unigram_df[unigram_df.freq>5].freq, bins=100)\nplt.show()","242a5ca2":"unigram_df=unigram_df[unigram_df.freq>10].copy()\nunigram_df.head()","056ec038":"unigram_df.head(20)","7afd6f4f":"unigram_df.tail(20)","6618b59f":"def get_context_tokens(sentence, direction, w=4):\n    sentence=sentence.strip()\n    tokens=[]\n    for token in nlp(sentence):\n        if not token.is_alpha:\n            continue\n        tokens.append(token.text)\n    if direction == -1:\n        return tokens[-w:]\n    return tokens[:w]\n\n\ndef get_context(row):\n    dataset_labels=eval(row['dataset_label'])\n    sentences=eval(row['sentences'])['pos_sents']\n    \n    context={\n        'left': [],\n        'right': []\n    }\n    for sentence in sentences:\n        for dl in dataset_labels:\n            for match in re.finditer(dl, sentence):\n                start=match.start()\n                end=match.end()\n                \n                left_sentence=sentence[:start]\n                right_sentence=sentence[end:]\n                \n                left_context=get_context_tokens(left_sentence, -1)\n                right_context=get_context_tokens(right_sentence, 1)\n                \n                context['left'].append(left_context)\n                context['right'].append(right_context)\n    return context","0ec794be":"pub_df['context']=pub_df.apply(get_context, axis=1)\ncontext_df=pd.DataFrame()\ncontext_df['left_context']=pub_df['context'].apply(lambda context: context['left'])\ncontext_df['right_context']=pub_df['context'].apply(lambda context: context['right'])\n\npub_df.head()","8e492b9a":"\ncontext_df.head(10)","f380dbff":"context_words=defaultdict(int)\nleft_context_words=defaultdict(int)\nright_context_words=defaultdict(int)\n\nall_context_text=''\nleft_context_text=''\nright_context_text=''\n\nfor left_context in context_df.left_context.values:\n    for words in left_context:\n        for word in words:\n            word=word.lower()\n            context_words[word]+=1\n            left_context_words[word]+=1\n            left_context_text+=word+\" \"\n            all_context_text+=word+\" \"\n\nfor right_context in context_df.right_context.values:\n    for words in right_context:\n        for word in words:\n            word=word.lower()\n            context_words[word]+=1\n            right_context_words[word]+=1\n            right_context_text+=word+\" \"\n            all_context_text+=word+\" \"\n\nprint(\"Number Of Context Words\", len(context_words))\nprint('Number Of Left Context Words:', len(left_context_words))\nprint('Number Of Right Context Words:', len(right_context_words))\n\ncontext_word_df=pd.DataFrame.from_dict({\n    'word': list(context_words.keys()),\n    'freq': list(context_words.values())\n})\n\nleft_context_word_df=pd.DataFrame.from_dict({\n    'word': list(left_context_words.keys()),\n    'freq': list(left_context_words.values())\n})\n\nright_context_word_df=pd.DataFrame.from_dict({\n    'word': list(right_context_words.keys()),\n    'freq': list(right_context_words.values())\n})\n\ncontext_word_df=context_word_df.sort_values('freq', ascending=False)\nleft_context_word_df=left_context_word_df.sort_values('freq', ascending=False)\nright_context_word_df=right_context_word_df.sort_values('freq', ascending=False)\ncontext_word_df.head(20)","d1604772":"context_word_df=context_word_df[(context_word_df.freq>10) & (context_word_df.freq<800)].copy()\ncontext_word_df.freq.describe()","957263a4":"context_word_df.head(20)","d170dd06":"all_context_wc=wordcloud.generate(all_context_text)\n\nplt.figure(figsize=(15, 4))\nplt.title('All Context')\nplt.imshow(all_context_wc)\nplt.show()\n","af8819c0":"def get_context_bigrams(context):\n    ctx_bigrams=[]\n    for ctx_list in context:\n        if len(ctx_list)<=1:\n            continue\n        for bg in nltk.bigrams(ctx_list):\n            ctx_bigrams.append( ' '.join(bg).lower() )\n    return ctx_bigrams","8896959e":"context_df['left_bigrams']=context_df.left_context.apply(get_context_bigrams)\ncontext_df['right_bigrams']=context_df.right_context.apply(get_context_bigrams)\ncontext_df.head()\n","212e8793":"context_bigrams=defaultdict(int)\n\nfor bgs in context_df.left_bigrams.values:\n    for bg in bgs:\n        context_bigrams[bg]+=1\n\nfor bgs in context_df.right_bigrams.values:\n    for bg in bgs:\n        context_bigrams[bg]+=1\n\ncontext_bigrams_df=pd.DataFrame.from_dict({\n    'bigram': list(context_bigrams.keys()),\n    'freq': list(context_bigrams.values())\n})\n\ncontext_bigrams_df=context_bigrams_df.sort_values('freq', ascending=False)\nprint('Number Of Context Bigrams:', len(context_bigrams))\ncontext_bigrams_df.head(30)","9d11376e":"context_bigrams_df[context_bigrams_df.bigram.apply(lambda x: 'sample' in x)].head()","ae9e068a":"context_bigrams_df[context_bigrams_df.bigram.apply(lambda x: 'taken' in x)].head()","22cffa2e":"# Unigrams\n","a56c1fb7":"EDA on the datalabels\n\nhttps:\/\/www.kaggle.com\/narendra\/datalabel-eda","be8a449f":"Positive Samples-sentences on which the dataset labels match.\n\nNegative Samples-sentences on which the dataset labels did not match.","249a4090":"# lets see how many positive samples are there in publications","540b054c":"# let us see the the ratio of the postive to negative samples","a1efe308":"By combining the Context-level features and lexical-level(word shapes, cap-letters etc.) we can understand get the candidate phrases of the dataset.","f98ca016":"Now that we had the positive samples; lets us see the words that appear in the context of the positive samples","641db5ec":"In the above word cloud observed that most words are releavant to the seach of datasets but dominated by some of the words that appear most in publications\n\nDominated Words from publications:\n1. alzheimer\n2. education\n3. neuroimaging\n\nContext words\n1. dataset\n2. sample\n3. et al\n4. cohert\n5. database\n6. obtained, study, using etc.","7602d911":"we can see from the above that, we need to focus on the terms\/phrases that are specific to datasets like\nextraction, sampling etc.\n\nwhere as some of the phrases are highly specific to the content of the publication.","01e483e2":"In most cases number of positive sentences is <1% compared to the negative sentences.","4c4da080":"# get only the context word around the datalabel(window size=5)","2c8f1eea":"taking a look at some common bigrams\n\n1. taken from (may be from left)\n2. were taken (may be from right)\n3. sampled from \n4. from the\n5. on the.\n6. et al, etc...","cbe1226e":"one publication had all the sentences as positive samples","973060be":"# word freq > 10","922efe6a":"The above unigrams represents the topics generated in the sentences.\nbut seems noisy when considering the context.\n\nTo get the context lets mask the document labels from the sentences and the get the context words in the window.","8515acd9":"looks like the top unigrams have words related mostly the prepositions, and related areas of interst\n\nlets remove more frequent and less frequent unigrams\n>10 and <800"}}