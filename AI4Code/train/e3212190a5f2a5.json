{"cell_type":{"fc99adf3":"code","9078ea33":"code","bb2f607f":"code","8c35468e":"code","289184bb":"code","eb4a2b41":"code","c9216a96":"code","a1a79642":"markdown","89bfe320":"markdown","8a99f863":"markdown","cc9f97d3":"markdown","85fb4c5c":"markdown","165ff88b":"markdown"},"source":{"fc99adf3":"########\n#some eda from https:\/\/www.kaggle.com\/danofer\/loading-sarcasm-data\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom subprocess import check_output\nfrom matplotlib import pyplot as plot\nimport os\nimport numpy as np\nimport xgboost\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nfrom sklearn import preprocessing, metrics, svm","9078ea33":"train = pd.read_csv(\"..\/input\/train-balanced-sarcasm.csv\")\nprint(train.shape)\nprint(train.columns)\n\n#drop rows that have missing comments\ntrain.dropna(subset=['comment'], inplace=True)\n\n# Parse UNIX epoch timestamp as datetime: \ntrain.created_utc = pd.to_datetime(train.created_utc,infer_datetime_format=True) # Applies to original data , which had UNIX Epoch timestamp! \n\ntrain.describe()\n\n########\ntrain['label'].hist() #50 50 split\n\n##see a sample of comments\ntrain['comment'].sample(10)\ntrain[train.label == 1][\"comment\"].sample(10).tolist()","bb2f607f":"#how many comments are in each subreddit?\ntrain.groupby([\"subreddit\"]).count()[\"comment\"].sort_values()\n\n#learn more about the subreddits and the frequency of sarcastic labels\nsub_df = train.groupby('subreddit')['label'].agg([np.size, np.mean, np.sum])\nsub_df.sort_values(by='sum', ascending=False).head(10)\nsub_df[sub_df['size'] > 1000].sort_values(by='mean', ascending=False).head(10)\n\n#learn more about authors and the frequency of sarcastic labels\nauthor_df = train.groupby('author')['label'].agg([np.size, np.mean, np.sum])\nauthor_df.sort_values(by='sum', ascending=False).head(10)\nauthor_df[author_df['size'] > 250].sort_values(by='mean', ascending=False).head(10)\n","8c35468e":"#split the df into training and validation parts\ntrain_texts, valid_texts, y_train, y_valid = \\\n        train_test_split(train['comment'], train['label'], random_state=17)\n        \nprint(train_texts.shape, valid_texts.shape, y_train.shape, y_valid.shape)\n\n'''\n#take small sample for testing\ntrain_texts_small = train_texts.sample(600, random_state=27)\ny_train_small = y_train.sample(600, random_state=27)\nvalid_texts_small = valid_texts.sample(600, random_state=27)\ny_valid_small = y_valid.sample(600, random_state=27)\n'''\n\n","289184bb":"%%time\n#Consider porter stemming\nimport nltk, re\ndef Tokenizer(str_input):\n    words = re.sub(r\"[^A-Za-z0-9\\-]\", \" \", str_input).lower().split()\n    #words = re.search(r'\\w{1,}',str_input).lower().split()\n    porter_stemmer=nltk.PorterStemmer()\n    words = [porter_stemmer.stem(word) for word in words]\n    return words\n\n\ndef train_model(classifier, feature_vector_train, label, feature_vector_valid, label_valid,is_neural_net=False):\n    # fit the training dataset on the classifier\n    classifier.fit(feature_vector_train, label)\n    \n    # predict the labels on validation dataset\n    predictions = classifier.predict(feature_vector_valid)\n    \n    if is_neural_net:\n        predictions = predictions.argmax(axis=-1)\n    \n    return metrics.accuracy_score(predictions, label_valid)\n    return metrics.classification_report(predictions, label_valid)\n\n\n#count vectors\n'''\ncount_vect = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(1, 3), max_features=50000, min_df=2, lowercase=True, max_df=0.9)\ncount_vect.fit(train['comment'])\n\n#count vectors\n# transform the training and validation data using count vectorizer object\nxtrain_count =  count_vect.transform(train_texts)\nxvalid_count =  count_vect.transform(valid_texts)\n'''\n\n# word and n-gram level tf-idf\n#tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=50000, ngram_range=(1,3), min_df=2, lowercase=True, max_df=0.9)\ntfidf_vect = TfidfVectorizer(analyzer='word', tokenizer=Tokenizer, max_features=50000, ngram_range=(1,2), min_df=2, lowercase=True, max_df=0.95)\ntfidf_vect.fit(train['comment'])\nxtrain_tfidf =  tfidf_vect.transform(train_texts)\nxvalid_tfidf =  tfidf_vect.transform(valid_texts)","eb4a2b41":"%%time\n#try XGBoost on word- and ngram-level vectors\n#accuracy = train_model(xgboost.XGBClassifier(), xtrainSVD, y_train, xvalidSVD)\naccuracy = train_model(xgboost.XGBClassifier(n_estimators=400), xtrain_tfidf.tocsc(), y_train, xvalid_tfidf.tocsc(), y_valid)\nprint(\"Xgb, WordLevel TF-IDF: \", accuracy)\n\n#try XGBoost on count vectors\n#accuracy = train_model(xgboost.XGBClassifier(n_estimators=400), xtrain_count.tocsc(), y_train, xvalid_count.tocsc())\n#print(\"Xgb, Count Vectors: \", accuracy)\n","c9216a96":"%%time\n#logistic regression\naccuracy = train_model(LogisticRegression(solver='lbfgs', random_state=17, max_iter=1000), xtrain_tfidf, y_train, xvalid_tfidf, y_valid)\nprint(\"Logistic Regression: \", accuracy)\n\n","a1a79642":"# Sarcasm Detection Using XGBoost and Logistic Regression","89bfe320":"##Logistic Regression Classifier","8a99f863":"##Preprocessing with NLTK","cc9f97d3":"##XGBoost Classifier","85fb4c5c":"Porter Stemming was inspired by the following source: \nhttps:\/\/medium.com\/@chrisfotache\/text-classification-in-python-pipelines-nlp-nltk-tf-idf-xgboost-and-more-b83451a327e0\n\nThe classifier function's source is the following: \nhttps:\/\/www.analyticsvidhya.com\/blog\/2018\/04\/a-comprehensive-guide-to-understand-and-implement-text-classification-in-python\/","165ff88b":"##Data Exploration\n\nRepeating a portion of the data exploration from YURY KASHNITSKY: https:\/\/www.kaggle.com\/kashnitsky\/a4-demo-sarcasm-detection-with-logit-solution"}}