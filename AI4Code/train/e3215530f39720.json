{"cell_type":{"6d48fd12":"code","2b1587e4":"code","f2fd670c":"code","0a96eaf1":"code","38d6d000":"code","18ce3231":"code","41961591":"code","91e4e45c":"code","9fd0df6f":"code","8e2a0dd4":"code","31622413":"code","3317aeae":"code","83a16bfb":"code","6db1214f":"code","96d8f302":"code","89a8ca5f":"code","a1b28012":"code","844bc9ea":"code","fb440d34":"code","3ad1ae79":"code","2cdbc966":"code","f9e63d6c":"code","273ebbf5":"code","a885c26c":"code","49e4e02b":"code","a6ff7b1b":"code","6179c66d":"code","4e8aaf5f":"code","4dd6704e":"code","cf67b31a":"code","638c90b2":"code","852e9a29":"code","8fefe6fe":"code","7aa3381c":"code","cddb51c6":"code","216e7da5":"code","0f5da12b":"code","ed76a06d":"code","e511b426":"code","7ca0cd76":"code","3854ef42":"code","6b18506f":"code","ac924501":"code","741b12ce":"code","1cf3bd22":"code","6f9a4de3":"code","6ede14de":"code","23e4f7b7":"code","ff9dc005":"code","d3d20172":"code","bda0ccd8":"code","e2d712e5":"code","461199b2":"code","2357494b":"code","a383bbdf":"code","22b9614e":"code","c90c23df":"code","b1306cb2":"code","c9440081":"code","54b6a4f2":"code","cba1b98b":"code","37248dbf":"code","0a363d0d":"code","2b047d48":"code","4dba457d":"code","88623f40":"code","1fde4e04":"code","62d45eeb":"code","4c85d960":"code","cbcaf23a":"code","5280320a":"code","c2e405f1":"code","3ff3e626":"code","71a8d188":"code","33a8b5e0":"code","a1ada3d6":"code","345eb76e":"code","5a71e84b":"code","da2f10f8":"code","f7b192c1":"code","898be2de":"code","6119e577":"code","80e66d9b":"code","29409299":"markdown","5355254d":"markdown","9afbc921":"markdown","ac33010c":"markdown","3efb40c4":"markdown","f859fa5c":"markdown","351a37f7":"markdown","2ef14264":"markdown","42d7b4fc":"markdown","e5acb8f1":"markdown","14d18629":"markdown","56dc7e94":"markdown","14adbfb9":"markdown","84670bba":"markdown","7cdddde8":"markdown","8a233977":"markdown","aaa5c3f5":"markdown","e1488fc9":"markdown","ebbce993":"markdown","55d2398a":"markdown","41c739dc":"markdown","47513e2b":"markdown","fbb9dc00":"markdown","c78d1068":"markdown","30db3eda":"markdown","cff87a89":"markdown","4fe413b3":"markdown","74256e77":"markdown","ebdebb0c":"markdown","5c05e84c":"markdown","012b7e2d":"markdown","0328d903":"markdown","70b393da":"markdown","3ca89659":"markdown","1cb47658":"markdown","4fee12fe":"markdown","4fc7f626":"markdown","0239dd1f":"markdown","b501896f":"markdown","0dda86fb":"markdown","4623841d":"markdown","46679904":"markdown","3215468a":"markdown","a2a3783e":"markdown","1f35ed2e":"markdown","2d18b821":"markdown","6a54a6ba":"markdown","1bd20507":"markdown","3ac7e075":"markdown","2977aec7":"markdown","c01698ad":"markdown","cbd62bde":"markdown","911058dd":"markdown","1aa34a08":"markdown","cf06fe7f":"markdown","b0fe85a6":"markdown","fc2ac106":"markdown","64004940":"markdown","bc59293c":"markdown","322adaa6":"markdown","4aa7b3aa":"markdown"},"source":{"6d48fd12":"#Author: Caleb Woy\n!python --version\n# preprocessing\nimport pandas as pd # data manipulation\nimport numpy as np # linear algebra\nfrom numpy.random import seed # setting random seeds\nfrom scipy.stats import kurtosis, skew, t # checking distributions, t score\nimport matplotlib.pyplot as plt # plotting\nimport math # checking for NaNs\nimport seaborn as sb # plotting\nimport operator # indexing for max\n# modelling\nimport scipy.spatial.distance as sp # kNN distance\nfrom sklearn.model_selection import train_test_split # validation\nfrom sklearn.model_selection import RandomizedSearchCV # tuning hyperparams \nfrom sklearn.model_selection import GridSearchCV # tuning hyperparams \nfrom sklearn.linear_model import LogisticRegression # Logistic Regression\nfrom sklearn import tree # Decision tree\nfrom sklearn.ensemble import RandomForestClassifier as rfc # Random Forest\nfrom sklearn.ensemble import GradientBoostingClassifier as gbc # GBC\nfrom sklearn.ensemble import AdaBoostClassifier as ada_boost # generic boosting\nfrom sklearn.neighbors import KNeighborsClassifier as knn # K nearest neighbors\nfrom sklearn.naive_bayes import MultinomialNB as mnb # Naive Bayes\nfrom sklearn.naive_bayes import GaussianNB as gnb # Naive Bayes\nimport tensorflow as tf # neural networks\nfrom keras.wrappers.scikit_learn import KerasClassifier # neural networks\nfrom keras.models import Sequential  # neural networks\nfrom keras.layers import Dense, Activation, Dropout  # neural networks","2b1587e4":"train_data = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntest_data = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\n# appending \ndata = train_data.append(test_data)\ndata.head()","f2fd670c":"data.dtypes","0a96eaf1":"data['Pclass'] = data['Pclass'].astype('category')\ndata['Cabin'] = data['Cabin'].astype('category')\ndata['Embarked'] = data['Embarked'].astype('category')\ndata['Sex'] = data['Sex'].astype('category')\ndata.dtypes","38d6d000":"\"\"\"\nCreating interaction plots between two categorical variables.\n\"\"\"\ndef interaction_plot(hue_lab, x_lab, data):\n    # Grouping by the hue_group, then counting by the x_lab group\n    hue_group = data.groupby([hue_lab], sort=False)\n    counts = hue_group[x_lab].value_counts(normalize=True, sort=False)\n\n    # Creating the percentage vector to measure the frequency of each type\n    data = [\n        {hue_lab: hue, x_lab: x, 'percentage': percentage*100} for \n        (hue, x), percentage in dict(counts).items()\n    ]\n\n    # Creating and plotting the new dataframe \n    df = pd.DataFrame(data)\n    p = sb.catplot(x=x_lab, y='percentage', hue=hue_lab, kind=\"point\", data=df);\n    p.set_xticklabels(rotation=90)\n    p.fig.suptitle(f'Interaction of {x_lab} ~ {hue_lab}')\n \n\n\"\"\"\nExtracting the title of an individual from their full name.\n\"\"\"\ndef extract_title(string_name, titles):\n    for title in titles:\n        if (title in string_name):\n            break\n    return title\n\n\"\"\"\nExtracting the last name of an individual from their full name.\n\"\"\"\ndef extract_last_name(string_name):\n    return string_name.split(',')[0]\n\n\"\"\"\nCreating box plots between x numeric and y categorical.\n\"\"\"\ndef box_plot(x, y, data):\n    ax = sb.boxplot(x=data[x], y=data[y])\n    ax.set_xticklabels(ax.get_xticklabels(),rotation=-90)\n\n\"\"\"\nReturn a float of the suspected family's mortality rate. -1 of unknown or solo\npassenger, else return rate of mortality.\n\"\"\"\ndef get_fam_mort(last_name, data, row):\n    mort = 0.0\n    if row['Parch'] == 0 and row['SibSp'] == 0:\n        # unknown\n        mort = -1 \n    else:\n        fam_rows = data[(data['LastName'] == last_name) & \n                        (data['Parch'] > 0) & (data['SibSp'] > 0)]\n        if len(fam_rows) == 0:\n            # unknown\n            mort = -1\n        else:\n            # mortality rate\n            mort = len(fam_rows[fam_rows['Survived'] == 0]) \/ len(fam_rows)\n    return mort\n\n\n\"\"\"\nImputes the missing age values based on the mean age value of the sub group \nmatching the class of the individual.\n\"\"\"\ndef impute_age_by_class(Pclass, data, row):\n    if math.isnan(row['Age']):\n        return np.mean(data[data['Pclass'] == Pclass]['Age'])\n    else:\n        return row['Age']\n    \n\"\"\"\nImputes the missing fare values based on the mean fare value of the sub group \nmatching the class of the individual. Also replaces fare values equal \nto 0 that're believed to be erroneous.\n\"\"\"\ndef impute_fare_by_class(Pclass, data, row):\n    if math.isnan(row['Fare']) or row['Fare'] == 0:\n        return np.mean(data[data['Pclass'] == Pclass]['Fare'])\n    else:\n        return row['Fare']\n\n\"\"\"\nMin-max scales the column.\n\"\"\"\ndef min_max(feature_name, data):\n    min_d, max_d = min(data[feature_name]), max(data[feature_name])\n    return data.apply(lambda x: (x[feature_name] - min_d) \/ (max_d - min_d),\n                    axis = 1)","18ce3231":"# setting the feature name we're examining so that we don't have to re-type it\nfeature_name = \"Pclass\"\n# getting the count of each factor level of Pclass\ndata.groupby(feature_name).count()[\"PassengerId\"]","41961591":"data[feature_name].isnull().sum()","91e4e45c":"interaction_plot(\"Survived\", feature_name, data)","9fd0df6f":"# setting the feature name we're examining so that we don't have to re-type it\nfeature_name = \"Name\"\n# Checking for missing values\ndata[feature_name].isnull().sum()","8e2a0dd4":"# list of all personal titles in our data set\ntitles = [\"Jonkheer.\", \"Countess.\",\"Lady.\",\"Sir.\",\"Dona.\", \"Mme.\", \"Master.\", \n          \"Rev.\", \"Don.\", \"Dr.\", \"Major.\", \"Col.\", \"Capt.\", \"Mr.\", \"Mrs.\", \n          \"Miss.\", \"Ms.\", \"Mlle.\"]\n# calling .apply() with the argument axis=1 will apply the lambda function\n# argument to each row of the column. The variable x in the lambda represents a \n# row. The funcion extract_title is defined above in our function definitions\n# cell. We'll save this as a new feature on our DataFrame called 'Title'\ndata['Title'] = data.apply(lambda x:\n                                extract_title(x[feature_name], titles), axis=1)\n# verifying\ndata.head()","31622413":"# this apply call is very similar to the previous. extract_last_name is \n# also defined above in our function definitions cell. We'll save this as a \n# new feature on our DataFrame called 'LastName'\ndata['LastName'] = data.apply(lambda x:\n                                extract_last_name(x[feature_name]), axis=1)\n# verifying\ndata.head()","3317aeae":"data[['LastName']].describe()","83a16bfb":"# this apply call is very similar to the previous. get_fam_mort is \n# also defined above in our function definitions cell. We'll save this as a \n# new feature on our DataFrame called 'FamMort'\ndata['FamMort'] = data.apply(lambda x:\n                        get_fam_mort(x['LastName'], \n                        data[~data[\"Survived\"].isna()], \n                        x),\n                        axis=1)\n# Individuals that are likely single passengers are encoded as '-1.0', all\n# others are encoded as the percentage of likely family members in the \n# training data that perished. We don't want the '-1.0' to have numeric \n# meaning so we're setting this column as a category.\ndata['FamMort'] = data['FamMort'].astype('category')\n\n# verifying\ndata.head()","6db1214f":"interaction_plot(\"Survived\", 'Title', data)","96d8f302":"box_plot(\"FamMort\",\"Survived\", data)","89a8ca5f":"data[['FamMort']].describe()","a1b28012":"# setting the feature name we're examining so that we don't have to re-type it\nfeature_name = \"Age\"\n# Checking for missing values\ndata[feature_name].isnull().sum()","844bc9ea":"correlation_matrix = data.corr().round(2)\nplt.figure(figsize=(10,8))\nsb.heatmap(data=correlation_matrix, annot=True, center=0.0, cmap='coolwarm')","fb440d34":"box_plot(\"Pclass\",\"Age\", data)","3ad1ae79":"# The impute_age_by_class function is defined above in our function\n# definitons. \ndata[feature_name] = data.apply(lambda x: impute_age_by_class(x['Pclass'], \n                                                data,\n                                                x), axis = 1)","2cdbc966":"# Now verifying that there are no more null values\ndata[feature_name].isnull().sum()","f9e63d6c":"# Next we'll graph the distribution to check for skewness.\nx = data[feature_name]\nplt.hist(x, bins=25)\nplt.xlabel('age')\nplt.ylabel('Frequency')","273ebbf5":"new_feat = f'{feature_name}_scaled'\ndata[new_feat] = min_max(feature_name, data)\ndata.head()","a885c26c":"# Next we'll make a box plot to get an idea of how significant the feature\n# will be for predicting survival.\nbox_plot(\"Survived\",\"Age_scaled\", data)","49e4e02b":"# setting the feature name we're examining so that we don't have to re-type it\nfeature_name = \"Fare\"\n# Checking for missing valeus\ndata[feature_name].isnull().sum()","a6ff7b1b":"data[data[feature_name].isnull()]","6179c66d":"box_plot(\"Pclass\",\"Fare\", data)","4e8aaf5f":"# The impute_fare_by_class function is defined above in our function\n# definitons. It also works the same as the impute_age_by_class function.\ndata[feature_name] = data.apply(lambda x: impute_fare_by_class(x['Pclass'], \n                                                data,\n                                                x), axis = 1)","4dd6704e":"# Next we'll graph the distribution to check for skewness.\nx = data[feature_name]\nplt.hist(x, bins=25)\nplt.xlabel('Fare')\nplt.ylabel('Frequency')","cf67b31a":"log_fare = np.log(data[feature_name])\nx = log_fare\nplt.hist(x, bins=30)\nplt.xlabel('log_Fare')\nplt.ylabel('Frequency')","638c90b2":"data['log_Fare'] = log_fare\nnew_feat = f'log_{feature_name}_scaled'\ndata[new_feat] = min_max('log_Fare', data)\ndata.head()","852e9a29":"# Next we'll make a box plot to get an idea of how significant the feature\n# will be for predicting survival.\nbox_plot(\"Survived\",\"log_Fare_scaled\", data)","8fefe6fe":"# setting the feature name we're examining so that we don't have to re-type it\nfeature_name = \"Cabin\"\n# checking for missing values\ndata[feature_name].isnull().sum()","7aa3381c":"# setting the feature name we're examining so that we don't have to re-type it\nfeature_name = \"Embarked\"\n# checking for missing values\ndata[feature_name].isnull().sum()","cddb51c6":"data[data[feature_name].isnull()]","216e7da5":"ax = sb.boxplot(x=data['Embarked'], y=data['Fare'])\nax.set_xticklabels(ax.get_xticklabels(),rotation=-90)\nax.set(ylim=(0, 100))","0f5da12b":"interaction_plot(\"Embarked\", 'Pclass', data)","ed76a06d":"data[feature_name] = data[feature_name].fillna('C')","e511b426":"data[feature_name].isnull().sum()","7ca0cd76":"# Next we'll make a box plot to get an idea of how significant the feature\n# will be for predicting survival.\ninteraction_plot(\"Survived\", 'Embarked', data)","3854ef42":"# setting the feature name we're examining so that we don't have to re-type it\nfeature_name = \"Sex\"\n# checking for missing values\ndata[feature_name].isnull().sum()","6b18506f":"interaction_plot(\"Survived\", 'Sex', data)","ac924501":"# setting the feature name we're examining so that we don't have to re-type it\nfeature_name = \"SibSp\"\n# checking for missing values\ndata[feature_name].isnull().sum()","741b12ce":"data[feature_name].describe()","1cf3bd22":"# lets take a look at the graph of the distribution. \nx = data[feature_name]\nplt.hist(x, bins=8)\nplt.xlabel('SibSp')\nplt.ylabel('Frequency')","6f9a4de3":"# I'll scale the data now.\ndata[f'{feature_name}_scaled'] = min_max(feature_name, data)","6ede14de":"data.head()","23e4f7b7":"# Next we'll make a box plot to get an idea of how significant the feature\n# will be for predicting survival.\nbox_plot(\"Survived\",\"SibSp_scaled\", data)","ff9dc005":"# setting the feature name we're examining so that we don't have to re-type it\nfeature_name = \"Parch\"\n# checking for missing values\ndata[feature_name].isnull().sum()","d3d20172":"data[feature_name].describe()","bda0ccd8":"x = data[feature_name]\nplt.hist(x, bins=9)\nplt.xlabel('Parch')\nplt.ylabel('Frequency')","e2d712e5":"# I'll scale the data now.\ndata[f'{feature_name}_scaled'] = min_max(feature_name, data)","461199b2":"data.head()","2357494b":"box_plot(\"Survived\",\"Parch_scaled\", data)","a383bbdf":"# dummies\nX_dummies = pd.get_dummies(data[[\"Parch_scaled\", \"log_Fare_scaled\", \"FamMort\", \n                                 \"Title\", \"Pclass\", \"Embarked\", \"Sex\"]])\n# splitting\nX_dummies_train = X_dummies.iloc[0:890]\nX_dummies_test = X_dummies.iloc[891:]\n\n# creating labels column\nY = data.iloc[0:890][\"Survived\"]\n# creating dataframe to store predictions\ntest_frame = pd.DataFrame(data.iloc[891:][\"PassengerId\"])","22b9614e":"# creating categorical frames\nX_dummies_cat = X_dummies_train.drop([\"Parch_scaled\", \"log_Fare_scaled\"],\n                                     axis = 1)\nX_dummies_cat_t = X_dummies_test.drop([\"Parch_scaled\",  \"log_Fare_scaled\"], \n                                      axis = 1)\n# creating numeric frames\nX_num = X_dummies_train[[\"Parch_scaled\", \"log_Fare_scaled\"]]\nX_num_t = X_dummies_test[[\"Parch_scaled\", \"log_Fare_scaled\"]]","c90c23df":"# creating data frames to store our predicted probabilities in. We can add\n# predicted probabilites to this as we model and then analyze Mean Squared\n# Error at the end to evaluate them.\ntrain_result_frame = pd.DataFrame(Y)\ntest_result_frame = pd.DataFrame()","b1306cb2":"# first step is setting up cross validation. The function train_test_split\n# will randomly split our training set into training and a validation set. \n# validation sets are used to get an idea of how well the model will\n# perform on unseen data.\nx_train, x_valid, y_train, y_valid = train_test_split(\n    X_dummies_train, Y, test_size=0.25, random_state=0)","c9440081":"# fitting\nclf = tree.DecisionTreeClassifier().fit(x_train, y_train)\n# checking accuracy on validation set\nclf.score(x_valid, y_valid)","54b6a4f2":"# fitting on entire training set\nclf = tree.DecisionTreeClassifier().fit(X_dummies_train, Y)\n# predicting on the test set\ntest_frame[\"Survived\"] = clf.predict(X_dummies_test).astype('int32')\n# writing to a file\ntest_frame.to_csv('\/kaggle\/working\/decision_tree.csv', index = False)\n\n# storing the predicted probabilities in dataframe for MSE analysis\ntrain_result_frame['dt'] = [x[1] for x in clf.predict_proba(X_dummies_train)]\ntest_result_frame['dt'] = [x[1] for x in clf.predict_proba(X_dummies_test)]","cba1b98b":"# creating training and validation sets\nx_train, x_valid, y_train, y_valid = train_test_split(\n    X_dummies_train, Y, test_size=0.25, random_state=0)","37248dbf":"# fitting \nlogr = LogisticRegression(solver = 'lbfgs').fit(x_train, y_train)\n# checking validation accuracy\nlogr.score(x_valid, y_valid)","0a363d0d":"# fitting to full training data\nlogr = LogisticRegression(solver = 'lbfgs').fit(X_dummies_train, Y)\n# predicting the test set\ntest_frame[\"Survived\"] = logr.predict(X_dummies_test).astype('int32')\n# writing results to a file\ntest_frame.to_csv('\/kaggle\/working\/logistic_reg.csv', index = False)\n\n# storing the predicted probabilities in dataframe for MSE analysis\ntrain_result_frame['logr'] = [x[1] for x in \n                                  logr.predict_proba(X_dummies_train)]\ntest_result_frame['logr'] = [x[1] for x in \n                                 logr.predict_proba(X_dummies_test)]","2b047d48":"# the number of random samples and decision trees to build\nn_estimators = [100, 200, 500, 600, 750, 1000, 1250]\n# the min number of samples allowed at each decision tree node (controls\n# against overfitting)\nmin_samples_split = [5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55]\n# the min number of samples allowed at each decision tree leaf (controls\n# against overfitting)\nmin_samples_leaf = [5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55]\n# placing the parameter values into a dictionary\ngrid_param = {'n_estimators': n_estimators,\n              'min_samples_split': min_samples_split,\n              'min_samples_leaf': min_samples_leaf}\n\n# initializing the model with a random state\nrandom_forest = rfc(random_state=4538756)\n\n# initializing the cross validator \nRFC_random = RandomizedSearchCV(estimator = random_forest, \n                             param_distributions = grid_param,\n                             n_iter = 120,\n                             verbose=2,\n                             cv = 5,\n                             random_state = 857436,\n                             n_jobs = -1)\n# starting the tuner\nRFC_random.fit(X_dummies_train, Y)\nprint(RFC_random.best_params_)\nprint(f'score: {RFC_random.best_score_}')","4dba457d":"# initializing the mdoel using best selected hyper-parameters\nrandom_forest = rfc(n_estimators = 750, min_samples_split = 10, \n                    min_samples_leaf = 5, random_state = 27394652)\n# fitting to entire training set\nrandf = random_forest.fit(X_dummies_train, Y)\n# predicting the test set\ntest_frame[\"Survived\"] = randf.predict(X_dummies_test).astype('int32')\n# writing to a file\ntest_frame.to_csv('\/kaggle\/working\/random_forest.csv', index = False)\n\n# storing the predicted probabilities in dataframe for MSE analysis\ntrain_result_frame['randf'] = [x[1] for x in \n                                  randf.predict_proba(X_dummies_train)]\ntest_result_frame['randf'] = [x[1] for x in \n                                 randf.predict_proba(X_dummies_test)]","88623f40":"# the number of trees to build\nn_estimators = [100, 250, 500, 750, 1000, 1250]\n# the learning rate to apply with every tree built\nlearning_rate = [0.01, 0.05, 0.1, 0.2, 0.3]\n# the min number of samples allowed at each decision tree node (controls\n# against overfitting)\nmin_samples_split = [20, 25, 30, 35, 40, 45, 50, 55]\n# the min number of samples allowed at each decision tree leaf (controls\n# against overfitting)\nmin_samples_leaf = [20, 25, 30, 35, 40, 45, 50, 55]\n# placing the parameter values into a dictionary\ngrid_param = {'n_estimators': n_estimators,\n              'learning_rate': learning_rate,\n              'min_samples_split': min_samples_split,\n              'min_samples_leaf': min_samples_leaf}\n\n# initialize model with a random state\ngradient_boosted_trees = gbc(random_state=24352345)\n# initialize the cross validator\nGB_random = RandomizedSearchCV(estimator = gradient_boosted_trees, \n                             param_distributions = grid_param,\n                             n_iter = 120,\n                             verbose=2,\n                             cv = 5,\n                             random_state = 47567,\n                             n_jobs = -1)\n# starting the tuner\nGB_random.fit(X_dummies_train, Y)\n# print result\nprint(GB_random.best_params_)\nprint(f'score: {GB_random.best_score_}')","1fde4e04":"# initializing the mdoel using best selected hyper-parameters\ngrad_booster = gbc(n_estimators = 1000, min_samples_split = 45, \n                    min_samples_leaf = 20, learning_rate = 0.05, \n                   random_state = 34737)\n# fitting to entire training set\ngradb = grad_booster.fit(X_dummies_train, Y)\n# predicting the test set\ntest_frame[\"Survived\"] = gradb.predict(X_dummies_test).astype('int32')\n# writing to a file\ntest_frame.to_csv('\/kaggle\/working\/gradient_boosted_trees.csv', index = False)\n\n# storing the predicted probabilities in dataframe for MSE analysis\ntrain_result_frame['gbc'] = [x[1] for x in \n                                  gradb.predict_proba(X_dummies_train)]\ntest_result_frame['gbc'] = [x[1] for x in \n                                 gradb.predict_proba(X_dummies_test)]","62d45eeb":"# define the values of k to test\nk = [5, 11, 15, 21, 25, 31, 35, 41, 45, 51, 55, 61, 65, 71, 75, 81, 85, 91]\n# placing the parameter values into a dictionary\ngrid_param = {'n_neighbors': k}\n\n# initialize model with given distance metric\nmodel = knn(metric = 'jaccard')\n# initialize the cross validator\nKNN_random = GridSearchCV(estimator = model, \n                             param_grid = grid_param,\n                             verbose = 2,\n                             cv = 5,\n                             n_jobs = -1)\n# begin tuning\nKNN_random.fit(X_dummies_cat, Y)\n# print results\nprint(KNN_random.best_params_)\nprint(f'score: {KNN_random.best_score_}')","4c85d960":"# initializing the mdoel using best selected hyper-parameters\nknn_grad = knn(metric = 'jaccard', n_neighbors = 11)\n# fitting to entire training set\nknn_c = knn_grad.fit(X_dummies_cat, Y)\n# predicting the test set\ntest_frame[\"Survived\"] = knn_c.predict(X_dummies_cat_t).astype('int32')\n# writing to a file\ntest_frame.to_csv('\/kaggle\/working\/knn_categorical.csv', index = False)\n\n# storing the predicted probabilities in dataframe for MSE analysis\ntrain_result_frame['knn_cat'] = [x[1] for x in \n                                  knn_c.predict_proba(X_dummies_cat)]\ntest_result_frame['knn_cat'] = [x[1] for x in \n                                 knn_c.predict_proba(X_dummies_cat_t)]","cbcaf23a":"# define the values of k to test\nk = [5, 11, 15, 21, 25, 31, 35, 41, 45, 51, 55, 61, 65, 71, 75, 81, 85, 91]\n# placing the parameter values into a dictionary\ngrid_param = {'n_neighbors': k}\n\n# initialize model with given distance metric\nmodel = knn(metric = 'euclidean')\n# initialize the cross validator\nKNN_random = GridSearchCV(estimator = model, \n                             param_grid = grid_param,\n                             verbose = 2,\n                             cv = 5,\n                             n_jobs = -1)\n# begin tuning\nKNN_random.fit(X_num, Y)\n# print results\nprint(KNN_random.best_params_)\nprint(f'score: {KNN_random.best_score_}')","5280320a":"# initializing the mdoel using best selected hyper-parameters\nknn_grad = knn(metric = 'euclidean', n_neighbors = 61)\n# fitting to entire training set\nknn_c = knn_grad.fit(X_num, Y)\n# predicting the test set\ntest_frame[\"Survived\"] = knn_c.predict(X_num_t).astype('int32')\n# writing to a file\ntest_frame.to_csv('\/kaggle\/working\/knn_numeric.csv', index = False)\n\n# storing the predicted probabilities in dataframe for MSE analysis\ntrain_result_frame['knn_num'] = [x[1] for x in \n                                  knn_c.predict_proba(X_num)]\ntest_result_frame['knn_num'] = [x[1] for x in \n                                 knn_c.predict_proba(X_num_t)]","c2e405f1":"# fitting K-NN numeric with the best selected hyperparameters\nknn_num = knn(metric = 'euclidean', n_neighbors = 61)\nknn_num.fit(X_num, Y)\n# fitting K-NN categorical with the best selected hyperparameters\nknn_cat = knn(metric = 'jaccard', n_neighbors = 11)\nknn_cat.fit(X_dummies_cat, Y)\n\n# creating a data frame to store the probablities in\nframe = pd.DataFrame(Y)\n# saving calculated probabilities for the numeric training data\nframe[\"num_prob\"] = [x[1] for x in knn_num.predict_proba(X_num)]\n# saving calculated probabilities for the categorical training data\nframe[\"cat_prob\"] = [x[1] for x in knn_cat.predict_proba(X_dummies_cat)]\n\n# define the values of k to test\nk = [5, 11, 15, 21, 25, 31, 35, 41, 45, 51, 55, 61, 65, 71, 75, 81, 85, 91]\n# placing the parameter values into a dictionary\ngrid_param = {'n_neighbors': k}\n\n# initialize model with given distance metric\nknn_full = knn(metric = 'euclidean')\n# initialize the cross validator\nKNN_random = GridSearchCV(estimator = knn_full, \n                             param_grid = grid_param,\n                             verbose = 2,\n                             cv = 5,\n                             n_jobs = -1)\n# begin tuning\nKNN_random.fit(frame[[\"num_prob\", \"cat_prob\"]], frame[\"Survived\"])\n# print results\nprint(KNN_random.best_params_)\nprint(f'score: {KNN_random.best_score_}')","3ff3e626":"# fitting K-NN numeric with the best selected hyperparameters\nknn_num = knn(metric = 'euclidean', n_neighbors = 61)\nknn_num.fit(X_num, Y)\n# fitting K-NN categorical with the best selected hyperparameters\nknn_cat = knn(metric = 'jaccard', n_neighbors = 11)\nknn_cat.fit(X_dummies_cat, Y)\n\n# creating a data frame to store the training probablities in\nframe = pd.DataFrame(Y)\n# saving calculated probabilities for the numeric training data\nframe[\"num_prob\"] = [x[1] for x in knn_num.predict_proba(X_num)]\n# saving calculated probabilities for the categorical training data\nframe[\"cat_prob\"] = [x[1] for x in knn_cat.predict_proba(X_dummies_cat)]\n\n# creating a data frame to store the test probablities in\nframe_test = pd.DataFrame()\n# saving calculated probabilities for the numeric test data\nframe_test[\"num_prob\"] = [x[1] for x in knn_num.predict_proba(X_num_t)]\n# saving calculated probabilities for the categorical test data\nframe_test[\"cat_prob\"] = [x[1] for x in knn_cat.predict_proba(X_dummies_cat_t)]\n\n# fitting full K-NN with the best selected hyperparameters\nknn_full = knn(metric = 'euclidean', n_neighbors = 31)\nknn_full = knn_full.fit(frame[[\"num_prob\", \"cat_prob\"]], frame[\"Survived\"])\n\n# predicting on the test set probabilities\ntest_frame[\"Survived\"] = knn_full.predict(\n    frame_test[[\"num_prob\", \"cat_prob\"]]\n).astype('int32')\n# saving the predictions\ntest_frame.to_csv('\/kaggle\/working\/knn_full.csv', index = False)\n\n# storing the predicted probabilities in dataframe for MSE analysis\ntrain_result_frame['knn'] = [x[1] for x in \n                                  knn_full.predict_proba(\n                                      frame[[\"num_prob\", \"cat_prob\"]])]\ntest_result_frame['knn'] = [x[1] for x in \n                                 knn_full.predict_proba(\n                                     frame_test[[\"num_prob\", \"cat_prob\"]])]","71a8d188":"def create_neural_net(in_shape, lyrs=[4], act='relu', opt='Adam', dr=0.0):\n    # set random seed for reproducibility\n    seed(37556)\n    tf.random.set_seed(37556)\n    \n    # initialize the model object\n    model = Sequential()\n    \n    # create first hidden layer\n    model.add(Dense(lyrs[0], input_dim=in_shape, activation=act))\n    \n    # create additional hidden layers\n    for i in range(1,len(lyrs)):\n        model.add(Dense(lyrs[i], activation=act))\n    \n    # add dropout, default is none\n    model.add(Dropout(dr))\n    \n    # create output layer\n    model.add(Dense(1, activation='sigmoid'))  # output layer\n    \n    model.compile(loss='binary_crossentropy', optimizer=opt,\n                  metrics=['accuracy'])\n    \n    return model","33a8b5e0":"# creating a neural network\nsingle_net = create_neural_net(X_dummies_train.shape[1], lyrs =[4])\nsingle_net.summary()","a1ada3d6":"# fitting the network to the training data with arbitrary hyper-parameters.\n# this is just an example to show off some cool things keras can do.\ntraining = single_net.fit(X_dummies_train, Y, epochs=100, batch_size=32,\n                         validation_split=0.25, verbose=0)\n# outputting the validation accuracy\nval_acc = np.mean(training.history['val_accuracy'])\nprint(\"\\n%s: %.2f%%\" % ('val_acc', val_acc*100))","345eb76e":"# plotting the training history\nplt.plot(training.history['accuracy'])\nplt.plot(training.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.show()","5a71e84b":"# We need this specific version of scikit-learn for hyperparameter tuning \n!pip install scikit-learn==0.21.2","da2f10f8":"# wrap the nerual network in an sklearn class so we can plug it into \n# RandomizedSearchCV\nneural_net = KerasClassifier(build_fn=create_neural_net, \n                             in_shape = X_dummies_train.shape[1], verbose = 0)\n# defining the batch size\nbatch_size = [1, 16, 32, 64]\n# defining the number of epochs\nepochs = [25, 50, 100, 150]\n# defining the size and number of layers\nlayers = [[4], [8], [12], [4, 4], [8, 4], [8, 8], [12, 8], [12, 4], [12, 8, 4]]\n# defining the rate of the dropout layer\ndrops = [0.0, 0.01, 0.05, 0.1, 0.2, 0.5]\n\n# initialize model with given hyper-parameters\ngrid_param = {'batch_size': batch_size,\n             'epochs': epochs,\n             'dr': drops,\n            'lyrs': layers}\n# initialize the cross validator\nnn_random = RandomizedSearchCV(estimator = neural_net, \n                             param_distributions = grid_param,\n                             n_iter = 200,\n                             verbose=2,\n                             cv = 5,\n                             random_state = 456745,\n                             n_jobs = -1)\n# begin tuning\nnn_random.fit(X_dummies_train, Y)\n\n# print results\nprint(nn_random.best_params_)\nprint(f'score: {nn_random.best_score_}')","f7b192c1":"# initializing the mdoel using best selected hyper-parameters\nnn = KerasClassifier(build_fn=create_neural_net, \n                        in_shape = X_dummies_train.shape[1],\n                        lyrs=[12, 8, 4], epochs=50, dr=0.1, batch_size=1, \n                         verbose=0)\n# fitting to entire training set\nnn.fit(X_dummies_train, Y)\n# predicting the entire test set\ntest_frame[\"Survived\"] = nn.predict(X_dummies_test).astype('int32')\n# writing to a file\ntest_frame.to_csv('\/kaggle\/working\/nn.csv', index = False)\n\n# storing the predicted probabilities in dataframe for MSE analysis\ntrain_result_frame['nn'] = [x[1] for x in \n                                  nn.predict_proba(X_dummies_train)]\ntest_result_frame['nn'] = [x[1] for x in \n                                 nn.predict_proba(X_dummies_test)]","898be2de":"test_result_frame.to_csv('\/kaggle\/working\/test_results.csv', index = False)\n#test_result_frame = pd.read_csv('.\/result_data\/test_results.csv')\n\ntrain_result_frame.to_csv('\/kaggle\/working\/train_results.csv', index = False)\n#train_result_frame = pd.read_csv('.\/result_data\/train_results.csv')\ntrain_result_frame.head()","6119e577":"mse = {col: 0 for col in train_result_frame.columns if col != \"Survived\"}\nfor col in mse:\n    error = mse[col]\n    for _,row in train_result_frame[['Survived', col]].iterrows():\n        error += math.pow(row['Survived'] - row[col], 2)\n    error \/= train_result_frame.shape[0]\n    mse[col] = error","80e66d9b":"for k, v in sorted(mse.items(), key=lambda item: item[1]):\n    print(f'{k:10} MSE: {mse[k]:.3f}')","29409299":"# Data Exploration, Pre-processing, & Feature Engineering\nHere we'll get to know our data and make it digestable for models later on.","5355254d":"Observation: The '-1.0' values have a fairly even mix of Survived and Perished observations. Almost all individuals with 0.0% percent FamMort survived but there are some outliers (the dot). Then as FamMort gets closer to 100.0% the pattern reverses. Non '-1.0' categories appear to be negatively correlated with survival.","9afbc921":"## Decision Tree\nA decision tree is probably the simlest model we can apply.\nThey are explained in the following reference:\n<\/br>https:\/\/en.wikipedia.org\/wiki\/Decision_tree_learning","ac33010c":"Both individuals paid 80.0 for fare. Based on Embarked factor levels they are most liekly 'C'.","3efb40c4":"## Neural Network\nNow we'll use the above function to create a neural network and tune the hyper-parameters. For more information on Neural Networks check the following reference:\n<\/br>https:\/\/en.wikipedia.org\/wiki\/Artificial_neural_network","f859fa5c":"## Sex \/ categorical","351a37f7":"These are essentially the same. We won't be using this during modelling.","2ef14264":"Observation: There are about as many 3rd class passengers as there are 1st and 2nd class passengers combined. Next we'll check to see if there are any missing values for this feature.","42d7b4fc":"## Defining a Neural Network\nThe first step of building and training a Neural Network is defining one. The function create_neural_net will do just that for us. All layers we add will use the Relu activation function, except the output layer which will use the sigmoid activation function. \n<\/br>For more information on the Relu activation function check the following reference:\n<\/br>https:\/\/en.wikipedia.org\/wiki\/Rectifier_(neural_networks)\n<\/br>For more information on the Sigmoid activation function check the following reference:\n<\/br>https:\/\/en.wikipedia.org\/wiki\/Sigmoid_function\n<\/br>We'll also use the Adam optimizer which implements and adaptive learning rate. For more information on the Adam optimizer check the following reference:\n<\/br>https:\/\/medium.com\/@nishantnikhil\/adam-optimizer-notes-ddac4fd7218\n<\/br>We'll also add a dropout layer to help prevent overfitting. For more information on dropout check the following reference:\n<\/br>https:\/\/www.cs.toronto.edu\/~hinton\/absps\/JMLRdropout.pdf","e5acb8f1":"Now we'll check the data types of the columns to make sure that they're all represented properly.","14d18629":"Observation: Fare is very right skewed. \n<\/br>Skewness is explained in the following reference:<\/br> https:\/\/en.wikipedia.org\/wiki\/Skewness\n<\/br>We should apply the log transformation to this feature to make it more normal.","56dc7e94":"Observation: Probably pretty significant. The means and quartiles vary across groups.","14adbfb9":"# Titantic Survival","84670bba":"Both individuals are first class. Based on Embarked they're likely 'C'. We'll imput ethe value 'C' for both of those observations.","7cdddde8":"## Random Forest\nA random forest is a very powerful ensemble model. It uses a decision tree as its underlying model and a applies a method know as bagging (bootstrap aggregation). Bagging works by taking many random samples with replacement from the training data. A model (decision tree) is fitted to every random sample. The predicted result is the majority voted classification of the underlying trees. The advantage bagging gives is that it reduces variance without adding bias. For more information check this reference:\n<\/br>https:\/\/en.wikipedia.org\/wiki\/Random_forest\n<\/br>For more information on the bias-variance tradeoff check this reference:\n<\/br>https:\/\/en.wikipedia.org\/wiki\/Bias%E2%80%93variance_tradeoff\n<\/br>![image.png](attachment:image.png)","8a233977":"Observation: Looks fairly normal. Not really skewed enough to justify any transformations.\n<\/br>Next we'll min-max scale our numeric variable. We're going to do this to every nuemric variable so that they're all scaled the same.\n<\/br>Min-max scaling is calculated the same in this reference, which also describes why feature scaling is important: https:\/\/en.wikipedia.org\/wiki\/Feature_scaling\n<\/br>The min_max function is defined above in the function definitions cell.","aaa5c3f5":"Next we'll make an interaction plot to get an idea of how much Pclass effects the odds of survival on average. If the feature is insignificant then the line for Survived will be close to parallel with the line for death.","e1488fc9":"## Name \/Categorical, non-binary","ebbce993":"Next we'll split our training and testing data into dataframes containing only categorical and only numeric data. Some models can only take one type of data at a time, so in those cases we'll have to feed them in seperatly and use an aggregate model to get a prediction from both.","55d2398a":"Now that we've created some features let's analyze them with plots. We'll make an interaction plot for the 'Title' feature.","41c739dc":"## Pclass \/Categorical, non-binary","47513e2b":"# Model training & validation","fbb9dc00":"# Results\nNow that we're done with modelling we can analyze the MSE (Mean Squared Error) of each of our models. For more information on MSE check the following reference:\n<\/br>https:\/\/en.wikipedia.org\/wiki\/Mean_squared_error","c78d1068":"## Age \/Continuous","30db3eda":"## Cabin \/ Categorical","cff87a89":"These look fine. Both observations are first class individuals that paid the same fare. Let's take a look at those relationsips to see if we can impute based on them.","4fe413b3":"## Logistic Regression\nLogistic regression is essentially regular linear regression mapped onto the odds domain using the sigmoid activation function. Using this technique a regression model can be used to classify. See the following reference for more detail:\n<\/br> https:\/\/en.wikipedia.org\/wiki\/Logistic_regression","74256e77":"Observation: There are only two missing values. Let's take a look at those rows and see if they look erroneous.","ebdebb0c":"# Splitting into train and test\nNow, we'll split our data back up into the original train and test sets. We'll also select the most significant features that we found during preprocessing. First we'll create dummy variables so that our models can ingest our categorical data.\n<\/br>Dummy variables are explained in the following reference: <\/br>https:\/\/en.wikipedia.org\/wiki\/Dummy_variable_(statistics)","5c05e84c":"## Embarked \/ Categorical","012b7e2d":"The columns Pclass, Cabin, Embarked, and Sex are all non-numeric categorical features. So we'll set them as 'category' and replace them.","0328d903":"The most meaningful realtionship among these is between Age and class. We're seeing variation among the means of each class that suggests a trend. We can take the mean value of each class and impute that for our missing Age values based on the missing row's Pclass.","70b393da":"There are a lot of different unique names in our data set. However, each name has a prefix associated with it. There are way fewer unique values of those prefixes so we'll extract those and create a new feature to draw value from.","3ca89659":"These are fairly significant. It appears individuals with more parents on board were more likely to survive.","1cb47658":"## SibSp \/ Discrete numerical","4fee12fe":"This looks fairly significant. Women were more likely to survive.\n<\/br>Men were more likely to perish.","4fc7f626":"There are 263 missing values. The most basic way of handling these is to impute the mean or median Age. A more intermediate method of handling these is to impute the value based on a relationship with another feature. We'll first check to see if Age is correllated to any of the other numeric features.","0239dd1f":"## Parch \/ Numeric discrete","b501896f":"That looks slightly better. Next, we'll min_max scale it like we did last time.","0dda86fb":"Observation: The data is right skewed. However, we wont do a log transform this time because the data is discrete. ","4623841d":"## Gradient Boosted Trees\nGradient Boosting is another very powerful ensemble model. Here it uses decision trees again as the underlying model. Boosting, however, unlike bagging works sequentially instead of in parallel. It also utilizes random sampling with replacement. However the random sampling is weighted. For each random sample that is drawn and modelled and predicted the resulting residual errors are used as sample weights for the next sampling. This way the Gradient boosted model trains more on hard to predict observtations. Gradient boosting reduces bias without adding variance.<\/br>\nGradient boosted Trees should work better for the titanic problem because it represents the imbalanced class problem. The hard to predict observations are the Survived individuals because there are proportionally less than them than those that perished.\n<\/br>For more information on the imbalanced class problem check the following reference:\n<\/br>https:\/\/towardsdatascience.com\/class-imbalance-a-classification-headache-1939297ff4a4\n<\/br>For more information on gradient boosted trees check the following refrence:\n<\/br>https:\/\/en.wikipedia.org\/wiki\/Gradient_boosting","46679904":"I'll do a walkthrough of modelling the simplest classification models that apply to this situation. Then we'll do a Neural Net example.","3215468a":"# Function Definitions\nNext we'll define some functions that we may re-use. In data science anything you can duplicate and re-use should be made portable. This speeds up the process. ","a2a3783e":"We won't be using cabin because too many of the values are missing. Imputing these missing values based on only the 300 or so cabin observations in our training data would likely just add bias to our data.","1f35ed2e":"## K-Nearest Neighbors - Categorgical Jaccard\nK-NN is a very simple model. It takes in the training set, calculates a distance metric for each training observation, and stores it. For each new observation to predict, K-NN calculates the same distance metric and find the K neighbors in the training data with distance metrics closest to the distance metric of the new data. K-NN takes the majority class vote from those K neighbors and assigns it to the new data. \n<\/br>For more information on K-NN check the following reference:\n<\/br>https:\/\/en.wikipedia.org\/wiki\/K-nearest_neighbors_algorithm\n<\/br>K-NN is dependent on the distance metric. For that reason we shouldn't pass in both the numeric and categorical data at once. First we'll train and predict just the categorical data. We'll use the distance metric called the 'Jaccard Index'. It is built specifically for determining the similarity of two boolean vectors. \n<\/br>For more information on the Jaccard Index check the following reference:\n<\/br>https:\/\/en.wikipedia.org\/wiki\/Jaccard_index","2d18b821":"Observation: There is only one missing value. Let's take a look at that row and see if it looks erroneous.","6a54a6ba":"Random forest is a more complex model than the previous models that we've fit. We'll have to do hyper-parameter tuning. Hyper-parameters are the parameters of our model that act as a mode of control for how the model fits the data. Tuning is completed by varying the hyper-parameters and cross validating the model for each variation. To accomplish this we'll use RandomizedSearchCV from sklearn. It utilizes parallel processing to speed up the process.","1bd20507":"Most of the training data consists of individual passengers. Therefore this feaure may not be the most significant but should help with edge cases. ","3ac7e075":"# Loading Data\nHere we'll load in the train and test data sets to pandas DataFrames. Then\nwe'll append the test DataFrame to the end of the train DataFrame so that \nall analysis or tranformations we perform during pre-proccessing will be \nuniversal.","2977aec7":"## Fare \/ Continuous","c01698ad":"Observation: Many of the more specialized titles such as 'Countess', 'Lady', 'Mlle', 'Ms', and 'Sir' only show observations that survived. These titles are mostly women and\/or upper class titles. All 'Capt', 'Don', and 'Rev' observations perished. 'Mr' is largely the most common and fatal title. <\/br>Next, we'll make a box and whisker plot for the feature 'FamMort'.","cbd62bde":"Observation: The odds of survival seem to be effected by Pclass pretty significantly.","911058dd":"This looks fine. We can impute the value based on another variable like we did last time. If we scroll up to our correlation heatmap in the Age section, we can observe that Fare is again not very highly correlated with the other numeric features. Logically, it's most likely that fare is best explained by which Pclass the individual belongs to. Let's check that with a plot.","1aa34a08":"## K-Nearest Neighbors - Numeric Vector norm\nWe'll now train another K-NN model on only the Numeric data. This time we'll use the 'eclidean' distance metric. This K-NN model will calculate vector norms co compare data.\n<\/br>For more information on the Euclidean distance metric check the following reference:\n<\/br>https:\/\/en.wikipedia.org\/wiki\/Euclidean_distance","cf06fe7f":"This looks like a fairly significant relationsip.\n<\/br>'C' observations were more likely to survive.\n<\/br>'S' observations were more likely to perish.","b0fe85a6":"Observation: Probably not that significant. The means and first and thrid quartiles are all very similar to eachother across both groups.","fc2ac106":"There is probably more value we can etract from the name feature. There are probably many families among our passengers that most likely share last names. If we extract the last name of each passenger it may be useful for identifying family structures.  ","64004940":"Observation: The data is right skewed. However, again, we wont do a log transform this time because the data is discrete. ","bc59293c":"The most highly correlated numeric feature is SibSp. However this is not a strong enough relationship to base an imputation on. Next we can look at relationships between Age and the other categorical features.","322adaa6":"I want to look for names and create a feature based\non the percent mortality rate of that family in the training data.","4aa7b3aa":"## K-Nearest Neighbors - Combined Numeric and categorical\nNow we'll train K-NN models on the categorical data and the numeric data. We'll output their predicted probabilities to a new dataframe and then train a K-NN model using the euclidean distance metric. This will work because both features (Numeric prob) and (Categorical prob) are both in the probability domain and scaled the same."}}