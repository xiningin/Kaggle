{"cell_type":{"8a3c1acc":"code","3503d506":"code","49832263":"code","6e571135":"code","a3358927":"code","01c9ab52":"code","fda5cc1e":"code","0f96c8b2":"code","a59a0d1f":"code","a73ce73c":"code","35adafea":"code","2c685eb7":"code","64aa6983":"code","359babbd":"code","ef8e7d81":"code","d17aa385":"code","321f0509":"code","b096656c":"markdown","f3aeb2cc":"markdown","eff8a81f":"markdown"},"source":{"8a3c1acc":"# read the data\nimport pandas as pd\ntrain_df = pd.read_csv(\"..\/input\/nlp-getting-started\/train.csv\")\ntest_df = pd.read_csv(\"..\/input\/nlp-getting-started\/test.csv\")\ntrain_df.head()","3503d506":"train_df[\"text\"][79]","49832263":"# Shuffle training dataframe\ntrain_df_shuffled = train_df.sample(frac=1, random_state=42)\ntrain_df_shuffled.head()\n\n","6e571135":"# How many classes of each class?\ntrain_df.target.value_counts()","a3358927":"# Let's visualize some random training examples\nimport random\nrandom_index = random.randint(0, len(train_df)-5)\nfor row in train_df_shuffled[[\"text\", \"target\"]][random_index:random_index+5].itertuples():\n  _, text, target = row\n  print(f\"Target: {target}\", \"(real_disaster)\" if target > 0 else \"(not real disaster)\")\n  print(f\"Text:\\n{text}\\n\")\n  print(f\"---\\n\")","01c9ab52":"from sklearn.model_selection import train_test_split\n\n# Use train_test_split to split training data into training and validation sets\ntrain_sentences, val_sentences, train_labels, val_labels = train_test_split(train_df_shuffled[\"text\"].to_numpy(),\n                                                                            train_df_shuffled[\"target\"].to_numpy(),\n                                                                            test_size=0.1, # dedicate 10% of samples to validation set\n                                                                            random_state=42) # random state for reproducibility","fda5cc1e":"# Create test data\ntest_sentences = test_df[\"text\"]","0f96c8b2":"# We can use this encoding layer in place of our text_vectorizer and embedding layer\nsentence_encoder_layer = hub.KerasLayer(\"https:\/\/tfhub.dev\/google\/universal-sentence-encoder\/4\",\n                                        input_shape=[], # shape of inputs coming to our model \n                                        dtype=tf.string, # data type of inputs coming to the USE layer\n                                        trainable=False, # keep the pretrained weights (we'll create a feature extractor)\n                                        )","a59a0d1f":"# Create model using the Sequential API\nfrom tensorflow.keras import layers\nmodel = tf.keras.Sequential([\n  sentence_encoder_layer, # take in sentences and then encode them into an embedding\n  layers.Dense(64, activation=\"relu\"),\n  layers.Dense(1, activation=\"sigmoid\")\n], name=\"model_6_USE\")\n\n# Compile model\nmodel.compile(loss=\"binary_crossentropy\",\n                optimizer=tf.keras.optimizers.Adam(),\n                metrics=[\"accuracy\"])\n\nmodel.summary()","a73ce73c":"# Train a classifier on top of pretrained embeddings\nmodel_history = model.fit(train_sentences,\n                              train_labels,\n                              epochs=5,\n                              validation_data=(val_sentences, val_labels),\n                              )","35adafea":"# Predictions\nmodel_pred_probs = model.predict(val_sentences)\nmodel_preds = tf.squeeze(tf.round(model_pred_probs))","2c685eb7":"# Function to evaluate: accuracy, precision, recall, f1-score\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support\n\ndef calculate_results(y_true, y_pred):\n  \"\"\"\n  Calculates model accuracy, precision, recall and f1 score of a binary classification model.\n  \"\"\"\n  # Calculate model accuracy\n  model_accuracy = accuracy_score(y_true, y_pred) * 100\n  # Calculate precision, recall an f-score using \"weighted\" average\n  model_precision, model_recall, model_f1, _ = precision_recall_fscore_support(y_true, y_pred, average=\"weighted\")\n\n  model_results = {\"accuracy\": model_accuracy,\n                   \"precision\": model_precision,\n                   \"recall\": model_recall,\n                   \"f1\": model_f1}\n\n  return model_results","64aa6983":"# Calculate model's performance metrics\nmodel_results = calculate_results(val_labels, model_preds)\nmodel_results","359babbd":"# Predictions on test data\ntest_preds = tf.squeeze(tf.round(model.predict(test_sentences)))\ntest_preds = tf.cast(test_preds, tf.int32)\n","ef8e7d81":"submission = pd.DataFrame({\"id\": test_df[\"id\"],\"target\": test_preds})","d17aa385":"submission","321f0509":"submission.to_csv(\"submission.csv\", index=False)","b096656c":"#### We're going to be using the [Universal Sentence Encoder](http:\/\/aclanthology.org\/D18-2029.pdf) from [TensorFlow Hub](http:\/\/tfhub.dev\/google\/universal-sentence-encoder\/4) (a great resource containing a plethora of pretrained model resources for a variety of tasks).\n\n#### An encoder is the name for a model which converts raw data such as text into a numerical representation (feature vector), a decoder converts the numerical representation to a desired output.\n\n#### The improvement is clear compared to [baseline model (naive bayes)](http:\/\/www.kaggle.com\/kensjourney\/baseline-model-naive-bayes-01).","f3aeb2cc":"## Get data","eff8a81f":"## TensorFlow Hub Pretrained Sentence Encoder"}}