{"cell_type":{"374aabe8":"code","cce7d9ed":"code","81392eb7":"code","71537bd7":"code","618e09dc":"code","582c60c7":"code","f2f41d1b":"code","a9f7c79c":"code","6ff5a787":"code","08c60364":"code","909ba8f2":"code","20e35bd4":"code","8288f996":"markdown","50861bce":"markdown","ef051781":"markdown","f000e0c4":"markdown","ee670aa0":"markdown","7fc426bf":"markdown","71c736e3":"markdown","4a793f88":"markdown","476281df":"markdown","1825588b":"markdown"},"source":{"374aabe8":"!pip install iterative_stratification -q","cce7d9ed":"from fastai.vision.all import *\nimport numpy as np\nimport pandas as pd\nimport torch\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold\nimport warnings\nwarnings.filterwarnings('ignore')\ntorch.set_printoptions(precision=3, sci_mode=False)","81392eb7":"sample_size = 1\nseed = 42\nstats = ([0.07237246, 0.04476176, 0.07661699], [0.17179589, 0.10284516, 0.14199627])\nitem_tfms = RandomResizedCrop(448, min_scale=0.75, ratio=(1.,1.))\nbatch_tfms = [*aug_transforms(flip_vert=True, max_warp=0), Normalize.from_stats(*stats)]\nbs = 32\nlr = 3e-2\nepochs = 2\ncbs = None","71537bd7":"df = pd.read_csv('..\/input\/hpa-single-cell-image-classification\/train.csv')\npath = Path('..\/input\/hpa-512x512-jpg-images-dataset\/512x512jpgs')\n\nlabels = [str(i) for i in range(19)]\nfor x in labels: df[x] = df['Label'].apply(lambda r: int(x in r.split('|')))\n\ndfs = df.sample(frac=sample_size, random_state=seed).reset_index(drop=True)\ny = dfs[labels].values\nX = dfs['ID'].values\ndfs['fold'] = np.nan\n\nmskf = MultilabelStratifiedKFold(n_splits=5)\nfor i, (_, test_index) in enumerate(mskf.split(X, y)):\n    dfs.iloc[test_index, -1] = i\n   \ndfs['fold'] = dfs['fold'].astype('int')\ndfs['is_valid'] = False\ndfs['is_valid'][dfs.fold == 0] = True","618e09dc":"def get_x(r): return path\/f'{r[\"ID\"]}.jpg'\ndef get_y(r): return list(set(r['Label'].split('|')))\n\ndblock = DataBlock(blocks=(ImageBlock, MultiCategoryBlock(vocab=labels)),\n                    splitter=ColSplitter(col='is_valid'),\n                    get_x=get_x,\n                    get_y=get_y,\n                    item_tfms=item_tfms,\n                    batch_tfms=batch_tfms\n                    )\ndls = dblock.dataloaders(dfs, bs=bs)\n\nlearn = cnn_learner(dls, resnet18, metrics=[accuracy_multi, APScoreMulti()]).to_fp16()\nlearn.fine_tune(epochs, base_lr=lr, cbs=cbs)","582c60c7":"class Hook():\n    def __init__(self,m):\n        self.hook = m.register_forward_hook(self.hook_func)\n    def hook_func(self,m,i,o):\n        self.stored = o.detach().clone()\n    def __enter__(self, *args): return self\n    def __exit__(self, *args):\n        self.hook.remove()","f2f41d1b":"labelnum = 12\nlabel = str(i)\nimage_id = '39c61ede-bbbc-11e8-b2ba-ac1f6b6435d0'\n\nsample = dfs[dfs.ID == image_id].reset_index(drop=True)\nprint(f'ID: {sample.ID.loc[0]}, Label: {sample.Label.loc[0]}')\nimg = PILImage.create(get_x(sample.loc[0]))\nimg.show(figsize=(10,10));","a9f7c79c":"x, = first(dls.test_dl([img]))\nwith Hook(learn.model[0]) as hook:\n    with torch.no_grad():\n        output = learn.model.eval()(x.cuda())\n    act = hook.stored\nprint(act.shape)","6ff5a787":"pred = F.sigmoid(output).cpu()\nprint(pred)\nprint(f'Class proba: {pred[0][i]}')","08c60364":"cam_map = torch.einsum('ck,kij->cij', learn.model[1][-1].weight, act[0])\ncam_map.shape","909ba8f2":"x_dec = TensorImage(dls.train.decode((x,))[0][0])\n_,ax = plt.subplots(figsize=(10,10))\nx_dec.show(ctx=ax)\nax.imshow(cam_map[labelnum].detach().cpu(), alpha=0.6, extent=(0,448,448,0), interpolation='bilinear', cmap='magma');","20e35bd4":"x_dec = TensorImage(dls.train.decode((x,))[0][0])\n_,ax = plt.subplots(figsize=(10,10))\nx_dec.show(ctx=ax)\nax.imshow(cam_map[0].detach().cpu(), alpha=0.7, extent=(0,448,448,0), interpolation='bilinear', cmap='magma');","8288f996":"We can now visualize the grid! Let's look at the CAM map for our centrosome class. ","50861bce":"fastai models are conveniently splitted after the last convolutional layer. We will need to store the outputs of the first layer group `learn.model[0]` with the hook. Let's confirm that the shape is a 14x14 grid with 512 channels as expected (plus batch size 1 dimension). ","ef051781":"We now see a stronger activation in the bottom left corner which I think makes sense. \n\n# End. ","f000e0c4":"Now we use einsum notation to calculate the dot product between our activation grid and the channel weights. The output should be a grid showing activation strength for each of classes that we predict. ","ee670aa0":"# Model Training","7fc426bf":"# CAM - Class Activation Map Explained\n\nI'm using this notebook to learn more about [CAM](https:\/\/arxiv.org\/pdf\/1512.04150.pdf). I will use a dataset of 512x512 trainset images converted into jpg format to quickly train a simple classifier (resnet18), and then use it to analyse class activation maps.\n\n![CAM.png](attachment:CAM.png)\n\nThe basic idea is as follows:\n- After a series of convolutions, we end up with a small grid and a large number of channels. In our case, starting with 448 image and resnet18, after all the convolutions we end up with a 14x14 grid and 512 channels.\n- We can think of the grid as downsampled image and the channels as different features discovered in that image.\n- We then average the channels across that grid and multiply them with a set of weights to come up with the final prediction for each class (global average pooling followed by linear layer).\n- To understand which region of the image contributed to the final prediction the most, we can apply the weights to each element of the 14x14 grid (before averaging).\n- The regions with highest value contributed the most. \n- We can then overlay the activation map with the original image to visualize this. \n\nThis notebook is largely based on chapter 18 in [Deep Learning for Coders with Fastai and PyTorch](https:\/\/www.amazon.com\/Deep-Learning-Coders-fastai-PyTorch\/dp\/1492045527).","71c736e3":"Now let's double check the class predictions. It looks good!","4a793f88":"# Pytorch Hook: get access to layer activations!\n\nTo implement CAM, we need to get access to the activations of the last convolutional layer - we will use Pytorch Hook for this. We use Hook as context manager to avoid leaking memory - for that reason we define `__enter__` and `__exit__` methods. ","476281df":"Worked like charm :) How about the other label for this image (0 - nucleoplasm)? ","1825588b":"For visualization, we will focus on a single image for now. Let's grab one from our validation set and show the image along with the labels. Note that we trained a model with random cropping, and we will apply the same transform here. We will select one with class 12:\n\n### excerpt from hosts' notebook describing cell classes:\n\n![centrosome.png](attachment:centrosome.png)"}}