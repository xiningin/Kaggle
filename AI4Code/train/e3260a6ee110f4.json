{"cell_type":{"0d7432cc":"code","0bf9f191":"code","456b4b08":"code","a093f74e":"code","38dbe9a8":"code","0c7d3696":"code","7b2019f0":"code","d0b8ece0":"code","fe3580cd":"markdown","6fc079a7":"markdown"},"source":{"0d7432cc":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport torch","0bf9f191":"w_list = []\nmse_list=[]","456b4b08":"# Input data\n\nx_data = [1.0, 2.0, 3.0]\ny_data = [2.0, 4.0, 6.0]\nw = 1.0","a093f74e":"# Function for forward pass to predict y\ndef forward(x):\n    return x*w","38dbe9a8":"# Function to calcuate the loss of the model\n# Loss is the square of difference of prediction and actual value\n\ndef loss(x,y):\n    y_pred = forward(x)\n    return (y_pred-y)**2","0c7d3696":"# Function to calcualte the gradient for w to be updated and get min loss.\n# y_pred closer to y\n\n# Gradient = derivative of the loss for constant x and y\n\n# We are going to use a as 0.01 for starters\n\ndef gradient(x,y):\n    return 2*x*(x*w-y)","7b2019f0":"# Training loop\n\nprint('Predict (before training)', 4, forward(4))\n\n# Training loop\n\nfor epoch in range(100):\n    l_sum=0\n    for x_val, y_val in zip(x_data, y_data):\n        grad = gradient(x_val, y_val)\n        w = w-0.01*grad\n        print('\\tgrad: ', x_val, y_val, grad)\n        l=loss(x_val, y_val)\n        l_sum+=l\n        \n    print('Progress: ', epoch, 'w=', w, 'loss=', l)\n    w_list.append(w)\n    mse_list.append(l_sum\/3)\n    \n    \nprint('Predict (After training)', '4 hours', forward(4))    ","d0b8ece0":"plt.plot(w_list, mse_list)\nplt.ylabel('Loss')\nplt.xlabel('w')\nplt.show()","fe3580cd":"## Session: 03\n\nGradient Descent\n\n* How the weights get update to ensure we reach minimum loss.\n* This method is called Gradient Descent.","6fc079a7":"#### Gradient Descent\n\nWe update the `w` such that loss is minimum. The factor by which `w` is updated is called `alpha(learning rate)`.\n\nNew `w` is `w` minus `alpha` times derivative of `loss` against `w`\n\n$w=w-a*\\frac{d(loss)}{dw}$\n\nThis equation is dependent on how the loss function has been defined. \nIn the current case below formula will dictate how to update the value of w for each pass. \n\n$w = w - a*2x(xw-y)$\n"}}