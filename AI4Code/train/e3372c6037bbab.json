{"cell_type":{"990438ee":"code","08727014":"code","6b2ec5df":"code","e8e1b676":"code","a13a87e3":"code","9f368402":"code","0296924b":"code","9380c6dd":"code","e08d979f":"code","7dc50010":"code","7d4f2222":"code","7cb0e2d5":"code","13d64461":"code","a25b5ab6":"code","6432f0db":"code","8b178609":"code","e1cb1eed":"code","66e67b38":"code","c8adaab6":"code","f644c249":"code","cbe8b7ca":"code","490f3539":"code","cfa3727c":"markdown","b6726e8d":"markdown","c56c73f3":"markdown","8ccc0ad7":"markdown","f6772993":"markdown"},"source":{"990438ee":"# library\nimport matplotlib.pyplot as plt\nfrom palettable.colorbrewer.qualitative import Pastel1_7\nimport numpy as np \nimport pandas as pd \nimport time\nfrom datetime import datetime\n\nfrom sklearn.model_selection import train_test_split\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \n#google cloud connection        \nfrom google.cloud import storage\nfrom google.cloud import automl_v1beta1 as automl\n\n# workaround to fix gapic_v1 error\nfrom google.api_core.gapic_v1.client_info import ClientInfo\n","08727014":"sample_submission = pd.read_csv(\"..\/input\/tabular-playground-series-may-2021\/sample_submission.csv\")\ntest_df = pd.read_csv(\"..\/input\/tabular-playground-series-may-2021\/test.csv\")\ntrain_df = pd.read_csv(\"..\/input\/tabular-playground-series-may-2021\/train.csv\")","6b2ec5df":"#examine the distribution of the class in train#\n \n\nnames = ['Class_1', 'Class_2', 'Class_3', 'Class_4']\nsize = train_df.target.value_counts()\n \nmy_circle = plt.Circle( (0,0), 0.7, color='white')\n\nfrom palettable.colorbrewer.qualitative import Pastel1_7\nplt.pie(size, labels=names, colors=Pastel1_7.hex_colors)\np = plt.gcf()\np.gca().add_artist(my_circle)\n\n# Show the graph\nplt.show()","e8e1b676":"#examine the missing values\n\n#uncomment to run the profiling - it takes quite some time to run(5 minutes at least)#\nfrom pandas_profiling import ProfileReport\n#profile = ProfileReport(train_df, title=\"Profiling Report\")\n#profile.to_file(\"report.html\")\n#profile","a13a87e3":"#Change the following for your project and bucket name\nPROJECT_ID = 'automl-exercise-march'\nBUCKET_NAME = 'playground_may'\n#Note: the bucket_region must be us-central1.\nBUCKET_REGION = 'us-central1'","9f368402":"from google.cloud import storage, automl_v1beta1 as automl\n\nstorage_client = storage.Client(project=PROJECT_ID)\ntables_gcs_client = automl.GcsClient(client=storage_client, bucket_name=BUCKET_NAME)\nautoml_client = automl.AutoMlClient()\n# Note: AutoML Tables currently is only eligible for region us-central1. \nprediction_client = automl.PredictionServiceClient()\n# Note: This line runs unsuccessfully without each one of these parameters\ntables_client = automl.TablesClient(project=PROJECT_ID, region=BUCKET_REGION, client=automl_client, gcs_client=tables_gcs_client, prediction_client=prediction_client)","0296924b":"# Create your GCS Bucket with your specified name and region (if it doesn't already exist)\nbucket = storage.Bucket(storage_client, name=BUCKET_NAME)\nif not bucket.exists():\n    bucket.create(location=BUCKET_REGION)","9380c6dd":"###these are awesome helper functions created in the above mentioned notebook. You can use it to upload the file if you don't want to hover to the GCP and upload there.\n\ndef upload_blob(bucket_name, source_file_name, destination_blob_name):\n    \"\"\"Uploads a file to the bucket. https:\/\/cloud.google.com\/storage\/docs\/ \"\"\"\n    bucket = storage_client.get_bucket(bucket_name)\n    blob = bucket.blob(destination_blob_name)\n    blob.upload_from_filename(source_file_name)\n    print('File {} uploaded to {}.'.format(\n        source_file_name,\n        destination_blob_name))\n    \ndef download_to_kaggle(bucket_name,destination_directory,file_name,prefix=None):\n    \"\"\"Takes the data from your GCS Bucket and puts it into the working directory of your Kaggle notebook\"\"\"\n    os.makedirs(destination_directory, exist_ok = True)\n    full_file_path = os.path.join(destination_directory, file_name)\n    blobs = storage_client.list_blobs(bucket_name,prefix=prefix)\n    for blob in blobs:\n        blob.download_to_filename(full_file_path)","e08d979f":"train_df.to_csv(path_or_buf='train.csv', index=False)\ntest_df.to_csv(path_or_buf='test.csv', index=False)","7dc50010":"###upload the train,test files into the bucket that you created earlier\nupload_blob(BUCKET_NAME, 'train.csv', 'train.csv')\nupload_blob(BUCKET_NAME, 'test.csv', 'test.csv')","7d4f2222":"###can create the dataset directly on GCP or create it using this snippet.\ndataset_display_name = 'playground_may'\nnew_dataset = False\ntry:\n    dataset = tables_client.get_dataset(dataset_display_name=dataset_display_name)\nexcept:\n    new_dataset = True\n    dataset = tables_client.create_dataset(dataset_display_name)","7cb0e2d5":"# gcs_input_uris have the familiar path of gs:\/\/BUCKETNAME\/\/file\n\nif new_dataset:\n    gcs_input_uris = ['gs:\/\/' + BUCKET_NAME + '\/train.csv']\n\n    import_data_operation = tables_client.import_data(\n        dataset=dataset,\n        gcs_input_uris=gcs_input_uris\n    )\n    print('Dataset import operation: {}'.format(import_data_operation))\n\n    # Synchronous check of operation status. Wait until import is done.\n    import_data_operation.result()\n    print(dataset)","13d64461":"model_display_name = 'playground_may'\nTARGET_COLUMN = 'target'\nID_COLUMN = 'id'\n\n# TODO: File bug: if you run this right after the last step, when data import isn't complete, you get a list index out of range\n# There might be a more general issue, if you provide invalid display names, etc.\n\ntables_client.set_target_column(\n    dataset=dataset,\n    column_spec_display_name=TARGET_COLUMN\n)","a25b5ab6":"#set the training budget and create a model\nTRAIN_BUDGET = 7000\nmodel = None\ntry:\n    model = tables_client.get_model(model_display_name=model_display_name)\nexcept:\n    response = tables_client.create_model(\n        model_display_name,\n        dataset=dataset,\n        train_budget_milli_node_hours=TRAIN_BUDGET,\n        exclude_column_spec_names=[ID_COLUMN, TARGET_COLUMN]\n    )\n    print('Create model operation: {}'.format(response.operation))\n    # Wait until model training is done.\n    model = response.result()\n# print(model)","6432f0db":"gcs_input_uris = 'gs:\/\/' + BUCKET_NAME + '\/test.csv'\ngcs_output_uri_prefix = 'gs:\/\/' + BUCKET_NAME + '\/predictions'\n\nbatch_predict_response = tables_client.batch_predict(\n    model=model, \n    gcs_input_uris=gcs_input_uris,\n    gcs_output_uri_prefix=gcs_output_uri_prefix,\n)\nprint('Batch prediction operation: {}'.format(batch_predict_response.operation))\n# Wait until batch prediction is done.\nbatch_predict_result = batch_predict_response.result()\nbatch_predict_response.metadata","8b178609":"# The output directory for the prediction results exists under the response metadata for the batch_predict operation\n# Specifically, under metadata --> batch_predict_details --> output_info --> gcs_output_directory\n#this step is neede for later to feed the path for retreiving the tables_1.csv\ngcs_output_folder = batch_predict_response.metadata.batch_predict_details.output_info.gcs_output_directory.replace('gs:\/\/' + BUCKET_NAME + '\/','')\n","e1cb1eed":"download_to_kaggle(BUCKET_NAME,'\/kaggle\/working','tables_1.csv', prefix=gcs_output_folder)","66e67b38":"prediction = pd.read_csv(\"tables_1.csv\")","c8adaab6":"submission = prediction[['id', 'target_Class_1_score','target_Class_2_score','target_Class_3_score','target_Class_4_score']]\nsubmission.head()","f644c249":"submission = submission.rename(columns={\"target_Class_1_score\": \"Class_1\", \"target_Class_2_score\": \"Class_2\",\"target_Class_3_score\": \"Class_3\",\"target_Class_4_score\": \"Class_4\"})","cbe8b7ca":"submission = submission.sort_values(by=['id'])\nsubmission.head()","490f3539":"submission.to_csv('submission.csv', index=False)","cfa3727c":"#### One can direct create the dataset on GCP and point the data source. (It can be files in your bucket, local, or bigquery)\n","b6726e8d":"#### The following cells serves as a purpose for easier retriving the table_1.csv by providing the path. One can hover to the GCP to see how exactly the path looks like.\n\n![Screenshot 2021-05-25 at 14.56.47.png](attachment:a356fd7e-470c-41b1-a02a-5baec26f78e0.png)\n\n![Screenshot 2021-05-25 at 14.52.56.png](attachment:1ff18c4c-e5d9-43d8-a08c-fe4acfd271ab.png)","c56c73f3":"#### To set up the google cloud project - hover to the google cloud platform. You need to specify an unique project_id so that you can get started. Afterwards, go to the \"bucket\" page and click create a bucket with universal unique bucket_name. Do not use the name in the next cell directly.\n![Screenshot 2021-05-25 at 14.34.55.png](attachment:cf8ed6bb-b4df-40e8-aaa1-bbabe42c8d36.png)\n\nNoted that the bucket needs to be set to 'us-central1' as the Automl Table currently supports us-central1","8ccc0ad7":"#### After training you can directly get the feature importance and the evaluation graph in the evaluation tab\n![Screenshot 2021-05-25 at 14.43.27.png](attachment:bba07ff9-9131-47c9-a967-c02f86042cf2.png)","f6772993":"### This notebook is a babystep of figuring out how to use AutoML tables for benchmark purpose. The notebook followed the steps and helper function defined in this tutorial **https:\/\/www.kaggle.com\/philculliton\/automlwrapper**"}}