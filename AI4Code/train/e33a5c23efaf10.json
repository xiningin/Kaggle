{"cell_type":{"af2a7d1e":"code","0d2fad45":"code","040d388c":"code","837a2d26":"code","e6d2b068":"code","b6d5a34f":"code","e25b3ff1":"code","36aef709":"code","8f83c41b":"code","632d7ba2":"code","a106dfea":"code","739ea74d":"code","3c5e394d":"code","2473d4dd":"code","630017df":"code","97b7b968":"code","64361056":"code","65286317":"code","87403c2e":"code","861ce78d":"code","3d6b3b5d":"code","255a1a0a":"code","b8c1e31d":"code","8be4c373":"code","334d313a":"code","a1690d8d":"code","2014a9f9":"code","240da152":"code","30ae7455":"code","e5b6be64":"code","6ef96611":"code","00a9dc44":"code","ef872e36":"code","d4635431":"code","34b63631":"code","5c0423b4":"code","c7e11b01":"code","e792be20":"code","9649b1a2":"code","f68d4746":"code","230eca63":"code","57212c9e":"code","be86296a":"code","25c407cb":"code","a3278e77":"code","692fda53":"code","a9155620":"code","154e33ce":"code","c8297394":"code","fafa8ac2":"code","58b356ab":"code","48ec399c":"code","d2c672c8":"markdown","b28ed34c":"markdown","3b73593d":"markdown","f11fa937":"markdown","966dc0b5":"markdown","feaa21ab":"markdown","61953863":"markdown","93734273":"markdown","a8a5abec":"markdown","0f2f4fe8":"markdown","d1594280":"markdown","1b15e521":"markdown","99e30a5c":"markdown","7f0379f5":"markdown","2c9ba5bf":"markdown","68fe42d9":"markdown","ad4579bc":"markdown","d491b83f":"markdown","5e1058ff":"markdown","7b933967":"markdown","6e4da9b9":"markdown"},"source":{"af2a7d1e":"import random\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly\nimport plotly.express as px\nimport ipywidgets as widgets\nimport tensorflow.keras.backend as K\nfrom tensorflow.keras.models import load_model\nfrom plotly.offline import init_notebook_mode\nfrom sklearn.utils import shuffle\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.preprocessing import MinMaxScaler\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Activation,Dropout, Conv1D, MaxPooling1D, Flatten, GlobalMaxPooling1D, Reshape, LSTM\nfrom tensorflow.keras.callbacks import EarlyStopping","0d2fad45":"init_notebook_mode(connected = True)","040d388c":"df = pd.read_csv('..\/input\/kepler-labelled-time-series-data\/exoTrain.csv')\ndf.head()","837a2d26":"df['LABEL'].value_counts()","e6d2b068":"time_points = list(df.columns)\ntime_points.remove('LABEL')","b6d5a34f":"df[df.isnull().any(axis=1)]","e25b3ff1":"def flux_graph(row):\n    plt.figure(figsize=(15,5))\n    line = df[time_points].iloc[row]\n    plt.plot([int(i.replace('FLUX.', '')) for i in line.index], line)\n    plt.xlabel('t')\n    plt.ylabel('flux')\n    plt.show()\n\nwith_planet = df[df['LABEL'] == 2].head(5).index\nwo_planet = df[df['LABEL'] == 1].head(5).index","36aef709":"for row in with_planet:\n    flux_graph(row)","8f83c41b":"for row in wo_planet:\n    flux_graph(row)","632d7ba2":"label_pro = lambda x: 0 if x == 2 else 1\n\n    \ndf_d = df.copy()\ndf_d['LABEL'] = df_d['LABEL'].apply(label_pro)\n\ndf_d['LABEL'].value_counts()","a106dfea":"X_train, X_test, y_train, y_test = train_test_split(df_d.drop('LABEL', axis = 1), \n                                                    df_d['LABEL'], \n                                                    test_size=0.3,\n                                                   random_state = 101)","739ea74d":"class AllFeaturesMinMaxScaler:\n    \n    def __init__(self, new_min = 0, new_max = 1):\n        self.new_min = new_min\n        self.new_max = new_max\n        \n    def fit(self, X_train):\n        self.set_min = np.min(np.min(X_train))\n        self.set_max = np.max(np.max(X_train))\n    \n    def transform(self, data):\n        self.x_std = (data - self.set_min) \/ (self.set_max - self.set_min)\n        return self.x_std * (self.new_max - self.new_min) + self.new_min","3c5e394d":"scaler = AllFeaturesMinMaxScaler()\nscaler.fit(X_train)\nX_train_sc = scaler.transform(X_train.values)\nX_test_sc = scaler.transform(X_test.values)\nX_test_sc","2473d4dd":"#check\nprint(f'Scaled train data \\nMinimal value: {np.min(np.min(X_train_sc))} \\nMaximum value: {np.max(np.max(X_train_sc))}', \n      end ='\\n\\n')\nprint(f'Scaled test data \\nMinimal value: {np.min(np.min(X_test_sc))} \\nMaximum value: {np.max(np.max(X_test_sc))}')","630017df":"def ANN_model_evaluation(model, X_train_sc, y_train, X_test_sc, y_test):\n\n    histo = pd.DataFrame(model.history.history)\n    \n    for metric in ['acc', 'val_acc']:\n        \n        histo[metric].plot()\n        plt.title(metric)\n        plt.show()\n    \n    pred_test_values = model.predict_classes(X_test_sc)\n    pred_train_values = model.predict_classes(X_train_sc)\n\n    print('test')\n    print(classification_report(y_test,pred_test_values))\n    print(confusion_matrix(y_test,pred_test_values))\n\n    print('\\ntrain')\n    print(classification_report(y_train,pred_train_values))\n    print(confusion_matrix(y_train,pred_train_values))","97b7b968":"X_train.shape","64361056":"model = Sequential()\nmodel.add(Reshape((3197, 1), input_shape=(3197,)))\nmodel.add(Conv1D(filters=12, kernel_size=7, activation='relu', input_shape=(3197, 1)))\nmodel.add(MaxPooling1D(pool_size=5))\nmodel.add(Conv1D(filters=24, kernel_size=7, activation='relu'))\nmodel.add(MaxPooling1D(pool_size=5))\n#model.add(GlobalMaxPooling1D())\nmodel.add(Flatten())\nmodel.add(Dense(128, activation='relu'))\nmodel.add(Dropout(0.3))\nmodel.add(Dense(1))\n\nmodel.compile(\n    optimizer='adam', \n    loss='binary_crossentropy',\n    metrics = ['acc']\n)\n\nprint(model.summary())\n\nmodel.fit(x = X_train_sc,\n              y = y_train,\n              epochs = 4,\n              validation_data=(X_test_sc, y_test), \n              batch_size = 128\n         )\n\n\n\nANN_model_evaluation(model, X_train_sc, y_train, X_test_sc, y_test)\n","65286317":"np.unique(y_train, return_counts=True)","87403c2e":"def data_augmentation(X_train_sc, y_train):\n    X_train_aug = X_train_sc.copy()\n    y_train_aug = y_train.copy()\n    X_train_0 = X_train_aug[y_train[y_train == 0].index]\n    y_train_0 = y_train[y_train == 0]\n    for mult in [0.85, 0.87, 0.91, 0.93, 0.95, 1.05, 1.07, 1.09, 1.11, 1.15]:\n        for free in [ 0.05, 0.06, 0.07, -0.05, -0.06, -0.07, 0.1, -0.1, 0.12, -0.12]:\n            new_X_train_0 = X_train_0 * mult + free\n            new_y_train_0 = y_train_0.copy()\n            X_train_aug = np.append(X_train_aug, new_X_train_0, axis = 0)\n            y_train_aug = np.append(y_train_aug, new_y_train_0)\n            \n    return X_train_aug, y_train_aug\n\nX_train_aug, y_train_aug = data_augmentation(X_train_sc, y_train)","861ce78d":"X_train_aug.shape","3d6b3b5d":"y_train_aug","255a1a0a":"np.unique(y_train_aug, return_counts=True)","b8c1e31d":"model = Sequential()\nmodel.add(Reshape((3197, 1), input_shape=(3197,)))\nmodel.add(Conv1D(filters=36, kernel_size=3, activation='relu', input_shape=(3197, 1)))\nmodel.add(MaxPooling1D(pool_size=5))\nmodel.add(Conv1D(filters=64, kernel_size=7, activation='relu', input_shape=(3197, 1)))\nmodel.add(MaxPooling1D(pool_size=5))\n#model.add(Conv1D(filters=24, kernel_size=100, activation='relu'))\n#model.add(MaxPooling1D(pool_size=5))\n#model.add(GlobalMaxPooling1D())\nmodel.add(Flatten())\nmodel.add(Dense(128, activation='relu'))\nmodel.add(Dropout(0.3))\nmodel.add(Dense(128, activation='relu'))\nmodel.add(Dropout(0.3))\nmodel.add(Dense(1))\n\nmodel.compile(\n    optimizer='adam', \n    loss='binary_crossentropy',\n    metrics = ['acc']\n)\n\nprint(model.summary())\n\nmodel.fit(x = X_train_aug,\n              y = y_train_aug,\n              epochs = 15,\n              validation_data=(X_test_sc, y_test), \n              batch_size = 128\n         )\n\n\n\nANN_model_evaluation(model, X_train_aug, y_train_aug, X_test_sc, y_test)","8be4c373":"def scale_line_by_line(data, new_min = 0, new_max = 1):\n    data_new = data.copy()\n    for i in data.index:\n        if 'LABEL' in data.columns:\n            min_of_line = np.min(data.loc[i][1:])\n            max_of_line = np.max(data.loc[i][1:])\n            x_std = (data.loc[i][1:] - min_of_line) \/ (max_of_line - min_of_line)\n            data_new.loc[i] = x_std * (new_max - new_min) + new_min\n            data_new.loc[i]['LABEL'] = data.loc[i]['LABEL']\n        else:\n            min_of_line = np.min(data.loc[i])\n            max_of_line = np.max(data.loc[i])\n            x_std = (data.loc[i] - min_of_line) \/ (max_of_line - min_of_line)\n            data_new.loc[i] = x_std * (new_max - new_min) + new_min\n    \n    return data_new","334d313a":"X_train_sc = scale_line_by_line(X_train).values\nX_test_sc = scale_line_by_line(X_test).values\nX_train_sc","a1690d8d":"X_train_sc[0].shape","2014a9f9":"model = Sequential()\nmodel.add(Reshape((3197, 1), input_shape=(3197,)))\nmodel.add(Conv1D(filters=36, kernel_size=3, activation='relu', input_shape=(3197, 1)))\nmodel.add(MaxPooling1D(pool_size=5))\nmodel.add(Conv1D(filters=64, kernel_size=7, activation='relu', input_shape=(3197, 1)))\nmodel.add(MaxPooling1D(pool_size=5))\n#model.add(Conv1D(filters=24, kernel_size=100, activation='relu'))\n#model.add(MaxPooling1D(pool_size=5))\n#model.add(GlobalMaxPooling1D())\nmodel.add(Flatten())\nmodel.add(Dense(128, activation='relu'))\nmodel.add(Dropout(0.3))\nmodel.add(Dense(128, activation='relu'))\nmodel.add(Dropout(0.3))\nmodel.add(Dense(1))\n\nmodel.compile(\n    optimizer='adam', \n    loss='binary_crossentropy',           #'binary_crossentropy',\n    metrics = ['acc']\n)\n\nprint(model.summary())\n\nmodel.fit(x = X_train_sc,\n              y = y_train,\n              epochs = 15,\n              validation_data=(X_test_sc, y_test), \n              batch_size = 128\n         )\n\n\n\nANN_model_evaluation(model, X_train_sc, y_train, X_test_sc, y_test)","240da152":"np.unique(y_train, return_counts=True)","30ae7455":"X_train_aug, y_train_aug = data_augmentation(X_train_sc, y_train)","e5b6be64":"model = Sequential()\nmodel.add(Reshape((3197, 1), input_shape=(3197,)))\nmodel.add(Conv1D(filters=36, kernel_size=3, activation='relu', input_shape=(3197, 1)))\nmodel.add(MaxPooling1D(pool_size=5))\nmodel.add(Conv1D(filters=64, kernel_size=7, activation='relu', input_shape=(3197, 1)))\nmodel.add(MaxPooling1D(pool_size=5))\n#model.add(Conv1D(filters=24, kernel_size=100, activation='relu'))\n#model.add(MaxPooling1D(pool_size=5))\n#model.add(GlobalMaxPooling1D())\nmodel.add(Flatten())\nmodel.add(Dense(128, activation='relu'))\nmodel.add(Dropout(0.3))\nmodel.add(Dense(128, activation='relu'))\nmodel.add(Dropout(0.3))\nmodel.add(Dense(1))\n\nmodel.compile(\n    optimizer='adam', \n    loss='binary_crossentropy',           #'binary_crossentropy',\n    metrics = ['acc']\n)\n\nprint(model.summary())\n\nmodel.fit(x = X_train_aug,\n              y = y_train_aug,\n              epochs = 15,\n              validation_data=(X_test_sc, y_test), \n              batch_size = 128\n         )\n\n\n\nANN_model_evaluation(model, X_train_aug, y_train_aug, X_test_sc, y_test)","6ef96611":"df_scaled = scale_line_by_line(df_d)","00a9dc44":"train, test = train_test_split(df_scaled, \n                            test_size=0.3,\n                            random_state = 101)\n\ntrain","ef872e36":"def create_lines(data, length_of_piece, slide):\n    lines = []\n    for i in data.index:\n        length_of_line_in_data = len(data.loc[i]) - 1\n        for slide_step in range((length_of_line_in_data - length_of_piece) \/\/ slide):\n            lines.append(list(data.loc[i][1 + slide_step:1 + length_of_piece + slide_step]))\n    return lines","d4635431":"data_planets = create_lines(\n    data = train[train['LABEL'] == 0], \n    length_of_piece= 200, \n    slide = 1\n)\n\nprint(len(data_planets))","34b63631":"data_no_planets = create_lines(\n    data = train[train['LABEL'] == 1], \n    length_of_piece= 200, \n    slide = 70\n)\n\nprint(len(data_no_planets))","5c0423b4":"data_planets_df = pd.DataFrame(data_planets)\ndata_planets_df['LABEL'] = 0\n\ndata_no_planets_df = pd.DataFrame(data_no_planets)\ndata_no_planets_df['LABEL'] = 1\n\ndf_train = shuffle(data_planets_df.append(data_no_planets_df), random_state = 101)","c7e11b01":"df_train","e792be20":"X_train = df_train.drop('LABEL', axis = 1).values\ny_train = df_train['LABEL'].values","9649b1a2":"def test_processing_for_RNN(df, lenght):\n    y_test = df['LABEL'].values\n    x_data = df.drop('LABEL', axis = 1)\n    cut_till = list(x_data.columns)[lenght - 1]\n    X_test = x_data.loc[:, :cut_till].values\n    return X_test, y_test","f68d4746":"X_test, y_test = test_processing_for_RNN(df = test, lenght = 200)","230eca63":"X_train.shape","57212c9e":"X_test.shape","be86296a":"model = Sequential()\nmodel.add(Reshape((200, 1), input_shape=(200,)))\nmodel.add(LSTM(32, activation='relu', return_sequences=True, input_shape=(200, 1), dropout = 0.3))\nmodel.add(LSTM(16, activation='relu'))\nmodel.add(Dense(1))\n\nmodel.compile(\n    optimizer='adam', \n    loss='binary_crossentropy',\n    metrics = ['acc']\n)\n\nprint(model.summary())\n\nmodel.fit(x = X_train,\n              y = y_train,\n              epochs = 3,\n              validation_data=(X_test, y_test), \n              batch_size = 1024\n         )\n\n\n\nANN_model_evaluation(model, X_train, y_train, X_test, y_test)","25c407cb":"model.save('classif_on_200periods_2_layers (32-do0.3, 16) v2.h5')","a3278e77":"class StarCategorizer:\n    \n    def __init__(self, ml_model):\n        self.ml_model = ml_model\n    \n    def fit(self, X_train, y_train, n_epoch, validation_data):\n        self.ml_model.fit(\n            x = X_train,\n            y = y_train,\n            epochs = n_epochs,\n            validation_data=validation_data, \n            batch_size = 128\n        )\n        \n    def __line_creation(self, data):\n        return data.values.tolist()\n\n    def __batch_creation(self, line):\n        list_of_batches = []\n        for i in range(len(line) \/\/ 200 - 1):\n            list_of_batches.append(line[i*200 : (i + 1)*200])\n        return list_of_batches\n\n    def __dataset_for_model_creation(self, data):\n        return [self.__batch_creation(line) for line in self.__line_creation(data)]\n    \n    def __predictions_for_line_evaluation(self, predictions_for_line):\n        if predictions_for_line.count(0) >= 2:\n            return 0\n        else:\n            return 1\n    \n    def __predict_for_line(self, X_line):\n        # X_line - list of batchlists\n        predictions_for_line = []\n        for batch in X_line:\n            predictions_for_line.append(self.ml_model.predict_classes(np.array([batch]))[0][0])\n        return self.__predictions_for_line_evaluation(predictions_for_line)\n    \n    def predict_classes(self, X_set):\n        if type(X_set) == pd.core.frame.DataFrame:\n            X_set = self.__dataset_for_model_creation(X_set)\n        else:\n            raise ValueError('Wrong data format')\n        \n        return np.array([self.__predict_for_line(line) for line in X_set])\n\nstack_model = StarCategorizer(model)","692fda53":"test[:2]","a9155620":"stack_model.predict_classes(test[:3].drop('LABEL',axis = 1))","154e33ce":"pred_test_values = stack_model.predict_classes(test.drop('LABEL',axis = 1))\n\nprint(classification_report(test['LABEL'],pred_test_values))\nprint(confusion_matrix(test['LABEL'],pred_test_values))","c8297394":"validation_data = pd.read_csv('..\/input\/kepler-labelled-time-series-data\/exoTest.csv')\nvalidation_data.head()","fafa8ac2":"validation_data_d = validation_data.copy()\nvalidation_data_d['LABEL'] = validation_data_d['LABEL'].apply(label_pro)\n\nvalidation_data_d['LABEL'].value_counts()","58b356ab":"validation_scaled = scale_line_by_line(validation_data_d)\nvalidation_scaled.head(10)","48ec399c":"pred_val_values = stack_model.predict_classes(validation_scaled.drop('LABEL',axis = 1))\n\nprint(classification_report(validation_scaled['LABEL'],pred_val_values))\nprint(confusion_matrix(validation_scaled['LABEL'],pred_val_values))","d2c672c8":"Too huge imbalance for effective training","b28ed34c":"All our features have the same nature, so we sould scale through all data.","3b73593d":"# Data preparation for modelling","f11fa937":"Examples of flux change for star without planets:","966dc0b5":"# EDA","feaa21ab":"# RNN","61953863":"I'd start with 1D Convolutional model, as one the most appropriate in this situation.","93734273":"Sensitivity can be improved by intergrating ML model as a part to rule-based model. Data of each star will be splited into 14 batches 200 dots each. Inner model will analyse each batch and in case at least two of them looks like star with exoplanet - this star would be marked as star with exoplanet.","a8a5abec":"Model overfits fast, so I use just a few epochs. Now some datapoints where correctly identified as stars with exoplanet, so using RNN looks like better approach.","0f2f4fe8":"* data -> \n* .predict() -> \n* .__dataset_for_model_creation() -> \n* .__line_creation() -> \n* .__batch_creation() -> \n* .__predict_for_line() -> \n* .__predictions_for_line_evaluation()","d1594280":"We can try different approach. Split data to batches, use in training more batches for class \"with exoplanet\", make prediction for star based off just sample from its data.","1b15e521":"# 1D Convolutional model","99e30a5c":"It isn't a \"good\" model, but working with such imbalanced data has its problems","7f0379f5":"with augmented data:","2c9ba5bf":"# Data augmentation","68fe42d9":"Dataset is highly imbalanced, it can be an issue in modelling","ad4579bc":"Examples of flux change for star with planets:","d491b83f":"# Scaling other way","5e1058ff":"Looks like convolutional model didn't work well on this data - all test datapoints were identified to just one class.","7b933967":"# Validation","6e4da9b9":"I way to go can be also in another way of data scaling. As different stars have different starlight intensity, may be it will be better to scale through each star data."}}