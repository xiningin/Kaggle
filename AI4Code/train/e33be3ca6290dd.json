{"cell_type":{"a583771a":"code","71e9a63e":"code","709a2a0f":"code","eab5a1e7":"code","38f0aa0f":"code","a99eae94":"code","f2271462":"code","1aa9c53a":"code","c9792f7e":"code","842b36ab":"code","886ee726":"code","366ab61f":"code","a0c3ed14":"code","96f09c0a":"code","2f8a24e8":"code","0a413612":"code","3f509dca":"code","a8f61f2f":"code","1fdf98ec":"code","5ebf6817":"code","79807e3c":"code","e0f45c31":"code","3b09330d":"code","b8b0e0e0":"code","c87b2df3":"code","bbc82569":"code","9ea5e342":"code","5e6bb7a2":"code","652effd1":"code","6a8fe122":"code","67dfe230":"code","0b60c026":"code","4691d729":"code","21a551ff":"code","96e6dc7e":"code","2a57b1b3":"code","e9b2e08d":"code","9fe08e55":"code","9bda4a97":"code","4a9e9d65":"code","4115653e":"code","aad56ba3":"code","08fb8ac9":"code","dea1ee45":"markdown","d4217b04":"markdown","b3a3c62d":"markdown","d29dc7d6":"markdown","8040c4e0":"markdown","8c29afc4":"markdown","49a92577":"markdown","439d3b40":"markdown","9bf53a2d":"markdown","74ccafb3":"markdown","d9dee469":"markdown","83c10d9a":"markdown","b1f6a75f":"markdown","65b07dcb":"markdown","30433783":"markdown","2614ced2":"markdown","348ff0c8":"markdown","4f73509e":"markdown","c108b000":"markdown","9956dc48":"markdown","cdf9b42a":"markdown","e34617fe":"markdown","26955b00":"markdown","0d7d65be":"markdown","f371042f":"markdown","071e9d3b":"markdown","94238721":"markdown","1e588f2d":"markdown","579d362c":"markdown","eba89873":"markdown","574a3f0f":"markdown","6012a7c6":"markdown","c1e4cb5e":"markdown","c614017a":"markdown","a2a1e6ab":"markdown","c38ee315":"markdown","43d55e97":"markdown","e0657439":"markdown","c5120332":"markdown","f839a733":"markdown","b31181c2":"markdown","c71948bb":"markdown","d1a54f10":"markdown"},"source":{"a583771a":"# list of keywords for each task\ntask_keywords = {\n    \"task_1\": \"transmission, incubation, environmental stability, natural history, diagnostics, infection prevention, control, incubation period, asymptomatic shedding, asymptomatic transmission, Seasonality, charge distribution, adhesion to surfaces, environmental survival, Persistence, stability, Disease models, Immune response, immunity, personal protective equipment, PPE\",\n    \"task_2\": \"risk factor, Smoking, pre-existing pulmonary disease, pulmonary disease, Co-infection, co-existing infection, co-morbidities, pregnancy, Socio-economic factor, behavioral factor, economic impact, Transmission dynamics, reproductive number, incubation period, serial interval, modes of transmission, environmental factor, Severity, high-risk, Public health mitigation measure\",\n    \"task_3\": \"Genetics, evolution, genomic, genome, strain, field surveillance, genetic sequencing, host, animal host\",\n    \"task_4\": \"Vaccine, therapeutic, drug, treatment, inhibitor, naproxen, clarithromycin, minocycline, viral replication, Antibody-Dependent Enhancement, ADE, animal model, antiviral agent, universal coronavirus vaccine, prophylaxis clinical\",\n    \"task_5\": \"medical care, surge capacity, nursing home, resource allocation, personal protective equipment, PPE, process of care, clinical characterization, management, long term care, skilled nursing, surge medical staff, Age-adjusted mortality, Acute Respiratory Distress Syndrome, ARDS, organ failure, Extracorporeal membrane oxygenation, ECMO, mechanical ventilation, extrapulmonary manifestation, cardiomyopathy, cardiac arrest, regulatory standard, elastomeric respirator, telemedicine, hospital flow, workforce protection, workforce allocation, community-based support, supply chain management, clinical care, public health intervention, infection prevention, supportive interventions, adjunctive interventions\",\n    \"task_6\": \"non-pharmaceutical intervention, non-pharmaceutical, NPI, school closure, travel ban, social distancing, compliance, economic impact\",\n    \"task_7\": \"geographic variation, geographic, geographic spread, geographic mortality\",\n    \"task_8\": \"Diagnostics, surveillance, mitigation measure, early detection, point-of-care test, point-of-care, rapid bed-side, rapid bed-side test, screening, evolutionary hosts, transmission host\",\n    \"task_9\": \"Ethical, ethics, ethical principle, ethical issue, fear, anxiety, social media\",\n    \"task_10\": \"information sharing, data sharing, inter-sectoral collaboration, data standards, nomenclature, risk communication, governmental public health, communicating with high-risk populations, community measure, equity consideration, inequity, data-gathering, standardized nomenclature, information-sharing, Risk communication, Misunderstanding, disadvantaged population, marginalized population, underrepresented minorities\"\n}","71e9a63e":"task_num = '2'\nKOI = task_keywords[f'task_{task_num}'] \ninput_path = 'https:\/\/covid19.insilicom.com\/task' + task_num + '\/'","709a2a0f":"from IPython.display import Image\nimport requests","eab5a1e7":"url=input_path + 'figure\/network1.png'\nImage(requests.get(url).content, width=800, height=800)","38f0aa0f":"url=input_path + 'figure\/network2.png'\nImage(requests.get(url).content, width=800, height=800)","a99eae94":"url=input_path + 'figure\/network3.png'\nImage(requests.get(url).content, width=800, height=800)","f2271462":"import os\nfrom Bio import Entrez\nfrom Bio import Medline\nimport random\nimport csv\nimport re\nimport json\nimport pandas as pd\nimport nltk\nfrom nltk import sent_tokenize\nfrom nltk import word_tokenize\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import stopwords\nfrom itertools import combinations\nimport string\nimport fasttext\nfrom sklearn.model_selection import train_test_split","1aa9c53a":"# directory of covid-19 dataset\ndatadir = '\/kaggle\/input\/CORD-19-research-challenge'\nmetadata = 'metadata.csv'\nsubsetdata = ['biorxiv_medrxiv', 'comm_use_subset', 'noncomm_use_subset', 'custom_license']\n\n# configuration for running all code or not\n# It will take a long time to run the whole notebook. \n# If you run whole notebook, please change if_run_all to True\nif_run_all = False\n\nif if_run_all:\n    covid19_dataset_path = 'covid19_datasets'\n    if not os.path.exists(covid19_dataset_path): os.mkdir(covid19_dataset_path)\n    pubmed_tiabs_path = 'pubmed_tiabs'\n    if not os.path.exists(pubmed_tiabs_path): os.mkdir(pubmed_tiabs_path)\n    bert_data_path = 'bert_datasets'\n    if not os.path.exists(bert_data_path): os.mkdir(bert_data_path)\nelse:\n    covid19_dataset_path = '\/kaggle\/input\/covid19-processed-data'\n    pubmed_tiabs_path = '\/kaggle\/input\/covid19-processed-data'\n    bert_data_path = '\/kaggle\/input\/covid19-processed-data'\n\n# configuration for BERT model\noutput_folder='.'\n# We set the finetune BERT model as false because BERT model cannot be trained on Kaggle server. \n# If you run the code at your local machine, please change it to True\nif_finetune_bert_model = False # if finetune BERT model or not\n\nif if_finetune_bert_model==True:\n    bert_model_name='bert_model'\nelse:\n    bert_model_name='path_to_model\/bert_model' #specify the path to the trained models\n\n# configuration of others\nedges_path = 'covid19_edges'\nif not os.path.exists(edges_path): os.mkdir(edges_path)\n# Entrez.email = \"\" # use your own email address\n# Entrez.api_key = \"\" # use your own pubmed api key\nstop_words = set(stopwords.words(\"english\"))\nwnlem = WordNetLemmatizer()\nmodel = fasttext.load_model('\/kaggle\/input\/fasttextmodel\/lid.176.ftz')\nrandom.seed(10000)","c9792f7e":"# function for developing the dictionary mapping location of full text files in json format to the coresponding paper\ndef get_fulltext_links(subsetdata):\n    \"\"\"\n    Develop a dictionary mapping the filename to the location of full text\n    json file if the json file exists.\n    Args:\n        subsetdata: a list of the directories for json files.\n    \"\"\"\n    fulltextlinks = {}\n    for subset in subsetdata:\n        subsetdir = f'{datadir}\/{subset}\/{subset}'\n        for root, dirs, _ in os.walk(subsetdir, topdown=False):\n            for name in dirs:\n                subsetfiles = os.listdir(f'{root}\/{name}')\n                for subsetfile in subsetfiles:\n                    filename = subsetfile.split('.')[0]\n                    fulltextlinks[filename] = f'{root}\/{name}\/{subsetfile}'\n    return fulltextlinks\n\n# function for combining all text of the papers together\ndef get_all_covid19_text(df, fulltextlinks):\n    \"\"\"\n    This function combine all the text of the covid-19 dataset\n    (the metadata file, and all json files) into one dataframe.\n    Args:\n        df: the dataframe to combine all text.\n        fulltextlinks: the dictionary mapping papers to their json files. \n    \"\"\"\n    for idx, row in df.iterrows():\n        if row['has_pdf_parse'] == True:\n            full_text_file = fulltextlinks[row['sha'].split(';')[0]]\n        elif row['has_pmc_xml_parse'] == True:\n            full_text_file = fulltextlinks[row['pmcid']]\n        else: continue\n        jsonfile = json.load(open(full_text_file, 'rb'))\n        title = jsonfile['metadata']['title']\n        if 'abstract' in jsonfile:\n            abstract = '\\n'.join([text['text'] for text in jsonfile['abstract']])\n        else: abstract = ''\n        if 'body_text' in jsonfile:\n            fulltext = '\\n'.join([text['text'] for text in jsonfile['body_text']])\n        else: fulltext = ''\n        if jsonfile['bib_entries'] != None:\n            bibs = '\\n'.join([bib['title'] for bib in jsonfile['bib_entries'].values()])\n        else: bibs = ''\n        if jsonfile['ref_entries'] != None:\n            figures = '\\n'.join([ref['text'] for ref in jsonfile['ref_entries'].values() if ref['type'] == 'figure'])\n            tables = '\\n'.join([ref['text'] for ref in jsonfile['ref_entries'].values() if ref['type'] == 'table'])\n        else:\n            figures = ''\n            tables =  ''\n        if jsonfile['back_matter'] != None:\n            othertext = '\\n'.join([text['text'] for text in jsonfile['back_matter']])\n        else: othertext = ''\n        \n        if df.loc[idx, 'title'] == '' and title != '': df.loc[idx, 'title'] = title\n        if df.loc[idx, 'abstract'] == '' and abstract != '': df.loc[idx, 'abstract'] = abstract\n        if fulltext != '': df.loc[idx, 'full_text'] = fulltext\n        if bibs != '': df.loc[idx, 'bib_entries'] = bibs\n        if figures != '': df.loc[idx, 'figures'] = figures\n        if tables != '': df.loc[idx, 'tables'] = tables\n        if othertext != '': df.loc[idx, 'other_text'] = othertext\n    return df","842b36ab":"# Preprocess the covid19 dataset\nif if_run_all:\n    fulltextlinks = get_fulltext_links(subsetdata)\n    df = pd.read_csv(f'{datadir}\/{metadata}', na_filter= False)\n    df['url'] = df['url'].apply(lambda x: x if x != '' else 'None')\n    df = df.assign(full_text = '', bib_entries = '', figures = '', tables = '', other_text = '')\n    df = get_all_covid19_text(df, fulltextlinks)\n    columns_to_keep = ['cord_uid', 'title', 'abstract', 'authors', 'journal', 'publish_time', 'url']\n    df = df[columns_to_keep]\n    df.to_csv(f'{covid19_dataset_path}\/covid19_dataset_alldata.csv', index = False)\nelse:\n    df = pd.read_csv(f'{covid19_dataset_path}\/covid19_dataset_alldata.csv', na_filter= False)","886ee726":"# function for covid19 data subset by key word phrase \ndef covid19_data_subset(df, phrases, if_and = False):\n    \"\"\"\n    Subset papers containing any keywords in the phrases by\n    returning a list of indexes of the subset papers in a dataframe.\n    Args:\n        df: the dataframe containing all papers.\n        phrases: a string of keywords split by ',' to subset the papers in df.\n        if_ann: if True, subset data containing all keywords in the phrases,\n                if False, subset data containing  any keyword in the phrase.\n    \"\"\"\n    assert isinstance(phrases, str) and phrases != '' \n    phrases = [phrase.split() for phrase in phrases.lower().split(',')]\n    and_indexes = set()\n    or_indexes = set()\n    for idx, row in df.iterrows():\n        text = ' '.join([row.title, row.abstract])\n        text = text.lower()\n        count_phrase = 0\n        for phrase in phrases:\n            count_word = 0\n            for word in phrase:\n                if word in text: count_word += 1\n            if count_word == len(phrase):\n                count_phrase += 1\n                or_indexes.add(idx)\n        if count_phrase == len(phrases): and_indexes.add(idx)\n    if if_and:\n        return list(and_indexes)\n    return list(or_indexes)","366ab61f":"# covid19 subset for risk factor\nif if_run_all:\n    df_positive_index = covid19_data_subset(df, 'risk factor')\n    df_positive = df.loc[df_positive_index]\n    df_negative = df.drop(df_positive_index)\n    df_positive.to_csv(f'{covid19_dataset_path}\/covid19_dataset_risk_positive.csv', index = False)\n    df_negative.to_csv(f'{covid19_dataset_path}\/covid19_dataset_risk_negative.csv', index = False)","a0c3ed14":"# covid19 subset for a task\nif if_run_all:\n    df_subset = df.loc[covid19_data_subset(df, KOI)]\n    # df_subset.to_csv(f'{covid19_dataset_path}\/covid19_subset_task_{task_num}.csv', index = False)","96f09c0a":"# Function for getting pmids for a query\ndef get_query_pmids(query, max_ret = 10000,\n                    start_date = '2009\/01\/01', end_date = '2018\/12\/31'):\n    \"\"\"\n    Return a list of pmids for papers published in a period and\n    containing keywords by query to the pubmed database through Entrez.\n    Args:\n        query: keywords for searching the pubmed database.\n        max_ret: maximum number of pmids returned in one query.\n    \"\"\"\n    query = ' OR '.join(query.lower().split(','))\n    search_results = Entrez.read(Entrez.esearch(\n        db = \"pubmed\", term = query, retmax = max_ret,\n        mindate = start_date, maxdate = end_date, datetype = \"pdat\"\n        ))\n    counts = int(search_results[\"Count\"])\n    idlist = set(search_results[\"IdList\"])\n    if counts > max_ret:\n        for start in range(max_ret, counts, max_ret):\n            search_results = Entrez.read(Entrez.esearch(\n                db = \"pubmed\", term = query, retstart = start, retmax = max_ret,\n                mindate = start_date, maxdate = end_date, datetype = \"pdat\"\n                ))\n            idlist = idlist | set(search_results[\"IdList\"])\n    return list(idlist)\n\n# Function for writing pmids list to text file\ndef write_pmids_list(pmids_list, file_name):\n    assert isinstance(file_name, str)\n    with open(f'{pubmed_tiabs_path}\/{file_name}.txt', 'w', encoding = 'utf8') as f:\n        for line in pmids_list:\n            f.write(line+'\\n')","2f8a24e8":"# Generate the pmid list of all papers between 2009\/01\/01-2018\/12\/31 on pubmed \nif if_run_all:\n    query = \"\"\n    pmids_all = get_query_pmids(query)\n    # write_pmids_list(pmids_all, 'pmids_all')","0a413612":"# Generate the pmid list for pubmed papers containing risk factor\nif if_run_all:\n    query = \"risk[TIAB] AND factor[TIAB] OR factors[TIAB]\"\n    pmids_positive = get_query_pmids(query)\n    #write_pmids_list(pmids_positive, 'pmids_risk')","3f509dca":"# Query list of pmids for pubmed papers contianing KOI of a task\nif if_run_all:\n    pmids_list = get_query_pmids(KOI)\n    # write_pmids_list(pmids_list, f'pmids_task_{task_num}')","a8f61f2f":"# Function for loading list of pmids\ndef load_pmids_list(pmids_file):\n    \"\"\"\n    Return a list of pmids from a text file.\n    Args:\n        pmids_file: a text file containing pmids with one pmid in each line.\n    \"\"\"\n    assert isinstance(pmids_file, str)\n    with open(f'{pubmed_tiabs_path}\/{pmids_file}.txt', 'r', encoding = 'utf8') as f:\n        pmids_list = [line.strip() for line in f]\n    return pmids_list\n\n# Function for getting titles and abstracts of a list of less than 10000 pmids\ndef get_pubmed_tiabs(pmids_list, max_len = 10000):\n    \"\"\"\n    Return a dictionary of title and abstract of papers by pmid\n    in the pmids_list in one query of the pubmed database.\n    Args:\n        pmids_list: a list of pmids to query for the title and abstract of papers.\n    \"\"\"\n    assert isinstance(pmids_list, list) and len(pmids_list) <= max_len\n    pubmed_tiabs = {}\n    records =  Medline.parse(Entrez.efetch(\n        db=\"pubmed\", id=pmids_list, rettype='medline', retmode='text'\n        ))\n    for record in records:\n        if 'PMID' not in record: continue\n        pubmed_tiabs[record['PMID']] = {'title': record.get('TI', ''),\n                                        'abstract': record.get('AB', '')}\n    return pubmed_tiabs\n\n# Function for getting titles and abs of a list of more than 10000 pmids and write to csv files\ndef get_pubmed_tiabs_all(pmids_list, batch = 10000):\n    \"\"\"\n    Acquire title and abstract of papers in a list of pmids by query the pubmed database\n    and write the pmid, title and abstract to a csv file.\n    Args:\n        pmids_list: a list of pmids to query for the title and abstract of papers.\n        file_name: a string as name of the csv file\n    \"\"\"\n    assert isinstance(pmids_list, list)\n    len_list = len(pmids_list)\n    df_tiabs = {'pmid':[], 'title':[], 'abstract':[]}\n    for i in range(0, len_list, batch):\n        pmid_list = pmids_list[i:i+batch] if i+batch <= len_list else pmids_list[i:]\n        pubmed_tiabs = get_pubmed_tiabs(pmid_list, max_len = batch)\n        for k,v in pubmed_tiabs.items():\n            df_tiabs['pmid'].append(k)\n            df_tiabs['title'].append(v['title'])\n            df_tiabs['abstract'].append(v['abstract'])\n    return pd.DataFrame(data=df_tiabs)","1fdf98ec":"# acquiring pubmed papers for background edges of a task\nif if_run_all:\n    # pmids_list = load_pmids_list(f'pmids_task_{task_num}')\n    pmids_list = random.sample(pmids_list, 100000) if len(pmids_list) > 100000 else pmids_list\n    df_tiabs = get_pubmed_tiabs_all(pmids_list)\n    # df_tiabs.to_csv(f'{pubmed_tiabs_path}\/covid19_tiabs_task_{task_num}.csv', index = False)","5ebf6817":"# acquiring pubmed papers relevant to KOI for BERT training\nif if_run_all:\n    # pmids_all = load_pmids_list('pmids_all')\n    # pmids_positive = load_pmids_list('pmids_risk')\n    # delete the intersection of pmids_all and pmids_positive to get the negative pmids\n    pmids_negative = list(set(pmids_all) - set(pmids_positive))\n\n    random.seed(10000)\n    # Randomly get 10000 samples of positive papers\n    pmids_positive = random.sample(pmids_positive, 10000)\n    get_pubmed_tiabs_all(pmids_positive).to_csv(f'{pubmed_tiabs_path}\/pubmed_tiabs_risk_positive.csv', index = False)\n    # Randomly get 30000 samples of negative papers\n    pmids_negative = random.sample(pmids_negative, 30000)\n    get_pubmed_tiabs_all(pmids_negative).to_csv(f'{pubmed_tiabs_path}\/pubmed_tiabs_risk_negative.csv', index = False)","79807e3c":"# Function for text preprocess\ndef text_preprocess(text):\n    \"\"\"\n    Preprocess a string of text including title and abstract.\n    Args:\n        text: a string of text.\n    \"\"\"\n    assert isinstance(text, str)\n    text = re.sub(r'[\\r\\n]', ' ', text)\n    text = re.sub('(?<![A-Z])\\.(?=[A-Z])', '. ', text)\n    text = re.sub(r\"\\(\\w+[^\\(\\)]+ et al\\.?.*?\\)\",'',text)\n    text = re.sub('((?<!\\d)|(?<=\\d{4}))\\.[\\[\\(]?(\\d+|\\d+-\\d+)((,|, )(\\d+|\\d+-\\d+))*[\\)\\]]?([\\sA-Z]|$)', '. ', text)\n    text = re.sub('(?<=\\s)\\[\\d+\\](\\s\\[\\d+\\])*\\s|\\[\\d+(,\\s\\d+)*?\\]\\s?', '', text)\n    text = re.sub(r\"\\(https?:\/\/.*?\\)|\\(?https?:\/\/.*?([\\s,]|$)\",'',text)\n    text = re.sub(r\"\\(doi:.*?\\)|\\(?doi:.*?([\\s,]|$)\",'',text)\n    text = re.sub(r'[\\t\\|\/]', ' ', text)\n    text = re.sub('\u2013|\u2010', '-', text)\n    text = re.sub('\u00b7', '.', text)\n    return text\n\n# Function for cleaning sentence\ndef sentence_preprocess(text):\n    \"\"\"\n    Preprocess a string of text in a sentence.\n    Args:\n        text: a string of text.\n    \"\"\"\n    assert isinstance(text, str)\n    text = re.sub(r\"^\\[|\\](?=\\.$)|^\\d+\\s+(?=[A-Z])\", '', text)\n    text = re.sub(r\"^\\[.*?\\]\\s+(?=[A-Z])\", '', text)\n    text = re.sub(r\"^[\\w\\s-]*?:\\s*(?=[A-Zo])\", '', text)\n    text = re.sub(r\"^\\((.*)\\)(.$)\", r\"\\1\\2\", text)\n    text = re.sub(r\"^(Publisher Summary|Material and methods)\\s*(?=[A-Z])\", '', text)\n    text = re.sub(r\"^(((Abstract|ABSTRACT|Background:?\\]?|Methods?\\]?|METHODS?|Objectives?|Aims?|Summary|SUMMARY|Conclusions?\\]?|Findings?|Results?\\]?|Discussion)\\s*)+)(?=[A-Z4])\", '', text)\n    return text\n\n# Function for tokenizing sentence into words\ndef word_tokens(text):\n    \"\"\"\n    Tokenize a string of text into a list of lemmatized words.\n    Args:\n        text: a string of text.\n    \"\"\"\n    assert isinstance(text, str)\n    tokens = word_tokenize(text.lower())\n    tokens = [w for w in tokens if w not in string.punctuation]\n    tokens = [w for w in tokens if not re.match(r'\\d*,?\\d*[\\.-]?\\d*$|\\d*,?\\d*\\.?\\d*-(\\d*,?\\d*\\.?\\d*)?$', w)]\n    tokens = [w for w in tokens if re.match(r'^[\\w\\d]', w)]\n    tokens = [w for w in tokens if w not in stop_words]\n    tokens = [wnlem.lemmatize(w) for w in tokens]\n    tokens = [w for w in tokens if len(w) > 2]\n    return tokens\n\n# Function for tokenizing sentence into noun words\ndef tagged_word_tokens(text):\n    \"\"\"\n    Tokenize a string of text into a list of lemmatized words,\n    return a list of words of noun.\n    Args:\n        text: a string of text.\n    \"\"\"\n    assert isinstance(text, str)\n    tokens = word_tokenize(text)\n    tokens = nltk.pos_tag(tokens)\n    tokens = [(w.lower(), t) for w, t in tokens]\n    tokens = [w for w, t in tokens if t in ['NN', 'NNS', 'NNP', 'NNPS']]\n    tokens = [w for w in tokens if w not in string.punctuation]\n    tokens = [w for w in tokens if not re.match(r'\\d*,?\\d*[\\.-]?\\d*$|\\d*,?\\d*\\.?\\d*-(\\d*,?\\d*\\.?\\d*)?$', w)]\n    tokens = [w for w in tokens if re.match(r'^[\\w\\d]', w)]\n    tokens = [w for w in tokens if w not in stop_words]\n    tokens = [wnlem.lemmatize(w) for w in tokens]\n    tokens = [w for w in tokens if len(w) > 2]\n    return tokens","e0f45c31":"# Function for processing data for information retrieval model\ndef model_data_process(df, phrases = ''):\n    \"\"\"\n    Process text of titles and abstracts in a dataframe\n    by removing keywords in the phrases and punctuations.\n    Args:\n        df: a dataframe of titles and abstracts by ids.\n        phrases: a list of keywords split by ',' to be removed from the titles and abstracts.\n    \"\"\"\n    assert isinstance(phrases, str)\n    colnames = list(df.columns)\n    df = df.rename(columns = {colnames[0]:'pid'})\n    if phrases != '': phrases = [phrase.split() for phrase in phrases.split(',')]\n    for idx, row in df.iterrows():\n        title = text_preprocess(row.title)\n        abstract = text_preprocess(row.abstract)\n        tokens_title = word_tokenize(title)\n        tokens_abstract = word_tokenize(abstract)\n        if phrases != '':\n            for phrase in phrases:\n                count_in_title = 0\n                count_in_abstract = 0\n                for word in phrase:\n                    if word in title.lower(): count_in_title += 1\n                    if word in abstract.lower(): count_in_abstract += 1\n                if count_in_title == len(phrase):\n                    tokens_title = [w for w in tokens_title if phrase[0] not in wnlem.lemmatize(w.lower()) ]\n                if count_in_abstract == len(phrase):\n                    tokens_abstract = [w for w in tokens_abstract if phrase[0] not in wnlem.lemmatize(w.lower())]\n        title = ' '.join([w for w in tokens_title]) # if w not in string.punctuation\n        abstract = ' '.join([w for w in tokens_abstract]) # if w not in string.punctuation\n        df.loc[idx, 'title'] = title\n        df.loc[idx, 'abstract'] = abstract\n    return df","3b09330d":"# training datasets\nif if_run_all:\n    df_ir = pd.read_csv(f'{pubmed_tiabs_path}\/pubmed_tiabs_risk_positive.csv', na_filter= False)\n    df_positive = model_data_process(df_ir, 'risk,factor')\n    df_positive['label'] = [1] * df_positive.shape[0]\n\n    df_ir = pd.read_csv(f'{pubmed_tiabs_path}\/pubmed_tiabs_risk_negative.csv', na_filter= False)\n    df_negative = model_data_process(df_ir)\n    df_negative['label'] = [-1] * df_negative.shape[0]\n\n    df_ir = pd.concat([df_positive, df_negative], ignore_index=True)\n    train_sample, test_sample = train_test_split(df_ir, test_size=0.20, random_state=42, shuffle=True)\n    train_sample.to_csv(f'{bert_data_path}\/train_sample.csv', index = False)\n    test_sample.to_csv(f'{bert_data_path}\/test_sample.csv', index = False)","b8b0e0e0":"# For building prediction dataset\n# Do the same thing to test sample as above. We use all the papers in four folders of kaggle Covid as test sample. \n# To model_negative samples, we will therefore use bert model, which will be discussed in the following part,\n# to find the papers that talk about the keyword.\n# prediction dataset\nif if_run_all:\n    df_ir = pd.read_csv(f'{covid19_dataset_path}\/covid19_dataset_risk_positive.csv', na_filter= False)\n    df_ir = df_ir[['cord_uid', 'title', 'abstract']]\n    df_positive = model_data_process(df_ir, 'risk, factor')\n    df_positive['label'] = [1] * df_positive.shape[0]\n    df_positive.to_csv(f'{bert_data_path}\/model_positive.csv', index = False)\n\n    df_ir = pd.read_csv(f'{covid19_dataset_path}\/covid19_dataset_risk_negative.csv', na_filter= False)\n    df_ir = df_ir[['cord_uid', 'title', 'abstract']]\n    df_negative = model_data_process(df_ir)\n    df_negative['label'] = [-1] * df_negative.shape[0]\n    df_negative.to_csv(f'{bert_data_path}\/model_negative.csv', index = False)","c87b2df3":"# load package \nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport collections\nimport unicodedata\n\nimport six\n\nimport sentencepiece as spm\n# Some ideas are from https:\/\/www.kaggle.com\/gunesevitan\/nlp-with-disaster-tweets-eda-cleaning-and-bert\/notebook\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport tensorflow as tf\nimport tensorflow_hub as hub\nimport tensorflow.keras.backend as K\n\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom tensorflow.keras.models import load_model\nimport matplotlib\nmatplotlib.use('agg')\nimport matplotlib.pyplot as plt\n\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import precision_score, recall_score, f1_score\nfrom tensorflow.keras.utils import to_categorical\nimport os\nimport re\nimport string\nfrom nltk.corpus import stopwords\n\nfrom nltk.tokenize import word_tokenize","bbc82569":"# convert input text to BERT format before train model\ndef convert_text_format(df,tokenizer,max_seq_length):\n    # Covert texts format into BERT model format\n    \n    texts_series=df['title']+' '+df['abstract']\n    texts_series=texts_series.apply(clean_texts)\n    texts=texts_series.values\n    texts_input_ids=[]\n    texts_input_masks=[]\n    texts_segment_ids=[]\n    max_len_temp=0\n    for i,text in enumerate(texts):\n    \n        tokens_text = tokenizer.tokenize(text)\n        # Account for [CLS] and [SEP] with \"- 2\"\n        if len(tokens_text) > max_seq_length - 2:\n            tokens_text = tokens_text[0:(max_seq_length - 2)]\n        tokens = []\n        segment_ids = []\n        tokens.append(\"[CLS]\")\n        segment_ids.append(0)\n        for token in tokens_text:\n            tokens.append(token)\n            segment_ids.append(0)\n        \n        tokens.append(\"[SEP]\")\n        segment_ids.append(0)\n        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n    \n        # The mask has 1 for real tokens and 0 for padding tokens. Only real\n        # tokens are attended to.\n        input_masks = [1] * len(input_ids)\n        max_len_temp=max(max_len_temp,len(input_ids))\n        \n        \n      # Zero-pad up to the sequence length.\n        while len(input_ids) < max_seq_length:\n            input_ids.append(0)\n            input_masks.append(0)\n            segment_ids.append(0)\n        \n        assert len(input_ids) == max_seq_length\n        assert len(input_masks) == max_seq_length\n        assert len(segment_ids) == max_seq_length\n        if i < 5:\n            print(\"*** Example ***\")\n            \n            print(\"tokens: \",\n                         \" \".join([x for x in tokens]))\n            print(\"input_ids: \", \" \".join([str(x) for x in input_ids]))\n            print(\"input_masks: \", \" \".join([str(x) for x in input_masks]))\n            print(\"segment_ids: \", \" \".join([str(x) for x in segment_ids]))\n        if i%10000==0:\n            print(i)\n        texts_input_ids.append(input_ids)\n        texts_input_masks.append(input_masks)\n        texts_segment_ids.append(segment_ids)\n    \n    print(\"Longest tokens: %d\"%max_len_temp)\n\n    return [np.array(texts_input_ids).astype(np.int32), \n                np.array(texts_input_masks).astype(np.int32), \n                np.array(texts_segment_ids).astype(np.int32)]\n\n# preproccesing \ndef greek_to_eng(c):\n    # map greek alphabets to English\n    d=greek_alphabet.get(c)\n    if d!=None:\n        c=d\n    return c\n\n# Some codes are from https:\/\/www.kaggle.com\/rftexas\/text-only-kfold-bert\n\ndef remove_emoji(text):\n    # remove emoji\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)\n\n\n\ndef clean_texts(text,remove_stopwords=False,remove_punc=True,more_clean=True,lower=True):\n    # Clean the texts.\n\n    # Convert words to lower case and split them\n    if lower:\n        text = text.lower().split()\n    else:\n        text = text.split()\n\n#    # Optionally, remove stop words\n    if remove_stopwords:\n        stops = set(stopwords.words(\"english\"))\n        text = [w for w in text if not w in stops]\n    \n    text = \" \".join(text)\n    # Clean the text\n    \n    text = remove_emoji(text)\n\n    \n    \n    # Remove url (https:\/\/stackoverflow.com\/questions\/3809401\/what-is-a-good-regular-expression-to-match-a-url)\n    text = re.sub(r\"https?:\\\/\\\/(www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{1,256}\\.[a-zA-Z0-9()]{1,6}\\b([-a-zA-Z0-9()@:%_\\+.~#?&\/\/=]*)\", \"\", text)\n    \n\n    # Map prime\n    text = re.sub(r\"what's\", \"what is \", text)\n    text = re.sub(r\"don\\x89\u00db\u00aat\", \"do not\", text)\n    text = re.sub(r\"i\\x89\u00db\u00aam\", \"i am\", text)\n    text = re.sub(r\"you\\x89\u00db\u00aave\", \"you have\", text)\n    text = re.sub(r\"it\\x89\u00db\u00aas\", \"it is\", text)\n    text = re.sub(r\"doesn\\x89\u00db\u00aat\", \"does not\", text)\n    text = re.sub(r\"i\\x89\u00db\u00aave\", \"i have\", text)\n    text = re.sub(r\"can\\x89\u00db\u00aat\", \"cannot\", text)\n    text = re.sub(r\"wouldn\\x89\u00db\u00aat\", \"would not\", text)\n    text = re.sub(r\"that\\x89\u00db\u00aas\", \"that is\", text)\n    text = re.sub(r\"\\'ve\", \" have \", text)\n    text = re.sub(r\"can't\", \"cannot \", text)\n    text = re.sub(r\"can\u2019t\", \"cannot \", text)\n    text = re.sub(r\"n't\", \" not \", text)\n    text = re.sub(r\"i'm\", \"i am \", text)\n    text = re.sub(r\"\\'re\", \" are \", text)\n    text = re.sub(r\"\\'d\", \" would \", text)\n    text = re.sub(r\"\\'ll\", \" will \", text)\n\n\n    text = text.split()\n    text = \" \".join(text)\n    text = [greek_to_eng(w) for w in text] # map greek\n    text = ''.join([x for x in text if x in string.printable])\n    if more_clean:\n        text = convert_abbrev_in_text(text) # map abbrev\n    if remove_punc:\n        punc=string.punctuation\n        for p in punc:\n            text = text.replace(p, '') # remove punctuation\n    text = text.split()\n    text = \" \".join(text)   \n    \n    return text\n\ndef convert_abbrev(word):\n    # map abbrev\n    return abbreviations[word.lower()] if word.lower() in abbreviations.keys() else word\n\n\ndef convert_abbrev_in_text(text):\n    # map abbrev\n    tokens = word_tokenize(text)\n    tokens = [convert_abbrev(word) for word in tokens]\n    text = ' '.join(tokens)\n    return text\n\n\n# map Greek alphabets to English\ngreek_alphabet = {\n    u'\\u0391': 'Alpha',\n    u'\\u0392': 'Beta',\n    u'\\u0393': 'Gamma',\n    u'\\u0394': 'Delta',\n    u'\\u0395': 'Epsilon',\n    u'\\u0396': 'Zeta',\n    u'\\u0397': 'Eta',\n    u'\\u0398': 'Theta',\n    u'\\u0399': 'Iota',\n    u'\\u039A': 'Kappa',\n    u'\\u039B': 'Lamda',\n    u'\\u039C': 'Mu',\n    u'\\u039D': 'Nu',\n    u'\\u039E': 'Xi',\n    u'\\u039F': 'Omicron',\n    u'\\u03A0': 'Pi',\n    u'\\u03A1': 'Rho',\n    u'\\u03A3': 'Sigma',\n    u'\\u03A4': 'Tau',\n    u'\\u03A5': 'Upsilon',\n    u'\\u03A6': 'Phi',\n    u'\\u03A7': 'Chi',\n    u'\\u03A8': 'Psi',\n    u'\\u03A9': 'Omega',\n    u'\\u03B1': 'alpha',\n    u'\\u03B2': 'beta',\n    u'\\u03B3': 'gamma',\n    u'\\u03B4': 'delta',\n    u'\\u03B5': 'epsilon',\n    u'\\u03B6': 'zeta',\n    u'\\u03B7': 'eta',\n    u'\\u03B8': 'theta',\n    u'\\u03B9': 'iota',\n    u'\\u03BA': 'kappa',\n    u'\\u03BB': 'lamda',\n    u'\\u03BC': 'mu',\n    u'\\u03BD': 'nu',\n    u'\\u03BE': 'xi',\n    u'\\u03BF': 'omicron',\n    u'\\u03C0': 'pi',\n    u'\\u03C1': 'rho',\n    u'\\u03C3': 'sigma',\n    u'\\u03C4': 'tau',\n    u'\\u03C5': 'upsilon',\n    u'\\u03C6': 'phi',\n    u'\\u03C7': 'chi',\n    u'\\u03C8': 'psi',\n    u'\\u03C9': 'omega',\n}\n\n# map abbreviations (from https:\/\/www.kaggle.com\/rftexas\/text-only-kfold-bert)\nabbreviations = {\n    \"$\" : \" dollar \",\n    \"\u20ac\" : \" euro \",\n    \"4ao\" : \"for adults only\",\n    \"a.m\" : \"before midday\",\n    \"a3\" : \"anytime anywhere anyplace\",\n    \"aamof\" : \"as a matter of fact\",\n    \"acct\" : \"account\",\n    \"adih\" : \"another day in hell\",\n    \"afaic\" : \"as far as i am concerned\",\n    \"afaict\" : \"as far as i can tell\",\n    \"afaik\" : \"as far as i know\",\n    \"afair\" : \"as far as i remember\",\n    \"afk\" : \"away from keyboard\",\n    \"app\" : \"application\",\n    \"approx\" : \"approximately\",\n    \"apps\" : \"applications\",\n    \"asap\" : \"as soon as possible\",\n    \"asl\" : \"age, sex, location\",\n    \"atk\" : \"at the keyboard\",\n    \"ave.\" : \"avenue\",\n    \"aymm\" : \"are you my mother\",\n    \"ayor\" : \"at your own risk\", \n    \"b&b\" : \"bed and breakfast\",\n    \"b+b\" : \"bed and breakfast\",\n    \"b.c\" : \"before christ\",\n    \"b2b\" : \"business to business\",\n    \"b2c\" : \"business to customer\",\n    \"b4\" : \"before\",\n    \"b4n\" : \"bye for now\",\n    \"b@u\" : \"back at you\",\n    \"bae\" : \"before anyone else\",\n    \"bak\" : \"back at keyboard\",\n    \"bbbg\" : \"bye bye be good\",\n    \"bbc\" : \"british broadcasting corporation\",\n    \"bbias\" : \"be back in a second\",\n    \"bbl\" : \"be back later\",\n    \"bbs\" : \"be back soon\",\n    \"be4\" : \"before\",\n    \"bfn\" : \"bye for now\",\n    \"blvd\" : \"boulevard\",\n    \"bout\" : \"about\",\n    \"brb\" : \"be right back\",\n    \"bros\" : \"brothers\",\n    \"brt\" : \"be right there\",\n    \"bsaaw\" : \"big smile and a wink\",\n    \"btw\" : \"by the way\",\n    \"bwl\" : \"bursting with laughter\",\n    \"c\/o\" : \"care of\",\n    \"cet\" : \"central european time\",\n    \"cf\" : \"compare\",\n    \"cia\" : \"central intelligence agency\",\n    \"csl\" : \"can not stop laughing\",\n    \"cu\" : \"see you\",\n    \"cul8r\" : \"see you later\",\n    \"cv\" : \"curriculum vitae\",\n    \"cwot\" : \"complete waste of time\",\n    \"cya\" : \"see you\",\n    \"cyt\" : \"see you tomorrow\",\n    \"dae\" : \"does anyone else\",\n    \"dbmib\" : \"do not bother me i am busy\",\n    \"diy\" : \"do it yourself\",\n    \"dm\" : \"direct message\",\n    \"dwh\" : \"during work hours\",\n    \"e123\" : \"easy as one two three\",\n    \"eet\" : \"eastern european time\",\n    \"eg\" : \"example\",\n    \"embm\" : \"early morning business meeting\",\n    \"encl\" : \"enclosed\",\n    \"encl.\" : \"enclosed\",\n    \"etc\" : \"and so on\",\n    \"faq\" : \"frequently asked questions\",\n    \"fawc\" : \"for anyone who cares\",\n    \"fb\" : \"facebook\",\n    \"fc\" : \"fingers crossed\",\n    \"fig\" : \"figure\",\n    \"fimh\" : \"forever in my heart\", \n    \"ft.\" : \"feet\",\n    \"ft\" : \"featuring\",\n    \"ftl\" : \"for the loss\",\n    \"ftw\" : \"for the win\",\n    \"fwiw\" : \"for what it is worth\",\n    \"fyi\" : \"for your information\",\n    \"g9\" : \"genius\",\n    \"gahoy\" : \"get a hold of yourself\",\n    \"gal\" : \"get a life\",\n    \"gcse\" : \"general certificate of secondary education\",\n    \"gfn\" : \"gone for now\",\n    \"gg\" : \"good game\",\n    \"gl\" : \"good luck\",\n    \"glhf\" : \"good luck have fun\",\n    \"gmt\" : \"greenwich mean time\",\n    \"gmta\" : \"great minds think alike\",\n    \"gn\" : \"good night\",\n    \"g.o.a.t\" : \"greatest of all time\",\n    \"goat\" : \"greatest of all time\",\n    \"goi\" : \"get over it\",\n    \"gps\" : \"global positioning system\",\n    \"gr8\" : \"great\",\n    \"gratz\" : \"congratulations\",\n    \"gyal\" : \"girl\",\n    \"h&c\" : \"hot and cold\",\n    \"hp\" : \"horsepower\",\n    \"hr\" : \"hour\",\n    \"hrh\" : \"his royal highness\",\n    \"ht\" : \"height\",\n    \"ibrb\" : \"i will be right back\",\n    \"ic\" : \"i see\",\n    \"icq\" : \"i seek you\",\n    \"icymi\" : \"in case you missed it\",\n    \"idc\" : \"i do not care\",\n    \"idgadf\" : \"i do not give a damn fuck\",\n    \"idgaf\" : \"i do not give a fuck\",\n    \"idk\" : \"i do not know\",\n    \"ie\" : \"that is\",\n    \"i.e\" : \"that is\",\n    \"ifyp\" : \"i feel your pain\",\n    \"IG\" : \"instagram\",\n    \"iirc\" : \"if i remember correctly\",\n    \"ilu\" : \"i love you\",\n    \"ily\" : \"i love you\",\n    \"imho\" : \"in my humble opinion\",\n    \"imo\" : \"in my opinion\",\n    \"imu\" : \"i miss you\",\n    \"iow\" : \"in other words\",\n    \"irl\" : \"in real life\",\n    \"j4f\" : \"just for fun\",\n    \"jic\" : \"just in case\",\n    \"jk\" : \"just kidding\",\n    \"jsyk\" : \"just so you know\",\n    \"l8r\" : \"later\",\n    \"lb\" : \"pound\",\n    \"lbs\" : \"pounds\",\n    \"ldr\" : \"long distance relationship\",\n    \"lmao\" : \"laugh my ass off\",\n    \"lmfao\" : \"laugh my fucking ass off\",\n    \"lol\" : \"laughing out loud\",\n    \"ltd\" : \"limited\",\n    \"ltns\" : \"long time no see\",\n    \"m8\" : \"mate\",\n    \"mf\" : \"motherfucker\",\n    \"mfs\" : \"motherfuckers\",\n    \"mfw\" : \"my face when\",\n    \"mofo\" : \"motherfucker\",\n    \"mph\" : \"miles per hour\",\n    \"mr\" : \"mister\",\n    \"mrw\" : \"my reaction when\",\n    \"ms\" : \"miss\",\n    \"mte\" : \"my thoughts exactly\",\n    \"nagi\" : \"not a good idea\",\n    \"nbc\" : \"national broadcasting company\",\n    \"nbd\" : \"not big deal\",\n    \"nfs\" : \"not for sale\",\n    \"ngl\" : \"not going to lie\",\n    \"nhs\" : \"national health service\",\n    \"nrn\" : \"no reply necessary\",\n    \"nsfl\" : \"not safe for life\",\n    \"nsfw\" : \"not safe for work\",\n    \"nth\" : \"nice to have\",\n    \"nvr\" : \"never\",\n    \"nyc\" : \"new york city\",\n    \"oc\" : \"original content\",\n    \"og\" : \"original\",\n    \"ohp\" : \"overhead projector\",\n    \"oic\" : \"oh i see\",\n    \"omdb\" : \"over my dead body\",\n    \"omg\" : \"oh my god\",\n    \"omw\" : \"on my way\",\n    \"p.a\" : \"per annum\",\n    \"p.m\" : \"after midday\",\n    \"pm\" : \"prime minister\",\n    \"poc\" : \"people of color\",\n    \"pov\" : \"point of view\",\n    \"pp\" : \"pages\",\n    \"ppl\" : \"people\",\n    \"prw\" : \"parents are watching\",\n    \"ps\" : \"postscript\",\n    \"pt\" : \"point\",\n    \"ptb\" : \"please text back\",\n    \"pto\" : \"please turn over\",\n    \"qpsa\" : \"what happens\", #\"que pasa\",\n    \"ratchet\" : \"rude\",\n    \"rbtl\" : \"read between the lines\",\n    \"rlrt\" : \"real life retweet\", \n    \"rofl\" : \"rolling on the floor laughing\",\n    \"roflol\" : \"rolling on the floor laughing out loud\",\n    \"rotflmao\" : \"rolling on the floor laughing my ass off\",\n    \"rt\" : \"retweet\",\n    \"ruok\" : \"are you ok\",\n    \"sfw\" : \"safe for work\",\n    \"sk8\" : \"skate\",\n    \"smh\" : \"shake my head\",\n    \"sq\" : \"square\",\n    \"srsly\" : \"seriously\", \n    \"ssdd\" : \"same stuff different day\",\n    \"tbh\" : \"to be honest\",\n    \"tbs\" : \"tablespooful\",\n    \"tbsp\" : \"tablespooful\",\n    \"tfw\" : \"that feeling when\",\n    \"thks\" : \"thank you\",\n    \"tho\" : \"though\",\n    \"thx\" : \"thank you\",\n    \"tia\" : \"thanks in advance\",\n    \"til\" : \"today i learned\",\n    \"tl;dr\" : \"too long i did not read\",\n    \"tldr\" : \"too long i did not read\",\n    \"tmb\" : \"tweet me back\",\n    \"tntl\" : \"trying not to laugh\",\n    \"ttyl\" : \"talk to you later\",\n    \"u\" : \"you\",\n    \"u2\" : \"you too\",\n    \"u4e\" : \"yours for ever\",\n    \"utc\" : \"coordinated universal time\",\n    \"w\/\" : \"with\",\n    \"w\/o\" : \"without\",\n    \"w8\" : \"wait\",\n    \"wassup\" : \"what is up\",\n    \"wb\" : \"welcome back\",\n    \"wtf\" : \"what the fuck\",\n    \"wtg\" : \"way to go\",\n    \"wtpa\" : \"where the party at\",\n    \"wuf\" : \"where are you from\",\n    \"wuzup\" : \"what is up\",\n    \"wywh\" : \"wish you were here\",\n    \"yd\" : \"yard\",\n    \"ygtr\" : \"you got that right\",\n    \"ynk\" : \"you never know\",\n    \"zzz\" : \"sleeping bored and tired\"\n}\n\n\n","9ea5e342":"\"\"\"Tokenization classes implementation.\n\nThe file is forked from:\nhttps:\/\/github.com\/google-research\/bert\/blob\/master\/tokenization.py.\n\"\"\"\nSPIECE_UNDERLINE = \"\u2581\"\n\ndef convert_to_unicode(text):\n    \"\"\"Converts `text` to Unicode (if it's not already), assuming utf-8 input.\"\"\"\n    if six.PY3:\n        if isinstance(text, str):\n            return text\n        elif isinstance(text, bytes):\n            return text.decode(\"utf-8\", \"ignore\")\n        else:\n            raise ValueError(\"Unsupported string type: %s\" % (type(text)))\n    elif six.PY2:\n        if isinstance(text, str):\n            return text.decode(\"utf-8\", \"ignore\")\n        elif isinstance(text, unicode):\n            return text\n        else:\n            raise ValueError(\"Unsupported string type: %s\" % (type(text)))\n    else:\n        raise ValueError(\"Not running on Python2 or Python 3?\")\n\n\n        \ndef printable_text(text):\n    \"\"\"Returns text encoded in a way suitable for print or `tf.logging`.\"\"\"\n\n    # These functions want `str` for both Python2 and Python3, but in one case\n    # it's a Unicode string and in the other it's a byte string.\n    if six.PY3:\n        if isinstance(text, str):\n            return text\n        elif isinstance(text, bytes):\n            return text.decode(\"utf-8\", \"ignore\")\n        else:\n            raise ValueError(\"Unsupported string type: %s\" % (type(text)))\n    elif six.PY2:\n        if isinstance(text, str):\n            return text\n        elif isinstance(text, unicode):\n            return text.encode(\"utf-8\")\n        else:\n            raise ValueError(\"Unsupported string type: %s\" % (type(text)))\n    else:\n        raise ValueError(\"Not running on Python2 or Python 3?\")\n\n\ndef load_vocab(vocab_file):\n    \"\"\"Loads a vocabulary file into a dictionary.\"\"\"\n    vocab = collections.OrderedDict()\n    index = 0\n    with tf.io.gfile.GFile(vocab_file, \"r\") as reader:\n        while True:\n            token = convert_to_unicode(reader.readline())\n            if not token:\n                break\n            token = token.strip()\n            vocab[token] = index\n            index += 1\n    return vocab\n\n\ndef convert_by_vocab(vocab, items):\n    \"\"\"Converts a sequence of [tokens|ids] using the vocab.\"\"\"\n    output = []\n    for item in items:\n        output.append(vocab[item])\n    return output\n\n\ndef convert_tokens_to_ids(vocab, tokens):\n    return convert_by_vocab(vocab, tokens)\n\n\ndef convert_ids_to_tokens(inv_vocab, ids):\n    return convert_by_vocab(inv_vocab, ids)\n\n\ndef whitespace_tokenize(text):\n    \"\"\"Runs basic whitespace cleaning and splitting on a piece of text.\"\"\"\n    text = text.strip()\n    if not text:\n        return []\n    tokens = text.split()\n    return tokens\n\n\nclass FullTokenizer(object):\n    \"\"\"Runs end-to-end tokenziation.\"\"\"\n\n    def __init__(self, vocab_file, do_lower_case=True, split_on_punc=True):\n        self.vocab = load_vocab(vocab_file)\n        self.inv_vocab = {v: k for k, v in self.vocab.items()}\n        self.basic_tokenizer = BasicTokenizer(\n                do_lower_case=do_lower_case, split_on_punc=split_on_punc)\n        self.wordpiece_tokenizer = WordpieceTokenizer(vocab=self.vocab)\n\n    def tokenize(self, text):\n        split_tokens = []\n        for token in self.basic_tokenizer.tokenize(text):\n            for sub_token in self.wordpiece_tokenizer.tokenize(token):\n                split_tokens.append(sub_token)\n\n        return split_tokens\n\n    def convert_tokens_to_ids(self, tokens):\n        return convert_by_vocab(self.vocab, tokens)\n\n    def convert_ids_to_tokens(self, ids):\n        return convert_by_vocab(self.inv_vocab, ids)\n\n\nclass BasicTokenizer(object):\n    \"\"\"Runs basic tokenization (punctuation splitting, lower casing, etc.).\"\"\"\n\n    def __init__(self, do_lower_case=True, split_on_punc=True):\n        \"\"\"Constructs a BasicTokenizer.\n\n        Args:\n            do_lower_case: Whether to lower case the input.\n            split_on_punc: Whether to apply split on punctuations. By default BERT\n                starts a new token for punctuations. This makes detokenization difficult\n                for tasks like seq2seq decoding.\n        \"\"\"\n        self.do_lower_case = do_lower_case\n        self.split_on_punc = split_on_punc\n\n    def tokenize(self, text):\n        \"\"\"Tokenizes a piece of text.\"\"\"\n        text = convert_to_unicode(text)\n        text = self._clean_text(text)\n\n        # This was added on November 1st, 2018 for the multilingual and Chinese\n        # models. This is also applied to the English models now, but it doesn't\n        # matter since the English models were not trained on any Chinese data\n        # and generally don't have any Chinese data in them (there are Chinese\n        # characters in the vocabulary because Wikipedia does have some Chinese\n        # words in the English Wikipedia.).\n        text = self._tokenize_chinese_chars(text)\n\n        orig_tokens = whitespace_tokenize(text)\n        split_tokens = []\n        for token in orig_tokens:\n            if self.do_lower_case:\n                token = token.lower()\n                token = self._run_strip_accents(token)\n            if self.split_on_punc:\n                split_tokens.extend(self._run_split_on_punc(token))\n            else:\n                split_tokens.append(token)\n\n        output_tokens = whitespace_tokenize(\" \".join(split_tokens))\n        return output_tokens\n\n    def _run_strip_accents(self, text):\n        \"\"\"Strips accents from a piece of text.\"\"\"\n        text = unicodedata.normalize(\"NFD\", text)\n        output = []\n        for char in text:\n            cat = unicodedata.category(char)\n            if cat == \"Mn\":\n                continue\n            output.append(char)\n        return \"\".join(output)\n\n    def _run_split_on_punc(self, text):\n        \"\"\"Splits punctuation on a piece of text.\"\"\"\n        chars = list(text)\n        i = 0\n        start_new_word = True\n        output = []\n        while i < len(chars):\n            char = chars[i]\n            if _is_punctuation(char):\n                output.append([char])\n                start_new_word = True\n            else:\n                if start_new_word:\n                    output.append([])\n                start_new_word = False\n                output[-1].append(char)\n            i += 1\n\n        return [\"\".join(x) for x in output]\n\n    def _tokenize_chinese_chars(self, text):\n        \"\"\"Adds whitespace around any CJK character.\"\"\"\n        output = []\n        for char in text:\n            cp = ord(char)\n            if self._is_chinese_char(cp):\n                output.append(\" \")\n                output.append(char)\n                output.append(\" \")\n            else:\n                output.append(char)\n        return \"\".join(output)\n\n    def _is_chinese_char(self, cp):\n        \"\"\"Checks whether CP is the codepoint of a CJK character.\"\"\"\n        # This defines a \"chinese character\" as anything in the CJK Unicode block:\n        #     https:\/\/en.wikipedia.org\/wiki\/CJK_Unified_Ideographs_(Unicode_block)\n        #\n        # Note that the CJK Unicode block is NOT all Japanese and Korean characters,\n        # despite its name. The modern Korean Hangul alphabet is a different block,\n        # as is Japanese Hiragana and Katakana. Those alphabets are used to write\n        # space-separated words, so they are not treated specially and handled\n        # like the all of the other languages.\n        if ((cp >= 0x4E00 and cp <= 0x9FFF) or    #\n                (cp >= 0x3400 and cp <= 0x4DBF) or    #\n                (cp >= 0x20000 and cp <= 0x2A6DF) or    #\n                (cp >= 0x2A700 and cp <= 0x2B73F) or    #\n                (cp >= 0x2B740 and cp <= 0x2B81F) or    #\n                (cp >= 0x2B820 and cp <= 0x2CEAF) or\n                (cp >= 0xF900 and cp <= 0xFAFF) or    #\n                (cp >= 0x2F800 and cp <= 0x2FA1F)):    #\n            return True\n\n        return False\n\n    def _clean_text(self, text):\n        \"\"\"Performs invalid character removal and whitespace cleanup on text.\"\"\"\n        output = []\n        for char in text:\n            cp = ord(char)\n            if cp == 0 or cp == 0xfffd or _is_control(char):\n                continue\n            if _is_whitespace(char):\n                output.append(\" \")\n            else:\n                output.append(char)\n        return \"\".join(output)\n\n\nclass WordpieceTokenizer(object):\n    \"\"\"Runs WordPiece tokenziation.\"\"\"\n\n    def __init__(self, vocab, unk_token=\"[UNK]\", max_input_chars_per_word=200):\n        self.vocab = vocab\n        self.unk_token = unk_token\n        self.max_input_chars_per_word = max_input_chars_per_word\n\n    def tokenize(self, text):\n        \"\"\"Tokenizes a piece of text into its word pieces.\n\n        This uses a greedy longest-match-first algorithm to perform tokenization\n        using the given vocabulary.\n\n        For example:\n            input = \"unaffable\"\n            output = [\"un\", \"##aff\", \"##able\"]\n\n        Args:\n            text: A single token or whitespace separated tokens. This should have\n                already been passed through `BasicTokenizer.\n\n        Returns:\n            A list of wordpiece tokens.\n        \"\"\"\n\n        text = convert_to_unicode(text)\n\n        output_tokens = []\n        for token in whitespace_tokenize(text):\n            chars = list(token)\n            if len(chars) > self.max_input_chars_per_word:\n                output_tokens.append(self.unk_token)\n                continue\n\n            is_bad = False\n            start = 0\n            sub_tokens = []\n            while start < len(chars):\n                end = len(chars)\n                cur_substr = None\n                while start < end:\n                    substr = \"\".join(chars[start:end])\n                    if start > 0:\n                        substr = \"##\" + substr\n                    if substr in self.vocab:\n                        cur_substr = substr\n                        break\n                    end -= 1\n                if cur_substr is None:\n                    is_bad = True\n                    break\n                sub_tokens.append(cur_substr)\n                start = end\n\n            if is_bad:\n                output_tokens.append(self.unk_token)\n            else:\n                output_tokens.extend(sub_tokens)\n        return output_tokens\n\n\ndef _is_whitespace(char):\n    \"\"\"Checks whether `chars` is a whitespace character.\"\"\"\n    # \\t, \\n, and \\r are technically control characters but we treat them\n    # as whitespace since they are generally considered as such.\n    if char == \" \" or char == \"\\t\" or char == \"\\n\" or char == \"\\r\":\n        return True\n    cat = unicodedata.category(char)\n    if cat == \"Zs\":\n        return True\n    return False\n\n\ndef _is_control(char):\n    \"\"\"Checks whether `chars` is a control character.\"\"\"\n    # These are technically control characters but we count them as whitespace\n    # characters.\n    if char == \"\\t\" or char == \"\\n\" or char == \"\\r\":\n        return False\n    cat = unicodedata.category(char)\n    if cat in (\"Cc\", \"Cf\"):\n        return True\n    return False\n\n\ndef _is_punctuation(char):\n    \"\"\"Checks whether `chars` is a punctuation character.\"\"\"\n    cp = ord(char)\n    # We treat all non-letter\/number ASCII as punctuation.\n    # Characters such as \"^\", \"$\", and \"`\" are not in the Unicode\n    # Punctuation class but we treat them as punctuation anyways, for\n    # consistency.\n    if ((cp >= 33 and cp <= 47) or (cp >= 58 and cp <= 64) or\n            (cp >= 91 and cp <= 96) or (cp >= 123 and cp <= 126)):\n        return True\n    cat = unicodedata.category(char)\n    if cat.startswith(\"P\"):\n        return True\n    return False\n\n\ndef preprocess_text(inputs, remove_space=True, lower=False):\n    \"\"\"Preprocesses data by removing extra space and normalize data.\n\n    This method is used together with sentence piece tokenizer and is forked from:\n    https:\/\/github.com\/google-research\/google-research\/blob\/master\/albert\/tokenization.py\n\n    Args:\n        inputs: The input text.\n        remove_space: Whether to remove the extra space.\n        lower: Whether to lowercase the text.\n\n    Returns:\n        The preprocessed text.\n\n    \"\"\"\n    outputs = inputs\n    if remove_space:\n        outputs = \" \".join(inputs.strip().split())\n\n    if six.PY2 and isinstance(outputs, str):\n        try:\n            outputs = six.ensure_text(outputs, \"utf-8\")\n        except UnicodeDecodeError:\n            outputs = six.ensure_text(outputs, \"latin-1\")\n\n    outputs = unicodedata.normalize(\"NFKD\", outputs)\n    outputs = \"\".join([c for c in outputs if not unicodedata.combining(c)])\n    if lower:\n        outputs = outputs.lower()\n\n    return outputs\n\ndef encode_pieces(sp_model, text, sample=False):\n    \"\"\"Segements text into pieces.\n\n    This method is used together with sentence piece tokenizer and is forked from:\n    https:\/\/github.com\/google-research\/google-research\/blob\/master\/albert\/tokenization.py\n\n\n    Args:\n        sp_model: A spm.SentencePieceProcessor object.\n        text: The input text to be segemented.\n        sample: Whether to randomly sample a segmentation output or return a\n            deterministic one.\n\n    Returns:\n        A list of token pieces.\n    \"\"\"\n    if six.PY2 and isinstance(text, six.text_type):\n        text = six.ensure_binary(text, \"utf-8\")\n\n    if not sample:\n        pieces = sp_model.EncodeAsPieces(text)\n    else:\n        pieces = sp_model.SampleEncodeAsPieces(text, 64, 0.1)\n    new_pieces = []\n    for piece in pieces:\n        piece = printable_text(piece)\n        if len(piece) > 1 and piece[-1] == \",\" and piece[-2].isdigit():\n            cur_pieces = sp_model.EncodeAsPieces(piece[:-1].replace(\n                    SPIECE_UNDERLINE, \"\"))\n            if piece[0] != SPIECE_UNDERLINE and cur_pieces[0][0] == SPIECE_UNDERLINE:\n                if len(cur_pieces[0]) == 1:\n                    cur_pieces = cur_pieces[1:]\n                else:\n                    cur_pieces[0] = cur_pieces[0][1:]\n            cur_pieces.append(piece[-1])\n            new_pieces.extend(cur_pieces)\n        else:\n            new_pieces.append(piece)\n\n    return new_pieces\n\ndef encode_ids(sp_model, text, sample=False):\n    \"\"\"Segments text and return token ids.\n\n    This method is used together with sentence piece tokenizer and is forked from:\n    https:\/\/github.com\/google-research\/google-research\/blob\/master\/albert\/tokenization.py\n\n    Args:\n        sp_model: A spm.SentencePieceProcessor object.\n        text: The input text to be segemented.\n        sample: Whether to randomly sample a segmentation output or return a\n            deterministic one.\n\n    Returns:\n        A list of token ids.\n    \"\"\"\n    pieces = encode_pieces(sp_model, text, sample=sample)\n    ids = [sp_model.PieceToId(piece) for piece in pieces]\n    return ids\n\nclass FullSentencePieceTokenizer(object):\n    \"\"\"Runs end-to-end sentence piece tokenization.\n\n    The interface of this class is intended to keep the same as above\n    `FullTokenizer` class for easier usage.\n    \"\"\"\n\n    def __init__(self, sp_model_file):\n        \"\"\"Inits FullSentencePieceTokenizer.\n\n        Args:\n            sp_model_file: The path to the sentence piece model file.\n        \"\"\"\n        self.sp_model = spm.SentencePieceProcessor()\n        self.sp_model.Load(sp_model_file)\n        self.vocab = {\n                self.sp_model.IdToPiece(i): i\n                for i in six.moves.range(self.sp_model.GetPieceSize())\n        }\n\n    def tokenize(self, text):\n        \"\"\"Tokenizes text into pieces.\"\"\"\n        return encode_pieces(self.sp_model, text)\n\n    def convert_tokens_to_ids(self, tokens):\n        \"\"\"Converts a list of tokens to a list of ids.\"\"\"\n        return [self.sp_model.PieceToId(printable_text(token)) for token in tokens]\n\n    def convert_ids_to_tokens(self, ids):\n        \"\"\"Converts a list of ids ot a list of tokens.\"\"\"\n        return [self.sp_model.IdToPiece(id_) for id_ in ids]","5e6bb7a2":"# Download BERT uncased WWM model (https:\/\/tfhub.dev\/tensorflow\/bert_en_wwm_uncased_L-24_H-1024_A-16\/1)\n\ndef load_bert_and_tokenizer(bert_model_path):\n    # Load trainable BERT model and tokenizer\n    bert_layer = hub.KerasLayer(bert_model_path,\n                                trainable=True)\n    if 'albert_en_large' in bert_model_path:\n        sp_model_file = bert_layer.resolved_object.sp_model_file.asset_path.numpy()\n        tokenizer = FullSentencePieceTokenizer(sp_model_file)\n        return bert_layer,tokenizer\n    \n    elif 'bert_en_wwm_uncased_L-24_H-1024_A-16' in bert_model_path:\n        vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n        do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n        tokenizer = FullTokenizer(vocab_file, do_lower_case)\n        return bert_layer,tokenizer\n    else:\n        raise Exception(\"undefined model\")\n","652effd1":"### Need much resources.\ndef build_model(bert_layer,max_seq_length,lr=0.000001):\n    # Finetune BERT model and add a classification layer\n   \n    input_word_ids=tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,\n                                           name=\"input_word_ids\")\n    input_masks=tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,\n                                       name=\"input_mask\")\n    segment_ids=tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,\n                                        name=\"segment_ids\")\n    \n    pooled_output, sequence_output = bert_layer([input_word_ids, input_masks, segment_ids])\n    \n    first_token_tensor=sequence_output[:, 0, :]\n    \n    output=tf.keras.layers.Dense(1, activation='sigmoid')(first_token_tensor)\n    model = tf.keras.models.Model(inputs=[input_word_ids,input_masks,segment_ids], outputs=output)\n    model.compile(optimizer=tf.keras.optimizers.Adam(lr=lr), loss='binary_crossentropy', \n                      metrics=[f1,'accuracy'])\n    return model","6a8fe122":"def finetune_bert(bert_layer,max_seq_length,learning_rate,batch_size,\n                  X_id_train, X_mask_train, X_seg_train,y_train,\n                  X_id_valid,X_mask_valid,X_seg_valid,y_valid,\n                  class_weight,filename,output_folder,epochs=15):\n    # Train BERT model\n    print(\"***** Finetune BERT model *****\")\n    model=build_model(bert_layer,max_seq_length,lr=learning_rate)\n\n\n    file_path = filename+\".hdf5\"\n    check_point = ModelCheckpoint(file_path, monitor = \"val_loss\", verbose = 1, save_best_only = True, mode = \"min\") # only save the best epoch\n    early_stop = EarlyStopping(monitor = \"val_loss\", mode = \"min\", patience=3) # Stop when val_loss does not improve\n    hist = model.fit([X_id_train, X_mask_train, X_seg_train], \n                     y_train, batch_size=batch_size, epochs=epochs, \n                     validation_data=([X_id_valid,X_mask_valid,X_seg_valid], y_valid), verbose=2,\n                     class_weight=class_weight,\n                     shuffle=True, callbacks = [check_point, early_stop])\n    model.load_weights(filename+\".hdf5\") # Load the best epoch\n    \n    # Check loss values \n    loss=hist.history['loss']\n    val_loss=np.array(hist.history['val_loss'])\n    min_loss = val_loss.min()\n    best_epoch=val_loss.argmin()\n    max_f1=max(hist.history['val_f1'])\n    print(\"max f1:%f\"%max_f1)\n    print(\"min_loss:%f\"%min_loss)\n    print(\"best epoch:%d\"%best_epoch)\n    \n    with open('log.txt','a') as f:\n        f.write(\"%s, f1: %.3f, min_loss: %.3f, best epoch: %d\\n\"%(filename,max_f1,min_loss,best_epoch))\n    plot_loss(loss,val_loss,path=output_folder+'\/'+filename+\"_bert_Loss.png\")\n    return model\n    ","67dfe230":"# plot confusion matrix \ndef plot_con_matrix(confmat,path):\n   \n    fig, ax = plt.subplots(figsize=(3, 3))\n    ax.matshow(confmat, cmap=plt.cm.Blues, alpha=0.3)\n    for i in range(confmat.shape[0]):\n        for j in range(confmat.shape[1]):\n            ax.text(x=j, y=i, s=confmat[i,j], va='center', ha='center')\n    plt.xlabel('predicted label')        \n    plt.ylabel('true label')\n    plt.savefig(path)\n    \n# prediction score\ndef prediction_scores(test_df,output_folder,filename):\n    test_same=pd.read_csv(output_folder+'\/'+filename,index_col=0)\n    test_df.loc[test_same.index,'target']=test_same['target']\n    test_df.loc[test_same.index,'pred_prob']=test_same['pred_prob']\n    test_labels=test_df['label'].replace({-1:0})\n    pred_class=np.array(test_df['target'].values)\n    confmat=confusion_matrix(test_labels, pred_class)\n    precision=precision_score(test_labels, pred_class)\n    recall=recall_score(test_labels, pred_class)\n    f1=f1_score(test_labels, pred_class)\n    print(\"Scores for \"+output_folder+'\/'+filename)\n    print(\"precision score:%.3f\"%precision)\n    print(\"recall score:%.3f\"%recall)\n    print(\"fl score:%.3f\"%f1)\n    plot_con_matrix(confmat,'%s\/confusion_matrix_bert.png'%output_folder)\n    test_df.to_csv(output_folder+'\/full_'+filename)\n    \n    \n# Predict\ndef make_prediction(test_df,prediction,output_folder,submission_name):\n    # Predict\n    \n    pred_class=np.array(prediction>0.5,dtype=int)\n    test_df['target']=pred_class\n    sub=test_df['target']\n    sub.to_csv(output_folder+'\/'+submission_name+'.csv',header=True)\n    sub=pd.DataFrame(sub)\n    sub['pred_prob']=prediction\n    sub.to_csv(output_folder+'\/'+submission_name+'_prob.csv',header=True)\n    prediction_scores(test_df,output_folder,submission_name+'_prob.csv')\n\n\n# display f1 score during running\ndef f1(y_true, y_pred):\n    def recall(y_true, y_pred):\n        \"\"\"Recall metric.\n\n        Only computes a batch-wise average of recall.\n\n        Computes the recall, a metric for multi-label classification of\n        how many relevant items are selected.\n        \"\"\"\n        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n        recall = true_positives \/ (possible_positives + K.epsilon())\n        return recall\n\n    def precision(y_true, y_pred):\n        \"\"\"Precision metric.\n\n        Only computes a batch-wise average of precision.\n\n        Computes the precision, a metric for multi-label classification of\n        how many selected items are relevant.\n        \"\"\"\n        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n        precision = true_positives \/ (predicted_positives + K.epsilon())\n        return precision\n    precision = precision(y_true, y_pred)\n    recall = recall(y_true, y_pred)\n    return 2*((precision*recall)\/(precision+recall+K.epsilon()))\n\n# plot loss function\ndef plot_loss(loss,val_loss,path=\"Model_Loss.png\"):\n    # Plot training and validation loss values\n    plt.figure()\n    plt.plot(loss)\n    plt.plot(val_loss)\n    plt.title('model loss')\n    plt.ylabel('loss')\n    plt.xlabel('epochs')\n    plt.legend(['train','valid'],loc='upper left')\n    plt.savefig(path)\n\n","0b60c026":"#tf.enable_eager_execution() # For tensorflow 1\n#gpus = tf.config.experimental.list_physical_devices('GPU')\n#tf.config.experimental.set_memory_growth(gpus[0], True)\nbert_model_path='https:\/\/tfhub.dev\/tensorflow\/bert_en_wwm_uncased_L-24_H-1024_A-16\/1'\n#bert_model_path='..\/albert_en_xlarge'\n\n'''\nHyperparameters. \n'''\nlearning_rate=0.000008\nlr_class_model=0.0005\nbatch_size=4\nmax_seq_length=512 # longest sequence after converting is 1200. But BERT can only accept up to 512.\n\n\n''' \nData pre-processing\n'''\n\n# Load training data\ntrain_df = pd.read_csv(f'{bert_data_path}\/train_sample.csv',encoding='utf-8',na_filter= False,\n                       index_col=0,dtype={'title':str,'abstract':str})\ntrain_df['title_count']=train_df['title'].str.split().apply(len)\ntrain_df['abstract_count']=train_df['abstract'].str.split().apply(len)\ntrain_df['total_count']=train_df['title_count']+train_df['abstract_count']\ntrain_df['label']=train_df['label'].replace({-1:0})\nprint(train_df['total_count'].max()) # Max word count\n\n# Load test data\ntest_df  = pd.read_csv(f'{bert_data_path}\/test_sample.csv',encoding='utf-8',na_filter= False,\n                       index_col=0,dtype={'title':str,'abstract':str})\ntest_df['title_count']=test_df['title'].str.split().apply(len)\ntest_df['abstract_count']=test_df['abstract'].str.split().apply(len)\ntest_df['total_count']=test_df['title_count']+test_df['abstract_count']\ntest_df['label']=test_df['label'].replace({-1:0})\nprint(test_df['total_count'].max())\n\nif if_finetune_bert_model: # Finetune BERT model\n    # Load tokenizer for BERT model\n    print(\"Load tokenizer\")\n    bert_layer,tokenizer=load_bert_and_tokenizer(bert_model_path)\n\n    train_target=np.array(train_df['label'])\n\n    # Unbalance data. Use class_weight\n    neg_target=(train_target==0).sum()\n    pos_target=(train_target==1).sum()\n    print(\"Negtive samples: %d\"%neg_target)\n    print(\"Positive samples: %d\"%pos_target)\n    pos_weight=neg_target\/pos_target\n    class_weight = {0: 1, 1: pos_weight} \n    print(\"class_weight\",class_weight)\n\n    # Convert text format for BERT model\n    train_texts_input_ids, train_texts_input_masks, train_texts_segment_ids=convert_text_format(\n            train_df,tokenizer,max_seq_length) \n\n    test_texts_input_ids, test_texts_input_masks, test_texts_segment_ids=convert_text_format(\n            test_df,tokenizer,max_seq_length)\n\n    '''\n    Split 80% data for training and 20% data for validation.\n    '''    \n    from sklearn.model_selection import train_test_split\n    X_id_train, X_id_valid, X_mask_train, X_mask_valid,X_seg_train, X_seg_valid,y_train, y_valid = \\\n            train_test_split(\n            train_texts_input_ids, train_texts_input_masks, \n            train_texts_segment_ids, \n            train_target, test_size=0.2)\n    \n    model=finetune_bert(bert_layer,max_seq_length,learning_rate,batch_size,\n                  X_id_train, X_mask_train, X_seg_train,y_train,\n                  X_id_valid,X_mask_valid,X_seg_valid,y_valid,\n                  class_weight,bert_model_name,output_folder,epochs=10)\n    print(model.summary())\n\n    # Load the best epoch\n    model=build_model(bert_layer,max_seq_length,lr=learning_rate)\n    model.load_weights(bert_model_name+\".hdf5\")\n\n    # Predict our PubMed test data \n    X_test=[test_texts_input_ids, test_texts_input_masks, test_texts_segment_ids] # input for BERT model\n    submission_name='submission_bert'\n    pred_bert = model.predict(X_test) \n    make_prediction(test_df,pred_bert,output_folder,submission_name)","4691d729":"def load_test_prediction(input_folder,output_folder,filename,model):\n    # Load test data\n    print('Load %s\/%s.csv'%(input_folder,filename))\n    test_df  = pd.read_csv('%s\/%s.csv'%(input_folder,filename),encoding='utf-8',na_filter= False,\n                           index_col=0,dtype={'title':str,'abstract':str}) # read test data\n    test_df=test_df[test_df['abstract'].notnull()]\n    test_df.fillna(' ',inplace=True)\n    if 'meta' in filename:\n        test_df=test_df[['pubmed_id','title','abstract']]\n    else:\n        test_df=test_df[['title','abstract']]\n\n    test_texts_input_ids, test_texts_input_masks, test_texts_segment_ids=convert_text_format(\n            test_df,tokenizer,max_seq_length) # texts to BERT model format\n    X_test=[test_texts_input_ids, test_texts_input_masks, test_texts_segment_ids] # input for BERT model\n    submission_name='%s_submission_bert'%filename\n    prediction = model.predict(X_test)\n    pred_class=np.array(prediction>0.5,dtype=int)\n    test_df['prediction']=pred_class\n    test_df['pred_prob']=prediction\n    test_df.to_csv(output_folder+'\/'+submission_name+'.csv',header=True)","21a551ff":"if if_finetune_bert_model: # Finetune BERT model\n    # Predict COVID-19 data\n    filenames=['model_positive','model_negative']\n    input_folder=f'{bert_data_path}'\n    output_folder=f'{bert_data_path}'\n    for filename in filenames:\n        load_test_prediction(input_folder,output_folder,filename,model)","96e6dc7e":"# Load titles of all papers in the covid-19 dataset to exclude papers\n# in both the covid-19 dataset and the pubmed papers for background edges \ncovid19_titles = set(df.title.str.strip().str.lower())\n\n# Function for getting edges and related sentences\ndef get_edges_sents(df, phrases = '', pubmed = False):\n    \"\"\"\n    Develop edges between lemmatized words or phrases in a sentence,\n    and keep the relevant sentences for each edge.\n    Return a dictionary of counts for each edge, and a dictionary of\n    relevant sentences for each edge. \n    Args:\n        df: a dataframe of titles and abstracts by ids.\n        phrases: a list of phases split by ',' which containing words need to be considered as phrases.\n        pubmed: for pubmed titles and abstracts, relevant sentences for each edge will not be kept.\n    \"\"\"\n    assert isinstance(phrases, str)\n    edges_and_counts = {}\n    sents_per_edge = {}\n    if phrases != '':\n        phrases = ','.join([' '.join([wnlem.lemmatize(word.lower()) for word in phrase.split()]) for phrase in phrases.split(',')])\n        phrases = [phrase.split() for phrase in phrases.split(',')]\n    for idx, row in df.iterrows():\n        if pubmed:\n            title = row.title.strip()\n            if title.endswith('.'): title = title[:-1]\n            if title in covid19_titles: continue\n        title_abstract = ' '.join([row.title, row.abstract])\n        if not row.title.strip().endswith('.'):\n            title_abstract = ' '.join([row.title.strip() + '.', row.abstract])\n        title_abstract = text_preprocess(title_abstract)\n        sentences = sent_tokenize(title_abstract)\n        for j, sentence in enumerate(sentences):\n            try:\n                if model.predict(sentence)[0][0] != '__label__en': continue\n            except: continue\n            if 'ELECTRONIC SUPPLEMENTARY MATERIAL' in sentence: continue\n            if 'CC-BY' in sentence: continue\n            sentence = sentence.strip()\n            if re.match(r'^(the )?author funder,', sentence): continue\n            sentence = sentence_preprocess(sentence)\n            word_list = word_tokens(sentence)\n            if len(word_list) < 5 or len(word_list) > 50: continue\n            word_list = list(set(word_list))\n            if phrases != '':\n                phrase_list = set()\n                for phrase in phrases:\n                    count = 0\n                    for word in phrase:\n                        if word in word_list: count += 1\n                    if count == len(phrase):\n                        word_list = [w for w in word_list if w not in phrase]\n                        phrase_list.add(' '.join(phrase))\n                word_list = list(phrase_list | set(word_list))\n            word_list.sort()\n            pairs = combinations(word_list, 2)\n            for pair in pairs:\n                word_pair = '|'.join(list(pair))\n                edges_and_counts[word_pair] = edges_and_counts.get(word_pair, 0) + 1\n                if pubmed: continue\n                sentinfo = {'sentence':sentence, 'title':row.title, 'authors':row.authors,\n                            'journal':row.journal, 'publish_time':row.publish_time, 'url':row.url}\n                if word_pair in sents_per_edge: sents_per_edge[word_pair].append(sentinfo)\n                else: sents_per_edge[word_pair] = [sentinfo]\n    return edges_and_counts, sents_per_edge\n\n# function to write covid19 edges and sents into tsv files\ndef write_edges_sents(covid19_edges, covid19_sents_per_edge, pubmed_edges, dataset_name):\n    \"\"\"\n    Write the nodes of each edge, counts of the edge and relevant sentences of the edge\n    into a csv file\n    Args:\n        covid19_edges: the dictionary of counts for each edge of the covid-19 subset data.\n        covid19_sents_per_edge: the dictionary of relevant sentences for each edge of the covid-19 subset data.\n        pubmed_edges: the dictionary of counts for each edge of the pubmed background edge data.\n        dataset_name: a string of text as the csv file name\n    \"\"\"\n    assert isinstance(dataset_name, str)\n    sents_file = open(f'{edges_path}\/covid19_edges_sents_{dataset_name}.csv', 'w',\n                      encoding = 'utf-8', newline = '')\n    writer = csv.writer(sents_file)\n    for k, v in covid19_edges.items():\n        if k in pubmed_edges: continue\n        writer.writerow(k.split('|') + [str(v), covid19_sents_per_edge[k]])\n    sents_file.close()","2a57b1b3":"# edges of a task\nif not if_run_all:\n    df_tiabs = pd.read_csv(f'{pubmed_tiabs_path}\/covid19_tiabs_task_{task_num}.csv', na_filter= False)\n    df_subset = pd.read_csv(f'{covid19_dataset_path}\/covid19_subset_task_{task_num}.csv', na_filter= False)\npubmed_edges, _ = get_edges_sents(df_tiabs, KOI, pubmed = True)\ncovid19_edges, covid19_sents_per_edge = get_edges_sents(df_subset, KOI)\ncovid19_edges = dict(sorted(covid19_edges.items(), key=lambda x:x[1], reverse=True))\nwrite_edges_sents(covid19_edges, covid19_sents_per_edge, pubmed_edges, f'task_{task_num}')","e9b2e08d":"# Function for getting vocabulary co-occuer with key words\ndef get_cooccur_vocab(df, phrases, if_and = False):\n    \"\"\"\n    Develop a vocabulary of nouns co-occur with the keywords in the phrases.\n    Return a dictionary of paper counts and sentence counts of each noun.\n    Args:\n        df: a dataframe of titles and abstracts.\n        phrases: a list of phases split by ','\n        if_and: if True, develop vocabulary co-occur with all keywords in the phrases,\n                if False, develop vocabulary co-occur with any keywords in the phrases\n    \"\"\"\n    assert isinstance(phrases, str) and phrases != ''\n    cooccur_vocab = {}\n    phrases = ','.join([' '.join([wnlem.lemmatize(word.lower()) for word in phrase.split()]) for phrase in phrases.split(',')])\n    phrases = [phrase.split() for phrase in phrases.split(',')]\n    for idx, row in df.iterrows():\n        title_abstract = ' '.join([row.title, row.abstract])\n        if not row.title.strip().endswith('.'):\n            title_abstract = ' '.join([row.title.strip() + '.', row.abstract])\n        title_abstract = text_preprocess(title_abstract)\n        sentences = sent_tokenize(title_abstract)\n        doc_vocab = {}\n        for j, sentence in enumerate(sentences):\n            try:\n                if model.predict(sentence)[0][0] != '__label__en': continue\n            except: continue\n            if 'ELECTRONIC SUPPLEMENTARY MATERIAL' in sentence: continue\n            if 'CC-BY' in sentence: continue\n            sentence = sentence.strip()\n            if re.match(r'^(the )?author funder,', sentence): continue\n            sentence = sentence_preprocess(sentence)\n            word_list = word_tokens(sentence)\n            if len(word_list) < 5 or len(word_list) > 50: continue\n            word_list = list(set(word_list))\n            phrase_list = set()\n            count_phrase = 0\n            for phrase in phrases:\n                count_word = 0\n                for word in phrase:\n                    if word in word_list: count_word += 1\n                if count_word == len(phrase):\n                    word_list = [w for w in word_list if w not in phrase]\n                    phrase_list.add(' '.join(phrase))\n                    count_phrase += 1\n            if count_phrase == 0: continue\n            noun_list = tagged_word_tokens(sentence)\n            word_list = [w for w in word_list if w in noun_list]\n            if if_and:\n                if count_phrase != len(phrases): continue\n            word_list = list(phrase_list | set(word_list))\n            for w in word_list:\n                doc_vocab[w] = doc_vocab.get(w, 0) + 1\n        for w, c in doc_vocab.items():\n            if w in cooccur_vocab:\n                cooccur_vocab[w][0], cooccur_vocab[w][1] = cooccur_vocab[w][0] + 1, cooccur_vocab[w][1] + c\n            else: cooccur_vocab[w] = [1, int(c)]\n    return cooccur_vocab","9fe08e55":"# co-occuering vocabulary for a task\n# df_subset = pd.read_csv(f'{covid19_dataset_path}\/covid19_subset_task_{task_num}.csv', na_filter= False)\ndf_subset = df_subset[['cord_uid', 'title', 'abstract']]\ncooccur_vocab = get_cooccur_vocab(df_subset, KOI)\ncooccur_vocab = dict(sorted(cooccur_vocab.items(), key = lambda x:x[1][1], reverse = True))\njson.dump(cooccur_vocab, open(f'{edges_path}\/covid19_cooccur_vocab_task_{task_num}.json',\n                               'w', encoding = 'utf-8'), indent = 4)","9bda4a97":"# import some useful packages\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib\nimport networkx as nx\nimport pandas as pd\nimport random\nimport json\nimport os\nimport re\nimport difflib\nimport random\nimport requests\nimport ipywidgets as widgets\nfrom IPython.display import display\nfrom random import sample \nfrom ipywidgets import HBox, Label\n\n# latex rendering of text in graphs\nimport matplotlib as mpl\nmpl.rc('text', usetex = False)\nmpl.rc('font', family = 'serif')\n\n%matplotlib inline\n\n!pip install visJS2jupyter\n# import visJS_2_jupyter \nfrom visJS2jupyter import visJS_module","4a9e9d65":"# function get_sub_edges is to extract a subset of edges from input edges dataframe\n# df_edges: input edge dataframe\n# node_cooccur: keywords in type II edges\n# top_node: number of top type I nodes ranked by degrees to be selected to display in network \n# top_node_cooccur: number of type II nodes to be selected to display in network\n# edge_top_perc: how many percentage of edges used for calculation of top_node\n\ndef get_sub_edges(df_edges, node_cooccur, top_node = 100, \n                  top_node_cooccur = 1, edge_top_perc= 10): \n    edge_num_total = df_edges.shape[0]\n    edge_num = int(edge_num_total*edge_top_perc\/100)\n    edge_df_sub = df_edges.iloc[0:edge_num, :]\n    G_covid19 = nx.from_pandas_edgelist(edge_df_sub,source='term1',target='term2')\n    sorted_node = sorted(G_covid19.degree, key=lambda x: x[1], reverse=True)\n\n    # select the top 100 nodes ranked by degree\n    total_type1_node = 1000\n    type1_nodes = [] # top 100 node\n    if len(sorted_node) < total_type1_node:\n        total_type1_node = len(sorted_node)\n        \n    for i in range(0,total_type1_node):\n        type1_nodes.append(sorted_node[i][0])\n    \n    # all the co-occurred nodes within top 100 nodes\n    type2_nodes = []\n    for node_c in list(node_cooccur.keys()):\n        if node_c in type1_nodes:\n            type2_nodes.append(node_c)\n    \n    # type1 nodes to be display\n    selected_type1_nodes = []\n    if len(sorted_node) < top_node:\n        top_node = len(sorted_node)\n        \n    for i in range(0,top_node):\n        selected_type1_nodes.append(sorted_node[i][0])\n        \n    selected_type2_nodes = []\n    len_type2_nodesccur = len(type2_nodes)\n    if top_node_cooccur > len_type2_nodesccur:\n        top_node_cooccur = len_type2_nodesccur\n    if top_node_cooccur > 0:\n        selected_type2_nodes = type2_nodes[0:top_node_cooccur]\n\n    for node in selected_type2_nodes:\n        if node not in selected_type1_nodes:\n            selected_type1_nodes.append(node)\n\n    edge_df_sel = edge_df_sub[edge_df_sub.term1.isin(selected_type1_nodes) & edge_df_sub.term2.isin(selected_type1_nodes)] \n            \n       \n    return(selected_type1_nodes, len_type2_nodesccur, selected_type2_nodes, edge_df_sel)\n\n\n# function show_sentences is to format sentences to show when clicking on edge\n# edges_with_data: edges contains sentences info\n# max_words: the maximum number words in each line for sentence display\n# max_sentences: the maximum number of sentenes to display\ndef show_sentences(edges_with_data, max_words = 16, max_sentences = 10):\n    for i in range(len(edges_with_data)):\n#         sents = eval(edges_with_data[i][2]['sentences'])\n        sents = edges_with_data[i][2]['sentences']\n        mystring = \"Edge \" + \"--\".join(edges_with_data[i][0:2]) + \" derived from sentences like: <br\/> <br\/> \"\n        num_paper = len(sents)\n        sel_paper = range(num_paper)\n        if num_paper > max_sentences:\n            sel_paper = sample(range(num_paper),max_sentences)\n        for k in sel_paper:\n            sent_p1 = sents[k]['sentence']\n            sent_p2 = sents[k]['url']         \n            TT_s = sent_p1.split(\" \")\n            if len(TT_s) > max_words:\n                for j in range(0, int(len(TT_s)\/max_words)+1):\n                    mystring = mystring + \" \".join(TT_s[max_words*j:max_words*j+max_words]) + \" <br\/>\"\n            else:\n                mystring = mystring + \" \".join(TT_s) + \" <br\/>\"\n\n            mystring = mystring + sent_p2 + \" <br\/><br\/>\"\n\n        edges_with_data[i][2]['sentences'] = mystring\n\n    return(edges_with_data)           \n    \n\n# function generate_network is to draw network\n# edge_df_sel: selected edges dataframe, returned from function get_sub_edges\n# key_node: KEY_WORD\n# scaling_factor: control the resolution of network, larger number, higher resolution\ndef generate_network(edge_df_sel, k_nodes, scaling_factor):\n    key_nodes = k_nodes\n    G_covid19 = nx.from_pandas_edgelist(edge_df_sel,source='term1',target='term2',edge_attr = ['score','sentences'])\n\n    nodes = list(G_covid19.nodes()) # must cast to list to maintain compatibility between nx 1.11 and 2.0\n    edges = list(G_covid19.edges()) # will return an \"EdgeView\" object in nx 2.0\n\n    pos = nx.spring_layout(G_covid19,k=1.0)\n\n    nodes = list(G_covid19.nodes()) # to make compatible between nx 1.11 and 2.0, must cast to list\n\n    numnodes = len(nodes)\n    edges = list(G_covid19.edges()) # to make compatible between nx 1.11 and 2.0, must cast to list\n    edges_with_data = list(G_covid19.edges(data=True)) # to make compatible between nx 1.11 and 2.0, must cast to list\n    numedges = len(edges)\n\n    # add a node attributes to color-code by\n    cc = nx.clustering(G_covid19)\n    degree = dict(G_covid19.degree()) # to make compatible between nx 1.11 and 2.0, must cast to dict\n    bc = nx.betweenness_centrality(G_covid19)\n    nx.set_node_attributes(G_covid19, name = 'clustering_coefficient', values = cc) # must explicitly define arguments\n    nx.set_node_attributes(G_covid19, name = 'degree', values = degree)             # for compatibility with nx 1.11 and 2.0\n    nx.set_node_attributes(G_covid19, name = 'betweenness_centrality', values = bc)\n\n    # set node_size to degree\n    node_size = [int(float(n)\/np.max(list(degree.values()))*300+1) for n in list(degree.values())]\n    node_to_nodeSize = dict(zip(degree.keys(),node_size))\n\n    sorted_nodeSize = sorted(node_to_nodeSize.items(), key=lambda kv: kv[1], reverse=True)\n    max_nodeSize = sorted_nodeSize[0][1]\n    \n    # # add nodes to highlight (key node)\n    nodes_HL = [0 if node in key_nodes else 0 for node in G_covid19.nodes()]  \n    nodes_HL = dict(zip(G_covid19.nodes(),nodes_HL))\n\n    nodes_shape=[]\n    node_shape = ['hexagon' if (node in key_nodes) else 'dot' for node in G_covid19.nodes()]\n    node_to_nodeShape=dict(zip(G_covid19.nodes(),node_shape))\n\n    # add a field for node labels\n    node_labels_temp = []\n    # list_of_genes = list(np.setdiff1d(G_covid19.nodes(),d_list))\n    for node in G_covid19.nodes():\n        label_temp = node\n        label_temp+= '<br\/>'\n        label_temp+='degree: ' + str(nx.degree(G_covid19,node)) + '<br\/>'\n\n        node_labels_temp.append(label_temp)\n\n    node_labels = dict(zip(G_covid19.nodes(),node_labels_temp))\n\n    node_titles = [node for node in G_covid19.nodes()]\n\n    node_titles = dict(zip(G_covid19.nodes(),node_titles))\n\n\n    edges_with_data = show_sentences(edges_with_data, max_words = 16)\n\n\n#     node_to_color = visJS_module.return_node_to_color(G_covid19,field_to_map='degree',cmap=mpl.cm.jet,alpha = 1)\n    node_to_color = dict(zip(nodes, ['rgba(85, 85, 85, 1)']*len(nodes)))\n    for k_node in key_nodes:\n        node_to_color[k_node] = 'green'\n\n    edge_to_color = visJS_module.return_edge_to_color(G_covid19,field_to_map = 'score',cmap=mpl.cm.Blues,alpha=1)\n\n    nodes_dict = [{\"id\":n,\"degree\":G_covid19.degree(n),\"color\":node_to_color[n],\n                  \"node_size\":node_to_nodeSize[n],'border_width':nodes_HL[n],\n                  \"node_label\":node_labels[n],\n                  \"title\":node_labels[n],\n                  \"node_shape\":node_to_nodeShape[n],\n                  \"x\":pos[n][0]*500*scaling_factor,\n                  \"y\":pos[n][1]*500*scaling_factor} for n in nodes\n                  ]\n\n\n    node_map = dict(zip(nodes,range(numnodes)))  # map to indices for source\/target in edges\n\n    # edges_dict = [{\"source\":node_map[edges[i][0]], \"target\":node_map[edges[i][1]], \n    #               \"color\":edge_to_color[edges[i]],\"title\":edges_with_data[i][2]['score']} for i in range(numedges)]\n    edges_dict = [{\"source\":node_map[edges[i][0]], \"target\":node_map[edges[i][1]], \n                  \"e_label\":edges_with_data[i][2]['sentences']} for i in range(numedges)]\n\n\n\n\n    p = visJS_module.visjs_network(nodes_dict, edges_dict,\n                               node_color_highlight_border=\"white\",\n                               node_color_hover_border = 'orange',\n                               node_color_hover_background = 'rgb(119, 130, 140, 1)',\n                               node_color_border='black',\n                               node_size_field='node_size',\n                               node_size_transform='Math.sqrt',\n                               node_size_multiplier=3*scaling_factor,\n                               node_border_width=1*scaling_factor,\n                               node_font_size = 25*scaling_factor,\n                               hover = True,\n                               edge_title_field='e_label',\n                               edge_width = 3*scaling_factor,\n                               edge_color_hover = 'red',\n                               edge_color_opacity = 0.5,  \n                               physics_enabled=False,\n                               min_velocity=.5,\n                               min_label_size=12*scaling_factor,\n                               max_label_size=25*scaling_factor,\n                               max_visible=10*scaling_factor,\n                               scaling_factor=scaling_factor,\n                               tooltip_delay = 300,\n                               graph_id = 1,\n                               graph_title = '' )\n    \n    return(p)\n\n\n# function show_network is to display network with control slider bar\n# df_edges: the original dataframe of edges\n# node_cooccur: type II nodes\n# s_factor: control the resolution of network, larger number, higher resolution\n# key_node: KEY_WORD\n\ndef show_network(df_edges, node_cooccur, s_factor = 1, key_node = ['risk factor']):\n    output_network = widgets.Output()\n    custom_style = {'description_width': 'initial'}\n    len_type2_nodesccur = 50\n    \n    # define a slider to control the number of top type I nodes\n    bounded_top_node = widgets.IntSlider(min=1, max=100, value=10, \n                                         step=1, description = \"\", \n                                         style=custom_style)\n    \n    # define a slider to control the number of top type II nodes\n    bounded_top_key_node = widgets.IntSlider(min=0, max=len_type2_nodesccur, value=1, \n                                         step=1, description = \"\", \n                                         style=custom_style)\n     \n    def draw_network(num_t_node, num_t_key_node):\n        output_network.clear_output()\n        selected_type1_nodes, len_type2_nodesccur, selected_type2_nodes, edge_df_sel = get_sub_edges(df_edges, \n                                                                  node_cooccur = node_cooccur,  \n                                                                  top_node = num_t_node, \n                                                                  top_node_cooccur = num_t_key_node)\n\n        network = generate_network(edge_df_sel, k_nodes = selected_type2_nodes, scaling_factor = s_factor)\n\n        with output_network:\n            display(network)  \n\n\n    def bounded_top_node_eventhandler(change):\n        draw_network(change.new, bounded_top_key_node.value)\n    def bounded_top_key_node_eventhandler(change):\n        draw_network(bounded_top_node.value, change.new)\n\n    bounded_top_node.observe(bounded_top_node_eventhandler, names='value')\n    bounded_top_key_node.observe(bounded_top_key_node_eventhandler, names='value')\n\n    display(HBox([Label('Number of top words'), bounded_top_node]))\n    display(HBox([Label('Number of top words co-occurred with selected key words'), bounded_top_key_node]))\n    display(output_network)\n\n    \n# function get_df_edges_sents is generete a edge dataframe from three different edges: \n# covid19_edges, covid19_sents_per_edge, pubmed_edges\ndef get_df_edges_sents(covid19_edges, covid19_sents_per_edge, pubmed_edges):\n    uniq_keys = []\n    for key in covid19_edges:\n        if key not in pubmed_edges:\n            uniq_keys.append(key)\n            \n    uniq_covid19_edges = dict((k, covid19_edges[k]) for k in uniq_keys)\n    uniq_covid19_sents_per_edge = dict((k, covid19_sents_per_edge[k]) for k in uniq_keys)\n    df_covid19 = pd.DataFrame(uniq_covid19_edges.items())\n    df_covid19.columns = ['terms', 'count']\n    df_sents_per_edge = pd.DataFrame(uniq_covid19_sents_per_edge.items())\n    df_sents_per_edge.columns = ['terms', 'sentences']\n    df = pd.merge(df_covid19, df_sents_per_edge, on=\"terms\")\n    df[['term1','term2']] = df.terms.str.split('|', expand=True)\n    df = df[['term1','term2', 'count', 'sentences']]\n    return df\n","4115653e":"# generete a edge dataframe from three different edges\nedge_df = get_df_edges_sents(covid19_edges, covid19_sents_per_edge, pubmed_edges)\nedge_df.columns = ['term1', 'term2', 'score', 'sentences']\nedge_df['score'] = edge_df['score']\/edge_df.score.max()\nedge_df['score'] = edge_df['score'].round(2)","aad56ba3":"#################################################\n### move the sliders to have network shown up ###\n#################################################\nshow_network(df_edges = edge_df, node_cooccur = cooccur_vocab, s_factor = 1)","08fb8ac9":"url=input_path + 'figure\/network1.png'\nImage(requests.get(url).content, width=800, height=800)","dea1ee45":"# Acquiring some PubMed abstracts for generating training data for document retrievel and background edges for information extraction\nThe PubMed database contains more than 30 million publications of biomedical research. We will download a subset of PubMed abstracts relevant to the KOI to generate data for training deep learning models as described in Introduction. The abstracts will also be used for generating background edges for the second module, information extraction.\n\nIn addition to abstracts relevant to the KOI, we also randomly sampled some abstracts to serve as negative samples in deep learning model training. We limited the publication time to between Jan. 1, 2009 and Dec. 31, 2018. \n","d4217b04":"# Words that co-occur with KOI\n\nIn order to ensure the network constructed contains information of key concepts and relationships relevant to KOI, we developed a function to extract the set of words co-occurs with KOI in the same sentence. This set of words is used together with the network to presents the key concepts and relationships relevant to KOI.","b3a3c62d":"# The knowledge network","d29dc7d6":"Make a covid19 subset for a task","8040c4e0":"Choose a task","8c29afc4":"####  Load BERT model","49a92577":"# Information extraction\n\nIn information extraction, we extract interesting relationships in the relevant articles to build an interactive network through which researchers can navigate the existing knowledge about COVID-19 on the topic defined by KOI.\nA function was developed to extract relationships (edges) between a pair of concepts. Using the function, we tokenized each sentence in the titles and abstracts into words after removing stop words. Word normalization and lemmatization were also performed. An edge was constructed for each pair of words in the sentence. The words and edges from all the sentences then form a network. This network contains many trivial edges. For example, if \"protein interaction network\" is in the sentence and they are treated as three separate words, then there will be three edges between any pair of the two words. These edges make the network look very crowded. We would like to filter out less interesting edges.\n\nTo do that, we used a large number of PubMed articles queried using KOI, but not in the COVID-19 dataset, to construct a background network. We subtract the edges in the background network from the network constructed for COVID-19. \n","439d3b40":"### Let's run the BERT model","9bf53a2d":"###  load packages","74ccafb3":"## Building training dataset\nRead positive papers and delete the KOI to build the positive training data\n<br>\nRead the negative papers to build the negative training data\n<br>\nMerge the positive and negative cases and build training and validation data with the ratio 4:1\n<br>\nSave the result","d9dee469":"# Import packages","83c10d9a":"## List of keywords for each task","b1f6a75f":"## Acquiring PubMed papers\nAfter a list of pmids was returned for a given KOI, we randomly selected 100,000 PMIDs from the list. Then we retrieve the titles and abstracts of the papers with the selected PMIDs from PubMed. The following functions read the list of PMIDs and retrieve paper titles and abstracts from the PubMed database.","65b07dcb":"Query list of pmids for pubmed papers contianing keywords of a task","30433783":"# The knowledge network of COVID-19 through text mining of COVID-19 literature","2614ced2":"# Conclusion of document retrieval\n\nIn this section, we show the results of the trained model, as well as providing discussions of its performance. Note that the model is applied to the articles containing KOI (keywords of Interest) and articles without KOI, respectively. The probability cutoff for calling positive predictions is 0.5.\n\n| Cross-table          | Predicted positive | Predicted negative | Total |\n| -----------          | -----------------: | -----------------: | ----: |\n| Articles without KOI |               374  |              33019 | 33393 |\n\nWe can see that although some articles do not contain KOI, our model still predicted them to be relevant to KOI. We sorted the predicted probabilities for the articles without KOI and manually read some of the articles to check whether they are indeed related to KOI. We will highlight the texts that indicate actual meaning of risk factor, followed by our analysis.\n\nPositive examples with high prediction score: \n\n1. ***Article:*** paper id: nj2707mv, Prediction Score: 0.990073\t\n    - ***Title:*** Association of Dynamic Changes in the ***CD4 T-Cell Transcriptome With Disease Severity During Primary Respiratory Syncytial Virus Infection*** in Young Infants\n    - ***Abstract:*** BACKGROUND: Nearly all children are infected with respiratory syncytial virus (RSV) within the first 2 years of life, with a minority developing severe disease (1%\u20133% hospitalized). We hypothesized that an assessment of the adaptive immune system, using CD4(+) T-lymphocyte transcriptomics, would identify gene expression correlates of disease severity. METHODS: Infants infected with RSV representing extremes of clinical severity were studied. Mild illness (n = 23) was defined as a ***respiratory rate (RR) < 55 and room air oxygen saturation (SaO(2)) \u2265 97%***, and severe illness (n = 23) was defined as RR \u2265 65 and SaO2 \u2264 92%. RNA from fresh, sort-purified CD4(+) T cells was assessed by RNA sequencing. RESULTS: ***Gestational age, age at illness onset, exposure to environmental tobacco smoke, bacterial colonization, and breastfeeding*** were associated (adjusted P < .05) with disease severity. RNA sequencing analysis reliably measured approximately 60% of the genome. Severity of RSV illness had the greatest effect size upon CD4 T-cell gene expression. Pathway analysis identified correlates of severity, including JAK\/STAT, prolactin, and interleukin 9 signaling. We also identified genes and pathways associated with timing of symptoms and RSV group (A\/B). CONCLUSIONS: These data suggest fundamental changes in adaptive immune cell phenotypes may be associated with RSV clinical severity.\n    - ***Analysis:*** This paper talks about the risk factors of disease severity during primary Respiratory Syncytial Virus(RSV) infection. Risk factors include: gestational age, age at illness onset, exposure to environmental tobacco smoke, bacterial colonization, and breastfeeding. It is obvious that this pape is a positive case and the prediction score is also high. \n    \n2. ***Article:*** Paper id: 1aluscl8, Prediction Score: 0.986705\n    - ***Title:*** Ebola Hemorrhagic Fever as a Public Health Emergency of International Concern; a Review Article\n    - ***Abstract:*** Ebola hemorrhagic fever (EHF) was first reported in 1976 with two concurrent outbreaks of acute viral hemorrhagic fever centered in Yambuku (near the Ebola river), Democratic Republic of Congo, and in Nzara, Sudan. The current outbreak of the Ebola virus was started by reporting the first case in March 2014 in the forest regions of southeastern Guinea. Due to infection rates raising over 13,000% within a 6-month period, Ebola is now considered as a global public health emergency and on August 8(th), 2014 the World Health Organization (WHO) declared the epidemic to be a Public Health Emergency of International Concern. With more than 5000 involved cases and nearly 3000 deaths, this event has turned into the largest and most dangerous Ebola virus outbreak in the world. Based on the above-mentioned, the present article aimed to review the ***virologic characteristics, transmission, clinical manifestation, diagnosis, treatment, and prevention of Ebola virus disease.***\n    - ***Analysis:*** This paper pays attention to the disease Ebola hemorrhagic fever (EHF). Since it is aimed to review the virologic characteristics, transmission, clinical manifestation, diagnosis, treatment, and prevention, we can induce that the the paper will talk about risk factors of this disease.\n\n3. ***Article:*** Paper id: t02jngq0, Prediction Score: 0.98595\n    - ***Title:*** A database of geopositioned Middle East Respiratory Syndrome Coronavirus occurrences\n    - ***Abstract:*** As a World Health Organization Research and Development Blueprint priority pathogen, there is a need to better understand the ***geographic distribution of Middle East Respiratory Syndrome Coronavirus (MERS-CoV) and its potential to infect mammals and humans.*** This database documents cases of MERS-CoV globally, with specific attention paid to zoonotic transmission. An initial literature search was conducted in PubMed, Web of Science, and Scopus; after screening articles according to the inclusion\/exclusion criteria, a total of 208 sources were selected for extraction and geo-positioning. Each MERS-CoV occurrence was assigned one of the following classifications based upon published contextual information: index, unspecified, secondary, mammal, environmental, or imported. In total, this database is comprised of 861 unique geo-positioned MERS-CoV occurrences. The purpose of this article is to share a collated MERS-CoV database and extraction protocol that can be utilized in future mapping efforts for both MERS-CoV and other infectious diseases. More broadly, it may also provide useful data for the development of targeted MERS-CoV surveillance, which would prove invaluable in preventing future ***zoonotic spillover***.\n    - ***Analysis:*** This paper aims to analyze specific risk factors of Middle East Respiratory Syndrome Coronavirus((MERS-CoV)). Geographic distribution and potential to infect mammals and humans are two main aspects that the authors concerned about. Through the study of the database, the readers can know how these two factors impact the spread of (MERS-CoV).\n\n4. ***Article:*** Paper id: 9t5ncsig, Prediction Score: 0.985887\n    - ***Title:*** Optimising Renewal Models for Real-Time Epidemic Prediction and Estimation\n    - ***Abstract:*** The effective reproduction number, ***Rt, is an important prognostic for infectious disease epidemics. Significant changes in Rt can forewarn about new transmissions or predict the efficacy of interventions.*** The renewal model infers Rt from incidence data and has been applied to Ebola virus disease and pandemic influenza outbreaks, among others. This model estimates Rt using a sliding window of length k. While this facilitates real-time detection of statistically significant Rt fluctuations, inference is highly k -sensitive. Models with too large or small k might ignore meaningful changes or over-interpret noise-induced ones. No principled k -selection scheme exists. We develop a practical yet rigorous scheme using the accumulated prediction error (APE) metric from information theory. We derive exact incidence prediction distributions and integrate these within an APE framework to identify the k best supported by available data. We find that this k optimises short-term prediction accuracy and expose how common, heuristic k -choices, which seem sensible, could be misleading.\n    - ***Analysis:*** This article summarizes a study about the renewal model that infers effective reproduction number, Rt, of Ebola virus disease and pandemic influenza outbreak. The effective reproduction number is a basic characteristic of pandemic disease, and is also an important risk factor to the spread of a pandemic. From this study, the readers can know how Rt impact the spread of Ebola virus disease and pandemic influenza. Therefore it may shed light to the control and prevention of such disease.\n\n5. ***Article:*** Paper id: 5zcydnre, Prediction Score: 0.984713\n    - ***Title:*** The time-series ***ages distribution*** of the reported COVID-2019 infected people suggests the undetected ***local spreading*** of COVID-2019 in Hubei and Guangdong provinces before 19th Jan 2020\n    - ***Abstract:*** COVID-2019 is broken out in China. It becomes a severe public health disaster in one month. Find the period in which the spreading of COVID-2019 was overlooked, and understand the epidemiological characteristics of COVID-2019 in the period will provide valuable information for the countries facing the threats of COVID-2019. The most extensive epidemiological analysis of COVID-2019 ***shows that older people have lower infection rates compared to middle-aged persons***. Common sense is that older people prefer to report their illness and get treatment from the hospital compared to middle-aged persons. ***We propose a hypothesis that when the spreading of COVID-2019 was overlooked, we will find more older cases than the middle-aged cases.*** At first, we tested the hypothesis with 4597 COVID-2019 infected samples reported from 26th Nov 2019 to 17th Feb 2020 across the mainland of China. We found that 19th Jan 2020 is a critical time point. Few samples were reported before that day, and most of them were older ones. Then samples were explosively increased after that day, and many of them were middle-aged people. We have demonstrated the hypothesis to this step. Then, we grouped samples by their residences(provinces). We found that, in the provinces of Hubei and Guangdong, the ages of samples reported before 19th Jan 2020 are significantly higher than the ages of samples reported after that day. It suggests the COVID-2019 may be spreading in Hubei and Guangdong provinces before 19th Jan 2020 while people were unconscious of it. At last, we proposed that ***the ages distribution*** of each-day-reported samples could serve as a ***warning indicator*** of whether all potential COVID-2019 infected people are found. We think the power of our analysis is limit because 1. the work is data-driven, and 2. only ~5% of the COVID-2019 infected people in China are included in the study. However, we believe it still shows some value for its ability to estimate the possible unfound COVID-2019 infected people.\n    - ***Analysis:*** This article summarizes a study on COVID-19 infected people in Hubei and Guangdong provinces of China. We can see that the main focus of this study is the distribution of age group of patients. Age is a key risk factor related to COVID-19. Specifically, the article points out that age distribution can serve as a warning indicator of whether all potentially infected people are all found. This will shed light to treating patients better and slowing down the spread of disease.\n\nIn conclusion, we see from the above examples that these articles are indeed relevant to risk factors to the outbreak, spread, infection, or mortality of a disease in some way, and to some degree, although the direct keyword \"risk\" and \"factor\" have never occurred in these articles.  \n","348ff0c8":"### Mouseover a node will pop up its info and highlight connecting edges","4f73509e":"## Model training","c108b000":"## Plot network\nThe network was displayed with two slider bar, number of top words and number of top words co-occurred with selected key words, to control the number of nodes in Type I and Type II, respectively. ","9956dc48":"## Building the PMID lists for positive and negative samples\n\nFirstly, we use load_pmids_list function to get the PMIDs of positive papers (the papers which contains KOI in it). \n\nThen, we get all the PMIDs from '2009\/01\/01' to '2018\/12\/31' and remove the PMIDs of positive papers from it to build the negative samples, which are papers not relevant to KOI. A small number of them are actually relevant, but such noise should be fine.","cdf9b42a":"# Functions for text processing\nAfter all data was retrieved from the COVID-19 dataset and PubMed, we can proceed to the second module, information extraction. Conducting research on text data requires extensive processing of the data. We developed several functions as follows for necessary text processing such as denoising, normalization and tokenization.","e34617fe":"#### Tokenization classes\n\nUse the code from BERT website\n###### Cite: [Tokenization Code ](https:\/\/github.com\/google-research\/bert\/blob\/master\/tokenization.py)","26955b00":"Acquiring pubmed papers for background edges of a task","0d7d65be":"## Prediction","f371042f":"\n### Model building\nadding layer for classification","071e9d3b":"# Preprocessing COVID-19 Dataset\nThe COVID-19 open research dataset consists of papers from different sources including CZI, PMC, BioRxiv\/MedRxiv. The dataset includes a metadata file and many files in json format. We start with combining the data in different formats into one dataframe. Then different keywords are used to query papers of different topics for downstream analysis.\n\n## Combining the metadata and JSON Data\nFirst, we combine the data in the metadata file and those in json format into one dataframe. We acquire all text including title, abstract, full text, bibliography entries, figures and table annotations, and others when available and organize them into the dataframe. This dataframe provides easier access to the dataset for downstream analysis, such as document retrieval and information extraction.","94238721":"## Querying CORD19 dataset by keywords\n\nWe first implement a function to search papers in the COVID-19 dataset relevant to KOI. The function can be used to search papers containing different keywords. Using the function we are able to search papers containing a specific keyword or a list of keywords.","1e588f2d":"\n## How to cite the work?\n```\n@inproceedings{The knowledge network of COVID-19 through text mining of COVID-19 literature,\n\tauthor = {Shubo Tian, Jian Wang, Yuhang Liu, Wanjing Wang, Chun-Chao Lo, Xiaodong Pang, Yuchuan Tao, Jinfeng Zhang},\n\ttitle = {The knowledge network of COVID-19 through text mining of COVID-19 literature},\n\taddress = {Florida State University, Insilicom LLC, Tallahassee, FL, USA},\n    year = {2020},\n\turl = {\\url{https:\/\/covid19.insilicom.com\/}},\n}\n```","579d362c":"# The BERT model\nBERT (Bidirectional Encoder Representations from Transformers) model (https:\/\/github.com\/google-research\/bert) is a method of pre-training language representations. \n\nThe whole proecess consists of the following steps:\n\n```\n1) Data pre-processing\n2) BERT model preparation\n3) Model building by adding layer for classification\n4) Model training\n5) Prediction\n```","eba89873":"# Configurations\n\nNote that the switch for running BERT model is set to False because the BERT cannot be trained on Kaggle server. We have tested on our local machine. If you run the code on your local machine with a GPU, you should change it to True.","574a3f0f":"Develop co-occuering vocabulary for a task","6012a7c6":"## Table of Contents\n\n1. [Introduction](#Introduction)\n\n2. [The knowledge network](#The-knowledge-network)\n\n3. [Import packages](#Import-packages)\n\n4. [Configurations](#Configurations)\n\n5. [Querying CORD19 dataset by keywords](#Querying-CORD19-dataset-by-keywords)\n\n6. [Acquiring PubMed data](#Acquiring-some-PubMed-abstracts-for-generating-training-data-for-document-retrievel-and-background-edges-for-information-extraction)\n\n7. [Acquiring PubMed papers](#Acquiring-PubMed-papers)\n\n8. [Functions for text processing](#Functions-for-text-processing)\n\n9. [Document retrieval](#Document-Retrieval)\n\n10. [The BERT model](#The-BERT-model)\n\n11. [Conclusion of document retrieval](#Conclusion-of-document-retrieval)\n\n12. [Information extraction](#Information-extraction)","c1e4cb5e":"# Document Retrieval\n\n## Data Preparation\n\nOur goal is to find the papers which are relevant to KOI, but do not contain the KOI. Therefore, we will delete the words in KOI from the papers to form the positive samples of our training data. We will randomly choose papers using PMIDs, which do not appear in the positive paper list,\u00a0as negative samples of the training data.","c614017a":"## Data Pre-processing\n\nIncluding the following steps:\n```\n- Convert input text to BERT model format\n- Map Greek alphabets to English and abbreviations \n- Clean up text\n```","a2a1e6ab":"## Building prediction dataset\nDo the same thing to test sample as above. \n<br>\nWe used CORD-19 data that do not contain KOI as the testset. \n<br>\nWe will use the BERT model to find the papers that are relevant to KOI in the testset.","c38ee315":"### Clicking an edge will pop up all the sentences relevent to the edge","43d55e97":"### <font color='red'>A full version of the interactive network can be found on our website:<\/font> \n\n### [https:\/\/covid19.insilicom.com\/](https:\/\/covid19.insilicom.com\/)\n\n\n","e0657439":"Note: We put a snapshot of the network below for your reference because the network can't be display on Kaggle somehow. Again, please go to [our website](https:\/\/covid19.insilicom.com\/) for the interactive network if the network does not show up.","c5120332":"# Display Knowledge Network\nWe use Risk Factor as selected key for example, we first extract two types (I and II) of edges from the literature as described above. Type I is the edges of all possible pairwise words in the same sentence; Type II is the edges of all possible words co-occurred with the selected key word(s) in the same sentence. \n\nWe processed the edges in following steps: \n```\n1. Rank edges: \n    The two types of edges are ranked by counts from high to low. For the type II edges, we rank them by the count of how many papers containing the edges, instead of how many sentences. \n2. Select nodes: \n    For type I edges, the top 100 nodes ranked by degree (number of edges connecting to the node) from high to low were selected. For type II edges, a intersect of the top 100 type I nodes and all type II nodes were selected. \n```","f839a733":"# Introduction\n\nIn the Kaggle Challenge, we aimed at building an automatic pipeline to extract knowledge from COVID-19 literature related to a topic of interest. The knowledge we will extract is represented as relationships between terms related to the topic of interest. The extracted knowledge can be visualized as an interactive network so that researchers can quickly explore and grasp existing knowledge by navigating in the network. Advanced knowledge discovery tools can be built on top of this framework, which will be explored in the next phase of the project.\n\nThe input to the pipeline is a set of keywords, called keywords of interest (KOI); the output of the pipeline is a set of relationships among terms related to the KOI, directly or indirectly, where each relationship is associated with a sentence from which the relationship was extracted from. The interactive network allows users to quickly see the related terms, their relationships, the corresponding sentences from which relationships were extracted, and the articles containing the sentences. Without reading a large number of articles, which can be very time-consuming, a user can quickly grasp the key knowledge related to the KOI using this tool.\n\nOur pipeline consists of two modules: document retrieval and information extraction. In the first module, we first use KOI to retrieve a set of relevant papers containing the KOI. In addition to this standard approach, we have developed a deep learning based method to retrieve additional papers that do not contain KOI, but are still relevant to KOI.\n\nTo build a model that is able to retrieve articles that are relevant to KOI, but do not contain KOI, we need to generate quality training data. The true cases were obtained by querying PubMed using KOI and then removing KOI from the returned articles. The rationale is that even by removing KOI from an article, by reading the rest of the words from the article, a knowledgeable researcher can still judge that the article is relevant to the KOI. We have manually tested this assumption and found that it was quite reasonable. The negative cases were obtained by sampling PubMed articles randomly and making sure that returned articles do not contain KOI. We trained the deep learning model using standard word embedding and BERT model. The model trained using this data was designed to learn the pattern in the articles that are relevant to KOI, but do not contain KOI. When we apply the model to articles that do not contain KOI, it will rank them according to how they are related to KOI. The model achieved satisfactory accuracy in the training dataset. We have manually read some of the top articles retrieved from CORD-19, and found many of them are indeed relevant to the KOI.\n\nTo extract relationships relevant to KOI, we first extracted all the terms in relevant articles and consider two terms as related if they co-occur in the same sentence. Other more sophisticated approaches can be used (i.e. some of our previous relationship extraction studies). These relationships contain some trivial ones, which we would like to remove. To that end, we queried a large number of PubMed articles from PubMed using its API using the same KOI, but not related to COVID-19. We extracted relationships using the same co-occurrence approach and subtracted frequently occurring background relationships (they are likely the trivial ones we want to remove) from those obtained from COVID-19 articles retrieved in the first module. We further extracted terms that co-occur with KOI in the same sentences and allow users to select these terms in the network. These terms and the relationships involving them are likely more relevant to KOI. \n\nBelow we will first show the images of the interactive networks, which are followed by our pipeline and some intermediate results.","b31181c2":"We used 'risk factor' as a keyword to search papers related to the topic on risk factors about COVID-19 in the dataset. All papers containing the words of risk and factor in title and abstract were acquired to deveplop a subset data for further research on the risk factor topic. Similarly, we used 'vaccine' and 'therapeutic' as keywords to search papers for further research of topics on vaccines and therapeutics respectively.","c71948bb":"Develop edges of a task","d1a54f10":"## BERT model preparation\nIncluding the following steps:\n```\n- Tokenization classes\n- Load BERT model\n```"}}