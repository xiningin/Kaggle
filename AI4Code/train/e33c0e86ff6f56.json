{"cell_type":{"66342250":"code","55c14b60":"code","e4d1e780":"code","d342a78c":"code","4fa500f6":"code","3d442240":"code","4a99c958":"code","25b0ba60":"code","a796664a":"code","cc08d054":"code","e6a37ad7":"code","ddbbf9b2":"code","975dd888":"code","4ed613f8":"code","0701636b":"code","bad38178":"code","e4f7193e":"code","c5c20fa4":"code","58241aa3":"code","04f3877d":"code","2406cec5":"code","ce3d2e88":"code","b14ab3dd":"code","d58e06a8":"code","f78a2ba1":"code","3aa0285b":"code","e1b6e014":"code","527b2fa8":"code","f2da954e":"code","e164a976":"code","5a46cf52":"code","7afd7418":"code","0442e0f4":"code","1505ebda":"code","37e7eeab":"code","01641469":"code","bd1a99ce":"code","817310e4":"code","340521b2":"code","6b46db51":"code","08a5367e":"code","8560fcb5":"code","684e561a":"code","7b74927b":"code","159e8b29":"code","df71671f":"code","4862f070":"code","e3371f4d":"code","8ede2a78":"code","cb614377":"code","61a4c382":"code","4ee64fdb":"code","1ee44381":"code","db45eeba":"code","1d5cf220":"code","8c59930e":"code","81f66b43":"code","76390acc":"code","130aedf8":"code","0bb90bdc":"code","96a995cc":"code","211bd925":"code","72d96e52":"code","7068664a":"code","8aa1d010":"code","25fc5dd6":"code","28ecddec":"code","be7bf993":"code","0c3195ff":"code","9c767711":"code","1a0c909a":"code","92652f5c":"code","5b4d4217":"code","4ebd7d0d":"code","04011052":"code","2627f321":"code","50ec6ea1":"code","d25fabc9":"code","d33b4662":"code","79d43bff":"code","a2e5df8d":"code","f5c4bfe0":"code","40ab408e":"code","c2842e30":"code","86fcc2d0":"code","0e8e2f2c":"code","07cb9aab":"code","66ef7889":"code","9a8f6ed4":"code","d403f421":"code","23760fc5":"code","a2533918":"code","974232a5":"code","f25603f9":"code","28ee16dd":"code","df85ae66":"code","6247ae73":"code","12d791c3":"code","6e37ba73":"markdown","70f16270":"markdown","b8edef3a":"markdown","8b97efca":"markdown","6bdc9827":"markdown","d9c21f06":"markdown","a70448b7":"markdown","35e1b9c8":"markdown","c65b4e23":"markdown","aaebdd03":"markdown","7d391d27":"markdown","7ef06edf":"markdown","84b19f81":"markdown","7fd0d02c":"markdown","5660474f":"markdown","00766002":"markdown","0a7b0a48":"markdown","aacae7b5":"markdown","ec8d4ee0":"markdown","bce600ff":"markdown","30052e6e":"markdown","b19a8615":"markdown","e7e9f468":"markdown","093ff337":"markdown","516c960f":"markdown","4f4a5ecd":"markdown","a38d822a":"markdown","dea07634":"markdown","880ac2a6":"markdown","7a0dc7af":"markdown","ed1416be":"markdown","d76ae817":"markdown","1cadb6d2":"markdown"},"source":{"66342250":"import pandas as pd\nfrom pandas import read_csv\nfrom scipy import linalg\nfrom sklearn import metrics\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.preprocessing import scale\nimport numpy as np\nimport matplotlib as mpl\nimport scipy\nfrom sklearn.metrics import classification_report\nfrom sklearn import preprocessing\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\n\n# Ignore  the warnings\nimport warnings\nwarnings.filterwarnings('always')\nwarnings.filterwarnings('ignore')\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split, RandomizedSearchCV, StratifiedKFold, KFold, GridSearchCV\nfrom sklearn.metrics import auc\nfrom sklearn.metrics import roc_curve\n\nfrom sklearn import metrics, mixture, cluster, datasets\nfrom sklearn.mixture import GaussianMixture\nfrom sklearn.calibration import CalibratedClassifierCV\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import adjusted_rand_score","55c14b60":"train = pd.read_csv(\"..\/input\/trainfinal\/train_final.csv\")","e4d1e780":"train.loc[train['Gender'] == 'Male', 'Gender'] = 1\ntrain.loc[train['Gender'] == 'Female', 'Gender'] = 0\n\ntrain.loc[train['Vehicle_Age'] == '> 2 Years', 'Vehicle_Age'] = 2\ntrain.loc[train['Vehicle_Age'] == '1-2 Year', 'Vehicle_Age'] = 1\ntrain.loc[train['Vehicle_Age'] == '< 1 Year', 'Vehicle_Age'] = 0\n\ntrain.loc[train['Vehicle_Damage'] == 'Yes', 'Vehicle_Damage'] = 1\ntrain.loc[train['Vehicle_Damage'] == 'No', 'Vehicle_Damage'] = 0","d342a78c":"train = train.drop(\"id\",axis=1)\ntrain.head()","4fa500f6":"corr=train.corr()\nplt.figure(figsize=(30,15))\nsns.heatmap(corr,annot=True, cmap=\"coolwarm\")\nplt.show()\n# From the heatmap, we could see the relationships between 'Response' and other factors.\n# Firstly, the relation between 'Vehicle_Damage' and it is the strongest, -0.35 and 0.35.\n# Meanwhile, the relation between 'Previously insured' and it is also remarkbale, while it is negative, at -0.34\n\n# Except for response, negative relations between age and previously insured means that young people tend to hold insurance.","3d442240":"df_business = train[['Region_Code','Annual_Premium','Policy_Sales_Channel','Vintage']]\nX = df_business.values\n\nGM_n_components = np.arange(1, 8)\nGM_models = [mixture.GaussianMixture(n, covariance_type='full', random_state=0).fit(X) for n in GM_n_components]\n\nplt.figure(num=None, figsize=(8, 6), dpi=60, facecolor='w', edgecolor='r')\nplt.plot(GM_n_components, [m.aic(X) for m in GM_models], label='AIC')\nplt.tight_layout()\nplt.legend(loc='best')\nplt.xlabel('n_components');","4a99c958":"# Finally, k equals to 2\n\ny = train['Response']\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import adjusted_rand_score\n\nX_train,X_test,y_train,y_test = train_test_split(df_business,y,test_size=0.25,random_state=0,stratify=y)\n\n\nGM_n_classes = 2\n\nGMcluster = mixture.GaussianMixture(n_components=GM_n_classes, covariance_type='full',random_state = 0)\nGMcluster_fit = GMcluster.fit(df_business)\nGMlabels = GMcluster_fit.predict(df_business)\ny_pred = GMcluster_fit.predict(X_test)\n\nprint(adjusted_rand_score(y_test, y_pred))","25b0ba60":"attributes = df_business\ncluster_labels = GMlabels\n\nfrom sklearn.metrics import silhouette_score\nsilhouette_score(attributes, cluster_labels)","a796664a":"KMlabels = KM.predict(df_business)\nattributes = df_business\ncluster_labels = KMlabels\n\nfrom sklearn.metrics import silhouette_score\nsilhouette_score(attributes, cluster_labels)","cc08d054":"from sklearn.cluster import KMeans\n# Create a list of SSE\nsse = []\n# We decide to try k from 1 to 14, to see which value is the best number of clusters.\nfor k in range(1,15):\n    # Using kmeans method to fit the data and compute clustering.\n    kmeans = KMeans(n_clusters = k)\n    kmeans.fit(train)\n    sse.append(kmeans.inertia_)\nprint(sse)","e6a37ad7":"plt.plot(range(1,15), sse,marker = \"o\")\nplt.title('Elbow method')\nplt.xlabel('No of clusters')\nplt.ylabel('SSE')\nplt.show()","ddbbf9b2":"X = df_business\ny = train['Response']\nfrom sklearn.model_selection import train_test_split\n\nX_train,X_test,y_train,y_test = train_test_split(df_business,y,test_size=0.25,random_state=0,stratify=y)\nX_train.shape,X_test.shape,y_train.shape,y_test.shape","975dd888":"from sklearn.cluster import KMeans\nfrom sklearn.metrics import adjusted_rand_score\nkm = KMeans(n_clusters=2,init='k-means++',n_init=10,max_iter=300,tol=1e-04,random_state=0)\n\nKM = km.fit(df_business)\n\ny_pred_business = km.predict(X_test)","4ed613f8":"VariablesClient = [x for x in train if x not in df_business.columns]\ndf_client = train[VariablesClient]\nX1 = df_client.values\nGMcluster_fit = GMcluster.fit(df_client)\nGMlabels = GMcluster_fit.predict(df_client)\n\ny = train['Response']\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import adjusted_rand_score\n\nX1_train,X1_test,y_train,y_test = train_test_split(df_client,y,test_size=0.25,random_state=0,stratify=y)\n\n\nGM_n_classes = 2\n\n\ny_pred = GMcluster_fit.predict(X1_test)\n\nX1_train,X1_test,y_train,y_test = train_test_split(df_client,y,test_size=0.25,random_state=0,stratify=y)\nX1_train.shape,X1_test.shape,y_train.shape,y_test.shape","0701636b":"km.fit(df_client)\n\ny_pred_client = km.predict(X1_test)\n\nKMlabels_client= km.predict(X1)","bad38178":"attributes = df_client\ncluster_labels = GMlabels\n\nfrom sklearn.metrics import silhouette_score\nsilhouette_score(attributes, cluster_labels)","e4f7193e":"KMlabels = KM.predict(df_client)\nattributes = df_client\ncluster_labels = KMlabels\n\nfrom sklearn.metrics import silhouette_score\nsilhouette_score(attributes, cluster_labels)","c5c20fa4":"train[\"Region_Code\"].head(10)","58241aa3":"# get dummy variables for \"Region_Code\"\ndummy_Region_Code = pd.get_dummies(train[\"Region_Code\"],prefix = \"Region_Code\")\ntrain = pd.concat([train.drop('Region_Code',axis=1),dummy_Region_Code],axis=1)","04f3877d":"train[\"Policy_Sales_Channel\"].head(10)","2406cec5":"# get dummy variables for \"Policy_Sales_Channel\"\ndummy_Policy_Sales_Channel = pd.get_dummies(train[\"Policy_Sales_Channel\"],prefix = \"Policy_Sales_Channel\")\ntrain = pd.concat([train.drop('Policy_Sales_Channel',axis=1),dummy_Policy_Sales_Channel],axis=1)","ce3d2e88":"train","b14ab3dd":"sns.countplot(x = train.Response)","d58e06a8":"sns.distplot(train.Age)","f78a2ba1":"sns.boxplot(y = 'Age', data = train,palette='Accent')","3aa0285b":"sns.scatterplot(x=train['Age'],y=train['Annual_Premium'])","e1b6e014":"sns.countplot(train.Gender)","527b2fa8":"df=train.groupby(['Gender'])['Driving_License'].count().to_frame().reset_index()","f2da954e":"sns.catplot(x=\"Gender\", y=\"Driving_License\",data=df, kind=\"bar\")","e164a976":"X = train.drop('Response',axis=1)\ny = train['Response']\n\nfrom sklearn.model_selection import train_test_split as split\nX_train, X_test, y_train, y_test = split(X,y, test_size=0.25, stratify=y, random_state=0)\nX_train.shape, X_test.shape, y_train.shape, y_test.shape","5a46cf52":"from sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.fit_transform(X_test)","7afd7418":"X_train_scaled.shape, X_test_scaled.shape","0442e0f4":"y_train=y_train.astype(\"float\")\ny_test=y_test.astype(\"float\")","1505ebda":"from sklearn.decomposition import PCA\nfrom sklearn.metrics import mean_squared_error\nimport mglearn","37e7eeab":"# Use the numpy.cov function, we computed the covariance matrix of the standardized training dataset.\ncov_mat = np.cov(X_train_scaled.T)\n# Use the linalg.eig function, we can get a vector (eigen_vals) consisting of the eigenvalues and the corresponding eigenvectors.\neigen_vals, eigen_vecs = np.linalg.eig(cov_mat)\n\n#print('\\nEigenvalues \\n%s' % eigen_vals)","01641469":"# We use the NumPy cumsum function to calculate the cumulative sum of explained variances.\n\ntot = sum(eigen_vals)\nvar_exp = [(i \/ tot) for i in sorted(eigen_vals, reverse=True)]\ncum_var_exp = np.cumsum(var_exp)","bd1a99ce":"cum_var_exp[:40]\n# According to the above plot, we chose 36 as the component of PCA that reached above 90% explained variance ratio.","817310e4":"# Then we can draw the the plot of the cumulative sum of explained variances.\n\nimport matplotlib.pyplot as plt\n\nplt.bar(range(1, 217), var_exp, alpha=0.5, align='center', label='individual explained variance')\nplt.step(range(1, 217), cum_var_exp, where='mid', label='cumulative explained variance')\nplt.ylabel('Explained variance ratio')\nplt.xlabel('Principal component index')\nplt.legend(loc='best')\nplt.tight_layout()\nplt.show()","340521b2":"pca = PCA(n_components=36) # According to the elbow plot, we can choose n=50.\npcaX_train_scaled = pca.fit_transform(X_train_scaled)\npcaX_test_scaled = pca.fit_transform(X_test_scaled)","6b46db51":"components = [30,36,40]    \nfor n in components:\n    pca = PCA(n_components=n)\n    recon = pca.inverse_transform(pca.fit_transform(X_train_scaled))\n    rmse = mean_squared_error(X_train_scaled[0], recon[0],squared=False)\n    print(\"RMSE: {} with {} components\".format(rmse, n))","08a5367e":"from sklearn.naive_bayes import GaussianNB\nmodel1 = GaussianNB()\nmodel1.fit(pcaX_train_scaled, y_train)\ny_pred1 = model1.predict(pcaX_test_scaled)\n\nprint(\"Classification Report:\\n \", classification_report(y_test, y_pred1))\nprint(\"\\nConfusion Matrix:\\n\",metrics.confusion_matrix(y_test, y_pred1))","8560fcb5":"y_pred11 = model1.predict_proba(pcaX_test_scaled)\n(fpr, tpr, thresholds) = roc_curve(y_test,y_pred11[:,1])\nroc_auc = auc(fpr,tpr)\n\nplt.figure()\nlw = 2\nplt.plot(fpr, tpr,lw=lw, label='ROC curve (area = %0.2f)' % roc_auc) \nplt.plot([0, 1], [0, 1], lw=lw, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('FPR')\nplt.ylabel('TPR')\nplt.title('ROC')\nplt.legend(loc=\"lower right\")\nplt.show()","684e561a":"from sklearn.naive_bayes import BernoulliNB\nmodel17 = BernoulliNB()\nmodel17.fit(pcaX_train_scaled, y_train)\ny_pred17 = model17.predict(pcaX_test_scaled)\n\nprint(\"Classification Report:\\n \", classification_report(y_test, y_pred17))\nprint(\"\\nConfusion Matrix:\\n\",metrics.confusion_matrix(y_test, y_pred17))","7b74927b":"y_pred171 = model17.predict_proba(pcaX_test_scaled)\n(fpr, tpr, thresholds) = roc_curve(y_test,y_pred171[:,1])\nroc_auc = auc(fpr,tpr)\n\nplt.figure()\nlw = 2\nplt.plot(fpr, tpr,lw=lw, label='ROC curve (area = %0.2f)' % roc_auc) \nplt.plot([0, 1], [0, 1], lw=lw, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('FPR')\nplt.ylabel('TPR')\nplt.title('ROC')\nplt.legend(loc=\"lower right\")\nplt.show()","159e8b29":"from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nlda = LinearDiscriminantAnalysis(solver=\"svd\", store_covariance=True)\nmodel2 = lda.fit(pcaX_train_scaled, y_train)\ny_pred2 = model2.predict(pcaX_test_scaled)\nprint(\"Classification Report:\\n \", classification_report(y_test, y_pred2))\nprint(\"\\nConfusion Matrix:\\n\",metrics.confusion_matrix(y_test, y_pred2))","df71671f":"y_pred21 = model2.predict_proba(pcaX_test_scaled)\n(fpr, tpr, thresholds) = roc_curve(y_test,y_pred21[:,1])\nroc_auc = auc(fpr,tpr)\n\nplt.figure()\nlw = 2\nplt.plot(fpr, tpr,lw=lw, label='ROC curve (area = %0.2f)' % roc_auc) \nplt.plot([0, 1], [0, 1], lw=lw, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('FPR')\nplt.ylabel('TPR')\nplt.title('ROC')\nplt.legend(loc=\"lower right\")\nplt.show()","4862f070":"random_search = {'C':[0.001, 1, 100],\n          'class_weight':['balanced', None],\n          'solver':['liblinear','sag','lbfgs','newton-cg']}\ngrid_search = GridSearchCV(estimator=LogisticRegression(), param_grid=random_search, cv=5, n_jobs=-1, scoring='neg_mean_squared_error')\ngrid_result=grid_search.fit(X_train_scaled, y_train)\nprint(f'Best:{grid_result.best_score_}using{grid_result.best_params_}','\\n')","e3371f4d":"from sklearn.linear_model import LogisticRegression\nmodel3 = LogisticRegression(C=0.001,solver=\"sag\")\nmodel3.fit(pcaX_train_scaled, y_train)\ny_pred3 = model3.predict(pcaX_test_scaled)\n\nprint(\"Classification Report:\\n \", classification_report(y_test, y_pred3))\nprint(\"\\nConfusion Matrix:\\n\",metrics.confusion_matrix(y_test, y_pred3))","8ede2a78":"y_pred31 = model3.predict_proba(pcaX_test_scaled)\n(fpr, tpr, thresholds) = roc_curve(y_test,y_pred31[:,1])\nroc_auc = auc(fpr,tpr)\n\nplt.figure()\nlw = 2\nplt.plot(fpr, tpr,lw=lw, label='ROC curve (area = %0.2f)' % roc_auc) \nplt.plot([0, 1], [0, 1], lw=lw, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('FPR')\nplt.ylabel('TPR')\nplt.title('ROC')\nplt.legend(loc=\"lower right\")\nplt.show()","cb614377":"%%time\nfrom sklearn.svm import SVC\nrandom_search = {'C': [0.1, 1, 10], 'gamma': [0.01, 0.1, 1]}\ngrid_search = GridSearchCV(estimator=SVC(), param_grid=random_search, cv=5, n_jobs=-1, scoring='neg_mean_squared_error')\ngrid_result=grid_search.fit(X_train_scaled, y_train)\nprint(f'Best:{grid_result.best_score_}using{grid_result.best_params_}','\\n')","61a4c382":"%%time\nfrom sklearn.svm import SVC\nmodel4 =SVC()\nmodel4.fit(X_train_scaled, y_train)\ny_pred4 = model4.predict(X_test_scaled)\n\nprint(\"Classification Report:\\n \", classification_report(y_test, y_pred4))","4ee64fdb":"# This model tooks us more than 3 days to run, but we still cannot get the result.","1ee44381":"from sklearn.svm import LinearSVC\nmodel19 = LinearSVC()\nmodel19.fit(pcaX_train_scaled, y_train)\ny_pred19 = model19.predict(pcaX_test_scaled)\n\nprint(\"Classification Report:\\n \", classification_report(y_test, y_pred19))","db45eeba":"from sklearn.linear_model import Perceptron\nmodel5=Perceptron()\nmodel5.fit(pcaX_train_scaled, y_train)\ny_pred5 = model5.predict(pcaX_test_scaled)\n\nprint(\"Classification Report:\\n \", classification_report(y_test, y_pred5))\nprint(\"\\nConfusion Matrix:\\n\",metrics.confusion_matrix(y_test, y_pred5))","1d5cf220":"from sklearn.neural_network import MLPClassifier\nmodel6 = MLPClassifier(hidden_layer_sizes=(400,100),alpha=0.01,max_iter=1000) \nmodel6.fit(pcaX_train_scaled, y_train) \ny_pred6 = model6.predict(pcaX_test_scaled)\n\nprint(\"Classification Report:\\n \", classification_report(y_test, y_pred6))","8c59930e":"y_pred61 = model6.predict_proba(pcaX_test_scaled)\n(fpr, tpr, thresholds) = roc_curve(y_test,y_pred61[:,1])\nroc_auc = auc(fpr,tpr)\n\nplt.figure()\nlw = 2\nplt.plot(fpr, tpr,lw=lw, label='ROC curve (area = %0.2f)' % roc_auc) \nplt.plot([0, 1], [0, 1], lw=lw, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('FPR')\nplt.ylabel('TPR')\nplt.title('ROC')\nplt.legend(loc=\"lower right\")\nplt.show()","81f66b43":"random_search = {'criterion': ['entropy', 'gini'],\n               'max_depth': [2,3,4,5,6,7,10],\n               'min_samples_leaf': [4, 6, 8],\n               'min_samples_split': [5, 7,10],\n               'n_estimators': [300]}\ngrid_search = GridSearchCV(estimator=RandomForestClassifier(), param_grid=random_search, cv=5, n_jobs=-1, scoring='neg_mean_squared_error')\ngrid_result=grid_search.fit(X_train_scaled, y_train)\nprint(f'Best:{grid_result.best_score_}using{grid_result.best_params_}','\\n')","76390acc":"from sklearn.ensemble import RandomForestClassifier\nmodel7 = RandomForestClassifier(criterion='entropy',n_estimators=300,max_depth=2,min_samples_leaf=4,min_samples_split=5)\nmodel7.fit(X_train_scaled, y_train) \ny_pred7 = model7.predict(X_test_scaled)\n\nprint(\"Classification Report:\\n \", classification_report(y_test, y_pred7))","130aedf8":"y_pred71 = model7.predict_proba(X_test_scaled)\n(fpr, tpr, thresholds) = roc_curve(y_test,y_pred71[:,1])\nroc_auc = auc(fpr,tpr)\n\nplt.figure()\nlw = 2\nplt.plot(fpr, tpr,lw=lw, label='ROC curve (area = %0.2f)' % roc_auc) \nplt.plot([0, 1], [0, 1], lw=lw, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('FPR')\nplt.ylabel('TPR')\nplt.title('ROC')\nplt.legend(loc=\"lower right\")\nplt.show()","0bb90bdc":"from sklearn.tree import DecisionTreeClassifier\nmodel18 = DecisionTreeClassifier(random_state=0)\nmodel18.fit(X_train_scaled, y_train) \ny_pred18 = model18.predict(X_test_scaled)\n\nprint(\"Classification Report:\\n \", classification_report(y_test, y_pred18))","96a995cc":"y_pred181 = model18.predict_proba(X_test_scaled)\n(fpr, tpr, thresholds) = roc_curve(y_test,y_pred181[:,1])\nroc_auc = auc(fpr,tpr)\n\nplt.figure()\nlw = 2\nplt.plot(fpr, tpr,lw=lw, label='ROC curve (area = %0.2f)' % roc_auc) \nplt.plot([0, 1], [0, 1], lw=lw, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('FPR')\nplt.ylabel('TPR')\nplt.title('ROC')\nplt.legend(loc=\"lower right\")\nplt.show()","211bd925":"from sklearn.ensemble import GradientBoostingClassifier\nmodel14= GradientBoostingClassifier(random_state=1)             \nmodel14.fit(X_train_scaled, y_train)           \ny_pred14 = model14.predict(X_test_scaled)\nprint(\"Classification Report:\\n \", classification_report(y_test, y_pred14))","72d96e52":"y_pred141 = model14.predict_proba(X_test_scaled)\n(fpr, tpr, thresholds) = roc_curve(y_test,y_pred141[:,1])\nroc_auc = auc(fpr,tpr)\n\nplt.figure()\nlw = 2\nplt.plot(fpr, tpr,lw=lw, label='ROC curve (area = %0.2f)' % roc_auc) \nplt.plot([0, 1], [0, 1], lw=lw, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('FPR')\nplt.ylabel('TPR')\nplt.title('ROC')\nplt.legend(loc=\"lower right\")\nplt.show()","7068664a":"from sklearn.ensemble import AdaBoostClassifier\nimport sklearn.tree as st\nimport sklearn.ensemble as se\nmodel10=AdaBoostClassifier(n_estimators=150, random_state=0)\nmodel10.fit(X_train_scaled, y_train) \ny_pred10 = model10.predict(X_test_scaled)\n\nprint(\"Classification Report:\\n \", classification_report(y_test, y_pred10))","8aa1d010":"y_pred101 = model10.predict_proba(X_test_scaled)\n(fpr, tpr, thresholds) = roc_curve(y_test,y_pred101[:,1])\nroc_auc = auc(fpr,tpr)\n\nplt.figure()\nlw = 2\nplt.plot(fpr, tpr,lw=lw, label='ROC curve (area = %0.2f)' % roc_auc) \nplt.plot([0, 1], [0, 1], lw=lw, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('FPR')\nplt.ylabel('TPR')\nplt.title('ROC')\nplt.legend(loc=\"lower right\")\nplt.show()","25fc5dd6":"%%time\nfrom catboost import CatBoostClassifier\nmodel16 = CatBoostClassifier()\nmodel16.fit(X_train_scaled, y_train)           \ny_pred16 = model16.predict(X_test_scaled)\nprint(\"Classification Report:\\n \", classification_report(y_test, y_pred16))","28ecddec":"y_pred161 = model16.predict_proba(X_test_scaled)\n(fpr, tpr, thresholds) = roc_curve(y_test,y_pred161[:,1])\nroc_auc = auc(fpr,tpr)\n\nplt.figure()\nlw = 2\nplt.plot(fpr, tpr, \n         lw=lw, label='ROC curve (area = %0.2f)' % roc_auc) \nplt.plot([0, 1], [0, 1], lw=lw, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('FPR')\nplt.ylabel('TPR')\nplt.title('ROC')\nplt.legend(loc=\"lower right\")\nplt.show()","be7bf993":"%%time\nfrom xgboost import XGBClassifier\nmodel13 = XGBClassifier(n_jobs=-1)              \nmodel13.fit(X_train_scaled, y_train)           \ny_pred13 = model13.predict(X_test_scaled)\nprint(\"Classification Report:\\n \", classification_report(y_test, y_pred13))","0c3195ff":"y_pred131 = model13.predict_proba(X_test_scaled)\n(fpr, tpr, thresholds) = roc_curve(y_test,y_pred131[:,1])\nroc_auc = auc(fpr,tpr)\n\nplt.figure()\nlw = 2\nplt.plot(fpr, tpr, \n         lw=lw, label='ROC curve (area = %0.2f)' % roc_auc) \nplt.plot([0, 1], [0, 1], lw=lw, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('FPR')\nplt.ylabel('TPR')\nplt.title('ROC')\nplt.legend(loc=\"lower right\")\nplt.show()","9c767711":"#pip install --user imbalanced-learn\n#pip install delay","1a0c909a":"from imblearn.over_sampling import SMOTE\n\nover_samples = SMOTE(random_state=0) \nX_train_smo, y_train_smo = over_samples.fit_resample(pcaX_train_scaled, y_train)\n\nprint(y_train.value_counts()\/len(y_train))\nprint(pd.Series(y_train_smo).value_counts()\/len(y_train_smo))","92652f5c":"from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nlda = LinearDiscriminantAnalysis(solver=\"svd\", store_covariance=True)\nmodel2 = lda.fit(X_train_smo, y_train_smo)\ny_pred_smo = model2.predict(pcaX_test_scaled)\nprint(\"Classification Report:\\n \", classification_report(y_test,y_pred_smo))\nprint(\"\\nConfusion Matrix:\\n\",metrics.confusion_matrix(y_test,y_pred_smo))","5b4d4217":"y_pred21 = model2.predict_proba(pcaX_test_scaled)\n(fpr, tpr, thresholds) = roc_curve(y_test,y_pred21[:,1])\nroc_auc = auc(fpr,tpr)\n\nplt.figure()\nlw = 2\nplt.plot(fpr, tpr,lw=lw, label='ROC curve (area = %0.2f)' % roc_auc) \nplt.plot([0, 1], [0, 1], lw=lw, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('FPR')\nplt.ylabel('TPR')\nplt.title('ROC')\nplt.legend(loc=\"lower right\")\nplt.show()","4ebd7d0d":"from sklearn.linear_model import LogisticRegression\nmodel3 = LogisticRegression(C=0.001,solver=\"sag\")\nmodel3.fit(X_train_smo, y_train_smo)\ny_pred3 = model3.predict(pcaX_test_scaled)\n\nprint(\"Classification Report:\\n \", classification_report(y_test, y_pred3))\nprint(\"\\nConfusion Matrix:\\n\",metrics.confusion_matrix(y_test, y_pred3))","04011052":"y_pred131 = model3.predict_proba(pcaX_test_scaled)\n(fpr, tpr, thresholds) = roc_curve(y_test,y_pred131[:,1])\nroc_auc = auc(fpr,tpr)\n\nplt.figure()\nlw = 2\nplt.plot(fpr, tpr, \n         lw=lw, label='ROC curve (area = %0.2f)' % roc_auc) \nplt.plot([0, 1], [0, 1], lw=lw, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('FPR')\nplt.ylabel('TPR')\nplt.title('ROC')\nplt.legend(loc=\"lower right\")\nplt.show()","2627f321":"from sklearn.naive_bayes import GaussianNB\nmodel1 = GaussianNB()\nmodel1.fit(X_train_smo, y_train_smo)\ny_pred1 = model1.predict(pcaX_test_scaled)\n\nprint(\"Classification Report:\\n \", classification_report(y_test, y_pred1))\nprint(\"\\nConfusion Matrix:\\n\",metrics.confusion_matrix(y_test, y_pred1))","50ec6ea1":"y_pred11 = model1.predict_proba(pcaX_test_scaled)\n(fpr, tpr, thresholds) = roc_curve(y_test,y_pred11[:,1])\nroc_auc = auc(fpr,tpr)\n\nplt.figure()\nlw = 2\nplt.plot(fpr, tpr, \n         lw=lw, label='ROC curve (area = %0.2f)' % roc_auc) \nplt.plot([0, 1], [0, 1], lw=lw, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('FPR')\nplt.ylabel('TPR')\nplt.title('ROC')\nplt.legend(loc=\"lower right\")\nplt.show()","d25fabc9":"from imblearn.over_sampling import RandomOverSampler\nfrom collections import Counter\n\nros = RandomOverSampler(0.9)\n\nX_train_ros, y_train_ros = ros.fit_resample(pcaX_train_scaled, y_train)\n\nprint(\"The number of classes before fit {}\".format(Counter(y_train)))\nprint(\"The number of classes after fit {}\".format(Counter(y_train_ros)))","d33b4662":"from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nlda = LinearDiscriminantAnalysis(solver=\"svd\", store_covariance=True)\nmodel2ros = lda.fit(X_train_ros, y_train_ros)\ny_pred_ros = model2ros.predict(pcaX_test_scaled)\nprint(\"Classification Report:\\n \", classification_report(y_test,y_pred_ros))\nprint(\"\\nConfusion Matrix:\\n\",metrics.confusion_matrix(y_test,y_pred_ros))","79d43bff":"y_pred21 = model2ros.predict_proba(pcaX_test_scaled)\n(fpr, tpr, thresholds) = roc_curve(y_test,y_pred21[:,1])\nroc_auc = auc(fpr,tpr)\n\nplt.figure()\nlw = 2\nplt.plot(fpr, tpr,lw=lw, label='ROC curve (area = %0.2f)' % roc_auc) \nplt.plot([0, 1], [0, 1], lw=lw, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('FPR')\nplt.ylabel('TPR')\nplt.title('ROC')\nplt.legend(loc=\"lower right\")\nplt.show()","a2e5df8d":"from sklearn.linear_model import LogisticRegression\nmodel3 = LogisticRegression(C=0.001,solver=\"sag\")\nmodel3.fit(X_train_ros, y_train_ros)\ny_pred3 = model3.predict(pcaX_test_scaled)\n\nprint(\"Classification Report:\\n \", classification_report(y_test, y_pred3))\nprint(\"\\nConfusion Matrix:\\n\",metrics.confusion_matrix(y_test, y_pred3))","f5c4bfe0":"y_pred131 = model3.predict_proba(pcaX_test_scaled)\n(fpr, tpr, thresholds) = roc_curve(y_test,y_pred131[:,1])\nroc_auc = auc(fpr,tpr)\n\nplt.figure()\nlw = 2\nplt.plot(fpr, tpr, \n         lw=lw, label='ROC curve (area = %0.2f)' % roc_auc) \nplt.plot([0, 1], [0, 1], lw=lw, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('FPR')\nplt.ylabel('TPR')\nplt.title('ROC')\nplt.legend(loc=\"lower right\")\nplt.show()","40ab408e":"from sklearn.naive_bayes import GaussianNB\nmodel1 = GaussianNB()\nmodel1.fit(X_train_ros, y_train_ros)\ny_pred1 = model1.predict(pcaX_test_scaled)\n\nprint(\"Classification Report:\\n \", classification_report(y_test, y_pred1))\nprint(\"\\nConfusion Matrix:\\n\",metrics.confusion_matrix(y_test, y_pred1))","c2842e30":"y_pred11 = model1.predict_proba(pcaX_test_scaled)\n(fpr, tpr, thresholds) = roc_curve(y_test,y_pred11[:,1])\nroc_auc = auc(fpr,tpr)\n\nplt.figure()\nlw = 2\nplt.plot(fpr, tpr, \n         lw=lw, label='ROC curve (area = %0.2f)' % roc_auc) \nplt.plot([0, 1], [0, 1], lw=lw, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('FPR')\nplt.ylabel('TPR')\nplt.title('ROC')\nplt.legend(loc=\"lower right\")\nplt.show()","86fcc2d0":"from imblearn.under_sampling import NearMiss\nns=NearMiss(0.9)\n\nX_train_ns,y_train_ns=ns.fit_resample(pcaX_train_scaled, y_train)\n\nprint(\"The number of classes before fit {} \".format(Counter(y_train)))\nprint(\"The number of classes after fit {} \".format(Counter(y_train_ns)))","0e8e2f2c":"from sklearn.naive_bayes import GaussianNB\nmodel1 = GaussianNB()\nmodel1.fit(X_train_ns, y_train_ns)\ny_pred1 = model1.predict(pcaX_test_scaled)\n\nprint(\"Classification Report:\\n \", classification_report(y_test, y_pred1))\nprint(\"\\nConfusion Matrix:\\n\",metrics.confusion_matrix(y_test, y_pred1))","07cb9aab":"y_pred11 = model1.predict_proba(pcaX_test_scaled)\n(fpr, tpr, thresholds) = roc_curve(y_test,y_pred11[:,1])\nroc_auc = auc(fpr,tpr)\n\nplt.figure()\nlw = 2\nplt.plot(fpr, tpr,lw=lw, label='ROC curve (area = %0.2f)' % roc_auc) \nplt.plot([0, 1], [0, 1], lw=lw, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('FPR')\nplt.ylabel('TPR')\nplt.title('ROC')\nplt.legend(loc=\"lower right\")\nplt.show()","66ef7889":"from sklearn.linear_model import LogisticRegression\nmodel3 = LogisticRegression(C=0.001,solver=\"sag\")\nmodel3.fit(X_train_ns, y_train_ns)\ny_pred3 = model3.predict(pcaX_test_scaled)\n\nprint(\"Classification Report:\\n \", classification_report(y_test, y_pred3))\nprint(\"\\nConfusion Matrix:\\n\",metrics.confusion_matrix(y_test, y_pred3))","9a8f6ed4":"y_pred131 = model3.predict_proba(pcaX_test_scaled)\n(fpr, tpr, thresholds) = roc_curve(y_test,y_pred131[:,1])\nroc_auc = auc(fpr,tpr)\n\nplt.figure()\nlw = 2\nplt.plot(fpr, tpr, \n         lw=lw, label='ROC curve (area = %0.2f)' % roc_auc) \nplt.plot([0, 1], [0, 1], lw=lw, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('FPR')\nplt.ylabel('TPR')\nplt.title('ROC')\nplt.legend(loc=\"lower right\")\nplt.show()","d403f421":"from sklearn.naive_bayes import GaussianNB\nmodel1 = GaussianNB()\nmodel1.fit(X_train_ns, y_train_ns)\ny_pred1 = model1.predict(pcaX_test_scaled)\n\nprint(\"Classification Report:\\n \", classification_report(y_test, y_pred1))\nprint(\"\\nConfusion Matrix:\\n\",metrics.confusion_matrix(y_test, y_pred1))","23760fc5":"y_pred11 = model1.predict_proba(pcaX_test_scaled)\n(fpr, tpr, thresholds) = roc_curve(y_test,y_pred11[:,1])\nroc_auc = auc(fpr,tpr)\n\nplt.figure()\nlw = 2\nplt.plot(fpr, tpr, \n         lw=lw, label='ROC curve (area = %0.2f)' % roc_auc) \nplt.plot([0, 1], [0, 1], lw=lw, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('FPR')\nplt.ylabel('TPR')\nplt.title('ROC')\nplt.legend(loc=\"lower right\")\nplt.show()","a2533918":"%%time\nfrom imblearn.combine import SMOTETomek\nkos = SMOTETomek(random_state=0)\nX_train_kos, y_train_kos = kos.fit_resample(pcaX_train_scaled, y_train)\n\nprint(\"The number of classes before fit {} \".format(Counter(y_train)))\nprint(\"The number of classes after fit {} \".format(Counter(y_train_kos)))","974232a5":"from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nlda = LinearDiscriminantAnalysis(solver=\"svd\", store_covariance=True)\nmodel2kos = lda.fit(X_train_kos, y_train_kos)\ny_pred_kos = model2kos.predict(pcaX_test_scaled)\nprint(\"Classification Report:\\n \", classification_report(y_test,y_pred_kos))\nprint(\"\\nConfusion Matrix:\\n\",metrics.confusion_matrix(y_test,y_pred_kos))","f25603f9":"y_pred_kos = model2kos.predict_proba(pcaX_test_scaled)\n(fpr, tpr, thresholds) = roc_curve(y_test,y_pred_kos[:,1])\nroc_auc = auc(fpr,tpr)\n\nplt.figure()\nlw = 2\nplt.plot(fpr, tpr,lw=lw, label='ROC curve (area = %0.2f)' % roc_auc) \nplt.plot([0, 1], [0, 1], lw=lw, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('FPR')\nplt.ylabel('TPR')\nplt.title('ROC')\nplt.legend(loc=\"lower right\")\nplt.show()","28ee16dd":"from sklearn.linear_model import LogisticRegression\nmodel3 = LogisticRegression(C=0.001,solver=\"sag\")\nmodel3.fit(X_train_kos, y_train_kos)\ny_pred3 = model3.predict(pcaX_test_scaled)\n\nprint(\"Classification Report:\\n \", classification_report(y_test, y_pred3))\nprint(\"\\nConfusion Matrix:\\n\",metrics.confusion_matrix(y_test, y_pred3))","df85ae66":"y_pred131 = model3.predict_proba(pcaX_test_scaled)\n(fpr, tpr, thresholds) = roc_curve(y_test,y_pred131[:,1])\nroc_auc = auc(fpr,tpr)\n\nplt.figure()\nlw = 2\nplt.plot(fpr, tpr, \n         lw=lw, label='ROC curve (area = %0.2f)' % roc_auc) \nplt.plot([0, 1], [0, 1], lw=lw, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('FPR')\nplt.ylabel('TPR')\nplt.title('ROC')\nplt.legend(loc=\"lower right\")\nplt.show()","6247ae73":"from sklearn.naive_bayes import GaussianNB\nmodel1 = GaussianNB()\nmodel1.fit(X_train_kos, y_train_kos)\ny_pred1 = model1.predict(pcaX_test_scaled)\n\nprint(\"Classification Report:\\n \", classification_report(y_test, y_pred1))\nprint(\"\\nConfusion Matrix:\\n\",metrics.confusion_matrix(y_test, y_pred1))","12d791c3":"y_pred11 = model1.predict_proba(pcaX_test_scaled)\n(fpr, tpr, thresholds) = roc_curve(y_test,y_pred11[:,1])\nroc_auc = auc(fpr,tpr)\n\nplt.figure()\nlw = 2\nplt.plot(fpr, tpr, \n         lw=lw, label='ROC curve (area = %0.2f)' % roc_auc) \nplt.plot([0, 1], [0, 1], lw=lw, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('FPR')\nplt.ylabel('TPR')\nplt.title('ROC')\nplt.legend(loc=\"lower right\")\nplt.show()","6e37ba73":"## XGBoost","70f16270":"## Perceptron","b8edef3a":"### 5. KMeans-client cluster","8b97efca":"## Correlation check and heatmap","6bdc9827":"## BernoulliNB","d9c21f06":"## Normalization and PCA","a70448b7":"## Random Forest","35e1b9c8":"## SVM (Non-Linear)","c65b4e23":"### 6. Sihouette score-Client cluster","aaebdd03":"## LinearSVM","7d391d27":"## Descriptive analysis","7ef06edf":"## Under sampling","84b19f81":"### LR","7fd0d02c":"## Catboost","5660474f":"## GaussianNB","00766002":"## Clustering","0a7b0a48":"## LDA","aacae7b5":"## LightGBM","ec8d4ee0":"### 3. Kmeans-business cluster","bce600ff":"## AdaBoost","30052e6e":"## Over Sampling","b19a8615":"## Get dummy variables","e7e9f468":"## SMOTETomek","093ff337":"## PCA Reconstruction Error","516c960f":"## Train and Test Split","4f4a5ecd":"## SMOTE","a38d822a":"### 2. Sihouette score-Business cluster","dea07634":"### 1. GM-business cluster","880ac2a6":"## Multi-layer Perceptron classifier","7a0dc7af":"### LDA","ed1416be":"## Decision Tree","d76ae817":"### 4. GM-Client cluster","1cadb6d2":"## Logistic Regression"}}