{"cell_type":{"79e5244f":"code","6c3968c3":"code","4301bed2":"code","ea08264d":"code","5a4b370c":"code","7cf22420":"code","959995a2":"code","7aee13b8":"code","e7f76b91":"code","f3d517cb":"code","09b8ab07":"code","83349eae":"code","7f78209d":"code","defe6942":"code","3fae77c0":"code","c03717ff":"code","d9b26408":"code","58c1b26f":"code","7ba8b470":"code","83d11c89":"code","a593c2d8":"code","50f1e5a5":"code","98d635b6":"code","4d63c5ca":"code","253d8276":"code","3be494d5":"code","ae230b5b":"code","6dfb940f":"code","2e8654d8":"code","abecc059":"code","40d04956":"code","34597495":"code","5c8faa52":"code","a2ebb3a7":"code","02b0c581":"code","82231e29":"code","4b3af334":"code","3c3b7993":"code","bf92d677":"code","6dc4b45a":"code","4bb40296":"code","df36ada3":"code","c2f20169":"markdown","df14dda0":"markdown","602b4c8c":"markdown","85700717":"markdown","b87c84c7":"markdown","f897f91a":"markdown","b0321994":"markdown","e82ab842":"markdown","0eae2ba3":"markdown","3cea936c":"markdown","462cbe97":"markdown","c7cccfc1":"markdown","d713ce28":"markdown","f1c4062e":"markdown","3189cbf9":"markdown","91886e09":"markdown","037fef50":"markdown","0eada887":"markdown","cd51f83d":"markdown","5d4c87f0":"markdown","f9826807":"markdown","5f9ba1ad":"markdown","9d9fb4ae":"markdown","95d2f7cf":"markdown","acfcf097":"markdown","b7592cc1":"markdown","6c5d88cf":"markdown","7bcfe423":"markdown","f526bf68":"markdown","cf9b1064":"markdown","a53dec39":"markdown","fec5255d":"markdown","79a97f50":"markdown","cd718286":"markdown","1a231a15":"markdown","c59aa7b9":"markdown","74220e3d":"markdown","9aa7873a":"markdown","e1c5e728":"markdown","b9e48f47":"markdown","6dc4bb02":"markdown","5b299e47":"markdown","18629b78":"markdown","42dcff91":"markdown","cc91bb82":"markdown","0e869c14":"markdown","a6ed8a6e":"markdown","90cbd69f":"markdown"},"source":{"79e5244f":"# General Libraries for Data Analysis and Visualization\nimport pandas as pd\nimport numpy as np\n\n# Visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport folium\n\n# Statistics\nimport statsmodels.stats.api as sms\n\n# Classification\nfrom xgboost import XGBClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\n\n# Standardization\nfrom sklearn.preprocessing import StandardScaler\n\n# Evaluation Metrics \nfrom sklearn.metrics import confusion_matrix, f1_score, roc_auc_score, plot_roc_curve, balanced_accuracy_score\n# Validation\nfrom sklearn.model_selection import  GridSearchCV, cross_validate, cross_val_score\nfrom sklearn.model_selection import StratifiedKFold\n\nfrom warnings import filterwarnings\nfilterwarnings('ignore')","6c3968c3":"# Load data set\ndata = pd.read_csv(\"..\/input\/covid19-air-pollution-data\/covid_data.csv\")\ndata.head()","4301bed2":"data.shape","ea08264d":"data.isnull().values.any()","5a4b370c":"data.info()","7cf22420":"pd.set_option('display.float_format', lambda x: '%.2f' % x)\ndata.describe().T","959995a2":"# World Map\ndef covid19_world_map(data, country_names, numerical_attribute_1, numerical_attribute_2, longitude_attribute, latitude_attribute):\n    \n    \"\"\"Return the world wide map of Covid-19 cases and deaths for the countries.\"\"\"\n    \n    world_map = folium.Map(location=[0, 0], \n                           tiles='cartodbpositron',\n                           min_zoom=1, \n                           max_zoom=4, \n                           zoom_start=1)\n\n    for i in range(0, len(data)):\n        folium.Circle(location=[data.iloc[i][latitude_attribute], data.iloc[i][longitude_attribute]],\n                      color='crimson', fill='crimson',\n                      tooltip =   '<li><bold>Country : ' \n                      + str(data.iloc[i][country_names]) \n                      + '<li><bold>Confirmed Cases : '\n                      + str(data.iloc[i][numerical_attribute_1])\n                      + '<li><bold>Deaths : '+str(data.iloc[i][numerical_attribute_2]),\n                      radius=int(data.iloc[i][numerical_attribute_1])**0.5).add_to(world_map)\n    return world_map\n\ncovid19_world_map(data,'Country', 'Total_Case', 'Total_Death', 'Longitude', 'Latitude')","7aee13b8":"data[\"Change\"].describe()","e7f76b91":"figure, axes = plt.subplots(nrows=1, ncols=1, figsize=(5, 5))\nchart = sns.countplot(x='Change', data=data, palette=\"gnuplot2\")\nchart.set_title('Positive Change \/ No Change')\n    \n","f3d517cb":"# Change attribute derived from the Avg_Pollution_2020 and Avg_Pollution_2019 atributes. \ndata1 = data.copy()\ndata1.drop(columns=[\"Country\",\"City\", \"Avg_Pollution_2020\",\"Avg_Pollution_2019\", \"Longitude\", \"Latitude\"], inplace=True) ","09b8ab07":"#Outlier Detection\ndef print_outliers(data):\n    \n    \"\"\"Return the number of outliers for each feature in the dataframe.\"\"\"\n    \n    for column in data.columns:\n        if column==\"Id\":\n            continue\n            \n        quantiles=data[column].quantile(q=[0.25,0.50,0.75]).values\n        q1=quantiles[0]\n        q2=quantiles[1]\n        q3=quantiles[2]\n        iqr=q3-q1   \n        lower_limit =  q1- 1.5*iqr\n        upper_limit =  q3 + 1.5*iqr\n        outliers=data[(data[column] < lower_limit) | (data[column] >  upper_limit)][column]\n        \n        print(\"number of outliers in {:10s} :\\t{}\\t({:.2%})\".format(column, len(outliers), len(outliers)\/len(data[column])))\n","83349eae":"print_outliers(data1)","7f78209d":"# 'Change' variable is the target.\n# Split dataset to X and y sets. X is for independent attribute and y for target attribute.\ny = data1.iloc[:,-1:]\nX = data1.iloc[:,:-1]","defe6942":"def standardize(data):\n    \n    \"\"\"Return the standardized dataframe.\"\"\"\n    \n    data1 = data.copy()\n    data1 = StandardScaler().fit_transform(data1)\n    data1 = pd.DataFrame(data1, columns = data.columns)\n    \n    return data1\n\nX = standardize(X)","3fae77c0":"def parameter_optimization(X, y, estimator, parameters, n_splits):\n    \n    \n    \"\"\"Return the best parameters using Grid Search method with Stratified Kfold CV.\"\"\"\n    cross_validation_method = StratifiedKFold(n_splits=5, shuffle=True, random_state=123)\n\n    grid_search_model = GridSearchCV(estimator=estimator, \n                                     param_grid=parameters, \n                                     cv=cross_validation_method,\n                                     scoring='f1',\n                                     n_jobs=-1,\n                                     verbose=2)\n    \n    grid_search_model.fit(X, y);\n    \n    return grid_search_model.best_params_","c03717ff":"def fit_predict(X, y, estimator):\n    \n    \"\"\"Fit model and predict the target, return predicted target.\"\"\"\n    \n    tuned_model = estimator\n    tuned_model.fit(X, y)\n    y_predict_test = tuned_model.predict(X)\n    \n    return y_predict_test","d9b26408":"def model_evaluate(estimator, X, y_true, y_pred):\n    \n    \"\"\"Evaluate the perofrmance of model using confusion matrix, balanced accuracy and F1 score.\"\"\"\n    \n    confusion_matrix_ = confusion_matrix(y_true, y_pred)\n    sns.heatmap(confusion_matrix_, annot=True, fmt=\"d\", cmap=\"Blues\")\n    print(\"Balanced accuracy score: \", \"{:.2f}\".format(balanced_accuracy_score(y_true, y_pred)), \n          \"\\nAUC: \", \"{:.2f}\".format(roc_auc_score(y_true, estimator.predict_proba(X)[:,1])),\n          \"\\nF1-score: \", \"{:.2f}\".format(f1_score(y_true, y_pred)),\n          \"\\nConfusion Matrix: \")\n    ","58c1b26f":"def model_general_performance(estimator, X, y, n_splits):\n    \n    \"\"\"Evaluate the performance of the model with whole data, using f1-score and area under curve with cross validation.\"\"\"\n    scoring = {'F1-score': 'f1', 'AUC': 'roc_auc'}\n    cv_method = StratifiedKFold(n_splits, shuffle=True, random_state=123)\n    cv_result = cross_validate(estimator, X=X, y=y, cv=cv_method, scoring=scoring, return_train_score=True)\n    test_f1_score = cv_result[\"test_F1-score\"].mean()\n    test_auc = cv_result[\"test_AUC\"].mean()\n    print(\"F1-score:\")\n    print(\"Train: \", \": %0.2f \"  %  cv_result[\"train_F1-score\"].mean(),\n          \"\\nTest: \", \"%0.2f\" % cv_result[\"test_F1-score\"].mean(), \n          \"\\nConfidence Interval: \", \"%0.2f, %0.2f\" % sms.DescrStatsW(cv_result[\"test_F1-score\"]).tconfint_mean())\n          \n        \n    print(\"-------------------------------------------------------\")\n    print(\"Area Under Curve (AUC):\")\n    print(\"Train: \", \"%0.2f\" % cv_result[\"train_AUC\"].mean(),\n          \"\\nTest: \",\"%0.2f\" % cv_result[\"test_AUC\"].mean(), \n          \"\\nConfidence Interval: \", \"%0.2f, %0.2f\" % sms.DescrStatsW(cv_result[\"test_AUC\"]).tconfint_mean()),\n    \n    \n","7ba8b470":"xgb_model = XGBClassifier()\n\ny_predict_xgb = fit_predict(X, y, xgb_model)","83d11c89":"model_evaluate(xgb_model, X, y, y_predict_xgb)","a593c2d8":"xgb_params = {\n        'subsample': [0.5, 0.75, 1], # Subsampling ratio of original dataset.\n        'colsample_bytree': [0.5, 0.75, 1], # Feature subsampling ratio per tree.\n        'learning_rate': [0.025, 0.05, 0.1, 0.2, 0.3], # learning rate.\n        'max_depth':[2, 3, 5, 7], # Maximum depth of a tree.\n        'min_child_weight':[0.5, 1, 2], # Minimum weight of a child node.\n        'gamma': [0, 0.1, 0.2, 0.3, 0.5, 1.0, 2], # Regularization parameter.\n        'n_estimators': [100, 200, 300, 500],  # Number of estimators in the ensemble.\n        }","50f1e5a5":"# Fit and predict the model (predictions are made on test data)\ntuned_xgb =  XGBClassifier(colsample_bytree=1, gamma=0.3, max_depth=2, min_child_weight=1, n_estimators=100, subsample=0.5)\ny_predict_xgb = fit_predict(X, y, tuned_xgb)","98d635b6":"# Evaluate the model on test data\nevaluate_xgb = model_evaluate(tuned_xgb, X, y, y_predict_xgb)","4d63c5ca":"# General performance of model\ngeneral_score_xgb = model_general_performance(tuned_xgb, X, y, n_splits=5)","253d8276":"lr_model = LogisticRegression()\n\ny_predict_lr = fit_predict(X, y, lr_model)","3be494d5":"# Evaluate the perofrmance of model on test data\nmodel_evaluate(lr_model, X, y, y_predict_lr)","ae230b5b":"# General performance of the model\ngeneral_score_lr = model_general_performance(lr_model, X, y, n_splits=5)","6dfb940f":"rf_model = RandomForestClassifier(random_state=123)\ny_pred_rf = fit_predict(X,y, rf_model)","2e8654d8":"model_evaluate(rf_model, X, y, y_pred_rf)","abecc059":"# Fit and predict the model\n\ntuned_rf = RandomForestClassifier(max_features=2, n_estimators=100, random_state=123)\n\ny_predict_rf = fit_predict(X, y, tuned_rf)","40d04956":"# Evaluate the perofrmance of model on test data\nevaluate_rf = model_evaluate(tuned_rf, X, y, y_predict_rf)","34597495":"# General perofrmance of the model\ngeneral_score_rf = model_general_performance(tuned_rf, X, y, n_splits=5)","5c8faa52":"kNN_model = KNeighborsClassifier()\ny_pred_knn = fit_predict(X, y, kNN_model)\nmodel_evaluate(kNN_model, X, y, y_pred_knn)","a2ebb3a7":"parameters_kNN = {'n_neighbors': [1, 2, 3, 4, 5, 6, 7, 8]}\n\nbest_parameters_kNN = parameter_optimization(X, y, kNN_model, parameters_kNN, 5)","02b0c581":"best_parameters_kNN","82231e29":"# Fit and predict the model (predictions are made on test data)\ntuned_kNN = KNeighborsClassifier(n_neighbors=5)\ny_predict_kNN = fit_predict(X, y, tuned_kNN)","4b3af334":"# Evaluate the perofrmance of model on test data\nevaluate_kNN = model_evaluate(tuned_kNN, X, y, y_predict_kNN)\nevaluate_kNN","3c3b7993":"# General perofrmance of the model\ngeneral_score_kNN = model_general_performance(tuned_kNN, X, y, 5)","bf92d677":"disp = plot_roc_curve(tuned_rf, X, y)\nplot_roc_curve(lr_model, X, y, ax=disp.ax_);\nplot_roc_curve(tuned_xgb, X, y, ax=disp.ax_);\nplot_roc_curve(tuned_kNN, X, y,ax=disp.ax_);\n","6dc4b45a":"def cv_score(model, X, y, scoring):\n    \n    \"\"\"Returns the cross-validated algorithm score for the specified evaluation metric.\"\"\"\n    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=123)\n    scores = cross_val_score(model, X, y, scoring=scoring, cv=cv, n_jobs=-1, error_score='raise')\n    return scores","4bb40296":"models = [tuned_xgb, lr_model, tuned_rf, tuned_kNN]\n\n# cross validate the models and store results\nresults1,results2, names = list(), list(), list()\n\nfor model in models:\n  names_ = model.__class__.__name__\n  names.append(names_)\n  f1_score = cv_score(model, X, y, 'f1')\n  results1.append(f1_score)\n  print('>%s %.3f (%.3f)' % (names_, f1_score.mean(), f1_score.std()))\n  auc = cv_score(model, X, y, 'roc_auc')\n  results2.append(auc)  \n  print('>%s %.3f (%.3f)' % (names_, auc.mean(), auc.std()))\nplt.figure(figsize=(10,10))\n","df36ada3":"fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(20, 10))\n# rectangular box plot\n\nbplot1 = ax1.boxplot(results1,\n                     vert=True,  # vertical box alignment\n                     patch_artist=True,  # fill with color\n                     labels=names)  # will be used to label x-ticks\nax1.set_title('F1 score')\nplt.xticks(fontsize=12)\n\n# notch shape box plot\n\n\nbplot2 = ax2.boxplot(results2,  \n                     vert=True,  # vertical box alignment\n                     patch_artist=True,  # fill with color\n                     labels=names)  # will be used to label x-ticks\nax2.set_title('Area Under Curve')\nplt.xticks(fontsize=10)\n# fill with colors\ncolors = ['pink', 'lightblue', 'lightgreen', 'purple']\nfor bplot in (bplot1, bplot2):\n    for patch, color in zip(bplot['boxes'], colors):\n        patch.set_facecolor(color)\n\n# adding horizontal grid lines\nfor ax in [ax1, ax2]:\n    ax.yaxis.grid(True)\n    ax.set_xlabel('Estimator')\n    ax.set_ylabel('Performance')\n\nplt.show()","c2f20169":"### ROC Curves","df14dda0":"### Evaluate The Performance","602b4c8c":"When independent variables in training data are measured in different units, it is important to standardize variables before applying algorithms because one variable may dominate all the others. Since in the dataset all variables are in different scale, before applying any algorithm to the data, explanatory variables are standardized. \n\n* Except XGBoost and Random Forest.","85700717":"# Covid-19 Impact on Air Pollution\n\n* The main purpose of this study is correctly predicting the countries\/cities air pollution change with the machine learning algorithms such as XGBoost, Random Forest, K-Nearest Neighbor.","b87c84c7":"### Evaluate The Performance ","f897f91a":"### Fit & Predict ","b0321994":"8 of the actual positive instances are correctly classified as positive, 3 of the instances is correctly classified as negative. 1 of the instances are incorrectly classified as positive and 3 of the instances are incorrectly classified as negative. ","e82ab842":"### Evaluate The Performance","0eae2ba3":"- All variables have different mean and standard deviation value, each of the attributes have dfferent scale.\n\n- In average this year PM2.5 level is lower than the last year. \n- 61% of the countries are developed according to their Gdp per capita.\n- PM2.5 level  decreased for the 68% of the cities.","3cea936c":"#### Worldwide Cases and Deaths \n\n\n- In this study one major cities of each 47 countries is taken, their PM2.5 levels are compared with 2020 and 2019 in the same period.\n\n- In the belove map, countries (which was selected in this study),  total number of confirmed cases and total number of deaths can be seen (as of 17th May 2020).","462cbe97":"## 2. Logistic Regression","c7cccfc1":"- There are two categorical variables in the dataset.\n\n- There are 10 numerical attributes.  ","d713ce28":"## References \n\n\nBanerjee, P. (2020, March 13). KNN Classifier Tutorial in Python. Kaggle. https:\/\/www.kaggle.com\/prashant111\/knn-classifier-tutorial\n\nCross Validation (2019) Retrieved May 31,2020, from https:\/\/scikit-learn.org\/stable\/modules\/cross_validation.html\n\n\nGrid Search Cross Validation. (2019). Retrieved May 31, 2020, from https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.GridSearchCV.html\n\nLogistic Regression (2019) Retrieved May 31, 2020, from https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.linear_model.LogisticRegression.html\n\nKumar, D. (2020, May 28). Covid-19 Analysis, Visualization & Comparisons. Kaggle. https:\/\/www.kaggle.com\/imdevskp\/covid-19-analysis-visualization-comparisons\n\nModel Evaluation (2019) Retrieved May 31, 2020, from https:\/\/scikit-learn.org\/stable\/modules\/model_evaluation.html\n\nModel Validation in Python. (2018, June 19). Retrieved May 2, 2020, from https:\/\/www.datavedas.com\/model-validation-in-python\/\n\nRandom Forest Classifier (2019) Retrieved May 31, 2020, from https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.RandomForestClassifier.html\n\nReceiver Operating Characteristic (ROC) (2019) Retrived May 31, 2020, from https:\/\/scikit-learn.org\/stable\/auto_examples\/model_selection\/plot_roc.html\n\nSVM Hyperparameter Tuning Using GridSearcCV. (n.d.). Retrieved May 31, 2020, from https:\/\/www.geeksforgeeks.org\/svm-hyperparameter-tuning-using-gridsearchcv-ml\/","f1c4062e":"* There are outliers in the *Country_Population, Avg_Pollution_2020, Avg_Pollution_2019, Total_Case, Total_Death*. ","3189cbf9":"###  Finding The Best Parameters (Hyperparameter Tuning)","91886e09":"### Performance without Hyperparameter Tuning","037fef50":"### Hyperparameter Tuning","0eada887":"random_forest = RandomForestClassifier(random_state=123)\n\nparameters_random_forest = {\"max_features\": [2, 3, 5, 7, 10],\n                            \"n_estimators\": [100, 300, 500, 700, 1000]}\n\nbest_parameters_random_forest = parameter_optimization(X,y, estimator=random_forest, parameters=parameters_random_forest, n_splits=5)\nbest_parameters_random_forest","cd51f83d":"* **Country:** name of the countries\n* **City:** name of the city\n* **Total_Day_Lockdown:** how many day lockdown applied to the country\n* **Avg_Pollution_2020:** average air pollution level for the specific time periods in 2020\n* **Avg_Pollution_2019:** average air pollution level for the specific time periods in 2019\n* **Development:** if the country developed according to the World Bank GDP per capita (if it is above the 12000$ threshold level), 1. Otherwise 0.\n* **Total_Case:** total number of Covid-19 cases until May 17, 2020\n* **Total_Death:** total number of death because of Covid-19 until May 17,2020\n* **Latitude:** latitiude of the country\n* **Longitude:** longitude of the country\n* **Change:** if there is a decrease in the air pollution level in 2020 compared to 2019, 1. Otherwise, 0. ","5d4c87f0":"### Hyperparameter Optimization\n\n\n* A hyperparameter is external to the model and the value of it cannot be estimated from data. It affects the model performance and because of that hyperparameter tuning is important for all algorithms. \n\n\n* In this study, for each of the algorithms best hyperparameters are found with Grid Search Cross Validation. Grid-searching is a method that searching for best combination of hyperparameters. \n\n\n* In Scikitlearn, there is a *GridSearchCV*. Grid Search CV tries all possible parameter combinations which are determined by the user and find the best parameter setting for a specificed scoring method. \n\n* For this study, hypothetical set for each the hyperparameter are defined first, and best combination of hyperparameters are selected by GridSearchCV() method with Stratified 5-fold cross validation on training dataset. \n\n**Note:** Stratified Kfold CV i a cross validiation technique which is usd when data is imbalanced. It protect the distribution of target in the original data in each fold. \n","f9826807":"*  For F1-score XGBoost has the best perofrmance, it has tight intevals and it outperfrom the other algorithms. However, for AUC, it has large intervals. Similarly, Random Forest has wide intervals for all overall performance. For ROC curve, they have the best performance. XGBoost can be a good choice for this study. \n\n* For both F1-score and AUC  logistic regression has good performance. \n\n* k-NN has good standard deviation and it has tight intervals. However, it has the worst performance among them in the average.","5f9ba1ad":"* K nearest neighbors is a simple algorithm that classifies based on a distance function. \n\n* In this study, different k number of neighbor values are tried for the hyperparameter tuning. \n\n    1. Number of neighbors considered 1 to 8, \n","9d9fb4ae":"### Model Performance without Parameter Optimization","95d2f7cf":"## Models\n\n\n- In this section Logistic Regression, Support Vector Machine, Random Forest and K-Nearest Neigbors algorithms is applied to the dataset. Their hyperparameters will be tuned, after tuning general performance of the model will be calculated with repeated 5-Fold cross validation.\n\n\n\n \n\n","acfcf097":"### Fit & Predict","b7592cc1":"## 1. XGBoost\n\n\n- XGBoost is one of the most popular machine learning algorithm.\n\n\n- It is an improved version Gradient Boosting Machine. \n\n\n- It is has both algoritmic and system optimization. These are\n\n\n\n\n    1. Approximate greedy algorithm to increase the speed.\n    2. Regularized objective function to prevent overfitting.\n    3. Sparsity-aware Split Finding algorithm to handle missing data. \n    4. Weighted Quantile Sketch to helping approximate greedy algorithm. \n    5. Parallel Learning to make computation faster.\n    6. Cache-aware Access.\n    7. Blocks for Out-of-core computation.\n","6c5d88cf":"## 3. Random Forest\n\nRandom Forest(s) is proposed by Breiman (2001). It is a popular machine learning algorithm. Random Forest is preferred by the user for its good performance and ease of use. \n\nIt contains two types of randomness. \n    1. Bootstrap aggregating (bagging),\n    2. Randm feature selection.\n\nHyperparameters: \n- max_features: the number of features to consider when looking for the best split,\n- n_estimators: the number of trees in the forest.\n\nAccording to Breiman (2001), these two parameters are the most important parameters for Random Forest. ","7bcfe423":"## Descriptive Statistics","f526bf68":"### Evaluate The Performance","cf9b1064":"Best parameters:\n\n{'colsample_bytree': 1,\n 'gamma': 0.3,\n 'learning_rate': 0.3,\n 'max_depth': 2,\n 'min_child_weight': 1,\n 'n_estimators': 100,\n 'subsample': 0.5}","a53dec39":"* In this graph it can be seen that total number of Class 1's is much larger than total number  Class 0's. Nearly 68% of the cities has been affected positively and 32% of the cities has been faced with negative change or stay same in terms of PM2.5 levels.\n","fec5255d":"* Logistic regression turns linear predictions into probabilities by using the logistic function. \n* Logistic Regression does not really have any critical hyperparameters to tune. Therefore Grid Search part is not applied to this algorithm.","79a97f50":"### Hyperparameter Tuning","cd718286":"### Standardization","1a231a15":"* There is no mising value in the dataset.","c59aa7b9":"- Hyperparameters are optimized and the best parameters are selected as k=5.","74220e3d":"### Fit & Predict","9aa7873a":"### General Performance of K-Nearest Neighbors","e1c5e728":"### Cross Validated Performance of Logistic Regression Model","b9e48f47":"### Fit & Predict","6dc4bb02":"### Performance without Hyperparameter Tuning","5b299e47":"## 4. k-Nearest Neighbor Classifier","18629b78":"## Final Evaluation","42dcff91":"## Descriptive Statistics of Target Variable","cc91bb82":"###  Cross Validated Performance of XGBoost Model\n\n* F1 score and is selected as general perofrmance evaluation metric.\n* Due to the limited number of observations and imbalanced distribution of classes, accuracy score may not be meaningful for evaluating the model.","0e869c14":"### Cross Validated Performance of Random Forest Model","a6ed8a6e":"xgboost_model = XGBClassifier()\n\nbest_parameters_xgboost = parameter_optimization(X=X, y=y, estimator=xgboost_model, parameters=xgb_params, n_splits=5)\nbest_parameters_xgboost","90cbd69f":"* There are 47 observation and 12 attributes in the dataset."}}