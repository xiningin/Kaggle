{"cell_type":{"6f4ee947":"code","ad11c88f":"code","d0571e6e":"code","38a134ef":"code","8a19bb8c":"code","802227b9":"code","9bf12385":"code","2ed8e568":"code","18bedee2":"code","c1a89934":"code","cadaf094":"code","c39d67e3":"code","673dba58":"code","acf9470b":"code","c1263b6b":"code","c4e18c62":"code","527ffe6c":"code","4f5783f6":"code","0fdc008f":"code","8a4894ae":"code","f9560cd7":"code","d3dbe769":"code","adafe8ac":"code","78dabe3b":"code","5dc5dbc6":"code","9b77173d":"code","c9318eab":"code","752be5b7":"code","254243da":"code","43188297":"code","be3b6033":"code","6cc63578":"code","f284c156":"code","8474df96":"code","4084afc6":"code","91c71585":"code","0b0a2cc8":"code","8dadbd4f":"code","f4602cba":"code","615291f1":"code","e4e73cb4":"code","38c4868b":"code","efdfc262":"code","d6310150":"code","c2a7cb29":"code","ce717dbc":"code","72c3e3e5":"code","191172ad":"code","56cf56fe":"code","064d51f9":"code","a3b620ab":"code","54a6bb37":"code","c5bbeac0":"markdown","11e4e2e8":"markdown","1bffaad1":"markdown","9c95b568":"markdown","c2f8153b":"markdown","0d39b667":"markdown","36ab4260":"markdown","d3b35529":"markdown","78887c22":"markdown","77714552":"markdown","66b24df8":"markdown","2c6bb1fa":"markdown","da76307d":"markdown","7ebd6fd1":"markdown","12e28282":"markdown","5dfc9348":"markdown","fdb9e9d5":"markdown","c01ba817":"markdown","72d8ef1a":"markdown","33ec91b8":"markdown","e70a12bb":"markdown","5c77b05a":"markdown","4f0eda76":"markdown","9085d074":"markdown"},"source":{"6f4ee947":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sb\nfrom sklearn.linear_model import LinearRegression \nfrom sklearn.model_selection import train_test_split  \nfrom math import sqrt\nfrom sklearn.metrics import mean_squared_error\nimport mpl_toolkits\nfrom sklearn.ensemble import GradientBoostingRegressor","ad11c88f":"data=pd.read_csv('..\/input\/kc_house_data.csv')","d0571e6e":"#Display the first 10 records in the dataset\ndata.head(10)","38a134ef":"#Display a concise summary of the dataset\n#There are 21613 records in the dataset. Each column has exactly 21613 entries indicating there are no null entries.\n\ndata.info()","8a19bb8c":"#Generate descriptive statistics that summarize the central tendency, dispersion and shape of the dataset\u2019s distribution.\n\ndata.describe()","802227b9":"corrmat = data.corr()\ncols = corrmat.nlargest(21, 'price')['price'].index #specify number of columns to display i.e 21\nf, ax = plt.subplots(figsize=(18, 10)) #size of matrix\ncm = np.corrcoef(data[cols].values.T)\nsb.set(font_scale=1.25)\nhm = sb.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size':12}, yticklabels=cols.values,\n                 xticklabels=cols.values)\nplt.yticks(rotation=0, size=15)\nplt.xticks(rotation=90, size=15)\nplt.title(\"Correlation Matrix\",style='oblique', size= 20)\n\nplt.show()","9bf12385":"plt.figure(figsize=(18,10))\n\nplt.scatter(data['grade'],data['price'])\nplt.xlabel(\"Grade\")\nplt.ylabel(\"Price\")\nplt.title(\"Price v. Grade\")\n","2ed8e568":"plt.figure(figsize=(18,10))\n\nplt.scatter(data['yr_built'],data['price'])\nplt.xlabel(\"Year Built\")\nplt.ylabel(\"Price\")\nplt.title(\"Price v. Year Built\")","18bedee2":"plt.figure(figsize=(18,10))\nplt.title(\"Price v. Bathrooms\")\n\nplt.scatter(data['price'],data['bathrooms'])\nplt.ylabel(\"Bathrooms\")\nplt.xlabel(\"Price\")\nplt.plot(np.unique(data['price']), np.poly1d(np.polyfit(data['price'], data['bathrooms'], 1))(np.unique(data['price'])), color='green') #line of best fit\n\n","c1a89934":"plt.figure(figsize=(18,10))\n\nplt.scatter(data['bedrooms'],data['price'])\nplt.ylabel('Price')\nplt.xlabel('No. of bedrooms')\nplt.title('Bedrooms v. Price')","cadaf094":"plt.figure(figsize=(18,10))\n\nplt.scatter(data['sqft_living'],data['price'])\nplt.title('Price v. Square footage (living area)')\nplt.xlabel('Square Footage')\nplt.ylabel(\"Price\")","c39d67e3":"plt.figure(figsize=(18,10))\n\nplt.scatter(data['price'],data['sqft_above'])\nplt.title('Price v. Square footage (above)')\nplt.ylabel('Square Footage')\nplt.xlabel(\"Price\")\nplt.plot(np.unique(data['price']), np.poly1d(np.polyfit(data['price'], data['sqft_above'], 1))(np.unique(data['price'])), color='green') #line of best fit\n","673dba58":"plt.figure(figsize=(18,10))\n\nsb.jointplot(x=data.lat.values, y=data.long.values, size=12,color='brown')\nplt.ylabel('Longitude', fontsize=12)\nplt.xlabel('Latitude', fontsize=12)\nplt.title(\"Concentration of Houses by Location\")\nsb.despine\n","acf9470b":"plt.figure(figsize=(18,10))\n\ndata[\"bedrooms\"].value_counts().plot(kind='bar')\nplt.title('Count vs bedrooms Bar Graph')\nplt.ylabel(\"Count\")\nplt.xlabel('Number of bedrooms')","c1263b6b":"plt.figure(figsize=(18,10))\n\nplt.boxplot(data['bedrooms'],1,'gD')","c4e18c62":"#count number of houses with more than ten bedrooms\ndata[data['bedrooms']>10].count()","527ffe6c":"#locate house with 33 bedrooms\ndata.loc[data['bedrooms'] == 33]","4f5783f6":"plt.figure(figsize=(18,10))\n\nplt.boxplot(data['price'],1,'gD')","0fdc008f":"#count number of houses with prices above 7,000,000\ndata[data['price']>7000000].count()","8a4894ae":"#locate houses with a value above 7,000,000\ndata.loc[data['price'] > 7000000]","f9560cd7":"data = data[data.bedrooms != 33]\ndata = data[data.price < 6000000]","d3dbe769":"plt.figure(figsize=(18,10))\n\nfrom scipy import stats\nfrom scipy.stats import skew,norm\nfrom scipy.stats.stats import pearsonr\n# kernel density plot\nsb.distplot(data.condition,fit=norm);\nplt.ylabel =('Frequency')\nplt.title = ('Condition Distribution');\n(mu,sigma)= norm.fit(data['condition']);\n\n#QQ plot\nplt.figure(figsize=(18,10))\nres = stats.probplot(data['condition'], plot=plt)\nplt.show()\n\nprint(\"skewness: %f\" % data['condition'].skew())\nprint(\"kurtosis: %f\" % data ['condition'].kurt())","adafe8ac":"plt.figure(figsize=(18,10))\n\nfrom scipy import stats\nfrom scipy.stats import skew,norm\nfrom scipy.stats.stats import pearsonr\n# kernel density plot\nsb.distplot(data.sqft_above,fit=norm);\nplt.ylabel =('Frequency')\nplt.title = ('Square Foot Above Distribution');\n(mu,sigma)= norm.fit(data['sqft_above']);\n\n#QQ plot\nplt.figure(figsize=(18,10))\nres = stats.probplot(data['sqft_above'], plot=plt)\nplt.show()\n\nprint(\"skewness: %f\" % data['sqft_above'].skew())\nprint(\"kurtosis: %f\" % data ['sqft_above'].kurt())","78dabe3b":"plt.figure(figsize=(18,10))\n\nfrom scipy import stats\nfrom scipy.stats import skew,norm\nfrom scipy.stats.stats import pearsonr\n# kernel density plot\nsb.distplot(data.sqft_living15,fit=norm);\nplt.ylabel =('Frequency')\nplt.title = ('Square Foot Living(2015) Distribution');\n(mu,sigma)= norm.fit(data['sqft_living15']);\n\n#QQ plot\nplt.figure(figsize=(18,10))\nres = stats.probplot(data['sqft_living15'], plot=plt)\nplt.show()\n\nprint(\"skewness: %f\" % data['sqft_living15'].skew())\nprint(\"kurtosis: %f\" % data ['sqft_living15'].kurt())","5dc5dbc6":"plt.figure(figsize=(18,10))\n\nfrom scipy import stats\nfrom scipy.stats import skew,norm\nfrom scipy.stats.stats import pearsonr\n# kernel density plot\nsb.distplot(data.sqft_living,fit=norm);\nplt.ylabel =('Frequency')\nplt.title = ('Square Foot Living Distribution');\n(mu,sigma)= norm.fit(data['sqft_living']);\n\n#QQ plot\nplt.figure(figsize=(18,10))\nres = stats.probplot(data['sqft_living'], plot=plt)\nplt.show()\n\nprint(\"skewness: %f\" % data['sqft_living'].skew())\nprint(\"kurtosis: %f\" % data ['sqft_living'].kurt())","9b77173d":"from scipy import stats\nfrom scipy.stats import skew,norm\nfrom scipy.stats.stats import pearsonr\n# kernel density plot\nplt.figure(figsize=(18,10))\n\nsb.distplot(data.sqft_lot15,fit=norm);\nplt.ylabel =('Frequency')\nplt.title = ('Square Foot Lot(2015) Distribution');\n(mu,sigma)= norm.fit(data['sqft_lot15']);\n\n#QQ plot\n\nplt.figure(figsize=(18,10))\nres = stats.probplot(data['sqft_lot15'], plot=plt)\nplt.show()\n\nprint(\"skewness: %f\" % data['sqft_lot15'].skew())\nprint(\"kurtosis: %f\" % data ['sqft_lot15'].kurt())","c9318eab":"\nfrom scipy import stats\nfrom scipy.stats import skew,norm\nfrom scipy.stats.stats import pearsonr\n# kernel density plot\nplt.figure(figsize=(18,10))\n\nsb.distplot(data.price,fit=norm);\nplt.ylabel =('Frequency')\nplt.title = ('Price Distribution');\n(mu,sigma)= norm.fit(data['price']);\n\n#QQ plot\nplt.figure(figsize=(18,10))\nres = stats.probplot(data['price'], plot=plt)\nplt.show()\n\n\nprint(\"skewness: %f\" % data['price'].skew())\nprint(\"kurtosis: %f\" % data ['price'].kurt())","752be5b7":"plt.figure(figsize=(18,10))\n\n#log transform the target \ndata[\"sqft_lot15\"] = np.log1p(data[\"sqft_lot15\"])\n\n#Kernel Density plot\nsb.distplot(data.sqft_lot15,fit=norm);\nplt.ylabel=('Frequency')\nplt.title=('Square Foot Lot(2015) distribution');\n#Get the fitted parameters used by the function\n(mu,sigma)= norm.fit(data['sqft_lot15']);\n\n\n\n#QQ plot\nplt.figure(figsize=(18,10))\n\nres =stats. probplot(data['sqft_lot15'], plot=plt)\nplt.show()\nprint(\"skewness: %f\" % data['sqft_lot15'].skew())\nprint(\"kurtosis: %f\" % data['sqft_lot15'].kurt())","254243da":"plt.figure(figsize=(18,10))\n\n#log transform the target \ndata[\"price\"] = np.log1p(data[\"price\"])\n\n#Kernel Density plot\nsb.distplot(data.price,fit=norm);\nplt.ylabel=('Frequency')\nplt.title=('Price distribution');\n#Get the fitted parameters used by the function\n(mu,sigma)= norm.fit(data['price']);\nplt.savefig('dist.png')\n\n\n#QQ plot\nplt.figure(figsize=(18,10))\nres =stats. probplot(data['price'], plot=plt)\nplt.show()\nprint(\"skewness: %f\" % data['price'].skew())\nprint(\"kurtosis: %f\" % data['price'].kurt())","43188297":"#Initialize Linear Regression to a variable reg\n\nreg=LinearRegression()","be3b6033":"#Initialize the value to be predicted(label) as price\n\nlabels=data['price']","6cc63578":"#convert date into a readable data-type by the algorithm\n#since the date variable had only 2014 and 2015, the date column can be trasformed into a nominal category with 1 representing 2014 and 0 representing 2015.\n\nconv_dates = [1 if values == 2014 else 0 for values in data.date ]\ndata['date']=conv_dates","f284c156":"#drop columns not used in training.\n#id, yr_built, condition and long (longitute) are droped because the have low corelation\/significance on the target.\n#price is also droped since it is not used as part of the independent variables.\n\ntrain1 = data.drop(['id', 'price','condition','yr_built','long'],axis=1)\n","8474df96":"#70%, 30% train, test split\n\nx_train , x_test , y_train , y_test = train_test_split(train1 , labels , test_size = 0.3,random_state =5)\n","4084afc6":"#Fitting the regression algorithm with data from the train set.\n#x_train represents the predictors (independent variables) and y_train represents the target.\n\nreg.fit(x_train,y_train)","91c71585":"#Testing our accuracy.\nacc1=reg.score(x_test,y_test)\nprint(str(\"The accuracy of the model is: \"+str(\"%.2f\" %(acc1*100))+\"%\"))\n","0b0a2cc8":"from sklearn.metrics import mean_squared_error\nfrom math import sqrt","8dadbd4f":"y_prediction1 = reg.predict(x_test)","f4602cba":"RMSE_lin = sqrt(mean_squared_error(y_true = y_test, y_pred = y_prediction1))","615291f1":"print(RMSE_lin)","e4e73cb4":"from sklearn.ensemble import GradientBoostingRegressor ","38c4868b":"gbr=GradientBoostingRegressor(n_estimators= 400, max_depth = 5, min_samples_split = 2, learning_rate = 0.08, loss = 'ls')","efdfc262":"train2 = data.drop(['id', 'price','condition','yr_built','long'],axis=1)\n\n#70%, 30% train, test split\n\nx_train1 , x_test1 , y_train1 , y_test1 = train_test_split(train2 , labels , test_size = 0.3,random_state =5)\n","d6310150":"gbr.fit(x_train1,y_train1)","c2a7cb29":"acc=gbr.score(x_test1,y_test1)\nacc","ce717dbc":"acc2=(\"%.2f\" % (acc*100))\nacc2","72c3e3e5":"print(str(\"The acccuracy of the model is: \"+str(acc2)+\"%\"))","191172ad":"feature_importance = gbr.feature_importances_\nfeature_importance = 100.0 * (feature_importance \/ feature_importance.max())\nsorted_idx = np.argsort(feature_importance)\npos = np.arange(sorted_idx.shape[0]) + .5\n\nplt.figure(figsize=(12,6))\nplt.barh(pos, feature_importance[sorted_idx], align='center')\nplt.yticks(pos, x_train.columns[sorted_idx])\nplt.xlabel('Relative Importance')\nplt.title('Feature Importance')\nplt.show()","56cf56fe":"from sklearn.metrics import mean_squared_error\nfrom math import sqrt","064d51f9":"y_prediction = gbr.predict(x_test)\n","a3b620ab":"RMSE_gbr = sqrt(mean_squared_error(y_true = y_test1, y_pred = y_prediction))","54a6bb37":"print(RMSE_gbr)","c5bbeac0":"### Bar graph\n\nThe bar graph was used to compare the count of different number bedrooms in the distribution.\n\nAs seen, there is a house with 33 bedrooms which could be a potential outlier.","11e4e2e8":"### **RMSE**","1bffaad1":"### Correlation Matrix\nA correlation matrix is a table showing correlation coefficients between variables.\n\nThis correalation matrix uses 'price', which we want to predict, as the target (dependent) variable.\n\nThe coeffiecients indictate the degree correlation of the independent variables against 'price'.","9c95b568":"### RMSE\n\nRMSE (Root Mean Squared Error) is a square root of MSE.\n\nMSE is the average squared loss per example. MSE is calculated by dividing the squared loss by the number of examples.\n\nThe square root is introduced to make scale of the errors to be the same as the scale of targets.","c2f8153b":"# Probability Distribution and Normalization\nA probability distribution is a device for indicating the values that a random variable may have.\n\nThe goal of normalization is to change the values of numeric columns in the dataset to use a common scale, without distorting differences in the ranges of values. This in turn creates a normal distribution.\n\nA normal distrution is a symmetric distribution where most of the observations cluster around the central peak and the mean, median, and mode of a normal distribution are almost equal therefore minimizing variance (deviation from the mean).\n\nA normal distribution is important as it eases training of a machine learning model and provides eveness while sampling data.\n\n","0d39b667":"# Identifying and Droping Outliers\nAn outlier is an observation point that is distant from other observations.\n\nOutliers  affect the mean and median of a distribution which in turn may affect the predictive model as it tries to fit the data for training.","36ab4260":" # Importing Libraries\n These are libraries necessary for data analysis, ploting and machine learning algorithms for predictive analytics.","d3b35529":"### Joint Plot\n\nThe joint plot below was used to display a map-like diagram by ploting Latitude and Longitude to show the distribution of the houses across the area.\n\nIt was also used to show areas with a high density of houses.","78887c22":"## Identifying Outliers\nBox plots, violin plots,  scatter plots, bar graphs and other statistical methods can be used to identify outliers.\n\nBox plots were used in this case to identify the outliers as a simple and accurate method.","77714552":"## **1. Linear Regression**","66b24df8":"# Predictive Analytics\n\nPredictive modeling in Machine Learning involves selection of the best algorithm that will be able to build a predictive model, with the highest accuracy and lowest loss, which will be able to predict the dependent variable given the independent variables.\n\nA model is a representation of what an ML system has learned from the training data.\n\nLoss is a measure of how far a model's predictions are from the actual value.\n\nLinear Regression and Gradient Boosting Regression algoithms were both used in an attempt to build a model as the  value to be predicted is continous.","2c6bb1fa":"## Probability distribution\n\nDistribution plots are used to show a probality distribution.\nFor a normal distribution, the ideal skewness and kurtosis value is approximately 0.\n\nSkewness is a measure of the asymmetry of the probability distribution of a random variable about its mean.\n\nKurtosis  is used to describe the extreme values in one versus the other tail of a distribution.","da76307d":"### **Feature Importance**\nFeature importance shows significance of predictors of variables on the target after training using GBR.","7ebd6fd1":"### Cross Validation\nCross validation is a method of estimating how well a model will perform  on new data by testing the model against one or more non-overlapping data subsets withheld from the training set.\n\nThe dataset will be split into two, train and test sets,  where the model will be trained on the train set and its performance, accuracy and loss, tested on the test set.","12e28282":"### Scatter Plots\nA scatter plot plots a graph of two variables where the pattern of the resulting points reveal any correlation present.\n\nScatter plots were used here to identify if there is a linear correlation betweeen indepent variable and price.\n\nA line of best fit was also included in some plots to indicate the best linear trend.","5dfc9348":"## **2. Gradient Boosting Regressor**\n","fdb9e9d5":"# House Sales in King County, USA - Predictive Analytics Model","c01ba817":"# Reading the dataset\n More details on the dataset and its columns can be found [here](https:\/\/www.kaggle.com\/harlfoxem\/housesalesprediction).","72d8ef1a":"## Plots","33ec91b8":"## Normalization\n\nVarious data transformation methods such as Box-Cox, arcsine, and log transformations can be used in normalization.\n\n(Natural) log transformation is often used where the data has a positively skewed distribution and therefore was used to normalize columns that were highly skewed.\n\n","e70a12bb":"The aim of this project is to develop a regression model to predict of house prices before their sale.\n\nThe data is drawn from house sales in King County, USA from the year 2014 to 2015.\n\nThe dataset has 19 columns and 21,613 rows.\n ","5c77b05a":"### Droping Outliers","4f0eda76":"# Exploratory Data Analysis\n  The purpose of EDA is for showing us what the data can tell us before building or hypothesis testing task.\n  \nThis helps in uderstanding the dataset and assist in data cleaning by identifying outlier, filling null entries and feature engineering (creation of new variables from existing variables).","9085d074":"### Box Plots"}}