{"cell_type":{"a12ce6ef":"code","5136f2ae":"code","f77a0a66":"code","7d1ccd84":"code","fecf10bf":"code","7175e2b4":"code","f4c3c5f6":"code","fa11d05f":"code","9dfaeab1":"code","6d6cadbe":"code","614d88ab":"code","26035809":"code","eebf5888":"code","83325585":"code","3b3d3a79":"code","a80c39d0":"code","5d7c2751":"code","d0ce7e79":"code","32aaf149":"code","cbacfb6b":"code","d59dfef4":"code","a63c1792":"code","37f2aaca":"code","95218990":"code","456ffe27":"code","b91adff1":"code","7b931455":"code","bc635f0e":"code","ac0b5541":"code","5e6cf08e":"code","79e0118f":"code","493e4b82":"code","29c2e014":"code","8fff5363":"code","c0e3bfb6":"code","17f3a47f":"code","84947c23":"code","4d7146e7":"code","6d784eb7":"code","9fc721b1":"code","5038c1b5":"markdown","23da17df":"markdown","97184f08":"markdown","14f5d14c":"markdown","4fa95e28":"markdown","1d3a96d2":"markdown","cdf687a7":"markdown","5e2a91e5":"markdown","9512970e":"markdown","07cf6052":"markdown","1696855b":"markdown","b5044095":"markdown","5bbb9374":"markdown","d888d0c3":"markdown","f7ade223":"markdown","ead2993c":"markdown","614481e4":"markdown","2e285343":"markdown","010c4685":"markdown","f405e724":"markdown","11377ada":"markdown","c9da870b":"markdown","acac77ae":"markdown"},"source":{"a12ce6ef":"import pandas as pd\nimport umap.umap_ as umap\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport plotly.graph_objects as go\nimport plotly.express as px\n\nfrom plotly.subplots import make_subplots\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import StratifiedKFold \nfrom sklearn.metrics import log_loss\n\n# ML\nfrom catboost import CatBoostClassifier\nfrom lightgbm import LGBMClassifier\nfrom xgboost import XGBClassifier","5136f2ae":"data_train = pd.read_csv('..\/input\/tabular-playground-series-may-2021\/train.csv').drop('id', axis=1)\ndata_test = pd.read_csv('..\/input\/tabular-playground-series-may-2021\/test.csv').drop('id', axis=1)","f77a0a66":"all_data = pd.concat([data_train, data_test], axis=0)","7d1ccd84":"data_train","fecf10bf":"all_data.isnull().sum()","7175e2b4":"data_test.describe().T.style.bar(subset=['mean', 'std'], color='#d65f5f')","f4c3c5f6":"data_train.describe().T.style.bar(subset=['mean', 'std'], color='#d65f5f')","fa11d05f":"fig = go.Figure()\n\nto_plot = data_train.value_counts('target')\n\nfig.add_trace(go.Pie(\n    labels = to_plot.index,\n    values = to_plot.values,\n    textinfo='label+percent'\n))\n\nfig.update_layout(\n    template='plotly_dark',\n    title_text = 'Target Distribution'\n)","9dfaeab1":"fig = make_subplots(\n    rows=10,\n    cols=5,\n    subplot_titles=data_train.columns,\n)\n\n# Add traces\ncolumns = data_train.drop('target', axis=1).columns.tolist()\n\nfor row in range(10):\n    for col in range(5):\n        column = columns.pop(0)\n        to_plot = data_train[column].value_counts()\n\n        fig.add_trace(go.Scatter(\n            x = to_plot.index,\n            y = to_plot.values,\n            name = column,\n            mode='lines'\n        ), col=col+1, row=row+1)\n\n        fig.update_yaxes(title='y', visible=False, showticklabels=False)\n\n        if(col+1 == 5):\n            break\n\nfig.update_layout(\n    height=1000,\n    width=700,\n    showlegend=False,\n    template='plotly_dark',\n)\nfig.update_annotations(font_size=12)","6d6cadbe":"to_plot = data_train.drop('target', axis=1).isin([0]).sum(axis=0)\npercent = np.array(to_plot)\/100000 * 100\n\nfig = go.Figure()\n\nfig.add_trace(go.Bar(\n    x = to_plot.values,\n    y = to_plot.index,\n    orientation='h',\n    text = np.round(percent, 2),\n    textposition='outside',\n    marker={\n        'color': to_plot.values,\n        'colorscale': 'Purples',\n\n    }\n))\n\nfig.update_layout(\n    height=1000,\n    width=700,\n    template='plotly_dark',\n    title_text='Percent of zeros in every column'\n)","614d88ab":"data_train_target_num = data_train.replace({'target': {'Class_1': 1, 'Class_2': 2, 'Class_3': 3, 'Class_4': 4}})\n\nplt.figure(figsize=(8, 12))\n\nheatmap = sns.heatmap(data_train_target_num.corr()[['target']].sort_values(by='target', ascending=False),\n                     vmin=-1, vmax=1, annot=True, cmap='Purples')\n\nheatmap.set_title('Linear correlation of features with target variable', fontdict={'fontsize': 18}, pad=16);","26035809":"pca = PCA().fit(data_train.drop('target', axis=1))\n\nfig = go.Figure()\n\nfig.add_trace(go.Scatter(\n    x = list(range(50)),\n    y = np.cumsum(pca.explained_variance_ratio_)\n))\n\nfig.update_layout(\n    template = 'plotly_dark',\n    title_text = 'PCA Performence',\n    xaxis_title = 'Number of components',\n    yaxis_title = 'Cumulative explained variance'\n)","eebf5888":"pca_vis = PCA(3)\nprojected = pca_vis.fit_transform(data_train.drop('target', axis=1))","83325585":"df_vis = pd.DataFrame(projected, columns=['x', 'y', 'z'])\ndf_vis['target'] = data_train['target']","3b3d3a79":"fig = px.scatter_3d(df_vis, x='x', y='y', z='z', color='target')\n\n# tight layout\nfig.update_layout(\n    template='plotly_dark'\n)","a80c39d0":"sample_data_train = data_train.sample(1000, random_state=42)\nscaled_sample_train = pd.DataFrame(StandardScaler().fit_transform(sample_data_train.drop('target', axis=1)))\nscaled_sample_target = sample_data_train.replace({'target': {'Class_1': 1, 'Class_2': 2, 'Class_3': 3, 'Class_4': 4}})['target'].reset_index(drop=True)","5d7c2751":"reducer_2d = umap.UMAP(random_state=1)\nembedding_2d = reducer_2d.fit_transform(scaled_sample_train, scaled_sample_target)","d0ce7e79":"df_test_2d = pd.DataFrame(embedding_2d, columns=['x', 'y'])\ndf_test_2d['target'] = scaled_sample_target","32aaf149":"fig = go.Figure()\n\nfig.add_trace(go.Scatter(\n    x = df_test_2d[df_test_2d['target'] == 1]['x'],\n    y = df_test_2d[df_test_2d['target'] == 1]['y'],\n    mode='markers',\n    name='Class_1'\n))\n\nfig.add_trace(go.Scatter(\n    x = df_test_2d[df_test_2d['target'] == 2]['x'],\n    y = df_test_2d[df_test_2d['target'] == 2]['y'],\n    mode='markers',\n    name='Class_2'\n))\n\nfig.add_trace(go.Scatter(\n    x = df_test_2d[df_test_2d['target'] == 3]['x'],\n    y = df_test_2d[df_test_2d['target'] == 3]['y'],\n    mode='markers',\n    name='Class_3'\n))\n\nfig.add_trace(go.Scatter(\n    x = df_test_2d[df_test_2d['target'] == 4]['x'],\n    y = df_test_2d[df_test_2d['target'] == 4]['y'],\n    mode='markers',\n    name='Class_4'\n))\n\nfig.update_layout(\n    title_text = '2d dataset visualization using UMAP',\n    template = 'plotly_dark'\n)","cbacfb6b":"reducer_3d = umap.UMAP(random_state=42, n_components=3)\nembedding_3d = reducer_3d.fit_transform(scaled_sample_train, scaled_sample_target)","d59dfef4":"df_test_3d = pd.DataFrame(embedding_3d, columns=['x', 'y', 'z'])\ndf_test_3d['target'] = scaled_sample_target","a63c1792":"fig = go.Figure()\n\nfig.add_trace(go.Scatter3d(\n    x = df_test_3d[df_test_3d['target'] == 1]['x'],\n    y = df_test_3d[df_test_3d['target'] == 1]['y'],\n    z = df_test_3d[df_test_3d['target'] == 1]['z'],\n    mode = 'markers',\n    name = 'Class_1',\n    marker = dict(\n        size=4\n    )\n))\n\nfig.add_trace(go.Scatter3d(\n    x = df_test_3d[df_test_3d['target'] == 2]['x'],\n    y = df_test_3d[df_test_3d['target'] == 2]['y'],\n    z = df_test_3d[df_test_3d['target'] == 2]['z'],\n    mode = 'markers',\n    name = 'Class_2',\n    marker = dict(\n        size=4\n    )\n))\n\nfig.add_trace(go.Scatter3d(\n    x = df_test_3d[df_test_3d['target'] == 3]['x'],\n    y = df_test_3d[df_test_3d['target'] == 3]['y'],\n    z = df_test_3d[df_test_3d['target'] == 3]['z'],\n    mode = 'markers',\n    name = 'Class_3',\n    marker = dict(\n        size=4\n    )\n))\n\nfig.add_trace(go.Scatter3d(\n    x = df_test_3d[df_test_3d['target'] == 4]['x'],\n    y = df_test_3d[df_test_3d['target'] == 4]['y'],\n    z = df_test_3d[df_test_3d['target'] == 4]['z'],\n    mode = 'markers',\n    name = 'Class_4',\n    marker = dict(\n        size=4\n    )\n))\n\nfig.update_layout(\n    title_text = '3d dataset visualization using UMAP',\n    template = 'plotly_dark'\n)","37f2aaca":"data_train_num = data_train\n\nX = data_train_num.drop('target', axis=1)\ny = data_train_num['target']","95218990":"log_pred = np.zeros((len(X), 4))\ntest_pred = np.zeros((len(data_test), 4))","456ffe27":"xgb_model = XGBClassifier()","b91adff1":"%%time\n\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\nfor fold_, (train_index, val_index) in enumerate(skf.split(X, y)):\n    print('Fold: ', fold_)\n    model = xgb_model.fit(\n        X.iloc[train_index],\n        y.iloc[train_index],\n        eval_set = [(X.iloc[train_index], y.iloc[train_index]), (X.iloc[val_index], y.iloc[val_index])],\n        eval_metric = 'mlogloss',\n        early_stopping_rounds = 50, \n        verbose = 0\n    )\n\n    temp_pred = model.predict_proba(X.iloc[val_index])\n    log_pred[val_index] = temp_pred\n\n    print(f'Log Loss: {log_loss(y.iloc[val_index], temp_pred)}')\n\n    temp_test = model.predict_proba(data_test)\n    test_pred += temp_test\n\ntest_pred1 = test_pred\/5\n\nprint(f'Overall Log Loss: {log_loss(y, log_pred)}')","7b931455":"log_pred = np.zeros((len(X), 4))\ntest_pred = np.zeros((len(data_test), 4))","bc635f0e":"lg_model = LGBMClassifier()","ac0b5541":"%%time\n\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\nfor fold_, (train_index, val_index) in enumerate(skf.split(X, y)):\n    print('Fold: ', fold_)\n    model = lg_model.fit(\n        X.iloc[train_index],\n        y.iloc[train_index],\n        eval_set = [(X.iloc[train_index], y.iloc[train_index]), (X.iloc[val_index], y.iloc[val_index])],\n        eval_metric = 'multi_logloss',\n        early_stopping_rounds = 50,\n        verbose = 0\n    )\n\n    temp_pred = model.predict_proba(X.iloc[val_index])\n    log_pred[val_index] = temp_pred\n\n    print(f'Log Loss: {log_loss(y.iloc[val_index], temp_pred)}')\n\n    temp_test = model.predict_proba(data_test)\n    test_pred += temp_test\n\ntest_pred2 = test_pred\/5\n\nprint(f'Overall Log Loss: {log_loss(y, log_pred)}')","5e6cf08e":"log_pred = np.zeros((len(X), 4))\ntest_pred = np.zeros((len(data_test), 4))","79e0118f":"cat_model = CatBoostClassifier()","493e4b82":"%%time\n\nskf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n\nfor fold_, (train_index, val_index) in enumerate(skf.split(X, y)):\n    print('Fold: ', fold_)\n    model = cat_model.fit(\n        X.iloc[train_index],\n        y.iloc[train_index],\n        eval_set = [(X.iloc[train_index], y.iloc[train_index]), (X.iloc[val_index], y.iloc[val_index])],\n        early_stopping_rounds = 50,\n        verbose=0\n\n    )\n\n    temp_pred = model.predict_proba(X.iloc[val_index])\n    log_pred[val_index] = temp_pred\n\n    print(f'Log Loss: {log_loss(y.iloc[val_index], temp_pred)}')\n\n    temp_test = model.predict_proba(data_test)\n    test_pred += temp_test\n\ntest_pred3 = test_pred\/10\n\nprint(f'Overall Log Loss: {log_loss(y, log_pred)}')","29c2e014":"df_pred1 = pd.DataFrame(test_pred1)\ndf_pred2 = pd.DataFrame(test_pred2)\ndf_pred3 = pd.DataFrame(test_pred3)\ndata_sub = pd.read_csv('..\/input\/tabular-playground-series-may-2021\/sample_submission.csv').drop(['Class_1', 'Class_2', 'Class_3', 'Class_4'], axis=1)","8fff5363":"data_sub1 = data_sub.copy()\n\ndata_sub1['Class_1'] = df_pred1[0]\ndata_sub1['Class_2'] = df_pred1[1]\ndata_sub1['Class_3'] = df_pred1[2]\ndata_sub1['Class_4'] = df_pred1[3]","c0e3bfb6":"data_sub2 = data_sub.copy()\n\ndata_sub2['Class_1'] = df_pred2[0]\ndata_sub2['Class_2'] = df_pred2[1]\ndata_sub2['Class_3'] = df_pred2[2]\ndata_sub2['Class_4'] = df_pred2[3]","17f3a47f":"data_sub3 = data_sub.copy()\n\ndata_sub3['Class_1'] = df_pred3[0]\ndata_sub3['Class_2'] = df_pred3[1]\ndata_sub3['Class_3'] = df_pred3[2]\ndata_sub3['Class_4'] = df_pred3[3]","84947c23":"data_sub1","4d7146e7":"data_sub2","6d784eb7":"data_sub3","9fc721b1":"data_sub3.to_csv('submission.csv', index=False)","5038c1b5":"<h3> Catboost <\/h3>","23da17df":"<h3>Conclusion<\/h3>","97184f08":"<h3>Feature Description<\/h3>","14f5d14c":"<h3> Light Gradient Boost <\/h3>","4fa95e28":"As we can see there aren't any missing values in this dataset","1d3a96d2":"As we can see from scatter plot above variance decreasing quite fast. By the time PCA reduce number of features to the 30 we had lost almost 10% of the variance. It's definitely not worth it to reduce dimensionality of this dataset in order to create prediction model but still we can use dimensionality reduction to visualize our dataset. ","cdf687a7":"<h3>Features Distribution<\/h3>","5e2a91e5":"<h2> Submission <\/h2>","9512970e":"PCA doesn't work very well but it doesn't mean that visualization is impossible. We gonna use other method to do so","07cf6052":"<h3>Target Distribution<\/h3>","1696855b":"There is a lot of zero values in every feature. I'm curious how much of dataset is filled with them.","b5044095":"<h3> XGBoost <\/h3>","5bbb9374":"<h3>Dimensionality reduction using PCA <\/h3>","d888d0c3":"<h2>EDA<\/h2>","f7ade223":"Hello everyone! This is my solution for this month tabular playground. I learned a lot during my work on this dataset and from notebooks of other participants. Those two kernels were most helpfull and informative. Don't forget to check them too :D <br>\n[TPS-May Categorical EDA](https:\/\/www.kaggle.com\/subinium\/tps-may-categorical-eda) <br>\n[TPS May: RAPIDS](https:\/\/www.kaggle.com\/ruchi798\/tps-may-rapids)\n","ead2993c":"Now our visualization looks much better, we can clearly see clouds of different classes. ","614481e4":"<h2>Dimensionality Reduction<\/h2>\nThere is 50 features in our dataset. It's good opportunity to perform dimensionality reduction but first we gonna check if it's necessary to do so.","2e285343":"<h2> Prediction model creation <\/h2>","010c4685":"<h3>Missing values<\/h3>","f405e724":"<h3> Dimensionality reduction using umap <\/h3>","11377ada":"Unfortunately we have huge disbalance in our target variable","c9da870b":"After some visualization and discussion couple of things come up to the light\n<ul>\n    <li>There aren't any missing values<\/li>\n    <li>Mean and standard deviation is fairly the same for train and test datasets<\/li>\n    <li>Target variable is disbalanced which can be a problem<\/li>\n    <li>Features are left skewed and nearly 60% of every feature is filled with zeros<\/li>\n    <li>Features show weak linear correlation with target variable<\/li>\n<\/ul>","acac77ae":"<h3>Correlation<\/h3>"}}