{"cell_type":{"4492239c":"code","531779e1":"code","40ff3631":"code","55eca0b6":"code","f9b18f21":"code","87c47032":"code","d9cb07b5":"code","420afc36":"code","16dd9019":"code","4cfed63b":"code","9c4be4b1":"code","812b74de":"code","d7f9d0cc":"code","35b8f6db":"code","0671b1a1":"code","31cf8697":"code","2595597b":"code","942d9782":"code","41933d16":"code","d3a946c0":"code","7501aa58":"code","b896b2dd":"markdown","7faaade1":"markdown","dee7c321":"markdown","3caec1bd":"markdown","c47da7f3":"markdown","28685152":"markdown","7ca37d02":"markdown","448e8bd1":"markdown"},"source":{"4492239c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","531779e1":"import tensorflow as tf\nfrom tensorflow.keras import layers, Sequential\nimport matplotlib.pyplot as plt","40ff3631":"LENGTH = 64\nDATA_SET_SIZE = 64 * 1024\nTEST_SET_SIZE = 1024\n\nBATCH_SIZE = 64\nEPOCHS = 100\nZ_DIM = 8","55eca0b6":"# Generating a data set\ndef generate_straight_lines(n):\n    A = tf.random.uniform((n,1), minval=-5, maxval=5)\n    B = tf.random.uniform((n,1), minval=-50, maxval=50)\n    E = tf.random.uniform((n, LENGTH), minval=-3, maxval=3)\n    \n    X = tf.range(0 - LENGTH \/\/ 2, LENGTH \/\/ 2, dtype=float)\n\n    return A * X + B + E","f9b18f21":"# Random noise for generator\ndef generate_noise(n):\n    return tf.random.normal((n, Z_DIM))\n\n# Applying truncation trick (picking values from the center of the normal distribution)\n# Because you have a higher change that the generator was well trained on those values, compared to far off values. \ndef generate_truncated_noise(n):\n    return tf.random.truncated_normal((n, Z_DIM))","87c47032":"def create_generator():\n    gen = Sequential()\n    \n    gen.add(layers.Input((Z_DIM,)))\n    gen.add(layers.Dense(LENGTH * 8))\n    gen.add(layers.Reshape((LENGTH \/\/ 8, 8 * 8)))\n    \n    gen.add(create_gen_block(128, 3))\n    gen.add(create_gen_block(64, 5))\n    gen.add(create_gen_block(32, 7))\n    \n    gen.add(layers.Conv1D(1, 9, activation='tanh', padding='same'))\n    gen.add(layers.Flatten())\n    \n    return gen\n\ndef create_gen_block(hidden, kernel):\n    block = Sequential()\n    \n    block.add(layers.Conv1DTranspose(hidden, kernel, strides=2, padding='same'))\n    block.add(layers.LeakyReLU())\n    block.add(layers.BatchNormalization())\n    block.add(layers.Conv1D(hidden, kernel, strides=1, padding='same'))\n    block.add(layers.LeakyReLU())\n    block.add(layers.BatchNormalization())\n    \n    return block\n\ncreate_generator().summary()","d9cb07b5":"def create_critic():\n    critic = Sequential()\n    \n    critic.add(layers.Input((LENGTH,)))\n    critic.add(layers.Reshape((LENGTH,1)))\n    critic.add(create_critic_block(16, 3))\n    critic.add(create_critic_block(32, 5))\n    critic.add(create_critic_block(64, 7))\n    critic.add(layers.Flatten())\n    critic.add(layers.Dense(1, activation='sigmoid'))\n    \n    return critic\n\ndef create_critic_block(hidden, kernel):\n    block = Sequential()\n    \n    block.add(layers.Conv1D(hidden, kernel, strides=2, padding='same'))\n    block.add(layers.LeakyReLU())\n    block.add(layers.Dropout(0.2))\n    block.add(layers.Conv1D(hidden, kernel, padding='same'))\n    block.add(layers.LeakyReLU())\n    block.add(layers.Dropout(0.2))\n    \n    return block\n\ncreate_critic().summary()","420afc36":"def scale_lines(lines):\n    max_value = tf.math.reduce_max(lines)\n    min_value = tf.math.reduce_min(lines)\n    \n    return ((lines - min_value) \/ (max_value - min_value)) * 2 - 1, min_value, max_value\n\ndef unscale_lines(lines, min_value, max_value):\n    min_max_range = max_value - min_value\n    return ((lines + 1) \/ 2) * min_max_range + min_value\n\n\nplt.plot(scale_lines(generate_straight_lines(1))[0][0])","16dd9019":"X_train, min_value, max_value = scale_lines(generate_straight_lines(DATA_SET_SIZE))\nprint(X_train.shape)","4cfed63b":"# Test sets\nX_test = scale_lines(generate_straight_lines(TEST_SET_SIZE))[0]\nZ_test = generate_noise(TEST_SET_SIZE)\n\nprint(X_test.shape)\nprint(Z_test.shape)","9c4be4b1":"cross_entropy = tf.keras.losses.BinaryCrossentropy()\n\ndef discriminator_loss(real_output, fake_output):\n    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n    total_loss = real_loss + fake_loss\n    return total_loss\n\ndef generator_loss(fake_output):\n    return cross_entropy(tf.ones_like(fake_output), fake_output)\n","812b74de":"@tf.function\ndef train_step(batch):\n    noise = generate_noise(BATCH_SIZE)\n\n    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n        generated = generator(noise, training=True)\n\n        real_output = discriminator(batch, training=True)\n        fake_output = discriminator(generated, training=True)\n\n        gen_loss = generator_loss(fake_output)\n        disc_loss = discriminator_loss(real_output, fake_output)\n\n    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n\n    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n","d7f9d0cc":"import time\n\ndef train(dataset, epochs):\n    for epoch in range(epochs):\n        start = time.time()\n\n        for batch in dataset:\n            train_step(batch)\n\n        real_output = discriminator(X_test)\n        test_generated = generator(Z_test)\n        fake_output = discriminator(test_generated)\n        \n        if epoch % 5 == 0:\n            checkpoint.save(file_prefix = checkpoint_prefix)\n            for i in range(5):\n                plt.plot(unscale_lines(test_generated[i], min_value, max_value))\n            plt.show()\n\n        \n        print('Epoch: %d\\tDisc: %.3f\\tGen: %.3f' % (\n            epoch + 1,\n            discriminator_loss(real_output, fake_output), \n            generator_loss(fake_output)\n        ))\n        print ('Time for epoch %d is %.2f sec' % (epoch + 1, time.time()-start))\n","35b8f6db":"discriminator = create_critic()\ngenerator_optimizer = tf.keras.optimizers.Adam(1e-4)\n\ngenerator = create_generator()\ndiscriminator_optimizer = tf.keras.optimizers.Adam(1e-4)","0671b1a1":"checkpoint_dir = '.\/training_checkpoints'\ncheckpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\ncheckpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n                                 discriminator_optimizer=discriminator_optimizer,\n                                 generator=generator,\n                                 discriminator=discriminator)","31cf8697":"print(tf.squeeze(discriminator(generate_straight_lines(10)), -1).numpy())\nprint(tf.squeeze(discriminator(generate_straight_lines(10)), -1).numpy())","2595597b":"dataset = tf.data.Dataset.from_tensor_slices(X_train).batch(BATCH_SIZE)\ntrain(dataset, EPOCHS)","942d9782":"rows = 3\ncols = 4\nnum_lines = 5\n\nfig, ax = plt.subplots(rows, cols)\nfor r in range(rows):\n    for c in range(cols):\n        gen_lines = unscale_lines(generator(generate_truncated_noise(num_lines)), min_value, max_value)\n        for line in gen_lines:\n            ax[r, c].plot(line)","41933d16":"for i in range(4):\n    plt.plot(X_train[i])","d3a946c0":"for i in range(4):\n    plt.plot(X_train[-i-1])","7501aa58":"plt.plot(unscale_lines(generator(generate_truncated_noise(1))[0], min_value, max_value))","b896b2dd":"# Data Set","7faaade1":"# Plots","dee7c321":"# Training Loop","3caec1bd":"# Networks\n\nNetwork structure is inspired by DCGAN, meaning both the generator and discriminator\/critic are built up from convolutional layers. \n\n## Generator\nThe generator starts by generating a structure of 1\/8th the line length and 32 channels. This data is upsampled with transposed convolutions to the desired length LENGTH. Each block of the generator consists of a transposed convolution for upsampling, with a LeakyReLU activation and a BatchNormalization. For \"smoothing\" the transposed convolution, an additional convolutional layer with LeakyReLU and BatchNormalization is added. \n\n## Discriminator\/Critic\nThe critic gets three blocks of convolutional blocks. Each block consists of two convolutional layers, LeakyReLU activation and a dropout. \n\nNote that the critic has a lot less parameters to train hence is a weaker network. The critic should be weaker, otherwise it will too quickly pass values close to zero to the generator. Then there is little to no gradient for the generator to learn from. Even more so because the gradient has to pass through the critic and the generator to reach the first layers of the generator. ","c47da7f3":"# Data Generation\n\nGenerating the real data. Lines with a bit of noise. A function to generate random noise for the generator. ","28685152":"# Loss","7ca37d02":"> # DCGAN\n\nGenerate straight lines with some noise. \n\nBased on [Deep Convolutional Generative Adversarial Network tutorial](https:\/\/www.tensorflow.org\/tutorials\/generative\/dcgan)","448e8bd1":"# Training"}}