{"cell_type":{"bbca1cd6":"code","299e8fdb":"code","829730a3":"code","c0db7552":"code","390dbdcc":"code","5f2e58dd":"code","465d33b5":"code","42d275b4":"code","59c058d0":"code","e1b5edb4":"code","f6c9afc5":"code","67ea57bb":"code","0402fde9":"code","0598ab79":"code","879c073c":"code","1dbaee71":"code","d9b6360b":"code","c703d1d3":"code","09d1bd5c":"code","310d2526":"code","8a6a1867":"code","9a65a837":"code","373e1d2b":"code","101f6252":"code","3f25d7dc":"code","38605700":"code","5acc1e4b":"code","a987af08":"code","82c6248c":"code","f58c15f3":"code","d716bc07":"code","4a4fa9e7":"code","be5a07ec":"code","11518aa4":"code","2fba68d6":"code","824811bb":"code","013343f4":"code","953c15f9":"code","a1ca15ba":"code","b60ff1e9":"code","4b10ff0f":"code","8a711e65":"code","b6783ee4":"code","568f374b":"code","b5b0a44a":"code","c764c5d7":"code","bd8c16d1":"markdown","eea0c10f":"markdown","8adceaef":"markdown","d12f2b31":"markdown","1e82e14b":"markdown","b11d834d":"markdown","cc03ecbe":"markdown","ed37514d":"markdown","0371afe3":"markdown","81b2ebda":"markdown","006835b8":"markdown","d7951924":"markdown","faddfdbb":"markdown","f2f542d9":"markdown","142a426a":"markdown","f932b8e5":"markdown","9b63aa25":"markdown","460f139c":"markdown","f6d04371":"markdown"},"source":{"bbca1cd6":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport missingno as msno\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n%matplotlib inline\n# Any results you write to the current directory are saved as output.","299e8fdb":"loan = pd.read_csv(\"..\/input\/lending-club-loan-data\/loan.csv\")\npop = pd.read_csv(\"..\/input\/population\/population_us.csv\")\nloan.head()\n","829730a3":"pop.head(5)","c0db7552":"loan.shape","390dbdcc":"loan.isnull().sum()","5f2e58dd":"print(loan.isnull().any().value_counts(), \"\\n\")\nprint(f\"The columns that have missing values are total {loan.isnull().any().sum()}\")","465d33b5":"total_num = loan.isnull().sum().sort_values(ascending=False)\nperc = loan.isnull().sum()\/loan.isnull().count() *100\n#perc1 = (round(perc,2).sort_values(ascending=False))\n\n# Creating a data frame:\ndf_miss = pd.concat([total_num, perc], axis =1 , keys =[\"Total Missing Values\", \"Percentage %\"]).sort_values(by =\"Percentage %\", ascending = False)\n\ntop_mis = df_miss[df_miss[\"Percentage %\"]>40]\ntop_mis.reset_index(inplace=True)\ntop_mis","42d275b4":"list_to_drop = top_mis['index']\nloan_copy = loan\nloan_copy = loan_copy.drop(list_to_drop,axis=1)","59c058d0":"loan_copy.shape","e1b5edb4":"print(loan_copy.isnull().any().value_counts(), \"\\n\")\nprint(f\"The columns that have missing values are total {loan_copy.isnull().any().sum()}\")","f6c9afc5":"total_num = loan_copy.isnull().sum().sort_values(ascending=False)\nperc = loan_copy.isnull().sum()\/loan_copy.isnull().count() *100\nperc1 = (round(perc,2).sort_values(ascending=False))\n\n# Creating a data frame:\ndf_miss_copy = pd.concat([total_num, perc], axis =1 , keys =[\"Total Missing Values\", \"Percentage %\"]).sort_values(by =\"Percentage %\", ascending = False)\n\ntop_mis_copy = df_miss_copy[df_miss_copy[\"Percentage %\"]>0]\ntop_mis_copy.reset_index(inplace=True)\ntop_mis_copy","67ea57bb":"list_drop_rows = list(top_mis_copy['index'][49:])\nlist_drop_rows","0402fde9":"loan_copy = loan_copy.dropna(axis=0,subset=list_drop_rows)","0598ab79":"print(\"The dimension of the dataset is {}\" .format(loan_copy.shape))\nprint(loan_copy.isnull().any().value_counts(), \"\\n\")\nprint(f\"The columns that have missing values are total {loan_copy.isnull().any().sum()}\")","879c073c":"%%time\n\nlist_object = []\nfor i in loan_copy.dtypes.index:  \n    if(loan_copy.dtypes[i] == \"object\"):\n        print(i + \" \", loan_copy.dtypes[i]) \n        list_object.append(i)\n       \nloan_object = loan_copy[list_object]\n\n##Print the names of the columns with percentage with object datatype and having missing data\n\ntotal_num = loan_object.isnull().sum().sort_values(ascending=False)\nperc = loan_object.isnull().sum()\/loan_object.isnull().count() *100\nperc1 = (round(perc,2).sort_values(ascending=False))\n\n# Creating a data frame:\ndf_miss_copy_object = pd.concat([total_num, perc1], axis =1 , keys =[\"Total Missing Values\", \"Percentage %\"]).sort_values(by =\"Percentage %\", ascending = False)\n\ntop_mis_copy_object = df_miss_copy_object[df_miss_copy_object[\"Percentage %\"]>0]\ntop_mis_copy_object.reset_index(inplace=True)\ntop_mis_copy_object","1dbaee71":"loan_copy[\"emp_title\"]= loan_copy[\"emp_title\"].fillna(loan_copy[\"emp_title\"].mode()[0])\nloan_copy[\"emp_length\"]= loan_copy[\"emp_length\"].fillna(loan_copy[\"emp_length\"].mode()[0])\nloan_copy[\"title\"]= loan_copy[\"title\"].fillna(loan_copy[\"title\"].mode()[0])","d9b6360b":"print(\"The dimension of the dataset is {}\" .format(loan_copy.shape))\nprint(loan_copy.isnull().any().value_counts(), \"\\n\")\nprint(f\"The columns that have missing values are total {loan_copy.isnull().any().sum()}\")","c703d1d3":"list_object = []\nfor i in loan_copy.dtypes.index:  \n    if(loan_copy.dtypes[i] == \"int64\"):\n        print(i + \" \", loan_copy.dtypes[i]) \n        list_object.append(i)\nlist_object\n\nprint(loan_copy[list_object].isnull().sum())","09d1bd5c":"list_object_float = []\nfor i in loan_copy.dtypes.index:  \n    if(loan_copy.dtypes[i] == \"float64\"):\n        print(i + \" \", loan_copy.dtypes[i]) \n        list_object_float.append(i)\n       \nloan_object_float = loan_copy[list_object_float]\nprint(loan_object_float.isnull().sum())\n\n##Print the names of the columns with percentage with object datatype and having missing data\n\ntotal_num = loan_object_float.isnull().sum().sort_values(ascending=False)\nperc = loan_object_float.isnull().sum()\/loan_object_float.isnull().count() *100\n#perc1 = (round(perc,2).sort_values(ascending=False))\n\n# Creating a data frame:\ndf_miss_copy_float = pd.concat([total_num, perc], axis =1 , keys =[\"Total Missing Values\", \"Percentage %\"]).sort_values(by =\"Percentage %\", ascending = False)\n\ntop_mis_copy_float = df_miss_copy_float[df_miss[\"Percentage %\"]>0]\ntop_mis_copy_float.reset_index(inplace=True)\ntop_mis_copy_float","310d2526":"list_fill_rows = list(top_mis_copy_float['index'][13:46])\nlist_fill_rows","8a6a1867":"for i in list_fill_rows:\n    print(\"For column {0} mode is {1}\".format(i,loan_copy[i].mode()[0]))\n    print(\"The unique values in column {} are {}  \".format(i,loan_copy[i].value_counts()))","9a65a837":"list_of_median_impute = ['mths_since_recent_bc','num_rev_accts','num_op_rev_tl','num_rev_tl_bal_gt_0','num_tl_op_past_12m','num_bc_tl','tot_hi_cred_lim','num_il_tl','total_rev_hi_lim','avg_cur_bal','num_actv_bc_tl','mo_sin_rcnt_tl','mo_sin_rcnt_rev_tl_op','mo_sin_old_rev_tl_op',\n'num_actv_rev_tl','num_bc_sats','num_sats','acc_open_past_24mths','mort_acc','total_bal_ex_mort','total_bc_limit']","373e1d2b":"list_of_mode_impute = set(list_fill_rows) - set(list_of_median_impute)\nlist_of_mode_impute= list(list_of_mode_impute)\nlist_of_mode_impute","101f6252":"for i in list_of_mode_impute:\n    loan_copy[i]= loan_copy[i].fillna(loan_copy[i].mode()[0])","3f25d7dc":"for i in list_of_median_impute:\n    loan_copy[i]= loan_copy[i].fillna(loan_copy[i].median())","38605700":"print(\"The dimension of the dataset is {}\" .format(loan_copy.shape))\nprint(loan_copy.isnull().any().value_counts(), \"\\n\")\nprint(f\"The columns that have missing values are total {loan_copy.isnull().any().sum()}\")","5acc1e4b":"total_num = loan_copy.isnull().sum().sort_values(ascending=False)\nperc = loan_copy.isnull().sum()\/loan_copy.isnull().count() *100\nperc1 = (round(perc,2).sort_values(ascending=False))\n\n# Creating a data frame:\ndf_miss_co = pd.concat([total_num, perc1], axis =1 , keys =[\"Total Missing Values\", \"Percentage %\"]).sort_values(by =\"Percentage %\", ascending = False)\n\ntop_mis_co = df_miss_co[df_miss[\"Percentage %\"]>0]\ntop_mis_co.reset_index(inplace=True)\ntop_mis_co","a987af08":"last_list_of_empty  = top_mis_co['index'][0:13]\nlast_list_of_empty","82c6248c":"for i in last_list_of_empty:\n    print(\"For column {0} mode is {1}\".format(i,loan_copy[i].mode()[0]))\n    print(\"The unique values in column {} are {}  \".format(i,loan_copy[i].value_counts()))","f58c15f3":"last_list_of_median_impute = ['inq_fi','open_acc_6m','open_rv_24m','open_rv_12m','open_il_24m','open_act_il','total_cu_tl','open_il_12m',\n                              'inq_last_12m','all_util','mths_since_recent_inq']","d716bc07":"last_set_of_mode_impute = set(last_list_of_empty) - set(last_list_of_median_impute)\nlast_list_of_mode_impute = list(last_set_of_mode_impute)\nlast_list_of_mode_impute","4a4fa9e7":"for i in last_list_of_median_impute:\n    loan_copy[i]= loan_copy[i].fillna(loan_copy[i].median())\n\nfor i in last_list_of_mode_impute:\n    loan_copy[i]= loan_copy[i].fillna(loan_copy[i].mode()[0])\n","be5a07ec":"print(\"The dimension of the dataset is {}\" .format(loan_copy.shape))\nprint(loan_copy.isnull().any().value_counts(), \"\\n\")\nprint(f\"The columns that have missing values are total {loan_copy.isnull().any().sum()}\")","11518aa4":"loan_copy.head(5)","2fba68d6":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(1, 3, figsize=(16,5))\n\nloan_amount = loan_copy[\"loan_amnt\"].values\nfunded_amount = loan_copy[\"funded_amnt\"].values\ninvestor_funds = loan_copy[\"funded_amnt_inv\"].values\n\n\nsns.distplot(loan_amount, ax=ax[0], color=\"#F7522F\")\nax[0].set_title(\"Loan Applied by the Borrower\", fontsize=14)\nsns.distplot(funded_amount, ax=ax[1], color=\"#2F8FF7\")\nax[1].set_title(\"Amount Funded by the Lender\", fontsize=14)\nsns.distplot(investor_funds, ax=ax[2], color=\"#2EAD46\")\nax[2].set_title(\"Total committed by Investors\", fontsize=14)","824811bb":"m = loan_copy['loan_status'].value_counts()\nm = pd.DataFrame(m)\nm.reset_index(level=0, inplace=True)\n\nm['index'][6] = \"DNMCP Fully Paid\"\nm['index'][7] = \"DNMCP Charged Off\"\nm\nm.columns = ['Loan Status','Count']\nplt.subplots(figsize=(20,8))\nsns.barplot(y='Count', x='Loan Status', data=m)\nplt.xlabel(\"Length\")\nplt.ylabel(\"Count\")\nplt.title(\"Distribution of Loan Status in our Dataset\")\nplt.show()","013343f4":"plt.figure(figsize=(12,6))\n\nplt.subplot(121)\ng = sns.distplot(loan_copy[\"loan_amnt\"])\ng.set_xlabel(\"\", fontsize=12)\ng.set_ylabel(\"Frequency Dist\", fontsize=12)\ng.set_title(\"Frequency Distribuition\", fontsize=20)\n\nplt.subplot(122)\ng1 = sns.violinplot(y=\"loan_amnt\", data=loan_copy, \n               inner=\"quartile\", palette=\"hls\")\ng1.set_xlabel(\"\", fontsize=12)\ng1.set_ylabel(\"Amount Dist\", fontsize=12)\ng1.set_title(\"Amount Distribuition\", fontsize=20)\n\nplt.show()","953c15f9":"loan_copy['int_round'] = loan_copy['int_rate'].round(0).astype(int)\nplt.figure(figsize = (10,8))\n\n#Exploring the Int_rate\nplt.subplot(211)\ng = sns.distplot(np.log(loan_copy[\"int_rate\"]))\ng.set_xlabel(\"\", fontsize=12)\ng.set_ylabel(\"Distribuition\", fontsize=12)\ng.set_title(\"Int Rate Log distribuition\", fontsize=20)\n\nplt.subplot(212)\ng1 = sns.countplot(x=\"int_round\",data=loan_copy, \n                   palette=\"Set1\")\ng1.set_xlabel(\"Int Rate\", fontsize=12)\ng1.set_ylabel(\"Count\", fontsize=12)\ng1.set_title(\"Int Rate Normal Distribuition\", fontsize=20)\n\nplt.subplots_adjust(wspace = 0.2, hspace = 0.6,top = 0.9)\n\nplt.show()","a1ca15ba":"plt.figure(figsize = (14,6))\n#Looking the count of defaults though the issue_d that is The month which the loan was funded\ng = sns.countplot(x='issue_d', data=loan_copy[loan_copy['loan_status'] =='Default'])\ng.set_xticklabels(g.get_xticklabels(),rotation=90)\ng.set_xlabel(\"Dates\", fontsize=15)\ng.set_ylabel(\"Count\", fontsize=15)\ng.legend(loc='upper left')\ng.set_title(\"Analysing Defaults Count by Time\", fontsize=20)\nplt.show()","b60ff1e9":"loan_copy['verified'] = loan_copy['verification_status'] == 'Verified'\ngrade_yr_loanamnt = pd.pivot_table(loan_copy,index=[\"grade\",\"verified\"], values=['loan_amnt'], aggfunc=np.sum)\n\ngrade_yr_loanamnt_default = pd.pivot_table(loan_copy[(loan_copy.loan_status == 'Charged Off') | (loan_copy.loan_status == 'Default')],\n                                           index=[\"grade\",\"verified\"], values=['loan_amnt'], aggfunc=np.sum)\n\ngrade_yr_loanamnt_default.columns = ['Charged_off']\n\nloan_verified = pd.merge(grade_yr_loanamnt, grade_yr_loanamnt_default, left_index = True, right_index = True)\nloan_verified['chargeoff_rate']  = loan_verified['Charged_off'] \/  loan_verified['loan_amnt'] \n\nloan_verified_unstack = loan_verified.unstack(\"verified\")\nverified_chargedoff = loan_verified_unstack['chargeoff_rate']\nverified_chargedoff.plot()","4b10ff0f":"import plotly.plotly as py\nimport plotly.graph_objs as go\nfrom plotly import tools\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\ninit_notebook_mode(connected=True)","8a711e65":"\n# create loan amount by state column\nloan_amnt_by_state = loan_copy.groupby([\"addr_state\"]).sum()[\"loan_amnt\"]\ndf_region = loan_amnt_by_state.to_frame()\ndf_region[\"loan_amnt\"] = df_region[\"loan_amnt\"].map(\"{:,.0f}\".format)\ntemp = []\nfor x in df_region[\"loan_amnt\"]:\n    a = int(x.replace(',', ''))\n    temp.append(a)\ndf_region[\"loan_amnt\"] = temp\n\n# create number of loan issued by state column\nnum_issued_loan = loan_copy.groupby([\"addr_state\"]).count()[\"loan_amnt\"]\ndf_region[\"num_issued\"] = num_issued_loan\n\n# create average loan amount column\navg_loan_amnt_by_state = []\nfor a,b in zip(df_region[\"loan_amnt\"], df_region[\"num_issued\"]):\n    temp = int(a\/b)\n    avg_loan_amnt_by_state.append(temp)\ndf_region[\"avg_loan_amnt_by_state\"] = avg_loan_amnt_by_state\n\n","b6783ee4":"df_region_copy = df_region.copy()\naddr_state = df_region_copy.index\ndf_region.index = list(range(1,52))\ndf_region[\"addr_state\"] = addr_state","568f374b":"# population by states from http:\/\/worldpopulationreview.com\/states\/\n\npop.index = list(range(1,53))\ndf_region[\"population\"] = pop['Pop']","b5b0a44a":"dti = loan_copy.groupby(\"addr_state\").agg([np.mean])[\"dti\"]\ndti.columns = [\"dti\"]\nlen(dti)","c764c5d7":"d = loan_copy[loan_copy[\"loan_status\"].isin([\"Late (16-30 days)\",\"Late (31-120 days)\",\"Default\", \"Charged Off\", \"Does not meet the credit policy. Status:Charged Off\"])].groupby(\"addr_state\").size()\nd = d.to_frame()\ne = pd.DataFrame([0],index=[\"ME\"])\nf = pd.concat([d,e])\nf_copy = f.copy()\naddr_state = f_copy.index\nf.index = list(range(1,53))\nf[\"addr_state\"] = addr_state\nf = f.sort_values(by=\"addr_state\")\nf.index = list(range(1,53))\ndf_region[\"num_default\"] = f[0]\n\n# create default_rate column\ntemp = []\nfor x, y in zip(df_region[\"num_default\"], df_region[\"num_issued\"].astype(int)):\n    if x is not 0 and y is not 0:\n        value = (x\/y)\n        value = \"{0:.2f}\".format(value)\n        value = float(value)\n        temp.append(value)\n    else:\n        temp.append(0)\ndf_region[\"default_rate\"] = temp\n\n# create average dti by the state\ndti = loan_copy.groupby(\"addr_state\").agg([np.mean])[\"dti\"]\ndti.columns = [\"dti\"]\ndti.index = list(range(1,52))\ndf_region = df_region.join(dti)\n# plotly color setting\nfor col in df_region.columns:\n    df_region[col] = df_region[col].astype(str)\n    scl = [[0.0, 'rgb(242,240,247)'],[0.2, 'rgb(218,218,235)'],[0.4, 'rgb(188,189,220)'],\\\n            [0.6, 'rgb(158,154,200)'],[0.8, 'rgb(117,107,177)'],[1.0, 'rgb(84,39,143)']]\n\n\n# create text column\ndf_region[\"text\"] = df_region[\"addr_state\"] + '<br>' + \\\n\"Population: \" + df_region[\"population\"] + '<br>' + \\\n\"Total loan amount ($ USD): \" + df_region[\"loan_amnt\"] + \"<br>\" + \\\n\"Avg loan amount ($ USD): \" + df_region[\"avg_loan_amnt_by_state\"] + '<br>' + \\\n\"Default rate: \" + df_region[\"default_rate\"] + \"<br>\" + \\\n\"DTI: \" + df_region[\"dti\"]\n\n# setting plotly and deploy the map\ndata = [ dict(\n        type='choropleth',\n        colorscale = scl,\n        autocolorscale = False,\n        locations = df_region['addr_state'],\n        z = df_region['avg_loan_amnt_by_state'], \n        locationmode = 'USA-states',\n        text = df_region['text'],\n        marker = dict(\n            line = dict (\n                color = 'rgb(255,255,255)',\n                width = 2\n            ) ),\n        colorbar = dict(\n            title = \"$s USD\")\n        ) ]\n\nlayout = dict(\n        title = 'Lending Club Loan<br> Average Loan By State',\n        geo = dict(\n            scope='usa',\n            projection=dict( type='albers usa' ),\n            showlakes = True,\n            lakecolor = 'rgb(255, 255, 255)'),\n             )\n    \nfig = dict( data=data, layout=layout )\niplot( fig, filename='d3-cloropleth-map' )\n","bd8c16d1":"**Lets start the treatment of missing values**\n\nSince we have too many columns, lets find the percentage of missing data in each column and print columns which has more that 40 percent missing data","eea0c10f":"Deleting the rows from columns which have less than <2500 missing data points ","8adceaef":"**Lets now understand the data:**\n\nOur dataset contains information about all loans issued between 2007 and 2015 including current loan status(Current, late, Fully Paid etc.). It also contains credit scores, number of finance inqueries, zip code, state, collections etc. Lets get Started...  ","d12f2b31":"Imputation has to be performed in different columns and the imputation will be performed in sets.\n","1e82e14b":"There are no missing values in these int type columns","b11d834d":"**Data Visualization**","cc03ecbe":"Due to lack of RAM in this kernel , the further details are available and continued with the dataset in the below link\n\nLink : https:\/\/www.kaggle.com\/shubhendra7\/lending-club-analysis\/","ed37514d":"D","0371afe3":"Reference : https:\/\/medium.com\/ibm-data-science-experience\/missing-data-conundrum-exploration-and-imputation-techniques-9f40abe0fd87\n\n**Imputation Techniques**\n\nMean, Median and Mode Imputation\n\nImputation with Regression\n\nk-Neareast Neighbor (kNN) Imputation\n\n\nAfter observing the various unique values in the missing columns, the kNN imputation seems to be the best. \nThe imputed values are obtained by using similarity-based methods that rely on distance metrics (Euclidean distance, Jaccard similarity, Minkowski norm etc). They can be used to predict both discrete and continuous attributes. The main disadvantage of using kNN imputation is that it becomes time-consuming when analyzing large datasets because it searches for similar instances through all the dataset ","81b2ebda":"Dropping these 45 columns from the dataset and will handle the remaining columns with missing data ","006835b8":"Mode and Median Imputations have been performed on the data on basis of the \"value_counts()\" function output","d7951924":"Based on the graph, we can safely reject the hypothesis. Loans that have verified income actually has a high level of charge-off. It is reasonable to assume that these loans are often associated with lower level of income, hence higher charge-off.\n\nThe graph also show that the charge-off rate changes almost linearly from grade A (highest grade) through grade G. The charge-off rate for grade F-G is approximately 30%. To compare, this is almost equal to the historic default rate of non-investment grade bond, which is greater than 30% (Wikipedia Credit Rating.)","faddfdbb":"![image.png](attachment:image.png)\n\n**Why do they need this analysis?**\n\nFrom above working model, it is clear that its very important for LendingClub to know if there is any chance of their borrowers defaulting.","f2f542d9":"![image.png](attachment:image.png)\n\n**A Brief about LendingClub: **\n\n**LendingClub** is the first US based peer to peer lending company, headquarter in SAN Francisco, California to register its offerings as securities and exchange commission. It offers loan trading on secondary market. LendingClub enables borrowers to create unsecured personal loans between 1000 and 40000 with standard loan period of 3 years. LendingClub acts like the \"bridge\" between borrowers and Investors.\n\n","142a426a":"Seaborn ","f932b8e5":"With this the data is clear and the imputations have been performed and the dataset is complete","9b63aa25":"We are left with 99 columns now","460f139c":"Added Population dataset from Kaggle which will help us perform EDA more comprehensively ","f6d04371":"**Charge off rate vs Verification status**\n\n\nMy hypothesis is that Lending Club tends to take the effort to verify a borrower's income only when it is high. \nTo quickly examine this hypothesis, I look at the charge off rate across each grade depending on whether the income is Verified or not.\n\nI define that a loan is considered charge-off when the value of loan_status is Charged Off or Default. I can expand it to 90 days behind dues, but it does not make that large of a difference."}}