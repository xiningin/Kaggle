{"cell_type":{"af6f43a7":"code","9054ba56":"code","60c30a50":"code","2d334b38":"code","9ba49ad8":"code","55611e80":"code","c91c11b3":"code","26990f8e":"code","b0c0484b":"code","69c3d8e5":"code","5e2a2f94":"code","a83dc53f":"code","39515daf":"code","bcb20ece":"code","49ea960c":"code","679b36d9":"code","9c9fb95e":"code","2b8c4697":"code","2fdc6b26":"code","e233e27c":"code","5eb0944c":"code","cc8508aa":"code","dedcf5da":"code","4dfe9de6":"code","ed5289cf":"code","f9920a57":"code","cfa66d31":"code","b7914a44":"code","f760d10a":"code","f23c360f":"code","f63db609":"code","561a8527":"code","85c50a7c":"code","f3106951":"code","13c12949":"code","14ffb457":"code","5a3fd5b5":"code","9d38d301":"code","52ac0863":"code","f5b6b560":"code","1e5ff924":"code","a956150c":"code","dcdd3a2a":"code","5dde7d5c":"code","72972e1b":"code","604a81f5":"code","cf78dd0c":"code","b4293c84":"code","cb64ead2":"code","6225bff7":"code","b10367d2":"code","0b5cb23b":"code","72219ecd":"code","d557e4aa":"code","898af46e":"code","993e4fb6":"code","9751a866":"code","477c6761":"code","6f67d40d":"code","3f8df9ef":"code","d3a28cd7":"code","37aa6838":"code","3df6e032":"code","a5178aab":"code","df4328ab":"markdown","e5e9cece":"markdown","aa55af90":"markdown","64142cfa":"markdown","ac43ea3d":"markdown","d76bfbd1":"markdown","e388b811":"markdown","9a130b7d":"markdown","4ebfea09":"markdown","f4662b60":"markdown","500463b8":"markdown","8a8992c2":"markdown","bf83244a":"markdown","92e70a99":"markdown","7280cdfd":"markdown","15bb9233":"markdown","f0587382":"markdown","67f0c215":"markdown","ecdc6504":"markdown","581e4ab3":"markdown","32aafcc2":"markdown","968ffd59":"markdown","2c1d5532":"markdown","f3d42b39":"markdown","fea6ff8d":"markdown","dc9ffd74":"markdown","7537bd2a":"markdown","66cc5fe1":"markdown","da118d71":"markdown","eaf8d711":"markdown","bcff86fc":"markdown","6a977386":"markdown","9c21890a":"markdown","7ea9ebcd":"markdown","afe1e5bb":"markdown","d4483488":"markdown","2e6a5bb9":"markdown","3bc0a6b9":"markdown","16cefb77":"markdown","da086e5f":"markdown","05014486":"markdown","1dd086d0":"markdown","d401f515":"markdown","cd546e15":"markdown","23835ece":"markdown","4a42758d":"markdown","9a967399":"markdown","24d874c2":"markdown"},"source":{"af6f43a7":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport missingno as msno\nfrom scipy import stats\nfrom sklearn.preprocessing import StandardScaler, RobustScaler\nfrom sklearn.base import BaseEstimator, RegressorMixin\nfrom sklearn.model_selection import cross_val_score, GridSearchCV\nfrom sklearn.linear_model import Ridge, Lasso, LassoCV, ElasticNet\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.cross_decomposition import PLSRegression\nfrom sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor, AdaBoostRegressor, GradientBoostingRegressor\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\nfrom mlxtend.regressor import StackingCVRegressor\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.metrics import mean_squared_error\nfrom datetime import datetime\n\nSEED = 42\n\n# Prevent Pandas from truncating displayed dataframes\npd.set_option('display.max_rows', None)\npd.set_option('display.max_columns', None)\n\nsns.set(style=\"white\", font_scale=1.2)\nplt.rcParams[\"figure.figsize\"] = [10,8]","9054ba56":"train_ = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/train.csv\")\ntest_ = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/test.csv\")\nsubmission = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv\")\n\n# Prepare our base training\/test data\ntrain = train_.copy().drop(columns=\"Id\")\ntest = test_.copy().drop(columns=\"Id\")","60c30a50":"train.head()","2d334b38":"def rmse_cv(model, X, y):\n    \"\"\"Return the cross validated RMSE scores for a given model, training data and target data.\"\"\"\n    rmse = np.sqrt(-cross_val_score(model, X, y, scoring=\"neg_mean_squared_error\", cv=5))\n    return rmse","9ba49ad8":"def generate_output(preds, save=False):\n    \"\"\"Collate predicted values into an output file and save as .csv.\"\"\"\n    output = submission.copy()\n    output[\"SalePrice\"] = preds\n    \n    if save:\n        date_time = datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")\n        output.to_csv(\"submissions\/submission-\"+date_time+\".csv\", index=False)\n    return output","55611e80":"def plot_coefs(model, X):\n    \"\"\"Display top 10 positive and top 10 negative model coefficients in a bar chart.\"\"\"\n    fig, ax = plt.subplots()\n    coefs = pd.Series(model.coef_, index=X.columns)\n    important_coefs = pd.concat([coefs.sort_values().head(10), coefs.sort_values().tail(10)])\n    sns.barplot(x=important_coefs, y=important_coefs.index, orient='h', ax=ax)\n    ax.grid()\n    plt.title(\"Top 20 Coefficients - Regularized Regression Model\")\n    plt.tight_layout()\n    plt.show()","c91c11b3":"def save_params_score(model_name, best_params, best_score):\n    \"\"\"Save optimized parameters and scores.\"\"\"\n    hyperparam_df.loc[model_name, \"Best Params\"] = [best_params]\n    hyperparam_df.loc[model_name, \"Best Score\"] = best_score","26990f8e":"def tune_hyperparameters(model_name, model, params, scaler):\n    \"\"\"Perform grid search to determine optimal hyperparameters and best score.\"\"\"\n    grid = GridSearchCV(model, params, n_jobs=4, cv=5, scoring=\"neg_mean_squared_error\", refit=True, verbose=3)\n    pipeline = make_pipeline(scaler, grid)\n    pipeline.fit(train, y)\n    best_params = grid.best_params_\n    best_score = np.round(np.sqrt(-grid.best_score_), 5)\n    \n    # Save params and score to hyperparameter df\n    save_params_score(model_name, best_params, best_score)\n    \n    return best_params, best_score","b0c0484b":"# https:\/\/www.kaggle.com\/fiorenza2\/journey-to-the-top-10\nclass CustomEnsembleRegressor(BaseEstimator, RegressorMixin):\n    \"\"\"Create an ensemble model.\"\"\"\n    def __init__(self, regressors=None):\n        self.regressors = regressors\n\n    def fit(self, X, y):\n        for regressor in self.regressors:\n            regressor.fit(X, y)\n\n    def predict(self, X):\n        self.predictions_ = list()\n        for regressor in self.regressors:\n            self.predictions_.append((regressor.predict(X).ravel()))\n        return (np.mean(self.predictions_, axis=0))","69c3d8e5":"def compare_predictions(file1, file2):\n    \"\"\"Concat two output files and find the difference between their predicted SalePrices.\"\"\"\n    directory = \"submissions\/\"\n    a = pd.read_csv(directory + file1)\n    b = pd.read_csv(directory + file2)\n    \n    c = pd.concat([a[\"SalePrice\"], b[\"SalePrice\"]], axis=1)\n    c.columns = [\"a\", \"b\"]\n    c[\"Diff\"] = c[\"b\"] - c[\"a\"]\n    return c","5e2a2f94":"temp = pd.concat([train.drop(columns=\"SalePrice\"), test])\nnulls = temp.isnull().sum()[temp.isnull().sum() > 0].sort_values(ascending=False).to_frame().rename(columns={0: \"MissingVals\"})\nnulls[\"MissingValsPct\"] = nulls[\"MissingVals\"] \/ len(temp)\nnulls","a83dc53f":"# Very helpful: https:\/\/www.kaggle.com\/juliencs\/a-study-on-regression-applied-to-the-ames-dataset\n\nfor df in [train, test]:\n    ## EASIER TO HANDLE:\n    \n    # PoolQC -> data description says NA = No Pool\n    df[\"PoolQC\"].fillna(value=\"None\", inplace=True)\n    # MiscFeature -> data description says NA = None\n    df[\"MiscFeature\"].fillna(value=\"None\", inplace=True)\n    # Alley -> data description says NA = No alley access\n    df[\"Alley\"].fillna(value=\"None\", inplace=True)\n    # Fence -> data description says NA = No fence\n    df[\"Fence\"].fillna(value=\"None\", inplace=True)\n    # FireplaceQu -> data description says NA = No fireplace\n    df[\"FireplaceQu\"].fillna(value=\"None\", inplace=True)\n    # Garage features -> data description says NA = No garage\n    df[\"GarageType\"].fillna(value=\"None\", inplace=True)\n    df[\"GarageFinish\"].fillna(value=\"None\", inplace=True)\n    df[\"GarageQual\"].fillna(value=\"None\", inplace=True)\n    df[\"GarageCond\"].fillna(value=\"None\", inplace=True)\n    df[\"GarageArea\"].fillna(value=0, inplace=True)\n    df[\"GarageCars\"].fillna(value=0, inplace=True)\n    # Basement features -> data description says NA = No garage\n    df[\"BsmtCond\"].fillna(value=\"None\", inplace=True)\n    df[\"BsmtExposure\"].fillna(value=\"None\", inplace=True)\n    df[\"BsmtQual\"].fillna(value=\"None\", inplace=True)\n    df[\"BsmtFinType1\"].fillna(value=\"None\", inplace=True)\n    df[\"BsmtFinSF1\"].fillna(value=0, inplace=True)\n    df[\"BsmtFinType2\"].fillna(value=\"None\", inplace=True)\n    df[\"BsmtFinSF2\"].fillna(value=0, inplace=True)\n    df[\"TotalBsmtSF\"].fillna(value=0, inplace=True)\n    df[\"BsmtUnfSF\"].fillna(value=0, inplace=True)\n    df[\"BsmtFullBath\"].fillna(value=0, inplace=True)\n    df[\"BsmtHalfBath\"].fillna(value=0, inplace=True)\n    # Functional -> data description says assume typical\n    df[\"Functional\"].fillna(value=\"Typ\", inplace=True)\n    \n    ## LESS CLEAR:\n    \n    # LotFrontage -> assume median\n    df[\"LotFrontage\"].fillna(value=df[\"LotFrontage\"].median(), inplace=True)\n    # GarageYrBlt -> assume equal to YearBuilt\n    df[\"GarageYrBlt\"].fillna(value=df[\"YearBuilt\"], inplace=True)\n    # MasVnrType -> NA most likely means no masonry veneer\n    df[\"MasVnrType\"].fillna(value=\"None\", inplace=True)\n    df[\"MasVnrArea\"].fillna(value=0, inplace=True)\n    # Utilities -> assume the mode\n    df[\"Utilities\"].fillna(value=df[\"Utilities\"].mode()[0], inplace=True)\n    # SaleType -> assume the mode\n    df[\"SaleType\"].fillna(value=df[\"SaleType\"].mode()[0], inplace=True)\n    # KitchenQual -> assume the mode\n    df[\"KitchenQual\"].fillna(value=df[\"KitchenQual\"].mode()[0], inplace=True)\n    # Electrical -> assume the mode\n    df[\"Electrical\"].fillna(value=df[\"Electrical\"].mode()[0], inplace=True)    \n    # MSZoning -> assume the mode\n    df[\"MSZoning\"].fillna(value=df[\"MSZoning\"].mode()[0], inplace=True)  \n    # Exterior1st -> assume the mode\n    df[\"Exterior1st\"].fillna(value=df[\"Exterior1st\"].mode()[0], inplace=True)  \n    # Exterior2nd -> assume the mode\n    df[\"Exterior2nd\"].fillna(value=df[\"Exterior2nd\"].mode()[0], inplace=True)","39515daf":"fig, ax = plt.subplots(1, 2, figsize=(10,5))\n\nsns.distplot(train[\"LotFrontage\"].dropna(), ax=ax[0])\nax[0].axvline(x=train[\"LotFrontage\"].mean(), ymin=0, ymax=1, color='r', label=\"Mean\")\nax[0].axvline(x=train[\"LotFrontage\"].median(), ymin=0, ymax=1, color='g', label=\"Median\")\nax[0].legend()\nax[0].set_title(\"LotFrontage Distribution\")\n\n\nsns.distplot(train[\"GarageYrBlt\"].dropna(), ax=ax[1])\nax[1].axvline(x=train[\"GarageYrBlt\"].mean(), ymin=0, ymax=1, color='r', label=\"Mean\")\nax[1].axvline(x=train[\"GarageYrBlt\"].median(), ymin=0, ymax=1, color='g', label=\"Median\")\nax[1].legend()\nax[1].set_title(\"GarageYrBlt Distribution\")\n\nplt.show()","bcb20ece":"mask = (train[\"GrLivArea\"] > 4000) & (train[\"SalePrice\"] < 300_000)\nmask = mask.replace(to_replace=[1, 0], value=[\"Outlier\", \"Data\"])\nsns.scatterplot(x=train[\"GrLivArea\"], y=train[\"SalePrice\"], hue=mask)\nplt.title(\"Spotting Outliers - GrLivArea > 4000 sq ft & SalePrice < $300,000\")\nplt.show()\n\ntrain[(train[\"GrLivArea\"] > 4000) & (train[\"SalePrice\"] < 300_000)]","49ea960c":"# https:\/\/www.kaggle.com\/fiorenza2\/journey-to-the-top-10\noutliers = [88, 462, 523, 588, 632, 968, 1298, 1324]\ntrain = train.drop(outliers)","679b36d9":"mask = test[\"GarageYrBlt\"] == 2207\nsns.scatterplot(x=test[\"YearBuilt\"], y=test[\"GarageYrBlt\"], hue=mask, legend=False)\nplt.title(\"Spotting Outliers - Incorrect GarageYrBlt Value\")\nplt.show()","9c9fb95e":"# Assume that 2207 was meant to be 2007\ntest.loc[test[\"GarageYrBlt\"] == 2207, \"GarageYrBlt\"] = 2007","2b8c4697":"# By making these substitutions, the columns are automatically cast to datatype object   \nfor df in [train, test]:\n    df.replace({\"MSSubClass\" : {20 : \"SC20\", 30 : \"SC30\", 40 : \"SC40\", 45 : \"SC45\", \n                               50 : \"SC50\", 60 : \"SC60\", 70 : \"SC70\", 75 : \"SC75\", \n                               80 : \"SC80\", 85 : \"SC85\", 90 : \"SC90\", 120 : \"SC120\", \n                               150 : \"SC150\", 160 : \"SC160\", 180 : \"SC180\", 190 : \"SC190\"}}, inplace=True)","2fdc6b26":"# https:\/\/www.kaggle.com\/lavanyashukla01\/how-i-made-top-0-3-on-a-kaggle-competition\nfor df in [train, test]:\n    df = df.drop(['Utilities', 'Street', 'PoolQC',], axis=1)\n    \n    df['BsmtFinType1_Unf'] = 1*(df['BsmtFinType1'] == 'Unf')\n    df['HasWoodDeck'] = (df['WoodDeckSF'] == 0) * 1\n    df['HasOpenPorch'] = (df['OpenPorchSF'] == 0) * 1\n    df['HasEnclosedPorch'] = (df['EnclosedPorch'] == 0) * 1\n    df['Has3SsnPorch'] = (df['3SsnPorch'] == 0) * 1\n    df['HasScreenPorch'] = (df['ScreenPorch'] == 0) * 1\n    df['YearsSinceRemodel'] = df['YrSold'].astype(int) - df['YearRemodAdd'].astype(int)\n    df['Total_Home_Quality'] = df['OverallQual'] + df['OverallCond']\n    df['TotalSF'] = df['TotalBsmtSF'] + df['1stFlrSF'] + df['2ndFlrSF']\n    df['YrBltAndRemod'] = df['YearBuilt'] + df['YearRemodAdd']\n    df['Total_sqr_footage'] = (df['BsmtFinSF1'] + df['BsmtFinSF2'] + df['1stFlrSF'] + df['2ndFlrSF'])\n    df['Total_Bathrooms'] = (df['FullBath'] + (0.5 * df['HalfBath']) + df['BsmtFullBath'] + (0.5 * df['BsmtHalfBath']))\n    df['Total_porch_sf'] = (df['OpenPorchSF'] + df['3SsnPorch'] + df['EnclosedPorch'] + df['ScreenPorch'] + df['WoodDeckSF'])\n    df['TotalBsmtSF'] = df['TotalBsmtSF'].apply(lambda x: np.exp(6) if x <= 0.0 else x)\n    df['2ndFlrSF'] = df['2ndFlrSF'].apply(lambda x: np.exp(6.5) if x <= 0.0 else x)\n    df['GarageArea'] = df['GarageArea'].apply(lambda x: np.exp(6) if x <= 0.0 else x)\n    df['GarageCars'] = df['GarageCars'].apply(lambda x: 0 if x <= 0.0 else x)\n    df['LotFrontage'] = df['LotFrontage'].apply(lambda x: np.exp(4.2) if x <= 0.0 else x)\n    df['MasVnrArea'] = df['MasVnrArea'].apply(lambda x: np.exp(4) if x <= 0.0 else x)\n    df['BsmtFinSF1'] = df['BsmtFinSF1'].apply(lambda x: np.exp(6.5) if x <= 0.0 else x)\n    df['haspool'] = df['PoolArea'].apply(lambda x: 1 if x > 0 else 0)\n    df['has2ndfloor'] = df['2ndFlrSF'].apply(lambda x: 1 if x > 0 else 0)\n    df['hasgarage'] = df['GarageArea'].apply(lambda x: 1 if x > 0 else 0)\n    df['hasbsmt'] = df['TotalBsmtSF'].apply(lambda x: 1 if x > 0 else 0)\n    df['hasfireplace'] = df['Fireplaces'].apply(lambda x: 1 if x > 0 else 0)","e233e27c":"numeric_features = np.array([c for c in train.select_dtypes(include=[np.number]).columns if c != \"SalePrice\"])\nnumeric_features","5eb0944c":"categorical_features = np.array(train.select_dtypes(include=[np.object]).columns)\ncategorical_features","cc8508aa":"ordinal_features = [\"ExterQual\", \"BsmtQual\", \"BsmtCond\", \"BsmtExposure\", \"BsmtFinType1\", \"BsmtFinType2\", \n                \"LandSlope\", \"ExterCond\", \"HeatingQC\", \"KitchenQual\", \"Functional\", \"FireplaceQu\", \n                \"GarageFinish\", \"GarageQual\", \"GarageCond\", \"PoolQC\", \"LotShape\", \"Utilities\"]\nnominal_features = list(set(categorical_features) - set(ordinal_features))","dedcf5da":"for df in [train, test]:\n    df.replace({\"BsmtCond\" : {\"None\" : 0, \"Po\" : 1, \"Fa\" : 2, \"TA\" : 3, \"Gd\" : 4, \"Ex\" : 5},\n               \"BsmtExposure\" : {\"None\" : 0, \"No\": 0, \"Mn\" : 1, \"Av\": 2, \"Gd\" : 3},\n               \"BsmtFinType1\" : {\"None\" : 0, \"Unf\" : 1, \"LwQ\": 2, \"Rec\" : 3, \"BLQ\" : 4, \"ALQ\" : 5, \"GLQ\" : 6},\n               \"BsmtFinType2\" : {\"None\" : 0, \"Unf\" : 1, \"LwQ\": 2, \"Rec\" : 3, \"BLQ\" : 4, \"ALQ\" : 5, \"GLQ\" : 6},\n               \"BsmtQual\" : {\"None\" : 0, \"Po\" : 1, \"Fa\" : 2, \"TA\": 3, \"Gd\" : 4, \"Ex\" : 5},\n               \"ExterCond\" : {\"Po\" : 1, \"Fa\" : 2, \"TA\": 3, \"Gd\": 4, \"Ex\" : 5},\n               \"ExterQual\" : {\"Po\" : 1, \"Fa\" : 2, \"TA\": 3, \"Gd\": 4, \"Ex\" : 5},\n               \"FireplaceQu\" : {\"None\" : 0, \"Po\" : 1, \"Fa\" : 2, \"TA\" : 3, \"Gd\" : 4, \"Ex\" : 5},\n               \"Functional\" : {\"Sal\" : 1, \"Sev\" : 2, \"Maj2\" : 3, \"Maj1\" : 4, \"Mod\": 5, \"Min2\" : 6, \"Min1\" : 7, \"Typ\" : 8},\n               \"GarageCond\" : {\"None\" : 0, \"Po\" : 1, \"Fa\" : 2, \"TA\" : 3, \"Gd\" : 4, \"Ex\" : 5},\n               \"GarageQual\" : {\"None\" : 0, \"Po\" : 1, \"Fa\" : 2, \"TA\" : 3, \"Gd\" : 4, \"Ex\" : 5},\n               \"GarageFinish\" : {\"None\" : 0, \"Unf\" : 1, \"RFn\" : 2, \"Fin\" : 3},\n               \"HeatingQC\" : {\"Po\" : 1, \"Fa\" : 2, \"TA\" : 3, \"Gd\" : 4, \"Ex\" : 5},\n               \"KitchenQual\" : {\"Po\" : 1, \"Fa\" : 2, \"TA\" : 3, \"Gd\" : 4, \"Ex\" : 5},\n               \"LandSlope\" : {\"Sev\" : 1, \"Mod\" : 2, \"Gtl\" : 3},\n               \"LotShape\" : {\"IR3\" : 1, \"IR2\" : 2, \"IR1\" : 3, \"Reg\" : 4},\n               \"PoolQC\" : {\"None\" : 0, \"Fa\" : 1, \"TA\" : 2, \"Gd\" : 3, \"Ex\" : 4},\n               \"Utilities\" : {\"ELO\" : 1, \"NoSeWa\" : 2, \"NoSewr\" : 3, \"AllPub\" : 4}}\n                , inplace=True)\n    \n    df[ordinal_features] = df[ordinal_features].astype('int64')","4dfe9de6":"nominal_data = pd.concat([train[nominal_features], test[nominal_features]])\nnominal_data.head()","ed5289cf":"train.shape, test.shape, nominal_data.shape","f9920a57":"nominal_data = pd.get_dummies(nominal_data)\nnominal_data.head()","cfa66d31":"train_dummies = nominal_data[:train.shape[0]]\ntest_dummies = nominal_data[train.shape[0]:]\ntrain_dummies.shape, test_dummies.shape","b7914a44":"y = train[\"SalePrice\"]\ntrain = pd.concat([train[numeric_features], train[ordinal_features], train_dummies], axis=1)\ntest = pd.concat([test[numeric_features], test[ordinal_features], test_dummies], axis=1)","f760d10a":"train.shape, test.shape, set(train.columns) - set(test.columns)","f23c360f":"y = np.log1p(y) # SalePrice\nskew_threshold = 0.5 # As a general rule of thumb, |skew| > 0.5 is considered moderately skewed\n\nfor df in [train[numeric_features], test[numeric_features]]:\n    skewness = df.apply(lambda x: stats.skew(x))\n    skewness = skewness[np.abs(skewness) > skew_threshold]\n    skewed_cols = skewness.index\n    log_transformed_cols = np.log1p(df[skewed_cols])\n    df[skewed_cols] = log_transformed_cols\n    print(\"Transforming {} features...\".format(len(skewed_cols)))\n\ntrain[numeric_features] = train[numeric_features].astype('int64')\ntest[numeric_features] = test[numeric_features].astype('int64')","f63db609":"def compare_ridge_lasso(X, y):\n    ridge_alphas = np.arange(2, 50, 1)\n    cv_ridge = [rmse_cv(Ridge(alpha=alpha), X, y).mean() for alpha in ridge_alphas]\n    cv_ridge = pd.Series(cv_ridge, index=ridge_alphas)\n    best_alpha_ridge = ridge_alphas[np.argmin(cv_ridge.values)]\n\n    lasso_alphas = np.arange(0.0001, 0.005, 0.0002)\n    cv_lasso = [rmse_cv(Lasso(alpha=alpha, tol=0.1), X, y).mean() for alpha in lasso_alphas]\n    cv_lasso = pd.Series(cv_lasso, index=lasso_alphas)\n    best_alpha_lasso = lasso_alphas[np.argmin(cv_lasso.values)]\n    \n    print(\"Best RMSE for Ridge Regression = {:.4f} for alpha = {}\".format(cv_ridge.min(), best_alpha_ridge))\n    print(\"Best RMSE for Lasso Regression = {:.4f} for alpha = {}\".format(cv_lasso.min(), best_alpha_lasso))\n    \n    fig, ax = plt.subplots(1, 2, figsize=(10,5))\n    \n    sns.lineplot(x=ridge_alphas, y=cv_ridge, ax=ax[0])\n    ax[0].set_title(\"Ridge Regression - Alpha Optimization\")\n    ax[0].set_xlabel(\"Alpha\")\n    ax[0].set_ylabel(\"RMSE\")\n    \n    sns.lineplot(x=lasso_alphas, y=cv_lasso, ax=ax[1])\n    ax[1].set_title(\"Lasso Regression - Alpha Optimization\")\n    ax[1].set_xlabel(\"Alpha\")\n    ax[1].set_ylabel(\"RMSE\")\n    \n    plt.tight_layout()\n    plt.show()\n    \n    return Ridge(alpha=best_alpha_ridge) if cv_ridge.min() < cv_lasso.min() else Lasso(alpha=best_alpha_lasso)","561a8527":"numeric_model = compare_ridge_lasso(train[numeric_features], y)","85c50a7c":"numeric_model.fit(train[numeric_features], y)\nplot_coefs(numeric_model, train[numeric_features])","f3106951":"numeric_ordinal_features = np.append(numeric_features, ordinal_features)\nnumeric_ordinal_model = compare_ridge_lasso(train[numeric_ordinal_features], y)\nnumeric_ordinal_model.fit(train[numeric_ordinal_features], y)","13c12949":"plot_coefs(numeric_ordinal_model, train[numeric_ordinal_features])","14ffb457":"full_model = compare_ridge_lasso(train, y)\nfull_model.fit(train, y)","5a3fd5b5":"# Baseline model predictions:\npreds = full_model.predict(test)\noutput = generate_output(preds, False)\noutput.sort_values(by=\"SalePrice\", ascending=False).head()","9d38d301":"fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n\nsns.distplot(train_[\"SalePrice\"], bins=25, ax=ax[0])\nax[0].set_title(\"Distribution of Training House Prices\")\nax[0].set_xlim([0,700000])\n\nsns.distplot(output[\"SalePrice\"], bins=60, ax=ax[1])\nax[1].set_title(\"Distribution of Predicted House Prices\")\nax[1].set_xlim([0,700000])\n\nplt.tight_layout()\nplt.show()","52ac0863":"plot_coefs(full_model, train)","f5b6b560":"# https:\/\/www.kaggle.com\/humananalog\/xgboost-lasso\ncols_to_drop = [\"MSZoning_C (all)\", \"MSSubClass_SC160\", \"Condition2_PosN\", \"Exterior1st_ImStucc\",\n                \"RoofMatl_Membran\", \"RoofMatl_Metal\", \"RoofMatl_Roll\", \"Condition2_RRAe\", \n                \"Condition2_RRAn\", \"Condition2_RRNn\", \"Heating_Floor\", \"Heating_OthW\", \n                \"Electrical_Mix\", \"MiscFeature_TenC\", \"MSSubClass_SC150\", \"Exterior1st_Stone\",\n                \"Exterior2nd_Other\", \"HouseStyle_2.5Fin\"]\ntrain = train.drop(columns=cols_to_drop)\ntest = test.drop(columns=cols_to_drop)","1e5ff924":"ridge = Ridge(random_state=SEED)\nlasso = Lasso(random_state=SEED)\nelnt = ElasticNet(random_state=SEED)\nkr = KernelRidge()\nsvr = SVR()\nknn = KNeighborsRegressor(n_jobs=-1)\npls = PLSRegression()\ndt = DecisionTreeRegressor(random_state=SEED)\nrf = RandomForestRegressor(random_state=SEED, n_jobs=-1)\ngb = GradientBoostingRegressor(random_state=SEED)\nxgb = XGBRegressor(random_state=SEED, n_jobs=-1, objective=\"reg:squarederror\")\n\nmodels = [(\"Ridge\", ridge), (\"Lasso\", lasso), (\"ElasticNet\", elnt), (\"KernelRidge\", kr), \\\n          (\"SVR\", svr), (\"KNNeighbors\", knn), (\"PLS\", pls), (\"DecisionTree\", dt), (\"RandomForest\", rf), \\\n          (\"GradientBoosting\", gb), (\"XGBoost\", xgb)]","a956150c":"hyperparam_df = pd.DataFrame(index=[m[0] for m in models],\n                             columns=[\"Baseline Score - StandardScaler\", \"Baseline Score - RobustScaler\", \n                                      \"Best Score\", \"Best Params\"])","dcdd3a2a":"rmses = []\nfor model in models:\n    pipeline = make_pipeline(StandardScaler(), model[1])\n    print(\"Scoring {}...\".format(model[0]))\n    rmse = np.round(np.sqrt(-cross_val_score(pipeline, train, y, scoring=\"neg_mean_squared_error\", cv=5)).mean(), 3)\n    rmses.append(rmse)\n    hyperparam_df.loc[model[0], \"Baseline Score - StandardScaler\"] = rmse","5dde7d5c":"rmses_robust = []\nfor model in models:\n    pipeline = make_pipeline(RobustScaler(), model[1])\n    print(\"Scoring {}...\".format(model[0]))\n    rmse = np.round(np.sqrt(-cross_val_score(pipeline, train, y, scoring=\"neg_mean_squared_error\", cv=5)).mean(), 3)\n    rmses_robust.append(rmse)\n    hyperparam_df.loc[model[0], \"Baseline Score - RobustScaler\"] = rmse","72972e1b":"hyperparam_df","604a81f5":"hyperparam_df[[\"Baseline Score - StandardScaler\", \"Baseline Score - RobustScaler\"]] = hyperparam_df[[\"Baseline Score - StandardScaler\", \"Baseline Score - RobustScaler\"]].astype('float64')","cf78dd0c":"fig, ax = plt.subplots()\nsns.scatterplot(x=hyperparam_df.index, y=np.log1p(hyperparam_df[\"Baseline Score - StandardScaler\"]), s=200, label=\"StandardScaler\", ax=ax)\nsns.scatterplot(x=hyperparam_df.index, y=np.log1p(hyperparam_df[\"Baseline Score - RobustScaler\"]), s=200, label=\"RobustScaler\", ax=ax)\nplt.title(\"Impact of StandardScaler vs RobustScaler on RMSE Cross Val Scores\")\nplt.ylabel(\"log(RMSE)\")\nplt.xticks(rotation=90)\nax.grid()\nplt.show()","b4293c84":"fig, ax = plt.subplots()\nsns.scatterplot(x=hyperparam_df.index, y=np.log1p(hyperparam_df[\"Baseline Score - RobustScaler\"]), s=200, hue=hyperparam_df[\"Baseline Score - RobustScaler\"], palette='coolwarm_r', legend=False, ax=ax)\nplt.title(\"RMSE Cross Val Scores by Model (RobustScaler)\")\nplt.xticks(rotation=90)\nax.grid()\nplt.show()","cb64ead2":"ridge_param_grid = {\"alpha\": np.arange(5, 30, 1), \n                    \"random_state\": [SEED]}\n\n# Optimized Params:\nridge_param_grid = {'alpha': [14], 'random_state': [42]}\n\nridge = Ridge()\nridge_best_params, ridge_best_score = tune_hyperparameters(\"Ridge\", ridge, ridge_param_grid, RobustScaler())\nridge_best_params, ridge_best_score","6225bff7":"lasso_param_grid = {\"alpha\": np.arange(0.0001, 0.001, .00002), \n                    \"random_state\": [SEED]}\n\n# Optimized Params:\nlasso_param_grid = {'alpha': [0.00092], 'random_state': [42]}\n\nlasso = Lasso()\nlasso_best_params, lasso_best_score = tune_hyperparameters(\"Lasso\", lasso, lasso_param_grid, StandardScaler())\nlasso_best_params, lasso_best_score","b10367d2":"elnt_param_grid = {\"alpha\": [0.0001, 0.0002, 0.0003, 0.01, 0.1, 2], \n                      \"l1_ratio\": [0.2, 0.85, 0.95, 0.98, 1], \n                      \"random_state\": [SEED]}\n\n# Optimized Params:\nelastic_param_grid = {'alpha': [0.0002], 'l1_ratio': [1], 'random_state': [42]}\n\nelnt = ElasticNet()\nelnt_best_params, elnt_best_score = tune_hyperparameters(\"ElasticNet\", elnt, elnt_param_grid, RobustScaler())\nelnt_best_params, elnt_best_score","0b5cb23b":"kr_param_grid = {\"alpha\": [0.25, 0.4, 0.5, 0.6, 0.75, 1], \n                     \"kernel\": [\"linear\", \"polynomial\"], \"degree\": [2, 3], \"coef0\": [1.5, 2, 3]}\n\n# Optimized Params:\nkernel_param_grid = {'alpha': [0.25], 'coef0': [1.5], 'degree': [2], 'kernel': ['linear']}\n\nkr = KernelRidge()\nkr_best_params, kr_best_score = tune_hyperparameters(\"KernelRidge\", kr, kr_param_grid, RobustScaler())\nkr_best_params, kr_best_score","72219ecd":"dt_param_grid = {\"max_depth\": [3, 4, 5],\n                \"min_samples_leaf\": [2, 3, 4],\n                \"min_samples_split\": [3, 4, 5],\n                \"random_state\": [SEED]}\n\n# Optimized Params:\ndt_param_grid = {'max_depth': [5],\n                 'min_samples_leaf': [4],\n                 'min_samples_split': [3],\n                 'random_state': [42]}\n\ndt = RandomForestRegressor()\ndt_best_params, dt_best_score = tune_hyperparameters(\"DecisionTree\", dt, dt_param_grid, StandardScaler())\ndt_best_params, dt_best_score","d557e4aa":"rf_param_grid = {\"n_estimators\": [50, 100, 500, 1000], \n                \"max_depth\": [1, 2, 3, 4, 5],\n                \"min_samples_leaf\": [2, 3, 4],\n                \"min_samples_split\": [3, 4, 5],\n                \"random_state\": [SEED]}\n\n# Optimized Params:\nrf_param_grid = {}\n\nrf = RandomForestRegressor()\nrf_best_params, rf_best_score = tune_hyperparameters(\"RandomForest\", rf, rf_param_grid, StandardScaler())\nrf_best_params, rf_best_score","898af46e":"knn_param_grid = {\"n_neighbors\": [3, 4, 5, 6], \n                \"algorithm\": [\"auto\", \"ball_tree\", \"kd_tree\", \"brute\"],\n                \"leaf_size\": [10, 20, 30, 40, 50],\n                \"p\": [1, 2]}\n\n# Optimized Params:\nknn_param_grid = {'algorithm': ['auto'], 'leaf_size': [10], 'n_neighbors': [4], 'p': [1]}\n\nknn = KNeighborsRegressor()\nknn_best_params, knn_best_score = tune_hyperparameters(\"KNNeighbors\", knn, knn_param_grid, StandardScaler())\nknn_best_params, knn_best_score","993e4fb6":"xgb_param_grid = {\"colsample_bytree\": [0.1, 0.2],\n                 \"learning_rate\": [0.01, 0.05],\n                 \"max_depth\": [3, 4],\n                 \"n_estimators\": [2000],\n                 \"random_state\": [42]}\n\n# Optimized Params:\nxgb_param_grid = {'colsample_bytree': [0.2], 'learning_rate': [0.01], 'max_depth': [4], 'n_estimators': [2000], 'random_state': [42]}\n\nxgb = XGBRegressor(n_jobs=-1, objective=\"reg:squarederror\")\nxgb_best_params, xgb_best_score = tune_hyperparameters(\"XGBoost\", xgb, xgb_param_grid, StandardScaler())\nxgb_best_params, xgb_best_score","9751a866":"svr_param_grid = {\"C\": [20, 30, 40, 50, 60, 70],\n                 \"epsilon\": [0.1, 0.01, 0.001],\n                 \"gamma\": [0.0005, 0.0001, 0.00005]}\n\n# Optimized Params:\nsvr_param_grid = {'C': [50], 'epsilon': [0.01], 'gamma': [0.0001]}\n\nsvr = SVR()\nsvr_best_params, svr_best_score = tune_hyperparameters(\"SVR\", svr, svr_param_grid, StandardScaler())\nsvr_best_params, svr_best_score","477c6761":"pls_param_grid = {\"n_components\": [2, 3, 4, 5]}\n\n# Optimized Params:\npls_param_grid = {\"n_components\": [5]}\n\npls = PLSRegression()\npls_best_params, pls_best_score = tune_hyperparameters(\"PLS\", pls, pls_param_grid, StandardScaler())\npls_best_params, pls_best_score","6f67d40d":"gb_param_grid = {\"n_estimators\": [1000, 3000],\n                \"learning_rate\": [0.01],\n                \"max_depth\": [4],\n                \"loss\": [\"huber\"],\n                \"max_features\": [\"sqrt\"],\n                \"min_samples_leaf\": [15],\n                \"min_samples_split\": [10]}\n\n# Optimized Params:\ngb_param_grid = {'learning_rate': [0.01],\n                 'loss': ['huber'],\n                 'max_depth': [4],\n                 'max_features': ['sqrt'],\n                 'min_samples_leaf': [15],\n                 'min_samples_split': [10],\n                 'n_estimators': [3000]}\n\ngb = GradientBoostingRegressor()\ngb_best_params, gb_best_score = tune_hyperparameters(\"GradientBoosting\", gb, gb_param_grid, StandardScaler())\ngb_best_params, gb_best_score","3f8df9ef":"hyperparam_df[[\"Best Score\"]] = hyperparam_df[[\"Best Score\"]].astype('float64')\nbest_baseline_scores = np.min(hyperparam_df[[\"Baseline Score - StandardScaler\", \"Baseline Score - RobustScaler\"]], axis=1)\n\nfig, ax = plt.subplots()\nsns.scatterplot(x=hyperparam_df.index, y=np.log1p(best_baseline_scores), s=200, label=\"Baseline Score\", ax=ax)\nsns.scatterplot(x=hyperparam_df.index, y=np.log1p(hyperparam_df[\"Best Score\"]), s=200, label=\"Best Score\", ax=ax)\nplt.title(\"Optimized vs Baseline RMSE Cross Val Scores\")\nplt.ylabel(\"log(RMSE)\")\nplt.xticks(rotation=90)\nax.grid()\nplt.show()","d3a28cd7":"hyperparam_df.sort_values(by=\"Best Score\", inplace=True)\nhyperparam_df","37aa6838":"# Lasso\nscaler = RobustScaler()\nlasso = Lasso(**lasso_best_params)\nlasso.fit(scaler.fit_transform(train), y)\npredictions_lasso = np.expm1(lasso.predict(scaler.transform(test)))\n\n# SVR\nscaler = StandardScaler()\nsvr = SVR(**svr_best_params)\nsvr.fit(scaler.fit_transform(train), y)\npredictions_svr = np.expm1(svr.predict(scaler.transform(test)))\n\n# XGB\n# https:\/\/www.kaggle.com\/fiorenza2\/journey-to-the-top-10\nscaler = StandardScaler()\n_ = xgb_best_params.pop(\"random_state\") if \"random_state\" in xgb_best_params else \"\"\nxgb1 = XGBRegressor(**xgb_best_params, random_state = 42)\nxgb2 = XGBRegressor(**xgb_best_params, random_state = 6666)\nxgb3 = XGBRegressor(**xgb_best_params, random_state = 0)\nxgb_ensemble = CustomEnsembleRegressor([xgb1, xgb2, xgb3])\nxgb_ensemble.fit(scaler.fit_transform(train), y)\npredictions_xgb = np.expm1(xgb_ensemble.predict(scaler.transform(test)))\n\n# Collate Predictions\npredictions = pd.concat([pd.Series(predictions_lasso), \n                         pd.Series(predictions_svr), \n                         pd.Series(predictions_xgb)], axis=1).head()\npredictions.columns = [\"Lasso\", \"SVR\", \"XGBoost\"]","3df6e032":"predictions.head()","a5178aab":"# Average Predictions\npredictions_avg = (predictions_lasso + predictions_svr + predictions_xgb) \/ 3\noutput = generate_output(predictions_avg, save=False)\noutput.head()","df4328ab":"Using the data description file, we can determine which categorical columns refer to ordinal characteristics and replace feature values with ordered numbers. The reason we choose to do this manually (and don't use something like LabelEncoder) is because we want to be able to control the ordering of features, i.e. be able to specify which value corresponds with 0, which value corresponds with 1 etc.","e5e9cece":"**Lasso**","aa55af90":"**LabelEncode the Ordinal Features**","64142cfa":"By plotting the top coefficients, we can get an idea of which features the model deems important.","ac43ea3d":"**Bucketing Features**\n\nIt's important that we handle each different type of feature correctly:\n\n* Ordinal features: replace with ordered labels\n* Categorical features: one-hot encode\n* Numeric features: correct for skew and scale","d76bfbd1":"### Initial Modeling\n\nI chose to create 3 baseline models using different sets of features:\n\n* Numeric only\n* Numeric + ordinal\n* All features\n\n**Baseline Model Using Numeric Data Only**","e388b811":"**Compare Optimized to Baseline Scores**\n\nHopefully, by performing hyperparameter optimization, we have succeeded in decreasing the RMSEs of our collection of models.","9a130b7d":"**Log-Transform Skewed Numeric Variables**\n\nI read in a few different places that a |skew| > 0.5 is considered \"moderately skewed\" and have chosen this as my threshold. If you have any input, please leave a comment below.","4ebfea09":"**3rd Model Using All Features (Numeric, Ordinal & Nominal)**","f4662b60":"**ElasticNet**","500463b8":"**SVR**","8a8992c2":"**Compare Predicted Price Distribution with Training Distribution**\n\nHow do our predictions compare with the actual house prices in the training data?","bf83244a":"**2nd Model Using Numeric & Ordinal Features**","92e70a99":"### Generate Predictions\n\nI experimented with a variety of different model combinations to varying success on the leaderboard.\n\nMy final average model is comprised of:\n\n* Lasso + `RobustScaler`\n* SVR + `StandardScaler`\n* XGBoost Ensemble + `StandardScaler`\n\nIf you tried averaging and found a different winning combination, let me know below.","7280cdfd":"Baseline model evaluation using `StandardScaler()`:","15bb9233":"**One-Hot Encode the Nominal Features**\n\nOne-hot encoding transforms one column (feature) into many \"dummy\" columns, one for each unique category\/value.\n\nConcat the appropriate train and test observations, so that we produce uniform columns between datasets:","f0587382":"### Conclusion\n\nI learned so much from this challenge, including:\n\n* Cross validation isn't the be-all end-all. This is a small dataset, so the fantastic CV scores you're getting on the training data will likely not translate to the official leaderboard.\n* Outliers can have a TREMENDOUS impact on model performance. Finding them still seems quite arbitrary to me, which probably means I don't understand it enough.\n* Some models perform better\/worse using different scalers (ex: `StandardScaler` or `RobustScaler`).\n* And on the same note, if you use a scaler, don't forget to transform your test set. This drove me crazy for two days!!\n\nI'd love to know what you think and if you have any suggestions for improvement.\n\nIf you're just getting started, please check out my [House Prices EDA notebook](https:\/\/www.kaggle.com\/amypeniston\/house-prices-eda-essentials-for-beginners) to familiarize yourself with the data.\n\nThanks so much for reading. Until next time, happy coding :)","67f0c215":"### Optimize Hyperparameters\n\nThe goal of hyperparameter optimization is to improve the performance of our models (decrease bias). However, we have to be careful not to overfit the data. \n\nNote that I selected either `StandardScaler` or `RobustScaler` based on the initial baseline scores.\n\nOptimized parameters are provided to cut down on runtime.","ecdc6504":"**Combine Top Scoring Models**\n\nI found that the using multiple models and averaging their predictions resulted in the best leaderboard score. This is likely due to the overfitting of one model offsetting the underfitting of another model. ","581e4ab3":"**Load the Data**","32aafcc2":"**Drop Additional Columns**\n\nWe want to remove columns corresponding to categorical features that are particularly rare. This helps to prevent the model from overfitting on features that aren't present in both train and test.","968ffd59":"In addition to the `GrLivArea` outliers, I found several notebooks citing additional outliers. I attempted to  recreate the code that generated their selections (using something called the Bonferroni test), however I struggled to come to the same conclusions. Would love to hear about your outlier selection process in the comments.","2c1d5532":"By using the data description .txt file (`ctrl`+`f` is your friend), we can make sensible choices with regards to filling missing values. However, in some cases, we have to use our best judgement, as it's not 100% straightforward.","f3d42b39":"When filling NAs, it can be helpful to plot the mean and the median alongside the feature's distribution. This can help illustrate when the mean is impacted by outliers and when it is better to fill NAs with the median.","fea6ff8d":"**Convert Select Numeric Features to Categorical Features**\n\nSome of the numerical features are actually categorical. While I found that converting `MSSubClass` improved model performance, converting `MoSold` did not.","dc9ffd74":"**XGBoost**","7537bd2a":"**DecisionTree**","66cc5fe1":"**Remove\/Fix Outliers**\n\nThere are potentially many outliers in the data, however, here, I will address only what I see as the biggest offenders. \n\nFirst, the `GrLivArea` outliers that are mentioned in the [author's original paper](http:\/\/jse.amstat.org\/v19n3\/decock.pdf) on pg. 4; let's follow the author's advice and remove the unusual homes that are more than 4000 square feet and less than \\\\$300,000. These points clearly do not follow the trend of the remaining data.","da118d71":"**Feature Engineering**\n\nFeature engineering allows us to create new features using existing data.","eaf8d711":"**GradientBoosting**","bcff86fc":"### Evaluate Models\n\n**Baseline Scores**\n\nRather than splitting the data using `train_test_split`, let's use `cross_val_score` to evaluate our models. The benefit of this approach is that each observation will be used in both train and test, which is critical when dealing with limited input data.","6a977386":"Join the dummy columns with the numeric and ordinal columns:","9c21890a":"**Create Helper Functions**","7ea9ebcd":"Notice how using `RobustScaler` significantly reduced the RMSE for the KernelRidge model while increasing RMSE for SVR and KNN.","afe1e5bb":"**KNN**","d4483488":"# House Prices - Data Preprocessing & Modeling\n\nGreetings! In this notebook we will apply what we have learned from [exploratory data analysis](https:\/\/www.kaggle.com\/amypeniston\/house-prices-eda-essentials-for-beginners) to data preprocessing. During this stage, we will fit some baseline models to make sure we're on the right track.\n\nOnce the data has been cleaned, we will evaluate a variety of different algorithms and perform hyperparameter optimization. Then we will create final predictions by averaging a selection of the best performing models.\n\n**At the time of uploading, the blended model created at the end of this notebook scored in the top 13% with a leaderboard score of `0.11666`.**\n\nSo, based on earlier EDA, we know:\n\n* There are a lot of missing values in the data -> need to process\n* There are also duplicative (highly-correlated) features that add no extra value) -> our dataset is a candidate for regularization\n* `Id` can probably be safely deleted as it is simply a auto-incrementing label for each observation -> delete this feature\n* The target variable, `SalePrice`, is right skewed -> apply a log-transformation\n* Other numeric variables may also be right skewed -> apply a log-transformation when skew is greater than our threshold\n\nLet's get started.","2e6a5bb9":"**RandomForest**","3bc0a6b9":"**Load the Good Stuff**","16cefb77":"### Preprocessing\n\n**Deal with Missing Values**\n\nModels don't do well with missing values. We'll need to perform substitutions or drop observations\/features to remove all missing values from the data.\n\nFirst, let's get a sense of how many missing values we have.","da086e5f":"**KernelRidge**","05014486":"Verify that we have extracted the correct number of rows for train vs. test. Also make sure that we have the same columns in both sets.","1dd086d0":"**Ridge**","d401f515":"**PLS**","cd546e15":"Narrowing our focus, we can look at the RMSEs for the `RobustScaler` results alone.","23835ece":"Create one dataframe to hold of our model scores:","4a42758d":"We can view log(RMSEs) on plot to get a better sense of how the choice of scaler influences model performance. ","9a967399":"Next, let's fix the `GarageYrBlt` value for one particular house in the test set by assuming that 2207 is supposed to be 2007.","24d874c2":"Baseline model evaluation using `RobustScaler()`:"}}